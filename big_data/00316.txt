Convergence of Limited Communications Gradient Methods

Sindri Magn´usson, Kathryn Heal, Chinwendu Enyioha, Na Li, Carlo Fischione, and Vahid Tarokh

6
1
0
2

 
r
a

M
 
1

 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
6
1
3
0
0

.

3
0
6
1
:
v
i
X
r
a

Abstract— Distributed control and decision making increas-
ingly play a central role in economical and sustainable operation
of cyber-physical systems. Nevertheless, the full potential of the
technology has not yet been fully exploited in practice due to
communication limitations of real-world infrastructures. This
work investigates the fundamental properties of gradient meth-
ods for distributed optimization, where gradient information is
communicated at every iteration, when using limited number of
communicated bits. In particular, a general class of quantized
gradient methods are studied where the gradient direction is
approximated by a ﬁnite quantization set. Conditions on the
quantization set are provided that are necessary and sufﬁcient
to guarantee the ability of these methods to minimize any
convex objective function with Lipschitz continuous gradient
and a nonempty, bounded set of optimizers. Moreover, a lower
bound on the cardinality of the quantization set is provided,
along with speciﬁc examples of minimal quantizations. Fur-
thermore, convergence rate results are established that connect
the ﬁneness of the quantization and number of iterations
needed to reach a predeﬁned solution accuracy. The results
provide a bound on the number of bits needed to achieve
the desired accuracy. Finally, an application of the theory to
resource allocation in power networks is demonstrated, and the
theoretical results are substantiated by numerical simulations.

I. INTRODUCTION

Recent advances in distributed control and optimization
techniques have enabled more economical and sustainable
operation of cyber-physical systems. However, the full po-
tential of the technology has not been fully exploited in
many application such as power networks due to inherent
communication constraints. For example, although power
networks are equipped with a natural communication in-
frastructure such as power line communications [1], it is
currently not used for distributed decision making due to
limited bandwidth/Shannon capacity. Instead, research efforts
in distributed operation of power networks usually assume
high data rates and low latency wireless communication
technologies that might be integrated into the networks
sometime in the future. Another component of cyber-physical
systems where economic communication plays a central role
are wireless sensor networks (WSN). WSNs are powered
by battery sources for communication over wireless links;
hence, are constrained in how much transmission they engage
in to prolong battery life and operation time of the sensors.
Motivated by the discussion above, the goal of this paper is to
investigate fundamental communication limits of distributed

This work was supported by the VR Chromos Project and NSF grant No.

1548204.

S. Magn´usson and C. Fischione are with Electrical Engineering School,
Access Linnaeus Center, KTH Royal Institute of Technology, Stockholm,
Sweden. E-mail: sindrim@kth.se.

K. Heal, C. Enyioha, N. Li, and V. Tarokh are with the School of Engi-
neering and Applied Sciences, Harvard University, Cambridge, MA USA.

optimization approaches based on gradient methods and
decomposition.

Decomposition methods in optimization have been widely
investigated in wired/wireless communication [2]–[5], power
networks [6], [7], and WSNs [8], among others. These
methods are typically based on communicating gradient
information from a set of source nodes to users, which
then solve a simple, local subproblem. The procedure can
be performed using a) one-way communication where the
source nodes estimate the gradient using available informa-
tion [3], [9] or b) two-way communication where users and
sources need to coordinate to evaluate the gradient. The main
contribution of this paper is to investigate the performance
of such decomposition methods where bandwidth is limited.
Limited bandwidth in distributed optimization has al-
ready received attention in the literature [10]–[12]. For
example, [10] considers a variant of incremental gradient
methods [13] over networks where each node projects its
iterate to a grid before sending the iterate to the next node.
Similar quantization ideas are explored in [11] in the con-
text of consensus-type subgradient methods [14]. Our work
differs from the aforementioned papers in that we consider
decomposition methods where the gradient is communicated
whereas in [10], [11] it is the decision variables that are
communicated. The work in [12] studies the convergence of
standard interference functions methods for power control
in cellular wireless systems where base stations send binary
signals to the cells. Unlike [12], in this work the gradient
information is quantized and transmitted by a constrained
number of bits.

A. Contributions of This Work

We consider quantized gradient methods (QGM) where at
each iteration the gradient direction is projected to a ﬁnite
quantization set D. We begin by investigating conditions
under which the quantization set D is proper in the sense
that QGMs can minimize any convex function f : RN → R
with Lipschitz continuous gradients and non-empty, bounded
set of minimizers. We provide necessary and sufﬁcient con-
ditions that characterize such proper quantization sets. We
then use this characterization to provide examples of proper
quantization sets D. Further, we show that if |D| ≤ N then
D can not be proper, i.e., for every such D there exists
an optimization problem which QGMs can not solve. In
addition, we show that there exists a proper quantization
with |D| = N + 1. We show that the stopping criteria
||∇f (x)|| <  and f (x) − f (cid:63) <  can be achieved for any
 > 0 in ﬁnite number of iterations. Moreover, we provide a
bound on the number of iterations needed to achieve these

stopping conditions; this bound depends on the ﬁneness of
the quantization set D. Speciﬁcally, the bound on number of
iterations decreases when the quantization set becomes ﬁner.
We also show that, when the step-sizes are non-summable
but square summable, then the iterates of QGMs converge to
the set of optimal values. We show how the theory presented
in this paper can be applied to a resource allocation problem
in electrical power grids. Finally, we numerically illustrate
the performance of the algorithm.

B. Notation

represent represent sets. We let S n={x∈Rn(cid:12)(cid:12)1=||x||} denote

Vectors and matrices are represented by boldface lower
and upper case letters, respectively. The set of real and nat-
ural numbers are denoted by R and N, respectively. The set
of real n vectors and n×m matrices are denoted by Rn and
Rn×m, respectively. Otherwise, we use calligraphy letters to
the unit sphere. The superscript (·)T stands for transpose.
diag(A1, . . ., An) denotes the diagonal block matrix with
A1, . . ., An on the diagonal. || · || denotes the 2-norm.
II. PRELIMINARIES AND APPLICATION EXAMPLE

In this paper we consider optimization problems of the

form

minimize

f (x).

x∈RN

(1)
We denote by f (cid:63) and X (cid:63) the optimal value and the set
of optimizers to Problem (1), respectively. We consider the
following class of functions f:
Deﬁnition 1: Let F denote the set of convex and dif-
ferentiable functions on RN with L-Lipschitz continuous
gradients where X (cid:63) is nonempty and bounded.
For f ∈ F it is well known that the gradient method

x(t+1) = x(t) − γ(t)∇f (x(t)),

(2)
converges to X (cid:63) under appropriate step-size rules [15]. When
only the gradient direction is known, recursion (2) becomes

x(t+1) = x(t) − γ(t)

(3)
(3) converges to X (cid:63) under appropriate diminishing step-size
rules, and for ﬁxed step-size γ(t) = γ the stopping condition
f (x(t)) − f (cid:63) <  can be achieved for all  > 0 [16].

∇f (x(t))
||∇f (x(t))|| .

Problems of the form (1) commonly appear as primal or
dual master problems in distributed optimization methods
such as primal or dual decomposition [4], [5]. In such
methods, the gradient information needs to be communicated
to perform the recursions (2) or (3). We now give an example
of such a distributed procedure.

A. Application Example: Distributed Power Allocation

Consider a network consisting of N resources and M
users. The generation of resource j = 1, . . . N and the usage
of user i = 1, . . . , M are donated by rj ∈ Rj ⊆ R and
qi ∈ Qi ⊆ RN , respectively. The local constraints Rj
and Qj represent generation limits and user preferences,
respectively. The generation of resource j has cost function
Cj, and the usage of user i has utility function Ui. The

operation goal of the network is to ﬁnd the social welfare
by solving the following minimization problem.

M(cid:88)

Ui(qi) − N(cid:88)

Cj(rj)

maximize
(q,r)∈RM N×N

subject to

j=1

for i = 1, . . . , M
for j = 1, . . . , N

(RA)

i=1

qi ∈ Qi,
rj ∈ Rj,

N(cid:88)

qi = r.

i=1

For notational ease, we write q = (q1, . . . , qM ), r =
(r1, . . . , rN ), Q = Q1 × . . .QM , and R = R1 × . . .RN .
The dual problem of (RA) is of the form given in (1) where
the dual function f : RN → R is given by

f (p) = maximize

(q,r)∈Q×RL(q, r, p) = L(q(p), r(p), p).

Here

L(q, r, p) =

M(cid:88)

Ui(qi) − N(cid:88)

Cj(rj) − pT

(cid:32)
r − M(cid:88)

i=1
and for all i = 1, . . . , M and j = 1, . . . , N we have

j=1

i=1

qi(p) =argmax
qi∈Qi
rj(pj) =argmax
rj∈Rj

Ui(qi) − pTqi,
− Ci(ri) + pjrj.

(4)

(cid:33)

qi

,

(5)

(6)

The following result is proved in the appendix.

Lemma 1: Suppose there exists µ > 0 such that Ui and Cj
are µ-strongly concave and µ-strongly convex, respectively,
for all i = 1, . . . , M and j = 1, . . . , n and Qi and Ri are
convex and compact sets. Then, f is continuously differen-
i=1 qi(p)

tiable on RN and the gradient ∇f (p) = r(p)−(cid:80)M
many applications, the gradient ∇f (p) = r(t)−(cid:80)M

is (M + 1)/µ-Lipschitz continuous.
By Lemma 1 the update rules (2) and (3) apply here.
However, the gradient information must be broadcasted to
the users so they can solve their subproblems (5) and (6). In
i=1 qi(t),
i.e., the amount used of each resource at time t, can be
measured at the source. Hence, using only one-way com-
munication is feasible.

III. QUANTIZED GRADIENT DESCENT METHODS

We consider general quantized gradient methods of the

form

x(t+1) = x(t) − γ(t)d(t),

(7)
with d(t) ∈ D ⊆ S N , where D is a ﬁnite set of quantized
gradient directions. Clearly, we have the following relation
between the cardinality of D and communicated bits at each
iteration of (7).
Remark 1: The set D can be coded using log2(|D|) bits.
We investigate what fundamental properties the set D needs
for the recursion (7) to minimize any f ∈ F. To formally
assert the meaning of such a proper quantization we make
the following deﬁnition.

Deﬁnition 2: A set D is a proper quantization if for every
f ∈ F and every initialization x(0) ∈ RN we can choose
d(t) ∈ D and γ(t) ∈ R+, for all t ∈ N, in the recursion (7)
such that

t→∞ dist(x(t),X (cid:63)) = 0.

lim

(8)

In other words, every limit point of x(t) is in X (cid:63).
Deﬁnition 2 is not constructive since its validation requires
testing the dynamics (7) on every function f ∈ F. In
addition, it does not provide an algorithm based on (7); it
does not provide a strategy for choosing d(t) even when D
is known to be a proper quantization. Moreover, Deﬁnition 2
gives no insight
into other interesting properties of the
quantization set D. For example, it gives no information
regarding the minimal size of |D| which ensures a proper
quantization or how the structure of D affects the conver-
gence behavior of potential quantized gradient methods. All
of the points mentioned above are investigated in this paper.
We speciﬁcally provide solutions to the following questions:
A) What are equivalent constructive conditions for the set D
being proper quantization that can be used to determine
whether D is a proper quantization or to construct such
sets?
B) Given a proper quantization D, how can we construct an
algorithm from (7) such that limt→∞ dist(x(t),X (cid:63)) =
0, i.e., choose the proper d(t) ∈ D γ(t) ∈ R+?
C) What are the connections between the ﬁneness of the
quantization, i.e., the size of |D|, to the possible conver-
gence of the algorithm?
D) What is the minimal quantization, i.e., size |D|, for which
D is a proper quantization?
In the following subsections we answer each of the
questions above, but refer to later sections for many of the
technical details.

A. θ-Covers: Solution to Question A)

We now provide conditions that are equivalent to D being
a proper quantization (Deﬁnition 2) but are constructive in
the sense that they can be used to determine if a set D is a
proper quantization or to construct such D. We then use this
condition to provide examples of proper quantization sets D.
Deﬁnition 3: We say that the set D is a θ-cover if θ > 0

and for every g ∈ S N there exists d ∈ D such that

cos(ang(g, d)) ≥ θ.

(9)
We say that the θ-cover D is tight if there exists a vector
g ∈ S R such that maxd∈D cos(ang(g, d)) = θ.
The following result asserts the equivalence between Deﬁni-
tions 2 and 3.
Theorem 1: Consider a quantization set D. D is a proper
quantization (Deﬁnition 2) if and only if there exists θ > 0
such that D is a θ-cover for some θ > 0 (Deﬁnition 3).
Proof: Let us start by showing via a contradiction that
D being a proper quantization implies that there exists θ > 0
such that D is θ-cover. Suppose there does not exists θ > 0
for which D is a θ-cover. Then, since D is ﬁnite, there exits

a ∈ S N such that cos(ang(a, d)) ≤ 0 for all d ∈ D. In
particular, we have for all d ∈ D that

(cid:104)a, d(cid:105) = (cid:107)a(cid:107)(cid:107)d(cid:107) cos(ang(a, d)) ≤ 0.

(10)

By choosing x(0) = a, using Recursion (7) and Cauchy-
Schwarz inequality we conclude that for all t ∈ N

||x(t)|| ≥ (cid:104)a, x(t)(cid:105) =(cid:104)a, a(cid:105) − t−1(cid:88)

γ(t)(cid:104)a, d(t)(cid:105) ≥ 1,

i=0

where the inequality follows from that ||a|| = 1 and that for
all d(t) ∈ D we have (cid:104)a, d(cid:105) ≤ 0. If we choose f (x) = ||x||
then f ∈ F and f has the unique optimizer x(cid:63) = 0, but

dist(x(t),X (cid:63)) = ||x(t)|| ≥ 1,

(11)
for all t ∈ N. Since (11) holds for all d(t) ∈ D and γ(t) ∈
R+, we can conclude that D is not a proper quantization.
The fact that D being a θ-cover implies that D is a proper
quantization, follows from Theorem 6 in Section IV-B, where
we show that for all f ∈ F we can choose d(t) ∈ D and
γ(t) ∈ R+ such that limt→∞ dist(x(t),X (cid:63)) = 0.

We now provide some examples of θ-covers.
Example 1 (Minimal Example: |D1| = N + 1): Set

√
D1 = {e1, . . . , eN ,−1/

N},

(12)

where ei is the i-th element of the normal basis and 1 is
N dimensional vector with 1 in every component. Clearly,
|D| = N + 1 and therefore D can be coded using only
log2(N + 1) bits. We show in Section III-C that this is a
minimal quantization, since in general if |D| ≤ N, then D
cannot be a proper quantization. We show in Lemma 3 (in
the Appendix), that D1 is a θ-cover with

(13)
Example 2 (Example in R2: |D2| = n): For every n ∈ N

N (N − 1)

N 2 + 2

.

1
√

θ =

(cid:113)
(cid:26)(cid:20) cos(2πk/n)

sin(2πk/n)

set

Dn =

(cid:21)

∈ R2

(cid:12)(cid:12)(cid:12)(cid:12)k = 0, 1, . . . , n − 1
(cid:27)

.

Clearly, if n ≥ 3, Dn is a θ-cover with θ = cos(π/n).
Example 3 (± Normal Basis: |D3| = 2N): Let D3 =
{e1,−e1, e2,−e2, . . . , eN ,−eN}. Clearly, |D3| = 2N and
hence log2(2N ) bits are needed to broadcast the quantized
gradient direction. Let us now show that D3 is θ-cover
√
N. Take x ∈ S N , then if we choose d =
with θ = 1/
sign(xi)ei where i = argmaxi=1,...,N|xi| then it holds that
√
cos(ang(x, d)) = (cid:104)x, d(cid:105) = xi · sign(xi) = |xi| ≥ 1/
√
N .
Example 4 (Signs of the gradients: |D4|=2N , θ=1/
√
N):
Let D = {(1/
each d ∈ D represents one orthant of RN . Therefore, this
choice is well suited when the sources cannot cooperate
and each source updates its price based on local estimates
of their part of the gradient, i.e., d(t) = sign(∇f (p(t))).
It can be checked that |D| = 2N hence log2(2N ) = N bits
are needed to broadcast the quantized gradient direction. To

N )(e1, e2, . . . , eN )(cid:12)(cid:12)ei ∈ {−1, 1}}. Here,

√
show that D4 is θ-cover with θ = 1/
Then it holds for d = (1/

√

N )sign(x) that

N, take any x ∈ S N .

N(cid:88)

cos(ang(x, d)) = (cid:104)x, d(cid:105) =

1√
N

x2

i =

N(cid:88)

i=1

xi · sign(xi)

i=1

1√
N

||x|| =

1√
N

.

≥ 1√
N

B. Algorithm: Solution to Questions B) and C)

Unlike Deﬁnition 2, Deﬁnition 3 actually provides us with
tools to construct algorithms for solving Problem (1) when
D is a proper quantization. In particular, for x(t) ∈ RN we
can quantize the gradient ∇f (x(t)) with a d(t) ∈ D such
that cos(ang(∇f (x(t)), d(t))) ≥ θ, as seen in Algorithm 1.
We study the convergence of Algorithm 1 in Section IV. In
particular, we provide a bound on the number of iterations
needed to achieve a speciﬁed accuracy that decreases as θ
becomes closer to 1. Moreover, we show how to choose
the step-sizes so that any limit point of the algorithm is an
optimizer of Problem (1).

Algorithm 1: θ-Quantized Gradient Methods (θ-QGM)
Initialization: Choose x(0)∈RN ;
for t = 0, 1 . . . do

Quantized Gradient: Choose d(t) ∈ D such that

cos(ang(∇f (x(t)), d(t))) ≥ θ

Gradient Step: x(t + 1) = x(t) − γ(t)d(t)

C. Minimal Quantization: Solution to Question D)

In Example 1 we provided a proper quantization D where
|D| = N + 1. We now show that N + 1 is a minimal proper
quantization in the sense that there does not exist a proper
quantization set D with cardinality less than N + 1.
Theorem 2: Suppose that |D| ≤ N. Then D is not a
proper quantization (Deﬁntion 2).
Proof: First consider the case where either |D| < N
or |D| = N and the elements of D are linearly dependent.
Then Span(D) is a proper subspace of RN , so there exists a
normal a ∈ S N such that cos(ang(a, d)) = (cid:104)a, d(cid:105) ≤ 0 for
all d ∈ Span(D). Since D ⊆ Span(D), D is not a θ-cover
for any θ > 0 and the result follows from Theorem 1.
Let us next consider the other case, where |D| = N and
the vectors of D are linearly independent, i.e., Span(D) =
RN . Deﬁne D ∈ RN×N such that for i = 1, . . . , N row
i in D is the i-th elemnt of D, where the elements have
some arbitrary order. Then D is invertable and we can choose
a = D−1(−1) where 1 ∈ RN is a vector of all ones. Then
we have for i = 1, . . . , N that (cid:104)di, a(cid:105) = −diD−11 = −1.
Hence, as in the previous case, we get that (cid:104)a, d(cid:105) ≤ 0 for all
d ∈ D implying that D can not be a θ-cover for any θ > 0,
and the result follows from Theorem 1.

We now study the convergence of the Algorithm 1.

IV. CONVERGENCE

We ﬁrst investigate the convergence Algorithm 1 when the
step-sizes are ﬁxed, i.e., γ(t) = γ, in subsection IV-A. Then
in subsection IV-B we consider diminishing step-size.

A. Constant Step Size

For constant step-size we consider the following two types

of stopping criteria:

Type-1:
Type-2:

||∇f (x)|| < ,
f (x(t)) − f (cid:63) < .

(14)
(15)

We note that when performing primal or dual decomposition
usually only the gradient of f is available but the objective
function is distributed between different users, hence (14)
tends to be a more practical stopping condition.

1) Stopping Condition of Type-1: We start by showing
that the Type-1 stopping criterion can be achieved for any
 > 0 in ﬁnitely many iterations. Further, we provide a bound
on number of iterations that depends on θ outlined next.

Theorem 3: For  > 0 we deﬁne the set

X () = {x ∈ RN(cid:12)(cid:12)||∇f (x)|| ≤ }.

(16)
If f ∈ F, D is a θ-cover, and the sequence (x(t))t∈N is
generated using Algorithm 1, then the following holds:
a) For any  > 0, if γ ∈ (0, 2θ/L) then there exists T ∈ N

such that x(T ) ∈ X (), with T bounded by

(cid:24) 2(f (x(0)) − f (cid:63))

(cid:25)

γ(2θ − Lγ)

T ≤

.

(17)

The upper bound (41) is minimized with the optimal step
size γ(cid:63) = θ/L.

b) For any step size γ > 0 and scalar κ > 0, if we choose

(κ, γ) = κ + γL/(2θ)

(18)
then there exists T ∈ N such that x(T ) ∈ X ((κ, γ)),
with T bounded by
T ≤

(cid:24) f (x(0)) − f (cid:63)

(cid:25)

(19)

.

θγκ

c) (Lower Bound on T ) For any step-size γ > 0 and  > 0

if x(T ) ∈ X () then

||∇f (x(0))|| − 

≤ T

Proof:

(20)
a) Let  > 0 be given and choose any γ ∈
(0, 2θ/L). From Lemma 2 below we have for all x(t) ∈
RN \ X () that

γL

f (x(t + 1)) ≤ f (x(t)) − δ(, γ, θ),

(21)

f (x(s)) ≤ f (x(0)) − s δ(, γ, θ).

where δ(, γ, θ)>0 is deﬁned in (27). By recursively us-
ing (21), it follows that if x(t)∈RN \ X () for all t<s then
(22)
Therefore, there must exist T ≤ (cid:100)(f (x(0))− f (cid:63))/δ(, γ, θ)(cid:101)
such that x(T ) ∈ X (); otherwise, we can use (21) with s =
(cid:100)(f (x(0)) − f (cid:63))/δ(, γ, θ)(cid:101) + 1 to get the contradiction that
f (x(s)) < f (cid:63), which cannot be true since f (cid:63) is the optimal

solution to (1). By rearranging (cid:100)(f (x(0)) − D(cid:63))/δ(, γ, θ)(cid:101),
we get (41). The optimal step-size γ(cid:63) = θ/L comes by
simply maximizing the denominator in (41).

b) This result can be obtained by using similar arguments
as were used to prove part a). The only difference is that
now we have an explicit form for  when using (21), which
results in

δ(, γ, θ) = δ

(23)
c) Using that the gradient ∇f is L-Lipschitz continuous

and the triangle inequality, we have for all t ∈ N that

= θγκ.

, γ, θ

κ +

γL
2θ

(cid:18)

(cid:19)

accuracy  that can be guaranteed in T max iterations. This
idea is formalized in the following Theorem.
Theorem 4: Suppose an upper bound T max ∈ N on
number of iterations is given. Then the minimal bound
(κ, γ) achieved from (18) in T max iterations is

(cid:63) =

L
θ

2(f (x(0))−f (cid:63))

,

LT max

where the corresponding optimal γ and κ are

γ(cid:63)=

2(f (x(0))−f (cid:63))

LT max

and κ(cid:63)=

(cid:112)L(f (x(0))−f (cid:63))

√

θ

2T max

(32)

.

(33)

(cid:114)

(cid:114)

(24)

In other words, equations (32) and (33) give an optimal
solution to the following optimization problem:

||∇f (x(t))|| − Lγ ≤ ||∇f (x(t+1))||.

Recursively applying (24) gives

||∇f (x(0))|| − Lγt ≤ ||∇f (x(t))||.

(25)
Hence, from (25), ||∇f (p(t))|| ≤  can only hold when
t ≥ (||∇f (p(0))|| − )/(Lγ).
Lemma 2: Let f : RN → R be a convex and continuously
differentiable function with L-continuous gradient. Suppose
 > 0, γ ∈ (0, 2θ/L), θ ∈ (0, 1], x ∈ RN , and d ∈ S N
where cos(ang(∇f (x), d))≥θ and ||∇f (x)||>. Then

f (x − γd) ≤ f (x) − δ(, γ, θ),

(26)

(cid:18) L

(cid:19)

where

δ(, γ, θ) = −

γ − θ

(27)
Proof: By using that the gradients of f are L-Lipschitz
continuous, we can apply the descent lemma (see for exam-
ple [17, eq. (2.1.6)] or [15, Proposition A.24]). The descent
lemma states that for all γ we have

γ > 0.

2

f (p − γd)≤f (p) − (cid:104)∇f (p), d(cid:105)γ+

||d||2γ2,

=f (x) +

γ − (cid:104)∇f (x), d(t)(cid:105)

γ,

(cid:18) L
(cid:18) L

2

(cid:19)

L
2

(cid:19)

γ

≤f (x) +
γ − θ
=f (x) − δ(, γ, θ)

2

(28)

(29)

(30)

(31)

||d|| = 1,

where (29) comes from using that
(30)
comes from using cos(ang(∇f (x), d)) ≥ θ, ||∇f (p)|| ≥
 (since x ∈ RN \ X (), and that (cid:104)∇f (x), d(cid:105) =
||∇f (x)|| cos(ang(d,∇f (x))). The fact that δ(, γ, θ) > 0
is a direct consequence of the choice of γ ∈ (0, 2θ/L).
Theorem 3-a) proves that if D is a θ-cover then the Type-
1 stopping condition, (eq. (14)) can be achieved with -
accuracy in ﬁnitely many iterations, for all  > 0. Moreover,
it gives a bound on the number of iterations needed to
achieve such -accuracy depending on θ, where the bound
decreases as θ approaches 1. Theorem 3-b) demonstrates
what -accuracy can be achieved for a given step-size. The
parameter κ captures a trade offs between the -accuracy and
the number of iterations. In fact, by optimizing over both γ
and κ in Theorem 3-b), we ﬁnd an optimal bound on the

minimize

κ,γ

subject to

(κ, γ) = κ + (L/(2θ))γ
f (x(0)) − f (cid:63)
≤ T max,

(34)

θγκ
γ, κ > 0.

Proof: First note that Problem (34) is convex, which

can be seen by equivalently writing (34) as

minimize

κ,γ

subject to

κ + (L/(2θ))γ

(cid:18) f (x(0)) − f (cid:63)

(cid:19) 1

θT max

γ

γ, κ > 0,

− κ ≤ 0,

(35)

and recalling that the reciprocal 1/γ is a convex function
for γ > 0. Therefore it sufﬁces to show that the KKT
conditions [18, Section 5.5] are satisﬁed by the solution
of (33). It can be checked that κ(cid:63) and γ(cid:63) satisfy the KKT
condition for (35) with the Lagrangian multiplier λ(cid:63)=1.

We next demonstrate how the convergence results translate

to Type-2 stopping conditions (eq. (15)).

result.

2) Stopping Condition of Type-2: We have the following
Theorem 5: Suppose f ∈ F, D is a θ-cover, and the
iterates x(t) come from Algorithm 1, then following holds:
a) For any  > 0, γ ∈ (0, 2θ/L) and T ∈ N such that

x(T ) ∈ X () it holds, for all t ≥ T , that

(cid:18)

(cid:19)

f (x(t)) ≤ F () +

L
γ
2
where F : R+ → R ∪ {∞} is given by

 +

γ,

(36)

F (κ) = sup{f (x)|x ∈ X (κ)}.

(37)
There exists ¯κ > 0 such that F (κ) < ∞ for all κ < ¯κ
and limκ→0+ F (κ) = f (cid:63).

Proof:

b) If f is µ-strongly convex then we have
F () ≤ f (cid:63) + 2/(2µ).

(38)
a) We start by proving the bound (36) by
mathematical induction. First note that (36) holds for T ,
because of the deﬁnition of F . Now suppose (36) holds
for some t ≥ T . Then if x(t) ∈ RN \ X (), we have that
f (x(t+1)) ≤ f (x(t)) due to Lemma (2), and hence (36)

holds. On the other hand, if x(t) ∈ X () then from [17,
eq. (2.1.6)] we have that
f (x(t+1)) ≤f (x(t)) + (cid:104)∇f (x(t)),−γd(t)(cid:105) +

(39)

γ2

(cid:18)

(cid:19)

≤F () +

 +

L
2

γ

γ,

L
2

(40)

where second inequality comes by the deﬁnition of F and
using the Cauchy-Schwarz inequality together with the fact
that ||∇f (x(t))|| ≤ . Therefore, we can conclude that (36)
holds for all t ≥ T .
Let us next show that there exists ¯κ > 0 such that F (κ) <
∞ for all κ < ¯κ. From Lemma 4, we know that there exists
¯κ > 0 such that X (κ) is bounded for all κ < ¯κ. Hence,
X (κ) is also closed for κ < ¯κ since ∇f is continuous and
hence compact. Therefore, since f is continuous it achieves
the optimal value in X () for κ < ¯κ.

Finally, we show that limκ→0+ F (κ) = f (cid:63). In particular,
we show that F is continuous at 0 which implies the
result, since F (0) = f (cid:63). Take any sequence (κk)k∈N in
R+ such that limk→∞ κk = 0. Then there exists K ∈ N
and a sequence (x(k))k∈N such that f (x(k)) = F (κk)
holds for all k ≥ K, since X (κ) is compact for all
κ ∈ [0, ¯κ). Moreover, by the deﬁnition of X () we have
that limk→∞ ||∇f (x(k))|| = 0. Now since ||∇f (x)|| is a
continuous function we can conclude that for every limit
point ¯x of (x(k))k∈N it holds that ||∇f (¯x)|| = 0, i.e.,
¯x ∈ X (cid:63) or f (¯x) = f (cid:63). Since f (¯x) = f (cid:63) holds for every limit
point of (x(k))k∈N and f is continuous we can conclude that
limk→∞ f (x(k)) = limk→∞ F (κk) = f (cid:63).

b) For any x(t) ∈ X () we have that

f (x(t)) ≤ f (cid:63) +

1
2µ

||∇f (x(t))||2 ≤ f (cid:63) +

2
2µ

,

where the ﬁrst inequality comes from using that f is µ-
strongly convex and [17, eq. (2.1.19) in Theorem 2.1.10]
together with that ∇f (x(cid:63)) = 0 for all x(cid:63) ∈ X (cid:63).
Theorem 5 yields the following immediate corollary.
Corollary 1: For any  > 0 there exists step-size γ > 0
and T ∈ N such that f (x(T )) − f (cid:63) < . Moreover, if f
is µ-strongly convex and 1, γ > 0 are chosen such that
γ ∈ (0, 2θ1) and 1/(2µ) + (1 + Lγ/2)γ <  then

(cid:24) 2(f (x(0)) − f (cid:63))

(cid:25)

.

γ(2θ1 − Lγ)

(41)
These results prove that the Type-2 stopping condition
(eq. (15)) can be achieved in ﬁnitely many iterations. More-
over, when f is strongly concave the results provide a bound
on the number of iterations..
B. Diminishing Step Size

T ≤

We now consider the diminishing step-size case.
Theorem 6: Suppose that f ∈ F, X (cid:63) is bounded, D is
a θ-cover, and the sequence (x(t))t∈N is generated using
Algorithm 1. If the step-size γ(t) is non-summable and
square summable, i.e.,

γ(t) = ∞ and

γ(t)2 < ∞,

(42)

t=0

t=0

N(cid:88)

N(cid:88)

(cid:115)

(cid:19)

(cid:18) L

2

then limt→∞ dist(x(t),X (cid:63)) = 0.
Proof: We start by showing by contradiction that
lim inf t→∞ ||∇f (x(t))|| = 0. Suppose the contrary; i.e.,
there exists an  > 0 and T0 ∈ N such that x(t) ∈ RN \X ()
for all t ≥ T0. Since limt→∞ γ(t) = 0 we can choose T1
such that γ(t) < 2θ/L for t ≥ T1. Then, by Lemma 2,

γ(t)−θ

f (x(t+1)) ≤ f (x(t))+γ(t)

(43)
for all t ≥ T2 = max{T0, T1}. By recursively using (43) we
have for all t ≥ T2 that
f (x(t+1))≤f (x(T2))+

γ(τ )2−θ

t(cid:88)

t(cid:88)

γ(τ ).

(44)

,

L
2

τ =T2

τ =T2

Since γ(t) is non-summable but square-summable (cf. (42)),
the right hand side of (44) diverges to −∞ implying that
limt→∞ f (x(t)) = −∞. That contradicts the fact
that
f (x) ≥ f (cid:63) > −∞ for all x ∈ RN . Therefore, we can
conclude that lim inf t→∞ ||∇f (x(t))|| = 0.
Let us next show that limt→∞ f (x(t)) = f (cid:63), i.e., for any
 > 0 there exists T ∈ N such that f (x(t)) − f (cid:63) <  for all
t ≥ T . Choose κ such that following holds

κ <

1
2

L

(1 + θ)θ

and

F (κ) < f (cid:63) +


2

,

(45)

where F is deﬁned in (37), such κ > 0 exists by Theorem 5-
a). Now, since lim inf t→∞ ||∇f (x(t))|| = 0, there exists
inﬁnitely many T ∈ N such that x(T ) ∈ X (κ). Therefore,
we can choose T ∈ N such that x(T ) ∈ X (κ) and γ(t) <
2θκ/L for all t ≥ T , since limt→∞ γ(t) = 0. We can now
follow similar steps as in the derivation of (36) in Theorem 5-
a) to show that for all t ≥ T we have that
f (x(t)) ≤F (κ)+ (κ + θκ)

= F (κ) + 2(1 + θ)θκ2/L

2θκ
L

= f (cid:63) + ,

(46)

≤f (cid:63) +


2

+


2

where (46) comes by using (45). Hence we can conclude
that limt→∞ f (x(t)) = f (cid:63).
We now show that limt→∞ ||∇f (x(t))|| = 0. Take  >
0, and choose T such that f (x(t))−f (cid:63) < 2/(2L), since
limt→∞ f (x(t)) = f (cid:63). Moreover, since ∇f is L-Lipschitz
continuous we have from [17, Theorem 2.1.5, (2.1.7)] that
for all x ∈ RN

f (cid:63) +

1
2L

||∇f (x)||2 ≤ f (x).

(47)

t ≥ T that ||∇f (x(t))|| ≤(cid:112)2L(f (x(t)) − f (cid:63)) < .

By inserting x(t) into (47) and rearranging, we get for all
Finally, we show that limt→∞ dist(x(t),X (cid:63)) = 0 using
contradiction. Suppose there is a subsequence (x(tk))k∈N
such that dist(x(tk),X (cid:63)) > δ for all k ≥ K for
some δ > 0 and K ∈ N. From Lemma 4 we know
there exists ¯κ > 0 such that X (κ) is bounded for all
κ < ¯κ. Hence, since limt→∞ ||∇f (x(t))|| = 0, the sub-
sequence (x(tk))k∈N is bounded and has a limit point
¯x ∈ RN . Since f is continuous and limt→∞ f (x(t)) =

f (cid:63), f (¯x) = f (cid:63). Hence, ¯x ∈ X (cid:63), which contradicts that
dist(x(tk),X (cid:63)) > δ for all k ≥ K. Therefore, we can
conclude that limt→∞ dist(x(t),X (cid:63))=0.
Theorem 6 shows that when D is a θ-cover then there
exists a step-size rule such that every limit point of the
quantized gradient methods is an optimal solution to (1). We
next numerically illustrate the quantized gradient methods.

V. SIMULATION RESULTS

We illustrate the performance of the quantized gradi-
ent methods on an instance of (RA) from Section II-
A with M =4 users and N =2 resources. For i=1, 2, 3, 4
and j = 1, 2, we set Cj(rj) = −cjr2
j , Ui(qi) =
ai1 log(0.1 + qi1) + ai2 log(0.1 + qi2), Rj = [0, 10] and
Qi = {(x, y) ∈ R2|x, y ≥ 0, x + y ≤ 3}. Clearly,
Cj and Ui are strongly concave on their domains with
concavity parameters cj and µi = min{ai1, ai2}/3.12 =
min{ai1, ai2}/9.61, respectively. By Lemma 1,
the dual
gradient is L-Lipschitz continuous with L = 5/µ, where
µ = min{c1, c2, µ1, . . . , µ4}. We choose c1 = c2 = 1, and
a1, a2, a3, a4 = (50, 14), (20, 11), (40, 12), (35, 10).

Figure 1 depicts the results when the step-size is γ = 0.1
and the initialization is x(0) = (0, 0) (recall that x is the
dual variable here). We use the quantization set D from
Example 2, which corresponds to the case where 2, 3, and
4 bits are communicated per iteration, i.e., |D| = 4, 8, 16,
see Remark 1. The norm of the gradient ||∇f|| reaches the
accuracy  = 0.1 in roughly 140, 180, and 240 using 560,
540, 480 bits when 2, 3, and 4 bits are communicated per it-
eration, respectively. We compare the results to recursions (2)
and (3) where no quantization is done, i.e., inﬁnite bandwidth
is used. Figure 1a shows that by using 4 bits per iteration,
the results achieved by QGM are almost as good as when the
full gradient direction is communicated in (3). However, the
QGMs do not perform as well as (2); this is to be expected,
since in (2) the full direction and magnitude of the gradient is
known. Our results illustrate that we can dramatically reduce
the number of bits communicated without sacriﬁcing much
in performance.

VI. CONCLUSIONS AND FUTURE WORK

In this paper quantized gradient methods for distributed
optimization were investigated. Necessary and sufﬁcient con-
ditions were provided that ensure that the quantized methods
can minimize any function with Lipschitz continuous gradi-
ent. These conditions were used to provide a lower bound
on the number of bits needed to quantizes the gradients.
Moreover,
the results demonstrated how the number of
iterations needed to achieve the desired solution accuracy
are related to the ﬁneness of the quantization. These results
can be used to provide a bound on the number of bits needed
to solve an optimization problem up to a desired accuracy.
Future research directions will consider problems with non-
differentiable objective functions and constraints. Moreover,
for given θ it is desirable to ﬁnd the optimal quantization, i.e.,
the one that can be coded using the fewest number of bits.

APPENDIX

ferentiable with gradient ∇f (p) = r(p)−(cid:80)M

Proof: (of Lemma 1) The fact that f is continuously dif-
i=1 q(p) under
the given assumptions follows directly from [15, Proposition
6.1.1], since L(q, s, p) is strictly concave for ﬁxed p. By
following similar arguments as in the proof of Lemma II.2
in [19] we also have that ∇f (p) is L-Lipschitz continuous
with L = ρ(ATA)/µ, where A = diag(aT, . . . , aT) ∈
Rn×n(M +1) and aT = [1, . . . , 1,−1] ∈ RM +1. We have that
ATA = diag(aaT, . . . , aaT), where aaT is a rank one matrix
with only one nonzero eigenvalue equal to aTa = M + 1.
Therefore, L = ρ(ATA) = M + 1.
Lemma 3: Recall D1 deﬁned in eq. (12) in Example 1 of
Section III-A. D1 is a θ-cover with the θ deﬁned in eq. (13).
Proof: We show that for θ as in equation (13) it
holds that for any x ∈ S N there exists d ∈ D1 such that
equation (3) holds.
First consider the case where xj ≥ θ for some component
j. Then if we choose ej ∈ D1 we get cos(ang(x, ej)) =
(cid:104)x, ej(cid:105) = xj ≥ θ. Therefore, we ﬁnalize the proof by
showing that if x ∈ S N and xi ≤ θ for i = 1, . . . , N then

(cid:18)

(cid:18)

cos

ang

x,− 1√
N
loss of generality,

1

(cid:19)(cid:19)

N(cid:88)

i=1

−1√

N

=

xi ≥ θ,

 (48)

x2
i

(49)

Without
the components of x be
ordered so that xi ≥ 0 if i = 1, . . . , K and xi < 0 if
i = K + 1, . . . , N, where K is the number of positive
components of x. Then it holds that

let

i=1

(cid:118)(cid:117)(cid:117)(cid:116)1 − K(cid:88)
(cid:17)
(cid:118)(cid:117)(cid:117)(cid:116)1 − K(cid:88)

i=1 x2

,

1 − Kθ2

inequality between the 1 and 2 norm, i.e.,

i=1

N

i=1

xi −

|xi| ≥

N(cid:88)

N(cid:88)

xi ≥ − 1√
N
= − 1√
N

 K(cid:88)
−1√
(cid:16)
Kθ −(cid:112)
where (48) comes by using that (cid:80)N
(cid:118)(cid:117)(cid:117)(cid:116) N(cid:88)
xi≥− K−(cid:113)
(cid:113)
≥− N−1−(cid:113)
(cid:113)
(cid:113)

N(cid:88)

N 2 + 2

N 2 + 2

x2
i =

N
√

i=K+1

i=K+1

√

=

√

√

√

N

i=1

N

N

N

N 2 + 2

N (N−1)−K
√
N (N − 1)
√
N (N−1)−(N−1)
N 2+2
√

N (N − 1)

and that xi < 0, for i > K, and (49) comes by noting
that (48) is decreasing and that xi ≤ θ for all i. Now, by
inserting our choice of θ from (13) in (49) we get
−1√

√
N 2+2

(50)

(51)

= θ

(52)

i = 1 and the

x2
i ,

i=1

N (N − 1)
(50)

where (51) comes from that
is decreasing in K
and K ≤ N − 1, and (52) comes by using that
√
N 2+2

N (N−1)−(N−1) = ((N − 1) +

N )2.

√

(a) Gradient

(b) Dual Objective Function Value

(c) Primal Objective Function Value

Fig. 1: Gradient and objective function value over the course of the algorithm.

Lemma 4: Suppose X (cid:63) is bounded. Then there exists κ >
Proof: Take any x(cid:63) ∈ X (cid:63) and choose R > 0 so that

0 such that X () is bounded for all  < κ.
X (cid:63) ⊆ B(x(cid:63), R). Then we choose κ1 > 0 given by

1
L

||∇f (x)||2.

κ1 =

minimize
x∈S(x(cid:63),R)

(53)
Note that such a κ1 exists since S(x(cid:63), R) is compact and
κ > 0 since S(x(cid:63), R) ∩ X (cid:63) is empty. Moreover, using [17,
(2.1.8) in Theorem 2.1.5], that ∇f (x(cid:63)) = 0, and (53), we
have for all x ∈ S(x(cid:63), R) that

(cid:104)∇f (x), x − x(cid:63)(cid:105) ≥ (1/L)||∇f (x)||2 ≥ κ1.

(54)
We now show that for all x ∈ RN \ B(x(cid:63), R) we
have ||∇f (x)|| ≥ κ, where κ = κ1/R. Take some x ∈
RN \ B(x(cid:63), R) and let ¯x denote the unique point in the
intersection of the line segment [x(cid:63), x] and S(x(cid:63), R), such
an ¯x exists because the gradient is continuous, x ∈ RN \
B(x(cid:63), R), and x(cid:63) ∈ B(x(cid:63), R). Consider now the function
G : [0,∞) → RN with G(α) = ∇f (x(cid:63) + α(¯x − x(cid:63))).
Clearly, G(0) = ∇f (x(cid:63)) = 0, G(1) = ∇f (¯x) and there
exists ˆα ≥ 1 such that G(ˆα) = ∇f (x). By using that
gradients of convex functions are monotone, i.e., for all
x1, x2 ∈ RN it holds that (cid:104)∇f (x1)−∇f (x2), x1− x2(cid:105) ≥ 0,
we conclude that for α, β ∈ R+ with α ≥ β it holds that
(cid:104)G(α) − G(β), (α − β)(¯x − x(cid:63))(cid:105) ≥ 0. Rearranging this,

(cid:104)G(α), (¯x − x(cid:63))(cid:105) ≥ (cid:104)G(β), (¯x − x(cid:63))(cid:105).

By combining (54) and (55) we get that

(cid:104)G(ˆα), (¯x − x(cid:63))(cid:105) ≥ (cid:104)G(1), (¯x − x(cid:63))(cid:105) ≥ κ1.

(55)

(56)

Hence, by the Cauchy-Schwarz
inequality we have
||∇f (x)||R = ||G(ˆα)||R ≥ κ1 and by rearranging we get
||∇f (x)|| ≥ κ1/R = κ. Since ||∇f (x)|| ≥ κ holds for
all x ∈ RN \ B(x(cid:63), R) we can conclude that that X () is
bounded for  < κ.

REFERENCES

[1] H. C. Ferreira, H. M. Grov´e, O. Hooijen, and A. Han Vinck, Power

line communication. Wiley Online Library, 2001.

[2] F. P. Kelly, A. K. Maulloo, and D. K. H. Tan, “Rate control
for communication networks: Shadow prices, proportional fairness
the Operational Research Society,
and stability,” The Journal of
vol. 49, no. 3, pp. pp. 237–252, 1998.
[Online]. Available:
http://www.jstor.org/stable/3010473

[3] S. Low and D. Lapsley, “Optimization ﬂow control. I. basic algorithm
and convergence,” Networking, IEEE/ACM Transactions on, vol. 7,
no. 6, pp. 861–874, Dec 1999.

[4] D. Palomar and M. Chiang, “A tutorial on decomposition methods
for network utility maximization,” Selected Areas in Communications,
IEEE Journal on, vol. 24, no. 8, pp. 1439–1451, Aug 2006.

[5] ——, “Alternative distributed algorithms for network utility max-
imization: Framework and applications,” Automatic Control, IEEE
Transactions on, vol. 52, no. 12, pp. 2254–2269, Dec 2007.

[6] N. Li, L. Chen, and S. H. Low, “Optimal demand response based on
utility maximization in power networks,” in Power and Energy Society
General Meeting, 2011 IEEE, 2011, pp. 1–8.

[7] L. Chen, N. Li, S. H. Low, and J. C. Doyle, “Two market models for
demand response in power networks,” IEEE SmartGridComm, vol. 10,
pp. 397–402, 2010.

[8] R. Madan and S. Lall, “Distributed algorithms for maximum lifetime
routing in wireless sensor networks,” Wireless Communications, IEEE
Transactions on, vol. 5, no. 8, pp. 2185–2193, Aug 2006.

[9] C. Zhao, U. Topcu, N. Li, and S. Low, “Design and stability of load-
side primary frequency control in power systems,” Automatic Control,
IEEE Transactions on, vol. 59, no. 5, pp. 1177–1189, May 2014.

[10] M. Rabbat and R. Nowak, “Quantized incremental algorithms for
distributed optimization,” Selected Areas in Communications, IEEE
Journal on, vol. 23, no. 4, pp. 798–808, April 2005.

[11] A. Nedic, A. Olshevsky, A. Ozdaglar, and J. Tsitsiklis, “Distributed
subgradient methods and quantization effects,” in Decision and Con-
trol, 2008. CDC 2008. 47th IEEE Conference on, Dec 2008, pp. 4177–
4184.

[12] J. Herdtner and E. Chong, “Analysis of a class of distributed asyn-
chronous power control algorithms for cellular wireless systems,”
Selected Areas in Communications, IEEE Journal on, vol. 18, no. 3,
pp. 436–446, March 2000.

[13] A. Nedic and D. P. Bertsekas, “Incremental subgradient methods
for nondifferentiable optimization,” SIAM Journal on Optimization,
vol. 12, no. 1, pp. 109–138, 2001.

[14] A. Nedic and A. Ozdaglar, “Distributed subgradient methods for multi-
agent optimization,” Automatic Control, IEEE Transactions on, vol. 54,
no. 1, pp. 48–61, Jan 2009.

[15] D. P. Bertsekas, Nonlinear Programming: 2nd Edition.

Athena

[16] N. Z. Shor, Minimization Methods for Non-Differentiable Functions.

Scientiﬁc, 1999.

Springer, 1985.

2004.

[17] Y. Nesterov, Introductory Lectures on Convex Optimization. Springer,

[18] S. Boyd and L. Vandenberghe, Convex Optimization. New York, NY,

USA: Cambridge University Press, 2004.

[19] A. Beck, A. Nedic, A. Ozdaglar, and M. Teboulle, “An o(1/k) gradient
method for network resource allocation problems,” Control of Network
Systems, IEEE Transactions on, vol. 1, no. 1, pp. 64–73, March 2014.

