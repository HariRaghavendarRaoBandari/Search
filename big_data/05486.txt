A ﬂexible state space model for learning

nonlinear dynamical systems

Andreas Svensson (andreas.svensson@it.uu.se) and Thomas B. Schön (thomas.schon@it.uu.se)

Department of Information Technology, Uppsala University, Sweden

March 18, 2016

6
1
0
2

 
r
a

 

M
7
1

 
 
]

O
C

.
t
a
t
s
[
 
 

1
v
6
8
4
5
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

We consider a nonlinear state space model with the state tran-
sition and observation functions expressed as basis function
expansions. We learn the coefﬁcients in the basis function ex-
pansions from data, and with a connection to Gaussian pro-
cesses we also develop priors on them for tuning the model
ﬂexibility and to prevent overﬁtting to data, akin to a Gaus-
sian process state space model. The priors can alternatively
be seen as a regularization, and helps the model in general-
izing the data without sacriﬁcing the richness offered by the
basis function expansion. To learn the coefﬁcients and other
unknown parameters efﬁciently, we tailor an algorithm for this
model using state-of-the-art sequential Monte Carlo methods,
which comes with theoretical guarantees on the learning. Our
approach indicates promising results when evaluated on a clas-
sical benchmark as well as real data.

1

Introduction

Nonlinear system identiﬁcation (Ljung, 1999, 2010; Sjöberg
et al., 1995) aims to learn nonlinear mathematical models from
data generated by a dynamical system. We will tackle the
problem of learning nonlinear state space models with only
weak assumptions on the nonlinear functions, and make use
of the Bayesian framework (Peterka, 1981) to encode prior
knowledge and assumptions to guide the otherwise too ﬂex-
ible model.

xt+1 = f (xt, ut) + vt,
yt = g(xt, ut) + et,

Consider the (time invariant) state space model
vt ∼ N (0, Q),
et ∼ N (0, R).

(1a)
(1b)
We denote the variables as follows: state xt ∈ Rnx, which is
not observed explicitly, input ut ∈ Rnu, and output yt ∈ Rny.
We will learn the state transition function f : Rnx × Rnu (cid:55)→
Rnx and the observation function g : Rnx × Rnu (cid:55)→ Rny, as
well as Q and R, from a set of training data of input-output
signals {u1:T , y1:T} (or, as a special case, only output data if
no input is measured).

A look-up table containing all training data is an extremely
ﬂexible black-box model. Such a model, however, is of very

1

limited use, as it does not provide any help in generalizing the
behavior to events not exactly seen in the training data. A sys-
tematic way to encode prior assumptions and thereby tuning
the ﬂexibility of the model can therefore be useful. A com-
monly used prior assumption is a linear model (Ljung, 1999),
and another example is to assume that f (·) belongs to the space
of third order polynomials (Paduart et al., 2010).

We will take inspiration from Gaussian processes (GPs,
Rasmussen and Williams 2006) as a way to encode prior as-
sumptions on f (·) and g(·). As illustrated by Figure 1, the
GP is a distribution over functions which gives a probabilis-
tic model for inter- and extrapolating from observed data.
GPs have successfully been used in system identiﬁcation for,
e.g., response estimation, nonlinear ARX models and GP state
space models (Pillonetto and De Nicolao, 2010; Kocijan et al.,
2005; Frigola, 2015).

We parameterize f (·) as a basis function expansion

m(cid:88)

f (x) =

w(j)φ(j)(x),

(2)

j=0

and similarly for g(·). By {φ(j)(·)}m
j=0 we denote a set of ba-
sis functions, whose weights {w(j)}m
j=0 will be learned from
data. By introducing certain priors p(w(j)) on the basis func-
tion weights the connection to GPs will be made (Solin and
Särkkä, 2014). We will thus be able to understand our model
in terms of the well-established and intuitively appealing GP
model, but still beneﬁt from the computational advantages of
the linear-in-the-parameter structure of (4), as well as extend
our model beyond the GP. Intuitively, the idea of the priors
p(w(j)) is to keep w(j) ‘small unless data convinces other-
wise’, or equivalently, introduce a regularization of w(j).

We will also tailor a learning method for the the state space
model (1) with a basis function expansion (2) and its priors, by
applying recent sequential Monte Carlo/particle ﬁlter methods
(Schön et al., 2015; Kantas et al., 2015) to infer the posterior
distribution of all parameters to learn. These methods come
with theoretical guarantees, and we will pay extra attention to
the problem of ﬁnding the maximum mode of the posterior,
i.e., regularized maximum likelihood estimation. A key in ob-
taining an efﬁcient learning algorithm is to utilize that (2) is
linearly parametrized in the weights w(j).

number of data points, which makes the learning hard (Frigola
et al., 2013) and calls for approximations (Frigola et al., 2014;
Mattos et al., 2016). In Svensson et al. (2016) we apply the
approximation method proposed by Solin and Särkkä (2014)
to the GP state space model, and we continue that work in this
paper by extending the model beyond GPs and including not
only the fully Bayesian learning.

To the best of our knowledge, the ﬁrst extensive paper on
the use of a basis function expansion inside a state space mod-
els was written by Ghahramani and Roweis (1998), who also
wrote a longer unpublished version (Roweis and Ghahramani,
2000). The recent work Tobar et al. (2015) resembles that of
Ghahramani and Roweis (1998) on the modeling side, as they
both use functions with locally concentrated mass spread in
the state space, referred to as reproducing kernels by Tobar
et al. (2015) On the learning side, Ghahramani and Roweis
(1998) use an expectation maximization (EM, Dempster et al.
1977) procedure with extended Kalman ﬁltering, whilst Tobar
et al. (2015) use particle marginal Metropolis-Hastings (An-
drieu et al., 2010). There are basically three major differences
between Tobar et al. (2015) and our work. We will (i) use
another (related) learning method, particle Gibbs, allowing us
to take advantage of the linear-in-parameter structure of the
model to increase the efﬁciency. Further, we will (ii) mainly
focus on a different set of basis functions (although our learn-
ing procedure will be applicable also to the model by Tobar
et al. (2015)), and – perhaps most important – (iii) pursue
systematic encoding of prior assumptions further than Tobar
et al. (2015), who instead assume g(·) is known and use ‘stan-
dard sparsiﬁcation criteria from kernel adaptive ﬁltering’ as an
heuristic approach to regularization.

There are connections to Paduart et al. (2010), who use a
polynomial basis inside a state space model. In contrast to our
work, however, Paduart et al. (2010) prevents the model from
overﬁtting to the training data not by regularization, but man-
ually choosing a low enough polynomial order and abort the
learning procedure prematurely (early stopping). The learning
is, in contrast to this paper, focused on the frequency properties
of the model and relying on optimization tools. An interesting
contribution by Paduart et al. (2010) is to ﬁrst use classical
methods to ﬁnd a linear model, which is then used to initialize
the linear term in the polynomial expansion. We suggest to
use this idea, either to initialize the learning algorithm, or, in
the case of a close-to-linear behavior, use a nonlinear model
only to describe deviations from the linear model. Further-
more, there are also strong connections to our previous work
(Svensson et al., 2015), a short paper only outlining the idea of
learning a regularized basis function expansion inside a state
space model. Compared to Svensson et al. (2015), this work
contains several extensions and new results.

A recent work using a regularized basis function expansion
for nonlinear system identiﬁcation is that of Delgado et al.
(2015), however not in the state space model framework. Del-
gado et al. (2015) use a rank constrained optimization, resem-
bling a L0-regularization. To achieve a good performance with
such a regularization, the system which generated the data

Figure 1: The Gaussian process as a modeling tool for an one-
dimensional function f : R (cid:55)→ R. The prior (upper left plot) is
shown in terms of its mean and 2 standard deviations, as well as 10
samples drawn from it. By combining the prior and the data (upper
right plot), the posterior (lower plot) is obtained. The posterior mean
basically interpolates between the data points, and adhere to the prior
in regions where the data is not providing any information. This is
clearly a desirable property when it comes to generalizing from the
training data; consider the thought experiment of using a 2nd order
polynomial instead. Further, the posterior also provides a quantiﬁca-
tion of the uncertainty present, here plotted as 2 standard deviations,
high in data-scarce regions and low where the data provides knowl-
edge about f (·).

The contribution of this paper is the development of a ﬂex-
ible nonlinear state space model with a tailored learning al-
gorithm, which together constitutes a new nonlinear system
identiﬁcation tool. The model can either be understood as a
generalization of an (approximate) GP state space model, or
alternatively as a nonlinear state space model with a regular-
ized basis function expansion.

2 Related work

Important work using GPs insystem identiﬁcation includes im-
pulse response estimation (Pillonetto and De Nicolao, 2010;
Pillonetto et al., 2011; Chen et al., 2012), nonlinear ARX mod-
els (Kocijan et al., 2005; Bijl et al., 2016), Bayesian learning
of ODEs (Calderhead et al., 2008; Wang and Barber, 2014;
Macdonald et al., 2015) and the latent force model (Alvarez
et al., 2009). In the GP state space model (Frigola, 2015) the
transition function f (·) in a state space model is learned using
a GP prior, which is particularly close to this paper. However,
the computational load of a full GP grows cubicly with the

2

−2−1012−2024xf(x)Data−2−1012−2024xf(x)PosteriorPosterioruncertaintyoff(x)Posteriormeanoff(x)Samplesfromposterior−2−1012−2024xf(x)PriorPrioruncertaintyoff(x)Priormeanoff(x)Samplesfrompriorhas to be well described by only a few number of the basis
functions being ‘active’, i.e., have non-zero weights, which
makes the choice of basis functions important and problem-
dependent. The recent work by Mattsson et al. (2016) is also
covering learning of a regularized basis function expansion,
however for input-output type of models.

3 Constructing the model
We want the model, whose parameters we will learn from data,
to be able to describe a broad class of nonlinear dynamical be-
haviors without overﬁtting to training data. To achieve this,
important building blocks will be the basis function expan-
sion (2) and a GP-inspired prior. The order nx of the state
space model (1) is assumed known or set by the user, and we
have to learn the transition and observation functions f (·) and
g(·) from data, as well as the noise covariance matrices Q and
R. We will for brevity focus on f (·) and Q, but the reasoning
extends analogously to g(·) and R.

3.1 Basis function expansion
The common approaches in the literature on black-box mod-
eling of functions inside state space models can be divided
into three groups: neural networks (Bishop, 2006; Narendra
and Li, 1996), basis function expansions (Sjöberg et al., 1995;
Ghahramani and Roweis, 1998; Paduart et al., 2010; Tobar
et al., 2015) and GPs (Rasmussen and Williams, 2006; Frigola,
2015). We will make use of basis function expansions inspired
by the GP. There are reasons for this: Firstly, a basis function
expansion provides an expression which is linear in its parame-
ters, leading to a computational advantage: neural networks do
not exhibit this property, and the naïve use of the nonparamet-
ric GP is computationally very expensive. Secondly, GPs and
some choices of basis functions allows for a straightforward
way of including prior assumptions on f (·) and help general-
izing the training data, also in contrast to the neural network.
We write the combination of the state space model (1) and

the basis function expansion (2) as

1

...
w(1)
nx

w(1)
(cid:124)
 w(1)
(cid:124)

...
w(1)
g,nx

g,1


(cid:125)


(cid:125)

φ(m)(xt, ut)

 φ(1)(xt, ut)
...
(cid:123)(cid:122)
(cid:124)
 φ(1)

(cid:124)
(cid:125)

φ(m)
g

¯ϕ(xt,ut)

...
(cid:123)(cid:122)
(xt, ut)

g (xt, ut)

¯ϕg(xt,ut)


(cid:125)

1

··· w(m)
...
··· w(m)

nx

(cid:123)(cid:122)

A

g,1

··· w(m)
...
··· w(m)

g,nx

(cid:123)(cid:122)

C

xt+1 =

yt =

+vt,

(3a)

+et.

(3b)

There are several alternatives for the basis functions, for in-
stance polynomials (Paduart et al., 2010),
the Fourier ba-
sis (Svensson et al., 2015), wavelets (Sjöberg et al., 1995),
Gaussian kernels (Ghahramani and Roweis, 1998; Tobar

3

et al., 2015) and piecewise constant functions. For the one-
dimensional problem (e.g., nx = 1, nu = 0) on the interval
[−L, L] ∈ R, we will choose the basis functions as

(cid:18) πj(x + L)

(cid:19)

2L

φ(j)(x) =

1√
L

sin

.

(4)

This choice, which is the eigenfunctions to the Laplace op-
erator, enables a particular connection to the GP framework
(Solin and Särkkä, 2014) together with the the priors we will
introduce in Section 3.2.1. This choice is, however, important
only for the interpretability of the model. The learning algo-
rithm we will devise will be applicable to any choice of basis
functions.

3.1.1 Higher state space dimensions
The generalization to models with a state space and input di-
mension such that nx + nu > 1 offers no conceptual chal-
lenges, but potentially computational ones. The counterpart
to the basis function (4) for the space [−L1, L1] × ··· ×
[−Lnx+nu , Lnx+nu ] ∈ Rnx+nu is

nx+nu(cid:89)

k=1

1√
Lk

sin

(cid:18) πjk(xk +Lk)

(cid:19)

2Lk

, (5)

φ(j1,...,jnx+nu )(x) =

which implies that the number of terms m in the basis function
expansion grows exponentially with nx + nu. This problem is
inherent for most basis function expansions in higher dimen-
sions, and is not a result of our particular choice. For nx > 1,
the problem of learning f : Rnx+nu (cid:55)→ Rnx can be under-
stood as learning nx number of functions fi : Rnx+nu (cid:55)→ R,
which was already hinted in (3).

There are some options available to overcome the exponen-
tial growth with nx + nu, at the cost of a limited capability of
the model. Alternative 1 is to assume f (·) to be ‘separable’
between some dimensions, e.g., f (xt, ut) = f x(xt) + f u(ut).
If this assumption is made for all dimensions, the total num-
ber of parameters present grows quadratically (instead of ex-
ponentially) with nx + nu. Alternative 2 is to use a radial
basis function expansion (Sjöberg et al., 1995), i.e., letting
f (·) only be a function of some norm | · | of (xt, ut), as
f (xt, ut) = f (|(xt, ut)|). The radial basis functions give a
total number of parameters growing linearly with nx + nu.
Alternative 1 and 2 can also be combined, but they will indeed
limit the space of functions possible to describe with the basis
function expansion. However, as a pragmatic solution to the
otherwise exponential growth in the number of parameters it
might still be worth to consider, depending on the particular
problem at hand.

3.1.2 Manual and data-driven truncation
To implement the model in practice, we need to ﬁx the number
of basis functions m to a ﬁnite value, i.e., truncate the expan-
sion. However, ﬁxing m will impose a harsh restriction of
which functions f (·) that can be described by the model. Such

a restriction of the model can prevent overﬁtting to training
data, an argument used by Paduart et al. (2010) for using poly-
nomials only up to 3rd order, for example. We suggest, on
the contrary, to use priors on w(j) to prevent overﬁtting, and
we argue that the approximate interpretation as a GP is a more
natural way to tune the model ﬂexibility, rather than manual
and careful tuning of the truncation. We therefore suggest to
choose m rather big, if the computational resources allows,
and let the prior and data decide which w(j) to be nonzero, a
data-driven truncation.

3.2 Encoding prior assumptions—regularization
The GP provides a rather intuitive but computationally expen-
sive tool for encoding prior assumptions, and thereby helping
the model to generalize from training data. We will use the
approximate GP representation by Solin and Särkkä (2014) to
design prior distributions on the basis function weights w(j).
Learning of the model will then amount to infer the posterior
p(w(j)|y1:T ) ∝ p(y1:T|w(j))p(w(j)), where p(w(j)) denotes
the prior and p(y1:T|w(j)) the data density for the model, i.e.,
the likelihood. To use the prior w(j) ∼ N (0, α−1) and infer-
ring the maximum mode of the posterior can equivalently be
seen in the classical setting as regularized maximum likelihood
estimation

arg min

w

− log p(y1:T|w(j)) + α|w(j)|2.

(6)

3.2.1 Smooth GP-priors for the functions
The Gaussian process (GP, Rasmussen and Williams 2006)
provide a framework for formulating prior assumptions on
functions, resulting in a non-parametric approach for regres-
sion. In many situations the GP allows for an intuitive gen-
eralization of the training data, as illustrated by Figure 1. We
will use the notation

f (x) ∼ GP(m(x), κ(x, x(cid:48)))

(7)
to denote a GP prior on f (·), where m(x) is the mean function
and κ(x, x(cid:48)) the covariance function. The work by Solin and
Särkkä (2014) provides an explicit link between basis function
expansions and GPs, in the case of isotropic1 covariance func-
tions, i.e., κ(x, x(cid:48)) =
κ(|x − x(cid:48)|).
as (4), then it holds that

In particular, if the basis functions are chosen

f (x) ∼ GP(0, κ(x, x(cid:48))) ⇔ f (x) ≈ m(cid:88)

w(j)φ(j)(x)

(8a)

j=0

with

w(j) ∼ N (0, S(λ(j))),

(8b)
where S is the spectral density of κ, and λ(j) is the eigen-
value of φ(j). Thus, this gives a systematic guidance in how
1Remember, however, that this concerns f (·), which resides inside the
state space model. This does not restrict the input-output behavior, from u(t)
to y(t), to have a isotropic covariance.

to choose basis functions and priors on w(i). In particular, the
eigenvalues for the basis function (4) are

(cid:18) πj

(cid:19)2

2L

nx+nu(cid:88)

(cid:18) πjk

(cid:19)2

2Lk

k=1

λ(j) =

, and λ(j1:nx+nu ) =

(9)

for (5). Two common types of covariance functions are ex-
ponentiated quadratic κeq and Matérn κM (Rasmussen and
Williams, 2006),

κeq(r) = sf exp
21−ν
Γ(ν)

κM(r) = sf

(10b)
where r (cid:44) x − x(cid:48), Kν is a modiﬁed Bessel function, and (cid:96), sf
and ν are hyperparameters. Their spectral densities are

Kν

,

l

l

,

2l2

(cid:17)

(cid:16)√2νr

(cid:17)
(cid:16)− r2
(cid:16)√2νr
(cid:17)ν
(cid:16)− π2l2s2
(cid:17)
(cid:0) 2ν
l2 + s2(cid:1)−(ν+ 1

1
2 )(2ν)ν

2

,

(10a)

(11a)

(11b)

2 ) .

√

Seq(s) = sf

2πl2 exp

SM(s) = sf

2π

1
2 Γ(ν+

Γ(ν)l2ν

Altogether, by choosing the priors for w(j) as (8b), it is possi-
ble to approximately interpret f (·), parameterized by the basis
function expansion (2), as a GP. For most covariance functions,
the spectral density S(λ(j)) tends towards 0 when λ(j) → ∞,
meaning that the prior for large values of λ(j) (and hence large
values of j) is tending towards a Dirac mass in 0. Returning
to the discussion on truncation (Section 3.1.2), we realize that
truncation of the basis function expansion with a reasonably
large m therefore has no major impact to the model, but the
approximate GP interpretation is still valid.

As discussed, ﬁnding the posterior mode under a Gaus-
sian prior is equivalent to L2-regularized maximum likeli-
hood estimation. There is no fundamental limitation prohibit-
ing other priors, for example Laplacian (corresponding to L1-
regularization). We use the Gaussian prior because of the con-
nection to a GP prior on f (·), a prior we think is possible to
understand intuitively for the user. The Gaussian prior will
also allow for closed form expressions in the learning.
For book-keeping, we express the prior on w(j) as a Matrix
normal (MN , Dawid 1981) distribution over A. The MN
distribution is parameterized by a mean matrix M ∈ Rnx×m, a
right covariance U ∈ Rnx×nx and left covariance V ∈ Rm×m.
The MN distribution can be deﬁned by the property that A ∼
MN (M, U, V ) if and only if vec(A) ∼ N (vec(M ), V ⊗ U ),
where ⊗ is the Kronecker product. Its density can be written
as

MN (A| M, U, V ) =

exp(cid:0)− 1

2 tr(cid:8)(A − M )TU−1(A − M )V −1(cid:9)(cid:1)

(2π)nxm|V |nx/2|U|m/2

.

(12)

By letting M = 0 and V a diagonal matrix with entries
S(λ(j)), the priors (8b) are incorporated into this parametriza-
tion. We will let U = Q for conjugacy properties, which we
will detail later. Indeed, the marginal variance of the elements

4

in A is then not scaled only by V , but also Q. That scaling is
however constant along the rows, so is the scaling by the hy-
perparameter sf (10). We therefore suggest to simply use sf as
tuning for the overall inﬂuence of the priors; letting sf → ∞
gives a ﬂat prior, or, a non-regularized basis function expan-
sion.

3.2.2 Prior for noise covariances
Apart from f (·), the nx × nx noise covariance matrix Q might
also be unknown. We formulate the prior over Q as an inverse
Wishart (IW, Dawid 1981) distribution. The IW distribu-
tion is a distribution over real-valued positive deﬁnite matrices,
which puts prior mass on all positive deﬁnite matrices and is
parametrized by its number of degrees of freedoms (cid:96) > nx− 1
and an nx × nx positive deﬁnite scale matrix Λ. The density
is deﬁned as

IW(Q| (cid:96), Λ) =

|Λ|(cid:96)/2|Q|−(nx+(cid:96)+1)/2

2(cid:96)nx/2Γnx ((cid:96)/2)

exp

− 1
2

(13)
where Γnx (·) is the multivariate gamma function. The mode
of the IW distribution is
It is a common choice
as a prior for covariance matrices due to its properties (e.g.,
Wills et al. 2012; Shah et al. 2014). When the MN distribu-
tion (12) is combined with the IW distribution (13) we ob-
tain the MNIW distribution, with the following hierarchical
structure

(cid:96)+nx+1.

Λ

(cid:18)

tr(cid:8)Q−1Λ(cid:9)(cid:19)

MNIW(A, Q| M, V, Λ, (cid:96)) =

MN (A| M, Q, V )IW(Q| (cid:96), Λ).

(14)
The MNIW distribution provides a joint prior for the A and
Q matrices, compactly parameterizing the prior scheme we
have discussed, and it is the conjugate prior for our model,
which will facilitate learning.

3.2.3 Discontinuous functions: Sparse singularities

The proposed GP-inspired choice of basis functions and priors
is encoding the assumption on a smooth function f (·). How-
ever, as discussed by Juditsky et al. (1995) and as will be moti-
vated by Example 5.3, there are situations where it is relevant
to say that f (·) is smooth except in a few points. Instead of
assuming an (approximate) GP prior for f (·) on the entire in-
terval [−L, L] we therefore suggest to divide [−L, L] into a
number np of segments, and then assume an individual GP
prior for each segment [pi, pi+1], independent of all other seg-
ment, as illustrated in Figure 2. The number of segments and
the discontinuity points dividing them need to be learned from
data, and an important prior is how the discontinuity points
are distributed, i.e., the number np (e.g., geometrically dis-
tributed) and the location {pi}np
i=1 (e.g., uniformly distributed)
of them. The extension to dimensions higher than nx +nu = 1
can be done by, e.g., a priori decide along which dimensions
there is a possibility for discontinuity points to appear.

with priors

[Ai, Qi] ∼ MNIW(0, V, (cid:96), Λ), i = 0, . . . , np,

np,{pi}np

i=1 ∼ arbitrary prior,

where 1 is the indicator function, parameterizing the piecewise
GP, and ¯ϕ(xt) was deﬁned in (3). If the dynamical behavior of
the data is close-to-linear, and a decent linear model is already
available, this can be incorporated by adding the known linear
function to the right hand side of (15a).

A good practice is to sample parameters from the priors and
simulate the model with those parameters, as a sanity check
before entering the learning phase. Such a habit can also be
fruitful for understanding what the prior assumptions mean in
terms of dynamical behavior. There are standard routines for
sampling from the MN as well as the IW distribution.

5

Figure 2: The idea of a piecewise GP. Here, the interval [−2,−2] is
divided by np = 2 discontinuity points p1 and p2, and a GP is used
to model a function on each of these segments, independently of the
other segments. For practical use, the division into segments needs to
be learned from data.

3.3 Partly known parametrization

,

The suggested model can be tailored if more detailed prior
knowledge is present, such as a physical relationship between
two state variables (perhaps known up to an unknown param-
eter). The suggested model can then be used to learn only the
unknown part of the equations, as brieﬂy illustrated by Svens-
son et al. (2015, Example IV.B).

3.4 Summary

We will now summarize the proposed model. To avoid no-
tational clutter, we omit ut as well as the observation func-
tion (1b):

np(cid:88)

xt+1 =

Ai ¯ϕ(xt)1pi≤xt<pi+1 + vt,

i=0

vt ∼ N (0, Q),

(15a)

(15b)

(15c)
(15d)

−2−10p11p2205xf(x)(cid:111)

θ (cid:44)(cid:110){Ai, Qi}np

4 Learning
We now have a state space model with a (potentially large)
number of unknown parameters

i=0, np,{pi}np

,

i=1

(16)
(We continue assuming g(·) is known, and
all with priors.
thus omit C and R, but learning them is analogous to learning
A and Q.) The learning of the parameters is a quite general
problem, and several learning strategies have been proposed
in the literature, including optimization (Paduart et al., 2010),
EM with extended Kalman ﬁltering (Ghahramani and Roweis,
1998) or sigma point ﬁlters (Kokkala et al., 2015), and par-
ticle Metropolis-Hastings (Tobar et al., 2015). We use a se-
quential Monte Carlo-based learning strategy, namely particle
Gibbs with ancestor sampling (PGAS, Lindsten et al. 2014).
PGAS allows us to take advantage of the fact that our pro-
posed model (3) is linear in A (given xt), at the same time as
it has desirable theoretical properties.

4.1 Sequential Monte Carlo for system identiﬁcation
As a tool for learning parameters in state space models, se-
quential Monte Carlo (SMC) methods have emerged recently,
and an up-to-date overview can be found in the tutorials by
Schön et al. (2015); Kantas et al. (2015).

At the very core in using SMC for system identiﬁcation is
the particle ﬁlter (Doucet and Johansen, 2011), which pro-
vides a numerical solution to the state ﬁltering problem, i.e.,
ﬁnding p(xt | y1:t), the distribution of the states xt given the
model and measurement up till t (the Kalman ﬁlter provides
an analytical solution to this problem, but only in the case
of a linear Gaussian model). The particle ﬁlter propagates
t}N
a set of weighted samples, particles, {xi
i=1 in the state
space model, approximating the ﬁltering density by the em-
(xt) for each t.
Algorithmically, it amounts to iteratively weight the particles
with respect to the measurement yt, resample among them,
and thereafter propagate the resampled particles to the next
time step t + 1. The convergence properties of this scheme
has been studied extensively (see references in Doucet and Jo-
hansen (2011)).

pirical distribution ˆp(xt | y1:t) = (cid:80)N

i=1 ωi

t, ωi

tδxi

t

When using SMC methods for learning parameters, a key
idea is to repeatedly infer the unknown states x1:T with a par-
ticle ﬁlter, and interleave this iteration with inference of the
unknown parameters θ, as follows:

I. Use Algorithm 1 to ﬁnd the states x1:T

for given parameters θ.

II. Update the parameters θ to ﬁt the states

x1:T from the previous step.

(17)

There are several details left to specify in this iteration, and
there is a growing literature including new alternatives (Schön
et al., 2015; Kantas et al., 2015). We will pursue two ap-
proaches for updating θ, one sample-based for exploring the

Algorithm 1 Particle Gibbs Markov kernel.
Input: Trajectory x1:T [k], number of particles N, known state space

1 ∼ p(x1), for i = 1, . . . , N − 1.

model (f, g, Q, R).

t with P(ai

Output: Trajectory x1:T [k + 1]
1: Sample xi
2: Set xN
1 = x1[k].
3: for t = 1 to T do
4:
5:
6:

t = N(cid:0)yt | g(xi
t+1 ∼ N(cid:16)

Set ωi
Sample ai
Sample xi
Set xN
Sample aN
Set xi
9:
10: end for
11: Sample J with P(J = i) ∝ ωi

t+1 = xt+1[k].
t w. P(aN
1:t+1 = {xai
1:t, xi

7:
8:

t

t), R(cid:1), for i = 1, . . . , N.

(cid:17)
t , for i = 1, . . . , N − 1.
t = j) ∝ ωj
, for i = 1, . . . , N − 1.
f (xai
tN(cid:0)xN
t ), Q
t = j) ∝ ωj

t ), Q(cid:1).

t+1 | f (xj

t

t+1}, for i = 1, . . . , N.

T and set x1:T [k + 1] = xJ

1:T .

full posterior p(θ|y1:T ), and one EM-based for ﬁnding the
maximum mode of the posterior, or equivalently, a regularized
maximum likelihood estimate. Both alternatives will utilize
the linear-in-parameter structure of the model (15), and use
a conditional particle ﬁlter with ancestor sampling (Lindsten
et al., 2014) to handle the states.

The conditional particle ﬁlter with ancestor sampling resem-
bles that of a standard particle ﬁlter, but has one of its state
space trajectories ﬁxed. It is outlined by Algorithm 1, and it
can be seen as a procedure for mapping one trajectory x1:T [k]
onto another x1:T [k + 1].
It can be shown (Lindsten et al.,
2014) that Algorithm 1 constitutes a uniformly ergodic Markov
kernel with invariant distribution p(x1:T | y1:T , θ).

4.2 Parameter posterior

As discussed, we will split the learning problem into the iter-
ation (17). We will in this section focus on a key ingredient
for performing Step II of this iteration, namely the conditional
distribution of θ given known states x1:T and measurements
y1:T . By utilizing the Markovian structure of the state space
model, the density p(θ | x1:T , y1:T ) can be written as the fol-
lowing product

p(x1:T , y1:T | θ) = p(x1)

p(xt+1 | xt, θ)p(yt | xt)

T(cid:89)

t=1

(cid:123)(cid:122)

T(cid:89)

t=1

= p(x1)

(cid:124)

p(xt+1 | xt, θ)

p(yt | xt)

.

(18)

T(cid:89)
(cid:124)

t=1

(cid:125)

(cid:123)(cid:122)

(cid:125)

p(x1:T | θ)

p(y1:T | x1:T )

Since we assume that the observation function (1b) is known,
p(yt | xt) is independent of θ, which in turn means that (18) is
proportional to p(x1:T | θ). We will for now assume that p(x1)
is also known, and thus omit it. Let us consider the case with-
out discontinuity points, np = 0. Since vt is assumed to be
Gaussian, p(xt+1 | xt, ut, θ) = N (xt+1 | A ¯ϕ(xt, ut), Q), we
can with some algebraic manipulations (Gibson and Ninness,

6

2005) write

log p(x1:T | A, Q) = − T nx

2 tr(cid:8)Q−1(cid:0)Φ − AΨT − ΨAT + AΣAT(cid:1)(cid:9) ,

2 log det(Q)−

log(2π) − T

2

1

with the (sufﬁcient) statistics

t=1

T(cid:88)
T(cid:88)
T(cid:88)

t=1

t=1

Φ =

Ψ =

Σ =

xt+1xT

t+1,

xt+1 ¯ϕ(xt, ut)T, and

¯ϕ(xt, ut) ¯ϕ(xt, ut)T.

(19)

(20a)

(20b)

(20c)

The density (19) gives via Bayes theorem, with the MNIW
prior distribution for A, Q from Section 3

log p(A, Q) = log p(A| Q) + log p(Q) ∝
2 (nx + (cid:96) + m + 1) log det(Q) − 1
− 1
the posterior

2 tr(cid:8)Q−1(cid:0)Λ + AV −1AT(cid:1)(cid:9) ,

(21)

− 1

log p(A, Q| x1:t) ∝ log p(x1:t | A, Q) + log p(A, Q) ∝

2 (nx + T + (cid:96) + m + 1) log det Q−

2 tr(cid:8)Q−1(cid:0)Λ + Φ − Ψ(Σ + V −1)−1ΨT+

(A − Ψ(Σ + V −1)−1)Q−1(A − Ψ(Σ + V −1)−1)T(cid:1)(cid:9).

1

(22)

This expression will be key for learning: For the fully
Bayesian case, we will recognize (22) as another MNIW
distribution and sample from it, whereas we will maximize it
when we are seeking a point estimate.
Remarks: The expressions needed for an unknown obser-
vation function g(·) are completely analogous. The case with
discontinuity points becomes essentially the same, but with in-
dividual Ai, Qi and statistics for each segment.
If the right
hand side of (15a) also contains a known function h(xt),
e.g., if the proposed model is used only to describe devia-
tions from a known linear model, this can easily be taken
care of by noting that now p(xt+1 | xt, ut, θ) = N (xt+1 −
h(xt)| A ¯ϕ(xt, ut), Q), and thus compute the statistics (20) for
(xt+1 − h(xt)) instead of xt+1.

Inferring the posterior—Bayesian learning

4.3
There is no closed form expression for p(θ | y1:T ), the distri-
bution we want to infer in Bayesian learning. We thus re-
sort to a numerical approximation by drawing samples from
p(θ, x1:T | y1:T ) using Markov chain Monte Carlo methods
(MCMC, Robert and Casella (2004); an alternative is to use
variational methods, akin to Frigola et al. (2014)). MCMC
amounts to constructing a procedure for ‘walking around’ in
θ-space in such a way that the steps . . . , θ[k], θ[k + 1], . . .

7

Algorithm 2 Bayesian learning of (15)
Input: Data y1:T , priors on A, Q and ξ.
Output: K MCMC-samples with p(x1:T , A, Q, ξ | y1:T ) as invari-

ant distribution.

1: Initialize A[0], Q[0], ξ[0].
2: for k = 0 to K do

3: Sample x1:T [k+1](cid:12)(cid:12) A[k], Q[k], ξ[k]
ξ[k+1](cid:12)(cid:12) x1:T [k+1]
5: Sample Q[k+1](cid:12)(cid:12) ξ[k+1], x1:T [k+1]
6: Sample A[k+1](cid:12)(cid:12) Q[k+1], ξ[k+1], x1:T [k+1] by (23)

Alg. 1
Section 4.3

4: Sample

by (23)

7: end for

eventually, for k large enough, become samples from the dis-
tribution of interest.
We start with the case without discontinuity points, i.e.,
np ≡ 0. Since (21) is MNIW, and (19) is a product of
(multivariate) Gaussian distributions, (22) is also an MNIW
distribution (Wills et al., 2012; Dawid, 1981). By identifying
components in (22), we conclude that

p(θ | x1:T , y1:T ) = MNIW(cid:0)A, Q| Ψ(Σ + V −1)−1,
(Σ + V −1)−1, Λ + Φ − Ψ(Σ + V −1)−1ΨT, (cid:96) + T(cid:1)

(23)

We now have (23) for sampling θ given the states x1:T (cf.
(17), step II), and Algorithm 1 for sampling the states x1:T
given the model θ (cf. (17), step I). This makes a particle Gibbs
sampler (Andrieu et al., 2010).

If there are discontinuity points to learn, i.e., np is to be
learned, we can do that by acknowledging the hierarchical
structure of the model. For brevity, we denote {np,{pi}np
i=1}
by ξ, and {Ai, Qi}np
i=1 simply by A, Q. We suggest to
ﬁrst sample ξ from p(ξ | x1:T ), and next sample A, Q from
p(A, Q| x1:T , ξ). The distribution for sampling A, Q is the
MNIW distribution (23), but conditional on data only in the
relevant segment. The other distribution, p(ξ | x1:T ), is trickier
to sample from. We suggest to use a Metropolis-within-Gibbs
(cid:17)
step (Müller, 1991), which means that we ﬁrst sample ξ∗ from
a proposal q(ξ∗ | ξ[k]) (e.g., a random walk), and then accept
,
it as ξ[k+1] with probability min
and otherwise just set ξ[k+1] = ξ[k]. Thus we need to evalu-
ate p(ξ∗ | x1:T ) ∝ p(x1:T | ξ∗)p(ξ∗). The prior p(ξ∗) is chosen
by the user. The density p(x1:T | ξ) can be evaluated using the
expression (see Appendix A.1 for its derivation)

| x1:T )
p(ξ[k] | x1:T )

q(ξ[k] | ξ[k])
q(ξ∗ | ξ[k])

1, p(ξ

(cid:16)

∗

p(x1:T | ξ) =

2nxTi/2
(2π)Ti/2

|V −1|nx/2

|Σi + V −1|nx/2

Γnx ( l+N
2 )
Γnx ( l
2 )
|Λ|l/2

|Λ + Φi + Ψi(Σi + V −1)−1ΨT

i | l+N

2

(24)

ment i ((cid:80)

where Φi etc. denotes the statistics (20) restricted to the corre-
sponding segment, and Ti is the number of data points in seg-
i Ti = T ). We summarize the suggested Bayesian

learning procedure in Algorithm 2.

Our proposed algorithm can be seen as a combination of a
collapsed Gibbs sampler and Metropolis-within-Gibbs, a com-

np(cid:89)

i=0

×

bination which requires some attention to be correct (van Dyk
and Jiao, 2014), see Appendix A.2 for details in our case.
If the hyperparameters parameterizing V are unknown (c.f.
Section 3.2.1), it can be included by extending Algorithm 2
with another Metropolis-within-Gibbs step (see Svensson et al.
(2016) for details).

Algorithm 3 Regularized maximum likelihood
1: Initialize θ[1].
2: for k > 0 do
3: Sample x1:T [k] by Algorithm 1 with parameters θ[k]
4: Compute and update the statistics of x1:T [k] (20,30)
5: Compute θ[k+1] = arg maxθ Q(θ) (26)
6: end for

4.4 Regularized maximum likelihood
A widely used alternative to Bayesian learning is to ﬁnd a point
estimate of θ maximizing the likelihood of the training data
p(y1:T | θ), i.e., maximum likelihood. However, if a very ﬂexi-
ble model is used, some kind of mechanism is needed to pre-
vent the model from overﬁt to training data. We will therefore
use the priors from Section 3 as regularization for the maxi-
mum likelihood estimation, which can also be understood as
seeking the maximum mode of the posterior. We will only treat
the case with no discontinuity points, as the case with disconti-
nuity points does not allow for closed form maximization, but
requires numerical optimization tools, and we therefore sug-
gest Bayesian learning for that case instead.

We will build or learning on the particle stochastic approxi-
mation EM (PSAEM) method proposed by Lindsten (2013),
which uses a stochastic approximation of the EM scheme
(Dempster et al., 1977; Delyon et al., 1999; Kuhn and Lavielle,
2004). EM addresses maximum likelihood estimation in prob-
lems with latent variables. For system identiﬁcation, EM can
be applied by taking the states x1:T as the latent variables,
(Ghahramani and Roweis (1998); another alternative would be
to take the noise sequence v1:T as the latent variables, Umen-
berger et al. (2015)).
The EM algorithm then amounts to
iteratively (cf. (17)) compute the expectation (E)-step
θ[k] [log p(θ | x1:T , y1:T )| y1:T ] ,

Q(θ, θ[k]) = E

(25a)

and update θ in the maximization (M)-step by solving

Q(θ, θ[k]),

θ

θ[k+1] = arg max

(25b)
In the standard formulation, Q is usually computed with re-
spect to the joint likelihood density for x1:T and y1:T . To
incorporate the prior (our regularization), we may consider
the prior as an additional observation of θ, and we have thus
replaced (19) by (22) in Q. Following Gibson and Ninness
(2005), the solution in the (M)-step is found as follows: Since
Q−1 is positive deﬁnite, the quadratic form in (22) is maxi-
mized by

Next, substituting this into (22), the maximizing Q is

Q =

1

nx+T +(cid:96)+m+1

A = Φ(Σ + V −1).

(cid:0)Λ + Φ − Ψ(Σ + V −1)−1Ψ(cid:1) .

(26a)

(26b)

et al. (2011). (Another option with weaker convergence guar-
antees used by, e.g., Ghahramani and Roweis (1998), would
be extended Kalman ﬁltering.) The computational load of a
particle smoother is, however, unfavorable. Lindsten (2013)
proposes a solution to these drawbacks, by using stochastic ap-
proximation EM (Delyon et al., 1999): Algorithm 1, providing
a Markov kernel with invariant distribution p(x1:T | y1:T , θ),
can be combined with stochastic approximation EM Kuhn and
Lavielle (2004). Convergence can then be shown under certain
assumptions.

Algorithmically, PSAEM amounts to run Algorithm 1
(which is much less computationally intense than a particle
smoother), and replace the Q-function (25a) with a Rubinson-
Munroe stochastic approximation of Q, as
Qk(θ) = (1− γk)Qk−1(θ) + γk log p(θ | x1:T [k], y1:T ), (27)
with γ1 = 1,(cid:80)
where {γk}k≥1 is a decreasing sequence of positive step sizes,
k < ∞. Here, x1:T [k] are
sample from an ergodic Markov kernel with p(x1:T | y1:T , θ)
as its invariant distribution, i.e., Algorithm 1. A potential
drawback is that the complexity of Qk(θ) will grow with k.
However, if p(x1:T , y1:T | θ) belongs to the exponential family
(which is the case for our proposed model) we can by deﬁni-
tion write

k γk = ∞ and(cid:80)

k γ2

p(x1:T [k], y1:T | θ) =

h(x1:T [k], y1:T )c(θ) exp(cid:104)η(θ), t[k](cid:105),

(28)
where (cid:104)·,·(cid:105) is an inner product, and t[k] is the statistics (20)
of {x1:T [k], y1:T}. The stochastic approximation Qk(θ) (27)
thus becomes
Qk(θ) ∝ log c(θ) + (cid:104)η(θ), γkt[k] + γk-1t[k-1] + . . .(cid:105).

(29)

Here we see that if we work with the statistics γkt[k] +
γk-1t[k-1]+. . . , the complexity of Q does not grow. We there-
fore introduce the iterative update of the statistics
Φk = (1 − γk)Φk−1 + γkΦ(x1:T [k]),
Ψk = (1 − γk)Ψk−1 + γkΨ(x1:T [k]),
Σk = (1 − γk)Σk−1 + γkΣ(x1:T [k]),

(30a)
(30b)
(30c)

We thus have solved the (M)-step exactly. To compute the ex-
pectation in the (E)-step, approximations are needed. For this,
we can use a particle smoother (Lindsten and Schön, 2013),
which would give a learning strategy in the ﬂavor of Schön

(where Φ(x1:T [k]) refers to (20a), etc). With this parametriza-
tion, we obtain arg maxθ Qk(θ) as the solutions for the vanilla
EM case by just replacing Φ by Φk, etc, in (26). We summa-
rize by Algorithm 3.

8

4.5 Convergence and consistency
We have now proposed two algorithms for learning the model
introduced in Section 3. An important question is in what
sense the algorithms are ‘learning’. The Bayesian learning, Al-
gorithm 2, will by construction (as detailed in Appendix A.2)
provide samples from the true posterior density p(θ | y1:T )
(Andrieu et al., 2010). However, no guarantees regarding the
length of the burn-in period can be given, which is the case for
all MCMC methods. However, the numerical comparisons in
Svensson et al. (2016) and in Section 5.1 suggest that the pro-
posed Gibbs scheme is efﬁcient compared to its state-of-the-
art alternatives. The regularized maximum likelihood learn-
ing, Algorithm 3, can be shown to converge under additional
assumptions (Lindsten, 2013; Kuhn and Lavielle, 2004) to a
stationary point of p(θ|y1:T ). The literature on PSAEM is not
(yet) very rich, but we have not experienced any problems of
non-convergence in practice.

Initialization

4.6
The convergence of Algorithm 2 is not relying on the initializa-
tion, but the burn-in period can nevertheless be reduced. One
useful idea by Paduart et al. (2010) is thus to start with a lin-
ear model, which can be obtained using classical methods. To
avoid Algorithm 3 from converging to a poor local minimum,
Algorithm 2 can ﬁrst be run to explore the ‘landscape’ and
from that, a promising point for initialization of Algorithm 3
can be chosen.

We have for convenience assumed the distribution for the
initial states, p(x1), to be known. This is perhaps not realistic,
but their inﬂuence is minor in many cases, such as all stable
linear models. If needed, they can be included in Algorithm 2
by an additional Metropolis-Hastings step, and in Algorithm 3
by including them in (22) and use numerical optimization tools
(preferably with (26) as initialization to the optimization rou-
tine).

5 Experiments

In this section, we will demonstrate and evaluate how we can
learn the proposed model from data. We start with a small toy
example, originally from Tobar et al. (2015), where we high-
light different aspects of our work. Next, we study the perfor-
mance on the classic benchmark by Narendra and Li (1996),
and thereafter consider an example with real data from two
cascaded water tanks (Schoukens et al., 2015). Matlab code
for all examples is available via the ﬁrst authors homepage.

5.1 A ﬁrst toy example
As a demonstration, we consider the following example,

(cid:16) xt

(cid:17)

7

xt+1 = 10sinc

yt = xt + et,

+ vt,

vt ∼ N (0, 4),
et ∼ N (0, 4).

(31a)

(31b)

9

We generate T = 40 observations, and the challenge is to learn
f (x), when g(x) and the noise variances are known. Note
that even though g(x) is known, y is still corrupted by a non-
negligible amount of noise.

In Figure 3 (a) we illustrate the performance of our proposed
model using m = 40 basis functions on the form (4) when Al-
gorithm 3 is used without regularization. This gives a nonsense
result that is overﬁtted to data, since m = 40 offers too much
ﬂexibility for this example. When a GP-inspired prior from an
exponentiated quadratic covariance function (10a) with length
scale (cid:96) = 3 and sf = 50 is considered, we obtain (b), that is
far more useful and follows the true function rather well in re-
gions were data is present. We conclude that we do not need to
choose m carefully, but can rely on the priors. In (c), we use
the same prior and explore the full posterior by Algorithm 2
to obtain a posterior distribution which provides information
about uncertainty as a part of the learned model (illustrated by
a credibility interval), in particular in regions where no data is
present.

In the next ﬁgure, (d), we replace the set of m = 40 ba-
sis functions on the form (4) with m = 8 Gaussian kernels
to reconstruct the model proposed by Tobar et al. (2015). As
clariﬁed by Tobar (2016), the prior on the weights is a Gaus-
sian distribution inspired by a GP, which makes a close con-
nection to out work. We use Algorithm 2 for learning also in
(d) (which is possible thanks to the Gaussian prior), in con-
trast to (e), where we use the learning algorithm from Tobar
et al. (2015), Metropolis-Hastings, which requires more com-
putation time. It is important to note that quite some effort is
spent by Tobar et al. (2015) to pre-process the data and care-
fully distribute the Gaussian kernels in the state space (see the
bottom of (d)). Further, not highlighted in this example, we
are also able to learn g(·), as well as the variances of vt and et,
in contrast to Tobar et al. (2015).

5.2 Narendra-Li benchmark

The example introduced by Narendra and Li (1996) has be-
come a benchmark for nonlinear system identiﬁcation, e.g.,
The MathWorks, Inc. 2015; Pan et al. 2009; Roll et al. 2005;
Stenman 1999; Wen et al. 2007; Xu et al. 2009. The bench-
mark is deﬁned by the model

(cid:16) x1

(cid:17)

x1
t+1 =

t
1+(x1

t )2 + 1

sin(x2

x2
t+1 =x2

t cos(x2

t exp

t ) + x1
(ut)3

+

1+(ut)2+0.5 cos(x1

yt =

x1
t

1+0.5 sin(x2
t )

+

1+0.5 sin(x1
t )

,

t ),

(cid:16)− (x1

,

t +x2
t )
x2
t

t )2+(x2

t )2

8

(cid:17)

(32a)

(32b)

(32c)

t x2

t ]T. The training data (only input-output
where xt = [x1
data u1:T , y1:T ) is obtained with an input sequence sam-
pled uniformly from the interval [−2.5, 2.5], independently
for each time step t. The input data for the test data is
ut = sin(2πt/10) + sin(2πt/25).

(a) Maximum likelihood estimation of our proposed model, without
regularization; a not useful model.

(b) Maximum likelihood estimation of our proposed model, with reg-
ularization. A subset of the m = 40 basis functions used are sketched
at the bottom. Computation time: 12 s.

(c) Bayesian learning for our proposed model, i.e., the entire posterior
is explored. Computation time: 12 s.

(d) Posterior distribution for the basis functions (sketched at the bot-
tom) used by Tobar et al. (2015), but Algorithm 2 for learning. Com-
putation time: 9 s.

(e) The example as presented by Tobar et al.
Metropolis-Hastings for learning. Computation time: 32 s.

(2015), using

Figure 3: True function (black), states underlying the data (red) and
learned model (blue, gray) for Examples 5.1.

According to Narendra and Li (1996, p. 369), it ‘does
not correspond to any real physical system and is deliber-
ately chosen to be complex and distinctly nonlinear’. The
original formulation is somewhat extreme, with no noise and
T = 500 000 data samples for learning. In the work by Sten-
man (1999), a white Gaussian measurement noise with vari-
ance 0.1 is added to the training data, and less data are used
for learning. We apply our proposed scheme with a second or-
der state space model, np = 0, and a known, linear g(·). (Even
though the data is generated with a nonlinear g(·), it turn out
this will give a satisfactory performance.) We use 7 basis func-
tions per dimension (i.e., 686 weights w(j) to learn in total) on
the form (5), with prior from the covariance function (10a)
with length scale (cid:96) = 1.

For the original case without any noise, but using only T =
500 data points, we obtain a root mean square error (RMSE)
for the simulation of only 0.039. Our result is in contrast to
the signiﬁcantly bigger simulation errors by Narendra and Li
(1996), although they use 1 000 times as many data points.
For the more interesting case with measurement noise in the
training data, we achieve a result almost the same as for the
noise-free data. We compare to some previous results reported
in the literature (T is the number of data samples in the training
data):

Reference
This paper
Roll et al. (2005)
Stenman (1999)
Xu et al. (2009) (AHH)
Xu et al. (2009) (MARS)

RMSE
0.06*
0.43
0.46
0.31
0.49

T
2 000
50 000
50 000
2 000
2 000

*The number is averaged over 10 realizations

It is clear that the proposed model is able to describe the

system behavior well, in contrast to previous results.

5.3 Water tank data

We consider the data sets provided by Schoukens et al. (2015),
collected from a physical system consisting of two cascaded
water tanks, where the outlet of the ﬁrst tank goes into the
second one. Two data sets are provided, both with 1024 data
samples, one for learning and one for evaluation. The input
u (voltage) is inﬂow to the ﬁrst tank, and the output y (volt-
age) is the measured water level in the second tank. This is a
well-studied system (e.g., Wigren and Schoukens 2013), but a
peculiarity in this data set is the presence of overﬂow, both in
the ﬁrst and the second tank. When the ﬁrst tank overﬂows, it
goes only partly into the second tank.

We apply our proposed model, with a two dimensional state

space. We use the following structure for our model:

x1
t+1 = f 1(x1
x2
t+1 = f 2(x1
yt = x2

t , ut) + v1
t ,
t , x2
t + et.

t , ut) + v2
t ,

(33a)
(33b)
(33c)

It is surprisingly hard to perform better than linear models in
this problem, perhaps because of the close-to-linear dynamics

10

−30−20−100102030−10010xtxt+1−30−20−100102030−10010xtxt+1−30−20−100102030−10010xtxt+1−30−20−100102030−10010xtxt+1−30−20−100102030−10010xtxt+1PosteriormodeluncertaintyLearnedmodelTruestatetransitionfunctionStatesamplesunderlyingdataBasisfunctions6 Conclusions and further work

During the recent years, there has been a rapid development
of powerful parameter estimation tools for state space models.
These methods allows for learning in complex and extremely
ﬂexible models, and this paper is a response to the situation
when the learning algorithm is able to learn a more complex
state space model than useful (cf. Figure 3a). For this purpose,
we have in the spirit of Peterka (1981) chosen to formulate
GP-inspired priors for a basis function expansion, in order to
‘softly’ tune its complexity and ﬂexibility in a way that hope-
fully resonates with the users intuition. In this sense, our work
resembles the recent work in the machine learning commu-
nity on using GPs for learning dynamical models (see, e.g.,
Frigola 2015; Bijl et al. 2016; Mattos et al. 2016). We have
also tailored efﬁcient learning algorithms for the model, both
for inferring the full posterior, and ﬁnding a point estimate.

It is a rather hard task to make a sensible comparison be-
tween our model-focused approach, and approaches which
provide a general-purpose black-box learning algorithm with
very few user choices. Because of their different nature, we
do not see any ground to claim superiority of one approach
over another. In light of the promising experimental results,
however, we believe this model-focused perspective can pro-
vide additional insight into the nonlinear system identiﬁcation
problem. There is certainly more to be done and understand
when it comes to this approach, in particular regarding the for-
mulation of priors and usage of the posterior.

Our preference for a particular set of basis functions (4) and
priors is due to their connection to GPs. It is, however, not the
only computationally feasible alternative, and an interesting
direction for future research would be to ﬁnd corresponding
interpretations for other such choices.
It might be possible,
for instance, to establish a connection to the student-t process
(Shah et al., 2014).

We have proposed an algorithm for Bayesian learning of
our model, which renders K samples of the parameter poste-
rior, representing a distribution over models. A relevant ques-
tion is then how to compactly represent and use these samples
to efﬁciently make predictions. Many control design meth-
ods provides performance guarantees for a perfectly known
model. An interesting topic would hence be to incorporate
model uncertainty (as provided by the posterior) into control
design and provide probabilistic guarantees, such that perfor-
mance requirements are fulﬁlled with, e.g., 95% probability.

Acknowledgements

This work was supported by the Swedish Research Council, project
Probabilistic modeling of dynamical systems (contract number: 621-
2013-5524). We would like to thank Dave Zachariah and Per Matts-
son for useful comments on the manuscript.

Figure 4: The simulated and true output for the evaluation data in the
water tank experiment (Section 5.3). The order of the NARX models
is the number of regressors in u and y.

in most regimes, in combination with the non-smooth over-
ﬂow events. We thus use discontinuity points. Since we can
identify the overﬂow level in the second tank directly in the
data, we ﬁx a discontinuity point at x2 = 10 for f 2(·), and
learn the discontinuity points for f 1(·). We use the covariance
function (10a) with length scale (cid:96) = 3 for prior, and 5 basis
functions per dimension, in total 150 weights w(j) to learn.

We apply the fully Bayesian learning algorithm to sample
from the model posterior. We use all sampled models to simu-
late the test output from the test input for each model to repre-
sent a posterior for the test data output. We compute the RMSE
for the difference between the mode of the posterior and the
true test output, and also compare it to nonlinear ARX-models
(NARX, Ljung 1999) in Figure 4. It is particularly interesting
to note how the different models handle the overﬂow around
time 3 000. We have tried to select the NARX conﬁgurations
minimizing the simulation errors, and when ﬁnding their pa-
rameters by maximizing their likelihood (which is equivalent
to minimize their 1-step-ahead prediction (Ljung, 1999)), the
best NARX model is performing approximately 35% worse
(in terms of RMSE) than our proposed model. When instead
learning the NARX models with ‘simulation focus’, i.e., mini-
mizing their simulation error on the training data, their RMSE
decreases, and approaches almost the one of our model for one
of the models. Since the corresponding change in learning ob-
jective is not available to our model, this comparison might
not offer much further insight. It would, however, be an inter-
esting question for further research how to implement learning
with ‘simulation focus’ for our model.

11

01,0002,0003,0004,000510output(V)01,0002,0003,0004,000510time(s)output(V)Validationdata2ndorderlinearstatespacemodel.RMSE:0.675thorderNARXwithsigmoidnet.RMSE:0.73”simulationfocus.RMSE:0.495thorderNARXwithwavelets.RMSE:0.61”simulationfocus.RMSE:0.64Theproposedmodel.RMSE:0.45Credibilityintervalfortheproposedmethod.References
M. A. Alvarez, D. Luengo, and N. D. Lawrence. Latent force models.
In Proceedings of 12th International Conference on Artiﬁcial Intel-
ligence and Statistics (AISTATS), pages 9–16, Clearwater Beach,
FL, USA, Apr. 2009.

C. Andrieu, A. Doucet, and R. Holenstein. Particle Markov chain
Monte Carlo methods. Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 72(3):269–342, 2010.

H. Bijl, T. B. Schön, J.-W. van Wingerden, and M. Verhaegen. Onlise
sparse Gaussian process training with input noise. arXiv preprint
arXiv:1601.08068, 2016. Submitted for publication.

C. M. Bishop. Pattern recognition and machine learning. Springer,

New York, NY, USA, 2006.

B. Calderhead, M. Girolami, and N. D. Lawrence. Accelerat-
ing Bayesian inference over nonlinear differential equations with
Gaussian processes. In Advances in neural information processing
systems 21 (NIPS), pages 217–224, Vancouver, BC, Canada, Dec.
2008.

T. Chen, H. Ohlsson, and L. Ljung. On the estimation of transfer
functions, regularizations and Gaussian processes—revisited. Au-
tomatica, 48(8):1525–1535, 2012.

A. P. Dawid. Some matrix-variate distribution theory: notational con-
siderations and a Bayesian application. Biometrika, 68(1):265–
274, 1981.

R. A. Delgado, J. C. Agüero, G. C. Goodwin, and E. M. Mendes.
Application of rank-constrained optimisation to nonlinear system
In Proceedings of 1st IFAC Conference on Mod-
identiﬁcation.
elling, Identiﬁcation and Control of Nonlinear Systems (MIC-
NON), pages 814–818, Saint Petersburg, Russia, June 2015.

B. Delyon, M. Lavielle, and E. Moulines. Convergence of a stochastic
approximation version of the EM algorithm. Annals of Statistics,
27(1):94–128, 1999.

A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood
from incomplete data via the EM algorithm. Journal of the Royal
Statistical Society. Series B (Methodological), 39(1):1–38, 1977.

A. Doucet and A. M. Johansen. A tutorial on particle ﬁltering and
smoothing: Fifteen years later. In D. Crisan and B. Rozovsky, ed-
itors, Nonlinear Filtering Handbook, pages 656–704. Oxford Uni-
versity Press, Oxford, UK, 2011.

R. Frigola. Bayesian Time Series Learning with Gaussian Processes.

PhD thesis, University of Cambridge, UK, 2015.

R. Frigola, F. Lindsten, T. B. Schön, and C. Rasmussen. Bayesian in-
ference and learning in Gaussian process state-space models with
particle MCMC. In Advances in Neural Information Processing
Systems 26 (NIPS), pages 3156–3164, Lake Tahoe, NV, USA, Dec.
2013.

R. Frigola, Y. Chen, and C. Rasmussen. Variational Gaussian process
state-space models. In Advances in Neural Information Processing
Systems 27 (NIPS), volume 27, pages 3680–3688, Montréal, QC,
Canada, Dec. 2014.

Z. Ghahramani and S. T. Roweis. Learning nonlinear dynamical sys-
tems using an EM algorithm. In Advances in Neural Information
Processing Systems (NIPS) 11, pages 431–437. Denver, CO, USA,
Nov. 1998.

S. Gibson and B. Ninness. Robust maximum-likelihood estimation of
multivariable dynamic systems. Automatica, 41(10):1667–1682,
2005.

A. Juditsky, H. Hjalmarsson, A. Benveniste, B. Delyon, L. Ljung,
J. Sjöberg, and Q. Zhang. Nonlinear black-box models in sys-
tem identiﬁcation: Mathematical foundations. Automatica, 31(12):
1725–1750, 1995.

N. Kantas, A. Doucet, S. S. Singh, J. M. Maciejowski, and N. Chopin.
On particle methods for parameter estimation in state-space mod-
els. Statistical Science, 30(3):328–351, 2015.

J. Kocijan, A. Girard, B. Banko, and R. Murray-Smith. Dynamic
systems identiﬁcation with Gaussian processes. Mathematical and
Computer Modelling of Dynamical Systems, 11(4):411–424, 2005.

J. Kokkala, A. Solin, and S. Särkkä.

Sigma-point ﬁltering and
smoothing based parameter estimation in nonlinear dynamic sys-
tems. arXiv preprint arXiv:1504.06173, 2015. Accepted for pub-
lication in Journal of Advances in Information Fusion.

E. Kuhn and M. Lavielle. Coupling a stochastic approximation ver-
sion of EM with an MCMC procedure. ESAIM: Probability and
Statistics, 8:115–131, 2004.

F. Lindsten. An efﬁcient stochastic approximation EM algorithm us-
In Proceedings of the 38th Inter-
ing conditional particle ﬁlters.
national Conference on Acoustics, Speech, and Signal Processing
(ICASSP), pages 6274–6278, Vancouver, BC, Canada, May 2013.

F. Lindsten and T. B. Schön. Backward simulation methods for Monte
Carlo statistical inference. Foundations and Trends in Machine
Learning, 6(1):1–143, 2013.

F. Lindsten, M. I. Jordan, and T. B. Schön. Particle Gibbs with ances-
tor sampling. The Journal of Machine Learning Research, 15(1):
2145–2184, 2014.

L. Ljung. System identiﬁcation: theory for the user. Prentice Hall,

Upper Saddle River, NJ, USA, 2 edition, 1999.

L. Ljung. Perspectives on system identiﬁcation. Annual Reviews in

Control, 34(1):1–12, 2010.

B. Macdonald, C. Higham, and D. Husmeier. Controversy in mech-
anistic modelling with Gaussian processes. In Proceedings of the
32st International Conference on Machine Learning (ICML), pages
1539–1547, Lille, France, July 2015.

C. L. C. Mattos, Z. Dai, A. Damianou, J. Forth, G. A. Barreto, and
N. D. Lawrence. Recurrent Gaussian processes. arXiv preprint
arXiv:1511.06644, 2016. To be presented at the 4th International
Conference on Learning Representations (ICLR), San Juan, Puerto
Rico, May 2016.

P. Mattsson, D. Zachariah, and P. Stoica. Recursive identiﬁcation of
nonlinear systems using latent variables. Submitted for publica-
tion, 2016.

12

P. Müller. A generic approach to posterior intergration and Gibbs
sampling. Technical report, Department of Statistics, Purdue Uni-
versity, West Lafayette, IN, USA, 1991.

K. S. Narendra and S.-M. Li. Neural networks in control systems,
chapter 11, pages 347–394. Lawrence Erlbaum Associates, Hills-
dale, NJ, USA, 1996.

J. Paduart, L. Lauwers, J. Swevers, K. Smolders, J. Schoukens, and
R. Pintelon. Identiﬁcation of nonlinear systems using polynomial
nonlinear state space models. Automatica, 46(4):647 – 656, 2010.
ISSN 0005-1098.

T. H. Pan, S. Li, and N. Li. Optimal bandwidth design for lazy learn-
ing via particle swarm optimization. Intelligent Automation & Soft
Computing, 15(1):1–11, 2009.

V. Peterka. Bayesian system identiﬁcation. Automatica, 17(1):41–53,

1981.

G. Pillonetto and G. De Nicolao. A new kernel-based approach for

linear system identiﬁcation. Automatica, 46(1):81–93, 2010.

G. Pillonetto, A. Chiuso, and G. De Nicolao. Prediction error iden-
tiﬁcation of linear systems: a nonparametric Gaussian regression
approach. Automatica, 47(2):291–305, 2011.

C. E. Rasmussen and C. K. I. Williams. Gaussian processes for ma-

chine learning. MIT Press, Cambridge, MA, USA, 2006.

C. P. Robert and G. Casella. Monte Carlo statistical methods.

Springer, New York, 2 edition, 2004.

J. Roll, A. Nazin, and L. Ljung. Nonlinear system identiﬁcation via

direct weight optimization. Automatica, 41(3):475–490, 2005.

S. Roweis and Z. Ghahramani. An EM algorithm for identiﬁca-
tion of nonlinear dynamical systems. Unpublished, available at
http://mlg.eng.cam.ac.uk/zoubin/papers.html, 2000.

T. B. Schön, A. Wills, and B. Ninness. System identiﬁcation of non-

linear state-space models. Automatica, 47(1):39–49, 2011.

T. B. Schön, F. Lindsten, J. Dahlin, J. Wågberg, C. A. Naesseth,
A. Svensson, and L. Dai. Sequential Monte Carlo methods for
system identiﬁcation. In Proceedings of the 17th IFAC Symposium
on System Identiﬁcation (SYSID), pages 775–786, Beijing, China,
Oct. 2015.

M. Schoukens, P. Mattson, T. Wigren, and J.-P. Noël. Cascaded tanks
benchmark combining soft and hard nonlinearities. Available:
homepages.vub.ac.be/~mschouke/benchmark2016.html, 2015.

A. Shah, A. G. Wilson, and Z. Ghahramani. Student-t processes as
alternatives to Gaussian processes. In Proceedings of 17th Inter-
national Conference on Artiﬁcial Intelligence and Statistics (AIS-
TATS), pages 877–885, Reykjavik, Iceland, Apr. 2014.

J. Sjöberg, Q. Zhang, L. Ljung, A. Benveniste, B. Delyon, P.-Y. Glo-
rennec, H. Hjalmarsson, and A. Juditsky. Nonlinear black-box
modeling in system identiﬁcation: a uniﬁed overview. Automat-
ica, 31(12):1691–1724, 1995.

A. Solin and S. Särkkä. Hilbert space methods for reduced-rank
arXiv preprint arXiv:1401.5508,

Gaussian process regression.
2014.

13

A. Stenman. Model on demand: Algorithms, analysis and applica-

tions. PhD thesis, Linköping University, Sweden, 1999.

A. Svensson, T. B. Schön, A. Solin, and S. Särkkä. Nonlinear state
space model identiﬁcation using a regularized basis function ex-
pansion. In Proceedings of the 6th IEEE international workshop
on computational advances in multi-sensor adaptive processing
(CAMSAP), pages 493–496, Cancun, Mexico, Dec. 2015.

A. Svensson, A. Solin, S. Särkkä, and T. B. Schön. Computationally
efﬁcient Bayesian learning of Gaussian process state space mod-
els. In Proceedings of 19th International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), Cadiz, Spain, May 2016.

Narendra-Li

benchmark

of

The MathWorks,
grey
Example ﬁle

Inc.
box modeling

Nonlinear
tem.
provided
System Identiﬁcation ToolboxTM,
http://mathworks.com/help/ident/examples/narendra-li-
benchmark-system-nonlinear-grey-box-modeling-of-a-discrete-
time-system.html.

system:
sys-
a
by Matlab R(cid:13) R2015b
2015.
at

discrete-time

Available

F. Tobar. Personal communication, 2016.

F. Tobar, P. M. Djuri´c, and D. P. Mandic. Unsupervised state-space
modeling using reproducing kernels. IEEE Transactions on Signal
Processing, 63(19):5210–5221, 2015.

J. Umenberger, J. Wågber, I. R. Manchester, and T. B. Schön. On
identiﬁcation via EM with latent disturbances and Lagrangian re-
laxation. In Proceedings of the 17th IFAC Symposium on System
Identiﬁcation (SYSID), pages 69–74, Beijing, China, Oct. 2015.

D. A. van Dyk and X. Jiao. Metropolis-Hastings within partially col-
lapsed Gibbs samplers. Journal of Computational and Graphical
Statistics, 24(2):301–327, 2014.

Y. Wang and D. Barber. Gaussian processes for Bayesian estimation
in ordinary differential equations. In Proceedings of the 31st Inter-
national Conference on Machine Learning (ICML), pages 1485–
1493, Beijing, China, June 2014.

C. Wen, S. Wang, X. Jin, and X. Ma. Identiﬁcation of dynamic sys-
tems using piecewise-afﬁne basis function models. Automatica, 43
(10):1824–1831, 2007.

T. Wigren and J. Schoukens. Three free data sets for development and
benchmarking in nonlinear system identiﬁcation. In Proceedings
of the 2013 European Control Conference (ECC), pages 2933–
2938, Zurich, Switzerland, July 2013.

A. Wills, T. B. Schön, F. Lindsten, and B. Ninness. Estimation of lin-
In Proceedings of the 16th
ear systems using a Gibbs sampler.
IFAC Symposium on System Identiﬁcation (SYSID), pages 203–
208, Brussels, Belgium, July 2012.

J. Xu, X. Huang, and S. Wang. Adaptive hinging hyperplanes and
its applications in dynamic system identiﬁcation. Automatica, 45
(10):2325–2332, 2009.

A Technical details

A.1 Derivation of (24)
By Bayes rule, we have
p(x1:T | ξ) =

p(A, Q| ξ)p(x1:T | A, Q, ξ)

p(A, Q| ξ, x1:T )

.

(34)

η(ξ)·|Q|χ(ξ)·exp(cid:0)− 1

2 tr(cid:8)Q−1τ (A, x1:T , ξ)(cid:9)(cid:1), with different

The expression for each term is found in (12-14),
(18)
and (23), respectively. All of them have a functional form
η, χ and τ. Starting with the |Q|-part, the sum of the exponents
for all such terms in both the numerator and the denominator
sums to 0. The same thing happens to the exp-part, which can
either be worked out algebraically, or realized since p(x1:T | ξ)
is independent of Q. What remains is everything stemming
from η, which indeed is p(x1:T | ξ), (24).

Invariant distribution of Algorithm 2

A.2
As pointed out by van Dyk and Jiao (2014), the combination of
Metropolis-within-Gibbs and partially collapsed Gibbs might
obstruct the invariant distribution of a sampler. In short, the
reason is that a Metropolis-Hastings (MH) step is conditioned
on the previous sample, and the combination with a partially
collapsed Gibbs sampler can therefore be problematic, which
becomes clear if we write the MH procedure as the operator
MH in the following simple example (from van Dyk and Jiao
(2014)) of a sampler for ﬁnding the distribution p(a, b):

Sample a[k+1] ∼ p(a| b[k]) (Gibbs)
Sample b [k+1] ∼ MH(b| a[k+1], b[k]) (MH)

So far, this is a valid sampler. However, if collapsing over b,
the sampler becomes

Sample a[k+1] ∼ p(a) (Partially collapsed Gibbs)
Sample b [k+1] ∼ MH(b| a[k+1], b[k]) (MH)

where the problematic issue, obstructing the invariant distri-
bution, is the joint conditioning on a[k +1] and b[k] (marked
in red), since a[k + 1] has been sampled not conditioned on
b[k]. However, spelling out the details from Algorithm 2 in
Algorithm 4, it is clear that this problematic conditioning is
not present in our case.

Algorithm 4 Details of Algorithm 2
2: for k = 0 to K do

3: Sample x1:T [k+1](cid:12)(cid:12) A[k], Q[k], ξ[k]
5: Sample Q[k+1](cid:12)(cid:12) ξ[k+1], x1:T [k+1]
6: Sample A[k+1](cid:12)(cid:12) Q[k+1], ξ[k+1], x1:T [k+1](Gibbs)

ξ[k+1] ∼ MH(x1:T [k+1], ξ[k])

4: Sample

7: end for

(Gibbs)

(Gibbs)

14

