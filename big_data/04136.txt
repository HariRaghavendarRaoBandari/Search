6
1
0
2

 
r
a

 

M
9
2
 
 
]

.

C
O
h
t
a
m

[
 
 

2
v
6
3
1
4
0

.

3
0
6
1
:
v
i
X
r
a

On the Inﬂuence of Momentum Acceleration on Online

Learning

Kun Yuan
Bicheng Ying
Ali H. Sayed
Department of Electrical Engineering
University of California
Los Angeles, CA 90095, USA

kunyuan@ucla.edu

ybc@ucla.edu

sayed@ucla.edu

Abstract

The article examines in some detail the convergence rate and mean-square-error perfor-
mance of momentum stochastic gradient methods in the constant step-size case and slow
adaptation regime. The results establish that momentum methods are equivalent to the
standard stochastic gradient method with a re-scaled (larger) step-size value. The size of
the re-scaling is determined by the value of the momentum parameter. The equivalence re-
sult is established for all time instants and not only in steady-state. The analysis is carried
out for general risk functions, and is not limited to quadratic risks. One notable conclusion
is that the well-known beneﬁts of momentum constructions for deterministic optimization
problems do not necessarily carry over to the stochastic (online) setting when adaptation
becomes necessary and when the true gradient vectors are not known beforehand. The anal-
ysis also suggests a method to retain some of the advantages of the momentum construction
by employing a decaying momentum parameter, as opposed to a decaying step-size. In this
way, the enhanced convergence rate during the initial stages of adaptation is preserved
without the often-observed degradation in MSD performance.
Keywords:
method, Nesterov method, mean-square-error analysis, convergence rate.

online learning, stochastic gradient, momentum acceleration, heavy-ball

1. Introduction

Stochastic optimization focuses on the problem of optimizing the expectation of a loss
function, written as

J(w) ∆= Eθ[Q(w; θ)],

min
w∈RM

(1)

where θ is a random variable whose distribution is generally unknown and J(w) is a convex
function (usually strongly-convex due to regularization). If the probability distribution of
the data, θ, is known beforehand, then one can evaluate J(w) and seek its minimizer by
means of a variety of gradient-descent or Newton-type methods (Polyak, 1987; Bertsekas,
1999; Nesterov, 2004). We refer to these types of problems, where J(w) is known, as
deterministic optimization problems. On the other hand, when the probability distribution
of the data is unknown, then the risk function J(w) is unknown as well; only instances
of the loss function, Q(w; θ), may be available at various observations θi, where i refers

1

to the sample index. We refer to these types of problems, where J(w) is unknown but
deﬁned implicity as the expectation of some known loss form, as stochastic optimization
problems. This article deals with this second type of problems, which are prevalent in
online adaptation and learning contexts (Widrow and Stearns, 1985; Haykin, 2008; Sayed,
2008; Theodoridis, 2015).

When J(w) is diﬀerentiable, one of the most popular techniques to seek minimizers for
(1) is to employ the stochastic gradient method. This algorithm is based on employing
instantaneous approximations for the true (unavailable) gradient vectors, ∇wJ(w), by us-
ing the gradients of the loss function, ∇wQ(w; θi), evaluated at successive samples of the
streaming data θi over the iteration index i, say, as:

wi = wi−1 − µ∇w Q(wi−1; θi),

i ≥ 0.

(2)

where µ > 0 is a step-size parameter. Note that we are denoting the successive iterates by
wi and using the boldface notation to refer to the fact that they are random quantities in
view of the randomness in the measurements {θi}. Due to their simplicity, robustness to
noise and uncertainty, and scalability to big data, such stochastic gradient methods have
become popular in large-scale optimization, machine learning, and data mining applications
(Zhang, 2004; Bottou, 2010; Gemulla et al., 2011; Sutskever et al., 2013; Kahou et al., 2013;
Cevher et al., 2014; Szegedy et al., 2015; Zareba et al., 2015).

1.1 Convergence Rates

Stochastic-gradient algorithms can be implemented with decaying step-sizes, such as µ(i) =
τ /i for some constant τ , or with constant step-sizes, µ > 0. The former generally ensure
asymptotic convergence to the true minimizer of (1), denoted by wo, at a convergence rate
that is on the order of O(1/i). This guarantee, however, comes at the expense of turning
oﬀ adaptation and learning as time progresses since the step-size value approaches zero in
the limit, as i → ∞. As a result, the algorithms lose the ability to track concept drifts. In
comparison, constant step-sizes keep adaptation and learning alive and infuse a desirable
tracking mechanism into the operation of the algorithm: even if the minimizers drift with
time, the algorithm will generally be able to adjust and track their locations. Moreover,
convergence can now occur at the considerably faster exponential rate, O(αi), for some
α ∈ (0, 1). These favorable properties come at the expense of a small deterioration in
the limiting accuracy of the iterates since almost-sure convergence is not guaranteed any
longer. Instead, the algorithm converges in the mean-square-error sense towards a small
neighborhood of the true minimizer, wo, whose radius is on the order of O(µ). This is still a
desirable conclusion because the value of µ is controlled by the designer and can be chosen
suﬃciently small.

In other words, a well-known tradeoﬀ develops between convergence rate and mean-
square-error (MSE) performance. The asymptotic MSE performance level approaches O(µ)
while the convergence rate is given by α = 1 − O(µ) (Polyak, 1987; Sayed, 2014a). It is
nowadays well-recognized that the small O(µ) degradation in performance is acceptable in
most large-scale learning and adaptation problems (Bousquet and Bottou, 2008; Bottou,
2010; Sayed, 2014b). This is because, in general, there are always modeling errors in formu-
lating optimization problems of the form (1); the cost function may not reﬂect perfectly the

2

scenario and data under study. Therefore, insisting on attaining asymptotic convergence to
the true minimizer may not be necessarily the best course of action or may not be worth the
eﬀort. It is often more advantageous to tolerate a small steady-state error that is negligible
in most cases, but is nevertheless attained at a faster exponential rate of convergence than
the slower rate of O(1/i). Furthermore, the data models in many applications are more
complex than assumed, with possibly local minima. In these cases, constant step-size im-
plementations can help reduce the risk of being trapped at local solutions. For these various
reasons, and since our emphasis is on algorithms that are able to learn continuously, we
shall focus on constant step-size implementations.

1.2 Acceleration Methods

In the deterministic optimization case, when the true gradient vectors of the risk function
J(w) are available, the iterative algorithm for seeking the minimizer of J(w) becomes the
following gradient-descent recursion

wi = wi−1 − µ∇w J(wi−1),

i ≥ 0,

(3)

There have been many ingenious methods proposed in the literature to enhance the con-
vergence of these methods for both cases of convex and strongly-convex risks, J(w). Two
of the most notable and successful techniques are the heavy ball method (Polyak, 1964,
1987; Qian, 1999) and Nesterov’s acceleration method (Nesterov, 1983, 2004, 2005). The
two methods are diﬀerent but they both rely on the concept of adding a momentum term
to the recursion. When the risk function J(w) is ν-strongly convex and has δ-Lipschitz
continuous gradients, both methods succeed in accelerating the gradient descent algorithm
to attain a faster exponential convergence rate (Polyak, 1987) (Nesterov, 2004), and this
rate cannot be attained by standard gradient descent method. Speciﬁcally, it is shown in
(Polyak, 1987) (Nesterov, 2004) that for heavy-ball and Nesterov’s acceleration methods,
the convergence of the iterates wi towards wo occurs at the rate:

√δ + √ν!2
kwi − wok2 ≤ √δ − √ν

kwi−1 − wok2,

(4)

In contrast, in Theorem 2.1.15 of (Nesterov, 2005) and Theorem 4 in Section 1.4 of (Polyak,
1987), the fastest rate for gradient descent method is shown to be

It can be veriﬁed that

δ + ν(cid:19)2
kwi − wok2 ≤(cid:18) δ − ν
√δ − √ν
√δ + √ν

<

kwi−1 − wok2.

δ − ν
δ + ν

(5)

(6)

when δ > ν. This inequality conﬁrms that the momentum algorithm can achieve a faster
rate in deterministic optimization and that rate cannot be attained by standard gradient
descent.

3

Motivated by these useful acceleration properties in the deterministic context, momen-
tum terms have been subsequently introduced into stochastic optimization algorithms as well
(Polyak, 1987; Proakis, 1974; Sharma et al., 1998; Shynk and Roy, June 1988; Roy and Shynk,
1990; Tugay and Tanik, 1989; Bellanger, 2001; Wiegerinck et al., 1994; Hu et al., 2009; Lan,
2012; Ghadimi and Lan, 2012; Zhong and Kwok, 2014) and applied, for example, to prob-
lems involving the tracking of chirped sinusoidal signals (Ting et al., 2000) or deep learning
(Sutskever et al., 2013; Kahou et al., 2013; Szegedy et al., 2015; Zareba et al., 2015). How-
ever, the analysis in this paper will show that their advantages from deterministic optimiza-
tion do not necessarily carry over to the stochastic setting due to the presence of stochastic
gradient noise (which is the diﬀerence between the actual gradient vector and its approx-
imation). Speciﬁcally, we will show that any advantage they bring forth can be achieved
by staying with the original stochastic-gradient algorithm and adjusting its step-size to a
larger value. For instance, for optimization problem (1), we will show that if the step-sizes,
µm for the momentum (heavy-ball or Nesterov) methods and µ for the standard stochastic
gradient algorithms, are suﬃciently small and satisfy the relation

µ =

µm
1 − β

where β ∈ [0, 1) is the coeﬃcient of the momentum term, then it will hold that

Ekwm,i − wik2 = O(µ3/2), i = 0, 1, 2, . . .

(7)

(8)

where wm,i and wi denote the iterates generated at time i by the momentum and standard
implementations, respectively. In the special case when J(w) is quadratic in w, as happens
in mean-square-error design problems, we can tighten (8) to

Ekwm,i − wik2 = O(µ2), i = 0, 1, 2, . . .

(9)

What is important to note is that, we will show that these results hold for every i, and not
only asymptotically. Therefore, when µ is suﬃciently small, property (8) establishes that the
stochastic gradient method and the momentum versions are fundamentally equivalent since
their iterates evolve close to each other at all times. We establish this equivalence theory
under the situation where the cost function is convex and diﬀerentiable. However, as our
numerical simulation over multi-layer neural network (see Sec. 7.4) shows, the equivalence
between standard and momentum stochastic gradient methods can still hold in non-convex
or non-diﬀerentiable scenarios.

1.3 Related Works in the Literature

There are useful results in the literature that deal with special instances of the general
framework developed in this work. These earlier results focus mainly on the mean-square-
error case when J(w) is quadratic in w, in which case the stochastic gradient algorithm
reduces to the famed least-mean-squares (LMS) algorithm. We will not be limiting our
analysis to this case so that our results will be applicable to a broader class of learning
problems beyond mean-square-error estimation (e.g., logistic regression would be covered by
our results as well). As the analysis and detailed derivations will reveal, the treatment of the

4

general J(w) case is demanding because the Hessian matrix of J(w) is now w−dependent,
whereas it is a constant matrix in the quadratic case.

Some of the earlier investigations in the literature led to the following observations.
It was noted in (Polyak, 1987) that, for quadratic costs, stochastic gradient implementa-
tions with a momentum term do not necessarily perform well. This work remarks that
although the heavy ball method can lead to faster convergence in the early stages of learn-
ing, it nevertheless converges to a region with worse mean-square-error in comparison to
standard stochastic-gradient (or LMS) iteration. A similar phenomenon is also observed
in (Proakis, 1974; Sharma et al., 1998). However, in the works (Proakis, 1974; Polyak,
1987; Sharma et al., 1998), no claim is made or established about the equivalence between
momentum and standard methods.

Heavy-ball LMS was further studied in the useful works (Roy and Shynk, 1990) and
(Tugay and Tanik, 1989). The reference (Roy and Shynk, 1990) claimed that no signiﬁcant
gain is achieved in convergence speed if both the heavy-ball and standard LMS algorithms
approach the same steady-state MSE performance. Reference (Tugay and Tanik, 1989)
observed that when the step-sizes satisfy relation (7), then heavy-ball LMS is “equivalent”
to standard LMS. However, they assumed Gaussian measurement noise in their data model,
and the notion of “equivalence” in this work is only referring to the fact that the algorithms
have similar starting convergence rates and similar steady-state MSE levels. There was
no analysis in (Tugay and Tanik, 1989) of the behavior of the algorithms during all stages
of learning – see also (Bellanger, 2001). Another useful work is (Wiegerinck et al., 1994),
which considered the heavy-ball stochastic gradient method for general risk, J(w). By
assuming a suﬃciently small step-size, and by transforming the error diﬀerence recursion
into a diﬀerential equation, the work concluded that heavy-ball can be equivalent to the
standard stochastic gradient method asymptotically (i.e., for i large enough). No results
were provided for the earlier stages of learning.

All of these previous works were limited to examining the heavy-ball momentum tech-
nique; none of them considered other forms of acceleration such as Nesterov’s technique
although this latter technique is nowadays widely applied to stochastic gradient learning,
including deep learning (Sutskever et al., 2013; Kahou et al., 2013; Szegedy et al., 2015;
Zareba et al., 2015). The performance of Nesterov’s acceleration with deterministic and
bounded gradient error was examined in (d’Aspremont, 2008; Baes, 2009; Devolder et al.,
2014; Lessard et al., 2016). The source of the inaccuracy in the gradient vector in these
works is either because the gradient was assessed by solving an auxiliary “simpler” opti-
mization problem or because of numerical approximations. Compared to the standard gra-
dient descent implementation, the works by (d’Aspremont, 2008; Baes, 2009; Lessard et al.,
2016) claimed that Nesterov’s acceleration is not robust to the errors in gradient. The
work by (Devolder et al., 2014) also observed that the superiority of Nesterov’s accelera-
tion is no longer absolute when inexact gradients are used, and they further proved that
the performance of Nesterov’s acceleration may be even worse than gradient descent due
to error accumulation. These works assumed bounded errors in the gradient vectors and
focused on the context of deterministic optimization. None of the works examined the
stochastic setting where the gradient error is random in nature and where the assump-
tion of bounded errors are generally unsuitable. We may add that there have also been
analyses of Nesterov’s acceleration for stochastic optimization problems albeit for decaying

5

step-sizes in more recent literature (Hu et al., 2009; Lan, 2012; Ghadimi and Lan, 2012;
Zhong and Kwok, 2014). These works proved that Nesterov’s acceleration can improve the
convergence rate of stochastic gradient descent to some extent. However, we are particularly
interested in the constant step-size case, which enables continuous adaptation and learning
and is regularly used, e.g., in deep learning implementations. There are limited analyses
for this scenario.

It is worth noting that all the literature reviewed in this section focuses either on Heavy-
ball momentum technique or on Nesterov’s acceleration. In our approach, we shall develop
a framework that uniﬁes both techniques and enables uniform performance analysis for
momentum acceleration under stochastic noise.

1.4 Outline of Paper

The outline of the paper is as follows. In Section 2, we introduce some basic assumptions
and review the stochastic gradient method and its convergence properties.
In Section 3
we embed the heavy-ball and Nesterov’s acceleration methods into a uniﬁed momentum
framework, and subsequently establish the mean-square stability and fourth-order stability
of the error moments. Next, we analyze the equivalence between momentum and standard
LMS algorithms in Section 4 and then extend the results to general risk functions in Section
5. In Section 6 we extend the equivalence results into a more general setting with diagonal
step-size matrices. We illustrate our results in Section 7, and in Section 8 we comment on
the stability ranges of standard and momentum stochastic gradient methods.

2. Stochastic Gradient Algorithms

In this section we review the stochastic gradient method and its convergence properties.
We denote the minimizer for problem (1) by wo, i.e., wo ∆= arg minw J(w). We introduce
the following assumption on J(w), which essentially amounts to assuming that J(w) is
strongly-convex with Lipschitz gradient. These conditions are satisﬁed by many problems
of interest, especially when regularization is employed (e.g., mean-square-error risks, logistic
risks, etc.). Under the strong-convexity condition, the minimizer wo is unique.

Assumption 1 (Conditions on risk function) The cost function J(w) is twice diﬀer-
entiable and its Hessian matrix satisﬁes

0 < νIM ≤ ∇2J(w) ≤ δIM ,

(10)

for some positive parameters ν ≤ δ. Condition (10) is equivalent to requiring J(w) to be
ν-strongly convex and for its gradient vector to be δ-Lipschitz, respectively.

(cid:4)

The stochastic-gradient algorithm for seeking wo takes the form (2), with initial condition
w−1. The diﬀerence between the true gradient vector and its approximation is designated
gradient noise and is denoted by:

si(wi−1)

△

= ∇Q(wi−1; θi) − ∇wE[Q(wi−1; θi)].

(11)

6

In order to examine the convergence of the standard and momentum stochastic gradient
methods, it is necessary to introduce some assumptions on the stochastic gradient noise.
Assumptions (12) and (13) below are satisﬁed by important cases of interest, as shown
in (Sayed, 2014a) and (Sayed, 2014b), such as logistic regression and mean-square-error
risks. Let the symbol F i−1 represent the ﬁltration generated by the random process wj for
j ≤ i − 1 (basically, the collection of past history until time i − 1):

F i−1

△

= ﬁltration{w−1, w0, w1, . . . , wi−1}.

Assumption 2 (Conditions on gradient noise) It is assumed that the ﬁrst and second-
order conditional moments of the gradient noise process satisfy the following conditions for
any w ∈ F i−1:

E[si(w)|F i−1] = 0

E[ksi(w)k2|F i−1] ≤ γ2kwo − wk2 + σ2

s

almost surely, for some nonnegative constants γ2 and σ2
s .

(12)

(13)

(cid:4)

Condition (12) essentially requires the gradient noise process to have zero mean, which
amounts to requiring the approximate gradient to correspond to an unbiased construction
for the true gradient. This is a reasonable requirement. Condition (13) requires the size of
the gradient noise (i.e., its mean-square value) to diminish as the iterate w gets closer to the
solution wo. This is again a reasonable requirement since it amounts to expecting the gra-
dient noise to get reduced as the algorithm approaches the minimizer. Under Assumptions
1 and 2, the following conclusion is proven in Lemma 3.1 of (Sayed, 2014a).

Lemma 1 (Second-order stability) Let Assumptions 1 and 2 hold, and consider the
stochastic gradient recursion (2).
suﬃciently small step-sizes µ, it holds for each iteration i = 0, 1, 2, . . . that

Introduce the error vector ewi = wo − wi. Then, for

and, furthermore,

Ekewik2 ≤ (1 − µν)Ekewi−1k2 + µ2σ2

s ,

lim sup

i→∞

Ekewik2 = O(µ).

(15)

(cid:4)

The above lemma characterizes the stability of the second-order moment of the error vector,

step-sizes,

Ekewik2. One useful conclusion that follows from (14) is that, again for suﬃciently small

(14)

(16)

for some constant D — see equation (99). We will exploit this fact in the sequel.

Ekewik2 ≤ D, ∀i ≥ 0

7

We can also examine the the stability of the fourth-order error moment, Ekewik4, which

will be used later in Section 5 to establish the equivalence between the standard and momen-
tum stochastic implementations. For this case, we tighten the assumption on the gradient
noise by replacing the bound in (13) on its second-error moment by a similar bound involv-
ing its fourth-order moment. Again, this assumption is satisﬁed by problems of interest,
such as mean-square-error and logistic risks (Sayed, 2014a,b).

Assumption 3 (Conditions on gradient noise) It is assumed that the ﬁrst and fourth-
order conditional moments of the gradient noise process satisfy the following conditions for
any w ∈ F i−1:

E[si(w)|F i−1] = 0
E[ksi(w)k4|F i−1] ≤ γ4
almost surely, for some nonnegative constants γ4

s,4

4kwo − wk4 + σ4
4 and σ4

s,4.

(17)

(18)

(cid:4)

It is straightforward to check that if Assumption 3 holds, then Assumption 2 will also hold.
The following conclusion is a modiﬁed version of Lemma 3.2 of (Sayed, 2014a).

Lemma 2 (Fourth-order stability) Let the conditions under Assumptions 1 and 3 hold,
and consider the stochastic gradient iteration (2). For suﬃciently small step-sizes, it holds
that

for some constant c, and, furthermore,

Ekewik4 ≤ (1 − µν)i+1Ekew−1k4 + cµ2,

Proof See Appendix A.

lim sup

i→∞

Ekewik4 ≤ O(µ2).

(19)

(20)

3. Momentum Acceleration

In this section, we present a generalized momentum stochastic gradient method, which is a
uniﬁed framework that captures both the heavy-ball and Nesterov’s acceleration methods.
Subsequently, we derive results for its convergence property.

3.1 Momentum Stochastic Gradient Method

Consider the following general form of a stochastic-gradient implementation, with two mo-
mentum parameters β1, β2 ∈ [0, 1):

ψi−1 = wi−1 + β1(wi−1 − wi−2),

wi = ψi−1 − µm∇wQ(ψi−1; θi) + β2(ψi−1−ψi−2),

with initial conditions

w−2 = ψ−2 = initial states,

8

(21)

(22)

(23)

w−1 = w−2 − µm∇wQ(w−2; θ−1),

(24)

where µm is some constant step-size. We refer to this formulation as the momentum stochas-
tic gradient method. 1

When β1 = 0 and β2 = β we recover the heavy-ball algorithm (Polyak, 1964, 1987),
and when β2 = 0 and β1 = β, we recover Nesterov’s algorithm (Nesterov, 2004). We note
that Nesterov’s method has several useful variations that ﬁt diﬀerent scenarios, such as
situations involving smooth but not strongly-convex risks (Nesterov, 1983, 2004) or non-
smooth risks (Nesterov, 2005; Beck and Teboulle, 2009). However, for the case when J(w) is
strongly convex and has Lipschitz continuous gradients, the Nesterov construction reduces to
what is presented above, with a constant momentum parameter. This type of construction
has also been studied in (Lessard et al., 2016; Dieuleveut et al., 2016) and applied in deep
learning implementations (Sutskever et al., 2013; Kahou et al., 2013; Szegedy et al., 2015;
Zareba et al., 2015).

In order to capture both the heavy-ball and Nesterov’s acceleration methods in a uniﬁed

treatment, we will assume that

β1 + β2 = β,

β1β2 = 0,

(25)

for some β ∈ (0, 1) that is not too close to 1, i.e., there exists small constant ǫ > 0 such that
(26)

β ≤ 1 − ǫ.

3.2 Mean-Square Error Stability

In preparation for studying the performance of the momentum stochastic gradient method,
we ﬁrst show in the next result how recursions (21)-(22) can be transformed into a ﬁrst-order
recursion by deﬁning extended state vectors. We introduce the transformation matrices:

Recall ewi = wo − wi and deﬁne the transformed error vectors, each of size 2M × 1:

1

1

V =(cid:20) IM −βIM
(cid:20) bwi
ˇwi (cid:21) ∆= V −1(cid:20)

IM −IM (cid:21) .
1 − β(cid:20) IM −βIM
IM −IM (cid:21) , V −1 =
1 − β(cid:20) ewi − βewi−1
ewi − ewi−1 (cid:21) .
ewi−1 (cid:21) =
ewi
1−β H i−1 #(cid:20) bwi−1
ˇwi−1(cid:21) +

− µm
1−β H i−1

βIM + µmβ ′

1−β H i−1

µmβ ′
1−β H i−1

Lemma 3 (Extended recursion) Under Assumption 1 and condition (25), the momen-
tum stochastic gradient recursion (21)–(22) can be transformed into the following extended
recursion:

(27)

(28)

ˇwi (cid:21) =" IM − µm
(cid:20) bwi

µm

1 − β(cid:20) si(ψi−1)
si(ψi−1) (cid:21) ,

(29)

1. Traditionally, the terminology of a “momentum method” has been used more frequently for the heavy-
ball method, which corresponds to the special case β1 = 0 and β2 = β. Given the uniﬁed description
(21)–(22), we will use this same terminology to refer to both the heavy-ball and Nesterov’s acceleration
methods.

9

where si(ψi−1) is deﬁned according to (11) and

β′

H i−1

where eψi−1 = wo − ψi−1.

Proof See Appendix B.

∆= ββ1 + β2,

∆= Z 1
0 ∇2

wJ(wo − teψi−1)dt,

(30)

(31)

The transformed recursion (29) is important for at least two reasons. First, it is a ﬁrst-order

recursion, which facilitates the convergence analysis of bwi and ˇwi and, subsequently, of the
error vector ewi in view of (28) — see next theorem. Second, as we will explain later, the ﬁrst

row of (29) turns out to be closely related to the standard stochastic gradient iteration; this
relation will play a critical role in establishing the claimed equivalence between momentum
and standard stochastic gradient methods.

The following statement establishes the convergence property of the momentum stochas-
tic gradient algorithm. It shows that recursions (21)–(22) converge exponentially fast to a
small neighborhood around wo with a steady-state error variance that is on the order of
O(µm). Note that in the following theorem the notation a (cid:22) b, for two vectors a and b,
signiﬁes element-wise comparisons.

Theorem 4 (Mean-square stability) Let Assumptions 1 and 2 hold and recall condi-
tions (25) and (26). Consider the momentum stochastic gradient method (21)–(22) and the
extended recursion (29). Then, for suﬃciently small step-sizes µm, it holds that the mean-
square values of the transformed error vectors evolve according to the following recursive
inequality:

where

Ek ˇwik2 (cid:21) (cid:22)(cid:20) a b
(cid:20) Ekbwik2

c d (cid:21)(cid:20) Ekbwi−1k2

Ek ˇwi−1k2 (cid:21) +(cid:20) e
f (cid:21) ,

a = 1 − µmν
d = β + O(µ2

m),

1−β + O(µ2

m),

b = O(µm),
e = O(µ2
m),

c = O(µ2
m)
f = O(µ2
m)

and the coeﬃcient matrix appearing in (32) is stable, namely,

ρ(cid:18)(cid:20) a b

c d (cid:21)(cid:19) < 1.

Furthermore, it follows from (32) that

lim sup

i→∞

and, consequently,

Ekbwik2 = O(µm)

and

lim sup

i→∞

Ek ˇwik2 = O(µ2

m),

lim sup

i→∞

Ekewik2 = O(µm).

10

(32)

(33)

(34)

(35)

(36)

Proof See Appendix C.

Although Ek ˇwik2 = O(µ2
m) in result (35) is shown to hold asymptotically in the statement
of the theorem, it can actually be strengthened and shown to hold for all time instants.
This fact is crucial for our later proof of the equivalence between standard and momentum
stochastic gradient methods.

Corollary 5 (Uniform mean-square bound) Under the same conditions as Theorem 4,
it holds for suﬃciently small step-sizes that
Ek ˇwik2 = O(µ2

∀i = 0, 1, 2, 3, . . .

(37)

m),

where ˇwi is deﬁned in (29).

Proof See Appendix D.

3.3 Stability of Fourth-Order Error Moment

In a manner similar to the treatment in Sec. 2, we can also establish the convergence of the

fourth-order moments of the error vectors, Ekbwik4 and Ekewik4.

Theorem 6 (Fourth-order stability) Let Assumptions 1 and 3 hold and recall condi-
tions (25) and (26). Then, for suﬃciently small step-sizes µm, it holds that

(38)

(39)

(40)

lim sup

i→∞

lim sup

i→∞

lim sup

i→∞

m),

m).

m),

Ek ˇwik4 = O(µ4

Ekbwik4 = O(µ2
Ekewik4 = O(µ2

Proof The derivation follows by extending the argument from Appendix C in a manner
similar to the proof of Theorem 9.2 in (Sayed, 2014a).

Again, result (39) is only shown to hold asymptotically in the statement of the theorem.
In fact, Ek ˇwik4 can also be shown to be bounded for all time instants, as the following
corollary states.

Corollary 7 (Uniform forth-moment bound) Under the same conditions as Theorem 6,
it holds for suﬃciently small step-sizes that
Ek ˇwik4 = O(µ2

∀i = 0, 1, 2, 3, . . .

(41)

m),

Furthermore, in the limit,

Proof See Appendix E.

lim sup

i→∞

Ek ˇwik4 = O(µ4

m).

(42)

11

4. Equivalence in the Quadratic Case

In Section 3 we showed the momentum stochastic gradient algorithm (21)–(22) converges ex-
ponentially for suﬃciently small step-sizes. But some important questions remain. Does the
momentum implementation converge faster than the standard stochastic gradient method
(2)? Does the momentum implementation lead to superior steady-state mean-square-

deviation (MSD) performance, measured in terms of the limiting value of Ekewik2? Is

the momentum method generally superior to the standard method when considering both
the convergence rate and MSD performance? In this and the next sections, we answer these
questions in some detail. Before treating the case of general risk functions, J(w), we ex-
amine ﬁrst the special case when J(w) is quadratic in w to illustrate the main conclusions
that will follow.

4.1 Quadratic Risks

We consider mean-square-error risks of the form

J(w) =

1
2

E(cid:16)d(i) − uT
i w(cid:17)2

,

(43)

where d(i) denotes a streaming sequence of zero-mean random variables with variance
d = Ed2(i), and ui ∈ RM denotes a streaming sequence of independent zero-mean random
σ2
vectors with covariance matrix Ru = EuiuT
i > 0. The cross covariance vector between d(i)
and ui is denoted by rdu = Ed(i)ui. The data {d(i), ui} are assumed to be wide-sense
stationary and related via a linear regression model of the form:

d(i) = uT

i wo + v(i),

(44)

for some unknown wo, and where v(i) is a zero-mean white noise process with power σ2
v =
Ev2(i) and assumed independent of uj for all i, j. If we multiply (44) by ui from the left
and take expectations, we ﬁnd that the model parameter wo satisﬁes the normal equations
Ruwo = rdu. The unique solution that minimizes (43) also satisﬁes these same equations.
Therefore, minimizing the quadratic risk (43) enables us to recover the desired wo. This
observation explains why mean-square-error costs are popular in the context of regression
models.

4.2 Adaptation Methods

For the least-mean-squares problem (43), the true gradient vector at any location w is

∇wJ(w) = Ruw − rdu = −Ru(wo − w),

(45)

while the approximate gradient vector constructed from an instantaneous sample realization
is:

Here the loss function is deﬁned by

i w).

∇wQ(w; d(i), ui) = −ui(d(i) − uT
E(cid:16)d(i) − uT
i w(cid:17)2

Q(w; d(i), ui) ∆=

1
2

12

(46)

(47)

The resulting LMS (stochastic-gradient) recursion is given by

and the corresponding gradient noise process is

wi = wi−1 + µui(d(i) − uT

i wi−1)

si(w) = (Ru − uiuT

i )(wo − w) − uiv(i).

(48)

(49)

It can be veriﬁed that this noise process satisﬁes Assumption 2 — see Example 3.3 in (Sayed,

the error recursion that corresponds to the LMS implementation:

2014a). Subtracting wo from both sides of (48), and recalling that ewi = wo − wi, we obtain

ewi = (IM − µRu)ewi−1 + µsi(wi−1),

where µ is some constant step-size. In order to distinguish the variables for LMS from the

variables for the momentum LMS version described below, we replace the notation {wi,ewi}
for LMS by {xi,exi} and keep the notation {wi,ewi} for momentum LMS, i.e., for the LMS

implementation (50) we shall write instead

(50)

(51)

exi = (IM − µRu)exi−1 + µsi(xi−1).

On the other hand, we conclude from (21)–(22) that the momentum LMS recursion will

be given by:

ψi−1 = wi−1 + β1(wi−1 − wi−2),
wi = ψi−1 + µmui(d(i) − uT

i ψi−1) + β2(ψi−1 − ψi−2),

(52)

(53)

Using the transformed recursion (29), we can transform the resulting relation for ewi into:
ˇwi (cid:21) =" IM − µm
(cid:20) bwi

1 − β(cid:20) si(ψi−1)
si(ψi−1) (cid:21) ,

µmβ ′
1−β Ru
βIM + µmβ ′

− µm
1−β Ru

1−β Ru

where the Hessian matrix, H i−1, is independent of the weight iterates and given by Ru for
quadratic risks. It follows from the ﬁrst row that

(54)

µm

1−β Ru #(cid:20) bwi−1
ˇwi−1 (cid:21) +
Ru(cid:19)bwi−1 +

µmβ′
1 − β

Ru ˇwi−1 +

µm
1 − β

bwi = (cid:18)IM −

µm
1 − β

µm
1 − β

si(ψi−1).

(55)

Next, we assume the step-sizes {µ, µm} and the momentum parameter are selected to satisfy
(56)

µ =

.

Since β ∈ [0, 1), this means that µm < µ. Then, recursion (55) becomes

bwi = (IM − µRu)bwi−1 + µβ′Ru ˇwi−1 + µsi(ψi−1).

Comparing (57) with the LMS recursion (51), we ﬁnd that both relations are quite similar,
except that the momentum recursion has an extra driving term dependent on ˇwi−1. How-

ever, recall from (28) that ˇwi−1 = (ewi−2 − ewi−1)/(1 − β), which is the diﬀerence between

13

(57)

two consecutive points generated by momentum LMS. Intuitively, it is not hard to see that
ˇwi−1 is in the order of O(µ), which makes µβ′Ru ˇwi−1 in the order of O(µ2). When the
step-size µ is very small, this O(µ2) term can be ignored. Consequently, the above recur-

sions for bwi andexi should evolve close to each other, which would imply that wi and xi will

evolve close to each other as well. This conclusion can be established formally as follows,
which proves the equivalence between the momentum and standard LMS methods.

Theorem 8 (Equivalence for LMS) Consider the LMS and momentum LMS recursions
(48) and (52)–(53). Assume both algorithms start from the same initial states, namely,
ψ−2 = w−2 = x−1. Suppose conditions (25) and (26) hold, and that the step-sizes {µ, µm}
satisfy (56). Then, it holds for suﬃciently small µ that

Ekwi − xik2 = O(µ2),

∀i = 0, 1, 2, 3, . . .

(58)

Proof See Appendix F.

Theorem 8 establishes that the standard and momentum LMS algorithms are fundamentally
equivalent since their iterates evolve close to each other at all times for suﬃciently small
step-sizes. More interpretation of this result is discussed in Section 5.2.

5. Equivalence in the General Case

We now extend the analysis from quadratic risks to more general risks (such as logistic
risks). The analysis in this case is more demanding because the Hessian matrix of J(w) is
now w−dependent, but the same equivalence conclusion will continue to hold as we proceed
to show.

5.1 Equivalence in the General Case

Note from the momentum recursion (29) that

bwi =(cid:18)IM −

µm
1 − β

H i−1(cid:19)bwi−1 +

µmβ′
1 − β

H i−1 ˇwi−1 +

µm
1 − β

si(ψi−1),

(59)

where H i−1 is deﬁned by (31). In the quadratic case, this matrix was constant and equal
to the covariance matrix, Ru. Here, however, it is time-variant and depends on the error

vector, eψi−1, as well. Likewise, for the standard stochastic gradient iteration (2), we obtain

that the error recursion in the general case is given by:

where we are introducing the matrix

exi = (IM − µRi−1)exi−1 + µsi(xi−1),
Ri−1 =Z 1
wJ(wo − rexi−1)dr
0 ∇2

and exi = wo − xi. Note that H i−1 and Ri−1 are diﬀerent matrices. In contrast, in the

quadratic case, they are both equal to Ru.

14

(60)

(61)

Under the assumed condition (56) relating {µ, µm}, if we subtract (60) from (59) we

obtain:

bwi −exi = (IM − µH i−1)(bwi−1 −exi−1) + µ(Ri−1 − H i−1)exi−1

+ µβ′H i−1 ˇwi−1 + µ[si(ψi−1) − si(xi−1)].

(62)

In the quadratic case, the second term on the right-hand side is zero since Ri−1 = H i−1 =
Ru. It is the presence of this term that makes the analysis more demanding in the general
case.

To examine how close bwi gets to exi for each iteration, we start by noting that
Ekbwi −exik2 = E(cid:13)(cid:13)(IM − µH i−1)(bwi−1 −exi−1) + µ(Ri−1 − H i−1)exi−1 + µβ′H i−1 ˇwi−1(cid:13)(cid:13)2

+ µ2Eksi(ψi−1) − si(xi−1)k2.

(63)

Now, applying a similar derivation to the one used to arrive at (122) in Appendix C, and
the inequality ka + bk2 ≤ 2kak2 + 2kbk2, we can conclude from (63) that

Ekbwi −exik2 ≤ (1 − µν)Ekbwi−1 −exi−1k2 +

+

2µ
ν

2µβ′2δ2

ν

Ek ˇwi−1k2

Ek(Ri−1 − H i−1)exi−1k2 + µ2Eksi(ψi−1) − si(xi−1)k2.

(64)

Using the Cauchy-Schwartz inequality we can bound the cross term as

Ek(Ri−1−H i−1)exi−1k2 ≤ E(kRi−1 − H i−1k2kexi−1k2)≤pEkRi−1 − H i−1k4 Ekexi−1k4. (65)
In the above inequality, the term Ekexi−1k4 can be bounded by using the result of Lemma

2. Therefore, we focus on bounding EkRi−1 − H i−1k4 next. To do so, we need to intro-
duce the following smoothness assumptions on the second and fourth-order moments of the
gradient noise process and on the Hessian matrix of the risk function. These assumptions
hold automatically for important cases of interest, such as least-mean-squares and logistic
regression problems — see Appendix G for the veriﬁcation.

Assumption 4 Consider the iterates ψi−1 and xi−1 that are generated by the momentum
recursion (21) and the stochastic gradient recursion (2). It is assumed that the gradient
noise process satisﬁes:

E[ksi(ψi−1)−si(xi−1)k2|F i−1] ≤ ξ1kψi−1−xi−1k2,
E[ksi(ψi−1)−si(xi−1)k4|F i−1] ≤ ξ2kψi−1−xi−1k4.

for some constants ξ1 and ξ2.

(66)

(67)

(cid:4)

Assumption 5 The Hessian of the risk function J(w) in (1) is Lipschitz continuous, i.e.,
for any two variables w1, w2 ∈ dom J(w), it holds that

k∇2

wJ(w1) − ∇2

wJ(w2)k ≤ κkw1 − w2k.

for some constant κ ≥ 0.

15

(68)

(cid:4)

Using these assumptions, we can now establish two auxiliary results in preparation for the
main equivalence theorem in the general case.

Lemma 9 (Uniform bound) Consider the standard and momentum stochastic gradient
recursions (2) and (21)-(22) and assume they start from the same initial states, namely,
ψ−2 = w−2 = x−1. We continue to assume conditions (25), (26), and (56). Under
Assumptions 1, 3, 4 and for suﬃciently small step-sizes µ, the following two results hold:

lim sup

i→∞

Proof See Appendix H.

Ekeψi −exik4 = O(µ),
Ekeψi −exik4 = O(µ2).

∀i = 0, 1, 2, 3 . . . ,

(69)

(70)

not tight. The reason is that in the derivation in Appendix H we employed a looser bound

Although suﬃcient for our purposes, we remark that the bound (69) for Ekeψi −exik4 is
for the term Ek(Ri−1 − H i−1)exi−1k4 in order to avoid the appearance of higher-order
powers, such as EkRi−1− H i−1k8 and Ekexi−1k8. To avoid this possibility, we employed the
Ek(Ri−1 − H i−1)exi−1k4 ≤ EkRi−1 − H i−1k4kexi−1k4

following bound (using (10) to bound kRi−1k4 and kH i−1k4 and the inequality ka + bk4 ≤
8kak4 + 8kbk4):

(71)
Based on Lemma 9, we can now bound EkRi−1 − H i−1k4, which is what the following
lemma states.

≤ 8E(cid:8)(kRi−1k4 + kH i−1k4)kexi−1k4(cid:9) ≤ 16δ4Ekexi−1k4.

Lemma 10 (Bound on Hessian diﬀerence) Consider the same setting of Lemma 9.
Under Assumptions 1, 3, 5 and for suﬃciently small step-sizes µ, the following two re-
sults hold:

EkRi−1 − H i−1k4 ≤ Cµ, ∀i = 0, 1, 2, 3 . . . ,
EkRi−1 − H i−1k4 = O(µ2).

lim sup

i→∞

(72)

(73)

for some constant C.

Proof See Appendix I.

With the upper bounds of EkRi−1 − H i−1k4 and Ekexi−1k4 established in Lemma 10 and
Lemma 2 respectively, we are able to bound k(Ri−1 − H i−1)exi−1k in (65), which in turn

helps to establish the main equivalence result.

Theorem 11 (Equivalence for general risks) Consider the standard and momentum
stochastic gradient recursions (2) and (21)–(22) and assume they start from the same initial
states, namely, ψ−2 = w−2 = x−1. Suppose conditions (25), (26), and (56) hold. Under
Assumptions 1, 3, 4, and 5, and for suﬃciently small step-size µ, it holds that

Ekewi −exik2 = O(µ3/2),

16

∀i = 0, 1, 2, 3, . . .

(74)

Furthermore, in the limit,

Proof See Appendix J.

lim sup

i→∞

Ekewi −exik2 = O(µ2).

(75)

5.2 Interpretation of Equivalence Result

The result of Theorem 11 shows that, for suﬃciently small step-sizes, the trajectories of
momentum and standard stochastic gradient methods remain within O(µ3/2) from each
other for every i (for quadratic cases the trajectories will remain within O(µ2) as stated in
Theorem 8). This means that these trajectories evolve together for all practical purposes
and, hence, we say that the two implementations are equivalent.

A second useful insight from Theorem 8 is that the momentum method is essentially
equivalent to running a standard stochastic gradient method with a larger step-size (since
µ > µm). This interpretation explains why the momentum method is observed to converge
faster during the transient phase albeit towards a worse MSD level in steady-state than
the standard method. This is because, as is well-known in the adaptive ﬁltering literature
(Sayed, 2008, 2014a) that larger step-sizes for stochastic gradient method do indeed lead to
faster convergence but worse limiting performance.

In addition, Theorem 11 enables us to compute the steady-state MSD performance of the
momentum stochastic gradient method. It is guaranteed by Theorem 11 that momentum
method is equivalent to standard stochastic gradient method with larger step-size, µ =
µm/(1 − β). Therefore, once we compute the MSD performance of the standard stochastic
gradient, according to (Haykin, 2008; Sayed, 2008, 2014a), we will also know the MSD
performance for the momentum method.

Another consequence of the equivalence result is that any beneﬁts that would be ex-
pected from a momentum implementation can be attained by simply using a standard
stochastic gradient implementation with a larger step-size; this is achieved without the
additional computational or memory burden that the momentum method entails.

Besides the theoretical analysis given above, there is an intuitive explanation as to why
the momentum variant leads to worse steady-state performance. While the momentum
terms wi−wi−1 and ψi−ψi−1 can smooth the convergence trajectories, and hence accelerate
the convergence rate, they nevertheless introduce additional noise into the evolution of the
algorithm because all iterates wi and ψi are distorted by perturbations. This fact illustrates
the essential diﬀerence between stochastic methods with constant step-sizes, and stochastic
or deterministic methods with decaying step-sizes:
in the former case, the presence of
gradient noise essentially eliminates the beneﬁts of the momentum term.

5.3 Stochastic Gradient Method with Diminishing Momentum

The above observations suggest one useful technique to retain the advantages of the mo-
mentum implementation by employing a diminishing momentum parameter, β(i), and by
ensuring β(i) → 0 in order not to degrade the limiting performance of the implementa-
tion. By doing so, the momentum term will help accelerate the convergence rate during the

17

transient phase because it will smooth the trajectory. On the other hand, momentum will
not cause degradation in MSD performance because the momentum eﬀect would have died
before the algorithm reaches state-state.

To be more speciﬁc, the proposed construction takes the form

ψi−1 = wi−1 + β1(i)(wi−1 − wi−2),

wi = ψi−1−µ∇wQ(ψi−1; θi)+β2(i)(ψi−1−ψi−2),

(76)

(77)

with the same initial conditions as in (23)–(24). Similar to condition (25), β1(i) and β2(i)
also need to satisfy

β1(i) + β2(i) = β(i),

β1(i)β2(i) = 0.

(78)

The eﬃcacy of (76)–(77) will depend on how the momentum decay, β(i), is selected. A
satisfactory sequence {β(i)} should decay slowly during the initial stages of adaptation so
that the momentum term can induce an acceleration eﬀect. However, the sequence {β(i)}
should also decrease drastically prior to steady-state so that the vanishing momentum term
will not introduce additional gradient noise and degrade performance. One strategy, which
is also employed in the numerical experiments in Section 7, is to design β(i) to decrease in
a stair-wise fashion, namely,

β(i) =



if i ∈ [1, T ]
β0
β0/T α
if i ∈ [T + 1, 2T ]
β0/(2T )α if i ∈ [2T + 1, 3T ]
β0/(3T )α if i ∈ [3T + 1, 4T ]
···

(79)

where the constants β0 ∈ [0, 1), α ∈ (0, 1) and T > 0 determines the width of the stair
steps. Fig. 1 illustrates how β(i) varies when T = 20, β0 = 0.5 and α = 0.4.

6. Diagonal Step-size Matrices

Sometimes it is advantageous to employ separate step-size for the individual entries of the
weight vectors. In this section we comment on how the results from the previous sections
extend to this scenario. First, we note that recursion (2) can be generalized to the following
form, with a diagonal matrix serving as the step-size parameter:

xi = xi−1 − D∇wQ(xi−1; θi), i ≥ 0,

(80)

where D = diag{µ1, µ2, . . . , µM}. Here, we continue to use the letter “x” to refer to the
variable iterates for the standard stochastic gradient descent iteration, while we reserve
the letter “w” for the momentum recursion. We let µmax = max{µ1, . . . , µM}. Similarly,
recursions (21) and (22) can be extended in the following manner:

ψi−1 = wi−1 + B1(wi−1 − wi−2),

wi = ψi−1−Dm∇wQ(ψi−1; θi)+B2(ψi−1−ψi−2),

(81)

(82)

18

 

β(i)
Reference curve

0.5

0.4

e
u
a
v

l

0.3

0.2

0.1

0
 
0

10

20

30

40

iterations

50

60

70

80

Figure 1: β(i) changes with iteration i according to (79), where β0 = 0.5, T = 20 and

α = 0.4. The reference curve is f (i) = 0.5/i0.4.

with initial conditions

w−2 = ψ−2 = initial states,
w−1 = w−2 − Dm∇wQ(w−2; θ−1),

(83)

(84)

M} and B2 = diag{β2
where B1 = diag{β1
M} are momentum coeﬃcient ma-
trices, while Dm is a diagonal step-size matrix for momentum stochastic gradient method.
In a manner similar to (25), we also assume that

1 , . . . , β1

1 , . . . , β2

0 ≤ Bk < IM , k = 1, 2,

B1 + B2 = B,

B1B2 = 0.

(85)

where B = diag{β1, . . . , βM} and 0 < B < IM . In addition, we further assume that B is
not too close to IM , i.e.

B ≤ (1 − ǫ)IM , for some constant ǫ > 0.

(86)

The following results extend Theorems 1, 3, and 4 and they can be established following
similar derivations.

Theorem 1B (Mean-square stability). Let Assumptions 1 and 2 hold and recall con-
ditions (85) and (86). Then, for the momentum stochastic gradient method (81)–(82), it
holds under suﬃciently small step-size µmax that

(87)

(cid:4)
Theorem 3B (Equivalence for quadratic costs). Consider recursions (48) and (52)–
(53) with {µ, µm, β1, β2} replaced by {D, Dm, B1, B2}. Assume they start from the same

lim sup

i→∞

Ekewik2 = O(µmax).

19

initial states, namely, ψ−2 = w−2 = x−1. Suppose further that conditions (85) and (86)
hold, and that the step-sizes matrices {D, Dm} satisfy a relation similar to (56), namely,
(88)

D = (I − B)−1Dm.

Then, it holds under suﬃciently small µmax, that

Ekewi −exik2 = O(µ2

max),

∀i = 0, 1, 2, 3, . . .

(89)

(cid:4)
Theorem 4B (Equivalence for general costs). Consider the stochastic gradient recur-
sion (80) and the momentum stochastic gradient recursions (81)–(82) to solve the general
problem (1). Assume they start from the same initial states, namely, ψ−2 = w−2 = x−1.
Suppose conditions (85), (86), and (88) hold. Under Assumptions 1, 3, 4, and 5, and for
suﬃciently small step-sizes, it holds that

Ekewi −exik2 = O(µ3/2

max),

lim sup

∀i = 0, 1, 2, 3, . . .

Ekewi −exik2 = O(µ2

max).

Furthermore, in the limit,

i→∞

7. Experimental Results

(90)

(91)

(cid:4)

In this section we illustrate the main conclusions by means of computer simulations for both
cases of mean-square-error designs and logistic regression designs. We also run simulations
for algorithm (76)–(77) and verify its advantages in the stochastic context.

7.1 Least Mean-Squares Error Designs

We apply the standard LMS algorithm to (43). To do so, we generate data according to
the linear regression model (44), where wo ∈ R10 is chosen randomly, and ui ∈ R10 is i.i.d
and follows ui ∼ N (0, Λ) where Λ ∈ R10×10 is randomly-generated diagonal matrix with
positive diagonal entries. Besides, v(i) is also i.i.d and follows v(i) ∼ N (0, σ2
s I10), where
σ2
s = 0.01. All results are averaged over 300 random trials. For each trial we generated 800
samples of ui, v(i) and d(i).

We ﬁrst compare the standard and momentum LMS algorithms using µ = µm = 0.01.
The momentum parameter β is set to 0.7. These two methods are illustrated in Fig. 2 with
blue and red curves, respectively. It is seen that the momentum LMS converges faster, but
the MSD performance is much worse. Next we set µm = µ(1 − β) = 0.003 and illustrate
this case with the magenta curve.
It is observed that the magenta and blue curves are
almost indistinguishable, which conﬁrms the equivalence predicted by Theorem 8 for all
time instants. We also illustrate an implementation with a decaying momentum parameter
β(i) by the green curve. In this simulation, we set µm = 0.01 and make β(i) decrease in a
stair-wise fashion: when i ∈ [1, 50], β(i) = 0.7; when i ∈ [51, 100], β(i) = 0.7/(500.3); when
i ∈ [101, 150], β(i) = 0.7/(1000.3); . . .; when i ∈ [751, 800], β(i) = 0.7/(7500.3). With this
decaying β(i), it is seen that the momentum LMS method recovers its faster convergence
rate and attains the same steady-state MSD performance as the LMS implementation.

20

)

B
d
(
D
S
M

5

0

−5

−10

−15

−20

−25

−30

−35

−40
 
0

 

LMS (48) with µ
Momentum (52)−(53) with  µ
=µ
m
Momentum (52)−(53) with  µ
=µ(1−β)
m
Decaying β(i) (76)−(77) with  µ
=µ
m

100

200

300

400

iterations

500

600

700

800

Figure 2: Convergence behavior of standard and momentum LMS algorithms applied to the
mean-square-error design problem (43). Mean-square-deviation (MSD) means
Ekwo − wik2.

7.2 Regularized Logistic Regression

We next consider a regularized logistic regression risk of the form:

where the approximate gradient vector is chosen as

2kwk2 + En ln(cid:2)1 + exp(−γ(i)hT

i w)(cid:3)o

J(w) ∆=

ρ

∇wQ(w; hi, γ(i)) = ρw −

exp(−γ(i)hT
i w)
1 + exp(−γ(i)hT

i w)

γ(i)hi

(92)

(93)

In the simulation, we generate 20000 samples (hi, γ(i)). Among these training points,
10000 feature vectors hi correspond to label γ(i) = 1 and each hi ∼ N (1.5 × 110, Rh) for
some diagonal covariance Rh. The remaining 10000 feature vectors hi correspond to label
γ(i) = −1 and each hi ∼ N (−1.5 × 110, Rh). We set ρ = 0.1. The optimal solution wo is
computed via the classic gradient descent method. All simulation results shown below are
averaged over 300 trials.

Similar to the least-mean-squares error problem, we ﬁrst compare the standard and
momentum stochastic methods using µ = µm = 0.01. The momentum parameter β is set
to 0.5. These two methods are illustrated in Fig. 3 with blue and red curves, respectively.
It is seen that the momentum method converges faster, but the MSD performance is much
worse. Next we set µm = µ(1 − β) = 0.005 and illustrate this case with the magenta
curve. It is observed that the magenta and blue curves are indistinguishable, which conﬁrms
the equivalence predicted by Theorem 11 for all time instants. Again we illustrate an
implementation with a decaying momentum parameter β(i) by the green curve.
In this
simulation, we set µm = 0.01 and make β(i) decrease in a stair-wise manner: when i ∈

21

[1, 200], β(i) = 0.5; when i ∈ [200, 400], β(i) = 0.5/(2000.3); when i ∈ [400, 600], β(i) =
0.5/(4000.3); . . .; when i ∈ [1800, 2000], β(i) = 0.5/(18000.3). With this decaying β(i), it is
seen that the momentum method recovers its faster convergence rate and attains the same
steady-state MSD performance as the stochastic-gradient implementation.

5

0

)

B
d
(
D
S
M

−5

−10

−15

 
0

 

Stochastic−gradient (2) with  µ
Momentum (21)−(22) with  µ=µ
m
Momentum (21)−(22) with  µ
=µ(1−β)
m
Decaying  β(i)  (76)−(77) with  µ
=µ
m

500

1000

iterations

1500

2000

Figure 3: Convergence behaviors of standard and momentum stochastic gradient methods applied

to the logistic regression problem (92).

Next, we test the standard and momentum stochastic methods for regularized logistic
regression problem over a benchmark data set — the Adult Data Set2. The aim of this
dataset is to predict whether a person earns over $50K a year based on census data such as
age, workclass, education, race, etc. The set is divided into 6414 training data and 26147 test
data, and each feature vector has 123 entries. In the simulation, we set µ = 0.01, ρ = 0.1,
and β = 0.5. To check the equivalence of the algorithms, we set µm = (1 − β)µ = 0.005.
In Fig. 4, the curve shows how the accuracy performance, i.e., the percentage of correct
prediction, over the test dataset evolved as the algorithm received more training data3.
The horizontal x-axis indicates the number of training data used.
It is observed that
the momentum and standard stochastic gradient methods cannot be distinguished, which
conﬁrms their equivalence when training the Adult Data Set.

7.3 Further Veriﬁcation of Theorems 8 and 11

In this section we further illustrate the conclusions of Theorems 8 and 11 by checking the
behavior of the iterate diﬀerence, i.e., Ekwi − xik2, between the standard and momentum
stochastic gradient methods.

2. Source:

https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/

or

http://archive.ics.uci.edu/ml/datasets/Adult

3. To smooth the performance curve, we applied the weighted average technique from equation (74) of

(Ying and Sayed, 2015).

22

86

85

84

83

82

81

80

79

78

77

)

%

(
 
y
c
a
r
u
c
c
A

 

 

Stocahstic−gradient (2) with  µ
Momentum (21)−(22) with  µ
=µ*(1−β)
m

84.5

84

83.5

83

 

5000

5500

6000

76
 
0

1000

2000

3000
iterations

4000

5000

6000

Figure 4: Performance accuracy of the standard and momentum stochastic gradient methods ap-

plied to logistic regression classiﬁcation on the adult data test set.

For the least-mean-squares error problem, the selection of ui, v(i), d(i) and β is the
same as in the simulation generated earlier in Subsection 7.1. For some speciﬁc step-size
µ, xi is the iterate generated through LMS recursion (48) with step-size µ, and wi is the
iterate generated momentum LMS recursion (52)–(53) with step-size µm = µ(1 − β). Now
we introduce the maximum diﬀerence:

and the diﬀerence at steady state

dmax(µ) = max

i

Ekwi − xik2

dss(µ) = lim sup

i→∞

Ekwi − xik2.

Note that both dmax(µ) and dss(µ) are related with µ and we will examine how they vary
according to diﬀerent step-sizes. Obviously, since Ekwi − xik2 ≤ dmax(µ), if dmax(µ) is
illustrated to be on the order of O(µ2), then it follows that Ekwi − xik2 = O(µ2) for i ≥ 0.
Similarly, if we can illustrate dss(µ) = O(µ2), then it follows that lim supi→∞
Ekwi− xik2 =
O(µ2).

Note that the fact dmax(µ) = cµ2 for some constant c holds if and only if

dmax(µ)(dB) = 20 log µ + 10 log c,

(94)

where dmax(µ)(dB) = 10 log dmax(µ). Relation (94) can be conﬁrmed with red circle line
in Fig. 5. In this simulation, we choose 8 diﬀerent step-size values {µk}8
k=1, and it can be
veriﬁed that each data pair(cid:16) log µk, dmax(µk)(dB)(cid:17) satisﬁes relation (94). For example, in
the red circle solid line, at µ1 = 10−2 we read dmax(µ1)(dB) = −32dB; while at µ2 = 10−4
we read dmax(µ2)(dB) = −72dB. It can be veriﬁed that

dmax(µ1)(dB) − dmax(µ2)(dB) = 20(log µ1 − log µ2) = 40.

(95)

23

Using a similar argument, the blue square solid line can also implies that dss = O(µ2).

Figure 5 also reveals the order of dmax and dss, with magenta and green dash lines
respectively, for the regularized logistic regression problem from Subsection 7.2. With the
same argument as above, dss(µ) can be conﬁrmed on the order of O(µ2). Now we check the
order of dmax(µ). The fact that dmax(µ) = cµ3/2 holds if and only if

dmax(µ)(dB) = 15 log µ + 10 log c.

According to the above relation, at µ1 = 10−2 and µ2 = 10−4 we should have

dmax(µ1)(dB) − dmax(µ2)(dB) = 15(log µ1 − log µ2) = 30.

(96)

(97)

However, in the triangle magenta dash line we read dmax(µ1) = −30dB while dmax(µ2) =
−66dB and hence

30dB < dmax(µ1)(dB) − dmax(µ2)(dB) < 40dB

Therefore, the order of dmax should be between O(µ3/2) and O(µ2), which still conﬁrms
Theorem 11.

−20

−30

−40

−50

−60

−70

−80

−90

−100

)

B
d
(
e
c
n
e
r
e

f
f
i

d

−110

 

10−2

 

MS: d

ss

MS: d

max

LR: d

ss

LR: d

max

10−3

stepsize

10−4

Figure 5: dmax and dss as a function of the step-size µ. MS stands for mean-square-error and LR

stands for logistic regression.

7.4 Handwritten Digit Recognition

In this subsection we illustrate the conclusions in this work by re-examining the well-known
problem of training a neural network to recognize handwritten digits. We will employ
the MNSIT database 4, which is a classical benchmark dataset of images for segmented

4. http://yann.lecun.com/exdb/mnist/

24

handwritten digits, each with 28 × 28 pixels. There are 60,000 training examples and
10,000 testing examples.
In this experiment we train the data through a two-layer fully-connected neural net-
work. We employ the cross-entropy measure with ℓ2 regularization as a cost objective,
and the ReLU as an activation function. The hidden layer has 100 units, the coeﬃcient of
the ℓ2 regularization term is set to 0.001, and the initial value w−1 is generated by Gaus-
sian distribution with 0.05 standard deviation. To accelerate training, we employ mini-
batch stochastic-gradient learning with batch size equal to 100. First, we apply momentum
stochastic-gradient algorithm to train this neural network. The momentum parameter is set
at β = 0.8, and the initial step-size µm is set to 0.04. To achieve better accuracy, we follow
a common technique (e.g., (Szegedy et al., 2015)) and reduce µm to 0.9µm after every 500
iterations. With the above settings, we attain an accuracy of 97% (see Figure 6), which is
the best performance we can achieve with momentum stochastic gradient method during
the experiment.

However, what is interesting, and somewhat surprising, is that the above 97% accuracy
can also be achieved with stochastic gradient descent itself. According to the step-size
relation µ = µm/(1− β), we set the initial step-size µ of SGD to 0.2. Similar to momentum
method, we also reduce µ to 0.9µ after every 500 iterations for SGD, and hence the relation
µ = µm/(1−β) still holds for each iteration. From Figure 6, we can observe that the accuracy
performance curves of both scenarios, with and without momentum, are indistinguishable
even when the overall risk is not necessarily convex or diﬀerentiable. Both scenarios attain
an accuracy of 97%, which is better than the reference accuracy for this handwritten digit
recognition task5. This result indicates that the performance of momentum SGD can still be
achieved by standard SGD by properly adjusting the step-size according to µ = µm/(1− β).
In fact, apart from the test accuracy for both scenarios being similar, we can further
verify a stronger conclusion that the converged weight matrix, i.e., the parameter w for
standard and momentum SGD are also almost identical. In Figure 7 we present visual rep-
resentations for the 20 weight matrices from the ﬁrst layer for both standard and momentum
SGD training. From this ﬁgure, we observe that the images are almost indistinguishable
by the human eye. The numerical result shows that the average diﬀerence for each entry
between these two weights is around 2.536 × 10−5, which amounts to a 0.4% relative error.

8. Comparison for Larger Step-sizes

According to Theorem 11, the equivalence results between the standard and momentum
stochastic gradient methods hold for suﬃciently small step-sizes µ. When larger values
for µ are used, the O(µ3/2) term is not negligible any longer so that the momentum and
gradient-descent implementations are not equivalent anymore under these conditions. While
in practical implementations small step-sizes are widely employed in order to ensure satis-
factory steady-state MSD performance, one may still wonder how both algorithms compare
to each other under larger step-sizes. For example, it is known that the larger the step-size
value is, the more likely it is that the stochastic-gradient algorithm will become unsta-
ble. Does the addition of momentum help enlarge the stability range and allow for proper
adaptation and learning over a wider range of step-sizes?

5. See, for example, http://yann.lecun.com/exdb/mnist/

25

0.98

0.96

0.94

0.92

0.9

0.88

0.86

0.84

0.82

y
c
a
r
u
c
c
a
 
n
o
i
t
a
c
i
f
i
s
a
C

l

 

 

0.976

0.974

0.972

0.97

0.968

 

24

25

26

27

28

Stochastic−gradient (2) with  µ
Momentum (21)−(22) with  µ
=µ*(1−β)
m

0.8
 
0

5

10

15

Epoch

20

25

30

Figure 6: Classiﬁcation accuracy of the standard and momentum stochastic gradient methods ap-

plied to a two-layer fully-connected neural network on the MNIST test data set.

(a) weights generated through SGD

(b) weights generated through momentum SGD

Figure 7: Comparison between weights generated through standard and momentum SGD.

Unfortunately, the answer to the above question is generally negative. In fact, we can
construct a simple numerical example in which the momentum can hurt the stability range.
This example considers the case of quadratic risks, namely problems of the form (43).
i wo + v(i) where v(i) ∼ N (0, 0.01).
We suppose M = 5, ui ∼ N (0, 0.5I5) and d(i) = uT

26

We compare the convergence of standard LMS and Nesterov’s acceleration method with
ﬁxed parameter β2 = 0 and β = 0.5. Both algorithms are set with the same step-size
µ = µm = 0.4, which is a relatively large step-size. All results are averaged over 1000
random trials. For each trial we generated 200 samples of ui, v(i) and d(i).
In Fig. 8,
it shows that standard LMS converges at µ = 0.4 while momentum LMS diverges, which
indicates that momentum LMS has narrower stability range than standard LMS.

)

B
d
(
D
S
M

60

50

40

30

20

10

0

−10

−20
 
0

LMS (48) with µ
Momentum (52)−(53) with µ
=µ
m

 

20

40

60

iteration

80

100

120

Figure 8: Convergence comparison between standard and momentum LMS algorithms when

µ = µm = 0.4 and β = 0.5.

9. Conclusion

In this paper we carefully analyzed the behavior (convergence and performance) of mo-
mentum stochastic gradient methods in the constant step-size and slow adaptation regime.
The results establish that the momentum method is essentially equivalent to employing the
standard stochastic gradient method with a re-scaled (larger) step-size value. The size of
the re-scaling is determined by the momentum parameter, β. The analysis was carried out
under general conditions and was not limited to quadratic risks, but is also applicable to
broader choices of the risk function. Overall, the conclusions indicate that the well-known
beneﬁts of momentum constructions in the deterministic optimization scenario do not neces-
sarily carry over to the stochastic setting when adaptation becomes necessary and gradient
noise is present. The analysis also suggests a way to retain some of the advantages of the
momentum construction by employing a decaying momentum parameter: one that starts at
a constant level and decays to zero over time. In this way, the enhanced convergence rate
during the initial stages of adaptation is retained without the often-observed degradation
in MSD performance.

27

acknowledgement

This work was supported in part by NSF grants CIF-1524250 and ECCS-1407712, by
DARPA project N66001-14-2-4029, and by a Visiting Professorship from the Leverhulme
Trust, United Kingdom. The authors would like to thank PhD student Chung-Kai Yu for
contributing to Sec. 5.3.

References

Michel Baes. Estimate sequence methods: extensions and approximations. IFOR Internal

report, ETH, Z¨urich, Switzerland, 2009.

A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.

M. Bellanger. Adaptive Digital Filters and Signal Analysis. 2nd Edition, Marcel Dekker,

2001.

D. P. Bertsekas. Nonlinear programming. Athena scientiﬁc, 1999.

L. Bottou. Large-scale machine learning with stochastic gradient descent. In Proc. Inter-
national Conference on Computational Statistics, pages 177–186. Springer, Paris, France,
2010.

O. Bousquet and L. Bottou. The tradeoﬀs of large scale learning. In Proc. Advances in

Neural Information Processing Systems, pages 161–168, Vancouver, Canada, 2008.

V. Cevher, S. Becker, and M. Schmidt. Convex optimization for big data: Scalable, random-
ized, and parallel algorithms for big data analytics. IEEE Signal Processing Magazine,
31(5):32–43, 2014.

A. d’Aspremont. Smooth optimization with approximate gradient. SIAM Journal on Op-

timization, 19(3):1171–1183, 2008.

O. Devolder, F. Glineur, and Y. Nesterov. First-order methods of smooth convex optimiza-

tion with inexact oracle. Mathematical Programming, 146(1-2):37–75, 2014.

A. Dieuleveut, N. Flammarion, and F. Bach. Harder, better, faster, stronger convergence
rates for least-squares regression. Available as arXiv preprint arXiv: 1602.05419, Feb.
2016.

R. Gemulla, E. Nijkamp, P. J. Haas, and Y. Sismanis. Large-scale matrix factorization with
distributed stochastic gradient descent. In Proc. International Conference on Knowledge
Discovery and Data Mining, pages 69–77, Alberta, Canada, 2011.

S. Ghadimi and G. Lan. Optimal stochastic approximation algorithms for strongly convex
stochastic composite optimization I: A generic algorithmic framework. SIAM Journal on
Optimization, 22(4):1469–1492, 2012.

S. S. Haykin. Adaptive Filter Theory. Fourth Edition, Prentice-Hall, NJ, 2008.

28

C. Hu, W. Pan, and J. T. Kwok. Accelerated gradient methods for stochastic optimization
and online learning. In Advances in Neural Information Processing Systems, pages 781–
789, 2009.

S. Kahou, C. Pal, X. Bouthillier, P. Froumenty, and et al. Combining modality speciﬁc
deep neural networks for emotion recognition in video. In Proc. International Conference
on Multimodal Interaction, pages 543–550, Sydney, Australia, 2013.

G. Lan. An optimal method for stochastic composite optimization. Mathematical Program-

ming, 133(1-2):365–397, 2012.

L. Lessard, B. Recht, and A. Packard. Analysis and design of optimization algorithms via

integral quadratic constraints. SIAM Journal on Optimization, 26(1):57–95, 2016.

Y. Nesterov. A method of solving a convex programming problem with convergence rate

O(1/k2). Soviet Mathematics Doklady, 27(2):372–376, 1983.

Y. Nesterov. Introductory Lectures on Convex Optimization. Springer, 2004.

Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming,

103(1):127–152, 2005.

B. T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR

Computational Mathematics and Mathematical Physics, 4(5):1–17, 1964.

B. T. Polyak. Introduction to Optimization. Optimization Software, NY, 1987.

J. G. Proakis. Channel identiﬁcation for high speed digital communications. IEEE Trans-

actions on Automatic Control, 19(6):916–922, 1974.

N. Qian. On the momentum term in gradient descent learning algorithms. Neural networks,

12(1):145–151, 1999.

S. Roy and J. J. Shynk. Analysis of the momentum LMS algorithm. IEEE Transactions on

Acoustics, Speech and Signal Processing, 38(12):2088–2098, 1990.

A. H. Sayed. Adaptive Filters. Wiley, NY, 2008.

A. H. Sayed. Adaptation, learning, and optimization over networks. Foundations and Trends

in Machine Learning, 7(4-5):311–801, 2014a.

A. H. Sayed. Adaptive networks. Proceedings of the IEEE, 102(4):460–497, 2014b.

R. Sharma, W. A. Sethares, and J. A. Bucklew. Analysis of momentum adaptive ﬁltering

algorithms. IEEE Transactions on Signal Processing, 46(5):1430–1434, 1998.

J. J. Shynk and S. Roy. The LMS algorithm with momentum updating. In Proc. IEEE
International Symposium on Circuits and Systems, pages 2651–2654, Espoo, Finland,
June 1988.

29

I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and
momentum in deep learning. In Proc. International Conference on Machine Learning,
pages 1139–1147, Atlanta, USA, 2013.

C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelo, D. Erhan, V. Vanhoucke, and
A. Rabinovich. Going deeper with convolutions. In Proc. IEEE Conference on Computer
Vision and Pattern Recognition, pages 1–9, Boston, USA, June 2015.

S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Academic

Press, NY, 2015.

L. K. Ting, C. F. N. Cowan, and R. F. Woods. Tracking performance of momentum LMS
algorithm for a chirped sinusoidal signal. In Proc. European Signal Processing Conference,
pages 1–4, Tampere, Finland, 2000.

M. A. Tugay and Y. Tanik. Properties of the momentum LMS algorithm. Signal Processing,

18(2):117–127, 1989.

B. Widrow and S. D. Stearns. Adaptive Signal Processing. Prentice-Hall, NJ, 1985.

W. Wiegerinck, A. Komoda, and T. Heskes. Stochastic dynamics of learning with mo-
mentum in neural networks. Journal of Physics A: Mathematical and General, 27(13):
4425–4438, 1994.

B. Ying and A. H. Sayed. Performance limits of online stochastic sub-gradient learning.

submitted for publication. Available as arXiv preprint arXiv:1511.07902, Oct. 2015.

S. Zareba, A. Gonczarek, J. M. Tomczak, and J. ´Swiatek. Accelerated learning for re-
stricted Boltzmann machine with momentum term. In Proc. International Convergence
on Systems Engineering, pages 187–192, Coventry, UK, 2015.

T. Zhang. Solving large scale linear prediction problems using stochastic gradient descent
algorithms. In Proc. International Conference on Machine Learning, page 116, Alberta,
Canada, 2004.

W. Zhong and J. T. Kwok. Accelerated stochastic gradient method for composite regular-
ization. In Proc. International Conference on Artiﬁcial Intelligence and Statistics, pages
1086–1094, Reykjavik, Iceland, 2014.

Appendix A. Proof of Lemma 2

It is shown in Eq. (3.76) of (Sayed, 2014a) that Ekewik4 evolves as follows:
Ekewik4 ≤ (1 − µν)Ekewi−1k4 + a1µ2Ekewi−1k2 + a2µ4,

for some constants a1 and a2. If we iterate (14) we ﬁnd that

Ekewi−1k2 ≤ (1 − µν)i+1Ekew−1k2 + a3µ,

30

(98)

(99)

where a4 = a1Ekew−1k2. Iterating the above inequality we get
Ekewik4
≤ (1−µν)i+1Ekew−1k4 +a2µ4
≤ (1 − µν)i+1Ekew−1k4 + a5µ3 + a6µ2 + a4µ2(i + 1)(1 − µν)i+1,
z(i) ∆= (i + 1)(1 − µν)i+1, ∀i = 0, 1, 2, . . .

iXs=0
(1−µν)s +a1a3µ3

where a5 = a4/ν and a6 = a1a3/ν. Let

iXs=0
(1 − µν)s +a4µ2(i + 1)(1 − µν)i+1
(101)

where a3 = σ2
iteration i = 0, 1, 2, . . .

s /ν. Substituting inequality (99) into (98), we ﬁnd that it holds for each

Ekewik4 ≤ (1 − µν)Ekewi−1k4 + a2µ4 + a1a3µ3 + a4µ2(1 − µν)i+1,

(100)

Since 1− µν < 1, it holds that the sequence z(i) converges to 0. Since convergent sequences
are bounded, it follows that

z(i) ≤ D, ∀i = 0, 1, 2, . . .

(103)

for some constant D. In this way, expression (101) leads to the desired result (19). Fur-
thermore, we conclude from this relation that (20) holds.

Appendix B. Proof of Lemma 3

We substitute the expression for the gradient noise from (11), evaluated at ψi−1, into (22)
to get:

wi = ψi−1 − µm∇wJ(ψi−1) + β2(ψi−1 − ψi−2) − µmsi(ψi−1).

(104)

Let again ewi = wo − wi and eψi = wo − ψi. Subtracting both sides of (104) from wo gives:

(105)

ewi = eψi−1 + µm∇wJ(ψi−1) − β2(ψi−1 − ψi−2) + µmsi(ψi−1).
∇Jw(ψi−1) = −(cid:16)Z 1
= −H i−1eψi−1.
0 ∇2

wJw(wo − teψi−1)dt(cid:17)eψi−1

∆

We now appeal to the mean-value theorem (relation (D.9) in (Sayed, 2014a)) to write

and express the momentum term in the form

Then, expression (105) can be rewritten as

ψi−1 − ψi−2 = ψi−1 − wo + wo − ψi−2 = −eψi−1 +eψi−2.
ewi = (IM + β2IM − µmH i−1)eψi−1 − β2eψi−2 +µmsi(ψi−1).

31

(102)

(106)

(107)

(108)

On the other hand, expression (21) gives

Substituting (109) into (108), we have

eψi−1 = ewi−1 + β1(ewi−1 − ewi−2).

where boldface quantities denote random variables:

ewi = J i−1ewi−1 + Ki−1ewi−2 + Lewi−3 + µmsi(ψi−1),

J i−1 = (1 + β1)(1 + β2)IM − µm(1 + β1)H i−1
Ki−1 = −(β1 + β2 + 2β1β2)IM + µmβ1H i−1 = −βIM + µmβ1H i−1

= (1 + β)IM − µm(1 + β1)H i−1

(25)

L = β1β2 = 0

It follows that we can write the extended relation:

(cid:20) ewi
ewi−1 (cid:21) =(cid:20) J i−1 K i−1
{z

∆
= Bi−1

|

IM

0

(cid:21)
}

(cid:20) ewi−1
ewi−2 (cid:21) + µm(cid:20) si(ψi−1)

0

(cid:21) .

(109)

(110)

(111)

(112)

(113)

(114)

where we are denoting the coeﬃcient matrix by Bi−1, which can be written as the diﬀerence

with

Bi−1

∆

= P − M i−1,

P =(cid:20) (1 + β)IM −βIM

IM

0

(cid:21) , M i−1 =(cid:20) µm(1 + β1)H i−1 −µmβ1H i−1

0

0

(115)

(116)

(cid:21).

The eigenvalue decomposition of P can be easily seen to be given by P = V DV −1, where

1

Therefore, we have

V −1 =

1 − β(cid:20) IM −βIM

IM −IM (cid:21) ,
V =(cid:20) IM −βIM
Bi−1 = V (D − V −1M i−1V )V −1 = V " IM − µm
− µm
1−β H i−1

1−β H i−1

IM −IM (cid:21) , D =(cid:20) IM

0

0

βIM (cid:21) .

(117)

µmβ ′
1−β H i−1

1−β H i−1 # V −1, (118)

βIM + µmβ ′

where

Multiplying both sides of (114) by V −1 from the left and recalling deﬁnition (28), we obtain

β′

∆

= ββ1 + β − β1 = ββ1 + β2.

(119)

ˇwi (cid:21) =" IM − µm
(cid:20) bwi

− µm
1−β H i−1

1−β H i−1

µmβ ′
1−β H i−1

1−β H i−1 #(cid:20) bwi−1
ˇwi−1 (cid:21) +

βIM + µmβ ′

µm

1 − β(cid:20) si(ψi−1)

si(ψi−1) (cid:21) (120)

32

Appendix C. Proof of Theorem 4

From the ﬁrst row of recursion (29) we have

bwi = (cid:18)IM −

µmH i−1

1 − β (cid:19)bwi−1 +

µmβ′H i−1

1 − β

ˇwi−1 +

µm
1 − β

si(ψi−1).

(121)

Let t ∈ (0, 1). Squaring both sides and taking expectations conditioned on F i−1, and using
Jensen’s inequality, we obtain under Assumptions 1 and 2:

1

(a)

µm
1−β

µmβ′
1 − β

µm
1 − β
1

1−t(cid:18)IM −

E[kbwik2|F i−1]
=(cid:13)(cid:13)(cid:13)(cid:13)(cid:18)IM −
≤ (cid:13)(cid:13)(cid:13)(cid:13)(1−t)
1 − t(cid:13)(cid:13)(cid:13)(cid:13)(cid:18)IM −
1 − β(cid:19)2
1 − t(cid:18)1 −
=(cid:18)1 −
1 − β(cid:19) kbwi−1k2 +

H i−1 ˇwi−1(cid:13)(cid:13)(cid:13)(cid:13)
H i−1(cid:19)bwi−1 +
H i−1(cid:19)bwi−1 +t
t(cid:13)(cid:13)(cid:13)(cid:13)
H i−1(cid:19)bwi−1(cid:13)(cid:13)(cid:13)(cid:13)
kbwi−1k2 +
ν(1 − β)k ˇwi−1k2 +

µmβ′
1
1−β
t
µmβ′
1 − β

1
t
µmβ′2δ2

µm
1 − β
µmν

µmν

≤

+

(b)

(c)

1

1

2

≤

µ2
mβ′2δ2
(1 − β)2k ˇwi−1k2 +

2

+

µ2
m

µ2
m

E[ksi(ψi−1)k2|F i−1]
(1−β)2 (γ2keψi−1k2 + σ2
(1 − β)2 (γ2keψi−1k2 + σ2

µ2
m

s )

s )

2

2

+

(1 − β)2
+

H i−1 ˇwi−1(cid:13)(cid:13)(cid:13)(cid:13)
H i−1 ˇwi−1(cid:13)(cid:13)(cid:13)(cid:13)
(1 − β)2 (γ2keψi−1k2 + σ2

µ2
m

µ2
m

(1 − β)2 (γ2keψi−1k2 + σ2

s )

s ).

(122)

where (a) holds because of equation (13) in Assumption (2), (b) holds because νI ≤ H i−1 ≤
δI under Assumption (1), and (c) holds because we selected t = µmν
1−β . Taking expectation
again, we remove the conditioning to ﬁnd:

s ).

µ2
m

µmν

(123)

Ek ˇwi−1k2 +

µmβ′2δ2
ν(1−β)

Furthermore, squaring (109) and using the inequality ka + bk2 ≤ 2kak2 + 2kbk2 we get

1−β(cid:19) Ekbwi−1k2 +
Ekbwik2 ≤(cid:18)1−
(1−β)2 (γ2Ekeψi−1k2 +σ2
keψi−1k2 ≤ 2(1 + β1)2kewi−1k2 + 2β2
1kewi−2k2 ≤ 2(1 + β1)2(kewi−1k2 + kewi−2k2)
= 2(1 + β1)2(cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ewi−1
ewi−2 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13)
= 2(1 + β1)2(cid:13)(cid:13)(cid:13)(cid:13)V V −1(cid:20) ewi−1
ewi−2 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13)
≤ 2(1 + β1)2kV k2(cid:13)(cid:13)(cid:13)(cid:13)(cid:20) bwi−1
ˇwi−1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13)
= 2(1 + β1)2v2(kbwi−1k2 + k ˇwi−1k2),

where we introduced v2 = kV k2. Therefore, under expectation, we conclude that it also
holds:

(124)

2

2

2

Substituting (125) into (123), we get

Ekeψi−1k2 ≤ 2(1 + β1)2v2(Ekbwi−1k2 + Ek ˇwi−1k2).
m(cid:17)Ekbwi−1k2 +

2(1 + β1)2γ2v2

µmν
1 − β

(1 − β)2

Ekbwik2 ≤(cid:16)1 −

µ2

+

33

mσ2
µ2
s
(1 − β)2

(125)

+

ν(1 − β)

+(cid:16) µmβ′2δ2
H i−1bwi−1 +(cid:18)βIM +

Now, let us consider the second row of (29), namely,

2(1 + β1)2γ2v2

(1 − β)2

µ2

m(cid:17)Ek ˇwi−1k2.

(126)

ˇwi = −

µm
1 − β

µmβ′
1 − β

H i−1(cid:19) ˇwi−1 +

µm
1 − β

si(ψi−1).

(127)

As before, squaring and taking expectations of both sides, and using Jensen’s inequality,
we obtain under Assumptions 1 and 2:

Ek ˇwik2

+

2

(a)

1 − β

µmH i−1

ˇwi−1 −
µmβ′H i−1

≤ E(cid:13)(cid:13)(cid:13)(cid:13)β ˇwi−1 +(cid:18) µmβ′H i−1
E(cid:13)(cid:13)(cid:13)(cid:13)
(1−β)3 (cid:17)Ek ˇwi−1k2 +

≤ βEk ˇwi−1k2 +
≤ βEk ˇwi−1k2 +
= (cid:16)β +
mβ′2δ2
where (a) holds since Ekβx + yk2 = Ekβx + (1 − β) 1
stituting (124) into (128), it follows that:

1 − β bwi−1(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)
1−β bwi−1(cid:13)(cid:13)(cid:13)(cid:13)
Ekbwi−1k2 +
Ekbwi−1k2 +

1−β
Ek ˇwi−1k2 +
mδ2
2µ2
(1−β)3

1
1−β
2µ2
mβ′2δ2
(1−β)3

ˇwi−1−
2µ2
mδ2
(1−β)3

µmH i−1

µ2
m

2µ2

2

µ2
m

s )

+

µ2
m

(1 − β)2 (γ2Ekeψi−1k2 + σ2
(1−β)2 (γ2Ekeψi−1k2 +σ2
(1−β)2 (γ2Ekeψi−1k2 +σ2

µ2
m

s )

s )

s ),

(128)

Ekyk2. Sub-

1−β

(1−β)2 (γ2Ekeψi−1k2 +σ2
1−β yk2 ≤ βEkxk2 + 1
(cid:17)Ek ˇwi−1k2 +
(cid:17)Ekbwi−1k2

µ2
mσ2
s
(1 − β)2

2µ2

Ek ˇwik2 ≤(cid:16)β +
mβ′2δ2
(1 − β)3 +
+(cid:16) 2µ2
mδ2
(1 − β)3 +

2µ2

mγ2(1 + β1)2v2

(1 − β)2

2µ2

mγ2(1 + β1)2v2

(1 − β)2

Γ

∆

= (cid:20) a b
c d (cid:21) .

Combing relations (126) and (129) leads to the desired result (32)–(33). Let us now examine
the stability of the 2 × 2 coeﬃcient matrix:

Since the spectral radius of a matrix is upper bounded by its 1-norm, we have that

ρ(Γ) ≤ max(cid:26)1 −

µmν
1 − β

+ O(µ2

m), β + O(µm)(cid:27) .

Since β < 1 and is not close to one (in view of condition (26)), we conclude that a suﬃciently
small step-size value µm exists that ensures ρ(Γ) < 1, in which case Γ will be a stable matrix.
It then follows from (32) that

lim sup

i→∞ (cid:20) Ekbwik2

Ek ˇwik2 (cid:21) ≤ (I2 − Γ)−1(cid:20) e

f (cid:21) =(cid:20) O(1/µm) O(1)

O(µm) O(1)(cid:21)(cid:20) O(µ2

O(µ2

m)(cid:21) =(cid:20) O(µm)
m)(cid:21) ,

O(µ2

m)

34

(129)

(130)

(131)

so that

and

lim sup

i→∞

Ekbwik2 = O(µm),
E(cid:13)(cid:13)(cid:13)(cid:13)(cid:20) ewi
ewi−1 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13)

i→∞

= lim sup

2

lim sup

i→∞

lim sup

i→∞

Ek ˇwik2 = O(µ2

m).

(132)

2

ˇwi (cid:21)(cid:13)(cid:13)(cid:13)(cid:13)
E(cid:13)(cid:13)(cid:13)(cid:13)V (cid:20) bwi
E(cid:13)(cid:13)(cid:13)(cid:13)(cid:20) bwi
ˇwi (cid:21)(cid:13)(cid:13)(cid:13)(cid:13)
(Ekbwik2+ Ek ˇwik2)(cid:19) = O(µm),

2!

≤ v2 lim sup
= v2(cid:18)lim sup

i→∞

i→∞

(133)

(134)

(135)

(136)

(137)

from which we conclude that (36) holds.

Appendix D. Proof of Corollary 5

To simplify the notation, we refer to (32) and introduce the quantities:

zi =(cid:20) Ekbwik2

Ek ˇwik2 (cid:21) , Γ =(cid:20) a b

c d (cid:21) , r =(cid:20) e
f (cid:21) .

Then, relation (32) can be rewritten as

It follows that, in terms of the 1−norm,

zi (cid:22) Γzi−1 + r.

kzik1 ≤ kΓk1kzi−1k1 + krk1,

where

kΓk1 = max(cid:26)1 −

µmν
1 − β

+ c′

1µ2

m, β + c′

2µm(cid:27)

for some constant c′
can choose µm suﬃciently small to satisfy

1 and c′

2. We recall from (26) that β < 1 − ǫ, for some ǫ > 0. Now we

c′
1µm <

ν

2(1 − β)

,

(cid:18)c′

2 +

ν

2(1 − β)(cid:19) µm < ǫ,

which implies that

kΓk1 ≤ 1 −

µmν
1 − β

Then, from (136) we have

+ c′

1µ2

m ≤ 1 −

µmν

2(1 − β)

∆
= ρ1 < 1

kzik1 ≤ ρ1kzi−1k1 + krk1.

35

(138)

(139)

(140)

Iterating (140) gives

kzik1 ≤ ρi+1

1 kz−1k1 + krk1
1 − ρ1

.

(141)

Since krk1 = O(µ2
m) and 1 − ρ1 = O(µm), we get krk1/(1 − ρ1) = O(µm). Moreover, since
kz−1k1 = O(1), we conclude that kzik1 ≤ D for i = 0, 1, 2,··· , where D is some constant.
Accordingly, using
(142)

Ekbwik2 ≤ Ekbwik2 + Ek ˇwik2 = kzik1

we also ﬁnd that Ekbwik2 ≤ D, i = 0, 1, 2, . . .

Ek ˇwik2 ≤(cid:0)β + c1µ2

m(cid:1)Ek ˇwi−1k2 + c2µ2

for some constants c1, c2 and c3. Using β ≤ 1 − ǫ we have

α

∆
= β + c1µ2

m ≤ 1 − ǫ + c1µ2

m = 1 −

1
2

m

m

Ekbwi−1k2 + c3µ2
ǫ(cid:19) .
ǫ +(cid:18)c1µ2

m −

1
2

On the other hand, we know from the second row of (32), or from inequality (129) that

(143)

(144)

(147)

It is clear that we can choose a suﬃciently small µm for the last term between brackets to
become negative, in which case

α ≤ 1 −

1
2

ǫ < 1

(145)

Let ρ2 = 1 − ǫ/2. It follows that
Ek ˇwik2 ≤ ρ2 Ek ˇwi−1k2 + c2µ2
Let c4 = c2D + c3. Then, recursion (146) can be simpliﬁed to
Ek ˇwik2 ≤ ρ2 Ek ˇwi−1k2 + c4µ2
m.

Ekbwi−1k2 + c3µ2

(D)

m

m

Iterating (147) gives

≤ ρ2 Ek ˇwi−1k2 + c2µ2

mD + c3µ2

m. (146)

2

m

Ek ˇwik2 ≤ ρi+1
Ek ˇw−1k2 + c4µ2
∞Xj=0

iXj=0
2  Ek ˇw−1k2 +
c4µ2
m
1 − ρ2 ≤

≤

ρj
2 ≤ ρi+1

ρj+1

2

Ek ˇw−1k2 +

c4µ2
m
1 − ρ2

1

1 − ρ2

Ek ˇw−1k2 +

c4µ2
m
1 − ρ2

.

(148)

To assess the term that depends on the initial state, Ek ˇw−1k2, let us consider the boundary
conditions (23)–(24), Then, from (28) it holds that
w−2 − w−1

µm∇wQ(w−2; θ−1)

(149)

=

=

1 − β

1 − β

ˇw−1 = ew−1 −ew−2

1 − β
m, where

so that Ek ˇw−1k2 = cµ2

Substituting this conclusion into (148) we arrive at conclusion (37).

c = Ek∇wQ(w−2; θ−1)k2/(1 − β)2.

(150)

36

Appendix E. Proof of Corollary 7

Recall (127) that

ˇwi = −

µm
1 − β

H i−1bwi−1 +(cid:18)βIM +

µmβ′
1 − β

H i−1(cid:19) ˇwi−1 +

µm
1 − β

si(ψi−1).

(151)

Now applying the following inequality, for any two vectors {a, b}:

ka + bk4 ≤ kak4 + 3kbk4 + 8kak2kbk2 + 4kak2(aTb)

(152)

we get

2

4

3µ4
m

+

µmβ′
1 − β

H i−1(cid:19) ˇwi−1(cid:13)(cid:13)(cid:13)(cid:13)
H i−1(cid:19) ˇwi−1(cid:13)(cid:13)(cid:13)(cid:13)

µmβ′
1 − β

4

3µ4

+

H i−1(cid:19)ˇwi−1(cid:13)(cid:13)(cid:13)(cid:13)
H i−1(cid:19)ˇwi−1(cid:13)(cid:13)(cid:13)(cid:13)

2

(1 − β)4

E(cid:2)ksi(ψi−1)k4|F i−1(cid:3)
E(cid:2)ksi(ψi−1)k2|F i−1(cid:3)
4keψi−1k4 + σ4
(γ2keψi−1k2 + σ2

(1 − β)4

s,4)

s ).

m(γ4

(153)

We next bound each of the terms that appear on the right-hand side. Using Jensen’s
inequality, the lower and upper bounds on the Hessian matrix from (10), and the inequality
ka + bk4 ≤ 8kak4 + 8kbk4, we have
µmβ′
1 − β

µmH i−1

4

E[k ˇwik4|F i−1]

+

+

8µ2
m

8µ2
m

µmH i−1

µmH i−1

1 − β

1 − β

µmβ′
1 − β

µmβ′
1 − β

−µmH i−1

−µmH i−1

bwi−1+(cid:18)βIM +
bwi−1+(cid:18)βIM +

=(cid:13)(cid:13)(cid:13)(cid:13)−
1 − β bwi−1 +(cid:18)βIM +
(1 − β)2(cid:13)(cid:13)(cid:13)(cid:13)
≤(cid:13)(cid:13)(cid:13)(cid:13)−
1 − β bwi−1 +(cid:18)βIM +
(1 − β)2(cid:13)(cid:13)(cid:13)(cid:13)
1 − β bwi−1 +(cid:18)βIM +
= (cid:13)(cid:13)(cid:13)(cid:13)β ˇwi−1 + (1 − β)(cid:18) µmβ′
≤ βk ˇwi−1k4 + (1 − β)(cid:13)(cid:13)(cid:13)(cid:13)

(1 − β)2
µmβ′
(1 − β)2

≤ βk ˇwi−1k4 +
= (β + p1µ4

(cid:13)(cid:13)(cid:13)(cid:13)−

where

H i−1(cid:19) ˇwi−1(cid:13)(cid:13)(cid:13)(cid:13)

H i−1 ˇwi−1 −

H i−1 ˇwi−1 −

µm

(1 − β)2
µm

(1 − β)2

4

H i−1bwi−1(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)
H i−1bwi−1(cid:13)(cid:13)(cid:13)(cid:13)

4

8µ4

mβ′4δ4
(1 − β)7 k ˇwi−1k4 +
mkbwi−1k4,

m)k ˇwi−1k4 + p2µ4

8µ4

mδ4

(1 − β)7kbwi−1k4

p1 =

8β′4δ4
(1 − β)7

p2 =

8δ4

(1 − β)7

(154)

are constants. Likewise, from a similar argument we get

(cid:13)(cid:13)(cid:13)(cid:13)−

µmH i−1

1 − β bwi−1 +(cid:18)βIM +

µmβ′
1 − β

2

H i−1(cid:19) ˇwi−1(cid:13)(cid:13)(cid:13)(cid:13)

37

≤ (β + p3µ2

m)k ˇwi−1k2 + p4µ2

mkbwi−1k2,

(155)

where

p3 =

2β′2δ2
(1 − β)3

p4 =

2δ2

(1 − β)3

are constants. Moreover, from (124) we have

where p5 = 2(1 + β1)2v2, which also implies that

keψi−1k2 ≤ p5(kbwi−1k2 + k ˇwi−1k2),

keψi−1k4 ≤ 4(1 + β1)4v4(kbwi−1k2 + k ˇwi−1k2)2

≤ 8(1 + β1)4v4(kbwi−1k4 + k ˇwi−1k4) = p6(kbwi−1k4 + k ˇwi−1k4),

where p6 = 8(1 + β1)4v4. Substituting (154), (155), (156) and (157) into (153), we get

(156)

(157)

E[k ˇwik4|F i−1]

≤ (β + p1µ4

+

+

8p5γ2µ2
m

m)k ˇwi−1k4 + p2µ4
(1 − β)2(cid:16)(β + p3µ2
(1 − β)2(cid:16)(β + p3µ2
m)k ˇwi−1k4 + p8µ4

mσ2
s

8µ2

mkbwi−1k4 +

m)k ˇwi−1k2 + p4µ2

m)k ˇwi−1k2 + p4µ2
3σ4

mk ˇwi−1k4

+

3p6γ4

3p6γ4

4 µ4
(1 − β)4

4 µ4
(1 − β)4

mkbwi−1k4
mkbwi−1k2(cid:17)(kbwi−1k2 + k ˇwi−1k2)
mkbwi−1k2(cid:17) +
s,4µ4
m
(1 − β)4 + p9µ2
mk ˇwi−1k2 + p13µ4

3µ4
mσ4
s,4
(1 − β)4
mk ˇwi−1k4 + p10µ4

mkbwi−1k4

(158)

= (β + p7µ4

+ p11µ2

where

mkbwi−1k4 +
mk ˇwi−1k2kbwi−1k2 + p12µ2
3p6γ4
4
(1 − β)4 ,

p8 = p2 +

3p6γ4
4
(1 − β)4 ,

p7 = p1 +

p10 =

mkbwi−1k2,
8p4p5γ2
(1 − β)2 ,

p13 =

8p4σ2
s
(1 − β)2 ,

and p9, p11, p12 are some positive constants such that

8p5γ2µ2

m)

m(β + p3µ2
(1 − β)2

8µ2

mσ2

8p5γ2µ2

m)

8p4p5γ2µ4
m

m(β + p3µ2
(1 − β)2
+

≤ p9µ2
m,
(1 − β)2 ≤ p11µ2
s (β + p3µ2
≤ p12µ2
m.
(1 − β)2

m)

m,

Since

then inequality (158) reduces to

p11µ2

mk ˇwi−1k2kbwi−1k2 ≤

p11
2

µ2

m(k ˇwi−1k4 + kbwi−1k4),

E[k ˇwik4|F i−1] ≤ ak ˇwi−1k4 + bkbwi−1k4 + ck ˇwi−1k2 + dkbwi−1k2 + e,

38

(159)

(160)

(161)

(162)

(163)

where

a = β + O(µ2

m),

b = O(µ2

m),

c = O(µ2

m),

d = O(µ4

m),

e = O(µ4

m)

(164)

Now taking expectation of both sides of (162), we have

E[k ˇwik4] ≤ aE[k ˇwi−1k4] + bE[kbwi−1k4] + cE[k ˇwi−1k2] + dE[kbwi−1k2] + e.

Now, Recall that in (38) we established

(165)

(166)

(167)

Let C = lim supi→∞
From the deﬁnition of the limit superior operation, we know that

Ekbwik4. The above equation states that C exists and C = O(µ2

m).

lim sup

i→∞

m).

Ekbwik4 = O(µ2

lim sup

i→∞

i→∞(cid:18)sup

n≥i

Ekbwik4 = lim

Ekbwnk4(cid:19) .

Let z(i) = supn≥i
a bounded sequence, i.e., z(i) ≤ D1 for all i and for some constant D1.
it holds that supn≥0
and supn≥0
namely, Ek ˇwik2 = O(µ2

Ekbwnk4. Then, we have limi→∞ z(i) = C, which implies that z(i) is
Ekbwnk2 ≤ D2
Ekbwnk4 ≤ D1. Similarly from (35) we obtain supn≥0
Ek ˇwik4 ≤(cid:0)β + q1µ2

Ek ˇwnk2 ≤ D3 for some constant D2 and D3. Combining these facts with (37),
m) for i = 0, 1, 2, . . ., and substituting into (165) we conclude that

m(cid:1) Ek ˇwi−1k4 + q2µ2

for some constants q1 and q2. Following the same arguments used in (144)–(148), there
must exist a constant ρ < 1 such that

In particular,

(168)

m,

Ek ˇwik4 ≤ ρi+1Ek ˇw−1k4 + q2µ2

m

iXj=0

ρj ≤ ρi+1Ek ˇw−1k4 +

q2µ2
m
1 − ρ ≤

1
1 − ρ

Ek ˇw−1k4 +

.

q2µ2
m
1 − ρ
(169)

Recall from (149) that Ek ˇw−1k4 = cµ4
(169) we reach that Ek ˇwik4 = O(µ2

m),∀i ≥ 0.

m, where c = Ek∇wQ(w−2; θ−1)k4. Substituting into

On the other hand, from (35) and (166), the recursion (165) will reduce to

lim sup

i→∞

Ek ˇwik4 ≤ ρ lim sup

i→∞

Ek ˇwi−1k4 + qµ4
m,

where ρ is the same as in (169) and q is some constant. It then follows that

lim sup

i→∞

Ek ˇwik4 ≤

µ4
m = O(µ4

m).

q

1 − ρ

39

(170)

(171)

Appendix F. Proof of Theorem 8

Subtracting (51) and (57) we get

where

si(ψi−1) = (Ru − uiuT

bwi −exi = (IM − µRu)(bwi−1 −exi−1) + µ(si(ψi−1) − si(xi−1)) + µβ′Ru ˇwi−1,
i )exi−1 − uiv(i).
i )(eψi−1 −exi−1) + µβ′Ru ˇwi−1.

bwi −exi =(IM − µRu)(bwi−1 −exi−1) + µ(Ru − uiuT

i )eψi−1 − uiv(i),

si(xi−1) = (Ru − uiuT

Substituting into (172) gives

Now note that in the quadratic case, the Hessian matrix of J(w) is equal to Ru. It follows
that condition (10) is satisﬁed with the identiﬁcations ν = λmin(Ru), δ = λmax(Ru). Let
t ∈ (0, 1). By squaring (174) and taking expectations, and applying Jensen’s inequality, we
obtain

(172)

(173)

(174)

(a)

(b)

≤

1
1 − t

Ekbwi −exik2
≤ E(cid:13)(cid:13)(IM − µRu)(bwi−1 −exi−1) + µβ′Ru ˇwi−1(cid:13)(cid:13)2 + µ2EkRu − uiuT
≤ (1 − µν)Ekbwi−1 −exi−1k2 +

µ2β′2δ2Ek ˇwi−1k2 + b1µ2Ekeψi−1 −exi−1k2
Ek ˇwi−1k2 + b1µ2Ekeψi−1 −exi−1k2,

(1 − µν)2Ekbwi−1 −exi−1k2 +

i k2, and (b)
where (a) holds because of Jensen’s inequality and we let b1 = EkRu − uiuT
holds by choosing t = µν. To bound the last term in the above relation, we use (109) to
note that

i k2Ekeψi−1 −exi−1k2

µβ′2δ2

(175)

1
t

ν

eψi −exi = ewi + β1(ewi −ewi−1) −exi = (ewi −exi) − β1(ewi−1 − ewi)

On the other hand, from (28) we have

(176)

(177)

1

so that

1 − β

bwi −exi =
eψi−exi = bwi−exi +

(ewi −exi) −

β
1 − β

(ewi−1 −exi) = (ewi −exi) −

β
1 − β

(ewi−1 − ewi).

β−β1 +ββ1

1−β

(ewi−1−ewi) = bwi−exi +

β′
1−β

(ewi−1−ewi) = bwi−exi−β′ ˇwi.

(178)

where we used the deﬁnition for β′ from (30) and the deﬁnition for ˇwi from (28). Therefore,
from Jensen’s inequality again, we get

Ekeψi −exik2 ≤ 2Ekbwi −exik2 + 2β′2Ek ˇwik2.

40

(179)

Substituting into (175) gives

Ekbwi −exik2 ≤ (1 − µν + 2b1µ2)Ekbwi−1 −exi−1k2 +(cid:18) µβ′2δ2

+ 2b1β′2µ2(cid:19) Ek ˇwi−1k2.(180)
Recall from Corollary 5 that Ek ˇwik2 ≤ cµ2 for each iteration i = 0, 1, 2, 3, . . .. This fact,
together with the above inequality, leads to

ν

for some constant b2. Note that

Ekbwi −exik2 ≤ (1 − µν + 2b1µ2)Ekbwi−1 −exi−1k2 + b2µ3,

α

∆

= 1 − µν + 2b1µ2 = 1 −

1
2

µν +(cid:18)2b1µ2 −

1
2

µν(cid:19) .

It is clear that we can choose a suﬃciently small µ for the last term between brackets to
become negative, in which case

α ≤ 1 −

1
2

µν < 1.

Let ρ = 1 − µν/2. It then holds that

Recall from the ﬁrst equation in (177) that for i = −1:

Ekbwi −exik2 ≤ ρEkbwi−1 −exi−1k2 + b2µ3.

bw−1 −ex−1 =

1

1 − β

(ew−1 −ex−1) −

β
1 − β

(ew−2 −ex−1).

Now, using the assumption that the momentum and standard recursions started from the
same initial states, w−2 = x−1 and w−1 = w−2 − µm∇wQ(w−2; d(−1), u−1), and recall
µ = µm/(1 − β), then we have

for some constant b3. Furthermore, from (177) we have

Ekewi −exik2 ≤ 2Ekbwi −exik2 +

2β2

(1 − β)2

Ek ˇwik2.

Since we have shown that Ekbwi −exik2 ≤ b3µ2 in (188) and Ek ˇwik2 ≤ cµ2 in (41) for each
iteration i = 0, 1, 2, 3 . . ., and it is obvious that Ekewi −exik2 = Ekwi − xik2, we then arrive

at the desired conclusion (58).

41

(181)

(182)

(183)

(184)

(185)

(186)

(188)

(189)

Therefore, it holds that

bw−1 −ex−1 =

1

1 − β

(187)
where c = Ek∇wQ(w−2; d(−1), u−1)k2. On the other hand, it follows by iterating (184)
that

(ew−1 −ew−2) = µ∇wQ(w−2; d(−1), u−1).
Ekbw−1 −ex−1k2 = cµ2,
iXj=0

ρjb2µ3 ≤ cρi+1µ2 +

b2µ3
1 − ρ

= cρi+1µ2 +

2b2µ2
ν ≤ b3µ2.

Ekbwi −exik2 ≤ ρi+1Ek ˆw−1 −ex−1k2 +

≤ cρi+1µ2 +

2b2µ3
νµ

Appendix G. Verifying Assumptions 4 and 5

Least-mean-squares problem. Consider ﬁrst the mean-squares cost (43). Since in this
case H i−1 = Ri−1 = Ru, we ﬁnd that Assumption 5 holds automatically. With regards to
Assumption 4, at any iteration i, we have

so that, under the assumption of independent and stationary regression vectors,

where ξ1 = EkRu − uiuT

i k2. Similarly,

si(wi−1)−si(xi−1) = (Ru − uiuT

i )(ewi−1 −exi−1).
E[ksi(wi−1) − si(xi−1)k2|F i−1] ≤ ξ1 kewi−1 −exi−1k2,
E[ksi(wi−1) − si(xi−1)k4|F i−1] ≤ ξ2 kewi−1 −exi−1k4,
i w)(cid:3)o,

2kwk2 + En ln(cid:2)1 + exp(−γ(i)hT

J(w)

∆
=

ρ

where ξ2 = EkRu − uiuT
Regularized logistic regression. Consider next the regularized logistic regression risk

i k4. Therefore, Assumption 4 holds.

(190)

(191)

(192)

(193)

(194)

(195)

where hi ∈ RM is a streaming sequence of independent feature vectors with Rh = EhihT
i >
0, and γ(i) ∈ {−1, +1} is a streaming sequence of class labels. We assume the random pro-
cesses {γ(i), hi} are wide-sense stationary. Moreover, ρ > 0 is a regularization parameter.
We ﬁrst verify the feasibility of Assumption 4. Note that the approximate gradient vector
is given by:

[∇wJ(w) = ρw −

exp(−γ(i)hT
i w)
1 + exp(−γ(i)hT

i w)

γ(i)hi

and, hence,

Note that

[∇wJ(ψi−1) − [∇wJ(xi−1)

≤ ρkψi−1 − xi−1k + khik(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

exp(−γ(i)hT
1 + exp(−γ(i)hT

exp(−γihT
1 + exp(−γihT

i ψi−1)
i ψi−1) −

exp(−γ(i)hT
1 + exp(−γ(i)hT

i xi−1)

i xi−1)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

i ψi−1)
i ψi−1) −
exp(−γ(i)hT
[1 + exp(−γ(i)hT
exp(−γ(i)hT
exp(−γ(i)hT
exp(γ(i)hT
i
exp(γ(i)hT
i

2

2

= (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
≤ (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
= (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

exp(−γ(i)hT
1 + exp(−γ(i)hT

i xi−1)

i xi−1)

i xi−1)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
i xi−1)](cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
i xi−1)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

xi−1−ψi−1

xi−1−ψi−1

)

2

2

i ψi−1) − exp(−γ(i)hT
i ψi−1)][1 + exp(−γ(i)hT
i xi−1)

i ψi−1) − exp(−γ(i)hT
i ψi−1) + exp(−γ(i)hT

xi−1−ψi−1

xi−1−ψi−1

) − exp(−γ(i)hT
) + exp(−γ(i)hT

i

i

42

= (cid:12)(cid:12)(cid:12)tanh(cid:16)γ(i)hT

i (xi−1 − ψi−1)/2(cid:17)(cid:12)(cid:12)(cid:12) ≤

1
2khikkψi−1 − xi−1k.

(196)

where in the last inequality we used the property | tanh(y)| ≤ |y|, ∀y ∈ R. Substituting
(196) into (195), we get

k[∇wJ(ψi−1) − [∇wJ(xi−1)k ≤ η1,ikψi−1 − xi−1k,

(197)

On the other hand, it is shown in Eq. (2.20) of (Sayed, 2014a) that the Hessian matrix

where η1,i = ρ + khik2/2 is a random variable.
wJ(w) is upper bounded by δIM , where δ =(cid:0)ρ + λmax(Rh)(cid:1). We conclude from Lemma
∇2
E.3 in the same reference that ∇wJ(w) is Lipschitz continuous with modulus δ, i.e.,

k∇wJ(ψi−1) − ∇wJ (xi−1)k ≤ δkψi−1 − xi−1k.

Combining these results we get

ksi(ψi−1)− si(xi−1)k = (cid:13)(cid:13)(cid:13)[d∇J(ψi−1)−d∇J (xi−1)]−[∇J (ψi−1)−∇J(xi−1)](cid:13)(cid:13)(cid:13)

≤ ηikψi−1 − xi−1k.

(198)

(199)

where ηi = η1,i + δ is a random variable. Since the {hi} are independent feature vectors
and ηi is only related to hi, it follows that

E[ksi(ψi−1)−si(xi−1)k2|F i−1] ≤ ξ1kψi−1−xi−1k2,
E[ksi(ψi−1)−si(xi−1)k4|F i−1] ≤ ξ2kψi−1−xi−1k4,

(200)

(201)

where ξ1 = Eηi

2 and ξ2 = Eηi

4.

Next we check the feasibility of Assumption 5. For simplicity, we write γ instead of

γ(i). It can be veriﬁed that for the cost function J(w) in (193):

i(cid:16)
wJ(w) = ρIM + EnhihT
∇2

Now, for any two variables w1 and w2 we have

Let x1 = −γhT

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

exp(−γhT
[1 + exp(−γhT

i w1)
i w1)]2 −

i w2)

exp(−γhT
[1 + exp(−γhT

i w2)]2(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

43

exp(−γhT
i w)
[1 + exp(−γhT

i w2)

i w)]2(cid:17)o.
i w2)]2(cid:17)o(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
i w2)]2(cid:17)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
i w2)]2(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
)

i w2)

exp(−γhT
[1 + exp(−γhT
exp(−γhT
i w2)
[1 + exp(−γhT
exp(−γhT
[1 + exp(−γhT

k∇2

wJ(w2)k

wJ(w1) − ∇2
= (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
i(cid:16)
EnhihT
exp(−γhT
i w1)
i w1)]2 −
[1 + exp(−γhT
≤ E(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
i(cid:16)
exp(−γhT
i w1)
i w1)]2 −
[1 + exp(−γhT
i(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
≤ E((cid:13)(cid:13)(cid:13)hihT
exp(−γhT
i w1)
i w1)]2 −
[1 + exp(−γhT
i w2. Then,

i w1 and x2 = −γhT

hihT

(202)

(203)

(a)

exp(x1)

exp(x2)

[1 + exp(x1)]2[1 + exp(x2)]2

exp(−x2) − exp(−x1) + exp(x2) − exp(x1)

[1 + exp(x2)]2(cid:13)(cid:13)(cid:13)(cid:13)

[1 + exp(x1)]2 −
exp(x1)[1 + exp(x2)]2 − exp(x2)[1 + exp(x1)]2

=(cid:13)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:13)
=(cid:13)(cid:13)(cid:13)(cid:13)
≤ (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
2(cid:0) exp(−x2) + exp(−x1) + exp(x2) + exp(x1)(cid:1)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
≤(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
+(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
2(cid:0) exp(−x2)+exp(−x1)+exp(x2)+exp(x1)(cid:1)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
2(cid:0) exp(−x2)+exp(−x1)+exp(x2)+exp(x1)(cid:1)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
≤(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
2(cid:0) exp(−x2) + exp(−x1)(cid:1)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
2(cid:0) exp(x2) + exp(x1)(cid:1)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
2(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
2(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

) − exp(− x2−x1
) + exp(− x2−x1

exp(− x2−x1
exp(− x2−x1

exp(−x2) − exp(−x1)

) − exp( x2−x1
) + exp( x2−x1

exp(−x2)−exp(−x1)

+(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

)

exp(x2) − exp(x1)

exp(x2)−exp(x1)

exp( x2−x1
exp( x2−x1

2

=| tanh[(x2 − x1)/2]|,
where (a) holds because of the following two facts:

)

)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

1

(b)
=

1

+

(204)

2

2

2

2

2

2

2

exp(x1)[1 + exp(x2)]2 − exp(x2)[1 + exp(x1)]2

= exp(x1) + exp(x1 + 2x2) − exp(x2) − exp(x2 + 2x1)
= exp(x1+x2)[exp(−x2)+exp(x2)−exp(−x1)−exp(x1)],

and

[1 + exp(x1)]2[1 + exp(x2)]2

=(cid:0)1 + 2 exp(x1) + exp(2x1)(cid:1)(cid:0)1 + 2 exp(x2) + exp(2x2)(cid:1)

≥ 2 exp(x1)+2 exp(x2)+2 exp(x1 + 2x2)+2 exp(x2 + 2x1)
= 2 exp(x1+x2)[exp(−x2)+exp(x2)+exp(−x1)+exp(x1)].

(205)

(206)

In addition, (b) holds if we extract exp(− x1+x2
and numerator of the ﬁrst and second terms respectively.

) and exp( x1+x2

2

2

) from both the denominator

Using the deﬁnitions for x1 and x2, this last expression gives

| tanh[(x2 − x1)/2]| =(cid:12)(cid:12)(cid:12)(cid:12)tanh(cid:18) 1

2

γhT

i (w2 − w1)(cid:19)(cid:12)(cid:12)(cid:12)(cid:12) ≤

wJ(w1) − ∇2

Substituting (207) into (203), we obtain k∇2
κ = EkhihT

i kkhik/2. Therefore, Assumption 5 holds.

1
2khikkw2 − w1k.

(207)

wJ(w2)k ≤ κkw1 − w2k, where

Appendix H. Proof of Lemma 9

We refer to relation (62) and apply inequality (152) to reach

E[kbwi −exik4|F i−1]
=k(I M − µH i−1)(bwi−1 −exi−1) + µ(Ri−1 − H i−1)exi−1 + µβ′H i−1 ˇwi−1k4

44

1

+

t3

2µ2

8µ4

8µ4β′4

(a)
≤

kH i−1 ˇwi−1k4

+ 3µ4E[ksi(ψi−1)− si(xi−1)k4|F i−1] + 8µ2k(IM −µH i−1)(bwi−1−exi−1)
+ µ(Ri−1−H i−1)exi−1 + µβ′H i−1 ˇwi−1k2E[ksi(ψi−1)− si(xi−1)k2|F i−1]
(1−t)3k(IM−µH i−1)(bwi−1−exi−1)k4+
t3 k(Ri−1−H i−1)exi−1k4+
+ 3µ4E[ksi(ψi−1)− si(xi−1)k4|F i−1] + 8µ2(cid:18) 1
kH i−1 ˇwi−1k2(cid:19) E[ksi(ψi−1) − si(xi−1)k2|F i−1]
t k(Ri−1−H i−1)exi−1k2 +
≤ (1 − µν)kbwi−1 −exi−1k4 +
+ 3µ4E[ksi(ψi−1)− si(xi−1)k4|F i−1] + 8µ2(cid:16)(1−µν)kbwi−1−exi−1k2
ν k(Ri−1−H i−1)exi−1k2 +

1 − tk(IM −µH i−1)(bwi−1−exi−1)k2

ν3 k(Ri−1 − H i−1)exi−1k4 +

where (a) holds because the facts that for any a, b, c ∈ Rm,

k ˇwi−1k4

2µβ′2δ2

8µβ′4δ4

2µ2β′2

(b)

8µ

2µ

+

ν3

t

ν

a + t

k ˇwi−1k2(cid:17)E[ksi(ψi−1)− si(xi−1)k2|F i−1], (208)
(b + c)k4
1
t
8
t3kbk4 +

(b + c)k4 =
8
t3kck4,

(1 − t)3kak4 +

1
t3kb + ck4

1
t
ak4 + tk

(209)

1

1
ka + b + ck4 = k(1 − t)
1 − t
1
≤ (1 − t)k
1 − t
(1 − t)3kak4 +

≤

1

and

1
ka + b + ck2 = k(1 − t)
1 − t
1
≤ (1 − t)k
1 − t
1 − tkak2 +
≤

1

a + t

1
t
ak2 + tk
2
t kbk2 +

(b + c)k2
1
(b + c)k2 =
t
2
t kck2,

1

1 − tkak2 +

1
t kb + ck2

In addition, (b) holds by choosing t = µν.

To further simplify inequality (208), we ﬁrst note that

kRi−1 − H i−1k2≤ 2(kRi−1k2 + kH i−1k2) ≤ 4δ2,
kRi−1 − H i−1k4≤ 8(kRi−1k4 + kH i−1k4) ≤ 16δ4.

As a result, we have

(210)

(211)

(212)

8µ

ν3 k(Ri−1 − H i−1)exi−1k4 ≤ q1µkexi−1k4,

ν k(Ri−1 − H i−1)exi−1k2 ≤ q2µkexi−1k2,

(213)

where q1 = 128δ4/ν3 and q2 = 8δ2/ν. On the other hand, from conditions (66)–(67), we
have

2µ

3µ4E[ksi(ψi−1) − si(xi−1)k4|F i−1] ≤ q3µ4keψi−1 −exi−1k4,

45

(214)

8µ2E[ksi(ψi−1) − si(xi−1)k2|F i−1] ≤ q4µ2keψi−1 −exi−1k2,

where q3 = 3ξ2 and q4 = 8ξ1. In addition, from (178) we get

(215)

Combining (214)–(216), we have

where q9 = 8β′4δ4/ν3 and q10 = 2µβ′2δ2/ν. Now note that

for some constants q5, q6, q7, q8. In this way, relation (208) becomes

keψi −exik2 ≤ 2kbwi −exik2 + 2β′2k ˇwik2,
keψi −exik4 ≤ 8kbwi −exik4 + 8β′4k ˇwik4.
3µ4E[ksi(ψi−1) − si(xi−1)k4|F i−1] ≤ q5µ4kbwi−1 −exi−1k4 + q6µ4k ˇwi−1k4,
8µ2E[ksi(ψi−1) − si(xi−1)k2|F i−1] ≤ q7µ2kbwi−1 −exi−1k2 + q8µ2k ˇwi−1k2,
E[kbwi −exik4|F i−1]
≤ (1 − µν)kbwi−1 −exi−1k4 + q1µkexi−1k4 + q9µk ˇwi−1k4 +(cid:2)(1 − µν)kbwi−1 −exi−1k2
+ q10µk ˇwi−1k2 + q2µkexi−1k2(cid:3)(cid:2)q7µ2kbwi−1 −exi−1k2 + q8µ2k ˇwi−1k2(cid:3)
+ q5µ4kbwi−1 −exi−1k4 + q6µ4k ˇwi−1k4,
(cid:2)(1−µν)kbwi−1−exi−1k2+q10µk ˇwi−1k2+q2µkexi−1k2(cid:3)(cid:2)q7µ2kbwi−1 −exi−1k2+q8µ2k ˇwi−1k2(cid:3)
= q7µ2(1 − µν)kbwi−1 −exi−1k4 + q8µ2(1 − µν)kbwi−1 −exi−1k2k ˇwi−1k2
+ q7q10µ3k ˇwi−1k2kbwi−1 −exi−1k2 + q8q10µ3k ˇwi−1k4
+ q2q7µ3kexi−1k2kbwi−1 −exi−1k2 + q2q8µ3kexi−1k2k ˇwi−1k2
≤ q11µ2kbwi−1 −exi−1k4 + q12µ2kbwi−1 −exi−1k4 + q12µ2k ˇwi−1k4 + q13µ3k ˇwi−1k4
+ q13µ3kbwi−1 −exi−1k4 + q14µ3k ˇwi−1k4 + q15µ3kexi−1k4
+ q15µ3kbwi−1 −exi−1k4 + q16µ3kexi−1k4 + q16µ3k ˇwi−1k4
≤ q17µ2kbwi−1 −exi−1k4 + q18µ2k ˇwi−1k4 + q19µ3kexi−1k4,

where inequality (a) holds because of the fact that for any a > 0 and b > 0, it holds that
ab < 2ab ≤ a2 + b2. Besides, in (220) q11 and q12 are some positive constants such that

(a)

(216)

(217)

(218)

(219)

(220)

q7µ2(1 − µν) ≤ q11µ2,

q8µ2(1 − µν) ≤ q12µ2,

and

q13 = q7q10,

q14 = q8q10,

q15 = q2q7,

q16 = q2q8,

where q17, q18 and q19 are some positive constants such that

q11µ2 + q12µ2 + q13µ3 ≤ q17µ2,
q12µ2 + q13µ3 + q14µ3 ≤ q18µ2,

q15 + q16 = q19.

46

(221)

(222)

(223)

Now, substituting (220) into (219), we reach

E[kbwi −exik4|F i−1]
≤ (1−µν+q17µ2+q5µ4)kbwi−1−exi−1k4+(q1µ+q19µ3)kexi−1k4+(q9µ+q6µ4+q19µ3)k ˇwi−1k4

≤ (1 −
where q20 and q21 are some positive constants such that

µν)kbwi−1 −exi−1k4 + q20µkexi−1k4 + q21µk ˇwi−1k4.

1
2

(224)

q1µ + q19µ3 ≤ q20µ,

q9µ + q6µ4 + q19µ3 ≤ q21µ.
2 µν.

and µ is suﬃciently small such that 1 − µν + q17µ2 + q5µ4 ≤ 1 − 1

Taking expectations of both sides of (224), we have

E[kbwi −exik4] ≤ (1 −

µν)Ekbwi−1 −exi−1k4 + q20µEkexi−1k4+q21µEk ˇwi−1k4.

Recall from Corollary 7 that Ek ˇwik4 = O(µ2) for i ≥ 0, then there exists some constant q22
such that

1
2

E[kbwi −exik4] ≤ (1 −

Now recall from (19) that

1
2

µν)Ekbwi−1 −exi−1k4 + q20µEkexi−1k4 + q22µ3.

that

where q25 = 2q24/ν. Recall from (186) that bw−1 −ex−1 = µ∇wQ(w−2; θ−1). Then it holds

Ekbwi −exik4 = cµ4,

47

where c is some constant. Let ρ = 1 − 1

Substituting (228) into (226), we get

2 µν, then it holds that

Ekexik4 ≤ (1 − µν)i+1Ekex−1k4 + cµ2,
Ekexik4 ≤ ρi+1Ekex−1k4 + cµ2,

where q23 = q20Ekex−1k4 and q24 = q22 + q20c. If we iterate this recursion we obtain

Ekbwi −exik4 ≤ ρEkbwi−1 −exi−1k4 + q23ρiµ + q24µ3,
Ekbwi −exik4
≤ρi+1Ekbw−1 −ex−1k4 + (i + 1)q23ρiµ + q24µ3
≤ρi+1Ekbw−1 −ex−1k4 + (i + 1)q23ρiµ +
≤ρi+1Ekbw−1 −ex−1k4 + (i + 1)q23ρiµ + q25µ2,

q24µ3
1 − ρ

iXs=0

ρi

(225)

(226)

(227)

(228)

(229)

(230)

(231)

where c = Ek∇wQ(w−2; θ−1)k4. Besides, recall from (103) that (i + 1)ρi ≤ C for each
iteration i. Now substitute the above two facts into relation (230), to conclude that for
i = 0, 1, 2, . . .

Ekbwi −exik4 ≤ cρi+1µ4 + q25µ2 + q23Cµ = O(µ).

In addition, as i → ∞, in (230) we have q23(i + 1)ρiµ → 0 and we conclude that

Furthermore, from (216) and from the fact that Ek ˇwik4 = O(µ2), we conclude from (232)
and (233) that

(232)

(233)

(234)

(235)

(236)

(237)

lim sup

i→∞

Ekbwi −exik4 = q25µ2 = O(µ2).

Ekeψi −exik4 = O(µ),∀i = 0, 1, 2, . . .
Ekeψi −exik4 = O(µ2).

lim sup

i→∞

and

Appendix I. Proof of Lemma 10

Under Assumption 5, we have

kH i−1 − Ri−1k
=(cid:13)(cid:13)(cid:13)(cid:13)Z 1
wJ(wo − rexi−1)dr(cid:13)(cid:13)(cid:13)(cid:13)
wJ(wo − reψi−1)dr −Z 1
0 ∇2
0 ∇2
≤Z 1
wJ(wo − reψi−1) − ∇2
wJ(wo − rexi−1)kdr
0 k∇2
≤Z 1
2keψi−1 −exi−1k.
κrkeψi−1 −exi−1kdr =
EkRi−1 − H i−1k4 ≤ ¯κ Ekeψi−1 −exi−1k4,

κ

0

As a result, it holds that

where ¯κ = κ4/16. Combining this inequality with (234)–(235), we reach the desired bounds

EkRi−1 − H i−1k4 ≤ Cµ, ∀i = 0, 1, 2, . . . .

and

i→∞ kRi−1 − H i−1k4 = O(µ2).
lim sup

48

(238)

(239)

Appendix J. Proof of Theorem 11

For (66) in Assumption 4, if we take expectation over F i−1 of both sides, it holds that

Eksi(ψi−1)−si(xi−1)k2 ≤ ξ1Ekψi−1−xi−1k2.

Combing the above fact and inequalities (64)–(65), we get

for some constant r1. Likewise, from (179) we have

Ekbwi −exik2
≤ (1 − µν)Ekbwi−1 −exi−1k2 + r1µEk ˇwi−1k2

2µ

+

ν pEkRi−1 − H i−1k4Ekexi−1k4 + ξ1µ2Ekeψi−1 −exi−1k2,
Ekeψi −exik2 ≤ 2Ekbwi −exik2 + r2Ek ˇwik2,

for some constant r2. Substituting this second inequality along with (238) into (241) gives

Ekbwi −exik2
≤ (1−µν+2ξ1µ2)Ekbwi−1−exi−1k2+(r1µ+r2ξ1µ2)Ek ˇwi−1k2+r3µ3/2pEkexi−1k4,

where r3 is some constant. Similar to the argument used in (182)–(184), a suﬃciently small
step-size µ exists such that

(243)

(240)

(241)

(242)

(244)

(245)

1 − µν + 2ξ1µ2 < 1 −

µν
2

∆
= ρ.

Using bound (37) that Ek ˇwik2 = O(µ2), expression (243) can be simpliﬁed to

Ekbwi −exik2 ≤ ρEkbwi−1 −exi−1k2 + r3µ3/2pEkexi−1k4 + r4µ3,

where r4 is some constant. Next, using (19) we have

for positive a and b. Substituting (246) into (245), we obtain

pEkexik4 ≤q(1−µν)i+1Ekex−1k4+cµ2 ≤ r5(p1−µν)i+1+r6µ ≤ r5(√ρ)i+1+r6µ.
where r5 =pEkex−1k4 and r6 = √c, and where we applied the inequality √a + b ≤ √a+√b
Ekbwi −exik2 ≤ ρ Ekbwi−1 −exi−1k2 + r3r6µ5/2 + r4µ3 + r3r5µ3/2(√ρ)i
≤ √ρ Ekbwi−1 −exi−1k2 + O(µ5/2) + r3r5µ3/2(√ρ)i.

Since ρ = 1 − µν/2 < 1, it holds that ρ < √ρ, and hence (a) holds in (247). Iterating (247)

(246)

(247)

(a)

gives

Ekbwi −exik2
≤ (√ρ)i+1Ekbw−1 −ex−1k2 + r7µ5/2

49

(√ρ)i + r3r5(i + 1)µ3/2(√ρ)i

iXs=0

≤ (√ρ)i+1Ekbw−1 −ex−1k2 +

r7µ5/2
1 − √ρ

where r7 and r8 are some constants and where we applied the fact that (i + 1)(√ρ)i ≤ D.
Next we check the order of the term r7µ5/2/(1 − √ρ). Since µ is suﬃciently small, we can
approximate √ρ =q1 − 1

4 µν. As a result, it holds that

2 µν = 1 − 1

+ r8µ3/2,

(248)

(249)

(250)

(251)

(252)

(253)

r7µ5/2
1 − √ρ

= O(µ3/2)

This proves the ﬁrst part of Theorem 11. Furthermore, using (20) and (239), we get

Now recall that Ekbw−1 −ex−1k2 = cµ2, ﬁnally we can show that
Ekbwi −exik2 = O(µ3/2), ∀i = 0, 1, 2, . . .
i→∞ pEkRi−1 − H i−1k4Ekexi−1k4 = O(µ2).

lim sup

Using again (242), it then follows from (241) that for suﬃciently small step-sizes:

lim sup

i→∞

which leads to

Ekbwi −exik2 ≤(1 − µν + 2ξ1µ2) lim sup

i→∞

µν

≤ (cid:16)1 −

2 (cid:17) lim sup

i→∞

Ekbwi−1 −exi−1k2 + O(µ3)

Ekbwi−1 −exi−1k2 + O(µ3),

This proves the second part of the Theorem 11.

lim sup

i→∞

Ekbwi −exik2 = O(µ2).

50

