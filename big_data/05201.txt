6
1
0
2

 
r
a

 

M
6
1

 
 
]

G
L
.
s
c
[
 
 

1
v
1
0
2
5
0

.

3
0
6
1
:
v
i
X
r
a

Understanding and Improving Convolutional Neural Networks via

Concatenated Rectiﬁed Linear Units

Wenling Shang1
Kihyuk Sohn2
Diogo Almeida3
Honglak Lee1
1University of Michigan Ann Arbor, 2NEC Laboratories America, 3Enlitic

SHANGW@UMICH.EDU
KSOHN@NEC-LABS.COM
DIOGO@ENLITIC.COM
HONGLAK@EECS.UMICH.EDU

Abstract

Recently, convolutional neural networks (CNNs)
have been used as a powerful tool to solve many
problems of machine learning and computer vi-
sion.
In this paper, we aim to provide insight
on the property of convolutional neural networks,
as well as a generic method to improve the per-
formance of many CNN architectures. Specif-
ically, we ﬁrst examine existing CNN models
and observe an intriguing property that the ﬁlters
in the lower layers form pairs (i.e., ﬁlters with
opposite phase).
Inspired by our observation,
we propose a novel, simple yet effective activa-
tion scheme called concatenated ReLU (CRelu)
and theoretically analyze its reconstruction prop-
erty in CNNs. We integrate CRelu into several
state-of-the-art CNN architectures and demon-
strate improvement in their recognition perfor-
mance on CIFAR-10/100 and ImageNet datasets
with fewer trainable parameters. Our results sug-
gest that better understanding of the properties
of CNNs can lead to signiﬁcant performance im-
provement with a simple modiﬁcation.

1. Introduction
In recent years, convolutional neural networks (CNNs)
have achieved great success in many problems of machine
learning and computer vision (Krizhevsky et al., 2012; Si-
monyan & Zisserman, 2014; Szegedy et al., 2015; Gir-
shick et al., 2014). In addition, a wide range of techniques
has been developed to enhance the performance or ease
the training of CNNs (Lin et al., 2013; Zeiler & Fergus,
2013; Maas et al., 2013; Ioffe & Szegedy, 2015). Despite

Please contact Wenling Shang for questions or comments.

Figure 1. Visualization of conv1 ﬁlters from AlexNet. Each ﬁl-
ter and its pairing ﬁlter (wi and ¯wi next to each other) appear
surprisingly opposite (in phase) to each other. See text for details.

the great empirical success, fundamental understanding of
CNNs is still lagging behind. Towards addressing this is-
sue, this paper aims to provide an insight on the intrinsic
property of convolutional neural networks.
To better comprehend the internal operations of CNNs,
we investigate the well-known AlexNet (Krizhevsky et al.,
2012) and thereafter discover that the network learns highly
negatively-correlated pairs of ﬁlters for the ﬁrst few con-
volution layers (Section 2.1). Following our preliminary
ﬁndings, we hypothesize that the lower convolution layers
of AlexNet learn redundant ﬁlters to extract both positive
and negative phase information of an input signal (Sec-
tion 2.1). Based on the premise of our conjecture, we pro-
pose a novel, simple yet effective activation scheme called
Concatenated Rectiﬁed Linear Unit (CRelu). The pro-
posed activation scheme preserves both positive and nega-
tive phase information while enforcing non-saturated non-
linearity. The unique nature of CRelu allows a mathemat-
ical characterization of convolution layers in terms of re-
construction property, which is an important indicator of
how expressive and generalizable the corresponding CNN
features are (Section 2.2).
In experiments, we evaluate the CNN models with CRelu
in comparison to Relu on benchmark object recognition
datasets, such as CIFAR-10/100 and ImageNet (Section 3).
We demonstrate that simply replacing Relu with CRelu for

Understanding and Improving Convolutional Neural Networks via Concatenated Rectiﬁed Linear Units

(a) conv1

(b) conv2

(e) conv5
Figure 2. Histograms of µr(red) and µw(blue) for AlexNet. Recall that for a set of unit length ﬁlters {φi}, we deﬁne µφ
i = (cid:104)φi, ¯φi(cid:105)
where ¯φi is the pairing ﬁlter of φi. For conv1 layer, the distribution of µw (from the AlexNet ﬁlters) is negatively centered, which totally
differs from that of µr (from random ﬁlters), whose center is very close to zero. The center gradually shifts towards zero when going
deeper into the network.

(c) conv3

(d) conv4

the lower convolution layers of an existing state-of-the-art
CNN architecture yields a substantial improvement in clas-
siﬁcation performance. In addition, CRelu allows to attain
notable parameter reduction without sacriﬁcing classiﬁca-
tion performance when applied appropriately.
We analyze our experimental results from several view-
points, such as regularization (Section 4.1) and invariant
representation learning (Section 4.2). Retrospectively, we
provide empirical evaluations on the reconstruction prop-
erty of CRelu models; we also conﬁrm that by integrating
CRelu, the original “pair-grouping” phenomenon vanishes
as expected (Section 4.3). Overall, our results suggest that
by better understanding the nature of CNNs, we are able to
reach their higher potential with a simple modiﬁcation of
the architecture.
2. CRelu and Reconstruction Property
2.1. Conjecture on Convolution Layers

During our initial exploration of classic CNNs trained on
natural images such as AlexNet (Krizhevsky et al., 2012),
we have noted a curious tendency of the ﬁrst convolu-
tion layer ﬁlters: these ﬁlters tend to form “pairs”. More
precisely, assuming unit length vector for each ﬁlter φi,
we deﬁne a pairing ﬁlter of φi in the following way:
¯φi = argminφj(cid:104)φi, φj(cid:105). We also deﬁne their correlation
i = (cid:104)φi, ¯φi(cid:105).
µφ
In Figure 1, we show each normalized ﬁlter of the ﬁrst con-
volution layer from AlexNet with its pairing ﬁlter. Interest-
ingly, they appear surprisingly opposite to each other, i.e.,
for each ﬁlter, there does exist another ﬁlter that is almost
on the opposite phase. Indeed, AlexNet employs the pop-
ular non-saturated activation function, rectiﬁed linear unit
(Relu) (Nair & Hinton, 2010), which zeros out negative
values and produces sparse activation. As a consequence,
if both the positive phase and negative phase along a spe-
ciﬁc direction participate in representing the input space,
the network then needs to learn two linearly dependent ﬁl-
ters of both phases.
To systematically study the pairing phenomenon in higher

layers as well, we graph the histograms of ¯µw
i ’s for conv1-
conv5 ﬁlters from AlexNet in Figure 2. For comparison,
we generate random Gaussian ﬁlters ri’s of unit norm1 and
i ’s together. For conv1 layer, we
plot the histograms of ¯µr
is negatively centered;
observe that the distribution of ¯µw
i
i is only slightly negative with a
by contrast, the mean of ¯µr
small standard deviation. Then the center of ¯µw
i shifts to-
wards zero gradually when going deeper into the network.
This implies that convolution ﬁlters of the lower layers tend
to be paired up with one or a few others that represent their
opposite phase, while the phenomenon gradually lessens as
they go deeper.
Following these observations, we hypothesize that despite
Relu erasing negative linear responses, the ﬁrst few con-
volution layers of a deep CNN manage to capture both
negative and positive phase information through learning
pairs or groups of negatively correlated ﬁlters. This con-
jecture implies that there exists a redundancy among the
ﬁlters from the lower convolution layers.
In fact, for a very special class of deep architecture, the in-
variant scattering convolutional network (Bruna & Mallat,
2013), it is well-known that its set of convolution ﬁlters,
which are wavelets, is overcomplete in order to be able to
fully recover the original input signals. On the one hand,
similar to Relu, each individual activation within the scat-
tering network only preserves partial information of the in-
put. On the other hand, different from Relu, scattering net-
work activation preserves the energy information, i.e. keep-
ing only the modulus of the convolution responses and eras-
ing the phase information; Relu from a generic CNN, as a
matter of fact, retains the phase information but eliminates
the modulus information when the phase of a response is
negative. In addition, while the wavelets for scattering net-
works are manually engineered, convolution ﬁlters from
CNNs must be learned, which challenges any rigorous the-
oretical justiﬁcation.

1We sample each entry from standard normal distribution in-
dependently and normalize the resulting vector to have unit l2
norm.

-1-0.8-0.6-0.4-0.20-0.7-0.6-0.5-0.4-0.3-0.2-0.10-0.5-0.4-0.3-0.2-0.10-0.4-0.3-0.2-0.10-0.5-0.4-0.3-0.2-0.10Understanding and Improving Convolutional Neural Networks via Concatenated Rectiﬁed Linear Units

Now suppose we can leverage the pairing prior and design
a simple scheme to explicitly allow both positive and nega-
tive activation, we will be able to alleviate the redundancy
among convolution ﬁlters caused by Relu non-linearity and
make more efﬁcient use of the trainable parameters.
We propose a novel activation scheme, Concatenated Rec-
tiﬁed Linear Units, or CRelu, with no additional hyperpa-
rameters. It simply makes an identical copy of the linear
responses after convolution, negate them, concatenate both
parts of activation, and then apply Relu altogether. More
precisely, we denote Relu as [·]+ (cid:44) max(·, 0), and deﬁne
CRelu as the following:
Deﬁnition 2.1. Deﬁne the CRelu activation, denoted by
: R → R2, as the following: ∀x ∈ R, ρc(x) (cid:44)
ρc
([x]+, [−x]+).
The main rationale of our activation scheme is to allow a
ﬁlter to be activated in both positive and negative direction
while maintaining the same degree of non-saturated non-
linearity.
Another way to allow negative activation is to employ the
broader class of non-saturated activation functions includ-
ing Leaky Relu and its variants (Xu et al., 2015; Maas et al.,
2013). Leaky Relu assigns a small slope to the negative
part instead of completely dropping it. These activation
functions share similar motivation with CRelu in the sense
that they both tackle the two potential problems caused by
the hard zero thresholding: (1) the weights of a ﬁlter will
not be adjusted if it is never activated, and (2) truncating
all negative information can potentially hamper the learn-
ing. However, CRelu is an activation scheme rather than
a function, which fundamentally differentiates itself from
Leaky Relu and its variants. In our version, we apply Relu
after separating the negative and positive part to compose
CRelu, but it is not the only feasible non-linearity. For ex-
ample, CRelu can also be combined with other activation
functions, such as Leaky Relu, to add more diversity to the
architecture (possibly with additional tuning).

2.2. Reconstruction Property

A notable property of CRelu is its information preserva-
tion nature: CRelu conserves both negative and positive
linear responses after convolution. A direct consequence
of information preserving is the reconstruction power of
the convolution layers equipped with CRelu.
Reconstruction property of a CNN implies that its features
are representative of the input data. This aspect of CNNs
has gained interest recently: Mahendran & Vedaldi (2015)
invert CNN features back to the input under simple natural
image priors; Zhao et al. (2015) stack autoencoders with
reconstruction objective to build better classiﬁers. Bruna
et al. (2013) theoretically investigate general conditions un-

Figure 3. An illustration of convolution, CRelu, and max-
pooling operation. For simplicity, we describe with 3 convolu-
tion ﬁlters (W1, W2, W3) with stride of s, and with 2× 2 pooling.
In Figure (a), g denotes CRelu followed by the max-pooling op-
eration.

der which the max-pooling layer followed by Relu is in-
jective and measure the stability of the inverting process
by computing the Lipschitz lower bound. However, their
bounds are non-trivial only when the number of ﬁlters sig-
niﬁcantly outnumbers the input dimension, which is not re-
alistic.
In our case, since CRelu preserves all the information af-
ter convolution, it becomes more straightforward to analyze
the reconstruction property. The rest of this section mathe-
matically characterizes the reconstruction property of a sin-
gle convolution layer followed by CRelu with or without
max-pooling layer.
Let x ∈ RD be an input vector2 and wi ∈ R(cid:96), i = 1, . . . , K
i ∈ RD the jth coordi-
be convolution ﬁlters. We denote wj
nate shift of the convolution ﬁlter wi with a ﬁxed stride
i [(j − 1)s + k] = wi[k] for k = 1, . . . , (cid:96),
length of s, i.e., wj
and 0’s for the rest of entries in the vector. Here, we assume
D − (cid:96) is divisible by s and thus there are n = D−(cid:96)
s + 1
shifts for each wi. We deﬁne W to be the D × nK ma-
trix whose columns are the shifts wj
i , j = 1, . . . , n, for wi;
the columns of W are divided into K blocks, with each
block consisting of n shifts of a single ﬁlter. The conv +
CRelu + max-pooling layer can be deﬁned by ﬁrst multi-
plying an input signal x by the matrix W T (conv), sepa-
rating positive and negative phases then applying the Relu
non-linearity (CRelu), and selecting the maximum value
in each of the K block (max-pooling). The operation is de-
where g (cid:44) pool ◦ CRelu. Figure 3 illustrates an example
of the problem setting.
First, we analyze for the case of convolution followed by
CRelu without max-pooling (or equivalently, with max-
pooling and n = 1).
Proposition 2.1. Let x ∈ RD and x = x(cid:48) + (x − x(cid:48)),
where x(cid:48) ∈ range(W ) and x− x(cid:48) ∈ ker(W ). Then we can

noted as fcnn : RD → R2K such that fcnn(x) (cid:44) g(cid:0)W T x(cid:1),

2For the sake of clarity, we assume the input signals are vec-
tors (1D) rather than images (2D); however, similar analysis can
be done for 2D case.

stride size: s 0.9 0.5 0.0 0.0 0.1 -0.7 0.9 0.8 -0.1 0.5 -0.7 -0.2 0.9 0.8 -0.1 0.5 -0.7 -0.2 Understanding and Improving Convolutional Neural Networks via Concatenated Rectiﬁed Linear Units

reconstruct x(cid:48) with fcnn(x).

Proposition A.1 (see Section A in the supplementary mate-
rials for proof) states that the part of an input signal spanned
by the shifts of the ﬁlters is well preserved.
Next, we add max-pooling into the picture. To reach a
non-trivial bound, we put a constraint on the input space
V: ∀x ∈ V, there exists {cj

i}j=1,··· ,n
i=1,··· ,K such that
n(cid:88)

1{cj

i > 0} ≤ 1, ∀i.

(1)

K(cid:88)

n(cid:88)

x =

i wj
cj

i , where

2009). To directly assess the impact of CRelu, we em-
ploy existing CNN architectures with Relu that have al-
ready shown a good recognition performance and demon-
strate improved performance on top by replacing Relu
into CRelu. Note that the models with CRelu activation
doesn’t need signiﬁcant hyperparameter tuning from the
baseline Relu model, and in most of our experiments, we
only tune dropout rate while other hyperparameters (e.g.,
learning rate, mini-batch size) remain the same. We put
details of network architecture and training procedure in
Section E of the supplementary materials.

i=1

j=1

j=1

3.1. CIFAR-10 and CIFAR-100

j=1 1{cj

participates: (cid:80)n

In other words, we assume that an input x is a linear combi-
i}j=1,··· ,n
nation of the shifted convolution ﬁlters {wj
i=1,··· ,K such
that over a single max-pooling region, only one of the shifts
i > 0} ≤ 1: a slight translation of
an object or viewpoint change does not alter the nature of
a natural image, which is how max-pooling generates shift
invariant features by taking away some ﬁne-scaled locality
information.
Next, we denote the matrix consisting of the shifts whose
corresponding cj
i ’s are non-zero by Wx , and the vector
consisting of the non-zero cj
i ’s by cx, i.e. Wxcx = x. Also,
we denote the matrix consisting of the shifts whose activa-
tion is positive and selected after max-pooling operation
. Finally,

(cid:105)
x . Let(cid:99)Wx (cid:44)(cid:104)(cid:99)W +
x ,(cid:99)W −
by(cid:99)W +
we give notation,(cid:102)Wx, to the matrix consisting of a subset
of(cid:99)Wx, such that the ith column comes from(cid:99)W +
or from (cid:99)W −
i ≥ 0
x if otherwise. Now we obtain the following
theorem (see Section A in the supplementary materials for
proof.) that characterizes the reconstruction property of a
convolution layer followed by CRelu and max-pooling.
Theorem 2.2. Let x ∈ V and satisfy the assumption from
Equation (1). Then we can obtain x(cid:48), the reconstruction of
x using fcnn(x) such that

x , negative by(cid:99)W −

x if cj

x

where λmin and(cid:101)λmax are square of the minimum and max-
imum singular values of Wx and(cid:102)Wx respectively.

λmin

We refer to the term (cid:107)x−x(cid:48)(cid:107)2
as the reconstruction ratio in
(cid:107)x(cid:107)2
later sections. We will revisit this subject after the experi-
ment section (Section 4.3).
3. Benchmark Results
We evaluate the effectiveness of our proposed CRelu ac-
tivation scheme on three benchmark datasets: CIFAR-10,
CIFAR-100 (Krizhevsky, 2009) and ImageNet (Deng et al.,

(cid:115)(cid:101)λmax − λmin

,

(cid:107)x − x(cid:48)(cid:107)2

(cid:107)x(cid:107)2

≤

The CIFAR-10 and 100 dataset (Krizhevsky, 2009) consists
of 50, 000 training and 10, 000 testing examples of 32× 32
images evenly drawn from 10 and 100 classes, respectively.
We subtract the mean and divide by the standard deviation
for preprocessing, and for data augmentation, we only use
random horizontal ﬂip.
We use the ConvPool-CNN-C model (Springenberg et al.,
2014) as our baseline model, which is composed of con-
volution and pooling followed by Relu without fully-
connected layers. This baseline model serves our purpose
well since it has clearly outlined network architecture only
with convolution, pooling, and Relu.
It has also shown
competitive recognition performance to state-of-the-art us-
ing a fairly small number of model parameters.
First, we integrate CRelu into the baseline model by sim-
ply replacing Relu while keeping the number of convolu-
tion ﬁlters the same. This doubles the number of output
channels at each convolution layer and the total number
of model parameters is doubled. To see whether the per-
formance gain comes from the increased model capacity,
we conduct additional experiments with the baseline model
while doubling the number of ﬁlters and the CRelu model
while halving the number of ﬁlters.
Since the dataset doesn’t provide pre-deﬁned validation set,
we conduct two different cross-validation schemes:

1. “Single”: we hold out a small subset of training set for
initial training and retrain the network from scratch
using whole training set until we reach at the same
loss on a hold out set (Goodfellow et al., 2013). For
this case, we also report the corresponding train error
rates.

2. 10-folds: we divide training set into 10 folds and do
validation on each of 10 folds while training the net-
works on the rest of 9 folds. The mean error rate of
each single network (“Average”) and the error rate
with model averaging of 10 networks (“Vote”) are
evaluated on the test set.

The recognition results are summarized in Table 1. On

Understanding and Improving Convolutional Neural Networks via Concatenated Rectiﬁed Linear Units

CIFAR-10

CIFAR-100

Model

Baseline
+ (double)

CRelu
+ (half)

Single

train
1.09
0.47
4.23
4.73

test
9.17
8.65
8.43
8.37

Average
10.20±0.09
9.87±0.09
9.39±0.11
9.44±0.09

Vote
7.55
7.28
7.09
7.09

Single

train
13.68
6.03
14.25
21.01

test
36.30
34.77
31.48
33.68

Average
38.52±0.12
36.73±0.15
33.76±0.12
36.20±0.18

Vote
31.26
28.34
27.60
29.93

params.

1.4M
5.6M
2.8M
0.7M

Table 1. Test set recognition error rates on CIFAR-10/100. We compare the performance of Relu models (baseline) and CRelu
models with different model capacities: “double” refers to the models that double the number of ﬁlters and “half” refers to the models
that halves the number of ﬁlters. The error rates are provided in multiple ways, such as “Single”, “Average” (with standard error), or
“Vote”, based on cross-validation methods. We also report the corresponding train error rates for the Single model. The number of
model parameters are given in million. Please see the main text for more details about model evaluation.

CIFAR-10, we observe signiﬁcant improvement with the
proposed CRelu activation over Relu. Especially, CRelu
models consistently improve over Relu models with the
same number of neurons (or activations) while reducing
the number of model parameters by half (e.g., CRelu +
half model and the baseline model have the same num-
ber of neurons while the number of model parameters are
0.7M and 1.4M, respectively). On CIFAR-100, the mod-
els with larger capacity generally improve the performance
for both activation schemes. Nevertheless, we can still ﬁnd
that there is a clear beneﬁt of using CRelu activation that
shows signiﬁcant performance gain when it is compared to
the model with the same number of neurons, i.e., half the
number of model parameters. One possible explanation for
the beneﬁt of using CRelu is its regularization effect, as
can be conﬁrmed in Table 1 that the CRelu models showed
signiﬁcantly lower gap between train and test set error rates
than those of the baseline Relu models.

Experiments on Deeper Networks. Motivated by the
importance of model capacity from experiments on
CIFAR-100, we conduct experiments with very deep CNN
that has a similar network architecture to the VGG net-
work (Simonyan & Zisserman, 2014). Speciﬁcally, we
follow the model architecture and training procedure de-
scribed by Zagoruyko (2015). Besides the convolution and
pooling layers, this network also contains batch normal-
ization (Ioffe & Szegedy, 2015) and fully connected lay-
ers. Due to the sophistication of the network composition
which may introduce complicated interaction with CRelu,
we only integrate CRelu into the ﬁrst few layers. Similarly
to the previous experiments, we subtract the mean and di-
vide by the standard deviation for preprocessing, and for
data augmentation, we use random horizontal ﬂip as well
as random shifts.
In this experiment, we gradually replace Relu after the ﬁrst,
third, and the ﬁfth convolution layers3 with CRelu while
halving the number of ﬁlters, resulting in a reduced num-
ber of model parameters. We report the test set error rates

3Integrating CRelu into the second or fourth layer before

max-pooling layers does not improve the performance.

Model
VGG
(conv1)
(conv1,3)
(conv1,3,5)

Model
VGG
(conv1)
(conv1,3)
(conv1,3,5)

CIFAR-10

Average
6.90±0.03
6.45±0.05
6.45±0.02
6.45±0.07

Single
6.35
6.18
5.94
6.06
CIFAR-100

Single
28.99
27.29
26.52
26.16

Average
30.27±0.09
28.43±0.11
27.79±0.08
27.67±0.07

Vote
5.43
5.22
5.09
5.16

Vote
26.85
24.67
23.93
23.66

Table 2. Test set recognition error rates on CIFAR-10/100 us-
ing deeper networks. We gradually apply CRelu in replace of
Relu after conv1, conv3, and conv5 layers of the baseline VGG
network while halving the number of convolution ﬁlters.

Model

(Rippel et al., 2015)
(Snoek et al., 2015)
(Liang & Hu, 2015)
(Lee et al., 2016)

(Srivastava et al., 2015)

VGG

VGG + CRelu

CIFAR-10

8.60
6.37
7.09
6.05
7.60
5.43
5.09

CIFAR-100

31.60
27.40
31.75
32.37
32.24
26.85
23.66

Table 3. Comparison to other methods on CIFAR-10/100. We
compare the performance of VGG network with CRelu to other
methods.

using the same cross-validation schemes as in the previous
experiments. As shown by Table 2, there is substantial per-
formance lift in both CIFAR-10 and CIFAR-100 datasets
by replacing Relu with CRelu for the ﬁrst few layers af-
ter convolution. Overall, the proposed CRelu activation
improves the performance of the state-of-the-art VGG net-
work signiﬁcantly, achieving highly competitive error rates
to other state-of-the-art methods, as summarized in Table 3.

3.2. ImageNet

To assess the impact of CRelu on large scale dataset, we
perform experiments on ImageNet dataset (Deng et al.,
2009), which contains about 1.3M images for training and
50, 000 for validation from 1, 000 object categories. For
preprocessing, we subtract the mean and divide by the stan-

Understanding and Improving Convolutional Neural Networks via Concatenated Rectiﬁed Linear Units

(a) CIFAR-10

(b) CIFAR-100

(c) ImageNet

Figure 4. Invariance Scores for Relu Models vs CRelu Models. The invariance scores for CRelu models are consistently higher than
Relu models. The invariance scores jump after max-pooling layers and lastly. Moreover, even though the invariance scores tend to
increase along with the depth of the networks, the progression is not monotonic.

Model
Baseline

CRelu (all)

CRelu (conv1,4,7)
CRelu (conv1–4)
CRelu (conv1–7)
CRelu (conv1–9)

top-1
41.81
40.93
40.45
39.82
39.97
40.15

top-5
19.74
19.39
18.58
18.28
18.33
18.58

top-1†
38.03
37.28
35.70
36.20
36.53
36.50

top-5†
17.17
16.72
15.32
15.72
16.01
16.14

Table 4. Validation error rates on ImageNet. We compare the
performance of baseline model with the proposed CRelu models
at different levels of activation scheme replacement. Error rates
with † are obtained by averaging scores from 10 patches.

Model
AlexNet
FriedNet
PrunedNet
AllConvB
CRelu (all)
(conv1,4,7)
(conv1–4)

top-1
42.6
41.93
42.77
41.81
40.93
40.45
39.82

top-5
19.6

–

19.67
19.74
19.39
18.58
18.28

top-1†
40.7

–
–

38.03
37.28
35.70
36.20

top-5†
18.2

–
–

17.17
16.72
15.32
15.72

params.

61M
32.8M
6.7M
9.4M
4.7M
8.6 M
10.1M

Table 5. Comparison to other methods on ImageNet. We com-
pare with AlexNet and other variants, such as FastFood-32-AD
(FriedNet) (Yang et al., 2015) and pruned AlexNet (Pruned-
Net) (Han et al., 2015), which are modiﬁcations of AlexNet aim-
ing at reducing the number of parameters, as well as All-CNN-B,
the baseline model (Springenberg et al., 2014). Error rates with †
are obtained by averaging scores from 10 patches.

dard deviation for each input channel, and follow the data
augmentation as described in (Krizhevsky et al., 2012).
We take the All-CNN-B model (Springenberg et al., 2014)
as our baseline model. The network architecture of All-
CNN-B is similar to that of AlexNet (Krizhevsky et al.,
2012), where the max-pooling layer is replaced by convo-
lution with the same kernel size and stride, the fully con-
nected layer is replaced by 1 × 1 convolution layers fol-
lowed by average pooling, and the local response normal-
ization layers are discarded. In sum, the layers other than
convolution layers are replaced or discarded and ﬁnally the
network consists of convolution layers only. We choose

this model since it reduces the potential complication in-
troduced by CRelu interacting with other types of layers,
such as batch normalization or fully connected layers.
Similarly to the previous experiments, we gradually inte-
grate more convolution layers with CRelu (e.g., conv1–
4, conv1–7, conv1–9), while keeping the same number
of ﬁlters. These models contain more parameters than
the baseline model. We also evaluate two models where
one replaces all Relu layers into CRelu and the other
conv1,conv4 and conv7 only, and both reduce the number
of convolution layers before CRelu by half. Hence, these
models contain fewer parameters than the baseline model.
The network architectures and the training details are in
Section D of supplementary material.
The summarized results are provided in Table 4. We re-
port the top-1 and top-5 error rates with center crop only
and by averaging scores over 10 patches from the center
crop and four corners and with horizontal ﬂip (Krizhevsky
et al., 2012). Interestingly, integrating CRelu to conv1-4
achieves the best results, whereas going deeper with higher
model capacity does not further beneﬁt the classiﬁcation
performance. In fact, this parallels with our initial observa-
tion on AlexNet (Figure 2 in Section 2.1)—there exists less
“pairing” in the deeper convolution layers and thus there is
not much gain by decomposing the phase in the deeper lay-
ers. Another interesting observation, which we will discuss
further in Section 4.2, is that the model integrating CRelu
into conv1, conv4 and conv7 layers also achieve highly
competitive recognition results with even fewer parameters
than the baseline model. In sum, we believe that such a
notable gain over the baseline model by simply modifying
the activation scheme is a pleasantly surprising result.4
We also compare our best models with AlexNet and other
variants in Table 5. Even though reducing the number of

4We note that Springenberg et al. (2014) reported slightly bet-
ter result (41.2% top-1 error rate with center crop only) than our
replication result, but still the improvement is signiﬁcant.

Understanding and Improving Convolutional Neural Networks via Concatenated Rectiﬁed Linear Units

(a) conv1

(b) conv2

(c) conv3

(d) conv4

Figure 5. Histograms of µr(red) and µw(blue) for CRelu model on ImageNet. The two distributions align with each other for all
conv1-conv4 layers–as we expected, the pairing phenomenon is not present any more after applying CRelu activation scheme.

parameters is not our primary goal, it is worth noting that
our model with only 4.6M parameters (CRelu + all) out-
performs FastFood-32-AD (FriedNet) (Yang et al., 2015)
and Pruned AlexNet (PrunedNet) (Han et al., 2015), whose
designs directly aim at parameter reduction. Therefore, be-
sides the performance boost, another signiﬁcance of CRelu
activation scheme is in designing more parameter and com-
putationally efﬁcient deep neural networks.
4. Discussion
In this section, we discuss qualitative properties of CRelu
activation scheme in several viewpoints, such as regulariza-
tion of the network and learning invariant representation.

4.1. A View from Regularization

In general, a model with more trainable parameters is
more prone to overﬁtting. However, somewhat counter-
intuitively, for the all-conv CIFAR experiments, the models
with CRelu display much less overﬁtting issue comparing
to the baseline models with Relu, even though it has twice
more parameters (Table 1). We contemplate that keeping
both positive and negative phase information makes the
training more challenging, and such effect has been lever-
aged to better regularize deep networks, especially when
working on small datasets.
Not only from empirical evidence, but also, we can de-
scribe the regularization effect by deriving a bound for the
Rademacher complexity of the CRelu layer followed by
linear transformation as follows:
Theorem 4.1. Let G be the class of real functions Rdin →
R with input dimension F, that is, G = [F]din
j=1. Let
H be a linear transformation function from R2din to R,
parametrized by W , where (cid:107)W(cid:107)2 ≤ B. Then, we have

ˆRL(H ◦ ρc ◦ G) ≤(cid:112)

dinB ˆRL(F).

The proof is in Section B of the supplementary materials.
Theorem 4.1 says that the complexity bound of CRelu +
linear transformation is the same as that of Relu + linear

transformation, which is proved by Wan et al. (2013). In
other words, although the number of model parameters are
doubled by CRelu, the model complexity does not neces-
sarily increase.

4.2. Towards Learning Invariant Features

We measure the invariance scores using the evaluation met-
rics from (Goodfellow et al., 2009) and draw another com-
parison between the CRelu models and the Relu models.
For a fair evaluation, we compare all 7 conv layers from all-
conv Relu model with those from all-conv CRelu model
trained on CIFAR-10/100; in the case of ImageNet exper-
iments, we choose the model where CRelu replaces Relu
for the ﬁrst 7 conv layers and compare the invariance scores
with the ﬁrst 7 conv layers from the baseline Relu model.
Section C in the supplementary materials details how the
invariance scores are measured.
Figure 4 plots the invariance scores for networks trained on
CIFAR-10, CIFAR-100, and ImageNet respectively. The
invariance scores of CRelu models are consistently higher
than those of Relu models. For CIFAR-10 and CIFAR-100,
there is a big increase between conv2 and conv3 then again
between conv4 and conv6, which are due to max-pooling
layer extracting shift invariance features. We also observe
that although as a general trend, the invariance scores in-
crease while going deeper into the networks–consistent
with the observations from (Goodfellow et al., 2009), rather
unexpectedly, the progression is not monotonic. This in-
teresting observation suggests the potentially diverse func-
tionality of different layers in the CNN, which would be
worthwhile for future investigation.
In particularly, the scores of ImageNet Relu model attain
local maximum at conv1, conv4 and conv7 layers. It in-
spires us to design the architecture where CRelu are placed
after conv1, 4, and 7 layers to encourage invariance rep-
resentations while halving the number of ﬁlters to limit
model capacity. Surprisingly yet as hoped, this architecture
achieves the best top1 and top5 recognition results when
averaging scores from 10 patches.

-0.3-0.25-0.2-0.15-0.1-0.05-0.16-0.14-0.12-0.1-0.08-0.06-0.04-0.02-0.4-0.35-0.3-0.25-0.2-0.15-0.1-0.05-0.14-0.12-0.1-0.08-0.06-0.04-0.02Understanding and Improving Convolutional Neural Networks via Concatenated Rectiﬁed Linear Units

(a) Original image

(b) conv1 recon

(c) conv2 recon

(d) conv3 recon

(e) conv4 recon

Figure 6. CRelu Model Reconstructions. We use the simple linear reconstruction algorithm described by Algorithm 1 to reconstruct
the original image from conv1-conv4 features (left to right). This image is best viewed in color/screen.

layer
conv2
conv5

layer
conv2
conv5

CIFAR-10

learned

0.92 ±0.0002
0.96 ±0.0003

CIFAR-100

learned

0.93 ±0.0002
0.96 ±0.0001

random

0.99 ±0.00005
0.99 ±0.00005

random

0.99 ±0.00005
0.99 ±0.00005

Table 6. Empirical mean of the reconstruction ratios. Recon-
struct the sampled images from test set using the features after
CRelu and max-pooling; then calculate the reconstruction ratio,
(cid:107)x − x(cid:48)(cid:107)2/(cid:107)x(cid:107)2.

4.3. Revisiting the Reconstruction Property

In Section 2.1, we observe that lower layer convolution
ﬁlters from Relu models form negatively-correlated pairs.
Does the pairing phenomenon still exist for CRelu models?
We take our best CRelu model trained on ImageNet (where
the ﬁrst 4 conv layers are integrated with CRelu) and repeat
the histogram experiments to generate Figure 5. In clear
contrast to Figure 2, the distributions of ¯µw
from CRelu
i
model well align with the distributions of ¯µr
i from random
Gaussian ﬁlters. In other words, each lower layer convolu-
tion ﬁlter now uniquely spans its own direction without a
negatively correlated pairing ﬁlter, while CRelu implicitly
plays the role of “pair-grouping”.
In Section 2.2, we mathematically characterize the re-
construction property of convolution layers with CRelu.
Proposition A.1 claims that the part of input spanned by
the shifts of the ﬁlters can be fully recovered. ImageNet
contains a large number of training images from a wide
variety of categories; the convolution ﬁlters learned from
ImageNet are thus expected to be diverse enough to de-
scribe the domain of natural images. Hence, to empirically
verify the result from Proposition A.1, we can directly in-
vert features from our best CRelu model trained on Im-
ageNet via the linear reconstruction algorithm described
in the proof of Proposition A.1 (Algorithm 1). Figure 6
shows an image from the validation set along with its re-
constructions using conv1-conv4 features (see Section F in

the supplementary materials for more reconstruction exam-
ples). Unlike other reconstruction methods (Dosovitskiy
& Brox, 2015; Mahendran & Vedaldi, 2015), our simple
linear reconstruction algorithm does not involve any addi-
tional learning. Nevertheless, it still produces reasonable
reconstructions, which qualitatively supports our theoreti-
cal claim in Proposition A.1.
Theorem 2.2 characterizes the reconstruction property
when max-pooling is added after CRelu. As an exam-
ple, we study the all-conv CRelu (half) models used for
CIFAR-10/100 experiments.
In this model, conv2 and
conv5 layers are followed by max-pooling. CIFAR images
are much less diverse than those from ImageNet. Instead
of directly inverting features all the way back to the origi-
nal images, we empirically calculate the reconstruction ra-
tio, (cid:107)x − x(cid:48)(cid:107)2/(cid:107)x(cid:107)2. We sample testing examples, extract
pooled features after conv2(conv5) layer and reconstruct
features from the previous layer via Algorithm 2. To com-
pare, we perform the same procedures on random convo-
lution ﬁlters5. Essentially, convolution imposes structured

zeros to the random(cid:102)Wx; there has not been published re-

(cid:113) D−K

sults on random subspace projection with such structured
zeros. In a simpliﬁed setting without structured zeros, i.e.
no convolution, it is straightforward to show that the ex-
D (see Theorem A.6
pected reconstruction ratio is
in the supplementary materials), where, in our case, D =
48(96)×5×5 and K = 48(96) for conv2(conv5) layer. Ta-
ble 6 compares between the empirical mean of reconstruc-
tion ratios using learned ﬁlters and random ﬁlters: random
ﬁlters only recover 1% of the original input, whereas the
learned ﬁlters span more of the input domain.
5. Conclusion
We propose a new activation scheme, CRelu, which con-
serves both positive and negative linear responses after
convolution so that each ﬁlter can efﬁciently represent its
unique direction. Our work demonstrates CRelu improves

5Each entry is sampled from standard normal distribution.

Understanding and Improving Convolutional Neural Networks via Concatenated Rectiﬁed Linear Units

deep networks with classiﬁcation objective. Since CRelu
preserves the available information from input yet main-
taining the non-saturated non-linearity at the same time,
it can potentially beneﬁt more complicated machine learn-
ing tasks such as structured output predication and image
generation. Another venue for future research involves en-
gaging CRelu to the abundant set of existing deep neural
network techniques and frameworks.We hope to investigate
along these directions in the near future.

ACKNOWLEDGMENTS

We thank Erik Brinkman, Harry Altman and Mark Rudel-
son for their helpful comments. We also thank Yuting
Zhang and Anna Gilbert for discussions during the prelim-
inary stage of this work. We acknowledge Technicolor Re-
search for providing resources and support and NVIDIA
for the donation of GPUs.

References
Bruna, Joan and Mallat, St´ephane. Invariant scattering con-

volution networks. PAMI, 2013.

Bruna, Joan, Szlam, Arthur, and LeCun, Yann. Signal re-

covery from pooling representations. In ICML, 2013.

Christensen, Ole. An introduction to frames and Riesz

bases. Birkhuser Basel, 2003.

Krizhevsky, Alex. Learning multiple layers of features

from tiny images, 2009.

Krizhevsky, Alex, Sutskever, lya., and Hinton, Geoffrey.
Imagenet classiﬁcation with deep convolutional neural
networks. In NIPS, 2012.

Lee, Cen-yu, Gallagher, Patrick W., and Tu, Zhuowen.
Generalizing pooling functions in convolutional neural
networks: Mixed, gated, and tree. In AISTATS, 2016.

Liang, Ming and Hu, Xiaolin. Recurrent convolutional neu-

ral network for object recognition. In CVPR, 2015.

Lin, Min, Chen, Qiang, and Yan, Shuicheng. Network in

network. In ICLR, 2013.

Maas, Andrew, Hannun, Awni Y., and Ng, Andrew. Recti-
ﬁer nonlinearities improve neural network acoustic mod-
els. In ICML, 2013.

Mahendran, Aravindh and Vedaldi, Andrea. Understanding
deep image representations by inverting them. In CVPR,
2015.

Nair, Vinod and Hinton, Geoffrey. Rectiﬁed linear units
improve restricted boltzmann machines. In ICML, 2010.

Rippel, Oren, Snoek, Jasper, and Adams, Ryan. Spectral
In

representations for convolutional neural networks.
NIPS, 2015.

Deng, Jia, Dong, Wei, Socher, Richard, Li, Li-jia, Li, Kai,
Imagenet: A large-scale hierarchical

and Fei-Fei, Li.
image database. In CVPR, 2009.

Simonyan, Karen and Zisserman, Andrew. Very deep con-
volutional networks for large-scale image recognition. In
ICLR, 2014.

Dosovitskiy, Alexey and Brox, Thomas. Inverting convolu-
tional networks with convolutional networks. In CVPR,
2015.

Girshick, Ross, Donahue, Jeff, Darrell, Trevor, and Malik,
Jitendra. Rich feature hierarchies for accurate object de-
tection and semantic segmentation. In CVPR, 2014.

Goodfellow, Ian, Lee, Honglak, Le, Quoc V., Saxe, An-
drew, and Ng, Andrew. Measuring invariances in deep
networks. In NIPS, 2009.

Goodfellow, Ian, Warde-Farley, David, Mirza, Mehdi,
Courville, Aaron, and Bengio, Yoshua. Maxout net-
works. In ICML, 2013.

Snoek, Jasper, Rippel, Oren, Swersky, Kevin, Kiros, Ryan,
Satish, Nadathur, Sundaram, Narayanan, Patwary, Md.
Mostofa Ali, and Adams, Ryan. Scalable bayesian opti-
mization using deep neural networks. In ICML, 2015.

Springenberg, Jost, Dosovitskiy, Alexey, Brox, Thomas,
and Riedmiller, Martin. Striving for simplicity: The all
convolutional net. In ICLR Workshop, 2014.

Srivastava, Rupesh, Greff, Klaus, and Schmidhuber, Jur-

gen. Training very deep networks. In NIPS, 2015.

Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet,
Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Du-
mitru, Vanhoucke, Vincent, and Rabinovich, Andrew.
Going deeper with convolutions. In CVPR, 2015.

Han, Song, Pool, Jeff, Tran, John, and Dally, William J.
Learning both weights and connections for efﬁcient neu-
ral network. In NIPS, 2015.

Wan, Li, Zeiler, Matthew, Zhang, Sixin, LeCun, Yann, and
Fergus, Rob. Regularization of neural networks using
dropconnect. In ICML, 2013.

Ioffe, Sergey and Szegedy, Christian. Batch normalization:
Accelerating deep network training by reducing internal
covariate shift. In ICML, 2015.

Xu, Bing, Wang, Naiyan, Chen, Tianqi, and Li, Mu. Em-
pirical evaluation of rectiﬁed activations in convolutional
network. In ICML Workshop, 2015.

Understanding and Improving Convolutional Neural Networks via Concatenated Rectiﬁed Linear Units

Yang, Zichao, Moczulski, Marcin, Denil, Misha, de Fre-
itas, Nndo de, Smola, Alex, Song, Le, and Wang, Ziyu.
Deep fried convnets. In ICCV, 2015.

Zagoruyko, Sergey. Torch blog. http://torch.ch/

blog/2015/07/30/cifar.html, 2015.

Zeiler, Matthew D. and Fergus, Rob. Stochastic pooling for
regularization of deep convolutional neural networks. In
ICLR, 2013.

Zhao, Junbo, Mathieu, Michael, Goroshin, Ross, and Le-
cun, Yann. Stacked what-where auto-encoders. In ICLR,
2015.

Understanding and Improving Convolutional Neural Networks via Concatenated Rectiﬁed Linear Units

Algorithm 1 Linear Reconstruction without Convolution
1: fcnn(x) ← conv features.
2: W ← weight matrix.
3: Obtain the linear responses after convolution by revert-
ing CRelu: z = ρ−1

c (fcnn(x)).

4: Compute the Moore Penrose pseudoinverse of W T ,
5: Obtain the ﬁnal reconstruction: x(cid:48) = (W T )+z.

(W T )+.

3: Obtain the linear responses after convolution by revert-

that are activated by x.
ing CRelu: z = ρ−1

Algorithm 2 Linear Reconstruction with Convolution
1: fcnn(x) ← conv features after max-pooling.

2: (cid:99)Wx ← weight matrix consisting of shifted conv ﬁlters
4: Compute the Moore Penrose pseudoinverse of (cid:99)W T
((cid:99)W T
5: Obtain the ﬁnal reconstruction: x(cid:48) = ((cid:99)W T
Appendix

c (fcnn(x)).

x )+z.

x )+.

x ,

A. Reconstruction Property Proofs
Proposition A.1. Let x ∈ RD, x = x(cid:48) + (x − x(cid:48)), where
x(cid:48) ∈ range(W ) and x − x(cid:48) ∈ ker(W ). Then we can re-
construct x(cid:48) with fcnn(x).

Proof. We show x(cid:48) can be reconstructed from fcnn(x)
by providing a simple linear reconstruction algorithm de-
scribed by Algorithm 1. First, apply the inverse function
of CRelu on fcnn(x): z = ρ−1
c (fcnn(x)). Then, com-
pute the Moore Penrose pseudoinverse of W T , denote by
(W T )+. By deﬁnition Q = (W T )+W T is the orthog-
onal projector onto range(W ), therefore we can obtain
x(cid:48) = (W T )+z.
Deﬁnition A.1. A frame is a set of elements of a vector
space V , {φk}k=1,··· ,K, which satisﬁes the frame condi-
tion: there exist two real numbers C1 and C2, the frame
bounds, such that 0 < C1 ≤ C2 < ∞, and ∀v ∈ V

2 ≤ K(cid:88)

C1(cid:107)v(cid:107)2

|(cid:104)v, φi(cid:105)|2 ≤ C2(cid:107)v(cid:107)2
2.

k=1

(Christensen, 2003)
Proposition A.2. Let {φk}k=1,...,K be a sequence in V ,
then {φk} is a frame for span{φk}. Hence, {φk} is a frame
for V if and only if V = span{φk}6. (Christensen, 2003)
6There exist inﬁnite spanning sets that are not frames, but we
will not be concerned with those here since we only deal with
ﬁnite dimensional vector spaces.

(cid:80)K

Deﬁnition A.2. Consider now V equipped with a frame
{φk}k=1,...,K. The Analysis Operator, T : V → RK, is
deﬁned by T v = {(cid:104)v, φk(cid:105)}k=1,...,K. The Synthesis Op-
erator, T ∗ : RK → V , is deﬁned by T ∗{ck}k=1,...,K =
k=1 ckφk, which is the adjoint of the Analysis Operator.
The Frame Operator, S : V → V , is deﬁned to be the com-
position of T with its adjoint:

Sv = T ∗T v.

(Christensen,

The Frame Operator is always invertible.
2003)
Theorem A.3. The optimal lower frame bound C1 is the
smallest eigenvalue of S; the optimal upper frame bound
C2 is the largest eigenvalue of S. (Christensen, 2003)
We would also like to investigate the matrix representa-
tion of the operators T ,T ∗ and S. Consider V , a sub-
space of RD, equipped with a frame {φk}k=1,··· ,K. Let
U ∈ RD×d be a matrix whose column vectors form an
orthonormal basis for V (here d is the dimension of V ).
Choosing U as the basis for V and choosing the standard
basis {ek}k=1,··· ,K as the basis for RK, the matrix repre-
sentation of T is ˜T = W T U, where W is the matrix whose
column vectors are {φT
k }k=1,··· ,K. Its transpose, ˜T ∗, is the
matrix representation for T ∗; the matrix representation for
S is ˜S = ˜T ∗ ˜T .
Lemma A.4. Let x ∈ RD and W an D-by-K matrix. If
x ∈ range(W ), then σmin(cid:107)x(cid:107)2 ≤ (cid:107)W T x(cid:107)2 ≤ σmax(cid:107)x(cid:107)2,
where σmin and σmax are the least and largest singular
value of V respectively.

Proof. By Proposition A.2,
the columns in W form a
frame for range(W ). Let U be an orthonormal basis for
range(W ). Then the matrix representation under U for
the Analysis Operator, T , is ˜T = W T U, and the corre-
sponding representation for x under U is U T x. Now, by
Theorem A.3, we have:
2 ≤ (cid:107) ˜T x(cid:107)2

2 = (cid:107)W T U U T x(cid:107)2

2 = (cid:107)W T x(cid:107)2
2,

λmin(cid:107)x(cid:107)2

where λ1 is the least eigenvalue of ˜S. Therefore, we
have σmin(cid:107)x(cid:107)2 ≤ (cid:107)W T x(cid:107)2, where σmin is the least sin-
gular value of W . Lastly, by the deﬁnition of operator-
induced matrix norm, we have the upper bound (cid:107)W T x(cid:107)2 ≤
σmax(cid:107)x(cid:107)2
Theorem A.5. 2.2 Let x ∈ V and satisfy the assumption
from Equation (1). Then we can obtain x(cid:48), the reconstruc-
tion of x using fcnn(x) such that

(cid:107)x − x(cid:48)(cid:107)2

(cid:107)x(cid:107)2

≤

where λmin and(cid:101)λmax are square of the minimum and max-
imum singular values of Wx and(cid:102)Wx respectively.

λmin

,

(cid:115)(cid:101)λmax − λmin

Understanding and Improving Convolutional Neural Networks via Concatenated Rectiﬁed Linear Units

x , denote by ((cid:99)W T

x )+z, since by deﬁnition, Q = ((cid:99)W T

Proof. We use similar method to reconstruct as described
by Algorithm 2: ﬁrst reverse the CRelu activation and ob-
tain z = ρ−1
c (fcnn(x)); then compute the Moore Penrose
x )+; ﬁnally, obtain
x is

pseudoinverse of (cid:99)W T
x(cid:48) = ((cid:99)W T
x )+(cid:99)W T
the orthogonal projector onto range((cid:99)Wx). To proceed the
responding activation of the ﬁlters from (cid:102)Wx by ˜z, com-
pute the Morre Penrose pseudoinverse of (cid:102)Wx and obtain
˜x = ((cid:102)W T
x )+ ˜z. Note that since range((cid:102)Wx) is a subspace of
range((cid:99)Wx), therefore, the reconstruction x(cid:48) will always be

proof, we denote the subset of z which matches the cor-

equal or better than ˜x, i.e. (cid:107)x − x(cid:48)(cid:107)2 ≤ (cid:107)x − ˜x(cid:107)2. From
Lemma A.4, the nature of max-pooling and the assumption
on x (Equation 1), we derive the following inequality

λmin(cid:107)x(cid:107)2

2 ≤ (cid:107)W T

x x(cid:107)2 ≤ (cid:107)(cid:102)W T
= (cid:107)(cid:102)W T
x x(cid:107)2
x ˜x(cid:107)2

2

where λmin and ˜λmax are square of the minimum and max-

2 ≤ ˜λmax(cid:107)˜x(cid:107)2
2,
imum singular values of Wx and(cid:102)Wx respectively.
range((cid:102)Wx), thus (cid:107)x(cid:107)2
2 with (cid:107)˜x(cid:107)2

Because ˜x is the orthogonal projection of x on to
2. Now substitute
(cid:107)x(cid:107)2

2 + (cid:107)x − ˜x(cid:107)2

2 +(cid:107)x− ˜x(cid:107)2

2 = (cid:107)˜x(cid:107)2

2, we have:

λmin((cid:107)˜x(cid:107)2
(cid:107)x − ˜x(cid:107)2

(cid:107)x − x(cid:48)(cid:107)2

(cid:107)x − x(cid:48)(cid:107)2 ≤

(cid:107)x − x(cid:48)(cid:107)2

(cid:107)x(cid:107)2

≤

2) ≤ ˜λmax(cid:107)˜x(cid:107)2

2

(cid:107)˜x(cid:107)2

2

(cid:107)x(cid:107)2

2

˜λmin

2 + (cid:107)x − ˜x(cid:107)2
2 ≤ ˜λmax − λmin
2 ≤ ˜λmax − λmin
(cid:115)
λmin
˜λmax − λmin
(cid:115)(cid:101)λmax − λmin

λmin

.

λmin

(cid:107)x(cid:107)2

Theorem A.6. Let x ∈ RD, and let xs ∈ RD be its pro-
jection onto a random subspace of dimension D2, then

(cid:114)

(cid:21)

(cid:20)(cid:107)xs(cid:107)2

(cid:107)x(cid:107)2

E

=

Ds
D

Proof. Without loss of generality, let (cid:107)x(cid:107)2 = 1. Pro-
jecting a ﬁxed x onto a random subspace of dimension
Ds is equivalent of projecting a random unit-norm vector
z = (z1, z2,··· , zD)T onto a ﬁxed subspace of dimen-
sion Ds thanks to the rotational invariance of inner prod-
uct. Without loss of generality, assume the ﬁxed subspace
here is spanned by the ﬁrst Ds standard basis covering the
ﬁrst D2 coordinates of z. Then the resulting projection is
zs = (z1, z2,··· , zDs, 0,··· , 0).

Because each entry of z, zi, is identically distributed, we
have

Because z is unit norm, we have

E(cid:2)(cid:107)z(cid:107)2

2

(cid:35)

z2
i

= 1.

(cid:34) D(cid:88)

(cid:3) = E
(cid:34) Ds(cid:88)

i=1

(cid:35)

2

E(cid:2)(cid:107)zs(cid:107)2
(cid:3) = E
(cid:21)
(cid:20)(cid:107)xs(cid:107)2

= E

(cid:107)x(cid:107)2

E

z2
i

=

Ds
D

.

i=1

(cid:21)

(cid:20)(cid:107)zs(cid:107)2

(cid:107)z(cid:107)2

(cid:114)

=

Ds
D

.

Together we have

 ,

(cid:12)(cid:12)(cid:12)(cid:12)x1,··· , xL
(cid:104) ˆRL(F)
(cid:105)

B. Proof of Model Complexity Bound
Deﬁnition B.1. (Rademacher Complexity) For a sample
S = {x1,··· , xL} generated by a distribution D on set X
and a real-valued function class F in domain X, the empir-
ical Rademacher complexity of F is the random variable:

(cid:88)

f∈F

ˆRL(F) = Eσ

σif (xi)|

| 2
L

independent uniform {±1}-valued
The Rademacher

where σi’s are
(Rademacher) random variables.
complexity of F is RL(F) = ES
Lemma B.1. (Composition Lemma) Assume ρ : R → R is
a Lρ-Lipschitz continuous function, i.e. , |ρ(x) − ρ(y)| ≤
Lρ|x − y|. Then ˆRL(ρ ◦ F) = Lρ ˆRL(F).
Proposition B.2. (Network Layer Bound) Let G be the
class of real functions Rdin → R with input dimension F,
that is, G = [F]din
j=1 and H is a linear transform function
parametrized by W with (cid:107)W(cid:107)2 ≤ B, then ˆRL(H ◦ G) ≤
√
dinB ˆRL(F). (Wan et al., 2013)
Corollary B.3. By Lemma B.1, Proposition B.2, and the
fact that Relu is 1-Lipschitz, we know that ˆRL(Relu◦G) =
ˆRL(G) and that ˆRL(H ◦ Relu ◦ G) ≤ √
Theorem B.4. 4.1 Let G be the class of real functions
Rdin → R with input dimension F, that is, G = [F]din
j=1.
Let H be a linear transform function from R2din
to R,
parametrized by W , where (cid:107)W(cid:107)2 ≤ B. Then ˆRL(H ◦
ρc ◦ G) ≤ √

dinB ˆRL(F).

dinB ˆRL(F).

Recall from Deﬁnition 2.1, ρc is the CRelu formulation.

Understanding and Improving Convolutional Neural Networks via Concatenated Rectiﬁed Linear Units

ˆRL(H ◦ ρc ◦ G) = Eσ

σih ◦ ρc ◦ g(xi)|

(cid:35)

(S1)

(S2)

(S3)

(S4)

(S5)

(S6)

be ﬁred only if both the transformed stimulus and the orig-
inal testing example ﬁre the same convolution ﬁlter at the
same spatial location.
At the end, for each convolution layer, we average the in-
variance scores of all the ﬁlters at this layer to form the ﬁnal
score.

D. Implementation Details on ImageNet

Models

The networks from Table S7, S8, S9,and S10, where the
number of convolution ﬁlters after CRelu are kept the
same, are optimized using SGD with mini-batch size of
64 examples and ﬁxed momentum 0.9. The learning rate
and weight decay is adapted using the following sched-
ule: epoch 1-10, 1e−2 and 5e−4; epoch 11-20, 1e−3 and
5e−4; epoch 21-25, 1e−4 and 5e−4; epoch 26-30, 5e−5
and 0; epoch 31-35, 1e−5 and 0; epoch 36-40, 5e−6 and
0; epoch 41-45, 1e−6 and 0.
The networks from Table S11 and S12, where the number
of convolution ﬁlters after CRelu are reduced by half, are
optimized using Adam with an initial learning rate 0.0002
and mini-batch size of 64 examples for 100 epochs.

Proof.

(cid:34)

= Eσ

≤ BEσ

(cid:34)

sup

h∈H,g∈G

| 2
L

L(cid:88)

i=1

L(cid:88)

(cid:35)


(cid:107)W(cid:107)≤B,g∈G

(cid:34)
(cid:34)

(cid:107)

sup

f∈F

sup
sup
(cid:34)

f∈F

(cid:107)

2
L

2
L

(cid:107)2

σiρc ◦ g(xi)(cid:105)|

|(cid:104)W,

2
L

i=1

i=1

(cid:35)din
i ρc ◦ f j(xi)

σj
(cid:35)din
(cid:35)

σj
i f j(xi)

L(cid:88)
L(cid:88)
L(cid:88)

(cid:107)2

j=1

i=1

j=1

σif (xi)|

| 2
L

i=1

= BEσ

(cid:112)
(cid:112)

= B

=

dinEσ
sup
f∈F
dinB ˆRL(F).

From (S1) to (S2), use the deﬁnition of linear transforma-
tion and inner product. From (S2) to (S3), use Cauchy-
Schwarz inequality and the assumption that (cid:107)W(cid:107)2 ≤ B.
From (S3) to (S4), use the deﬁnition of CRelu and l2 norm.
From (S4) to (S5), use the deﬁnition of l2 norm and sup op-
erator. From (S5) to (S6), use the deﬁnition of ˆRL

We see that CRelu followed by linear transformation
reaches the same Rademacher complexity bound as Relu
followed by linear transformation with the same input di-
mension.

C. Invariance Score
We use consistent terminology employed by Goodfellow
et al. (2009) to illustrate the calculation of the invariance
scores.
For CIFAR-10/100, we utilize all 50k testing images to cal-
culate the invariance scores; for ImageNet, we take the cen-
ter crop from 5k randomly sampled validation images
For each individual ﬁlter, we calculate its own ﬁring thresh-
old, such that it is ﬁred one percent of the time, i.e.
the
global ﬁring rate is 0.01. For Relu models, we zero out
all the negative negative responses when calculating the
threshold; for CRelu models, we take the absolute value.
To build the set of semantically similar stimuli for each
testing image x, we apply horizontal ﬂip, 15 degree ro-
tation and translation. For CIFAR-10/100, translation is
composed of horizontal/vertical shifts by 3 pixels; for Ima-
geNet, translation is composed of cropping from the 4 cor-
ners.
Because our setup is convolutional, we consider a ﬁlter to

Understanding and Improving Convolutional Neural Networks via Concatenated Rectiﬁed Linear Units

E. Details of Network Architecture

3×3, 2, 0

Baseline
kernel, stride, padding
3×3×3×96, 1, 1
3×3×96×96, 1, 1
3×3×96×192, 1, 1
3×3×192×192, 1, 1
3×3×192×192, 1, 1
3×3×192×192, 1, 1
1×1×192×192, 1, 1
1×1×192×10/100, 1, 0

3×3, 2, 0

10×10 (100 for CIFAR-100)

activation

Relu
Relu
max
Relu
Relu
Relu
max
Relu
Relu
Relu
avg

Baseline (double)

3×3, 2, 0

kernel, stride, padding
3×3×3×192, 1, 1
3×3×192×192, 1, 1
3×3×192×384, 1, 1
3×3×384×384, 1, 1
3×3×384×384, 1, 1
3×3×384×384, 1, 1
1×1×384×384, 1, 1
1×1×384×10/100, 1, 0

3×3, 2, 0

10×10 (100 for CIFAR-100)

activation

Relu
Relu
max
Relu
Relu
Relu
max
Relu
Relu
Relu
avg

Layer
conv1
conv2
pool1
conv3
conv4
conv5
pool2
conv6
conv7
conv8
pool3

Table S1. (Left) Baseline and (right) baseline (double) models used for CIFAR-10/100 experiment. “avg” refers average pooling.

Layer
conv1
conv2
pool1
conv3
conv4
conv5
pool2
conv6
conv7
conv8
pool3

3×3, 2, 0

CRelu
kernel, stride, padding
3×3×3×96, 1, 1
3×3×192×96, 1, 1
3×3×192×192, 1, 1
3×3×384×192, 1, 1
3×3×384×192, 1, 1
3×3×384×192, 1, 1
1×1×384×192, 1, 1
1×1×384×10/100, 1, 0

3×3, 2, 0

10×10 (100 for CIFAR-100)

activation
CRelu
CRelu
max
CRelu
CRelu
CRelu
max
CRelu
CRelu
Relu
avg

CRelu (half)

3×3, 2, 0

kernel, stride, padding
3×3×3×48, 1, 1
3×3×96×48, 1, 1
3×3×96×48, 1, 1
3×3×96×96, 1, 1
3×3×192×96, 1, 1
3×3×192×96, 1, 1
1×1×192×96, 1, 1

3×3, 2, 0

1×1×192×10/100, 1, 0

10×10 (100 for CIFAR-100)

activation
CRelu
CRelu
max
CRelu
CRelu
CRelu
max
CRelu
CRelu
Relu
avg

Table S2. (Left) CRelu and (right) CRelu (half) models used for CIFAR-10/100 experiment.

Understanding and Improving Convolutional Neural Networks via Concatenated Rectiﬁed Linear Units

Layer
conv1

conv2
pool1
conv3

conv4
pool2
conv5

conv6

conv7
pool3
conv8

conv9

conv10
pool4
conv11

conv12

conv13
pool5

fc14

fc15

kernel, stride, padding

2×2, 2, 0

2×2, 2, 0

3×3×3×64, 1, 1
dropout with ratio 0.3
3×3×64×64, 1, 1
3×3×64×128, 1, 1
dropout with ratio 0.4
3×3×128×128, 1, 1
3×3×128×256, 1, 1
dropout with ratio 0.4
3×3×256×256, 1, 1
dropout with ratio 0.4
3×3×256×256, 1, 1
3×3×256×512, 1, 1
dropout with ratio 0.4
3×3×512×512, 1, 1
dropout with ratio 0.4
3×3×512×512, 1, 1
3×3×512×512, 1, 1
dropout with ratio 0.4
3×3×512×512, 1, 1
dropout with ratio 0.4
3×3×512×512, 1, 1

2×2, 2, 0

2×2, 2, 0

2×2, 2, 0
512×512
512×10/100

dropout with ratio 0.5

dropout with ratio 0.5

activation
BN+Relu

BN+Relu

BN+Relu

BN+Relu

BN+Relu

BN+Relu

BN+Relu

BN+Relu

BN+Relu

BN+Relu

BN+Relu

BN+Relu

BN+Relu

BN+Relu

Layer
conv1

kernel, stride, padding

3×3×3×32, 1, 1

activation
CRelu

dropout with ratio 0.1

···

conv2

Table S4. VGG + (conv1) for CIFAR-10/100

Layer
conv1

conv2
pool1
conv3

conv4

kernel, stride, padding

3×3×3×32, 1, 1
dropout with ratio 0.1
3×3×64×64, 1, 1
3×3×64×64, 1, 1
dropout with ratio 0.2

2×2, 2, 0

···

activation
CRelu

BN+Relu

CRelu

Table S5. VGG + (conv1, 3) for CIFAR-10/100

Layer
conv1

conv2
pool1
conv3

conv4
pool2
conv5

conv6

conv7

kernel, stride, padding

2×2, 2, 0

3×3×3×32, 1, 1
dropout with ratio 0.1
3×3×64×64, 1, 1
3×3×64×64, 1, 1
dropout with ratio 0.2
3×3×128×128, 1, 1
3×3×128×128, 1, 1
dropout with ratio 0.2
3×3×256×256, 1, 1
dropout with ratio 0.2

2×2, 2, 0

···

activation
CRelu

BN+Relu

CRelu

BN+Relu

CRelu

BN+Relu

Table S3. VGG for CIFAR-10/100

Table S6. VGG + (conv1, 3, 5) for CIFAR-10/100

Understanding and Improving Convolutional Neural Networks via Concatenated Rectiﬁed Linear Units

Layer
conv1
conv2
conv3
conv4
conv5
conv6
conv7
conv8
conv9

conv10
conv11
conv12
pool

Layer
conv1
conv2
conv3
conv4
conv5
conv6
conv7
conv8
conv9

conv10
conv11
conv12
pool

kernel, stride, padding
11×11×3×96, 4,0
1×1×96×96, 1,0
3×3×96×96, 2,0
5×5×96×256, 1, 2
1×1×256×256, 1,0
3×3×256×256, 2,0
3×3×256×384, 1 ,1
1×1×384×384, 1,0
3×3×384×384, 2,1
3×3×384×1024, 1,1
1×1×1024×1024, 1,0
1×1×1024×1000, 1
6×6 average-pooling

no dropout

activation

Relu
Relu
Relu
Relu
Relu
Relu
Relu
Relu
Relu

Relu
Relu
Relu

Table S7. Baseline for ImageNet

kernel, stride, padding
11×11×3×96, 4,0
1×1×192×96, 1,0
3×3×192×96, 2,0
5×5×192×256, 1, 2
1×1×512×256, 1,0
3×3×256×256, 2,0
3×3×256×384, 1 ,1
1×1×384×384, 1,0
3×3×384×384, 2,1
3×3×384×1024, 1,1
1×1×1024×1024, 1,0
1×1×1024×1000, 1
6×6 average-pooling

no dropout

activation
CRelu
CRelu
CRelu
CRelu
Relu
Relu
Relu
Relu
Relu

Relu
Relu
Relu

Layer
conv1
conv2
conv3
conv4
conv5
conv6
conv7
conv8
conv9

conv10
conv11
conv12
pool

kernel, stride, padding
11×11×3×96, 4,0
1×1×192×96, 1,0
3×3×192×96, 2,0
5×5×192×256, 1, 2
1×1×512×256, 1,0
3×3×512×256, 2,0
3×3×512×384, 1 ,1
1×1×768×384, 1,0
3×3×768×384, 2,1
dropout with ratio 0.25
3×3×768×1024, 1,1
1×1×1024×1024, 1,0
1×1×1024×1000, 1
6×6 average-pooling

activation
CRelu
CRelu
CRelu
CRelu
CRelu
CRelu
CRelu
CRelu
CRelu

Relu
Relu
Relu

Table S10. CRelu (conv1-9) for ImageNet

Layer
conv1
conv2
conv3
conv4
conv5
conv6
conv7
conv8
conv9

conv10
conv11
conv12
pool

kernel, stride, padding
11×11×3×48, 4,0
1×1×96×48, 1,0
3×3×96×48, 2,0
5×5×96×128, 1, 2
1×1×256×128, 1,0
3×3×256×128, 2,0
3×3×256×192, 1 ,1
1×1×384×192, 1,0
3×3×384×192, 2,1
dropout with ratio 0.25
3×3×384×512, 1,1
1×1×512×512, 1,0
1×1×512×1000, 1
6×6 average-pooling

activation
CRelu
CRelu
CRelu
CRelu
CRelu
CRelu
CRelu
CRelu
CRelu

CRelu
CRelu
CRelu

Table S8. CRelu (conv1-4) for ImageNet

Table S11. CRelu (all) for ImageNet

Layer
conv1
conv2
conv3
conv4
conv5
conv6
conv7
conv8
conv9

conv10
conv11
conv12
pool

kernel, stride, padding
11×11×3×96, 4,0
1×1×192×96, 1,0
3×3×192×96, 2,0
5×5×192×256, 1, 2
1×1×512×256, 1,0
3×3×512×256, 2,0
3×3×512×384, 1 ,1
1×1×768×384, 1,0
3×3×384×384, 2,1
dropout with ratio 0.25
3×3×384×1024, 1,1
1×1×1024×1024, 1,0
1×1×1024×1000, 1
6×6 average-pooling

activation
CRelu
CRelu
CRelu
CRelu
CRelu
CRelu
CRelu
Relu
Relu

Relu
Relu
Relu

Table S9. CRelu (conv1-7) for ImageNet

Layer
conv1
conv2
conv3
conv4
conv5
conv6
conv7
conv8
conv9

conv10
conv11
conv12
pool

kernel, stride, padding
11×11×3×48, 4,0
1×1×96×96, 1,0
3×3×96×96, 2,0
5×5×96×128, 1, 2
1×1×256×256, 1,0
3×3×256×256, 2,0
3×3×256×192, 1 ,1
1×1×384×384, 1,0
3×3×384×384, 2,1
dropout with ratio 0.25
3×3×384×1024, 1,1
1×1×1024×1024, 1,0
1×1×1024×1000, 1
6×6 average-pooling

activation
CRelu
Relu
Relu
CRelu
Relu
Relu
CRelu
Relu
Relu

Relu
Relu
Relu

Table S12. CRelu (conv1,4,7) for ImageNet

Understanding and Improving Convolutional Neural Networks via Concatenated Rectiﬁed Linear Units

F. Image Reconstruction
In this section, we provide more image reconstruction examples.

(a) Original image

(b) CRelu conv1

(c) CRelu conv2

(d) CRelu conv3

(e) CRelu conv4

(f) Original image

(g) conv1 Recon

(h) conv2 Recon

(i) conv3 Recon

(j) conv4 Recon

(k) Original image

(l) conv1 Recon

(m) conv2 Recon

(n) conv3 Recon

(o) conv4 Recon

(p) Original image

(q) conv1 Recon

(r) conv2 Recon

(s) conv3 Recon

(t) conv4 Recon

