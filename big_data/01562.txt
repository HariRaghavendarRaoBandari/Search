6
1
0
2

 
r
a

M
4

 

 
 
]

A
N
.
s
c
[
 
 

1
v
2
6
5
1
0

.

3
0
6
1
:
v
i
X
r
a

A Randomized Misﬁt Approach for Data Reduction
in Large-Scale Inverse Problems

Ellen B. Le1, Aaron Myers1, Tan Bui-Thanh1,2
1 Institute for Computational Engineering and Sciences, The University of Texas at
Austin, Austin, TX 78712, USA
2 Department of Aerospace Engineering and Engineering Mechanics, The University
of Texas at Austin, Austin, TX 78712, USA

E-mail:

{ellenle,aaron,tanbui}@ices.utexas.edu

Abstract. We present a randomized misﬁt approach as a data reduction method
for large-scale inverse problems via a novel randomization of the objective function.
Derived using stochastic programming, the method may be understood as an
application of a Johnson-Lindenstrauss transform or random projection to reduce the
dimension of the data-misﬁt vector. Using large deviation bounds, it is shown that the
solution of this reduced inverse problem with a signiﬁcantly reduced data dimension
is a discrepancy principle-satisfying solution to the original problem. The purpose
of this paper is to develop a systematic framework for the application of random
projections in the context of ill-posed inverse problems with large data sets and give
theoretical insight into the eﬃcacy. This randomized misﬁt approach permits the use
of a large class of distributions from the extensive literature on random projections. In
particular, we analyze sparse random projections which have additional data-reduction
properties. We also provide a diﬀerent proof for a variant of the Johnson-Lindenstrauss
lemma. This proof provides intuition into the O(ε−2) factor. The main contribution
of this paper is a theoretical result that shows the method eﬃcacy is attributable
to the combination of both Johnson-Lindenstrauss theory and Morozov’s discrepancy
principle. This result provides justiﬁcation for the suitability of the proposed approach
in solving inverse problems with big data. Numerical veriﬁcation of theoretical ﬁndings
are presented for model problems of estimating a distributed parameter in an elliptic
partial diﬀerential equation. Results with diﬀerent random projections are presented
to demonstrate the viability and accuracy of the proposed approach.

Keywords Random projections, stochastic programming, large-scale inverse problems,
model reduction, large deviation theory, Johnson-Lindenstrauss lemma, data dimension
reduction.

AMS classiﬁcation scheme numbers: 35Q62, 62F15, 35R30, 35Q93, 65C60

Randomized Misﬁt Approach for Data Reduction

1. Introduction

1.1. The randomized misﬁt approach for inverse problems

Consider the following additive noise-corrupted pointwise observational model

dj = w (xj; u) + ηj,

j = 1, . . . , N,

2

(1)

where the objective is to reconstruct the distributed parameter u given N data points dj,
with N large. For a given u, the observations w (x; u) are obtained by solving a complex
forward model, governed by PDEs. The location of an observational data point in an
open and bounded spatial domain Ω is denoted by xj, and ηj is assumed to be Gaussian
random noise with mean 0 and variance σ2.

Concatenating the observations, we rewrite (1) as

d = F (u) + η,

(2)

(cid:62)
:= [w (x1; u) , . . . , w (xN ; u)]

where F (u)
is the parameter-to-observable map.
Although the forward problem is usually well-posed, the inverse problem is ill-posed.
An intuitive reason is that the dimension of observations d is much smaller than that
of the parameter u (which is inﬁnite before discretization), and hence the observations
provide limited information about u. As a result, the null space of the Jacobian of the
parameter-to-observable map F is non-empty. For a broad class of inverse scattering
problems in particular, the Gauss-Newton approximation of the Hessian is a compact
operator and therefore its range space is eﬀectively ﬁnite-dimensional [1, 2, 3]. The
standard deterministic Tikhonov approach mitigates the ill-conditioning by adding a
quadratic term to the cost function, so that our inverse problem may now be formulated
as

+

min

u

J (u) :=

where (cid:98)d − (cid:98)F (u) := 1
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)R 1
2·(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) is a norm weighted by a regularization operator R.

σ [d − F(u)] is the data-misﬁt vector, Euclidean norm in RN is

denoted by (cid:107)·(cid:107), and

1

2 u

This method is representative of the classical non-statistical approach which does
not typically account for the randomness due to measurements and other sources
(though one can equip the deterministic solution with a conﬁdence region by post-
processing—see, e.g., [4] and references therein). If the regularization term is replaced
by the Cameron-Martin norm of u (the second term in (4) below), then Tikhonov
solution is in fact identical to the maximum a posteriori (MAP) point of (4) [5, 6, 7, 8].
This point estimate, however, oﬀers limited insight regarding the randomness inherent
in the problem.

(cid:13)(cid:13)(cid:13)(cid:98)d −(cid:98)F (u)

(cid:13)(cid:13)(cid:13)2

1
2

(cid:13)(cid:13)(cid:13)R

(cid:13)(cid:13)(cid:13)2

,

The Bayesian paradigm [7, 9, 10, 11, 12] allows for a systematic accounting of the
ill-posedness and error uncertainty in inverse problems. Here, we seek a statistical
description of all possible parameter ﬁelds that conform to some prior knowledge
and are consistent with the observations. The Bayesian approach reformulates the

expressed as

πlike (d|u) ∝ exp

(cid:18)1

2

(cid:13)(cid:13)(cid:13)2(cid:19)
(cid:13)(cid:13)(cid:13)(cid:98)d −(cid:98)F (u)

.

Randomized Misﬁt Approach for Data Reduction

3

inverse problem in the framework of statistical inference, incorporating uncertainties
in the observations, the forward map F, and prior information into a probability
distribution. The probability density is the solution to the Bayesian inverse problem.
It requires speciﬁcation of a likelihood model, which characterizes the probability that
the parameter u could have produced the observed data d, and a prior model, which
is problem-dependent and encodes all information regarding u before observations are
made.

The additive-noise model (2) is used to construct the likelihood pdf which is

The prior is chosen to be the Gaussian measure µ := N (u0,C) where u0 is a
mean function and C is an appropriate covariance operator [7]. Here C is deﬁned to
be the inverse of an elliptic second-order diﬀerential operator with high-enough order
to guarantee well-posedness [7, 13, 14]. We then discretize the prior, the forward
equation, and the parameter u (yielding a ﬁnite-dimensional vector u ∈ Rm) through
the ﬁnite element method (see [13, 14] for a comprehensive treatment) so that the ﬁnite-
dimensional posterior probability of u is given by Bayes formula as
(cid:107)u − u0(cid:107)2C

π (u|d) ∝ exp

(cid:18)1

(cid:19)

(3)

where (cid:107)·(cid:107)C :=
inner product (cid:104)·,·(cid:105)L2(Ω). We then deﬁne the MAP point (or estimator) of (3) by

denotes the weighted L2 (Ω) norm induced by the L2 (Ω)

(cid:13)(cid:13)(cid:13)C− 1
2·(cid:13)(cid:13)(cid:13)L2(Ω)

2

+

1
2

(cid:13)(cid:13)(cid:13)(cid:98)d −(cid:98)F (u)
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:98)d −(cid:98)F (u)
(cid:13)(cid:13)(cid:13)2

1
2

u(cid:63) := arg min

u

J (u, d) =

+

1
2

(cid:107)u − u0(cid:107)2C .

(4)

We note that the last term in (4) may be viewed as a Tikhonov regularization term,
and subsequently the MAP point may be considered as a solution to the deterministic
inverse problem with a regularization “inspired” by the prior. It is important to stress
that the Bayesian solution provides more than a point solution. The Bayesian posterior
encodes all possible solutions with an associated conﬁdence (probability). For the
purpose of this paper, we restrict ourselves to the MAP computation, in order to focus
on methodology development in addressing the challenge of big data, i.e., large N .

The main idea of the paper is as follows. Let r ∈ RN be a random vector with

mean zero and identity covariance, i.e. Er
N i.i.d. random variables ζ with mean zero and variance 1).

Then the misﬁt term of (4) can be rewritten as:

(cid:13)(cid:13)(cid:13)(cid:98)d −(cid:98)F(u)

(cid:13)(cid:13)(cid:13)2

(cid:16)(cid:98)d −(cid:98)F(u)

=

(cid:2)rr(cid:62)(cid:3) = I (equivalently, let r be the vector of
(cid:2)rr(cid:62)(cid:3)(cid:16)(cid:98)d −(cid:98)F(u)
(cid:17)
(cid:17)(cid:62) Er
(cid:104)
r(cid:62)(cid:16)(cid:98)d −(cid:98)F(u)
(cid:17)(cid:105)2

r(cid:62)(cid:16)(cid:98)d −(cid:98)F(u)
(cid:104)
(cid:17)(cid:105)2

(cid:107)u − u0(cid:107)2
C .

= Er

+

,

1
2

which allows us to write the objective functional in (4) as

J (u) =

Er

1
2

Randomized Misﬁt Approach for Data Reduction

4
We then approximate the expectation Er [·] using a Monte Carlo approximation
(also known as the Sample Average Approximation (SAA) [15, 16]) with n i.i.d. samples
{rj}n

j=1. This leads us to deﬁne the following randomized inverse problem

min

u

Jn (u; r) =

j

(cid:107)u − u0(cid:107)2
C .

r(cid:62)

1
2n

(cid:16)(cid:98)d −(cid:98)F (u)
(cid:17)(cid:105)2
(cid:104)
n(cid:88)
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)˜d − ˜F (u)
(cid:62)(cid:98)d, and ˜F (u) := 1√

1
2

j=1

=

1
2

+

+

1
2
(cid:107)u − u0(cid:107)2
C ,

n [r1, . . . , rn]

(cid:62)(cid:98)F (u) ∈ Rn. We call

(5)

where ˜d := 1√
˜d − ˜F (u) the reduced data-misﬁt vector.

n [r1, . . . , rn]

For a reduced misﬁt vector dimension n (cid:28) N , we call this randomization the
randomized misﬁt approach (RMA). The reduced data problem (5) may be solved using
any scalable robust iterative optimization algorithm. For our numerical experiments in
section 3 we choose an implementation of the state-of-the-art bound constrained trust
region Newton-CG method [17].

If we deﬁne the MAP point of (5) as

u(cid:63)

n := arg min
u

Jn (u) ,

(6)

the optimal RMA cost as J (cid:63)
we need to address the challenge of estimating the errors |J (cid:63)
the sample size n increases. This is the subject of section 2.

n), and the optimal true cost as J (cid:63) := J (u(cid:63)), then
n − u(cid:63)(cid:107) as

n − J (cid:63)| and (cid:107)u(cid:63)

n := Jn (u(cid:63)

1.2. A brief literature review and our contributions

Since [18], the majority of randomized methods aimed at reducing the computational
complexity of large-scale inverse problems focus on using the randomized SVD (RSVD)
algorithm of [19] to generate truncated SVD approximations of the parameter-to-
observable operator [20, 21, 22, 23], the regularization operator [24, 25], or the prior-
preconditioned Hessian of the objective function [13, 26, 27, 28, 29]. The RSVD
algorithm uses a Gaussian random projection matrix to produce a low-rank operator (the
“sample matrix”). This operator is subsequently factored to generate an approximate
SVD decomposition for the original operator A. Theoretical results in [19] guarantee
the spectral norm accuracy of this approximation is of order σk+1(A) with a very high
user-deﬁned probability. Here k is equal to the reduced parameter dimension n plus a
small number of oversampling vectors. Theoretical results known about the accuracy
of an inverse solution (e.g., Proposition 2 in [30], Theorem 1 in [22]) to a problem
approximated with a randomized method are derived using this result.

A method referred to as source encoding is shown in [31] to be a fast, eﬀective
method for parameter estimation in the context of seismic inversion. Source encoding
involves taking random linear combinations of sources to produce a randomized objective
function, in an eﬀort to reduce the computational complexity of an inverse problem with

Randomized Misﬁt Approach for Data Reduction

5

multiple sources. The work in [32] provides an explanation for the eﬃcacy this method
by showing that the method is equivalent to the SAA [15, 16] through a stochastic
programming reformulation of the inverse optimization problem. Recently, the work in
[33] shows that source encoding in its stochastic reformulation (and as a stochastic trace
estimator method [34]) is equivalent to an application of the random projection in [35].
This paper extends above work in many directions. The randomized misﬁt
approach,
in contrast with the aforementioned methods which typically reduce the
parameter dimension through low-rank approximations, focuses on data dimension
reduction. Our work extends to a much larger class of random projections than the
distribution given in [35]. This class includes Gaussian random projections and very
sparse random projections. We also oﬀer an insight into why the reduced data dimension
n is O(ε−2), where ε is the relative error of the randomized cost function. We build a
theoretical foundation for why a small number of samples works well in the context of
inverse problems. Moreover, we show the solution of the randomized problem is indeed
a reasonable solution to the original problem. Although this result has been suggested
with numerical results in the above-indicated papers, to our knowledge, a theoretical
guarantee of solution viability has not been derived previously.

The two main theoretical contributions of this paper are: 1) we use large deviation
theory and the stochastic reformulation of a general inverse optimization problem [32, 33]
to show a version of the Johnson-Lindenstrauss embedding theorem [36, 37, 38] and 2)
we discover that the eﬃcacy of the randomized misﬁt approach is due to Morozov’s
discrepancy principle and large deviation theory acting in concert with each other. For
the speciﬁc application of random projections to inverse problems, we prove that it is the
combination of both eﬀects that allows this method to work well, even with very small
sample size n. Related methods [32, 33] note that the lower bounds on the reduced data
dimension given by the current literature are pessimistic in practice. In particular, it is
well-known that taking only a handful of randomly weighted combinations is surprisingly
eﬀective. In this paper, we present a discrepancy principle-based justiﬁcation: it is the
error inherent to inverse problems that allows for further reduction in the reduced data
dimension oﬀered by random projection theory. We stress here that the randomized
misﬁt approach achieves overall data reduction, even in the case of problems with just one
source. The approach does not take random combinations of sources as in [31, 32, 33, 39]
but rather of the data-misﬁt vector itself.

The structure of the paper is as follows. Section 2 presents the theoretical analysis
for the randomized misﬁt approach by deriving the large deviation bounds on the
error for a broad class of distributions. The reduced data dimension is shown to be
independent of the original dimension. This derivation leads to a diﬀerent proof of a
variant of the celebrated Johnson-Lindenstrauss embedding theorem. Using Morozov’s
discrepancy principle, Theorem 2 shows that the eﬀective reduced data dimension is also
bounded below by the noise in the problem. Therefore, the RMA solution is a guaranteed
solution for the original problem with a high user-deﬁned probability.
Section 3
summarizes numerical experiments on a model inverse heat conduction problem in one-,

Randomized Misﬁt Approach for Data Reduction

6

two-, and three-dimensions. We compare the RMA solution obtained with four diﬀerent
distributions to the solution of the full problem. We also provide numerical support for
Theorem 2.

2. An analysis of the randomized misﬁt approach (RMA)

For a given u in parameter space, it is clear that Jn (u; r) in (5) is an unbiased estimator
of J (u). It is also clear from the Law of Large Numbers that Jn (u) converges almost
surely to J (u). However, the eﬃcacy of the randomized misﬁt approach lies in the
exponential decay of its errors which, as shown below, is provided by large deviation
theory.

We ﬁrst show that errors larger than δ/2, for a given δ > 0, decay with a rate at
least as fast as the tail of a centered Gaussian. That is, for some distribution in (5) we
have

(cid:20)

P

|Jn (u; r) − J (u)| >

≤ e−N I(δ),

(7)

(cid:21)

δ
2

where

for some c > 0 and some θ.

I (δ) ≥ c

δ2
2θ2 .

2

rate function I (δ) [40].

This rate is suﬃcient to guarantee the solution attained from the the randomized
misﬁt approach is a discrepancy principle-satisfying solution for the original inverse
problem as will be shown in Theorem 2. Inequality (7) is equivalent to the statement

that P(cid:2)|Jn (u; r) − J (u)| > δ

(cid:3) satisﬁes a large deviation principle with large deviation
(cid:8)kδ − ln E(cid:2)ekX(cid:3)(cid:9) [40]. However we

The following proposition may be viewed as a special case of Cram´er’s Theorem,
which states that a sample mean of i.i.d. random variables X asymptotically obeys
a large deviation principle with rate I (δ) = supk
require the exact non-asymptotic bounds as derived here to show convergence of the
RMA for n = O(1). Recall that a real-valued random variable X is θ-subgaussian if

there exists some θ > 0 such that for all t ∈ R, E(cid:2)etX(cid:3) ≤ eθ2t2/2.

Proposition 1 The RMA error |Jn (u; r) − J (u)| has a tail probability that decays
exponentially in n with a nontrivial large deviation rate. Furthermore, if the RMA
is constructed with r such that 2|Jn (u; r) − J (u)| is the sample mean of i.i.d.
θ-
subgaussian random variables, then its large deviation rate is bounded below by c δ2
2θ2
for some c > 0.

Proof. Given r, deﬁne the random variable

(cid:104)
r(cid:62)(cid:16)(cid:98)d −(cid:98)F (u)

(cid:17)(cid:105)2 −(cid:13)(cid:13)(cid:13)(cid:98)d −(cid:98)F (u)

(cid:13)(cid:13)(cid:13)2

.

(8)

T (r; u) :=

Randomized Misﬁt Approach for Data Reduction

7

(cid:34)

P

1
n

n(cid:88)

j=1

(cid:35)

By a standard Chernoﬀ bound (see, e.g.[41]), we have that the RMA tail error decays
exponentially as

T (rj; u) > δ

(cid:8)tδ − ln E(cid:2)etT (r;u)(cid:3)(cid:9) is the large deviation rate.

The second part of the proposition follows with c = 1 by bounding E(cid:2)etT (r;u)(cid:3) in

where I(δ) = maxt
(9) and computing the maximum of tδ − θ2t2/2.

(9)

≤ e−nI(δ),

A great number of distributions are known to be subgaussian, notably the Gaussian
and Rademacher (also referred to as Bernoulli) distributions, and in fact any bounded
random variable is subgaussian. One class of subgaussian distributions that oﬀers
theoretically veriﬁable data dimension reduction is the following.

Deﬁnition 1 ((cid:96)-percent sparse random variables [37, 42]) Let s = 1
(cid:96) ∈ [0, 1) is the level of sparsity desired. Then

1−(cid:96) where

+1 with probability 1

0
−1 with probability 1

2s,

with probability (cid:96) = 1 − 1
s ,

2s

√

s

ζ =

(10)

is a (cid:96)-percent sparse distribution.

Note that for (cid:96) = 0, ζ corresponds to a Rademacher distribution, and that (cid:96) = 2/3
corresponds to the Achlioptas distribution [35]. By inspection we have that E [ζ] = 0
and E [ζ 2] = 1, and thus draws from ζ can be used in the randomized misﬁt approach.
it is easy to
implement, and the computation of the randomized misﬁt vector amounts to only
summations and subtractions, adding a further speedup to the method.
Increasing
from s = 1 to s > 1 results in a s-fold speedup as only 1/s of the data is included.

Distribution (10) is well-suited for the randomized misﬁt approach:

The RMA method takes n random combinations from the N -dimensional misﬁt
vector to form an n-dimensional misﬁt vector. Note that since each random combination
has a diﬀerent sparsity pattern, we eﬀectively do not exclude any data, yet each
computation requires only (cid:96)-percent of the data. MCMC-based sampling methods
require the number of samples to be large to ensure accuracy. However, RMA is not an
MCMC method for attaining the inverse solution. It is a Monte Carlo approximation
of the cost function for large-scale inverse problems with a large data dimension, to be
subsequently solved with any robust optimization algorithm. Additionally, the noise
inherent in the problem, along with the large deviation principles make the probability
of failure very small for moderate size n. This will be made rigorous in Theorem 2. The
methods mentioned in section 1.2 rely on similar ideas to obtain accurate results with
relatively few random vectors, most notably the RSVD algorithm [18].

We note that for the distribution (10), 1 ≤ s < ∞, the random variable ζ distributed

8
√
s − 2 ln s, ∀t ∈ (0, 1] . So, we may use it in the

Randomized Misﬁt Approach for Data Reduction

by (10) has† E(cid:2)etζ(cid:3) ≤ e b2t2
Theorem 1 Deﬁne v := (cid:98)d − (cid:98)F (u) ∈ RN .

following theorem.

2 with b =

√
subgaussian for some b ≥ 1/
√
2θ2 for θ = (cid:107)v(cid:107)2 /
below by c δ2

et(cid:107)v(cid:107)2X 2(cid:105)
E(cid:2)etT(cid:3) = e−t(cid:107)v(cid:107)2E(cid:104)
etX 2(cid:105) ≤
E(cid:104)
et(cid:107)v(cid:107)2X 2(cid:105) ≤ 1 + t(cid:107)v(cid:107)2 + t(cid:107)v(cid:107)4 E [X 4]

4b2(cid:107)v(cid:107)2 , we have

√

+

2.

2

For 0 < t ≤ 1

E(cid:104)

If r in (8) has components that are b-
2, then the RMA error has a large deviation rate bounded
2 and some 0 < c < 1

8b4 .

Proof. Let r ∈ RN such that r has i.i.d.
√
b ≥ 1/

2, E [ri] = 0, and E [r2

i ] = 1. Deﬁne w = v(cid:107)v(cid:107) and X = r(cid:62)w. Then

b-subgaussian components ri, with

∀t ∈ R.

(11)

From [37, Lemma 2.2], E [X 2] = 1 and X is also b-subgaussian. Then, by [38, Remark
5.1], for 0 ≤ t ≤ 1
4b2 ,

(12)

4b2

(cid:0) 1

∞(cid:88)
∞(cid:88)
+(cid:0)4b2t(cid:107)v(cid:107)2(cid:1)3
+(cid:0)4b2t(cid:107)v(cid:107)2(cid:1)3 E(cid:104)

(cid:1)k(cid:0)4b2t(cid:107)v(cid:107)2(cid:1)k E(cid:2)X 2k(cid:3)
(cid:0) 1
(cid:1)k E(cid:2)X 2k(cid:3)
4b2 X 2(cid:105)

k=3

k=3

k!

k!

4b2

e

1

√
2b6t3 (cid:107)v(cid:107)6

+ 64
√

2b6t3 (cid:107)v(cid:107)6

2

≤ 1 + t(cid:107)v(cid:107)2 + t2 (cid:107)v(cid:107)4 E [X 4]
≤ 1 + t(cid:107)v(cid:107)2 + t2 (cid:107)v(cid:107)4 E [X 4]
≤ 1 + t(cid:107)v(cid:107)2 + t2 (cid:107)v(cid:107)4 E [X 4]
≤ 1 + t(cid:107)v(cid:107)2 + 8b4t2 (cid:107)v(cid:107)4 + 64
√
≤ et(cid:107)v(cid:107)2+8b4t2(cid:107)v(cid:107)4+64
,

2b6t3(cid:107)v(cid:107)6

2

2

using (12) in the fourth inequality and [43, p.93] in the ﬁfth inequality. Let t(cid:63) =
where q > 1. Assuming (cid:107)v(cid:107)2 (cid:29) δ, we have that

δ

8b4(cid:107)v(cid:107)4q

√
(cid:63)(cid:107)v(cid:107)4+64

2b6t3

(cid:63)(cid:107)v(cid:107)6

δ2

8b4(cid:107)v(cid:107)4q2 +

= e

√

2

δ3

8b6(cid:107)v(cid:107)6q3 .

Then

E(cid:2)et(cid:63)T(cid:3) ≤ e8b4t2
I (δ) ≥ δt(cid:63) − ln E(cid:2)et(cid:63)T(cid:3) ≥
(cid:0)st2(cid:1)k

E(cid:2)etζ(cid:3) =

∞(cid:88)

1
s

(2k)!

k=0

(cid:18)

(cid:19)

1 − 1
q
(cid:0)st2(cid:1)

∞(cid:88)

2kk!

k=0

√

2

−

δ2

8b4 (cid:107)v(cid:107)4 q

δ3

8b6 (cid:107)v(cid:107)6 q3

≥ c

δ
(cid:107)v(cid:107)4 ,

† Using the inequality (2k)! ≥ 2kk! and the Taylor expansion around 0, we have that for t ∈ (0, 1]

≤ 1
s

s

2 t2

e

= e− ln s+ s

2 t2 ≤ e−t2 ln s+ s
2 t2

.

=

1
s

Randomized Misﬁt Approach for Data Reduction

where 0 < c < 1

8b4 . Taking 2θ2 = (cid:107)v(cid:107)4 concludes the proof.

9

A sharper result can be obtained for RMA constructed with b-subgaussian random
variables where b ≤ 1. Note that this includes the distribution (10) with s = 1
(Rademacher) and s = 3 (Achlioptas) by the above theorem. Following [38, (5)], let
g be a standard Gaussian random variable, independent of all other random variables.
Then, we have that for 0 < t < 1

2(cid:107)v(cid:107)2 ,

(cid:35)

eb2t(cid:107)v(cid:107)2w2
i g2

≤ Eg

(cid:104)

et(cid:107)v(cid:107)2g2(cid:105)

1(cid:113)

=

.

1 − 2t(cid:107)v(cid:107)2

(cid:34) N(cid:89)
E(cid:104)
et(cid:107)v(cid:107)2X(cid:105) ≤ Eg
E(cid:2)etT (u,r)(cid:3) ≤

i

So from (11) we have that

(cid:113)

e−t(cid:107)v(cid:107)2
1 − 2t(cid:107)v(cid:107)2

−t(cid:107)v(cid:107)2− 1

2 ln(1−2t(cid:107)v(cid:107)2).

= e

ln(cid:0)1 − 2t(cid:107)v(cid:107)2(cid:1) =: f (t) .

1
2

Then

tδ − ln (E [T (u, r)]) ≥ tδ + t(cid:107)v(cid:107)2 +

Computing the derivative, we have that f (t) attains a maximum at

Thus, we have

max f (t) =

=

=

δ

δ

δ2

tmax =

2(cid:0)(cid:107)v(cid:107)4 + δ (cid:107)v(cid:107)2(cid:1) .
2(cid:0)(cid:107)v(cid:107)2 + δ(cid:1) +
2(cid:0)(cid:107)v(cid:107)4 + δ (cid:107)v(cid:107)2(cid:1) +
2(cid:0)(cid:107)v(cid:107)4 + δ (cid:107)v(cid:107)2(cid:1) − 1
(cid:0)(cid:107)v(cid:107)2 + δ(cid:1)2 − 1
(cid:40)
(cid:0)(cid:107)v(cid:107)4 + δ (cid:107)v(cid:107)2(cid:1) −
4(cid:0)(cid:107)v(cid:107)4 + δ (cid:107)v(cid:107)2(cid:1) +
(cid:0)(cid:107)v(cid:107)2 + δ(cid:1)3 − ··· ≥ c

δ2
(cid:107)v(cid:107)4 ,

1
2

1
4

δ2

δ2

δ2

δ2

δ3

ln

6

4

− 1
6

δ

1 −

(cid:107)v(cid:107)2 + δ

(cid:18)
(cid:19)
(cid:0)(cid:107)v(cid:107)2 + δ(cid:1)3 − ···
(cid:41)
(cid:0)(cid:107)v(cid:107)2 + δ(cid:1)2

δ2

δ3

where we employed the Taylor expansion in the second equality, and in the last inequality
c is some constant less than 1/4. Note that the last inequality holds for δ (cid:28) (cid:107)v(cid:107)2 and
taking 2θ2 = (cid:107)v(cid:107)4 concludes the proof.

The next theorem is our main result. It guarantees with high probability that the
RMA solution will be a solution of the original problem under Morozov’s discrepancy
principle, for relatively small n. We ﬁrst need the following lemma.

Lemma 1 Let v :=(cid:98)d−(cid:98)F (u). Suppose that r is distributed such that the large deviation

√
2θ2 for some c > 0 and θ = (cid:107)v(cid:107)2 /

rate of the RMA error is bounded below by c δ2
a cost distortion tolerance ε > 0 and a failure rate β > 0, let

2. Given

(13)

n ≥ β
cε2 .

Randomized Misﬁt Approach for Data Reduction
Then with probability at least 1 − e−β,
(1 − ε)(cid:107)v(cid:107)2 ≤ 1
n

n(cid:88)

(cid:0)r(cid:62)
j v(cid:1)2 ≤ (1 + ε)(cid:107)v(cid:107)2 ,

j=1

10

(14)

and hence,

(1 − ε) J (u) ≤ Jn (u; r) ≤ (1 + ε) J (u) .

(15)

Proof. The proof follows from setting δ = ε(cid:107)v(cid:107)2 in (7).
√
This lemma demonstrates a remarkable fact that with n i.i.d. draws one can reduce
the data dimension from N to n while bearing a relative error of ε = O (1/
n) in
the cost function, where the reduced dimension n is independent of the dimension N
of the data. This idea is the basis for data-reduction techniques via variants of the
Johnson-Lindenstrauss Lemma in the current research area of random projections (see,
e.g. [44, 45, 46] for examples of recent applications).

Unlike other applications of the Monte Carlo method, e.g. Markov chain Monte
Carlo, in which n must be large to converge, n can be moderate or small for inverse
problems, depending on the noise η in (2). In the following theorem we show this is
possible via Morozov’s discrepancy principle [47]. To avoid over-ﬁtting the noise, from
(1) one seeks a MAP point u(cid:63) such that |dj − w (xj; u(cid:63))| ≈ σ, i.e.
We say that an inverse solution u(cid:63) satisﬁes Morozov’s discrepancy principle with
parameter τ if

(cid:13)(cid:13)(cid:13)(cid:98)d −(cid:98)F (u(cid:63))

(cid:13)(cid:13)(cid:13)2 ≈ N .

(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:98)d −(cid:98)F (u(cid:63))

= τ N

for some τ ≈ 1.
Theorem 2 Suppose that the conditions of Lemma 1 are met. If u(cid:63)
satisfying solution for the RMA cost, i.e.,

n is a discrepancy-

Jn (u(cid:63)

n, r) :=

1
n

(cid:17)(cid:105)2

n)

= τ(cid:48)N

n(cid:88)

j=1

r(cid:62)

j

(cid:104)

(cid:16)(cid:98)d −(cid:98)F (u(cid:63)
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:98)d −(cid:98)F (u(cid:63)

n)

= τ N.

for some τ(cid:48) ≈ 1, then with probability at least 1− e−β, u(cid:63)
problem that satisﬁes Morozov’s discrepancy principle with parameter τ , i.e.

n is also solution for the original

for τ ∈(cid:2) τ(cid:48)

1+ε , τ(cid:48)
1−ε

(cid:3).

J (u(cid:63)

n) :=

Proof. The claim is a direct consequence of (14).
We now in the position to show a diﬀerent proof of the Johnson-Lindenstrauss
embedding theorem using a stochastic programming derivation of the RMA. Following
[48], we deﬁne a map S from Rn to RN , where n (cid:28) N , to be a Johnson-Lindenstrauss
transform (or JLT) if

(1 − ε)(cid:107)v(cid:107)2 ≤ (cid:107)Sv(cid:107)2 ≤ (1 + ε)(cid:107)v(cid:107)2 ,

holds with some probability p = p (n, ε), where ε > 0.

Randomized Misﬁt Approach for Data Reduction

11

Theorem 3 (Johnson-Lindenstrauss embedding theorem [36, 37, 38]) Suppose
that r is distributed such that the large deviation rate of the RMA error is bounded be-
low by c δ2
n = O (ε−2 ln m). Then there exists a map F : RN → Rn such that

2θ2 for some c > 0 and some θ. Let 0 < ε < 1, vi ∈ RN , i = 1, . . . , m, and

(1 − ε)(cid:107)vi − vj(cid:107)2 ≤ (cid:107)F (vi) − F (vj)(cid:107)2 ≤ (1 + ε)(cid:107)vi − vj(cid:107)2

(16)
Proof. The conditions of Lemma 1 hold, thus for a given v ∈ RN , note that (14) is

∀i, j.

(1 − ε)(cid:107)v(cid:107)2 ≤ (cid:107)Σv(cid:107)2 ≤ (1 + ε)(cid:107)v(cid:107)2 ,

(17)

equivalent to

where

(18)

Σ :=

1√
n

[r1, . . . , rn]

(cid:62)

.

Deﬁne F (v) := Σv.
(vi, vj) with probability at least 1 − e− c
(16) holds for any pair with probability at least 1 − m−α if n ≥ c (2+α)
As discussed above, Jn (u; r) is an unbiased estimator of J (u).

Inequality (16) is then a direct consequence of (17) for a pair
2 nε2. Using an union bound over all pairs, claim

reasonable to expect that J (cid:63)
following result [16, Propositions 5.2 and 5.6] states that under mild conditions J (cid:63)
fact converges to J (cid:63). It is not unbiased, but is however downward biased.

It is therefore
n := minu Jn (u; r) converges to J (cid:63) := minu J (u). The
n in

ln m.

ε2

Proposition 2 Assume that Jn (u; r) converges to J (u) with probability 1 uniformly in
u, then J (cid:63)

n converges to J (cid:63) with probability 1. Furthermore, it holds that

n] ≤ E(cid:2)J (cid:63)

n+1

(cid:3) ≤ J (cid:63),

E [J (cid:63)

that is, J (cid:63)

n is a downward-biased estimator of J (cid:63).

Stochastic programming theory gives a stronger

characterization of
n converges weakly to u(cid:63) with the rate of n− 1

this
convergence. One can show that u(cid:63)
2 . If
J (u) is convex with ﬁnite value, then u(cid:63)
n = u(cid:63) with probability exponentially converging
to 1. See Chapter 5 in [16] for details. For a linear forward map F (u) = Fu, that is,
J (u) is quadratic, we can derive a bound on the solution error using the spectral norm
of F.

Theorem 4 Suppose the conditions of Lemma 1 hold. Let m := rank((cid:98)F). Then

i) (1 − ε) J (cid:63) ≤ J (cid:63)
ii) if F is linear, then with probability at least 1 − m−α

n ≤ (1 + ε) J (cid:63), and

(cid:16)(cid:13)(cid:13)(cid:13)(cid:98)F
(cid:13)(cid:13)(cid:13)(cid:107)u(cid:63)(cid:107) +

(cid:13)(cid:13)(cid:13)(cid:98)d

(cid:13)(cid:13)(cid:13)(cid:17)(cid:13)(cid:13)(cid:13)(cid:98)F
(cid:13)(cid:13)(cid:13) ,

ε

(cid:107)u(cid:63)

n − u(cid:63)(cid:107) ≤

(cid:16)(cid:98)F(cid:62)ΣΣ(cid:62)(cid:98)F + C−1(cid:17) 1

σ2
min (G)
2 , and n = O (ε−2 (2 + α) ln r).

where G :=

Randomized Misﬁt Approach for Data Reduction

12

Proof. The ﬁrst assertion follows from (15) and the deﬁnition of u(cid:63)

n (6), indeed

J (cid:63)
n = Jn (u(cid:63)

n) ≤ J (u(cid:63)) ≤ (1 + ε) J (u(cid:63)) = (1 + ε) J (cid:63),

and the other direction is similar. For the second assertion, note that u(cid:63) and u(cid:63)
solutions of the following ﬁrst optimality conditions

n are

(cid:16)(cid:98)F(cid:62)(cid:98)F + C−1(cid:17)
(cid:16)(cid:98)F(cid:62)ΣΣ(cid:62)(cid:98)F + C−1(cid:17)

u(cid:63) =(cid:98)F(cid:62)(cid:98)d + C−1u0,
n =(cid:98)F(cid:62)ΣΣ(cid:62)(cid:98)d + C−1u0.

u(cid:63)

n. An algebraic manipulation of (19) gives

Taking the inner product of both sides with ∆ we have

(cid:68)

Deﬁne ∆ := u(cid:63) − u(cid:63)

(cid:16)(cid:98)F(cid:62)ΣΣ(cid:62)(cid:98)F + C−1(cid:17)
(cid:16)(cid:98)FT ΣΣT(cid:98)F + C−1(cid:17)
(cid:68)

∆

∆,

∆ =

(cid:69)

=

Then we can bound the left-hand side of (20):

(cid:16)(cid:98)F(cid:62)ΣΣ(cid:62)(cid:98)F + C−1(cid:17)

∆,

u(cid:63) +(cid:98)F(cid:62)(cid:98)d −(cid:98)F(cid:62)ΣΣ(cid:62)(cid:98)d,
(cid:68)(cid:98)F∆,(cid:98)d − ΣΣT(cid:98)d

+

(cid:17)
(cid:16)(cid:98)F(cid:62)ΣΣ(cid:62)(cid:98)F −(cid:98)F(cid:62)(cid:98)F
(cid:68)(cid:98)F∆, ΣΣT(cid:98)Fu(cid:63) −(cid:98)Fu(cid:63)(cid:69)
(cid:69) ≥ σ2
(cid:13)(cid:13)ΣΣ(cid:62)v − v(cid:13)(cid:13) ≤ ε(cid:107)v(cid:107) .
(cid:13)(cid:13)(cid:13)2 (cid:107)∆(cid:107)(cid:107)u(cid:63)(cid:107) ,
(cid:13)(cid:13)(cid:13)(cid:98)F
(cid:13)(cid:13)(cid:13)(cid:98)F
(cid:13)(cid:13)(cid:13)(cid:107)∆(cid:107)(cid:13)(cid:13)(cid:13)(cid:98)d
(cid:13)(cid:13)(cid:13) ,

min (G) ∆2.

∆

(19a)

(19b)

(cid:69)

.

(20)

(21)

(22)

(23a)

(23b)

To bound terms on right hand side of (20), we need the following straightforward variant
of (17), i.e. ∀v ∈ RN and n = O (ε−2):

Using Cauchy-Schwarz inequality we have

(cid:68)(cid:98)F∆, ΣΣT(cid:98)Fu(cid:63) −(cid:98)Fu(cid:63)(cid:69) ≤ ε
(cid:69) ≤ ε
(cid:68)(cid:98)F∆,(cid:98)d − ΣΣT(cid:98)d

where we have used (22) and deﬁnition of matrix norm. Next, combining (23) and (21)
ends the proof.

that (22) is valid for m basis vectors spanning the column space of (cid:98)F, and hence

Note that for inequalities in (23) to be valid, it is suﬃcient to choose n, α, ε such

n = O (ε−2 (2 + α) ln m) by the union bound.
Remark 1 The bound in (18) is not a unique estimation. One can ﬁrst rewrite J (u)
and Jn (u; r) as

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:34)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:34)

(cid:98)d

C−1/2u0
Σ(cid:62) 0
I
0

(cid:34)

(cid:35)
(cid:35)(cid:40)(cid:34)

−

(cid:98)F
(cid:98)d

C−1/2

C−1/2u0

(cid:35)
(cid:35)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:34)

u

−

J (u)

=

Jn (u; r) =

1
2

1
2

,

(cid:98)F

C−1/2

(cid:35)

(cid:41)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

.

u

Randomized Misﬁt Approach for Data Reduction

If Σ is a Johnson-Lindenstrauss transform, then S :=

(cid:34)

(cid:35)

Σ(cid:62) 0
I
0

13

is also a JLT

with the same parameters:

(cid:34)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)S

(cid:35)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

v
w

= (cid:107)Σv(cid:107)2 + (cid:107)w(cid:107)2 ≤ (1 + ε)(cid:107)v(cid:107)2 + (cid:107)w(cid:107)2 ≤ (1 + ε)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:34)

v
w

(cid:35)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

.

Applying [48, Theorem 12], we conclude that with probability at least 1/3,

(cid:107)u(cid:63)

n − u(cid:63)(cid:107) ≤ ε
λmin

√

J (cid:63),

(cid:104) (cid:98)F(cid:62), C−1/2 (cid:105)(cid:62)

.

where λmin is the minimum nonzero singular value of

3. Numerical Experiments

In this section we evaluate the performance of the randomized misﬁt approach using
√
four diﬀerent distributions for r in (5). We also verify that the convergence is indeed
O(1/

n) as guaranteed by Theorem 3. Lastly we verify Theorem 2.

The distributions that we test with the randomized misﬁt approach are:

• Gaussian
• Rademacher
• Achlioptas
• 95%-sparse (s-sparse (10) with s = 20)

There are a number of other distributions suitable for RMA in the current
literature on Johnson-Lindenstrauss transforms that we do not consider, particularly
the Subsampled Randomized Hadamard Transform of [49, 50] and its subsequent fast
and sparse variants. These will be tested in future work.

For our model problem we consider the estimation of a coeﬃcient in an elliptic
partial diﬀerential equation. This Poisson-type problem arises in various inverse
applications, such as the heat conductivity or groundwater problem, or in ﬁnding a
membrane with a given spatially-varying stiﬀness.

For concreteness we consider the heat conduction problem on an open bounded

domain Ω, governed by

− ∇ · (eu∇w) = 0
−eu∇w · n
−eu∇w · n

= Bi w
= −1

in Ω
on ∂Ω \ ΓR,
on ΓR,

(24)

where u is the logarithm of distributed thermal conductivity, w is the forward state
(the map from log conductivity to temperature), n is the unit outward normal on ∂Ω,
and Bi is the Biot number. Here, ΓR is a portion of the boundary ∂Ω on which the

Randomized Misﬁt Approach for Data Reduction

14

inﬂow heat ﬂux is 1. The rest of the boundary is assumed to have Robin boundary
condition. We are interested in reconstructing the distributed log conductivity u, given
some temperature measurements w observed on Ω.

The standard H 1 (Ω) ﬁnite element method is used to discretize the misﬁt and
the regularization operator. The synthetic truths that we seek to recover are a 1-
D sinusoidal curve, a 2-D Gaussian on a thermal ﬁn, and a cube with non-zero log
conductivity values on a sphere in the center and semispheres in the opposing corners.
Figure 1 shows representations of utruth on a mesh for these cases.

The synthetic noisy temperature observations are then generated at all mesh points
through the forward model (24). The misﬁt vector generated from 1(a) has data
dimension N = 1025 (with 1% percent added noise), from 1(b) has data dimension
N = 1333 (with .1% percent added noise), and from 1(c) has data dimension N = 2474
(with .2% percent added noise), respectively.

(a) truth u for 1D experiment

(b) truth u for 2D experiment

(c) truth u for 3D experiment

Figure 1. The distributed truth log conductivity parameters used in the experiments.
The parameter ﬁelds are used to obtain noise-corrupted temperature data through the
forward model (24).

For the optimization algorithm we use a subspace trust region inexact Newton
conjugate gradient method developed in [51, 17, 52, 53, 54]. The stopping criteria used

x00.51u-2-1012-0.0500.050.10.150.20.250.30.35Randomized Misﬁt Approach for Data Reduction

15

is when either the Newton step size, cost function value, or norm of the gradient is below
10−6.

3.1. Convergence results

We ﬁrst verify the convergence of the RMA cost Jn (u0) to the original cost J (u0)
for a ﬁxed distributed parameter u0, using the model heat problem (24). We choose
a random u0 from the prior distribution and construct the RMA cost Jn (u0) with
the various random projections listed above.
Since u0 lives in high-dimensional
space Rm, where m is the number of ﬁnite element nodal values, for the purpose of
visualization Figure 2 shows plots of the RMA cost ˆJn (κ) := Jn (u0 + κs) in a direction
s := ∇J (u0) for the 3D example. For each of the random projections tested we observe
convergence of ˆJn (κ) to ˆJ (κ) as n increases. More importantly, for all distributions,
the minimizer of ˆJ (κ) is well-approximated by ˆJn (κ), even for n small, thus verifying
Theorem 2. The plots for the 1D and 2D examples are similar and thus omitted (see
http://users.ices.utexas.edu/~ellenle/RMAplots.pdf).

Theorem 4 states that u(cid:63)

n, the minimizer of Jn, and the minimum objective function
value J (cid:63)
n converge at the same rate, given by the distortion tolerance ε, but with diﬀerent
n may converge quickly to ˆu(cid:63),
constants. Figure 2 illustrates how an RMA solution ˆu(cid:63)
although convergence of the minimum value ˆJn( ˆun) to ˆJ(ˆu) can be slow due to the
diﬀerent constant. To test this hypothesis at the actual minimizer u(cid:63)
n, we plot the error
n and its corresponding optimal value J (cid:63)
of the RMA MAP point u(cid:63)
n in Figure 3 for the
3D example and diﬀerent random projections‡. Both the absolute errors |J (cid:63)
n − J (cid:63)| and
n − u(cid:63)(cid:107) /(cid:107)u(cid:63)(cid:107) are shown, and a
(cid:107)u(cid:63)
√
√
n for both
n is plotted to show the convergence rate is indeed 1/
reference curve 1/
u(cid:63)
n and J (cid:63)
n is orders of magnitude
smaller than J (cid:63)

n. However as can be seen, the absolute error of u(cid:63)

n for all considered random projections.

n − u(cid:63)(cid:107) and normalized errors |J (cid:63)

n − J (cid:63)|/|J (cid:63)| and (cid:107)u(cid:63)

The results of minimizing the RMA cost with diﬀerent n in the 1D, 2D, and 3D
example are shown alongside the true MAP estimate u(cid:63) in Figures 4, 5 and 6. The ﬁgures
shown are results with r distributed by the Achlioptas distribution (66% sparse). We
see that the original MAP point u(cid:63) is well-approximated by the RMA solution u(cid:63)
n in all
cases with 50 ≤ n ≤ 100.

In a diﬀerent experiment, we consider a 3D example in which only surface
observations are available. The parameters are the same as the problem in Figure 1(c)
but the data are now obtained from 901 observations on the surface of the cube (except
the bottom surface), and the truth log conductivity is non-zero within the sphere of
radius 0.5 centered at the origin as seen in Figure 7. Figure 8(d) shows the original
MAP estimate u(cid:63). Compared to the above example the recovery is poorer, but this
is expected due to having less observational data. Our interest however is in reducing
the data dimension while recovering a reasonable MAP estimation. Subsequently, we
compare the RMA MAP point u(cid:63)
n to the true MAP point u(cid:63) (a minimizer of J). The
‡ Again, similar results have been obtained for the 1D and 2D examples and are omitted here.

Randomized Misﬁt Approach for Data Reduction

16

(a) Gaussian

(b) Rademacher

(c) Achlioptas

(d) 95% sparse

(e) all distributions, n = 50

Figure 2. Convergence of the RMA cost to the original cost for various random
projections in the 3D example at a random parameter u0 drawn from the prior
distribution. G = Gaussian, R = Rademacher, A = Achlioptas, S = 95%-sparse

5#10-3-202costJ(u0+5s)1021041061081010J1J10J25J50Jmins5#10-3-202costJ(u0+5s)1021041061081010J1J10J25J50Jmins5#10-3-202costJ(u0+5s)1021041061081010J1J10J25J50Jmins5#10-3-202costJ(u0+5s)1021041061081010J1J10J25J50Jmins5#10-3-202costJ(u0+5s)#106024681012J50;r9GJ50;r9RJ50;r9AJ50;r9SJminsRandomized Misﬁt Approach for Data Reduction

17

(a) Gaussian

(b) Rademacher

(c) Achlioptas

(d) 95% sparse

Figure 3. Error convergence of the minimizer u(cid:63)
n and the corresponding optimal cost
J (cid:63)
n. Both absolute and relative errors are shown. Data shown is an average of ﬁve runs.
√
Theorem 4 states that u(cid:63)
n converge at the same rate, given by the distortion
tolerance ε = O(1/

n), but with diﬀerent constants.

n and J (cid:63)

results in Figure 8 show the RMA solutions u(cid:63)
n = 150, i.e. a 6-fold reduction in the data dimension, the RMA approximation u(cid:63)
still a good approximation to the original MAP solution u(cid:63).

n as n increases. As can be seen, with
150 is

3.2. Veriﬁcation of Theorem 2

Table 1 presents results for solving the model problem for the 1D, 2D, and 3D
examples with Morozov’s criterion, again using the Achlioptas random projection in
the randomized misﬁt approach. We perform several numerical experiments and choose
n) with τ(cid:48) ≈ 1. We
an n for each example such that Morozov’s principle is met for Jn (u(cid:63)
then compute the corresponding ranges for τ that are guaranteed with probability at
least p ≥ 1 − e−β, after choosing an acceptable cost distortion tolerance of ε = 0.5 and
β as large as possible from (13). As can be seen, evaluating J (u(cid:63)
n) gives a τ within

n050100150200Error10-210-11001011021031041=pnju?!u?njjJ?!J?njju?!u?nj=ju?jjJ?!J?nj=J?n050100150200Error10-210-11001011021031041=pnju?!u?njjJ?!J?njju?!u?nj=ju?jjJ?!J?nj=J?n050100150200Error10-210-11001011021031041=pnju?!u?njjJ?!J?njju?!u?nj=ju?jjJ?!J?nj=J?n050100150200Error10-210-11001011021031041=pnju?!u?njjJ?!J?njju?!u?nj=ju?jjJ?!J?nj=J?Randomized Misﬁt Approach for Data Reduction

18

(a) n = 5

(b) n = 10

(c) n = 50

(d) n = 100

(e) n = 300

(f) n = 600

1D elliptic PDE example:

n to u(cid:63) as n increases.
Figure 4.
The Achlioptas random projection (66% sparse) is used for Σ and the original data
dimension is N = 1025.

convergence of u(cid:63)

the speciﬁed range, which satisﬁes Morozov’s criterion. That is, even for moderately
small values of n, if the discrepancy principle is satisﬁed for Jn (u(cid:63)
n), then the discrepancy
principle is also satisﬁed for J (u(cid:63)
n is a discrepancy principle satisfying solution
for both the randomized reduced data dimension problem (5) and the original problem
(4).

n). Thus u(cid:63)

Table 1. Veriﬁcation of Morozov’s discrepancy principle for the RMA solution with
ε = 0.5.

N
1D 1025
2D 1333
3D 2474

n Jn (u(cid:63)
n)
100
1220
1240
50
75
2646

τ(cid:48)
1.190
0.930
1.070

p J (u(cid:63)
n)
1074
1406
3928

95.6%
79.0%
90.4%

τ
1.048
1.055
1.588

(cid:3)

(cid:2) τ(cid:48)
1+ε , τ(cid:48)
1−ε
[0.793, 2.380]
[0.620, 1.860]
[0.713, 2.139]

4. Conclusions and future work

We have presented a randomized misﬁt approach as a general framework for reduction
of the data dimension in large-scale inverse problems. The method builds on recent
eﬀorts to reduce the computational complexity of large-scale inverse problems via
randomization, but applies the random reduction directly to the data misﬁt vector. Any

x00.51u-2-1.5-1-0.500.511.5u?u?5x00.51u-2-1.5-1-0.500.511.5u?u?10x00.51u-2-1.5-1-0.500.511.5u?u?50x00.51u-2-1.5-1-0.500.511.5u?u?100x00.51u-2-1.5-1-0.500.511.5u?u?300x00.51u-2-1.5-1-0.500.511.5u?u?600Randomized Misﬁt Approach for Data Reduction

19

(a) n = 30

(b) n = 50

(c) n = 100

(d) u(cid:63)

2D elliptic PDE example:

n to u(cid:63) as n increases.
Figure 5.
The Achlioptas random projection (66% sparse) is used for Σ and the original data
dimension is N = 1333.

convergence of u(cid:63)

subgaussian distribution guarantees the solution obtained from the randomized misﬁt
approach will satisfy Morozov’s discrepancy principle with a low failure rate (that decays
exponentially with respect to the reduced dimension n).

By a stochastic approximation, we have shown that the method is equivalent to
applying a random projection to the data misﬁt vector. This leads to a diﬀerent
stochastic programming-based proof (up to a constant) of a Johnson-Lindenstrauss
lemma variant proved previously (see, e.g.
[55, 56] for proofs based on combinatorics
and communication theory, respectively). Our connection provides two main theoretical
insights. The ﬁrst is intuition into the numerical accuracy of previous work that uses
a surprisingly small reduced data dimension n using Morozov’s discrepancy principle.
These methods may be understood as random projection methods, for which this high-
accuracy dimension reduction property is well-established and drives an active area of
√
research in computer science and machine learning. The second is an intuition into the
ubiquitous O(1/
n) factor in Johnson-Lindenstrauss transforms (a rate shown to be

-0.0500.050.10.150.20.250.30.35-0.0500.050.10.150.20.250.30.35-0.0500.050.10.150.20.250.30.35-0.0500.050.10.150.20.250.30.35Randomized Misﬁt Approach for Data Reduction

20

(a) n = 50

(b) n = 100

(c) n = 150

(d) u(cid:63)

3D elliptic PDE example:

n to u(cid:63) as n increases.
Figure 6.
The Achlioptas random projection (66% sparse) is used for Σ and the original data
dimension is N = 2474.

convergence of u(cid:63)

Figure 7.
truth u for 3D experiment with surface observations: Using the same
number of mesh elements as in Figure 1(c) except the synthetic parameter is a single
sphere, and observational data is obtained from N = 901 mesh points on the top and
side surfaces of the cube.

tight by [55]) using a Monte Carlo framework.

The focus of this work is on the presentation and analysis of the method. We
presented results for a medium size (N = O(103)) synthetic example in 1D, 2D, and
3D with four diﬀerent distributions for numerical justiﬁcation of theoretical results and

Randomized Misﬁt Approach for Data Reduction

21

(a) n = 10

(b) n = 50

(c) n = 150

(d) u(cid:63) (solution of the full cost)

Figure 8. 3D elliptic PDE example with surface observations: convergence of u(cid:63)
n to
u(cid:63) as n increases. The MAP solution is nearly approximated with an RMA reduced
data dimension of n = 150 (a 6-fold reduction from the N = 901 observational data
points on the surface). The Achlioptas random projection (66% sparse) is used for Σ.

illustration of the method. Future work will apply the method to larger problems with
big data, e.g. time dependent data governed by complex forward models, and extend
it to a Bayesian framework. Our results are valid for nonlinear inverse problems with
the exception of part (ii) in Theorem 4 (which only applies to linear forward models).
We expect such a result is also true for nonlinear inverse problems, and this is under
investigation. Combining dimension reduction and uncertainty quantiﬁcation is the
broader focus of our current research towards developing scalable methods for large-
scale inverse problems in high-dimensional parameter space with big data.

Acknowledgments

We would like to thank Prof. Mark Girolami for pointing out the similarity between
randomized projections and the randomized misﬁt approach, which led to the connection
with Johnson-Lindenstrauss theory. This in turn allowed us to carry out the analysis of
the randomized misﬁt approach presented here. We also thank Vishwas Rao for careful
proofreading. This research was partially supported by DOE grants de-sc0010518 and
de-sc0011118. We are grateful for the support.

Randomized Misﬁt Approach for Data Reduction

22

References

[1] Bui-Thanh T and Ghattas O 2012 Inverse Problems 28 055001
[2] Bui-Thanh T and Ghattas O 2012 Inverse Problems 28 055002
[3] Bui-Thanh T and Ghattas O 2013 Analysis of the Hessian for inverse scattering problems. Part

III: Inverse medium scattering of electromagnetic waves Inverse Problems and Imaging

[4] Vexler B 2004 Adaptive ﬁnite element methods for parameter identiﬁcation problems Ph.D. thesis

University of Heidelberg

[5] Prato G D 2006 An Introduction to Inﬁnite-dimensional Analysis Universitext (Springer)
[6] Prato G D and Zabczyk J 1992 Stochastic Equations in Inﬁnite Dimensions (Cambidge University

Press)

[7] Stuart A M 2010 Acta Numerica 19 451–559
[8] Dashti M, Law K J, Stuart A M and Voss J 2013 Inverse Problems 29 095017
[9] Franklin J N 1970 Journal of Mathematical Analysis and Applications 31 682–716
[10] Lehtinen M S, Paivarinta L and Somersalo E 1989 Inverse Problems 5 599
[11] Lasanen S 2002 Discretizations of generalized random variables with applications to inverse

problems Ph.D. thesis University of Oulu

[12] Piiroinen P 2005 Statistical measurements, experiments, and applications Ph.D. thesis Department

of Mathematics and Statistics, University of Helsinki

[13] Bui-Thanh T, Ghattas O, Martin J and Stadler G 2013 SIAM Journal on Scientiﬁc Computing

35 A2494–A2523

[14] Petra N, Martin J, Stadler G and Ghattas O 2014 SIAM Journal on Scientiﬁc Computing
[15] Nemirovski A, Juditsky A, Lan G and Shapiro A 2009 SIAM Journal on Optimization 19 1574–

1609

[16] Shapiro A, Dentcheva D and Ruszczynski A 2009 Lectures on Stochastic Programming: Modeling

and Theory (Society for Industrial and Applied Mathematics)

[17] Branch M A, Coleman T F and Li Y 1999 SIAM Journal on Scientiﬁc Computing 21 1–23

(electronic)

[18] Halko N, Martinsson P G and Tropp J A 2011 SIAM Review 53 217–288
[19] Martinsson P G, Rokhlin V and Tygert M 2011 Applied and Computational Harmonic Analysis

30 47–68 ISSN 10635203

[20] Isaac T, Petra N, Stadler G and Ghattas O 2015 Journal of Computational Physics 296 348–368
[21] Xiang H and Zou J 2015 Inverse Problems 31 085008
[22] Xiang H and Zou J 2013 Inverse Problems 29 085008
[23] Chaillat S and Biros G 2012 Journal of Computational Physics 231 4403–4421
[24] Lee J and Kitanidis P 2014 Water Resources Research 50 5410–5427
[25] Kitanidis P and Lee J 2014 Water Resources Research 50 5428–5443
[26] Saibaba A K and Kitanidis P K 2015 Advances in Water Resources 82 124–138 ISSN 03091708
[27] Alexanderian A, Petra N, Stadler G and Ghattas O 2014 SIAM Journal on Scientiﬁc Computing

36 A2122–A2148

[28] Bui-Thanh T and Girolami M A 2014 Inverse Problems Special Issue 114014
[29] Bui-Thanh T, Burstedde C, Ghattas O, Martin J, Stadler G and Wilcox L C 2012 Extreme-scale
UQ for Bayesian inverse problems governed by PDEs SC12: Proceedings of the International
Conference for High Performance Computing, Networking, Storage and Analysis

[30] Saibaba A K, Lee J and Kitanidis P K 2015 Numerical Linear Algebra with Applications ISSN

1099-1506

[31] Krebs J R, Anderson J E, Hinkley D, Neelamani R, Lee S, Baumstein A and Lacasse M D 2009

Geophysics 74 WCC177–WCC188

[32] Haber E, Chung M and Herrmann F 2012 SIAM Journal on Optimization 22 739–757
[33] Young J and Ridzal D 2012 SIAM Journal on Scientiﬁc Computing 34 A2344–A2365
[34] Hutchinson M F 1990 Communications in Statistics-Simulation and Computation 19 433–450

Randomized Misﬁt Approach for Data Reduction

23

[35] Achlioptas D 2003 Journal of Computer and System Sciences 66 671–687 ISSN 00220000
[36] Dirksen S 2015 Foundations of Computational Mathematics
[37] Matousek J 2008 Random Struct. Algorithms 33 142–156
[38] Indyk P and Naor A 2007 ACM Transactions on Algorithms (TALG) 3 31
[39] van Leeuwen T, Aravkin A Y and Herrmann F J 2011 International Journal of Geophysics 2011
[40] Touchette H 2009 Physics Reports 478 1–69
[41] Kelly F P 1991 Queueing systems 9 5–15
[42] Li P, Hastie T J and Church K W 2006 Proceedings of the 12th ACM SIGKDD international

conference on Knowledge discovery and data mining - KDD ’06 287

[43] Stroock D W 2011 Probablity Theory: An Analytic View 2nd ed (Cambrdige University Press,

Cambridge)

[44] Holub V and Fridrich J 2013 Information Forensics and Security, IEEE Transactions on 8 1996–

2006

[45] Liu L, Fieguth P, Clausi D and Kuang G 2012 Pattern Recognition 45 2405–2418
[46] Fowler J E and Du Q 2012 Image Processing, IEEE Transactions on 21 184–195
[47] Morozov V A 1966 Soviet Math. Dokl.
[48] Sarlos T 2006 2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS’06)

143–152 ISSN 0272-5428

[49] Ailon N and Chazelle B 2009 SIAM Journal of Computing 39 302–322
[50] Tropp J A 2010 Advances in Adaptive Data Analysis 03 8 ISSN 1793-5369
[51] Coleman T F and Li Y 1996 SIAM Journal on Optimization 6 418–445
[52] Heinkenschloss M, Ulbrich M and Ulbrich S 1999 Mathematical Programming 86 615–635
[53] Bui-Thanh T 2007 Model-Constrained Optimization Methods for Reduction of Parameterized

Large-Scale Systems Ph.D. thesis Department of Aeronautics and Astronautics, MIT

[54] Bui-Thanh T, Willcox K and Ghattas O 2008 SIAM Journal on Scientiﬁc Computing 30 3270–

3288

[55] Alon N 2003 Discrete Mathematics 273 31–53
[56] Jayram T and Woodruﬀ D P 2013 ACM Transactions on Algorithms (TALG) 9 26

