Skew-t Inference with

Improved Covariance Matrix Approximation

Henri Nurminen, Tohid Ardeshiri, Robert Pich´e, and Fredrik Gustafsson,

1

6
1
0
2

 
r
a

 

M
0
2

 
 
]

Y
S
.
s
c
[
 
 

1
v
6
1
2
6
0

.

3
0
6
1
:
v
i
X
r
a

Abstract—Filtering and smoothing algorithms

for linear
discrete-time state-space models with skew-t distributed mea-
surement noise are presented. The proposed algorithms improve
upon our earlier proposed ﬁlter and smoother using the mean
ﬁeld variational Bayes approximation of the posterior distribution
to a skew-t likelihood and normal prior. Our simulations show
that the proposed variational Bayes approximation gives a more
accurate approximation of the posterior covariance matrix than
our earlier proposed method. Furthermore,
the novel ﬁlter
and smoother outperform our earlier proposed methods and
conventional low complexity alternatives in accuracy and speed.

Index Terms—skew t, skewness, t-distribution, robust ﬁlter-
ing, Kalman ﬁlter, variational Bayes, RTS smoother, truncated
normal distribution

I. INTRODUCTION

Asymmetric and heavy-tailed noise processes are present
in many inference problems. In radio signal based distance
estimation [1]–[3], for example, obstacles cause large positive
errors that dominate over symmetrically distributed errors
from other sources [4]. The skew t-distribution [5]–[7] is
the generalization of the t-distribution that has the modeling
ﬂexibility to capture both skewness and heavy-tailedness of
such noise processes. To exemplify this, Fig. 1 illustrates the
contours of the likelihood function for three independent range
measurements where some of the measurements are positive
outliers. In this example, skew-t, t, and normal likelihoods are
compared. The skew-t likelihood gives a more realistic spread
of the probability mass than the normal and t likelihoods.

Filtering and smoothing algorithms for linear discrete-time
state-space models with skew-t measurement noise using a
variational Bayes (VB) method are presented in [8]. This ﬁlter
is applied to indoor localization with real ultra-wideband data
in [9]. This letter proposes improvements to the ﬁlter and
smoother proposed in [8]. Analogous to [8], the measurement
noise is modeled by the skew t-distribution, and the proposed
ﬁlter and smoother use a VB approximation of the posterior.
However, the main contributions of this letter are (1) a new
factorization of the approximate posterior distribution, (2) the
application of an existing method for approximating the statis-
tics of a truncated multivariate normal distribution (TMND),
and (3) a proof of optimality for a truncation ordering in
approximation of the moments of the TMND. A TMND is
a multivariate normal distribution whose support is restricted
(truncated) by linear constraints and that is re-normalized to
integrate to unity. The aforementioned contributions improve

H. Nurminen and R. Pich´e are with the Department of Automation Science
and Engineering, Tampere University of Technology (TUT), PO Box 692,
33101 Tampere, Finland (e-mails: henri.nurminen@tut.ﬁ, robert.piche@tut.ﬁ).
H. Nurminen receives funding from TUT Graduate School, the Foundation of
Nokia Corporation, and Tekniikan edist¨amiss¨a¨ati¨o.

T. Ardeshiri and F. Gustafsson are with the Department of Electrical
Engineering, Link¨oping University, 58183 Link¨oping, Sweden, (e-mails:
tohid@isy.liu.se,
receives funding from
Swedish research council (VR), project scalable Kalman ﬁlters.

fredrik@isy.liu.se). T. Ardeshiri

Fig. 1. The contours of the likelihood function for three range measurements
for the normal (left), t (middle) and skew-t (right) measurement noise models
are presented. The t and skew-t likelihoods can handle one outlier (upper row),
while only the skew-t model can handle the two positive outlier measurements
(bottom row) due to its asymmetry. The likelihoods’ parameters are selected
such that the ﬁrst two moments of the normal, t and skew-t PDFs coincide.

the estimation performance by reducing the covariance un-
derestimation common to most VB inference algorithms [10,
Chapter 10]. To our knowledge, VB approximations have been
applied to the skew t-distribution only in our work [8], [9] and
by Wand et al. [11].

II. PROBLEM FORMULATION

Consider the linear and Gaussian state evolution model

iid∼ N (0, Q),

wk

xk+1 = Axk + wk,
p(x1) = N (x1; x1|0, P1|0),

(1a)
(1b)
where N (·; µ, Σ) denotes a (multivariate) normal PDF with
mean µ and covariance matrix Σ; A ∈ Rnx×nx is the state
transition matrix; xk ∈ Rnx indexed by 1≤ k≤ K is the state
to be estimated with initial prior distribution (1b), where the
subscript “a|b” is read “at time a using measurements up to
time b”. Further, consider the measurements yk ∈ Rny to be
governed by the measurement equation

yk = Cxk + ek,

iid∼ ST(0, Rii, ∆ii, νi),

[ek]i

(2)

where the measurement noise distribution is a product of
independent univariate skew t-distributions. This model
is
justiﬁed in applications where one-dimensional data from dif-
ferent sensors can be assumed to have statistically independent
noise [9]. The PDF and the ﬁrst two moments of the skew t-
distribution can be found in [9] and [12], respectively.
The model (2) admits the hierarchical representation

yk|xk, uk, Λk ∼ N (Cxk + ∆uk, Λ−1

k R),

uk|Λk ∼ N+(0, Λ−1
k ),
[Λk]ii ∼ G( νi
2 ),

2 , νi

(3a)
(3b)
(3c)

anchorrangetrue positionlikelihood2

√

where R ∈ Rny×ny is a diagonal matrix of which the
Rii, are the spread
square roots of the diagonal elements,
parameters of the skew t-distribution in (2); ∆ ∈ Rny×ny
is a diagonal matrix whose diagonal elements ∆ii are the
shape parameters; ν ∈ Rny is a vector whose elements νi
are the degrees of freedom; C ∈ Rny×nx is the measurement
matrix; {wk ∈ Rnx|1 ≤ k ≤ K} and {ek ∈ Rny|1 ≤ k ≤ K}
are mutually independent noise sequences; the operator [·]ij
gives the (i, j) entry of its argument; Λk is a diagonal
matrix with a priori independent random diagonal elements
[Λk]ii. Also, N+(µ, Σ) is the TMND with closed positive
orthant as support, location parameter µ, and squared-scale
matrix Σ. Furthermore, G(α, β) is the gamma distribution
with shape parameter α and rate parameter β. Models where
the measurement noise components are vector-valued with
independently multivariate skew-t distributed noises [5]–[7],
[13]–[15] require only a straightforward modiﬁcation to the
update of the approximate posterior of Λk in the proposed
ﬁltering and smoothing algorithms.
Bayesian smoothing means ﬁnding the smoothing poste-
rior p(x1:K, u1:K, Λ1:K|y1:K). In [8],
the smoothing pos-
terior is approximated by a factorized distribution of the
form q [8] (cid:44) qx(x1:K)qu(u1:K)qΛ(Λ1:K). Subsequently, the
approximate posterior distributions are computed using the
VB approach. The VB approach minimizes the Kullback–
p(x) dx [16]
of the true posterior from the factorized approximation. That
is, DKL(q [8]||p(x1:K, u1:K, Λ1:K|y1:K)) is minimized in [8].
The numerical simulations in [8] manifest the covariance
underestimation of the VB approach, which is a known
weakness of the method [10, Chapter 10]. The aim of this
letter is to reduce the covariance underestimation of the ﬁlter
and smoother proposed in [8] by removing independence
approximations of the posterior approximation.

Leibler divergence (KLD) DKL(q||p)(cid:44)(cid:82) q(x) log q(x)

III. PROPOSED SOLUTION

Using Bayes’ theorem, the state evolution model (1), and
the likelihood (3), the joint smoothing posterior PDF can be
derived as in [8]. This posterior is not analytically tractable.
We propose to seek an approximation in the form
p(x1:K,u1:K, Λ1:K|y1:K) ≈ qxu(x1:K, u1:K) qΛ(Λ1:K),
where the factors in (11) are speciﬁed by

(4)

ˆqxu, ˆqΛ = argmin
qxu,qΛ

DKL(qN||p(x1:K, u1:K, Λ1:K|y1:K))

and where qN (cid:44) qxu(x1:K, u1:K)qΛ(Λ1:K). Hence, x1:K and
u1:K are not approximated as independent as in [8] because
they can be highly correlated a posteriori [8]. The analytical
solutions for ˆqxu and ˆqΛ are obtained by cyclic iteration of

log qxu(·) ← E
log qΛ(·) ← E

qΛ

qxu

[log p(y1:K , x1:K , u1:K , Λ1:K )] + cxu

[log p(y1:K , x1:K , u1:K , Λ1:K )] + cΛ

(5a)

(5b)

where the expected values on the right hand sides are taken
with respect to the current qxu and qΛ [10, Chapter 10] [17],
[18]. Also, cxu and cΛ are constants with respect
to the
variables (x1:K, u1:K) and Λ1:K, respectively.

Computation of the expectation in (5b) requires the ﬁrst two
moments of a TMND, because the support of u1:K is the non-
negative orthant. These moments can be computed using the
formulas presented in [19]. They require evaluating the CDF
(cumulative distribution function) of general multivariate nor-
mal distributions. The MATLAB function mvncdf implements

Table I

Σkk

Σii | i ∈ T }

OPTIMAL RECURSIVE TRUNCATION TO THE POSITIVE ORTHANT
1: Inputs: µ, Σ, and the set of the truncated components’ indices T
2: while T (cid:54)= ∅ do
√
k ← argmini{µi/
√
3:
ξ ← µk/
4:
if Φ(ξ) does not underﬂow to 0 then
5:
6:
7:
8:
9:
10:
11:
12:
13:
14: end while
15: Outputs: µ and Σ; ([µ, Σ] ← rec_trunc(µ, Σ, T ))

 ← φ(ξ)/Φ(ξ)
√
µ ← µ + (/
Σ ← Σ − ((ξ + 2)/Σkk) · Σ:,kΣk,:
µ ← µ + (−ξ/
(cid:46) limξ→−∞( + ξ) = 0 [22]
Σ ← Σ − (1/Σkk) · Σ:,kΣk,: (cid:46) limξ→−∞(ξ + 2) = 1 [22]

(cid:46) φ is the PDF of N (0, 1), Φ its CDF

Σkk) · Σ:,k
√

end if
T ← T \{k}

Σkk) · Σ:,k

else

the numerical quadrature of [20] in 2 and 3 dimensional cases
and the quasi-Monte Carlo method of [21] for the dimensional-
ities 4−25. However, these methods can be prohibitively slow.
Therefore, we approximate the TMND’s moments using the
fast recursive algorithm suggested in [22], [23]. The method is
initialized with the original normal density whose parameters
are then updated by applying one linear constraint at a time.
For each constraint, the mean and covariance matrix of the
once-truncated normal distribution are computed analytically,
and the once-truncated distribution is approximated by a non-
truncated normal with the updated moments.

The result of the recursive truncation depends on the order
in which the constraints are applied. Finding the optimal order
of applying the truncations is a combinatorial problem. Hence,
we choose a greedy approach, whereby the constraint to be
applied is chosen from among the remaining constraints so
that the resulting once-truncated normal is closest to the true
TMND. By Lemma 1, the optimal constraint in KLD-sense is
the one that truncates the most probability. The obtained algo-
rithm with the optimal processing sequence for computing the
mean and covariance of a given normal distribution truncated
to the positive orthant is given in Table I.
Lemma 1. Let p(z) be a TMND with the support {z ≥ 0}
and q(z) = N (z; µ, Σ). Then,

q(z)[[zi ≥ 0]]

argmin

DKL

i

of Σ, [[·]] is the Iverson bracket, and ci =(cid:82) q(z)[[zi ≥ 0]] dz.

where µi is the ith element of µ, Σii is the ith diagonal element

i

= argmin

µi√
Σii

,

(6)

(cid:17)
(cid:17)

ci

(cid:16)
p(z)(cid:12)(cid:12)(cid:12)(cid:12) 1
(cid:16)
p(z)(cid:12)(cid:12)(cid:12)(cid:12) 1
(cid:90) ∞
(cid:90) ∞

ci

Proof: DKL
+= −
= log ci −

0

q(z)[[zi ≥ 0]]

p(z) log( 1
ci

q(z)[[zi ≥ 0]]) dz

p(z) log q(z) dz − 1 += log ci,

0

µi√
Σii

the proof follows.

where += means equality up to an additive constant. Since ci
is an increasing function of
(cid:4)
The recursion (5) is convergent to a local optimum [10,
Chapter 10]. However,
there is no proof of convergence
available when the moments of the TMND are approximated.
In spite of lack of a convergence proof the iterations did not
diverge in the numerical simulations presented in section IV.
The derivations for the expectations of (5) are presented
in the appendixes. In the smoother, the update (5a) includes

SMOOTHING FOR SKEW-t MEASUREMENT NOISE

Table II

1: Inputs: A, C, Q, R, ∆, ν, x1|0, P1|0 and y1:K

2: Az ←(cid:2) A 0

0 0
initialization

(cid:3), Cz ← [ C ∆ ]

3: Λk|K ← Iny for k = 1 · · · K
4: repeat

5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:

22:
23:

0

for k = 1 to K do

k|K ∆T +Λ−1

update qxu(x1:K , u1:K ) given qΛ(Λ1:K )
Zk|k−1 ← blockdiag(Pk|k−1, Λ−1
(cid:101)zk|k ←(cid:104) xk|k−1
(cid:105)
k|K )
z (CPk|k−1CT +∆Λ−1
Kz ← Zk|k−1CT
k|K R)−1
(cid:101)Zk|k ← (I − KzCz)Pk|k−1
+ Kz(yk − Cxk|k−1)
[zk|k, Zk|k] ← rec_trunc((cid:101)zk|k,(cid:101)Zk|k, {nx + 1 · · · nx + ny})
xk|k ← [zk|k]1:nx , Pk|k ← [Zk|k]1:nx,1:nx
xk+1|k ← Axk|k
Pk+1|k ← APk|kAT + Q
end for
for k = K − 1 down to 1 do

Gk ← Zk|kAzZ−1
zk|K ← zk|k + Gk(zk+1|K − Azzk|k)
Zk|K ← Zk|k + Gk(Zk+1|K − Zk+1|k)GT
xk|K ← [zk|K ]1:nx , Pk|K ← [Zk|K ]1:nx,1:nx
uk|K←[zk|K ]nx+(1:ny ), Uk|K←[Zk|K ]nx+(1:ny ),nx+(1:ny )

k+1|k

k

update qΛ(Λ1:K ) given qxu(x1:K , u1:K )

for k = 1 to K do

Ψk ← (yk−Czzk|K )(yk−Czzk|K )TR−1 +CzZk|K CT
[Λk|K ]ii ← νi+2

k|K + Uk|K

+uk|K uT

z R−1

end for

24:
25:
26: until converged
27: Outputs: xk|K and Pk|K for k = 1 · · · K

end for

νi+[Ψk]ii

a forward ﬁltering step of the Rauch–Tung–Striebel smoother
(RTSS) [24] where the ﬁrst ﬁltering posterior is a TMND. The
TMND is approximated as a multivariate normal distribution
whose parameters are obtained using the recursive trunca-
tion. This approximation enables recursive forward ﬁltering
and the use of RTSS’s backward smoothing step that gives
normal approximations to the marginal smoothing posteriors
qxu(xk, uk) ≈ N ([ xk
uk ] ; zk|K, Zk|K). After the iterations con-
verge, the variables u1:K are integrated out to get the ap-
proximate smoothing posteriors qx(xk) =N (xk; xk|K, Pk|K),
where the parameters xk|K and Pk|K are the output of the
skew t smoother (STS) algorithm in Table II. STS can be
restricted to an online recursive algorithm to synthesize a
ﬁlter which is summarized in Table III. In the ﬁlter,
the
output of a ﬁltering step is also a TMND which in analogy
to STS is approximated by a multivariate normal distribution
to have a recursive algorithm. Using recursive truncation, the
TMND is approximated by a normal distribution qx(xk) =
N (xk; xk|k, Pk|k) whose parameters are the outputs of the
skew t ﬁlter (STF) algorithm in Table III.

IV. SIMULATIONS

Our numerical simulations use satellite navigation pseudo-

range measurements of the model

is the ith satellite’s position,

[yk]i | xk ∼ ST((cid:107)si − [xk]1:3(cid:107) + [xk]4, 1 m, δ m, 4)

(7)
where si
[xk]4 is bias with
prior N (0, (0.75 m)2), and δ is skewness parameter. The
linearization error is negligible because the satellites are far.
The state model is a random walk with process covariance
Q = diag((q m)2, (q m)2, (0.2 m)2, 0), where q is a parameter.

FILTERING FOR SKEW-t MEASUREMENT NOISE

Table III

3

1: Inputs: A, C, Q, R, ∆, ν, x1|0, P1|0 and y1:K
2: Cz ← [ C ∆ ]
3: for k = 1 to K do
initialization
Λk|k ← Iny
repeat

4:
5:

uk

(cid:3) ; zk|k, Zk|k) given qΛ(Λk)

update qxu(xk, uk) = N ((cid:2) xk
Zk|k−1 ← blockdiag(Pk|k−1, Λ−1
(cid:101)zk|k ←(cid:104) xk|k−1
(cid:105)
k|k)
z (CPk|k−1CT +∆Λ−1
Kz ← Zk|k−1CT
k|kR)−1
(cid:101)Zk|k ← (I − KzCz)Pk|k−1
+ Kz(yk − Cxk|k−1)
[zk|k, Zk|k] ← rec_trunc((cid:101)zk|k,(cid:101)Zk|k, {nx + 1 · · · nx + ny})
xk|k ← [zk|k]1:nx , Pk|k ← [Zk|k]1:nx,1:nx
i=1 G(cid:16)
update qΛ(Λk) =(cid:81)ny
uk|k←[zk|k]nx+(1:ny ), Uk|k←[Zk|k]nx+(1:ny ),nx+(1:ny )

k|k∆T +Λ−1

2 + 1, νi+[Ψk]ii

[Λk]ii; νi

(cid:17)

0

2

given qxu(xk, uk)
Ψk ← (yk − Czzk|k)(yk − Czzk|k)TR−1 + CzZk|kCT
[Λk|k]ii ← νi+2

k|k + Uk|k

+uk|kuT

z R−1

6:
7:
8:
9:
10:
11:
12:

13:

νi+[Ψk]ii

14:
15:
16:
17:
18: end for
19: Outputs: xk|k and Pk|k for k = 1 · · · K

until converged
xk+1|k ← Axk|k
Pk+1|k ← APk|kAT + Q

A satellite constellation of Global Positioning System provided
by the International GNSS service [25] is used with 8 mea-
sured satellites. The RMSE is computed for [xk]1:3.

A. Computation of TMND statistics

In this subsection we study the computation of the mo-
ments of the untruncated components of a TMND. One state
and one measurement vector per Monte Carlo replication
are generated from the model (7) with ν = ∞ degrees
of freedom (corresponding to skew-normal likelihood), prior
x ∼ N (0, diag(ρ m2, ρ m2, (0.22 m)2, (0.1 m)2)), and 10 000
replications. The compared methods are recursive truncations
with the optimal truncation order (RTopt) and with random
order (RTrand),
the variational Bayes (VB), and the ana-
lytical formulas of [19] using MATLAB function mvncdf
(MVNCDF). In RTrand any of the non-optimal constraints
is chosen at each truncation. VB is an update of the skew
t variational Bayes ﬁlter (STVBF) [8] where Λk = I and the
VB iteration is terminated when the position estimate changes
less than 0.005 m or at the 1000th iteration.

Fig. 2 shows distributions of the distance from the estimate
of the bootstrap particle ﬁlter (PF) with 100 000 samples. The
box levels are 5 %, 25 %, 50 %, 75 %, and 95 % quantiles
and the asterisks show minimum and maximum values. With
small ρ and δ the differences between RTrand, RTopt, and
MVNCDF are small. With large δ there are statistically
signiﬁcant differences as the p-values of two-sided Wilcoxon
signed rank test in Fig. 2 show. RTopt outperforms RTrand
in the cases with high skewness, which reﬂects the result of
Lemma 1. MVNCDF is more accurate than RTopt in the cases
with high skewness, but MVNCDF’s computational load is
roughly 40 000 times that of the RTopt. This justiﬁes the use
of recursive truncation approximation.

The approximation of the posterior covariance matrix is
tested by studying the normalized estimation error squared
(NEES) values [26, Ch. 5.4.2] shown by Fig. 3. If the
covariance matrix is correct, the expected value of NEES is

4

Fig. 2. With large δ values RTopt is closer to PF than RTrand but less
accurate than computationally heavy MVNCDF (upper row). p-values of two-
sided Wilcoxon signed rank test (bottom row) show that the differences from
RTopt are signiﬁcant with large δ. (left) ρ = 12, (right) ρ = 202.

Fig. 4. STF converges in ﬁve iterations. The required number of PF particles
can be 10.000. (left) q = 0.5, δ = 5, (right) q = 5, δ = 5.

Fig. 3. RTopt’s NEES is closest to the optimal value 3, so recursive truncation
gives the most realistic covariance matrix. (left) ρ = 12, (right) ρ = 202.

Fig. 5.
STF outperforms the comparison methods with skew-t-distributed
noise. RMSE differences per cent of the STF’s RMSE. The relative differences
increase as δ is increased. (left) q = 0.5, (right) q = 5.

the state dimensionality 3 [26, Ch. 5.4.2]. VB gets large NEES
values when δ is large, which indicates that VB underestimates
the covariance matrix. RTopt and RTrand give NEES values
closest to 3, so the recursive truncation provides the most
accurate covariance matrix approximation.

B. Skew-t inference

Fig. 6.
differences per cent of the STF’s RMSE.

STF outperforms TVBF and STVBF with UWB noise. RMSE

In this section, the proposed skew t ﬁlter (STF) is compared
with state-of-the-art ﬁlters using numerical simulations of a
100-step trajectory. The compared methods are a bootstrap-
type PF, STVBF [8], t variational Bayes ﬁlter (TVBF) [27],
and Kalman ﬁlter (KF) with measurement validation gating
[26, Ch. 5.7.2] that discards the measurement components
whose normalized innovation squared is larger than the χ2
1-
distribution’s 99 % quantile. TVBF and KF’s parameters are
numerically optimized maximum expected likelihood parame-
ters. The results are based on 1000 Monte Carlo replications.
Fig. 4 illustrates the ﬁlter iterations’ convergence. The ﬁgure
shows that the proposed STF converges within 5 VB iterations
and outperforms the other ﬁlters except for PF already with 2
VB iterations. Furthermore, Fig. 4 shows that STF’s converged
state is close to the PF’s converged state in RMSE, and PF can
require as many as 10 000 particles to outperform STF. STF
also converges faster than STVBF when the process variance
parameter q is large. With a small q, STVBF with a small
number of VB iterations can give a lower RMSE than the
converged STVBF. The reason for this is probably that in the
ﬁrst iterations STVBF accommodates outliers by decreasing
the Λk estimates, which also affects the covariance, while in
the later iterations uk estimates are increased, which makes
the mean more accurate but underestimates the covariance.

Fig. 5 shows the distributions of the RMSE differences from
the STF’s RMSE as percentages of the STF’s RMSE. STF
clearly has the smallest RMSE when δ ≥ 3. Unlike STVBF,
the new STF improves accuracy even with small q, which can
be explained by the improved covariance approximation.

Fig. 6 shows the results of a test where the measurement
noise in (7) is generated from the histogram distribution of
the UWB time-of-ﬂight data set used in [9]. The ﬁlters use
the maximum likelihood parameters ﬁtted to the data set
numerically with the degrees-of-freedom parameters ﬁxed to

Fig. 7.
0.5, δ = 5, (right) q = 5, δ = 5.

Five STS iterations give the converged state’s RMSE. (left) q =

4. The proposed method STF has the lowest RMSE also in
this test, which shows that the method is robust to deviations
from the assumed distribution and thus usable with real data.
The proposed smoother is also tested with measurements
generated from (7). The compared smoothers are the proposed
skew t smoother (STS), skew t variational Bayes Smoother
(STVBS) [8], t variational Bayes smoother (TVBS) [27], and
the RTSS with 99 % measurement validation gating [24]. Fig.
7 shows that STS has lower RMSE than the smoothers based
on symmetric distributions. Furthermore, STF’s VB iteration
converges in ﬁve iterations, so it is faster than STVBF.

V. CONCLUSIONS

We have proposed a novel approximate ﬁlter and smoother
for linear state-space models with heavy-tailed and skewed
measurement noise distribution. The algorithms are based on
the variational Bayes approximation, where some posterior
independence approximations are removed from the earlier
versions of the algorithms to avoid signiﬁcant underestimation
of the posterior covariance matrix. Removal of independence
approximations is enabled by the recursive truncation algo-
rithm for approximating the mean and covariance matrix of
truncated multivariate normal distribution. An optimal process-
ing sequence is given for the recursive truncation.

δ00.5123451020error from PF (% of PF error)0.0010.010.11101001000MVNCDFVBRTrandRToptδ00.5123451020error from PF (% of PF error)0.0010.010.11101001000δ00.5123451020p-value0.0010.010.11MVNCDFVBRTrandδ00.5123451020p-value0.0010.010.11δ05101520averageNEES22.533.54MVNCDFVBRTrandRToptδ05101520averageNEES22.533.54NVB1020304050median RMSE (m)1.41.61.82PFKFTVBFSTVBFSTFNp×10412345NVB1020304050median RMSE (m)33.544.5Np×10412345δ123451020RMSE difference (%)-20020406080100120KFTVBFSTVBFδ123451020RMSE difference (%)-20020406080100120q0.10.5520RMSE difference (%)-50050100TVBFSTVBFNVB1020304050median RMSE (m)11.21.41.6RTSSTVBSSTVBSSTSNVB1020304050median RMSE (m)33.54APPENDIX A

DERIVATIONS FOR THE SMOOTHER

5

We derive the expectations for the iterations of the variational Bayes smoother approximating the joint smoothing density
p(x1:K, u1:K, Λ1:K|y1:K) ∝ p(x1:K, u1:K, Λ1:K, y1:K)
(8)

= p(x1)

p(xl+1|xl)

p(yk|xk, uk, Λk) p(uk|Λk) p(Λk)

K−1(cid:89)

K(cid:89)
N (xl+1; Axl, Q) · K(cid:89)

k=1

(cid:40)

K−1(cid:89)

l=1

= N (x1; x1|0, P1|0)

N (yk; Cxk + ∆uk, Λ−1

k R)N+(uk; 0, Λ−1
k )

l=1

k=1

which is approximated by a factorized probability density function (PDF) in the form

p(x1:K,u1:K, Λ1:K|y1:K) ≈ qxu(x1:K, u1:K) qΛ(Λ1:K).

The VB solutions ˆqxu and ˆqΛ can be obtained by cyclic iteration of

log qxu(x1:K, u1:K) ← E
log qΛ(Λ1:K) ← E

qΛ

qxu

[log p(y1:K, x1:K, u1:K, Λ1:K)] + cxu

[log p(y1:K, x1:K, u1:K, Λ1:K)] + cΛ

G(cid:16)

ny(cid:89)

i=1

[Λk]ii;

νi
2

,

νi
2

(cid:17)(cid:41)

(9)

(10)

(11)

(12a)

(12b)

where the expected values are taken with respect to the current qxu and qΛ, and cxu and cΛ are constants with respect to the
variables [ xk
uk ] and Λk, respectively [10, Chapter 10] [17]. This appendix gives the derivations for one iteration of (12). For
brevity all constant values are denoted by c. The logarithm of the joint smoothing distribution is

log p(x1:K, u1:K, Λ1:K, y1:K) = log N (x1; x1|0, P1|0) +

log N (xl+1; Axl, Q)

K−1(cid:88)

(cid:8)log N (yk; Cxk + ∆uk, Λ−1

+

k R) + log N+(uk; 0, Λ−1

log G([Λk]ii; νi

2 , νi

2 ),

k )(cid:9) +

K(cid:88)

ny(cid:88)

K(cid:88)

l=1

k=1

A. Derivations for qxu

Eq. (12a) gives

k=1

i=1

K−1(cid:88)

l=1

log N (xl+1; Axl, Q)

logqxu(x1:K, u1:K) = log N (x1; x1|0, P1|0) +

[log N (yk; Cxk + ∆uk, Λ−1

k R) + log N+(uk; 0, Λ−1

k )] + c

= log N (x1; x1|0, P1|0) +

log N (xl+1; Axl, Q)

(13)

(14)

(15)

(16)

(17)

(18)

K(cid:88)

k=1

E
qΛ

+

K(cid:88)

k=1

K(cid:88)

k=1

K−1(cid:88)

l=1

K−1(cid:88)

K−1(cid:88)

l=1

− 1
2

E
qΛ

[(yk − Cxk − ∆uk)TR−1Λk(yk − Cxk − ∆uk) + uT

k Λkuk] + c

= log N (x1; x1|0, P1|0) +

log N (xl+1; Axl, Q)

(cid:8)(yk − Cxk − ∆uk)TR−1Λk|K(yk − Cxk − ∆uk) + uT

l=1

− 1
2

(cid:9) + c

k Λk|Kuk

= log N (x1; x1|0, P1|0) +

log N (xl+1; Axl, Q)

K(cid:88)

+

k=1

= log N

(cid:110)
(cid:18)(cid:20)x1
(cid:18)

u1

log N (yk; Axk + ∆uk, Λ−1

(cid:21)

;

(cid:20)x1|0

(cid:21)

0

(cid:20)P1|0
(cid:20)xk
(cid:21)

,

uk

O
O Λ−1
1|K

, Λ−1

k|KR

K−1(cid:88)

k|KR) + log N (uk; 0, Λ−1
(cid:21)(cid:19)
k|K)
(cid:19)

(cid:18)(cid:20)xl+1

log N

ul+1

l=1

+

+ c, u1:K ≥ 0,

+ log N

yk; [C ∆]

(cid:111)
(cid:21)

+ c

(cid:20)A O

O O

(cid:21)(cid:20)xl

ul

(cid:21)

,

(cid:20)Q

O
O Λ−1
l+1|K

(cid:21)(cid:19)

;

6

(cid:105)

where Λk|K (cid:44) EqΛ [Λk] is derived in Section A-B, and u1:K ≥ 0 means that all the components of all uk are required to be
joint smoothing posterior of a linear state-space model with the state transition matrix (cid:101)A (cid:44) [ A O
matrix (cid:102)Qk (cid:44)(cid:104) Q
nonnegative for each k = 1··· K. Up to the truncation of the u components, qxu(x1:K, u1:K) has thus the same form as the
, measurement model matrix (cid:101)C (cid:44) [ C ∆ ], and measurement noise covariance matrix (cid:101)R (cid:44) Λ−1
O O ], process noise covariance
Let us denote the PDFs related to this state-space model with (cid:101)p.
k|KR.
(cid:101)p ([ x1:K
It would be possible to compute the truncated multivariate normal posterior of
the joint smoothing distribution
u1:K ]|y1:K), and account for the truncation of u1:K to the positive orthant using the recursive truncation. However, this
would be impractical with large K due to the large dimensionality K × (nx + ny). A feasible solution is to approximate
each ﬁltering distribution in the Rauch–Tung–Striebel smoother’s (RTSS [24]) forward ﬁltering step with a multivariate normal
distribution by

O
−1
k+1|K

O Λ

(cid:101)p(xk, uk|y1:k) =

1
C
≈ N

(cid:19)

(cid:21)

(cid:18)(cid:20)xk
(cid:18)(cid:20)xk
(cid:21)

N

uk

uk

(cid:19)
k|k, Z(cid:48)
; z(cid:48)
k|k

; zk|k, Zk|k

· [uk ≥ 0]

(19)

(20)

[uk ≥ 0] =

if all components of uk are non-negative

for each k = 1··· K, where [uk ≥ 0] is the Iverson bracket notation

(cid:26) 1,
truncation. Given the multivariate normal approximations of the ﬁltering posteriors (cid:101)p(xk, uk|y1:k), by Lemma 2 the backward
C is the normalization factor, and zk|k (cid:44) E(cid:101)p [[ xk
uk ]|y1:k] are approximated using the recursive
recursion of the RTSS gives multivariate normal approximations of the smoothing posteriors (cid:101)p(xk, uk|y1:K). The quantities

uk ]|y1:k] and Zk|k (cid:44) Var(cid:101)p [[ xk

required in the derivations of Section A-B are the expectations of the smoother posteriors xk|K (cid:44) Eqxu [xk], uk|K (cid:44) Eqxu[uk],
and the covariance matrices Zk|K (cid:44) Varqxu [ xk
Lemma 2. Let {zk}K

k=1 be a linear–Gaussian process, and {yk}K

k=1 a measurement process such that

uk ] and Uk|K (cid:44) Varqxu [uk].

0, otherwise

,

(21)
(22)
(23)
with the standard Markovianity assumptions. Then, if the ﬁltering posterior p(zk|y1:k) is a multivariate normal distribution
for each k, then for each k < K

yk|zk ∼ (a known distribution).

z1 ∼ N (z1|0, Z1|0)
zk|zk−1 ∼ N (Azk−1, Q)

zk|y1:K ∼ N (zk|K, Zk|K),

where

zk|K = zk|k + Gk(zk+1|K − Azk|k),
Zk|K = Zk|k + Gk(Zk+1|K − AZk|kAT − Q)GT
k ,
Gk = Zk|kAT(AZk|kAT + Q)−1,

and zk|k and Zk|k are the mean and covariance matrix of the ﬁltering posterior p(zk|y1:k).

Proof: The proof is mostly similar to the proof of [29, Theorem 8.2]. First, assume that

zk+1|y1:K ∼ N (zk+1|K, Zk+1|K).

for some k < K. The joint conditional distribution of zk and zk+1 is then

p(zk, zk+1|y1:k) = p(zk+1|zk) p(zk|y1:k) || Markovianity assumption

= N (zk+1; Azk, Q)N (zk; zk|k, Zk|k)
= N

;

,

(cid:20) Zk|k

(cid:21)

(cid:20) zk|k

Azk|k

(cid:18)(cid:20) zk

(cid:21)

zk+1

Zk|kAT

AZk|k AZk|kAT + Q

(cid:21)(cid:19)

,

so by the conditioning rule of the multivariate normal distribution

p(zk|zk+1, y1:k) = N (zk; zk|k + Gk(zk+1 − Azk|k), Zk|k − Zk|kAT(AZk|kAT + Q)−1AZk|k)

= N (zk; zk|k + Gk(zk+1 − Azk|k), Zk|k − Gk(AZk|kAT + Q)GT
k ).

(24)

(25)
(26)
(27)

(28)

(29)
(30)

(31)

(32)
(33)

We use this formula in

p(zk, zk+1|y1:K) = p(zk|zk+1, y1:K) p(zk+1|y1:K)

= p(zk|zk+1, y1:k) p(zk+1|y1:K) || Markovianity assumption
= N (zk; zk|k + Gk(zk+1 − Azk|k), Zk|k − Gk(AZk|kAT + Q)GT
= N

(cid:20)zk|k + Gk(zk+1|K − Azk|k)
(cid:21)
(cid:20)zk|k + Gk(zk+1|K − Azk|k)
(cid:21)

(cid:18)(cid:20) zk
(cid:18)(cid:20) zk

k )N (zk+1|zk+1|K, Zk+1|K)

(cid:20)GkZk+1|KGT
(cid:20)Zk|k + Gk(Zk+1|K − AZk|kAT − Q)GT

k + Zk|k − Gk(AZk|kAT + Q)GT
(cid:21)(cid:19)

(cid:21)
(cid:21)

zk+1

•

•

k

;

,

•

,

•

= N

;

zk+1

•
•

k

,

so

p(zk|y1:K) = N (zk; zk|k + Gk(zk+1|K − Azk|k), Zk|k + Gk(Zk+1|K − AZk|kAT − Q)GT
k )

= N (zk; zk|K, Zk|K).

Because zK|y1:K ∼ N (zK|K, ZK|K), and because (28) implies (40), the statement holds by the induction argument.

7

(34)
(35)
(36)

(cid:21)(cid:19)

•
•

(37)

(38)

(39)
(40)

B. Derivations for qΛ

Eq. (12b) gives

(cid:26)
K(cid:88)
Therefore, qΛ(Λ1:K) =(cid:81)K

log qΛ(Λ1:K) =

E
qxu

k=1

(cid:2)log N (yk; Cxk + ∆uk, Λ−1

k R) + log N+(uk; 0, Λ−1

k )(cid:3)(cid:27)

K(cid:88)

ny(cid:88)

log G(cid:16)

+

k=1

i=1

[Λk]ii;

νi
2

,

νi
2

(cid:17)

+ c.

(41)

k=1 qΛ(Λk) where

logqΛ(Λk) = − 1
2

E
qxu

[tr{(yk − Cxk − ∆uk)(yk − Cxk − ∆uk)TR−1Λk}]

(cid:17)

ny(cid:88)

(cid:16) νi

2

i=1

[tr{ukuT

k Λk}] +

log[Λk]ii − νi
2

[Λk]ii

+ c

(yk − Cxk|K − ∆uk|K)(yk − Cxk|K − ∆uk|K)T + [C ∆] Zk|K

(uk|KuT

k|K + Uk|K)Λk

+

log[Λk]ii − νi
2

[Λk]ii

+ c

(cid:17)

(cid:111)

(cid:16) νi

2

ny(cid:88)
(cid:19)

i=1

log[Λk]ii − νi + [Ψk]ii

[Λk]ii

+ c

2

− 1
2
= − 1
2
− 1
2

ny(cid:88)

tr

E
qxu

(cid:26)(cid:18)
(cid:110)
(cid:18) νi

tr

2

i=1

=

where

Ψk = (yk − Cxk|K − ∆uk|K)(yk − Cxk|K − ∆uk|K)TR−1 + [C ∆] Zk|K

Therefore,

(cid:18)

ny(cid:89)

i=1

G

[Λk]ii;

νi
2

+ 1,

qΛ(Λk) =

νi + [Ψk]ii

2

(cid:21)(cid:19)

(cid:20)C T

∆T

(cid:27)

R−1Λ

(42)

(43)

(44)

(45)

(cid:20)C T
(cid:19)

∆T

.

(cid:21)

R−1 + uk|KuT

k|K + Uk|K.

(46)

(47)

Note that only the diagonal elements of the matrix Ψk are required. In the derivations of Section A-A, Λk|K (cid:44) EqΛ [Λk] is
required. EqΛ[Λk] is a diagonal matrix with the diagonal elements

[Λk|K]ii =

νi + 2

νi + [Ψk]ii

.

(48)

8

APPENDIX B

DERIVATIONS FOR THE FILTER

Suppose that at time index k the measurement vector yk is available, and the prediction PDF p(xk|y1:k−1) is

p(xk|y1:k−1) = N (xk; xk|k−1, Pk|k−1).

Then, using Bayes’ theorem the joint ﬁltering posterior PDF is

p(xk, uk, Λk|y1:k) ∝ p(yk, xk, uk, Λk|y1:k−1)

= p(yk|xk, uk, Λk) p(xk|y1:k−1) p(uk|Λk) p(Λk)
= N (yk; Cxk + ∆uk, Λ−1

k R)N (xk; xk|k−1, Pk|k−1)N+(uk; 0, Λ−1
k )

G(cid:16)

ny(cid:89)

i=1

(cid:17)

.

[Λk]ii;

νi
2

,

νi
2

This posterior is not analytically tractable. We seek an approximation in the form
p(xk,uk, Λk|y1:k) ≈ qxu(xk, uk) qΛ(Λk).

The VB solutions ˆqxu and ˆqΛ can be obtained by cyclic iteration of

log qxu(xk, uk) ← E
log qΛ(Λk) ← E

qΛ

[log p(yk, xk, uk, Λk|y1:k−1)] + cxu
[log p(yk, xk, uk, Λk|y1:k−1)] + cΛ

qxu

(49)

(50)
(51)

(52)

(53)

(54a)

(54b)

where the expected values on the right hand sides of (54) are taken with respect to the current qxu and qΛ, and cxu and cΛ are
constants with respect to the variables [ xk
uk ] and Λk, respectively [10, Chapter 10] [17]. In sections B-A and B-B the derivations
for the variational solution (54) are given. For brevity all constant values are denoted by c in the derivations. The logarithm
of the joint ﬁltering posterior which is needed for the derivations is given by

log p(yk, xk, uk, Λk|y1:k−1) = − 1
2
− 1
2
− 1
2

(yk − Cxk − ∆uk)TR−1Λk(yk − Cxk − ∆uk)
(xk − xk|k−1)TP −1

ny(cid:88)

k|k−1(xk − xk|k−1)
(cid:16) νi

log[Λk]ii − νi
2

2

i=1

(cid:17)

uT
k Λkuk +

[Λk]ii

+ c, uk ≥ 0,

(55)

where uk ≥ 0 means that every component of uk is non-negative.

A. Derivations for qxu

Using equation (54a) we obtain

log qxu(xk, uk) = − 1
2
− 1
2
= − 1
2
− 1
2

E
qΛ
(xk − xk|k−1)TP −1

yk − [C ∆]

(cid:18)
(cid:18)(cid:20)xk

uk

(cid:21)

where Λk|k (cid:44) EqΛ [Λk] is derived in section B-B. Hence,

(cid:18)

qxu(xk, uk) ∝ N

0
Λ−1
k|k
where [uk ≥ 0] is the Iverson bracket. By Kalman ﬁlter’s [28] measurement update, this becomes

yk; [C ∆]

, Λ−1

k|kR

N

uk

uk

0

0

;

,

[(yk − Cxk − ∆uk)TR−1Λk(yk − Cxk − ∆uk)]

−

0

0

2

uk

[uT

E
qΛ

(cid:18)

(cid:21)(cid:19)

uk
−

0
Λ−1
k|k

k Λkuk] + c

R−1Λk|k

yk − [C ∆]

(cid:20)xk
(cid:21)(cid:19)T
k|k−1(xk − xk|k−1) − 1
(cid:20)xk|k−1
(cid:21)(cid:19)T(cid:20)Pk|k−1
(cid:18)(cid:20)xk
(cid:19)
(cid:21)
(cid:20)xk
(cid:21)
(cid:18)(cid:20)xk

(cid:20)xk
(cid:21)(cid:19)
(cid:21)−1(cid:18)(cid:20)xk
(cid:21)
(cid:20)xk|k−1
(cid:20)xk|k−1
(cid:21)(cid:19)
(cid:20)Pk|k−1
(cid:21)
(cid:19)

(cid:21)

uk

0

; z(cid:48)
k|k, Z(cid:48)
k|k

· [uk ≥ 0],

N

1
C

uk

, uk ≥ 0,

· [uk ≥ 0],

(56)

(57)

(58)

(59)

(60)

qxu(xk, uk) =

where

(cid:20)xk|k−1
(cid:21)
(cid:20)Pk|k−1C T

0

(cid:21)

z(cid:48)
k|k =
k|k = (I − Kk [C ∆])
Z(cid:48)

(cid:20)Pk|k−1

+ Kk(yk − Cxk|k−1),
0
Λ−1
k|k

0

(cid:21)

,

9

(61)

(62)

k|k∆T + Λ−1
The ﬁrst and second moments xk|k (cid:44) Eqxu [xk], uk|k (cid:44) Eqxu [uk], Zk|k (cid:44) Varqxu [ xk

k|kR)−1.
uk ], and Uk|k (cid:44) Varqxu [uk] are required
in the derivation of qΛ in Section B-B, and they can be approximated using the recursive truncation algorithm. For the linear–
Gaussian time update to be analytically tractable, the marginal distribution qxu(xk) is approximated as a normal distribution

(CPk|k−1C T + ∆Λ−1

Λ−1
k|k∆T

Kk =

(63)

qxu(xk) =

where Pk|k (cid:44) Varqxu[xk].

B. Derivations for qΛ

Using equation (54b) we obtain

qxu(xk, uk) duk ≈ N (xk|k, Pk|k),

log qΛ(Λk) = − 1
2
− 1
2
= − 1
2
− 1
2

E
qxu

tr

E
qxu

(cid:26)(cid:18)
(cid:110)
(cid:18) νi

tr

ny(cid:88)

2

i=1

=

where

[tr{(yk − Cxk − ∆uk)(yk − Cxk − ∆uk)TR−1Λk}]

[tr{ukuT

k Λk}] +

log[Λk]ii − νi
2

[Λk]ii

+ c

(cid:17)

(yk − Cxk|k − ∆uk|k)(yk − Cxk|k − ∆uk|k)T + [C ∆] Zk|k

(uk|kuT

k|k + Uk|k)Λk

+

log[Λk]ii − νi
2

[Λk]ii

+ c

(cid:111)

ny(cid:88)

i=1

(cid:16) νi
(cid:19)

2

log[Λk]ii − νi + [Ψk]ii

[Λk]ii

+ c

2

Ψk = (yk − Cxk|k − ∆uk|k)(yk − Cxk|k − ∆uk|k)TR−1 + [C ∆] Zk|k

and the moments xk|k (cid:44) Eqxu[xk], uk|k (cid:44) Eqxu[uk], Zk|k (cid:44) Varqxu [ xk
of this report. Therefore,

R−1 + uk|kuT

(69)
uk ], and Uk|k (cid:44) Varqxu[uk] are derived in Section B-A

k[k + Uk|k

∆T

qΛ(Λk) =

[Λk]ii;

νi
2

+ 1,

νi + [Ψk]ii

2

.

(70)

Note that only the diagonal elements of the matrix Ψk are required. In the derivations of Section B-A Λk|k (cid:44) EqΛ [Λk] is
required. EqΛ[Λk] is a diagonal matrix with the diagonal elements

[Λk|k]ii =

νi + 2

νi + [Ψk]ii

.

(71)

(cid:90)

ny(cid:88)

(cid:16) νi

2

i=1

(cid:18)

ny(cid:89)

i=1

G

(64)

(65)

(66)

(67)

(68)

(cid:21)(cid:19)

(cid:20)C T

∆T

(cid:27)

R−1Λk

(cid:17)

(cid:20)C T

(cid:21)

(cid:19)

10

REFERENCES

[1] F. Gustafsson and F. Gunnarsson, “Mobile positioning using wireless networks: possibilities and fundamental limitations based on available wireless

network measurements,” IEEE Signal Processing Magazine, vol. 22, no. 4, pp. 41–53, July 2005.

[2] B.-S. Chen, C.-Y. Yang, F.-K. Liao, and J.-F. Liao, “Mobile location estimator in a rough wireless environment using Extended Kalman-based IMM and

data fusion,” IEEE Transactions on Vehicular Technology, vol. 58, no. 3, pp. 1157–1169, March 2009.

[3] M. Kok, J. D. Hol, and T. B. Sch¨on, “Indoor positioning using ultra-wideband and inertial measurements,” IEEE Transactions on Vehicular Technology,

[4] K. Kaemarungsi and P. Krishnamurthy, “Analysis of WLAN’s received signal strength indication for indoor location ﬁngerprinting,” Pervasive and Mobile

Computing, vol. 8, no. 2, pp. 292–316, 2012, special Issue: Wide-Scale Vehicular Sensor Networks and Mobile Sensing.

[5] M. D. Branco and D. K. Dey, “A general class of multivariate skew-elliptical distributions,” Journal of Multivariate Analysis, vol. 79, no. 1, pp. 99–113,

vol. 64, no. 4, 2015.

October 2001.

[6] A. Azzalini and A. Capitanio, “Distributions generated by perturbation of symmetry with emphasis on a multivariate skew t-distribution,” Journal of the

Royal Statistical Society. Series B (Statistical Methodology), vol. 65, no. 2, pp. 367–389, 2003.

[7] A. K. Gupta, “Multivariate skew t-distribution,” Statistics, vol. 37, no. 4, pp. 359–363, 2003.
[8] H. Nurminen, T. Ardeshiri, R. Piche, and F. Gustafsson, “Robust inference for state-space models with skewed measurement noise,” IEEE Signal

Processing Letters, vol. 22, no. 11, pp. 1898–1902, Nov 2015.

[9] H. Nurminen, T. Ardeshiri, R. Pich´e, and F. Gustafsson, “A NLOS-robust TOA positioning ﬁlter based on a skew-t measurement noise model,” in

International Conference on Indoor Positioning and Indoor Navigation (IPIN), October 2015, pp. 1–7.

[10] C. M. Bishop, Pattern Recognition and Machine Learning. Springer, 2007.
[11] M. P. Wand, J. T. Ormerod, S. A. Padoan, and R. Fr¨uhwirth, “Mean ﬁeld variational Bayes for elaborate distributions,” Bayesian Analysis, vol. 6, no. 4,

pp. 847–900, 2011.

pp. 129–150, 2003.

[12] S. K. Sahu, D. K. Dey, and M. D. Branco, “Erratum: A new class of multivariate skew distributions with applications to Bayesian regression models,”

Canadian Journal of Statistics, vol. 37, no. 2, pp. 301–302, 2009.

[13] ——, “A new class of multivariate skew distributions with applications to Bayesian regression models,” Canadian Journal of Statistics, vol. 31, no. 2,

[14] T.-I. Lin, “Robust mixture modeling using multivariate skew t distributions,” Statistics and Computing, vol. 20, pp. 343–356, 2010.
[15] S. X. Lee and G. J. McLachlan, “EMMIXuskew: An R package for ﬁtting mixtures of multivariate skew t distributions via the EM algorithm,” Journal

of Statistical Software, vol. 55, no. 12, pp. 1–22, November 2013.

[16] T. M. Cover and J. Thomas, Elements of Information Theory.
[17] D. G. Tzikas, A. C. Likas, and N. P. Galatsanos, “The variational approximation for Bayesian inference,” IEEE Signal Processing Magazine, vol. 25,

John Wiley and Sons, 2006.

[18] M. J. Beal, “Variational algorithms for approximate Bayesian inference,” Ph.D. dissertation, Gatsby Computational Neuroscience Unit, University College

[19] G. Tallis, “The moment generating function of the truncated multi-normal distribution,” Journal of the Royal Statistical Society. Series B (Methodological),

no. 6, pp. 131–146, Nov. 2008.

London, 2003.

vol. 23, no. 1, pp. 223–119, 1961.

vol. 11, no. 4, pp. 950–971, 2002.

[20] A. Genz, “Numerical computation of rectangular bivariate and trivariate normal numerical computation of rectangular bivariate and trivariate normal and

t probabilities,” Statistics and Computing, vol. 14, pp. 251–260, 2004.

[21] A. Genz and F. Bretz, “Comparison of methods for the computation of multivariate t probabilities,” Journal of Computational and Graphical Statistics,

[22] T. Per¨al¨a and S. Ali-L¨oytty, “Kalman-type positioning ﬁlters with ﬂoor plan information,” in 6th International Conference on Advances in Mobile

Computing and Multimedia (MoMM). New York, NY, USA: ACM, 2008, pp. 350–355.

[23] D. J. Simon and D. L. Simon, “Constrained Kalman ﬁltering via density function truncation for turbofan engine health estimation,” International Journal

[24] H. E. Rauch, C. T. Striebel, and F. Tung, “Maximum Likelihood Estimates of Linear Dynamic Systems,” Journal of the American Institute of Aeronautics

of Systems Science, vol. 41, no. 2, pp. 159–171, 2010.

and Astronautics, vol. 3, no. 8, pp. 1445–1450, August 1965.

[25] J. M. Dow, R. Neilan, and C. Rizos, “The international GNSS service in a changing landscape of global navigation satellite systems,” Journal of Geodesy,

[26] Y. Bar-Shalom, R. X. Li, and T. Kirubarajan, Estimation with Applications to Tracking and Navigation, Theory Algorithms and Software.

John Wiley

vol. 83, no. 7, p. 689, February 2009.

& Sons, 2001.

[27] R. Pich´e, S. S¨arkk¨a, and J. Hartikainen, “Recursive outlier-robust ﬁltering and smoothing for nonlinear systems using the multivariate Student-t

distribution,” in IEEE International Workshop on Machine Learning for Signal Processing (MLSP), September 2012.

[28] R. E. Kalman, “A new approach to linear ﬁltering and prediction problems,” Transactions of the ASME–Journal of Basic Engineering, vol. 82, no. Series

D, pp. 35–45, 1960.

[29] S. S¨arkk¨a, Bayesian Filtering and Smoothing. Cambridge, UK: Cambridge University Press, 2013.

