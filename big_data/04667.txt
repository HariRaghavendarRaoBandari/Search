Stationary Graph Processes and Spectral Estimation

Antonio G. Marques, Santiago Segarra, Geert Leus, and Alejandro Ribeiro

1

Abstract—Stationarity is a cornerstone property that facilitates
the analysis and processing of random signals in the time domain.
Although time-varying signals are abundant in nature,
in many
practical scenarios the information of
interest resides in more
irregular graph domains. This lack of regularity hampers the
generalization of the classical notion of stationarity to graph signals.
The contribution in this paper is twofold. Firstly, we propose a
deﬁnition of weak stationarity for random graph signals that takes
into account the structure of the graph where the random process
takes place, while inheriting many of the meaningful properties of
the classical deﬁnition in the time domain. Our deﬁnition requires
that stationary graph processes can be modeled as the output
of a linear graph ﬁlter applied to a white input. We will show
that this is equivalent to requiring the correlation matrix to be
diagonalized by the graph Fourier transform. Secondly, we analyze
the properties of the power spectral density and propose a number
of methods to estimate it. We start with nonparametric approaches,
including periodograms, window-based average periodograms, and
ﬁlter banks. We then shift the focus to parametric approaches,
discussing the estimation of moving-average (MA), autoregressive
(AR) and ARMA processes. Finally, we illustrate the power spectral
density estimation in synthetic and real-world graphs.

Index Terms—Graph signal processing, Weak stationarity, Ran-
dom graph process, Periodogram, Windowing, Power spectral den-
sity, Parametric estimation.

I. INTRODUCTION

6
1
0
2

 
r
a

 

M
4
1

 
 
]

Y
S
.
s
c
[
 
 

1
v
7
6
6
4
0

.

3
0
6
1
:
v
i
X
r
a

Networks and graphs, which can be used to represent pairwise
relationships between elements of a set, have often intrinsic value
and are themselves the object of study. In other occasions, they
deﬁne underlying notions of proximity or dependence, but the
interest is in signals associated with the nodes of the graph. This
is the matter addressed in the ﬁeld of graph signal processing
(GSP), where notions such as frequency and linear ﬁltering are
extended to signals supported on graphs [2], [3]. A plethora of
graph signals exist in different ﬁelds, including gene-expression
patterns deﬁned on top of gene networks, the spread of epidemics
over a social network, and the congestion level at the nodes of
a communication network, to name a few. Transversal to the
particular application, the question that arises is how to redesign
tools originally conceived to study and process signals on regular
domains to be used in the more complex graph domain.

This paper investigates the problem of generalizing the notion
of stationary processes [4], [5] to the graph domain [6], [7]. We
begin by proposing different equivalent deﬁnitions of weak sta-
tionarity for graph signals, all taking into account the structure of
the graph where the random process takes place, while inheriting
many of the meaningful properties of the classical deﬁnition for
time-varying random processes. A straightforward generalization
is not trivial because the shift (translation) operation in the graph
domain is more involved, it changes the energy of the shifted
signal (unless normalized [6]), and its effect in the frequency

Work in this paper is supported by Spanish MINECO grant No TEC2013-
41604-R and USA NSF CCF-1217963. A. G. Marques is with the Dept. of Signal
Theory and Comms., King Juan Carlos Univ. S. Segarra and A. Ribeiro are with
the Dept. of Electrical and Systems Eng., Univ. of Pennsylvania. G. Leus is with
the Dept. of Electrical Eng., Mathematics and Comp. Science, Delft Univ. of
Technology. Emails: antonio.garcia.marques@urjc.es, ssegarra@seas.upenn.edu,
g.j.t.leus@tudelft.nl, and aribeiro@seas.upenn.edu. Part of the results in this paper
will be presented at the 2016 IEEE SAM Workshop [1].

domain is more difﬁcult to analyze. With these considerations in
mind, a random process in a graph is said to be stationary if either
its correlation is invariant with respect to a constant number of
applications of the graph shift operator or, alternatively, if it can be
modeled as the output of a linear graph ﬁlter [2], [8]–[10] applied
to a white input. This is shown to be equivalent to requiring the
correlation matrix of the process and the graph-shift operator to
be normal and simultaneously diagonalizable. Under these deﬁni-
tions, notions like the power spectral density (PSD) or results such
as the spectral convolution theorem can be generalized to signals
supported on graphs. After showing that stationary processes are
easier to understand in the frequency domain, we propose and
analyze different methods to estimate the PSD, which, if needed,
can also be used to improve the estimate of the correlation matrix
itself. We begin by looking at simple nonparametric methods
for PSD estimation. We ﬁrst extend the periodogram and the
correlogram to the graph domain, and analyze their estimation
performance. We then generalize more advanced estimators such
as window-based average periodograms and ﬁlter banks [5, Chs.
2 and 5]. Differences relative to their time-domain counterparts
are highlighted and open research problems are identiﬁed. After
this, we shift the attention to parametric estimation. The focus is
on estimating the PSD of autoregressive (AR), moving-average
(MA) and ARMA graph processes, which are likely to arise in
distributed setups driven by linear dynamics [11]–[13]. As in time,
it turns out that the estimation of the ARMA parameters is a non-
convex problem, although for certain particular cases –including
that of positive semideﬁnite shifts– the optimization is tractable.
Along with the proposed schemes, comparisons with traditional
time-domain designs are provided.

Preliminary results generalizing the deﬁnition of stationarity
to graph signals for Laplacian shifts were reported in [6] and
[7]. Our contribution here is to draw a parallel between the
fundamental properties of stationary stochastic processes in time
and stationary processes in graphs. Speciﬁcally, we introduce the
notions of shift-invariant correlations, responses of linear shift
invariant systems to white inputs, and the existence of a PSD
for graph processes as extensions of their analogous deﬁnitions
for time signals. We show that while each of these extensions
could lead to a different notion of stationarity in graphs, they
all turn out equivalent. We also consider general normal shifts,
identify properties hitherto unreported, establish connections with
application settings such as diffusion dynamics, and formulate
and evaluate different parametric and nonparametric approaches
for PSD estimation.
Paper outline: Section II introduces the concepts of graph signals
and ﬁlters. Section III presents the deﬁnition of weak stationarity
and discusses some of its properties. Different nonparametric
methods to estimate the PSD of a random graph process are
presented in Section IV. Section V addresses the parametric
estimation for AR, MA, and ARMA graph processes. Simulations
and conclusions in Sections VI and VII wrap-up the paper.
Notation: Entries of a vector x are written as xi and entries of a
matrix X as Xij. If needed for clarity we may alternatively write
[x]i and [X]ij. We use X∗, XT , and XH to denote conjugate,

2

transpose, and transpose conjugate, respectively. For a square
matrix X, we use tr[X] for its trace and diag(X) for an operator
returning a vector with the diagonal elements of X. For a vector
x, we let diag(x) denote a diagonal matrix with diagonal elements
diag[diag(x)] = x. The notation x ◦ y denotes the elementwise
product of x and y. We use 0 and 1 for the all-zero and all-one
vectors and ei for the ith element of the canonical basis of RN .

II. GRAPH SIGNALS AND FILTERS

Let G = (N ,E) be a directed graph or network with a set of
N nodes N and directed edges E such that (i, j) ∈ E implies
that node i is connected to node j. We associate with G the graph
shift operator S, deﬁned as an N ×N matrix whose entry Sji (cid:54)= 0
only if i = j or if (i, j) ∈ E [3], [8]. The sparsity pattern of
the matrix S captures the local structure of G, but we make no
speciﬁc assumptions on the values of the nonzero entries of S.
Frequent choices for S are the adjacency matrix of the graph [3],
[8], its Laplacian [2], and their respective generalizations [14]. The
intuition behind S is to represent a linear transformation that can
be computed locally at the nodes of the graph. More rigorously, if
the set Nl(i) stands for the nodes within the l-hop neighborhood
of node i and the signal y is deﬁned as y = Sx, then node i
can compute yi provided that it has access to the value of xj at
j ∈ N1(i). We assume henceforth that S is normal, so that there
exists an N × N unitary matrix V and an N × N diagonal matrix
Λ that can be used to decompose S as S = VΛVH.
The focus of this paper is not on G, but on graph signals
deﬁned on the set of nodes N . Formally, each of these signals
can be represented as a vector x = [x1, ..., xN ]T ∈ RN where
the i-th element represents the value of the signal at node i or,
alternatively, as a function f : N → R, deﬁned on the vertices of
the graph. Given a graph signal x, we refer to ˜x := VH x as the
frequency representation of x, with VH being the graph Fourier
transform (GFT) [8].
We further introduce the notion of a graph ﬁlter H : RN →
RN which is deﬁned as a linear graph signal operator of the form

L−1(cid:88)

l=0

H :=

hlSl,

(1)

where h = [h0, . . . , hL−1]T is a vector of L ≤ N scalar
coefﬁcients. According to (1) graph ﬁlters are polynomials of
degree L − 1 in the graph-shift operator S [3], which due to
the local structure of the shift can be implemented locally too [9],
[10]. It is easy to see that graph ﬁlters are invariant to applications
of the shift in the sense that if y = Hx, it must hold that
Sy = H(Sx). Using the factorization S = VΛVH the ﬁlter in

(1) can be rewritten as H = V(cid:0)(cid:80)L−1
matrix(cid:80)L−1
and extracted in the vector ˜h := diag((cid:80)L−1

l=0 hlΛl(cid:1)VH. The diagonal

l=0 hlΛl is termed the frequency response of the ﬁlter

To relate the frequency response ˜h with the ﬁlter coefﬁcients h
let λk = [Λ]kk be the kth eigenvalue of S and deﬁne the N × N
Vandermonde matrix Ψ with entries Ψkl = λl−1
. Further deﬁne
ΨL as a tall matrix containing the ﬁrst L columns of Ψ to write
˜h = ΨLh and conclude that the ﬁlter in (1) can be alternatively
written as

l=0 hlΛl).

k

L−1(cid:88)
hlSl = Vdiag(˜h)VH = Vdiag(cid:0)ΨLh(cid:1)VH .
˜y = diag(cid:0)ΨLh(cid:1)VH x = diag(cid:0)˜h(cid:1)˜x = ˜h ◦ ˜x,

l=0

Equation (2) implies that if y is deﬁned as y = Hx, its frequency
representation ˜y = VH y satisﬁes

H =

(3)

(2)

which demonstrates that the output at a given frequency depends
only on the value of the input and the ﬁlter response at that given
frequency. Observe that when L = N we have ΨN = Ψ and
that, if we are given a ﬁlter with order greater than N − 1, we
can rewrite it as a different ﬁlter of order not larger than N − 1
due to the Cayley-Hamilton theorem.

III. WEAKLY STATIONARY RANDOM GRAPH PROCESSES
A stochastic process is said weakly stationary in ﬁnite discrete
time if its correlation matrix is invariant to circular time shifts.
Moreover, two important properties of time stationary processes
are: (i) They can always be represented as the response of a linear
time invariant ﬁlter to a white input. (ii) The correlation matrix can
be represented with a power spectral density. Each of these three
facts, can be used as a starting point to deﬁne graph stationary
processes.

Commence then by considering the time invariance of correla-
tion matrices. In the language of GSP, time invariance means that
the correlation of a stationary signal x and its shifted version Slx
are the same. Save for a technical modiﬁcation we adopt this as
a deﬁnition that we formally state next.

Deﬁnition 1 Given a normal shift operator S, a zero-mean ran-
dom process x is said to be weakly stationary with respect to S
if for any set of nonnegative integers a, b, and c ≤ b it holds

= E(cid:104)(cid:0)Sa+cx(cid:1)(cid:0)(cid:0)SH(cid:1)b−c

x(cid:1)H(cid:105)

(4)

E(cid:104)(cid:0)Sax(cid:1)(cid:0)(cid:0)SH(cid:1)b

x(cid:1)H(cid:105)

.

In Deﬁnition 1 the constants a and b act as a reference and
c as a shift that is applied backward and forward. The signal is
said stationary if the application of these shifts does not alter the
covariance matrix. In the particular case in which S is a cyclic
shift, the Hermitian SH = S−1 is a shift in the opposite direction.

Then, if we set a = 0, b = N and c = l we recover E(cid:2)xxH(cid:3) =
E(cid:2)Slx(Slx)H(cid:3), which is the deﬁnition of a stationary signal in

time. The use of a reference shift in Deﬁnition 1 is necessary
because the eigenvalues of the shift operator S do not have unit
magnitude and can change the energy of the signal; see [6] for
an alternative approach and Remark 2 for further details.

A second deﬁnition follows from the representation of a station-
ary process as the response of a linear time-invariant ﬁlter to white
noise. To deﬁne a stationary process in a graph it would sufﬁce to
consider processes that are outputs of linear shift-invariant ﬁlters.
Formally, deﬁne a standard zero-mean white random process w

as one with mean E [w] = 0 and covariance E(cid:2)wwH(cid:3) = I. We

can then alternatively deﬁne a stationary graph process as follows:

l=0 hlSl to a zero-mean white input w.

Deﬁnition 2 Given a normal shift operator S, a zero-mean ran-
dom process x is said to be weakly stationary with respect to S
if it can be written as the response of a linear shift-invariant ﬁlter

H =(cid:80)N−1
write x = Hw for some ﬁlter H = (cid:80)N−1
the covariance matrix Cx := E(cid:2)xxH(cid:3) of the signal x can be

Deﬁnition 2 states that the process x is stationary if we can
l=0 hlSl that we excite
with a white input w. Further observe that if we write x = Hw,

Cx = E(cid:2)(Hw)(Hw)H(cid:3) = HE(cid:2)wwH(cid:3) H = HHH .

(5)
The statement in (5) implies that the color of the process is
determined by the ﬁlter H and that the rank of the covariance
matrix is the rank of H.

written as

As a third alternative deﬁnition, we leverage the observation
that in the case of time signals Cx is characterizable with a

3

PSD. This latter property is a consequence of the fact that the
correlation matrix Cx of a stationary process is circulant and
therefore diagonalized by the Fourier matrix. In the case of a graph
signal the analogy is to have a covariance matrix Cx that can be
diagonalized by the GFT matrix V as we formally state next.

Deﬁnition 3 Given a normal shift operator S, a zero-mean ran-
dom process x is said to be weakly stationary with respect to S
if the matrices Cx and S are simultaneously diagonalizable.

We remark that the different deﬁnitions of stationarity that we
state in Deﬁnitions 1, 2, and 3 are with respect to a graph-shift
operator S, which is required to be normal. We also emphasize
that Deﬁnitions 1, 2, and 3 are equivalent for time signals by
construction. As we shall see next, despite appearing possibly
different, they are also equivalent for graph signals in most graphs.

eigenvalues of the positive semideﬁnite correlation matrix Cx.
Thus, (8) is equivalent to

Cx = Vdiag(p)VH .

(9)

Further note that if we interpret x as the output of a graph ﬁlter
applied to a white input with unitary variance [cf. (3)], it follows
from comparing (7) and (8) that the PSD is given by the squared
magnitude of the ﬁlter’s frequency response, i.e., p = |˜h|2. This
latter observation is consistent with the interpretation that the color
of a process is determined by the frequency response ˜h of the ﬁlter
H. We give representative examples below and move on to study
properties of the PSD p and the correlation Cx.

Example 1 (White noise) Zero-mean white noise is stationary
in any graph shift S. The PSD of white noise with covariance

E(cid:2)wwH(cid:3) = σ2I is p = σ21.

SaCxSb = Sa+cCxSb−c.

Proposition 1 If the eigenvalues of S are all distinct, Deﬁnitions
1, 2, and 3 are equivalent.
Proof: If Deﬁnition 1 holds, reorder terms in (4) to conclude that
(6)
For (6) to be true, Sc and Cx must commute for all c. In turn, this
can happen if and only if S and Cx are simultaneously diagonal-
izable [15], which implies Deﬁnition 3. Conversely, if Deﬁnition
3 holds we can write Cx = Vdiag(λc)VH. Substituting this
expression into (6) the equality checks and Deﬁnition 1 follows.
After proving that Deﬁnitions 1 and 3 are equivalent, we
show that under the conditions in the proposition, Deﬁnition 2
is equivalent to 3. Assume ﬁrst that Deﬁnition 2 holds. Since
the graph ﬁlter H in (5) is linear shift invariant, it is completely
characterized by its frequency response ˜h = Ψh as stated in (2).
Using this characterization we rewrite (5) as

Cx =(cid:0)Vdiag(˜h)VH(cid:1)(cid:0)Vdiag(˜h)VH(cid:1)H = Vdiag2(cid:0)|˜h|(cid:1)VH.

(7)

This means that Deﬁnition 3 holds. Conversely,
if Deﬁnition
3 holds it means that we can write Cx = Vdiag(λc)VH for
√
some component-wise nonnegative vector λc. Deﬁne now vector
λc where the square root is applied element-wise. Then, for
Deﬁnition 2 to hold, there must exist a ﬁlter with coefﬁcients h
satisfying ˜h = Ψh =
λc. Since Ψ is Vandermonde, the system
is guaranteed to have a solution with respect to h provided that
all modes of Ψ (the eigenvalues of S) are distinct, as required in
the proposition.

√

As seen from the proof of Proposition 1, Deﬁnitions 1 and 3
are equivalent, while Deﬁnition 2 implies Deﬁnitions 1 and 3.
The shift S having distinct eigenvalues is required for Deﬁnitions
1 and 3 to imply Deﬁnition 2. Note that if for the shift S one
has that λk = λk(cid:48), then the frequency response of a graph ﬁlter
satisﬁes ˜hk = ˜hk(cid:48), which implies that the generated covariance,
besides being simultaneously diagonalizable, must have repeated
eigenvalues too.

The fact of Cx being diagonalized by the GFT matrix motivates

the following deﬁnition.

Deﬁnition 4 The power spectral density (PSD) of a random
process x that is stationary with respect to the normal graph shift
S = VΛVH is the nonnegative N × 1 vector p

p := diag(cid:0)VH CxV(cid:1) .

(8)

Since Cx is diagonalized by V [cf. (3)] the matrix VH CxV
is diagonal and it follows that the PSD in (8) corresponds to the

Example 2 (Covariance matrix graph) Any random process is
stationary with respect to the graph shift S = Ca
x deﬁned as the
a-th power of the covariance matrix, with a < 0 only if the inverse
exists. The PSD in this case is p = diag(Λ)1/a. Two examples
of particular interest are setting S = Cx (the covariance matrix
graph) and S = C−1

(the precision matrix graph).

x

(cid:80)∞
Example 3 (Heat diffusion process) Suppose that a zero-mean
white input w diffuses through a graph with Laplacian L =
l=0(αL)lw = α0(I −
VΛVH to generate the signal x = α0
αL)−1w. The process x is stationary in the shift S = L with PSD
0(I − αΛ)−2] because we can write the covariance
p = diag[α2
0(I − αΛ)−2VH [cf. (9)].
matrix as Cx = α2
This is a particular case of a more general category that we discuss
in Sections III-B and V-B.

0(I − αL)−2 = Vα2

A. Properties of Graph Stationary Processes

We start by analyzing the effect of a linear graph ﬁlter on
the covariance matrix and the PSD of a stationary graph random
process.

Property 1 Let x be a stationary process in S with covariance
matrix Cx and PSD px. Consider a ﬁlter H with coefﬁcients h
and frequency response ˜h and deﬁne y := Hx as the response
of H to input x. Then, the process y:

(a) Is stationary in S with covariance Cy = HCxHH.
(b) Has a PSD given by py = |˜h|2 ◦ px.

Proof: Use the deﬁnition y := Hx to compute the covariance

(10)
Since the process x is stationary with PSD px we use (9) to write
Cx = Vdiag(px)VH. We attempt to diagonalize Cy with V and
use this expression along with the expression in (10) to write

Cy = E(cid:2)yyH(cid:3) = HE(cid:2)xxH(cid:3) HH = HCxHH .
VH CyV = VH H(cid:0)Vdiag(px)VH(cid:1) HH VH .

(11)

Regrouping factors in (11) and observing that the ﬁlter’s graph
frequency response is ˜h = diag(VH HV) we can reduce (11) to

VH CyV = diag(˜h)diag(px)diag(˜h)H .

(12)
Since matrices in the right hand side of (12) are diagonal, Cy is
diagonalized by V. This means y is stationary in S with Cy as
in (10) and (a) follows. Since diagonal matrices commute in (12),
(b) follows from the PSD deﬁnition in (8).

4

Property 1 is a statement of the spectral convolution theorem
for stationary graph signals. When ﬁltering stationary graph
processes, the PSD of the output can be found as the product
of the PSD of the input with the squared magnitude of the
frequency response of the ﬁlter. Property 1 is also a generalization
of Deﬁnition 2 for inputs that are not necessarily white.

In time signals the correlation matrix often conveys a notion of
locality as the signiﬁcant values of Cx accumulate close to the
diagonal. To prove a similar result for graph signals, recall that
Nl(i) denotes the l-hop neighborhood of a node i. Formally, this
set can be written as Nl(i) := {j : [Sl(cid:48)
]ij (cid:54)= 0 for some l(cid:48) ≤ l}.
The following proposition shows that the correlation between l-
hop neighbors depends on the order L of the ﬁlter that generates
x from white noise.

a linear graph ﬁlter H =(cid:80)L−1
((cid:80)L−1
l=0 hlSl)((cid:80)L−1

Property 2 Let x = Hw be a process written as the response of
l=0 hlSl of degree L − 1 to a white
input. Then, [Cx]ij = 0 for all j /∈ N2(L−1)(i).
Proof : Using (5) the covariance of x is Cx = HHH =
l=0 hl(SH )l). Hence, Cx is a polynomial of S
and SH, with the highest degree monomial being (SSH )L−1. All
of these monomials have zero (i, j) entries if j /∈ N2(L−1)(i).

Property 2 puts a limit on the spatial extent of the components
xj of a graph signal that can be correlated with a given element xi.
Only those elements that are in the 2(L− 1)-hop neighborhood –
i.e., elements xj with indices j ∈ N2(L−1)(i) – can be correlated
with xi. This spatial limitation of correlations can be used to
design windows for spectral estimation as we explain in Section
IV-B.

We close by considering the covariance matrix associated with

the GFT of a graph stationary process.

Property 3 Given a process x that is stationary in S = VΛVH
with PSD p, deﬁne the GFT process as ˜x = VH x. The covariance
matrix of ˜x is

C˜x := E(cid:2)˜x˜xH(cid:3) := E(cid:2)(VH x)(VH x)H(cid:3) = diag(p).
deﬁnitions we have C˜x := E(cid:2)˜x˜xH(cid:3) = E(cid:2)VH xxH V(cid:3) =
VHE(cid:2)xxH(cid:3) V = VH CxV. To conclude,

just observe that
VH CxV is diagonal for a stationary process x (cf. Deﬁnition
3) and that p = diag(VH CxV) as per Deﬁnition 4.

is by direct computation.

Proof : The proof

Indeed, using

(13)

An important consequence of Property 3 is that the elements of
˜x are uncorrelated. This provides motivation for the analysis and
modeling of stationary graph processes in the frequency domain
which we undertake in ensuing sections. It also shows that if a
process x is stationary in the shift S = VΛVH, then the GFT
VH provides the Karhunen-Lo`eve expansion of the process. This
is as it should be because Cx is diagonalized by V.

Remark 1 Deﬁnitions 1, 2, and 3 assumed that
the random
process x has mean ¯x := E [x] = 0. In time processes, stationarity
implies that the mean must be constant. This restriction can be
incorporated into our deﬁnitions by requiring ¯x = ¯x1 for some
scalar ¯x. This is possible but it is not difﬁcult to see that for a
generic S this restriction invalidates (the desirable) Property 1.
Alternatively, we can require ¯x = ¯xvk where vk is an arbitrary
eigenvector of S. This choice maintains validity of Property 1.
If S is either the adjacency matrix of the directed cycle or the
Laplacian of any graph and we set vk = 1, the choices coincide.
The selection of vk = 1 is further justiﬁed because 1 is the

Laplacian eigenvector associated with smallest total variation [16]
and therefore a natural choice for the notion of DC component.

Remark 2 Deﬁnitions 2 and 3 are straightforward extensions of
time properties but Deﬁnition 1 makes use of reference shifts
Sa and Sb from which we move forward and backward with
the shift Sc. This is a more contrived deﬁnition of the more

natural requirement of making E(cid:2)Slx(Slx)H(cid:3) equal to E(cid:2)xxH(cid:3).

To understand the justiﬁcation of this choice write Sl = VΛlVH
and utilize the PSD deﬁnition in (8) to write the covariance of
the shifted signal Slx as

E(cid:2)Slx(Slx)H(cid:3) = VΛldiag(p)(Λl)H VH .

(14)

The expression in (14) implies that the shifted signal Slx is also
stationary. However, when the eigenvalues of the shift operator
are different from the identity, the PSD of Slx is scaled by
the eigenvalue norms |Λ|2l. This issue is circumvented in [6],
which deﬁnes stationarity based on the novel translation operator
introduced in [17]. However, such an operator is not local and thus
hard to implement in a distributed fashion. The use of reference
shifts in Deﬁnition 1 avoids the need of deﬁning this new operator.

B. Stationarity in Network Diffusion Processes

= x(l)

i −γ(l)(cid:80)

Many graph processes are characterized by local interactions
between nodes of a (sparse) graph that can be approximated as
linear [12], [13]. This implies that we can track the evolution of
the signal at the ith node through a local recursion of the form
x(l+1)
j sijx(l)
j where the system is seeded with an
i
initial condition x(0)
, sij are elements of the graph shift operator,
and γ(l) are time varying diffusion coefﬁcients. The diffusion halts
after L iterations – which can be possibly inﬁnite – to produce
the output yi = x(L)
. Diffusion dynamics appear in, e.g., network
control systems [18], opinion formation [12], brain networks [19],
and molecular communications [20].
Utilizing the graph shift operator, diffusion dynamics can be
written as x(l+1) = (I − γ(l)S)x(l) and the output y = x(L) can
be written in terms of the input as

i

i

y = x(L) =(cid:81)L

l=0(I − γ(l)Sl)x.

(15)

y as the output of a graph ﬁlter H = (cid:80)N−1

Regardless of whether L is ﬁnite or inﬁnite, the Cayley-Hamilton
theorem guarantees that the mapping can always be expressed as a
polynomial of S with degree less than N. Hence, we can construe
l=0 hlSl applied to
the initial condition x. If we now think of x as a process that
is stationary in S it follows that the process y – which ﬁlters
realizations of x according to (15) – is also stationary. This is a
corollary to Property 3 that we state below for future reference.

Corollary 1 A process y is obtained by the linear diffusion
dynamics in (15) in response to an input process x. If x is
stationary in S, so is y.

Proof: Property 1 applies because (cid:81)L

l=0(I − γ(l)Sl) is a linear

shift-invariant ﬁlter.

The corollary includes white seeds x as a particular input model
and the heat diffusion dynamics in Example 3 as a particular case
with γ(l) = γ for all l and L = ∞.

IV. NONPARAMETRIC PSD ESTIMATION

The interest in this and the following section is in estimating
the PSD of a random process x that is stationary with respect
to S using as input either one or a few realizations {xr}R
r=1 of
x. In this section we consider nonparametric methods that do
not assume a particular model for x. We generalize to graph
signals the periodogram, correlogram, windowing, and ﬁlter bank
techniques that are used for PSD estimation in the time domain.
Parametric methods are analyzed in Section V.

A. Periodogram and Correlogram

(13) to conclude that the PSD can be written as p = E(cid:2)|VH x|2(cid:3).

Since Property 3 implies that C˜x is diagonal, we can rewrite

This yields a natural approach to estimate p with the GFT of
realizations of x. Thus, compute the GFTs ˜xr = VH xr of each
of the samples xr in the training set and estimate p as

R(cid:88)

r=1

(cid:12)(cid:12)VH xr

(cid:12)(cid:12)2

R(cid:88)

r=1

ˆppg :=

1
R

|˜xr|2 =

1
R

.

(16)

The estimator in (16) is the analogous of the periodogram of time
signals and is referred as such from now on. Its intuitive appeal
is that it writes the PSD of the process x as the average of the
squared magnitudes of the GFTs of realizations of x.

Alternatively, one can replace Cx in (8) by its empirical
r and propose the PSD estimate

r=1 xrxH

estimate ˆCx = (1/R)(cid:80)R
(cid:17)

(cid:16)

(cid:20) 1

(cid:20)

(cid:21)

(cid:21)

R(cid:88)

ˆpcg := diag

VH ˆCxV

:= diag

VH

xrxH
r

V

.

(17)

R

r=1

An important observation in the correlogram deﬁnition in (17) is
that the empirical covariance ˆCx is not necessarily diagonalized
by V. However, since we know that the actual covariance Cx is
diagonal, we retain only the diagonal elements of VH ˆCxV to
estimate the PSD of x. The expression in (17) is the analogous
of the time correlogram. Its intuitive appeal is that it estimates
the PSD with a double GFT transformation of the empirical
covariance matrix.

Although different in genesis, it is apparent from their respec-
tive expressions that the periodogram in (16) and the correlogram
in (17) are identical estimates as we prove next.
Proposition 2 Given observations {xr}R
(16) and the correlogram in (17) are equivalent, i.e.,

r=1, the periodogram in

(cid:12)(cid:12)VH xr

(cid:12)(cid:12)2

R(cid:88)

r=1

ˆppg :=

1
R

(cid:20)

(cid:20) 1

R(cid:88)

R

r=1

(cid:21)

(cid:21)

=diag

VH

xrxH
r

V

:= ˆpcg. (18)

Proof: Move matrices VH and V into the empirical covariance
sum in the correlogram deﬁnition in (18). Observe that
the
summands end up being of the form (VH xr)(VH xr)H. The
diagonal elements of these outer products are |VH xr|2, which
are the summands in the periodogram deﬁnition in (18).

The result in Proposition 2 is consistent with the equivalence of
correlograms and periodograms in time signals. Henceforth, we
choose to call ˆppg = ˆpcg the periodogram estimate of p.

To evaluate the performance of the periodogram estimator in
(16) we assess its mean and variance. The estimator is unbiased
by design and, as we shall prove next, this is easy to establish
formally. To study the estimator’s variance we need the additional
hypothesis of the process having a Gaussian distribution. Expres-
sions for means and variances of periodogram estimators are given
in the following proposition.

5

Proposition 3 Let p be the PSD of a process x that is stationary
with respect to the shift S = VΛVH. Independent samples
{xr}R
r=1 are drawn from the distribution of the process x and
the periodogram ˆppg is computed as in (16). The bias bpg of the
estimator is zero,

bpg := E [ˆppg] − p = 0.

(19)
Further deﬁne the covariance matrix of the periodogram estimator

as Σpg := E(cid:2)(ˆppg − p)(ˆppg − p)H(cid:3). If the process x is assumed
Σpg := E(cid:2)(ˆppg − p)(ˆppg − p)H(cid:3) = (2/R)diag2(p).

Gaussian and S is symmetric, the covariance matrix can be written
as

(20)

Proof: See Appendix A.

The expression in (19) states that the bias of the periodogram is
bpg = 0, or, equivalently, that the expectation of the periodogram
estimator is the PSD itself, i.e., E [ˆppg] = p. The expression for
the covariance matrix of Σpg holds true only when the process
x has a Gaussian distribution. The reason for this limitation is
that the determination of this covariance involves operations with
the fourth order moments of the process x. This necessity and
associated limitation also arise in time signals [4, Sec. 8.2]. To
help readability and intuition, the covariance Σpg in (20) is stated
for a symmetric S, however, in Appendix A the proof is done for
a generic normal, not necessarily symmetric, S.

The variance expression in (20) is analogous to the periodogram
variances of PSDs of time domain signals in that: (i) Estimates of
different frequencies are uncorrelated – because Σpg is diagonal.
(ii) The variance of the periodogram is proportional to the square
of the PSD. The latter fact is more often expressed in terms of
the mean squared error (MSE), which we deﬁne as MSE(ˆppg) :=

(cid:3) and write as [cf. (19) and (20)]

E(cid:2)(cid:107)(ˆppg − p)(cid:107)2

MSE(ˆppg) = (cid:107)bpg(cid:107)2

2 + tr[Σpg] = (2/R)(cid:107)p(cid:107)2
2.

(21)
As it happens in time signals, the MSE in (21) is large and results
in estimation errors that are on the order of the magnitude of the
frequency component itself. Two of the workarounds to reduce
the MSE in (21) are the use of windows and ﬁlterbanks. Both
tradeoff bias for variance as we explain in the following sections.

2

B. Windowed Average Periodogram

The Bartlett and Welch methods for PSD estimation of time
signals utilize windows to, in effect, generate multiple samples of
the process even if only a single realization is given [5, Sec. 2.7].
These methods reduce variances of PSD estimates but introduce
some distortion (bias). The purpose of this section is to deﬁne
counterparts of windowing methods for PSD estimation of graph
signals.

To understand the use of windows in estimating a PSD let us
begin by deﬁning windows for graph signals and understanding
their effect on the graph frequency domain. We say that a signal
w is a window if its energy is (cid:107)w(cid:107)2
2 = N. Applying the
window w to a signal1 x entails componentwise multiplication
to produce the signal xw = diag(w)x. In the graph frequency
domain we can use the deﬁnition of the GFT ˜xw = VH xw,
the deﬁnition of the windowed signal xw = diag(w)x, and the
deﬁnition of the iGFT to write

2 = (cid:107)1(cid:107)2

˜xw = VH xw = VHdiag(w)x = VHdiag(w)V˜x := ˜W˜x,

(22)
where in the last equality we deﬁned the dual of the windowing
operator diag(w) in the frequency domain as the matrix ˜W :=

1To keep notation simple, we use x to denote a realization of process x.

6

VHdiag(w)V. In time signals the frequency representation of a
window is its Fourier transform and dual operator of windowing is
the convolution between the spectra of the window and the signal.
This decoupled explanation is lost in graph signals. Nonetheless,
(22) can be used to design windows with small spectral distortion.
Ideal windows are such that ˜W = I which can be achieved
by setting w = 1, although this is unlikely to be of any use.
More interestingly, (22) implies that good windows for spectral
estimation must have ˜W ≈ I or can allow nonzero values in
columns k where the components ˜xk of ˜x are known or expected
to be small.

Turning now to the problem of PSD estimation, consider the
estimate ˆp obtained after computing the periodogram in (16) using
a single realization x, so that we have that ˆp = |VH x|2. Suppose
now that we window the realization x to produce xw = diag(w)x

and compute the windowed periodogram ˆpw := (cid:12)(cid:12)VH xw
ˆpw :=(cid:12)(cid:12)VH xw

Utilizing the deﬁnition of the window’s frequency representation,
we can write this windowed periodogram as

=(cid:12)(cid:12)VHdiag(w)x(cid:12)(cid:12)2

=(cid:12)(cid:12) ˜WVH x(cid:12)(cid:12)2

(cid:12)(cid:12)2.

(cid:12)(cid:12)2

(23)

.

The expression in (23) can be used to compute the expectation
of the windowed periodogram that we report in the following
proposition.

Proposition 4 Let p be the PSD of a process x that is stationary
with respect to the shift S = VΛVH. The expectation of the
windowed periodogram ˆpw in (23) is,

E [ˆpw] = ( ˜W ◦ ˜W∗)p.

Proof : Write ˆpw = (cid:12)(cid:12) ˜WVH x(cid:12)(cid:12)2
Take expectation and use E(cid:2)xxH(cid:3) = Cx to write E [ˆpw] =

= diag( ˜WVH xxH V ˜W).

(24)

diag( ˜WVH CxV ˜WH ). Further observe that VH CxV = diag(p)
to conclude that ˆpw = diag( ˜Wdiag(p) ˜WH ). This latter expres-
sion is identical to (24).

Proposition 4 implies that the windowed periodogram in (25)
is a biased estimate of the PSD p and that the bias is determined
by the dual of the windowing operator in the frequency domain
˜W.
Multiple windows yield, in general, better estimates than single
windows. Consider then a bank of M windows W = {wm}M
m=1
and use each of the windows wm to construct the windowed signal
xm := diag(wm)x. We estimate the PSD p with the windowed
average periodogram

(cid:12)(cid:12)VH xm

(cid:12)(cid:12)2

M(cid:88)

m=1

=

1
M

M(cid:88)

m=1

(cid:12)(cid:12)VHdiag(wm)x(cid:12)(cid:12)2

ˆpW :=

1
M

.

(25)

The estimator ˆpW is an average of the windowed periodograms
in (25) but is also reminiscent of the periodogram in (16). The
difference is that in (16) the samples {xr}R
r=1 are independent ob-
servations whereas in (25) the samples {xm}M
m=1 are all generated
through multiplications with the window bank W. This means
that: (i) There is some distortion in the windowed periodogram
estimate because the windowed signals xm are used in lieu of x.
(ii) The different signals xm are correlated with each other and
the reduction in variance resulting from the averaging operation
in (25) is less signiﬁcant than the reduction of variance that we
observe in Proposition 3.

To study these effects, given the windowing operation
diag(wm), we obtain its dual in the frequency domain as ˜Wm :=
VHdiag(wm)V [cf. (22)], and use those to deﬁne the power spec-
trum mixing matrix of windows m and m(cid:48) as the componentwise

product

˜Wmm(cid:48) := ˜Wm ◦ ˜W∗
m(cid:48).

(26)

We use these matrices to give expressions for the bias and
covariance of the estimator in (25) in the following proposition.

Proposition 5 Let p be the PSD of a process x that is stationary
with respect to the shift S = VΛVH. A single observation x
is given along with the window bank W = {wm}M
m=1 and the
windowed average periodogram ˆpW is computed as in (25). The
expectation of the estimator ˆpW is

E [ˆpW ] =

1
M

˜Wmmp.

(27)

M(cid:88)

m=1

Equivalently, ˆpW is biased with bias bW := E [ˆpW ]− p. Further
deﬁne the covariance matrix of the windowed periodogram as

ΣW := E(cid:2)(ˆpW − E [ˆpW ])(ˆpW − E [ˆpW ])H(cid:3). If the process x is

assumed Gaussian and S is symmetric, the trace of the covariance
matrix can be written as

M(cid:88)

(cid:104)(cid:0) ˜Wmm(cid:48)p(cid:1)(cid:0) ˜Wmm(cid:48)p(cid:1)H(cid:105)

tr

.

(28)

tr[ΣW ] =

2
M 2

m=1,m(cid:48)=1

Proof : See Appendix B, where the expression of tr[ΣW ] for
nonsymmetric normal shifts is given too [cf. (64)].

The ﬁrst claim of Proposition 5 is a generalization of the bias
expression for the bias of windowed periodograms in (24). It states
that the estimator ˆpW is biased with a bias determined by the
average of the spectrum mixing matrices ˜Wmm = ˜Wm ◦ ˜W∗
m.
The form of these mixing matrices depends on the design of the
window bank W = {wm}M
m=1 and on the topology of the graph
G. Observe that even if the individual spectrum mixing matrices
˜Wmm are not close to identity we can still have small bias by

making their weighted sum M−1(cid:80)

˜Wmm ≈ I.

The second claim in Proposition 5 is a characterization of the
trace of the covariance matrix ΣW. To interpret the expression
in (28) it is convenient to separate the summands with m = m(cid:48)
from the rest to write

m

M(cid:88)

m=1

tr

(cid:104)(cid:0) ˜Wmmp(cid:1)(cid:0) ˜Wmmp(cid:1)H(cid:105)
M(cid:88)

(cid:104)(cid:0) ˜Wmm(cid:48)p(cid:1)(cid:0) ˜Wmm(cid:48)p(cid:1)H(cid:105)

tr

(29)

+

2
M 2

m=1,m(cid:48)(cid:54)=m

tr[ΣW ] =

2
M 2

Comparing (29) with (24), we see that the terms in the ﬁrst
summand are tr[( ˜Wmmp)( ˜Wmmp)H ] = (cid:107)E[ˆpwm](cid:107)2
2. Given
2 = (cid:107)1(cid:107)2
that the windows have energy (cid:107)w(cid:107)2
2 = N we expect
(cid:107)E[ˆpwm](cid:107) ≈ (cid:107)E [ˆpx](cid:107)2 = (cid:107)p(cid:107)2. This implies that the ﬁrst sum
in (29) is approximately proportional to 2(cid:107)p(cid:107)2
2/M. Thus, this ﬁrst
term behaves as if the different windows in the bank W were
generating independent samples [cf. (21)].

The effect of the correlation between different windowed sig-
nals appears in the cross terms of the second sum, which can
be viewed as the price to pay for the signals {xm}M
m=1 being
generated from the same realization x instead of actually being
independent. We explain in the following section that the price
we pay in this second sum is smaller than the gain we get in the
ﬁrst term.
Window design. The overall MSE is given by the squared bias
norm summed to the trace of the covariance matrix,

MSE(ˆpW ) = (cid:107)bW(cid:107)2

2 + tr[ΣW ].

(30)

The expression in (30) can be used to design (optimal) windows
with minimum MSE. Do notice that the bias in (30) depends
on the unknown PSD p. This problem can be circumvented by
making p = 1 in the bias and trace expressions in Proposition
5. This choice implies that the PSD is assumed white a priori. If
some other knowledge of the PSD is available, it can be used as
an alternative prior. Irrespectively of the choice of prior, ﬁnding
windows that minimize the MSE is computationally challenging.
An alternative approach to design the window bank W =
{wm}M
m=1 is to exploit the local properties of the random process
x. As stated in Property 2, some stationary processes are expected
to have correlations with local structure. It is then reasonable
to expect
that windows without overlap capture independent
information that results in a reduction of the cross-terms in (29).
This intuition is formalized in the following proposition.

:

[wm]i

Proposition 6 Consider two windows wm and wm(cid:48) and assume
that the distance between any node in wm and any node in wm(cid:48) is
larger than 2L hops. If the process x can be modeled as the output
of an L-degree ﬁlter, then it holds that tr[ ˜Wmm(cid:48)p( ˜Wmm(cid:48)p)H ] =
0.
Proof : Let N (wm) = {i
(cid:54)= 0} be the set of
nodes in the support of wm, and N (wm(cid:48)) the ones in the
support of wm(cid:48). We know that the distance between any i ∈
N (wm) and any i(cid:48) ∈ N (wm(cid:48)) is greater than 2L . Invoking
Property 2, this implies that [Cx]ii(cid:48) = 0, which allows us to
write diag(ei)Cxdiag(ei(cid:48)) = 0. Since this is true for any pair
(i, i(cid:48)) with i ∈ N (wm) and i(cid:48) ∈ N (wm(cid:48)), we have that:
f1) diag(wm)Cxdiag(wm(cid:48)) = 0. Note now that ˜Wmm(cid:48)p =
˜Wm ◦ ˜W∗
m(cid:48), which using the deﬁnitions
˜Wm = VHdiag(wm)V and Cx = Vdiag(p)VH can be written
as: f2) ˜Wmm(cid:48)p = VHdiag(wm)Cxdiag(w∗
m(cid:48))V. Sustituting f1)
into f2) yields ˜Wmm(cid:48)p = VH 0V = 0.

m(cid:48)p = ˜Wmdiag(p) ˜W∗

The result in Proposition 6 implies that if we use windows
without overlap, the second sum in (29) is null. This means
that the covariance matrix ΣW of the PSD estimator in (25)
behaves as if the separate windowed samples were independent
samples. We emphasize that Proposition 6 provides a lax bound
for a rather stringent correlation model. The purpose of the result
is to illustrate the reasons why local windows are expected to
reduce estimation MSE. In practice, we expect local windows to
reduce MSE as long as the correlation decreases with the hop
distance between nodes. Windowing design can then be related
to clustering (if the windows are non-overlapping) and covering
(if they overlap) with the goal of keeping the diameter of the
window large enough so that most of the correlation of the process
is preserved. Section VI evaluates the estimation performance
when estimating the PSD of a graph process for different types
of windows.

C. Filter Banks

Windows reduce the MSE of periodograms by exploiting the
locality of correlations. Filter banks reduce MSE by exploiting
the locality of the PSD [5, Sec. 5]. To deﬁne ﬁlter bank PSD
estimators for graph processes, suppose that we are given a ﬁlter
bank Q := {Qk}N
k=1 with N ﬁlters (as many as frequencies).
The ﬁlters Qk are assumed linear shift invariant with frequency
responses ˜qk, so that we can write Qk = Vdiag(˜qk)VH. We
further assume that their energies are normalized (cid:107)˜qk(cid:107)2
2 = 1. The
kth (bandpass) ﬁlter is intended to estimate the kth component
of the PSD p = [p1, . . . , pN ]T through the energy of the output

signal xk := Qkx, i.e.,

ˆp˜qk := (cid:107)xk(cid:107)2

2 = (cid:107)Qkx(cid:107)2
2.

7

(31)

The ﬁlter bank PSD estimate is given by the concatenation
of the individual estimates in (31) into the vector ˆpQ :=
[ˆp˜q1 , . . . , ˆp˜qN ]T . We emphasize that we can think of ﬁlter banks
as an alternative approach to generate multiple virtual realizations
xk from a single actual realization x. In the case of windowing,
realizations xm correspond to different pieces of x. In the case
of ﬁlter banks, realizations xk correspond to different ﬁltered
versions of x.

2 = (cid:107)diag(˜qk)˜x(cid:107)2
2.

Using Parseval’s theorem and the frequency representations ˜qk
and ˜x of the ﬁlter Qk and the realization x, respectively, the
estimate in (31) is equivalent to
ˆp˜qk = (cid:107)˜xk(cid:107)2

(32)
The expression in (32) guides the selection of the response ˜qk.
E.g., if the PSD values at frequencies k and k(cid:48) are expected to be
√
similar, i.e., if pk ≈ pk(cid:48) we can make [˜qk]k = [˜qk]k(cid:48) = (1/
2) so
that the PSD components [|VH x|2]k and [|VH x|2]k(cid:48) are averaged.
More generically, the design of the ﬁlter ˜qk can be guided by the
bias and variance expressions that we present in the following
proposition.

Proposition 7 Let p = [p1, . . . , pN ]T be the PSD of a process x
that is stationary with respect to the shift S = VΛVH. A single
observation x is given along with the ﬁlter bank Q = {˜qk}N
k=1
and the ﬁlter bank PSD estimates ˆpQ are computed as in (32).
The expectation of the k entry of ˆpQ is,

E [ˆp˜qk ] := (|˜qk|2)T p.

(33)
Equivalently, ˆp˜qk is biased with bias b˜qk := E [ˆp˜qk ]− pk. Further
deﬁne the variance of the kth entry of the ﬁlter bank estimate ˆpQ

as var [ˆp˜qk ] := E(cid:2)(ˆp˜qk − E [ˆp˜qk ])2(cid:3). If the process x is assumed
var [ˆp˜qk ] := E(cid:2)(ˆp˜qk − E [ˆp˜qk ])2(cid:3) = 2(cid:13)(cid:13)diag(cid:0)|˜qk|2(cid:1) p(cid:13)(cid:13)2

(34)
Proof : See Appendix C, where the expression of var [ˆp˜qk ] for
nonsymmetric normal shifts is given too [cf. (69)].

Gaussian and S is symmetric, the variance can be written as

2 .

k=1 b2
˜qk

(cid:80)N

The variance expression in (33) shows that

the estimation
accuracy of a ﬁlter bank beneﬁts from an averaging effect –
recall that the ﬁlter is normalized to have unit energy (cid:107)˜qk(cid:107)2
2 = 1.
This averaging advantage manifests only if the bias in (33) is
made small so that the overall MSE, given by MSE(ˆpQ) =
+ var [ˆp˜qk ], decreases. In time signals the bias is made
small by exploiting the fact that the PSD of nearby frequencies are
similar. In graph signals, some extra information is necessary to
identify frequency components with similar PSD values. If, e.g.,
the process x is a diffusion process as the one in Example 3, the
PSD components pk and pk(cid:48) are similar – irrespectively of α – if
the eigenvalues λk and λk(cid:48) of the Laplacian L are similar. If the
eigenvalues of the Laplacian are furthered ordered, averaging of
nearby PSD estimates can be interpreted as the use of a bandpass
ﬁlter.

Qk = (cid:80)L

A generic approach to designing bandpass ﬁlters for PSD
estimation is to exploit (FIR) ﬁlters, which are attractive due
their ability to be implemented distributedly [9]. Formally, write
k ]T the vector of
ﬁlter coefﬁcients, and as ˜qk = ΨLqk its frequency response,
where we recall that ΨL stands for the ﬁrst L columns of Ψ. The
coefﬁcients qk could be designed upon substituting ˜qk = ΨLqk
into both (33) and (34) and minimizing the resultant MSE. This

kSl, denote as qk = [q1

k, ..., qL

l=1 ql

8

can be challenging because it involves fourth-order polynomials
and requires some prior knowledge on p. For the purpose of
PSD estimation in time, a traditional approach for suboptimal FIR
design is to select coefﬁcients guaranteeing that [˜qk]k = 1 while
minimizing the out-of-band power [4]. Deﬁning ψT
k,L as the kth
row of ΨL, this can be formalized as

qk := argmin

qL

(cid:107)ΨLqL(cid:107)2
2,

s. t. ψT

k,LqL = 1,

(35)

with the constraint forcing [˜qk]k = 1 and the objective attempting
to minimize the contribution to ˆp˜qk from frequencies other than
k. If we make L ≥ N the solution to (35) is [˜qk]k(cid:48) = 0 for all
k(cid:48) (cid:54)= k. For L < N, the ﬁlter qk has a response in which nonzero
coefﬁcients [˜qk]k(cid:48) are clustered at frequencies k(cid:48) that are similar
to k – in the sense of being associated with multipliers λk(cid:48) ≈ λk.
We further observe that (35) has the additional advantage of being
solved in closed form as
k,LΨH

ψk,L,
which does not have unit energy but can be normalized.

qk =(cid:0)ψT

(cid:1)−1(cid:0)ΨH

(cid:1)−1

L ΨLψ

L ΨL

∗
k,L

(36)

V. PARAMETRIC PSD ESTIMATION

We address PSD estimation assuming that the graph process x
can be well approximated by a parametric model in which x is the
response of a graph ﬁlter H to a white input. As per Deﬁnition
2, this is always possible if the ﬁlter’s order is sufﬁciently large.
The goal here is to devise ﬁlter representations of an order (much)
smaller than the number of signal elements N. Mimicking time
processes, we devise moving average (MA), autoregressive (AR),
and ARMA models.

A. Moving Average Graph Processes

FIR ﬁlter H(β) = (cid:80)L−1

Consider a vector of coefﬁcients β = [β0, ..., βL−1]T and
assume that x is stationary in the graph S and generated by the
l=0 βlSl. The degree of the ﬁlter is less
that N − 1 although we want in practice to have L (cid:28) N. If the
process is indeed generated as the response of H(β) to a white
input, the covariance matrix of the process can be written as

Cx(β) = H(β)HH (β) =

(βlSl)(βl(cid:48)SH )l(cid:48)

.

(37)

L−1(cid:88)

l=0,l(cid:48)=0

The PSD corresponding to the covariance in (37) is the magnitude
squared of the frequency representation of the ﬁlter. To see this
formally, notice that it follows from the deﬁnition in (8) that

p(β) = diag(cid:0)VH Cx(β)V(cid:1). Writing the covariance matrix as

Cx(β) = H(β)HH (β) and the frequency representation of
the ﬁlter as ˜h(β) = diag(VH(β)VH ), it follows readily that
p(β) = |˜h(β)|2. For the purposes of this section it is convenient
to write the latter explicitly in terms of β as [cf. (3)]

p(β) = |˜h(β)|2 = |ΨLβ|2.

(38)
The covariance and PSD expressions in (37) and (38) are graph
counterparts of MA time processes generated by FIR ﬁlters – see
Section III-B for a discussion on their practical relevance.

The estimation of the coefﬁcients β can be addressed in either
the graph or graph frequency domain. In the graph domain we
compute the sample covariance ˆCx = xxH and introduce a
distortion function DC( ˆCx, Cx(β)) to measure the similarity of
ˆCx and Cx(β). The ﬁlter coefﬁcients β are then selected as the
ones with minimal distortion,
ˆβ = argmin

DC( ˆCx, Cx(β)).

(39)

β

L−1(cid:88)

l=0,l(cid:48)=0

Cx(β) =

γl :=(cid:80)

2(L−1)(cid:88)

l=0

The expression for Cx(β) in (37) is a quadratic function of β
that is generally indeﬁnite. The optimization problem in (39) will
therefore be not convex in general.

To perform estimation in the frequency domain we ﬁrst com-
pute the periodogram ˆppg deﬁned in (16). We then introduce a
distortion measure Dp(ˆppg,|ΨLβ|2) to compare the periodogram
ˆppg with the PSD |ΨLβ|2 and select the coefﬁcients β that solve
the following optimization

ˆβ := argmin

β

Dp(ˆppg,|ΨLβ|2).

(40)

We observe that although we use the periodogram in (40), any of
the nonparametric methods of Section IV-A can be used instead.
Since the quadratic form |ΨLβ|2 in (40) is also indeﬁnite, the
optimization problem in (40) is not necessarily convex. In the
particular case when the distortion Dp(ˆppg,|ΨLβ|2) = (cid:107)ˆppg −
|ΨLβ|2(cid:107)2
2 is the Euclidean 2-norm, efﬁcient (phase-retrieval)
solvers with probabilistic guarantees are available [21], [22].
Alternative tractable formulations of (39) and (40) when the shifts
are symmetric, or, when the shifts are positive semideﬁnite and
the ﬁlter coefﬁcients are nonnegative are discussed below.
Symmetric shifts. If the shift S is symmetric, the expression for
the covariance matrix in (37) can be simpliﬁed to a polynomial
of degree 2(L − 1) in S,

βlβl(cid:48)Sl+l(cid:48)

:=

γlSl := Cx(γ)

(41)

In the second equality in (41) we have deﬁned the coefﬁcients
l(cid:48)+l(cid:48)(cid:48)=l βl(cid:48)βl(cid:48)(cid:48) summing all the coefﬁcient crossproducts
that multiply Sl and introduced Cx(γ) to denote the covariance
matrix written in terms of the γ coefﬁcients. We propose now a
relaxation of (39) in which Cx(γ) is used in lieu of Cx(β) to
yield the optimization problem
ˆβ = argmin

DC( ˆCx, Cx(γ)).

(42)

If we add the constraints γl = (cid:80)

γ

l(cid:48)+l(cid:48)(cid:48)=l βl(cid:48)βl(cid:48)(cid:48), the problem in
(42) is equivalent to (39). By dropping these constraints we end up
with a tractable relaxation because (42) is convex for all convex
distortion metrics DC( ˆCx, Cx(γ)). A tractable relaxation of (40)
can be derived analogously.
Nonnegative ﬁlter coefﬁcients. When the shift S is positive
semideﬁnite, the elements of the matrix Ψ are all nonnegative.
If we further restrict the coefﬁcients β to be nonnegative, all the
elements in the product ΨLβ are also nonnegative. This means
that in (40) we can replace the comparison Dp(ˆppg,|ΨLβ|2) by

the comparison Dp((cid:112)ˆppg, ΨLβ) . We can then replace (40) by
for all convex distortion metrics Dp((cid:112)ˆppg, ΨLβ). Do notice that

The optimization problem in (43) is convex, therefore tractable,

(cid:0)(cid:112)ˆppg, ΨLβ(cid:1).

ˆβ := argmin

the objective costs in (43) and (40) are not equivalent and that
(43) requires positive semideﬁnite shifts –such as the Laplacian–
and restricts coefﬁcients to satisfy β ≥ 0. A tractable restriction
of (39) can be derived analogously.

(43)

β≥0

Dp

B. Autoregressive Graph Processes

For some processes it is more convenient to use a parametric
model that generates an inﬁnite impulse response through an
autoregressive ﬁlter. As a simple example consider the heat
diffusion process in Example 3 that is completely characterized

(cid:80)∞

by the diffusion rate α and the scaling coefﬁcient α0. This process
can be represented with the ﬁlter H = α0
l=0 αlSl. If the series
is summable, the ﬁlter can be rewritten as H = α0(I − αS)−1
its frequency response is
from where we can conclude that
˜h = diag(VH HV) = α0diag(I− αΛ−1). This demonstrates that
H can be viewed as a single pole AR ﬁlter – see also [10].
Suppose now that x is a random graph process whose realiza-
tions are generated by applying H = α0(I − αS)−1 to a white
input w. Then, it readily holds that its covariance Cx is [cf. (5)]
(44)

0(I − αS)−1(I − αS)−H ,

Cx(α0, α) = HHH = α2

which implies that the PSD of x is

p(α0, α) = diag(cid:2)α2

0|I − αΛ|−2(cid:3) ,

(45)

conﬁrming the fact that the expression for the PSD of x is similar
to that of a ﬁrst-order AR time-varying process. We can now
proceed to estimate the PSD utilizing the AR parametric models
in (44) and (45) as we did in Section V-A for MA models.
Substituting Cx(α0, α) for Cx(β) in (39) yields a graph domain
formulation and substituting p(α0, α) for |ΨLβ|2 in (40) yields a
graph frequency domain formulation. Since only two parameters
must be estimated the corresponding optimization problems are
tractable.
(cid:81)M
If the ﬁlter H = α0(I − αS)−1 is the equivalent of an AR
process of order one, an AR process of order M can be written
m=1(I − αmS)−1 for some set of diffusion rates
as H = α0
α = [α0, . . . , αM ]T . The frequency response ˜h = diag(VH HV)
of this ﬁlter is

˜h = α0 diag

(I − αmΛ)−1

.

(46)

If we deﬁne the graph process x = Hw with w white and unitary
energy, the covariance matrix Cx can be written as

Cx(α) = α2
0

(I − αmS)−1(I − αmS)−H .

(47)

(cid:20) M(cid:89)

The process x is stationary with respect to S, because of, e.g.,
Deﬁnition 2. The PSD of x can be written as

p(α) = α2

0 diag

|I − αmΛ|−2

.

(48)

m=1

As before, we substitute Cx(α) for Cx(β) in (39) to obtain a
graph domain formulation and substitute p(α) for |ΨLβ|2 in (40)
to obtain a graph frequency domain formulation. For large degree
M the problems can become intractable. Yule-Walker schemes [5,
Sec. 3.4] tailored to graph signals may be of help. Their derivation
and analysis are left as future work.

Remark 3 To further motivate AR processes, consider the ex-
ample of x = Hw with H being a single-pole ﬁlter and w
a white and Gaussian input, so that x is Gaussian too. The
covariance of this ﬁrst-order AR process is given by (44), with its
inverse covariance (precision matrix) being simply Θ := C−1
x =
(ρ)−2(I− αS)H (I− αS) . Since S is sparse, the precision matrix
Θ is sparse too. Speciﬁcally, Θi,j (cid:54)= 0 only if j is in the two-
hop neighborhood of i. Then, it follows that a Gaussian AR
process of order one on the shift S is a Gaussian Markov Random
Fields (GMRF) [23, Ch. 19], with the Markov blanket of a node i
being given by N2(i), i.e., the nodes that are within the two-hop
neighborhood of i. As explained in Example 2, such a process is
stationary both on S and on Θ. The same holds true for an AR

(cid:20) M(cid:89)

m=1

M(cid:89)

m=1

(cid:21)

(cid:21)

9

process of order M, which in this case will give rise to a GMRF
whose Markov blankets are given by the 2M-hop neighborhoods
of the original graph.

C. Autoregressive Moving Average Graph Processes

The techniques in sections V-A and V-B can be combined to
form ARMA models for PSD estimation. However, as is also
done for time signals, we formulate ARMA ﬁlters directly in
the frequency domain as a ratio of polynomials in the graph
eigenvalues. We then deﬁne coefﬁcients a := [a1, ..., aM ]T and
b := [b0, ..., bL−1]T and postulate ﬁlters with frequency response

(cid:104)(cid:0)(cid:80)L−1
l=0 blΛl(cid:1)(cid:0)1 −(cid:80)M
matrices B := (cid:80)L−1
l=0 blSl and A := (cid:80)M

To ﬁnd the counterpart of (49) in the graph domain deﬁne the
m=1 amSm. It then
follows readily that the ﬁlter whose frequency response is in (49)
is H = (I − A)−1B = B(I − A)−1. These expressions conﬁrm
that we can interpret the ﬁlter as the sequential application of
ﬁnite and inﬁnite response ﬁlters.

m=1 amΛm(cid:1)−1(cid:105)

˜h = diag

(49)

If we now deﬁne the graph process x = Hw, its covariance

.

matrix follows readily as

Cx(a, b) = (I − A)−1BBH (I − A)−H .

(50)
Since Cx(a, b) is diagonalized by the GFT V, the process x is
stationary with PSD [cf. (49)]

(cid:104)(cid:12)(cid:12)(cid:80)L−1
l=0 blΛl(cid:12)(cid:12)2 (cid:12)(cid:12)1 −(cid:80)M

m=1 amΛm(cid:12)(cid:12)−2(cid:105)

p(a, b) =diag

(51)

.

As in the AR and MA models, we can identify the model coefﬁ-
cients by minimizing the covariance distortion DC( ˆCx, Cx(a, b))
or the PSD distortion Dp(ˆppg, p(a, b)) [cf. (38) and (39)]. These
optimization problems are computationally difﬁcult.
Alternative estimation schemes can be obtained by reordering
(50) into (I− A)Cx(I− A)H = BBH and solving for either the
graph domain distortion

(cid:0)(I − A) ˆCx(I − A)H , BBH(cid:1).

(52)

(ˆa, ˆb) := argmin

a,b

DC

or the graph frequency domain distortion

(ˆa, ˆb) := argmin

(cid:104)(cid:12)(cid:12)1 −(cid:80)M

a,b

m=1 amΛm(cid:12)(cid:12)2 ˆppg, diag

(cid:104)(cid:12)(cid:12)(cid:80)L−1
l=0 blΛl(cid:12)(cid:12)2(cid:105)(cid:105)

.

(53)

Dp

The formulations in (52) and (53) can still be intractable but
we observe that the problems have the same structure as the
ones considered in Section V-A. The tractable relaxation that
we discussed for symmetric shifts and the tractable restriction
to nonnegative ﬁlter coefﬁcients for positive semideﬁnite shifts
can be then used here as well.

Remark 4 The parametric methods in this section are well tai-
lored to PSD estimation of diffusion processes – see Section III-B.
When L (cid:28) N the dynamics in (15) are accurately represented
by a low-order FIR model. Parametric estimation with a MA
model as in Section V-A is recommendable. Single pole AR
models arise when γ(l) = γ for all l and L = ∞ as we already
explained in Section V-B. An AR model of order M arises when
M single pole models are applied sequentially. The latter implies
that AR models are applicable when the diffusion constants γ(l)
are constant during stretches of time or vary slowly with time. If
we consider now M diffusion dynamics with constant coefﬁcients
γ(l) = γm for all l running in parallel and we produce an output

10

as the sum of the M outcomes we obtain an ARMA system with
M poles and M − 1 zeros [10].
Remark 5 The methods for PSD estimation that we presented
in Sections IV and V can be used for covariance estimation as
well. This follows directly for some of the parametric formulations
in which we estimate ﬁlter coefﬁcients that minimize a graph
domain distortion – these include (39), its relaxation in (42), and
the analogous formulations in Sections V-B and V-C. When this
is not done, an estimate for Cx can be computed from a PSD
estimate as ˆCx = Vdiag(ˆppg)VH.

VI. NUMERICAL EXPERIMENTS

The implementation and associated beneﬁts of the proposed
schemes are illustrated through three test cases (TC). TC1 and
TC2 rely on synthetic graphs to evaluate the performance of
nonparametric and parametric PSD estimation methods. TC3 illus-
trates how the concepts and tools of stationary graph processes
can be leveraged in practical applications involving real-world
signals and graphs. Unless otherwise stated, the results shown are
averages across 100 realizations of the particular experiment.
TC1. Nonparametric methods: We ﬁrst evaluate the estimation
performance of the average periodogram [cf. (16) and (21)] as a
function of R, the number of realizations observed. Consider a
baseline Erd˝os-R´enyi (ER) graph with N = 100 nodes and edge
probability p = 0.05 [24]. We deﬁne its adjacency matrix as the
shift and generate signals by ﬁltering white Gaussian noise with
a ﬁlter of degree 3. In this case, the normalized MSE equals
2/R [cf. (21)] as can be corroborated in Fig. 1a (top). To further
conﬁrm this result, we consider three variations of the baseline
setting: i) a smaller ER graph with N = 10 nodes and p = 0.3,
ii) a small-world graph [25] obtained by rewiring with probability
q = 0.1 the edges in a regular graph of the same size as the
baseline ER, and iii) ﬁltering the noise with a longer ﬁlter of
degree 6. As expected, Fig. 1a (top) indicates that the normalized
MSE is independent of these variations. We then repeat the above
setting but for signals generated as ﬁltered versions of non-
Gaussian white noise drawn from a uniform distribution of unit
variance. Even though the MSE expression in (21) was shown
for Gaussian signals, we observe that in the tested non-Gaussian
setup the evolution of the MSE with R is the same; see Fig. 1a
(bottom).

The second experiment evaluates the performance of window-
based estimators. To assess the role of locality in the window
design, we consider graphs generated via a stochastic block
model [26] with N = 100 nodes and 10 communities with
10 nodes each. The edge probability within each community is
p = 0.9, while the probability for edges across communities is
q = 0.1. We design rectangular non-overlapping windows where
the nodes are chosen following two strategies: i) M = 10 local
windows corresponding to the 10 communities, and ii) M = 10
windows of equal size with randomly chosen nodes. We use
the Laplacian as shift and generate the graph process using a
ﬁlter with L = 2 coefﬁcients. Fig. 1b shows the theoretical
and empirical normalized MSE for the two designs as well as
that of the periodogram for a single window. We ﬁrst observe
that the periodogram has no bias and that the theoretical and
empirical errors coincide for the three cases, validating the results
in Propositions 3 and 5, respectively. Moreover, we corroborate
that windowing contributes to reduce the variance of the estimator.
Fig. 1b also illustrates that windows that leverage the community
structure of the graph yield a better estimation performance. To
gain insights on the latter observation, we now consider a small-
world graph of size N = 100 obtained by rewiring with probability

q = 0.05 a regular graph where each node has 10 neighbors.
Both local and random windows are considered, where the local
windows are obtained by applying complete linkage clustering
[27] to a metric space given by the shortest path distances between
nodes. In order to obtain increasing number of windows M,
we cut the output dendrogram at smaller resolutions [28]. The
random windows are designed to have the same sizes as the
local ones. The windows are tested for graph processes generated
by two ﬁlters of different degrees: i) L = 2, so that nodes
that are more than 2 hops away are not correlated (cf. Property
2); and ii) L = 10, which is greater than the graph diameter,
inducing correlations between every pair of nodes. In Fig. 1c
we illustrate the performance of local and random windows in
these two settings as a function of M. We ﬁrst observe that as
M increases, the error ﬁrst decreases until it reaches an optimal
point and then starts to increase. Intuitively, this indicates that at
ﬁrst the reduction in variance outweighs the increase in bias but,
after some point, the marginal variance reduction when adding
one extra window does not compensate the detrimental effect on
the bias. Moreover, it can be seen that local windows outperform
the random ones, especially for localized graph processes (L = 2).
These ﬁndings are consistent for other types of graphs, although
for graphs with a weaker clustered structure the beneﬁts of local
windows are less conspicuous.

The last experiment evaluates the performance of ﬁlter-bank
estimators, where two types of bandpass ﬁlters are considered.
The ﬁrst type designs the k-th ﬁlter as an ideal bandpass ﬁlter
with unit response for the k-th frequency and the B frequencies
closest to it, and zero otherwise [cf. (32)]. More precisely, being
λk the eigenvalue associated to the k-th frequency, we consider
the closest frequencies as those with eigenvalues λk(cid:48) minimizing
|λk − λk(cid:48)|. The second ﬁlter bank type designs the ﬁlters using
the FIR approach in (36). To run the experiments we consider the
adjacency matrix of an ER graph with N = 100 and p = 0.05, and
generate signals by ﬁltering white noise. Fig. 1d shows the MSE
performance of both approaches as a function of the degree of
the ﬁlter that generates the process as well as the nonparametric
periodogram estimation. We consider two ideal bandpass ﬁlters
with B = 3 and B = 7 and two FIR bandpass ﬁlters with L =
5 and L = 10. Fig. 1d indicates that ﬁlter banks contribute to
reduce the MSE compared to the periodogram. Moreover, the ideal
bandpass ﬁlter outperforms the FIR design and, within each type,
ﬁlters with larger bandwidth (B = 7 and L = 5) tend to perform
better. The reason being that the periodogram-based estimation
for a single observation is very noisy, thus, it beneﬁts from the
averaging effect of larger bandwidths.
TC2. Parametric methods: We ﬁrst illustrate the parametric
estimation of a MA process. Consider the Laplacian of an ER
graph with N = 100 and p = 0.2 and processes generated
by an FIR ﬁlter of length L whose coefﬁcients β are selected
randomly. The performance of a periodogram is contrasted with
that of two parametric approaches: i) an algorithm that estimates
the L values in β by minimizing (40) via phase-retrieval [22];
and ii) a least squares algorithm that estimates the 2L − 1 values
in γ by minimizing (41). The results are shown in Fig. 1e
(solid lines). It can be observed that both parametric methods
outperform the periodogram since they leverage the FIR structure
of the generating ﬁlter. Moreover, this difference is largest for
smaller values of the degree, since in these cases a few parameters
are sufﬁcient
to completely characterize the ﬁlter of interest.
Furthermore, we also test our schemes for a model mismatch
(MM) scenario where the MA schemes assume that the order
of the process is L + 2 instead of L (dashed lines in Fig. 1e).

11

(a)

(d)

(b)

(e)

(c)

(f)

r=1

tested schemes,

xr =(cid:80)L−1

The results show that, although the model mismatch degrades
the performance, the parametric estimates are still superior to the
periodogram.

Fig. 1: Normalized MSE (NMSE) for different PSD estimation schemes. (a) Top: NMSE for the periodogram using Gaussian inputs. Bottom: NMSE
for non-Gaussian inputs. (b) Theoretical and empirical NMSE for different window strategies. (c) NMSE as a function of the number of windows for
local and random windows. (d) NMSE for ﬁlter banks as a function of the degree of the generating ﬁlter. (e) NMSE for MA parametric estimation as
a function of the degree of the generating ﬁlter. (f) NMSE for different ARMA parametric estimators based on R = 1 and R = 2 signal realizations.
probability 0.05, and wi = 0 otherwise. We are given a set {xr}R
of opinion proﬁles generated from different sources {wr}R
r=1
diffused through a ﬁlter of unknown nonnegative coefﬁcients β.
Our goal is to identify the sources of the different opinions, i.e.,
the nonzero entries of wr for every r. Our approach proceeds
in two phases. First, we use {xr}R
r=1 to identify the parameters
β of the generating ﬁlter. We do this by solving (43) via least
squares. Second, given the set of coefﬁcients β, we have that
l=0 βlSlwr. Thus, we estimate the sources wr by solving
a (cid:96)1-regularized least squares problem to promote sparsity in
the input. In Fig. 2a (blue) we show the proportion of sources
misidentiﬁed as a function of the number of observations R. As
R increases, the estimates of the parameters β become more
reliable, thus leading to a higher success rate. Finally, we consider
cases where the observations are noisy. Formally, we deﬁne noisy
observations ˆxr by perturbing the original ones ˆxr = xr +σz◦xr
where σ denotes the magnitude of the perturbation and z is a
vector with elements drawn from a standard normal distribution.
As expected, higher levels of noise have detrimental effects on
the recovery of sources. Nevertheless, for moderate noise levels
(σ = 0.1) a performance comparable to the noiseless case can be
achieved when observing 20 signals or more.
For our last experiment, we consider a set of 100 grayscale
images {xi}100
i=1 corresponding to 10 face pictures of 10 different
people2. Since spectral representations of images can be used for
successful face recognition [30], our goal is to use PSD estimation
to generate consistent spectral signatures for different faces of the
same person. Formally, every image is represented by a vector
xi ∈ R10304 where the entries correspond to grayscale values of
pixels, normalized to have zero mean. We consider the images
xi to be realizations of a graph process which, by deﬁnition, is
stationary on the shift given by its covariance, here approximated
by the sample covariance S = ˆCx = VΛcVH. Moreover, denote
by Ij the set of images corresponding to faces of person j and
consider the sample covariance ˆC(j)
of the

The second experiment considers ARMA processes with L
poles and L zeros. The coefﬁcients are drawn randomly from
a uniform distribution with support [0, 1] and the shift is selected
as in the previous experiment. We compare the periodogram
estimation with two schemes: i) a least squares (LS) algorithm
that estimates 2L coefﬁcients, i.e., the counterpart of (41) for
the problem in (53); and ii) a LS algorithm that estimates L
nonnegative coefﬁcients, i.e., the counterpart of (43) for (53).
Note that the latter is computationally tractable because both
the eigenvalues of the shift and the coefﬁcients of the ﬁlters are
nonnegative. The algorithms are tested in two scenarios, with one
and two signal realizations available, respectively. Fig. 1f shows
that the parametric methods attain smaller MSEs compared to the
periodogram. Moreover, note that while increasing the number
of observations reduces the MSE for all
the
reduction is more pronounced for nonparametric schemes. This
is a manifestation of the fact that parametric approaches tend to
be more robust to noisy or imperfect observations.
TC3. Real-world graphs and signals: We now demonstrate
how the tools developed in this paper can be useful in practice
through two real-world experiments. The ﬁrst one deals with
source identiﬁcation in opinion formation dynamics. We consider
the social network of Zachary’s karate club [29] represented by
a graph G consisting of 34 nodes or members of the club and
78 undirected edges symbolizing friendships among members.
Denoting by L the Laplacian of G, we deﬁne the graph shift
operator S = I−αL with α = 1/λmax(L), modeling the diffusion
of opinions between the members of the club. A signal x can
be regarded as a unidimensional opinion of each club member
regarding a speciﬁc topic, and each application of S can be seen
as an opinion update. We assume that an opinion proﬁle x is
generated by the diffusion through the network of an initially
sparse (rumor) signal w. More precisely, we model w as a white
process such that wi = 1 with probability 0.05, wi = −1 with

2http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html

x = V(j)

x Λ(j)

c V(j)

x

135791113151719Number of observations012Normalized MSEBaseline ERSmaller ERLonger filterSmall-world135791113151719Number of observations012Normalized MSEBaseline ERSmaller ERLonger filterSmall-worldSingle windowTen local wind.Ten random wind.00.511.522.5Normalized MSE Bias squared Variance Theoretical error Empirical error135791113151719Number of windows00.511.52Normalized MSELocal window (L=10)Local window (L=2)Random window (L=10)Random window (L=2)01234Degree of the generating filter00.511.522.5Normalized MSENon-parametricIdeal B=3Ideal B=7Fir L=5Fir L=100123456Degree of the generating filter00.511.522.5Normalized MSENon-parametricPhase retPhase ret MMLS with 2LLS with 2L MM1234567Number of ARMA parameters00.511.522.5Normalized MSENon-parametric (R=2)LS with L (R=2)LS with 2L (R=2)Non-parametric (R=1)LS with L (R=1)LS with 2L (R=1)12

(a)

(b)

(c)

x

We may split the above summation into the terms where r (cid:54)= r(cid:48)
and those where r = r(cid:48). For the former case, since xr is assumed
to be independent from xr(cid:48), we have that

Fig. 2: (a) Error in the identiﬁcation of sources in opinion formation dynamics as a function of the number of observed opinions and parametrized
by the noise level σ. (b) Diagonalization of the sample covariance of the images of an individual with the basis of the ensemble sample covariance.
(c) Normalized MSE for the windowed average periodogram as a function of the number of windows for individual face processes.
subset {xi}i∈Ij . Matrix ˆC(j)
is perfectly diagonalized by V(j)
x
but only approximately diagonalized by the principal component
basis V. To conﬁrm this in Fig. 2b we show a block of the matrix
ˆΛ1 = VH ˆC(1)
x V for the ﬁrst individual. The diagonally dominant
structure indicates that the process generating face images of
subject j is approximately stationary in S, motivating looking at
the (squared) frequency coefﬁcients ˆpj = diag( ˆΛj). Given a new
unobserved image xi of individual j, our goal is to implement the
windowed average periodogram with random windows of equal
size to improve the estimate of ˆpj from the single new realization
xi. In Fig. 2c we plot the estimation error as a function of the
number of windows for three different individuals j ∈ {1, 2, 3}
and observe that using multiple windows enhances the PSD
estimation. Interestingly, even though the process associated with
the images of a given individual is only approximately stationary,
the MSE gains with random windows are similar to those in
bonaﬁde stationary processes (cf. Fig. 1c).

(56)
By contrast, for the case where r = r(cid:48) we undertake an ele-
mentwise analysis of the elements in the expected value in (55).
Notice that if we denote by vi the ith column of V we have that
r vi. Replacing this expression in
[diag(VH xrxH
(55), setting r = r(cid:48), and using the formula for the quartic form
of a Gaussian, see e.g. [31, pp. 43], we obtain that

(cid:2)E(cid:2)diag(VH xrxH
= E(cid:2)vH

r V)(cid:3) E(cid:2)diag(VH xr(cid:48)xH

E(cid:2)diag(VH xrxH

r(cid:48) V)H(cid:3) = ppH .

r vivH
j xrxH
i Cxv∗
j vT

r V)diag(VH xr(cid:48)xH

i Cxvj + vH

r V)]i = vH

= pipj + vH

i xrxH

i xrxH

r vj

(57)

When S is symmetric, hence V is real, the above expression can
be further simpliﬁed to obtain

i

r(cid:48) V)H(cid:3)(cid:3)
(cid:3) = vH
E(cid:2)xrxH
(cid:3) vj
r V)H(cid:3) = 2 diag2(p) + ppH.

ij
r vivH
i Cxvj.

j CxvivH

j xrxH
r

E(cid:2)diag(VH xrxH

r V)diag(VH xrxH

Upon substituting (56) and (58) into (55), the result follows.

(58)

VII. CONCLUSION

Three equivalent ways of generalizing the notion of station-
arity to graph processes were proposed. Given that stationary
processes were shown to be diagonalized by the Graph Fourier
basis, the concept of power spectral density for graph processes
was introduced and several estimation methods were studied in
detail. We ﬁrst generalized nonparametric methods including peri-
odograms, window-based average periodograms, and ﬁlter banks.
Their performance was analyzed, comparisons with the traditional
time domain schemes were established and open issues, such as
how to group nodes and frequencies in a graph, were identiﬁed.
We then focused on parametric estimation, where we examined
MA, AR, and ARMA processes. We not only showed that those
processes are useful
linear diffusion dynamics, but
also identiﬁed particular scenarios where the optimal parameter
estimation problem is tractable.

to model

APPENDICES

A. Proof of Proposition 3

To prove that

To compute the covariance,

the estimate is unbiased, we use the equiv-
to con-
r ])V] =

in Proposition 2 combined with (17)
E[xrxH

alence result
diag[VH R−1RCxV] = p. Hence, bpg = E[ˆppg] − p = 0.

clude that E[ˆppg] = diag[VH R−1((cid:80)R
(cid:3) − ppH. We may expand the leftmost term as
E(cid:2)ˆppg ˆpH
(cid:3) = E(cid:104)
diag(VH ˆCxV)diag(VH ˆCxV)H(cid:105)
E(cid:2)ˆppg ˆpH
R(cid:88)
R(cid:88)
E(cid:2)diag(VH xrxH

r(cid:48) V)H(cid:3). (55)

start by writing Σpg =

r V)diag(VH xr(cid:48)xH

(54)

r=1

=

pg

pg

1
R2

r=1

r(cid:48)=1

B. Proof of Proposition 5

From the deﬁnition of ˆpW in (25), it follows that

M(cid:88)

m=1

ˆpW =

1
M

diag(VHdiag(wm)V˜x˜xH VHdiag(w∗

m)V). (59)

Thus, using the deﬁnition of ˜Wm, we may write

M(cid:88)

diag( ˜WmE(cid:2)˜x˜xH(cid:3) ˜WH

E [ˆpW ] =

Leveraging the fact that E(cid:2)˜x˜xH(cid:3) = diag(p) is diagonal and

(60)

m).

m=1

1
M

recalling that ˜Wmm = ˜Wm ◦ ˜W∗

m, the result in (27) follows.

In order to show (28), notice that only the diagonal elements

of ΣW are needed. Each of them can be found as

[ΣW ]k,k = E [[ˆpW ]k[ˆpW ]k] − E [[ˆpW ]k]2 .

Rewriting ˆpW as a sum across the M windows, it holds that
[| ˜Wm(cid:48) ˜x|2]k

E [[ˆpW ]k[ˆpW ]k] = E

[| ˜Wm ˜x|2]k

M(cid:88)

m(cid:48)=1

1
M

(61)

(cid:35)

(cid:105)

(cid:34)
M(cid:88)

1
M

M(cid:88)
E(cid:104)

m=1

=

1
M 2

m=1,m(cid:48)=1

[| ˜Wm ˜x|2]k[| ˜Wm(cid:48) ˜x|2]k

.

(62)

1015202530Number of observations0.050.10.150.20.250.30.350.40.45Source identification error< = 0< = 0.10< = 0.15< = 0.201020304050Frequency index5101520253035404550Frequency index-4-202468#10505101520Number of windows0.511.522.533.5Normalized MSEIndividual 1Individual 2Individual 313

John

[4] M. H. Hayes, Statistical Digital Signal Processing and Modeling.

Wiley & Sons, 2009.

[5] P. Stoica and R. L. Moses, Spectral Analysis of Signals. Pearson/Prentice

Hall Upper Saddle River, NJ, 2005.

[6] B. Girault, “Stationary graph signals using an isometric graph translation,”

in European Signal Process. Conf. (EUSIPCO), 2015, pp. 1516–1520.

[7] N. Perraudin and P. Vandergheynst, “Stationary signal processing on graphs,”

arXiv preprint arXiv:1601.02522, 2016.

[8] A. Sandryhaila and J. Moura, “Discrete signal processing on graphs: Fre-
quency analysis,” IEEE Trans. Signal Process., vol. 62, no. 12, pp. 3042–
3054, June 2014.

[9] S. Segarra, A. G. Marques, and A. Ribeiro, “Distributed implementation
of linear network operators using graph ﬁlters,” in 53rd Allerton Conf. on
Commun. Control and Computing, Univ. of Illinois at U-C, Monticello, IL,
Sept. 30- Oct. 2 2015.

[10] A. Loukas, A. Simonetto, and G. Leus, “Distributed autoregressive moving
average graph ﬁlters,” IEEE Signal Process. Lett., vol. 22, no. 11, pp. 1931–
1935, 2015.

[11] A. Sandryhaila, S. Kar, and J. Moura, “Finite-time distributed consensus
through graph ﬁlters,” in IEEE Intl. Conf. Acoust., Speech and Signal
Process. (ICASSP), May 2014, pp. 1080–1084.

[12] S. Segarra, A. G. Marques, G. Leus, and A. Ribeiro, “Reconstruction
of graph signals through percolation from seeding nodes,” arXiv preprint
arXiv:1507.08364, 2015.

[13] S. Segarra, G. Mateos, A. G. Marques, and A. Ribeiro, “Blind identiﬁcation
of graph ﬁlters with sparse inputs,” in IEEE Intl. Wrksp. on Computational
Advances in Multi-Sensor Adaptive Process. (CAMSAP). Cancun, Mexico:
IEEE, Dec. 13-16 2015.

[14] C. Godsil and G. Royle, Algebraic Graph Theory. Springer-Verlag, Graduate

Texts in Mathematics, 2001, vol. 207.

[15] K. Conrad, The Minimal Polynomial and some Applications. Tech. report.

Dept. of Mathematics. Univ. of Connecticut.

[16] X. Zhu and M. Rabbat, “Approximating signals supported on graphs,” in
IEEE Intl. Conf. Acoust., Speech and Signal Process. (ICASSP), Mar. 2012,
pp. 3921–3924.

[17] B. Girault, P. Gonc¸alves, and E. Fleury, “Translation on graphs: an isometric
shift operator,” IEEE Signal Processing Letters, vol. 22, no. 12, pp. 2416–
2420, 2015.

[18] F. Pasqualetti, S. Zampieri, and F. Bullo, “Controllability metrics, limitations
and algorithms for complex networks,” IEEE Trans. Control Netw. Syst.,
vol. 1, no. 1, pp. 40–52, March 2014.

[19] K. Friston, L. Harrison, and W. Penny, “Dynamic causal modelling,” Neu-

roImage, vol. 19, no. 4, pp. 1273 – 1302, 2003.

[20] T. Nakano, M. Moore, F. Wei, A. Vasilakos, and J. Shuai, “Molecular
communication and networking: Opportunities and challenges,” IEEE Trans.
Nanobiosci., vol. 11, no. 2, pp. 135–148, June 2012.

[21] J. R. Fienup, “Phase retrieval algorithms: a comparison,” Applied optics,

vol. 21, no. 15, pp. 2758–2769, 1982.

[22] E. J. Candes, X. Li, and M. Soltanolkotabi, “Phase retrieval via Wirtinger
ﬂow: Theory and algorithms,” IEEE Trans. Inf. Theory, vol. 61, no. 4, pp.
1985–2007, 2015.

[23] K. P. Murphy, Machine Learning: A Probabilistic Perspective. MIT press,

[24] B. Bollob´as, Random Graphs. Springer, 1998.
[25] E. D. Kolaczyk, Statistical Analysis of Network Data: Methods and Models.

2012.

Springer, 2009.

[26] P. W. Holland, K. B. Laskey, and S. Leinhardt, “Stochastic blockmodels:

First steps,” Social networks, vol. 5, no. 2, pp. 109–137, 1983.

[27] A. Jain and R. C. Dubes, Algorithms for clustering data, ser. Prentice Hall

Advanced Reference Series. Prentice Hall Inc., 1988.

[28] G. Carlsson, F. M´emoli, A. Ribeiro, and S. Segarra, “Axiomatic construc-
tion of hierarchical clustering in asymmetric networks,” arXiv preprint
arXiv:1301.7724, 2014.

[29] W. W. Zachary, “An information ﬂow model for conﬂict and ﬁssion in small

groups,” J. Anthropol. Res., vol. 33, no. 4, pp. pp. 452–473, 1977.

[30] L. Sirovich and M. Kirby, “Low-dimensional procedure for the characteri-
zation of human faces,” J Optical Soc Am, vol. 4, no. 3, pp. 519–524, 1987.

[31] K. B. Petersen and M. S. Pedersen, “The matrix cookbook,” Nov. 2012.

E(cid:104)

E(cid:104)

Denoting the kth row of ˜Wm by ˜wT
k|m ˜x˜xH ˜w∗
above can be written as ˜wT
write

[| ˆWm ˜x|2]k[| ˆWm(cid:48) ˜x|2]k

=E(cid:104)
(cid:105)

k|m, each of the [| ˆWm ˜x|2]k
(cid:105)
k|m. Consequently, we may

k|m(cid:48) ˜x˜xH ˜w∗

k|m(cid:48)

k|m(cid:48) ˜x˜xH(cid:105)

k|m ˜x˜xH ˜w∗
k|m ˜wT
˜wT
˜w∗
k|m(cid:48),

(63)

= ˜wT

k|m

˜x˜xH ˜w∗

k|m ˜wT

E(cid:104)

where the middle factor is a quartic form of a Gaussian with
covariance diag(p) (cf. Property 3). Solving this fourth moment,
see e.g. [31, pp. 43], it follows that

[| ˆWm ˜x|2]k[| ˆWm(cid:48) ˜x|2]k

=

(64)

(cid:105)

k|m|2p| ˜wT
| ˜wT
k|mdiag(p)VH V∗ ˜wk|m(cid:48) ˜wH
+ ˜wT

k|m(cid:48)|2p + | ˜wT

k|mdiag(p) ˜w∗

k|m(cid:48)|2

k|mVT Vdiag(p) ˜w∗

k|m(cid:48)

When S is symmetric, hence V is real, the third summand in (64)
is equal to the second one. Thus, substituting ﬁrst (64) into (62)
and then, (62) into (61) yields

(cid:80)M
m=1,m(cid:48)=1| ˜wT
k|mdiag(p) ˜w∗
m(cid:48), it follows that | ˜wT
(cid:80)M
m=1,m(cid:48)=1|[ ˜Wmm(cid:48)p]k|2.

k|m(cid:48)|2.
k|mdiag(p) ˜w∗

(65)
k|m(cid:48)|

(66)

k=1[ΣW ]k,k, we obtain

[ΣW ]k,k = 2
M 2

Since ˜Wmm(cid:48) = ˜Wm◦ ˜W∗
= [ ˜Wmm(cid:48)p]k, thus

Finally, using (66) to write tr[ΣW ] =(cid:80)N

[ΣW ]k,k = 2
M 2

(28) and the proof concludes.

C. Proof of Proposition 7

k)].

(67)

outer product we get that

Rewriting the norm in (32) as the trace of the corresponding

E [ˆp˜qk ] = tr[diag(˜qk)E(cid:2)˜x˜xH(cid:3) diag(˜q∗

Expression (33) follows from replacing E(cid:2)˜x˜xH(cid:3) by diag(p) and

noting that the trace of the product of diagonal matrices equals
the sum of the entrywise products of the diagonals.
To prove (34), we ﬁrst ﬁnd E[ˆp˜qk ˆp˜qk ]. Since ˆp˜qk =
tr[diag(˜qk)˜x˜xHdiag(˜qk)H ] = tr[˜xHdiag(|˜qk|2)˜x] then, since the
argument of the trace is a scalar, we can write

E [ˆp˜qk ˆp˜qk ] = E(cid:2)tr[˜xHdiag(|˜qk|2)˜x˜xHdiag(|˜qk|2)˜x](cid:3)
= tr[E(cid:2)˜x˜xHdiag(|˜qk|2)˜x˜xH(cid:3) diag(|˜qk|2)],
E [ˆp˜qk ˆp˜qk ] =(cid:13)(cid:13)diag(cid:0)|˜qk|2(cid:1) p(cid:13)(cid:13)2
(cid:13)(cid:13)diag(cid:0)|˜qk|2(cid:1) p(cid:13)(cid:13)2

(69)
+ tr[diag(p)VH V∗diag(|˜qk|2)VT Vdiag(p)diag(|˜qk|2)].
When S is symmetric, hence V is real, the trace in (69) is equal
2 and, since var [ˆp˜qk ] = E [ˆp˜qk ˆp˜qk ] − E [ˆp˜qk ]2,

where we have a quartic form of a Gaussian. From the expression
of this quartic form, see e.g. [31, pp. 43], we obtain

2 + ((|˜qk|2)T p)2

using (69) and (33), the expression (34) follows.

(68)

REFERENCES

[1] A. G. Marques, S. Segarra, G. Leus, and A. Ribeiro, “Stationary graph
processes: Nonparametric power spectral density estimation,” in IEEE Sensor
Array and Multichannel Signal Process. Wrksp., Rio de Janeiro, Brazil, July
10-13 2016.

[2] D. Shuman, S. Narang, P. Frossard, A. Ortega, and P. Vandergheynst, “The
emerging ﬁeld of signal processing on graphs: Extending high-dimensional
data analysis to networks and other irregular domains,” IEEE Signal Process.
Mag., vol. 30, no. 3, pp. 83–98, Mar. 2013.

[3] A. Sandryhaila and J. Moura, “Discrete signal processing on graphs,” IEEE

Trans. Signal Process., vol. 61, no. 7, pp. 1644–1656, Apr. 2013.

