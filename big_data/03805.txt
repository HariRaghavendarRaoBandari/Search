6
1
0
2

 
r
a

 

M
1
1

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
5
0
8
3
0

.

3
0
6
1
:
v
i
X
r
a

Provable Non-convex Phase Retrieval with Outliers:

Median Truncated Wirtinger Flow

Huishuai Zhang
Dept. of EECS

Syracuse University
Syracuse, NY 13244
hzhan23@syr.edu

Yuejie Chi
Dept. of ECE

Ohio State University
Columbus, OH 43210
chi.97@osu.edu

Yingbin Liang
Dept. of EECS

Syracuse University
Syracuse, NY 13244

yliang06@syr.edu

Abstract

Solving systems of quadratic equations is a central problem in machine learn-
ing and signal processing. One important example is phase retrieval, which aims
to recover a signal from only magnitudes of its linear measurements. This pa-
per focuses on the situation when the measurements are corrupted by arbitrary
outliers, for which the recently developed non-convex gradient descent Wirtinger
ﬂow (WF) and truncated Wirtinger ﬂow (TWF) algorithms likely fail. We develop
a novel median-TWF algorithm that exploits robustness of sample median to re-
sist arbitrary outliers in the initialization and the gradient update in each iteration.
We show that such a non-convex algorithm provably recovers the signal from a
near-optimal number of measurements composed of i.i.d. Gaussian entries, up
to a logarithmic factor, even when a constant portion of the measurements are
corrupted by arbitrary outliers. We further show that median-TWF is also robust
when measurements are corrupted by both arbitrary outliers and bounded noise.
Our analysis of performance guarantee is accomplished by development of non-
trivial concentration measures of median-related quantities, which may be of in-
dependent interest. We further provide numerical experiments to demonstrate the
effectiveness of the approach.

1 Introduction

Phase retrieval is a classical problem in machine learning, signal processing and optical imaging,
where one aims to recover a signal x ∈ Rn from only observing the magnitudes of its linear mea-
surements:

yi = |hai, xi|2,

i = 1, . . . , m.

It has many important applications such as X-ray crystallography [1], but is known to be notoriously
difﬁcult due to the quadratic form of the measurements. Classical methods based on alternating
minimization between the signal of interest and the phase information [2], though computationally
simple, are often trapped at local minima and lack rigorous performance guarantees.

Using the lifting trick, the phase retrieval problem can be reformulated as estimating a rank-one
positive semideﬁnite matrix X = xxT from linear measurements [3], to which convex relaxations
into semideﬁnite programming are considered [4–9]. In particular, when the measurement vectors
ai’s are composed of i.i.d. Gaussian entries, Phaselift [5] perfectly recovers all x ∈ Rn with high
probability as long as the number m of measurements is on the order of O(n).
However, the computational cost of Phaselift becomes prohibitive when the signal dimension is
large. Appealingly, a so-called Wirtinger ﬂow (WF) algorithm based on gradient descent was re-
cently proposed in [10, 11] and shown to work remarkably well: it converges to the global optima

1

when properly initialized using the spectral method. The truncated Wirtinger ﬂow (TWF) algo-
rithm [12] further improves WF by eliminating samples whose contributions to both the initializa-
tion and the search direction are excessively deviated from the sample mean, so that the behavior of
each gradient update is well controlled. TWF is shown to converge globally at a geometric rate as
long as m is on the order of O(n) for i.i.d. Gaussian measurement vectors using a constant step size.
Both WF and TWF algorithms have been shown to be robust to bounded noise in the measurements.

However, the performance of WF and TWF can be very sensitive to outliers that take arbitrary
values and can introduce anomalous search directions. Even for TWF, since the sample mean can be
arbitrarily perturbed, the truncation rule based on such sample mean cannot control the gradient well.
On the other hand, the ability to handle outliers is of great importance for phase retrieval algorithms,
because outliers arise frequently from the phase imaging applications [13] due to various reasons
such as detector failures, recording errors, and missing data. While a form of Phaselift [14] is shown
to be robust to sparse outliers even when they constitute a constant portion of all measurements, it is
computationally too expensive.

1.1 Main Contributions

The main contribution of this paper lies in the development of a non-convex phase retrieval algo-
rithm with both statistical and computational efﬁciency, and provable robustness to even a constant
proportion of outliers. To the best of the authors knowledge, our work is the ﬁrst application of the
median to robustify high-dimensional statistical estimation in the presence of arbitrary outliers with
rigorous non-asymptotic performance guarantees.

Our strategy is to carefully robustify the TWF algorithm by replacing the sample mean used in
the truncation rule by its robust counterpart, the sample median. We refer to the new algorithm as
median truncated Wirtinger ﬂow (median-TWF). Appealingly, median-TWF does not require any
knowledge of the outliers. The robustness property of median lies in the fact that the median cannot
be arbitrarily perturbed unless the outliers dominate the inliers [15]. This is in sharp contrast to the
mean, which can be made arbitrarily large even by a single outlier. Thus, using the sample median
in the truncation rule can effectively remove the impact of outliers and indeed, the performance of
median-TWF can be provably guaranteed.

Statistically, the sample complexity of median-TWF is near-optimal up to a logarithmic factor when
the measurement vectors are composed of i.i.d. Gaussian entries. We demonstrate that as soon as
the number m of measurements is on the order O(n log n), median-TWF converges to the global
optima, i.e. recovers the ground truth up to a global sign difference, even when the number of outliers
scales linearly with m. Computationally, median-TWF converges at a geometric rate, requiring
a computational cost of O(mn log 1/ǫ) to reach ǫ-accuracy, which is linear in the problem size.
Reassuringly, under the same sample complexity, median-TWF still recovers the ground truth when
outliers are absent. It can therefore handle outliers in an oblivious fashion. Finally, median-TWF is
also stable when the measurements are further corrupted by dense bounded noise besides outliers.

Our proof proceeds by ﬁrst showing the initialization of median-TWF is close enough to the ground
truth, and that the neighborhood of the ground truth, where the initialization lands in, satisﬁes certain
Regularity Condition [10, 12] that guarantees convergence of the descent rule, as long as the size of
the corruption is small enough and the sample size is large enough. However, as a nonlinear operator,
the sample median used in median-TWF is much more difﬁcult to analyze than the sample mean
used in TWF, which is a linear operator and many existing concentration inequalities are readily
applicable. Considerable technical efforts lie in developing novel non-asymptotic concentrations of
the sample median, and various statistical properties of the sample median related quantities, which
may be of independent interest.

1.2 Related Work

The adoption of median in machine learning and computer science is not unfamiliar, for example,
K-median clustering [16] and resilient data aggregation for sensor networks [17]. Our work here
further extends the applications of median to robustifying high-dimensional estimation problems.

Another popular approach in robust estimation is to use the trimmed mean [15], which has found
success in robustifying sparse regression [18], subspace clustering [19], etc. However, using the

2

trimmed mean requires knowledge of an upper bound on the number of outliers, whereas median
does not require such information.

Developing non-convex algorithms with provable global convergence guarantees has attracted in-
tensive research interest recently. A partial list includes low-rank matrix recovery [20–23], robust
principal component analysis [24], robust tensor decomposition [25], dictionary learning [26, 27],
etc. We expect our analysis in this paper may be extended to robustly solving other systems of
quadratic or bilinear equations in a non-convex fashion, such as mixed linear regression [28], sparse
phase retrieval [29], and blind deconvolution [30].

1.3 Paper Organization and Notations

The rest of this paper is organized as follows. Section 2 provides the problem formulation, and
Section 3 describes the proposed median-TWF algorithm. Theoretical performance guarantees are
stated in Section 4, with proof outlines given in Section 5. Numerical experiments are presented in
Section 6. Finally, we conclude in Section 7.
i=1, the sample
We adopt the following notations in this paper. Given a vector of numbers {βi}m
i=1). The indicator function 1A = 1 if the event A holds, and
median is denoted as med({βi}m
1A = 0 otherwise. For two matrices, A ≺ B if B − A is a positive semideﬁnite matrix. We
deﬁne the Euclidean distance between two vectors up to a global sign difference as dist(z, x) :=
min{kz − xk,kz + xk}.
2 Problem Formulation

Suppose the following set of m measurements are given

i = 1,··· , m,

yi = |hai, xi|2 + ηi,

(1)
where x ∈ Rn is the unknown signal,1 ai ∈ Rn for i = 1, . . . , m are measurement vectors with each
ai having i.i.d. Gaussian entries distributed as N (0, 1), and ηi ∈ R for i = 1, . . . , m are outliers
with arbitrary values. We assume that outliers are sparse with sm nonzero values, i.e., kηk0 ≤ sm,
i=1 ∈ Rm. Here, s is a nonzero constant, representing the faction of measurements
where η = {ηi}m
that are corrupted by outliers.
We are also interested in the model when the measurements are corrupted by not only sparse arbitrary
outliers but also dense bounded noise. Under such a model, the measurements are given by

i = 1,··· , m,

yi = |hai, xi|2 + wi + ηi,

(2)
where the bounded noise w = {wi}m
i=1 satisﬁes kwk∞ ≤ c1kxk2 for some universal constant c1,
and as before, the outlier satisﬁes kηk0 ≤ sm.
The goal is to recover the signal x (up to a global sign difference) from the measurements y =
{yi}m
3 Median-TWF Algorithm

i=1.
i=1 and measurement vectors {ai}m

A natural idea is to recover the signal as a solution to the following optimization problem

min

z

−ℓ(z; yi)

(3)

m

Xi=1

where ℓ(z, yi) is a likelihood function, e.g., using Gaussian or Poisson likelihood. Since the mea-
surements are quadratic in x, the objective function is non-convex. A typical gradient descent
procedure to solve (3) proceeds as

z(t+1) = z(t) +

µt
m

m

Xi=1

∇ℓ(z(t); yi),

(4)

1We focus on real signals here, but our analysis can be extended to complex signals.

3

where z(t) denotes the tth iterate of the algorithm, and µt is the step size. By a careful initialization
using the spectral method, the WF algorithm [10] using the gradient descent update (4) is shown to
converge globally under the Gaussian likelihood (i.e., quadratic loss) function, as long as the number
of measurements is on the order of O(n log n) for i.i.d. Gaussian measurement vectors.
The TWF algorithm [12] improves WF in both initialization and the descent rule: only a subset of
samples T0 contributes to the spectral method, and only a subset of data-dependent and iteration-
varying samples Tt+1 contributes to the search directions:

z(t+1) = z(t) +

µt

m Xi∈Tt+1

∇ℓ(z(t); yi).

(5)

The idea is that through pruning, i.e., samples with gradient components ∇ℓ(z(t); yi) being much
larger than the sample mean are truncated so that each update is well controlled. This modiﬁcation
yields both statistical and computational beneﬁts – under the Poisson loss function, TWF converges
globally geometrically to the true signal with a constant step size with measurements at the order of
O(n) and with i.i.d. Gaussian measurement vectors.
However, if some measurements are corrupted by arbitrary-valued outliers as in (1), both WF and
TWF can fail. This is because the gradient of the loss function typically contains the term |yi −
i z|2|. With yi being corrupted by arbitrarily large ηi, the gradient can deviate the search direction
|aT
from the signal arbitrarily. In TWF, since the truncation rule is based on the sample mean of the
gradient, which can be affected signiﬁcantly even by a single outlier, we cannot expect it to converge
globally, particularly when the fraction of corrupted measurements is linear with the total number
m of measurements, which is the regime we are interested in.
To handle outliers, our central idea is to prune the samples in both the initialization and each iteration
via the sample median related quantities. Compared to the sample mean used in TWF, the sample
median is much less affected even in the presence of a certain linear fraction of outliers, and is thus
more robust to outliers with arbitrary values.

In the following, we describe our median-TWF in more details. We adopt the following Poisson
likelihood function,

ℓ(z; yi) = yi log |aT

i z|2 − |aT

i z|2,

(6)

which is motivated by the maximum likelihood estimation of the signal when the measurements are
corrupted by Poisson distributed noise. We note that our analysis is also applicable to the quadratic
loss function, but in order to compare more directly to TWF in [12], we adopt the Poisson likelihood
function in (6).

Our median-TWF algorithm (summarized in Algorithm 1) contains the following main steps:

1. Initialization: We initialize z(0) by the spectral method with a truncated set of samples, where
i=1. In comparison, WF does not truncate samples,
the threshold is determined by the median of {yi}m
and the truncation in TWF is based on the mean of {yi}m
i=1, which is not robust to outliers. As will
be shown, as long as the portion of outliers is not too large, our initialization (7) is guaranteed to be
within a small neighborhood of the ground truth signal.
2. Gradient loop: for each iteration 0 ≤ t ≤ T − 1, comparing (4) and (8), median-TWF uses an
iteration-varying truncated gradient given as

∇ℓtr(z(t)) = 2

Xi=1
2 (see Algorithm 1), that samples are truncated by the
It is clear from the deﬁnition of the set E i
sample median of gradient components evaluated at the current iteration, as opposed to the sample
mean in TWF.

i z(t)|2
yi − |aT
aT
i z(t)

ai1

(9)

m

.

1∩E i
E i

2

4

Algorithm 1 Median Truncated Wirtinger Flow (Median-TWF)
Input: y = {yi}m
Parameters: thresholds αy, αh, αl, and αu, stepsize µt;
Initialization: Let z(0) = λ0 ˜z, where λ0 =pmed(y)/0.455 and ˜z is the leading eigenvector of

i=1;
i=1, {ai}m

m

(7)

Gradient loop: for t = 0 : T − 1 do

i z(t)|2
yi − |aT
aT
i z(t)

ai1

,

E i
1∩E i

2

(8)

Y :=

1
m

Xi=1

yiaiaT
i

1

{|yi|≤α2

0}.
yλ2

z(t+1) = z(t) +

m

2µt
m

Xi=1
1 :=nαlkz(t)k ≤ |aT
E i
2 :=(cid:26)|yi − |aT
E i
Kt := med(cid:16){|yi − |aT

i z(t)| ≤ αukz(t)ko ,
kz(t)k (cid:27) ,
i z(t)|2| ≤ αhKt|aT
i z(t)|
i=1(cid:17).
i z(t)|2|}m

where

Output zT .

We set the step size in the median-TWF to be a ﬁxed small constant, i.e., µt = 0.2. The rest of the
parameters {αy, αh, αl, αu} are set to satisfy

ζ1 := maxnEhξ21{|ξ|<√1.01αl or |ξ|>√0.99αu}i , Eh1{|ξ|<√1.01αl or |ξ|>√0.99αu}io,
ζ2 := E(cid:2)ξ21{|ξ|>0.248αh}(cid:3) ,
2(ζ1 + ζ2) +p8/πα−1
αy ≥ 3,

h < 1.99

(10)

where ξ ∼ N (0, 1). For example, we set αl = 0.3, αu = 5, αy = 3 and αh = 12, and consequently
ζ1 ≈ 0.24 and ζ2 ≈ 0.032.
4 Performance Guarantees of Median-TWF

In this section, we characterize the performance guarantees of median-TWF.

Theorem 1 (Exact recovery with sparse arbitrary outliers) Consider the phase retrieval prob-
lem with sparse outliers given in (1). There exist constants µ0, s0 > 0, 0 < ρ, ν < 1 and
c0, c1, c2 > 0 such that if m ≥ c0n log n, s < s0, µ ≤ µ0, then with probability at least
1 − c1 exp(−c2m), the median-TWF yields

dist(z(t), x) ≤ ν(1 − ρ)tkxk,

∀t ∈ N

(11)

simultaneously for all x ∈ Rn\{0}.
Theorem 1 indicates that median-TWF admits exact recovery for all signals in the presence of sparse
outliers with arbitrary magnitudes even when the number of outliers scales linearly with the number
of measurements, as long as the number of samples satisﬁes m & n log n. This is near-optimal up
to a logarithmic factor.

Moreover, median-TWF converges at a geometric rate using a constant step size, with per-iteration
cost O(mn) (note that the median can be computed in linear time [31]). To reach ǫ-accuracy,
i.e., dist(z(t), x) ≤ ǫ, only O(log 1/ǫ) iterations are needed, and the total computational cost is
O(mn log 1/ǫ), which is highly efﬁcient.

5

Not surprisingly, Theorem 1 implies that median-TWF also performs well for the noise-free model,
as a special case of the model with outliers. This justiﬁes utilization of median-TWF in an oblivious
situation without knowing whether the underlying measurements are corrupted by outliers.

Corollary 1 (Exact recovery for noise-free model) Suppose that the measurements are noise-free,
i.e., ηi = 0 for i = 1,··· , m in the model (1). There exist constants µ0 > 0, 0 < ρ, ν < 1
and c0, c1, c2 > 0 such that if m ≥ c0n log n and µ ≤ µ0, then with probability at least 1 −
c1 exp(−c2m), the median-TWF yields

dist(z(t), x) ≤ ν(1 − ρ)tkxk,

∀t ∈ N

(12)

simultaneously for all x ∈ Rn\{0}.
We next consider the model when the measurements are corrupted by both sparse arbitrary out-
liers and dense bounded noise. Our following theorem characterizes that median-TWF is robust to
coexistence of the two types of noises.

Theorem 2 (Stability to sparse arbitrary outliers and dense bounded noises) Consider
the
phase retrieval problem given in (2) in which measurements are corrupted by both sparse arbitrary
and dense bounded noises. There exist constants µ0, s0 > 0, 0 < ρ < 1 and c0, c1, c2 > 0 such that
if m ≥ c0n log n, s < s0, µ ≤ µ0, then with probability at least 1 − c1 exp(−c2m), median-TWF
yields

dist(z(t), x) . kwk∞
kxk

+ (1 − ρ)tkxk,

∀t ∈ N

(13)

simultaneously for all x ∈ Rn\{0}.
Theorem 2 immediately implies the stability of median-TWF for the model corrupted only by dense
bounded noise.

Corollary 2 Consider the phase retrieval problem in which measurements are corrupted only by
dense bounded noises, i.e., ηi = 0 for i = 1,··· , m in the model (2). There exist constants µ0 > 0,
0 < ρ < 1 and c0, c1, c2 > 0 such that if m ≥ c0n log n, µ ≤ µ0, then with probability at least
1 − c1 exp(−c2m), median-TWF yields

dist(z(t), x) . kwk∞
kxk

+ (1 − ρ)tkxk,

∀t ∈ N

(14)

simultaneously for all x ∈ Rn\{0}.
Thus, Theorem 2 and Corollary 2 imply that median-TWF for the model with both sparse arbitrary
outliers and dense bounded noises achieves the same convergence rate and the same level of esti-
mation error as the model with only bounded noise. In fact, together with Theorem 1 and Corollary
1, it can be seen that applying median-TWF does not require the knowledge of the noise corruption
models. When there indeed exist outliers, median-TWF achieves the same performance as if the
outliers do not exist.

5 Proof Outlines of Main Results

In this section, we ﬁrst develop a few statistical properties of median that will be useful for our
analysis of performance guarantees, and then provide a proof sketch of our main results. The details
of the proofs can be found in Supplemental Materials.

5.1 Properties of Median

We start by the deﬁnitions of the quantile of a population distribution and its sample version.

Deﬁnition 1 (Generalized quantile function) Let 0 < p < 1. For a cumulative distribution func-
tion (CDF) F , the generalized quantile function is deﬁned as

F −1(p) = inf{x ∈ R : F (x) ≥ p}.

(15)

6

i=1 .

For simplicity, denote θp(F ) = F −1(p) as the p-quantile of F . Moreover for a sample sequence
i=1, the sample p-quantile θp({Xi}) means θp( ˆF ), where ˆF is the empirical distribution of the
{Xi}m
samples {Xi}m
Remark 1 We take the median med({Xi}) = θ1/2({Xi}) and use both notations interchangeably.
Next, we show that as long as the sample size is large enough, the sample quantile concentrates
around the population quantile (motivated from [32]), as in Lemma 1.
Lemma 1 Suppose F (·) is cumulative distribution function (i.e., non-decreasing and right-
continuous) with continuous density function F ′(·). Assume the samples {Xi}m
i=1 are i.i.d. drawn
from F . Let 0 < p < 1. If l < F ′(θ) < L for all θ in {θ : |θ − θp| ≤ ǫ}, then

|θp({Xi}m

i=1) − θp(F )| < ǫ

(16)

holds with probability at least 1 − 2 exp(−2mǫ2l2).
Lemma 2 bounds the distance between the median of two sequences.

Lemma 2 Given a vector X = (X1, X2, ..., Xn), reorder them in a non-decreasing manner

Given another vector Y = (Y1, Y2, ..., Yn), then one has

X(1) ≤ X(2) ≤ ... ≤ X(n−1) ≤ X(n).

for all k = 1, ..., n.

|X(k) − Y(k)| ≤ kX − Y k∞,

(17)

Lemma 3 suggests that in the presence of outliers, one can lower and upper bound the sample median
by neighboring quantiles of the corresponding clean samples.

Lemma 3 Consider clean samples { ˜Xi}m
obtains contaminated samples {Xi}m
samples. Then

i=1. If a fraction s of them are corrupted by outliers, one
i=1 which contain sm corrupted samples and (1 − s)m clean

2−s({ ˜Xi}) ≤ θ 1
θ 1

2

({Xi}) ≤ θ 1

2 +s({ ˜Xi}).

Finally, Lemma 4 is related to bound the median and density at the median for the product of two
possibly correlated standard Gaussian random variables.
Lemma 4 Let u, v ∼ N (0, 1) which can be correlated with the correlation coefﬁcient |ρ| ≤ 1. Let
(ψρ) as the median of r, and the value of
r = |uv|, and ψρ(x) represent the density of r. Denote θ 1
ψρ(x) at the median as ψρ(θ1/2). Then for all ρ,

2

0.348 < θ1/2(ψρ) < 0.455,
0.47 < ψρ(θ1/2) < 0.76.

5.2 Robust Initialization with Outliers

We show that the initialization provided by the median-truncated spectral method in (7) is close
enough to the ground truth, i.e. dist(z(0), x) ≤ 1/11kxk, even if there are sm arbitrary outliers, as
long as s is a small enough constant.
1. We ﬁrst bound the concentration of med({yi}), also denoted by θ1/2({yi}). Lemma 3 suggests
that

i x)2}) < θ1/2({yi}) < θ 1

2−s({(aT
θ 1
i1kxk2, where ˜ai1 = aT

2 +s({(aT

i x)2})

Observe that (aT
Thus |˜ai1|2 is a χ2

i x)2 = ˜a2

i x/kxk is a standard Gaussian random variable.
1 random variable, whose CDF is denoted as K. By Lemma 1, for a small ǫ,

7

Y2 :=

1

mX aiaT
mX aiaT
mXi∈S

+

1

i (aT

i x)21

i

{(aT

x)2≤α2
y(ζs + ǫ)/0.455.

aiaT

i α2

y(ζs+ǫ)/0.455}

one has(cid:12)(cid:12)(cid:12)θ 1

2−s({|˜ai1|2}) − θ 1

1 − exp(−cmǫ2). Thus, let ζs := θ 1
exp(−cmǫ2)

2 −s(K)(cid:12)(cid:12)(cid:12) < ǫ and(cid:12)(cid:12)(cid:12)θ 1

2 −s(K) and ζs := θ 1

2 +s({|˜ai1|2}) − θ 1

2 +s(K)(cid:12)(cid:12)(cid:12) < ǫ with probability

2 +s(K), we have with probability 1 −

where ζs and ζs can be arbitrarily close if s is small enough.
2. We next estimate the direction of x, assuming kxk = 1. The norm kxk can be estimated as in
Algorithm 1. For simplicity of presentation, we assume kxk = 1. Using (18), the matrix Y in (7)
can be bounded by Y1 ≺ Y ≺ Y2 with high probability, where
x)2≤α2

y(ζs−ǫ)/0.455}

i x)21

i (aT

Y1 :=

{(aT

1

i

(ζs − ǫ)kxk2 < θ1/2({yi}) < (ζs + ǫ)kxk2,

(18)

It can then be shown by concentration of random matrices with non-isotropic sub-Gaussian rows [33]
that Y1 and Y2 concentrate on their means which can be made arbitrarily close with s sufﬁciently
small. Together with the fact that both means of Y1 and Y2 have leading eigenvector x, one can jus-
tify that the leading eigenvector of Y can be made close enough to x for sufﬁciently small constant
s.

5.3 Geometric Convergence

−(cid:28) 1

We now show that within a small neighborhood of the ground truth, the truncated gradient (9)
satisﬁes the Regularity Condition (RC) [10, 12], which guarantees the geometric convergence of
median-TWF once the initialization lands into this neighborhood.
Deﬁnition 2 The gradient ∇ℓtr(z) is said to satisfy the Regularity Condition RC(µ, λ, ǫ) if

m∇ℓtr(z)(cid:13)(cid:13)(cid:13)(cid:13)

m∇ℓtr(z), h(cid:29) ≥
for all z and h = z − x obeying khk ≤ ckzk.
We will ﬁrst show that ∇ℓtr(z) satisﬁes the RC for the noise-free case, and then extend it to model
(1) with sparse outliers, thus establishing the global convergence of median-TWF in both cases.
The central step to establish the RC is to show that the sample median used in the truncation rule
concentrates at the level kz − xkkzk as stated in the following proposition.
Proposition 1 If m > c0n log n, then with probability at least 1 − c1 exp(−c2m), there exist con-
stants β and β′ such that

2 (cid:13)(cid:13)(cid:13)(cid:13)

λ
2khk2

(19)

+

µ

1

2

βkzkkhk ≤ med((cid:8)(cid:12)(cid:12)|aT

i x|2 − |aT
holds for all z, h := z − x satisfying khk < 1/11kzk.
We note that a similar property for the sample mean has been shown in [12] as long as the number
of measurements m is O(n). In fact, the median is much more challenging to handle due to its
non-linearity, which also causes slightly more measurements compared to the sample mean.

i=1) ≤ β′kzkkhk,

i z|2(cid:12)(cid:12)(cid:9)m

We next brieﬂy sketch how we exploit the properties of the median developed in Section 5.1 to show
Proposition 1. First ﬁx x and z satisfying kx − zk < 1/11kzk. It can be shown that

|(aT
where ui and vi are correlated N (0, 1) Gaussian random variables and 1.89 < c < 2. Hence,
Lemma 4 and Lemma 1 imply that

i z)2| = c|uivi| · khkkzk,

i x)2 − (aT

(0.65 − ǫ)kzkkhk ≤ med(cid:0)(cid:8)|(aT

≤ (0.91 + ǫ)kzkkhk

i x)2 − (aT

i z)2|(cid:9)(cid:1)

(20)

8

for given x and z with high probability. Then applying the net covering argument we prove that
(20) holds for all z and x with kz − xk ≤ 1
11kzk. In particular, Lemma 2 is applied to bound the
distance between the medians of two points on and off the net.
For the model in (1) with outliers, we show that med({|yi − (aiz)2|}) continues to have property
as in Proposition 1 even with the presence of a small constant portion of outliers. This can be
accomplished by ﬁrst observing

i x)2 − (aT
2 +s({|(aT

2 −s({|(aT
θ 1
≤ θ 1

i z)2|}) ≤ θ 1
i x)2 − (aT
using Lemma 3, and then extending (20) to quantiles θ 1
s. Taking all these together yields bounds for both sides of (21) at the level of kzkkhk.
Remark 2 With ǫ small enough, β = 0.6, β′ = 1 is a valid choice for Proposition 1. We will set the
algorithm parameters based on these two values.

2 +s respectively for a small constant

i z)2|}).
2−s and θ 1

({|yi − (aT

i z)2|})

(21)

2

5.4 Stability with Additional Dense Bounded Noise

Now, consider the model in (2) with both sparse outliers and dense bounded noise. We omit the
analysis of the initialization step as it is similar to Section 5.2. We split our analysis of the gradient
loop into two regimes.
Regime 1: Assume c4kzk ≥ khk ≥ c3(kwk∞/kzk). Lemma 3 implies

(22)

(23)

where ˜yi := (aT
Lemma 2 and assumption of the regime implies

i x)2 + wi i.e., measurements that are corrupted by only bounded noise. Moreover,

θ 1

2−s({(cid:12)(cid:12)˜yi − (aT
i z)2(cid:12)(cid:12)}) ≤ med((cid:8)|yi − (aT
(cid:12)(cid:12)(cid:12)θ 1
2 +s({(cid:12)(cid:12)(aT
2 +s({(cid:12)(cid:12)˜yi − (aT
(cid:12)(cid:12)(cid:12)θ 1
2−s({(cid:12)(cid:12)(aT
2−s({(cid:12)(cid:12)˜yi − (aT
βkx − zkkzk ≤ med({(cid:12)(cid:12)yi − (aT

i z)2(cid:12)(cid:12)}) − θ 1
i z)2(cid:12)(cid:12)}) − θ 1

i z)2|(cid:9)) ≤ θ 1

2 +s({(cid:12)(cid:12)˜yi − (aT
i z)2(cid:12)(cid:12)}),
i z)2(cid:12)(cid:12)})(cid:12)(cid:12)(cid:12) ≤ kwk∞,
i z)2(cid:12)(cid:12)})(cid:12)(cid:12)(cid:12) ≤ kwk∞.
i z)2(cid:12)(cid:12)}) ≤ β′kx − zkkzk,

i x)2 − (aT
i x)2 − (aT

Therefore, combining the above inequalities with Proposition 1, we have

implying that RC holds in Regime 1, and the error decreases geometrically in each iteration.
Regime 2: Assume khk ≤ c3(kwk∞/kzk). Since each update µ
m∇ℓtr(z) is at most the order of
O(kwk∞/kzk), the estimation error cannot increase by more than (kwk∞/kzk) with a constant
factor. Thus, one has

µ

m∇ℓtr(z), x(cid:17) ≤ c5(kwk∞/kxk)
for some constant c5. As long as kwk∞/kxk2 is sufﬁciently small,
it is guaranteed that
c5(kwk∞/kxk) ≤ c4kxk. If the iteration jumps out of Regime 2, it falls into Regime 1 described
above.

dist(cid:16)z +

(24)

6 Numerical Experiments

In this section, we provide numerical experiments to demonstrate the effectiveness of median-TWF,
which corroborates with our theoretical ﬁndings.We ﬁrst show that, in the noise-free case, our
median-TWF performs similarly as TWF [12] for exact recovery. We set the parameters of median-
TWF as speciﬁed in Section 3, and those of TWF as suggested in [12]. Let the signal length n take
values from 1000 to 10000 by a step size of 1000, and the ratio of the sample complexity to the sig-
nal length, m/n, take values from 2 to 6 by a step size of 0.1. For each pair of (m, n), we generate a
signal x ∼ N (0, In×n), and the measurement vectors ai ∼ N (0, In×n) i.i.d. for i = 1, . . . , m. For
both algorithms, a ﬁxed number of iterations T = 500 are run, and the trial is declared successful if
z(T ), the output of the algorithm, satisﬁes dist(z(T ), x)/kxk ≤ 10−8. Figure 1 shows the number

9

of successful trials out of 20 trials for both algorithms, with respect to m/n and n. It can be seen
that for both algorithms, as soon as m is above 4n, exact recovery is achieved for both algorithms.
Around the phase transition boundary, the performance of median-TWF is slightly worse than that
of TWF, which is possibly due to the inefﬁciency of median compared to mean in the noise-free
case [15].

(cid:25)

(cid:24)

(cid:18)

(cid:81)
(cid:80)

(cid:23)

(cid:22)

(cid:21)

(cid:21)(cid:19)

(cid:20)(cid:24)

(cid:20)(cid:19)

(cid:24)

(cid:19)

(cid:21)(cid:19)(cid:19)(cid:19)

(cid:25)(cid:19)(cid:19)(cid:19)
(cid:81)

(cid:20)(cid:19)(cid:19)(cid:19)(cid:19)

(a) median-TWF

(b) TWF

Figure 1: Phase transition of median-TWF and TWF for noise-free data: the gray scale of each cell
(m/n, n) indicates the number of successful recovery out of 20 trials.

We next examine the performance of median-TWF in the presence of sparse outliers. We com-
pare the performance of median-TWF with not only TWF but also an alternative which we call the
trimean-TWF, based on replacing the sample mean in TWF by the trimmed mean. More speciﬁ-
cally, trimean-TWF requires knowing the fraction s of outliers so that samples corresponding to sm
largest gradient values are removed, and truncation is then based on the mean of remaining samples.
We ﬁx the signal length n = 1000 and the number of measurements m = 8000. We assume each
measurement yi is corrupted with probability s ∈ [0, 0.4] independently, where the corruption value
ηi ∼ U(0,kηk∞) is randomly generated from a uniform distribution. Figure 2 shows the success
rate of exact recovery over 100 trials as a function of s at different levels of outlier magnitudes
kηk∞/kxk2 = 0.1, 1, 10, 100, for the three algorithms median-TWF, trimean-TWF and TWF.
From Figure 2, it can be seen that median-TWF allows exact recovery as long as s is not too large
for all levels of outlier magnitudes, without any knowledge of the outliers, which validates our
theoretical analysis. Unsurprisingly, TWF fails quickly even with very small fraction of outliers. No
successful instance is observed for TWF when s ≥ 0.02 irrespective of the value of kηk∞. Trimean-
TWF does not exhibit as sharp phase transition as median-TWF, and in general underperforms our
median-TWF, except when both kηk∞ and s gets very large, see Figure 2(d). This is because in
this range with large outliers, knowing the fraction s of outliers provides substantial advantage for
trimean-TWF to eliminate them, while the sample median can be deviated signiﬁcantly from the
true median for large s. Moreover, it is worth mentioning that exact recovery is more challenging
for median-TWF when the magnitudes of most outliers are comparable to the measurements, as in
Figure 2(c). In such a case, the outliers are more difﬁcult to exclude as opposed to the case with very
large outlier magnitudes as in Figure 2(d); and meanwhile, the outlier magnitudes in Figure 2(c) are
large enough to affect the accuracy heavily in contrast to the cases in Figure 2(a) and 2(b) where
outliers are less prominent.

1

0.8

0.6

0.4

0.2

e

t

a
r
 
s
s
e
c
c
u
S

0

0

median-TWF
trimean-TWF
TWF

0.1

0.2

0.3

0.4

Outliers fraction s

1

0.8

0.6

0.4

0.2

e

t

a
r
 
s
s
e
c
c
u
S

0

0

median-TWF
trimean-TWF
TWF

0.1

0.2

0.3

0.4

Outliers fraction s

1

0.8

0.6

0.4

0.2

e

t

a
r
 
s
s
e
c
c
u
S

0

0

median TWF
trimean TWF
TWF

0.1

0.2

0.3

0.4

Outliers fraction s

1

0.8

0.6

0.4

0.2

e

t

a
r
 
s
s
e
c
c
u
S

0

0

median-TWF
trimean-TWF
TWF

0.1

0.2

0.3

0.4

Outliers fraction s

(a) kηk∞ = 0.1kxk2

(b) kηk∞ = kxk2

(c) kηk∞ = 10kxk2

(d) kηk∞ = 100kxk2

Figure 2: Success rate of exact recovery with outliers for median-TWF, trimean-TWF, and TWF at
different levels of outlier magnitudes.

10

0

10

-1

10

-2

10

-3

10

r
o
r
r
e
 
e
v
i
t
a
e
R

l

-4

10

0

median-TWF with outliers

TWF with outliers

TWF without outliers

50

100

150

Iterations

0

10

-1

10

-2

10

-3

10

r
o
r
r
e

 

e
v
i
t

l

a
e
R

-4

10

0

median-TWF with outliers

TWF with outliers

TWF  without outliers

50

100

150

Iterations

100

r
o
r
r
e

 

e
v
i
t

l

a
e
R

10-1

10-2

10-3

0

median-TWF with outliers
TWF with outliers
TWF without outliers

50

100

150

Iterations

(a) kwk∞ = 0.01kxk2

(b) kwk∞ = 0.001kxk2

(c) Poisson noise

Figure 3: The relative error with respect to the iteration count for median-TWF and TWF with both
dense noise and sparse outliers, and TWF with only dense noise. Performance of median-TWF with
both dense noise and sparse outliers is comparable to that of TWF without outliers. (a) and (b):
Uniform noise with different levels; (c) Poisson noise.

We now examine the performance of median-TWF in the presence of both sparse outliers and
dense bounded noise. The entries of the dense bounded noise w is generated independently from
U(0,kwk∞), with kwk∞/kxk2 = 0.001, 0.01 respectively. The entries of the outlier is then gen-
erated as ηi ∼ kwk · Bernoulli(0.1) independently. Figure 3(a) and Figure 3(b) depict the relative
error dist(z(t), x)/kxk with respect to the iteration count t, for uniform noise at different levels.
It can be seen that median-TWF under outlier corruption clearly outperforms TWF under the same
situation, and acts as if the outliers do not exist by achieving almost the same accuracy as TWF
under no outliers. Moreover, the solution accuracy of median-TWF has 10 times gain from Figure
3(a) to Figure 3(b) as kwk∞ shrinks by 1/10 , which corroborates Theorem 2 nicely.
Finally, we consider when the measurements are corrupted both by Poisson noise and outliers, which
models photon detection in optical imaging applications. We generate each measurement as yi ∼
Poisson(|hai, xi|2), for i = 1,··· , m, which is then corrupted with probability s = 0.1 by outliers.
The entries of the outlier are obtained by ﬁrst generating ηi ∼ kxk2·U(0, 1) independently, and then
rounding it to the nearest integer. Figure 3(c) depicts the relative error dist(z(t), x)/kxk with respect
to the iteration count t, where again median-TWF under both Poisson noise and sparse outlier noise
has almost the same accuracy as TWF under only Poisson noise.

7 Conclusions

In this paper, we proposed median-TWF, and showed that it allows exact recovery even with a
constant proportion of arbitrary outliers for robust phase retrieval. This is in contrast to recently
developed WF and TWF, which likely to fail under outlier corruptions. We anticipate that sample
median can be applied to designing provably robust non-convex algorithms for other inference prob-
lems under sparse arbitrary corruptions. The techniques we develop here to analyze performance
guarantee for median-based algorithms will be useful in those contexts as well.

11

Supplementary Materials

A Proof of Properties of Median (Section 5.1)

A.1 Proof of Lemma 1
For simplicity, denote θp := θp(F ) and ˆθp := θp({Xi}m
for an ǫ, there exists a δ1 such that P(X ≤ θp − ǫ) = p − δ1, where δ1 ∈ (ǫl, ǫL). Then one has
1{Xi≤θp−ǫ} ≥ pm! = P  1
1{Xi≤θp−ǫ} ≥ (p − δ1) + δ1!
P(cid:16)ˆθp < θp − ǫ(cid:17) (a)
1) ≤ exp(−2mǫ2l2),

= P  m
Xi=1
≤ exp(−2mδ2

i=1). Since F ′ is continuous and positive,

(b)

m

Xi=1

m

where (a) is due to the deﬁnition of the quantile function in (15) and (b) is due to the fact that
1{Xi≤θp−ǫ} ∼ Bernoulli(p − δ1) i.i.d., followed by the Hoeffding inequality. Similarly, one can
show for some δ2 ∈ (ǫl, ǫL),

P(cid:16)ˆθp > θp + ǫ(cid:17) ≤ exp(−2mδ2

2) ≤ exp(−2mǫ2l2).

Combining these two inequalities, one has the conclusion.

A.2 Proof of Lemma 2

It sufﬁces to show that

|X(k) − Y(k)| ≤ max

l

|Xl − Yl|,

∀k = 1,··· , n.

(25)

Case 1: k = n, suppose X(n) = Xi and Y(n) = Yj, i.e., Xi is the largest among {Xl}n
the largest among {Yl}n

l=1. Then we have either Xj ≤ Xi ≤ Yj or Yi ≤ Yj ≤ Xi. Hence,

l=1 and Yj is

|X(n) − Y(n)| = |Xi − Yj| ≤ max{|Xi − Yi|,|Xj − Yj|}.

Case 2: k = 1, suppose that X(1) = Xi and Y(1) = Yj. Similarly

|X(1) − Y(1)| = |Xi − Yj| ≤ max{|Xi − Yi|,|Xj − Yj|}.

Case 3: 1 < k < n, suppose that X(k) = Xi, Y(k) = Yj, and without loss of generality assume that
Xi < Yj (if Xi = Yj, 0 = |X(k) − Y(k)| ≤ maxl |Xl − Yl| holds trivially). We show the conclusion
by contradiction.
Assume |X(k) − Y(k)| > maxl |Xl − Yl|. Then one must have Yi < Yj and Xj > Xi and i 6= j.
Moreover for any p < k and q > k, the index of X(p) cannot be equal to the index of Y(q); otherwise
the assumption is violated.
Thus, all Y(q) for q > k must share the same index set with X(p) for p > k. However, Xj, which is
larger than Xi (thus if Xj = X(k′), then k′ > k), shares the same index with Yj, where Yj = Y(k).
This yields contradiction.

A.3 Proof of Lemma 3

Assume that sm is an integer. Since there are sm corrupted samples in total, one can se-

2 − s)m(cid:7) clean samples from the left half of ordered contaminated samples
lect out at least (cid:6)( 1
{θ1/m({Xi}), θ2/m({Xi}),··· , θ1/2({Xi})}. Thus one has the left inequality. Furthermore, one
2 − s)m(cid:7) clean samples from the right half of ordered contaminated
can also select out at least(cid:6)( 1
samples {θ1/2({Xi}),··· , θ1({Xi})}. One has the right inequality.

12

A.4 Proof of Lemma 4

First we introduce some general facts for the distribution of the product of two correlated standard
Gaussian random variables [34]. Let u ∼ N (0, 1), v ∼ N (0, 1), and their correlation coefﬁcient be
ρ ∈ [−1, 1]. Then the density of uv is given by
exp(cid:18) ρx

1 − ρ2(cid:19) K0(cid:18) |x|

1 − ρ2(cid:19) ,

x 6= 0,

φρ(x) =

1

where K0(·) is the modiﬁed Bessel function of the second kind. Thus the density of r = |uv| is

x > 0,

(26)

πp1 − ρ2
πp1 − ρ2 (cid:20)exp(cid:18) ρx

1

ψρ(x) =

for |ρ| < 1. If |ρ| = 1, r becomes a χ2
ψ|ρ|=1(x) =

1 − ρ2(cid:19) + exp(cid:18)−

ρx

1 − ρ2(cid:19)(cid:21) K0(cid:18) |x|

1 − ρ2(cid:19) ,

1 random variable, with the density

1
√2π

x−1/2 exp(−x/2),

x > 0.

It can be seen from (26) that the density of r only relates to the correlation coefﬁcient ρ ∈ [−1, 1].
Let θ1/2(ψρ) be the 1/2 quantile (median) of the distribution ψρ(x), and ψρ(θ1/2) be the value of
the function ψρ at the point θ1/2(ψρ). Although it is difﬁcult to derive the analytical expressions of
θ1/2(ψρ) and ψρ(θ1/2) due to the complicated form of ψρ in (26), due to the continuity of ψρ(x)
and θ1/2(ψρ), we can calculate them numerically, as illustrated in Figure 4. From the numerical

ρ

p

)

ψ

(

θ
 
e

l
i
t
n
a
u
Q

0.48

0.46

0.44

0.42

0.4

0.38

0.36

0.34

0.32

0

p=0.49
p=0.50
p=0.51

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Correlation ρ

)

p

ρ

θ
(

ψ
y
t
i
s
n
e
D

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0.45

0

p=0.49
p=0.50
p=0.51

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Correlation ρ

Figure 4: Quantiles and density at quantiles across ρ

calculation, one can see that both ψρ(θ1/2) and θ1/2(ψρ) are bounded from below and above for all
ρ ∈ [0, 1] (ψρ(·) is symmetric over ρ, hence it is sufﬁcient to consider ρ ∈ [0, 1]), satisfying

0.348 < θ1/2(ψρ) < 0.455,

0.47 < ψρ(θ1/2) < 0.76.

B Robust Initialization with Outliers (Section 5.2)

(27)

This section proves that the truncated spectral method provides a good initialization even if sm
measurements are corrupted by arbitrary outliers as long as s is small.
Consider the model in (1). Lemma 3 yields

Observe that aT
i x = ˜a2
able. Thus |˜ai1|2 is a χ2

i x)2}) < θ1/2({yi}) < θ 1

2−s({(aT
θ 1
i1kxk2, where ˜ai1 = aT
i x/kxk is a standard Gaussian random vari-
1 random variable, whose cumulative distribution function is denoted as

2 +s({(aT

i x)2}).

(28)

K(x). Moreover by Lemma 1, for a small ǫ, one has (cid:12)(cid:12)(cid:12)θ 1
(cid:12)(cid:12)(cid:12)θ 1

2−s(K)(cid:12)(cid:12)(cid:12) < ǫ and
2 +s(K)(cid:12)(cid:12)(cid:12) < ǫ with probability 1 − 2 exp(−cmǫ2) and c is a constant around

2 +s({|˜ai1|2}) − θ 1
2 × 0.472 (c.f. Figure 4). We note that θ1/2(K) = 0.455 and both θ 1

2−s({|˜ai1|2}) − θ 1

2−s(K) and θ 1

2 +s(K) can be

13

arbitrarily close to θ 1
has

2

(K) simultaneously as long as s is small enough (independent of n). Thus one

(cid:16)θ 1
2 −s(K) − ǫ(cid:17)kxk2 < θ1/2({yi}) <(cid:16)θ 1

(29)
with probability at least 1−exp(−cmǫ2). For the sake of simplicity, we introduce two new notations
2 +s(K). Speciﬁcally for the instance of s = 0.01, one has ζs = 0.434
ζs := θ 1
and ζs = 0.477. It is easy to see that ζs − ζs can be arbitrarily small if s is small enough.
We ﬁrst consider the case when kxk = 1. On the event that (29) holds, the truncation function has
the following bounds,

2 +s(K) + ǫ(cid:17)kxk2,

2−s(K) and ζs := θ 1

1{yi≤α2
1{yi≤α2

yθ1/2({yi})/0.455} ≤ 1{yi≤α2
yθ1/2({yi})/0.455} ≥ 1{yi≤α2

y(ζs+ǫ)/0.455}
y(ζs−ǫ)/0.455}.

On the other hand, denote the support of the outliers as S, we have

Y =

1

mXi /∈S

aiaT

i (aT

i x)21

{(aT

i

x)2≤α2

yθ1/2({yi})/0.455} +

1

mXi∈S

Consequently, one can bound Y as

aiaT

i yi1{yi≤α2

yθ1/2({yi})/0.455}.

Y1 :=

≺

1

mXi /∈S
mXi /∈S

1

where we have

aiaT

i (aT

i x)21

aiaT

i (aT

i x)21

{(aT

i

{(aT

i

x)2≤α2

x)2≤α2

y(ζs−ǫ)/0.455} ≺ Y
1

y(ζs+ǫ)/0.455} +

mXi∈S

aiaT

i α2

y(ζs + ǫ)/0.455 =: Y2,

E[Y1] = (1 − s)(β1xxT + β2I), E[Y2] = (1 − s)(β3xxT + β4I) + sα2

y

(ζs + ǫ)
0.455

I,

(30)

:=

E[ξ41n|ξ|≤αy√(ζs−ǫ)/0.455o] − E[ξ21n|ξ|≤αy√(ζs−ǫ)/0.455o],

with β1
E[ξ21n|ξ|≤αy√(ζs−ǫ)/0.455o]
E[ξ21n|ξ|≤αy√(ζs+ǫ)/0.455o],
Applying standard results on random matrices with non-isotropic sub-Gaussian rows [33, equation
(5.26)] and noticing that aiaT
i for some sub-Gaussian
vector bi := ai(aT

and
β4 := E[ξ21n|ξ|≤αy√(ζs+ǫ)/0.455o], assuming ζ ∼ N (0, 1).

E[ξ41n|ξ|≤αy√(ζs+ǫ)/0.455o] −

can be rewritten as bibT

i (aT

:=

:=

β2

β3

i x)1

{|aT

i

i

{|aT

i x)21
x|≤c}
, one can deduce
x|≤c}
kY1 − E[Y1]k ≤ δ,

(31)
with probability 1 − exp(−Ω(m)), provided that m/n exceeds some large constant. Besides, when
ǫ and s are sufﬁciently small, one further has kE[Y1] − E[Y2]k ≤ δ. Putting these together, one has
(32)

kY2 − E[Y2]k ≤ δ

kY − (1 − s)(β1xxT + β2I)k ≤ 3δ.

Let ˜z(0) be the normalized leading eigenvector of Y . Repeating the same argument as in [10, Section
7.8] and taking δ, ǫ to be sufﬁciently small, one has

for a given ˜δ > 0, as long as m/n exceeds some large constant.

dist( ˜z(0), x) ≤ ˜δ,

(33)

med({yi})

Furthermore let z(0) =pmed{yi}/0.455˜z(0) to handle cases kxk 6= 1. By the bound (29), one has
(cid:12)(cid:12)(cid:12)(cid:12)

0.455 − kxk2(cid:12)(cid:12)(cid:12)(cid:12) ≤ max(cid:26)(cid:12)(cid:12)(cid:12)(cid:12)

0.455 − 1(cid:12)(cid:12)(cid:12)(cid:12)

0.455 − 1(cid:12)(cid:12)(cid:12)(cid:12)

(cid:27)kxk2 ≤

ζs − ζs + ǫ

ζs − ǫ

,(cid:12)(cid:12)(cid:12)(cid:12)

kxk2

ζs + ǫ

0.455

Thus

(34)

ζs − ζs + ǫ

0.455

kxk + ˜δkxk ≤

1
11kxk

(35)

dist(z(0), x) ≤
as long as s is a small enough constant.

14

C Geometric Convergence for Noise-free Model (Proof of Corollary 2)

After obtaining a good initialization, the central idea to establish geometric convergence is to show
that the truncated gradient ∇ℓtr(z) in the neighborhood of the global optima satisﬁes the Regularity
Condition RC(µ, λ, ǫ) deﬁned in Deﬁnition 2. We show this by two steps. Step 1 establishes a key
concentration property for the sample median used in the truncation rule, which is then subsequently
exploited to prove RC in Step 2.

C.1 Proof of Concentration Property for Sample Median

We show that the sample median used in the truncation rule concentrates at the level kz − xkkzk
as stated in the following proposition. Along the way, we also establish that the sample quantiles
around the median are also concentrated at the level kz − xkkzk.
Proposition 2 (Reﬁned version of Proposition 1) Fix ǫ ∈ (0, 1). If m > c0(ǫ−2 log 1
then with probability at least 1 − c1 exp(−c2mǫ2),

ǫ )n log n,

(0.65 − ǫ)kzkkhk ≤ med{|(aT
(0.63 − ǫ)kzkkhk ≤ θ0.49, θ0.51{|(aT

i x)2 − (aT

i z)2|} ≤ (0.91 + ǫ)kzkkhk,

i x)2 − (aT

i z)2|} ≤ (0.95 + ǫ)kzkkhk,

(36)
(37)

hold for all x, z with kx − zk < 1/11kzk, where h := z − x.
proof
We ﬁrst show for a ﬁxed pair z and x, (36) and (37) hold with high probability.
Let ri = |(aT
i z)2|. Then ri’s are i.i.d. copies of a random variable r, where r =
|(aT x)2 − (aT z)2| with the entries of a composed of i.i.d. standard Gaussian random variables.
Note that the distribution of r is ﬁxed once given h and z.
Let x(1) denote the ﬁrst element of a generic vector x, and x−1 denote the remaining vector of x
after eliminating the ﬁrst element. Let Uh be an orthonormal matrix with ﬁrst row being hT /khk,
and ˜a = Uha, ˜z = Uhz. Similarly deﬁne U˜z−1 and let ˜b = U˜z−1 ˜a−1. Then ˜a(1) and ˜b(1) are
independent standard normal random variables. We further express r as follows.

i x)2 − (aT

r = |(aT z)2 − (aT x)2|
= |(2aT z − aT h)(aT h)|
= |(2˜aT ˜z − ˜a(1)khk)(˜a(1)khk)|
= |(2hT z − khk2)˜a(1)2 + 2(˜aT
−1 ˜z−1)(˜a(1)khk)|
= |(2hT z − khk2)˜a(1)2 + 2˜b(1)k ˜z−1k˜a(1)khk|
= |(2hT z − khk2)˜a(1)2 + 2pkzk2 − ˜z(1)2 ˜a(1)˜b(1)khk|
˜a(1)˜b(1)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
kzk(cid:19) ˜a(1)2 + 2s1 −(cid:18) hT z
(cid:18)2
=:(cid:12)(cid:12)(cid:12)(2 cos(ω) − t)˜a(1)2 + 2p1 − cos2(ω)˜a(1)˜b(1)(cid:12)(cid:12)(cid:12) · khkkzk

khkkzk − khk

khkkzk(cid:19)2

=: |u˜v| · khkkzk

hT z

· khkkzk

where ω is the angle between h and z, and t = khk/kzk < 1/11. Consequently, u = ˜a(1) ∼
N (0, 1) and ˜v = (2 cos(ω) − t)˜a(1) + 2| sin(ω)|˜b(1) is also a Gaussian random variable with
variance 3.6 < Var(˜v) < 4 under the assumption t < 1/11.
Let v = ˜v/pVar(˜v), then v ∼ N (0, 1). Furthermore, let r′ = |uv|. Denote the density function of
r′ as ψρ(·) and the 1/2-quantile point of r′ as θ1/2(ψρ). By Lemma 4, we have
0.348 < θ1/2(ψρ) < 0.455.

0.47 < ψρ(θ1/2) < 0.76,

(38)

By Lemma 1, we have with probability at least 1 − 2 exp(−cmǫ2) (here c is around 2 × 0.472),

0.348 − ǫ < med({r′i}m

i=1) < 0.455 + ǫ.

(39)

15

The same arguments carry over to other quantiles θ0.49({r′i}) and θ0.51({r′i}). From Figure. 4, we
observe that for ρ ∈ [0, 1]

0.45 < ψρ(θ0.49), ψρ(θ0.51) < 0.78,

0.336 < θ0.49(ψρ), θ0.51(ψρ) < 0.477
and then we have with probability at least 1 − 2 exp(−cmǫ2) (here c is around 2 × 0.452),

0.336 − ǫ < θ0.49({r′m}), θ0.51({r′m}) < 0.477 + ǫ.

(40)

(41)

(42)
(43)

(44)

Hence, by multiplying backpVar(˜v), we have with probability 1 − 2 exp(−cmǫ2),
(0.65 − ǫ)kz − xkkzk ≤ med(cid:0)(cid:8)|(aT
(0.63 − ǫ)kz − xkkzk ≤ θ0.49, θ0.51(cid:0)(cid:8)|(aT
(cid:8)|(aT
i z)2 − (aT
prove (36) and (37) for all z and x with kz − xk ≤ 1
we argue for median ﬁrst and the same arguments carry over to other quantiles smoothly.
To proceed, we restate (42) as

We note that, to keep notation simple, c and ǫ may vary line by line within constant factors.
Up to now, we proved for any ﬁxed z and x,

i x)2|(cid:9) are upper and lower bounded by kz − xkkzk times constant factors. To
11kzk, we use the net covering argument. Still

i x)2|(cid:9)(cid:1) ≤ (0.91 + ǫ)kz − xkkzk,
i x)2|(cid:9)(cid:1) ≤ (0.95 + ǫ)kz − xkkzk.

the median or neighboring quantiles of

i z)2 − (aT
i z)2 − (aT

(0.65 − ǫ) ≤ med(cid:18)(cid:26)(cid:12)(cid:12)(cid:12)(cid:12)

i z)

(cid:18) 2(aT
kzk −

aT
i h
khk

i h

khk(cid:12)(cid:12)(cid:12)(cid:12)
(cid:27)(cid:19) ≤ (0.91 + ǫ),
kzk(cid:19) aT
khk

holds with probability at least 1 − 2 exp(−cmǫ2) for a given pair h, z satisfying khk/kzk ≤ 1/11.
Let τ = ǫ/(6n + 6m), and let Sτ be a τ-net covering the unit sphere, Lτ be a τ-net covering a line
with length 1/11, and set

One has cardinality bound (i.e.,
2/τ )2n/(11τ ) < (1 + 2/τ )2n+1. Taking the union bound we have

Nτ = {(z0, h0, t0) : (z0, h0, t0) ∈ Sτ × Sτ × Lτ}.

(45)
the upper bound on the covering number) |Nτ| ≤ (1 +

i z0) − (aT

(0.65 − ǫ) ≤ med(cid:0)(cid:8)|2(aT
with probability at least 1 − (1 + 2/τ )2n+1 exp(−cmǫ2).
We next argue that (46) holds with probability 1− c1 exp(−c2mǫ2) for some constants c1, c2 as long
as m ≥ c0(ǫ−2 log ǫ−1)n log n for sufﬁcient large constant c0. To prove this claim, we ﬁrst observe

i h0|(cid:9)(cid:1) ≤ (0.91 + ǫ),

∀(z0, h0, t0) ∈ Nǫ

i h0)t0||aT

(46)

(1 + 2/τ )2n+1 ≍ exp(2n(log(n + m) + log 12 + log(1/ǫ))) ≍ exp(2n(log m)).

We note that once ǫ is chosen, it is ﬁxed in the whole proof and does not scale with m or n. For
simplicity, assume that ǫ < 1/e. Fix some positive constant c′ < c− c2. It then sufﬁces to show that
there exist large constant c0 such that if m ≥ c0(ǫ−2 log ǫ−1)n log n, then

2n log m < c′mǫ2.

(47)

For any ﬁxed n, if (47) holds for some m and m > (2/c′)ǫ−2n, then (47) always holds for larger
m, because

2n log(m + 1) = 2n log m + 2n(log(m + 1) − log m) = 2n log m +

2n
m

log(1 +

1
m

)m

≤ 2n log m +

2n
m ≤ c′mǫ2 + c′ǫ2 = c′(m + 1)ǫ2.

Next, for any n, we can always ﬁnd a c0 such that (47) holds for m = c0(ǫ−2 log ǫ−1)n log n. Such
c0 can be easily found for large n, i.e., c0 = 4/c′ is a valid option if

(4/c′)(ǫ−2 log ǫ−1)n log n < n2.

(48)

Moreover, since the number of n that violates (48) is ﬁnite, the maximum over all such c0 serves the
purpose.

16

Next, one needs to bound

(cid:12)(cid:12)med(cid:0)(cid:8)|2(aT

i z0) − (aT

i h0)t0||aT

i h0|(cid:9)(cid:1) − med(cid:0)(cid:8)|2(aT

i z) − (aT

i h)t||aT

i h|(cid:9)(cid:1)(cid:12)(cid:12)

for any kz − z0k < τ,kz − z0k < τ and kt − t0k < τ.

+ max

i h)t||aT

i h|(cid:9)(cid:1)(cid:12)(cid:12)

≤ max
≤ max

i z0) − (aT
i z0) − (aT
i z0) − (aT
i z) − (aT

i z) − (aT
i h)(cid:12)(cid:12)
i h)t(cid:1) (aT
i h0)(cid:12)(cid:12)
i h)t(cid:1) (aT
i h)(cid:12)(cid:12)
i h)t(cid:1) (aT
i∈[m](cid:12)(cid:12)2(aT
i h0(cid:12)(cid:12) + max

By Lemma 2 and the relation(cid:12)(cid:12)|x| − |y|(cid:12)(cid:12) ≤ |x − y|, we have
(cid:12)(cid:12)med(cid:0)(cid:8)|2(aT
i h0|(cid:9)(cid:1) − med(cid:0)(cid:8)|2(aT
i h0)t0||aT
i∈[m](cid:12)(cid:12)(cid:0)2(aT
i h0) −(cid:0)2(aT
i h0)t0(cid:1) (aT
i z) − (aT
i∈[m](cid:12)(cid:12)(cid:0)2(aT
i h0) −(cid:0)2(aT
i h0)t0(cid:1) (aT
i z) − (aT
i∈[m](cid:12)(cid:12)(cid:0)2(aT
i h)t(cid:1) (aT
i h0) −(cid:0)2(aT
i z) − (aT
i h)t(cid:12)(cid:12)(cid:1)(cid:12)(cid:12)aT
i (z0 − z)(cid:12)(cid:12) +(cid:12)(cid:12)(aT
i∈[m](cid:0)(cid:12)(cid:12)2aT
i h0)t0 − (aT
≤ max
i∈[m]kaik2(2 + t)τ
i∈[m]kaik2(3 + t)τ + max
≤ max
i∈[m]kaik2(5 + 2t)τ
≤ max
On the event E1 :=(cid:8)maxi∈[m] kaik2 ≤ m + n(cid:9), one can show that
(cid:12)(cid:12)med(cid:0)(cid:8)|2(aT
i z) − (aT
(49)
We claim that E1 holds with probability at least 1 − m exp(−m/8) if m > n. This can be argued
as follows. Notice that kaik2 = Pn
j=1 ai(j)2, where ai(j) is the j th element of ai. In other
1 random variables. Applying the Bernstein-type inequality
words, kaik2 is a sum of n i.i.d. χ2
1 is smaller than 2, we
(Corollary5.17 Vershynin) and observing that the sub-exponential norm of χ2
have

i h0|(cid:9)(cid:1) − med(cid:0)(cid:8)|2(aT

i h|(cid:9)(cid:1)(cid:12)(cid:12) < 6(m + n)τ < ǫ.

i h)t(cid:12)(cid:12)|aT

i z0) − (aT

i h0)t0||aT

i z) − (aT

i h)t||aT

i (h0 − h)|

P(cid:8)kaik2 ≥ m + n(cid:9) ≤ exp(−m/8).

ǫ )n log n. On the intersection of E1 and E2, (36) holds.

Then a union bound concludes the claim.
Note that (46) holds on an event E2, which has probability 1 − c1 exp(−c2mǫ2) as long as m ≥
c0(ǫ−2 log 1
The net covering arguments can also carry over to show that (37) holds for all x and z obeying
kx − zk ≤ 1
11kzk.
C.2 Proof of RC

Following Proposition 2, we choose some small ǫ (i.e. ǫ < 0.03), then with probability at least
1 − exp(−Ω(m)),

holds for all z and x satisfying khk ≤ 1/11kzk. For each i, we introduce two new events

i x)2 − (aT

i z)2|(cid:9)) ≤ 1.0kz − xkkzk

(50)

(51)

(52)
(53)

(54)

0.6kz − xkkzk ≤ med((cid:8)|(aT
i x)2 − (aT
i x)2 − (aT

E i
E i

3 := {(cid:12)(cid:12)(aT
4 := {(cid:12)(cid:12)(aT

i z)2(cid:12)(cid:12) ≤ 0.6αhkhk · |aT
i z)2(cid:12)(cid:12) ≤ 1.0αhkhk · |aT

i z|},
i z|}.

E i
3 ⊆ E i

2 ⊆ E i

4

Conditioned on (51), the following inclusion property

holds for all i, where E i
because E i
quadratic inequalities in E i
their properties in the following lemma.

It is easier to work with these new events
4’s) are statistically independent for any ﬁxed x and z. To further decouple the
4 into linear inequalities, we introduce two more events and states

2 is deﬁned in Algorithm 1.
3 and E i

3’s (resp. E i

17

Lemma 5 (Lemma 3 in [12]) For any γ > 0, deﬁne

γ

γ

(56)

(55)

2a∗i z

Di
Di,1
Di,2

1 deﬁned in Algorithm 1, the quadratic inequality specifying Di

On the event E i
belongs to two intervals centered around 0 and 2aT
inclusion property holds

γ := {(cid:12)(cid:12)(a∗i x)2 − (a∗i z)2(cid:12)(cid:12) ≤ γkhk|a∗i z|},
khk ≤ γ(cid:27) ,
:=(cid:26)|a∗i h|
:=(cid:26)(cid:12)(cid:12)(cid:12)(cid:12)
khk (cid:12)(cid:12)(cid:12)(cid:12) ≤ γ(cid:27) .
a∗i h
khk −
1(cid:19) ∪(cid:18)Di,2
1(cid:19) ⊆ Di
1+√2 ∩ E i
m∇ℓtr(z), h(cid:11) is lower bounded on the order of khk2, as
Using Lemma 2, we can establish that −(cid:10) 1
in Proposition 3, and that(cid:13)(cid:13) 1
m∇ℓtr(z)(cid:13)(cid:13) is upper bounded on the order of khk, as in Proposition 4.
Proposition 3 (Adapted version of Proposition 2 of [12]) Consider the noise-free measurements
i x|2 and any ﬁxed constant ǫ > 0. Under the condition (10), if m > c0n log n, then with
yi = |aT
probability at least 1 − c1 exp(−c2m),

γ and Di,2
1(cid:1) ∪(cid:0)Di,2

γ implicates that aT

i h
γ . The following

i z, respectively, i.e. Di,1

(cid:18)Di,1
1+√2 ∩ E i

1(cid:1) .
γ ∩ E i

1 ⊆(cid:0)Di,1

γ ∩ E i

γ ∩ E i

(58)

(57)

γ

γ

m∇ℓtr(z), h(cid:29) ≥ 2n1.99 − 2(ζ1 + ζ2) −p8/πα−1
−(cid:28) 1
2αu + αl ) ,
,p98/3(αl)2

holds uniformly over all x, z ∈ Rn satisfying
kzk ≤ min( 1
,

khk

αl
αh

αl
6

11

,

h − ǫokhk2

where c0, c1, c2 > 0 are some universal constants, and ζ1, ζ2 are deﬁned in (10).

The proof of Proposition 3 adapts the proof of Proposition 2 of [12], by properly setting parameters
based on the properties of sample median. For completeness, we include a short outline of the proof
in Appendix F.

Proposition 4 (Lemma 7 of [12]) Under the same condition as in Proposition 3, if m > c0n log n,
then there exist some constants, c1, c2 > 0 such that with probability at least 1 − c1 exp(−c2m),

(59)

(60)

(61)

(62)

and

1

m∇ℓtr(z)(cid:13)(cid:13)(cid:13)(cid:13) ≤ (1 + δ) · 4p1.02 + 2/αhkhk
holds uniformly over all x, z ∈ Rn satisfying
2αu + αl ) ,
kzk ≤ min( 1
,p98/3(αl)2
,

khk

(cid:13)(cid:13)(cid:13)(cid:13)

αl
αh

αl
6

11

,

where δ can be arbitrarily small as long as m/n sufﬁciently large.

With these two propositions, RC is guaranteed by setting µ < µ0 :=

λ + µ · 16(1 + δ)2 · (1.02 + 2/αh) < 4n1.99 − 2(ζ1 + ζ2) −p8/πα−1

D Geometric Convergence with Outliers (Proof of Theorem 1)

(1.99−2(ζ1+ζ2)−√8/πα−1
4(1+δ)2·(1.02+2/αh)
h − ǫo.

h

We consider the model (1) with outliers, i.e., yi = |hai, xi|2 + ηi for i = 1,··· , m. It sufﬁces to
show that ∇ℓtr(z) satisﬁes the RC. The critical step is to lower and upper bound the sample median
of the corrupted measurements. Lemma 3 yields

2 −s({|(aT
θ 1

i x)2 − (aT

i z)2|}) ≤ θ 1

2

({|yi − (aT

i z)2|}) ≤ θ 1

2 +s({|(aT

i x)2 − (aT

i z)2|}.

(63)

18

For the instance of s = 0.01, by (37) in Proposition 2, we have with probability at least
1 − 2 exp(−Ω(m)ǫ2),

(0.63 − ǫ)kzkkhk ≤ θ 1

2

({|yi − (aT

i z)2|}) ≤ (0.95 + ǫ)kzkkhk.

(64)

To differentiate from E i
We then have

2, we deﬁne ˜E i

2 :=n(cid:12)(cid:12)(aT

i x)2 − (aT

i z)2(cid:12)(cid:12) ≤ αhmed(cid:8)(cid:12)(cid:12)yi − (aT

i z)2(cid:12)(cid:12)(cid:9) |a

T

i z|

kzk o.

−∇ℓtr(z) = 2

= 2

ai1

E i
1∩E i

2

m

m

Xi=1
Xi=1

(aT

(aT

i z

i z)2 − yi
aT
i z)2 − (aT
{z

i z

aT

∇cleanℓtr(z)

i x)2

|

ai1

1∩ ˜E i
E i

2

+ 2Xi∈S(cid:18) (aT
|

}

i z)2 − yi
aT

i z

1

E i
1∩E i

(aT

i z)2 − (aT

i x)2

aT

i z

2 −
{z

∇extra ℓtr(z)

1

.

1∩ ˜E i
E i

2(cid:19) ai
}

Choosing ǫ small enough, the inclusion property (i.e. E i
arguments for Proposition 3 and 4 are also valid to ∇cleanℓtr(z). Thus, one has

4) holds, and all the proof

h − ǫokhk2,

We next bound the contribution of ∇extraℓtr(z). Introduce q = [q1, . . . , qm]T , where
2(cid:19) 1{i∈S},

i z)2 − yi
aT

1∩ ˜E i
E i

i x)2

2 −

i z

aT

1

and then |qi| ≤ 2αhkhk. Thus kqk ≤ √sm · 2αhkhk, and

1

1

3 ⊆ ˜E i

2 ⊆ E i
mh∇cleanℓtr(z), hi ≥ 2n1.99 − 2(ζ1 + ζ2) −p8/πα−1
m(cid:13)(cid:13)∇cleanℓtr(z)(cid:13)(cid:13) ≤ (1 + δ) · 4p1.02 + 2/αhkhk.
qi :=(cid:18) (aT
(cid:13)(cid:13)(cid:13)(cid:13)
m∇extraℓtr(z)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:12)(cid:12)(cid:12)(cid:12)
m∇extraℓtr(z), h(cid:29)(cid:12)(cid:12)(cid:12)(cid:12) ≤ khk ·(cid:13)(cid:13)(cid:13)(cid:13)
(cid:28) 1
m∇ℓtr(z), h(cid:29) ≥(cid:28) 1

m(cid:13)(cid:13)AT q(cid:13)(cid:13) ≤ 2(1 + δ)√sαhkhk,

i z)2 − (aT

E i
1∩E i

(aT

i z

=

1

1

1

1

m∇extraℓtr(z)(cid:13)(cid:13)(cid:13)(cid:13) ≤ 2(1 + δ)√sαhkhk2,

where A = [a1, . . . , am]T . Then, we have

−(cid:28) 1

m∇cleanℓtr(z), h(cid:29) −(cid:12)(cid:12)(cid:12)(cid:12)
m∇extraℓtr(z), h(cid:29)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:28) 1
h − ǫ − (1 + δ)√sαh(cid:17)khk2,
≥ 2(cid:16)1.99 − 2(ζ1 + ζ2) −p8/πα−1
m∇extraℓtr(z)(cid:13)(cid:13)(cid:13)(cid:13)
m∇cleanℓtr(z)(cid:13)(cid:13)(cid:13)(cid:13)
m∇ℓtr(z)(cid:13)(cid:13)(cid:13)(cid:13) ≤(cid:13)(cid:13)(cid:13)(cid:13)
+(cid:13)(cid:13)(cid:13)(cid:13)
≤ (1 + δ)(cid:16)4p1.02 + 2/αh + 2√sαh(cid:17)khk.

1

1

1

and

(cid:13)(cid:13)(cid:13)(cid:13)

(65)

(66)

(67)

(68)

(69)

(70)

(71)

(72)

(73)

The RC is guaranteed if µ, λ, ǫ are chosen properly and s is sufﬁciently small.

E Geometric Convergence with Outliers and Bounded Noise (Proof of

Theorem 2)

We consider the model (2) with outliers and bounded noise, i.e., yi = |hai, xi|2 + wi + ηi for
i = 1,··· , m. We omit the initialization analysis as it is similar to Appendix B. We split our
analysis of the gradient loop into two regimes.

19

Assume that s = 0.01 and apply Proposition 2. Moreover, if c3 is sufﬁciently large (i.e., c3 > 100)
and ǫ is small enough (i.e., ǫ < 0.02), then we have

• Regime 1: c4kzk ≥ khk ≥ c3 kwk∞
kzk
given by
µ

. In this regime, error contraction by each gradient step is

It sufﬁces to justify that ∇ℓtr(z) satisﬁes the RC. Denote ˜yi := (aT
we have

i x)2 + wi. Then by Lemma 3,

m∇ℓtr(z), x(cid:17) ≤ (1 − ρ)dist(z, x).

Moreover, by Lemma 2 we have

θ 1

dist(cid:16)z +
i z)2(cid:12)(cid:12)(cid:9) ≤ med(cid:8)(cid:12)(cid:12)yi − (aT
2−s(cid:8)(cid:12)(cid:12)˜yi − (aT
(cid:12)(cid:12)(cid:12)θ 1
2 +s(cid:8)(cid:12)(cid:12)(aT
2 +s(cid:8)(cid:12)(cid:12)˜yi − (aT
(cid:12)(cid:12)(cid:12)θ 1
2−s(cid:8)(cid:12)(cid:12)(aT
2−s(cid:8)(cid:12)(cid:12)˜yi − (aT
0.6kx − zkkzk ≤ med(cid:8)(cid:12)(cid:12)yi − (aT
2 :=n(cid:12)(cid:12)(aT

i z)2(cid:12)(cid:12)(cid:9) − θ 1
i z)2(cid:12)(cid:12)(cid:9) − θ 1

i x)2 − (aT

m

i z)2(cid:12)(cid:12) ≤ αhmed(cid:8)(cid:12)(cid:12)(aT

2

2

aT

i z

ai1

ai1

(aT

E i
1∩E i

E i
1∩E i

i x)2

i z)2 − yi
aT

−∇ℓtr(z) = 2

i z
i z)2 − (aT

Furthermore, recall ˜E i
(aT
Xi=1
= 2 Xi /∈S
|
− 2Xi /∈S
|

+Xi∈S
{z
+ 2Xi∈S(cid:18) (aT
i z)2 − yi
aT
|
}
{z
For i /∈ S, the inclusion property (i.e. E i
3 ⊆ E i
i z)2(cid:12)(cid:12) ∈(cid:12)(cid:12)(aT
(cid:12)(cid:12)yi − (aT

∇noiseℓtr (z)

∇cleanℓtr (z)

2 ⊆ E i

wi
aT
i z

i x)2 − (aT

E i
1∩E i

ai1

i z

2

4) holds because

∇extra ℓtr(z)

1

E i
1∩E i

2 −
{z
i z)2(cid:12)(cid:12) ± |wi|

(74)

(75)

(76)

(77)

(78)

i z)2(cid:12)(cid:12)(cid:9) ≤ θ 1

i x)2 − (aT
i x)2 − (aT

i z)2(cid:12)(cid:12)(cid:9) .
2 +s(cid:8)(cid:12)(cid:12)˜yi − (aT
i z)2(cid:12)(cid:12)(cid:9)(cid:12)(cid:12)(cid:12) ≤ kwk∞,
i z)2(cid:12)(cid:12)(cid:9)(cid:12)(cid:12)(cid:12) ≤ kwk∞.
i z)2(cid:12)(cid:12)(cid:9) ≤ 1kx − zkkzk.
i z)2 − yi(cid:12)(cid:12)(cid:9) |a

T

i z|

kzk o, then

(aT

i z)2 − (aT

i x)2

aT

i z

ai1

1∩ ˜E i
E i

2!
}

(aT

i z)2 − (aT

i x)2

aT

i z

1

.

1∩ ˜E i
E i

2(cid:19) ai
}

c3khkkzk for some sufﬁcient large c3. For i ∈ S, the inclusion E i

and |wi| ≤ 1
4 holds
because of (78). All the proof arguments for Proposition 3 and 4 are also valid for ∇cleanℓtr(z),
and thus we have
1

2 ⊆ E i

3 ⊆ ˜E i

1

mh∇cleanℓtr(z), hi ≥ 2n1.99 − 2(ζ1 + ζ2) −p8/πα−1
m(cid:13)(cid:13)∇cleanℓtr(z)(cid:13)(cid:13) ≤ (1 + δ) · 4p1.02 + 2/αhkhk.

h − ǫokhk2,

(79)

(80)

Next, we turn to control the contribution of the noise. Let ˜wi = 2wi

1

, then we have

aT
i

z

E i
1∩E i

2

1

mk∇noiseℓtr(z)k =(cid:13)(cid:13)(cid:13)(cid:13)

1
m

AT ˜w(cid:13)(cid:13)(cid:13)(cid:13) ≤(cid:13)(cid:13)(cid:13)(cid:13)

1
√m

AT(cid:13)(cid:13)(cid:13)(cid:13)

(cid:13)(cid:13)(cid:13)(cid:13)

˜w

√m(cid:13)(cid:13)(cid:13)(cid:13) ≤ (1 + δ)k ˜wk∞ ≤ (1 + δ)

when m/n is sufﬁciently large. Given the regime condition khk ≥ c3 kwk∞
kzk

, we further have

2kwk∞
αlkzk

,

(81)

2(1 + δ)
c3αl khk,

1
mk∇noiseℓtr(z)k ≤
1

m(cid:12)(cid:12)(cid:10)∇noiseℓtr(z), h(cid:11)(cid:12)(cid:12) ≤

1

m(cid:13)(cid:13)∇noiseℓtr(z)(cid:13)(cid:13) · khk ≤

2(1 + δ)
c3αl khk2.

20

(82)

(83)

1

1

1

1

=

aT

i z

i z

(aT

2 −

E i
1∩E i

i x)2

1∩ ˜E i
E i

We next bound the contribution of ∇extraℓtr(z). Introduce q = [q1, . . . , qm]T , where
2(cid:19) 1{i∈S}.

i z)2 − yi
aT

i z)2 − (aT
Then |qi| ≤ 2αhkhk, and kqk ≤ √sm · 2αhkhk. We thus have
m(cid:13)(cid:13)AT q(cid:13)(cid:13) ≤ 2(1 + δ)√sαhkhk,

qi := 2(cid:18) (aT
(cid:13)(cid:13)(cid:13)(cid:13)
m∇extraℓtr(z)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:12)(cid:12)(cid:12)(cid:12)
m∇extraℓtr(z), h(cid:29)(cid:12)(cid:12)(cid:12)(cid:12) ≤ khk ·(cid:13)(cid:13)(cid:13)(cid:13)
(cid:28) 1
m(cid:10)∇cleanℓtr(z), h(cid:11) −
≥ 2(cid:16)1.99 − 2(ζ1 + ζ2) −p8/πα−1

1
m h∇ℓtr(z), hi ≥

Putting these together, one has

m∇extraℓtr(z)(cid:13)(cid:13)(cid:13)(cid:13) ≤ 2(1 + δ)√sαhkhk2.
m(cid:12)(cid:12)(cid:10)∇noiseℓtr(z), h(cid:11)(cid:12)(cid:12) −

h − ǫ − (1 + δ)(1/(c3αl

−

1

1

1

1

m(cid:12)(cid:12)(cid:10)∇extraℓtr(z), h(cid:11)(cid:12)(cid:12)
z) + √sαh)(cid:17) khk2,

(87)

(84)

(85)

(86)

and

1
m k∇ℓtr(z)k ≤

1

1

m(cid:13)(cid:13)∇cleanℓtr(z)(cid:13)(cid:13) +
≤ 2(1 + δ)(cid:16)2p1.02 + 2/αh + 1/(c3αl

m(cid:13)(cid:13)∇noiseℓtr(z)(cid:13)(cid:13) +

z) + √sαh(cid:17)khk.

m(cid:13)(cid:13)∇extraℓtr(z)(cid:13)(cid:13)

1

(88)

The RC is guaranteed if µ, λ, ǫ are chosen properly, c3 is sufﬁciently large and s is sufﬁciently small.
• Regime 2: Once the iterate enters this regime with khk ≤ c3kwk∞
, each gradient iterate may
kzk
not reduce the estimation error. However, in this regime each move size µ
m∇ℓtr(z) is at most
O(kwk∞/kzk). Then the estimation error cannot increase by more than kwk∞
with a constant
kzk
factor. Thus one has

µ

dist(cid:16)z +

m∇ℓtr(z), x(cid:17) ≤ c5kwk∞
kxk
for some constant c5. As long as kwk∞/kxk2 is sufﬁciently small, it is guaranteed that c5 kwk∞
c4kxk. If the iterate jumps out of Regime 2, it falls into Regime 1.
F Proof of Proposition 3

kxk ≤

(89)

The proof adapts the proof of Proposition 2 in [12]. We outline the main steps for completeness.
Observe that for the noise-free case, yi = (aT

i x)2. We obtain

1
2m∇ℓtr(z) =

−

=

1
m

1
m

m

m

Xi=1
Xi=1

(aT

i z)2 − (aT

i x)2

aT

i z

ai1

1∩E i
E i

2

2(aT

i h)ai1

E i
1∩E i

2 −

1
m

i h)2
(aT
aT
i z

ai1

.

E i
1∩E i

2

m

Xi=1

One expects the contribution of the second term in (90) to be small as khk/kzk decreases.
Speciﬁcally, following the two inclusion properties (54) and (58), we have

3 ∩ E i
where the parameters γ3, γ4 are given by

γ3 ∩ E i
Di,1

1,γ3 ⊆ E i

1 ⊆ E i

2 ∩ E i

1 ⊆ E i

4 ∩ E i

1 ⊆ (Di,1

γ4 ∪ Di,2

γ4 ) ∩ E i

1

γ3 := 0.248αh,

and

γ4 := αh.

21

(90)

(91)

(92)

Continuing with the identity (90), we have a lower bound

−(cid:28) 1

2m∇ℓtr(z), h(cid:29) ≥

2
m

m

Xi=1

(aT

i h)21

1∩Di,1
E i

γ3 −

1
m

m

Xi=1

i h|3
|aT
|aT
i z|

1

Di,1
γ4 ∩E i

1 −

1
m

m

Xi=1

i h|3
|aT
|aT
i z|

1

.

1

Di,2
γ4 ∩E i
(93)

The three terms in (93) can be bounded following Lemmas 4, 5, and 6 in [12], which concludes the
proof.

References

[1] J. Drenth. X-Ray Crystallography. Wiley Online Library, 2007.
[2] J. R. Fienup. Phase retrieval algorithms: a comparison. Applied optics, 21(15):2758–2769,

1982.

[3] R. Balan, P. Casazza, and D. Edidin. On signal reconstruction without phase. Applied and

Computational Harmonic Analysis, 20(3):345–356, 2006.

[4] I. Waldspurger, A. d’Aspremont, and S. Mallat. Phase recovery, maxcut and complex semidef-

inite programming. Mathematical Programming, 149(1-2):47–81, 2015.

[5] E. J. Cand`es, T. Strohmer, and V. Voroninski. Phaselift: Exact and stable signal recovery from
magnitude measurements via convex programming. Communications on Pure and Applied
Mathematics, 66(8):1241–1274, 2013.

[6] Y. Chen, Y. Chi, and A. Goldsmith. Exact and stable covariance estimation from quadratic
sampling via convex programming. IEEE Transactions on Information Theory, 61(7):4034–
4059, July 2015.

[7] L. Demanet and P. Hand. Stable optimizationless recovery from phaseless linear measure-

ments. Journal of Fourier Analysis and Applications, 20(1):199–221, 2014.

[8] E. J. Cand`es and X. Li. Solving quadratic equations via phaselift when there are about as many
equations as unknowns. Foundations of Computational Mathematics, 14(5):1017–1026, 2014.
[9] X. Li and V. Voroninski. Sparse signal recovery from quadratic measurements via convex

programming. SIAM Journal on Mathematical Analysis, 2013.

[10] E. J. Cand`es, X. Li, and M. Soltanolkotabi. Phase retrieval via wirtinger ﬂow: Theory and

algorithms. IEEE Transactions on Information Theory, 61(4):1985–2007, 2015.

[11] M. Soltanolkotabi. Algorithms and theory for clustering and nonconvex quadratic program-

ming. PhD thesis, Stanford University, 2014.

[12] Y. Chen and E. J. Cand`es. Solving random quadratic systems of equations is nearly as easy as

solving linear systems. arXiv preprint arXiv:1505.05114, 2015.

[13] D. Weller, A. Pnueli, G. Divon, O. Radzyner, Y. Eldar, and J. Fessler. Undersampled phase
retrieval with outliers. Computational Imaging, IEEE Transactions on, 1(4):247–258, Dec
2015.

[14] P. Hand.

Phaselift is robust to a constant fraction of arbitrary errors.

arXiv:1502.04241, 2015.

arXiv preprint

[15] P. J. Huber. Robust statistics. Springer, 2011.
[16] K. Chen. On k-median clustering in high dimensions. In Proceedings of the seventeenth annual
ACM-SIAM symposium on Discrete algorithm, pages 1177–1185. Society for Industrial and
Applied Mathematics, 2006.

[17] D. Wagner. Resilient aggregation in sensor networks. In Proceedings of the 2nd ACM workshop

on Security of ad hoc and sensor networks, pages 78–87. ACM, 2004.

[18] Y. Chen, C. Caramanis, and S. Mannor. Robust sparse regression under adversarial corruption.
In Proceedings of the 30th International Conference on Machine Learning (ICML), pages 774–
782, 2013.

[19] C. Qu and H. Xu. Subspace clustering with irrelevant features via robust dantzig selector. In

Advances in Neural Information Processing Systems 28, pages 757–765. 2015.

22

[20] Q. Zheng and J. Lafferty. A convergent gradient descent algorithm for rank minimization
In Advances in Neural

and semideﬁnite programming from random linear measurements.
Information Processing Systems, pages 109–117, 2015.

[21] S. Tu, R. Boczar, M. Soltanolkotabi, and B. Recht. Low-rank solutions of linear matrix equa-

tions via procrustes ﬂow. arXiv preprint arXiv:1507.03566, 2015.

[22] Y. Chen and M. J. Wainwright. Fast low-rank estimation by projected gradient descent: General

statistical and algorithmic guarantees. arXiv preprint arXiv:1509.03025, 2015.

[23] C. D. White, R. Ward, and S. Sanghavi. The local convexity of solving quadratic equations.

arXiv preprint arXiv:1506.07868, 2015.

[24] P. Netrapalli, U. Niranjan, S. Sanghavi, A. Anandkumar, and P. Jain. Non-convex robust pca.

In Advances in Neural Information Processing Systems, pages 1107–1115, 2014.

[25] A. Anandkumar, P. Jain, Y. Shi, and U. Niranjan. Tensor vs matrix methods: Robust tensor

decomposition under block sparse perturbations. arXiv preprint arXiv:1510.04747, 2015.

[26] S. Arora, R. Ge, T. Ma, and A. Moitra. Simple, efﬁcient, and neural algorithms for sparse

coding. arXiv preprint arXiv:1503.00778, 2015.

[27] J. Sun, Q. Qu, and J. Wright. Complete dictionary recovery using nonconvex optimization. In
Proceedings of the 32nd International Conference on Machine Learning, pages 2351–2360,
2015.

[28] Y. Chen, X. Yi, and C. Caramanis. A convex formulation for mixed regression with two

components: Minimax optimal rates. In Conf. on Learning Theory, 2014.

[29] T. T. Cai, X. Li, and Z. Ma. Optimal rates of convergence for noisy sparse phase retrieval via

thresholded wirtinger ﬂow. arXiv preprint arXiv:1506.03382, 2015.

[30] K. Lee, Y. Li, M. Junge, and Y. Bresler. Blind recovery of sparse signals from subsampled

convolution. arXiv preprint arXiv:1511.06149, 2015.

[31] R. J. Tibshirani. Fast computation of the median by successive binning. arXiv preprint

arXiv:0806.3301, 2008.

[32] M. Charikar, K. Chen, and M. Farach-Colton. Finding frequent items in data streams.

Automata, languages and programming, pages 693–703. Springer, 2002.

In

[33] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. Compressed

Sensing, Theory and Applications, pages 210 – 268, 2012.

[34] J. D. Donahue. Products and quotients of random variables and their applications. Technical

report, DTIC Document, 1964.

23

