JADE for Tensor-Valued Observations

Joni Virta, Bing Li, Klaus Nordhausen and Hannu Oja

1

6
1
0
2

 
r
a

 

M
7
1

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
6
0
4
5
0

.

3
0
6
1
:
v
i
X
r
a

Abstract—Independent component analysis is a standard tool
in modern data analysis and numerous different techniques for
applying it exist. The standard methods however quickly lose
their effectiveness when the data are made up of structures
of higher order than vectors, namely matrices or tensors (for
example, images or videos), being unable to handle the high
amounts of noise. Recently, an extension of the classic fourth
order blind identiﬁcation (FOBI) speciﬁcally suited for tensor-
valued observations was proposed and showed to outperform
its vector version for tensor data. In this paper we extend
another popular independent component analysis method, the
joint approximate diagonalization of eigen-matrices (JADE), for
tensor observations. In addition to the theoretical background we
also provide the asymptotic properties of the proposed estimator
and use both simulations and real data to show its usefulness
and superiority over its competitors.

Index Terms—Independent component analysis, multilinear
algebra, kurtosis, limiting normality, minimum distance index.

I. INTRODUCTION

The following presentation relies on multilinear algebra and
before the actual ideas can be described we ﬁrst review some
key properties of tensors and matrices needed later.
A tensor of rth order X ∈ Rp1×···×pr can be seen as a higher
order analogy of vectors and matrices. Whereas a matrix can
be viewed either as a collection of rows or that of columns, a
tensor of rth order has in total r modes. The m-mode vectors
of a tensor are given by letting the mth index vary while
keeping all other indices ﬁxed, m = 1, . . . , r. A tensor X ∈
Rp1×···×pr thus contains ρm := Πr
s(cid:54)=mps m-mode vectors of
length pm. The opposite construct, ﬁxing a single index im
and varying the others, then gives what we call the m-mode
faces of a tensor. The number of m-mode faces then totals pm
and each is a tensor of size p1×···× pm−1× pm+1×···× pr.
For representing tensor contraction, or summation, we use
the Einstein summation convention in which a twice-appearing
index in a product implies summation over the range of the
index. For example, for a tensor X = {xi1i2i3} we have

p1(cid:88)

p2(cid:88)

xi1i2jxi1i2k :=

xi1i2jxi1i2k.

i1=1

i2=1

Two special cases of tensor contraction prove especially useful
for us. The product X (cid:12)m A of tensor X ∈ Rp1×···×pr with
a matrix A ∈ Rpm×pm, m = 1, . . . , r, is deﬁned as the
p1 × ··· × pr-dimensional tensor with the elements
= xi1...im−1jmim+1...ir aimjm.

(X (cid:12)m A)i1...ir

(1)

J. Virta, K. Nordhausen and H. Oja are with the Department of Math-
ematics and Statistics, University of Turku, 20014 Turku, Finland (e-mail:
joni.virta@utu.ﬁ).

B. Li is with the Department of Statistics, Pennsylvania State University,

326 Thomas Building, University Park, Pennsylvania 16802, USA.

(X (cid:12)−m Y)jk = xi1...im−1jim+1...ir yi1...im−1kim+1...ir .

That is, the multiplication X(cid:12)m A linearly transforms X from
the direction of the mth mode without changing the size of the
tensor. The operation can alternatively be viewed as applying
the linear transformation given by A separately to each m-
mode vector of the tensor. The second useful product, X(cid:12)−m
Y, of two tensors of the same size, X, Y ∈ Rp1×···×pr is
deﬁned as the pm × pm-dimensional matrix with the elements
(2)
The special case X(cid:12)−m X provides higher order counterparts
for the products of a vector x ∈ Rp1 or a matrix X ∈ Rp1×p2
with itself, such as xxT , XXT or XT X, and proves useful in
deﬁning the “covariance matrix” of a tensor.
Finally, deﬁne the vectorization vec(X) ∈ Rp1···pr of a
tensor X ∈ Rp1×···×pr as the stacking of the elements
xi1...ir in such a way that the leftmost index goes through
its cycle the quickest and the rightmost index the slowest.
Then it holds for a tensor X ∈ Rp1×···×pr and matrices
A1 ∈ Rp1×p1, . . . , Ar ∈ Rpr×pr that

vec(X (cid:12)1 A1 ··· (cid:12)r Ar) = (Ar ⊗ ··· ⊗ A1)vec(X),

where ⊗ is the Kronecker product.
In this paper we assume that the tensor-valued i.i.d. random
elements Xi ∈ Rp1×···×pr, i = 1, . . . , n, are observed from the
recently suggested [1] tensor independent component model:

X = µ + Z (cid:12)1 Ω1 ··· (cid:12)r Ωr,

(3)
where Ω1 ∈ Rp1×p1, . . . , Ωr ∈ Rpr×pr are full rank mix-
ing matrices, µ ∈ Rp1×···×pr
is the location center, and
Z ∈ Rp1×···×pr is an unobserved random tensor. The model
(3) is further equipped with the following assumptions.
Assumption 1. The components of Z are mutually indepen-
dent.
Assumption 2. The components of Z are standardized in the
sense that E[vec(Z)] = 0 and Cov[vec(Z)] = I.
Assumption 3. For each m = 1, . . . , r, at most one m-mode
face of Z consists entirely of Gaussian components.

Assumption 2 implies that E[X] = µ and that
r ) ⊗ ··· ⊗ (Ω1ΩT
1 )

Cov[vec(X)] = (ΩrΩT

has the so-called Kronecker structure. Assumption 3 is a
tensor analogy for the usual vector independent component
model assumption on maximally one Gaussian component
and without it some column blocks of some of the matrices
Ω1, . . . , Ωr could be identiﬁable only up to a rotation. After
the above assumptions we can still freely change the signs and
orders of the columns of all Ω1, . . . , Ωr, or multiply any Ωs
by a constant and divide any Ωt by the same constant, but

this indeterminacy is acceptable in practice. The model along
with its assumptions now provides a natural extension for the
standard independent component model which is obtained as
a special case when r = 1.

Alternatively, the model can be seen as an extension of the
general location-scatter model for tensor-valued data, which
is equivalent to (3) with only Assumption 2 and is often,
for r = 1, 2, combined with the assumption on Gaussianity
or sphericity of vec(Z). Under the location-scatter model the
covariance matrix of vec(X) again has the above Kronecker
structure. In addition to requiring less parameters to estimate
than a full p1 ··· pr × p1 ··· pr covariance matrix, the as-
sumption on Kronecker structure is a natural choice in many
applications, see e.g. [2]. For the estimation of covariance
parameters under the assumption on Kronecker structure in
the matrix case, r = 2, see [3], [4], [5]. For the general tensor
Gaussian distribution and the estimation of its parameters see
[6], [7].

The extension of dimension reduction methods from vector
to matrix or tensor observations is in signal processing usually
approached via different tensor decompositions such as the
CP-decomposition and the Tucker decomposition. A review
of them along with a plethora of references for applications
is given in [8], see also [9] for more applications. For exam-
ples of particular dimension reduction methods incorporating
matrix or tensor predictors, see e.g. [10], [11], [1] for indepen-
dent component analysis, [12], [13], [14], [15] for sufﬁcient
dimension reduction and [16], [17] for principal components
analysis-based techniques. More references are also given in
[12], [1].

In tensor independent component analysis the objective is
to estimate, based on the sample X1, . . . , Xn, some unmixing
matrices Φ1, . . . , Φr such that X(cid:12)1 Φ1 ···(cid:12)r Φr has mutually
independent components. A na¨ıve method for accomplishing
this would be to vectorize the observations and resort to some
standard method of independent component analysis, but in
doing so the resulting estimate lacks the desired Kronecker
structure. In addition, vectorizing and using standard tools
meant for vector-valued data requires the stronger, component-
wise version of Assumption 3, inﬂates the number of param-
eters and can make the dimension of the data too large for
standard methods to handle. To circumvent this, [10], [11], [1]
proposed estimating an unmixing matrix separately for each of
the modes and [1] presented an extension of the classic fourth
order blind identiﬁcation (FOBI) [18] for tensor observations
called TFOBI.
In the vector independent component model, x = µ + Ωz,
the standardized vector xst := Cov[x]−1/2(x − E[x]) equals
Uz for some orthogonal matrix U, see e.g. [19]. In FOBI the
rotation U is then found using the eigendecomposition of the
matrix of fourth moments B := E[xstxT
st]. This same
approach is taken in TFOBI by performing both steps of the
procedure, the standardization and the rotation, on all r modes
of X. Assuming centered X, in [1] the m-mode covariance
matrices,

stxstxT

Σm(X) := ρ−1

m E [X (cid:12)−m X] , m = 1, . . . , r,

are ﬁrst used to standardize the observations as Xst

(4)

:=

2

−1/2
r

−1/2
1

··· (cid:12)r Σ

X (cid:12)1 Σ
. The tensor Z is then found by
rotating Xst from all r modes and the rotation matrices can be
found from the eigendecompositions of the m-mode matrices
of fourth moments:
Bm := ρ−1

m E [(Xst (cid:12)−m Xst)(Xst (cid:12)−m Xst)] .

Another widely used independent component analysis
method for vector-valued data, called the joint approximate di-
agonalization of eigen-matrices (JADE) [20], also uses fourth
moments to estimate the required ﬁnal rotation but utilizes
them in the form of cumulant matrices (assuming E(x) = 0)
(5)

Cij(x) := E(cid:2)xixj · xxT(cid:3) − E[xixj]E(cid:2)xxT(cid:3)

− E [xi · x] E(cid:2)xj · xT(cid:3) − E [xj · x] E(cid:2)xi · xT(cid:3) .
(cid:3) − δijI − Eij − Eji,

Cij(xst) = E(cid:2)xst,ixst,j · xstxT

The ﬁnal rotation from xst to z is in JADE obtained by jointly
diagonalizing the matrices

(6)
where Eij is a matrix with a single one as element (i, j) and
zeroes elsewhere and δij is the Kronecker delta. Compared
to FOBI which only uses p(p + 1)/2 sums of fourth joint
moments of xst JADE thus has a clear advantage in using all
possible fourth joint cumulants of xst in the estimation of the
rotation matrix.

st

Because of the well-known fact that JADE outperforms
FOBI in most cases it is natural to expect that the extension
of JADE to tensor-valued data would similarly be superior to
TFOBI. This is indeed the case, and in the following sections
we formulate the tensor joint diagonalization of eigen-matrices
(TJADE) which is obtained from JADE by applying very
much the same extensions as required when moving from
FOBI to TFOBI. We ﬁrst brieﬂy discuss the standard vector-
valued independent component model and review the theory
and assumptions behind the original JADE in Section II. The
corresponding aspects of TJADE are presented in Section III
and the asymptotical properties of both methods in Section IV.
Simulations comparing TJADE to TFOBI and both the original
JADE and original FOBI are presented in Section V along
with a real data example and we close in Section VI with
some discussion. The proofs can be found in Appendix A.

II. ORIGINAL JADE

The original JADE assumes that the vector-valued obser-
vations are generated by the vector independent component
model

i = 1, . . . , n,

xi = µ + Ωzi,

(7)
where the mixing matrix Ω ∈ Rp×p has full rank, µ ∈ Rp
and the i.i.d. random vectors zi ∈ Rp have mutually indepen-
dent components standardized to have zero means and unit
variances. To ensure the existence of the JADE solution we
have to further assume that at most one of the independent
components has zero excess kurtosis [19].
Assuming next that the data are centered, that is, E[x] = 0,
we standardize the vectors as xst = Σ−1/2x. The standardized
vectors can be shown to satisfy xst = Uz for some orthogonal
matrix U, see for example [19]. To estimate U, JADE uses the

cumulant matrices Cij(xst), i, j = 1, . . . , p, in (6). Under the
independent component model the cumulant matrices can be
shown to satisfy, for all i, j = 1, . . . , p,

(cid:32) p(cid:88)

k=1

(cid:33)

Cij(xst) = U

uikujkκkEkk

UT ,

(8)

B. Rotation step

3

We extend the cumulant matrices by noting that the opera-
tion (cid:12)−m provides an m-mode analogy for the outer product
of vectors. By writing the random quantity xixj · xxT in (5)
with outer products either as eT
j xxT
two straightforward tensor m-mode analogies for the matrix of
fourth cumulants Cij, i, j = 1, . . . , pm, in (5) are then given
by

i xxT ej ·xxT or as xxT eieT

i (X (cid:12)−m X)ej · (X (cid:12)−m X)(cid:3)
m E(cid:2)eT
i (X∗ (cid:12)−m X∗)ej · (X (cid:12)−m X)(cid:3)
m E(cid:2)eT
i (X∗ (cid:12)−m X)ej · (X∗ (cid:12)−m X)(cid:3)
m E(cid:2)eT
i (X∗ (cid:12)−m X)ej · (X (cid:12)−m X∗)(cid:3) ,
m E(cid:2)eT
m E(cid:2)(X (cid:12)−m X)Eij(X (cid:12)−m X)(cid:3)
m E(cid:2)(X∗ (cid:12)−m X∗)Eij(X (cid:12)−m X)(cid:3)
m E(cid:2)(X∗ (cid:12)−m X)Eij(X∗ (cid:12)−m X)(cid:3)
m E(cid:2)(X∗ (cid:12)−m X)Eij(X (cid:12)−m X∗)(cid:3) ,

1,m(X) = ρ−1
Cij
− ρ−1
− ρ−1
− ρ−1

2,m(X) = ρ−1
Cij
− ρ−1
− ρ−1
− ρ−1

(11)

(12)

:= E[z4

with m = 1, . . . , r, where X∗ is an independent copy of X.
Theoretically, a third way to generalize the idea is obtained by
considering xxT ejeT
i xxT . However, that would be redundant
as the resulting set of matrices for i, j = 1, . . . , pm is the same
as with (12) and the individual matrices can be obtained by just
reversing i and j in (12). Naturally, for vector observations,
r = 1, both (11) and (12) are equivalent.
Deﬁne next for the model (3) its kurtosis tensor κ ∈
] − 3 and its m-mode
Rp1×···×pr as (κ)i1...ir
average kurtosis vector as ¯κ(m) := (¯κ(m)
pm ), where
¯κ(m)
is the average of the excess kurtoses of the random vari-
k
ables in the kth m-mode face of the tensor Z, k = 1, . . . , pm.
The following theorem then shows that (11) and (12) actually
serve in TJADE the same purpose as their vector counterpart
in JADE.
Theorem 1. If τ, U1, . . . , Ur are as deﬁned in (10), then, for
c = 1, 2 and m = 1, . . . , r, the matrices of fourth cumulants
Cij
c,m, i, j = 1, . . . , p satisfy
c,m(Xst) = τ 4 · Um
Cij

(cid:32) pm(cid:88)

, . . . , ¯κ(m)

u(m)
ik u(m)

jk ¯κ(m)

k Ekk

(cid:33)

UT
m.

i1...ir

1

k=1
According to Theorem 1, UT

m simultaneously diagonalizes
all matrices Cij
c,m(Xst), i, j = 1, . . . , pm, regardless of c,
giving two straightforward ways of estimating the m-mode
rotation Um using (9) with Cij(xst) replaced by Cij
c,m(Xst)
for the chosen value of c. However, in estimating an individual
matrix Cij
c,m(Xst) in (11) or (12) we have to estimate four
matrices in total, the last two of which are costly to estimate
because of the independent copies X∗. Using the method of
the proof of Theorem 1 one can show that, analogously to the
vector-valued case,

(cid:0)δijρmI + Eij + Eji(cid:1) ΞT

i (Xst (cid:12)−m Xst)ej · (Xst (cid:12)−m Xst)(cid:3)

m,

1,m − Ξm

m E(cid:2)eT

1,m(Xst) = Bij
Cij
1,m := ρ−1

where Bij
and Ξm := ρ−1

m E [Xst (cid:12)−m Xst] = τ 2I, which provides a

k) − 3,

where κk := E(z4
the excess kurtosis of the kth
component, and uab are the components of U. The expression
in (8) is the eigendecomposition of Cij(xst) and thus any
single matrix Cij(xst) could be used to ﬁnd U. However, to
use all the information available in the fourth joint cumulants,
JADE simultaneously (approximately) diagonalizes them all,
that is, ﬁnds UT as

p(cid:88)

p(cid:88)

i=1

j=1

UT = argmax
U: UT U=I

(cid:107)diag(UCij(xst)UT )(cid:107)2.

(9)

and

Optimization problems of type (9) are so-called joint diago-
nalization problems for which many algorithms exist, see [21]
for discussion and one particular algorithm.

In [19], a thorough analysis of the statistical properties of
JADE is given and it is shown there that the JADE estimator
is an independent component functional, that is, the resulting
components are invariant up to sign-change and permutation
under afﬁne transformations to the original data.

III. TENSOR JOINT APPROXIMATE DIAGONALIZATION OF

EIGEN-MATRICES

In formulating TJADE we assume that the data are gen-
erated by the tensor independent component model (3) and
satisfy Assumptions 1, 2 and 3. Assuming E[X] = 0, we next
go separately through the tensor analogies of the standardiza-
tion and rotation steps of the original JADE.

A. Standardization step

We take the same approach for standardization of X as in
[1], that is, use the m-mode covariance matrices, Σ1, . . . , Σr,
to standardize X simultaneously from all r modes. This gives
us the standardized tensor

Xst := X (cid:12)1 Σ

−1/2
1

··· (cid:12)r Σ−1/2

r

.

where, for the asymptotics, we assume that the standardization
−1/2
m , m = 1, . . . , r, are chosen to be symmetric,
functionals Σ
see e.g. [22]. Estimates ˆΣ1, . . . , ˆΣr of the m-mode covariance
matrices are obtained by applying (4) to the empirical distri-
bution of X. The next step towards Z is guided by Theorem
5.3.1 in [1] which states that

Xst = τ · Z (cid:12)1 U1 ··· (cid:12)r Ur,

(10)
for some orthogonal matrices U1 ∈ Rp1×p1, . . . , Ur ∈ Rpr×pr
F , where (cid:107)·(cid:107)F

and for τ = ((cid:81)m

m )r−1(cid:107)Ωr ⊗···⊗ Ω1(cid:107)1−r

i=1 p1/2
is the Frobenius norm.

natural estimator for τ 2. Similarly

2,m(Xst) = Bij
Cij
2,m := ρ−1

where Bij
and Ξm is as above.

ˆCij
1,m := ˆBij

1,m − ˆΞm

and

(cid:0)δijI + ρmEij + Eji(cid:1) ΞT

m E(cid:2)(Xst (cid:12)−m Xst)Eij(Xst (cid:12)−m Xst)(cid:3)

2,m − Ξm

m,

(cid:0)δijρmI + Eij + Eji(cid:1) ˆΞ
(cid:0)δijI + ρmEij + Eji(cid:1) ˆΞ

T
m,

(13)

Natural estimates for the previous matrices are provided by

2,m − ˆΞm

(14)

2,m := ˆBij
ˆCij

T
m,
1,m, ˆBij
where i, j = 1, . . . , pm, and the estimates ˆBij
2,m and ˆΞm
are obtained by applying the deﬁnitions of Bij
2,m and
Ξm to the empirical distribution of X, including an empirical
−1/2
standardization by ˆΣ
. Choosing then either
1
of the sets, c = 1, 2, the rotation matrix UT
m, m = 1, . . . , r is
found by simultaneous (approximate) diagonalization as

1,m, Bij

, . . . , ˆΣ

−1/2
r

pm(cid:88)

pm(cid:88)

i=1

j=1

UT

m = argmax
U: UT U=I

(cid:107)diag(UCij

c,m(Xst)UT )(cid:107)2.

(15)

m, m = 1, . . . , r, are obtained
c,m(Xst) with their esti-

The corresponding estimates ˆUT
by replacing in (15) the matrices Cij
mates ˆCij
Combining the standardization and the rotation, the ﬁnal
TJADE algorithm for a sample, Xi ∈ Rp1×···×pr, i = 1, . . . , n,
consists of the following steps.

c,m .

1) Center Xi and estimate ˆΣ1, . . . , ˆΣr.
−1/2
2) Standardize: Xi ← Xi (cid:12)1 ˆΣ
··· (cid:12)r ˆΣ
r
3) Choose c and estimate the r rotations ˆUT
1 , . . . , ˆUT

r by
diagonalizing for each m = 1, . . . , r simultaneously the
sets ˆCij

c,m, i, j = 1, . . . , pm.

−1/2
1

.

4) Rotate: Xi ← Xi (cid:12)1 ˆUT

1 ··· (cid:12)r ˆUT
r .

Using Lemma 5.1.1 from [1] the ﬁnal result can be written
−1/2
as the product Xi (cid:12)1 ˆΦ1 ··· (cid:12)r ˆΦr, where ˆΦm := ˆUT
m ,
m = 1, . . . , r, is the m-mode TJADE estimate.
Remark 1. Technically, there is no reason why we could not
use different c for estimating different rotations Um. However,
the asymptotic properties of the different approaches are in
the next section shown to be equivalent and thus the choice
of c is for large enough samples irrelevant.

ˆΣ

m

For a vector valued x ∈ Rp and a full-rank matrix
A ∈ Rp×p, (Ax)st = Uxst for some orthogonal U [22].
Unfortunately, the analogous relation in the tensor setting,

(X (cid:12)1 A1 ··· (cid:12)r Ar)st = Xst (cid:12)1 U1 ··· (cid:12)r Ur

(16)
for some orthogonal U1, . . . , Ur, holds only for orthogonal
A1, . . . , Ar. This lack of m-afﬁne equivariance of Σm(X),
m = 1, . . . , r, is discussed in [1] along with a conjecture that
in the general tensor case, r > 1, no standardization functional
exists which would lead into the property (16). In practice this
means that outside the model (3) a change (other than rotation
or reﬂection) in the coordinate system leads into different
estimated components. However, the TJADE estimator is still
Fisher consistent by Theorem 1.

4

IV. ASYMPTOTIC PROPERTIES

The asymptotical properties of JADE were considered in
[23], [19], [24] and are in [19], [24] based on the fact that
the JADE functional is afﬁne equivariant, allowing them to
consider only the case of no mixing, Ω = I. In the following
we consider the analogous case of Ω1 = I, . . . , Ωr = I
for TJADE. However, because of the lack of full afﬁne
equivariance, the results generalize only to orthogonal mixing
from all r modes.
For a tensor X ∈ Rp1×···×pr deﬁne its m-ﬂattening X(m) ∈
Rpm×ρm as the horizontal stacking of all m-mode vectors of
the tensor into a matrix in a predeﬁned order, see [25] for
a rigorous deﬁnition. If the stacking order is assumed to be
cyclical in the dimensions in the sense of [25] we have for
X∗ := X (cid:12)1 A1 ··· (cid:12)r Ar the identity

(m) = AmX(m) (Am+1 ⊗ ··· ⊗ Ar ⊗ A1 ⊗ ··· ⊗ Am−1)T .
X∗
(17)
The reason why m-ﬂattening is particularly useful for us is that
it allows us to write the m-mode product of a tensor with itself
as an ordinary matrix product, namely X(cid:12)−mX = X(m)XT
(m),
regardless of the stacking order. This, combined with the fact
c,m(X) depend on X only via the previous
that the matrices Cij
product, implies that it is sufﬁcient to derive the asymptotics
for the case r = 2 only. The results for tensors of order r > 2
are then obtained by applying the case r = 2 for each of
the m-ﬂattened matrices X(1), . . . , X(r). Similarly, even for
the case r = 2 we only need to consider the 1-mode TJADE
estimate ˆΦ1 (matrix multiplication from left) as the results
for ˆΦ2 follow by simply transposing X or, in the language of
tensors, ﬂattening X from the second mode. Interestingly, we
also have no need to specify the used set of cumulant matrices
c, as the two choices, c = 1 and c = 2, are shown to lead into
asymptotically equivalent estimators.

We next provide the asymptotic expressions for the elements
of the TJADE estimate ˆΦ1 =: ˆΦ in the case of a matrix-
valued sample Xi ∈ Rp1×p2, i = 1, . . . , n. The asymptotic
properties of ˆΦ can be shown to depend on row means of
various moments of Z, particularly on the elements of ¯κ(1)
but also on the following

(cid:0)E[z4
(cid:0)Var[z3

p2(cid:88)
p2(cid:88)

l=1

l=1

¯β(1) :=

¯ω(1) :=

1
p2

1
p2

p1l](cid:1) T ,
p1l](cid:1) T .

1l], . . . , E[z4

1l], . . . , Var[z3

Deﬁne further the covariance of two rows of kurtoses as

p2(cid:88)

l=1

ρkk(cid:48) =

1
p2

(βklβk(cid:48)l) − ¯β(1)

k

¯β(1)
k(cid:48) ,

where βkl := E[z4
for ˆΦ in Theorem 2 we need the terms

kl]. To construct an asymptotic expression

1
n

(cid:32)
(cid:32)
p2(cid:88)

1
n

l=1

p2(cid:88)
p2(cid:88)
p2(cid:88)

l=1

l=1

l(cid:48)=1
l(cid:48)(cid:54)=l

ˆskk(cid:48) :=

ˆqkk(cid:48) :=

ˆrkk(cid:48) :=

1
p2

1
p2

1
p2

n(cid:88)
n(cid:88)
(cid:32)

i=1

i=1

1
n

(cid:33)

,

zi,klzi,k(cid:48)l

(cid:0)z3
i,kl − E[z3
n(cid:88)

kl](cid:1) zi,k(cid:48)l
(cid:33)

z2
i,klzi,kl(cid:48)zi,k(cid:48)l(cid:48)

i=1

(cid:33)

,

,

the joint limiting normality of which is easy to show, assuming
the eighth moments of Z exist.
Theorem 2. Let Z1, . . . , Zn be a random sample from a distri-
bution with ﬁnite eighth moments and satisfying Assumptions
1, 2 and 4 (see below). Then there exists a sequence of TJADE
estimates such that ˆΦ →P I and
√

√

n( ˆφkk − 1) = − 1
√
2
n ˆψkk(cid:48) +

√

n ˆφkk(cid:48) =

√

n(ˆskk − 1) + oP (1),
n ˆψk(cid:48)k − dkk(cid:48)
k(cid:48) )2

k )2 + (¯κ(1)

(¯κ(1)

√

nˆskk(cid:48)

+ oP (1),

where k (cid:54)= k(cid:48), ˆψkk(cid:48) := ¯κ(1)
2)(¯κ(1)

k (ˆrkk(cid:48) + ˆqkk(cid:48)) and dkk(cid:48) := (p2 +

k − ¯κ(1)
Using the expressions of Theorem 2 the asymptotic vari-

k(cid:48) ) + (¯κ(1)

k )2.

ances of the elements of ˆΦ can now be computed.
Corollary 1. Under the assumptions of Theorem 2 the limiting
n vec( ˆΦ−I) is multivariate normal with mean
distribution of
vector 0 and the following asymptotic variances.

√

k − 1
¯β(1)
4p2

,

ASV ( ˆφkk) =

ASV ( ˆφkk(cid:48)) =

, k (cid:54)= k(cid:48),

ζk + ζk(cid:48) + (¯κ(1)

k ¯κ(1)

k(cid:48) )4 − 2¯κ(1)
k )2 + (¯κ(1)
k(cid:48) )2)2
k )2(¯κ(1)
k )2]+(¯κ(1)

p2((¯κ(1)
k −( ¯β(1)

k(cid:48) ρkk(cid:48)

k )2[¯ω(1)

where ζk := (¯κ(1)

k −2)(p2−1).
It is easily seen that the expressions in Corollary 1 revert
to the forms of Corollary 4 in [19] when r = 1, that is, we
observe just a vector x. In this case ¯κ(1) contains just the
element-wise kurtoses of the elements of z. Of the popular
ICA methods, FastICA, FOBI and JADE, it is well-known that
only for FOBI does the asymptotic behavior of ˆφkk(cid:48) depend
on components other than zk and zk(cid:48). The analogous result
holds also for TFOBI and TJADE in the sense that in TFOBI
the asymptotic behavior of ˆφ(m)
kk(cid:48) depends on the whole tensor
Z [1] and in TJADE only on the kth and k(cid:48)th m-mode faces
of Z.

The denominators in Theorem 2 imply that for the existence
of the limiting distributions we need the following assumption.
Assumption 4. For each m = 1, . . . , r, at most one of the
components of ¯κ(m) is zero.

Assumption 4 for TJADE is much less restrictive than the
assumption needed for TFOBI, for each m = 1, .., r the
components of ¯κ(m)are distinct [1], and the one needed for
vector JADE, at most one element of κ is zero [19]. More

5

speciﬁcally, in TJADE, and in tensor independent component
individual elements of Z are
analysis in general, several
allowed to be Gaussian, as long as Assumption 3 is not
violated. Conveniently located, a majority of the elements of
Z can thus be Gaussian.

The analytical comparison of TJADE and TFOBI via the
asymptotic variances involves in general case rather compli-
cated expressions and thus we resort to simulations for their
comparison in the next section.

V. SIMULATIONS AND EXAMPLES

In the following all computations were done in R 3.1.2 [26]
especially using the R-packages JADE [27], Rcpp [28], [29]
and ggplot2 [30]. For the approximate joint diagonalization,
an algorithm based on Jacobi rotations was used, see e.g [21].
Testing the algorithms in various settings showed that both
c = 1 and c = 2 yield almost identical results with respect to
the MDI-values (see below) but the former is computationally
more efﬁcient and thus the TJADE solution in the simulations
is computed with the choice c = 1.

A. Efﬁciency comparisons

We compared the separation performance of TJADE with
its nearest competitor, TFOBI, and also with regular FOBI and
JADE as applied to vectorized tensor data, called here VFOBI
and VJADE. Note that VFOBI and VJADE do not use the prior
information on the data structure and are therefore expected to
be worse than TFOBI and TJADE. The simulation setting was
the same as in [1]: we simulated n independent 3 × 4 matrix
observations with individual elements coming from a diverse
array of distributions. The excess kurtoses of the distributions
used were -1.2, -0.6, 0, 1, 2, 3, 4, 5, 6, 8, 10 and 15 and the
exact distributions used are given in Appendix A.

We generated 2000 repetitions for each of the sample sizes,
n = 1000, 2000, 4000, 8000, 16000, 32000, and for each sam-
ple the same data was mixed using three different distributions
for the elements of the 1-mode and 2-mode mixing matrices,
A and B. In the ﬁrst case the mixing matrices were random
orthogonal matrices of sizes 3 × 3 and 4 × 4 distributed
uniformly with respect to the Haar measure. In the second
and third case the elements of both matrices were generated
independently from N (0, 1) and Uniform(−1, 1) distributions,
respectively.

The mixed data were then subjected to each of the four
different methods which produced the four unmixing matrix
estimates, ˆΦV F , ( ˆΦ2,M F ⊗ ˆΦ1,M F ), ˆΦV J and ( ˆΦ2,M J ⊗
ˆΦ1,M J ). To allow comparing we took the Kronecker product
of the 2-mode and 1-mode unmixing matrices of TFOBI and
TJADE meaning that all the four previous matrices estimate
the inverse of the same matrix (B ⊗ A), up to scaling, sign-
change and permutation of its columns.

The actual comparison was done by ﬁrst computing the

minimum distance index (MDI) [31] of the estimates

D( ˆΦΩ) =

1√
p − 1

C∈C(cid:107)C ˆΦΩ − I(cid:107)F ,
inf

where ˆΦ ∈ Rp×p is the estimated unmixing matrix, Ω ∈ Rp×p
is the true mixing matrix and C is the set of all p× p matrices

having a single non-zero element in each row and column.
MDI thus measures how far away ˆΦΩ is from the set C. The
index varies from 0 to 1 with 0 indicating that the separation
worked perfectly. In our simulation we further transformed
the MDI-values as n(p − 1)MDI2 which in vector-valued
independent component analysis converges in distribution to a
random variable with ﬁnite mean and variance [31].

The mean transformed MDI-values for different sample
sizes, methods and mixing matrices are shown in Figure 1. The
lines for both FOBI and JADE are for all mixings identical
since both methods are afﬁne equivariant. For TFOBI and
TJADE the separation is best under orthogonal mixing, the
results for normal and uniform mixing being a bit worse.
But
is that none of the
other methods can really compete with TJADE in matrix
independent component analysis. Interestingly, also regular
JADE combined with vectorization is better than TFOBI.

the main implication of the plot

B. Assumption comparisons

As our second simulation study we compared the four
methods of the previous simulation via their assumptions. For
this we constructed three simulation settings of 3×3×2 tensors
with independent elements having either Gaussian (N), Laplace
(L), exponential (E), or continuous uniform (U) distributions
standardized to have zero means and unit variances. The
distributions of the tensors are shown in the following by the
two 3 × 3 × 1 faces of each setting:

U L L
U L E

U U U
U U U
N N N

U L L
U L L

N N N
N N N





L L E
E E E

N L E
N L L
E E N

L L L
L L L

E E N
N N N





Setting 1 :

Setting 2 :

Setting 3 :

It is easy to see that none of the above settings satisﬁes
the assumptions of VFOBI as all of them have at least two
identical components. Only setting 1 satiﬁes the assumption of
TFOBI on distinct kurtosis means in all modes and settings 1
and 2 satisfy the assumption on maximally one component
having zero excess kurtosis required by VJADE. All three
settings satisfy Assumption 4 on maximally one zero kurtosis
mean in each mode required by TJADE.
We simulated 2000 repetitions of all

three settings for
different sample sizes using identity mixing and the resulting
transformed MDI-values of the four methods are depicted in
Figure 2. The above reasoning about the violation of assump-
tions is clearly visible in the plots. The mean transformed
MDI-values of the different methods break one-by-one when
the setting changes from 1 to 2 to 3 leaving TJADE as the
only method able to handle all three settings. Interestingly,
VJADE failed to converge 4601 times out of the 36000 total
repetitions across all settings, the majority of failures occurring
in the third setting.

6

The plot for setting 1 further indicates that

there exist
cases where TFOBI beats VJADE, proving that, though very
efﬁcient, the JADE methodology itself is not the only factor in
the superior performance of TJADE; the tensor structure also
plays an important role.

C. Real data example

Extreme kurtosis can be shown to be associated with multi-
modal distributions and thus independent component analysis
is commonly used as preprocessing step in classiﬁcation to
obtain directions of interest. In this spirit we consider the
semeion1 data set, available in the UCI Machine Learning
Repository [32] as a classiﬁcation problem. The data consist
of 1593 binary 16 × 16 pixel images of hand-written digits.
For this example we chose only the images representing the
digits 0, 1 and 7, having respective group sizes of 161, 162
and 158. The objective is to ﬁnd a few components separating
the three digits.

Subjecting the data to TJADE gives the results depicted in
Figure 3. The left-hand side plot shows the scatter plot of the
two resulting components with the lowest kurtoses using the
individual digit images as plot markers. Clearly the two found
directions are sufﬁcient to separate all three groups of digits.
The same conclusion can be drawn from the corresponding
density estimators and rug plots on the right-hand side of
Figure 3. As a next step, some low-dimensional classiﬁcation
algorithm could be applied to the extracted components to
create a classiﬁcation rule.

VI. DISCUSSION

In this paper we proposed TJADE, an extension of the
classic JADE suited for tensor-valued observations. Based on
the same idea of diagonalizing multiple cumulant matrices as
JADE, TJADE was shown to be very effective in solving the
independent component problem. In the course of the paper
we ﬁrst reviewed the theory and the algorithm behind JADE
and then formulated TJADE analogously giving two different,
although asymptotically equivalent, ways of estimating the
needed rotations. The asymptotic behaviors of the elements
of the TJADE-estimates under orthogonal mixing were next
provided allowing theoretical comparison to other methods. Fi-
nally, simulation studies comparing TJADE to TFOBI, and the
na¨ıve approaches combining vectorization with either FOBI
or JADE showed that TJADE is superior to all the previous
competitors in tensor independent component analysis.

As the investigation of ICA methods for non-vector-valued
objects is still in an early stage, much further research is
needed. Below we outline some ideas planned to follow this
work.

As the number of matrices to jointly diagonalize in esti-
mating the m-mode rotation in TJADE grows proportional to
the square of the corresponding dimension pm, an extension
like k-JADE [33] is worth considering also for TJADE.
And as a competing alternative also a tensor version of the

1Semeion Research Center of Sciences of Communication, via Sersale
117, 00128 Rome, Italy; Tattile Via Gaetano Donizetti, 1-3-5,25030 Mairano
(Brescia), Italy.

7

Fig. 1. Plot of sample size versus the transformation n(p − 1)MDI2 under combinations of the four different methods and three different distributions for
the mixing matrices.

Fig. 2. Means of transformed MDI-values for different combinations of setting, sample-size and method. Moving from left to right, all other methods but
TJADE break down one-by-one.

FastICA algorithm [34] will be investigated. This opens many
possibilities allowing choosing both the non-linearity function
g and the norm used in the maximization problem, see [35].

APPENDIX

the matrix and moving down and right, Uniform(−√
Triangular(−√
√
√
6,
2), χ2

√
√
3),
3),
Laplace(0, 1/
1.5, χ2
1.2
and InverseGaussian(1, 1). The distributions were further
shifted to have zero means and unit variances.

3,
t10, Gamma(3,
1.2), Exp(1), χ2

6, 0), N (0, 1),
√
3, Gamma(1.2,

The

in

distributions

the ﬁrst
Section V are, starting from the upper

used

simulation
of
left corner of

The proof of Theorem 1. Consider ﬁrst the case c = 1 and the

llllllllllllllllll125250500100020004000800010002000400080001600032000NN (p-1) MDI2MIXINGOrthogonalNormalUniformMETHODlVFOBITFOBIVJADETJADESetting 1Setting 2Setting 3llllllllllllllllll1252505001000200040008000160003200064000128000256000100020004000800016000320001000200040008000160003200010002000400080001600032000NN (p-1) MDI2METHODlVFOBITFOBIVJADETJADE8

Fig. 3. Results of applying TJADE on the semeion data. The plot on the left-hand side shows the scatter plot of the two components having the lowest
kurtoses found by TJADE with the individual images as markers. The three digits clearly form three groups in the plane. The density plots along with the
rugs on the right-hand side imply the same. The lower rug corresponds to the component with the lowest kurtosis (min kurtosis 1) and the coloring of the
groups in the rugs is the same as in the scatter plot.

four terms in (11) separately ﬁxing the choice of m. Denoting
the ﬁrst term of (11) by Bij
1,m(X), then according to Lemma
5.4.1 in [1] we have

(cid:104)

Bij
1,m(Xst)

=

τ 4
ρm

UmE

(u(m)

i

(cid:105)

UT
m,

· Z(m)ZT

(m)

)T Z(m)ZT

(m)u(m)

j

i

where Z(m) is the ﬂattened matrix deﬁned in Section IV and
(u(m)
)T is the ith row of Um. Using the standard properties of
expected value and independent random variables the (k, k(cid:48))
element of the inner expectation can be shown to be for k (cid:54)=
k(cid:48) equal to u(m)
ik(cid:48) and for k = k(cid:48) equal to
jk(cid:48) + u(m)
δijρm +u(m)
ik u(m)
k +2). Using these to construct a matrix
form for the expectation we have

ik u(m)
jk (¯κ(m)

jk u(m)

1,m(Xst) = τ 4Um
Bij

u(m)
ik u(m)

jk ¯κ(m)

k Ekk

UT
m

k=1

+ τ 4δijρmI + τ 4Eij + τ 4Eji.

The second, third and fourth terms in (11) then serve to remove
the extra constant terms above. That they indeed cancel one-
by-one the ﬁnal terms can easily be shown by examining them
in the above manner using the independence of X and X∗. This
concludes the proof for c = 1 and the corresponding result for
c = 2 can be proven in precisely the same manner.

(cid:32) p(cid:88)

(cid:33)

The proof of Theorem 2. The consistency of the TJADE es-
timator is proven similarly as the consistency of the TFOBI
estimator in the proof of Theorem 5.2.1 in [1].

In the following we assume that r = 2 and we are interested
in the asymptotical behavior of the 1-mode unmixing matrix.
As discussed in Section IV, for the general case of arbitrary
r and m-mode unmixing matrix, it sufﬁces to m-ﬂatten the
−1/2
−1/2
−1/2
m , ˆΣ
tensor and replace in the following ˆΣ
with ˆΣ
2
1
−1/2
−1/2
⊗ ··· ⊗ ˆΣ
with ˆΣ
m−1 , p2 with
1
ρm and use the corresponding row mean quantities.
√
For the asymptotic expressions of the diagonal elements of
n( ˆΦ−I) it sufﬁces to use the same arguments as in the proof
of Theorem 5.2.1 in [1] and for the off-diagonal elements we
aim to use Lemma 2 from [19].

−1/2
m+1 ⊗ ··· ⊗ ˆΣ

−1/2
r

⊗ ˆΣ

√

−1/2
and ˆR = (ˆrll(cid:48)) := ˆΣ
2

But ﬁrst, deﬁne the symmetric standardization functionals
−1/2
ˆL = (ˆlkk(cid:48)) := ˆΣ
giving the
1
standardized identity-mixed observations as Xst,i = ˆL˜Zi ˆRT ,
where ˜Zi = Zi − ¯Z. We then have
√
n(ˆlkk(cid:48) − δkk(cid:48)) = −(1/2)
√

√
see [1], and as simple moment-based estimators we have both
n(ˆL − I) = OP (1) and
n( ˆR − I) = OP (1), regardless
of whether we really have r = 2 or use ﬂattened tensors of
higher order.
1,1 , k, k(cid:48) =
1, . . . , p, in (13) to be simultaneously diagonalized satisfy
ˆCkk(cid:48)
k Ekk. In the view of

Assume then ﬁrst that c = 1. The matrices ˆCkk(cid:48)

n(ˆskk(cid:48) − δkk(cid:48)) + oP (1),

1,1 (Zi) = δkk(cid:48) ¯κ(1)

1,1 →P Ckk(cid:48)

:= ˆCkk(cid:48)

−2−101−2−1012Component with the lowest kurtosisComponent with the second lowest kurtosis−3−2−10120.00.20.40.6Component valueDensitycomp_1st_lowest_kurtosiscomp_2nd_lowest_kurtosis1,1(Zi) and Ck(cid:48)k(cid:48)

Lemma 2 in [19] this means that the only matrices Crs
1,1(Zi),
r, s = 1, . . . , p having non-zero kth or k(cid:48)th diagonal ele-
ments are Ckk
1,1 (Zi), respectively, yielding the
following form for the (k, k(cid:48)), k (cid:54)= k(cid:48), element of the rotation
ˆU := ˆUT
1 estimated by (15).
√
kk(cid:48) − ¯κ(1)
n ˆCkk
k(cid:48)
k )2 + (¯κ(1)
(¯κ(1)
k(cid:48) )2

n ˆCk(cid:48)k(cid:48)
kk(cid:48)

+ oP (1),

nˆukk(cid:48) =

¯κ(1)
k

√

√

where ˆCkk
rs is the (r, s) element of ˆCkk. The above expression
then together with the (k, k(cid:48)), k (cid:54)= k(cid:48), element of the left
standardization matrix ˆL gives an asymptotic expression for
the off-diagonal elements of the estimated left TJADE matrix,
see [1]: √

√

√

n ˆφkk(cid:48) =

nˆukk(cid:48) +

nˆlkk(cid:48) + oP (1),

(18)

√

reducing the problem of ﬁnding the asymptotics of TJADE
n ˆCkk
into the task of ﬁnding the asymptotic behaviors of
kk(cid:48)
and
kk(cid:48) . Dropping the subscripts for clarity, note that
ˆCaa = ˆBaa − ˆΞ(p2I + 2Eaa) ˆΞT and starting from ˆBaa write
it out as

n ˆCk(cid:48)k(cid:48)

√

ˆBaa =

1
p2n

(ˆLT
a

˜Zi ˆR∗ ˜ZT

i

ˆLa) · ˆL˜Zi ˆR∗ ˜ZT

i

ˆLT ,

1
p2n

√

where ˆLT
off-diagonal element of
matrix multiplication the form
√
nˆr∗

a is the ath row of ˆL and ˆR∗ := ˆRT ˆR. An arbitrary
n(ˆBaa−Baa(Zi)) then has after the
√

ˆlk(cid:48)v ˆHde,gf,st,vu,

(cid:88)

nˆBaa

ˆlad

ˆlag

ˆlks

ef ˆr∗

kk(cid:48) =

tu

def gstuv

where ˆHde,gf,st,vu = (1/n)(cid:80)n

(19)
i=1 ˜zi,de ˜zi,gf ˜zi,st ˜zi,vu →P
E(zi,dezi,gf zi,stzi,vu). Next we expand the multiplicands ˆr∗
··
and ˆl·· in (19) one-by-one such as ˆlab = (ˆlab − δab) + δab,
√
the ﬁrst term of which is OP (1) when combined with
n
allowing the use of Slutsky’s theorem to the whole multiple
sum and the second term of which produces an expression like
(19) only with one summation index less.

√
Starting from left
√

√
this process then produces the terms
nˆlkk(cid:48) +
nˆlkk(cid:48) + δak(cid:48)
oP (1); oP (1); δak
√
nˆlkk(cid:48) + (1 −
nˆlk(cid:48)k + oP (1); δak(cid:48)(¯κ(1)
√
√
δak(cid:48)
nˆlk(cid:48)k + (1 −
nˆlkk(cid:48) + oP (1) and δak(¯κ(1)
√
δak(cid:48))p2
k + p2 + 2)
nˆlk(cid:48)k + oP (1) ﬁnally leaving us with the expression
δak)p2

nˆlk(cid:48)k + oP (1); δak

k(cid:48) + p2 + 2)

√

n(cid:88)

i=1

(cid:88)

et

1
p2

1√
n

n(cid:88)

i=1

˜z2
i,ae ˜zi,kt ˜zi,k(cid:48)t + oP (1).

(20)

Substituting now either a = k or a = k(cid:48), expanding ˜zi,ab =
zi,ab − ¯zab and using the quantities deﬁned in Section IV the
expression in (20) gets the forms
nˆqkk(cid:48) + oP (1)
and

nˆrkk(cid:48) +
nˆqk(cid:48)k + oP (1), respectively.

√
Using the above, e.g.

√

√

√

√

nˆBkk
kk(cid:48) gets the form
√
k +p2+2)

nˆlk(cid:48)k+

√

nˆrkk(cid:48)+

√

nˆqkk(cid:48)+oP (1).

nˆrk(cid:48)k +
√
(p2+2)

nˆlkk(cid:48)+(¯κ(1)

For the asymptotic behavior of the remaining term ˆΞ(p2I +
2Eaa) ˆΞT one can ﬁrst use techniques similar to the above
n( ˆξkk(cid:48) − δkk(cid:48)) = oP (1) for
to show for ˆΞ = ( ˆξkk(cid:48)) that

√

9

k (cid:54)= k(cid:48). Consequently an arbitrary off-diagonal element of
√
n( ˆΞ(p2I + 2Eaa) ˆΞT − p2I − 2Eaa) is also oP (1) implying
that the term actually contributes nothing to the asymptotic
variances of the estimator. Thus
kk(cid:48) + oP (1)
and the result of Theorem 2 is obtained by plugging everything
in into (18) and using the fact that the standardization func-
tionals are symmetric. The asymptotic variances of Corollary
1 are then straightforward to obtain, e.g. using the table of
covariances in the proof of Theorem 5.2.1 in [1].

n ˆCaa

kk(cid:48) =

nˆBaa

√

√

√

Although the starting expressions for c = 1 and c = 2 are
n ˆCk(cid:48)k(cid:48)
different the ﬁnal expressions for both
kk(cid:48)
actually match exactly. The corresponding proof for c = 2
is then obtained in exactly likewise manner, expanding the
terms suitably and using Slustky’s theorem, and is thus omitted
here.

kk(cid:48) and

n ˆCkk

√

ACKNOWLEDGMENTS

The work of Virta, Nordhausen and Oja was supported by
the Academy of Finland Grant 268703. The work of Li was
supported by the National Science Foundation Grant DMS-
1407537.

REFERENCES

[1] J. Virta, B. Li, K. Nordhausen, and H. Oja, “Independent component
analysis for tensor-valued data,” arXiv preprint arXiv:1602.00879, 2016.
[2] K. Werner, M. Jansson, and P. Stoica, “On estimation of covariance
matrices with kronecker product structure,” Signal Processing, IEEE
Transactions on, vol. 56, no. 2, pp. 478–491, 2008.

[3] M. S. Srivastava, T. von Rosen, and D. Von Rosen, “Models with a
kronecker product covariance structure: estimation and testing,” Mathe-
matical Methods of Statistics, vol. 17, no. 4, pp. 357–370, 2008.

[4] A. Wiesel, “Geodesic convexity and covariance estimation,” Signal
Processing, IEEE Transactions on, vol. 60, no. 12, pp. 6182–6189, 2012.
[5] Y. Sun, P. Babu, and D. P. Palomar, “Robust estimation of structured
covariance matrix for heavy-tailed elliptical distributions,” arXiv preprint
arXiv:1506.05215, 2015.

[6] M. Ohlson, M. R. Ahmad, and D. Von Rosen, “The multilinear normal
distribution: Introduction and some basic properties,” Journal of Multi-
variate Analysis, vol. 113, pp. 37–47, 2013.

[7] A. M. Manceur and P. Dutilleul, “Maximum likelihood estimation for
the tensor normal distribution: Algorithm, minimum sample size, and
empirical bias and dispersion,” Journal of Computational and Applied
Mathematics, vol. 239, pp. 37–49, 2013.

[8] T. G. Kolda and B. W. Bader, “Tensor decompositions and applications,”

SIAM review, vol. 51, no. 3, pp. 455–500, 2009.

[9] H. Lu, K. N. Plataniotis, and A. N. Venetsanopoulos, “A survey of
multilinear subspace learning for tensor data,” Pattern Recognition,
vol. 44, no. 7, pp. 1540–1551, 2011.

[10] M. A. O. Vasilescu and D. Terzopoulos, “Multilinear independent
components analysis,” in Computer Vision and Pattern Recognition,
2005. CVPR 2005. IEEE Computer Society Conference on, vol. 1.
IEEE, 2005, pp. 547–553.

[11] L. Zhang, Q. Gao, and L. Zhang, “Directional independent component
analysis with tensor representation,” in Computer Vision and Pattern
Recognition, 2008. CVPR 2008. IEEE Conference on.
IEEE, 2008, pp.
1–7.

[12] B. Li, M. K. Kim, and N. Altman, “On dimension folding of matrix-or
array-valued statistical objects,” The Annals of Statistics, pp. 1094–1121,
2010.

[13] R. M. Pfeiffer, L. Forzani, and E. Bura, “Sufﬁcient dimension reduction
for longitudinally measured predictors,” Statistics in medicine, vol. 31,
no. 22, pp. 2414–2427, 2012.

[14] Y. Xue and X. Yin, “Sufﬁcient dimension folding for regression mean
function,” Journal of Computational and Graphical Statistics, vol. 23,
no. 4, pp. 1028–1043, 2014.

[15] S. Ding and R. D. Cook, “Tensor sliced inverse regression,” Journal of

Multivariate Analysis, vol. 133, pp. 216–231, 2015.

[16] ——, “Dimension folding PCA and PFC for matrix-valued predictors,”

[26] R Core Team, R: A language and environment for statistical computing,

10

R Foundation for Statistical Computing, Vienna, Austria, 2014.

[27] J. Miettinen, K. Nordhausen, and S. Taskinen, “Blind source sepa-
ration based on joint diagonalization in R: The packages JADE and
BSSasymp,” Conditional accepted for publication in the Journal of
Statistical Software, 2015.

[28] D. Eddelbuettel, R. Franc¸ois, J. Allaire, J. Chambers, D. Bates, and
K. Ushey, “Rcpp: Seamless R and C++ integration,” Journal of Statis-
tical Software, vol. 40, no. 8, pp. 1–18, 2011.

[29] D. Eddelbuettel, Seamless R and C++ integration with Rcpp. Springer,

2013.

Statistica Sinica, vol. 24, pp. 463–492, 2014.

[17] K. Greenewald and A. Hero, “Robust kronecker product PCA for
spatio-temporal covariance estimation,” IEEE Transactions on Signal
Processing, vol. 63, no. 23, pp. 6368–6378, 2015.

[18] J.-F. Cardoso, “Source separation using higher order moments,” in
International Conference on Acoustics, Speech, and Signal Processing,
1989. ICASSP-89.

IEEE, 1989, pp. 2109–2112.

[19] J. Miettinen, S. Taskinen, K. Nordhausen, and H. Oja, “Fourth moments
and independent component analysis,” Statistical science, vol. 30, no. 3,
pp. 372–390, 2015.

[20] J.-F. Cardoso and A. Souloumiac, “Blind beamforming for non-gaussian
signals,” in IEE Proceedings F (Radar and Signal Processing), vol. 140,
no. 6.

IET, 1993, pp. 362–370.

[21] A. Belouchrani, K. Abed-Meraim, J.-F. Cardoso, and E. Moulines, “A
blind source separation technique using second-order statistics,” Signal
Processing, IEEE Transactions on, vol. 45, no. 2, pp. 434–444, 1997.
[22] P. Ilmonen, H. Oja, and R. Serﬂing, “On invariant coordinate system
(ICS) functionals,” International Statistical Review, vol. 80, no. 1, pp.
93–110, 2012.

[23] S. Bonhomme and J.-M. Robin, “Consistent noisy independent compo-
nent analysis,” Journal of Econometrics, vol. 149, no. 1, pp. 12 – 25,
2009.

[24] J. Virta, K. Nordhausen, and H. Oja, “Joint use of

third and
fourth cumulants in independent component analysis,” arXiv preprint
arXiv:1505.02613, 2015.

[25] L. De Lathauwer, B. De Moor, and J. Vandewalle, “A multilinear
singular value decomposition,” SIAM journal on Matrix Analysis and
Applications, vol. 21, no. 4, pp. 1253–1278, 2000.

[30] H. Wickham, ggplot2: elegant graphics for data analysis. Springer

New York, 2009. [Online]. Available: http://had.co.nz/ggplot2/book

[31] P. Ilmonen, K. Nordhausen, H. Oja, and E. Ollila, “A new performance
index for ICA: properties, computation and asymptotic analysis,” in
Latent Variable Analysis and Signal Separation.
Springer, 2010, pp.
229–236.

[32] M. Lichman, “UCI machine learning repository,” 2013.

[Online].

Available: http://archive.ics.uci.edu/ml

[33] J. Miettinen, K. Nordhausen, H. Oja, and S. Taskinen, “Fast equivariant
JADE,” in IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP) 2013, May 2013, pp. 6153–6157.

[34] A. Hyv¨arinen, J. Karhunen, and E. Oja, Independent Component Anal-

ysis. New York, USA: John Wiley & Sons, 2001.

[35] J. Miettinen, K. Nordhausen, H. Oja, S. Taskinen, and J. Virta,
preprint

FastICA estimator,”

symmetric

arXiv

squared

“The
arXiv:1512.05534, 2015.

