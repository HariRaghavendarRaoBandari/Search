EPJ manuscript No.
(will be inserted by the editor)

6
1
0
2

 

b
e
F
7

 

 
 
]

M

I
.

h
p
-
o
r
t
s
a
[
 
 

1
v
8
4
0
0
0

.

3
0
6
1
:
v
i
X
r
a

Non-random structures in universal compression and the Fermi
paradox

A.V. Gurzadyan1 and A.E. Allahverdyan2

1 Russian-Armenian (Slavonic) University, Emin Street 123, Yerevan 0051, Armenia
2 Yerevan Physics Institute, Alikhanian Brothers Street 2, Yerevan 0036, Armenia

Received: date / Revised version: date

Abstract. We study the hypothesis of information panspermia assigned recently among possible solutions
of the Fermi paradox (“where are the aliens?”). It suggests that the expenses of alien signaling can be
signiﬁcantly reduced, if their messages contain compressed information. To this end we consider universal
compression and decoding mechanisms (e.g. the Lempel-Ziv-Welch algorithm) that can reveal non-random
structures in compressed bit strings. The eﬃciency of Kolmogorov stochasticity parameter for detection of
non-randomness is illustrated, along with the Zipf’s law. The universality of these methods, i.e. indepen-
dence on data details, can be principal in searching for intelligent messages.

PACS. 89.70.Hj Communication complexity

Introduction

The Fermi paradox [1] outlines the puzzling silence of the universe regarding intelligent life. It is a widely discussed
topic that connects to various disciplines, from astrophysics to linguistics. The concept of information panspermia
introduced in [2] and listed in [1] as Solution 23 to the Fermi paradox, implies the transmission of terrestrial life
via compressed strings. The idea is based on the estimation [2] that Earth organisms—including humans, and the
entire terrestrial life up to bacteria—have common parts in their genomes. Hence the entire genome can be eﬃciently
compressed—the theoretical, but uncomputable, lower bound of the compression degree is given by the Kolmogorov
complexity [3,4]—and the resulting bit string can be transmitted to over Galactic distances, e.g. by Arecibo type
radio telescope. After decoding, signals can represent themselves as traveling life streams. One can imagine that the
terrestrial life is a result of such a transmitted package.

The principal diﬃculty with checking this hypothesis is that the decoding of such signals has to be based on criteria
that diﬀer from those employed in intra-terrestrial communication. For instance, the spectral (monochromaticity) and
temporal (modulations) features are likely to be not eﬃcient for extra-terrestrial messages [1]. Indeed, an eﬃcient
signal has to behave as white noise by its statistical properties and hence it cannot be distinguished from natural
signals, except in the cases when the precise code is known in advance [6]. When searching for intelligent signals
one looks for “artiﬁcial” features such as monochromaticity. The latter is however costly, especially at semi-isotropic
transmission.

Thus traveling bit strings require diﬀerent strategies for distinguishing them from known physical mechanisms and
eventually for their decoding. Our aim is to outline several strategies based on universal information compression and
decoding features.

Conjecture: Once intelligent bit string signals will be compressed, we have to look at and reveal universal features

of information compression which are absent in compressed signals originated via natural physical mechanisms.

Below we suggest schemes that possibly enable to reveal non-random structures in bit strings. We focus on universal
schemes, i.e. those applying to the large classes of situations. This is consistent with Minsky’s methodology on the
priority of universal concepts—in computer science and (more generally) mathematics—when searching for intelligent
signals [5].

1 Kolmogorov’s complexity

We start with the most fundamental notion of compression.

2

A.V. Gurzadyan, A.E. Allahverdyan: Non-random structures in universal compression and the Fermi paradox

The Kolmogorov’s complexity K(x) of a bit string x with length n is deﬁned to be the length (in bits) of the

shortest program that—starting from some ﬁxed initial state—will generate x and halt [3,4]:

K(x) = min l(x).

(1)

K(x) depends on a speciﬁc computer that executes candidate programs. This dependence is weak in the following
sense: there exists a universal computer U such that for any other computer C [3,4]:

KU (x) < KC(x) + CC,

(2)

where CC depends only on C (it does not depend on x), and CC/n → 0 for n → ∞. The validity of (2) stems from the
fact that any computer operates with ﬁnite means, hence a program to be executed on U can be represented as πCpC,
where πC is the index (coordinate) of C, while pC is a program of C. Since πC is ﬁnite, we are back to (2).

Note that the notion of the computation time is not involved in deﬁnition (1).
Thus the Kolmogorov’s complexity does provide a universal lower bound for the length of possible compressions

of x that can recover x uniquely.

Eq. (1) easily generalizes to the conditional complexity K(x|y), where during the (conditional) minimization all
programs are assumed to start with y, i.e. the latter is known. In particular, one can consider K(x|l(x)): the conditional
complexity given the knowledge of the string length.

It is important that almost all long bit strings x are incompressible or algorithmically random [3,4]:

K(x|l(x)) ≥ l(x),

(3)

i.e. their (conditional) complexity is not much shorter than their length [3,4]. This follows from the fact that there
are 2n bit strings of length n, while the amount of shorter strings (i.e. shorter programs) is much less, i.e. there are
2n−k ≪ 2n bit strings of length n − k. It thus is contradictory to assume that almost all strings of length n can be
generated by such programs. An example of an arbitrary long string that can be generated by a short program is the
n-digit rational approximation to π or to e.

On the other hand, we have

where

K(x) < K(x|l(x)) + log∗

2[l(x)] + const,

log∗

2[l(x)] ≡ log2[l(x)] + log2 log2[l(x)] + ...,

(4)

(5)

and iterated logarithms continue as far as the logarithm is well-deﬁned. Indeed, knowing l(x) we can just order the
computer to print x; the knowledge of l(x) is obligatory, since the computer should know where to halt. This knowledge
demands log2[l(x)] bits, while the knowledge of the latter string demands log2 log2[l(x)] bits etc.

However, it is impossible to prove that some single bit string of length n is not compressible, if n is large enough.
Thus the corresponding function is not computable [4,3]. To understand this point order the strings in a lexicographic
way (i.e. 0 comes before 1, 00 comes before 01 etc) and try to look for the ﬁrst string of length n whose complexity is
not smaller than n. If such a string can be found, then the very description “ﬁrst string of length n whose complexity
is not smaller than n” constitutes a rather short program for that string, hence this string is compressible to a large
extent. The resulting contradiction shows that although most of strings are incompressible, one can never show that an
individual string is incompressible, because showing that would immediately mean that the string is compressible. This
undecidability is the major hindrance for practical applications of the Kolmogorov’s complexity. Another implication
of the undecidability is that one can never compute after how many steps n a given (even a short) program is going
to halt. For, if we can compute n for any such program, we shall sequentially check all short programs and eventually
determine that certain strings are not compressible.

Non-compressible strings hold all feasible (computable) features of probability theory. The precise deﬁnition of
randomness (which is equivalent to the algorithmic randomness) was given by Martin-Lof [7]. It does improve upon
the the earlier concept of random “kollektiv” by von Mises, which does not support all features of the probability
theory (e.g. the law of the iterated logarithm) [7].

For an ergodic, ﬁnite-memory random process (X1, ..., Xn), the Kolmogorov’s complexity of almost any realization

(x1, ..., xn) converges for n → ∞ to the entropy of the random process [4]:

|H(X1, ..., Xn) − C(x1, ..., xn)| = O(log2 n),

(6)

and

H(X1, ..., Xn) ≡ − X

x1,...,xn

P (x1, ..., xn) log2 P (x1, ..., xn),

A.V. Gurzadyan, A.E. Allahverdyan: Non-random structures in universal compression and the Fermi paradox

3

where P is the probability. Note that

Eq. (6) is consistent with the (ﬁrst) Shannon’s theorem that determines H(X1, ..., Xn) to be the minimal number

of bits necessary for describing (with a negligible error) realizations of the random process [4].

H(X1, ..., Xn) = O(n),

C(x1, ..., xn) = O(n).

(7)

2 Universal coding on the example of Lempel-Ziv-Welch algorithm

Two related aspects of the Kolmogorov’s complexity is that it does not involve the computation time—hence the search
for a shorter description can take indeﬁnitely long—and that it is not computable [3,4]. It is useful for establishing
bounds [2], but it cannot be employed for quantifying the degree of compression/complexity in practice.

At this point it is useful to employ the notion of a universal, asymptotically optimal data-compression code that
is to large extent inspired by the notion of the Kolmogorov’s complexity [4]. Here asymptotically optimal means that
when applied to random, ergodic processes, the average code-length of the code approaches for long sequences the
entropy of the process, i.e. the analogue of (6) holds. And universal means that for achieving this optimal performance,
the code need not be given the frequencies of the symbols to be coded [4]. Normally, the instantaneously-decodable (i.e.
preﬁx-free) feature is also assumed within the deﬁnition of universality. Recall that preﬁx-free codes can be decoded
on-line, i.e. without knowing the whole (long) message [4]. (Otherwise, the knowledge of the space-symbol is required
for the decoder [4].)

Recall that several standard examples of optimal data-compression codes—e.g. the Huﬀman’s code or the arithmetic
code—do demand a priori knowledge of symbol frequencies, since they operate by coding more frequent symbols via
shorter code-words (the same idea is employed already in the Morse code) [4].

The most known (but not the only) example of a universal, asymptotically optimal data-compression code is the
Lempel-Ziv-Welch algorithm [28]; see [4] for a review. The algorithm and its modiﬁcations have been widely used
in practice; although newer schemes improve upon them, they provide a simple approach to understanding universal
data compression algorithms. The rough idea of this method is as follows [28,4]: the string to be coded is parsed (e.g.
by commas) into substrings that are selected according to the following criteria. After each comma one selects the
shortest substring that has not occurred before. Hence the preﬁx of this substring did occur before, and the parsed
substring can be coded via the coordinate of this occurrence and the last symbol (bit) of the substring.

The Lempel-Ziv-Welch algorithm presents an example of simple and robust scheme that is relatively easy to uncover.
For a given string the code-length of its Lempel-Ziv-Welch compression provides an estimate for the algorithmic
complexity. This Lempel-Ziv complexity [29] is also widely employed in practice, in particular in biomedical research
[30].

For our purposes it is important to stress that the Lempel-Ziv complexity can be employed for detecting non-random

structure inherent in noisy data, e.g. it can detect random permutations of words in a text [31].

3 The Kolmogorov’s stochasticity parameter

Now we will recall that non-random structures in strings can be also eﬃciently detected by means of the Kolmogorov’s
stochasticity parameter. This parameter is deﬁned for n independent, ordered, real-valued random variables X1 ≤
X2 ≤ ··· ≤ Xn. Assuming the cumulative distribution function of X,
F (x) = P{X ≤ x},

(8)

one deﬁnes an empirical distribution function Fn(x)

Fn(x) =




x < X1 ;

0 ,
k/n , Xk ≤ x < Xk+1, k = 1, 2, . . . , n − 1 ;
1 ,

Xn ≤ x .

The Kolmogorov’s stochasticity parameter λn is deﬁned as [21,22,23,24]

λn = √n sup

x |Fn(x) − F (x)| .

Kolmogorov proved [21] that for any continuous cumulative distribution function F :

lim
n→∞

P{λn ≤ λ} = Φ(λ) ,

(9)

(10)

4

A.V. Gurzadyan, A.E. Allahverdyan: Non-random structures in universal compression and the Fermi paradox

where the limit converges uniformly, and where Φ(λ) is the fourth elliptic function

Φ(λ > 0) = ϑ4(0, e−λ2

) =

+∞

X

k=−∞

(−1)k e−2k2λ2

,

(11)

where Φ(0) = 0. Now Φ(λ) is invariant with respect to F , hence it is universal. As a function of λ, Φ(λ) monotonously
grows from 0 to 1. It is close to zero for λ < 0.4, where Φ(0.4) = 0.0028, and close to 1 for λ > 1.8, where Φ(1.8) =
0.9969.

Eqs. (8–11) is the base for the standard Kolmgorov-Smirnov test. A simpler implementation of this test was
proposed by Arnold [23]. Let λ∗ be the empirically observed value of λn. It was shown that even for a single bit string,
Φ(λ∗) ≈ 0, e.g. λ∗ < 0.4, is a good indicator of randomness [23,24].
To illustrate the power of this method in revealing the degree of randomness of sequences, we will consider the
following example. Let us consider sequences as superposition of random and regular sequences. Namely, we will
consider one-parameter sequences deﬁned as

where xn are random sequences and

zn = αxn + (1 − α)yn,

yn =

a n mod b

b

,

(12)

(13)

are regular sequences deﬁned within the interval (0, 1), and a and b are mutually ﬁxed prime numbers, parameter α
while varying within 0 and 1 is indicating the variation in the fraction of random and regular sequences.

Thus we have zn with a cumulative distribution function

F (X) =




2αX−α2

2α(1−α) , 0 < X ≤ α
2α(1−α) , α < X ≤ 1 − α
2α(1−α) , 1 − α < X ≤ 1

1 − (1−X)2

0, X ≤ 0

X 2

1, X > 1.

(14)

We will inquire into the stochastic properties of zn vs the parameter α varying within the interval [0, 1 for ﬁxed a

and b, in order to follow the transition of the sequences from regular to random ones.

For each value of α for ﬁxed a and b, namely, a = 611, b = 2157, 100 sequences zn were generated, of 10,000
elements the each sequence. The latter then were divided into 50 subsequences and for each subsequence parameter
Φ(λn)m was obtained and hence the empirical distribution function G(Φ)m of them was determined (m varied within
1, ..., 50). In accord to Kolmogorov’s theorem, for example, for purely random sequences, the empirical distribution
should be uniform. So, we estimated χ2 of the functions G(Φ)m and G0(Φ) = Φ as an indicator for randomness.
Grouping 100 χ2 values per one value of α, we constructed mean and error values for χ2. The dependence of χ2 vs
α for given values of a and b is shown in Fig.1, revealing the informativity of Kolmogorov’s parameter approach in
quantitying the degree of non-randomness in the sequences.

The Kolmogorov’s parameter has been applied, for example, in revealing of non-random structures in genomic

sequences [25]. Here is a sample piece of the latter:

ACAGAGCT GAGT CACGT GGT GGAAT...,

(15)

where A, G, C and T stand for adenine, guanine, cytosine, and thymine, respectively. Such non-random structures
were employed for detecting mutations in human genome sequences, because mutations can be related to their regions
with locally higher randomness [25]. In particular, it was shown that only 3-base nucleotide (codon) coding enables to
distinguish somatic mutations.

4 Zipf’s law

It is natural to look for general features of communication systems, those which will presumably be present in meaning-
conveying (non-random) messages, even if it is not (yet) known how decipher this meaning [11]. One general feature
of man-created texts—that remains stable across diﬀerent languages, and to some extent can be traced out as well in
certain animal communication systems [11]—is the law discovered by Estoup [8] and independently (and much later)

A.V. Gurzadyan, A.E. Allahverdyan: Non-random structures in universal compression and the Fermi paradox

5

a=611
b=2157

0.35

0.30

0.25

0.20

0.15

0.10

0.05

0.00

-0.05

-0.1

0.0

0.2

0.4

0.6

0.8

1.0

Fig. 1. The χ

2 for the sequence zn vs the parameter α reﬂecting the variation of the ratio of random and regular ﬂuctuations.

by Zipf [9]; see [12,13,14,15,16,17,18,19] for an incomplete list of references and [10] for a review. This Zipf’s law
states that ordered frequencies

of words display a univeral dependency

f1 ≥ f2 ≥ ... ≥ fm,

fr ∝ r−1

(16)

(17)

of the frequency on the rank r, 1 ≤ r ≤ m. The law expressed by (17) gets modiﬁed for very rare words, those with
frequency O( 1
m ). This set of rare words is known as hapax legomena. However, the Zipf’s law eﬃciently generalizes to
this situation, so that a single expression diescribes well both moderate and small frequencies [26].

The main point of looking at such rank-frequency relation is that once they hold the Zipf’s law, this fact will likely

indicate on a non-random structure implicit in a message, and allow to identify this message as a text [11,20].

The precise origin of the Zipf’s law is not yet completely clear. Zipf himself believed that this origin is to be sought
in speciﬁc co-adaptation of the speaker (coder) and hearer (decoder) [9]: the former ﬁnds it economical to employ
a possibly smaller set of words for denoting possibly larger set of meanings. Hence the speaker tends to minimize
the redundancy. In contrast the hearer (decoder) would like to introduce suﬃcient redundancy, so as to minimize
communication errors. The qualitative idea by Zipf was formalized in several derivations (of certain aspects) of the
Zipf’s law [12,14,16]. However, these derivations do not explain all the relevant aspects of the law, e.g. its unique
extension to the hapax legomena domain. Such aspects are explained by the derivation of the law proposed in [26]
that deduces the law from rather general properties of the mental lexicon, the cognitive (mental) system responsible
for the organization of speech and thought [26].

The Zipf’s law gets modiﬁed in a deﬁnite way if one goes out of the realm of alphabetic languages. E.g. in a
suﬃciently long Chinese text the frequencies of characters (that are more morphemes than words) follow a modiﬁed
Zipf’s law [27]. However, these modiﬁcations concern only relatively low-frequency characters [27]. Moreover, for
suﬃciently short Chinese texts the Zipf’s law holds fully and is not distinguishable from its standard form of English
texts [27,26].

5 Conclusions

We expanded over the information panspermia hypothesis proposed in [2] and reviewed in [1]. The basic implication
of this hypothesis is that alien messages may not be rare, but they are compressed. Recognizing such messages may

6

A.V. Gurzadyan, A.E. Allahverdyan: Non-random structures in universal compression and the Fermi paradox

be challenging, since they may look like a noise. In fact, an ideally compressed message should be operationally (algo-
rithmically) random, according to the Kolmogorov’s complexity theory; see section 1 for a remainder. However, this
ideal compression is not achievable in principle, because it relates to the undecidability. Thus practically compressed
messages can still show certain ordered patterns. In this contribution we considered several methods that can detect
those patterns.

Though the ideal compression is not achievable in principle, there are several optimal and universal compression
algorithms. Here optimal means that for a long random data the algorithm compresses according to the ﬁrst Shannon’s
theorem. Following, the methodology proposed by Minsky [5], one can conjecture that some of those compression
schemes is known to aliens. Here we analyzed one of the most widespread schemes of this kind, the Lempel-Ziv-Welch
compression algorithm.

Next, we considered the Kolmogorov’s stochasticity parameter, an approach closely related to the Kolmogorov-
Smirnov test [22,23,24]. This parameter also has a substantial degree of universality and was recently shown to be very
eﬀective in detecting genetic mutations [25]. Searching for mutations can be regarded as looking for locally random
domains in DNA [25].

Finally, we turned to the Zipf’s law: (again) a universal law that folds for ordered word frequencies of (almost all)
human texts. The law does indicate on a non-random structure—though the underlying cause of this structure is not
yet clear—and it can survive after compression.

The common thread of these methods is that they are universal, i.e. independent from details of data. This is a

principal condition in searching for extra-terrestrial messages.

References

1. S. Webb, If the Universe Is Teeming with Aliens ... Where is Everybody?: Seventy-Five Solutions to the Fermi Paradox and

the Problem of Extraterrestrial Life (Springer, NY 2015).

2. V.G. Gurzadyan, Observatory, 125, 352 (2005).
3. A.N. Kolmogorov, Probl. Inform. Transfer, 1, 3 (1965).
4. T. Cover and J. Thomas, Elements of Information Theory (Wiley, New York, 1991).
5. M. Minsky, Why intelligent aliens will be intelligible, Extraterrestrials: Science and Alien Intelligence, Vol. 1 (1985).
6. S. Shostak, Progress in the Search for Extraterrestrial Life, 74, 447 (1995).
7. P. Martin-Lof, Information and Control, 9, 602, (1966).
8. J.B. Estoup, Gammes st´enographique (Institut St´enographique de France, Paris, 1916).
9. G.K. Zipf, The Psycho-Biology of Language: An Introduction to Dynamic Philology (Cambridge, MA, MIT Press, 1965).
10. H. Baayen, Word frequency distribution (Kluwer Academic Publishers, 2001).
11. L.R. Doyle, B. McCowan, S. Johnston and S.F. Hanser, Acta Astronautica, 68, 406-417 (2011).
12. B. Mandelbrot, Fractal geometry of nature (W. H. Freeman, New York, 1983).
13. R. Ferrer-i-Cancho and R. Sol´e, PNAS, 100, 788 (2003).
14. B. Corominas-Murtra, J. Fortuny and R.V. Sole, Phys. Rev. E 83, 036115 (2011).
15. D. Manin, Cognitive Science, 32, 1075 (2008).
16. M.V. Arapov and Yu.A. Shrejder, in Semiotics and Informatics, v. 10, p. 74 (Moscow, VINITI, 1978).
17. H.A. Simon, Biometrika 42, 425 (1955).
18. D.H. Zanette and M.A. Montemurro, J. Quant. Ling. 12, 29 (2005).
19. I. Kanter and D.A. Kessler, Phys. Rev. Lett. 74, 4559 (1995).
20. J. Elliott, Acta Astronautica, 78, 26-30 (2012).
21. A.N. Kolmogorov, Giorn.Ist.Ital.Attuari, 4, 83 (1933)
22. V. Arnold, Nonlinearity, 21, T109 (2008)
23. V.I. Arnold, Uspekhi Mat. Nauk, 63, 5 (2008)
24. V.I. Arnold, Trans. Moscow Math. Soc., 70, 31 (2009)
25. V.G. Gurzadyan, H. Yan, G. Vlahovic, et al, Royal Soc. Open Sci. 2, 150143 (2015).
26. A. E. Allahverdyan, W. Deng and Q. A. Wang, Phys. Rev. E 88, 062804 (2013).
27. W. Deng, A. E. Allahverdyan, Bo Li and Q. A. Wang, Eur. Phys. J. B, 87, 47 (2014).
28. J. Ziv and A. Lempel, IEEE Transactions on Information Theory, 23, 337–342 (1977).

J. Ziv and A. Lempel, IEEE Transactions on Information Theory, 24, 530–536 (1978).
T. A. Welch, Computer, 8–18 (1984).

29. A. Lempel, J. Ziv, IEEE Transactions on Information Theory, 22, 75-81 (1976)
30. M. Aboy, R. Hornero, D. Abasolo and D. Alvarez, IEEE Trans. Biomed. Eng. 53, 2282-2288 (2006).
31. D.V. Lande, A.A. Snarskii, On the role of autocorrelations in texts, arXiv:0710.0225.

