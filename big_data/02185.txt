Distributed Multi-Task Learning with Shared Representation

6
1
0
2

 
r
a

M
7

 

 
 
]

G
L
.
s
c
[
 
 

1
v
5
8
1
2
0

.

3
0
6
1
:
v
i
X
r
a

Jialei Wang

Mladen Kolar

Nathan Srebro

Department of Computer Science

Booth School of Business

Toyota Technological Institute

University of Chicago

Chicago, IL 60637

University of Chicago

Chicago, IL 60637

at Chicago

Chicago, IL 60637

Abstract

We study the problem of distributed multi-
task learning with shared representation,
where each machine aims to learn a sepa-
rate, but related, task in an unknown shared
low-dimensional subspaces, i.e. when the pre-
dictor matrix has low rank. We consider a
setting where each task is handled by a dif-
ferent machine, with samples for the task
available locally on the machine, and study
communication-eﬃcient methods for exploit-
ing the shared structure.

1

Introduction

Multi-task learning is widely used learning framework
in which similar tasks are considered jointly for the
purpose of improving performance compared to learn-
ing the tasks separately [13]. By transferring informa-
tion between related tasks it is hoped that samples will
be better utilized, leading to improved generalization
performance. Multi-task learning has been success-
fully applied, for example, in natural language under-
standing [15], speech recognition [32], remote sensing
[44], image classiﬁcation [25], spam ﬁltering [43], web
search [14], disease prediction [49], and eQTL mapping
[23] among other applications.

Here, we study multi-task learning in a distributed set-
ting, where each task is handled by a diﬀerent ma-
chine and communication between machines is expen-
sive. That is, each machine has access to data for a
diﬀerent task and needs to learn a predictor for that
task, where machines communicate with each other in
order to leverage the relationship between the tasks.
This situation lies between a homogeneous distributed
learning setting [e.g. 36], where all machines have data
from the same source distribution, and inhomogeneous

consensus problems [e.g. 30, 11, 6] where the goal is to
reach a single consensus predictor or iterate which is
the same on all machines. The main argument for this
setting is that if each machine indeed has access to dif-
ferent data (e.g. from a diﬀerent geographical region or
diﬀerent types of users), as in the consensus problems
studied by Balcan et al. [6], then we should allow a
diﬀerent predictor for each distribution, instead of in-
sisting on a single consensus predictor, while still try-
ing to leverage the relationship and similarity between
data distributions, as in classical multi-task learning.
As was recently pointed out by Wang et al. [41], allow-
ing separate predictors for each task instead of insist-
ing on a consensus predictor changes the fundamental
nature of the distributed learning problem, allows for
diﬀerent optimization methods, and necessitates a dif-
ferent analysis approach, more similar to homogeneous
distributed learning as studied by Shamir and Srebro
[36].

The success of multi-task learning relies on the relat-
edness between tasks. While Wang et al. [41] studied
tasks related through shared sparsity, here we turn to a
more general, powerful and empirically more successful
model of relatedness, where the predictors for diﬀer-
ent tasks lie in some (a-priori unknown) shared low-
dimensional subspace and so the matrix of predictors
is of low rank [3, 2, 45, 4]. In a shared sparsity model,
information from all tasks is used to learn a subset of
the input features which are then used by all tasks. In
contrast, in a shared subspace model, novel features,
which are linear functions of the input features, are
learned. The model can thus be viewed as a two-layer
neural network, with the bottom layer learned jointly
across tasks and the top layer task-speciﬁc. Being ar-
guably the most complex multi-layer network that we
can fully analyze, studying such models can also serve
as a gateway to using deeper networks for learning
shared representations.

Multi-task learning with a shared subspace is well-
studied in a centralized setting, where data for all tasks
are on the same machine, and some global centralized
procedure is used to ﬁnd a good predictor for each
task. In such a situation, nuclear norm regularization
is often used to leverage the low rank structure [e.g.
4, 2] and learning guarantees are known ([28] and see
also Section 2). With the growth of modern massive
data sets, where tasks and data often too big to handle
on a single machine, it is important to develop meth-
ods also for the distributed setting. Unfortunately,
the distributed multi-task setting is largely unexplored
and we are not aware of any prior for on distributed
multi-task learning with shared subspaces.

In this paper we focus on methods with eﬃcient com-
munication complexity (i.e. with as small as possi-
ble communication between machines), that can still
leverage most of the statistical beneﬁt of shared-
subspace multi-task learning. Although all our meth-
ods are also computationally tractable and can be im-
plemented eﬃciently, we are less concerned here with
minimizing the runtime on each machine separately,
considering communication, instead, as the main bot-
tleneck and the main resource to be minimized [8].
This is similar to the focus in distributed optimiza-
tion approaches such as ADMM [11] and DANE [37]
where optimization within each machine is taken as an
atomic step.

Contribution The main contributions of this article
are:

• Present and formalize the shared-subspace multi-
task learning [4] in the novel distributed multi-
task setting, identifying the relevant problems and
possible approaches. We analyze two baselines,
several representative ﬁrst-order distributed opti-
mization methods, with careful sample and com-
munication complexity analysis.

• We proposed and analyzed two subspace pursuit
approaches which learns the shared representa-
tion in a greedy fashion, which leverage the low-
dimensional predictive structure in a communica-
tion eﬃcient way.

• We conducted comprehensive experimental com-
parisons of the discussed approaches on both sim-
ulated and real datasets, where we demonstrated
that the proposed approaches are more communi-
cation eﬃcient than ﬁrst-order convex optimiza-
tion methods.

Table 1 summarized the approaches studied in this pa-
per, which will be discussed in detail in the following
sections.

Homogeneous, Inhomogeneous and Multi-Task
Distributed Learning. We brieﬂy review the re-
lationship between homogeneous, inhomogeneous and
multi-task learning, as recently presented by Wang
et al. [41].

A typical situation considered in the literature is one
in which data on diﬀerent machines are all drawn i.i.d
from the same source distribution.
In this setting,
tasks on diﬀerent machines are all the same, which
should be taken advantage of in optimization [37]. Fur-
thermore, as each machine has access to samples from
the source distribution it can perform computations
locally, without ever communicating with other ma-
chines. While having zero communication cost, this
approach does not compare favorably with the cen-
tralized approach, in which all data are communicated
to the central machine and used to obtain one predic-
tor, when measured in terms of statistical eﬃciency.
The goal in this setting is to obtain performance close
to that of the centralized approach, using the same
number of samples, but with low communication and
computation costs [36, 20, 48, 47, 26]. Another setting
considered in the distributed optimization literature
is that of consensus optimization. Here each machine
has data from a diﬀerent distribution and the goal is
to ﬁnd one vector of coeﬃcients that is good for all the
separate learning or optimization problems [11, 30, 6].
The diﬃculty of consensus problems is that the local
objectives might be rather diﬀerent, and, as a result,
one can obtain lower bounds on the amount of com-
munication that must be exchanged in order to reach
a joint optimum.

In this paper we suggest a novel setting that combines
aspects of the above two settings. On one hand, we
assume that each machine has a diﬀerent source dis-
tributions Dj, corresponding to a diﬀerent task, as in
consensus problems. For example, each machine serves
a diﬀerent geographical location, or each is at a dif-
ferent hospital or school with diﬀerent characteristics.
But if indeed there are diﬀerences between the source
distributions, it is natural to learn diﬀerent predictors
wj for each machine, so that wj is good for the dis-
tribution typical to that machine. In this regard, our
distributed multi-task learning problem is more simi-
lar to single-source problems, in that machines could
potentially learn on their own given enough samples
and enough time. Furthermore, availability of other
machines just makes the problem easier by allowing

Approach

Samples

Rounds

Communication Worker Comp.

Master Comp.

Local

Centralize

ProxGD

AccProxGD

ADMM

DFW

DGSP
DNSP

1

1

A2
ε2
m + r

m + r
m + r
m + r
m + r

A2

A2

A2

ǫ2 (cid:16) r
ǫ2 (cid:16) r
ǫ2 (cid:16) r
ǫ2 (cid:16) r
ǫ2 (cid:16) r

A2

A2

ε

mHA2

ep(cid:17)
ep(cid:17)
ep(cid:17) q mHA2
ep(cid:17)
ep(cid:17)

ε
mA2

mHA2

mHA2

ε

ε

−

−

ε
−

0
m + r

ep(cid:17)

A2

ǫ2 (cid:16) r

2 · p

2 · p

3 · p

2 · p

2 · p
2 · p

ERM

0

0

Nuclear Norm Minimization

Gradient Comp.

SV Shrinkage

Gradient Comp.

SV Shrinkage

ERM

SV Shrinkage

Gradient Comp.

Leading SV Comp.

ERM
ERM

Leading SV Comp.
Leading SV Comp.

Table 1: Summary of resources required by diﬀerent approaches to distributed multi-task learning with shared represen-
tations (for squared loss), in units of vector operations/communications, ignoring log-factors.

transfer between the machine, thus reducing the sam-
ple complexity and potentially runtime. The goal,
then, is to leverage as much transfer as possible, while
limiting communication and runtime. As with single-
source problems, we compare our method to the two
baselines, where we would like to be much better than
the local approach, achieving performance nearly as
good as the centralized approach, but with minimal
communication and eﬃcient runtime.

2 Setting, Formulation and Baselines

We consider a setting with m tasks, each characterized
by a source distribution Dj(X, Y ) over feature vectors
X ∈ Rp and associated labels Y , and out goal is to
ﬁnd linear predictors w1, . . . , wm ∈ Rp minimizing the
overall expected loss (risk) across tasks:

L(W ) =

1
m

mXj=1

E(Xj ,Yj)∼Dj(cid:2)ℓ(wT

j Xj, Yj)(cid:3) ,

(2.1)

where for convenience we denote W ∈ Rp×m for the
matrix with columns wi, and ℓ(·,·) is some speciﬁed
instantaneous loss function.
In the learning setting, we cannot observe L(W ) di-
rectly and only have access to i.i.d. sample {xji, yji}nj
from each distribution Dj, j = 1, . . . , m. For sim-
plicity of presentation, we will assume that nj = n,
j = 1, . . . , m, throughout the paper. We will denote
the empirical loss Ln(W ) = 1
j=1 Lnj(wj) where

i=1

Lnj(wj) =

1
n

j xji, yji)

mPm
nXi=1

ℓ(wT

is the local (per-task) empirical loss.

We consider a distributed setting, where each task is
handled on one of m separate machines, and each ma-
chine j has access only to the samples drawn from

Dj. Communication between the machines is by send-
ing real-valued vectors. Our methods work either in a
broadcast communication setting, where at each itera-
tion each machine sends a vector which is received by
all other machines, or in a master-at-the-center topol-
ogy where each machine sends a vector to the master
node, whom in turn performs some computation and
broadcasts some other vectors to all machines. Either
way, we count to total number of vectors communi-
cated.

As in standard agnostic-PAC type analysis, our goal
will be to obtain expected loss L(W ) which is not much
larger then the expected loss of some (unknown) ref-
erence predictor1 W ∗, and we will measure the excess
error over this goal. To allow obtaining such guaran-
tees we will assume:
Assumption 2.1. The loss function ℓ(·) is 1-Lipschitz
and bounded2 by 1, be twice diﬀerentiable and H-
smooth, that is

|ℓ′(a, c) − ℓ′(b, c)| ≤ H|a − b|,

∀a, b, c ∈ R.
All the data points are bounded by unit length, i.e.

||xji||2 ≤ 1,∀i, j,

and the reference predictors have bounded norm:

max

j∈[m]||w∗j||2

2 ≤ A2

for some A < ∞.
The simplest approach, which we refer to as Local, is
to learn a linear predictor on each machine indepen-
dently of other machines. This single task learning ap-
proach ignores the fact that the tasks are related and

1Despite the notation, W ∗ need not be the minimizer
of the expected loss. We can think of it as the minimizer
inside some restricted hypothesis class, though all analysis
and statements hold for any chosen reference predictor W ∗.
2This is only required for the high probability bounds.

that sharing information between them could improve
statistical performance. However, the communication
cost for this procedure is zero, and with enough sam-
ples it can still drive the excess error to zero. However,
compared to procedures discussed later, sample com-
plexity (number of samples n required to achieve small
excess error) is larger. A standard Rademacher com-
plexity argument [7] gives the following generalization
guarantee, which is an extension of Theorem 26.12 in
Shalev-Shwartz and Ben-David [33].

constraint on W . This is a hard and non-convex op-
timization task, but we can instead use the nuclear
norm (aka trace-norm) ||W||∗ as a convex surrogate
for the rank. This is because if Assumptions 2.1 and
2.3 hold, then we also have:

||W ∗||∗ ≤ √rmA.

(2.2)

With this in mind, we can deﬁne the following central-
ized predictor:

cWcentralize = arg

min

||W||∗≤√rmALn(W )

(2.3)

which achieves the improved excess error guarantee:

Proposition 2.5. (Theorem 1 in Maurer and Pon-
til [28]) Suppose Assumptions 2.1, 2.3 and 2.4 hold.
Then with probability at least 1 − δ,

=

+ 5r ln(mn) + 1

mn

!

Proposition 2.2. Suppose Assumption 2.1 holds.
Then with probability at least 1 − δ,

,

n

+r 2 ln(2m/δ)
bwj

L(cWlocal) − L(W ∗) ≤

=

2A
√n

where cWlocal

arg min||w||≤A Lnj(w).
That is, in order to ensure ǫ excess error, we need

[bw1, . . . ,bwm] with
ǫ2(cid:19)
n = O(cid:18) A2

samples from each task.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

At the other extreme, if we ignore all communication
costs, and, e.g. communicate all data to a single ma-
chine, we can signiﬁcantly leverage the shared sub-
space. To understand this, we will ﬁrst need to in-
troduce two assumptions: one about the existence of
a shared subspace (i.e. that the reference predictor is
indeed low-rank), and the other about the spread of
the data:
Assumption 2.3. rank(W ∗) ≤ r

Assumption 2.4. There is a constant ep, such that

1

1
m

mXj=1

E(Xj ,Yj)∼Dj(cid:2)Xj XT

j(cid:3)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2 ≤

.

ep

is entirely contained in a one-dimensional line. In this
case, the predictor matrix will also always be rank-
one, imposing a low-rank structure is meaningless and

Since the data is bounded, we always have 1 ≤ep ≤ p,
withep being a measure of how spread out the data is in
diﬀerent direction. A value of 1 =ep indicates the data
we can’t expect to gain from it. However, when ep is
We can think of ep as the “eﬀective dimensionality” of
the data, and hope to gain when r ≪ep.

close to p, or at least high, the data is spread in many
directions and the low-rank assumption is meaningful.

With these two assumptions in hand, we can think
of minimizing the empirical error subject to a rank

nm

L(cWcentralize) ≤L(W ∗) +r 2 ln(2/δ)
+ 2√rA r 1
epn
ǫ2 (cid:18) r
n = eO(cid:18) A2

m

+

r

ep(cid:19)(cid:19)

The sample complexity per task, up to logarithmic fac-
tors, is thus only:

requires collecting all data on a single machine,

That is, it is as if we needed to only learn r linear
predictors instead of m.

When ep ≫ m, this is a reduction by a factor of r/m.
The problem is that a naive computation ofcWcentralize
i.e. communicating O(n) = eO(cid:16) A2
ǫ2 (cid:16) r
ep(cid:17)(cid:17) samples
ing methods of approximating cWcentralized using com-

munication eﬃcient methods, or computing an alter-
nate predictor with similar statistical properties but
using much less communication.

per machine. In the next Sections, we aim at develop-

m + r

3 Distributed Convex Optimization

In this section, we study how to obtain the sharing
beneﬁt of the centralized approach using distributed
convex optimization techniques, while keeping the
communication requirements at low.

To enjoy the beneﬁt of nuclear-norm regularization
while avoid heavy communication cost of Centralize,
a ﬂexible strategy is to solve the convex objective (2.3)
via distributed optimization techniques. Let W (t) be
the solution at t-iteration for some iterative distributed

optimization algorithm for the following constrained
objective:

min

||W||∗≤√rmALn(W ).

(3.1)

By the generalization error decompsition [10],

L(W (t)) − L(W ∗) ≤2ǫ + ǫopt,

Suppose W (t) satisfying Ln(W (t)) ≤ Ln(cW ) +O(ǫopt)

with ǫopt = O(ǫ). Then W (t) will have the gener-
alization error of order O(ǫ). Therefore in order to
study the generalization performance, we will study
how the optimization error decreases as the function
of the number of iterations t.

Constrained vs Regularized Objective Note
that the constrained objective (3.1) is equivalent to the
following regularized objective with a proper choice of
λ:

W Ln(W ) + λ||W||∗.
min

(3.2)

Though they are equivalent, speciﬁc optimization al-
gorithms might sometimes be more suitable for one
particular type of objectives 3. For convenience in
the following discussion we didn’t distinguish between
these two formulations.

3.1 Distributed Proximal Gradient

Maybe the simplest distributed optimization algo-
rithm for (3.2) is the proximal gradient descent.
It
is not hard to see that computation of the gradient
∇Ln(W ) can be easily done in a distributed way as
the losses are decomposable across machines:

where

∇Ln(W ) =(cid:2)∇Ln1(w1), . . . ,∇Lnm(wm)(cid:3)
ℓ′(hwj , xjii, yji)xji.

∇Lnj(wj) =

1
nm

nXi=1

Thus each machine j needs to compute the gradient
∇Lnj(wj ) on the local dataset and send it to the mas-
ter. The master concatenates the gradient vectors to
form the gradient matrix ∇Ln(W ). Finally, the mas-
ter computes the proximal step

W (t+1) = arg min

W ||W − (W (t) − η∇Ln(W (t)))||2

F

+ λ||W||∗,

(3.3)

3e.g. ADMM for regularized objective and Frank-Wolfe
for constrained objective. Gradient descent methods can
be adopted for both, leads to proximal and projected meth-
ods, respectively.

which has the following closed form solution [12]: let
W (t) − η∇Ln(W (t)) = U ΣV T be the SVD of W (t) −
η∇Ln(W (t)), then W (t+1) = U (Σ − 0.5λI)+ V T with
(x)+ = max{0, x} applied element-wise.
The algorithm is summarized in Algorithm 4 (in Ap-
pendix), which has well established convergence rates
[5]:

Ln(W (t)) − Ln(cW ) ≤

mHA2

2t

.

To obtain ε-generalization error,

proximal gradient descent requires O(cid:16) mHA2
of communication, with a total O(cid:16) mHA2p

ε (cid:17) rounds
(cid:17) bits

communications per machine.

the distributed

ε

3.2 Distributed Accelerated Gradient

It is also possible to use Nesterov’s acceleration idea
[29] to improve the convergence of the proximal gra-

t(cid:1) to O(cid:0) 1

distributed accelerated proximal gradient descent, one

dient algorithm from O(cid:0) 1
needs O(cid:18)q mHA2
a total O(cid:18)q mHA2

t2(cid:1) [22]. Using the
ε (cid:19) rounds of communication with
· p(cid:19) bits communicated per ma-

chine to achieve ε-generalization error. The algorithm
is summarized in Algorithm 5 (in Appendix), where
the master maintains two sequences: W and Z. First,
a proximal gradient update of W is done based on Z

ε

W (t+1) = arg min

Z ||Z − (Z (t) − η∇Ln(Z (t)))||2

F

+ λ||Z||∗

(3.4)

and then Z is updated based on a combination of the
current W and the diﬀerence with previous W

Z (t+1) = W (t+1) + γt(W (t+1) − W (t)).

(3.5)

ADMM and DFW We also discuss the implemen-
tation and guarantees for two other popular optimiza-
tion methods: ADMM and Frank-Wolfe, which are
presented in the Appendix A and B.

4 Greedy Representation Learning

In this section we propose two distributed algorithms
which select the subspaces in a greedy fashion, instead
of solving the nuclear norm regularized convex pro-
gram.

Algorithm 1: DGSP: Distributed Gradient Subspace
Pursuit.

1 for t = 1, 2, . . . do
2 Workers:

3

4

5

6

7

8

9

for j = 1, 2, . . . , m do

Each worker compute the its gradient

direction ∇Lnj(w(t)

master

j ), and send it to the

end
if Receive u from the master then

Update the projection matrix U = [U u];
Solve the projected ERM problem:
vj = arg minvj Lnj (U vj);
Update w(t+1)

= U vj.

j

10

end

11 Master:

12

13

14

15

if Receive ∇Lnj (w(t)

j ) from all workers then

Concatenate the gradient vectors, and
compute the largest singular vectors:
(u, v) = SV(∇Ln(W (t)));
Send u to all workers.

end

16 end

4.1 Distributed Greedy Subspace Pursuit

Our greedy approach is inspired by the methods used
for sparse signal reconstruction [39, 34]. Under the
assumption that the optimal model W ∗ is low-rank,
say rank r, we can write W ∗ as a sum of r rank-1
matrices:

W ∗ =

rXi=1

aiuivT

i = U V T ,

where ai ∈ R, ui ∈ Rp, vi ∈ Rm, and ||ui||2 = ||vi||2 =
1. In the proposed approach, the projection matrix U
is learned in a greedy fashion. At every iteration, a
new one-dimensional subspace is identiﬁed that leads
to an improvement in the objective. This subspace is
then included into the existing projection matrix. Us-
ing the new expanded projection matrix as the current
feature representation, we reﬁt the model to obtain
the coeﬃcient vectors V .
In the distributed setting,
there is a master that gathers local gradient informa-
tion from each task. Based on this information, it
then computes the subspace to be added to the pro-
jection matrix and sends it to each machine. The key
step in the distributed greedy subspace pursuit algo-
rithm is the addition of the subspace. One possible
choice is the principle component of the gradient di-
rection; after the master collected the gradient matrix

∇Ln(W (t)), it computes the top left and right singular
vectors of ∇Ln(W (t)). Let (u, v) = SV(∇Ln(W (t)))
be the largest singular vectors of ∇Ln(W (t)). The left
singular vector u is used as a new subspace to be added
to the projection matrix U . This vector is sent to each
machine, which then concatenate it to the projection
matrix and reﬁt the model with the new representa-
tion. Algorithm 1 details the steps.

Distributed gradient subspace pursuit (DGSP), detailed
in Algorithm 1, creates subspaces that are orthogonal
to each other, as shown in the following proposition
which is proved in Appendix D:

Proposition 4.1. At every iteration of Algorithm 1,
the columns of U are orthonormal.

Both the distributed gradient subspace pursuit and the
distributed Frank-Wolfe use the leading singular vector
of the gradient matrix iteratively. Moreover, leading
singular vectors of the gradient matrix have been used
in greedy selection procedures for solving low-rank ma-
trix learning problems [35, 42]. However, DGSP utilize
the learned subspace in a very diﬀerent way: GECO
[35] re-ﬁt the low-rank matrix under a larger subspace
which is spanned by all left and right singular vectors;
while OR1MP [42] only adjust the linear combination pa-
rameters {ai}r
i=1 of the rank-1 matrices. The DGSP al-
gorithm do not restrict on the joint subspaces {uivT
i },
but focused on the low-dimensional subspace induced
the projection matrix U , and estimate the task speciﬁc
predictors V based on the learned representation.

Next, we present convergence guarantees for the dis-
tributed gradient subspace pursuit. First, note that
the smoothness of ℓ(·) implies the smoothness prop-
erty for any rank-1 update.

Proposition 4.2. Suppose Assumption 2.1 holds.
Then for any W and unit length vectors u ∈ Rp and
v ∈ Rm, we have
Ln(W + ηuvT ) ≤ Ln(W ) + uT∇Ln(W )v +

Hη2

2

.

We defer the proof in Appendix E. The following the-
orem states the number of iterations needed for the
distributed gradient subspace pursuit to ﬁnd an ε-
suboptimal solution.

Theorem 4.3. Suppose Assumption 2.1 holds. Then
the distributed gradient subspace pursuit ﬁnds W (t)
such that Ln(W (t)) ≤ Ln(W ∗) + ε when

t ≥(cid:24) 4HmA2

ε

(cid:25) .

We defer the proof in Appendix F. Theorem 4.3 tells us
that for the distributed gradient subspace pursuit re-

each iteration requires communicating p number, the

quires O(cid:16) mHA2
communication cost per machine is O(cid:16) mHA2

ε (cid:17) iterations to reach ǫ accuracy. Since
· p(cid:17). In

ε

some applications this communication cost might be
still too high and in order to improve it we will try
to reduce the number of rounds of communication.
To that end, we develop a procedure that utilizes the
second-order information to improve the convergence.
Algorithm 6 describes the Distributed Newton Sub-
space Pursuit algorithm (DNSP). Note that distributed
optimization with second-order information have been
studied recently to achieve communication eﬃciency
[37, 46].

Compared to the gradient based methods, the DNSP
algorithm uses second-order information to ﬁnd sub-
spaces to work with. At each iteration, each machine
computes the Newton direction

∆Lnj(wj ) =[∇2Lnj (wj)]−1∇Lnj (wj)

=" 1

mn

n

Xi=1

ℓ′′(w

T
j xji, yji)xji x

T

ji#−1

∇Lnj (wj),

based on the current solution and sends it to the mas-
ter. The master computes the overall Newton direction
by concatenating the Newton direction for each task

∆Ln(W ) = [∆Ln1(w1), ∆Ln2(w2), . . . , ∆Lnm(wm)]
and computes the top singular vectors of ∆Ln(W ).
The top left singular vector u is is sent back to every
machine, which is then concatenated to the current
projection matrix. Each machine re-ﬁts the predictors
using the new representation. Note that at every it-
eration a Gram-Schmidt step is performed to ensure
that the learned basis are orthonormal.

DNSP is a Newton-like method which uses second-order
information, thus its generic analysis is not immedi-
ately apparent. However empirical results in the next
section illustrate good performance of the proposed
DNSP.

5 Experiments

We ﬁrst illustrate performance of diﬀerent procedures
on simulated data. We generate data according to

yji | xji ∼ N (wT

j xji, 1)

for regression problems and

yji | xji ∼ Bernoulli(cid:16)(cid:0)1 + exp(−wT

j xji)(cid:1)−1(cid:17)

for classiﬁcation problems. We generate the low-
rank W ∗ as follows. We ﬁrst generate two matrices
A ∈ Rp×r, B ∈ Rm×r with entries sampled indepen-
dently from a standard normal distribution. Then we
extract the left and right singular vectors of ABT ,
denoted as U, V . Finally, we set W ∗ = U SV T ,
where S is a diagonal matrix with exponentially decay-
ing entries: diag(S) = [1, 1/1.5, 1/(1.5)2, . . . , 1/(1.5)r].
The feature vectors xji are sampled from a mean
zero multivariate normal with the covariance matrix
Σ = (Σab)a,b∈[p], Σab = 2−|a−b|. The regularization
parameters for all approaches were optimized to give
the best prediction performance over a held-out vali-
dation dataset. For ProxGD and AccProxGD, we initial-
ized the solution from Local. Our simulation results
are averaged over 10 independent runs.

We investigate how the performance of various pro-
cedures changes as a function of problem parameters
(n, p, m, r). We compare the following procedures: i)
Local, where each machine solves an empirical risk
minimization problem (ordinary least squares or lo-
gistic regression) .
ii) Nuclear-norm regularization:
which is a popular Centralize approach: all machines
send their data to the master, the master solves a
nuclear-norm regularized loss minimization problem.
iii) Learning with the best representation (BestRep):
which assumes the true projection matrix U is known,
and just ﬁt ordinal least squares or logistic regres-
sion model in the projected low-dimensional subspace .
Note that this is not a practical approach since in prac-
tice we do not know the best low-dimensional represen-
tations of the data. iv) Convex optimization approach
which runs distributed optimization algorithms over
the nuclear norm-regularized objective: here we imple-
mented and compared the following algorithms: dis-
tributed proximal gradient (ProxGD); distributed ac-
celerated proximal gradient, (AccProxGD); distributed
alternating direction method of multipliers (ADMM); dis-
tributed Frank-Wolfe (DFW) . v) The proposed DGSP
and DNSP approaches. The simulation results for re-
gression and classiﬁcation problems are shown in Fig-
ure 1 and 24, respectively. We plot how the excess
prediction error decreases as the number of rounds
of communications increases (Local, Centralize and
BestRep are one shot approaches thus the lines are
horizontal). From the plots, we have the following ob-
servations:

• Nuclear norm regularization boosts the prediction
performance over plain single task learning signif-
icantly, which shows clear advantage of leveraging

4For better visualization, here we omit the plot for DFW

as its performance is signiﬁcantly worse than others.

10 2

10 1

10 0

10 -1

10 -2

r
o
r
r

 

E
n
o

i

i
t
c
d
e
r
P
 
s
s
e
c
x
E

10 -3

0

0.1

0.09

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

r
o
r
r

 

E
n
o

i

i
t
c
d
e
r
P
 
s
s
e
c
x
E

0

0

Regression, (n,p,m,r) = (2000,200,200,5)

OLS
Nuclear
BestRep
ProxGD
AccProxGD
ADMM
DGSP
DNSP

5

10

15

20

25

30

Rounds of Communication

10 3

10 2

10 1

10 0

10 -1

10 -2

r
o
r
r

 

E
n
o

i

i
t
c
d
e
r
P
 
s
s
e
c
x
E

10 -3

0

Regression, (n,p,m,r) = (2000,400,50,5)

OLS
Nuclear
BestRep
ProxGD
AccProxGD
ADMM
DGSP
DNSP

5

10

15

20

25

30

Rounds of Communication

10 2

10 1

10 0

10 -1

10 -2

10 -3

r
o
r
r

 

E
n
o

i

i
t
c
d
e
r
P
 
s
s
e
c
x
E

10 -4

0

Regression, (n,p,m,r) = (10000,100,100,5)

OLS
Nuclear
BestRep
ProxGD
AccProxGD
ADMM
DGSP
DNSP

5

10

15

20

25

30

Rounds of Communication

Figure 1: Excess prediction error for multi-task regression.

Classification, (n,p,m,r) = (1000,100,50,5)

LR
Nuclear
BestRep
ProxGD
AccProxGD
ADMM
DGSP
DNSP

Classification, (n,p,m,r) = (2000,100,500,5)

LR
Nuclear
BestRep
ProxGD
AccProxGD
ADMM
DGSP
DNSP

0.06

0.05

0.04

0.03

0.02

0.01

0.07

0.06

0.05

0.04

0.03

0.02

0.01

r
o
r
r

E
 
n
o

i

i
t
c
d
e
r
P
 
s
s
e
c
x
E

Classification, (n,p,m,r) = (2000,200,200,5)

LR
Nuclear
BestRep
ProxGD
AccProxGD
ADMM
DGSP
DNSP

r
o
r
r

i

 

E
n
o
i
t
c
d
e
r
P
 
s
s
e
c
x
E

5

10

15

20

25

30

Rounds of Communication

0

0

5

10

15

20

25

30

Rounds of Communication

0

0

5

10

15

20

25

30

Rounds of Communication

Figure 2: Excess prediction error for multi-task classiﬁcation.

the shared representation in multi-task learning.
• ADMM and AccProxGD perform reasonably well , es-
pecially ADMM. One reason for the eﬀectiveness of
ADMM is that for the problem of nuclear norm reg-
ularized multi-task learning considered here, the
ADMM update solves regularized ERM problems at
every iteration. ADMM and AccProxGD clearly out-
perform ProxGD.

• ProxGD and DGSP perform similarly. DGSP usually
becomes worse as the iterations increases , while
ProxGD converges to a global optimum of the nu-
clear norm regularized objective.

• DNSP is the most communication-eﬃcient method,
and usually converges to a solution that is slightly
better compared to the optimum of the nuclear
regularization. This shows that second-order in-
formation helps a lot in reducing the communica-
tion cost.

• The DFW performs the worst in most cases, even
though DFW shares some similarity with DGSP in
learning the subspace. The empirical results sug-
gest the re-ﬁtting step in DGSP is very important.

One-shot SVD truncation A natural question to
ask is whether there exists a one-shot communication

method for the shared representation problem consid-
ered here, that still matches the performance of cen-
tralized methods. One reasonable solution is to con-
sider the following SVD truncation approach, which is
based on the following derivation: consider the follow-
ing well speciﬁed linear regression model:

yji = hxji, w∗ji + ǫji,

where ǫji is drawn from mean-zero Gaussian noise. It
is easy to verify the following equation for OLS esti-
mation:

xjixT

ǫjixji! .
bwlocal(j) = w∗j + Xi
SincecWlocal is just W ∗ plus some mean-zero Gaussian

noise, it is natural to consider the following low-rank
matrix denoising estimator:

ji!−1 Xi

min

W ||cWlocal − W||2

F

s.t.

rank(W ) = r.

where the solution is a simple SVD truncation, and
can be implemented in a one-shot way: each worker
send its Local solution to the master, which then per-
forms an SVD truncation step to maintain the top-r
components

cWsvd = UrSrV T

r , where U SV T = SVD(cWlocal),

10 2

10 1

10 0

10 -1

10 -2

r
o
r
r

 

E
n
o

i

i
t
c
d
e
r
P
 
s
s
e
c
x
E

10 -3

0

Regression, (n,p,m,r) = (500,100,50,5)

OLS
SVD
Nuclear
BestRep
ProxGD
AccProxGD
ADMM
DGSP
DNSP

5

10

15

20

25

30

Rounds of Communication

r
o
r
r

i

E
 
n
o
i
t
c
d
e
r
P
 
s
s
e
c
x
E

10 3

10 2

10 1

10 0

10 -1

10 -2

0

Regression, (n,p,m,r) = (500,100,50,10)

OLS
SVD
Nuclear
BestRep
ProxGD
AccProxGD
ADMM
DGSP
DNSP

5

10

15

20

25

30

Rounds of Communication

10 2

10 1

10 0

10 -1

10 -2

r
o
r
r

i

E
 
n
o
i
t
c
d
e
r
P
 
s
s
e
c
x
E

10 -3

0

Regression, (n,p,m,r) = (1000,100,50,5)

OLS
SVD
Nuclear
BestRep
ProxGD
AccProxGD
ADMM
DGSP
DNSP

5

10

15

20

25

30

Rounds of Communication

Figure 3: Excess prediction error for multi-task regression, with highly correlated features.

and send the resulting estimation back to each worker,
where Ur, Sr, Vr are top-r components of U, S, V .
Though this approach might work well for some simple
scenarios, but will generally fail when the features are

can output normal estimation of W ∗, the estimation

highly correlated: although the Local solution cWlocal
ji(cid:1)−1
noise (cid:0)Pi xjixT
(Pi ǫjixji) might be highly cor-

related (depend on the correlation between features),
which makes the SVD truncation estimation not re-
liable. To illustrate this, consider a more complex
simulation which follows the same setup as above set-
ting, except that now the feature vectors xji are sam-
pled from a higher correlation matrix Σ = (Σab)a,b∈[p],
Σab = 2−0.1|a−b|. The regression simulation results
are shown in Figure 3, where we see that the one-shot
SVD truncation approach does not signiﬁcantly out-
performs Local, sometimes even slightly worse.

Besides simulation, we also conducted extensive exper-
iments on real world datasets, which are presented in
Appendix H due to space limitation.

6 Conclusion

We studied the problem of distributed representa-
tion learning for multiple tasks, discussed the imple-
mentation and guarantees for distributed convex op-
timization methods, and presented two novel algo-
rithms to learn low-dimensional projection in a greedy
way, which can be communication more eﬃcient than
distributed convex optimization approaches. All ap-
proaches are extensively evaluated on simulation and
real world datasets.

Bibliography

A. Agarwal, S. Negahban, and M. J. Wainwright. Fast
global convergence of gradient methods for high-
dimensional statistical recovery. Ann. Stat., 40(5):
2452–2482, 2012.

Y. Amit, M. Fink, N. Srebro, and S. Ullman. Uncover-
ing shared structures in multiclass classiﬁcation. In
ICML, pages 17–24. ACM, 2007.

R. K. Ando and T. Zhang. A framework for learning
predictive structures from multiple tasks and un-
labeled data. J. Mach. Learn. Res., 6:1817–1853,
2005.

A. Argyriou, T. Evgeniou, and M. Pontil. Convex
multi-task feature learning. Mach. Learn., 73(3):
243–272, 2008.

F. Bach, R. Jenatton, J. Mairal, and G. Obozin-
ski. Optimization with sparsity-inducing penalties.
Found. Trends Mach. Learn., 4(1):1–106, 2011.

M.-F. Balcan, A. Blum, S. Fine, and Y. Mansour. Dis-
tributed learning, communication complexity and
privacy.
In JMLR W&CP 23: COLT 2012, vol-
ume 23, pages 26.1–26.22, 2012.

P. L. Bartlett and S. Mendelson. Rademacher and
gaussian complexities: Risk bounds and structural
results. J. Mach. Learn. Res., 3:463–482, 2002.

R. Bekkerman, M. Bilenko, and J. Langford. Scaling
up machine learning: Parallel and distributed ap-
proaches. Cambridge University Press, 2011.

A. Bellet, Y. Liang, A. B. Garakani, M.-F. Balcan,
and F. Sha. A distributed frank-wolfe algorithm for
communication-eﬃcient sparse learning.
In SDM,
pages 478–486. 2015.

O. Bousquet and L. Bottou. The tradeoﬀs of large

scale learning. In NIPS, pages 161–168, 2008.

S. P. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eck-
stein. Distributed optimization and statistical learn-
ing via the alternating direction method of multipli-
ers. Found. Trends Mach. Learn., 3(1):1–122, 2011.

J.-F. Cai, E. J. Cand`es, and Z. Shen. A singular
value thresholding algorithm for matrix completion.
SIAM Journal on Optimization, 20(4):1956–1982,
2010.

R. Caruana. Multitask learning. Mach. Learn., 28(1):

41–75, 1997.

O. Chapelle, P. Shivaswamy, S. Vadrevu, K. Wein-
berger, Y. Zhang, and B. Tseng. Multi-task learning
for boosting with application to web search ranking.
In KDD, pages 1189–1198. ACM, 2010.

R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. Natural language
processing (almost) from scratch. J. Mach. Learn.
Res., 12:2493–2537, 2011.

M. Frank and P. Wolfe. An algorithm for quadratic
programming. Naval research logistics quarterly, 3
(1-2):95–110, 1956.

B. He and X. Yuan. On the o(1/n) convergence rate of
the douglas-rachford alternating direction method.
SIAM Journal on Numerical Analysis, 50(2):700–
709, 2012.

M. Hong and Z.-Q. Luo. On the linear convergence
of the alternating direction method of multipliers.
ArXiv e-prints, arXiv:1208.3922, 2012.

M. Jaggi. Revisiting frank-wolfe: Projection-free
In ICML, pages 427–

sparse convex optimization.
435, 2013.

M. Jaggi, V. Smith, M. Tak´ac,

J. Terhorst,
I. Jordan.
S. Krishnan, T. Hofmann, and M.
Communication-eﬃcient distributed dual coordi-
nate ascent. In NIPS, pages 3068–3076, 2014.

P. Jain, P. Netrapalli, and S. Sanghavi. Low-rank ma-
trix completion using alternating minimization. In
STOC, pages 665–674. ACM, 2013.

S. Ji and J. Ye. An accelerated gradient method for
trace norm minimization. In ICML, pages 457–464.
ACM, 2009.

S. Kim and E. P. Xing. Tree-guided group lasso for
In

multi-task regression with structured sparsity.
ICML, pages 543–550, 2010.

S. Lacoste-Julien and M. Jaggi. On the global linear
convergence of frank-wolfe optimization variants. In
NIPS, pages 496–504, 2015.

M. Lapin, B. Schiele, and M. Hein. Scalable multitask
In

representation learning for scene classiﬁcation.
CVPR, pages 1434–1441, 2014.

J. D. Lee, Y. Sun, Q. Liu, and J. E. Taylor.
Communication-eﬃcient sparse regression: a one-
shot approach. ArXiv e-prints, arXiv:1503.04337,
2015.

P. J. Lenk, W. S. DeSarbo, P. E. Green, and M. R.
Young. Hierarchical bayes conjoint analysis: Recov-
ery of partworth heterogeneity from reduced exper-
imental designs. Marketing Science, 15(2):173–191,
1996.

A. Maurer and M. Pontil. Excess risk bounds for mul-
titask learning with trace norm regularization. pages
55–76, 2013.

Y. Nesterov. A method of solving a convex program-
ming problem with convergence rate ≀(1/k2). In So-
viet Mathematics Doklady, volume 27, pages 372–
376, 1983.

K. Weinberger, A. Dasgupta, J. Langford, A. Smola,
and J. Attenberg. Feature hashing for large scale
multitask learning.
In ICML, pages 1113–1120.
ACM, 2009.

Y. Xue, X. Liao, L. Carin, and B. Krishnapuram.
Multi-task learning for classiﬁcation with dirichlet
process priors. J. Mach. Learn. Res., 8:35–63, 2007.

M. Yuan, A. Ekici, Z. Lu, and R. Monteiro. Dimension
reduction and coeﬃcient estimation in multivariate
linear regression. J. R. Stat. Soc. B, 69(3):329–346,
2007.

Y. Zhang and L. Xiao. Communication-eﬃcient dis-
tributed optimization of self-concordant empirical
loss. ArXiv e-prints, arXiv:1501.00263, 2015.

Y. Zhang, M. J. Wainwright, and J. C. Duchi.
Communication-eﬃcient algorithms for statistical
optimization. In NIPS, pages 1502–1510, 2012.

Y. Zhang, J. C. Duchi, M. I. Jordan, and M. J. Wain-
wright. Information-theoretic lower bounds for dis-
tributed statistical estimation with communication
constraints. In NIPS, pages 2328–2336, 2013.

J. Zhou, J. Liu, V. A. Narayan, and J. Ye. Modeling
disease progression via multi-task learning. Neu-
roImage, 78:233 – 248, 2013.

S. S. Ram, A. Nedi´c, and V. V. Veeravalli. Distributed
stochastic subgradient projection algorithms for
convex optimization. Journal of optimization the-
ory and applications, 147(3):516–545, 2010.

C. Sander and R. Schneider. Database of homology-
derived protein structures and the structural mean-
ing of sequence alignment. Proteins: Structure,
Function, and Bioinformatics, pages 56–68, 1991.

M. L. Seltzer and J. Droppo. Multi-task learning in
deep neural networks for improved phoneme recog-
nition. In ICASSP, pages 6965–6969. IEEE, 2013.

S. Shalev-Shwartz and S. Ben-David. Understanding
machine learning: From theory to algorithms. Cam-
bridge University Press, 2014.

S. Shalev-Shwartz, N. Srebro, and T. Zhang. Trading
accuracy for sparsity in optimization problems with
sparsity constraints. SIAM Journal on Optimiza-
tion, 20(6):2807–2832, 2010.

S. Shalev-Shwartz, A. Gonen, and O. Shamir. Large-
scale convex minimization with a low-rank con-
straint. In ICML, 2011.

O. Shamir and N. Srebro. Distributed stochastic opti-
mization and learning. In Allerton, pages 850–857.
IEEE, 2014.

O. Shamir, N. Srebro, and T. Zhang. Communication
eﬃcient distributed optimization using an approxi-
mate newton-type method. In ICML, pages 1000–
1008, 2014.

E. Spyromitros-Xiouﬁs, G. Tsoumakas, W. Groves,
and I. Vlahavas. Multi-target regression via input
space expansion: Treating targets as inputs. ArXiv
e-prints, arXiv:1211.6581, 2012.

J. A. Tropp. Greed is good: Algorithmic results for
IEEEit, 50(10):2231–2242,

sparse approximation.
2004.

D. Turnbull, L. Barrington, D. Torres, and G. Lanck-
riet. Semantic annotation and retrieval of music
and sound eﬀects. IEEE Transactions on Acoustics,
Speech and Signal Processing, 16(2):467–476, 2008.

J. Wang, M. Kolar, and N. Srebro. Distributed mul-
titask learning. ArXiv e-prints, arXiv:1510.00633,
2015a.

Z. Wang, M.-J. Lai, Z. Lu, W. Fan, H. Davulcu, and
J. Ye. Orthogonal rank-one matrix pursuit for low
rank matrix completion. SIAM Journal on Scientiﬁc
Computing, 37(1):A488–A514, 2015b.

Appendix

B Distributed Frank-Wolfe Method

A Distributed Alternating Direction

Methods of Multipliers

The Alternating Direction Methods of Multipliers
(ADMM) is also a popular method for distributed opti-
mization (11) and can be used to solve the distributed
low-rank multi-task learning problem. We ﬁrst write
the objective (2.3) as

Another approach we consider is the distributed
Frank-Wolfe method (16, 19, 9). This methods does
not require performing SVD, which might bring addi-
tional computational advantages. Instead of directly
minimizing the nuclear norm regularized objective, the
Frank-Wolfe algorithm considers the equivalent con-
strained minimization problem

W Ln(W )
min

subject to

||W||∗ ≤ R.

arg min

W,Z Ln(W ) + λ||Z||∗,

subject to W = Z.

At each step, Frank-Wolfe algorithm considers the fol-
lowing direction to update

By introducing the Lagrangian and augmented terms,
we get the following unconstrained problem:

Z (t) = arg min

||Z||∗≤Rh∇Ln(W (t)), Zi = −R · uvT ,

eL(W, Z, Q) =Ln(W ) + λ||Z||∗ + hW − Z, Qi

ρ
2||W − Z||2
F ,

+

where ρ is a parameter controlling the augmentation
level. Note that except for Z, the augmented La-
grangian objective are decomposable across tasks. To
implement the distributed ADMM algorithm, we let
the workers maintain the data and W , while the mas-
ter maintains Z and Q. At round t, each machine
separately solves

where (u, v) = SV(∇Ln(W (t))) is the leading singular
vectors of ∇Ln(W (t)). The next iterate is obtained as

W (t+1) = (1 − γ)W (t) + γZ (t),

where γ is a step size parameter. To implement this
algorithm in a distributed way, the master ﬁrst collects
the gradient matrix ∇Ln(W (t)) and computes u and v.
The vector vj u is sent to j-th machine, which performs
the following update:

w(t+1)

j

= (1 − γ)w(t)

j − γRvju.

(B.1)

w(t+1)

j

= arg min

w Lnj(wj ) + hw(t+1)
− z(t)

2||w(t+1)

+

ρ

j

j

− z(t)

j , q(t)
j i
(A.1)

j ||2,

which is minimizing the local loss plus a regularization
term. Next, each worker sends their solution to the
master, which performs the following updates for Z
and Q

Z (t+1) = arg min

Z hW (t+1) − Z, Qti + λ||Z||∗
ρ
2||W (t+1) − Z||2
F ,

+

(A.2)

The algorithm is summarized in Algorithm 3. Sim-
ilar to the distributed (accelerated) proximal gra-
dient descent, the distributed Frank-Wolfe only re-
quires communication of two p-dimensional vectors per
round. Though computationally cheaper compared
to other methods considered in this section, the dis-
tributed Frank-Wolfe algorithm enjoys similar conver-
gence guarantees to the distributed proximal gradient

descent (19), that is, after O(cid:16) mHA2

solution will be ε suboptimal.

ε (cid:17) iterations, the

C Pseudocode of the algorithms

Q(t+1) =Q(t) + ρ(W (t+1) − Z (t+1)),

(A.3)

D Proof of Proposition 4.1

which have closed-form solutions.

The algorithm ADMM is summarized in Algorithm 2.
Note that compared to methods discussed before, ADMM
needs to communicate three p-dimensional vectors be-
tween each worker and the master at each round, while
the proximal gradient approaches only communicate
two p-dimensional vectors per round. Based on con-

vergence results of ADMM (17), O(cid:16) mA2

communication are needed to obtain ε-generalization
error.

ε (cid:17) rounds of

Proof. It is suﬃcient to prove that at every iteration,
the current projection matrix U and the subspace to
be added u are orthogonal to each other. Note that
by the optimality condition:

∇V (cid:0)Ln(U V T )(cid:1) = U T∇Ln(W (t)) = 0.

the leading left

Since u is
singular vector of
∇Ln(W (t)), we have U T u = 0. Each column of U has
unit length, since it is a left singular vector of some
matrix.

E Proof of Proposition 4.2

G An auxiliary lemma

Proof. It is suﬃcient to prove that the largest eigen-
value of ∇2Ln(W ) does not exceed H. Since ∇2Ln(W )
is a block diagonal matrix, it is suﬃcient to show that
for every block j ∈ [m], the largest eigenvalue of the
block ∇2Lnj(wj) is not larger than H.
This is true by the H-smoothness of ℓ(·) and the fact
that the data points have bounded length:

||∇2Lnj(wj)||2 ≤ H · max

i,j

||xji||2 ≤ H.

F Proof of Theorem 4.3

Proof. By the smoothness of Ln, we know
Ln(W (t+1)) ≤ min

b Ln(W (t) + buvT )

≤Ln(W (t)) + bhuvT ,∇Ln(W (t))i +
≤Ln(W (t)) +
+

bhW ∗,∇Ln(W (t))i

||W ∗||F

Hb2

2
Hb2

2

.

(F.1)

Since V
of Ln(U V T ) with respect
U T∇Ln(W (t))

Let W (t) = U V T .
is a mini-
mizer
to V , we
have
therefore
hW (t),∇Ln(W (t))i = trace(V U T∇Ln(W (t))) = 0.
From convexity of Ln(·), we have

and

=

0

hW ∗,∇Ln(W (t))i =hW ∗ − W (t),∇Ln(W (t))i

≤Ln(W ∗) − Ln(W (t)).

Combining with the display above

b(Ln(W (t)) − Ln(W ∗))

Ln(W (t)) − Ln(W (t+1)) ≥

By choosing

Hb2

2

−

||W ∗||F
.

Lemma G.1. (Lemma B.2 of Shalev-Shwartz et al.
(34)) Let x > 0 and let ε0, ε1, ... be a sequence such
that ε ≤ εt − rε2
t for all t. Let ε be a positive scalar
and t be a positive integer such that t ≥ ⌈ 1
xε⌉. Then
εt ≤ ε.

H Evaluation on Real World Datasets

We also evaluate discussed algorithms on several real
world data sets, with 20% of the whole dataset as
training set, 20% as held-out validation, then report
the testing performance on the remaining 60%. For
the real data, we have observed that adding ℓ2 regu-
larization usually helps improving the generalization
performance. For the Local procedure we added an
ℓ2 regularization term (leads to ridge regression or ℓ2
regularized logistic regression). For DGSP and DNSP, we
also add an ℓ2 regularization in ﬁnding the subspaces
and reﬁtting . We have worked on the following multi-
task learning datasets:

School.5 The dataset consists of examination scores
of students from London’s secondary schools during
the years 1985, 1986, 1987. There are 27 school-
speciﬁc and student-speciﬁc features to describe each
student.
The instances are divided by diﬀerent
schools, and the task is to predict the students’ perfor-
mance. We only considered schools with at least 100
records, which results in 72 tasks in total. The max-
imum number of records for each individual school is
260.

Computer Survey. The data is taken from a con-
joint analysis experiment (27) which surveyed 180 per-
sons about the probability of purchasing 20 kinds of
personal computers. There are 14 variables for each
computer, the response is an integer rating with scale
0 − 10.
ATP.6 The task here is to predict the airline ticket
price (38). We are interested in the minimum prices
next day for some speciﬁc observation date and depar-
ture date pairs. Each case is described by 411 features,
and there are 6 target minimum prices for diﬀerent air-
lines to predict. The sample size is 337.

b = Ln(W (t)) − Ln(W ∗)

H||W ∗||F

we have

Ln(W (t)) − Ln(W (t+1)) ≥(cid:0)Ln(W (t)) − Ln(W ∗)(cid:1)2
≥(cid:0)Ln(W (t)) − Ln(W ∗)(cid:1)2

2H||W ∗||2

2mHA2

F

Using Lemma G.1 in Appendix we know that after

t ≥(cid:24) 2mHA2

ε

(cid:25)

iterations, we have Ln(W (t)) ≤ Ln(W ∗) + ε.

.

Protein. Given the amino acid sequence, we are inter-
ested predicting the protein secondary structure (31).
We tackle the problem by considering the following
three binary classiﬁcation tasks: coil vs helix, helix vs

5http://cvn.ecp.fr/personnel/andreas/code/mtl/index.html
6http://mulan.sourceforge.net/datasets.html

8

7

6

5

4

3

2

 

E
S
M
R
d
e
g
a
r
e
v
A

Regression, Computer Survey(20,14,190)

OLS
Nuclear
AltMin
ProxGD
AccProxGD
ADMM
DGSP
DNSP

0

5

10

15

20

25

30

Rounds of Communication

0.26

0.24

0.22

0.2

0.18

0.16

C
U
A
 
d
e
g
a
r
e
v
A

 
-
 
1

0.14

0

Classification, Protein(13701,357,3)

LR
Nuclear
AltMin
ProxGD
AccProxGD
ADMM
DGSP
DNSP

5

10

15

20

25

30

Rounds of Communication

E
S
M
R
 
d
e
g
a
r
e
v
A

15.5

15

14.5

14

13.5

13

12.5

12

11.5

11

10.5

0

0.6

0.55

0.5

0.45

0.4

0.35

0.3

0.25

 

C
U
A
d
e
g
a
r
e
v
A

 
-
 
1

0.2

0

Regression, School(260,27,72)

OLS
Nuclear
AltMin
ProxGD
AccProxGD
ADMM
DGSP
DNSP

5

10

15

20

25

30

Rounds of Communication

Classification, Landmine(690,10,19)

LR
Nuclear
AltMin
ProxGD
AccProxGD
ADMM
DGSP
DNSP

5

10

15

20

25

30

Rounds of Communication

220

200

180

160

140

120

100

 

E
S
M
R
d
e
g
a
r
e
v
A

80

60

0

0.55

0.5

0.45

0.4

 

C
U
A
d
e
g
a
r
e
v
A

 
-
 
1

0.35

0

Figure 4: Prediction Error on real data.

Regression, ATP(337,411,6)

OLS
Nuclear
AltMin
ProxGD
AccProxGD
ADMM
DGSP
DNSP

5

10

15

20

25

30

Rounds of Communication

Classification, Cal500(502,68,78)

LR
Nuclear
AltMin
ProxGD
AccProxGD
ADMM
DGSP
DNSP

5

10

15

20

25

30

Rounds of Communication

Algorithm 2: ADMM: Distributed ADMM for Multi-
Task Learning.

1 for t = 1, 2, . . . do
2 Workers:

3

4

5

6

7

for j = 1, 2, . . . , m do

Each worker solves the regularized ERM

(t+1)
j

, and send it to

problem as (A.1) to get w
the master;
Wait;
Receive z(t+1)

, q(t+1)

j

j

from master.

end

8 Master:

9

10

11

12

13

if Receive w(t+1)

j

from all workers then

Concatenate the current solutions w(t+1)
update Z (t+1) as (A.2);
Update Q(t+1) as (A.3);
Send z(t+1)
j
worker.

to the corresponding

, q(t+1)

j

j

, and

end

14 end

strand, strand vs coil. Each sequence is described by
357 features. There are 24,387 instances in total.

Landmine. The data is collected from 19 landmine
detection tasks (44). Each landmine ﬁeld is repre-
sented by a 9-dimensional vector extracted from radar
images, containing moment-based, correlation-based,
energy ratio, and spatial variance features. The sam-
ple size for each task varies from 445 to 690.

Cal500.7 This music dataset (40) consists of 502
songs, where for each song 68 features are extracted.
Each task is to predict whether a particular musically
relevant semantic keyword should be an annotation for
the song. We only consider tags with at least 50 times
apperance, which results in 78 prediction tasks.

We compared various approaches as in the simulation
study, except the BestRep as the best low-dimensional
representation is unknown. We also compared with
AltMin, which learns low-rank prediction matrix us-
ing the alternating minimization (21). The results are
shown in Figure 4. Since the labels for the real world
classiﬁcation datasets are often unbalanced, we report
averaged area under the curve (AUC) instead of classi-
ﬁcation accuracy. We have the following observations:

• The distributed ﬁrst-order approaches converge
much slower than in simulations, especially on

7http://eceweb.ucsd.edu/~gert/calab/

Algorithm 3: DFW: Distributed Frank-Wolfe for
Multi-Task Learning.

1 for t = 0, 2, . . . do
2 Workers:

3

4

5

6

7

8

9

for j = 1, 2, . . . , m do

Each worker compute the its gradient

direction ∇Lnj(w(t)

master;

j ), and send it to the

end
if Receive vj u from the master then

Set γ = 2
Update w(t+1)

t+2 ;

j

as (B.1).

end

10 Master:

11

12

13

14

if Receive ∇Lnj (w(t)

j ) from all workers then

Concatenate the gradient vectors, and
compute the largest singular vectors:
(u, v) = SV(∇Ln(W (t)));
Send vj u to j-th worker.

end

15 end

13 end

ATP and Cal500. We suspect this is because in
the simulation study, the generated data are usu-
ally well conditioned, which makes faster conver-
gence possible for such methods (1, 18). On real
data, the condition number can be much worse.

• In most case, DNSP is the best in terms of
communication-eﬃciency. DGSP also has reason-
able performance with fewer round of commu-
nications compared to distributed ﬁrst-order ap-
proaches.

• Among the ﬁrst-order distributed convex opti-
mization methods, AccProxGD is overall the most
communication-eﬃcient, while DFW is the worst,
though it might have some advantages in terms
of computation. Also, we observed signiﬁcant zig-
zag behavior of the DFW algorithm, as discussed in
(24).

I Full experimental results with

Distributed Frank-Wolfe

3

4

5

6

7

3

4

5

6

7

Algorithm 4: ProxGD: Distributed Proximal Gradi-
ent.

1 for t = 1, 2, . . . do
2 Workers:

for j = 1, 2, . . . , m do

Each worker compute the its gradient
direction

∇Lnj(w(t)

j ) = 1

i=1 ℓ′(hw(t)

j

, xjii, yji)xji,

mnPn

and send it to the master;
Wait;
Receive w(t+1)

from master.

j

end

8 Master:

9

10

11

12

j ) from all workers then

if Receive ∇Lnj (w(t)
Concatenate the gradient vectors, and update
W (t+1) as (3.3);
Send w(t+1)

to all workers.

j

end

Algorithm 5: AccProxGD: Accelerated Distributed
Proximal Gradient for Multi-Task Learning.

1 for t = 1, 2, . . . do
2 Workers:

for j = 1, 2, . . . , m do

Each worker compute the its gradient
direction

∇Ln(z(t)

j ) = 1

i=1 ℓ′(hz(t)

j , xjii, yji)xji,

mnPn

and send it to the master;
Wait;
Receive z(t+1)

from master.

j

end

8 Master:

9

10

11

12

13

j ) from all workers then

if Receive ∇Ln(z(t)
Concatenate the gradient vectors, and update
W (t+1) as (3.4);
Update Z (t+1) as (3.5);
Send z(t+1)

to all workers.

j

end

14 end

Regression, (n,p,m,r) = (500,100,50,5)

OLS
Nuclear
BestRep
ProxGD
AccProxGD
ADMM
DFW
DGSP
DNSP

10 4

10 3

10 2

10 1

10 0

10 -1

r
o
r
r

 

E
n
o

i

i
t
c
d
e
r
P
 
s
s
e
c
x
E

0

5

10

15

20

25

30

Rounds of Communication

10 3

10 2

10 1

10 0

10 -1

10 -2

r
o
r
r

 

E
n
o

i

i
t
c
d
e
r
P
 
s
s
e
c
x
E

10 -3

0

Regression, (n,p,m,r) = (2000,200,200,5)

5

10

15

20

25

30

Rounds of Communication

r
o
r
r

 

E
n
o

i

i
t
c
d
e
r
P
 
s
s
e
c
x
E

10 3

10 2

10 1

10 0

10 -1

10 -2

0

10 3

10 2

10 1

10 0

10 -1

10 -2

r
o
r
r

 

E
n
o

i

i
t
c
d
e
r
P
 
s
s
e
c
x
E

10 -3

0

Regression, (n,p,m,r) = (500,100,50,10)

5

10

15

20

25

30

Rounds of Communication

Regression, (n,p,m,r) = (2000,400,50,5)

5

10

15

20

25

30

Rounds of Communication

10 2

10 1

10 0

10 -1

10 -2

r
o
r
r

 

E
n
o

i

i
t
c
d
e
r
P
 
s
s
e
c
x
E

10 -3

0

10 2

10 1

10 0

10 -1

10 -2

10 -3

r
o
r
r

 

E
n
o

i

i
t
c
d
e
r
P
 
s
s
e
c
x
E

10 -4

0

Regression, (n,p,m,r) = (2000,100,500,5)

5

10

15

20

25

30

Rounds of Communication

Regression, (n,p,m,r) = (10000,100,100,5)

5

10

15

20

25

30

Rounds of Communication

Figure 5: Excess prediction error for multi-task regression.

Algorithm 6: DNSP: Distributed Newton Subspace
Pursuit.

1 for t = 1, 2, . . . do
2 Workers:

for j = 1, 2, . . . , m do

Each worker computes the Newton direction

∆Lnj(w(t)

t ) =(cid:16)∇2Lnj(w(t)

t )(cid:17)−1

and sends it to the master.

end
if Receive u from the master then

∇Lnj(w(t)
t )

Perform Gram-Schmidt orthogonalization:

u ← u −Pt−1

k=1hUk, ui;
Normalize u = u/||u||2;
Update the projection matrix U = [U u];
Solve the projected ERM problem:
vj = arg minvj
Update w(t+1)

i=1 ℓ(hvj , U T Xjii, yji);

1

nPn

= U vj.

j

end

15 Master:

if Receive ∆Lnj (w(t)

t ) from all workers then

Concatenate the Newton vectors, and
compute the largest singular vectors:
(u, v) = SV(∆Ln(W (t)));
Send u to all workers.

3

4

5

6

7

8

9

10

11

12

13

14

16

17

18

19

end

20 end

0.1

0.09

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

r
o
r
r

 

E
n
o

i

i
t
c
d
e
r
P
 
s
s
e
c
x
E

0

0

0.1

0.09

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

r
o
r
r

i

E
 
n
o
i
t
c
d
e
r
P
 
s
s
e
c
x
E

0

0

Classification, (n,p,m,r) = (500,50,50,5)

LR
Nuclear
BestRep
ProxGD
AccProxGD
ADMM
DFW
DGSP
DNSP

5

10

15

20

25

30

Rounds of Communication

Classification, (n,p,m,r) = (1000,100,50,5)

0.12

0.1

0.08

0.06

0.04

0.02

r
o
r
r

 

E
n
o

i

i
t
c
d
e
r
P
 
s
s
e
c
x
E

0

0

Classification, (n,p,m,r) = (500,100,50,5)

Classification, (n,p,m,r) = (500,100,50,10)

0.25

0.2

0.15

0.1

0.05

r
o
r
r

i

E
 
n
o
i
t
c
d
e
r
P
 
s
s
e
c
x
E

5

10

15

20

25

30

Rounds of Communication

0

0

5

10

15

20

25

30

Rounds of Communication

Classification, (n,p,m,r) = (2000,200,200,5)

Classification, (n,p,m,r) = (2000,100,500,5)

0.12

0.1

0.08

0.06

0.04

0.02

r
o
r
r

i

E
 
n
o
i
t
c
d
e
r
P
 
s
s
e
c
x
E

0.1

0.09

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

r
o
r
r

i

E
 
n
o
i
t
c
d
e
r
P
 
s
s
e
c
x
E

5

10

15

20

25

30

Rounds of Communication

0

0

5

10

15

20

25

30

Rounds of Communication

0

0

5

10

15

20

25

30

Rounds of Communication

Figure 6: Excess prediction error for multi-task classiﬁcation.

r
o
r
r

 

E
n
o

i

i
t
c
d
e
r
P
 
s
s
e
c
x
E

10 6

10 4

10 2

10 0

10 -2

0

Regression, (n,p,m,r) = (500,100,50,5)

OLS
SVD
Nuclear
BestRep
ProxGD
AccProxGD
ADMM
DFW
DGSP
DNSP

5

10

15

20

25

30

Rounds of Communication

r
o
r
r

 

E
n
o

i

i
t
c
d
e
r
P
 
s
s
e
c
x
E

10 4

10 3

10 2

10 1

10 0

10 -1

10 -2

0

Regression, (n,p,m,r) = (500,100,50,10)

5

10

15

20

25

30

Rounds of Communication

r
o
r
r

 

E
n
o

i

i
t
c
d
e
r
P
 
s
s
e
c
x
E

10 3

10 2

10 1

10 0

10 -1

10 -2

10 -3

0

Regression, (n,p,m,r) = (1000,100,50,5)

5

10

15

20

25

30

Rounds of Communication

Figure 7: Excess prediction error for multi-task regression, with highly correlated features.

8

7

6

5

4

3

2

E
S
M
R
 
d
e
g
a
r
e
v
A

Regression, Computer Survey(20,14,190)

OLS
Nuclear
AltMin
ProxGD
AccProxGD
ADMM
DFW
DGSP
DNSP

0

5

10

15

20

25

30

Rounds of Communication

18

17

16

15

14

13

12

11

10

E
S
M
R
 
d
e
g
a
r
e
v
A

Regression, School(260,27,72)

OLS
Nuclear
AltMin
ProxGD
AccProxGD
ADMM
DFW
DGSP
DNSP

0

5

10

15

20

25

30

Rounds of Communication

0.26

0.24

0.22

0.2

0.18

0.16

C
U
A
 
d
e
g
a
r
e
v
A

 
-
 
1

0.14

0

Classification, Protein(13701,357,3)

LR
Nuclear
AltMin
ProxGD
AccProxGD
ADMM
DFW
DGSP
DNSP

5

10

15

20

25

30

Rounds of Communication

0.6

0.55

0.5

0.45

0.4

0.35

0.3

0.25

C
U
A
 
d
e
g
a
r
e
v
A

 
-
 
1

0.2

0

Classification, Landmine(690,10,19)

LR
Nuclear
AltMin
ProxGD
AccProxGD
ADMM
DFW
DGSP
DNSP

5

10

15

20

25

30

Rounds of Communication

220

200

180

160

140

120

100

E
S
M
R
 
d
e
g
a
r
e
v
A

80

60

0

0.65

0.6

0.55

0.5

0.45

 

C
U
A
d
e
g
a
r
e
v
A

 
-
 
1

0.4

0

Regression, ATP(337,411,6)

OLS
Nuclear
AltMin
ProxGD
AccProxGD
ADMM
DGSP
DNSP

5

10

15

20

25

30

Rounds of Communication

Classification, Cal500(502,68,78)

LR
Nuclear
AltMin
ProxGD
AccProxGD
ADMM
DFW
DGSP
DNSP

5

10

15

20

25

30

Rounds of Communication

Figure 8: Prediction Error on real data.

