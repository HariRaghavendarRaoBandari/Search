6
1
0
2

 
r
a

 

M
1
2

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
1
4
5
6
0

.

3
0
6
1
:
v
i
X
r
a

A Comparison Study of Nonlinear Kernels

Ping Li

Department of Statistics and Biostatistics

Department of Computer Science

Rutgers University

Piscataway, NJ 08854, USA
pingli@stat.rutgers.edu

Abstract

Compared to linear kernel, nonlinear kernels can often substantially improve the accuracies of
many machine learning algorithms. In this paper, we compare 5 diﬀerent nonlinear kernels: min-
max, RBF, fRBF (folded RBF), acos, and acos-χ2, on a wide range of publicly available datasets.
The proposed fRBF kernel performs very similarly to the RBF kernel. Both RBF and fRBF
kernels require an important tuning parameter (γ). Interestingly, for a signiﬁcant portion of the
datasets, the min-max kernel outperforms the best-tuned RBF/fRBF kernels. The acos kernel
and acos-χ2 kernel also perform well in general and in some datasets achieve the best accuracies.

One crucial issue with the use of nonlinear kernels is the excessive computational and memory
cost. These days, one increasingly popular strategy is to linearize the kernels through various
randomization algorithms.
In our study, the randomization method for the min-max kernel
demonstrates excellent performance compared to the randomization methods for other types of
nonlinear kernels, measured in terms of the number of nonzero terms in the transformed dataset.

Our study provides evidence for supporting the use of the min-max kernel and the corresponding
randomized linearization method (i.e., the so-called “0-bit CWS”). Furthermore, the results mo-
tivate at least two directions for future research: (i) To develop new (and linearizable) nonlinear
kernels for better accuracies; and (ii) To develop better linearization algorithms for improving
the current linearization methods for the RBF kernel, the acos kernel, and the acos-χ2 kernel.
One attempt is to combine the min-max kernel with the acos kernel or the acos-χ2 kernel.
The advantages of these two new and tuning-free nonlinear kernels are demonstrated vias our
extensive experiments. A variety of new nonlinear kernels can be constructed in a similar fashion.

Like other tools such as (ensembles of) trees and deep nets, nonlinear kernels have been providing
eﬀective solutions to many machine learning tasks. We hope our (mostly empirical) comparison
study will help advance the development of the theory and the practice of nonlinear kernels.

1

Introduction

It is known in statistical machine learning and data mining that nonlinear algorithms can often
achieve substantially better accuracies than linear methods, although typically nonlinear algorithms
are considerably more expensive in terms of the computation and/or storage cost. The purpose of
this paper is to compare the performance of 5 important nonlinear kernels and their corresponding
linearization methods, to provide guidelines for practitioners and motivate new research directions.

1

We start the introduction with the basic linear kernel. Consider two data vectors u, v ∈ RD. It

is common to use the normalized linear kernel (i.e., the correlation):

ρ = ρ(u, v) =

(1)

i=1 uivi

PD
qPD
i=1 u2

iqPD

i=1 v2
i

This normalization step is in general a recommended practice. For example, when using LIB-
LINEAR or LIBSVM packages [5], it is often suggested to ﬁrst normalize the input data vectors
to unit l2 norm. The use of linear kernel is extremely popular in practice. In addition to pack-
ages such as LIBLINEAR which implement batch linear algorithms, methods based on stochastic
gradient descent (SGD) become increasing important especially for very large-scale applications [1].

Next, we will brieﬂy introduce ﬁve diﬀerent types of nonlinear kernels and the corresponding
randomization algorithms for linearizing these kernels. Without resorting to linearization, it is
rather diﬃcult to scale nonlinear kernels for large datasets [2]. In a sense, it is not very practically
meaningful to discuss nonlinear kernels without knowing how to compute them eﬃciently.

Note that in this paper, we restrict our attention to nonnegative data, which are common in

practice. Several nonlinear kernels to be studied are only applicable to nonnegative data.

1.1 The acos Kernel
Consider two data vectors u, v ∈ RD. The acos kernel is deﬁned as a monotonic function of the
correlation (1):

acos(u, v) = 1 −

1
π

cos−1(ρ(u, v)) = 1 −

1
π

cos−1(ρ)

(2)

There is a known randomization algorithm [8, 4] for linearizing the acos kernel. That is, if we
sample i.i.d. rij from the standard normal distribution and compute the inner products:

xj =

D

Xi=1

uirij,

yj =

D

Xi=1

virij,

rij ∼ N (0, 1)

then the following probability relation holds:

Pr (sign(xj) = sign(yj)) = acos(u, v)

(3)

If we generate independently k such pairs of (xj, yj), we will be able to estimate the probability
which approximates the acos kernel. Obviously, this is just a “pseudo linearization” and the ac-
curacy of approximation improves with increasing sample size k. In the transformed dataset, the
number of nonzero entries in each data vector is exactly k.

Speciﬁcally, we can encode (expand) xj (or yj) as a 2-dim vector [0 1] if xj ≥ 0 and [1 0] if xj < 0.
Then we concatenate k such 2-dim vectors to form a binary vector of length 2k. The inner product
(divided by k) between the two new vectors approximates the probability Pr (sign(xj) = sign(yj)).

1.2 The acos-χ

2 Kernel

The χ2 kernel is commonly used for histograms [20, 3]

ρχ2(u, v) =

D

Xi=1

2uivi
ui + vi

,

D

Xi=1

ui =

D

Xi=1

vi = 1,

ui ≥ 0, vi ≥ 0

(4)

2

For the convenience of linearization via randomization, we consider the following acos-χ2 kernel:

acos − χ2(u, v) = 1 −

1
π

cos−1(ρχ2(u, v))

(5)

As shown in [16], if we sample i.i.d. rij from the standard cauchy distribution C(0, 1) and again
compute the inner product

xj =

D

Xi=1

uirij,

yj =

D

Xi=1

virij,

rij ∼ C(0, 1)

then we obtain a good approximation (as extensively validated in [16]):
Pr (sign(xj) = sign(yj)) ≈ acos-χ2(u, v)

(6)

Again, we can encode/expand xj (or yj) as a 2-dim vector [0 1] if xj ≥ 0 and [1 0] if xj < 0. In the
transformed dataset, the number of nonzeros per data vector is also exactly k.

1.3 Min-Max Kernel

The min-max (MM) kernel is also deﬁned on nonnegative data:

M M (u, v) = PD
PD

i=1 min(ui, vi)
i=1 max(ui, vi)

,

ui ≥ 0, vi ≥ 0

(7)

Given u and v, the so-called “consistent weighted sampling” (CWS) [17, 10] generates random
tuples:

u,j, t∗

(cid:0)i∗

u,j(cid:1) and (cid:0)i∗

v,j, t∗

v,j(cid:1) ,

j = 1, 2, ..., k

(8)

where i∗ ∈ [1, D] and t∗ is unbounded. See Appendix A for details. The basic theoretical result of
CWS says

u,j, t∗

Pr(cid:8)(cid:0)i∗

u,j(cid:1) = (cid:0)i∗

v,j, t∗

v,j(cid:1)(cid:9) = M M (u, v)

(9)

The recent work on “0-bit CWS” [15] showed that, by discarding t∗, Prni∗

is a good approximation, which also leads to a convenient implementation. Basically, we can keep
the lowest b bits (e.g., b = 4 or 8) of i∗ and view i∗ as a binary vector of length 2b with exactly one
1. This way, the number of nonzeros per data vector in the transformed dataset is also exactly k.

v,jo ≈ M M (u, v)

u,j = i∗

1.4 RBF Kernel and Folded RBF (fRBF) Kernel

The RBF (radial basis function) kernel is commonly used. For convenience (e.g., parameter tuning),
we recommend this version:

RBF (u, v; γ) = e−γ(1−ρ)

(10)

where ρ = ρ(u, v) is the correlation deﬁned in (1) and γ > 0 is a crucial tuning parameter.

Based on Bochners Theorem [19], it is known [18] that, if we sample w ∼ unif orm(0, 2π),

ri ∼ N (0, 1) i.i.d., and let x = PD

i=1 uirij, y = PD

i=1 virij, where kuk2 = kvk2 = 1, then we have

E (cos(√γx + w) cos(√γy + w)) = e−γ(1−ρ)

(11)

3

This provides a mechanism for linearizing the RBF kernel.

It turns out that, one can simplify (11) by removing the need of w. In this paper, we deﬁne the

“folded RBF” (fRBF) kernel as follows:

f RBF (u, v; γ) =

e−γ(1−ρ) +

1
2

e−γ(1+ρ)

1
2

(12)

which is monotonic in ρ ≥ 0.
Lemma 1 Suppose x ∼ N (0, 1) and y ∼ N (0, 1) and E(xy) = ρ. Then the following identity
holds:

E (cos(√γx) cos(√γy)) =

e−γ(1−ρ) +

1
2

e−γ(1+ρ)

1
2

Proof: See Appendix B.

1.5 Summary of Contributions

(13)

(cid:3)

1. We propose the “folded RBF” (fRBF) kernel to simplify the linearization step of the tradi-
tional RBF kernel. Via our extensive kernel SVM experiments (i.e., Table 2), we show that
the RBF kernel and the fRBF kernel perform similarly. Then through the experiments on
linearizing RBF and fRBF kernels, both linearization schemes also perform similarly.

2. Our classiﬁcation experiments on kernel SVM illustrate that even the best-tuned RBF/fRBF
kernels in many datasets do not perform as well as those tuning-free kernels, i.e., the min-max
kernel, the acos kernel, and the acos-χ2 kernel.

3. It is known that nonlinear kernel machines are in general expensive in computation and/or
storage [2]. For example, for a small dataset with merely 60, 000 data points, the 60, 000 ×
60, 000 kernel matrix already has 3.6 × 109 entries. Thus, being able to linearize the kernels
becomes crucial in practice. Our extensive experiments show that in general, the consistent
weighted sampling (CWS) for linearizing the min-max kernel performs well, compared to
randomization methods for linearizing the RBF/fRBF kernel, the acos kernel, or the acos-χ2
kernel. In particular, CWS usually requires only a relatively small number of samples to reach
a good accuracy while other methods typically need a large number of samples.

4. We propose two new nonlinear kernels by combining the min-max kernel with the acos kernel
or the acos-χ2 kernel. This idea can be generalized to create other types of nonlinear kernels.

The work in this paper suggests at least two interesting directions for future research: (i) To
develop improved kernel functions. For example, the (tuning-free) min-max kernel in some datasets
does not perform as well as the best-tuned RBF/fRBF kernels. Thus there is room for improvement.
(ii) To develop better randomization algorithms for linearizing the RBF/fRBF kernels, the acos
kernel, and the acos-χ2 kernel. Existing methods require too many samples, which means the
transformed dataset will have many nonzeros per data vector (which will cause signiﬁcant burden
on computation/storage). Towards the end of the paper, we report our proposal of combining the
min-max kernel with the acos kernel or the acos-χ2 kernel. The initial results appear promising.

4

2 An Experimental Study on Kernel SVMs

Table 1: 35 Datasets. We use the same 35 datasets as in the recent paper [15] on 0-bit CWS. The
data are public (and mostly well-known), from various sources including the UCI repository, the
LIBSVM web site, the web site for the book [9], and the papers [11, 12, 13]. Whenever possible, we
use the conventional partitions of training and testing sets. The last column reports the best linear
SVM classiﬁcation results (at the best C value) using LIBLINEAR package and l2-regularization
(with a tuning parameter C). See Figures 1 to 3 for detailed linear SVM results for all C values.

Dataset
Covertype10k
Covertype20k
IJCNN5k
IJCNN10k
Isolet
Letter
Letter4k
M-Basic
M-Image
MNIST10k
M-Noise1
M-Noise2
M-Noise3
M-Noise4
M-Noise5
M-Noise6
M-Rand
M-Rotate
M-RotImg
Optdigits
Pendigits
Phoneme
Protein
RCV1
Satimage
Segment
SensIT20k
Shuttle1k
Spam
Splice
USPS
Vowel
WebspamN1-20k
YoutubeVision
WebspamN1

# train # test # dim linear (%)

70.9
71.1
91.6
91.6
95.5
62.4
61.2
90.0
70.7
90.0
60.3
62.1
65.2
68.4
72.3
78.7
78.9
48.0
31.4
95.3
87.6
91.4
69.1
96.3
78.5
92.6
80.5
90.9
92.6
85.1
91.7
40.9
93.0
62.3
93.3

54
54
22
22
617
16
16
784
784
784
784
784
784
784
784
784
784
784
784
64
16
256
357

47,236

36
19
100
9
54
60
256
10
254
512
254

10,000
20,000
5,000
10,000
6,238
16,000
4,000
12,000
12,000
10,000
10,000
10,000
10,000
10,000
10,000
10,000
12,000
12,000
12,000
3,823
7,494
3,340
17,766
20,242
4,435
1,155
20,000
1,000
3,065
1,000
7,291
528
20,000
11,736
175,000

50,000
50,000
91,701
91,701
1,559
4,000
16,000
50,000
50,000
60,000
4,000
4,000
4,000
4,000
4,000
4,000
50,000
50,000
50,000
1,797
3,498
1,169
6,621
60,000
2,000
1,155
19,705
14,500
1,536
2,175
2,007
462
60,000
10,000
175,000

5

Table 1 lists the 35 datasets for our experimental study in this paper. These are the same
datasets used in a recent paper [15] on the min-max kernel and consistent weighted sampling (0-bit
CWS). The last column of Table 1 also presents the best classiﬁcation results using linear SVM.

Table 2 summarizes the classiﬁcation results using 5 diﬀerent kernel SVMs: the min-max kernel,
the RBF kernel, the fRBF kernel, the acos kernel, and the acos-χ2 kernel. More detailed results
(for all regularization C values) are available in Figures 1 to 3. To ensure repeatability, for all the
kernels, we use the LIBSVM pre-computed kernel functionality. This also means we can not (eas-
ily) test nonlinear kernels on larger datasets, for example, “WebspamN1” in the last row of Table 1.

For both RBF and fRBF kernels, we need to choose γ, the important tuning parameter. For all
the datasets, we exhaustively experimented with 58 diﬀerent values of γ ∈ {0.001, 0.01, 0.1:0.1:2,
2.5, 3:1:20 25:5:50, 60:10:100, 120, 150, 200, 300, 500, 1000}. Here, we adopt the MATLAB nota-
tion that (e.g.,) 3:1:20 means all the numbers from 3 to 20 spaced at 1. Basically, Table 2 reports
the best RBF/fRBF results among all γ and C values in our experiments.

Table 2 shows that RBF kernel and fRBF kernel perform very similarly.

Interestingly, even
with the best tuning parameters, RBF/fRBF kernels do not always achieve the highest classiﬁca-
tion accuracies. In fact, for about 40% of the datasets, the min-max kernel (which is tuning-free)
achieves the highest accuracies. It is also interesting that the acos kernel and the acos-χ2 kernel
perform reasonably well compared to the RBF/fRBF kernels.

Overall, it appears that the RBF/fRBF kernels tend to perform well in very low dimensional
datasets. One interesting future study is to develop new kernel functions based on the min-max
kernel, the acos kernel, or the acos-χ2 kernel, to improve the accuracies. The new kernels could be
the original kernels equipped with a tuning parameter via a nonlinear transformation. One challenge
is that, for any new (and tunable) kernel, we must also be able to ﬁnd a randomization algorithm
to linearize the kernel; otherwise, it would not be too meaningful for large-scale applications.

6

Table 2: Classiﬁcation accuracies (in %) using 5 diﬀerent kernels. We use LIBSVM’s “pre-
computed” kernel functionality for training nonlinear l2-regularized kernel SVMs (with a tuning
parameter C). The reported test classiﬁcation accuracies are the best accuracies from a wide range
of C values; see Figures 1 to 3 for more details. In particular, for the RBF kernel and the fRBF
kernel, we experimented with 58 diﬀerent γ values ranging from 0.001 to 1000 and the reported
accuracies are the best values among all γ (and all C). See Table 1 for more information on the
datasets. The numbers in parentheses are the best γ values for RBF and fRBF.

min-max RBF

Dataset
Covertype10k
Covertype20k
IJCNN5k
IJCNN10k
Isolet
Letter
Letter4k
M-Basic
M-Image
MNIST10k
M-Noise1
M-Noise2
M-Noise3
M-Noise4
M-Noise5
M-Noise6
M-Rand
M-Rotate
M-RotImg
Optdigits
Pendigits
Phoneme
Protein
RCV1
Satimage
Segment
SensIT20k
Shuttle1k
Spam
Splice
USPS
Vowel
WebspamN1-20k
YoutubeVision

80.4
83.3
94.4
95.7
96.4
96.2
91.4
96.2
80.8
95.7
71.4
72.4
73.6
76.1
79.0
84.2
84.2
84.8
41.0
97.7
97.9
92.5
72.4
96.9
90.5
98.1
86.9
99.7
95.0
95.2
95.3
59.1
97.9
72.2

fRBF
80.1 (120)
80.1 (100)
83.8 (150)
83.8 (150)
98.0 (45)
98.0 (40)
98.3 (60)
98.2 (50)
96.8 (6)
96.9 (11)
97.6 (100) 97.6 (100)
94.1 (50)
94.0 (40)
97.2 (5)
97.2 (5)
77.8 (16)
77.8 (16)
96.9 (5)
96.8 (5)
66.8 (10)
66.8 (10)
69.2 (11)
69.2 (11)
71.7 (11)
71.7 (11)
75.3 (14)
75.3 (14)
78.6 (11)
78.7 (12)
85.3 (15)
85.3 (15)
85.4 (12)
85.4 (12)
89.7 (5)
89.7 (5)
45.8 (18)
45.8 (18)
98.7 (8)
98.7 (8)
98.7 (11)
98.7 (13)
92.5 (9)
92.4 (10)
70.3 (4)
70.2 (4)
96.7 (0.3)
96.7 (1.7)
89.8 (150)
89.8 (150)
97.5 (15)
97.5 (15)
85.7 (4)
85.7 (4)
99.7 (15)
99.7 (10)
94.6 (1.7)
94.6 (1.2)
90.0 (15)
89.8 (16)
96.2 (11)
96.2 (11)
65.6 (20)
65.6 (20)
98.0 (35)
98.0 (35)
70.2 (3)
70.1 (4)

acos
81.9
85.3
96.9
97.5
96.5
97.0
93.3
95.7
76.2
95.2
65.0
66.9
69.0
73.1
76.6
83.9
83.5
84.5
41.5
97.7
98.3
92.2
69.2
96.5
89.5
97.6
85.7
99.7
94.2
89.2
95.3
63.0
98.1
69.6

acos-χ2

81.6
85.0
96.6
97.4
96.1
97.0
93.3
95.8
75.2
95.2
64.0
65.7
68.0
71.1
74.9
82.8
82.3
84.6
39.3
97.5
98.1
90.2
70.5
96.7
89.4
97.2
87.5
99.7
95.2
91.7
95.5
61.3
98.5
74.4

7

Covertype20k

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

 

 

40
10−2

100

)

%

(
 
y
c
a
r
u
c
c
A

98

96

94

92

10−1

100

101

C

IJCNN10k
min−max
RBF
fRBF
acos
acos−χ2
linear

 

90
10−2

10−1

100

101

102

103

C

100

Letter

)

%

(
 
y
c
a
r
u
c
c
A

90

80

70

60

 

min−max
RBF
fRBF
acos
acos−χ2
linear

 

50
10−2

10−1

100

100

M−Basic

101

102

103

C

 

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

 

90

80

70

60

50

)

%

(
 
y
c
a
r
u
c
c
A

 

40
10−2

100

)

%

(
 
y
c
a
r
u
c
c
A

98

96

94

92

Covertype10k

10−1

100

101

C

IJCNN5k

min−max
RBF
fRBF
acos
acos−χ2
linear

 

90
10−2

10−1

100

100

Isolet

101

102

103

C

 

)

%

(
 
y
c
a
r
u
c
c
A

95

90

85

 

80
10−2

100

)

%

(
 
y
c
a
r
u
c
c
A

90

80

70

60

10−1

100

101

C

Letter4k

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

 

min−max
RBF
fRBF
acos
acos−χ2
linear

90

80

70

60

50

)

%

(
 
y
c
a
r
u
c
c
A

)

%

(
 
y
c
a
r
u
c
c
A

95

90

85

80

 

50
10−2

10−1

100

M−Image

90

80

70

60

50

40

)

%

(
 
y
c
a
r
u
c
c
A

 

30
10−2

10−1

100

101

C

101

102

103

C

 

 

75
10−2

10−1

100

101

C

100

MNIST10k

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

)

%

(
 
y
c
a
r
u
c
c
A

95

90

85

80

 

75
10−2

10−1

100

101

C

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

Figure 1: Test classiﬁcation accuracies for 5 nonlinear kernels using l2-regularized SVM (with a
tuning parameter C, i.e., the x-axis). Each panel presents the results for one dataset (see data
information in Table 1). For RBF/fRBF kernels (with a tuning parameter γ), at each C, we report
the best accuracy from the results among all γ values. See Figures 2 and 3 for results on more
datasets. For comparison, we include the linear SVM results (green if color is available).

8

80

60

40

)

%

(
 
y
c
a
r
u
c
c
A

 

20
10−2

10−1

100

101

C

M−Noise3

80

60

40

)

%

(
 
y
c
a
r
u
c
c
A

 

20
10−2

10−1

100

101

C

80

M−Noise5

)

%

(
 
y
c
a
r
u
c
c
A

60

40

 

20
10−2

10−1

100

101

C

)

%

(
 
y
c
a
r
u
c
c
A

90
80
70
60
50
40
30
20
10−2

 

50

40

30

20

)

%

(
 
y
c
a
r
u
c
c
A

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

M−Rand

M−Rotate

10−1

100

101

C

10−1

100

101

C

M−RotImg

 

10
10−2

10−1

100

101

C

100

Pendigits

)

%

(
 
y
c
a
r
u
c
c
A

95

90

85

 

80
10−2

10−1

100

101

C

10−1

100

101

C

100

Optdigits

)

%

(
 
y
c
a
r
u
c
c
A

95

90

85

 

80
10−2

10−1

100

101

C

Phoneme

95

90

85

80

75

)

%

(
 
y
c
a
r
u
c
c
A

9

 

70
10−2

10−1

100

101

C

Figure 2: Test classiﬁcation accuracies for 5 nonlinear kernels using l2-regularized SVM.

M−Noise1

 

M−Noise2

 

 

20
10−2

10−1

100

101

C

80

M−Noise4

)

%

(
 
y
c
a
r
u
c
c
A

60

40

 

20
10−2

10−1

100

101

C

M−Noise6

80

60

40

)

%

(
 
y
c
a
r
u
c
c
A

)

%

(
 
y
c
a
r
u
c
c
A

)

%

(
 
y
c
a
r
u
c
c
A

90
80
70
60
50
40
30
20
10−2

 

90
80
70
60
50
40
30
20
10−2

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

Protein

75

70

65

60

55

)

%

(
 
y
c
a
r
u
c
c
A

 

50
10−2

10−1

100

101

C

Satimage

95

90

85

80

75

70

)

%

(
 
y
c
a
r
u
c
c
A

 

65
10−2

10−1

100

101

C

SensIT20k

88

86

84

82

80

78

)

%

(
 
y
c
a
r
u
c
c
A

 

76
10−2

10−1

100

101

C

Spam

96

94

92

90

88

)

%

(
 
y
c
a
r
u
c
c
A

 

86
10−2

10−1

100

101

C

100

USPS

)

%

(
 
y
c
a
r
u
c
c
A

90

80

70

 

60
10−2

10−1

100

101

C

WebspamN1−20k

100

95

90

85

)

%

(
 
y
c
a
r
u
c
c
A

 

80
10−2

10−1

100

101

C

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

Figure 3: Test classiﬁcation accuracies for 5 nonlinear kernels using l2-regularized SVM.

RCV1

97

96

95

94

)

%

(
 
y
c
a
r
u
c
c
A

10−1

100

101

C

Segment

 

93
10−2

100

)

%

(
 
y
c
a
r
u
c
c
A

95

90

85

80

 

75
10−2

10−1

100

101

C

100

Shuttle1k

)

%

(
 
y
c
a
r
u
c
c
A

95

90

85

 

80
10−2

10−1

100

101

C

100

Splice

)

%

(
 
y
c
a
r
u
c
c
A

90

80

70

 

60
10−2

10−1

100

101

C

Vowel

70

60

50

40

30

)

%

(
 
y
c
a
r
u
c
c
A

 

20
10−2

10−1

100

101

C

YoutubeVision

80

70

60

50

40

)

%

(
 
y
c
a
r
u
c
c
A

10

 

30
10−2

10−1

100

101

C

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

 

min−max
RBF
fRBF
acos
acos−χ2
linear
102

103

3 Linearization of Nonlinear Kernels

It is known that a straightforward implementation of nonlinear kernels can be diﬃcult for large
datasets [2]. As mentioned earlier, for a small dataset with merely 60, 000 data points, the
60, 000 × 60, 000 kernel matrix has 3.6 × 109 entries. In practice, being able to linearize nonlinear
kernels becomes very beneﬁcial, as that would allow us to easily apply eﬃcient linear algorithms in
particular online learning [1]. Randomization is a popular tool for kernel linearization.

Since LIBSVM did not implement most of the nonlinear kernels in our study, we simply used
the LIBSVM pre-computed kernel functionality in our experimental study as reported in Table 2.
While this strategy ensures repeatability, it requires very large memory.

In the introduction, we have explained how to linearize these 5 types of nonlinear kernels. From
practitioner’s perspective, while results in Table 2 are informative, they are not suﬃcient for guiding
the choice of kernels. For example, as we will show, for some datasets, even though the RBF/fRBF
kernels perform better than the min-max kernel in the kernel SVM experiments, their linearization
algorithms require many more samples (i.e., large k) to reach the same accuracy as the linearization
method (i.e., 0-bit CWS) for the min-max kernel.

3.1 RBF Kernel versus fRBF Kernel

We have explained how to linearize both the RBF kernel and the fRBF kernel in Introduction.
For two normalized vectors u, v ∈ RD, we generate i.i.d. samples ri ∼ N (0, 1) and independent
w ∼ unif orm(0, 2π). Let x = PD

i=1 uiri and y = PD

i=1 viri. Then we have
E (cos(√γx + w) cos(√γy + w)) = RBF (u, v; γ)
E (cos(√γx) cos(√γy)) = f RBF (u, v; γ)

In order to approximate the expectations with suﬃcient accuracies, we need to generate the samples
(x, y) many (say k) times. Typically k has to be large. In our experiments, even though we use k
as large as 4096, it looks we will have to further increase k in order to reach the accuracy of the
original RBF/fRBF kernels (as in Table 2).

Figure 4 reports the linear SVM experiments on the linearized data for 10 datasets, for k as
large as 4096. We can see that, for most datasets, the linearized RBF and linearized fRBF kernels
perform almost identically. For a few datasets, there are visible discrepancies but the diﬀerences
are small. We repeat the experiments 10 times and the reported results are the averages. Note
that we always use the best γ values as provided in Table 2.

Together with the results on Table 2, the results as shown in Figure 4 allow us to conclude that
the fRBF kernel can replace the RBF kernel and we can simplify the linearization algorithm by
removing the additional random variable w.

11

Covertype10k

80

70

60

50

)

%

(
 
y
c
a
r
u
c
c
A

4096

1024

512

256

128

70

60

50

40

)

%

(
 
y
c
a
r
u
c
c
A

4096

M−Image

1024

512

256

128

40
10−2

100

)

%

(
 
y
c
a
r
u
c
c
A

90

80

70

10−1

100

C

101

102

103

30
10−2

10−1

100

101

102

103

C

4096

M−Noise1

MNIST10k

4096

1024
512
256

128

60

50

40

30

)

%

(
 
y
c
a
r
u
c
c
A

1024

512

256

128

60
10−2

10−1

100

101

102

103

C

20
10−2

10−1

100

101

102

103

C

M−Rotate

90

80

70

60

50

40

)

%

(
 
y
c
a
r
u
c
c
A

Satimage

4096

1024
512

256

128

90

85

80

75

70

)

%

(
 
y
c
a
r
u
c
c
A

4096

1024
512

256

128

30
10−2

10−1

100

101

102

103

C

Splice

90

80

70

)

%

(
 
y
c
a
r
u
c
c
A

4096

1024

512
256
128

65
10−2

100

)

%

(
 
y
c
a
r
u
c
c
A

95

90

85

10−1

100

C

101

102

103

WebspamN1−20k

4096
1024
512

256

128

60
10−2

100

)

%

(
 
y
c
a
r
u
c
c
A

90

80

10−1

100

C

101

102

103

80
10−2

10−1

100

101

102

103

C

USPS

4096
1024

512
256

128

YoutubeVision

70

60

50

40

)

%

(
 
y
c
a
r
u
c
c
A

4096

1024
512
256

128

70
10−2

10−1

100

101

102

103

C

30
10−2

10−1

100

101

102

103

C

Figure 4: Classiﬁcation accuracies of the linearized RBF (solid curves) and linearized fRBF (dashed
curves) kernels, using LIBLINEAR. We report the results on 5 diﬀerent k (sample size) values: 128,
256, 512, 1024, 4096. For most datasets, both RBF and fRBF perform almost identically.

12

3.2 Min-max Kernel versus RBF/fRBF Kernels

Table 2 has shown that for quite a few datasets, the RBF/fRBF kernels outperform the min-max
kernel. Now we compare their corresponding linearization algorithms. We adopt the 0-bit CWS [15]
strategy and use at most 8 bits for storing each sample. See the Introduction and Appendix A for
more details on consistent weighted sampling (CWS).

Figure 5 compares the linearization results of the min-max kernel with the results of the RBF
kernel. We can see that the linearization algorithm for RBF performs very poorly when the sample
size k is small (e.g., k < 1024). Even with k = 4096, the accuracies still do not reach the accuracies
using the original RBF kernel as reported in Table 2.

There is an interesting example. For the “M-Rotate” dataset, the original RBF kernel notably
outperforms the original min-max kernel (89.7% versus 84.8%). However, as shown in Figure 5,
even with 4096 samples, the accuracy of the linearized RBF kernel is still substantially lower than
the accuracy of the linearized min-max kernel.

These observations motivate a useful future research: Can we develop an improved linearization
algorithm for RBF/fRBF kernels which would require much fewer samples to reach good accuracies?

3.3 Min-max Kernel versus acos and acos-χ

2 Kernels

As introduced at the beginning of the paper, sign Gaussian random projections and sign Cauchy
random projections are the linearization methods for the acos kernel and the acos-χ2 kernel, re-
spectively. Figures 6 and 7 compare them with 0-bit CWS, where we use “α = 2” for sign Gaussian
projections and “α = 1” for sign Cauchy projections.

Again, like in Figure 5, we can see that the linearization method for the min-max kernel requires
substantially few samples than the linearization methods for the acos and acos-χ2 kernels. Since
both kernels show reasonably good performance (without linearization), This should also motivate
us to pursue improved linearization algorithms for the acos and acos-χ2 kernels, as future research.

13

80

70

60

50

)

%

(
 
y
c
a
r
u
c
c
A

Covertype10k

80 M−Image

k = 4096

k = 1024

k = 256

k = 128

)

%

(
 
y
c
a
r
u
c
c
A

60

40

k = 4096

k = 1024

k = 256

k = 128

40
10−2

100

)

%

(
 
y
c
a
r
u
c
c
A

90

80

70

10−1

100

C

101

102

103

20
10−2

10−1

100

101

102

103

C

MNIST10k

k = 4096

k = 1024

k = 256

k = 128

M−Noise1

70

60

50

40

30

)

%

(
 
y
c
a
r
u
c
c
A

k = 4096
k = 1024
k = 256

k = 128

60
10−2

10−1

100

101

102

103

C

20
10−2

10−1

100

101

102

103

C

M−Rotate

90

80

70

60

50

40

)

%

(
 
y
c
a
r
u
c
c
A

k = 4096

k = 1024

k = 256

k = 128

30
10−2

100

)

%

(
 
y
c
a
r
u
c
c
A

90

80

70

10−1

100

C

101

102

103

Splice

k = 4096

k = 1024
k = 256
k = 128

95

90

85

80

75

70

)

%

(
 
y
c
a
r
u
c
c
A

Satimage

k = 4096

k = 1024

k = 256
k = 128

65
10−2

100

)

%

(
 
y
c
a
r
u
c
c
A

90

80

10−1

100

C

101

102

103

USPS

k = 4096

k = 1024

k = 256

k = 128

10−1

100

C

101

102

103

70
10−2

10−1

100

101

102

103

C

60
10−2

100

WebspamN1−20k

)

%

(
 
y
c
a
r
u
c
c
A

95

90

85

k = 4096
k = 1024

k = 256

k = 128

70

60

50

40

)

%

(
 
y
c
a
r
u
c
c
A

k = 4096

k = 1024

k = 256

k = 128

80
10−2

10−1

100

101

102

103

C

30
10−2

YoutubeVision

10−1

100

C

101

102

103

Figure 5: Classiﬁcation accuracies of the linearized min-max kernel (solid curves) and linearized
RBF (dashed curves) kernel, using LIBLINEAR. We report the results on 4 diﬀerent k (sample size)
values: 128, 256, 1024, 4096. We only label the dashed curves. We can see that linearized RBF
would require substantially more samples in order to reach the same accuracies as the linearized
min-max method.

14

80 Covertype10k: α = 1

)

%

(
 
y
c
a
r
u
c
c
A

75

70

65

k = 4096

k = 1024

k = 256

k = 128

80

75

70

65

)

%

(
 
y
c
a
r
u
c
c
A

Covertype10k: α = 2

k = 4096

k = 1024

k = 256

k = 128

60
10−2

10−1

100

80

M−Image: α = 1

101

102

103

C

60
10−2

10−1

100

80

M−Image: α = 2

101

102

103

C

k = 4096

k = 1024

k = 256

10−1

k = 128
101

100

C

MNIST10k: α = 1

)

%

(
 
y
c
a
r
u
c
c
A

60

40

20
10−2

100

)

%

(
 
y
c
a
r
u
c
c
A

90

80

70

102

103

k = 4096
k = 1024
k = 256
k = 128

)

%

(
 
y
c
a
r
u
c
c
A

60

40

k = 4096

k = 1024

k = 256

k = 128

20
10−2

10−1

100

C
100 MNIST10k: α = 2

)

%

(
 
y
c
a
r
u
c
c
A

90

80

70

101

102

103

k = 4096

k = 1024
k = 256
k = 128

60
10−2

10−1

100

101

102

103

C

60
10−2

10−1

100

101

102

103

C

70

60

50

40

30

)

%

(
 
y
c
a
r
u
c
c
A

M−Noise1: α = 1

k = 4096

k = 1024

k = 256

70 M−Noise1: α = 2
60

)

%

(
 
y
c
a
r
u
c
c
A

50

40

30

k = 4096

k = 1024

k = 256

k = 128

20
10−2

k = 128
100

10−1

101

102

103

C

20
10−2

10−1

100

101

102

103

C

M−Rotate: α = 1

90

80

70

60

50

40

)

%

(
 
y
c
a
r
u
c
c
A

90

80

70

60

50

40

)

%

(
 
y
c
a
r
u
c
c
A

k = 4096

k = 1024

k = 256

k = 128

M−Rotate: α = 2

30
10−2

10−1

100

101

102

103

C

30
10−2

10−1

100

101

C

k = 4096

k = 1024

k = 256

k = 128
102

103

Figure 6: Classiﬁcation accuracies of the linearized min-max kernel (solid curves) and acos (dashed
curves) kernel (right panels, i.e., α = 2) and the acos-χ2 (dashed curves) kernel (left panels, i.e.,
α = 1), using LIBLINEAR. We report the results on 4 diﬀerent k (sample size) values: 128, 256,
1024, 4096. We only label the dashed curves. We can see that linearized acos and acos-χ2 kernels
require substantially more samples in order to reach the same accuracies as the linearized min-max
method.

15

Satimage: α = 1

95

90

85

80

75

70

)

%

(
 
y
c
a
r
u
c
c
A

k = 4096
k = 1024

k = 256
k = 128

Satimage: α = 2

95

90

85

80

75

70

)

%

(
 
y
c
a
r
u
c
c
A

k = 4096
k = 1024

k = 256
k = 128

65
10−2

100

)

%

(
 
y
c
a
r
u
c
c
A

90

80

70

10−1

100

C

101

102

103

Splice: α = 1

k = 4096

k = 1024

k = 256
k = 128

65
10−2

100

)

%

(
 
y
c
a
r
u
c
c
A

90

80

70

10−1

100

C

101

102

103

Splice: α = 2

k = 4096

k = 1024
k = 256
k = 128

60
10−2

10−1

100

101

102

103

C

60
10−2

10−1

100

101

102

103

C

USPS: α = 1

k = 4096
k = 1024

k = 256
k = 128

100

)

%

(
 
y
c
a
r
u
c
c
A

90

80

USPS: α = 2

k = 4096
k = 1024

k = 256
k = 128

100

)

%

(
 
y
c
a
r
u
c
c
A

90

80

70
10−2

10−1

100

101

102

103

C

70
10−2

10−1

100

101

102

103

C

100

WebspamN1−20k: α = 1

100

WebspamN1−20k: α = 2

)

%

(
 
y
c
a
r
u
c
c
A

95

90

85

k = 4096
k = 1024

k = 256

k = 128

)

%

(
 
y
c
a
r
u
c
c
A

95

90

85

k = 4096
k = 1024

k = 256
k = 128

80
10−2

10−1

100

101

102

103

C

80
10−2

10−1

100

101

102

103

C

70

60

50

40

)

%

(
 
y
c
a
r
u
c
c
A

30
10−2

k = 128
10−1

k = 4096

k = 1024

k = 256

YoutubeVision: α = 1
100

102

101

103

C

70

60

50

40

)

%

(
 
y
c
a
r
u
c
c
A

30
10−2

k = 4096

k = 1024

k = 256

k = 128
10−1

YoutubeVision: α = 2
100

101

102

103

C

Figure 7: Classiﬁcation accuracies of the linearized min-max kernel (solid curves) and acos (dashed
curves) kernel (right panels, i.e., α = 2) and the acos-χ2 (dashed curves) kernel (left panels,
i.e., α = 1), using LIBLINEAR. Again, we can see that linearized acos and acos-χ2 kernels require
substantially more samples in order to reach the same accuracies as the linearized min-max method.

16

3.4 Comparisons on a Larger Dataset

Figure 8 provides the comparison study on the “WebspamN1” dataset, which has 175,000 examples
for training and 175,000 examples for testing. It is too large for using the LIBSVM pre-computed
kernel functionality in common workstations. On the other hand, we can easily linearize the non-
linear kernels and run LIBLINEAR on the transformed dataset.

The left panel of Figure 8 compares the results of linearization method (i.e., 0-bit CWS) for the
min-max kernel with the results of the linearization method for the RBF kernel. The right panel
compares 0-bit CWS with sign Gaussian random projections (i.e., α = 2). We do not present the
results for α = 1 since they are quite similar. The plots again conﬁrm that 0-bit CWS signiﬁcantly
outperforms the linearization methods for both the RBF kernel and the acos kernel.

100

WebspamN1

100

WebspamN1: α = 2

)

%

(
 
y
c
a
r
u
c
c
A

95

90

k = 4096

k = 1024

k = 256

k = 128

)

%

(
 
y
c
a
r
u
c
c
A

95

90

k = 4096
k = 1024

k = 256

k = 128

85
10−2

10−1

100

101

102

103

C

85
10−2

10−1

100

101

102

103

C

Figure 8: Experiments on a larger dataset. Left panel: Classiﬁcation accuracies of the lin-
earized min-max kernel (solid curves) and the linearized RBF (dashed curves) kernel. Right panel:
Classiﬁcation accuracies of the linearized min-max kernel (solid curves) and the linearized acos
(dashed curves) kernel (i.e., α = 2). The linearization method for the min-max kernel (i.e., 0-bit
CWS) substantially outperforms the linearization methods for the other two kernels.

4 Kernel Combinations

It is an interesting idea to combine kernels for better (or more robust) performance. One simple
strategy is to use multiplication of kernels. For example, the following two new kernels

MM-acos(u, v) = MM(u, v) × acos(u, v)
MM-acos-χ2(u, v) = MM(u, v) × acos-χ2(u, v)

(15)
combine the min-max kernel with the acos kernel or the acos-χ2 kernel. They are still positive
deﬁnite because they are the multiplications of positive deﬁnite kernels.

(14)

Table 3 presents the kernel SVM experiments for these two new kernels (i.e., the last two
columns). We can see that for majority of the datasets, these two kernels outperform the min-max
kernel. For a few datasets, the min-max kernel still performs the best (for example, “M-Noise1”);
and on these datasets, the acos kernel and the acos-χ2 kernel usually do not perform as well.
Overall, these two new kernels appear to be fairly robust combinations. Of course, the story will
not be complete until we have also studied their corresponding linearization methods.

A recent study [14] explored the idea of combing the “resemblance” kernel with the linear kernel,
designed only for sparse non-binary data. Since most of the datasets we experiment with are not
sparse, we can not directly use the special kernel developed in [14].

17

Table 3: Classiﬁcation accuracies (in %) of the two new kernels: MM-acos deﬁned in (14) and
MM-acos-χ2 deﬁned in (15), as presented in the last two columns.

Dataset
Covertype10k
Covertype20k
IJCNN5k
IJCNN10k
Isolet
Letter
Letter4k
M-Basic
M-Image
MNIST10k
M-Noise1
M-Noise2
M-Noise3
M-Noise4
M-Noise5
M-Noise6
M-Rand
M-Rotate
M-RotImg
Optdigits
Pendigits
Phoneme
Protein
RCV1
Satimage
Segment
SensIT20k
Shuttle1k
Spam
Splice
USPS
Vowel
WebspamN1-20k
YoutubeVision

min-max

80.4
83.3
94.4
95.7
96.4
96.2
91.4
96.2
80.8
95.7
71.4
72.4
73.6
76.1
79.0
84.2
84.2
84.8
41.0
97.7
97.9
92.5
72.4
96.9
90.5
98.1
86.9
99.7
95.0
95.2
95.3
59.1
97.9
72.2

acos
81.9
85.3
96.9
97.5
96.5
97.0
93.3
95.7
76.2
95.2
65.0
66.9
69.0
73.1
76.6
83.9
83.5
84.5
41.5
97.7
98.3
92.2
69.2
96.5
89.5
97.6
85.7
99.7
94.2
89.2
95.3
63.0
98.1
69.6

acos-χ2 MM-acos MM-acos-χ2

81.6
85.0
96.6
97.4
96.1
97.0
93.3
95.8
75.2
95.2
64.0
65.7
68.0
71.1
74.9
82.8
82.3
84.6
39.3
97.5
98.1
90.2
70.5
96.7
89.4
97.2
87.5
99.7
95.2
91.7
95.5
61.3
98.5
74.4

81.9
85.3
95.6
96.2
96.7
97.2
92.9
96.6
81.0
96.1
71.0
72.2
73.9
75.8
78.7
84.6
84.5
86.5
42.8
97.8
98.2
92.6
71.2
96.8
91.2
98.1
87.1
99.7
94.9
95.9
95.5
58.9
98.0
72.0

81.9
85.3
95.4
96.1
96.6
97.2
92.8
96.5
80.8
96.1
70.8
72.0
73.5
75.5
78.5
84.3
84.3
86.4
41.8
97.9
98.0
92.1
71.4
96.8
90.9
98.3
87.3
99.7
95.0
95.7
95.5
58.7
98.2
72.3

Now we study the linearization methods for these two new kernels, which turn out to be easy.
Take the MM-acos kernel as an example. We can separately and independently generate samples
for the min-max kernel and the acos kernel. The sample for the min-max kernel can be viewed as a
binary vector with one 1. For example, if the sample for the min-max kernel is [0, 0, 1, 0] and the
sample for the acos kernel is -1. Then we can encode the combined sample as [0, 0, 0, 0, 1, 0, 0, 0].
If the sample for the acos kernel is 1, then the combined vector becomes [0, 0, 0, 0, 0, 1, 0, 0].
Basically, if the j-th location in the vector corresponding to the original min-max sample is 1, then
the combined vector will double the length and all the entries will be zero except the (2j-1)-th or
(2j)-th location, depending on the sample value of acos kernel.

Clearly, the idea also applies for combining min-max kernel with RBF kernel. We just need to

replace the “1” in the vector for the min-max kernel sample with the sample of the RBF kernel.

18

Covertype10k: α = 1

k = 4096

k = 1024

Covertype10k: α = 2

k = 4096

k = 1024

68
10−2

10−1

100

C
M−Image: α = 1

68
10−2

10−1

100

C
M−Image: α = 2

k = 256

k = 128

k = 64

101

102

103

k = 4096

k = 256

k = 1024

k = 128

k = 64

80

78

76

74

72

70

)

%

(
 
y
c
a
r
u
c
c
A

80

70

60

50

)

%

(
 
y
c
a
r
u
c
c
A

80

78

76

74

72

70

)

%

(
 
y
c
a
r
u
c
c
A

80

70

60

50

)

%

(
 
y
c
a
r
u
c
c
A

k = 256

k = 128

k = 64

101

102

103

k = 4096

k = 1024

k = 256

k = 128

k = 64

40
10−2

10−1

100

101

102

103

C

40
10−2

10−1

100

101

102

103

C

MNIST10k: α = 1

96

94

92

90

88

)

%

(
 
y
c
a
r
u
c
c
A

k = 4096
k = 1024

k = 256

k = 128

k = 64

96

94

92

90

88

)

%

(
 
y
c
a
r
u
c
c
A

MNIST10k: α = 2

86
10−2

10−1

100

C
M−Noise1: α = 1

70

60

50

40

30

)

%

(
 
y
c
a
r
u
c
c
A

101

102

103

k = 4096

k = 1024
k = 256

k = 128
k = 64

86
10−2

10−1

100

C
M−Noise1: α = 2

70

60

50

40

30

)

%

(
 
y
c
a
r
u
c
c
A

k = 4096
k = 1024

k = 256

k = 128

k = 64

101

102

103

k = 4096

k = 1024

k = 256

k = 128
k = 64

20
10−2

10−1

100

101

102

103

C

20
10−2

10−1

100

101

102

103

C

M−Rotate: α = 1

90

80

70

60

)

%

(
 
y
c
a
r
u
c
c
A

k = 4096
k = 1024

k = 256

k = 128

90

80

70

60

)

%

(
 
y
c
a
r
u
c
c
A

M−Rotate: α = 2

k = 4096
k = 1024

k = 256

k = 128

50
10−2

10−1

100

101

C

k = 64
102

103

50
10−2

10−1

100

101

C

k = 64
102

103

Figure 9: Classiﬁcation accuracies of the linearized min-max kernel (solid curves), the MM-acos
kernel (right panels (α = 2), dash-dotted curves), and the MM-acos-χ2 kernel (left panels (α = 1),
dash-dotted curves), using LIBLINEAR. We report the results for k ∈ {64, 128, 256, 1024, 4096}.
We can see that the linearized MM-acos kernel and the linearized MM-acos-χ2 kernel outperform
the linearized min-max kernel when k is not large.

19

Satimage: α = 1

k = 4096
k = 1024
k = 256
k = 128
k = 64

92

90

88

86

84

)

%

(
 
y
c
a
r
u
c
c
A

Satimage: α = 2

k = 4096
k = 1024
k = 256
k = 128
k = 64

92

90

88

86

84

)

%

(
 
y
c
a
r
u
c
c
A

10−1

100

C

101

102

103

Splice: α = 1

k = 256
k = 128
k = 64

k = 4096
k = 1024

82
10−2

100

)

%

(
 
y
c
a
r
u
c
c
A

90

80

70

60

82
10−2

100

)

%

(
 
y
c
a
r
u
c
c
A

90

80

70

60

10−1

100

C

101

102

103

Splice: α = 2

k = 256
k = 128
k = 64

k = 4096
k = 1024

50
10−2

10−1

100

101

102

103

C

50
10−2

10−1

100

101

102

103

C

USPS: α = 1

96

94

92

90

88

)

%

(
 
y
c
a
r
u
c
c
A

k = 4096
k = 1024
k = 256

k = 128

k = 64

USPS: α = 2

96

94

92

90

88

)

%

(
 
y
c
a
r
u
c
c
A

k = 4096
k = 1024
k = 256
k = 128

k = 64

86
10−2

10−1

100

101

102

103

C

86
10−2

10−1

100

101

102

103

C

98

96

94

92

90

)

%

(
 
y
c
a
r
u
c
c
A

WebspamN1−20k: α = 1

k = 4096
k = 1024
k = 256
k = 128
k = 64

98

96

94

92

90

)

%

(
 
y
c
a
r
u
c
c
A

WebspamN1−20k: α = 2

k = 4096
k = 1024
k = 256
k = 128
k = 64

88
10−2

10−1

100

101

102

103

C

88
10−2

10−1

100

101

102

103

C

YoutubeVision: α = 1

YoutubeVision: α = 2

k = 4096

k = 1024

k = 256

k = 128
k = 64
102

103

)

%

(
 
y
c
a
r
u
c
c
A

75
70
65
60
55
50
45
40
10−2

k = 4096

k = 1024

k = 256

k = 128

k = 64
102

103

)

%

(
 
y
c
a
r
u
c
c
A

75
70
65
60
55
50
45
40
10−2

10−1

100

101

C

10−1

100

101

C

Figure 10: Classiﬁcation accuracies of the linearized min-max kernel (solid curves), the MM-acos
kernel (right panels (α = 2), dash-dotted curves), and the MM-acos-χ2 kernel (left panels (α = 1),
dash-dotted curves), using LIBLINEAR. We report the results for k ∈ {64, 128, 256, 1024, 4096}.

20

Figure 9 and Figure 10 report the linear SVM results using linearized data for the MM-acos
kernel (right panels) and the MM-acos-χ2 kernel (left panels), to compare with the results using
linearized data for the min-max kernel (solid curves). We can see that the linearization methods
for the MM-acos kernel and the MM-acos-χ2 kernel outperform the linearization method for the
min-max kernel when k is not large. These preliminary results are encouraging.

5 Conclusion

Nonlinear kernels can be potentially very useful if there are eﬃcient (in both storage and memory)
algorithms for computing them. It has been known that the RBF kernel, the acos kernel, and the
acos-χ2 kernel can be linearized via randomization algorithms. There are two major aspects when
we compare nonlinear kernels: (i) the accuracy of the original kernel; (ii) how many samples are
needed in order to reach a good accuracy. In this paper, we try to address these two issues by
providing an extensive empirical study on a wide variety of publicly available datasets.

To simplify the linearization procedure for the RBF kernel, we propose the folded RBF (fRBF)
kernel and demonstrates that its performance (either with the original kernel or with linearization)
is very similar to that of the RBF kernel. On the other hand, our extensive nonlinear kernel SVM
experiments demonstrate that the RBF/fRBF kernels, even with the best-tuned parameters, do not
always achieve the best accuracies. The min-max kernel (which is tuning-free) in general performs
well (except for some very low dimensional datasets). The acos kernel and the acos-χ2 kernel also
perform reasonably well.

Linearization is a crucial step in order to use nonlinear kernels for large-scale applications. Our
experimental study illustrates that the linearization method for the min-max kernel, called “0-bit
CWS”, performs well in that it does not require a large number of samples to reach a good accuracy.
In comparison, the linearization methods for the RBF/fRBF kernels and the acos/acos-χ2 kernels
typically require many more samples (e.g., ≥ 4096).
Our study motivates two interesting research problems for future work: (i) how to design better
(and still linearizable) kernels to improve the tuning-free kernels; (ii) how to improve the lineariza-
tion algorithms for the RBF/fRBF kernels as well as the acos/acos-χ2 kernels, in order to reduce
the required sample sizes. The interesting and simple idea of combing two nonlinear kernels by
multiplication appears to be eﬀective but we still hope to ﬁnd an even better strategy in the future.

Another challenging task is to develop (linearizable) kernel algorithms to compete with (ensem-
bles of) trees in terms of accuracy. It is known that tree algorithms are usually slow. Even though
the parallelization of trees is easy, it will still consume excessive energy (e.g., electric power). One
can see from [12, 13] that trees are in general perform really well in terms of accuracy and can
be remarkably more accurate than other methods in some datasets (such as “M-Noise1” and “M-
Image”). On top of the fundamental works [7, 6], the recent papers [12, 13] improved tree algorithms
via two ideas: (i) an explicit tree-split formula using 2nd-order derivatives; (ii) a re-formulation of
the classical logistic loss function which leads to a diﬀerent set of ﬁrst and second derivatives from
textbooks. Ideally, it would be great to develop statistical machine learning algorithms which are
as accurate as (ensembles of) trees and are as fast as linearizable kernels.

21

A Consistent Weighted Sampling

Algorithm 1 Consistent Weighted Sampling (CWS)
Input: Data vector u = (ui ≥ 0, i = 1 to D)
Output: Consistent uniform sample (i∗, t∗)

For i from 1 to D

ri ∼ Gamma(2, 1), ci ∼ Gamma(2, 1), βi ∼ U nif orm(0, 1)
ti ← ⌊ log ui
+ βi⌋, yi ← exp(ri(ti − βi)), ai ← ci/(yi exp(ri))

ri

End For
i∗ ← arg mini ai,

t∗ ← ti∗

Given a data vector u ∈ RD, Alg. 1 (following [10]) provides the procedure for generating one
CWS sample (i∗, t∗). In order to generate k such samples, we have to repeat the procedure k times
using independent random numbers ri, ci, βi.

B Proof of Lemma 1
Let t = √γ. Using the bivariate normal density function, we obtain

E (cos(tx) cos(ty))

cos(tx) cos(ty)

cos(tx) cos(ty)

1

e−

p1 − ρ2

x2

−∞

−∞

−∞

−∞

e−

1
2π

−∞Z ∞
−∞Z ∞

=Z ∞
=Z ∞
=Z ∞
=Z ∞
=Z ∞
=Z ∞
1
√2π
1
√2π
1
e−t2(1−ρ) +
2

1
2π
1
2π
1
2π
e−t2 1−ρ2

e−t2 1−ρ2

e−

e−

=

=

=

−∞

−∞

1

1
2π

1

1
2π

p1 − ρ2
p1 − ρ2
2 cos(tx)dxZ ∞

−∞

x2

x2+y2−2ρxy

2(1−ρ2) dxdy

x2+y2−2ρxy+ρ2x2−ρ2x2

2(1−ρ2)

dxdy

−

e

−

e

cos(ty)e

−

(y−ρx)2
2(1−ρ2) dy

cos(typ1 − ρ2 + tρx)e−y2/2dy
cos(typ1 − ρ2)e−y2/2dy

−∞

−∞

x2

x2

2 cos(tx)dxZ ∞
2 cos(tx) cos(tρx)dxZ ∞
2 cos(tx) cos(tρx)√2πe−t2 1−ρ2
2 Z ∞
√2π
2 (cid:20)e−t2 (1−ρ)2
e−t2(1+ρ)

x2
2 cos(tx) cos(tρx)dx

2 (cid:21)
2 + e−t2 (1+ρ)2

e−

−∞

2

1
2

2 dx

22

References

[1] L. Bottou. http://leon.bottou.org/projects/sgd.

[2] L. Bottou, O. Chapelle, D. DeCoste, and J. Weston, editors. Large-Scale Kernel Machines.

The MIT Press, Cambridge, MA, 2007.

[3] O. Chapelle, P. Haﬀner, and V. N. Vapnik. Support vector machines for histogram-based

image classiﬁcation. IEEE Transactions on Neural Networks, 10(5):1055–1064, 1999.

[4] M. S. Charikar. Similarity estimation techniques from rounding algorithms. In STOC, pages

380–388, Montreal, Quebec, Canada, 2002.

[5] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. Liblinear: A library for

large linear classiﬁcation. Journal of Machine Learning Research, 9:1871–1874, 2008.

[6] J. H. Friedman. Greedy function approximation: A gradient boosting machine. The Annals

of Statistics, 29(5):1189–1232, 2001.

[7] J. H. Friedman, T. J. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view

of boosting. The Annals of Statistics, 28(2):337–407, 2000.

[8] M. X. Goemans and D. P. Williamson. Improved approximation algorithms for maximum cut
and satisﬁability problems using semideﬁnite programming. Journal of ACM, 42(6):1115–1145,
1995.

[9] T. J. Hastie, R. Tibshirani, and J. H. Friedman. The Elements of Statistical Learning:Data

Mining, Inference, and Prediction. Springer, New York, NY, 2001.

[10] S. Ioﬀe. Improved consistent sampling, weighted minhash and L1 sketching. In ICDM, pages

246–255, Sydney, AU, 2010.

[11] H. Larochelle, D. Erhan, A. C. Courville, J. Bergstra, and Y. Bengio. An empirical evaluation
of deep architectures on problems with many factors of variation. In ICML, pages 473–480,
Corvalis, Oregon, 2007.

[12] P. Li. Abc-boost: Adaptive base class boost for multi-class classiﬁcation.

In ICML, pages

625–632, Montreal, Canada, 2009.

[13] P. Li. Robust logitboost and adaptive base class (abc) logitboost. In UAI, 2010.

[14] P. Li. CoRE kernels. In UAI, Quebec City, CA, 2014.

[15] P. Li. 0-bit consistent weighted sampling. In KDD, Sydney, Australia, 2015.

[16] P. Li, G. Samorodnitsky, and J. Hopcroft. Sign cauchy projections and chi-square kernel. In

NIPS, Lake Tahoe, NV, 2013.

[17] M. Manasse, F. McSherry, and K. Talwar. Consistent weighted sampling. Technical Report

MSR-TR-2010-73, Microsoft Research, 2010.

[18] A. Rahimi and B. Recht. andom features for large-scale kernel machines. In NIPS, 2007.

[19] W. Rudin. Fourier Analysis on Groups. John Wiley & Sons, New York, NY, 1990.

23

[20] B. Schiele and J. L. Crowley. Object recognition using multidimensional receptive ﬁeld his-

tograms. In ECCV, pages 610–619, Helsinki, Finland, 1996.

24

