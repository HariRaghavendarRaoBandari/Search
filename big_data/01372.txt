6
1
0
2

 
r
a

M
4

 

 
 
]

.

A
N
h
t
a
m

[
 
 

1
v
2
7
3
1
0

.

3
0
6
1
:
v
i
X
r
a

Numerical CP Decomposition of Some Diﬃcult Tensors

Petr Tichavsk´ya,∗, Anh Huy Phanb and Andrzej Cichockib

aInstitute of Information Theory and Automation, Prague 182 08, Czech Republic

b Brain Science Institute, RIKEN, Wakoshi, Japan.

Abstract

In this paper, a numerical method is proposed for canonical polyadic (CP)
decomposition of small size tensors. The focus is primarily on decomposition
of tensors that correspond to small matrix multiplications. Here, rank of
the tensors is equal to the smallest number of scalar multiplications that are
necessary to accomplish the matrix multiplication. The proposed method
is based on a constrained Levenberg-Marquardt optimization. Numerical
results indicate the rank and border ranks of tensors that correspond to
multiplication of matrices of the size 2 × 3 and 3 × 2, 3 × 3 and 3 × 2, 3 × 3
and 3× 3, and 3× 4 and 4× 3. The ranks are 11, 15, 23 and 29, respectively.
In particular, a novel algorithm for multiplying the matrices of the sizes 3× 3
and 3 × 2 with 15 multiplications is presented.
Keywords: Small matrix multiplication, canonical polyadic tensor
decomposition, Levenberg-Marquardt method

1. Introduction

The problem of determining the complexity of matrix multiplication be-
came a well studied topic since the discovery of the Strassen’s algorithm
[1]. The Strassen’s algorithm allows multiplying 2 × 2 matrices using seven
multiplications. A consequence of this algorithm is that n × n matrices can
be multiplied by performing of the order n2.81 operations. More recent ad-
vances have brought the number of operations needed even closer to the n2
operations. The current record is O(n2.373) operations due to Williams [2].

The problem of the matrix multiplication can be rephrased as a problem
of decomposing a particular tensor according to its rank [3]. The tensor
rank is equal to the lowest number of the scalar multiplications needed to
compute the matrix product. The focus of this paper is not on improving
the above asymptotic results but on numerical decomposition of tensors that
correspond to multiplication of small matrices and determining their rank
[4]. Although the problem is quite old, only partial results are known so far.

Preprint submitted to Journal of Computational and Applied Mathematics March 7, 2016

The matrix multiplication tensor for the 2 × 2 matrices is already com-
pletely clear [5]. Its rank is 7 and its border rank is 7 as well. The border
rank is the lowest rank of tensors that approximate the given tensor. For the
3× 3 case, an algorithm for computing the product with 23 scalar multiplica-
tions was found by Laderman [6]. It means that the rank is at most 23. For
multiplying two 4 × 4 matrices, one can use twice the Strassen’s algorithm,
and therefore the rank is at most 49. Multiplication of 5 × 5 matrices was
studied by Makarov [7] with the result of 100 multiplications (rank 100).
In this paper we present a numerical decomposition of the matrix mul-
tiplication tensors. For now, we are not able to improve the known results
of Strassen, Laderman and Makarov, we rather show a method of the de-
composition with these ranks and numerical results indicating that further
improvements are probably not possible. Moreover, the numerical methods
allow to guess the border rank of the tensors. As a new result, we have
derived a novel algorithm for multiplying two matrices of the size 3 × 3 and
3 × 2 through 15 multiplications.
Traditional numerical tensor decomposition methods include the alternat-
ing least squares method (ALS) [8], improved ALS through the enhanced line
search (ELS) [9], damped Gauss-Newton method, also known as Levenberg-
Marquardt (LM) method [10], and diﬀerent nonlinear optimization methods,
e.g. [11]. For decomposition of the multiplication tensors we have developed
a special variant of the constrained LM method. Once an exact ﬁt solution is
found, we propose a method of ﬁnding another solution such that the factor
matrices only contain nulls, ones and minus ones.

The rest of the paper is organized as follows. The tensors of the matrix
multiplication are introduced in Section 2. The numerical method of their
decomposition is presented in Section 3. Section 4 presents numerical results
and section 5 concludes the paper.

2. Tensor of Matrix Multiplication

Consider two matrices E and F of the sizes P ×Q and Q×S, respectively,
and their matrix product G = EF of the size P × S. The operation of
the matrix multiplication can be represented by a tensor TP QS of the size
P Q × QS × P S which is ﬁlled with nulls and ones only, such that

vec(G) = TP QS ×1 vec(ET )T ×2 vec(FT )T

(1)

regardless of the elements values of E and F. Here, ×i denotes a tensor-
matrix multiplication along the dimension i, and vec is an operator that
stacks all elements of a matrix or tensor in one long column vector.

2

elements
T (P QS)
ijkℓmn = δiℓδjmδkn
to the format P Q × QS × P S.

for

For example,

T222 =





1 0 0 0
0 0 1 0
0 0 0 0
0 0 0 0

0 0 0 0
0 0 0 0
1 0 0 0
0 0 1 0

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

i, k = 1, . . . , P ; ℓ, m = 1, . . . , R; j, n = 1, . . . , S

(2)

0 0 0 0
0 0 0 0
0 1 0 0
0 0 0 1





.

(3)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

0 1 0 0
0 0 0 1
0 0 0 0
0 0 0 0

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Note that the number of ones in the tensor TP QS is P QS; it is the num-
ber of scalar multiplications needed for evaluating the matrix product by a
conventional matrix multiplication algorithm.

The tensor TP QS can be obtained by reshaping an order-6 tensor with

This tensor has the size 4 × 4 × 4, the vertical lines separate the four frontal
slices of the tensor.

A canonical polyadic decomposition of the tensor TP QS is a representation

of the tensor as a sum of R rank-one components

TP QS =

R

X

r=1

ar ◦ br ◦ cr

where {ar}, {br}, {cr} are columns of so called factor matrices A, B, C, sym-
bolically TP QS = [[A, B, C]]. For example, a CP decomposition of the tensor
T222 in (3) corresponding to the Strassen algorithm [2] is T222 = [[A, B, C]]
with

A =

B =

C =













1 0 1 0 1 −1
0
0
0 0 0 0 1
1
1
0 1 0 0 0
0
1 1 0 1 0
0 −1
1 1
0 −1 0 1 0
0 0
0 1 0
1
0 0 1
0 0
0
1 0 1
1 0 −1
1
0 1 −1 0 1
0 0
0 1
0
0
1 0
0 0
1 0
1 −1 1 0

0
1
0

0
1
0

0
1
0













The multiplication tensors have the following properties:

3

1. Ranks of these tensors exceed the tensors’ dimensions.
2. The CP decompositions are not unique.
3. The border ranks of the tensors might be strictly lower than their true

ranks.

4. Tensors TN N N are invariant with respect to some permutations of in-

dices. Using the matlab notation we can write TN N N = permute(TN N N , [2, 3, 1]) =
permute(TN N N , [3, 1, 2])
tiplications [1].

5. Tensors TP QS are invariant with respect to certain tensor-matrix mul-

Let us explain the last item in more details. Since it holds G = EF =
(EX)(X−1F) for any invertible matrix X, we have

vec(G) = TP QS×1vec(ET )T×2vec(FT )T = TP QS×1vec(XT ET )T×2vec(FT X−T )T .
The multiplication with X and X−1 can be absorbed into TP QS, because

vec(XT ET ) = (I ⊗ XT )vec(ET )
vec(FT X−T ) = (X−1 ⊗ I)vec(FT )

where I is identity matrix of an appropriate size. Therefore

TP QS = TP QS ×1 S1(X) ×2 S2(X)

where S1(X) = I ⊗ XT and S2(X) = X−1 ⊗ I.

3. Numerical CP Decomposition

For numerical CP decomposition of the multiplication tensors we propose

a three–step procedure.

1. Finding an “exact ﬁt” solution, if it exists.
2. Finding another solution which would be as much sparse as possible.
3. Finding a solution with factor matrices containing only integer (or ra-

tional) entries.

Step 1: Finding an Exact Fit

We seek a vector of parameters θR = [vec(A)T , vec(B)T , vec(C)T ]T of the
F in the

size 3N 2R × 1 that minimizes the cost function ϕ(θ) = kTN − ˆT (θ)k2
compact set {θ ∈ R3N 2R;kθk2 = c}, where c is a suitable constant.

4

The ordinary (unconstrained) LM algorithm updates θ as

where

θ ← θ − (H + µI)−1g

H = JT J,

J =

∂vec( ˆT (θ))

∂θ

,

g = JT vec(TN − ˆT (θ))

(4)

(5)

and µ is a damping parameter, which is sequentially updated according to
a rule described in [12]. Closed-form expressions for the Hessian H and
gradient g can be found in [10].

Optimization constrained to the ball is performed by minimizing the cost
function in the tangent plane {θ; (θ − θ0)T θ0 = 0} ﬁrst, where θ0 is the latest
1 be the minimizer
available estimate of θ which obeys the constraint. Let θ′
in the tangent plane. Then, θ′
1 is projected on the ball by an appropriate
1√c/kθ′
scale change, θ1 = θ′
1k.
1, let the following second-order approximation of

Towards computing θ′

the cost function be minimized,

ϕ(θ) ≈ ϕ(θ0) + gT (θ − θ0) +

1
2

(θ − θ0)T H(θ − θ0)

(6)

under the linear constraint (θ − θ0)T θ0 = 0. We use the method of Lagrange
multiplier to get

1 = θ0 − H−1g +
θ′

0 H−1g
θT
kθ0k2

H−1θ0 .

(7)

Instead of using (4) directly, we propose replacing H−1 by (H + µI)−1 as in
the LM method.

We need to do multiple random initializations to get close to the global
minimum of the cost function; in the optimum case it is the exact ﬁt solution,
i.e. with ϕ(θ) = 0. The method works well for small matrices. For example,
for decomposition of the T333 and constraint c = 150 we need only a few
random trials to obtain an exact ﬁt solution. On the other hand, for tensor
T444 the false local minima are so numerous that it is almost impossible to
get an exact ﬁt decomposition when the algorithm is started from random
initial conditions.

Step 2: Finding a Sparse Solution

5

For simplicity, we describe a method of ﬁnding a sparse CP decomposi-
tion in the case of tensors TN N N . Let TN N N = [[A, B, C]] be an exact CP
decomposition with certain A, B, C. We have

TN N N = [[S1(X)A, S2(X)B, C]]

(8)

where X is an arbitrary invertible matrix of size N × N.
First, we seek a matrix X of determinant 1 such that kS1(X)Ak1 +
kS2(X)Bk1 is minimized, and update A, B as A ← S1(X)A, B ← S2(X)B.
We use the Nelder-Mead algorithm for the minimization.
Second, we seek another X such that kS1(X)Bk1 + kS2(X)Ck1 is mini-
Third, we seek another X such that kS1(X)Ck1 + kS2(X)Ak1 is mini-
The sequence of three partial optimizations is repeated until convergence

mized, and update C and A.

mized, and update B and C.

is obtained.

As a result, we obtain TN N N = [[A, B, C]] where many elements of

A, B, C are nulls.

Step 3: Finding a Rational Solution

We continue to modify the exact ﬁt solution obtained in the previous
step by constraining some other elements of θR to be 1 or -1. We do this
by sequentially increasing the number of elements of θR to be in the set
{0, 1,−1}. In each step, the function ϕ(θR) is minimized, starting from the
latest available solution, with another free element of θR changed and ﬁxed
to 1 or -1. If an exact ﬁt cannot be achieved, another free element is tried
instead. At the very end, it might happen that none of the free elements of
θR can be set to 1 or -1. In that case, we suggest to try the values 2 or -2 or
higher. Some other elements of θR may become 1/2 or −1/2.

4. Experiments

4.1. Estimating the tensor rank

For the multiplication tensor T333, holds min ϕ(θ23) = 0 under the con-
straint kθ23k2 = 150. The exact ﬁt can be obtained quite quickly. For a rank–
22 approximation of T333, even with a more relaxed constraint kθ22k2 = 594,
the lowest possible value of the ﬁt that we were able to ﬁnd was min ϕ(θ22) =
6.766 · 10−5 > 0 . These observations indicate that the rank of the tensor
T333 is 23. Similarly, if we attempt to decompose the tensor to 22 and 21

6

Upper bounds for ranks and border ranks of multiplication tensors

acronym matrix sizes # of 1’s

rank border rank

TABLE I

222
232
322
332
333
343
443
444

2 × 2,
2 × 3,
3 × 2,
3 × 3,
3 × 3,
3 × 4,
4 × 4,
4 × 4,

2 × 2
3 × 2
2 × 2
3 × 2
3 × 3
4 × 3
4 × 3
4 × 4

8
12
12
18
27
36
48
64

7
11
11
15
23
29
40
49

7
10
10
14
21
28
39
49

terms, min ϕ(θ) converges to zero for c → ∞. However, for decomposition
to 20 terms, min ϕ(θ) does not converge to zero. Therefore we make the
conjecture that the border rank of the tensor is 21.

A more complete table of numerical results obtained by the above de-
scribed procedure is as follows. The table shows rank of the exact-ﬁt so-
lutions of the CP decomposition of the tensors obtained by the constrained
optimization. The numerical border rank was determined as the minimimum
rank for which minϕ(θ) constrained by kθk = c converges to zero for c → ∞.
It is not a mathematical proof that the border rank has the displayed values,
but an empirical observation. The true border ranks can be theoretically
smaller.

The results in the table are rather discouraging for multiplication of the
matrices 2×3 with 3×2 and 3×2 with 2×2. Our experiments indicate that the
necessary number of multiplications is 11 in these two cases. Corresponding
algorithm can be obtained by applying the Strassen algorithm to the 2 × 2
blocks. It is not interesting from the computational point of view.
The only novel results are obtained for the cases 3 × 3 with 3 × 2 and
3 × 4 with 4 × 3. The former case is studied in the next subsection in more
details.

7

 

RANK 23
RANK 22

101

100

 

R
O
R
R
E
D
E
R
A
U
Q
S

10−1

10−2

10−3

10−4

 
0

100

200

300

c

400

500

600

Fig. 1: Reconstruction error as a function of parameter c in the constraint

kθRk2 = c for N = 3, R = 22 and R = 23.

102

 

RANK 48
RANK 49

 

R
O
R
R
E
D
E
R
A
U
Q
S

101

100

10−1

10−2
 
0

200

400

600

c

800

1000

1200

Fig. 2: Reconstruction error as a function of parameter c in the constraint

kθRk2 = c for N = 4, R = 48 and R = 49.
4.2. Multiplication of Matrices 3 × 3 and 3 × 2

Consider multiplication of matrices 3 × 3 and 3 × 2 of the form




e11 e12 e13
e21 e22 e23
e31 e32 e33







f11 f12
f21 f22
f31 f32




=




g11 g12
g21 g22
g31 g32




(9)

8

Standard algorithm for computing g11, . . . , g32 from {eij} and {fij} requires
18 scalar multiplications. We show that the computation can be accom-
plished through 15 scalar multiplications only.
The tensor representing the multiplication has dimension 9 × 6 × 6. A CP
decomposition of this tensor obtained by applying the proposed algorithm is
T332 = [[A, B, C]], where

0
1

0
0

0

0
1

0

0
0
0
0
0

0
0
0
0
0

0
0
0
1
0
0

0
0
0
0
0
0
0

0
0
0
0
0
0
1
0
1
0
0 −1
0
0

−1
0
0
0 −1 0 0 0
0
0
0 1 0
−1
0 −1
0
1
0
0 −1
0 −1 0
0 0 0
0
1
1
1
0
0 −1 0 0 0
0
0
0 0 0
0
0
0
0
1
0
0 0 1 −1 1
0
0
0
0
0
0
0
0
1
1 0 0 −1 0
0
0 −1 −1 −1 0
0 0 0
0
0
0
1
1
0
0
0 0 0
0
0
0
1
1
0 −1 0 −1
0 0 0
1
−1 0 −1 −1
1
0
1
0 0 0
0
0
0
0
1
0
0 1 0 −1 −1 0
1
0
0
0
0 −1
0 0 1
1
0
0
1
0
0 −1 0 0 0
1 0 0 −1 −1 0
0
1
0 −1
0

0
0
0
1
0
0
0
0
0
0
1 −1 0 1 0
0
0 0 0 0
1
0
0 −1
0 0 0 −1 −1 0 1 0 0
0
0 −1 1 0 0 1
0
0
1
1 0 0
1
0 0 0 0
0
1 −1
1 −1 0 1 1
0
0 0 0
0
0 −1
0 −1 0 0 1 1 −1 0
0 −1 1 0 0 1 −1 0
0
0
0
0 0 1



1

0
0
0
1

0
0
1
0

0

1
0

0
0

0

0
0

0
1

0

0
0

1
1

0
0
0
0

1

0

0




















A =

B =

C =

The 15 scalar multiplications are

m1 = −(f11 − f12)(e11 + e12 − e31)
m2 = −(f11 + f32)(e13 − e31)
m3 = −e21(f12 − f22)
m4 = −(e12 − e21)(f11 − f12 + f22)
m5 = (f31 − f32)(e32 − e13 + e33)
m6 = f11(e11 − e13 + e21)
m7 = e31(f12 + f32)
m8 = −e12(f11 − f12 − f21 + f22)
m9 = e23(f21 − f31)

9

m10 = −f32(e23 + e31 − e33)
m11 = f21(e12 + e22 + e23)
m12 = f22(e21 + e22 − e32)
m13 = (e23 + e32)(f21 − f31 + f32)
m14 = e32(f21 − f22 − f31 + f32)
m15 = e13(f11 + f31)

Having computed these products, the elements gij in (9) can be written as




g11 g12
g21 g22
g31 g32




=




m3 − m4 + m6 + m8 + m12
−m3 + m4 − m8 − m9 + m11

m1 − m2 + m3 − m4 + m6 + m7
−m3 − m9 + m12 + m13 − m14
m2 + m5 − m9 + m10 + m13 + m15 m7 − m9 + m10 + m13 − m14




5. Conclusions

The constrained LM algorithm may serve for decomposition of diﬃcult
tensors that have the border rank lower than the true rank and when unique-
ness is not required. Numerical decomposition of tensors larger than T333,
e.g. T444, is still a challenging task. We have provided a decomposition of
T332 tensor to 15 rank-one terms, i.e. showed that product of the matrices
3 × 3 and 3 × 2 can be computed through 15 scalar multiplications. Matlab
codes of the proposed algorithms are available on the web page of the ﬁrst
author.

Acknowledgements

The work of Petr Tichavsk´y was supported by Czech Science Foundation

through project 14-13713S.

References

[1] V. Strassen, Gaussian elimination is not optimal, Numer. Math. 13

(1969) 354–356.

[2] V.V. Williams, Multiplying Matrices Faster Than Coppersmith-
Winograd, in Proc. of the 44th Symposium on Theory of Computing,
STOC 12, New York, NY, USA, (2012) 887-898.

[3] J. M. Landsberg, Tensors: Geometry and Applications, AMS 2012.

[4] C.-E. Drevet, M.N. Islam and E. Schost, Optimization techniques for
small matrix multiplications, Theoretical Computer Science 412 (2011),
2219-2236.

10

[5] S. Winograd, On Multiplication of 2 × 2 Matrices, Linear Algebra and

Appl. 4 (1971) 381-388.

[6] J.D. Laderman, A noncommutative algorithm for multiplying 3× 3 ma-
trices using 23 multiplications, Bul. Amer. Math. Soc. 82 (1976) 126–128.

[7] O.M. Makarov, A noncommutative algorithm for multiplying 5 × 5−
matrices using one hundred multiplications, U.S.S.R. Comput. Maths.
Math. Phys. 27 (1987) 311–315.

[8] P. Comon, X. Luciani and A. L. F. de Almeida, Tensor decompositions,
alternating least squares and other tales, Chemometrics 23 (2009) 393-
405.

[9] M. Rajih, P. Comon, and R. A. Harshman, Enhanced line search: A
novel method to accelerate PARAFAC, SIAM Journal on Matrix Anal-
ysis Appl. 30 (2008) 1148–1171.

[10] A.H. Phan, P. Tichavsk´y and A. Cichocki, Low Complexity Damped
Gauss-Newton Algorithms for Parallel Factor Analysis, SIAM J. Matrix
Anal. and Appl. 34 (2013) 126–147.

[11] L. Sorber, M. Van Barel, and L. De Lathauwer, Structured data fusion,

Tech. Rep., ESAT-SISTA, Internal Report 13-177, 2013.

[12] K. Madsen, H. B. Nielsen, O. Tingleﬀ, Methods for nonlinear least
squares problems, second ed., Department of Mathematical Modelling,
Technical University of Denmark, Lyngby, Denmark, 2004.

11

