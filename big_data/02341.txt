Enhanced Robot Audition Based on Microphone

Array Source Separation with Post-Filter

Jean-Marc Valin, Jean Rouat, François Michaud

LABORIUS, Department of Electrical Engineering and Computer Engineering

Université de Sherbrooke, Sherbrooke (Quebec) CANADA, J1K 2R1
{Jean-Marc.Valin, Jean.Rouat, Francois.Michaud}@USherbrooke.ca

6
1
0
2

 
r
a

M
7

 

 
 
]

O
R
.
s
c
[
 
 

1
v
1
4
3
2
0

.

3
0
6
1
:
v
i
X
r
a

Abstract—We propose a system that gives a mobile robot the
ability to separate simultaneous sound sources. A microphone
array is used along with a real-time dedicated implementation
of Geometric Source Separation and a post-ﬁlter that gives us a
further reduction of interferences from other sources. We present
results and comparisons for separation of multiple non-stationary
speech sources combined with noise sources. The main advantage
of our approach for mobile robots resides in the fact that both
the frequency-domain Geometric Source Separation algorithm
and the post-ﬁlter are able to adapt rapidly to new sources
and non-stationarity. Separation results are presented for three
simultaneous interfering speakers in the presence of noise. A
reduction of log spectral distortion (LSD) and increase of signal-
to-noise ratio (SNR) of approximately 10 dB and 14 dB are
observed.

I. INTRODUCTION

Our hearing sense allows us to perceive all kinds of sounds
(speech, music, phone ring, closing a door, etc.) in our world,
whether we are moving or not. To operate in human and
natural settings, autonomous mobile robots should be able to
do the same. This requires the robots not just to detect sounds,
but also to localise their origin, separate the different sound
sources (since sounds may occur simultaneously), and process
all of this data to extract useful information about the world.
Even though artiﬁcial hearing would be an important sens-
ing capability for autonomous systems,
the research topic
is still in its infancy. Only a few robots are using hearing
capabilities: SAIL [1] uses one microphone to develop online
audio-driven behaviors; ROBITA [2] uses two microphones
to follow a conversation between two persons; SIG [3], [4],
[5] uses one pair of microphones to collect sound from the
external world, and another pair placed inside the head to
collect internal sounds (caused by motors) for noise cancel-
lation; Sony SDR-4X has seven microphones; a service robot
uses eight microphones organised in a circular array to do
speech enhancement and recognition [6]. Even though robots
are not limited to only two ears, they still have not shown the
capabilities of the human hearing sense.

We address the problem of isolating sound sources from
the environment. The human hearing sense is very good at
focusing on a single source of interest despite all kinds of

0 c(cid:13)2004 IEEE. Personal use of this material is permitted. Permission from
IEEE must be obtained for all other uses, in any current or future media,
including reprinting/republishing this material for advertising or promotional
purposes, creating new collective works, for resale or redistribution to servers
or lists, or reuse of any copyrighted component of this work in other works.

interferences. We generally refer to this ability as the cocktail
party effect, where a human listener is able to follow a
conversation even when several people are speaking at the
same time. For a mobile robot, it would mean being able to
separate all sound sources present in the environment at any
moment.

Working toward that goal, our interest in this paper is to
describe a two-step approach for performing sound source
separation on a mobile robot equipped with an array of eight
low-cost microphones. The initial step consists of a linear
separation based on a simpliﬁed version of the Geometric
Source Separation approach proposed by Parra and Alvino
[7] with a faster stochastic gradient estimation and shorter
time frames estimations. The second step is a generalisation
of beamformer post-ﬁltering [8], [9] for multiple sources and
uses adaptive spectral estimation of background noise and
interfering sources to enhance the signal produced during the
initial separation. The novelty of this post-ﬁlter resides in the
fact that, for each source of interest, the noise estimate is
decomposed into stationary and transient components assumed
to be due to leakage between the output channels of the initial
separation stage.

The paper is organised as follows. Section II gives an
overview of the system. Section III presents the linear sep-
aration algorithm and Section IV describes the proposed post-
ﬁlter. Results are presented in Section V, followed by the
conclusion.

II. SYSTEM OVERVIEW

The proposed sound separation algorithm as shown in

Figure 1 is composed of three parts:

1) A microphone array;
2) A linear source separation algorithm (LSS) implemented
as a variant of the Geometric Source Separation (GSS)
algorithm;

3) A multi-channel post-ﬁlter.
The microphone array is composed of a number of omni-
directional elements mounted on the robot. The microphone
signals are combined linearly in a ﬁrst-pass separation algo-
rithm. The output of this initial separation is then enhanced
by a (non-linear) post-ﬁlter designed to optimally attenuate the
remaining noise and interference from other sources.

We assume that these sources are detected and localised
by an algorithm such as [10] (our approach is not speciﬁc to

1) Decorrelation of the separation algorithm outputs, ex-

pressed as Ryy(k) − diag [Ryy(k)] = 01.

2) The geometric constraint W(k)A(k) = I, which en-
sures unity gain in the direction of the source of interest
and places zeros in the direction of interferences.

In theory, constraint 2) could be used alone for separation
(the method is referred to as LS-C2 in [7]), but in practice,
the method does not take into account reverberation or errors
in localisation. It is also subject to instability if A(k) is
not invertible at a speciﬁc frequency. When used together,
constraints 1) and 2) are too strong. For this reason, we
propose “soft” constraints that are a combination of 1) and
2) in the context of a gradient descent algorithm.

Two cost functions are created by computing the square
of the error associated with constraints 1) and 2). These cost
functions are respectively deﬁned as:

J1(W(k)) = kRyy(k) − diag [Ryy(k)]k2
J2(W(k)) = kW(k)A(k) − Ik2

(3)
(4)

where the matrix norm is deﬁned as kMk2 = trace(cid:2)MMH(cid:3)

and is equal to the sum of the square of all elements in the
matrix. The gradient of the cost functions with respect to
W(k) is equal to [7]:

∂J1(W(k))

∂W∗(k)

∂J2(W(k))

∂W∗(k)

= 4E(k)W(k)Rxx(k)

= 2 [W(k)A(k) − I] A(k)

(5)

(6)

where E(k) = Ryy(k) − diag [Ryy(k)].

The separation matrix W(k) is then updated as follows:

Wn+1(k) = Wn(k) − µ(cid:20)α(k)

∂J1(W(k))

∂W∗(k)

+

∂J2(W(k))

∂W∗(k) (cid:21)

(7)
to

where α(f ) is an energy normalisation factor equal
kRxx(k)k−2 and µ is the adaptation rate.

B. Stochastic Gradient Adaptation

The difference between our algorithm and the original GSS
algorithm described in [7] is that instead of estimating the
correlation matrices Rxx(k) and Ryy(k) on several seconds
of data, our approach uses instantaneous estimations. This
is analogous to the approximation made in the Least Mean
Square (LMS) adaptive ﬁlter [11]. We thus assume that:

Rxx(k) = x(k)x(k)H
Ryy(k) = y(k)y(k)H

(8)
(9)

It is then possible to rewrite the gradient ∂J1(W(k))
∂W∗(k)

as:

∂J1(W(k))

∂W∗(k)

= 4 [E(k)W(k)x(k)] x(k)H

(10)

which only requires matrix-by-vector products, greatly reduc-
ing the complexity of the algorithm. The normalisation factor

1Assuming non-stationary sources, second order statistics are sufﬁcient for

ensuring independence of the separated sources.

Figure 1. Overview of the separation system

any localisation algorithm). We also assume that sources may
appear, disappear or move at any time. It is thus necessary
to maximise the adaptation rate for both the LSS and the
multi-channel post-ﬁlter. Mobile robotics also imposes real-
time constraints: the algorithmic delay must be kept small
and the complexity must be low enough for the data to be
processed in real-time on a conventional processor.

III. LINEAR SOURCE SEPARATION

The LSS algorithm we propose in this section is based on
the Geometric Source Separation (GSS) approach proposed
by Parra and Alvino [7]. Unlike the Linearly Constrained
Minimum Variance (LCMV) beamformer that minimises the
output power subject to a distortionless constraint, GSS ex-
plicitly minimises cross-talk, leading to faster adaptation. The
method is also interesting for use in the mobile robotics
context because it allows easy addition and removal of sources.
Using some approximations described in Subsection III-B,
it is also possible to implement separation with relatively
low complexity (i.e. complexity that grows linearly with the
number of microphones).

A. Geometric Source Separation

The method operates in the frequency domain. Let Sm(k, ℓ)
be the real (unknown) sound source m at time frame ℓ and
for discrete frequency k. We denote as s(k, ℓ) the vector
corresponding to the sources Sm(k, ℓ) and matrix A(k) is the
transfer function leading from the sources to the microphones.
The signal received at the microphones is thus given by:

x(k, ℓ) = A(k)s(k, ℓ) + n(k, ℓ)

(1)

where n(k, ℓ) is the non-coherent background noise received
at the microphones. The matrix A(k) can be estimated using
the result of a sound localisation algorithm. Assuming that all
transfer functions have unity gain, the elements of A(k) can
be expressed as:

aij (k) = e−2πkδij

(2)

where δij is the time delay (in samples) to reach microphone
i from source j.

The separation result

then deﬁned as y(k, ℓ) =
W(k, ℓ)x(k, ℓ), where W(k, ℓ) is the separation matrix that
must be estimated. This is done by providing two constraints
(the index ℓ is omitted for the sake of clarity):

is

Xn(k,l)

Geometric

source

separation

Ym(k,l)

^
Sm(k,l)

Attenuation

rule

Interference

leak

estimation

Stationary

noise

estimation

λm

leak(k,l)

+

SNR & speech

probatility
estimation

λm

stat.(k,l)

λm(k,l)

Figure 2. Overview of the post-ﬁlter.
Xn(k, ℓ), n = 0 . . . N −1: Microphone inputs, Ym(k, ℓ), m = 0 . . . M −1:
Inputs to the post-ﬁlter, ˆSm(k, ℓ) = Gm(k, ℓ)Ym(k, ℓ), m = 0 . . . M − 1:
Post-ﬁlter outputs.

α(k) can also be simpliﬁed as hkx(k)k2i−2

. From this work,
the instantaneous estimation of the correlation has not shown
any reduction in accuracy and furthermore eases real-time
integration.

C. Initialisation

The fact that sources can appear or disappear at any time
imposes constraints on the initialisation of the separation
matrix W(k). The initialisation must provide the following:

• The initial weights for a new source;
• Acceptable separation (before adaptation).

For this post-ﬁlter, we consider that all interferences (except
the background noise) are localised (detected by the local-
isation algorithm) sources and we assume that the leakage
between channels is constant. This leakage is due to reverber-
ation, localisation error, differences in microphone frequency
responses, near-ﬁeld effects, etc.

Section IV-A describes the estimation of noise variances that
are used to compute the weighting function Gm by which the
outputs Ym of the LSS is multiplied to generate a cleaned
signal whose spectrum is denoted ˆSm.

A. Noise estimation

The noise variance estimation λm(k, ℓ) is expressed as:

λm(k, ℓ) = λstat.

m (k, ℓ) + λleak

m (k, ℓ)

(12)

where λstat.
m (k, ℓ) is the estimate of the stationary component
of the noise for source m at frame ℓ for frequency k, and
m (k, ℓ) is the estimate of source leakage.
λleak
We compute the stationary noise estimate λstat.

m (k, ℓ) using
the Minima Controlled Recursive Average (MCRA) technique
proposed by Cohen [16].

To estimate λleak

m we assume that the interference from
other sources is reduced by a factor η (typically −10 dB ≤
η ≤ −5 dB) by the separation algorithm (LSS). The leakage
estimate is thus expressed as:

λleak
m (k, ℓ) = η

M −1

Xi=0,i6=m

Zi(k, ℓ)

(13)

Furthermore, when a source appears or disappears, other
sources must be unaffected.

One easy way to satisfy both constraints is to initialise the

column of W(k) corresponding to the new source m as:

where Zm(k, ℓ) is the smoothed spectrum of the mth source,
Ym(k, ℓ), and is recursively deﬁned (with αs = 0.7) as:

Zm(k, ℓ) = αsZm(k, ℓ − 1) + (1 − αs)Ym(k, ℓ)

(14)

wm,i(k) =

ai,m(k)

N

(11)

This initialisation is equivalent to a delay-and-sum beam-
former, and is referred to as the I1 initialisation method in
[7].

IV. MULTI-CHANNEL POST-FILTER

In order to enhance the output of the GSS algorithm
presented in Section III, we derive a frequency-domain post-
ﬁlter that is based on the optimal estimator originally proposed
by Ephraim and Malah [12], [13]. Several approaches to
microphone array post-ﬁltering have been proposed in the
past. Most of these post-ﬁlters address reduction of stationary
background noise [14], [15]. Recently, a multi-channel post-
ﬁlter taking into account non-stationary interferences was
proposed by Cohen [8]. The novelty of our approach resides
in the fact that, for a given channel output of the GSS, the
transient components of the corrupting sources is assumed to
be due to leakage from the other channels during the GSS
process. Furthermore, for a given channel, the stationary and
the transient components are combined into a single noise
estimator used for noise suppression, as shown in Figure 2.

B. Suppression rule in the presence of speech

We now derive the suppression rule under H1, the hypoth-
esis that speech is present. From here on, unless otherwise
stated, the m index and the ℓ arguments are omitted for clarity
and the equations are given for each m and for each ℓ.

The proposed noise suppression rule is based on mini-
mum mean-square error (MMSE) estimation of the spectral
amplitude in the loudness domain, |X(k)|1/2. The choice of
the loudness domain over the spectral amplitude [12] or log-
spectral amplitude [13] is motivated by better results obtained
using this technique, mostly when dealing with speech pres-
ence uncertainty (Section IV-C).

The loudness-domain amplitude estimator is deﬁned by:

ˆA(k) = (E [|S(k)|α |Y (k) ])

1

α = GH1 (k) |Y (k)|

(15)

where α = 1/2 for the loudness domain and GH1 (k) is the
spectral gain assuming that speech is present.

The spectral gain for arbitrary α is derived from Equation

13 in [13]:

GH1 (k) = pυ(k)

γ(k) hΓ(cid:16)1 +

α

2(cid:17) M (cid:16)−

α
2

; 1; −υ(k)(cid:17)i

1

α (16)

where M (a; c; x) is the conﬂuent hypergeometric function,

γ(k) , |Y (k)|2 /λ(k) and ξ(k) , Eh|S(k)|2i /λ(k) are

respectively the a posteriori SNR and the a priori SNR. We
also have υ(k) , γ(k)ξ(k)/ (ξ(k) + 1) [12].

The a priori SNR ξ(k) is estimated recursively as:

ˆξ(k, l) = αpG2

H1 (k, ℓ − 1)γ(k, ℓ − 1)

+ (1 − αp) max {γ(k, ℓ) − 1, 0}

(17)

using the modiﬁcations proposed in [16] to take into account
speech presence uncertainty.

C. Optimal gain modiﬁcation under speech presence uncer-
tainty

In order to take into account the probability of speech

presence, we derive the estimator for the loudness domain:

ˆA(k) = (E [ Aα(k)| Y (k)])

1
α

(18)

Figure 3. Pioneer 2 robot with an array of eight microphones

Considering H1,

the hypothesis of speech presence for
source m, and H0, the hypothesis of speech absence, we
obtain:

D. Initialisation

E[Aα(k)|Y(k)] = p(k)E [ Aα(k)| H1, Y (k)]

+ [1−p(k)]E[Aα(k)|H0,Y(k)]

(19)

where p(k) is the probability of speech at frequency k.

The optimally modiﬁed gain is thus given by:

G(k) = (cid:2)p(k)Gα

min(cid:3)

H1 (k) + (1 − p(k))Gα

1
α

(20)

where GH1 (k) is deﬁned in (16), and Gmin is the minimum
gain allowed when speech is absent. Unlike the log-amplitude
case, it is possible to set Gmin = 0 without running into
problems. For α = 1/2, this leads to:

G(k) = p2(k)GH1 (k)

(21)

Setting Gmin = 0 means that there is no arbitrary limit
on attenuation. Therefore, when the signal is certain to be
non-speech, the gain can tend toward zero. This is especially
important when the interference is also speech since, unlike
stationary noise, residual babble noise always results in musi-
cal noise.

The probability of speech presence is computed as:

p(k) = (cid:26)1 +

ˆq(k)

1 − ˆq(k)

(1 + ξ(k)) exp (−υ(k))(cid:27)−1

(22)

where ˆq(k) is the a priori probability of speech presence for
frequency k and is deﬁned as:

ˆq(k) = 1 − Plocal(k)Pglobal(k)Pf rame

(23)

where Plocal(k), Pglobal(k) and Pf rame are deﬁned in [16]
and correspond respectively to a speech measurement on the
current frame for a local frequency window, a larger frequency
and for the whole frame.

When a new source appears, post-ﬁlter state variables need
to be initialised. Most of these variables may safely be set
m (k, ℓ0), the initial stationary
to zero. The exception is λstat.
noise estimation for source m. The MCRA algorithm requires
several seconds to produce its ﬁrst estimate for source m, so
it is necessary to ﬁnd another way to estimate the background
noise until a better estimate is available. This initial estimate
is thus computed using noise estimations at the microphones.
Assuming the delay-and-sum initialisation of the weights from
Equation 11, the initial background noise estimate is thus:

λstat.
m (k, ℓ0) =

1
N 2

N −1

Xn=0

σ2
xn (k)

(24)

where σ2

xn (k) is the noise estimation for microphone n.

V. RESULTS

Our system is evaluated on a Pioneer 2 robot, on which an
array of eight microphones is installed. In order to test the
system, three voices (two female, one male) were recorded
separately, in a quiet environment. The background noise was
recorded on the robot and includes the room ventilation and
the internal robot fans. All four signals were recorded using
the same microphone array and subsequently mixed together.
This procedure was required in order to compute the distance
measures (such as SNR) presented in this section. It is worth
noting that although the signals were mixed artiﬁcially, the
result still represents real conditions with background noise,
interfering sources, and reverberation.

In evaluating our source separation system, we use the
conventional signal-to-noise ratio (SNR) and the log spectral

SIGNAL-TO-NOISE RATIO (SNR) FOR EACH OF THE THREE SEPARATED

SOURCES.

Table I

SNR (dB)

female 1

female 2 male 1

Microphone inputs

Delay-and-sum

GSS

GSS+single channel
GSS+multi-channel

-1.8
7.3
9.0
9.9
12.1

-3.7
4.4
6.0
6.9
9.5

-5.2
-1.2
3.7
4.5
9.4

LOG-SPECTRAL DISTORTION (LSD) FOR EACH OF THE THREE SEPARATED

Table II

SOURCES.

LSD (dB)

female 1

Microphone inputs

Delay-and-sum

GSS

GSS+single channel
GSS+multi-channel

17.5
15.8
15.0
9.7
6.5

female 2 male 1
14.8
15.1
14.2
10.4
7.4

15.9
15.0
14.2
9.5
6.8

e
d
u

t
i
l

p
m
A

e
d
u

t
i
l

p
m
A

e
d
u

t
i
l

p
m
A

2000

1000

0

−1000

−2000

0

2000

1000

0

−1000

−2000

0

2000

1000

0

−1000

−2000

0

1

1

1

2

2

2

3

Time [s]

3

Time [s]

3

Time [s]

4

4

4

5

5

5

6

6

6

distortion (LSD), that is deﬁned as:

LSD =

1
L

L−1

Xℓ=0

1
K

K−1

Xk=0







|S(k, ℓ)|2 + ǫ

10 log10

2

+ ǫ

(cid:12)(cid:12)(cid:12)
ˆS(k, ℓ)(cid:12)(cid:12)(cid:12)

1

2




2


(25)
where L is the number of frames, K is the number of
frequency bins and ǫ is meant to prevent extreme values for
spectral regions of low energy.

Tables I and II compare the results obtained for different
conﬁgurations: unprocessed microphone inputs, delay-and-
sum algorithm, GSS algorithm, GSS algorithm with single-
channel post-ﬁlter, and GSS algorithm with multi-channel
post-ﬁlter (proposed). It is worth noting that the delay-and-
sum algorithm corresponds to the initial value of the sep-
aration matrix provided to our algorithm. While it is clear
that GSS performs better than delay-and-sum, the latter still
provides acceptable separation capabilities. These results also
show that our multi-channel post-ﬁlter provides a signiﬁcant
improvement over both the single-channel post-ﬁlter and plain
GSS.

The signals amplitude for the ﬁrst source (female) are
shown in Figure 5 and the spectrograms are shown in Figure
4. Even though the task involves non-stationary interference
with the same frequency content as the signal of interest, we
observe that our post-ﬁlter (unlike the single-channel post-
ﬁlter) is able to remove most of the interference, while not
causing excessive distortion to the signal of interest. Informal
subjective evaluation has conﬁrmed that the post-ﬁlter has
a positive impact on both quality and intelligibility of the
speech2.

VI. CONCLUSION

In this paper we describe a microphone array linear source
separator and a post-ﬁlter in the context of multiple and

2Audio signals and spectrograms for all three sources are available at:

http://www.speex.org/~jm/phd/separation/

Figure 5. Signal amplitude for separation of ﬁrst source (female voice). top:
signal at one microphone. middle: system output. bottom: reference (clean)
signal.

simultaneous sound sources. The linear source separator is
based on a simpliﬁcation of the geometric source separation
algorithm that performs instantaneous estimation of the corre-
lation matrix Rxx(k). The post-ﬁlter is based on a loudness-
domain MMSE estimator in the frequency domain with a noise
estimate that is computed as the sum of a stationary noise
estimate and an estimation of leakage from the geometric
source separation algorithm. The proposed post-ﬁlter is also
sufﬁciently general to be used in addition to most linear source
separation algorithms.

Experimental results show a reduction in log spectral dis-
tortion of up to 11 dB and an increase of the signal-to-noise
ratio of 14dB compared to the noisy signal inputs. Preliminary
perceptive test and visual inspection of spectrograms show us
that the distortions introduced by the system are acceptable to
most listeners.

A possible next step for this work would consist of directly
optimizing the separation results for speech recognition ac-
curacy. Also, a possible improvement to the algorithm would
be to derive a method that automatically adapts the leakage
coefﬁcient η to track the leakage of the GSS algorithm.

ACKNOWLEDGMENT

François Michaud holds the Canada Research Chair (CRC)
in Mobile Robotics and Autonomous Intelligent Systems. This
research is supported ﬁnancially by the CRC Program, the
Natural Sciences and Engineering Research Council of Canada
(NSERC) and the Canadian Foundation for Innovation (CFI).
Special thanks to Dominic Létourneau and Serge Caron for
their help in this work.

REFERENCES

[1] Y. Zhang and J. Weng, “Grounded auditory development by a devel-
opmental robot,” in Proc. INNS/IEEE International Joint Conference of
Neural Networks, 2001, pp. 1059–1064.

a)

c)

e)

b)

d)

f)

Figure 4.
with single-channel post-ﬁlter, e) GSS with multi-channel post-ﬁlter, f) reference (clean) signal.

Spectrograms for separation of ﬁrst source (female voice): a) signal at one microphone, b) delay-and-sum beamformer, c) GSS output, d) GSS

[2] Y. Matsusaka, T. Tojo, S. Kubota, K. Furukawa, D. Tamiya, K. Hayata,
Y. Nakano, and T. Kobayashi, “Multi-person conversation via multi-
modal interface - a robot who communicate with multi-user,” in Proc.
EUROSPEECH, 1999, pp. 1723–1726.

[3] K. Nakadai, H. G. Okuno, and H. Kitano, “Real-time sound source lo-
calization and separation for robot audition,” in Proc. IEEE International
Conference on Spoken Language Processing, 2002, pp. 193–196.

[4] H. G. Okuno, K. Nakadai, and H. Kitano, “Social

interaction of
humanoid robot based on audio-visual tracking,” in Proc. of Eighteenth
International Conference on Industrial and Engineering Applications of
Artiﬁcial Intelligence and Expert Systems, 2002, pp. 725–735.

[5] K. Nakadai, D. Matsuura, H. G. Okuno, and H. Kitano, “Applying
scattering theory to robot audition system: Robust sound source local-
ization and extraction,” in Proc. IEEE/RSJ International Conference on
Intelligent Robots and Systems, 2003, pp. 1147–1152.

[6] C. Choi, D. Kong, J. Kim, and S. Bang, “Speech enhancement and
recognition using circular microphone array for service robots,” in Proc.
IEEE/RSJ International Conference on Intelligent Robots and Systems,
2003, pp. 3516–3521.

[7] L. C. Parra and C. V. Alvino, “Geometric source separation: Merg-
ing convolutive source separation with geometric beamforming,” IEEE
Transactions on Speech and Audio Processing, vol. 10, no. 6, pp. 352–
362, 2002.

[8] I. Cohen and B. Berdugo, “Microphone array post-ﬁltering for non-
stationary noise suppression,” in Proc. IEEE International Conference
on Acoustics, Speech, and Signal Processing, 2002, pp. 901–904.

[9] J.-M. Valin, J. Rouat, and F. Michaud, “Microphone array post-ﬁlter
for separation of simultaneous non-stationary sources,” in Proc. IEEE

International Conference on Acoustics, Speech, and Signal Processing,
2004.

[10] J.-M. Valin, F. Michaud, B. Hadjou, and J. Rouat, “Localization of
simultaneous moving sound sources for mobile robot using a frequency-
domain steered beamformer approach,” in Proc. IEEE International
Conference on Robotics and Automation, 2004.

[11] S. Haykin, Adaptive Filter Theory, 4th ed. Prentice Hall, 2002.
[12] Y. Ephraim and D. Malah, “Speech enhancement using minimum mean-
square error short-time spectral amplitude estimator,” IEEE Transactions
on Acoustics, Speech and Signal Processing, vol. ASSP-32, no. 6, pp.
1109–1121, 1984.

[13] ——, “Speech enhancement using minimum mean-square error log-
spectral amplitude estimator,” IEEE Transactions on Acoustics, Speech
and Signal Processing, vol. ASSP-33, no. 2, pp. 443–445, 1985.

[14] R. Zelinski, “A microphone array with adaptive post-ﬁltering for noise
reduction in reverberant rooms,” in Proc. IEEE International Conference
on Acoustics, Speech, and Signal Processing, 1988, pp. 2578–2581.

[15] I. McCowan and H. Bourlard, “Microphone array post-ﬁlter for diffuse
noise ﬁeld,” in Proc. IEEE International Conference on Acoustics,
Speech, and Signal Processing, vol. 1, 2002, pp. 905–908.

[16] I. Cohen and B. Berdugo, “Speech enhancement for non-stationary noise
environments,” Signal Processing, vol. 81, no. 2, pp. 2403–2418, 2001.

