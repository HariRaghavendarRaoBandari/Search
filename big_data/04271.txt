6
1
0
2

 
r
a

 

M
4
1

 
 
]
h
p
-
t
n
a
u
q
[
 
 

1
v
1
7
2
4
0

.

3
0
6
1
:
v
i
X
r
a

INFORMATION GAIN IN REPEATED QUANTUM

MEASUREMENTS

ERKKA HAAPASALO, TEIKO HEINOSAARI, AND YUI KURAMOCHI

Abstract. We study information gain in a sequential measure-
ment scenario where the system is repeatedly subjected to the same
measurement process. We ﬁrst provide examples of such repeated
measurements where all the obtainable information is reached af-
ter a ﬁnite number of repetitions. We also prove, however, that
repeating the L¨uders measurement of an unsharp non-trivial two-
outcome observable yields novel information at each step, and we
characterize the information obtained in the limit of inﬁnitely many
repetitions. Our result implies that a repeated measurement can
be used to correct the inherent noise of an unsharp observable.

1. Introduction

A quantum measurement that gives information necessarily perturbs
the initial state of the measured system. Hence, subsequent mea-
surements on the same system are typically disturbed compared to
the situation without the ﬁrst measurement. Despite the unavoidable
measurement disturbance, we can still hope to get more information
by performing measurements in sequence. For instance, by measuring
sequentially unsharp versions of conjugated observables, we can imple-
ment any covariant phase space observable, hence also an information-
ally complete observable [1],[2]. Properties of sequential measurements
can also be used to study quantum foundations. For instance, the de-
mand for the existence of a sequential product on an eﬀect algebra
excludes certain types of unphysical eﬀect algebras [3],[4].

Given that a sequential measurement of two diﬀerent observables can
give more information than the independent measurements of the same
observables, then how useful can a repetition of the same measurement
on the same system be? A repetition of a von Neumann measurement
just gives the same outcome every time, so a sequence of such measure-
ments cannot give more information than just a single measurement.
However, quantum observables generally also allow less invasive mea-
surements, so the question arises if repeating the same measurement
can give more information than just a single measurement alone.

1

2

ERKKA HAAPASALO, TEIKO HEINOSAARI, AND YUI KURAMOCHI

In the present work, we discuss the information gain of subsequent
measurements applying the same measurement device at each step.
This kind of error correction by repeated measurements has been pro-
posed in a quantum optical setting [5] and has also been performed
experimentally [6]. To properly understand the limits of information
gain in a repeated measurement, we deﬁne the saturation step to be the
least number of repetitions of the measurement after which we obtain
no added information. We discuss cases when a single application of
the measurement gives all the information that can be extracted by
repetition, but also cases when the saturation step is ﬁnite but greater
than one.

Our most surprising ﬁnding is that, in some repeated measurement
setups, saturation is never reached after ﬁnitely many steps but, in
the limit of inﬁnitely many repetitions, we may correct the inherent
noise of the original observable produced by a single application of the
measurement process. We show that this is the case for the L¨uders
measurement of a binary observable (apart from trivial cases), and
we characterize the information gain of the inﬁnite consecutive L¨uders
measurement.

2. Preliminaries

A measurement can be described at diﬀerent levels. If we are only in-
terested in measurement outcome probabilities, description at the level
of observables suﬃces.
If we, in addition, discuss conditional state
transformation caused by the measurement, then the relevant concept
is that of an instrument. We start by recalling the mathematical deﬁ-
nitions of these two concepts. For more details, we refer to [7], [8], [9],
[10].
Notation. In the rest of the paper H is a ﬁxed Hilbert space. The
dimension of H can be either ﬁnite or countably inﬁnite. We denote
by L(H) the set of all bounded operators on H.

(cid:80)

2.1. Observables and post-processing. A quantum observable with
ﬁnite number of outcomes is described by a map ω (cid:55)→ A(ω) from a ﬁ-
nite set ΩA to L(H) such that each A(ω) is a positive operator and
ω A(ω) = 1, where 1 is the identity operator on H. An observable A

is called sharp if each operator A(ω) is a projection.

Since we need to compare two observables, we recall the following
post-processing preorder of observables [11, 12, 13]. Given two observ-
ables A and B, we denote A (cid:22) B if there exists a map κ(·|·) : ΩA×ΩB →

[0, 1] such that

for all ω(cid:48) ∈ ΩB and

κ(ω|ω(cid:48)) = 1

(cid:88)
(cid:88)

ω

3

(1)

(3)

(4)

κ(ω|ω(cid:48))B(ω(cid:48))

ω(cid:48)

A(ω) =

(2)
for all ω ∈ ΩA. Further, we denote A (cid:39) B if A (cid:22) B (cid:22) A, and A ≺ B if
A (cid:22) B but not B (cid:22) A.
A map κ(·|·) : ΩA × ΩB → [0, 1] satisfying (1) is called a Markov
kernel. Note that Markov kernels deﬁned on a ﬁxed product space
ΩA × ΩB form a convex set. A special class of Markov kernels are
related to relabelings, where we relabel the measurement outcomes,
possibly giving the same label for diﬀerent outcomes and hence merging
them, but doing nothing else. If we start from an outcome space ΩB, a
relabeling is determined by a function f : ΩB → ΩA. The corresponding
Markov kernel κf is then

and the post-processed observable is A = B ◦ f−1, i.e.,

κf (ω|ω(cid:48)) = δω,f (ω(cid:48)) ,

(cid:88)

A(ω) =

B(ω(cid:48)) .

ω(cid:48):f (ω(cid:48))=ω

Iω is a normal completely positive map on L(H) and(cid:80)

2.2. Instruments and their composition. A quantum instrument,
deﬁned in the Heisenberg picture, is a function ω (cid:55)→ Iω such that each
ω Iω(1) = 1.
The probability of getting a measurement outcome ω in an initial state
 is tr [Iω(1)], hence the observable AI determined by I is given as
(5)
An instrument I describes not only measurement outcome probabil-
ities but also the conditional state transformation caused by the mea-
surement process. As we use the Heisenberg picture, this is reﬂected
in the transformation of observables. If we measure ﬁrst I and then
some other observable B on the same system, we obtain measurement
outcomes ω and ω(cid:48) with the probability tr [Iω(B(ω(cid:48)))].

AI(ω) = Iω(1) .

If we perform subsequently more than two measurements, then we
need to form a composition of instruments [14],[15]. Suppose we have
two instruments I and J . We denote by Iω ◦ Jω(cid:48) the functional com-
position of maps Iω and Jω(cid:48), i.e.,

Iω ◦ Jω(cid:48)(T ) = Iω(Jω(cid:48)(T )) .

(6)

4

ERKKA HAAPASALO, TEIKO HEINOSAARI, AND YUI KURAMOCHI

Figure 1. The measurement outcome is a ﬁnite se-
quence (ω1, . . . , ωn) and its probability is determined by
the observable AI

n and the initial state .

The order of the instruments in the composition Iω ◦ Jω(cid:48) is such that
the instrument I has been applied to the input state ﬁrst.

3. Repeated measurement

Let I be the instrument describing a measurement device. We will
now consider a repeated measurement scheme where we use the same
measurement device repeatedly on the same system (Fig. 1).
If we
repeat the measurement n times, then the measurement outcome of
the whole procedure is an element from the Cartesian product Ωn, and
the probability of obtaining a particular n-tuple (ω1, . . . , ωn) is

tr [Iω1 ◦ ··· ◦ Iωn(1)] .

We conclude that the observable AI
of I is given as

n corresponding to the n repetitions

n(ω1, . . . , ωn) = Iω1 ◦ ··· ◦ Iωn(1) ,
AI

and if we still make one more measurement round the related observable
AI
n+1 is given by the formula

Since(cid:80)

n+1(ω1, . . . , ωn+1) = Iω1 ◦ ··· ◦ Iωn+1(1)
AI
(cid:88)

= Iω1(AI

ω Iω(1) = 1, we obtain

AI
n+1(ω1, . . . , ωn+1) = AI

n(ω1, . . . , ωn) .

n(ω2, . . . , ωn+1)) .

ωn+1

(7)

(8)

(9)

Thus, according to (4), An is a relabeling of An+1, so that An (cid:22) An+1.
Therefore, the information achievable with repeated uses of the instru-
ment I is described by the sequence
2 (cid:22) AI

3 (cid:22) ··· .

1 (cid:22) AI
AI

(10)

5

n (cid:39) AI

n+1 for some n ∈ N, then AI

The properties of this sequence will be the focus of our investigation.
We start with a simple but important observation.
Proposition 1. If AI
m ≥ n.
Proof. Let us assume that the claim holds for each m = n, . . . , k for
some k ∈ N, k > n. Hence, there is a Markov kernel κ(·|·) : Ωk
A ×
(cid:88)
A → [0, 1] such that
Ωk−1
AI
k (ω1, . . . , ωk) =

κ(ω1, . . . , ωk|ω(cid:48)

1, . . . , ω(cid:48)

1, . . . , ω(cid:48)

n (cid:39) AI

k−1)AI

k−1(ω(cid:48)

m for all

k−1).

ω(cid:48)
1,...,ω(cid:48)

k−1

(cid:0)AI

k−1)(cid:1)

κ(ω2, . . . , ωk+1|ω(cid:48)

1, . . . , ω(cid:48)

k−1)Iω1

k−1(ω(cid:48)

1, . . . , ω(cid:48)

κ(ω2, . . . , ωk+1|ω(cid:48)

1, . . . , ω(cid:48)

k−1)AI

k (ω1, ω(cid:48)

1, . . . , ω(cid:48)

k−1)

We may write

(cid:0)AI
k (ω2, . . . , ωk+1)(cid:1)
AI
k+1(ω1, . . . , ωk+1)
(cid:88)
= Iω1
(cid:88)
=
(cid:88)

ω(cid:48)
1,...,ω(cid:48)

ω(cid:48)
1,...,ω(cid:48)

k−1

k−1

=

=

ω(cid:48)
1,...,ω(cid:48)

k

where

˜κ(ω1, . . . , ωk+1|ω(cid:48)

1, . . . , ω(cid:48)

k)AI

k (ω(cid:48)

1, . . . , ω(cid:48)
k),

˜κ(ω1, . . . , ωk+1|ω(cid:48)

1, . . . , ω(cid:48)

k) = δω1,ω(cid:48)

1

κ(ω2, . . . , ωk+1|ω(cid:48)

2, . . . , ω(cid:48)
k)

is a Markov kernel. Hence also AI
always holds, we have AI
all m.

k+1 (cid:39) AI

k+1 (cid:22) AI
k (cid:39) ··· (cid:39) AI

k and, since the converse
n. Thus the claim holds for
(cid:3)

The previous result makes the following notion meaningful.

Deﬁnition 1. Let I be an instrument and let {AI
of observables deﬁned as

j }∞
n(ω1, . . . , ωn) = Iω1 ◦ ··· ◦ Iωn(1) .
AI
n ≺ AI

(11)
We say that the saturation step of I, denoted by s(I), is the smallest
n+1 for all n ∈ N,
positive integer n such that AI
then we denote s(I) = ∞.

j=1 be the sequence

n+1. If AI

n (cid:39) AI

6

ERKKA HAAPASALO, TEIKO HEINOSAARI, AND YUI KURAMOCHI

We say that an instrument I saturates at step n if n = s(I) < ∞.

4. Instruments that saturate at the first step

4.1. Repeatable instrument. An instrument I is called repeatable
if in the second repetition step we get the same measurement outcome
as in the ﬁrst step, with certainty [14], [16].
It is quite clear from
this deﬁnition that a repeatable instrument cannot give any further
information in additional repetitions. We will next show this using our
framework.
Repeatability means that the probability of getting two diﬀerent
outcomes in subsequent measurements is zero, so for ω1 (cid:54)= ω2 we must
have

tr [Iω1 ◦ Iω2(1)] = 0

in all initial states . It follows that

Iω1 ◦ Iω2(1) = δω1ω2Iω1(1) .
In terms of the corresponding observables we have

(cid:88)

ω

δωω1δω1ω2AI(ω) ,

AI
2 (ω1, ω2) = δω1ω2AI(ω1) =
(14)
2 (cid:39) AI. In conclusion, if an instrument I is repeatable,

and hence AI
then s(I) = 1.
4.2. Preparative instrument. Suppose we perform a measurement
of an observable A. After the measurement we prepare a state ηω
depending on the outcome ω. The corresponding instrument is given
by

Iω(T ) = tr [ηωT ] A(ω),

ω ∈ ΩA, T ∈ L(H) ,

and we say that I is a preparative instrument. We have
2 (ω1, ω2) = Iω1 ◦ Iω2(1) = tr [ηω1A(ω2)] A(ω1)
AI

(12)

(13)

(cid:88)

ω

=

δωω1tr [ηωA(ω2)] A(ω) ,

2 (cid:39) A. Therefore, s(I) = 1.

and hence AI
4.3. Mixtures. Let I and J be two instruments related to an observ-
able A. For each 0 ≤ t ≤ 1, we form their convex mixture tI + (1− t)J
by deﬁning

(tI + (1 − t)J )ω(T ) = tIω(T ) + (1 − t)Jω(T )

for all ω ∈ ΩA and T ∈ L(H). Since

(tI + (1 − t)J )ω(1) = tA(ω) + (1 − t)A(ω) = A(ω) ,

(15)

(16)

this instrument determines the same observable A as well.
Proposition 2. If s(I) = s(J ) = 1, then s(tI + (1 − t)J ) = 1.
Proof. Assume that s(I) = s(J ) = 1, hence AI
follows that there are Markov kernels κ and κ(cid:48) such that

2 (cid:39) A and AJ

2 (cid:39) A. It

and

AI
2 (ω1, ω2) =

AJ
2 (ω1, ω2) =

κ(ω1, ω2|ω)A(ω)

κ(cid:48)(ω1, ω2|ω)A(ω) .

(cid:88)
(cid:88)

ω

7

(17)

(18)

(19)

(20)

We have

AtI+(1−t)J

2

ω

(ω1, ω2) = (tI + (1 − t)J )ω1(A(ω2))
(cid:88)
2 (ω1, ω2) + (1 − t)AJ
(tκ(ω1, ω2|ω) + (1 − t)κ(cid:48)(ω1, ω2|ω))A(ω) ,

= tAI
=

2 (ω1, ω2)

thus AtI+(1−t)J

2

(cid:22) A. Therefore, s(tI + (1 − t)J ) = 1.

(cid:3)

ω

5. Instruments that saturate at a finite step n > 1

Let H be a Hilbert space with a ﬁnite dimension d ≥ 2 and let
k=1 be an orthonormal basis of H. We deﬁne an instrument I with

{φk}d
an outcome set Ω = {0, 1} by

where

L0 := |φd(cid:105)(cid:104)φd| , L1 :=

|φk+1(cid:105)(cid:104)φk| .

Iω(T ) := L∗

ωT Lω ,

d−1(cid:88)

k=1

Then we have the following:
Proposition 3. (a) If 1 ≤ n ≤ d − 1, then AI

n (cid:39) Pn, where Pn is the

sharp observable deﬁned by
|φl(cid:105)(cid:104)φl|,

Pn(j) :=

d−n(cid:88)

l=1

|φd−n+j−1(cid:105)(cid:104)φd−n+j−1|,

(j = 1);
(2 ≤ j ≤ n + 1).

(b) If d − 1 ≤ n, then AI

n (cid:39) Pd−1, where Pd−1 is the sharp observable

deﬁned by

Pd−1(j) := |φj(cid:105)(cid:104)φj|

(1 ≤ j ≤ n).

8

ERKKA HAAPASALO, TEIKO HEINOSAARI, AND YUI KURAMOCHI

Proof. (a) Assume that 1 ≤ n ≤ d−1. Operators of the form LωnLωn−1 ··· Lω1

are calculated to be

Ln

1 =

|φl+n(cid:105)(cid:104)φl|,

d−n(cid:88)

l=1

(1 ≤ j ≤ n),

0Ln−j
Lj

1 = |φd−n+j(cid:105)(cid:104)φd−n+j|
d−n(cid:88)

and the others vanish since L1L0 = 0. Thus the nonzero operators
An(ω1,··· , ωn) are given by

(cid:124) (cid:123)(cid:122) (cid:125)

An( 1,··· , 1
n − j elements

An(1,··· , 1) =
, 0,··· , 0

|φl(cid:105)(cid:104)φl|,
) = |φd−n+j(cid:105)(cid:104)φd−n+j|,

(cid:124) (cid:123)(cid:122) (cid:125)

l=1

j elements

(1 ≤ j ≤ n).

(b) Let us assume d − 1 ≤ n. It is suﬃcient to show that Ad−1 (cid:39) Ad.

Hence we obtain An (cid:39) Pn.
Since Ad−1 (cid:39) Pd−1, Ad is equivalent to an observable given by

Iω(Pd−1(j))

(ω ∈ {0, 1}, 1 ≤ j ≤ d).

The non-vanishing elements of this observable are given by

I0(Pd−1(d)) = Pd−1(d),

I1(Pd−1(j)) = Pd−1(j − 1)

(2 ≤ j ≤ d).

Thus we obtain Ad (cid:39) Pd−1 (cid:39) Ad−1, which completes the proof. (cid:3)
Looking at the form of the observables Pj, it is clear that

P1 ≺ P2 ≺ ··· ≺ Pd−2 ≺ Pd−1 .

Hence, from Prop. 3 we conclude that

1 ≺ AI
AI

2 ≺ ··· ≺ AI

d−2 ≺ AI

d−1 (cid:39) AI

d (cid:39) ···

and therefore s(I) = d − 1.

6. Instruments with s(I) = ∞

6.1. Observable AI
∞. In the following we consider instruments with
inﬁnite saturation steps. For such cases it is convenient to consider
the inﬁnite consecutive measurement of I for which we have to study
observables whose outcomes are elements of more general measurable
spaces. The generalized deﬁnition of the observable is as follows. Let
(Ω,B(Ω)) be a measurable space modelling the physical outcome space.
A mapping A : B(Ω) → L(H) is an observable, or a positive-operator-
valued measure (POVM), if A(E) is positive for any E ∈ B(Ω), A(Ω) =

1, and A(∪jEj) =(cid:80)

9

j A(Ej) (in the weak operator topology) for any
j=1 ⊂ B(Ω). The triple (Ω,B(Ω), A) is also called
disjoint sequence {Ej}∞
an observable. For simplicity we will assume that the outcome spaces
of observables are standard Borel spaces.
The equivalence relation by classical post-processing is also general-
ized as follows [17]: Let (ΩA,B(ΩA), A) and (ΩB,B(ΩB), B) be observ-
ables. We denote A (cid:22) B if there exists a map κ(·|·) : B(ΩA)×ΩB → [0, 1]
such that κ(·|ω(cid:48)) is a probability measure on (ΩA,B(ΩA)) for each
ω(cid:48) ∈ ΩB, κ(E|·) is B(ΩB)-measurable for each E ∈ B(ΩA), and

(cid:90)

Ω2

A(E) =

κ(E|ω2)dB(ω2)

for each E ∈ B(Ω1). As before, we denote A (cid:39) B if A (cid:22) B and B (cid:22) A,
and A ≺ B if A (cid:22) B but not B (cid:22) A. The map κ(·|·) is called a Markov
kernel.
Let Ω be a ﬁnite set and let I be an instrument on Ω. We de-
ﬁne an observable AI
∞ with the countable Cartesian product space
(Ω∞,B(Ω∞)), called the inﬁnite composition of I, by

n(ω1, . . . , ωn)

∞({ω1} × ··· × {ωn} × Ω∞) = AI
AI

(21)
for each n ≥ 1 and (ω1, . . . , ωn) ∈ Ωn, where AI
n is given by (8) and
B(Ω∞) is the σ-algebra generated by the cylinder sets {ω1} × ··· ×
{ωn} × Ω∞. The existence and uniqueness of AI
∞ satisfying (21) can
be proved [18] by using a POVM version of the Kolmogorov extension
theorem [19]. Physically speaking, the observable AI
∞ corresponds to
the inﬁnite consecutive measurement of I.
From the deﬁnition of the inﬁnite composition (21), we see that
n (cid:22) AI
AI
Theorem 1. Let I be an instrument with a ﬁnite outcome space Ω,
and let AI
∞ be observables given by (8) and (21),
respectively. Then we have

∞ for any n ≥ 1. Furthermore, we have the following:

n (n ≥ 1) and AI

n (cid:39) AI

s(I) = inf{n ∈ N|AI

∞}.
Proof. We denote the RHS of (22) as s(I)(cid:48). From
∞ (cid:39) AI
s(I)(cid:48)

s(I)(cid:48)+1 (cid:22) AI

s(I)(cid:48) (cid:22) AI
AI
(23)
s(I)(cid:48) (cid:39) AI
s(I)(cid:48)+1 and thus s(I) ≤ s(I)(cid:48). In order to show
we have AI
the converse inequality, it is suﬃcient to prove AI
s(I). Without
loss of generality, we may assume s(I) < ∞. Since the observable
corresponding to a consecutive measurement of I followed by AI
s(I) is
∞ (cid:22) AI
equivalent to AI
s(I),
(cid:3)
and the assertion holds.

s(I) itself, from Theorem 4 of [18], we obtain AI

∞ (cid:22) AI

(22)

10

ERKKA HAAPASALO, TEIKO HEINOSAARI, AND YUI KURAMOCHI

6.2. L¨uders instrument of a two-outcome observable. Let A ∈
L(H) be an eﬀect, i.e. 0 ≤ A ≤ 1, and let I be an instrument with the
outcome space Ω = {0, 1} and deﬁned by

I0(T ) =

√
1 − AT

√

1 − A,

√

√

I1(T ) =

(24)
for all T ∈ L(H). Let (Ω∞,B(Ω∞)) be the countable product space
of Ω, and let AI
∞ be the observables for the consecutive measure-
ment processes deﬁned by (8) and (21), respectively. Let ([0, 1],B([0, 1]), PA)
be the spectral measure of A such that

n and AI

AT

A

(cid:90)

A =

λdPA(λ),

where B([0, 1]) is the σ-algebra generated by the intervals. Then we
have the following:

[0,1]

Theorem 2.

Proof. We ﬁrst show AI
[0, 1] → [0, 1] by

∞ (cid:39) PA.
AI

(25)
∞ (cid:22) PA. We deﬁne a Markov kernel κ1(·|·) : 2Ω×

κ1({1}|λ) = λ ,

κ1({0}|λ) = 1 − λ,

0 ≤ λ ≤ 1,

and we also deﬁne a probability measure κ∞(·|λ) on (Ω∞,B(Ω∞) by
the inﬁnite direct product of κ1(·|λ). Since κ1({ω1}|·) is measurable, the
Dynkin class theorem assures that [0, 1] (cid:51) λ (cid:55)→ κ∞(E|λ) is measurable
for each E ∈ B(Ω∞), and therefore κ∞(·|·) is a Markov kernel. For
each n ≥ 1 and each (ωi)n
∞({ω1} × ··· × {ωn} × Ω∞) = Iω1 ◦ ··· ◦ Iωn(1)
AI
i=1 ωi(1 − A)n−(cid:80)n
(cid:80)n
(cid:80)n
i=1 ωi(1 − λ)n−(cid:80)n
κ∞({ω1} × ··· × {ωn} × Ω∞|λ)dPA(λ),

i=1 ∈ Ωn, we have

i=1 ωidPA(λ)

(cid:90)
(cid:90)

= A

i=1 ωi

[0,1]

=

=

λ

implying

AI
∞(E) =
for all E ∈ B(Ω∞). Hence AI
In order to show PA (cid:22) AI
ables Xn : Ω∞ → [0, 1] by

[0,1]

(cid:90)
∞ (cid:22) PA.
∞, we deﬁne a sequence of stochastic vari-

κ∞(E|λ)dPA(λ)

[0,1]

n(cid:88)

i=1

ωi,

Xn(ω) :=

1
n

X∞(ω) =

n→∞ Xn(ω)
0

if the limit exists;

otherwise,

then X∞(ω) = λ κ∞(·|λ)-almost surely. Thus for each Borel subset E
of [0, 1], we have

χE(X∞(ω))dAI

∞(ω) = AI

∞(X−1∞ (E))

(cid:90)

Ω∞

(cid:40) lim

(cid:90)
(cid:90)
(cid:90)
(cid:90)

=

=

=

=

where ω := (ωi)∞
i=1. The strong law of large numbers implies that Xn(ω)
converges to λ κ∞(·|λ)-almost surely for every λ ∈ [0, 1]. If we deﬁne a
stochastic variable X∞ : Ω∞ → [0, 1] by

11

[0,1]

[0,1]

(cid:19)

(cid:18)(cid:90)
κ∞(X−1∞ (E)|λ)dPA(λ)
(cid:19)
(cid:18)(cid:90)

χE(X∞(ω))κ∞(dω|λ)

Ω∞

χE(λ)κ∞(dω|λ)

dPA(λ)

[0,1]

Ω∞

dPA(λ)

χE(λ)dPA(λ)

[0,1]

= PA(E),

where χE(·) is the indicator function. The above equation implies the
desired relation PA (cid:22) AI
(cid:3)

∞, which completes the proof.

The next theorem states that the L¨uders instrument of (24) does
not saturate after any ﬁnite number of steps except for in a couple of
trivial cases.
Theorem 3. Let A be an eﬀect and I the associated L¨uders instrument
given by (24).
(a) If A is a projection, then s(I) = 1.
(b) If A = λ1 for some 0 ≤ λ ≤ 1, then s(I) = 1.
(c) In all other cases s(I) = ∞.
Proof. The assertion (a) follows from the repeatability of the instru-
ment I. When A = λ1, we have AI
∞ (cid:39) PA (cid:39) {1} and the assertion (b)
is obvious.
Now we prove the assertion (c), for which it is suﬃcient to show
n ≺ PA for each n ≥ 1. By the assumption, there exist λ1 and λ2 in
AI
the specturm σ(A) of A such that

0 < λ1 < 1,

λ1 (cid:54)= λ2.

12

ERKKA HAAPASALO, TEIKO HEINOSAARI, AND YUI KURAMOCHI

We ﬁx  > 0 such that  < λ1 < 1 −  and

[λ1 − , λ1 + ] ∩ [λ2 − , λ2 + ] = ∅ .

Since λ1, λ2 ∈ σ(A), the operators

P1 := PA([λ1 − , λ1 + ]), P2 := PA([λ2 − , λ2 + ])

are nonzero orthogonal projections. Hence, there exist normalized vec-
tors ψ1 and ψ2 such that Piψi = ψi (i = 1, 2). We will show that

where H 2(·,·) is the square of the Hellinger distance deﬁned by

H 2(ΠAI

n
ψ1

, ΠAI

n
ψ2

H 2(p, q) :=

1
2

Ω(cid:48)

(cid:90)

) < H 2(ΠPA

ψ1 , ΠPA

ψ2 ) = 1,

(cid:16)(cid:112)dp(ω(cid:48)) −(cid:112)dq(ω(cid:48))
(cid:17)2

(26)

for arbitrary probability measures p and q on a measurable space
(Ω(cid:48),B(Ω(cid:48))), and

ψ (ω) := (cid:104)ψ|AI
ΠAI

n(ω)|ψ(cid:105), ΠPA

ψ (E) := (cid:104)ψ|PA(E)|ψ(cid:105),

n

n

n

ψ , ΠPA

ψ , ΠAI

ψ(cid:48) ) = H 2(ΠPA

Now we prove (26). Since ΠPA
ψ1

are the distributions of the corresponding measurement outcomes when
n (cid:39) PA implies
the system is prepared in vector state ψ. Since AI
H 2(ΠAI
ψ(cid:48) ) for all unit vectors ψ and ψ(cid:48) [20], the
claim immediately follows from (26).
are concentrated on disjoint
intervals [λ1−, λ1 +] and [λ2−, λ2 +], respectively, they are singular
to each other and hence H 2(ΠPA
) = 1. On the other hand, for each
i=1 ∈ Ωn we have
ψ1
ω = (ωi)n
(cid:80)n
i=1 ωi(1 − λ)n−(cid:80)n
ΠAI

and ΠPA
ψ2

i=1 ωidΠPA

, ΠPA
ψ2

(ω) =

λ

ψ1 (λ)

n
ψ1

(cid:90)
(cid:90)

[0,1]

=

Therefore,

[λ1−,λ1+]

H 2(ΠAI

n
ψ1

, ΠAI

n
ψ2

) =

λ

(cid:80)n
i=1 ωi(1 − λ)n−(cid:80)n
(cid:18)(cid:113)
(ω) −(cid:113)
(cid:88)
(cid:113)
= 1 − (cid:88)

(ω)ΠAI

ΠAI

ΠAI

ω∈Ωn

1
2

n
ψ1

n
ψ1

ω∈Ωn

i=1 ωidΠPA

ψ1 (λ) (cid:54)= 0.
(cid:19)2

ΠAI

n
ψ2

(ω)

(ω) < 1.

n
ψ2

Thus we obtain (26), which completes the proof.

(cid:3)

7. Discussion

13

In our investigation we have given examples of instruments with
the saturation steps 1, n ∈ N and ∞. The natural question is to
characterize these types of instruments. For instance, we do not yet
know if the mixtures of repeatable and preparative instruments are
the only instruments with the saturation step 1. Moreover, it is still
unsure how the saturation step of a mixture of instruments relates
to the saturation steps of the component instruments in the convex
decomposition.

We proved that repeated measurements can be used to correct the
noise in an unsharp two-outcome observable.
It remains to be seen
if a similar error correction is possible with other measurements than
those corresponding to the L¨uders instruments. We hope that this work
stimulates further theoretical and experimental investigations into the
full information-yielding potential of repeated uses of a single quantum
instrument.

References

[1] C. Carmeli, T. Heinosaari, and A. Toigo. Sequential measurements of conjugate

observables. J. Phys. A: Math. Theor., 44:285304, 2011.

[2] C. Carmeli, T. Heinosaari, and A. Toigo. Informationally complete joint mea-

surements on ﬁnite quantum systems. Phys. Rev. A, 85:012109, 2012.

[3] S. Gudder and R. Greechie. Sequential products on eﬀect algebras. Rep. Math.

Phys., 49:87–111, 2002.

[4] A. Gheondea and S. Gudder. Sequential products of quantum eﬀects. Proceed-

ings of AMS, 132:503-512, 2003.

[5] M. Brune, S. Haroche, J.M. Raimond, L. Davidovich, and N. Zagury. Ma-
nipulation of photons in a cavity by dispersive atom-ﬁeld coupling: Quantum-
nondemolition measurements and generation of “Schr¨odinger cat” states. Phys.
Rev. A, 45:5193–5214, 1992.

[6] C. Guerlin, J. Bernu, S. Del´eglise, C. Sayrin, S. Gleyzes, S. Kuhr, M. Brune,
J.M. Raimond, and S. Haroche. Progressive ﬁeld-state collapse and quantum
non-demolition photon counting. Nature, 448:889–894, 2007.

[7] A.S. Holevo. Probabilistic and Statistical Aspects of Quantum Theory. North-

Holland Publishing Co., Amsterdam, 1982.

[8] P. Busch, M. Grabowski, and P.J. Lahti. Operational Quantum Physics.

Springer-Verlag, Berlin, 1997. second corrected printing.

[9] T. Heinosaari and M. Ziman. The Mathematical Language of Quantum Theory.

Cambridge University Press, Cambridge, 2012.

[10] M. Ozawa. Mathematical foundations of quantum information: Measurement
and foundations. Sugaku Expositions 27 (2), 195–221 (2014). arXiv:1201.5334
[quant-ph]; originally published in Sugaku 61 (2), 113-132 (2009) (in Japanese).
[11] H. Martens and W.M. de Muynck. Nonideal quantum measurements. Found.

Phys., 20:255–281, 1990.

14

ERKKA HAAPASALO, TEIKO HEINOSAARI, AND YUI KURAMOCHI

[12] T. Heinonen. Optimal measurements in quantum mechanics. Phys. Lett. A,

346:77–86, 2005.

[13] F. Buscemi, G.M. D’Ariano, M. Keyl, P. Perinotti, and R.F. Werner. Clean

positive operator valued measures. J. Math. Phys., 46:082109, 2005.

[14] E.B. Davies and J.T. Lewis. An operational approach to quantum probability.

Comm. Math. Phys., 17:239–260, 1970.

[15] P. Busch, G. Cassinelli, and P. Lahti. On the quantum theory of sequential

measurements. Found. Phys., 20:757–775, 1990.

[16] M. Ozawa. Quantum measuring processes of continuous observables. J. Math.

Phys., 25:79–87, 1984.

[17] S.V. Dorofeev and J. de Graaf. Some maximality results for eﬀect-valued mea-

sures. Indag. Mathem., N.S., 8:349–369, 1997.

[18] Y. Kuramochi. Construction of the least informative observable conserved by

a given quantum instrument. J. Math. Phys., 56:092202, 2015.

[19] R. Tumulka. A Kolmogorov extension theorem for POVMs. Lett. Math. Phys.,

84:41–46, 2008.

[20] A. Jenˇcov´a, S. Pulmannov´a, and E. Vincekov´a. Sharp and fuzzy observables

on eﬀect algebras. Int. J. Theor. Phys., 47:125–148, 2008.

