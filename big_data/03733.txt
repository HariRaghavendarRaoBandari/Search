6
1
0
2

 
r
a

 

M
1
1

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
3
3
7
3
0

.

3
0
6
1
:
v
i
X
r
a

MUTUAL CONDITIONAL INDEPENDENCE AND INFERENCE IN MARKOV NETWORKS

Mutual Conditional Independence and Its Applications to Inference in

Markov Networks

Niharika Gauraha
Systems Science and Informatics Unit

Indian Statistical Institute

8th Mile, Mysore Road Bangalore, India

Editor:

NIHARIKA.GAURAHA@GMAIL.COM

Abstract

The fundamental concepts underlying in Markov networks are the conditional independence
and the set of rules called Markov properties that translates conditional independence con-
straints into graphs. In this article we introduce the concept of mutual conditional indepen-
dence relationship among elements of an independent set of a Markov network. We ﬁrst prove
that the mutual conditional independence property holds within the elements of a maximal in-
dependent set afterwards we prove equivalence between the set of mutual conditional indepen-
dence relations encoded by all the maximal independent sets and the three Markov properties(pair-
wise, local and the global) under certain regularity conditions. The proof employs diverse meth-
ods involving graphoid axioms, factorization of the joint probability density functions and the
graph theory. We present inference methods for decomposable and non-decomposable graph-
ical models exploiting newly revealed mutual conditional independence property.

Keywords: Markov Networks, Mutual Conditional Independence, Graphical Models

1. Introduction

A Markov network is a way of specifying conditional independence constraints between com-
ponents of a multivariate distribution. Markov properties are the set of rules that determine
how conditional independence constraints is translated into a graph. For details on Markov net-
works we refer the reader to Lauritzen (1996) and Jordan (2004). The three Markov properties
usually considered for Markov networks are pairwise, local and the global Markov properties.
These Markov properties are equivalent to one another for positive distributions, for details on
equivalence of Markov properties see Matus (1992).

In an undirected graph, an independent set consists of mutually non-adjacent vertices or
equivalently the elements of an independent set are mutually separated by the rest. For example
let G = {V, E} be an undirected graph and let I = {V1, V2, ..., Vk } be an independent set of G then the
vertices {V1, V2, ..., Vk } are mutually separated by {V \ I}.

We extend the notion of similarity between separation in graph and conditional indepen-
dence in probability to similarity between the mutual separation in graph and the mutual con-
ditional independence in probability. The proof involves various methods from different disci-
plines; graphoid axioms, probability theory and the graph theory.

Using graph theoretic concepts, we ﬁrst prove that All the Maximal Independent Sets(AMIS)
uniquely determine the graph. There is one-to-one relationship between graphs and AMIS. Then

1

NIHARIKA

by applying probability theory concepts we prove that Mutual Conditional Independence Prop-
erty(MCIP) holds among the elements of a maximal independent set. Since for any Markov net-
work there will be a unique set of AMIS and hence a unique set of mutual conditional indepen-
dence relations. Considering all mutual conditional independence relations obtained by AMIS,
we derive an alternative formulation for the three Markov properties. Then we prove equivalence
between the mutual conditional independence property and Markov properties, under positive
distribution assumption.

Finally we shift our focus to the problem of probabilistic inference in Markov networks. In a
multivariate set up, inference is the problem of computing a conditional probability distribution
for the set of components where the values for some of the components are given, for details on
inference on graphical models we refer to Wainwright and Jordan (2008).

We introduce inference methods that take the MCIP of the model into account. It provides
quick answers to the queries by ﬁltering on speciﬁc criteria. For example let G be any Markov
network graph, let U represent the set of unobserved components and O represent the set of
observed components or evidence and we wish to compute conditional probability of U given O.
The simplest possible inference is that suppose U forms an independent set and corresponding
separator set V \ U is same as set of evidence O, where V is a set of vertices then it is straight
forward to conclude that given the set of evidence O the elements of U are mutually conditionally
independent. We note that the time complexity for checking whether a set of vertices form an
independent set is linear in terms of number of vertices in the graph.

Our approach of the inference based on MCIP will be very useful for non-decomposable
models where a closed form solution does not exist and for applications where it is more desir-
able to examine relationship among components than computing probabilities such as analysis
of categorical data and gene expression arrays.

The discussion below is organized as follows. In Section 2 we start with brief overview and
mathematical foundations of the theory of Markov networks. Section 3 is concerned with prov-
ing that MCIP holds within the elements of an independent set. Section 4 involves deriving the
global Markov property using the MCIP and proving equivalence between them. In section 5
we discuss some applications of mutual conditional independence relations in terms of statis-
tical inferences for decomposable and non-decomposable graphical models. In section 6 we
give computational details that we used for statistical inferences. In Section 7 we conclude and
discuss future scope and applicability of MCIP.

2. Overview and Mathematical Foundations

This section gives a general overview and mathematical foundations of Markov networks.

A graphical model is a technique for representation of the conditional independencies be-
tween variables in a multivariate probability distribution. The nodes or vertices in the graph cor-
respond to random variables. The absence of an edge between two random variables denotes a
conditional independence relation between them. In the literature several classes of graphs with
various conditional independence interpretations have been described. Undirected graphical
models (Markov Network) and directed acyclic graphs based graphical models(Bayesian Net-
works) are the most commonly known graphical models. In this article we only consider undi-
rected graphical models, also known as Markov random ﬁelds or Markov networks. For details on

2

MUTUAL CONDITIONAL INDEPENDENCE AND INFERENCE IN MARKOV NETWORKS

the foundation of the theory of Markov networks we refer to Lauritzen (1996) , Whittaker (1990),
Preston (1974) andSpitzer (1971).

2.1 Graph Theory

This section provides necessary concepts and deﬁnition of the graph theory that we will be using
in later sections. For details on graph theory for graphical models we refer to Lauritzen (1996).

NOTATIONS

A graph G, is a pair G = (V, E), where V is a set of vertices and E is a set of edges.

Deﬁnition 1 (Undirected Graphs) A graph is said to be an undirected graph if its vertices are con-
nected by undirected edges. We consider only simple graph that has neither self loops nor multiple
edges.

Deﬁnition 2 (Maximal Independent set) An independent set of a graph G is a subset S of nodes
such that no two nodes in S are adjacent. An independent set is said to be maximal if no node can
be added to S without violating independent set property.

Theorem 3 (Uniqueness of Maximal Independent Sets) Given the complete list of maximal in-
dependent sets of a graph, the graph is uniquely determined.

Proof Let C1, C2, ...Cm be the complete list of the maximal independent sets of nodes of a graph,
then we have to show that the node set V is the union V = C1 ∪ C2 · · · ∪ Cm and the edge set E
consists of all unordered pairs {x, y} of distinct elements of V such that {x, y} is not contained in
any of the Ci .

Clearly V contains the above union. Conversely, if x ∈ V then {x} is an independent set and
hence is contained in some maximal independent set Ci and then x ∈ Ci and hence x belongs to
the union. This determines V.

Also it is clear that any edge {x, y} (where x, y are distinct elements of V) is not contained in
any Ci . Conversely, if {x, y} is a pair of nodes which is not an edge, then {x, y} is an independent
set and hence is contained in some Ci . Thus edge set E is also uniquely determined, as stated.

2.2 Conditional Independence

In this section we deﬁne conditional independence in probability and Markov properties for
Markov networks.

Deﬁnition 4 (Conditional Independence) If X, Y, Z are random variables with joint distribution
P. Random variables X and Y are said to be conditionally independent given the random variable
Z if following holds.

X ⊥⊥ Y | Z ⇐⇒ P(X, Y | Z) = P(X | Z)P(Y | Z)

⇐⇒ p(X | Y, Z) = p(X | Z)

3

NIHARIKA

2.3 Properties of Conditional Independence(graphoid axioms)

We deﬁne some properties of conditional independence in terms of graphoid axioms as follows.

Symmetry: X ⊥⊥ Y | Z =⇒ Y ⊥⊥ X | Z

Decomposition: X ⊥⊥ (Y ∪ W) | Z =⇒ X ⊥⊥ Y | Z

Weak Union: X ⊥⊥ (Y ∪ W) | Z =⇒ X ⊥⊥ Y | (Z ∪ W)

Contraction: X ⊥⊥ (Y) | Z and X ⊥⊥ (W) | (Z ∪ Y) =⇒ X ⊥⊥ (Y ∪ W) | (Z)

Intersection: X ⊥⊥ (Y) | (Z ∪ W) and X ⊥⊥ (W) | (Z ∪ Y) =⇒ X ⊥⊥ (Y ∪ W) | (Z)

An alternative set of complete axioms we refer to Geiger and Pearl (1993).

(1)

(2)

(3)

(4)

(5)

Deﬁnition 5 (semi-graphoid and graphoid) A semi-graphoid is a dependency model which sat-
isﬁes Eq(1) − Eq(4). If also Eq(5) holds it is called a graphoid.

Deﬁnition 6 ( Probabilistic graphoid ) In probability, Conditional independence deﬁned as

P(X, Y | Z) = P(X | Z)

is a semi-graphoid. when P is strictly positive conditional independence becomes a graphoid.

Deﬁnition 7 ( Graph Separation as graphoid) Graph separation in undirected graph satisﬁes graphoid
axioms. For details we refer to Lauritzen (1996) and Dawid (1979).

2.4 Markov Properties of Undirected Graphs

In this sections we deﬁne the following three Markov properties for Markov networks. Let G =
{V, E} be an undirected graph and P be a probability distribution over G.

Deﬁnition 8 ((P) Pairwise Markov Property) The probability distribution P satisﬁes the pairwise
Markov property for the graph G if for every pair of non adjacent edges X and Y , X is independent
of Y given everything else.

X ⊥⊥ Y | (V \ X, Y)

Deﬁnition 9 ((L) Local Markov Property) The probability distribution P satisﬁes the local Markov
property for the graph G if every variable X is conditionally independent of its non-neighbours in
the graph, given its neighbours.

where bd(X) denotes boundary of X.

X ⊥⊥ (V \ X ∪ bd (X)) | bd (X)

4

MUTUAL CONDITIONAL INDEPENDENCE AND INFERENCE IN MARKOV NETWORKS

Deﬁnition 10 ((G) Global Markov Property) The probability distribution P, is said to be global
Markov with respect to an undirected graph G if, for any disjoint subsets of nodes A, B, C such that
C separates A and B on the graph, if and only if the distribution satisﬁes

A ⊥⊥ B | C

Proposition 11 Let G be an undirected graph. A probabilistic independence model that satisﬁes
semi-graphoid axioms with respect to G, the following holds. For proof we refer Lauritzen (1996).

(G) =⇒ (L) =⇒ (P)

Proposition 12 Let G be an undirected graph. A probabilistic independence model that satisﬁes
graphoid axioms with respect to G, the following holds. For proof we refer Pearl (1988) and Dawid
(1979).

(G) ⇐⇒ (L) ⇐⇒ (P)

2.5 Markov Network Graphs and Markov Network

After discussing the graph theory, conditional independence and Markov properties for undi-
rected graphs, now we are ready to deﬁne Markov network graphs and Markov networks.

Deﬁnition 13 (Markov Network Graph) A Markov network graph is an undirected graph G = ( V,
E ) where V = {X1, X2, .., Xn} corresponds to random variables of a multivariate distribution.

Deﬁnition 14 (Markov Network) A Markov network is a tuple M = (G, ψ) where G is a Markov
network graph, ψ = {ψ1, ψ2, ..., ψm} is a set of non negative functions for each clique Ci ∈ G ∀i =
1 . . . m and the joint pdf can be decomposed into factors as

P(x) =

1

Z

Y
a∈Cm

ψa (x)

where Z is a normalizing constant.

Theorem 15 (Hammersley-Clifford theorem) Let M = (G, ψ) be a Markov network. Let proba-
bility density function p(X) of the distribution of X = {X1, X2, .., Xn} be strictly positive. X satisﬁes
global Markov property with respect to graph G if and only if it factorizes as follows.

P(x) = Y
a∈Cm

ψa (x)

where Cm are the maximal cliques of G and ψa (x) depends on x through xa = (xv )v ∈a only.

It follows from the above discussion that if a strictly positive probability distribution factor-
izes with respect to G then it also satisﬁes all Markov properties(pair-wise,local and global) w.r.t.
G.

5

NIHARIKA

3. Mutual Conditional Independence

In this section we prove that the elements of an independent set are mutually conditionally in-
dependent given the rest.

Theorem 16 (Mutual Conditional Independence in Markov networks) Let G be a Markov net-
work graph and P(X) is a strictly positive probability which supports the conditional indepen-
dences relations required to satisfy pairwise, local, and the global Markov property for G, then
elements of an independent set I of G are mutually conditionally independent given the rest {V \ I}.

Proof Let I = {X1, X2, ..., Xk } be an independent set of G. Since {X1, X2, ..., Xk } are mutually non-
adjacent, when we condition on V \ I or equivalently when we remove the nodes V \ I from G, the
remaining vertices {X1, X2, ..., Xk } are disconnected which implies in probability complete inde-
pendence among vertices of I.

Since I = {X1, X2, ..., Xk } form independent set they belong to separate cliques say Xi ∈ Ci , for
i = 1 t o k, where Ci is a maximal clique in G. Without loss of generality we can assume that there
are exactly k maximal cliques. From Theorem(Hammersley-Clifford theorem) the P factorizes as
follows.

P = ψ1(X1, Y1) ψ2(X2, Y2)...ψk (Xk , Yk )

i s are the sets of nodes that connects two or more C′

where Y′
i s and each {Xi , Yi } forms a maximal
clique in G. It can be noted that Yi can be empty in case of a disconnected graph and also union
of ∪Yi = V \ I .
The conditional probability P(I | V \ I) can be expressed as

P(I | (Y1 = y1, ..., Yk = yk ) = ψ1(X1, y1) ψ2(X2, y2)...ψk (Vk , kk )

P(I | V \ I) = φ1(X1) φ2(X2)...φk (Xk )

Hence {X1, X2, ..., Xk } are mutually conditionally independent given {V \ I}.

4. Mutual Conditional Independence and the Markov Properties

In this section we represent an alternative way to derive conditional independence relations re-
quired for satisfying the Markov properties of Markov networks. Speciﬁcally we prove equiva-
lence between MCIP and pairwise Markov property and from proposition(12) it follows for other
Markov properties(Local and the global).

Theorem 17 (Equivalence of MCIP and Markov properties) Let G be a Markov network graph
and let P be a strictly positive probability distribution which satisﬁes mutual conditional inde-
pendence relations implied by maximal independent sets of G. Then conditional independence
relations required to satisfy pairwise, local, and the global Markov properties for G also holds in P.

Proof We prove equivalence of MCIP and pair-wise Markov property. Then under assumption
of positive distribution MCIP is equal to local and the global Markov property as well.

Let C1, C2, ...Cm be the complete list of the maximal independent sets of nodes of G then as

stated before V = C1 ∪ C2 · · · ∪ Cm and E = {(x, y) : (x, y) ∉ Ci ∀i = 1 · · · m}.

6

MUTUAL CONDITIONAL INDEPENDENCE AND INFERENCE IN MARKOV NETWORKS

A

D

E

F

B

C

G

Figure 1: A Markov network graph

If conditional independence relations required to satisfy pairwise Markov property holds in
P w.r.t. G, then elements of Ci are mutually conditionally independent conditioned on {V \Ci } by
Theorem 16. So we have proved as

pairwise Marko Property =⇒ MCIP

We must recall that the Mutual conditional independence implies pair-wise conditional in-
dependence. Since elements of a Ci are mutually conditionally independent given the V \ Ci ,
therefore they are also pairwise independent given V \ Ci .

Now let us Suppose that {x, y} ∈ V is a pair of nodes which is not an edge, then {x, y} is an
independent set and hence is contained in some Ci and hence pairwise independent given the
rest. Therefore

pairwise Marko Property ⇐⇒ MCIP

We illustrate the proof by an example as follows. Let us consider the Markov network as given

in ﬁgure (1).

All Maximal independent sets for graph G are as :

S = {{A, C, F}, {A, C, G}, {A, E}, {B, D, F}, {B, D, G}, {B, E}}

Let us consider the ﬁrst maximal independent sets C1 = {A, C, F} and let us suppose that MCIP
holds which implies that A, C, F are mutually independent given rest of the random vectors
B,D,E,G.
Or equivalently independence relation can be expressed as

Applying weak union graphoid axiom (Equation (3)) to the above independence relation we get

A ⊥⊥ C ⊥⊥ F | (B, D, E, G)

A ⊥⊥ F | (B, D, E, G) ∪ C

C ⊥⊥ F | (B, D, E, G) ∪ A

A ⊥⊥ C | (B, D, E, G) ∪ F

7

NIHARIKA

Applying similarly arguments for the other set of maximal independent set we can show that for
every non-adjacent pair x, y ∈ V

x ⊥⊥ y | V \ {x, y}

which is also a deﬁnition of pair-wise Markov property.

Conversely given pair-wise Markov property we have to show that MCIP holds. From the-
orem (Hammersley-Clifford theorem), it is clear that under positive distribution assumption P
satisﬁes pairwise Markov property with respect to graph G if and only if it factorizes as follows.

P(x) = ψ1(A, B) ψ2(A, D) ψ3(B, C) ψ4(C, D, E) ψ(E, F, G)

Let us consider probability of (A,C,F) conditioned on (B,D,E,G), we obtain conditional probabil-
ity as

P(A, C, F | B = b, D = d , E = e, G = g ) = φ11(A, b) φ12(A, d ) φ21(C, b) φ22(C, d , e) φ3(F, e, g )

= φ1(A) φ2(C) φ3(F)

From above factorization of pdf it follows that (A,C,F) are mutually independent conditioned on
(B,D,E,G).

Similarly we can show mutual conditional independence relations for the remaining maximal
independent sets. Hence it follows as

MCIP ⇐⇒ pair-wise Markov property

Applying proposition 12 we get following equivalence relation that completes the proof.

MCIP ⇐⇒ P ⇐⇒ L ⇐⇒ G

5. Applications and Illustrations

In the following, we illustrate applications of MCIP for inference in Markov networks for discrete
and continuous data set.

5.1 Inference in Graphical Log-linear Models

First we consider the discrete data set, the Reinis data taken from the "GRbase" R package(Risk
factors for coronary heart disease, for details on Reinis dataset see Reinis et al. (1981)).

Example 1 (Decomposable Graphical Model for Rienis Dataset:) The Reinis data is shown in the
table (1).

8

MUTUAL CONDITIONAL INDEPENDENCE AND INFERENCE IN MARKOV NETWORKS

Table 1: Reinis data

Family Protein Systol Phys

Smoke
Mental

neg

< 3

< 140

≥ 140

≥ 3

< 140

≥ 140

pos

< 3

< 140

≥ 140

≥ 3

< 140

≥ 140

no
yes
no
yes
no
yes
no
yes
no
yes
no
yes
no
yes
no
yes

no

yes

no

44
129
35
109
23
50
24
51
5
9
4
14
7
9
4
5

yes

40
145
12
67
32
80
25
63
7
17
3
17
3
16
0
14

no

yes

112
12
80
7
70
7
73
7
21
1
11
5
14
2
13
4

67
23
33
9
66
13
57
16
9
4
8
2
14
3
11
4

Using stepwise model selection for Reinis data, the best decomposable model we get is as given
in ﬁgure (2) with following χ2 and G2 test statistics. We use Wermuth’s backward elimination
algorithm, for details see Wermuth (1976).

X2 = 51.11705
G2 = 51.35869

d f = 46

X2 << χ2(.95, 46) = 62.8, Hence the data supports the model selected.

We note that the variable set {phys, systol,family} forms an independent set as per the Markov

network in ﬁgure (2).

Now we derive a closed form expression for expected count using MCIP as follows.

Let X1, X2, X3, X4, X5, X6 represent the random variables Family, Protein, Systol, Phys, Smoke , Men-
tal respectively.

9

NIHARIKA

systol

smoke

mental

family

protein

phys

Figure 2: A decomposable graphical model for the reinis data

P(X1, X3, X4 | X2, X5, X6) =

P(X1, X2, X3, X4, X5, X6)

P(X2, X5, X6)

the joint pdf can be expressed as

P(X1, X2, X3, X4, X5, X6) = P(X1, X3, X4 | X2, X5, X6) ∗ P(X2, X5, X6)

si nce X1, X3, X4 are mutually independent conditioned on X2, X5, X6

hence P(X1, X3, X4 | X2, X5, X6) can be factorized as

P(X1, X3, X4 | X2, X5, X6) = P(X1 | X2, X5, X6) ∗ P(X3 | X2, X5, X6) ∗ P(X4 | X2, X5, X6)

the joint pdf can be written as

P(X1, X2, X3, X4, X5, X6) = P(X1 | X2, X5, X6) ∗ P(X3 | X2, X5, X6) ∗ P(X4 | X2, X5, X6) ∗ P(X2, X5, X6)

=

(X1, X2, X5, X6) ∗ P(X3, X2, X5, X6) ∗ P(X4, X2, X5, X6)

P(X1, X2, X5, X6)2

After simpliﬁcation we get following closed form expression for the maximum likelihood estimator
of the expected cell counts. For details on computing closed form expressions for the expected cell
counts for decomposable log-linear models, see Bishop et al. (1989 edition).

ˆmi j kl mn =

ni j ..mnn. j k.mnn. j .l mn

n2

. j ..mn

where ni j kl mn = observed count in cell (i , j , k, l , m, n)

ˆmi j kl mn = Maximum Likelihood Estimate of the expected cell count mi j kl mn

Under the mutual conditional independence assumption the table of ﬁtted values is given in the
table(2).

10

MUTUAL CONDITIONAL INDEPENDENCE AND INFERENCE IN MARKOV NETWORKS

Table 2: Fitted values for Reinis table

Family Protein Systol Phys

Smoke
Mental

no

no

yes

neg

< 3

< 140

≥ 140

≥ 3

< 140

≥ 140

pos

< 3

< 140

≥ 140

≥ 3

< 140

≥ 140

no
yes
no
yes
no
yes
no
yes
no
yes
no
yes
no
yes
no
yes

42.828483
127.025386
336.061224
143.081633
111.297302
12.421574
66.211530
21.504599
25.526279
50.612449
28.956142
83.490210
68.758172
8.089197
63.788280
15.516068

4.323380
12.822752
6.010204
23.846939
20.044064
2.237061
11.536857
3.747014
4.311871
8.549400
4.777763
13.775885
14.452355
1.700277
13.429112
3.266541

yes

no

37.102750
110.043382
17.081633
67.775510
78.517959
8.763165
33.427180
10.856691
24.092218
47.769053
22.546004
65.007644
71.715512
8.437119
58.472590
14.223062

yes

3.745388
11.108480
2.846939
11.295918
14.140675
1.578200
5.824433
1.891696
4.069631
8.069097
3.720091
10.726261
15.073961
1.773407
12.310019
2.994329

To test if the above model holds, we perform Perason’s chi-square test. We use the follwing

formula.

X2 = X
i

(Oi − Ei )2

Ei

where O denotes observed cell count and E as expected cell count.
The following test statistcs we get

X2 = 35.01022

d f = 46

χ2(.95, 46) = 62.8

As per the Chi-Squared test, the data supports the mutual conditional independence among {phys,
systol,family} conditioned on {phys, systol,family}. For details on graphical log-linear model we
refer the reader to Christensen (1997), and Bishop et al. (1989 edition).

Example 2 (Non-Decomposable Graphical Model for Rienis Dataset:) Let us consider the Rei-
nis data once again. We get the best non-decomposable graphical model as given in the ﬁgure
(3) with following χ2 and G2 test statistics.

X2 = 61.87653
G2 = 62.84262

d f = 49

X2 < χ2(.95, 49) = 66.3, Hence the data supports the model selected .

11

NIHARIKA

We notice that in this model the following mutual conditional relation holds.

(ph y s ⊥⊥ s y st ol ⊥⊥ f ami l y) | (ph y s, s y st ol , f ami l y).

Since the above relation is same as relation we got for decomposable model, hence the factorization
of joint pdf, getting closed form expression for expected cell counts, chi-square test for model testing
are the same as decomposable model as computed previously in example 1. We also note that the

systol

protein

mental

smoke

phys

family

Figure 3: A non-decomposable graphical model for the reinis data

closed form expression we get for estimated cell counts for non-decomposable graphical model
must be same as one of the decomposable models. Hence MCIP can be directly used to get closed
form estimates for non-decomposable graphical models without converting it to decomposable
graphical models.

5.2 Inference in Gaussian Graphical Models

In this section, we illustrate application of MCIP for inference in Gaussiam graphical models(GGM).
We consider the "seeds" dataset which is available at UCI machine learning repository
https://archive.ics.uci.edu/ml/datasets/seeds. For details on GGM we refer to some selected
classical and recent research papers Dempster (1972), Mohan et al. (2014), Tan et al. (2014) and
Janzamin and Anandkumar (2014)

Example 3 (A Decomposable Model for the Seeds dataset) The best decomposable graphical model
we get using stepwise selection method is given in ﬁgure(4).

As per the graph in ﬁgure (4), the vertex set {V1, V4, V6} forms an independent set. The follow-
ing conditional test results also supports that the variables {V1, V4, V6} are pair wise conditionally
independent conditioned on the rest {V2, V3, V5, V7}.

Test:V1 ⊥⊥ V6 | V2, V3, V5, V7
Chi-Square test statistic : 0.893 df:1 p-value: 0.3447

Test V1 ⊥⊥ V4 | V2, V3, V5, V7
Chi-Square test statistic: 1.055 df: 1 p-value: 0.3044

12

MUTUAL CONDITIONAL INDEPENDENCE AND INFERENCE IN MARKOV NETWORKS

V1

V7

V5

V2

V4

V3

V6

Figure 4: A decomposable graphical model for the seeds dataset

Test V4 ⊥⊥ V6 | V2, V3, V5, V7
Chi-Square test statistic: 0.952 df: 1 p-value: 0.3293

For normal variables, zero correlation implies independence and pairwise independence im-
plies mutual independence. Hence the variables V1, V4, V6 are also mutually conditionally inde-
pendent conditioned on {V2, V3, V5, V7}. Equivalently it can be expressed as

V1 ⊥⊥ V4 ⊥⊥ V6 | V2, V3, V5, V7

6. Computational details

All the experimental results in this paper were carried out using R 3.1.3 with the packages gRim
and MASS. All packages used are available at http://CRAN.R-project.org/.

7. Conclusion

In summary, we discussed different Markov properties for the class of Markov networks. We
derived an alternative formulation of the Markov properties of Markov networks. We have given
a new perspective on conditional independence over an independent set as mutual conditional
independence. We have proved equivalence between MCIP and the Markov properties, under
positive distribution assumption. We have presented MCIP based approach for inference. The
experimental results are carried out for the proposed MCIP based approach for inferences on
discrete and continuous datasets. MCIP can be a promising new direction for model selection
and inference in Markov networks.

13

NIHARIKA

References

Y.M.M. Bishop, S.E. Fienberg, and P.W. Holland. Discrete Multivariate Analysis: Theory and Prac-

tice. The MIT Press, Cambridge, MA, 1989 edition.

R. Christensen. Log-Linear Models and Logistic Regression. Springer, 2nd edition edition, 1997.

A. P. Dawid. Conditional independence in statistical theory. Journal of the Royal Statistical Soci-

ety, 41(1):1–31, 1979.

A. Dempster. Covariance selection. Biometrics, 28:157–175, 1972.

Dan Geiger and Judea Pearl. Logical and algorithmic properties of conditional independence

and graphical models. The Annals of Statistics, 24(4):2001–2021, 1993.

Majid Janzamin and Animashree Anandkumar. High-dimensional covariance decomposition
Journal of Machine Learning Research, 15:

into sparse markov and independence models.
1549–1591, 2014. URL http://jmlr.org/papers/v15/janzamin14a.html.

Michael I. Jordan. Graphical models. Statistical Science, 19(1):140–155, 2004.

S. L Lauritzen. Graphical Models. Oxford University Press Inc., New York, 2nd edition edition,

1996.

F. Matus. On equivalence of markov properties over undirected graphs. Journals of Applied Prob-

ability, 29:745–749, 1992.

Karthik Mohan, Palma London, Maryam Fazel, Daniela Witten, and Su-In Lee. Node-based
learning of multiple gaussian graphical models. Journal of Machine Learning Research, 15:
445–488, 2014. URL http://jmlr.org/papers/v15/mohan14a.html.

Judea Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Mor-

gan Kaufmann Publishers Inc. San Francisco, 1988.

C. Preston. Random ﬁelds. Berlin, Germany: Springer-Verlag, 1974.

Z. Reinis, J. Pokorny, V. Basika, J. Tiserova, K. Gorican, D. Horakova, E. Stuchlikova, T.and
Havranek, and F Hrabovsky. Prognostic signiﬁcance of the risk proﬁle in the prevention of
coronary heart disease. Bratis. lek. Listy, 76:137–150, 1981.

F. Spitzer. Random ﬁelds and interacting particle systems. M.A.A.Summer Seminar Notes, 1971.

Kean Ming Tan, Palma London, Karthik Mohan, Su-In Lee, Maryam Fazel, and Daniela Witten.
Learning graphical models with hubs. Journal of Machine Learning Research, 15:3297–3331,
2014. URL http://jmlr.org/papers/v15/tan14b.html.

Martin J. Wainwright and Michael I. Jordan. Graphical Models, Exponential Families, and Varia-

tional Inference. Foundations and Trends in Machine Learning, 2008.

Nanny Wermuth. Model search among multiplicative models. Biometrics, 32:253–263, 1976.

J. Whittaker. Graphical Models in Applied Multivariate Statistics. Chichester: Wiley, 2nd edition

edition, 1990.

14

