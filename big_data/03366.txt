6
1
0
2

 
r
a

 

M
0
1

 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
6
6
3
3
0

.

3
0
6
1
:
v
i
X
r
a

A Second-Order Cone Based Approach for Solving the Trust

Region Subproblem and Its Variants

Nam Ho-Nguyen1 and Fatma Kılın¸c-Karzan1

1Tepper School of Business, Carnegie Mellon University, Pittsburgh, PA, 15213, USA.

March 2016

Abstract

We study the trust region subproblem (TRS) of minimizing a nonconvex quadratic function
over the unit ball with additional conic constraints. Despite having a nonconvex objective,
it is known that the TRS and a number of its variants are polynomial-time solvable. In this
paper, we follow a second-order cone based approach to derive an exact convex formulation of
the TRS, and under slightly stronger conditions, give a low-complexity characterization of the
convex hull of its epigraph without any additional variables. As a result, our study highlights
an explicit connection between the nonconvex TRS and smooth convex quadratic minimization,
which allows for the application of cheap iterative methods to the TRS. We also explore the
inclusion of additional hollow constraints to the domain of the TRS, and convexiﬁcation of the
associated epigraph.

1 Introduction

(cid:26)

h(y) := y(cid:62)Qy + 2 g(cid:62)y :

(cid:107)y(cid:107) ≤ 1
Ay − b ∈ K

(cid:27)

In this paper, we study the polynomial-time solvable variants of the trust region subproblem (TRS)
[15] given by

Opth := min
y∈Rn

(1)
where (cid:107)y(cid:107) denotes the Euclidean norm of y, A ∈ Rm×n, b ∈ Rm, and K ⊆ Rm is a closed convex
cone. We assume that the minimum eigenvalue of Q, λmin(Q), is negative. This problem (1) is
equivalent to the classical TRS when there are no additional conic constraints, i.e., A = In, b = 0,
and K = Rm.

,

The classical trust region subproblem is an essential ingredient of the trust region methods
that are commonly used to solve continuous nonconvex optimization problems (see [15, 30, 32] and
references therein). In each iteration of a trust region method, a quadratic approximation of the
objective function is built and then optimized over a ball, called trust region, (or intersection of
a ball with linear constraints) to ﬁnd the new search point. Trust region subproblems are also
encountered in the context of robust optimization under matrix norm or polyhedral uncertainty
(see [4, 6] and references therein), nonlinear optimization problems with discrete variables [10, 12],
least-squares problems [40], constrained eigenvalue problems [20], and more.

As stated above, the optimization problem in (1) is nonlinear and nonconvex when λmin(Q) <
0. Nevertheless, it is well-known that classical TRS and a number of its variants can be solved

1

in polynomial time via semideﬁnite programming [33] or using specialized nonlinear algorithms
[22, 27]. Speciﬁcally, the semideﬁnite programming (SDP) relaxation for the classical TRS is
known to be exact [33].
Several variants of the TRS that enforce additional constraints on the trust region have been
proposed. Among these the most commonly studied is the case when K is taken to be a nonnegative
orthant, i.e., additional linear constraints are modeled via the polyhedral set P := {y ∈ Rn :
Ay − b ∈ K} intersected with the unit ball. TRS with additional linear inequalities arises in
nonlinear programming and robust optimization (see [11, 25] and references therein) and is studied
in [9, 11, 12, 14, 25, 37, 39] under a variety of assumptions. Speciﬁcally, [12, 37] give a tight
semideﬁnite formulation when there is a single linear constraint a(cid:62)y ≤ b based on an additional
constraint derived from second-order reformularization linerazitation technique (SOC-RLT). This
approach was extended to two linear constraints in [12, 39] and the tightness of the SDP relaxation is
shown when the linear constraints are parallel. More recently, Burer and Yang [14] give a tight SDP
relaxation with additional SOC-RLT constraints for an arbitrary number of linear constraints, under
the condition that these additional linear inequalities do not intersect on the interior of the unit ball.
We refer the readers to Burer [11] for a recent survey and related references for the results on tight
SDP relaxations associated with these variants. Recently, Jeyakumar and Li [25] prove convexity
of the joint numerical range, exactness of the SDP relaxation, and strong Lagrangian duality for
the TRS with additional linear and SOC constraints. A key tool in their analysis is to recast the
TRS as a convex quadratic minimization problem under a dimensionality condition. Following a
diﬀerent approach and without relying on a convex reformulation of the problem, Bienstock and
Michalka [9] show that TRS with linear inequality constraints is polynomial-time solvable under
the milder condition that the number of faces of the linear constraints P intersecting with the unit
ball is polynomially bounded. Hollow constraints deﬁned by a single ellipsoid [5, 8, 32, 36, 39] or
several ellipsoids [9, 38] or arbitrary quadratics constraints [7] have also attracted some attention
in the literature. These approaches are once again either SDP based convexiﬁcation schemes or
customized algorithms.

While the SDP reformulations of the TRS and its variants can be solved using interior-point
methods in polynomial time [1, 29], this approach is not practical because the worst-case complexity
of these methods for solving SDPs is a relatively large polynomial and these methods do not exploit
the sparsity of the data. That said, the classical TRS is closely connected to eigenvalue problems. In
the speciﬁc case of classical TRS where the objective is convex, i.e., when Q is positive semideﬁnite,
this problem becomes simply the minimization of a smooth convex function over the Euclidean ball;
and thus it can be solved eﬃciently via iterative methods such as Nesterov’s accelerated gradient
descent algorithm [28]. Moreover, in the nonconvex case with λmin(Q) < 0, when the problem is
purely quadratic, i.e., when g = 0 as well, the TRS reduces to ﬁnding the minimum eigenvalue
of Q. This can be approximated in linear time by the well-known Power iteration method [21,
Chapter 8.2] and can be found eﬃciently via the Lanczos method [21, Chapter 10.1] in practice.
When g (cid:54)= 0, even though the problem is no longer equivalent to an eigenvalue problem and
these methods cannot be applied directly, this observation has lead to the development of eﬃcient,
matrix-free algorithms that are based solely on matrix-vector products. The dual-based algorithms
of [27], [33] and [35], the generalized Lanczos trust-region method of [22], and the more recent
developments of [17, 18, 23, 24, 34] are examples of such iterative algorithms. In most cases, these
algorithms are presented together with their convergence proofs. Nevertheless, to the best of our
knowledge, the theoretical runtime evaluation of these algorithms lacks formal guarantees with the

2

exception of [24]. In addition, in most of these iterative methods, numerical diﬃculties are reported
in the so-called the “hard case” [27]. The hard case occurs when the linear component vector g
is nearly orthogonal to the eigenspace of the smallest eigenvalue of Q. In many cases, the lack of
provable worst-case convergence bounds for the TRS is attributed to the hard case. As a result,
most research on speciﬁc algorithms for the TRS thus far focuses on addressing this issue.

Recently, Hazan and Koren [24] suggested a linear-time algorithm for approximately solving
the TRS within a given tolerance . Their approach relies on an eﬃcient, linear-time solver for a
speciﬁc SDP relaxation of a feasibility version of the TRS and reduces the TRS into a series of
eigenvalue computations. In their approach, they exploit the special structure of the dual problem,
a one-dimensional problem for which bisection techniques can be applied, to avoid using interior-
point solvers. Each dual step of their algorithm requires a single approximate maximal eigenvalue
computation which takes O
time, where N is the number of nonzero entries in Q and
δ is the desired accuracy to which the maximal eigenvalue is computed. Their overall algorithm

(cid:1)(cid:1) iterations, where Γ := max{2((cid:107)Q(cid:107) + (cid:107)b(cid:107)), 1} and (cid:107)Q(cid:107) stands for the

converges in O(cid:0)log(cid:0) Γ

(cid:16) N√
 log(cid:0) n

(cid:1)(cid:17)

δ







spectral norm of the matrix Q, i.e. the maximum absolute eigenvalue. Then a primal solution is
recovered by solving a small linear program formed by the dual iterates. Finally, they provide an
eﬃcient and accurate rounding procedure for converting the SDP solution into a feasible solution to
the TRS. As a consequence, their approach does not require the use of interior-point SDP solvers,
bypasses the diﬃculties noted for the hard-case of the TRS, and can exploit data sparsity (runs in
time linear in the number of nonzero entries of the input). The overall complexity of their algorithm
is O

(cid:1)(cid:1) log(cid:0) Γ

(cid:16) N√
 log(cid:0) n

δ log(cid:0) Γ

(cid:1)(cid:17)

.

In this paper, as opposed to the previous specialized algorithms or the SDP based approaches,
we suggest a second-order cone (SOC) based approach to solve the trust region subproblem and
its various variants. That is, under easy-to-verify conditions, we derive tight SOC-based convex
reformulations and convex hull characterizations of sets associated with the TRS with additional
constraints. Our results on convex hull representations are based on the recent developments of
Burer and Kılın¸c-Karzan [13] for representing certain nonconvex sets deﬁned by the intersection of
SOC constraints and constraints involving general nonconvex quadratics. In particular, [13] studies
nonconvex sets obtained as the intersections of the form F +∩Q and F +∩Q∩H where the cone F +
is second-order-cone representable (SOCr), Q is a nonconvex cone deﬁned by a single homogeneous
quadratic, and H is an aﬃne hyperplane. For such sets, under several easy-to-verify conditions,
[13] suggests a simple, computable convex relaxation F + ∩ S of F + ∩ Q, where S is an additional
SOC representable cone, and identiﬁes several stronger conditions guaranteeing the tightness of
these relaxations, i.e., F + ∩ S = cone(F + ∩ Q) and F + ∩ S ∩ H = conv(F + ∩ Q ∩ H), where cone
indicates the closed conic hull, and conv indicates the closed convex hull. These conditions have
been further veriﬁed in many speciﬁc cases; and it was shown in [13] that the classical TRS can be
solved via the optimization of two second-order conic programs. Similar convex hull descriptions of
SOCs intersected with general nonconvex quadratics are also studied recently in [26] under diﬀerent
assumptions.

Our contributions in this paper can be summarized as follows.

(i) We give a tight SOC-based low-complexity convex relaxation of the TRS with additional
conic constraints (1). Under an easily checkable and general structural condition on the
conic constraints, our Theorem 2.4 states that the convex relaxation of problem (1) obtained
by simply replacing the nonconvex objective function h(y) in (1) with the convex objective

3

f (y) := y(cid:62) (Q − λmin(Q)In) y + 2 g(cid:62)y + λmin(Q) is tight.

(ii) Consequently, we show that our convex reformulation of the classical TRS can be solved via
a single minimum eigenvalue computation and then minimizing a smooth convex quadratic
over the unit ball. Thus, the overall complexity of solving the classical TRS using ﬁrst-order
methods is

(cid:18)

(cid:18)

(cid:18) 1

(cid:19)

δ

O

N

log

+

λmax(Q) − λmin(Q)

√



(cid:19)(cid:19)

,

 .

where λmax(Q) is the maximum eigenvalue of the matrix Q. In addition, we discuss implica-
tions of approximating the minimum eigenvalue computation in this scheme.

(iii) Finally, we study the convex hull of the epigraph of the TRS given by

(cid:21)


(cid:20)y

t

X :=

∈ Rn+1 :

(cid:107)y(cid:107) ≤ 1
Ay − b ∈ K
h(y) ≤ t

In Theorem 3.3, under a slightly stronger condition, we provide an explicit characterization of
conv(X) in the space of original variables. We also examine the inclusion of additional hollow
constraints y ∈ H = Rn \P to the TRS; and in Theorem 3.8 we give a more general condition
on the hollow set H than the existing ones from the literature for which we can derive the
convex hull.

The papers [25] and [13] are closely related to our approach due to their SOC based approaches.
In particular, our convex reformulation result of the TRS is a generalization of a result in [25]. We
show that the conditions from [25] imply our condition and we provide an example where our
condition is satisﬁed but the ones in [25] are not. We also provide an example to demonstrate the
necessity of our condition. In the context of the classical TRS with no additional constraints, our
structural assumption is immediately satisﬁed. We note that [13] also give a scheme to solve the
classical TRS via SOC programming. The scheme suggested in [13] is in a lifted space with one
additional variable and requires solving two related SOC optimization problems. In contrast, our
convex reformulation is in the space of original variables and requires only a single minimization of
a smooth convex quadratic function over the unit ball.

On the algorithmic side, our transformation of the TRS requires mainly the computation of a
minimum eigenvalue of Q which can be done in linear time with O(N ) arithmetic operations per
iteration, for instance by using the Power iteration method [21, Chapter 8.2]. Due to the fact that
f (x) is a convex quadratic function, our convex reformulation for the classical TRS can simply
be cast as a conic optimization problem. Speciﬁcally, when there are no additional constraints,
this problem becomes minimizing a smooth convex function over the Euclidean ball; and thus it is
readily amenable to eﬃcient ﬁrst-order methods. In particular, for this class of problems, given a
desired accuracy level of  > 0, Nesterov’s accelerated gradient descent algorithm [28] involves only
elementary operations such as addition, multiplication, and matrix-vector product computations
and achieves the optimal iteration complexity of O
. In the speciﬁc case where
the problem is convex (i.e., when Q is positive semideﬁnite), the same complexity guarantees can
be obtained by applying Nesterov’s accelerated gradient descent [28] to the problem. Thus, our
approach can be seen as an analog of the latter algorithm to the general nonconvex case. To the

(cid:16) λmax(Q)−λmin(Q)

(cid:17)

√



4

best of our knowledge, this connection with iterative methods in the context of the classical TRS
has not been made before.

Moreover, convexiﬁcation based approaches bypass the so-called ‘hard case’ because they work
directly with a convex formulations. Among these the ones more amenable to iterative methods are
[5, 25, 24] and ours. This is so for our approach, as well as that of Jeyakumar and Li [25], because
we work with an SOC based reformulation of the problem and it only requires the computation of a
minimum eigenvalue. To the best of our knowledge iterative algorithms for SDP based relaxations
of the TRS have not been studied in the literature with the exception of Hazan and Koren [24]. As
compared to the approach in [24], we believe our approach is straightforward and easy to implement
while it achieves a slightly better convergence guarantee in the worst case.
In particular, our
approach directly solves the TRS, as opposed to only solving a feasibility version of the TRS; thus
we save an extra logarithmic factor. Also, the convex reformulation given by Ben-Tal and Teboulle
[5] requires a full eigenvalue decomposition which is more expensive, i.e., O(n3) time, and due to
its speciﬁc structure can be solved at a slower rate by iterative methods.

Our convex hull results on the epigraph of the TRS is inspired by the recent work of Burer
and Kılın¸c-Karzan [13] on convex hulls of general quadratic cones. While the convex hull results in
[13] are applicable to many problems, including a set associated with the classical TRS, we present
a much more direct analysis for the TRS. There are two main beneﬁts of our approach. Firstly,
the convex hull result for the classical TRS in [13] requires the assumption that the optimal value
is nonpositive. While this is not an issue for the classical TRS since the optimal value is always
negative, with the existence of additional constraints, this may no longer be true.
In contrast,
our analysis does not require any nonpositivity assumptions; and hence we are able to include
additional conic constraints in our convex hull results under only minor conditions. Secondly, our
direct analysis of the TRS allows us to bypass verifying several conditions from [13] and to work
directly with a single condition which is always satisﬁed in the case of the classical TRS.

Several papers [2, 3, 25] exploit convexity results on the joint numerical range of quadratic
mappings to explore strong duality properties of the TRS and its variants. These convexity results
are based on Yakubovich’s S-lemma [19] and Dines [16], see also the survey by P´olik and Terlaky
[31] for a more detailed discussion. While these results as well as ours both analyze sets associated
with the TRS, the actual sets in question are quite diﬀerent. In the context of the TRS, the joint

numerical range is a set of the form(cid:8)[h(y); (cid:107)y(cid:107)2; Ay − b] : y ∈ Rn(cid:9) ⊆ Rm+2.

Under certain conditions, this set is shown to be convex. In contrast, we study the epigraphical
set X, which is nonconvex if h(y) is, and give its convex hull description in the original space of
variables.

Our paper is structured as follows. Section 2 details the derivation of our convex reformulation of
the TRS with additional conic constraints under a structural assumption on the conic constraints.
We then discuss the complexity of solving our reformulation of the classical TRS with iterative
methods in Section 2.2, as well as the implications of working with an approximate minimum
eigenvalue in our reformulation. In addition to our convex reformulation of the TRS with additional
conic constraints, we study exact and explicit low-complexity convex hull results for its epigraph
obtained by new SOC constraints in Section 3. Finally, we discuss an extension of our SOC based
approach to the TRS to handle additional speciﬁc convex constraints and nonconvex constraints
given by hollows in Section 3.1.

5

We use Matlab notation to denote vectors and matrices. Furthermore, we let In be the n × n
identify matrix and denote the minimum eigenvalue of a symmetric matrix Q as λQ := λmin(Q).
Our notation is mostly standard; we will deﬁne any particular notation upon its ﬁrst use.

2 Tight Low-Complexity Convex Reformulation of the TRS
In this section, we present an exact convex, SOC based reformulation of the trust region subproblem
with additional conic constraints. We then explore algorithmic aspects of the classical TRS implied
by our SOC formulation.

2.1 Deriving the Reformulation

In this section, we present an exact convex reformulation the trust region subproblem with addi-
tional conic constraints:

Opth := min
y∈Rn

h(y) := y(cid:62)Qy + 2g(cid:62)y :

(cid:107)y(cid:107) ≤ 1
Ay − b ∈ K

(2)

(cid:26)

(cid:27)

where A ∈ Rm×n, b ∈ Rm, and K ⊂ Rm is a closed convex cone.

We start with the following simple observation about the optimal solutions of (2).

Lemma 2.1. Any optimal solution y∗ of (2) must be on the boundary of the domain {y : (cid:107)y(cid:107) ≤ 1, Ay − b ∈ K}.
Proof. Suppose y∗ ∈ int({y : (cid:107)y(cid:107) ≤ 1, Ay − b ∈ K}). Consider d (cid:54)= 0 such that Qd = λQd and
(g + λQy∗)(cid:62)d ≤ 0 (one such d always exists since if (g + λQy∗)(cid:62)d > 0, we can take the negative
instead). Because y∗ is in the interior of the domain, there exists a small  > 0 such that y∗ + d
remains feasible. However, the objective function at the point y∗ + d satisﬁes

h(y∗ + d) = (y∗)(cid:62)Q(y∗) + 2g(cid:62)y∗ + 2(g + λQy∗)(cid:62)d + λQ(cid:107)d(cid:107)22 < h(y∗)

because λQ < 0 and d is chosen so that (g + λQy∗)(cid:62)d ≤ 0. This contradicts the optimality of y∗;
and hence any optimal solution to (2) must be on the boundary.

For nonconvex quadratic objective functions, Lemma 2.1 points out the important role of the
boundary of our feasible set {y : (cid:107)y(cid:107) ≤ 1, Ay − b ∈ K}. In particular, let us examine the situation
when there exists a convex function f (y) such that
(i) f (y) ≤ h(y) for all y ∈ {y : (cid:107)y(cid:107) ≤ 1, Ay − b ∈ K},
(ii) f (y) = h(y) for all y ∈ bd({y : (cid:107)y(cid:107) ≤ 1, Ay − b ∈ K}), and
(iii) f (y) has a minimizer on bd({y : (cid:107)y(cid:107) ≤ 1, Ay − b ∈ K}).

Then the convex relaxation given by

(cid:26)

Optf := min

y

f (y) :

(cid:107)y(cid:107) ≤ 1
Ay − b ∈ K

(cid:27)

is tight. Given these observations, we ideally would like to ﬁnd a convex function f (y) ≤ h(y)
satisfying all of the above properties. In general, this is not possible. An natural alternative is to
instead consider

f (y) := h(y) + λQ(1 − (cid:107)y(cid:107)2) = y(cid:62)(Q − λQIn)y + 2g(cid:62)y + λQ.

(3)

6

Because Q − λQIn (cid:23) 0, that is, Q − λQIn is positive semideﬁnite, the function f (y) is convex by
construction. Also, f (y) = h(y) if and only if (cid:107)y(cid:107) = 1. However, note that f (y) may not be
equal to h(y) on all of bd({y : (cid:107)y(cid:107) ≤ 1, Ay − b ∈ K}). More precisely, we will have f (y) < h(y) for
y ∈ bd({y : (cid:107)y(cid:107) ≤ 1, Ay − b ∈ K})∩{y : (cid:107)y(cid:107) < 1}. Despite this obstacle, we will present conditions
for which the convex relaxation with objective f (y) will still be tight.

The function f (y) given in (3) exempliﬁes a speciﬁc way of obtaining convex functions that
underestimates h(y) on the domain via aggregation of h(y) with the convex constraints deﬁning
our domain. This type of aggregation based convex functions can be utilized in building convex
relaxations of nonconvex optimization problems. In certain cases, such convex relaxations turn out
to be tight. We demonstrate these in the following result.

Lemma 2.2. Let C be a given set and cj(x) for j = 1, . . . , m be given functions. Suppose h(x) is a
given nonconvex function and fj(x) is a convex function on the domain C := {x : cj(x) ≤ 0, ∀j =
1, . . . , m; x ∈ C} such that fj(x) = h(x) + αj cj(x) for some αj > 0. Let F (x) := maxj=1,...,m fj(x).
Then

Opth := min

x

x

{h(x) : x ∈ C} ≥ min

{F (x) : x ∈ C} =: Optf .

Moreover, the relation between the optimal values of these optimization problems above holds as
equality if and only if there exists an optimal solution x∗ to the problem on the right-hand-side
satisfying cj(x∗) = 0 for some j ∈ {1, . . . , m}.
Proof. First, we note that for any x ∈ C, we have cj(x) ≤ 0 and thus for all j ∈ {1, . . . , m},
fj(x) = g(x) + αj cj(x) ≤ g(x) because αj > 0. This establishes Optg ≥ Optf .
Let x∗ be an optimal solution to minx {F (x) : x ∈ C}. When cj(x∗) = 0 for some j, we have
fj(x∗) = g(x∗) which implies that x∗ is also optimal to the ﬁrst problem. Now consider the case
where every optimal solution x∗ ∈ arg minx {F (x) : x ∈ C} satisﬁes cj(x∗) < 0 for all j. Note that
for any x ∈ C satisfying cj(x) < 0 for all j, we have F (x) < g(x). Let ¯x ∈ arg minx {g(x) : x ∈ C}.
If cj(¯x) < 0 for all j, then we have F (¯x) < g(¯x) implying Optf < Optg. If cj(¯x) = 0 for some j,
then ¯x is not optimal for the second problem; and hence Optg = g(¯x) = fj(¯x) > F (x∗) = Optf
holds for any x∗ ∈ arg minx {F (x) : x ∈ C}.

Lemma 2.2 gives us a precise characterization for when the convex relaxation using f (y) is tight.

Corollary 2.3. Consider the convex relaxation for problem (2) given by

(cid:26)

(cid:27)

Optf = min

y

f (y) :

(cid:107)y(cid:107) ≤ 1
Ay − b ∈ K

,

(4)

where f (y) is deﬁned in (3). This convex relaxation is tight if and only if there exists an optimal
solution y∗ to (4) such that (cid:107)y∗(cid:107) = 1.

Based on Corollary 2.3, we next provide a condition that guarantees us ﬁnd minimizers of the

convex function f (y) deﬁned in (3) on the boundary of the unit ball.
Condition 2.1. There exists a vector d (cid:54)= 0 such that Qd = λQd, Ad ∈ K and g(cid:62)d ≤ 0.

Note that Condition 2.1 can be checked by solving the conic program

(cid:110)
g(cid:62)d : (Q − λQIn)d = 0, Ad ∈ K(cid:111)

.

min

d

7

Remark 2.1. From the deﬁnition of λQ, Condition 2.1 is immediately satisﬁed when A = In, b = 0,
♦
and K = Rn, i.e., for the classical TRS without additional constraints.

Moreover, Condition 2.1 has an immediate application in giving tight convex relations as follows.

Theorem 2.4. Suppose that Condition 2.1 holds for the TRS given in (2). Then the convex
relaxation given by (4) is tight.
Proof. Let y∗ be an optimum solution for (4). If (cid:107)y∗(cid:107) = 1, then from Corollary 2.3, the result
follows immediately. Hence, we assume (cid:107)y∗(cid:107) < 1.
Let d (cid:54)= 0 be the vector from Condition 2.1, thus Qd = λQd, Ad ∈ K and g(cid:62)d ≤ 0. Then for
any  > 0, A(y∗ + d) − b = (Ay∗ − b) + Ad ∈ K because K is a convex cone and Ad(cid:48) ∈ K by
assumption. Because (cid:107)y∗(cid:107) < 1, we may increase  until (cid:107)y∗ + d(cid:107) = 1 and the vector y∗ + d(cid:48) is
still feasible. Note (Q − λQIn)d = 0, so for any  > 0,

f (y∗ + d) = f (y∗) + 2(g(cid:62)d) ≤ f (y∗).

If g(cid:62)d < 0, this violates optimality of y∗ since  > 0, thus g(cid:62)d = 0. Then the vector y∗ + d is an
alternative optimum solution to (4) satisfying (cid:107)y∗ + d(cid:107) = 1. Hence, the tightness of the relaxation
(4) follows from Corollary 2.3.

We note that a similar result was implicitly proven by Jeyakumar and Li [25] under a dimen-
sionality condition for the case of linear and conic quadratic constraints. We state the linear version
of their condition below; the conic quadratic one is very similar.
Condition 2.2. Consider the case of nonnegative orthant, i.e., K = Rm
+ . Suppose that the system
of linear inequalities, i.e., the constraint Ay − b ∈ K satisﬁes the requirement that dim (Null(Q −
λQIn)) ≥ n − dim (Null(A)) + 1.
Observation 2.5. Condition 2.1 generalizes the dimensionality Condition 2.2 stated for linear and
conic quadratic constraints.
Proof. Suppose Condition 2.2 holds. Then dim (Null(A))+dim (Null(Q−λQIn)) ≥ n+1; thus, there
must exist d (cid:54)= 0 which is in the intersection. This means that Qd = λQd and Ad = 0 ∈ Rm
+ = K.
If g(cid:62)d ≤ 0, then Condition 2.1 holds with the vector d. If g(cid:62)d > 0, then Condition 2.1 holds with
the vector d(cid:48) = −d.

Jeyakumar and Li [25] demonstrates that Condition 2.2 is satisﬁed in a number of cases re-
lated to the robust least squares and robust SOC programming problems. As a consequence of
Observation 2.5, our Condition 2.1 is satisﬁed in these cases as well.

Remark 2.2. In contrast to the results given in [25], Theorem 2.4 holds for general conic constraints
when Condition 2.1 holds. Note that such general conic constraints can be a variety of convex
restrictions, and in particular, include positive semideﬁniteness requirements.
In addition, for
problem data given by

1
2
Condition 2.1 is satisﬁed with d = [0;−1]; but Condition 2.2 is not.

0
0 −1

, A =

Q =

g =

0

,

b =

, K = R2
+,

♦

(cid:20)1

(cid:21)

,

(cid:21)

(cid:20)1

(cid:20) 1 −1
(cid:21)

−1 −1

(cid:21)

(cid:20)1

1

8

We next discuss the necessity of Condition 2.1 via the following example.

Example 2.6. Suppose we are given the problem data:

(cid:20)1

(cid:21)

Q =

0
0 −2

(cid:20)−3
(cid:21)

0

(cid:20)0

(cid:21)

,

g =

, A =

1
0 −1

,

b =

1
2

, K = R2
+.

(cid:21)

(cid:20)1

1

Condition 2.1 is violated. To see this, note that any d satisfying Qd = λQd is of the form d = [0; d2].
1 − 2y2
However, Ad = [d2;−d2], so if d2 (cid:54)= 0, Ad (cid:54)∈ K = R2
2 − 3y1
1 − 3y1 − 2. It is easy to compute the minimizers of f (y) to be the line y1 = 1/2,
and f (y) = 3y2
with value Optf = −11/4. The constraints Ay − b ∈ K are equivalent to −1/2 ≤ y2 ≤ 1/2.

+. For this problem data, h(y) = y2

(a) h(y) ≤ −11/4 = Optf

(b) h(y) ≤ (1 − 6

√
3)/2 = Opth

Figure 1: Contour plots of h(y) over the feasible set.

y = [1/2;±√

Figure 1 shows that the minimizers of h(y) over the unit ball (cid:107)y(cid:107) ≤ 1 lie on the boundary at
3/2]. Due to the linear constraints −1/2 ≤ y2 ≤ 1/2, these points are cut oﬀ from
the feasible region. As a result, any minimizer of f (y) inside the feasible region has norm strictly
less than 1. By Corollary 2.3, the relaxation (4) is not tight. We note that while our relaxation is
not tight for this example, the SDP relaxation of [39, 12] strengthened with additional SOC-RLT
♦
inequalities is tight.

A generalization of Condition 2.1 is instrumental in giving exact convex hull characterization

of the sets associated with the TRS. We discuss these further in Section 3.

2.2 Complexity of Solving the Classical TRS

We note that all of the variants of the TRS that we can provide a convex reformulation are solvable
via interior point methods and standard software as long as the cone K has an explicit barrier
function. In this section, we explore the classical TRS given by

Opth = min

y

h(y) := y(cid:62)Qy + 2g(cid:62)y : (cid:107)y(cid:107) ≤ 1

(5)

(cid:110)

(cid:111)

and its solution via iterative methods. The problem (5) is equivalent to the TRS with conic
constraints given in (2) where A = In, b = 0, and K = Rm. Then by Remark 2.1, Condition 2.1

9

-1.0-0.50.00.51.0-1.0-0.50.00.51.0-1.0-0.50.00.51.0-1.0-0.50.00.51.0is satisﬁed immediately. Consequently, we have the following theorem as a simple corollary of
Theorem 2.4.

Theorem 2.7. A tight convex relaxation of (5) is given by

Optf = min

y

f (y) := y(cid:62)(Q − λQIn)y + 2g(cid:62)y + λQ : (cid:107)y(cid:107) ≤ 1

.

(6)

(cid:110)

(cid:111)

The reformulation (6) is a second-order conic program and can easily be solved whenever we
have λQ. However, computing the minimum eigenvalue of Q, λQ, itself is a TRS with no linear
term because

(cid:110)

This computation of λQ can be done in linear time via the power iteration method [21, Chapter 8.2].
Each iteration of Power method involves a matrix-vector multiplication and requires O(N ) time,

where N is the number of nonzero entries in Q. Thus, computing λQ can be done in O(cid:0)N log(cid:0) 1

(cid:1)(cid:1)

δ

λQ = min

y

y(cid:62)Qy : (cid:107)y(cid:107) ≤ 1

.

(cid:111)

time.

Given λQ, the problem

{f (y) : (cid:107)y(cid:107) ≤ 1}

min

y



√

(cid:17)

(cid:16) λmax(Q)−λQ

is simply minimizing a smooth convex quadratic function over the unit ball. This can be done
eﬃciently using Nesterov’s accelerated gradient descent algorithm [28]. For this class of prob-
lems, Nesterov’s accelerated gradient descent algorithm achieves the optimal convergence rate of
O
. Nesterov’s algorithm is a classical ﬁrst-order method; and the major compu-
tational burden in each iteration is the evaluation of the gradient of objective function. For a
quadratic function, this involves simply a matrix-vector product; and hence each iteration costs
O(N ) time. The only other main operation in each iteration of Nesterov’s algorithm applied to this
problem is the projection onto the Euclidean ball; and this can be done in O(n) time. Consequently,
. Thus, the total complexity of solving (6) is
Nesterov’s algorithm runs in time O

(cid:17)

(cid:16) N (λmax(Q)−λQ)
(cid:18)

(cid:18) 1

(cid:19)

√



(cid:18)

O

N

log

+

δ

(cid:19)(cid:19)

.

λmax(Q) − λQ

√



Remark 2.3. Theorem 2.7 shows that the classical TRS decomposes into two special TRS problems:
one without a linear term, i.e., g = 0, making it a pure minimum eigenvalue problem; and the other
one with a convex quadratic objective function. This once again highlights the connection between
the TRS and eigenvalue problems. Furthermore, our analysis demonstrates that the notorious ‘hard
case’ reported in much of the existing TRS literature, i.e., when g is orthogonal to the eigenspace
♦
of λQ, is no longer an issue for our exact reformulation.

2.2.1 Working with Approximate Eigenvalues
In practice, we will actually form the objective y(cid:62)(Q − γIn)y + 2g(cid:62)y + γ where γ ≈ λQ is an
approximation. Due to this imprecision, we must ensure that the objective remains convex. To
do this, suppose that we solve the minimum eigenvalue problem of Q to within δ-accuracy, and
obtain an approximate solution λQ − δ < λ < λQ + δ. Subtracting δ from the inequality, we obtain
λQ − 2δ < λ− δ < λQ. To ensure the convexity of the objective, we set γ := λ− δ < λQ which is an
underestimate of λQ, ensuring that Q − γIn (cid:31) 0. Let η := λQ − γ which satisﬁes 0 < η < 2δ, and

fη(y) := y(cid:62)(Q − γIn)y + 2g(cid:62)y = y(cid:62)(Q − (λQ − η)In)y + 2g(cid:62)y = f (y) + η(cid:107)y(cid:107)2.

10

Based on this scheme, we next explore the eﬀects of solving
{fη(y) : (cid:107)y(cid:107) ≤ 1}

min

y

(7)

instead of (6). Let y∗ be an optimal solution to the true convex reformulation (6). Let yη be
an optimal solution to (7) and ¯yη be an approximate optimal solution. Then we can bound the
objective value f (¯yη) as

f (¯yη) − f (y∗) = fη(¯yη) − fη(y∗) + η((cid:107)y∗(cid:107)2 − (cid:107)¯yη(cid:107)2) ≤ fη(¯yη) − fη(yη) + η,

where the last inequality follows from (cid:107)y∗(cid:107) ≤ 1 and (cid:107)¯yη(cid:107) ≤ 1. Thus, the convergence rate of ¯yη to
the optimum of (6) is controlled by the size of η and the convergence rate for solving (7).
We can also control the distance between yη and y∗. Because fη(y) is a strongly convex function

with parameter 2η, we have

η (cid:107)y∗ − yη(cid:107)2 ≤ fη(y∗) − fη(yη) + ∇fη(yη)(cid:62)(yη − y∗)

= f (y∗) − f (yη) + ∇fη(yη)(cid:62)(yη − y∗) + η((cid:107)y∗(cid:107)2 − (cid:107)yη(cid:107)2)
≤ η((cid:107)y∗(cid:107)2 − (cid:107)yη(cid:107)2),

where the last inequality follows from the optimality of yη for the problem (7), i.e., ∇fη(yη)(cid:62)(yη −
y∗) ≤ 0, and the optimality of y∗ for the problem (6). Then (cid:107)yη(cid:107) ≤ (cid:107)y∗(cid:107). Also, from (cid:107)y∗(cid:107) ≤ 1,
we deduce that if (cid:107)yη(cid:107) = 1, then y∗ = yη. When (cid:107)yη(cid:107) < 1, the only constraint in our domain is
inactive; and thus we conclude that yη is also optimum for the unconstrained minimization problem.
Then the optimality conditions leads to ∇fη(yη) = 0. This implies that yη = −(Q+(η−λQ)In)−1g.
Moreover, y∗ satisﬁes the optimality condition ∇f (y∗)(cid:62)(y∗ − y) ≤ 0 for all y such that (cid:107)y(cid:107) ≤ 1.
Since our domain is the unit ball, this is true if and only if ∇f (y∗) = −αy∗, for some α ≥ 0.
Therefore, y∗ = −(Q + (α − λQ)In)†g, where A† denotes the pseudo-inverse of a matrix A. If we
denote the ordered eigenvalues of Q by qi and their corresponding orthonormal eigenvectors by ui,
we obtain

n(cid:88)

i=1

(cid:107)yη(cid:107)2 =

(u(cid:62)

i g)2

(qi − qn + η)2

and (cid:107)y∗(cid:107)2 =

n(cid:88)

i=1

(u(cid:62)

i g)2

(qi − qn + α)2 .

Note that it is possible to have α = 0 and qi − qn = 0. However, this happens only when u(cid:62)
so we follow the convention 0

0 = 0. After some simple algebra, we have the equality

i g = 0,

(cid:107)y∗(cid:107)2 − (cid:107)yη(cid:107)2 =

n(cid:88)

i=1

(u(cid:62)

(qi − qn + α)2 − n(cid:88)
n(cid:88)

i g)2

i=1

(u(cid:62)

i g)2

= (η − α)

i=1

(u(cid:62)

i g)2

(qi − qn + η)2
2qi − 2qn + η + α

(qi − qn + α)2(qi − qn + η)2 .

Since (cid:107)y∗(cid:107) ≥ (cid:107)yη(cid:107) and η > 0, we must have η ≥ α. Also, η ≤ α is possible only if yη = y∗. Hence,

11

we have

(cid:107)y∗(cid:107)2 − (cid:107)yη(cid:107)2 = (η − α)+

≤ (η − α)+

= 2(η − α)+

i=1

n(cid:88)
n(cid:88)
n(cid:88)

i=1

i=1

2qi − 2qn + (η − α)+ + 2α

(qi − qn + α)2(qi − qn + (η − α)+ + α)2

(u(cid:62)
i g)2
i g)2 2qi − 2qn + (η − α)+ + 2α
(u(cid:62)
n(cid:88)

(qi − qn + α)4
(qi − qn + α)3 + (η − α)2

i g)2

(u(cid:62)

+

i=1

(u(cid:62)

i g)2

(qi − qn + α)4 .

This shows that (cid:107)y∗(cid:107)2 − (cid:107)yη(cid:107)2 ≤ Cη + o(η), where C = 2(y∗)(cid:62)(Q + (α − λQ)In)†y∗. Therefore,

(cid:107)yη − y∗(cid:107)2 ≤ (cid:107)y∗(cid:107)2 − (cid:107)yη(cid:107)2 ≤ Cη + o(η).

3 Convexiﬁcation of the Epigraph of the TRS
As opposed to the convex reformulation of the TRS and its variants discussed in Section 2.1, in
this section, we employ an SOC based approach to explore stronger results on exact convex hull
representation results for the epigraph of the TRS and its variants.

By deﬁning a new variable xn+2, and moving the nonconvex function from the objective to the

constraints, we can equivalently write (2) as minimizing xn+2 over its epigraph
(cid:107)y(cid:107) ≤ xn+1
xn+1 = 1
Ay − b ∈ K

Opth = min

h(y) = y(cid:62)Qy + 2g(cid:62)y ≤ xn+2

(8)

Since the objective xn+2 is linear, optimizing over the epigraph is equivalent to optimizing over its
convex hull. In this section, under slightly stronger assumptions than in Section 2.1, we give an
explicit characterization of the convex hull of the following epigraphical set:
(cid:107)y(cid:107) ≤ xn+1
xn+1 = 1
Ay − b ∈ K

X :=

(9)

y(cid:62)Qy + 2g(cid:62)y ≤ xn+2

We follow a second-order cone based approach. That is, we focus mainly on the quadratic
parts of the TRS, namely the nonconvex quadratic y(cid:62)Qy + 2g(cid:62)y and the SOC unit ball constraint
(cid:107)y(cid:107) ≤ 1 and provide the convexiﬁcation of this set via a single new SOC constraint. Our approach
is an adaptation of the one from Burer and Kılın¸c-Karzan [13]. We start with a number of relevant
deﬁnitions and conditions and then present their main result.
A cone F + ⊆ Rn is said to be second-order-cone representable (or SOCr) if there exists a matrix
0 (cid:54)= R ∈ Rn×(n−1) and a vector r ∈ Rn such that the nonzero columns of R are linearly independent,
r (cid:54)∈ Range(R), and

F + =

x : (cid:107)R(cid:62)x(cid:107) ≤ r(cid:62)x

,

(10)

(cid:110)

(cid:111)

where (cid:107) · (cid:107) denotes the usual Euclidean norm.

12

y,xn+1,xn+2

xn+2 :
x = [y; xn+1; xn+2] ∈ Rn+2 :

 .
 .

Given an SOCr cone F +, the cone F− := −F + is also SOCr. Based on F + from (10), we deﬁne
A := RR(cid:62) − rr(cid:62) and consider the union F + ∪ (F−) = F + ∪ (−F +) =: F. Note that F corresponds
to a nonconvex cone deﬁned by the homogeneous quadratic inequality x(cid:62)Ax ≤ 0:

(cid:110)
x : (cid:107)R(cid:62)x(cid:107)2 ≤ (r(cid:62)x)2(cid:111)

(cid:110)

x : x(cid:62)Ax ≤ 0

,

=

(cid:111)

F := F + ∪ (F−) =

and apex(F +) = apex(F−) = apex(F) = Null(A) where Null(A) denotes the nullspace of the
matrix A.
The analysis of [13] is mainly based on two cones (not necessarily convex) F0,F1 deﬁned by ho-
mogeneous quadratic inequalities given by the symmetric matrices A0, A1 and relies on the following
conditions:

Condition 3.1. A0 has at least one positive eigenvalue and exactly one negative eigenvalue.
Condition 3.2. There exists ¯x such that ¯x(cid:62)A0 ¯x < 0 and ¯x(cid:62)A1 ¯x < 0.

Condition 3.3. Either (i) A0 is nonsingular, (ii) A0 is singular and A1 is positive deﬁnite on
Null(A0), or (iii) A0 is singular and A1 is negative deﬁnite on Null(A0).

(1 − t)A0 + tA1. Then the set Ft = (cid:8)x : x(cid:62)Atx ≤ 0(cid:9) is obtained by linearly aggregating the

The method of [13] can be roughly described as follows. For any t ∈ [0, 1], deﬁne At

:=

quadratic constraints from the matrices A0 and A1, and immediately gives us a relaxation

0 ∩ F1 ⊆ F +
F +

0 ∩ Ft.

Conditions 3.1–3.3 ensure the existence of a maximal s ∈ [0, 1] such that At has a single negative
eigenvalue for all t ∈ [0, s], At is invertible for all t ∈ (0, s), and As is singular—that is, Null(As)
is nontrivial whenever s < 1. Then, for all At with t ∈ [0, s], SOCr sets Ft = F +
t can be
explicitly characterized (see [13, Section 5.1]). Furthermore, for ¯x of Condition 3.2, noting that
¯x(cid:62)At ¯x = (1− t) ¯xA0 ¯x + t ¯x(cid:62)A1 ¯x < 0, we can choose ¯x ∈ F +
for all such t without loss of generality.
Then [13, Theorem 1] asserts that the closed conic hull of F +
0 ∩ F1), is
s under Conditions 3.1–3.3. Let int (·) stand for the interior
contained in the convex cone F +
of a given set. Furthermore, [13, Theorem 1] establishes cone(F +
s under the
following condition:
Condition 3.4. When s < 1, apex(F +

0 ∩ F1, that is, cone(F +
0 ∩ F +

0 ∩ F1) = F +

t ∪ F−

0 ∩ F +

s ) ∩ int (F1) (cid:54)= ∅.

t

Because the TRS deals with the cross-section of an SOC, the reformulation of the TRS requires a
0 ∩F1 is intersected with an aﬃne hyperplane.

specialization of [13, Theorem 1] for the case when F +
For this, let h ∈ Rn be given; and deﬁne the hyperplanes

H 0 := {x : h(cid:62)x = 0},
H 1 := {x : h(cid:62)x = 1}.

(11)

(12)
0 ∩F1∩ H 1), the following

In order to study the closed convex hull of F +
additional condition related to H0 is introduced in [13]:
Condition 3.5. When s < 1, apex(F +

0 ∩F1∩ H 1, that is, conv(F +

s ) ∩ int (F1) ∩ H 0 (cid:54)= ∅ or F +

0 ∩ F +

s ∩ H 0 ⊆ F1.

13

Conditions 3.1–3.5 are all that is needed to state the main result of [13]. Here, we state [13,

Theorem 1] for completeness.

Theorem 3.1 ([13, Theorem 1]). Suppose Conditions 3.1–3.3 are satisﬁed; and let s be the maximal
s ∈ [0, 1] such that At := (1 − t)A0 + tA1 has a single negative eigenvalue for all t ∈ [0, s]. Then
cone(F +
s , and equality holds under Condition 3.4. Moreover, Conditions 3.1–3.5
imply F +

0 ∩ F1) ⊆ F +
0 ∩ F +

s ∩ H 1 = conv(F +

0 ∩ F +

These convexiﬁcation results were also applied to the classical TRS (5) in [13]. In particular, it

is shown in [13, Section 7.2] that the classical TRS (5) can be reformulated in the form of

the nonconvex feasible set can be convexiﬁed; and then this problem can be solved eﬃciently in
two stages. Speciﬁcally, [13] deﬁne a new variable x = [y; xn+1; xn+2] and the matrices

0 ∩ F1 ∩ H 1).
(cid:26)

−x2

n+2 :

Opth = min
y,xn+2

In

A0 =

0

0
0(cid:62) −1 0
0
0

0

(cid:27)

;

(cid:107)y(cid:107) ≤ 1

y(cid:62)Qy + 2g(cid:62)y ≤ −x2

n+2

g(cid:62) 0 0
0 1
0

 , A1 =
 Q g 0

x = [y; xn+1; xn+2] :
(cid:27)

=

= F +

0 ∩ F1 ∩ {x : xn+1 = 1} .

x(cid:62)A0x ≤ 0
x(cid:62)A1x ≤ 0
xn+1 = 1

which then gives

(cid:26)

[y; 1; xn+2] :

(cid:107)y(cid:107) ≤ 1

y(cid:62)Qy + 2g(cid:62)y ≤ −x2

n+1

(13)



(14)

(15)

It is then proved in [13] that there exists some s ∈ (0, 1) such that

conv(F +

0 ∩ F1 ∩ {x : xn+1 = 1}) = F +

0 ∩ F +

s ∩ {x : xn+1 = 1} .

While the precise value of s is not given in [13], one can show that it is in fact s = 1
Remark 3.1. The reformulation (14) implicitly requires that Opth ≤ 0 because of the constraint
h(y) = y(cid:62)Qy + 2g(cid:62)y ≤ −x2
n+2 ≤ 0. For the classical TRS without additional constraints, this is
not an additional limitation because y = 0 will always be a feasible solution with objective value
0 and thus the optimum solution will have a nonpositive objective value. However, this becomes a
limitation when we want to extend such arguments for the TRS with additional conic constraints
Ay − b ∈ K because Opth may no longer be nonpositive.
♦

1−λQ

.

Due to Remark 3.1, we instead choose to model the TRS as in (8), which allows for positive

objective values. To this end, we deﬁne the matrices

In

A0 =

0

0
0(cid:62) −1 0
0
0

0

 , A1 =

 Q g

g(cid:62)
0 − 1

0
0 − 1
2
0

2

 ,

14

and the corresponding sets

x : y(cid:62)Qy + 2g(cid:62)yxn+1 ≤ xn+1xn+2

x : x(cid:62)A1x ≤ 0

=

,

(16)

(cid:110)

n+1, xn+1 ≥ 0(cid:9) =

0 =(cid:8)x : (cid:107)y(cid:107)2 ≤ x2
(cid:110)

F +
F1 =
K+ = {x : Ay − bxn+1 ∈ K} ,
H 1 = {x : xn+1 = 1} .

x : x(cid:62)A0x ≤ 0, xn+1 ≥ 0

(cid:111)

(cid:110)

(cid:111)

,

(cid:111)

With these deﬁnitions, the epigraph X from (9) can be written as

X = F +

0 ∩ F1 ∩ K+ ∩ H 1.

It is mentioned in [13] that the matrices (15) do not satisfy the necessary conditions to apply
Theorem 3.1 directly. In particular, Condition 3.3 is violated for the choice of matrices (15); and
as a result, the reformulation of the TRS with matrices (13) were opted in [13, Section 7.2] instead.
In contrast, we next show that for the special case of the TRS, via a direct analysis, ﬁnding the
convex hull through linear aggregation of constraints will still carry through for the matrices in
(15). This indicates that while Condition 3.3 is suﬃcient, it is not necessary to obtain the convex
hull result. In fact, we show that the value of s = 1
that works for the matrices (13) will also
1−λQ
work for our matrices (15). More precisely, for s = 1
1−λQ

, we deﬁne

(cid:110)

(cid:111)

(cid:110)

Fs =

x : x(cid:62)Asx ≤ 0

=

x : y(cid:62)(Q − λQIn)y + 2g(cid:62)yxn+1 + λQx2

(cid:111)

n+1 ≤ xn+1xn+2

(17)
0 ∩ Fs ∩ K+ ∩ H 1 directly under the

,

0 ∩ F1 ∩ K+ ∩ H 1) = F +

and prove that conv(X) = conv(F +
following condition.
Condition 3.6. There exists a vector d (cid:54)= 0 such that Qd = λQd and ±Ad ∈ K.
Remark 3.2. Condition 3.6 implies Condition 2.1. To see this, suppose d (cid:54)= 0 satisﬁes Condition 3.6.
Then if g(cid:62)d ≤ 0, d satisﬁes Condition 2.1 also. Otherwise, −d will satisfy Condition 2.1. We
demonstrate that Condition 2.1 does not imply Condition 3.6 in Example 3.5.
Furthermore, Condition 3.6 holds whenever Condition 2.2 of [25] is satisﬁed because Condi-
tion 2.2 implies that there exists d such that Qd = λQd and Ad = 0 and since K is a closed convex
cone, ±Ad = 0 ∈ K as well.
♦

One of the ingredients of our convex hull result is given in the next lemma.

Lemma 3.2. Let Fs be deﬁned as in (17). Then the cone Fs ∩ {x : xn+1 > 0} is convex; and the
set Fs ∩ H 1 where H 1 is as deﬁned in (16) is SOC representable.
Proof. Let x = [y; xn+1; xn+2] ∈ Rn+2. Note that by deﬁnition, we have

x : y(cid:62)(Q − λQIn)y + 2g(cid:62)yxn+1 + λQx2
x : y(cid:62)(Q − λQIn)y ≤ xn+1(xn+2 − 2g(cid:62)y − λQxn+1), xn+1 > 0

=

(cid:111)
(cid:111)
n+1 ≤ xn+1xn+2, xn+1 > 0

Fs ∩ {x : xn+1 > 0}
=

(cid:110)
(cid:110)
(cid:26)

=

x :

y(cid:62)(Q − λQIn)y ≤ xn+1(xn+2 − 2g(cid:62)y − λQxn+1),
xn+1 > 0, xn+2 − 2g(cid:62)y − λQxn+1 ≥ 0

15

(cid:27)

,

where the last equation follows because Q − λQIn (cid:23) 0, we have y(cid:62)(Q − λQIn)y ≥ 0 for all y and
then xn+1 > 0 implies xn+2 − 2g(cid:62)y − λQxn+1 ≥ 0. As a result, xn+1 + xn+2 − 2g(cid:62)y − λQxn+1 ≥ 0
holds for all x ∈ Fs ∩{x : xn+1 > 0}. In addition, from these derivations, we immediately conclude
that the set Fs ∩ {x : xn+1 = 1} is an SOC representable set.
Theorem 3.3. Let F +
holds. Then

0 ,F1, H 1,K+,Fs be deﬁned as in (16) and (17). Assume that Condition 3.6

conv(F +

0 ∩ F1 ∩ K+ ∩ H 1) = F +

0 ∩ Fs ∩ K+ ∩ H 1.

Proof. Let x = [y; xn+1; xn+2] be a vector in F +

0 ∩ K+ ∩ H 1 ∩ Fs. Then x satisﬁes
x(cid:62)A0x ≤ 0,
Ay − bxn+1 ∈ K,
xn+1 = 1,
x(cid:62)Asx ≤ 0.

We will show that x ∈ conv(F +
that is, x(cid:62)A1x > 0. Then from the deﬁnition of s, 0 < x(cid:62)A1x, and x(cid:62)Asx ≤ 0, we have

0 ∩ F1 ∩ K+ ∩ H 1). If x ∈ F1, then we are done. Suppose x (cid:54)∈ F1,

0 < s(x(cid:62)A1x) − x(cid:62)Asx = −(1 − s)x(cid:62)A0x =

λQ
1 − λQ

n+1 − (cid:107)y(cid:107)2).
(x2

Because λQ < 0, this implies (cid:107)y(cid:107)2 < x2
n+1. Let d be the vector given by Condition 3.6 such that
Qd = λQd, ±Ad ∈ K and (cid:107)d(cid:107)2 = 1. We now consider the points xη := [y + ηd; xn+1; xn+2 + 2g(cid:62)dη]
for η ∈ R. We ﬁrst argue that xη ∈ Fs holds for all η ∈ R. To see this, note that

n+1

n+1 − (cid:107)y + ηd(cid:107)2)

n+1 − (cid:107)y(cid:107)2 − 2y(cid:62)dη − η2)

(y + ηd)(cid:62)(Q − λQIn)(y + ηd) + 2g(cid:62)(y + ηd)xn+1 + λQx2
= (y + ηd)(cid:62)Q(y + ηd) + 2g(cid:62)(y + ηd)xn+1 + λQ(x2
= y(cid:62)Qy + 2 y(cid:62)Qd
=λQy(cid:62)d

(cid:124)(cid:123)(cid:122)(cid:125)

(cid:124)(cid:123)(cid:122)(cid:125)

η2 + 2g(cid:62)y + 2g(cid:62)dxn+1η + λQ(x2

η + d(cid:62)Qd

=λQ

n+1 − (cid:107)y(cid:107)2) + 2g(cid:62)dxn+1η
n+1 + 2g(cid:62)dxn+1η

= y(cid:62)Qy + 2g(cid:62)yxn+1 + λQ(x2
= y(cid:62)(Q − λQIn)y + 2g(cid:62)yxn+1 + λQx2
= xn+1xn+2 + (1 − λQ)(x(cid:62)Asx) + 2g(cid:62)dxn+1η
≤ xn+1xn+2 + 2g(cid:62)dxn+1η
= xn+1(xn+2 + 2g(cid:62)dη),

(18)
where the third equation follows from Qd = λQd and (cid:107)d(cid:107)2 = 1, and the inequality holds because
x(cid:62)Asx ≤ 0 and λQ < 0. Then from the inequality (18) and the deﬁnition of Fs in (17), we conclude
xη ∈ Fs for all η ∈ R. Moreover, because (cid:107)y(cid:107)2 < x2
n+1 and d (cid:54)= 0, there must exist δ,  > 0 such
that (cid:107)y − δd(cid:107)2 = (cid:107)y + d(cid:107)2 = x2

n+1. We deﬁne
xδ := [y − δd; xn+1; xn+2 − 2g(cid:62)dδ]
x := [y + d; xn+1; xn+2 + 2g(cid:62)d].

16

Then by our choice of δ, , we have xδ, x ∈ bd(F +
the relation

0 ). From s ∈ (0, 1), xη ∈ Fs for all η ∈ R, and

(xη)(cid:62)Asxη = (1 − s)[(xη)(cid:62)A1xη] + s[(xη)(cid:62)A0xη],

we conclude that xη ∈ F1 for all η such that xη ∈ bd(F +
0 ). In particular, xδ, x ∈ F1. Furthermore,
by Condition 3.6, ±Ad ∈ K, and since K is a cone, −Adδ, Ad ∈ K; thus xδ, x ∈ K+. Finally,
xn+1 = 1 in both xδ, x; so we have xδ, x ∈ F +

0 ∩ F1 ∩ K+ ∩ H 1. Now it is easy to see that

x =



δ + 

xδ +

δ

δ + 

x ∈ conv(F +

0 ∩ F1 ∩ K+ ∩ H 1).

As a consequence, we have the relation

0 ∩ F1 ∩ K+ ∩ H 1 ⊆ F +
F +

0 ∩ Fs ∩ K+ ∩ H 1 ⊆ conv(F +

0 ∩ F1 ∩ K+ ∩ H 1).

By Lemma 3.2, the set Fs ∩ H 1 is SOC representable and hence convex; this implies that F +
0 ∩
Fs ∩K+ ∩ H 1 is convex also. Then taking the convex hull of all terms in the above inequality gives
us the result.

Note that the set F +

0 ∩ Fs ∩ K+ ∩ H 1 is closed and our explicit convex hull result becomes a

simple corollary of Theorem 3.3.

Corollary 3.4. Let X be the set deﬁned in (9). Then under Condition 3.6

y(cid:62)(Q − λQIn)y + 2g(cid:62)y + λQ ≤ xn+2

(cid:107)y(cid:107) ≤ 1
Ay − b ∈ K

 .

Opth = min

= min

y

h(y) = y(cid:62)Qy + 2g(cid:62)y :

(cid:107)y(cid:107) ≤ 1
Ay − b ∈ K
f (y) = y(cid:62)(Q − λQIn)y + 2g(cid:62)y + λQ :

(cid:27)

(cid:107)y(cid:107) ≤ 1
Ay − b ∈ K

(cid:27)

.

conv(X) =

As a result,

x = [y; 1; xn+2] :
(cid:26)
(cid:26)

y

Remark 3.3. In general, a tight convex relaxation for a nonconvex optimization problem does not
necessarily imply that the epigraph of the convex relaxation is giving the exact convex hull of the
epigraph of the nonconvex optimization problem. However, in the particular case of the TRS with
additional conic constraints, i.e., problem (2), under Condition 3.6, Corollary 3.4 shows that not
only our convex relaxation given by (4) is tight but also we can characterize the convex hull of its
♦
epigraph exactly.

As a consequence of Corollary 3.4 and Remark 3.2, we note that in all of the cases where
Jeyakumar and Li [25] show the tightness of their convex reformulation, i.e. robust least squares
and robust SOC programming, we can further give the exact convex hull characterizations.

We next discuss the necessity of Condition 3.6 for giving conv(X) via the following example.

17

(a) X = F +

0 ∩ F1 ∩ K+ ∩ H 1

(b) F +

0 ∩ Fs ∩ K+ ∩ H 1

(c) X vs F +

0 ∩ Fs ∩ K+ ∩ H 1

Figure 2: Plots of the epigraph of Example 3.5.

Example 3.5. Consider the following problem with the data given by

Q =

0
0 −1

,

g =

1

b =

1
2

, K = R+.

(cid:20)1

(cid:21)

(cid:21)

(cid:20)0

, A =(cid:2)0 −1(cid:3) ,

Note that Condition 3.6 is violated. To see this, any vector d such that Qd = λQd is of the form
d = [0; d2]. But then Ad = −d2. Hence, if d2 > 0 then Ad (cid:54)∈ K; and similarly, if d2 < 0 then
−Ad (cid:54)∈ K.
Figure 2(c) shows that the convex relaxation for the epigraph X = F +
0 ∩ F1 ∩ K+ ∩ H 1 given by
0 ∩ Fs ∩ K+ ∩ H 1 does not give the convex hull of X. Note also that Condition 2.1 is satisﬁed
F +
for this example by taking d = [0; 1]; so by Theorem 2.4, the SOC optimization problem (4) is a
tight relaxation for (2). Despite this, we cannot give the exact convex hull characterization because
Condition 3.6 is violated.

Note that in this example, our convex relaxation simply gives the convex hull of the set obtained
by eliminating the additional linear inequality constraint because Condition 3.6 is satisﬁed for that
♦
set. This is illustrated in Figure 3 below.

3.1 Additional Convex and Nonconvex Constraints
In this section we explore additional constraints y ∈ H included in the domain of the TRS, where
H is a given, convex or nonconvex set. More precisely, we derive the convex hull of the set X ∩
H+ = F0 ∩ F1 ∩ K+ ∩ H 1 ∩ H+ where F0, F1, K+, and H 1 are as deﬁned in (16), and H+ :=
{[y; xn+1; xn+2] : y ∈ H}.

Our analysis relies on the following technical lemma which is a generalization of [13, Lemma 5].

Lemma 3.6. Let F and H+ be sets such that conv(F ) ∩ bd(H+) ⊆ F ∩ H+. Then

conv(F ) ∩ H+ ⊆ conv(F ∩ H+).

Proof. Consider a point x from conv(F )∩H+. If x ∈ bd(H+), then by the hypothesis of the lemma,
x ∈ F ∩ H+ ⊆ conv(F ∩ H+).

18

(a) X = F +

0 ∩ F1 ∩ K+ ∩ H 1

(b) F +

0 ∩ Fs ∩ K+ ∩ H 1

(c) X vs F +

0 ∩ Fs ∩ K+ ∩ H 1

Figure 3: Plots of the epigraph of Example 3.5 without the linear inequality.

x =(cid:80)

k λkxk where λk > 0,(cid:80)

Now suppose x ∈ relint (H+). Since x ∈ conv(F ), we can write x as a ﬁnite convex combination
k λk = 1 and xk ∈ F . Let I = {k : xk ∈ H+} and J = {k : xk (cid:54)∈ H+}.
If J = ∅, then we are done since x ∈ conv(F ∩H+). If not, for each j ∈ J, the line segment between
x and xj must intersect bd(H+) because x ∈ relint (H+) and xj (cid:54)∈ H+. Thus, there exists a point
zj = αjx + (1 − αj)xj ∈ bd(H+) for some αj ∈ (0, 1). Note that zj ∈ conv(F ) because it is
a convex combination of x, xj ∈ conv(F ). By a reasoning similar to the above, we deduce that
zj ∈ F ∩ bd(H+). We now rewrite

 x =

(cid:88)

i∈I

(cid:88)

j∈J

λixi +

λj
1 − αj

zj.

(cid:88)

i∈I

(cid:88)

j∈J

x =

λixi +

λj
1 − αj

(zj − αjx) ⇐⇒

Notice also that

1 +

1 +
(cid:88)
(cid:88)
(cid:88)

j∈J

j∈J

j∈J

(cid:88)

j∈J

αjλj
1 − αj

(cid:88)

αjλj
1 − αj

(cid:88)

j∈J

+

j∈J

λj +
(1 − αj)λj
1 − αj
λj
1 − αj

.

αjλj
1 − αj

(cid:88)

j∈J

i∈I

(cid:88)
(cid:88)
(cid:88)

i∈I

i∈I

αjλj
1 − αj

=

=

=

λi +

λi +

λi +

Then we conclude that x is a convex combination of points xi, zj ∈ F ∩ H+ as desired.

3.1.1 Additional Hollow Constraints
Here we consider a hollow constraint y ∈ H given by H = Rn \ P where P is a given possibly
nonconvex set. We impose the following condition on H = Rn \ P.

Condition 3.7. The set P ⊆ Rn satisﬁes P ⊆(cid:8)y : (cid:107)y(cid:107)2 < 1, Ay − b ∈ K(cid:9).
i=1 Ei is a union of open ellipsoids Ei = (cid:8)y : y(cid:62)Aiy + 2b(cid:62)
If P = (cid:83)m
(cid:111)

i y + ci ≤ 0(cid:9), then Condi-

tion 3.7 can be checked by solving

(cid:110)

1 − (cid:107)y(cid:107)2 : y(cid:62)Aiy + 2b(cid:62)

i y + ci ≤ 0

.

vi = min

y

19

In particular, Ei satisﬁes Condition 3.7 if and only if vi > 0. While this is a nonconvex quadratic
program, our developments from the previous section give a tight SOC reformulation for it. In addi-
tion, the S-lemma ensures that the associated semideﬁnite relaxation is tight. Thus, Condition 3.7
can be veriﬁed eﬃciently when P is a union of ellipsoids.
A number of papers have studied conditions similar to Condition 3.7. Most notably, the gener-
alized trust region subproblem corresponds to the case when H is a single lower bounded, nonconvex
quadratic constraint y(cid:62)Dy ≥ l; this is studied in [5, 8, 32, 36, 39] for example. More recently, Yang
et al. [38] study the case when the hollow set P is the disjoint union of ellipsoids which do not
intersect the boundary of the unit ball {y : (cid:107)y(cid:107) ≤ 1}. A number of these papers [32, 38, 39] show
that the natural SDP relaxation is tight. As opposed to these results on tight SDP relaxations, it
is shown in [7] that the general quadratic programming problem

0 y : y(cid:62)Qiy + 2g(cid:62)

i y + ci ≤ 0, i = 1, . . . , m

(cid:111)

(cid:110)

y(cid:62)Q0y + 2g(cid:62)

min

y

is polynomially solvable using a weak feasibility oracle, under the assumption that at least one
i y + ci ≤ 0 is strictly convex. In a similar vein, [9] also study
quadratic constraint y(cid:62)Qiy + 2g(cid:62)
the TRS with additional ellipsoidal hollow constraints. In [9], instead of giving the convex hull,
they explore conditions that allow for polynomial solvability using a combinatorial enumeration
technique and thus are able to cover cases where the hollow set P may not be contained in the unit
ball. On a related domain, [8] studies the characterization and separation of valid linear inequalities
that convexify the epigraph of a convex, diﬀerentiable function whose domain is restricted to the
complement of a convex set deﬁned by linear or convex quadratic inequalities.

We note that these papers [5, 7, 8, 9, 32, 36, 38, 39] consider the more general case of minimizing
an arbitrary quadratic objective, which can be convex, over a domain given by possibly nonconvex
quadratic constraints. On the other hand, our result applies to the special case of minimizing a
nonconvex quadratic over the unit ball, a convex quadratic constraint. As a result, we are able to
relax the assumptions that the hollow set P is generated by quadratics and the ellipsoidal hollows
are disjoint. Speciﬁcally, we show that under Condition 3.7, the original SOC based reformulation
of the TRS with additional conic constraints obtained by ignoring H is tight.

We start with the following lemma on the structure of extreme points of conv(X). Given a set

S, we let Ext(S) denote the set of extreme points of S and Rec(S) denote its recession cone.

Lemma 3.7. Let X be deﬁned as in (9); and suppose that Condition 3.6 holds. Let x = [y; 1; xn+2]
be an extreme point of conv(X). Then x must satisfy (cid:107)y(cid:107) = 1.
Proof. Suppose for contradiction that (cid:107)y(cid:107) < 1. By Corollary 3.4, x ∈ conv(X) must satisfy

(cid:107)y(cid:107) ≤ 1
xn+2 ≥ y(cid:62)(Q − λQIn)y + 2g(cid:62)y + λQ
Ay − b ∈ K.

Let d (cid:54)= 0 be the vector given by Condition 3.6. Since d satisﬁes Qd = λQd and ±Ad ∈ K, for any
 ∈ R,

(y + d)(cid:62)(Q − λQIn)(y + d) + 2g(cid:62)(y + d) + λQ

= [y(cid:62)(Q − λQIn)y + 2g(cid:62)y + λQ] + 2g(cid:62)d
≤ xn+2 + 2g(cid:62)d.

20

Since (cid:107)y(cid:107)2 < 1, there exists  > 0 suﬃciently small such that (cid:107)y ± d(cid:107)2 ≤ 1. Note that

A(y ± d) − b ∈ K since ±Ad ∈ K. Now deﬁne the points

x+ := [y + d; 1; xn+2 + 2g(cid:62)d]

and x− := [y − d; 1; xn+2 − 2g(cid:62)d].

Both x+, x− satisfy all constraints of conv(X), and hence x+, x− ∈ conv(X). In particular, x+ (cid:54)=
x (cid:54)= x−, but x = 1
Theorem 3.8. Let X be deﬁned in (9) and H = Rn \P be a set satisfying Condition 3.7. Assume
Condition 3.6 also holds. Then

2 (x+ + x−); and thus x cannot be an extreme point of conv(X).

[y; 1; xn+2] :


conv

(cid:107)y(cid:107) ≤ 1
y ∈ H
Ay − b ∈ K


 = conv(X).
(cid:9). Then

Proof. Deﬁne P + =(cid:8)[y; 1; xn+2] : y ∈ P, y(cid:62)Qy + 2g(cid:62)y ≤ xn+2

y(cid:62)Qy + 2g(cid:62)y ≤ xn+2

H+ = Rn+2 \ P + = {[y; 1; xn+2] : y ∈ H} ∪(cid:110)
[y; 1; xn+2] :

X ∩ H+ =

and hence

[y; 1; xn+2] : y(cid:62)Qy + 2g(cid:62)y > xn+2

(cid:107)y(cid:107) ≤ 1
y ∈ H
Ay − b ∈ K

y(cid:62)Qy + 2g(cid:62)y ≤ xn+2

 .

(cid:111)

,

Now by our deﬁnition of P + and the fact that P ⊆ {y : (cid:107)y(cid:107) < 1, Ay − b ∈ K}, we have bd(P +) =
bd(H+) and conv(X) ∩ bd(H+) ⊆ X because bd(P +) ⊆ X. By Lemma 3.6, we have

X ∩ H+ ⊆ conv(X) ∩ H+ ⊆ conv(X ∩ H+),

and taking the convex hull of all sides gives us

conv(conv(X) ∩ H+) = conv(X ∩ H+).

It remains to show that conv(conv(X) ∩ H+) = conv(X). We can immediately deduce that
conv(X) ⊇ conv(conv(X) ∩ H+); we next prove the reverse direction.

By Condition 3.7, P ⊆(cid:8)y : (cid:107)y(cid:107)2 < 1(cid:9), so by Lemma 3.7, we have Ext(conv(X)) ∩ P + = ∅, or

equivalently

Ext(conv(X)) ⊆ H+.

Intersecting both sides with conv(X), we obtain Ext(conv(X)) ⊆ conv(X) ∩ H+. Note that
conv(X) = conv(Ext(conv(X))) + Rec(conv(X)); so

conv(X) = conv(Ext(conv(X))) + Rec(conv(X)) ⊆ conv(conv(X) ∩ H+) + Rec(conv(X)).

Furthermore, from Corollary 3.4, it is easy to see that

Rec(conv(X) ∩ H+) = {[0; 0; xn+2] : xn+2 ≥ 0} = Rec(conv(X)).

21

Hence, we deduce that

conv(X) ⊆ conv(conv(X) ∩ H+) + Rec(conv(X))

= conv(conv(X) ∩ H+) + Rec(conv(X) ∩ H+)
= conv(conv(X) ∩ H+).

The last equality holds because conv(S) = conv(S) + Rec(S) for any set S. This gives us the result.

3.1.2 Additional Convex Constraints
Lemma 3.6 allows us to add further convex constraints x ∈ H as long as these new constraints
satisfy the following boundary condition.
Condition 3.8. The new set H satisﬁes F +
Corollary 3.9. Suppose H is convex and Conditions 3.6 and 3.8 are satisﬁed. Then we have

0 ∩ Fs ∩ K+ ∩ H 1 ∩ bd(H) ⊆ F1.

conv(F +

0 ∩ F1 ∩ K+ ∩ H 1 ∩ H) = F +

0 ∩ Fs ∩ K+ ∩ H 1 ∩ H.

Proof. From Theorem 3.3, we have conv(F +
is convex, this immediately implies conv(F +
Condition 3.8 gives us F +
we satisfy the premise conv(F ) ∩ bd(H) ⊆ F ∩ H of Lemma 3.6. From Lemma 3.6, we deduce

0 ∩F1∩K+∩ H 1∩ bd(H) ⊆ F1. Then by selecting F = F +

0 ∩ Fs ∩ K+ ∩ H 1. When H
0 ∩ F1 ∩ K+ ∩ H 1 ∩ H.
0 ∩F1∩K+∩ H 1,

0 ∩ F1 ∩ K+ ∩ H 1) = F +
0 ∩ F1 ∩ K+ ∩ H 1 ∩ H) ⊆ F +

conv(cid:0)F +

0 ∩ F1 ∩ K+ ∩ H 1 ∩ H(cid:1) ⊇ F +

0 ∩ Fs ∩ K+ ∩ H 1 ∩ H.

Acknowledgments
This research is supported in part by NSF grant CMMI 1454548.

References

[1] F. Alizadeh. Interior point methods in semideﬁnite programming with applications to combi-

natorial optimization. SIAM Journal on Optimization, 5(1):13–51, 1995.

[2] A. Beck. Convexity properties associated with nonconvex quadratic matrix functions and
applications to quadratic programming. Journal of Optimization Theory and Applications,
142(1):129, 2009.

[3] A. Beck and Y. C. Eldar. Strong duality in nonconvex quadratic optimization with two

quadratic constraints. SIAM Journal on Optimization, 17(3):844–860, 2006.

[4] A. Ben-Tal, L. El Ghaoui, and A. Nemirovski. Robust Optimization. Princeton University

Press. Princeton Series in Applied Mathematics, Philadelphia, PA, USA, 2009.

[5] A. Ben-Tal and M. Teboulle. Hidden convexity in some nonconvex quadratically constrained

quadratic programming. Mathematical Programming, 72(1):51–63.

22

[6] D. Bertsimas, D. B. Brown, and C. Caramanis. Theory and applications of robust optimization.

SIAM review, 53(3):464–501, 2011.

[7] D. Bienstock. A note on polynomial solvability of the CDT problem. SIAM Journal on

Optimization, 26(1):488–498, 2016.

[8] D. Bienstock and A. Michalka. Cutting-planes for optimization of convex functions over non-

convex sets. SIAM Journal on Optimization, 24(2):643–677, 2014.

[9] D. Bienstock and A. Michalka. Polynomial solvability of variants of the trust-region sub-
In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete

problem.
Algorithms, pages 380–390, 2014.

[10] C. Buchheim, M. De Santis, L. Palagi, and M. Piacentini. An exact algorithm for nonconvex
quadratic integer minimization using ellipsoidal relaxations. SIAM Journal on Optimization,
23(3):1867–1889, 2013.

[11] S. Burer. A gentle, geometric introduction to copositive optimization. Mathematical Program-

ming, 151(1):89–116, 2015.

[12] S. Burer and K. M. Anstreicher. Second-order-cone constraints for extended trust-region

subproblems. SIAM Journal on Optimization, 23(1):432–451, 2013.

[13] S. Burer and F. Kılın¸c-Karzan. How to convexify the intersection of a second order cone and a
nonconvex quadratic. Technical report, June 2014. http://www.optimization-online.org/
DB_FILE/2014/06/4383.pdf.

[14] S. Burer and B. Yang. The trust region subproblem with non-intersecting linear constraints.

Mathematical Programming, 149(1):253–264, 2014.

[15] A. R. Conn, N. I. M. Gould, and P. L. Toint. Trust-Region Methods. MPS/SIAM Series on

Optimization. SIAM, Philadelphia, PA, 2000.

[16] L. L. Dines. On the mapping of quadratic forms. Bull. Amer. Math. Soc., 47(6):494–498, 06

1941.

[17] J. B. Erway and P. E. Gill. A subspace minimization method for the trust-region step. SIAM

Journal on Optimization, 20(3):1439–1461, 2010.

[18] J. B. Erway, P. E. Gill, and J. D. Griﬃn. Iterative methods for ﬁnding a trust-region step.

SIAM Journal on Optimization, 20(2):1110–1131, 2009.

[19] A. L. Fradkov and V. A. Yakubovich. The S-procedure and duality relations in nonconvex
problems of quadratic programming. Vestn. LGU, Ser. Mat., Mekh., Astron, (1):101–109,
1979.

[20] W. Gander, G. H. Golub, and U. von Matt. A constrained eigenvalue problem. Linear Algebra

and its Applications, 114115:815 – 839, 1989. Special Issue Dedicated to Alan J. Hoﬀman.

[21] G. H. Golub and C. F. Van Loan. Matrix computations. Johns Hopkins studies in the mathe-

matical sciences. The Johns Hopkins University Press, Baltimore, London, 1996.

23

[22] N. I. M. Gould, S. Lucidi, M. Roma, and P. L. Toint. Solving the trust-region subproblem

using the Lanczos method. SIAM Journal on Optimization, 9(2):504–525 (electronic), 1999.

[23] N. I. M. Gould, D. P. Robinson, and H. S. Thorne. On solving trust-region and other regularized

subproblems in optimization. Mathematical Programming Computation, 2(1):21–57, 2010.

[24] E. Hazan and T. Koren. A linear-time algorithm for trust region problems. Mathematical

Programming, pages 1–19, 2015.

[25] V. Jeyakumar and G. Y. Li. Trust-region problems with linear inequality constraints: Ex-
act SDP relaxation, global optimality and robust optimization. Mathematical Programming,
147(1):171–206, 2013.

[26] S. Modaresi and J. Vielma. Convex hull of two quadratic or a conic quadratic and a quadratic
inequality. Technical report, November 2014. http://www.optimization-online.org/DB_
HTML/2014/11/4641.html.

[27] J. J. Mor´e and D. C. Sorensen. Computing a trust region step. SIAM Journal on Scientiﬁc

and Statistical Computing, 4(3):553–572, 1983.

[28] Y. Nesterov. A method for solving a convex programming problem with rate of convergence

O(1/k2). Soviet Math. Dokl., 27(2):372–376, 1983.

[29] Y. Nesterov and A. Nemirovski. Interior-Point Polynomial Algorithms in Convex Program-

ming. Society for Industrial and Applied Mathematics, 1994.

[30] J. Nocedal and S. Wright. Numerical Optimization. Springer Series in Operations Research

and Financial Engineering. Springer New York, 2000.

[31] I. P´olik and T. Terlaky. A survey of the s-lemma. SIAM Review, 49(3):371–418, 2007.

[32] T. K. Pong and H. Wolkowicz. The generalized trust region subproblem. Computational

Optimization and Applications, 58(2):273–322, 2014.

[33] F. Rendl and H. Wolkowicz. A semideﬁnite framework for trust region subproblems with

applications to large scale minimization. Mathematical Programming, 77(2):273–299, 1997.

[34] M. Rojas, S. A. Santos, and D. C. Sorensen. A new matrix-free algorithm for the large-scale

trust-region subproblem. SIAM Journal on Optimization, 11(3):611–646, 2001.

[35] D. C. Sorensen. Minimization of a large-scale quadratic function subject to a spherical con-

straint. SIAM Journal on Optimization, 7(1):141–161, 1997.

[36] R. J. Stern and H. Wolkowicz. Indeﬁnite trust region subproblems and nonsymmetric eigen-

value perturbations. SIAM Journal on Optimization, 5(2):286–313, 1995.

[37] J. F. Sturm and S. Zhang. On cones of nonnegative quadratic functions. Mathematics of

Operations Research, 28(2):246–267, 2003.

[38] B. Yang, K. Anstreicher, and S. Burer. Quadratic programs with hollows. Technical report,

January 2016. http://www.optimization-online.org/DB_FILE/2016/01/5277.pdf.

24

[39] Y. Ye and S. Zhang. New results on quadratic minimization. SIAM Journal on Optimization,

14(1):245–267, 2003.

[40] H. Zhang, A. R. Conn, and K. Scheinberg. A derivative-free algorithm for least-squares mini-

mization. SIAM Journal on Optimization, 20(6):3555–3576, 2010.

25

