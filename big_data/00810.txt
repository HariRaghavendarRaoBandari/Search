Character-based Neural Machine Translation

Marta R. Costa-juss`a and Jos´e A. R. Fonollosa

TALP Research Center

Universitat Polit`ecnica de Catalunya, Barcelona
{marta.ruiz,jose.fonollosa}@upc.edu

6
1
0
2

 
r
a

M
2

 

 
 
]
L
C
.
s
c
[
 
 

1
v
0
1
8
0
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

Neural Machine Translation (MT) has
reached state-of-the-art results. However,
one of the main challenges that neural MT
still faces is dealing with very large vo-
cabularies and morphologically rich lan-
guages.
In this paper, we propose a neural MT
system using character-based embeddings
in combination with convolutional and
highway layers to replace the standard
lookup-based word representations. The
resulting unlimited-vocabulary and afﬁx-
aware source word embeddings are tested
in a state-of-the-art neural MT based on
an attention-based bidirectional recurrent
neural network. The proposed MT scheme
completely avoids the problem of un-
known source words and provides im-
proved results even when the source lan-
guage is not morphologically rich. The
number of target words is still limited
by the standard word-based softmax out-
put layer. However the number of un-
knowns at the output of the translation net-
work is dramatically reduced (by a relative
66%) with a signiﬁcant overall improve-
ment over both neural and phrase-based
baselines.
Improvements up to 3 BLEU
points are obtained in the German-English
WMT task.

1

Introduction

Machine Translation (MT) is the set of algorithms
that aim at transforming a source language into
a target language. For the last 20 years, one of
the most popular approaches has been statistical
phrase-based MT, which uses a combination of

features to maximise the probability of the tar-
get sentence given the source sentence (Koehn et
al., 2003). Just recently, the neural MT approach
has appeared (Kalchbrenner and Blunsom, 2013;
Sutskever et al., 2014; Cho et al., 2014; Bahdanau
et al., 2015) and obtained state-of-the-art results.
Among its different strengths neural MT does
not need to pre-design feature functions before-
hand; optimizes the entire system at once because
it provides a fully trainable model; uses word em-
beddings (Sutskever et al., 2014) so that words (or
minimal units) are not independent anymore; and
is easily extendable to multimodal sources of in-
formation (Elliott et al., 2015). As for weaknesses,
neural MT has a strong limitation in vocabulary
due to its architecture and it is difﬁcult and com-
putationally expensive to tune all parameters in the
deep learning structure.

In this paper, we use the neural MT baseline
system from (Bahdanau et al., 2015), which fol-
lows an encoder-decoder architecture with atten-
tion, and introduce elements from the character-
based neural language model (Kim et al., 2016).
The translation unit continues to be the word, and
we continue using word embeddings related to
each word as an input vector to the bidirectional
recurrent neural network (attention-based mecha-
nism). The difference is that now the embeddings
of each word are no longer an independent vec-
tor, but are computed from the characters of the
corresponding word. The system architecture has
changed in that we are using a convolutional neu-
ral network (CNN) and a highway network over
characters before the attention-based mechanism
of the encoder. This is a signiﬁcant difference
from previous work (Sennrich et al., 2015) which
uses the neural MT architecture from (Bahdanau
et al., 2015) without modiﬁcation to deal with sub-
word units (but not including unigram characters).
Subword-based representations have already

been explored in Natural Language Process-
ing (NLP), e.g.
for POS tagging (Santos and
Zadrozny, 2014), name entity recognition (San-
tos and aes, 2015), parsing (Ballesteros et al.,
2015), normalization (Chrupala, 2014) or learning
word representations (Botha and Blunsom, 2014;
Chen et al., 2015). These previous works show
different advantages of using character-level in-
formation.
In our case, with the new character-
based neural MT architecture, we take advantage
of intra-word information, which is proven to be
extremely useful in other NLP applications (San-
tos and Zadrozny, 2014; Ling et al., 2015), espe-
cially when dealing with morphologically rich lan-
guages. When using the character-based source
word embeddings in MT, there ceases to be un-
known words in the source input, while the size
of the target vocabulary remains unchanged. Al-
though the target vocabulary continues with the
same limitation as in the standard neural MT sys-
tem, the fact that there are no unknown words
in the source helps to reduce the number of un-
knowns in the target. Moreover, the remaining un-
known target words can now be more successfully
replaced with the corresponding source-aligned
words. As a consequence, in addition to obtain-
ing a clear reduction of unknown words (66% rela-
tive), we obtain a signiﬁcant improvement in terms
of translation quality (up to 3 BLEU points).

The rest of the paper is organized as follows.
Section 2 brieﬂy explains the architecture of the
neural MT that we are using as a baseline sys-
tem. Section 3 describes the changes introduced in
the baseline architecture in order to use character-
based embeddings instead of the standard lookup-
based word representations. Section 4 reports the
experimental framework and the results obtained
in the German-English WMT task. Finally, sec-
tion 5 concludes with the contributions of the pa-
per and further work.

2 Neural Machine Translation

Neural MT uses a neural network approach to
compute the conditional probability of the tar-
get sentence given the source sentence (Cho et
al., 2014; Bahdanau et al., 2015). The approach
used in this work (Bahdanau et al., 2015) fol-
lows the encoder-decoder architecture.First, the
encoder reads the source sentence s = (s1, ..sI )
and encodes it into a sequence of hidden states
h = (h1, ..hI ). Then, the decoder generates a

corresponding translation t = t1, ..., tJ based on
the encoded sequence of hidden states h. Both en-
coder and decoder are jointly trained to maximize
the conditional log-probability of the correct trans-
lation.

This baseline autoencoder architecture is im-
proved with a attention-based mechanism (Bah-
danau et al., 2015), in which the encoder uses
a bi-directional gated recurrent unit (GRU). This
GRU allows for a better performance with long
sentences. The decoder also becomes a GRU and
each word tj is predicted based on a recurrent hid-
den state, the previously predicted word tj−1, and
a context vector. This context vector is obtained
from the weighted sum of the annotations hk,
which in turn, is computed through an alignment
model αjk (a feedforward neural network). This
neural MT approach has achieved competitive re-
sults against the standard phrase-based system in
the WMT 2015 evaluation (Jean et al., 2015).

3 Character-based Machine Translation

Word embeddings have been shown to boost the
performance in many NLP tasks, including ma-
chine translation. However, the standard lookup-
based embeddings are limited to a ﬁnite-size vo-
cabulary for both computational and sparsity rea-
sons. Moreover, the orthographic representation
of the words is completely ignored. The standard
learning process is blind to the presence of stems,
preﬁxes, sufﬁxes and any other kind of afﬁxes in
words.

As a solution to those drawbacks, new alterna-
tive character-based word embeddings have been
recently proposed for tasks as language model-
ing (Kim et al., 2016; Ling et al., 2015), parsing
(Ballesteros et al., 2015) or POS tagging (Ling et
al., 2015; Santos and Zadrozny, 2014).

For our experiments in neural MT, we selected
the best character-based embedding architecture
proposed by Kim et al. (Kim et al., 2016) for lan-
guage modeling. As the Figure 1 shows, the com-
putation of the representation of each word starts
with a character-based embedding layer that as-
sociates each word (sequence of characters) with
a sequence of vectors. This sequence of vectors
is then processed with a set of 1D convolution
ﬁlters of different lengths (from 1 to 7 charac-
ters) followed with a max pooling layer. For each
convolutional ﬁlter, we keep only the output with
the maximum value. The concatenation of these

max values already provides us with a representa-
tion of each word as a vector with a ﬁxed length
equal to the total number of convolutional ker-
nels. However, the addition of two highway layers
was shown to improve the quality of the language
model in (Kim et al., 2016) so we also kept these
additional layers in our case. The output of the
second Highway layer will give us the ﬁnal vec-
tor representation of each source word, replacing
the standard source word embedding in the neural
machine translation system.

Figure 1: Character-based word embedding

In the target size we are still limited in vocabu-
lary by the softmax layer at the output of the net-
work and we kept the standard target word em-
beddings in our experiments. However, the results
seem to show that the afﬁx-aware representation of
the source words has a positive inﬂuence on all the
components of the network. The global optimiza-
tion of the integrated model forces the translation
model and the internal vector representation of the
target words to follow the afﬁx-aware codiﬁcation
of the source words.
4 Experimental framework
This section reports the data used, its preprocess-
ing, baseline details and results with the enhanced
character-based neural MT system.

4.1 Data
We used the German-English WMT data1 includ-
ing the EPPS, NEWS and Commoncrawl. Pre-
truecasing,
processing consisted of tokenizing,

normalizing punctuation and ﬁltering sentences
with more than 5% of their words in a language
other than German or English. Statistics are shown
in Table 1.

Set

L
De Train
Dev
Test
En Train
Dev
Test

S

W

V

OOV

63.1k
44.1k

3.5M 77.7M 1.6M
13.6k
3k
2.2k
9.8k
3.5M 81.2M 0.8M
10.1k
3k
2.2k
7.8k

67.6k
46.8k

-

1.7k
1.3k

-
0.8
0.6k

Table 1: Corpus details. Number of sentences (S),
words (W), vocabulary (V) and out-of-vocabulary-
words (OOV) per set and language (L). M standing
for millions, k standing for thousands.

4.2 Baseline systems
The phrase-based system was built using Moses
(Koehn et al., 2007), with standard parameters
such as grow-ﬁnal-diag for alignment, Good-
Turing smoothing of the relative frequencies, 5-
gram language modeling using Kneser-Ney dis-
counting, and lexicalized reordering, among oth-
ers. The neural-based system was built using the
software from DL4MT2 available in github. We
generally used settings from previous work (Jean
et al., 2015): networks have an embedding of 620
and a dimension of 1024, a batch size of 32, and
no dropout. We used a vocabulary size of 90 thou-
sand words in German-English. Also, as proposed
in (Jean et al., 2015) we replaced unknown words
(UNKs) with the corresponding source word using
the alignment information.

4.3 Results
Table 3 shows the BLEU results for the baseline
systems (including phrase and neural-based, NN)
and the character-based neural MT (CHAR). We
also include the results for the CHAR and NN
systems with post-processing of unknown words,
which consists in replacing the UNKs with the cor-
responding source word (+Src), as suggested in
(Jean et al., 2015). BLEU results improve by al-
most 1.5 points in German-to-English and by more
than 3 points in English-to-German. The reduc-
tion in the number of unknown words is of 66%
relative in German-to-English and 15% relative in
English-to-German. This reduction in unknown

1http://www.statmt.org/wmt15/translation-task.html

2http://dl4mt.computing.dcu.ie/

!"#$%&’()*)+,)$-)./0.-1&"&-2)".)3’)!!#$456,(2#7().-/$8/(,2#/$.0#(2)"5./0.!#00)")$2.()$41256&9./,27,2./0.)&-1.0#(2)":#41;&<.=&<)":#41;&<.=&<)":#41;&<.=&<)"51

4

3

4

5

SRC
NN
CHAR
REF
SRC
NN
CHAR
REF
SRC
NN
CHAR
REF
SRC
NN
CHAR
REF
SRC
NN
CHAR
REF

Berichten zufolge hofft Indien darber hinaus auf einen Vertrag zur Verteidigungszusammenarbeit zwischen den beiden Nationen .
according to reports , India also hopes to establish a contract for the UNK between the two nations .
according to reports , India hopes to see a Treaty of Defence Cooperation between the two nations .
India is also reportedly hoping for a deal on defence collaboration between the two nations .
der durchtrainierte Mainzer sagt von sich , dass er ein “ ambitionierter Rennradler “ ist .
the UNK Mainz says that he is a “ ambitious , . “
the UNK in Mainz says that he is a ’ ambitious racer ’ .
the well-conditioned man from Mainz said he was an “ ambitious racing cyclist . “
die GDL habe jedoch nicht gesagt , wo sie streiken wolle , so dass es schwer sei , die Folgen konkret vorherzusehen .
however , the UNK did not tell which they wanted to UNK , so it is difﬁcult to predict the consequences .
however , the UNK did not say where they wanted to strike , so it is difﬁcult to predict the consequences .
the GDL have not said , however , where they will strike , making it difﬁcult to predict exactly what the consequences will be .
die Premierminister Indiens und Japans trafen sich in Tokio .
the Prime Minister of India and Japan met in Tokyo
the Prime Ministers of India and Japan met in Tokyo
India and Japan prime ministers meet in Tokyo
wo die Beamten es aus den Augen verloren .
where the ofﬁcials lost it out of the eyes
where ofﬁcials lose sight of it
causing the ofﬁcers to lose sight of it

Table 2: Translation examples.

De->En

En->De

BLEU UNK BLEU UNK
794
20.99
4771
18.83
20.64
-
4095
21.40
22.10
-

17.04
16.47
17.15
19.53
20.22

1539
8140
-
2745
-

Phrase
NN
NN+Src
CHAR
CHAR+Src

Table 3: De-En BLEU results and number of un-
known words.

words is not proportional to the improvement in
BLEU, but it is relevant in both directions. Note
the number of out-of-vocabulary words of the test
set is shown in Table 1. In the cases with post-
processing of unknown words, it is not applica-
ble to compute the number of unknowns since
they have all been replaced with the correspond-
ing aligned source word.

The character-based embedding seems to have
an impact in learning a better translation model
at various levels, which may include better align-
ment, reordering, morphological generation and
disambiguation. Table 2 shows some examples of
the kind of improvements that the character-based
neural MT system is capable of achieving com-
pared to baseline systems. Examples 1 and 2 show
how the reduction of source unknowns improves
the adequacy of the translation. Examples 3 and 4
show how the character-based approach is able to
handle morphological variations. Finally, example
5 shows an appropriate semantic disambiguation.

5 Conclusions
Neural MT offers a new perspective in the way
MT is managed. Its main advantages when com-

statistical
pared with previous approaches, e.g.
phrase-based, are that the translation is faced with
trainable features and optimized in an end-to-end
scheme. However, there still remain many chal-
lenges left to solve, such as dealing with the limi-
tation in vocabulary size.

In this paper we have proposed a modiﬁcation to
the standard encoder/decoder neural MT architec-
ture to use unlimited-vocabulary character-based
source word embeddings. This modiﬁcation al-
lows for a dramatic reduction in the number of
unknown words and a high improvement in trans-
lation quality. The reduction of unknown words
reaches a 66% relative in the German-to-English
direction and the improvement in BLEU is about
1.5 points in German-to-English and more than 3
points in English-to-German.

As further work, we are currently studying dif-
ferent alternatives to extend the character-based
approach to the target side of the neural MT sys-
tem.

Acknowledgements

This work is supported by the 7th Framework Pro-
gram of the European Commission through the In-
ternational Outgoing Fellowship Marie Curie Ac-
tion (IMTraP-2011-29951) and also by the Span-
ish Ministerio de Econom´ıa y Competitividad,
contract TEC2015-69266-P.

References
[Bahdanau et al.2015] Dimitry Bahdanau, Kyunghyun
Cho, and Yoshua Bengio. 2015. Neural machine
translation by jointly learning to align and translate.
CoRR, abs/1409.0473.

[Koehn et al.2007] Philipp Koehn, Hieu Hoang,
Alexandra Birch, Chris Callison-Burch, Marcello
Federico, Nicolas Bertoldi, Brooke Cowan, Wade
Shen, Christine Moran, Richard Zens, Chris Dyer,
Ondrej Bojar, Alexandra Constantin, and Evan
Herbst.
2007. Moses: Open Source Toolkit
In Proc. of
for Statistical Machine Translation.
the 45th Annual Meeting of the Association for
Computational Linguistics, pages 177–180.

[Ling et al.2015] Wang Ling, Chris Dyer, Alan W
Black, Isabel Trancoso, Ramon Fermandez, Silvio
Amir, Luis Marujo, and Tiago Luis. 2015. Finding
function in form: Compositional character models
In Pro-
for open vocabulary word representation.
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1520–
1530, Lisbon, Portugal, September. Association for
Computational Linguistics.

[Santos and aes2015] Cicero D. Santos

and Vic-
2015. Boosting named entity
tor Guimar aes.
recognition with neural character embeddings.
In
Proceedings of the Fifth Named Entity Workshop,
pages 25–33, Beijing, China, July. Association for
Computational Linguistics.

[Santos and Zadrozny2014] Cicero D. Santos

and
Bianca Zadrozny. 2014. Learning character-level
representations for part-of-speech tagging. In Tony
Jebara and Eric P. Xing, editors, Proceedings of the
31st International Conference on Machine Learning
(ICML-14), pages 1818–1826.

[Sennrich et al.2015] Rico Sennrich, Barry Haddow,
and Alexandra Birch. 2015. Neural machine trans-
lation of rare words with subword units. CoRR,
abs/1508.07909.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals,
2014. Sequence to sequence
and Quoc V. Le.
learning with neural networks.
In Z. Ghahramani,
M. Welling, C. Cortes, N. D. Lawrence, and K. Q.
Weinberger, editors, Advances in Neural Informa-
tion Processing Systems 27, pages 3104–3112. Cur-
ran Associates, Inc.

[Ballesteros et al.2015] Miguel Ballesteros, Chris Dyer,
and Noah A. Smith. 2015.
Improved transition-
based parsing by modeling characters instead of
In Proceedings of the 2015
words with lstms.
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 349–359, Lisbon, Portugal,
September. Association for Computational Linguis-
tics.

[Botha and Blunsom2014] Jan A. Botha and Phil Blun-
som. 2014. Compositional Morphology for Word
Representations and Language Modelling. In Pro-
ceedings of the 31st International Conference on
Machine Learning (ICML), Beijing, China,
jun.
*Award for best application paper*.

[Chen et al.2015] Xinxiong Chen, Lei Xu, Zhiyuan Liu,
Joint
Maosong Sun, and Huan-Bo Luan. 2015.
learning of character and word embeddings.
In
Qiang Yang and Michael Wooldridge, editors, IJ-
CAI, pages 1236–1242. AAAI Press.

[Cho et al.2014] Kyunghyun Cho, Bart van van Mer-
rienboer, Dzmitry Bahdanau, and Yoshua Bengio.
2014. On the properties of neural machine trans-
lation: Encoder–decoder approaches. In Proc. of the
Eighth Workshop on Syntax, Semantics and Struc-
ture in Statistical Translation, Doha.

[Chrupala2014] Grzegorz Chrupala. 2014. Normaliz-
ing tweets with edit scripts and recurrent neural em-
beddings. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2014, June 22-27, 2014, Baltimore, MD,
USA, Volume 2: Short Papers, pages 680–686.

[Elliott et al.2015] Desmond Elliott, Stella Frank, and
2015. Multi-language image de-
CoRR,

Eva Hasler.
scription with neural sequence models.
abs/1510.04709.

Jean,

Orhan

[Jean et al.2015] Sebastien

Firat,
Kyunghun Cho, Roland Memisevic, and Yoshua
Bengio. 2015. Montreal neural machine translation
systems for wmt15. In Proc. of the 10th Workshop
on Statistical Machine Translation, Lisbon.

[Kalchbrenner and Blunsom2013] Nal Kalchbrenner
and Phil Blunsom. 2013. Recurrent continuous
In Proc. of the Conference
translation models.
on Empirical Methods
in Natural Language
Processing, Seattle.

[Kim et al.2016] Yoon Kim, Yacine Jernite, David Son-
tag, and Alexander M. Rush.
2016. Character-
In Proceedings of
aware neural language models.
the 30th AAAI Conference on Artiﬁcial Intelligence
(AAAI’16).

[Koehn et al.2003] Philipp Koehn, Franz Joseph Och,
and Daniel Marcu. 2003. Statistical Phrase-Based
Translation. In Proc. of the 41th Annual Meeting of
the Association for Computational Linguistics.

