Enhancing Freebase Question Answering Using Textual Evidence

Kun Xu1, Yansong Feng1, Siva Reddy2, Songfang Huang3, Dongyan Zhao1
1Institute of Computer Science & Technology, Peking University, Beijing, China

2School of Informatics, University of Edinburgh

3IBM China Research Lab, Beijing, China

6
1
0
2

 
r
a

M
3

 

 
 
]
L
C
.
s
c
[
 
 

1
v
7
5
9
0
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

Existing knowledge-based question answering
systems often rely on small annotated train-
ing data. While shallow methods like in-
formation extraction techniques are robust to
data scarcity,
they are less expressive than
deep understanding methods,
thereby fail-
ing at answering questions involving multiple
constraints. Here we alleviate this problem
by empowering a relation extraction method
with additional evidence from Wikipedia. We
ﬁrst present a novel neural network based rela-
tion extractor to retrieve the candidate answers
from Freebase, and then develop a reﬁnement
model to validate answers using Wikipedia.
We achieve 53.3 F1 on WEBQUESTIONS, a
substantial improvement over the state-of-the-
art.

Introduction

1
As very large structured knowledge bases (KBs)
have become available, e.g., Freebase (Bollacker et
al., 2008), YAGO (Suchanek et al., 2007) and DB-
pedia (Auer et al., 2007), KBs have become a new
source of potential answers for people to mine. An-
swering factoid questions over such structured KBs,
known as KB-based question answering (or KB-
QA), is attracting increasing research efforts from
both information retrieval and natural language pro-
cessing communities.

The state-of-the-art methods for this task can be
categorized into two streams. The ﬁrst is based on
semantic parsing (Berant et al., 2013; Kwiatkowski
et al., 2013), which typically learns a grammar
that can parse natural language to a sophisticated

meaning representation language. But such sophis-
tication requires a lot of annotated training exam-
ples that contains compositional structures which is
practically impossible for large KBs such as Free-
base. Furthermore, mismatches between grammar
predicted structures and KB structure is also a com-
mon problem (Kwiatkowski et al., 2013; Berant and
Liang, 2014; Reddy et al., 2014).

On the other hand,

instead of building a for-
mal meaning representation, information extraction
methods retrieve a set of candidate answers from KB
using relation extraction (Yao and Van Durme, 2014;
Yih et al., 2014; Yao, 2015; Bast and Haussmann,
2015) or distributed representations (Bordes et al.,
2014; Dong et al., 2015). Designing large training
datasets for these methods is relatively easy (Yao
and Van Durme, 2014; Bordes et al., 2015). These
methods are often good at producing an answer ir-
respective of their correctness. However, handling
compositional questions that involve multiple enti-
ties and relations, still remains a challenge. Consider
the question what mountain is the highest in north
america. Relation extraction methods typically an-
swer with all the mountains in North America be-
cause of the lack of sophisticated representation for
the mathematical function highest. To select the cor-
rect answer, one has to retrieve all the heights of the
mountains, and sort them in descending order, and
then pick the ﬁrst entry. We propose a method based
on text based evidence which help solve this prob-
lem without having to do this.

Knowledge bases like Freebase capture real world
facts which people are interested in, and web like
Wikipedia provides a large repository of sentences

that validate or support these facts. For example,
a sentence in Wikipedia says, Denali (also known
as Mount McKinley, its former ofﬁcial name) is the
highest mountain peak in North America, with a
summit elevation of 20,310 feet (6,190 m) above sea
level. To answer our example question on a KB us-
ing a relation extractor, we can use this sentence as
external evidence, and ﬁlter out wrong answers and
pick the correct one.

Using external evidence not only mitigates rep-
resentational issues in relation extraction,
it also
alleviates data scarcity problem to some extent.
Consider the question, Who was queen isabella’s
mother. Answering this question involves predicting
two constraints hidden in the word mother. One con-
straint is the answer should be the parent of Isabella,
and the other is the answer’s gender is female. Such
words with multiple latent constraints have been a
pain-in-the-neck for both semantic parsing and rela-
tion extraction approaches, and requires larger train-
ing data (this phenomenon is coined as sub-lexical
compositionally by Wang et al. (2015)). Most sys-
tems are good at triggering the parent constraint,
but fail on the other. The textual evidence from
Wikipedia, . . . her mother was Isabella of Barcelos
. . . , can act as a further constraint to answer the
question.

We present a novel method for answering factoid
questions, which infers both on structured and un-
structured resources. Figure 1 gives an overview.
First we employ a relation extractor to ﬁnd an-
swer entities from a KB (indicated by KB-QA box).
Then a reﬁnement model uses textual evidence to
select answer entities (indicated by Answer Reﬁne-
ment box). The main contributions of this paper are
two-fold.
• We treat the KB-QA problem as a joint task of
relation extraction and entity linking, and further
propose an Multi Channel Convolutional Neural
Network (MCCNN) (Section 4.2) to learn a robust
relation representation. By performing a joint in-
ference approach (Section 4.3), our KB-based ap-
proach obtains competitive results on WEBQUES-
TIONS.

• We introduce a reﬁnement model to further vali-
date the answers (Section 5). Experimental results
show that our system outperforms the state-of-the-

Figure 1: The framework of our QA system.

art systems on WEBQUESTIONS.

2 Framework
Consider the question “who did shaq ﬁrst play for”,
we ﬁrst identify the topic entity in the question, and
then employ an MCCNN model to recognize the KB
relation between the topic entity and the answer. For
both entity linking and relation extraction compo-
nents, we keep up to 5 top ranked entities and rela-
tions, and develop a joint inference model to ﬁnd the
best entity-relation conﬁguration, and accordingly
retrieve candidate answers from Freebase. Finally,
we reﬁne these candidate answers by applying a re-
ﬁnement model which takes the Wikipedia page of
the topic entity into consideration.

3 Question Decomposition
A question often involves multiple constraints to its
answers, e.g., who plays anakin skywalker in star
wars 1. All actors as the answers of this ques-
tion should satisfy the following two constraints: (1)

who did shaq first play forKB-QAEntity LinkingRelation ExtractionJoint Inferenceshaq: m.012xdfshaq: m.05n7bpshall: m.06_ttvhsports.pro_athlete.teams..sports.sports_team_roster.teambasketball.player.statistics..basketball.player_stats.team……Answer Reﬁnementm.012xdf  sports.pro_athlete.teams..sports.sports_team_roster.teamLos Angeles Lakers,Boston Celtics,Orlando Magic,Miami HeatFreebaseShaquille O'NealO'Neal signedas a free agent with the Los Angeles LakersShaquille O'NealO'Neal played for the Boston Celtics in the 2010-11 season before retiringShaquille O'NealO'Neal was drafted in the 1992 NBA draftby the Orlando Magic with the first overall pickLos Angeles LakersBoston CelticsOrlando MagicO’Neal was drafted by the OrlandoMagic with the first overall pick in the 1992 NBA draftO’Neal played for the Boston Celtics in the 2010-11 season before retiringO’Neal signed as a free agent with the Los Angeles LakersReﬁnement Model+--Orlando MagicWikipedia Dump(with NLP results tagged)the actor played anakin skywalker; and (2) the ac-
tor played in star wars 1. Inspired by (Bao et al.,
2014), we design a dependency tree-based method
to handle such multiple-constraint questions. In gen-
eral, we ﬁrst decompose the original question into a
set of sub-questions using syntactic-based patterns.
Then the ﬁnal answers of the original question can
be obtained by intersecting the answers of all sub-
questions. For the example question, its two sub-
questions are who plays anakin skywalker and who
plays in star wars 1.

4 KB-based Question Answering
Given a sub-question, we assume the question word1
that represents the answer has a distinct KB relation
with the entity found in the question, and predict
a single KB triple for each sub-question. The QA
problem is thus formulated as an information extrac-
tion problem that involves two sub-tasks, i.e., entity
linking and relation extraction, which are challeng-
ing enough by themselves, and whose results are
even expected to be compatible to each other.

4.1 Entity Linking
For each question, we use hand-built sequences
of part-of-speech categories to identify all possible
named entity mention spans, e.g., the sequence NN
(shaq) may indicate an entity. For each mention
span, we employ an entity linking tool S-MART
(Yang and Chang, 2015) to retrieve the top 5 enti-
ties from Freebase. These entities are treated as can-
didate entities that will be eventually disambiguated
in the joint inference step.

4.2 Relation Extraction
We now proceed to identify the relation between
the answer and the entity in the question. Re-
cently, a large number of methods have been pro-
posed to learn the mapping from relational phrases
to KB relations, such as Naive Bayes based (Yao and
Van Durme, 2014), logistic regression based (Berant
et al., 2013), and neural network based (Yih et al.,
2015). These methods usually take the whole ques-
tion to predict the relation, which may be effected
by noisy information that are unrelated to the rela-
tion, especially for those short questions. Syntactic

Figure 2: Overview of the multi channel convolutional neural
network for relation extraction.

features such as the shortest dependency path have
been proven to be more concise and representative
in relation extraction (Liu et al., 2015; Xu et al.,
2015). Therefore, we develop a multi-channel con-
volutional neural networks (MCCNNs) to learn the
relation representations from both the syntactic level
and sentence level.

4.2.1 Syntactic Level Features

We use the shortest path between an entity men-
tion and the question word in the dependency tree2
as input to the ﬁrst channel. We treat the path as a
concatenation of vectors of words, dependency edge
directions, and dependency labels. Note that, the
entity mention and the question word are excluded
from the dependency path so as to learn a more gen-
eral relation representation in syntactic level. As
shown in Figure 2, the dependency path between
shaq and who is ← nsubj – play – dobj →.
4.2.2 Sentence Level Features

The other channel takes the context of the entities
in the question as input, i.e., the words of the ques-
tion after removing the entity mention and the ques-

1who, when, what, where, how, which, why, whom, whose.

2We use Stanford CoreNLP dependency parser.

[Who]did[shaq]firstplayforplay did first play for  Word Representation Feature Extraction max(  ).ConvolutionFeature VectorOutputSoftmaxdobjnsubjdobjauxnsubj(cid:81)(cid:80)(cid:82)(cid:71)KB relationstion word. As illustrated in Figure 2, did ﬁrst play
for is fed into the other channel to learn the relation
representation at a sentence level.

4.2.3 MCCNNs for Relation Classiﬁcation

For each channel, the network structure illustrated
in Figure 2 is used to tackle the input of various
length, which returns a ﬁxed length vector represen-
tation. Finally the two feature vectors are concate-
nated and then fed into a softmax classiﬁer, the
output dimension of which is equal to the number
of predeﬁned relation types. The value of each di-
mension indicates the conﬁdence score of the corre-
sponding relation. In the syntactic feature represen-
tations, the vectors of the dependency edge direc-
tion and dependency label are randomly initialized
and optimized through back-propagation. The word
embeddings are shared across two channels and also
updated in the training process.

4.2.4 Objective Function and Learning

The model is learned using pairs of question and
its corresponding simulated gold relation from the
training data. Given an input question x with an an-
notated entity mention, the network outputs a vector
o(x), where the entry ok(x) is the probability that
there exists the k-th relation between the entity and
the expected answer. We denote t(x) ∈ RK×1 as
the target distribution vector, in which the value for
gold relation is set 1, others are set 0. We compute
the cross entropy error between t(x) and o(x), and
further deﬁne the objective function over all training
data:

J(θ) = −(cid:88)

K(cid:88)

tk(x) log ok(x) + λ||θ||2

x

k=1

where θ is the set of model parameters to be learned,
and λ is a vector of regularization parameters. The
model parameters θ can be efﬁciently computed via
back-propagation through network structures. To
minimize J(θ), we apply stochastic gradient descent
(SGD) with AdaGrad (Duchi et al., 2011) in our ex-
periments.

Joint Inference

4.3
A pipeline of entity linking and relation extraction
may suffer from error propagations. As we know,

entities and relations have strong selectional prefer-
ences that certain entities do not appear with certain
relations and vice versa. Locally optimized models
are not able to exploit these implicit bi-directional
preferences. We thus exploit a ranking based joint
inference model to ﬁnd a globally optimal entity-
relation assignment from local predictions. The key
idea behind is to leverage various clues from the two
local models and the KB available to rank a correct
entity-relation assignment higher than those prob-
lematic ones.

4.3.1 Features

For a given entity-relation assignment (e, r), we

exploit from the KB the following clues:

Entity Clues The score of predicted entity e re-
turned by the entity linking system is directly used
as a feature. In addition, the mention’s text overlap
with the Freebase name for entity e is also included
as a feature. In Freebase, most entities have a re-
lation fb:description that describes the basic
introduction of this entity. This description serves
as an important clue to distinguish the entity, since
in some sense, the description could be treated as a
bag of relational phrases related to this entity. For
instance, in the running example, shaq is linked
to three potential entities m.06 ttvh (an American
reality television show), m.05n7bp (a multiplayer
video game) and m.012xdf (a famous NBA basket-
ball player). Interestingly, the relational phrase play
only appears in the description of m.012xdf and oc-
curs three times. Therefore, we count how many
times the relational phrase appearing in e’s descrip-
tion, and include it as a feature.

Relation Clues The score of relation r returned by
the MCCNNs is used as a feature. Furthermore, we
view each relation as a document which consists of
the training questions that this relation is expressed
in. For a given question, we use the sum of the tf-idf
scores of its words with respect to the relation r as a
feature.

In a natural

Answer Clues
language question,
question word itself often indicates the answer type.
For example, the question word when usually in-
dicates that the answer entity has a Freebase type
type.datetime. We thus use the co-occurrences of

the question word and the answer types as a feature.
Secondly, a Freebase relation r is a concatenation of
a series of fragments r = r1.r2.r3. For instance, the
three fragments of people.person.parents are peo-
ple, person and parents. The ﬁrst two fragments
indicate the Freebase type of the subject of this rela-
tion, and the third fragment indicates the object type,
in our case the answer type. We use an indicator fea-
ture to denote if the surface form of third fragment r
appears in the question.
4.3.2 Learning

To train a ranking based joint inference model
needs entity-relation assignments (e, r). We take all
our entity/relation predictions, and create a ranked
list as follows. Suppose that (egold, rgold) is the gold
entity/relation pair for question q. Let (ea, ra) be a
candidate entity-relation assignment for q. Speciﬁ-
cally, if both ea and ra are correct (i.e., ea = egold,
ra = rgold), we assign it with the highest ranking
score 3.
In contrast, if only the entity or relation
equals to the gold one (i.e., ea = egold, ra (cid:54)= rgold
or ea (cid:54)= egold, ra = rgold), we assign it with score
2. When both entity and relation assignments are
wrong, we assign it with score 1. The intuition be-
hind is that even if an assignment is not completely
correct, it is still preferred than some other totally in-
correct assignments. This gives the training data for
our svm-ranker (Joachims, 2006), which we hope
the gold-standard entity-relation assignments rank
higher than incorrect ones.

5 Reﬁning Answers Using Wikipedia
After the joint inference, we are able to generate a
KB query for the question, thus retrieve a collection
of candidate answers from the KB. To further reﬁne
the answers, we use Wikipedia as the unstruc-
tured knowledge resource, where most statements
are validated by multiple people.

Our reﬁnement model is inspired by the intuition
of how people reﬁne their answers. If you ask some-
one: who did shaq ﬁrst play for, and give them
four candidate answers (Los Angeles Lakers, Boston
Celtics, Orlando Magic and Miami Heat), as well
as access to Wikipedia, that person might ﬁrst de-
termine that the question is about Shaquille O’Neal,
then go to O’Neal’s Wikipedia page, and search for
the sentences that contain the candidate answers as

evidences. By analyzing these sentences, one can
ﬁgure out whether a candidate answer is correct or
not.

5.1 Finding Evidence from Wikipedia
As mentioned above, we should ﬁrst link topic
entity selected by joint inference model and an-
swers retrieved from Freebase into the Wikipedia3.
In the topic entity’s Wikipedia page, we utilize
wikifier (Cheng and Roth, 2013) to recognize
the Wikipedia entities in the sentences. Then, we
collect the sentences that mention the candidate an-
swers as the evidences, which are the input of the
reﬁnement model. For example, in the Wikipedia
page of O’Neal, we can ﬁnd a sentence “O’Neal was
drafted by the Orlando Magic with the ﬁrst over-
all pick in the 1992 NBA draft”, which is treated
as an evidence that supports the answer Orlando
Magic. This sentence is then taken into account by
the reﬁnement model to discriminate whether Or-
lando Magic is the answer of the question.

5.2 Lexical Features
We treat the reﬁnement process as a binary clas-
siﬁcation task, i.e., correct (positive) and incorrect
(negative). In particular, we rely on the lexical fea-
tures extracted from the question and the evidence.
Formally, given a question q = <q1, .., qn> and an
evidence sentence s = <s1, .., sm>, we denote the
token of q and s by qi and sj, respectively. For
each pair (q, s), we identify a set of all possible
token pairs (qi, sj), the occurrences of which are
used as features. In this way, we hope to to learn a
higher weight for features like: (first, drafted)
and lower weight for (first, played).

5.3 Learning
We prepare the training data for the reﬁnement
model as follows. On the training dataset, we ﬁrst
use our KB-based approach to retrieve the answers
from the KB. Then we use the gold answers of these
questions to supervise the training of the reﬁnement
model. Speciﬁcally, we treat the sentences that con-
tain correct/incorrect answers as positive/negative
examples for the reﬁnement model. We then employ

3We apply the Freebase API to retrieve the corresponding

Wikipedia id of a Freebase entity.

LIBSVM (Chang and Lin, 2011) to learn the weights
for all the token pairs.

Note that, in the Wikipedia page of the topic en-
tity, we may collect more than one sentence that con-
tain a candidate answer. However, some of these
sentences may be irrelevant to the question, we thus
consider this candidate answer as correct if at least
one evidence can be predicted positive by our reﬁne-
ment model. On the other hand, sometimes, we may
not ﬁnd any evidence for the candidate answer. In
these cases, we fall back to the results of the KB-
based approach.

6 Experiments

In this section we introduce the experimental setup,
the main results and detailed analysis of our system.

6.1 Data
We use the WEBQUESTIONS dataset, which con-
tains 5,810 questions crawled via Google Suggest
service, with answers annotated on Amazon Me-
chanical Turk. The questions are split into training
and test sets, which contain 3,778 questions (65%)
and 2,032 questions (35%), respectively. We further
split the training questions into the training set and
the development set by 80%/20%.

Notice that, to train the MCCNNs and the joint
inference model, we need the gold standard rela-
tions of the questions. Since this dataset only con-
tains question-answer pairs and annotated topic enti-
ties, we retrieve the simulated gold relations in three
steps: for a given question, we ﬁrst locate the topic
entity e in the Freebase graph, then view the 1-hop
and 2-hop predicates connected to the topic entity as
the gold relation candidates. For each relation can-
didate r, we issue the query (e, r, ?) to the KB, and
label the one that has the answers which have the
highest F 1 value compared with the annotated an-
swers, as the simulated gold relation.

6.2 Parameter Settings
The dimension of the word embedding is set to
50. We initialized the word embeddings with the
word vectors trained with the model of Turian et
al. (2010). The hyper parameters in our model are
tuned using the development set. The window size
of MCCNNs is 3. The sizes of hidden layer 1 and

Prec. Rec.
41.3
48.0

-

-
-
-
-

40.5

Method
(Berant et al., 2013)
(Yao and Van Durme, 2014)
(Berant and Liang, 2014)
(Bao et al., 2014)
(Bordes et al., 2014)
(Dong et al., 2015)
(Yao, 2015)
(Yih et al., 2015)
(Yih et al., 2015)(w/o ClueWeb)
DeepQA
DeepJQA
DeepRQA
DeepJRQA

52.8
50.7
44.8
48.0
50.2
53.1
Table 1: Results on the test set.

46.6

-

-
-
-
-

60.7
60.6
53.7
56.9
53.2
65.0

F 1
35.7
33.0
39.9
37.5
39.2
40.8
44.3
52.5
50.8
44.1
47.1
47.0
53.3

Entity Linking Relation Extraction

Isolated Model
Joint Inference

79.8%
83.2%

45.9%
55.3%

Table 2: Performances of entity linking and relation extraction
on the development set: how possible the retrieved entities or
relations contain the gold standard topic entity or simulated gold
relation.

hidden layer 2 of two channels in MCCNNs are set
to 200 and 100, respectively.

6.3 Results and Discussion
We use Berant’s ofﬁcial evaluation script to com-
pute the average question-wise F 1, and compare our
model DeepJRQA, which performs both the Joint
inference and answer Reﬁnement, to existing mod-
els in the literature. Note that, DeepJRQA treats the
executed results of 2 top ranked KB triples generated
by the joint inference model as candidate answers.

Moreover, we use three baselines. The ﬁrst base-
line is DeepQA, which ﬁnds the answers by taking
the best predictions from a pipeline of EL and RE
to query the KB. The second baseline is DeepJQA,
which takes the joint inference approach but without
answer reﬁnement. The third baseline is DeepRQA,
which performs the answer reﬁnement but without
joint inference. Table 1 summarizes the results.

We can see that when adding the joint inference
and answer reﬁning step, i.e., DeepJRQA, our per-
formance is improved by 9.2% over DeepQA, out-
performing all systems including the current state-
of-the-art (Yih et al., 2015). We also report Yih et
al. removing its external resource ClueWeb.

Method
DeepQA (syn.)
DeepQA (sen.)
DeepQA (syn. + sen.)
DeepJQA (syn.)
DeepJQA (sen.)
DeepJQA (syn. + sen.)

Prec. Rec.
49.1
36.4
50.2
37.8
50.3
38.9
41.5
55.0
56.6
42.5
44.0
56.3

F 1
38.1
38.7
40.1
43.6
44.1
45.8

Table 3: Results on the development set.

6.3.1 Detailed analysis

Now let us explore the contributions of each com-

ponent of our system.

Joint inference From Table 1, we can see that
adding the joint inference step make a substantial
performance increase around 3%. The question of
interest is how much performance gain the EL and
RE components contribute during the joint inference
approach.

We ﬁrst evaluate the entity linking step with the
gold entity annotation on the development set. As
shown in Table 2, for 79.8% questions, our entity
linker can correctly ﬁnd the gold standard topic enti-
ties. We also evaluate DeepJQA to see if the joint in-
ference helps entity linking. We see 3.4% improve-
ment on the best prediction revealing that the joint
inference is indeed beneﬁcial. Next we use the sim-
ulated gold relations to evaluate the performance of
RE component on the development set. As shown in
Table 2, the relation prediction accuracy increases
by 9.4% when using the joint inference approach.

Feature variations Table 3 shows the results of
feature ablation studies.
In particular, we present
the results using single-channel network , i.e., tun-
ing the parameters of one channel while switching
off the other. As seen, the sentence level features are
found to be more important than syntactic features.
This could be due to short and noisy nature of WE-
BQUESTIONS questions which causes the shortest
dependency path inaccessible to sufﬁcient informa-
tion to predict a relation. By mixing the syntax and
context channels, the joint inference model boosts
its performance by 5.7% from 40.1% to 45.8%.

Answer reﬁnement As shown in Table 1, when
augmented with the answer
reﬁnement model,
DeepRQA and DeepJRQA obtain signiﬁcant perfor-
mance improvements, 2.9% and 6.2%, respectively.

Question & Answers
1. what is the largest nation in europe
DeepJQA: Kazakhstan, Turkey, Russia, ...
DeepJRQA: Russia
2. which country in europe has the largest land area
DeepJQA: Georgia, France, Russia, ...
DeepJRQA: Russian Empire, Russia
3. what year did ray allen join the nba
DeepJQA: 2007, 2003, 1996, 1993, 2012
DeepJRQA: 1996
4. who is emma stone father
DeepJQA: Jeff Stone, Krista Stone
DeepJRQA: Jeff Stone
5. where did john steinbeck go to college
DeepJQA: Salinas High School, Stanford University
DeepJRQA: Stanford University

Table 4: Example questions of the test set and corresponding
answers of our systems. The blue colored answers are cor-
rect while red colored ones are wrong. The upper 3 questions
involve the aggregation operations and the lower 2 questions
query for ﬁne-grained relations.

On the test set, we ﬁnd 192 examples get a higher
F1 value of the answers after taking the reﬁnement
method. Table 4 lists some questions and the corre-
sponding answers of our system.

We manually analyze these examples and ﬁnd
out that, as we expected, the reﬁnement model im-
proves the KB-based approach mainly on 2 types
of questions: (i) the questions that involve aggre-
gation operations. Examples are Question 1-3 in
Table 4. For Question 3, DeepJQA takes the Free-
base relation fb:teams..from as the prediction
of the joint inference approach. This relation de-
scribes the ﬁrst years that ray allen joined the bas-
ketball teams including in the college and NBA. To
answer such a question, the min(·) operation should
be performed on these years. Note that, Ray Allen
joined a college basketball team (University of Con-
necticut) in the 1993 rather than a NBA team. In-
terestingly, the reﬁnement model can not only cor-
rectly choose the earliest year but also distinguish
his basketball career period. (ii) the questions that
query for ﬁne-grained relations such as Question 4-
5 in Table 4. Take Question 5 as an example, the
user queries for the college that John Steinbeck at-
tended. However, Freebase only uses a single re-
lation fb:education..institution to de-
scribe the person’s general education information,

without discriminating the speciﬁc periods such as
high school or college.

These empirical ﬁndings demonstrate again that
it is appropriate to exploit the Wikipedia for answer
reﬁnement.

Error analysis We analyze the errors our model
makes on the development set. Around 15% of
the errors are caused by the incorrect entity link-
ing. Around 50% of the errors are due to incor-
rect relation predictions. We analyze these relation
prediction errors and ﬁnd out two reasons: (i) in
some short-length questions, both the dependency
path and the sentence are too short to provide sufﬁ-
cient information to the relation classiﬁer; (ii) the in-
sufﬁciency and unbalanced distribution of the train-
ing data for the relation classiﬁer (3022 training ex-
amples for 461 relations) heavily inﬂuences the per-
formance of relation classiﬁcation.

The remaining errors are due to the failure of our
reﬁnement model for two reasons. First, for some
questions, we cannot ﬁnd evidences with respect to
the candidate answers, and our system fall back to
the KB-based approach, which may obtain a collec-
tion of answers in low accuracy. Second, recall that
we only use the lexical features to reﬁne the answers,
the feature sparse problem inevitably hurts the per-
formance of our reﬁnement model .

7 Related Work

Over time, the QA task has evolved into two main
streams – QA on unstructured data, and QA on
structured data. TREC QA evaluations (Voorhees
and Tice, 1999) were a major boost to unstruc-
tured QA which lead to richer datasets and sophis-
ticated methods (Wang et al., 2007; Heilman and
Smith, 2010; Yao et al., 2013; Yih et al., 2013;
Yu et al., 2014; Yang et al., 2015; Hermann et
al., 2015). While initial progress on structured QA
started with small toy domains like GeoQuery (Zelle
and Mooney, 1996), recent focus has shifted to large
scale structured KBs like Freebase, DBPedia (Unger
et al., 2012; Cai and Yates, 2013; Berant et al., 2013;
Kwiatkowski et al., 2013), and on noisy KBs (Banko
et al., 2007; Carlson et al., 2010; Krishnamurthy
and Mitchell, 2012; Fader et al., 2013; Parikh et al.,
2015). An exciting development in structured QA
is to exploit multiple KBs (with different schemas)

at the same time to answer questions jointly (Yahya
et al., 2012; Fader et al., 2014; Zhang et al., 2016).
QALD tasks and linked data initiatives are contribut-
ing to this trend.

Our model combines the best of both worlds
by inferring over structured and unstructured data.
Though earlier methods exploited unstructured data
for KB-QA (Krishnamurthy and Mitchell, 2012; Be-
rant et al., 2013; Yao and Van Durme, 2014; Reddy
et al., 2014; Yih et al., 2015), these methods do not
rely on unstructured data at test time. Our work is
closely related to Joshi et al. (2014) who aim to an-
swer noisy telegraphic queries using both structured
and unstructured data. Their work is limited in an-
swer single relation queries.

Our work also intersects with relation extraction
methods. While these methods aim to predict a re-
lation between two entities in order to populate KBs
(Mintz et al., 2009; Hoffmann et al., 2011; Riedel et
al., 2013), we work with sentence level relation ex-
traction for question answering. Krishnamurthy and
Mitchell (2012) and Fader et al. (2014) adopt open
relation extraction methods for QA but they require
hand-coded grammar for parsing queries. Closest to
our extraction method is (Yao and Van Durme, 2014;
Yao, 2015) who also uses sentence level relation ex-
traction for QA. Unlike them, we can predict mul-
tiple relations per question, and our MCCNN archi-
tecture is more robust to unseen contexts compared
to their logistic regression models.

8 Conclusion and Future Work

In this paper, we presented a novel method that ex-
ploits both Freebase and Wikipedia to answer natu-
ral language questions. We introduced a joint en-
tity linking and relation extraction method to re-
trieve answers from Freebase, and then reﬁne these
answers using additional evidence from Wikipedia.
Our method outperforms the state-of-the-art on the
WEBQUESTIONS dataset.

Although the reﬁnement model helps, our perfor-
mance is limited by the coverage of Freebase. More-
over, in the future, to answer subjective questions
such as what is the most popular attraction in Amer-
ica which cannot be answered using Freebase only,
we should consult the unstructured knowledge.

References
[Auer et al.2007] Sren Auer, Christian Bizer, Georgi
Kobilarov, Jens Lehmann, Richard Cyganiak, and
Zachary G. Ives. 2007. Dbpedia: A nucleus for a
web of open data. In ISWC/ASWC.

[Banko et al.2007] Michele Banko, Michael J Cafarella,
Stephen Soderland, Matthew Broadhead, and Oren Et-
zioni. 2007. Open information extraction for the web.
In IJCAI.

[Bao et al.2014] Junwei Bao, Nan Duan, Ming Zhou, and
Tiejun Zhao. 2014. Knowledge-based question an-
swering as machine translation. In ACL.
[Bast and Haussmann2015] Hannah Bast

and Elmar
Haussmann. 2015. More accurate question answering
on freebase. In CIKM.

[Berant and Liang2014] Jonathan Berant

and Percy
Liang. 2014. Semantic parsing via paraphrasing. In
ACL.

[Berant et al.2013] Jonathan Berant, Andrew Chou, Roy
Frostig, and Percy Liang. 2013. Semantic parsing on
freebase from question-answer pairs. In EMNLP.

[Bollacker et al.2008] Kurt D. Bollacker, Colin Evans,
Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008.
Freebase: a collaboratively created graph database for
structuring human knowledge. In SIGMOD.

[Bordes et al.2014] Antoine Bordes, Sumit Chopra, and
Jason Weston. 2014. Question answering with sub-
graph embeddings. In EMNLP.

[Bordes et al.2015] Antoine Bordes, Nicolas Usunier,
Sumit Chopra, and Jason Weston. 2015. Large-scale
simple question answering with memory networks.
CoRR, abs/1506.02075.

[Cai and Yates2013] Qingqing Cai and Alexander Yates.
Large-scale semantic parsing via schema

2013.
matching and lexicon extension. In ACL.

[Carlson et al.2010] Andrew Carlson, Justin Betteridge,
Bryan Kisiel, Burr Settles, Estevam R Hruschka Jr,
and Tom M Mitchell. 2010. Toward an architecture
for never-ending language learning. In AAAI.

[Chang and Lin2011] Chih-Chung Chang and Chih-Jen
Lin. 2011. LIBSVM: A library for support vector ma-
chines. ACM TIST, 2(3):27.

[Cheng and Roth2013] Xiao Cheng and Dan Roth. 2013.

Relational inference for wikiﬁcation. In ACL.

[Dong et al.2015] Li Dong, Furu Wei, Ming Zhou, and
Ke Xu. 2015. Question answering over freebase with
multi-column convolutional neural networks. In ACL-
IJCNLP.

[Duchi et al.2011] John C. Duchi, Elad Hazan, and Yoram
Singer. 2011. Adaptive subgradient methods for on-
line learning and stochastic optimization. Journal of
Machine Learning Research, 12:2121–2159.

[Fader et al.2013] Anthony Fader, Luke S. Zettlemoyer,
and Oren Etzioni. 2013. Paraphrase-driven learning
for open question answering. In ACL.

[Fader et al.2014] Anthony Fader, Luke Zettlemoyer, and
Oren Etzioni. 2014. Open question answering over
curated and extracted knowledge bases. In SIGKDD.
[Heilman and Smith2010] Michael Heilman and Noah A
Smith. 2010. Tree edit models for recognizing textual
entailments, paraphrases, and answers to questions. In
NAACL.

[Hermann et al.2015] Karl Moritz Hermann, Tomas Ko-
cisky, Edward Grefenstette, Lasse Espeholt, Will Kay,
Mustafa Suleyman, and Phil Blunsom. 2015. Teach-
ing machines to read and comprehend. In Advances in
Neural Information Processing Systems.

Hoffmann,

[Hoffmann et al.2011] Raphael

Congle
Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S
Weld. 2011. Knowledge-based weak supervision for
information extraction of overlapping relations.
In
ACL.

[Joachims2006] Thorsten Joachims. 2006. Training lin-

ear svms in linear time. In SIGKDD.

[Joshi et al.2014] Mandar

Joshi, Uma Sawant,

and
Soumen Chakrabarti. 2014. Knowledge graph and
corpus driven segmentation and answer inference for
telegraphic entity-seeking queries. In EMNLP.

[Krishnamurthy and Mitchell2012] Jayant

Krishna-
murthy and Tom M Mitchell.
2012. Weakly
supervised training of semantic parsers. In EMNLP-
CoNLL.

[Kwiatkowski et al.2013] Tom Kwiatkowski,

Eunsol
Choi, Yoav Artzi, and Luke S. Zettlemoyer. 2013.
Scaling semantic parsers with on-the-ﬂy ontology
matching. In EMNLP.

[Liu et al.2015] Yang Liu, Furu Wei, Sujian Li, Heng
Ji, Ming Zhou, and Houfeng WANG.
2015. A
dependency-based neural network for relation classi-
ﬁcation. In ACL.

[Mintz et al.2009] Mike Mintz, Steven Bills, Rion Snow,
and Dan Jurafsky. 2009. Distant supervision for rela-
tion extraction without labeled data. In ACL.

[Parikh et al.2015] Ankur P. Parikh, Hoifung Poon, and
Kristina Toutanova. 2015. Grounded semantic pars-
ing for complex knowledge extraction. In NAACL.

[Reddy et al.2014] Siva Reddy, Mirella Lapata, and Mark
Steedman. 2014. Large-scale semantic parsing with-
out question-answer pairs. Transactions of the Associ-
ation of Computational Linguistics, pages 377–392.

[Riedel et al.2013] Sebastian Riedel, Limin Yao, Andrew
McCallum, and Benjamin M. Marlin. 2013. Rela-
tion extraction with matrix factorization and universal
schemas. In NAACL.

[Yu et al.2014] Lei Yu, Karl Moritz Hermann, Phil Blun-
2014. Deep learn-
arXiv preprint

som, and Stephen Pulman.
ing for answer sentence selection.
arXiv:1412.1632.

[Zelle and Mooney1996] John M Zelle and Raymond J
Mooney. 1996. Learning to parse database queries
using inductive logic programming. In AAAI.

[Zhang et al.2016] Yuanzhe Zhang, Shizhu He, Kang Liu,
and Jun Zhao. 2016. A joint model for question an-
swering over multiple knowledge bases. In AAAI.

[Suchanek et al.2007] Fabian M. Suchanek, Gjergji Kas-
neci, and Gerhard Weikum. 2007. Yago: a core of
semantic knowledge. In WWW.

[Unger et al.2012] Christina Unger, Lorenz B¨uhmann,
Jens Lehmann, Axel-Cyrille Ngonga Ngomo, Daniel
Gerber, and Philipp Cimiano. 2012. Template-based
question answering over rdf data. In WWW.

[Voorhees and Tice1999] Ellen M Voorhees and Dawn M.
Tice. 1999. The trec-8 question answering track re-
port. In TREC.

[Wang et al.2007] Mengqiu Wang, Noah A Smith, and
Teruko Mitamura. 2007. What is the jeopardy model?
In EMNLP-
a quasi-synchronous grammar for qa.
CoNLL.

[Wang et al.2015] Yushi Wang, Jonathan Berant, and
2015. Building a semantic parser

Percy Liang.
overnight. In ACL.

[Xu et al.2015] Kun Xu, Yansong Feng, Songfang Huang,
and Dongyan Zhao. 2015. Semantic relation classiﬁ-
cation via convolutional neural networks with simple
negative sampling. In EMNLP.

[Yahya et al.2012] Mohamed Yahya, Klaus Berberich,
Shady Elbassuoni, Maya Ramanath, Volker Tresp, and
Gerhard Weikum. 2012. Natural language questions
for the web of data. In EMNLP.

[Yang and Chang2015] Yi Yang and Ming-Wei Chang.
2015. S-mart: Novel tree-based structured learning
In ACL-
algorithms applied to tweet entity linking.
IJNLP.

[Yang et al.2015] Yi Yang, Wen-tau Yih, and Christopher
Meek. 2015. Wikiqa: A challenge dataset for open-
domain question answering. In EMNLP.

[Yao and Van Durme2014] Xuchen Yao and Benjamin
Van Durme. 2014. Information extraction over struc-
tured data: Question answering with freebase. In ACL.
[Yao et al.2013] Xuchen Yao, Benjamin Van Durme, and
Peter Clark. 2013. Answer extraction as sequence tag-
ging with tree edit distance. In NAACL.

[Yao2015] Xuchen Yao. 2015. Lean question answering

over freebase from scratch. In NAACL.

[Yih et al.2013] Wen-tau Yih, Ming-Wei Chang, Christo-
pher Meek, and Andrzej Pastusiak. 2013. Question
answering using enhanced lexical semantic models. In
ACL.

[Yih et al.2014] Wen-tau Yih, Xiaodong He, and Christo-
pher Meek. 2014. Semantic parsing for single-relation
question answering. In ACL.

[Yih et al.2015] Wen-tau Yih, Ming-Wei Chang, Xi-
aodong He, and Jianfeng Gao. 2015. Semantic pars-
ing via staged query graph generation: Question an-
swering with knowledge base. In ACL-IJCNLP.

