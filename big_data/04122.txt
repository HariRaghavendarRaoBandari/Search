6
1
0
2

 
r
a

 

M
4
1

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
2
2
1
4
0

.

3
0
6
1
:
v
i
X
r
a

Graphical Log-linear Models: Fundamental

Concepts and Applications

Niharika Gauraha

Indian Statistical Institute

March 15, 2016

Abstract: We present a comprehensive study of graphical log-linear mod-
els for contingency tables. High dimensional contingency tables arise in
many areas such as computational biology, collection of survey and census
data and others. Analysis of contingency tables involving several factors or
categorical variables is very hard. To determine interactions among various
factors, graphical and decomposable log-linear models are preferred. First,
we explore connections between the conditional independence in probability
and graphs; thereafter we provide a few illustrations to describe how graph-
ical log-linear model are useful to interpret the conditional independences
between factors. We also discuss the problem of estimation and model se-
lection in decomposable models.

Keywords: Graphical Log-linear Models, Contingency Tables, Decompos-
able Models, Hierarchical Log-linear Models

1

Introduction

In this paper, our aim is to provide the reader with insight into the graph-
ical log-linear models by providing a concise explanation of the underlying
mathematics and statistics, by pointing out relationships to conditional in-
dependence in probability and graphs and providing pointers to available
software and important references.

Log Linear Models(abbreviated as LLMs) are the most widely used mod-
els for analyzing cross-classiﬁed categorical data, see [9]. Though LLM sup-
ports various range of models based on non-interaction assumptions. But
for fairly large dimensional tables the analysis becomes diﬃcult, as the num-
ber of factors increases the number of interaction terms grows exponentially.
Graphical Log Linear Models(abbreviated as GLLMs) are a way of repre-
senting relationships among the factors of a contingency table using a graph.
GLLMs have two great advantages: from the graph structure it is easy to

1

read oﬀ the conditional independence relations and graph based algorithms
usually provide eﬃcient computational algorithms for parameter estimation
and model selection.

The decomposable log linear models are a restricted class of GLLMs
which are based on chordal graphs. There are several reasons for using
decomposable models over an ordinary GLLM. Firstly, the maximum like-
lihood estimates can be found explicitly. Secondly, closed form expressions
exists for test statistics. Another advantage is that it has triangulated graph
based, eﬃcient inference algorithms. Thus decomposable models are mostly
used for analysis of high dimensional tables.

We have organized the rest of the article in the following manner. In
section 2, we brieﬂy review graph theory, conditional independence in prob-
ability and Markov Networks. Section 3 gives overview of contingency tables
and describes diﬀerent types of contingency tables based on the underlying
sampling distributions. Section 4 introduces the theory of log-linear inter-
action models and deﬁnes various classes of LLMs such as comprehensive,
hierarchical, graphical and decomposable LLMs. Section 5 is concerned with
statistical properties of LLMs such as the suﬃcient statistics, the Maximum
Likelihood Estimates(MLE) and model testing. In section 6, we discuss the
analysis of three-factor contingency tables. In section 7, the backward model
selection algorithm for decomposable models is illustrated with an example.
Section 8 gives computational details. We shall provide some concluding
remarks in section 9.

2 Graph Theory and Markov Networks

In this section, we brieﬂy review the graph theoretic concepts, the condi-
tional independence in probability and Markov Networks.

2.1 Graph Theory

Here we list and deﬁne the necessary concepts of graph theory that we will
be using in later sections. See [49], for further details on graph theory.

A graph G, is a pair G = (V, E), where V is a set of vertices and E is a
set of edges. A graph is said to be an undirected graph when E is a set of
unordered pairs of vertices. We consider only simple graph that has neither
loops nor multiple edges.

Deﬁnition 1 (Boundary) Let G = (V, E) be an undirected graph. The
neighbours or boundary of a subset A of vertices is a subset C of vertices
such that all nodes in C are not in A but are adjacent to some vertex in A.

bd(A) = {u ∈ V \ A | ∃v ∈ A : {u, v} ∈ E}

2

Deﬁnition 2 (Maximal Clique) A clique of a graph G is a subset C of
vertices such that all vertices in C are mutually adjacent. A clique is said to
be maximal if no vertex can be added to C without violating clique property.

Deﬁnition 3 (Chordal(Triangulated) Graphs) In graph theory, a chord
of a cycle C is deﬁned as the edge which is not in the edge set of C but joins
two vertices from the vertex set C. A graph is said to be chordal graph if
every cycle of four or more length has a chord.

Deﬁnition 4 (Isomorphic Graphs) Two graphs are said to be isomor-
phic if they have same number of vertices, same number of edges and they
are connected in the same way.

2.2 Conditional Independence

The concept of conditional independence in probability theory is very im-
portant and it is the basis for the graphical models. It is deﬁned as follows.

Deﬁnition 5 (Conditional Independence) Let X, Y and Z be random
variables with a joint distribution P. The random variables X and Y are said
to be conditionally independent given the random variable Z if and only if
the following holds.

P (X, Y | Z) = P (X | Z)P (Y | Z)

P (X | Y, Z) = P (X | Z)

We sometimes also use David’s notation (see [15] for details), X ⊥⊥ Y | Z.

Conditional independence has a vast literature in the ﬁeld of probability and
statistics, we refer to [15] and [43] for further details.

2.3 Markov Networks and Markov Properties

In this section, we deﬁne Markov network graphs, Markov networks and
diﬀerent Markov properties for the Markov Networks.

Deﬁnition 6 (Markov Network Graphs) A Markov network graph is
an undirected graph G = ( V, E ), where V = {X1, X2, .., Xn} represents
random variables of a multivariate distribution.

Deﬁnition 7 (Markov Networks) A Markov network M, is a pair M =
(G, ψ). Where G is a Markov network graph and ψ = {ψ1, ψ2, ..., ψm} is a
set of non negative functions for each maximal clique Ci ∈ G ∀i = 1 . . . m
and the joint pdf can be decomposed into factors as

P (x) =

1

Z Ya∈Cm

ψa(x)

3

where Z is a normalizing constant.

Deﬁnition 8 ((P) Pairwise Markov Property) A probability distribu-
tion P satisﬁes the pairwise Markov property for a given undirected graph
G if, for every pair of non adjacent edges X and Y , X is independent of Y
given the rest.

X ⊥⊥ Y | (V \ X, Y )

Deﬁnition 9 ((L) Local Markov Property) A probability distribution P
satisﬁes the local Markov property for a given undirected graph G if, every
variable X is conditionally independent of its non neighbours in the graph,
given its neighbours.

X ⊥⊥ (V \ X ∪ bd(X)) | bd(X)

where bd(X) denotes boundary of X.

Deﬁnition 10 ((G) Global Markov Property) A probability distribu-
tion P, is said to be global Markov with respect to an undirected graph G
if, for any disjoint subsets of nodes A, B, C such that C separates A and B
on the graph, if and only if the distribution satisﬁes

A ⊥⊥ B|C

We must note that the above three Markov properties are not equivalent
to each other. The Local Markov property is stronger than the pairwise
one, while weaker than the global one. More precisely, we have the following
proposition.

Proposition 11 For any probability measure the following holds.

(G) =⇒ (L) =⇒ (P )

See [39] for the proof of the proposition (11). We refer to [50], [39] and
[17] for more details on graphical models and to [13] and [12] for Markov
ﬁelds for LLMs.

Notations and Assumptions

In this section, we discuss the notations and the assumptions which we
will be using throughout the remaining sections of this article. We mainly
consider the three-dimensional tables for notational simpliﬁcation, which is
also a true representative of k-dimensions and thus can be easily extended
to any higher dimensions by increasing the number of subscripts. We mostly

4

follow the notation from [9] and [8], for additional details we refer to these
books.

Let us consider a three dimensional table with factors X, Y and Z. It must

be noted that we interchangeably use numeric{1, 2, 3} and alphabetic{X, Y, Z}
symbols for representing the factors of a contingency table. Suppose the fac-
tors X, Y and Z have I,J and K levels respectively, then we have an I ×J ×K
contingency table.
The following notations are deﬁned for each elementary cell (i, j, k) ∀i =
1 . . . I, ∀j = 1 . . . J, ∀k = 1 . . . K

nijk = the observed counts in the cell (i, j, k)
mijk = the expected counts in the cell (i, j, k)
ˆmijk = The Maximum Likelihood Estimate of mijk
pijk = probability of a count falling in cell (i, j, k)
ˆpijk = The Maximum Likelihood Estimate of pi,j,k

The following notations are used for sums of elementary cell counts. where
“.” represents summation over that factor. For example

ni.. =Xjk

nijk =Xk

ni.k

N = n... = total number of observations

Similarly the marginal totals of probabilities and the expected counts are
denoted by p.jk, and m.jk etc.

We represent “C” as tables of sums obtained by summing over one or
more factors, i.e. C12 represents tables of counts {nij.}. Subscripted u-
terms notation are used for main eﬀects and interactions. For example
u12(ij) is used for two-factor interactions ∀i = 1 . . . I, ∀j = 1 . . . J. We may
interchangeably use u12(ij) and u12, later one is obtained by simply dropping
the second set of subscript. Thus u12 = u12(ij)∀i = 1 . . . I, ∀j = 1 . . . J.

We assume that the observed cell counts are strictly positive for all

models we consider throughout this article.

3 Overview of Contingency Tables

In this section, we brieﬂy review structural representation for count data
called contingency tables. A contingency table is a table of counts that
summarizes relationship between factors. In a multivariate qualitative data
where each individual is described by a set of attributes, all individual with
same attributes are counted and this count is entered into cell of a corre-
sponding contingency table, see [8]. The term “contingency” was introduced
by [44]. There is an extensive literature on contingency tables, see [3], [5]
and [24].

5

Example 12 (Example of a three-dimensional contingency table) example
3.2.1 of [9].

Table 1: Personality Type Table

Personality Type

Cholestrol Normal

High

Diastolic Blood Pressure

A

B

Normal

High

Normal

High

716
207
819
186

79
25
67
22

3.1 Types of Contingency Tables

Based on the underlying assumption of sampling distributions, contingency
tables are divided into three main categories as follows.

3.1.1 The Poisson Model

In this model, it is assumed that cell counts are independent and Poisson-
distributed. The total number of counts and the marginal counts are random
variables. For three-dimensional tables with counts as random variables as
nijk, the joint probability density function(pdf) can be written as

m

nijk
ijk e−mijk

nijk!

Yijk

(1)

3.1.2 The Multinomial Model

In this model, it is assumed that total number of subjects, N, are ﬁxed.
With this constraint imposed on independent Poisson distributions, the cell
counts yield a multinomial distribution. For proof we refer to [21].

The pdf for this model is given as

N !

Qijk nijk!Yijk(cid:16) mijk
N (cid:17)nijk

(2)

3.1.3 The Product Multinomial Model

In this model, it is assumed that one set of marginal count is ﬁxed and The
corresponding table of sums follow a product-multinomial distribution. For

6

example, consider a three-dimensional table with total counts for factor 1,
n.jk, ﬁxed. The pdf is given as

Yjk " n.jk!
Qi nijk!Yi (cid:18) mijk

n.jk (cid:19)nijk#

(3)

4

Introduction to Log-linear Models

This section introduces log-linear models for contingency tables. As dis-
cussed previously, the distribution of cell probabilities belong to exponential
family(Poisson, multinomial and product-multinomial). Here we construct
a linear model in the log scale of the expected cell count.

A log-linear model for three-factor table is deﬁne as

ln mijk = u + u1(i) + u2(j) + u3(k) + u12(ij) + u13(ik) + u23(jk) + u123(ijk)

(4)

with the following identiﬁability constraints:

Xi
u1(i) =Xj
u12(ij) =Xj
Xi
u13(ik) =Xk
Xi
u23(jk) =Xk
Xj
u123(ijk) =Xj
Xi

u3(k) = 0

u2(j) =Xk

u12(ij) = 0

u13(ik) = 0

u23(jk) = 0

u123(ijk) =Xk

u123(ijk) = 0

The above model is called saturated or unrestricted because it contains all
possible one-way, two-way and three-way eﬀects. In general if no interaction
terms are set to zero, it is called the saturated model.

We must note that the number of terms in a log-linear model depends on
the dimensions or number of factors and the interdependencies between the
factors, it does not depend on the number of cells, see [6] for more details.
The model given by the equation(4) applies to the all three kinds of
contingency tables with three factors(as discussed in previous section) but
there may be diﬀerences in the interpretations of the interaction terms, see
[34] and [37]. There is a wide literature on LLMs, see for instance, [1], [9],
[51], [8] and [31].

7

4.1 Log-Linear Models as Generalized Linear Models

Let us recall the generalized linear model(GLM). It consists of a linear pre-
dictor and a link function. Link function determines relationship between
the mean and the linear predictor. Here we show that the LLMs are special
instances of GLMs for Poisson-distributed data, see [42] for the details.

Consider a 2 × 2 Poisson model with two factors say X and Y, suppose
cell counts nij are response variables such that nij ∼ P oisson(mij) and the
factors X and Y are explanatory variables.
deﬁne a link function g as

g(mij ) = ln mij

the linear predictor is deﬁned as X
where X is the design matrix and β is the vector of unknown parameters.
For this model, X and β are deﬁned as

β

′

X =


1 1 0 1 0 1 0 0 0
1 1 0 0 1 0 1 0 0
1 0 1 1 0 0 0 1 0
1 0 1 0 1 0 0 0 1

β =




µ
α1
α2
β1
β2

(αβ)11
(αβ)12
(αβ)21
(αβ)22









The model can be expressed as

ln mij = x

′

iβ

= µ + αi + βj + (αβ)ij

We rename the parameters as

ln mij = u + u1(i) + u2(j) + u12(ij)

We notice that the above model is the same as the LLM deﬁned for two-
factor tables, where u is the overall mean, u1, u2 are the main eﬀects and
u12 is the interaction eﬀect.

We note that we can ﬁt LLMs as generalized linear models by using
software packages available for generalized linear models, for example glm()
function in “stats” R package.

4.2 Classes of Log Linear Models

In this section, we discuss various classes of LLMs.

8

4.2.1 Comprehensive Log-linear models

The class of comprehensive Log-linear models is deﬁned as follows.

Deﬁnition 13 (Comprehensive Log-linear Model) A log-linear model
is said to be comprehensive if it contains the main eﬀects of all the factors.

For example, A Comprehensive LLM for the three-factor contingency tables
must include all the main eﬀects u1, u2 and u3 along with other interaction
eﬀects if any, see [51] for the details.

4.2.2 Hierarchical Log-linear models

The class of hierarchical log-linear models is deﬁned as follows.

Deﬁnition 14 (Hierarchical Log-linear Models) A LLM is said to be
hierarchical if it contains all the lower-order terms which can be derived from
the variables contained in a higher-order term.

For example, if a model for three-dimension table includes u12 then u1 and
u2 must be present or conversely if u2 = 0 then we must have u12 = u123 = 0.
It can be noted that hierarchical models may be represented by giving
only the terms of highest order, also known as generating class, since all the
lower-order terms are implicit. Generating class is deﬁned as follows.

Deﬁnition 15 (Generating class) The highest order terms in hierarchi-
cal Log-linear models are called generating class because they generate all of
the lower order terms in the model.

Example 16 A log linear model with generating class C = {[123],[34]} cor-
responds to the following log-linear model.

ln mhijk = u + u1(h) + u2(i) + u3(j) + u4(k) + u12(hi) + u23(ij) + u13(hj) + u123(hij) + u34(jk)

members of generating class [123] = {[1], [2], [3], [12], [23], [13], [123]}

members of generating class [34] = {[3], [4], [34]}

All models considered in the remaining sections of this article are hierarchical
and comprehensive LLMs unless stated otherwise.

4.2.3 Graphical Log-linear Models

In this section, we consider a class of LLMs that can be represented by
graphs, called graphical log-linear models(GLLMs).

Deﬁnition 17 (Graphical Log-linear Models(GLLMs)) A LLM model
is said to be graphical if it contains all the lower order terms which can be
derived from variables contained in a higher-order term, the model also con-
tains the higher order interaction.

9

For example, if a model includes u12, u23 and u31 it also contains the term
u123.

In GLLMs, the vertices correspond to the factors and the edges cor-
respond to the two-factor interactions. But the factors(vertices) and the
two-factor interactions(edges) alone do not specify the graphical models.
As mentioned previously, factorization of the probability distribution with
respect to a graph must satisfy the Markov properties. For such a graph
that respects Markov property with respect to a probability distribution,
there is one-to-one correspondence between GLLMs and graphs. It follows
that every GLLM determines a graph and every graph determines a GLLM,
it is illustrated by the following examples.

Example 18 Let us consider the model [123][134]. The two factor terms
generated by [123] are [12][13][23], similarly two factor terms generated by
[134] are [13][14][34]. The corresponding graph is as given in the ﬁgure (1):

4

1

3

2

Figure 1: Graphical Model [123],[134]

Conversely we can also read log-linear model directly from the corre-

sponding graph.

4

1

3

2

Figure 2: Graphical Model [123] [34]

Consider a graph given in the ﬁgure(2), the edges are [12], [23], [13] and

10

[34]. Since the generating class for the terms [12], [23] and [13] is the term
[123], we must include [123] in the model. Hence the corresponding GLLM
is [123][34].

We must note that, generating classes of graphical log-linear models are in a
one-to-one correspondence with the maximal cliques of corresponding graph.
Let us also note that not all hierarchical log-linear models have graphical
representation. For example the model [12][13][23] is hierarchical but it is
not graphical since it does not contain the higher order term [123].

4.2.4 Decomposable Models

In this section, we deﬁne the class of decomposable models which is a sub-
class of GLLMs.

Deﬁnition 19 (Decomposable log-linear Models) A LLM model is de-
composable if it is both graphical and chordal.

The main advantage of this model over other models is that it has closed form
Maximum Likelihood Estimates(abbreviated as MLE, which is explained in
the later sections).

For example, let us consider a decomposable model given by the ﬁgure
(1). The only conditional independence implied by the graph is that, given
the factors 1 and 3, factors 2 and 4 are independent. The maximum likeli-
hood estimates for the expected cell counts are factorized in a closed form
in the terms of suﬃcient statistics as

ˆmhijk = nhij.nh.jk/nh.j.

The derivation of the above such expressions are discussed in the details in
section 5.

The table (2) shows all the possible non-isomorphic graphical and de-

composable models for the four-factor contingency tables.

Few important articles concerned with the decomposable models are

[[26], [27], [29], [29], [40],[41], [10]].

Table 2: Graphical Log-linear Models for four-way tables

Model

Graph

Closed-form Estimates

[1][2][3][4]

4

1

3

2

11

ˆmhijk = nh...n.i..n..j.n...k

n3

....

ˆmhijk = nhi..n..j.n...k

n2

....

ˆmhijk = nhi..nh.j.n...k

nh...n....

ˆmhijk = nhi..n..jk

n....

ˆmhijk = nhi..nh.j.nh..k

n2

h...

ˆmhijk = nhi..n.ij.n..jk

n.i..n..j.

[12][3][4]

[12][13][4]

[12][34]

[12][13][14]

[12][23][34]

4

4

4

4

4

1

3

1

3

1

3

1

3

1

3

2

2

2

2

2

12

ˆmhijk = nhij.n...k

n....

ˆmhijk = nhij.nh..k

nh...

No closed-form estimates exist

ˆmhijk = nhij.nh.jk

nh.j.

ˆmhijk = nhijk

[123][4]

[123][14]

[12][23][34][14]

[123][134]

[1234]

4

4

4

4

4

1

3

1

3

1

3

1

3

1

3

2

2

2

2

2

13

5 Statistical Properties of the Log-linear Models

In this section, we discuss statistical properties of the hierarchical LLMs,
like the existence of suﬃcient statistics, uniqueness of the MLE and model
testing. First, we derive suﬃcient statistics for the unknown parameters of
the model. Then we show how to compute the MLE of the expected cell
counts from the suﬃcient statistics without computing the model parame-
ters. We also show that for some models the estimated cell counts are the
explicit closed function of the suﬃcient statistics, whereas for others we need
iterative procedures.

5.1 The Suﬃcient Statistics for LLMs

We show that the suﬃcient statistics exist for the hierarchical LLMs and
they are very easy to obtain. Let us consider the saturated model with simple
multinomial sampling distribution for the 3-factor contingency tables. The
log-likelihood function of the multinomial is obtained from the pdf given by
the equation(1) as follows.

lnf ({nijk}) = ln  N !

Qijk nijk!! +Xijk

Or equivalently we can write the above expression as

nijk ln(mijk) − N ln N

(5)

lnf ({nijk}) =Xijk

nijk ln(mijk) + C

(6)

Where “C” represents the constant terms. Substituting the value for ln(mijk)
as given by the equation (4) we get the following expression.

nijk(u + u1 + u2 + u3 + u12 + u13 + u23 + u123) + C

the above expression can be also written as

lnf ({nijk}) =Xijk
f ({nijk}) = exp(N u +Xi
u12nij. +Xik

Xij

u1ni.. +Xi
u13ni.k +Xjk

u1ni.. +Xj
u23n.jk +Xijk

u2n.j. +Xk

u3n..k+

u123nijk + C)

Since multinomial distribution belongs to exponential family suﬃcient statis-
tic exists, see [4]. From the above expression it is apparent that for the three-
factor saturated model, the full table itself is the suﬃcient statistic since the
lower order terms are redundant and it will be subsumed in the full table.
We note that the marginal sub-tables which correspond to the set of
generating classes are the suﬃcient statistics for the log-linear models, see
[6].

14

Example 20 Consider a four-factors table with the following generating
classes.

{C1, C2} = {[123], [34]}

then C1(n) = [nijk.] , it is a three-dimensional marginal sub table.
and C2(n) = [n..kl] , it is a two-dimensional marginal sub table.

These two marginal sub-tables are the suﬃcient for this model.

For more details and the proofs on the suﬃcient statistics for hierarchical
LLMs see [6] and [28].

5.2 The Maximum Likelihood Estimates for LLMs

First, we state that a unique set of MLE for every cell count can be obtained
from the suﬃcient statistics alone, see [6] for the proof.

Now we state the Birch criteria as follows:
1. The marginal sub-tables obtained by summing over the factors not present
in the max-cliques are the suﬃcient statistics for the corresponding expected
cell counts. i.e., for the model {[123], [34] }, C1(n) = ((nijk.)) and C2(n) =
((n..jk)) are suﬃcient statistics for mijk. and m..jk respectively.

2. All the suﬃcient statistics must be the same as the corresponding marginal
sub-tables of their estimate means.

Ci( ˆm) = Ci(n)

∀i = 1 to # of generating classes

i.e., for the model {[123], [34] } the estimated cell counts are

ˆmijk. = nijk.
ˆm..kl = n..kl

Finally, the MLE of the expected cell counts for the model {[123], [34]}

is expressed as follows.

ˆmhijk =

nhij.n..jk

n..j.

In section 5.4, we derive the closed form expressions for the MLEs in terms
of suﬃcient statistics for three-factor contingency tables.

The reason for choosing MLE for computing the expected cell counts
is its consistency and eﬃciency in the large samples. There is extensive
research on the MLE of LLMs, we refer few of them here [23], [3], [29],
[41],[6], [20], [36], [38] and [11] .

15

5.3 Testing models

The assessment of a model ﬁt is very important as it describes how well it
ﬁts the data. We use the following test statistics.

• Pearson’s χ2 Statistic: which is deﬁned as

(Oi − Ei)2

Ei

χ2 =Xi

where O denotes the observed cell counts and E as the expected cell
counts.

• Deviance goodness of ﬁt test statistics: We test a model against the
saturated model using the deviance goodness of ﬁt test, which is de-
ﬁned as follows.

G2 = −2Xi

Oi log

Ei
Oi

Under null hypotheses deviance is also distributes as χ2 with the ap-
propriate degrees of freedom.

Table (3), lists the degree of freedom of all the possible models for three-
factor tables. For more information about the model testing we refer [14],
[33] and [35].

Table 3: Degrees of freedom

Model

[1][2][3]
[12][3]
[13][2]
[23][1]
[12][13]
[12][23]
[13][23]

df

IJK - I - J - K + 2

(IJ-1)(K-1)
(IK-1)(J-1)
(JK-1)(I-1)
I(J-1)(K-1)
J(I-1)(K-1)
K(I-1)(J-1)

[12][13][23]

(I-1)(J-1)(K-1)

[123]

0

6 The Analysis of three-factor Contingency Tables

In this section, we discuss the diﬀerent interaction models for three-factor
tables.We also derive mathematical formulation for the MLE of the expected
counts( when it is possible) for each model.

16

6.1 The Complete Independence Model

This is the simplest model where all the factors are mutually independent
and u12 = u13 = u23 = u123 = 0. The following diﬀerent equivalent nota-
tions can be used to represent this model.

X ⊥⊥Y ⊥⊥ Z

ln(mijk) = u + u1 + u2 + u3

(7)

C = {[1], [2], [3]}

This model can be represented graphically as given in the ﬁgure (3).

1

3

2

Figure 3: The Complete Independence Model

Example 21 When we substitute the value of ln(mijk) as given in the equa-
tion (4) to log-likelihood kernel as given by the equation (6) and ignoring the
constant term we get

f ({nijk}) =Xijk
=Xijk

after simpliﬁcation we obtain

nijk ln(mijk)

nijk(u + u1 + u2 + u3+)

f ({nijk}) = exp(N u +Xi

u1ni.. +Xj

u2n.j. +Xk

u3n..k)

From the above expression we obtain the suﬃcient statistics for this models
as marginal sub-tables: C1 = {ni..}, C2 = {n.j.} and C3 = {n..j} which are
estimates of mi.., m.j. and m..k respectively.

From the equation (7), by summing over jk, ik, ij and ijk we obtain mi..,

17

m.j. and m..k and m... as

mi.. = exp(u + u1)Xjk
= exp(u + u1)Xj
m.j. = exp(u + u2)Xik
= exp(u + u2)Xi
m..k = exp(u + u3)Xij
= exp(u + u3)Xi

exp(u2 + u3)

exp(u2)Xk

exp(u3)

exp(u1 + u3)

exp(u1)Xk

exp(u3)

exp(u1 + u2)

exp(u1)Xj

exp(u2)

m... = exp(u)Xijk
= exp(u)Xi

exp(u1 + u2 + u3)

exp(u1)Xj

exp(u2)Xk

exp(u3)

From the above equations we get the expression for mijk as

mijk =

mi..m.j.m..k

m2
...

Applying Birch’s result we get the estimates of mijk as

ˆmijk =

ni..n.j.n..k

n2
...

Let us consider a contingency table as in table (1), Under the complete
independence assumption the suﬃcient statistics are the following marginal
sub-tables.

Table 4: Personality Type

Table 5: Cholestrol

Table 6: DBP

Personality Type

A
B

1027
1094

Cholestrol

Normal

High

1681
440

Cholestrol

Normal

High

1005
1116

Under the complete independence assumption the table of ﬁtted values

are

18

Table 7: Table of Estimated Cell Counts

Diastolic Blood Pressure

Personality Type

Cholestrol Normal

A

B

Normal

High

Normal

High

739.9
193.7
788.2
206.3

High

74.07
19.39
78.9
20.65

The G2 statistic for the model is 8.723(df:4, p-value:0.068), hence we con-
clude that the data supports the complete independence model. For details
on Chi-Squared test of Independence we refer the reader to [27].

6.2 The Joint Independence Model

Under this model two factors are jointly independent of the third factor.
There are three versions of this model, depending on which factor is unre-
lated to the other two. These three models are (X, Y ) ⊥⊥ Z , (X, Z) ⊥⊥ Y
and (Y, Z) ⊥⊥ X. We consider only (X, Y ) ⊥⊥ Z in detail as others are
comparable.
Equivalent diﬀerent notations are as

(X, Y ) ⊥⊥ Z

ln(mijk) = u + u1 + u2 + u3 + u12

(8)

C = {[12], [3]}

This model can also be represented graphically as given in the ﬁgure (4). The

1

3

2

Figure 4: The Joint Independence Model

suﬃcient statistics for this models are the marginal sub-tables: C1 = {nij.}
and C2 = {n..j} which are the estimates of mij. and m..k. From the equation

19

(8) we obtain

exp(u3)

exp(u1 + u2 + u12)

mij. = exp(u + u1 + u2 + u12)Xk
m..k = exp(u + u3)Xij
m... = exp(u)Xij

exp(u1 + u2 + u12)Xk

exp(u3)

From the above equations we derive the closed form expression for mijk as

mijk =

mij.m..k

m...

Applying Birch’s criteria we get

ˆmijk =

nij.n..k

n...

We note that if the previous model of the complete independence (X ⊥⊥
Y ⊥⊥ Z) ﬁts a data set, then the model ((XY ) ⊥⊥ Z) will also ﬁt. But the
smallest model will be preferred.

Example 22 Let us consider another example to discuss this model.

Table 8: Classroom Behaviour Table (Everitt,1977)

Classroom Behaviour Adversity of School Not at risk at Risk

Risk

Nondeviant

Deviant

Low

Medium

High
Low

Medium

High

16
15
5
1
3
1

7
34
3
1
8
3

20

The suﬃcient statistics are

Table 9: Adv*risk

Risk

Table 10: Classroom Beha.

Adversity Not at risk at Risk

Low

Medium

High

17
18
6

8
42
6

Classroom Beha. Total

Nondeviant

Deviant

80
17

Under assumption of this model, the table of the expected cell counts is give
in the table (11).

Table 11: Table of Estimated Cell Counts

Classroom Behaviour Adversity of School Not at risk at Risk

Risk

Nondeviant

Deviant

Low

Medium

High
Low

Medium

High

14.020
14.845
4.948
2.979
3.154
1.051

6.597
34.639
4.948
1.402
7.360
1.051

The G2 statistic for this model is 5.560(df:5, p-value:0.351), hence we

conclude that the data supports the joint independence model.

6.3 The Conditional Independence Model

Under this model two factor are conditionally independent given the third
factor. There are three version for this model as well, these are X ⊥⊥ Y |Z,
X ⊥⊥ Z|Y and Y ⊥⊥ Z|X. We consider only X ⊥⊥ Y |Z in the detail as
derivation for others are similar.

This model can be equivalently represented as

ln(mijk) = u + u1 + u1 + u3 + u13 + u23

(9)

C = {[13][23]}

The graph for this model is given in the ﬁgure(5). The suﬃcient statistics
for this model are the marginal sub-tables: C13 = {ni.k} and C23 = {n.jk}

21

1

3

2

Figure 5: The Conditional Independence Model

which are estimates of mi.k and m.jk. From the equation (9) we get

mi.k = exp(u + u1 + u3 + u13)Xj
m.jk = exp(u + u2 + u3 + u23)Xi
m..k = exp(u + u3)Xi

exp(u1 + u13)Xj

exp(u2 + u23)

exp(u1 + u13)

exp(u2 + u23)

From the above three equations we obtain the closed form expression for
mijk

mijk =

mij.m.jk

m..k

As before applying Birch’s criteria we derive the expected counts for each
cell as

ˆmijk =

nij.n.jk

n..k

Example 23 Let us consider the following infant’s survival table, data taken
from [7].

Table 12: Infant Survival Table

Infant’s Survival

Clinic Pre-natal care Died Survived

A

B

Less
More
Less
More

3
4
17
2

176
293
197
23

22

Assuming pre-natal care and survival is independent given clinic. The

suﬃcient statistics are

Table 13: survival*clinic

Table 14: clinic*prenatalcare

Infant’s Survival

Prenatal Care

Clinic died

Survived

Clinic Less More

A
B

7
19

469
220

A 179
B
214

297
25

Table 15: clinic

Clinic Total

A
B

476
239

Table 16: Table of Estimated Cell Counts

Infant’s Survival

Clinic Pre-natal care Died Survived

A

B

Less
More
Less
More

176.367
2.632
292.632
4.367
17.012 196.987
1.987
23.012

The G2 statistic for this model is 0.082(df:2, p-value:0.959), hence we

conclude that the data supports the conditional independence model.

6.4 The Uniform Association Model

This model is also known as no three-factor interaction model, where u123 =
0. For this model the log-linear notation is ([12], [13], [23]) but there is no
graphical representation for this model. Unlike the previous models, there
are no closed-form estimates for the expected cell counts/probabilities under
this model. The maximum likelihood estimates can be computed by iterative
procedures such as iterative proportional ﬁtting(IPF) and Newton Raphson
method.

23

Example 24 Let us consider the following table taken from [18].

Table 17: Auto Accident Table

Injury

Accident Type Driver Ejected Not Sever Severe

Collision

RollOver

No
Yes
No
Yes

350
26
60
19

150
23
112
80

None of the models discussed in previous sections ﬁt the data. We use
iterative proportional ﬁtting algorithm to obtain the table of estimated counts
as given in the table (18).

Table 18: Table of Estimated Cell Counts

Injury

Accident Type Driver Ejected Not Sever

Severe

Collision

RollOver

No
Yes
No
Yes

350.48858 149.51130
25.51142 23.48870
59.51104 112.48921
19.48896 79.51079

The G2 statistic for this model is 0.043(df:1, p-value:0.835), hence we

conclude that the data supports the marginal association model.

For more information on IPF we refer to [16] and [19]. We used the
IPF procedure implemented in the R package “cat”, available at cran.r-
project.org.

6.5 The Saturated Model

For this model the log-linear notation is ([123]). In this case there is no in-
dependence relationship between the three factors. The expected cell counts
are the same as the observed cell frequencies.

ˆmijk = nijk

Graphical representation for the saturated model is given in the fugure(6).

Example 25 Let us consider partial table which is based on clinical trial
data [32].

24

1

3

2

Figure 6: The Saturated Model

Table 19: Results of Clinical Trial for the Eﬀectiveness of an Analgesic Drug

Response

Status Treatment Poor Moderate Excellent

1

2

Active
placebo
Active
placebo

3
11
3
6

20
4
14
13

5
8
12
5

None of the model ﬁts the data, we leave it for the reader to verify.

7 Model selection for the Decomposable Models

In this section, we discuss model selection for the decomposable models only,
since a non-decomposable graphical model can be reduced to the decompos-
able one by adding minimal number of edges to the graph. For the details
on minimum triangulation we refer to [46] and an excellent survey article by
[30].

Though decomposable models are a restricted family of GLLMs, selecting
an optimal model from the class of decomposable graphical models is known
to be an intractable problem. Most of all existing model selection algorithms
are based on forward selection, backward elimination or combination of the
both. There is a vast literature available for model selection and inference
on graphical models, i.e., see [47], [10] , [25], [45] and [2].

We now illustrate the backward model selection procedure for a real
data called “women and mathematics(WAM)”, data used in [22]. We use
Wermuth’s backward elimination algorithm, see [48] for the details. The
data is shown in the table(20).

25

Table 20: The table WAM

School

Sex

Suburban School

Urban School

Female

Male

Female

Male

Plan

Preference

Attend Not Attend Not Attend Not Attend Not

College Maths-sciences

Job

Agree
Disagree
Liberal arts
Agree
Disagree
Maths-sciences
Agree
Disagree
Liberal arts
Agree
Disagree

37
16

16
12

10
9

7
8

27
11

15
24

8
4

10
4

51
10

7
13

12
8

7
6

48
19

6
7

15
9

3
4

51
24

32
55

2
8

5
10

55
28

34
39

1
9

2
9

109
21

30
26

9
4

1
3

86
25

31
19

5
5

3
6

Let us recall that graphical models are completely speciﬁed by their two-
factor interactions. By the hierarchical principle, if a two-factor term is set
to zero then any higher order term that contain that particular two-factor
term will be also set to zero.

The Wermuth’s procedure starts with the saturated model, a single
clique that includes all the two-factor eﬀects as given in the ﬁgure (7). The
vertices “a”,“b”,“c”,“d”,“e”,“f” correspond to the factors “Attendance”,“Sex”,“School”,
“Agree”, “Subject” and “Plans” respectively.

f

e

a

d

b

c

Figure 7: The Saturated Model for WAM

In the next step, all two-factor interactions (cid:0)6

ination. We ﬁx a backward elimination cut oﬀ level α = 0.5. Among the
two-factor interactions the term having the largest p-value are considered
for elimination , only if the p-value exceeds α. From the table (21), we

2(cid:1) are considered for elim-

26

choose the edge (b,f) for deletion the resulting graphical model is the cliques
[abcde][acdef]

Table 21: WAM: [abcde]

d.f. G2

Edge Clique
ab
ac
ad
ae
af
bc
bd
be
bf
cd
ce
cf
de
df
ef

p-value
[acdef][bcdef] 16 18.585 0.29078
[abdef][bcdef] 16 20.689 0.19080
[abcef][bcdef] 16 14.172 0.58588
[abcdf][bcdef] 16 18.781 0.28017
[abcde][bcdef] 16 11.951 0.74734
[acdef][abdef] 16 26.739 0.04447
[acdef][abcef] 16 34.733 0.00432
[acdef][abcdf] 16 56.570 0.00000
[acdef][abcde] 16 11.673 0.76616
[abcef][abdef] 16 29.439 0.02114
[abcdf][abdef] 16 26.052 0.05329
[abcde][abdef] 16 81.657 0.00000
[abcdf][abcef] 16 78.248 0.00000
[abcef][abcde] 16 46.221 0.00009
[abcde][abcde] 16 17.728 0.34005

In the next step, we consider the cliques [abcde] and [acdef] . The edges
ac, ad, ae, cd, ce and de are common to both the cliques, hence they are
not considered for elimination. The candidate edges for deletion are ab, bc,
bd, be, af, cf, df, ef. Let us examine the p-values for these edges as in the
table(22).

Table 22: WAM: [abcde][acdef]

Edge Clique
ab
bc
bd
be
af
cf
df
ef

[bcde][acdef] 8
[acde][acdef] 8
[acde][acdef] 8
[acde][acdef] 8
[abcde][cdef] 8
[abcde][adef] 8
[abcde][acef] 8
[abcde][acdf] 8

d.f. G2

p-value
12.456 0.13198
18.097 0.02051
27.358 0.00061
49.723 0.00000
5.822 0.66711
73.014 0.00000
38.845 0.00001
10.881 0.20852

We delete the edges (af), the resulting graphical model is [abcde] [cdef].
Similarly we proceed further and in the next step the edge (ad) gets deleted
and the resulting graphical model is [abce] [bcde] [cdef] as given in the
ﬁgure(8).

27

f

e

a

d

b

c

Figure 8: The Fitted Model for WAM

In the next step candidate edges for deletion are [ab],[ac],[ae],[bd],[cf].
We notice that none of the p-value is greater than α = 0.05 as given in the
table (23). So we stop with the model [abce][bcde][cdef].

Table 23: WAM: [abce][bcde][cdef]

Edge Clique

d.f. G2

p-value

ab
ac
ae
bd
cf

[ace] [bce][bcde][cdef] 4
[bce] [ace][bcde][cdef] 4
[bce] [abc][bcde][cdef] 4
[abce] [cde][bce] [cdef] 4
[abce] [bcde][def] [cde] 4

10.606 0.03137
10.432 0.03374
10.426 0.03383
25.507 0.00004
67.832 0.00000

8 Computational details

All the experimental results in this paper were carried out using R 3.1.3 .
For ﬁtting LLMs, there are several function in R, for example glm( ) and
loglin( ) in the “stats” library and loglm( ) in the ”MASS” library. For
model selection, we used dmod() and backward() functions implemented in
the package “gRim”. All the packages used are available at http://CRAN.R-
project.org/.

9 Concluding Remarks

In summary, we have discussed fundamental mathematical and statistical
theory of GLLM and its applications. We restricted out attention to the
complete table to make our discussion simple, as the tables having zero
entries require special treatment. See chapter 8 of [9] for the analysis of
contingency tables with zero cell counts. The limitations, and open problems
in the use of GLLM for recursive relationships can be further explored, see
section 5.4 of [9].

28

References

[1] A. Agresti. Categorical Data Analysis. 2nd edition. Wiley-Interscience,

New York, 2002.

[2] Genevera I. Allen and Zhandong Liu. “A Log-Linear Graphical Model

for Inferring Genetic Networks from High-Throughput Sequencing Data”.
In: BIBM (2012), pp. 1–6.

[3] A. H. Andersen. “Multidimensional contingency tables”. In: Scand. J.

Statist. 3 (1974), pp. 115–127.

[4] E. B. Andersen. “Suﬃciency and Exponential Families for Discrete
Sample Spaces”. In: Journal of the American Statistical Association
65.331 (1970), pp. 1248–1255.

[5] M. S. Bartlett. “Contingency table interactions”. In: Supplement to

the Journal of the Royal Statistical Society. 2 (1935), pp. 248–252.

[6] M.W. Birch. “Maximum likelihood in three-way contingency tables”.

In: Journal of the Royal Statistical Society 25 (1963), pp. 220–233.

[7] Y.M.M. Bishop. “Full contingency tables, logits, and split contingency

tables”. In: Biometrics 25 (1969), pp. 383–400.

[8] Y.M.M. Bishop, S.E. Fienberg, and P.W. Holland. Discrete Multivari-
ate Analysis: Theory and Practice. The MIT Press, Cambridge, MA,
1989 edition.

[9] R. Christensen. Log-Linear Models and Logistic Regression. 2nd edi-

tion. Springer, 1997.

[10] Corinne Dahinden, Markus Kalisch, and Peter Buhlmann. “Decom-
position and Model Selection for Large Contingency Tables”. In: Bio-
metrical Journal 52 (2010), 233–252.

[11] J. N. Darroch. “Interactions in multi-factor contingency tables”. In:

J. Roy. Statist. Soc. Ser. B 24 (1962), pp. 251–263.

[12] J. N. Darroch et al. “Markov ﬁelds and log-linear interaction models

for contingency tables”. In: Ann. Statist. 8 (1980), pp. 522–539.

[13] J. N. Darrocha et al. “Markov ﬁelds and log-linear interaction models

for contingency tables”. In: Ann. Statist. 8 (1980), pp. 522–539.

[14] L. J. Davis. “Exact tests for 2 by 2 contingency tables”. In: Amer.

Statist. 40 (1968), pp. 139–141.

[15] A. P. Dawid. “Conditional Independence in Statistical Theory”. In:

Journal of the Royal Statistical Society 41.1 (1979), pp. 1–31.

[16] W. E. Deming and F. F. Stephan. “On a Least Squares Adjustment of
a Sampled Frequency Table When the Expected Marginal Totals are
Known”. In: Annals of Mathematical Statistics 11.4 (1940), pp. 427–
444.

29

[17] D. Edwards. Introduction to Graphical Modeling. 2nd edition. NY:

SpringerVerlag, 2000.

[18] S.E. Fienberg. The Analysis of Cross-classiﬁed Categorical Data. 2nd

Edition. The MIT Press, Cambridge, MA, 1980.

[19] Stephen E. Fienberg. “An Iterative Procedure for Estimation in Con-

tingency Tables”. In: Ann. Math. Statist. 41.3 (1970), pp. 901–917.

[20] Stephen E. Fienberg and Alessandro Rinaldo. “Three Centuries of Cat-
egorical Data Analysis: Log-linear Models and Maximum Likelihood
Estimation”. In: Journal of Statistical Planning and Inference 137(11)
(2007), pp. 3430–3445.

[21] R.A. Fisher. “On the mathematical foundations of theoretical statis-
tics”. In: Philosophical Transactions of the Royal Society of London
Part A.222 (1922), pp. 309–368.

[22] E. B. Fowlkes, A. E. Freeny, and J. M. Landwehr. “Evaluating logistic
models for large contingency tables”. In: J. Amer. Statist. Assoc 83
(1988), pp. 611–622.

[23] G. F. V. Glonek, J. N. Darroch, and T. P. Speed. “On the Existence of
Maximum Likelihood Estimators for Hierarchical Loglinear Models”.
In: Scand. J. Statist. 15 (1988), pp. 187–193.

[24] L. A. Goodman. “How to ransack social mobility tables and other
kinds of cross-classiﬁcation tables”. In: Amer. J. Sociol. 75 (1969),
pp. 1–40.

[25] L. A. Goodman. “The analysis of multidimensional contingency tables:
Stepwise procedures and direct estimation methods for building mod-
els for multiple classiﬁcations”. In: Technometrics 13 (1971a), pp. 31–
66.

[26] L. A. Goodman. “The multivariate analysis of qualitative data: Inter-
action among multiple classiﬁcations”. In: J. Amer. Statist. Assoc 65
(1970), pp. 226–256.

[27] L. A. Goodman. “The partitioning of chi-square, the analysis of marginal

contingency tables, and the estimation of expected frequencies in mul-
tidimensional contingency tables”. In: J. Amer. Statist. Assoc. 66
(1971b), pp. 339–344.

[28] S. J. Haberman. “Log-linear models for frequency data: Suﬃcient
statistics and likelihood equations”. In: Annals of Statistics 1.4 (1973),
pp. 617–632.

[29] S. J. Haberman. “The Analysis of Frequency Data”. In: University of

Chicago Press (1974).

[30] Pinar Heggernes. “Minimal triangulations of graphs: A survey”. In:

Discrete Mathematics 306.3 (2006), pp. 297–317.

30

[31] D. Knoke and P.J Burke. “Log-linear Models”. In: Sage, Beverly Hills,

CA (1980).

[32] G.G. Koch et al. “Overview of categorical analysis methods”. In: SAS-

SUGI 8 (1983), pp. 785–795.

[33] S. Kreiner. “Analysis of multidimensional contingency tables by exact
conditional tests: Techniques and strategies”. In: Scand. J. Statist. 14
(1987), pp. 97–112.

[34] S. Kreiner. “Interaction models”. In: Encyclopedia of Biostatistics Chich-

ester,UK: Wiley (1998).

[35] J. R. Landis et al. “Average partial association in three-way contin-
gency tables: A review and discussion of alternative tests”. In: Inter-
nat. Statist. 46 (1978), pp. 237–254.

[36] J. B Lang. “Maximum likelihood methods for a generalized class of

log-linear models”. In: Ann. Statist 24 (1996), pp. 726–752.

[37] J. B Lang. “On the comparison of multinomial and Poisson log-linear

models”. In: J. Roy. Statist. Soc. Ser. B 58 (1996), pp. 253–266.

[38] J. B. Lang, J. W. McDonald, and P. W. F. Smith. “Association-
marginal modeling of multivariate categorical responses: A maximum
likelihood approach”. In: J. Amer. Statist. Assoc. 94 (1999), pp. 1161–
1171.

[39] S. L Lauritzen. Graphical Models. 2nd edition. Oxford University Press

Inc., New York, 1996.

[40] S. L. Lauritzen, T. P. Speed, and K. Vijayan. “Decomposable graphs
and hypergraphs”. In: Journal of The Australian Mathematical Society
36.01 (1984).

[41] Glen Meeden et al. “The Admissibility Of The Maximum Likelihood
Estimator For Decomposable Log-Linear Interaction Models For Con-
tingency Tables”. In: Commun. Statist. Ser. A 27 (1998), pp. 473–
493.

[42] J. A. Nelder and R. W. M. Wedderburn. “Generalized linear models”.

In: Journal of the Royal Statistical Society 135 (1972), pp. 370–384.

[43] J. Pearl and A. Paz. “Graphoids: A graph based logic for reasoning
about relevance relations”. In: Advances in Artiﬁcial Intelligence 2
(1987), pp. 357–363.

[44] Karl Pearson. “Mathematical contributions to the theory of evolu-

tion”. In: Dulau and Co (1904).

[45] P. Ravikumar, M. J. Wainwright, and J. Laﬀerty. “High-dimensional
Ising model selection using L1-regularized logistic regression”. In: An-
nals of Statistics 38.3 (2010), pp. 1287–1319.

31

[46] D. Rose et al. “Algorithmic aspects of vertex elimination on graphs”.

In: SIAM J. Comput. 5 (1976), pp. 146–160.

[47] Martin J. Wainwright and Michael I. Jordan. Graphical Models, Expo-
nential Families, and Variational Inference. Foundations and Trends
in Machine Learning, 2008.

[48] Nanny Wermuth. “Model search among multiplicative models”. In:

Biometrics 32 (1976), pp. 253–263.

[49] Douglas B. West. Introduction to Graph Theory. 2nd Edition. The

MIT Press, Cambridge, MA, 2000.

[50] J. Whittaker. Graphical Models in Applied Multivariate Statistics. 2nd

edition. Chichester: Wiley, 1990.

[51] Daniel Zelterman. Models for Discreet Data. 2nd edition. Oxford Uni-

versity Press Inc., New York, 2006.

32

This figure "finalmodel.png" is available in "png"(cid:10) format from:

http://arxiv.org/ps/1603.04122v1

This figure "saturated.png" is available in "png"(cid:10) format from:

http://arxiv.org/ps/1603.04122v1

This figure "step1.png" is available in "png"(cid:10) format from:

http://arxiv.org/ps/1603.04122v1

