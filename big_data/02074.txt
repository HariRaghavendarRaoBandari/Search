6
1
0
2

 
r
a

M
7

 

 
 
]

G
L
.
s
c
[
 
 

1
v
4
7
0
2
0

.

3
0
6
1
:
v
i
X
r
a

OPTIMAL DICTIONARY FOR LEAST SQUARES

REPRESENTATION

MOHAMMED RAYYAN SHERIFF AND DEBASISH CHATTERJEE

Abstract. Dictionary Learning problems are concerned with ﬁnding a col-
lection of vectors usually referred to as the dictionary, such that the repre-
sentation of random vectors with a given distribution using this dictionary is
optimal. Most of the recent research in dictionary learning is focused on devel-
oping dictionaries which oﬀer sparse representation i.e. optimal representation
in the ℓ0 sense. We consider the problem of ﬁnding an optimal dictionary with
which representation of samples of a random vector on an average is optimal.
Optimality of representation is deﬁned in the sense of attaining minimal av-
erage ℓ2-norm of the coeﬃcient vector used to represent the random vector.
With the help of recent results related to rank-one decompositions of real sym-
metric positive semi-deﬁnite matrices, an explicit solution for an ℓ2-optimal
dictionary is obtained.

1. Introduction

A dictionary of vectors is a collection of vectors in a ﬁnite dimensional vector
space over R, with which other vectors of the vector space are represented. The
concept of a dictionary is a generalization of the concept of a basis and they diﬀer in
the fact that a dictionary of vectors could potentially contain arbitrary number of
vectors, whereas the number of vectors in a basis is exactly equal to the dimension
of the vector space.
In this article we consider a problem of ﬁnding an optimal
dictionary, where we are interested in ﬁnding a dictionary of vectors with which
the representation of other vectors in the underlying vector space is optimal in a
certain sense. For us optimality is assessed by the average size of the coeﬃcients
required to represent other vectors.

For example, suppose that our dictionary consists of vectors v1 “ p1 ´ ǫqJ,
ǫqJ in R2 with a small value of ǫ, and our objective is to represent a vector
v2 “ p1
1qJ with high probability, using v1 and v2. Then the corre-
v that is close to p0
ǫqJ ` α2p1 ´
sponding coeﬃcients of representation α1 and α2 such that α1p1
1qJ will be approximately equal to 1{p2ǫq in magnitude. Clearly, the
ǫqJ “ v « p0
magnitudes of these coeﬃcients are large for small values of ǫ. It is therefore more
1qJ
prudent to consider a dictionary consisting of vectors v1 “ pǫ
1qJ with high
in order to represent vectors which are known to be close to p0
probability, in which case, the magnitudes of the coeﬃcients of representation are
closer to 1{2, which are comparatively smaller to the earlier value of 1{p2ǫq. Thus,
given some information regarding the vectors to be represented, one can consider
designing a dictionary in order to minimize the cost of representation.

1qJ, v2 “ p´ǫ

In order to motivate the relevance of the problem of ﬁnding an optimal dictionary,
let us consider the following two examples. For the ﬁrst, let us consider a signal
source which outputs random discrete time signals, each of length n such that the
frequency spectrum of possible sample signals put out by the source are similar.
Without loss of generality let us assume that the Fourier transform of the sample

Date: 07 March 2016.
The authors thank Prof. Vivek S. Borkar for insighful discussions and pointers to the literature.

1

2

M. RAYYAN SHERIFF AND D. CHATTERJEE

signals is concentrated towards low frequency. Our task is to ﬁnd a collection of n
vectors with which the representation is optimal. The cost of representation of a
sample signal is taken to be the ℓ2-norm of the coeﬃcient vector used to represent
it. It is quite evident that on an average the coeﬃcient vector will have smaller
ℓ2-norm if we represent the random signals coming out from the source using low
frequency signals instead of the usual euclidean signals since we know for a fact that
euclidean signals have both high and low frequency components. One could also
consider representing random signals with a larger K-length coeﬃcient vector using
K dictionary vectors instead of just n. The results presented in this article predict
that the average ℓ2-norm of the K-length coeﬃcient vector decreases monotonically
with increasing K.

Let us consider the second example for the motivation to ﬁnd a suitable dictio-
nary from a control engineer’s point of view. Consider an nth order linear time
invariant system modeled by the recursion

xpt ` 1q “ Axptq ` Buptq,

(1)
If we consider the standard reachability problem of the linear system with xp0q “ x0
and xpKq “ xf for any K ě n, we see at once that the control inputs upiq that are
needed to transfer the linear system (1) from x0 to xf must be a solution to the
following system of linear equations:

t “ 0, 1, . . . .

xf ´ AK x0 “

K´1

ÿi“0 `AiB˘ upK ´ 1 ´ iq.

diﬀerent costřK´1

Given that the objective is to reach xf from x0, diﬀerent linear systems incur
i“0 upiq2. Suppose that there is a ﬂexibility in designing the linear
system and one of the primary objective is to minimize the cost of reachability. Then
the system design problem is similar to the one of ﬁnding an optimal dictionary,
where the matrices A and B are to be properly designed such that the Dictionary
vectors AiB are optimal.

There has been a signiﬁcant amount of recent research in ﬁnding optimal dictio-
naries, brieﬂy outlined in [14], and current research is focussed around developing
learning algorithms for ﬁnding optimal dictionaries. Most of the work is centered
around arriving at a dictionary that oﬀers approximate but sparse representation
of sample vectors. One of the ﬁrst learning algorithms to develop a dictionary that
oﬀers sparse representation of images was given in [10]. Learning algorithms have
been developed to obtain dictionaries that oﬀer sparse representation along with
other special properties like online computation capability [7], better classiﬁcation
property [8, 16], better adaptive properties [12]; and several other algorithms are
given in [6, 9, 15].

The problem that we have considered in this article diﬀers from the mainstream
in the sense that our objective is to ﬁnd a dictionary that gives minimum average
ℓ2-norm of the coeﬃcient vector used for exact representation of sample vectors
without any approximation. Our work presented here is a form of generalization to
the recent developments in representation of signals/vectors using frames for ﬁnite
dimensional Hilbert spaces.

In Section 3 we formally introduce our problem of ﬁnding an optimal dictionary
which oﬀers least square representation. In Section 4 we establish a result related
to decomposition of positive semi-deﬁnite matrices that is essential in solving our
problem. In Section 5 we shall solve the problem of ﬁnding an optimal dictionary
and come up with an explicit solution. Procedures to obtain optimal dictionaries
are also given in Section 5. In Section 6 we consider the special case of ﬁnding an
optimal dictionary to represent samples uniformly distributed over the surface of

OPTIMAL DICTIONARY FOR LEAST SQUARES REPRESENTATION

3

the unit sphere; in particular we demonstrate that the optimal dictionaries that
oﬀer least square representation in such cases are ﬁnite tight frames.

2. Notations

We employ standard notations in this article, a catalog of which is given below:
σpMq : spectrum of mtrix M
trpMq : trace of M

M` : the Moore-Penrose pseudo inverse of M

: the set of real symmetric positive semi-deﬁnite matrices of order n

Rnˆn
`
Rnˆn
`` : the set of real symmetric positive deﬁnite matrices of order n
Snˆn : the set of real symmetric matrices of order n

In : identity matrix of order n ˆ n

ρV p¨q : probability density function of random variable V
Eρr¨s : expectation with respect to the density ρ
Onˆm : a zero matrix of order n ˆ m (the order is not mentioned in obvious cases)

imagepfq : image of a map f

∇f : gradient of function f represented as column

∇xi f : gradient of function f only with respect to variable xi

kxk : ℓ2 norm of the vector x

A Z B : union of ﬁnite sets A and B, where elements of set A are considered

ﬁrst followed by those of B.
For example: A “ t1, 2u and B “ t´5,´7u then A Z B “ t1, 2,´5,´7u.

3. Problem description

Throughout this article we let V denote a random vector taking values in Rn with
distribution µ, let RV denote the support of V , and XV be the smallest subspace
containing RV .

Our goal is to represent the samples of V using a Dictionary of vectors

DK ≔ tvi P Rn| kvik “ 1 for i “ 1, . . . , Ku for some K ě n.

The representation of an instance of the random vector V “ v is done with the
coeﬃcient vector α “ pα1 . . . αKqJ, such that
(2)

αivi .

v “

K

ÿi“1

i“1 α2

The reconstruction of the sample v is carried out by taking the linear combination
řK
i“1 αivi. We consider the cost associated with representing v in coeﬃcients αi’s
as řK
i“1 should be able to represent any
sample of V , it is clear that spantviuK
We denote by DK the set of all feasible dictionaries DK “ tviuK
i“1, where a
dictionary DK “ tviuK

i . Since the dictionary vectors tviuK
i“1 Ą RV .
i“1 is said to be feasible if

‚ kvik “ 1 for all i “ 1, . . . , K, and
‚ spantviuK

i“1 Ą RV .

For a given vector v and a dictionary consisting of a set of vectors vi’s, the linear
equation in (2) is satisﬁed by inﬁnitely many values of αi’s, say for example when
K ą n. In fact the solution space to (2) constitutes a K ´ n dimensional aﬃne

4

M. RAYYAN SHERIFF AND D. CHATTERJEE

ÿi“1

subspace of RK. Therefore in order to represent a given v one has to deﬁne a
mechanism of selecting a particular point from the aﬃne subspace, thus making
the coeﬃcient vector α “ pα1 . . . αKqJ a function of v. Let fpvq ≔ α indicate
the coeﬃcient vector used to represent the sample v. We shall call the map RV Q
v ÞÝÑ fpvq P RK the scheme of representation, then representation of instances of
random vector using a dictionary DK and a scheme fp¨q is said to be proper if any
vector v P RV can be uniquely represented and then reconstructed back.
Claim 1. For proper representation with a dictionary consisting of vectors tviuK
the mapping RV Q v ÞÝÑ fpvq P RK should be an injection that satisﬁes

i“1,

v “`v1

v2

¨¨¨

vK˘ fpvq.

Proof. The linear equation part is straight-forward because by deﬁnition, fpvq “
α “ pα1 . . . αKqJ, and the αi’s satisfy
v “

αivi;

K

therefore

v2

¨¨¨

vK˘ fpvq.

v “`v1

v2

¨¨¨

vK˘ α “`v1

Let u ‰ v P RV and let α, β P RK be coeﬃcient vectors such that fpvq “ α
and fpuq “ β. Suppose α “ β. Then the vectors reconstructed from α and β
are one and the same, which contradicts u ‰ v. Therefore for u ‰ v, we have
fpuq ‰ fpvq.

Let us denote by F , the set of all injective maps f : RV ÝÑ RK. Then we say

a scheme of representation fp¨q to be feasible for representation if

(cid:3)

‚ f P F ,
‚ for some feasible dictionary tviuK
`v1
v2

i“1 P DK the equality
vK˘ fpvq “ v, is satisﬁed for all v P RV .

¨¨¨

With a scheme fp¨q in hand, the random cost associated with representing V
is given by kfpV qk2. Then the problem of ﬁnding an optimal dictionary can be
summarized as the one to ﬁnd a pair consisting of dictionary D˚K P DK, and a
feasible scheme of representation f˚p¨q, such that the average cost of representation
Eµ”kf˚pV qk2ı is minimal (here µ indicates the distribution of random vector V ,

with respect to which the expectation is evaluated). In other words, we have the
following optimization problem.

(3)

minimize

DK , f

subject to $’’&
’’%

Eµ”kfpV qk2ı
DK P DK ,
f P F ,
´v1
v2

¨¨¨

vk¯ fpV q “ V.

From here on the problem given in (3) is referred to as the ℓ2-optimal dictionary
problem. It should be noted that the ℓ2-optimal dictionary problem is a non con-
vex problem because of the constraint that individual vectors tviuK
i“1 of a feasible
dictionary should be of unit length. Even if we change the constraint to tkvik ď 1u
from tkvik “ 1u, which makes the feasible region of dictionary vectors convex, the
problem lies in the fact that the possible set of feasible schemes of representation is
not well characterized. Therefore, the non convex problem (3) will be transformed
to a convex one where, instead of optimizing over the set of unit length vectors we
shall optimize over the set of positive deﬁnite matrices; this transformed problem
is given in (17). The transformation of problem (3) to problem (17) is governed by

OPTIMAL DICTIONARY FOR LEAST SQUARES REPRESENTATION

5

a recently developed method of decomposing positive deﬁnite matrices, called the
rank one decomposition.

The ℓ2-optimal dictionary problem in (3) is solved in two steps: First we consider
XV to be the whole of Rn, ℓ2-optimal dictionaries for this case are characterized by
the assertion given in Theorem 5. Secondly, we consider the general case where XV
can be a subspace of Rn, and an ℓ2-optimal dictionary in such cases is characterized
by the result given in (34).

4. Results related to matrix theory

A standard result from matrix theory [2, page 2] says that a real symmetric
positive semi-deﬁnite matrix M P Rnˆn
` , can be decomposed as Y Y J for some
Y P Rnˆr and r “ rankpMq. Let yi indicate the i th column of the matrix Y . We
can then write

r

More generally for K ě r, let

yiyJi .

M “

ÿi“1

M ≔ˆ M

OJ IK´r˙ ,

O

ÿi“1

where O is a zero matrix of order nˆpK´rq. Now if we consider the decomposition
of M as M “ Y Y J where Y P Rpn`K´rqˆK, and indicate by Y the upper n ˆ K
matrix block of Y , we get M “ Y Y J. In other words
yiyJi , K ě r.

M “

(4)

K

There are numerous ways of decomposing a positive semi-deﬁnite matrix. Some of
them are discussed in [17, Theorem 7.3]. The speciality of a particular decomposi-
tion lies in the characteristics exhibited by the vectors yi’s. Rank one decomposition
as given in [13, Corollary 4 to Proposition 3] is a particular decomposition where
the yi’s in equation (4) also satisfy

yJi yi “

trpMq

K

for all i “ 1, . . . , K.

We shall present our ﬁrst result in the following theorem, from which the rank

one decomposition theorem can be obtained as a consequence.
Theorem 2. For any matrix Λ P Rnˆn, there exists an orthonormal collection of
vectors txiun

i“1 Ă Rn that satisfy
xJi Λxi “

trpΛq
n

for all i “ 1, . . . , n,

and such a collection can be obtained via Algorithm 5.
Proof. First we shall prove that the collection of vectors txiun´1
i“1 contained in Sn´1
(which comes out of the for loop in the Algorithm 5) are orthonormal and satisfy
xJi Λxi “ trpΛqn
For i “ 1, we have P1 “ te1, e2, . . . , enu and sinceřn
m“1 eJmΛem “ trpΛq, vectors
pj, pk P P1 exist such that pJj Λpj ď trpΛqn ď pJk Λpk. We solve for θ in the following
equation

for i “ 1, . . . , n ´ 1. We shall prove this by induction on i.

(5)

gpj ;pkpθq ≔ `p1 ´ θqpj ` θpkqJΛpp1 ´ θqpj ` θpk˘

pp1 ´ θq2 ` θ2q

trpΛq
n

.

“

6

M. RAYYAN SHERIFF AND D. CHATTERJEE

Input: Any matrix Λ.
Output: An orthonormal collection of vectors txiun

xJi Λxi “ trpΛqn

for all i “ 1, . . . , n.

i“1 Ă Rn such that

1 Initialize quantities by S0 “ H, i “ 1.
2 for i from 1 to pn ´ 1q do

S1i “ Si´1 Z te1, e2, . . . , enu.
Pi “ OrthopS1iqzSi´1.
Find pj, pk P Pi such that pJj Λpj ď trpΛqn ď pJk Λpk.
Let Θ P r0, 1s be the solution to the equation
pp1 ´ θqpj ` θpkqJΛpp1 ´ θqpj ` θpkq “ trpΛqn `p1 ´ θq2 ` θ2˘.
Deﬁne xi ≔ p1´Θqpj`Θpk
pp1´Θq2`Θ2q1{2 .
Deﬁne Si ≔ Si´1 Y txiu.
4 S1n “ Sn´1 Z te1, e2, . . . , enu.
5 Output Sn ≔ OrthopS1nq.

3 end for loop

Algorithm 1: Procedure to obtain an orthonormal basis of theorem 1

pp1 ´ θq2 ` θ2q

and for θ “ 1 we have

We know that a solution exists in the range r0, 1s because for θ “ 0 we have
trpΛq
n

gpj ;pkp0q “„pp1 ´ θqpj ` θpkqJΛpp1 ´ θqpj ` θpkq
gpj ;pkp1q “„pp1 ´ θqpj ` θpkqJΛpp1 ´ θqpj ` θpkq

θ“0 “ pJj Λpj ď
θ“1 “ pJk Λpk ě
Since gpj ;pkp¨q is a continuous function of θ, a solution of (5) must exist in the set
r0, 1s. Let Θ be the solution. Then as deﬁned in Algorithm 5 we have

pp1 ´ θq2 ` θ2q

trpΛq
n

,

.

x1 ≔ p1 ´ Θqpj ` Θpk
ap1 ´ Θq2 ` Θ2

,

since pj, pk are elements of P1, they are orthonormal; therefore, we get

kx1k “ bp1 ´ Θq2 kpjk2 ` Θ2 kpkk2

ap1 ´ Θq2 ` Θ2
and since Θ is a solution of equation (5) we have
trpΛq
xJ1 Λx1 “
n

.

“ 1,

Assume that the for some i where 1 ď i ă n ´ 1, the collection Si “ txℓui
orthonormal and satisﬁes

ℓ“1 is

xJℓ Λxℓ “

trpΛq
n

for all ℓ “ 1, . . . , i.

Then,

S1i`1 “ Si Z te1, e2, . . . , enu “ tx1, x2, . . . xi, e1, e2, . . . , enu,

OrthopS1i`1q “ tx1, x2, . . . , xi, p1, p2, . . . , pn´iu,

Pi`1 “ tp1, p2, . . . , pn´iu.

Since the collection txℓui

ℓ“1 is an orthonormal basis for Rn we have

i

ℓ“1 Y tpℓun´i
ÿℓ“1
xJℓ Λxℓ `

n´i

ÿℓ“1

pJℓ Λpℓ “ trpΛq,

OPTIMAL DICTIONARY FOR LEAST SQUARES REPRESENTATION

7

therefore ,

pJℓ Λpℓ “ pn ´ iq

n

trpΛq.

n´i

ÿℓ“1

Thus, there exists vectors pj, pk P Pi`1 such that pJj Λpj ď trpΛqn ď pJk Λpk. Let us
consider the following equation

(6)

gpj ,pkpθq ≔ pp1 ´ θqpj ` θpkqJΛpp1 ´ θqpj ` θpkq

trpΛq
n

.

“

pp1 ´ θq2 ` θ2q

From arguments given in the case of i “ 1, we know that a solution Θ of (6) exists
in the set r0, 1s. Let us deﬁne xi`1 as

xi`1 ≔ p1 ´ Θqpj ` Θpk
ap1 ´ Θq2 ` Θ2

,

since pj, pk are orthogonal to the vectors txℓui
pj, pk. Therefore, xi`1 is orthogonal to the vectors txℓui
fact that
kxi`1k “ bp1 ´ Θq2 kpjk2 ` Θ2 kpkk2

“ 1,

ℓ“1, so is any linear combination of
ℓ“1, which, along with the

makes the collection txℓui`1

ℓ“1 orthonormal. Also, since Θ is a solution to (6), we get

ap1 ´ Θq2 ` Θ2
trpΛq
n

.

xJi`1Λxi`1 “

Therefore by induction on i, we can conclude that the collection txiun´1
in Sn´1 has the required properties.

Finally, in the 4th and 5th steps of Algorithm 5, we have
S1n “ tx1, x2, . . . , xn´1, e1, e2, . . . , enu,
OrthopS1nq “ tx1, x2, . . . , xn´1, xnu.
ℓ“1 is an orthonormal collection; therefore we haveřn

It is obvious that txℓun
trpΛq, which gives

i“1 contained

i“1 xJi Λxi “

(7)

xJn Λxn “

n

n´1

xJi Λxi

ÿi“1
ÿi“1
xJi Λxi ´
“ trpΛq ´ˆ n ´ 1
n ˙ trpΛq
trpΛq
“
n

.

Thus, Algorithm 5 gives out a collection of orthonormal vectors txiun

i“1 such that

xJi Λxi “

trpΛq
n

for all i “ 1, 2, . . . , n.

(cid:3)

Corollary 3 (Rank one decomposition theorem). For any X P Rnˆn
with r ≔ rankpXq, there exists a collection of vectors txiur

i“1 Ă Rn such that

and T P Snˆn

`

r

X “

ÿj“1

xjxJj

and xJi T xi “

1
r

trpXTq

for all i “ 1, . . . , r.

Proof. We know that any real symmetric positive semi-deﬁnite matrix X of rank
r can be decomposed as CCJ where C P Rnˆr. Let us deﬁne Λ P Rrˆr as Λ ≔
CJT C. Then, from the assertion of Theorem 2, a collection of orthonormal vectors
tyiur

i“1 Ă Rr can be obtained such that

8

M. RAYYAN SHERIFF AND D. CHATTERJEE

yJi CJT Cyi “ yJi Λyi “

trpΛq

.

r

r

Let us deﬁne the collection txiur
i“1 as xi ≔ Cyi for i “ 1, . . . , r. Then
xixJi “ C« r
ÿi“1
trpΛq

ÿi“1
and for any i “ 1, . . . , r,

yiyJi ﬀ CJ “ CIrCJ “ X,

xJi T xi “ yJi CJT Cyi “

trpCJT Cq “

trpXTq.

r “

1
r

1
r

(cid:3)

The assertion of Corollary 3 is slightly generalized by the following one and it is
in this form that rank one decomposition is used to solve the ℓ2-optimal dictionary
problem in Theorem 5.
Corollary 4. For any M P Rnˆn
exists a collection of vectors tyiuK
(8)

` , A P Snˆn and K ě r, with r ≔ rankpMq, there

i“1 Ă Rn , such that

K

trpM Aq for all i “ 1, . . . , K.

yjyJj and yJi Ayi “

1
K

M “

ÿj“1

Proof. Let us consider the K ` n ´ r order square matrices X, T in Corollary 3 to
be

O

X ≔ˆ M

OJ IK´r˙ ,

T ≔ˆ A
i“1 Ă Rn`K´r exist such that they sat-
Then rankpXq “ K. Therefore, vectors txiuK
isfy the properties asserted by Corollary 3. Let us denote Rn Q yi “ pxi1 . . . xinqJ
for i “ 1, . . . , K, i.e., yi is the vector formed by the ﬁrst n components of xi. Then

OJ OK´r˙ .

O

K

and for any i “ 1, . . . , K,

yiyJi “ M,

ÿi“1

yJi Ayi “ xJi T xi “

1
K

trpXTq “

1
K

trpM Aq.

(cid:3)

5. Solution to the ℓ2-Optimal Dictionary Problem

Before considering the ℓ2-optimal dictionary problem in its most general setting
given in (3), we will consider the case when XV “ Rn. In this case, a dictionary
i“1 Ă Rn is said to be feasible if and only if kvik “ 1 for all
of vectors DK “ tviuK
i“1 “ Rn. Thus the optimization problem (3) reduces to
i “ 1, . . . , K, and spantviuK
the following :

(9)

minimize

vi, f

subject to

Eµ”kfpV qk2ı
$’’’’&
kvik “ 1 for all i “ 1, . . . , K,
spantviuK
f P F ,
’’’’%
´v1
v2

vk¯ fpV q “ V.

i“1 “ Rn,

¨¨¨

Optimal dictionaries which oﬀer average least squares representation are character-
ized by the following theorem.

Theorem 5. The optimization problem in (9) admits an optimal solution consisting
of

OPTIMAL DICTIONARY FOR LEAST SQUARES REPRESENTATION

9

‚ a dictionary D˚K P DK such that the dictionary vectors ty˚i uK

i“1 satisfy

(10)

K

ÿi“1

y˚i y˚i J “ M˚ “

Σ1{2
V ,

K

tr`Σ1{2
V ˘
y˚K˘` v,

V

y˚2

¨¨¨

‚ a scheme f˚D˚

denotes the real symmetric positive deﬁnite square

Kpvq “`y˚1
where ΣV “ EµrV V Js and Σ1{2
root of ΣV .
Proof. For a given Dictionary DK P DK of vectors tviuK
let us deﬁne a scheme of representation f˚DKp¨q as
vK˘` v.
solves the equation `v1

Then clearly,`v1
Rn, then `v1
Also for u, v P RV and u ‰ v, we have
`v1

f˚DKpvq “`v1
vK˘ f˚DKpvq “ v for any v P Rn. Because if spantviuK
¨¨¨
i“1 “
vK˘` v
vK˘ x “ v.
vK˘`f˚DKpuq ´ f˚DKpvq˘

i“1 which is feasible for (9),

v2
¨¨¨

¨¨¨

¨¨¨

¨¨¨

v2

v2

v2

v2

v2

v2

¨¨¨

vK˘`v1

¨¨¨

vK˘` pu ´ vq

(11)

“`v1
“ u ´ v
‰ 0.

This implies that f˚DKpuq ´ f˚DKpvq ‰ 0 and thus f˚DKp¨q P F .
We know for a fact that f˚DKpvq “ `v1

least squares problem

¨¨¨

v2

vK˘` v is the solution to the

minimize

xPRK

subject to

kxk2

`v1

v2

¨¨¨

vK˘ x “ v
¨¨¨

v2

Therefore, for an arbitrary f P F which satisﬁes `v1
all v P RV , we get

2

(cid:13)

(cid:13)f˚DKpvq(cid:13)

(cid:13)

ď kfpvqk2 for all v P Rn.

Therefore,

(cid:13)

(cid:13)

(cid:13)f˚DKpV q(cid:13)
(cid:13)f˚DKpV q(cid:13)

(cid:13)

2

ď kfpV qk2 ,
2ı ď Eµ”kfpV qk2ı .

and hence

Minimizing over all feasible dictionaries and schemes, we get

Eµ”(cid:13)
(cid:13)f˚DKpV q(cid:13)

(cid:13)

inf

DKPDK

(12)

Eµ”(cid:13)

2ı ď

inf
DK , f

Eµ”kfpV qk2ı
DK P DK,
f P F ,
`v1
v2

¨¨¨

subject to $&
%
2ı
Eµ”(cid:13)
i“1 “ Rn.
spantviuK

(cid:13)f˚DKpV q(cid:13)

(cid:13)

subject to " kvik “ 1 for all i “ 1, . . . , K,

The problem on the left hand side of the inequality (12) is

(13)

minimize

vi

vK˘ fpvq “ v, for

vk˘ fpV q “ V.

From (12) we can conclude that the optimal value (if it exists) to problem in (9)

is bounded below by the optimal value (if it exists) to the one given in (13).

10

M. RAYYAN SHERIFF AND D. CHATTERJEE

We shall show that optimization problem in (13) is solvable, and in fact we shall
come up with a feasible solution to the problem (9) that acheives a value of the
objective function equal to the optimal value of the problem in (13), thereby solving
(9).

v2

be computed as

Let C denote the matrix`v1
Eµ”(cid:13)

(cid:13)f˚DKpV q(cid:13)

(cid:13)

(cid:13)

2ı

(cid:13)C`V (cid:13)

vK˘. The objective function in (13) can

¨¨¨
2ı “ Eµ”(cid:13)
“ Eµ“V JpC`qJC`V‰
“ Eµ”V J`CJpCCJq´1˘J`CJpCCJq´1˘Vı
“ Eµ“V JpCCJq´1CCJpCCJq´1V‰
“ Eµ“V JpCCJq´1V‰ .
2ı “ Eµ“trpV JpCCJq´1V q‰
“ Eµ“trpV V JpCCJq´1q‰
“ tr`Eµ“V V J‰pCCJq´1˘ .

i“1 vivJi . Then the optimization

But V JpCCJq´1V “ trpV JpCCJq´1V q, which shows that

(cid:13)f˚DKpV q(cid:13)

(cid:13)

Eµ”(cid:13)

problem (13) reduces to

Let ΣV ≔ Eµ“V V J‰ and also we have CCJ “ řK
vivJi ¸´1˛
‚

tr¨
˝ΣV ˜ K
ÿi“1

minimize

(14)

vi

subject to #kvik “ 1 for all i “ 1, . . . , K,

spantviuK

i“1 “ Rn.

Let S be the feasible region for the problem in (14). The problem in (14), appears
to be a non convex one. We know that whenever ΣV is a positive deﬁnite matrix,
the function gpMq “ trpΣV M´1q is a convex function on the set of real symmetric
positive deﬁnite matrices, because:

trpΣV M´1q “ tr´Σ1{2

V M´1Σ1{2

V ¯ “ tr´Σ´1{2

V M Σ´1{2

V

.

¯´1

i“1 vivJi

i“1 which is feasible for (14), hpv1, . . . , vKq ≔ řK

From [3, p. 113], we know that inverse of a matrix is a matrix convex function
on the set of positive deﬁnite matrices . Also, we know that for a collection of
tviuK
is a positive deﬁnite
matrix; hence the objective function in (14) is a convex function on imagephq. This
motivates us to consider optimizing over the set of matrices formed by all feasible
collections tviuK
On the one hand, in Corollary 4 if we consider A “ In, then any real symmetric
i“1 yiyJi , with kyik “ b trpMqK
positive deﬁnite matrix M can be decomposed as řK
for all i “ 1, . . . , K. Let us consider a set R Ă Rnˆn
`` , deﬁned by R ≔  M P
Rnˆn
`` | trpMq “ K(. Then for any M P R there exits a decomposition of M as
řK
i“1 yiyJi with kyik “ 1 for all i “ 1, . . . , K. Hence M “ hpy1, . . . , yKq, which
implies

i“1, i.e., on hpSq.

(15)

R Ă hpSq .

OPTIMAL DICTIONARY FOR LEAST SQUARES REPRESENTATION

11

`` and tr`hpy1, . . . , yKq˘ “ řK
hpSq Ă R .

On the other hand, for any collection of vectors tyiuK
řK
i“1 yiyJi P Rnˆn
tion of R,
From (15) and (16), it is clear that hpSq “ R. Then the optimization problem
in (14) can be modiﬁed to the one where optimization is done on the set of positive
deﬁnite matrices. From (14) we obtain the following optimization problem

i“1 P S, hpy1, . . . , yKq “
i“1 yJi yi “ K. Therefore, by deﬁni-

(16)

(17)

minimize
MPRnˆn
subject to

``

tr`ΣV M´1˘
tr pMq ´ K “ 0

The optimization problem in (17) is convex because, the objective function is
convex (as a function of M ) and the feasible region is the intersection of a convex
cone Rnˆn
`` and an aﬃne space tM| tr pMq´ K “ 0u. Hence, we conclude that (17)
can be solved by considering only the ﬁrst order optimality conditions [4, p. 244].
These ﬁrst order optimality conditions are written using the Lagrangian

and the KKT multiplier γ as

LpM, γq ≔ trpM´1ΣV q ` γ ` trpMq ´ K˘,

(18)

0 “ ∇M LpM˚, γq “ ∇M`trpM´1ΣV q ` γ ` trpMq ´ K˘˘ˇˇˇˇM“M˚

“ ´`M´1
`` , therefore MJ
M´1

˚ ΣV M´1
˚ “ M˚ and ΣJV “ ΣV . Consequently,
˚ ΣV M´1

˚ ˘J ` γIn .
˚ “ γIn,

But M˚, ΣV P Rnˆn

leading to

(19)
Clearly since ΣV ‰ Onˆn, we get γ ‰ 0 and M˚ can be written as

ΣV “ γM 2
˚.

To evaluate γ we use the fact that

M˚ “

1
?γ

Σ1{2
V .

which gives

K “ trpM˚q “

tr`Σ1{2
V ˘,

1
?γ
γ “ˆ tr`Σ1{2

V ˘K ˙2

.

M˚ “

tr`Σ1{2
V ˘

In other words, the ﬁnal expression of the optimizer M˚ in the problem (17) is
(20)

K

Σ1{2
V .

The optimal value of the problem in (14) and (17) is ´ptrpΣ

. Therefore, this
value should be a lower bound to the optimal value for the problem in (9)(if it
exits).

V q¯2

1{2

K

i“1 y˚i y˚i J with ky˚i k “ 1
Using Corollary 4, M˚ can be decomposed as M˚ “řK
for each i “ 1, . . . , K. Let us consider the Dictionary D˚K consisting of vectors
i“1. Recall that XV “ Rn, therefore the matrices ΣV , Σ1{2
ty˚i uK
V and M˚ are of rank

12

M. RAYYAN SHERIFF AND D. CHATTERJEE

Let us deﬁne the scheme f˚D˚

n and thus we get spanty˚i uK
that the dictionary D˚K of vectors ty˚i uK
Kpvq “ `y˚1
RV Q v ÞÝÑ f˚D˚

given in (11), it follows that the mapping

i“1 “ Rn. Along with the fact that ky˚i k “ 1, one sees
y˚K˘` v. From arguments

i“1 is feasible for the problem (9).

Kpvq P RK

¨¨¨

y˚2

is feasible for (9). Then the objective function in (9) evaluated for arguments
DK “ D˚K and fp¨q “ f˚D˚
bound for the optimal value of problem (9). Hence the problem (9) is solvable, and
an optimal dictionary and scheme pair is given by,

Kp¨q would equal ´ptrpΣ

. But this value is a lower

V q¯2

1{2

K

(cid:3)

(21)

D˚K “ ty˚i uK
I“1

and f˚p¨q “`y˚1

y˚2

¨¨¨

y˚K˘` p¨q.

We would like to give the following Algorithm which computes the optimal dic-

tionary and scheme; given that the matrix ΣV is known.

Input: The matrix ΣV and a number K ą n.
Output: An ℓ2-optimal dictionary ty˚i uK

i“1 and a scheme f˚p¨q .

1 Compute M1 ≔ K

1{2

Σ1{2
V .

O

O

i“1 Ă RK.

OJ OK´n˙

the collection of vectors txiuK

V ˘
tr`Σ
OJ IK´n˙ , A ≔ˆ In
2 Compute M2 ≔ˆM1
3 Compute C P RKˆK such that M2 “ CCJ.
4 Deﬁne and compute Λ P RKˆK as Λ ≔ CJAC and apply Algorithm 5 to get
5 Deﬁne the collection tviuK
i“1 Ă RK as vi ≔ Cxi for i “ 1, . . . , K.
i“1 Ă Rn such that the jth component of y˚i pjq is
6 Deﬁne the collection ty˚i uK
given by y˚i pjq ≔ vipjq for j “ 1, . . . , n and for every i “ 1, . . . , K.
y˚K˘` pvq .
7 Deﬁne the scheme f˚ : Rn ÝÑ RK by f˚pvq ≔`y˚1
Example 1. We shall consider an example to illustrate the result. Let V “
pV1 V2qJ be a random vector in R2, where V1 and V2 are the component ran-
dom variables. The density functions of random variables V1 and V2 are taken to
be

Algorithm 2: ℓ2-optimal dictionary for the case XV “ Rn.

¨¨¨

y˚2

ρV1pvq “ 2pv ´ 1q1

r1,2spvq,
Then ΣV ≔ EρrV V Js “ˆ17{6 20{9
20{9 11{6˙. With the help of MATLAB, an optimal
dictionary is calculated using the procedure described in Algorithm 2 for the given
value of ΣV and K “ 3. An optimal dictionary was obtained whose consituent
vectors are:

ρV2pvq “ 2p2 ´ vq1

r1,2spvq.

y˚1 “ˆ 0.9789

0.2045 ˙ y˚2 “ˆ 0.6792

0.7339 ˙ y˚3 “ˆ 0.5870

0.0.8096 ˙ ,

with an optimum value of the objective function as 1.8930. This collection of
optimal vectors are marked with crosses on the circumference of unit circle as
shown in ﬁgure 1. An another optimal dictionary tz˚1 , z˚2 , z˚3 u was obtained whose
constituent vectors are marked with circles on the circumference of the circle in the
ﬁgure and the dictionary vectors are.

z˚1 “ˆ 0.4214

0.9069 ˙ z˚2 “ˆ 0.9284

0.3717 ˙ z˚3 “ˆ 0.8513

0.5247 ˙ .

OPTIMAL DICTIONARY FOR LEAST SQUARES REPRESENTATION

13

x2

x1

Figure 1. An example showing optimal dictionaries.

Intuitively it is expected that the optimal dictionary vectors are directed more
towards the bottom right corner of the support (the area with strong shading as
shown in ﬁgure 1). In the optimal solution tz˚i u3
i“1 two vectors tz˚2 , z˚3 u are oriented
towards the region where density of V is concentrated the most. Also for the solu-
i“1, two vectors ty˚2 , y˚3u are oriented towards middle of the distribution
tion ty˚i u3
with the remaining one pointing towards the concentrated region. These results
correlate positively with the expected nature of optimal dictionary.

Henceforth we shall consider the general setting of the ℓ2-optimal dictionary
problem described in (3). Recall that XV is the smallest subspace containing the
support of V (RV ).
In the setting of problem (3), XV need not be the entire
ambient vector space Rn. Let the dimension of XV be m, where m ď n. The
following Lemma is essential in computing an optimal solution to problem (3).

i“1 satisfy vi P XV for all i “ 1, . . . , K.

Lemma 6. The optimal solution (if it exists) to problem (3) is such that the dic-
tionary vectors tviuK
Proof. We shall prove this by contradiction. Suppose that the assertion of the
Lemma is false. Then, if we indicate xi as the projection of vi onto XV and yi as
the projection of vi onto the orthogonal complement of XV , we have kxik ă 1 for
at least one value of i. If fp¨q is the optimal scheme of representation, feasibility of
fp¨q gives, for any v P RV ,

K

(22)

ÿi“1
v “
vifipvq
yifipvqﬀ
xifipvqﬀ `« K
“« K
ÿi“1
ÿi“1
xifipvqﬁ
“»
– ÿti|kxik‰0u
ﬂ ` 0.

Let us deﬁne a Dictionary of vectors v˚i as

v˚i “# xi

x

kxik

for i such that kxik ‰ 0,
otherwise,

where x is some unit length vector in XV . Then clearly

spantv˚i uK

i“1 Ą spantxiuK

i“1 Ą RV and kv˚i k “ 1 for all i “ 1, . . . , K.

14

M. RAYYAN SHERIFF AND D. CHATTERJEE

Therefore the dictionary of vectors tv˚i uK
now deﬁne a scheme f˚p¨q as

i“1 is feasible for the problem (3). Let us

f˚pvq “ diagtkx1k , kx2k , . . . , kxKkufpvq.

Then for any v P RV , using the dictionary consisting of vectors tv˚i uK

i“1, we get

K

ÿi“1

(23)

v˚i f˚i pvq “

v˚i kxik fipvq

K

ÿi“1
“ ÿti|kxik‰0u
“ v,

xi
kxik

kxik fipvq

where the last equality follows from (22). To prove that f˚p¨q is injective, let
u, v P RV . One can write using (22) that
(24)
xi pfipuq ´ fipvqqﬁ
u ´ v “»
– ÿkxik‰0
ﬂ ` 0
“»
ﬂ `»
kxik pfipuq ´ fipvqqﬁ
– ÿkxik‰0
– ÿkxik“0
“« K
ÿi“1

x kxikpfipuq ´ fipvqqﬁ
ﬂ

v˚i pf˚i puq ´ f˚i pvqqﬀ .

Therefore, if u ‰ v, then it follows that f˚puq ‰ f˚pvq. Thus, f˚p¨q is feasible for
problem (3) along with the dictionary of vectors tv˚i uK
i“1. But for any v P RV we
have

xi
kxik

K

K

K

kf˚pvqk2 “

ÿi“1pf˚i pvqq2 “

ÿi“1pfipvqq2 “ kfpvqk2 ,
where the inequality comes from the fact that kxik ă 1 for atleast one value of i.
This contradicts the assumption that the pair tviuK
i“1 along with the scheme fp¨q
is optimal for (3).

kxik2 pfipvqq2 ă

ÿi“1

(cid:3)

Now we shall consider solving the ℓ2-optimal dictionary problem in its general
setting as given in (3). Lemma 6 guarantees that if the problem (3) can be solved,
then the optimal dictionary vectors must take values in XV . This motivates us to
consider optimizing over vectors in XV instead of the whole of Rn. Therefore the
constraint spantviuK
i“1 “ XV .
Let us consider a basis for XV to be B “ txium
i“1, and let B be the matrix
xm˘. If
which contains the basis vectors xi’s as columns, i.e B “ `x1 x2
βi “ pβi1βi2 ¨¨¨ βimqJ is the representation of the dictionary vector vi in basis B,
then the constraints on vi’s get transformed to that on βi’s as

i“1 Ą RV is replaced with spantviuK

¨¨¨

‚ kvik2 “ 1 ñ βJi “BJB‰ βi “ 1, and
i“1 Ą RV ñ spantviuK
‚ spantviuK
Let VX ≔ “pBJBq´1BJ‰ V . Then VX is an Rm valued random vector which
indicates the representation of random vector V in basis B. For every feasible
scheme fp¨q for (3) let us deﬁne an associated scheme fXp¨q : Rm ÝÑ RK which is
used to represent the samples of random vector VX such that

i“1 “ XV ñ spantβiuK

i“1 “ Rm.

fXpVXq ≔ fpBVXq.

OPTIMAL DICTIONARY FOR LEAST SQUARES REPRESENTATION

15

Then the conditions on feasibility of fp¨q in (3) imply that the scheme fXp¨q is
feasible if

‚ Rm Q v ÞÝÑ fXpvq P RK is an injective map,
‚ for all v P Rm

`β1 β2

¨¨¨

βK˘ fXpvq “ v.

In contrast to the problem in (3) where the optimization is done over vectors
in Rn, one can consider the same problem in Rm but with the following modiﬁed
constraints:

minimize
βi, fXp¨q

subject to

(25)

Eµ”kfXpVXqk2ı
$’’’’&
βJi rBJBsβi “ 1 for all i “ 1, . . . , K,
spantβiuK
Rm Q v ÞÝÑ fXpvq P RK is an injective map,
’’’’%
´β1 β2

βK¯ fXpVXq “ VX .

i“1 “ Rm,

¨¨¨

The problem in (25) is similar to that in (9), except for the ﬁrst constraint.
In (9) we optimize over vectors taking values on the surface of the unit sphere,
whereas in (25) we optimize over vectors taking values on the surface of the elipsoid
tx|xJrBJBsx “ 1u.
the problem of (25) to the following one:

Following the arguments given in the proof of Theorem 5 till (14), one can reduce

βi

minimize

tr˜ΣVXˆ K
ÿi“1
i“1 “ Rm,
spantβiuK

βiβJi ˙´1¸
subject to #βJi rBJBsβi “ 1 for all
≔ EµrVX V JX s ““pBJBq´1BJ‰ Eµ“V V J‰“pBJBq´1BJ‰J.

i “ 1, . . . , K,

(26)

where ΣVX

Let us deﬁne

‚ S ≔ The feasible region to the problem in (26),
‚ R “ tH P Rmˆm
‚ a mapping h : RmˆK ÝÑ Rmˆm

`` | trpHrBJBsq “ Ku,

`

From Corollary 4 we see that for every H P R there exists a collection of vectors
tβiuK

i“1 such that

as hpβ1, β2, . . . , βKq ≔řK

i“1 βiβJi .

K

ÿi“1

βiβJi “ H, and
trpH“BJB‰q

K

βJi “BJB‰ βi “

“ 1,
which, along with the fact that rankpHq “ m ñ spantβiuK
(27)
Moreover, for any collection tβiuK

R Ă hpSq.

i“1 P S, we have
trˆhpβ1, β2, . . . , βKqrBJBs˙ “
ÿi“1
hpβ1, β2, . . . , βKq P Rmˆm
`` ,

K

βJi rBJBsβi “ K,

i“1 “ Rm imply that

16

M. RAYYAN SHERIFF AND D. CHATTERJEE

which implies

hpSq Ă R.
(28)
From (27) and (28) we infer that R “ hpSq.
In other words, in (26) instead of
optimizing over the feasible collection of vectors in S one can opt for considering
optimizing over the set of real symmetric positive deﬁnite matrices in R. This leads
us to the following problem:

(29)

minimize
HPRmˆm
subject to

``

tr`ΣVX H´1˘
tr`HrBJBs˘ ´ K “ 0.

Letting M ≔ rBJBs1{2HrBJBs1{2, one can write the optimization problem (29)
with M as the variable instead of H. due to this, the constraint and the objective
function change as

tr`HrBJBs˘ “ tr´rBJBs1{2HrBJBs1{2¯ “ trpMq, and
tr´ΣVX H´1¯ “ tr´ΣVXrBJBs1{2M´1rBJBs1{2¯
“ tr´rBJBs1{2ΣVXrBJBs1{2M´1¯
“ tr´ΣM´1¯,

Σ ≔ rBJBs1{2ΣVXrBJBs1{2
“ rBJBs1{2“pBJBq´1BJ‰ Eµ“V V J‰“pBJBq´1BJ‰J rBJBs1{2
“ rBJBs´1{2“BJΣV B‰rBJBs´1{2.

Using (30), one can modify the problem (29) as the following one with optimization
variable M :

(32)

minimize
MPRnˆn
subject to

``

tr´ΣM´1¯
tr pMq ´ K “ 0.

The problem in (32) is exactly the same as (17), which leads us to conclude that
the problem (32) is solvable, and the optimal solution is

Therefore, the problem in (29) is solvable, and the optimal solution is

M˚ “

Σ1{2.

K

tr`Σ1{2˘

H˚ ≔ rBJBs´1{2M˚rBJBs´1{2

(33)

“

K

tr`Σ1{2˘”rBJBs´1{2Σ1{2rBJBs´1{2ı .

From Corollary 4 there exists a collection of vectors tβ˚i uK

i“1 such that

(30)

where

(31)

K

ÿi“1

β˚i β˚i J “ H˚, and
trpH˚“BJB‰q

K

“ 1.

β˚i J“BJB‰ β˚i “
‚ the collection of vectors tβ˚i uK
i“1,

that the pair

Following the arguments given in the proof of theorem 5, it can be concluded

OPTIMAL DICTIONARY FOR LEAST SQUARES REPRESENTATION

17

“`β˚1

¨¨¨

β˚K˘`“pBJBq´1BJ‰ v.

β˚2

¨¨¨

β˚K˘` v,
‚ the scheme f˚Xpvq “`β˚1
is optimal for the problem (25). Using the optimal solution of (25), let us deﬁne a
dictionary and a representation-scheme pair as:
y˚i “ Bβ˚i
for i “ 1, . . . , K,
f˚pvq “ f˚X`“pBJBq´1BJ‰ v˘

(34)

β˚2

.

K

It is quite clear that the solution in (34) is feasible for the optimization problem
(3). The feasibility argument and the assertion of Lemma 6 leads us to conclude
that the problem (3) is solvable, and in fact the optimal solution is given by (34)
with an optimal value of pptrpΣ1{2qq2
Remark 7. ℓ2-optimal dictionaries which are optimal in representing samples of
a random vector V are also optimal for representing samples of any scalar multiple
of V , i.e. αV for any 0 ‰ α P R.

In order to validate the remark let us consider (33), then it is easy to see that H˚
deﬁned in (33) is invariant under nonzero scalar multipliction of V . Therefore op-
timal dictionaries are also invariant under nonzero scalar multiplication of random
vector V .

Similarly to the case discussed when XV “ Rn, we shall give an algorithm to
obtain an optimal dictionary and the scheme for the general ℓ2-optimal dictionary
problem given in (3). In the following algorithm we assume that the matrix ΣV is
known, or that it can be computed. The information needed to describe XV is given
by a matrix B P Rnˆm, which contains the basis vectors for XV as its columns.

O

2 Compute H ≔ K

i“1 and a scheme f˚p¨q .

Input: The matrices ΣV , B and a number K ě dimpXV q.
Output: An ℓ2-optimal dictionary ty˚i uK
1 Compute Σ ≔ rBJBs´1{2“BJΣV B‰rBJBs´1{2.
tr`Σ1{2˘“rBJBs´1{2Σ1{2rBJBs´1{2‰.
OJ IK´m˙ , A ≔ˆrBJBs
3 Compute M ≔ˆ H
OJ
4 Compute C P RKˆK such that M “ CCJ.
5 Deﬁne and compute Λ P RKˆK as Λ ≔ CJAC and apply Algorithm 5 to get
6 Deﬁne the collection tviuK
7 Deﬁne the collection tβ˚i uK
given by β˚i pjq ≔ vipjq for j “ 1, . . . , m and for every i “ 1, . . . , K.
i“1 Ă Rn as y˚i
8 Deﬁne the Optimal Dictionary ty˚i uK
9 Deﬁne the scheme f˚ : Rn ÝÑ RK by f˚pvq ≔`y˚1

i“1 Ă RK as vi ≔ Cxi for i “ 1, . . . , K.
i“1 Ă Rm such that the jth component of β˚i pjq is
for i “ 1, . . . , K.
y˚K˘` pvq .
¨¨¨
Algorithm 3: The general ℓ2-optimal dictionary.

the collection of vectors txiuK

OK´m˙

i“1 Ă RK.

≔ Bβ˚i

y˚2

O

6. Uniform Distribution of V

One of the most important and practically relevant case is that of the uniform
distribution of the random vetor V over the unit sphere. Due to the symmetry of
distribution exhibited by an uniformly distributed random vector, its easy to see
that any rotation of optimal dictionary is also optimal. Here rotation of a dictionary
is meant by rotation of each constituent vector of the dictionary by a common angle
and about a common axis.

18

M. RAYYAN SHERIFF AND D. CHATTERJEE

In order to motivate the solution, let us consider a dictionary consisting of vectors
which are close by, i.e. the inner product between any pair of vectors is close to one.
It is quite intuitive to conclude that such a dictionary is not optimal for representing
uniformly distributed samples due to the obvious fact that sample vectors which are
almost orthogonal to the dictionary vectors carry equal priority but require large
coeﬃcients for representation. Therefore it is more prudent to consider a dictionary
in which vectors are maximally spaced out. Several examples of such collection of
vectors which are maximally spaced out are given in [1, Section 4]. The collection
of vectors which are maximally spaced are shown to attain equilibrium under the
action of diﬀerent kinds of forces deﬁned and explained in [1, Section 4] and also
in [11, p. 6]. Such collection of vectors are generalized by the concept called Finite
Normalized Tight Frames (FNTF) as explained in [1]. A good introduction to ﬁnite
tight frames is given in [5], [1] and [18].

Lemma 8. A dictionary DK “ tviuK
i“1 is optimal in representing samples of ran-
dom vector V which is uniformly distributed over the surface of unit sphere if and
only if the dictionary vectors form a K
n -FNTF.

Proof. When V is uniformly distributed over the unit sphere, we have ΣV “ 1
Then the collection of vectors tviuK

n In.

(35)

K

ÿi“1

vivJi “

i“1 is an optimal dictionary if and only if
trp 1?n Inqˆ 1
?n

In˙ “

K
n

In.

K

For ﬁnite dimensional spaces, the linear map deﬁned by the frame operator

S : Rn ÞÝÑ Rn [1, Section 2] for a tight frame tviuK
ÿi“1xy, viyvi “˜ K
ÿi“1

Spyq “

K

i“1 is given by
vivJi ¸ y.

From [1, Theorem 3.1], we conclude that a collection of vectors tviuK
frame in Rn if and only if the collection is a K
2.1], a collection of vectors tviuK

n -FNTF if and only if

i“1 forms a tight
n -FNTF. Again from [1, Theorem

(36)

i“1 is a K
ÿi“1

S “

K

vivJi “

K
n

In.

From (35) and (36) we conclude the assertion of the Lemma.

(cid:3)

7. Conclusion and future work

In this article we have given an explicit solution to the ℓ2-optimal dictionary
problem in the form of rank one decomposition of a positive deﬁnite matrix, and al-
gorithms to compute the ℓ2-optimal dictionaries are also provided. A result related
to matrix theory is also provided in Theorem 2 from which the rank one decompo-
sition of a positive semi deﬁnite matrices can be obtained as a consequence. The
natural extension of Theorem 2 is to check for possible extension of it in the setting
of separable Hilbert spaces of inﬁnite dimensions.

The entire analysis provided in this article assumes that we know the distribution
of the random vector whose samples are to be represented. An online algorithm
which in parallel estimates the distribution of the random vector and computes the
dictionary vectors can be developed, and will be reported in subsequent articles.

OPTIMAL DICTIONARY FOR LEAST SQUARES REPRESENTATION

19

References

[1] J.J. Benedetto and M. Fickus. Finite normalized tight frames. Advances in Computational

Mathematics, 18(2-4):357–385, 2003.

[2] R. Bhatia. Positive deﬁnite Matrices. Princeton University Press, 2009.
[3] R. Bhatia. Matrix Analysis, volume 169. Springer Science & Business Media, 2013.
[4] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge university press, 2004.
[5] I. Daubechies, A. Grossmann, and Y. Meyer. Painless nonorthogonal expansions. Journal of

Mathematical Physics, 27(5):1271–1283, 1986.

[6] K. K. Delgado, J.F. Murray, B.D. Rao, K. Engan, T. Lee, and T.J. Sejnowski. Dictionary

learning algorithms for sparse representation. Neural computation, 15(2):349–396, 2003.

[7] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online dictionary learning for sparse coding.
In Proceedings of the 26th Annual International Conference on Machine Learning, pages
689–696. ACM, 2009.

[8] J. Mairal, J. Ponce, G. Sapiro, A. Zisserman, and F.R. Bach. Supervised dictionary learning.

In Advances in neural information processing systems, pages 1033–1040, 2009.

[9] S.G. Mallat and Z. Zhang. Matching pursuits with time-frequency dictionaries. IEEE Trans-

actions on Signal Processing, 41(12):3397–3415, 1993.

[10] B.A. Olshausen and D.J. Field. Sparse coding with an overcomplete basis set: A strategy

employed by v1? Vision research, 37(23):3311–3325, 1997.

[11] E.B. Saﬀ and A. Kuijlaars. Distributing many points on a sphere. The mathematical intelli-

gencer, 19(1):5–11, 1997.

[12] K. Skretting and K. Engan. Recursive least squares dictionary learning algorithm. IEEE

Transactions on Signal Processing, 58(4):2121–2130, 2010.

[13] J.F. Sturm and S. Zhang. On cones of nonnegative quadratic functions. Mathematics of

Operations Research, 28(2):246–267, 2003.

[14] I. Tošić and P. Frossard. Dictionary learning. Signal Processing Magazine, IEEE, 28(2):27–38,

2011.

[15] M. Yaghoobi, T. Blumensath, and M.E. Davies. Dictionary learning for sparse approximations
with the majorization method. IEEE Transactions on Signal Processing, 57(6):2178–2191,
2009.

[16] M. Yang, L. Zhang, X. Feng, and D. Zhang. Fisher discrimination dictionary learning for
sparse representation. In IEEE International Conference on Computer Vision (ICCV), 2011,
pages 543–550. IEEE, 2011.

[17] F. Zhang. Matrix Theory: Basic results and techniques. Springer Science & Business Media,

2011.

[18] G. Zimmermann. Normalized tight frames in ﬁnite dimensions. In Recent Progress in Multi-

variate Approximation, pages 249–252. Springer, 2001.

E-mail address: mohammedrayyan@sc.iitb.ac.in, dchatter@iitb.ac.in

Systems & Control Engineering, Indian Institute of Technology Bombay, Powai,

Mumbai 400076, India

