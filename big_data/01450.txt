6
1
0
2

 
r
a

M
4

 

 
 
]
S
D
.
s
c
[
 
 

1
v
0
5
4
1
0

.

3
0
6
1
:
v
i
X
r
a

Sequential ranking under random semi-bandit

feedback

Hossein Vahabi , Paul Lagr´ee , Claire Vernade , Olivier Capp´e

March 7, 2016

Abstract

In many web applications, a recommendation is not a single item sug-
gested to a user but a list of possibly interesting contents that may be
ranked in some contexts. The combinatorial bandit problem has been
studied quite extensively these last two years and many theoretical re-
sults now exist :
lower bounds on the regret or asymptotically optimal
algorithms. However, because of the variety of situations that can be
considered, results are designed to solve the problem for a speciﬁc reward
structure such as the Cascade Model. The present work focuses on the
problem of ranking items when the user is allowed to click on several items
while scanning the list from top to bottom.

1 Introduction

In the context of online learning, the classical bandit framework refers to situa-
tions for which the learner is only allowed to observe the rewards of the action
he took. This is actually to be compared to the full information framework in
which after choosing one action among a pool of candidates, the learner observes
the result of every action, including the non-chosen ones. The canonical appli-
cation of online learning with bandit feedback is the case of online advertising.
Suppose each time a user lands on a webpage, she is shown an ad among K in
the current catalog. The goal of the ad displayer is to select the ad that has the
highest probability of leading to a click, that is to maximize its click-through
rate (CTR). Each time an ad is tried out, we can only record click/non-click
on that ad and update our knowledge on its CTR but we cannot say anything
about the other possible ads.

The same kind of process can be imagined in the context of recommendation.
However, recommending items rarely boils down to showing a single one. In
general, the user is presented a set of recommendations and she can click on
many of them, giving larger feedback to the learner. Just as in the Cascade
Model that we describe in section 3, we suppose that the user scans the list
from the top but we assume that she has a limited patience and suddenly stops
rating and leaves the system independently of what she was looking at. In that

1

case, we are not strictly observing a bandit feedback: one multiple-action is
made and we receive each individual reward instead of a unique response that
would stand for the set of chosen items. We are not either in the case of full
information since rewards corresponding to non-chosen options are not revealed.
This framework is often called the semi-bandit feedback.

In this paper, we consider the following multiple recommendation scenario.
A list of items is chosen by the learner. The user then scans the list sequentially:
when observing an item, she can either rate it (0 or 1) or leave and stop rating
the items in the list. Deﬁned this way, the problem is not to ﬁnd an optimal set
of items but rather to ﬁnd the best ordered list of items among K options.

We start by describing the bandit model we consider and especially the novel
feedback structure that allows us to tackle problems of recommendation rather
than information search. A large overview of the literature is provided next that
aims at localizing the present work in a ﬂourishing literature on combinatorial
bandit problems. Technical sections come after: we present and show a lower
bound for the bandit problem in our setting and then suggest eﬃcient algorithms
to solve the proposed problem.

2 Model

We consider the binary bandit problem with K arms. The parameters of the
arms are the expectation of a Bernoulli distribution which lies in Θ = [0, 1]K.
A bandit model is then a tuple θ = (θ1, θ2, . . . , θK). Without loss of generality,
we shall always suppose that θ1 > . . . > θK. At each round t, the learner selects
a list of L elements chosen among the K arms, which we materialize by a list
of indices belonging to {1, . . . , K}. The set of those lists is denoted by A and
contains L!/(K − L)! elements; the list chosen at time t will be denoted At.
The user scans the list from the top to the end and gives a feedback for each
observed item until she chooses to stop scanning, which occurs independently of
previous recommendations. Let (Λt) be a sequence of i.i.d. random variables in
{1, . . . , L} corresponding to the number of observed items at each round. Thus,
l=1 Xt,l at round t,
where Xt,l is an independent draw with probability distribution B(θAt(l)). The
probability of scanning the item in position l ∈ {1, . . . , L} is modeled by κl,
that is κl = P(Λ ≥ l). We suppose that the learner knows when the user stops
rating so that no ambiguity is left on the last unrated items. The items without
feedback are unobserved and nothing can be inferred concerning the rewards
that could have been earned.

playing action At, the learner receives a reward rAt(t) =(cid:80)Λt

Clearly, the optimal list of L arms is a∗ = (1, 2, . . . , L), and the regret of the

2

learner up to time T writes

ra∗ − rAt

t=1

T(cid:88)
T(cid:88)
T(cid:88)

t=1

L(cid:88)
(cid:88)

l=1

R(T ) =

Eθ[R(T )] =

=

κl(θa∗(l) − θAt(l))

(cid:32) L(cid:88)

(cid:33)

t=1

a∈A

l=1

κl(θl − θa(l))

1{At = a}

where the expectation is over the probability distribution of the bandit model.
Taking the expectation of the above quantity with respect to Monte-Carlo rep-
etitions, we obtain the expected regret

(cid:88)
Let us introduce µa = (cid:80)L

E[R(T )] =

κl(θl − θa(l))

E[Na(T )]

l=1

a∈A
l=1 κlθa(l) the reward expectation of arm a ∈ A.

Denoting µa∗ by µ∗, the expected regret can be rewritten

E[R(T )] =

3 Related Work

(µ∗

− µa)E[Na(T )]

(cid:33)

(cid:32) L(cid:88)

(cid:88)

a∈A

The closest work to ours is [7] that consider the problem of learning to rank items
in a stochastic setting with semi-bandit feedback. The main diﬀerence with our
work lies in the reward structure: in [7], the authors rely on the Cascade Model.
This model comes from the Information Retrieval (IR) literature and suggests
that the users of a search engine scan the proposed list from top to bottom and
click on the ﬁrst interesting item. In order to induce the necessity of ranking
items, weights are sometimes attributed to positions in the chosen list: a click
on position i ∈ {1, . . . , L} gives reward r(i) > r(i + 1). Thus, order matters and
the algorithm must be able to rank items according to their CTR in order to
maximize reward. The authors suggest an algorithm based on KL-UCB ([10])
and prove a lower bound on the regret as well as an asymptotically optimal
upper bound. Their ideas mainly come from very related previous works from
same co-authors such as [5] and [8] as well as on the general analysis of lower
bound for Markov Decision Processes [11] done by Graves & Lai in 1997.

On the contrary, recent works on the Cascade Model ([14],[15]) do not solve
the ranking problem.They are still closely linked to this work because of the
randomized size of the semi-bandit feedback they consider. In [14], the authors
suggest and study a UCB-based algorithm for building such lists of items in
order to maximize the number of clicks : the reward is 1 if the user clicks on
at least one item so the best multiple arm to play is the set of the L-best arms

3

where L is the size of the list. Concretely, they compare to the optimal strategy
that chooses the L best arms – no matter how ordered – and prove that they
obtain a O(log T ) regret. The model studied in [15] is slightly diﬀerent as they
need to ﬁnd lists for the reward of each item is 1. They also provide an algorithm
to solve their problem and show a regret upper bound in O(log T )

More generally, multiple-plays bandits – for which the agent draws a super-
action at each round – have received a lot of attention recently both in the
adversarial and in the stochastic setting.
In the adversarial setting, primary
work was presented in [18] for semi-bandit feedback and in [3] for more general
bandit feedback. In the stochastic setting, an early work was done in order to
propose algorithms for the multi-user channel allocation problem in cognitive
radio ([9]). Then, more eﬃcient algorithms were suggested successively in [4]
and in the very recent work [6].

Even if the literature on combinatorial bandits seems to be quite recent,
the simpler problem that consists in ﬁnding the L−best arms with semi-bandit
feedback was already studied in 1987 by Anantharam et al. ([1]) who provided
a lower-bound for this problem as well as an asymptotically optimal algorithm.
Their contribution can be seen as the equivalent of the work of Lai & Robbins
([16]) for the classical multi-armed bandit with single play. However, the al-
gorithm they proposed was not computationally eﬃcient and the recent work
by Komiyama et al. ([13]) suggest and analyze an eﬃcient algorithm based on
Thompson Sampling ([17]). They prove that Thompson Sampling is optimal for
the problem of ﬁnding the best items when the order does not matter. Whether
Thompson Sampling is optimal also for ranking items remains an open problem.

4 Lower Bound on the Regret

In this section, we state and prove a theorem giving a lower bound on the regret
of a uniformly eﬃcient algorithm for the sequential MP-MAB problem. The
proof builds on ideas from ([16, 1, 12, 7]) .

4.1 Deﬁnitions and Notations

We recall that the set of arms – that is the lists of L elements chosen without

replacement among K – is denoted by A. We also denote A∗ the set of actions
that contain at least one arm k /∈ a∗(θ), or equivalently that is not a permutation
of the arms of a∗(θ). The Kullback-Leibler divergence from p to q is denoted by
KL(p, q) while d(x, y) := x log(x/y) + (1 − x) log((1 − x)/(1 − y)) is the binary
relative entropy.

Deﬁnition 1. A class M of bandit models is identiﬁable if it is of the form
M = P1 × . . .×PK, where Pa is the set of possible distributions for arm a, and
if for all a, Pa is such that
∀p, q ∈ Pa

p (cid:54)= q =⇒ 0 < KL(p, q) < ∞

(1)

4

Deﬁnition 2. An algorithm A is said uniformly eﬃcient if for all bandit model
ν and for all α ∈]0, 1], the expected regret of algorithm A after T rounds is such
that R(T,A) = o(T α). Then, for the problem of ranking, the unique optimal
arm is a∗ = (1, 2, . . . , L) and any uniformly eﬃcient algorithm must satisfy

T − E[Na∗ (T )] = o(T )

E[Na(T )] = o(T ) , ∀a (cid:54)= a∗

Deﬁnition 3. For all bandit model θ ∈ Θ, we deﬁne an interesting set of
changes of measure:

B(θ) = {λ ∈ Θ| ∀k ∈ a∗(θ), θk = λk

and µ∗(θ) < µ∗(λ)} .

For each suboptimal arm k, a subset of B(θ) will be used:

Bk(θ) = {λ ∈ Θ| ∀j (cid:54)= k, θj = λj

and µ∗(θ) < µ∗(λ)} .

Notice that this last deﬁnition implies that λk > θL.

(2)

(3)

4.2 Related works on lower-bounds for MAB problems
For a classical MAB problem ν = (ν1, . . . , νK) with one only optimal arm ν∗
with mean µ∗ and one observation at each step, Lai & Robbins gave a lower-
bound on the regret that comes from a lower bound on the number of suboptimal
draws

µa < µ∗ =⇒ lim inf
T→∞

Eν[Na(T )]

log(T ) ≥

1

KL(νa, ν∗)

.

This bound was later generalized by [2] to families that depend on multiple
parameters. Closer to our work, [1] consider a multiple play bandit problem in
the semi-bandit setting that consists in drawing L diﬀerent arms at each round.
They give a lower bound on the regret that is based on a lower-bound on the
number of pulls of the worst arms : for each worst arm j – i.e. that does not
belong to the L best ones –,

lim inf
T→∞

Eν[Nj(T )]
log(T ) ≥

1

KL(νj, νL)

.

This last bound is closely related to our problem even if it seems not to carry all
the complexity of the ranking aspect related to sequential rewards. Surprisingly,
the lower-bound of [7] as well as ours does not show any additional term that
would stand for the complexity of ranking the optimal arms. In fact, for the
”learning to rank” problem in [7] where rewards follow the weighted Cascade
Model with convex decreasing weights (r(l))i=1,...,L on the positions, they obtain
the following lower-bound :

K(cid:88)

k=L+1

θL − θk
KL(θk, θL)

.

lim inf
T→∞

R(T )
log T ≥ r(L)

5

The end of the section is dedicated to the detailed proof of the lower-bound on
the regret of our model for sequences of feedback whose length do not depend on
the actual items proposed in the list. The proof uses ideas from [7] and [6] but
is not strictly built on the same architecture and relies on generic lower-bound
results from [12].

4.3 Lower-bound on the sequential MP-MAB problem

The main result of this section is stated in the following theorem.

Theorem 4. Simplifying the bound of Theorem 5, the regret of any uniformly
eﬃcient algorithm can be lower-bounded by

lim inf
T→∞

E[R(T )]
log T ≥

θL − θk
KL(θk, θL)

.

K(cid:88)

k=L+1

This theorem is actually a more explicit version of the very generic result on

lower-bounds given by Graves & Lai ([11]) for our own bandit model.

Theorem 5. The regret of any uniformly eﬃcient algorithm is lower bounded
as follows

where

c(θ) = inf
c(cid:23)0

lim inf
T→∞

E[R(T )]
log T ≥ c(θ).

(cid:88)

a∈A

da(θ)ca

(cid:88)

a∈A

s.t

inf

λ∈B(θ)

Ka(θ, λ)ca ≥ 1

This result comes from a more general work on controlled Markov chains and
is pointed out by [6] as a very powerful tool to obtain lower-bounds for bandit
problem. Interestingly, it is possible to provide an original proof of this theorem
using results from [12]. The following proposition allows us to prove Theorem 5
and for completeness we give a full proof of it in Appendix. The main problem-
dependent step of the proof relies in the computation of the expectation of the
log-likelihood ratio of the observations. The result of it provides the numerator
of the following expression.

Proposition 6. For any uniformly eﬃcient algorithm

∀λ ∈ B(θ),

where Ka(θ, λ) =(cid:80)L

lim inf
T→∞

l=1 κl KL(θa(l), λa(l)).

a∈A Ka(θ, λ)E[Na(T )]

log(T )

≥ 1

(cid:80)

6

Proof. (Theorem 5) In order to prove the lower bound on the regret, one needs
to show that the uniform eﬃciency of any algorithm requires a certain amount of
suboptimal pulls that avoid any confusion with the optimal option. We proceed
in two steps. First, we rewrite the regret lower bounding some of the numerous
terms in order to get a more simple expression. Then, we lower bound each of
the remaining terms using changes of measure arguments.

Step 1: A∗ denotes the set of actions containing at least one suboptimal
∗, the regret can be written as

arm. Suppressing the terms due to actions in A
follows

(cid:33)

(cid:32) L(cid:88)

l=1

a∈A

(cid:88)
(cid:88)
(cid:88)

a∈A

a∈A

E[R(T )] =

=

=

κl(θa∗(l) − θa(l))

(cid:32) L(cid:88)

E[Na(T )]

(cid:33)

E[Na(T )]

l=1
da(θ)E[Na(T )].

κl(θl − θa(l))

In the last line, we denote the positive regret associated to each suboptimal

Step 2: We can use the previous decomposition to lower bound the regret

action da(θ) :=(cid:80)L

l=1 κl(θl − θa(l)).

as follows

lim inf
T→∞

E[R(T )]
log T ≥

(cid:88)

a∈A

da(θ) lim inf
T→∞

E[Na(T )]

log(T )

.

According to Proposition 6, for all λ ∈ B(θ),

Ka(θ, λ) lim inf
T→∞

E[Na(T )]
log(T ) ≥ 1.

(cid:88)

a∈A

This result means that the regret is actually lower bounded by a sum of non-
negative quantities that satisfy the above constraint provided by Proposition
6. The solution of the optimization problem stated in the theorem is then
eﬀectively a lower bound of this sum.

4.4 Simpliﬁed lower-bound : proof of Theorem 4

Following the ideas of [5], we can simplify the lower-bound of Theorem 5 in
order to obtain the more explicit version of Theorem 4.

It can be noticed that this lower-bound is actually the same as the one of [1]
even if the combinatorial structure of the problem seems more complex here. Of
course, we do not show that this lower-bound is in fact the optimal one as long as
we do not provide an algorithm whose regret is asymptotically upper-bounded
by the same quantity. Developments and experiments of the next Sections show
that it can be attained in practice and thus that it is eventually the optimal
lower-bound for our problem.

7

Proof. The detailed proof is provided in Appendix. We only give the sketch of
our analysis here.

The ﬁrst argument of the proof is that when we release constraints, we
actually widen the feasible set and this allows us to possibly reach a lower-bound
of the wanted result. Concretely, instead of considering all possible changes of

measure in the constraint equation, we only allow λ to lie in Bk(θ) for k /∈ a∗(θ).

We obtain the following relaxed optimization problem :

(cid:88)

c(θ) = inf
c(cid:23)0

a(cid:54)=a∗(θ)

da(θ)ca

s.t ∀k /∈ a∗(θ), ∀λ ∈ Bk(θ),

(cid:88)

a∈A

Ka(θ, λ)ca ≥ 1.

(4)

(5)

In the above problem, the constraints are still not really explicit since each

of them must be valid for all λ ∈ Bk(θ) even in the worst case : for k /∈ a∗(θ),

one must satisfy

(cid:88)

a∈A

inf

λ∈Bk(θ)

Ka(θ, λ)ca ≥ 1.

Fortunately, computing this inﬁmum is quite easy on the given sets Bk(θ) and
returns an explicit optimization problem

It remains to rewrite the argument using Lemma 9 to obtain

c(θ) = inf
c(cid:23)0

da(θ)ca

(cid:88)
L(cid:88)
(cid:88)
s.t ∀k /∈ a∗(θ),

a(cid:54)=a∗(θ)

ca

a(cid:54)=a∗(θ)

l=1

da(θ)ca

a(cid:54)=a∗(θ)

(cid:88)
(cid:88)
(cid:88)

k /∈a∗(θ)

k /∈a∗(θ)

≥

≥

(θL − θk)
θL − θk
KL(θk, θL)

1{a(l) = k}κl KL(θk, θL) ≥ 1.

(cid:88)

a(cid:54)=a∗(θ)

L(cid:88)

l=1

ca

κl1{a(l) = k}

5 Algorithm

In this section, we propose extensions of Thompson sampling, UCB and KL-
UCB to the semi-bandit feedback problem. The TS-like version (RSF-TS) is
given in Algorithm .

8

Algorithm 1 Random semi-bandit feedback Thompson sampling (RSF-TS)
Require: number of arms K, number of positions L

for k = 1, . . . , K do

αk, βk ← 1, 1

end for
for t = 1, . . . , N do

for k = 1, . . . , K do

θk(t) ∼ Beta(αk, βk)

end for
At = top-L arms ordered by decreasing θk(t)
for l = 1, . . . , Λt do

if Xt,At(l) = 1 then
αAt(l) ← αAt(l) + 1
βAt(l) ← βAt(l) + 1

else

end if
end for

end for

Each of these algorithms reuses upper-conﬁdence based indexes and choses
the action by pulling the arms with the best indices in decreasing order. Con-
cretely, we set

• in RSF-UCB, for k ∈ 1, ..., K, uk(t) = ˆµi(t) +(cid:112)(2 log t)/(3Ni(t));

• in RSF-KL-UCB, for k ∈ 1, ..., K, uk(t) = supq∈[ˆµi(t),1]{q|Ni(t)d(ˆµi(t), q) ≤

log t} as proposed in [10];

• and in RSF-TS, for k ∈ 1, ..., K, uk(t) ∼ πk, t is a sample from the
posterior distribution over the mean of arm k. Recall that here only
Bernoulli bandits are considered so that the posterior pk, t is always a
Beta distribution.

6 Experiments

In this section we evaluate the empirical properties of RSF-TS and its variants
in several scenarios. Our performance are compared to those obtained by the
PIE(L) algorithm of [7]. 1.

6.1 On the optimality of PIE(L) for a multiple-click model

6.2 Synthetic experiment

We consider a simple scenario with 6 Bernoulli distributions with respective
In this
expected rewards 0.6, 0.58, 0.55, 0.45, 0.45 and 0.4. We ﬁx L = 3.

1The source code of the simulations is available at https://github.com/plagree/random-

semi-bandit.

9

section, results are averaged over 10K runs for a number of rounds T = 105.

Inﬂuence of the vector κ on the regret First, we evaluate the eﬀect of
the probabilities of observation κ when setting various values. The simulation
results are shown in Figure 1. Unsurprisingly, the graph comfort the lower bound
regret analysis. The diﬀerent values for κ have no inﬂuence on the asymptotic
behavior. Low values for κ2 and κ3 provoke a delay on the round when the
algorithm regret and the lower bound become parallel.

Figure 1:
of the regret with respect to the theoretic lower-bound.

Impact of κ. the results in log-scale allowing to compare the growing rate

Figure 2 shows the eﬀectiveness of several algorithms. The PIE(L) algorithm
used in our framework seems to have a quite high regret that asymptotically
shows an optimal behavior with respect to the proven lower-bound.

Figure 2: Regret of RSF-TS, RSF-KL-UCB and RSF-UCB.

10

100101102103104105106Roundt050100150200250300RegretR(T)κ=(1,0.75,0.2)κ=(1,0.2,0.1)κ=(1,1,1)κ=(1,0.9,0.8)LowerBound101102103104105106Roundt050100150200250300RegretR(T)RSF-PIERSF-KL-UCBRSF-TSLowerBound6.3 Search advertising

We used a publicly available dataset provided for KDD Cup 2012 track 2 2. The
dataset involves session logs of soso.com, a search engine owned by Tencent. It
consists in ads that were inserted among the results of the search engine. Each
of the 150M lines from the log contains the user, the query she typed, an ad,
a position (1, 2 or 3) at which it was displayed and a binary reward (click/no-
click). First, for every query, we excluded ads that were not displayed at least
1, 000 times at every position. We also ﬁltered queries that had less than 5 ads
satisfying previous constraints. As a result, we obtained 8 queries with 5 up to

11 ads. For each query q, we computed the matrix Mq ∈ RK×L (where K is the

number of ads and L the number of positions) and ﬁlled it with the average click-
through-rate (CTR). We computed the SVD of Mq matrix and discarded every
singular value but the largest. Thus, we obtained a decomposition Mq = θqκT
q
where θq ∈ RK and κq ∈ RL. We emphasize that we normalized κq such that
the probability to observe an ad in ﬁrst position is 1. Finally, each ad a was
converted into a Bernoulli distribution of expectation θq,a. Table 1 reports some
statistics about the resulting bandit model.

We conducted a serie of N = 250 simulations over this dataset. At the be-
ginning of each run, a query was randomly selected together with corresponding
probabilities of scanning positions and arm expectations. Even though rewards
were still simulated, this scenario was more realistic since the values of the
parameters were extracted from a real-world dataset.

#ads (K) #records min θ max θ

5
5
6
6
6
8
11
11

216, 565
68, 179
435, 951
110, 071
147, 214
122, 218
1, 228, 004
391, 951

0.016
0.031
0.025
0.023
0.004
0.108
0.022
0.022

0.077
0.050
0.067
0.069
0.148
0.146
0.149
0.084

Table 1: Statistics on the resulting queries.

We show the results of the simulations in Figure 3. We display the evolution
of the CTR for 3 competitive algorithms: RSF-PIE(L), RSF-KL-UCBand
RSF-TS. Note that we do not show the CTR until t > 500 to avoid unrelevant
measures. Similarly to the previous scenario, RSF-TSslightly outperforms the
other algorithms. Furthermore, we emphasize that the CTR rapidly grows to a
near optimal value which is an important characteristic for production use.

2http://www.kddcup2012.org/

11

Figure 3: CTR of RSF-TS, RSF-KL-UCB and RSF-PIE(L).

7 Conclusion

The main idea of this work is to study a reward model that would be independent
of the content of the recommendation. The now well understood multiple-plays
problems with semi-bandits feedback has been studied in the Cascading Model
([14, 15, 7]) where the total reward is either 1 or 0 depending on the items
proposed in each round. We suggest that in some situations the user may be
allowed to click on several items stopping only because she gets tired of rating.
We study the lower-bound on the regret associated with such a bandit model
and propose a generic algorithm to solve this problem showing that optimal
performances can be attained. Even if empirical evaluation are quite convincing,
the statistical analysis of RSF-TS remains to be done.

Acknowledgements

References

[1] Venkatachalam Anantharam, Pravin Varaiya, and Jean Walrand. Asymp-
totically eﬃcient allocation rules for the multiarmed bandit problem with
multiple plays-part i: Iid rewards. Automatic Control, IEEE Transactions
on, 32(11):968–976, 1987.

[2] Apostolos N Burnetas and Micha¨el N Katehakis. Optimal adaptive poli-
cies for sequential allocation problems. Advances in Applied Mathematics,
17(2):122–142, 1996.

[3] Nicolo Cesa-Bianchi and G´abor Lugosi. Combinatorial bandits. Journal of

Computer and System Sciences, 78(5):1404–1422, 2012.

[4] Wei Chen, Yajun Wang, and Yang Yuan. Combinatorial multi-armed ban-
dit: General framework and applications. In Proceedings of the 30th Inter-
national Conference on Machine Learning, pages 151–159, 2013.

[5] Richard Combes, Marc Lelarge, Alexandre Proutiere, and M Sadegh
Talebi. Stochastic and adversarial combinatorial bandits. arXiv preprint
arXiv:1502.03475, 2015.

12

135710152040100(×103)051015202530#{(q,ad)-displays}distributionPositionp=1p=2p=300.010.020.030.040.050.070.10.20.50510152025CTRdistributionPositionp=1p=2p=301000020000300004000050000Roundt0.1200.1250.1300.1350.140CTRRSF-PIERSF-KL-UCBRSF-TS[6] Richard Combes, Marc Lelarge, Alexandre Proutiere, and M. Sadegh
CoRR,

Stochastic and adversarial combinatorial bandits.

Talebi.
abs/1502.03475, 2015.

[7] Richard Combes, Stefan Magureanu, Alexandre Proutiere, and Cyrille
Laroche. Learning to rank: Regret lower bounds and eﬃcient algorithms.
In Proceedings of the 2015 ACM SIGMETRICS International Conference
on Measurement and Modeling of Computer Systems, pages 231–244. ACM,
2015.

[8] Richard Combes and Alexandre Proutiere. Unimodal bandits: Regret lower

bounds and optimal algorithms. arXiv preprint arXiv:1405.5096, 2014.

[9] Yi Gai, Bhaskar Krishnamachari, and Rahul Jain. Learning multiuser chan-
nel allocations in cognitive radio networks: A combinatorial multi-armed
bandit formulation. In New Frontiers in Dynamic Spectrum, 2010 IEEE
Symposium on, pages 1–9. IEEE, 2010.

[10] Aur´elien Garivier and Olivier Capp´e. The kl-ucb algorithm for bounded
stochastic bandits and beyond. Conference on Learning Theory, pages 359–
376, 2011.

[11] Todd L Graves and Tze Leung Lai. Asymptotically eﬃcient adaptive choice
of control laws incontrolled markov chains. SIAM journal on control and
optimization, 35(3):715–743, 1997.

[12] Emilie Kaufmann, Olivier Capp´e, and Aur´elien Garivier. On the complexity
of best arm identication in multi-armed bandit models. Journal of Machine
Learning Research, 2015.

[13] Junpei Komiyama, Junya Honda, and Hiroshi Nakagawa. Optimal regret
analysis of thompson sampling in stochastic multi-armed bandit problem
with multiple plays. In Proceedings of the 32nd International Conference
on Machine Learning, 2015.

[14] Branislav Kveton, Csaba Szepesvari, Zheng Wen, and Azin Ashkan. Cas-
cading bandits : Learning to rank in the cascade model. In Proceedings of
the 32nd International Conference on Machine Learning, 2015.

[15] Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvari. Com-

binatorial cascading bandits. arXiv preprint arXiv:1507.04208, 2015.

[16] Tze Leung Lai and Herbert Robbins. Asymptotically eﬃcient adaptive

allocation rules. Advances in applied mathematics, 6(1):4–22, 1985.

[17] William R Thompson. On the likelihood that one unknown probability
exceeds another in view of the evidence of two samples. Biometrika, pages
285–294, 1933.

13

[18] Taishi Uchiya, Atsuyoshi Nakamura, and Mineichi Kudo. Algorithms for
adversarial bandit problems with multiple plays. In Algorithmic Learning
Theory, pages 375–389. Springer, 2010.

14

A Appendix

A.1 Lemmas
We call A the set of all possible actions, that is the set of all sequences of L
elements chosen among K options. We have |A| = K!/(K − L)! . We recall that
at each round s, the random variable Λs is equal to the length of the observed
sequence of feedback and it is independent of the chosen action.

Lemma 7. Let θ = (θ1, . . . , θK) and λ = (λ1, . . . , λK) be two bandit models such
that the distributions of all arms in θ and λ are mutually absolutely continuous.
Let σ be a stopping time with respect to (Ft) such that (σ < +∞) a.s. under
both models. Let E ∈ Fσ be an event such that 0 < Pθ(E) < 1. Then one has

(cid:88)
where Ka(θ, λ) =(cid:80)L

a∈A

Ka(θ, λ)Eθ[Na(T )] ≥ d(Pθ(E), Pλ(E))

l=1 κl KL(θa(l), λa(l)).

Proof. Let us denote by Xs the reward obtained at time s, following the selection
of arm As. Thus, Xs is a vector of length Λs.

The log-likelihood ratio of the observations up to time t under a bandit

algorithm is deﬁned by

Lt :=

t(cid:88)
(cid:88)
t(cid:88)

log

s=1

f (Xs; θ | Fs−1, Λs)
f (Xs; λ | Fs−1, Λs)
1{As = a}

L(cid:88)

l=1

1{Λs ≥ l} log

fa(l)(Xs,l; θ)
fa(l)(Xs,l; λ)

=

s=1

a∈A

We proceed in two steps : we ﬁrst justify that the expectation of the log-
likelihood ratio is lower-bounded by the binary entropy d(Pθ(E), Pλ(E)) and
then we show that the same expectation can be rewritten in a more convenient
form that gives the desired result. We follow the ideas of the proof of Lemma 1
in Appendix A.1 of [12].

Step 1 : Using the conditional Jensen’s inequality and Lemma 8, we have :

Pλ(E) = Eθ[Eθ[exp(−Lt)|1E ]1E ]
≥ Eθ[exp(−Eθ[Lt|E])1E ]
= exp(−Eθ[Lt|E])Pθ(E).

Writing the same for E yields

Pλ(E) ≥ exp(−Eθ[Lt|E])Pθ(E).

15

Finally, using the formula of total expectation, one gets

Eθ[Lt] = Eθ[Lt|E]Pθ(E) + Eθ[Lt|E]Pθ(E)
+ Pθ(E) log

(cid:18) Pθ(E)

(cid:19)

(cid:19)

(cid:18) Pθ(E)

Pλ(E)

Step 2 : Now we rewrite the expectation of the log-likelihood ratio using

the tower-property :

≥ Pθ(E) log
Pλ(E)
= d(Pθ(E), Pλ(E))
L(cid:88)

(cid:34) t(cid:88)

(cid:34)

Eθ[Lt] =Eθ

E

1{Λs ≥ l}

s=1

l=1

log

dνa(l)(θ)
dνa(l)(λ)

(Xs,l|Ft−1)

(cid:21)(cid:21)

(cid:12)(cid:12)(cid:12)(cid:12)Ft−1

As the action As at time s is determined by the past observations and actions,
the central conditional expectation can be scattered into action-related parts :

(cid:34) t(cid:88)

L(cid:88)

s=1

l=1

Eθ[Lt] = Eθ

(cid:88)

1{Λs ≥ l}

(cid:21)
1{As = a}

(Xs,l|Ft−1)

a∈A
dνa(l)(θ)
dνa(l)(λ)

log

Using the independence of Λs with respect to the sequence of feedback at

each round, we obtain

Eθ[Lt] =

t(cid:88)

L(cid:88)

s=1

l=1

(cid:34)(cid:88)

κlEθ

1{As = a}

(cid:21)
(Xs,l|Ft−1)

a∈A
dνa(l)(θ)
dνa(l)(λ)

log

and ﬁnally, rewriting the above sum using the notations Na(t) and Ka(θ, λ)

previously introduced, we obtain the result

(cid:88)

a∈A

Eθ[Lt] =

Ka(θ, λ)Eθ[Na(t)]

Lemma 8. Let σ be any stopping time with respect to (Ft). For every event
A ∈ Fσ,

Pν(cid:48)(A) = Eν[1{A} exp(−Lσ)]

A full proof of Lemma 8 can be found in the paper [12].

16

A.2 Proof of Proposition 6
Proof. We denote by a∗(θ) (resp. a∗(λ)) the optimal action under bandit model
parameterized by θ (resp. λ).

Let λ be in B(θ) and ET be the event

ET =

Na∗(θ)(T ) ≤ T −

(cid:16)

(cid:17)

√T

The event ET is not very likely to hold under bandit model parameterized by
θ since the optimal arm a∗(θ) should be pulled in order of T − O(log T ). On the
opposite, ET is very likely to hold under model parameterized by λ because a∗(θ)
is not an optimal action anymore. Thus, it should be chosen little compared to
a∗(λ).

Markov inequality gives

 (cid:88)
(cid:16)

a(cid:54)=a∗(θ)

Na∗(θ) ≥ T −
a(cid:54)=a∗(λ)
T − √T

Eλ[Na(T )]

Pθ(ET ) = Pθ
(cid:80)

Pλ(E c

T ) = Pλ

≤

(cid:80)

 ≤

√T

Na(T ) ≥
√T

(cid:17)

a(cid:54)=a∗(θ)

Eθ[Na(T )]
√T
Eλ[Na∗(θ)(T )]

≤

T − √T

From deﬁnition 2, one obtains Pθ(ET ) −−−−→T→∞ 0 and Pλ(E c
using the deﬁnition of binary entropy, we get

T ) −−−−→T→∞ 0. Therefore,

log(1 − 1√

T

)

Here, we used again deﬁnition 2 which tells us that(cid:80)

a(cid:54)=a∗(λ)

log T

log T

1 +

−

Eλ[Na(T )])

a(cid:54)=a∗(λ)

−−−−→T→∞ 1
Eλ[Na(T )] = o(T α)

for any α ∈]0, 1].

(cid:80)

lim inf
T→∞

Now, for all λ ∈ Bk(θ) Why Bk(θ)?, lemma 7 applied with event ET gives
(6)

a∈A Ka(θ, λ)Eθ[Na(T )]

(cid:80)

log T

a∈Ak

= lim inf
T→∞

≥ lim inf
T→∞

Ka(θ, λ)Eθ[Na(T )]

log T

d(Pθ(ET ), Pλ(ET ))

log T

= 1

(7)

(8)

where equality 7 is due to the elimination of all null terms in the sum that
appears when arm k is not selected in action a.

17

(cid:19)
(cid:18) 1
Pλ(E c
T )
T − √T

1

log T

log

(cid:32)

log

(cid:80)

a(cid:54)=a∗(λ)

Eλ[Na(T )]

(cid:33)

d(Pθ(ET ), Pλ(ET ))

log T

The RHS rewrites

∼T→∞
1

≥

log T

log((cid:80)

Inequality for suboptimal pulls

A.3
Suppose arm k is drawn in position l ≤ L. The overall regret of that action can
be minimized by pulling arms 1, ..., L − 1, which are the best possible ones, in
decreasing order. Acting so leads to a null regret on ﬁrst l − 1 positions and
negative regret on last L − l positions. The same reasoning can be done for
multiple suboptimal pulls. We summarize this property in the following lemma.

Lemma 9. Let a be an action containing S suboptimal arms k1, . . . , kS in
positions l1, . . . , lS, we have da(θ) ≥

i=1 κli(θL − θki ). Or, more generally,

(cid:80)S

(cid:88)

L(cid:88)

k>L

l=1

da(θ) ≥

1{a(l) = k}κl(θL − θk)

Proof. The regret of action a containing 1 suboptimal arm and optimally com-
pleted can be decomposed as follows.

(cid:122)
L(cid:88)

negative rest

(cid:123)
(cid:125)(cid:124)
κj(θj − θj−1)

j=l+1

= κl(θl − θl+1 + θl+1 − θl+2 + ... + θL − θk)+

da(θ) = 0 + κl(θl − θk) +

L(cid:88)

j=l+1

κj(θj − θj−1)
L(cid:88)

= κl(θL − θk) +

(θj−1 − θj)(κl − κj)

j=l+1

> κl(θL − θk).

The result for multiple suboptimal plays can be obtained by decomposing in
the same way both the positive regret incurred by each suboptimal arm and the
negative rest resulting from the shift of optimal arms.

Notice that this means that pulling arm k in position l ≤ L is always worst
than pulling it in position L. Moreover, if the action a contains more than one
suboptimal arm, the sharpest lower-bound is obtained by considering the worst
suboptimal arm in the list.

A.4 Proof of Theorem 4

In order to prove the simpliﬁed lower-bound of Theorem 4 we basically have
two arguments :

1. a lower-bound on c(θ) can be obtained by widening the feasible set, that

is by relaxing some constraints;

18

2. the Lemma 9 can be used to lower-bound the objective function of the

problem.

The constant c(θ) is deﬁned by

c(θ) = inf
c(cid:23)0

(cid:88)
(cid:88)

a(cid:54)=a∗(θ)

a∈A

da(θ)ca

s.t

inf

λ∈B(θ)

Ka(θ, λ)ca ≥ 1.

(9)

(10)

We begin by relaxing some constraints : we only allow the change of measure λ
to belong to the sets Bk(θ) deﬁned in Section 4, Equation (3) :

(cid:88)

c(θ) = inf
c(cid:23)0

a(cid:54)=a∗(θ)

da(θ)ca

s.t ∀k /∈ a∗(θ), ∀λ ∈ Bk(θ),

(cid:88)

a∈A

Ka(θ, λ)ca ≥ 1.

(11)

(12)

The K − L constraints (12) only let one parameter move and must be true for
any value satisfying the deﬁnition of the corresponding set Bk(θ). In practice,
for each k, the parameter λk must be set to at least θL. Consequently, these
constraints may then be rewritten

(cid:88)

a(cid:54)=a∗(θ)

L(cid:88)

l=1

∀k /∈ a∗(θ)

ca

1{a(l) = k}κl KL(θk, θL) ≥ 1

where we used the fact that the hardest constraint to satisfy is when λk = θL,
which allows to reach the inﬁmum of the above sum. Notice that the above sum
actually does not contain all coeﬃcients of suboptimal actions ca: only appear
coeﬃcients of actions containing the suboptimal arm k in any position. Before
going on, remark that we end up with a set of constraints that do not concern
actions containing no suboptimal arm. Consequently, those coeﬃcients can be
set to any value such as 0.

Finally, it remains to lower bound the objective function of the optimization
problem (9). To do so, the actions containing suboptimal arms must be carefully
treated because they are those which induce the highest regret. Then, because
of Lemma 9, we can lower bound the objective function as follows

da(θ)ca

a(cid:54)=a∗(θ)

(cid:88)
(cid:88)
(cid:88)

k /∈a∗(θ)

k /∈a∗(θ)

≥

≥

(θL − θk)
θL − θk
KL(θk, θL)

(cid:88)

a(cid:54)=a∗(θ)

L(cid:88)

l=1

ca

κl1{a(l) = k}

(13)

(14)

where inequality (13) is a due to Lemma 9 and the last inequality is obtained by
plugging in the constraints of the optimization problem previously rewritten.

19

