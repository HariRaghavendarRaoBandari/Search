6
1
0
2

 
r
a

 

M
4
1

 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
9
7
3
4
0

.

3
0
6
1
:
v
i
X
r
a

On Data Dependence in Distributed Stochastic Optimization

Avleen S. Bijral∗1, Anand D. Sarwate†2, and Nathan Srebro‡1

1Toyota Technological Institute at Chicago , 6045 S. Kenwood Ave , Chicago, IL 60637

2Rutgers University Department of ECE , 94 Brett Road , Piscataway, NJ 07302

March 15, 2016

Abstract

We study a distributed consensus-based stochastic gradient (SG) algorithm and show that the rate of
convergence involves the spectral properties of two matrices: the standard spectral gap of a weight matrix
from the network topology and a new term depending on the spectral norm of the sample covariance
matrix of the data. This data-dependent convergence rate shows that distributed SG algorithms perform
better on datasets with small spectral norm. Our analysis method also allows us to ﬁnd data-dependent
convergence rates as we limit the amount of communication. Spreading a ﬁxed amount of data across
more nodes slows convergence; for asymptotically growing data sets we show that adding more machines
can help when minimizing twice-differentiable losses.

1 Introduction

Decentralized optimization algorithms for statistical computation and machine learning on large data sets
try to trade off efﬁciency (in terms of estimation error) and speed (from parallelization). From an empirical
perspective, it is often unclear when these methods will work will for a particular data set, and to what degree
additional communication can improve performance. For example, in high-dimensional problems commu-
nication can be costly, so we would like to know when limiting communication is feasible or beneﬁcial. The
theoretical analysis of distributed optimization methods has focused on providing strong data-independent
convergence rates under analytic assumptions on the objective function such as convexity and smoothness.
In this paper we show how the tradeoff between efﬁciency and speed is affected by the data distribution itself.
We study a class of distributed optimization algorithms and prove an upper bound on the error that depends
on the spectral norm of the data covariance. By tuning the frequency with which nodes communicate, we
obtain a bound that depends on data distribution, network size and topology, and amount of communication.
This allows us to interpolate between regimes where communication is cheap (e.g. shared memory systems)
and those where it is not (clusters and sensor networks).

We study the problem of minimizing a regularized convex function [Rakhlin et al., 2012] of the form

(cid:104)

(cid:105)

(cid:96)(w(cid:62)x)

(cid:107)w(cid:107)2 ,

+

µ
2

(1)

J(w) = E

x∼ ˆP

∗abijral@ttic.edu
†asarwate@ece.rutgers.edu
‡nati@ttic.edu

1

(cid:80)m

where (cid:96)(·) is convex and Lipschitz and the expectation is with respect to the empirical distribution ˆP on Rd
corresponding to a given data set with N total data points. This problem is representative of several machine
learning problems such as support vector machines, ridge regression, and others. In a distributed setting, we
can distribute these N points across m nodes and each can form a local objective Jj(w) approximating (1).
The ﬁrst strategy we analyze is what is referred to as distributed primal averaging [Nedic and Ozdaglar,
2009]: each node in the network processes points sequentially, performing a stochastic gradient descent
(SGD) update locally and averaging the current iterate values of their neighbors after each gradient step.
This can also be thought of as a distributed consensus-based version of Pegasos [Shalev-Shwartz et al.,
2011] when the loss function is the hinge loss. We consider a general topology with m nodes attempting
to minimize a global objective function J(w) that decomposes into a sum of m local objectives: J(w) =
i=1 Ji(w). This is a model for optimization in systems such as data centers, distributed control systems,

and sensor networks.
Main Results. Our goal in this paper is characterize how limiting communication affects the rate of con-
vergence when solving (1), and to show how the spectral norm ρ2 = σ1(E ˆP [xx(cid:62)]) of the sample covariance
of the data affects this rate. Elucidating this dependence can help guide empirical practice by providing
insight into when these methods will work well. We prove an upper bound on the suboptimality gap for
distributed primal averaging that depends on ρ2 as well as the mixing time of the weight matrix associated
to the algorithm. Our result shows that networks of size m < 1
ρ2 gain from parallelization. To understand
the communication-limited regime, we extend our analysis to intermittent communication. In a setting with
ﬁnite data and sparse connectivity, convergence will deteriorate with increasing m because we split the data
to more machines that are farther apart. We show that by using a mini-batching strategy we can offset the
penalty of infrequent communication by communicating after a mini-batch (sub)gradient step. Finally, in an
asymptotic regime with inﬁnite data at every node we show that for twice-differentiable loss functions this
network effect disappears and that we gain from additional parallelization.

Related Work. Several authors have proposed distributed algorithms involving nodes computing local
gradient steps and averaging iterates, gradients, or other functions of their neighbors [Nedic and Ozdaglar,
2009, Duchi et al., 2011, Ram et al., 2011]. By alternating local updates and consensus with neighbors,
estimates at the nodes converge to the optimizer of J(·). In these works no assumption is made on the
local objective functions and they can be arbitrary. Consequently the convergence guarantees do not reﬂect
the setting when the data is homogenous (for e.g. when data has the same distribution), speciﬁcally error
increases as we add more machines. This is counter intuitive, especially in the large scale regime, since this
suggests that despite homogeneity the methods perform worse than the centralized setting (all data on one
node).

We provide a ﬁrst analysis of a consensus based stochastic gradient method in the homogenous setting
and demonstrate that there exist regimes where we beneﬁt from having more machines in any network. To
mitigate the effect of limited communication, we propose and analyze a mini-batched extension to reduce
communication costs. We interpret this as an intermediate regime between full communication and one-
shot communication [Zhang et al., 2012, Shamir et al., 2014]. Finally, we show that for twice-differentiable
losses, having more machines always helps (via a variance reduction) in the inﬁnite data regime, using
results of Bianchi et al. Bianchi et al. [2013].

In contrast to our stochastic gradient based results, data dependence via the Hessian of the objective has
also been demonstrated in parallel coordinate descent based approaches of Liu et al. Liu et al. [2014] and
the Shotgun algorithm of Bradley et al. Bradley et al. [2011]. The assumptions differ from us in that the
objective function is assumed to be smooth Liu et al. [2014] or L1 regularized Bradley et al. [2011]. Most
importantly, our results hold for arbitrary networks of compute nodes, while the coordinate descent based

2

results hold only for networks where all nodes communicate with a central aggregator (sometimes referred
to as a master-slave architecture, or a star network), which can be used to model shared-memory systems.

Another interesting line of work is the impact of delay on convergence in distributed optimization Agar-
wal and Duchi [2011]. These results show that delays in the gradient computation for a star network are
asymptotically negligible when optimizing smooth loss functions. We study general network topologies
but with intermittent, rather than delayed communication. Our result suggest that certain datasets are more
tolerant of skipped communication rounds, based on the spectral norm of their covariance.

We take an approach similar to that of Tak´aˇc et al. [Tak´aˇc et al., 2013], who developed a spectral-norm
based analysis of mini-batching for non-smooth functions. We decompose the iterate in terms of the data
points encountered in the sample path [Cotter et al., 2011]. This differs from analysis based on smoothness
considerations alone [Cotter et al., 2011, Agarwal and Duchi, 2011, Dekel et al., 2012, Shamir et al., 2014]
and gives practical insight into how communication (full or intermittent) impacts the performance of these
algorithms. Note that our work is fundamentally different in that these other works either assume a cen-
tralized setting [Cotter et al., 2011, Dekel et al., 2012, Shamir et al., 2014] or implicitly assume a speciﬁc
network topology (e.g. [Zhang et al., 2012] uses a star topology). For the main results we only assume
strong convexity while the existing guarantees for the cited methods depend on a variety of regularity and
smoothness conditions.

Limitation. In the stochastic convex optimization (see for e.g. Shalev-Shwartz et al. [2009]) setting
the quantity of interest is the population objective corresponding to problem 1. When minimizing this
population objective our results suggest that adding more machines worsens convergence (See Theorem 1).
For ﬁnite data our convergence results satisfy the inutition that adding more nodes in an arbitrary network
will hurt convergence. The ﬁnite homogenous setting is often relevant in the wireless sensor network setting
wherein a network of nodes collects and processes information from a given spatial area (See for e.g. Forest
ﬁre detection of [Yu et al., 2005]).

In the inﬁnite or large scale data setting, common in machine learning applications, this is counter in-
tuitive since when each node has inﬁnite data, any distributed scheme including one on arbitrary networks
shouldn’t perform worse than the centralized scheme (all data on one node). Thus our analysis is limited in
that it doesn’t unify the stochastic optimization and the consensus setting in a completely satisfactory man-
ner. To partially remedy this we explore distributed primal averaging for smooth strongly convex objectives
in the asymptotic regime and show that one can gain from adding more machines in any network.

2 Model
We will use boldface for vectors. Let [k] = {1, 2, . . . , k}. Unless otherwise speciﬁed, the norm (cid:107)·(cid:107) is the
standard Euclidean norm.
Data model. Let P be a distribution on Rd such that for x ∼ P, we have (cid:107)xi(cid:107) ≤ 1 almost surely.
Let S = {x1, x2, . . . , xN} be i.i.d sample from P and let ˆP be the empirical distribution of S. Let ˆΣ =
x∼ ˆP [xx(cid:62)] be the sample second-moment matrix of S. Our goal is to express the performance of our
E
algorithms in terms of ρ = σ1( ˆΣ) where σ1(·) denotes the maximum singular value.

Problem. Our problem is to minimize (1):

(cid:32)

w∗ def

= argmin

w

J(w)

def
=

(cid:80)N
i=1 (cid:96)(w(cid:62)xi)

N

(cid:107)w(cid:107)2

+

µ
2

(cid:33)

(2)

We bound the expected gap (over the data distribution) between J(w∗) and the value J( ˆwi(T )) of the
global objective J( ˆwi(T )) at the output ˆwi(T ) of each node i in our distributed network. It would also be

3

interesting to study the gap between the iterates and the minimizer of the population objective in (1); we
defer this for future work. We will denote the subgradient set of J(w) by ∂J(w) and a subgradient of J(w)
by ∇J(w) ∈ ∂J(w).
In our analysis we will make the following assumptions about the individual functions (cid:96)(w(cid:62)x): (a) The
loss functions {(cid:96)(·)} are convex, and (b) The loss functions {(cid:96)(·)} are L-Lipschitz for some L > 0. Note
that J(w) is µ-strongly convex due to the (cid:96)2-regularization. For binary classiﬁcation problems x is assumed
to be scaled by its label in {−1, +1}.
Network Model. We consider a model in which minimization in (2) must be carried out by m compu-
tational devices, or nodes. These nodes are arranged in a network whose topology is given by a graph G –
an edge (i, j) in the graph means nodes i and j can communicate. A matrix P is called graph conformant if
Pij > 0 only if the edge (i, j) is in the graph. We will consider algorithms which use a doubly stochastic
and graph conformant sequence of matrices P(t).

Sampling Model. We assume the N data points are divided evenly and uniformly at random among the
def
m nodes, and deﬁne n
= N/m to be the number of points at each node. This is a necessary assumption
since our bounds are data dependent and depend on subsampling bounds of spectral norm of certain random
submatrices. However our data independent bound holds for arbitrary splits. Let Si be the subset of n points
at node i. The local stochastic gradient procedure consists of each node i ∈ [m] sampling from Si with
replacement. This is an approximation to the local objective function

(cid:88)

(cid:96)(w(cid:62)xi,j)

Ji(w) =

j∈Si

n

(cid:107)w(cid:107)2 .

+

µ
2

(3)

Algorithm. Algorithm 1 (CoSCO) describes the distributed strategy we analyze in the subsequent sec-
tions. Every node i samples a point uniformly with replacement from a local pool of n points and then
updates its iterate by computing a weighted sum with its neighbors followed by a local subgradient step.
Different choices of P(t) will allow us to understand the effect of limiting communication in this distributed
optimization algorithm.

Algorithm 1 Consensus Strongly Convex Optimization - CoSCO

Input: {xi,j},where i ∈ [m] and j ∈ [n] and N = mn, matrix P, µ > 0, T ≥ 1
{Each i ∈ [m] executes}
Initialize: set wi(1) = 0 ∈ Rd.
for t = 1 to T do

wi(t + 1) =(cid:80)

Sample xi,t uniformly with replacement from Si.
Compute gi(t) ∈ ∂(cid:96)(wi(t)(cid:62)xi,t)xi,t + µwi(t)
j∈N (i) wj(t)Pij(t) − ηtgi(t)
(cid:80)T
t=1 wi(t) for any i ∈ [m].

end for
Output: ˆwi(T ) = 1
T

3 Convergence and Implications

Algorithms like CoSCO (1), also referred to as primal averaging, have been analyzed previously [Nedic
In these works it is shown that the
and Ozdaglar, 2009, Ram et al., 2011, Tsianos and Rabbat, 2012].

4

convergence properties depend on the structure of the underlying network via the second largest eigenvalue
of P. We consider in this section the case when P(t) = P for all t where P is a ﬁxed Markov matrix. This
corresponds to a synchronous setting where communication occurs at every iteration.

We analyze the use of the step-size ηt = 1/(µt) in Algorithm 1 and show that the convergence depends

on the spectral norm ρ2 = σ1( ˆΣ) of the sample covariance matrix.
Theorem 1. Fix a Markov matrix P and let ρ2 = σ1( ˆΣ) denote the spectral norm of the covariance matrix
of the data distribution. Consider Algorithm 1 when the objective J(w) is strongly convex, P(t) = P for
all t, and ηt = 1/(µt). Let λ2(P) denote the second largest eigenvalue of P. Then if the number of samples
on each machine n satisﬁes

and the number of iterations T satisﬁes

n >

4
3ρ2 log (d)

(4)

(5)

(6)

(7)

then the expected error for each node i satisﬁes

T

log(T )

> max

T > 2e log(1/(cid:112)λ2(P))

3ρ2 log (d) ,

 4
100(cid:112)mρ2 · log T
1 −(cid:112)λ2(P )

E [J( ˆwi(T )) − J(w∗)] ≤

(cid:32)

1
m

+

(cid:0) 8

5

4(cid:112)m/ρ
(cid:1) 1

log(1/λ2(P))

 ,

(cid:33)

· L2
µ

· log T
T

.

Remark 1: Theorem 1 indicates that the number of machines should be chosen as a function of ρ. We can
identify three sub-cases of interest:

ρ2/3 : In this regime since 1/m >(cid:112)mρ2 (ignoring the constants and the log T term) we

Case (a): m ≤ 1

always beneﬁt from adding more machines.

1

ρ2/3 < m ≤ 1

Case (b):

√

ρ2 : The result tells us that there is no degradation in the error and the bound
mρ. Sparse data sets generally have a smaller value of ρ2 (as seen in Tak´aˇc et
improves by a factor
al. Tak´aˇc et al. [2013]); Theorem 1 suggests that for such data sets we can use a larger number of machines
without losing performance. However the requirements on the number of iterations also increases. This
provides additional perspective on the observation by Tak´aˇc et al Tak´aˇc et al. [2013] that sparse datasets are
more amenable to parallelization via mini-batching. The same holds for our type of parallelization as well.

ρ2 : In this case we pay a pay a penalty(cid:112)mρ2 ≥ 1 suggesting that for datasets with
Note that m > 1 is implicit in the condition T > 2e log(1/(cid:112)λ2)) since λ2 = 0 for m = 1. This

large ρ we should expect to lose performance even with relatively fewer machines.

Case (c): m > 1

excludes the single node Pegasos [Tak´aˇc et al., 2013] case. Additionally in the case of general strongly
convex losses (not necessarily dependent on w(cid:62)x) we can obtain a convergence rate of O(log2(T )/T ). We
do not provide the proof here.
Remark 2: The lower bound on the number of iterations 6 can be considerably improved by instead looking
at the intrinsic dimension of the data since for several real datasets the intrinsic dimension can be much
smaller than the dimension of the ambient space. However this requires us to assume a lower bound on the
norm of the data samples, which is a less natural assumption.

5

3.1 Proof of Theorem 1
Let Ft be the sigma algebra generated by data and random selections of the algorithm up to time t, so that
the iterates {wi(t) : i ∈ [m]} are measurable with respect to Ft. Theorem 1 provides a bound on the
suboptimality gap for the output ˆwi(T ) of CoSCO at node i, which is the average of that node’s iterates. In
the analysis we relate this local average to the average iterate across nodes at time t:

m(cid:88)

i=1

¯w(t) =

wi(t)

m

.

(8)

We will also consider the average of ¯w(t) over time.

The proof consists of three main steps.
• We establish the following inequality for the objective error:

E [J( ¯w(t)) − J(w∗)] ≤

t
2

E(cid:104)(cid:107) ¯w(t) − w∗(cid:107)2(cid:105)
(η−1
t − µ)
E(cid:104)(cid:107) ¯w(t + 1) − w∗(cid:107)2(cid:105)
2
− η−1
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) m(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:114)
E(cid:104)(cid:107) ¯w(t) − wi(t)(cid:107)2(cid:105)
m(cid:88)
(cid:114)
((cid:107)∇Ji(wi(t))(cid:107) + (cid:107)∇Ji( ¯w(t))(cid:107))2(cid:105)
E(cid:104)

gi(t)

ηt
2

i=1

i=1

E

m

+

+

·

/m,

(9)

where ¯w(t) is the average of the iterates at all nodes and the expectation is with respect to Ft while
conditioned on the sample split across nodes. All expectations, except when explicitly stated, will be
conditioned on this split.

(cid:20)(cid:13)(cid:13)(cid:13)(cid:80)m
• We bound E(cid:104)(cid:107)∇J(wi(t))(cid:107)2(cid:105)
• We bound the network error E(cid:104)(cid:107) ¯w(t) − wi(t)(cid:107)2(cid:105)

and ηt
2

i=1

E

(cid:13)(cid:13)(cid:13)2(cid:21)

gi(t)

m

ance matrix of the distribution P by additionally taking expectation with respect to the sample S.

in terms of the spectral norm of the covari-

in term of the network size m and a spectral prop-

erty of the matrix P.

Combining the bounds using inequality (9) and applying the deﬁnition of subgradients yields the result of
Theorem 1.

3.2 Spectral Norm of Random Submatrices
In this section we establish a Lemma pertaining to the spectral norm of submatrices that is central to our
results. Speciﬁcally we prove the following inequality, which follows by applying the Matrix Bernstein
inequality of Tropp Tropp [2012].

6

Lemma 2. Let P be a distribution on Rd with second moment matrix Σ = EY∼P [YY(cid:62)] such that (cid:107)Yk(cid:107) ≤
1 almost surely. Let ζ 2 = σ1(Σ). Let Y1, Y2, . . . , YK be an i.i.d. sample from P and let

YkY(cid:62)

k

QK =

K(cid:88)
(cid:20) σ1(QK)

k=1

(cid:21)

be the empirical second moment matrix of the data. Then for K > 4

3ζ2 log(d),

Thus when P is the empirical distribution we get that E(cid:104) σ1(QK )

K

(cid:105) ≤ 5ζ 2.

(10)

E

≤ 5ζ 2.

K

Remark: We can replace the ambient dimension d in the requirement for K by an intrinsic dimensionality
term but this requires a lower bound on the norm of any data point in the sample.
Proof. Let Y be the d × K matrix whose columns are {Yk}. Deﬁne Xk = YkY(cid:62)
and

k − Σ. Then E[Xk] = 0

(cid:16)

(cid:17)
k − Σ

YkY(cid:62)

λmax(Xk) = λmax
≤ (cid:107)Yk(cid:107)2
≤ 1,

because Σ is positive semideﬁnite and (cid:107)xi(cid:107) ≤ 1 for all i. Furthermore,

(cid:3)(cid:33)

E(cid:2)X2

k

(cid:32) K(cid:88)

k=1

σ1

(cid:16)E(cid:104)
(cid:16)E(cid:104)(cid:107)Yk(cid:107)2 YkY(cid:62)

(cid:105) − Σ2(cid:17)
k YkY(cid:62)
(cid:105)(cid:17)

YkY(cid:62)

k

k

+ Kσ1 (Σ)2

= Kσ1

≤ Kσ1
≤ K(ζ 2 + ζ 4)
≤ 2Kζ 2,

since ρ ≤ 1.

P

(cid:33)

Applying the Matrix Bernstein inequality of Tropp [Tropp, 2012, Theorem 6.1]:
K ≤ 2ζ 2
K ≥ 2ζ 2
(cid:33)
k − KΣ

(cid:16)−3 r2
(cid:40)
(cid:1)
d exp(cid:0)−3 r
(cid:32) K(cid:88)

YkY(cid:62)

(cid:33)

≥ r

(cid:17)

d exp

= σ1

16Kζ2

Xk

≤

8

,

r

r

.

Now, note that

k=1

σ1

Xk

(cid:32)

(cid:33)
(cid:32) K(cid:88)
(cid:32) K(cid:88)
(cid:17) ≥ r is implied by(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

σ1

σ1

k=1

K

(cid:32) K(cid:88)

k=1

k=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ r

K

.

− σ1 (Σ)

(cid:33)

YkY(cid:62)

k

7

(cid:16)(cid:80)K

so σ1

k=1 Xk

(11)

Therefore

P

Integrating (12) yields

r(cid:48) ≤ 2ζ 2
r(cid:48) ≥ 2ζ 2

.

(12)

(cid:19)

dx

K

E

− ζ 2

(cid:18)(cid:12)(cid:12)(cid:12)(cid:12) σ1(QK)
(cid:12)(cid:12)(cid:12)(cid:12) ≥ r(cid:48)(cid:19)
(cid:20) σ1(QK)
(cid:21)
(cid:18) σ1(QK)
(cid:90) ∞
(cid:90) ∞
(cid:90) ∞
(cid:90) ∞

≤ 3ζ 2 +

≤ 3ζ 2 +

K
P

3ζ2

2ζ2

P

P

K

=

0

≤

d exp

d exp
(cid:19)
(cid:18) σ1(Qt)
(cid:18) σ1(Qt)
(cid:18)
(cid:18)

K
− 3
8
− 3
4

≥ x

dx

exp

K

ζ 2K

d exp

≤ 3ζ 2 +

= 3ζ 2 +

2ζ2
8
3

· d
K

(cid:16)− 3Kr(cid:48)2
(cid:17)
(cid:17)
(cid:16)− 3Kr(cid:48)

16ζ2

8

− ζ 2 ≥ x − ζ 2

− ζ 2 ≥ r(cid:48)(cid:19)
Kr(cid:48)(cid:19)
(cid:19)

dr(cid:48)

dr(cid:48)

For K > 4

3ζ2 log d,

(cid:20) σ1(QK)

(cid:21)

K

E

8
3

· 3
4

· ζ 2
log d

≤ 3ζ 2 +
≤ 5ζ 2.

3.3 Decomposing the expected suboptimality gap
Proof. The proof in part follows [Nedic and Ozdaglar, 2009]. It is easy to verify that because P is doubly
stochastic the average of the iterates across the nodes at time t, the average of the iterates across the nodes
in (8) satisﬁes the following update rule:

¯w(t + 1) = ¯w(t) − ηt

gi(t)

m

.

(13)

m(cid:88)

i=1

We emphasize that in Algorithm 1 we do not perform a ﬁnal averaging across nodes at the end as in (8).
Rather, we analyze the average at a single node across its iterates (sometimes called Polyak averaging).
Analyzing (8) provides us with a way to understand how the objective J(wi(t)) evaluated at any node i’s
iterate wi(t) compares to the minimum value J(w∗). The details can be found in Section 3.8.

8

To simplify notation, we treat all expectations as conditioned on the sample S. Then (13),



(cid:105)

gi(t)

+ η2
t

E(cid:104)(cid:107) ¯w(t + 1) − w∗(cid:107)2(cid:12)(cid:12)(cid:12)Ft
(cid:105)
= E(cid:104)(cid:107) ¯w(t) − w∗(cid:107)2 |Ft
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) m(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2(cid:12)(cid:12)(cid:12)Ft
− 2ηt( ¯w(t) − w∗)(cid:62) m(cid:88)
= E(cid:104)(cid:107) ¯w(t) − w∗(cid:107)2 |Ft
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) m(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2(cid:12)(cid:12)(cid:12)Ft
m(cid:88)

+ η2
t

gi(t)

(cid:105)

m

i=1

m

i=1

E

E

i=1

E [gi(t)|Ft]

m



− 2ηt

i=1

( ¯w(t) − w∗)(cid:62) E [gi(t)|Ft]

m

.

(14)

Note that ∇Ji(wi(t)) = E [gi(t)|Ft], so for the last term, for each i we have

∇Ji(wi(t))(cid:62)( ¯w(t) − w∗)

= ∇Ji(wi(t))(cid:62) ( ¯w(t) − wi(t))

+ ∇Ji(wi(t))(cid:62) (wi(t) − w∗)
≥ −(cid:107)∇Ji(wi(t))(cid:107)(cid:107) ¯w(t) − wi(t)(cid:107)
+ ∇Ji(wi(t))(cid:62) (wi(t) − w∗)
≥ −(cid:107)∇Ji(wi(t))(cid:107)(cid:107) ¯w(t) − wi(t)(cid:107)
µ
2
= −(cid:107)∇Ji(wi(t))(cid:107)(cid:107) ¯w(t) − wi(t)(cid:107)

+ Ji(wi(t)) − Ji(w∗) +

(cid:107)wi(t) − w∗(cid:107)2

+ Ji(wi(t)) − Ji( ¯w(t))
+

(cid:107)wi(t) − w∗(cid:107)2 + Ji( ¯w(t)) − Ji(w∗)

≥ −(cid:107)∇Ji(wi(t))(cid:107)(cid:107) ¯w(t) − wi(t)(cid:107)
+ ∇Ji( ¯w(t))(cid:62) (wi(t) − ¯w(t))
+

(cid:107)wi(t) − w∗(cid:107)2 + Ji( ¯w(t)) − Ji(w∗)

µ
2

µ
2

≥ − ((cid:107)∇Ji(wi(t))(cid:107) + (cid:107)∇Ji( ¯w(t))(cid:107))(cid:107) ¯w(t) − wi(t)(cid:107)

+

µ
2

(cid:107)wi(t) − w∗(cid:107)2 + Ji( ¯w(t)) − Ji(w∗),

(15)

where the second and third lines comes from applying the Cauchy-Shwartz inequality and strong convexity,
the ﬁfth line comes from the deﬁnition of subgradient, and the last line is another application of the Cauchy-
Shwartz inequality.

9

Averaging over all the nodes, using convexity of (cid:107)·(cid:107)2, the deﬁnition of J(·), and Jensen’s inequality

yields the following inequality:

(cid:107) ¯w(t) − wi(t)(cid:107) ((cid:107)∇Ji(wi(t))(cid:107) + (cid:107)∇Ji( ¯w(t))(cid:107))

(16)

(17)

−2ηt

m(cid:88)

i=1

≤ 2ηt

m

( ¯w(t) − w∗)(cid:62) E[gi(t)|Ft]
m(cid:88)
(cid:32) m(cid:88)
m(cid:88)

(cid:107)wi(t) − w∗(cid:107)2

m
Ji( ¯w(t)) − Ji(w∗)

i=1

i=1

m

− 2ηt

(cid:33)

i=1

m

− µηt

m(cid:88)

i=1

≤ 2ηt

(cid:107) ¯w(t) − wi(t)(cid:107) ((cid:107)∇Ji(wi(t))(cid:107) + (cid:107)∇Ji( ¯w(t))(cid:107))

− 2ηt (J( ¯w(t)) − J(w∗)) − µηt (cid:107) ¯w(t) − w∗(cid:107)2

m

Substituting inequality (16) in recursion (14),

E(cid:104)(cid:107) ¯w(t + 1) − w∗(cid:107)2(cid:12)(cid:12)Ft
(cid:105)
≤ E(cid:104)(cid:107) ¯w(t) − w∗(cid:107)2 |Ft
(cid:105)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2 (cid:12)(cid:12)(cid:12) Ft

+ η2
t

gi(t)

E

m

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) m(cid:88)
m(cid:88)

i=1



(cid:107) ¯w(t) − wi(t)(cid:107) ((cid:107)∇Ji(wi(t))(cid:107) + (cid:107)∇Ji( ¯w(t))(cid:107))

+ 2ηt
− 2ηt (J( ¯w(t)) − J(w∗)) − µηt (cid:107) ¯w(t) − w∗(cid:107)2 .

i=1

m

10

Taking expectations with respect to the entire history Ft,

E [(cid:107) ¯w(t) − wi(t)(cid:107) ((cid:107)∇Ji(wi(t))(cid:107) + (cid:107)∇Ji( ¯w(t))(cid:107))]

i=1

m

E

+ η2
t

gi(t)

m

E

+ η2
t

+ 2ηt·

m(cid:88)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) m(cid:88)

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

≤ −2ηt (E [J( ¯w(t)) − J(w∗)])

E(cid:104)(cid:107) ¯w(t + 1) − w∗(cid:107)2(cid:105)
≤ E(cid:104)(cid:107) ¯w(t) − w∗(cid:107)2(cid:105)
m(cid:88)
− 2ηt (E [J( ¯w(t)) − J(w∗)]) − µηtE(cid:104)(cid:107) ¯w(t) − w∗(cid:107)2(cid:105)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) m(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
+ (1 − µηt)E(cid:104)(cid:107) ¯w(t) − w∗(cid:107)2(cid:105)
(cid:114)
E(cid:104)(cid:107) ¯w(t) − wi(t)(cid:107)2(cid:105)
(cid:114)
E(cid:104)
((cid:107)∇Ji(wi(t))(cid:107) + (cid:107)∇Ji( ¯w(t))(cid:107))2(cid:105)
E(cid:104)(cid:107) ¯w(t) − w∗(cid:107)2(cid:105)
(η−1
t − µ)
E(cid:104)(cid:107) ¯w(t + 1) − w∗(cid:107)2(cid:105)
2
− η−1
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) m(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:114)
E(cid:104)(cid:107) ¯w(t) − wi(t)(cid:107)2(cid:105)
m(cid:88)
(cid:114)
((cid:107)∇Ji(wi(t))(cid:107) + (cid:107)∇Ji( ¯w(t))(cid:107))2(cid:105)
E(cid:104)

gi(t)

m

gi(t)

m

+

2ηt
m

i=1

·

T2 =

E

ηt
2

T3 =

1
m

t
2

i=1

i=1

·

i=1

This lets us bound the expected suboptimality gap E [J( ¯w(t)) − J(w∗)] via three terms:

T1 =

,

where

E [J( ¯w(t)) − J(w∗)] ≤ T1 + T2 + T3.

The remainder of the proof is to bound these three terms separately.

3.4 Network Error Bound
We need to prove an intermediate bound ﬁrst to handle term T3.

11

(18)

(19)

(20)

(21)

(22)

Lemma 3. Fix a Markov matrix P and consider Algorithm 1 when the objective J(w) is strongly convex
we have the following inequality for the expected squared error between the iterate wi(t) at node i at time
t and the average ¯w(t) deﬁned in Algorithm 1:

(cid:114)
E(cid:104)(cid:107) ¯w(t) − wi(t)(cid:107)2(cid:105) ≤ 2L

·

√

m
b

· log(2bet2)

t

,

(23)

µ

where b = (1/2) log(1/λ2(P)).

Proof. We follow a similar analysis as others [Nedic and Ozdaglar, 2009, Prop. 3] [Duchi et al., 2011,
IV.A] [Tsianos and Rabbat, 2012]. Let W(t) be the m × d matrix whose i-th row is wi(t) and G(t) be the
m × d matrix whose i-th row is gi(t) . Then the iteration can be compactly written as

W(t + 1) = P(t)W(t) − ηtG(t)

and the network average matrix ¯W(t) = 1
P(t) = P for all t:

m 11(cid:62)W(t). Then we can write the difference using the fact that
(cid:19)

¯W(t + 1) − W(t + 1) =

11(cid:62) − I

(PW(t) − ηtG(t))

11(cid:62) − P

W(t) − ηt

11(cid:62) − I

(cid:18) 1

m

(cid:19)

G(t)

11(cid:62) − P

(PW(t − 1) − ηt−1G(t − 1))

11(cid:62) − I

G(t)

11(cid:62) − P2

W(t − 1)

11(cid:62) − P

G(t − 1)

m
11(cid:62) − I

G(t)

11(cid:62) − P2

W(t − 1)

(cid:18) 1

m

ηs

s=t−1

11(cid:62) − Pt−s

12

m

(cid:18) 1
(cid:18) 1
(cid:18) 1

=

=

m

m
− ηt

(cid:18) 1

m

=

=

− ηt−1

− ηt

(cid:18) 1
− t(cid:88)

m

(cid:18) 1

m

(cid:18) 1
(cid:18) 1

m

(cid:19)
(cid:19)

(cid:19)

(cid:19)

(cid:19)

(cid:19)

(cid:19)

(cid:19)

G(s).

(24)

Continuing the expansion and using the fact that W(1) = 0,

(cid:19)

G(s)

11(cid:62) − Pt−s

¯W(t + 1) − W(t + 1) =

11(cid:62) − Pt

m

(cid:18) 1
= − t(cid:88)
= − t−1(cid:88)
(cid:18) 1

s=1

s=1

− ηt

m

ηs

ηs

ηs

(cid:19)
W(1) − t(cid:88)
(cid:18) 1
(cid:18) 1

11(cid:62) − Pt−s

11(cid:62) − Pt−s

s=1

m

m

(cid:18) 1
(cid:19)
(cid:19)

G(s)

G(s)

(cid:19)

m
11(cid:62) − I

G(t).

(25)

Now looking at the norm of the i-th row of (25) and using the bound on the gradient norm:

(cid:107) ¯w(t) − wi(t)(cid:107)

m(cid:88)

j=1

ηs

s=1

≤

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) t−1(cid:88)
≤ t−1(cid:88)

L
µs

− (Pt−s)ij

m

(cid:18) 1
 m(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)1

j=1

1
m

(cid:19)

gj(s)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

gj(t) − gi(t)

·

+ ηt

(cid:13)(cid:13)(cid:13)(cid:13) 1
(cid:13)(cid:13)1 using a bound on the mixing rate of Markov chains (c.f. (74) in

− (Pt−s)i

2L
µt

(27)

m

+

.

(26)

L
µs

·

(cid:13)(cid:13)(cid:13)(cid:13) 1
t(cid:88)

(cid:13)(cid:13)(cid:13)(cid:13)1
t(cid:88)

τ =1

at−τ +1

=

τ

τ =1

aτ

t − τ + 1

=

(cid:17)t−s 1

.

t−1(cid:88)

(cid:16)(cid:112)λ2(P)
t(cid:88)

exp(−bτ )
t − τ + 1

τ =1

.

s=1

We handle the term(cid:13)(cid:13) 1
m − (Pt−s)i
t−1(cid:88)

Tsianos and Rabbat Tsianos and Rabbat [2012]):

− (Pt−s)i

√
≤ L

m

Deﬁne a =(cid:112)λ2(P) ≤ 1 and b = − log(a) > 0. Then we have the following identities:

s=1

s=1

m

µ

s

(28)

(29)

Now using the fact that when x > −1 we have exp(−x) < 1/(1 + x) and using the integral upper bound

13

we get

at−τ +1

τ

t(cid:88)
≤ t(cid:88)

τ =1

τ =1

≤

=

=

(1 + b)t

1

(1 + b)t

1

(1 + b)t

+

+

1

(1 + bτ )(t − τ + 1)
1

(cid:90) t
(cid:20) log(bτ + 1) − log(t − τ + 1)

(1 + bτ )(t − τ + 1)

dτ

+

1

(cid:21)t

bt + b + 1

log(bt + 1) − log(b + 1) + log(t)

τ =1

bt + b + 1

≤ log(et(bt + 1))
bt
≤ log(2bet2)

.

bt

Using (28) and (30) in (27) we get

Therefore we have

√
(cid:107) ¯w(t) − wi(t)(cid:107) ≤ L

m
√
µ
≤ 2L
µ

log(2bet2)

bt

+

2L
µt

m

log(2bet2)

.

bt

(cid:114)
E(cid:104)(cid:107) ¯w(t) − wi(t)(cid:107)2(cid:105) ≤ 2L

√

µ

m

log(2bet2)

bt

.

(30)

(31)

(32)

3.5 Bounds for expected gradient norms

3.5.1 Bounding E(cid:104)(cid:107)∇Ji( ¯w(t))(cid:107)2(cid:105)

Let βj,t ∈ ∂(cid:96)( ¯w(t)(cid:62)xi,j) denote a subgradient for the j-th point at node i and βt = (β1,t, β2,t, . . . , βn,t)(cid:62)
be the vector of subgradients at time t. Let QSi be the n × n Gram matrix of the data set Si. From the
deﬁnition of (cid:107)∇Ji( ¯w(t))(cid:107) and using the Lipschitz property of the loss functions, we have the following

14

bound:

n

≤

j∈Si

≤ 2

βj,txi,j

βj,txi,j

+ µ ¯w(t)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

(cid:107)∇Ji( ¯w(t))(cid:107)2

+ 2µ2 (cid:107) ¯w(t)(cid:107)2

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:88)
2(cid:80)
j∈Si
j∈Si
n2 β(cid:62)
=
≤ 2
n2 (cid:107)βt(cid:107)2 σ1(QSi) + 2µ2 (cid:107) ¯w(t)(cid:107)2
≤ 2L2 σ1(QSi)

(cid:80)
t QSiβt + 2µ2 (cid:107) ¯w(t)(cid:107)2

βj,tβj(cid:48),tx(cid:62)
j(cid:48)∈Si
n2

+ 2µ2 (cid:107) ¯w(t)(cid:107)2 .

i,jx(cid:48)

=

n

2

n

i,j

+ 2µ2 (cid:107) ¯w(t)(cid:107)2

We rewrite the update (13) in terms of {xi,t}, the points sampled at the nodes at time t:

¯w(t + 1) = ¯w(t)(1 − µηt) − ηt

m(cid:88)

∂(cid:96)(wi(t)(cid:62)xi,t)xi,t

.

i=1

m

Now from equation (34), after unrolling the recursion as in Shalev-Shwarz et al. Shalev-Shwartz et al. [2011]
we see

t−1(cid:88)

(cid:80)m
i=1 ∂(cid:96)(wi(τ )(cid:62)xi,τ )xi,τ

.

τ =1

m

¯w(t) =

1

µ(t − 1)

Let γi

τ ∈ ∂(cid:96)(wi(τ )(cid:62)xi,τ ) the subgradient set for the ith node computed at time τ, then we have

(33)

(34)

(35)

(36)

(37)

Let us in turn bound for each node i the term

τ =1 γi
gradient for the point sampled at time τ at node i and γi = (γi
up to time t − 1. We have

τ xi,τ
1, γi

τ ∈ ∂(cid:96)(wi(τ )(cid:62)xi,τ ) denote a sub-
t−1)(cid:62) be the vector of subgradients

(cid:107) ¯w(t)(cid:107) ≤

1

µ(t − 1)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) t−1(cid:88)

τ =1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

γi
τ xi,τ

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) t−1(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) .
(cid:13)(cid:13)(cid:13). Let γi

γi
τ xi,τ

τ =1

2, . . . , γi

i=1

· 1
m

m(cid:88)
(cid:13)(cid:13)(cid:13)(cid:80)t−1
(cid:88)
≤(cid:13)(cid:13)γi(cid:13)(cid:13)2

γi
τ γi

15

i,τ xi,τ(cid:48)

=

τ(cid:48)x(cid:62)
= (γi)(cid:62)Qi,t−1γi

τ,τ(cid:48)

σ1(Qi,t−1)

≤ (t − 1)L2σ1(Qi,t−1),

where Qi,t−1 is the (t − 1) × (t − 1) Gram submatrix corresponding to the points sampled at the i-th node
until time t − 1.

Further bounding (36):

(cid:107) ¯w(t)(cid:107)2 ≤

(cid:32)

(cid:33)2

(cid:112)(t − 1)L2σ1(Qi,t−1)
(cid:80)m
(cid:114) σ1(Qi,t−1)
m(cid:88)

(cid:33)2

i=1

m

t − 1

.

1

µ(t − 1)

(cid:32)

≤ L2
µ2

1
m

i=1

Since as stated before everything is conditioned on the sample split we take expectations w.r.t the history and
the random split and using the Cauchy-Schwarz inequality again, and the fact that the points are sampled
i.i.d. from the same distribution,

≤ L2
µ2

E(cid:104)(cid:107) ¯w(t)(cid:107)2(cid:105)
(cid:34)(cid:112)σ1(Qi,t−1)σ1(Qj,t−1)
m(cid:88)
m(cid:88)
(cid:115)
(cid:20) σ1(Qi,t−1)
m(cid:88)
m(cid:88)
(cid:20) σ1(Qi,t−1)

(cid:35)
(cid:20) σ1(Qj,t−1)

≤ L2
µ2

t − 1

t − 1

t − 1

1
m2

1
m2

E

(cid:21)

i=1

j=1

i=1

j=1

(cid:21)

E

E

=

E

L2
µ2

t − 1

.

(cid:21)

(38)

The last line follows from the expectation over the sampling model: the data at node i and node j have the
same expected covariance since they are sampled uniformly at random from the total data.

Taking the expectation in (33) and substituting (38) we have

E(cid:104)(cid:107)∇Ji( ¯w(t))(cid:107)2(cid:105) ≤ 2L2E

(cid:20) σ1(QSi)

n
+ 2L2E

(cid:21)
(cid:20) σ1(Qi,t−1)

t − 1

(cid:21)

.

(39)

Since Si is a uniform random draw from S and by assuming both t and n to be greater than 4/(3ρ2) log(d),
applying Lemma 2 gives us

E(cid:104)(cid:107)∇Ji( ¯w(t))(cid:107)2(cid:105) ≤ 20L2ρ2.

(40)

3.5.2 Bounding E(cid:104)(cid:107)∇Ji(wi(t))(cid:107)2(cid:105)

We have just as in the previous subsection

(cid:107)∇Ji(wi(t))(cid:107)2 ≤ 2L2 σ1(QSi)

n

+ 2µ2 (cid:107)wi(t)(cid:107)2 .

16

Using the triangle inequality, the fact that (a1 + a2)2 ≤ 2a2

E(cid:104)(cid:107)wi(t)(cid:107)2(cid:105) ≤ 2E(cid:104)(cid:107)wi(t) − ¯w(t)(cid:107)2(cid:105)

1 + 2a2

+ 2E(cid:104)(cid:107) ¯w(t)(cid:107)2(cid:105)

2, the bounds (31) and (38), and Lemma 2:

(41)

(42)

(43)

≤ 8L2m
µ2

log2(2bet2)
b2(t − 1)2 +

5L2ρ2

µ2

.

From (41) we can infer that for the second term to dominate the ﬁrst we require

(cid:114) 8

5

√

m
ρb

.

t

log(t)

>

This gives us

and therefore

3.6 Bound for T 2
Because the gradients are bounded,

,

µ2

E(cid:104)(cid:107)wi(t)(cid:107)2(cid:105) ≤ 10L2ρ2
E(cid:104)(cid:107)∇Ji(wi(t))(cid:107)2(cid:105) ≤ 30L2ρ2.
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) m(cid:88)
(cid:88)
E(cid:104)(cid:107)gi(t)(cid:107)2(cid:105)
m(cid:88)
(cid:88)
(cid:80)


(cid:88)
E(cid:2)gi(t)(cid:62)gj(t)(cid:3)

gi(t)(cid:62)gi(t)

gi(t)

m2
EFt−1

i(cid:54)=j

i(cid:54)=j

m2

m2

m2

i=1

m

i=1

+

i,j

i(cid:54)=j

E

= E

=

≤ L2
m

+

=

+

L2
m

E(cid:2)gi(t)(cid:62)gj(t)(cid:3)

(cid:2)E(cid:2)gi(t)(cid:62)gj(t)|Ft−1

(cid:3)(cid:3)

.

m2

17

Now using the fact that the gradients gi(t) are unbiased estimates of ∇Ji(wt) and that gi(t) and gj(t) are
independent given past history and inequality (43) for node i and j we get

EFt−1

m2

(cid:2)E(cid:2)gi(t)(cid:62)gj(t)|Ft−1
(cid:3)(cid:3)
(cid:2)∇Ji(wi(t))(cid:62)∇Jj(wj(t))(cid:3)
(cid:104)(cid:107)∇Ji(wi(t))(cid:107)2(cid:105)(cid:114)

m2

EFt−1

EFt−1

(cid:114)

EFt−1

(cid:80)

=

i(cid:54)=j

(cid:88)
≤(cid:88)

i(cid:54)=j

m2

· 30L2ρ2

i(cid:54)=j
(m − 1)

=
≤ 30L2ρ2.

m

(cid:104)(cid:107)∇Jj(wj(t))(cid:107)2(cid:105)

bt
· log(T )

t

· ρ.

18

Therefore to bound the term T2 in (22) we can use

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) m(cid:88)

i=1

E

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2 ≤ L2

m

gi(t)

m

+ 30L2ρ2.

(44)

(45)

3.7 Bound for T 3
Applying (32), (40), and (43) to T3 in (22), as well as Lemma 3 and the fact that (a1 + a2)2 ≤ 2a2
we obtain the following bound:

1 + 2a2
2

(cid:114)
E(cid:104)(cid:107) ¯w(t) − wi(t)(cid:107)2(cid:105)
m(cid:88)
(cid:114)
((cid:107)∇Ji(wi(t))(cid:107) + (cid:107)∇Ji( ¯w(t))(cid:107))2(cid:105)
E(cid:104)
m(cid:88)

log(2bet2)

i=1

m

·

· 10Lρ

T 3 ≤ 1
m

≤ 1
m

√
2L
√
m
b

µ

i=1

≤ 20L2
µ

·

(46)

3.8 Combining the Bounds
Finally combining (45) and (46) in (22) and applying the step size assumption ηt = 1/(µt):

t
2

µ

E [J( ¯w(t)) − J(w∗)]
≤ (η−1

E(cid:104)(cid:107) ¯w(t) − w∗(cid:107)2(cid:105)
t − µ)
E(cid:104)(cid:107) ¯w(t + 1) − w∗(cid:107)2(cid:105)
2
− η−1
(cid:19)
(cid:18) 30L2ρ2
E(cid:104)(cid:107) ¯w(t) − w∗(cid:107)2(cid:105)
E(cid:104)(cid:107) ¯w(t + 1) − w∗(cid:107)2(cid:105)
(cid:17)

L2
µm
· log(2bet2)

≤ µ(t − 1)
2
− µt
(cid:17)
(cid:16)
60 ·(cid:112)mρ2 · log(T )
2

µ
20L2
·

· 1
t

+

+

√

+

m
b

· ρ

t

+ K0 · L2
µt

,

(47)

, using t ≤ T and assuming T > 2be.

/b

T(cid:88)
T(cid:88)

t=1

t=1

1
T

1
T

¯w(t)

wi(t).

ˆw(T ) =

ˆwi(T ) =

(48)

(49)

(cid:16)

30ρ2 + 1/m +

where K0 =
and the average for any node i ∈ [m]

Let us now deﬁne two new sequences, the average of the average of iterates over nodes from t = 1 to T

Then summing (47) from t = 1 to T , using the convexity of J and collapsing the telescoping sum in the
ﬁrst two terms of (47),

E [J( ˆw(T )) − J(w∗)]
≤ 1
T

T(cid:88)
E(cid:104)(cid:107) ¯w(T + 1) − w∗(cid:107)2(cid:105)

E [J( ¯w(t)) − J(w∗)]

t=1

≤ − µT
2
≤ K0 · L2
µ

(cid:80)T

t=1 1/t
T

+ K0 · L2
µ

·

· log(T )

T

.

(50)

19

Now using the deﬁnition of subgradient, Cauchy-Schwarz, and Jensen’s inequality we have

J( ˆwi(T )) − J(w∗)
≤ J( ˆw(T )) − J(w∗) + ∇J( ˆwi(T ))(cid:62)( ˆwi(t) − ˆw(T ))
≤ J( ˆw(T )) − J(w∗) + (cid:107)∇J( ˆwi(T )(cid:107)(cid:107) ˆwi(t) − ˆw(T )(cid:107)
≤ J( ˆw(T )) − J(w∗)

+ (cid:107)∇J( ˆwi(T ))(cid:107) · T(cid:88)

t=1

(cid:107)wi(t) − ¯w(t)(cid:107)

T

.

(51)

in a similar way as the bound (40). First, let αi =
∂(cid:96)( ˆwi(T )(cid:62)xi) denote the subgradient for the i-th loss function of J(·) in (1), evaluated at ˆwi(T ), and
αT = (α1, α2, . . . , αN )(cid:62) be the vector of subgradients. As before,

To proceed we must bound E(cid:104)(cid:107)∇J( ˆwi(T ))(cid:107)2(cid:105)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

(cid:107)∇J( ˆwi(T ))(cid:107)2 =

Taking expectations of both sides and using (42) as before:

Taking expectations of both sides of (51) and using the Cauchy-Schwarz inequality, (50), the preceding
gradient bound, Lemma 3 and the deﬁnition of K0 we get

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

i=1

αixi + µ ˆwi(T )

N(cid:88)
N
N 2 α(cid:62)Qα + 2µ2 (cid:107) ˆwi(T )(cid:107)2
≤ 2
≤ 10L2ρ2 + 2µ2 (cid:107) ˆwi(T )(cid:107)2
≤ 10L2ρ2 + 2µ2 1
T

T(cid:88)
E(cid:104)(cid:107)∇J( ˆwi(T ))(cid:107)2(cid:105) ≤ 30L2ρ2.

t=1

(cid:107)wi(t)(cid:107)2 .

E [J( ˆwi(T )) − J(w∗)]
≤ K0 · L2
(cid:32)
µ
(cid:32)

· log(T )
√

K0 +

≤

T

2

√
2

b

·

+

30L2
µ

(cid:33)
30 ·(cid:112)mρ2 · log T
70(cid:112)mρ2 · log T
(cid:32)

+

1
m

≤

E [J( ˆwi(T )) − J(w∗)] ≤

· T(cid:88)

t=1

1
t

√
m
b

· ρ · log(T )

T

(cid:33)

· log T
T
· L2
µ

· log T
T

100(cid:112)mρ2 · log T

(cid:33)

1 − λ2(P )
· log T
T

.

1
m

+

· L2
µ

20

(52)
Recalling that b = log(1/λ2(P)) ≥ 1 − λ2(P ), assuming T > 2be and subsuming the ﬁrst term in the third
and taking expectations with respect to the sample split the above bound can be written as

30ρ2 +

b

.

(53)

4 Stochastic Communication

In this section we generalize our analysis in Theorem 1 to handle time-varying and stochastic communication
matrices P(t). In particular, we study the case where the matrices are chosen i.i.d. over time. Any strategy
that doesn’t involve communicating at every step will incur a larger gap between the local node estimates
and their average. We call this the network error. Our goal is to show how knowing ρ2 can help us balance
the network error and optimality gap.
Lemma 4. Let {P(t)} be a i.i.d sequence of doubly stochastic Markov matrices and consider Algorithm 1
when the objective J(w) is strongly convex. We have the following inequality for the expected squared error
between the iterate wi(t) at node i at time t and the average ¯w(t) deﬁned in Algorithm 1:

(cid:114)
E(cid:104)(cid:107) ¯w(t) − wi(t)(cid:107)2(cid:105) ≤ 2L
(cid:0)E(cid:2)P2(t)(cid:3)(cid:1)(cid:1).

µ

√

·

where b = log(cid:0)1/λ2

m
b

· log(2bet2)

t

.

Proof. Let us deﬁne the product of the sequence of random matrices {P(τ ) : s ≤ τ ≤ t}:

Φ(s : t) = P(t)··· P(s).

Then proceeding as in proof of Lemma 3 and using the step size ηt = 1/(µt), we get

(cid:107) ¯w(t) − wi(t)(cid:107) ≤

ηs

− Φ(s : t)ij

gj(s)

Let ei be a vector with 0’s everywhere except at the the ith position, then
− Φ(s : t)ei

− Φ(s : t)ei

mE

E

Consider the recursion u(t + 1) = P(t)u(t) and let v(t + 1) = P(t)u(t) − 1

m then we have

(cid:20)(cid:13)(cid:13)(cid:13)(cid:13) 1
E(cid:104)

m

v(t + 1)(cid:62)v(t + 1)|v(t)

since v(t) is orthogonal to the largest eigenvector of P(t).

Taking expectations w.r.t to v(t) we get

E(cid:104)(cid:107)v(t + 1)(cid:107)2(cid:105) ≤ E(cid:104)(cid:107)v(t)(cid:107)2(cid:105)

(54)

(55)

(56)

(57)

(58)

(59)

(60)

(cid:19)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

m

j=1

(cid:18) 1
m(cid:88)
 m(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13) 1

j=1

m

1
m

s=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) t−1(cid:88)
≤ t−1(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)1
(cid:21)

s=1

+ ηt

·

L
µs

gj(t) − gi(t)

(cid:13)(cid:13)(cid:13)(cid:13)1

− Φ(s : t)ei

+

2L
µt

.

.

m

(cid:21)

(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:20)(cid:13)(cid:13)(cid:13)(cid:13) 1
≤ √
= E(cid:104)
(cid:105)
(cid:105)
= v(t)(cid:62)E(cid:2)P2(t)(cid:3) v(t)
(cid:0)E(cid:2)P2(t)(cid:3)(cid:1) ,
(cid:0)E(cid:2)P2(t)(cid:3)(cid:1) .

v(t)(cid:62)P2(t)v(t)|v(t)

≤ (cid:107)v(t)(cid:107)2 λ2

λ2

21

.

(cid:13)(cid:13)2, this ﬁnally gives us

(61)

(cid:20)(cid:13)(cid:13)(cid:13)(cid:13) 1

m − Φ(s : t)i

Recursively expanding (60) we obtain

(cid:0)E(cid:2)P2(t)(cid:3)(cid:1)t−s+1

E(cid:104)(cid:107)v(t + 1)(cid:107)2(cid:105) ≤ (cid:107)v(0)(cid:107)2 λ2
Consider an initial vector u(0) = ei. We see that (cid:107)v(t + 1)(cid:107)2 =(cid:13)(cid:13) 1
(cid:13)(cid:13)(cid:13)(cid:13)1
(cid:21)
(cid:20)(cid:13)(cid:13)(cid:13)(cid:13) 1
(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:21)
E
≤ √
(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)ei − 1
(cid:0)E(cid:2)P2(t)(cid:3)(cid:1)t−s+1
≤ √
(cid:0)E(cid:2)P2(t)(cid:3)(cid:1)t−s+1
≤ √
(cid:0)E(cid:2)P2(t)(cid:3)(cid:1) and b = − log(a) we get
(cid:114)
E(cid:104)(cid:107) ¯w(t) − wi(t)(cid:107)2(cid:105) ≤ 2L

Proceeding like the proof of Lemma 4 where a = λ2

− Φ(s : t)i

− Φ(s : t)i

m

log(2bet2)

m
mE

mλ2

λ2

m

m

m

.

√

µ

.

bt

(62)

(63)

Armed with Lemma 4 we prove the following theorem for Algorithm 1 in the case of stochastic com-

munication.
Theorem 5. Let {P(t)} be an i.i.d sequence of doubly stochastic matrices and ρ2 = σ1( ˆΣ) denote the
spectral norm of the sample covariance matrix. Consider Algorithm 1 when the objective J(w) is strongly
convex, and ηt = 1/(µt). Then if the number of samples on each machine n satisﬁes

and the number of iterations T satisﬁes

n >

4
3ρ2 log (d)

(cid:112)

T > 2e log(1/

λ2(E [P2(t)]))

and

T

log(T )

> max

(cid:32)

then the expected error for the output of each node i satisﬁes

·

5

(cid:114) 8

4
3ρ2 log(d),

(cid:114) m
ρ2 ·
100(cid:112)mρ2 · log T
1 −(cid:112)λ2(E [P2(t)])

(cid:32)

E [J( ˆwi(T )) − J(w∗)]
≤

+

1
m

1

log(1/λ2(E [P2(t)]))

(cid:33)

· L2
µ

· log T
T

.

(cid:33)

,

(64)

(65)

(66)

(67)

Proof. Since (22) still holds, we merely apply Lemma 4 in (22) and continue in the same way as the proof
of Theorem 1.

22

5 Limiting Communication

As an application of the stochastic communication scenario of Theorem 5 we now analyze the effect of
reducing the communication overhead of Algorithm 1. This reduction can improve the overall running
time (“wall time”) of the algorithm because communication latency can hinder the convergence of many
algorithms in practice [Tsianos et al., 2012]. A natural way of limiting communication is to communicate
only a fraction ν of the T total iterations; at other times nodes simply perform local gradient steps.
We consider a sequence of i.i.d random matrices {P(t)} for Algorithm 1 where P(t) ∈ {I, P} with
probabilities 1 − ν and ν, respectively, where I is the identity matrix (implying no communication since
Pij(t) = 0 for i (cid:54)= j) and, as in the previous section, P is a ﬁxed doubly stochastic matrix respecting
the graph constraints. For this model the expected number of times communication takes place is simply
νT . Note that now we have an additional randomization due to the Bernoulli distribution over the doubly
stochastic matrices. Analyzing a matrix P(t) that depends on the current value of the iterates is considerably
more complicated.

A straightforward application of Theorem 5 reveals that the optimization error is proportional to 1

ν and

). However, this ignores the effect of the local communication-free iterations.

A mini-batch approach. To account for local communication free iterations we modify the intermittent
communication scheme. Instead of taking communication free steps, each node gathers (sub)gradients and
then at the communication step takes the following step:

decays as O( 1

ν · log2(T )

T

wi(t + 1) =

wj(t)Pij(t) − ηtg1/ν

i

(t).

(68)

(cid:88)

j∈Ni

Here the iteration count is over the communication steps and g1/ν
of size 1/ν. Note that this is analogous to the random scheme above but the analysis is more tractable.

(t) is the aggregated mini-batch (sub)gradient

i

Theorem 6. Fix a Markov matrix P and let ρ2 = σ1( ˆΣ) denote the spectral norm of the covariance matrix
of the data distribution. Consider Algorithm 1 when the objective J(w) is strongly convex, P(t) = P for
all t, and ηt = 1/(µt) for scheme (68). Let λ2(P) denote the second largest eigenvalue of P. Then if the
number of samples on each machine n satisﬁes



(69)

(70)

(cid:0) 8

4(cid:112)m/ρ2
(cid:1) 1

5
log(1/λ2)

n >

4
3ρ2 log (d)

and

T >

2e
ν

T

log(1/(cid:112)λ2(P))
 4

> max

log(νT )

3νρ2 log(d),

1
ν

>

4

3ρ2 · log(d)

23

and then the expected error for each node i satisﬁes

(cid:32)

E [J( ˆwi(T )) − J(w∗)]
≤

+ 200

5 ·

√

1
m

(cid:112)mρ4 · log(νT )
1 − √
λ2
· log(νT )

.

(cid:33)

(71)

· L2
µ

T

obtain a sharper bound on the error by a factor of ρ. This is signiﬁcantly better than a O((cid:112)mρ2 · log νT

where ν is the frequency of communication and where λ2 = λ2(P).
Remark: Theorem 6 suggests that if the inverse frequency of communication is large enough than we can
νT )

baseline guarantee from a direct application of Theorem 1 when the number of iterations is νT .
Additionally the result suggests that if we communicate on a mini batch(where batch size b = 1/ν) that
is large enough we can improve Theorem 1, speciﬁcally now we get a 1/m improvement when m ≤ 1/ρ4/3.

5.1 Proof of Theorem 6
Proof. We will ﬁrst establish the network lemma for scheme (68).
Lemma 7. Fix a Markov matrix P and consider Algorithm 1 when the objective J(w) is strongly convex
and the frequency of communication satisﬁes

1/ν >

4
3ρ2 log(d)

(72)

we have the following inequality for the expected squared error between the iterate wi(t) at node i at time
t and the average ¯w(t) deﬁned in Algorithm 1 for scheme (68)

E(cid:104)(cid:107) ¯w(t) − wi(t)(cid:107)2(cid:105) ≤ 4L(cid:112)5mρ2
(cid:114)

· log(2bet2)

bt

µ

where b = (1/2) log(1/λ2(P)).

Proof. It is easy to see that we can write the update equation in CoSCO (Algorithm 1),

(73)

(74)

(75)

(76)

where

and gi(t) = g1/ν

i

(t) + µwi(t).

We need ﬁrst a bound on

(s)

wi(t + 1) =

˜Pij(t) =

j=1

(t)

i

Pii(t) − 1

˜Pij(t)wj(t) − ηtg1/ν

when i (cid:54)= j
mt when i = j

m(cid:88)
(cid:26) Pij(t)
(cid:13)(cid:13)(cid:13) using the deﬁnition of the minibatch (sub)gradient:
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:80)
(cid:13)(cid:13)(cid:13)2
≤ L2ν(cid:13)(cid:13)Q1/ν

∂(cid:96)(wi(s)(cid:62)xkis

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

iks∈H i

)xkis

(cid:13)(cid:13)

1/ν

s

(s)

=

(cid:13)(cid:13)(cid:13)g1/ν
(cid:13)(cid:13)(cid:13)g1/ν

j

i

24

From (27) and the minibatch (sub)gradient bound

g1/ν
j

(t)

(s)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)
ν(cid:13)(cid:13)Q1/ν
(cid:13)(cid:13)(cid:13)(Pt−s)i − ( ˜Pt−s)i
(cid:13)(cid:13)(cid:13)1

(cid:113)

2L

µt

+

µs

(cid:19)

− ( ˜Pt−s)ij

(cid:107) ¯w(t) − wi(t)(cid:107)

≤

≤ L

≤ L

m

ηs

j=1

j=1

s=1

+ ηt

1
m

g1/ν
j

(cid:13)(cid:13)(cid:13) 1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) t−1(cid:88)
(cid:18) 1
m(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
 m(cid:88)
(cid:113)
(cid:13)(cid:13) t−1(cid:88)
ν(cid:13)(cid:13)Q1/ν
(cid:113)
(cid:13)(cid:13)
ν(cid:13)(cid:13)Q1/ν
(cid:13)(cid:13) 1
t−1(cid:88)
(cid:113)
ν(cid:13)(cid:13)Q1/ν
(cid:13)(cid:13)
(cid:13)(cid:13) 1
(cid:113)
(cid:13)(cid:13) t−1(cid:88)
ν(cid:13)(cid:13)Q1/ν

2L

s=1

s=1

µt

+

s=1

≤ 2L

(t) − g1/ν

i

m − ( ˜Pt−s)i

µs

(cid:13)(cid:13)1 +

m − (Pt−s)i

(cid:13)(cid:13)1

+

(cid:113)
ν(cid:13)(cid:13)Q1/ν

2L

(cid:13)(cid:13)

µt

m − (Pt−s)i

µs

Continuing as in the proof of Lemma 3, taking expectations and using Lemma 2, for 1/ν > 4

3ρ2 log(d)

we have

(cid:113)
mνE(cid:2)(cid:13)(cid:13)Q1/ν(cid:13)(cid:13)(cid:3)
(cid:114)
E(cid:104)(cid:107) ¯w(t) − wi(t)(cid:107)2(cid:105) ≤ 4L
≤ 4L(cid:112)5mρ2

log(2bet2)

µ

µ

bt

log(2bet2)

bt

(77)

For the scheme (68) all the steps until bound (22) from proof of Theorem 5 remain the same. The
difference in the rest of the proof arises primarily from the mini batch gradient norm factor in Lemma 7.
We have the same decomposition as (22) with T1, T2, and T3 as in (19), (20), and (21). The gradient
norm bounds also don’t change since the minibatch gradient is also an unbiased gradient of the true gradient
∇J(·). Thus substituting Lemma 7 in the above and following the same steps as in proof of Theorem 5,
replacing T by νT where T is now the total iterations including the communication as well as the minibatch
gathering rounds, we get Theorem 6.

6 Inﬁnite Data
In this section we explore the sub-optimality of distributed primal averaging when T → ∞ for the case of
smooth strongly convex objectives. As discussed before the results of Section 3 do not explain the behaviour

25

of Algorithm 1 when each node has inﬁnite data, as we expect to gain from adding more machines in any
network. Towards that goal we investigate the behaviour of CoSCO in the asymptotic regime and show
that the network effect disappears and we can gain from more machines in any network.

Our analysis depends on the asymptotic normality of a variation of Algorithm 1 (See Thm. 5 of Bianchi

et al. [2013]) and we present it here for completeness in Algorithm 2.
Algorithm 2 Consensus Strongly Convex Optimization - CoSCO-∞

Input: {xi,j},where i ∈ [m] and j ∈ [n] and N = mn, µ > 0, T ≥ 1
{Each i ∈ [m] executes}
Initialize: set wi(1) = 0 ∈ Rd.
for t = 1 to T do

Sample gi(t) an unbiased estimate of the true gradient
wi(t + 1) = wi(t) − ηtgi(t)

˜wi(t + 1) =(cid:80)

j∈N (i) wj(t + 1)Pij(t + 1)

end for
Output: ˜wi(T ) for any i ∈ [m].

The main differences between CoSCO and CoSCO-∞ are that we average the iterates after making
the local update and each node has access to an inﬁnite reservoir of samples. Thus the sampled gradient at
each node is an unbiased estimate of the gradient of the objective function in (1).
We make the following assumptions for the analysis in this section: (1) The loss function differentials
{∂ ((cid:96)(·))} are differentiable and G-Lipschitz for some G > 0, (2) the stochastic gradients are of the form
gi(t) = ∇J(wi(t)) + ξt where E[ξt] = 0 and E[ξtξ(cid:62)
t ] = C, and (3) there exists p > 0 such that
< ∞. Our results hold for all smooth strongly convex objectives not necessarily dependent on

E(cid:104)(cid:107)ξt(cid:107)2+p(cid:105)

w(cid:62)x.
Lemma 8. Fix a Markov matrix P. Consider Algorithm 1 when the objective J(w) is strongly convex and
twice differentiable, P(t) = P for all t, and ηt = 1/(λt). then the expected error for each node i satisﬁes
for a arbitrary split of N samples into m nodes

J

 m(cid:88)

j=1

 − J(w∗)



Pijwj(T )

T · E

lim sup
T→∞

≤ (cid:88)

j∈N (i)

(Pij)2 · Tr (H) · G
µ

where H is the solution to the equation

∇J 2(w∗)H + H∇J 2(w∗)T = C.

Remark: This result shows that asymptotically the network effect from Theorem 5 disappears and that
additional nodes can speed convergence.

An application of Lemma 8 to problem (1) gives us the following result for the specialized case of a

complete graph with constant weight matrix P.

26

(78)

(79)

Theorem 9. Consider Algorithm 1 when the objective J(w) has the form 1 , P(t) = P and corresponds to
a complete graph with uniform weights for all t, and ηt = 1/(λt). then the expected error for each node i
satisﬁes

T · E

J
 m(cid:88)
· Tr(cid:0)∇2J(w∗)−1(cid:1) · G

Pijwj(T )

j=1

µ

lim sup
T→∞
≤ 25ρL2
m

 − J(w∗)



(80)

(81)

where the expectation is with respect to the history of the sampled gradients as well as the uniform random
splits of N data points across m machines.

Remark: For objective 1 we obtain a 1/m variance reduction and the network effect disappears.

6.1 Proof of Lemma 8
Proof. In the proof we will ﬁrst show that the iterate of Algorithm 1 is asymptotically normal by showing it
is close to the iterate of Algorithm 2 and then use the corresponding multivariate normality result of Bianchi
et al. [Bianchi et al., 2013, Theorem 5]. Finally using smoothness and strong convexity we shall get Lemma
8.

We need to verify that Algorithm 1 satisﬁes all the assumptions necessary (Assumptions 1, 4, 6, 7, 8a,

and 8b in Bianchi et al. Bianchi et al. [2013]) for the result to hold.

• Assumption 1 requires the weight matrix P(t) to be row stochastic almost surely, identically dis-
tributed over time, and that E[P(t)] is column stochastic. Our Markov matrix is constant over time
and doubly stochastic. Assumption 1b follows because P is constant and independent of the stochastic
gradients, which are sampled uniformly with replacement.

• Assumption 4 requires square integrability of the gradients as well as a regularity condition. In our

setting, this follows since the sampled gradients are bounded almost everywhere.

• Assumption 6 imposes some analytic conditions at the optimum value. These hold since the gradient
is assumed to be differentiable and the Hessian matrix at w∗ is positive deﬁnite with its smallest
eigenvalue is at least µ > 0 (this follows from strong convexity).

• Assumption 7 of Bianchi et al. Bianchi et al. [2013] follows from our existing assumptions.
• Assumptions 8a and 8b are standard stochastic approximation assumptions on the step size that are

easily satisﬁed by ηt = 1
µt.

It is easy to show that the average over the nodes of the iterates ˜wi(t), wi(t) for CoSCO and CoSCO-

∞ respectively are the same and satisfy

(cid:80)m
(cid:80)m

i=1 gi(t)

i=1 gi(t)

m

m

¯˜w(t + 1) = ¯˜w(t) − ηt
¯wi(t + 1) = ¯wi(t + 1) − ηt

27

Now note that

wi(t) − w∗ = wi(t) − ¯wi(t)
T1=Network Error

+

(cid:125)

¯wi(t) − w∗

(cid:123)(cid:122)

(cid:125)

(cid:124)

T2=Asymptotically Normal

(82)

(cid:124)

(cid:123)(cid:122)

From Lemma 3 we know that the network error (T1) decays and from update equation (81) we know that
the averaged iterate for CoSCO and CoSCO-∞ are the same. Then the proof of Theorem 5 of Bianchi et
al. Bianchi et al. [2013] shows that the term T2, under the above assumptions when appropriately normalized
converges to a centered Gaussian distribution. Equation (82) then implies

√

µt (wi(t) − w∗) ∼ N (0, H)

j=1 Pijwj(t). It is easy to see that for a differen-

(83)

(84)

(85)

(86)

(87)

(88)

(89)

(90)

where H is the solution to the equation

Let Y ∼ N (0, I), so we can always write for any X ∼ N (0, H)

∇J 2(w∗)H + H∇J 2(w∗)T = C.

X = YH1/2,

and thus

= Tr(H)

J

Pijwj(t)

(cid:107)X(cid:107)2 = Y(cid:62)HY.

 m(cid:88)

j=1

tiable and strongly convex function

2
Now it is easy to see from (83) that for a node j ∈ N (i)

Then it is well known that (cid:107)X(cid:107)2 ∼ χ2(Tr(H)) and so E(cid:104)(cid:107)X(cid:107)2(cid:105)
Let us now consider the suboptimality at the iterate(cid:80)m
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) m(cid:88)
 − J(w∗) ≤ G
µt (wj(t) − w∗) ∼ N(cid:0)0, (Pij)2H(cid:1) .
 (cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

This implies that (cid:88)

µt (wj(t) − w∗) ∼ N

0,

Pij

µt (wj(t) − w∗)

√

Pij

√

Pij

j∈N (i)

j∈N (i)

√

j=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:88)
 (cid:88)

j∈N (i)

j∈N (i)

E

=

 Tr (H) .

(Pij)2

28

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

Pijwj(t) − w∗

.

 H
 .

(Pij)2

Then taking expectation w.r.t to the distribution (89) and using standard properties of norms of multivariate
normal variables,

Then substituting in bound (87) and taking the limit we ﬁnally get

J

 m(cid:88)

j=1

 − J(w∗)



Pijwj(T )

T · E

lim sup
T→∞

≤ (cid:88)

j∈N (i)

(Pij)2 · Tr (H) · G
µ

.

(91)

6.2 Proof of Theorem 9
Proof. The the covariance of the gradient noise under the sampling with replacement model is

C = E(cid:104)
gi(t)gi(t)(cid:62)(cid:105) − ∇J(wi(t))∇J(wi(t))(cid:62)
(cid:80)N

N(cid:88)

i=1 βi,txixT
i

+

N

=
+ µ2wi(t)wi(t)(cid:62) − ∇J(wi(t))∇J(wi(t))(cid:62)

βi,t

i=1

(cid:16)

µ
N

xiwi(t)(cid:62) + wi(t)x(cid:62)

i

Thus we can bound the spectral norm of C as

σ1(C) ≤ L2ρ2 + 2µLE [(cid:107)wi(t)(cid:107)] + µ2E(cid:104)(cid:107)wi(t)(cid:107)2(cid:105)

(cid:17)

(92)

(93)

Now from bound (42) since T → ∞ we have

+ E(cid:104)(cid:107)∇J(wi(t))(cid:107)2(cid:105)
E(cid:104)(cid:107)wi(t)(cid:107)2(cid:105) ≤ 10L2ρ2
E(cid:104)(cid:107)∇Ji(wi(t))(cid:107)2(cid:105) ≤ 30L2ρ2

µ2

Putting everything together we get

Next note that H = C(cid:0)∇2J(w∗)(cid:1)−1 /2. From the completeness and uniform weight assumptions on

σ1(C) ≤ 50ρL2.

(94)

the graph, we have

.

(cid:88)

j∈N (i)

(95)

(Pij)2 =

1
m

29

Table 1: Data sets and parameters for experiments

training
data set
RCV1
781, 265
Astro-ph
29, 882
Alpha
100, 000
Covertype 522, 911

test

23, 149
32, 487
50, 000
58, 001

dim.
47, 236
99, 757

500

47, 236

λ
10−4
5 × 10−5
10−4
10−6

ρ2
0.01
0.01
0.35
0.21

Thus substituting in Lemma 8, using (94) gives us

 − J(w∗)


Pijwj(t)

J
 m(cid:88)
(cid:16)(cid:0)C∇2J(w∗)(cid:1)−1(cid:17)
· Tr(cid:0)∇2J(w∗)−1(cid:1) · G

j=1

2

· G
µ

µ

lim sup

t→∞ t · E

≤ 1
m

· Tr
≤ 25ρL2
m

(96)

7 Experiments

Our goals in our experimental evaluation are to validate the theoretical dependence of the convergence rate
on ρ2.

7.1 Data sets, tasks, and parameter settings
The data sets used in our experiments are summarized in Table 7.1. Covertype is the forest covertype
dataset Blackard and Dean [1999] used in Shalev-Shwartz et al. [2011] obtained from the UC Irvine Machine
Learning Repository Lichman [2013], Astro-ph is comprised of abstracts of papers from physics also
of Shalev-Shwartz et al. [2011], rcv1 is from the Reuters collection Amini et al. [2009] obtained from
libsvm collection Chang and Lin [2011]. Alpha is a dense dataset obtained from the Pascal large scale
learning challenge alp. The RCV1 and Astro-ph data sets have small values of ˆρ2, whereas Alpha and
Covertype have larger values of ˆρ2. In all the experiments we looked at (cid:96)2-regularized classiﬁcation
objectives for problem (1). Each plot is averaged over 5 runs.
The data consists of pairs {(x1, y1), . . . , (xN , yN )} where xi ∈ Rd and yi ∈ {−1, +1}. We performed
experiments on two objectives. To analyze the effect of communication chose the hinge loss (cid:96)(w(cid:62)x) =
(1 − w(cid:62)xy)+. The values of the regularization parameter µ are chosen from to be the same as those in
Shalev-Shwarz et al. Shalev-Shwartz et al. [2011].
We simulated networks of compute nodes of varying size (m) arranged in a k-regular graph with k =
(cid:98)0.25m(cid:99) or a ﬁxed degree (not dependent on m). Note that the dependence of the convergence rate of
procedures like CoSCO (1) on the properties of the underlying network has been investigated before and
we refer the reader to Agarwal and Duchi Agarwal and Duchi [2011] for more details. In this paper we

30

Figure 1:
decay for increasing m is worse for larger ρ2. (Covertype with ρ2 = 0.21 and RCV1 with ρ2 = 0.013)

Iterations of CoSCO till  = 0.01 error on datasets with very different ρ2. The performance

experiment only with k-regular graphs. The weights on the Markov matrix P are set by the Metropolis-
Hastings weights [Boyd et al., 2004]).

 min{1/di, 1/dj}
(cid:80)

0

(i, j) ∈ E
(i, j) /∈ E

Pij =

(i,k)∈E max{0, 1/di − 1/dk} i = j

where di is the degree of node i. Each node is randomly assigned n = (cid:98)N/m(cid:99) points. For better perfor-
mance we could optimize the weights on each edge [Bijral and Srebro, 2009].

7.2 Performance of as a function of ρ2
In the ﬁrst set of experiments we look at the number of iterations till  = 0.01 error as we vary the number
of machines. We looked at the objective after a ﬁxed number of iterations and computed the number of
iterations for different ms to reach this point. The network considered is a bounded degree expander with
Metropolis-Hastings weights. We can see in Figure 1 that the iterations for dataset with a smaller ρ2 grow
slower as we spread the data across more machines.

The ﬁrst question we answer pertains to the competitiveness of Algorithm 1 in a real setting. We show
that the algorithm is competitive with a recently proposed method and the corresponding convergence guar-
antees give us a theoretical understanding of when it is expected to perform better.

To do so we compare Algorithm 1 to the distributed method presented in [Tsianos and Rabbat, 2012](re-
ferred to here as DOGD- Distributed Online Gradient Descent). They are quite similar and differ essen-
tially in how the step-sizes decay (constant over epoch and decaying vs strictly decaying). As shown in
the theoretical results we expect Algorithm 1 to perform better on datasets with a smaller spectral norm
(Astro-ph and RCV1) and we can see in Figure 2 that is the case, while on Covertype DOGD per-
forms slightly better (especially on test error). In general, we can see that tuning Algorithm 1 appropriately
gives competitive performance.

31

Machines16   3264128256512Iterations754285776030800782824710861292896550Covertype Iterations till 0.01 ErrorMachines16    32    64128256512Iterations216427323598465652689884RCV Iterations till 0.01 ErrorFigure 2: Performance of CoSCO with intermittent communication scheme on datasets with very different
ρ2. The algorithm works better for smaller ρ2. (Covertype with ρ2 = 0.21 and RCV1 with ρ2 = 0.013)

Inifnite Data

7.3
To provide some empirical evidence of Lemma 8 we generate a very large (N = 108) synthetic dataset from
a multivariate Normal distribution and created a simple binary classiﬁcation task using a random hyperplane.
As we can see in ﬁgure 3 for the SVM problem and a k-regular network we continue to gain as we add more
machines and then eventually we stabilize but never lose from more machines.

8 Discussion and Implications

In this paper we described a distributed primal-averaging stochastic gradient descent algorithm, called
CoSCO, and analyzed its performance in terms of the spectral norm ρ2 of the data covariance matrix under
a homogenous assumption. In the consensus problem this setting has not been analyzed before and existing
work corresponds to weaker results when this assumption holds.

For certain strongly convex objectives we showed that the objective value gap between any node’s iterate
and the optimum centralized estimate decreases as O(log2(T )/T ); crucially, the constant depended on ρ2
and the spectral gap of the network matrix. We showed how limiting communication can improve the
total runtime and reduce network costs by extending our analysis with a similar data dependent bound.
Moreover we show that in the asymptotic regime the network penalty disappears. Our analysis suggests that
distribution-dependent bounds can help us understand how data properties can mediate the tradeoff between
computation and communication in distributed optimization. In a sense, data distributions with smaller ρ2
are easier to optimize over in a distributed setting. This set of distributions includes sparse data sets, an
important class for applications.

In the future we will extend data dependent guarantees to serial algorithms as well as the average-at-
end scheme [Zhang et al., 2012, Shamir et al., 2014]. Extending our ﬁxed batch-size to random size can
help us understand the beneﬁt of communication-free iterations. Finally, we can also study the impact of
asynchrony and more general time-varying topologies.

32

00.20.40.60.811.21.41.61.82x 106−50510log(Primal Objective)Covertype − (m=128)  DiSCODOGD00.20.40.60.811.21.41.61.82x 1060.20.30.4TTest Error  DiSCODOGD00.20.40.60.811.21.41.61.82x 104−1.5−1−0.50log(Primal Objective)RCV − (m=128)  DiSCODOGD00.20.40.60.811.21.41.61.82x 1040.060.070.080.090.10.11TTest Error  DiSCODOGDFigure 3: No network effect in the case of inﬁnite data.

References

URL http://largescale.ml.tu-berlin.de/about/.

A. Agarwal and J. C. Duchi. Distributed delayed stochastic optimization. In J. Shawe-Taylor, R. Zemel,
P. Bartlett, F. Pereira, and K. Weinberger, editors, Advances in Neural Information Processing Systems 24,
pages 873–881, 2011. URL http://books.nips.cc/papers/files/nips24/NIPS2011_
0574.pdf.

M.-R. Amini, N. Usunier, and C. Goutte. Learning from multiple partially observed views - an application

to multilingual text categorization. In Advances in Neural Information Processing Systems 22, 2009.

P. Bianchi, G. Fort, and W. Hachem. Performance of a distributed stochastic approximation algorithm. IEEE
Transactions on Information Theory, 59(11):7405–7418, 2013. doi: 10.1109/TIT.2013.2275131. URL
http://dx.doi.org/10.1109/TIT.2013.2275131.

A. S. Bijral and N. Srebro. On doubly stochastic graph optimization.

In NIPS Workshop on Analyzing

Networks and Learning with Graphs, 2009.

J. A. Blackard and D. J. Dean. Comparative accuracies of artiﬁcial neural networks and discriminant analysis
in predicting forest cover types from cartographic variables. Computers and Electronics in Agriculture, 24
(3):131–151, December 1999. doi: 10.1016/S0168-1699(99)00046-0. URL http://dx.doi.org/
10.1016/S0168-1699(99)00046-0.

33

00.511.522.5x 104−3−2.5−2−1.5−1TLog(Objective)Infinite Data (Disco−∞)  m=4m=8m=16m=32m=64m=128m=256m=512S. Boyd, L. Xiao, and P. Diaconis.

SIAM Review, 46
(4):667–689, 2004. doi: 10.1137/S0036144503423264. URL http://dx.doi.org/10.1137/
S0036144503423264.

Fastest mixing markov chain on a graph.

J. K. Bradley, A. Kyrola, D. Bickson, and C. Guestrin.

l1-
the 28th
regularized loss minimization.
International Conference on Machine Learning, volume 28 of JMLR Workshop and Conference
Proceedings, 2011. URL http://www.select.cs.cmu.edu/publications/paperdir/
icml2011-bradley-kyrola-bickson-guestrin.pdf.

Parallel coordinate descent
In L. Getoor and T. Scheffer, editors, Proceedings of

for

C.-C. Chang and C.-J. Lin. Libsvm: A library for support vector machines. ACM Trans. Intell. Syst.
Technol., 2(3):27:1–27:27, May 2011. doi: 10.1145/1961189.1961199. URL http://dx.doi.org/
10.1145/1961189.1961199.

A. Cotter, O. Shamir, N. Srebro,
gradient methods.

via
F. Pereira,
Systems
4432-better-mini-batch-algorithms-via-accelerated-gradient-methods.

algorithms
P. Bartlett,
Information Processing
URL http://papers.nips.cc/paper/

and K. Weinberger,
1647–1655,

In
editors,
2011.

Shawe-Taylor,

R. Zemel,

J.
Advances

and K. Sridharan.

Better mini-batch

accelerated

24,

pages

in Neural

O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction using mini-
batches. Journal of Machine Learning Research, 13:165–202, January 2012. URL http://jmlr.
org/papers/v13/dekel12a.html.

J. Duchi, A. Agarwal, and M. Wainwright. Dual averaging for distributed optimization: Convergence anal-
ysis and network scaling. IEEE Transactions on Automatic Control, 57(3):592–606, March 2011. doi:
10.1109/TAC.2011.2161027. URL http://dx.doi.org/10.1109/TAC.2011.2161027.

M. Lichman. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/ml.

J. Liu, S. J. Wright, C. Re, V. Bittorf, and S. Sridhar. An asynchronous parallel stochastic coordinate descent
algorithm. In L. Getoor and T. Scheffer, editors, Proceedings of the 31st International Conference on
Machine Learning, volume 32 of JMLR Workshop and Conference Proceedings, 2014. URL http:
//jmlr.org/proceedings/papers/v32/liud14.pdf.

A. Nedic and A. Ozdaglar. Distributed subgradient methods for multi-agent optimization.

IEEE Trans-
actions on Automatic Control, 54(1):48–61, January 2009. doi: 10.1109/TAC.2008.2009515. URL
http://dx.doi.org/10.1109/TAC.2008.2009515.

A. Rakhlin, O. Shamir, and K. Sridharan. Making gradient descent optimal for strongly convex stochastic
optimization. extended version of ICML paper arXiv:1109.5647 [cs.LG], ArXiV, 2012. URL http:
//arxiv.org/abs/1109.5647.

S. S. Ram, A. Nedic, and V. V. Veeravalli.

Distributed stochastic subgradient projection algo-
rithms for convex optimization. Journal of Optimization Theory and Applications, 147(3):516–545,
December 2011.
doi: 10.1007/s10957-010-9737-7. URL http://dx.doi.org/10.1007/
s10957-010-9737-7.

34

S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan. Stochastic convex optimization. In P. Bartlett,
F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, Proceedings Conference on Learning Theory,
2009.

S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter.

Pegasos: Primal Estimated sub-GrAdient
SOlver for SVM. Mathematical Programming, Series B, 127(1):3–30, October 2011. doi: 10.1007/
s10107-010-0420-4. URL http://dx.doi.org/10.1007/s10107-010-0420-4.

O. Shamir, N. Srebro, and T. Zhang. Communication-efﬁcient distributed optimization using an approx-
imate newton-type method. In E. P. Xing and T. Jebara, editors, Proceedings of the 31st International
Conference on Machine Learning, volume 32 of JMLR Workshop and Conference Proceedings, pages
1000–1008, 2014. URL http://jmlr.org/proceedings/papers/v32/shamir14.html.

M. Tak´aˇc, A. Bijral, P. Richt´arik, and N. Srebro. Mini-batch primal and dual methods for SVMs.

In
S. Dasgupta and D. McAllester, editors, Proceedings of the 30th International Conference on Machine
Learning (ICML), volume 28 of JMLR Workshop and Conference Proceedings, pages 1022–1030, 2013.
URL http://jmlr.org/proceedings/papers/v28/takac13.html.

J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational Math-
ematics, 12(4):389–434, August 2012. doi: 10.1007/s10208-011-9099-z. URL http://dx.doi.
org/10.1007/s10208-011-9099-z.

K. I. Tsianos and M. G. Rabbat.

Distributed strongly convex optimization.

Technical Report

arXiv:1207.3031 [cs.DC], ArXiV, July 2012. URL http://arxiv.org/abs/1207.3031.

K. I. Tsianos, S. Lawlor, and M. G. Rabbat. Communication/computation tradeoffs in consensus-based
In P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors,

distributed optimization.
Advances in Neural Information Processing Systems 25, pages 1952–1960, 2012.

L. Yu, N. Wang, and X. Meng. Real-time forest ﬁre detection with wireless sensor networks. In Wireless
Communications, Networking and Mobile Computing Proceedings, volume 2, pages 1214 – 1217, 2005.
doi: 10.1109/WCNM.2005.1544272.

Y. Zhang, J. Duchi, and M. Wainwright. Communication-efﬁcient algorithms for statistical optimization. In
P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, Advances in Neural Information
Processing Systems 25, pages 1511–1519, 2012. URL http://books.nips.cc/papers/files/
nips25/NIPS2012_0716.pdf.

35

