6
1
0
2

 
r
a

 

M
9
1

 
 
]

G
L
.
s
c
[
 
 

1
v
8
3
0
6
0

.

3
0
6
1
:
v
i
X
r
a

Tensor Methods and Recommender Systems†

Evgeny Frolov‡

Ivan Oseledets‡¶

Abstract

A substantial progress in development of new and eﬃcient tensor fac-
torization techniques has led to an extensive research of their applicability
in recommender systems ﬁeld. Tensor-based recommender models push
the boundaries of traditional collaborative ﬁltering techniques by taking
into account a multifaceted nature of real environments, which allows to
produce more accurate, situational (e.g. context-aware, criteria-driven)
recommendations. Despite the promising results, tensor-based methods
are poorly covered in existing recommender systems surveys. This survey
aims to complement previous works and provide a comprehensive overview
on the subject. To the best of our knowledge, this is the ﬁrst attempt to
consolidate studies from various application domains in an easily readable,
digestible format, which helps to get a notion of the current state of the
ﬁeld. We also provide a high level discussion of the future perspectives
and directions for further improvement of tensor-based recommendation
systems.

Keywords:

collaborative ﬁltering, tensor factorization, tensor decomposi-

tions, context-aware recommender systems

1

Introduction

We live in the era of data explosion. With rapid development of an internet
technologies and constant growth of accessibility of the World Wide Web the
amount of information generated by humans increases exponentially. This fast
and easy access to enormous digital space creates a lot of opportunities for all
people around the globe, drives markets and businesses, positively impacts many
diﬀerent aspects of life, but also leads to a new kinds of problems. The term “fear
of missing out” (FoMO) emerged from a fear of missing some novel experiences
or opportunities which leads to an ongoing attempts to stay connected, swiping
through numerous sources of information. No doubt, it would be impossible for
us to analyze, manage and navigate through all these endless bits of data ocean
‡Skolkovo Institute of Science and Technology, Nobel St. 3, Skolkovo Innovation Center,
¶Institute of Numerical Mathematics Russian Academy of Sciences, Gubkina St. 8, 119333
†This work was supported by Russian Science Foundation grant 14-1100659

Moscow, 143025 Moscow Region, Russia; {i.oseledets,evgeny.frolov}@skolkovotech.ru

Moscow, Russia

1

without any help of intelligent systems which are able to process and ﬁlter huge
amounts of information in just a blink of an eye.

The need for an information categorization and ﬁltering was already recog-
nized in late 1970s in the Usenet, a distributed discussion platform, founded at
Duke University. One of its goals was to help users to maintain numerous posts
by grouping them into newsgroups. However, an active research on the topic
of information ﬁltering started in 1990s. The general term Recommender Sys-
tems (RS) was brought to the academia in the mid-90’s with works of Resnick,
Hill, Shardanand and Maes [4] and was preceded by several famous projects:
Tapestry, Lotus Notes, GroupLens [12]. A signiﬁcant boost in RS research
started after a famous Netﬂix prize competition with US$1 million award for
the winners, announced back in 2006. This has not only attracted a lot of at-
tention from scientists and engineers, but also depicted the great interest from
an industry.

Conventional RS deal with two major types of entities which are typically
users (e.g. customers, consumers) and items (e.g. products, resources). Users
interact with items by viewing or purchasing them, assigning ratings, leaving
text reviews, placing likes or dislikes, etc. These interactions, also called events
or transactions, create an observation history, typically collected in a form of
transaction/event log, that reﬂects the relations between users and items. Rec-
ognizing and learning these relations in order to predict new possible interac-
tions is one of the key goals of RS.

As we will see further, the deﬁnition of entities is not limited to users and
items only. Entities can be practically of any type as long as predicting new
interactions between them may bring a valuable knowledge and/or help to make
better decisions. In some cases entities can be even of the same type, like in
the task of predicting new connections between people in a social network or
recommending relevant paper citations for a scientiﬁc papers.

Modern recommender models may also have to deal with more than 2 types
of entities within a single system. For instance, users may want to assign tags
(e.g. keywords) to the items they like. Tags become the third type of entity, that
relates to both users and items, as it represents the user motivation and clariﬁes
items relevance (more on that in Section 5.2). Time can be another example of
an additional entity, as both user preferences and items relevance may evolve
in time (see Section 5.3). Taking into account these multiple relations between
several entities typically helps to provide more relevant, dynamic and situational
recommendations.
It also increases complexity of RS models, which in turn
brings new challenges and opens the door for new types of algorithms, such as
tensor factorization (TF) methods.

The topic of building a production-ready recommender system is very broad
and includes not only algorithms but also concerns a lot about business logic,
dataﬂow design, integration with infrastructure, service delivery and user ex-
perience. This also may require a speciﬁc domain knowledge and always needs
a comprehensive evaluation. Speaking about the latter, the most appropriate
way of assessing RS qualty is an online A/B testing and massive user studies
[36, 26, 44], which are typically not available right at hand in academia. In this

2

work we will only touch mathematical and algorithmic aspects which will be
accompanied with examples from various application domains.

The rest of the survey is divided into the following parts: Sections 2 and
3 cover general concepts and major challenges in RS ﬁeld; Section 4 gives a
brief introduction to tensor-related concepts, that are essential for understand-
ing how tensors can be used in RS models; Section 5 contains a comprehensive
overview of various tensor-based techniques with examples from diﬀerent do-
mains; Section 6 concludes the review and provides thoughts on possible future
directions.

2 Recommender systems at a glance

Let us consider without loss of generality a task of product recommendations.
The main goal of this task is, given some prior information about users and
items (products), try to predict what particular items will be the most relevant
to a selected user. The relevance is measured with some relevance score (or
utility) function fR that is estimated from the user feedbacks. More formally,

fR : U ser × Item → Relevance Score,

(1)

where U ser is a domain of all users, Item is a domain of all items. The feedback
can be either explicit or implicit, depending on whether it is directly provided
by a user (e.g. ratings, likes/dislikes, etc.) or implicitly collected through an
observation of his/her actions (e.g. page clicks, product purchases, etc.).

The type of prior information available in RS model deﬁnes what class of
techniques will be used for building recommendations. If only an observation
history of interactions can be accessed, then this is a task for collaborative
ﬁltering (CF) approach. If RS model uses intrinsic properties of items as well as
proﬁle attributes of users in order to ﬁnd the best matching (user, item) pairs,
then this is a content-based (CB) approach.

The complete overview of RS methods and challenges is out of scope of this

survey and for a deeper introduction we refer the reader to [26, 10, 81, 72].

2.1 Content-based ﬁltering

As already mentioned, the general idea behind the CB approach is to use some
prior knowledge about users preferences and items’ properties in order to gen-
erate the most relevant recommendations. One of its main advantages is the
ability to alleviate the cold start problem (see Section 3.1) as long as all the
needed content information is collected. Recommendations can be produced
instantly even for those items that were never recommended to any user before.
This approach also has a number of issues, among which are the limited
content analysis, over-specialization and high sensitivity to users input [4, 53].
The key drawback from practical viewpoint is a diﬃculty of gathering descriptive
and thorough properties of both items and users. This can be either manually
done by humans, i.e. with help of users and/or domain experts, or extracted

3

automatically with data processing tools. The former method is usually very
time consuming and requires considerable amount of work before RS can be
built up. The latter method is highly dependent on IR algorithms and is not
always accurate or even possible.

2.2 Collaborative ﬁltering

In contrast to CB ﬁltering, CF does not require any speciﬁc knowledge about
users or items and only uses prior observations of users’ collective behavior in
order to build new recommendations. The class of CF techniques is generally
divided into two categories: memory-based and model-based methods [10, 5].

2.2.1 Memory-based collaborative ﬁltering

A widely used and very popular approach in this category is based on k Near-
est Neighbours (kNN) algorithm [35]. It ﬁnds relevance scores for any (user,
item) pair by calculating contributions from its neighbors. The neighborhood is
typically determined by a similarity between either users (user-based approach)
or items (item-based approach) [76] in terms of some similarity measure. This
is also called a similarity-based approach. In its simplest implementation, the
method requires to store in memory all prior information about user-item inter-
actions in order to make predictions.

Performance of the similarity models may be greatly impacted by a selected
measure of similarity (or a distance measure). Cosine similarity, Jaccard index,
Pearson correlation, Okapi BM25 [64] are a few examples of possible choices.

Even though the pure similarity-based models are still popular today and in
some application domains may give a good recommendations quality, generally
they are outperformed by factorization models (see Section 2.2.2) and will not
be covered in details in this survey.

2.2.2 Model-based collaborative ﬁltering

In the model-based approach a predictive model is generated from a long enough
history of observations and uses collective behavior of the crowd (a “wisdom
of crowds”) in order to extract general behavioral patterns. One of the most
successful model-based approaches is a matrix factorization (MF). The power
of factorization models comes from the ability to embed users and items as
vectors in a lower dimensional space of latent (or hidden) features (see Section
4.3). These models represent both users’ preferences and corresponding items’
features in a uniﬁed way so that the relevance score of the user-item interaction
can be simply measured as an inner product of their vectors in the latent feature
space.

As it follows from the description, both CF and CB tackle the problem
of building relevant recommendations in a very diﬀerent ways and have their
own sets of advantages and disadvantages. Many successful RS use hybrid ap-
proaches, that combine the advantages of both methods within a single model

4

[49, 13].

3 Challenges for recommender systems

Building high quality RS is a complex problem, that involves not only a certain
level of scientiﬁc knowledge but also greatly relies on an experience, passed from
an industry and facing the real world implementations. This topic is also very
broad and we will brieﬂy discuss only the most common challenges, that are
closely related to an initial model design and its algorithmic implementations.

3.1 Cold-start

Cold-start is the problem of handling new entities, that concerns with both
users and items [26]. When a new user is introduced to the system we usually
know little or nothing about the user preferences and thus it makes it diﬃcult
or impossible to predict any interesting items for him or her. Similar problem
arises when a new item appears in a product catalog. If an item has no con-
tent description or it was not rated by any user it will be impossible to build
recommendations with this item.

3.2 Missing values

Users typically engage with only a small subset of items and considerable amount
of possible interactions stays unobserved. Excluding the trivial case of the lack
of interest in speciﬁc items, there may be some other reasons for not interacting
with them. For example, users may be simply unaware of existing alternatives
for the items of their choice. Finding out those reasons helps to make better
predictions and, of course, is a part of RS task. However, high level of uncer-
tainty may bring an undesirable bias against unobserved data or even prevent
RS models from learning representative patterns, resulting in low recommenda-
tions quality.

There are several commonly used techniques, that help to alleviate these is-
sues and improve RS quality. In MF case, simple regularization may prevent the
undesired biases. Another eﬀective technique is to assign some non-zero weights
to the missing data, instead of completely ignoring it [42]. In hybrid models a
content information can be used in order to pre-process observations and assign
non-zero relevance scores to some of the unobserved interactions (sometimes
called as sparsity smoothing). This new data is then fed into standard CF pro-
cedure. Data clustering is another eﬀective approach that is typically used to
split the problem into a number of subproblems of smaller size with more con-
nected information. Nevertheless, in case of a particular MF method, based
on Singular Value Decomposition (SVD) [31], simply imputing zero relevance
scores for an unobserved values may produce better results [19, 51]. Additional
smoothing can be achieved in that case with help of kernel trick [79].

5

As we will see in Section 5, all of these techniques are valid in TF case as

well.

3.3

Implicit feedback

In many real systems users are not motivated or not technically equipped to
provide any information about their actual experience after interacting with
an item. Hence, user preferences can only be inferred from an implicit feed-
back, which may not necessarily reﬂect the actual user taste or even tell with
guarantees whether the user likes an item or dislikes it [42].

3.4 Model evaluation

Without a well designed evaluation workﬂow and an adequate quality measures
it is impossible to build a reliable RS model that behaves equally well in both
laboratory and production environments. Moreover, there are many aspects of
a model assessment beyond recommendations accuracy, that are related to both
user experience and business goals. This includes but not limited to metrics
like diversity, novelty, serendipity (united under the term coverage in [78]),
generated revenue, etc. This is still an open and ongoing research problem as
it is not totally clear what are the most relevant and informative oﬄine metrics
and how to align them with the real online performance.

As mentioned in the beginning of Section 3, the most reliable evaluation
of RS performance is an online testing and user studies. Researchers typically
do not have an access to a production systems so a number of oﬄine metrics
(mostly borrowed from IR ﬁeld), became very popular. The most important
among them are the relevance metrics: precision, recall, F1-score and the rank-
ing metrics: normalized discounted cumulative gain (NDCG), mean average pre-
cision (MAP), mean reciprocal rank (MRR), area under the ROC curve (AUC).
These metrics may to some extent simulate a real environment, and in same
cases have strong correlation with business metrics (e.g. recall and clickthrough
rates (CTR) [40]).

It is also important to emphasize that while there are some real-world sys-
tems that target a direct prediction of a relevance score (e.g. rating), in most
cases the main goal of RS is to build a good ranked list of items (top-n recom-
mendations task). This imposes some constraints on the evaluation techniques
and model construction. It might be tempting to use and optimize for error-
based metrics like root mean squared error (RMSE) or mean absolute error
(MAE) due to their simplicity. However, good performance in terms of RMSE
does not guarantee a good performance on generating a ranked list of top-n
recommendations [26]. In other words, the predicted relevance score may not
align well with the perceived quality of recommendations.

6

3.5 Reproducible results

The problem of reproducibility is closely related to recommendations quality
evaluation. Careful design of evaluation procedures is critical for fair comparison
of various methods. However, independent studies show, that in controlled
environments it is problematic to get consistent evaluation results even for the
same algorithms on ﬁxed datasets but within diﬀerent platforms [74].

Situation gets even worse, taking into account that many models, that tackle
similar problems, use diﬀerent datasets (sometimes not publicly available), dif-
ferent data pre-processing techniques [22] or diﬀerent evaluation metrics.
In
order to avoid unintended biases, we will focus mostly on the description of the
key features of existing methods rather than on a side-by-side comparison of
quantitative results.

3.6 Real-time recommendations

A high quality RS are expected not only to produce relevant recommendations
but also respond instantly to the system updates, such as new (or unrecog-
nized) users, new items or new feedbacks [48]. Satisfying the latter requirement
highly depends on the implementation: the predictive algorithms must have
low computational complexity for producing new recommendations and take
into account the dynamic nature of a real environments. Recomputation of the
full RS model in order to include the new entities may take prohibitively long
time and the user may never see a recommendation before he or she leaves. This
means, that RS application should be capable of making incremental updates
and also be able to provide instant recommendations at a low computational
cost outside of the full model recomputation loop. A number of techniques has
been developed to fulﬁll these requirements for the MF case [29, 100, 11]. As it
will be shown in Section 5.2, these ideas can be also applied in the TF case.

3.7

Incorporating context information

In the real world scenarios interactions between users and items exhibit a mul-
tifaceted nature. User preferences are typically not ﬁxed and may change with
respect to a speciﬁc situation. For example, buyers may prefer diﬀerent goods
depending on the season of the year or time of the day. A user may prefer
to watch diﬀerent movies when alone or with a company of friends. We will
informally call these situational aspects, that shape user behavior, a contextual
information or a context for short (see Figure 1). Another examples of con-
text are location, day of week, mood, the type of a user’s electronic device, etc.
Essentially, it can be almost anything [8, 23].

Context-aware recommender systems (CARS) can be built with 3 distinct
techniques [3]: contextual preﬁltering, where a separate model is learned for
every context type; contextual postﬁltering, where adjustments are performed
after a general context-unaware model was built; and contextual modelling,
where context becomes an essential part of the training process. The ﬁrst two

7

Figure 1: Examples of contextual information.

techniques may lose information about the interrelations within a context itself.
Contextual modelling, in turn, extends the dimensionality of the problem and
promotes multirelational aspect into it. Therefore it is likely to provide more
accurate results [43]. Following the (1), we can formalize it as follows:

fR : U ser × Item × Context1 × . . . × ContextN → Relevance Score,

(2)

where Contexti denotes one of N contextual domains and the overall dimen-
sionality of the model is N +2.

As we will see further, TF models ﬁt perfectly into the concept of CARS.
With a very broad deﬁnition of context, tensor-based methods turn into a ﬂex-
ible tool, that allows to naturally model very interesting and non-trivial setups,
where the concept of context goes beyond a typical connotation.

As a precaution, it should be noted that a nonspeciﬁty of a context may
lead to an interpretability problems. Using a general deﬁnition of a context, a
content information such as user proﬁle attributes (e.g. age, gender) or items
properties (e.g. movie genre or product category) can also be regarded as some
type of context(see, for example, [43], where age and gender are used to build
new context dimensions). However, in practice, especially for TF models, this
mixing is typically avoided [95, 71]. One of the possible reasons is a deterministic
nature of content information in contrast to what is usually denoted as a context.
Similarly to MF techniques, TF reveals new unseen associations (see Section 4.3)
which in the case of deterministic attributes may be hard to interpret. It is easy
to see in the following example.

For a triplet (user, movie, gender ) the movie rating may be associated with
the only one of two possible pairs of (user, gender ), depending on the actual
user’s gender. However, once a reconstruction (with help of some TF technique)
is made, a non-zero value of rating may now pop-up for both values of gender.
The interpretation of such an association may become tricky and highly depends
on initial problem formulation.

8

UserTime contextLocation contextCompany contextMorningAfternoonNightHomeWorkCinemaAloneFriendsFamily4

Introduction to tensors

In this section we will only brieﬂy introduce some general concepts needed for
better understanding of further material. For a deeper introduction to the key
mathematical aspects of multilinear algebra and tensor factorizations we refer
the reader to [46, 18, 34]. As in the case of MF in RS, TF produces a predictive
model by revealing patterns from the data. The major advantage of a tensor-
based approach is the ability to take into account a multifaceted nature of
user-item interactions.

4.1 Deﬁnitions and notations

We will regard an array of numbers with more than 2 dimensions as a tensor.
This is a natural extension of matrices to a higher order case. A tensor with N
distinct dimensions or modes is called an N -way tensor or a tensor of order N .
Without loss of generality and for the sake of simplicity we will start our
considerations with a 3rd order tensors to illustrate some important concepts.
We will denote tensors with calligraphic capital letters, e.g. T ∈ RM×N×K
stands for a 3rd order tensor of real numbers with dimensions of sizes M, N, K.
We will also use a compact form T = [tijk]M,N,K
i,j,k=1 , where tijk is an element or
entry at position (i, j, k), and will assume everywhere in the text the values of
the tensor to be real.

Tensor ﬁbers. A generalization of matrix rows and columns to a higher order
case is called a ﬁber. Fiber represents a sequence of elements along a ﬁxed mode
when all but one indices are ﬁxed. Thus, a mode-1 ﬁber of a tensor is equivalent
to a matrix column, a mode-2 ﬁber of a tensor corresponds to a matrix row. A
mode-3 ﬁber in a tensor is also called a tube.

Tensor slices. Another important concept is a tensor slice. Slices can be
obtained by ﬁxing all but two indices in a tensor, thus forming a two-dimensional
array, i.e. matrix.
In a third order tensor there could be 3 types of slices:
horizontal, lateral, and frontal, which are denoted as Ti::,T:j:,T::k respectively.

Matricization. Matricization is a key term in tensor factorization techniques.
This is a procedure of reshaping a tensor into a matrix. Sometimes it is also
called unfolding or ﬂattening. We will follow the deﬁnition introduced in [46].
The n-mode matricization of a tensor T ∈ RM×N×K arranges the mode-n
ﬁbers to be the columns of the resulting matrix (see Figure 2). For the 1-mode
matricization T(1) the resulting matrix size is M × (N K), for the 2-mode ma-
tricization T(2) the size is N × (M K) and the 3-mode matricization T(3) has the
size K × (M N ). In the general case of an N -th order tensor T ∈ RI1×I2×···×IN
the n-mode matricization T(n) will have the size In × (I1I2 . . . In−1In+1 . . . IN ).
For the corresponding index mapping rules we refer the reader to [46].

9

Figure 2: Tensor of order 3 (top) and its unfolding (bottom). Arrow denotes
the mode of matricization.

Diagonal tensors. Another helpful concept is a diagonal tensor. A tensor
T ∈ RI1×I2×···×IN is called diagonal if ti1i2...iN (cid:54)= 0 only if i1 = i2 = . . . = iN .
This concept helps to build a connection between diﬀerent kinds of tensor de-
compositions.

4.2 Tensor Factorization techniques

The concept of TF can be better understood via an analogy with MF. For this
reason we will ﬁrst introduce a convenient notation and representation for the
MF case and then generalize it to a higher order.

4.3 Dimensionality reduction as a learning task

Let us ﬁrst start with SVD, as it helps to illustrate some important concepts and
also serves as a workhorse for certain TF techniques. Any matrix A ∈ RM×N
can be represented in the form:

A = U ΣV T ,

(3)
where U ∈ RM×K and V ∈ RN×K are orthogonal matrices, Σ = diag(σ1, . . . , σK)
is a diagonal matrix of non-negative singular values σ1 ≥ . . . ≥ σK and K =
min(M, N ) is a rank of SVD. According to the Eckart-Young theorem [25],
the truncated SVD of rank r < K with σr+1, . . . , σK set to 0 gives the best
rank-r (also called low-rank) approximation of matrix A. This has a number of
important implications for RS models.

A typical user-item matrix in RS represents a snapshot of real “noisy” data
and it is practically never of low-rank. However, the collective behavior has
some common patterns which yield a low-rank structure and the real data can
be modelled as:

M = Ar + E,

10

ABABwhere E is a “noise” in and Ar is a rank-r approximation of data matrix.
Here we assume that missing data is simply replaced with zeroes, which is a
fair assumption (see Section 3.2). The Eckart-Young theorem states, that an
optimal solution to an optimization task

(cid:107)M − Ar(cid:107)2

min
Ar

is given by the truncated SVD:

(4)
Here and further in the text we use (cid:107) · (cid:107) to denote Frobenius norm (both for
matrix and tensor case), if not speciﬁed otherwise.

M ≈ Ar = UrΣrV T
r .

In terms of RS, matrices Ur and Vr, learned from observations, represent
an embedding of users and items into the reduced latent space with r latent
features. The dimensionality reduction produces a “denoised picture” of data, it
reveals a hidden (latent) structure that describes the relations between users and
items. With this latent representation some previously unobserved interactions
can be uncovered and used to generate recommendations. This idea can be
extended to a case of higher order relations between more than 2 entities and
that is where a tensor factorizations techniques come into play.

From now on we will always assume the low-rank approximation and will
omit the subscript r in the equations for both matrix and tensor factorizations.
Before going to higher order extension let us rewrite the equation with help of
an n-mode product notation [46]:

A = Σ ×1 U ×2 V,

(5)

Even though an n-mode product is generally used to denote a tensor-matrix
product, we ﬁnd it useful to introduce it here and prepare the ground for later
generalization. For the same purpose we will rewrite this equation in 2 other
forms, the index form:

aij =

σαuiαvjα,

r(cid:88)
r(cid:88)

α=1

and a sum of rank-1 terms:

A =

σαuα ⊗ vα,

(6)

α=1

where uα, vα denote columns of the factor matrices, e.g. U = [u1 . . . ur], V =
[v1 . . . vr] and ⊗ denotes the vector outer product (or dyadic product).
In the tensor case we will also be interested in the task of learning a fac-
tor model from a real observations data Y. This turns into a dimensionality
reduction problem that gives a suitable (not necessarily the best in terms of
error-based metrics) approximation:

Y ≈ T ,

11

where T is calculated with help of some of the tensor decomposition methods,
described further. We will keep this notation throughout the text, e.g. Y will
always be used to denote a real data and T will always be used to represent the
reconstructed model, learned from Y.

4.3.1 Candecomp/Parafac

The most straightforward way of extending SVD to higher orders is to add new
factors in (6). In the third order case this will have the following form:

T =

λαuα ⊗ vα ⊗ wα,

(7)

where each summation component uα ⊗ vα ⊗ wα is a rank-1 tensor. We can
also equivalently rewrite (7) in a more concise notation:

α=1

T = [[λλλ; U, V, W ]],

(8)
where λλλ is a vector of length r with elements λ1 ≥ . . . ≥ λr > 0 and U ∈ RM×r,
V ∈ RN×r, W ∈ RK×r deﬁned similarly to (6). The expression assumes that
factors U, V, W are normalized. As we will see further, in some cases values of λλλ
can have a meaningful interpretation. However, in general, the assumption can
be safely omitted, which yields:

r(cid:88)

T = [[U, V, W ]] ≡ r(cid:88)
r(cid:88)

α=1

tijk =

uiα vjα wkα.

uα ⊗ vα ⊗ wα,

(9)

(10)

or in the index from:

α=1

The right-hand side of (9) gives an approximation of real observations data
and is called Candecomp/Parafac (CP) decomposition of a tensor Y. Despite
a similar to (6) formulation, there is a number of substantial diﬀerences in the
concepts of tensor rank and low-rank approximation, thoroughly explained in
[46]. The most important is that there is no higher order extension of the Eckart-
if an exact low-rank decomposition of Y is known, than
Young theorem, i.e.
its truncation to the ﬁrst r terms does not give the best rank-r approximation.
Moreover, the optimization task in terms of low-rank approximation is ill-posed
[21] which is likely to lead to numerical instabilities and issues with conver-
gence, until additional constraints (e.g. orthogonality, non-negativity, etc.) are
imposed.

4.3.2 Tucker decomposition

A stable way of extending SVD to a higher order case is to transform the
diagonal matrix Σ from (5) into a third order tensor G and add an additional

12

storage

CP
dnr

TD

dnr + rd

TT
dnr2

HT

dnr + dr3

Table 1: Storage requirements for diﬀerent TF methods. For the sake of sim-
plicity, this assumes a tensor with d dimensions of equal size n and all ranks (or
rank in case of CP) of a tensor decomposition set to r.

mode-3 tensor product with a new factor matrix W :

T = [[G; U, V, W ]] ≡ G ×1 U ×2 V ×3 W,

(11)
where U ∈ RM×r1 , V ∈ RN×r2, W ∈ RK×r3 are orthogonal matrices, having
similar meaning of the latent feature matrices as in the case of SVD. Tensor
G ∈ Rr1×r2×r3 is called a core tensor of the TD and a tuple of numbers (r1, r2, r3)
is called a multilinear rank. The decomposition is not unique, however the
optimization problem with respect to multilinear rank is well-posed. Note, that
if tensor G is diagonal with all ones on its diagonal, than the decomposition
turns into CP. In the index notation TD takes the following form:

r1,r2,r3(cid:88)

tijk =

gαβγ uiα vjβ wkγ.

(12)

α,β,γ=1

A very popular method of computing TD is the Higher Order SVD (HOSVD)
proposed in [20]. In the nutshell it sequentially applies SVD to all 3 matriciza-
tions of tensor Y, i.e. Y(1), Y(2), Y(3).

The deﬁnition of TD is not restricted to have 3 modes only. Generally, the
number of modes is not limited, however storage requirements depend expo-
nentially on the number of dimensions (see Table 1), which is often referred
as a curse of dimensionality. This imposes strict limitations on the number of
modes for many practical cases. In order to break the curse of dimensionality, a
number of eﬃcient methods has been developed recently, namely Tensor Train
(TT) [61] and Hierarchical Tucker (HT) [33]. However, we are not aware of any
published results related to TT- or HT-based implementations in RS.

5 Tensor-based models in recommender systems

Treating data as tensor may bring new levels of ﬂexibility and/or quality into
RS models, however there are nuances that should be taken into account and
treated properly. This section covers diﬀerent tensorization techniques used to
build advanced RS in various application domains. For all the examples we will
use a uniﬁed notation (where it is possible) introduced in Section 4, hence it
might look diﬀerent from the notation used in the original papers. This helps
to reuse some concepts within diﬀerent models and build a consistent narrative
throughout the text.

13

5.1 Personalized search and resource recommendations

There is a very tight connection between personalized search and RS. Essentially,
recommendations can be considered as a zero query search [6] and, in turn,
personalized search engine can be regarded as a query-based RS.

Personalized search systems aim at providing a better search experience
by returning the most relevant results, typically web pages (or resources), in
response to a user’s request. A clicktrough data (i.e. an event log of clicks on the
search results after submitting a search query) can be used for this purpose as it
contains an information about users’ actions and may provide valuable insights
into search patterns. The essential part of this data is not just a web page that
a user clicks on, but also a context, a query associated with every search request
that carries a justiﬁcation for the user’s choice. The utility function in that case
can be formulated as:

fR : U ser × Resource × Query → Relevance Score.

5.1.1 CubeSVD

One of the earliest and at the same time very illustrative works where this
formulation was explored with help of tensor factorization is CubeSVD [83].
The authors build a 3-rd order tensor Y = [yijk]M,N,K
i,j,k=1 . Values of the tensor
represent the level of association (the relevance score) between the user i and
the web-page j in presence of the query k:

(cid:40)

yijk > 0,
yijk = 0,

if (i, j, k) ∈ S,
otherwise,

where S is an observation history, e.g. a sequence of events described by the
triplets (user, resource, query).

The association level can be expressed in various ways, the simplest one is to
measure a co-occurence frequency f , e.g. how many times a user has clicked on
a speciﬁc page after submitting a certain query. In order to prevent an unfair
bias towards the pages with high click rates, it can be restricted to have only
values of 0 (no interactions) or 1 (at least one interaction). Or it can be rescaled
with a logarithmic function:

f(cid:48) = log2(1 + f /f0),

where f(cid:48) is a new normalized frequency and f0 is, for example, an IDF (Inverse
Document Frequency) measure of a web page. Another scaling approach can
also be used.

The authors proposed to model the data with a third order TD (11) and
in order to ﬁnd it they applied the HOSVD. Similarly to SVD (3), factors
U ∈ RM×r1 , V ∈ RN×r2 and W ∈ RK×r3 represent embedding of users, web
pages and queries vectors into a lower-dimensional latent factors space with di-
mensionalities r1, r2 and r3 correspondingly. The core tensor G ∈ Rr1×r2×r3

14

deﬁnes the form and the strength of multilinear relations between all three enti-
ties in the latent feature space. Once the decomposition is found, the relevance
score for any (user, resource, query) triplet can be recovered with (12).

With the introduction of new dimensions the data sparsity becomes even
higher, which may lead to a numerical instabilities and general failure of the
learning algorithm. In order to mitigate that problem, the authors propose sev-
eral smoothing techniques: based on value imputation with small constant and
based on the content similarity of web pages. They reported an improvement
in the overall quality of the model after these modiﬁcations.
After applying the decomposition technique the reconstructed tensor T will
contain new non-zero values denoting potential associations between users and
web resources inﬂuenced by certain queries. The tensor values can be directly
used to rank a list of the most relevant resources: the higher the value tijk is
the higher the relevance of the page j to the user i within the query k.

This simple TF model does not contain a remedy for some of the typical
RS problems such as cold start or real-time recommendations and is most likely
to have issues with scalability. Nevertheless, this work is very illustrative and
demonstrates the general concepts for building a tensor-based RS.

5.1.2 TOPHITS

As has been discussed in Section 3.6, new entities can appear in the system
dynamically and rapidly, which in the case of higher order models creates
even more computational load, i.e. full recomputation of tensor decomposition
quickly becomes infeasible and incremental techniques should be used instead.
However, in some cases simply redeﬁning the model might lower the complexity.
As we mentioned in Section 2.2.1, a simple approach to reduce the model is to
eliminate one of the entities with some sort of aggregation.

For example, instead of considering (user, resource, query) triplets we could
work with aggregated (resource, resource, query) triplets, where every frontal
slice Y: : k of the tensor is simply an adjacency matrix of a resources browsed
together under a speciﬁc query. Therefore users are no longer explicitly stored
and their actions are recorded only in the form of a co-occurence of resources
they searched for.

An example of such a technique is TOPHITS model [47, 45]. This analogy
requires an extra explanation as the authors are not modelling users clicking
behavior. The model is designed for web-link analysis of a static set of web
pages referencing each other via hyperlinks. The data is collected by crawling
those web pages and collecting not only links but also a keywords associated
with them. However, the crawler can be interpreted as a set of users browsing
those sites by clicking on the hyperlinked keywords. This draws the connection
between CubeSVD and TOPHITS model as the keywords can be interpreted as
a short search queries in that case. And, as we stated earlier, users (or crawlers)
can be eliminated from the model by constructing an adjacency matrix of linked
resources.

The authors of TOPHITS model extend an adjacency matrix of interlinked

15

web pages with the collected keyword information and build a so called adja-
cency tensor T ∈ RN×N×K, that encodes hubs, authorities and keywords. As
has been mentioned, the keyword information is conceptually very similar to
queries, hence it can be also modelled in a multirelational way. Instead of TD
format the authors prefer to use CP in the form of (8) with U, V ∈ RN×r and
W ∈ RK×r.
The interpretation of this decomposition is diﬀerent from the CubeSVD. As
the authors demonstrate, the weights λk, (1 ≤ k ≤ r) have a straightforward
semantic meaning as they correspond to a set of r speciﬁc topics extracted from
the overall web page collection. Accordingly, every triplet of vectors (uk, vk, wk)
represents a collection of hubs, authorities and keyword terms respectively, char-
acterized by a topic k. The elements with higher values in these vectors provide
the best-matching candidates under the selected topic, which allows a better
grouping of web pages within every topic and provide means for a personaliza-
tion.

For example, as the authors show, a personalized ranked list of authorities

can be obtained with:

a∗ = V ΛW T q,

(13)

where Λ = diag(λλλ) is a diagonal matrix and q is a user-deﬁned query vector of
length K with elements qt = 1 if term t belongs to the query and 0 otherwise,
t = {1, . . . , K}. Similarly, a personalized list of hubs can be built simply by
substituting factor V with U in (13).

The interpretation of tensor values might seem very natural, however there
is an important note to keep in mind. Generally, the restored tensor values
might turn both positive and negative. And in most applicatons the negative
values have no meaningful explanation. The non-negative tensor factorization
(NTF) [17, 104] can be employed to resolve that issue (see example in [15], and
also the connection of NTF to probabilistic factorization model under a speciﬁc
conditions [16]).

5.2 Social tagging

A remarkable amount of research is devoted to one speciﬁc domain, namely so-
cial tagging systems (STS), where predictions and recommendations are based
on commonalities in social tagging behavior (also referred as collaborative tag-
ging). A comprehensive overview of the general challenges and state-of-the-art
RS methods can be found in [56].

Tags carry a complementary semantic information, that helps STS users
to categorize and organize items of their choice. This brings an additional
level of interpretation of the user-item interactions, as it exposes the motives
behind the user preferences and explains the relevance of particular items. This
observation suggests that tags play an important role in deﬁning the relevance
of (user, item) pairs, hence all the three entities should be modelled mutually
in a multirelational way. The scoring function in that case can be deﬁned as

16

Figure 3: Higher order folding-in for Tucker decomposition. A slice with new
user information in the original data (left) and a corresponding row update of
the factor matrix in TD (right) are marked with solid color.

follows:

fR : U ser × Item × T ag → Relevance Score.

The triplets (user, item, tag), coming from an observation history S, can be
easily translated into a 3rd order tensor Y = [yijk]M,N,K
i,j,k=1 , where M, N, K denote
the number of users, items and tags respectively. Users are typically not allowed
to assign the same tags to the same items more than once, hence the tensor
values are strictly binary and deﬁned as:

(cid:40)

yijk = 1,
yijk = 0,

if (i, j, k) ∈ S,
otherwise.

5.2.1 Uniﬁed framework

As in the case of keywords and queries, tensor dimensionality reduction helps
to uncover latent semantic structure of the ternary relations. The values of the
reconstructed tensor T can be interpreted as the likeliness or weight of new
links between users, items and tags. These links might be used for building
recommendations in various ways: help users assign relevant tags for items [90],
ﬁnd interesting new items [89], or even ﬁnd like-minded users [84].

The model that is built on top of all three possibilities is described in [87].
The authors perform a latent semantic analysis on the data with help of the
HOSVD. Generally, the base model is similar to CubeSVD (see Section 5.1):
items can be treated as resources and tags as queries.
The authors also face the same problem with sparsity. The tensor matri-
cizations Y(n), 1 ≤ n ≤ 3 within the HOSVD procedure produce highly sparse
matrices which may prevent the algorithm from learning the accurate model.
In order to overcome that problem they propose a smoothing technique based
on a kernel trick.

17

𝒢𝑈𝑉𝑊𝒴≈In order to deal with the problem of real-time recommendations (see Section
3.6) the authors adopt a well known folding-in method [29] to a higher order case.
The folding-in procedure helps to quickly embed a previously unseen entity into
the latent features space without recomputing the whole model. For example,
an update to a users feature matrix U can be obtained with:

unew = pV1Σ−1
1 ,

where p is a new user information that corresponds to a row in the matricitized
tensor Y(1); V1 is an already computed (during HOSVD step) matrix of right
singular vectors of T(1), and Σ1 is a corresponding diagonal matrix of singular
values; unew is an update row which is appended to the latent factor matrix U .
The resulting update to reconstructed tensor T is computed with (see Figure
3):

Tnew = [G ×2 V ×3 W ] ×1

(cid:20) U

unew

(cid:21)

,

where the term within the left brackets of the right hand side does not contain
any new values, e.g. does not require the full recomputation and can be pre-
stored, which makes the update procedure much more eﬃcient.

Nevertheless, this typically leads to a loss of orthogonality in factors and
negatively impacts the accuracy of the model in the long run. This can be
avoided with an incremental SVD update, which for the matrices with missing
entries was initially proposed by [11]. As the authors demonstrate, it can be
also adopted for tensors.

It should be noted, that this is not the only possible option for incremental
updates. For example, a diﬀerent incremental HOSVD-based model is proposed
in [102] for a highly dynamic, evolving environment (not related to tag-based rec-
ommendations). The authors of this work use an extension of a two-dimensional
incremental approach from [73].

5.2.2 RTF and PITF

The models, overviewed so far, has a common “1/0” interpretation scheme for
a missing values, i.e. all triplets (i, j, k) ∈ S are assumed to be positive feed-
back and all others (missing) are negative feedback with zero relevance score.
However, as the authors of ranking with TF model (RTF) [69] and more elab-
orate pairwise interaction TF (PITF) [70] model emphasize, all missing entries
can be split into 2 groups: the true negative feedback and the unknown values.
The true negatives correspond to those triplets of (i, j, k) where the user i has
interacted with the item j and has assigned tags diﬀerent from the tag k. More
formally, if PS is a set of all posts that correspond to all observed (user, item)
interactions, than true negative feedback within any interaction is deﬁned as:

ij := {k|(i, j) ∈ PS ∧ (i, j, k) /∈ S};
Y −

trivially, true positive feedback is:

ij := {k|(i, j) ∈ PS ∧ (i, j, k) ∈ S}.
Y +

18

All other entries are unknowns and are to be uncovered by the model.

Furthermore, both RTF and PITF models do not require any speciﬁc values
to be imposed on either known or unknown entries. Instead they impose only
ranking constraints on the reconstructed tensor values:

tijk1 > tijk2 ⇔ (i, j, k1) ∈ Y +

ij ∧ (i, j, k2) ∈ Y −
ij .

These post-based ranking constraints become the essential part of an opti-
mization procedure. The RTF model uses the Tucker format, however it aims
at directly maximizing AUC measure, which, according to the authors, takes
the following form:

AU C(θ, i, j) :=

1
ij ||Y −
|Y +
ij |

σ(tijk+ − tijk− ),

(cid:88)

(cid:88)

k+∈Y +

ij

k−∈Y

−
ij

where θ := (U, V, W,G) and σ(x) is a sigmoid function, introduced to make the
term diﬀerentiable:

σ(x) =

1

1 + e−x .

The overall regularized optimization criteria takes the following form:

AU C(θ, i, j) − (λG(cid:107)G(cid:107)2

F + λ(cid:107)U(cid:107)2

F + λ(cid:107)V (cid:107)2

F + λ(cid:107)W(cid:107)2
F ),

(cid:88)

argmax

θ

(i,j)∈PS

(14)

(16)

(17)

where λ, λG are the regularization parameters. We ﬁnd it useful to rewrite this
optimization criteria in a more general model independent notation:

θ∗ = argmin

J(θ),

θ

(15)

where θ∗ denotes the learned factors U, V, W in case of CP-based models and
additionally includes the core tensor G in case of TD; the optimization objective
J(θ) is deﬁned as a sum of a loss function and a regularization term, i.e.:

In case of RTF model the loss function L(Y,T ) is a negative AUC gain:

J(θ) = L(Y,T ) + Ω(θ).

L(Y,T ) = − (cid:88)

(i,j)∈PS

AU C(θ, i, j),

and the regularization term Ω(θ) is:

Ω(θ) = λG(cid:107)G(cid:107)2

F + λU(cid:107)U(cid:107)2

F + λV (cid:107)V (cid:107)2

F + λW(cid:107)W(cid:107)2
F ,

with λU = λV = λW = λ.

The authors adopt a stochastic gradient descent (SGD) algorithm for solving
the optimization task. However, as they state, directly optimizing the AUC
objective is computationally infeasible. Instead, they exploit a smart trick of

19

recombining and reusing precomputed summation terms within the objective
and its derivatives. They use this trick for both tasks of learning and building
recommendations.

The PITF model is built on top of RTF. It adopts Bayesian Personalized
Ranking (BPR) technique proposed for MF case in [67] to the ranking approach.
The tags rankings for every observed post (i, j) are not deterministically learned
like in RTF model but instead are derived from the observations by optimizing
the maximum aposteriori estimation. This leads to a similar to RTF optimiza-
tion objective (see (16)) with a slightly diﬀerent loss function:
ln σ(tijk1 − tijk2),

L(Y,T ) = − (cid:88)

(i,j,k1,k2)∈DS

where the same notation as in RTF is used; σ is a sigmoid function from (14)
and DS is a training data, i.e. a set of quadruples:

DS = {(i, j, k1, k2)| (i, j, k1) ∈ S ∧ (i, j, k2) /∈ S}.

(18)

An important diﬀerence of PITF from RTF is that the complexity of mul-
tilinear relations is signiﬁcantly reduced by leaving only pairwise interactions
between all entities. From the mathematical viewpoint it can be considered as
a CP model with a special form of partially ﬁxed factor matrices (cf. (10)):

(cid:88)

(cid:88)

(cid:88)

tijk =

uiα wU

kα +

vjα wV

kα +

uiα vjα,

(19)

α

α

α

kα and wV

where wU
kα are the parts of the same matrix W responsible for tags
relation to users and items respectively; uiα and vjα are interactional parts of
U and V .

The authors emphasize, that the user-item interaction term does not con-
tribute to the BPR-based ranking optimization scheme which yields even more
simple equation, that becomes an essential part of the PITF model:

(cid:88)

(cid:88)

tijk =

uiα wU

kα +

vjα wV

kα.

(20)

α

α

Another computational trick that helps to train the model even faster with-
out sacriﬁcing the quality is random sampling within the SGD routine. All the
quadruples in DS corresponding to a post (i, j) are highly overlapped with re-
spect to the tags associated with them. Therefore, learning with some randomly
sampled quadruples is likely to have a positive eﬀect on learning the remaining
ones.

In order to verify the correctness and eﬀectiveness of such simpliﬁcations the
authors conduct experiments with both BPR-tuned TD and CP and demon-
strate that PITF algorithm achieves close or even better quality of recommen-
dations while learning features faster than the other two TF methods.

Despite its computational eﬀectiveness, the original PITF model is lacking
the support for the real-time recommendation scenarios, where rebuilding the

20

full model for each new user, item or tag could be prohibitive. The authors of
[52] overcome this limitation by introducing the folding-in procedure compatible
with the PITF model and demonstrate its ability to provide high recommenda-
tions quality. Worth noting here, that a number of variations of the folding-in
technique are available for diﬀerent TF methods, see [101].

The idea of modelling higher order relations in a joint pairwise manner simi-
lar to (20) has been explored in various application domains and is implemented
in various settings, either straightforwardly or as a part of a more elaborate RS
model [30, 37, 103, 77]. There are several generalized models [94, 71], [40], that
also use this idea. They are covered in more details in Sections 5.4.3 and 5.4.5
of this work.

5.2.3

Improving the prediction quality

As has been already mentioned in Section 5.2.1 high data sparsity typically
leads to a less accurate predictive models. This problem is common across
various RS domains. Another problem, speciﬁc to STS, is a tag ambiguity
and redundancy. The following are the examples of some of the most common
techniques, developed to deal with these problems.

The authors of CubeRec [97] propose a clustering-based separation mecha-
nism. This mechanism builds clusters of triplets (user, item, tag) based on the
proximity of tags derived from the item-tag matrix. With this clustering some
of the items and tags can belong to several clusters at the same time, according
to their meaning. After that the initial problem is split into a number of sub-
problems (corresponding to clusters) of a smaller size and hence, with a more
dense data. Every subproblem is then factorized with the HOSVD similarly to
[83], and the resulting model is constructed as a combination of all the smaller
TF models.

The authors of the clustering-based HOSVD model (ClustHOSVD) [85] also
employ clustering approach. However, instead of splitting the problem, they
replace tags by tag clusters and apply the HOSVD directly to the modiﬁed data
consisting of (user, item, tag cluster ) triplets. They also demonstrate the eﬀect
of diﬀerent clustering techniques on the quality of RS.

We have also seen that many models beneﬁt from clustering either prior to
TF or after it. This suggests that these models could beneﬁt from simultaneous
clustering and factorization (see as an example).[28]

A further improvement can be achieved with hybrid models (see Section
2.2.2), that exploit a content information and incorporate it into a tensor-based
CF model. Of course, there is no “single-bullet” approach, suitable for all kinds
of tasks, as it highly depends on the type of data used as a source of content
information.

The authors of [58] exploit acoustic features for music recommendations in a
tag-based environment. The features, extracted with speciﬁc audio-processing
techniques, are used to measure the similarity between diﬀerent music samples.
The authors make an assumption that similarly sounding music is likely to have
similar tags, which allows to propagate tags to the music that was not tagged

21

yet. With this assumption the data is augmented with new triplets of (user,
item, tag), which leads to a more dense data and results in a better predictive
quality of the HOSVD model.

The TF and tag clustering (TFC) model [65] combines both content ex-
ploitation and tag clustering techniques. The authors focus on the image rec-
ommendations problem, thus they use an image processing techniques in order
to ﬁnd items similarities and propagate highly relevant tags. Once the tag prop-
agation is completed, the authors ﬁnd tag clusters (naming them topics) and
build new association triplets (user, item, topic), which are further factorized
with the HOSVD.

As a last remark in this section, the idea of model splitting, proposed in
the CubeRec model, was also explored in a more general setup in [91]. The
authors consider a multiple context environment, where user-item interactions
may depend on various contexts such as location, time, activity, etc. This is
generally modelled with an N -th order tensor, where N > 3. Instead of dealing
with higher number of dimensions and greater sparsity, the authors propose
to build a separate model for every context type, which transforms the initial
problem into a collection of a smaller problems of order 3. Then all the resulting
TF models are combined with speciﬁc weights (based on the context inﬂuence
measure proposed by the authors) and can be used to produce recommendations.
However, despite the ability to better handle the sparsity issue, the model may
loose some valuable information about the relations between diﬀerent types of
context. A more general methods for multi-context problems are covered in
Section 5.4.

5.3 Temporal models

User consumption patterns may change in time. For example, the interest of
TV users may correlate not only with a topic of a TV program, but also with
a speciﬁc time of the day. In retail user preferences may vary depending on the
season. Temporal models are designed to learn those time-evolving patterns in
data by taking the time aspect into account, which can be formalized with the
following way scoring function:

fR : U ser × Item × T ime → Relevance Score.

Even though the general problem statement looks already familiar, when
working with the T ime domain one should mind the diﬀerence between the
evolving and periodic (e.g. seasonal) events which may require a special treat-
ment.

5.3.1 BPTF

One of the models that exploits periodicity of events is the Bayesian Probabilistic
TF (BPTF) [96]. It uses seasonality to reveal trends in retail data and predict
the orders that will arrive in the ongoing season based on the season’s start
and previous purchasing history. The key feature of the model is the ability

22

to produce forecasts on the sales of the new products, that were not present in
previous seasons. The model captures dynamic changes in both product designs
and customers’ preferences solely from the history of transactions and does not
require any kind of an expert knowledge.

The authors develop a probabilistic latent factors model by introducing pri-
ors on the parameters; i.e. the latent feature vectors are allowed to vary and
the variance of relevance scores is assumed to follow a Gaussian distribution:

tijk|U, V, W ∼ N (< Ui:, Vj:, Wk: >, γ−1),

where γ is an observations precision and < Ui:, Vj:, Wk: > denotes a right hand
side of (10). Note, that in the original work the authors use a transposed version
of the factor matrices, e.g. any column of the factor U in their work represents
a single user, the same holds for two other factors.

In order to prevent the overﬁtting the authors also impose prior distributions

on U and V :

Ui: ∼ N (0, σ2
Vj: ∼ N (0, σ2

U I),
V I).

Furthermore, the formulation for the time factor W takes into account its evolv-
ing nature and implies smooth changes in time:
Wk: ∼ N (Wk−1:, σ2
W0: ∼ N (µW , σ2

dW I),
0I).

The time factor W rescales the user-item relevance score with respect to the
time-evolving trends and the probabilistic formulation helps to account for the
users who do not follow those trends.

The authors show that maximizing the log-posterior distribution log p(U, V, W, W0,:|Y)

with respect to U, V, W and W0: is equivalent to an optimization task with the
weighted square loss function (see (16)):

L(Y,T ) =

(yijk − tijk)2,

(21)

(cid:88)

(i,j,k)∈S

K(cid:88)

k=1

and a bit more complex regularization term:

Ω(θ) =

(cid:107)U(cid:107)2

F +

λU
2

λV
2

(cid:107)V (cid:107)2

F +

λdT
2

(cid:107)Wk: − Wk−1:(cid:107)2 +

(cid:107)W0: − µW(cid:107)2,

λ0
2

where λU = (ασU )−1, λV = (ασV )−1, λdW = (ασdW )−1, λ0 = (ασ0)−1. The
number of parameters of this model makes the task of optimization almost
infeasible. However, the authors come up with an elaborate MCMC-based inte-
gration approach, that makes the model almost parameter-free and also scales
well.

23

5.3.2 TCC

The authors of TF-based subspace clustering and preferences consolidation
model (TCC) [93] exploit the periodicity in usage patterns of the IPTV users in
order to, at ﬁrst, identify them and, secondly, provide with more relevant rec-
ommendations, even if those users share the same IPTV account (for example,
across all family members). This gives a slightly diﬀerent deﬁnition of a utility
function:

fR : Account × Item × T ime → Relevance Score,

where Account is the domain of all registered accounts and the number of ac-
counts is not greater than the number of users, i.e. |Account| ≤ |U ser|. Initial
tensor Y is built from the triplets (account, item, time) and its values are just
the play counts.

In order to be able to ﬁnd a correct mapping of the real users to the known
accounts, the authors introduce a concept of a virtual user. Within the model
the real user is assumed to be a composition of particular virtual users uak which
express the speciﬁc user’s preferences tied to a certain time periods, e.g.:

uak := {(a, pk)| a ∈ A, pk ∈ P, pk (cid:54)= ∅},

where a is an account from the set of all accounts A, pk is a sub-period from
the set of all non-overlapping time periods P .

As the authors state, manually splitting the time data into the time slots pk
does not ﬁt the data well and they propose to ﬁnd those sub-periods from the
model. They ﬁrst solve the SGD-based optimization task (16) for the TD with
the same weighted squared loss function as in (21) and regularization term as
in (17) (with λG = λU = λV = λW = 1
2 λ). Once the model factors are found,
the sub-periods pk can be obtained by clustering the time feature vectors:

P ← k-Means clustering of the rows of W.

Then the consolidation of virtual users into the real ones can be done in 2
steps. At ﬁrst, a binary similarity measure is computed between diﬀerent pairs
of virtual users (uak, uak(cid:48)) corresponding to the same account a. The second
step is to combine similar virtual users so that every real user is represented
as a set of virtual ones. This is done with help of a graph-based techniques.
Once the real users are identiﬁed, recommendations can be produced with a
user-based kNN approach. As the authors demonstrate, the proposed method
not only provides a tool for user identiﬁcation, but also outperforms standard
kNN and TF-based methods applied without any prior clustering.

5.4 General context-aware models

In previous sections we have discussed TF methods targeted at speciﬁc classes
of problems: keyword- or tag-based recommendations, temporal models. They
all have one thing in common - the use of a third entity leading to a higher
level of granularity and better predictive capabilities of a model. This leads to

24

an idea of generalization of such an approach, that is suitable for any model
formulated in the form of (2).

5.4.1 Multiverse

One of the ﬁrst attempts towards this generalization is the Multiverse model
[43]. The authors deﬁne context as any set of variables that inﬂuence users’
preferences and propose to model it by the N -th order TD with N−2 contextual
dimensions:

T = [[G; U, V, W1, W2, . . . , WN−2]],

where factors Wi, i = {1, . . . , N − 2} represent a corresponding embedding of
every contextual variable into a reduced latent space and all factors including
U and V are not restricted to be orthogonal. As the authors state, the model is
suitable for any contextual variables over a ﬁnite categorical domain. It should
be noted, that the main focus of the work is systems with an explicit feedback
and the model is optimized for the error-based metrics, which does not guarantee
an optimal items ranking as has been discussed in Section 3.4.

Following the general optimization formulation stated in (16), the authors

use the weighted loss objective:
L(T ,Y) =

1
(cid:107)G(cid:107)1

(cid:88)

(i,j,k)∈S

l(tijk, yijk),

where l(tijk, yijk) is a pointwise loss function, that can be based on l2, l1 or
other types of distance measure. The example is provided for the 3rd order
case, however, it can be easily generalized to a higher orders. The authors also
use the same form of the regularization term as in (17), as it enables trivial
optimization procedure.

In order to ﬁght against the growing complexity for the higher order cases
they propose a modiﬁcation of the SGD algorithm. Within a single optimization
step the updates are performed on every row of the latent factors independently.
For example, an update for i-th row of U :

Ui: ← Ui: − ηλU Ui: − η∂tijk l(tijk, yijk)

is independent on all other factors and thus all the updates can be performed
in parallel. The parameter η deﬁnes the model’s learning step.

In addition to the general results on the real dataset, this work features a
comprehensive experimentation on the semi-synthetic data that shows the im-
pact of a contextual information on the RS models performance. It demonstrates
that high context inﬂuence leads to a better quality of the selected context-aware
methods, among which the proposed TF approach gives the best results, while
a context-unaware method’s quality signiﬁcantly degrades.

5.4.2 TFMAP

Similarly to the previously discussed PITF model, the TF for MAP optimiza-
tion model (TFMAP) [80] also targets optimal ranking, however it exploits the

25

MAP metric instead of AUC. The model is designed for an implicit feedback
systems which means that the original tensor Y is binary with non-zero elements
reﬂecting the fact that the interaction has occurred:
if (i, j, k) ∈ S,

(cid:40)

(22)

yijk =

1,
0, otherwise.

(cid:80)N
The optimization objective is drawn from the MAP deﬁnition:
(cid:80)N
j(cid:48)=1 yij(cid:48)kI (rij(cid:48)k ≤ rijk)

(cid:80)N

K(cid:88)

M(cid:88)

M AP =

yijk
rijk

j=1

1

M K

,

i=1

k=1

j=1 yijk

where rijk denotes the rank of the item j in the items list of the user i under
the context type k and I (·) is an indicator function, which is equal to 1 if the
condition is satisﬁed and 0 otherwise, both depend on the reconstructed values
of T . In order to make the metric smooth and diﬀerentiable the authors propose
two approximations:

≈ σ(tijk),

1
rijk

I (rij(cid:48)k ≤ rijk) ≈ σ(tij(cid:48)k − tijk),

where tijk is calculated with (10) (which makes the model a CP-based) and σ is
a sigmoid function deﬁned by (14). Notably, tij(cid:48)k−tijk =< Ui:, Vj(cid:48):−Vj:, Wk: >,
where we use the same notation as in BPTF model, see Section 5.3.1.
The model also follows the standard optimization formulation stated in (16),
where the loss function is just a negative MAP gain, i.e. L(T ,Y) = −M AP ,
and the regularization has the form of (17) where the term, related to the core
tensor G is omitted.

Note, that MAP optimization also has a weighted form due to (22), however,
the computation complexity would still be prohibitively high due to its complex
structure.
In order to mitigate that, the authors propose the fast learning
algorithm: for each (user, context) pair only a limited set of a representative
items (a buﬀer) is considered, which in turn, allows to control the computational
complexity. They also provide an eﬃcient algorithm of sampling the “right”
items and constructing the buﬀer, which does not harm the overall quality of
the model.

5.4.3 CARTD

The CARTD model (context-aware recommendation tensor decomposition) [94,
71] provides a generalized framework for an arbitrary number of contexts and
also targets an optimal ranking instead of a rating prediction. Under the hood
the model extends the BPR-based ranking approach used in the PITF model
to the higher order cases.

26

The authors introduce a uniﬁed notion of an entity. A formal task is to
ﬁnd the list of the most relevant entities within a given contextual situation.
Remarkably, all the information, that is used to make recommendations more
accurate and relevant, is deﬁned as a context. In that sense, not only information
like tag, time, location, user attributes, etc. is considered to be a context, even
users themselves might be deﬁned as a context of an item. This gives a more
universal formulation for the recommendations task:

fR : Entity × Context1 × . . . × Contextn → Relevance Score.

(23)

As an illustration to that, a quadruple (user, item, time, location) maps
to (context1, entity, context2, context3). Obviously, the deﬁnition of the entity
depends on the task. For example, in case of social interactions prediction with
(user, user, attribute) triplets, the main entity as well as one of the context
variables will be a user.

The observation data in a typical case of a user-item interactions can be

encoded similarly to (18):

DS := {(e, f, c1, . . . , cn) | (e, c1, . . . , cn) ∈ S ∧ (f, c1, . . . , cn) /∈ S},

items) and ci, i = {1, . . . , n} denotes a
where e and f are the entities (i.e.
context type (includes users). As the authors emphasize, this leads to a huge
sparsity problem, and instead they propose to relax conditions and instead build
the following set for learning the model:

DA := {(e, f, c1, . . . , cn) | ∀ci : #ci (e) > #ci(f )},

where #ci(·) indicates the number of occurrences of an entity within the context
ci. The rule #ci(e) > #ci (f ) denotes the prevalence of the entity e over the
entity f with respect to all possible contexts.

The optimization objective will also look similar to the one used in the PITF

model with the loss function deﬁned as:

L(T ,Y) = − (cid:88)

ln σ(cid:0)t{c},e − t{c},f

(cid:1) ,

(e,f,c1,...,cn)∈DA

where {c} denotes a set of all context variables ci and the tensor values tijk are
calculated with help of the reduced CP model with the pairwise only interac-
tions, similarly to (20):

n(cid:88)

t{c},e =

vE,Ci
e

vCi,E
ci

,

i=1

e

ci

and vCi,E

where vE,Ci
are the elements at the crossection of the e-th row and
the i-th column of the factor matrices V E,Ci and V Ci,E respectively. As in the
previous cases, the regularization term Ω(θ) have similar to (17) form, which
includes all the factors from θ:

θ = {V E,C1, V C1,E, . . . , V E,Cn , V Cn,E}.

27

5.4.4

iTALS

As has been mentioned in the introduction (see Section 3.3), an implicit feed-
back does not always correlate with the actual user preferences, thus a simple
binary scheme (as in (22)) may not be accurate enough. For this reason, the
authors of the iTALS model (ALS-based implicit feedback recommender algo-
rithm) [38] propose to use the conﬁdence-based interpretation of an implicit
feedback introduced in [42] and adopt it for the higher order case.
They introduce the dense tensor W that assigns non-zero weights for both
observed and unobserved interactions. For the n-th order tensor it has the

following form:(cid:40)

wi1,...,in = α · #(i1, . . . , in),
wi1,...,in = 1,

if (i1, . . . , in) ∈ S,
otherwise,

(24)

where #(i1, . . . , in) is the number of occurrences of the tuple (i1, . . . , in) (e.g.
the combination of the user i1 and the item i2 interacted within the set of
contexts i3, . . . , in) in the observation history; α is set empirically and α ·
#(i1, . . . , in) > 1 which means that the observed events provide more conﬁ-
dence in the user preferences than the unobserved ones.

The loss function will then take the form:

(cid:88)

L(T ,Y) =

wi1,...,in (ti1,...,in − yi1,...,in )2,

i1,...,in

where weights wi1,...,in are deﬁned by (24), yi1,...,in are the values of a binary
feedback tensor of order n, deﬁned similarly to (22), and ti1,...,in are the values
of the reconstructed tensor.

The model uses CP with an ALS-based optimization procedure and a stan-
dard regularization of all of the factors. The latent feature vectors are encoded
in the rows of the factor matrices, not the columns, i.e. following the authors’
notation, we should rewrite (9) as:

T = [[M T

1 , . . . , M T

n ]],

where Mi (1 ≤ i ≤ n) are transposed factors of the CP decomposition.

The authors show, how an eﬃcient computation over the dense tensor can
be achieved with the same tricks, that are used in [42] for the matrix case. The
model also has a number of modiﬁcations [39]: based on the conjugate gradient
approach (iTALS-CG) and the coordinates descent approach (iTALS-CD) where
an additional features compression is achieved by the Cholesky decomposition.
This makes the iTALS-CD model to learn even faster than MF methods. While
performing on approximately the same level of accuracy as the state-of-the-art
Factorization Machines (FM) method [68], it is capable of learning more complex
latent relations structure. Another modiﬁcation is the pairwise “PITF-like”
reduction model, named iTALSx [37].

28

5.4.5 GFF

The General Factorization Framework (GFF) [40] further develops the main
ideas of the family of iTALS models. Within the GFF model diﬀerent CP-
based factorization models (also called a preference models) are combined in
order to capture the intrinsic relations between users and items inﬂuenced by
an arbitrary number of contexts. As in many other works the authors of GFF
model ﬁx the broad deﬁnition of the context as an entity, which “value is not
determined solely by the user or the item”, i.e. not a content information (see
Section 3.7).

The model can be better explained with the example. Let us consider the

problem of learning the scoring function as follows:

fR : U × I × S × Q → Relevance Score,

(25)

where U and I are the domains of users and items respectively; S stands for
season and denotes the periodicity of the events (see Section 5.3); Q describes
the sequential consumption patterns, e.g. what are the previous items that were
also consumed with the current one (see [38] for broader set of examples). Let us
also deﬁne the pairwise interactions between users and items as U I (standard
CF model), between items and seasons as IS and so forth. Using the same
notation we can also deﬁne multi-relational interactions, such as U IS for a 3-
way user-item-season interactions or U ISQ for the 4-way interactions between
all 4 types of entities.

In total, there could be 2047 diﬀerent combinations of interactions, yet not
all of them are feasible in terms RS model, as not all of them may contribute
to the preference model.

As the result, GFF generates a very ﬂexible multirelational model that al-
lows to pick the most appropriate scheme of interactions, that does not explode
the complexity of the model and meanwhile achieves a high quality of recom-
mendations. As the authors show from the experiments, “leaving out useless
interactions results in more accurate models”.

We have reviewed so far a diverse set of tensor-based recommendation tech-
niques. Clearly, tensors help represent and model complex environments in a
convenient and congruent way, suitable for various problem formulations. Nev-
ertheless, as we have already stated earlier, the most common practical task
for RS is to build a ranked list of recommendations (a top-n recommendations
task). In this regard, we summarize related features of some of the most illus-
trative in our opinion methods in Table 2. We also take into account a support
for real-time scenarios in dynamic environments.

5.5 Other models

Unfortunately, it is almost impossible to review all available TF models from
various domains. The ﬂexibility that comes with the tensor-based formulation
provides means for limitless combinations of various RS settings and models.

29

Method

Decomposition Year

Ranking

optimization

Ranking
prediction

Online

TOPHITS
CubeSVD
RTF
BPTF
Multiverse
PITF
TagTR∗∗
TFMAP
GFF

CP
TD
TD
CP
TD
CP∗
TD
CP
CP∗

2005
2005
2009
2010
2010
2010
2010
2012
2015

No
No
Yes
No
No
Yes
No
Yes
No

Yes
Yes
Yes
No
No
Yes
Yes
Yes
Yes

No
No
No
No
No
No
Yes
No
No

Table 2: Comparison of popular TF methods. The Ranking optimization column
denotes whether a ranking-based metric is explicitly used for optimization. The
Ranking prediction column shows whether a method is evaluated against ranking
metrics. The Online column denotes a support for real-time recommendations
for new users (e.g. folding-in).
∗ Method uses pairwise reduction concept, initially introduced in PITF.
∗∗ This is a tag-based tensor reduction model, described in Section 5.2.1.

Here we brieﬂy describe some of them, that were not referenced yet, but have
an interesting application and/or implementation.

Social interactions. The authors of [50] focus on recommending new connec-
tions for users in speciﬁc communities like online dating or professional networks
They emphasize that typically there are two types of groups of people (for in-
stance, employee and employer) in job seeking networks. In order to account
for that and avoid unnecessary recommendations within the same group (e.g.
employer-employer) they split the problem into two parallel subproblems corre-
sponding to each individual group and model it with the CP. The ﬁnal result is
than aggregated from both subproblems by selecting only those predicted links
(i.e. recommendations) which are present in both groups.

The TOPHITS approach, described in Section 5.1.2, is shown to be ap-
plicable for the authorities ranking task in the Twitter community [82]. This
technique can be potentially used for improving the followee recommendations
for twitter users.

Social tagging. A few works for image tagging [63, 7] use a interesting rep-
resentation of data, initially proposed in [24]. Users and images, uploaded by
them in social network, are encoded together into a single long vector. These
vectors are used to build a set of adjacency matrices, that are made with respect
to certain conditions and then stacked together in a tensor. With this approach,
every frontal slice of the tensor describes diﬀerent kinds of relations: friendship
relations between users, user-image connections, tag relations for both users and
items, etc.

30

Temporal models. The authors of [99] add a so called social regularization,
introduced in [54], into a standard optimization routine. The idea behind this
modiﬁcation is to use not only a “wisdom of crowds” like in standard CF ap-
proach, but also utilize information about social relationships (i.e. friendship)
of the user in order to bring more trust into the recommendations an improve
the overall accuracy.

The work [57] combines both social tagging and temporal models. The au-
thors build a 4-th order tensor from (user, item, tag, time) quadruples and
decompose it with the HOSVD. In order estimate relevance scores and recom-
mend new items for users, they ﬁrst summarize values, corresponding to the
observations, over the third (tag) mode.

An interesting hybrid approach for modelling user preferences dynamics is
proposed in [66]. The authors build a tensor from (user, item, time-period )
triplets and combine it with an auxiliary content information (user attributes)
with help of a coupled tensor-matrix factorization method [1, 27]. The idea of
coupled tensor-matrix factorizations provides an additional level of ﬂexibility
in model construction and is used in various RS domains with complex setup
[98, 9].

Multi-criteria ratings systems. The authors of [92] explore the rich senti-
ment information in a product reviews. They extract opinions from text and
craft a multi-aspect (or multi-criteria, see [2]) ratings system on top of it. This
data is used to build a third order tensor in the form (user, item, aspect), with
tensor values denoting the ratings within each aspect (including the explicit
ratings). A CP-based factorization model is used to reconstruct missing values
and predict unknown ratings more accurately.

A similar idea of multi-criteria ratings model was also used as a part of a
sophisticated model in [59]. However, the authors did not have to do any text
analysis as the aspect data was populated by users themselves and provided
within the dataset. They also applied the HOSVD method instead of the CP.

Mobility and geolocation. Modern social networks allow to share not only
a content, such as images or videos, but also link that content to a speciﬁc
locations using the Global Positioning System (GPS) services. With the broad
access of mobile devices to the internet, this provides rich information about
user interests and behavior and allows building highly personalized context-
aware services and applications. For example, the authors of [88, 86] model
location-based user activities with a third order tensor (user, activity, location)
for providing locations and activities recommendations. The authors of [75] use
the tensor for the personalized activities rating prediction. These works use the
Tucker tensor format and apply the HOSVD for its reconstruction.

Cross-domain recommendations. Another interesting direction is combin-
ing a cross-domain knowledge, e.g. user consumption patterns of books, movies,
music, etc., in order to improve recommendations quality. Knowledge about the

31

patterns from one domain may help to build more accurate predictions in an-
other (this is a so called knowledge transfer learning). Moreover, modelling
these cross-domain relations mutually may also help to achieve a higher rec-
ommendations quality across all domains. An interesting challenge in these
tasks is a varying number of items in diﬀerent domains, which requires a special
treatment. A few notable and quite diﬀerent techniques of the tensor-based
knowledge transfer learning are proposed in [14] and [41].

In the theory of matrix approximations
Special factorization methods.
there is well known pseudo-skeleton decomposition method [32], that allows
to use only a small sample of the original matrix elements in order to ﬁnd
an approximate matrix factorization within the desired accuracy. This result
is shown to be generalizable to a higher order case [62, 60], and, remarkably,
is especially suitable for sparse data. The main beneﬁt of such a sampling
approach is the reduced factorization complexity in terms of both the number
of operations and the number of elements required for computation, which is
especially advantageous in case of tensor-based models. A special case of such
a class of TF algorithms is used in the TensorCUR model [55] for product
recommendations.

6 Conclusion

In this survey we have attempted to overview a broad range of tensor-based
methods used in recommender systems to date. As we have seen, these methods
provide powerful set of tools for merging various types of additional informa-
tion, that increases ﬂexibility, customizability and quality of recommendation
models. Tensorization enables creative and non-trivial setups, going far beyond
standard user-item paradigm, and ﬁnds its applications in various domains.
Tensor-based models can also be used as a part of more elaborate systems, pro-
viding compressed latent representations as an input for other well-developed
techniques.

One of the main concerns for the higher order models is inevitable growth of
computational complexity with increasing number of dimensions. Even for mid-
sized production systems, that have to deal with highly dynamic environments,
this might have negative implications, such as inability to produce recommen-
dations for new users instantly, in a timely manner. This type of issues can be
ﬁrmly addressed with incremental update and higher order folding-in techniques.
The former allow to update the entire model, while performing computations
only on new data. The latter allows to calculate recommendations in cases
when new data is already present in the system but was not yet included into
the model.

Despite the encouraging results, there is an issue related to the applicability
of CP and TD decompositions. When the number of dimensions becomes much
higher than 3, application of TD-based methods becomes infeasible due to ex-
plosion of storage requirements. On the other side, CP is generally ill-posed

32

which may potentially lead to numerical instabilities. A possible cure for this
problem is to use TT/HT decomposition. In our opinion, this is a promising
direction for further investigations.

7 Acknowledgements

The authors would like to thank Maxim Rakhuba and Alexander Fonarev for
their help for improving the manuscript, and also Michael Thess for insightful
conversations.

References

[1] E. Acar, T. G. Kolda, and D. M. Dunlavy. All-at-once Optimization for
Coupled Matrix and Tensor Factorizations. arXiv Prepr. arXiv1105.3422,
2011.

[2] G. Adomavicius, N. Manouselis, and Y. Kwon. Multi-criteria recom-

mender systems. Recomm. Syst. Handbook, pages 769–803, 2011.

[3] G. Adomavicius, B. Mobasher, F. Ricci, and A. Tuzhilin. Context-Aware

Recommender Systems. AI Mag., 32(3):67–80, 2011.

[4] G. Adomavicius, R. Sankaranarayanan, S. Sen, and A. Tuzhilin. Incor-
porating contextual information in recommender systems using a multi-
dimensional approach. ACM Trans. Inf. Syst., 23(1):103–145, 2005.

[5] G. Adomavicius and A. Tuzhilin. Toward the Next Generation of Recom-
mender Systems: a Survey of the State of the Art and Possible Extensions.
IEEE Trans. Knowl. Data Eng., 17(6):734–749, 2005.

[6] J. Allan, B. Croft, A. Moﬀat, M. Sanderson, J. Aslam, L. Azzopardi,
N. Belkin, P. Borlund, P. Bruza, J. Callan, and et. al. Frontiers, Chal-
lenges, and Opportunities for Information Retrieval: Report from SWIRL
2012 the Second Strategic Workshop on Information Retrieval in Lorne.
SIGIR Forum, 46(1):2–32, 2012.

[7] P. Barmpoutis, C. Kotropoulos, and K. Pliakos. Image tag recommenda-
tion based on novel tensor structures and their decompositions. In Image
Signal Process. Anal. (ISPA), 2015 9th Int. Symp. on. IEEE, pages 7–12,
2015.

[8] M. Bazire and P. Br´ezillon. Understanding context before using it.

In

Model. using Context, pages 29–40. 2005.

[9] P. Bhargava, T. Phan, J. Zhou, and J. Lee. Who, What, When, and
Where: Multi-Dimensional Collaborative Recommendations Using Tensor
Factorization on Sparse User-Generated Data. Proc. 24th Int. Conf. World
Wide Web, pages 130–140, 2015.

33

[10] J. Bobadilla, F. Ortega, A. Hernando, and A. Guti´errez. Recommender

systems survey. Knowledge-Based Syst., 46:109–132, 2013.

[11] M. Brand.

Incremental singular value decomposition of uncertain data
In Comput. Vision-ECCV 2002, pages 707–720.

with missing values.
Springer, 2002.

[12] P. Brusilovsky. Social information access: the other side of the social web.
In SOFSEM 2008 Theory Pract. Comput. Sci., pages 5–22. Springer, 2008.

[13] R. Burke. Hybrid web recommender systems. Adapt. web, pages 377–408,

2007.

[14] W. Chen, W. Hsu, and M. L. Lee. Making recommendations from multiple
domains. In Proc. 19th ACM SIGKDD Int. Conf. Knowl. Discov. data
Min. - KDD ’13, page 892, New York, New York, USA, aug 2013. ACM
Press.

[15] Y. Chi and S. Zhu. FacetCube. Proc. 19th ACM Int. Conf. Inf. Knowl.

Manag. - CIKM ’10, page 569, 2010.

[16] Y. Chi, S. Zhu, Y. Gong, and Y. Zhang. Probabilistic polyadic factoriza-
tion and its application to personalized recommendation. In Int. Conf.
Inf. Knowl. Manag. Proc., pages 941–950, 2008.

[17] A. Cichocki, R. Zdunek, A. H. Phan, and S. I. Amari. Nonnegative Matrix
and Tensor Factorizations: Applications to Exploratory Multi-Way Data
Analysis and Blind Source Separation. 2009.

[18] P. Comon. Tensors : A brief introduction. IEEE Signal Process. Mag.,

31(3):44–53, 2014.

[19] P. Cremonesi, Y. Koren, and R. Turrin. Performance of recommender algo-
rithms on top-n recommendation tasks. Proc. fourth ACM Conf. Recomm.
Syst. - RecSys ’10, 2010.

[20] L. De Lathauwer, B. De Moor, and J. Vandewalle. A Multilinear Singular
Value Decomposition. SIAM J. Matrix Anal. Appl., 21(4):1253–1278, jan
2000.

[21] V. De Silva and L.-H. Lim. Tensor rank and the ill-posedness of the
best low-rank approximation problem. SIAM J. Matrix Anal. Appl.,
30(3):1084—-1127, 2008.

[22] S. Doerfel and K. D. E. Group. The Role of Cores in Recommender
Benchmarking for Social Bookmarking Systems. ACM Trans. Intell. Syst.
Technol., 7(3), 2016.

[23] P. Dourish. What we talk about when we talk about context. Pers.

Ubiquitous Comput., 8(1):19–30, 2004.

34

[24] D. M. Dunlavy, T. G. Kolda, and W. P. Kegelmeyer. Multilinear algebra
for analyzing data with multiple linkages. Graph Algorithms Lang. Linear
Algebr., (April 2006):85–114, 2011.

[25] C. Eckart and G. Young. The approximation of one matrix by another of

lower rank. Psychometrika, 1(3):211–218.

[26] M. D. Ekstrand, J. T. Riedl, and J. A. Konstan. Collaborative ﬁlter-
ing recommender systems. Found. Trends Human-Computer Interact.,
4(2):81–173, 2011.

[27] B. Ermis, E. Acar, and A. T. Cemgil. Link prediction in heterogeneous
data via generalized coupled tensor factorization. Data Min. Knowl. Dis-
cov., 29(1):203–236, 2013.

[28] X. Fu, K. Huang, W.-K. Ma, N. D. Sidiropoulos, and R. Bro. Joint Tensor
Factorization and Outlying Slab Suppression With Applications. Signal
Process. IEEE Trans., 63(23):6315–6328, 2015.

[29] G. W. Furnas, S. Deerwester, S. T. Dumais, T. K. Landauer, R. A. Harsh-
man, L. A. Streeter, and K. E. Lochbaum. Information retrieval using a
singular value decomposition model of latent semantic structure. In Proc.
11th Annu. Int. ACM SIGIR Conf. Res. Dev. Inf. Retr., pages 465–480.
ACM, 1988.

[30] Z. Gantner, S. Rendle, and L. Schmidt-Thieme. Factorization models
for context-/time-aware movie recommendations. Proc. Work. Context.
Movie Recomm., pages 14–19, 2010.

[31] G. H. Golub and C. Reinsch. Singular value decomposition and least

squares solutions. Numer. Math., 14(5):403–420, 1970.

[32] S. A. Goreinov, E. E. Tyrtyshnikov, and N. L. Zamarashkin. A Theory of

Pseudoskeleton Approximations. Linear Algebra Appl., 261:1–21, 1997.

[33] L. Grasedyck. Hierarchical singular value decomposition of tensors. SIAM

J. Matrix Anal. Appl., 31(4):2029–2054, 2010.

[34] L. Grasedyck, D. Kressner, and C. Tobler. A literature survey of low-
rank tensor approximation techniques. GAMM Mitteilungen, 36(1):53–78,
2013.

[35] T. Hastie, R. Tibshirani, J. Friedman, and J. Franklin. The elements of
statistical learning: data mining, inference and prediction. Math. Intell.,
27(2):83–85, 2005.

[36] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and J. T. Riedl. Evaluat-
ing collaborative ﬁltering recommender systems. ACM Trans. Inf. Syst.,
22(1):5–53, 2004.

35

[37] B. Hidasi. Factorization models for context-aware recommendations. In-

focommun J, VI(4):27–34, dec 2014.

[38] B. Hidasi and D. Tikk. Fast ALS-based tensor factorization for context-
aware recommendation from implicit feedback. In Mach. Learn. Knowl.
Discov. Databases, pages 67–82. 2012.

[39] B. Hidasi and D. Tikk. Context-aware recommendations from implicit
data via scalable tensor factorization. arXiv Prepr. arXiv1309.7611, 2013.

[40] B. Hidasi and D. Tikk. General factorization framework for context-aware

recommendations. Data Min. Knowl. Discov., may 2015.

[41] L. Hu, J. Cao, G. Xu, L. Cao, Z. Gu, and C. Zhu. Personalized Recom-
mendation via Cross-Domain Triadic Factorization. Proc. 22nd Int. Conf.
World Wide Web . Int. World Wide Web Conf. Steer. Comm., pages
595–605, 2013.

[42] Y. Hu, C. Volinsky, and Y. Koren. Collaborative ﬁltering for implicit
feedback datasets. Proc. - IEEE Int. Conf. Data Mining, ICDM, pages
263–272, 2008.

[43] A. Karatzoglou, X. Amatriain, L. Baltrunas, and N. Oliver. Multiverse
recommendation: n-dimensional tensor factorization for context-aware
collaborative ﬁltering. In Proc. fourth ACM Conf. Recomm. Syst. - RecSys
’10, page 79, 2010.

[44] B. P. Knijnenburg and M. C. Willemsen. Evaluating Recommender Sys-
tems with User Experiments. In Recomm. Syst. Handbook, pages 309–352.
Springer, 2015.

[45] T. Kolda and B. Bader. The TOPHITS model for higher-order web link

analysis. Work. Link Anal. Counterterrorism Secur., 7:26–29, 2006.

[46] T. G. Kolda and B. W. Bader. Tensor Decompositions and Applications.

SIAM Rev., 51(3):455–500, aug 2009.

[47] T. G. Kolda, B. W. Bader, and J. P. Kenny. Higher-order web link analysis
using multilinear algebra. In Proc. - IEEE Int. Conf. Data Mining, ICDM,
pages 242–249, 2005.

[48] J. A. Konstan and J. Riedl. Recommender systems: From algorithms
to user experience. User Model. User-adapt. Interact., 22(1-2):101–123,
2012.

[49] Y. Koren, P. Ave, and F. Park. Factorization Meets the Neighborhood : a
Multifaceted Collaborative Filtering Model. In Proc. 14th ACM SIGKDD
Int. Conf. Knowl. Discov. data Min., pages 426—-434, 2008.

36

[50] S. Kutty, L. Chen, and R. Nayak. A people-to-people recommendation
system using tensor space models. Proc. 27th Annu. ACM Symp. Appl.
Comput. - SAC ’12, page 187, 2012.

[51] J. Lee, D. Lee, Y.-C. Lee, W.-S. Hwang, and S.-W. Kim. Improving the
Accuracy of Top-N Recommendation using a Preference Model. Inf. Sci.
(Ny)., 2016.

[52] Z. Liao, C. Wang, and M. Zhang. A Tripartite Tensor Decomposition

Fold- in for Social Tagging. J. Appl. Sci. Eng., 17(4):363–370, 2014.

[53] P. Lops, M. D. Gemmis, and G. Semeraro. Content-based Recommender
Systems: State of the Art and Trends. ”Recomm. Syst. Handbook”, pages
73–105, 2011.

[54] H. Ma, D. Zhou, C. Liu, M. R. Lyu, and I. King. Recommender systems
with social regularization. In Proc. fourth ACM Int. Conf. Web search
data Min., pages 287–296. ACM, 2011.

[55] M. W. Mahoney, M. Maggioni, and P. Drineas. Tensor-CUR Decomposi-
tions for Tensor-Based Data. SIAM J. Matrix Anal. Appl., 30(3):957–987,
2008.

[56] L. B. Marinho, A. Nanopoulos, L. Thieme-Schmidt, R. Jaschke, A. Hotho,
G. Stumme, and P. Symeonidis. Social Tagging Recommender Systems.
Recomm. Syst. Handbook, pages 615–644, 2010.

[57] N. Misaghian, M. Jalali, and M. H. Moattar. Resource recommender
system based on tag and time for social tagging system. Proc. 3rd Int.
Conf. Comput. Knowl. Eng. ICCKE 2013, pages 97–101, 2013.

[58] A. Nanopoulos, D. Rafailidis, P. Symeonidis, and Y. Manolopoulos. Mu-
sicBox: Personalized Music Recommendation Based on Cubic Analysis of
Social Tags. IEEE Trans. Audio. Speech. Lang. Processing, 18(2):407–412,
2010.

[59] M. Nilashi, O. B. Ibrahim, and N. Ithnin. Multi-criteria collaborative ﬁl-
tering with high accuracy using higher order singular value decomposition
and Neuro-Fuzzy system. Knowledge-Based Syst., 60:82–101, 2014.

[60] I. Oseledets and E. Tyrtyshnikov. TT-cross approximation for multidi-

mensional arrays. Linear Algebra Appl., 432(1):70–88, 2010.

[61] I. V. Oseledets. Tensor-Train Decomposition. SIAM J. Sci. Comput.,

33(5):2295–2317, 2011.

[62] I. V. Oseledets, D. V. Savostianov, and E. E. Tyrtyshnikov. Tucker Dimen-
sionality Reduction of Three-Dimensional Arrays in Linear Time. SIAM
J. Matrix Anal. Appl., 30(3):939–956, 2008.

37

[63] M. Panagopoulos and C. Kotropoulos. Image tagging using tensor decom-
position. In Information, Intell. Syst. Appl. (IISA), 2015 6th Int. Conf.,
2015.

[64] D. Parra and P. Brusilovsky. Collaborative ﬁltering for social tagging sys-
tems: an experiment with CiteULike. In Proc. third ACM Conf. Recomm.
Syst., pages 237–240. ACM, 2009.

[65] D. Rafailidis and P. Daras. The TFC model: Tensor factorization and
tag clustering for item recommendation in social tagging systems. IEEE
Trans. Syst. Man, Cybern. Part ASystems Humans, 43(3):673–688, 2013.

[66] D. Rafailidis and A. Nanopoulos. Modeling the Dynamics of User Prefer-
ences in Coupled Tensor Factorization. In Proc. 8th ACM Conf. Recomm.
Syst. - RecSys ’14, pages 321—-324, 2014.

[67] S. Rendle, C. Freudenthaler, Z. Gantner, and L. Schmidt-thieme. BPR
: Bayesian Personalized Ranking from Implicit Feedback. Proc. Twenty-
Fifth Conf. Uncertain. Artif. Intell., cs.LG:452–461, 2009.

[68] S. Rendle, Z. Gantner, C. Freudenthaler, and L. Schmidt-Thieme. Fast
context-aware recommendations with factorization machines.
In Proc.
34th Int. ACM SIGIR Conf. Res. Dev. Inf. Retr., pages 635–644. ACM,
2011.

[69] S. Rendle and L. B. Marinho. Learning optimal ranking with tensor fac-

torization for tag recommendation. Kdd, pages 727–736, 2009.

[70] S. Rendle and L. Schmidt-Thieme. Pairwise interaction tensor factoriza-
tion for personalized tag recommendation. Proc. third ACM Int. Conf.
Web search data Min. (WSDM ’10), pages 81–90, 2010.

[71] A. Rettinger, H. Wermser, Y. Huang, and V. Tresp. Context-aware tensor
decomposition for relation prediction in social networks. Soc. Netw. Anal.
Min., 2(4):373–385, 2012.

[72] F. Ricci, L. Rokach, and B. Shapira. Recommender Systems: Introduction

and Challenges, pages 1–34. Springer, 2015.

[73] D. A. Ross, J. Lim, R.-S. Lin, and M.-H. Yang. Incremental learning for

robust visual tracking. Int. J. Comput. Vis., 77(1-3):125–141, 2008.

[74] A. Said and A. Bellog´ın. Comparative recommender system evaluation.
Proc. 8th ACM Conf. Recomm. Syst. - RecSys ’14, pages 129–136, 2014.

[75] M. Sattari, I. H. Toroslu, P. Karagoz, P. Symeonidis, and Y. Manolopou-
los. Extended feature combination model for recommendations in location-
based mobile services. Knowl. Inf. Syst., 44(3):629–661, 2014.

38

[76] J. B. Schafer, D. Frankowski, J. Herlocker, and S. Sen. The Adaptive Web:
Methods and Strategies of Web Personalization, chapter Collaborat, pages
291–324. Springer Berlin Heidelberg, Berlin, Heidelberg, 2007.

[77] L. Shan, L. Lin, C. Sun, and X. Wang. Predicting ad click-through rates
via feature-based fully coupled interaction tensor factorization. Electron.
Commer. Res. Appl., 16:30–42, 2016.

[78] G. Shani and A. Gunawardana. Evaluating recommendation systems.

Recomm. Syst. Handbook, pages 257–298, 2011.

[79] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis,

volume 47. 2004.

[80] Y. Shi, A. Karatzoglou, L. Baltrunas, M. Larson, A. Hanjalic, and
N. Oliver. TFMAP: Optimizing MAP for top-n context-aware recom-
mendation. Proc. 35th Int. ACM SIGIR Conf. Res. Dev. Inf. Retr., pages
155–164, 2012.

[81] Y. Shi, M. Larson, and A. Hanjalic. Collaborative Filtering beyond the
User-Item Matrix : A Survey of the State of the Art and Future Chal-
lenges. ACM Comput. Surv., 47(1):1–45, 2014.

[82] S. Sizov, S. Staab, and T. Franz. Analysis of Social Networks by Tensor

Decomposition, pages 45–58. Springer, 2010.

[83] J.-T. Sun, H.-J. Zeng, H. Liu, Y. Lu, and Z. Chen. CubeSVD: A Novel
Approach to Personalized Web Search. In Proc. 14th Int. Conf. World
Wide Web - WWW ’05, page 382, 2005.

[84] P. Symeonidis. User recommendations based on tensor dimensionality

reduction. IFIP Int. Fed. Inf. Process., 296:331–340, 2009.

[85] P. Symeonidis. ClustHOSVD: Item Recommendation by Combining Se-
mantically Enhanced Tag Clustering With Tensor HOSVD. IEEE Trans.
Syst. Man, Cybern. Syst., pages 1–12, 2015.

[86] P. Symeonidis, A. Krinis, and Y. Manolopoulos. GeoSocialRec: Explain-
ing recommendations in location-based social networks. In Adv. Databases
Inf. Syst., pages 84–97, 2013.

[87] P. Symeonidis, A. Nanopoulos, and Y. Manolopoulos. A uniﬁed framework
for providing recommendations in social tagging systems based on ternary
semantic analysis. IEEE Trans. Knowl. Data Eng., 22(2):179–192, 2010.

[88] P. Symeonidis, A. Papadimitriou, Y. Manolopoulos, P. Senkul, and
I. Toroslu. Geo-social recommendations based on incremental tensor re-
duction and local path traversal. Proc. 3rd ACM SIGSPATIAL Int. Work.
Locat. Soc. Networks - LBSN ’11, page 1, 2011.

39

[89] P. Symeonidis, M. Ruxanda, A. Nanopoulos, and Y. Manolopoulos.
Ternary Semantic Analysis of Social Tags for Personalized Music Rec-
ommendation. In ISMIR, volume 8, pages 219–224, 2008.

[90] P. Symeonidis, P. Symeonidis, A. Nanopoulos, A. Nanopoulos,
Y. Manolopoulos, and Y. Manolopoulos. Tag recommendations based
on tensor dimensionality reduction. ACM Conf. Recomm. Syst., page 7,
2008.

[91] L. Wang, X. Meng, and Y. Zhang. Applying HOSVD to alleviate the
sparsity problem in Context-aware recommender systems. Chinese J.
Electron., 22(4):773–778, 2013.

[92] Y. Wang, Y. Liu, and X. Yu. Collaborative ﬁltering with aspect-based
opinion mining: A tensor factorization approach. In Proc. - IEEE Int.
Conf. Data Mining, ICDM, pages 1152–1157, 2012.

[93] Z. Wang and L. He. User identiﬁcation for enhancing IP-TV recommen-

dation. Knowledge-Based Syst., 2016.

[94] H. Wermser, A. Rettinger, and V. Tresp. Modeling and learning context-
aware recommendation scenarios using tensor decomposition. In Proc. -
2011 Int. Conf. Adv. Soc. Networks Anal. Mining, ASONAM 2011, pages
137–144, 2011.

[95] W. Woerndl and J. Schlichter. Introducing Context into Recommender
Systems. AAAI’07 Work. Recomm. Syst. e-Commerce, pages 138–140,
2007.

[96] L. Xiong, X. Chen, T.-k. Huang, J. Schneider, and J. G. Carbonell. Tem-
poral Collaborative Filtering with Bayesian Probabilistic Tensor Factor-
ization. Proc. SIAM Int. Conf. Data Min., pages 211—-222, 2010.

[97] Y. Xu, L. Zhang, and W. Liu. Cubic Analysis of Social Bookmarking for
Personalized Recommendation. Front. WWW Res. Dev. - APWeb 2006,
pages 733–738, 2006.

[98] Z. Yang, D. Yin, and B. D. Davison. Recommendation in Academia: A
joint multi-relational model. In ASONAM 2014 - Proc. 2014 IEEE/ACM
Int. Conf. Adv. Soc. Networks Anal. Min., pages 566–571. IEEE, aug 2014.

[99] L. Yao, Q. Z. Sheng, Y. Qin, X. Wang, A. Shemshadi, and Q. He. Context-
aware Point-of-Interest Recommendation Using Tensor Factorization with
Social Regularization. SIGIR 2015 Proc. 38th Int. ACM SIGIR Conf. Res.
Dev. Inf., pages 1007–1010, 2015.

[100] H. Zha and H. Simon. On updating problems in latent semantic indexing.

SIAM J. Sci. Comput., 21(2):1–9, 1999.

40

[101] M. Zhang, C. Ding, and Z. Liao. Tensor fold-in algorithms for social
tagging prediction. In Proc. - IEEE Int. Conf. Data Mining, ICDM, pages
1254–1259, 2011.

[102] W. Zhang, H. Sun, X. Liu, Xiaohui, and Guo. An Incremental Tensor
Factorization Approach for Web Service Recommendation. 2014 IEEE
Int. Conf. Data Min. Work., pages 346–351, 2014.

[103] X. Zhao, X. Li, L. Liao, D. Song, and W. K. Cheung. Crafting a Time-
Aware Point-of-Interest Recommendation via Pairwise Interaction Tensor
Factorization. In Knowl. Sci. Eng. Manag., pages 458–470. Springer, 2015.

[104] G. Zhou, A. Cichocki, Q. Zhao, and S. Xie. Nonnegative matrix and tensor
factorizations: An algorithmic perspective. IEEE Signal Process. Mag.,
31(3):54–65, 2014.

41

