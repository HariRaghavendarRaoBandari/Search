6
1
0
2

 
r
a

 

M
5
1

 
 
]
L
C
.
s
c
[
 
 

1
v
3
5
5
4
0

.

3
0
6
1
:
v
i
X
r
a

Unsupervised Ranking Model for Entity Coreference Resolution

Xuezhe Ma and Zhengzhong Liu and Eduard Hovy

Language Technologies Institute

Carnegie Mellon University
Pittsburgh, PA 15213, USA

{xuezhem, liu}@cs.cmu.edu, ehovy@cmu.edu

Abstract

Coreference resolution is one of the ﬁrst
stages in deep language understanding and
its importance has been well recognized in
the natural language processing community.
In this paper, we propose a generative, un-
supervised ranking model for entity coref-
erence resolution by introducing resolution
mode variables. Our unsupervised system
achieves 58.44% F1 score of the CoNLL met-
ric on the English data from the CoNLL-
2012 shared task (Pradhan et al., 2012), out-
performing the Stanford deterministic sys-
tem (Lee et al., 2013) by 3.01%.

1 Introduction

tasks.

Systems

component

Entity coreference resolution has become a crit-
for many Natural Language
ical
Processing (NLP)
requiring
deep language understanding, such as informa-
tion extraction (Wellner et al., 2004),
semantic
event
(Chambers and Jurafsky, 2008;
Chambers and Jurafsky, 2009), and named entity
linking (Durrett and Klein, 2014; Ji et al., 2014) all
beneﬁt from entity coreference information.

learning

Entity coreference resolution is the task of
identifying mentions (i.e., noun phrases)
in a
text or dialogue that refer to the same real-world
entities.
In recent years, several supervised en-
tity coreference resolution systems have been
proposed, which, according to Ng (2010), can
be categorized into three classes — mention-
(McCarthy and Lehnert, 1995),
pair
models
entity-mention
models
(Yang et al., 2008a;

ranking

Lee et al., 2011)
Haghighi and Klein, 2010;
(Yang et al., 2008b;
and
models
Durrett and Klein, 2013;
Fernandes et al., 2014)
— among which ranking models recently ob-
However,
tained state-of-the-art performance.
the manually annotated corpora that
these sys-
tems rely on are highly expensive to create,
in
particular when we want to build data for resource-
poor languages (Ma and Xia, 2014). That makes
unsupervised approaches, which only require unan-
notated text for training, a desirable solution to this
problem.

Several

unsupervised

algorithms
learning
have been applied to coreference
resolution.
Haghighi and Klein (2007) presented a mention-
pair
fully-generative Bayesian
nonparametric
model
for unsupervised coreference resolution.
Based on this model, Ng (2008) probabilistically
induced coreference partitions via EM cluster-
ing.
Poon and Domingos (2008) proposed an
entity-mention model that is able to perform joint
inference across mentions by using Markov Logic.
Unfortunately, these unsupervised systems’ perfor-
mance on accuracy signiﬁcantly falls behind those
of supervised systems, and are even worse than
the deterministic rule-based systems. Furthermore,
there is no previous work exploring the possibility
of developing an unsupervised ranking model
which achieved state-of-the-art performance under
supervised settings for entity coreference resolution.

In this paper, we propose an unsupervised genera-
tive ranking model for entity coreference resolution.
Our experimental results on the English data from
the CoNLL-2012 shared task (Pradhan et al., 2012)

show that our unsupervised system outperforms the
Stanford deterministic system (Lee et al., 2013) by
3.01% absolute on the CoNLL ofﬁcial metric. The
contributions of this work are (i) proposing the ﬁrst
unsupervised ranking model for entity coreference
resolution. (ii) giving empirical evaluations of this
model on benchmark data sets.
(iii) considerably
narrowing the gap to supervised coreference reso-
lution accuracy.

2 Unsupervised Ranking Model

2.1 Notations and Deﬁnitions
In the following, D = {m0, m1, . . . , mn} repre-
sents a generic input document which is a sequence
of coreference mentions, including the artiﬁcial root
mention (denoted by m0). The method to detect
and extract these mentions is discussed later in Sec-
tion 2.6. Let C = {c1, c2, . . . , cn} denote the
coreference assignment of a given document, where
each mention mi has an associated random vari-
able ci taking values in the set {0, i, . . . , i − 1};
this variable speciﬁes mi’s selected antecedent (ci ∈
{1, 2, . . . , i − 1}), or indicates that it begins a new
coreference chain (ci = 0).

2.2 Generative Ranking Model

The following is a straightforward way to build a
generative model for coreference:

P (D, C) = P (D|C)P (C)

=

n

Qj=1

P (mj|mcj )

P (cj|j)

(1)

n

Qj=1

where we factorize the probabilities P (D|C) and
P (C) into each position j by adopting appropri-
ate independence assumptions that given the coref-
erence assignment cj and corresponding corefer-
ent mention mcj , the mention mj is independent
with other mentions in front of it. This indepen-
dent assumption is similar to that in the IBM 1
model on machine translation (Brown et al., 1993),
where it assumes that given the corresponding En-
glish word, the aligned foreign word is independent
with other English and foreign words. We do not
make any independent assumptions among different
features (see Section 2.4 for details).

Inference in this model is efﬁcient, because we

can compute cj separately for each mention:

c∗
j = argmax

cj

P (mj|mcj )P (cj|j)

The model is a so-called ranking model because it
is able to identify the most probable candidate an-
tecedent given a mention to be resolved.

to

Lee et al., 2013),

2.3 Resolution Mode Variables
previous
According
(Haghighi and Klein, 2009;
work
an-
Ratinov and Roth, 2012;
tecedents are resolved by different categories of
information for different mentions.
For exam-
ple,
the Stanford system (Lee et al., 2013) uses
string-matching sieves to link two mentions with
similar text and precise-construct sieve to link two
mentions which satisfy special syntactic or semantic
relations such as apposition or acronym. Motivated
by this, we introduce resolution mode variables
Π = {π1, . . . , πn}, where for each mention j
the variable πj ∈ {str, prec, attr} indicates in
which mode the mention should be resolved.
In
our model, we deﬁne three resolution modes —
string-matching (str), precise-construct (prec), and
attribute-matching (attr) — and Π is deterministic
when D is given (i.e. P (Π|D) is a point distribu-
tion). We determine πj for each mention mj in the
following way:

• πj = str, if there exists a mention mi, i < j
such that the two mentions satisfy the String
Match sieve, the Relaxed String Match sieve,
or the Strict Head Match A sieve in the Stanford
multi-sieve system (Lee et al., 2013).

• πj = prec, if there exists a mention mi, i < j
such that the two mentions satisfy the Speaker
Identiﬁcation sieve, or the Precise Constructs
sieve.

• πj = attr, if there is no mention mi, i < j

satisﬁes the above two conditions.

Now, we can extend the generative model in Eq. 1
to:

P (D, C) = P (D, C, Π)
n

P (mj|mcj , πj)P (cj |πj, j)P (πj |j)

=

Qj=1

Mode π

prec

str

attr

Feature

Mention Type
Mention Type
Exact Match
Relaxed Match

Head Match
Mention Type

Number
Gender
Person
Animacy

Semantic Class

Distance

Description
the type of a mention. We use three mention types: P roper, N ominal, P ronoun
the same as the mention type feature under prec mode.
boolean feature corresponding to String Match sieve in Stanford system.
boolean feature corresponding to Relaxed String Match sieve in Stanford system.
boolean feature corresponding to Strict Head Match A sieve in Stanford system.
the same as the mention type feature under prec mode.
the number of a mention similarly derived from Lee et al. (2013).
the gender of a mention from Bergsma and Lin (2006) and Ji and Lin (2009).
the person attribute from Lee et al. (2013). We assign person attributes to all mentions, not only pronouns.
the animacy attribute same as Lee et al. (2013).
semantic classes derived from WordNet (Soon et al., 2001).
sentence distance between the two mentions. This feature is for parameter q(k|j, π)

Table 1: Feature set for representing a mention under different resolution modes. The Distance feature is for parameter q, while all
other features are for parameter t.

where we deﬁne P (πj|j) to be uniform distribution.
We model P (mj|mcj , πj) and P (cj|πj, j) in the fol-
lowing way:

P (mj|mcj , πj) = t(mj|mcj , πj)

P (cj|πj , j) = (cid:26) q(cj|πj , j) πj = attr

otherwise

1
j

where θ = {t, q} are parameters of our model. Note
that in the attribute-matching mode (πj = attr)
we model P (cj|πj, j) with parameter q, while in
the other two modes, we use the uniform distribu-
tion. It makes sense because the position informa-
tion is important for coreference resolved by match-
ing attributes of two mentions such as resolving pro-
noun coreference, but not that important for those
resolved by matching text or special relations like
two mentions referring the same person and match-
ing by the name.

2.4 Features
In this section, we describe the features we use to
represent mentions. Speciﬁcally, as shown in Ta-
ble 1, we use different features under different reso-
lution modes. It should be noted that only the Dis-
tance feature is designed for parameter q, all other
features are designed for parameter t.

2.5 Model Learning
run EM algo-
learning,
For model
we
rithm (Dempster et al., 1977)
our Model,
on
treating D as observed data and C as latent vari-
ables. We run EM with 10 iterations and select the
parameters achieving the best performance on the
development data. Each iteration takes around 12
hours with 10 CPUs parallelly. The best parameters

Algorithm 1: Learning Model with EM
1 Initialization: Initialize θ0 = {t0, q0}
2 for t = 0 to T do
3

set all counts c(. . .) = 0
for each document D do

for j = 1 to n do

for k = 0 to j − 1 do

Ljk = t(mj |mk,πj )q(k|πj ,j)
t(mj |mi,πj)q(i|πj ,j)

j−1
P
i=0

c(mj , mk, πj ) += Ljk
c(mk, πj) += Ljk
c(k, j, πj) += Ljk
c(j, πj) += Ljk

// Recalculate the parameters
t(m|m′, π) = c(m,m′,π)
c(m′,π)
q(k, j, π) = c(k,j,π)
c(j,π)

4
5

6

7

8

9
10

11

12

13

Corpora
Gigaword
ON-Dev
ON-Test

# Doc
# Sent
3.6M 75.4M
9,142
343
348
9,615

# Word
1.6B
160K
170K

# Entity

# Mention

-

4,546
4,532

-

19,156
19,764

Table 2: Corpora statistics. “ON-Dev” and “ON-Test” are the
development and testing sets of the OntoNotes corpus.

appear at around the 5th iteration, according to
our experiments.The detailed derivation of
the
learning algorithm is shown in Appendix A. The
pseudo-code is shown is Algorithm 1. We use
uniform initialization for all the parameters in our
model.

Several

previous

work

has

use EM for

to
lution.
Charniak and Elsner (2009)

entity

coreference
Cherry and Bergsma (2005)

applied

attempted
reso-
and
EM for

MIR

Stanford

Multigraph

Our Model

IMS

Latent-Tree

Berkeley

LaSO

Latent-Strc

Model-Stack

Non-Linear

MUC

65.39

64.96

66.22

67.89

67.15

69.46

70.44

70.74

72.11

72.59

72.74

CoNLL’12 English development data

CoNLL’12 English test data

B3

CEAFm CEAFe

Blanc

CoNLL MUC

B3

CEAFm CEAFe

Blanc

CoNLL

54.89

54.49

56.41

57.83

55.19

57.83

59.10

60.03

60.74

61.98

61.77

–

59.39

60.87

62.11

58.86

–

–

65.01

–

–

–

51.36

51.24

52.61

53.76

50.94

54.43

55.57

56.80

57.72

57.58

58.63

–

56.03

58.15

60.58

56.22

–

–

–

–

–

–

57.21

56.90

58.41

59.83

57.76

60.57

61.71

62.52

63.52

64.05

64.38

64.64

64.71

65.41

67.69

67.58

70.51

70.62

70.72

72.17

72.59

72.60

52.52

52.26

54.38

55.86

54.47

57.58

58.20

58.58

59.58

60.44

60.52

–

56.01

58.60

59.66

58.17

–

–

63.45

–

–

–

49.11

49.32

50.21

51.75

50.21

53.86

54.80

59.40

55.67

56.02

57.05

–

53.92

56.03

57.78

55.41

–

–

–

–

–

–

55.42

55.43

56.67

58.44

57.42

60.65

61.21

61.63

62.47

63.02

63.39

Table 3: F1 scores of different evaluation metrics for our model, together with two deterministic systems and one unsupervised
system as baseline (above the dashed line) and seven supervised systems (below the dashed line) for comparison on CoNLL 2012
development and test datasets.

pronoun anaphora resolution. Ng (2008) proba-
bilistically induced coreference partitions via EM
clustering. Recently, Moosavi and Strube (2014)
proposed an unsupervised model utilizing the most
informative relations and achieved competitive
performance with the Stanford system.

2.6 Mention Detection
The basic rules we used to detect mentions are simi-
lar to those of Lee et al. (2013), except that their sys-
tem uses a set of ﬁltering rules designed to discard
instances of pleonastic it, partitives, certain quanti-
ﬁed noun phrases and other spurious mentions. Our
system keeps partitives, quantiﬁed noun phrases and
bare NP mentions, but discards pleonastic it and
other spurious mentions.

3 Experiments

Due to the availability of

3.1 Experimental Setup
Datasets.
readily
parsed data, we select the APW and NYT sec-
tions of Gigaword Corpus
(years 1994-2010)
(Parker et al., 2011) to train the model. Following
previous work (Chambers and Jurafsky, 2008), we
remove duplicated documents and the documents
which include fewer than 3 sentences. The devel-
opment and test data are the English data from the
CoNLL-2012 shared task (Pradhan et al., 2012),
which is derived from the OntoNotes
cor-

on

three measures widely

used

The corpora statistics
pus (Hovy et al., 2006).
are shown in Table 2. Our system is evaluated with
automatically extracted mentions on the version of
the data with automatic preprocessing information
(e.g., predicted parse trees).
Evaluation Metrics.
our
in
model
(Vilain et al., 1995),
the
B3
Entity-
In addition,
based CEAF (CEAFe) (Luo, 2005).
we also report results on another
two popular
metrics: Mention-based CEAF (CEAFm) and
BLANC (Recasens and Hovy, 2011). All the results
are given by the latest version of CoNLL-2012
scorer 1

literature:
(Bagga and Baldwin, 1998),

evaluate

MUC

We

and

3.2 Results and Comparison

and Multigraph:

the results of our model
Table 3 illustrates
as baseline with two deterministic
together
systems, namely Stanford:
the Stanford sys-
tem (Lee et al., 2011)
the
unsupervised multigraph system (Martschat, 2013),
and one unsupervised system, namely MIR: the
unsupervised system using most
informative re-
lations (Moosavi and Strube, 2014). Our model
outperforms the three baseline systems on all the
evaluation metrics. Speciﬁcally, our model achieves
improvements of 2.93% and 3.01% on CoNLL F1

1http://conll.cemantix.org/2012/software.html

score over the Stanford system, the winner of the
CoNLL 2011 shared task, on the CoNLL 2012
development and test sets, respectively. The im-
provements on CoNLL F1 score over the Multigraph
model are 1.41% and 1.77% on the development
and test sets, respectively. Comparing with the
MIR model, we obtain signiﬁcant improvements of
2.62% and 3.02% on CoNLL F1 score.

Acknowledgements

This research was supported in part by DARPA
grant FA8750-12-2-0342 funded under the DEFT
program. Any opinions, ﬁndings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reﬂect
the views of DARPA.

set

features

structured

perceptron

latent

tree model

(Durrett and Klein, 2013);

To make a thorough empirical comparison with
previous studies, Table 3 (below the dashed line)
also shows the results of some state-of-the-art
supervised coreference resolution systems — IMS:
the second best system in the CoNLL 2012 shared
task (Bj¨orkelund and Farkas, 2012); Latent-Tree:
(Fernandes et al., 2012)
the
obtaining the best
results in the shared task;
Berkeley:
the Berkeley system with the ﬁnal
LaSO:
feature
system with non-
the
(Bj¨orkelund and Kuhn, 2014);
local
Latent-Strc:
sys-
tem (Martschat and Strube, 2015); Model-Stack:
the
stack-
ing (Clark and Manning, 2015); and Non-Linear:
the non-linear mention-ranking model with feature
representations (Wiseman et al., 2015). Our unsu-
pervised ranking model outperforms the supervised
IMS system by 1.02% on the CoNLL F1 score, and
achieves competitive performance with the latent
tree model. Moreover, our approach considerably
narrows the gap to other supervised systems listed
in Table 3.

system with model

entity-centric

the

latent

structure

4 Conclusion

We proposed a new generative, unsupervised rank-
ing model for entity coreference resolution into
which we introduced resolution mode variables to
distinguish mentions resolved by different cate-
gories of information. Experimental results on the
data from CoNLL-2012 shared task show that our
system signiﬁcantly improves the accuracy on dif-
ferent evaluation metrics over the baseline systems.
One possible direction for future work is to dif-
ferentiate more resolution modes. Another one is to
add more precise or even event-based features to im-
prove the model’s performance.

References

[Bagga and Baldwin1998] Amit Bagga and Breck Bald-
win. 1998. Algorithms for scoring coreference chains.
In The ﬁrst international conference on language re-
sources and evaluation workshop on linguistics coref-
erence, volume 1, pages 563–566. Citeseer.

[Bergsma and Lin2006] Shane Bergsma and Dekang Lin.
2006. Bootstrapping path-based pronoun resolution.
In Proceedings of ACL-2006, pages 33–40, Sydney,
Australia, July. Association for Computational Lin-
guistics.

[Bj¨orkelund and Farkas2012] Anders Bj¨orkelund

and
2012. Data-driven multilingual
Rich´ard Farkas.
coreference resolution using resolver stacking.
In
Proceedings of EMNLP-CoNLL-2012 - Shared Task,
pages 49–55, Jeju Island, Korea, July. Association for
Computational Linguistics.

[Bj¨orkelund and Kuhn2014] Anders Bj¨orkelund

and
Jonas Kuhn. 2014. Learning structured perceptrons
for coreference resolution with latent antecedents
and non-local features. In Proceedings of ACL-2014,
pages 47–57, Baltimore, Maryland, June. Association
for Computational Linguistics.

[Brown et al.1993] Peter F Brown, Vincent J Della Pietra,
Stephen A Della Pietra, and Robert L Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation.
linguistics,
19(2):263–311.

Computational

[Chambers and Jurafsky2008] Nathanael Chambers and
Dan Jurafsky. 2008. Unsupervised learning of narra-
tive event chains. In Proceedings of ACL-2008: HLT,
pages 789–797, Columbus, Ohio, June. Association
for Computational Linguistics.

[Chambers and Jurafsky2009] Nathanael Chambers and
Dan Jurafsky. 2009. Unsupervised learning of nar-
rative schemas and their participants. In Proceedings
of ACL-2009, pages 602–610, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.

[Charniak and Elsner2009] Eugene Charniak and Micha
Elsner. 2009. EM works for pronoun anaphora reso-
lution. In Proceedings of EACL 2009, pages 148–156,
Athens, Greece, March.

[Cherry and Bergsma2005] Colin Cherry and Shane
2005. An Expectation Maximization
Bergsma.
approach to pronoun resolution.
In Proceedings of
CoNLL-2005, pages 88–95, Ann Arbor, Michigan,
June.

[Clark and Manning2015] Kevin Clark and Christo-
pher D. Manning. 2015. Entity-centric coreference
resolution with model stacking.
In Proceedings of
ACL-IJCNLP-2015, pages 1405–1415, Beijing, China,
July. Association for Computational Linguistics.

[Dempster et al.1977] Arthur P Dempster, Nan M Laird,
and Donald B Rubin. 1977. Maximum likelihood
from incomplete data via the em algorithm. Journal
of the royal statistical society. Series B (methodologi-
cal), pages 1–38.

[Durrett and Klein2013] Greg Durrett and Dan Klein.
2013. Easy victories and uphill battles in coreference
resolution.
In Proceedings of EMNLP-2013, pages
1971–1982, Seattle, Washington, USA, October. As-
sociation for Computational Linguistics.

[Durrett and Klein2014] Greg Durrett and Dan Klein.
2014. A joint model for entity analysis: Coreference,
typing, and linking.
In Proceedings of the Transac-
tions of the Association for Computational Linguistics.
[Fernandes et al.2012] Eraldo Fernandes, C´ıcero dos San-
tos, and Ruy Milidi´u. 2012. Latent structure percep-
tron with feature induction for unrestricted coreference
resolution.
In Proceedings of EMNLP-CoNLL-2012
- Shared Task, pages 41–48, Jeju Island, Korea, July.
Association for Computational Linguistics.

[Fernandes et al.2014] Eraldo

Fernandes,
C´ıcero Nogueira dos Santos, and Ruy Luiz Mi-
lidi´u. 2014. Latent trees for coreference resolution.
Computational Linguistics.

Rezende

[Haghighi and Klein2007] Aria Haghighi and Dan Klein.
2007. Unsupervised coreference resolution in a non-
parametric bayesian model. In Proceedings of ACL-
2007, pages 848–855, Prague, Czech Republic, June.
Association for Computational Linguistics.

[Haghighi and Klein2009] Aria Haghighi and Dan Klein.
2009. Simple coreference resolution with rich syntac-
tic and semantic features. In Proceedings of EMNLP-
2009, pages 1152–1161, Singapore, August. Associa-
tion for Computational Linguistics.

[Haghighi and Klein2010] Aria Haghighi and Dan Klein.
2010. Coreference resolution in a modular, entity-
centered model.
In Proceedings of NAACL-2010,
pages 385–393, Los Angeles, California, June. Asso-
ciation for Computational Linguistics.

[Hovy et al.2006] Eduard Hovy, Mitchell Marcus, Martha
Palmer, Lance Ramshaw, and Ralph Weischedel.
2006. Ontonotes: The 90% solution. In Proceedings
of NAACL-2006, pages 57–60, New York City, USA,
June. Association for Computational Linguistics.

[Ji and Lin2009] Heng Ji and Dekang Lin. 2009. Gender
and animacy knowledge discovery from web-scale n-
grams for unsupervised person mention detection. In
Proceedings of PACLIC-2009, pages 220–229.

[Ji et al.2014] Heng Ji, HT Dang,

J Nothman, and
B Hachey. 2014. Overview of tac-kbp2014 entity dis-
covery and linking tasks. In Proc. Text Analysis Con-
ference (TAC2014).

[Lee et al.2011] Heeyoung Lee, Yves Peirsman, Angel
Chang, Nathanael Chambers, Mihai Surdeanu, and
Dan Jurafsky. 2011. Stanford’s multi-pass sieve coref-
erence resolution system at the conll-2011 shared task.
In Proceedings of CoNLL-2011: Shared Task, pages
28–34, Portland, Oregon, USA, June. Association for
Computational Linguistics.

[Lee et al.2013] Heeyoung Lee, Angel Chang, Yves
Peirsman, Nathanael Chambers, Mihai Surdeanu, and
Dan Jurafsky. 2013. Deterministic coreference reso-
lution based on entity-centric, precision-ranked rules.
Comput. Linguist., 39(4):885–916, December.

[Luo2005] Xiaoqiang Luo. 2005. On coreference resolu-
tion performance metrics. In Proceedings of EMNLP-
2005, pages 25–32, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.

[Ma and Xia2014] Xuezhe Ma and Fei Xia. 2014. Un-
supervised dependency parsing with transferring dis-
tribution via parallel guidance and entropy regulariza-
tion. In Proceedings of ACL-2014, pages 1337–1348,
Baltimore, Maryland, June.

[Martschat and Strube2015] Sebastian Martschat

and
Michael Strube. 2015. Latent structures for corefer-
ence resolution. Transactions of the Association for
Computational Linguistics, 3:405–418.

[Martschat2013] Sebastian Martschat. 2013. Multigraph
clustering for unsupervised coreference resolution. In
ACL-2013: Student Research Workshop, pages 81–
88, Soﬁa, Bulgaria, August. Association for Compu-
tational Linguistics.

[McCarthy and Lehnert1995] Joseph F McCarthy and
Wendy G Lehnert. 1995. Using decision trees for
conference resolution. In Proceedings of IJCAI-1995,
pages 1050–1055. Morgan Kaufmann Publishers Inc.
[Moosavi and Strube2014] Naﬁse Sadat Moosavi and
Michael Strube. 2014. Unsupervised coreference res-
olution by utilizing the most informative relations. In
Proceedings of COLING-2014, pages 644–655.

[Ng2008] Vincent Ng. 2008. Unsupervised models for
coreference resolution.
In Proceedings of EMNLP-
2008, pages 640–649, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.

[Ng2010] Vincent Ng. 2010. Supervised noun phrase
coreference research: The ﬁrst ﬁfteen years. In Pro-
ceedings of ACL-2010, pages 1396–1411, Uppsala,

Sweden, July. Association for Computational Linguis-
tics.

[Parker et al.2011] Robert Parker, David Graff, Junbo
Kong, Ke Chen, and Kazuaki Maeda. 2011. English
gigaword ﬁfth edition. Linguistic Data Consortium,
LDC2011T07.

2008.

[Poon and Domingos2008] Hoifung Poon and Pedro
Joint unsupervised coreference
Domingos.
In Proceedings of
resolution with Markov Logic.
EMNLP-2008, pages 650–659, Honolulu, Hawaii,
October. Association for Computational Linguistics.

[Pradhan et al.2012] Sameer Pradhan, Alessandro Mos-
chitti, Nianwen Xue, Olga Uryupina, and Yuchen
Zhang. 2012. Conll-2012 shared task: Modeling
multilingual unrestricted coreference in ontonotes. In
Proceedings of EMNLP-CoNLL-2012 - Shared Task,
pages 1–40, Jeju Island, Korea, July. Association for
Computational Linguistics.

[Ratinov and Roth2012] Lev Ratinov and Dan Roth.
2012. Learning-based multi-sieve co-reference reso-
lution with knowledge.
In Proceedings of EMNLP-
CoNLL-2012, pages 1234–1244, Jeju Island, Korea,
July. Association for Computational Linguistics.

[Recasens and Hovy2011] Marta Recasens and Eduard
Hovy. 2011. Blanc: Implementing the rand index for
coreference evaluation. Natural Language Engineer-
ing, 17(04):485–510.

[Soon et al.2001] Wee Meng Soon, Hwee Tou Ng, and
Daniel Chung Yong Lim. 2001. A machine learning
approach to coreference resolution of noun phrases.
Computational linguistics, 27(4):521–544.

[Vilain et al.1995] Marc Vilain, John Burger, John Ab-
erdeen, Dennis Connolly, and Lynette Hirschman.
1995. A model-theoretic coreference scoring scheme.
In Proceedings of the 6th conference on Message un-
derstanding, pages 45–52. Association for Computa-
tional Linguistics.

[Wellner et al.2004] Ben Wellner, Andrew McCallum,
Fuchun Peng, and Michael Hay. 2004. An integrated,
conditional model of information extraction and coref-
erence with application to citation matching. In Pro-
ceedings of the 20th conference on Uncertainty in ar-
tiﬁcial intelligence, pages 593–601. AUAI Press.

[Wiseman et al.2015] Sam Wiseman, Alexander M. Rush,
Stuart Shieber, and Jason Weston. 2015. Learning
anaphoricity and antecedent ranking features for coref-
erence resolution.
In Proceedings of ACL-IJCNLP-
2015, pages 1416–1426, Beijing, China, July. Asso-
ciation for Computational Linguistics.

[Yang et al.2008a] Xiaofeng Yang, Jian Su, Jun Lang,
Chew Lim Tan, Ting Liu, and Sheng Li. 2008a. An
entity-mention model for coreference resolution with
inductive logic programming. In Proceedings of ACL-
2008, pages 843–851.

[Yang et al.2008b] Xiaofeng Yang,

and
Chew Lim Tan.
2008b. A twin-candidate model
for learning-based anaphora resolution. Computa-
tional Linguistics, 34(3):327–356.

Jian

Su,

Appendix A. Derivation of Model Learning
Formally, we iteratively estimate the model parame-
ters θ, employing the following EM algorithm:

E-step: Compute the posterior probabilities of C,

P (C|D; θ), based on the current θ.

M-step: Calculate the new θ′

expected

complete
the
EP (C|D;θ)[log P (D, C; θ′)]

that maximizes
log
likelihood,

For simplicity, we denote:

P (C|D; θ) = ˜P (C|D)
P (C|D; θ′) = P (C|D)

In addition, we use τ (πj|j) to denote the probability
P (πj|j) which is uniform distribution in our model.
Moreover, we use the following notation for conve-
nience:

θ(mj, mk, j, k, πj ) = t(mj|mk, πj)q(k|πj , j)τ (πj |j)

Then, we have

E ˜P (c|D)[log P (D, C)]

˜P (C|D) log P (D, C)

= P
C
= P
C
n
P
j=1

=

˜P (C|D)(cid:0)

n
P
j=1

log t(mj |mcj , πj) + log q(cj|πj , j) + log τ (πj |j)(cid:1)

j−1
P
k=0

Ljk(cid:0) log t(mj |mk, πj ) + log q(k|πj , j) + log τ (πj|j)(cid:1)

Then the parameters
E ˜P (c|D)[log P (D, C)] satisfy that

t and q that maximize

t(mj |mk, πj ) =

q(k|πj , j) =

Ljk
n
P
i=1

Ljk

Lik

j−1
P
i=0

Lji

where Ljk can be calculated by

Ljk = PC,cj =k

˜P (C|D) =

P

˜P (C,D)

C,cj =k

˜P (C,D)

P
C

=

=

=

=

=

n
Q
i=1

P

C,cj =k
n
Q
i=1

P
C

˜θ(mi,mci ,ci,i,πi)

˜θ(mi,mci ,ci,i,πi)

˜θ(mj ,mk,k,j,πj ) P

˜P (C(−j)|D)

C(−j)

j−1
P
i=0

˜θ(mj ,mi,i,j,πj ) P

˜P (C(−j)|D)

C(−j)

˜θ(mj ,mk,k,j,πj )

˜θ(mj ,mi,i,j,πj )

˜t(mj|mk,πj )˜q(k|πj ,j)˜τ (πj |j)

˜t(mj |mi,πj)˜q(i|πj ,j)˜τ (πj |j)

j−1
P
i=0

j−1
P
i=0

˜t(mj|mk,πj )˜q(k|πj ,j)

j−1
P
i=0

˜t(mj |mi,πj)˜q(i|πj ,j)

where C(−j) = {c1, . . . , cj−1, cj+1, . . . , cn}. The
above derivations correspond to the learning algo-
rithm in Algorithm 1.

