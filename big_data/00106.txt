Designing Domain Speciﬁc Word Embeddings: Applications

to Disease Surveillance

Saurav Ghosh∗1,2, Prithwish Chakraborty†1,2, Emily Cohn‡3,4, John S.

Brownstein§3,4, and Naren Ramakrishnan¶1,2

1Department of Computer Science, Virginia Tech, VA, USA

2Discovery Analytics Center, Virginia Tech, VA, USA

3Children’s Hospital Informatics Program, Boston Children’s Hospital, Boston,

4Department of Pediatrics, Harvard Medical School, Boston, MA, USA

MA, USA

Abstract

Traditional disease surveillance can be augmented with a wide variety of real-
time sources such as news and social media. However, these sources are in general
unstructured and construction of surveillance tools such as taxonomical correlations
and trace mapping involves considerable human supervision. In this paper, we mo-
tivate a disease vocabulary driven word2vec model (Dis2Vec) which we use to model
diseases and constituent attributes as word embeddings from the HealthMap news
corpus. We use these word embeddings to create disease taxonomies and evaluate
our model accuracy against human annotated taxonomies. We compare our accu-
racies against several state-of-the art word2vec methods. Our results demonstrate
that Dis2Vec outperforms traditional distributed vector representations in its ability
to faithfully capture disease attributes and accurately forecast outbreaks.

1

Introduction

Traditional disease surveillance has often relied on a multitude of reporting networks
such as outpatient networks, on-ﬁeld healthcare workers, and lab networks. Some of
the most eﬀective tools while analyzing or mapping diseases, especially new diseases or
disease spreading to new regions, are reliant on building disease taxonomies and early
detection of outbreaks.

∗sauravcsvt@vt.edu
†prithwi@vt.edu
‡Emily.Cohn@childrens.harvard.edu
§John.Brownstein@childrens.harvard.edu
¶naren@cs.vt.edu

1

6
1
0
2

 
r
a

M
1

 

 
 
]

G
L
.
s
c
[
 
 

1
v
6
0
1
0
0

.

3
0
6
1
:
v
i
X
r
a

In recent years, the ready availability of social and news media has led to services
such as HealthMap which have been used to track several disease outbreaks from news
media ranging from the ﬂu to Ebola. However, most of this data is unstructured and
often noisy. Annotating such corpora thus requires signiﬁcant human oversight. While
signiﬁcant information about both endemic [4, 21] and rare [18] diseases can be extracted
from such news corpora, traditional text analytics methods such as lemmatization and
tokenization are often shallow and do not retain suﬃcient contextual information. More
involved methods such as topic models are too computationally expensive for real-time
worldwide surveillance and do not provide simple semantic contexts that could be used
to comprehend the data.

Figure 1: Comparative performance of disease speciﬁc model (Dis2Vec: red) against
general word2vec models (SGNS : blue, SGHS : green, and CBOW : orange). The four
corners represent the disease characteristics of interest. Accuracy of each model for a
speciﬁc characteristic is plotted along the corresponding corner. Shaded area under the
curve for each model represent the overall accuracy for characterizations. Disease speciﬁc
models shows better characterization performance than the classical ones.

In recent years, several deep learning based methods such as word2vec and doc2vec
has been found to be promising in analyzing such text corpora. These methods once
trained over a representative corpus can be readily used to analyze new text and ﬁnd
semantic constructs (e.g. rabies:zoonotic = salmonella:foodborne) which can be useful
both for taxonomy creation as well as other learning tasks such as disease tag prediction.
Classical word2vec methods are generally unsupervised requiring no domain information
and as such has broad applicability. However, for highly speciﬁed domains such as disease
surveillance with moderate sized corpus, classical methods fails to ﬁnd semantic relation-

2

SymptomsExposuresTransmissionMethodTransmissionAgents0.950.600.790.480.620.600.760.480.560.510.750.460.480.350.450.26Dis2VecSGNSSGHSCBOWships that are meaningful with respect to diseases. For example, traditional word2vec
methods such as skip-gram model from HealthMap generates the word saintpaul as the
answer to the query rabies:zoonotic = salmonella:??.

Motivated by this problem, in this paper we postulate a vocabulary-driven word2vec
algorithm that can ﬁnd meaningful disease constructs that can be used towards such dis-
ease knowledge extractions. For example, in response to the same query rabies:zoonotic
= salmonella:??, vocabulary-driven word2vec algorithm generates the word foodborne
which is more meaningful in the context of disease knowledge extraction. Our contribu-
tions can be listed as below:

• We formalize a vocabulary driven word2vec method that uses a novel domain
driven negative sampling procedure to generate disease speciﬁc word embeddings.
• We use such word embeddings to diﬀerentiate between disease speciﬁc and general
words. These distinctions are then used to generate automated disease taxonomies
that are then evaluated against gold standard human curated ones for accuracies.
• We demonstrate the use of such word embeddings to identify diseases tags and
outbreaks from unstructured texts and compare against human annotated cor-
pora to assess the applicability of such word embeddings towards general disease
surveillance methods.

2 Related Work

The related works of interest for our problem are primarily from the ﬁeld of neural-
network based word embeddings and their applications in a variety of NLP tasks. In
recent years, we have witnessed a tremendous surge of research concerned with represent-
ing words from unstructured corpus to dense low-dimensional vectors using inspirations
from neural-network language modeling [3, 5, 15]. These representations, referred to
as word embeddings, have been shown to perform with considerable accuracy and ease
across a variety of linguistic tasks [1, 6, 19].

Mikolov et al. [12, 13] proposed skip-gram model, currently a state-of-the-art word
embedding method, which can be trained using either hierarchical softmax (SGHS) [13]
or the negative sampling technique (SGNS) [13]. Skip-gram models have been found to be
highly eﬃcient in ﬁnding word embedding templates from huge amounts of unstructured
text data and uncover various semantic and syntactic relationships. Mikolov et al. [13]
also showed that the such word embeddings have the capability to capture linguistic
regularities and patterns. These patterns can be represented as linear translations in
the vector space. For example, vec(’Madrid’) - vec(’Spain’) + vec(’France’) is closer to
vec(’Paris’) than to any other word [14, 9].

Levy et al. [10] analyzed the theoretical founding of skip-gram model and showed
that the training method of SGNS can be converted into a weighted matrix factorization
and its objective induces a implicit factorization of a shifted PMI matrix - the well-
known word-context PMI matrix [2, 20] shifted by a constant oﬀset. In [11], Levy et

3

al. performed an exhaustive evaluation showing the impact of each parameter (window
size, context distribution smoothing, sub-sampling of frequent words and others) on the
performance of SGNS and other recent word embedding methods, such as GLoVe [16].
They found that SGNS consistently proﬁts from larger negative samples (> 1) showing
signiﬁcant improvement on various NLP tasks with higher values of negative samples.

Previous works on neural embeddings (including the skip-gram model) deﬁne the
contexts of a word to be its linear context (words preceding and following the target
word). Levy et al. [8] generalized the skip-gram model and used syntactic contexts
derived from automatically generated dependency parse-trees. These syntactic contexts
were found to capture more functional similarities, while the bag-of-words nature of the
contexts in the original skip-gram model generates broad topical similarities.

3 Model Formulation

We are provided with a human curated taxonomy of 40 diseases of interest where each
disease was classiﬁed into four categories - transmission method(s) (foodborne, water-
borne, etc.), transmission agent(s) (mosquito, ﬂy, etc.), clinical symptoms (cough, runny
nose, etc.) and exposures or risk factors (healthcare worker, school-age child, etc.). In
Table 1, we provide classiﬁcation of three emerging diseases (Ebola, MERS and H7N9 )
into four above-mentioned categories as recorded in the human annotated taxonomy.
We constructed a disease-related vocabulary V by converting the entire taxonomy into
a ﬂat list of words or terms. The vocabulary V consists of disease names and all the
terms related to transmission method(s), transmission agent(s), clinical symptoms and
exposures or risk factors. Given this pre-speciﬁed disease-related vocabulary V (domain
information) and an unstructured health-related news corpus D, our goal is to derive
the human curated taxonomy of diseases automatically by means of neural embeddings
or word embeddings.

Table 1: Classiﬁcation of three emerging diseases into four categories as provided in the
human curated taxonomy.

Disease

Ebola

MERS

Method(s) of
transmission
direct
con-
tact

direct
contact,
zoonotic

Transmission
agents

wild animal

domestic ani-
mal, wild an-
imal

Clinical symptoms

Hemorrhagic, fever,
diarrhea, vomiting

Fever, cough, pneu-
monia, diarrhea

H7N9

zoonotic

domestic ani-
mal

Fever, cough, pneu-
monia

Exposures or risk fac-
tores
Healthcare worker, ani-
mal slaughter
facility,
healthcare
worker,
healthcare
animal exposure, mass
gathering, travel
farmer, market, slaugh-
ter, animal, exposure

4

3.1 The Skip-gram Model with Negative Sampling (SGNS )

In this section, we present a brief description of SGNS - the skip-gram model introduced
in [12] trained using the negative sampling procedure in [13]. The basic objective of
the skip-gram model is to infer word embeddings that will be relevant for predicting
the surrounding words in a sentence or a document. The skip-gram model can also be
trained using Hierarchical Softmax method as shown in [13].

3.1.1 Setting and Notation
The inputs to the skip-gram model are a corpus of words w ∈ W and their corresponding
contexts c ∈ C where W and C are the word and context vocabularies.
In SGNS ,
the contexts of word wi are deﬁned by the words surrounding it in an L-sized context
window wi−L, . . . , wi−1, wi+1, . . . , wi+L. Therefore, the corpus can be transformed into
a collection of observed context and word pairs as D. The notation #(w, c) represents
c∈C(w, c) and
w∈W (w, c) where #(w) and #(c) are the total number of times w and c
occurred in D. Each word w ∈ W corresponds to a vector w ∈ Rd and similarly, each
context c ∈ C is represented as a vector c ∈ Rd, where d is the dimensionality of the
word or context embedding. The entries in the vectors are the latent parameters to be
learned.

the number of times the pair (w, c) occurs in D. Therefore, #(w) = (cid:80)
#(c) = (cid:80)

3.1.2 Objective of SGNS

SGNS tries to maximize the probability whether a single word-context pair (w, c) was
generated from the observed corpus D. Let P (D = 1|w, c) refers to the probability
that (w, c) was generated from the corpus, and P (D = 0|w, c) = 1 − P (D = 1|w, c) the
probability that (w, c) was not. The objective function for a single (w, c) pair is modeled
as:

P (D = 1|w, c) = σ(w · c) =

1

1+e−w·c

(1)

where w and c are the d-dimensional latent parameters or vectors to be learned.
The objective of the negative sampling is to maximize P (D = 1|w, c) for observed
(w, c) pairs while minimizing P (D = 0|w, c) for randomly sampled ”negative” contexts
(hence the name ”negative sampling”), under the assumption that randomly selecting
a context for a given word will tend to generate an unobserved (w, c) pair. SGNS ’s
objective for a single (w, c) observation is then:

(2)

log σ(w · c) + k · EcN∼PD [log σ(−w · cN)]
according to the smoothed unigram distribution PD(c) = #(c)α(cid:80)

where k is the number of ”negative” samples and cN is the sampled context, drawn
c #(c)α where α = 0.75 is the

smoothing parameter.

5

The objective of SGN S is trained in an online fashion using stochastic gradient
updates over the observed pairs in the corpus D. The global objective then sums over
the observed (w, c) pairs in the corpus:

(cid:18)
(cid:19)
log σ(w · c) + k · EcN∼PD [log σ(−w · cN)]

lSGNS =(cid:80)

(w,c)∈D

(3)

Optimizing this objective will have a tendency to generate similar embeddings for ob-
served word-context pairs, while scattering unobserved pairs in the vector space. Intu-
itively, words that appear in similar contexts or tend to appear in the contexts of each
other should have similar embeddings.

3.2 Disease Speciﬁc Word2vec Model (Dis2Vec)

In this section, we introduce Dis2Vec, a disease speciﬁc word2vec model whose objective
is to derive taxonomy of diseases automatically given an input unstructured corpus D.
We used a pre-speciﬁed disease-related vocabulary V (domain information) to guide the
discovery process of word embeddings in Dis2Vec. The input corpus D consists of a
collection of (w, c) pairs. Based on V, we can categorize the (w, c) pairs into three types
as shown below.

• D(d) = {(w, c) : w ∈ V ∧ c ∈ V}.
• D(¬d) = {(w, c) : w /∈ V ∧ c /∈ V}.
• D(d)(¬d) = {(w, c) : w ∈ V ⊕ c ∈ V}.

D = D(d) + D(¬d) + D(d)(¬d)

3.2.1 Vocabulary Driven Negative Sampling
The ﬁrst category (D(d)) of (w, c) pairs where both w ∈ V and c ∈ V is relevant to us
in generating disease speciﬁc word embeddings. We need to maximize log σ(w · c) in
order to achieve similar embeddings for these disease word-context pairs. For negative
sampling, we adopted a vocabulary (V) driven approach for these disease word-context
pairs. Since minimizing the dot product leads to dissimilar embeddings of word-context
pairs, we argue that for an arbitrary (w, c) ∈ D(d), instead of random sampling it would
be favorable to sample negative examples from the set of non-disease contexts (c /∈ V).
This targeted sampling of negative contexts will ensure dissimilar embeddings of disease
words (w ∈ V) and non-disease contexts c /∈ V, thus scattering them in the vector
space. However, sampling negative examples only from the set of non-disease contexts
may lead to overﬁtting and thus we introduce a sampling parameter πs which controls
the probability of drawing a negative example from non-disease contexts versus disease

6

contexts (c ∈ V). Dis2Vec’s objective for (w, c) ∈ D(d) is then:

(cid:18)

(cid:88)

lD(d) =

log σ(w · c)

(w,c)∈D(d)

(cid:19)
+ k · [P (xk < πs)EcN∼PDc /∈V [log σ(−w · cN)]
+ P (xk ≥ πs)EcN∼PDc∈V [log σ(−w · cN)]]

(4)

(cid:80)

(cid:80)

where xk ∼ U (0, 1), U(0,1) being the uniform distribution on the interval [0,1].
If
xk < πs, we sample a negative context cN from the unigram distribution PDc /∈V where
Dc /∈V is the collection of (w, c) pairs for which c /∈ V and PDc /∈V =
#(c)α
c /∈V #(c)α where α
is the smoothing parameter. For values of xk ≥ πs, we sample cN from the unigram
distribution PDc∈V and PDc∈V = #(c)α
c∈V #(c)α . When πs > 0.5, higher number of negative
contexts will be sampled from the set cN /∈ V . Therefore, optimizing the objective in
equation 4 in such a scenario will make disease word-context pairs ((w, c) ∈ D(d)) have
similar embeddings, while scattering (w, cN ) pairs where w ∈ V and cN /∈ V . As a
result, Dis2Vec will have a tendency to generate disease speciﬁc word embeddings when
πs > 0.5.
3.2.2 Objective of D(¬d)
The second category (D(¬d)) of (w, c) pairs consists of those pairs for which w /∈ V and
c /∈ V. These pairs are irrelevant to us in generating disease speciﬁc word embeddings
since both w and c are not a part of V. However, minimizing their dot products will
scatter these pairs in the embedding space and thus, a word w /∈ V can have similar
embeddings (or, get closer) to a word w ∈ V which should be avoidable in our scenario.
Therefore, we need to maximize log σ(w · c) for these (w, c) pairs in order to achieve
similar (or, closer) embeddings. We adopted the objective function in equation 3 for
(w, c) ∈ D(¬d).

(cid:88)

(cid:18)
(cid:19)
log σ(w · c) + k · EcN∼PD [log σ(−w · cN)]

(5)

lD(¬d) =

(w,c)∈D(¬d)

3.2.3 Vocabulary Driven Objective Minimization
Lastly, the third category (D(d)(¬d)) consists of (w, c) pairs where either w ∈ V or c ∈ V
but both cannot be in V. Consider an arbitrary (w, c) pair belonging to D(d)(¬d). As per
the objective (equation 3) of SGNS , two words are similar to each other if they share
the same contexts or if they tend to appear in the contexts of each other (and preferably
If w ∈ V and c /∈ V, then maximizing log σ(w · c) will have the tendency to
both).
generate similar embeddings for the disease word w ∈ V and non-disease words w /∈ V
which share the same non-disease context c /∈ V. On the other word, if c ∈ V and
w /∈ V, then maximizing log σ(w · c) will drive the embedding of the non-disease word

7

w /∈ V closer to the embeddings of disease words w ∈ V sharing the same disease context
c ∈ V. Therefore, we posit that the objective log σ(w · c) for this category of (w, c)
pairs should be minimized in order to generate disease speciﬁc embeddings. However,
minimizing the dot products of all such (w, c) pairs may lead to over-penalization and
thus we introduce an objective selection parameter πo which controls the probability of
selecting log σ(−w · c) versus log σ(w · c). The objective for (w, c) ∈ D(d)(¬d) is then:

(cid:88)

(cid:18)

lD(d)(¬d) =

(w,c)∈D(d)(¬d)

P (z < πo) log σ(−w · c)

(6)

(cid:19)

+ P (z ≥ πo) log σ(w · c)

where z ∼ U (0, 1), U(0,1) being the uniform distribution over the interval [0,1].
If
z < πo, Dis2Vec optimizes log σ(−w · c), otherwise Dis2Vec optimizes log σ(w · c).
When πo > 0.5, the objective log σ(−w · c) will be selected for optimization with a
higher probability, thus scattering disease words from those non-disease words sharing
the same non-disease contexts or vice-versa. Therefore, Dis2Vec will tend to generate
disease speciﬁc embeddings for values of πo > 0.5.
Finally, the overall objective function of Dis2Vec comprising all three categories of (w, c)
pairs can be deﬁned as below.

lDis2Vec = lD(d) + lD(¬d) + lD(d)(¬d)

(7)

Similar to SGNS , the objective in equation 7 is trained in an online fashion using

stochastic gradient updates over the three categories of (w, c) pairs.

Algorithm 1: Dis2Vec model
Input : Unstructured corpus D = {(w, c)}, V
Output: word embeddings w∀w ∈ W , column embeddings c∀c ∈ C
D(¬d) = {(w, c) : w /∈ V ∧ c /∈ V}, D(d)(¬d) = {(w, c) : w ∈ V ⊕ c ∈ V}

1 Categorize D into 3 types: D(d) = {(w, c) : w ∈ V ∧ c ∈ V},
2 for each (w, c) ∈ D do
if (w, c) ∈ D(d) then

3

4

5

6

7

8

train the (w, c) pair using the objective in equation 4

else if (w, c) ∈ D(¬d) then

train the (w, c) pair using the objective in equation 5

else

train the (w, c) pair using the objective in equation 6

8

3.3 Parameters in Dis2Vec

Dis2Vec inherits all the parameters of SGNS , such as window size (L), number of neg-
ative samples (k) and context distribution smoothing (α). It also introduces two new
parameters - the objective selection parameter (πo) and the sampling parameter (πs).
We provide a brief description of the impact of each of the parameters below:

• Window size (L): The window size (L) deﬁnes the length of the context to each
side of the target word wi. Levy et al. [11] showed that SGNS shows superior
performance with larger values of L. The reason is that large value of L will result
in more (w, c) training pairs and can lead to higher accuracy at the expense of
training time. For Dis2Vec, we experimented with three values of L : 5, 10, 15.

• Negative samples (k): According to Mikolov et al. [13], values of k in the range
of (5 − 20) are preferable with small training datasets. For larger datasets, values
of k in the range of (2 − 5) are useful. Levy et el. [11] showed that SGNS always
prefers larger negative samples (k > 1) in terms of providing higher accuracies on
diﬀerent tasks. For Dis2Vec, we experimented with three values of k : 1, 5, 15.

• Context distribution smoothing (α): In both SGNS and Dis2Vec, the negative con-
texts are sampled from a smoothed unigram distribution of contexts. By smoothed
unigram distribution, we mean that all context counts are raised to the power of
α. Levy et al. [11] found that the value of α = 0.75 shows consistent improvement
over the unsmoothed value (α = 1.0) for all the similarity and analogy tasks. For
Dis2Vec, we experimented with two values of alpha : unsmoothed (α = 1.0) and
smoother (α = 0.75).

• Sampling parameter (πs): For (w, c) ∈ D(d), the value of πs decides whether the
negative contexts should be sampled from the set cN /∈ V versus cN ∈ V. We
experimented with three values of πs : 0.3, 0.5, 0.7.

• Objective selection parameter (πo): For (w, c) ∈ D(d)(¬d), the value of πo decides
whether the dot products of (w, c) pairs should be minimized or they should be
maximized. We experimented with three values of πo : 0.3, 0.5, 0.7.

4 Experimental Evaluation

In this section we will brieﬂy describe our experimental setup in Section 4.1 where
will discuss the news corpus and the human annotated taxonomy against which we
evaluatedDis2Vec. We will also brieﬂy describe the disease speciﬁc information that was
used in the process. We will conclude this section with our experimental ﬁndings in
Section 4.2 where we will compare our model against several baselines and also explore
its applicability to emerging diseases.

9

Table 2: Symptom categories and corresponding words

Symptom Category Words
General
Gastrointestinal
Respiratory

Nervous system

Cutaneous
Circulatory
Musculoskeletal

Fever, chill, weight loss, fatigue, lethargy, headache
Abdominal pain, nausea, diarrhea, vomiting
Cough, runny nose, sneezing, chest pain, sore throat,
pneumonia, dyspnea
Mental status, paralysis, paresthesia, encephalitis,
meningitis
Rash, sore, pink eye
Hemorrhagic
Joint pain, muscle pain, muscle ache

4.1 Experimental Setup

4.1.1 Corpus

We collected a dataset corresponding to a corpus of public health related articles ex-
tracted from HealthMap [7], a prominent online aggregator of news articles from all over
the world for disease outbreak monitoring and real-time surveillance of emerging public
health threats. The articles so collected were in English and pre-processed by remov-
ing non-textual elements, sentence splitting, tokenization and lemmatization via BASIS
technologies’ Rosette Language Processing (RLP) tools [17]. After pre-processing, the
corpus consisting of 124850 articles were found to contain 1607921 sentences, spanning
52679298 lemmas. Words that appeared less than 5 times in the corpus were ignored,
resulting in a vocabulary of 91178 terms for both words and contexts.

4.1.2 Human Annotated Taxonomy

Literature reviews were conducted for each of the 40 diseases of interest in order to make
classiﬁcations for method(s) of transmission, transmission agent(s), clinical symptoms
and exposures or risk factors.

Method(s) of transmission was ﬁrst classiﬁed as either chronic/non-infectious or in-
fectious. We focus on the infectious category in this study. Then, within the infectious
category, further classiﬁcation was attained using nine transmission method subcate-
gories - direct contact, droplet, airborne, bloodborne, zoonotic, vectorborne, waterborne,
foodborne, and ‘environmental’. For many diseases, multiple transmission method sub-
categories could be assigned. Transmission agent(s) was classiﬁed into eight categories
- wild animal, fomite, ﬂy, mosquito, bushmeat, ﬂea, tick and domestic animal. Clinical
symptoms were broken down into eight categories: general, gastrointestinal, respiratory,
nervous system, cutaneous, circulatory, musculoskeletal, and urogenital. A full list of the
symptoms within each category can be found in Table 2. For disease exposures or risk
factors, six categories were assigned based on those listed/most commonly reported in
the literature. The categories include: healthcare facility/worker, school-age child, mass

10

gathering, travel-related, animal exposure, and weakened immune system. The animal
exposure category was further broken down into four subcategories: farmer, veterinar-
ian, market, slaughter. For some diseases, there were no risk factors listed, and for other
diseases, multiple exposures were assigned.

Figure 2: Distribution of word counts corresponding to each taxonomical category in the
disease vocabulary (V). Words related to clinical symptoms constitute the majority of
V with relatively much smaller percentages of terms related to exposures, transmission
agent(s) and transmission method(s)

4.1.3 Disease Vocabulary V
Disease vocabulary V is provided as domain knowledge to Dis2Vec in order to generate
disease speciﬁc word embeddings as explained in section 3.2. V is represented by a ﬂat
list of disease-related terms consisting of disease names and all other words related to
four categories of disease taxonomy - transmission method(s), transmission agent(s),
clinical symptoms and exposures or risk factors. Total number of terms in V was found
to be 103.
In Figure 2, we show the distribution of counts of terms associated with
diﬀerent taxonomical categories in the disease vocabulary (V). As depicted in Figure 2,
half of the words in V are terms related to clinical symptoms followed by exposures or
risk factors (22.4%), transmission method(s) (13.8%) and transmission agent(s) (13.8%).

4.1.4 Baselines

We compared the following baseline models with Dis2Vec on the four disease taxonomy
tasks.

11

22.4%50.0%13.8%13.8%TransmissionAgentsExposuresSymptomsTransmissionMethod• SGNS : Unsupervised skip-gram model with negative sampling [13] described in

section 3.1.

• SGHS : skip-gram model trained using the hierarchical softmax algorithm instead
of negative sampling. The description of the hierarchical softmax algorithm can
be found in [13].

• CBOW : Continuous bag-of-words model described in [12]. Unlike skip-gram mod-
els, the training objective of the CBOW model is to correctly predict the target
word given its contexts (surrounding words). CBOW is denoted as a bag-of-words
model as the order of words in the contexts does not have any impact on the model.

All models (both baselines and Dis2Vec) are trained on the HealthMap corpus using
a 300-dimensional word embedding via gensim’s word2vec software. We explored a large
space of parameters for each model.
In Table 3, we provide the list of parameters,
the explored values for each parameter and the applicable models corresponding to
each parameter. Apart from the parameters listed in Table 3, we also applied the sub-
sampling technique developed by Mikolov et al. [13] to each model in order to counter
the imbalance between common words (such as, is, of, the, a, etc.) and rare words. In
the context of NLP, these common words are called stop words. In the sub-sampling
approach, a word w with an associated frequency f (w) is discarded against a threshold
t (typically, 10−5 according to probability:

P (w) = 1 −(cid:112)t/f (w)

Our initial experiments (not reported) showed that both the baselines and Dis2Vec
showed improved results on the disease taxonomy tasks with sub-sampling versus without
sub-sampling.

Table 3: The space of parameters explored in this work. For each parameter, we provide
the corresponding list of explored values and the applicable models.

Parameters

Window size (L)

Explored
values

5, 10, 15

‘Negative’ samples (k)

1, 5, 15

Applicable Models

Dis2Vec, Dis2Vec-objective, Dis2Vec-
sample, SGNS , SGHS , CBOW
Dis2Vec, Dis2Vec-objective, Dis2Vec-
sample, SGNS
Dis2Vec, Dis2Vec-objective, Dis2Vec-
sample, SGNS

Context distribution smooth-
ing (α)
Objective function selection
parameter (πo)
Sampling parameter (πs)

0.75, 1.0

0.3, 0.5, 0.7

Dis2Vec, Dis2Vec-objective

0.3, 0.5, 0.7

Dis2Vec, Dis2Vec-sample

12

4.2 Results

In this section we try to ascertain the eﬃcacy and the applicability of Dis2Vec by investi-
gating some of the pertinent questions related to the problem of disease characterization.

1. Are Dis2Vec word embeddings consistent with disease corpora?

2. Sample-vs-objective: which is the better method to incorporate disease

domain information into Dis2Vec?

3. Does disease vocabulary information improve disease characterization?

4. What are beneﬁcial parameter conﬁgurations for generating disease spe-

ciﬁc word embeddings?

5. Importance of taxonomical categories - how should we construct the

disease speciﬁcations?

6. Can Dis2Vec be applied to characterize emerging diseases?

We use cosine similarity in a min-max setting between words identiﬁed by a model

and the human annotated words for a particular category as our accuracy metric.

Are Dis2Vec word embeddings consistent with disease corpora? Classical
Word2Vec embeddings has been found to be consistent with semantic and syntactic
information present in a corpus. However, its not immediately evident, especially for
highly speciﬁed corpus such as disease related news, whether such embeddings are also
consistent across at a document level with respect to the disease tags. Focusing on the
diseases articles for United States, w e investigate this consistency by taking the average
vectors for all the words in a document under Dis2Vec as features and the disease tag
for the corresponding document as the label. We split the documents into 80% − 20%
train-test set using a stratiﬁed scheme and ﬁt a K-nearest neighbor using cosine distance
on the trains set. The overall classiﬁcation accuracy on the training set was found to
be 97.62%. Figure3 shows the confusion matrix for the top 10 diseases. As can be seen,
the confusion matrix indicates that we achieve high number of correct classiﬁcations
across all the major diseases. Thus, we can argue that our word vectors retains the
discriminative information for the disease corpus.

Sample-vs-objective: which is the better method to incorporate disease
domain information into Dis2Vec? As described in Section 3, there are primarily
two diﬀerent ways by which disease vocabulary information (V) guides the embeddings
for Dis2Vec (a) by modulating negative sampling parameter (πs) for disease word-context
pairs ((w, c) ∈ D(d)) referred to as Dis2Vec-sample and (b) by modulating the objective
selection parameter (πo) for non-disease words or non-disease contexts ((w, c) ∈ D(d)(¬d))
referred to as Dis2Vec-objective. We investigate the importance of these two strategies by
comparing the accuracies for each strategy individually (Dis2Vec-sample and Dis2Vec-
objective) as well as combined together (Dis2Vec-combined ) under the best parameter
conﬁguration for a particular task in Table 4. As can be seen, no single strategy is best

13

Figure 3: Confusion Matrix for Disease Tag prediction for top 10 diseases in US.

Table 4: Comparative performance evaluation of Dis2Vec-combined against Dis2Vec-
objective and Dis2Vec-sample across the four taxonomy tasks under the best parameter
conﬁguration for that model and task combination

Taxonomy tasks
Symptoms
Exposures
Transmission method
Transmission agents
Overall average accuracy

Dis2Vec-sample Dis2Vec-objective Dis2Vec-combined
0.637
0.599
0.789
0.481
0.626

0.941
0.582
0.723
0.486
0.683

0.946
0.546
0.753
0.486
0.683

across all tasks. Henceforth, we select the best performing strategy for a particular task
as our Dis2Vec in the next Table 5.

Does disease domain information improve disease characterization? Dis2Vec

was designed to incorporate disease domain information in the form of pre-speciﬁed dis-
ease vocabulary. To evaluate the importance of such domain information in Dis2Vec, we
compare the performance of Dis2Vec against the baseline word2vec models under best

14

tuberculosisebolameaslesrabiessalmonellameningitiswest_nile_virusinfluenzamalariadenguedenguemalariainfluenzawest_nile_virusmeningitissalmonellarabiesmeaslesebolatuberculosis00000000010.0060.0120.0060000.01200.9600.00320.0080.0080.00320.00160.00640.00640.960000.001200.003700.00250.970.0170.00120.00120.00230.00230.002300.00230.980.00470.00470000.0040.00200.990.00200.0040000.001900.99000.00370.00930000.00470.970.00470000.0230001000000.0029000.900.00560.011000.00560.0730.005600.00.20.40.60.81.0Table 5: Comparative performance evaluation of Dis2Vec against SGNS , SGHS and
CBOW across the four taxonomy tasks under the best parameter conﬁguration for that
model and task combination

Taxonomy tasks
Symptoms
Exposures
Transmission method
Transmission agents
Overall average accuracy

CBOW SGHS
0.561
0.477
0.509
0.347
0.452
0.752
0.455
0.259
0.384
0.569

SGNS Dis2Vec
0.617
0.604
0.759
0.486
0.617

0.946
0.599
0.789
0.486
0.705

parameter conﬁguration described in section 4.1.4. These baseline models do not per-
mit incorporation of any domain information due to their unsupervised nature. Table 4
presents the accuracy of the models for the 4 taxonomy tasks viz. symptoms, exposures,
transmission nature and transmission agents. As can be seen, Dis2Vec performs the
best for 2 tasks and in average.
It is also interesting to note that Dis2Vec achieves
higher performance gain over the baseline models for the symptoms category than the
other categories. The superior performance of Dis2Vec in the symptoms category can
be attributed to the higher percentage of symptom words in the disease vocabulary
compared to the percentage of words related to other categories (see Figure 2). This
exhibits a trade-oﬀ relationship where one can increase characterization accuracy for a
given task by providing more domain information for the said task, thereby necessitating
more domain knowledge.

What are suitable parameter conﬁgurations for generating disease speciﬁc
word embeddings? To identify which parameter settings are beneﬁcial for generating
disease speciﬁc word embeddings, we looked at the best parameter conﬁguration of all the
6 models on each task. We then counted the number of times each parameter setting was
chosen in these conﬁgurations. For Dis2Vec-sample and Dis2Vec-objective, some trends
emerge regarding the parameter πo that these two models consistently beneﬁt from val-
ues of πo > 0.5 validating our claims in section 3.2 that when πo > 0.5, disease words
and non-disease words get scattered from each other in the vector space, thus tending
to generate more disease speciﬁc word embeddings. However, for πs we observed mixed
trends. As expected, Dis2Vec-sample beneﬁts from higher values of sampling parameter
πs > 0.5. But Dis2Vec-combined seems to prefer lower values of πs < 0.5 and higher
values of πr > 0.5 for the disease taxonomy tasks. For the smoothing parameter(α),
all the applicable models prefer smoothed unigram distribution (α = 0.75) for negative
sampling except Dis2Vec-combined which is in total favor of unsmoothed distribution
(α = 1.0) for the disease characterization tasks. For the number of negative samples k,
all the applicable models seem to beneﬁt from k > 1 for the disease taxonomy tasks.
For the window size (L), all the models prefer smaller-sized context windows for the dis-
ease taxonomy tasks except SGHS which prefers larger-sized windows for characterizing
diseases.

Importance of taxonomical categories - how should we construct the dis-
ease speciﬁcations? We followup our previous analysis by investigating the impor-

15

Table 6: Evaluating the importance of disease vocabulary (V) on the Dis2Vec model.

Taxonomy tasks

Dis2Vec

(Exposures)

Symptoms
Exposures
Transmission
method
Transmission
agents

0.609
0.584

0.742

0.459

Dis2Vec
(Transmission
method)
0.581
0.594

Dis2Vec
(Transmission
agents)
0.373
0.400

0.757

0.463

0.504

0.428

Dis2Vec

Dis2Vec

(Symptoms)

0.937
0.435

0.464

0.425

0.946
0.599

0.789

0.486

Table 7: The impact of each parameter, measured by the number of tasks in which the
best conﬁguration had that parameter setting.

Method

Dis2Vec-combined
Dis2Vec-sample
Dis2Vec-objective
SGNS
SGHS
CBOW

L

k

α

πs

πo

5 : 10 : 15
2 : 2 : 0
2 : 1 : 1
3 : 1 : 0
2 : 2 : 0
1 : 0 : 3
4 : 0 : 0

1 : 5 : 15
1 : 2 : 1
1 : 1 : 2
2 : 0 : 2
0 : 2 : 2

NA
NA

0.75 : 1

0.3 : 0.5 : 0.7

0.3 : 0.5 : 0.7

0 : 4
4 : 0
4 : 0
4 : 0
NA
NA

3 : 1 : 0
1 : 1 : 2

NA
NA
NA
NA

0 : 1 : 3

NA

2 : 0 : 2

NA
NA
NA

tance of each category in the disease vocabulary towards ﬁnal characterization accuracy.
To evaluate a particular category, we used a truncated domain speciﬁcation consisting
of disease names and the words in the corresponding category to drive the discovery of
word embeddings in Dis2Vec under the best parameter conﬁguration corresponding to
that category. We compared the characterization accuracy of each of these conditions
across the 4 taxonomy tasks. Table 6 presents our results for this analysis and depicts
that supplying all the categories leads to better characterizations across all the tasks.
However, for some categories (such as symptoms), the symptoms-only model performs
almost similar to the Dis2Vec. This similar performance can again be attributed to the
higher percentage of symptom-related words in the disease vocabulary. However, the
performance for such biased models is severely degraded for other tasks.

Can Dis2Vec be applied to characterizing emerging diseases? Character-
izing emerging diseases is of prime importance to public health agencies. Incidentally,
such diseases also make for an interesting case study as the outbreaks are in general
associated with enhanced media coverage. We show a distribution of the news articles
with disease tags corresponding to diseases worldwide over the HealthMap corpus in
Figure 5. Figure 5 depicts the heightened media coverage for emerging diseases, specif-
ically Ebola. Thus analyzing such emerging diseases using Dis2Vec from unstructured
news corpus can lead to better understanding of these diseases and can be potentially
characterized with relatively good accuracies due to the presence of a large number of

16

related disease speciﬁc contexts in the corpus. We show the accuracy distribution for
these three emerging diseases in Figure 4. As can be seen, Dis2Vec(red) depicts the best
performance for most categories and for most diseases. It performs especially well with
respect to symptoms which has the highest proportion of words in the disease vocabulary
(see Figure 2). Conversely, it comes oﬀ second best to SGNS w.r.t exposures, indicating
that the model needs more exposures-related words to learn better embeddings.

Ebola: In Table 1, we show the human annotated taxonomy for the Ebola disease.
On comparing the symptoms in the human annotated taxonomy with the top symptom
terms discovered by Dis2Vec(Figure 5), we ﬁnd that Dis2Vec is able to detect three
human annotated symptom words diarrhea, fever and hemorrhagic in comparison to the
second best performing model SGNS which is only able to detect one human annotated
symptom word hemorrhagic. For transmission method(s), transmission agent(s) and
exposures, Dis2Vec and SGNS show almost similar performance.

H7N9: For H7N9, Dis2Vec is able to classify correctly all the three human annotated
symptom words (see Table 1) fever, cough, pneumonia with 100% accuracy. SGNS is
only able to able to detect two human annotated symptom words pneumonia and fever.
Apart from symptoms, for transmission method(s), Dis2Vec is correctly able to classify
H7N 9 as zoonotic disease, but SGNS failed to do so.

MERS: For MERS, Dis2Vec performs better than SGNS for the symptoms category.

For other categories, Dis2Vec and SGNS show almost similar performance.

5 Conclusions

Classical word2vec methods such as SGNS and SGHS have been applied to solve a vari-
ety of linguistic tasks with considerable accuracy. However, such methods fail to generate
satisfactory embeddings for highly speciﬁc domains such as health where uncovering the
relationships with respect to domain speciﬁc words is of greater importance than the
non-domain ones. These algorithms are by design unsupervised and do not permit the
inclusion of domain information to ﬁnd interesting embeddings. In this paper, we have
proposed Dis2Vec, a disease speciﬁc word2vec framework that given an unstructured
news corpus and domain knowledge in terms of important words, can ﬁnd interesting
disease characterizations. We demonstrated the strength of our model by comparing it
against three classical word2vec method on 4 disease characterization tasks. Dis2Vec
exhibits the best overall accuracy for the tasks and in general its relative performance
improvement is found to be empirically dependent on the amount of supplied domain
knowledge. Dis2Vec is especially useful to analyze emerging diseases and we show the
disease characterization performances over three emerging disease viz. Ebola, MERS and
H7N9. As before, Dis2Vec works especially well for characteristics with more domain
knowledge (symptoms) and is found to be a promising tool to analyze such emerging
diseases. In future, we aim to analyze a greater variety of emerging diseases and try
to ascertain common relationships between such diseases across diﬀerent geographical
regions.

17

6 Acknowledgements

Supported by the Intelligence Advanced Research Projects Activity (IARPA) via Depart-
ment of Interior National Business Center (DoI/NBC) contract number D12PC000337,
the US Government is authorized to reproduce and distribute reprints of this work for
Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer:
The views and conclusions contained herein are those of the authors and should not
be interpreted as necessarily representing the oﬃcial policies or endorsements, either
expressed or implied, of IARPA, DoI/NBC, or the US Government.

References

[1] M. Baroni, G. Dinu, and G. Kruszewski. Don’t count, predict! a systematic com-
parison of context-counting vs. context-predicting semantic vectors. In ACL (1),
pages 238–247, 2014.

[2] M. Baroni and A. Lenci. Distributional memory: A general framework for corpus-

based semantics. Computational Linguistics, 36(4):673–721, 2010.

[3] Y. Bengio, H. Schwenk, J.-S. Sen´ecal, F. Morin, and J.-L. Gauvain. Neural prob-
In Innovations in Machine Learning, pages 137–186.

abilistic language models.
Springer, 2006.

[4] P. Chakraborty, N. Ramakrishnan, et al. Forecasting a moving target: Ensemble
models for ILI case count predictions. In Proceedings of the 2014 SIAM International
Conference on Data Mining, Philadelphia, Pennsylvania, USA, April 24-26, 2014,
pages 262–270, 2014.

[5] R. Collobert and J. Weston. A uniﬁed architecture for natural language processing:
Deep neural networks with multitask learning. In Proceedings of the 25th interna-
tional conference on Machine learning, pages 160–167. ACM, 2008.

[6] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Nat-
ural language processing (almost) from scratch. The Journal of Machine Learning
Research, 12:2493–2537, 2011.

[7] C. C. Freifeld, K. D. Mandl, B. Y. Reis, and J. S. Brownstein. Healthmap: global
infectious disease monitoring through automated classiﬁcation and visualization of
internet media reports. Journal of the American Medical Informatics Association,
15(2):150–157, 2008.

[8] O. Levy and Y. Goldberg. Dependency-based word embeddings. In Proceedings of
the 52nd Annual Meeting of the Association for Computational Linguistics, ACL
2014, June 22-27, 2014, Baltimore, MD, USA, Volume 2: Short Papers, pages
302–308, 2014.

18

[9] O. Levy and Y. Goldberg. Linguistic regularities in sparse and explicit word repre-
sentations. In Proceedings of the Eighteenth Conference on Computational Natural
Language Learning, CoNLL 2014, Baltimore, Maryland, USA, June 26-27, 2014,
pages 171–180, 2014.

[10] O. Levy and Y. Goldberg. Neural word embedding as implicit matrix factorization.
In Advances in Neural Information Processing Systems 27: Annual Conference on
Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Que-
bec, Canada, pages 2177–2185, 2014.

[11] O. Levy, Y. Goldberg, and I. Dagan. Improving distributional similarity with lessons

learned from word embeddings. TACL, 3:211–225, 2015.

[12] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Eﬃcient estimation of word repre-

sentations in vector space. CoRR, abs/1301.3781, 2013.

[13] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed repre-
sentations of words and phrases and their compositionality. In Advances in Neural
Information Processing Systems 26: 27th Annual Conference on Neural Informa-
tion Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013,
Lake Tahoe, Nevada, United States., pages 3111–3119, 2013.

[14] T. Mikolov, W. Yih, and G. Zweig. Linguistic regularities in continuous space
word representations. In Human Language Technologies: Conference of the North
American Chapter of the Association of Computational Linguistics, Proceedings,
June 9-14, 2013, Westin Peachtree Plaza Hotel, Atlanta, Georgia, USA, pages 746–
751, 2013.

[15] A. Mnih and G. E. Hinton. A scalable hierarchical distributed language model. In

Advances in neural information processing systems, pages 1081–1088, 2009.

[16] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word
representation.
In Proceedings of the 2014 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A
meeting of SIGDAT, a Special Interest Group of the ACL, pages 1532–1543, 2014.

[17] N. Ramakrishnan, P. Butler, S. Muthiah, et al.

’beating the news’ with embers:
Forecasting civil unrest using open source indicators.
In Proceedings of the 20th
ACM SIGKDD International Conference on Knowledge Discovery and Data Min-
ing, pages 1799–1808, New York, NY, USA, 2014. ACM.

[18] T. Rekatsinas, S. Ghosh, S. R. Mekaru, E. O. Nsoesie, J. S. Brownstein, L. Getoor,
and N. Ramakrishnan. Sourceseer: Forecasting rare disease outbreaks using multiple
data sources. In Proceedings of the 2015 SIAM International Conference on Data
Mining, pages 379–387.

19

[19] J. Turian, L. Ratinov, and Y. Bengio. Word representations: a simple and general
method for semi-supervised learning. In Proceedings of the 48th annual meeting of
the association for computational linguistics, pages 384–394. Association for Com-
putational Linguistics, 2010.

[20] P. D. Turney and P. Pantel. From frequency to meaning: Vector space models of

semantics. Journal of Artiﬁcial Intelligence Research, 37:141–188, 2010.

[21] Z. Wang, P. Chakraborty, S. R. Mekaru, J. S. Brownstein, J. Ye, and N. Ramakrish-
nan. Dynamic poisson autoregression for inﬂuenza-like-illness case count prediction.
In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 1285–1294. ACM, 08 2015.

20

Ebola

MERS

H7N9

(a) Symptoms

(b) Symptoms

(c) Symptoms

(d) Exposures

(e) Exposures

(f) Exposures

(g) Transmission Agents

(h) Transmission Agents

(i) Transmission Agents

(j) Transmission Method

(k) Transmission Method

(l) Transmission Method

Figure 4: Case Study for Emerging Diseases: Disease characterization accuracy plot for
Dis2Vec (ﬁrst quadrant, red), SGNS (second quadrant, blue), SGHS (third quadrant,
green), and CBOW (fourth quadrant, orange) w.r.t Ebola (left), MERS (middle) and
H7N9 (right). The shaded area in a quadrant indicates the cosine similarity (scaled
between 0 and 1) of the top words found for the category of interest using corresponding
model, as evaluated against against the human annotated words (see Table 1). The top
words found for each model is shown in the corresponding quadrant with radius equal to
its average similarity with the human annotated words for the disease. Dis2Vec shows
best overall performance with noticeable improvements for symptoms w.r.t all diseases.

21

Dis2VecSGNSSGHSCBOWhemorrhagicfeverencephalitismeningitisdiarrheahemorrhagicmental_statusparesthesiameningitispink_eyehemorrhagicfeverpink_eyechest_painmeningitisparesthesiachillhemorrhagicheadacheweight_lossEBOLADis2VecSGNSSGHSCBOWhemorrhagicheadachevomitingdiarrheaencephalitispneumoniamental_statusparesthesiahemorrhagicsneezingpneumoniahemorrhagicparalysisdyspneachest_painchillpink_eyemeningitisencephalitisheadacheMERSDis2VecSGNSSGHSCBOWdiarrheafeverhemorrhagiccoughpneumoniahemorrhagicparesthesiamental_statuspneumoniafeverhemorrhagicfeverpneumoniaencephalitisdyspneafeverchillnauseasore_throatmental_statusH7N9Dis2VecSGNSSGHSCBOWhealthcare_workermass_gatheringtravelhealthcare_facilityschoolchildhealthcare_workermass_gatheringhealthcare_facilitytravelschoolchildhealthcare_workertravelmass_gatheringchildhealthcare_facilitychildhealthcare_facilityanimalexposuretravelEBOLADis2VecSGNSSGHSCBOWhealthcare_workermass_gatheringtravelhealthcare_facilityweakened_immune_systemmass_gatheringhealthcare_workertravelhealthcare_facilityschoolchildhealthcare_facilityhealthcare_workertravelmass_gatheringexposurehealthcare_workerslaughterveterinarianschoolchildfarmerMERSDis2VecSGNSSGHSCBOWhealthcare_workermass_gatheringexposurehealthcare_facilityanimalmass_gatheringhealthcare_workerslaughterhealthcare_facilityanimalhealthcare_facilityslaughtertravelanimalexposureslaughterweakened_immune_systemtravelmarkethealthcare_workerH7N9Dis2VecSGNSSGHSCBOWflywild_animalmosquitodomestic_animalbushmeatflybushmeatfomitedomestic_animalmosquitoflybushmeatfomitedomestic_animalmosquitoflyfleamosquitofomitetickEBOLADis2VecSGNSSGHSCBOWdomestic_animalfleawild_animalflymosquitofomitedomestic_animalflybushmeatwild_animalflyfomitebushmeattickdomestic_animalbushmeatwild_animalmosquitodomestic_animalfleaMERSDis2VecSGNSSGHSCBOWwild_animalfleadomestic_animalflymosquitodomestic_animalfomitebushmeatfleawild_animalfomiteflybushmeatdomestic_animalmosquitobushmeatdomestic_animaltickfomitefleaH7N9Dis2VecSGNSSGHSCBOWairbornedirect_contactdropletairbornevectorbornedirect_contactairbornedirect_contactdropletfoodborneairbornedirect_contactEBOLADis2VecSGNSSGHSCBOWairbornedropletdirect_contactairbornedropletdirect_contactairbornedropletdirect_contactenvironmentaldirect_contactzoonoticMERSDis2VecSGNSSGHSCBOWzoonoticairbornedirect_contactairbornevectorbornedirect_contactairbornezoonoticdirect_contactenvironmentalvectorbornewaterborneH7N9Figure 5: Distribution of news articles corresponding to diseases ( y-axis), worldwide, in
the HealthMap corpus. Emerging diseases, such as Ebola are associated with heightened
coverage while relatively low number of articles are present for non-endemic and non-
emerging diseases

22

ebola20.2%west_nile_virus9.0%influenza7.3%rabies5.8%malaria5.7%salmonella5.4%meningitis5.3%dengue4.5%measles4.4%tuberculosis4.1%whooping_cough3.9%norovirus3.8%cholera3.6%polio3.4%chikungunya1.9%avian_influenza1.9%listeriosis1.7%mers1.6%typhoid0.9%other5.6%