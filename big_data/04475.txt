A MODIFIED IMPLEMENTATION OF MINRES TO MONITOR
RESIDUAL SUBVECTOR NORMS FOR BLOCK SYSTEMS∗

ROLAND HERZOG† AND KIRK M. SOODHALTER‡

Abstract. Saddle-point systems, i.e., structured linear systems with symmetric matrices are
considered. A modiﬁed implementation of (preconditioned) MINRES is derived which allows to
monitor the norms of the subvectors individually. Compared to the implementation from the textbook
of [Elman, Sylvester and Wathen, Oxford University Press, 2014], our method requires one extra
vector of storage and no additional applications of the preconditioner. Numerical experiments are
included.

Key words. MINRES, saddle-point problems, structured linear systems, preconditioning, sub-

vector norms

AMS subject classiﬁcations. 65F08, 65F10, 15B57, 65M22, 74S05, 76M10

(cid:20)A BT

(cid:21)(cid:20)u
(cid:21)

(cid:21)

(cid:20)fu

1. Introduction. We are solving symmetric linear systems of the form

(1.1)
where A ∈ Rm×m and C ∈ Rp×p are symmetric, and B ∈ Rp×m, by applying MINRES
or preconditioned MINRES [7] iteration. After j iterations, we have approximation
(u(j), p(j)) and residual

B −C

fp

=

p

,

(cid:21)

(cid:20)fu

(cid:20)A BT
(cid:21)(cid:20)u(j)
u = fu −(cid:0)Au(j) + BT p(j)(cid:1)

B −C

p(j)
We denote the two parts of the residual as

r(j) =

r(j)

−

fp

(cid:21)

=

−

(cid:20)Au(j) + BT p(j)

(cid:20)fu
(cid:21)
(cid:21)
p = fp −(cid:0)Bu(j) − Cp(j)(cid:1).

Bu(j) − Cp(j)

fp

.

and r(j)

The question we seek to answer is: can we monitor the individual norms of r(j)
u and
r(j)
p (as opposed to the full norm of r(j)) using only quantities arising in an eﬃcient
implementation of the preconditioned MINRES algorithm, namely that in [2, Algo-
rithm 4.1]? The answer is yes. In section 2, we demonstrate that at the storage cost
of one additional full-length vector and six scalars but no additional applications of
the preconditioner, one can modify the preconditioned MINRES method to calculate
these norms in a progressive fashion. An extension to more than two residual parts is
straightforward. An implementation of our modiﬁed version of MINRES is given in
algorithm 1 and is available at [5] as a Matlab ﬁle.

As a motivation for our study, we mention that the individual parts of the residual
in (1.1) often have diﬀerent physical interpretations. Monitoring them individually
allows a better insight into the convergence of MINRES and it allows the formula-
tion of reﬁned stopping criteria. We present examples and numerical experiments in
section 3.

∗This version dated March 16, 2016.
†Technische Universität Chemnitz, Faculty of Mathematics, Professorship Numerical Mathemat-
ics (Partial Diﬀerential Equations), D–09107 Chemnitz, Germany (roland.herzog@mathematik.tu-
chemnitz.de, https://www.tu-chemnitz.de/herzog).
‡Industrial Mathematics Institute, Johannes Kepler University, Altenbergerstraße 69, A–4040
Linz, Austria (kirk.soodhalter@indmath.uni-linz.ac.at, https://www.indmath.uni-linz.ac.at/index.
php/members/55-soodhalter).

1

6
1
0
2

 
r
a

M
 
4
1

 
 
]

.

A
N
h
t
a
m

[
 
 

1
v
5
7
4
4
0

.

3
0
6
1
:
v
i
X
r
a

2

R. HERZOG AND K. M. SOODHALTER

2. How to monitor both parts of the preconditioned residual. In this
section, we derive how one monitors these norms without incurring much extra com-
putational or storage expense. We will describe everything in terms of the variable
names use in [2, Algorithm 4.1] with two exceptions, which will be noted below.

Here we describe quickly the derivation of preconditioned MINRES as a Krylov
subspace method, speciﬁcally in order to relate certain quantities from [2, Algo-
rithm 4.1] to the common quantities arising in the Lanczos process.
In principle,
the preconditioned MINRES algorithm is an implementation of the minimum resid-
ual Krylov subspace method for matrices which are symmetric. It can be equivalently
formulated as an iteration for operator equations in which the operator is self-adjoint,
mapping elements of a Hilbert space into its dual; see [4]. We present the derivation for
the ﬁnite dimensional case. For the purposes of this discussion, let x(j) = (u(j), p(j))
be the jth approximation. Let K =
. We further assume that the precon-
ditioner P is symmetric positive deﬁnite and has block diagonal structure

(cid:20)A BT

B 0

(cid:21)

(cid:20)Pu

P =

(cid:21)

Pp

with Pu ∈ Rm×m and Pp ∈ Rp×p.

This assumption is natural in many situations when we acknowledge that the residual
subvectors often have diﬀerent physical meaning and need to be measured in diﬀerent
norms; see section 3 for examples.
We brieﬂy review some basic facts about Krylov subspaces for symmetric matrices.
We begin with the unpreconditioned situation. Let A ∈ Rn×n be symmetric. For
some starting element h ∈ Rn, we deﬁne the jth Krylov subspace generated by A
and h to be

Kj(A, h) = span(cid:8)h,Ah,A2h, . . . ,Aj−1h(cid:9)

using the Lanczos process, where if Vj = (cid:2)v1 v2

(cid:3) ∈ Rn×j is the matrix

with columns that form an orthonormal basis of Kj(A, h), then we have the Lanczos
relation

··· vj

(2.2)

AVj = Vj+1Tj where Tj ∈ R(j+1)×j.

The matrix Tj is tridiagonal, and the matrix Tj (deﬁned as the ﬁrst j rows of Tj) is
symmetric. This implies that to generate each new Lanczos vector, we must orthog-
onalize the newest vector against only the two previous Lanczos vectors. This leads
to the following well-known three-term recurrence formula
Avj = γj+1vj+1 + δjvj + γjvj−1.
Using the naming conventions in [2, Algorithm 2.4], we have

(2.3)



δ1

γ2

 .

γ2
...
...

...
...
γj

γj
δj
γj+1

Tj =

In the case that we have a symmetric positive deﬁnite preconditioner P, we show
that a Krylov subspace can still constructed using the short-term Lanczos iteration

MONITORING OF RESIDUAL IN SADDLE-POINT PROBLEMS

3

H−1KH−T y = H−1b with y = HT x where x =

and that a MINRES method can be used for solving (1.1). In, e.g., [2], preconditioned
MINRES is derived by ﬁrst observing that P admits the Cholesky decomposition
P = HHT . Thus one could consider solving the two-sided preconditioned equations

(cid:21)
H−1r(k) =(cid:13)(cid:13)r(k)(cid:13)(cid:13)P−1 where (cid:107)·(cid:107)P−1 is the norm arising from the inner product induced
by P−1, i.e., (cid:104)·,·(cid:105)P−1 =(cid:10)P−1·,·(cid:11). One further notes that we have the equivalence

where we have the initial approximation y(0) = HT x(0). The matrix H−1KH−T
is still symmetric. However, one can notice that the preconditioned residual satisﬁes

(cid:20)u
(cid:21)

and b =

p

(cid:20)f1

f2

,

Kj(H−1KH−T , H−1r(0)) = H−1Kj(KP−1, r(0)).

If preconditioned MINRES at iteration j produce a correction

then we recover the approximation for x with x(j) = H−T(cid:0)y(0) + s(j)(cid:1). Since x(0) =

s(j) ∈ Kj(H−1KH−T , H−1r(0))

such that y(j) = y(0) + s(j),

H−T y(0) the correction space for preconditioned MINRES with respect to the orig-
inal variables is actually P−1Kj(KP−1, r(0)). From here, one can show that the
preconditioned MINRES iteration is equivalent to an iteration with the subspace
Kj(KP−1, r(0)) but with respect to the inner produced (cid:104)·,·(cid:105)P−1 induced by the pre-
conditioner. This is indeed a short-term recurrence iteration, as it can be shown that
KP−1 is symmetric with respect to (cid:104)·,·(cid:105)P−1. Thus the derivation of the precondi-
tioned MINRES method [2, Algorithm 4.1] can be presented as a small modiﬁcation
of the unpreconditioned MINRES [2, Algorithm 2.4].

This same fact was also shown in [4] but by interpreting the underlying operator
as a mapping from a Hilbert space to its dual. There are many beneﬁts to this
interpretation. One advantage is that even in the ﬁnite dimensional situation, it
allows the derivation of preconditioned MINRES without the temporary introduction
of Cholesky factors.
Let the columns of Vj still form an orthonormal basis for a Krylov subspace,
but this time the space Kj(KP−1, r(0)), and the orthonormality is with respect to
(cid:104)·,·(cid:105)P−1. Let Zj = P−1Vj have as columns the image of those vectors under the
action of the preconditioner. Then we have the preconditioned Lanczos relation,

(2.4)

KP−1Vj = KZj = VjTj

where Tj has the tridiagonal structure described earlier. What diﬀers here is that the
columns of Vj have been orthonormalized with respect to the P−1 inner product, and
the entries of the tridiagonal matrix Tj are formed from these P−1 inner products.
We have the three-term recurrence

(2.5)

Kzj = γj+1vj+1 + δjvj + γjvj−1.

The preconditioned MINRES method solves the following residual minimization prob-
lem,

(cid:13)(cid:13)γ1e1 − Tjy(cid:13)(cid:13)2 ,

x(j) = x(0) + Zjy(j) where y(j) = argmin
y∈Rj

(2.6)

where γ1 =(cid:13)(cid:13)r0(cid:13)(cid:13)P−1.

4

R. HERZOG AND K. M. SOODHALTER

Though this reduces the minimization of the residual to a small least-squares
problem, this is not the most eﬃcient way to implement the MINRES method; and
indeed, this is not what is done in, e.g., [2, Algorithm 4.1]. Due to (2.3), MINRES
can be derived such that only six full-length vectors must be stored. In the interest
of not again deriving everything, we will simply provide a few relationships between
quantities from the above explanation of MINRES and those in [2, Algorithm 4.1].
Further implementation details can be found in, e.g., [3, Section 2.5].
Let Tj = QjRj be a QR-factorization of Tj computed with Givens rotations
where Qj ∈ R(j+1)×(j+1) is a unitary matrix constructed from the product of Givens
rotations, and Rj ∈ R(j+1)×j is upper triangular. The matrix Rj ∈ Rj×j is simply
Rj with the last row (of zeros) deleted. We can write the matrix QT
j as the product of
the Givens rotation used to triangularize Tj. Since Tj is tridiagonal and Hessenberg,
we must annihilate only one subdiagonal entry per column. Following from [2, Algo-
rithm 4.1], we denote si and ci to be the Givens sine and cosine used to annihilate
the ith entry of column i − 1 using the unitary matrix Fi =
. Thus we can
denote

(cid:20) ci

−si

(cid:21)

si
ci

QT

j = G(j)

j+1G(j)

j

··· G(j)

2

j (γ1e1)(cid:9)
(cid:8)QT

i ∈ R(j+1)×(j+1) to be the matrix applying the ith Givens rotation
where we deﬁne G(j)
to rows i− 1 and i to a (j + 1)× (j + 1) matrix. Using a normal equations formulation
of (2.6), we can derive an expression for the least squares minimizer

y(j) = R−1

j

(2.7)
where {·}1:j indicates we take only the ﬁrst j rows of the argument. For the purpose
of discussion, we make an additional modiﬁcation to the variable naming using in [2,
Algorithm 4.1]. We index η, which is used to track the residual norm. On Line 4 of the
algorithm, we would have η0 = γ1, and at Line 18, we would write ηj = −sj+1ηj−1.
We have then that at iteration j, the least squares residual can be written

1:j

,

(2.8)
where ej+1 is the last column of the (j + 1)× (j + 1) identity matrix, and because Qj

is unitary, it follows that |ηj| is the norm of the residual(cid:13)(cid:13)r(j)(cid:13)(cid:13)P−1 pertaining to x(j).

γ1e1 − Tjy(j) = Qj(ηjej+1),

We now derive an expression for the norms of r(j)

p in terms of quantities
arising in the Lanczos process and residual minimization. Due to the block structure
of K, we partition Vj and Zj similarly,

u and r(j)

and Zj =

Vj =

(cid:21)

Vj,p

(cid:20)Vj,u
(cid:20)AZj,u + BT Zj,p

BZj,u − CZj,p

(cid:21)

(cid:20)Zj,u
(cid:21)
(cid:21)
(cid:20)Vj+1,u

Zj,p

Vj+1,p

=

Tj =

(cid:20)Vj+1,uTj

(cid:21)

Vj+1,pTj

.

and then insert the partitioned vectors into (2.2)

(2.9)

(cid:20)A BT

B −C

(cid:21)(cid:20)Zj,u

(cid:21)

Zj,p

=

We note that for tracking and updating the full residual vector, this partition is arti-
ﬁcially imposed. One could simply track the full residual and partition at the end to
compute the subvector norms separately. We derive everything using this block parti-
tion because one may wish to track the norm of only one subvector. Furthermore, one

MONITORING OF RESIDUAL IN SADDLE-POINT PROBLEMS

5

needs the partition quantities to deﬁne the recursions for updating the preconditioned
subvector norms. It then follows that we have Lanczos relations for the blocks
and BZj,u − CZj,p = Vj+1,pTj.

AZj,u + BT Zj,p = Vj+1,uTj

(2.10)

From (2.9) at iteration j, we have u(j) = Zj,uy(j) and p(j) = Zj,py(j), and thus using
(2.10), we can derive expressions for r(j)

u and r(j)
p

u = γ1v1,u − Vj+1,uTjy(j)
r(j)

and r(j)

p = γ1v1,p − Vj+1,pTjy(j).

From (2.8), we can write Tjy(j) = γ1e1 − Qj(ηjej+1) leading to

(2.11)

r(j)
u = ηjVj+1,uQjej+1

and r(j)

p = ηjVj+1,pQjej+1.

Thus, we simply need to ﬁnd a low-cost method of computing Vj+1,pQjej+1 at each
iteration.

Lemma 2.1. Let Mj+1 =

= Vj+1Qj where we write

(cid:20)Mj+1,u

(cid:21)

Mj+1,p

(cid:104)
(cid:104)

Mj+1,u =

Mj+1,p =

m(j+1)

1,u

m(j+1)

2,u

m(j+1)

1,p

m(j+1)

2,p

··· m(j+1)
··· m(j+1)

j,u

j,p

m(j+1)
j+1,u

m(j+1)
j+1,p

and

.

(cid:105)
(cid:105)

We have that m(j+1)
following recursion formulas hold,

i,u = m(j)

i,u and m(j+1)

i,p = m(j)

i,p for all 1 ≤ i ≤ j − 1, and the

m(j+1)
m(j+1)

j,u = cj+1m(j)
j,p = cj+1m(j)

j,u + sj+1vj+1,u

j,p + sj+1vj+1,p

(2.12)

and m(j+1)
and m(j+1)

j+1,u = −sj+1m(j)
j+1,p = −sj+1m(j)

j,u + cj+1vj+1,u,

j,p + cj+1vj+1,p,

Proof. We can prove this by induction, and it suﬃces to prove it for Mj,u as the

1,p = v1,p.

where we set m(1)

1,u = v1,u and m(1)

proofs are identical. Observe ﬁrst that Qj =(cid:0)G(j)
(cid:104)

(cid:105)(cid:20)c2 −s2

(cid:21)

(cid:104)

2

M2,u = V2,uQ1 =

m(1)

1,u v2,u

s2

c2

(cid:1)T ···(cid:0)G(j)

j+1

(cid:1)T . For j = 1, we have
(cid:105)

=

c2m(1)

1,u + s2v2,u −s2m(1)

1,u + c2v2,u

where the second equality comes from how we deﬁned m1,u. For the induction step,
we have

Mj+1,u = Vj+1,uQj = Vj+1,u

Due to the structure of G(j)

all i, we have that right multiplication by(cid:0)G(j)

i

the target. Thus, we can write

(cid:0)G(j)

(cid:1)T ···(cid:0)G(j)

2

j+1

(cid:1)T

Vj+1,u

(cid:0)G(j)

2

i

(cid:1)T ···(cid:0)G(j)

Vj,u

(cid:104)
(cid:104)

=

=

m(j)

1,u m(j)

2,u

2

j+1

(cid:1)T

(cid:1)T ···(cid:0)G(j)

(cid:0)G(j)
(cid:1)T aﬀects only columns i and i + 1 of
(cid:1)T
(cid:105)(cid:0)G(j)
(cid:1)T

(cid:105)(cid:0)G(j)

(cid:1)T

j−1,u m(j)

j,u vj+1,u

vj+1,u

j+1

j+1

.

j

··· m(j)

(Givens rotation embedded in an identity matrix), for

R. HERZOG AND K. M. SOODHALTER

6

Multiplication from the right by(cid:0)G(j)

(cid:1)T eﬀects only the last two columns. Thus the

ﬁrst j − 1 columns remain unchanged, yielding the ﬁrst result. The actual multipli-
cation yields (2.12) in the last two columns. The exact same holds for Mj,p.
From Lemma 2.1, we can drop the superindices for the ﬁrst j − 1 columns of

j+1

Mj+1,u and Mj+1,p which remain unchanged, i.e.,

Mj+1,u =

Mj+1,p =

m1,u m2,u

m1,p m2,p

··· mj−1,u m(j+1)
··· mj−1,p m(j+1)

j,u

j,p

m(j+1)
j+1,u

m(j+1)
j+1,p

and

.

(cid:104)
(cid:104)

(cid:105)
(cid:105)

Corollary 2.2. Since we have that from (2.2), the unit norm residual (up to

sign) satisﬁes

(cid:35)

(cid:34)m(j+1)

j+1,u
m(j+1)
j+1,p

,

Vj+1Qjej+1 =

it follows that r(j)
using a short-term recurrence updates using (2.12).

u = ηjVj+1,uQjej+1 and r(j)

p = ηjVj+1,pQjej+1 can be computed

Thus we have proven the following.
Theorem 2.3. If we apply a preconditioned MINRES iteration (using the imple-
mentation in [2, Algorithm 4.1]) to solve (1.1) with preconditioner P, then we have
that the following expressions for the partial residuals hold,

(2.13)

u = ηjm(j+1)
r(j)

j+1,u

and

p = ηjm(j+1)
r(j)
j+1,p,

where m(j+1)

j+1,u and m(j+1)

j+1,p can be computed recursively at each iteration using (2.12).

This requires the storage of one full-length vector, namely

, whose subvectors

are overwritten at each iteration using the recursion formula (2.12).

We can also rewrite (2.13) as progressive updating formulas, but if one uses these

progressive forms, storage of two full-length vectors is required.

Corollary 2.4. The residual subvectors r(j)

u and r(j)

p satisfy the progressive up-

dating formulas

(cid:35)

(cid:34)m(j+1)

j+1,u
m(j+1)
j+1,p

u = r(j−1)
r(j)

u

− cj+1ηj−1m(j+1)

j,u

and

p = r(j−1)
r(j)

p

Proof. As the proofs are the same, we again only prove this for r(j)

prove this by direct computation with some substitutions,

− cj+1ηj−1m(j+1)
j,p .
u . We can

(cid:17)

u = ηjm(j+1)
r(j)

j+1,u

(cid:16)−sj+1m(j)
= (−sj+1ηj−1)
j+1ηj−1m(j)
= s2
(cid:16)
j,u − c2
= ηj−1m(j)
− cj+1
= r(j−1)
− cj+1ηj−1m(j+1)
= r(j−1)
j,u .

j+1ηj−1m(j)
cj+1ηj−1m(j)

u

u

j,u + cj+1vj+1,u

j,u + cj+1sj+1ηj−1vj+1,u

j,u + cj+1sj+1ηj−1vj+1,u

j,u + sj+1ηj−1vj+1,u

(cid:17)

MONITORING OF RESIDUAL IN SADDLE-POINT PROBLEMS

7

We have shown that through recursion formulas, we can compute one or both
subvectors of the residual vector produced by preconditioned MINRES. However, we
actually want to compute the appropriate norm of each piece. The full residual
minimization is with respect to the P−1 norm (cid:107)·(cid:107)P−1. We have assumed in this paper
that the preconditioner has block diagonal structure; and as P is symmetric positive
deﬁnite, it follows that its blocks are as well. Thus we can write

(cid:13)(cid:13)r(j)(cid:13)(cid:13)2

P−1 =(cid:13)(cid:13)r(j)

u

(cid:13)(cid:13)2

−1
u

P

+(cid:13)(cid:13)r(j)

p

(cid:13)(cid:13)2

.

−1
p

P

These norms can also be computed recursively using information already on hand.
We note that doing it as in the following allows us to avoid additional applications of
the preconditioner.

In order to compute the norms separately, it is necessary that we also use the

same fact for the newest Lanczos vector,

1 = (cid:107)vj+1(cid:107)2

P−1 = (cid:107)vj+1,u(cid:107)2

P

−1
u

Let

+ (cid:107)vj+1,p(cid:107)2

−1
p

P

.

−1
u

−1
p

P

ψj+1,u = (cid:107)vj+1,u(cid:107)2
ψj+1,p = (cid:107)vj+1,p(cid:107)2

P

and ηj,p =(cid:13)(cid:13)r(j)

p

(cid:13)(cid:13)P

−1
p

ηj,u =(cid:13)(cid:13)r(j)

u

(cid:13)(cid:13)P

theorem.

−1
u

= (cid:104)zj+1,u, vj+1,u(cid:105)
= (cid:104)zj+1,p, vj+1,p(cid:105) .

and

We can store these values so they are available for later use. We further denote
. This allows us to concisely state the following

Theorem 2.5. The ratio between the full preconditioned residual norm and the
respective preconditioned residual subvector norms can be written progressively as fol-

lows:(cid:18) ηj,u
(cid:18) ηj,p

ηj

(cid:19)2
(cid:19)2

ηj

(cid:18) ηj−1,u
(cid:18) ηj−1,p

ηj−1

ηj−1

(cid:19)2 − 2sj+1cj+1
(cid:19)2 − 2sj+1cj+1

(cid:16)
(cid:16)

= s2

j+1

= s2

j+1

(cid:17)T
(cid:17)T

m(j)
j,u

m(j)
j,p

zj+1,u + c2

j+1ψj+1,u

and

zj+1,p + c2

j+1ψj+1,p.

Proof. Again the proofs for each subvector are identical, so we prove it only for
one. We can verify the identity by direct calculation and then by showing that the
quantities arising from the calculation are available by recursion. We compute

(cid:13)(cid:13)r(j)

u

(cid:13)(cid:13)2

−1
u

P

u

j+1,u

j+1,u

(cid:1)

= η2
j

P−1

(cid:1)T

j,u + cj+1vj+1,u

=(cid:0)ηjm(j+1)
(cid:1)T
(cid:0) − sj+1m(j)
(cid:16)
(cid:1)T
(cid:0)m(j)
P−1
u m(j)
(cid:32)
j+1,uP−1
η2
j−1,u
η2
j−1

(cid:0)ηjm(j+1)
(cid:17)
j,u − 2sj+1cj+1
(cid:17)T

u vj+1,u
− 2sj+1cj+1

j+1vT

P−1

m(j)
j,u

= η2
j

= η2
j

s2
j+1

s2
j+1

(cid:16)

+ c2

j,u

u

(cid:0) − sj+1m(j)
(cid:1)T
(cid:0)m(j)

j,u

j,u + cj+1vj+1,u
P−1
u vj+1,u

(cid:1)

(cid:33)

zj+1,u + c2

j+1ψj+1,u

,

8

R. HERZOG AND K. M. SOODHALTER

Basic algebra ﬁnishes the proof.
For completeness, we now present a modiﬁed version of [2, Algorithm 4.1], here
called algorithm 1. Note that we omit superscripts for m((cid:96))
i,p and some other subscripts
where they are not needed. Furthermore, we introduce the scalars θu and θp deﬁned
by

(2.14)
and the squared residual norm fractions

j,u, zj+1,u

θu =(cid:10)m(j)
(cid:18) ηj,u

(cid:11)
(cid:19)2

µu =

ηj

and θp =(cid:10)m(j)
(cid:18) ηj,p

and µp =

ηj

(cid:19)2

.

(cid:11)

j,p, zj+1,p

(2.15)

Notice that one can work with full-length vectors and the partitioning is only impor-
tant for the computation of partial inner products as in (2.14).

3. Examples and Numerical Experiments. In the introduction, we moti-
vated our study by mentioning that the residual subvectors in saddle-point systems
(1.1) often have diﬀerent physical interpretations.
In this section, we discuss sev-
eral examples to underline this fact, and we also present numerical results which
demonstrate the correctness of algorithm 1. We do this by storing all the residual
vectors throughout the iteration and evaluating the preconditioned subvector norms
a-posteriori. This is a debugging step which is introduced to verify the correctness of
algorithm 1. Our Matlab implementation of algorithm 1 is available at [5].

In all examples, we begin with an all-zero initial guess and we stop when the

relative reduction of the total residual

ηj
η0

=

(cid:13)(cid:13)r(j)(cid:13)(cid:13)P−1
(cid:13)(cid:13)r(0)(cid:13)(cid:13)P−1
and |ηj,p| =(cid:13)(cid:13)r(j)

p

(cid:13)(cid:13)P

≤ εp

−1
p

|ηj,u| =(cid:13)(cid:13)r(j)

u

(cid:13)(cid:13)P

≤ εu

−1
u

falls below 10−6. We could easily utilize a reﬁned stopping criterion such as

to take advantage of the ability to monitor the residual subvector norms. Concrete
applications of this (e.g., in optimization) are outside the scope of this paper and will
be addressed elsewhere.

Example 3.1 (Least-Norm Solution of Underdetermined Linear System).

We consider an underdetermined and consistent linear system Bu = b with a matrix
B ∈ Rm×n of full row rank and m < n. Its least-norm solution

Minimize

1
2

(cid:107)u(cid:107)2

H such that Bu = b, u ∈ Rn

in the sense of the inner product deﬁned by the symmetric positive deﬁnite matrix
H ∈ Rn×n, is uniquely determined by the saddle-point system

(cid:20)H BT

(cid:21)(cid:20)u
(cid:21)

B 0

p

(cid:21)

(cid:20)0

b

.

=

Notice that the ﬁrst block of equations represents optimality, while the second block
represents feasibility of a candidate solution (u, p)T .
Our test case uses data created through the commands

MONITORING OF RESIDUAL IN SADDLE-POINT PROBLEMS

9

u

p

−1
u

−1
p

(cid:13)(cid:13)P

and |ηj,p| =(cid:13)(cid:13)r(j)

P = blkdiag(Pu, Pp) ∈ R(m+p)×(m+p), symmetric positive deﬁnite.

Algorithm 1: The preconditioned MINRES method with monitoring of |ηj,u| =

Input : K ∈ R(m+p)×(m+p) deﬁned as in (1.1),

(cid:13)(cid:13)r(j)
(cid:13)(cid:13)P
Output: x(j) for some j such that(cid:13)(cid:13)r(j)(cid:13)(cid:13)P−1 satisﬁes some convergence
3 γ1 =(cid:112)(cid:104)z1, v1(cid:105),v1 ← v1/γ1, z1 ← z1/γ1

1 Set v0 = 0, w0 = w1 = 0
2 Choose x(0), set v1 =

− K x(0), z1 = P−1v1

(cid:20)f1

criteria

(cid:21)

f2

√

(cid:113)

ψu, η0,p = γ1

4 ψu = (cid:104)z1,u, v1,u(cid:105), ψp = (cid:104)z1,p, v1,p(cid:105)
5 µu = ψu, µp = ψp
6 m1 = v1
7 Set η0 = γ1, η0,u = γ1
8 for j = 1 until convergence do
9
10
11
12
13
14
15

γj+1 =(cid:112)(cid:104)zj+1, vj+1(cid:105)

δj = (cid:104)Kzj, zj(cid:105)
vj+1 = Kzj − δjvj − γjvj−1 ;
zj+1 = P−1vj+1
vj+1 ← vj+1/γj+1
zj+1 ← zj+1/γj+1
α0 = cjδj − cj−1sjγj ;
α1 =
α2 = sjδj + cj−1cjγj
α3 = sj−1γj
cj+1 = α0/α1; sj+1 = γj+1/α1 ;
θu ← (cid:104)mj,u, zj+1,u(cid:105), θp ← (cid:104)mj,p, zj+1,p(cid:105)
ψu ← (cid:104)zj+1,u, vj+1,u(cid:105), ψp ← (cid:104)zj+1,p, vj+1,p(cid:105)
mj+1 = −sj+1mj + cj+1vj+1
wj+1 = (zj − α3wj−1 − α2wj)/α1
x(j) = x(j−1) + cj+1ηj−1wj+1
µu ← s2
j+1µu − 2sj+1cj+1θu + c2
j+1µp − 2sj+1cj+1θp + c2
µp ← s2
ηj = −sj+1ηj−1;
√
ηj,u = ηj
Test for convergence

16

17
18
19
20
21
22
23
24
25
26
27
28
29

µu, ηj,p = ηj

j+1ψu
j+1ψp

α2
0 + γ2

µp;

√

j+1

(cid:112)ψp, s0 = s1 = 0, c0 = c1 = 1

// Lanczos

// Update QR factorization

// Givens rotations

// total residual norm
// partial residual norms

rng (42); n =100; m =30; B= randn (m ,n ); b= randn (m ,1);
H= spdiags ( rand (n ,1) ,0 ,n ,n );

As preconditioner we use either P1 = blkdiag(In×n, Im×m) (the unpreconditioned
case) or P2 = blkdiag(H, Im×m). The convergence histories in Figure 3.1 show that
the amount by which the two residual subvectors contribute to their combined norm
may indeed be quite diﬀerent, and it depends on the preconditioner. In this example,
the feasibility residual rp = b−Bu is signiﬁcantly smaller than the optimality residual
ru = −Hu − BT p in the unpreconditioned case where we used P1. To be more

10

R. HERZOG AND K. M. SOODHALTER

precise, the average value of µp throughout the iteration history is about 21%. Quite
the opposite is true for the case of P2, when µp is close to 100%.

Fig. 3.1. Convergence history of the residual subvectors for Example 3.1 (underdetermined
linear system) in the unpreconditioned (left) and preconditioned case (right). The plot, as all fol-
lowing convergence plots, also conﬁrms the correctness of algorithm 1 and of our implementation.
Notice that in the right ﬁgure, the ﬁrst residual norm (cid:107)ru(cid:107)
is partially outside of the plot range.
This results from our choice to maintain the same scales for both ﬁgures.

−1
u

P

Example 3.2 (Least-Squares Solution of Overdetermined Linear System).

Here we consider an overdetermined linear system BT p = b with a matrix B ∈ Rm×n
of full row rank and m < n. Its least-squares solution

(cid:13)(cid:13)BT p − b(cid:13)(cid:13)2

H−1 , p ∈ Rm

Minimize

1
2

with H ∈ Rn×n symmetric positive deﬁnite as above, is uniquely determined by the
normal equations, B H−1(BT p−b) = 0. By deﬁning u as the ’preconditioned residual’
H−1(BT p − b), we ﬁnd that the least-squares solution is in turn equivalent to the
saddle-point system

(cid:20)H BT

(cid:21)(cid:20)u
(cid:21)

B 0

p

(cid:20)b
(cid:21)

0

=

.

The ﬁrst block of equations now represents feasibility for the constraint deﬁning the
auxiliary quantity u, while the second requires u to lie in the kernel of B.
Similarly as above, we derive test data through

rng (42); n =100; m =30; B= randn (m ,n ); b= randn (n ,1);
H= spdiags ( rand (n ,1) ,0 ,n ,n );

We employ the same preconditioners P1 and P2 as in Example 3.1. Once again, the
convergence behavior of the two residual subvectors is fundamentally diﬀerent (Fig-
ure 3.2) for the two cases: in the unpreconditioned case, the average value of µp is
about 9%, while it is 72% in the preconditioned case.

Our remaining examples involve partial diﬀerential equations and they will be
stated in variational form, by specifying bilinear forms a(·,·) : V × V → R, b(·,·) :
V × Q → R, and, where appropriate, c(·,·) : Q × Q → R. Here V and Q are (real)
Hilbert spaces. The matrices A, B and C in (1.1) are then obtained by evaluating the

01020304050607080iter10-710-610-510-410-310-210-1100underdetermined unpreconditioned (m=100, p=30)|r||ru||rp||ru| predicted|rp| predicted01020304050607080iter10-710-610-510-410-310-210-1100underdetermined preconditioned (m=100, p=30)|r||ru||rp||ru| predicted|rp| predictedMONITORING OF RESIDUAL IN SADDLE-POINT PROBLEMS

11

Fig. 3.2. Convergence history of the residual subvectors for Example 3.2 (overdetermined

linear system) in the unpreconditioned (left) and preconditioned case (right).

respective bilinear forms on a basis of an appropriate ﬁnite dimensional subspace Vh or
Qh, e.g., [B]ij = b(ϕj, ψi) for basis elements ϕj ∈ Vh and ψi ∈ Qh. Similarly, the right
hand side vectors fu and fp are obtained from evaluating problem dependent linear
forms fu(·) and fp(·) on the same basis functions ϕi and ψi, respectively. Finally,
the matrices and vectors obtained in this way may need to be updated due to the
incorporation of essential (Dirichlet) boundary conditions.

All numerical tests were conducted using the Python interface of the ﬁnite element
library FEniCS [6] (version 1.6) to generate the matrices and vectors. Those were
then exported in PETSc binary format and read from Matlab through the helper
functions readPetscBinMat.m and readPetscBinVec.m provided by Samar Khati-
wala on his web page. We deliberately turned oﬀ the reordering feature of FEniCS
for the degrees of freedom to preserve the block structure of (1.1) for illustration pur-
poses (see Figure 3.6) and in order for the subvectors u and p and the residuals ru
and rp to remain contiguous in memory. However, our theory does not rely on a par-
ticular ordering of the subvector components, and our implementation of algorithm 1
allows for arbitrary component ordering.

It will turn out to be useful for the following examples to specify the physical
units (Newton: N, meters: m, seconds: s, Watt: W, and Kelvin: K) for all involved
quantities in the following examples.

Example 3.3 (Stokes Channel Flow).

We consider the variational formulation of a 3D stationary Stokes channel ﬂow con-
ﬁguration within the domain Ω = (0, 10) × (0, 1) × (0, 1). Dirichlet conditions are
imposed on the ﬂuid velocity u everywhere except at the ’right’ (outﬂow) boundary
(x = 10), where do-nothing conditions hold. The Dirichlet conditions are homoge-
neous (no-slip) except at the ’left’ (inﬂow) boundary (x = 0), where u(x, y, z) =
uin(x, y, z) = (y (1 − y) z (1 − z), 0, 0)T m s−1 is imposed.
Appropriate function spaces for this setup are V = {v ∈ H 1(Ω; R3) : v = 0 on Γ\
Γright} for the velocity and Q = L2(Ω) for the pressure. The relevant bilinear and
linear forms associated with this problem are

a(u, v) = µ

∇u : ∇v dx,

b(u, q) =

q div u dx and

fu(v) =

f · v dx.

Ω

Ω

Ω

We use the dynamic viscosity parameter µ = 1 × 10−3 N s m−2 (water) and zero right

(cid:90)

(cid:90)

(cid:90)

01020304050607080iter10-710-610-510-410-310-210-1100101overdetermined unpreconditioned (m=100, p=30)|r||ru||rp||ru| predicted|rp| predicted01020304050607080iter10-710-610-510-410-310-210-1100101overdetermined preconditioned (m=100, p=30)|r||ru||rp||ru| predicted|rp| predictedR. HERZOG AND K. M. SOODHALTER

12
hand side force f = (0, 0, 0)T N m−3.
By considering units, or by investigating the underlying physics, we infer that the
ﬁrst component of the residual, ru = fu − a(u,·) − b(·, p), represents a net sum of
forces, measured in N. Similarly, the second residual rp = −b(u,·) represents the net
ﬂux of ﬂuid through the impermeable channel walls, measured in m3 s−1. Clearly, both
parts of the residual must be zero at the converged solution, but their departure from
zero at intermediate iterates has diﬀerent physical interpretations.

As preconditioner P = blkdiag(Pu, Pp), we use the block diagonal matrix induced

by the bilinear forms

(cid:90)

a(u, v)

and µ−1

p q dx,

Ω

respectively, similar to [2, Section 4.2], where the constant-free problem is considered.
Notice that the inclusion of the constant µ−1 into the pressure mass matrix renders
the preconditioner compatible with the physical units of the problem.

For our numerical test, we discretized the problem using the Taylor-Hood ﬁnite
element. The homogeneous and non-homogeneous Dirichlet boundary conditions were
included by modifying the saddle-point components A, B, BT , and right hand side
fu in (1.1) in a symmetric way through the assemble_system call in FEniCS. No-
tice that this eﬀectively modiﬁes some components of the ﬁrst residual subvector ru
by expressions of the form uin(x, y, z) − u(x, y, z), measured in m s−1. The same
modiﬁcations apply to the preconditioner P.

Figure 3.3 displays the convergence behavior of the residual subvector norms on a
relatively coarse mesh and its reﬁnement. The result illustrates the mesh independence
of the preconditioned iteration, and it also shows that the residual rp (representing
mass conservation) lags behind the residual ru over the majority of the iterations.

Fig. 3.3. Convergence history of the residual subvectors for Example 3.3 (Stokes) on a coarse

grid (left) and its uniform reﬁnement (right).

Example 3.4 (Linear Elasticity with Nearly Incompressible Material).

This example describes a tensile test with a rod of square cross section, which occupies
the domain Ω = (0, 100) × (0, 10) × (0, 10). We use mm here in place of m as our
length unit. Homogeneous Dirichlet conditions for the displacement u are imposed at
the ’left’ (clamping) boundary (x = 0), while natural (traction) boundary conditions
are imposed elsewhere. The imposed traction pressure is zero except at the ’right’
(forcing) boundary (x = 100), where a uniform pressure of g = (1, 0, 0)T N mm−2 is

01020304050607080iter10-710-610-510-410-310-210-1100Stokes3D level 1 (m=19683, p=1025)|r||ru||rp||ru| predicted|rp| predicted01020304050607080iter10-710-610-510-410-310-210-1100Stokes3D level 2 (m=139587, p=6561)|r||ru||rp||ru| predicted|rp| predictedMONITORING OF RESIDUAL IN SADDLE-POINT PROBLEMS

13

imposed. As is customary for nearly incompressible material, we introduce an extra
variable p for the hydrostatic pressure (see for instance [9]) in order to overcome the
ill-conditioning of a purely displacement based formulation known as locking [1]. We
employ the standard isotropic stress-strain relation, σ = 2 µ ε(u) + p I and div u =
λ−1p. Here ε(u) = (∇u + ∇uT )/2 denotes the symmetrized Jacobian of u, while µ
and λ denote the Lamé constants. We choose as material parameters Young’s modulus
E = 50 N mm−2 and a Poisson ratio of ν = 0.49. These particular values describe
a material like nearly incompressible rubber, and a conversion to the Lamé constants
yields µ = E
The variational mixed formulation obtained in this way is described by the spaces
V = {v ∈ H 1(Ω; R3) : v = 0 on Γleft} for the displacement and Q = L2(Ω) for the
hydrostatic pressure. The bilinear and linear forms associated with this problem are

2 (1+ν) = 16.78 N mm−2 and λ =

(1+ν)(1−2 ν) = 822.15 N mm−2.

νE

(cid:90)
(cid:90)

Ω

a(u, v) = 2 µ

c(p, q) = λ−1

(cid:90)

(cid:90)

ε(u) : ε(v) dx,

b(u, q) =

q div u dx,

p q dx and

fu(v) =

Ω

g · v dx.

Ω

Γright

At the converged solution, the body is in equilibrium. At an intermediate iterate,
we can interpret ru = fu − a(u,·)− b(·, p) as a net force acting on the body, measured
in N. This force is attributed to a violation of the equilibrium conditions div σ = −f
in Ω, and σ n = 0 or σ n = g at the non-clamping boundary parts at an intermediate
iterate (u, p)T . The second residual rp = −b(u,·) + c(p,·) admits an interpretation
of a volume measured in m3 due to a violation of div u = λ−1p.

As preconditioner P = blkdiag(Pu, Pp), we use the block diagonal matrix induced
by the bilinear forms a(u, v) and c(p, q), respectively. Once again, we remark that this
choice is compatible with the physical units of the problem.

Similarly as for Example 3.3, we discretize the problem using the Taylor-Hood
ﬁnite element, and similar modiﬁcations due to Dirichlet displacement boundary con-
ditions apply. Figure 3.4 illustrates the convergence behavior on successively reﬁned
meshes, revealing once again diﬀerent orders of magnitude for the two components of
the residual.

Fig. 3.4. Convergence history of the residual subvectors for Example 3.4 (elasticity) on a

coarse grid (left) and its uniform reﬁnement (right).

Example 3.5 (Optimal Boundary Control).

051015202530iter10-710-610-510-410-310-210-1100101Elasticity3Dpressure level 1 (m=6075, p=369)|r||ru||rp||ru| predicted|rp| predicted051015202530iter10-710-610-510-410-310-210-1100101Elasticity3Dpressure level 2 (m=39123, p=2025)|r||ru||rp||ru| predicted|rp| predicted(cid:90)
(cid:90)

Ω

(cid:90)

(cid:90)

(cid:90)

14

R. HERZOG AND K. M. SOODHALTER

Our ﬁnal example is an optimal boundary control problem for the stationary heat
equation on the unit cube Ω = (0, 1) × (0, 1) × (0, 1) with boundary Γ. The problem
statement is

Minimize α1
2

s.t.

L2(Ω) +

(cid:107)u − ud(cid:107)2

−κ(cid:52)u + δ u = 0

= f

κ

∂u
∂n

(cid:107)f(cid:107)2
α2
2
in Ω,

L2(Γ)

on Γ.

As data we use α1 = 1 m−3 K−2, ud(x, y, z) = x K, α2 = 1 × 10−2 m2 W−2, heat
conduction coeﬃcient κ = 1 W m−1 K−1, and radiation coeﬃcient δ = 1 W m−3 K−1.
The necessary and suﬃcient optimality conditions for this problem are standard;
see, for instance, [8, Chapter 2.8]. When we eliminate the control function f from
the problem, a saddle-point system for the state u and adjoint state p remains, which
is described by the following data:

(cid:90)

a(u, v) = α1

u v dx,

b(u, q) = κ

∇u · ∇q dx + δ

u q dx,

c(p, q) = α−1

2

Ω

p q dx and

fu(v) = α1

Ω

ud v dx.

Γ

Ω

(cid:90)

The underlying spaces are V = Q = H 1(Ω), and we discretize both using piecewise
linear, continuous ﬁnite elements. Note that the elements u, v ∈ V are measured in
K while the elements p, q ∈ Q are measured in W−1.

As preconditioner P = blkdiag(Pu, Pp), we use the block diagonal matrix induced

by the bilinear forms

pu(u, v) = α1κ δ−1
pp(p, q) = α−1

2 (κ δ−1)1/2

Ω

(cid:90)

Ω

∇u · ∇v dx + α1

u v dx

Ω

∇p · ∇q dx + α−1

2 (κ−1 δ)1/2

(cid:90)

Ω

p q dx,

respectively. As in our previous examples, this choice is compatible with the physical
units of the problem. Figure 3.5 illustrates the convergence behavior on successively re-
ﬁned meshes. Besides the mesh independence, we observe that both residual subvector
norms converge in unison in this example.

Fig. 3.5. Convergence history of the residual subvectors for Example 3.5 (optimal boundary

control) on a coarse grid (left) and its uniform reﬁnements (middle and right).

0510152025303540iter10-710-610-510-410-310-210-1OptimalControl3D level 2 (m=729, p=729)|r||ru||rp||ru| predicted|rp| predicted0510152025303540iter10-710-610-510-410-310-210-1OptimalControl3D level 3 (m=4913, p=4913)|r||ru||rp||ru| predicted|rp| predicted0510152025303540iter10-710-610-510-410-310-210-1OptimalControl3D level 4 (m=35937, p=35937)|r||ru||rp||ru| predicted|rp| predictedMONITORING OF RESIDUAL IN SADDLE-POINT PROBLEMS

15

Fig. 3.6. Sparsity plots of the saddle-point systems arising in Example 3.1 (underdetermined
linear system), Example 3.3 (Stokes), Example 3.4 (elasticity), and Example 3.5 (optimal boundary
control).

u

−1
u

p

−1
p

(cid:13)(cid:13)P

4. Discussion. In this paper we developed a modiﬁed implementation of MIN-
RES. When applied to saddle-point systems, the new implementation allows us to
individually, while con-

monitor the norms of the subvectors(cid:13)(cid:13)r(j)
ventional implementations keep track of only the total residual norm (cid:13)(cid:13)r(j)(cid:13)(cid:13)P−1. It

and(cid:13)(cid:13)r(j)

(cid:13)(cid:13)P

should be obvious how algorithm 1 generalizes to systems with more than two residual
subvectors and block-diagonal preconditioners structured accordingly. The price to
pay to monitor the subvector norms is the storage of one additional vector compared
to the implementation of MINRES given in [2, Algorithm 4.1]. While we developed the
details in ﬁnite dimensions using matrices and vectors, our approach directly transfers
to linear saddle-point systems in a Hilbert space setting using linear operators and
linear forms, as described in [4].

Being able to diﬀerentiate between the contributions to the total residual oﬀers
new opportunities for the design of iterative algorithms for nonlinear problems, which
require inexact solves of (1.1) as their main ingredient. We envision for instance
solvers for equality constrained nonlinear optimization problems which may now assign
individual stopping criteria for the residuals representing optimality and feasibility,
respectively. The design of such an algorithm is, however, beyond the scope of this
work. Similarly, for the Stokes Example 3.3, the user may now assign individual
stopping criteria for the fulﬁllment of the balance of forces (ﬁrst residual) and the
conservation of mass (second residual).

A topic of future research could be to study the decay properties of the subvector
norms, rather than study the decay of the total residual norm, see for instance [2,
Section 4.2.4]. We expect that such an analysis will be more involved but it may shed

16

R. HERZOG AND K. M. SOODHALTER

light on how the relative scaling of the preconditioners blocks aﬀects the convergence
of the residual subvectors.

REFERENCES

[1] I. Babuška and M. Suri, Locking eﬀects in the ﬁnite element approximation of elasticity

problems, Numerische Mathematik, 62 (1992), pp. 439–463, doi:10.1007/BF01396238.

[2] H. C. Elman, D. J. Silvester, and A. J. Wathen, Finite elements and fast iterative solvers:
with applications in incompressible ﬂuid dynamics, Numerical Mathematics and Scientiﬁc
Computation, Oxford University Press, Oxford, second ed., 2014, doi:10.1093/acprof:oso/
9780199678792.001.0001.

[3] A. Greenbaum, Iterative Methods for Solving Linear Systems, SIAM, Philadelphia, 1997.
[4] A. Günnel, R. Herzog, and E. Sachs, A note on preconditioners and scalar products in
Krylov subspace methods for self-adjoint problems in Hilbert space, Electronic Transactions
on Numerical Analysis, 41 (2014), pp. 13–20.

[5] R. Herzog and K. Soodhalter, SUBMINRES. A modiﬁed implementation of MINRES to

monitor residual subvector norms for block systems, 2016, doi:10.5281/zenodo.47393.

[6] A. Logg, K.-A. Mardal, G. N. Wells, et al., Automated Solution of Diﬀerential Equations

by the Finite Element Method, Springer, 2012, doi:10.1007/978-3-642-23099-8.

[7] C. C. Paige and M. A. Saunders, Solutions of sparse indeﬁnite systems of linear equations,

SIAM Journal on Numerical Analysis, 12 (1975), pp. 617–629.

[8] F. Tröltzsch, Optimal Control of Partial Diﬀerential Equations, vol. 112 of Graduate Studies
in Mathematics, American Mathematical Society, Providence, 2010, doi:10.1090/gsm/112.
[9] C. Wieners, Robust multigrid methods for nearly incompressible elasticity, Computing.
Archives for Scientiﬁc Computing, 64 (2000), pp. 289–306, doi:10.1007/s006070070026. In-
ternational GAMM-Workshop on Multigrid Methods (Bonn, 1998).

