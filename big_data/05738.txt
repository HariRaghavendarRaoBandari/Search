Noname manuscript No.
(will be inserted by the editor)

On the non-ergodic convergence rate of an inexact
augmented Lagrangian framework for composite
convex programming

Ya-Feng Liu · Xin Liu · Shiqian Ma

Received: date / Accepted: date

Abstract In this paper, we consider the linearly constrained composite convex
optimization problem, whose objective is a sum of a smooth function and a pos-
sibly nonsmooth function. We propose an inexact augmented Lagrangian (IAL)
framework for solving the problem. The proposed IAL framework requires solving
the augmented Lagrangian (AL) subproblem at each iteration less accurately than
most of the existing IAL frameworks/methods. We analyze the global convergence
and the non-ergodic convergence rate of the proposed IAL framework.

Keywords Inexact augmented Lagrangian framework · Non-ergodic convergence
rate

Mathematics Subject Classiﬁcation (2000) 90C25 · 65K05

1 Introduction

In this paper, we consider the linearly constrained composite convex optimization
problem

min
x∈Rn

F (x) := f (x) + g(x)

s.t. Ax = b,

(1.1)

where A ∈ Rm×n and b ∈ Rm; f (x) is a convex smooth function with Lipschitz
continuous gradient; and g(x) is a closed convex (not necessarily smooth) function

Y.-F. Liu and X. Liu
LSEC, ICMSEC, Academy of Mathematics and Systems Science,
Chinese Academy of Sciences, Beijing 100190, China
E-mail: yaﬂiu@lsec.cc.ac.cn; liuxin@lsec.cc.ac.cn

S. Ma
Department of Systems Engineering and Engineering Management,
The Chinese University of Hong Kong, Shatin, New Territories, Hong Kong, China
E-mail: sqma@se.cuhk.edu.hk

6
1
0
2

 
r
a

 

M
8
1
 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
8
3
7
5
0

.

3
0
6
1
:
v
i
X
r
a

2

Ya-Feng Liu et al.

with a bounded domain1. An important example of problem (1.1) is that g(x) is
an indicator function of a convex compact set X , that is,

g(x) = IndX (x) :=


0, if x ∈ X ,

+∞, otherwise.

In this case, problem (1.1) can be rewritten as

min
x∈X

f (x), s.t. Ax = b.

One eﬃcient approach to solving problem (1.1) is the AL method [6, 19, 20].

The AL function of problem (1.1) is

Lβ(x; λ) := ˆfβ(x; λ) + g(x),

(1.2)

where

ˆfβ(x; λ) := f (x) + hλ, Ax − bi +

β
2 kAx − bk2,

λ ∈ Rm is the Lagrange multiplier associated with the linear constraint, and β > 0
is the penalty parameter. The (augmented) Lagrangian dual of problem (1.1) is

with

max
λ∈Rm

d(λ)

d(λ) := min

x∈Rn Lβ(x; λ).

(1.3)

(1.4)

It is well-known that the dual function d(λ) in (1.4) is diﬀerentiable and its gradient
is given by ∇d(λ) = Ax(λ)−b, where x(λ) is the solution of problem (1.4) (see [1]).
Given λk, the AL method for solving problem (1.1) updates the primal and
dual variables via

(1.5)

and

x(λk) = arg min

x∈Rn Lβ(x; λk)
λk+1 = λk + β(cid:16)Ax(λk) − b(cid:17) ,

λk+1 = λk + β∇d(λk).

respectively. The AL method for solving problem (1.1) is essentially a dual gradient
ascent method, which updates the dual variable by performing a dual gradient
ascent step

When the problem dimension n is large, ﬁnding an exact solution of AL
subproblem (1.5) can be computationally expensive and thus the exact gradient
∇d(λk) is often unavailable. As a result, many works focused on inexact versions
1 The bounded domain assumption on g(x) is made in this paper mainly for ease of presen-
tation. For problem (1.1) arising from many applications of interest such as machine learning,
statistics, and signal processing, we often can easily ﬁnd a bounded set X such that the solution
of problem (1.1) lies in X . Therefore, we can restrict the deﬁnition of g(x) over this bounded
set. Let us take the compressed sensing problem, which is a special case of problem (1.1) with
f (x) = 0 and g(x) = kxk1, as an example. We can restrict of the deﬁnition of kxk1 over the
bounded domain {x | kxk1 ≤ kˆxk1} , where ˆx is any point satisfying Aˆx = b.

Non-ergodic convergence rate of an IAL framework for convex programming

3

of (dual) gradient methods; see [2–4, 10–14, 20–22] and references therein. Rock-
afellar [20] proposed an IAL framework, where the AL subproblem is solved until
a point xk+1 is found such that

Lβ(cid:16)xk+1; λk(cid:17) − Lβ(cid:16)x(λk); λk(cid:17) ≤ η2

k,

(1.6)

and showed that the proposed IAL framework converges if the nonnegative toler-
ance sequence {ηk} is summable. Very recently, Devolder, Glineur, and Nesterov [3]
proposed a general inexact gradient framework and analyzed the ergodic conver-
gence rate of their framework when it is applied to solve dual problem (1.3).
In [14], Nedelcu, Necoara, and Tran-Dinh proposed an IAL method, where the
AL subproblem was approximately solved by Nesterov’s gradient method [15–17]
such that (1.6) is satisﬁed and showed again the ergodic convergence rate of the
proposed IAL method. The non-ergodic convergence rate result for the IAL frame-
work/method has been missing in the literature for a long time until in a very re-
cent work by Lan and Monterio [10], where they proposed an IAL method (where
the AL subproblems are approximately solved by Nesterov’s gradient method) and
analyzed the non-ergodic convergence rate for the proposed method.

In this paper, we propose a new IAL framework (see Algorithm 1) for solving
problem (1.1), where the AL subproblem is approximately solved until a point
xk+1 is found such that

max

x∈RnnD∇ ˆfβ(xk+1; λk), xk+1 − xE + g(xk+1) − g(x)o ≤ ηk.

(1.7)

Here ∇ ˆfβ(x; λ) is the gradient of ˆfβ(x; λ) with respect to x. The termination
condition (1.7) in our proposed IAL framework is weaker and (potentially) easier
to check2 than (1.6) in most of the existing IAL frameworks/methods. We establish
the global convergence of the proposed IAL framework under the assumption that
the sequence {ηk} in (1.7) is summable; see Theorem 2.1. Moreover, we show, in
Theorems 2.2 and 2.3, the non-ergodic convergence rate (under weaker conditions
than that in [10]) for the proposed IAL framework, which reveals how the error in
solving the AL subprblem aﬀects the convergence rate.

2 The IAL framework and non-ergodic convergence rate analysis

In this section, we present the IAL framework for solving problem (1.1) and analyze
its global convergence and non-ergodic convergence rate.

2.1 The IAL framework

The proposed IAL framework is given in Algorithm 1. At the k-th iteration, the
IAL framework ﬁrst solves AL subproblem (2.1) with ﬁxed dual variable λk in an
inexact manner until a point xk+1 satisfying (1.7) is found; then updates the dual
variable by performing an inexact gradient ascent step (2.2).

2 To check whether xk+1 satisﬁes (1.7) or not, we only need to solve the convex optimization
problem on the left-hand side of (1.7), which can be solved exactly or to a high precision in
time (essentially) linear in the size of the input for many g(x); see [7]. In contrast, it is generally
hard to check whether xk+1 satisﬁes (1.6) or not (because x(λk) is unknown).

4

Ya-Feng Liu et al.

Algorithm 1: The IAL framework for problem (1.1)

1 Initialize x1 ∈ X , λ1 ∈ Rm, and the nonnegative sequence {ηk} .
2 for k ≥ 1: do

3

Find an approximate solution xk+1 of the AL subproblem

x∈Rn nLβ(x; λk) := ˆfβ (x; λk) + g(x)o
min

such that (1.7) is satisﬁed;
Update the dual variable via

4

λk+1 = λk + β (cid:16)Axk+1 − b(cid:17) .

(2.1)

(2.2)

Two remarks on the proposed IAL framework are in order.
First, AL subproblem (2.1) can be eﬃciently solved in an inexact manner by
various (ﬁrst-order) methods such as Nesterov’s gradient methods [15–17] and the
Frank-Wolfe (a.k.a. conditional gradient) methods [5, 8, 9, 18]. Speciﬁcally, Nes-

iterations. Since (1.6) implies (1.7) [14, Lemma 2.3], it follows that Nesterov’s gra-

ηk(cid:17)
terov’s gradient methods can ﬁnd the point xk+1 satisfying (1.6) within O(cid:16) 1
ηk(cid:17) iterations.
dient methods can ﬁnd the point xk+1 satisfying (1.7) within O(cid:16) 1
ηk(cid:17) iterations. Compared to Nesterov’s gradient methods, the Frank-Wolfe
O(cid:16) 1

The Frank-Wolfe methods can also ﬁnd the point xk+1 satisfying (1.7) within

methods converge slower in practice, while the computational cost per iteration in
the Frank-Wolfe methods is generally cheaper.

Second, the smaller the tolerance ηk is, the more computational cost is needed
in Algorithm 1 to ﬁnd the point xk+1 satisfying (1.7). On the other hand, the
larger the tolerance ηk is, the larger the approximation error between the approx-
imate gradient Axk+1 − b and the true gradient ∇d(λk) (see Lemma 2.6 further
ahead) which might lead to slow convergence or even divergence of the proposed
Algorithm 1. Therefore, the choice of {ηk} is important in balancing the computa-
tional cost (of ﬁnding the point xk+1 satisfying (1.7)) and the global convergence
and convergence rate (of the framework). We will discuss the possible choices of
{ηk} in the next subsection.

2.2 Global convergence and non-ergodic convergence rate

In this subsection, we present global convergence and non-ergodic convergence rate
results of our IAL framework (Algorithm 1), which are regardless of the meth-
ods used to ﬁnd the point xk+1 satisfying (1.7). More speciﬁcally, Theorem 2.1
shows the global convergence of the IAL framework under the assumption that
the nonnegative sequence {ηk} is summable; and Theorems 2.2 and 2.3 show the
non-ergodic convergence rate of the IAL framework.

Non-ergodic convergence rate of an IAL framework for convex programming

5

Theorem 2.1 Let (cid:8)xk(cid:9) and (cid:8)λk(cid:9) be generated by Algorithm 1. Suppose the non-
negative sequence {ηk} satisﬁes

+∞

Xk=1

ηk < +∞.

(2.3)

Then,

where λ∗ is an optimal solution to problem (1.3) and d(λ) is deﬁned in (1.4).

δk := d(cid:0)λ∗(cid:1) − d(λk) → 0 and (cid:13)(cid:13)(cid:13)

Axk+1 − b(cid:13)(cid:13)(cid:13) → 0,

Theorem 2.1 shows the global convergence of Algorithm 1 under conditions
(1.7) and (2.3). Classical conditions in [20] that guarantee the global convergence
of the IAL framework are (1.6) and (2.3). Since (1.6) implies (1.7) by [14, Lemma
2.3], our conditions (1.7) and (2.3) are weaker than conditions (1.6) and (2.3)
in [20].

Before presenting the non-ergodic convergence rate of Algorithm 1, we ﬁrst

deﬁne some notation. Let

θ :=

β

4B2 and B :=vuutkλ1 − λ∗k2 + 2β

+∞

Xk=1

ηk < +∞.

(2.4)

Since δk → 0 (cf. Theorem 2.1) and ηk → 0, there exists k0 ≥ 4 such that

Also, we deﬁne

It is easy to verify

max
k≥k0 {δk} ≤

1
2θ

and max

k≥k0 {ηk} ≤

1

24θ

.

τ1 :=

k0
4θ

and τ2 :=

1

4θ√ηk0

.

τ1 ≥

1
θ

and τ2 ≥r 3

2θ

.

(2.5)

(2.6)

(2.7)

Theorem 2.2 Let (cid:8)λk(cid:9) be generated by Algorithm 1. Suppose that the positive
sequence {ηk} is nonincreasing and satisﬁes (2.3) and

r ηk+1
ηk ≥

k − 2

k

, k = k0, k0 + 1, . . . ,

(2.8)

where k0 satisﬁes (2.5). Then,

δk ≤

τ1
k

+ τ2√ηk, k = k0, k0 + 1, . . . ,

(2.9)

where τ1 and τ2 are deﬁned in (2.6).

6

Ya-Feng Liu et al.

i.e., τ1

As shown in (2.9), the rate that {δk} converges to zero depends on two terms,
k and τ2√ηk, and the rate is determined by the slower one of them: if k√ηk →
0, then δk → 0 with a rate of O (1/k) ; otherwise, δk → 0 with a rate of O(cid:0)√ηk(cid:1) .
In particular, if ηk = 0 for all k ≥ 1, then Algorithm 1 reduces to the exact dual
gradient ascent method, and achieves the O (1/k) convergence rate.
These facts indicate that the sequence {ηk} in Algorithm 1 should not be
chosen such that(cid:8)√ηk(cid:9) converges faster than {1/k} to zero. This is because that
such a choice would increase the computational cost of solving the AL subproblem,
but theoretically cannot improve the convergence rate of {δk} , which is O (1/k)
in this case. One possible choice of the sequence {ηk} is

ηk =

σ
k2α , k = 1, 2, . . .

(2.10)

with some constant σ > 0 and α ∈ ( 1
all conditions required in Theorem 2.2.

2 , 1]. It is easy to check that (2.10) satisﬁes

Theorem 2.3 gives the non-ergodic convergence rate of Algorithm 1 when ηk is

chosen as in (2.10).

Theorem 2.3 Let (cid:8)xk(cid:9) and (cid:8)λk(cid:9) be generated by Algorithm 1. Suppose that the
positive sequence {ηk} is chosen as in (2.10). Then,

δk ≤

C
kα , k = 1, 2, . . . ,

where

C = 4s 3(cid:0) 3

2 θσ + 1(cid:1) σ

θ

+

4
3

max(cid:26)δ1,

4

θ(cid:27) ,

and θ is given in (2.4);

(2.11)

(2.12)

and

(cid:13)(cid:13)(cid:13)
−(cid:13)(cid:13)λ∗(cid:13)(cid:13)pψk −

2

β (cid:18) C + √σ

2

≤ ψk :=

kα (cid:19) , k = 1, 2, . . . ;

Axk+1 − b(cid:13)(cid:13)(cid:13)
ψk ≤ F (xk+1) − F (x∗) ≤(cid:0)(cid:13)(cid:13)λ∗(cid:13)(cid:13) + B(cid:1)pψk + ηk, k = 1, 2, . . . ;

β
2

(2.13)

(2.14)

where λ∗ is an optimal solution to problem (1.3) and B is given in (2.4).

As a direct consequence of Theorem 2.3, we obtain the following result.

Then,

Corollary 2.4 Let (cid:8)xk(cid:9) and (cid:8)λk(cid:9) be generated by Algorithm 1 with ηk = σ
δk = O(cid:18) 1
√k(cid:19) .

= O(cid:18) 1

= O(cid:18) 1

2

k2 .

k(cid:19) , and (cid:12)(cid:12)(cid:12)

F (xk+1) − F (x∗)(cid:12)(cid:12)(cid:12)

k(cid:19) , (cid:13)(cid:13)(cid:13)

Axk+1 − b(cid:13)(cid:13)(cid:13)

Non-ergodic convergence rate of an IAL framework for convex programming

7

2.3 Proof of Theorems 2.1, 2.2, and 2.3

In this subsection, we prove Theorems 2.1, 2.2, and 2.3. For ease of presentation,
we deﬁne

x(λk) := arg min

x∈Rn Lβ(x; λk), k = 1, 2, . . . ,

∇d(λk) :=Ax(λk) − b, k = 1, 2, . . . ,
¯d(λk) :=Lβ(xk+1; λk), k = 1, 2, . . . ,
∇ ¯d(λk) :=Axk+1 − b, k = 1, 2, . . . ,

(2.15)

(2.16)

(2.17)

where xk+1 is generated by Algorithm 1 and satisﬁes (1.7).

We ﬁrst prove the following two lemmas, which have been proved for smooth
function F (x) in [3,10,14]. We now extend them to composite nonsmooth function
F (x). Lemma 2.5 shows that d(λk+1) can be bounded from both above and below

and Lemma 2.6 shows that (cid:13)(cid:13)∇d(λk) − ∇ ¯d(λk)(cid:13)(cid:13) is bounded by pηk/β.

Lemma 2.5 The following two inequalities hold:

and

d(λ) ≤ ¯d(µ) +(cid:10)∇ ¯d(µ), λ − µ(cid:11) , ∀ λ, µ,
d(λk+1) ≥ ¯d(λk) +
− ηk.

β

2

2 (cid:13)(cid:13)(cid:13)∇ ¯d(λk)(cid:13)(cid:13)(cid:13)

(2.18)

(2.19)

Proof We ﬁrst show (2.18). By the deﬁnitions of d(λ) and Lβ(x; λ), we have
x∈Rn {Lβ(x; λ)} ≤ Lβ(xµ; λ) = Lβ(xµ; µ) + hλ − µ, Axµ − bi,

d(λ) = min

where xµ satisﬁes ¯d(µ) = Lβ(xµ; µ). This, together with the deﬁnition of ∇ ¯d(µ)
(cf. (2.16)), yields (2.18).
We now prove (2.19). By the convexity of f (x), the deﬁnition of ∇ ¯d(λk), and

(2.2), we get

β
Lβ(x; λk+1) ≥ f (xk+1) +D∇f (xk+1), x − xk+1E + g(x) +Dλk+1, Ax − bE +
2 kAx − bk2
2(cid:19)
(Ax − b) − ∇ ¯d(λk)(cid:13)(cid:13)(cid:13)

= Lβ(xk+1; λk) + β(cid:18)D∇ ¯d(λk), Ax − bE +
+D∇ ˆfβ(xk+1; λk), x − xk+1E + g(x) − g(xk+1).

2(cid:13)(cid:13)(cid:13)

1

Taking the minimum over x ∈ Rn on both sides of the above inequality, we have
2(cid:27)
d(λk+1) ≥ Lβ(xk+1; λk) + β min

x∈Rn(cid:26)D∇ ¯d(λk), Ax − bE +

1

2(cid:13)(cid:13)(cid:13)

(Ax − b) − ∇ ¯d(λk)(cid:13)(cid:13)(cid:13)

+ min

x∈RnnD∇ ˆfβ(xk+1; λk), x − xk+1E + g(x) − g(xk+1)o .

By using the deﬁnition of ¯d(λ) and (1.7), we immediately get the desired result
(2.19).

8

Ya-Feng Liu et al.

Lemma 2.6 The following inequality holds:

Proof It follows from the optimality of x(λk) (cf. (2.15)) that

(cid:13)(cid:13)(cid:13)∇d(λk) − ∇ ¯d(λk)(cid:13)(cid:13)(cid:13)

2

ηk
β

.

≤

(2.20)

D∇ ˆfβ(x(λk); λk), x(λk) − xk+1E + g(x(λk)) − g(xk+1) ≤ 0.

By setting x = x(λk) in (1.7), we get

D∇ ˆfβ(xk+1; λk), xk+1 − x(λk)E − g(x(λk)) + g(xk+1) ≤ ηk.

Adding the above two inequalities yields

2

ηk ≥D∇ ˆfβ(xk+1; λk) − ∇ ˆfβ(x(λk); λk), xk+1 − x(λk)E
=D∇f (xk+1) + βAT (Axk+1 − b) − ∇f (x(λk)) − βAT (Ax(λk) − b), xk+1 − x(λk)E
≥DβAT (Axk+1 − b) − βAT (Ax(λk) − b), xk+1 − x(λk)E
=β(cid:13)(cid:13)(cid:13)∇d(λk) − ∇ ¯d(λk)(cid:13)(cid:13)(cid:13)
where the second inequality is due to the convexity of f (x) and the second equality
is due to the deﬁnitions of ∇d(λk) and ∇ ¯d(λk).
(cid:3)
Lemma 2.7 shows that the sequence (cid:8)λk(cid:9) generated by Algorithm 1 is bounded.
Lemma 2.7 Let (cid:8)λk(cid:9) be generated by Algorithm 1. Suppose the sequence {ηk}

satisﬁes (2.3), then

,

(2.21)

λk − λ∗(cid:13)(cid:13)(cid:13) ≤ B, k = 1, 2, . . . ,
(cid:13)(cid:13)(cid:13)

where the ﬁrst inequality is due to (2.18) (with λ and µ replaced by λ∗ and λk
respectively), the second inequality is due to (2.19), and the last inequality is due

where B is given in (2.4).

Proof We have

2

(cid:13)(cid:13)(cid:13)

λk+1 − λ∗(cid:13)(cid:13)(cid:13)

2

=(cid:13)(cid:13)(cid:13)
=(cid:13)(cid:13)(cid:13)
≤(cid:13)(cid:13)(cid:13)
=(cid:13)(cid:13)(cid:13)
≤(cid:13)(cid:13)(cid:13)
≤(cid:13)(cid:13)(cid:13)

2

2

λk − λ∗ + β∇ ¯d(λk)(cid:13)(cid:13)(cid:13)
λk − λ∗(cid:13)(cid:13)(cid:13)
λk − λ∗(cid:13)(cid:13)(cid:13)
λk − λ∗(cid:13)(cid:13)(cid:13)
λk − λ∗(cid:13)(cid:13)(cid:13)
λk − λ∗(cid:13)(cid:13)(cid:13)

+ 2βηk,

2

2

2

2

+ 2βD∇ ¯d(λk), λk − λ∗E + β2(cid:13)(cid:13)(cid:13)∇ ¯d(λk)(cid:13)(cid:13)(cid:13)
+ 2β(cid:16) ¯d(λk) − d(λ∗)(cid:17) + β2(cid:13)(cid:13)(cid:13)∇ ¯d(λk)(cid:13)(cid:13)(cid:13)
+ 2β(cid:16)d(λk+1) − d(λ∗)(cid:17) + 2β(cid:16) ¯d(λk) − d(λk+1)(cid:17) + β2(cid:13)(cid:13)(cid:13)∇ ¯d(λk)(cid:13)(cid:13)(cid:13)
+ 2β(cid:16)d(λk+1) − d(λ∗)(cid:17) + 2βηk

2

2

Non-ergodic convergence rate of an IAL framework for convex programming

9

to the fact that d(λk+1) ≤ d(λ∗) for all k ≥ 1. Summing the above inequality, we
obtain

2

(cid:13)(cid:13)(cid:13)

λk − λ∗(cid:13)(cid:13)(cid:13)

λ1 − λ∗(cid:13)(cid:13)(cid:13)
≤(cid:13)(cid:13)(cid:13)

2

+ 2β

k−1

Xi=1

ηi, k = 1, 2, . . . ,

which, together with (2.4), completes the proof.

Proof of Theorem 2.1. To prove Theorem 2.1, it suﬃces to show

δ2
k < +∞,

Xk

and

2

Xk (cid:13)(cid:13)(cid:13)

Axk+1 − b(cid:13)(cid:13)(cid:13)

< +∞.

By (2.19) and the deﬁnition of ¯d(λk), we obtain

(cid:3)

(2.22)

(2.23)

which, together with (2.20) and the inequality a2 ≥ b2/2 − (a − b)2, implies

d(λk+1) ≥ d(λk) +

β

d(λk+1) ≥ d(λk) +

(2.24)

(2.25)

2

β

2

− ηk,

2 (cid:13)(cid:13)(cid:13)∇ ¯d(λk)(cid:13)(cid:13)(cid:13)
4 (cid:13)(cid:13)(cid:13)∇d(λk)(cid:13)(cid:13)(cid:13)
λk − λ∗(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)∇d(λk)(cid:13)(cid:13)(cid:13) ≤ B(cid:13)(cid:13)(cid:13)∇d(λk)(cid:13)(cid:13)(cid:13)

ηk.

−

3
2

.

Moreover, it follows from (2.21) and the concavity of d(λ) that

d(λ∗) − d(λk) ≤D∇d(λk), λ∗ − λkE ≤(cid:13)(cid:13)(cid:13)

Combining the above and (2.25), we immediately obtain

δk+1 ≤ δk − θδ2

k +

3
2

ηk, k = 1, 2, . . . ,

(2.26)

which further implies

δk ≤ δ1 − θ

δ2
i +

k−1

Xi=1

3
2

k−1

Xi=1

ηi, k = 1, 2, . . .

(2.27)

From the deﬁnition of δk, we know δk ≥ 0 for all k ≥ 1. From this, (2.3), and
(2.27), we obtain (2.22).

Next, we prove (2.23). It follows from (2.17) and (2.24) that

2

Axk+1 − b(cid:13)(cid:13)(cid:13)

(cid:13)(cid:13)(cid:13)

≤

2

β (cid:16)d(λk+1) − d(λk) + ηk(cid:17) , k = 1, 2, . . . .

(2.28)

10

Ya-Feng Liu et al.

Summing (2.28) from i = 1 to k yields

k

Xi=1(cid:13)(cid:13)(cid:13)

2

Axi+1 − b(cid:13)(cid:13)(cid:13)

≤

≤

≤

≤

k

k

2

2

ηi!
β  d(λk+1) − d(λ1) +
Xi=1
ηi!
β  d(λ∗) − d(λ1) +
Xi=1
ηi!
β   1
λ1 − λ∗(cid:13)(cid:13)(cid:13)
2β (cid:13)(cid:13)(cid:13)
Xi=1
B2
β2 ,

+

2

k

2

where the last second inequality is due to the facts that ∇d(λ∗) = 0 and ∇d(λ)
is 1
β -Lipschitz continuous [1] and the last inequality is due to (2.4). The proof of
Theorem 2.1 is completed.
(cid:3)

Proof of Theorem 2.2. We prove the theorem by induction. From (2.5) and

(2.6), we know

δk0 ≤

1
2θ

=

τ1
k0

+ τ2√ηk0 .

Therefore, the inequality (2.9) holds for k = k0. Next, we assume that (2.9) holds
for some k ≥ k0, and we consider the case k + 1. We have

δk+1 ≤ δk − θδ2

k +

3
2

ηk

τ1
k
τ1

k

+ τ2√ηk − θ(cid:16) τ1
(k + 1) (k − θτ1)
+ τ2√ηk+1,

k2

k + 1

τ1

k + 1

+

+ τ2√ηk(cid:17)2
+ τ2√ηk+1

3
2

ηk

√ηk(cid:0)1 − 2θτ1
k (cid:1)

√ηk+1

+(cid:18) 3

2 − θτ 2

2(cid:19) ηk

≤

=

≤

where the ﬁrst inequality is due to (2.26), the second inequality is due to the fact
+ τ2√ηk0 =
that {ηk} is nonincreasing, which further implies τ1
2θ for all k ≥ k0, and the last inequality is due to (2.7) and (2.8). The proof of
Theorem 2.2 is completed.

k + τ2√ηk ≤ τ1

(cid:3)

k0

1

To prove Theorem 2.3, we need the following lemma.

Lemma 2.8 Suppose the nonnegative sequence {δk} satisﬁes

E
2

δ2
k+1 + δk+1 ≤ δk, k = 1, 2, . . . ,

(2.29)

where E > 0 is a constant. Then, we have

δk ≤

max(cid:8)δ1, 4
E(cid:9)

k

, k = 1, 2, . . . .

(2.30)

Non-ergodic convergence rate of an IAL framework for convex programming

11

δk+1 ≤ −1 + √1 + 2Eδk

Proof Clearly, the inequality (2.30) is true for k = 1. Next, assuming (2.30) is true
for some k ≥ 1, we show it is also true for k + 1. In fact, we have
≤ −1 +q1 + 2E
E}
max{δ1, 4
2 max(cid:8)δ1, 4
E(cid:9)
k +qk2 + 2E max(cid:8)δ1, 4
E(cid:9) k
max(cid:8)δ1, 4
E(cid:9)

k + 1

E

=

≤

E

,

k

where the ﬁrst inequality is due to the inequality (2.29), and the second inequality
is due to the assumption that (2.30) holds for k.
(cid:3)

Proof of Theorem 2.3. We show (2.11), (2.13), and (2.14) separately.
We ﬁrst show (2.11). Clearly, the inequality (2.11) holds for k = 1. Next, we
assume that (2.11) holds for some k ≥ 1, and show it is also true for k + 1. We use
the contrapositive argument. Assume that (2.11) does not hold for k + 1, i.e.,

where C is given in (2.12). Let z∗ = C

θ > 0. If

θ

2(cid:18)δk+1 −

z∗

(k + 1)α(cid:19)2

z∗

(k + 1)α(cid:19) ≤(cid:18)δk −

z∗

kα(cid:19)

δk+1 >

C

(k + 1)α ,

4 − 1
+(cid:18)δk+1 −

holds, then it follows from Lemma 2.8 that

and

δk+1 −

z∗

(k + 1)α ≤

max(cid:8)δ1, 4
θ(cid:9)

k + 1

,

δk+1 ≤

z∗ + max(cid:8)δ1, 4
θ(cid:9)

(k + 1)α

<

C

(k + 1)α .

(2.31)

(2.32)

(2.33)

Clearly, (2.33) contradicts (2.31), which implies that (2.11) is true. Next, we prove
(2.32), which is equivalent to

P (z∗) :=

θ

2(cid:18)δk+1 −

z∗

(k + 1)α(cid:19)2

+(cid:18)δk+1 −

z∗

(k + 1)α(cid:19) −(cid:18)δk −

z∗

kα(cid:19) ≤ 0.

We consider the following quadratic function Q(z) with respect to z :

Q(z) =

θ
2

z2 −(cid:18) θC

4 − 1(cid:19) z +

3

2(cid:18) 3θσ

2

+ 1(cid:19) σ.

It can be veriﬁed that the minimizer of Q(z) is z∗. Since the discriminant of Q(z)
is nonnegative, it follows that the minimum value

Q(z∗) ≤ 0.

(2.34)

12

Ya-Feng Liu et al.

Moreover, for any k ≥ 1, we have

(2.26) =⇒ δk+1 ≤ δk +

3
2

ηk =⇒

1
2

k+1 ≤ δ2
δ2

k +

9
4

k =⇒ −δ2
η2

k ≤ −

1
2

δ2
k+1 +

9
4

σηk.

Combining the last inequality in the above with (2.26) yields

θ
2

δ2
k+1 + δk+1 ≤ δk +

which further implies

3

2(cid:18) 3θσ

2

+ 1(cid:19) ηk,

(2.35)

P (z∗) =

≤

θ
2

δ2
k+1 + δk+1 − δk −
2(cid:18) 3θσ
+ 1(cid:19) ηk −

3

2

θδk+1z∗
(k + 1)α +

θδk+1z∗
(k + 1)α +

z∗

θ(z∗)2

2(k + 1)2α −
θ(z∗)2

2(k + 1)2α −

(k + 1)α +
z∗
z∗
kα . (2.36)

(k + 1)α +

z∗
kα

k2α , the assumption δk+1 > C

By ηk ≤ σ
and (k + 1)α − kα ≤ 1 for all k ≥ 1 and α ∈ (0, 1], we get

(k+1)α (cf. (2.31)), the facts (k+1)2α ≤ 4k2α

3

2

3

+ 1(cid:19) ηk −
−

2(cid:18) 3θσ
2(cid:0) 3θσ
2 + 1(cid:1) σ
k2α
Q(z∗)
k2α ,

≤
=

θ(z∗)2

θδk+1z∗
(k + 1)α +
θ
θC
2k2α (z∗)2 +
4k2α z∗ +

2(k + 1)2α −
1
k2α z∗

z∗

(k + 1)α +

z∗
kα

which, together with (2.34) and (2.36), yields P (z∗) ≤ 0.

We now show (2.13). From (2.28), we obtain

2

2

Axk+1 − b(cid:13)(cid:13)(cid:13)

(cid:13)(cid:13)(cid:13)

≤

β (cid:16)d(λk+1) − d(λk) + ηk(cid:17) ≤

β (cid:16)d(λ∗) − d(λk) + ηk(cid:17)

=

(δk + ηk) ,

2

2
β

which, together with (2.10) and (2.11), yields (2.13).

Finally, we show (2.14). From the strong duality and the deﬁnition of Lβ(x; λ)

(cf. (1.2)), we obtain

F (x∗) ≤ Lβ(xk+1; λ∗) = F (xk+1) +Dλ∗, Axk+1 − bE +
Axk+1 − b(cid:13)(cid:13)(cid:13)
F (xk+1) − F (x∗) ≥ −kλ∗kpψk −

≤ F (xk+1) +(cid:13)(cid:13)λ∗(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

β
2

This, together with (2.13), implies

β

2

β

2 (cid:13)(cid:13)(cid:13)
Axk+1 − b(cid:13)(cid:13)(cid:13)
2 (cid:13)(cid:13)(cid:13)
Axk+1 − b(cid:13)(cid:13)(cid:13)

+

2

.

ψk.

(2.37)

Non-ergodic convergence rate of an IAL framework for convex programming

13

On the other hand, we have

Lβ(xk+1; λk) ≤ ˆfβ(x(λk); λk) +D∇ ˆfβ(xk+1; λk), xk+1 − x(λk)E + g(xk+1)

= d(λk) +D∇ ˆfβ(xk+1; λk), xk+1 − x(λk)E + g(xk+1) − g(x(λk))
≤ d(λk) + ηk
≤ F (x∗) + ηk,

where the ﬁrst inequality is due to the convexity of ˆfβ(x; λ) with respect to x, the
ﬁrst equality is due to the deﬁnition of d(λk), the second inequality is due to (1.7),
and the last inequality is due to the fact d(λk) ≤ F (x∗). Recall the deﬁnition of
Lβ(x; λ), we get

F (xk+1) +Dλk, Axk+1 − bE +

which, together with (2.21), immediately implies

β

2 (cid:13)(cid:13)(cid:13)

Axk+1 − b(cid:13)(cid:13)(cid:13)

2

≤ F (x∗) + ηk,

F (xk+1) − F (x∗) ≤(cid:0)kλ∗k + B(cid:1)pψk + ηk.

Combining (2.37) and (2.38) yields (2.14). The proof of Theorem 2.3 is completed.
(cid:3)

(2.38)

2.4 Remarks

In this subsection, we make some remarks on the comparison of our proposed IAL
framework (Algorithm 1) and one closely related IAL method [10] for solving the
linearly constrained convex programming.

The method in [10] is designed for solving problem (1.1) with g(x) = IndX (x).
They apply Nesterov’s optimal ﬁrst-order method to solve AL subproblem (2.1)
until a point xk+1 satisfying (1.6) is found. Our IAL framework can be used to
solve more general problem (1.1) (with a general composite function g(x)). Our
framework requires approximately solving subproblem (2.1) until a point xk+1
satisfying (1.7) (which is easier to check than (1.6)) is found.

The work [10] shows the same non-ergodic convergence rate results as ours in
Corollary 2.4 but under a much stronger condition that the sequence {ηk} in (1.6)
satisﬁes

k

Xi=1

η2

k(cid:19) .
i = O(cid:18) 1

To make it more clearly, consider the special case where we are interested in ﬁnding
an exact solution of problem (1.1), which requires k → +∞ in Corollary 2.4. In
this case, the method in [10] needs solving each AL subproblem exactly (i.e., ηi
in (1.6) needs to be zero for all i = 1, 2, . . .) while our IAL framework only needs
solving each subproblem approximately (i.e., ηi in (1.7) only needs to be in the
order of O(1/i2) for i = 1, 2, . . .).

Acknowledgements We would like to thank Guanghui Lan, Zhaosong Lu, Zaiwen Wen, and
Wotao Yin for their insightful comments, which helped us in improving the results in this
paper. We thank Xiangfeng Wang for the useful discussion on an earlier version of this paper.

14

References

Ya-Feng Liu et al.

1. Bertsekas, D.P.: Nonlinear Programming. Athena Scientiﬁc, Belmont, MA (1999)
2. Bertsekas, D.P., Tsitsiklis, J.N.: Gradient convergence in gradient methods with errors.

SIAM J. Optim. 10(3), 627–642 (2000)

3. Devolder, O., Glineur, F., Nesterov, Y.: First-order methods of smooth convex optimization

with inexact oracle. Math. Program. 146(1-2), 37–75 (2014)

4. Eckstein, J., Silva, P.J.: A practical relative error criterion for augmented Lagrangians.

Math. Program. 141(1-2), 319–348 (2013)

5. Frank, M., Wolfe, P.: An algorithm for quadratic programming. Naval Research Logistics

Quarterly 3(1-2), 95–110 (1956)

6. Hestenes, M.R.: Multiplier and gradient methods. J. Optim. Theory Appl. 4(5), 303–330

(1969)

7. Jaggi, M.: Sparse convex optimization methods for machine learning. Ph.D. thesis, ETH

Zurich (2011)

8. Jaggi, M.: Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In: Pro-

ceeding of ICML, Atlanta (2013)

9. Lan, G.: Gradient sliding for composite optimization. Math. Program. (2015). DOI

10.1007/s10107-015-0955-5

10. Lan, G., Monteiro, R.D.: Iteration-complexity of ﬁrst-order augmented Lagrangian meth-

ods for convex programming. Math. Program. (2015). DOI 10.1007/s10107-015-0861-x

11. Luo, Z.Q., Tseng, P.: Error bounds and convergence analysis of feasible descent methods:

A general approach. Ann. Oper. Res. 46(1), 157–178 (1993)

12. Necoara, I., Patrascu, A.: Iteration complexity analysis of dual ﬁrst order methods for

conic convex programming. http://arxiv.org/abs/1409.1462 (2015)

13. Necoara,

I., Patrascu, A., Glineur, F.: Complexity certiﬁcations

of ﬁrst or-
convex programming.

inexact Lagrangian and penalty methods

der
http://arxiv.org/abs/1506.05320 (2015)

for

conic

14. Nedelcu, V., Necoara, I., Tran-Dinh, Q.: Computational complexity of inexact gradient
augmented Lagrangian methods: Application to constrained MPC. SIAM J. Control Op-
tim. 52(5), 3109–3134 (2014)

15. Nesterov, Y.: A method of solving a convex programming problem with convergence rate

O(1/k2). Sov. Math. Dokl. 27(2), 372–376 (1983)

16. Nesterov, Y.: Smooth minimization of non-smooth functions. Math. Program. 103(1),

127–152 (2005)

17. Nesterov, Y.: Gradient methods for minimizing composite functions. Math. Program.

140(1), 125–161 (2013)

18. Nesterov, Y.: Complexity bounds for primal-dual methods minimizing the model of ob-

jective function. http://www.uclouvain.be/en-452691.html (2015)

19. Powell, M.J.D.: A method for nonlinear constraints in minimization problems.

In: R.

Fletcher (ed.) Optimization pp. 283–298 (1969)

20. Rockafellar, R.T.: Augmented Lagrangian and applications of the proximal point algorithm

in convex progremming. Math. Oper. Res. 1(2), 97–116 (1976)

21. So, A.M.C.: Non-asymptotic convergence analysis of inexact gradient methods for machine

learning without strong convexity. http://arxiv.org/abs/1309.0113 (2013)

22. Yang, L., Sun, D., Toh, K.C.: SDPNAL+: A majorized semismooth Newton-CG aug-
mented Lagrangian method for semideﬁnite programming with nonnegative constraints.
Math. Prog. Comp. (2015). DOI 10.1007/s12532-015-0082-6

