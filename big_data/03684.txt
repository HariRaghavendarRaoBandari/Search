6
1
0
2

 
r
a

 

M
1
1

 
 
]
h
p
-
d
e
.
s
c
i
s
y
h
p
[
 
 

1
v
4
8
6
3
0

.

3
0
6
1
:
v
i
X
r
a

Investigating the role of model-based reasoning while troubleshooting an electric

circuit

Dimitri R. Dounas-Frazer,1, ∗ Kevin L. Van De Bogart,2 MacKenzie R. Stetzer,2, 3 and H. J. Lewandowski1, 4

1Department of Physics, University of Colorado Boulder, Boulder, CO 80309, USA

2Department of Physics and Astronomy, University of Maine, Orono, ME 04469, USA

3Maine Center for Research in STEM Education, University of Maine, Orono, ME 04469, USA

4JILA, National Institute of Standards and Technology and University of Colorado Boulder, Boulder, CO 80309, USA

(Dated: March 14, 2016)

We explore the overlap of two nationally-recognized learning outcomes for physics lab courses,
namely, the ability to model experimental systems and the ability to troubleshoot a malfunctioning
apparatus. Modeling and troubleshooting are both nonlinear, recursive processes that involve using
models to inform revisions to an apparatus. To probe the overlap of modeling and troubleshooting,
we collected audiovisual data from think-aloud activities in which eight pairs of students from two
institutions attempted to diagnose and repair a malfunctioning electrical circuit. We characterize
the cognitive tasks and model-based reasoning that students employed during this activity. In doing
so, we demonstrate that troubleshooting engages students in the core scientiﬁc practice of modeling.

PACS numbers: 01.30.Cc, 01.40.Fk, 01.50.Qb, 07.50.Ek

I.

INTRODUCTION

Recently, there have been national calls to study [1]
and improve [2] lab instruction in the sciences. Along
these lines, the American Association of Physics Teach-
ers (AAPT) released guidelines for learning outcomes in
undergraduate physics lab courses [3]. The AAPT guide-
lines focus on skill-based learning outcomes that align
with the cognitive tasks involved in, for example, table-
top experimental physics research [4].
In the present
work, we investigate the overlap of two major learning
goals of instructional physics lab environments: (1) the
ability to model experimental systems, and (2) the ability
to troubleshoot a malfunctioning apparatus. While these
two abilities are sometimes presented as distinct, we aim
to show that they are in fact overlapping both in theory
and in practice. We show that, for some students, model-
based reasoning plays a key role in the troubleshooting
process.

Modeling is the nonlinear, recursive process of con-
structing, testing, and reﬁning models [5]. Modeling has
been identiﬁed as an important physics practice at both
secondary [6] and post-secondary levels [3]. While tra-
ditional introductory physics lab courses have been crit-
icized as rote and inauthentic [4, 7], there nevertheless
exist innovative approaches that engage students in the
iterative process of modeling at the introductory level,
such as ISLE [8] and Modeling Instruction [9]. At the
upper-division level, the Modeling Framework for Exper-
imental Physics (hereafter, “the Modeling Framework”)
has been developed to characterize students’ model-based
reasoning [5] and to inform development of instructional
lab environments that engage students in the practice of
modeling [10–12].

∗ dimitri.dounasfrazer@colorado.edu

Like modeling, troubleshooting is also a nonlinear, re-
cursive process, though the goal is more narrow: to repair
(or revise) a malfunctioning apparatus [13–15]. Indeed,
Zwickl et al. [5] noted similarities between modeling and
troubleshooting in their work characterizing student rea-
soning on an experimental optical physics activity. They
found that, while troubleshooting, students sometimes
engaged in “what appeared to be a rapid modeling cycle
involving a series of qualitative predictions and qualita-
tive measurements . . . in order to identify the source of
the problem” (Ref. [5], p. 8). It is precisely this overlap
that we interrogate in the present work.

Troubleshooting is a task that spans a wide range of
contexts, such as making medical diagnoses, maintain-
ing manufacturing equipment, and debugging computer
software systems (see Refs. [13–15] for more comprehen-
sive reviews of the troubleshooting literature).
In the
education research literature, major research foci include
identifying the skills and knowledge required for eﬀective
troubleshooting [16, 17], characterizing the novice-to-
expert transition [17–19], and developing teaching strate-
gies for troubleshooting [20–25]. Of particular relevance
to the present work, some previous work has focused on
high school students’ ability to troubleshoot simulated
electric circuits [18–23].

Three factors make electronics courses an ideal context
for studying physics students’ troubleshooting abilities.
First, the physical systems and models with which stu-
dents interact are relatively simple. Second, the electric
circuits that students build during lab activities consist
of low-cost, easy-to-replace components, thus facilitating
multiple revisions to the experimental system. Finally,
students often construct circuits that don’t initially work,
and the need to troubleshoot arises naturally in most
lab activities. Previous work in the domain of electron-
ics courses for physics students has focused on: charac-
terizing college students’ understanding of electric cir-
cuits [26–30]; characterizing expertise-related diﬀerences

among high school students troubleshooting simulated
circuits [18, 19]; and designing teaching strategies to de-
velop college students’ conceptual understanding [31–33],
engage college students in model-based reasoning [12],
and improve high school students’ troubleshooting abil-
ity [20–23]. However, we are not aware of work that
focuses on physics students’ ability to troubleshoot phys-
ical (as opposed to simulated) electric circuits, or of work
that focuses on the troubleshooting processes employed
by post-secondary physics students.

In this paper, we report on a study that explores the
overlap of modeling and troubleshooting in the context of
an activity that is typical of an upper-division electron-
ics course for physics students. In this study, eight pairs
of students attempted to diagnose and repair a malfunc-
tioning electric circuit. Using audiovisual data collected
from two institutions, we characterize the cognitive tasks
and model-based reasoning that students employed dur-
ing this activity. The work herein builds on preliminary
analyses of a subset of our data, which have been reported
elsewhere [34, 35].

Our work expands current knowledge of instructional
physics laboratory environments in two ways. First, we
apply frameworks for both modeling and troubleshooting
to a new domain in physics education, namely, upper-
division electronics. Second, we examine the synergies
of two nationally-recognized learning outcomes for lab
courses, namely, (1) the ability to model experimental
systems and (2) the ability to troubleshoot a malfunc-
tioning apparatus. In doing so, we demonstrate that elec-
tronics courses—whose content is sometimes dismissed as
not real physics—can engage students in important ex-
perimental physics practices.

This paper is organized as follows. In Sec. II, we de-
scribe the two theoretical perspectives that inform our
work: a cognitive task analysis of troubleshooting and
a framework for describing the modeling process.
In
Sec. III, we provide institutional context for our study
and a description of our study participants. In Sec. IV,
we describe our research methods, including a detailed
description of the troubleshooting activity. Our results
are presented in Sec. V and discussed in Sec. VI. Finally,
we summarize our ﬁndings and discuss future directions
for our work in Sec. VII.

II. THEORETICAL PERSPECTIVES

Throughout this work, we deﬁne troubleshooting as
the process of repairing a malfunctioning system. Trou-
bleshooting is a type of problem-solving for which the
solution state is known, but the troubleshooter must de-
termine what information is needed for problem diag-
nosis [15]. Our goal is to identify and describe exam-
ples of how students use (or don’t use) model-based rea-
soning while troubleshooting. We grounded our design
and analysis in two diﬀerent, complementary theoretical
perspectives: a cognitive task analysis of troubleshoot-

2

ing [14, 15, 17], and the Modeling Framework, which de-
scribes physicists’ use of models when conducting physics
experiments [5]. The motivations for using these two
perspectives are twofold. First, we are able to map the
Modeling Framework onto existing analyses of the trou-
bleshooting process. Second, when analyzing students’
approaches to troubleshooting, we use the cognitive trou-
bleshooting tasks and the Modeling Framework to pro-
vide complementary coarse- and ﬁne-grained descriptions
of students’ thought processes. In this section, we elab-
orate on each of these perspectives and identify areas of
overlap. Because the system we consider here is a circuit,
we provide examples from electronics to help clarify ideas
throughout the discussion.

A. Cognitive Task Analysis of Troubleshooting

Cognitive task analysis is “a family of methods used
for studying and describing reasoning and knowledge”
(Ref. [36], p. 3). Our summary of various cognitive task
analyses of troubleshooting [14, 15, 17] explicates both
the types of knowledge and the types of tasks that facil-
itate eﬀective troubleshooting.

1. Types of troubleshooting knowledge

Other work [15, 18] has identiﬁed six kinds of knowl-
edge that facilitate competent troubleshooting: Domain,
System, Procedural, Strategic, Metacognitive, and Ex-
periential. Domain Knowledge consists of the theories
and principles upon which the system was designed [15].
In the case of a circuit, Domain Knowledge may include
underlying principles like electron transport or conser-
vation of charge as well as models like Ohm’s Law or
the Golden Rules for op-amps. These principles enable
the troubleshooter to both represent the problem and
identify relevant problem-solving operations [18], such as
which voltages to measure in order to determine whether
or not a particular component is functioning properly.

System Knowledge includes understanding of the
structure and function of the system and the components
within the system [37].
In a circuit, this may involve
recognizing that a complex circuit is composed of mul-
tiple subsystems or identifying a particular resistor as a
“feedback resistor.” System Knowledge further includes
understanding of spatial representations of the system
and ﬂow control within the system [15]. For circuits,
diagrams and schematics of the conﬁguration of subsys-
tems and components are common representations that
are used to trace current through the system.

Procedural Knowledge refers to the appropriate use
of test equipment and procedures [15]. For electronic
systems, this includes understanding how to use oscillo-
scopes, multimeters, and power supplies.

Strategic Knowledge includes heuristic techniques
and systematic approaches to troubleshooting the sys-

tem [18]. Strategic Knowledge is an essential part of
competent troubleshooting [13]. For example, experts
employ particular sequences of operations to reduce the
problem space by reducing the number of potential loca-
tions for faults [17]. Three commonly used troubleshoot-
ing strategies are [15]:

1. Exhaustive, which involves identifying all possible
faults and testing them one-by-one until the actual
fault is discovered;

2. Topographic, which involves performing a series of
tests that follow a trace through the system, either
moving “downstream” from a point where the sys-
tem behaves correctly or “upstream” from a point
of malfunction; and,

3. Split-Half, which involves checking the functional-
ity of the system at a midpoint in order to reduce
the problem space by isolating the fault in one half
or the other.

Metacognitive Knowledge “is used to monitor [the trou-
bleshooting process] by keeping track of the progress to-
ward the goal state” (Ref. [18], p. 237). Such monitoring
is required in order to evaluate the eﬀectiveness of a strat-
egy and, if needed, to switch to a diﬀerent strategy [13].
In ongoing work [35], we are exploring the role of socially-
mediated metacognition in troubleshooting [38].

Finally, Experiential Knowledge is the historical infor-
mation accumulated by experienced troubleshooters [15].
Experiential Knowledge enables troubleshooters to pro-
pose likely faults by recalling historical information that
links symptoms to likely causes. This process can be
faster than relying on other types of knowledge to create
logical connections between symptoms and causes based
on the function of the system and its components. Re-
call of historical information has been shown to be a
frequent diagnosis strategy among technicians in man-
ufacturing [39], machining [40], and maintenance [16]
contexts.
In an electronics course, neglecting to prop-
erly power a circuit is a common mistake. Thus, when
students encounter a malfunctioning circuit whose out-
put voltage is zero, students’ experience might prompt
them to immediately check power connections rather
than speculate about the conﬁguration or misbehavior
of the circuit.

These six types of knowledge—Domain, System, Pro-
cedural, Strategic, Metacognitive, and Experiential—are
brought to bear when attempting to repair a malfunction-
ing system, though they play diﬀerent roles at diﬀerent
stages of the troubleshooting process.

2. Cognitive troubleshooting tasks

Several models of the troubleshooting process exist,
all of which describe the process as recursive and non-
linear [14, 15, 17]. In the present work, we draw on the
cognitive task analysis proposed by Schaafstal et al. [14],

3

FIG. 1. Cognitive troubleshooting tasks. These tasks de-
scribe the iterative process of repairing a malfunctioning ap-
paratus. Performing each task requires up to six types of
knowledge: domain, system, procedural, strategic, metacog-
nitive, and experiential. This ﬁgure is based on the cognitive
task analysis proposed by Schaafstal et al. [14].

which subdivides troubleshooting into four iterative sub-
tasks: Formulate Problem Description, Generate Causes,
Test, and Repair and Evaluate. A graphical representa-
tion of these tasks is provided in Fig. 1.

Formulate Problem Description refers to the early
stage of the troubleshooting process, during which the
troubleshooter determines both what the system is do-
ing wrong and what it is doing right [14]. During this
stage, the troubleshooter performs initial checks, mea-
surements, and inspections of the apparatus. In an elec-
tronic system, this process involves orienting to the cir-
cuit [19] by building mental representations of the sys-
tem structure and functions or using external representa-
tions [15], such as schematics, datasheets, and equations.
Generate Causes involves generating causal hypothe-
ses, either by recognition of common symptoms (typical
of experts) or by using reasoning skills, functional think-
ing, and external documentation (typical of troubleshoot-
ers encountering a problem for the ﬁrst time) [14].
In
addition to generating hypotheses that propose explana-
tions for symptoms, this phase of troubleshooting also
involves the use of Strategic Knowledge to propose pro-
cedures to facilitate identiﬁcation of faults [15].

Test

involves performing measurements,

tests, or
checks to determine whether or not a proposed cause is
indeed the actual fault that needs to be repaired. Accord-
ing to Shaafstal et al. [14], this task includes “choosing
the right testing methods and the right testing [equip-
ment]” as well as “correctly setting up and operating the
testing [equipment] and correctly reading the outcome of
the test” (p. 79). In the case of a malfunctioning circuit,
testing requires correct use of oscilloscopes, multimeters,
and powers supplies. Performing tests further involves
evaluating and interpreting the outcome [14, 19], which
requires that the troubleshooter form expectations about
the behavior of a functional system and compare those

StopFormulate Problem DescriptionGenerate CausesTest:"Is the proposed cause an actual fault?"Repair and Evaluate:"Is the apparatus functioning as expected?"NoYesYesNoexpectations to the actual performance of the system. If
the observed and expected outcomes of a test are in align-
ment, the proposed cause that informed the test must be
rejected and the troubleshooter must generate additional
causes. Alternatively, if a fault is identiﬁed, the next task
is to repair the system.

Lastly, Repair and Evaluate includes generating, en-
acting, and verifying solutions, in direct service to the
goal of returning the system to its normal working
state [13–15]. Simple repairs involve replacing a com-
ponent, though other types of repair are possible (e.g.,
soldering a broken connection). After performing a re-
pair, evaluative measurements must be performed to de-
termine whether the system is functioning normally. If
not, the troubleshooter may conclude either that the re-
pair did not address the fault or that the malfunction is
due to multiple faults. In either case, the troubleshooter
must return to the task of generating causes. If, on the
other hand, the system behaves normally, then the trou-
bleshooter may conclude that the repair is complete.

In this paper, our theoretical understanding of trou-
bleshooting is partially informed by the six types of
knowledge and the four subtasks described above. How-
ever, one goal of the present work is to understand the
troubleshooting process through the lens of the Modeling
Framework, which we describe in the following subsec-
tion.

B. Modeling Framework

The Modeling Framework describes the dynamic pro-
cess through which experimental physicists develop and
reﬁne models and apparatus. A diagram of the Model-
ing Framework is provided in Fig. 2. To explicate the
framework, we deﬁne both “models” and the process of
“modeling.” In doing so, we draw heavily on the work of
Zwickl et al. [5].

1. Models

Models are abstract representations of the real world.
A well-deﬁned model is associated with a target system or
phenomenon of interest, and the model can be used for
either explanatory and/or predictive purposes. Models
are embedded in underlying principles and concepts rel-
evant for understanding the target system. In addition,
models are externally articulated through equations, di-
agrams, descriptions, and other representations. These
representations are often informed by the topography of
the target system and the ﬂow of matter, energy, or in-
formation through the system. A circuit diagram, for
instance, is a graphical representation of a circuit that
shows how components are connected to one another and
how charges ﬂow through the circuit.

Importantly, models contain simplifying assumptions
that yield tractable mathematical, graphical, and other

4

representations. These assumptions limit the applicabil-
ity of a model, meaning that users of the model must un-
derstand whether and when it can be accurately applied.
Moreover, model limitations give rise to the possibility of
model reﬁnement by eliminating some assumptions, thus
increasing the complexity of the model and broadening
its scope of applicability. The iterative improvement of
models to make them more accurate and sophisticated is
one path in the process of modeling.

2. Modeling

Modeling is the process through which models and sys-
tems are brought into better agreement, either by reﬁn-
ing the model or the target system itself. The Modeling
Framework subdivides the target system into two parts,
each with its own corresponding model (Fig. 2): the phys-
ical system and the measurement system. This subdivi-
sion reﬂects the fact that experimental physicists often
operate measurement equipment in regimes where the
limitations of that equipment become important. Such
limitations must be accounted for either by making mod-
iﬁcations to existing equipment or by developing an un-
derstanding of the tools’ performance in new parameter
regimes.

In many cases, the division between physical and mea-
surement systems is fuzzy. For example, in circuits such
as the one considered here, the physical system consists of
the circuit itself (wires, resistors, and other components)
whereas the measurement system comprises voltmeters,
ammeters, and other measurement tools. Whether power
supplies and other “test equipment” are included in the
physical or measurement system reﬂects an arbitrary
choice on the part of the modeler. Here, we include power
supplies in the measurement system.

Modeling is a dynamic and iterative process, involving
the following phases: model construction, interpretation,
prediction, comparison, proposal, and revision. Model
construction refers to the development of models of the
measurement and physical systems, depicted at the top
left and right of Fig. 2, respectively. This process in-
volves:
identifying general principles and concepts that
underly the model; making assumptions that simplify the
model and identifying the corresponding limitations on
the model’s applicability; and choosing realistic values
for model parameters.

While the model of the physical system is used to make
predictions about the performance of the physical appa-
ratus, the model of the measurement system is used to
interpret raw data output by the measurement appara-
tus.
In an optical system, this might involve using a
known calibration factor to convert the output voltage
of a photometer into a measurement of optical power.
In electrical systems, when using a digital multimeter to
measure the voltage of an oscillating signal, it is impor-
tant to know whether the multimeter is displaying the
amplitude of the signal or its root-mean-square value.

5

FIG. 2. Modeling Framework for Experimental Physics. This Framework describes the iterative process of constructing models
of the measurement and physical systems, comparing measurements to predictions, proposing explanations for discrepancies,
and revising models and/or apparatus. Darker shades of gray correspond to phases common in the troubleshooting process.
Bold phrases indicate aspects of the Framework that informed our a priori analysis scheme. This ﬁgure is adapted from the
visualization presented by Zwickl et al. [5].

Comparison is the act of comparing predictions to in-
terpreted measurements. Discrepant measurements and
predictions prompt physicists to propose potential ex-
planations for, and/or solutions to, those discrepancies.
Resolving discrepancies requires a revision to either the
models or apparatus. The framework describes four
pathways of revision:
reﬁne the measurement system
model, the measurement system apparatus, the physical
system apparatus, or the physical system model. Priori-
tization of one particular revision pathway over others de-
pends on many factors, including the nature of the task.
For example, based on the deﬁnition of troubleshooting
used here, a troubleshooting activity will likely result in
revision to the physical system apparatus.

C. Synthesizing the Frameworks

While the cognitive task analysis of troubleshooting
and the Modeling Framework provide two distinct per-
spectives through which to understand the troubleshoot-
ing process, they are nevertheless connected. In this sub-
section, we synthesize these two perspectives by describ-
ing how both the types of knowledge and the cognitive
tasks involved in troubleshooting relate to the modeling
process.

1. Modeling and types of troubleshooting knowledge

Domain, System, and Procedural Knowledge can be di-
rectly connected to modeling. For example, these types
of knowledge are required for the construction of models
of the physical and measurement systems. Strategic and
Metacognitive Knowledge, on the other hand, are only
implicitly connected to modeling. For example, the pro-
cess of modeling involves deciding which measurements
to perform, in what order, and for what purpose. Alter-
natively, in response to a discrepancy between measure-
ment and prediction, a physicist must decide which of
the four revision pathways to enact. While such strate-
gic and metacognitive decisions are necessary parts of
the modeling process, they are not explicitly represented
in the Modeling Framework. Rather, they are implicitly
embedded in the arrows of Fig. 2: each arrow represents
diﬀerent possible metacognitive and strategic choices on
the part of the experimentalist while navigating between
diﬀerent phases of the modeling process.

Depending on the circumstances, Experiential Knowl-
edge can also be implicitly embedded in the Modeling
Framework. For example, a troubleshooter may rely on
historical information when making decisions about what
to measure or what to revise. In this sense, the role of Ex-
periential Knowledge in modeling is similar to the roles of
Strategic and Metacognitive Knowledge. In other cases,
however, Experiential Knowledge may limit the relevance
of the framework for understanding a particular instance
of troubleshooting. Experienced troubleshooters call on

AbstractionAbstractionAbstractionAbstractionMeasurementMeasurementInterpretation of dataPredictionModel Construction:Measurement SystemMeasurementapparatusMeasurementmodelPrinciples,conceptsLimitations,assumptionsParametersComparison: "Is the agreement good enough?"Proposal: "How can we get better agreement?"Revision:MeasurementmodelRevision:MeasurementapparatusRevision:Physical systemapparatusRevision:Physical systemmodelModel Construction:Physical SystemPhysical systemapparatusPhysical systemmodelPrinciples,conceptsLimitations,assumptionsParametersNoYesStopevent schemas based on their historical experience with
a system and its speciﬁc fault tendencies, often shorten-
ing their diagnostic process [15]. Using schemas to solve
problems quickly is a common feature of expert problem-
solving in physics and other contexts [41]. Thus, Expe-
riential Knowledge may facilitate direct connections be-
tween symptoms and diagnoses without the need to en-
gage in the recursive, nonlinear processes the Modeling
Framework was designed to describe. In these situations,
the framework may not be the most appropriate tool for
characterizing the troubleshooting process.

2. Modeling and cognitive troubleshooting tasks

The cognitive troubleshooting tasks provide a taxon-
omy for some of the modeling phases. For example, con-
sider the role of measurement in troubleshooting. Mea-
surements can be classiﬁed into three types according to
the cognitive task with which they are aﬃliated: forma-
tive measurements, which serve to formulate the prob-
lem description during initial stages of the troubleshoot-
ing process; diagnostic measurements, used to test causal
hypotheses during the testing phase; and evaluative mea-
surements, used to determine whether the system has
been restored to its functional state after a revision has
been made. Similarly, the cognitive troubleshooting tasks
discriminate between two types of proposals: proposed
explanations for discrepancies between measurement and
prediction, which facilitate generation of causes; and pro-
posed solutions for resolving those discrepancies, which
inform repairs to the system.

Conversely, the Modeling Framework provides a tax-
onomy of repair types: any of the four revision pathways
in the Modeling Framework could constitute a type of
repair during the troubleshooting process. In our study,
however, repairs primarily consisted of revisions to the
physical system apparatus, ultimately privileging one re-
cursive pathway in the Modeling Framework (shaded in
dark grey in Fig. 2).

One major goal of the present work is to identify and
describe examples of how students use (or don’t use)
model-based reasoning while troubleshooting. To help us
unpack the mapping between the Modeling Framework
and the troubleshooting process, we designed an observa-
tional study in which pairs of students were tasked with
repairing a malfunctioning circuit. In the following sec-
tion, we describe the institutional context and the par-
ticipants involved in our study.

III. CONTEXT AND PARTICIPANTS

Our study was carried out at two universities, the Uni-
versity of Colorado Boulder (CU) and the University of
Maine (UM). Both institutions are predominantly white
four-year public research universities with high under-
graduate enrollment. CU is a large, more selective insti-

6

TABLE I. Demographic breakdown of physics and engineer-
ing physics bachelor degree recipients at CU (∼ 47 per year)
and UM (∼ 11 per year).

Group
Men
Women
White students
Asian students
URM students
Unspeciﬁed race/ethnicity
International students

CUa
84%
16%
77%
6%
5%
10%
2%

UMb
85%
15%
85%
2%
4%
11%
2%

a Demographic data for 2005–14 were provided by the CU Oﬃce

of Planning, Budget, and Analysis.

b Demographic data for 2010–15 were provided by the UM Oﬃce

of Institutional Research.

tution with very high research activity; UM is a medium,
selective institution with high research activity [42]. De-
mographic information about the physics programs at
each institution is summarized in Table I. These demo-
graphics reﬂect the makeup of the students enrolled in
the Electronics Courses at CU and UM as well as those
who participated in our study.

The Electronics Courses at CU and UM share many
similarities. Each course is required for all physics ma-
jors, with students typically completing the course dur-
ing their third year of instruction. Both courses convene
three times per week: twice for one-hour lectures and
once for a multi-hour lab (three hours at CU, two at
UM). Both Electronics Courses consist of 2–3 lab sec-
tions, with 15–20 students per section at CU and 5–
8 at UM. Lectures and labs are taught by tenured or
tenure-track physics faculty members. Teaching and/or
learning assistants support instruction at each institu-
tion. Both courses focus on analog components (e.g.,
op-amps, diodes, and transistors) and circuits (e.g., di-
viders, ﬁlters, and ampliﬁers). To learn this material,
students work in pairs on guided lab activities. There
is no formal instruction about troubleshooting in either
course; instead, discussion about troubleshooting is lim-
ited to impromptu conversations between students and
instructors in response to problems that inevitably arise
during lab.

including weekends.

The CU and UM courses diﬀer in several ways. At
CU, for example, the course is oﬀered every semester and
enrollment varies from 30–60 students per term. Stu-
dents have keycard access to the lab room at all hours
of the day,
In addition, the CU
course culminates in a ﬁve-week ﬁnal project. Finally,
the CU Electronics Course was recently redesigned to
engage students in modeling of canonical measurement
equipment and analog circuits [12], in alignment with
consensus learning goals for upper-division labs identi-
ﬁed by physics faculty members at CU [10]. Additional
learning goals for this course were identiﬁed through in-
terviews with graduate students who use electronics as
part of their experimental physics research [43].

7

FIG. 3. (Left) Schematic diagram of inverting cascade ampliﬁer, with design elements highlighted. A comparison of nominal
voltages and resistances in both functioning and malfunctioning versions of the circuit is provided in Table II. (Right) Theoretical
output signals of the inverting cascade ampliﬁer with an input signal of amplitude 100 mV and frequency 100 Hz. In all plots,
the vertical scales on the left and right respectively correspond to the output signal (solid black curve) and the input signal
(dashed grey curve). Plots (a) and (b) show the outputs of Stages 1 and 2, respectively, in a functioning circuit. Plot (c) shows
the output of Stage 2 due to the faulty op-amp, and Plot (d) shows the output of Stage 2 when both op-amps are functional
but the input signal is suﬃciently large that clipping occurs.

At UM, on the other hand, the course is oﬀered only
in the fall, with roughly 10–15 students per term. More-
over, the UM Electronics Course is designated a “writing
intensive” laboratory course, which means that students
are required to complete formal lab write-ups that are
critiqued by an outside technical writing expert (in ad-
dition to the Electronics instructor).

Study participants were physics or engineering physics
majors enrolled in the Electronics Course at either CU or
UM during Fall 2014. During that time, two of the au-
thors (HJL and MRS) taught lab and lecture sections
for the CU and UM courses, and one of the authors
(KLVDB) was a teaching assistant for the UM course.
We solicited participation in the study via email and in-
person requests during the last few weeks of Fall 2014
and the ﬁrst few weeks of Spring 2015. The study was
not an oﬃcial part of either the CU or UM course, and
no course credit was associated with participation in the
study. Participants were consenting volunteers who re-
ceived small monetary incentives for their participation.
In total, 16 students participated in the study, 8 each
from CU and UM. We interviewed students in pairs,
forming four pairs at each institution. Two pairs con-
sisted of students who were lab partners during their
Electronics Course, and six pairs consisted of students
who were not lab partners. The latter six pairs were
formed by the research team by pairing students who had
expressed interest in participating in the study. Fifteen
participants earned grades ranging from A to B– in their
Electronics Course, which required students to work with
the components and systems used in our study. One stu-
dent did not receive a passing grade due to a failure to
submit all of the lab reports and lab notebooks, per the
grading policy of the course. During the interviews, all
eight student pairs attempted to repair a malfunctioning
electrical circuit, as described in the following section.

IV. METHODS

To probe whether and how students engaged in model-
based reasoning while troubleshooting, we conducted
Think-Aloud Pair Problem Solving (TAPPS) interviews
with eight pairs of students. TAPPS interviews involve
students working on an activity while concurrently ver-
balizing their thoughts aloud [44, 45], providing the re-
search team with information on student reasoning about
actions and outcomes [46]. In the troubleshooting liter-
ature, TAPPS interviews have been used in both train-
ing [24] and research [47] contexts. During our TAPPS
interviews, student pairs were tasked with repairing a
malfunctioning electrical circuit, namely, an inverting
cascade ampliﬁer. In this section, we describe the design
of the TAPPS interview and elaborate on data collection
and analysis methods.

A. Troubleshooting activity

We designed an inverting cascade ampliﬁer that con-
tained two subsystems, or stages: a noninverting ampli-
ﬁer (Stage 1) and an inverting ampliﬁer (Stage 2). Each
stage consisted of an op-amp and two resistors: R1 and
R2 in Stage 1, and R3 and R4 in Stage 2. A diagram of
the circuit is provided in the left panel of Fig. 3.

1. Functioning circuit behavior

In a functional circuit, each stage would have ampliﬁed
its input voltage by a multiplicative factor called the gain.
The theoretical gains for Stages 1 and 2 were G1 = (1 +
R2/R1) and G2 = −R4/R3, respectively. Because the

VOUT1 ≡ VIN2VINVOUTR4R3R2R1V1–V2–VS1–VS1+VS2–VS2+Stage 1Stage 2Fault 1Fault 2Volts (V)Volts (V)Time (ms)–1Time (ms)–1Volts (V)Volts (V)(a)(b)(c)(d)10–110–1200–2010–101020100–1010–101020200–2010–1TABLE II. Nominal voltages and resistances in functioning
(func.) and malfunctioning (mal.) versions of the circuit in
Fig. 3, by stage. Characteristics of the malfunctioning circuit
are listed only if they diﬀer from the functioning case.

Stage Quantity

1

2

VIN
VS1+
VS1−
V1−
VOUT1 ≡ VIN2
R1
R2
VIN2 ≡ VOUT1
VS2+
VS2−
V2−
VOUT
R3
R4

Value (func.) Value (mal.)
User deﬁned
+15 V dc
−15 V dc
VIN
2VIN
460 Ω
460 Ω
2VIN
+15 V dc
−15 V dc
GND
−10VIN2 = −20VIN
1 kΩ
10 kΩ

+15 V dc
−15 V dc
100 Ω

two stages were connected in series, the overall gain of
the cascade ampliﬁer was the product of the gains of
Stages 1 and 2: Gtot = G1G2. For a given input voltage,
VIN, a functional circuit’s output voltage, VOUT, would
be given by:

VOUT = −VIN(1 + R2/R1)(R4/R3).

(1)

Equation 1 is valid under the following two conditions:
ﬁrst, the magnitude of the input voltage is suﬃciently
small that the outputs of Stages 1 and 2 are smaller in
magnitude than the corresponding supply voltages, VS1±
and VS2±; and second, the frequency of the input is suf-
ﬁciently low that bandwidth limitations of the op-amps
can be ignored.

The resistances and voltages characteristic of a func-
tioning circuit are given in Table II. Nominal resistances
and supply voltages were communicated to interview par-
ticipants via the schematic and datasheet which were
provided to them during the activity. For these nomi-
nal values, G1 = 2, G2 = −10, and Gtot = −20. That is,
in a functional circuit, Stage 1 would double the input
voltage, Stage 2 would both invert the output of Stage 1
and amplify it by a factor of 10, and the circuit as a whole
would invert the input voltage and amplify it by a factor
of 20. In the context of alternating current (ac) signals,
“inverting” is equivalent to shifting the phase of the sig-
nal by 180◦. Plots (a) and (b) in the right panel of Fig. 3
show the outputs of Stages 1 and 2 when an ac signal is
input to a functioning inverting cascade ampliﬁer.

Finally, each of the op-amps in a functional circuit
would obey the following “Golden Rule” for op-amps in a
closed loop with negative feedback: there is zero voltage
diﬀerence between the two input terminals.

8

2. Malfunctioning circuit behavior

To ensure that students engaged in more than one it-
eration of troubleshooting, we introduced two faults in
the circuit, as shown in the left panel of Fig. 3. First, the
resistor R3 had a value of 100 Ω rather than the nomi-
nal value of 1 kΩ, increasing the actual gain of Stage 2
by an order of magnitude compared to the nominal gain.
Second, we used a malfunctioning op-amp in Stage 2,
which manifested in a direct current (dc) output voltage
of −15 V regardless of the input voltage. Both faults
were localized in Stage 2 so that the cascade ampliﬁer
consisted of both a functional subsystem (Stage 1) and
a malfunctioning one (Stage 2), making it possible for
students to use the Split-Half Strategy. The output of
the malfunctioning circuit is shown in Plot (c) of Fig. 3.
Additional characteristics of the malfunctioning circuit
are provided in Table II.

If students were to replace the 100 Ω resistor with a
1 kΩ resistor but leave the faulty op-amp in the circuit,
the output of the circuit would remain unchanged. On
the other hand, if student were to replace the faulty op-
amp with a functioning chip but leave the 100 Ω resistor
in the circuit, the circuit would eﬀectively be a function-
ing inverting cascade ampliﬁer with an overall gain of
−200. In this case, the output of the circuit could po-
tentially be a clipped signal. A“clipped signal” refers to
a sinusoid with ﬂattened peaks, as shown in Plot (d) of
Fig. 3. The ﬂattening is due to limitations of the op-
amp, which cannot output voltages larger than about
13 V in magnitude. A gain of −200 would result in clip-
ping for any ac input signals with amplitude larger than
only about 6 mV. Even in a functioning circuit, clipping
may arise for input ac signals with amplitude larger than
about 650 mV.

In the malfunctioning state, the op-amp in Stage 2
did not obey the Golden Rule for op-amps. There was a
nonzero dc voltage diﬀerence between the input terminals
of the faulty op-amp, as indicated in Table II.

To troubleshoot the circuit, student pairs had access
to test and measurement equipment that was typical of
the equipment used in their Electronics Course. All stu-
dents had access to an oscilloscope, digital multimeter,
low-voltage dc power supply, ac function generator, pli-
ers, wire strippers, various types of cables, and extra re-
sistors, op-amps, and wire. CU students used a bread-
board that needed to be connected to external voltage
sources. UM students, on the other hand, used a com-
mercial prototyping board that had on-board ac and dc
voltage sources.
In both cases, the malfunctioning cir-
cuit was pre-built on the board by the research team. A
photograph of the setup used at CU is provided in Fig. 4.
We designed the think-aloud troubleshooting task
to closely mimic the types of troubleshooting events
that students typically encounter during the Electron-
ics Course. After students completed the task, we asked
them to comment on the extent to which the task felt
like a typical Electronics Course activity to gain insight

9

One diﬀerence from class is that you’re work-
ing with a circuit someone else built. Another
diﬀerence is that I’m interested in what you
say to yourself as you perform this task, so I
will ask you to talk aloud as you work on the
circuit.

What I mean by talk aloud is that I want you
to say out loud everything that comes into
your mind while doing the task. Put another
way, I want you say out loud what you might
otherwise say to yourself silently. Of course,
you should also feel free to ask each other
questions and interact as you would when
working together in [the Electronics Course].
But the more you both say out loud what
you’re thinking in your head, the more help-
ful it will be.

Act as if I am not in the room. Just keep
talking.
If you are silent for any length of
time, I will remind you to keep talking aloud.

After reading the prompt, the interviewer asked the
students to begin working on the task. During this time,
the interviewer interacted only minimally with the stu-
dents. The activity ended when either the students re-
paired the circuit or an hour had passed. After the activ-
ity was over, the interviewer asked students a few short
follow-up questions, including a question about the ex-
tent to which the activity felt typical of students’ expe-
riences in their Electronics Course.

The interviewers’ prompts and follow-up questions ac-
counted for only 5–10 minutes per activity.
In six in-
terviews, students spent 40–45 min troubleshooting the
circuit.
In the other two interviews, students repaired
the circuit in 20–25 min. Together, all eight TAPPS in-
terviews lasted a total of six hours, with about ﬁve hours
devoted to troubleshooting the circuit. Video and au-
dio data were collected for all interviews, and audio data
were fully transcribed.

C. Data analysis

Our approach to data analysis involved two parts.
First, we used the cognitive troubleshooting tasks to code
successive two-minute intervals that spanned the dura-
tion of each of the interviews. Second, we used the Mod-
eling Framework to code two types of events present in
most interviews: (1) isolation of the second stage as the
source of faults, and (2) evaluation of the circuit after
replacing the faulty op-amp.

Both the cognitive troubleshooting tasks and the Mod-
eling Framework were used as a priori analysis schemes.
For each scheme, we initially developed operational code
deﬁnitions based on global deﬁnitions from the trou-
bleshooting and modeling literature [5, 14] and a re-
view of the content logs. Operational deﬁnitions were
reﬁned through iterative cycles of collaborative coding

FIG. 4. Photograph of the setup for the TAPPS interview.

into the ecological validity of the task [48]. Student re-
sponses indicated that the think-aloud activity was sim-
ilar to their typical course experiences along several di-
mensions, including the components used, the equipment
used, the faults they encountered, and the processes they
used to troubleshoot the circuit. We conclude that the
activity was similar to students’ in-class experiences.

B. Data collection

We conducted TAPPS interviews in which pairs of stu-
dents were tasked with diagnosing and repairing the mal-
functioning inverting cascade ampliﬁer shown in Fig. 3.
At the start of each task, the interviewer provided stu-
dents with the following materials: a schematic diagram
of the circuit, a datasheet for the op-amp, and a pre-built
malfunctioning circuit. The circuit schematic and corre-
sponding text are shown in Fig. 5. No expressions for the
gains of either of the two individual stages were provided
to the students.

The interviewer read a short prompt to the students
before they began the task. The prompt framed the ac-
tivity as follows:

For this activity, you will be repairing a mal-
functioning circuit.
Speciﬁcally, you’ll be
working with an inverting cascade ampliﬁer,
described on this page here [Fig. 5]. For con-
text, let’s imagine that some of your peers
built this circuit as part of class. They built
the circuit using the same chip you’ve been
using in class this semester. Here’s the stan-
dard data sheet for that chip. Your tasks are
to diagnose any issues and make the circuit
work properly.

This interview is very similar to what you’ve
been doing in class. You’ll have access to
much of the equipment from class,
includ-
ing power supplies, measurement tools, and
a limited selection of electrical components.

DC power supplyOscilloscopeMalfunctioning circuitFunction generator10

articulating which chip corresponds to which stage,
which resistors correspond to R1–R4, which pins
are input and output, and/or where the power and
ground connections are located.

• Discern functions of systems, components: Stu-
dents do at least one of the following:
identify
the circuit or one of its subsystems as an invert-
ing or noninverting ampliﬁer; discuss the function
of a component (e.g., “this is a feedback resistor”);
or rationalize the absence of capacitors (e.g., “ca-
pacitors are just needed to clean up high-frequency
noise from the signal”).

• Perform formative measurements: Students per-
form initial checks of the circuit conﬁguration, re-
sistor values, pin voltages, or the performance of
the test and measurement equipment. These mea-
surements are typically accompanied by statements
like, “I’m just trying to ﬁgure out what’s going on.”

According to our subcode deﬁnitions, a measurement
of, say, voltage could be an example of either Formu-
late Problem Description, Test, or Repair and Evaluate
depending on whether it was performed in a formative,
diagnostic, or evaluative capacity. For example, an initial
check that the op-amps are powered would be an example
of a formative measurement. On the other hand, mea-
suring the midpoint voltage as part of a Split-Half Strat-
egy would constitute a diagnostic measurement. Finally,
checking the output signal after replacing the op-amp in
Stage 2 would be an evaluative measurement.

Although Schaafstal et al. [14] include setup and oper-
ation of test and measurement equipment in their global
deﬁnition of “performing the test”, we did not include
these actions in our ﬁnal deﬁnition of Test.
In our
dataset, students were adjusting settings on the oscillo-
scope, multimeter, power supply, and/or function genera-
tor throughout the activity, which eﬀectively contributed
a “constant background” of this aspect of testing to our
analysis.

2. Modeling Framework

To characterize students’ model-based reasoning dur-
ing the troubleshooting process, we developed codes
based on the Physical System half of the Modeling
Framework. We applied these codes to two types of
events: (1) isolation of the second stage as the source
of faults, and (2) evaluation of the circuit after replacing
the faulty op-amp. For both types of events, we used
the Modeling Framework codes to perform line-by-line
analyses of the corresponding transcribed student dia-
logue. A detailed example of this approach is provided
elsewhere [34].

In total, we identiﬁed 12 excerpts (5 isolation-type
events and 7 evaluation-type events) which lasted about
2–3 minutes each. Excerpts for isolation-type events

FIG. 5.
Schematic diagram of inverting cascade ampliﬁer,
provided to study participants during the TAPPS activity.
The diagram was accompanied by the following text: “Figure
1 is a schematic of the inverting cascade ampliﬁer we built.
The [output] of this circuit is [given by Eq. (1)]. The main
advantage of a cascade ampliﬁer over a regular ampliﬁer is
that we can achieve high gain while maintaining a relatively
large bandwidth. Disadvantages of this circuit compared to
the regular ampliﬁer include more components and increased
power consumption.”

and discussions with the research team. Whereas codes
related to the Cognitive Troubleshooting Tasks were ap-
plied to a total of 5 hours of troubleshooting activity
across all 8 interviews, the Modeling Framework codes
were applied only to a total of about 30 minutes across
all eight TAPPS interviews. Below, we describe the cod-
ing schemes in greater detail.

1. Cognitive troubleshooting tasks

To characterize students’ approach to troubleshooting
the malfunctioning cascade ampliﬁer, we developed codes
corresponding to the four cognitive troubleshooting tasks
described in Sec. II A 2: Formulate Problem Description,
Generate Causes, Test, and Repair and Evaluate. Each
code was associated with three or four subcodes that
were generated through an emergent and iterative pro-
cess, starting with a review of the audiovisual recordings.
The subcodes are listed in Table III. To apply the sub-
codes to our data, we divided each video into successive
two-minute intervals. We collaboratively coded each in-
terval through three iterations of coding, discussion, and
reﬁnement of subcode deﬁnitions and applications. De-
pending on the nature of student activity during a given
time interval, we assigned no subcodes, one subcode, or
multiple subcodes to the interval.

As an example of our coding scheme, the Formulate
Problem Description subcodes (italicized font) and their
operational deﬁnitions (normal font) were:

• Map circuit onto schematic and/or datasheet: Stu-
dents orient themselves to the circuit topographi-
cally by mapping the circuit onto the schematic or
datasheet. This typically involves looking back and
forth between the circuit, schematic, and datasheet,

LF356LF356VINVOUTR4 = 10 kR3 = 1 kR2 = 460 R1 = 460 TABLE III. Codes and subcodes for cognitive troubleshoot-
ing tasks.

Code
Formulate
Problem
Description

Generate
Causes

Test

Repair and
Evaluate

Subcode
Map circuit onto schematic and/or datasheet
Discern functions of systems, components
Perform formative measurements
Brainstorm potential causes or strategies
Isolate subsystems as (mal)functioning
General discussion about causes or strategies
Make a plan or prioritize measurements
Formulate expectations about measurements
Perform diagnostic measurements
Propose a potential solution
Replace component(s)
Change circuit conﬁguration
Perform evaluative measurements

started when one or both students suggested measur-
ing the output of Stage 1 and ended when the students
concluded that Stage 1 was functioning as expected and
the faults could therefore be isolated in Stage 2. Ex-
cerpts for evaluation-type events started just after the
students replaced the faulty op-amp and ended once the
students concluded that the circuit as a whole had been
repaired and was now behaving as expected. Isolation-
type events, when they occurred, took place between half
and two-thirds of the way though the think-aloud activ-
ity. Evaluation-type events, on the other hand, spanned
the last three minutes of the activity.

The Modeling Framework codes (bolded font) and

their operational deﬁnitions (normal font) were:

• Model Construction (Physical System): Stu-
dents do any of the following: model the circuit as
being comprised of two abstract subsystems, each
with its own gain; identify relevant principles and
concepts from electronics, such as the Golden Rule
for op-amps in a closed loop; or identify limitations
of the transfer function, such as the gain-bandwidth
product or voltage-related limits on the amplitude
of the output signal.

• Prediction: Students compute expected outputs,
such as: the phase, amplitude, or frequency of the
signal at various points in the circuit; or the gain of
a subsystem or the system as a whole. This includes
articulating expectations about what would hap-
pen if a component had a diﬀerent value (e.g., they
compute the gain of a hypothetical circuit with,
say, R4 = 1 kΩ).

• Comparison: Students compare expected and
measured values of amplitude, phase, or frequency
of a signal (e.g., “We see 100 mV, but it should
be 4 V.”). This includes making relational state-
ments about the size of a signal (e.g., “This signal
is too small.”) and making evaluative judgements

11

about the observed signal (e.g., “This is strange,”
or “This isn’t what it’s supposed to do.”).

• Proposal: Students suggest explanations for a dis-
crepancy between measurement and prediction. Al-
ternatively, students suggest solutions for bringing
the actual performance of the circuit into alignment
with expectations.

• Revision (Physical System): Students change
the circuit conﬁguration, replace a resistor, or re-
place an op-amp in response to a discrepancy be-
tween measurement and prediction.

The deﬁnition of Prediction does not require that stu-
dents’ computations or expectations are correct. Neither
does the deﬁnition require that computations or expecta-
tions are carried out or articulated before a measurement
is performed. Indeed, in our dataset, many instances of
Prediction occur after a measurement takes place in an
eﬀort to determine whether or not the measured value
“makes sense.”

While Zwickl et al. [5] include “identify parameters” as
part of constructing models, we did not address this as-
pect in our ﬁnal deﬁnition of Model Construction. In the
episodes we chose to analyze with the Modeling Frame-
work, there were no instances of students identifying pa-
rameters in service of constructing a model of the circuit.

V. RESULTS

We describe the troubleshooting processes of eight
pairs of students. Since we are not performing a compar-
ative analysis, we do not distinguish between students at
CU and those at UM. Pairs of students are labeled A–
H and individual students are labeled according to their
pair membership. For example, the students in Pair A
are labeled A1 and A2. When providing examples of stu-
dents’ verbalizations, we indicate the speaker as well as
the time interval during which they were speaking.

In the following subsections, we describe the students’
troubleshooting process and the changes they made to
the circuit. Using results from two coding schemes (de-
tailed in Secs. IV C 1 and IV C 2), we show that each pair
of students engaged in all of the cognitive troubleshooting
tasks and demonstrated model-based reasoning during
strategic and/or evaluative stages of the troubleshooting
process.

A. Cognitive tasks

The results of coding for the four cognitive trou-
bleshooting tasks are shown in Fig. 6. Formulate Prob-
lem Description, Generate Causes, Test, and Repair and
Evaluate are represented as orange, blue, green, and yel-
low bands, respectively. Based on these codes, several
patterns can be discerned. For example, all eight pairs

12

FIG. 6. Cognitive troubleshooting task coding results. Codes were applied to discrete two-minute time intervals. Colored
bands indicate the times during which each pair engaged in the following tasks: Formulate Problem Description (FRM,
orange); Generate Causes (GEN, blue); Test (TST, green); and Repair and Evaluate (REP, yellow). Filled black triangles and
stars in the yellow REP bands indicate times at which students replaced the 100 Ω resistor and faulty op-amp, respectively.
Filled black circles in the blue GEN bands indicate times at which students employed the Split-Half Strategy. There are no
codes in the ﬁrst 2 minutes of the activity because the interviewer was giving verbal instructions during these times.

engaged in all four cognitive troubleshooting tasks. Most
pairs transitioned from formulating the problem descrip-
tion to generating causes about halfway through the ac-
tivity, though the transition isn’t always clear cut. Test-
ing happened almost continuously throughout the du-
ration of activity, whereas repairs and evaluations were
performed more sporadically.

The ﬁlled black triangles and stars in Fig. 6 correspond
to times when students replaced the 100 Ω resistor and
the faulty op-amp, respectively. These symbols reveal
additional patterns. For example, all pairs replaced the
resistor and all-but-one replaced the op-amp.
In each
case, the resistor was replaced before the op-amp was re-
placed. Indeed, most pairs replaced the resistor early in
the troubleshooting process, while they were still formu-
lating the problem description and before they started
generating causes. In addition to replacing the 100 Ω re-
sistor and/or the faulty op-amp, many pairs performed
additional, unnecessary revisions to the circuit. In Fig. 6,
such revisions sometimes manifest as yellow blocks which

contain neither a triangle nor a star.

The ﬁlled black circles in Fig. 6 indicate the times when
students employed the Split-Half Strategy. Pairs D–H
employed this strategy at some point during the second
half of the activity, often within a few minutes after the
onset of generating causes. Pairs who used the Split-Half
Strategy did not repair the circuit signiﬁcantly faster or
slower than those who did not. For example, Pairs A
and E repaired the circuit about twice as quickly as other
groups, but only E employed the Split-Half Strategy.

Below, we elaborate on the results from our cognitive

task coding scheme.

1. Formulate Problem Description

Formulate Problem Description refers to the early
stage of the troubleshooting process, during which stu-
dents oriented themselves to the activity and performed
formative measurements to determine what was work-

01020304050Time (minutes)FRMGENTSTREP(B)FRMGENTSTREP(C)FRMGENTSTREP(A)FRMGENTSTREP(F)FRMGENTSTREP(E)FRMGENTSTREP(D)01020304050Time (minutes)FRMGENTSTREP(G)FRMGENTSTREP(H)Repair and Evaluate (REP)Test (TST)Generate Causes (GEN)Formulate Problem Description (FRM)Use Split-Half StrategyReplace faulty op-ampReplace R3Legending and what was not. Students engaged in this task
throughout the ﬁrst half of the activity. We report ver-
balizations that reﬂect three diﬀerent aspects of problem
formulation: (1) mapping the circuit to the schematic,
(2) discerning the intended function of the circuit, and
(3) performing initial measurements and inspections to
check various aspects of the circuit.

All eight pairs mapped the schematic to the circuit
almost immediately after the interviewer ﬁnished giving
instructions. For example, as soon as the activity started,
Student B1 said:

“So, we should identify which one [stage] is
which to start with. . . . This is a noninverting
[stage] to start with. This part only. And the
next one is an inverting [stage].”

(B1; 2:35–3:09)

Here B1 correctly mapped the components in the circuit
to their symbolic representations in the schematic.
In
addition, B1 simultaneously recognized the existence of
noninverting and inverting stages, an important part of
discerning the intended function of the circuit and its
subsystems.

Seven pairs (B–H) identiﬁed the existence of the two
stages and discerned their intended functions. For exam-
ple, soon after examining the schematic for the ﬁrst time,
Student E1 said:

“That makes sense, just like inverting and
non-inverting smashed together.”

(E1; 2:45–2:49)

Thus, E1 parsed the circuit as comprising two subsys-
tems early in the activity, before performing any mea-
surements. Pairs B and C did the same. Pairs D and
F–H, on the other hand, discussed the intended function
of the circuit after they had begun performing measure-
ments and generating hypotheses about potential faults.
In these cases, discerning the function of subsystems was
a crucial part of understanding the results of diagnostic
measurements. For example, after measuring the output
of Stage 1, Student F2 said:

“And it [the output of Stage 1] shouldn’t be
inver–, should be . . . An inverting ampliﬁer
is connected to the, VIN is connected to the
negative terminal, right? . . . So it [the output
of Stage 1] shouldn’t be inverted.
. . . The
second one [stage] is inverting.”

(F2; 24:52–25:27)

13

and voltages. All eight pairs began checking the cir-
cuit conﬁguration within a few minutes of receiving in-
structions. For example, after mapping the circuit to the
schematic, Student A1 said:

“Let’s see if everything’s connected right ﬁrst
oﬀ.”

(A2; 2:50–2:53)

Here A1 suggested checking the circuit conﬁguration as
one of the ﬁrst steps in the troubleshooting process. Six
pairs (B–E, G, and H) also measured the resistances of all
four resistors during this phase, leading them to identify
and replace the 100 Ω resistor early in the activity. Stu-
dents employed the Topographic Strategy when checking
circuit conﬁguration and resistor values, starting from
the input and tracing through the circuit to the output,
or vice versa. For example, when deciding to measure
the resistor values, Student G1 said:

“Check all the resistor values and then make
sure they’re all connected.
. . . You want to
just start at the bottom [input] and go to the
top [output]?”

(G1; 2:32–2:50)

Here G1 suggested checking the resistors in order of
where they occur along the path from the input to the
output of the circuit. Each pair also performed forma-
tive voltage measurements to ensure that the circuit was
properly powered and grounded. In all cases, measure-
ment of the output signal triggered students to begin
generating causes and testing the circuit.

2. Generate Causes

Generate Causes involves making hypotheses about
potential faults. Students generated causes throughout
the second half of the activity, starting after measuring
the output signal for the ﬁrst time. Students proposed
many diﬀerent potential explanations for the malfunc-
tioning behavior of the circuit, including short circuits,
saturation, and faulty chips. We provide three examples
of proposals: one that was dismissed, one that resulted in
a revision to the circuit, and a third proposal that gave
rise to diagnostic measurements.

The following explanation, proposed by Student C1, is

an example of a hypothesis that was dismissed:

“It’s [the circuit is] probably saturated.”

(C1; 25:44–25:48)

Here F2 discerned the function of the subsystems in re-
sponse to a diagnostic measurement. In Fig. 6, this and
similar occurrences in Pairs D, G, and H are depicted
by instances of Formulate Problem Description (orange
bands) that occur after the onset of Generating Causes
(blue bands).

To formulate the problem description, students per-
formed formative checks of conﬁguration, resistances,

Here, C1 suggested that the observed dc output signal
was caused by output limitations of the op-amps; when
the expected output voltage exceeds the limitations of
the op-amp, the circuit is sometimes referred to as being
“saturated.” This idea was immediately dismissed by C2,
who noted that the output was a dc signal rather than
the clipped ac signal characteristic of saturated ampliﬁer
circuits, as in Plot (d) of Fig. 3.

14

Not all hypotheses were dismissed. For example, dur-
ing a brainstorming session, Student F1 simultaneously
proposed an explanation and a revision:

“Yeah, for sure. And then we’ll measure all
the power, too, and make sure it’s doing what
it should be doing.”

“Could the op-amps be faulty? Should we
just replace them with new ones? For the
second op-amp, let’s just replace it with a new
one.”

(F1; 40:18–40:26)

Here F1 suggested that faulty op-amps might be the
cause of the dc output signal, and then immediately pro-
posed replacing the op-amp in Stage 2 with a new chip.
F1 and F2 went on to replace the op-amp in Stage 2 as
suggested.

Many proposed causes informed follow-up diagnostic
measurements. For instance, upon seeing a dc output
signal equal to the negative supply voltage, Student B1
said:

“Maybe this red one [wire], the power is some-
how touching the output.”

(B1; 27:57–28:03)

Here, B1 suggested there might be a short circuit con-
necting the negative dc supply voltage and the output of
the circuit. After making this suggestion, B1 went on to
perform a diagnostic visual inspection of the circuit for
such a short.

3. Test

Test involves performing diagnostic measurements to
determine whether or not a proposed fault is an actual
fault. Test further includes prioritizing measurements,
making plans, and making predictions about expected
outcomes. Students engaged in testing throughout the
duration of the think-aloud activity, focusing on prioritiz-
ing, planning, and predicting during the ﬁrst half of the
activity. During the second half of the activity, students
began performing diagnostic measurements to check pro-
posed causes. We report verbalizations that reﬂect the
planning, prioritizing, and predicting aspects of testing.
The following exchange is an example of students pri-
oritizing measurements. Immediately after observing the
erroneous dc output of the circuit for the ﬁrst time,
Pair E discussed their plans for diagnostic measurements.
Student E1 outlined the following plan:

“What we could do is get out a probe and
we can just go through the ﬁrst one [stage]
and measure VOUT, and we could see if that’s
what we expect it to be.”

(E1; 15:41–15:52)

Here, E1 suggested performing diagnostic voltage mea-
surements at various points in the ﬁrst stage as well as of
the output. E2 agreed with this plan, and suggested per-
forming diagnostic measurements of the supply voltages
as a follow-up:

(E2; 15:52–16:00)

In this exchange, making measurements of Stage 1 was
prioritized over checking supply voltages.

Near the end of their troubleshooting process, Stu-
dents H1 and H2 came up with the following plan to
determine whether the op-amp in Stage 2 was faulty:

“It seems like it [the cause of the malfunc-
tioning behavior] might be that second op-
amp. So let’s just put the input [signal] to
the—let’s pull—yeah, let’s just hook the in-
put [signal] to the 1k [resistor] and look at
the output [of Stage 2].
. . . We should get
inverted times ten output.”

(H2; 42:15–43:03)

Here H2 recommended connecting the input signal, VIN,
to the input of Stage 2, thus bypassing Stage 1 altogether.
This plan, which the students carried out, allowed the
students to test Stage 2 as an isolated system. In addition
to making a plan, H2 also made a new prediction; H2
correctly predicated that the second stage, if functioning
properly, would invert the input signal and amplify it by
a factor of 10.

Through testing, students were able to determine that
some of their proposed causes were indeed actual faults in
the circuit. For example, after isolating the second stage
to test the performance of the second op-amp, Pair H
concluded that the second op-amp was indeed faulty and
in need of replacement. Testing thus paved the way for
repairs to the circuit.

4. Repair and Evaluate

Repair and evaluation involves proposing, enacting,
and evaluating revisions to the circuit. Repairs and eval-
uation typically happened in short bursts (Fig. 6). Here
we focus on three aspects of repairing and evaluating the
circuit: replacing the 100 Ω resistor, making erroneous
revisions to the circuit, and replacing the faulty op-amp.
All eight pairs correctly identiﬁed the 100 Ω resistor as
a fault and replaced it with a 1 kΩ resistor. Six pairs (B–
E, G, and H) identiﬁed the resistor early in the activity,
while checking that the circuit was constructed properly.
In these cases, there could be no evaluative measurement
of the revised circuit’s performance because the students
hadn’t yet observed the output signal. Pairs A and F, on
the other hand, identiﬁed the resistor as part of the test-
ing process. Both of these pairs performed a quick eval-
uative measurement of the output voltage to determine
whether their revision repaired the circuit. For example,
after replacing the 100 Ω resistor with a 1 kΩ resistor,
Student F2 measured the output signal and said:

“That resistor was wrong.
circuit is] still not working.”

. . . But it’s [the

(F2; 30:26–30:32)

F2’s statement that the circuit was “still not working”
frames the measurement of the output signal as an eval-
uative measurement.

Five pairs made unnecessary or erroneous changes to
the circuit. There were three types of erroneous revisions.
First, Pair A replaced R1, nominally 460 Ω, with a resis-
tor whose measured resistance was closer to the nominal
value than the original resistor. Second, Pairs B, D, and
H changed the circuit conﬁguration. In each case, these
changes were due to incorrect mapping of the circuit to
the schematic or datasheet and the students eventually
realized their mistake and restored the original circuit
conﬁguration. Finally, Pairs A–C replaced the op-amp
in Stage 1 with a new chip.

For Pairs A and B, the decision to replace the ﬁrst op-
amp was based on an Exhaustive Strategy in which both
op-amps were replaced simultaneously. For example, af-
ter Pair A had checked the circuit conﬁguration, mea-
sured the resistor values, and replaced R3, Student A2
made the following suggestion:

“Want to replace the op-amps? . . . We’ve
gone over all the wiring twice at this point.
We’ve gone over each individual resistor. So,
do you have spare op-amps here?”

(A2; 19:46–20:07)

A2 reasoned that, since the wiring and resistors had pre-
viously been checked, the next step was to replace the op-
amps—the only remaining type of component that had
not yet been tested. The suggestion to replace the op-
amps was the result of having exhausted other potential
causes for the malfunctioning behavior of the circuit.

Pair C, on the other hand, replaced the ﬁrst op-amp in-
dividually. After observing noisy voltage measurements
at various points along the circuit, C2 oﬀered the follow-
ing explanation:

“You know, [another student] and I had this
problem the other day and the problem was
the op-amp was just bad. . . . We popped in
a new op-amp and it ﬁxed it.”

(C2; 26:41–26:49)

C1 responded by suggesting that they “swap out that
ﬁrst op-amp.” Here Pair C relied on Experiential Knowl-
edge to propose a solution to the noisy voltage measure-
ments: C2 had previously encountered noise in an op-
amp circuit, the cause of the noise was a “bad” op-amp,
and the solution path was to replace the op-amp with a
new chip.

Seven pairs (A–C and E–H) correctly identiﬁed the
op-amp in Stage 2 as a fault and replaced it with a func-
tional chip, successfully repairing the malfunctioning cas-
cade ampliﬁer. Each of these successful pairs replaced
the resistor before replacing the op-amp. Upon repair-
ing the circuit, some pairs performed cursory evaluative

15

TABLE IV. Number and percent of groups who engaged in
a particular aspect of the Modeling Framework in each of
two troubleshooting episodes: isolating Stage 2 as the source
of faults, and replacing the faulty op-amp. Percents were
computed relative to the total number of groups who engaged
in the corresponding troubleshooting episode.

Isolate Stage 2

Replace Op-Amp

(N = 5)

(N = 7)

Code Category
Model Construction
Prediction
Comparison
Proposal
Revision

5
5
5
2
0

Number Percent Number Percent
57%
57%
100%
100%
100%

100%
100%
100%
40%
0%

4
4
7
7
7

measurements while others adopted a more extensive ap-
proach to evaluation. We used the Modeling Framework
to gain insight into students’ evaluation of the repaired
circuit, as described below.

B. Modeling

All eight pairs engaged in model-based reasoning dur-
ing either employment of the Split-Half Strategy to iso-
late Stage 2 as the source of faults (Pairs D–H) and/or
evaluation of the circuit’s performance upon replacement
of the op-amp in Stage 2 (Pairs A–C and E–H). A sum-
mary of students’ model-based reasoning during these
episodes is given in Table IV. While students engaged
in a variety of Modeling phases (Sec. IV C 2) during one
or both of these episodes, there are nevertheless diﬀer-
ences in the nature of students’ model-based reasoning
in each case. For example, Model Construction and Pre-
diction were more common—and Proposal and Revision
were less common—during isolation of Stage 2 as a fault
source than during replacement of the faulty op-amp.

1.

Isolating Stage 2 as the fault source

Five pairs (D–H) employed the Split-Half Strategy to
isolate Stage 2 as the source of faults. All ﬁve pairs
made explicit statements in which they correctly iden-
tiﬁed Stage 1 as functional and/or isolated Stage 2 as
the source of faults. For example, D2 concluded that
“the ﬁrst one [Stage 1] is giving us a good voltage” and
F1 said, “The problem is in the second one [Stage 2].”

All ﬁve pairs engaged in Model Construction, Predic-
tion, and Comparison (Table IV). In all cases, Model
Construction involved recognizing that the cascade am-
pliﬁer consisted of two distinct stages, each character-
ized by a unique gain. Because Model Construction in-
cluded recognizing that the overall gain of the circuit was
the product of the gains of two stages, Model Construc-
tion was intertwined with Prediction. For example, after

Students D1 and D2 decided to measure the output of
Stage 1, D2 said:

“And this [Stage 1] is noninverting and then
[Stage 2 is] inverting. Oh yeah, that seems
right. Because VOUT equals negative this one
[R4] over that one [R3]. Which is the second
one [Stage 2]. So second one [Stage 2] should
be negative ten [kΩ] over one [kΩ times] VIN.
Which would be, which would be ten.”

(D2; 34:59–35:31)

D2 went on to clarify that in “VIN” in this instance meant
“VOUT of the ﬁrst one,” i.e., the output of Stage 1. In
this utterance, D2 ﬁrst identiﬁed the existence of the two
stages: “this [Stage 1] is noninverting and then [Stage 2
is] inverting.” D2 then identiﬁed and computed the gain
of Stage 2: “VOUT equals negative this one [R4] over that
one [R3] . . . which would be ten.” Thus, Model Construc-
tion and Prediction were occurring nearly simultaneously.
Similarly, Pair E engaged in Model Construction, Pre-
diction, and Comparison in short succession. After mea-
suring the output of Stage 1, Student E1 said:

“It looks like it’s [Stage 1 is] doubling [the
input signal]. So that seems right because
the gain [of Stage 1] is one plus R2 over R1,
which is one plus one. Two.”

(E1; 19:36–19:47)

E1 began by articulating the result of their measurement:
the output of Stage 1 has twice the amplitude of the in-
put to Stage 1 (“It looks like it’s [Stage 1 is] doubling”).
E1 then suggested that this measurement was in good
agreement with the prediction (“that seems right”) be-
cause the expected gain of Stage 1 was two. While Pair E
identiﬁed the existence of noninverting and inverting sub-
systems early on in the troubleshooting process, they did
not identify algebraic expressions for the gains of either
subsystem until this utterance. Thus the statement “the
gain is one plus R2 over R1” is an example of Model Con-
struction, which was intertwined with both Comparison
and Prediction.

Two pairs (D and F) engaged in Proposal. In addition
to successfully eliminating Stage 1 as a potential source of
faults, these pairs oﬀered potential explanations for the
discrepancy between expected and actual performance of
the cascade ampliﬁer: D2 suggested that the 1 kΩ and
10 kΩ resistors had accidentally been switched, and F1
suggested that the inputs to the op-amp in Stage 2 were
incorrectly wired. While neither of these examples of
Proposal yielded correct explanations for the observed
discrepancies, both focused on the existence of potential
faults in Stage 2.

2. Replacing the faulty op-amp

16

describe how these pairs engaged in all ﬁve aspects of
the Modeling Framework listed in Table IV.

Each of the seven pairs engaged in both Proposal and
Revision: the students ﬁrst proposed that the op-amp in
Stage 2 was faulty and/or needed to be replaced, and
then the students revised the circuit by replacing the
faulty op-amp with a functional chip. For example, while
brainstorming potential faults, Student F1 said:

“Could the op-amps be faulty? Should we
just replace them with new ones? ’Cause the
second op amp—let’s just replace it [the op-
amp in Stage 2] with a new one.
. . . ’Cause
the ﬁrst one [Stage 1] is functioning ﬁne.”

(F1; 40:18–40:38)

F1 initially proposed that faulty op-amps could be the
cause of the malfunction, and suggested replacing them
both with new chips. F1 then revised this proposal
based on the results of earlier measurements, which re-
vealed that the op-amp in Stage 1 was “functioning ﬁne.”
F1’s revised proposal was to replace just the op-amp in
Stage 2. Here, F1 used the results of the Split-Half Strat-
egy to inform a proposed revision to the circuit.

Pair C used slightly diﬀerent reasoning when proposing
that the op-amp in Stage 2 was faulty. After performing
diagnostic measurements of the voltage at several points
in Stage 2, Student C2 said:

“Pin 3 [the noninverting input terminal of the
op-amp in Stage 2] is in fact zero. However,
pin 2 [V2−, the inverting input terminal of the
op-amp in Stage 2] is not zero. And that’s a
problem.
. . . Certainly the second one [the
op-amp in Stage 2] is not working because the
Golden Rules are not being followed here.”

(C2; 39:15–39:34)

Here, C2 relied on their Domain Knowledge (i.e., their
knowledge of the Golden Rule for op-amps in a closed
loop) to identify the op-amp in Stage 2 as faulty.

Each of the seven pairs also engaged in Comparison:
after replacing the faulty op-amp, the students performed
evaluative measurements during which they compared
the actual performance of the repaired circuit to their
expectations. Four pairs (C and E–G) also engaged in
Prediction by explicitly stating their expectations during
the evaluation process.

The depth of evaluation varied among pairs of stu-
dents. For example, after replacing both op-amps, Pair B
performed evaluative measurements of the amplitude and
phase of the output of Stage 2. Upon performing these
measurements, Student B1 said:

“That was it. Nice. And now the output is 10
volts, maximum. And it’s [the output signal
is] inverted, which is good.”

(B1; 46:35–46:46)

Seven pairs (A–C and E–H) successfully repaired the
circuit by replacing the op-amp in Stage 2. Here, we

B1 concluded that that the op-amps were indeed a fault
source (“That was it.”). B1 further articulated that the

measured amplitude (“10 volts”) and phase (“inverted”)
of the output signal were “good,” indicating satisfactory
agreement with the expected amplitude and phase.

In addition to verifying the amplitude and phase of
their output signal, Pair G also checked to see that the
new chip was satisfying the Golden Rule for op-amps in
a closed loop:

“We should check the negative terminal [V2−]
here and it should be zero volts.
[It is]
point nine millivolts. So, that’s basically zero.
Okay. So, everything’s behaving as it should
now.”

. . .

(G1; 32:22–33:02)

Before measuring V2− of the new op-amp in Stage 2, G1
predicted that the voltage should be zero volts. G1 deter-
mined that the measured value, 0.9 mV, was “basically
zero” and that the circuit had been repaired (“every-
thing’s behaving as it should now.”), indicating that the
expected and actual performance of the circuit were in
agreement.

Whereas Pair G performed a more rigorous evaluation
than Pair B, Pair A performed a more cursory evaluation.
Like all other groups, Students A1 and A2 measured the
output of Stage 2 immediately after enacting their Revi-
sion. Because the amplitude of their ac input signal was
larger than 650 mV, the output of the repaired circuit
was a clipped ac signal, as shown in Plot (d) of Fig. 3.
After observing a clipped output signal, A2 remarked:

“That’s just ’cause it’s [the output signal is]
railing.
. . . That’s ﬁne. Now, is the signal
inverted? Yes, it is. Awesome. It works.”

(A2; 21:18–21:30)

A2 observed that the signal was being clipped (“it’s rail-
ing”), but did not reduce the amplitude of the input sig-
nal in order to produce a sinusoidal output signal. This
prevented Pair A from determining the actual gain of
the repaired circuit, which in turn prevented a quanti-
tative comparison of the predicted and measured gains.
Instead, A2 was satisﬁed (“That’s ﬁne.”) with basic qual-
itative features of the output signal: it was an ac signal
that was inverted with respect to the input. A2 con-
cluded that the circuit has been repaired (“It works.”).
Four pairs (A, C, E, and G) engaged in Model Con-
struction during the evaluation of the repaired circuit. In
all four cases, students articulated limitations of Eq. (1).
For example, after determining that their circuit had
been repaired, Pair E brieﬂy discussed whether the ob-
served behavior of the circuit made sense given the volt-
age limitations of the op-amps in the circuit:

“Let’s see, how do we get 20 volts [for the
output signal] when we only have 15 volts on
the inputs [op-amp supply voltage]?”

(E1; 28:22–28:35)

E1 wondered how it was possible for the repaired circuit
to provide an output of 20 V given that the positive and

17
negative power rails of the op-amps were only ±15 V. E2
noted that the output signal was 20 V from peak to peak,
so the output was “only going from minus 10 to plus 10.”
Thus, E1 and E2 engaged in Model Construction through
articulation of model limitations.

VI. DISCUSSION

Our results demonstrate that each pair of students in
our study engaged in all four cognitive troubleshooting
tasks and used model-based reasoning during key strate-
gic and/or evaluative phases of the troubleshooting ac-
tivity.

Not only did all pairs engage in all four cognitive trou-
bleshooting tasks, but the students were engaged in one
or more tasks during almost every two-minute time inter-
val throughout the activity. Moreover, the emergent pat-
terns of engagement give rise to a sensible troubleshoot-
ing narrative with a beginning, middle, and end.
In
the beginning of the activity, students got their bear-
ings on the problem by discerning the intended func-
tion of the circuit, performing visual inspections of the
conﬁguration, checking component values, replacing the
faulty resistor, and making plans for how to test the
circuit. Halfway through the activity, students began
proposing potential faults, performing diagnostic mea-
surements, and—in some cases—isolating the source of
faults to the second stage of the circuit. Finally, at the
end of the activity, almost all pairs identiﬁed the faulty
op-amp, replaced it with a new chip, and favorably eval-
uated the performance of the repaired circuit.

While the cognitive tasks give us a coarse picture
of students’ approach to troubleshooting a malfunction-
ing circuit, the Modeling Framework allows us to look
at two types of episodes in ﬁner detail. All ﬁve pairs
who employed the Split-Half Strategy engaged in model
construction, prediction, and comparison. The Model-
ing Framework allows us to tell a sub-narrative about
the Split-Half Strategy: the students ﬁrst constructed a
model of the circuit which consisted of two subsystems,
each with its own gain; they were then able to form expec-
tations about the outputs of the ﬁrst and second stages;
and, ﬁnally, by comparing their predictions to their mea-
surements, the students successfully identiﬁed Stage 1 as
functional and isolated Stage 2 as the source of faults.

Similarly, the Modeling Framework gives rise to a sub-
narrative about the students’ approach to evaluating the
repaired circuit. In this case, all seven pairs who repaired
the circuit engaged in proposal, revision, and compari-
son. The students ﬁrst proposed a revision to the phys-
ical system apparatus, namely, replacing one or both of
the op-amps with new chips; they then enacted this pro-
posed revision; and, ﬁnally, the students compared the
performance of the revised circuit to their expectations,
thus ensuring that the circuit had indeed been repaired.
In order to more thoroughly understand the performance
of the repaired circuit, four pairs engaged in model con-

struction by articulating constraints on the amplitude of
the output voltage and checking to see that those con-
straints were being satisﬁed. For these students, model
construction facilitated evaluation of their repairs.

The narratives above demonstrate the nonlinear, re-
cursive nature of modeling. During the testing phase of
the troubleshooting process, some students engaged in a
modeling cycle to isolate the second stage as the source
of faults. Later, another modeling cycle was needed to
repair and evaluate the circuit. Furthermore, each model-
ing cycle had its own particular signature: pairs predom-
inantly engaged in construction, prediction, and compar-
ison in the former case compared to proposal, revision,
and comparison in the latter. Thus, study participants
engaged in multiple, distinct iterations of model-based
reasoning while navigating the cognitive tasks required
to troubleshoot a malfunctioning electrical circuit.

Because our participant pool was both small and ho-
mogenous (e.g., most participants were white men and
both CU and UM are selective research-intensive insti-
tutions), our ﬁndings do not represent a comprehensive
picture of students’ approaches to troubleshooting elec-
tric circuits, nor do they necessarily speak to common
or typical student responses to a troubleshooting activ-
ity. Rather, our ﬁndings show that the process of trou-
bleshooting can engage students in the core scientiﬁc
practice of modeling.

VII. SUMMARY

We designed a think-aloud activity in which pairs of
students attempted to repair a malfunctioning electri-
cal circuit. The circuit was designed such that several

18

troubleshooting strategies could be employed. Audiovi-
sual data were collected for eight pairs of students from
two diﬀerent institutions. We used both a cognitive task
analysis of troubleshooting and the Modeling Framework
as a priori schemes to analyze the data. Two types of
episodes were chosen for in-depth analysis using the Mod-
eling Framework: (1) isolation of one subsystem of the
circuit as the source of faults, and (2) repair and evalua-
tion of the circuit.

We found that all eight pairs engaged in all four cog-
nitive troubleshooting tasks. Furthermore, in each of the
two episodes chosen for in-depth analysis, we found a
good mapping between students’ actions and the Model-
ing Framework. Thus, we have shown that model-based
reasoning facilitates the cognitive tasks required for trou-
bleshooting. We have also demonstrated that the process
of troubleshooting can engage students in the core scien-
tiﬁc practice of modeling.

In ongoing work [35], we are analyzing the data de-
scribed here using a framework for socially mediated
metacognition. Ultimately, we aim to use our under-
standing of the cognitive, metacognitive, and modeling-
oriented aspects of troubleshooting to inform the devel-
opment of activities to develop and assess students’ trou-
bleshooting abilities.

ACKNOWLEDGMENTS

The authors acknowledge the PER group at CU Boul-
der for useful input on study design and interpretation
of results. This material is based upon work supported
by the National Science Foundation under Grant Nos.
DUE-1323101, DUE-1323426, DUE-1245313, and DUE-
0962805.

[1] National Research Council, Discipline-Based Education
Research: Understanding and Improving Learning in Un-
dergraduate Science and Engineering, edited by Susan R.
Singer, Natalie R. Nielsen, and Heidi A. Schweingruber
(National Academies Press, 2012).

[2] Presidents Council of Advisors on Science and Tech-
nology, Engage to excel: Producing one million addi-
tional college graduates with degrees in science, technol-
ogy, engineering, and mathematics (Executive Oﬃce of
the President, 2012).

[3] AAPT Committee on Laboratories, AAPT Recommen-
dations for the Undergraduate Physics Laboratory Cur-
riculum (American Association of Physics Teachers,
2015).

[4] Carl Wieman, “Comparative cognitive task analyses
laboratory

of experimental science and instructional
courses,” The Physics Teacher 53, 349–351 (2015).

[5] Benjamin M. Zwickl, Dehui Hu, Noah Finkelstein, and
H. J. Lewandowski, “Model-based reasoning in the
physics laboratory: Framework and initial results,” Phys.
Rev. ST Phys. Educ. Res. 11, 020113 (2015).

[6] NGSS Lead States, “Next Generation Science Standards:

For States, By States,” (2013).

[7] Committee on Undergraduate Physics Education Re-
Implementation; Board on Physics, Astron-
search,
omy; Division on Engineering,
and Physical Sci-
ences; National Research Council, Adapting to a Chang-
ing World–Challenges and Opportunities in Undergradu-
ate Physics Education (The National Academies Press,
2013).

[8] Eugenia Etkina and Alan Van Heuvelen, “Investigative
science learning environment - a science process approach
to learning physics,” in Research-Based Reform of Uni-
versity Physics, Vol. 1 (American Association of Physics
Teachers, College Park, MD, 2007).

[9] Eric Brewe, “Modeling theory applied: Modeling in-
struction in introductory physics,” American Journal of
Physics 76, 1155–1160 (2008).

[10] Benjamin M. Zwickl, Noah Finkelstein, and Heather J.
Lewandowski, “Transforming the advanced lab: Part I -
Learning goals,” AIP Conference Proceedings 1413, 391–
394 (2012).

[11] Benjamin M. Zwickl, Noah Finkelstein, and Heather J.
Lewandowski, “The process of transforming an advanced
lab course: Goals, curriculum, and assessments,” Amer-
ican Journal of Physics 81, 63–70 (2013).

[12] H. J. Lewandowski and Noah Finkelstein, “Redesigning a
junior-level electronics course to support engagement in
scientiﬁc practices,” in Physics Education Research Con-
ference 2015 , PER Conference (College Park, MD, 2015)
pp. 191–194.

[13] Ray S. Perez, “A view from troubleshooting,” in Toward
a Uniﬁed Theory of Problem Solving: Views From the
Content Domains, edited by Mike U. Smith (Taylor &
Francis Group, New York, 1991) pp. 115–153.

[14] Alma Schaafstal, Jan Maarten Schraagen,

and Mar-
cel Van Berl, “Cognitive task analysis and innovation of
training: The case of structured troubleshooting,” Hu-
man Factors: The Journal of the Human Factors and
Ergonomics Society 42, 75–86 (2000).

[15] David H. Jonassen and Woei Hung, “Learning to trou-
bleshoot: A new theory-based design architecture,” Ed-
ucational Psychology Review 18, 77–114 (2006).

[16] Randall T. MacPherson, “Factors aﬀecting technologi-
cal trouble shooting skills,” Journal of Industrial Teacher
Education 35, 5–28 (1998).

[18] Tamara Van Gog,

Jeroen

[17] Scott D. Johnson, “Cognitive analysis of expert and
novice troubleshooting performance,” Performance Im-
provement Quarterly 1, 38–54 (1988).
Fred Paas,

J. G.
Van Merri¨enboer,
and Puk Witte, “Uncovering the
problem-solving process: Cued retrospective reporting
versus concurrent and retrospective reporting,” Journal
of Experimental Psychology: Applied 11, 237–244
(2005).

[19] Tamara Van Gog, Fred Paas,

and Jeroen J. G.
Van Merri¨enboer, “Uncovering expertise-related diﬀer-
ences in troubleshooting performance:
combining eye
movement and concurrent verbal protocol data,” Applied
Cognitive Psychology 19, 205–221 (2005).

[20] Tamara Van Gog, Fred Paas,

and Jeroen J. G. Van
Merri¨enboer, “Eﬀects of process-oriented worked exam-
ples on troubleshooting transfer performance,” Learning
and Instruction 16, 154–164 (2006).

[21] Tamara Van Gog, Fred Paas,

and Jeroen J. G. Van
Merri¨enboer, “Eﬀects of studying sequences of process-
oriented and product-oriented worked examples on trou-
bleshooting transfer eﬃciency,” Learning and Instruction
18, 211–222 (2008).

[22] Liesbeth Kester, Paul A. Kirschner,

and Jeroen J.
G. Van Merri¨enboer, “Information presentation and trou-
bleshooting in electrical circuits,” International Journal
of Science Education 26, 239–256 (2004).

[23] Liesbeth Kester, Paul A. Kirschner,

and Jeroen J.
G. Van Merri¨enboer, “Just-in-time information presenta-
tion: Improving learning a troubleshooting skill,” Con-
temporary Educational Psychology 31, 167–185 (2006).
[24] Scott D. Johnson and Shih-Ping Chung, “The eﬀect of
thinking aloud pair problem solving (tapps) on the trou-
bleshooting ability of aviation technician students,” Jour-
nal of Industrial Teacher Education 37 (1999).

[25] Craig Ross and R. Robert Orr, “Teaching structured
troubleshooting:
integrating a standard methodology
into an information technology program,” Educational
Technology Research and Development 57, 251–265
(2007).

19

[26] Lillian C. McDermott and Peter S. Shaﬀer, “Research as
a guide for curriculum development: An example from in-
troductory electricity. Part I: Investigation of student un-
derstanding,” American Journal of Physics 60, 994–1003
(1992); “Erratum: Research as a guide for curriculum
development: An example from introductory electricity.
Part I: Investigation of student understanding [Am. J.
Phys. 60, 9941003 (1992)],” American Journal of Physics
61, 81–81 (1993).

[27] Paula Vetter Engelhardt and Robert J. Beichner, “Stu-
dents understanding of direct current resistive electrical
circuits,” American Journal of Physics 72, 98–115 (2004).
[28] P. Coppens, M. De Cock, and C. Kautz, “Student un-
derstanding of ﬁlters in analog electronics lab courses,”
in Proceedings of the 40th SEFI Annual Conference 2012
(Thessaloniki, Greece, 2012).

[29] MacKenzie R. Stetzer, Paul Van Kampen, Peter S. Shaf-
fer, and Lillian C. McDermott, “New insights into stu-
dent understanding of complete circuits and the conser-
vation of current,” American Journal of Physics 81, 134–
143 (2013).

[30] Christos P. Papanikolaou, George S. Tombras, Kevin L.
Van De Bogart,
and MacKenzie R. Stetzer, “Inves-
tigating student understanding of operational-ampliﬁer
circuits,” American Journal of Physics 83, 1039–1050
(2015).

[31] Peter S. Shaﬀer and Lillian C. McDermott, “Research as
a guide for curriculum development: An example from
introductory electricity. Part II: Design of instructional
strategies,” American Journal of Physics 60, 1003–1013
(1992).

[32] J.C. Getty, “Assessing inquiry learning in a cir-
cuits/electronics course,” in Frontiers in Education Con-
ference, 2009. FIE ’09. 39th IEEE (2009) pp. 1–6.

[33] A. Mazzolini, T. Edwards, W. Rachinger, and S. Noppa-
ratjamjomras, “The use of interactive lecture demonstra-
tions to improve students’ understanding of operational
ampliﬁers in a tertiary introductory electronics course,”
Latin-American Journal of Physics Education 5, 147–153
(2011).

[34] Dimitri Dounas-Frazer, Kevin Van De Bogart, MacKen-
zie R. Stetzer, and H. J. Lewandowski, “The role of mod-
eling in troubleshooting: An example from electronics,”
in Physics Education Research Conference 2015 , PER
Conference (College Park, MD, 2015) pp. 103–106.

[35] Kevin Van De Bogart, Dimitri Dounas-Frazer, H. J.
Lewandowski,
and MacKenzie R. Stetzer, “The role
of metacognition in troubleshooting: An example from
electronics,” in Physics Education Research Conference
2015 , PER Conference (College Park, MD, 2015) pp.
339–342.

[36] Beth Crandall, Gary Klein,

and Robert R. Hoﬀman,
Working minds: A practitioner’s guide to Cognitive Task
Analysis (MIT Press, Cambridge, 2006).

[37] Randall Davis, “Reasoning from ﬁrst principles in elec-
tronic troubleshooting,” International Journal of Man-
Machine Studies 19, 403–423 (1983).

[38] Merrilyn Goos, Peter Galbraith, and Peter Renshaw,
“Socially mediated metacognition: creating collaborative
zones of proximal development in small group problem
solving,” Educational Studies in Mathematics 49, 193–
223 (2002).

[39] S.R. Bereiter and S.M. Miller, “A ﬁeld-based study of
troubleshooting in computer-controlled manufacturing

systems,” Systems, Man and Cybernetics, IEEE Trans-
actions on 19, 205–219 (1989).

[40] Udo Konradt, “Strategies of

in
computer-controlled manufacturing systems: empirical
analysis and implications for the design of adaptive deci-
sion support systems,” International Journal of Human-
Computer Studies 43, 503 – 521 (1995).

failure diagnosis

[41] Edward F. Redish, Teaching Physics with the Physics
Suite (University of Maryland Physics Education Re-
search Group, College Park, 2002).

[42] Carnegie Foundation for the Advancement of Teaching,
The Carnegie Classiﬁcation of Institutions of Higher Ed-
ucation, 2010 edition (Menlo Park, CA, 2011).

[43] H. J. Lewandowski, Noah Finkelstein, and Benjamin Pol-
lard, “Studying expert practices to create learning goals
for electronics labs,” in Physics Education Research Con-
ference 2014 , PER Conference (Minneapolis, MN, 2014)
pp. 155–158.

[44] Ericsson, K. A., and Simon, H. A, Protocol analysis: Ver-
bal reports as data (Rev. ed.) (MIT Press, Cambridge,

20

MA, 1993).

[45] M. W. Van Someren, Y. F. Barnard, and J. A. C. Sand-
berg, The think aloud method: A practical guide to model-
ing cognitive processes (Academic Press, London, 1994).
[46] K. Lynn Taylor and Jean-Paul Dionne, “Accessing
problem-solving strategy knowledge: The complemen-
tary use of concurrent verbal protocols and retrospective
debrieﬁng,” Journal of Educational Psychology 92, 413–
425 (2000).

[47] Michael L. Pate and Greg Miller, “A descriptive in-
terpretive analysis of students oral verbalization during
the use of thinkaloud pair problem solving while trou-
bleshooting,” Journal of Agricultural Education 52, 107–
119 (2011).

[48] Urie Bronfenbrenner, “Toward an experimental ecology
of human development,” American Psychologist 32, 513–
531 (1977).

