Overdispersed Black-Box Variational Inference

6
1
0
2

 
r
a

M
3

 

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
0
4
1
1
0

.

3
0
6
1
:
v
i
X
r
a

Francisco J. R. Ruiz
Columbia University

New York, USA

Michalis K. Titsias

Athens University of Economics and Business

Athens, Greece

David M. Blei

Columbia University

New York, USA

Abstract

We introduce overdispersed black-box varia-
tional inference, a method to reduce the variance
of the Monte Carlo estimator of the gradient in
black-box variational inference. Instead of tak-
ing samples from the variational distribution, we
use importance sampling to take samples from an
overdispersed distribution in the same exponen-
tial family as the variational approximation. Our
approach is general since it can be readily applied
to any exponential family distribution, which is
the typical choice for the variational approxima-
tion. We run experiments on two non-conjugate
probabilistic models to show that our method ef-
fectively reduces the variance, and the overhead
introduced by the computation of the proposal
parameters and the importance weights is neg-
ligible. We ﬁnd that our overdispersed impor-
tance sampling scheme provides lower variance
than black-box variational inference, even when
the latter uses twice the number of samples. This
results in faster convergence of the black-box in-
ference procedure.

1

Introduction

Generative probabilistic modeling is an effective approach
for understanding real-world data in many areas of science
(Bishop, 2006; Murphy, 2012). A probabilistic model de-
scribes a data-generating process through a joint distribu-
tion of observed data and latent (unobserved) variables.
With a model in place, the investigator uses an inference
algorithm to calculate or approximate the posterior, i.e.,
the conditional distribution of the latent variables given the
available observations. It is through the posterior that the
investigator explores the latent structure in the data and
forms a predictive distribution of future data. Approxi-
mating the posterior is the central algorithmic problem for
probabilistic modeling.

One of the most widely used methods to approximate the
posterior distribution is variational inference (Wainwright
and Jordan, 2008; Jordan et al., 1999). Variational in-
ference aims to approximate the posterior with a simpler
distribution, ﬁtting that distribution to be close to the ex-
act posterior, where closeness is measured in terms of
Kullback-Leibler (KL) divergence. In minimizing the KL,
variational inference converts the problem of approximat-
ing the posterior into an optimization problem.
Traditional variational inference uses coordinate ascent to
optimize its objective. This works well for models in which
each conditional distribution is easy to compute (Ghahra-
mani and Beal, 2001), but is difﬁcult to use in more com-
plex models where the variational objective involves in-
tractable expectations. Recent innovations in variational
inference have addressed this with stochastic optimization,
forming noisy gradients with Monte Carlo approximation.
This strategy expands the scope of variational inference be-
yond traditional models, e.g., to non-conjugate probabilis-
tic models (Carbonetto et al., 2009; Paisley et al., 2012; Sal-
imans and Knowles, 2013; Ranganath et al., 2014; Titsias
and Lázaro-Gredilla, 2014), deep neural networks (Neal,
1992; Hinton et al., 1995; Mnih and Gregor, 2014; Kingma
and Welling, 2014; Ranganath et al., 2015a), and proba-
bilistic programming (Wingate and Weber, 2013; Kucukel-
bir et al., 2015). Some of these techniques ﬁnd their roots in
classical policy search algorithms for reinforcement learn-
ing (Williams, 1992; van de Meent et al., 2016).
These approaches must address a core problem with Monte
Carlo estimates of the gradient, which is that they suffer
from high variance. The estimated gradient can signiﬁ-
cantly differ from the truth and this leads to slow con-
vergence of the optimization. There are several strategies
to reduce the variance of the gradients, including Rao-
Blackwellization (Casella and Robert, 1996; Ranganath
et al., 2014), control variates (Ross, 2002; Paisley et al.,
2012; Ranganath et al., 2014; Gu et al., 2016), reparameter-
ization (Price, 1958; Bonnet, 1964; Salimans and Knowles,
2013; Kingma and Welling, 2014; Rezende et al., 2014;
Kucukelbir et al., 2015), and local expectations (Titsias and

Lázaro-Gredilla, 2015).
In this paper we develop overdispersed black-box vari-
ational inference (O-BBVI), a new method for reducing
the variance of Monte Carlo gradients in variational in-
ference. The main idea is to use importance sampling to
estimate the gradient, in order to construct a good pro-
posal distribution that is matched to the variational prob-
lem. We show that O-BBVI applies more generally than
methods such as reparameterization and local expectations,
and it further improves the proﬁle of gradients that use Rao-
Blackwellization and control variates.
We demonstrate O-BBVI on two complex models:
a
non-conjugate time series model (Ranganath et al., 2014)
and Poisson-based deep exponential familys (DEFs) (Ran-
ganath et al., 2015a). Our study shows that O-BBVI re-
duces the variance of the original black-box variational in-
ference (BBVI) estimates (Ranganath et al., 2014), even
when using only half the number of Monte Carlo samples.
This provides signiﬁcant savings in run-time complexity.
Technical summary.
Consider a probabilistic model
p(x, z), where z are the latent variables and x are the obser-
vations. Variational inference sets up a parameterized dis-
tribution of the latent variables q(z; λ) and ﬁnds the param-
eter λ(cid:63) that minimizes the KL divergence between q(z; λ)
and the posterior p(z| x). We then use q(z; λ(cid:63)) as a proxy
for the posterior.
We build on BBVI, which solves this problem with a
stochastic optimization procedure that uses Monte Carlo
estimates of the gradient (Ranganath et al., 2014). Let L(λ)
be the variational objective, which is the (negative) KL di-
vergence up to an additive constant. BBVI uses samples
from q(z; λ) to approximate its gradient,
∇λL = Eq(z;λ) [f (z)] ,

(1)

where

f (z) = ∇λ log q(z; λ) (log p(x, z) − log q(z; λ)) .

(2)

The resulting Monte Carlo estimator, based on sampling
from q(z; λ), only requires evaluating the log-joint distribu-
tion log p(z, x), the log-variational distribution log q(z; λ),
and the score function ∇λ log q(z; λ). Calculations about
q(z; λ) can be derived once and stored in a library and,
as a consequence, BBVI can be easily applied to a large
class of models. However, as we mentioned above, Monte
Carlo estimates of this gradient usually have high vari-
ance. Ranganath et al. (2014) correct for this with Rao-
Blackwellization and control variates.
We expand on this idea by approximating the gradient with
importance sampling. We introduce a proposal distribution
r(z; λ, τ ), which depends on both the variational parame-
ters and an additional parameter. (We discuss the additional

parameter below.) We then write the gradient as

∇λL = Er(z;λ,τ )

,

(3)

(cid:20)

f (z)

q(z; λ)
r(z; λ, τ )

(cid:21)

and form noisy estimates with samples from the proposal.
The key idea behind our method is that the optimal pro-
posal distribution (in terms of minimizing the variance
of the resulting estimator) is not the original distribution
q(z; λ) (Owen, 2013, Chapter 9). Rather, the optimal pro-
posal is a skewed version of that distribution with heav-
ier tails. Unfortunately, this distribution is not available to
us—it involves an intractable normalization constant. But
we use this insight to set a proposal with heavier tails than
the variational distribution, thus making it closer to the op-
timal proposal. Note this is an unconventional use of im-
portance sampling, which is usually employed to approx-
imate expectations of intractable distributions. Here we
use importance sampling to improve the characteristics of
a Monte Carlo estimator by sampling from a different dis-
tribution.
In detail, we ﬁrst assume that the variational distribution is
in the exponential family. (This is not an assumption about
the model; most applications of variational inference use
exponential family variational distributions.) We then set
the proposal distribution to be in the corresponding overdis-
persed exponential family (Jørgensen, 1987), where τ is the
dispersion parameter. We show that the corresponding es-
timator has lower variance than the BBVI estimator, we put
forward a method to adapt the dispersion parameter during
optimization, and we demonstrate that this method is more
efﬁcient than BBVI. We call our approach overdispersed
black-box variational inference (O-BBVI).
Organization. The rest of the paper is organized as fol-
lows. We review BBVI in Section 2. We develop O-BBVI
in Section 3, describing both the basic algorithm and its
extensions to adaptive proposals and high-dimensional set-
tings. Section 4 reports on our empirical study of two non-
conjugate models. We conclude the paper in Section 5.

2 Black-Box Variational Inference

Consider a probabilistic model p(x, z) and a variational
family q(z; λ) which is in the exponential family, i.e.,

q(z; λ) = g(z) exp(cid:8)λ(cid:62)t(z) − A(λ)(cid:9) ,

(4)

where g(z) is the base measure, λ are the natural parame-
ters, t(z) are the sufﬁcient statistics, and A(λ) is the log-
normalizer. We are interested in a variational approxima-
tion to the intractable posterior p(z| x), i.e., we aim to min-
imize the KL divergence DKL (q(z; λ) (cid:107) p(z| x)) with re-
spect to λ (Jordan et al., 1999). This is equivalent to maxi-
mizing the evidence lower bound (ELBO),

L(λ) = Eq(z;λ) [log p(x, z) − log q(z; λ)] ,

(5)

which is a lower bound on the log of the marginal proba-
bility of the observations, log p(x).
With a tractable variational family (e.g., the mean-ﬁeld
family) and a conditionally conjugate model,1 the expec-
tations in Eq. 5 can be computed in closed form and we
can use coordinate-ascent variational inference (Ghahra-
mani and Beal, 2001). However, many models of interest
are not conditionally conjugate. For these models, we need
alternative methods to optimize the ELBO. One approach
is BBVI, which uses Monte Carlo estimates of the gradient
and requires few model-speciﬁc calculations (Ranganath
et al., 2014). Thus, BBVI is a variational inference algo-
rithm that can be applied to a large class of models.
BBVI relies on the “log-derivative trick,” also called RE-
INFORCE or score function method (Williams, 1992; Klei-
jnen and Rubinstein, 1996; Glynn, 1990), to obtain Monte
Carlo estimates of the gradient. In detail, we recover the
Monte Carlo estimate driven by Eqs. 1 and 2 by taking the
gradient of (5) with respect to the variational parameters λ,
and then applying the following two identities:
∇λq(z; λ) = q(z; λ)∇λ log q(z; λ),
Eq(z;λ) [∇λ log q(z; λ)] = 0.

(6)
(7)

Eq. 1 enables noisy gradients of the ELBO by taking sam-
ples from q(z; λ). However, the resulting estimator may
have high variance. This is especially the case when the
variational distribution q(z; λ) is a poor ﬁt to the posterior
p(z| x), which is typical in early iterations of optimization.
In order to reduce the variance of the estimator, BBVI uses
two strategies: control variates and Rao-Blackwellization.
Because we will also use these ideas in our algorithm, we
brieﬂy discuss them here.
Control variates. A control variate is a random vari-
able that is included in the estimator, preserving its ex-
pectation but reducing its variance (Ross, 2002). Al-
though there are many possible choices for control vari-
ates, Ranganath et al. (2014) advocate for the weighted
score function because it is not model-dependent. Denote
the score function by h(z) = ∇λ log q(z; λ), and note
again that its expected value is zero. With this function,
each component n of the gradient in (1) can be rewritten
as Eq(z;λ) [fn(z) − anhn(z)] where an is a constant and
f (z) is deﬁned in (2). (Here, fn(z) and hn(z) denote the
n-th component of f (z) and h(z), respectively.) We can
set each element an to minimize the variance of the Monte
Carlo estimates of this expectation,

an =

Cov(fn(z), hn(z))

Var(hn(z))

.

(8)

1A conditionally conjugate model is a model for which all the
complete conditionals (i.e., the posterior distribution of each hid-
den variable conditioned on the observations and the rest of hid-
den variables) are in the same exponential family as the prior.

In BBVI, a separate set of samples from q(z; λ) is used to
estimate an (otherwise, the estimator would be biased).
Rao-Blackwellization.
Rao-Blackwellization (Casella
and Robert, 1996) reduces the variance of a random vari-
able by replacing it with its conditional expectation, given
a subset of other variables. In BBVI, each component of
the gradient is Rao-Blackwellized with respect to variables
outside of the Markov blanket of the involved hidden vari-
able. More precisely, assume a mean-ﬁeld2 variational dis-
n q(zn; λn). We can equivalently

tribution q(z; λ) = (cid:81)

rewrite the expectation of each element in Eq. 1 as
∇λnL = Eq(z(n);λ(n))

(cid:2)∇λn log q(zn; λn)
×(cid:0)log pn(x, z(n)) − log q(zn; λn)(cid:1)(cid:3),

(9)

where z(n) denotes the variable zn together with all latent
variables in its Markov blanket, q(z(n); λ(n)) denotes the
variational distribution on z(n), and log pn(x, z(n)) con-
tains all terms of the log-joint distribution that depend
on z(n). The Monte Carlo estimate based on the Rao-
Blackwellized expectation has signiﬁcantly smaller vari-
ance than the estimator driven by Eq. 1.

3 Overdispersed Black-Box Variational

Inference

We have described BBVI and its two strategies for re-
ducing the variance of the noisy gradient. We now des-
cribe O-BBVI, a method for further reducing the variance.
The main idea is to use importance sampling (Robert and
Casella, 2005; Rubinstein and Kroese, 2011) to estimate
the gradient. We ﬁrst describe O-BBVI and the proposal
distribution it uses. We then show that this reduces vari-
ance, discuss several important implementation details, and
present the full algorithm.
O-BBVI does not sample from the variational distribution
q(z; λ) to estimate the expectation Eq(z;λ) [f (z)]. Rather,
it takes samples from a proposal distribution r(z; λ, τ ) and
constructs estimates of the gradient in Eq. 3, where the im-
portance weights are w(z) = q(z; λ)/r(z; λ, τ ). This guar-
antees that the resulting estimator is unbiased. The pro-
posal distribution involves the current setting of the varia-
tional parameters λ and an additional parameter τ.
The optimal proposal. The particular proposal that O-
BBVI uses is inspired by a result from the importance sam-
pling literature (Robert and Casella, 2005; Owen, 2013).
This result states that the optimal proposal distribution,
which minimizes the variance of the estimator, is not the
variational distribution q(z; λ). Rather, the optimal pro-

2A structured variational approach is also amenable to Rao-
Blackwellization, but we assume a fully factorized variational dis-
tribution for simplicity.

posal is

n(z) ∝ q(z; λ)|fn(z)|,
r(cid:63)

(10)

for each component n of the gradient. (Recall that f (z) is
a vector of the same length as λ.)
While interesting, the optimal proposal distribution is not
tractable in general—it involves normalizing a complex
product—and is not “black box” in the sense that it depends
on the model via f (z). In O-BBVI, we build an alternative
proposal based on overdispersed exponential families (Jør-
gensen, 1987). We will argue that this proposal is closer
to the (intractable) optimal r∗(z) than the variational dis-
tribution q(z; λ), and that it is still practical in the context
of stochastic optimization of the variational objective.
The overdispersed proposal. Our motivation for using
overdispersed exponential families is that the optimal dis-
tribution of Eq. 10 assigns higher probability density to the
tails of q(z; λ). There are two reasons for this fact. First,
consider settings of the variational parameters where the
variational distribution is a poor ﬁt to the posterior. For
these parameters, there are values of z for which the poste-
rior is high but the variational distribution is small. While
the optimal proposal would sample conﬁgurations of z for
which fn(z) is large, these realizations are in the tails of
the variational distribution.
The second reason has to do with the score function. The
score function hn(z) vanishes for values of z for which the
n-th sufﬁcient statistic tn(z) equals its expected value, and
this pushes probability mass (in the optimal proposal) to
the tails of q(z; λ). To see this, recall the exponential fam-
ily form of the variational distribution given in Eq. 4. For
any exponential family distribution, the score function is
hn(z) = tn(z) − Eq(z;λ) [tn(z)]. (This result follows from
simple properties of exponential families.3) For values of z
for which tn(z) is close to its expectation, hn(z) becomes
very close to zero. This zeros out fn(z) in Eq. 10, which
pushes mass to other parts of q(z; λ). As an example, in
the case where q(z; λ) is a Gaussian distribution, the opti-
mal proposal distribution places zero mass on the mean of
that Gaussian and hence more probability mass on its tails.
Thus, we design a proposal distribution r(z; λ, τ ) that as-
signs higher mass to the tails of q(z; λ). Speciﬁcally, we
use an overdispersed distribution in the same exponential
family as q(z; λ). The proposal is

(cid:26) λ(cid:62)t(z) − A(λ)

(cid:27)

r(z; λ, τ ) = g(z, τ ) exp

,

(11)

τ

where τ ≥ 1 is the dispersion coefﬁcient of the overdis-
persed distribution (Jørgensen, 1987). Hence, the O-BBVI

3The gradient of the log-normalizer (with respect to the na-
tural parameters) equals the ﬁrst-order moment of the sufﬁcient
statistics, i.e., ∇λA(λ) = Eq(z;λ) [t(z)].

(cid:98)∇O-BB
λ L =

(cid:88)

estimator of the gradient can be expressed as

s

,

1
S

f (z(s))

q(z(s); λ)
r(z(s))

z(s) iid∼ r(z; λ, τ ),
(12)
where S is the number of samples of the Monte Carlo ap-
proximation.
This choice of r(z; λ, τ ) has several desired properties for
a proposal distribution. First, it is easy to sample from,
since for ﬁxed values of τ it belongs to the same exponen-
tial family as q(z; λ). Second, as for the optimal proposal,
it is adaptive, since it explicitly depends on the parameters
λ which we are optimizing. Finally, by deﬁnition, it assigns
higher mass to the tails of q(z; λ), which was our motiva-
tion for choosing it.
The dispersion coefﬁcient τ can be itself adaptive to better
match the optimal proposal at each iteration of the varia-
tional optimization procedure. We put forward a method to
update the value of τ in Section 3.2.
Note that our approach differs from importance weighted
autoencoders (Burda et al., 2016), which also make use of
importance sampling but with the goal of deriving a tighter
log-likelihood lower bound in the context of the variational
autoencoder (Kingma and Welling, 2014). In contrast, we
use importance sampling to reduce the variance of the esti-
mator of the gradient.

3.1 Variance reduction

(13)

(14)

1
S

s

f (z(s)),

=

1
S

Eq(z;λ)

z(s) iid∼ q(z; λ).

After some algebra, we can express the variance of the
BBVI estimator as

(cid:98)∇O-BB
Here, we compare the variance of the O-BBVI estimator
BBVI estimator (cid:98)∇BB
λ L given in Eq. 12 with the variance of the original
(cid:88)
λ L, which samples from q(z; λ):
(cid:98)∇BB
λ L =
V(cid:104)(cid:98)∇BB
λ L(cid:105)
V(cid:104)(cid:98)∇O-BB
λ L(cid:105)

and we can also express the variance of the O-BBVI esti-
mator in terms of an expectation with respect to the varia-
tional distribution as

(cid:2)f 2(z)(cid:3) − 1

q2(z; λ)
r2(z; λ, τ )
q(z; λ)
r(z; λ, τ )

− 1
S
(∇λL)2.
(15)
Variance reduction for the O-BBVI approach is achieved

(cid:20)
λ L(cid:105) ≤ V(cid:104)(cid:98)∇BB
when V(cid:104)(cid:98)∇O-BB
λ L(cid:105)
(cid:21)
(cid:20)

or, equivalently,

(∇λL)2,

Er(z;λ,τ )

Eq(z;λ)

− 1
S

=

=

1
S
1
S

f 2(z)

f 2(z)

(cid:21)

(cid:21)

(cid:20)

S

(cid:2)f 2(z)(cid:3) .

(16)

Eq(z;λ)

f 2(z)

q(z; λ)
r(z; λ, τ )

≤ Eq(z;λ)

(∇λL)2

This inequality is trivially satisﬁed when we set r(z) to
the optimal proposal distribution, r(cid:63)(z). Moreover, Eq. 16
also gives us some intuition on why the use of an overdis-
persed proposal distribution can reduce the variance, since
r(z; λ, τ ) will be larger than q(z; λ) for those values of
z for which the product q(z; λ)f 2(z) is highest, i.e., in
the tails of q(z; λ). Our experimental results in Section 4
demonstrate that the variance is effectively reduced when
we use our O-BBVI.

3.2 Implementation

We now discuss several extensions of O-BBVI that make it
more suitable for real applications.

High dimensionality. Previously, we deﬁned the pro-
posal distribution r(z; λ, τ ) as an overdispersed version
of the variational distribution q(z; λ). However, impor-
tance sampling is known to fail when the dimensional-
ity of the hidden space is moderately high, due to the
high resulting variance of the importance weights w(z) =
q(z; λ)/r(z; λ, τ ). To address this, we rely on the fact that
hidden variable zn is the variable with the highest inﬂuence
on the estimator of the n-th component of the gradient. We
exploit this idea, which was also considered by Titsias and
Lázaro-Gredilla (2015) in their algorithm based on local
expectations.
More precisely, for the variational parameters of variable
zn, we ﬁrst write the gradient as
∇λnL = Eq(zn;λn)

(cid:2)Eq(z¬n;λ¬n) [fn(z)](cid:3)
(cid:2)w(zn)Eq(z¬n;λ¬n) [fn(z)](cid:3) ,

= Er(zn;λn,τn)

(17)

where r(zn; λn, τn) is the overdispersed version of
q(zn; λn) with dispersion coefﬁcient τn, z¬n denotes all
hidden variables in the model except zn, and similarly for
λ¬n. Thus, the corresponding importance weights in (17)
for each component of the gradient depend only on variable
zn, i.e.,

w(zn) =

q(zn; λn)

r(zn; λn, τn)

.

(18)

We use a single sample from q(z¬n; λ¬n) to estimate
the inner expectation in (17), and S samples of zn from
r(zn; λn, τn) to estimate the outer expectation.

Adaptation of the dispersion coefﬁcients. Our algo-
rithm requires setting the value of the dispersion param-
eters τn; we would like to automate this procedure. Here,
we develop a method to learn these coefﬁcients during opti-
mization by minimizing the variance of the estimator. More
precisely, we introduce stochastic gradient descent steps for
τn that minimize the variance. The exact derivative of the

(negative) variance with respect to τn is

(cid:34)

L(cid:105)

− ∂V(cid:104)(cid:98)∇O-BB

λn
∂τn

=

1
S

Er(zn;λn,τn)

Eq(z¬n;λ¬n) [fn(z)]2

× w2(zn)

∂ log r(zn; λn, τn)

∂τn

, (19)

(cid:35)

∂V(cid:104)(cid:98)∇O-BB

L(cid:105)

λn
∂τn

where we have applied the log-derivative trick once again,
as well as the extension to high dimensionality detailed
above. Now a Monte Carlo estimate of this derivative can
be obtained by using the same set of S samples used in
the update of λn. The resulting procedure is fast, with lit-
tle extra overhead, since both fn(z) and w(zn) have been
pre-computed.
Thus, we perform gradient steps of the form

n = τ (t−1)
τ (t)

n

− αn

,

(20)

where τn is constrained as τn ≥ 1 and the derivatives
are estimated via Monte Carlo approximation. Since the
derivatives in Eq. 20 can be several orders of magnitude
greater than τn, we opt for a simple approach to choose
an appropriate step size αn. In particular, we ignore the
magnitude of the derivative in (20) and take a small gra-
dient step in the direction given by its sign. Note that we
do not need to satisfy the Robbins-Monro conditions here
(Robbins and Monro, 1951), because the adaptation of τn
only deﬁnes the proposal distribution and it is not part of
the original stochastic optimization procedure.
Eq. 20 can still be applied even if λn is a vector; it only
requires replacing the derivative of the variance with the
summation of the derivatives for all components of λn.

Multiple importance sampling.
It may be more stable
(in terms of the variance of the importance weights) to con-
sider a set of J dispersion coefﬁcients, τn1, . . . , τnJ, in-
stead of a single coefﬁcient τn. We propose to use a mix-
ture with equal weights to build the proposal as follows:

1
J

J(cid:88)
(cid:111)

j=1

r(zn; λn, τn1, . . . , τnJ ) =

r(zn; λn, τnj),

(21)

(cid:110) λ(cid:62)

τnj

n t(zn)−A(λn)

where each term in the mixture is given by r(zn; λn, τnj) =
. In the importance sam-
g(zn, τnj) exp
pling literature, this is known as multiple importance sam-
pling (MIS), as multiple proposals are used (Veach and
Guibas, 1995). Within the MIS methods, we opt for full de-
terministic multiple importance sampling (DMIS) because
it is the approach that presents lowest variance (Hesterberg,
1995; Owen and Zhou, 2000; Elvira et al., 2015). In DMIS,
the number of samples S of the Monte Carlo estimator must

be an integer multiple of the number of mixture compo-
nents J, and S/J samples are deterministically assigned
to each proposal r(zn; λn, τnj). However, the importance
weights are obtained as if the samples had been actually
drawn from the mixture, i.e.,

(cid:80)J

w(zn) =

1
J

q(zn; λn)

j=1 r(zn; λn, τnj)

.

(22)

This choice of the importance weights yields an unbiased
estimator with smaller variance than the standard MIS ap-
proach (Owen and Zhou, 2000; Elvira et al., 2015).
In the experiments in Section 4 we investigate the per-
formance of two-component proposal distributions, where
J = 2, and compare it against our initial algorithm that
uses a unique proposal, which corresponds to J = 1.
We have also conducted some additional experiments (not
shown in the paper) with mixtures with higher number of
components, with no signiﬁcant improvements.

3.3 Full algorithm

We now present our full algorithm for O-BBVI. It makes
use of control variates, Rao-Blackwellization, and overdis-
persed importance sampling with adaptation of the disper-
sion coefﬁcients. At each iteration, we draw a single sam-
ple z(0) from the variational distribution, as well as S sam-
ples z(s)
n from the overdispersed proposal for each n (using
DMIS in this step). We obtain the score function as

hn(z(s)

n ) = ∇λn log q(z(s)

n ; λn),

(23)

and the argument of the expectation in (9) as

Algorithm 1: Overdispersed black-box variational infer-
ence (O-BBVI)
input : data x, joint distribution p(x, z), mean-ﬁeld

variational family q(z; λ)
output: variational parameters λ
Initialize λ;
Initialize the dispersion coefﬁcients τnj;
while algorithm has not converged do

/* draw samples
Draw a single sample z(0) ∼ q(z; λ);
for n = 1 to N do

Draw S samples z(s)
Compute the importance weights w(z(s)

n ∼ r(zn; λn,{τnj}) (DMIS);
n ) (Eq. 22);

end
/* estimate gradient
for n = 1 to N do

For each sample s, compute hn(z(s)
n ) (Eq. 23);
For each sample s, compute fn(z(s)) (Eq. 24);
Compute the weighted f w
Compute the weighted hw
Estimate the optimal an (Eq. 28);

Estimate the gradient (cid:98)∇λnL (Eq. 25);

n (z(s)) (Eq. 26);
n(z(s)
n ) (Eq. 27);

end
/* update dispersion coefficients */
for n = 1 to N do

Estimate the derivatives ∂V[∇λnL]
Take a gradient step for τnj (Eq. 20);

∂τnj

(Eq. 19);

end
/* take gradient step
Set the step size ρt (Eq. 29);
Take a gradient step for λ (Eq. 30);

*/

*/

*/

fn(z(s)) = hn(z(s)

n )(log pn(x, z(s)

n ; λn)),
(24)
where pn indicates that we use Rao-Blackwellization. Fi-
nally, the estimator of the gradient is obtained as

n , z(0)¬n)−log q(z(s)

end

(cid:17)

n (z(s)) − anhw
f w

n(z(s)
n )

,

(25)

(cid:98)∇λnL =

(cid:88)

(cid:16)

1
S

s

where the superscript ‘w’ stands for ‘weighted,’ i.e.,

f w
n (z(s)) = w(z(s)
hw
n(z(s)
n ) = w(z(s)

n )fn(z(s)),
n )hn(z(s)
n ).

(26)
(27)

Following Eq. 8, we use a separate set of samples to esti-
mate the optimal an as

instead as long as they satisfy the standard Robbins-Monro
conditions (Robbins and Monro, 1951). In AdaGrad, the
learning rate is obtained as

ρt = η diag (Gt)

−1/2 ,

(29)

where Gt is a matrix that contains the sum across the ﬁrst
t iterations of the outer products of the gradient, and η is a
constant. Thus, the stochastic gradient step is given by

λ(t) = λ(t−1) + ρt ◦(cid:98)∇λL,

(30)
where ‘◦’ denotes the element-wise (Hadamard) product.
Algorithm 1 summarizes the full procedure.

(cid:100)Cov(f w
(cid:99)Var(hw

n , hw
n)
n)

an =

.

(28)

4 Empirical Study

We use AdaGrad (Duchi et al., 2011) to obtain adaptive
learning rates that ensure convergence of the stochastic op-
timization procedure, although other schedules can be used

We study our method with two non-conjugate probabilis-
the gamma-normal time series model (GN-
tic models:
TS) and the Poisson deep exponential family (DEF). We

found that overdispersed black-box variational inference
(O-BBVI) reduces the variance of the black-box variational
inference (BBVI) estimator and leads to faster convergence.

4.1 Description of the experiments

Models description and datasets. The GN-TS model
(Ranganath et al., 2014) is a non-conjugate state-space
model for sequential data that was used to showcase BBVI.
The model is described by
wkd ∼ N (0, σ2
w),
ond ∼ N (0, σ2
o),
zn1k ∼ GammaE(σz, σz),
zntk ∼ GammaE(zn(t−1)k, σz),
xndt ∼ N

(cid:88)

zntkwkd, σ2
x

(cid:32)

(cid:33)

ond +

(31)

.

k

product (cid:80)

The indices n, t, d and k denote observations, time instants,
observation dimensions, and latent factors, respectively.
The distribution GammaE denotes the expectation/variance
parameterization of the gamma distribution. The model ex-
plains each datapoint xndt with a latent factor model. For
each time instant t, the mean of xndt depends on the inner
k zntkwkd, where zntk varies smoothly across
time. The variables ond are an intercept that capture the
baseline in the observations.
o = 1, σz = 1,
We set the hyperparameters to be σ2
x = 0.01. We use a synthetic dataset of N = 900
and σ2
time sequences of length T = 30 and dimensionality D =
20. We use K = 30 latent factors, leading to 828, 600
hidden variables.
The Poisson DEF (Ranganath et al., 2015a) is a multi-
layered latent variable model of discrete data, such as text.
The model is described by

w = 1, σ2

kv ∼ Gamma(αw, βw),
w(0)
kk(cid:48) ∼ Gamma(αw, βw),
w((cid:96))
dk ∼ Poisson(λz),
z(L)
dk ∼ Poisson
z((cid:96))

xdv ∼ Poisson

(cid:33)

,

(32)

(cid:32)(cid:88)
(cid:32)(cid:88)

k(cid:48)

k(cid:48)

dk(cid:48) w((cid:96))
z((cid:96)+1)
k(cid:48)k

(cid:33)

dk(cid:48)w(0)
z(1)
k(cid:48)v

.

The indices d, v, k and (cid:96) denote documents, vocabulary
words, latent factors, and hidden layers, respectively. This
model captures a hierarchy of dependencies between latent
variables similar to the hidden structure in deep neural net-
works. In detail, the number of times that word v appears in
document d is xdv. It has a Poisson distribution with rate
given by an inner product of gamma-distributed weights

and Poisson-distributed hidden variables from layer 1. The
Poisson-distributed hidden variables depend, in turn, on an-
other set of weights and another layer of hidden Poisson-
distributed variables. This structure repeats for a speciﬁed
number of layers.
We set the prior shape and rate as αw = 0.1 and βw = 0.3,
and the prior mean for the top level of the Poisson DEF as
λz = 0.1. We use L = 3 layers with K = 50 latent factors
each. We model the papers at the Neural Information Pro-
cessing Systems (NIPS) 2011 conference. This is a data set
with D = 305 documents, 612, 508 words, and V = 5715
vocabulary words (after removing stop words). This leads
to a model with 336, 500 hidden variables.
Evaluation. We compare O-BBVI with BBVI (Ranganath
et al., 2014). For a fair comparison, we use the same num-
ber of samples in both methods and estimate the inner ex-
pectation in Eq. 17 with only one sample. For the outer ex-
pectation, we use 8 samples to estimate the gradient itself
and 8 separate samples to estimate the optimal coefﬁcient
an for the control variates. For BBVI, we also doubled the
number of samples to 16 + 16; this is marked as “BBVI
(×2)” in the plots.
For O-BBVI, we study both a single proposal and a mixture
proposal with two components, respectively labeled as “O-
BBVI (single proposal)” and “O-BBVI (mixture).” For the
latter, we ﬁx the dispersion coefﬁcients τn1 = 1 for all
hidden variables and run stochastic gradient descent steps
for τn2.
At each iteration (and for each method) we evaluate sev-
eral quantities: the evidence lower bound (ELBO), the av-
eraged sample variance of the estimator of the gradient, and
a model-speciﬁc performance metric on the test set. The
estimation of the ELBO is based on a single sample of the
variational distribution q(z; λ) for all methods. For the GN-
TS model, we compute the average log-likelihood (up to a
constant term) on the test set, which is generated with one
additional time instant in all sequences. For the Poisson
(cid:80)
DEF, we compute the average held-out perplexity,
w∈doc(d) log p(w | #held out in d)

(cid:32)−(cid:80)

(cid:33)

d

exp

#held out words

,

(33)
where the held-out data contains 25% randomly selected
words of all documents.
Experimental setup. For each method, we initialize the
variational parameters to the same point and run each algo-
rithm with a ﬁxed computational budget (of CPU time).
We use AdaGrad (Duchi et al., 2011) for the learning rate.
We set the parameter η in Eq. 29 to η = 0.5 for the GN-TS
model and η = 1 for the Poisson DEF. When optimiz-
ing the O-BBVI dispersion coefﬁcients τn, we take steps
of length 0.1 in the direction of the (negative) gradient. We
initialize the dispersion coefﬁcients as τn = 2 for the single

proposal and τn2 = 3 for the two-component mixture.
We parameterize the normal distribution in terms of its
mean and variance, the gamma in terms of its shape and
mean, and the Poisson in terms of its mean parameter. In
order to avoid constrained optimization, we apply the trans-
formation λ(cid:48) = log(exp(λ)−1) to those variational param-
eters that are constrained to be positive and take stochastic
gradient steps with respect to λ(cid:48).
Overdispersed exponential families. For a ﬁxed disper-
sion coefﬁcient τ, the overdispersed exponential family of
the Gaussian distribution with mean µ and variance σ2 is a
Gaussian distribution with mean µ and variance τ σ2. The
overdispersed gamma distribution with shape s and rate r
is given by a new gamma distribution with shape s+τ−1
and rate r
τ . The overdispersed Poisson(λ) distribution is a
Poisson(λ1/τ ) distribution.

τ

4.2 Results

Figures 1 and 2 show the evolution of the ELBO, the predic-
tive performance, and the average sample variance of the
estimator for both models and all methods. We plot these
metrics as a function of running time, and each method is
run with the same computational budget.
For the GN-TS model, Figure 1a shows that the variance of
O-BBVI is signiﬁcantly lower than BBVI and BBVI with
twice the number of samples. Additionally, Figures 1b and
1c show that O-BBVI outperforms vanilla BBVI in terms
of both ELBO and held-out likelihood. According to these
ﬁgures, using a single or mixture proposal does not seem
to signiﬁcantly affect performance.
The results on the Poisson DEF are similar (Figure 2). Fig-
ure 2a shows the average sample variance of the estimator;
again, O-BBVI outperforms both BBVI algorithms. Fig-
ures 2b and 2c show the evolution of the ELBO and the
held-out perplexity, respectively, where O-BBVI also out-
performs BBVI. Here, the two-component mixture pro-
posal performs slightly better than the single proposal. This
is consistent with Figure 2a, which indicates that the mix-
ture proposal gives more stable estimates than the single
proposal.
Finally, for the GN-TS model only, we also apply the lo-
cal expectations algorithm of Titsias and Lázaro-Gredilla
(2015), which relies on exact or numerical integration to
reduce the variance of the estimator. We form noisy gra-
dients using numerical quadratures for the Gaussian ran-
dom variables and standard BBVI for the gamma variables
(these results are not plotted in the paper). We found that
local expectations accurately approximate the gradient for
the Gaussian distributions. It converges slightly faster at the
beginning of the run, although O-BBVI quickly reaches the
same performance. (We conjecture that it is faster because
the local expectations algorithm does not require the use

(a) Averaged sample variance of the estimator.

(b) Traceplot of the ELBO.

(c) Predictive performance (higher is better).
Figure 1: Results for the GN-TS model.

of control variates. This saves evaluations of the log-joint
probability of the model and thus it can run more iterations
in the same period of time.)
However, we emphasize that O-BBVI is a more general al-
gorithm than local expectations. The local expectations of
Titsias and Lázaro-Gredilla (2015) are only available for
discrete distributions with ﬁnite support and for continuous
distributions for which numerical quadratures are accurate
(such as Gaussian distributions). They fail to approximate
the expectations for other exponential family distributions
(e.g., gamma,4 Poisson, and others). For example, they
cannot handle the Poisson DEF.

4Although the univariate gamma distribution is amenable to
numerical integration, we have found that the approximation of
the expectations are not accurate when the shape parameter of the
gamma distribution is below 1, due to the singularity at 0.

01020304050101010151020Time (h)Variance (avg)  BBVI (x2)BBVIO−BBVI (single proposal)O−BBVI (mixture)01020304050−2−1.8−1.6−1.4−1.2−1−0.8−0.6−0.4−0.2x 109Time (h)ELBO  BBVI (x2)BBVIO−BBVI (single proposal)O−BBVI (mixture)01020304050−3500−3000−2500−2000−1500Time (h)Predictive log−likelihood  BBVI (x2)BBVIO−BBVI (single proposal)O−BBVI (mixture)the resulting stochastic optimization procedure.
There are several avenues for future work. First, we can ex-
plore other proposal distributions to provide a better ﬁt to
the optimal ones while still maintaining computational efﬁ-
ciency. Second, we can apply quasi-Monte Carlo methods
to further decrease the sampling variance, as already sug-
gested by Ranganath et al. (2014). Finally, we can combine
the reparameterization trick with overdispersed proposals
to explore whether variance is further reduced.

References
Bishop, C. M. (2006). Pattern Recognition and Machine
Learning (Information Science and Statistics). Springer-
Verlag New York, Inc., Secaucus, NJ, USA.

Bonnet, G. (1964). Transformations des signaux aléatoires
a travers les systemes non linéaires sans mémoire. An-
nals of Telecommunications, 19(9):203–220.

Burda, Y., Grosse, R., and Salakhutdinov, R. (2016). Im-
portance weighted autoencoders. In International Con-
ference on Learning Representations.

Carbonetto, P., King, M., and Hamze, F. (2009). A stochas-
tic approximation method for inference in probabilistic
In Advances in Neural Information
graphical models.
Processing Systems.

Casella, G. and Robert, C. P. (1996). Rao-Blackwellisation

of sampling schemes. Biometrika, 83(1):81–94.

Duchi, J., Hazan, E., and Singer, Y. (2011). Adaptive
subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research,
12:2121–2159.

Elvira, V., Martino, L., Luengo, D., and Bugallo, M. F.
(2015). Generalized multiple importance sampling.
arXiv:1511.03095.

Ghahramani, Z. and Beal, M. J. (2001). Propagation algo-
rithms for variational Bayesian learning. In Advances in
Neural Information Processing Systems.

Glynn, P. W. (1990). Likelihood ratio gradient estimation
for stochastic systems. Communications of the ACM,
33(10):75–84.

Gu, S., Levine, S., Sutskever, I., and Mnih, A. (2016).
MuProp: Unbiased backpropagation for stochastic neu-
ral networks. In International Conference on Learning
Representations.

Hesterberg, T. (1995). Weighted average importance sam-
pling and defensive mixture distributions. Technomet-
rics, 37(2):185–194.

Hinton, G., Dayan, P., Frey, B. J., and Neal, R. M. (1995).
The wake-sleep algorithm for unsupervised neural net-
works. Science, 268(5214):1158–1161.

(a) Averaged sample variance of the estimator.

(b) Traceplot of the ELBO.

(c) Predictive performance (lower is better).

Figure 2: Results for the Poisson DEF model.

5 Conclusions

We have developed overdispersed black-box variational in-
ference (O-BBVI), a method that relies on importance sam-
pling to reduce the variance of the stochastic gradients in
black-box variational inference (BBVI). O-BBVI uses an
importance sampling proposal distribution that has heav-
ier tails than the actual variational distribution. In particu-
lar, we choose the proposal as an overdispersed distribution
in the same exponential family as the variational distribu-
tion. Like BBVI, our approach is amenable to mean ﬁeld or
structured variational inference, as well as variational mod-
els (Ranganath et al., 2015b; Tran et al., 2016).
We have studied the performance of our method on two
complex probabilistic models. Our results show that BBVI
effectively beneﬁts from the use of overdispersed impor-
tance sampling, and O-BBVI leads to faster convergence in

01020304050607080100101010201030Time (h)Variance (avg)  BBVI (x2)BBVIO−BBVI (single proposal)O−BBVI (mixture)01020304050607080−2−1.9−1.8−1.7−1.6−1.5−1.4−1.3−1.2x 106Time (h)ELBO  BBVI (x2)BBVIO−BBVI (single proposal)O−BBVI (mixture)0102030405060708015002000250030003500Time (h)Held−out perplexity  BBVI (x2)BBVIO−BBVI (single proposal)O−BBVI (mixture)Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., and Saul,
L. K. (1999). An introduction to variational methods for
graphical models. Machine Learning, 37(2):183–233.

Robert, C. P. and Casella, G. (2005). Monte Carlo Sta-
tistical Methods (Springer Texts in Statistics). Springer-
Verlag New York, Inc., Secaucus, NJ, USA.

Ross, S. M. (2002). Simulation. Elsevier.
Rubinstein, R. Y. and Kroese, D. P. (2011). Simulation and
the Monte Carlo method. Wiley Series in Probability and
Statistics.

Salimans, T. and Knowles, D. A. (2013). Fixed-form varia-
tional posterior approximation through stochastic linear
regression. Bayesian Analysis, 8(4):837–882.

Titsias, M. K. and Lázaro-Gredilla, M. (2014). Doubly
stochastic variational Bayes for non-conjugate inference.
In International Conference on Machine Learning.

Titsias, M. K. and Lázaro-Gredilla, M. (2015). Local ex-
pectation gradients for black box variational inference.
In Advances in Neural Information Processing Systems.
Tran, D., Ranganath, R., and Blei, D. M. (2016). Varia-
tional Gaussian processes. In International Conference
on Learning Representations.

van de Meent, J.-W., Tolpin, D., Paige, B., and Wood, F.
(2016). Black-box policy search with probabilistic pro-
grams. In Artiﬁcial Intelligence and Statistics.

Veach, E. and Guibas, L. (1995). Optimally combining
sampling techniques for Monte Carlo rendering. In ACM
SIGGRAPH.

Wainwright, M. J. and Jordan, M. I. (2008). Graphical
models, exponential families, and variational inference.
Foundations and Trends in Machine Learning, 1(1–2):1–
305.

Williams, R. J. (1992).

Simple statistical gradient-
following algorithms for connectionist reinforcement
learning. Machine Learning, 8(3–4):229–256.

Wingate, D. and Weber, T.

variational
arXiv:1301.1299.

Automated
inference in probabilistic programming.

(2013).

Jørgensen, B. (1987).

Exponential dispersion mod-
els. Journal of the Royal Statistical Society, Series B
(Methodological), 49(2):127–162.

Kingma, D. P. and Welling, M. (2014). Auto-encoding vari-
ational Bayes. In International Conference on Learning
Representations.

Kleijnen, J. P. C. and Rubinstein, R. Y. (1996). Opti-
mization and sensitivity analysis of computer simulation
models by the score function method. Technical report,
Tilburg University, School of Economics and Manage-
ment.

Kucukelbir, A., Ranganath, R., Gelman, A., and Blei,
D. M. (2015). Automatic variational inference in Stan.
In Advances in Neural Information Processing Systems.
Mnih, A. and Gregor, K. (2014). Neural variational infer-
In International

ence and learning in belief networks.
Conference on Machine Learning.

Murphy, K. P. (2012). Machine Learning: A Probabilistic

Perspective. MIT Press.

Neal, R. (1992). Connectionist learning of belief networks.

Artiﬁcial Intelligence, 56(1):71–113.

Owen, A. and Zhou, Y. (2000). Safe and effective impor-
tance sampling. Journal of the American Statistical As-
sociation, 95(449):135—143.

Owen, A. B. (2013). Monte Carlo theory, methods and

examples. Book in preparation.

Paisley, J. W., Blei, D. M., and Jordan, M. I. (2012). Vari-
ational Bayesian inference with stochastic search. In In-
ternational Conference on Machine Learning.

Price, R. (1958). A useful theorem for nonlinear devices
having Gaussian inputs. IRE Transactions on Informa-
tion Theory, 4(2):69–72.

Ranganath, R., Gerrish, S., and Blei, D. M. (2014). Black
box variational inference. In Artiﬁcial Intelligence and
Statistics.

Ranganath, R., Tang, L., Charlin, L., and Blei, D. M.
(2015a). Deep exponential families. In Artiﬁcial Intelli-
gence and Statistics.

Ranganath, R., Tran, D., and Blei, D. M. (2015b). Hierar-

chical variational models. arXiv:1511.02386.

Rezende, D. J., Mohamed, S., and Wierstra, D. (2014).
Stochastic backpropagation and approximate inference
in deep generative models. In International Conference
on Machine Learning.

Robbins, H. and Monro, S. (1951). A stochastic approxi-
mation method. The Annals of Mathematical Statistics,
22(3):400–407.

