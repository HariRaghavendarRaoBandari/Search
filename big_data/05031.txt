6
1
0
2

 
r
a

 

M
6
1

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
1
3
0
5
0

.

3
0
6
1
:
v
i
X
r
a

Estimating orthant probabilities of high

dimensional Gaussian vectors with an application

to set estimation

Dario Azzimonti ∗

David Ginsbourger †∗,

Abstract

The computation of Gaussian orthant probabilities has been extensively stud-
ied for low dimensional vectors. Here we focus on the high dimensional case and
we present a two step procedure relying on both deterministic and stochastic tech-
niques. The proposed estimator relies indeed on splitting the probability into a
low dimensional term and a remainder. While the low dimensional probability can
be estimated by fast and accurate quadrature, the remainder requires Monte Carlo
sampling. We show that an estimator obtained with this technique has higher ef-
ﬁciency than standard Monte Carlo methods. We further reﬁne the estimation by
using a novel asymmetric nested Monte Carlo algorithm for the remainder and we
highlight cases where this approximation brings substantial efﬁciency gains. Fi-
nally this method is applied to derive conservative estimates of excursion sets of
expensive to evaluate deterministic functions under a Gaussian random ﬁeld prior
without requiring a Markov assumption.
Keywords: Gaussian probabilities; Monte Carlo; Gaussian random ﬁelds; Con-
servative set estimation.

1

Introduction

Assume that X = (X1, . . . , Xd) is a random vector with Gaussian distribution Nd(µ, Σ).
We are interested in estimating, for any ﬁxed t ∈ R, the following probability

π(t) = P (X ≤ (t, . . . , t)).

(1)

The general problem of evaluating π(t), which, for a full rank matrix Σ, is the in-
tegral of the multivariate normal density φ(·; µ, Σ) over the one-sided d-dimensional
rectangle (−∞, t]d, has been extensively studied in moderate dimensions with many
different methods. In low dimensions tables are available (see, e.g., Owen (1956) for
d = 2). Furthermore, when the dimension is smaller than 20, there exist methods
(see, e.g., Abrahamson (1964), Moran (1984) and Miwa et al. (2003)) exploiting the
speciﬁc orthant structure of the probability in (1). Currently, however, most of the lit-
erature uses numerical integration techniques to approximate the quantity. In moderate
dimensions fast reliable methods are established to approximate π(t) (see, e.g. Cox
and Wermuth (1991)) and more recently the methods introduced in Schervish (1984);
∗IMSV, Department of Mathematics and Statistics, University of Bern, Alpeneggstrasse 22, 3012 Bern,
†Idiap Research Institute, Centre du Parc, Rue Marconi 19, PO Box 592, 1920 Martigny, Switzerland.

Switzerland

1

Genz (1992) and Hajivassiliou et al. (1996) (see also Genz and Bretz (2002), Ridgway
(2014) and the book Genz and Bretz (2009) for a broader overview) provide state-of-
the-art algorithms when d < 100. Those techniques rely on fast quasi Monte Carlo
(qMC) methods and are very accurate for moderate dimensions. However, when d is
larger than 1000 they are not computationally efﬁcient or become intractable. Com-
monly used alternative methods are standard Monte Carlo (MC) techniques (see Tong
(2012), Chapter 8 for an extensive review), for which getting accurate estimates can be
computationally prohibitive.

We propose here a two step method that exploits the power of qMC quadratures
and the ﬂexibility of stochastic simulation. We rely on the following equivalent formu-
lation.

π(t) = 1 − P (max X > t),

In the following we ﬁx t and denote p =

where max X denotes maxi=1,...,d Xi.
P (max X > t).
The central idea here is using a moderate dimensional subvector of X to approx-
imate p and then correcting bias by MC. Let us ﬁx q (cid:28) d and deﬁne the active di-
mensions as Eq = {i1, . . . , iq} ⊂ {1, . . . , d}. Let us further denote with X q the
q dimensional vector X q = (Xi1, . . . , Xiq ) and with X−q the (d − q) dimensional
vector X−q = (Xj)j∈E\Eq. Then,

p = P (max X > t) = pq + (1 − pq)Rq,
pq = P (max X q > t),
Rq = P (max X−q > t | max X q ≤ t).

(2)

The quantity pq is always smaller or equal to p as Eq ⊂ {1, . . . , d}. Selecting a non-
degenerate vector X q, we propose to estimate pq with the QRSVN algorithm (Genz
et al., 2012) which is efﬁcient as we choose a number of active dimensions q much
smaller than d.

In Chevalier (2013), Chapter 6, the similar problem of approximating the non-
exceedance probability of the maximum of a Gaussian random ﬁeld Z based on a few
well-selected points is presented. In that setting each component of X stands for the
value of Z at one point of a discretization of the index set. Active dimensions (i.e.
the well-selected points) were chosen by numerically maximizing pq, and the remain-
der was not accounted for. Here a full optimization of the active dimensions is not
needed as we, instead, exploit the decomposition in (2) to correct the error introduced
by pq. For this task, the reminder Rq is estimated with a standard MC technique and a
novel asymmetric nested Monte Carlo (anMC) algorithm. The anMC technique draws
samples by taking into account the computational cost, resulting in a more efﬁcient
estimator.

In the remainder of the paper, we propose an unbiased estimator for p and we com-
pute its variance in Section 2. In Section 3 we introduce the anMC algorithm in the
more general setting of estimating expectations depending on two vectors with different
simulation costs. It is then explicitly applied to efﬁciently estimate Rq. Finally, in Sec-
tion 4, we show an implementation of this method to compute conservative estimates
of excursion sets for expensive to evaluate functions under non-necessarily Markovian
Gaussian random ﬁeld priors. In Appendix B, we present two heuristic methods to
select active dimensions. All proofs are in Appendix A.

2

2 The estimator properties
2.1 An unbiased estimator for p
Equation (2) gives us a decomposition that can be exploited to obtain an unbiased
estimator for p. In the following proposition we deﬁne the estimator and we compute
its variance.

Proposition 1. Consider (cid:98)pq and (cid:99)Rq, independent unbiased estimators of pq and Rq
respectively, then(cid:98)p =(cid:98)p = (cid:98)pq + (1 − (cid:98)pq)(cid:99)Rq is an unbiased estimator for p. Moreover

var((cid:98)p) = (1 − Rq)2 var((cid:98)pq) + (1 − pq)2 var((cid:99)Rq) + var((cid:98)pq) var((cid:99)Rq).
In what follows we present options for (cid:98)pq and (cid:99)Rq that form an efﬁcient computa-

its variance is

(3)

tional strategy.

2.2 Quasi Monte Carlo estimator for pq
The quantity pq can also be computed as

pq = 1 − P (X q ≤ tq) ,

pq, see Genz and Bretz (2009).

in Genz (1992), Hajivassiliou et al. (1996).

where tq denotes the q dimensional vector (t, . . . , t). The approximation of pq thus
requires only an evaluation of the cumulative distribution function of X q, selected as
non-degenerate. Since we assume that q (cid:28) d, then the dimension is low and we

propose to estimate pq with the estimator (cid:98)pq that uses the method QRSVN introduced
integration. The quantity (cid:98)pq
Figure 1 shows the boxplots of 30 replications of an experiment where (cid:98)pq

This method computes a randomized quasi Monte Carlo integration of the normal
density. The estimate’s error is approximated with the variance of the randomized
G obtained with this procedure is an unbiased estimator of

G is used
to approximate p. The dimension of the vector X is d = 1000, the threshold is ﬁxed at
t = 11. In particular the vector X comes from the discretization of a six dimensional
Gaussian random ﬁeld on the ﬁrst 1000 points of the Sobol’ sequence (Bratley and Fox,
1988). The Gaussian random ﬁeld was chosen with tensor product Mat´ern (ν = 5/2)
covariance kernel and a non constant mean function m. The hyperparameters of the
covariance kernel were ﬁxed as θ = [0.5, 0.5, 1, 1, 0.5, 0.5]T and σ2 = 8, see Ras-
mussen and Williams (2006), Chapter 4, for details on the parametrization. The active
points were chosen with Method 1 reviewed in Appendix B. As the number of active
G with respect to p decrease.
G gives an inexpensive estimation of most of the probability mass with as few as 40
active dimensions, however it is intrinsically biased as an estimator of p. Estimating
Rq enables to correct the bias of this ﬁrst step.

dimensions increases both the variance and the error of (cid:98)pq
(cid:98)pq

2.3 Monte Carlo estimator for Rq

Debiasing (cid:98)pq

G as an estimator of p can be done at the price of estimating

Rq = P(cid:0)max X−q > t | max X q ≤ t(cid:1) .

3

Figure 1: Estimate of p = P (max X ≥ t) with (cid:98)pq

number of active dimensions.

G for different values of q, the

1, . . . , xq

There is no close formula for Rq, so it is approximated here via MC. Since X is
Gaussian then so are X q, X−q and X−q | X q = xq, for any deterministic vector
xq ∈ Rq.

In order to estimate Rq = P(cid:0)max X−q > t | Xi1 ≤ t, . . . , Xiq ≤ t(cid:1), we ﬁrst gen-

n of X q such that X q ≤ tq. Second, we compute the
l , l = 1, . . . , n

erate n realizations xq
mean and covariance matrix of X−q conditional on each realization xq
with the following formulas
l = Σ−q − Σ−q,q(Σq)−1Σq,−q,
µ−q|xq
(4)
where µq, Σq and µ−q, Σ−q are the mean vector and covariance matrix of X q and
X−q respectively, Σ−q,q is the cross-covariance between the dimensions E \ Eq and
Eq, Σq,−q is the transpose of Σ−q,q. Given the mean and covariance matrix conditional
on each sample xq
l , we can easily draw a realization y
l . Once
−q|q
n couples (xq
), l = 1, . . . , n are drawn from the respective distributions, an
l , y
l
estimator for Rq is ﬁnally obtained as follows

from X−q | X q = xq

l = µ−q + Σ−q,q(Σq)−1(xq

l − µq),

Σ−q|xq

−q|q
l

(cid:99)Rq

MC

=

1
n

n(cid:88)

l=1

1max y

−q|q
l

>t.

The realizations of X q are obtained with a rejection sampling algorithm (Robert,
1995; Horrace, 2005). This step is computationally expensive as many draws of the
vector X q might be rejected depending on the size of the d-dimensional rectangle
[−∞, t]d. Drawing samples from the distribution of X−q | X q = xq
l is instead less
expensive. The computation of the mean vector and covariance matrix requires only
linear algebra operations as described in (4) and realizations of X−q | X q = xq
l can be
generated by sampling from a multivariate normal distribution.

The difference in computational cost between the ﬁrst step and the second step of
the MC procedure can be exploited to reduce the variance. In Section 3 we present
a new MC procedure that at a ﬁxed computational cost reduces the variance of the
estimate.

4

0.040.060.080.100.1210203040501000qProbabilityMethodpqGEstimate p with pqG(cid:98)pGMC = (cid:98)pq

G + (1 − (cid:98)pq

the same quantity is shown for comparison

Figure 2: Estimate of p with(cid:98)pGMC for different values of q. A full MC estimation of
We denote with(cid:98)pGMC the unbiased estimator of p deﬁned as
proximated with(cid:98)pGMC. The set-up is the same as in Fig. 1. The core of the probability
is approximated with (cid:98)pq
residual Rq is estimated with (cid:99)Rq
. The remainder allows to correct the bias of (cid:98)pq
the same computational cost. The estimator(cid:98)pGMC exploits an almost exact method to
estimate the largest part of the probability p, therefore the MC estimator(cid:99)Rq

G and the active dimensions are chosen with Method 1. The
G
even with a small number of active dimensions. As comparison the results of the same
experiment with a full MC estimator for p are also shown. For all experiments and
for each method the number of samples was chosen in order to have approximately

Figure 2 shows the box plots of 30 replications of an experiment where p is ap-

G)(cid:99)Rq

MC

.

MC

variance than a full MC procedure for a ﬁxed computational cost.

MC

has less

Monte Carlo

3 Estimation of the residual with asymmetric nested
In section 2, Rq was estimated by (cid:99)Rq

. There exists many methods to reduce the
variance of such estimators, including antithetic variables (Hammersley and Morton,
1956), importance sampling (Kahn, 1950; Kahn and Marshall, 1953) or conditional
Monte Carlo (Hammersley, 1956) among many others, see, Lemieux (2009), Chapter 4,
and Robert and Casella (2013), Chapter 4, for a broader overview. Here we propose
a so-called asymmetric nested Monte Carlo (anMC) estimator for Rq that reduces the
variance by a parsimonious multiple use of conditioning data.

MC

The idea is to use an asymmetric sampling scheme that assigns the available com-
putational resources by taking into account also the actual cost of simulating each com-
ponent. This type of asymmetric sampling scheme was already introduced in the partic-
ular case of comparing the performance of stopping times for a real-valued stochastic

5

0.040.080.120.1610203040501000stdMCqProbabilityMethodMonte CarlopGMCpqGEstimate p with pGMCprocess in discrete times in Dickmann and Schweizer (2014). Here we introduce this
procedure in a general fashion and then we detail how to use it as variance reduction
. Consider two random elements W ∈ W and Z ∈ Z, deﬁned on the same

for (cid:99)Rq

MC

probability space and not independent. We are interested in estimating

G = E [g(W, Z)] ,

(5)
where g : W × Z → R is a measurable function, assumed integrable with respect to
(W, Z)’s probability measure. Let us also assume that it is possible to draw realizations
from the marginal distribution of W , Z and from the conditional distribution of Z |
W = wi, for each wi sample of W . We can then obtain realizations (wi, zi), i =
1, . . . , n of (W, Z) by simulating wi from the distribution of W and then zi from the
conditional distribution Z | W = wi, leading to:

(cid:98)G =

1
n

n(cid:88)

i=1

g(wi, zi).

(6)

MC

that the estimator(cid:99)Rq

This MC estimator can actually be seen as the result of a two step nested MC procedure
where, for each realization wi, one inner sample zi is drawn from Z | W = wi. Note
used in Section 2 is a particular case of Equation (6) with W =
X q | X q ≤ tq, Z = X−q and g(x, y) = 1max y>t. As noted in Section 2, drawing
realizations of X q | X q ≤ tq has a higher computational cost than simulating X−q
because rejection sampling is required. More generally, if we denote with CW (n) the
cost of n realizations of W and with CZ|W (m; wi) the cost of drawing m conditional
simulations from Z | W = wi, then sampling several conditional realizations for a
given wi might bring savings if CW (1) is much higher than CZ|W (1; wi).
In the proposed asymmetric sampling scheme for each realization wi we sample m
realizations zi,1, . . . , zi,m from Z | W = wi. Assume that we sample with this scheme
the couples (wi, zi,j), i = 1, . . . , n, j = 1, . . . , m, then we can write the following
estimator for G

n(cid:88)

m(cid:88)

(cid:101)G =

1
nm

(7)

g(wi, zi,j).

i=1

j=1

For a ﬁxed number of samples, the estimator (cid:101)G may have a higher variance than
(cid:98)G due to the dependency between pairs sharing the same replicate of W . However,

in many cases, it may be more relevant to focus on obtaining good estimates within a
ﬁxed time. If we set the computational budget instead of the number of samples and if
CZ|W is smaller than CW , then anMC may lead to an overall variance reduction thanks
to an increased number of simulated pairs. We show in the remainder of this section
that, in the case of an afﬁne cost function CZ|W , there exists an optimal number of

inner simulations m diminishing the variance of (cid:101)G below that of (cid:98)G. Assume

CW (n) = cn and, for each sample wi

CZ|W (m; wi) = CZ|W (m) = α + βm,

with c, α, β ∈ R+ dependent on the simulators chosen for W and Z | W . The sec-
ond equation entails that the cost of conditional simulations does not depend on the
conditioning value.
If W = X q | X q ≤ tq, Z = X−q as in Section 2, then Z | W is Gaussian with
mean and covariance matrix described in (4). In this case the cost for sampling Z | W

6

is afﬁne, with α describing pre-calculation times and β random number generation and
algebraic operations.
Denote with W1, . . . , Wn replications of W . For each Wi we consider the con-
ditional distribution Z | Wi and Z1,i, . . . , Zm,i replications from it. We study here
Cﬁx ∈ R+. First observe that

the properties of (cid:101)G when the total simulation budget, denoted Ctot(n, m) is ﬁxed to

Ctot(n, m) = n(c + α + βm).

Then we can derive the number of replications of W as a function of m:

NCﬁx(m) =

Cﬁx

.

nm

(cid:115)

var((cid:101)G) =

1
n

c + α + βm

The following proposition shows a decomposition of var((cid:101)G) that is useful to ﬁnd

the optimal number of simulations m∗ under a ﬁxed simulation budget Ctot(n, m) =
Cﬁx.
Proposition 2. Consider n independent copies W1, . . . , Wn of W and, for each Wi,
m copies Zi,j = Zj | Wi j = 1, . . . , m, independent conditionally on Wi. Then,

var(g(W1, Z1,1)) − m − 1

E(cid:2) var(g(W1, Z1,1) | W1)(cid:3).
Corollary 1. Under the same assumptions, (cid:101)G has minimal variance when
m = (cid:101)m =
where A = var(g(W1, Z1,1)) and B = E(cid:2) var(g(W1, Z1,1) | W1)(cid:3). Moreover denote
with ε = (cid:101)m − (cid:98)(cid:101)m(cid:99), then the optimal integer is m∗ = (cid:98)(cid:101)m(cid:99) if
(2(cid:101)m + 1) −(cid:112)4((cid:101)m)2 + 1
or m∗ = (cid:100)(cid:101)m(cid:101) otherwise.
var((cid:98)G) [1 − η], where η ∈ (0, 1).
In order to compute m∗, we need to know A = var(g(W1, Z1,1)) and B = E(cid:2) var(g(W1, Z1,1) |
W1)(cid:3) and the constants c, α and β. A and B depend on the speciﬁc problem at hand

(c+α)B+β(A−B) then var((cid:101)G) =

Proposition 3. Under the same assumptions, if m∗ >

3.1 Algorithmic considerations

(α + c)B
β(A − B)

,

2(α+c)B

ε <

2

(8)

(9)

and are usually not known in advance. Part of the total computational budget is then
needed to estimate A and B. This preliminary phase is also used to estimate the system
dependent constants c and β. Algorithm 1 reports the pseudo-code for anMC.

7

Input

Output: (cid:101)G

: µW , µZ, ΣW , ΣZ, ΣW Z, g, Ctot

Part 0: estimate c, β, α ;
initialize compute the conditional covariance ΣZ|W and initialize n0, m0;
for i ← 1 to n0 do
Part 1:
estimate A, B

simulate wi from the distribution of W ;
compute the conditional mean µZ|W =wi;
draw m0 simulations zi,1, . . . , zi,m0 from the conditional distribution
Z | W = wi;
estimate E [g(W, Z) | W = wi] with ˜Ei = 1
(cid:80)m0
estimate var (g(W, Z) | W = wi) with
(cid:114)
j=1(g(wi, zi,j) − ˜Ei)2;
˜Vi = 1
(cid:80)n0

˜Ei)2 , m∗ as in Corollary 1 and

(cid:80)n0
(cid:80)n0
i=1( ˜Ei− 1

j=1 g(wi, zi,j);

(cid:80)m0

compute (cid:101)m =

(α+c) 1
n0

m0−1

m0

˜Vi

i=1

β 1

n0−1

n0

i=1

n∗ = NCﬁx (m∗);
for i ← 1 to n∗ do
if i ≤ n0 then

Part 2:

compute (cid:101)G

for j ← 1 to m∗ do
if j ≤ m0 then

use previously calculated (cid:101)Ei and (cid:101)Vi;
compute (cid:101)Ei = 1

m∗(cid:80)m∗

j=1 g(wi, zi,j);

simulate zi,j from the distribution Z | W = wi;

else

else

simulate wi from the distribution of W ;
compute the conditional mean µZ|W =wi;
for j ← 1 to m∗ do

simulate zi,j from the conditional distribution Z | W = wi;

compute (cid:101)Ei = 1

m∗(cid:80)m∗
estimate E [g(W, Z)] with (cid:101)G = 1
3.2 Estimate p with(cid:98)pGanMC

n∗(cid:80)n∗

j=1 g(wi, zi,j);
˜Ei;

i=1

Algorithm 1: Asymmetric nested Monte Carlo.

The anMC algorithm can be used to reduce the variance compared to Rq’s MC estimate
proposed in Section 2.3. In fact, let us consider W = X q | X q ≤ tq and Z = X−q.
We have that W is expensive to simulate as it requires rejection sampling while, for a
given sample wi, Z | W = wi is Gaussian with mean and covariance matrix described
in Equation (4). It is much cheaper to obtain samples from Z | W = wi than from
W . Moreover, as noted earlier, Rq can be written in the form of Equation (5) with
g(x, y) = 1max y>t. We can then use Algorithm 1 to calculate m∗, sample n∗ realiza-
tions w1, . . . , wn∗ of W and for each realization wi obtain m∗ samples zi,1, . . . , zi,m∗
of Z | W = wi. Then we can estimate Rq via

(cid:99)Rq

anMC

=

1

n∗m∗

1max zi,j >t.

n∗(cid:88)

m∗(cid:88)

i=1

j=1

8

G,(cid:98)pGMC and(cid:98)pGanMC on the example introduced

in Fig. 1.

Figure 3: Comparison of results with (cid:98)pq
Finally plugging in(cid:99)Rq
and (cid:98)pq
(cid:98)pGanMC = (cid:98)pq

anMC

G in Equation (2), we obtain

G + (1 − (cid:98)pq

G)(cid:99)Rq

anMC

.

(2009), Section 4.2) as

Figure 3 shows a comparison of results using 30 replications of the experiment pre-
sented in Section 2.3. Results obtained with a MC estimator are shown for comparison.
While the simulations of all experiment were obtained under the constraint of a
ﬁxed computational cost, the actual time to obtain the simulations was not exactly the
same. In order to be able compare the methods in more general settings we further

rely on the notion of efﬁciency. For an estimator(cid:98)p, we deﬁne the efﬁciency (Lemieux
where time[(cid:98)p] denotes the computational time of the estimator(cid:98)p.
Figure 4 shows a comparison of the efﬁciency of (cid:98)pGMC and (cid:98)pGanMC with a full
with the estimator(cid:98)pGMC. The estimator(cid:98)pGanMC shows a higher median efﬁciency than

Monte Carlo estimator. With as few as q = 50 active dimensions we obtain an increase
in efﬁciency of around 10 times on average over the 30 replications of the experiment
the others for all q ≥ 20.

var((cid:98)p) time[(cid:98)p]

Eﬀ[(cid:98)p] =

1

,

4 Application: efﬁcient computation of conservative es-

timates

We show here that anMC is key in conservative excursion set estimation relying on
Gaussian ﬁeld models. We consider an expensive to evaluate system described by a
continuous function f : D ⊂ R(cid:96) → R, (cid:96) ≥ 1, where D is a compact domain, and we
focus on estimating, for some ﬁxed threshold t ∈ R, the set

Γ∗ = {x ∈ D : f (x) ≤ t}.

9

0.040.080.120.1610203040501000stdMCqProbabilityMethodpGanMCMonte CarlopGMCpqGEstimate p with pGanMCFigure 4: Efﬁciency of the estimators(cid:98)pGMC and(cid:98)pGanMC compared with the efﬁciency

of a standard MC estimator on 30 replications of the experiment from Fig. 3. Values in
logarithmic scale.

Such problems arise in many applications such as reliability engineering (see, e.g.,
Picheny et al. (2013), Chevalier et al. (2014a)) climatological studies (Bolin and Lind-
gren, 2015; French and Sain, 2013) or in natural sciences (Bayarri et al., 2009). Often
f can only be evaluated with computer simulations and is seen as expensive to evaluate
black-box (Sacks et al., 1989).
In practice we assume here that f was only evalu-
ated at points χk = {x1, . . . , xk} ⊂ D and the associated responses are denoted with
fk = (f (x1), . . . , f (xk)) ∈ Rk and we are interested in giving an estimate of Γ∗
starting from these k evaluations.

In a Bayesian framework we consider f as one realization of a Gaussian random
ﬁeld (Zx)x∈D with prior mean function m and covariance kernel K. A prior distribution
of the excursion set is hence obtained by thresholding Z, thus obtaining the following
random closed set

Γ = {x ∈ D : Zx ≤ t}.

Denoting with Zχk the random vector (Zx1, . . . , Zxk ), we can then condition Z on the
observations fk and obtain a posterior distribution for the ﬁeld Zx | Zχk = fk. This
gives rise to a posterior distribution for Γ. Different deﬁnitions of random closed set
expectation (Molchanov (2005), Chapter 2) can be used to summarize the posterior dis-
tribution of Γ and to provide estimates for Γ∗. In Chevalier et al. (2013), for example,
the Vorob’ev expectation was introduced in this setting. Let us recall here the basic
tools needed to compute this estimate. We denote with pΓ,k : D → [0, 1] the coverage
function of the posterior set Γ | Zχk = fk, deﬁned as

pΓ,k(x) = Pk(x ∈ Γ), x ∈ D,

where Pk(·) = P (· | Zχk = fk). This function associates to each point in D its
probability of being inside the posterior excursion set. The function pΓ,k gives rise to a
family of excursion set estimates; in fact, for each ρ ∈ [0, 1] we can deﬁne the posterior
ρ-level Vorob’ev quantile of Γ

Qρ = {x ∈ D : pΓ,k(x) ≥ ρ}.

10

3.54.04.55.05.51020304050stdMCqlog10 (efficiency)MethodpGanMCMonte CarlopGMCEfficiency of pGMC and pGanMCThe Vorob’ev expectation of Γ (Molchanov, 2005) is the quantile QρV that satisﬁes
|Qρ| ≤ Ek[|Γ|] ≤ |QρV | for all ρ ≥ ρV , where |A| denotes the volume of a set A ⊂ Rl.
This set expectation consists of the points that have high enough marginal probability
of being inside the excursion set. The lower limit for the marginal probability is chosen
in order to approximate well the volume of the set. In some applications, however, it
is important to provide conﬁdence statements on the whole set estimate. Conservative
estimates introduced in Bolin and Lindgren (2015) for Gaussian Markov random ﬁelds
address this issue. A conservative estimate of Γ∗ is

CΓ,k = arg max

C⊂D

{|C| : Pk(C ⊂ {Zx ≤ t}) ≥ α},

(10)

where |C| denotes the volume of C. This deﬁnition however leads to major computa-
tional issues.

First of all we need to select a family of sets to use for the optimization procedure
in Equation (10). Here we follow Bolin and Lindgren (2015) and select the Vorob’ev
quantiles as family of sets. This family has the advantage that it is parametrized by one
real number ρ and thus it renders the optimization straightforward. We use here the
dichotomy algorithm detailed in Algorithm 2.
Second, for each candidate Q we need to evaluate Pnext = Pk(Q ⊂ {Zx ≤ t}), the
probability that Q is inside the excursion. In fact, this quantity is a high dimensional
orthant probability. For the Vorob’ev quantile Qρ(cid:48), consider the discretization over the
points c1, . . . , cr, then

Pk(Qρ(cid:48) ⊂ {Zx ≤ t}) = Pk(Zc1 ≤ t, . . . , Zcr ≤ t) = 1 − Pk( max
Thus we use the estimator(cid:98)pGanMC to approximate 1 − Pk(Qρ(cid:48) ⊂ {Zx ≤ t}). The use

of anMC allows a discretization of the Vorob’ev quantiles at resolutions that seem out
of reach otherwise.

Zci > t).

i=1,...,r

Input

:

• mk, Kk, conditional mean and covariance of Z | Zχk = fk;
• ﬁne discretization design G;

Output: Conservative estimate for Γ∗ at level α.

compute iB, iT ﬁnd the highest index iT such that(cid:81)T

Part 0: sort the points in G in decreasing order of pΓ,k, with indices Gs = {i1, . . . im};

j=1 pΓ,k(Gs)[ij] ≥ α;

ﬁnd the highest index iB such that pΓ,k(Gs)[iB] ≥ α;
evaluate mean and covariance matrix mk(iB) and ΣiB ,iB ;
initialize iL = iT , iR = iB ;
⊂ {Zx ≤ t}), PR = Pk(QρiR

Initialize dichotomy compute PL = Pk(QρiL

Part 2: while PR < α and (iR − iL) ≥ 2 do

Part 1:

optimization

⊂ {Zx ≤ t});

next evaluation inext = iL+iR
compute Pnext = Pk(Qρinext ⊂ {Zx ≤ t});
if Pnext ≥ α then

;

2

iL = inext, iR = iR;

else

iL = iL, iR = inext;

Algorithm 2: Conservative estimates algorithm.

We apply Algorithm 2 to a two dimensional artiﬁcial test case. We consider as
function f a realization of a Gaussian ﬁeld (Zx)x∈D, where D ⊂ R2 is the unit square.

11

(a) Realization obtained with a Mat´ern kernel.

(b) Realization obtained with Gaussian kernel.

Figure 5: Conservative estimates at 95% (white region) for the excursion below t = 1.
Both models are based on 15 evaluations of the function (black triangles). The true
excursion level is plotted in blue, the Vorob’ev expectation in green and the 0.95-level
set in red.

We consider two parametrizations for the prior covariance kernel: a tensor product
Mat´ern covariance kernel with ν = 5/2, variance σ2 = 0.5 and range parameters
θ = [0.4, 0.2] and a Gaussian covariance kernel with variance σ2 = 0.5 and range
parameters θ = [0.2, 0.4]. In both cases we assume a prior constant mean function. We
are interested in the set Γ∗ with t = 1. For both cases we consider k = 15 evaluations
of f at the same points chosen by Latin hypercube sampling. Figures 5a and 5b show
the conservative estimate at level 95% compared with the true excursion, the Vorob’ev
expectation and the 0.95-quantile for the Mat´ern and the Gaussian kernel.
In both
cases we see that the 0.95-quantile does not guarantee that the estimate is included in
the true excursion with probability 0.95. The conservative estimates instead are guar-
anteed to be inside the true excursion with probability α = 0.95. They correspond
to Vorob’ev quantiles at levels 0.998 and 0.993 for Mat´ern and Gaussian respectively.
The conservative estimates were obtained with a 100 × 100 discretization of the unit
square. Such high resolution grids lead to very high dimensional probability calcula-
tions. In fact, the dichotomy algorithm required 11 computations of the probability
1 − Pk(Qρ(cid:48) ⊂ {Zx ≤ t}) for each case. The discretization’s size for Qρ varied be-
tween 1213 and 3201 points in the Mat´ern kernel case and between 1692 and 2462
points in the Gaussian case. Such high dimensional probabilities cannot be computed
with the current implementation of the algorithm by Genz, however they could also
be computed with a standard Monte Carlo at very high computational costs. Instead,
with the proposed method, the total computational time on a laptop with Intel Core
i7 1.7GHz CPU and 8GB of RAM was equal to 365 and 390 seconds respectively for
Mat´ern and Gaussian kernel.

12

Conservative estimate at 95% (Matern kernel)Conservative estimate (95%)0.95-level setVorob'ev expectationTrue excursionConservative estimate at 95% (Gauss kernel)Conservative estimate (95%)0.95-level setVorob'ev expectationTrue excursion5 Discussion

In this paper we introduced a new method to approximate high dimensional orthant
Gaussian probabilities. The procedure resulted in estimators with greater efﬁciency
than standard Monte Carlo, scalable on dimensions larger than 1000. The remainder
Rq in the decomposition of Equation (2) was estimated with standard Monte Carlo and
with a novel asymmetric nested Monte Carlo (anMC) procedure. The anMC estimator
showed an increase in efﬁciency in the simulation studies presented. The efﬁciency of
the overall method depends on the general structure of the Gaussian vector and there
are situations where it brings only moderate improvements over a standard Monte Carlo
approach. A study of the relationships between the covariance structure and the efﬁ-
ciency of the method might be beneﬁcial for understanding this behaviour. The issue
of choosing q, the number of active dimensions, was not addressed here however fur-
ther studies in this direction might lead to a more automated estimation procedure. In

the application section we showed that the estimator(cid:98)pGanMC made possible the compu-

tation of conservative estimates of excursion sets with general Gaussian random ﬁeld
priors. All code was developed in R.

Acknowledgement

The ﬁrst author acknowledges the support of the Swiss National Science Foundation,
grant number 146354.

A Proofs
Proof of Proposition 1

(cid:123)(cid:122)

=(cid:4)

var((cid:98)p) = var((cid:98)pq) + var((1 − (cid:98)pq)(cid:99)Rq)
(cid:125)

Proof. We have that E[(cid:98)pq] = pq and E[(cid:99)Rq] = Rq. Then we have
(cid:123)(cid:122)

+2 cov((cid:98)pq, (1 − (cid:98)pq)(cid:99)Rq)
(cid:125)
(cid:124)
q var((cid:98)pq) + var((cid:98)pq) var((cid:99)Rq),
(cid:4) = var((1 − (cid:98)pq)(cid:99)Rq) = (1 − pq)2 var((cid:99)Rq) + R2
(cid:78) = cov[(cid:98)pq, (1 − (cid:98)pq)(cid:99)Rq] = − var((cid:98)pq)Rq,
respectively, by exploiting the independence of (cid:98)pq and (cid:99)Rq. By plugging in those ex-

We can write the variance (cid:4) and the covariance (cid:78) as

pressions in Equation (11) we obtain the result in Equation (3).

(cid:124)

.

(11)

=(cid:78)

Proof of Proposition 2

13

Proof.

var((cid:101)G) =

=

=

=

1

n2m2 var

m(cid:88)
(cid:20)

j=1

1

nm2

1

nm2

 =

 n(cid:88)
m(cid:88)
m(cid:88)
cov(cid:0)g(W1, Z1,j), g(W1, Z1,j(cid:48))(cid:1)

1
nm2 var

g(Wi, Zi,j)

j=1

i=1

j(cid:48)=1



g(W1, Z1,j)

 m(cid:88)

j=1

(cid:21)

m var(g(W1, Z1,1)) + m(m − 1) cov(g(W1, Z1,1), g(W1, Z1,2))

1

nm2 [m var(g(W1, Z1,1)) + m(m − 1)(cid:7)] .

(12)

where the ﬁrst equality is a consequence of the independence of W1, . . . , Wn and the
third equality is a consequence of the independence of Zi,j and Zi,j(cid:48) conditionally on
Wi. Moreover the covariance denoted by (cid:7) in (12) can be written as follows.

(cid:7) = E(cid:2) cov(g(W1, Z1,1), g(W1, Z1,2) | W1)(cid:3)
(cid:124)
(cid:125)
(cid:20)
= var(cid:0)E[g(W1, Z1,1) | W1](cid:1) = var(cid:0)g(W1, Z1,1)(cid:1) − E

=0 Z1,1,Z1,2 independent conditionally on W1

(cid:123)(cid:122)

+ cov(cid:0)E[g(W1, Z1,1) | W1], E[g(W1, Z1,2) | W1](cid:1)
(cid:125)
(cid:124)

(cid:123)(cid:122)

=var(E[g(W1,Z1,1)|W1])

var(cid:0)g(W1, Z1,1) | W1

(cid:1)(cid:21)

.

var((cid:101)G)(m) =

Equations (12) and (13) give the result (8).

Proof of Corollary 1

(13)

Proof. Denote with e = β(A−B), f = (α+c)(A−B)+βB, g = (c+α)B, h = Ctot,
then

em2 + f m + g

.

,

=

1
h

hm

∂m

(cid:105)

(cid:104)

(14)

tively

e − g
m2

∂ var((cid:101)G)

∂2 var((cid:101)G)

Observe that the ﬁrst and second derivatives of var((cid:101)G) with respect to m are respec-
The second derivative is positive for all m > 0 then var((cid:101)G) is a convex function for
m > 0 and the point of minimum is equal to the zero of ∂ var((cid:101)G)/∂m, which is
m =(cid:112)g/e = (cid:101)m.
Since var((cid:101)G) is convex in m, the integer that realizes the minimal variance is either
(cid:98)(cid:101)m(cid:99) or (cid:100)(cid:101)m(cid:101). By plugging in m = (cid:101)m − ε = (cid:112)g/e − ε and m = (cid:101)m − ε + 1 =
(cid:112)g/e − ε + 1 in Equation (14), we obtain the condition in (9).
Proof. First of all notice that the total cost of sampling (cid:98)G is Ctot = n(c + CZ|W ) =

Proof of Proposition 3

2g
hm3 .

∂m2

=

n(c + α + β). By isolating n in the previous equation we obtain n = Ctot
computations similar to those in Proposition 2 we obtain

c+α+β and, by

var((cid:98)G) =

c + α + β

Ctot

var(g(W1, Z1,1)) =

c + α + β

Ctot

A,

14

where A = var(g(W1, Z1,1)). In the following we will also denote B = E(cid:2) var(g(W1, Z1,1) |
W1)(cid:3) as in Corollary 1. Let us now substitute NCﬁx (m∗) in equation (8), thus obtaining
var((cid:101)G) =

(c + α + βm∗)Am∗ − (m∗ − 1)(c + α + βm∗)B

(m∗)2β(A − B) + m∗[(c + α)(A − B) + βB] + (c + α)B

Ctotm∗

A(c + α + β)m∗

2(α + c)B + m∗[(c + α)(A − B) + βB]

= var((cid:98)G)
= var((cid:98)G)
(cid:20)
1 − (m∗ − 2)(c + α)B + m∗β(B − A)

A(c + α + β)m∗

(cid:21)

A(c + α + β)m∗

var((cid:101)G) = var((cid:98)G)

where in (15) we substituted (m∗)2 from Corollary 1. By further rearranging the terms,
we obtain

,

(15)

= var((cid:98)G) [1 − η] .

Since A − B, B, c, β, α are always positive, then η < 1 for all m∗ > 0. Moreover
η > 0 if

m∗ >

2(α + c)B

(α + c)B + β(A − B)

.

B Choice of active dimensions
B.1 Basics
The choice of active dimensions Eq plays a crucial role in the approximation of p

because it determines the error (cid:98)pq−p. Since this error is always negative, we implement
(2013), optimally reduces the bias of (cid:98)pq as an estimator for p. Here we are not interested
(cid:98)pq − p is lowered.

in such optimal bias reduction, as we completely remove this error with a second step.
However we aim at fast heuristics methods to select Eq in such a way that the error

Selecting Eq such that P (max X q > t) is numerically maximized, as in Chevalier

procedures to select Eq that exploit this property.

The basic tool used here to select active dimensions is the excursion probability

function:

pt(i) = P (Xi > t) = Φ

(cid:32)

(cid:33)

.

µi − t(cid:112)Σi,i

This function is widely used in spatial statistics (Bolin and Lindgren, 2015) and Bayesian
optimization (Kushner, 1964; Bect et al., 2012). In our setting it can be used to iden-
tify the dimensions where we have a high probability of exceeding the threshold. The
indices that realize a high value for pt enable identifying dimensions that actively con-
tribute to the maximum. We propose the following methods.
Method 1: sample q indices with probability given by pt.
Method 2: sample q indices with probability given by pt(1 − pt).
These methods require only µ and Σ, and are thus very fast to compute. Both
methods were already introduced for sequential evaluations of expensive to evaluate
functions, see, e.g., Chevalier et al. (2014b).

Figure 6 shows a comparison of the estimates pq obtained with different methods to
select Eq. The two methods clearly outperform a random choice of active dimensions.

15

Figure 6: Distribution of (cid:98)pq

mensions.

G estimates obtained with different choices of active di-

B.2 Choice of active dimensions: spatial methods
In many situations the random vector X comes from a discretization of a Gaussian
random ﬁeld over a set of points E ⊂ Rl. Let us denote with e1, . . . , ed the points in
the discretization of E. In this case, we can exploit the spatial information to select the
active dimensions with the following two methods.

Method 3: select the ﬁrst dimension by sampling the index with probability given
by pt. The jth dimension is sampled from the set of indices with probability given by
ptδj, where

j(cid:89)

δj =

dist(ek, E \ {ek})

(j = 2, . . . , d)

k=2

with dist(ek, E\{ek}) denoting the vector of Euclidean distances between ek and each
point in E \ {ek}.
Method 4: select the ﬁrst dimension by sampling the index with probability given
by pt(1 − pt). The jth dimension is sampled from the set of indices with probability
given by pt(1 − pt)δj.

Methods 3 and 4 give better results than Method 1 and 2 when the probability p is
not too small because in such situation the function pt has several local maxima and
the spatial correction helps selecting dimensions in each of the modes of pt.

References

Abrahamson, I. (1964). Orthant probabilities for the quadrivariate normal distribution.

The Annals of Mathematical Statistics, 35(4):1685–1703.

Bayarri, M. J., Berger, J. O., Calder, E. S., Dalbey, K., Lunagomez, S., Patra, A. K., Pit-
man, E. B., Spiller, E. T., and Wolpert, R. L. (2009). Using statistical and computer
models to quantify volcanic hazards. Technometrics, 51(4):402–413.

16

0.000.050.101020304050benchmarkqProbabilityMethodBenchmarkpqG (Method 1)pqG (Method 2)Random choiceMethods to select active dimensionsBect, J., Ginsbourger, D., Li, L., Picheny, V., and Vazquez, E. (2012). Sequential
design of computer experiments for the estimation of a probability of failure. Stat.
Comput., 22 (3):773–793.

Bolin, D. and Lindgren, F. (2015). Excursion and contour uncertainty regions for
latent Gaussian models. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 77(1):85–106.

Bratley, P. and Fox, B. L. (1988). Algorithm 659: Implementing Sobol’s quasirandom

sequence generator. ACM Trans. Math. Software, 14(1):88–100.

Chevalier, C. (2013). Fast uncertainty reduction strategies relying on Gaussian process

models. PhD thesis, University of Bern.

Chevalier, C., Bect, J., Ginsbourger, D., Vazquez, E., Picheny, V., and Richet, Y.
(2014a). Fast kriging-based stepwise uncertainty reduction with application to the
identiﬁcation of an excursion set. Technometrics, 56(4):455–465.

Chevalier, C., Ginsbourger, D., Bect, J., and Molchanov, I. (2013). Estimating and
quantifying uncertainties on level sets using the Vorob’ev expectation and deviation
with Gaussian process models. In Uci´nski, D., Atkinson, A., and Patan, C., editors,
mODa 10 Advances in Model-Oriented Design and Analysis. Physica-Verlag HD.

Chevalier, C., Picheny, V., and Ginsbourger, D. (2014b). The KrigInv package: An
efﬁcient and user-friendly R implementation of kriging-based inversion algorithms.
Comput. Statist. Data Anal., 71:1021–1034.

Cox, D. R. and Wermuth, N. (1991). A simple approximation for bivariate and trivariate

normal integrals. International Statistical Review, 59(2):263–269.

Dickmann, F. and Schweizer, N. (2014). Faster comparison of stopping times by nested

conditional Monte Carlo. arXiv preprint arXiv:1402.0243.

French, J. P. and Sain, S. R. (2013). Spatio-temporal exceedance locations and conﬁ-

dence regions. Ann. Appl. Stat., 7(3):1421–1449.

Genz, A. (1992). Numerical computation of multivariate normal probabilities. J. Com-

put. Graph. Statist., 1(2):141–149.

Genz, A. and Bretz, F. (2002). Comparison of methods for the computation of
Journal of Computational and Graphical Statistics,

multivariate t probabilities.
11(4):950–971.

Genz, A. and Bretz, F. (2009). Computation of Multivariate Normal and t Probabilities.

Lecture Notes in Statistics 195. Springer-Verlag.

Genz, A., Bretz, F., Miwa, T., Mi, X., Leisch, F., Scheipl, F., Bornkamp, B., and
Hothorn, T. (2012). mvtnorm: Multivariate Normal and t Distributions. R package
version 0.9-9992.

Hajivassiliou, V., McFadden, D., and Ruud, P. (1996). Simulation of multivariate nor-
mal rectangle probabilities and their derivatives theoretical and computational re-
sults. Journal of econometrics, 72(1):85–134.

17

Hammersley, J. (1956). Conditional Monte Carlo.

3(2):73–76.

Journal of the ACM (JACM),

Hammersley, J. and Morton, K. (1956). A new monte carlo technique: antithetic vari-
In Mathematical proceedings of the Cambridge philosophical society, vol-

ates.
ume 52, pages 449–475. Cambridge Univ Press.

Horrace, W. C. (2005). Some results on the multivariate truncated normal distribution.

Journal of Multivariate Analysis, 94(1):209–221.

Kahn, H. (1950). Random sampling (Monte Carlo) techniques in neutron attenuation

problems–I. Nucleonics, 6(5):27–33.

Kahn, H. and Marshall, A. W. (1953). Methods of reducing sample size in monte carlo
computations. Journal of the Operations Research Society of America, 1(5):263–
278.

Kushner, H. J. (1964). A new method of locating the maximum point of an arbitrary
multipeak curve in the presence of noise. Journal of Basic Engineering, 86(1):97–
106.

Lemieux, C. (2009). Monte Carlo and quasi-Monte Carlo sampling. Springer.

Miwa, T., Hayter, A., and Kuriki, S. (2003). The evaluation of general non-centred
orthant probabilities. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 65(1):223–234.

Molchanov, I. (2005). Theory of Random Sets. Springer, London.

Moran, P. (1984). The monte carlo evaluation of orthant probabilities for multivariate
normal distributionsrlo evaluation of orthant probabilities for multivariate normal
distributions. Australian Journal of Statistics, 26(1):39–44.

Owen, D. B. (1956). Tables for computing bivariate normal probabilities. The Annals

of Mathematical Statistics, 27(4):1075–1090.

Picheny, V., Ginsbourger, D., Richet, Y., and Caplin, G. (2013). Quantile-based op-
timization of noisy computer experiments with tunable precision. Technometrics,
55(1):2–13.

Rasmussen, C. E. and Williams, C. K. (2006). Gaussian processes for machine learn-

ing. MIT Press.

Ridgway, J. (2014). Computation of gaussian orthant probabilities in high dimension.

arXiv preprint arXiv:1411.1314.

Robert, C. and Casella, G. (2013). Monte Carlo statistical methods. Springer.

Robert, C. P. (1995). Simulation of truncated normal variables. Statistics and comput-

ing, 5(2):121–125.

Sacks, J., Welch, W. J., Mitchell, T. J., and Wynn, H. P. (1989). Design and analysis of

computer experiments. Statist. Sci., 4(4):409–435.

18

Schervish, M. J. (1984). Algorithm AS 195: Multivariate normal probabilities with
error bound. Journal of the Royal Statistical Society. Series C (Applied Statistics),
33(1):81–94.

Tong, Y. L. (2012). The multivariate normal distribution. Springer.

19

