A Penalty Function Promoting Individual Sparsity

in Groups

˙Ilker Bayram and Savas¸kan Bulek

1

6
1
0
2

 
r
a

 

M
1
1

 
 
]

A
N
.
s
c
[
 
 

1
v
0
5
6
3
0

.

3
0
6
1
:
v
i
X
r
a

Abstract—We introduce a weakly-convex penalty function for
sparse signals with isolated non-zeros. We derive the threshold
function associated with the proposed penalty and discuss its
properties. We demonstrate the use of the penalty/threshold
functions in a convex denoising and a non-convex deconvolution
formulation. We provide convergent algorithms for both formu-
lations and compare the performance with well-known state-of-
the-art methods.

I. INTRODUCTION

Constraints or prior information derived from sparsity is
widely used in signal processing. Depending on the appli-
cation domain, the signal of interest may exhibit additional
features than mere sparsity. In this paper, we consider sparse
signals with isolated non-zeros. We propose a prior function
that speciﬁcally forces the non-zeros to be isolated and we
demonstrate how to use the function in basic inverse problems
of potential interest.

Many natural phenomena can be associated with a sparse
underlying process with isolated non-zero components. For
instance, the DFT coefﬁcients of a periodic signal are equidis-
tant with respect
to the frequency variable. Consequently,
quasi-periodic audio signals like speech, music can be rep-
resented in the time-frequency domain (via linear transforms
[3]) using time-frequency components (i.e., harmonics) that
appear isolated along the frequency axis. Another example is
related to reﬂection seismology, where one aims to discover
the subsoil layers by sending seismic waves and processing
the returning seismic trace [28]. The seismic trace can be
modelled as the convolution of the input seismic wave and
the reﬂection function. The reﬂection function is a sparse
signal with non-zeros occuring due to difference in acoustic
impedance between the boundaries of different layers. Since
the layers are expected to have some non-zero thickness, the
non-zeros, which occur at the boundaries, are isolated. Other
than these natural signals, isolated sparsity is also relevant
for designed systems. For instance,
in frequency hopping
systems, the parameters of the signal components are constant
in between the hopping instances, during which transmission
occurs [1]. Since transmission has to last for sometime, the
hopping instances may be modeled as sparse signals with
isolated non-zeros.

In order to isolate the non-zeros, we work with non-
overlapping groups of variables and enforce the solitude of the
non-zeros in each group. Assuming that the size of the groups

˙I. Bayram is with the Dept. of Electronics and Telecommunications Eng.,
Istanbul Technical University, Istanbul, Turkey. E-mail : ibayram@itu.edu.tr.
S. Bulek is with Qualcomm Atheros, Inc., Auburn Hills, MI, USA. E-mail :
sbulek@gmail.com.

is n, we let x(l) ∈ Cn denote the coefﬁcients belonging to the
lth group and deﬁne a penalty function as,

(1)

(2)

Pγ(x) =(cid:88)l
where for x ∈ Cn, Pγ is deﬁned as,
Pγ(x) = γ (cid:32)n−1(cid:88)i=1

Pγ(cid:0)x(l)(cid:1),
n(cid:88)m=i+1

|xi xm|(cid:33) + (cid:107)x(cid:107)1.

Given this penalty, we derive the associated threshold function
(or the proximity operator [9]) deﬁned as

Tλ,γ(z) = arg min

x

1
2(cid:107)z − x(cid:107)2

2 + λ Pγ(x).

(3)

We show that Tλ,γ is well-deﬁned when λ γ < 1 and study
its behavior. We also show that, as γ → 1/λ, the threshold
function suppresses all but the largest coefﬁcient in each group,
provided the magnitude of the largest coefﬁcient exceeds the
threshold λ. We demonstrate the use of the proposed penalty
and the threshold function in a convex denoising formulation
and a non-convex deconvolution formulation. We provide
convergent algorithms for both formulations and compare
the reconstructions with those obtained using other penal-
ties/threshold functions.

Related Work

The proposed penalty function may be regarded as a mem-
ber of the family of group-based penalty functions (see e.g.
[31], [16], [18], [15], [4], [26], [7], [22], [27] for a sample from
the literature). In contrast to our interest, many of these works
seek to set whole groups of coefﬁcients to zero, thus achieving
sparsity on a group level, and do not enforce sparsity within
groups. An important exception is the Elitist-Lasso (E-Lasso)
formulation [16], [18] (see also [32] where the method is
referred to as Exclusive-Lasso) where the target signal contains
few non-zeros within each group. The E-Lasso penalty is
obtained by summing the squares of the (cid:96)1 norm of each
group and it is convex, unlike the proposed penalty function.
Sparsity within groups is also addressed by the sparse-group
lasso (SGL) proposed in [27]. SGL uses a linear combination
of an (cid:96)1 norm and a mixed (cid:96)2,1 norm as the penalty – it may
also be interpreted as a sum of elastic-net-like penalties [33]
applied to each group. Therefore SGL uses a convex penalty
function. SGL was extended to non-overlapping groups and
its performance is thoroughly analyzed in [22].

Unlike these previous works, the penalty proposed in this
paper is non-convex. However, its degree of non-convexity
is controlled by the parameter γ and this in turn allows

to formulate convex problems. As will be clariﬁed in the
sequel (see the proof of Prop. 1), the proposed penalty can be
related to the E-Lasso penalty. However, the E-Lasso penalty
is convex and can be shown to contain an additive energy term,
which introduces bias in the reconstructed signal. Further, the
E-Lasso threshold never sets the whole group to zero, unless
the group is zero to start with (see [16], Remark-6). This in
turn means that if a group consists entirely of noise, it will
not be totally eliminated, even if it has components with small
magnitudes. The penalty function introduced in this paper is
non-convex and the associated threshold function contains a
deadzone such that if the coefﬁcients in the group fall in the
deadzone, the whole group is eliminated. As far as we are
aware, such an extension has not appeared in the literature.

Outline

We motivate the penalty function and derive the associated
threshold function in Section II. We discuss how the non-
convex penalty function may be employed to formulate a con-
vex denoising problem with a sparsifying frame and present
a minimization algorithm in Section III. In Section IV, we
discuss a non-convex deconvolution formulation. We discuss
the convergence of an iterative thresholding algorithm and
demonstrate its performance. Section V is the conclusion.

II. A WEAKLY CONVEX PENALTY

We deﬁne our penalty function to be separable with respect
to the groups. Thanks to separability, it sufﬁces to deﬁne a
penalty function and an associated threshold function with
domain Rn or Cn, with the proviso that the groups (whatever
their physical interpretation might be) have size n. That is, for
a length-n vector u (complex or real valued), if we deﬁne

Tλ,γ(u) = arg min

x

1
2(cid:107)u − x(cid:107)2

2 + λ Pγ(x),

(4)

then Tλ,γ(z) can be realized by applying Tλ,γ to each group
of z separately.

We start our discussion with penalty/threshold functions
deﬁned on Rn. However, before a general n value, we discuss
in Section II-A the case n = 2 which is easier to visualize
and interpret. After that, we generalize the discussion to Rn
in Section II-B. A discussion of how to tune the parameters
and a numerical demonstration of the discussions is provided
in Sec. II-C and Sec. II-D respectively. Finally, we discuss the
extension to Cn in Sec. II-E.

A. The Penalty and the Threshold Function on R2

1) The Penalty Function: For a ﬁxed energy x = (x1, x2),

we seek a penalty function P such that,
• if |x1| (cid:28) |x2| or |x2| (cid:28) |x1|, P (x) assumes a low value,
• if |x1| ≈ |x2|, P (x) assumes a high value.
Observe that ˜P = |x1 x2| satisﬁes these requirements. How-
ever, ˜P is exactly zero when one of the components is zero.
In order to penalize small non-zero components, we add an
(cid:96)1 term and propose the penalty
Pγ(x) = γ |x1 x2| + (cid:107)x(cid:107)1.

(5)

2

Fig. 1. Mesh and contour plots of the proposed Pγ (x1, x2). Notice that the
function is not convex.

where γ is a tuning parameter. Mesh and contour plots of this
function are shown in Fig. 1. We note that P is not convex but
it is weakly convex [29]. That is, for λ ≤ 1/γ, it can be shown
that the function ‘(cid:107)x(cid:107)2
2/2 + λ Pγ(x)’ is convex (discussed in
more detail in Sec. II-B). Therefore, γ may be regarded as a
parameter that controls how much the function deviates from
being convex. As a consequence of the weak-convexity of Pγ,
we ﬁnd that if λ < 1/γ, the function

Cλ,γ(z) =

1
2(cid:107)z − x(cid:107)2

2 + λ Pγ(x)

(6)

is strictly convex and has a unique minimizer, denoted as
Tλ,γ(z) (see (4)). Thus, Tλ,γ(z) is well-deﬁned when λ γ < 1.
Before we discuss the threshold function, we would like to
compare Pγ to the Elitist-Lasso (E-Lasso) penalty function,
which is known to favor large components in a group [16],
[17], [18]. For groups of size two, the E-Lasso penalty uses
PEL(x) = (cid:107)x(cid:107)2

1. Expanding this, we can write,

PEL(x) = 2|x1 x2| + (cid:107)x(cid:107)2
2,

(7)

Both penalties employ the term |x1 x2| and this term is
actually responsible for the elitist character of the penalties.
The difference lies in the remaining components. The proposed
penalty contains an additive (cid:96)1 term which helps enforce
sparsity if the components have small magnitudes. In contrast,
the E-Lasso penalty contains an additive energy term, which
renders the overall penalty convex but arguably introduces bias
in the reconstructions.

2) The Threshold Function: The threshold function Tλ,γ
can be derived via the optimality conditions for the minimiza-
tion problem

Tλ,γ(z) = arg min

x

Cλ,γ(x).

(8)

Let us assume that zi ≥ 0, i.e., z lies in the ﬁrst quadrant (ex-
tension to the other quadrants can be achieved by symmetry).
Let x = Tλ,γ(z). For the regions R1, R2, R3, R4, deﬁned in

−202−202x1x2Pγ(x1,x2)x1x2−2−1012−2−1012Fig. 2.
E-Lasso have constant gradients in the regions indicated above.

The threshold functions associated with the proposed penalty and

Fig. 3. The ﬁrst component of the proposed threshold and E-Lasso.

3

the proposed threshold, the E-Lasso threshold does not contain
a deadzone that eliminates both components.
B. The Penalty and the Threshold Function on Rn

We extend P to Rn as

Pγ(x) = γ (cid:32)n−1(cid:88)i=1

n(cid:88)m=i+1

|xi xm|(cid:33) + (cid:107)x(cid:107)1.

(9)

This function is not convex but it becomes convex if we add
a quadratic. Such functions are called weakly-convex [29].
Deﬁnition 1. A function f is said to be γ-weakly convex if
(10)

γ
2(cid:107)x(cid:107)2
is convex.
Proposition 1. The function Pγ : Rn → R in (9) is γ-weakly
convex. Consequently, a cost function of the form

2 + f (x)

Cλ,γ(x) =

1
2(cid:107)x − z(cid:107)2
is strictly convex if λ γ < 1.

2 + λ Pγ(x)

Proof: To see the ﬁrst claim, observe that,

n−1(cid:88)i=1

n(cid:88)m=i+1

2 + 2

|xi xm|.

1 = (cid:107)x(cid:107)2
(cid:107)x(cid:107)2
Since (cid:107)x(cid:107)2
1 is convex, this observation implies that the term
in the parentheses in (9) is 1-weakly convex. Since (cid:107)x(cid:107)1 is
convex, it follows that Pγ is γ-weakly convex.
To see that Cλ,γ is convex, note that it can be written as

(12)

(11)

(13)

Fig. 2a, x = (x1, x2) is given as follows.

x1 = z1 − λ
x2 = 0

(cid:41) , if z ∈ R1,
x2 = z2 − λ(cid:41) , if z ∈ R2,

x1 = 0

x1 =

x2 =

z1 − λ γ z2 − (1 − λ γ) λ

z2 − λ γ z1 − (1 − λ γ) λ

1 − λ2 γ2
1 − λ2 γ2

x1 = 0

x2 = 0(cid:41) if z ∈ R4.



the sum of a convex function and

1
2(cid:107)x(cid:107)2

2 + λ γ (cid:32)n−1(cid:88)i=1

n(cid:88)m=i+1

|xi xm|(cid:33) .

But the function in (13) is strictly convex if λ γ < 1. Thus it
follows that Cλ,γ is strictly convex if λ γ < 1.

, if z ∈ R3,

Since

Tλ,γ(z) = arg min

x

Cλ,γ(x),

(14)

Note that the regions are determined by the two parameters
λ and γ (see τ1 and τ2 in Fig. 2a). R4 is the deadzone for the
bivariate threshold function and is determined by the weight
λ. The ﬁrst component of the threshold function (i.e., the
mapping that takes z = (z1, z2) to x1) is shown in Fig. 3a.
Note that for this function, R2 (on which |z2| (cid:29) |z1|) is also
a deadzone.
We remark that the proposed threshold function behaves
quite differently than a threshold function derived from a
separable penalty of the form ‘p(x1) + p(x2)’. For such a
penalty, even if p is non-convex, the deadzone of the threshold
function is rectangular (and hence z1 does not depend on x2).
The relevant regions and the ﬁrst component of the E-
Lasso threshold are shown in Fig. 2b and Fig. 3b respectively.
Note that, unlike the proposed threshold (which contains four
different regions with constant gradient), the E-Lasso threshold
has three regions where its gradient is constant. Also, unlike

it follows from Prop. 1 that, when λ γ < 1, Tλ,γ is well-
deﬁned thanks to the strict convexity of Cλ,γ.

In the following, we will derive two ﬁnite-terminating
algorithms that realize Tλ,γ. For that, we discuss the properties
of Tλ,γ. We start by showing that Tλ,γ(z) shrinks z towards
zero and it is monotone in the sense that it preserves the
ordering of |zi| with respect to i. More precisely,
Proposition 2. Let x = Tλ(z).
(a) If zi ≥ 0, then zi ≥ xi ≥ 0. If zi ≤ 0, then zi ≤ xi ≤ 0.
(b) If |zi| > |zm|, then |xi| ≥ |xm|.
(c) If |zi| = |zm|, then |xi| = |xm|.

Proof: See the appendix.

We now derive an expression for Tλ using the optimality

conditions. Notice that Cλ,γ can be written as,

Cλ,γ(x) =

1
2(cid:107)z(cid:107)2
+

2 − (cid:104)z, x(cid:105)
1 − λ γ
(cid:107)x(cid:107)2

2

2 +

λ γ
2 (cid:107)x(cid:107)2

1 + λ(cid:107)x(cid:107)1.

(15)

RegionswithaConstantGradient(a)ProposedThresholdτ1τ2τ1τ2R3R4R1R2τ1=λτ2=(1−λγ)λz1z2O(b)E-LassoThresholds1s2R2R1R3s2=1/s11s1=λ/(1+λ)z1z2O(a)ProposedThreshold00.5100.5100.20.40.6z1z2x1(b)E-Lasso00.5100.5100.20.4z1z2x1Using this expression, the optimality conditions are found as,

(16)

(17)

where sign(x) is a set valued separable function of the vector
x, whose kth component is given as,

z ∈ (1 − λ γ)x + λ(cid:0)γ(cid:107)x(cid:107)1 + 1(cid:1) sign(x),
sign(x)k =

if xk > 0,
if xk = 0,
if xk < 0.

{1},
[−1, 1],
{−1},

In the following, we assume for simplicity that zi’s are
non-negative and ordered, i.e, z1 ≥ z2 ≥ ··· ≥ zn ≥ 0. The
general case can be recovered by a permutation of the vector
components and changing signs, thanks to Prop. 2.

(18a)

k(cid:88)m=1

In order to ﬁnd an expression for xi, let us deﬁne

xm + 1(cid:33) , if i ≤ k,

Prop. 2 implies that there is an index k ∈ {0, 1, . . . , n},
such that xi > 0 if i ≤ k and xi = 0 if i > k. For this special
integer k, the optimality conditions can be written as,
zi = (1 − λ γ)xi + λ(cid:32)γ
xm + 1(cid:33) , if i > k.
zi ≤ λ(cid:32)γ
k(cid:88)m=1
1k =
¯z =
¯x =


k ¯x + 1(cid:1) 1k.
¯z = (1 − λ γ) ¯x + λ(cid:0)γ1T

 ∈ Rk.

(20)
k 1k = k) and

We can now express (18a) as

k (noting 1T

Multiplying both sides by 1T
rearranging, we have

x1
x2
...
xk

z1
z2
...
zk

(18b)

1
1

...

1

(19)

,

,

1T
k ¯x =

Therefore,

1T
k ¯z − k λ

1 + (k − 1)λ γ

(21)

.

(22)

The rhs of (22) will be of interest in the following. Let us
therefore deﬁne, for each i,

λ (1 − λ γ) + λ γ (cid:80)k

1 + (k − 1) λ γ

j=1 zj

λ(cid:0)γ1T

h(i) =

k ¯x + 1(cid:1) =
λ (1 − λ γ) + λ γ (cid:80)i

1 + (i − 1) λ γ

m=1 zm

.

(23)

Plugging the expression in (22) back into (18), we ﬁnd the
equivalent conditions

−1(cid:0)zi − h(k)(cid:1),

if i ≤ k,
if i > k.

xi = (1 − λ γ)
zi ≤ h(k),

(24a)
(24b)
Notice that the requirement xi > 0 for i ≤ k implies that
zi > h(k) for i ≤ k.
The foregoing discussion assumes that the correct value of
k is known. If it is not, then we need to ﬁnd it by a search.
We will present two different search schemes for ﬁnding the

4

correct value of k. The following lemma will be useful for
that end.
Lemma 1. Suppose z1 ≥ z2 ≥ ··· zn ≥ 0 and λ γ < 1. Let
h(i) be deﬁned as in (23). Then,
(a) if zi+1 > h(i), then zi > h(i − 1),
(b) if zi ≤ h(i), then zi+1 ≤ h(i + 1).

Proof: See the appendix.

We can use this lemma as a guide for the selection of k. It
follows from the lemma that we can start from k = 0 and keep
increasing k until h(k) > zk+1. This procedure is summarized
in Algorithm 1.

Algorithm 1 Realization of Tλ,γ – Linear Search for k
Require: y ∈ Cn
z ← descending-sort(|y|)
k ← 0
while h(k) < zk+1 do

end while

k ← k + 1 {increment k}
xi ← (1 − λ γ)−1 (cid:18)1 −
h(k)

|yi| (cid:19)+

yi, for all i

If k is suspected to be small, then this algorithm terminates
quickly. In the worst case, the algorithm will execute the
‘while’ loop n times. If, however, n is large and k is not
expected to be small,
then a binary search for k might
be computationally more suitable. The following discussion,
that relies on Lemma 1 implies that such a binary search
terminates.

Suppose we pick an arbitrary i and check the following

conditions.

zi > h(i),
zi+1 ≤ h(i).

(25a)
(25b)
Notice that since zi ≥ zi+1, the conditions cannot be violated
simultaneously. Now observe that
• If both conditions hold, the current guess of i is equal to

• If (25a) holds and (25b) is violated, then by Lemma 1, k

the sought k.

must be greater than i.

• If (25b) holds and (25a) is violated,

then again by

Lemma 1, k must be less than i.

These observations lead to an implementation of Tλ,γ
as in Algorithm 2. In contrast to the O(n) complexity of

Algorithm 1, this algorithm has O(cid:0)log(n)(cid:1) complexity.

C. Tuning the Parameters of the Threshold Function

Let us now discuss some special cases to better understand
the role of the parameters λ and γ. As in the previous
subsection, we will assume that z1 ≥ ··· ≥ zn ≥ 0 and
x = Tλ(z).
Observe ﬁrst that h(0) = λ. If zi < λ for all i, then x = 0.
Thus the deadzone of the threshold function is a cube of width
λ in Rn.

Algorithm 2 Realization of Tλ,γ – Binary Search for k
Require: y ∈ Cn
z ← descending-sort(|y|)
ﬂag ← true
if z1 < λ then

5

else if zn > h(n) then

k ← 0
ﬂag ← false
k ← n
ﬂag ← false
k0 ← 0 {left end of the interval}
k1 ← n {right end of the interval}

else

end if
while ﬂag do

k ← (cid:98)(k0 + k1)/2(cid:99) {middle of the working interval}
if zk > h(k) and zk+1 ≤ h(k) then
ﬂag ← false {correct value of k is found}
else if zk ≤ h(k) then
k1 ← k {update the right end}
else
k0 ← k {update the left end}
|yi| (cid:19)+

xi ← (1 − λ γ)−1 (cid:18)1 −

yi, for all i

end while

end if

h(k)

Suppose now that z1 > λ. In that case, we will deﬁnitely

have x1 > 0. We compute

h(1) = λ + λγ(z1 − λ).

(26)
Notice that in order for x2 to be non-zero, the threshold
that z2 needs to exceed has increased from λ by an amount
proportional to (z1 − λ). The higher z1 is, the higher will
be the new threshold. In fact, observe that as λ γ → 1, the
threshold converges to z1. Since z2 ≤ z1, we can therefore
force only a single component to survive by choosing γ close
to 1/λ. When z2 < h(1), we ﬁnd that,

x1 = z1 − λ.

(27)
is obtained by soft

Thus the single surviving component
thresholding the largest component with λ.

The following proposition provides further information on

how the potential thresholds h(i) behave for arbitrary i.
Proposition 3. Suppose z1 ≥ z2 ≥ ··· zn ≥ 0 and
λ γ < 1. Let h(i) be deﬁned as in (23). If zi+1 > h(i), then
zi+1 > h(i + 1) > h(i).

Proof: See the appendix.

We know that if zi+1 > h(i), then h(i) is not the actual
threshold and k > i. Prop. 3 implies that h(k) is actually
greater than h(i), but it is bounded above by zi+1. In fact, we
can deduce from Prop. 3 that

z1 ≥ ··· ≥ zk > h(k) ≥ h(k − 1) ≥ ··· ≥ h(0) = λ. (28)
In the case where the observations are purely noise, we would
like to set xi = 0 for all i. This motivates choosing λ = cσ,

Fig. 4. Average SNR gains with respect to λ, γ, where λ is ﬁxed and γ is
varied. K denotes the number of non-zero components present in the clean
signal.

where σ denotes the noise standard deviation and c is a
constant around unity. Once we ﬁx the value of λ, the number
of non-zero components, k, and the threshold h(k) will depend
on γ (and z). The following proposition provides precise
bounds on γ.
Proposition 4. Suppose x = Tλ,γ(z). x1 ≥ ··· ≥ xk > 0 and
xk+1 = ··· = xn = 0 if and only if
(zk+1 − λ)+

(29a)

γ >

,

1
λ

γ <

1
λ

i=1 (zi − zk+1)

(zk+1 − λ)+ +(cid:80)k
(zk − λ)+ +(cid:80)k−1

(zk − λ)+

have

i=1 (zi − zk)
that

.

(29b)

Proof: We

and
xk+1 = ··· = xn = 0 if and only if zk > h(k) > zk+1. Using
the deﬁnition of h(k), we obtain (29) by rearranging these
inequalities.

x1 ≥ ··· ≥ xk > 0

Prop. 4 suggests that,

if we would like to retain more
components in x, then we need to choose a small γ. In order
to demonstrate this, and also to compare the performance of
the proposed threshold function, we next present a simple
experiment.

D. Numerical Experiment

The desired signal

is of length 10 and it has K non-
zero components, where the non-zero values are obtained by
sampling from a Gaussian distribution. We add Gaussian noise
to this signal so that the observation SNR is 5 dB.

We apply Tλ,γ to the observation for a ﬁxed λ = σ/2
and varying γ. We repeat the experiment for 10K trials to
obtain an average performance. The average gain in SNR
(dB) with respect to λ γ is shown in Fig. 4. We see from
the ﬁgure that the best λ γ value decreases with increasing
number of non-zero components in the clean signal, i.e., K.
This is in line with Prop. 4, which suggests that in order for
the reconstruction to have more non-zeros, the product λ γ
must be smaller.

In order to compare the performance of Tλ,γ with other
thresholds, we repeated this experiment for different input
SNRs. We speciﬁcally considered input SNRs to be 0, 5,
10 dB. For comparison, we employed the hard threshold,
soft threshold and the E-Lasso threshold [16]. Each of these

SNRGainwrtλγ00.10.20.30.40.50.60.70.802468λ γSNR Gain (dB)  K=1K=2K=3K=4K=5SNR GAINS FOR THE EXPERIMENT IN SEC. II-D

TABLE I

SNRin

0 dB

5 dB

10 dB

Hard Th.
Soft Th.
E-Lasso
Proposed
Hard Th.
Soft Th.
E-Lasso
Proposed
Hard Th.
Soft Th.
E-Lasso
Proposed

1

5.65
6.12
6.02
6.77
6.37
5.58
5.32
7.29
7.12
5.22
4.68
8.1

2
3.5
4.67
4.62
4.78
3.59
3.78
3.68
4.13
4.11
3.31
3.1
3.79

K
3

2.38
3.93
3.9
3.96
2.16
2.93
2.87
3.06
2.52
2.4
2.29
2.63

4

1.64
3.44
3.43
3.45
1.28
2.32
2.28
2.37
1.54
1.78
1.71
1.9

5

1.13
3.06
3.06
3.06
0.638
1.85
1.84
1.88
0.825
1.31
1.28
1.38

threshold functions require a single parameter controlling the
threshold (see [16] for the role of the weight in E-Lasso).
This threshold value is selected by a sweep search so as to
maximize the average output SNR in each case. In contrast, for
the proposed threshold Tλ,γ, we need to select two parameters,
namely λ and γ. If σ denotes the noise standard deviation, we
set λ to be σ for input SNR = 0 dB, σ/2 for input SNR = 5 dB
and σ/4 for input SNR = 10 dB. We note that these values are
not optimal but they produced good estimates. Once λ is set,
we select γ by a sweep search. In this setting, the gains in SNR
in dB for the different methods and varying K are tabulated in
Table I. The hard and soft thresholds perform well under high
and low input SNRs respectively. However, in many of the
cases, the proposed threshold function returns a higher output
SNR. We see that the performance of the proposed threshold
function is especially high for K = 1.

E. Extension to Cn
For x ∈ Cn, we extend Pγ straightforwardly as,

P c

γ (x) = γ 

n−1(cid:88)i=1

n(cid:88)j=i+1

|xi xj| + (cid:107)x(cid:107)1.

The threshold function is similarly deﬁned as,

T c
λ,γ(z) = arg min
x∈Cn

1
2(cid:107)x − z(cid:107)2

2 + λ Pγ(x).

(30)

(31)

Fortunately, the threshold function derived for Rn applies for
Cn with a little modiﬁcation. The following observation is
useful for showing that.
Lemma 2. Suppose z ∈ Cn and x = T c
then arg(zk) = arg(xk).

λ,γ(z). If |xk| > 0,
Proof: Suppose not. Deﬁne ˜x such that |˜xk| = |xk| for
γ (˜x) =
2, contradicting the fact that x

all k and for |˜xk| > 0, set arg(˜xk) = arg(zk). Then, P c
γ (x) but (cid:107)z − ˜x(cid:107)2
P c
minimizes the cost in (31).

2 < (cid:107)z − x(cid:107)2

With the help of this lemma, we obtain an expression for
λ,γ in terms of Tλ,γ.
T c

6

λ,γ(|z|),

λ γ(z) and u = Tλ,γ(|z|). Then, xk = uk ej arg(zk).
Proof: Notice that

Proposition 5. Suppose z ∈ Cn. Let |z| denote the vec-
tor containing the magnitudes of the components of z. Let
x = T c
|zk| = zk e−j arg(zk). Using this
observation, it can be shown by a change of variables that
then ˜xk = xk e−j arg(zk). Now since
if ˜x = T c
arg(|zk|) = 0, for all k, it follows by Lemma 2 that ˜xk are
real and non-negative. Thus, for the input |z|, we can restrict
the minimization in (31) to Rn. Thus ˜x = Tλ γ(|z|) = u and
the claim follows.
It follows from this proposition that the threshold function
on Cn can be realized by applying Tλ,γ to the magnitudes
of the input, followed by a correction of the argument of the
complex number. For this reason, in the following, we will
not differentiate between Tλ,γ and T c

λ,γ.

III. APPLICATION-I : CONVEX DENOISING WITH A

SPARSIFYING FRAME

We now consider the application of the proposed penalty in
a denoising problem, when a sparsifying frame is given. We
will speciﬁcally seek a convex formulation for this problem.

A. A Convex Denoising Formulation

Let y be a noisy observation of a clean signal ˆx for which
a sparsifying frame is given. We assume that the frame is
tight. Let S and S∗ denote the analysis and synthesis operators
for the frame [8]. We have two choices for formulating
the denoising problem, namely synthesis and analysis prior
formulations [12], [24]. The two behave quite differently under
a non-convex penalty such as the one considered in this paper.
In the setting described above, the synthesis prior denoising

formulation is,

min

t

∗

1
2 (cid:107)y − S

t(cid:107)2
2 + λ Pγ(t).

(32)

If we denote the minimizer as ˆt, the denoised signal is given
as x = S∗ ˆt. If S is overcomplete, this problem is not convex
when λ γ > 0. Therefore the synthesis prior problem is convex
only for γ = 0, for which Pγ is equivalent to an (cid:96)1 norm. This
leads us to consider the analysis prior formulation given as,

1
2 (cid:107)y − x(cid:107)2

x

min

2 + λ Pγ(S x).

(33)
Proposition 6. Suppose S is the analysis operator of a tight
frame. The cost function in (33) is convex if λ γ ≤ 1.
Proof: Since the frame is tight, we have (cid:107)Sx(cid:107)2
Therefore the cost function in (33) can be written as,

2.
2 = (cid:107)x(cid:107)2

2 + λ Pγ(S x)

=

2 +

1
1
2 (cid:107)y − x(cid:107)2
2(cid:107)x(cid:107)2
2 −
1
2 (cid:107)y(cid:107)2
2 − (cid:104)x, y(cid:105)
(cid:124)
(cid:123)(cid:122)

1
2(cid:107)Sx(cid:107)2
1
2(cid:107)Sx(cid:107)2
+
(cid:124)
(cid:125)

f1(x)

f2(Sx)

(cid:123)(cid:122)

(cid:125)

2 + λ Pγ(S x)

.

(34)

Here, f1 is an afﬁne function and is therefore convex. The
function f2(x) is convex when λ γ ≤ 1, by Prop. 1. Since pre-
composition with a linear operator preserves convexity [14],
˜f (x) = f2(Sx) is also convex. Thus the cost function in (33)
can be expressed as the sum of two convex functions and is
therefore convex.

1) The Douglas-Rachford Algorithm:

In order to obtain
a minimizer of (33), we will adapt the Douglas-Rachford
algorithm [20], [11], [9]. The Douglas-Rachford algorithm is
suitable for minimization problems of the form

min

t

f (t) + g(t),

(35)

where both f and g are convex. The Douglas-Rachford itera-
tions for such a problem are,

tk+1 =

1
2

tk +

1

2(cid:0)2Jαf − I(cid:1)(cid:0)2Jαg − I(cid:1) tk,

where α > 0 is a parameter and Jαf , Jαg are proximity
operators associated with f and g. Recall that for a convex
function h, the proximity operator is deﬁned as [9]

(36)

Jαh(z) = arg min

t

1
2α(cid:107)z − t(cid:107)2

2 + h(t).

(37)

The sequence tk constructed in (36) converges to some t∗ such
that Jαg(t∗) minimizes (35).

2) Adapting the Douglas-Rachford Algorithm: The prob-
lem in (33) is not readily suitable for the application of the
Douglas-Rachford algorithm. We now transform the problem
to write it in a suitable form.
Since S∗ S = I, we have
2 = (cid:107)Sy − Sx(cid:107)2
2.

(38)
Now if R(S) denotes the range of S, we can change variables
and obtain a problem equivalent to (33) as,

(cid:107)x − y(cid:107)2

2 + λ Pγ(u)

+ iR(S)(u)
,

(39)

min

u

1
2 (cid:107)S y − u(cid:107)2
(cid:123)(cid:122)
(cid:124)

f (u)

g(u)

(cid:124) (cid:123)(cid:122) (cid:125)

(cid:125)

where iR(S) is the indicator function of R(S) [14]. If u∗
denotes a minimizer of (39), then S∗ u∗ minimizes (33). In this
formulation, both f and g are convex, provided that λ γ ≤ 1.
Thus the Douglas-Rachford algorithm is applicable for this
splitting. We remark that in this setting, the proximity operator
for g = iR(S) is simply a projection onto R(S) (see e.g.
[9]), which can be achieved by applying S S∗, thanks to the
tightness of the frame. The proximity operator for f can be
expressed in terms of the threshold function as follows.

Jαf (z)

= arg min

u

1
2α(cid:107)z − u(cid:107)2

2 +

1
2(cid:107)Sy − u(cid:107)2

2 + λ Pγ(u)

u

1

α

= arg min

2(cid:13)(cid:13)(cid:13)(cid:13)u −
= Tβ λ,γ(cid:18)β (cid:16)Sy +

α + 1(cid:16)Sy +
α(cid:17)(cid:19),

z

2

2

z

α(cid:17)(cid:13)(cid:13)(cid:13)(cid:13)

(40a)

+

α

α + 1

λ Pγ(u)
(40b)

(40c)

where β =

α

.

α + 1

Resulting pseudocode for the Douglas-Rachford iterations

for this problem is given in Algorithm 3.

7

Algorithm 3 Analysis Prior Denoising Algorithm
Initialize α > 0, t.
Set β ← (1 + α)−1 α
repeat
u ← S S∗ t
z ← Tβ λ,γ(cid:16)β(cid:0)Sy + α−1(2u − t)(cid:1)(cid:17)
t ← z + t − u
until convergence
← S∗ t
x∗

Fig. 5. Deﬁnition of the groups in the time-frequency lattice used in the
denoising experiment in Section III-B. The parameters l and w denote the
length along the time axis and the width along the frequency axis respectively.

B. Numerical Experiment
now demonstrate

We

denoising

how the

formula-
tion/algorithm performs on an audio signal and compare it
against formulations that employ different regularizers. The
clean signal is a speech signal, sampled at 16 Khz, whose
spectrogram is shown in Fig. 6a. We use the short-time Fourier
transform (STFT) as the tight frame in this experiment. The
window size is 60 ms (960 samples) and the hop-size is 15 ms
(240 samples). For the penalty/threshold function, we select
the groups in the time-frequency plane as shown in Fig. 5. We
select the length along the time axis as l = 1 and the width
along the frequency axis as w = 16. This covers a frequency
bandwidth of 320 Hz. Our aim is to exploit the isolated appear-
ance of the harmonics viewed along the frequency axis. We
remark that it might also be of interest to consider extensions
along the time-axis as in [17] but our primary purpose here is
to compare the proposed penalty/threshold function and show
how its behavior differs from other formulations.

We produce a noisy observation by adding the ambient
noise shown in Fig. 6b to this signal. Notice that energy
of the ambient noise is not uniform over the frequencies
and decreases with increasing frequency. The input SNR is
5 dB. The spectrogram of the noisy signal
is shown in
Fig. 7a. For this observation, we perform denoising using
three different regularizers in the formulation (33), namely
the (cid:96)1 norm, the E-Lasso penalty and the proposed penalty.
The denoising algorithms for the (cid:96)1 norm and E-Lasso can
be obtained by replacing Tλ,γ with the soft threshold and the
E-Lasso threshold in Algorithm 3. The value of the regularizer
weight λ is selected by a sweep search for the (cid:96)1 norm
and the E-Lasso (the maximizing value is chosen). For Tλ,γ
we set λ to be half the value of λ used for the (cid:96)1 norm
and perform a sweep search for γ subject
to γ < 1/λ,
to maximize the output SNR. The resulting reconstructions
are shown in Fig. 7 b,c,d for the three methods. The SNR
gains are 6.42 dB, 6.54 dB, 6.67 dB for (cid:96)1 regularization, E-

TimeFrequencylw(cid:26)(cid:8)8

convolution operator, we forgo convexity and consider a non-
convex formulation. We provide an algorithm for the provided
formulation and discuss its convergence properties. We also
compare the performance of the penalty/threshold function
with a state-of-the-art iterative thresholding method.

A. A Non-Convex Formulation and a Convergent Algorithm

Consider a minimization formulation as

x {C(x) =
min

1
2(cid:107)y − H x(cid:107)2

2 + λ Pγ(x)},

(41)

where H denotes a convolution operator and P is the proposed
penalty. We remark that if H is not invertible, then C(x) may
not be convex unless γ = 0. But if γ = 0, Pγ is the (cid:96)1
norm. For this reason, unlike Section III, we will not restrict
the cost function to be convex in this section. We employ the
forward-backward splitting algorithm (FBS) [10], [9], [2] for
obtaining a local minimizer of (41). In the current context,
FBS constructs a sequence deﬁned as,

Fig. 6. Spectrograms of (a) the clean speech and (b) the noise signal used
in the analysis prior denoising experiment.

(42)

xk+1 = Tα λ,γ(cid:0)xk − αH T (Hxk − y)(cid:1).

We remark that Tα λ,γ is well-deﬁned when α λ γ < 1. This
sets an upper bound on α. If, in addition, α < 1/σ(H),
where σ(H) denotes the spectral norm of H, it can also be
shown using majorization-minimization techniques [19] that
the sequence in (42) monotonically decreases the cost, i.e.,
C(xk+1) < C(xk). Attouch et al. show in [2] that under the
additional conditions,

(i) P is a Kurdyka-Lojasiewicz function ([2], Defn. 2.4),
(ii) xk’s form a bounded sequence,
the algorithm (42) converges. Both of these are satisﬁed for our
setup. We ﬁrst remark that the proposed penalty function Pγ is
continuous and for each orthant in Rn, it can be expressed as
a polynomial function. Therefore Pγ (therefore Pγ) is semi-
algebraic (see Defn. 2.1 in [2]) and hence is a Kurdyka-
Lojasiewicz function (see the discussion at the end of Sec. 2.2
in [2]). Also, the proposed penalty is coercive, i.e., Pγ(x)
increases without bound as (cid:107)x(cid:107)2 increases. Therefore C(x) is
also coercive and any sequence that monotonically decreases
C(x) lies in a bounded set. To summarize, the following
proposition is a corollary of Thm. 5.1 of [2].
Proposition 7. If α < min(σ(H), 1/(λ γ)),
then xk’s de-
crease the cost C(x) monotonically, and converge to a local
minimizer.

B. Numerical Experiment

In exploration seismology, the goal is to estimate an un-
known reﬂectivity signal x ∈ Rn from the observed seismic
trace y ∈ Rn, which is modeled as,

y = h ∗ x + w,

(43)

Fig. 7.
Spectrograms of (a) the noisy signal and the reconstructions using
(b) the (cid:96)1 norm and (c) E-Lasso penalty, (d) the proposed n-variate penalty.

Lasso and the proposed regularization respectively. Although
the SNRs are close to each other, the reconstructions show
different behaviors. Both (cid:96)1 regularization and the proposed
regularization have been successful in removing noise in the
time-frequency regions with no activity. However since E-
Lasso always keeps a component within each group, it has
been less successful in suppressing noise in silent regions. We
see that especially for higher frequencies, (cid:96)1 regularization
suppresses the harmonics of the speech signal. In contrast,
the proposed penalty admits a smaller weight λ and is able
to retain high frequency harmonics, while achieving a similar
suppression of noise as (cid:96)1 regularization.

IV. APPLICATION II : DECONVOLUTION

In a second application, we consider a sparse deconvo-
lution problem. In order to be able to handle an arbitrary

where h represents the seismic wavelet and w denotes white
noise. Denoting the convolution operator with h as H, we can
use the formulation in (41) to estimate x from y. We will
assume that the seismic wavelet, h is known. Speciﬁcally, we

(a)CleanSpeechTime (seconds)Frequency (kHz)0123400.511.522.5(b)AmbientNoiseTime (seconds)Frequency (kHz)0123400.511.522.5(a)NoisyObservationTime (seconds)Frequency (kHz)0123400.511.522.5(c)E-LassoReconstructionTime (seconds)Frequency (kHz)0123400.511.522.5(b)ℓ1RegularizationTime (seconds)Frequency (kHz)0123400.511.522.5(d)ProposedRegularizationTime (seconds)Frequency (kHz)0123400.511.522.59

SRER PERFORMANCE COMPARISON FOR DECONVOLUTION

TABLE II

SNRin

Method

E(SRER)

σ(SRER)

5 dB

10 dB

15 dB

20 dB

SGL
IPS

Proposed

SGL
IPS

Proposed

SGL
IPS

Proposed

SGL
IPS

Proposed

9.98
9.16
11.32
14.62
15.25
16.53
19.43
20.86
21.57
24.32
23.66
26.67

0.84
2.21
1.18
0.87
3.17
1.17
0.89
3.42
1.19
0.90
2.09
1.16

Fig. 9.
Reliability plots comparing the proposed algorithm, iterative p-
shrinkage (IPS) [30] and the sparse group lasso formulations. Each ﬁgure
shows the signal to reconstruction error with respect to iterations. In each
case, the means are marked with white circles. The other lines indicate three
times the standard deviation from the means for each method.

We selected p = −1/2, which gave fairly good results. The
parameter λ is the threshold value and is selected with a sweep
search.

Finally, for Pγ, we use the same groups as SGL. We set

γ = 0.9/λ and select λ with a sweep search.

We considered four different input SNRs (5, 10, 15, 20 dB)
and evaluated the deconvolution performance using signal to
reconstruction error ratio (SRER), (cid:107)x(cid:107)2/(cid:107)x − ˆx(cid:107)2. For each
input SNR value, we repeat the experiment for 500 different
noise realizations to obtain average and standard value statis-
tics of the performance. We set α to be near the upper bound
allowed in Prop. 7. We remark that the proposed formulation
and IPS are essentially non-convex formulations but we have
seen that both algorithms converge in our experiments (as
claimed by Prop. 7 and in [30]).

We observed an interesting trend with respect to different
input SNRs. For low input SNRs, the proposed formulation
in terms of
performs better than the other two methods,

Fig. 8. Signals from the deconvolution experiment. (a) The seismic wavelet
which is assumed to be known, (b) the sparse reﬂectivity signal, (c) observed
noisy seismic trace.

experiment with the band-pass Ricker wavelet (dominant fre-
quencies in the range 10 ∼ 40 Hz), sampled at fs = 300 Hz,
which is shown in Fig. 8a.
We use a synthetic sparse reﬂectivity signal for this experi-
ment. The signal is selected by sampling a stochastic process
where the probability of observing a non-zero at a speciﬁc
sample is 0.1, provided that a non-zero has not occured in the
last 10 samples. The value of the non-zero sample is obtained
by sampling a normal distribution. Notice that, this process
is not a sparse Bernoulli process but a Markov process due
to the dependence on the past. The resulting synthetic x, of
length N = 512 is shown in Fig. 8b. The observed seismic
trace y, generated according to (43) is shown in Fig. 8c. We
used zero-mean white Gaussian noise to produce y. The input
SNR for this observation is 5 dB.

We compared the performance of the proposed algorithm
with the sparse-group lasso formulation (SGL) [27] and the
iterated p-shrinkage (IPS) algorithm [30], which was observed
to perform very well for sparse deconvolution (see e.g. the
comparisons in [25]).

SGL aims to achieve sparsity within groups and uses as
few groups as possible for reconstruction. The SGL penalty is
given as,

PSGL(x) = β (cid:107)x(cid:107)1 + (1 − β)(cid:88)l

(cid:107)x(l)(cid:107)2,

(44)

where β ∈ (0, 1) and (cid:107)x(l)(cid:107)2 denotes the (cid:96)2 norm of the lth
group. Replacing Pγ with PSGL in (41), we obtain the SGL
formulation. We set β = 0.95 as in [27] and make a sweep
search for selecting λ. The groups consist of neighboring
intervals of length 8.

IPS employs a threshold function depending on two param-
eters, namely λ and p. The parameter p determines the shape
of the threshold function and deﬁnes a family of functions that
lie between the soft (p = 1) and the hard threshold (p → −∞).

(a)SeismicWavelet−20−15−10−50510152000.51(b)Sparsereﬂectivitysignal50100150200250300350400450500−1012(c)Seismictrace50100150200250300350400450500−1012Proposedvs.SGL(a)InputSNR=5dB500100015002000051015IterationsSRER (dB)  ProposedSGL(c)InputSNR=15dB100020003000400050000510152025IterationsSRER (dB)  ProposedSGLProposedvs.IPS(b)InputSNR=5dB500100015002000051015IterationsSRER (dB)  ProposedIPS(d)InputSNR=15dB100020003000400050000102030IterationsSRER (dB)  ProposedIPSaverage SRER. As the input SNR increases, the best SRER
achieved with IPS surpasses those of the proposed formulation
and SGL. However, the performance of IPS varies a lot with
respect to different trials. On average, the proposed formula-
tion performs better than both methods. Also, the proposed
threshold function requires fewer iterations to fairly converge.
In order to visualize these, we show in Fig. 9 the SRER perfor-
mance with respect to iterations for input SNRs 5 and 15 dB.
Here, in addition to average SRER, three times the standard
deviation of the SRER with respect to iterations is also shown.
For the limits, the average SRER and the standard deviation of
the SRER for the different algorithms are tabulated in Table II.
In conclusion, the proposed penalty/formulation yields a better
average SRER over SGL, which targets a similar property.
However, its performance is less consistent with respect to
SGL as seen by a comparison of σ(SRER. On the other hand,
the proposed method is favorable compared to IPS. Especially
for high input SNRs, the average SRER returned by IPS can
be higher but the stability is poorer. We think that this partly
due to the clean sparse reﬂectivity signal which enforces a
certain distance between the zeros. This ‘prior information’ is
taken into account by the proposed formulation but the IPS,
which performs very well for arbitrary sparse signals, does not
make use of this information.

V. CONCLUSION

We proposed a group separable penalty function suitable
for sparse signals with isolated non-zeros. We derived an
associated threshold function Tλ,γ and studied how it behaves
as its parameters vary. We argued that a good strategy is to
choose λ proportional to the noise standard deviation and γ
according to how many non-zero coefﬁcients are expected in
each group. Speciﬁcally, as γ → 1/λ, we showed that in each
group, there remains at most one non-zero coefﬁcient (when
the largest coefﬁcient exceeds the threshold λ).

We think that the proposed penalty/threshold would be of
interest in several areas, such as EEG source localization [13],
seismic deconvolution [28], [23], audio processing, speciﬁcally
decomposing audio signals into transient and tonal compo-
nents [18], [5], low-rank matrix recovery [6], [21] with a
bound on the rank. We hope to consider such applications
in future work.

Acknowledgement

We thank Pavel Rajmic, Brno University of Technology, Czech
Republic, for discussions and comments.

APPENDIX A

PROOF OF PROP. 2

Recall that the cost function is deﬁned as,

Cλ,γ(u) =

1
2(cid:107)u − z(cid:107)2

2 + λ Pγ(u),

(45)

and x = Tλ,γ(z) is the minimizer of Cλ,γ(u).
(a) Let zi ≥ 0.

10

Assume xi > zi. Deﬁne a new vector ˆx as

ˆxi =(cid:0)zi − (xi − zi)(cid:1)+,
ˆxm = xm, if m (cid:54)= i.
2 ≤ (cid:107)x− z(cid:107)2

(46)
(47)
Then, (cid:107)ˆx− z(cid:107)2
2 and P (ˆx) < P (x). Therefore,
Cλ,γ(ˆx) < Cλ,γ(x). In words, ˆx achieves a strictly lower
cost than x∗, which is a contradiction. Thus we must have,
xi ≤ zi.
Assume xi < 0. Deﬁne a new vector ˆx as

ˆxi = 0,
ˆxm = xm, if m (cid:54)= i.
2 < (cid:107)x− z(cid:107)2

(48)
(49)
2 and P (ˆx) < P (x). Therefore,
than x, which is a

Then, (cid:107)ˆx− z(cid:107)2
ˆx achieves a strictly lower cost
contradiction. Thus we must have, xi ≥ 0.
The second part of the claim follows similarly.

(b) By part (a), we can assume without loss of generality that
z has non-negative components. Assume zi > zm ≥ 0.
Note that by part (a), xi ≥ 0, xm ≥ 0. Suppose now that
xi < xm. Deﬁne a new vector ˆx as

ˆxi = xm,
ˆxm = xi,
ˆxl = xl, if l (cid:54)= i or l (cid:54)= m.

(50)
(51)
(52)

Then, Pγ(x) = Pγ(ˆx). We obtain after some algebraic
manipulations that

1

2(cid:0)(cid:107)x − z(cid:107)2

= − (zm − zi) (xm − xi) > 0.

2(cid:1)
2 − (cid:107)ˆx − z(cid:107)2
From these, it follows Cλ,γ(ˆx) < Cλ,γ(x), which is a
contradiction. Thus xi ≥ xm.
(c) Without loss of generality, assume z has non-negative
components. Assume zi = zm ≥ 0. By part (a), we will
have xi ≥ 0, xm ≥ 0. Suppose now that xi > xm ≥ 0.
Set a = (xi + xm)/2 and deﬁne a new vector ˆx as

(53)

First, we have,

1

ˆxi = ˆxm = a,
ˆxl = xl, if l (cid:54)= i or l (cid:54)= m.
2(cid:1) =
2(cid:0)(cid:107)x − z(cid:107)2
2 − (cid:107)ˆx − z(cid:107)2
Pγ(x) − Pγ(ˆx) = γ(xi xm − a2).

x2
i + x2
m

2

(54)
(55)

(56)

(57)

− a2.

Also,

Using these, we obtain, after some algebraic manipula-
tions, that

Cλ,γ(x) − Cλ,γ(ˆx) = (1 − λγ)(a2 − xixm).

(58)

Since the algebraic mean is greater than the geometric
mean, we must have a > √xixm. Also, since λγ < 1,
it follows that Cλ,γ(x) − Cλ,γ(ˆx) > 0. Thus ˆx achieves
a lower cost than x, which is a contradiction. Therefore
xi ≤ xm. Changing the roles of the indices m and i, we
must also have xm ≤ xi. Therefore, xi = xm.

APPENDIX B

PROOF OF LEMMA 1

then h(i) = a/b and h(i + 1) = (a + c)/(b + d). But we have,
by (64)

11

a d
b c

=

λ (1 − λ γ) + λ γ (cid:80)i

(1 + (i − 1) λ γ) zi+1

j=1 zj

< 1.

(74)

.

(59)

Thus h(i + 1) > h(i).

REFERENCES

[1] D. Angelosante, G. B. Giannakis, and N. D. Sidiropoulos. Sparse
parametric models for robust nonstationary signal analysis. IEEE Signal
Processing Magazine, 30(6):64–73, November 2013.

[2] H. Attouch, J. Bolte, and B. F. Svaiter. Convergence of descent methods
for semi-algebraic and tame problems: proximal algorithms, forward-
backward splitting, and regularized GaussSeidel methods. Mathematical
Programming, 137(1):91–129, February 2013.

[3] P. Balazs, M. D¨orﬂer, M. Kowalski, and B. Torr´esani. Adapted and
adaptive linear time-frequency representations: A synthesis point of
view. IEEE Signal Processing Magazine, 30(6):20–31, November 2013.
[4] ˙I. Bayram. Mixed-norms with overlapping groups as signal priors. In
Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Proc. (ICASSP),
2011.

[5] ˙I. Bayram and ¨O. D. Akyıldız.

Primal-dual algorithms for audio
decomposition using mixed norms. Signal, Image and Video Processing,
8(1):95–110, January 2014.

[6] E. J. Cand´es and B. Recht. Exact matrix completion via convex
optimization. Foundations of Computational Mathematics, 9(6):717–
772, December 2009.

[7] P.-Y. Chen and I. W. Selesnick.

shrink-
age/thresholding of group sparse signals. Signal Processing, 94:476–
489, January 2014.

Translation-invariant

[8] O. Christensen. An Introduction to Frames and Riesz Bases. Birkh¨auser,

2003.

[9] P. L. Combettes and J.-C. Pesquet. Proximal splitting methods in signal
processing.
In H. H. Bauschke, R. S. Burachik, P. L. Combettes,
V. Elser, D. R. Luke, and H. Wolkowicz, editors, Fixed-Point Algorithms
for Inverse Problems in Science and Engineering. Springer, New York,
2011.

[10] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-
backward splitting. SIAM J. Multiscale Model. Simul., 4(4):1168–1200,
November 2005.

[11] J. Eckstein and D. P. Bertsekas. On the Douglas-Rachford splitting
method and the proximal point algorithm for maximal monotone oper-
ators. Mathematical Programming, 55(3):293–318, 1992.

[12] M. Elad, P. Milanfar, and R. Rubinstein. Analysis versus synthesis in

signal priors. Inverse Problems, 23(3):947–968, June 2007.

[13] A. Gramfort and M. Kowalski. Improving M/EEG source localization
with an inter-condition sparse prior. In Proc. International Symposium
on Biomedical Imaging (ISBI), 2009.

[14] J.-B. Hiriart-Urruty and C. Lemar´echal.

Fundamentals of Convex

Analysis. Springer, 2004.

[15] L. Jacob, G. Obozinsky, and J. P. Vert. Group lasso with overlap and

graph lasso. In Proc. Int. Conf. Machine Learning (ICML), 2009.

[16] M. Kowalski. Sparse regression using mixed norms. J. of Appl. and

Comp. Harm. Analysis, 27(3):303–324, November 2009.

[17] M. Kowalski, K. Siedenburg, and M. D¨orﬂer. Social sparsity! Neighbor-
hood systems enrich structured shrinkage operators. IEEE Trans. Signal
Processing, 61(10):2498–2511, May 2013.

[18] M. Kowalski and B. Torr´esani. Sparsity and persistence: Mixed norms
provide simple signal models with dependent coefﬁcients. Signal, Image
and Video Processing, 3(3):251–264, 2009.

[19] K. Lange. Optimization. Springer, 2004.
[20] P. L. Lions and B. Mercier. Splitting algorithms for the sum of two
nonlinear operators. SIAM Journal on Numerical Analysis, 16(6):964–
979, 1979.

[21] A. Parekh and I. W. Selesnick. Enhanced low-rank matrix approxima-

tion. arXiv:1511.01966, 2015.

[22] N. Rao, R. Nowak, C. Cox, and T. Rogers. Classiﬁcation with the sparse
IEEE Trans. Signal Processing, 64(2):448–463, January

group lasso.
2016.

[23] A. Repetti, M. Q. Pham, L. Duval, ´E. Chouzenoux, and J. C. Pesquet.
Euclid in a taxicab: Sparse blind deconvolution with smoothed (cid:96)1/(cid:96)2
IEEE Signal Processing Letters, 22(5):539–543, May
regularization.
2015.

(a) Since zi’s are ordered, the assumption zi+1 > h(i) implies

zi ≥ zi+1 >

This in turn implies

λ (1 − λ γ) + λ γ (cid:80)i

1 + (i − 1) λ γ

j=1 zj

zi(1 + (i − 1) λ γ) > λ (1 − λ γ) + λ γ

zj.

(60)

i(cid:88)j=1

Subtracting λ γ zi from both sides and rearranging, we
obtain

zi >

λ (1 − λ γ) + λ γ (cid:80)i−1

j=1 zj

1 + (i − 2) λ γ
assumption zi ≤ h(i) implies

(b) The proof of this part is similar. Since zi+1 ≤ zi, the

= h(i − 1).

(61)

zi+1(1 + (i− 1) λ γ) ≤ λ (1− λ γ) + λ γ

zj. (62)

i(cid:88)j=1

Adding λ γ zi+1 to both sides and rearranging, we obtain

λ (1 − λ γ) + λ γ (cid:80)i+1

1 + i λ γ

j=1 zj

zi+1 ≤

= h(i + 1). (63)

APPENDIX C

PROOF OF PROP. 3

Assume zi+1 > h(i). This inequality implies, by the

deﬁnition of h(i) that

λ (1 − λ γ) + λ γ

zj < (1 + (i − 1) λ γ) zi+1.

(64)

We ﬁrst show that zi+1 > h(i + 1). Using (64) in h(i + 1),

i(cid:88)j=1

we ﬁnd

h(i + 1) =

<

=

j=1 zj + λ γzi+1

λ (1 − λ γ) + λ γ (cid:80)i

1 + i λ γ

(1 + (i − 1) λ γ) zi+1 + λ γzi+1

1 + i λ γ

(1 + i λ γ) zi+1

1 + i λ γ

(65)

(66)

(67)

(68)
Let us now show that h(i + 1) > h(i). Notice that for

= zi+1.

positive a, b, c, d,

a + c
b + d

>

a
b

,

if and only if ad < bc. Now if we set

zm,

i(cid:88)m=1

a = λ (1 − λ γ) + λ γ
b = (1 + (i − 1) λ γ) ,
c = λ γ zi+1,
d = λ γ,

(69)

(70)

(71)
(72)
(73)

12

[24] I. Selesnick and M. A. T. Figueiredo. Signal restoration with overcom-
plete wavelet transforms : Comparison of analysis and synthesis priors.
In Proceedings of SPIE (Wavelets XIII), 2009.

[25] I. W. Selesnick and ˙I. Bayram. Enhanced sparsity by non-separable

regularization. IEEE Trans. Signal Processing, 2016.

[26] K. Siedenburg and M. D¨orﬂer. Structured sparsity for audio signals. In

Proc. Int. Conf. on Digital Audio Effects (DAFx), 2011.

[27] N. Simon, J. Friedman, T. Hastie, and R. Tibshirani. A sparse-group
lasso. Journal of computational and graphical statistics, 22(2):231–245,
2013.

[28] A. K. Takahata, E. Z. Nadalin, R. Ferrari, L. T. Duarte, R. Suyama,
R. R. Lopes, J. M. T. Romano, and M. Tygel. Unsupervised processing
of geophysical signals: A review of some key aspects of blind deconvo-
lution and blind source separation. IEEE Signal Processing Magazine,
29(4):27–35, July 2012.

[29] J.-P. Vial. Strong and weak convexity of sets and functions. Mathematics

of Operations Research, 8:231–259, May 1983.

[30] J. Woodworth and R. Chartrand. Compressed sensing recovery via

nonconvex shrinkage penalties. arXiv:1504.02923v1, 2015.

[31] M. Yuan and Y. Lin. Model selection and estimation in regression with
grouped variables. Journal of the Royal Statistical Society: Series B,
68(1):49–67, 2006.

[32] Y. Zhou, R. Jin, and S. Hoi. Exclusive lasso for multi-task feature
In Proc. Int. Conf. Artiﬁcial Intelligence and Statististics,

selection.
2010.

[33] H. Zou and T. Hastie. Regularization and variable selection via the

elastic net. J. R. Statist. Soc. B, 67(2):301–320, April 2005.

