6
1
0
2

 
r
a

 

M
9
1

 
 
]

V
C
.
s
c
[
 
 

1
v
1
2
1
6
0

.

3
0
6
1
:
v
i
X
r
a

Buried object detection using handheld WEMI with
task-driven extended functions of multiple instances

Matthew Cook, Alina Zare, Dominic K. C. Ho

University of Missouri, Columbia, MO 65211

ABSTRACT

Many eﬀective supervised discriminative dictionary learning methods have been developed in the literature.
However, when training these algorithms, precise ground-truth of the training data is required to provide very
accurate point-wise labels. Yet, in many applications, accurate labels are not always feasible. This is especially
true in the case of buried object detection in which the size of the objects are not consistent. In this paper, a
new multiple instance dictionary learning algorithm for detecting buried objects using a handheld WEMI sensor
is detailed. The new algorithm, Task Driven Extended Functions of Multiple Instances, can overcome data
that does not have very precise point-wise labels and still learn a highly discriminative dictionary. Results are
presented and discussed on measured WEMI data.

Keywords: Supervised Dictionary Learning, Landmine Detection, Electromagnetic Induction, Extended Func-
tions of Multiple Instances

1. INTRODUCTION

Electromagnetic induction (EMI) sensors have been studied extensively for their use in detecting buried land-
mines.1, 2 The traditional EMI sensor sends a time-varying electromagnetic ﬁeld down through the ground to
interact with subsurface objects. When these ﬁelds interact with buried objects they induce a response that can
then be measured by the sensor, indicating there is some metal content in the buried object.3

The data used in this investigation was collected utilizing a handheld sensor system that contained both an
EMI sensor and ground penetrating radar that work together to detect buried objects. The two sensors function
independently of each other to enable specialized algorithms for each sensor whose outputs can then be fused
to create a ﬁnal detection measure. In this paper, an algorithm for the detection of buried objects using the
handheld EMI sensor is presented.

Previous work has explored the use of data modeling and ﬁxed dictionaries for detection. Several algorithms
have set out to model the metallic objects such as the Gradient Angle Model Algorithm4 which computes
properties of metallic objects and then searches for these in the data. Another approach to model the signature
of metallic objects is the Discrete Spectrum of Relaxation Frequencies (DSRF)5 which seeks to build a model of
metallic objects in the frequency domain.

Other methods such as the Joint Orthogonal Matching Pursuits (JOMP)6 use a ﬁxed dictionary generated
from the DSRF model to look for potential landmines. The JOMP algorithm also returns which dictionary
element was chosen for a particular data point which allows other algorithms to be trained on this output
and improve the results even further. The Possibilistic K-Nearest Neighbors (PKNN)7 algorithm is one such
algorithm that can then classify the JOMP output to determine the type of object that is found by JOMP.

More recently, however, multiple instance dictionary learning algorithms have been created to learn a dictio-
nary from imprecisely labeled data.8 This paper proposes a new multiple instance dictionary learning algorithm
that builds upon the earlier work.

In Section 2, several algorithms related to the proposed algorithm are brieﬂy reviewed. In Section 3, the
Task Driven Extended Functions of Multiple Instances (TD-eFUMI) algorithm is introduced. Section 4 presents
TD-eFUMI results on an EMI data set. Finally, Section 5 provides a discussion of future work.

2. BACKGROUND

Before moving onto the speciﬁcs of the TD-eFUMI algorithm, ﬁrst a review is provided of some of the previously
published methods upon which the TD-eFUMI algorithm extends.

2.1 Discrete Spectrum of Relaxation Frequencies
The Discrete Spectrum of Relaxation Frequencies (DSRF)9 is a model that represents buried metallic objects
in wide-band EMI data. The DSRF is a discrete case of the Distribution of Relaxtion Times (DRT) model.10
The DSRF model is formulated in terms of relaxation frequencies (where, in contrast, the DRT is in terms of
relaxation times). The beneﬁts to using the DSRF is that it is directly related to the physical properties of
potential targets and is invariant to the relative position and orientation of the target to the EMI sensor.9

The DSRF model is as follows,

K

H(ω) = c0 +

Xk=1

ck

1 + jω/ζk

,

(1)

where c0 is the DC shift, ck is the real spectral amplitudes at each of the K relaxation frequencies ζk (relaxation
frequencies are in radians), and ω is the transmitted frequency in radians. In previous work, several detection
algorithms have leveraged the DSRF model to generate features that can then be used to detect landmines.6, 11, 12
The DSRF model can also be used to generate representative signals that will model diﬀerent types of signals.
When a group of these signals are joined they create a dictionary that can also be used for detection. Since the
transmit frequencies, ω, are ﬁxed the dictionary can be generated by simply selecting several diﬀerent values of
ζk then computing the resulting signals. Generation of the dictionary elements then takes the form,

di =(cid:20)

1

1 + jω1/ζi

,

1

1 + jω2/ζi

, . . . ,

1

1 + jωN /ζi(cid:21)T

,

(2)

where the values of ω are known from the sensor. A dictionary generated via this method is shown in Figure 1.
Each of the created dictionary elements can then be compared to EMI data to help ﬁnd metal objects. Since the
model only accounts for metal objects the dictionary will, hopefully, only look like the data in locations where
metal targets exist. The many dictionary elements will allow for the detection of a larger amount of possible
mine-like objects.

−0.1

−0.2

−0.3

−0.4

−0.5

−0.6

−0.4

−0.3

−0.2

−0.1

0

0.1

0.2

0.3

Figure 1. DSRF dictionary created from Equation 2.

With this ﬁxed dictionary, algorithms can be developed to ﬁnd these signals in the data. One such algorithm
is the Joint Orthogonal Matching Pursuits (JOMP).6 JOMP is an algorithm that is developed from the Matching

Pursuits (MP)13 algorithm which is use to deconstruct a given signal using a known dictionary. MP is a greedy
method that compares each dictionary element to the given signal to ﬁnd the dictionary element that matches
the input signal the closest. Once the match has been made the dictionary element is subtracted from the given
signal. This process is repeated, with each successive match being made to what is left of the original signal, until
the signal becomes negligible, or a set number of dictionary elements have been used. A slight modiﬁcation to
the MP algorithm is the Orthogonal Matching Pursuits (OMP)14 algorithm that behaves the same except that
an orthogonal transformation is used to ensure that each new element is orthogonal to each of the previously
added dictionary elements.

The JOMP algorithm uses the same methodology as OMP except that instead of deconstructing a single
data points with a given dictionary, the JOMP algorithm considers several points simultaneously. In this im-
plementation of JOMP, two data points are processed simultaneously (one positioned 5 points ahead and one 5
points behind the current location being processed). JOMP is only selecting one dictionary element to model
both points. JOMP then assigns a conﬁdence based on the average of the inverse of the two residual errors. The
residual error is the error in reconstruction from the chosen dictionary element.

In some cases, results can be improved by learning a dictionary from the data as opposed to using a ﬁxed
universal dictionary. Thus, in this paper, we develop an approach to estimate a dictionary that optimizes target
detection results (with respect to an objective function as deﬁned in the following section). Our approach for
estimating this dictionary from the data is an extension of Task-driven Dictionary Learning15 and the Extended
Functions of Multiple Instances16 approaches.

2.2 Task-Driven Dictionary Learning
The Task-Driven Dictionary Learning (TDDL)15 algorithm is a supervised dictionary learning model that si-
multaneously learns a classiﬁer with the dictionary to aid classiﬁcation results. The algorithm uses a two layer
model, the top layer being the classiﬁcation error,

min

D∈D,w∈W

IEy,x[ℓs(y, w, α∗(x, D))] +

v
2

||w||2
F ,

(3)

where ℓs is a loss function comparing the actual class label y to the estimated class label computed from the
classiﬁer w and the sparse weights α∗(x, D) which is a function of the known dictionary D and the current data
point x. This model uses a simple linear classiﬁer to do the classiﬁcation,

ˆy = w⊺α∗ + ψ,

(4)

where ˆy is the estimated class label and ψ is an oﬀset or threshold. In Equation 3 the loss function can be
replaced with any twice diﬀerentiable convex loss function.

The second layer of the TDDL model is the dictionary learning/sparse coding layer. In this layer, the objective

function used to determine the dictionary and sparse weights is:

α∗(x, D) ∆= arg min
α∈IRK

1
2

||x − Dα||2

2 + λ1||α||1 +

λ2
2

||α||2
2.

In this model the dictionary is also subject to the constraint:

D ∆= {D ∈ IRL×Ks.t.∀k ∈ {1, . . . , K}, ||dk||2 ≤ 1},

(5)

(6)

which limits each dictionary element to be on or inside of the unit hypersphere.

The optimization for this algorithm is done using stochastic gradient descent17 with a mini-batch strategy,
i.e. a small number of data points are used to complete each update instead of a single data point. In each step
of the optimization process includes two sub-steps: the ﬁrst sub-step is to compute the sparse weights, then, the
second sub-step is to use these sparse weights to compute updates for both the classiﬁer w and the dictionary
D.

This algorithm will learn a discriminative dictionary trained for the target detection task and, furthermore,
trains a classiﬁer that can be used for target detection. However, this algorithm also introduces a new potential
problem, namely, as TDDL is a supervised learning algorithm it requires precisely labeled ground-truth data
(i.e., ground-truth with data point level accuracy). Obtaining precisely labeled data can be very diﬃcult to
acquire for buried objects.

2.3 Extended Functions of Multiple Instances
The Extended Functions of Multiple Instances algorithm (eFUMI)16 is a form of multiple instance dictionary
learning that separates the target and non-target dictionary elements. This algorithm has a built in method
to estimate whether any given point contains any portion of target. This estimation allows the algorithm to
determine if a point that is labeled as target actually contains any portion of the target dictionary elements.
The structure of this algorithm is

F =

−(1 − u)

2

N

Xn=1(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

−

T

xn − zn

u

Xt=1
2   M
Xm=1(cid:13)(cid:13)d−

αntd+

t −

m − µ0(cid:13)(cid:13)

2

2

M

αnmd−

m(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Xm=1
Xt=1(cid:13)(cid:13)d+
t − µ0(cid:13)(cid:13)

2
2 +

T

2

2! −

(7)

γm

M

Xm=1

N

Xn=1

αnm,

where M is the number of non-target dictionary elements, T indicates the target dictionary element/weight, µ0
is the global data mean, γm is a sparsity promoting term on the number of dictionary elements, zn is an unknown
latent variable, and the remaining terms are carried over from Equations 3 and 5.

The latent variable zn is an indicator variable that tells the algorithm whether the current data point is
a true target or not. The indicator is then used to determine whether the full dictionary should be used for
reconstruction or just the non-target dictionary. In order to minimize the objective function when this value is
unknown (as is the case for positively-labeled points under a multiple instance learning framework), expectation
maximization (EM) is used to ﬁnd the expected value of zn for each data point. When evaluating the expectation
of Equation 7 with respect to zn the resulting expression is

E[F ] = Xzn∈{0,1}

(1 − u)

1
2


−

N

P (zn|xn, θ(i−1))(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Xn=1
Xm=1

kdm − µ0k2

2 −

u
2

M

−

xn − znαnT dT −

u
2

kdT − µ0k2

2 −

2

2




(8)

M

Xm=1
Xm=1

M

αnmdm(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Xn=1

N

γm

αnm,

where the probabilities, P (zn|xn, θ(i−1)), are computed via

P (zn|xn, θ(i−1)) =

p(zn = 0|xn ∈ B+

p(zn = 1|xn ∈ B+

p(zn = 0|xn ∈ B−
p(zn = 1|xn ∈ B−

xn −

j , θ(i−1)) = exp −β(cid:13)(cid:13)(cid:13)(cid:13)
j , θ(i−1)) = 1 − exp −β(cid:13)(cid:13)(cid:13)(cid:13)

j , θ(i−1)) = 1
j , θ(i−1)) = 0

M

Pm=1

xn −

2

M

2!
αnmdm(cid:13)(cid:13)(cid:13)(cid:13)
2!
αnmdm(cid:13)(cid:13)(cid:13)(cid:13)
Pm=1

2

.

(9)




The probabilities that are computed are dependent on only how well the non-target dictionary elements can
reconstruct the current data point and a scaling parameter β. If the non-target dictionary elements can accurately
reconstruct the current data point then the data point is assigned a high probability of being non-target, while
if the non-target dictionary elements cannot describe the data, a high probability of target is then assigned. For

points labeled as non-target, the probability of target is set to zero for the training because they are known to
be non-target.

The eFUMI algorithm gives a way to handle imprecise training data but unlike the TDDL algorithm it does

not leverage a classiﬁer to learn a discriminative dictionary that aims to boost the classiﬁcation performance.

3. TASK-DRIVEN EXTENDED FUNCTIONS OF MULTIPLE INSTANCES

The proposed Task-Driven Extended Functions of Multiple Instances (TD-eFUMI) is a combination of the TDDL
and eFUMI algorithms. Therefore the TD-eFUMI algorithm will be a multiple instance learning algorithm that
also learns a classiﬁer alongside the dictionary. The classiﬁcation model is similar to the TDDL model while the
sparse weight/dictionary model was taken from eFUMI.

The combined classiﬁcation model is

where

F (w, α∗

L

K

2

u

+

n(xn, D)) = Ez(cid:20) (1 − u)δn
2   T
Xt=1(cid:13)(cid:13)(cid:13)
Xk=1
Xl=2
δn =


NM
NT

s
2

1,

+

ǫ

,

(z′

n − w⊺α∗

d(T )

t − µ0(cid:13)(cid:13)(cid:13)

2

M

n(xn, D))2(cid:21)
Xm=1(cid:13)(cid:13)(cid:13)

+

2

2

2! +

d(N T )

m − µ0(cid:13)(cid:13)(cid:13)

(dk(l) − dk(l − 1))2 ,

for target points

,

for background points

v
2

kwk2
2

(10)

(11)

m

and d(N T )

d(T )
t
value of the kth dictionary element from the entire dictionary, z′
parameter that controls the smoothing term.

are respectively elements of the target and non-target dictionaries, Dkf (l) is the lth dimension
n is an unknown latent variable, and s is the

For the dictionary learning portion to keep this similar to the TDDL algorithm the same sparse coding

function was chosen,

α∗

n(xn, D) = arg min

α

xn − zn

αnmd(N T )

+ λkαnk1,

(12)

1

2(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

αntd(T )

t −

T

Xt=1

M

Xm=1

2

2

m (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

with a few modiﬁcations. First the latent variable zn was added to allow the method to adapt to and learn which
data points are target and which are non-target, also the ℓ2 norm was dropped from this equation.

During optimization, ﬁrst the expectation of equation 10 with respect to the unknown variable z′

n. The
probabilities for z′
n are identical to those for zn shown in Equation 9. Once this is done stochastic gradient descent
is used to alternatively update the sparse weights and then the dictionary and classiﬁer until the dictionary and
classiﬁer converge.

Once the dictionary and classiﬁer have been trained, the classiﬁcation algorithm consists of only two steps:

(1) First the sparse weights of each data point to be tested are found using

α∗

n(xn, D) = arg min

α

1
2

kxn − Dαnk2

2 + λkαnk1,

(13)

which is a small modiﬁcation of Equation 12 where the latent variable is removed, so that the entire dictionary
can be used to reconstruct the entire data; (2) The second step is to compute the algorithm output which is
done via Equation 4.

4. EXPERIMENTS AND RESULTS

The data used in this investigation was collected using a handheld EMI sensor swept over several lanes mounted
on a cart system. Buried objects are classiﬁed as clutter or high, low, or non metal targets. Six lanes of data
are used in the experiments shown in this paper. The number of objects of each type are shown in Table 1.

Table 1. Object description for EMI data. Abbreviations, HMT: High-Metal Target, LMT: Low-Metal Target, NMT:
Non-Metal Target, CL: Clutter.

HMT LMT NMT CL

Lane 1
Lane 2
Lane 3
Lane 4
Lane 5
Lane 6
Totals

4
4
4
6
7
6
31

7
10
7
6
5
6
41

0
0
0
3
5
2
10

6
4
8
0
0
3
11

4.1 Alarm Generation

This algorithm is implemented as a classiﬁer for the data used in these experiments. Speciﬁcally, in our imple-
mentation, the algorithm is not run directly on the entirety of the data. Instead alarms or points of interest are
found and then the TD-eFUMI algorithm is used to classify each of the returned alarms as either a mine-like
object or non-target.

To generate alarms for the TD-eFUMI algorithm, the JOMP detector that was mentioned in Section 2.1 is
used. The typical output from JOMP is a conﬁdence map that indicates where JOMP believes a target exists.
To generate alarms from this conﬁdence map a threshold is applied to the conﬁdence values so as to only keep
the most conﬁdence points. To then ﬁnd the alarm locations the Mean Shift18 algorithm is used. This algorithm
clusters data and returns the centroid of each cluster, this centroid is used as the alarm location.

Since one data point for each alarm is not enough to train the algorithm, a group of data points in a 0.25m
radius around each alarm location is gathered and included in each alarm to act as the alarm data. The data
that is collected to be used for the alarm data is processed after being collected to ensure some consistency
throughout the diﬀerent lanes. The process for this is only to subtract the mean of the lane from all of the alarm
data.

The last step in generating the alarms is to assign whether each alarm was a target or a false alarm for
training. To do this the ground truth was compared to the alarms returned by JOMP and if an alarm location
was within 0.25m of any target ground truth location the alarm was labeled as target. The alarm results from
the JOMP prescreener are shown in Table 2, the number of targets and false alarms were not made to be equal
it is simply a coincidence that the totals match. In this data that leaves six sets of alarms corresponding to the
six lanes in the data set.

Table 2. Alarms found by the JOMP prescreener over the six testing lanes.

Target Alarms False Alarms Total Alarms

Lane 1
Lane 2
Lane 3
Lane 4
Lane 5
Lane 6
Totals

20
19
18
14
19
19
109

29
22
20
17
13
8

109

49
41
38
31
32
27
218

4.2 Alarm Based TD-eFUMI

To avoid any test on train situations lane based cross validation is used. This means that when TD-eFUMI is
being trained only ﬁve of the six sets of alarms are used for training data, and the sixth set is used for testing.
As an example when lane one is being tested the alarms from lanes two through six are used to train TD-eFUMI.
With this approach TD-eFUMI learns six diﬀerent dictionaries, one for each fold of the cross validation. An
example of a learned dictionary is shown in Figure 2.

0.2

0.15

0.1

0.05

0

-0.05

-0.1

-0.15

-0.2

-0.25

-0.3

Figure 2. Sample Dictionary learned by TD-eFUMI. Target elements are in green and non-target elements in red.

0.2

0.3

0.4

-0.2

-0.1

0

0.1

The dictionary learned by the TD-eFUMI algorithm here is similar to what is expected in the DSRF model
introduced in Section 2.1. The one noticeable diﬀerence is that the newly learned dictionary elements are not
centered on the real, horizontal, axis. The reasoning for this is that in the DSRF model the real mean is
subtracted from all of the data samples. In this dictionary only the overall data mean was subtracted, which
leaves any shift in the real axis behind. Leaving the real shifts in the dictionary allows the dictionary to cover a
much larger search space during the optimization routine, potentially leading to a better result with this data.

Once all six sets of dictionaries and classiﬁers are trained the test data was reconstructed using Equation 13
and then the classiﬁcation is done via Equation 4. The results are shown in the ROC curve in Figure 3. From
these ROC curves it can be seen that for the data used in this experiment the TD-eFUMI algorithm is able to
detect nearly all of the targets found by the detector at a false alarm rate that is signiﬁcantly lower than the
prescreener.

With this study it is also worth noting that the metal detector will not be capable of detecting every target,
as many of them do not contain metal. This explains why the ROC curves never actually get to 100% detection.

Two additional ROC curves are shown in Figures 4 and 5. In these ROC curves only a portion of the targets
are used. In Figure 4 the ROC curve is for only the high metal targets and the ROC curve in Figure 5 is for the
low metal targets. There are many more low metal targets than there are high metal objects so the low metal
ROC curve will appear much more similar to the full results in Figure 3.

The scoring results shown in this paper were computed while ignoring clutter objects, all other targets are
included in the results. The reason for ignoring clutter during scoring is that it is not anticipated that the
proposed algorithm can diﬀerentiate between metal targets and metal clutter at this time. The process of
ignoring clutter is to remove any hits that appear close to any of the ground truth locations for the clutter
objects within a ﬁxed halo.

These two ROC curves indicate that the TD-eFUMI algorithm does much better than the JOMP prescreener
for the low metal targets included in this data set, but yet trails when only considering high metal targets. As

1

0.9

0.8

0.7

0.6

D
P

0.5

0.4

0.3

0.2

0.1

0

 

 

TD−eFUMI
JOMP

FAR

Figure 3. ROC curve comparing the classiﬁcation performance of TD-eFUMI with the prescreener that was used to
generate alarms.

1

0.9

0.8

0.7

0.6

D
P

0.5

0.4

0.3

0.2

0.1

0

 

 

1

0.9

0.8

0.7

0.6

D
P

0.5

0.4

0.3

0.2

0.1

0

 

TD−eFUMI
JOMP

FAR

 

TD−eFUMI
JOMP

FAR

Figure 4. ROC curve for only high metal targets

Figure 5. ROC curve for only low metal targets

there are many more low metal targets these represent the majority of training samples therefore allowing the
TD-eFUMI algorithm to learn a better model for the low metal objects versus the high metal ones. JOMP does
not have this problem because it does not require any training which means it will ﬁnd the objects that do not
occur very often better than an algorithm that has to be trained.

5. SUMMARY AND FUTURE WORK

This paper investigated the TD-eFUMI algorithm and its eﬀectiveness for landmine detection and classiﬁcation
in EMI data. Based on the results presented in this paper the proposed TD-eFUMI classiﬁer is able to improve
upon the JOMP prescreener, however more testing needs to be conducted to make any deﬁnitive conclusions.

Potential future work could be to modify the TD-eFUMI framework in order to allow for multi-class clas-
siﬁcation to allow for detecting diﬀerent types of mines or mine features instead of an all encompassing target
class.

This work was funded by Army Research Oﬃce grant number 66398-CS to support the US Army RDECOM
CERDEC NVESD. The views and conclusions contained in this document are those of the authors and should

ACKNOWLEDGMENTS

not be interpreted as representing the oﬃcial policies either expressed or implied, of the Army Research Oﬃce,
Army Research Laboratory, or the U.S. Government. The U.S. Government is authorized to reproduce and
distribute reprints for Government purposes notwithstanding any copyright notation hereon.

REFERENCES

[1] Scott, W., “Broadband electromagnetic induction sensor for detecting buried landmines,” in [IEEE Inter-

national Geoscience and Remote Sensing Symposium], 22–25 (July 2007).

[2] Scott, W. and McFadden, M., “Wideband measurement of the magnetic susceptibility of soils and the mag-
netic polarizability of metallic objects,” in [IEEE International Geoscience and Remote Sensing Symposium],
3170–3173 (July 2012).

[3] Wilson, J. N., Ramachandran, G., Gader, P. D., Smock, B., and Scott, W. R., “Wideband emi pre-screening

for landmine detection,” Proc. SPIE 7303, 730324–730324–8 (2009).

[4] Ramachandran, G., Gader, P., and Wilson, J., “Granma: Gradient angle model algorithm on wideband emi

data for land-mine detection,” IEEE Geoscience and Remote Sensing Letters 7, 535–539 (July 2010).

[5] Scott, W. and Larson, G., “Modeling the measured em induction response of targets as a sum of dipole
terms each with a discrete relaxation frequency,” in [2010 IEEE International Geoscience and Remote
Sensing Symposium,], 4188–4191 (July 2010).

[6] Goldberg, S., Glenn, T., Wilson, J. N., and Gader, P. D., “Landmine detection using two-tapped joint

orthogonal matching pursuits,” Proc. SPIE 8357, 83570B–83570B–8 (2012).

[7] Dula, J., Zare, A., Ho, D., and Gader, P., “Landmine classiﬁcation using possibilistic k-nearest neighbors

with wideband electromagnetic induction data,” Proc. SPIE 8709, 87091F–87091F–12 (2013).

[8] Zare, A., Cook, M., Alvey, B., and Ho, D. K., “Multiple instance dictionary learning for subsurface object

detection using handheld emi,” Proc. SPIE 9454, 94540G–94540G–8 (2015).

[9] Wei, M.-H., Scott, W., and McClellan, J., “Robust estimation of the discrete spectrum of relaxations for
electromagnetic induction responses,” IEEE Transactions on Geoscience and Remote Sensing 48, 1169–1179
(March 2010).

[10] Honerkamp, J. and Weese, J., “A nonlinear regularization method for the calculation of relaxation spectra,”

Rheologica Acta 32(1), 65–73 (1993).

[11] Wei, M.-H., Scott, W., and McClellan, J., “Landmine detection using the discrete spectrum of relaxation

frequencies,” in [IEEE International Geoscience and Remote Sensing Symposium], 834–837 (July 2011).

[12] Wei, M.-H., Scott, W., and McClellan, J., “Estimation and application of discrete spectrum of relaxations for
electromagnetic induction responses,” in [IEEE International Geoscience and Remote Sensing Symposium],
2, 105–108 (July 2009).

[13] Mallat, S. and Zhang, Z., “Matching pursuits with time-frequency dictionaries,” IEEE Transactions on

Signal Processing 41, 3397–3415 (Dec 1993).

[14] Cai, T. and Wang, L., “Orthogonal matching pursuit for sparse signal recovery with noise,” IEEE Trans-

actions on Information Theory 57, 4680–4688 (July 2011).

[15] Mairal, J., Bach, F., and Ponce, J., “Task-driven dictionary learning,” IEEE Transactions on Pattern

Analysis and Machine Intelligence 34, 791–804 (April 2012).

[16] Zare, A. and Jiao, C., “Extended functions of multiple instances for target characterization,” in [IEEE

Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing], (June 2014).

[17] Bottou, L., “Large-scale machine learning with stochastic gradient descent,” in [Proceedings of the 19th
International Conference on Computational Statistics], Lechevallier, Y. and Saporta, G., eds., 177–187,
Springer, Paris, France (August 2010).

[18] Cheng, Y., “Mean shift, mode seeking, and clustering,” IEEE Transactions on Pattern Analysis and Machine

Intelligence 17, 790–799 (Aug 1995).

