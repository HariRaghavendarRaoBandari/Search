6
1
0
2

 
r
a

M
9

 

 
 
]

G
L
.
s
c
[
 
 

1
v
6
3
8
2
0

.

3
0
6
1
:
v
i
X
r
a

Faster learning of deep stacked autoencoders on
multi-core systems using synchronized layer-wise

pre-training

Anirban Santara1, Debapriya Maji2, DP Tejas1, Pabitra Mitra1 and

Arobinda Gupta1

1 Department of Computer Science and Engineering, Indian Institute of Technology

Kharagpur, Kharagpur 721302, WB, India

2 Department of Electronics and Electrical Communication Engineering, Indian

Institute of Technology Kharagpur, Kharagpur 721302, WB, India

anirban santara@iitkgp.ac.in

Abstract. Deep neural networks are capable of modelling highly non-
linear functions by capturing diﬀerent levels of abstraction of data hi-
erarchically. While training deep networks, ﬁrst the system is initialized
near a good optimum by greedy layer-wise unsupervised pre-training.
However, with burgeoning data and increasing dimensions of the archi-
tecture, the time complexity of this approach becomes enormous. Also,
greedy pre-training of the layers often turns detrimental by over-training
a layer causing it to lose harmony with the rest of the network. In this
paper a synchronized parallel algorithm for pre-training deep networks
on multi-core machines has been proposed. Diﬀerent layers are trained
by parallel threads running on diﬀerent cores with regular synchroniza-
tion. Thus the pre-training process becomes faster and chances of over-
training are reduced. This is experimentally validated using a stacked
autoencoder for dimensionality reduction of MNIST handwritten digit
database. The proposed algorithm achieved 26% speed-up compared to
greedy layer-wise pre-training for achieving the same reconstruction ac-
curacy substantiating its potential as an alternative.

Keywords: Deep learning, stacked autoencoder, RBM, backpropagation, con-
trastive divergence, MNIST

1

Introduction

Artiﬁcial neural networks that have more than three hidden layers of neurons
are called Deep Neural Networks (DNN) [1]. Every layer of a trained DNN rep-
resents a higher level abstraction of the input data than the previous one. DNNs
are highly successful and provide the current state-of-the art results in many
AI tasks. Stacked autoencoder is a class of DNN that is used for unsupervised
representation learning and dimensionality reduction [2]. However, training deep
architectures faced challenges ever since their introduction. The cost function be-
ing non-linear, gradient based routines usually get stuck in poor local minima for

Presented as a paper at the Workshop on Parallel and Distributed Computing for

Knowledge Discovery in Databases (PDCKDD 2015) as a part of European
Conference on Machine Learning and Principles and Practices of Knowledge

Discovery in Databases (ECML/PKDD 2015)

large initialization weights. On the other hand, with small initial weights back-
propagated error gradients in the layers close to the input become tiny resulting
in no appreciable training of those layers [3]. Greedy layer-wise pre-training was
introduced with a motivation to approximately capture statistical abstractions
of the input data in every layer and initialize the network parameters in a region
near a good local optimum [4, 5]. This was followed by ﬁne-tuning via back-
propagation to push the solution deeper into the optimum. This approach of
training deep architectures has brought great success in many areas of applica-
tion [2, 3].

The time complexity of this process becomes forbiddingly large for big datasets
and architectures. Hence there have been eﬀorts at parallelization. These parallel
algorithms fall in two broad categories [6]. The algorithms of the ﬁrst category
partition the neural network and train the parts in parallel. The neural network
can be physically partitioned [7] or logically partitioned [8, 9]. The challenge is
to ensure that the computational load is evenly distributed among the paral-
lel processors and data communication is minimized. The second category of
algorithms carry out calculations relative to the entire network but speciﬁc to
subsets of data (single patterns or small batches of data) at diﬀerent process-
ing nodes [10]. This method is typically suitable for computer clusters as this
requires very little communication. Each computing node i, generates its own
update vector ∆wi on the basis of the data-set Di assigned to it. At the end of
an epoch when all the nodes are ready, the overall ∆w is calculated by averaging
the outputs of each of the nodes.

As mentioned earlier, the pre-training phase is crucial for convergence but to the
best of our knowledge, all existing methodologies perform pre-training layer-by-
layer in a greedy way. In this work we propose a synchronized parallel layer-wise
pre-training algorithm. The layers are learnt on parallel running threads that are
synchronized regularly by cascading of maturity information. It achieves faster
learning due to parallelism and reduces chances of misﬁt among the layers owing
to synchronization.

Section 2 brieﬂy introduces some theoretical concepts relevant to stacked au-
toencoders. The synchronized layer-wise pre-training algorithm is described in
Section 3. Section 4 presents an experimental methodology to compare the pro-
posed algorithm with greedy layer-wise pre-training and Section 5 discusses the
results of the experiment. Finally Section 6 concludes the paper with a summary
of the work.

2 Preliminaries of stacked autoencoder

In this section we describe stacked autoencoder and state brieﬂy about the tools
and methodologies used in their training.

2.1 Autoencoder

An autoencoder is a type of neural network that is operated in unsupervised
learning mode mainly for representation learning and dimensionality reduc-
tion [11]. It is aimed at encoding input x into a representation c(x) so that
the input can be reconstructed from the representation with minimum amount
of deformation. Given a set of unlabelled training examples {x(1), x(2), ...} the
autoencoder sets the outputs y(i) = x(i) and tries to learn a function hW,b such
that hW,b(x) = ˆx ≈ x.

A stacked autoencoder [2, 3] has multiple hidden layers of neurons between the
input and output layers. A stacked autoencoder of depth n has an input layer,
2n − 3 hidden layers, and an output layer of neurons. Layers 1, 2, ..., n comprise
the encoder and the remaining layers form the decoder. The nth layer is called
the code layer.

Autoencoder is usually trained by the backpropagation algorithm with an error
criterion like mean squared error or cross entropy error [11].

2.2 Restricted Boltzmann Machine (RBM)

Restricted Boltzmann Machine (RBM) is commonly used for initializing deep
neural networks like stacked autoencoder. An RBM is a Markov Random Field
associated with an undirected graph that consists of m visible units V = (V1, . . . ,
Vm) comprising the visible layer and n hidden units H = (H1, . . . , Hn) compris-
ing the hidden layer [2, 12]. There exists no connection among the units of the
same layer in the graph. The joint probability distribution associated with the
model is given by the Gibbs distribution:

p(v,h) =

e−E(v,h)

1
Z

with the energy function:

E(v,h) = − n(cid:88)

m(cid:88)

wi,jhivj − m(cid:88)

bjvj − n(cid:88)

cihi

i=1

j=1

j=1

i=1

(1)

(2)

The absence of connections among the nodes of the same layer implies that
the hidden variables are conditionally independent given the state of the visible
variables and vice versa:

n(cid:89)
m(cid:89)

i=1

and

p(h|v) =

p(v|h) =

p(hi|v)

p(vi|h)

(3)

We can use an RBM to model unknown probability distributions. The param-
eters are trained by gradient ascent on the log-likelihood of given data using
Markov Chain Monte Carlo methods like Contrastive Divergence [13].

i=1

2.3 Greedy Layer-wise pre-training

Due to the presence of numerous local optima, proper initialization of the param-
eters is crucial for training deep neural networks. Greedy layer-wise pre-training
was introduced with a motivation to roughly capture statistical abstractions of
the input data in every layer and initialize the network parameters in a region
near a good local optimum [5], [3]. Figure 1 outlines the greedy layer-wise pre-
training algorithm.

Let us consider a deep neural network with K hidden layers of neurons. Let the
input dimension be Ni, output dimension be No and the hidden layer dimen-
sions be N1, N2, . . . , NK. Let us denote the training set by S. Greedy layer-wise
pre-training follows one of the two methodologies described below.

Pre-training using RBM: In this approach, ﬁrst an RBM with Ni visible nodes
and N1 hidden nodes is trained. After training, the hidden weights and biases
are used to initialize the incoming weights and biases respectively of the ﬁrst
hidden layer of the deep neural network. The input data of dimensionality Ni is
transformed to the dimension N1 of the hidden layer. For an RBM with Bernoulli
hidden units this is done by calculating the activation probabilities of the hidden
units corresponding to the input examples v ∈ S as follows:

 m(cid:88)



p(Hi = 1|v) = σ

wi,jvj + ci

(4)

j=1

Now another RBM with N1 visible units and N2 hidden units is trained with the
transformed data to ﬁnd weights and biases which go and initialize the second
hidden layer. The data is transformed to the dimension of the second hidden
layer and this procedure is repeated for all of the hidden layers of the deep neu-
ral network.

Pre-training using autoencoder: Deep networks can also be initialized using au-
toencoders. In this methodology, single hidden-layer autoencoders are trained
for each of the layers of the deep architecture and their weights and biases are
used to initialize the corresponding layers of the deep architecture.

3 Synchronized layer-wise pre-training

This section describes the proposed algorithm for accelerated training of stacked
autoencoders. Greedy layer-wise pre-training makes a layer wait for all the pre-
vious layers to ﬁnish learning before it can start itself and for all the following
layers to ﬁnish after it has ﬁnished. The guiding philosophy of the proposed algo-
rithm is to reduce the idle time of greedy layer-wise pre-training by introducing

Fig. 1. Schematic of timing diagram for Greedy layer-wise pre-training. Each arrow
indicates an epoch of learning while dashed line indicates idle time.

Fig. 2. Schematic of space-time diagram for proposed multi-core synchronized layer-
wise pre-training. Like Figure 1 each arrow indicates an epoch of learning and dashed
lines mark the idle times of each layer.

parallelism with synchronization. This algorithm is suitable for multi-core ma-
chines where diﬀerent layers can learn in parallel on threads running in diﬀerent
cores of the CPU.

3.1 Methodology

Let Ll denote the lth layer of an autoencoder of depth K. The thread assigned
to pre-train Ll is denoted by Tl. The training and validation data for the nth
epoch of its pre-training are denoted by DT
l [n] respectively. Let the
activation function for Ll be fl(.) and the incoming weights and biases after the
nth epoch be Wl[n] and bl[n] respectively [∀l = 1, 2, . . . , K].

l [n] and DV

The training and validation data for the deep network are directly used for pre-
training L1 and correspond to DT
1 [0] respectively. These are consid-
ered mature right from the beginning and are kept constant throughout. ∀n > 0:

1 [0] and DV

DT

1 [n] = DT

1 [0]

As the training and validation data for T1 are readily available, the algorithm
starts with T1 beginning to learn the ﬁrst layer of the autoencoder.

DV

1 [n] = DV

1 [0]

(5)

– Due to data dependency Tl waits until Tl−1 has completed one epoch of
training. Once Tl starts executing, every time it completes one epoch, it
l using the current weights and biases Wl[n] and
transforms DT
bl[n] and modiﬁes DT

l and DV

l+1 as:

l+1 and DV
l+1[m + 1] = fl(Wl[n] ∗ DT
DT
l+1[m + 1] = fl(Wl[n] ∗ DV
DV

l [n] + bl[n])

l [n] + bl[n])

(6)

Tl executes for a stipulated Nl epochs speciﬁed by the user. Cascading of
knowledge synchronizes the threads and promotes harmonious training of
the layers.

– After completing Nl epochs, Tl goes to sleep. If Tl−1 modiﬁes DT

l and DV
l
after that, Tl wakes up, executes one iteration of learning with the modiﬁed
data and goes back to sleep.

– Pre-training ends when all the threads have ﬁnished their stipulated number

of iterations.

– After pre-training has been completed, the entire architecture can be ﬁne-

tuned by backpropagation learning.

Pre-training can be done using RBM or autoencoder. Though described in terms
of an autoencoder, this algorithm can also be used for pre-training other deep
neural networks meant for classiﬁcation, regression, etc.

4 Experimental methodology

The synchronized layer-wise pre-training algorithm was tested on the task of
dimensionality reduction of MNIST handwritten digit dataset using a stacked
autoencoder. The reconstruction accuracy was measured using a mean squared
error (MSE) crterion.

4.1 Dataset

MNIST3 is a database of (8-bit) 28x28 gray-scale handwritten digits, popularly
used for training and testing image processing and machine learning systems.
The database contains 60, 000 training examples and 10, 000 test examples.
For our experiments, the training set was further divided into 50, 000 training
examples and 10, 000 validation examples. The validation set contained 1000
examples from each class of digits (0− 9) which ensures equal participation from
all of the classes. The training set was randomized and divided into minibatches
of size 100 before every new epoch of pre-training and ﬁne-tuning.
Figure 5(a) shows some sample digits from the dataset.

3 Available for free download at http://yann.lecun.com/exdb/mnist/

4.2 Parameters of the stacked autoencoder

Details of the architecture of the stacked autoencoder have been presented in
Table 1. This is same as the architcture used by [3] for the same task with the
only exception of sigmoid activation functions for all the layers.

Table 1. Architectural parameters of the stacked autoencoder

Parameter

Depth

Value

5

Layer dimensions 784, 1000, 500, 250, 30
Activation function

sigmoid

4.3 Parameters of the learning algorithms

The learning rate of both the weights and biases was set at 0.1 for contrastive
divergence and 0.001 for backpropagation. In addition a momentum of 0.5 for
the ﬁrst 5 epochs and 0.9 for the remaining epochs was used in contrastive di-
vergence learning.

The performances of greedy layer-wise pre-training (the baseline) [3] and the pro-
posed synchronized layer-wise pre-training algorithm were compared on a system
with 8 CPU cores (dual-hyperthreaded quad-core) and 8GB RAM in terms of
reconstruction error calculated as the average squared reconstruction error per
digit and the total training time. The baseline experiment was performed with 20
epochs of RBM pre-training for every layer followed by 10 epochs of ﬁne-tuning
of the entire architecture via backpropagation. Next, the proposed algorithm was
executed for the same amount of pre-training and ﬁne-tuning but the threads
L2, L3 and L4 carried out 5, 20 and 40 additional epochs of pre-training respec-
tively, every time the previous thread modiﬁed their data.

5 Results and discussion

This section presents the results of the comparison and discusses a few aspects
of the proposed algorithm.

5.1 Convergence

Figure 6 shows the variation of reconstruction error of the validation set during
the execution of the proposed algorithm. Table 2 gives the ﬁnal reconstruction
errors for the two experiments. Figure 5 compares the reconstructions of 25 ran-
domly chosen samples from the test set by the two methods. The performances
are almost equivalent.

Fig. 3. Space-time diagram of the proposed synchronized layer-wise pre-training al-
gorithm. The compute times of each of the threads are marked with thick lines. The
black spikes between the threads indicate modiﬁcation of data of the upper thread by
the lower thread after every epoch of pre-training. Plots of the reconstruction errors
for training (solid line) and validation data (circles) for each of the RBMs have been
shown above the timing diagrams for the corresponding threads.

Fig. 4. Magniﬁed view of the timing diagram for the innermost layer (marked by dotted
rectangle in Figure 3). Notice how L4 gradually corrects its initial over-ﬁtting as the
training data matures. The solid line indicates training error and the circles denote
validation error

Table 2. Average squared reconstruction error per digit

pre-

Algorithm Training Error Test Error
Greedy
training
Synchronized
pre-training

8.19

8.39

8.00

8.57

Table 3. Execution times

pre-

Algorithm Pre-training time Fine-tuning time
Greedy
training
Synchronized
pre-training

2h 16m 59s

1h 49m 11s

3h 14m 43s

2h 15m 42s

(a)

(b)

(c)

Fig. 5. Comparison of performance: (a) Original (b) reconstruction with greedy layer-
wise pre-training (c) reconstruction with synchronized layer-wise pre-training

5.2 Speed-up

Table 5.1 compares the execution times of the two algorithms. The proposed al-
gorithm achieves the same performance 1h26m49s faster than greedy layer-wise
pre-training which is a 26.17% speedup. The four threads, T1 through T4 were
run on four diﬀerent cores of the same CPU. The time for communication of
data among the threads was observed to be 5 to 6 orders of magnitude lesser
than the time for an epoch of RBM pre-training.

In our experiment pre-training of the stacked autoencoder was concluded as soon
as the RBM for the ﬁrst layer (784 − 1000) completed its stipulated 20 epochs.
2 after the 20th epoch, the parameters of L2 or
Though it changed DT
any of its data-dependent layers were not updated. This was just to keep the
total pre-training time equal to the time taken for L1 (the largest and the slow-
est to train layer) to complete its prescribed amount of pre-training. Performing
an extra round of update for L2, L3 and L4 prolonged the training but did not

2 and DV

Fig. 6. Variation of the error in reconstruction of validation set for the stacked auto-
encoder during synchronized pre-training (dashed line) and ﬁne-tuning (solid line).
Reconstruction error was checked at regular intervals during pre-training and after
every epoch of backpropagation during ﬁne-tuning

improve the ﬁnal reconstruction accuracy.

5.3 Analysis of the space-time diagram

Figure 3 shows the space-time diagram for the proposed algorithm. The varia-
tion of reconstruction errors of the training and validation examples have been
plotted for each layer. Both the errors show a decreasing trend for the ﬁrst three
layers indicating the parameters of these layers, getting tuned to the given data.
Initially the training and validation errors diﬀer for all the layers but eventually
converge to almost equal values. This indicates that the parameters achieve gen-
eralization beyond the training set in the course of pre-training.

The behavior of the fourth layer is particularly interesting. Figure 4 zooms into
its timing diagram (marked with a dotted rectangle in Figure 3). The validation
and training errors diﬀer by a large margin for quite sometime (throughout
the stipulated amount of pre-training and beyond) but eventually converge as
T4 updates its parameters every time DT
4 are changed by T3. Also
both the training and validation errors show an increasing trend throughout
the process. Low initial reconstruction error that increases with the progress of
training indicates that the random values to which the weights and biases of L4
were initialized, overﬁt the data that was provided by L3 initially. Signiﬁcant
diﬀerence between the training and validation errors during this period of time
further corroborates the overﬁtting. However with the progress of training, as
the data matures, the overﬁtting slowly cures and the training and validation

4 and DV

errors converge to close values. The increasing trend of the reconstruction error
implicitly indicates how synchronization forces the innermost layer to increase
its own error to conform to the remaining of the network. The behavior of the
fourth layer also indicates the necessity of updating the parameters even after
the stipulated amount of pre-training has been completed. As the training data
of a hidden layer changes throughout the process of pre-training, the parameters
must also be updated regularly to keep up with the changing data. This does not
add an overhead to the total pre-training time which is always dictated by the
time taken by the largest (or the most complex) layer to complete the stipulated
number of epochs of pre-training.

6 Conclusion

In this work the existing methods of parallelizing deep architecture learning
have been explored and a new algorithm for parallelizing the pre-training phase
has been proposed. The convergence and performance of the algorithm have
been tested on the task of dimensionality reduction of MNIST handwritten digit
database using stacked autoencoder and 26% speed-up has been observed for
achieving the same performance. The key feature of the proposed algorithm is
that it is not greedy. This reduces chances of misﬁt among the layers, a com-
mon hazard of greedy learning and minimizes the idle time for each layer. The
proposed algorithm can be seamlessly integrated with the existing methods of
parallelization which mainly concentrate on the ﬁne-tuning phase.

References

1. D. Erhan, A. Courville, and P. Vincent, “Why Does Unsupervised Pre-training
Help Deep Learning ?” Journal of Machine Learning Research, vol. 11, pp. 625–
660, 2010.

2. Y. Bengio, Learning Deep Architectures for AI. now, 2009, vol. 2, no. 1.
3. G. E. Hinton and R. R. Salakhutdinov, “Reducing the Dimensionality of Data with

Neural Networks,” Science, vol. 504, no. July, 2006.

4. G. E. Hinton and S. Osindero, “A fast learning algorithm for deep belief nets,”

Neural computation, 2006.

5. Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, “Greedy Layer-Wise Train-
ing of Deep Networks,” Proceedings of Neural Information Processing Systems,
2007.

6. A. Petrowski and C. Girault, “Performance analysis of a pipelined backpropagation
parallel algorithm,” IEEE Transactions on Neural Networks, vol. 4, no. 6, pp. 970–
981, 1993.

7. X. Chen, A. Eversole, G. Li, D. Yu, and F. Seide, “Pipelined Back-Propagation

for Context-Dependent Deep Neural Networks,” pp. 2–5, 2012.

8. L. Deng and D. Yu, “Deep Convex Net : A Scalable Architecture for Speech Pattern

Classiﬁcation,” ISCA, no. August, pp. 2285–2288, 2011.

9. L. Deng, B. Hutchinson, and D. Yu, Parallel Training of Deep Stacking Networks,

2012.

10. M. D. DeGrazia, I. Stoianov, M. Zorzi, and M. De Filippo De Grazia, “Paral-
lelization of Deep Networks,” European Symposium on Artiﬁcial Neural Networks,
Computational Intelligence and Machine Learning, no. April, pp. 621–626, 2012.

11. S. Haykin, Neural networks: A comprehensive foundation, 1998, vol. 3, no. 5.
12. A. Fischer and C. Igel, “An Introduction to Restricted Boltzmann Machines,”
Progress in Pattern Recognition, Image Analysis2, Computer Vision, and Applica-
tions, pp. 14–36, 2012.

13. G. E. Hinton, “Training Products of Experts by Minimizing Contrastive Diver-

gence,” Neural computation, vol. 14, no. 0899-7667, pp. 1771–1800, 2002.

