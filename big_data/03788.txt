A Primer on the Signature Method in Machine
Learning

Ilya Chevyreva and Andrey Kormilitzina,b

aMathematical Institute, University of Oxford, Andrew Wiles Building,Woodstock Road, Oxford,
OX2 6CG, UK

bOxford-Man Institute, University of Oxford, Eagle House, Walton Well Road, Oxford, OX2 6ED,
UK
E-mail: ilya.chevyrev@maths.ox.ac.uk,
andrey.kormilitzin@maths.ox.ac.uk

Abstract:

In these notes, we wish to provide an introduction to the signature method,

focusing on its basic theoretical properties and recent numerical applications.

The notes are split into two parts. The ﬁrst part focuses on the deﬁnition and fundamental
properties of the signature of a path, or the path signature. We have aimed for a minimalistic
approach, assuming only familiarity with classical real analysis and integration theory, and
supplementing theory with straightforward examples. We have chosen to focus in detail on
the principle properties of the signature which we believe are fundamental to understanding
its role in applications. We also present an informal discussion on some of its deeper
properties and brieﬂy mention the role of the signature in rough paths theory, which we
hope could serve as a light introduction to rough paths for the interested reader.

The second part of these notes discusses practical applications of the path signature to
the area of machine learning. The signature approach represents a non-parametric way
for extraction of characteristic features from data. The data are converted into a multi-
dimensional path by means of various embedding algorithms and then processed for compu-
tation of individual terms of the signature which summarise certain information contained
in the data. The signature thus transforms raw data into a set of features which are used
in machine learning tasks. We will review current progress in applications of signatures to
machine learning problems.

6
1
0
2

 
r
a

 

M
1
1

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
8
8
7
3
0

.

3
0
6
1
:
v
i
X
r
a

Contents

1 Theoretical Foundations

1.1 Preliminaries

1.1.1 Paths in Euclidean space

1.1.2 Path integrals

1.2 The signature of a path

1.2.1 Deﬁnition

1.2.2 Examples

1.2.3 Picard iterations: motivation for the signature

1.2.4 Geometric intuition of the ﬁrst two levels

1.3

Important properties of signature

1.3.1

Invariance under time reparametrisations

1.3.2

Shuﬄe product

1.3.3 Chen’s identity

1.3.4 Time-reversal

1.3.5 Log signature

1.4 Relation with rough paths and path uniqueness

1.4.1 Rough paths

1.4.2 Path uniqueness

2 Practical Applications

2.1 Elementary operations with signature transformation

2.1.1 Paths from discrete data

2.1.2 The lead-lag transformation

2.1.3 The signature of paths

2.1.4 Rules for the sign of the enclosed area

2.1.5

Statistical moments from the signature

– i –

1

2

2

3

4

4

6

7

10

11

11

12

13

14

15

17

17

18

18

19

19

20

22

24

24

2.2 The signature in machine learning

2.2.1 Application of the signature method to data streams

2.2.2 Dealing with missing data in time-series

2.3 Computational considerations of the signature

2.4 Overview of recent progress of the signature method in machine learning

28

29

32

34

34

2.4.1 Extracting information from the signature of a ﬁnancial data stream 34

2.4.2

Sound compression - the rough paths approach

2.4.3 Character recognition

2.4.4 Learning from the past, predicting the statistics for the future, learn-

ing an evolving system

2.4.5

Identifying patterns in MEG scans

2.4.6 Learning the eﬀect of treatment on behavioural patterns of patients

with bipolar disorder.

37

37

38

39

41

1 Theoretical Foundations

The purpose of the ﬁrst part of these notes is to introduce the deﬁnition of the signature
and present some of its fundamental properties. The point of view we take in these notes
is that the signature is an object associated with a path which captures many of the path’s
important analytic and geometric properties.

The signature has recently gained attention in the mathematical community in part due
to its connection with Lyons’ theory of rough paths. At the end of this ﬁrst part, we
shall brieﬂy highlight the role the signature plays in the theory of rough paths on an
informal level. However one of the main points we wish to emphasize is that no knowledge
beyond classical integration theory is required to deﬁne and study the basic properties of
the signature. Indeed, K. T. Chen was one of the ﬁrst authors to study the signature, and
his primary results can be stated completely in terms of piecewise smooth paths, which
already provide an elegant and deep mathematical theory.

While we attempt to make all statements mathematically precise, we refrain from going
into complete detail. A further in-depth discussion, along with proofs of many of the results
covered in the ﬁrst part, can now be found in several texts, and we recommend the St.
Flour lecture notes [20] for the curious reader.

– 1 –

1.1 Preliminaries

1.1.1 Paths in Euclidean space

Paths form one of the basic elements of this theory. A path X in Rd is a continuous
[a, b] (cid:55)→ Rd. We will use the
mapping from some interval [a, b] to Rd, written as X :
subscript notation Xt = X(t) to denote dependence on the parameter t ∈ [a, b].

For our discussion of the signature, unless otherwise stated, we will always assume that
paths are piecewise diﬀerentiable (more generally, one may assume that the paths are of
bounded variation for which exactly the same classical theory holds). By a smooth path,
we mean a path which has derivatives of all orders.
Two simple examples of smooth paths in R2 are presented in Fig.1:
t } = {t, t3} , t ∈ [−2, 2]
t } = {cos t, sin t} , t ∈ [0, 2π]

left panel: Xt = {X 1
right panel: Xt = {X 1

t , X 2
t , X 2

(1.1)

Figure 1: Example of two-dimensional smooth paths.

This parametrisation generalizes in d-dimensions (Xt ∈ Rd) as:

(cid:110)

X : [a, b] (cid:55)→ Rd , Xt =

X 1

t , X 2

t , X 3

t , ..., X d
t

(cid:111)

.

(1.2)

(1.3)

An example of a piecewise linear path is presented in Fig.2:

Xt = {X 1

t , X 2

t } = {t, f (t)} , t ∈ [0, 1],

where f is a piecewise linear function on the time domain [0, 1]. One possible example of the
function f is a stock price at time t. Such non-smooth paths may represent sequential data
or time series, typically consisting of successive measurements made over a time interval.

– 2 –

Figure 2: Example of non-smooth stochastic path.

1.1.2 Path integrals

We now brieﬂy review the path (or line) integral. The reader may already be familiar with
the common deﬁnition a path integral against a ﬁxed function f (also called a one-form).
Namely, for a one-dimensional path X : [a, b] (cid:55)→ R and a function f : R (cid:55)→ R, the path
integral of X against f is deﬁned by

f (Xt) dXt =

f (Xt) ˙Xtdt,

(1.4)

a

a

where the last integral is the usual (Riemann) integral of a continuous bounded function
and where we use the “upper-dot” notation for diﬀerentiation with respect to a single
variable:

˙Xt = dXt/dt.

In the expression (1.4), note that f (Xt) is itself a real-valued path deﬁned on [a, b]. In fact,
(1.4) is a special case of the Riemann-Stieltjes integral of one path against another. In
general, one can integrate any path Y : [a, b] (cid:55)→ R against a path X : [a, b] (cid:55)→ R. Namely,
for a path Y : [a, b] (cid:55)→ R, we can deﬁne the integral

(cid:90) b

(cid:90) b

(cid:90) b

(cid:90) b

Yt dXt =

a

a

Yt ˙Xtdt.

(1.5)

As previously remarked, we recover the usual path integral upon setting Yt = f (Xt).
Example 1. Consider the constant path Yt = 1 for all t ∈ [a, b]. Then the path integral
of Y against any path X : [a, b] (cid:55)→ R is simply the increment of X:

˙Xt dt = Xb − Xa.

(1.6)

Example 2. Consider the path Xt = t for all t ∈ [a, b]. It follows that
˙Xt = 1 for all
t ∈ [a, b], and so the path integral for any Y : [a, b] (cid:55)→ R is the usual Riemann integral of
Y :

(cid:90) b

(cid:90) b

dXt =

a

a

(cid:90) b

(cid:90) b

a

a

– 3 –

YtdXt =

Yt dt.

(1.7)

Example 3. We present an example involving numerical computations. Consider the
two-dimensional path

Xt = {X 1

t , X 2

t } = {t2, t3},

t ∈ [0, 1].

Then we can compute the path integral

(cid:90) 1

0

X 1

t dX 2

t =

(cid:90) 1

0

t23t2dt =

3
5

.

(1.8)

(1.9)

The above example is a special case of an iterated integral, which, as discussed in the
following section, is central to the deﬁnition of the path signature.

1.2 The signature of a path

1.2.1 Deﬁnition

Having recalled the path integral of one real-valued path against another, we are now ready
to deﬁne the signature of a path. For a path X : [a, b] (cid:55)→ Rd, recall that we denote the
t ), where each X i : [a, b] (cid:55)→ R is a real-valued path. For any
coordinate paths by (X 1
single index i ∈ {1, . . . , d}, let us deﬁne the quantity

t , . . . X d

S(X)i

a,t =

a<s<t

dX i

s = X i

t − X i
0,

(1.10)

which is the increment of the i-th coordinate of the path at time t ∈ [a, b]. We emphasise
a,· : [a, b] (cid:55)→ R is itself a real-valued path. Note that a in the subscript of S(X)i
that S(X)i
is only used to denote the starting point of the interval [a, b].
Now for any pair i, j ∈ {1, . . . , d}, let us deﬁne the double-iterated integral

a,t

(cid:90)

(cid:90)

S(X)i,j

a,t =

S(X)i

a,sdX j

s =

dX i

r dX j
s ,

(1.11)

a<s<t

a<r<s<t

where S(X)i

a,s is given by (1.10) and the integration limits are simply:

(cid:90)

(cid:40)

a < r < s < t =

a < r < s

a < s < t.

(1.12)

The integration limits (1.12) also correspond to integration over a triangle (or, more gen-
erally, over a simplex in higher dimension). We emphasise again that S(X)i
s are
simply real-valued paths, so the expression (1.11) is a special case of the path integral, and
that S(X)i,j
Likewise for any triple i, j, k ∈ {1, . . . , d} we deﬁne the triple-iterated integral

a,· : [a, b] (cid:55)→ R is itself a real-valued path.

a,s and X j

a<s<t

a<q<r<s<t

dX i

qdX j

r dX k
s .

(1.13)

(cid:90)

S(X)i,j,k

a,t =

(cid:90)

S(X)i,j

a,sdX k

s =

– 4 –

a,s and X k

Again, since S(X)i,j
path integral, and S(X)i,j,k
a,·
We can continue recursively, and for any integer k ≥ 1 and collection of indexes i1, . . . , ik ∈
{1, . . . , d}, we deﬁne

s are real-valued paths, the above is just a special case of the
: [a, b] (cid:55)→ R is itself a real-valued path.

S(X)i1,...,ik

a,t

=

S(X)i1,...,ik−1

a,s

dX ik
s .

(1.14)

(cid:90)

a<s<t

(cid:90)

(cid:90)

. . .

a<tk<t

a<t1<t2

As before, since S(X)i1,...,ik−1
a,s
integral, and S(X)i1,...,ik
equivalently write

a,·

and X ik

s are real-valued paths, the above is deﬁned as a path
: [a, b] (cid:55)→ R is itself a real-valued path. Observe that we may

S(X)i1,...,ik

a,t

=

dX i1
t1

. . . dX ik
tk

.

(1.15)

a,b

is called the k-fold iterated integral of X along the indexes

The real number S(X)i1,...,ik
i1, . . . , ik.
Deﬁnition 1 (Signature). The signature of a path X : [a, b] (cid:55)→ Rd, denoted by S(X)a,b,
is the collection (inﬁnite series) of all the iterated integrals of X. Formally, S(X)a,b is the
sequence of real numbers

S(X)a,b = (1, S(X)1

a,b, . . . , S(X)d

a,b, S(X)1,1

a,b, S(X)1,2

a,b, . . .)

(1.16)

where the “zeroth” term, by convention, is equal to 1, and the superscripts run along the
set of all multi-indexes

W = {(i1, . . . , ik) | k ≥ 1, i1, . . . , ik ∈ {1, . . . , d}}.

(1.17)

The set W above is also frequently called the set of words on the alphabet A = {1, . . . , d}
consisting of d letters.
Example 4. Consider an alphabet consisting of three letters only: {1, 2, 3}. There is
inﬁnite number of words which could be composed from this alphabet, namely:
{1, 2, 3} → (1, 2, 3, 11, 12, 13, 21, 22, 23, 31, 32, 33, 111, 112, 113, 121, . . . ).

(1.18)

An important property of the signature which we immediately note is that the iterated
integrals of a path X are independent of the starting point of X. That is, if for some

x ∈ Rd, we deﬁne the path (cid:101)Xt = Xt + x, then S((cid:101)X)i1,...,ik

= S(X)i1,...,ik

a,b

.

a,b

We shall often consider the k-th level of the signature, deﬁned as the ﬁnite collection of all
terms S(X)i1,...,ik
where the multi-index is of length k. For example, the ﬁrst level of the
signature is the collection of d real numbers S(X)1
a,b, and the second level is
the collection of d2 real numbers

a,b, . . . , S(X)d

a,b

S(X)1,1

a,b, . . . , S(X)1,d

a,b, S(X)2,1

a,b, . . . , S(X)d,d
a,b .

(1.19)

– 5 –

1.2.2 Examples

Example 5. The simplest example of a signature which one should keep in mind is that
of a one-dimensional path.
In this case our set of indexes (or alphabet) is of size one,
A = {1}, and the set of multi-indexes (or words) is W = {(1, . . . , 1) | k ≥ 1}, where 1
appears k times in (1, . . . , 1).
Consider the path X : [a, b] (cid:55)→ R, Xt = t. One can immediately verify that the signature
of X is given by

S(X)1

a,b = Xb − Xa,

S(X)1,1

a,b =

S(X)1,1,1
a,b =
... .

(Xb − Xa)2
(Xb − Xa)3

2!

3!

(1.20)

,

,

One can in fact show that the above expression of the signature remains true for any path
X : [a, b] (cid:55)→ R. Hence, for one-dimensional paths, the signature depends only on the
increment Xb − Xa.

Example 6. We present now a more involved example of the signature for a two-dimensional
path. Our set of indexes is now A = {1, 2}, and the set of multi-indexes is

W = {(i1, . . . , ik) | k ≥ 1, i1, . . . , ik ∈ {1, 2}},

(1.21)
the collection of all ﬁnite sequences of 1’s and 2’s. Consider a parabolic path in R2 as
depicted in Fig.3.

Figure 3: Example of a two dimensional path parametrized in (1.22).

– 6 –

Explicitly:

Xt = {X 1
dXt = {dX 1

t , X 2
t , dX 2

t } = {3 + t, (3 + t)2} t ∈ [0, 5] , (a = 0, b = 5),

t } = {dt, 2(3 + t)dt}.
(cid:90) 5
(cid:90) 5

dt = X 1

0

2 (3 + t) dt = X 2

0 = 5,

A straightforward computation gives:

(cid:90)
(cid:90)
(cid:90)(cid:90)
(cid:90)(cid:90)
(cid:90)(cid:90)
(cid:90)(cid:90)
(cid:90)(cid:90)(cid:90)

0<t<5

dX 1

t =

dX 2

t =

0<t<5

0

dX 1

t1dX 1

t2 =

dX 1

t1dX 2

t2 =

dX 2

t1dX 1

t2 =

dX 2

t1dX 2

t2 =

0<t1<t2<5

0<t1<t2<5

0<t1<t2<5

0<t1<t2<5

0<t1<t2<t3<5

.

S(X)1

0,5 =

S(X)2

0,5 =

S(X)1,1

0,5 =

S(X)1,2

0,5 =

S(X)2,1

0,5 =

S(X)2,2

0,5 =

S(X)1,1,1

0,5 =
...

(1.22)

(1.23)

3025

2

,

0

0

5 − X 1
(cid:20)(cid:90) t2
(cid:90) 5
(cid:20)(cid:90) t2
(cid:90) 5
(cid:20)(cid:90) t2
(cid:90) 5
(cid:20)(cid:90) t2
(cid:90) 5
(cid:90) 1

0

0

0

0

0

0

(cid:21)
5 − X 2
(cid:21)

dt1

0 = 55,

dt2 =

25
2

,

dt1

2 (3 + t2) dt2 =

2 (3 + t1) dt1

dt2 =

(cid:21)
(cid:21)
(cid:20)(cid:90) t2

(cid:20)(cid:90) t3

475
3

350
3

,

,

(cid:21)

(cid:21)

2 (3 + t1) dt1

2 (3 + t2) dt2 =

dX 1

t1dX 1

t2dX 1

t3 =

0

0

0

dt1

dt2

dt3 =

125
6

,

Continuing this way, one can compute every term S(X)i1,...,ik
multi-index (i1, . . . , ik), i1, . . . , ik ∈ {1, 2}.

0,5

of the signature for every

1.2.3 Picard iterations: motivation for the signature

Before reviewing the basic properties of the signature, we take a moment to show how the
signature arises naturally in the classical theory of ordinary diﬀerential equations (ODEs).
In a sense, this provides one of the ﬁrst reasons for our interest in the signature. We do
not provide all the details, but hope that the reader ﬁnds the general idea clear.

It is instructive to start the discussion with an intuitive and simplistic example of Picard’s
method. Let us consider the ﬁrst order ODE

dy
dx

= f (x, y),

y(x0) = y0,

(1.24)

where y(x) is a real valued function of a scalar variable x. Picard’s method allows us to
construct an approximated solution to (1.24) in the form of an iterative series. The integral
form of (1.24) is given by

y(x) = y(x0) +

f (t, y(t)) dt.

(1.25)

(cid:90) x

x0

– 7 –

(cid:90) x

We now deﬁne a sequence of functions yk(x), k = 0, 1, . . ., where the ﬁrst term is the
constant function y0(x) = y(x0), and for k ≥ 1, we deﬁne inductively

yk(x) = y(x0) +

f (t, yk−1(t)) dt.

(1.26)

x0

The classical Picard-Lindel¨of theorem states that, under suitable conditions, the solution
to (1.24) is given by y(x) = limk→∞ yk(x).

Example 7. Consider the ODE:

(1.27)

(1.28)

dy
dx

= y(x),

y(0) = 1.

The ﬁrst k terms of the Picard iterations are given by:

(cid:90) x
(cid:90) x
(cid:90) x
(cid:90) x

0

0

0

y0(x) = 1

y1(x) = 1 +

y2(x) = 1 +

y3(x) = 1 +

y4(x) = 1 +

...

yk(x) =

0

1
n!

xn,

k(cid:88)

n=0

y0(t)dt = 1 + x

y1(t)dt = 1 + x +

y2(t)dt = 1 + x +

y3(t)dt = 1 + x +

1
2
1
2
1
2

x2

x2 +

x2 +

1
6
1
6

x3

x3 +

1
24

x4

which converges to y(x) = ex as k → ∞, which is indeed the solution to (1.27). These
approximations are plotted in Fig. 4.

Figure 4: Example of sequential Picard approximation to the true solution.

– 8 –

We are now ready to consider a controlled diﬀerential equation and the role of the signature
in its solution. Consider a path X : [a, b] (cid:55)→ Rd. Let L(Rd, Re) denote the vector space of
linear maps from Rd to Re. Equivalently, L(Rd, Re) can be regarded as the vector space of
d × e real matrices. For a path Z : [a, b] (cid:55)→ L(Rd, Re), note that we can deﬁne the integral

(cid:90) b

a

(cid:90) t

(cid:90) t

a

ZtdXt

(1.29)

as an element of Re in exactly the same way as the usual path integral. For a function
V : Re (cid:55)→ L(Rd, Re) and a path Y : [a, b] (cid:55)→ Re, we say that Y solves the controlled
diﬀerential equation

dYt = V (Yt)dXt, Ya = y ∈ Re,

precisely when for all times t ∈ [a, b]

The map V in the above expression is often called a collection of driving vector ﬁelds, the
path X is called the control or the driver, and Y is called the solution or the response.

a

Yt = y +

V (Ys)dXs.

A standard procedure to obtain a solution to (1.31) is through Picard iterations. For an
arbitrary path Y : [a, b] (cid:55)→ Re, deﬁne a new path F (Y ) : [a, b] (cid:55)→ Re by

(1.30)

(1.31)

F (Y )t = y +

V (Ys)dXs.

(1.32)

t = F (Y n−1)t with initial arbitrary path Y 0

Observe that Y a solution to (1.31) if and only if Y is a ﬁxed point of F . Consider
the sequence of paths Y n
t (often taken as the
constant path Y 0
t = y). Under suitable assumptions, one can show that F possesses a
unique ﬁxed point Y and that Y n
Consider now the case when V : Re (cid:55)→ L(Rd, Re) is a linear map. Note that we may
equivalently treat V as a linear map Rd (cid:55)→ L(Re, Re), where L(Re, Re) is the space of all
e× e real matrices. Let us start the Picard iterations with the initial constant path Y 0
t = y
for all t ∈ [a, b]. Denoting by Ie the identity operator (or matrix) in L(Re, Re), it follows
that the iterates of F can be expressed as follows:

t converges to Y as n → ∞.

a

(cid:90) t
(cid:90) t
(cid:90) t

a

a

Y 0
t = y,

Y 1
t = y +

Y 2
t = y +
...

Y n
t = y +

...

V (Y 0

s )dXs =

V (Y 1

s )dXs =

dV (Xs) + Ie

(y),

dV (Xu)dV (Xs) +

(1.33)

(cid:90) t

a

(cid:19)

dV (Xs) + Ie

(y),

(cid:33)

V (Y n−1

s

)dXs =

dV (Xt1) . . . dV (Xtk ) + Ie

(y),

(cid:19)

a

(cid:18)(cid:90) t
(cid:18)(cid:90) t
(cid:90) s
(cid:32) n(cid:88)
(cid:90)

a

a

k=1

a<t1<...<tk<t

– 9 –

Due to the fact that L(Re, Re) is an algebra of matrices, each quantity

(cid:90)

dV (Xt1) . . . dV (Xtk )

(1.34)

a<t1<...<tk<t

can naturally be deﬁned as an element of L(Re, Re), which, one can check, is completely
determined (in a linear way) by the k-th level of the signature S(X)a,t of X at time t ∈ [a, b].

The conclusion we obtain is that the solution Yt is completely determined by the signature

S(X)a,t for every t ∈ [a, b]. In particular, if the signatures of two controls X and (cid:101)X coincide
at time t ∈ [a, b], that is, S(X)a,t = S((cid:101)X)a,t, then the corresponding solutions to (1.31)

will also agree at time t for any choice of the linear vector ﬁelds V .

An important, but far less obvious result, is that the same conclusion holds true for non-
linear vector ﬁelds V . This result was ﬁrst obtained by Chen [5] for a certain class of
piecewise smooth paths, and recently extended by Hambly and Lyons [14] to paths of
bounded variation, and by Boedihardjo, Geng, Lyons and Yang [2] to a completely non-
smooth setting of geometric rough paths for which the signature is still well-deﬁned (see
Section 1.4 for further discussion). The latter class of paths is of particular interest from
the point of view of stochastic analysis.

1.2.4 Geometric intuition of the ﬁrst two levels

a,b, . . . , S(X)d

While the signature is deﬁned analytically using path integrals, we brieﬂy discuss here the
geometric meaning of the ﬁrst two levels. As already mentioned, the ﬁrst level, given by
a,b), is simply the increment of the path X : [a, b] (cid:55)→ Rd. For
the terms (S(X)1
the second level, note that the term S(X)i,i
a)2/2. This relation
is a special case of the shuﬄe product which we shall review in Section 1.3.2. To give
a,b for i (cid:54)= j, consider the L´evy area (illustrated in Fig.5), which
meaning to the term S(X)i,j
is a signed area enclosed by the path (solid red line) and the chord (blue straight dashed
t } is
line) connecting the endpoints. The L´evy area of the two dimensional path {X 1
given by:

a,b is always equal to (X i

b − X i

t , X 2

(cid:16)

(cid:17)

A =

1
2

S(X)1,2

a,b − S(X)2,1

a,b

.

(1.35)

The signed areas denoted by A− and A+ are the negative and positive areas respectively,
and ∆X 1 and ∆X 2 represent the increments along each coordinate.

– 10 –

Figure 5: Example of signed L´evy area of a curve. Areas above and under the chord
connecting two endpoints are negative and positive respectively.

1.3

Important properties of signature

We now review several fundamental properties of the signature of paths. We do not provide
all the details behind the proofs of these properties, but we emphasize that they are all
straightforward consequences of classical integration theory. Several deeper results are
discussed in the following Section 1.4, but only on an informal level.

1.3.1 Invariance under time reparametrisations
We call a surjective, continuous, non-decreasing function ψ : [a, b] (cid:55)→ [a, b] a reparametriza-
tion. For simplicity, we shall only consider smooth reparametrizations, although, just like
in the deﬁnition of the path integral, this is not strictly necessary.
Let X, Y : [a, b] (cid:55)→ R be two real-valued paths and ψ : [a, b] (cid:55)→ [a, b] a reparametrization.

Deﬁne the paths (cid:101)X,(cid:101)Y : [a, b] (cid:55)→ R by (cid:101)Xt = Xψ(t) and (cid:101)Yt = Yψ(t). Observe that

from which it follows that(cid:90) b

(cid:101)Ytd(cid:101)Xt =

a

a

˙(cid:101)X t = ˙Xψ(t)
(cid:90) b

Yψ(t)

˙Xψ(t)

˙ψ(t),

˙ψ(t)dt =

(cid:90) b

a

(1.36)

YudXu,

(1.37)

where the last equality follows by making the substitution u = ψ(t). This shows that path
integrals are invariant under a time reparametrization of both paths.

– 11 –

Consider now a multi-dimensional path X : [a, b] (cid:55)→ Rd and a reparametrization ψ : [a, b] (cid:55)→

[a, b]. As before, denote by (cid:101)X : [a, b] (cid:55)→ Rd the reparametrized path (cid:101)Xt = Xψ(t). Since

is deﬁned as an iterated path integral of X, it

every term of the signature S(X)i1,...,ik
follows from the above that

a,b

S((cid:101)X)i1,...,ik

a,b

= S(X)i1,...,ik

a,b

, ∀k ≥ 0, i1, . . . , ik ∈ {1, . . . , d}.

(1.38)

That is to say, the signature S(X)a,b remains invariant under time reparametrizations of
X.

1.3.2 Shuﬄe product

One of the fundamental properties of the signature, shown originally by Ree [22], is that
the product of two terms S(X)i1,...,ik
can always be expressed as a sum of
another collection of terms of S(X)a,b which only depends on the multi-indexes (i1, . . . , ik)
and (j1, . . . , jm).

and S(X)j1,...,jm

a,b

a,b

To make this statement precise, we deﬁne the shuﬄe product of two multi-indexes. First,
a permutation σ of the set {1, . . . , k + m} is called a (k, m)-shuﬄe if σ−1(1) < . . . < σ−1(k)
and σ−1(k + 1) < . . . < σ−1(k + m). The list (σ(1), . . . , σ(k + m)) is also called a shuﬄe
of (1, . . . , k) and (k + 1, . . . , k + m). Let Shuﬄes(k, m) denote the collection of all (k, m)-
shuﬄes.

Deﬁnition 2 (Shuﬄe product). Consider two multi-indexes I = (i1, . . . , ik) and J =
(j1, . . . , jm) with i1, . . . , ik, j1, . . . , jm ∈ {1, . . . , d}. Deﬁne the multi-index

(r1, . . . , rk, rk+1, . . . rk+m) = (i1, . . . , ik, j1, . . . , jm).

(1.39)

The shuﬄe product of I and J, denoted I  J, is a ﬁnite set of multi-indexes of length
k + m deﬁned as follows

I  J = {(rσ(1), . . . , rσ(k+m)) | σ ∈ Shuﬄes(k, m)}.

(1.40)
Theorem 1 (Shuﬄe product identity). For a path X : [a, b] (cid:55)→ Rd and two multi-indexes
I = (i1, . . . , ik) and J = (j1, . . . , jm) with i1, . . . , ik, j1, . . . , jm ∈ {1, . . . , d}, it holds that

S(X)I

a,bS(X)J

a,b =

S(X)K

a,b.

(1.41)

(cid:88)

K∈IJ

Example 8. To make things more clear, let us consider a simple example of a two-
dimensional path X : [a, b] (cid:55)→ R2. The shuﬄe product implies that

S(X)1
S(X)1,2

a,bS(X)2
a,bS(X)1

a,b = S(X)1,2
a,b = 2S(X)1,1,2

a,b + S(X)2,1
a,b,
a,b + S(X)1,2,1

a,b

(1.42)

.

The shuﬄe product in particular implies that the product of two terms of the signature
can be expressed as a linear combination of higher order terms. This fact will be useful for
practical applications of the signature to regression analysis which will be further discussed
in Chapter 2.

– 12 –

1.3.3 Chen’s identity

We now describe a property of the signature known as Chen’s identity, which provides an
algebraic relationship between paths and their signatures. To formulate Chen’s identity,
we need to introduce the algebra of formal power series, which we have not mentioned thus
far, but which should appear natural in light of the deﬁnition of the signature.

Deﬁnition 3 (Formal power series). Let e1, . . . , ed be d formal indeterminates. The algebra
of (non-commuting) formal power series in d indeterminates is the vector space of all series
of the form

λi1,...,ik ei1 . . . eik ,

(1.43)

where the second summation runs over all multi-indexes (i1, . . . , ik), i1, . . . , ik ∈ {1, . . . , d},
and λi1,...,ik are real numbers.

A (non-commuting) formal polynomial is a formal power series for which only a ﬁnite
number of coeﬃcients λi1,...,ik are non-zero. The terms ei1 . . . eik are called monomials.
The term corresponding to k = 0 is simply just a real number λ0. The space of formal
power series is often also called the tensor algebra of Rd. We stress that the power series
we consider are non-commutative; for example, the elements e1e2 and e2e1 are distinct.

Observe that the space of formal power series may be naturally equipped with a vector
space structure by deﬁning addition and scalar multiplication as

∞(cid:88)

(cid:88)

k=0

i1,...,ik∈{1,...d}

 ∞(cid:88)

(cid:88)

k=0

i1,...,ik∈{1,...d}

λi1,...,ik ei1 . . . eik

µi1,...,ik ei1 . . . eik

 +
 ∞(cid:88)
∞(cid:88)

k=0

=

(cid:88)

i1,...,ik∈{1,...d}

(cid:88)

i1,...,ik∈{1,...d}



(λi1,...,ik + µi1,...,ik )ei1 . . . eik

(1.44)

and

 ∞(cid:88)

k=0

c

(cid:88)

i1,...,ik∈{1,...d}

k=0

λi1,...,ik ei1 . . . eik

 =

∞(cid:88)

(cid:88)

k=0

i1,...,ik∈{1,...d}

cλi1,...,ik ei1 . . . .eik .

(1.45)

Moreover, one may deﬁne the product ⊗ between monomials by joining together multi-
indexes

ei1 . . . eik ⊗ ej1 . . . ejm = ei1 . . . eik ej1 . . . ejm.

(1.46)

– 13 –

The product ⊗ then extends uniquely and linearly to all power series. We demonstrate the
ﬁrst few terms of the product in the following expression

 ∞(cid:88)

(cid:88)

(cid:88)

 ⊗

 ∞(cid:88)
d(cid:88)

k=0

d(cid:88)



k=0

i1,...,ik∈{1,...d}

i1,...,ik∈{1,...d}

λi1,...,ik ei1 . . . eik

µi1,...,ik ei1 . . . eik

= λ0µ0 +

(λ0µi + λiµ0)ei +

(λ0µi,j + λiµj + λi,jµ0) eiej + . . . .

(1.47)

i=1

i,j=1

The space of formal power series becomes an algebra when equipped with this vector space
structure and the product ⊗.

The reader may have noticed that the indexing set of the monomials ei1 . . . eik coincides
with the indexing set of the terms of the signature of a path X : [a, b] (cid:55)→ Rd, namely
the collection of all multi-indexes (i1, . . . , ik), i1, . . . , ik ∈ {1, . . . , d}.
It follows that a
convenient way to express the signature of X is by a formal power series where the coeﬃcient
of each monomial ei1 . . . eik is deﬁned to be S(X)i1,...,ik
. We use the same symbol S(X)a,b
to denote this representation

a,b

S(X)a,b =

S(X)i1,...,ik

a,b

ei1 . . . eik ,

(1.48)

∞(cid:88)

(cid:88)

k=0

i1,...,ik∈{1,...d}

where, as before, we set the “zero-th” level of the signature S(X)0
k = 0).

a,b = 1 (corresponding to

To state Chen’s identity, it remains to deﬁne the concatenations of paths.
Deﬁnition 4 (Concatenation). For two paths X : [a, b] (cid:55)→ Rd and Y : [b, c] (cid:55)→ Rd, we
deﬁne their concatenation as the path X ∗ Y : [a, c] (cid:55)→ Rd for which (X ∗ Y )t = Xt for
t ∈ [a, b] and (X ∗ Y )t = Xb + (Yt − Yb) for t ∈ [b, c].
Chen’s identity informally states that the signature turns the “concatenation product” ∗
into the product ⊗. More precisely, we have the following result.
Theorem 2 (Chen’s identity). Let X : [a, b] (cid:55)→ Rd and Y : [b, c] (cid:55)→ Rd be two paths. Then

S(X ∗ Y )a,c = S(X)a,b ⊗ S(Y )b,c.

(1.49)

1.3.4 Time-reversal

The time-reversal property informally states that the signature S(X)a,b of a path X :
[a, b] (cid:55)→ Rd is precisely the inverse under the product ⊗ of the signature obtained from
running X backwards in time. To make this precise, we make the following deﬁnition.
Deﬁnition 5 (Time-reversal). For a path X : [a, b] (cid:55)→ Rd, we deﬁne its time-reversal as
the path

←−
X t = Xa+b−t for all t ∈ [a, b].

←−
X : [a, b] (cid:55)→ Rd for which

– 14 –

Theorem 3 (Time-reversed signature). For a path X : [a, b] (cid:55)→ Rd, it holds that

S(X)a,b ⊗ S(

←−
X )a,b = 1.

(1.50)

The element 1 in the above expression should be understood as the formal power series
where λ0 = 1 and λi1,...,ik = 0 for all k ≥ 1 and i1, . . . , ik ∈ {1, . . . , d}, which is the identity
element under the product ⊗.

1.3.5 Log signature

We now deﬁne a transform of the path signature called the log signature. The log signature
essentially corresponds to taking the formal logarithm of the signature in the algebra of
formal power series.

To this end, for a power series

x =

∞(cid:88)

(cid:88)

k=0

i1,...,ik∈{1,...d}

λi1,...,ik ei1 . . . eik

(1.51)

for which λ0 > 0, deﬁne its logarithm as the power series given by

(cid:88)

(−1)n

n≥1

n

(cid:19)⊗n

(cid:18)

1 − x
λ0

log x = log(λ0) +

,

(1.52)

where ⊗n denotes the n-th power with respect to the product ⊗.
For example, for a real number λ ∈ R and the series

(cid:88)

k≥1

x = 1 +

λk
k!

e⊗k
1 ,

one can readily check that

log x = λe1.

(1.53)

(1.54)

Observe that, in general, log x is a series with an inﬁnite number of terms, however for every
multi-index (i1, . . . , ik), the coeﬃcient of ei1 . . . eik in log x depends only on the coeﬃcients
of x of the form λj1,...,jm with m ≤ k, of which there are only ﬁnitely many, so that log x
is well-deﬁned without the need to consider convergence of inﬁnite series.
Deﬁnition 6 (Log signature). For a path X : [a, b] (cid:55)→ Rd, the log signature of X is deﬁned
as the formal power series log S(X)a,b.

For two formal power series x and y, let us deﬁne their Lie bracket by

[x, y] = x ⊗ y − y ⊗ x.

(1.55)

– 15 –

A direct computation shows that the ﬁrst few terms of the log signature are given by

log S(X)a,b =

S(X)i

a,bei +

S(X)i,j

a,b − S(X)j,i

a,b

[ei, ej] + . . . .

(1.56)

d(cid:88)

i=1

(cid:88)

(cid:16)

1
2

1≤i<j≤d

(cid:17)

In particular one can see that the coeﬃcient of the polynomials [ei, ej] in the log signature
is precisely the L´evy area introduced in Section 1.2.4.

Example 9. Consider the two-dimensional path

X : [0, 2] (cid:55)→ R2, Xt =

if t ∈ [0, 1],
if t ∈ [1, 2].

(1.57)

(cid:40){t, 0}

{1, t − 1}

Note that X is the concatenation of the two linear paths, Y : [0, 1] (cid:55)→ R2, Yt = {t, 0}, and
Z : [1, 2] (cid:55)→ R2, Zt (cid:55)→ {0, t − 1}. One can readily check that the signatures of Y and Z (as
formal power series) are given by

S(Y )0,1 = 1 +

e⊗k
1 , S(Z)1,2 = 1 +

1
k!

e⊗k
2 .

1
k!

(1.58)

(cid:88)

k≥1

(cid:88)

k≥1

It follows by Chen’s identity that

S(X)0,2 = S(Y )0,1 ⊗ S(Z)1,2 = 1 + e1 + e2 +

1
2!

e1 +

1
2!

e2 + e1e2 + . . . .

(1.59)

Hence the ﬁrst few terms of the log signature of X are

log S(X)0,2 = e1 + e2 +

1
2

[e1, e2] + . . . .

(1.60)

In fact, one can readily check that coeﬃcients of log S(X)0,2 in the above example are given
precisely by the classical Campbell-Baker-Hausdorﬀ formula.

The above example in fact demonstrates the general fact that the log signature can always
be expressed as a power series composed entirely of so-called Lie polynomials. This is the
content of the following theorem due to Chen [4], which generalises the Campbell-Baker-
Hausdorﬀ theorem.
Theorem 4. Let X : [a, b] (cid:55)→ Rd be a path. Then there exist real numbers λi1,...,ik such
that

log S(X)a,b =

λi1,...,ik [ei1, [ei2, . . . , [eik−1, eik ] . . .]].

(1.61)

(cid:88)

(cid:88)

k≥1

i1,...,ik∈{1,...,d}

Note that the coeﬃcients λi1,...,ik are in general not unique since the polynomials of the
form [ei1, [ei2, . . . , [eik−1, eik ] . . .]] are not linearly independent (e.g., [e1, e2] = −[e2, e1]).

– 16 –

1.4 Relation with rough paths and path uniqueness

We conclude the ﬁrst part of these notes with a brief discussion about the role of the
signature in the theory of rough paths and the extent to which the signature is able to
determine the underlying path. These topics are substantially more involved than the basic
properties of the signature discussed above, and so we only oﬀer an informal discussion.

1.4.1 Rough paths

Our discussion of the signature has so far been restricted to paths which are piecewise
diﬀerentiable (or more generally of bounded variation). This restriction was needed to
ensure that the iterated integrals of the path existed as Riemann-Stieltjes integrals. More
generally, one can deﬁne the iterated integrals of a path using the Young integral for any
path of ﬁnite p-variation with 1 ≤ p < 2. The Young integral goes beyond the “classical”
deﬁnition of the integral and is already able to cover a class of paths substantially more
irregular than those of bounded variation.

A problem that arises for paths of inﬁnite p-variation for all p < 2 (which is a situation
of great interest in stochastic analysis due to the fact that the sample paths of Brownian
motion have almost surely ﬁnite p-variation if and only if p > 2), is that one can show there
is no well-deﬁned notion of an iterated integral for such paths. We stress that this is not
due to any technical limitation of the Young integral, but rather due to the fact that there
is no unique candidate for the iterated integrals. That is to say, there is not necessarily
only one unique way to deﬁne the iterated integrals.

One of the key observations of T. Lyons in his introduction of rough paths in [19] was that
if one deﬁnes the ﬁrst (cid:98)p(cid:99) iterated integrals of a path X of ﬁnite p-variation, then there
is indeed a unique way to obtain all the other iterated integrals, and hence the signature
of X. This notion of a path of ﬁnite p-variation, along with its ﬁrst (cid:98)p(cid:99) iterated integrals
(which may be deﬁned arbitrarily provided that they satisfy Chen’s identity and possess
ﬁnite p-variation), is precisely the deﬁnition of a p-rough path.

A key feature of the theory of rough paths, known now as the universal limit theorem,
is that one is able to give meaning to controlled diﬀerential equations where the driver
belongs to a special class of p-rough paths (know as geometric p-rough paths), and where
the solution depends in a continuous way on the driver provided that the space of p-rough
paths is equipped with a suitable topology (known as the p-variation metric).

For the reader interested, we strongly recommend the St. Flour lecture notes of Lyons,
Caruana, and L´evy [20] for an introduction to the theory of rough paths, and the mono-
graph of Friz and Hairer [9] for a more recent treatment of the topic and an introduction
to Hairer’s emerging theory of regularity structures in the study of stochastic partial dif-
ferential equations.

– 17 –

1.4.2 Path uniqueness
As discussed in Section 1.2.3, the signature of a path X : [a, b] (cid:55)→ Rd is all that is needed to
determine the endpoint of the solution to a linear (and, less trivially, non-linear) diﬀerential
equation driven by X, which was ﬁrst shown by Chen [5] in the smooth setting, and later
extended by Hambly and Lyons [14] and Boedihardjo et al. [2] to less regular paths. The
works of these authors in fact shows that the signature captures deep geometric properties
of a path, which we brieﬂy discuss here.

A natural question one may ask is the following:
is a path completely determined by
its signature? In light of the time-reversal property and Chen’s identity, as well as the
invariance of the signature under time reparametrizations, the answer, in general, is no.
For example, one can never recover from the signature the exact speed at which the path
is traversed (due to invariance under time reparametrizations), nor can one tell apart the
signature of a trivial constant path and that of a path concatenated with its time-reversal.

However, a far less elementary fact is that this is essentially the only information one loses
from the signature. For example, for a path X which never crosses itself, the signature is
able to completely describe the image and direction of traversal of the path (that is, all
the points that X visits and the order in which it visits them). This demonstrates the
signature’s ability to completely determine the geometric properties of a path which does
not possess degeneracies of a certain kind consisting of movements going directly back onto
itself (this is made precise using the notion of a tree-like path introduced in [14]).

We emphasise however that the question of how one may eﬀectively recover various prop-
erties of a path from its signature currently remains a challenging area of research. For
recent progress on this topic, see Lyons and Xu [17, 18]) and Geng [11].

2 Practical Applications

One of the practical applications of the signature transformation lies in the ﬁeld of machine
learning algorithms. As it has already been discussed, the signature summarizes important
information about a path. When the path is composed of a sequential data stream {Xi},
the terms of the signature S(X)ijk... are good candidates for characteristic features of the
data stream. The shuﬄe product property allows us to represent a non-linear function
of the signature as a linear combination of iterated integrals. That is similar to basis
function expansion and naturally applies to computations of regression models. In the next
sections we will demonstrate numerical computations of signatures of paths from various
data streams and applications of signatures to machine learning problems, regression and
classiﬁcation. Recently, the method of kernelization of the signature has been proposed
[15] and applied to hand movement classiﬁcation problem [24] from the UCI repository for
machine learning.

– 18 –

2.1 Elementary operations with signature transformation

In the following sections we introduce the essential ingredients of the signature method and
demonstrate how to perform basic computations with the signature.

2.1.1 Paths from discrete data

We start with basic computations of the signature applied to synthetic data streams. Con-
sider three one-dimensional sequences of length four:

i }4
{X 1
{X 2
i }4
{ti}4

i=1 = {1, 3, 5, 8}
i=1 = {1, 4, 2, 6}
i=1 = {0, 1, 2, 3},

(2.1)

(2.2)

(2.3)
where the variable {ti} corresponds to the ordering of the terms in (2.1)-(2.2) and is
normally interpreted as time. We are interested in transforming this discrete series into a
continuous function - a path. Among various ways to ﬁnd this transformation we focus on
two main approaches:

(a) piece-wise linear interpolation,

(b) rectilinear interpolation (i.e. axis path).

These two methods are present in Fig. 6 for the 2-dim path comprised of (2.2) and (2.3).
The explicit parametrisations in Cartesian coordinates for the two methods read as (2.4)

(a) Piece-wise linear interpolation of

{ti, X 2
i }.

(b) Rectilinear interpolation of {ti, X 2
i }.

Figure 6: Examples of two diﬀerent interpolations.

and (2.5)

{ti, X 2
{ti, X 2

i } = {(0, 1), (1, 4), (2, 2), (3, 6)}
i } = {(0, 1), (1, 1), (1, 4), (2, 4), (2, 2), (3, 2), (3, 6)},

(2.4)

(2.5)

– 19 –

with three auxiliary points {(1, 1), (2, 4), (3, 2)} added to construct the rectilinear path in
Fig. 6b denoted by empty red circles. For various numerical applications, the original data
might be mapped into diﬀerent forms to exhibit its structure. One of the examples of
such a mapping is the cumulative sum, or sequence of partial sums. More precisely the
cumulative sum is deﬁned as:

CS({Xi}n

i=1) = {X1, X1 + X2, . . . , Sk, . . . , Sn} ;

Sk =

Using again the previous example (2.2), (2.3), a new path is:
{ ˜X 2} = CS(X 2) = {1, 5, 7, 13}
{t} = {0, 1, 2, 3},

i=1

Xi.

(2.6)

(2.7)

k(cid:88)

and applying the two diﬀerent interpolations mentioned above, the paths are depicted in
Fig. 7, with added auxiliary points denoted by the empty red circles as in previous case.

(a) Piece-wise linear

(b) Rectilinear interpolation of

interpolation of CS(X 2).

CS(X 2).

(c) Lead-Lag transform of
one-dimensional data {X 2
i }.

Figure 7: Examples of two diﬀerent interpolations of CS(X 2) (a)-(b) and Lead-Lag trans-
form of {X 2

i } (c).

2.1.2 The lead-lag transformation

Another very important and interesting embedding is the Lead-Lag transformation of data,
which maps a one-dimensional path into a two-dimensional path. Considering the sequence
(2.2), the Lead-Lag mapping is given by:

LeadLag : X 2 = {1, 2, 4, 6} (cid:55)→

X 2,Lead = {1, 4, 4, 2, 2, 6, 6}
X 2,Lag = {1, 1, 4, 4, 2, 2, 6}

(2.8)

(cid:40)

and the resulting embedded path is presented in Fig. 7c with three additional points
{(1, 4), (4, 2), (2, 6)}.
Consider two processes {Xt} and {Yt} which follow each other’s paths with a small time
lag:

Xt+k ∝ Yt, k > 0,

(2.9)

– 20 –

giving the name Lead-Lag process. Certain properties of the lead-lag process are easily
captured by the signature of the path comprised of these processes {Xt, Yt} [8]. This is
demonstrated in the following example. Consider a path in R2 given by X = {X 1, X 2}. If
an increase (resp. decrease) of the component X 1 is followed by an increase (resp. decrease)
in the component X 2, then the area A given by (1.35) is positive. If the relative moves of
the components X 1 and X 2 are in the opposite direction, then the area A is negative. In
Fig.8 we can see how the area changes as we increase the endpoint.

Figure 8: Example of the change of the area enclosed by the curve and the chord at
diﬀerent endpoints.

i ,X 1

The notation here is: A[X 1
f ] area between two points, where (i, f ) are initial and ﬁnal
endpoints along the direction of X 1. Moving at ﬁrst from left to right (increasing the
parameter t), X 1 and X 2 both increase, resulting in the increasing area Aij: A[0,3] = 35,
A[0,5] = 179. At some point (X 1 = 6.72) the area attains its maximum value A[0,6.72] = 296
and after this point the area decreases as we increase the value of X 1. The total area
between the two endpoints is A[0,9] = −393.
The advantage of using the signature method is that any multivariate distribution of data
could be represented as a path in a high-dimensional space Rd. For example, using the data
(2.1)-(2.3) we can construct paths in R2 and R3 spaces, presented in Fig. 9a and Fig. 9b
respectively.

Another example of a multidimensional embedding involves the lead-lag transform. Here
we include the time vector t = {0, 1, 2, 3, 4} with X = {0, 1, 3, 5, 2} and compute the lead-
lag transform of X. The red dots represent the actual data points of the Cartesian ordered
tuple:

{(tlead, X lead, X lag)i} = {(0, 0, 0), (1, 1, 1), (2, 3, 3), (3, 5, 5), (4, 2, 2)}.

(2.10)

Applying the axis-path interpolation, we get the path presented in Fig. 10.

– 21 –

(a) 2-dim path from two 1-dim paths

{X 1

i }.
i , X 2

(b) 3-dim path from three 1-dim paths

{ti, X 1

i }.
i , X 2

Figure 9: Examples of embedding a collection of one-dimensional paths into a single
multi-dimensional path.

Example of a three dimensional

Figure 10:
the data
{(tlead, X lead, X lag)i}. The path is build by means of sequential increments along each
Cartesian direction. This path allows to study the variance of {X} as well as its time-
dependent properties.

lead-lag transform of

2.1.3 The signature of paths

For a given path, one can compute its signature according to the rules and deﬁnition in the
Sec. 1. We will present computations of signatures for various embeddings as presented in
Figs. 6-9 and discuss their properties.

We start with the embedded path presented in Fig. 9(a), for which the total increment and
the signed area are depicted in Fig. 11.

Computing the signature and the log signature of this path up to level L = 2 gives:

S(X) = (1, 7, 5, 24.5, 19, 16, 12.5) = (1, S(1), S(2), S(1,1), S(1,2), S(2,1), S(2,2))

(2.11)

– 22 –

Figure 11: Example of signed area enclosed by the piece-wise linear path (blue) and the
chord (red dashed line). The light blue area is negative and pink area is positive.

and

log S(X) = (7, 5, 1.5) = (S(1), S(2), S[1,2]),

where the last term in log S(X) is given by 1
area between the endpoints. The fact that it is positive means that the pink area is larger
than the light blue one. The geometric interpretation of the second order terms S(1,2) and
S(2,1) are presented in Fig. 12.

2 (S(1,2) − S(2,1)) and corresponds to the total

(a) Area given by the term S(1,2) = 19.

(b) Area given by the term S(2,1) = 16.

Figure 12: The geometric meaning of the terms S(1,2) and S(2,1). The left panel (a) repre-
sents the area enclosed by the path and two perpendicular dashed lines passing through the
endpoints of the path, while the right panel (b) shows another possibility for the area to be
enclosed by the path and two perpendicular dashed lines passing through the endpoints.

Switching the order of integration over the path in the terms S(1,2) and S(2,1) gives rise to

– 23 –

two areas which complete each other and add up to the total area of a rectangular with side
lengths X 1 and X 2. This simple geometrical meaning is nothing but the shuﬄe product
relation:

S(1) · S(2) = S(1,2) + S(2,1)

5 · 7 = 19 + 16.

(2.12)

The geometric interpretation of the higher order terms is less intuitive and we omit this
discussion.

2.1.4 Rules for the sign of the enclosed area

Before we continue, it is important to emphasize the signiﬁcance of the direction in which we
travel along a path and the sign of the area which it encloses. Consider the six possibilities
in Fig. 13

(a)

(b)

(c)

(d)

(e)

(f)

Figure 13: Various possibilities of signed area. The ﬁrst row (a)-(c) corresponds to counter
clock-wise movement along the path, and the bottom row (d)-(f) to clock-wise movement.

where we clearly see how direction of movement along the axis paths corresponds to the
sign of the enclosed area. This is intimately related to the sign of the winding number of
the path [3].

2.1.5 Statistical moments from the signature

Having established the basic rules we are ready to explore more important properties of
the signature transformation. Combining several data streams X = {Xi} into a single one

– 24 –

allows us to compute statistical moments and cross correlations between the individual
streams. In the next sections we present a close relation between the statistical moments
of a set of data X and the terms of signature S(X)I .

Relationship between the lead-lag transformation and the variance of data

Consider the lead-lag path Fig. 7c constructed from (2.1). We can decompose this ﬁgure
into three right-angled isosceles triangles Fig. 14. Note the direction of movement along
this path, starting from the point X 2
4 corresponds
to a negative sign, thus the total area will be negative. All three triangles have the same
direction of movement resulting in the sum of their individual contributions.

1 and moving towards the end point X 2

Figure 14: Decomposition of the full lead-lag path into individual parts. The direction of
movement along the path is shown by arrows.

(2.13)

The absolute value of the total area is then given by:

2 − X 2

(cid:1)(cid:0)X 2
(cid:1)(cid:0)X 2
(cid:1)(cid:0)X 2

1

(cid:2)(cid:0)X 2
+ (cid:0)X 2
+ (cid:0)X 2
(cid:2)(4 − 1)2 + (2 − 4)2 + (6 − 2)2(cid:3) .

(cid:1) +
(cid:1) +
2 − X 2
(cid:1)(cid:3)

3 − X 2
4 − X 2

3 − X 2
4 − X 2

3

2

2

3

1

|A| =

1
2

1
2

=

Let us write:

QV (X) =

N−1(cid:88)

i

(Xi+1 − Xi)2 ,

(2.14)

which has the simple meaning of the quadratic variation of the path constructed from
{Xi}N
i=1 and is related to the variance of the path. Thus one can generally write for any
sequence {Xi}N

i=1

ALead−Lag =

1
2

QV (X).

(2.15)

– 25 –

The ﬁrst order terms of the signature correspond to the total increments in each dimension,
which are the same and equal to:

∆X 2,Lead = ∆X 2,Lag = X 2

4 − X 2

Putting all the terms together and omitting all unessential notation, we obtain the trun-
cated log signature of {X 2} from (2.2) at level L = 2:

log S(cid:0)X 2(cid:1) =

(cid:18)

∆X 2, ∆X 2,

1
2

QV (X 2)

= (5, 5,−14.5).

1 = 6 − 1 = 5.
(cid:19)

(2.16)

(2.17)

The only reason we are using the log signature above to present our result is because of
its compactness and simplicity. One can easily rewrite the above result in terms of the
signature with all the terms included:

S(X 2) = (1, 5, 5, 12.5,−2, 27, 12.5).

(2.18)

We are free to work with either of these two representations of the signature, but certain
applications will dictate the rationale behind a particular choice.

The cumulative sum of a sequence

Next we explore certain properties of paths which originate from embedding points using
cumulative sums. Consider again the example (2.2). According to (2.6), the cumulative
sum of a sequence is:

1 , X 2

1 + X 2

2 , X 2

1 + X 2

2 + X 2

3 , X 2

1 + X 2

2 + X 2

4}
3 + X 2

˜X 2 = {X 2

= {1, 5, 7, 13}.

(2.19)

Pairing the above with the time component (2.3), we get a 2-dim path as shown in Fig. 7b.
Augmenting the original series {Xi} with the zero value and truncating the signature of
the new series { ˜X} at level L will determine the statistical moments up to level L of the
original sequence. Explicitly, for any general sequence {Xi}:

{X}i → {0,{X}N

i=1} → CS({X}i) = { ˜X}N

i=0 = {0, X1, X1 + X2, . . .} .

(2.20)
One should pay attention to the indexing of the sequence, i = 1 → i = 0. As we have
learnt from (2.13)-(2.17), we can apply the lead-lag transform to the augmented sequence,
compute the truncated signature at level L = 2, and we will obtain quantities which are
proportional to the ﬁrst two statistical moments of the data points, namely mean and
variance. For an arbitrary series {X}N

i=1 and its cumulative sum representation { ˜X}N
N(cid:88)
i=1:
N−1(cid:88)

(cid:16) ˜Xi+1 − ˜Xi

N(cid:88)

(cid:17)2

(Xi)2

Xi

i=1

=

(2.21)

(2.22)

∆ ˜X =

QV ( ˜X) =

i=0

i=1

– 26 –

It is easy to see that these expressions are related to the mean and variance of X if one
makes use of the expected value. Namely, for a collection of data points {Xi}N

i=1:

M ean(X) = E[X] =

1
N

Xi =

∆ ˜X
N

(uniformly distributed data)

V ar(X) = E[(X − E[X])2] = E[X 2] − (E[X])2 =

1
N

QV ( ˜X) − 1
N

(cid:18)

(cid:19)

(∆ ˜X)2

N(cid:88)

i=1

Recall that the ﬁrst two levels of the signature (log or full) correspond to the total in-
crement and the L´evy area, which shows how these signature terms determine the ﬁrst
two statistical moments. Similarly, it is straightforward to demonstrate that the higher
statistical moments can be obtained from the higher order signature terms.

Signature terms as a function of data points

To make a closer connection and develop further the intuition behind the ﬁrst terms of the
signature of a data stream, we shall present two diﬀerent examples of an embedding and
the related signature terms. First, consider the cumulative lead-lag embedding of N data
points {Xi}N
i=1 (according to (2.20)) into a continuous path [cf. Fig. 7c], then the resulting
truncated signature at level L = 2 is simply given by:

(cid:16)

1, S(1), S(2), S(1,1), S(1,2), S(2,1), S(2,2)(cid:17)

S( ˜X)|L=2 =

with

(2.23)

(2.24)

S(1) = S(2) =

S(1,1) = S(2,2) =

S(1,2) =

S(2,1) =

i

1
2

Xi

N(cid:88)
(cid:32) N(cid:88)
(cid:32) N(cid:88)
(cid:32) N(cid:88)

1
2

i

i

1
2

Xi

(cid:33)2
(cid:33)2
(cid:33)2

Xi

Xi

i

i

+

N(cid:88)
− N(cid:88)

i


 .

X 2
i

X 2
i

Note that, although we computed the signature of the transformed data { ˜X}, the ﬁnal
result is given in terms of the original untransformed data {Xi}.
Next, we consider the untransformed original data {Xi} with the same lead-lag embedding
which results in the following expression for the signature:

(cid:16)

1, S(1), S(2), S(1,1), S(1,2), S(2,1), S(2,2)(cid:17)

S(X)|L=2 =

(2.25)

– 27 –

(Xi+1 − Xi)

(Xi+1 − Xi)

with

S(1) = S(2) =

S(1,1) = S(2,2) =

S(1,2) =

S(2,1) =

i

1
2

N−1(cid:88)
(cid:32)N−1(cid:88)
(cid:32)N−1(cid:88)
(cid:32)N−1(cid:88)

1
2

1
2

i

i

(cid:33)2
(cid:33)2
(cid:33)2

+

N−1(cid:88)
− N−1(cid:88)

i

(Xi+1 − Xi)

(Xi+1 − Xi)

(Xi+1 − Xi)

(Xi+1 − Xi)

i

i

(2.26)


 .

Comparing (2.24) with (2.26), one can immediately see how the cumulative sum of the
terms aﬀects the result. This observation is crucial for further applications of the signature
approach, since each individual problem should be treated in the most suitable way. As an
illustrative example, one can derive the empirical sample mean and sample variance from
the signature terms. Considering data {Xi}N
i=1 with the cumulative sum and the lead-lag
embedding (as demonstrated in (2.24)), a bit of algebra brings us to:

1
N

S(1)
M ean(X) =
V ar(X) = − N + 1

N 2 S(1,2) +

N − 1
N 2 S(2,1),

(2.27)

where N is the total number of data points in the sample. The possible caveat here is the
degeneracy in the terms of the signature, causing this representation not to be unique and
introducing a problem of colinearity of the signature terms. One of the standard approaches
to resolve this problem is the shrinkage techniques, such as the LASSO [25], ridge or their
hybrid combination known as the elastic net regularization [27].

2.2 The signature in machine learning

The main idea of using the signature transformation for machine learning problems is mo-
tivated by its ability to extract characteristic features from data. As already discussed,
embedding data into a path and computing its signature provides us with important infor-
mation about the original data. The workﬂow is simple and is summarised in the following
algorithm:

data → path → signature of path → features of data

This algorithm is absolutely general and works for any type of sequential data which can
be embedded into a continuous path. The extracted features might be used for various
types of machine learning applications, including both supervised and unsupervised learn-
ing. For example, one can classify time-series or distinguish clusters of data. One of the
advantages of feature extraction with the signature method is that the signature is sen-
sitive to the geometric shape of a path. Sensitivity to the geometric shape of the input

– 28 –

data has lead to a successful application of the signature method to Chinese character
recognition problem [12]. One of the most known and natural applications of the signature
method is in quantitative ﬁnance, namely analysis of time-series data [7, 16]. Time-series
data represent ordered sequential data which is an ideal candidate for creating a path from
data, followed by computing the signature and applying machine learning algorithms for
further analysis. Any type of time-ordered sequential data naturally ﬁts into the signature
framework. Moreover, if the input data come from several parallel sources, this will result
in a multi-dimensional path. An example for such a type of data is panel data (in Econo-
metrics) or longitudinal data (in Medicine, Psychology, Biostatistics etc.) which involve
repeated observations of the same quantity over periods of time.

Concluding this brief introduction to applications of the path signature method to machine
learning problems, we emphasise that this novel method introduces a new concept of dealing
with data: thinking of data as geometric paths and using the path signature method in data
analysis. In the following sections we will elaborate on these statements and demonstrate
practical applications of the signature method.

2.2.1 Application of the signature method to data streams

In this section we demonstrate an explicit example of an application of the signature
method to classiﬁcation of time-series. This method is moreover easily applied to analysis of
sequential data of any type. In the next section we overview some of the recent applications
of the signature method to various problem in machine learning.

We aim at learning a classiﬁer which discriminates between two types of univariate time-
series, synthetically simulated using the ARM A(p, q) model. The diﬀerence between time-
series is encoded in diﬀerent value of the model parameters. Consider a collection of
observations of time-series {Yi,j} labeled by two indices i, j, where the ﬁrst index i denotes
each distinct time-series and the second index j accounts for the measurement at time tj.

The ARM A(p, q) model is AutoRegressive Moving Average model with parameters p (the
order of the autoregressive model) and q (the order of the moving-average model). This is
a superposition of the autoregressive AR(p) and the moving-average M A(q) models:

AR(p) : Yt = φ0 + φ1Yt−1 + φ2Yt2 + ··· + φpYt−p + t
MA(q) : Yt = µ + t + θ1t−1 + θ2t−2 + ··· + θqt−q,

(2.28)

where µ is the mean of the series, θi with φi are the parameters of the models and t−i is the
white noise error term. We are not diving into the mathematical foundation of the model,
but rather focus on its application. To demonstrate how the feature extraction method
works in this case, we create a binary classiﬁcation problem using the ARMA model. We
generate 1000 distinct time-series (500 time-series of each class) of length 100.
In this
particular, the case two classes correspond to the following models:
class “0” : Yt − 0.4Yt−1 = 0.5 + t + 0.5t−1
class “1” : Yt − 0.8Yt−1 = 0.5 + t + 0.7t−1

(2.29)

– 29 –

where t is normally distributed with zero mean and unit variance. An example of a time-
series from each class is depicted in Fig. 15. To give a simple intuitive picture, the resulting

Figure 15: Example of time-series Yi from two distinct classes generated from ARMA(1,1)
model speciﬁed by (2.29).

two-classes time-series data {Yi} and their class labels are organised in a matrix form:

t1
Y1,1
Y2,1
Y3,1
...

t2
Y1,2
Y2,2
Y3,2
...

t3
Y1,3
Y2,3
Y3,3
...

t4
Y1,4
Y2,4
Y3,4
...

Y500,1 Y500,2 Y500,3 Y500,4
Y1,4
Y1,1
Y2,4
Y2,1
Y3,4
Y3,1
...
...

Y1,2
Y2,2
Y3,2
...

Y1,3
Y2,3
Y3,3
...

Y500,1 Y500,2 Y500,3 Y500,4

We outline the essential steps of the procedure.

. . .
. . .
. . .
. . .
...
. . .
. . .
. . .
. . .
...
. . .

t100
Y1,100
Y2,100
Y3,100

...

Y500,100
Y1,100
Y2,100
Y3,100

...

Y500,100

Class

0
0
0
...
0
1
1
1
...
1

• create a continuous path Xi from each time-series {Yi} (row-wise)
• if needed, make use of the lead-lag transform to account for the variability in data
• compute the truncated signature S(Xi)|L of the path Xi up to level L
• use the terms of signature {SI

i } as features

Following the aforementioned steps, each observation {Yi} is converted into a two-dimensional
path. We also used a cumulative sum transformation (2.20):

(cid:110)(cid:16) ˜Y lead

i

(cid:17)(cid:111)

Xi =

, ˜Y lag

i

,

(2.30)

– 30 –

t020406080100Yi-1.5-1-0.500.511.501and then the signature terms are computed:

(cid:16)

S(Xi) =

1, S(1)

i

, S(2)

i

, S(1,1)

i

, S(1,2)

i

, . . . , SI

i , . . .

(cid:17)

.

(2.31)

The resulting feature matrix has the form:

ˆS(1)
1
ˆS(1)
2
ˆS(1)
3
...
ˆS(1)
500
ˆS(1)
1
ˆS(1)
2
ˆS(1)
3
...
ˆS(1)
500

ˆS(2)
1
ˆS(2)
2
ˆS(2)
3
...
ˆS(2)
500
ˆS(2)
1
ˆS(2)
2
ˆS(2)
3
...
ˆS(2)
500

Feature Set
ˆS(1,1)
ˆS(1,2)
1
1
ˆS(1,2)
ˆS(1,1)
2
2
ˆS(1,1)
ˆS(1,2)
3
3

...

...

ˆS(1,1)
500
ˆS(1,1)
1
ˆS(1,1)
2
ˆS(1,1)
3

...

ˆS(1,2)
500
ˆS(1,2)
1
ˆS(1,2)
2
ˆS(1,2)
3

...

ˆS(1,1)
500

ˆS(1,2)
500

. . .
. . .
. . .
...
. . .
. . .
. . .
. . .
...
. . .

ˆSI
1
ˆSI
2
ˆSI
3
...
ˆSI
500
ˆSI
1
ˆSI
2
ˆSI
3
...
ˆSI
500

Class

0
0
0
...
0
1
1
1
...
1

It is usual practice to standardise the signatures column-wise, where the hat corresponds
to standardised values, computed by subtracting from each value SI
i the column mean and
dividing by the column standard deviation.

For sake of simplicity, we compute the truncated signature up to level L = 2, remove the
ﬁrst constant term and thus produce a 6-dimensional feature space. The example of the
signature and the log-signature terms projected on the two dimensional plane are presented
in Fig. 16.

(a)

(b)

Figure 16: Feature extraction from ARMA(1,1) time-series. The left panel (a) represents
two signature terms S(1,2) and S(2,1), while the right panel (b) shows the ﬁrst and the third
terms of log-signature log S1,log S[1,2].

– 31 –

S(1,2)-1-0.500.511.52S(2,1)-1-0.500.511.522.501logS(1)-3-2-101234logS[1,2]-2-10123401Since we are not aiming to compare diﬀerent classiﬁcation algorithms, but rather to demon-
strate the signature method at work, we arbitrarily choose a standard logistic regression
algorithm with LASSO [25] penalization scheme. The relevant features which are selected
by LASSO are:

(cid:110)
S(1), S(2), S(1,2), S(2,1)(cid:111)

.

The classiﬁcation results are summarised in Table 1.

Predicted

True

0
1

0

1

326
35

20
319

Predicted

True

0
1

0

1

139
14

15
132

(a) Training data set (70%)

with accuracy 0.92

(b) Testing data set (30%)

with accuracy 0.90

Table 1: Confusion matrix of the classiﬁcation showing both the training and the testing
data sets results.

Using the full signature for feature extraction is conceptually similar to working with basis
functions. Having obtained the signature terms as representing features of data, one can
continue further with standard methods in machine learning to solve problems.

2.2.2 Dealing with missing data in time-series

Sometimes sequential observations are not complete and some values might be missing due
to various reasons. For example in medical follow up studies patients may not provide
response data at required times due to personal reasons or clinical conditions. There are
many ways to deal with missing data, for example imputing, various sophisticated inter-
polation methods, Gaussian smoothing kernels, and many more techniques are thoroughly
studied and described in the literature. These methods might perform well and even result
in robust analysis and predictions, but conceptually, imputing missing values may change
the underlying information about initial data and thus create biased analysis which masks
the original message.

One of the methods to treat missing values is to introduce a new binary variable: indicator
matrix Rij along with observations Yij, where i corresponds to individual time-series and
j denotes a time instance as explained in [6]. The elements of the indicator matrix are
deﬁned by:

(cid:40)

Rij =

0 if Yij observed,
1 if Yij
is missing.

(2.32)

The data might be of three major types:

– 32 –

• MCAR - missing completely at random: missingness does not depend on data at all

P (M|Y, θ) = P (M|θ)

• MAR - missing at random: missingness does not depend on the unobserved data,

but does depend on the observed values P (M|Y, θ) = P (M|Yobs, θ)

• NMAR - not missing at random: missingness depends on the unobserved data

P (M|Y, θ) = P (M|Ymiss, θ)

A common approach to treat missing vales is to base inference on the likelihood function for
the incomplete data by treating the indicator matrix Rij as a random variable. Specifying
the joint distribution of Rij and Yij together with the Expectation-Maximisation algorithm
is usually the method of choice.

The signature framework naturally allows us to embed the indicator vector (a single row
of the indicator matrix Rij) into the path by lifting the path in to higher dimension. For
example, consider a time-series with several missing values and its corresponding indicator
vector (here the index i is ﬁxed):

Yj = {1, 3, (cid:63), 5, 3, (cid:63), (cid:63), 9, 3, 5}
Rj = {0, 0, 1, 0, 0, 1, 1, 0, 0, 0},

(2.33)

where the symbol “(cid:63)” denotes a missing value at the expected time point. The idea is
to create a two dimensional path from this data. The evolution of the path is from the
starting point towards the end and can be seen as propagation in three dimensional space;
the ﬁrst two dimensions are “observed data” and “missing data”, while the third direction
corresponds to time. At every time point where we have a missing point, we jump from the
“observed” to the “missing” dimension and ﬁll in the missing place with the same value as
seen before (a.k.a feed forward method). The intuition is simple: unless we have any new
information about the data, we continue walking along a “missing” axis direction, which
is in another dimension than the “observed” data. The resulting path is given by (2.34)
˜Yj = {(0, 1, 0), (1, 3, 0), (2, 3, 1), (3, 5, 1), (4, 3, 1), (5, 3, 2), (6, 3, 3), (7, 9, 3), (8, 3, 3), (9, 5, 3)}.
(2.34)

Here we introduce an auxiliary time parametrisation vector t = {0, 1, 2, . . . , 9} to account
for the time propagation and create a path in three-dimensional space as depicted in Fig. 17.
Red points represent the missing data. The path propagates from t = 0 to t = 9, and the
observed points lie in the plane {t, Y }, while in the presence of a missing value, the path
jumps along the increasing direction of R axis.

This approach allows us to treat data streams with unobserved data at the same footing as
complete data streams, which demonstrates another conceptual advantage of the signature
framework.

– 33 –

Figure 17: Example of embedding of data with missing values into a single path. The red
dots represent unobserved data.

2.3 Computational considerations of the signature

So far, we have presented a theoretical background and practical applications of this
method, but nothing has been mentioned about exact numerical computations of the sig-
nature and iterated integrals of paths. As it has been deﬁned in (1.16), the terms of the
signature are iterated integrals of a path, while the path is normally constructed by an
interpolation of data points. One can compute such iterated integrals using several compu-
tational algorithms(cubature methods) which are generally straightforward to implement.
The research group at the Oxford-Man Institute (University of Oxford, UK) works with
a particular implementation of such algorithms written in C++ (CoRoPa project) 1 with
Python wrapper package sigtools. Research groups from other universities have successfully
implemented computations of iterated integrals with MatlabTM.

2.4 Overview of recent progress of the signature method in machine learning

We would like to devote this ﬁnal section to an overview of the applications of the signature
method to various problems in machine learning and data analysis which have appeared
in the literature. The application areas are quite wide, including ﬁnancial data, sound
compression, time-series analysis, medical data, and image recognition tasks.

2.4.1 Extracting information from the signature of a ﬁnancial data stream

Field et al. [7] have applied the signature transformation to ﬁnancial data streams to ﬁnd
hidden patterns in trading strategies. The authors presented several examples of learning
from data and further classiﬁcation of unseen streams. The advantage of using the signature
method is its ability to represent data using a small set of features which captures the
most important properties of data in a non-parametric way without traditional statistical
In their ﬁrst experiment, the authors looked for atypical market behaviour
modelling.

1http://coropa.sourceforge.net/

– 34 –

across standard 30-minute time buckets obtained from the WTI crude oil future market.
In the second experiment, they aimed to distinguish between orders generated by two
diﬀerent trade execution algorithms. We will overview their methodology and results.

As we have already seen, the basics steps within the signature approach is to convert data
into paths and then compute the iterated integrals of the resulting paths. In [7], the authors
considered the following data streams:

• P a : best ask price
• P b : best bid price
• V a : number of orders at best ask price
• V b : number of orders at best bid price
• C : cumulative traded volume

The ﬁnal path was constructed from an embedding of normalisations of these streams.
The normalisation was introduced in order to standardise the data and remove any spu-
rious patterns. The ask and bid prices were transformed into the mid price pt and the
spread st. Also, they included normalised time as ut and imbalance dt. The details of the
normalisation and transformation are described in [7]. The path is then:

In order to capture the quadratic variation of the price, the path is extended by means of
a lead-lag transform:

Z =

ulead
ti

, plead

ti

, slead

ti

, dlead

ti

, clead

ti

, plag
ti

Notice, if one is interested in the quadratic variation of any other variables, then the “lag”
transformations of these variables should be included in the resulting path.

The aim is to learn a discriminant function which can classify new streams based on features
extracted from signature of paths embedded from the data. The classiﬁcation method was
chosen as linear regression combined with LASSO shrinkage in order to select relevant
terms in the signature. To measure the signiﬁcance of separation, the authors used the
non-parametric Kolmogorov-Smirnov test, and the receiver operating characteristic (ROC)
curve and the area under the curve.

The objectives of the ﬁrst experiment in [7] are WTI crude oil futures, sampled by min-
utes from standard 30-minutes interval between 09:30-10:00, 10:30-11.00, and 14:00-14:30.
Considering the 6-dimensional input path (2.36) and its truncated signature at level L = 4,
there are 1555 signature terms in total, but successful application of the LASSO algorithm
allowed to select four terms which are the most informative. The results are illustrated
in Fig 18. The upper panels show a visible separation between two groups plotted on
the feature planes S(1,5,1,5,), S(5,1,5,1) (18a) and S(5,1,5,1), S(1,5,5,1) (18b) respectively. The

– 35 –

X =

(uti, pti, sti, dti, cti)N
i=0

(cid:110)

(cid:26)(cid:16)

(cid:111)

(cid:17)N

(cid:27)

i=0

(2.35)

(2.36)

bottom panels demonstrate estimated densities of regressed values with KS distance 0.9
(training) and 0.91 (out-of-sample) and 95% accuracy 18c. The ROC curve of the classiﬁer
with AUC = 0.986 (out-of-sample AUC 0.984) is presented in 18d.

(a)

(b)

(c)

(d)

Figure 18: Visible separation between two groups projected onto S(i,j,k,l) feature plane
(upper panels) and accuracy of separation (bottom panels). Courtesy of Field et al. [7].

The results from the second experiment of classiﬁcation of trading algorithms are presented
in Fig. 19. The aim is to classify the traces of trade execution algorithm. The data
streams are sampled from the FTSE 100 index future market (NYSE Liﬀe) with the same
transformation as described earlier. The beginning and the end of the streams are deﬁned
by the start and the end of parent orders generated by two diﬀerent trade algorithms which
are denoted by A and B. Left panel 19a shows the estimated densities of regressed values
with KS distance of 0.66 (0.518) with accuracy 82% (74.3%) for training (out-of-sample)
data sets. The right panel shows the ROC curve of the classiﬁer with the AUC 0.892
(0.777) for training (out-of-sample) sets respectively.

– 36 –

0102030405060(5,1,5,1)010203040506070(1,5,1,5)14:00-14:3010:30-11:000102030405060(5,1,5,1)0102030405060(1,5,5,1)14:00-14:3010:30-11:000.50.00.51.01.50.00.51.01.52.02.53.010:30-11:0014:00-14:300.50.00.51.01.50.00.51.01.52.02.53.010:30-11:0014:00-14:300.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive Ratelearning setout of sample(a)

(b)

Figure 19: Result of classiﬁcation of two trading algorithms. Courtesy of Field et al. [7].

The conclusions and numerical results of this work demonstrate a great potential for ap-
plications of the signature method to various problems in ﬁnancial data.

2.4.2 Sound compression - the rough paths approach

This research project dates back to 2005, when T. Lyons and N. Sidorova worked on the
application of rough paths theory to the problem of sound compression [21]. They pre-
sented a new approach which turned out to be more eﬀective than the traditional Fourier
and wavelet transforms. The main diﬀerence between these methods is in the linearity
of the Fourier approach, while the signature method accounts for non-linear dependen-
cies. Consider a problem of digital compression of a continuous multi-dimensional signal
X. Assuming that the signal is measured on a ﬁne time scale, the signal can be seen as
a continuous piece-wise linear path. Computing the signature of the resulting path, will
produce signature terms SI (X) which represent the coeﬃcients of compression of the orig-
inal signal. The authors of [21] presented an eﬃcient algorithm for reconstruction of the
original signal X from its signature terms.

2.4.3 Character recognition

Paths as geometric objects are legitimate candidates for applications to image recogni-
tion tasks. For example, hand written digits can be seen as continuous paths, and then
the signature method becomes a natural approach to classiﬁcation problems. The signa-
ture method has been successfully applied to the challenging problem of Chinese character
recognition by B. Graham [12], and later by J. Lianwen [26] to a more general problem of
hand written character recognition. Graham and Lianwen have shown (separately) that
using signature terms as features of the image, for use in a convolutional neural network
(CNN), signiﬁcantly improved accuracy of online character recognition. This approach,

– 37 –

0.50.00.51.01.50.00.51.01.52.02.53.03.5algo Aalgo B0.50.00.51.01.50.00.51.01.52.02.53.03.5algo Aalgo B0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive Ratelearning setout of samplewhich combines the signature and CNN methods, has won the ICDAR2013 Online Iso-
lated Chinese Character recognition competition receiving 0.95530 accuracy1. The group
of J. Lianwen has developed the application2 with graphical user interface for character
recognition. This application is based on the signature method for feature extraction from
elements of images.

These successful results indicate that applications of the signature method combined with
deep neural networks represent a new and eﬃcient way to tackle challenges in the ﬁeld of
image recognition.

2.4.4 Learning from the past, predicting the statistics for the future, learning

an evolving system

In [16], Levin, Lyons, and Ni address an application of the signature method to time-series
analysis and develop a new regression method based on the expected signature of paths.
This method of linear regression on the signature of data streams is general and allows
explicit computations and predictions. The main goal of this approach is to identify the
speciﬁc feature set of the observed data and linearise the functional relationships between
them (features). The authors introduced the expected signature model as a linear mapping:
L : X (cid:55)→ Y

Y = L(X) + ,
with E[] = 0 and X ∈ T ((Rd)), Y ∈ T ((Re)), where

T ((Rd)) =

(Rd)⊗n

∞(cid:77)

n=0

(2.37)

(2.38)

is the algebra of tensor series over Rd. We brieﬂy outline the main idea of this model.
Consider the univariate time-series (returns) denoted by {ri}N
i=1 and some ﬁxed k ∈ N,
such that 1 < k < N . Deﬁne the p-past returns up to time tk by Fk = {ri}k
i=k−p and the
q-future returns (from time tk+1) by Gk+1 = {ri}k+q
i=k+1. The signatures of Fk and Gk+1 are
respectively:

(cid:16){ti, ri}k
(cid:16){ti, ri}k+q

i=k−p

i=k+1

(cid:17)
(cid:17)

Xk = S

Yk = S

(2.39)

.

The deﬁnition of the Expected Signature model ES(p, q, n, m) states that the stationary
time-series {ri}N
i=1 satisﬁes the assumptions of the ES model with parameters p, q, n, m if

there exists a linear functional f : T n(cid:0)R2(cid:1) (cid:55)→ T n(cid:0)R2(cid:1) such that:

ρm (S({rt+i}q

i=1)) = f (ρm (S({rt−i}p

i=0))) + at,

(2.40)

1http://tinyurl.com/nfvbdlk
2http://www.deephcr.net/en.html

– 38 –

where N ≥ p + q is a positive integer, m is the depth of truncation of signature, and the
residual term at satisﬁes E[at|Ft] = 0. Let µk be the expectation of S({ti, ri}k+q

i=k+1)

µk = E[S({ti, ri}k+q

i=k+1)|Fk].

It thus follows from the ES model that

µk = f (Xk),

(2.41)

(2.42)

where Xk is deﬁned in (2.39) with both µk and ak in T ((R2)). Next, the conditional
covariance of the signature of the future return conditioned on Fk is deﬁned as

k(I, J) = Cov(cid:0)SI (Yk), SJ (Yk)|Fk

(cid:1) ,

Σ2

(2.43)

where I, J are two multi-indexes as deﬁned previously in the text. In time-series analysis,
(2.41) corresponds to the mean equation for Yk, and (2.43) is the volatility equation for Yk.
The fundamental assumption of the ES model is the stationarity of the time-series {ri},
which is common in time-series analysis.

It has been shown that many standard autoregressive models (AR, ARCH, GARCH, etc.)
are special cases of the ES model. For example, given the time-series {ti, ri} the mean
equation and volatility are written as:

AR

mk : E[rk+1|Fk]
k : V ar[rk+1|Fk]
σ2

ES
S(2)(µk)

2S(2,2)(µk) −(cid:0)S(2)(µk)(cid:1)2

.

Several important properties linking the autoregressive models and ES model are proven
in [16]. One of them states that if a time-series {rk} satisﬁes the assumptions of ARCH(q)
model and its mean equation is given by

Q(cid:88)

µk = β0 +

βirk−i,

(2.44)

then there exists a suﬃciently large integer n such that the time-series {rk} satisﬁes the
assumption of ES(q + Q, 1, n, 2). Also, the model ES(p, 1, n, 2) is considered as a classical
time-series, where the signature of the past returns as an explanatory variable.

i=1

The work in [16] provides explicit numerical benchmarks between various autoregressive
models and the expected signature model for various set-ups and conﬁgurations of the
models.

2.4.5 Identifying patterns in MEG scans

The project of Gyurko et al. [13] is devoted to an application of the signature method
to pattern recognition in medical studies and neuroimaging (MagnetoEncephaloGraphy).
The patients are asked to press a button whenever they like, and their electromagnetic

– 39 –

brain activity is constantly monitored during this trial. The data streams of the electric
signal are shown in Fig. 20 in the ﬁrst two upper panels. For the sake of simplicity, Gyurko
et al. considered only two channels from the MEG scanner as sources for the streaming
data. The main goal was to identify a pattern in these two data streams which is able to
predict the pressing of a button. The button-pressing instance was presented by a vector
with binary outcome: “1” and “0” stand for the button being pressed and not being pressed
respectively (Fig. 20, bottom panel). They used a rolling window analysis of time-series
and regressed the button-pressing instances on signature of the data streams inside the
rolling window.

Figure 20: Finding patterns in two-dimensional data stream (two upper panels). The
bottom panel shows the binary vector of the button-state (pressed or not pressed). The
horizontal axis is in time units.

The detection and prediction results were good: AUC of ROC: 0.92 (learning set) and AUC
of ROC (out-of-sample) 0.93, which indicates good separability between the two states (1
and 0). This example shows that applications of the signature approach are fairly general
and are not constrained to particular cases.

– 40 –

2.4.6 Learning the eﬀect of treatment on behavioural patterns of patients with

bipolar disorder.

This is another example of an application of the signature method to analysis of medical
data. Our main goal of this project is to learn the behavioural patterns of patients suﬀering
from bipolar disorder. The data are follow-up self-reported assessment scale collected by
the True Colours1 web platform which is developed by the Department of Psychiatry
at the University of Oxford and is used for the CONBRIO programme2. Patients are
encouraged to submit via this platform their answers to the self-assessment questionnaires.
These questionnaires are standard clinical tools for assessment of the severity of depression
(QIDS-SR16) [23] and mania (AMSR) [1] symptoms. These questionnaires contain 16 and
5 questions and scales are ranged [0 27] and [0 25] for QIDS-SR16 and AMSR respectively.
In Fig. 21, an example of the QIDS score for 50 weeks follow-up observations is displayed.

Figure 21: A typical example of the follow-up observations of the QIDS score.

As a part of our research programme, we analysed data from the CEQUEL clinical trial
[10]. The primary objective is to compare the combination therapy of quetiapine plus
lamotrigine with quetiapine monotherapy for treatment of bipolar depression. Among
other interesting research questions, we focused on the behavioural pattern of patients in
two diﬀerent treatment groups: placebo and lamotrigine. Patients are asked to submit their
self-assessment scores to the True Colours platform on a weekly basis. The system sends a
scheduled prompt text every week and patients should reply within one week interval. If
no response is received within a week, the system ﬂags this week as a missing observation
and sends a new prompt at the beginning of the next week. We analysed the delays - the
time intervals between the prompt text and the patient’s reply measured in integer units
of days. The delays are presented schematically in the Fig. 22.

1https://oxfordhealth.truecolours.nhs.uk/www/en/
2http://conbrio.psych.ox.ac.uk/home

– 41 –

Figure 22: Deﬁnition of the delays di. The time line is divided by the dashed lines into
weekly intervals. The delay is deﬁned as the time diﬀerence between the beginning of the
week (dashed line) and the time when the response is received (red dot). This particular
conﬁguration of delays corresponds to: {di} = {4, 2, 5, 0, 5, 0}.

The delays of the QIDS data from the Fig. 21 are shown in the Fig. 23.

Figure 23: Example of the delays di of the QIDS scores presented in the Fig. 21. Sub-
missions are denoted by the red dots.

We were interested in the question, whether the distribution of delays {di} is diﬀerent be-
tween the two diﬀerent treatment groups, namely placebo and lamotrigine. To answer this
question, we applied the signature method to time-series constructed from the delay data
{di} and used regularised logistic regression to discriminate between the two groups. The
sample sizes of the two groups are: 18 and 11 patients in the placebo and the lamotrigine
group respectively. We constructed continuous paths from the sequence of integer delays
{di} by means of two types of embeddings: the ﬁrst is the three dimensional simple lead-lag
transformation of the raw data {di} with time stamps:

X =

tlead
i

, dlead

i

, dlag

i

,

(2.45)

and the second embedding corresponds to the lead-lag transformation of the cumulative
sum (partial sums) of the delays:

(cid:110)(cid:16)

(cid:110)(cid:16)

(cid:17)

(cid:111)

i

(cid:17)

(cid:111)

i

S =

slead
i

, slag

i

.

(2.46)

We computed signature terms from these two embeddings separately up to level L and
combined these terms in a single design matrix, which we used as the feature matrix for

– 42 –

the logistic regression. The design matrix exhibited some degree of multicollinearity, thus
we applied the elastic net regularisation scheme [27] to the logistic regression. The elastic
net allows a smooth regularisation, which beneﬁts from both L1 and L2 norm shrinkage
schemes, groups the correlated covariates, and removes the less relevant features. The
choice of the α parameter of the elastic net is performed by the cross validated grid search,
and estimated as α = 0.55 giving the smallest deviance error as shown in Fig. 24.

Figure 24: Statistical results of cross validated elastic net regularisation scheme.

The ﬁnal classiﬁcation model is chosen by the “one-standard-error” rule, denoted by the
position of the blue dashed line, given the fact that the initial 10-dimensional feature set
was shrunk to a 6-dimensional subset. Some of the features of this subset are displayed in
the Fig. 25.

Figure 25: Examples of relevant features selected by the penalised logistic regression.
These two groups denoted by “0” and “1” correspond to the placebo and the lamotrigine
groups respectively.

The results of this project demonstrated the ability of the signature method to extract
characteristic features from the data in a systematic way without introducing any ad hoc
methods. The signature feature extraction method serves as an initial step in the machine

– 43 –

learning pipeline. Once we transformed the raw data into a set of signature terms, we
naturally proceed further with standard machine learning techniques and methodologies
using these features as inputs.

Acknowledgement

A.K. wishes to thank the Oxford-Man Institute of Quantitative Finance for the hospitality
during the course of this work and gratefully acknowledges the support of the Welcome
Trust grant No: 102616/Z/13/Z, “CONBRIO”.

References

[1] Edward G Altman, Donald Hedeker, James L Peterson, and John M Davis. The altman

self-rating mania scale. Biological psychiatry, 42(10):948–955, 1997.

[2] Horatio Boedihardjo, Xi Geng, Terry Lyons, and Danyu Yang. The signature of a rough

path: Uniqueness. arXiv:1406.7871, August 2014. Preprint.

[3] Horatio Boedihardjo, Hao Ni, and Zhongmin Qian. Uniqueness of signature for simple

curves. J. Funct. Anal., 267(6):1778–1806, 2014.

[4] Kuo-Tsai Chen. Integration of paths, geometric invariants and a generalized Baker-Hausdorﬀ

formula. Ann. of Math. (2), 65:163–178, 1957.

[5] Kuo-Tsai Chen. Integration of paths—a faithful representation of paths by non-commutative

formal power series. Trans. Amer. Math. Soc., 89:395–407, 1958.

[6] Peter Diggle, Patrick Heagerty, Kung-Yee Liang, and Scott Zeger. Analysis of longitudinal

data. Oxford University Press, 2002.

[7] Jonathann Field, Lajos Gergely Gyurk´o, Mark Kontkowski, and Terry Lyons. Extracting

information from the signature of a ﬁnancial data stream. arXiv:1307.7244, July 2014.
Preprint.

[8] Guy Flint, Ben Hambly, and Terry Lyons. Discretely sampled signals and the rough hoﬀ

process. arXiv preprint arXiv:1310.4054, 2015.

[9] Peter K. Friz and Martin Hairer. A course on rough paths. Universitext. Springer, Cham,

2014. With an introduction to regularity structures.

[10] John R Geddes, Alexandra Gardiner, Jennifer Rendell, Merryn Voysey, Elizabeth Tunbridge,

Christopher Hinds, Ly-Mee Yu, Jane Hainsworth, Mary-Jane Attenburrow, Judit Simon,
et al. Comparative evaluation of quetiapine plus lamotrigine combination versus quetiapine
monotherapy (and folic acid versus placebo) in bipolar depression (cequel): a 2× 2 factorial
randomised trial. The Lancet Psychiatry, 2015.

[11] Xi Geng. Reconstruction for the signature of a rough path. arXiv:1508.06890, August 2015.

Preprint.

[12] Benjamin Graham. Sparse arrays of signatures for online character recognition.

arXiv:1308.0371, December 2013. Preprint.

– 44 –

[13] Lajos Gergely Gyurko, Terry Lyons, and Harald Oberhauser. Identifying patterns via the

signature for the comnbrio project. University of Oxford, 2014.

[14] Ben Hambly and Terry Lyons. Uniqueness for the signature of a path of bounded variation

and the reduced path group. Ann. of Math. (2), 171(1):109–167, 2010.

[15] Franz J Kir´aly and Harald Oberhauser. Kernels for sequentially ordered data. arXiv preprint

arXiv:1601.08169, 2016.

[16] Daniel Levin, Terry Lyons, and Hao Ni. Learning from the past, predicting the statistics for

the future, learning an evolving system. arXiv:1309.0260, September 2015. Preprint.

[17] Terry Lyons and Weijun Xu. Hyperbolic development and inversion of signature.

arXiv:1507.00286, July 2015. Preprint.

[18] Terry Lyons and Weijun Xu. Inverting the signature of a path. arXiv:1406.7833, July 2015.

Preprint.

[19] Terry J. Lyons. Diﬀerential equations driven by rough signals. Rev. Mat. Iberoamericana,

14(2):215–310, 1998.

[20] Terry J. Lyons, Michael Caruana, and Thierry L´evy. Diﬀerential equations driven by rough

paths, volume 1908 of Lecture Notes in Mathematics. Springer, Berlin, 2007.

[21] Terry J Lyons and Nadia Sidorova. Sound compression: a rough path approach. In
Proceedings of the 4th international symposium on Information and communication
technologies, pages 223–228. Trinity College Dublin, 2005.

[22] Rimhak Ree. Lie elements and an algebra associated with shuﬄes. Ann. of Math. (2),

68:210–220, 1958.

[23] A John Rush, Madhukar H Trivedi, Hicham M Ibrahim, Thomas J Carmody, Bruce Arnow,
Daniel N Klein, John C Markowitz, Philip T Ninan, Susan Kornstein, Rachel Manber, et al.
The 16-item quick inventory of depressive symptomatology (qids), clinician rating (qids-c),
and self-report (qids-sr): a psychometric evaluation in patients with chronic major
depression. Biological psychiatry, 54(5):573–583, 2003.

[24] Christos Sapsanis, George Georgoulas, and Anthony Tzes. Emg based classiﬁcation of basic
hand movements based on time-frequency features. In Control & Automation (MED), 2013
21st Mediterranean Conference on, pages 716–722. IEEE, 2013.

[25] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal

Statistical Society. Series B (Methodological), pages 267–288, 1996.

[26] Weixin Yang, Lianwen Jin, and Manfei Liu. Deepwriterid: An end-to-end online

text-independent writer identiﬁcation system. arXiv preprint arXiv:1508.04945, 2015.

[27] Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal

of the Royal Statistical Society: Series B (Statistical Methodology), 67(2):301–320, 2005.

– 45 –

