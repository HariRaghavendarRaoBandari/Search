6
1
0
2

 
r
p
A
3

 

 
 
]
T
I
.
s
c
[
 
 

2
v
3
1
3
6
0

.

3
0
6
1
:
v
i
X
r
a

Convex block-sparse linear regression with expanders – provably

Anastasios Kyrillidis

UT Austin

Bubacarr Bah

UT Austin

Rouzbeh Hasheminezhad

Sharif University of Technology

Quoc Tran-Dinh

Luca Baldassarre

Volkan Cevher

University of North Carolina

EPFL

EPFL

April 5, 2016

Abstract

Sparse matrices are favorable objects in machine learning and optimization. When such matrices are used,
in place of dense ones, the overall complexity requirements in optimization can be signiﬁcantly reduced in
practice, both in terms of space and run-time. Prompted by this observation, we study a convex optimization
scheme for block-sparse recovery from linear measurements. To obtain linear sketches, we use expander
matrices, i.e., sparse matrices containing only few non-zeros per column. Hitherto, to the best of our
knowledge, such algorithmic solutions have been only studied from a non-convex perspective. Our aim here
is to theoretically characterize the performance of convex approaches under such setting.

Our key novelty is the expression of the recovery error in terms of the model-based norm, while assuring
that solution lives in the model. To achieve this, we show that sparse model-based matrices satisfy a group
version of the null-space property. Our experimental ﬁndings on synthetic and real applications support our
claims for faster recovery in the convex setting – as opposed to using dense sensing matrices, while showing
a competitive recovery performance.

Introduction

1
Consider the sparse recovery problem in the linear setting: given a measurement matrix X ∈ Rn×p (n < p), we
seek a sparse vector β(cid:63) ∈ Rp that satisﬁes a set of measurements y = Xβ(cid:63) or ﬁnds a solution in the feasible set
{β : (cid:107)y − Xβ(cid:107)2 ≤ (cid:107)ξ(cid:107)2}, when noise ξ ∈ Rn is present. This problem is closely related to compressive sensing
algorithm ∆ to obtain estimate (cid:98)β = ∆(Xβ(cid:63) + ξ) that yields a guarantee:

Typically, the analysis included in such task focuses on investigating conditions on X and the recovery

[11, 9] and the subset selection problem [27], as well as to graph sketching [1] and data streaming [25].

Here, β(cid:63)
This type of guarantee is known as the (cid:96)q1 /(cid:96)q2 error guarantee.

(1)
s denotes the best s-sparse approximation to β(cid:63) for constants C1, C2 > 0 and 1 ≤ q2 ≤ q1 ≤ 2 [7, 5].
Without further assumptions, there is a range of recovery algorithms that achieve the above, both from a
non-convex [24, 21, 23] and a convex perspective [10, 5]. In this work, we focus on the latter case. Within
this context, we highlight (cid:96)1-minimization method, also known as Basis pursuit (BP) [10], which is one of the
prevalent schemes for compressive sensing:

s(cid:107)q2 + C2(cid:107)ξ(cid:107)q1.

(cid:107)β(cid:63) −(cid:98)β(cid:107)q1 ≤ C1(cid:107)β(cid:63) − β(cid:63)

β (cid:107)β(cid:107)1
min

subject to: y = Xβ.

(2)

Next, we discuss common conﬁgurations for such problem settings, as in (2), and how a priori knowledge can
be incorporated in optimization. We conclude this section with our contributions.
Measurement matrices in sparse recovery: Conventional wisdom states that the best compression per-
formance, i.e., using the least number of measurements to guarantee (1), is achieved by random, independent
and identically distributed dense sub-gaussian matrices [8]. Such matrices are known to satisfy the (cid:96)q-norm

restricted isometry property (RIP-q) with high-probability. That is, for some δs ∈ (0, 1) and ∀ s-sparse β ∈ Rp

[6, 5]:

(3)
In this case, we can tractably and provably approximate sparse signals from n = O (s log (p/s)) sketches for
generic s-sparse signals. Unfortunately, dense matrices require high time- and storage-complexity when applied
in algorithmic solutions.1

q ≤ (1 + δs)(cid:107)β(cid:107)q
q.

(1 − δs)(cid:107)β(cid:107)q

q ≤ (cid:107)Xβ(cid:107)q

1There exist structured dense matrices that achieve better time and storage complexity, as compared to random Gaussian
matrices; see [26] for discussion on subsampled Fourier matrices for sparse recovery. However, sparse design matrices have been
proved to be more advantageous over such structured dense matrices [5].

1

On the other hand, in several areas, such as data stream computing and combinatorial group testing [25],
it is vital to use a sparse sensing matrix X, containing only very few non-zero elements per column [18, 5]. In
this work, we consider random sparse matrices X that are adjacency matrices of expander graphs; a precise
deﬁnition is given below.

Deﬁnition 1.1. Let a graph (p, n,E) be a left-regular bipartite graph with p left (variable) nodes, n right (check)
nodes, a set of edges E and left degree d. If, any S ⊆ {1, . . . , p} with |S| = s (cid:28) p and for some s ∈ (0, 1/2), we
have that the set of neighbours of S, Γ(S), satisfy |Γ(S)| ≥ (1 − s)d|S|, then the graph is called a (s, d, s)-
lossless expander graph. The adjacency matrix of an expander graph is denoted as X ∈ {0, 1}n×p and is called
as expander matrix.

In the above, the attribute |Γ(S)| ≥ (1 − s)d|S| is known as the expansion property, where s (cid:28) 1 [20]. As [5]
states, expander matrices satisfy the RIP-1 condition if ∀s-sparse vector β ∈ Rp, we have:

(1 − 2s)d(cid:107)β(cid:107)1 ≤ (cid:107)Xβ(cid:107)1 ≤ d(cid:107)β(cid:107)1.

(4)

Since the recovery algorithms typically use X and its adjoint XT as subroutines over vectors, sparse matrices
have low computational complexity, even in the convex case (2), without any loss in sample complexity n =
O (s log (p/s)); see Section 7 for some illustrative results on this matter. Along with computational beneﬁts, [5]
provides recovery guarantees of the form (1) for BP recovery where q1 = q2 = 1.
Model-based sparse recovery: Nevertheless, sparsity is merely a ﬁrst-order description of β(cid:63) and in many
applications we have considerably more information a priori. In this work, we consider the k-sparse block model
[29, 3, 22]:

Deﬁnition 1.2. We denote a block-sparse structure by M := {G1, . . . ,GM} where Gi ⊆ [p],|Gi| = g for
i = 1, . . . , M , Gi ∩Gj = ∅, i (cid:54)= j, and M is the total number of groups. Given a group structure M, the k-sparse
block model is deﬁned as the collection of sets Mk := {Gi1, . . . ,Gik} of k groups from M.

We consider block-sparse models such that ∪G∈MG ≡ {1, . . . , p} and g = p/M where M is the number of
groups. The idea behind group models is the identiﬁcation of group of variates that should be either selected
or discarded (i.e., set to zero) together. Such settings naturally appear in applications such as gene expression
data [30] and neuroimaging [13].

As an indicator of what can be achieved, Model-based Compressive Sensing [4],

leverages such mod-
els with dense sensing matrices to provably reduce the number of measurements for stable recovery from
O (kg log (p/(kg)) to O (kg + k log(M/k)) using non-convex schemes2; see [14] for a more recent discussion and
extensions on this matter. Along this research direction, [17] and [2] propose non-convex algorithms for equally-
sized and variable-sized block-sparse signal recovery, respectively, using sparse matrices. To show reduction in
sampling complexity, X is assumed to satisfy the model RIP-1 [17]:3

(1 − 2Mk )d(cid:107)β(cid:107)1 ≤ (cid:107)Xβ(cid:107)1 ≤ d(cid:107)β(cid:107)1,

∀k-block-sparse vector β ∈ Rp, where Mk represents the model-based expansion constant.
1.1 Contributions

(5)

Restricting error guarantees to only variations of standard (cid:96)q1 /(cid:96)q2-norms might be inconvenient in some ap-
plications. Here, we broaden the results in [16] towards having non-standard (cid:96)q distances in approximation
guarantees. To the best of our knowledge, this work is the ﬁrst attempt for provable convex recovery with
approximate guarantees in the appropriate norm and using sparse matrices. Similar results – but for a diﬀerent
model – for the case of dense (Gaussian) sensing matrices are presented in [12].

In particular, in Section 4, we provide provable error guarantees for the convex criterion:

β (cid:107)β(cid:107)2,1
min

subject to: y = Xβ,

(6)

where (cid:107)·(cid:107)2,1 is the (cid:96)2,1-norm; see next Section for details. Our key novelty is to provide (cid:96)2,1/(cid:96)2,1 approximation
guarantees using sparse matrices for sensing; see Section 5. In practice, we show the merits of using sparse
matrices in convex approaches for (6), both on synthetic and real data cases; see Section 7.

2Most of the non-convex approaches known heretofore consider a (block) sparse constrained optimization criterion, where one

is minimizing a data ﬁdelity term (e.g., a least-squares function) over cardinality constraints.

3While probabilistic constructions of expander matrices satisfying model RIP-1 coincide with those of regular RIP-1 expander
matrices, the model-based assumption on signals β in (5) results into constructions of expanders X with less number of rows and
guaranteed signal recovery.

2

2 Preliminaries

Scalars are mostly denoted by plain letters (e.g. k, p), vectors by lowercase boldface letters (e.g., x), matrices
by uppercase boldface letters (e.g., A) and sets by uppercase calligraphic letters (e.g., S), with the exception
of [p] which denotes the index set {1, . . . , p}. Given a set S in item space I such that S ⊆ I, we denote its
complement by I \ S; we also use the notation S c, when the item space is clearly described by context. For
vector β ∈ Rp, βS denotes the restriction of β onto S, i.e., (βS )i = βi if i ∈ S and 0 otherwise. We use |S| to

denote the cardinality of a set S. The (cid:96)p norm of a vector x ∈ Rp is deﬁned as (cid:107)x(cid:107)p := ((cid:80)p

The (cid:96)2,1-norm: The ﬁrst convex relaxation for block-sparse approximation is due to [29], who propose group
sparsity-inducing (cid:96)2,1 convex norm:

i=1 |xi|p)1/p.

(cid:88)

G∈M

(cid:107)β(cid:107)2,1 :=

wi(cid:107)βG(cid:107)2, wi > 0.

The (cid:96)2,1-norm construction follows group supports according to a predeﬁned model M and promotes sparsity
in the group level. We assume all the wi = 1, but may be otherwise for a more general setting [19].

Given a vector w ∈ Rp, we deﬁne M(cid:63)

k as the best k-block sparse index set according to:
M(cid:63)

Mk⊆M (cid:107)w − wMk(cid:107)2,1

k ∈ argmin

3 Problem statement

To properly set up the problem and our results, consider the following question:
Question: Let M be a predeﬁned and known a priori block sparse model of M groups of size g and k > 0 be a
user-deﬁned group-sparsity level. Consider X ∈ {0, 1}n×p (n < p) be a known (k · g, d, Mk ) expander sparse
β(cid:63) ∈ Rp is observed through y = Xβ(cid:63). Using criterion (6) to obtain a solution (cid:98)β such that y = X(cid:98)β, can we
matrix, satisfying model-RIP-1 in (5) for some degree d > 0 and Mk ∈ (0, 1/2). Assume an unknown vector
achieve a constant approximation of the form:

(cid:107)(cid:98)β − β(cid:63)(cid:107)2,1 ≤ C1 · (cid:107)β(cid:63) − β(cid:63)M(cid:63)

k(cid:107)2,1, C1 > 0,
k contains the groups of the best k-block sparse approximation of β(cid:63)?

where M(cid:63)

4 Main result

In this section, we answer aﬃrmatively to the above question with the following result:

model M, as in Deﬁnition 1.2. Consider two vectors β(cid:63),(cid:98)β ∈ Rp such that Xβ(cid:63) = X(cid:98)β, where β(cid:63) is unknown
Theorem 4.1. Let X ∈ {0, 1}n×p be a (k · g, d, Mk ) expander, satisfying model-RIP-1 (5) for block sparse
and (cid:98)β is the solution to (6). Then, (cid:107)(cid:98)β(cid:107)2,1 ≤ (cid:107)β(cid:63)(cid:107)2,1 and:
2
1 − 4Mk·g
1−2Mk

(cid:107)β(cid:63) −(cid:98)β(cid:107)2,1 ≤

a corollary to the theorem above for the more realistic noisy case.

In other words, Theorem 4.1 shows that, given a proper sparse matrix X satisfying the model-RIP-1 (5) for

structure M, the (cid:96)2,1-convex program in (6) provides a good block-sparse solution (cid:98)β ∈ Rp. Next, we also state
Corollary 4.2. Assume the setting in Theorem 4.1 and let two vectors β(cid:63),(cid:98)β ∈ Rp such that (cid:107)X(β(cid:63) −(cid:98)β)(cid:107)1 =
γ ≥ 0, where β(cid:63) is unknown and (cid:98)β is the solution to (6). Then, (cid:107)(cid:98)β(cid:107)2,1 ≤ (cid:107)β(cid:63)(cid:107)2,1 and:
γ
1 − 4Mk·g
1−2Mk

(cid:107)β(cid:63) −(cid:98)β(cid:107)2,1 ≤

2
1 − 4Mk·g
1−2Mk

· (cid:107)β(cid:63) − β(cid:63)M(cid:63)

· (cid:107)β(cid:63) − β(cid:63)M(cid:63)

k(cid:107)2,1 +

k(cid:107)2,1.

.

5 Proof of Theorem 4.1

Let us consider two solutions to the set of linear equations Xβ(cid:63) = y = X(cid:98)β, where β(cid:63),(cid:98)β ∈ Rp and let
X ∈ {0, 1}n×p be a given expander satisfying (5). We deﬁne z = (cid:98)β − β(cid:63) ∈ ker(X) since X(cid:98)β = Xβ(cid:63) =⇒
(cid:16)(cid:98)β − β(cid:63)(cid:17)

= 0.

X

3

above decompositions, we also have:

Using the group model formulation, each vector (cid:98)β, β(cid:63) can be decomposed as: (cid:98)β =(cid:80)G∈M(cid:98)βG
(cid:80)G∈M β(cid:63)G, where supp((cid:98)βG) = G, supp(β(cid:63)G) = G, ∀G ∈ M; moreover, assume that (cid:98)β (cid:54)= β(cid:63). Thus, given the
where supp(zG) = G. By assumption, we know that (cid:107)β(cid:63)(cid:107)2,1 ≥ (cid:107)(cid:98)β(cid:107)2,1. Using the deﬁnition of the (cid:107) · (cid:107)2,1, we

and β(cid:63) =

(cid:88)

G∈M

G∈M

(cid:17)

zG,

(7)

have:

Denote the set of k groups in the best k-block sparse approximation of β(cid:63) as M(cid:63)
(cid:107)zG + β(cid:63)G(cid:107)2

k. Then,

=:

(cid:88)

(cid:16)(cid:98)βG − β(cid:63)G
z =(cid:98)β − β(cid:63) =
(cid:88)
(cid:88)
(cid:88)
G∈M(cid:107)(cid:98)βG(cid:107)2 =
G∈M(cid:107)β(cid:63)G(cid:107)2 ≥
G∈M(cid:107)zG + β(cid:63)G(cid:107)2
(cid:88)
(cid:88)
(cid:88)
G∈M\M(cid:63)
(cid:88)
(cid:88)
(cid:107)zG(cid:107)2
(cid:88)
(cid:88)
(cid:107)zG(cid:107)2 −
G∈M\M(cid:63)
(cid:88)
(cid:88)
G∈M(cid:107)β(cid:63)G(cid:107)2 − 2
G∈M(cid:107)zG(cid:107)2 − 2

(cid:88)
(cid:107)zG + β(cid:63)G(cid:107)2 +

(cid:107)β(cid:63)G(cid:107)2 −

(cid:107)β(cid:63)G(cid:107)2

(cid:107)β(cid:63)G(cid:107)2

(cid:107)zG(cid:107)2

G∈M\M(cid:63)

G∈M\M(cid:63)

G∈M(cid:63)

G∈M(cid:63)

G∈M(cid:63)

G∈M(cid:63)

+

+

=

k

k

k

k

k

k

k

k

(cid:88)
G∈M(cid:107)β(cid:63)G(cid:107)2 ≥
≥

(8)

(9)

(10)

where the ﬁrst equality is due to the block-sparse model and the ﬁrst inequality is due to the triangle inequality.
To proceed, we require the following Lemma; the proof is based on [5].

Lemma 5.1. Let z ∈ Rp such that Xz = 0 and z =(cid:80)G∈M zG, where supp(zG) ⊆ G,∀G ∈ M. Then,

(cid:88)

G∈M(cid:63)

k

(cid:107)zG(cid:107)2 ≤

2Mk · g
1 − 2Mk ·

(cid:88)
G∈M(cid:107)zG(cid:107)2,

where Mk ∈ (0, 1/2) denotes the model-based expansion parameter of expander matrix X ∈ {0, 1}n×p.
Proof. Following [5], we assume the following decomposition of p indices, according to M: Due to the non-
overlapping nature of M, we split coordinates in p into k-block sparse index sets M0
k, . . . , Mt
k, M2
k
such that (i) each Ml
k has g indices and (iii)
there is ordering on groups such that:

k,∀l, has k groups (except probably Mt

k ≡ M(cid:63)

k, M1

k), (ii) each group G ∈ Ml
(cid:48)

(cid:107)zG(cid:107)2 ≥ (cid:107)zG(cid:48)(cid:107)2,

k s.t.
Since z ∈ ker(X), we have 0 = (cid:107)Xz(cid:107)1. Moreover, we denote as Γ(M(cid:63)

k) the set of indices of the rows of X
k; see Figure 1. Thus, without loss of generality, we reorder
k)—the union of the set of neighbors of
k. Given the above partition, we denote the submatrix of X composed of these rows as XΓ, such that

that correspond to the neighbours of left-nodes in M(cid:63)
the rows of X such that the top | ∪G∈M(cid:63)
all G ∈ M(cid:63)

k G| rows are indexed by Γ (M(cid:63)

∀G ∈ Ml

k, ∀G

l ≤ q.

∈ Mq

X =

Thus, we have:



 .

XΓ

XΓc

(cid:88)

G∈M

zG

zG

(cid:88)

k

zG(cid:107)1

(cid:107)1
(cid:107)1

(cid:88)
(cid:88)

G∈M\M(cid:63)

k

G∈M\M0

zG(cid:107)1 − (cid:107)XΓ

zG(cid:107)1

G∈M\M0

k

0 = (cid:107)Xz(cid:107)1 = (cid:107)XΓz(cid:107)1 = (cid:107)XΓ ·

 (cid:88)
 (cid:88)
(cid:88)

G∈M0

k

G∈M(cid:63)

k

G∈M0

k

= (cid:107)XΓ ·

= (cid:107)XΓ ·

≥ (cid:107)XΓ ·

zG +

zG +

4

Figure 1: Representation of neighbours of left-nodes, indexed by the k-group sparse set M(cid:63)
k.

(cid:88)

By using the model-RIP-1 property (5) of X on the input vector(cid:80)G∈M0
(cid:88)
(cid:88)
≥ (1 − 2Mk ) · d · (cid:107)
(cid:88)

= (1 − 2Mk ) · d ·

zG(cid:107)1 = (cid:107)X ·

(cid:88)

(cid:107)XΓ ·

zG(cid:107)1

G∈M0

G∈M0

G∈M0

k

k

k

G∈M0

k

k

zG we have:

zG(cid:107)1

(cid:107)zG(cid:107)1

(cid:107)zG(cid:107)2

≥ (1 − 2Mk ) · d ·

G∈M0

k

(cid:80)G∈M0

zG = 0, the second equality due to the non-overlapping groups

k

(cid:107)zG(cid:107)2 − (cid:107)XΓ

Therefore, we have:

where the ﬁrst equality is due to XΓc ·
and the last inequality since (cid:107)β(cid:107)1 ≥ (cid:107)β(cid:107)2, ∀β ∈ Rp.
(cid:88)
(cid:107)zG(cid:107)2 − (cid:88)
(cid:107)zG(cid:107)2 −(cid:88)
(cid:88)
(cid:107)zG(cid:107)2 −(cid:88)

0 ≥ (1 − 2Mk ) · d · (cid:88)
≥ (1 − 2Mk ) · d · (cid:88)
= (1 − 2Mk ) · d · (cid:88)
≥ (1 − 2Mk ) · d · (cid:88)

G∈M\M0

G∈M0

G∈M0

G∈M0

G∈Ml

l≥1

k

k

k

k

k

G∈M0

k

l≥1

zG(cid:107)1

G∈M\M0

k

(cid:107)XΓzG(cid:107)1

(cid:88)

(i,j)∈E
i∈G
j∈Γ

|(zG)i|

The quantity |# edges (i, j) : i ∈ G, j ∈ Γ,G ∈ Ml
|# edges (i, j) : i ∈ G, j ∈ Γ,G ∈ Ml

|# edges (i, j) : i ∈ G, j ∈ Γ,G ∈ Ml

k| · max
G∈Ml

k

i∈G |(zG)i|

max

(i)

k| further satisﬁes:
k) ∩ Γ(Ml
k| = |Γ(M0
k)|
k) ∪ Γ(Ml
= |Γ(M0
k)| + |Γ(Ml
k)| − |Γ(M0
k)|
≤ d · |M0
k| + d · |Ml
k| − |Γ(M0
k) ∪ Γ(Ml
k)|
≤ 2d · k · g − |Γ(M0
k) ∪ Γ(Ml
k)|
≤ 2d · k · g − d(1 − Mk )|M0
k ∪ Ml
k|
≤ 2d · k · Mk · g
(cid:88)

(iii)

(ii)

5

where (i) is due to the inclusion-exclusion principle, (ii) is due to the expansion property and, (iii) is due to
|G| = g, ∀G ∈ M.

Thus, the above inequality becomes:

0 ≥ (1 − 2Mk ) · d · (cid:88)

(cid:107)zG(cid:107)2 − 2d · k · Mk · g

max
G∈Ml
k

l≥1

i∈G |(zG)i|

max

G∈M0

k

......S? (S?)......M?k (M?k)Let Gl

max denote the group that contains the maximizing index imax such that:

imax ∈

argmax

i∈G s.t. G∈Ml

k

|(zG)i|.

Then:

0 ≥ (1 − 2Mk ) · d · (cid:88)

G∈M0

k

However, due to the ordering of (cid:96)2-norm of groups:

(cid:107)zGl

max

(cid:107)∞ ≤ (cid:107)zGl

max

0 ≥ (1 − 2Mk ) · d · (cid:88)
≥ (1 − 2Mk ) · d · (cid:88)

G∈M0

k

G∈M0

k

we further have:

which leads to:

(cid:88)

(cid:107)zGl

max

(cid:107)∞

(cid:107)zG(cid:107)2 − 2d · k · Mk · g
(cid:88)

l≥1

(cid:107)2 ≤ min
G∈Ml−1

(cid:107)zG(cid:107)2 ≤ 1
k

k

G∈Ml−1

k

1
k

(cid:107)zG(cid:107)2 − 2d · Mk · g

(cid:88)
(cid:107)zG(cid:107)2 − 2d · k · Mk · g
(cid:88)
(cid:88)
G∈M(cid:107)zG(cid:107)2

2Mk · g
1 − 2Mk ·

G∈M

(cid:107)zG(cid:107)2 ≤

(cid:107)zG(cid:107)2,

(cid:88)

(cid:107)zG(cid:107)2

G∈Ml−1

k

l≥1
(cid:107)zG(cid:107)2

(cid:88)

G∈M0

k

(cid:88)

Using (10) in (9), we further have:

0 ≥ −2

(cid:88)

G∈M

(cid:107)zG(cid:107)2 ≤

G∈M\M0

k

2

·g
1 − 4Mk
1−2Mk

(cid:107)zG(cid:107)2 − 4Mk · g
1 − 2Mk

(cid:88)

G∈M
(cid:107)β(cid:63)G(cid:107)2,

· (cid:88)

G∈M

(cid:107)zG(cid:107)2 =⇒

(cid:107)β(cid:63)G(cid:107)2 +
(cid:88)

G∈M\M0

k

which is the desired result.

6 Comparison to state-of-the-art

To justify our theoretical results, here we compare with the only known results on convex sparse recovery using
expander matrices of [5]. We note that, in [5], no apriori knowledge is assumed, beyond plain sparsity.

We start our discussion with the following remark.

Remark 6.1. In the extreme case of g = 1, Lemma 5.1 is analogous to Lemma 16 of [5]. To see this, observe
that (cid:107)zG(cid:107)2 = |(z)i| for g = 1, where |G| = 1 and G ≡ i ∈ M0

k or ∈ M.

We highlight that, as g grows, feasible values of Mk → 0, i.e., we require more rows to construct an expander
matrix X with such expansion property. For simplicity, in the discussion below we use  ≡ Mk interchangeably,
where the type of  used is apparent from context.
In the case where we are oblivious to any, a-priori known, structured sparsity model, [5] prove the following

error guarantees for the vanilla BP formulation (2), using expander matrices:

2
1 − 4 ·

1−2 · (cid:107)β(cid:63) − β(cid:63)S(cid:107)1,



(11)

k ∪i∈Gi|; i.e., we are looking for a solution of the same sparsity as the
where S ⊆ [p] such that |S| = | ∪G∈M(cid:63)
union of groups in the block-sparse case. In order to compare (11) with our result, a naive transformation of
(11) into (cid:96)2,1 terms leads to:

(cid:107)β(cid:63) −(cid:98)β(cid:107)1 ≤

(cid:107)β(cid:63) −(cid:98)β(cid:107)2,1 ≤

2√g
1 − 4 ·

1−2 · (cid:107)β(cid:63) − β(cid:63)M(cid:63)



k(cid:107)2,1.

(12)

6

Figure 2: Feasibility regions for our approach and [5] as a function of  and g variables.

To deﬁne the conditions under which (12) is valid, we require:

1 − 2
i.e.,  is independent of g, while in our case, we have:



1 − 4 ·

> 0 ⇒  < 1/6,

1 − 4 ·

 · g
1 − 2

> 0 ⇒  <

1

2(1 + 2g)

= 1/6

;

(cid:16)g=1

(cid:17)

i.e., our analysis provides weaker bounds with respect to the range of  values such that the error guarantees is
meaningful; see also Figure 2.

However, as already mentioned, (12) is oblivious to any model M: the solution (cid:98)β does not necessarily belong
does not guarantee (cid:98)β ∈ M, considering only simple sparsity. Section 7 includes examples that highlight the

to M. This fact provides degrees of freedom to obtain better approximation constants. Nevertheless, this
superiority of (cid:96)2,1-norm in optimization.

7 Experiments

We start our discussion (i) with a comparison between (cid:96)1- and (cid:96)2,1-norm convex approaches, when the ground
truth is known to be block sparse, and (ii) with a comparison between dense sub-Gaussian and sparse sensing
matrices in (6), both w.r.t. sampling complexity and computational complexity requirements. We conclude
with the task of recovering 2D images from compressed measurements, using block sparsity.

Solver. To solve both (cid:96)1- and (cid:96)2,1-norm instances (2) and (6), we use the primal-dual convex optimization
framework in [28], that solves (2) and (6) – among other objectives, by using ideas from the alternating direction
methods of multipliers. Using the same framework for solution, we obtain more accurate and credible comparison
results between diﬀerent problem settings.

non-overlapping groups. Observe that g = p/M = 10.

In this experiment, we verify that a priori structure knowledge helps in recovery. To show
(cid:96)1- vs. (cid:96)2,1-norm.
this in the convex domain, consider the following artiﬁcial example. Let y = Xβ(cid:63) be the set of observations,
where β(cid:63) ∈ R103
is a k-block sparse vector, for k = 8. Here, we assume a block-sparse model M with M = 100
Both for (cid:96)1- and (cid:96)2,1-norm cases, X ∈ Rn×p is designed as an expander, with degree d = (cid:100)22 · log (M )/g(cid:101) = 11,
complying with our theory. Further, we make the convention X := 1/d · X.
We consider two cases: (i) β(cid:63) is generated from a Gaussian distribution and (ii) β(cid:63) is generated as a
binary signal ∈ {±1}. In both cases, β(cid:63) is normalized such that (cid:107)β(cid:63)(cid:107)2 = 1. For all cases, we used the same
algorithmic framework [28] and performed 10 Monte Carlo realizations. Figure 3 shows the average probability
for successful recovery as a function of the total number of measurements observed; we declare a success when

(cid:107)(cid:98)β − β(cid:63)(cid:107)2 ≤ 10−5. It is apparent that knowing the model a priori can guarantee recovery with less number of

observations.

Sub-Gaussian vs. sparse sensing matrices in (cid:96)2,1-norm recovery. Let us focus now on the (cid:96)2,1-norm
case. For this setting, we perform two experiments. First, we consider a similar setting as previously; the only
diﬀerence lies in the way we generate the sensing matrix X. We consider two cases: (i) X ∼ N (0, 1/n · I) and
(ii) X is a sparse expander matrix, again with degree d = (cid:100)22 · log (M )/g(cid:101) = 11. Here, we only use the (cid:96)2,1-norm
solver. Figure 4 depicts some results. We observe that expander matrices perform worse – as a function of

7

Feasibleregion-`1/`1Feasibleregion-`2,1/`2,1✏1/601gFigure 3: Average probability of successful recovery, as a function of the total number of observations, over 10
Monte Carlo iterations. Top panel : β(cid:63) is generated from a normal distribution. Bottom panel : β(cid:63) is generated
from a Bernoulli distribution.
(cid:96)2,1-norm solver requires much less number of measurements for successful
recovery, compared to (cid:96)1-norm solver.

the total number of measurements required for successful recovery. Nevertheless, using sparse matrices is still
considerably competitive to dense Gaussian matrices.

Figure 4: Average probability of successful recovery, as a function of the total number of observations, over 10
Monte Carlo iterations. Expander matrices perform worse but competitive to dense Gaussian matrices.

Now, to grasp the full picture in this setting, we scale our experiments to higher dimensions. Table 1
summons up the results. All experiments were repeated for 10 independent realizations and the table contains

the median scores. For p = (cid:8)104, 2 · 104, 5 · 104, 105(cid:9), the total number of non-overlapping groups in M was
M =(cid:8)103, 2 · 103, 5 · 103, 5 · 103(cid:9), respectively. The column cardinality is selected as d = (cid:100)22 · log (M )/g(cid:101). For

each p, the block sparsity ranges as k ∈ {3, . . . , 6}. In all cases, n = (cid:100)0.4 · p(cid:101).
One can observe that using sparse matrices in this setting results into faster convergence – as in total time
required for stopping criterion to be met. Meanwhile, the solution quality is at least at the same levels, compared
to that when dense Gaussian matrices are used.

Finally, we highlight that, for p = 105, since M does not increase, the number of non-zeros d per column
decreases (i.e., d = 7 while d > 11 in all other cases). This results into a small deterioration of the recovery
quality; though, still comparable to the convex counterpart.4 Meanwhile, the time required for convergence in
the expander case remains at the same levels as when p = 5 · 104; however, the same does not apply for the
4Observe that in most conﬁgurations, expander matrices ﬁnd a solution closer to β(cid:63), compared to the dense setting, except for

the case of p = 105, where we decrease the number of zeros per column.

8

Numberofmeasurements0100200300400Probabilityofsuccess00.20.40.60.81`1-normBasisPursuit`2;1-normBasisPursuitNumberofmeasurements0100200300400Probabilityofsuccess00.20.40.60.81`1-normBasisPursuit`2;1-normBasisPursuitNumberofmeasurements50100150200250300350400Probabilityofsuccess00.10.20.30.40.50.60.70.80.91UsingexpandersmatricesUsingGaussianmatricesModel

p

k · g

(cid:107)(cid:98)β − β(cid:63)(cid:107)2

Time (sec)

Gaus.

Exp.

Gaus.

Exp.

10000

20000

50000

100000

300
400
500
600

300
400
500
600

300
400
500
600

600
800
1000
1200

8.6e-07
8.2e-06
8.6e-06
8.6e-06

8.1e-07
8.1e-06
8.5e-06
8.5e-06

8.2e-06
8.1e-06
8.5e-06
8.4e-06

8.1e-06
8.1e-06
8.4e-06
8.1e-06

3.3e-06
3.4e-06
3.2e-06
3.4e-06

3.4e-06
3.3e-06
3.4e-06
3.5e-06

3.3e-06
3.4e-06
3.6e-06
3.5e-06

9.4e-06
9.5e-06
9.4e-06
9.3e-06

24.3
27.5
27.8
31.2

95.5
79.4
83.9
91.3

419.3
432.8
436.0
435.4

1585.5
1598.2
1600.6
1648.0

5.8
6.2
6.0
7.6

18.5
16.4
15.8
18.9

49.6
45.8
52.9
51.1

55.1
54.5
56.2
55.6

Table 1: Summary of comparison results for reconstruction and eﬃciency. Median values are reported. As a

stopping criterion, we used (cid:107)βi+1 − βi(cid:107)2/(cid:107)βi+1(cid:107)2 ≤ 10−6, where βi is the estimate at the i-th iteration. In all
cases, n = (cid:100)0.4 · p(cid:101).

dense counterpart. This constitutes the use of expander matrices appealing in real applications.

Block sparsity in image processing. For this experiment, we use the real background-subtracted image
dataset, presented in [15]. Out of 205 frames, we randomly selected 100 frames to process. Each frame is rescaled
to be of size 28 × 28 pixels. Each pixel takes values in [0, 1]. We observe y = Xβ(cid:63) where β(cid:63) ∈ [0, 1]p, p = 216, is
the ground-truth vectorized frame and, X is either sparse or dense sensing matrix, designed as in the previous
experiments.

For the purpose of this experiment, we set up an upper wall time of 104 seconds (i.e., 2.8 hours) to process

100 frames for each solver. This translates into 100 seconds per frame.

Due to the nature of the dataset, we can safely assume that nonzeros are clustered together. Thus, we assume
group models M where groups are constituted of consecutive column pixels and the p indices are divided in
consecutive groups of equal size g = {4, 8, 16}. No other parameters are required – this is an advantage over
non-convex approaches, where a sparsity level is usually required to be known a priori. All experiments are
repeated 10 independent times for diﬀerent X’s.

Figure 5 shows some representative results. Left panel illustrates the recovery performance for diﬀerent
settings – (cid:96)1- vs. (cid:96)2,1-norm and Gaussian vs. sparse matrices X; results for other conﬁgurations are presented
in the appendix. The ﬁrst row considers a “simple” image with a small number of non-zeros; the other two rows
show two less sparse cases. While for the “simple” case, solving (cid:96)2,1-norm minimization with Gaussian matrices
lead to better recovery results – within the time constraints, the same does not apply for the more “complex”
cases. Overall, we observe that, given such time restrictions per frame, by using expander matrices one can
achieve a better solution in terms of PSNR faster. This is shown in more detail in Figure 5 (right panel); see
also the appendix for more results.

8 Conclusions

Sparse matrices are favorable objects in machine learning and optimization. When such matrices can be applied
in favor of dense ones, the computational requirements can be signiﬁcantly reduced in practice, both in terms of
space and runtime complexity. In this work, we both show theoretically and experimentally that such selection
is advantageous for the case of group-based basis pursuit recovery from linear measurements. As future work,
one can consider other sparsity models, as well as diﬀerent objective criteria.

References

[1] K. Ahn, S. Guha, and A. McGregor. Graph sketches: Sparsiﬁcation, spanners, and subgraphs. In Proceedings of

the 31st symposium on Principles of Database Systems, pages 5–14. ACM, 2012.

[2] B. Bah, L. Baldassarre, and V. Cevher. Model-based sketching and recovery with expanders. In Proceedings of

ACM-SIAM Symposium on Discrete Algorithms, pages 1529–1543. ACM–SIAM, 2014.

9

Original

(cid:96)1 + Exp.

(cid:96)1 + Gaus.

(cid:96)2,1 + Exp.

(cid:96)2,1 + Gaus.

57.3 dB

138.6 dB

128.8 dB

142.6 dB

19.2 dB

16.9 dB

25.8 dB

19.4 dB

20.6 dB

18.5 dB

31.5 dB

24.8 dB

Figure 5: Left panel : representative examples of subtracted frame recovery from compressed measurements.
Here, n = (cid:100)0.3·p(cid:101) measurements are observed for p = 216. Block sparse model M contains groups of consecutive
indices where g = 4. Right panel : Accumulative computational time required to process 100 frames. Overall,
using Gaussian matrices in the (cid:96)2,1-norm case, DecOpt required almost 2.8 hours (upper bound), as compared
to 0.55 hours when X is a sparse expander matrix. Thus, while Gaussian matrices is known to lead to better
recovery results if no time restrictions apply, sparse sensing matrices constitute an appealing choice in practice.

[3] Luca Baldassarre, Nirav Bhan, Volkan Cevher, Anastasios Kyrillidis, and Siddhartha Satpathi. Group-sparse model

selection: Hardness and relaxations. arXiv preprint arXiv:1303.3207, 2013.

[4] R. Baraniuk, V. Cevher, M. Duarte, and C. Hegde. Model-based compressive sensing. Information Theory, IEEE

Transactions on, 56(4):1982–2001, 2010.

[5] R. Berinde, A. Gilbert, P. Indyk, H. Karloﬀ, and M. Strauss. Combining geometry and combinatorics: A uniﬁed
In Communication, Control, and Computing, 2008 Allerton Conference on,

approach to sparse signal recovery.
pages 798–805. IEEE, 2008.

[6] E. Cand`es, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruction from highly

incomplete frequency information. Information Theory, IEEE Transactions on, 52(2):489–509, 2006.

[7] E. Cand`es, J. Romberg, and T. Tao. Stable signal recovery from incomplete and inaccurate measurements. Com-

munications on pure and applied mathematics, 59(8):1207–1223, 2006.

[8] E. Candes and T. Tao. Decoding by linear programming. Information Theory, IEEE Transactions on, 51(12):4203–

4215, 2005.

[9] E. Cand`es and M. Wakin. An introduction to compressive sampling. IEEE Signal Processing Magazine, 25(2):21 –

30, March 2008.

[10] S. Chen, D. Donoho, and M. Saunders. Atomic decomposition by basis pursuit. SIAM journal on scientiﬁc

computing, 20(1):33–61, 1998.

[11] D. Donoho. Compressed sensing. Information Theory, IEEE Transactions on, 52(4):1289–1306, 2006.

[12] Y. Eldar and M. Mishali. Robust recovery of signals from a structured union of subspaces. Information Theory,

IEEE Transactions on, 55(11):5302–5316, 2009.

[13] A. Gramfort and M. Kowalski.

Improving M/EEG source localization with an inter-condition sparse prior.

In

ISBI’09. IEEE International Symposium on, pages 141–144. IEEE, 2009.

[14] Chinmay Hegde, Piotr Indyk, and Ludwig Schmidt. A nearly-linear time framework for graph-structured sparsity.

In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 928–937, 2015.

[15] Junzhou Huang, Tong Zhang, and Dimitris Metaxas. Learning with structured sparsity. The Journal of Machine

Learning Research, 12:3371–3412, 2011.

[16] P. Indyk and E. Price. K-median clustering, model-based compressive sensing, and sparse recovery for earth mover

distance. In Proceedings of ACM symposium on Theory of Computing, pages 627–636. ACM, 2011.

[17] P. Indyk and I. Razenshteyn. On model-based RIP-1 matrices. In Automata, Languages, and Programming, pages

564–575. Springer, 2013.

[18] P. Indyk and M. Ruzic. Near-optimal sparse recovery in the (cid:96)1-norm. In Foundations of Computer Science, 2008.

IEEE 49th Annual IEEE Symposium on, pages 199–207. IEEE, 2008.

[19] L. Jacob, G. Obozinski, and J.-P. Vert. Group lasso with overlap and graph lasso. In Proceedings of the 26th Annual

International Conference on Machine Learning, pages 433–440. ACM, 2009.

[20] S. Jafarpour, W. Xu, B. Hassibi, and R. Calderbank. Eﬃcient and robust compressed sensing using optimized

expander graphs. Information Theory, IEEE Transactions on, 55(9):4299–4308, 2009.

10

Totalnumberofframesprocessed20406080100Accumulativeprocessingtime(sec)102103104`1+Exp.`1+Gaus.`2;1+Exp.`2;1+Gaus.[21] A. Kyrillidis and V. Cevher. Recipes on hard thresholding methods.

In 4th IEEE International Workshop on

Computational Advances in Multi-Sensor Adaptive Processing, 2011.

[22] Anastasios Kyrillidis, Luca Baldassarre, Marwa El Halabi, Quoc Tran-Dinh, and Volkan Cevher. Structured sparsity:

Discrete and convex approaches. In Compressed Sensing and its Applications, pages 341–387. Springer, 2015.

[23] Anastasios Kyrillidis and Volkan Cevher. Combinatorial selection and least absolute shrinkage via the CLASH
algorithm. In Information Theory Proceedings (ISIT), 2012 IEEE International Symposium on, pages 2216–2220.
IEEE, 2012.

[24] D. Needell and J. Tropp. CoSaMP: Iterative signal recovery from incomplete and inaccurate samples. Applied and

Computational Harmonic Analysis, 26(3):301–321, 2009.

[25] J. Nelson, H. Nguyen, and D. Woodruﬀ. On deterministic sketching and streaming for sparse recovery and norm

estimation. pages 627–638, 2012.

[26] Eric Price. Sparse recovery and Fourier sampling. PhD thesis, Massachusetts Institute of Technology, 2013.

[27] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B

(Methodological), pages 267–288, 1996.

[28] Quoc Tran-Dinh and Volkan Cevher. Constrained convex minimization via model-based excessive gap. In Advances

in Neural Information Processing Systems, pages 721–729, 2014.

[29] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal

Statistical Society: Series B, 68(1):49–67, 2006.

[30] H. Zhou, M. Sehl, J. Sinsheimer, and K. Lange. Association screening of common and rare genetic variants by

penalized regression. Bioinformatics, 26(19):2375, 2010.

9 Appendix

Here, we report further results on the 2D image recovery problem. We remind that, for the purpose of this
experiment, we set up an upper wall time of 104 seconds (i.e., 2.8 hours) to process 100 frames for each solver.
This translates into 100 seconds per frame.

9.1 Varying group size g

For this case, we focus on a single frame. Due to its higher number of non-zeros, we have selected the frame
shown in Figure 6. For this case, we consider a roughly suﬃcient number of measurements is acquired where
n = (cid:100)0.3 · p(cid:101). By varying the group size g, we obtain the results in Figure 6.

9.2 Varying number of measurements

Here, let g = 4 as this group selection performs better, as shown in the previous subection. Here, we consider
n take values from n ∈ (cid:100){0.25, 0.3, 0.35, 0.4} · p(cid:101). The results, are shown in Figure 7.

11

Original

(cid:96)1 + Exp.

(cid:96)1 + Gaus.

(cid:96)2,1 + Exp.

(cid:96)2,1 + Gaus.

20.6 dB

18.5 dB

31.5 dB

24.8 dB

20.6 dB

18.4 dB

30.8 dB

23.3 dB

20.7 dB

18.5 dB

28.0 dB

22.2 dB

Figure 6: Results from real data. Representative examples of subtracted frame recovery from compressed
measurements. Here, n = (cid:100)0.3 · p(cid:101) measurements are observed for p = 216. From top to bottom, each line
corresponds to block sparse model M with groups of consecutive indices, where g = 4, g = 8, and g = 16,
respectively. One can observe that one obtains worse recovery as the group size increases; thus a model with
groups g = 4 is a good choice for this case.

Original

(cid:96)1 + Exp.

(cid:96)1 + Gaus.

(cid:96)2,1 + Exp.

(cid:96)2,1 + Gaus.

19.8 dB

17.8 dB

29.0 dB

20.3 dB

20.6 dB

18.5 dB

31.5 dB

22.8 dB

21.7 dB

19.3 dB

34.9 dB

25.3 dB

23.9 dB

20.4 dB

39.1 dB

29.8 dB

Figure 7: Results from real data. Representative examples of subtracted frame recovery from compressed
measurements. Here, we consider a block sparse model ﬁxed, with g = 4 block size per group. From top to
bottom, the number of measurements range from (cid:100)0.25 · p(cid:101) to (cid:100)0.4 · p(cid:101), for p = 216. One can observe that one
obtains better recovery as the number of measurements increases.

12

g=4g=8g=16n=d0:25"pen=d0:3"pen=d0:35"pen=d0:4"pe