6
1
0
2

 

b
e
F
9
2

 

 
 
]

O
C

.
t
a
t
s
[
 
 

1
v
9
6
0
0
0

.

3
0
6
1
:
v
i
X
r
a

Tukey depth: linear programming and

applications

Pavlo Mozharovskyi

Centre Henri Lebesgue

Agrocampus Ouest

Institute of Mathematical Research of Rennes

February 29, 2016

Abstract

Determining the representativeness of a point within a data cloud
has recently become a desirable task in multivariate analysis. The
concept of statistical depth function, which reﬂects centrality of an
arbitrary point, appears to be useful and has been studied intensively
during the last decades. Here the issue of exact computation of the
classical Tukey data depth is addressed. The paper suggests an algo-
rithm that exploits connection between the Tukey depth and linear
separability and is based on iterative application of linear program-
ming. The algorithm further develops the idea of the cone segmen-
tation of the Euclidean space and allows for eﬃcient implementation
due to the special search structure. The presentation is complemented
by relationship to similar concepts and examples of application.

Keywords: Tukey depth; Linear programming; Cone segmentation; Breadth-

ﬁrst search algorithm; Exact computation; Simplex algorithm.

1

Introduction

Determining the representativeness of a point within a bunch of data or
a probability measure has recently become a desirable task in multivariate

1

analysis. Nowadays it ﬁnds applications in diﬀerent domains of economics,
biology, geography, medicine, cosmology and many others. In his celebrated
work, Tukey (1975) introduced an idea to order multivariate data, which
has later been developed by Donoho & Gasko (1992) and is known as the
Tukey (=halfspace, location) depth. Generally, the statistical data depth is a
function determining how centrally a point is located in a data cloud. The
upper-level sets it generates — trimmed regions — are set-valued statistics.
They trim data w.r.t. the degree of centrality. For more information on the
data depth the reader is referred to Zuo & Serﬂing (2000), Dyckerhoﬀ (2004),
Mosler (2013) and mentioned there references.

The Tukey data depth is one of the most important depth notions and
is historically the ﬁrst one. Regard a random vector X distributed as P ,
in particular empirically on {x1, ..., xn}, in Rd. The Tukey depth of a point
z ∈ Rd w.r.t. X, further D(z|X), is deﬁned as the smallest probability mass
of a closed halfspace containing z:

D(z|P ) = inf{P (H)| H closed half-space, z ∈ H}.

(1)

The Tukey depth possesses many desirable properties: it is aﬃne invari-
ant, tends to zero at inﬁnity, is monotone on rays from any deepest point,
quasiconcave and upper semicontinuous. By that it satisﬁes all the postulates
imposed on a depth function (Zuo & Serﬂing, 2000, Dyckerhoﬀ, 2004, Mosler,
2013). If P is absolutely continuous, the Tukey depth is a continuous function
of z achieving maximum value of 1
2, for angularly symmetric distributions at
the center of symmetry. If P has no Lebesgue density, the Tukey depth is a
discrete function of z and can have a non-unique maximum. By deﬁnition,
its empirical version vanishes beyond the convex hull of the data. The Tukey
depth determines uniquely empirical distribution (Koshevoy, 2002), taking
a ﬁnite number of values in the interval from 0 (for the points lying outside
the convex hull of the data) to 1
n. Naturally,
it has attractive breakdown properties and converges for a sample from P
almost surely to the depth w.r.t. P (Donoho & Gasko, 1992).
For a data cloud D(z|X) can be expressed as the smallest portion of X
to be cut oﬀ by a hyperplane through z so that the remaining points lie in
an open halfspace not containing z:

2, increasing by a multiple of 1

D(z|X) =

1
n

min
r∈Sd−1

#{i|x(cid:48)

ir ≥ z(cid:48)r, xi ∈ X}.

(2)

2

Exact calculation of the Tukey depth is a computationally challenging
task of non-polynomial complexity. For this reason, great part of the litera-
ture on the Tukey depth concerns its computational aspects. The reader is
referred to Dyckerhoﬀ & Mozharovskyi (2015) for the exact algorithm and a
reference to some preceding works. Dyckerhoﬀ (2004) introduced the weak
projection property, which is satisﬁed inter alia by the Tukey depth. This
allows to approximate the depth as the minimum over univarite depths in
the projections onto one-dimensional spaces. For the latest research in this
area see Chen et al. (2013) and contained there references.

In the current paper an algorithm based on linear programming is pro-
posed. Here, the idea of the conic segmentation of Rd, introduced by Mosler
et al. (2009) for constructing zonoid trimmed region, is exploited. Applied to
the Tukey depth this can be formulated as follows: The entire space is divided
into polyhedral cones, each having — in the projection onto any direction in
its interior — the same subset of X above (below) the projection of z, and
thus delivering the same univariate Tukey depth. For each of the cones this
depth value is calculated, and the Tukey depth is then the minimum over all
these depths. Then, starting from an arbitrary cone, all cones are regarded
by means of the breadth-ﬁrst search algorithm. A procedure employing this
principle for computation of the Tukey depth has been suggested by Liu &
Zuo (2014a), where the convex hull algorithm (Barber et al., 1996) is used
to detect neighboring ones.

In the proposed method, the connection between the problem of linear
separability in binary classiﬁcation problem and the Tukey depth is exploited.
Based on this, linear programming is used for ﬁnding cone’s neighbors, and
each cone is coded by a binary sequence. First, this gives possibility to exam-
ine only candidates of interest for the neighboring cones, separately. Second,
the number of the candidates to be checked can be substantially reduced.
Third, when employing the binary coding, one does not need to compute
the cone’s location in Rd, which allows for reducing precision problems and
further saves computational expenses. Also, the calculations are performed
in the spaces of dimension d − 1, by a simplex algorithm. Finally, linear
programming allows for caching by remembering (last) found basis for each
hyperplane.
To be precise, below the following task is being solved: Given a data
sample X = {x1, ..., xn} ∈ Rd, d < n, and a point z ∈ Rd, the Tukey depth
of z w.r.t. X shall be calculated. For the rest of the paper, we assume that
{z}∪ X are in general position, i.e., every subset of k + 1 points ∈ ({z}∪ X)

3

spans a subspace of dimension min{k, d}. Violation of these assumptions
can be compensated by a location shift and a slight perturbation of the
data. The Tukey depth is discrete, so such a perturbation can be potentially
harmful, as only a small shift of one point can change the depth value of z
in a non-continuous way. Before performing such a perturbation, we suggest
to ﬁrst check whether z ∈ conv(X) (if not, D(z|X) = 0), and only then
calculate the depth of z using perturbed data. When n is not very small
and the zero-depth case is specially treated, possible perturbation damage is
negligible.

The rest of the paper is organized as follows. First, in Section 2, we regard
connection of the concept of the Tukey depth to some important problems
of the multivariate analysis. Then, Section 3 provides the theoretical results
for the proposed algorithm, which is given in Section 4. Further, Section 5
suggests two demonstrative examples where application of the Tukey depth
can be advantageous. Section 6 concludes.

2 Connections

Due to its indicator-loss nature, the Tukey depth is known to be connected
to a number of problems. Although just the smallest portion of the points to
be cut oﬀ by a closed halfspace is required as an output, in many algorithms
the boundary hyperplane is found or can be restored based on the output
information. For shortness, let R = arg minr∈Sd−1 #{i|x(cid:48)
ir ≥ z(cid:48)r, xi ∈ X} be
the set of directions r ⊂ Sd−1 each achieving 1
n#{i|x(cid:48)
ir ≥ z(cid:48)r, xi ∈ X} =
D(z|X).

Densest hemisphere. The problem of computing the Tukey depth is in-
variant more than just in the aﬃne way. Thus shifting X to get z in the
origin and projecting X onto the unit sphere Sd−1 after that (a non-aﬃne
transformation), changes neither the value of the depth D(z|X) nor the set of
optimal normals to separating hyperplanes R, i.e. D(z|X) = D(0|Y ) as well
as the optimal argument set with Y = { xi−z
(cid:107)xi−z(cid:107)|i = 1, ..., n} ⊂ Sd−1. The last
one corresponds to the open densest hemisphere problem shown by John-
son & Preparata (1978) to be of non-polynomial time complexity, namely
O(nd−1 log n) if dimension d is ﬁxed, for the Tukey depth, see Dyckerhoﬀ &
Mozharovskyi (2015).

Classiﬁcation. By a trivial modiﬁcation, the task of supervised binary
linear classiﬁcation can be narrowed down to ﬁnding an optimal argument

4

r from (2), see also Ghosh & Chaudhuri (2005).
Indeed, regard X1 =
{x1, ..., xn1} and X2 = {xn1+1, ..., xn1+n2} being two training classes in Rd,
and let Y = {xi − xj|i = 1, ..., n1, j = n1 + 1, ..., n1 + n2}. When minimizing
empirical risk of a linear classiﬁer, we are interested in a direction r ∈ Sd−1
in projection on which possibly many diﬀerences xi − xj have the same sign,
or in other words, as many (few) as possible points from Y lie on the same
side of a hyperplane through 0. The last holds for any element of R.

Regression depth. Rousseeuw & Hubert (1999) deﬁne data depth for
a liner regression model based on the notion of nonﬁt: Given regression
input X = {x1, ..., xn} in Rd and output y = {y1, ..., yn} in R, a ﬁt b =
(b0, b1, ..., bd) is called a nonﬁt if there exists an aﬃne hyperplane in the input
space not containing any point from X such that all the points from X lying
on its same side have residuals strictly of the same sign. Regression depth
of a ﬁt b ∈ Rd+1 is the smallest number of observations to be removed from
X suﬃcient for b to become a nonﬁt. Given a ﬁt b ∈ Rd+1, after splitting
X into two classes on the basis of the sign of residuals (and acting as in the
preceding paragraph), regression depth is given by the empirical risk in the
projection on any element of R.

Maximum feasible subsystem. Again, let Y = {yi = xi−z

(cid:107)xi−z(cid:107)|i = 1, ..., n} ⊂
Sd−1. First consider the case d = 2, where Y lies on the unit circle. Here yi
deﬁnes an open halfcircle, in which each point deﬁnes a closed halfspace with
the origin on its boundary and never containing yi. Then the task of compu-
tation of the Tukey depth narrows down to ﬁnding a point on the unit circle
contained in the largest number of these halfcircles; the set of such points
coincides with R. Extending this logic to higher dimensions (Bremner et al.,
2008) leads to (two instances of) the maximum feasible subsystem problem
in Rd−1. For d > 2, in the same way, each yi deﬁnes an open halfspace in
which each element (=point ∈ Rd) is a normal to a hyperplane deﬁning a
halfspace with the origin on its boundary and not containing yi. We are
interested in a point lying in the intersection of the highes number of these
halfspaces. Let H+ = {(x1, ..., xd)(cid:48)|xd = 1} (H− = {(x1, ..., xd)(cid:48)|xd = −1})
be a positive (respectively negative) hyperplane. For each yi, intersection of
such open halfspace with H+ yields an open halfspace in H+ of dimension
Rd−1; the same holds for H−. Then, the maximum number such halfspaces ei-
ther in H+ or in H− having nonempty intersection divided through n equals
the Tukey depth. The two maximum feasible subsystem problems consist
2 − arccos yi,(d)) for H+ and or
of linear inequalities x yi,(1,...,d−1)

(cid:107)yi,(1,...,d−1)(cid:107) ≤ − tan( π

5

(cid:107)yi,(1,...,d−1)(cid:107) ≤ tan( π
2 − arccos yi,(d)) for H−. (Note that due to the general
x yi,(1,...,d−1)
position assumption there is no diﬀerence between open and closed halfs-
paces, and the equator {(x1, ..., xd)(cid:48)|xd = 0} should not be searched through
because R has nonzero volume on Sd−1.)

Hallin et al. (2010) establish connection of the Tukey depth to the linear
programming via quantile regression (see also Koenker & Basset, 1978) and
exploit this to compute Tukey depth regions. In what follows, connection
between the Tukey depth and linear programming via linear separability is
used as a basis for construction of an algorithm computing the Tukey depth.

3 Theoretical background

πr(n)r.

in Rd), X +

r

πr(1)r ≤ x(cid:48)

πr(2)r ≤

πr(1)r < x(cid:48)

πr(2)r < ... < x(cid:48)

r = {x ∈ X|x(cid:48)r > 0} and X−

For simplicity and w.l.o.g., assume for the rest that z = 0. Consider a
direction, i.e. a point on the unit sphere r ∈ Sd−1.
It yields an ordered
sequence, a permutation πr on N = {1, ..., n} such that x(cid:48)
... ≤ x(cid:48)
If the data are in general position a vector r can be found
such that all inequalities hold strictly x(cid:48)
πr(n)r, and
πr(i)r (cid:54)= 0, i = 1, ..., n. Then such r splits X into two disjoint subsets (by
x(cid:48)
its normal hyperplane Hr through 0 yielding two open halfspaces H +
r and
r = {x ∈ X|x(cid:48)r < 0} containing
H−
the points with strictly positive, respectively negative, projections on r. Let
us call the closure of the set of all λr, λ ≥ 0, maintaining the same X +
C = {x ∈ X|x(cid:48)r > 0∀ r ∈ int(C)}
and X−
r , a direction cone C (yielding X +
C = {x ∈ X|x(cid:48)r < 0∀ r ∈ int(C)} respectively). This is because its
and X−
form constitutes an inﬁnite polyhedral cone with the apex in the origin. The
entire Rd is then ﬁlled by the set of all direction cones, say C(X), while each
cone C ∈ C(X) deﬁnes some portion of the sample, that can be cut oﬀ by
the hyperplane normal to any r ∈ int(C). Denote this portion DC(0|X) =
C} ((cid:93) stands for the set’s cardinality), then the Tukey depth
n min{(cid:93)X +
is D(0|X) = minC∈C(X) DC(0|X). Below C(X) will be mentioned as cone
segmentation; see Figure 1 left for a cone segmentation on the unit cube for
ten standard normal deviates. One of the direction cones can be seen in the
unit cube’s corner directed to the reader.
C , (cid:93)X−

The further task is then to go through all such cones and to ﬁnd the one(s)
C}, i.e., the Tukey depth. Starting

delivering the smallest 1
with Mosler et al. (2009), the usual way to proceed is:

1

C , (cid:93)X−

r

n min{(cid:93)X +

6

Figure 1: Cone segmentation on the unit cube (left) and a cone’s facet deﬁned
by a point (right)

(1) choose an arbitrary direction cone,

(2) move from each direction cone to the neighbors,
(3) by that cover the entire Rd using breath-ﬁrst search algorithm,

(4) on each step check whether a direction cone has already been consid-
ered, i.e. saved in a structure maintaining fast search (usually a binary
search tree).

Ad (1), the task is trivial: a direction r ∈ Sd−1 maintaining the ordering
with strict inequalities x(cid:48)
πr(n)r and no projection
coinciding with z(cid:48)r = 0 has to be generated. When drawing r randomly, the
theoretical probability of this event = 1. As in practice draw concerns only
a ﬁnite number of digits, it can (though extremely rarely) happen that one
needs more than one drawing.

πr(2)r < ... < x(cid:48)

πr(1)r < x(cid:48)

3.1

Identiﬁcation of neighboring cones

Ad (2), identifying neighboring direction cones (2a) and transition to each
of them if new (2b) is to be done. Let us take a closer look at the direction

7

xi00C1

C2

, X−

, X−

) and (X +
C2

cone. Two diﬀerent cones C1 and C2 diﬀer in their corresponding set pairs
). So, if a point r ∈ Sd−1 moves from one direction
(X +
C1
cone to another, projections of one or more points on r migrate passing the
origin, i.e., change the sign. Let C1 and C2 be two cones, such that a direct
(i.e., not crossing other cones) rotational movement of r from C1 to C2 (and
vice-versa) is possible. That means that C1 and C2 have an intersection of
aﬃne dimension between 1 and d−1. If transition of r from C1 to C2 involves
r or vice versa) by one point ∈ X only
changing the halfspace (from H +
(correspondingly changing the sign of its projection on r), then C1 and C2
intersect in aﬃne dimension d − 1. This intersection constitutes the cones’
common facet. We call such two cones neighboring cones.

r to H−

C1

C2

to X−

So, the transition of a single point xi ∈ X from X +

means
traversing of r from C1 to a neighboring cone C2 through a facet, and thus
the facet is deﬁned by this point xi, see Figure 1 right. Naturally, given a
cone C, any facet of C lies in a hyperplane, normal to the line, connecting a
point ∈ X with z = 0, as it is shown in Figure 1 right, but not each point
∈ X generates a facet of C, see Figure 2. A direction cone C is deﬁned
by the intersection of closed halfspaces {y|y(cid:48)(x − z) ≥ 0, x ∈ X +
C} and
{y|y(cid:48)(x− z) ≤ 0, x ∈ X−
C}. Hyperplanes directly involved in the intersection
(generated by points x1, x2, x3 in Figure 2) contain the cone’s facets and
those outside (generated by points x4, x5 in Figure 2) do not. Thus, given
a direction cone, a natural question is: Which points ∈ X deﬁne its facets,
and which do not? This is summarized in Theorem 1.
Theorem 1 Given X = {x1, ..., xn} ∈ Rd, assume that {0} ∪ X are in
general position, and let C be a direction cone. Also, for a point x ∈ X let
XHx be the orthogonal projection of X onto the (d − 1)-dimensional linear
Hx,C be the two subsets of XHx \
subspace Hx normal to x, and X +
{0} corresponding to X +
C and X−

Hx,C and X−
C , respectively. Then:

(i) Hx contains a facet of C if and only if X +

Hx,C are linearly
strictly separable through 0 ∈ Rd−1, i.e., can be separated by a (d-2)-
hyperplane ⊂ Hx containing the origin and no points from X +
Hx,C ∪
X−
Hx,C,

Hx,C and X−

(ii) if r ∈ Sd−1 moves from C to a neighboring direction cone through a
facet ⊂ Hx, the projection of x only on the line through r changes sign.

8

Figure 2: A direction cone in R3 deﬁned by the points x1, x2 and x3, halfs-
paces formed by x4 and x5 are not directly involved (left); arbitrary cutting
hyperplane h visualizing how the hyperplanes are involved (right).

Proof 1

(i) “=⇒”: If x ∈ X deﬁnes a facet of C, then Hx contains this
facet, and thus there should exist some direction v ∈ Sd−1 ∩ Hx such
C and X−
that X +
C projected onto v maintain their signs, except for the
single point x being projected into 0 ∈ Rd−1. So, these projections of
C \ {x} and X−
C \ {x}) are separated in Hx by the
C and X−
X +
hyperplane normal to v through 0.
“⇐=”: Strict linear separability of X +
Hx,C through 0 means
that there exists some v ∈ Sd−1 ∩ Hx, such that x(cid:48)v > 0 ∀ x ∈ X +
Hx,C
and x(cid:48)v < 0 ∀ x ∈ X−
Hx,C. Then a slight inﬁnitesimal rotation of
v towards (and inside of ) the cone does not cause the projections to
change signs, and thus maintains X +

Hx,C and X−

C (or X +

C and X−
C .

(ii) Let x deﬁne a common facet of C and C(cid:48). As x is projected into
0 ∈ Rd−1 for all r ∈ Sd−1∩ Hx, then obviously when (slightly) deviating
v to diﬀerent sides of Hx, the signs of x(cid:48)v will be opposite. All points
∈ {λx, λ ∈ R} change sign in their projection on v, but as {0} ∪ X
are in general position, x is the only one.

3.2 Optimization of the breadth-ﬁrst search algorithm

In Section 3.1 we have addressed (2a) and (2b) by Theorem 1. From the ﬁrst
part, one can easily ﬁnd out which points deﬁne the cone’s facets. Then,

9

x5x30x1x2x4hhHx1Hx2Hx3Hx4Hx5following the second part, moving the direction r to a neighboring cone by
traversing their common facet constitutes in changing sign of the projection
on r of the point which deﬁnes this facet.

Ad (3), we use the results from above to describe the breadth-ﬁrst search
algorithm: generate an initial direction cone (ad (1)) and move to the neigh-
boring cones (ad (2)), calculating the depth in each of them, till the entire Rd
is covered. Note, that covering a cone segmentation of Rd by a breadth-ﬁrst
search is general for some depth-calculating algorithms (Liu & Zuo (2014a,b))
and algorithms constructing trimmed regions (Mosler et al. (2009), Paindav-
eine & ˇSiman (2012a,b), Bazovkin & Mosler (2012)) when d ≥ 3. Below the
algorithm is summarized to be referenced it in further explanations. The
algorithms of Paindaveine & ˇSiman (2012a,b), Liu & Zuo (2014a,b) diﬀer
from this one in that they store cones’ facets and not cones while employing
the convex hull algorithm.

The breadth-ﬁrst search algorithm on a cone segmentation of Rd proceeds

in following steps:

(a) Draw an initial cone and store it in a queue.

(b) Pop one cone from the head of the queue, process it, remember it, and

for each of its neighboring cones do:

(c) If the cone has not been processed till now push it into the tail of

the queue.

(d) If the queue is not empty, go to Step (b).

Further, let us introduce the notion of the cone’s generation, a number
given to each cone (in Step c) when it is pushed into the queue. The initially
drawn cone (in Step a) is given the initial number, say 1. (The generation can
be thought of as the ‘depth’ of the current searching path of the algorithm.)
Obviously, when covering a cone segmentation of Rd with the breadth-ﬁrst
search algorithm, for processing cones of the i-generation, only cones of the
(i − 1)-, i- and (i + 1)-generation have to be remembered. While on starting
(low) generations the number of the cones from one generation to another
grows rapidly, on close to ‘equatorial’ generations (these basically constitute
the segmentation) the increase is much less. Also, though the store-search
structure for the cones is usually a binary tree, the computational time for
search can be saved either, especially when the search is frequently performed.

10

Ad (4): When calculating the Tukey depth under the general position
assumption of {z} ∪ X, one can step much further in this direction, and
save less than tree generations. First, to simplify further presentation, let us
code the cones. As mentioned above, the interior of each cone C maintains
the disjoint division of X into X +
C according to the signs in X’s
projection onto any r ∈ int(C), and thus is uniquely deﬁned by this division.
So, binary identiﬁers for the cones can be used: a cone is coded by a binary
sequence (“0” and “1” say) of length n, where each bit represents a point
∈ X w.r.t. some initial ordering of the points ∈ X that is kept constant
during the entire procedure. Points belonging to X +
C are coded by “1”, those
belonging to X−

C and X−

C by “0”.

After coding the initial cone (C0 say) this way, other cones can be coded
either the same way, or by another binary sequence identifying whether a
point has changed the sign w.r.t. C0 (“1”) or not (“0”). Then any cone’s
code can be obtained as the code of C0 with those bits inverted that have
been switched on in this sequence. This leads to Lemma 1.

Lemma 1 Let us start the breadth-ﬁrst search algorithm with an arbitrary
initial cone C0, and in Step b, when checking for neighboring cones, always
regard only cones deﬁned by points which have not changed their sign in
the projection onto r ∈ int(C0) yet. Then in processing cones of the i-th
generation, only cones of the i-th generation have to be remembered to check
for neighboring cones and of the (i + 1)-th generation to check whether a new
cone has already been seen.

Proof 2 If the cones deﬁned by already processed points, i.e. those having
changed their sign in the projection, are not considered, then only cones of
the (i + 1)-th generation can be taken into account when deciding whether a
cone has already been seen. No cones of the (i − 1)-th or i-th generation can
be found because points deﬁning them are not checked at all. Then one can
go through all the cones of the i-th generation, and add those newly found
from the (i + 1)-th generation to the queue.

Theorem 1 (ii) and Lemma 1 lead to Lemma 2. Note that (cid:98)u(cid:99) stands for

the largest integer ≤ u.
Lemma 2 When starting the breadth-ﬁrst search algorithm with an arbitrary
initial cone C0, and in Step b regard only neighboring cones deﬁned by points
which have not changed their sign in the projection onto r ∈ int(C0) yet, only
(cid:98) n+2

2 (cid:99) generations have to be considered.

11

Proof 3 From Theorem 1 (ii), each point may deﬁne a cone’s facet, changing
its sign in projections on all directions of the neighboring cone. If, following
Lemma 1, on each new step only not yet considered points are taken into
account, then in each new generation exactly one point more has its sign on
projection changed (compared to C0). The maximum generation (if C0 is
denoted as 1st generation) is then (n + 1)-th generation.

Each cone has its mirror-copy cone, where projections of X on all di-
rections have exactly opposite signs; these cones need not be considered, of
course. Then, if n is odd, exactly n+1
generations have to be considered, if n
is even, (cid:98) n+1
2
of the ‘equatorial’ (having number (cid:98) n+1
equatorial generation. Thus, at most (cid:98) n+2

2 (cid:99) +1 generations have to be considered, as the mirror-copy cones
2 (cid:99) + 1) generation also belong to the
2 (cid:99) generations have to be regarded.

4 Algorithm

Basically, Algorithm 1 is the application of the breadth-ﬁrst-search algorithm
for searching over the direction cones covering the entire Rd. We will need
some notation. As described above, let br be a binary sequence of length n
where each bit br(i) corresponds to a point xi ∈ {x1, x2, ..., xn} = X with
ir > 0) for any r that maintains strict ordering of x ∈ X in the
br(i) = I(x(cid:48)
projection on it. Also, let b0
i be a zero-ﬁlled binary sequence with the i-th bit
set to “1”, ⊕ denote the binary ‘exclusive disjunction’=“XOR” operation, !

be the bit inversion operator, and(cid:80) br be the number of “1”s in br (Hamming

distance between br and b0).
Algorithm 1 Input: X = {x1, ..., xn} ∈ Rd, d < n, {0} ∪ X in general
position.

1. Initialization: Calculate XHxi

n }, i = 1, ..., n, set
D = n. Draw r0 ∈ Sd−1 yielding a permutation πr0 on N = {1, 2, ..., n}
maintaining strict order x(cid:48)
πr0 (n)r0 and let
br0 be the corresponding binary code. Initialize B = {b1, b2, ..., bn} with
bi = br0 ∀ i = 1, ..., n. Initialize a queue Btopical containing br0 only and
an empty searchable storage Bf uture (e.g., binary tree).

, xHxi
πr0 (2)r0 < ... < x(cid:48)

1

= {xHxi
πr0 (1)r0 < x(cid:48)

, ..., xHxi

2

2. For i = 1 : n do:

(a) For j = 1 : n do:

12

i. If br0(j) = 0 then xHxi

j = −1 · xHxi

j

.

3. For i = 1 : (cid:98) n+2

(a) Pop b = head of Btopical, D = min{D,(cid:80) b, n −(cid:80) b}.

2 (cid:99) do:

(b) If i = (cid:98) n+2
(c) For j = 1 : n do:

2 (cid:99), then go to Step 3d.

If (b ⊕ br0)(j) = 0 then
i. For k = 1 : n do:

If (bj ⊕ b)(k) = 1 then x

ii. If (i) 0 ∈ conv(XHxj
then add (b ⊕ b0

Hxj

k = −1 · x

Hxj
k
\ {0}) and (ii) (b ⊕ b0
j ) to Bf uture.

, bj(k) =!bj(k).
j ) /∈ Bf uture

(d) If Btopical (cid:54)= ∅, then go to Step 3a, else Btopical = Bf uture, Bf uture =

∅.

4. Return: D/n.
Nontrivial is the check of condition (i) in Step 3(c)ii, i.e. whether 0 ∈
\{0}) (xj is projected into 0 in Hxj ; it is excluded). In other words,
conv(XHxj
given a cone C unambiguously deﬁned by the corresponding bj, the linear
separability (through the origin) of X +
Hxj ,C has to be checked, i.e.
whether ∃ r ∈ Sd−1 ∩ Hxj such that r(cid:48)x > 0 ∀ x ∈ X +
Hxj ,C and r(cid:48)x < 0 ∀
x ∈ X−
Hxj ,C. This can be done by means of linear programming as follows.
Let Y be the (n−1)×(d−1) matrix, which rows are the points ∈ (XHxj
\
{0}) for an iteration of Step 3(c)ii of the algorithm. The task from above
narrows down to ﬁnding a feasible solution Λ0 satisfying the constraints:

Hxj ,C and X−

Y(cid:48)Λ = 0d−1,

Λ(cid:48)1n−1 = 1,

Λ ≥ 0n−1,

with Λ = (λ1, ..., λn−1)(cid:48) and 0k (1k) being a vector-column of k zeros (ones).
This is what is done in the ﬁrst phase of the simplex algorithm.
, i = 1, ..., n — projections of X onto zero hyperplanes
normal to data points ∈ X — are cached, an on each following step of the
Algorithm for each i these projections signs of several points have to be

In Step 1 the XHxi

13

changed only, which computationally is a cheap operation. Please note, that
the simplex algorithm is executed in these hyperplanes, i.e.
in dimension
d − 1. This mechanism allows for further caching as well. If on Step 3(c)ii
for some j 0 ∈ conv(XHxj
\{0}), a basis consisting of d points will be found.
If, on the next iteration of the Algorithm, on Step 3(c)i for the same j the
points changing sign do not belong to the previously found basis, clearly
\ {0}) again, an no new execution of the simplex algorithm
is needed. A more complicated caching scheme can be used here, though.
One can see in Step 3, that the outer cycle of the Algorithm is completely
deterministic and is always executed (cid:98) n+2
the exact positioning of the data.

2 (cid:99) iterations only, independent of

0 ∈ conv(XHxj

5 Applications

In this section, we regard two applications of the Tukey depth: principal
component analysis and supervised classiﬁcation.

5.1 Robust PCA

(cid:0)Dmax − D(xi|X)(cid:1)|i = 1, ..., n}, where Dmax = maxx∈Rd D(x|X) and

First consider a problem of robust principal component analysis (PCA). Ma-
jumdar (2015) suggests to apply principal component analysis (PCA) to the
signs of centered X, scaled by a depth-based distance (or ranks). Let X rsgn =
{ xi−c
(cid:107)xi−c(cid:107)
D(·|·) is a depth. For c any suited estimate of center can be taken, e.g.,
the depth median c = arg maxx∈Rd D(x|X). Finding depth median, as well
as its depth Dmax (Chan, 2004), is a computationally nontrivial task. We
obtain X rsgn using the depth-weighted mean c =
(see Donoho
& Gasko, 1992, for the properties of the estimator), and simply set Dmax
equal to the upper bound on depth, which, for the Tukey depth, = 0.5. Let
U SV (cid:48) = SVD(X rsgn) be the standard singular value decomposition (SVD)
of X rsgn, where we are interested in the matrix V of eigenvectors v1, ..., vd
only. These indicate the (orthogonal) directions of the highest variance (in
case this exists), or the axis of density ellipsoids.

(cid:80)n
(cid:80)n
i=1 xiD(xi|X)
i=1 D(xi|X)

We contrast this method based on the exactly computed Tukey depth with
the standard SVD algorithm and with one of the most powerful contemporary
tools for the robust PCA, the ROBPCA method by Hubert et al. (2005), on a

14

1 1

1
1 4
4
1 4 10

. Setting

3-dimensional elliptically symmetric Gaussian and Student-t1 (Cauchy) dis-
tribution with µ = (1, 1, 1)(cid:48) and structure matrix Σ =

n = 100 we repeat the experiment 100 times for each setting. Boxplots of the
inner products of the obtained eigenvectors with the true ones are presented
in Figures 3 and 4 for Gaussian and Student t1 distributions respectively.
(Ideally they all should be equal to one.)

Figure 3: Boxplots of the inner products of obtained eigenvectors with the
true ones for 100 points drawn from the Gaussian elliptically symmetric dis-
tribution centered in µ and scattered as Σ over 100 runs; for the standard
SVD algorithm (left), ROBPCA (middle), and the depth-based PCA (right).

For the Gaussian case, although one can observe that the standard SVD
outperforms the other two methods (price of robustness), the loss of perfor-
mance is not dramatic. When switching to the Cauchy setting though, the
standard method performs plainly wrong: its inner products cover the entire
range between 1 and 0 where the obtained eigenvectors are orthogonal the
the wished ones. While both other methods behave quite well, the depth-
based PCA outperforms the ROBPCA slightly, which can be explained by
the following. The ROBPCA relies much on the Minimum Covariance Deter-
minant (MCD, see Rousseeuw, 1984) which assumes that there is a portion
of data preserving its elliptical shape, while the Tukey depth describes data
geometry in a non-parametric way.

15

v1v2v30.9650.9700.9750.9800.9850.9900.9951.000v1v2v30.950.960.970.980.991.00v1v2v30.920.940.960.981.00Figure 4: Boxplots of the inner products of obtained eigenvectors with the
true ones for 100 points drawn from the Student-t1 (Cauchy) elliptically
symmetric distribution centered in µ and scattered as Σ over 100 runs; for
the standard SVD algorithm (left), ROBPCA (middle), and the depth-based
PCA (right).

5.2 Robust supervised classiﬁcation

Li et al. (2012) introduced a depth-based classiﬁcation principle exploiting
the idea of the depth-vs-depth plot (DD-plot). Given X1 = {x1, ..., xn1} and
X2 = {xn1+1, ..., xn1+n2} two training classes in Rd, the DD-plot is a subset of
the unit square [0, 1]2 X DD = {(D(xi|X1), D(xi|X2))|i = 1, ..., n1 + n2}. Any
classiﬁer can be applied to X DD. We employ the DDα-classiﬁer — a fast
heuristic technique based on the α-procedure (Vasil’ev & Lange, 1998); the
reader is referred to Lange et al. (2014) for the detailed reference. Here we
demonstrate the performance of the DDα-classiﬁer with the exactly com-
puted Tukey depth compared to three standard techniques: Linear Dis-
criminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), and
k-nearest neighbors classiﬁers (KNN) for a pair of elliptically distributed
classes diﬀering in location, scale, shape and prior, a particularly compli-
cated case. Taking Σ from above as a basis, we generate X1 and X2 each
from Gaussian or Student-t1 elliptically symmetric distributions with param-

16

v1v2v30.00.20.40.60.81.0v1v2v30.800.850.900.951.00v1v2v30.800.850.900.951.00eters π1 = 2

3, µ1 = (0, 0, 0)(cid:48), Σ1 = 4 ×

1 1

 , respectively π2 = 1
. We generate in total 200 training and 1000

1
1 4
4
1 4 10

3,

µ2 = (1, 1, 1)(cid:48), Σ2 =

4 1

4
1 1
1
4 1 10

testing points and exclude outsiders (points lying beyond the convex hull of
each of the classes, see Mozharovskyi et al., 2015, for additional information)
when calculating the error rate. Boxplots of the corresponding error rates
over 100 tries are shown in Figure 5.

Figure 5: Boxplots of the error rates over 100 tries for two classes generated
from elliptically symmetric Gaussian (left) and Student-t1 (right) distribu-
tions.

In general the DDα-classiﬁer coupled with the exactly computed Tukey
depth performs well, for the Gaussian distribution its is outperformed by the
QDA only, which is optimal in this setting. For the Student-t1 distribution
it outperforms such a strong competitor as KNN. Regarding the Student-t1
elliptical setting, it is worth to mention that a further study not directly re-
lated to the current presentation and thus skipped here, shows that using the
DDα-classiﬁer with other depths such as projection depth or Mahalanobis
depth with robust estimates of location and scatter, allows to achieve per-
formance similar to this when using the Tukey depth and even better.

17

LDAQDAKNNDDA−TD0.150.250.35LDAQDAKNNDDA−TD0.30.40.50.66 Conclusions

The paper presents an algorithm computing the exact Tukey depth by ﬁnd-
ing a global minimum over a ﬁnite range of variants. The task of computing
the Tukey depth is NP-complete while all separations of X into two subsets
by hyperplanes through z are regarded. The algorithm follows the traditions
of the cone segmentation of a ﬁnite-dimensional space and regards candidate
cones of constant depth according to a ﬁrst-breadth order. It employs the
initial idea of Liu & Zuo (2014a), but identiﬁes a facet using linear program-
ming and exploits the fact that each point ∈ X changes the halfspace only
once during the entire execution of the breadth-ﬁrst search algorithm. Linear
programming is executed in Rd−1 and the found basis can be cached for each
of these n (d − 1)-dimensional projections. Also, binary coding of the cones
does not require computation of their location.

The fact that the task of computation of the Tukey depth is of non-
polynomial time complexity, a number of existing procedures, deterministic
and output-sensitive ones, but also diﬀerent platforms and often absence of
implementation intricate the speed comparison, or even make this unrea-
sonable. Because of this, in the current presentation a possible simulation
study is substituted by application examples. In general, one can expect that
the computation time of exact algorithms grows exponentially with increas-
ing dimension. For this reason, in practical tasks the Tukey depth is often
approximated, see e.g. Dyckerhoﬀ (2004), Cuesta-Albertos & Nieto-Reyes
(2008), or Chen et al. (2013).

Nevertheless exact algorithms are needed to assess the approximating
ones. On the other hand, if the dimension is kept ﬁxed, the time complexity
is polynomial. Considering the task of the supervised classiﬁcation from
Section 2, it is the number of points that increases and not the dimension.
As another example, one can regard multivariate functional halfspace depth
(Claeskens et al., 2014), where even for a (high-dimensional) functional data
set the number of output dimensions can still be quite low, which allows for
exact computation of time marginals for their subsequent integration.

Acknowledgements

Major part of the research has been conducted during the PhD study of the
author at the University of Cologne. The author is grateful to the supervisor

18

of his PhD thesis professor Karl Mosler for his valuable comments on the
earlier version of the paper and to his colleagues professor Rainer Dyckerhoﬀ
and Pavel Bazovkin for their helpful remarks. The author wants to thank
Xiaohui Liu for providing the Matlab source code of the algorithm from the
work by Liu & Zuo (2014a) and for his insightful comments on the earlier
version of the manuscript.

References

Barber C.B., Dobkin, D.P. and Huhdanpaa, H. (1996). The Quickhull

algorithm for convex hulls. Computational Geometry 46 566–573.

Bazovkin, P. and Mosler, K. (2012). An exact algorithm for weighted-
mean trimmed regions in any dimension. Journal of Statistical Software
47.

Bremner, D., Chen, D., Iacono, J., Langerman, S. and Morin, P.
(2008) Output-sensitive algorithms for Tukey depth and related problems.
Statistics and Computing 18 259–266.

Chan, T. M. (2004). An optimal randomized algorithm for maximum Tukey
depth. In: Munro, J. I. (Ed.), Proceedings of the 15th Annual ACM-SIAM
Symposium on Discrete Algorithms, SIAM, New Orleans, 430–436.

Chen, C., Morin, P. and Wagner, U. (2013). Absolute approximation
of Tukey depth: Theory and experiments. Computational Geometry 46
566–573.

Claeskens, G., Hubert, M., Slaets, L. and Vakili K. (2014). Mul-
tivariate functional halfspace depth. Journal of the American Statistical
Association 109 411–423.

Cuesta-Albertos, J. A. and Nieto-Reyes, A. (2008). The random
Tukey depth. Computational Statistics and Data Analysis 52 4979–4988.

Donoho, D.L. and Gasko, M. (1992). Breakdown properties of location
estimates based on halfspace depth and projected outlyingness. The Annals
of Statistics 20 1803–1827.

19

Dyckerhoff, R. (2004). Data depths satisfying the projection property.

AStA - Advances in Statistical Analysis 88 163–190.

Dyckerhoff, R. and Mozharovskyi, P. (2015). Exact computation
of the halfspace depth. Computational Statistics and Data Analysis, to
appear.

Ghosh, A.K. and Chaudhuri, P. (2005). On data depth and distribution

free discriminant analysis using separating surfaces. Bernoulli 11 1–27.

Hallin, M., Paindaveine, D. and ˇSiman, M. (2010). Multivariate quan-
tiles and multiple-output regression quantiles: From L1 optimization to
halfspace depth. The Annals of Statistics 38 635–669.

Hubert, M., Rousseeuw, P. J., and Vanden Branden, K. (2005).
ROBPCA: a new approach to robust principal components analysis. Tech-
nometrics 47 64–79.

Johnson, D.S. and Preparata, F.P. (1978). The densest hemisphere

problem. Theoretical Computer Science 6 93–107.

Koenker, R and Basset, G. (1978). Regression quantiles. Econometrica

46 33–50.

Koshevoy, G.A. (2002). The Tukey depth characterizes the atomic mea-

sure. Journal of Multivariate Analysis 83 360–364.

Lange, T., Mosler, K. and Mozharovskyi, P. (2014). Fast nonpara-

metric classiﬁcation based on data depth. Statistical Papers 55 49–69.

Li, J., Cuesta-Albertos, J. A. and Liu, R. Y. (2012). DD-classiﬁer:
nonparametric classiﬁcation procedure based on DD-plot. Journal of the
American Statistical Association 107 737–753.

Liu, X. and Zuo, Y. (2014a). Computing halfspace depth and regression
depth. Communications in Statistics - Simulation and Computation 43
969–985.

Liu, X. and Zuo, Y. (2014b). Computing projection depth and its associ-

ated estimators. Statistics and Computing 24 51–63.

20

Majumdar, S. (2015). Robust estimation of principal components from
depth-based multivariate rank covariance matrix. arXiv:1502.07042v2
[math.ST].

Mosler, K. (2013). Depth statistics. In: Becker, C., Fried, R., Kuhnt, S.
(eds.), Robustness and Complex Data Structures, Festschrift in Honour of
Ursula Gather, Springer-Verlag, Berlin, 17–34.

Mosler, K., Lange, T. and Bazovkin, P. (2009). Computing zonoid
trimmed regions of dimension d > 2. Computational Statistics and Data
Analysis 53 2500–2510.

Mozharovskyi, P., Mosler, K. and Lange, T. (2015). Classifying
real-world data with the DDα-procedure. Advances in Data Analysis and
Classiﬁcation 9 287–314.

Paindaveine, D. and ˇSiman, M. (2012a). Computing multiple-output
regression quantile regions. Computational Statistics and Data Analysis
56 840–853.

Paindaveine, D. and ˇSiman, M. (2012b). Computing multiple-output re-
gression quantile regions from projection quantiles. Computational Statis-
tics 27 29–49.

Rousseeuw, P. J. (1984). Least median of squares regression. Journal of

the American Statistical Association 79 871–880.

Rousseeuw, P. J. and Hubert, M. (1999). Regression depth. Journal of

the American Statistical Association 94 388–433.

Tukey, J.W. (1975). Mathematics and the picturing of data. Proceeding of

the International Congress of Mathematicians, Vancouver, 523–531.

Vasil’ev, V.I. and Lange, T. (1998). The duality principle in learning for
pattern recognition (in Russian). Kibernetika i Vytschislit’elnaya Technika
121 7–16.

Zuo, Y.J. and Serfling, R. (2000). General notions of statistical depth

function. The Annals of Statistics 28 461–482.

21

