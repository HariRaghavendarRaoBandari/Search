6
1
0
2

 
r
a

 

M
8
1

 
 
]

.

R
P
h
t
a
m

[
 
 

1
v
4
5
9
5
0

.

3
0
6
1
:
v
i
X
r
a

THE STRUCTURE OF COMBINATORIAL MARKOV

PROCESSES

HARRY CRANE AND HENRY TOWSNER

Abstract. Every exchangeable Feller process taking values in a suit-
ably nice combinatorial state space can be constructed by a system of
iterated random Lipschitz functions. In discrete time, the construction
proceeds by iterative application of independent, identically distributed
functions, while in continuous time the random functions occur as the
atoms of a time homogeneous Poisson point process. These observa-
tions endow all exchangeable combinatorial Feller processes with similar
structural features regardless of the dynamics or resident state space.
Our main results generalize prior observations in the special cases of co-
alescent and fragmentation processes on spaces of partitions and graph-
valued processes.

1. Introduction

Many combinatorial Markov processes exhibit common mathematical be-
haviors regardless of the resident state space.
Initial observations come
from the ﬁeld of mathematical population genetics, where Kingman [15]
introduced his coalescent process as a model for the evolution of ancestral
lineages looking backwards in time. The simple dynamics of Kingman’s
coalescent—each pair of lineages merges independently at unit exponential
rate—produce the family of [n]-coalescents, where for each n ≥ 1 the [n]-
coalescent is a Markov process on partitions of [n] := {1, . . . , n}. Each
[n]-coalescent is exchangeable, that is, invariant under relabeling [n] by any
permutation, and together the family of all [n]-coalescents is sampling con-
sistent, that is, the process obtained by removing all elements in [n] \ [m]
from an [n]-coalescent behaves as an [m]-coalescent. Kingman’s coalescent is
the projective limit of these processes to an exchangeable, consistent process
on partitions of N := {1, 2, . . .}.

Since its inception, coalescent theory has played a signiﬁcant role in pop-
ulation genetics and stochastic process theory. Pitman [18] and Schweins-
berg [21] expanded upon Kingman’s coalescent by allowing multiple blocks

Date: March 21, 2016.
H. Crane is partially supported by NSF grant DMS-1308899.
H. Towsner is partially supported by NSF grant DMS-1340666.

1

2

HARRY CRANE AND HENRY TOWSNER

to merge at once. Together, Kingman’s coalescent and multiple merger coa-
lescents describe the behavior of all exchangeable, consistent coalescent pro-
cesses, recalling the representation of Lévy processes [5, Chapter 1] as an in-
dependent superposition of Brownian motion with drift, compound Poisson
process, and pure jump martingale. In the combinatorial setting, however,
the representation is purely Poissonian, providing an explicit construction
for combinatorial Markov processes via a Poisson point process on an appro-
priate space of operators. Analogous decompositions have been shown for
homogeneous fragmentation processes [6] and exchangeable fragmentation-
coalescent processes [4], which combine the dynamics of coalescence and
fragmentation. See [7,19] for many other deep connections between random
partitions and stochastic process theory.

The transitions of coalescent, fragmentation, and fragmentation-coalescent
processes are deﬁned in terms of the coagulation and fragmentation opera-
tors, which determine a class of Lipschitz continuous functions on the space
of partitions. Their descriptions by iterated composition of Lipschitz con-
tinuous functions endow these processes with the Feller property, and ulti-
mately Poissonian structure. In subsequent work, Lévy–Itô structure has
been proven much more generally as a consequence of the properties of
exchangeability and sampling consistency without any further description
of the dynamics.
In this generality, the class of exchangeable, consistent
Markov processes on spaces of partitions with a bounded number of blocks,
so-called cut-and-paste processes [8], and on spaces of graphs and hyper-
graphs [9] have been characterized in the Lévy–Itô sense. These outcomes
show that exchangeable Feller processes on partitions and graphs can al-
ways be generated by a system of iterated random functions, which play an
important role much more broadly in statistics and applied probability prob-
lems on more general state spaces; see [12] for a general survey on iterated
random functions.

Exchangeable combinatorial stochastic processes also play an important
role in Bayesian nonparametrics and hidden Markov modeling of certain dis-
crete structures that arise in applications [10,17]. Given the common behav-
iors exhibited by these processes with diﬀerent dynamics on diﬀerent state
spaces, it is natural to ask the extent to which the observed behavior reﬂects
a universal property of combinatorial Markov processes. This consideration
strips away speciﬁc attributes of the aforementioned processes, for example,
the coalescent and fragmentation-type processes are deﬁned to have speciﬁc
semigroup behavior while the cut-and-paste and graph-valued processes are
deﬁned on state spaces with suﬃciently tractable structure. Our discussion
establishes common elements of combinatorial stochastic processes that obey
minimal regularity conditions, such as exchangeability, càdlàg sample paths,
and the Feller property, but can otherwise evolve on exotic state spaces.
Our main theorems, therefore, encompass the partition- and graph-valued
processes studied previously as well as Markov processes evolving on more
general combinatorial state spaces.

THE STRUCTURE OF COMBINATORIAL MARKOV PROCESSES

3

We prove a generic representation and corresponding properties for all ex-
changeable, consistent combinatorial Markov processes evolving on a Fraïssé
space. The latter term, which we borrow from the model theory literature
on homogeneous structures, captures the property that all elements of the
state space embed into a single universal object, called the Fraïssé limit.
By the Feller property, the dynamics of these processes are determined by
the behavior at the Fraïssé limit, mimicking the structure of Lévy processes,
which are characterized by their behavior at the origin, and eliciting a nice
representation of the inﬁnitesimal jump rates. Each of the examples men-
tioned above evolves on a Fraïssé space, whether the space of set partitions,
k-colorings, graphs, or hypergraphs.

Though our main theorems emphasize universal structural properties among

combinatorial Markov processes, our main discussion also highlights some
key diﬀerences between certain common spaces that arise. The nature of
the representation boils down to the way in which substructures ﬁt together
to form larger structures, a notion we make precise in due course. Ulti-
mately, our main results draw a connection between Fraïsse’s theorem and
countably categorical structures in model theory [13, Chapter 6], the Lévy–
Itô–Khintchine representation for Lévy processes and inﬁnitely divisible dis-
tributions in probability theory [5, Chapter 1], and graph limits and limits
of more general combinatorial structures in combinatorial theory [3, 16].

2. Preliminaries

To establish results in the above generality, we draw on concepts from
model theory and ﬁrst order logic. Understanding that the reader may
come from any one of several backgrounds, we often spell things out more
explicitly than a specialist would require. We avoid specialized terminology
as much as possible.

2.1. Combinatorial structures. A signature is a ﬁnite set of relation
symbols L = {R1, . . . , Rr} together with, for every j = 1, . . . , r, a pos-
itive integer ar(Rj), called the arity of Rj. An L-structure is a collec-
tion M = (S, R1, . . . , Rr), where S is a set and Rj ⊆ Sar(Rj ) for each
:= [r]. For any L-structure M = (S, R1, . . . , Rr), we write
j ∈ [1, r]
dom M := S to denote the domain or universe of M and RM
:= Rj, j ∈ [r],
j
to denote the interpretation of Rj in M.

We write LS to denote the set of L-structures M for which dom M = S.
Speciﬁcally, LN denotes L-structures with dom M = N and L[n] denotes L-
structures with dom M = [n]. Writing |S| to denote the cardinality of a set
S, we call M ﬁnite if |dom M| < ∞. If dom M is countable then we call M
countable and without loss of generality we assume dom M = N.

Example 2.1 (Common examples). The deﬁnition of an L-structure gen-
eralizes many common combinatorial structures, for example, subsets, par-
titions, orderings, directed and undirected graphs, and k-ary hypergraphs.

4

HARRY CRANE AND HENRY TOWSNER

For example, a partition π of S is a collection of nonempty, disjoint subsets

{B1, B2, . . .}, called blocks, satisfying Si≥1 Bi = S. We can represent π as

an L-structure M = (S, R) with L = {R} and ar(R) = 2 by

(i, j) ∈ R ⇐⇒ i and j are in the same block of π.

A directed graph G with vertex set S and edges E ⊆ S × S is also an
{R}-structure with ar(R) = 2. In this case, M = (S, R) has

(i, j) ∈ R ⇐⇒ G has an edge from i to j.

(An undirected graph can be described similarly with the additional symme-
try constraint: (i, j) ∈ R if and only if (j, i) ∈ R.)

Though partitions and graphs have the same signature, there are impor-
tant diﬀerences between the behavior of Markov processes on these state
spaces. It is instructive for the reader to keep these two examples in mind
as we continue the general exposition. We discuss the diﬀerences between
these cases further beginning in Section 4.1.

Every injection φ : S′ → S determines a map LS → LS′, M 7→ Mφ :=

(S′, Rφ

1 , . . . , Rφ

r ), with

(s1, . . . , sar(Rj )) ∈ Rφ

j ⇐⇒ (φ(s1), . . . , φ(sar(Rj ))) ∈ Rj.

In particular, every permutation σ : S → S determines a relabeling of M ∈
LS, and when S′ ⊂ S the inclusion map, s 7→ s, determines the restriction
of M by

M|S′ := (S′, R1 ∩ S′ ar(R1), . . . , Rr ∩ S′ ar(Rr )).

For L-structures N, M, we call φ : dom N → dom M an embedding of N into
M, denoted φ : N → M, if Mφ = N. We write N ⊆ M to denote that N is
an embedded substructure of M, that is, dom N ⊆ dom M and M|dom N = N.
Two L-structures M and N are isomorphic, written M ∼= N, if there is a
bijection φ : dom M → dom N such that Mφ = N and Nφ−1

= M.

We equip LN with the product discrete topology induced by the ultramet-

ric

(1)

d(M, M′) := 1/(1 + sup{n ∈ N : M|[n] = M′|[n]}), M, M′ ∈ LN,

with the convention that 1/∞ = 0. Under this metric, LN is complete,
separable, and compact. We equip LN with the Borel σ-ﬁeld generated by
the restriction maps ·|[n] : LN → L[n], and we write A ⊆Borel LN to denote
that A is a Borel subset.

2.2. Combinatorial Markov processes. We are primarily interested in
stochastic processes on subspaces of L-structures that behave nicely with
respect to the natural actions of relabeling and restriction, as for the coales-
cent and other processes mentioned in Section 1. We focus speciﬁcally on
processes with the Markov property.

THE STRUCTURE OF COMBINATORIAL MARKOV PROCESSES

5

In the following deﬁnition and throughout the paper, L is a signature, S
is an at most countable set, T is either Z+ := {0, 1, . . .} (discrete time) or
R+ := [0, ∞) (continuous time), and XS ⊆ LS is closed under isomorphism.

Deﬁnition 2.2 (Markov property). A family of XS-valued random objects
(Xt)t∈T satisﬁes the Markov property if the σ-algebras σhXsis<t and σhXsis>t
are conditionally independent given σhXti for all t ∈ T , where σh·i is the
σ-algebra generated by ·.

Deﬁnition 2.3 (Combinatorial Markov process). A combinatorial Markov
process on XS is a collection X = {XM : M ∈ XS}, where each XM =
(Xt)t∈T is XS-valued, has initial state X0 = M, and satisﬁes the Markov
property with a common time homogeneous transition law

(2)

Ps(x, ·) := P{Xt+s ∈ · | Xt = x, X0 = M},

s, t ∈ T,

for all M ∈ XS.

Remark 2.4 (Notation and terminology). Since combinatorial Markov pro-
cesses exhibit diﬀerent behaviors in discrete and continuous time, we avoid
confusion by calling X a Markov chain when time is discrete and a Markov
process when time is continuous. To further distinguish these cases, we in-
dex time by m = 0, 1, . . . in discrete time and t ∈ [0, ∞) in continuous time.
When speaking generically about discrete and continuous time processes,
as we do in this section, we employ the notation and terminology of the
continuous time case.

Deﬁnition 2.5 (Exchangeable processes). A combinatorial Markov process
X = {XM : M ∈ XS} is exchangeable if its transition law (2) satisﬁes

x ∈ XS, A ⊆Borel XS,
(3)
for all permutations σ : S → S, where Aσ := {x′σ : x′ ∈ A}.

Ps(x, A) = Ps(xσ, Aσ),

When S is countable, without loss of generality S = N, we assume the
further minimal condition that every XM ∈ X has càdlàg sample paths with
probability 1, that is, the map t 7→ Xt is right continuous and has left
limits. In the product discrete topology, the càdlàg sample paths property
is equivalent to the condition that every ﬁnitely embedded process XS
M :=
(Xt|S)t∈T stays in each state it visits for a strictly positive hold time with
probability 1. When S is ﬁnite or T = Z+, the càdlàg paths assumption is
implicit in the Markov assumption of Deﬁnition 2.3.

The stronger property of projectivity, or sampling consistency, is a com-

mon assumption in statistical applications.

Deﬁnition 2.6 (Projective Markov property). We say that X = {XM :
M ∈ XS} exhibits the projective Markov property, or is consistent under
subsampling, if XS′
M := (Xt|S′)t∈T satisﬁes the Markov property on XS′ for
every S′ ⊆ S and M ∈ XS. In particular, X determines a combinatorial
Markov process on XS′ for every S′ ⊆ S.

6

HARRY CRANE AND HENRY TOWSNER

Since X[n] is ﬁnite for every n ∈ N, the projective Markov property im-
plies that each XM has càdlàg sample paths with probability 1. Our main
theorems pertain to processes that evolve on a Fraïssé space XN ⊆ LN, which
we deﬁne formally in Deﬁnition 4.12. The key feature of a Fraïssé space is
that it contains a universal representative F ∈ XN such that every M ∈ XN
embeds into F.
In Proposition 4.22 we prove that the projective Markov
property is equivalent to the Feller property for exchangeable processes on
Fraïssé spaces. From here on we refer to X interchangeably as an exchange-
able, projective Markov process, an exchangeable, consistent Markov process,
or an exchangeable Feller process.

The Markov semigroup of an XS-valued Markov process X = (Xt)t∈T
is a family of operators (Pt)t∈T which is deﬁned for bounded, measurable
functions g : XS → R by

(4)

Ptg(M) := E(g(Xt) | X0 = M), M ∈ XS .

Deﬁnition 2.7 (Feller property). An XS-valued Markov process X = {XM :
M ∈ XS} has the Feller property if for all bounded, measurable g : XS → R
its semigroup (Pt)t∈T satisﬁes

• M 7→ Ptg(M) is continuous for all t ∈ T and
• limt↓0 Ptg(M) = g(M) for all M ∈ XS.

Most important for our purposes is that the inﬁnitesimal generator, which

determines the behavior of X, always exists for Feller processes.

2.3. Lipschitz continuous functions. A map F : XS → XS is Lipschitz
continuous if

(5)

d(F (M), F (M′)) ≤ d(M, M′),

for all M, M′ ∈ XS,

where d(·, ·) is the ultrametric from (1). For any S ⊆ N, we write Lip(XS)
to denote the set of Lipschitz continuous functions XS → XS.

Remark 2.8. The Lipschitz condition is natural in the context of projective
Markov processes. In our characterization of discrete time Markov chains,
Lipschitz continuous functions act as random transition operators which
relate the state Xt = M at time t to the state Xt+1 = F (M) at time t + 1.
The ultrametric property of (1) implies that F (M)|[n] depends only on M|[n],
for every F ∈ Lip(XN), so that the Lipschitz continuous functions are those
which can determine the restriction to X[n] at time t + 1 by looking only at
the restriction to X[n] at time t.

Given F ∈ Lip(XN), we deﬁne the restriction F [n] ∈ Lip(X[n]) by setting

(6)

F [n](S) := F (M)|[n], S ∈ X[n],

where M ∈ XN is any structure such that M|[n] = S. Thus, Lip(XN) also
comes equipped with the product discrete topology induced by the analog
of (1):

d(F, F ′) := 1/(1 + sup{n ∈ N : F [n] = F ′[n]}), F, F ′ ∈ Lip(XN).

THE STRUCTURE OF COMBINATORIAL MARKOV PROCESSES

7

We equip Lip(XN) with its Borel σ-ﬁeld.

Since we have assumed XS is closed under isomorphism, any permutation
σ : S → S acts on F ∈ Lip(XS) by conjugation, F 7→ σF σ−1, deﬁned by
(σF σ−1)(M) := F (Mσ)σ−1
. We call F ∈ Lip(XS) conjugation invariant if
σF σ−1 is Lipschitz continuous for all permutations σ : S → S. We call a
measure µ on Lip(XN) exchangeable if

(7)

µ({F ∈ Lip(XN) : F [n] ∈ A}) = µ({F ∈ Lip(XN) : σF [n]σ−1 ∈ A})

for all A ⊆Borel Lip(X[n]) and all permutations σ : [n] → [n], for all n ∈
N. Speciﬁcally, a probability measure µ is exchangeable if F ∼ µ implies
F (Mσ) =D F (M)σ for all permutations σ : N → N and all M ∈ XN, where
=D denotes equality in distribution.

Remark 2.9. Conjugation invariance is a natural strengthening of the Lip-
schitz condition, saying that F (M)|S depends only on M|S for any subset
S, whereas Lipschitz continuity only ensures this when S = [n]. This is the
strongest reasonable requirement we could place on our transition functions
to ensure that they are determined “locally.”

2.4. Notation. We adopt the following notational conventions: L and L′
always denote signatures, M is always an L-structure, and X is always a
random structure. In general, we use fraktur letters, M, N, S, T, to denote
structures with the base set indicated by plain Roman letters, M , N , S, T ,
respectively. When no confusion will result, we often write ~s ∈ S to denote
that ~s is a tuple in S without specifying its length.

Capital letters at the end of the alphabet X, Y, Z denote random struc-
tures and X = (Xt)t∈T denotes a family of random structures. We write
X =D X∗ to mean that X and X∗ are equal in distribution, and for pro-
cesses we write X =D X∗ to mean that X and X∗ have all the same ﬁnite-
dimensional distributions. For ~x = (x1, . . . , xk), we write rng ~x = {x1, . . . , xk}
for the set of distinct elements in ~x and we write ~y ⊑ ~x to indicate that ~y
occurs as a subsequence of ~x, that is, there are indices 1 ≤ i1 < · · · < im ≤ k
such that ~y = (xi1, . . . , xim ).

2.5. Outline. We organize the rest of the paper as follows. We summarize
our most general theorems in Section 3. In Sections 4 and 5, we collect the
necessary background on combinatorial state spaces and relatively exchange-
able structures. In Section 6 we prove Theorem 3.1 and other theorems for
discrete time Markov chains. In Section 7 we prove Theorem 3.2 for contin-
uous time processes.

3. Summary of main theorems

We now state two of our main theorems, saving many technical details for
later. Throughout the section, L is a ﬁxed signature and XN ⊆ LN is closed
under isomorphism.

8

HARRY CRANE AND HENRY TOWSNER

3.1. Discrete time Markov chains. Given any probability measure µ on
Lip(XN), we construct the standard µ-process X∗
M,µ : M ∈ XN} on
XN by taking F1, F2, . . . independent, identically distributed (i.i.d.) from µ
and deﬁning X∗

µ := {X∗

m)m∈Z+ by X∗

0 = M and

M,µ := (X∗
m+1 = Fm+1(X∗

X∗

(8)

m) = (Fm+1 ◦ Fm ◦ · · · ◦ F1)(M), m ≥ 0.

Theorem 3.1. Let X be a discrete time, exchangeable Markov chain on a
Fraïssé space XN. Then X has the Feller property if and only if there exists
an exchangeable probability measure µ on Lip(XN) such that X =D X∗
µ, where
X∗

µ is the standard µ-process constructed in (8).

If XN satisﬁes a stronger model theoretic property called n-disjoint amal-
gamation (Deﬁnition 4.13), we can specify µ in Theorem 3.1 to concentrate
on conjugation invariant functions XN → XN. In this case, we obtain a more
explicit description of the measure µ in Theorem 3.1, which we describe
further in Theorem 6.3.

3.2. Continuous time Markov processes. Theorem 3.1 serves as a pre-
cursor to our main theorems about continuous time processes, but the con-
tinuous time case does not follow from the discrete time results above. More
subtle behaviors ensue because of the possibility that countably many small
jumps bunch together in arbitrarily small time intervals.

In this direction, let Λ be a measure on Lip(XN) such that

(9)

Λ({idN}) = 0

and Λ({F ∈ Lip(XN) : F [n] 6= id[n]}) < ∞ for all n ∈ N,

:= {X

Λ := {X∗

∗[n]

S,Λ = (X∗[n]

t

∗[n]
S,Λ : S ∈ X[n]}, where each X

t− ), if t is an atom time of Φ with F [n]

0 = S,
t = F [n]
t = X∗[n]
t− := lims↑t X∗[n]

where idS denotes the identity XS → XS. We deﬁne the standard Λ-process
X∗
M,Λ : M ∈ XN} as follows. Let Φ := {(t, Ft)} ⊆ [0, ∞) × Lip(XN)
be a Poisson point process with intensity dt ⊗ Λ, where dt denotes Lebesgue
measure on [0, ∞). We build each X∗
M,Λ as the projective limit of its ﬁnite
)t∈[0,∞) has

∗[n]
processes X
Λ
• X∗[n]
• X∗[n]
• X∗[n]
∗[n]
where X∗[n]
S,Λ just before time t. For every
∗[n]
n ∈ N, X
S,Λ is Markovian since the righthand side of (9) implies only
ﬁnitely many jumps occur in bounded intervals with probability 1. By the
∗[n]
Λ from the same Poisson process on Lipschitz
construction of X
∗[n+1]
S′,Λ to
continuous functions, it is immediate that the restriction of each X
X[n] coincides exactly with X∗[n]
S′|[n],Λ, so that there is a well deﬁned projective
limit process X∗

t
t− otherwise,

is the state of X

6= id[n], and

t

∗[n+1]
Λ

and X

(X∗[n]

s

Λ on XN.

THE STRUCTURE OF COMBINATORIAL MARKOV PROCESSES

9

Theorem 3.2. Let X be a continuous time, exchangeable Markov process on
a Fraïssé space XN. Then X has the Feller property if and only if there exists
an exchangeable measure Λ on Lip(XN) satisfying (9) such that X =D X∗
Λ.
Just as in discrete time, when XN also satisﬁes the n-disjoint amalgama-
tion property for all n ≥ 1, we can choose Λ to concentrate on the subspace
of conjugation invariant functions on XN. Since this outcome requires more
technical notation and deﬁnitions, we delay its formal statement until later.
The next two sections prepare the key elements of model theory and ex-
changeable random structures needed to prove our main theorems.

4. Fraïssé spaces

4.1. Model theoretic properties. The connection to the Feller property
in Theorems 3.1 and 3.2 indicates that the components {XM : M ∈ XN}
of the processes we study ﬁt together to produce a jointly continuous ﬂow
(M, t) 7→ XM,t, where here we write XM,t to denote the state of XM at time t.
(We usually suppress the dependence on M when no confusion will result.)
It seems reasonable to expect that the structure of such a process depends
on the structure of the state space, which we characterize in terms of model
theoretic and ﬁrst-order logical properties.

The projective Markov property facilitates the study of X through its
restriction to ﬁnite substructures. In this direction, we identify a structure
M = (N, R1, . . . , Rr) with its collection of ﬁnite substructures, called the
age.

Deﬁnition 4.1 (Age of a structure). The age of M, denoted age(M), is the
set of all ﬁnite L-structures embedded in M, that is,

age(M) := {S ∈ [n∈N

L[n] : there exists an embedding φ : S → M}.

Remark 4.2. The age of a structure is usually deﬁned as the set of all
ﬁnite substructures of M, perhaps identiﬁed if they are isomorphic. For
our purposes, it is more convenient to specify the domain of structures with
cardinality n to be precisely [n] and to regard S, S′ ∈ age(M) as diﬀerent
elements even when they are isomorphic.

Example 4.3. The partition 0N := {{1}, {2}, . . .} of N into singletons has
age(0N) = {0[n] : n ∈ N} consisting only of partitions of ﬁnite sets into
singletons. The age of a partition π = {B1, . . . , Bk} of N for which each
block B1, . . . , Bk is inﬁnite consists of all ﬁnite partitions with at most k
blocks. The countable universal partition FΠ = {B1, B2, . . .} has inﬁnitely
many blocks of inﬁnite size so that every ﬁnite partition embeds into FΠ
and age(FΠ) contains all ﬁnite partitions.

We extend our deﬁnition of the age of a structure to the entire space XN by

age(XN) := SM∈XN age(M). With agen(M) := age(M) ∩ X[n] denoting those

elements of age(M) of size n, and likewise for agen(XN), we immediately

10

HARRY CRANE AND HENRY TOWSNER

see that agen(XN) = X[n] is the state space for the process X restricted to
X[n]. We are primarily interested in those spaces XN ⊆ LN which contain a
universal object.

Deﬁnition 4.4 (Hereditary property). A collection of ﬁnite structures K
has the hereditary property (HP) if S ∈ K and T ⊆ S implies T ∈ K. In
this case, we say that K is closed under substructures.

Deﬁnition 4.5 (Joint embedding property). A collection of ﬁnite structures
K has the joint embedding property (JEP) if for all S, T ∈ K, there exists
U ∈ K such that S and T both embed into U.

Deﬁnition 4.6 (Disjoint amalgamation property). A collection of ﬁnite
structures K has the disjoint amalgamation property (DAP) if for any S, T, T′ ∈
K and embeddings φ : S → T and φ′ : S → T′ there exist U ∈ K and em-
beddings ψ : T → U and ψ′ : T′ → U such that ψ ◦φ = ψ′ ◦φ′ and im(ψ ◦φ) =
im(ψ) ∩ im(ψ′), where im(φ) = {t ∈ dom T : ∃s ∈ dom S (φ(s) = t)} is the
image of φ.

Disjoint amalgamation implies that any two structures T, T′ can be em-
bedded into a larger structure without identifying any elements that are not
already identiﬁed.

Example 4.7. As an example of a family for which DAP fails, let L =
{R} have ar(R) = 1 and let K consist of all structures ([n], R), n ≥ 1,
such that R ⊆ [n] is either a singleton or empty. Let l > m > n ≥ 1,
S = ([n], ∅), T = ([m], {j}), and T′ = ([l], {j′}) for j ∈ [m] and j′ ∈ [l].
Any U satisfying the conditions of Deﬁnition 4.6 must have T and T′ as
embedded substructures; thus, for some k ≥ l we must have U = ([k], {j′′}),
j′′ ∈ [k]. In this case, ψ : T → U and ψ′ : T′ → U must have ψ(j) = j′′
and ψ(j′) = j′′. Any embeddings φ : S → T and φ : S → T′ must have
im(φ) ∩ {j} = ∅ = im(φ′) ∩ {j′}. However, im(ψ ◦ φ) ∩ {j′′} = ∅ and
im(ψ) ∩ im(ψ′) ⊆ {j′′} prevents im(ψ ◦ φ) = im(ψ) ∩ im(ψ′), in violation of
DAP.

Though abstract in appearance, the hereditary, joint embedding, and dis-
joint amalgamation properties capture key features of combinatorial Markov
processes. Let X = {XM : M ∈ XN} be a process indexed by countable
structures. By the consistency property, we can alternatively regard X as

a collection of ﬁnite state space Markov processes {XS : S ∈ Sn∈N X[n]},

akin to the age of a structure deﬁned above, so that for every substructure
S′ ⊆ S the restriction XS|dom S′ =D XS′. The joint embedding property
reﬂects our assumption that no two processes deﬁned on structures with
starting states S ∈ X[n] and S′ ∈ X[n′], n, n′ ∈ N, are mutually exclusive,
that is, there are T ∈ age(XN) and embeddings φ : S → T and φ′ : S′ → T
such that Xφ
T =D XS′. The disjoint amalgamation property
ensures that any two processes XT and XT′ for which there are embeddings
φ : S → T and φ′ : S → T′ can both be embedded into a process XU by

T =D XS and Xφ′

THE STRUCTURE OF COMBINATORIAL MARKOV PROCESSES

11

ψ : T → U and ψ′ : T′ → U in such a way that the behaviors of Xψ
and Xψ′
distribution of Xψ

U =D XT′ are coupled so that Xψ◦φ

U = Xψ′◦φ′
.

U =D XT
but the conditional

We demonstrate each of these properties in the following example.

U given Xψ′

U depends only on Xψ◦φ

U

U

Example 4.8. Consider the space GN of countable undirected graphs. The
hereditary property is plain since age(GN) contains all ﬁnite subgraphs. For
the joint embedding property, we can embed any G ∈ G[m] and G′ ∈ G[m′]
into a common graph G′′ ∼= G ∪ G′ ∈ G[m+m′] by putting G′′|[m] = G
∼= G′ with no edges between the vertex sets [m] and
and G′′|[m+m′]\[m]
[m + m′] \ [m]. For disjoint amalgamation, suppose G embeds into both H
and H′. Then we can embed both H and H′ into a larger structure H′′ by
performing a similar operation as for joint embedding above but with the
modiﬁcation that the piece of H and H′ corresponding to G is the same part
of H′′.

Collections of structures satisfying HP, JEP, and DAP are exactly those
which can be embedded into a single structure with a nice homogeneity
property.

Deﬁnition 4.9 (Ultrahomogeneity). An L-structure M is ultrahomogeneous
if every embedding φ : M|S → M, with S ⊆ dom M ﬁnite, extends to an
automorphism φ : M → M.

Theorem 4.10 (Fraïssé’s theorem, [13], Theorem 6.1.2). Let L be a signa-
ture and let K be a nonempty collection of ﬁnite L-structures that has HP,
JEP, and DAP. Then there exists an L-structure F that is ultrahomogeneous,
unique up to isomorphism, and has age(F) = K. Furthermore, if N is any
countable structure with age(N) ⊆ K, then there is an embedding of N into
F. If K is countable, then dom F is countable and can be taken as N.

We call any set K that satisﬁes HP, JEP, and DAP a Fraïssé class and
the unique (up to isomorphism) structure F from Theorem 4.10 the Fraïssé
limit of K.

Example 4.11. Let GN be the space of countable undirected graphs. By
Example 4.8, age(GN) satisﬁes HP, JEP, and DAP and, by Theorem 4.10,
there exists a unique (up to isomorphism) ultrahomogeneous graph whose
age coincides with age(GN). This countable universal graph is a quintessen-
tial example of a Fraïssé limit and is called the Rado graph [20], which we
denote by FΓ.

Our main theorems beneﬁt from the fact that Fraïssé limits can be treated
as exchangeable random structures generated by a probability measure con-
centrated on its isomorphism class. In the case of the Rado graph, the family
of Erdős–Rényi measures with parameter p ∈ (0, 1) produces an exchange-
able copy of the Rado graph with probability 1. To see this, take p = 1/2 so
that every edge is present independently with probability 1/2. The Erdős–
Rényi measure assigns probability 2−(n
2) > 0 to every S ∈ G[n], n ≥ 1. The

12

HARRY CRANE AND HENRY TOWSNER

Borel–Cantelli lemma implies that every S embeds into the random graph
with probability 1. Since there are countably many ﬁnite graphs, this holds

for all S ∈Sn∈N G[n] and the Erdős–Rényi graph is isomorphic to the Rado

graph with probability 1. We revisit this notion in a more general context
surrounding Theorem 5.7.

Deﬁnition 4.12 (Fraïssé space). We call XN ⊆ LN a Fraïssé space if age(XN)
satisﬁes HP, JEP, and DAP.

The signiﬁcance of Theorem 4.10 is easily illustrated in the special cases
of partitions and graphs. Let PN and GN be the sets of all partitions of N
and undirected graphs with vertex set N, respectively. Both age(PN) and
age(GN) satisfy HP, JEP, and DAP. By Theorem 4.10, PN and GN can be
represented by structures FΠ and FΓ, respectively, into which every element
of the state space embeds. This allows us to study the transition behavior
of Feller processes on these spaces by considering the transition behavior
out of their respective Fraïssé limits. We stress, however, that the mere
existence of the Fraïssé limit is not enough to characterize exchangeable
processes as we do in Theorems 3.1 and 3.2. As we alluded in Example 4.11,
we also need the ability to generate an exchangeable version of the Fraïssé
limit. The space of countable graphs GN satisﬁes a stronger amalgamation
property than PN, which permits a more precise description of processes on
these spaces.

Deﬁnition 4.13 (n-DAP). Let K be a collection of ﬁnite structures that
is closed under isomorphism. For n ≥ 1, we say that K satisﬁes the n-
disjoint amaglamation property (n-DAP) if for every collection (Si)1≤i≤n
of structures with Si ∈ K, dom Si = [n] \ {i} for each 1 ≤ i ≤ n, and
Si|[n]\{i,j} = Sj|[n]\{i,j} for all 1 ≤ i, j ≤ n there exists S ∈ K with dom S =
[n] such that S|[n]\{i} = Si for every 1 ≤ i ≤ n.

Under n-disjoint amalgamation, if we specify a structure on each proper
subset of [n] in a way that is pairwise compatible, then we can unify these
structures into a single structure on all of [n]. By slight abuse of terminology,
if K is a collection of ﬁnite structures not closed under isomorphism, we say
K has n-DAP if its closure under isomorphism has n-DAP.

We realize the probabilistic signiﬁcance of n-DAP by considering condi-
tional distributions of a random substructure Xt+s|S given Xt, where (Xt)t≥0
is an exchangeable combinatorial Feller process. Under n-DAP and ex-
changeability, the conditional distribution of Xt+s|S given Xt depends only
on Xt|S, that is, the dependence is localized to the corresponding substruc-
ture of Xt. This is not true if n-DAP fails for some n ≥ 1.

The most interesting applications of our results are to symmetric struc-
tures, including partitions and undirected graphs. Restricting to this case
simpliﬁes some of the statements.

THE STRUCTURE OF COMBINATORIAL MARKOV PROCESSES

13

Deﬁnition 4.14. An L-structure M = (S, RM
each of its relations RM
j

is symmetric, that is,

1 , . . . , RM

r ) is symmetric if

(x1, . . . , xar(Rj )) ∈ RM

j ⇐⇒ (xσ(1), . . . , xσ(ar(Rj ))) ∈ RM

j

for all permutations σ : [ar(Rj)] → [ar(Rj)], for every j = 1, . . . , r. A
combinatorial state space XS is symmetric if every M ∈ XS is symmetric.

Lemma 4.15. The Fraïssé limit of a Fraïssé class K is symmetric if and
only if every S ∈ K is symmetric.

4.2. Examples. We now show several examples of Fraïssé classes and their
limits.

Example 4.16. Consider a signature L = {R, B} with ar(R) = ar(B) = 1.
Here we may view an L-structure as a 4-coloring, where the four colors are
represented by each of the four possible combinations of R and B.

If K is the collection of all ﬁnite L-structures, its Fraïssé limit is simply
the countably inﬁnite structure in which there are inﬁnitely many elements
of each color. This example is symmetric and has n-DAP for all n.

More generally, one can represent a k-coloring for any ﬁxed k ≥ 1 by
taking L = {R1, . . . , Rk} with ar(Ri) = 1, 1 ≤ i ≤ n, and requiring of each
M that x ∈ RM

i holds of exactly one i = 1, . . . , k, for every x ∈ dom M.

Example 4.17. Consider a signature L = {R} with ar(R) = 2. Let Kd be
the collection of all ﬁnite irreﬂexive L-structures. We may view Kd as the
collection of ﬁnite directed graphs: for S ∈ Kd and a, b ∈ dom S,
(a, b) ∈ RS if and only if S had an edge from a to b.

The Fraïssé limit of Kd is the universal directed graph, which has a natu-
ral random representation by taking a countable set of points and letting
each directed edge be present independently with probability 1/2 (or any
probability p ∈ (0, 1)).

A natural subset of Kd is Kg, the collection of ﬁnite irreﬂexive symmetric
L-structures corresponding to the set of ﬁnite undirected graphs. The Fraïssé
limit is the Rado graph discussed in Example 4.11 above.

Another subset of Kd is Kt, the collection of ﬁnite L-structures S in
which exactly one of the pairs (i, j) and (j, i) is present in RS for each pair
i, j with i 6= j. The structures in Kt are called ﬁnite tournaments. The
Fraïssé limit is the universal tournament, which can be randomly generated
by independently selecting an orientation i → j or j → i for each pair i, j.
Clearly Kg is symmetric but Kd, Kt are not. All three have n-DAP for

all n.

Example 4.18. Consider again the signature with L = {R} and ar(R) =
2. Let Kp be the collection of all ﬁnite structures S in which RS is an
equivalence relation. The Fraïssé limit is an inﬁnite structure with inﬁnitely
many inﬁnite equivalence classes.

14

HARRY CRANE AND HENRY TOWSNER

A subset of Kp is Kp,2, the collection of all structures S in which RS is
an equivalence relation with at most 2 equivalence classes. The Fraïssé limit
is an inﬁnite structure with two inﬁnite equivalence classes.

Kp and Kp,2 are symmetric but do not have 3-DAP: take S1 to be the
structure with equivalence classes {2} and {3}, S2 to be the structure with
equivalence class {1, 3}, and S3 to be the structure with equivalence class
{1, 2}. Then the result of taking the union of these three structures fails to
be an equivalence relation.

Example 4.19. Consider the signature with L = {S} and ar(S) = n. Let
K be the collection of all ﬁnite irreﬂexive symmetric L-structures, that is,
structures S for which (a1, . . . , an) ∈ SS implies that a1, . . . , an are pairwise
distinct and (aσ(1), . . . , aσ(n)) ∈ SS for every permutation σ : [n] → [n].
Then K is the collection of ﬁnite n-ary undirected hypergraphs, which are
symmetric and have n-DAP for all n. (Undirectedness is ensured by the
closure under permutations and n-arity by the fact that a1, . . . , an must be
pairwise distinct.)

The Fraïssé limit is the natural generalization of the Rado graph to a
hypergraph, a countable n-ary hypergraph in which every ﬁnite hypergraph
appears, which can be produced randomly by choosing a hypergraph on N
for which every hyperedge is present independently with probabiltiy 1/2.

Example 4.20. Consider the signature with L = {S} and ar(S) = 4. Let
K be the collection of all ﬁnite structures S such that

• for all i, j ∈ dom S, (i, j, i, j) ∈ SS,
• for all i, j, i′, j′ ∈ dom S, if (i, j, i′, j′) ∈ SS then (i′, j′, i, j) ∈ SS,
• for all i, j, i′, j′, i′′, j′′ ∈ dom S, if (i, j, i′, j′) ∈ SS and (i′, j′, i′′, j′′) ∈

SS then (i, j, i′′, j′′) ∈ SS.

That is, SS is an equivalence relation on pairs from dom S.

The Fraïssé limit is an inﬁnite structure with an equivalence relation on
pairs with inﬁnitely many inﬁnite equivalence relations. There are additional
universality properties. For instance, for any ﬁnite list of equivalence classes
C1, . . . , Cd, there is an i and values j1, . . . , jd such that (i, jn) ∈ Cn for each
n ≤ d.

This example fails to have n-DAP for n = 3, 4, 5.

Example 4.21. Consider the signature with L = {P, R}, ar(P ) = 1, and
ar(R) = 2. Let K be the collection of all ﬁnite structures S such that RS is
irreﬂexive and symmetric, that is, K consists of graphs with a 2-coloring on
the vertices. The Fraïssé limit is the Rado graph together with a 2-coloring
of the vertices such that every possible colored ﬁnite graph appears as a
subgraph. This can be generated by labeling each of the vertices in an
Erdős–Rényi graph independently and uniformly in {0, 1}. This example is
symmetric and has n-DAP.

THE STRUCTURE OF COMBINATORIAL MARKOV PROCESSES

15

Further elaborations include all manner of combinations, including edge
colored graphs and hypergraphs, graphs with vertex partitions, hypergraphs
with edge partitions, and so on.

4.3. Equivalence of Feller and projective Markov properties. We
have the following equivalence.

Proposition 4.22. Let X = {XM : M ∈ XN} be an exchangeable Markov
process on a Fraïssé space XN. Then the following are equivalent.

(i) X has the projective Markov property.
(ii) X has the Feller property.

Proof. Below we write XN ⊆ LN to denote the state space of a Markov
process X and X[n] := {M|[n] : M ∈ XN} ⊆ L[n] to denote the state space
of X[n], the restriction of X to a process on [n]-labeled structures, for each
n ∈ N.

(i) ⇒ (ii). Consider the space
C(XN) := {g : XN → R : ∃n ∈ N such that M|[n] = M′|[n] =⇒ g(M) = g(M′)}.
By the Stone–Weierstrass theorem for compact Hausdorﬀ spaces, C(XN) is
dense in the space of bounded, continuous functions, and it suﬃces to prove
the Feller property for g ∈ C(XN).

Take any g ∈ C(XN) and let n ∈ N be such that g(M) only depends on
M|[n]. We can, therefore, deﬁne g′ : X[n] → R so that g(M) = g′(M|[n]) for
every M ∈ XN. Under the topology induced by (1), all points in X[n] are
isolated, making g′ is continuous by default.

Suppose X0 = M. By the projective property, X

[n]
M is a Markov process
with initial state M|[n]. As X[n] is a ﬁnite state space, X[n]
M must have a
strictly positive hold time in its initial state with probability 1. In partic-
ular, Xt|[n] →P X0|[n] = M|[n] as t ↓ 0, where →P denotes convergence in
probability. By the projective Markov property, continuity of g, and the
bounded convergence theorem, we observe

lim
t↓0

Ptg(M) = lim
t↓0

E(g(Xt) | X0 = M)

= E(lim
t↓0

g′(Xt|[n]) | X0|[n] = M|[n])

Xt|[n]) | X0|[n] = M|[n])

= E(g′(lim
t↓0
= g′(M|[n])
= g(M),

establishing the ﬁrst part of the Feller property.

For the second part, we must show that M 7→ Ptg(M) is continuous for
all t > 0. Let g ∈ C(XN) and (Mn)n∈N be a sequence in XN that converges
to M ∈ XN under the product discrete topology. Then, for every k ∈ N,
there is an Nk ∈ N such that Mn|[k] = M|[k] for all n ≥ Nk. In particular,

16

HARRY CRANE AND HENRY TOWSNER

we can choose k so that g(Mn) = g(M) for all n ≥ Nk. Continuity follows
by continuity of g and the bounded convergence theorem.
(cid:3)

(ii) ⇒ (i). For ﬁnite S ⊂ N and S ∈ XS, we deﬁne ψS : XN → {0, 1} by

ψS(M) := 1{M|S = S}, M ∈ XN,

which is bounded and continuous. To establish the projective property, we
must prove that

P{Xt|S = S | X0 = M} = P{Xt|S = S | X0|S = M|S}

for every t ≥ 0, S ∈ XS, M ∈ XN, and S ⊂ N, which amounts to showing
that

(10)
for all M, M′ ∈ XN for which M|S = M′|S.

PtψS(M) = PtψS(M′)

It is enough to establish (10) on a dense subset of {M′ ∈ XN : M′|S =
M|S}. By exchangeability, PtψS(M) = PtψS(Mσ) for every σ : N → N
that coincides with the identity S → S. To obtain (10), we let F be a
Fraïssé limit of XN with F|S = M|S. By ultrahomogeneity and universality
of F,

{Fσ : σ : N → N coincides with identity S → S}

is dense in {M′ ∈ XN : M′|S = M|S}. Since PtψS(M) is invariant with
respect to permutations that ﬁx S, we have (10) on a dense subset of {M′ ∈
XN : M′|S = M|S}. Continuity of Pt implies that (10) must hold on the
closure of this set and, therefore, can only depend on M|S .

By the second part of the Feller property, we must have PtψS(M) →
ψS(M) as t ↓ 0, proving that XM has càdlàg sample paths and the ﬁnite
restrictions XS
(cid:3)

M are consistent for every ﬁnite subset S ⊂ N.

(cid:3)

5. Relative exchangeability

We now present key details about relative exchangeability.

In [11], we
studied the generalization of exchangeability to structures whose distribu-
tions are invariant with respect to the symmetries of another structure; see
also [1] for related work in a slightly diﬀerent setting. Vis-à-vis Deﬁnition
2.5, the following notion of relative exchangeability occurs naturally in our
study of exchangeable combinatorial Feller processes.

Deﬁnition 5.1 (Relative exchangeability). Let L, L′ be signatures with
M ∈ LN. A countable random L′-structure X is relatively exchangeable with
respect to M, alternatively M-exchangeable or exchangeable relative to M, if
X|φ
T = M|S.

T =D X|S for every φ : S → T , with S ⊆ N ﬁnite, such that M|φ
The projective Markov and exchangeability properties of any exchange-
able combinatorial Feller process X = {XM : M ∈ XN} imply that Xs+u

THE STRUCTURE OF COMBINATORIAL MARKOV PROCESSES

17

is relatively exchangeable with respect to Xs for all s, u ≥ 0, for every
XM = (Xt)t≥0.

Proposition 5.2. Let X be an exchangeable Feller process on XN. For
every M ∈ XN, the conditional distribution of Xt+s in XM = (Xt)t≥0, given
Xt = N, is relatively exchangeable with respect to N for all s, t ≥ 0.

Proof. For s, t ≥ 0, let Pt(N, ·) = P{Xt+s ∈ · | Xs = N} be the t-step transi-
tion probability measure for X. By (2.5), it is clear that X ∼ Pt(N, ·) satisﬁes
X =D Xσ for all automorphisms σ : N → N. Furthermore, the projectivity
assumption implies that X|[n] depends only on N|[n]. The combination of
these properties implies relative exchangeability of X with respect to N. In
XM, the Markov property and (2) imply that Xt+s is relatively exchangeable
with respect to N on the event Xs = N, for all s, t ≥ 0.
(cid:3)

In proving our main theorems below, we need the general representation
theorem for relatively exchangeable structures from [11]. We ﬁrst state the
following simpler version which holds of symmetric structures and conveys
the main idea.

Theorem 5.3 (Crane & Towsner [11]). Let L = {R1, . . . , Rr} be a signature
and XN ⊆ LN be a symmetric Fraïssé space with Fraïssé limit F. Suppose X is
relatively exchangeable with respect to F. Then there exist Borel measurable
1 , . . . , X ∗
functions f1, . . . , fr with range {0, 1} such that X =D X∗ = (N, X ∗
r )
with

(11)

~x ∈ X∗

j ⇐⇒ fj(F|[max ~x], (ξs)s⊆rng ~x) = 1,

for (ξs)s⊆N:|s|≤k i.i.d. Uniform[0, 1] random variables.

Remark 5.4. Note that the ﬁrst argument of fj in (12) depends on F|[max ~x],
that is, the entire initial segment of F that contains all the points in rng ~x.
Under stronger conditions on XN we achieve dependence on only F|rng ~x, that
is, the smallest substructure containing all the points in rng ~x. This nicer
representation enables a more precise description of combinatorial Markov
processes.

The corresponding representation is somewhat more complicated when
XN is asymmetric because in this case, for instance, (1, 2) ∈ X∗
1 and (2, 1) ∈
X∗
1 may be negatively correlated, so f1 must be able to distinguish (1, 2) from
(2, 1) and also coordinate the outcomes between the two. The presence of
the random orderings (≺~y)~y⊑~x in the following representation is needed to
coordinate between possibly asymmetric outcomes. (Recall from Section 2.4
that ~y ⊑ ~x means ~y is a subsequence of ~x.)

Deﬁnition 5.5. When s ⊆ N is a ﬁnite set, a uniform random ordering of
s is an ordering ≺s of s chosen uniformly at random. Given ≺rng ~x, we write
≺~x for the ordering of [1, | rng ~x|] induced by i ≺~x j if and only if xi ≺rng ~x xj.
If xi = xj, then i 6≺~x j and j 6≺~x i.

18

HARRY CRANE AND HENRY TOWSNER

Since we only need the existence of a suitable representation, we state the

theorem below and leave the details to [11].

Theorem 5.6 (Crane & Towsner [11]). Let L = {R1, . . . , Rr} be a signature
and XN ⊆ LN be a Fraïssé space with Fraïssé limit F. Suppose X is a
random L-structure that is relatively exchangeable with respect to F. Then
there exist Borel measurable functions f1, . . . , fr with range {0, 1} such that
X =D X∗ = (N, X ∗
~x ∈ X∗
(12)

1 , . . . , X ∗
j ⇐⇒ fj(F|[max ~x], (ξs)s⊆rng ~x, (≺~y)~y⊑~x) = 1,

r ) with

for (ξs)s⊆N:|s|≤k i.i.d. Uniform[0, 1] random variables and (≺s)s⊆N:|s|≤k uni-
form random orderings.

5.1. Lipschitz continuous functions. Theorems 5.3 and 5.6 are general
Aldous–Hoover-type theorems for relatively exchangeable structures. Ex-
changeability arises as a special case by taking F to be either the complete
or empty L-structure so that the dependence of each fj on the ﬁrst argument
is moot. The representation in (12) is a key element in our proofs below be-
cause it allows us to construct a random Lipschitz continuous function from
which we build the standard versions of X by either an i.i.d. sequence or a
Poisson point process, depending on whether time is discrete or continuous,
respectively.

In our main theorems for discrete time chains, X is a time homogeneous
combinatorial Feller process on a Fraïssé space and, thus, all of its transitions
can be represented by a single exchangeable transition measure

P (N, ·) = P{X1 ∈ · | X0 = N}, N ∈ XN,

which satisﬁes (3) and

P{X1|[n] = S | X0 = N} = P{X1|[n] = S | X0 = N′}

(13)
for all N, N′ ∈ XN with N|[n] = N′|[n], for all S ∈ X[n], for all n ∈ N. By
(13), we deﬁne the ﬁnite state space transition probabilities Pn on X[n] by
(14)

Pn(S, S′) = P (N, {N′ ∈ XN : N′|[n] = S′}), S, S′ ∈ X[n],

for any choice of N ∈ XN with N|[n] = S.

Among our main objectives is to show that any random structure X ∼
P (N, ·) satisﬁes X =D Φ(N) for some exchangeable random Lipschitz contin-
uous function Φ ∈ Lip(XN) whose distribution does not depend on N. To
this end, we deﬁne a canonical embedding of each N ∈ XN into its Fraïssé
limit.

Let XN be a Fraïssé space with Fraïssé limit F and for any injection φ :
[n] → N recall the notation Fφ from Section 2.1. Given F and S ∈ age(XN),
we deﬁne ρS = ρS,F : [| dom S|] → N (suppressing the dependence on F
for convenience) as follows. We begin by choosing ρS(1) to be the smallest
n ≥ 1 such that FρS↾{1} = S|{1}, where in general ρS ↾ S denotes the
domain restriction of ρS to a function S → N. Given ρS(1), . . . , ρS(m),
with m < | dom S|, we then choose ρS(m + 1) to be the smallest n > ρS(m)

THE STRUCTURE OF COMBINATORIAL MARKOV PROCESSES

19

such that ρS ↾ [m + 1] is an embedding S|[m+1] → F. We continue until ρS
deﬁnes an embedding S → F. The existence of such an embedding for every
S is guaranteed by the status of F as a Fraïssé limit.

Here we have deﬁned ρS for ﬁnite structures, but our construction extends
to inﬁnite structures N ∈ XN by noting that ρS′ ↾ dom S = ρS for S, S′ ∈
age(XN) with S′|[| dom S|] = S. (Here our convention that the domain of S
is not merely a subset of S′, but actually an initial segment, plays a crucial
role.) For convenience, we can work with an exchangeable version of F on
account of the following theorem, which we phrase in the terminology most
relevant to our discussion.

Theorem 5.7 (Ackerman, Freer & Patel [2], Theorem 3). Let F be a Fraïssé
limit of some Fraïssé space XN ⊆ LN. Then there exists an exchangeable
probability measure µ on LN such that N ∼= F for µ-almost every N ∈ LN.
Example 5.8. For an easy illustration of the above procedure, let L = {R}
have ar(R) = 1 and let XN = LN correspond to the set of all subsets of
N. An exchangeable Fraïssé limit F = (N, RF can be constructed from a
sequence ξ1, ξ2, . . . of i.i.d. Bernoulli random variables with P{ξn = 1} = 1/2
for every n ≥ 1 by putting

n ∈ RF ⇐⇒ ξn = 1.

For a concrete example, suppose RF = {1, 2, 4, 8, . . .}. For S = ([1], {1}),
we deﬁne ρS,F(1) = 1; for S = ([1], ∅), we deﬁne ρS,F(1) = 3; for S =
([2], {1}), we deﬁne ρS,F(1) = 1 and ρS,F(2) = 3; for S = ([2], {2}), we
deﬁne ρS,F(1) = 3 and ρS,F(2) = 4; and so on.

In what follows, we always assume F is an exchangeable Fraïssé limit
for XN, as guaranteed by Theorem 5.7. Given an exchangeable, consistent
transition probability measure P and an exchangeable Fraïssé limit F, we
construct an exchangeable probability measure ϕ on Lip(XN) by taking Y ∼
P (M, ·) conditional on the event F = M. Given Y, we deﬁne the random
function Φ = ΦY : XN → XN by putting Φ(N) = YρN for each N ∈ XN,
where ρN := ρN,F is the random canonical embedding N → F as deﬁned
above. (Note that the randomness in ρN,F is derived from the randomness
in the construction of F ∼ µ.)

Theorem 5.9. Let P be an exchangeable, consistent transition probability
on XN, let F be an exchangeable Fraïssé limit of XN and, given F = M, let
Y ∼ P (M, ·). The function Φ = ΦY is well deﬁned and Lipschitz continuous
with probability 1 and exchangeable in the sense of Section 2.3.

Proof. In general, XN is uncountable, so establishing these properties with
probability 1 for each N ∈ XN is not enough. However, since the signature
L = {R1, . . . , Rr} contains only ﬁnitely many relations, L[n] is ﬁnite for every
n ≥ 1 and, therefore, so is X[n] ⊆ L[n]. It follows that age(XN) is at most
countable, and so it is enough to establish that the relevant properties of Φ
hold with probability 1 for each S ∈ age(XN). In this way, we establish that

20

HARRY CRANE AND HENRY TOWSNER

the sequence of ﬁnite restrictions (Φ[n])n≥1 determines a projective limit Φ
that is well deﬁned and Lipschitz continuous with probability 1.

For Φ to be well deﬁned, there must exist an embedding ρS : S → F of the
type we describe for every S ∈ age(XN). Fix S ∈ age(XN). By Theorem 5.7,
F is isomorphic to the Fraïssé limit of XN with probability 1 and, therefore,
S embeds into F with probability 1. The fact that ρS : S → F can be
chosen so that ρS(m) < ρS(m + 1) is a consequence of ultrahomogeneity
and induction. Since this holds for every S ∈ age(XN) with probability 1,
the embedding ρS can be deﬁned for all S ∈ age(XN) with probability 1.
For N ∈ XN, we deﬁne ρN by ρN(n) = ρN|[n](n) for every n ≥ 1. Since ρN|[n]
is well deﬁned with probability 1 for every n ≥ 1, ρN is also well deﬁned
for every N ∈ XN with probability 1. Lipschitz continuity of Φ is plain by
noting that ρS′ ↾ dom S = ρS for all S ⊆ S′.

For exchangeability, we need P{Φ(N) ∈ A} = P{Φ(Nσ) ∈ Aσ} for all N ∈
XN, A ⊆Borel XN, and permutations σ : N → N, where Aσ = {xσ : x ∈ A} is
the set of relabeled structures in A. We have deﬁned Φ(N) = YρN, where
ρN = ρN,F for F an exchangeable Fraïssé limit and Y ∼ P (M, ·) conditional
on F = M. For any N ∈ XN and A ⊆Borel XN,

P{Φ(N) ∈ A} = P{YρN ∈ A}

= EP{YρN,F ∈ A | F}
= EP{Yφ ∈ A | F, ρN,F = φ}
= EP (N, A)
= EP (Nσ, Aσ)
= EP{Yφ ∈ Aσ | F, ρNσ ,F = φ}
= EP{YρNσ ,F ∈ Aσ | F}
= P{YρNσ ∈ Aσ}
= P{Φ(Nσ) ∈ Aσ}

for all permutations σ : N → N; thus, Φ is exchangeable.

(cid:3)

For an injection φ : S → N, we write F φ to denote the image of F under
φ, that is, F φ(M) = [F (M)]φ for M ∈ XN. Note that F φ is not Lipschitz
continuous in general.

Recall that any permutation σ : N → N acts on F ∈ Lip(XN) by conjuga-
tion, that is, (σ−1F σ)(M) = F (Mσ)σ−1
. We call F ∈ Lip(XN) conjugation
invariant if σ−1F σ ∈ Lip(XN) for every permutation σ : N → N. Conju-
gation invariant functions exist, for example, the identity XN → XN, but
not every F ∈ Lip(XN) is conjugation invariant, for example, the coagu-
lation operator in Example 6.2 below. Below we establish an important
connection between conjugation invariant functions and exchangeable com-
binatorial Feller processes.

THE STRUCTURE OF COMBINATORIAL MARKOV PROCESSES

21

Proposition 5.10. Let F ∈ Lip(XN). Then F is conjugation invariant if
and only if for all ﬁnite S ⊂ N and all M, M′ ∈ XN

(15)

M|S = M′|S =⇒ F (M)|S = F (M′)|S.

Proof. Suppose F ∈ Lip(XN) is conjugation invariant and, for ﬁnite S ⊂ N
with |S| = n, assume M, M′ ∈ XN satisfy M|S = M′|S. With the elements
of S ordered s1 < · · · < sn, we deﬁne the transpositions τi = (isi) and put
τ = τn · · · τ1, so that τ 2 = id and Mτ |[n] = M′τ |[n].

For any map σ : S → S′, we deﬁne σ[S] = {σ(s) : s ∈ S} ⊆ S′ as the
image of S under σ. By conjugation invariance, σ−1F σ ∈ Lip(XN) for all
permutations σ : N → N. Taking σ = τ −1, we have

Furthermore,

τ F τ −1(Mτ )|[n] = τ F τ −1(M′τ )|[n].

τ F τ −1(Mτ )|[n] = (F (M))τ |[n]
= F (M)|τ −1[n]
= F (M)|S ,

and likewise for τ F τ −1(M′τ )|[n], establishing the conclusion.

For the converse, suppose F satisﬁes (15), M|[n] = M′|[n], and σ : N → N
is a permutation. Then Mσ and M′σ agree on σ[n]. By (15), F (Mσ) and
F (M′σ) agree on σ[n] and, thus, F (Mσ)σ−1 |[n] = F (M′σ)σ−1 |[n], so that
σF σ−1 is Lipschitz continuous. Conjugation invariance follows immediately.
(cid:3)

Corollary 5.11. Let φ : S → N be an injection and let F ∈ Lip(XN) be
conjugation invariant. Then F φ ∈ Lip(XS).
Proof. Given conjugation invariant F ∈ Lip(XN), we deﬁne F ′ : XS → XS
by F ′(S) = F (M)φ for any M ∈ XN such that M|S = S. This deﬁnition is
well deﬁned and Lipschitz continuous by Proposition 5.10, and F ′ coincides
with our deﬁnition of F φ above.
(cid:3)

Theorem 5.12 (Crane & Towsner [11]). Let L = {R1, . . . , Rr} be a sig-
nature and XN ⊆ LN be a Fraïssé space that has Fraïssé limit F and which
satisﬁes n-DAP for all n ≥ 1. Suppose X is relatively exchangeable with
respect to F. Then there exist Borel measurable functions f1, . . . , fr such
that X =D X∗ = (N, X ∗
(16)

j ⇐⇒ fj(F|rng ~x, (ξs)s⊆rng ~x, (≺~y)~y⊑~x) = 1,

~x ∈ X∗

1 , . . . , X ∗

r ) with

for (ξs)s⊆N:|s|≤k i.i.d. Uniform[0, 1] random variables and (≺s)s⊆N:|s|≤k are
uniform random orderings.

Remark 5.13. The diﬀerence between the representations in Theorems
5.6 and 5.12 is that the assignment of ~x in X∗
j depends on the entire initial
substructure F|[max ~x] in (12) but only on F|rng ~x in (16). The intuition behind
the weaker representation in (12) is that in general the structure of F is

22

HARRY CRANE AND HENRY TOWSNER

such that statements about ~x have implications beyond the substructure
F|rng ~x. A prime example is the case of an equivalence relation ∼π, where
the transitivity axiom implies that i ∼π k whenever i ∼π j and j ∼π k.
Therefore, when building a relatively exchangeable random partition from
another, it is important to account for the possible non-local restrictions
imposed by the transitivity axiom, as the diﬀerence between (12) and (16)
indicates.

Let F be an exchangeable Fraïssé limit for a space XN with n-DAP for
all n ≥ 1, µ be an F-exchangeable measure on XN, and f = (f1, . . . , fr)
be any Borel measurable functions as in (16). Let (ξs)s⊆N:|s|≤k be i.i.d.
Uniform[0, 1] random variables and (≺~y)~y∈Nr:1≤r≤k be independent uniform
random orderings. We deﬁne a function Ψ : XN → XN by putting M′ =
Ψ(M) = (N, R′

r), where

1, . . . , R′

(17)

~x ∈ R′

j ⇐⇒ fj(M|rng ~x, (ξs)s⊆rng ~x, (≺~y)~y⊆~x) = 1.

This map is clearly Lipschitz continuous and has the further property that
N|rng ~x = N′|rng ~x implies Ψ(N) and Ψ(N′) agree in their ~x-indexed entries.
Note further that Mσ|rng ~x = M|rng σ−1(~x) for every ~x.

The following is an immediate consequence of Theorem 5.9 and the con-

struction of Ψ.

Proposition 5.14. Let P be an exchangeable, consistent transition proba-
bility on a Fraïssé space XN that satisﬁes n-DAP for all n ≥ 1, let F be an
exchangeable Fraïssé limit of XN, and, given F = M, let f = (f1, . . . , fr)
be a collection of Borel measurable functions that determine the law of the
M-exchangeable probability distribution P (M, ·) as in (16). The function
Ψ deﬁned from f as in (17) is well deﬁned and conjugation invariant with
probability 1 and exchangeable in the sense of Section 2.3.

6. Discrete time chains

Throughout this section, T = Z+ and X is a discrete time Markov chain
on a Fraïssé space XN. We have already established most of the ingredients
necessary to prove our main theorems for discrete time processes. Recall the
deﬁnition of the standard µ-process X∗
µ from Section 3.1 for a probability
measure µ on Lip(XN).

Proposition 6.1. For any exchangeable probability measure µ on Lip(XN),
the standard µ-process is an exchangeable, consistent Markov chain on XN.

Proof. Let µ be any exchangeable probability measure on Lip(XN) so that
F ∼ µ implies F (Mσ) =D F (M)σ for every M ∈ XN and every permutation
σ : N → N. The standard µ-process X∗
µ is constructed by an i.i.d. sequence
F1, F2, . . . from µ as in (8). Each X∗
M,µ clearly satisﬁes the Markov property
on XN by its construction from the i.i.d. sequence F1, F2, . . .. Exchangeabil-
ity of µ implies

X∗σ

M,µ = (F (m)(M)σ)m∈Z+ =D(F (m)(Mσ))m∈Z+ = X∗

Mσ,µ

THE STRUCTURE OF COMBINATORIAL MARKOV PROCESSES

23

for every permutation σ : N → N and every M ∈ XN, where F (m)(·) = (Fm ◦
· · · ◦ F1)(·) and F (0)(·) is deﬁned as the identity. The transition probabilities
of X∗

µ therefore satisfy

P{X∗

m+1|[n] = S′ | X∗

m|[n] = S} = P{F [n]
= P{F [n]
= P{X∗

m+1(S) = S′}
m+1(Sσ) = S′σ}
m+1|[n] = S′σ | X∗

m|[n] = Sσ},

and X∗
a common sequence of Lipschitz continuous functions, X∗
Feller property follows from Proposition 4.22.

µ is exchangeable in the sense of Deﬁnition 2.5. By construction from
µ is consistent. The
(cid:3)

Our main theorems in discrete time establish the converse based on the

concept of relative exchangeability from Section 5.

Proof of Theorem 3.1. The ‘if’ direction follows from Proposition 6.1. The
‘only if’ direction goes as follows.

By assumption, XN is a Fraïssé space, which by Theorem 4.10 possesses
a unique (up to isomorphism) ultrahomogeneous L-structure F ∈ XN into
which every M ∈ XN embeds. By Theorem 5.7, we can assume F results
from an exchangeable construction. Let P (·, ·) be the transition probability
for X as in (2) and let Pn(·, ·) be the induced transition probabilities for X[n].
By Deﬁnition 5.1, Y ∼ P (F, ·) is relatively exchangeable with respect to F.
By Theorem 4.10, F satisﬁes the conditions of Theorem 5.6 with probability
1, so that Theorem 5.9 implies that Φ = ΦY deﬁned from Y ∼ P (F, ·) is an
exchangeable Lipschitz continuous function XN → XN.

Let µ be the (essentially unique) probability measure governing Φ. By

relative exchangeability and Theorem 5.6, we have

Φ(N)|[n] = YρN|[n] ∼ Pn(N|[n], ·).

M =D XM by taking X∗

Thus, we can construct X∗
i.i.d. from µ, and putting X∗
changeability guarantees that X∗
bilities for each m ≥ 1. We conclude X∗
induction on m ≥ 1; thus, X∗

0 = M, generating Φ1, Φ2, . . .
m−1) for each m ≥ 1. Relative ex-
m obeys the appropriate transition proba-
M =D XM for every M ∈ XN by
(cid:3)

µ =D X. This completes the proof.

m = Φm(X∗

There are a few examples of Theorem 3.1 scattered throughout the liter-

ature. Perhaps the most well known is the discrete time coalescent chain.

Example 6.2 (Coalescent chain). Let PN be the set of partitions of N.
For each π ∈ PN, the coagulation operator is a Lipschitz continuous map
Coag(·, π) : PN → PN that acts by π′ = Coag(π′′, π) = {B′
2, . . .}, where

1, B′

B′

j = [i∈Bj

B′′
i ,

j ≥ 1,

for π = {B1, B2, . . .} and π′′ = {B′′
called the coagulation of π′′ by π.

1 , B′′

2 , . . .}. The operation Coag(π′′, π) is

24

HARRY CRANE AND HENRY TOWSNER

An exchangeable coalescent chain X = (Xm)m∈Z+ evolves on PN as follows.
Let µ be an exchangeable probability distribution on PN, put X0 = 0N =
{{1}, {2}, . . .} (the partition of N into singletons), and take Π1, Π2, . . . i.i.d.
from µ. For each m ≥ 1, we deﬁne Xm = Coag(Xm−1, Πm). Exchangeability
of X is endowed by exchangeability of each partition Π1, Π2, . . . and the
action of the coagulation operator.

In typical treatments, for example, [7], the coalescent chain is deﬁned by
the transitions in terms of the coagulation operator and it is later shown
that it satisﬁes the projective property. Theorem 3.1 establishes a general
converse, which guarantees the existence of some random Lipschitz continu-
ous function such that the above iterative description is possible. Theorem
3.1 applies to the coalescent chain because PN is a Fraïssé space. Theorem
3.1 cannot provide the precise dynamics in terms of the coagulation operator
because that behavior is speciﬁc to the coalescent chain; it need not hold
for general Feller chains on PN.

Although Coag(·, π) is Lipschitz continuous, it is not conjugation invari-
ant. For example, take π = {{1, 2}, {3}}, π′(1) = {{1, 4}, {2, 3, 5}}, and
π′(2) = {{1, 4}, {2}, {3, 5}}, so that π′(1)|{3,4,5} = π′(2)|{3,4,5} = {{3, 5}, {4}}.
In this case, Coag(π′(1), π) = {{1, 2, 3, 4, 5}} and Coag(π′(2), π) = {{1, 2, 4}, {3, 5}},
so that Coag(π′(1), π)|{3,4,5} 6= Coag(π′(2), π)|{3,4,5}. By Proposition 5.10,
Coag(·, π) is not conjugation invariant. This observation goes hand-in-hand
with Example 4.18, in which we show that partitions do not have the stronger
n-disjoint amalgamation property for n = 3, and Theorem 6.3, which asso-
ciates conjugation invariant functions with Fraïssé spaces that exhibit n-
DAP for all n ≥ 1.

6.1. Reﬁning Theorem 3.1. When XN also has n-DAP for every n ≥ 1,
we can reﬁne Theorem 3.1 so that it is constructed by i.i.d. conjugation
invariant functions.

Theorem 6.3. In addition to the hypotheses of Theorem 3.1, suppose that
XN is a Fraïssé space with n-DAP for all n ≥ 1. Then the characteristic
measure µ can be deﬁned so that µ-almost every F ∈ Lip(XN) is conjugation
invariant.

Proof. The proof follows the same recipe as that of Theorem 3.1, except
now we use the stronger representation in (16). Let Ψ be the conjugation
invariant function constructed in Proposition 5.14. For any N ∈ XN, N′ =
Ψ(N) ∼ P (N, ·) by deﬁnition. The rest follows by combining Theorem 3.1
with Theorem 5.12, Proposition 5.10, and Corollary 5.11.
(cid:3)

Unlike Theorem 3.1, Theorem 6.3 also provides the form of those Lipschitz
continuous functions that determine the evolution of exchangeable Feller
chains. The space of partitions does not have 3-DAP, see Example 4.18, and
therefore Theorem 6.3 does not apply to the coalescent chain above or more
general Markov chains on PN. Exchangeable Feller chains on {0, 1}N give
an easy illustration of Theorem 6.3, as we now show.

THE STRUCTURE OF COMBINATORIAL MARKOV PROCESSES

25

Example 6.4. Here we let L = {R} be the signature with a single unary
relation, so that any {0, 1}-valued sequence x = x1x2 · · · corresponds to an
L-structure N = (N, R) with R = {n ∈ N : xn = 1}. The space {0, 1}N
corresponds to the set of all subsets of N, which is plainly a Fraïssé class
and has n-DAP for all n ≥ 1. Theorem 6.3 applies to Feller chains on
LN = {0, 1}N .

0 Y 2

0 · · · and Y1 = Y 1

We construct X by specifying µ on Lip({0, 1}N ) as follows. Let Θ be a
probability measure on [0, 1] × [0, 1], take (θ0, θ1) ∼ Θ, and, given (θ0, θ1),
deﬁne Y0 = Y 1
1 · · · to be conditionally independent
with Y0 a sequence of i.i.d. Bernoulli(θ0) random variables and Y1 a sequence
of i.i.d. Bernoulli(θ1) random variables. Based on Y = (Y0, Y1), we deﬁne
FY : {0, 1}N → {0, 1}N by x 7→ x′ = FY (x) with
0 , xn = 0,
1 , xn = 1.
Y n

x′n := (cid:26) Y n

1 Y 2

The map FY is exchangeable and conjugation invariant. The measure µ gov-
erning FY corresponds to the measure governing the i.i.d. sequence F1, F2, . . .
of Lipschitz continuous functions in Theorem 6.3. The above representation
is characteristic of so-called cut-and-paste processes; see [8].

7. Continuous time processes

In continuous time, each XM = (Xt)t∈[0,∞) is deﬁned at uncountably many
times and, therefore, can exhibit more nuanced behavior than in discrete
time. The Markov property no longer prohibits inﬁnitely many jumps in an
arbitrarily small period of time, but the càdlàg paths property reigns in the
behavior somewhat.

As before, we assume X is a combinatorial Feller process on a Fraïssé
space XN with semigroup (Pt)t≥0. By the Feller property, its inﬁnitesimal
jump rates are given by
(18)

Q(M, A) := lim
t↓0

1
t

P{Xt ∈ A | X0 = M}, M ∈ XN, A ⊆Borel XN \{M}.

Equation (18) determines the transition rates of the ﬁnite state space pro-
cesses X[n] through
(19)

Qn(S, S′) := Q(M, {M′ ∈ XN : M′|[n] = S′}), S, S′ ∈ X[n], S 6= S′,

where M is any element of {M∗ ∈ XN : M∗|[n] = S}. Consistency implies
Qn(S, X[n] \{S}) < ∞ for all S ∈ X[n] and all n ∈ N, and exchangeability (3)
guarantees that Qn(Sσ, S′σ) = Qn(S, S′) for all permutations σ : [n] → [n].
The continuous time analog to the i.i.d. sequence of Lipschitz continuous
functions in Theorem 3.1 is a construction by a time homogeneous Poisson
point process on the space of Lipschitz continuous functions. Let Λ be an
exchangeable measure on Lip(XN) satisfying (9) and let Φ := {(t, Ft)} ⊆

26

HARRY CRANE AND HENRY TOWSNER

[0, ∞) × Lip(XN) be a Poisson point process with intensity dt ⊗ Λ just as in
Section 3.2.

Proposition 7.1. Let Λ be an exchangeable measure satisfying (9) and
let X∗
Λ be the standard Λ-process constructed from Poisson point process
Φ = {(t, Ft)} ⊆ [0, ∞) × Lip(XN) with intensity dt ⊗ Λ as in Section 3.2.
Then X∗

Λ is an exchangeable Feller process on XN.

∗[n]
Λ

on X[n]
Proof. We need to establish that each of the ﬁnite processes X
is an exchangeable Markov process. This is a consequence of the thinning
property for Poisson point processes, exchangeability of Λ, and almost sure
Lipschitz continuity of the functions that determine the jumps of X∗
Λ.

To see this explicitly, let Φ = {(t, Ft)} ⊆ [0, ∞) × Lip(XN) be a Poisson
point process with intensity dt ⊗ Λ. We deﬁne Φ[n] := {(t, F [n]
)} ⊆ [0, ∞) ×
Lip(X[n])\{id[n]} from the atoms of Φ for which F [n]
6= id[n]. By the thinning
property of Poisson processes, see [14], Φ[n] is also a Poisson process with
intensity dt ⊗ Λn, where

t

t

Λn(F ′) := Λ({F ∈ Lip(XN) : F [n] = F ′}), F ′ ∈ Lip(X[n]) \ {id[n]},

and Λn(id[n]) = 0. By (9), Λn(Lip(X[n])) = Λ({F : F [n] 6= id[n]}) < ∞ and
the rate at which X

jumps out of each S ∈ X[n] is

∗[n]
Λ

Qn(S, ·) = Λn({F ∈ Lip(X[n]) : F (S) ∈ ·}).

∗[n]
Λ

follows by exchangeability of Λ, as deﬁned in (7).
Exchangeability of X
∗[n]
The family of processes (X
Λ )n∈N is compatible by their construction from
for all n ≥
the same Poisson point process Φ, that is, X
1. Compatibility implies the existence of a Markov process X∗
Λ on XN for
which the projective Markov property holds. The Feller property follows by
Proposition 4.22.
(cid:3)

|[n] = X

∗[n+1]
Λ

∗[n]
Λ

7.1. Existence of exchangeable jump measure. Proposition 7.1 shows
that each X∗
Λ is exchangeable and has the Feller property. Theorem 3.2
asserts the converse. To show that such a construction is always possible,
let X be an exchangeable Feller process on a Fraïssé space XN. For every
t > 0, Theorems 3.1 and 5.9 guarantee the existence of an exchangeable
probability measure Λt on Lip(XN) such that, for every M, N ∈ XN,

(20)

Λt({F ∈ Lip(XN) : F (N) ∈ ·}) = P{Xt+s ∈ · | Xs = N, X0 = M},

for all s ≥ 0. The Chapman–Kolmogorov theorem implies that (Λt)t≥0
satisﬁes

(21)

Λt+s({F ∈ Lip(XN) : F (M) ∈ ·}) =

= ZLip(XN )

Λt({F ′ ∈ Lip(XN) : (F ′ ◦ F )(M) ∈ ·})Λs(dF ),

THE STRUCTURE OF COMBINATORIAL MARKOV PROCESSES

27

for all s, t ≥ 0 and all M ∈ XN. The family (Λt)t≥0 is not determined by
these conditions, but nevertheless time homogeneity and (20) implies the
existence of an exchangeable rate measure Λ on Lip(XN) that satisﬁes (9)
and determines the jump rates of X.

Theorem 7.2. Let X = {XM : M ∈ XN} be an exchangeable Feller pro-
cess on a Fraïssé space XN. Then there exists an exchangeable measure Λ
satisfying (9) such that X =D X∗

Λ as constructed in Section 3.2.

Proof. Let X be a continuous time exchangeable Feller process on XN. For
every t > 0, Theorem 3.1 guarantees that there exists a probability measure
Λt on Lip(XN) satisfying (20). For each n ≥ 1 and t > 0, we deﬁne Λ(n)
on
Lip(X[n]) by

t

(22)

Λ(n)

t

(F ) := ( t−1Λt({H ∈ Lip(XN) : H [n] = F }), F 6= id[n],

otherwise.

0,

We deﬁne λn := maxS∈X[n] Qn(S, X[n] \{S}) < ∞ for every n ≥ 1.

For every F ∈ Lip(X[n]) \ {id[n]}, there is some S∗ ∈ X[n] such that

F (S∗) 6= S∗. Let S be any such S∗. Then

Λ(n)

t

(F ) ≤ Λ(n)

t

({H ∈ Lip(X[n]) : H(S) 6= S})

s )s≥0 is discontinuous on [0, t] | X[n]

0 = S}

0 = S}

6= S | X[n]

= t−1P{X[n]
t
≤ t−1P{(X[n]
= t−1(1 − exp{−tQn(S, X[n] \{S})})
≤ t−1(1 − exp{−tλn})
≤ t−1(λnt)
= λn
< ∞.

For a signature L = {R1, . . . , Rr} with max1≤j≤r ar(Rj) = k < ∞, there
can be no more than 2rnk
elements in X[n] and, therefore, there are at most
4rnk
is a ﬁnite measure bounded
by 4rnk
exchangeable measures for each n ≥ 1.

< ∞ elements in Lip(X[n]). Thus, Λ(n)
t
λ for all t > 0 and n ≥ 1, and (Λ(n)

)t↓0 is a bounded sequence of

t

Finiteness of Lip(X[n]) and the Bolzano–Weierstrass theorem allows us
to choose a subsequence tn,1 > tn,2 > · · · > 0 with tn,k ↓ 0 such that
Λ(n)
tn,k → Λ(n), a ﬁnite measure on Lip(X[n]) with

(23)

Λ(n)(F ) = lim
k→∞

Λ(n)

tn,k (F ), F ∈ Lip(X[n]).

t

Each Λ(n)
sequence. We have also deﬁned Λ(n)
Λ(n)(id[n]) = 0 for any choice of subsequence.

is exchangeable and, therefore, so is Λ(n) for any choice of sub-
(id[n]) = 0 for all t > 0 so that

t

28

HARRY CRANE AND HENRY TOWSNER

We ensure that (Λ(n))n≥1 determines a measure Λ on Lip(XN) by deﬁning

the collection (Λ(n))n≥1 from a reﬁning collection of subsequences {{tn,k}k≥1}n≥1.
We ﬁrst choose {t1,k}k≥1 so that Λ(1)
converges to a measure Λ(1) on
t1,k
Lip(X[1]). Given {tn,k}k≥1 for any n ≥ 1, we then choose {tn+1,k}k≥1 as
a subsequence of {tn,k}k≥1 so that Λtn+1,k converges to a measure Λ(n+1)
on Lip(X[n+1]. We can always choose such a subsequence by the Bolzano–
Weierstrass theorem.

By our construction from a reﬁning collection of subsequences, we have

Λ(n+1)({F ′ ∈ Lip(X[n+1]) : F ′[n] = F }) =

=

=

X
X

F ′∈Lip(X[n+1]):F ′[n]=F

F ′∈Lip(X[n+1]):F ′[n]=F

Λ(n+1)(F ′)

lim
k→∞

Λ(n+1)
tn+1,k

(F ′)

= lim
k→∞

t−1
n+1,k

X

F ′∈Lip(X[n+1]):F ′[n]=F

Λtn+1,k ({F ∗ ∈ Lip(XN) : F ∗[n+1] = F })

= lim
k→∞

t−1
n+1,kΛtn+1,k ({F ∗ ∈ Lip(XN) : F ∗[n] = F })

= Λ(n)(F ),

where the interchange of sum and limit is justiﬁed by the bounded conver-
gence theorem.

The Borel σ-ﬁeld on Lip(XN) is generated by the π-system of events of

the form

{F ∗ ∈ Lip(XN) : F ∗[n] = F }, F ∈ Lip(X[n]), n ∈ N;

therefore, we can deﬁne a set function Λ on Lip(XN) on sets of the form
{F ∗ ∈ Lip(XN) : F ∗[n] = F } for F ∈ Lip(X[n]) \ {id[n]} by

Λ({F ∗ ∈ Lip(XN) : F ∗[n] = F }) = Λ(n)(F ), F ∈ Lip(X[n]) \ {id[n]}.

By construction, (Λ(n))n∈N satisﬁes

Λ(m)(F ) =

X

F ′∈Lip(X[n]):F ′[m]=F

Λ(n)(F ′)

for every m ≤ n and F ∈ Lip(X[m]) \ {id[m]}. Carathéodory’s extension
theorem guarantees an extension to a measure Λ on Lip(XN) \ {idN}. For
each n ∈ N, Λ(n) determines the jump rates of an exchangeable Markov
chain on X[n], giving

Λ({F ∗ ∈ Lip(XN) : F ∗[n] 6= id[n]}) = Λ(n)(X[n] \{id[n]}) < ∞

for all n ∈ N. Putting Λ({idN}) = 0 gives (9). Exchangeability of every
Λt, t > 0, makes each Λ(n), n ∈ N, exchangeable and, hence, implies Λ is
exchangeable. We must show that such Λ gives X∗

Λ =D X.

THE STRUCTURE OF COMBINATORIAL MARKOV PROCESSES

29

Let Λ be the exchangeable measure from above and let Φ = {(t, Φt)} ⊆
[0, ∞) × Lip(XN) be a Poisson point process with intensity dt ⊗ Λ. Since Λ
satisﬁes (9), Proposition 7.1 allows us to construct X∗
Λ from Φ as in Section
are determined by a thinned version
3.2. The jump rates of each X
Φ[n] of Φ that only keeps the atoms (t, Φt) for which Φ[n]
6= id[n]. By the
thinning property of Poisson random measures and our construction of Λ,
the intensity of Φ[n] is Λ(n) as deﬁned in (23), and it follows immediately
that the jump rate from S to S′ 6= S in X

∗[n]
Λ

t

∗[n]
Λ

is

Λ(n)({F ∈ Lip(X[n]) : F (S) = S′}) = Qn(S, S′),

∗[n]
for Qn(·, ·) as in (19). By construction, (X
Λ )n∈N is a compatible collection
of càdlàg exchangeable Markov chains governed by the ﬁnite-dimensional
transition law of X. The proof is complete.
(cid:3)

Proof of Theorem 3.2. The ‘if’ direction follows directly from Proposition
7.1. The ‘only if’ direction follows from Theorem 7.2.
(cid:3)

Example 7.3 (Coalescent process). The coalescent chain in Example 6.2
has a continuous time analog deﬁned as follows. Let (Pt)t≥0 be the semi-
group of a Markov process on PN such that for every t ≥ 0 and every
bounded continuous g : PN → R

Ptg(π) = Eg(Coag(π, Πt))

for some exchangeable random partition Πt. By deﬁnition, this process is
deﬁned in terms of the Lipschitz continuous coagulation operator. Pitman
[18] has shown that the transition rates enjoy a special structure with Λ =
cκ + ̺ν for a unique constant c ≥ 0 and measures κ and ν deﬁned as follows.

Let ν be a measure on the ranked-simplex

∆↓ := {(s1, s2, . . .) : s1 ≥ s2 ≥ · · · ≥ 0, Xi≥1

si ≤ 1}

satisfying

(24)

ν({0}) = 0 and

Z∆↓  ∞
Xi=1

s2

i! ν(ds) < ∞,

where 0 := (0, 0, . . .). We write ̺ν to denote Kingman’s paintbox measure
directed by ν on the space of partitions of N,

̺ν (·) := Z∆↓

̺s(·)ν(ds),

where ̺s(·) is the paintbox measure for a ﬁxed s ∈ ∆↓; see Bertoin [7,
Chapter 2] for more details. By (24), the image measure of ̺ν on Lip(XN)
deﬁned by π 7→ Coag(·, π) satisﬁes (9) and is exchangeable.

For i < j let ei,j be the partition of N with all blocks singletons except one

block of size two given by {i, j}. The Kingman measure κ(·) = Pi<j δei,j (·)

30

HARRY CRANE AND HENRY TOWSNER

puts mass 1 on each of these partitions. The image of κ by π 7→ Coag(·, π)
also satisﬁes (9) and is exchangeable.

This is a special case of Theorem 3.2.

Example 7.4 (Fragmentation process). The class of homogeneous fragmen-
tation processes evolves in the opposite direction of the above coalescent
process.
Instead of merging blocks together by the coagulation operator,
a fragmentation process breaks them apart using the fragmentation oper-
ator. For a partition π = {B1, B2, . . .} with blocks listed in increasing
order of their smallest element, we deﬁne Frag : PN × PN × N → PN by
π 7→ π′ = Frag(π, π′′, k), where π′ has blocks B1, . . . , Bk−1, Bk+1, . . . just as
in π along with the blocks Bk ∩ B′′
2 , . . ., that is, the blocks obtained
by restricting π′′ to Bk. We call Frag(π, π′′, k) the fragmentation of the kth
block of π by π′′.

1 , Bk ∩ B′′

Heuristically, the homogeneous fragmentation process (Xt)t∈[0,∞) is an
exchangeable Feller process on PN with initial state 1N = {N} and behavior
constructed from a Poisson point process {(t, Πt, Kt)} ⊂ [0, ∞) × PN × N
by putting Xt = Frag(Xt−, Πt, Kt) if t ≥ 0 is an atom time. (The rigorous
construction follows the same program as in Section 3.2). Here the measure
Λ decomposes as Λ = cǫ + ̺ν for a unique constant c ≥ 0, ̺ν as deﬁned in
Example 7.3 with regularity constraint

(25)

ν({(1, 0, . . .)}) = 0 and

(1 − s1)ν(ds) < ∞,

Z∆↓

and erosion measure ǫ deﬁned as follows. For each n ∈ N,
let en :=
{N \{n}, {n}} be the partition with element n isolated from the rest and de-

ﬁne ǫ(·) := Pn∈N δen(·). The construction proceeds by putting Λ = cǫ + ̺ν

and generating a Poisson point process on [0, ∞) × PN × N with intensity
dt⊗Λ⊗K, where K is counting measure on N. Bertoin [6] showed that every
homogeneous fragmentation process can be constructed from this procedure
for unique c ≥ 0 and ν satisfying (25).

References

[1] N. Ackerman. Representations

of Aut(M)-invariant measures:

Part

I.

arXiv:1509.06170, 2015.

[2] N. Ackerman, C. Freer, and R. Patel. Invariant measures concentrated on countable

structures. arXiv:1206.4011v3, 2012.

[3] A. Aroskar and J. Cummings. Limits, regularity and removal for ﬁnite structures.

arXiv:1412.8084v1, 2014.

[4] J. Berestycki. Exchangeable fragmentation-coalescence processes and their equilib-

rium measures. Electron. J. Probab., 9:no. 25, 770–824 (electronic), 2004.

[5] J. Bertoin. Lévy Processes. Cambridge University Press, Cambridge, 1996.
[6] J. Bertoin. Homogeneous fragmentation processes. Probab. Theory Related Fields,

121(3):301–318, 2001.

[7] J. Bertoin. Random fragmentation and coagulation processes, volume 102 of Cam-
bridge Studies in Advanced Mathematics. Cambridge University Press, Cambridge,
2006.

[8] H. Crane. The cut-and-paste process. Annals of Probability, 42(5):1952–1979, 2014.

THE STRUCTURE OF COMBINATORIAL MARKOV PROCESSES

31

[9] H. Crane. Exchangeable graph-valued Feller processes. Probability Theory and Related

Fields, in press, 2016.

[10] H. Crane. The ubiquitous Ewens sampling formula (with discussion). Statistical Sci-

ence, 1, 2016.

[11] H. Crane and H. Towsner. Relatively exchangeable structures. 2015.
[12] P. Diaconis and D. Freedman. Iterated random functions. SIAM Review, 41(1):45–76,

1999.

[13] W. Hodges. a shorter model theory. Encyclopedia of Mathematics and Its Applica-

tions. Cambridge University Press, 1997.

[14] O. Kallenberg. Random Measures. Academic Press, New York, 1986.
[15] J. F. C. Kingman. The coalescent. Stochastic Process. Appl., 13(3):235–248, 1982.
[16] L. Lovász and B. Szegedy. Limits of dense graph sequences. J. Comb. Th. B, 96:933–

957, 2006.

[17] P. M. N.L. Hjort, C. Holmes and E. S.G. Walker. Bayesian Nonparametrics. Cam-
bridge Series in Statistical and Probabilistic Mathematics. Cambridge University
Press, 2010.

[18] J. Pitman. Coalescents with multiple collisions. Ann. Probab., 27(4):1870–1902, 1999.
[19] J. Pitman. Combinatorial stochastic processes, volume 1875 of Lecture Notes in Math-
ematics. Springer-Verlag, Berlin, 2006. Lectures from the 32nd Summer School on
Probability Theory held in Saint-Flour, July 7–24, 2002, With a foreword by Jean
Picard.

[20] R. Rado. Universal graphs and universal functions. Acta Arithmetica, 9:331–340, 1964.
[21] J. Schwesinsberg. Coalescents with simultaneous multiple collisions. Electron. J.

Probab., 5:1–50, 2000.

Department of Statistics & Biostatistics, Rutgers University, 110 Frel-

inghuysen Avenue, Piscataway, NJ 08854, USA

E-mail address: hcrane@stat.rutgers.edu
URL: http://stat.rutgers.edu/home/hcrane

Department of Mathematics, University of Pennsylvania, 209 South 33rd

Street, Philadelphia, PA 19104-6395, USA

E-mail address: htowsner@math.upenn.edu
URL: http://www.math.upenn.edu/~htowsner

