Doubly-nonparametric generalized linear models

Alan Huang

University of Queensland

4 Dec, 2014

Abstract

6
1
0
2

 
r
a

M
2

 

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
1
2
9
0
0

.

3
0
6
1
:
v
i
X
r
a

We extend nonparametric generalized linear models to allow both the mean curve and
the response distribution to be nonparametric. The seemingly intractable task of working
with two inﬁnite-dimensional parameters is shown to be reducible to a ﬁnite optimization
problem, which is easily implemented via existing algorithms. We demonstrate using
various examples that the proposed approach can be a ﬂexible tool for data analysis in
its own right, but can also be useful for model selection and diagnosis in a more classical
generalized linear model framework.

Keywords: Generalized linear models; Scatterplot smoothing; Smoothing splines.

1

Introduction

Nonparametric generalized linear models (e.g., Green & Silverman, 1994, Chapter 5; Rup-
pert, Wand & Carroll 2003, Chapter 10) have become popular tools for applied regression
analysis because of their ability to model a wide range of data and patterns in a ﬂexible,
nonparametric way. The phrase ,“nonparametric generalized linear model”, is, however,
somewhat of a misnomer because such models are really only semiparametric in nature.
This is because while the mean curve is nonparametric, the error distribution around the
mean (or, equivalently, the variance function) is typically fully speciﬁed, except perhaps for
a ﬁnite number of scale parameters. It seems rather paradoxical to be entertaining such ﬂex-
ible curves for the mean yet remain so rigid with the response distribution. Indeed, it is
well-known that even in simple linear regression settings, misspeciﬁcation of the response
distribution can lead to signiﬁcantly biased inferences (e.g. Eicker, 1967; White, 1982). Our
simulation results in Section 6 suggest that model misspeciﬁcation can be equally, if not more,
detrimental in nonparametric regression settings. The problem is further ampliﬁed in small
to moderate sized problems where accurate model selection and diagnosis can be difﬁcult
precisely because of the lack of information.

This paper introduces a novel extension of nonparametric generalized linear models
(GLMs) that allows both the mean curve and the response distribution to be nonparametric.
The proposed approach is a genuine extension of GLMs, in that the only model assumption
we make is that the data come from some exponential family. Consequently, we always re-
main in a full probability setting. In contrast, quasilikelihood (QL) based methods, such as
QL with unknown link and variance (Chiou & M ¨uller, 1998), typically do not correspond
to any probability model for the data and thus do not provide any further insight into the
probabilistic mechanism generating the data beyond that of the ﬁrst two moments. Having
a full probability model is particularly useful for model selection and diagnosis, predictive
inferences and nonparametric bootstrap resampling.

1

Figure 1: Main display: Scatterplot of the original data (circles) with estimated mean curve
(solid) and approximate pointwise 95% variability band for the mean (shaded) for the LIDAR
dataset. Second row: Estimated response distributions at quintiles of the covariate values, along
with the estimated means and standard deviations. Sample size n = 221, smoothing parameter
obtained by two-fold cross-validation

In Figure 1, we display the output from a MATLAB routine that ﬁts the proposed model
to a light detection and ranging (LIDAR) dataset from Ruppert, Wand & Carroll (2003). The
main features of this dataset are the nonlinear relationship between the response and the co-
variate and the heteroscedastic errors, making it difﬁcult to model using classical regression
approaches. Here, the response (y) is the logarithm of the ratio of received light from two
laser sources and the covariate (t) is the distance traveled before the light is reﬂected back to
its source.

In the main display of Figure 1, the circles represent the data points, the solid line is the
ﬁtted mean curve and the shaded area is an approximate 95% pointwise variability band
for the mean. We see that the nonlinear relationship between the response and covariate
has been captured well by the ﬁtted nonparametric mean curve, with the variability band
suitably wider in regions where the variance is evidently larger. In the second row of the
ﬁgure, the estimated response distributions at quintiles of the covariate values are displayed,
along with their corresponding means and standard deviations. We see that both the shape
and the spread of the ﬁtted response distributions change as a function of the covariate value,
following the non-constant variance patterns apparent in the data. Note that both the mean
curve and the response distributions were estimated simultaneously and automatically, and

2

400450500550600650700−0.9−0.8−0.7−0.6−0.5−0.4−0.3−0.2−0.10Data (dots) and fitted mean curve (line)−1−0.5000.20.40.60.81Estimated distribution at t = 454mean = −0.051537, sd =0.031191−1−0.5000.20.40.60.81Estimated distribution at t = 520mean = −0.059451, sd =0.03306−1−0.5000.20.40.60.81Estimated distribution at t = 588mean = −0.36114, sd =0.09799−1−0.5000.20.40.60.81Estimated distribution at t = 654mean = −0.63283, sd =0.12167at no stage did we specify what kind of patterns to look for.

The only aspect of the proposed approach that requires user input is the selection of a
smoothing parameter. However, this process can also be automated via a selection method
such as cross-validation (see Section 8). Note that smoothing parameters are central to all
smoothing methods in statistics, including the classical approach of Green & Silverman
(1994) which this paper extends. An attractive aspect of our proposed approach is that it
only requires a smoothing parameter, whereas existing methods require speciﬁcation of both
a smoothing parameter and an underlying response distribution for the data. As is evi-
denced through the introductory example in Figure 1 and the various synthetic examples in
Section 6, the relaxing of distributional assumptions makes the doubly-nonparametric GLM
approach a powerful and ﬂexible tool for data analysis.

The proposed approach also possesses two very attractive theoretical properties that are
generally not true for classical nonparametric GLMs. First, it automatically preserves sup-
port bounds, without requiring these bounds to be known a priori. Thus, if the data are
non-negative, say, then the ﬁtted mean curves and conﬁdence bands are automatically non-
negative also. Second, the proposed approach is invariant to shifting and scaling of the data.
Thus, ﬁtted mean curves and conﬁdence bands for linearly transformed data are precisely
the transformed versions of the ﬁtted means and conﬁdence bands for the original data. Fi-
nally, the proposed approach is easily extendible to multiple covariates, thereby extending
the generalized additive framework of Hastie & Tibshirani (1990) as well.

2 Model and method

2.1 Classical nonparametric GLMs
We ﬁrst review the classical penalized likelihood approach to nonparametric GLMs of Green
& Silverman (1994). The extension to doubly-nonparametric GLMs is then developed using a
novel exponential tilt representation of GLMs introduced in Rathouz & Gao (2009). For sim-
plicity, we focus on the case where there is only one quantitative covariate, denoted generi-
cally by t, with the observed covariate values being distinct and ordered, t1 < t2 < . . . < tn;
extensions of this framework are discussed in Section 7. The responses are denoted generi-
cally by Y , with a particular sample is denoted by Y = (Y1, . . . , Yn)T .

Following Chapter 5 of Green & Silverman (1994), recall that a nonparametric GLM as-
sumes that the responses Yi, conditional on the covariates ti, are independently drawn from
an exponential family of distributions with densities of the form

dFi(y; g(ti); φ) = exp

+ c(y, φ)

,

(1)

(cid:26) g(ti)y − b(g(ti))

φ

(cid:27)

where φ is a scale parameter, the functions b and c are known functions that determine the
form of the distribution, and g is a smooth but otherwise unspeciﬁed function. Note that the

mean function is E(Y |t) = µ(t) =(cid:82) ydF (y; g(t), φ) = b(cid:48)(g(t)), a smooth function of t, related

to g via the canonical link b(cid:48).

The loglikelihood function for g is given by

n(cid:88)

i=1

l(g) =

1
n

{g(ti)Yi − b(g(ti))} ,

up to constants not involving g. It is clear that attempting to maximize this loglikelihood
function over all smooth functions g is useless, since it is always possible to choose g sufﬁ-
ciently complicated that it interpolates the data, resulting in an arbitrarily large loglikelihood
value. An intuitive remedy is to add an additional term to the loglikelihood that penalizes

3

(cid:90)

n(cid:88)

i=1

functions g that are overly complicated. Perhaps the most popular and arguably most elegant
way of doing this is that of Green & Silverman (1994), which uses a penalty term proportional

to(cid:82) {g(cid:48)(cid:48)(t)}2dt. The relative merits of this penalty term are well-discussed in the literature,

for example, in Green & Silverman (1994, Section 3.8) and in Ruppert, Wand & Carroll (2003,
Section 3.15). The resulting penalized loglikelihood function is now of the form

lα(g) =

1
n

{g(ti)Yi − b(g(ti))} − 1
2

α

{g(cid:48)(cid:48)(t)}2dt ,

(2)

where α ≥ 0 is a smoothing parameter. Small values of α lead to mean curves that ad-
here quite closely to the data, whereas increasing the value of α leads to increasingly linear
functions g on the canonical scale. How this translates to the smoothness of the mean curve
depends on the link function, b(cid:48), which is in turn determined by the assumed distribution.

Maximizing (2) over functions g in the space of all continuous, twice-differentiable func-
tions with absolutely continuously ﬁrst derivatives leads to a smoothing spline estimator of g,
also known as the natural cubic spline estimator. For more discussion on the theoretical and
practical properties of smoothing splines, see Green & Silverman (1994).

2.2 Doubly-nonparametric GLMs
A recent innovation by Rathouz & Gao (2009) showed that any family of distributions with
densities of the form (1) can be rewritten as

dFi(y) = exp{−bi + g(ti)y}dF (y)

for some reference distribution F , where bi are normalizing constants given by

bi ≡ b(g(ti), F ) = log

exp{g(ti)y}dF (y) ,

i = 1, . . . , n .

(3)

(cid:90)

In other words, each density dFi is an exponential tilt of the reference density dF , with the
amount of tilting determined by the covariate value ti through g(ti). Note that the scale
parameter φ has been absorbed into the functions b, g and F . Note also that, as with any
GLM, the distribution F must have a Laplace transform in some neighborhood of 0, so that
the cumulant generating function in (3) is well-deﬁned.

The key advantage of this exponential tilt representation is that it makes apparent the idea
that the reference distribution F can itself be considered an inﬁnite-dimensional parameter
in the model. Indeed, we can now write the loglikelihood as a function of both g and F ,

{log dF (Yi) − b(g(ti), F ) + g(ti)Yi} ,

(4)

n(cid:88)

i=1

l(g, F ) =

1
n

with b(g(ti), F ) given by (3). Treating F as a free parameter introduces much ﬂexibility and
robustness into the model. For example, overdispersed count data can be dealt with simply
by F having heavier tails than a Poisson distribution. Similarly, zero-inﬂated counts can be
dealt with simply by F having excess probability mass at zero. Most remarkable, perhaps,
is that F can be left completely unspeciﬁed and estimated nonparametrically from the data,
along with the mean-curve. That is, we let the data inform us as to which mean-curve and
response distribution ﬁt best.

As with classical nonparametric GLMs, the loglikelihood function (4) can be made arbi-
trarily large for any given F by choosing g to be sufﬁciently complicated. Hence, we consider
a doubly-nonparametric analogue of the penalized loglikelihood function (2) given by

(cid:80)n
i=1{log dF (Yi) − b(g(ti), F ) + g(ti)Yi} − 1

2 α(cid:82) {g(cid:48)(cid:48)(t)}2dt ,

lα(g, F|t, Y ) = 1
with b(g(ti), F ) given by (3) .

n

(5)

4

A doubly-nonparametric estimator of g and F can then be deﬁned as the joint maximizer
of the penalized loglikelihood (5), where the maximization in g is taken over S2, the space of
all continuous, twice-differentiable functions with absolutely continuously ﬁrst derivatives,
and the maximization in F is taken over F, the space of all distributions having a Laplace
transform in some neighborhood of 0. Working in this doubly inﬁnite-dimensional space
appears at ﬁrst to be an intractable task, but as we show in Proposition 1 below, this is re-
ducible to a ﬁnite maximization problem involving at most 2n − 1 variables, where n is the
sample size. As a comparison, simultaneous estimation of the mean and the variance func-
tion using smoothing splines generally requires an optimization over 2n variables, which is
of the same order. Note that all functions involved in the simpliﬁed optimization problem
of Proposition 1 below have explicit forms, making it easily implementable in mathematical
computing software such as MATLAB.
Proposition 1. Denote by (ˆg, ˆF ) the maximizer of the penalized likelihood (5) over S2 × F. Then,
(i) ˆF is necessarily a probability mass function {ˆp1, . . . , ˆpn} on the observations Yi, . . . , Yn;
(ii) ˆg is necessarily a natural cubic spline with knots at the observations t1, . . . , tn; and
(iii) the penalized loglikelihood at the maximum reduces to the form

n(cid:88)

i=1

n(cid:88)

lα(ˆg, ˆp) =

1
n

{log ˆpi − b(ˆgi, ˆp) + giYi} − 1
2

αˆgT K ˆg ,

with b(ˆgi, ˆp) = log

exp{ˆgiYj}ˆpj ,

i = 1, . . . , n ,

j=1

and ˆg = (ˆg1, . . . , ˆgn)T denotes a vector of knot values. Note that(cid:80)n

where ˆp = (ˆp1, . . . , ˆpn)T , K is a known, symmetric, banded matrix depending only on the covariates,
i=1 pi = 1, so that the maximiza-
tion problem involves at most 2n − 1 variables, with this number being smaller when there are ties in
the dataset.

Proof. For ﬁxed g, Vardi (1985) showed that the maximizer ˆF of the unpenalized log-
likelihood function (4) is necessarily a probability mass function {ˆp1, . . . , ˆpn} on the ob-
served support Y1, . . . , Yn. Thus, for ﬁxed g, the loglikelihood portion of lα at the maxi-
i=1{log ˆpi − b(g(ti), ˆp) + giYi} with b(g(ti), ˆp) =
j=1 exp{g(ti)Yj}ˆpj, the empirical analogue of (3). On the other hand, for ﬁxed distribu-
tion F , Green & Silverman (1994) showed that the maximizer of lα over g must be a natural
cubic spline with knots at the observed covariates, t1, . . . , tn. Moreover, the penalty term

mum must be of the empirical form n−1(cid:80)n
log(cid:80)n
(cid:82) {g(cid:48)(cid:48)(t)}2dt necessarily reduces to the quadratic expression ˆgT K ˆg, where ˆg = (ˆg1, . . . , ˆgn)T

denotes the vector of knot values at the observed covariates and K is a matrix given in Green
& Silverman (1994, Section 2.1.2) that depends only on the covariates. Combining these two
arguments gives the result.

mated cumulant generating function by b(ˆg(t), ˆp) = log(cid:80)n

To translate the estimated natural cubic spline ˆg(t) into a mean curve, deﬁne the esti-
i=1 exp{ˆg(t)Yi}pi. The doubly-
nonparametric estimator of the mean µ(t) = E(Y |t) and error distribution Ft(y) = Pr(Y ≤
y|t) at covariate value t can then be computed as

ˆµ(t) =

ˆFt(y) =

Yi exp{−b(ˆg(t), ˆp) + ˆg(t)Yi}ˆpi

and

1(Yi ≤ y) exp{−b(ˆg(t), ˆp) + ˆg(t)Yi}ˆpi ,

n(cid:88)
n(cid:88)

i=1

i=1

respectively. The ﬁtted model also induces a variance function for the data, given by

ˆV (t) = ˆVar(Y |t) =

(Yi − ˆµ(t))2 exp{−b(ˆg(t), ˆp) + ˆg(t)Yi}ˆpi .

n(cid:88)

i=1

5

Variance functions are sometimes of direct interest (e.g., Ruppert, Wand, Holst & H¨ossjer,
1997), but usually play more of a secondary role in the computation of variability bands for
inferences on the mean.

In all of the above, and throughout the rest of this paper, we treat the smoothing param-
eter α as being given. There are a few competing ways to choose the smoothing parameter
in practice, with perhaps the most popular approach being cross-validation. We postpone
discussion on practical selection of the smoothing parameter until Section 8.

It is worth mentioning here that the exponential tilt representation is also used in Huang
(2014) to develop a semiparametric extension of GLMs in which the mean function is para-
metric but the response distribution is nonparametric. The current proposal is therefore an
extension of this that allows both components to be nonparametric. Having a parametric
form for the mean curve is particularly useful in biostatistical, agricultural and engineering
contexts where explicit treatment effects are of interest, but the doubly-nonparametric ap-
proach can still be useful for model diagnosis and model selection in those scenarios. The
proposed approach also extends the semiparametric proportional likelihood ratio model of
Luo & Tsai (2012), and subsequent work by Chan (2013) and Davidov & Ilipoulos (2013),
by letting the canonical parameter θ be an arbitrary smooth function of t rather than setting
θ = tβ for some β.

3

Implementation via existing algorithms

(cid:17)−1(cid:16)

Y − µ(m)(cid:17)

An attractive feature of the doubly-nonparametric approach is that it can be readily ﬁtted
using existing algorithms. More precisely, computations can be carried out via the following
two steps. First, for ﬁxed empirical distribution p, optimization over g can be achieved by
Fisher scoring (see Green & Silverman, 1994, Theorem 5.1), whereby a current estimate g(m)
of g is updated via

(cid:16)

(cid:17)−1

(cid:16)

g(m+1) =

W (m) + αK

W (m)g(m) +

W (m) + αK

.

(6)

(cid:80)n
Here, µ(m) is the current mean vector with components
(cid:80)n
i Yj}pj
j=1 Yj exp{g(m)
j=1 exp{g(m)
i Yj}pj
(cid:17)2
(cid:16)
and W (m) is a diagonal matrix of the current variances with
(cid:80)n
Yj − µ(m)
exp{g(m)
i Yj}pj
j=1 exp{g(m)

(cid:80)n

µ(m)
i

W (m)

ii =

j=1

=

i

,

i Yj}pj

.

Second, for ﬁxed vector g, it can be shown via Lagrangian multipliers (e.g. Fokianos et al.,

2001) that the optimizer in p is of the form

where b = (b1, . . . , bn)T satisﬁes

n(cid:88)

j=1

pi =

1

(cid:80)n
k=1 exp{−bk + gkYi} ,
(cid:80)n
exp{−bi + giYj}
k=1 exp{−bk + gkYj} = 1 .

(7)

(8)

The doubly-nonparametric MLE can then be obtained by iterating between the two steps,
(6) and (7)–(8); see Green & Silverman (1994) and Fokianos et al. (2001) for more details on the
computation of the individual steps. For smaller sized problems, such as those with less than
100 observations, we found that maximizing over g and p jointly can work just as well as this
iterative scheme, although for larger sample sizes the iterative scheme is more efﬁcient.

6

4 Variability bands for the mean

−1 V [V + αK]

For given F , it can be shown (c.f. Hastie & Tibshirani, 1990, equation (6.26)) that the co-
−1, where V =
variance matrix Vg of ˆg is approximately Vg ≈ [V + αK]
Diag(Var(Y1), . . . , Var(Yn)) is a diagonal matrix of the variances. Moreover, each ˆgi − E(ˆgi) is
approximately N (0, V i
g denotes the ith diagonal entry of
Vg. Thus, approximate 95% pointwise upper and lower variability bounds for each ˆgi ≡ ˆg(ti)
can be computed as gU
g )1/2, respectively, as in classical
smoothing spline GLMs. We can then translate these bounds into upper and lower pointwise
variability bounds for each ˆµi via

g ) in distribution for large n, where V i
i = ˆgi − 2(V i
(cid:90)

i = ˆgi + 2(V i

g )1/2 and gL

(cid:90)

y exp(cid:8)−b(gU

y exp(cid:8)−b(gL

i y(cid:9) dF (y) ,

i y(cid:9) dF (y)

µU

i =

i , F ) + gU

and µL

i =

i , F ) + gL

respectively. For moderately large datasets, interpolating each pointwise upper or lower
bound values using piecewise linear functions generally sufﬁces to produce a decent picture
of the variability around the ﬁtted mean curve. Other more sophisticated methods exist, but
it is not apparent that the associated accuracy gain are worth the additional computational
costs.

Since F is not known in the doubly-nonparametric case, we perform the above calcula-
tions using the plug-in estimator ˆF instead to obtain approximate, pointwise, conditional
95% variability bands for the estimated mean. This is how the variability band in the LIDAR
example in Figure 1 is computed. The plug-in method seems to work reasonably well even
for moderately small sample sizes, as our examples in Section 6 indicate. This is consistent
with the ﬁnding in Huang & Rathouz (2013) that the mean and error distribution in any GLM
are always orthogonal. Note that variability bands can be interpreted as conﬁdence bands
if the target function is taken to be E(ˆg) rather than the true value g∗ (see Ruppert, Wand &
Carroll, 2003, Section 6)

5 Two useful properties

5.1 Automatic preservation of support bounds
Suppose the support of the responses is constrained in a subset of R of the form (−∞, U ],
[L, U ] or [L,∞), where L and/or U are not necessarily known. An attractive property of the
proposed approach is that it automatically produces estimated mean curves and variability
bands that are contained within the same interval. This is due to the exponential tilt mecha-
nism which preserves the underlying support and the fact that the estimated distribution ˆF
only places positive mass on the observed support.

5.2 Invariance to shift and scaling
The exponential tilt approach to GLMs is also invariant to location-shifts and scaling of the
response. This is generally not true of GLMs with a given exponential family or QL models
with a given variance function.

Yi|ti ∼ Gamma(cid:8)mean = µ(ti), var = φµ(ti)2(cid:9), we observe ˜Yi = aYi +b for some scalars a and

For example, suppose that instead of observing data from the Gamma regression model

b. Then, it is easy to see that the observed data (ti, ˜Yi) no longer come from a Gamma model.
Moreover, the observed data do not come from the same QL family as the original data.
Modeling such data with a classical Gamma GLM would require an appropriate recentering
and rescaling of the data, a task that is not trivial if a and b are unknown. In contrast, the
transformed data can be directly analyzed by the proposed approach, with the estimated

7

mean-curve and variability bands being precisely the appropriate shifted and scaled version
of the mean-curve and variability bands for the original, unobserved data. The same is also
true for shifting and scaling of the predictors ti. The doubly-nonparametric approach is
invariant to shifting and scaling of the data.

6 Estimation accuracy, coverage rates and robustness to
model misspeciﬁcation

The doubly-nonparametric GLM is an extension of the parametric GLMs of McCullagh &
Nelder (1989), the nonparametric GLMs of Green & Silverman (1994), the semiparametric
GLMs of Huang (2014) and the semiparametric proportional likelihood ratio models of Luo
& Tsai (2012). Thus, the approach is expected to be ﬂexible enough to handle a very wide
range of scenarios. Here, we examine the practical performance of the proposed approach
using various simulations. Recently, Huang & Rathouz (2013) showed that estimation and in-
ferences for a parametric GLM can remain asymptotically efﬁcient even in the presence of an
inﬁnite-dimensional error distribution parameter. Our simulation results here suggest that
nonparametric estimation of the error distribution also has minimal effect on the estimation
of and inferences on a nonparametric mean-curve.

In all our simulations, the covariates t1, . . . , tn are equally-spaced between 0 and 1 with a
sample size n = 80, and the mean-curve is taken to be the “bump function” from Ruppert,
Wand & Carroll (2003, Section 5.6.3), which is given by

E(Y |t) = µ(t) =

1

0.1 + t

+ 8 exp(cid:8)−400(x − 0.5)2(cid:9) .

(9)

The mean-curve (9) comprises of a sudden peak in the middle of an otherwise smoothly
decaying trend and is a good example for testing the local and global properties of nonpara-
metric regression methods. It is plotted as the dashed curve in Figure 2.

To examine practical performance in both correctly-speciﬁed and misspeciﬁed scenarios,
we simulate data under two classical nonparametric GLM settings (settings 1 and 2) and two
misspeciﬁed settings (settings 3 and 4):

1. Y |t ∼ N{µ(t), 1}
2. Y |t ∼ Poisson{µ(t)}
3. Y |t ∼ negative-binomial{mean = µ(t), variance = µ(t) + µ(t)2/ν} with ν = 10
4. Y |t ∼ N{µ(t), 1 + 0.7µ(t) + 0.5µ2(t)}

Setting 3 here reﬂects the popular use of the negative-binomial distribution as a model for
overdispersed or underdispersed count data relative to the Poisson distribution. Note that
the negative-binomial is not a GLM unless the dispersion parameter ν is known a priori, so
that this setting is outside our model space. Note also that setting ν = 10 here corresponds to
moderate overdispersion. In Setting 4, the data come from a normal distribution but the vari-
ance is a quadratic function of the mean. This heteroscedastic normal family is also outside
the model space for both classical nonparametric and the proposed douby-nonparametric
frameworks.

Each simulated dataset was modeled using (i) a nonparametric GLM assuming a constant
variance function V (µ) = σ2, (ii) a nonparametric GLM assuming quasipoisson variance
V (µ) = φµ, and (iii) a doubly-nonparametric GLM with no assumption on the variance
function. Note that method (i) is correctly speciﬁed for Setting 1, method (ii) is correctly
speciﬁed for Setting 2, and all three methods are misspeciﬁed for Settings 3 and 4.

The value of the smoothing parameter used in each approach was ﬁxed at the value that
minimized the average root (weighted) mean-square error (RMSE) across the simulations.

8

Figure 2: Fitted model (solid) and 95% variability band (shaded) for a heteroscedastic dataset
from Setting 4, with true mean-curve (dashed) and response distributions (dashed) at the quin-
tiles. Sample size n = 80, smoothing parameter α = 7.35 × 10−6.

Recall that the root weighted mean-square error is deﬁned by

(cid:118)(cid:117)(cid:117)(cid:116) 1

n

RMSE =

n(cid:88)

i=1

(ˆµi − µ∗
i )2
V ∗

i

,

i

where for each observation i, ˆµi and µ∗
i are the ﬁtted and true values of the mean, respectively,
and V ∗
is its true variance. Of course, we never know the true mean-curve µ∗ in practice, nor
do we know the true variance V ∗ for assessing the appropriate goodness-of-ﬁt, but the choice
of smoothing parameter here allows us to make fair comparisons using the best possible ﬁts
under all three methods. The MATLAB code used to ﬁt each model can be obtained by
emailing the author.

Pointwise 95% variability bands for the mean-curve were then constructed using either
the method in Green & Silverman (1994, Section 5) for classical nonparametric GLMs or the
prescription in Section 4 for the doubly-nonparametric approach. Coverage rates of these
variability bands for the true mean at three locations, namely t = 0.4, 0.5 and 0.75, corre-
sponding to a trough, a sudden peak and an area of steady decline, respectively, are given in
Table 1. The table also displays the best RMSE achieved by each method.

We see from Table 1 that the doubly-nonparametric approach can perform as well as a
correctly-speciﬁed model for both estimation and inference. For misspeciﬁed models, its
performance is consistently better than classical approaches with the wrong response distri-
bution or variance function. In particular, coverage rates for classical nonparametric GLMs
can be quite disastrous under model misspeciﬁcation.

To gain some insight into how the doubly-nonparametric approach deals with model
misspeciﬁcation, the default graphical output from the MATLAB routine dnpglm.m for a

9

0.10.20.30.40.50.60.70.80.91−505101520Data (dots) and fitted mean curve (line)ty0102000.20.40.60.81Estimated distribution at t = 0.2mean = 2.1786, sd =2.71720102000.20.40.60.81Estimated distribution at t = 0.4mean = 2.2844, sd =2.78170102000.20.40.60.81Estimated distribution at t = 0.6mean = 2.5827, sd =2.98590102000.20.40.60.81Estimated distribution at t = 0.8mean = 1.6721, sd =2.4781generic dataset under Setting 4 is given in Figure 2. We see that the doubly-nonparametric
approach tries to ﬁnd an exponential family that best approximates the N (µ, 1+0.7µ2+0.5µ2)
subfamily across the range of means µ in the dataset. Of course, it can never replicate the true
response distribution exactly because it is outside the model space, but it does a surprisingly
good job in approximating it. This reinforces the versatility and ﬂexibility of exponential
families for modeling data, as suggested in Hiejima (1997).

7 Extension to multiple covariates

Two popular ways of extending classical nonparametric GLMs to multiple covariates are
generalized additive models (Hastie & Tibshirani, 1990) and reducing the dimensionality
of the problem using linear predictors (e.g., Green & Silverman, 1994, Section 6.4). These
two approaches are directly applicable to the doubly-nonparametric case also, with minimal
additional effort. For notational convenience, write ti = (t1
i )T to be a d-vector of
covariates for observation i.

i , . . . , td

(cid:80)d

7.1 Additive models
Additive models make the assumption that the natural parameter θi has the form θi =
i ) for a set of smooth but otherwise unspeciﬁed functions {g1, . . . , gd}. That is, the
effect of each covariate on the response is additive on the natural, canonical scale. The cor-
responding doubly-nonparametric penalized loglikelihood function for {gj} and F is then
given by

j=1 gj(tj

log dF (Yi) − b({gj(ti)}, F ) +

gj(tj

i )Yi

− 1
2

αj

{g(cid:48)(cid:48)

j (t)}2dt ,

(cid:26)

n(cid:88)

i=1

lα({gj}, F ) =

1
n

where

(cid:27)

(cid:90)

d(cid:88)

j=1

d(cid:88)
(cid:27)

j=1

b({gj(ti)}, F ) = log

gj(tj

i )y

dF (y) ,

i = 1 . . . , n.

(cid:90)

(cid:26) d(cid:88)

j=1

exp

Note that α = (α1, . . . , αd)T is now a d-vector of smoothing parameters, one for each covari-
ate.
By a similar argument to Proposition 1, maximizing this penalized loglikelihood over the
d + 1 inﬁnite-dimensional parameters in {gj} and F reduces to a ﬁnite optimization over at
most (d + 1)n − 1 variables. As with classical nonparametric GLMs, additive models can
quickly become computationally unfeasible as the number d of covariates increases. How-
ever, if some of the covariates only take on a small number of values, such as those that are
binary or ﬁxed by design, then the additional computational burden corresponding to those
covariates is relatively minimal. For a general discussion on additive models, see Hastie &
Tibshirani (1990).

7.2 Linear predictors and nonparametric links
A computationally less demanding approach to handle multiple covariates is to reduce di-
mensionality through the use of a linear predictor. For any vector γ ∈ Rd, deﬁne si(γ) = tT
i γ
to be the associated linear combination of the covariates ti, which is called a linear predictor.
The natural parameter θi of the exponential family is then assumed to be related to the linear
predictor through θi = g(si(γ)), where g is some smooth but unspeciﬁed function. Note that
for each ﬁxed γ, the problem reduces to the univariate problem of Section 2.2. Thus, the
extension here corresponds an additional level of optimization in d dimensions over γ. This
approach is essentially equivalent to using a nonparametric link function.

10

8 Choosing the smoothing parameter

8.1 Cross-validation via Pearson residuals
An excellent summary of the competing approaches to smoothing parameter selection in
smoothing spline GLMs is given in Green & Silverman (1994, Section 3.1). The points raised
there apply equally to the doubly-nonparametric approach.
In particular, we are sympa-
thetic to the philosophical view that the free choice of smoothing parameter is an advan-
tageous feature of the method that allows different features of the data to be explored on
different scales, and that “it may well be that such a subjective approach is in reality the
most useful one”. However, a more pragmatic statistician may insist on the need for an
automatic method, whereby the smoothing parameter is chosen by the data. To this end,
cross-validation, perhaps the most popular method for automatic selection, is also applica-
ble in the doubly-nonparametric setting. We discuss one approach to cross-validation here.
We also make a brief comment on the sensitivity of the approach to the choice of smoothing
parameter.

A preliminary step that we have found useful in both reducing computation time and
to “standardize” the magnitude of the smoothing parameter across different problems is to
ﬁrst rescale both the covariates and responses to the unit interval [0, 1]. This rescaling, and
back-transforming on to the original scale, is done internally by the MATLAB routine that
ﬁts the doubly-nonparametric GLM.

One way to implement cross-validation for selecting the smoothing parameter in the
doubly-nonparametric framework is an adaptation of least-squares cross-validation for
smoothing splines (e.g. Green & Silverman, 1994, Section 3.2) using Pearson residuals. Pear-
son residuals are more appropriate than ordinary residuals when the variance is not assumed
constant. For given α, let ˆµ(−i)(t; α) and ˆV (−i)(t; α) be the estimated mean curve and vari-
ance function, respectively, obtained by omitting observation i. A cross-validation score for
α based on Pearson residuals can then be deﬁned as

CV (α) =

.

(cid:8)Yi − ˆµ(−i)(ti; α)(cid:9)2

ˆV (−i)(ti; α)

n(cid:88)

i=1

This cross-validation score can then be minimized over α > 0, leading a data-dependent
“optimal” choice for α.
In practice, a grid search over a suitable range of α values often
sufﬁces.

As with any smoothing method, computational burden can be reduced by cross-
validating more than one observation at a time. For example, two-fold cross-validation, in
which half the observations are omitted at a time, would require only two passes through the
data per split. The smoothing parameter used to produce Figure 1 was chosen in this man-
ner via minimizing a two-fold cross-validation score over α values between 0.01n−4/5 and
0.50n−4/5 in steps of 0.01n−4/5, where we have used the optimal rate n−4/5 from penalized
smoothing splines (Claeskens, Krivobokova & Opsomer, 2009) as a guide for the range of α
to search over.

8.2 Sensitivity to the smoothing parameter
The smoothing parameter in doubly-nonparametric GLMs plays a similar role in controlling
the smoothness of the mean curve as it does in classical smoothing spline nonparametric
GLMs. Interestingly, we have found that its effect on the estimated response distributions to
be much less pronounced. This perhaps alludes to a doubly-nonparametric analogue of the
recent discovery in Huang & Rathouz (2013) which showed that a parametric mean function
is orthogonal to the response distribution in any GLM. This also reinforces that there may be

11

little to lose by adopting the more general doubly-nonparametric framework over classical
nonparametric GLMs.

9 Discussion

The smoothing spline estimator used in this paper is one of many approaches that can be
used in the doubly-nonparametric framework for estimating the mean curve. An alternative
method is to use low-ranked penalized splines with a ﬁxed number of knots. This may
reduce computational burden at the cost of decreased ﬂexibility. Kernel-type smoothers,
however, are not compatible with the doubly-nonparametric framework because they are
usually explicitly constructed from data rather than via optimizing a (penalized) likelihood
function. For the error distribution, empirical likelihood is also one of many approaches that
can be used in the doubly-nonparametric framework. Other approaches include mixture
models and series expansions. Again, ﬁnite (or truncated) versions of these methods can
increase computational efﬁciency at the cost of decreased ﬂexibility. The development of
software to incorporate these variations is currently work in progress.

Acknowledgements

The author thanks Professor Peter Green for discussions that led to the writing of this paper
and Professors Matt Wand, Paul J. Rathouz and Joe Guinness for comments that improved
the paper.

Bibliography
Chan, K. C. G. (2013) Nuisance parameter elimination for proportional likelihood ratio models with

nonignorable missingness and random truncation. Biometrika, 100, 269–276.

Chiou, J. and M ¨uller, H. (1998) Quasi-likelihood regression with unknown link and variance func-

tions. Journal of the American Statistical Association. 93, pp. 1376-1387.

Claeskens, G., Krivobokova, T. and Opsomer, J. D. (2009) Asymptotic properties of penalized spline

estimators. Biometrika. 96, pp. 529-544.

Davidov, O and Iliopoulos, G. (2013) Convergence of Luo and Tsai’s iterative algorithm for estimation

in proportional likelihood ratio models. Biometrika, 100, 778–780.

Eicker, F. (1967) Limit theorems for regressions with unequal and dependent errors. Proc. Fifth Berkeley

Symp. on Math. Statist. and Prob., 1, 59–82.

Fokianos, K, Kedem, B, Qin, J. and Short, D. A. (2001) A semiparametric approach to the one-way

layout. Technometrics, 43, 56–65.

Green, P. J. and Silverman, B. W. (1994) Nonparametric Regression and Generalized Linear Models. Boca

Raton: Chapman and Hall.

Hastie, T. J. and Tibshirani, R. J. (1990) Generalized Additive Models. Boca Raton: Chapman and Hall.
Hiejima Y. (1997) Interpretation of the quasi-likelihood via the tilted exponential family. Journal of the

Japan Statistical Society, 2, 157–164.

Huang, A. (2014) Joint estimation of the mean and error distribution in generalized linear models.

Journal of the American Statistical Association., 109, 186–196.

Huang, A. and Rathouz, P. J. (2013) Orthogonality of the mean and error distribution in generalized

linear models. Communications in Statistics: Theory and Methods. (to appear).

Luo, X. and Tsai, W. Y. (2012). A proportional likelihood ratio model. Biometrika, 99, 211–222.
McCullagh, P. and Nelder, J. A (1989) Generalized Linear Models. 2nd edition. London: Chapman and

Hall.

Rathouz, P. J. and Gao. L. (2009) Generalized linear models with unspeciﬁed reference distribution.

Biostatistics. 10, pp. 205–218.

Ruppert, D., Wand, M. P., and Carroll, R. J. (2003) Semiparametric Regression. New York: Cambridge

University Press.

12

Table 1: Coverage rates (%) for pointwise 95% variability bands for the mean at t = 0.4, 0.5
and 0.75, with corresponding average root weighted mean-squared errors (RMSE), for classical
nonparametric (NP) and doubly-nonparametric (DNP) GLMs. N=5,000 simulations for each
setting. Sample size n = 80 for each simulation.

working

1. Normal

2. Poisson

NP

method variance t=0.4 t=0.5 t=0.75 RMSE t=0.4 t=0.5 t=0.75 RMSE
0.417
0.371
0.382

86.2 27.9
88.1 87.1
89.5 79.2

96.8 53.8
99.2 99.1
92.6 52.1

0.475
0.414
0.456

96.2
96.7
93.9

97.5
95.8
93.5

σ2
φµ
—

DNP

working

3. Negative-binomial

4. Heteroscedastic normal
method variance t=0.4 t=0.5 t=0.75 RMSE t=0.4 t=0.5 t=0.75 RMSE
0.383
0.341
0.346

64.4 23.1
30.5 30.2
71.5 68.4

82.9 26.4
75.3 69.7
83.8 80.2

0.398
0.356
0.354

98.8
90.8
94.5

97.2
53.4
96.7

σ2
φµ
—

NP

DNP

Ruppert, D., Wand, M. P., Holst, U., & H¨ossjer, O. (1997) Local polynomial variance-function estima-

tion. Technometrics. 39, pp. 262–273.

Vardi, Y. (1985) Empirical distributions in selection bias models. Annals of Statistics 13, pp. 178-203.
White, H. (1982) Maximum likelihood estimation of misspeciﬁed models. Econometrica 50, pp. 1-25.

13

