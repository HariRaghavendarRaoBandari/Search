Optimized Sensor Collaboration for Estimation of

Temporally Correlated Parameters

Sijia Liu Student Member, IEEE, Swarnendu Kar, Member, IEEE, Makan Fardad, Member, IEEE,

and Pramod K. Varshney Fellow, IEEE

1

6
1
0
2

 
r
a

 

M
0
1

 
 
]
T
I
.
s
c
[
 
 

1
v
8
4
4
3
0

.

3
0
6
1
:
v
i
X
r
a

Abstract—In this paper, we aim to design the optimal sen-
sor collaboration strategy for the estimation of time-varying
parameters, where collaboration refers to the act of sharing
measurements with neighboring sensors prior to transmission to
a fusion center. We begin by addressing the sensor collaboration
problem for the estimation of uncorrelated parameters. We
show that the resulting collaboration problem can be trans-
formed into a special nonconvex optimization problem, where
a difference of convex functions carries all the nonconvexity.
This speciﬁc problem structure enables the use of a convex-
concave procedure to obtain a near-optimal solution. When the
parameters of interest are temporally correlated, a penalized
version of convex-concave procedure becomes well suited for
designing the optimal collaboration scheme. In order to improve
computational efﬁciency, we further propose a fast algorithm
that scales gracefully with problem size via the alternating
direction method of multipliers. Numerical results are provided to
demonstrate the effectiveness of our approach and the impact of
parameter correlation and temporal dynamics of sensor networks
on estimation performance.

Index Terms—Distributed estimation, sensor collaboration,
temporal correlation, convex-concave procedure, semideﬁnite
programming, alternating direction method of multipliers, wire-
less sensor networks.

I. INTRODUCTION

Wireless sensor networks (WSNs) consist of a large number
of spatially distributed sensors that often cooperate to perform
parameter estimation; example applications include environ-
ment monitoring, source localization and target tracking [1]–
[3]. Under limited resources such as limited communication
bandwidth and sensor battery power, it is important to de-
sign an energy-efﬁcient architecture for distributed estimation.
In this paper, we employ a WSN to estimate time-varying
parameters in the presence of inter-sensor communication
that is referred to as sensor collaboration. Here sensors are
allowed to update their measurements by taking a linear
combination of the measurements of those they interact with
prior to transmission to a fusion center (FC). The presence
of sensor collaboration smooths out the observation noise,

Copyright (c) 2015 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending a request to pubs-permissions@ieee.org.
S. Liu, M. Fardad and P. K. Varshney are with the Department of Electrical
Engineering and Computer Science, Syracuse University, Syracuse, NY, 13244
USA e-mail: {sliu17, makan, varshney}@syr.edu.

S. Kar is with New Devices Group, Intel Corporation, Hillsboro, Oregon,

97124 USA email: swarnendu.kar@intel.com.

The work of S. Liu and P. K. Varshney was supported by the U.S. Air Force
Ofﬁce of Scientiﬁc Research under grants FA9550-10-1-0458. The work of
M. Fardad was supported by the National Science Foundation under awards
EAGER ECCS-1545270 and CNS-1329885.

thereby improving the quality of the signal and the eventual
estimation performance.

Early research efforts [4]–[9] focused on the problem of
distributed estimation in the absence of sensor collaboration,
where an amplify-and-forward transmission strategy is com-
monly used. The key problem in this setting is to design
optimal power amplifying factors under certain performance
criteria, such as estimation distortion and energy cost. In [4],
the optimal power allocation scheme was proposed for dis-
tributed estimation over an orthogonal multiple access channel
(MAC). In [5], the problem of power allocation was studied
when MAC is coherent, where sensors coherently form a
beam into a common channel received at the FC. In [6] and
[7], quantized measurements were transmitted to the FC for
distributed estimation. In [8] and [9], optimal power allocation
was studied in sensor networks with energy harvesting nodes.
Recently, the problem of distributed estimation with sensor
collaboration has attracted attention [10]–[18]. In [10], the op-
timal power allocation strategy was found for a fully connected
network, where all the sensors are allowed to collaborate,
namely, share their measurements with the other sensors.
It was shown that sensor collaboration results in signiﬁ-
cant improvement of estimation performance compared with
the conventional amplify-and-forward transmission scheme.
In [11] and [12], optimal power allocation schemes were
found for star, branch and linear network topologies. In [13],
the sensor collaboration problem was studied for parameter
estimation via the best linear unbiased estimator. In [14]–
[16], the problem of sensor collaboration was studied given
an arbitrary collaboration topology. It was observed that even
a partially connected network can yield performance close to
that of a fully connected network. In [17] and [18], nonzero
collaboration costs were taken into account, and a sparsity
inducing optimization framework was proposed to jointly
design both sensor selection and sensor collaboration schemes.
In the aforementioned literature [10]–[18], sensor collabora-
tion was studied in static networks, where sensors take a single
snapshot of the static parameter, and then initiate sensor col-
laboration protocols designed in the setting of single-snapshot
estimation. In contrast, here we study the problem of sensor
collaboration for the estimation of time-varying parameters
in dynamic networks that involve, for example, time-varying
observation and channel gains. Solving such a problem is
also motivated by real-life applications, in which the physical
phenomenon to be monitored such as daily temperature and
precipitation [19]–[21] is temporally correlated. Due to the
presence of temporal dynamics and parameter correlation, the

design of optimal sensor collaboration schemes at multiple
time steps are coupled with each other, and thus poses
many challenges in problem formulation and optimization
compared to the previous work [10]–[18]. For example, when
parameters of interest are temporally correlated, expressing the
estimation distortion in a succinct closed form (with respect
to collaboration variables) becomes intractable. It should be
pointed out that even for uncorrelated parameters, ﬁnding the
optimal collaboration scheme for each time step is nontrivial
since energy constraints are temporally inseparable. In this
paper, we seek the optimal sensor collaboration scheme by
minimizing the estimation distortion subject
to individual
energy constraints of sensors in the presence of (a) temporal
dynamics in system, (b) temporal correlation of parameter, and
(c) energy constraints in time.

In our work, design of the optimal collaboration scheme
is studied under two scenarios: a) parameters are temporally
uncorrelated or the prior knowledge about temporal correlation
is not available, and b) parameters are temporally correlated.
When parameters are uncorrelated, we derive the closed form
of the estimation distortion with respect to sensor collaboration
variables, which is in the form of a sum of quadratic ratios.
We show that the resulting sensor collaboration problem is
equivalent to a nonconvex quadratically constrained problem,
in which the difference of convex functions carries all the
nonconvexity. This speciﬁc problem structure enables the use
of convex-concave procedure (CCP) [22] to solve the sensor
collaboration problem in a numerically efﬁcient manner.

When parameters of interest are temporally correlated,
expressing the estimation error as an explicit function of
the collaboration variables becomes more difﬁcult. In this
case, we show that the sensor collaboration problem can be
converted into a semideﬁnite program together with a rank-
one (nonconvex) constraint. After convexiﬁcation, the method
of penalty CCP [23] becomes well suited for seeking the
optimal sensor collaboration scheme. However, the proposed
algorithm is computationally intensive for large-scale prob-
lems. To improve computational efﬁciency, we develop a fast
algorithm that scales gracefully with problem size by using
the alternating direction method of multipliers (ADMM) [24].

We summarize our contributions as follows.
• We propose a tractable optimization framework for the
design of the optimal collaboration scheme that accounts
for parameter correlation and temporal dynamics of sen-
sor networks.

• We show that the problem of sensor collaboration for the
estimation of temporally uncorrelated parameters can be
solved as a special nonconvex problem, where the only
source of nonconvexity can be isolated to a constraint
that contains the difference of convex functions.

• We provide valuable insights into the problem structure
of sensor collaboration with correlated parameters, and
propose an ADMM-based algorithm for improving the
computational efﬁciency.

The rest of the paper is organized as follows. In Section II,
we introduce the collaborative estimation system, and present
the general formulation of the optimal sensor collaboration
problem. In Section III, we discuss two types of sensor collab-

2

oration problems for the estimation of temporally uncorrelated
and correlated parameters. In Section IV, we study the sensor
collaboration problem with uncorrelated parameters. In Sec-
tion V, we propose efﬁcient optimization methods to solve the
sensor collaboration problem with correlated parameters. In
Section VI, we demonstrate the effectiveness of our approach
through numerical examples. Finally, in Section VII we sum-
marize our work and discuss future research directions.

II. SYSTEM MODEL

In this section, we introduce the collaborative estimation
system and formulate the sensor collaboration problem consid-
ered in this work. The task here is to estimate a time-varying
parameter θk over a time horizon of length K. In the esti-
mation system, sensors ﬁrst accquire their raw measurements
via a linear sensing model, and then update their observations
through spatial collaboration, where collaboration refers to the
act of sharing measurements with neighboring sensors. The
collaborative signals are then transmitted through a coherent
MAC to the FC, which ﬁnally determines a global estimate of
θk for k ∈ [K]. The overall architecture of the collaborative
estimation system is shown in Fig. 1.

Fig. 1: Collaborative estimation architecture.

The vector of measurements from N sensors at time k is

given by the linear sensing model

xk = hkθk + k, k ∈ [K],

(1)

where for notational simplicity, let [K] denote the integer set
{1, 2, . . . , K}, xk = [xk,1, . . . , xk,N ]T is the vector of mea-
surements, hk = [hk,1, . . . , hk,N ]T is the vector of observation
gains, θk is a random process with zero mean and variance
θ, k = [k,1, . . . , k,N ]T is the vector of Gaussian noises
σ2
with i.i.d variables k,n ∼ N (0, σ2
 ) for k ∈ [K] and n ∈ [N ].
After linear sensing, each sensor may pass its observation
to another sensor for collaboration prior to transmission to the
FC. With a relabelling of sensors, we assume that the ﬁrst
M sensors (out of a total of N sensor nodes) communicate
with the FC. Collaboration among sensors is represented by a
known adjacency matrix A ∈ RM×N with zero-one entries,
namely, Amn ∈ {0, 1} for m ∈ [M ] and n ∈ [N ]. Here
Amn = 1 signiﬁes that the nth sensor shares its observation
with the mth sensor. Conversely, Amn = 0 indicates the
absence of a collaboration link between the nth and mth
sensors.

random signalLinear sensingℎ𝑘,1𝜖𝑘,1{𝜃𝑘}𝑥𝑘,1linear spatial collaboration𝑔𝑘,1𝑧𝑘,𝑀𝑧𝑘,2𝑧𝑘,1𝑔𝑘,2𝑔𝑘,𝑀𝑊𝑘LMMSEestimate𝜍𝑘𝑦𝑘coherent-MACFC{ 𝜃𝑘}ℎ𝑘,2𝜖𝑘,2𝑥𝑘,2𝜖𝑘,𝑁𝑥𝑘,𝑁ℎ𝑘,𝑁Based on the adjacency matrix, the sensor collaboration

process at time k is given by

zk = Wkxk, k ∈ [K]
Wk ◦ (1M 1T

N − A) = 0,

(2)

where zk = [zk,1, zk,2, . . . , zk,M ]T , zk,m is the signal after
collaboration at sensor m and time k, Wk ∈ RM×N is the
collaboration matrix that contains collaboration weights (based
on the energy allocated) used to combine sensor measurements
time k, ◦ denotes the elementwise product, 1N is the
at
N × 1 vector of all ones, and 0 is the M × N matrix of all
zeros. In what follows, while refering to vectors of all ones
and all zeros, their dimensions will be omitted for simplicity
but can be inferred from the context. In (2), we assume that
sharing of an observation is realized through an ideal (noise-
less and cost-free) communication link. The proposed ideal
collaboration model enables us to obtain explicit expressions
for transmission cost and estimation distortion.

After sensor collaboration, the message zk is transmitted
through a coherent MAC so that the received signal yk at the
FC is a coherent sum [5]

yk = gT

k zk + ςk, k ∈ [K],

(3)

where gk = [gk,1,, gk,2, . . . , gk,M ]T is the vector of channel
gains, and ςk is temporally white Gaussian noise with zero
ς .
mean and variance σ2

We next deﬁne the transmission cost of the mth sensor at
time k, which refers to the energy consumption of transmitting
the collaborative message zk to the FC. That is,

Tm(Wk) = Eθk,k [z2
mWk(σ2

= eT

k,m]
θ hkhT

k + σ2

 IN )WT

(4)
for m ∈ [M ] and k ∈ [K], where em ∈ RM is a basis vector
with 1 at the mth coordinate and 0s elsewhere, and IN is
the N × N identity matrix. In what follows, while refering
to basis vector and identity matrix, their dimensions will be
omitted for simplicity but can be inferred from the context.

k em,

From (1) – (3), the vector of received signals at the FC can
be compactly expressed as a linear function of parameters θ =
[θ1, θ2, . . . , θK]T ,

y = DW Dhθ + ν, DW := blkdiag{gT

k Wk}K

k=1,

(5)

where y = [y1, y2, . . . , yK]T , ν = [ν1, ν2, . . . , νK]T , νk :=
k=1, and blkdiag{Xi}n
k Wkk + ςk, Dh := blkdiag{hk}K
gT
i=1
denotes the block-diagonal matrix with diagonal blocks
X1, X2, . . . , Xn.

At the FC, we employ a linear minimum mean squared-
error estimator (LMMSE) [25] to estimate θ, where we assume
that the FC knows the observation gains, channel gains, and
the second-order statistics of the parameters of interest and
additive noises. The corresponding estimator and estimation
error covariance are given by [25, Theorem 10.3]

θ + DT
θ + DT

h DT
h DT

W D−1
W D−1

ν DW Dh)−1DT
ν DW Dh)−1,

W D−1
υ y

h DT

(6)

(cid:26) ˆθW = (Σ−1

PW = (Σ−1

where Σθ represents the prior knowledge about the parameter
θ I for temporally uncorrelated
correlation, particularly Σθ = σ2

3

W + σ2

 DW DT

parameters, and Dν := σ2
ς I. It is clear from
(6) that both the LMMSE and the estimation error covariance
matrix are functions of collaboration matrices {Wk}, and their
dependence on {Wk} is through DW . This dependency does
not lend itself to easy optimization of scalar-valued functions
of PW for design of the optimal sensor collaboration scheme.
More insights into the LMMSE (6) will be provided in Sec. III.
We now state the main optimization problem considered in

this work for sensor collaboration

minimize

subject to

tr (PW )

K(cid:88)

Tm(Wk) ≤ Em,

k=1

Wk ◦ (1M 1T

N − A) = 0,

m ∈ [M ]
k ∈ [K],

(7)

where Wk is the optimization variable for k ∈ [K], tr(PW )
denotes the estimation distortion of using the LMMSE, which
has a special form (shown in Sec. III) if parameters are
uncorrelated or the correlation prior is not available, Tm(Wk)
is the transmission cost given by (4), Em is a prescribed
energy budget of the mth sensor, and A is the adjacency
matrix to characterize the network topology. We remark that
although sensor collaboration is performed with respect to a
time-invariant (ﬁxed) topology matrix A, energy allocation
in terms of the magnitude of nonzero entries in Wt is time
varying in the presence of temporal dynamics of the sensor
network (e.g., time-varying observation and channel gains).
As will be evident later, the proposed sensor collaboration
approach is also applicable to the problem with time-varying
topologies. The problem structure and the solution of (7) will
be elaborated on in the rest of the paper.

III. REFORMULATION AND SIMPLIFICATION USING

MATRIX VECTORIZATION

In this section, we simplify problem (7) by exploiting the
sparsity structure of the adjacency matrix and concatenating
the nonzero entries of a collaboration matrix into a collabora-
tion vector. We show that the resulting optimization problem
involves special types of nonconvexities.

Fig. 2: Example of vectorization of Wk.

In problem (7), the only unknowns are the nonzero entries
of collaboration matrices. Motivated by that, we concatenate
these nonzero entries (columnwise) into a collaboration vector

wk = [wk,1, wk,2, . . . , wk,L]T ,

(8)

123FC𝑨100011011𝑾𝑘→𝒘𝑘𝑤𝑘,1000𝑤𝑘,2𝑤𝑘,40𝑤𝑘,3𝑤𝑘,5𝑙𝑚𝑙𝑛𝑙111222332423533where wk,l denotes the lth entry of wk, and L is the number
of nonzero entries of the adjacency matrix A. We note that
given wk,l, there exists a row index ml and a column index
nl such that wk,l = [Wk]mlnl, where [X]mn (or Xmn)
denotes the (m, n)th entry of a matrix X. We demonstrate
the vectorization of Wk through an example in Fig. 2, where
we consider N = 3 sensor nodes, M = 3 communicating
nodes, and 2 collaboration links.

A. Collaboration problem for the estimation of uncorrelated
parameters

When the parameters of interest are uncorrelated, the esti-

mation error covariance matrix (6) simpliﬁes to

PW =(cid:0)σ−2
(cid:32)
(cid:26)

σ−2
θ I + diag

=

= diag

σ2
θ gT

(cid:26) gT

θ I + DT

w(σ2

w + σ2

h DT
k WkhkhT
 gT
t WkWT
σ2
k WkWT
 gT
θ σ2
σ2
k WkhkhT
k WT
k gk+σ2

 DwDT
k WT
k gk
k gt + σ2
ς
k gk + σ2
 gT

(cid:33)−1
(cid:27)K
ς I)−1DwDh

k=1

θ σ2
ς

k WkWT

k gk+σ2
ς

(cid:1)−1
(cid:27)K

,

k=1
(9)

where diag{ak}K
entries a1, a2, . . . , aK.

k=1 denotes a diagonal matrix with diagonal

It

is clear from (9) and (4) both the estimation error
covariance matrix and the transmission cost contain quadratic
matrix functions [26], which can be converted into quadratic
vector functions according to the relationship between Wk
and wk stated in Proposition 1.
Proposition 1: Let w ∈ RL be the vector of stacking the
nonzero entries of W ∈ RM×N columnwise, the expression
bT W can be written as a function of w,

bT W = wT B,

(10)
where b ∈ RN is a coefﬁcient vector, B is an L × N matrix
whose (l, n)th entry is given by

(cid:26) bml n = nl

Bln =

(11)
and the indices ml and nl satisfy that wl = Wmlnl for l ∈ [L].
(cid:4)
Proof: The proof follows from [14, Sec. III-A].
From (9) and Proposition 1, the objective function of prob-

otherwise,

0

lem (7) can be rewritten as

K(cid:88)

k=1

where we use the fact that gT
L × N matrix deﬁned as (11), Rk := GkGT
Gk(σ2
 I)GT
semideﬁnite matrices.

k Gk, Gk is an
k , and Sk :=
k . Clearly, both Rk and Sk are positive

k Wk = wT

θ hkhT

k + σ2

Moreover, the transmission cost (4) can be rewritten as

Tm(wk) := wT

k Qk,mwk,

(13)

where Qk,m := Em(σ2
as (11) such that eT
positive semideﬁnite for k ∈ [K] and m ∈ [M ].

m, and Em is deﬁned
k + σ2
k Em. We remark that Qk,m is

θ,khkhT
mWk = wT

 I)ET

φ(w) := tr(PW ) =

σ2
θ σ2

 wT
wT

k Rkwk + σ2
k Skwk + σ2
ς

θ σ2
ς

,

(12)

where Gk has been introduced in (12).

Combining (15) and (16), we can rewrite the estimation

error covariance as a function of the collaboration vector

4

From (12) and (13), the sensor collaboration problem for
the estimation of temporally uncorrelated parameters becomes

minimize φ(w)

K(cid:88)

subject to

k Qk,mwk ≤ Em, m ∈ [M ],
wT

(P1)

k=1
1 , wT

2 , . . . , wT

where w = [wT
K] is the optimization variable,
and φ(w) is the estimation distortion given by (12). Note
that (P1) cannot be decomposed in time since sensor energy
constraints are temporally inseparable.

Compared to problem (7), the topology constraint in terms
of the adjacency matrix is eliminated without loss of perfor-
mance in (P1) since the sparsity structure of the adjacency
matrix has been taken into account while constructing the
collaboration vector. (P1) is a nonconvex optimization problem
since its objective function is given by a sum of rational
functions [27]. In the case of single-snapshot estimation
(namely, K = 1), the objective function of (P1) simpliﬁes to a
single quadratic ratio. It has been shown in [14] and [18] that
such a nonconvex problem can be readily solved via convex
programming. In contrast, the presence of the sum of quadratic
ratios makes solving (P1) more challenging. We present the
solution to (P1) in Sec. IV.

B. Collaboration problem for the estimation of correlated
parameters

When parameters are temporally correlated, the covariance
matrix Σθ is no longer diagonal. As a result, expressing
the estimation error in a succinct form as in (12) becomes
intractable. We recall from (6) that the dependence of the es-
timation error covariance on collaboration matrices is through
DW . According to the matrix inversion lemma, we are able
to simplify (6) via the relationship
ς I)−1Dw
 I + σ4

W + σ2
 I − (σ2

 DW DT

= σ−2

ς DT

W (σ2

(14)

DT

 σ−2
Substituting (14) into (6), we obtain
ς DT

PW =(cid:0)C − σ−2

 σ−2

W DW )−1.
(cid:1)−1

W DW )−1Dh

h (I + σ2
 DT

(15)
h Dh. According to the deﬁnition

 DT
where C := Σ−1
θ + σ−2
of DW in (5), we obtain

W DW = blkdiag{WT
DT
= blkdiag{GT

k gkgT
k wkwT

k Wk}K
k Gk}K

k=1
k=1,

(16)

(cid:1)−1

(cid:16)

Pw :=

C−σ−2

 diag(cid:8)hT

k

· hk

(cid:0)I+σ2
(cid:17)−1
(cid:9)K
 σ−2

ς GT

k wkwT

k Gk

(17)
From (17), the sensor collaboration problem for the estima-

k=1

.

tion of temporally correlated parameters becomes

minimize

subject to

tr(Pw)

K(cid:88)

k=1

k Qk,mwk ≤ Em, m ∈ [M ],
wT

(P2)

where w is the optimization variable, and the matrix Ωm is
deﬁned in (P1). Note that (P2) is not a convex optimization
k in
problem due to the presence of the rank-one matrix wkwT
(17). However, such a nonconvexity can be effectively handled
via proper convexiﬁcation techniques. We will present the
solution to (P2) in Sec. V.

We ﬁnally remark that the proposed sensor collaboration
methodologies in this paper apply equally well to the case of
sensor collaboration with respect to time-varying topologies,
namely, At for t ∈ [T ]. The only difference from sensor
collaboration with a ﬁxed topology is that the collaboration
vector wt would be constructed by concatenating the nonzero
entries of Wt according to At rather than A at each time
step.

From (19) and (20), problem (18) becomes

5

minimize
subject to

ς ≤ 0,
ς ≤ rk,

1T u
k + 2rk ≤ (sk + uk)2, k ∈ [K]
s2
k + u2
k ∈ [K]
sk − wT
k Skwk − σ2
k ∈ [K]
σ2
 wT
k Rkwk + σ2
m ∈ [M ]
wT Qmw ≤ Em,
s > 0,

(21a)
(21b)
(21c)
(21d)
(21e)
(21f)
where the optimization variables are w, u, r and s, r =
[r1, r2, . . . , rK]T , s = [s1, s2, . . . , sK]T , and > denotes el-
ementwise inequality. Note that the quadratic functions of DC
type in (21b) and (21c) bring in the nonconvexity of problem
(21). In what follows, we will show that CCP is a suitable
convex restriction approach for solving this problem.

IV. SPECIAL CASE: OPTIMAL SENSOR COLLABORATION
FOR THE ESTIMATION OF UNCORRELATED PARAMETERS

B. Convex restriction

In this section, we show that (P1) can be transformed into a
special nonconvex optimization problem, where the difference
of convex (DC) functions carries all the nonconvexity. Spurred
by the problem structure, we employ a convex-concave pro-
cedure (CCP) to solve (P1).

A. Equivalent optimization problem

We express (P1) in its epigraph form [28]

minimize

subject to

1T u
 wT
k Rkwk + σ2
σ2
ς
wT
k Skwk + σ2
ς
wT Qmw ≤ Em,

≤ uk, k ∈ [K]
m ∈ [M ],

(18a)

(18b)

(18c)

where u = [u1, u2, . . . , uK]T is the vector of newly introduced
optimization variables, and Qm := blkdiag{Qk,m}K
k=1. We
note that the inequality (18b) implicitly adds the additional
constraint uk ≥ 0 since both Rk and Sk are positive semidef-
inite for k ∈ [K].
We further introduce new variables rk and sk for k ∈ [K]

to rewrite (18b) as

≤ uk, sk > 0
rk
sk
wT
k Skwk + σ2
σ2
 wT

k Rkwk + σ2

ς ≥ sk

ς ≤ rk,

(19)

where the equivalence between (18b) and (19) holds since
the minimization of 1T u with the above inequalities forces
the variable sk and rk to achieve its upper and lower bound,
respectively.
In (19), the ratio rk/sk ≤ uk together with sk > 0 can be

reformulated as a quadratic inequality of DC type

s2
k + u2

k + 2rk − (sk + uk)2 ≤ 0,

(20)

where both s2
functions.

k + u2

k + 2rk and (sk + uk)2 are convex quadratic

Problem (21) is convex except for the nonconvex quadratic

constraints (21b) and (21c), which have the DC form

f (v) − g(v) ≤ 0,

(22)

where both f and g are convex functions. In (21b), we have
k + 2rk, and g(sk, uk) = (sk + uk)2.
f (sk, uk, rk) = s2
In (21c), f (sk) = sk, and g(wk) = wT

ς .
k Skwk + σ2

We can convexify (22) by linearizing g around a feasible

k + u2

point ˆv,

(23)

f (v) − ˆg(v) ≤ 0,
where ˆg(v) := g(ˆv) + (v − ˆv)T ∂g(ˆv)
is the ﬁrst-order
derivative of g at the point ˆv. In (23), ˆg is an afﬁne lower
bound on the convex function g, and therefore, the set of v
that satisfy (23) is a strict subset of the set of v that satisfy
(22). This implies that a solution of the optimization problem
with the linearized constraint (23) is locally optimal for the
problem with the original nonconvex constraint (22).

∂v , ∂g(ˆv)

∂v

We can obtain a ‘restricted’ convex version of problem (21)
by linearizing (21b) and (21c) as (23). We then solve a se-
quence of convex programs with iteratively updated lineariza-
tion points. The use of linearization to convexify nonconvex
problems with DC type functions is known as CCP [22], [23],
[29]. At each iteration of CCP, we solve

minimize 1T u
subject to s2

k + 2rk − ˆg1(sk, uk) ≤ 0,

k + u2
sk − ˆg2(wk) ≤ 0,
σ2
 wT
k Rkwk + σ2
wT Qmw ≤ Em,
s > 0,

ς ≤ rk,

k ∈ [K]
k ∈ [K]
k ∈ [K]
m ∈ [M ]

(24)
where the optimization variables are w, u, r, and s, ˆg1 and ˆg2
are afﬁne approximations of (sk + uk)2 and wT
ς ,
k Skwk + σ2
namely, ˆg1(sk, uk) := 2(sk + uk)(ˆsk + ˆuk) − (ˆsk + ˆuk)2, and
ς . We summarize CCP
ˆg2(wk) := 2 ˆwT
for solving problem (21) or (P1) in Algorithm 1.

k Skwk − ˆwT

k Sk ˆwk + σ2

To initialize Algorithm 1, we can choose the random points
(drawn from a standard uniform distribution) that are then

Algorithm 1 CCP for solving (P1)
Require: initial points ˆw, ˆs and ˆu, and ccp > 0
1: for iteration t = 1, 2, . . . do
2:
3:

solve problem (24) for the solution (wt, st, ut)
update the linearization point, ˆw = wt, ˆs = st, and
ˆu = ut
until |1T ut − 1T ut−1| ≤ ccp with t ≥ 2.

4:
5: end for

scaled to satisfy the constraints (21b) – (21e). Our extensive
numerical examples show that Algorithm 1 is fairly robust with
respect to the choice of the initial point; see Fig. 4-(a) for an
example.

The convergence of Algorithm 1 is guaranteed, since at each
iteration, we solve a restricted convex problem with a smaller
feasible set which contains the linearization point (i.e., the
solution after the previous iteration) [23]. In other words, for
a given linearization point, we always obtain a new feasible
point with a lower or equal objective value at each iteration.
The computation cost of Algorithm 1 is dominated by the
solution of the convex program with quadratic constraints at
Step 2. This has the computational complexity O(a3 + a2b) in
the use of interior-point algorithm [30, Chapter. 10], where a
and b denote the number of optimization variables and con-
straints, respectively. In problem (24), we have a = 3K + KL
and b = 4K + M. Therefore, the complexity of our algorithm
is roughly given by O(L3) per iteration. Here we focus on
the scenario in which the number of collaboration links L is
much larger than K or M.

V. GENERAL CASE: OPTIMAL SENSOR COLLABORATION

FOR THE ESTIMATION OF CORRELATED PARAMETERS
Different from (P1), the presence of temporal correlation
makes ﬁnding the solution of (P2) more challenging. However,
we show that (P2) can be cast as a semideﬁnite program
(SDP) with a (nonconvex) rank-one constraint. Spurred by the
problem structure, we employ a penalty CCP to solve (P2), and
propose a fast optimization algorithm by using the alternating
direction method of multipliers (ADMM).

A. Equivalent optimization problem

From (17), (P2) can be equivalently transformed to

minimize
tr(V)
w (cid:23) V−1
subject to P−1
wT Qmw ≤ Em, m ∈ [M ]
k ∈ [K],
Uk = wkwT
k ,

(25)

where V ∈ SK and Uk ∈ SL are newly introduced optimiza-
tion variables for k ∈ [K], Sn represents the set of n × n
symmetric matrices, and the notation X (cid:23) Y (or X (cid:22) Y)
indicates that X− Y (or Y − X) is positive semideﬁnite. The
ﬁrst inequality constraint of problem (25) is obtained from
Pw (cid:22) V, where Pw is given by (17), and P−1
w represents the
Bayesian Fisher information matrix.

6

(cid:1)−1

(cid:0)I + σ2

We further introduce a new vector of optimization variables
p = [p1, . . . , pK]T such that the ﬁrst matrix inequality of
problem (25) is expressed as

 hT
k

ς GT

 σ−2

k UkGk

hk, k ∈ [K],

C − diag(p) (cid:23) V−1,
(26)
pk ≥ σ−2
(27)
where we use the expression of Pw given by (17), and the
fact that Uk = wkwT
k . Note that the minimization of tr(V)
with inequalities (26) and (27) would force the variable pk
to achieve its lower bound. In other words, problem (25)
is equivalent
inequality
constraint of (25) is replaced by the above two inequalities.

to the problem in which the ﬁrst

By employing the Schur complement, we can express (26)

and (27) as the linear matrix inequalities (LMIs)

(cid:21)

(cid:20)C − diag(p)
(cid:20) pk

I

σ−1
 hk

I + σ2

(cid:23) 0,

I
V
σ−1
 hT
k
 σ−2
ς GT
k UkGk

(cid:21)

(cid:23) 0, k ∈ [K].

(28)

(29)

Replacing the ﬁrst inequality of problem (25) with LMIs
(28) – (29), we obtain an SDP together with a (nonconvex)
rank-one constraint Uk = wkwT
k . This nonconvex constraint
can be recast as two inequalities

Uk − wkwT

k (cid:23) 0, Uk − wkwT

(30)
According to the Shur complement, the ﬁrst matrix inequality
is equivalent to the LMI

k (cid:22) 0, k ∈ [K].

(cid:23) 0, k ∈ [K].

(31)

(cid:20)Uk wk

(cid:21)

wT
k

1

And the second inequality in (30) involves a function of DC
type, where Uk and wkwT
k are matrix convex functions [28].

From (28) – (31), problem (25) or (P2) is equivalent to

tr(V)

minimize
subject to wT Qmw ≤ Em, m ∈ [M ]

(32a)
(32b)
(32c)
(32d)
(32e)
where the optimization variables are w, p, V and Uk for
k ∈ [K], and (32e) is a nonconvex constraint of DC type.

LMIs in (28) – (29)
LMIs in (31)
Uk − wkwT

k ∈ [K],

k (cid:22) 0,

B. Convexiﬁcation

Proceeding with the same logic as in Sec. IV to convexify

the constraint (22), we linearize (32e) around a point ˆwk,

k + ˆwk ˆwT

k − wk ˆwT

k (cid:22) 0, k ∈ [K].

Uk − ˆwkwT
(33)
It
is straightforward to apply CCP to solve problem (32)
by replacing (32e) with (33). However, such an approach
fails in practice. This is not surprising, since the feasible set
determined by (32d) and (33) only contains the linearization
point. Speciﬁcally, from (32d) and (33), we obtain

(wk − ˆwk)(wk − ˆwk)T
= wkwT
(cid:22) Uk − ˆwkwT

k − ˆwkwT

k − wk ˆwT

k − wk ˆwT

k + ˆwk ˆwT
k

k + ˆwk ˆwT

k (cid:22) 0,

(34)

which indicates that wk = ˆwk. Therefore, CCP gets trapped
in a linearization point.

Remark 1: Dropping the nonconvex constraint (32e) is
another method to convexify problem (32), known as semidef-
inite relaxation [31]. However, such an approach makes the
optimization variable Uk unbounded, since the minimization
of tr(V) forces Uk to be as large as possible such that the
variable pk in (27) is as small as possible.

In order to circumvent the drawback of the standard CCP,
we consider its penalized version, known as penalty CCP [23],
where we add new variables to allow for constraints (33) to be
violated and penalize the sum of the violations in the objective
function. As a result, the convexiﬁcation (33) is modiﬁed by
(35)
where Zk ∈ SL is a newly introduced variable. The constraint
(35) implicitly adds the additional constraint Zk (cid:23) 0 due to
Uk (cid:23) wkwk from (32d).

k (cid:22) Zk, k ∈ [K],

Uk − ˆwkwT

k − wk ˆwT

k + ˆwk ˆwT

After replacing (32e) with (35), we obtain the SDP,

minimize

tr(V) + τ

tr(Zk)

subject to

(32b) – (32d) and (35)

k=1

(36)

where the optimization variables are w, p, V, Uk and Zk
for k ∈ [K], and τ > 0 is a penalty parameter. Compared
to the standard CCP, problem (36) is optimized over a larger
feasible set since we allow for constraints to be violated by
adding variables Zk for k ∈ [K]. We summarize the use of
penalty CCP to solve (P2) in Algorithm 2.

Algorithm 2 Penalty CCP for solving (P2)
Require: an initial point ˆw, ccp > 0, τ 0 > 0, τmax > 0 and

K(cid:88)

µ > 1.

1: for iteration t = 1, 2, . . . do
2:

solve problem (36) for its solution wt via SDP solver
or ADMM-based algorithm in Sec. V-C
update the linearization point, ˆw = wt
update the penalty parameter τ t = min{µτ t−1, τmax}
let ψt be the objective value of (36)
until |ψt − ψt−1| ≤ ccp with t ≥ 2.

3:
4:
5:
6:
7: end for

In Algorithm 2, the initial point ˆw is randomly picked from
a standard uniform distribution. Note that ˆw is not necessarily
feasible for (P2) since violations of constraints are allowed.
We also remark that when τ = τmax, penalty CCP reduces to
CCP, and therefore, its convergence is guaranteed [23].

The computation cost of Algorithm 2 is dominated by the
solution of the SDP (36) at Step 2. This leads to the complexity
O(a2b2 + ab3) by using the interior-point alogrithm in off-the-
shelf solvers [30, Chapter. 11], where a and b are the number of
optimization variables and the size of the semideﬁnite matrix,
respectively. In (36), the number of optimization variables is
proportional to L2. Therefore, the complexity of Algorithm 2 is
roughly given by O(L6). Clearly, computing solutions to SDPs
becomes inefﬁcient for problems of medium or large size. In
what follows, we will develop an ADMM-based algorithm that
is more amenable to large-scale optimization.

7

C. Fast algorithm via ADMM

It has been shown in [24], [32]–[34] that ADMM is a
powerful tool for solving large-scale optimization problems.
The major advantage of ADMM is that it allows us to split
the original problem into subproblems, each of which can be
solved more efﬁciently or even analytically. In what follows,
we will employ ADMM to solve problem (36).

It is shown in Appendix A that problem (36) can be refor-
mulated in a way that lends itself to the application of ADMM.
This is achieved by introducing slack variables and indicator
functions to express the inequality constraints of problem
(36) as linear equality constraints together with proper cone
constraints with respect to slack variables, including second-
order cone and positive semideﬁnite cone constraints.

ADMM is performed based on the augmented Lagrangian
[24] of the reformualted problem (36), and leads to two
problems, the ﬁrst of which can be treated as an unconstrained
quadratic program and the latter renders an analytical solution.
These two problems are solved iteratively and communicate to
each other through special quadratic terms in their objectives;
the quadratic term in each problem contains information about
the solution of the other problem and also about dual variables
(also known as Lagrange multipliers). In what follows, we
refer to these problems as the ‘X -minimization’ and ‘Z -
minimization’ problems. Here X denotes the set of primal
variables w, p, V, Uk and Zk for k ∈ [K], and Z denotes the
set of slack variables λm, Λ1 and {Λi,k}i=2,3,4 for m ∈ [M ]
and k ∈ [K]. We also use Y to denote the set of dual variables
πm, Π1 and {Πi,k}i=2,3,4 for m ∈ [M ] and k ∈ [K].
The ADMM algorithm is precisely described by (55) – (57)
in Appendix A.

We emphasize that

the crucial property of the ADMM
approach is that, as we demonstrate in the rest of this section,
the solution of each of the X - and Z -minimization problems
can be found efﬁciently and exactly.

1) X -minimization step: The X -minimization problem

can be cast as

minimize ϕ(w, p, V,{Uk},{Zk}).

(37)

m − cm − (1/ρ)πt
1, and Υi,k := Λt

The objective function of problem (37) is given by (38), where
1 −
αm := λt
i,k for i ∈ {2, 3, 4}
(1/ρ)Πt
and k ∈ [K], and t denotes the ADMM iteration. For ease of
notation, we will omit the ADMM iteration index t in what
follows.

m for m ∈ [M ], Υ1 := Λt
i,k − (1/ρ)Πt

We note that problem (37) is an unconstrained quadratic
program (UQP) with large amounts of variables. In order to
reduce the computational complexity and memory requirement
in optimization, we will employ a gradient descent method
[28] together with a backtracking line search [28, Chapter 9.2]
to solve this UQP. In Proposition 2, we show the gradient of
the objective function of problem (37).

Proposition 2: The gradient of the objective function of

ϕ(w, p, V,{Uk},{Zk}) := tr(V) + τ

(cid:13)(cid:13)2

2 +

(cid:13)(cid:13) ¯Qmw − αm
(cid:21)

− Υ2,k

σ−1
 hT
k
 σ−2
ς GT
k UkGk

M(cid:88)

k=1

ρ
2

tr(Zk) +

K(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:20) pk
(cid:13)(cid:13)Zk − Uk + ˆwkwT

σ−1
 hk

I + σ2

m=1

k + wk ˆwT

k − ˆwk ˆwT

k − Υ4,k

I

ρ
2

I
V

(cid:13)(cid:13)(cid:13)(cid:13)(cid:20)C − diag(p)
(cid:21)
(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:20)Uk wk
K(cid:88)
(cid:13)(cid:13)2

wT
k

ρ
2

k=1

+

1

F

F

(cid:13)(cid:13)(cid:13)(cid:13)2

F

− Υ1

(cid:21)

− Υ3,k

8

(38)

(cid:13)(cid:13)(cid:13)(cid:13)2

F

K(cid:88)
K(cid:88)

k=1

k=1

+

+

ρ
2

ρ
2

problem (37) is given by
m( ¯Qmw − αm) + 2ρ(w − γ3)
¯QT
k=1 ˆw

∇wϕ = ρ(cid:80)M
∇pϕ = ρ diag(cid:0)C − diag(p) − Υ11

(cid:1) + ρ(p − γ2)

+2ρ blkdiag{ ˆwkwT

k − Hk}K

k + wk ˆwT

m=1

1

∇Vϕ = I + ρ(V − Υ22
1 )
k UkGk − Υ22
∇Uk ϕ = ρσ2
3,k − Zk − Tk), k ∈ [K]
+ρ(2Uk − Υ11

ς Gk(I + σ2

 σ−2

 σ−2

ς GT

∇Zk ϕ = τ I + ρ(Zk − Uk + Tk), k ∈ [K],

2,k)GT
k

1

1 , . . . , ˆwT

K]T , Υ11

3,1, . . . , γT

k + Υ4,k, ˆw = [ ˆwT

where γ3 = [γT
3,K]T , γ3,k is the (L + 1) column
of Υ3,k after the last entry is removed, Hk := Uk − Zk +
is a submatrix
ˆwk ˆwT
of Υ1 that contains its ﬁrst K rows and columns, γ2 =
[γ2,1, . . . , γ2,K]T , γ2,k is the ﬁrst element of Υ2,k, diag(·)
returns the diagonal entries of its matrix argument in vector
is a submatrix of Υ1 after the ﬁrst K rows and
form, Υ22
1
2,k is a submatrix of Υ2,k after
columns are removed, Υ22
the ﬁrst row and column are removed, Υ11
3,k is a submatrix
of Υ3,k after the last row and column are removed, and
Tk := ˆwkwT
k + wk ˆwT
(cid:4)
Proof: See Appendix B.
In Proposition 2, we note that the optimal value of V is
achieved by letting ∇Vϕ = 0. This leads to

k − ˆwk ˆwT

k − Υ4,k.

1 − (1/ρ)I.

V = Υ22

(39)
To solve problem (37) for other variables, we employ the
gradient descent method summarized in Algorithm 3. This
algorithm calls on the backtracking line search (Algorithm 4)
to properly determine the step size such that the convergence
to a stationary point of problem (37) is accelerated.

Algorithm 3 Gradient descent method for solving UQP (37)
Require: values of w, p, {Uk} and {Zk} at the previous

ADMM iteration, grad > 0, and V given by (39)

1: repeat
2:
3:

4:
5:

:= (cid:80)K

2 +(cid:80)K

compute the gradient of φ following Proposition 2
compute cgrad
+(cid:107)∇pϕ(cid:107)2
call Algorithm 4 to determine a step size κ
update variables

k=1 (cid:107)∇Uk ϕ(cid:107)2

k=1 (cid:107)∇Zk ϕ(cid:107)2

F + (cid:107)∇wϕ(cid:107)2

F

2

w := w + κ∇wϕ, p := p + κ∇pϕ
Uk := Uk + κ∇Uk ϕ, Zk := Zk + κ∇Zk ϕ

6: until cgrad ≤ grad.

Algorithm 4 Backtracking line search for choosing κ
1: Given κ := 1, a1 ∈ (0, 0.5), a2 ∈ (0, 1), and cgrad
2: repeat
3:
4:
5: until ˆϕ < ϕ(w, p, V,{Uk},{Zk}) − a1κ cgrad.

κ := a2κ,
let ˆϕ be the value of ϕ at the points w + κ∇wϕ,
p + κ∇pϕ, V, Uk + κ∇Uk ϕ, and Zk + κ∇Zk ϕ

decomposed with respect to each of slack variables.

2) Z -minimization step: The Z -minimization problem is
• Subproblem with respect to λm:
(cid:107)λm − βm(cid:107)2

minimize
subject to (cid:107)[λm]1:KL(cid:107)2 ≤ [λm]KL+1,

(40)

2

where βm := ¯Qmwt+1 + cm + (1/ρ)πt
m, and t is the ADMM
iteration index. For notational simplicity, the ADMM iteration
will be omitted in what follows. The solution of problem (40)
is achieved by projecting βm onto a second-order cone [32,
Sec. 6.3],

 0

λm =

(cid:18)

for m ∈ [M ], where

(cid:107)[βm]1:KL(cid:107)2 ≤ −[βm]KL+1
βm (cid:107)[βm]1:KL(cid:107)2 ≤ [βm]KL+1
˜βm (cid:107)[βm]1:KL(cid:107)2 ≥ |[βm]KL+1|,

(cid:19)(cid:2)[βm]T

1
2

1+

[βm]KL+1
(cid:107)[βm]1:KL(cid:107)2

˜βm =
• Subproblem with respect to Λ1:

1:KL,(cid:107)[βm]1:KL(cid:107)2

(41)

(cid:3)T

.

(42)

minimize
subject to Λ1 (cid:23) 0,

(cid:107)Λ1 − Φ1(cid:107)2

F

(cid:21)

I
V

(cid:20)C − diag(p)
2K(cid:88)

Λ1 =

i=1

where Φ1 :=
of problem (42) is given by [32, Sec. 6.3]

I

+ (1/ρ)Π1. The solution

(σi)+ωiωT
i ,

(43)

where(cid:80)2K

i=1 σiωiωT
i

is the eigenvalue decomposition of Φ1,
• Subproblem with respect to Λi,k for i ∈ {2, 3, 4} and

and (·)+ is the positive part operator.
k ∈ [K]:

(cid:107)Λi,k − Φi,k(cid:107)2

minimize
subject to Λi,k (cid:23) 0,

F

(44)

where

Φ2,k :=

(cid:20) pk
(cid:20)Uk wk

σ−1
 hk

(cid:21)

(cid:21)

+ 1

ρ Π2,k

9

I + σ2

σ−1
 hT
k
 σ−2
ς GT
k UkGk
ρ Π3,k

+ 1

1

wT
k

Φ3,k :=
Φ4,k := Zk − Uk + ˆwkwT
The solution of problem (44) is the same as (43) except that
Φ1 is replaced with Φi,k for i ∈ {2, 3, 4} and k ∈ [K].

k − ˆwk ˆwT

k + wk ˆwT

ρ Π4,k.

k + 1

k = I for k ∈ [K], λ0
1 = 0, and Λ0
i,k = Π0

3) Summary of the proposed ADMM algorithm: We initial-
ize the ADMM algorithm by setting w0 = 1, p0 = 1, V0 = I,
m = 0 for m ∈ [M ],
U0
m = π0
k = Z0
i,k = 0 for i ∈ {2, 3, 4,} and
Λ0
1 = Π0
k ∈ [K]. The ADMM algorithm proceeds as (55) – (57) shown
in Appendix A. The convergence of ADMM is guaranteed in
solving convex problems, and it typically takes a few tens of
iterations to converge with satisfactory accuracy [24].

At each iteration of ADMM, the computational complex-
ity of the X -minimization step is approximated by O(L4),
where O(L) roughly counts for the number of iterations of
the gradient descent method, and O(L3) is the complexity
of matrix multiplication while computing the gradient. Here
we assume that L is much larger than K and N. In Z -
minimization step, the computational complexity is dominated
by the eigenvalue decomposition used in (43). This leads to
the complexity O(L3.5). As a result, the total computation cost
of the ADMM algorithm is given by O(L4). For additional
perspective, we compare the computational complexity of
the ADMM algorithm with the interior-point algorithm that
takes complexity O(L6). Clearly, the complexity of ADMM
decreases signiﬁcantly in terms of the number of collaboration
links by a factor L2.

VI. NUMERICAL RESULTS

1

e−ρcorr



Σθ = σ2
θ

This section empirically shows the effectiveness of our
approach for sensor collaboration in time-varying sensor net-
works. We assume that θk follows a Ornstein-Uhlenbeck pro-
θ e−|k1−k2|/ρcorr
cess [16] with correlation cov(θk1, θk2 ) = σ2
for k1 ∈ [K] and k2 ∈ [K], where ρcorr is a parameter that
governs the correlation strength, namely, a larger (or smaller)
ρcorr corresponds to a weaker (or stronger) correlation. The
covariance matrix of θ is given by
e−ρcorr

···
···
...
···
θ = 1 and ρcorr =
where unless speciﬁed otherwise, we set σ2
0.5. The spatial placement and neighborhood structure of the
sensor network is modeled by a random geometric graph [14],
RGG(N, d), where N = 10 sensors are randomly deployed
over a unit square and bidirectional communication links are
possible only for pairwise distances at most d. Clearly, the
adjacency matrix A is determined by RGG(N, d), and the
number of collaboration links increases as d increases. In our
numerical examples unless speciﬁed otherwise, we set d = 0.3
which leads to RGG(10, 0.3) shown in Fig. 3.

e−(K−1)ρcorr
e−(K−2)ρcorr

e−(K−1)ρcorr

e−(K−2)ρcorr

 .

...

...

...

1

1

Fig. 3: RGG(10, 0.3), collaboration is depicted for sensors 3, 6 and 9.

 = σ2

In the collaborative estimation system shown in Fig. 1,
ς = 1, and
we assume that M = N, K = 3, σ2
Em = Etotal/M for m ∈ [M ], where Etotal = 1 gives
the total energy budget of M sensors. For simplicity, the
obverstion gain hk and channel gain gk are randomly chosen
from the uniform distribution U(0.1, 1). Moreover, we select
τ 0 = 0.1, µ = 1.5, τmax = 100 in penalty CCP (namely,
Algorithm 2), a1 = 0.02 and a2 = 0.5 in backtracking line
search (namely, Algorithm 4) and ccp = admm = grad =
10−3 for the stopping tolerance of the proposed algorithms.
Unless speciﬁed otherwise, the ADMM algorithm is adopted
at Step 2 of penalty CCP, and we use CVX [35] for all
other computations. The estimation performance is measured
through the empirical mean squared error (MSE), which is
computed over 1000 numerical trials.

In Fig. 4, we present convergence trajectories of CCP
(namely, Algorithm 1) and penalty CCP (namely, Algorithm 2)
as functions of interation index for 10 different initial points.
For comparison, we plot the worst objective function value of
collaboration problem (7) when w = 0, namely, LMMSE is
determined only by the prior information, which leads to the
worst estimation error tr(Σθ) = K = 3. As we can see, much
of the beneﬁt of using CCP or penalty CCP is gained during
the ﬁrst few iterations. And each algorithm converges to almost
the same objective function value for different initial points.
Compared to CCP, the convergence trajectory of penalty CCP
is not monotonically decreasing. Namely, penalty CCP is not
a descent algorithm. The non-monotonicity of penalty CCP
is caused by the penalization on the violation of constraints
in the objective function. The objective function value of
penalty CCP converges until the penalization ceases to change
signiﬁcantly (after 15 iterations in this example).

In Fig. 5, we present the trace of error covariance matrix
PW given by (6) as a function of the correlation parameter
ρcorr, where the sensor collaboration scheme is obtained
from Algorithm 1 and Algorithm 2 to solve (P1) and (P2),
respectively. We observe that the estimation error resulting
from the solution of (P1) remains unchanged for different
values of ρcorr since the formulation of (P1) is independent

00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91X−axisY−axis SensorCollaboration LinkBoundarySensor 3Sensor 6Sensor 910

Fig. 5: Estimation error versus correlation parameter ρcorr.

(a)

(b)

Fig. 4: Convergence of Algorithm 1 and 2 for different initial points.

Fig. 6: MSE versus total energy budget.

of the prior knowledge about parameter correlation. The
estimation error resulting from the solution of (P2) increases
as ρcorr increases, and it eventually converges to the error
resulting from the solution of (P1) at an extremely large ρcorr,
where parameters become uncorrelated. This is not surprising,
since the prior information about parameter correlation was
taken into account in (P2), thereby signiﬁcantly improving the
estimation performance.

In Fig. 6, we present the MSE of collaborative estimation as
a function of the total energy budget Etotal for ρcorr = 0.5. For
comparison, we plot the estimation performance when using
a time-invariant collaboration scheme to solve (P1) and (P2),
respectively. The assumption of time-invariant collaboration
implicitly adds the additional constraint w1 = . . . = wK,
which reduces the problem size. By ﬁxing the type of algo-
rithm, we observe that the MSE when using time-invariant sen-
sor collaboration is larger than that of the originally proposed
algorithm. This is because the latter accounts for temporal
dynamics of the network, where observation and channel gains

vary in time. Moreover, the solution of (P2) yields lower MSE
than that of (P1). This result is consistent with Fig. 5 for a ﬁxed
correlation parameter. Lastly, the estimation error is smaller as
more energy is used in sensor collaboration.

In Fig. 7, we present the MSE and the number of collab-
oration links as functions of the collaboration radius d for
ρcorr = 0.5 and Etotal = 1. We note that the estimation
accuracy improves as d increases, since a larger value of d
corresponds to more collaboration links in the network. For
a ﬁxed value of d, the MSE when solving (P2) is lower than
that when solving (P1), since the latter ignores the information
about parameter correlation. Moreover, we observe that the
MSE tends to saturate beyond a collaboration radius d ≈ 0.7.
This indicates that a large part of the performance improve-
ment is achieved only through partial collaboration.

In Fig. 8, we present the computation time of our algorithms
as functions of problem size speciﬁed in terms of the number
of collaboration links L. For comparison, we plot the computa-
tion time of penalty CCP when using an interior-point solver in

024681012142.12.22.32.42.52.62.72.82.933.1Iteration of CCPObjective function valueCCP with 10initial pointsWorst objective functionvalue provided by w = 0024681012141618202211.522.53Iteration of CCPObjective function valuePenatly CCP with differentinitial pointsWorst objective function value provided by w = 010−210−11001011021.31.41.51.61.71.81.922.12.22.3Correlation parameter, ρcorrTrace of error covariance, tr(PW) Algorithm 1, CCP for (P1)Algorithm 2, penalty CCP for (P2)10−110010110211.522.53Total energy budget, EtotalMSEAlgorithm 1, CCP for (P1)Algorithm 1, CCP for (P1)under time−invariant collaborationAlgorithm 2, penalty CCP for (P2)Algorithm 2, penalty CCP for (P2)under time−invariant collaboration11

optimization problem, where a difference of convex functions
carries all the nonconvexity. By exploiting problem structure,
we solve the problem by using a convex-concave procedure,
which renders a good locally optimal solution evidenced by
numerical results. In the case of correlated parameters, we
show that the sensor collaboration problem can be converted
into a semideﬁnite program together with a nonconvex rank-
one constraint. Spurred by problem structure, we employ a
semideﬁnite programming based penalty convex-concave pro-
cedure to solve the sensor collaboration problem. Moreover,
we propose an ADMM-based algorithm that is more scalable
to large-scale optimization. Numerical results are provided to
demonstrate the effectiveness of our approach and the impact
of parameter correlation and temporal dynamics of sensor
networks on the performance of distributed estimation with
sensor collaboration.

There are multiple directions for future research. We would
like to consider noise-corrupted or quantization-based imper-
fect communication links in sensor collaboration. It is also
of interest to seek theoretical guarantees on the performance
of CCP and penalty CCP for sensor collaboration. Another
direction of future work is to seek an approach for the joint
design of optimal power allocation schemes and collaboration
topologies in dynamic networks.

APPENDIX A

APPLICATION OF ADMM

We introduce slack variables λm ∈ RKL+1 for m ∈ [M ] to
rewrite (32b) as an equality constraint together with a second-
order cone constraint,

¯Qmw − λm + cm = 0, (cid:107)[λm]1:KL(cid:107)2 ≤ [λm]KL+1,

(45)

1
2

1
2

1
2

√

m, 0]T , Q

where ¯Qm := [Q
m is the square root of Qm
given by the matrix decomposition Qm = (Q
m, cm =
Em]T , and [a]1:n denotes a subvector of a that consists
[0T ,
of its ﬁrst n entries.
We further introduce slack variables Λ1 ∈ S2K, Λ2,k ∈
SN +1, Λ3,k ∈ SL+1 and Λ4,k ∈ SL for k ∈ [K] to rewrite
LMIs of problem (36) as a sequence of equality constraints
together with positive semideﬁnite cone constraints

m)T Q

1
2

(cid:21)

(cid:20)C − diag(p)
(cid:20) pk
(cid:21)
(cid:20)Uk wk

σ−1
 hk

I

− Λ1 = 0
I
V
σ−1
 hT
k
 σ−2
I + σ2
ς GT
k UkGk
− Λ3,k = 0

(cid:21)

− Λ2,k = 0

(46)

(47)

(48)

(49)

1

wT
k
Zk − Uk + ˆwkwT

k + wk ˆwT

k − ˆwk ˆwT

k − Λ4,k = 0,

where Λ1 (cid:23) 0, Λ2,k (cid:23) 0, Λ3,k (cid:23) 0, and Λ4,k (cid:23) 0 for
k ∈ [K].

Fig. 7: MSE and collaboration links versus collaboration radius d.

CVX [35]. As we can see, penalty CCP requires much higher
computation time than CCP, since the former requires solutions
of SDPs. When L is small, we observe that the ADMM
based penalty CCP has a higher computation time than when
using the interior-point solver. This is because the gradient
descent method in ADMM takes relatively more iterations
(compared to small L) to converge with satisfactory accuracy.
However, the ADMM based algorithm performs much faster
for a relatively large problem with L > 80.

Fig. 8: Computation time versus number of collaboration links.

VII. CONCLUSIONS

We study the problem of sensor collaboration for estimation
of time-varying parameters in sensor networks. Based on prior
knowledge about parameter correlation, the resulting sensor
collaboration problem is solved for estimation of temporally
uncorrelated and correlated parameters. In the case of tem-
porally uncorrelated parameters, we show that
the sensor
collaboration problem can be cast as a special nonconvex

0.10.20.30.40.50.60.70.80.911.1123MSECollaboration radius, d 0.10.20.30.40.50.60.70.80.911.1050100No. of collaboration linksAlgorithm 1, CCP for (P1)Algorithm 2, penalty CCP for (P2)No. of collaboration links102030405060708090100100101102103No. of collaboration links, LComputation time (seconds)Algorithm 1, CCP for (P1)Algorithm 2, penalty CCP via CVX for (P2)Algorithm 2, penalty CCP via ADMM for (P2)12

until both of the conditions (cid:107)X t+1 − Z t(cid:107)F ≤ admm and
(cid:107)Z t+1 − Z t(cid:107)F ≤ admm are satisﬁed, where with an abuse
of notation, (cid:107)X (cid:107)F denotes the sum of Frobenius norms of
variables in X , and admm is a stopping tolerance.

Substituting (54) into (55) and completing the squares with
respect to primal variables, the X -minimization problem (55)
becomes the unconstrained quadratic program given by (37).
Substituting (54) into (56), the Z -minimization problem
(56) is decomposed into a sequence of subproblems with
respect to each of slack variables, given by (40), (42) and
(cid:4)
(44).

We begin by collecting terms in ϕ associated with w,

APPENDIX B

PROOF OF PROPOSITION 2

ϕw :=

ρ
2

(cid:13)(cid:13)2

M(cid:88)
(cid:13)(cid:13) ¯Qmw − αm
K(cid:88)

(cid:107) ˆwkwT

m=1

+

ρ
2

k=1

K(cid:88)

k=1

2 + ρ

(cid:107)wk − γ3,k(cid:107)2

2

k + wk ˆwT

k − Hk(cid:107)2
F ,

(58)

where γ3,k is the (L + 1) column of Υ3,k after the last entry
is removed, and Hk := Uk − Zk + ˆwk ˆwT
k + Υ4,k, which is
a symmetric matrix.

In (58), we assume an incremental change δw in w.
Replacing w with w + δw and ϕw with ϕw + δϕw and
collecting ﬁrst order variation terms on both sides of (58),
we obtain

δϕw =ρ

( ¯Qmw − αm)T ¯Qmδw + 2ρ(w − γ3)T δw

+ 2ρ ˆwT blkdiag{ ˆwkwT

(59)
K]T . It
where γ3 = [γT
is clear from (59) that the gradient of ϕ with respect to w is
given by

3,K]T , and ˆw = [ ˆwT

k − Hk}δw,
1 , . . . , ˆwT

k + wk ˆwT

3,1, . . . , γT

M(cid:88)

m=1

M(cid:88)

From (45) – (49), problem (36) becomes

minimize

tr(V) + τ

tr(Zk) +

M(cid:88)

I0(λm)

K(cid:88)
4(cid:88)

k=1

K(cid:88)

+I1(Λ1) +

m=1

Ii(Λi,k)

(50)

subject to

equality constraints in (45) – (49),

i=2

k=1

where the optimization variables are w, p, V, Uk, Zk, λm,
Λ1, and {Λi,k}i=2,3,4 for m ∈ [M ] and k ∈ [K], and Ii is
the indicator function speciﬁed by

(cid:26) 0,
(cid:26) 0,
(cid:26) 0,

I0(λm) =
I1(Λ1) =
Ii(Λi,k) =

if (cid:107)[λm]1:KL(cid:107)2 ≤ [λm]KL+1
∞ otherwise,
if Λ1 (cid:23) 0
∞ otherwise,
if Λi,k (cid:23) 0
∞ otherwise,

i = 2, 3, 4.

(51)

(52)

(53)

It is clear from problem (50) that the introduced indicator
functions helps to isolate the second-order cone and positive
semideﬁnite cone constraints with respect to slack variables.
Problem (50) is now in a form suitable for the application
of ADMM. The corresponding augmented Lagrangian [24] in
ADMM is given by

K(cid:88)

M(cid:88)

I0(λm)

Ii(Λi,k) +

k=1

m=1

tr(Zk) +

mfm(X , Z )
πT

M(cid:88)
1 F1(X , Z )(cid:1)
2 + tr(cid:0)ΠT
K(cid:88)
4(cid:88)
tr(cid:0)ΠT
i,kFi,k(X , Z )(cid:1)

m=1

Lρ(X , Z , Y ) = tr(V) + τ

4(cid:88)

K(cid:88)

i=2

k=1

(cid:107)fm(X , Z )(cid:107)2

(cid:107)F1(X , Z )(cid:107)2

F +

+ I1(Λ1) +

M(cid:88)

m=1

4(cid:88)

K(cid:88)

i=2

k=1

+

+

+

ρ
2

ρ
2

ρ
2

i=2

k=1

(cid:107)Fi,k(X , Z )(cid:107)2
F ,

where X denotes the set of primal variables w, p, V, Uk and
Zk for k ∈ [K], Z denotes the set of primal slack variables
λm, Λ1 and {Λi,k}i=2,3,4 for m ∈ [M ] and k ∈ [K], Y
is the set of dual variables πm, Π1 and {Πi,k}i=2,3,4 for
m ∈ [M ] and k ∈ [K], fm(·), F1(·), and Fi,k(·) for i ∈
{2, 3, 4} represent linear functions at the left hand side of
equality constraints in (45) – (49), ρ > 0 is a regularization
parameter, and (cid:107)·(cid:107)F denotes the Frobenius norm of a matrix.
We iteratively execute the following three steps for ADMM

iteration t = 0, 1, . . .

X t+1 = arg min

X

Z t+1 = arg min

 πt+1

Πt+1
Πt+1

Z
m = πt
1 = Πt
i,k = Πt

L(X , Z t, Y t)
L(X t+1, Z , Y t)

m + ρ fm(X t+1, Z t+1), ∀m
1 + ρ F1(X t+1, Z t+1)
i,k + ρ Fi,k(X t+1, Z t+1), ∀i, k,

(55)

(56)

(57)

(54)

∇wϕ =ρ

m( ¯Qmw − αm) + 2ρ(w − γ3)
¯QT
k − Hk} ˆw.

k + wk ˆwT

+ 2ρ blkdiag{ ˆwkwT

m=1

(60)

Second, we collect the terms associated with p in ϕ to

construct the function

ρ
2

ϕp :=

(cid:107)C − diag(p) − Υ11
1 (cid:107)2

ρ
2
is a matrix that consists of the ﬁrst K rows and
where Υ11
1
columns of Υ1, and γ2 is a vector whose kth entry is given
by the ﬁrst entry of Υ2,k for k ∈ [K].

(cid:107)p − γ2(cid:107)2
2,

(61)

F +

In (61), replacing p with p+δp and ϕp with ϕp +δϕp and
collecting ﬁrst order variation terms on both sides, we obtain
δϕp = ρ[diag(C − diag(p) − Υ11
(62)
where diag(·) returns in vector form the diagonal entries of
its matrix argument. Therefore, the gradient of ϕ with respect
to p is given by

1 ) + (p − γ2)]T δp,

∇pϕ = ρ diag(C − diag(p) − Υ11

1 ) + ρ(p − γ2).

(63)

Third, given the terms associated with V in ϕ, the gradient

of ϕ with respect to V is readily cast as
∇Vϕ = I + ρ(V − Υ22
1 ),

(64)

(cid:13)(cid:13)2

F

(cid:13)(cid:13)I + σ2

where Υ22
1
columns are removed.

is a submatrix of Υ1 after the ﬁrst K rows and

Further, we collect the terms in ϕ with respect to the variable

Uk, and consider the function

ϕUk :=

ρ
2
+

ρ
2

 σ−2
ς GT
3,k(cid:107)2
(cid:107)Uk − Υ11

k UkGk − Υ22
F +

2,k

ρ
2

(cid:107)Uk − Zk − Tk(cid:107)2
F ,

(65)

where Υ22
column are removed, Υ11
row and column are removed, and Tk := ˆwkwT
ˆwk ˆwT

2,k is a submatrix of Υ2,k after the ﬁrst row and
3,k is a submatrix of Υ3,k after the last
k −

k + wk ˆwT

k − Υ4,k.

In (65), replacing Uk with Uk + δUk and ϕUk with ϕUk +
δϕUk and collecting ﬁrst order variation terms on both sides,
we obtain

(cid:18)
(cid:16)(cid:0)Uk − Υ11

ρσ2

σ2
ς

tr

δϕUk =

+ ρ tr

Gk(I +

k UkGk − Υ22
GT

2,k)T GT

k δUk

σ2

σ2
ς

3,k)T δUk + (Uk − Zk − Tk

δUk

(cid:1)T

(cid:19)
(cid:17)

.

Therefore, the gradient of ϕ with respect to Uk is given by

∇Uk ϕ =ρσ2

ς Gk(I + σ2

 σ−2
+ ρ(Uk − Υ11

ς GT

 σ−2

k UkGk − Υ22
3,k) + ρ(Uk − Zk − Tk).

2,k)GT
k
(66)

Finally, the gradient of ϕ with respect to Zk is given by

∇Zk ϕ = τ I + ρ(Zk − Uk + Tk),

(67)

where Tk is deﬁned in (65). We now complete the proof by
(cid:4)
combining (60), (63), (64), (66) and (67).

REFERENCES

[1] L. Oliveira and J. Rodrigues, “Wireless sensor networks: a survey on
environmental monitoring,” Journal of Communications, vol. 6, no. 2,
2011.

[2] Y. Zou and K. Chakrabarty, “Sensor deployment and target localization
ACM Transactions on Embedded

in distributed sensor networks,”
Computing Systems, vol. 3, no. 1, pp. 61–91, Feb. 2004.

[3] T. He, P. Vicaire, T. Yan, L. Luo, L. Gu, G. Zhou, S. Stoleru, Q. Cao,
J. A. Stankovic, and T. Abdelzaher, “Achieving real-time target tracking
using wireless sensor networks,” in Proceedings of IEEE Real Time
Technology and Applications Symposium, 2006, pp. 37–48.

[4] S. Cui, J.-J. Xiao, A. J. Goldsmith, Z.-Q. Luo, and H. V. Poor,
“Estimation diversity and energy efﬁciency in distributed sensing,” IEEE
Transactions on Signal Processing, vol. 55, no. 9, pp. 4683–4695, 2007.
[5] J.-J. Xiao, S. Cui, Z.-Q. Luo, and A. J. Goldsmith, “Linear coherent
decentralized estimation,” IEEE Transactions on Signal Processing, vol.
56, no. 2, pp. 757–770, 2008.

[6] A. Ribeiro and G. B. Giannakis, “Bandwidth-constrained distributed
estimation for wireless sensor networks-part i: Gaussian case,” IEEE
Transactions on Signal Processing, vol. 54, no. 3, pp. 1131–1143, 2006.
identical binary
IEEE Transactions on

[7] S. Kar, H. Chen, and P. K. Varshney,

quantizer design for distributed estimation,”
Signal Processing, vol. 60, no. 7, pp. 3896–3901, 2012.

“Optimal

[8] C. Huang, Y. Zhou, T. Jiang, P. Zhang, and S. Cui, “Power allocation
for joint estimation with energy harvesting constraints,” in Proceedings
of IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), May 2013, pp. 4804–4808.

[9] M. Nourian, S. Dey, and A. Ahlen, “Distortion minimization in multi-
sensor estimation with energy harvesting,” IEEE Journal on Selected
Areas in Communications, vol. 33, no. 3, pp. 524–539, Mar. 2015.

13

[10] J. Fang and H. Li,

cluster-based sensor collaboration,”
Communications, vol. 8, no. 7, pp. 3822–3832, 2009.

“Power constrained distributed estimation with
IEEE Transactions on Wireless

[11] G. Thatte and U. Mitra,

“Power allocation in linear and tree wsn
topologies,” in Proceedings of Asilomar Conference on Signals, Systems
and Computers, Oct 2006, pp. 1342–1346.

[12] G. Thatte and U. Mitra, “Sensor selection and power allocation for
distributed estimation in sensor networks: Beyond the star topology,”
IEEE Transactions on Signal Processing, vol. 56, no. 7, pp. 2649–2661,
July 2008.

[13] M. Fanaei, M. C. Valenti, A. Jamalipour, and N. A. Schmid, “Optimal
power allocation for distributed blue estimation with linear spatial
collaboration,” in Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), May 2014, pp.
5452–5456.

[14] S. Kar and P. K. Varshney, “Linear coherent estimation with spatial
collaboration,” IEEE Transactions on Information Theory, vol. 59, no.
6, pp. 3532–3553, June 2013.
[15] S. Kar and P. K. Varshney,

“On linear coherent estimation with
spatial collaboration,” in Proceedings of the 2012 IEEE International
Symposium on Information Theory Proceedings (ISIT), 2012, pp. 1448–
1452.

[16] S. Kar and P. K. Varshney,

“Controlled collaboration for linear
coherent estimation in wireless sensor networks,” in Proceedings of
the 50th Annual Allerton Conference on Communication, Control, and
Computing (Allerton), 2012, pp. 334–341.

[17] S. Liu, S. Kar, M. Fardad, and P. K. Varshney, “On optimal sensor
collaboration topologies for linear coherent estimation,” in Proceedings
of IEEE International Symposium on Information Theory (ISIT), 2014,
pp. 2624–2628.

[18] S. Liu, S. Kar, M. Fardad, and P. K. Varshney, “Sparsity-aware sensor
IEEE Transactions on

collaboration for linear coherent estimation,”
Signal Processing, vol. 63, no. 10, pp. 2582–2596, May 2015.

[19] M. C. Vuran, O. B. Akan, and I. F. Akyildiz, “Spatio-temporal correla-
tion: theory and applications for wireless sensor networks,” Computer
Networks, vol. 45, no. 3, pp. 245–259, June 2004.

[20] M. C. Vuran and O. B. Akan,

“Spatio-temporal characteristics of
point and ﬁeld sources in wireless sensor networks,” in Proc. IEEE
International Conference on Communications (ICC), June 2006, vol. 1,
pp. 234–239.

[21] Phaeton C. Kyriakidis, “A spatial time series framework for modeling
daily precipitation at regional scales,” Journal of Hydrology, vol. 297,
no. 4, pp. 236 – 255, Apr. 2004.

[22] A. L. Yuille and A. Rangarajan,

“The concave-convex procedure,”

[23] T. Lipp and S. Boyd,

Neural Computation, vol. 15, no. 4, pp. 915–936, 2003.
“Variations and extensions of the convex-
concave procedure,” http://web.stanford.edu/∼boyd/papers/pdf/cvx ccv.
pdf, 2014.

[24] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed
optimization and statistical learning via the alternating direction method
of multipliers,” Foundations and Trends in Machine Learning, vol. 3,
no. 1, pp. 1–122, 2011.

[25] S. M. Kay, Fundamentals of Statistical Signal Processing, Volume I:

Estimation Theory, Prentice Hall, Englewood Cliffs, NJ, 1993.

[26] A. Beck, “Quadratic matrix programming,” SIAM Journal on Optimiza-

tion, vol. 17, no. 4, pp. 1224–1238, 2007.

[27] F. Bugarin, D. Henrion, and J.-B. Lasserre, “Minimizing the sum of
many rational functions,” Mathematical Programming Computation, pp.
1–29, 2015.

[28] S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge

University Press, Cambridge, 2004.

[29] A. J. Smola, S. V. N. Vishwanathan, and T. Hofmann, “Kernel methods
for missing variables,” Proceedings of the 10th International Workshop
on Artiﬁcial Intelligence and Statistics (AISTATS), 2005.

[30] A. Nemirovski,

“Interior point polynomial time methods in convex
programming,” 2012 [Online], Available: http://www2.isye.gatech.edu/
∼nemirovs/Lect IPM.pdf .

[31] Z.-Q. Luo, W.-K. Ma, A. M.-C. So, Y. Ye, and S. Zhang, “Semideﬁnite
relaxation of quadratic optimization problems,” IEEE Signal Processing
Magazine, vol. 27, no. 3, pp. 20–34, May 2010.

[32] N. Parikh and S. Boyd, “Proximal algorithms,” Foundations and Trends

in Optimization, vol. 1, no. 3, pp. 123–231, 2013.

[33] B. O’Donoghue, E. Chu, N. Parikh, and S. Boyd, “Operator splitting
for conic optimization via homogeneous self-dual embedding,” Arxiv
preprint http://arxiv.org/abs/1312.3039, 2013.

[34] Y. Shi, J. Zhang, B. O’Donoghue, and K. B. Letaief,

“Large-scale
convex optimization for dense wireless cooperative networks,” IEEE
Transactions on Signal Processing, vol. 63, no. 18, pp. 4729–4743, Sept.
2015.

[35] Inc. CVX Research, “CVX: Matlab software for disciplined convex

programming, version 2.0,” http://cvxr.com/cvx, Aug 2012.

14

