6
1
0
2

 
r
a

M
9

 

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
1
9
7
2
0

.

3
0
6
1
:
v
i
X
r
a

Asymptotically optimal, sequential,

multiple testing procedures with prior
information on the number of signals

Y. Song and G. Fellouris

Department of Statistics, University of Illinois, Urbana-Champaign,

725 S. Wright Street, Champaign 61820, USA

e-mail: ysong44@illinois.edu and fellouri@illinois.edu

Abstract: Assuming that data are collected sequentially from indepen-
dent streams, we consider the simultaneous testing of multiple binary hy-
potheses under two general setups; when the number of signals (correct
alternatives) is known in advance, and when we only have a lower and
an upper bound for it. In each of these setups, we propose feasible pro-
cedures that control, without any distributional assumptions, the fami-
lywise error probabilities of both type I and type II below given, user-
speciﬁed levels. Then, in the case of i.i.d. observations in each stream,
we show that the proposed procedures achieve the optimal expected
sample size, under every possible signal conﬁguration, asymptotically as
the two error probabilities vanish. A simulation study is presented in
a completely symmetric case and supports insights obtained from our
asymptotic results, such as the fact that knowledge of the exact number
of signals roughly halves the expected number of observations compared
to the case of no prior information.

MSC 2010 subject classiﬁcations: Primary 62L10; 60G40.
Keywords and phrases: Multiple testing, sequential analysis, asymp-
totic optimality, prior information.

1. Introduction

0 versus H k

Multiple testing, that is the simultaneous consideration of K hypothesis
1 , 1 ≤ k ≤ K, is one of the oldest, yet still
testing problems, H k
very active areas of statistical research. The vast majority of work in this
area assumes a ﬁxed set of observations and focuses on testing procedures
that control the familywise type I error (i.e., at least one false positive), as
in Marcus, Eric and Gabriel (1976); Holm (1979); Hommel (1988), or less
stringent metrics of this error, as in Benjamini and Hochberg (1995) and
Lehmann and Romano (2005).

The multiple testing problem has been studied less under the assump-
tion that observations are acquired sequentially, in which case the sam-

1

Y. Song and G. Fellouris/ Sequential Multiple Testing

2

ple size is random and determined by the experimenter. The sequential
setup is relevant in many applications, such as multichannel signal detec-
tion (Mei, 2008; Dragalin, Tartakovsky and Veeravalli, 1999), outlier de-
tection (Li, Nitinawarat and Veeravalli, 2014), clinical trials with multiple
end-points (Bartroﬀ and Lai, 2008), in which it is vital to make a decision
in real time, using the smallest possible number of observations.

Bartroﬀ and Lai (2010) were the ﬁrst to propose a sequential test that
controls the familywise error of type I. De and Baron (2012a,b) and Bartroﬀ
and Song (2014) proposed universal sequential procedures that control si-
multaneously the familywise errors of both type I and type II, a feature
that is possible due to the sequential nature of sampling. The proposed se-
quential procedures in these works were shown through simulation studies
to oﬀer substantial savings in the average sample size in comparison to the
corresponding ﬁxed-sample size tests.

A very relevant problem to multiple testing is the classiﬁcation problem,
in which there are M mutually exclusive hypotheses, H1, . . . , HM , and the
goal is to select the correct one among them. The classiﬁcation problem has
been studied extensively in the literature of sequential analysis, see e.g. Sobel
and Wald (1949); Armitage (1950); Lorden (1977); Dragalin, Tartakovsky
and Veeravalli (1999, 2000) in the eﬀort to generalize the foundational work
of Wald (1945) on binary testing (M = 2). Dragalin, Tartakovsky and Veer-
avalli (2000) considered the multiple testing problem as a special case of the
classiﬁcation problem under the assumption of a single signal in K inde-
pendent streams, and focused on procedures that control the probability of
erroneously claiming the signal to be in stream i for every 1 ≤ i ≤ M = K.
In this framework, they proposed an asymptotically optimal sequential test
as all these error probabilities go to 0. The same approach of treating the
multiple testing problem as a classiﬁcation problem has been taken by Li,
Nitinawarat and Veeravalli (2014) under the assumption of an upper bound
on the number of signals in the K independent streams, and a single control
on the maximal mis-classiﬁcation probability.

We should stress that interpreting multiple testing as a classiﬁcation prob-
lem does not generally lead to feasible procedures. Consider, for example, the
case of no prior information, which is the default assumption in the multiple
testing literature. Then, multiple testing becomes a classiﬁcation problem
with M = 2K categories and a brute-force implementation of existing clas-
siﬁcation procedures becomes infeasible even for moderate values of K, as
the number of statistics that need to be computed sequentially grows expo-
nentially with K. Independently of feasibility considerations, to the best of
our knowledge there is no optimality theory regarding the expected sample

Y. Song and G. Fellouris/ Sequential Multiple Testing

3

size that can be achieved by multiple testing procedures, with or without
prior information, that control the familywise errors of both type I and type
II. Filling this gap was one of the motivations of this paper.

The main contributions of the current work are the following: ﬁrst of all,
assuming that the data streams that correspond to the various hypotheses
are independent, we propose feasible procedures that control the familywise
errors of both type I and type II below arbitrary, user-speciﬁed levels. We
do so under two general setups regarding prior information; when the true
number of signals is known in advance, and when there is only a lower and
an upper bound for it. The former setup includes the case of a single signal
considered in Dragalin, Tartakovsky and Veeravalli (1999, 2000), whereas
the latter includes the case of no prior information, which is the underlying
assumption in De and Baron (2012a,b); Bartroﬀ and Song (2014). While we
provide universal threshold values that guarantee the desired error control
in the spirit of the above works, we also propose a Monte Carlo simulation
method based on importance sampling for the eﬃcient calculation of non-
conservative thresholds in practice, even for very small error probabilities.
More importantly, in the case of independent and identically distributed
(i.i.d.) observations in each stream, we show that the proposed multiple
testing procedures attain the optimal expected sample size, for any possi-
ble signal conﬁguration, to a ﬁrst-order asymptotic approximation as the
two error probabilities vanish. Our asymptotic results also provide insights
about the eﬀect of prior information on the number of signals, which are
corroborated by a small simulation study.

The remainder of the paper is organized as follows. In Section 2 we for-
mulate the problem mathematically. In Section 3 we present the proposed
procedures and show how they can be designed to guarantee the desired
error control, whereas in Section 4 we propose an eﬃcient Monte Carlo sim-
ulation method for the determination of non-conservative critical values in
practice. In Section 5 we establish the asymptotic optimality of the proposed
procedures in the i.i.d. setup. In Section 6 we illustrate our asymptotic re-
sults with a simulation study. In Section 7 we conclude and discuss potential
generalizations of our work. Finally, we present two useful lemmas for our
proofs in an Appendix.

2. Problem formulation
Consider K independent streams of observations, X k := {X k
n : n ∈ N},
k ∈ [K], where [K] := {1, . . . , K} and N := {1, 2, . . .}. For each k ∈ [K], let

Y. Song and G. Fellouris/ Sequential Multiple Testing

4

Pk be the distribution of X k, for which we consider two simple hypotheses,

H k

0 : Pk = Pk

0 versus H k

1 : Pk = Pk
1,

0 and Pk

where Pk
1 are distinct probability measures on the canonical space
of X k. We will say that there is “noise” in the kth stream under Pk
0 and
“signal” under Pk
1. Our goal is to simultaneously test these K hypotheses
when data from all streams become available sequentially and we want to
make a decision as soon as possible.
Let Fn be the σ-ﬁeld generated by all streams up to time n, i.e., Fn =
σ(X1, . . . , Xn), where Xn = (X 1
n ). Then, a sequential test for this
multiple testing problem is a pair (T, d) that consists of an {Fn}-stopping
time T , at which we stop sampling in all streams, and an FT -measurable
decision rule, d = (d1, . . . , dK), each component of which takes values in
{0, 1}, such that we declare upon stopping that there is signal (resp. noise)
in the kth stream when dk = 1 (resp. dk = 0). With an abuse of notation,
we will also use d to denote the subset of streams in which we declare that
signal is present, that is, we also set d = {k ∈ [K] : dk = 1}.
For any subset A ⊂ [K] we deﬁne the probability measure

n, . . . , X K

K(cid:79)

k=1

PA :=

Pk;

Pk =

(cid:40)

Pk
0,
Pk
1,

if k /∈ A
if k ∈ A ,

such that the distribution of {Xn, n ∈ N} is PA when A is the true subset
of signals, and for an arbitrary sequential test (T, d) we set:

{A (cid:46) d} := {d \ A (cid:54)= ∅} =

{d (cid:46) A} := {A \ d (cid:54)= ∅} =

{dj = 1},

{dk = 0}.

(cid:91)
(cid:91)

j(cid:54)∈A

k∈A

Then, PA(A (cid:46) d) is the probability of at least one false positive (familywise
type I error ) and PA (d (cid:46) A) the probability of at least one false negative
(familywise type II error ) of (T, d) when the true subset of signals is A.
In this work we are interested in sequential tests that control these prob-
abilities below user-speciﬁed levels α and β respectively, where α, β ∈ (0, 1),
for any possible subset of signals A. In order to be able to incorporate prior
information, we assume that the true subset of signals is known to belong
to a class P of subsets of [K], not necessarily equal to the powerset, and we
focus on sequential tests in the class
∆α,β(P) := {(T, d) : PA(A (cid:46) d) ≤ α and PA (d (cid:46) A) ≤ β for every A ∈ P} .

Y. Song and G. Fellouris/ Sequential Multiple Testing

5

We consider, in particular, two general cases for class P. In the ﬁrst one,
it is known that there are exactly m signals in the K streams, where 1 ≤
m ≤ K − 1. In the second, it is known that there are at least (cid:96) and at most
u signals, where 0 ≤ (cid:96) < u ≤ K. In the former case we write P = Pm and
in the latter P = P(cid:96),u, where

Pm := {A ⊂ [K] : |A| = m} , P(cid:96),u := {A ⊂ [K] : (cid:96) ≤ |A| ≤ u} .

When (cid:96) = 0 and u = K, the class P(cid:96),u is the powerset of [K], which cor-
responds to the case of no prior information regarding the multiple testing
problem.
However, our main focus is on multiple testing procedures that not only
belong to ∆α,β(P) for a given class P, but also achieve the minimum possible
expected sample size, under each possible signal conﬁguration, for small error
probabilities. To be more speciﬁc, let P be a given class of subsets and let
(T ∗, d∗) be a sequential test that can designed to belong to ∆α,β(P) for
any given α, β ∈ (0, 1). We say that (T ∗, d∗) is asymptotically optimal with
respect to class P, if for every A ∈ P we have as α, β → 0

EA [T ∗] ∼

inf

(T,d)∈∆α,β (P)

EA [T ] ,

where EA refers to expectation under PA and x ∼ y means that x/y → 1.
The ultimate goal of this work is to propose feasible sequential tests that
are asymptotically optimal with respect to classes of the form Pm and P(cid:96),u.

2.1. Notation

Before we continue with the presentation and analysis of the proposed mul-
tiple testing procedures, we need to introduce some additional notations.
First of all assume that for each stream k ∈ [K] and ﬁnite time n ∈ N
1 are equivalent when restricted to the

the probability measures Pk
σ-algebra F k
1 , . . . , X k

n = σ(X k

0 and Pk
n), and we denote by

λk(n) := log

(F k
n)

dPk
1
dPk
0

(1)

the cumulative log-likelihood ratio statistic up to time n based on the data
in the kth stream. Moreover, for any two subsets A,C ⊂ [K] we denote by
λA,C the log-likelihood ratio process of PA versus PC, i.e.,

(cid:88)

λk(n) − (cid:88)

k∈A\C

k∈C\A

λA,C(n) := log

dPA
dPC

(Fn) =

λk(n), n ∈ N.

(2)

Y. Song and G. Fellouris/ Sequential Multiple Testing

6

To avoid trivial cases, we assume that the two hypotheses in each stream

are well separated, in the sense that for each k ∈ [K] we have

(cid:16)

n→∞ λk(n) = −∞(cid:17)

lim

Pk
0

(cid:16)

n→∞ λk(n) = ∞(cid:17)

lim

= Pk
1

= 1.

(3)

This assumption is necessary in order to design non-trivial stopping times
that terminate with probability 1 under every scenario. Apart from that, we
do not make any other distributional assumption about the data until Section
5.

We use the following notation for the ordered log-likelihood ratio statistics

at time n:

λ(1)(n) ≥ . . . ≥ λ(K)(n),

and we denote by i1(n), . . . , iK(n) the corresponding stream indices, i.e.,

λ(k)(n) = λik(n)(n), for every k ∈ [K].

Moreover, for every n ∈ N we denote by p(n) the number of positive log-
likelihood ratio statistics at time n, i.e.,

λ(1)(n) ≥ . . . ≥ λ(p(n))(n) > 0 ≥ λ(p(n)+1)(n) ≥ . . . ≥ λ(K)(n).

Finally, we use |·| to denote set cardinality, for any two real numbers x, y
we set x ∧ y = min{x, y} and x ∨ y = max{x, y}, and for any measurable
event Γ and random variable Y we use the following notation

EA[Y ; Γ] :=

Y dPA.

Γ

3. Proposed sequential multiple testing procedures

In this section we present the proposed procedures and show how they can
be designed in order to guarantee the desired error control.

3.1. Known number of signals

In this subsection we consider the setup in which the number of signals is
known to be equal to m for some 1 ≤ m ≤ K − 1, thus, P = Pm. Without
loss of generality, we restrict ourselves to multiple testing procedures (T, d)
such that |d| = m, and the class of admissible sequential tests takes the form

∆α,β(Pm) = {(T, d) : PA(d (cid:54)= A) ≤ α ∧ β for every A ∈ Pm} ,

(cid:90)

Y. Song and G. Fellouris/ Sequential Multiple Testing

7

since for any A ∈ Pm and (T, d) such that |d| = m we have

{A (cid:46) d} = {d (cid:46) A} = {d (cid:54)= A}.

In this context, we propose the following sequential scheme: stop as soon
as the gap between the m-th and (m + 1)-th ordered log-likelihood ratio
statistics becomes larger than some constant c > 0, and declare that signal
is present in the m streams with the top log-likelihood ratios at the time of
stopping. In other words, we propose the following sequential test, to which
we refer as “gap rule”:

n ≥ 1 : λ(m)(n) − λ(m+1)(n) ≥ c

TG := inf
dG := {i1(TG), . . . , im(TG)}.

,

(4)

(cid:110)

(cid:111)

Here, we suppress the dependence of (TG, dG) on m and c to lighten the
notation. The next theorem shows how to select threshold c in order to
guarantee the desired error control.
Theorem 3.1. Suppose that assumption (3) holds. Then, for any A ∈ Pm
and c > 0 we have PA(TG < ∞) = 1 and

PA (dG (cid:54)= A) ≤ m(K − m)e−c.

Consequently, (TG, dG) ∈ ∆α,β(Pm) when threshold c is selected as

c = | log(α ∧ β)| + log(m(K − m)).
Proof. Fix A ∈ Pm and c > 0. We observe that TG ≤ T (cid:48)

(cid:110)
n ≥ 1 : λ(m)(n) − λ(m+1)(n) ≥ c, i1(n) ∈ A, . . . , im(n) ∈ A(cid:111)
(cid:110)
n ≥ 1 : λk(n) − λj(n) ≥ c for every k ∈ A and j /∈ A(cid:111)

G, where

T (cid:48)
G = inf

= inf

.

(5)

(6)

(7)
G < ∞) = 1, which proves that
Due to condition (3), it is clear that PA(T (cid:48)
TG is also almost surely ﬁnite under PA. We now focus on proving (5). The
gap rule makes a mistake under PA if there exist k ∈ A and j /∈ A such that

the event Γk,j =(cid:8)λj(TG) − λk(TG) ≥ c(cid:9) occurs. In other words,

(cid:91)

{dG (cid:54)= A} =

Γk,j,

k∈A,j /∈A

Y. Song and G. Fellouris/ Sequential Multiple Testing

8

and from Boole’s inequality we have

PA(dG (cid:54)= A) ≤ (cid:88)
PA(Γk,j) = EC(cid:2)exp{λA,C(TG)}; Γk,j

k∈A,j /∈A

(cid:3)

(cid:104)

PA(Γk,j).

Fix k ∈ A, j /∈ A and set C = A ∪ {j} \ {k}. Then, from (2) we have that
λA,C = λk − λj and from Wald’s likelihood ratio identity it follows that

(cid:105) ≤ e−c,

= EC

exp{λk(TG) − λj(TG)}; Γk,j

(8)

where the last inequality holds because λj(TG) − λk(TG) ≥ c on Γk,j. Since
|A| = m and |Ac| = K − m, from the last two inequalities we obtain (5),
which completes the proof.

3.2. Lower and upper bounds on the number of signals

In this subsection, we consider the setup in which we know that there are
at least (cid:96) and at most u signals for some 0 ≤ (cid:96) < u ≤ K, that is, P = P(cid:96),u.
In order to describe the proposed procedure, it is useful to ﬁrst introduce
the “intersection rule”, (TI , dI ), according to which we stop sampling as soon
as all log-likelihood ratio statistics are outside the interval (−a, b), and at
this time we declare that signal is present (resp. absent) in those streams
with positive (resp. negative) log-likelihood ratio, i.e.,

n ≥ 1 : λk(n) (cid:54)∈ (−a, b) for every k ∈ [K]

TI := inf
dI := {i1(TI ), . . . , ip(TI )(TI )},

,

(9)

(cid:110)

(cid:111)

recalling that p(n) is the number of positive log-likelihood ratios at time n.
This procedure was proposed by De and Baron (2012a), where it was also
shown that the familywise type-I and type-II error probabilities are bounded
by α and β for any possible signal conﬁguration, i.e., (TI , dI ) ∈ ∆α,β(P0,K),
when the thresholds are selected as follows:

a = | log β| + log K,

b = | log α| + log K.

(10)

A straightforward way to incorporate in the intersection rule the prior
information of at least (cid:96) and at most u signals is to modify the stopping
time in (9) as follows:

τ2 := inf

n ≥ 1 : (cid:96) ≤ p(n) ≤ u and λk(n) (cid:54)∈ (−a, b) for every k ∈ [K]

,

(cid:110)

(cid:111)

(11)

Y. Song and G. Fellouris/ Sequential Multiple Testing

9

while keeping the same decision rule as in (9). Indeed, stopping according
to τ2 guarantees that the number of null hypotheses rejected upon stopping
will be between (cid:96) and u. However, as we will see in Subsection 5.3, this
rule will not in general achieve asymptotic optimality in the boundary cases
of exactly (cid:96) and exactly u signals. In order to obtain an asymptotically
optimal rule under every possible signal conﬁguration, we need to be able
to stop faster when there are exactly (cid:96) or u signals, which can be achieved
by stopping at

(cid:110)
(cid:110)

τ1 := inf

and τ3 := inf

n ≥ 1 : λ((cid:96)+1)(n) ≤ −a, λ((cid:96))(n) − λ((cid:96)+1)(n) ≥ c
n ≥ 1 : λ(u)(n) ≥ b, λ(u)(n) − λ(u+1)(n) ≥ d

,

(cid:111)

(cid:111)

,

respectively. Here, c and d are additional positive thresholds that will be
selected, together with a and b, in order to guarantee the desired error
control.

We can think of τ1 as a combination of the intersection rule and the gap
rule that corresponds to the case of exactly (cid:96) signals. Indeed, τ1 stops when
K − (cid:96) log-likelihood ratio statistics are simultaneously below −a, but unlike
the intersection rule it does not wait for the remaining (cid:96) statistics to be
larger than b; instead, similarly to the gap-rule in (4) with m = (cid:96), it requires
the gap between the top (cid:96) and the bottom K − (cid:96) statistics to be larger than
c. In a similar way, τ3 is a combination of the intersection rule and the gap
rule that corresponds to the case of exactly u signals.

Based on the above discussion, when we know that there are at least (cid:96)
and at most u signals, we propose the following procedure, to which we refer
as “gap-intersection” rule:

TGI := min{τ1, τ2, τ3},

dGI := {i1(TGI ), . . . , ip(cid:48)(TGI )},

(12)
where p(cid:48) := (p(TGI ) ∧ (cid:96)) ∨ u is a truncated version of the number of positive
log-likelihood ratios at TGI , i.e., if p(cid:48) = (cid:96) when p(TGI ) ≤ (cid:96), p(cid:48) = u when
p(TGI ) ≥ u and p(cid:48) = p(TGI ) otherwise. In other words, we stop sampling as
soon as one of the stopping criterion in τ1, τ2 or τ3 is is satisﬁed, and we
reject upon stopping the null hypotheses in the p(cid:48) streams with the highest
log-likelihood ratio values at time TGI .
As before, we suppress the dependence on (cid:96), u and a, b, c, d in order to
lighten the notation. Moreover, we set λ(0)(n) = −∞ and λ(K+1)(n) = ∞
for every n ∈ N, which implies that if (cid:96) = 0, then τ1 = ∞, and if u = K,
then τ3 = ∞. When in particular (cid:96) = 0 and u = K, that is the case of no

Y. Song and G. Fellouris/ Sequential Multiple Testing

10

prior information, TGI = τ2 and (TGI , dGI ) reduces to the intersection rule,
(TI , dI ), deﬁned in (9).

The following theorem shows how to select thresholds a, b, c, d in order to

guarantee the desired error control for the gap-intersection rule.
Theorem 3.2. Suppose that assumption (3) holds. For any subset A ∈ P(cid:96),u
and positive thresholds a, b, c, d, we have PA(TGI < ∞) = 1 and

PA(A (cid:46) dGI ) ≤ |Ac|(cid:16)
PA(dGI (cid:46) A) ≤ |A|(cid:16)

e−b + |A| e−c(cid:17)
e−a + |Ac| e−d(cid:17)

,

.

(13)

In particular, (TGI , dGI ) ∈ ∆α,β(P(cid:96),u) when the thresholds a, b, c, d are se-
lected as follows:

a = | log β| + log K,
b = | log α| + log K,

d = | log β| + log(uK),
c = | log α| + log((K − (cid:96))K).

(14)

Proof. Fix A ∈ P(cid:96),u and a, b, c, d > 0. Observe that TGI ≤ τ2 ≤ τ(cid:48)

2, where

2 = inf{n ≥ 1 : −λj(n) ≥ a, λk(n) ≥ b for every k ∈ A, j /∈ A}.
τ(cid:48)

(15)
2 < ∞) = 1, which proves that TGI is also
Due to assumption (3), PA(τ(cid:48)
almost surely ﬁnite under PA. We now focus on proving the bound in (13)
for the familywise type-II error probability, since the corresponding result for
the familywise type-I error can be shown similarly. From Boole’s inequality
we have

PA(dGI (cid:46) A) = PA

{dk

GI = 0}

PA

dk
GI = 0

.

(16)

(cid:32)(cid:91)

k∈A

(cid:33)

≤(cid:88)

k∈A

(cid:16)

(cid:17)

Fix k ∈ A. Whenever the gap-intersection rule mistakenly accepts H k
0 , either
the event Γk := {λk(TGI ) ≤ −a} occurs (which is the case when stopping
at τ1 or τ2), or there is at least one j /∈ A such that the event Γk,j :=
{λj(TGI ) − λk(TGI ) ≥ d} occurs (which is the case when stopping at τ3).
Therefore,

{dk

GI = 0} ⊂ Γk ∪ (∪j /∈AΓk,j),

and from Boole’s inequality we have

PA(dk

GI = 0) ≤ PA(Γk) +

PA (Γk,j) .

(cid:88)

j /∈A

Y. Song and G. Fellouris/ Sequential Multiple Testing

11

Identically to (8) we can show that for every j /∈ A we have PA (Γk,j) ≤ e−d.
Moreover, if we set C = A\{k} (note that C /∈ P(cid:96),u, but this does not aﬀect
our argument), then λA,C = λk and from Wald’s likelihood ratio identity we
have

PA(Γk) = EC(cid:2)exp{λA,C(TGI )}; Γk

(cid:3) = EC

(cid:104)

exp{λk(TGI )}; Γk

(cid:105) ≤ e−a.

Thus,

PA(dk

GI = 0) ≤ e−a + (K − |A|)e−d,

which together with (16) yields

PA(dGI (cid:46) A) ≤ |A|(e−a + |Ac|e−d) ≤ |A|

K

(Ke−a) +

|Ac|
K

(uKe−d).

Therefore, if the thresholds are selected according to (14), then Ke−a = β
and uKe−d = β, which implies that

PA(dGI (cid:46) A) ≤ |A|

K

β +

|Ac|
K

β = β,

and the proof is complete.

4. Computation of familywise error probabilities via importance

sampling

The threshold speciﬁcations in (6) and (14) guarantee the desired error
control for the gap rule and gap-intersection rule respectively, however they
can be extremely conservative. In practice, it is preferable to use Monte Carlo
simulation to determine the thresholds that equate (at least, approximately)
the familywise type I and type II error probabilities to the corresponding
target levels α and β, respectively. Note that this needs to be done oﬄine,
before the implementation of the procedure. However, when α and β are very
small, the corresponding errors are “rare events” and plain Monte Carlo will
not be eﬃcient. For this reason, in this section we propose a Monte Carlo
approach based on importance sampling for the eﬃcient computation of the
familywise error probabilities of the proposed multiple testing procedures.
To be more speciﬁc, let A ⊂ [K] be the true subset of signals and consider
the computation of the familywise type I error probability, PA(A (cid:46) d), of an
arbitrary multiple testing procedure, (T, d). The idea of importance sampling
is to ﬁnd a probability measure P∗
A, under which the stopping time T is ﬁnite

Y. Song and G. Fellouris/ Sequential Multiple Testing

12

almost surely, and compute the desired probability by estimating (via plain
Monte Carlo) the expectation in the right-hand side of the following identity:

A(cid:2)(Λ∗

A)−1;A (cid:46) d(cid:3) ,

PA(A (cid:46) d) = E∗

which is obtained by an application of Wald’s likelihood ratio identity. Here,
we denote by Λ∗

A the likelihood ratio of P∗

A against PA at time T , i.e.,
(FT ),

Λ∗
A =
A the expectation under P∗

dP∗
A
dPA
and by E∗
A. The proposal distribution P∗
A should
A is “large” on the event {A (cid:46) d} and “small” on
be selected such that Λ∗
its complement. This intuition will guide us in the selection of P∗
A for the
proposed rules. To motivate the selection, let us assume for the moment
that the hypotheses are identical, in the sense that Pk
0 = P0
for every k ∈ [K].
uniform mixture over {PA∪{j}\{k}, k ∈ A, j /∈ A}, i.e.,

For the gap rule (TG, dG) we suggest the proposal distribution to be a

1 = P1 and Pk

PA∪{j}\{k},

(17)

PGA :=

1

|A||Ac|

(cid:88)

(cid:88)

k∈A

j /∈A

(cid:88)

(cid:88)

k∈A

j /∈A

(cid:88)

j /∈A

(cid:88)

j /∈A

whose likelihood ratio against PA at time TG is

ΛGA :=

1

|A||Ac|

exp{λj(TG) − λk(TG)}.

Then, on the event {A (cid:46) dG} there exists some k ∈ A and j /∈ A such that
λj(TG)−λk(TG) ≥ c, which leads to a large value for ΛGA. On the other hand,
on the complement of {A (cid:46) dG}, {dG = A}, we have λj(TG)− λk(TG) ≤ −c
for every k ∈ A, j /∈ A, which leads to a value of ΛGA close to 0.
be a uniform mixture over {PA∪{j}, j /∈ A}, i.e.,

For the intersection rule (TI , dI ) we suggest the proposal distribution to

PIA :=

1
|Ac|

PA∪{j},

(18)

whose likelihood ratio against PA at time TI takes the form

ΛIA :=

1
|Ac|

exp{λj(TI )}.

Y. Song and G. Fellouris/ Sequential Multiple Testing

13

Note that on the event {A (cid:46) dI} there exists some j /∈ A such that
λj(TI ) ≥ b, which results in a large value for ΛIA. On the other hand, on
the complement of {A (cid:46) dI} we have λj(TI ) ≤ −a for every j /∈ A, which
results in a value of ΛIA close to 0.
Finally, for the gap-intersection rule we suggest to use PIA, the same pro-
posal distribution as in the intersection rule, when (cid:96) < |A| < u. In the
boundary case, i.e. |A| = (cid:96) or |A| = u, we propose the following mixture of
PGA and PIA:

PGIA :=

|A|
1 + |A| PGA +

1

1 + |A| PIA.

In Section 6 we apply the proposed simulation approach for the speci-
ﬁcation of non-conservative thresholds in the case of identical, symmetric
hypotheses with Gaussian i.i.d. data. When we have non-identical hypothe-
sis testing problems, we believe that the proposal distributions PGA and PIA,
deﬁned in (17) and (18), can be improved if we select instead uniform mix-
tures over {PA∪{j}\{k}, k ∈ A1, j ∈ A0} and {PA∪{j}, j ∈ A0} respectively,
where A1 and A0 are the subsets of the most “diﬃcult” hypotheses in A and
Ac, respectively. It is an interesting topic for future research, but beyond
the scope of the present paper, to determine the subsets A1 and A0 and
establish any optimality properties of the above proposal distributions.

5. Asymptotic optimality in the i.i.d. setup
From now on, we assume that, for each stream k ∈ [K], the observations
{X k
n, n ∈ N} are independent random variables with common density f k
i
with respect to a σ-ﬁnite measure µk under Pk
i , i = 0, 1, such that the
Kullback–Leibler information numbers

(cid:90)

(cid:19)

(cid:18) f k

0
f k
1

(cid:90)

(cid:19)

(cid:18) f k

1
f k
0

Dk

0 :=

log

f k
0 dµk, Dk

1 :=

log

f k
1 dµk

are both positive and ﬁnite. As a result, for each k ∈ [K] the log-likelihood
ratio process in the kth stream, deﬁned in (1), takes the form

n(cid:88)

j=1

λk(n) =

log

1 (X k
f k
j )
f k
0 (X k
j )

, n ∈ N,

and it is a random walk with drift Dk

1 under Pk

1 and −Dk

0 under Pk
0.

Y. Song and G. Fellouris/ Sequential Multiple Testing

14

Our goal in this section is to show that the proposed multiple testing
procedures in Section 3 are asymptotically optimal. Our strategy for proving
this is ﬁrst to establish a non-asymptotic lower bound on the minimum
possible expected sample size in ∆α,β(P) for some arbitrary class P, and then
show that this lower bound is attained by the gap rule when P = Pm and
by the gap-intersection rule when P = P(cid:96),u as α, β → 0 such that α log(β) +
β log(α) → 0. This is a minor restriction on the rates at which α and β
approach zero, and is satisﬁed for example when β = c1αc2 for arbitrary
positive constants c1, c2. Therefore, our asymptotic optimality results will
apply even when α and β are of very diﬀerent orders of magnitude, allowing
for a very asymmetric treatment of the two types of error, which may be
desirable in many applications.

5.1. A lower bound on the optimal performance

In order to state the lower bound on the optimal performance, we introduce
the function

ϕ(x, y) := x log

+ (1 − x) log

x, y ∈ (0, 1),

,

(19)

(cid:18) 1 − x

(cid:19)

y

1 − y

(cid:18) x
(cid:19)
ϕ(α, β),

and for any subsets C,A ⊂ [K] such that C (cid:54)= A we set

γA,C(α, β) :=

ϕ(β, α),
ϕ(α, β) ∨ ϕ(β, α),

if C \ A (cid:54)= ∅, A \ C = ∅,
if C \ A = ∅, A \ C (cid:54)= ∅,
otherwise.

Theorem 5.1. For any class P, A ∈ P and α, β ∈ (0, 1) such that α+β < 1
we have

inf

(T,d)∈∆α,β (P)

EA[T ] ≥ max
C∈P,C(cid:54)=A

(cid:80)

γA,C(α, β)

1 +(cid:80)

k∈A\C Dk

k∈C\A Dk
0

.

(20)

Proof. Fix (T, d) ∈ ∆α,β(P) and A ∈ P. Without loss of generality, we
assume that EA[T ] < ∞. For any C ∈ P such that C (cid:54)= A, the log-likelihood
ratio process λA,C, deﬁned in (2), is a random walk under PA with drift
equal to

(cid:88)

(cid:88)

EA[λA,C(1)] =

Dk

1 +

Dk
0 ,

k∈A\C

k∈C\A

Y. Song and G. Fellouris/ Sequential Multiple Testing

15

since each λk is a random walk with drift Dk
Thus, from Wald’s identity it follows that

1 under Pk

1 and −Dk

0 under Pk
0.

EA[T ] =

(cid:80)

EA[λA,C(T )]

1 +(cid:80)

k∈A\C Dk

,

k∈C\A Dk
0

and it suﬃces to show that for any C ∈ P such that C (cid:54)= A we have

EA[λA,C(T )] ≥ γA,C(α, β).

(21)
Suppose that C \ A (cid:54)= ∅ and let j ∈ C \ A. Then, from Lemma A.1 in the
Appendix we have

(cid:21)

≥ ϕ(cid:0)PA(dj = 1), PC(dj = 0)(cid:1) .

EA(cid:2)λA,C(T )(cid:3) = EA

(cid:20)

log

dPA
dPC

(FT )

By the deﬁnition of ∆α,β(P), we have PA(dj = 1) ≤ α and PC(dj = 0) ≤ β.
Since the function ϕ(x, y) is decreasing on the set {(x, y) : x + y ≤ 1}, and
by assumption α + β ≤ 1, we conclude that if C \ A (cid:54)= ∅, then

EA[λA,C(T )] ≥ ϕ(α, β).

With a symmetric argument we can show that if A \ C (cid:54)= ∅, then

EA[λA,C(T )] ≥ ϕ(β, α).

The two last inequalities imply (21), and this completes the proof.

5.2. Asymptotic optimality of the proposed schemes
In what follows, we assume that for each stream k ∈ [K] we have:

(cid:90) (cid:18)

log

(cid:19)(cid:19)2

(cid:18) f k

0
f k
1

i dµk < ∞,
f k

i = 0, 1.

(22)

Although this assumption is not necessary for the asymptotic optimality of
the proposed rules to hold, it will allow us to use Lemma A.2 in the Appendix
and obtain valuable insights regarding the eﬀect of prior information on the
optimal performance. Moreover, for each subset A ⊂ [K] we set:

ηA
1 := min

k∈A Dk
1 ,

ηA
0 := min

j /∈A Dj
0,

and, following the convention that the minimum over the empty set is ∞,
we deﬁne: η∅

:= ∞.

1 = η[K]

0

Y. Song and G. Fellouris/ Sequential Multiple Testing

16

5.2.1. Known number of signals

We will ﬁrst show that the gap rule, deﬁned in (4), is asymptotically optimal
with respect to class Pm, where 1 ≤ m ≤ K − 1. In order to do so, we start
with an upper bound on the expected sample size of this procedure.
Lemma 5.2. Suppose that assumption (22) holds. Then, for any A ∈ Pm,
as c → ∞ we have

+ O(cid:0)m(K − m)

√

c(cid:1) .

EA[TG] ≤

c

1 + ηA
ηA

0

Proof. Fix A ∈ Pm. For any c > 0 we have TG ≤ T (cid:48)
G is deﬁned
in (7), and it is the ﬁrst time that all m(K−m) processes of the form λk−λj
with k ∈ A and j /∈ A exceed c. Due to condition (22), each λk − λj with
k ∈ A and j /∈ A is a random walk under PA with positive drift Dk
1 + Dj
0
and ﬁnite second moment. Therefore, from Lemma A.2 it follows that as
c → ∞:

G, where T (cid:48)

(cid:18)

(cid:19)−1

+ O(cid:0)m(K − m)

√

c(cid:1) ,

EA[T (cid:48)

G] ≤ c

k∈A,j /∈A(Dk
min

1 + Dj
0)

and this completes the proof, since mink∈A,j /∈A(Dk

1 + Dj

0) = ηA

1 + ηA
0 .

The next theorem establishes the asymptotic optimality of the gap rule.

Theorem 5.3. Suppose that assumption (22) holds and let the threshold c
in the gap rule be selected according to (6). Then, for every A ∈ Pm we have

EA[TG] ∼ | log(α ∧ β)|
ηA
1 + ηA

0

∼

inf

(T,d)∈∆α,β (Pm)

EA[T ]

as α, β → 0 such that α log(β) + β log(α) → 0.
Proof. Fix A ∈ Pm. If thresholds are selected according to (6), then from
Lemma 5.2 it follows that as α, β → 0

(cid:16)

m(K − m)(cid:112)| log(α ∧ β)|(cid:17)

.

(23)

EA[TG] ≤ | log(α ∧ β)|
ηA
1 + ηA

0

+ O

Therefore, it suﬃces to show that the lower bound in Theorem 5.1 agrees
with the upper bound in (23) in the ﬁrst-order term as α, β → 0. To see
this, note that for any C ∈ Pm such that C (cid:54)= A we have C \ A (cid:54)= ∅ and
A \ C (cid:54)= ∅, and consequently

γA,C(α, β) = ϕ(α, β) ∨ ϕ(β, α)

Y. Song and G. Fellouris/ Sequential Multiple Testing

17

This means that the numerator in (20) does not depend on C. Moreover, if
we restrict our attention to subsets in Pm that diﬀer from A in two streams,
i.e., subsets of the form C = A ∪ {j} \ {k} for some k ∈ A and j /∈ A, for
which

(cid:88)

then we have

min

C∈Pm,C(cid:54)=A

i∈A\C

Di

1 +

 (cid:88)

i∈A\C

(cid:88)

i∈C\A

Di

1 +

0 = Dk

1 + Dj
0,

i∈C\A

Di

(cid:88)
 ≤ min

Di
0

k∈A,j /∈A

(cid:104)

Dk

1 + Dj

0

(cid:105)

= ηA

1 + ηA
0 .

By the last inequality and Theorem 5.1 we obtain the following non-asymptotic
lower bound, which holds for any α, β such that α + β < 1:

EA[T ] ≥ max{ϕ(α, β), ϕ(β, α)}
When α, β → 0 such that α log(β) + β log(α) → 0, we have

(T,d)∈∆α,β (Pm)

1 + ηA
ηA

inf

0

.

ϕ(α, β) = − log(β) + o(1), ϕ(β, α) = − log(α) + o(1),

(24)

which implies max{ϕ(α, β), ϕ(β, α)} = | log(α ∧ β)| + o(1). Consequently,

inf

(T,d)∈∆α,β (Pm)

EA (T ) ≥ | log(α ∧ β)|
ηA
1 + ηA

0

+ o(1),

which completes the proof.

0 = f0, and consequently Dk
1 = D1 and ηA

Remark 5.1. It is interesting to consider the special case of identical hy-
potheses, in which f k
1 = f1 and f k
1 = D1 and
0 = D0 for every k ∈ [K]. Then, ηA
0 = D0 for every A ⊂ [K],
Dk
and from Theorem 5.3 it follows that the ﬁrst-order asymptotic approxima-
tion to the expected sample size of the gap rule (as well as to the optimal
expected sample size within ∆α,β(Pm)), | log(α∧ β)|/(D1 + D0), is indepen-
dent of the number of signals, m. However, we should stress that this does
not mean that the actual performance of the gap rule is independent of m.
Indeed, the second term in the right-hand side of (23) suggests that the
smaller m(K − m) is, i.e., the further away the proportion of signals m/K
is from 1/2, the smaller the expected sample size of the gap rule will be.
This intuition will be corroborated by the simulation study in Section 6 (see
Fig. 2).

Y. Song and G. Fellouris/ Sequential Multiple Testing

18

5.2.2. Lower and upper bounds on the number of signals

We will now show that the gap-intersection rule, deﬁned in (12), is asymp-
totically optimal with respect to class P(cid:96),u for some 0 ≤ (cid:96) < u ≤ K. As
before, we start with establishing an upper bound on the expected sample
size of this rule.
Lemma 5.4. Suppose that assumption (22) holds. Then, for any A ∈ P(cid:96),u
we have as a, b, c, d → ∞

0 + ηA

1 )(cid:9) (1 + o(1))
(cid:9) + O(K
1 )(cid:9) (1 + o(1))

a ∨ b)

0 + ηA

√

0 , c/(ηA
0 , b/ηA
1 , d/(ηA

1

|A| = (cid:96)

if
if (cid:96) < |A| < u
if

|A| = u

EA[TGI ] ≤

max(cid:8)a/ηA
max(cid:8)a/ηA
max(cid:8)b/ηA
(cid:40)

Furthermore, if c − a = O(1) and d − b = O(1), then

|A| = (cid:96)
|A| = u
Proof. Fix A ∈ P(cid:96),u. By the deﬁnition of the stopping time TGI ,

√
0 + O((K − (cid:96))
√
1 + O(u

EA[TGI ] ≤

a/ηA
b/ηA

if
if

a)

b)

(25)

EA[TGI ] ≤ min{EA[τ1], EA[τ2], EA[τ3]} .

Suppose ﬁrst (cid:96) < |A| < u and observe that τ2 ≤ τ(cid:48)

2 is deﬁned
in (15). Under condition (22), for every k ∈ A and j /∈ A, −λj and λk are
random walks with ﬁnite second moments and positive drifts Dj
0 and Dk
1 ,
respectively. Therefore, from Lemma A.2 we have that
√

2, where τ(cid:48)

2] ≤ max(cid:8)a/ηA

EA[τ(cid:48)

(cid:9) + O(K

0 , b/ηA

1

a ∨ b).

1, where

Suppose now that |A| = (cid:96) and observe that τ1 ≤ τ(cid:48)
1 := inf{n ≥ 1 : −λj(n) ≥ a, λk(n) − λj(n) ≥ c for every k ∈ A, j /∈ A},
τ(cid:48)
where −λj and λk − λj are random walks with ﬁnite second moments and
positive drifts Dj
0, respectively. The result follows again from
an application of Lemma A.2. If in addition we have that c− a = O(1), then
τ1 ≤ τ(cid:48)(cid:48)
1 , where
1 := inf{n ≥ 1 : −λj(n) ≥ a, λk(n) ≥ c − a for every k ∈ A, j /∈ A}.
τ(cid:48)(cid:48)

0 and Dk

1 + Dj

Therefore, the second part of the lemma follows again from an application
of Lemma A.2.

Y. Song and G. Fellouris/ Sequential Multiple Testing

19

The next theorem establishes the asymptotic optimality of the gap-inter-

section rule.

inf

EA[T ]

(T,d)∈∆α,β (P(cid:96),u)

max(cid:8)| log β|/ηA
max(cid:8)| log β|/ηA
max(cid:8)| log α|/ηA

Theorem 5.5. Suppose that assumption (22) holds and let the thresholds
in the gap-intersection rule be selected according to (14). Then, for any
A ∈ P(cid:96),u we have
EA[TGI ] ∼

(cid:9)
0 , | log α|/(ηA
0 + ηA
0 , | log α|/ηA
1 , | log β|/(ηA
0 + ηA
as α, β → 0 so that α log(β) + β log(α) → 0.
Proof. Fix A ∈ P(cid:96),u. We will prove the result only in the case that |A| = (cid:96),
as the other two cases can be proved similarly. If thresholds are selected
according to (14), then from Lemma 5.4 it follows that

if
if (cid:96) < |A| < u
if

1 )(cid:9)
1 )(cid:9)

|A| = u

|A| = (cid:96)

∼

1

EA[TGI ] ≤ max

(1 + o(1)).

(cid:26)| log β|

ηA

0

| log α|
0 + ηA
ηA

1

,

(cid:27)

Thus, it suﬃces to show that this asymptotic upper bound agrees asymp-
totically, up to a ﬁrst order, with the lower bound in Theorem 5.1. Indeed,
if C is a subset in P(cid:96),u that has one more stream than A, i.e., C = A ∪ {j}
for some j /∈ A, then

(cid:80)

γA,C(α, β)

1 +(cid:80)

i∈A\C Di

i∈C\A Di
0

=

ϕ(α, β)

Dj
0

.

Further, consider C = A ∪ {j}/{k} ∈ P(cid:96),u for some k ∈ A and j /∈ A, then

(cid:80)

γA,C(α, β)

1 +(cid:80)

i∈A\C Di

i∈C\A Di
0

=

max{ϕ(α, β), ϕ(β, α)}

Dk

1 + Dj

0

.

Therefore, from (5.1) it follows that for every α, β such that α + β < 1
max{ϕ(α, β), ϕ(β, α)}

ϕ(α, β)

inf

(T,d)∈∆α,β (P(cid:96),u)

EA[T ] ≥ max

k∈A,j /∈A max

(cid:41)

(cid:40)
(cid:26) ϕ(α, β)

,

ηA

0

= max

(cid:27)

,

Dj
0
ϕ(β, α)
ηA
1 + ηA

0

Dk

1 + Dj

0

.

Y. Song and G. Fellouris/ Sequential Multiple Testing

20

But from (24) it follows that

inf

(T,d)∈∆α,β (Pl,u)

EA[T ] ≥ max

(cid:26)| log β|

ηA

0

(cid:27)

| log α|
1 + ηA
ηA

0

,

+ o(1),

as α, β → 0 such that α log(β)+β log(α) → 0, which completes the proof.

5.3. The case of no prior information

Recall that when we set (cid:96) = 0 and u = K, the gap-intersection rule reduces
to the intersection rule, deﬁned in (9). Therefore, setting (cid:96) = 0 and u = K
in Theorem 5.5 we immediately obtain that the intersection rule is asymp-
totically optimal with respect to class P0,K, the case of no prior information;
this is itself a novel result to the best of our knowledge.

However, a more surprising corollary of Theorem 5.5 is that the inter-
section rule, which does not use any prior information, is asymptotically
optimal even when there is a lower and an upper bound on the number of
signals, as long as the following conditions are satisﬁed:

(i) the error probabilities are of the same order of magnitude, in the sense

(ii) the hypotheses are identical and symmetric, in the sense that Dk

1 =

that | log α| ∼ | log β|,
0 = D for every k ∈ [K].
Dk

On the other hand, a comparison with Theorem 5.3 reveals that, even in
this special case, the intersection rule is never asymptotically optimal when
the true number of signals is known in advance, in which case it requires
roughly twice as many observations on average as the gap rule for the same
precision level. The following corollary summarizes these observations.

Corollary 5.6. Suppose that assumption (22) holds and that the thresholds
in the intersection rule are selected according to (10). Then, for any A ⊂ [K]
we have as α, β → 0

+ O(K(cid:112)| log(α ∧ β)|).

(26)

EA[TI ] ≤ max

| log β|
ηA
If also α log(β) + β log(α) → 0, then

1

0

,

(cid:26)| log α|
(cid:26)| log α|

ηA

ηA

1

(cid:27)

(cid:27)

EA[TI ] ∼ max

| log β|
ηA

0

,

∼

inf

(T,d)∈∆α,β (P0,K )

EA[T ].

Y. Song and G. Fellouris/ Sequential Multiple Testing

21

In the special case that | log α| ∼ | log β| and Dk
k ∈ [K],

1 = Dk

0 = D for every

EA[TI ] ∼ | log α|
EA[TI ] ∼ | log α|

D

D

∼

inf

(T,d)∈∆α,β (P(cid:96),u)

EA[T ]

∼ 2

inf

(T,d)∈∆α,β (Pm)

EA[T ]

for every A ∈ P(cid:96),u,

for every A ∈ Pm,

for every 0 ≤ (cid:96) < u ≤ K and 1 ≤ m ≤ K − 1.
Remark 5.2. Corollary 5.6 implies that, in the special symmetric case that
| log α| ∼ | log β| and Dk
0 = D, prior lower and upper bounds on
the true number of signals do not improve the optimal expected sample
size up to a ﬁrst-order asymptotic approximation. However, a comparison
between the second-order terms in (25) and (26) suggests that such prior
information does improve the optimal performance, an intuition that will be
corroborated by the simulation study in Section 6 (see Fig. 2).

1 = Dk

Remark 5.3. In addition to the intersection rule, De and Baron (2012a)
proposed the “incomplete rule”, (Tmax, dmax), which is deﬁned as

Tmax := max{σ1, . . . , σK} and dmax := (d1

max, . . . , dK

max),

where for every k ∈ [K] we have

σk := inf

n ≥ 1 : λk(n) (cid:54)∈ (−a, b)

(cid:110)

(cid:111)

,

dk
max :=

(cid:40)

1,
0,

if λk(σk) ≥ b
if λk(σk) ≤ −a

.

(27)

According to this rule, each stream is sampled until the corresponding test
statistic exits the interval (−a, b), independently of the other streams. It is
clear that, for the same thresholds a and b, Tmax ≤ TI . Moreover, with a
direct application of Boole’s inequality, as in De and Baron (2012a), it follows
that selecting the thresholds according to (10) guarantees the desired error
control for the incomplete rule. Therefore, Corollary 5.6 remains valid if we
replace the intersection rule with the incomplete rule.

6. Simulation study

6.1. Description

In this section we present a simulation study whose goal is to corroborate the
asymptotic results and insights of Section 5 in the symmetric case described

Y. Song and G. Fellouris/ Sequential Multiple Testing

22

(a) Gap rule

(b) Gap-intersection rule

(c) Intersection rule

Fig 1: The x-axis is | log10(PA(A (cid:46) d))|. The y-axis is the relative error of
the estimate of the familywise type-I error, PA(A (cid:46) d), that is the ratio of
the standard deviation of the estimate over the estimate itself. Each curve
is computed based on 50, 000 realizations.

i = N (θi, 1) for each k ∈ [K],
in Corollary 5.6. Thus, we set K = 6 and let f k
i = 0, 1, where θ0 = 0, θ1 = 0.5, in which case Dk
1 = D = (1/2)(θ1)2 =
0 = Dk
1/8. Furthermore, we set α = β. This is a convenient setup for simulation
purposes, since the expected sample size and the two familywise errors of
each proposed procedure are the same for all scenarios with the same number
of signals.

We consider three diﬀerent situations depending on the available prior
information regarding the number of signals. In each of them, the importance
sampling technique of Section 4 was used in order to compute the maximal
familywise type I error probability. As we see in Fig. 1, the relative errors
of the proposed Monte Carlo estimators, even for error probabilities of the
order 10−8, are smaller than 5% for the gap rule, 15% for the gap-intersection
rule, 20% for the intersection rule.

6.1.1. Gap rule

First, we consider the case in which the number of signals is known to be
equal to m (P = Pm) for m ∈ {1, . . . , 5}, and we can apply the corresponding
gap rule, deﬁned in (4). Due to the symmetry of our setup, the expected
sample size EA[TG] and the error probability PA(dG (cid:54)= A) are the same for
A ∈ Pm and A ∈ PK−m; thus, it suﬃces to consider m in {1, 2, 3}.
For each m ∈ {1, 2, 3} and some arbitrary A ∈ Pm, we consider α’s
ranging from 10−2 to 10−8. For each such α, we compute the threshold c
in the gap-rule that guarantees α = PA(dG (cid:54)= A), and then the expected
sample size EA[TG] that corresponds to this threshold. In Fig. 2a we plot

Y. Song and G. Fellouris/ Sequential Multiple Testing

23

EA[TG] against | log10(α)| when m = 1 or 2, since the results for m = 2 and
3 were too close. In Table 1a we present the actual numerical results for
c = 10.

In Fig. 2a we also plot the ﬁrst-order asymptotic approximation to the
optimal expected sample size obtained in Theorem 5.3, which in this par-
ticular symmetric case takes the form | log α|/(2D) = 4| log α|. From our
asymptotic theory we know that the ratio of EA[TG] over this quantity goes
to 1 as α → 0, and this convergence is illustrated in Fig. 2d.

6.1.2. Gap-intersection rule

Second, we consider the case in which the number of signals is known to be
between 2 and 4 (P = P(cid:96),u = P2,4), and we can apply the gap-intersection rule,
deﬁned in (12). Due to the symmetry of the setup and Lemma 3.2, we set
a = b and c = d = b + log(u) = b + log(4).
As before, we consider α’s ranging from 10−2 to 10−8. For each such α,
we obtain the threshold b such that max PA(A (cid:46) dGI ) = α, where the
maximum is taken over A ∈ P(cid:96),u, and then compute the corresponding
expected sample size EA[TGI ] for every A ∈ P(cid:96),u. In Fig. 2b we plot EA[TGI ]
against | log10(α)| for |A| = 2 and 3, since by symmetry EA[TGI ] is the same
for |A| = 2 and 4. This is also evident from Table 1b, where we present
the numerical results for b = 10. In the same graph we also plot the ﬁrst-
order asymptotic approximation to the optimal performance obtained in
Theorem 5.5, which in this case is | log α|/D = 8| log α|. By Theorem 5.5,
we know that the ratio of EA[TGI ] over 8| log α| goes to 1 as α → 0, which
is corroborated in Fig. 2e.

6.1.3. Intersection versus incomplete rule
Third, we consider the case of no prior information (P = P0,6), in which we
compare the intersection rule with the incomplete rule. This is a special case
of the previous setup with (cid:96) = 0 and u = K, but now the expected sample
size (for both schemes) is the same for every subset of signals A, which allows
us to plot only one curve for each scheme in Fig. 2c. In the same graph we also
plot the ﬁrst-order approximation to the optimal performance, | log α|/D =
8| log α|, whereas in Fig. 2f. we plot the corresponding normalized version.

Y. Song and G. Fellouris/ Sequential Multiple Testing

24

6.2. Results

There are a number of conclusions that can be drawn from the presented
graphs. First of all, from Fig. 2a it follows that the gap rule performs the
best when there are exactly m = 1 or 5 signals, whereas its performance
is quite similar in every other case. As we mentioned before, this can be
explained by the fact that the second term in the right-hand side in (23)
grows with m(K − m).

Second, from Fig. 2b we can see that the gap-intersection rule performs
better in the boundary cases that there are exactly 2 or 4 signals than in
the case of 3 signals.

Third, from Fig. 2c we can see that the intersection rule is always better
than the incomplete rule, although they share the same prior information.
Fourth, from the graphs in the second row of Fig. 2 we can see that all
curves approach 1, as expected from our asymptotic results, however the
convergence is relatively slow. This is reasonable, as we do not divide the
expected sample sizes by the optimal performance in each case, but with a
strict lower bound on it instead.

Fifth, comparing Fig. 2a and 2b we verify that knowledge of the exact
number of signals roughly halves the required expected sample size in com-
parison to the case that we only have a lower and an upper bound on the
number of signals.

Finally, comparing Fig. 2e and 2f we observe that upper and lower bounds
on the number of signals help reduce somewhat the expected sample size in
comparison to the case of no prior information, even in the non-boundary
case m = 3.

Table 1

The standard error of the estimate is included in the parenthesis.

m
1
2
3

|A|
2
3
4

(a) P = Pm. (TG, dG) with c = 10.
EA(TG)

PA(dG (cid:54)= A)

58.357 (0.215)
65.762 (0.217)
67.965 (0.218)

8.676E-06 (1.057E-07)
2.576E-05 (2.398E-07)
1.461E-05 (1.977E-07)
(b) P = P2,4. (TGI , dGI ) with b = 10.
EA(TGI )

PA(A (cid:46) dGI )

1.935E-06(8.351E-08)
8.333E-06(9.006E-08)
6.462E-07(1.638E-08)

124.580(0.168)
136.940(0.169)
124.650(0.168)

Y. Song and G. Fellouris/ Sequential Multiple Testing

25

(a) P = Pm

(b) P = P2,4

(c) P = P0,6

(d) P = Pm

(e) P = P2,4

(f) P = P0,6

Fig 2: The x-axis in all graphs is | log10(α)|. In the ﬁrst row, the y-axis
represents the expected sample size under PA that is required in order to
control the maximal familywise type I error probability exactly at level α.
The dotted lines in each plot correspond to the ﬁrst-order approximation
to the optimal expected sample size (given by the lower bound in
Theorem 5.1) for the class ∆α,α(P); due to symmetry, the lower bound
doesn’t depend on |A| in each setup. In (a), the gap rule is applied: the
solid line is for m = 1, and the dashed line for m = 2. In (b), the
gap-intersection rule is applied: the solid line is for |A| = 2, and the dashed
line for |A| = 3. In (c), the solid line is for the intersection rule, and dashed
line for the incomplete rule. In (d), the solid (resp. dashed) line is the
solid(resp. dashed) line in (a) divided by the lower bound. Similarly, (e)
and (f) are the normalized version of (b) and (c).

Y. Song and G. Fellouris/ Sequential Multiple Testing

26

7. Conclusions

In this work, we considered the problem of simultaneously testing multiple
simple null hypotheses, each of them against a simple alternative, when the
data for each of these testing problems are acquired sequentially, and the goal
is to stop sampling as soon as possible, simultaneously in all streams, and
make a correct decision for each individual testing problem. The main goal of
this work was to propose feasible, yet asymptotically optimal procedures in
the above setup that are able to incorporate prior information on the number
of signals (correct alternatives), and also to understand the eﬃciency gains
that can be obtained by such information.

We studied this problem under the assumption that the data streams
for the various hypotheses are independent. First of all, without any dis-
tributional assumptions on the data that are acquired in each stream, we
proposed procedures that can be designed to control the probabilities of at
least one false positive and at least one false negative below arbitrary user-
speciﬁed levels in two general cases: when the exact number of signals is
known in advance, and when we only have an upper and a lower bound for
it. Furthermore, we proposed a Monte Carlo simulation method, based on
importance sampling, that can facilitate the speciﬁcation of non-conservative
critical values for the proposed multiple testing procedures in practice. More
importantly, in the special case of i.i.d. data in each stream, we were able
to show that the proposed multiple testing procedures are asymptotically
optimal, in the sense that they require the minimum possible expected sam-
ple size to a ﬁrst-order asymptotic approximation as the error probabilities
vanish.

These asymptotic optimality results have some interesting ramiﬁcations.
First of all, they imply that any reﬁnements of the proposed procedures, for
example using a more judicious choice of alpha-spending and beta-spending
functions, will not reduce the expected sample size to a ﬁrst-order approxi-
mation. Second, they imply that a lower and an upper bound on the number
of signals do not improve the minimum possible expected sample size to a
ﬁrst-order approximation, except for the case of asymmetric error proba-
bilities or hypotheses, when the true number of signals is equal to either
the lower or the upper bound. On the other hand, knowledge of the exact
number of signals does reduce the minimum possible expected sample size
to a ﬁrst order approximation, roughly by a factor of 2. These insights were
corroborated by a simulation study, which however also revealed the limita-
tions of a ﬁrst-order asymptotic analysis and emphasized the importance of
second-order terms.

Y. Song and G. Fellouris/ Sequential Multiple Testing

27

To our knowledge, these are the ﬁrst results on the asymptotic optimality
of multiple testing procedures, with or without prior information, that con-
trol the familywise error probabilities of both types. However, there are still
some important open questions that remain to be addressed. First, do the
proposed procedures attain, in the i.i.d. setup, the optimal expected sam-
ple size to a second-order asymptotic approximation as well, in which case
there is little room for improvement? Second, does the ﬁrst-order asymp-
totic optimality property remain valid for more general, non-i.i.d. data in
the streams? While we conjecture that the answer to both questions is aﬃr-
mative, we believe that the corresponding proofs require diﬀerent techniques
from the ones we have used in the current paper.

There are also interesting generalizations of the setup we considered in
this paper. For example, it is interesting to consider the sequential multi-
ple testing problem when the goal is to control generalized error rates, such
as the false discovery rate (Bartroﬀ and Song, 2013), instead of the more
stringent familywise error rates. Another interesting direction is to allow the
hypotheses in the streams to be speciﬁed up to an unknown parameter, or
to consider a non-parametric setup similarly to Li, Nitinawarat and Veer-
avalli (2014). Finally, it is still an open problem to design asymptotically
optimal multiple testing procedures that incorporate prior information on
the number of signals when it is possible and desirable to stop sampling at
diﬀerent times in the various streams.

Appendix A: Two lemmas

A.1. An information-theoretic inequality

In the proof of Theorem 5.1 we use the following, well-known, information-
theoretic inequality, whose proof can be found, e.g., in Tartakovsky, Niki-
forov and Basseville (2014) (Chapter 3.2).

Lemma A.1. Let Q, P be equivalent probability measures on a measurable
space (Ω,G) and recall the function ϕ deﬁned in (19). Then, for every A ∈ G
we have

(cid:20)

(cid:21)

dQ
dP

EQ

log

≥ ϕ (Q(A), P(Ac)) .

A.2. A lemma on multiple random walks

For the proof of Lemmas 5.2 and 5.4 we need an upper bound on the ex-
pectation of the ﬁrst time that multiple random walks, not necessarily in-

Y. Song and G. Fellouris/ Sequential Multiple Testing

28

dependent, are simultaneously above given thresholds. We state here the
corresponding result in some generality.
Thus, let L ≥ 2 and suppose that for each l ∈ [L] we have a sequence of
i.i.d. random variables, {ξl
1] <
∞. For each l ∈ [L], let

n, n ∈ N}, such that µl = E[ξl

1] > 0 and Var[ξl

n(cid:88)

i=1

Sl

n =

i, n ∈ N
ξl

be the corresponding random walk. Here, no assumption is made on the
dependence structure among these random walks. For an arbitrary vector
(a1, . . . , aL), consider the stopping time

(cid:110)

(cid:111)
n ≥ al for every l ∈ [L]

.

T = inf

n ≥ 1 : Sl

The following lemma provides an upper bound on the expected value of T .
The proof is identical to the one in Theorem 2 in Mei (2008); thus we omit
it. We stress that although the theorem in the reference assumes indepen-
dent random walks, exactly the same proof applies to the case of dependent
random walks.
Lemma A.2. As a1, . . . , aL → ∞,

(cid:18) al

(cid:19)

µl

+ O

(cid:88)

l∈[L]

 ≤ max

l∈[L]

(cid:114) al

µl

(cid:18) al

(cid:19)

µl

(cid:32)

(cid:114)

(cid:33)

+ O

L

{al}

.

max
l∈[L]

E[T ] ≤ max
l∈[L]

References

Armitage, P. (1950). Sequential Analysis with More than Two Alternative
Hypotheses, and its Relation to Discriminant Function Analysis. Journal
of the Royal Statistical Society. Series B (Methodological) 12 137-144.

Bartroff, J. and Lai, T. L. (2008). Generalized likelihood ratio statistics
and uncertainty adjustments in eﬃcient adaptive design of clinical trials.
Sequential Analysis 27 254–276.

Bartroff, J. and Lai, T. L. (2010). Multistage tests of multiple hypothe-

ses. Communications in Statistics–Theory and Methods 39 1597–1607.

Bartroff, J. and Song, J. (2013). Sequential Tests of Multiple Hypothe-
ses Controlling False Discovery and Nondiscovery Rates. arXiv:1311.3350
[stat.ME].

Y. Song and G. Fellouris/ Sequential Multiple Testing

29

Bartroff, J. and Song, J. (2014). Sequential tests of multiple hypothe-
ses controlling type I and II familywise error rates. Journal of statistical
planning and inference 153 100–114.

Benjamini, Y. and Hochberg, Y. (1995). Controlling the false discovery
rate: a practical and powerful approach to multiple testing. Journal of the
Royal Statistical Society. Series B (Methodological) 289–300.

De, S. K. and Baron, M. (2012a). Sequential Bonferroni methods for
multiple hypothesis testing with strong control of family-wise error rates
I and II. Sequential Analysis 31 238–262.

De, S. K. and Baron, M. (2012b). Step-up and step-down methods for
testing multiple hypotheses in sequential experiments. Journal of Statis-
tical Planning and Inference 142 2059–2070.

Dragalin, V. P., Tartakovsky, A. G. and Veeravalli, V. V. (1999).
Multihypothesis sequential probability ratio tests. I. Asymptotic optimal-
ity. Information Theory, IEEE Transactions on 45 2448–2461.

Dragalin, V. P., Tartakovsky, A. G. and Veeravalli, V. V. (2000).
Multihypothesis sequential probability ratio tests. II. Accurate asymp-
totic expansions for the expected sample size. Information Theory, IEEE
Transactions on 46 1366–1383.

Holm, S. (1979). A simple sequentially rejective multiple test procedure.

Scandinavian journal of statistics 65–70.

Hommel, G. (1988). A stagewise rejective multiple test procedure based on

a modiﬁed Bonferroni test. Biometrika 75 383–386.

Lehmann, E. L. and Romano, J. P. (2005). Generalizations of the fami-

lywise error rate. Ann. Statist. 33 1138–1154.

Li, Y., Nitinawarat, S. and Veeravalli, V. V. (2014). Universal se-
quential outlier hypothesis testing. In Information Theory (ISIT), 2014
IEEE International Symposium on 3205–3209. IEEE.

Lorden, G. (1977). Nearly-optimal sequential tests for ﬁnitely many pa-

rameter values. Ann. Statist. 1–21.

Marcus, R., Eric, P. and Gabriel, K. R. (1976). On closed testing pro-
cedures with special reference to ordered analysis of variance. Biometrika
63 655–660.

Mei, Y. (2008). Asymptotic optimality theory for decentralized sequential
hypothesis testing in sensor networks. Information Theory, IEEE Trans-
actions on 54 2072–2089.

Sobel, M. and Wald, A. (1949). A Sequential Decision Procedure for
Choosing One of Three Hypotheses Concerning the Unknown Mean of a
Normal Distribution. Ann. Math. Statist. 20 502–522.

Tartakovsky, A., Nikiforov, I. and Basseville, M. (2014). Sequential

Y. Song and G. Fellouris/ Sequential Multiple Testing

30

analysis: Hypothesis testing and changepoint detection. CRC Press.

Wald, A. (1945). Sequential tests of statistical hypotheses. The Annals of

Mathematical Statistics 16 117–186.

