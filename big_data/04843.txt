6
1
0
2

 
r
a

 

M
5
1

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
3
4
8
4
0

.

3
0
6
1
:
v
i
X
r
a

Approximating faces of marginal polytopes in

discrete hierarchical models

Nanwei Wang∗, Johannes Rauh†and H´el`ene Massam‡

March 16, 2016

Abstract

The existence of the maximum likelihood estimate (abbreviated
mle) in hierarchical loglinear models has important consequences for
inference. Determining whether this estimate exists is equivalent to
ﬁnding whether the data belongs to the boundary of the marginal
polytope of the model. For higher-dimensional problems, it may not
be possible to solve this problem exactly. Massam and Wang (2015)
found an outer approximation to the smallest face, that is a face con-
taining the smallest face. The purpose of this paper is to reﬁne the
methodology to ﬁnd this outer approximation and to devise a new
methodology to ﬁnd an inner approximation. The inner approxima-
tion is given not in terms of a face of the marginal polytope, but in
terms of a subset of the vertices of the smallest face. While outer
approximations can be found by looking at submodels of the original
hierarchical model, inner approximations correspond to larger mod-
els. To obtain larger models that are easier to understand than the
original model, we suggest to complete separators in order to obtain
a reducible model.

Using real-world data and simulated data we illustrate that the
two approximations often agree, and in this case we have found the
smallest face. Even if the two approximations do not agree, they

∗Department of Statistics, York University, Toronto, M3J 1P3, Canada
†Department of Statistics, York University, Toronto, M3J 1P3, Canada
‡Department of Statistics, York University, Toronto, M3J 1P3, Canada. This author

gratefully acknowledges support from NSERC Discovery Grant A8947.

1

provide us with information that can be used to better estimate the
parameters of the mle (or the extended mle whenever the mle does
not exist).

Keywords: existence of the maximum likelihood estimate, marginal
polytope, faces, facial sets, extended maximum likelihood estimate.

1

Introduction

Discrete hierarchical models are an essential tool for the analysis of cate-
gorical data given under the form of a contingency table. The study of
these models goes back more than a century, and a detailed history of their
development is given in Fienberg and Rinaldo (2007). Nowadays, discrete
hierarchical models are used for the analysis of large sparse contingency ta-
bles where many, if not most, of the entries are small or zero counts. It is
well-known that in such cases, the maximum likelihood estimate (henceforth
abbreviated mle) of the parameters may not exist. The non existence of the
mle has problematic consequences for inference, clearly for estimation, but
also for testing and model selection. The reader is referred to Fienberg and
Rinaldo (2012) for a list of statistical implications of the non existence of
the mle. The most serious of these implications is that, when the mle does
not exist, the degrees of freedom used to approximate various measures of ﬁt
are incorrect. Fienberg and Rinaldo (2012) also give necessary and suﬃcient
conditions for the existence of the mle. Geyer (2009) describes the problems
attached to the nonexistence of the mle and presents an R program that
yields meaningful conﬁdence intervals and tests. Letac and Massam (2012)
study the statistical implications of the nonexistence of the mle on model
selection in Bayesian inference.

The necessary and suﬃcient conditions for the existence of the mle given
by Fienberg and Rinaldo (2012) are extensions of those given earlier by
Haberman (1974), Barndorﬀ-Nielsen (1978) and Eriksson et al. (2006) and
are essentially as follows. Let V be a ﬁnite index set, and let X = (Xv, v ∈ V )
be a vector of discrete random variables representing |V | characteristics of
an object. We consider N objects classiﬁed in a |V |-dimensional contingency
table according to these characteristics. Let I denote the set of cells in the
contingency table and let n(i), i ∈ I, be the number of observations falling

2

in cell i ∈ I. We assume that (n(i), i ∈ I) follows a multinomial distribution
and that the cell probabilities are modelled according to a hierarchical model.
As we recall in Section 2, this latter condition implies that the expression
i∈I p(i)n(i) in the multinomial density can be written as the density of an

(cid:81)

exponential family distribution

f (t)dt = exp{(cid:104)θ, t(cid:105) − N k(θ)}ν(dt),

(1)

where θ is a vector of parameters and t is the vector of suﬃcient statistic
determined by the hierarchical model and ν(dt) is a discrete measure with
support on a ﬁnite number of vectors fi, i ∈ I. The domain of the means
of this natural exponential family is the convex hull of the support of ν and
forms a convex polytope P with extreme points fi, i ∈ I, called the marginal
polytope (because t is a vector of marginal counts of subsets of the random
variables Xv). Then the mle exists if and only if t belongs to the relative
interior of the marginal polytope P. If the mle does not exist, then t belongs
to the relative interior of a proper face Ft of P. Thus, determining whether,
for a given data set, the mle of a discrete hierarchical loglinear model exists
is equivalent to determining whether t belongs to a proper face of P.

To the best of our knowledge, the papers that have tackled this topic so
far are Eriksson et al. (2006), Geyer (2009), Fienberg and Rinaldo (2012)
and Massam and Wang (2015). The ﬁrst three seek to identify Ft in or-
der to compute the extended mle as deﬁned by Barndorﬀ-Nielsen (1978) or
Lauritzen (1996). To do so, they solve a sequence of linear programming
problems aimed at ﬁnding the fi in Ft. Unfortunately this method becomes
computationally infeasible in large dimensions. In our experience, it is not
possible to use the linear programming approach when the number of nodes
is larger than 16. Massam and Wang (2015) show how to ﬁnd an approxi-
mation F2 to Ft. They consider subsets Vi, i = 1, . . . , k, of V containing less
than 16 nodes and compute the facial set of the smallest face containing the
corresponding suﬃcient statistic in each of the corresponding models using
linear programming. They prove that the intersection of the smallest faces
corresponding to Vi, i = 1, . . . , k, is a face F2 of P containing Ft. The face
F2 gives us some information in the sense that if F2 is a proper face of P,
then Ft is a proper face and therefore the mle does not exist. We call F2 an
outer approximation to Ft.

The purpose of this paper is to add to this outer approximation F2, an
inner approximation F1 that is included in the face Ft and to explore in detail

3

the most eﬃcient ways to obtain an inner and an outer approximations to Ft.
The inner approximation F1 we ﬁnd is not a face of the marginal polytope P,
but it is a face of the marginal polytope of a model containing the original
model. We have the relationship

F1 ⊂ Ft ⊂ F2.

Clearly, if F1 = F2, then we have found Ft. In this paper, we will present
our methodology for ﬁnding F1 and F2 in a uniﬁed way.
We now sketch the main lines of our argument. We will work in terms
of facial sets, that is, for a given face F, in terms of F = {i ∈ I | fi ∈ F}
rather than in terms of F. A hierarchical model for the discrete random
variable X = (Xv, v ∈ V ) is determined by the set of possible interactions
between the variables Xv, v ∈ V . This set of interactions is represented by a
simplicial complex ∆, that is, ∆ is a set of subsets D ⊂ V such that D ∈ ∆
and D(cid:48) ⊂ D implies D(cid:48) ∈ ∆. For any T ⊆ I, denote by F∆(T ) the smallest
facial set containing T . Thus F∆(T ) is the facial set of the smallest face of the
marginal polytope P∆ of the hierarchical model given by ∆ which contains
fi, i ∈ T .
The data is given by the cell counts (n(i), i ∈ I). We let I+ = {i ∈ I |
n(i) > 0} denote the subset of I indexing the cells with positive counts. As
we shall see in Section 2, the vector t in (1) is

(cid:88)

i∈I

(cid:88)

i∈I+

t =

n(i)
N

fi =

n(i)
N

fi,

where N =(cid:80)

i∈I n(i). Thus Ft is the smallest face of P∆ containing {fi, i ∈
I+}, and so its facial set is Ft = F∆(I+). Identifying Ft is therefore equivalent
to identifying F∆(I+).
To ﬁnd inner and outer approximations to F∆(I+) := Ft, we rely on
the following facts which will be proved in Section 3. If ∆1 ⊂ ∆2 are two
simplicial complexes on the same vertex set V , then

F∆2(T ) ⊆ F∆1(T ).

(2)

From (2), any sub-complex ∆1 of our original simplicial complex ∆ yields
an outer approximation to Ft. We can improve this outer approximation
(cid:84)r
by taking the intersection of several such approximations. Namely, given
∆1,1, ∆1,2, . . . , ∆1,r ⊆ ∆, then F∆(I+) ⊆ F∆1,k(I+) for all k, and so F∆(I+) ⊆
k=1 F∆1,k(I+).

4

Similarly, for every simplicial complex ∆2 that contains the original sim-
plicial complex ∆, by (2) we obtain an inner approximation F∆2(I+) ⊆
F∆(I+). We can improve this approximation by looking at several simpli-
cial complexes in turn and iterating. Namely, if ∆ ⊆ ∆2,1 and ∆ ⊆ ∆2,2,
then F∆2,1(I+) ⊆ F∆(I+) = Ft, and thus F∆2,2(F∆2,1(I+)) ⊆ F∆(Ft) = Ft.
Moreover, for any simplicial complex Γ on V and any T ⊂ I, we have
FΓ(T ) ⊃ T by the very deﬁnition of a facial set. Applying this inclusion to
T = F∆2,1(I+) and Γ = ∆2,2 yields I+ ⊆ F∆2,1(I+) ⊆ F∆2,2(F∆2,1(I+)) ⊆ Ft,
that is, F∆2,2(F∆2,1(I+)) is a better approximation of Ft than F∆2,1(I+). We
can further improve our approximation by applying F∆2,1 once more:
in
general, F∆2,1(F∆2,2(F∆2,1(I+))) will again be a superset of F∆2,2(F∆2,1(I+))
contained in Ft. We carry on this iteration until the sets do not increase any
more and we arrive at a set F1 that satisﬁes I+ ⊆ F1 ⊆ Ft; that is, F1 is an
inner approximation of Ft. F1 is not necessarily facial for ∆ (unless F1 = Ft),
but F1 is facial for ∆2,1 and ∆2,2. This procedure easily generalizes to the
case of more than two simplicial complexes containing ∆.

The various approximating facial sets will be computed using the linear
programming technique of Fienberg and Rinaldo (2012) and Massam and
Wang (2015). The question that remains is how to ﬁnd suitable simplicial
complexes that are contained in ∆ or that contain ∆ and for which it is easy
to compute faces.
To obtain a simplicial complex containing ∆, we complete separators.
That is, we ﬁnd sets V1, V2 ⊆ V such that ∆ can be written as a union
∆1∪ ∆2 of simplicial complexes ∆1 on V1 and ∆2 on V2. We call S := V1∩ V2
a separator. If S ∈ ∆, then the hierarchical model is reducible, and facial
sets of ∆ can easily be computed by computing facial sets with respect to
the hierarchical models of ∆1 and ∆2 (see Lemma 3.6 below). If S /∈ ∆, we
can use ∆S = ∆ ∪ {M : M ⊆ S} as a simplicial complex containing ∆ to
ﬁnd an inner approximation to Ft.

To ﬁnd an outer approximation, we show that the approach of Massam
and Wang (2015) of looking at a small subset of nodes can be seen as a
special case of looking at a simplicial sub-complex.

Our results apply to not only hierarchical models, but also more general
discrete exponential families. While all of our examples are hierarchical mod-
els, our theoretical results are best understood from a more general point of
view.

The remainder of this paper is organized as follows. In Section 2, we give

5

preliminaries on hierarchical models, and faces and facial sets. In Section 3,
we develop our tools for the inner and outer approximation F1 and F2 to Ft.
In Section 4, we show how to use F1 and F2 to identify the parameters
of the hierarchical models that can be estimated and those that cannot be
estimated. This extends in some ways the work of Fienberg and Rinaldo
(2012) and that of Geyer (2009). In Section 5, we illustrate our methodology
with simulated data and a real world example using the NLTCS data. This
data set has been studied in Dobra and Lenkoski (2011) and Dobra et al.
(2003). Both of these examples have 16 nodes. In Section 6, we apply our
methodology to larger models, one with simulated data and the other on
a real-world data set. Our simulated data set is obtained from a graphical
model of the 5× 10 grid, while the real-world data set uses the voting records
in the US Senate during the portion of 2015 year that was available at the
time of our study. A similar data set but for a diﬀerent year was used in
Banerjee et al. (2008).

2 Preliminaries

In the following four subsections, we recall some basic facts about hierar-
chical models, discrete exponential families, polytopes and the closure of
exponential families, and we also deﬁne the extended mle.

2.1 Hierarchical models

For details and proofs on the material in this subsection, we refer to Letac
and Massam (2012) and Rauh et al. (2011). Let X = (Xv, v ∈ V ) be a
discrete random vector with components indexed by V = {1, . . . , p}, a ﬁnite
set. Each variable Xv takes values in a ﬁnite set Iv, v ∈ V . The vector X
takes its values in

(cid:89)

I =

Iv,

v∈V

the set of cells i = (iv, v ∈ V ) of the p-dimensional contingency table. Let ∆
be a set of subsets of V which is a simplicial complex, that is, ∆ is a set of
subsets D ⊂ V such that D ∈ ∆ and D(cid:48) ⊂ D implies D(cid:48) ∈ ∆. We say that
the joint distribution of X is hierarchical with underlying simplicial complex
∆ (or generating set ∆) if the probability p(i) = P (X = i) of a single cell

6

i = (iv, v ∈ V ) is of the form

log(p(i)) =

(cid:88)

θD(iD)

(3)

D∈∆

where θD(iD) is a function of the marginal cell iD = (iv, v ∈ D) only. If we
need to make precise the dependence on θ, then we write pθ(i) instead of p(i).
The set of all such distributions E∆ := {pθ} is called the hierarchical model
of ∆.

The parametrization (3) is not identiﬁable; that is, for any joint dis-
tribution p from the hierarchical model there are diﬀerent choices for the
functions θD that satisfy (3). One way to make the parameters unique is
to choose a special element within each set Iv, which we denote by 0. The
choice of 0 is arbitrary, and a diﬀerent choice of 0 leads to a simple aﬃne
change of parameters. With this choice, the functions θD become unique if
one requires θD(iD) = 0 whenever iv = 0 for some v ∈ D. Thus, we arrive at
the identiﬁable parametrization

log pθ(i) = θ0 +

θD(iD),

(4)

(cid:88)

D∈∆\{∅},iv(cid:54)=0,∀v∈D

θ0 is determined by the requirement (cid:80)

where θ0 := θ∅. We separate the parameter θ0 corresponding to the empty
set, since it has a special role. It does not depend on the cell index i and
acts as a normalizing constant: When all other parameters are chosen freely,
i∈I pθ(i) = 1. To make it clear that

we consider θ0 as a dependent parameter, we also write

(cid:16)(cid:88)

(cid:16) (cid:88)

−θ0 = k(θ) = log

exp

(cid:17)(cid:17)

θD(iD)

.

The parametrization (4) can be further reformulated using the deﬁnitions

i∈I

D∈∆\{∅},iv(cid:54)=0,∀v∈D

S(i) = {v ∈ V ; iv (cid:54)= 0}

J = {j ∈ I \ {0}, S(j) ∈ ∆}.

For a given D ∈ ∆ and for given θD(iD) such that iγ (cid:54)= 0,∀γ ∈ D, there is
only one j ∈ J such that S(j) = D and jD = jS(j) = iD and conversely. We
can therefore write

θD(iD) = θj

for the unique j ∈ J with S(j) = D, iD = jD.

7

To simplify the notation, we write j (cid:47)i whenever S(j) ⊆ S(i) and jS(j) = iS(j).
Then the parametrization (4) in terms of the free parameters θ = {θj, j ∈ J}
becomes

log pθ(i) =

θj − k(θ).

(5)

(cid:88)
(cid:88)

j∈J:j(cid:47)i

ej,

j∈J:j(cid:47)i

It is convenient to introduce the vectors

fi =

i ∈ I

where ej, j ∈ J are the unit vectors in RJ . Moreover, let A be the J × I
matrix with columns fi, i ∈ I, and let ˜A be the (1 + |J|) × I matrix with

(cid:1), i ∈ I. The representation (5) becomes

columns equal to(cid:0) 1

log pθ(i) = (cid:104)θ, fi(cid:105) − k(θ) = Atθ − k(θ) = ˜At ˜θ,

(6)
where ˜θ = (θ0, θ) as a column vector. Both A and ˜A are called design matrices
of the model.
From the deﬁnition of fi, i ∈ I, it follows immediately that if n = (n(i), i ∈

fi

I) denotes the I-dimensional column vector of cell counts, then

˜An =

and An = t,

(7)

(cid:19)

(cid:18) N

t

N = (cid:80)

where N =(cid:80)
(cid:80)

n(i), j ∈ J.

jS(j)-marginal counts n(jS(j)), i.e.

i∈I n(i) is the total cell counts and t is the column vector of
t = (tj, j ∈ J) where tj = n(jS(j)) =

i|iS(j)=jS(j)
It follows from (7) that t

n(i)
N fi. Therefore, t belongs to the
convex polytope with extreme points fi, i ∈ I. This polytope is called the
marginal polytope of the hierarchical model, and we denote it by P∆.
Example 2.1. For the model deﬁned by V = {a, b, c}, Ia = {0, 1} = Ib = Ic
and ∆ = {a, b, c, ab, bc}, we have I = (000, 100, 010, 110, 001, 101, 011, 111)
and J = {(100), (010), (001), (110), (011)}. Then

i∈I

f000(cid:122)(cid:125)(cid:124)(cid:123)

f001(cid:122)(cid:125)(cid:124)(cid:123)

f010(cid:122)(cid:125)(cid:124)(cid:123)

f011(cid:122)(cid:125)(cid:124)(cid:123)

f100(cid:122)(cid:125)(cid:124)(cid:123)

f101(cid:122)(cid:125)(cid:124)(cid:123)

f110(cid:122)(cid:125)(cid:124)(cid:123)

f111(cid:122)(cid:125)(cid:124)(cid:123)



1
0
0
0
0
0

˜A =

1
1
0
0
0
0

1
0
1
0
0
0

1
0
0
1
0
0

1
1
1
0
1
0

8



1
1
1
1
1
1

θ000
θ100
θ010
θ001
θ110
θ011

1
1
0
1
0
0

1
0
1
1
0
1

An important subclass of hierarchical model is the class of graphical mod-
els. Let G = (V, E) be an undirected graph with vertex set V and edge
set E. A subset D ⊆ V is a clique of G if any i, j ∈ D, i (cid:54)= j, deﬁne an edge
(i, j) ∈ E. The set of cliques of G, denoted by ∆(G), is a simplicial com-
plex. The graphical model of G is deﬁned as the hierarchical model of ∆(G).
Graphical models are important because of their interpretation in terms of
conditional independence, see Lauritzen (1996).

Another class of hierarchical models related to graphs is the class of Ising
models. For an undirected graph G = (V, E), the corresponding Ising model
is the hierarchical model of the simplicial complex ∆ that contains the sin-
gletons {x}, x ∈ V , and the pairs {x, y} with (x, y) ∈ E.

In Sections 5.1 and 6.2, we will consider models with underlying graphs
that are grid graphs, i.e., four-neighbour lattices. In such cases, the cliques
are at most of size two, and the corresponding graphical models are the same
as the corresponding Ising models. The hierarchical model for the NLTCS
data studied in Section 5.2 is a graphical model, while the hierarchical model
used for the US Senate voting data studied in Section 6.1 is an Ising model.

2.2 Discrete exponential families

Hierarchical models are examples of discrete exponential families. Let I and
J be ﬁnite sets, and let A ∈ RJ×I be a real matrix. Denote the columns of
A by fi, i ∈ I. The discrete exponential family corresponding to A, denoted
by EA, consists of all probability distributions on I that are of the form

where k(θ) = log(cid:80)
is, ˜A ∈ R ˜J×I is the matrix with columns(cid:0) 1

pθ(i) = exp((cid:104)θ, fi(cid:105) − k(θ)),
i∈I exp((cid:104)θ, fi(cid:105)). Deﬁne ˜J = J ∪ {0}, θ0 = −k(θ) and
(cid:1), i ∈ I. Then EA consists of the
˜θ = (θ0, θ), and let ˜A be the matrix A with one additional row of ones; that

θ ∈ RJ ,

fi

probability distributions pθ that satisfy log pθ = ˜At ˜θ for some ˜θ ∈ R ˜J . The
convex hull of the columns fi, i ∈ I, is called the convex support polytope,
denoted by PA. It generalizes the marginal polytope.
The parametrization θ → pθ is identiﬁable if and only if ˜A has full rank.
If ˜A does not have full rank, then one can drop certain rows of A to obtain a
submatrix A(cid:48) such that ˜A(cid:48) has full rank. This is equivalent to setting certain
parameters to zero until the remaining parameters are identiﬁable.

9

Later, the following reparametrization will be useful: Select an element
of I, denoted by 0 in the following. Let A0 be the matrix with columns
fi − f0, i ∈ I \ {0}.
It is not diﬃcult to see that A and A0 deﬁne the
same exponential family (since ˜A and ˜A0 have the same row span). Let
h(cid:48) = rank(A0) = rank( ˜A0) − 1, and select a set L of h(cid:48) linearly independent
vectors among the columns of A0. For i ∈ L let µi = µi(θ) := (cid:104)θ, fi −
f0(cid:105), and let µL = (µi, i ∈ L).
It is not diﬃcult to see that the µL are
identiﬁable parameters on EA: In fact, their number is equal to h(cid:48), and they
are independent by construction.
It is possible to extend the deﬁnition of µi(θ) to all i ∈ I. Note that only
the parameters µi with i ∈ L are free parameters, while the paramters µi
with i ∈ I \ L are linear functions of µL.

The parameters µi can be interpreted as log-likelihood ratios:

(cid:80)

pθ(0) =

1

i∈I\{0}

.

exp(µi)

form of a natural exponential family as given in (1). Indeed,

i∈I pθ(i)n(i) can be written under the

pθ(i)
pθ(0)

,

µ0(θ) = 0,

µi(θ) = log

For a discrete exponential family,(cid:81)
(cid:89)

(cid:88)

pθ(i)n(i) = exp(

i∈I

i∈I

n(i) log pθ(i)) = exp((cid:104)n, log(pθ)(cid:105)) = exp((cid:104)n, ˜At ˜θ(cid:105))
θjtj − N k(θ)).

= exp((cid:104) ˜An, ˜θ(cid:105)) = exp(

(cid:88)

j∈J

The log-likelihood function in θ for the loglinear parameter of the multi-

nomial distribution with the given hierarchical model is therefore

l(θ) =

θjtj − N k(θ).

(8)

(cid:88)

It is well-known that l(θ) is concave. If the parameters are identiﬁable, then
it is strictly concave.

We can also express the log-likelihood as a function of µ = (µi, i ∈ I):

l(µ) =

n(i) log p(i) =

+ N log p(0)

i∈I

i∈I\{0}

(cid:88)

n(i)µi − N log(

exp µi).

(9)

i∈I

i∈I

(cid:88)

j∈J

(cid:88)

p(i)
p(0)

(cid:88)

n(i) log

=

10

As stated before, only a subset µL of the parameters µ are independent, and
the remaining µi, i /∈ L, can be expressed as linar functions of µL.

2.3 Polytopes

We next recall some general facts about polytopes and their faces. We refer
to Ziegler (1998) for details and more information.
Deﬁnition 2.2. A set P ⊂ Rh is a (convex) polytope if P is the convex
hull of a ﬁnite subset of Rh. Equivalently, a polytope can be deﬁned as a
bounded subset of Rh deﬁned by linear inequalities.
Deﬁnition 2.3. For any vector g ∈ Rh and any constant c ∈ R, deﬁne

three sets Hg,c =(cid:8)x ∈ Rh : (cid:104)g, x(cid:105) = c(cid:9), H +
g,c =(cid:8)x ∈ Rh : (cid:104)g, x(cid:105) ≥ c(cid:9) and
H−g,c =(cid:8)x ∈ Rh : (cid:104)g, x(cid:105) ≤ c(cid:9). If g (cid:54)= 0, then Hg,c is an (aﬃne) hyperplane,

g,c and H−g,c are the positive and negative halfspace deﬁned by g and c.
and H +
Let P ⊆ Rh be a polytope, let g ∈ Rh and c ∈ R, and suppose that
P ⊂ H +
g,c or P ⊂ H−g,c. Then F := Hg,c ∩ P is called a face of P. If g (cid:54)= 0,
then Hg,c is called a supporting hyperplane of P. If F (cid:54)= P and F (cid:54)= ∅, then
F is a proper face of P.
The dimension of a face F is the dimension of the smallest aﬃne subspace
of Rh that contains it. Its co-dimension is dim(P) − dim(F). A facet of a
polytope P is a proper face that is maximal with respect to inclusion and is
thus of co-dimension 1. A minimal proper face of a polytope is a singleton
{p} ⊆ P; in this case, p is a vertex.

∩ H +

Intersections of faces are again faces: If g1, g2 ∈ Rh and c1, c2 ∈ R deﬁne
faces F1, F2 of P and if P ⊂ H +
g2,c2, then P ⊂ H +
g1+g2,c1+c2, and
F1 ∩ F2 = P ∩ Hg1+g2,c1+c2. Any face is an intersection of facets.
By deﬁnition, every face F of a polytope P ⊂ Rh is characterized by a
linear inequality (cid:104)g, x(cid:105) ≥ c that is valid on P and that holds as an equality
on F. This linear inequality is unique only if F is a facet. Sometimes it is
convenient to give all linear equations that hold on a face F. These linear
equations determine the smallest aﬃne subspace of Rh containing F.

g1,c1

When a polytope is deﬁned as the convex hull of a ﬁnite number of

points fi, i ∈ I, then it is of interest to know, which subsets of {fi}i∈I

lie on a common face. Indeed, it is the purpose of this paper to compute the
smallest face of the marginal polytope containing the data vector t, and we
will determine this face by identifying which vectors fi belong to it.

11

Deﬁnition 2.4. For a ﬁnite set I let {fi}i∈I ⊂ Rh, and let P be the convex
hull of {fi}i∈I. A subset F ⊆ I is called facial (with respect to P), if there
exists a face F of P with F = {i : fi ∈ F}. For any subset S ⊆ I, denote
by FP(S) the smallest facial set that contains S.

Since the intersection of facial sets is again facial, FP(S) is well-deﬁned.

Lemma 2.5. Let {fi}i∈I ⊂ Rh, let φ : Rh → Rh(cid:48), x (cid:55)→ Bx + d be an aﬃne
map, and let f(cid:48)i = φ(fi). If P is the convex hull of the fi, then P(cid:48) := φ(P) is
the convex hull of the f(cid:48)i. The faces and facial sets of P and P(cid:48) are related
as follows:

1. Any inequality (cid:104)g(cid:48), x(cid:48)(cid:105) ≥ c(cid:48) that is valid on P(cid:48) corresponds to an inequal-
ity (cid:104)g, x(cid:105) ≥ c that is valid on P, where g = Btg(cid:48) and c = c(cid:48) − (cid:104)g(cid:48), d(cid:105).
Thus, if F(cid:48) is a face of P(cid:48), then φ−1(F(cid:48)) is a face of P.

2. A subset of I that is facial with respect to P(cid:48) is also facial with respect

to P. Thus, FP(S) ⊆ FP(cid:48)(S) for any S ⊆ I.

Proof. The ﬁrst statement follows from

c ≤ (cid:104)g(cid:48), φ(fi)(cid:105) = (cid:104)g(cid:48), Bfi + d(cid:105) = (cid:104)Btg(cid:48), fi(cid:105) + (cid:104)g(cid:48), d(cid:105),

which holds for any i ∈ I. The second statement follows immediately from
the equation above and the fact that FP(S) is the smallest facial set contain-
ing S.

We note that in Lemma 2.5, the dimension of φ(P) is at most equal to h.

We will only apply Lemma 2.5 to coordinate projections φ with h(cid:48) < h.
Remark 2.6. Sometimes it is convenient to embed the polytope in a vector
space that has one additional dimension using a map Rh → Rh+1, x (cid:55)→ ˜x :=
(1, x). This has the advantage that all deﬁning inequalities can be brought
into a homogeneous form with vanishing constant c: Note that (cid:104)g, fi(cid:105) − c =
(cid:104)˜gc, ˜fi(cid:105), where ˜gc := (c, g).
When a deﬁning inequality of a face F is given, its facial set F can be
obtained by checking whether fi ∈ F for each i ∈ I. In the other direction,
when a facial set F is given, it is much more diﬃcult to compute a deﬁning
inequality of the corresponding face F. However, it is straightforward to
compute the linear equations deﬁning F: The set of such equations 0 =
(cid:104)g, x(cid:105) − c = (cid:104)˜g, ˜x(cid:105) corresponds to the set of vectors ˜g ∈ ker ˜At
F , where ˜AF
is the matrix obtained from A by adding a row of ones and dropping the
columns not in F .

12

2.4 The closure of an exponential family and existence

of the mle

We ﬁx a discrete exponential family EA. While our main interest lies in
hierarchical models, the results that we need are more naturally formulated
in the language of discrete exponentiel families. We assume that a vector of
observed counts n = (n(i) : i ∈ I) is given.

Deﬁnition 2.7. A parameter value θ∗ is a maximum likelihood estimate
(mle) if it is a global maximum of l(θ).

The function l(θ) is always bounded (clearly, it is never positive). As
stated above, l(θ) is strictly concave (if the parameters are identiﬁable), and
so the maximum is unique (up to identiﬁability), if it exists. However, a max-
imum need not exist, since the domain of the parameters θ is unbounded.
To understand this, it is convenient to interpret the likelihood as a func-
tion of probabilities. Let ˜l be the function that assigns to any probability
distribution p on I the value

(cid:89)

i∈I

˜l(p) = log(

p(i)n(i))

Then l(θ) = ˜l(pθ), and θ∗ is an mle if and only if pθ∗ maximizes ˜l subject
to the constraint that p belongs to the hierarchical model (and thus is of
the form pθ for some θ). While the set of all probability distributions on I is
compact, the hierarchical model itself is not closed and therefore not compact,
and so there is no guarantee that ˜l attains its maximum on the hierarchical
model. However, things become better when we pass from the hierarchical
model to its topological closure, where the topology comes from interpreting

a probability distibution as a vector p = (p(i))i∈I ∈ RI of real numbers (this

choice of the topology is canonical since we are dealing with a ﬁnite set I;
for inﬁnite sample spaces see Csisz´ar and Mat´uˇs (2005)). The closure is
sometimes also called completion (Barndorﬀ-Nielsen, 1978, p. 154). Since the
closure of the hierarchical model is again compact, the continuous function ˜l
always attains its maximum.

Theorem 2.8. The closure of a discrete exponential family can be written
as a union

EA =

EF,A,

(cid:91)

F

13

where F runs over all facial sets of the convex support polytope PA and where
EF,A consists of all probability distributions of the form pF,θ, with

(cid:40)

exp((cid:104)θ, fi(cid:105) − kF (θ)),
0,

if i ∈ F,
otherwise,

pF,θ =

where kF (θ) = log(cid:80)

i∈F exp((cid:104)θ, fi(cid:105).

Proof. See Barndorﬀ-Nielsen (1978). For self-containedness we provide a
proof in our notation in Appendix A.1.

Theorem 2.8 shows that EA is a ﬁnite union of sets EF,A that are expo-
nential families themselves with a very similar parametrization, using the
same number of parameters and the same design matrix A (or, rather, the
submatrix AF consisting of those columns of A indexed by F ). However, for
any proper facial set F , the parametrization θ (cid:55)→ pF,θ is not injective, i.e. the
parameters θ are not identiﬁable on EF,∆. The reason is that the matrix ˜AF
does not have full rank, even if ˜A has full rank, since all columns of ˜AF lie
on a supporting hyperplane deﬁning F .
A second thing to note is that although the parameters θ on EA and the
parameters θ on EF,A play similar roles, they are very diﬀerent in the following
sense: If θ(s) is a sequence of parameters with pθ(s) → pF,θ for some θ, then,
in general, lims→∞ θ(s)
Theorem 2.9. For any vector of observed counts n, there is a unique max-
imum p∗ of ˜l in EA. For t as deﬁned in (7), this maximum p∗ satisﬁes:

(cid:54)= θj for all j ∈ J.

j

• Ap∗ = t
N .
• supp(p∗) = Ft.

Proof. See Barndorﬀ-Nielsen (1978). For self-containedness we provide a
proof in our notation in Appendix A.2.

Deﬁnition 2.10. The maximum in Theorem 2.9 is called the extended max-
imum likelihood estimate (EMLE).

Clearly, if the mle θ∗ exists, then p∗ = pθ∗.

14

3 Approximations of Facial sets

We consider a hierarchical model with simplicial complex ∆ and marginal
polytope P∆. In this section, we develop the details of our methodology to
obtain an inner and an outer approximation to the facial set Ft of the smallest
face Ft of P∆ containing the data vector t. Our main tool is Lemma 3.1,
which is an application of Lemma 2.5 to hierarchical models of simplicial
complexes that are contained in each other. For any S ⊆ I, we abbreviate
the facial set FP∆(S) by F∆(S).
Lemma 3.1. Let ∆ and ∆(cid:48) be simplicial complexes on the same vertex
set with ∆(cid:48) ⊆ ∆, and denote by fi, f(cid:48)i (i ∈ I) the columns of the design
matrices of the corresponding hierarchical models. Then there is a linear
map φ : Rh → Rh(cid:48) with φ(fi) = f(cid:48)i. In fact, φ is a coordinate projection. In
particular, the marginal polytope P∆(cid:48) is a coordinate projection of P∆. Thus,
for any S ⊆ I, we have F∆(S) ⊆ F∆(cid:48)(S)
Proof. The matrix A∆ has one row for each parameter θj, j ∈ J∆. Removing
sets from ∆ leads to a smaller set J∆(cid:48) and thus leads to a matrix A∆(cid:48) with
less rows. The deﬁnition of each row that remains does not change. The
lemma now clearly follows from Lemma 2.5.

Next we discuss marginal polytopes of decomposable (or reducible) mod-
els. Then, in Sections 3.2 and 3.3, we explain how to use Lemma 3.1 to
obtain inner and outer approximations to F∆(S).

3.1 Decomposable models
Deﬁnition 3.2. Let V (cid:48) ⊂ V . The restriction or induced sub-complex is
∆|V (cid:48) = {S ∈ ∆ | S ⊆ V (cid:48)}. The sub-complex ∆|V (cid:48) is complete, if ∆|V (cid:48)
contains V (cid:48) (and thus all subsets of V (cid:48)). For brevity, in this case we say that
V (cid:48) is complete in ∆.
Deﬁnition 3.3. A subset S ⊂ V is a separator of ∆ if there exist V1, V2 ⊂ V
with V1 ∩ V2 = S, ∆ = ∆|V1 ∪ ∆|V2 and V1 (cid:54)= S (cid:54)= V2. A simplicial complex
that has a complete separator is called reducible. By extension, we also call
the hierarchical model reducible.

Deﬁnition 3.4. A hierarchical model is decomposable if ∆ can be written
as a union ∆ = ∆1 ∪ ∆2 ∪ . . . ∆r of induced sub-complexes ∆i = ∆|Vi in such
a way that

15

1. each ∆i is a complete simplex: ∆i = {S ⊆ Vi}; and
2. (∆1 ∪ ··· ∪ ∆i) ∩ ∆i+1 is a complete simplex.

In other words, ∆ arises by iteratively gluing simplices along complete sub-
simplices.

Faces of a reducible hierarchical model are combinations of the faces of

its two parts:

Proposition 3.5. Suppose that ∆ has a complete separator S that separates
V into V1 and V2. Each face of P∆|V1

corresponds to an inequality
j tj ≥ c1.
g(1)

(cid:88)

j∈J∆|V1

.

The same inequality also deﬁnes a face of P∆. Similarly, each face of P∆|V2
deﬁnes a face of P∆. Each face of P∆ either arises in this way, or it is
the intersection of two such faces, one induced by P∆|V1
and one induced
by P∆|V2
Proof. See Eriksson et al. (2006), Lemma 8.

In the sequel, for any V (cid:48) ⊆ V and i ∈ I =(cid:81)
marginal cell iV (cid:48) ∈ IV (cid:48) :=(cid:81)
v∈V Iv, it will be convenient
to use the seemingly more complicated notation πV (cid:48)(i) = (iv, v ∈ V (cid:48)) for the
to V (cid:48) is πV (cid:48)(S) :=(cid:8)πV (cid:48)(i) : i ∈ S(cid:9). For T ⊂ IV (cid:48), the opposite action yields
v∈V (cid:48) Iv. Similarly, for a set S ⊆ I, the restriction

V (cid:48) (T ) = {i ∈ I | iV (cid:48) ∈ T}.
π−1

We next translate Proposition 3.5 to the language of facial sets:

Lemma 3.6. Suppose that ∆ has a complete separator S that separates V
into V1 and V2.

1. If F ⊆ I is facial with respect to ∆, then πV1(F ) and πV2(F ) are facial

with respect to ∆|V1 and ∆|V2.

2. Conversely, if F1 ⊆ IV1 and F2 ⊆ IV2 are facial with respect to ∆|V1

and ∆|V2, then π−1

V1 (F1) ∩ π−1

V2 (F2) is facial with respect to ∆.

Thus, for any T ⊆ I, let T1 = πV1(T ) and T2 = πV2(T ).
V2 (F∆|V2

(T1)) ∩ π−1

F∆(T ) = π−1

V1 (F∆|V1

(T2)).

16

Proof. Consider an inequality as in Proposition 3.5 that deﬁnes a face F of
P∆ as well as a face F1 of P∆1. Then the corresponding facial sets F and
V1 (F1); because in order to check whether some fi, i ∈ I,
F1 satisfy F = π−1
satisﬁes the inequality, we only need to look at the components involving V1;
that is, we only need to look at πV1(i).

Lemma 3.6 easily generalizes to more than one separator and thus to
more than two components and it becomes particularly simple when these
components are complete. Indeed, in that case, F∆|V1
(T1) = T1 and taking
the preimage we obtain

V1 (πV1(T )) = {i ∈ I : ∃i(cid:48) ∈ T such that πV1(i) = πV1(i(cid:48))} ⊇ T.
π−1

The following lemma is an immediate consequence of Lemma 3.6.
Lemma 3.7. Let ∆ be a decomposable model with decomposition ∆ = ∆1 ∪
∆2 ∪ ··· ∪ ∆r where ∆i is a complete simplex on Vi, and let πi = πVi be the
corresponding marginalization map. Then, for any T ⊆ I,
2 (π2(T )) ∩ ··· ∩ π−1

1 (π1(T )) ∩ π−1

F∆(T ) = π−1

r (πr(T )).

3.2

Inner approximations

To obtain an inner approximation, our strategy is to ﬁnd a separator S of
∆ and to complete it. To be precise, we augment ∆ by adding all subsets
of S. Thus, we obtain a simplicial complex ∆S = ∆∪{M : M ⊆ S} in which
S is a complete separator. We can apply Lemma 3.6 to ﬁnd the facial set
F∆S (I+), and this will be our inner approximation of F∆(I).

An even simpler approximation is obtained by not only completing the
separator itself, but also the two parts V1, V2 separated by S: The simplicial
complex ∆V1,V2 := {M : M ⊆ V1} ∪ {M : M ⊆ V2} is decomposable and
contains ∆. Its facial sets can be computed from Lemma 3.7.

In general, the approximation obtained from a single separator (or, in
general, a single super-complex) is not good; that is, Ft = F∆(I+) tends to
be much larger than F∆S (I+) or F∆V1,V2
(I+). Thus we need to combine infor-
mation from several separators. For example, given two separators S, S(cid:48) ⊆ V ,

17

we ﬁnd a chain of approximations

G(cid:48)0 := I+,

G1 := F∆S (G(cid:48)0), G(cid:48)1 := F∆S(cid:48) (G1),
G2 := F∆S (G(cid:48)1), G(cid:48)2 := F∆S(cid:48) (G2),

...

that satisfy

I+ ⊆ G1 ⊆ G(cid:48)1 ⊆ G2 ⊆ ··· ⊆ Ft,

any more. The limit, which we denote by FS,S(cid:48)(I +) :=(cid:83)

where all inclusions except the last one are due to the deﬁnition of F∆S (T )
or F∆S(cid:48) (T ) as the smallest facial sets containing T in ∆S or ∆S(cid:48). The last
inclusion is a consequence of Lemma 3.1 since both ∆S and ∆S(cid:48) contain ∆.
This chain of approximations has to stabilize at a certain point; that is,
after a certain number of iterations, the approximations will not improve
i G(cid:48)i, can
be characterized as the smallest subset of I that contains I + and is facial
both with respect to ∆S and ∆S(cid:48). The same iteration can be done replacing
∆S and ∆S(cid:48) by ∆V1,V2 and ∆V (cid:48)1 ,V (cid:48)2 . Applying in turn F∆V1,V2
and F∆V (cid:48)1,V (cid:48)2
gives another approximation ˜FS,S(cid:48)(I +), namely the smallest subset of I that
contains I + and is facial both with respect to ∆V1,V2 and ∆V (cid:48)1 ,V (cid:48)2 . This latter
approximation will be used in Section 5.1. Clearly, ˜FS,S(cid:48)(I +) is a worse
approximation than FS,S(cid:48)(I +), since ˜FS,S(cid:48)(I +) ⊆ FS,S(cid:48)(I +) ⊆ Ft, but it is
easier to compute.

i Gi =(cid:83)

We use the following strategies:

1. If possible, use all separators of a graph.

There are two problems with this strategy: First, if S is such that either
V1 or V2 is large, then it is almost as diﬃcult to compute F∆|V1
and F∆|V2
as F∆|V . Such “bad” separators always exist: namely, each node i ∈ V is
separated by its neighbours from all other nodes. In this case, V1 consists of
i its neighbours, and V2 consists of V \ {i}. For such a “bad” separator we
can only compute F∆V1,V2
, but not F∆S . Second, the number of separators
may be large. Since we have to iterate over this set until the approximation
converges, it may take a long time to compute the inner approximation.

A faster alternative strategy is the following:

18

2. Look at separators such that both V1 \ S and V2 \ S are not too small

(for example, min{|V1 \ S|,|V1 \ S|} ≥ 3).

We illustrate the ﬁrst strategy in Section 5.2, using a graphical model associ-
ated with the NLTCS data set. In the case of the grids studied in Sections 5.1
and 6.2, which have a lot of regularity, we use an adpated strategy:

3. In a grid, use the horizontal, vertical and diagonal separators.

In the case of grids, the vertical separators form a family of pairwise disjoint
separators. In Section 6 we show how we can make use of such a family to
study faces of hierarchical models, even if the facial sets are so large that
they become computationally intractable.

3.3 Outer approximations
By Lemma 3.1, when we compute F∆(cid:48)(S) for a simplicial complex ∆(cid:48) ⊆ ∆,
then we obtain an outer approximation of F∆(S). Removing sets from ∆
decreases the dimension of the marginal polytope, so it is often easier to
compute F∆(cid:48)(S) than to compute F∆(S). Our main strategy is to look at
subcomplexes induced by subset V (cid:48) ⊂ V .
Let ∆V (cid:48) be the simplicial complex induced by V (cid:48). Let J ⊂ I be its set
of interactions. When comparing ∆ with ∆|V (cid:48), we have to be precise about
whether we consider ∆|V (cid:48) as a simplex on V or on V (cid:48). When we consider it
on V , let A be its I × J design matrix with rows fi, i ∈ I. When we consider
it on V (cid:48), the design matrix A(cid:48) is an IV (cid:48) × J matrix with columns f(cid:48)i(cid:48), i(cid:48) ∈ IV (cid:48).
Because we have the same set of interactions whether we are on V or V (cid:48), we
have for i ∈ I and i(cid:48) ∈ IV (cid:48),

fi = f(cid:48)i(cid:48) ⇔ i ∈ π−1

V (cid:48) (i(cid:48)).

(10)

Therefore the marginal polytopes of the two models are the same since they

are the convex hull of the same set of vectors {fi, i ∈ I} = {f(cid:48)i(cid:48), i(cid:48) ∈ IV (cid:48)}.
The relationship between the facial sets on V and V (cid:48) is as follows:
Lemma 3.8. Let V (cid:48) ⊆ V . For K ⊂ I, we have

F∆|V (cid:48) (K) = π−1

V (cid:48) (F (cid:48)∆|V (cid:48)

(πV (cid:48)(K))).

Here, F (cid:48)∆|V (cid:48)
complex on V (cid:48), and F∆|V (cid:48)
simplicial complex on V .

denotes the facial set when ∆V (cid:48) is considered as a simplicial
denotes the facial set when ∆V (cid:48) is considered as a

19

By deﬁnition of F (cid:48)∆V (cid:48)

Proof. For K ⊂ I, the two sets A = {ai, i ∈ K} and B = {bi(cid:48), i(cid:48) ∈ πV (cid:48)(K)}
are identical and therefore the smallest faces of the marginal polytopes for
∆V (cid:48) on V or V (cid:48) containing A and B respectively are the same.
(πV (cid:48)(K)), the smallest face containing B is deﬁned
(πV (cid:48)(K))}. By deﬁnition of F∆V (cid:48) (K), the smallest face
by {bi(cid:48), i(cid:48) ∈ F (cid:48)∆V (cid:48)
containing A is {ai, i ∈ F∆V (cid:48) (K)}. Also by (10), we have that {ai, i ∈
(πV (cid:48)(K))}. Therefore F∆V (cid:48) (K) =
π−1
V (cid:48) (F (cid:48)∆V (cid:48)
π−1
V (cid:48) (F (cid:48)∆V (cid:48)

(πV (cid:48)(K)))} = {bi(cid:48),
(πV (cid:48)(K))).

i(cid:48) ∈ F (cid:48)∆V (cid:48)

i=1 F∆|Vi

In general, F∆|V (cid:48) (I+) is not a good approximation of F∆(I+). We can
improve this approximation by considering several subsets of V . To be pre-
(I+) for i = 1, . . . , r, and thus

F∆(I+) ⊆(cid:84)r
cise, if V1, . . . , Vr ⊆ V , then F∆(I+) ⊆ F∆|Vi
(I+) =: FV1,...,Vr;∆(I+).

The question is now how to choose the subsets Vi. Clearly, the subsets
Vi should cover V , and, more precisely, they should cover ∆, in the sense
that for any D ∈ ∆ there should be one Vi with D ⊆ Vi. The larger the
sets Vi, the better the approximation becomes, but the more diﬃcult it is to
compute FV1,...,Vr;∆(I+). One generic strategy is the following:

1. Use all subsets of V of ﬁxed cardinality k plus all facets D ∈ ∆

with |D| ≥ k.

This choice of subsets indeed covers ∆. The parameter k should be chosen as
large as possible such that computing FV1,...,Vr;∆(I+) is still feasible. Note that
computing F∆|D(I+) for D ∈ ∆ is trivial, since P∆|D is a simplex. Another
2. For ﬁxed k, use balls Bk(v) = {w : d(v, w) ≤ k} around the nodes

natural strategy due to Massam and Wang (2015) is the following:

v ∈ V , where d(·,·) denotes the edge distance in the graph.

In general, our philosophy is that the subsets Vi should be large enough
to preserve some of the structure of ∆. For example, for the grid graphs, we
suggest to use 3 × 3-subgrids. These graphs have two nice properties: First,
they already have the appearance of a small grid. Second, for any vertex
v ∈ V , there is a 3 × 3 subgrid that contains v and all neighbours of v. We
will compare two diﬀerent strategies:

3. For a grid, use all 3 × 3-subgrids.
4. Cover a grid by 3 × 3-subgrids.

20

In Section 6.2 we compare these two methods, and we observe that, in the
example of the 5 × 10 grid, it suﬃces to only look at a covering.

In general, it is not enough to look at induced sub-complexes, unless
∆ has a complete separator (see Section 3.1). However, the approximation
tends to be good and gives the correct facial set in many cases.

3.4 Comparing the two approximations

Suppose that we have computed two approximations F1, F2 of Ft such that
F1 ⊆ Ft ⊆ F2. If we are in the lucky case that F1 = F2, then we know that
Ft = F1 = F2. In general, the cardinality of F2 \ F1 indicates the quality of
our approximations.

F1, F2 and Ft can also be compared by the ranks of the matrices ˜AF1,
˜AF2 and ˜AFt obtained from ˜A by keeping only the columns indexed by F1,
F2 and Ft, respectively. Clearly, rank ˜AF1 ≤ rank ˜AFt ≤ rank ˜AF2. Note
that rank ˜AF2 equals the dimension of the corresponding face F2 of P, and
rank ˜AFt equals the dimension of Ft. But F1 does not necessarily correspond
to a face of P. Nevertheless, we can bound the codimension of Ft in F2 by

dim F2 − dim Ft ≤ rank AF2 − rank AF1.

In particular, if rank AF2 = rank AF1, then we know that Ft = F2. In this
case, our approximations give us a precise answer, even if F1 (cid:54)= F2 and the
lower approximation F1 is not tight.

4 Parameter Estimation when the MLE does

not exist

4.1 Computing the extended MLE

If the mle θ∗ exists, then it can be computed by ﬁnding the unique maximum
of the log-likelihood function l(θ) given in (8). As mentioned before, l(θ) is
concave (or even strictly concave, if the parameters θ are identiﬁable), and
thus the maximum is, at least in principle, easy to ﬁnd (in practice, for larger
models, it may be diﬃcult to evaluate the function k(θ), which involves a sum
over I; but we will not discuss this problem here). In general, the maximum
cannot be found symbolically, but there are eﬃcient numerical algorithms to

21

maximize concave functions. Any reasonable hill-climbing algorithm should
be capable of ﬁnding the mle. An example of an algorithm commonly used is
iterative proportional ﬁtting (IPF), which can be thought of as an algorithm
of Gauss-Seidel type.

When the mle does not exist but the facial set F = Ft of the data is
known, then it is straight forward to compute the extended mle p∗. In this
case, we know that p∗ lies in EF,A. To ﬁnd p∗, we need to optimize the log-
likelihood ˜l over EF,A = {pF,θ : θ ∈ Rh}. Plugging the parametrization pF,θ
into ˜l tells us that we need to optimize the restricted log-likelihood function

(cid:89)

(cid:88)

lF (θ) = log(

pF,θ(i)n(i)) =

θjtj − N kF (θ).

(11)

i∈I+

j∈J

This problem is of a similar type as the problem to maximize l in the case that
the mle exists, and the same algorithms as discussed above can be used. The
problem here is slightly easier, since F is smaller than I. However, as stated
above, the parametrization θ (cid:55)→ pF,θ is never identiﬁable. Of course, this
problem is easy to solve by selecting a set of independent parameters among
the θj, as explained in Section 2.2. However, depending on the choice of the
independent subset, the values of the parameters change, and in particular,
it is meaningless to compare the values of the parameters θj with parameter
values of any other distribution in EA or in the closure EA.
Before explaining how to ﬁnd better parameters on EF,A, let us discuss
what happens if the facial set Ft of the data is not known. As mentioned be-
fore, whether or not the mle exists, the log-likelihood function l(θ) is always
strictly concave (assuming that the parametrization is identiﬁable). When
the mle does not exist, then the maximum is not at a ﬁnite value θ∗, but lies
“at inﬁnity.” Still, as observed by Geyer (2009, Section 3.15), any reasonable
numerical “hill-climbing” algorithm that tries to maximize the likelihood will
tend towards the right direction. Such a numeric algorithms generates a se-
quence of parameter values θ(1), θ(2), θ(3), . . . with increasing log-likelihood
values l(θ(1)) ≤ l(θ(2)) ≤ . . . . Since l(θ) is concave, our optimization prob-
lem is numerically easy (at least in theory), and for any reasonable such
˜l(p). The
algorithms, the limit lims→∞ l(θ(s)) will equal supθ l(θ) = maxp∈EA
algorithm will stop when the diﬀerence l(θ(s+1))− l(θ(s)) becomes negligeably
small. The output, θ(s), then gives a good approximation of the EMLE, in
the sense that p∗ and pθ(s) are close to each other. For many applications,
such as in machine learning, where it is more important to have good values

22

of the parameters instead of trying to model the “true underlying distribu-
tion,” or when doing a likelihood test, where the value of the likelihood is
more important than the parameter values, this may be good enough.
However, in this numerical optimization, some of the parameters θj will
tend to ±∞, which may lead to numerical problems. For example, it may
happen that one parameter goes to +∞ and a second parameter to −∞
in such a way that their sum remains ﬁnite. This implies that a diﬀerence
between two large numbers has to computed, which is numerically unstable.
Also, it is not clear, which parameters tend to inﬁnity numerically. In fact,
this may depend on the chosen algorithm; i.e. diﬀerent algorithms may yield
approximations of the EMLE that are qualitatively diﬀerent in the sense that
diﬀerent parameters diverge. We give an example of this in Appendix B.

To avoid such problems, we propose a change of coordinates that allows us
to control which parameters diverge, at least in the case where we know the
facial set Ft. If we don’t know Ft, but if we know approximations F1 ⊆ Ft ⊆
F2, we can use this knowledge to identify some parameters that deﬁnitely
remain ﬁnite, while some parameters deﬁnitely diverge. While we cannot
control the behaviour of the remaining parameters, the hope is that the
more information we have about the faciel set Ft, the better control we have
about the above mentioned pathologies.

4.2 An identiﬁable parametrization
We have seen that when we use the parametrization θ (cid:55)→ pFt,θ of EA,Ft in the
case where Ft (cid:54)= I, we have to expect the following (interrelated) issues:

1. The parametrization is not identiﬁable, i.e. there are parameters θ, θ(cid:48)

with pFt,θ = pFt,θ(cid:48).

2. While the parametrization θ (cid:55)→ pFt,θ looks similar to the parametriza-
tion θ (cid:55)→ pθ of EA, the values of the parameters in both parametrizations
are not related to each other.

3. When pθ(s) → pFt,θ as s → ∞ for some parameter values θ(s), θ, then
some of the parameter values θ(s) diverge to ±∞. When computing
probabilities, there may be linear combinations of these diverging pa-
rameters that remain ﬁnite.

Next we show that if Ft is known, then, with a convenient choice of L, the
parameters µL (introduced in Section 2.2) solve 1 and 2 and improves 3.

23

Afterwards, we discuss what can be done if Ft is not known. We brieﬂy
discuss the general solution towards 3 in Appendix D. In any case, the choice
of the parameters will depend on the facial set Ft; i.e. it is not possible to
deﬁne a single parametrization that works for all facial sets simultaneously.
Suppose that Ft is known. We consider the parameters µi as in Sec-
tion 2.2, and we make sure that we choose the zero element 0 in I+. Recall
that

µi(θ) = (cid:104)θ, fi − f0(cid:105) = log p(i)/p(0), i ∈ I.

As mentioned in Section 2.2, the parameters µi are not independent, and we
need to choose an independent subset L. We will do this in two steps:

1. Choose a maximal subset Lt of Ft such that the parameters µi, i ∈ Lt

are independent.

2. Then extend Lt to a maximal subset L ⊆ I such that the parameters µi,

i ∈ L are independent by adding elements i ∈ I \ Ft.

It follows from Theorem 2.9 that the following holds:

1. The subset µi, i ∈ Lt, of the parameters µL gives an identiﬁable

parametrization of EFt,A.

2. Let µ∗i , i ∈ Lt, be the parameter values that maximize lFt (and thus give
the EMLE). When the likelihood l(µ) in (9) is maximized numerically
on I, then in successive iterations of the maximization, the estimates
µ(s)
i

are such that

(cid:40)

i →
µ(s)

µ∗i ,
−∞,

i = 1, . . . , ht,
otherwise.

In particular, no parameter tends to +∞.

The last property ensures a consistency of the parameters µi on EA and
on EFt,A. This is important in those cases where the parameters have an in-
terpretation and where it is of interest to know the value of some parameters,
if it is well-deﬁned. For example, in hierarchical models, the parameters cor-
respond to “interactions” of the random variables, and it may be of interest
to know, which of these interactions are important. Thus, it is of interest to
know the size of the corresponding parameter. Usually, it is not the parame-
ter µi, but the original parameters θi that have an interpretation. But when

24

we understand the parameters µi, we can also tell which of the paramters θi
or which combinations of the parameters θi have ﬁnite well-deﬁned values
and can be computed, and which parameters diverge:
Lemma 4.1. Suppose that θ(s), s ∈ N, are parameter values such that pθ(s) →
p∗ as s → ∞. For any i ∈ Lt, the linear combination

i = (cid:104)θ(s), fi(cid:105)
µ(s)

has a well-deﬁned ﬁnite limit as s → ∞. Any linear combination of the
θ(s)
that has a well-deﬁned ﬁnite limit (that is, a limit that is independent of
i
the choice of the sequence θ(s)) is itself a linear-combination of the µ(s)
i with
i ∈ Lt.
Proof. The ﬁrst statement follows from

i = log pθ(s)(i)/pθ(s)(0) → log p∗(i)/p∗(0).
µ(s)

now show that if a linear combination(cid:80)

For the second statement, note that any linear combination of the θ is also a
linear combination of the µ, since the linear map θ (cid:55)→ µ(θ) is invertible. We
i aiµi involves some µj with j /∈ Lt,
(cid:88)

then there exist sequences µ(s), µ(cid:48)(s) of parameters with
(cid:54)= lim
s→∞

pµ(s) = lim
s→∞

(cid:88)

lim
s→∞

lim
s→∞

aiµ(s)
i

pµ(cid:48)(s)

aiµ(cid:48)(s)

i

.

and

i

i

So suppose that µ(s) is a sequence of parameters such that lims→∞ pµ(s) exists
and such that lims→∞

is ﬁnite. Deﬁne

i aiµ(s)

i

(cid:80)

(cid:40)

µ(cid:48)(s)
i =

µ(s)
j + 1,
µ(s)
i

,

An easy computation shows that

lim
s→∞

pµ(cid:48)(s) = lim
s→∞

pµ(s)

and

lim
s→∞

if i=j,
otherwise.

aiµ(cid:48)(s)

i = lim
s→∞

(cid:88)

i

aiµ(s)

i + aj.

(cid:88)

i

Suppose now that we do not know Ft, but that instead we have approxi-

mations F1, F2 that satisfy

I+ ⊆ F1 ⊆ Ft ⊆ F2 ⊆ I.

In this case, we proceed as follows to obtain an independent subset L among
the paramters µi:

25

1. Choose a maximal subset L1 of F1 such that the parameters µi, i ∈ L1

are independent.

2. Then extend L1 to a maximal subset L2 ⊆ F2 such that the parame-

ters µi, i ∈ L2 are independent by adding elements i ∈ F2 \ F1.

3. Finally, extend L2 to a maximal subset L ⊆ I such that the parame-

ters µi, i ∈ L are independent by adding elements i ∈ I \ F2.

These parameters have the following properties that follow directly from
Lemma 4.1:
Lemma 4.2. Suppose that θ(s), s ∈ N, are parameter values such that pθ(s) →
p∗ as s → ∞, and let µ(s)

i = (cid:104)θ(s), fi(cid:105).

1. For any i ∈ L1, the linear combination

i = (cid:104)θ, fi(cid:105)
µ(s)

has a well-deﬁned ﬁnite limit as s → ∞. Thus, any linear combination
of the µ(s)

i with i ∈ L1 has a well-deﬁned limit as s → ∞.

2. Any linear combination(cid:80)

is in fact a linear combination of the µ(s)
combination that involves at least one µ(s)
a well-deﬁned limit.

i aiµ(s)

i

that has a well-deﬁned limit as s → ∞
i with i ∈ L2. Thus, a linear
j with j ∈ L\L2 does not have

5 Simulation study and applications to real

data

In this section, we illustrate our methodology. In 5.1, we simulate data for
the graphical model of the 4 × 4 grid and show how to exploit the various
types of separators in order to obtain good inner and outer approximations.
We ﬁnd that our method gives very accurate result in this model of modest
size. In 5.2, we work with the NLTCS data set, a real-world data set. We
compare diﬀerent inner approximations F1 and ﬁnd that most of the time,
F1 and F2 are equal, and thus they are both equal to Ft. We also compute
the EMLE and compare the result to what happens when maximizing the
likelihood functions l and lF2.

26

Figure 1: 4 × 4 grid graph

5.1 4 × 4 grid graph

We generated random samples of varying sizes for the graphical model of
the 4 × 4 grid graph (Fig. 1). For each sample, we compute inner and
outer approximations F1 and F2, and we compare them to the true facial
set Ft, which we can obtain using linear programming. To obtain an inner
approximation, we use two strategies. Either, we iterate over all possible
separators of which there are 106 (Strategy (1) in Section 3.2) or we iterate
over the 3 horizontal, 3 vertical and 8 diagonal separators only (Strategy (3)
in Section 3.2). We obtain the same result with either strategy. Clearly,
Strategy (3) is much faster. To compute the outer approximation, we cover
the 4 × 4 grid by four 3 × 3-grids (Strategy (3) in Section 3.3).

We ﬁrst generate random samples from the uniform distribution, that
is from the probability distribution Pθ in the hierarchical model where all
parameters θj, j ∈ J are set to zero. The results are given in Table 1. For
each sample size, 1000 samples were obtained. As the table shows, for larger
samples the probability that our random sample lies on a proper face becomes
very small. If Ft = I, then clearly Ft = F2. But we also found Ft = F2 for
all samples with t lying on a proper face, which shows that F2 is an excellent
approximation of Ft in this model. For the inner approximation, we observed
some samples with F1 (cid:54)= Ft, but they seem to be very rare.

Second, to better understand what happens for large samples, we change
our sampling scheme.
Instead of sampling from the uniform distribution,
we generate samples from the hierarchical model Pθ, where the vector of
parameters θ is drawn from a multivariate standard normal distribution (for
each sample, new parameters were drawn). The results are given in Table 2.
Again, for each sample size, 1000 samples were obtained. One can see that in
this sampling scheme, we are much more likely to ﬁnd that Ft (cid:54)= I. Observe

27

Table 1: facial set approximation of 4 × 4 grid graph (sample from uniform
distribution)

sample size data on face F1 = Ft F2 = Ft

10
15
20
50

98.5%
68.9%
29.0%
0.0%

96.3% 100.0%
99.9% 100.0%
100.0% 100.0%
100.0% 100.0%

Table 2: facial set approximation of 4 × 4 grid graph(hierarchical log-linear
model with parameters from standard normal distribution)
sample size data on face F1 = Ft F2 = Ft

10
50
100
150

100.0%
89.5%
71.0%
52.0%

97.7% 100.0%
100.0% 100.0%
100.0% 100.0%
100.0% 100.0%

that the squared length of the parameter vector θ is χ2-distributed with
39 degrees of freedom (since the number of parameters is 40). Thus, the
expected length of θ is 39, which is large enough to move the distribution
pθ close to the boundary of the model. Indeed, we observed that when the
mle does not exist, the length of the numerical estimate of the mle vector is
of the order of magnitude of 40 (see also the next example in Section 5.2).
Again, in all the samples that we generated, Ft = F2, and F1 = F2 in the
vast majority of cases. Thus, for this graph of relatively modest size, our
approximations are very good.

5.2 NLTCS data set

To illustrate how approximate knowledge of the facial set allows us to say
which parameters can be estimated (as explained in Section 4), we study
the NLTCS data set, which consists of 21 574 observations on 16 binary
variables, called ADL1, . . . , ADL6, IADL1, . . . , IADL10. The reader is
referred to Dobra and Lenkoski (2011) for a detailed description of the data
set. To associate a hierarchical to this data, we rely on the results of Dobra

28

Figure 2: Graphical model for NLTCS data set. The label “An” abbreviates
ADLn, “In” abbreviates IADLn.

and Lenkoski (2011) who use a Bayesian approach to estimate the posterior
inclusion probabilities of edges. We construct a graph by saying that (x, y)
is an edge if and only if the posterior inclusion probability of (x, y) is at
least 0.40; see Figure 2. Then we take the corresponding clique complex of
this graph so that our hierarchical model is a graphical model. There are
314 parameters in this model, including up to 6-way interactions. In total,
the graph has 40 separators.

In order to compare the maximum likelihood estimates obtained with
or without worrying about its existence and with or without approximation
to Ft, we maximize the loglikelihood given in terms of µ (rather than θ) as
in (9). There are 314 independent parameters.

First we ignore the fact that the mle might not exist and compute the mle
of µ using the standard minfunc optimization software in Matlab: we call this
estimate ˆµMLE. Second, we ﬁnd Ft and compute the EMLE with parameters
denoted ˆµEMLE. Third, we obtain an inner and outer approximation to Ft
and consider the resulting information on the mle of the parameters. We call

29

A1A2A3A4A5A6I1I2I3I4I5I6I7I8I9I10the resulting estimate ˆµF (cid:48)1/F (cid:48)2.

5

To compute ˆµEMLE, we ﬁrst compute the inner approximation F1 that
makes use of all the separators in the graph (Strategy 1 in Section 3.2). We

also compute an outer approximation F2 from all(cid:0)16

(cid:1) = 4368 size ﬁve local

models and the cliques of size six (Strategy 1 in Section 3.3). We obtain
F1 = F2 and thus deduce that Ft = F1 = F2. We ﬁnd |Ft| = 49 536, and so
|F c
t | = 216 − 49 536 = 16 000. Therefore, 16 000 cell probabilities are zero in
the EMLE. We can obtain the mle by maximizing the loglikelihood function
lFt as in (11). Since rank(AFt) = 302, the dimension of Ft is 302, and there
are only 302 parameters in lF .

To show how to use the inner and outer approximations when Ft is not
known, we choose to ﬁnd coarser inner and outer approximations to Ft,
respectively denoted F (cid:48)1 and F (cid:48)2, and use them to compute the other approxi-
mation ˆµF (cid:48)1/F (cid:48)2 to the mle. To compute F (cid:48)1, we just use 10 random separators.
We ﬁnd |F (cid:48)1| = 36 954 and dim F(cid:48)1 = rank AF (cid:48)1 = 300. To compute the outer
approximation F (cid:48)2, we consider the 4368 local size-ﬁve induced models and
select among them the 1000 with the facial sets of smallest cardinality, which
we glue together. We ﬁnd |F (cid:48)2| = 50 688 and dim F(cid:48)2 = rank AF (cid:48)2 = 310. Thus,
we know that at least |I \ F (cid:48)2| = 216− 50 688 = 14 848 cell probabilities vanish
in the extended mle. Since we pretend not to know Ft, we replace lFt by

lF (cid:48)2(µ) =

µin(i) − N

exp(µi).

(12)

(cid:88)

i∈I+

(cid:88)

i∈F (cid:48)2

(cid:88)

We know that µi is estimable, for i ∈ F (cid:48)1, µi goes to negative inﬁnity for
i ∈ F (cid:48)c
2 , and we cannot say anything for µi with i ∈ F (cid:48)2 \ F (cid:48)1.
As explained in Section 4.2, the components of µ are not functionally
independent. We choose L1 ⊆ F (cid:48)1, L2 ⊆ F (cid:48)2 and L ⊆ I as in Section 4.2 (we
note that the zero cell belongs to I+). Then any µi, i ∈ F (cid:48)2, can be written as
a linear combination of µL2 = (µi, i ∈ L2), and we can write µi = (cid:104)bi, µL(cid:105) for
an appropriate vector bi. Thus, lF (cid:48)2(µ) only depends on µL2 = (µi, i ∈ L2),

and (12) can be rewritten as

lF (cid:48)2(µL) =

(cid:104)bi, µL(cid:105)n(i) − N

exp(cid:104)bi, µL(cid:105).

(13)

i∈I+

i∈F (cid:48)2

Of course, the maximum of lF (cid:48)2 does not exist but, as for the maximization
of l, the computer still gives us a numerical approximation, ˆµL, and thus also
a numerical estimate ˆµi = (cid:104)bi, ˆµL(cid:105), i ∈ F (cid:48)2.

30

(cid:88)

Table 3: The MLE estimation from 3 methods compared with the relative
frequency in the NLTCS data. Here, each i = (i1, . . . , i16) ∈ I = {0, 1}16 is

represented by the natural number(cid:80)16

j=1 ij2j−1 ∈ {0, . . . , 216 − 1}.

i ∈ F (cid:48)1

Parameter
µ512
µ65536
µ16
µ528
µ2048
i ∈ Ft \ F (cid:48)1 µ32960
µ34881
i ∈ F (cid:48)2 \ Ft µ36864
µ36880
µ388
µ32769
µ385
µ449
µ32785
µ389
µ256
µ320
µ257
µ321

i ∈ I \ F (cid:48)2

i

i

ˆµEMLE

naive estimate maximum likelihood estimates
ˆµF (cid:48)1/F (cid:48)2
ˆµMLE
−1.2482 −1.2482 −1.2482
i
−1.7976 −1.7975 −1.7975
−2.3844 −2.3846 −2.3846
−2.6504 −2.6504 −2.6504
−2.7246 −2.7243 −2.7243
−13.8205 −13.8207 −13.8205
−14.3693 −14.3693 −14.3692
−30.8729
−34.9805
−45.2229
−39.6536
−28.9090
−29.4525
−32.3799
−36.9537
−37.1365
−35.9399
−44.9405
−38.9673
−40.1221
−45.8318
−43.7297
−40.0158
−35.5482
−42.5454
−52.9224
−60.2208

log ni/n0
−1.2472
−1.7644
−2.3958
−2.5429
−2.8813
−∞
−∞
−∞
−∞
−∞
−∞
−∞
−∞
−∞
−∞
−∞
−∞
−∞
−∞

−∞
−∞
−∞
−∞
−∞
−∞
−∞
−∞
−∞
−∞
−∞
−∞

−∞
−∞
−∞
−∞

In total, there are |L2| = rank(AF (cid:48)2) = 310 independent parameters in the
loglikelihood function (13). Among them, we ﬁnd |L2| = rank(AF (cid:48)2) = 300
estimable parameters µi, i ∈ L2. We cannot say anything about the 10
parameters indexed by L2 \ L1. If we know Ft, we can identify two more
estimable parameters.

i

i

i

, ˆµEMLE

and ˆµF (cid:48)1/F (cid:48)2

In Table 3, we give the three estimates of µi that we mentioned above,
namely, ˆµMLE
. We
list estimates for 19 of the 310 possible parameters. In the ﬁrst column of
the table, we indicate which category the index i belongs to, that is, whether
it belongs to F (cid:48)1, Ft or F (cid:48)2.
In the second column, we list the particular
parameters considered.

. We also list the naive estimator log ni
n0

31

Table 4: Expected cell counts for the top six largest counts cells in the
NLTCS data estimated according to Grade of Membership models (GoM),
Latent class models (LC), copula Gaussian graphical models (CGGM) and
maximum likelihood (MLE).

Support of Cell Observed GoM

LC

CGGMs MLE on facial set

∅
{10}
{1 : 16}
{5}
{5, 10}
{12}

3853
1107
660
351
303
216

3269
1010
612
331
273
202

3836.01
1111.51
646.39
360.52
285.27
220.47

3767.76
1145.86
574.76
452.75
350.24
202.12

3647.4
1046.9
604.4
336

257.59
239.24

In Table 4, we list the estimates of the top ﬁve cell counts obtained using
our method and compare them with those obtained by other methods in
Dobra and Lenkoski (2011).

6 Computing faces for large complexes

If our statistical model contains many variables and is not reducible, the
problem of determining Ft quickly becomes infeasible. Not only does the
marginal polytope become very complicated, but also the size of the objects
that one has to store or compute grows exponentially. Consider for example a
10× 10 grid of binary random variables. This hierarchical model has 280 pa-
rameters, and the total sample space has cardinality |I| = 2100 ≈ 1.27 × 1030.
If Ft is close to I, we cannot even list the elements of Ft, which consists of
approximately 1030 elements. Therefore, we take a local approach and look
for separators.

V1 (FV1) ∩ π−1

If the simplicial complex ∆ contains a complete separator separating V
into V1 and V2, we can identify a facial set F implicitly without listing it ex-
plicitly. We only need the two projections FV1 = πV1(F ) and FV2 = πV2(F ).
Since F = π−1
V2 (FV2) (by Lemma 3.6), these two projections iden-
tify F , and they allow us to do most of the operations that we would want
to do with F . For example, for any i ∈ I, we can check whether i ∈ F by
checking whether πV1(i) ∈ FV1 and πV2(i) ∈ FV2, and we can check whether
F = I by checking whether FV1 = IV1 and FV2 = IV2. In particular, we can
check whether the MLE exists by looking only at the two subsets V1 and V2.

32

If ∆ contains a separator that is not complete, we can use similar ideas
when computing inner and outer approximations to Ft, and also when com-
paring these two approximations. Suppose that S separates V1 from V2
in ∆. We want to use F2 := F∆|V1
(I+) as an outer approxi-
mation and F1 := F∆S (I+) as an inner approximation to Ft. Due to the
problems mentioned above, we do not directly compute F1 and F2, but we
compute their projections on V1 and V2. Instead of F2, we compute the facial
(πV1(I+)) of the V1-marginal πV1(I+) with respect to ∆|V1,
set F2,V1 := F∆|V1
and similarly we compute F2,V2 := F∆|V2
(πV2(I+)). Instead of F1, we compute
F1,V1 := F∆S|V1
(πV2(I+)). Then we could recover
F1 and F2 from the equations

(πV1(I+)) and F1,V2 := F∆S|V2

(I+) ∩ F∆|V2

F2 = π−1

V1 (F2,V1) ∩ π−1

V2 (F2,V2)

and

F1 = π−1

V1 (F1,V1) ∩ π−1

V2 (F1,V2).

For any x ∈ I, we can check whether x ∈ F1 by checking whether πV1(x) ∈
F1,V1 and πV2(x) ∈ F1,V2. More importantly, we can check whether F1 = F2
by checking whether F1,V1 = F2,V1 and F1,V2 = F2,V2. This idea can be applied
iteratively when ∆|V1 or ∆|V2 has a separator.

The next two subsections illustrate these ideas. In Section 6.1, we consider
a graph with no particular regularity pattern on 100 nodes and identify two
convenient separators. In Section 6.2, we consider a grid graph and work with
two families of “parallel” separators that can be used to iteratively improve
the inner approximation.

6.1 US Senate Voting Records Data

We consider the voting record of all 100 US senators on 309 bills from January
1 to November 19 2015. Similar data for the years 2004–2006 was analyzed
by Banerjee et al. (2008). The votes are recorded as “yea,” “nay” or “not
voting.” We transformed the “not voting” into “nay” and consequently have
a 100-dimensional binary data set. To ﬁt a hierarchical model to this data set,
we use the (cid:96)1-regularized logistic regression method proposed by Ravikumar
et al. (2010) to identify the neighbours of each variable and construct an Ising
log p/n ≈ 0.35, resulting
model. We set the penalty parameter to λ = 32
in the sparse graph in Figure 3. There are 277 parameters in this model (the
number of vertices plus the number of edges). The graph consists of two
large connected components and 14 independent nodes.
There are 309 sample points, and |I+| = 278. We want to know whether
the data lies on a proper face of the marginal polytope to see if the mle of the

(cid:112)

33

Figure 3: The graph for the US senate voting records data. Golden nodes
are independent senators, blue nodes are democratic, and red nodes are re-
publican.

34

KingSandersBlumenthalBookerBrownCantwellCardinCoonsDurbinFeinsteinGillibrandHeinrichHironoMarkeyMen´endezMerkleyMikulskiMurphyMurrayNelsonPetersReedSchumerShaheenUdallWarrenWhitehouseWydenBaldwinBennetBoxerCarperCaseyFrankenKaineKlobucharLeahyMcCaskillReidSchatzStabenowTesterWarnerDonnellyHeitkampManchinBarrassoBluntBoozmanCassidyCochranCornynCrapoDainesErnstFischerHatchHoevenInhofeMoranRischRobertsRoundsSasseTillisWickerAlexanderBurrCapitoCoatsCorkerCottonEnziFlakeGardnerGrassleyIsaksonJohnsonLankfordLeeMcCainMcConnellPerdueSessionsScottShelbySullivanToomeyThuneAyotteCollinsCruzGrahamHellerKirkMurkowskiPaulPortmanRubioVitter(a)

(b)

Figure 4: The simplicial complexes after cutting oﬀ the small prime com-
ponents: (a) the republican party prime component ∆r. (b) the democratic
party prime component ∆d. The yellow and pink nodes are the two separator
sets we found to compute the facial set.

parameters exist. From Lemma 3.6, we know that if we ﬁnd complete sepa-
rators, we need only work with each of the irreducible simplicial complexes
deﬁned by these separators. We easily “cut-oﬀ” a number of relatively small
prime components and verify that the data does not lie on a proper face
of their corresponding marginal polytopes. We are left with one irreducible
prime component in each of the two connected subgraphs, i.e. for each of the
two parties as shown in Figure 4.

The democratic party simplicial complex ∆d consists of 26 variables, and
the model induced from ∆d contains 77 parameters. The size of the design
matrix A∆d is 226 × 77, which is too large to use linear programming to com-
pute the facial set of the face P∆d containing the vector td. Therefore we look
for separators that will help us obtain good inner and outer approximations.
In Figure 4b, we indicate in yellow and pink two separators, which separate
∆d into three simplicial complexes denoted (from top to bottom) by ∆α, ∆β
and ∆γ. The number of vertices of the three simplicial complexes are 9, 13,
11, respectively, and so we can apply the linear programming method to the
three corresponding marginal polytopes.

The dimension of the model induced by ∆α is 24. The corresponding

data vector tα lies in the relative interior of P∆α.

The dimension of the model induced by ∆β is 34, and the data vector
tβ lies on a facet Ftβ of P∆β . To simplify our notation, we denote the 100
senators not by their name but by an integer between 1 and 100. We only

35

BarrassoBluntBoozmanCassidyCochranCornynCrapoDainesErnstFischerHatchHoevenInhofeMoranRischRobertsRoundsSasseTillisWickerBlumenthalBookerBrownCantwellCardinCoonsDurbinFeinsteinGillibrandHeinrichHironoMarkeyMen´endezMerkleyMikulskiMurphyMurrayNelsonPetersReedSchumerShaheenUdallWarrenWhitehouseWydenID Senator

ID Senator

ID Senator

ID Senator

22 Nelson
23 Reed
26 Schumer

37 Cardin
41 Markey
47 Udall

52 Murphy
53 Hirono
56 Gillibrand

61 Whitehouse
87 Warren

Table 5: Numbering of some senators

need to identify a few and their numbers are given in Table 5. The inequality
of Ftβ is

t87 − t56,87 ≥ 0,

(14)

where t87 denotes the marginal count of senator Warren voting “yea” and
t56,87 denotes the marginal counts of both senators Gillibrand and Warren
voting “yea.”

The dimension of the model induced by ∆γ is 27. The data vector tγ lies

on the facet of P∆γ with inequality

t23 − t23,53 ≥ 0.

(15)

The intersection of the two facets (14) and (15) gives the outer aproximation
F2 to Ft.

To get an inner approximation, we complete each separator, i.e. the yellow
vertices are completed and the pink vertices are completed in Figure 4b. De-
note the three simplicial complexes with complete separators as ∆ ˜α, ∆ ˜β, ∆˜γ
respectively. Then ∆ ˜d = ∆ ˜α ∪ ∆ ˜β ∪ ∆˜γ is a simplicial complex with two
complete separators. The smallest face Ft ˜d
of the marginal polytope P∆ ˜d
containing the data vector t ˜d is our inner approximation. Now the mod-
els of ∆ ˜α, ∆ ˜β, ∆˜γ and ∆ ˜d are not models with main eﬀects and two-way
interactions only; they also include parameters for third and fourth order
interactions. The dimension of the model induced by ∆ ˜d is 91: we added 14
parameters to the original model by completing the two separators. Again,
we apply the linear programming method to the three marginal polytopes
P∆ ˜α, P∆ ˜β

and P∆˜γ .

The dimension of the model of ∆ ˜α is 27, and Ft ˜α is a facet with equation

(cid:104)g1, t ˜α(cid:105) = t41 − t22,41 − t41,70 + t22,41,70 = 0.

(16)

It follows that {g1} is a basis of the kernel of At

F ˜α.

36

The dimension of the model for ∆ ˜β is 48. The face Ft ˜β

has codimension 5,

with deﬁning equations

It is deﬁned by the equations

(cid:104)g2, t ˜β(cid:105) = t87 − t56,87 = 0
(cid:104)g3, t ˜β(cid:105) = t47,52,61 + t37,52 − t37,52,61 − t37,47,52 = 0
(cid:104)g4, t ˜β(cid:105) = t37,47,52,61 − t47,52,61 = 0
(cid:104)g5, t ˜β(cid:105) = t37,52 + t26 − t26,52 − t26,37 = 0
(cid:104)g6, t ˜β(cid:105) = t41 − t22,41 − t41,70 + t22,41,70 = 0
Again, {g2, g3, g4, g5, g6} is a basis of the kernel of AF ˜β
.


(cid:104)g7, t˜γ(cid:105) = t47,52,61 + t37,52 − t37,52,61 − t37,47,52 = 0
are

(cid:104)g(cid:48)1, t ˜d(cid:105) = t41 − t22,41 − t41,70 + t22,41,70 = 0
(cid:104)g(cid:48)2, t ˜d(cid:105) = t87 − t56,87 = 0
(cid:104)g(cid:48)3, t ˜d(cid:105) = t47,52,61 + t37,52 − t37,52,61 − t37,47,52 = 0
(cid:104)g(cid:48)4, t ˜d(cid:105) = t37,47,52,61 − t47,52,61 = 0
(cid:104)g(cid:48)5, t ˜d(cid:105) = t37,52 + t26 − t26,52 − t26,37 = 0
(cid:104)g(cid:48)9, t ˜d(cid:105) = t23 − t23,53 = 0

(cid:104)g8, t˜γ(cid:105) = t37,47,52,61 − t47,52,61 = 0
(cid:104)g9, t˜γ(cid:105) = t23 − t23,53 = 0

for Ft ˜d

Again, {g7, g8, g9} is a basis of the kernel of AF˜γ .

From Lemma 3.6, we know that Ft ˜d

= F ˜α ∩ F ˜β ∩ F˜γ, and the equations

The dimension of the model for ∆˜γ is 38. The face Ft˜γ has codimension 3.

.

(17)

.

(18)

,

(19)

where the vectors g(cid:48)1, . . . , g(cid:48)9 are the vectors g1, . . . , g9 extended to R91 by
adding zeros on the corresponding complementary coordinates. Note that
since g(cid:48)1 = g(cid:48)6, g(cid:48)3 = g(cid:48)7, g(cid:48)4 = g(cid:48)8, we only need six of the nine equations. Thus,
F1 := Ft ˜d
, deﬁned by (19), is a strict subset of the face F2 deﬁned by (14)
and (15). Next, we will reﬁne our argument and show that indeed Ftd = F2.
From what we know, it follows that the orthogonal complement of the

subspace generated by Ft ˜d

is

G = {g(cid:48) ∈ R91|g(cid:48) = k1g(cid:48)1 + k2g(cid:48)2 + k3g(cid:48)3 + k4g(cid:48)4 + k5g(cid:48)5 + k9g(cid:48)9}.

37

To describe Ftd, we want to describe the deﬁning equations of Ftd. Each such
equation is of the form (cid:104)g, td(cid:105) = 0, where g is orthogonal to Ftd. For any such
g, let g(cid:48) be its extension to a vector in R91 by adding zero components. Then
g(cid:48) ⊥ Ft ˜d
, which implies that g(cid:48) ∈ G. Therefore, we can ﬁnd g by ﬁnding all
vectors g(cid:48) ∈ G that vanish on all added components. This yields a system
of linear equations in k1, . . . , k5, k9. We claim that all solution must satisfy
k1 = k3 = k4 = k5 = 0. Indeed, the coeﬃcient of any triple or quadruple
interaction must vanish (since these don’t belong to the original Ising model),
which implies k1 = k3 = k4 = 0, and also the coeﬃcient of t37,52 must vanish,
which implies k5 = 0. On the other hand, the vectors g(cid:48)2 and g(cid:48)9 only contain
interactions that are already present in ∆, and so the coeﬃcients k2 and k9
are free. Thus the equations for Ftd are

(20)

(cid:40)(cid:104)g2, t ˜β(cid:105) = t87 − t56,87 = 0,

(cid:104)g9, t˜γ(cid:105) = t23 − t23,53 = 0.

This is the same as the outer approximation F2.

The republican simplicial complex ∆r consists of 20 variables, and the
model induced from ∆r contains 46 parameters. The size of the design ma-
trix A∆r is 220 × 46, which is also too large to directly compute Ft. The
yellow nodes in Figure 4a separate ∆r into two simplicial complexes denoted
(from left to right) by ∆a and ∆b. To compute the inner approximation,
we complete the yellow separators and we get two new simplicial complexes
∆˜a and ∆˜b. With the linear programming algorithm, we ﬁnd that the corre-
sponding data t˜a and t˜b lie in the relative interior of the polytopes P∆˜a and
P∆˜a, respectively. Therefore we have F1 = P∆r. Since F1 ⊆ Ft ⊆ P∆r, we
conclude that the corresponding data vector tr lies in the relative interior
of P∆r.
6.2 The 5 × 10-grid
Let ∆ be the simplicial complex of the 5 × 10 grid graph. We exploit the
regularity of this graph and make use of the vertical separators in the grid
to obtain inner and outer approximations of the facial sets. The graph has
50 nodes, which is too many to directly compute a facial set or even to store
it. However, the 5× 10 grid has 8 vertical separators marked in red and blue
in Figure 5, and we can use these to approximate Ft. Since facial sets for
5× 3-grids can be computed reasonably fast (3 to 4 seconds on a laptop with

38

Figure 5: 5 × 10 grid graph, the red and blue nodes are the set of separa-
tors we use to compute F1, they are used iteratively to get a better lower
approximation

Figure 6: Five induced sub-grids

2.50 GHz processor and 12 GB memory), we only use three of these vertical
separators at a time, say the blue separators
S2 = {11, . . . , 15}, S4 = {21, . . . , 25}, S6 = {31, . . . , 35}, S8 = {41, . . . , 45}.
These separators separate the vertex sets

V1 = {1, . . . , 15}, V3 = {11, . . . , 25}, V5 = {21, . . . , 35},

V7 = {31, . . . , 45}, V9 = {41, . . . , 50}.

Adding the blue separators to ∆ gives a simplicial complex

∆S2;S4;S6;S8 := ∆

{F : F ⊆ Sj}

(cid:91)

j=2,4,6,8

39

156101115162021252630313536404145465015610111511151620212521252630313531353640414541454650with ﬁve irreducible components supported on the vertex sets V1, V3, V5, V7
and V9 (Figure 7). To compute a facial set with respect to ∆S2;S4;S6;S8, ac-
cording to Lemma 3.6 applied four times, we need to compute

G1,V1 := F∆S2|V1
G1,V5 := F∆S4;S6|V5

(πV1(I+)), G1,V3 := F∆S2;S4|V3
(πV5(I+)), G1,V7 := F∆S6;S8|V7
G1,V9 := F∆S8|V9

(πV9(I+)).

(πV3(I+)),
(πV7(I+)),

Then G1 :=(cid:84)

i π−1

(I+), and thus an inner ap-
proximation of Ft. As stated before, we do not need to compute G1 explicitly,
but we represent it by means of the G1,Vi.

Vi (G1,Vi) is equal to F∆S2;S4;S6;S8

We can improve the approximations by also considering the red separators
S1 = {6, . . . , 10}, S3 = {16, . . . , 20}, S5 = {26, . . . , 30}, S7 = {36, . . . , 40},

that separate

V0 = {1, . . . , 10}, V2 = {6, . . . , 20}, V4 = {16, . . . , 30},

V6 = {26, . . . , 40}, V8 = {36, . . . , 50}.

As explained in Section 3.2, we want to compute G(2)
1
Again, instead of computing G(2)
1 ), . . . , G(2)
smaller sets G(2)
1,V2 := πV2(G(2)
So the question is: Is it possible to compute G(2)
1,V0, G(2)
G1,V1, G1,V3, . . . , G1,V9, without computing G1 in between?

(G1).
1 directly, we need only compute the much
1,V8 := πV8(G(2)
1 ).
1,V8 from

1,V0 := πV0(G(2)

1,V2, . . . , G(2)

:= F∆S1;S3;S5;S7

1 ), G(2)

1,Vi is G1,Vj

It turns out that this is indeed possible: By Lemma 3.6, all we need
:= πVj (G1), j = i − 1, i + 1. For i = 0, since
to compute G(2)
V0 ⊂ V1, we can compute G1,V0 from πV1(G1) = G1,V1. For i = 2, 4, 6, 8, since
Vi ⊂ Vi−1 ∪ Vi+1, we can compute G1,Vi from πVi−1∪Vi+1(G1), which itself can
be obtained by “gluing” πVi−1(G1) = G1,Vi−1 and πVi+1(G1) = G1,Vi+1:

πVi−1∪Vi+1(G1) =

πVi−1∪Vi+1
Vi−1

πVi−1∪Vi+1
Vi+1

(G1,Vi+1),

(cid:16)
(cid:17)−1
V (cid:48)(cid:48) for V (cid:48)(cid:48) ⊆ V (cid:48) denotes the marginalization map from IV (cid:48) to IV (cid:48)(cid:48) and
πV (cid:48)
V (cid:48)(cid:48)

denotes the lifting from IV (cid:48)(cid:48) to IV (cid:48).

where πV (cid:48)

where

As explained in Section 3.2, we have to iterate this procedure: From G(2)
1
(G(cid:48)1) or, more precisely, we want to

we want to compute G(3)
1

:= F∆S2;S4;S6;S8

40

(cid:16)

(cid:17)−1

(G1,Vi−1) ∩(cid:16)

(cid:17)−1

1,Vi = πVi(G(3)

compute G(3)
ing at G(2)
, G(k)
Iterating this procedure, we obtain a sequence of sets G(k)
1,Vi
i and even j), which stabilizes after a ﬁnite number of steps. Let

1 ) for i = 1, 3, . . . , 9. Again, we do this without look-
1 directly by just using the information available through the G(3)
1,Vi.
1,Vj (with odd

not compute F1 explicitly, but we represent it in terms of the F1,Vi.

i=0 π−1

Vi (F1,Vi). Again, we do

(cid:91)

Our best inner approximation is then F1 = (cid:84)9

F1,Vi :=

G(k)
1,Vi

,

(a)

(b)

Figure 7: (a) The 5 × 10-grid with the blue separators completed. (b) The
ﬁve irreducible subcomplexes after completing the separators.

The process is visualized in Figure 8.
Let us now consider the outer approximation F2. We adapt Strategy 3 of
Section 3.3 and cover the graph with 5 × 3 grid subgraphs, since the facial
sets for such graphs can easily be computed. These subgrids are supported
on the same vertex subsets Vi, i = 1, . . . , 8 as used when computing F1. This
makes it possible to compare F1 and F2. For i = 1, 3, . . . , 8 we compute

41

156101115162021252630313536404145465015610111511151620212521252630313531353640414541454650Table 6: facial set approximation of 5 × 10 grid graph

sample size F2 (cid:54)= I F1 = F2
94.3%
82.5%
76.5%
81.2%
87.7%
91.5%
93.9%
99.9%

100.0%
100.0%
99.9%
99.6%
96.4%
92.9%
84.8%
44.7%

50
100
150
200
300
400
500
1000

(πVi(I+)). Our outer approximation is then F2 =(cid:84)

i π−1

Vi (F2,Vi).
F2,Vi = F∆|Vi
Again, we don’t compute F2 explicitly, but we only store F2,Vi in a computer
as a representation of F2. To compare the two approximations F1 and F2,
we need only compare their projections F1,Vi and F2,Vi pairwise, i = 1, . . . , 8.
We generated random data of varying sample size. For each ﬁxed sample
size, we generated 100 data samples. The simulation results are show in
Table 6. For each simulated sample, we compute the sets F1,Vi and F2,Vi as
described above. When computing F1,Vi, we found that 2 iterations actually
suﬃce. Then we checked whether F2 is a proper subset of I (second column),
and we checked whether F1 = F2 (third column). enough. Both for small
and large sample sizes, we found that the F1 = F2 quite often.
We also investigated what happens when the outer approximation is not
computed using all 3 × 5-subgrids, but only a cover of four 3 × 5-subgrids
and one 2 × 5-subgrid (as in Figure 6). In all our simulations, this easier
approximation gave the same result. The same is not true for the inner
approximation: When using just one of the two families of parallel separators
we obtain an inner approximation that is much too small.

References

Onureena Banerjee, Laurent El Ghaoui, and Alexandre d’Aspremont. Model
selection through sparse maximum likelihood estimation for multivariate
gaussian or binary data. The Journal of Machine Learning Research, 9:
485–516, 2008.

42

Figure 8: Flow chart

43

dataI+on5×10gridmarginalizeI5+:=πV5(I+)I3+=πV3(I+)I1+=πV1(I+)I7+=πV7(I+)I9+=πV9(I+)G1,V1G1,V3G1,V5G1,V7G1,V9marginalizeandgluemarginalizeandgluemarginalizeandgluemarginalizeandglueG1,V2G1,V4G1,V6G1,V8G1,V0marginalizeG01,V0G01,V2G01,V4G01,V6G01,V8...............LPLPLPLPLPLPLPLPLPLPO.E. Barndorﬀ-Nielsen. Information and Exponential Families. Wiley, Chich-

ester, ﬁrst edition, 1978.

Imre Csisz´ar and Frantiˇsek Mat´uˇs. Closures of exponential families. Annals

of Probability, 33:582–600, 2005.

A. Dobra, E.A. Erosheva, and S.E. Fienberg. Disclosure limitation methods
based on bounds for large contingency tables with application to disability
data. In Proceedings of conference on the new frontiers of statistical data
mining, pages 93–116, 2003.

Adrian Dobra and Alex Lenkoski. Copula Gaussian graphical models and
their application to modeling functional disability data. Ann. Appl. Stat.,
5(2A):969–993, 06 2011.

N. Eriksson, S. Fienberg, A. Rinaldo, and S. Sullivant. Polyhedral conditions
for the nonexistence of the MLE for hierarchical log-linear models. J.
Symbolic Comput., 41:222–233, 2006.

S. E. Fienberg and A. Rinaldo. Three centuries of categorical data analysis:
log-linear models and maximum likelihood estimation. J. of Statistical
PLanning and Inference, 137:3430–3445, 2007.

S. E. Fienberg and A. Rinaldo. Maximum likelihood estimation in log-linear

models. Annals of Statistics, 40:9961023, 2012.

Charles J. Geyer. Likelihood inference in exponential families and directions

of recession. Electron. J. Statist., 3:259–289, 2009.

S. J. Haberman. The Analysis of Frequency Data. Univ. Chicago Press,

Chicago, IL, 1974.

S.L. Lauritzen. Graphical Models. Oxford Science Publications, ﬁrst edition,

1996.

G. Letac and H. Massam. Bayes regularization and the geometry of discrete

hierarchical loglinear models. Annals of Statistics, 40:861–890, 2012.

H. Massam and N. Wang. A local approach to estimation in discrete loglinear

models. Preprint, 2015. arXiv:1504.05434.

44

Johannes Rauh, Thomas Kahle, and Nihat Ay. Support sets of exponen-
tial families and oriented matroids. International Journal of Approximate
Reasoning, 52(5):613–626, 2011.

Pradeep Ravikumar, Martin J Wainwright, John D Laﬀerty, et al. High-
dimensional ising model selection using (cid:96)1-regularized logistic regression.
The Annals of Statistics, 38(3):1287–1319, 2010.

G¨unter Ziegler. Lectures on Polytopes. Springer, second edition, 1998.

A Some proofs

A.1 Proof of Theorem 2.8

Theorem 2.8 goes back to Barndorﬀ-Nielsen (1978), who studies the closure
of much more general exponential families. The case of a discrete exponential
family is much easier.

The theorem follows from the following lemmas:

Lemma A.1. Let p ∈ EA. Then p ∈ EA,supp(p).
Lemma A.2. Let p ∈ EA. Then EA,supp(p) ⊆ EA.
Lemma A.3. Let p ∈ EA. Then supp(p) is facial.
Lemma A.4. If F is facial, then there exists p ∈ EA with supp(p) = F .

Indeed, Lemma A.1 shows that EA ⊆ (cid:83)
EA =(cid:83)

F EA,F , where the union is over
all support sets F . Lemma A.2 shows the converse containment, so that
F EA,F . It remains to see that a subset F ⊆ I is a support set if and

only if F is facial. This follows from Lemmas A.3 and A.4.

In the proofs of Lemmas A.1 to A.4, we need the following easy lemma

of which we omit the proof:
Lemma A.5. p ∈ EA if and only if log(p) ⊥ ker A.
Proof of Lemma A.1. Let p = limk→∞ pk, where pk ∈ EA, and let F =
supp(p). Then EA,F is the exponential family EAF , where AF consists of

45

the columns of A indexed by F . Any v ∈ ker AF can be extended by zeros
to v(cid:48) ∈ ker A. By Lemma A.5,
0 = (cid:104)log(pk), v(cid:48)(cid:105) =

log(pk(i))v(i) → (cid:104)log(p), v(cid:105).

(cid:88)

i∈F

Thus, log(p) ⊥ ker AF , which implies p ∈ EA,F .
Proof of Lemma A.2. Let p = limk→∞ pk, where pk ∈ EA, let F = supp(p),
and let q ∈ EA,F . Then there exists parameters θ with log(q(i))− log(p(i)) =
(cid:104)θ, fi(cid:105) for all i ∈ F . For any k, there exists a positive constant ck such that
qk := ckpk exp((cid:104)θ, A(cid:105)) ∈ EA. Then qk → q as k → ∞, and so q ∈ EA.
Proof of Lemma A.3. Let p = limk→∞ pk, where pk ∈ EA, and let F =
with x =(cid:80)
i∈supp(p) fi is an interior point of the face
FA(supp(p)). Then x =
corresponding to F , and thus there exist positive coeﬃcients λi > 0, i ∈ F ,

(cid:80)

| supp(p)|

1

i∈F λifi. The vector v = (vi, i ∈ I) deﬁned by
i ∈ supp(p),
i ∈ F \ supp(p),
i /∈ F,

| supp(p)|
−λi,
0,

 1

− λi,

vi =

satisﬁes Av = x− x = 0. By Lemma A.5, log(pk) ⊥ v for all k. In particular,

(cid:88)

log(pk(i))vi → (cid:88)

λi log(pk(i)) =

log(p(i))vi.

i∈F\supp(p)

i∈supp(p)

i∈supp(p)

On the other hand, note that each coeﬃcient λi for i ∈ F \ supp(p) on the
left hand side is positive, while log(pk(i)) → −∞ for i /∈ supp(p). This shows
that F \ supp(p) = ∅.
Proof of Lemma A.4. If F is facial, there exist g ∈ Rh and c ∈ R with
(cid:104)g, fi(cid:105) ≥ c for all i ∈ I and (cid:104)g, fi(cid:105) = c if and only if i ∈ F . Let θ(s) = −s · g.
Then

kF (θ(s)) + sc = log

exp(−s(cid:104)g, fi(cid:105) + sc) → log |F|,

(cid:88)

(cid:88)

and so

i∈I

log pθ(s)(i) = −s(cid:104)g, fi(cid:105) − kF (θ(s)) = (sc − s(cid:104)g, fi(cid:105)) − (kF (θ(s)) + sc)

(cid:40)− log |F|,

−∞,

→

if i ∈ F,
if i /∈ F,

as s → ∞. Thus, pθ(s) converges to the uniform distribution on F .

46

A.2 Proof of Theorem 2.9

By deﬁnition, any EMLE p∗ belongs to the closure of the model. Accord-
If supp(p) does not con-
ing to Theorem 2.8, the support of p∗ is facial.
tain supp(n), then the log-likelihood goes to minus inﬁnity, ˜l(p) = −∞, and
so p does not maximize the likelihood, Therefore, supp(p∗) is a facial set
containing supp(n). Thus, Ft ⊆ supp(p∗).
By Lemma A.1, p∗ belongs to E∆,supp(p∗), which is parametrized by a
vector θ, see Theorem 2.8. On E∆,supp(p∗), the log-likelihood function in terms

of this parameter θ is

lF (θ) =

θjtj − N kF (θ).

(cid:88)

j∈J

lF is strictly concave, and so it has a unique maximum. The critical equations
are

Ap∗ =

t
N

,

proving the ﬁrst property. Note that these equations are independent of the
parameters and the support of p∗. We now show that any solution to these
equations is supported on the same face of P as t
N .
Let p be a probability distribution on I such that supp(p) does not con-
tain Ft. This means that there is a linear inequality (cid:104)g, t(cid:105) ≥ c that is valid
on P and such that

• (cid:104)g, fi(cid:105) = c for all i ∈ Ft;
• (cid:104)g, fi(cid:105) > c for some i ∈ supp(p).

Then

(cid:104)g, Ap(cid:105) =

(cid:88)

(cid:104)g, fi(cid:105)p(i) > c =

i

(cid:88)

i

1
N

n(i)(cid:104)g, fi(cid:105) = (cid:104)g,

(cid:105),

t
N

which implies Ap (cid:54)= t
supp(p∗) = Ft.

N . This shows supp(p∗) ⊆ Ft and ﬁnishes the proof of

We have now shown the two properties, and it remains to argue that the
EMLE is unique. But this follows from the fact that supp(p∗) is equal to Ft,
and lF is strictly convex, such that the likelihood has a unique maximizer
on E∆,Ft.

47

B Example: Two binary random variables
Consider two binary random variables, and let ∆ = {∅,{1},{2},{1, 2}}. The
hierarchical model E∆ is the saturated model ; that is, it contains all possible
probability distributions with full support. Then

 θ00

θ01
θ10
θ11

1
1
1
1

1
1
0
0

1
0
1
0

f00(cid:122)(cid:125)(cid:124)(cid:123)

f01(cid:122)(cid:125)(cid:124)(cid:123)

f10(cid:122)(cid:125)(cid:124)(cid:123)

f11(cid:122)(cid:125)(cid:124)(cid:123)



1
0
0
0

˜A =

The marginal polytope is a 3-simplex (a tetrahedron) with facets

F00 : 1 − t01 − t10 + t11 ≥ 0, F01 : t01 − t11 ≥ 0,

F10 : t10 − t11 ≥ 0, F11 : t11 ≥ 0.

Each of the corresponding facets contains three columns of ˜A. In fact, the
facet Fi in the above list does not contain the column fi of A.

The EMLE of the saturated model is just the empirical distribution; that
N n. Suppose that t lies on the facet F00 (i.e. n = (0, n01, n10, n11)
If pθ(s) → p∗, then pθ(s)(00) → 0, while all

is, p∗ = 1
with n(01), n(10), n(11) > 0).
other probabilities converge to a non-zero value. It follows that

00 = log pθ(s)(00) → −∞,
θ(s)
→ +∞,
θ(s)
01 = log

pθ(s)(01)
pθ(s)(00)
pθ(s)(10)
pθ(s)(00)
pθ(s)(11)pθ(s)(00)
pθ(s)(01)pθ(s)(10)

→ +∞,

θ(s)
10 = log

θ(s)
11 = log

→ −∞.

On the other hand, θ(s)
do θ(s)

01 + θ(s)

00 = log pθ(s)(01) converges to a ﬁnite value, as

00 = log pθ(s)(10) and θ(s)

10 + θ(s)
Proceeding similarly for the other facets, one can show for the limits

01 = log pθ(s)(11)/pθ(s)(10).

11 + θ(s)

θij := lims→∞ θ(s)
ij :

48

θ00

θ01

θ10

θ11

F00 −∞ +∞ +∞ −∞ θ(s)
F01 ﬁnite −∞ ﬁnite +∞
F10 ﬁnite ﬁnite −∞ +∞
F11 ﬁnite ﬁnite ﬁnite −∞

ﬁnite parameter combinations:
01 + θ(s)
00 , θ(s)
11 + θ(s)
10 + θ(s)
θ(s)
00 , θ(s)
10 , θ(s)
01 , θ(s)
θ(s)
00 , θ(s)
θ(s)
00 , θ(s)

00 , θ(s)
01 + θ(s)
10 + θ(s)
10 , θ(s)

01

11

11

01

Each line of the last column contains three combinations of the parameters
θ(s)
that converge to a ﬁnite value. Any other parameter combination that
i
converges is a linear combination of these three. This can be seen by using
the coordinates µi introduced in Section 4.2 and applying Lemma 4.1. For
example, on the facet F01, consider the parameters

µ10 = log p(10)/p(00) = θ10,

µ11 = log p(11)/p(00) = θ10 + θ01 + θ11,

µ01 = log p(01)/p(00) = θ01.

Then µ10 and µ11 are identiﬁable parameters on EF01, and µ01 diverges close
to F01. By Lemma 4.1, the linear combinations that are well-deﬁned are
µ10 = θ10 and µ11 = θ10 + (θ01 + θ11). The above table also lists θ00, which is
not a linear combination of those but that is ﬁne because it is not free.

We obtain similar results for the facets F01 and F11. The results are

summarized in the following table:

µ10

µ11
µ01
facet
F01 −∞ ﬁnite ﬁnite
ﬁnite −∞ ﬁnite
F10
ﬁnite ﬁnite −∞
F11

Of course, by deﬁnition of the µis, we cannot consider the facet F00 where
n(00) = 0. To study F00, we have to choose another zero cell and redeﬁne
the parameters µi.

The situation is more complicated for faces smaller than facets, because
sending a single parameter to plus or minus inﬁnity can be enough to send
the distribution to a face F of higher codimension, as we will see below. The
remaining parameters then determine the position within E∆,F . Thus, in this
case there are more remaining parameters than the dimension of E∆,F .

For example, the data vector n = (n00, 0, n10, 0) (with n00, n10 > 0) lies

49

,

n00
N
→ −∞,

→ log

n10
n00

.

θ(s)
01 = log

θ(s)
10 = log

11 = log

pθ(s)(01)
pθ(s)(00)
pθ(s)(10)
pθ(s)(00)
θ(s) (11)p
p
θ(s) (01)p
p

on the face F = F01 ∩ F11 of codimension two. If pθ(s) → p∗, then

00 = log pθ(s)(00) → log
θ(s)

θ(s) (00)
θ(s) (10) is not determined. The only
01 goes to −∞, since

However, the limit of θ(s)
constraint is that θ(s)
00 + θ(s)
p

11 cannot go to +∞ faster than θ(s)
01 + θ(s)
11 ) has to converge to zero.

10 + θ(s)

θ(s)
11

= exp(θ(s)
With the same data vector n = (n00, 0, n10, 0), suppose we use a numerical
algorithm to optimize the likelihood function by optimizing the parameters θj
in turn. To be precise, we order the parameters θj in some way. For simplicity,
say that the parameters are θ1, θ2, . . . , θh. Then we let
j−1 , y, θ(k)

j+1, . . . , θ(k)
h )

, . . . , θ(k+1)

l(θ(k+1)

θ(k+1)
j

1

= arg max
y∈R

(this is called the non-linear Gauss-Seidel method ). Let us choose the order-
ing θ01, θ10, θ11 (note that θ00 = −k(θ) is not a free parameter). We start at
θ(0)
01 = θ(0)
11 = 0. In the ﬁrst step, we only look at θ01. That is, we want
to solve

10 = θ(0)

0 =

∂

∂θ01

l(θ) = −

exp(θ(1)

01 ) + exp(θ(1)

01 + θ(0)
10 ) + exp(θ(1)

1 + exp(θ(1)

01 ) + exp(θ(0)

10 + θ(0)
11 )
01 + θ(0)
10 + θ(0)
11 )
= − 2 exp(θ(1)
01 )
1 + 2 exp(θ(1)
01 )

.

(21)

Clearly, the derivative is negative for any ﬁnite value of θ(1)
01 , and thus the
critical equation has no ﬁnite solution. If we try to solve this equation nu-
merically, we will ﬁnd that θ(1)
01 will be a large negative number. Next, we
look at θ10. We ﬁx the other variables and try to solve

0 =

∂

∂θ10

l(θ) =

−

n10
N

exp(θ(1)

10 ) + exp(θ(1)

1 + exp(θ(1)

01 ) + exp(θ(1)

01 + θ(1)
10 ) + exp(θ(1)
≈ n10
N

10 + θ(0)
11 )
01 + θ(1)
10 + θ(0)
11 )
− exp(θ(1)
10 )
1 + exp(θ(1)
10 )

,

50

where we have used that θ(1)
01
always has a unique solution

is a large negative number. This equation

10 ≈ log
θ(1)

n10

N − n10

.

Finally, we look at θ11. We have to solve
exp(θ(1)

∂

l(θ) = −

0 =

∂θ11

1 + exp(θ(1)

01 ) + exp(θ(1)

01 + θ(1)

10 + θ(1)
11 )
01 + θ(1)
10 ) + exp(θ(1)

10 + θ(1)
11 )

≈ 0.

Actually, this equation again has no solution, and the numerical solution for
θ(1)
11 should be close to numerical minus inﬁnity. However, since θ(1)
01 is already
close to −∞, the equation is already approximately satisﬁed. Thus, there
is no need to change θ11. In simulations, we observed that usually θ(1)
11 will
be negative, but not as negative as θ(1)
01 . In theory, we would have to iterate
and now optimize θ01 again. But the values will not change much, since the
critical equations are already satisﬁed to a high numerical precision after one
iteration.

It is not diﬃcult to see that the result is diﬀerent if we change the order
11 will in any case be a

of the variables. If θ11 is optimized before θ01, then θ1
large negative number.

For general data, the derivative of with respect to θ01 (equation (21))

takes the form

∂

∂θ01

l(θ) =

−

t01
N

exp(θ(1)

01 ) + exp(θ(1)

01 + θ(0)
10 ) + exp(θ(1)

10 + θ(0)
11 )
10 + θ(0)
01 + θ(0)
11 )

.

1 + exp(θ(1)

01 ) + exp(θ(0)

Setting this derivative to zero and solving for θ(1)
in θ(1)

01 with symbolic solution

01 leads to a linear equation

θ(1)
01 = log

1 + exp(θ(0)
10 )

1 + exp(θ(0)

10 + θ(0)
11 )

t01
N

1 − t01

N

.

In fact, for any hierarchical model, the likelihood equation is linear in any
single parameter θj, as long as all other parameters are kept ﬁxed (more
generally this is true when the design matrix A is a 0-1-matrix).
Instead
of optimizing the likelihood numerically with respect to one parameter, it is
possible to use these symbolic solutions. This leads to the Iterative Propor-
tional Fitting Procedure (IPFP). In our example, the IPFP would lead to
a division by zero right in the ﬁrst step, indicating that the MLE does not
exist.

51

C A Linear Programming algorithm to com-

pute facial set

Denote A as the design matrix, A+ as the sub-matrix with columns indexed
by the positive cells and A0 as the sub-matrix indexed by the empty cells.

Lemma C.1. Solution g∗ of the non-linear problem

max

z = (cid:107)Ag(cid:107)0
A0g (cid:62) 0

s.t. A+g = 0

(22)

is a perpendicular vector to the smallest face containing t. The corresponding
facial set is Ft = I \ supp(Ag∗).

The optimization problem (22) is highly non-linear and non-convex: it can
be solved by repeatedly solving the associated (cid:96)1-norm optimization problem:

max

z = (cid:107)A0g(cid:107)1
s.t. A+g = 0
A0g ≥ 0
A0g ≤ 1

(23)

Problem (23) is a linear programming problem: we can solve it repeatedly
until we get the smallest facial set Ft. The process is as follows:

The algorithm is introduced in the supplementary material to (Fienberg
and Rinaldo, 2012), where it is also proved that it outputs the correct result.

D Parametrizations adapted to facial sets

Let us brieﬂy discuss how to remedy problems 1. to 3. from the beginning of
Section 4.2. The idea to remedy 1. and 2. is to deﬁne parameters µi, i ∈ L,
of EA, such that a subset Lt ⊆ L of the parameters parametrizes EFt,A in a
j,i, j ∈ L, i ∈ I) the design matrix of EA
consistent way. Denote by Aµ = (aµ
corresponding to the new parameters µ. Then the necessary conditions are:
(∗) Let Aµ
j,i, j ∈ Lt, i ∈ Ft) be the submatrix of Aµ with rows
Lt,Ft the
Lt,Ft is equal

indexed by Lt and columns indexed by Lt, and denote by ˜Aµ
same matrix with an additional row of ones. The rank of ˜Aµ
to |Lt| + 1, the number of its rows (and thus, Aµ

Lt,Ft has rank |Lt|).

Lt,Ft := (aµ

52

Algorithm 1 Face computation by Linear programming method
Require: Design matrix A and positive cell index I+

INITIALIZE A+ = A(I+, :), A0 = A \ A+
Solve problem 23, get the solution g∗ and the corresponding maximum z∗
while A0 (cid:54)= ∅ and z∗ (cid:54)= 0 do

Let matrix B be the submatrix of A0, by taking columns of A0 which
satisfy (cid:104)fi, g∗(cid:105) > 0, update A0 = A0 \ B,
Solve problem 23, get the solution g∗ and the corresponding maximum
z∗

end while
if A0 = ∅ then

Ft = I+

end if
if Z∗ = 0 then

Ft = I+ ∪ {i|i is the index of A0}

end if

j,i = 0 for all i ∈ Ft and j ∈ L \ Lt.

(∗∗) aµ
Lt,Ft is the design matrix of EA,Ft, since the pa-
In fact, (∗∗) implies that Aµ
rameters µi with i /∈ Lt do not play a role in the parametrization µ (cid:55)→ pFt,µ.
Moreover, (∗) implies that the parametrization µ (cid:55)→ pFt,µ is identiﬁable. In
this sense, we have remedied problem 1. from the beginning of the section.
Lt,Ft has full row rank, it has a right inverse matrix ˜C,
˜C = I|Lt|+1 equals the identity matrix of size |Lt| + 1. Recall

Since the matrix ˜Aµ

Lt,Ft

such that ˜Aµ
that

log pFt,µ(i) = (cid:104)µt, f µ
log pµ(i) = (cid:104)˜µt, f µ

i (cid:105) − kF (µ),
i (cid:105) − k(µ),
for any parameter vector µ and all i ∈ Ft. Since f µ
i are the columns of Aµ
i corresponding to L \ Lt vanish by (∗∗), we
and since the components of f µ
may apply the matrix C obtained from ˜C by dropping the row corresponding
to kF or k and obtain

(log pµ)C = µLt

(24)
When pµ(s) is a sequence in EA with limit pµ in EFt,A, then (24) shows that
i → µi for i ∈ Lt. In this sense, we have remedied problem 2.
µ(s)

(log pFt,µ)C = µL.

and

53

Finally, we solve problem 3. Suppose that we have chosen parameters
µL as in Section 4.2, and let AµL be the design matrix with respect to these
parameters. Then (AµL)j,i = 0 if i ∈ Ft and j /∈ Lt. Moreover, for j ∈ Lt,
the jth column of AµL has a single non-vanishing entry (equal to one) at
position j. Suppose that Ft corresponds to a face Ft of codimension c. Then
there are c facets of P whose intersection is Ft. Thus, following the notation
introduced in Remark 2.6, there exist c inequalities
(cid:104)˜gc, ˜x(cid:105) ≥ 0

(cid:104)˜g1, ˜x(cid:105) ≥ 0,

(25)

. . . ,

that together deﬁne Ft. In this case, the vectors ˜g1, . . . , ˜gc are linearly inde-
pendent and satisfy (cid:104)˜gj, ˜fi(cid:105) = 0 (thus, they are a basis of the kernel of ( ˜AµL
Ft )t).
It follows that the kth component of gj, denoted by gj,k, vanishes if k ∈ Lt;
that is, the inequalities (25) do not involve the variables corresponding to Lt.
Let G be the square matrix, indexed by L\ Lt with entries gj,k, j, k ∈ L\ Lt.
Then the square matrix

(cid:18)1 0

(cid:19)

0 G

˜G =

is invertible. We claim that the parameters λ = ˜G−1µL are what we are
looking for.
any j /∈ Lt,

The design matrix with respect to the parameters λ is Aλ = ˜GAµL. For

Aλ

j,i = 0,

if i ∈ Ft,

and Aλ

j,i = (cid:104)˜gj, ˜fi(cid:105) ≥ 0,

if i /∈ Ft.

This implies the following properties:

1. If all parameters λj with j /∈ Lt are sent to −∞, then pλ tends towards

a limit distribution with support Ft.

2. The coeﬃcient of λj in any log-probability is non-negative, so there is

no cancellation of ±∞.

So far, we only used the fact that the vectors ˜gj deﬁne valid inequalities
for the face Ft. Suppose that we choose ˜gj in such a way that each inequality
(cid:104)˜gj, ˜x(cid:105) ≥ 0 deﬁnes a facet. The intersection of less than c facets is a face that
strictly contains Ft. This implies that for each j, there exists ij ∈ I \ Ft such
that fij satisﬁes

(cid:104)˜gj, ˜fij(cid:105) > 0,

and

(cid:104)˜gj(cid:48), ˜fij(cid:105) = 0 for all j(cid:48) (cid:54)= j.

54

This implies

Aλ

j,ij

> 0,

and Aλ

j(cid:48), ij = 0 for all j(cid:48) (cid:54)= j.

This implies the following:

3. If λ(s)

j are sequences of parameters such that pλ(s) tends towards a limit

distribution with support Ft, then λ(s)

j → −∞ for all j /∈ Lt.

It is not diﬃcult to see that, conversely, any parametrization that satisﬁes

these three properties comes from facets deﬁning the face Ft.

55

