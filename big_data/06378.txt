6
1
0
2

 
r
a

 

M
1
2

 
 
]

.

R
P
h
t
a
m

[
 
 

1
v
8
7
3
6
0

.

3
0
6
1
:
v
i
X
r
a

Conditional Monte Carlo: A Change-of-Variables Approach

Guiyun Feng

Department of Industrial & Systems Engineering, University of Minnesota

111 Church Street S.E., Minneapolis, MN 55455, USA

Guangwu Liu

Department of Management Sciences, City University of Hong Kong

Tat Chee Avenue, Kowloon, Hong Kong

March 22, 2016

Abstract

Conditional Monte Carlo (CMC) has been widely used for sensitivity estimation with dis-
continuous integrands as a standard simulation technique. A major limitation of using CMC in
this context is that ﬁnding conditioning variables to ensure continuity and tractability of the
resulting conditional expectation is often problem dependent and may be diﬃcult. In this paper,
we attempt to circumvent this diﬃculty by proposing a change-of-variables approach to CMC,
leading to eﬃcient sensitivity estimators under mild conditions that are satisﬁed by a wide class
of discontinuous integrands. These estimators do not rely on the structure of the simulation
models and are less problem dependent. The value of the proposed approach is exempliﬁed
through applications in sensitivity estimation for ﬁnancial options and gradient estimation of
chance-constrained optimization problems.

1

Introduction

Conditional Monte Carlo (CMC) is a standard simulation technique that has been widely discussed

by many simulation textbooks; see, e.g., Law and Kelton (2000) and Asmussen and Glynn (2007).

When estimating the expectation of a random performance, the basic idea of CMC is to use condi-

tional expectation of the performance, rather than the random performance itself, as an estimator,

where appropriate conditioning variables are chosen to obtain the conditional expectation. It is well

known that the CMC estimator has a smaller variance, as guaranteed by the law of total variance.

In this paper, our main interest is focused on the use of CMC in sensitivity estimation of an

expectation, another context in which CMC plays an important role. In particular, we consider a

setting that the integrand of the expectation is discontinuous, under which sensitivity estimation

is challenging, and has received a signiﬁcant amount of attention in the simulation community in

recent years. In this setting, CMC works in conjunction with inﬁnitesimal perturbation analysis

(IPA, also known as the pathwise method) that suggests interchanging the order of diﬀerentia-

tion and expectation. Typically, IPA does not work when the integrand is discontinuous, because

1

the interchange is not valid due to discontinuity. To ﬁx this issue, CMC suggests ﬁnding appro-

priate conditioning variables and then applying IPA on the conditional expectation that is often

continuous. The intuition behind is that taking conditional expectation can often “integrate out”

discontinuity and smooth the integrand. This idea was ﬁrst proposed by Gong and Ho (1987) and

Suri and Zazanis (1988), and is also referred to as smoothed perturbation analysis; see Fu and Hu

(1997) for a monograph on detailed treatments, and Wang at al. (2009) and Fu et al. (2009) for

some of its recent applications.

A major limitation of CMC for sensitivity estimation is how to ﬁnd conditioning variables such

that the conditional expectation is smooth and easily computable. Finding such conditioning vari-

ables is problem dependent and could be diﬃcult for some cases. For instance, when discontinuity

comes from an indicator function that equals 1 if the maximum of a random vector is smaller than

a threshold and 0 otherwise, it may be diﬃcult to ﬁnd conditioning variables to ensure continuity of

the resulting conditional expectation. This limitation motivates us to reexamine CMC from other

perspectives, aiming to circumvent the diﬃculty on ﬁnding conditioning variables.

In this paper, we study CMC from a change-of-variables perspective, and we call it a change-

of-variables approach. Intuitively, it proceeds by constructing a one-to-one mapping and applying

a change of variables, followed by taking iterated integrations. The approach is appealing when

the discontinuous integrand involves an indicator function. In this setting, we want to estimate the

sensitivities of the expectation of

l(X) · 1{h(X)≤ξ},

(1)

for continuous functions l(·) and h(·), where X is a random vector that captures the randomness
of the simulation, and ξ is a given constant. A one-to-one mapping X 7→ (u(X), h(X)) can be
constructed with a vector function u, and a change-of-variables formula represents the expectation

of (1) as a double integral taken over the supports of u(X) and h(X), respectively. Then integrating
along the dimension of h(X) over (−∞, ξ) removes the indicator and produces a smooth integrand,
which enables the use of IPA for sensitivity estimation.

Though the intuition of the change-of-variables approach is straightforward, its theoretical jus-

tiﬁcation under a general setting is not trivial, because the domain and image sets of the mapping
may have diﬀerent Euclidean dimensions. For instance, when X , (X1, . . . , Xm) and h(X) =
max(X1, . . . , Xm), a useful one-to-one mapping that we may construct is X 7→ (X/h(X), h(X)).
While the domain of this mapping is a subset of Rm, its image is a subset of Rm+1, which is indeed
an m-dimensional manifold in Rm+1. Change-of-variables formulas for such cases involve an exten-

sion of Lebesgue measure, and more generally, geometric measure theory; see Federer (1996) for a

monograph. In this paper, we provide theoretical underpinnings of the approach under a general

setting, and discuss how it can be applied to develop sensitivity estimators.

Sensitivity estimation ﬁnds applications in a wide range of areas in operations research. For

2

instance, it can be used in simulation optimization to estimate gradients that serve as key inputs

to many gradient-based optimization algorithms.

In ﬁnancial applications, it estimates hedging

parameters such as delta and gamma, which play important roles in risk management of ﬁnancial

securities. There has been a vast literature on sensitivity estimation, and various methods have

been proposed, traditional ones including ﬁnite-diﬀerence approximations, IPA (see, e.g., Ho and

Cao 1983 and Broadie and Glasserman 1996), the likelihood ratio method (see, e.g., Glynn 1987 and

L’Ecuyer 1990), the weak derivative method (see, e.g., Pﬂug 1988 and Pﬂug and Weisshaupt 2005),

and the Malliavin calculus method (see, e.g., Bernis et al. 2003 and Chen and Glasserman 2007).

In recent years, sensitivity estimation for expectations with discontinuous integrands has received

a signiﬁcant amount of attention among simulation researchers and several methods have been

proposed. Lyuu and Teng (2011) showed that the sensitivity can be written as an integral taken over

an appropriate subset, and suggested using importance sampling to estimate the integral. Liu and

Hong (2011) showed that the sensitivity is a summation of two terms with the latter one involving

a conditional expectation and a density, and proposed a kernel smoothing method to estimate the

second term. Wang et al. (2012) proposed the so-called SLRIPA method that moves the parameter

of interest out of the indicator function to smooth the integrand and enables the use of IPA. It

uniﬁes the likelihood ratio method and the IPA method in certain sense. Chan and Joshi (2013)

suggested a novel bumping on sample paths in a way that discontinuous points are eliminated.

Essentially their approach relies on an appropriate change of variables on the sample paths. Along

the line of Liu and Hong (2011), Tong and Liu (2016) proposed an importance sampling method

to estimate conditional expectations, leading to unbiased estimators of the sensitivities.

As a remark, we would like to point out that the change-of-variables idea is not new for Monte

Carlo simulation. Dating back to 1950s, a change-of-variables argument was discussed by Ham-

mersley (1956) in the context of computing a conditional expectation at a ﬁxed point, and further

elaborated by Wendel (1957) from a group-theoretic aspect. This line of research, however, received

little attention afterwards and its theoretical underpinnings were underdeveloped.

To summarize, we make the following contributions in this paper.

• We develop a change-of-variables framework of CMC for general integrands and provide the-
oretical underpinnings. This framework is adapted to Hausdorﬀ measure that oﬀers great

ﬂexibility in construction of CMC estimators.

• We propose sensitivity estimators for expectations with discontinuous integrands of the form
(1). These estimators require only the probability density function of X as an input and

certain smoothness conditions on the function h. They are less problem dependent, and work

for a wide class of h’s.

• We study two applications, including sensitivity estimation for ﬁnancial options with dis-

3

continuous payoﬀs and gradient estimation of chance-constrained optimization problems, to

illustrate how the proposed approach may lead to new and eﬃcient estimators.

The remainder of the paper is organized as follows. We formulate the problem in Section 2,

and introduce the main idea of the change-of-variables approach in Section 3. Section 4 shows how

to use the change-of-variables approach for sensitivity estimation with discontinuous integrands.

Two applications with numerical examples are considered in Section 5 to exemplify the value of

the proposed approach, followed by concluding remarks in Section 6. Proofs of the main results are

provided in the appendix, while lengthy details of the estimators used in numerical examples are

put in an online supplement.

Notation. Throughout the paper, we let boldface and italic letters denote vectors and vector

elements respectively, and upper-case letters denote random counterparts of the lower-case ones.
For a mapping u, u{A} denotes the set of images of a set A in the domain, while u(x) denotes the
image of an element x ∈ A.

2 Problem Formulation

Consider a sensitivity estimation problem for an expectation with a discontinuous integrand of the

form

l(X) · 1{h(X)≤ξ},

where l and h are continuous functions, ξ is a given constant, and X = (X1, . . . , Xm) is a random

vector that captures the randomness of the simulation. This form of integrands has received a

signiﬁcant amount of attention in the simulation society in recent years; see, e.g., Lyuu and Teng

(2011), Hong and Liu (2011), Wang et al. (2012), Chan and Joshi (2013), and Tong and Liu (2016).

Throughout the paper, we assume that X has a known density function.

Assumption 1. The random vector X has a density function f (x) on a support Ω ⊂ Rm.

This assumption can be further relaxed to a requirement of a conditional density of X given a

random vector G; see Section B.1.2 of the online supplement for such an example.

Suppose the sensitivity of interest is taken with respect to (w.r.t.) θ, a parameter on which X
may depend. Without loss of generality we assume that θ is one-dimensional and θ ∈ Θ, where
If θ is multidimensional, one may treat each dimension as a one-dimensional
Θ is an open set.

parameter while ﬁxing other dimensions as constants.

Explicitly accounting for its dependence on θ, the quantity we want to estimate is written as

γ′(θ) , d
dθ

E(cid:2)l(X(θ)) · 1{h(X(θ))≤ξ}(cid:3) .

4

(2)

One of the methods for estimating γ′(θ) is to explore the possibility of interchanging the order of

diﬀerentiation and expectation, which is, unfortunately, invalid for the discontinuous integrand in

(2). As a remedy, CMC suggests choosing conditioning variables Y, leading to

γ′(θ) =

d
dθ

E [w(Y; θ)] ,

(3)

where

w(Y; θ) , E(cid:2) l(X(θ)) · 1{h(X(θ))≤ξ}(cid:12)(cid:12) Y(cid:3) .

It is then expected that the interchange of diﬀerentiation and expectation on (3) may be valid for
appropriate Y, and if so, γ′(θ) can be estimated by a sample mean of dw(Y; θ)/dθ, provided that

it is easily computable.

A major limitation of CMC is on how to ﬁnd appropriate conditioning variables Y. For some

cases, ﬁnding Y to ensure both diﬀerentiability of w(Y; θ) and tractability of dw(Y; θ)/dθ is diﬃ-

cult, e.g., when h(X) = max(X1, . . . , Xm).

In this paper, we study CMC from a change-of-variables perspective, aiming to circumvent the

diﬃculty on choosing Y. Before we proceed further, we decompose the problem by converting the

sensitivity w.r.t. any parameter θ to a sensitivity w.r.t. ξ. In particular, Liu and Hong (2011,
Theorem 1) showed that under mild regularity conditions, γ′(θ) can be represented as

γ′(θ) = E(cid:2)∂θl(X(θ)) · 1{h(X(θ))≤ξ}(cid:3) − ∂ξE(cid:2)l(X(θ))∂θh(X(θ)) · 1{h(X(θ))≤ξ}(cid:3) ,

where ∂ denotes the diﬀerentiation operator, and ∂θl(X(θ)) and ∂θh(X(θ)) are pathwise derivatives
that are often readily computable from simulation.

The result in (4) shows that the sensitivity w.r.t. any θ can be related to the sensitivity w.r.t.

ξ. Note that the ﬁrst term on the right-hand-side can be straightforwardly estimated by a sample
mean. The problem of estimating γ′(θ) is then reduced to how to estimate the second term.

Without loss of generality, the remainder of this paper is focused on the reduced problem in

which we want to estimate

α′(ξ) ,

d
dξ

E(cid:2)g(X) · 1{h(X)≤ξ}(cid:3) ,

for some function g, where the dependence of X on θ is suppressed when there is no confusion.1

(4)

(5)

3 A Change-of-Variables Approach

This section introduces the main idea of the change-of-variables approach. Section 3.1 provides

intuitions and motivating examples. Section 3.2 lays down the framework of the approach. Discus-

sion of the approach in sensitivity estimation for integrands of the form (1) will be presented in a

following section.

1There is no smoothness requirement on the function g which is allowed to be discontinuous.

5

The change-of-variables argument applies to a general integrand, beyond the form speciﬁed in

(1). To avoid heavy notations, in this section we work with a general integrand p(X), and discuss

the change-of-variables approach for E [p(X)].

3.1

Intuitions and Motivating Examples

Consider the expectation E[p(X)]. Recall that CMC yields E[p(X)] = E [w(Y)], where w(y) =
E [p(X)|Y = y]. In terminology, we refer to CMC based on conditioning variables as conventional
CMC.

We highlight the intuition of our approach by recovering the above conventional CMC estimator

from a change-of-variables perspective, where we ignore mathematical rigor at this stage. The

change-of-variables approach proceeds by constructing a one-to-one mapping

Let f (x) and ˜f (y, z) denote the density functions of X and (Y, Z), respectively. Then,

u : X 7→ (Y, Z).

If we set

it can be easily veriﬁed that

E[p(X)] =Z p(x)f (x) dx =Z Z p(u−1(y, z)) ˜f (y, z) dz dy.
w(y) = Z p(u−1(y, z)) ˜f (y, z) dz(cid:30)Z ˜f (y, z) dz,
E[p(X)] =Z w(y)Z ˜f (y, z) dz dy = E [w(Y)] ,

(6)

(7)

where the second equality follows from the fact thatR ˜f (y, z) dz is the marginal density of Y.

In short, the conventional CMC estimator can be recovered by a change-of-variables argument in

(6) in conjunction with an application of Fubini’s Theorem in (7). Compared to ﬁnding conditioning

variables Y, the change-of-variables perspective oﬀers more ﬂexibility in deriving CMC estimators.

To see this, we consider two examples, where X = (X1, X2) with X1 and X2 following independent

exponential distributions with mean 1, and let Fe and fe denote the c.d.f. and p.d.f. of the

exponential distribution, respectively.

Example 1. p(X) = X1 · 1{X1+X2≤ξ} for a given constant ξ > 0. Conventional CMC may suggest
choosing X1 as a conditioning variable. Then,

E[p(X)] = E(cid:0)E(cid:2)X1 · 1{X1+X2≤ξ}|X1(cid:3)(cid:1) = E [X1Fe(ξ − X1)] .

This estimator can be recovered from a change-of-variables perspective using a one-to-one mapping:
(x1, x2) 7→ (x1, x1 + x2) , (y1, y2). The Jacobian (the absolute value of the determinant of the

6

derivative matrix) of the mapping is 1, and the density of (Y1, Y2) , (X1, X1 + X2) is fe(y1)fe(y2 −
y1). Then,

where

E[p(X)] =Z Z 1{y2≤ξ}y1fe(y1)fe(y2 − y1) dy2 dy1 = E [w(Y1)] ,
w(y1) = R 1{y2≤ξ}y1fe(y1)fe(y2 − y1) dy2

= y1Fe(ξ − y1).

R fe(y1)fe(y2 − y1) dy2

Interestingly, it is also possible to construct a one-to-one mapping which leads to a new estimator

that cannot be derived by conventional CMC. Consider the one-to-one mapping:

(x1, x2) 7→ (y1, y2, z) ,(cid:18) x1

x1 + x2

,

x2

x1 + x2

, x1 + x2(cid:19) .

2

x

1

0
0

z

1

0
1

y2

x1

1

0

0

1

y1

Figure 1: Illustration of the mapping (x1, x2) 7→ (y1, y2, z) , (cid:16) x1

left and right panels show the domain and image sets of the mapping, respectively.

, x1 + x2(cid:17), where the

,

x2

x1+x2

x1+x2

As shown in Figure 1, image set of the mapping is {(y1, y2, z) : y1 + y2 = 1, y1 > 0, y2 >
It can be checked that the mapping is indeed one-to-one, and the Jacobian of the

0, z > 0}.
mapping is √2/z. Deﬁne (Y1, Y2, Z) ,(cid:16) X1
is ˜f (y1, y2, z) = zfe(y1z)fe(y2z)/√2, and

X1+X2

, X2
X1+X2

, X1 + X2(cid:17), then the density of (Y1, Y2, Z)

E[p(X)] =Z Z Z 1{z≤ξ}y1z ˜f (y1, y2, z) dz dy1 dy2 = E [w(Y1, Y2)] ,

(8)

where

w(y1, y2) = R 1{z≤ξ}y1z ˜f (y1, y2, z) dz

R ˜f (y1, y2, z) dz

=

2y1

y1 + y2 − e−(y1+y2)ξ(cid:18) 2y1

y1 + y2 − 2y1ξ − y1(y1 + y2)ξ(cid:19) .

(9)

Example 2. p(X) = X1 · 1{max(X1,X2)≤ξ}. Conventional CMC may suggest conditioning on X1,
leading to

E[p(X)] = E(cid:2)Fe(ξ)X11{X1≤ξ}(cid:3) .

7

Similar to the argument in Example 1, this estimator follows from a change of variables with an

identity mapping.

If we construct another one-to-one mapping:

(x1, x2) 7→ (y1, y2, z) ,(cid:18)

x1

max(x1, x2)

,

x2

max(x1, x2)

, max(x1, x2)(cid:19) .

Then by a similar argument as in Example 1,

E[p(X)] = E [w(Y1, Y2)] ,

where (Y1, Y2) ,(cid:16)

X1

max(X1,X2) ,

X2

max(X1,X2)(cid:17), and w(y1, y2) has exactly the same form as in (9).

These examples illustrate that a change-of-variables argument oﬀers more ﬂexibility for CMC

and may lead to new estimators. However, it should be pointed out that the integral in (8) is not
well deﬁned in Lebesgue sense. Note that the integral in (8) is taken over a set {(y1, y2, z)|y1 >
0, y2 > 0, z > 0, y1 + y2 = 1} which is a hyperplane in R3. This set has Lebesgue measure 0 and the
integral is thus not well deﬁned in Lebesgue sense. Rigorous deﬁnition of integrals for such cases

will be provided in Section 3.2.

3.2 A Change-of-Variables Framework of CMC

We lay down a mathematical framework of the change-of-variables approach for E [p(X)]. Consider

a one-to-one mapping on Ω, the support of X:

u , (u1, . . . , un) : Ω ⊂ Rm 7→ Rn,

where n ≥ m, ui’s are functions of x, and the image set of the mapping is denoted by u{Ω} ⊂
Rn. The mapping is said to be diﬀerentiable at x ∈ Rm if partial derivatives Du(x) exist, and
continuously diﬀerentiable if all partial derivatives are continuous, where the partial derivative

matrix is deﬁned as

Furthermore, for a subset E ⊂ Rm, u is said to be Lipschitz continuous if |u(x1)−u(x2)| ≤ c|x1−x2|
for any x1, x2 ∈ E, and a constant c.

It should be emphasized that for a general one-to-one mapping, n may not equal m. When
n > m, the image set of the mapping is essentially an “m-dimensional” subset of Rn. For instance,

the image set of the mapping illustrated in Figures 1 is indeed a hyperplane that is a 2-dimensional
subsets of R3. For such cases, the integrals taken over the image sets are not well deﬁned in

Lebesgue sense. To deal with this issue, we work with Hausdorﬀ measure, an extension of Lebesgue

measure.

8

Du(x) =

∂u1(x)/∂x1
∂u1(x)/∂x2

...

∂u2(x)/∂x1
∂u2(x)/∂x2

...

. . .
. . .
...

∂un(x)/∂x1
∂un(x)/∂x2

...

∂u1(x)/∂xm ∂u2(x)/∂xm . . . ∂un(x)/∂xm

.



Deﬁnition 1 (Hausdorﬀ Measure on Rn). Let A be a nonempty subset of Rn, and deﬁne its
diameter by diam(A) , {|x − y| : x, y ∈ A}. For any δ ∈ (0,∞] and s ∈ [0,∞), deﬁne Hs
infnP∞
R ∞

i=1Ai, diam(Ai) ≤ δo, where ω(s) = πs/2/Γ(s/2 + 1), and Γ(t) =

0 e−xxt−1 dx. Then s-dimensional Hausdorﬀ measure on Rn is deﬁned as

; A ⊂ ∪∞

δ(A) =

i=1 ω(s)(cid:16) diam(Ai)

2

(cid:17)s

Hs(A) = lim

δ→0Hs

δ(A).

Furthermore, Hausdorﬀ dimension of A is deﬁned by dimH(A) = inf {s ∈ [0,∞);Hs(A) = 0}.

Essentially, Hausdorﬀ measure generalizes the concepts of length, area, and volume. For in-
stance, 1-dimensional Hausdorﬀ measure of a smooth curve in Rn is the length of the curve, and
2-dimensional Hausdorﬀ measure of a smooth surface in Rn is its area. Hausdorﬀ measure has
several properties: Hs ≡ 0 on Rn for any s > n, Hs(λA) = λsHs(A) for any λ > 0, and Hs is
equivalent to Lebesgue measure on Rn when s = n. Hausdorﬀ measure is a useful tool to mea-
sure “small” subsets in Rn. For instance, a set A = [0, 1] × [0, 1] × {1} ⊂ R3 is a “copy” of the
2-dimensional square [0, 1] × [0, 1] in R3. Its volume is 0, as its Lebesgue measure is 0. It has,
however, an area of 1, i.e., H2(A) = 1, and its Hausdorﬀ dimension is 2.

To characterize smoothness of a set on Rn, we introduce the concept of rectiﬁability.

i=1 vi (Rm)) = 0.

Deﬁnition 2 (Rectiﬁable Sets). For m ≤ n, a set A ⊂ Rn is said to be (Hm, m) rectiﬁable if there
exist a countable collection {vi}i≥1 of continuously diﬀerentiable mappings vi : Rm 7→ Rn such that
Hm (A\S∞
Typically, a set is rectiﬁable if it can be represented as the union of images of continuously
diﬀerentiable mappings. For instance, the set A = [0, 1] × [0, 1] × {1} ⊂ R3 is (H2, 2) rectiﬁable,
because it is the image of a mapping u : R2 7→ R3 where u(x1, x2) = (x1, x2, 1). Intuitively speaking,
a rectiﬁable set is a piece-wise “smooth” set in Rn. It has many of the desirable properties when

taking Hausdorﬀ integrals.

When applying changes of variables for the mapping u : Rm 7→ Rn, the Jacobian plays a critical
role. When n = m, the Jacobian is deﬁned as the absolute value of the determinant of Du, serving

as a corrective factor that relates the “volumes” of the domain and image sets. In a more general
setting where n ≥ m, Du may not be a square matrix, and an extension of the Jacobian is deﬁned
as follows(see, e.g., Morgan (2009)).

k (x) denote the k-dimensional Jacobian of u at x.
k (x) = 0 if rank(Du(x)) < k, and it is equal to square root of the sum of squares of the

Deﬁnition 3 (k-dimensional Jacobian). Let J u
Then J u
determinants of the k × k submatrices of Du(x) if rank(Du(x)) ≥ k.

In what follows, we discuss how the change-of-variables approach works as in Examples 1 and 2.
It proceeds by two steps. In the ﬁrst step, we note that m ≤ n, u{Ω} is essentially an m-dimensional

9

subset on Rn because u is one-to-one. Then E [p(X)], a Lebesgue integral taken over Ω, is equal
to a Hausdorﬀ integral taken over the set u{Ω}, which is justiﬁed by a change-of-variables formula
for Hausdorﬀ measure.

The second step is to take iterated integration by using an extended version of Fubini’s Theorem.
To this end, we ﬁrst note that u{Ω} can be written in a product form of u{Ω} = S1 × S2 under
mild smoothness conditions, where S1 ⊂ Rn−q and S2 ⊂ Rq for some integer q ≤ n. Then similar
to that in Example 1, taking integral2 over the set S1 yields a function of w, which is in fact a
function of the elements in S2 ⊂ Rq. More generally, this step can be viewed through an argument
on mappings. Speciﬁcally, construct a mapping

ϕ : u{Ω} 7→ Rq.

Suppose Hausdorﬀ dimension of the image set ϕ{u{Ω}} is k for some k ≤ q. Then for any ﬁxed
z ∈ ϕ{u{Ω}}, Hausdorﬀ dimension of the set ϕ−1{z} is m − k. Then ﬁxing z and taking integral
over ϕ−1{z} lead to a function of w as in Examples 1 and 2. This result is stated in the following
theorem, whose proof is provided in the appendix.

Theorem 1. Assume that Assumption 1 holds. Suppose n ≥ m ≥ k ≤ q, u : Ω 7→ Rn is a
continuously diﬀerentiable one-to-one mapping, and ϕ : u{Ω} 7→ Rq is Lipschitz continuous. If
ϕ{u{Ω}} is (Hk, k) rectiﬁable, and J u
k (y) 6= 0 for almost all x ∈ Ω and y ∈ u{Ω}
up to null sets, then

m(x) 6= 0 and J ϕ

E [p(X)] = E [w (ϕ(u(X)))] ,

where the function w is deﬁned by

w(z) , Zϕ−1{z}

p(u−1(y))f (u−1(y))
m(u−1(y))J ϕ
J u

k (y) Hm−k(dy),Zϕ−1{z}

f (u−1(y))
m(u−1(y))J ϕ
J u

k (y)Hm−k(dy).

Moreover, w (ϕ(u(X))) has a smaller variance than p(X), i.e.,

(10)

(11)

Var [w (ϕ(u(X)))] ≤ Var [p(X)] .

Theorem 1 shows that w(ϕ(u(X))) has the same expectation as p(X), and provides a closed-

form expression for the function w. In addition, it shows that w(ϕ(u(X))) has a smaller variance

than p(X), which is expected as in conventional CMC.

Theorem 1 works with Hausdorﬀ integrals, aiming to accommodate a more general setting. For

a special case when n = m, k = q < n, and the mapping ϕ is speciﬁed by ϕ(y) = (y1, . . . , yk)
for y , (y1, . . . , ym), Hausdorﬀ integrals are equivalent to Lebesgue integrals, and w (ϕ(u(X))) is
equivalent to E [p(X)|u1(X), . . . , uk(X)].

2 Hausdorﬀ dimension of S1 plays an important role in taking integrals. If Hausdorﬀ dimension of S1 is k for some

k ≤ q, Hausdorﬀ dimension of S2 should be m − k, because Hausdorﬀ dimension of u{Ω} is m.

10

To better understand the insights oﬀered by Theorem 1, we look into the result in greater detail.

Note that by the theorem,

E [w (ϕ(u(X)))] = E [p(X)] = E (E [p(X)|ϕ(u(X))]) .

It is then reasonable to expect that w (ϕ(u(X))) = E [p(X)|ϕ(u(X))], which is also conﬁrmed as a
by-product in the proof of the theorem. Compared to conventional CMC that directly conditions

on ϕ(u(X)), Theorem 1 is meaningful in the following two aspects.

First, Theorem 1 provides a closed-form expression of E [p(X)|ϕ(u(X))], that applies to a very
general simulation model of X, in contrast to conventional CMC where deriving explicit formula of

the conditional expectation often relies highly on the structure of X. As seen in Examples 1 and 2,

the expression of the function w may lead to estimators with explicit formulas. Derivation of such

explicit formulas shall be further exempliﬁed in Section 5 for applications under more practical

settings.

Second, Theorem 1 oﬀers great ﬂexibility for us to choose among various mappings u and ϕ, so

as to make use of the form of the integrand p(X). This point shall be made clearer in Section 4

where we construct mappings u and ϕ to exploit the unique structure of a discontinuous integrand

of the form (1), leading to eﬃcient sensitivity estimators.

4 Change-of-Variables Approach for Sensitivity Estimation

In this section we study the change-of-variables approach for sensitivity estimation for a discontin-

uous integrand of the form (1). In particular, we ﬁrst provide a general estimator in Theorem 2

for the sensitivity, which applies to general mappings that satisfy mild conditions. We then con-

sider two cases in Sections 4.1 and 4.2, providing sensitivity estimators when the function h in (1)

satisﬁes Conditions 1 and 2, respectively. These estimators require mainly smoothness conditions

on h, and their implementation do not require any construction of mappings. We ﬁnd that the

function h encountered in many practical applications satisﬁes Condition 1 and/or Condition 2.

The proposed estimators are, therefore, useful for a wide range of applications.

Recall that our problem is to estimate
α′(ξ) , d
dξ

for given functions g and h, as stated in Section 2.

E(cid:2)g(X) · 1{h(X)≤ξ}(cid:3) ,

(12)

To facilitate analysis, we make the following smoothness assumption on the function h. More-
over, we let (c0, c1) denote the interval in which h(x) may take values for x ∈ Ω, where c0 = −∞
and c1 = ∞ are allowed. To avoid trivial cases, we assume ξ ∈ (c0, c1).
Assumption 2. The function h(x) is continuously diﬀerentiable for almost all x ∈ Ω up to a
Lebesgue null set.

11

To analyze the estimation of α′(ξ), we start our discussion with the result in Theorem 1, which

can be applied to obtain a function of w such that

α(ξ) = E(cid:2)g(X) · 1{h(X)≤ξ}(cid:3) = E [w (ϕ(u(X)); ξ)] ,

where ξ is added as an argument of the function w to explicitly account for the dependence. To

enable the use of IPA, one may attempt to construct mappings u and ϕ such that w (ϕ(u(X)); ξ) is
smooth in ξ. To this end, it is natural to take into account the unique structure of g(X) · 1{h(X)≤ξ}
that ξ appears in the indicator function only.

Inspired by Examples 1 and 2, we consider the following mappings:

u : x ∈ Ω 7→ (u1(x), . . . , um(x), h(x)), ϕ : y ∈ u{Ω} 7→ (y1, . . . , ym),

(13)

for x , (x1, . . . , xm) and y , (y1, . . . , ym+1), where {ui, 1 ≤ i ≤ m} are continuously diﬀerentiable
functions of x.

A common feature of the u mappings we consider is that h(x) serves as the last dimension in
the image set, taking integral along which removes discontinuity in the integrand g(x) · 1{h(x)≤ξ}.
The resulting w (ϕ(u(X)); ξ) is thus continuous in ξ, which enables the use of IPA. For the sake of

using change-of-variables formula, we require that u is one-to-one.

It should also be pointed out that u in (13) is in general a mapping from Ω ⊂ Rm to Rm+1.
To ensure that it is one-to-one, Hausforﬀ dimension of the image set u{Ω} should be m.
In
other words, u{Ω} is an m-dimensional subset in Rm+1. Furthermore, for the integral taken along
the last dimension to make sense, Hausdorﬀ dimension of the last coordinate of u{Ω} should be
1, which intuitively implies that for ﬁxed values of the ﬁrst m coordinates, the last dimension
In plain words, for any ﬁxed z ∈ Rm, the curve
of u{Ω} contains at least a closed interval.
{t ∈ R : t = h(x), (u1(x), . . . , um(x)) = z} has a positive length. Mathematically, this requirement
can be translated to

H1(cid:0)ϕ−1{z}(cid:1) > 0,
where by the deﬁnition of ϕ, the set ϕ−1{z} is
ϕ−1{z} = {(z, t) ∈ u{Ω} : t ∈ (c0, c1)}.

∀ z ∈ ϕ{u{Ω}},

Given these conditions, we apply Theorem 1 and have

α(ξ) = E(cid:2)g(X) · 1{h(X)≤ξ}(cid:3) = E [w (ϕ(u(X)); ξ)] ,

where

w(z; ξ) = Z ξ

c0

g(u−1(z, t))f (u−1(z, t))

J u
m(u−1(z, t))

dt(cid:30)Z c1

c0

f (u−1(z, t))
J u
m(u−1(z, t))

dt,

(14)

12

where the Jacobians of ϕ have been cancelled out because J ϕ

m−1(y) ≡ √m for all y.

The function w(z; ξ) in (14) is typically continuous in ξ, because ξ is simply a limit of integration.

It is then reasonable to expect that

α′(ξ) =

d
dξ

E [w (ϕ(u(X)); ξ)] = E(cid:20) d

dξ

w (ϕ(u(X)); ξ)(cid:21) .

This result is summarized in the following theorem, whose proof is provided in the appendix.

Theorem 2. Suppose that the mappings u and ϕ are speciﬁed as in (13), and u is one-to-one
and continuously diﬀerentiable. If Assumptions 1-2 hold, J u
m(x) 6= 0 for almost all x ∈ Ω, and

H1(cid:0)ϕ−1{z}(cid:1) > 0 for almost all z ∈ ϕ{u{Ω}}, then

α(ξ) = E [w (ϕ(u(X)); ξ)] ,

where w(·; ξ) is deﬁned in (14).

If, in addition, w (ϕ(u(X));·) satisﬁes a Lipschitz continuity condition w.r.t. ξ, i.e., there exists

a random variable K with E(K) < ∞ such that for all small enough ∆,

|w (ϕ(u(X)); ξ + ∆) − w (ϕ(u(X)); ξ) | ≤ K|∆|, with probability 1,

then,

where

α′(ξ) = E [ν(ϕ(u(X)); ξ)] ,

ν(z; ξ) =

g(u−1(z, ξ))f (u−1(z, ξ))

J u
m(u−1(z, ξ))

(cid:30)Z c1

c0

f (u−1(z, t))
J u
m(u−1(z, t))

dt.

Remark 1. A special case of the result of Theorem 2 is g(x) ≡ 1 in which the quantity of interest,
α′(ξ), is in fact the density of h(X) evaluated at ξ. When Theorem 2 is applicable, a sample-mean

estimator can be derived for the density of h(X).

Theorem 2 shows that the sensitivity α′(ξ) is equal to E [ν(ϕ(u(X)); ξ)], and provides a closed-

form expression of the function ν. When the one-dimensional integral in ν can be derived in explicit

forms, an explicit formula of ν can be obtained, as in Examples 1 and 2. When explicit formulas

are not available, one may resort to one-dimensional numerical integration methods to approximate

ν(ϕ(u(X)); ξ) for any realization of ϕ(u(X)). Eﬃcient one-dimensional numerical integration tools

are available in many commercial softwares such as Matlab.

To ensure the validity of interchanging diﬀerentiation and expectation, Theorem 2 requires that

w (ϕ(u(X)); ξ) satisﬁes a Lipschitz continuity condition w.r.t. ξ, which has been a commonly used

condition in sensitivity estimation literature; see, e.g., Broadie and Glasserman (1996) and Liu

and Hong (2011). This assumption typically does not impose any obstacle in practice, because the

13

interchange is usually valid when w (ϕ(u(X)); ξ) is continuous in ξ, which is obviously true by the

way w is deﬁned.

A key condition of Theorem 2 is H1(cid:0)ϕ−1{z}(cid:1) > 0 for any given z, which may not hold in general.

This condition also depends on how the u and ϕ mappings are constructed. In practice, before

applying the result in Theorem 2, one may need to construct the mappings and verify this condition

accordingly. To reduce simulation practitioners’ eﬀort in carrying out veriﬁcations, Sections 4.1 and

4.2 consider two cases for which we provide two suﬃcient conditions imposed on the function h

only, and specify explicitly what mappings to be used. These two suﬃcient conditions are easier to

verify. We also note that the function h in many practical applications satisﬁes either one of the

suﬃcient conditions, or both.

4.1 Case 1

Consider a function h that satisﬁes the following condition:

Condition 1. The mapping x 7→ (x1, . . . , xm−1, h(x)) is one-to-one, and Pr{∂mh(X) = 0} = 0.

We claim that when Condition 1 holds, the key condition of Theorem 2, i.e., H1(cid:0)ϕ−1{z}(cid:1) > 0

for any given z, is satisﬁed for appropriate mappings u and ϕ. To see this, we set ui(x) = xi for
1 ≤ i ≤ m − 1 and um(x) ≡ 1 in (13). Note that um(x) takes a constant value. The mappings in
(13) are therefore equivalent to the following:

˜u : x 7→ (x1, . . . , xm−1, h(x)),

˜ϕ : y 7→ (y1, . . . , ym−1),

(15)

where y , (y1, . . . , ym).

To see why Condition 1 implies H1(cid:0) ˜ϕ−1{z}(cid:1) > 0 for any given z, we note that when Condition
1 is satisﬁed, the set ˜ϕ−1{z} for a given z is {(z, t) : t ∈ R, t = h(x) for some x ∈ Ω}, which
is indeed a straight line with a positive length because Ω has at least a compact subset and h is
continuous by Assumption 2. The condition that H1(cid:0) ˜ϕ−1{z}(cid:1) > 0 for almost all z is therefore

satisﬁed.

It can be easily seen that the Jacobian of ˜u is

˜J (x) = |∂mh(x)|,

where ∂mh(x) denotes the partial derivative of h(x) w.r.t. xm. Denote (y1, . . . , ym−1) by z. For

any given y = (z, ym), let v(z, ym) denote the solution to the equation h(z, xm) = ym, which is

unique because ˜u is one-to-one. Then it can be easily seen that

and h(cid:0)˜u−1(y)(cid:1) = ym.

˜u−1(y) = (z, v(z, ym)) ,

14

Applying Theorem 2, we obtain an expression of α′(ξ). This result is summarized in the

following proposition, whose proof is straightforward and thus omitted.

Proposition 1. Suppose Assumptions 1-2 and Condition 1 are satisﬁed. Then,

where X−m , (X1, . . . , Xm−1), and

α(ξ) = E [ew (X−m; ξ)] ,
dt(cid:30)Z c1

˜J(z, v(z, t))

c0

g (z, v(z, t)) f (z, v(z, t))

f (z, v(z, t))
˜J(z, v(z, t))

dt.

If, in addition, ew(X−m;·) satisﬁes a Lipschitz continuity condition w.r.t. ξ as in Theorem 2, then,

where

Remark 2. It should be pointed out that the result in Proposition 1 still applies if we change the

mapping in Condition 1 to

f (z, v(z, t))
˜J(z, v(z, t))

dt.

(16)

ew(z; ξ) = Z ξ

c0

eν(z; ξ) =

α′(ξ) = E [eν (X−m; ξ)] ,
(cid:30)Z c1

˜J(z, v(z, ξ))

c0

g (z, v(z, ξ)) f (z, v(z, ξ))

x 7→(cid:0)xi1, . . . , xim−1 , h(x)(cid:1)

for any permutation (i1, . . . , im) of {1, . . . , m}. We only need to change the indices of the function
arguments accordingly. This oﬀers more ﬂexibility to verify Condition 1 in practice.

Proposition 1 shows that under appropriate conditions, α′(ξ) can be estimated by a sample mean

ofeν (X−m; ξ), where the functioneν is speciﬁed in (16). For any given z, evaluation of the functioneν

requires knowing v(z, t) that is in fact the inverse of h(z, xm) as a function of xm. When an explicit

formula of v(z, t) can be derived, the evaluation is straightforward. When explicit formulas are

not available, in principle it can be approximated by numerically solving the equation h(z, xm) = t

by using, e.g., one-dimensional line search methods. For instance, when t = ξ, v(z, ξ) can be

approximated by line search algorithms for any given z. However, evaluation of the denominator

term of eν may require line search operations for all possible z’s and a grid of t’s in (c0, c1), which

may be too computationally expensive to aﬀord.

To resolve this issue, we may further apply a change of variables on the denominator term.
Speciﬁcally, for any given z, consider the one-to-one mapping t 7→ v(z, t). Recall that by Assumption
2 and the inverse function theorem, v(z, t) is continuously diﬀerentiable in t, and the Jacobian

Then by the change-of-variables formula, it can be easily veriﬁed that

f (z, s) ds,

(17)

dv(z, t)

dt

(cid:12)(cid:12)(cid:12)(cid:12)

Z c1

c0

f (z, v(z, t))
˜J (z, v(z, t))

|∂mh(z, v(z, t))|

.

1

(cid:12)(cid:12)(cid:12)(cid:12) =
dt =ZV(z)

15

where V(z) denotes the set in which xm takes values, i.e., V(z) = {s : (z, s) ∈ Ω}. From (17), it

can be seen that the denominator term of eν is indeed the density function of X−m evaluated at

z. It can be evaluated based on the integral on the RHS of (17), which can be eﬃciently done by

one-dimensional integration methods and does not require knowing the function v(z, t).

We close the discussion for Case 1 by a remark on Condition 1. Many functions of h in

practical applications may satisfy Condition 1, e.g., h(x) = x1 + x2 and h(x) = x2
1x2). In
practice, the requirement of the mapping being one-to-one in Condition 1 may be violated for some

1 + exp(x3

functions, and some modiﬁcations may ﬁx this issue. For instance, the mapping is not one-to-one
when h(x) = x1 + x2
2. In this case, we may either change the mapping to x 7→ (x2, h(x)), or divide
the support of X into two parts, Ω ∩ {x : x2 ≥ 0} and Ω ∩ {x : x2 < 0}, and apply Proposition 1
to each part separately.

However, such modiﬁcations may not work for some cases where Condition 1 is violated. For

instance, in a commonly encountered case where h(x) = max(x1, . . . , xm), the mapping in Condition

1 is obviously not one-to-one. To deal with such cases, we study another suﬃcient condition in the

following subsection.

4.2 Case 2

We consider a function of h that is homogeneous, i.e.,

Condition 2. The function h is homogeneous, i.e., h(tx) = tx for t > 0.

Homogeneous functions include, for example, the maximum function h(x) = max(x1, . . . , xm),
the minimum function h(x) = min(x1, . . . , xm), linear functions h(x) = a1x1 + ··· + amxm. These
homogeneous functions ﬁnd important applications in a wide range of areas in operations research.

For homogeneous functions of h, we consider the following mapping

¯u : x 7→ (x1/h(x), . . . , xm/h(x), h(x)) ,

¯ϕ : y 7→ (y1, . . . , ym),

(18)

for y = (y1, . . . , ym, ym+1).

In this case, we work with a modiﬁed version of X’s support, Ω′ = Ω\{x ∈ Ω : h(x) = 0}, to
avoid zeros values for denominators. When h is continuously diﬀerentiable, this modiﬁcation does

not change the result of our analysis. This is because X is a continuous random vector, and thus
Pr(h(X) = 0) = 0. Therefore, ignoring the set {x ∈ Ω : h(x) = 0} does not aﬀect our analysis on
integrations.

The key condition of Theorem 2 that H1(cid:0) ¯ϕ−1{z}(cid:1) > 0 for any given z is satisﬁed when h is

homogeneous and the mappings are set as in (18). To see this, we only need to verify the condition

for two possible scenarios. The ﬁrst scenario considers a given z for which there exists an x0 such

that z = x0/h(x0) and h(x0) > 0. Since (c0, c1) is the interval in which h(x) takes values, we can

16

see that c1 > 0, and

¯ϕ−1{z} = {(z, t) ∈ ¯u{Ω′} : t ∈ (c0, c1)} = {(z, t) : t ∈ (c0, c1), h(tz) = t} = {(z, t) : t ∈ (0, c1)},

where the last equality follows from Condition 2 that h(tz) = th(z) = th(x0)/h(x0) = t for all
t > 0. Therefore, ¯ϕ−1{z} is a straight line and has positive length, i.e, H1( ¯ϕ−1{z}) > 0. The
second scenario considers a given z for which there exists an x0 such that z = x0/h(x0) and

h(x0) < 0. Then c0 < 0, and

¯ϕ−1{z} = {(z, t) ∈ ¯u{Ω′} : t ∈ (c0, c1)} = {(z, t) : t ∈ (c0, c1), h(tz) = t} = {(z, t) : t ∈ (c0, 0)},

where the last equality follows from Condition 2 that for any t < 0,

h(tz) = −th(−z) = −th(x0/(−h(x0))) = −th(x0)/(−h(x0)) = t,

because h(x0) < 0. Therefore, ¯ϕ−1{z} is a straight line and has positive length, i.e., H1( ¯ϕ−1{z}) >
0.

It can be easily check that the mapping ¯u is continuously diﬀerentiable because h is continuously
diﬀerentiable. It is also one-to-one, and ¯u−1(y) = tz for any y = (z, t) ∈ ¯u{Ω′}. Moreover, the
Jacobian of ¯u has a neat form. By elementary algebra, the m-dimensional Jacobian of ¯u is

m(x) =s (∂1h(x))2 + ··· + (∂mh(x))2

h2(m−1)(x)

¯J ¯u

,

where ∂ih denotes the partial derivative of h(x) w.r.t. xi for i = 1, . . . , m. Deﬁne

¯Jm(z, t) =

1

|t|m−1q(∂1h(tz))2 + ··· + (∂mh(tz))2.

Then it can be veriﬁed that

¯J ¯u

m(¯u−1(y)) = ¯Jm(z, t)(cid:14)|h(sign(t)z)|m−1 ,

∀ y = (z, t) ∈ u{Ω′},

where sign(t) is a sign function that is equal to 1 if t > 0 and −1 if t < 0.

Applying the result of Theorem 2, we arrive at an expression of α′(ξ), which is summarized in

the following proposition, whose proof is a direct application of Theorem 2 and thus omitted.

Proposition 2. If Assumptions 1-2 and Condition 2 are satisﬁed, then

α(ξ) = E [ ¯w (X/h(X); ξ)] ,

where

¯w(z; ξ) = Z ξ

c0 |h(sign(t)z)|m−1g(tz)f (tz)/ ¯Jm(z, t)dt(cid:30)Z c1

c0

|h(sign(t)z)|m−1f (tz)/ ¯Jm(z, t)dt.

17

If, in addition, ¯w (X/h(X);·) satisﬁes a Lipschitz continuity condition w.r.t. ξ as in Theorem 2,
then

α′(ξ) = E [¯ν(X/h(X); ξ)] ,

where

¯ν(z; ξ) = |h(sign(ξ)z)|m−1g(ξz)f (ξz)

¯Jm(z, ξ)

(cid:30)Z c1

c0

|h(sign(t)z)|m−1f (tz)

¯Jm(z, t)

dt.

To illustrate how Proposition 2 can be applied to derive estimators for diﬀerent functions of

h. We consider three examples, where h is chosen to be a maximum function, a linear function,

and a quadratic function, respectively, i.e., h(x) = max(x1, . . . , xm), h(x) = x1 + . . . + xm and
h(x) = x2

1 + ··· + x2
m.

Example 3. When h(x) = max(x1, . . . , xm), Ω = Rm
¯Jm(z, t) = 1/tm−1. Applying Proposition 2, we have

+ and ξ > 0, it can be easily veriﬁed that

0

g(tz)tm−1f (tz)dt(cid:30)Z ∞
¯w(z; ξ) = Z ξ
¯ν(z; ξ) = g(ξz)ξm−1f (ξz)(cid:30)Z ∞

0

0

tm−1f (tz)dt,

tm−1f (tz)dt .

and

and

When h(x) = x1 + ··· + xm and Ω = Rm

¯w(z; ξ) and ¯ν(z; ξ) have the same forms as those for the maximum function.

+ , it can be veriﬁed that ¯Jm(z, t) = √m/tm−1, and
m and Ω = Rm, we work with ph(x) which satisﬁes Condition 2.

When h(x) = x2

1 + . . . + x2

Then applying Proposition 2 leads to

0

¯w(z; ξ) = Z ξ2

g(tz)tm−1f (tz)dt,Z ∞
¯ν(z; ξ) = 2ξg(ξ2z)ξ2m−2f (ξ2z)(cid:30)Z ∞

0

0

tm−1f (tz)dt,

tm−1f (tz)dt .

5 Applications

5.1 Estimating Price Sensitivities for Financial Options

In ﬁnancial risk management, the Greek letters of options play an important role in constructing

hedging strategies. Mathematically, the Greek letters of an option are deﬁned as the sensitivities of

the option price with respect to market parameters such as underlying asset prices, volatilities and

risk-free interest rate. When the payoﬀ of the option is discontinuous, estimating the Greek letters

18

has been a challenging problem in simulation. In what follows, we apply the change-of-variables

approach to estimate the Greek letters for options with discontinuous payoﬀs.

Typically, the (discounted) payoﬀ of an option with a discontinuous payoﬀ is of the form

l(X)

qYi=1

1{hi(X)≤ai},

(19)

for Lipschitz continuous functions l and hi’s, where X = (X1, . . . , Xm) is a random vector that

represents the price dynamics of the underlying asset and depends on a market parameter θ, and

for notational ease we suppress this dependence when there is no confusion. Without loss of

generality, we assume that θ is a scalar. Based on Theorem 1 of Liu and Hong (2011), a Greek

letter associated with the market parameter θ is represented as

d
dθ

E"l(X)
qYi=1
= E"∂θl(X)

1{hi(X)≤ai}#
qYi=1

1{hi(X)≤ai}# −

qXi=1

d
dai

El(X)∂θhi(X)Yk6=i

1{hk(X)≤ak} · 1{hi(X)≤ai} ,

where ∂θ denotes the operator of takin derivative with respect to θ.

Note that in a simulation run, both {l(X), hi(X), 1 ≤ i ≤ q} and their pathwise derivatives
{∂θl(X), ∂θhi(X), 1 ≤ i ≤ q} are usually readily computable; see, e.g., Broadie and Glasserman
(1996). The ﬁrst term on the RHS of the above equation can then be easily estimated by a sample-

mean estimator. In the rest of this subsection, we will discuss how to estimate the second term

using the change-of-variables approach.

To simplify notation, we let

gi(X) = l(X)∂θhi(X)Yk6=i

1{hk(X)≤ak }.

Then the problem is reduced to how to estimate

βi ,

d
dai

E(cid:2)gi(X) · 1{hi(X)≤ai}(cid:3) ,

i = 1, . . . , q.

For many options traded in ﬁnancial markets, the function hi(X) in their payoﬀs is often
i=1 Xi/m, hi(X) = max(X1, . . . , Xm), or hi(X) =
min(X1, . . . , Xm); see, e.g., Tong and Liu (2016) for more detailed discussions. It can be easily

in the form of hi(X) = Xm, hi(X) = Pm

seen that these are all homogenous functions and satisfy Condition 2. Applying Proposition 2, we

have

βi =

d
dai

E(cid:2)gi(X) · 1{hi(X)≤ai}(cid:3) = E [ν (Z; ai)] ,

19

where Z , (X1/hi(X), . . . , Xm/hi(X)), and

ν(z; ai) = gi(zai)am−1

i

f (zai)(cid:30)Z ∞

0

tm−1f (tz)dt .

It turns out that the same form of ν applies to all βi, i = 1, . . . , q.

Deriving an explicit formula of ν(z; ai) requires knowing the joint density function f , which is

available for many commonly used pricing models, for instance, when X represents the underlying

asset prices observed at diﬀerent time points under the Black-Scholes model. As a remark, it is

worth pointing out that this requirement can be further relaxed. Indeed, one may replace f by a

conditional density of X given some variables. This relaxation oﬀers considerable ﬂexibility during

implementation, as conditional densities can often be obtained for most, if not all, pricing models

for ﬁnancial options. As an illustrative example, we derive ν(z; ai) for the more complex variance

gamma model using a conditional density where an explicit joint density is not available; see Section

B.1.2 of the online supplement for more details.

When estimating βi, we generate n identically and independently distributed (i.i.d.) observa-

tions of X, denoted by {X1, . . . , Xn}, and compute
Xk,m

Zk =(cid:18) Xk,1

hi(Xk)

hi(Xk)(cid:19) ,

, . . . ,

k = 1, . . . , n,

where Xk = (Xk,1, . . . , Xk,m).

Then βi can be estimated by

¯Mn =

1
n

ν(Zk; ai).

nXk=1

It is worth mentioning that ¯Mn is an unbiased estimator, thus it has desirable asymptotic properties
as a typical sample-mean estimator. Although it involves another random vector Z, the estima-

tor does not require any change of probability measures in the simulation, because Z is readily

computable once X is generated.

5.1.1 Numerical Experiments

We consider two pricing models, the Black-Scholes (BS) model and the variance gamma (VG)

model, to conduct numerical experiments and examine the performances of the estimators.
In
particular, the price of the underlying asset is monitored at m time points {t1 < ··· < tm} evenly
spaced over (0, T ), i.e., ti = iT /m for i = 1, . . . , m, where T is the maturity date of the option. To

simplify notation, we let Xi denote the underlying asset price at ti for i = 0, . . . , m.

Under the BS model, the price of the underlying asset is governed by a geometric Brownian

motion, i.e.,

Xi+1 = Xi exp(cid:16)(r − σ2/2)T /m + σpT /mNi+1(cid:17) ,

i = 0, . . . , m − 1,

20

where {N1, . . . , Nm} are independent standard normal random variables, r and σ denote the risk-
free interest rate and volatility of the underlying asset. Denote the initial price of the underlying

asset by X0 = x0. The joint density of X is then

f (x) =

m−1Yi=0

1

σpT /mxi+1

φ 

1

σpT /m(cid:0)log(xi+1/xi) − (r − σ2/2)T /m(cid:1)! ,

where φ(·) denotes the standard normal density function.

Compared to the BS model, the VG model is a pure jump process and allows for more ﬂexible

skewness and kurtosis; see, e.g., Madan et al. (1998). A discretization of the VG model is given by

(Fu 2000),

Xi+1 = Xi exp(µT /m + θGi+1 + σpGi+1Ni+1),

i = 0, . . . , m − 1,

where {G1, . . . , Gm} are independent gamma random variables with scale parameter T /(mβ) and
shape parameter β, {N1, . . . , Nm} are independent standard normal random variables, µ = r +
1/β log(1 − θβ − σ2β/2), and θ and σ are parameters of the model. Then the conditional density
of X given G , (G1, . . . , Gm) is

f (x|G) =

m−1Yi=0

1

xi+1σ√Gi+1

φ(cid:18)

1

σ√Gi+1

(log(xi+1/xi) − µT /m − θGi+1)(cid:19) .

Under each of the BS and VG models, we consider three options with discontinuous payoﬀs,
including a digital option with discounted payoﬀ e−rT 1{Xm≥K}, an Asian digital option with dis-
i=1 Xi/m≥K}, and a barrier call option with discounted payoﬀ e−rT (Xm −
counted payoﬀ e−rT 1{Pm
K)+1{max(X1,...,Xm)≤κ}, where K and κ denote the strike price and the barrier respectively. For each
of the above options, we estimate the Greek letters delta and gamma, i.e., the ﬁrst- and second-

order derivatives of the option price w.r.t. x0, and theta and vega, i.e., the ﬁrst-order derivatives

of the option price w.r.t. T and σ, respectively. We compare the proposed change-of-variables

estimators to existing ones in the literature, including the likelihood ratio method and conventional

CMC when applicable. Detailed derivation of various estimators is provided in Section B.1 of the

online supplement.

In all experiments, we set the sample size as n = 105. To examine the performance of an
estimator ¯Mn, we use its relative error, deﬁned as the ratio of the standard deviation of ¯Mn to
the absolute value of the quantity being estimated. True values of the quantities being estimated

are either computed by closed-form formulas when available, or approximated using other existing
methods with an extremely large sample size (109).

For the options under the BS model, we let x0 = K = 100, κ = 120, r = 5%, σ = 0.3, and

T = 1. We vary the number of discretization steps, m, to examine its impact on the performances

of various estimators, including the likelihood ratio (LR) estimator, conventional CMC estimator,

and the proposed change-of-variables (CoV) estimator. Comparison results for the digital, Asian

21

and barrier options are summarized in Tables 1, 2 and 3, respectively. From these tables it can be

seen that the proposed CoV estimators have the best performances in all settings. Its improvement

upon existing methods can be dramatic. For instance, when estimating gamma for the Asian digital

option with m = 100, relative errors of the LR and conventional CMC estimators are over 10 and

1000 times of that of the CoV estimator respectively, implying that sample sizes of the LR and
CMC estimators have to be as large as 100 and 106 times of that of the CoV estimator in order to

achieve the same level of accuracy. For the barrier option, it is not clear how conventional CMC

estimators can be derived, while the proposed CoV estimators perform very well.

Table 1: Relative errors (%) of various Greek estimators for the digital option under the BS model

delta

vega

theta

gamma

m LR CMC CoV

LR CMC CoV

LR CMC CoV

LR CMC CoV

10
50
100

1.8
4.0
5.6

0.4
0.6
0.8

0.4
0.6
0.8

7.9
17.8
25.0

0.4
0.6
0.8

0.4
0.6
0.8

23.0
51.1
71.8

0.4
0.4
0.4

0.4
0.4
0.4

24.9
125
252

3.3
11.1
18.7

3.3
11.1
18.7

Table 2: Relative errors (%) of various Greek estimators for the Asian digital option under the BS
model

delta

vega

theta

gamma

m LR CMC CoV

LR CMC CoV

LR CMC CoV

LR CMC CoV

10
50
100

1.1
2.3
3.3

1.1
3.7
6.3

0.2
0.4
0.6

10.0
22.8
32.7

1.1
3.7
6.2

0.2
0.4
0.6

28.8
64.0
91.3

0.7
2.1
3.5

0.5
0.5
0.6

14.9
74.1
151

86.2
3210
15164

2.5
8.6
14.4

Table 3: Relative errors (%) of various Greek estimators for the barrier call option under the BS
model

delta

vega

theta

gamma

m

LR

CoV

10
50
100

5.5
11.1
15.1

5.6
5.6
6.2

LR

2.6
5.9
8.3

CoV

0.5
0.8
0.9

LR

2.6
5.9
8.3

CoV

LR

CoV

0.7
1.1
1.3

5.6
41.7
84.9

3.0
11.2
19.8

For options under the VG model, we let x0 = K = 100, κ = 120, r = 5%, σ = 0.2, β = 10,
θ = −0.2, and T = 1. Comparison results for the digital, Asian and barrier options are summarized

22

in Tables 4, 5 and 6, respectively. From the tables it can be seen that the proposed CoV estimators

signiﬁcantly outperform the existing estimators in many cases. For instance, when estimating theta

for the Asian option with m = 100, relative errors of the LR and conventional CMC estimators are

over 280 and 10 times of that of the CoV estimator.

Table 4: Relative errors (%) of various Greek estimators for the digital option under the VG model

delta

vega

theta

gamma

k LR CMC CoV

LR CMC CoV

LR CMC CoV

LR CMC CoV

10
50
100

1.7
3.9
5.4

0.4
0.7
0.8

0.4
0.7
0.8

14.9
32.1
44.8

0.5
0.7
0.8

0.5
0.7
0.8

31.1
68.5
97.2

0.3
0.5
0.6

0.3
0.5
0.6

383
1343
2579

52.9
121
197

52.8
121
197

Table 5: Relative errors (%) of various Greek estimators for the Asian digital option under the VG
model

delta

vega

theta

gamma

k LR CMC CoV

LR CMC CoV

LR CMC CoV

LR CMC CoV

10
50
100

1.0
2.3
3.2

1.1
3.9
6.7

0.2
0.5
0.6

14.9
33.2
46.8

1.4
4.2
6.8

0.3
0.5
0.6

35.8
80.0
112

0.9
2.5
4.2

0.2
0.3
0.4

34.8
171
323

188
7007
34262

5.6
19.8
32

Table 6: Relative errors (%) of various Greek estimators for the barrier call option under the VG
model

delta

vega

theta

gamma

k

LR

CoV

LR

CoV

LR

CoV

LR

CoV

10
50
100

8.7
50.4
106

5.6
19.5
35.2

3.1
6.1
8.3

1.1
1.6
1.9

11.0
22.3
29.7

0.6
0.9
1.0

7.3
40.0
72.5

2.3
7.8
12.9

5.2 Estimating Gradient of Chance Constrained Programs

Consider a chance constrained program

minimize

t

r(t)

subject to Pr{L(t, X) ≤ 0} ≥ β,

23

(20)

where t , (t1, . . . , tm)T ∈ Rm represents the vector of decision variables, r(t) is a deterministic
objective function, X = (X1, . . . , Xm)T is an m-dimensional random vector, L is a known function,
and β ∈ (0, 1) is a parameter speciﬁed by the modeler.

Sample average approximation (SAA) is a popular method for solving the chance constrained

program (20); see, e.g., Luedtke and Ahmed (2008) and Pagnoncelli et al. (2009). To use SAA, it

would be helpful if one has an estimate of the gradient of the constraint, i.e.,

∇t Pr{L(t, X) ≤ 0}.

To illustrate how the change-of-variables approach can be applied to estimate this gradient, we

consider an example where L is linear, i.e.,

L(t, X) = tT X − b,

where b is a given constant.

By using Theorem 1 of Liu and Hong (2011), it can be shown that for i = 1, . . . , m,

∂
∂ti

Pr{L(t, X) ≤ 0} = −

∂
∂b

EhXi · 1{tT X≤b}i .

Let h(x) = tT x. It is straightforward that Condition 2 is satisﬁed. Applying Proposition 2, we

have

where

∇t Pr(cid:8)tT X ≤ b(cid:9) = E [ν(Z; b)] ,
bZf (bZ)|b|m−1
R ∞
−∞ f (tZ)|t|m−1dt

ν(Z; b) = −

(21)

,

where f is the density function of X, and Z = X/tT X.

When estimating the gradient, we generate n i.i.d. observations of X, denoted by {X1, . . . , Xn},

and compute Zk = Xk/tT Xk for k = 1, . . . , n. Then the gradient can be estimated by

¯Mn =

1
n

nXk=1

ν(Zk; b).

5.2.1 Numerical Experiments

Numerical experiments are conducted to illustrate the performance of the gradient estimator re-

sulting from (21). In particular, we estimate ∂t1 Pr(cid:8)tT X ≤ b(cid:9) at t = (1, 1, . . . , 1)T . We vary b to
examine the performance of the estimator in diﬀerent settings. More speciﬁcally, we choose b such
that the probability of {tT X ≤ b} takes values in {0.90, 0.95, 0.99}.

Consider two cases, where X follows a multivariate normal and a multivariate Student’s t-

distribution respectively. Detailed derivation of explicit formulas for ν in (21) is provided in Section

24

B.2 of the online supplement. In our numerical setting, we let the means of the distributions be

zero and the covariance matrix Σ is speciﬁed in a way that individual Xi has a unit variance while
every pair (Xi, Xj ) has a correlation ρ when i 6= j. We let ρ = 0.3 in the numerical experiments,
and the degrees of freedom be 4 for the multivariate t-distribution.

We compare the estimators resulting from the proposed change-of-variables (CoV) approach to

conventional CMC estimators. To measure the performance of an estimator, we report its relative

error.

In addition, we report the ratio of the relative error of the conventional CMC estimator
In all experiments, we set the sample size n as 105. Numerical

to that of the CoV estimator.

results are summarized in Tables 7 and 8 for the multivariate normal and t cases, respectively.

From the tables it can be seen that the proposed CoV estimator performs better in all settings.

Its performance is signiﬁcantly better than conventional CMC in some cases. For instance, for the

multivariate t case with m = 50 and a probability level of 99%, the ratio of their relative errors is

over 10, implying that the sample size of the conventional CMC estimator has to be as large as 100

times of that of the CoV estimator in order to achieve the same level of accuracy.

Table 7: Relative errors (%) and performance ratios of the conventional CMC and CoV estimators
for the multivariate normal case.

Pr(cid:8)tT X ≤ b(cid:9)

CMC
CoV
Ratio

m = 5

m = 10

m = 50

0.90

0.95

0.99

0.90

0.95

0.99

0.90

0.95

0.99

0.7
0.5
1.5

0.9
0.5
1.7

1.7
0.8
2.3

1.2
0.7
1.7

1.4
0.7
2.0

2.6
1.0
2.5

2.9
1.2
2.5

3.5
1.2
3.0

6.4
1.7
3.7

Table 8: Relative errors (%) and performance ratios of the conventional CMC and CoV estimators
for the multivariate t case.

Pr(cid:8)tT X ≤ b(cid:9)

CMC
CoV
Ratio

m = 5

m = 10

m = 50

0.90

0.95

0.99

0.90

0.95

0.99

0.90

0.95

0.99

0.9
0.4
2.0

1.1
0.5
2.4

2.3
0.5
4.2

1.4
0.5
2.6

1.7
0.6
3.2

3.5
0.7
5.2

3.4
0.6
5.4

4.2
0.6
6.6

8.3
0.8
10.6

25

6 Concluding Remarks

In this paper, we propose a change-of-variables approach to CMC, and provide theoretical under-

pinnings. We attempt to circumvent the diﬃculty on ﬁnding conditioning variables for CMC. To

this end, we provide sensitivity estimators for discontinuous integrands under appropriate smooth-

ness conditions. Many practical applications may ﬁt into our setting. We show that the proposed

approach may lead to new estimators, exempliﬁed by two applications, including estimation of

sensitivities of ﬁnancial options with discontinuous payoﬀs and gradient estimation of chance con-

strained programs.

The change-of-variables approach might also ﬁnd applications in other simulation problems,

e.g., sampling in a hyperplane in Rn from a given distribution.
In principle, sampling from an
properly chosen distribution in Rn−1 and then mapping the generated samples to the hyperplane

might achieve the goal. However, how to design the mapping is challenging, and it is left as a topic

for future research.

Acknowledgements

The research of the second author is partially supported by the Hong Kong Research Grants Council

under grants CityU 155312 and 192313.

References

Asmussen, S., and P. W. Glynn. 2007. Stochastic Simulation: Algorithms and Analysis, Springer,

New York.

Bernis, G., E. Gobet and A. Kohatsu-Higa. 2003. Monte Carlo evaluation of Greeks for multidi-

mensional barrier and lookback options. Mathematical Finance, 13 99-113.

Broadie, M., and P. Glasserman. 1996. Estimating security price derivatives using simulation.

Management Science, 42 269-285.

Chan, J. H. and M. Joshi. 2013. Fast Monte Carlo Greeks for ﬁnancial products with discontinuous

pay-oﬀs. Mathematical Finance, 23(3) 459-495.

Chen, N., and P. Glasserman. 2007. Malliavin Greeks without Malliavin calculus. Stochastic

Processes and their Applications, 117:1689-1723.

Durrett, R. 2005. Probability: Theory and Examples, Third Edition. Duxbury Press, Belmont.

Federer, H. 1996. Geometric Measure Theory, Reprint of the 1969 Edition, Springer, Berlin.

Fu, M. C. 2000. Variance-Gamma and Monte Carlo. M. C. Fu, R. A. Jarrow, J.-Y. Yen, and R. J.

Elliott eds. Advances in Mathematical Finance. Boston, Birkhauser.

Fu, M. C., L. J. Hong, and J.-Q. Hu. 2009. Conditional Monte Carlo estimation of quantile

26

sensitivities. Management Science, 55, 2019-2027.

Fu, M. C., and J.-Q. Hu. 1997. Conditional Monte Carlo: Gradient Estimation and Optimization

Applications, Kluwer Academic Publishers, Boston, MA.

Glynn, P. W. 1987. Likelihood ratio gradient estimation: An overview. Proceedings of the 1987

Winter Simulation Conference, 366-374.

Gong, W. B., and Y. C. Ho. 1987. Smoothed perturbation analysis of discrete-event dynamic

systems. IEEE Transactions on Automatic Control, 32, 858-867.

Hammersley, J. M. 1956. Conditional Monte Carlo. Journal of the ACM, 3 73-76.

Ho, Y. C., and X.-R. Cao. 1983. Perturbation analysis and optimization of queueing networks.

Journal of Optimization Theory and Applications, 40 559-582.

Law, A. M., and W. D. Kelton. 2000. Simulation Modeling and Analysis, Third Edition, McGraw-

Hill.

Liu, G., and L. J. Hong. 2011. Kernel estimation of the Greeks for options with discontinuous

payoﬀs. Operations Research, 59 96-108.

L’Ecuyer, P. 1990. A uniﬁed view of the IPA, SF, and LR gradient estimation technique. Manage-

ment Science, 36, 1364-1383.

Luedtke, J., S. Ahmed. 2008. A sample approximation approach for optimization with probabilistic

constraints. SIAM Journal on Optimization, 19(2) 674-699.

Lyuu, Y.-D.,and H.-W. Teng. 2011. Unbiased and eﬃcient Greeks of ﬁnancial options. Finance

and Stochastics, 15 141-181.

Madan, D. B., P. Carr, and E. C. Chang. 1998. The variance gamma process and option pricing.

European Finance Review, 2 79-105.

Mal´y, J. 2001. Lectures on change of variables in integral. Preprint 305, Department of Mathe-

matics, University of Helsinki.

Morgan, F. 2009. Geometric Measure Theory: A Beginner’s Guide, 3rd Edition, Academic Press.

Pagnoncelli, B. K., S. Ahmed, A. Shapiro. 2009. Sample average approximation method for

chance constrained programming: Theory and applications. Journal of Optimization Theory

and Applications, 142 399-416.

Pﬂug, G. C. 1988. Derivatives of Probability Measures: Concepts and Applications to the Opti-

mization of Stochastic Systems. Springer, Berlin.

Pﬂug, G. C., and H. Weisshaupt. 2005. Probability gradient estimation by set-valued calculus and

applications in network design. SIAM Journal on Optimization, 15(3) 898-914.

Suri, R., and M. A. Zazanis. 1988. Perturbation analysis gives strongly consistent sensitivity

estimates for the M/G/1 queue. Management Science, 34, 39-64.

Tong, S., and G. Liu. 2016. Importance sampling for option Greeks with discontinuous payoﬀs.

INFORMS Journal on Computing, in press.

27

Wang Y., M. C. Fu, and S. I. Marcus. 2009. Sensitivity analysis for barrier options. Proceedings

of the 2009 Winter Simulation Conference, 1272-1282.

Wang, Y., M. C. Fu, and S. I. Marcus. 2012. A new stochastic derivative estimator for discontinuous

payoﬀ functions with application to ﬁnancial derivatives. Operations Research, 60(2) 447-460.

Wendel, J. G. 1957. Groups and conditional Monte Carlo. The Annals of Mathematical Statistics,

28 1048-1052.

A Appendix

In what follows we provide the proofs of the main results. To facilitate the proofs, an area formula

and a coarea formula for Hausdorﬀ integrations will be used repeatedly.

Interested readers are

referred to Mal´y (2001, Theorem 4.6, Exercise 3) for details of the area formula, and to Federer

(1996, Theorems 3.2.22 and 3.2.31) for those of the coarea formula.
Lemma 1 (Area Formula). Let E0 ⊂ Rm be an open set, and u : E0 7→ Rn be a continuously
diﬀerentiable one-to-one mapping, n ≥ m. Then for any Borel set E ⊂ E0 we have

J u

ZE
ZE

m(x)dx =Zu{E} Hm(dy) = Hm(u{E}).
p(y)Hm(dy),

m(x)dx =Zu{E}

p(u(x))J u

(22)

(23)

Furthermore,

for all Borel functions p : u{E} 7→ R for which one side exists.
Lemma 2 (Coarea Formula). Suppose n ≥ m ≥ k ≤ q are positive integers, A is an (Hm, m)
rectiﬁable Borel set of Rn, E ⊂ Rq, and the mapping ϕ : A → E is Lipschitz continuous. Then,
(a) If Hm−k(ϕ−1{z}) > 0 for Hk almost all z in E, then E is (Hk, k) rectiﬁable.
(b) If E is (Hk, k) rectiﬁable, then the set ϕ−1{z} is (Hm−k, m− k) rectiﬁable for Hk almost all z,

and

ZA

p(y)J ϕ

k (y)Hm(dy) =ZEZϕ−1{z}

p(y)Hm−k(dy)Hk(dz),

where p(·) is an Hm integrable function.
The area formula can be viewed as a generalization of the concept of change of variables in

integration. When n = m, it is the typically change-of-variables formula in calculus. This extension

to cases with n > m provides a powerful tool for computing integrals taken over m-dimensional

surfaces in the n-dimensional space, which is transformed into a Lebesgue integration over a subset
of Rm. In particular, the m-dimensional area of the image of a continuously diﬀerentiable mapping
u from a domain E ⊂ Rm into Rn is deﬁned as the integral of the Jacobian J u
m over E. The coarea
formula can be viewed as an extension of the Fubini’s Theorem to Hausdorﬀ measure, i.e., a double

integral can be computed using iterated integrals.

28

A.1 Proof of Theorem 1

Proof. Note that J u

m(x) 6= 0 for almost all x ∈ Ω up to a null set. Then,
p(x)f (x)

E [p(X)] = E [p(X)] =ZΩ

p(x)f (x)dx =ZΩ
J u
m(x)
Hm(dy),

Jm(u−1(y))

p(u−1(y))f (u−1(y))

= Zu{Ω}

where the last equality follows from the area formula in Lemma 1.

J u
m(x)dx

Note that by deﬁnition, u{Ω} is (Hm, m) rectiﬁable because u is a continuously diﬀerentiable

mapping. If ϕ{u{Ω}} is (Hk, k) rectiﬁable, applying the coarea formula in Lemma 2 yields

E [p(X)] =Zu{Ω}
= Zu{Ω}
= Zϕ{u{Ω}}Zϕ−1{z}

p(u−1(y))f (u−1(y))

J u
m(u−1(y))

Hm(dy)

p(u−1(y))f (u−1(y))J ϕ
k (y)

m(u−1(y))J ϕ
J u

k (y)

Hm(dy)
k (y) Hm−k(dy)Hk(dz),

p(u−1(y))f (u−1(y))
m(u−1(y))J ϕ
J u

(24)

(25)

(26)

where the last equality follows from the coarea formula in Lemma 2.

For w(z) deﬁned in Equation (11), it can be seen that

E [p(X)] =Zϕ{u{Ω}}
= Zϕ{u{Ω}}Zϕ−1{z}
= Zu{Ω}

w(ϕ(y))

w(z)Zϕ−1(z)

w(z)

f (u−1(y))
m(u−1(y))J ϕ
J u
f (u−1(y))
m(u−1(y))Hm(dy),
J u

f (u−1(y))
m(u−1(y))J ϕ
J u

k (y)Hm−k(dy)Hk(dz)

k (y)Hm−k(dy)Hk(dz)

where the last equality follows from the coarea formula in Lemma 2.

Then by the area formula in Lemma 1,

E [p(X)] =Zu(Ω)
= ZΩ

w(ϕ(y)) ·

f (u−1(y))
m(u−1(y))Hm(dy)
J u

w (ϕ(u(x))) f (x)dx = E [w (ϕ(u(X)))] .

The rest of proof will be devoted to the second half, i.e., Var [w (ϕ(u(X)))] ≤ Var [p(X)]. Because

these two integrands have the same mean, it suﬃces to show that

E(cid:2)w2 (ϕ(u(X)))(cid:3) ≤ E(cid:2)p2(X)(cid:3) .

29

Note that for any given z ∈ ϕ{u{Ω}},

w2(z) = "Zϕ−1{z}
≤ Rϕ−1{z}
= Zϕ−1{z}

p(u−1(y))f (u−1(y))
m(u−1(y))J ϕ
J u

k (y) Hm−k(dy)#2,"Zϕ−1{z}

f (u−1(y))
m(u−1(y))J ϕ
J u

k (y)Hm−k(dy)#2

f (u−1(y))
m(u−1(y))J ϕ
J u

k (y)Hm−k(dy)

p2(u−1(y))f (u−1(y))

m(u−1(y))J ϕ
J u

k (y) Hm−k(dy)Rϕ−1{z}
hRϕ−1{z}
k (y)Hm−k(dy)i2
k (y) Hm−k(dy),Zϕ−1{z}

f (u−1(y))
m(u−1(y))J ϕ
J u

p2(u−1(y))f (u−1(y))

m(u−1(y))J ϕ
J u

f (u−1(y))
m(u−1(y))J ϕ
J u

k (y)Hm−k(dy),
1(y) dyR q2

2(y) dy

m(u−1(y))J ϕ

k (y).

by letting

where the inequality follows from Cauchy-Schwarz inequality(cid:12)(cid:12)R q1(y)q2(y) dy(cid:12)(cid:12)2 ≤R q2
q2(y) =pf (u−1(y))/qJ u
q1(y) = p(u−1(y))pf (u−1(y))/qJ u

m(u−1(y))J ϕ

k (y),

Then similar to the arguments in Equations (25) and (26), we have

E(cid:2)w2 (ϕ(u(X)))(cid:3) =Zϕ{u{Ω}}
≤ Zϕ{u{Ω}}Zϕ−1{z}
= E(cid:2)p2(X)(cid:3) ,

w2(z)Zϕ−1(z)

f (u−1(y))
m(u−1(y))J ϕ
J u

k (y)Hm−k(dy)Hk(dz)

p2(u−1(y))f (u−1(y))

m(u−1(y))J ϕ
J u

k (y) Hm−k(dy)Hk(dz)

where the last equality follows from a similar argument as in Equation (24), and the proof is

completed.

A.2 Proof of Theorem 2

Proof. Note that by assumption, H1(ϕ−1{z}) > 0 for almost all z. Then by the ﬁrst part of Lemma
2, ϕ{u{Ω}} is (Hm−1, m − 1) rectiﬁable.

Applying Theorem 1, we have

α(ξ) = E [w (ϕ(u(X)); ξ)] .

Because w (ϕ(u(X)); ξ) satisﬁes the Lipschitz continuous condition w.r.t. ξ, by the dominated

convergence theorem (Durrett 2005), we interchange the order of diﬀerentiation and expectation,

leading to

α′(ξ) = E [∂ξw (ϕ(u(X)); ξ)] = E [ν (ϕ(u(X)); ξ)] ,

which completes the proof.

30

B Online Supplement

B.1 Derivation of Estimators for Section 5.1.1

B.1.1 Derivation under the Black-Scholes Model

Under the Black-Scholes (BS) model,

Xi+1 = Xi exp(cid:16)(r − σ2/2)T /m + σpT /mNi+1(cid:17) ,

i = 0, . . . , m − 1,

where {N1, . . . , Nm} are independent standard normal random variables, and the initial underlying
asset price X0 = x0 is a constant.

To simply notation, we let ¯X = Pm

τ = T /m. For i = 1, . . . , m − 1, the conditional density of Xi+1 given Xi = xi is

i=1 Xi/m, bX = max(X1, . . . , Xm), µ = r − σ2/2, and

fi+1(xi+1|xi) =

1

σxi+1√τ

φ(cid:18) 1

σ√τ

(log(xi+1/xi) − µτ )(cid:19) ,

(27)

where φ denotes the standard normal density function. Let x = (x1, . . . , xm). The joint density of

X = (X1, . . . , Xm) is

f (x) =

The Change-of-Variables Approach

m−1Yi=0

fi+1(xi+1|xi).

We apply Proposition 2 to derive change-of-variables estimators, where h(x) shall be speciﬁed

in the context.

• The digital option.

Note that dXm/dx0 = Xm/x0. By Theorem 1 of Liu and Hong (2011),

delta =

d
dx0

E(cid:2)e−rT 1{Xm≥K}(cid:3) = −

d
dK

E(cid:20)e−rT dXm

dx0

1{Xm≥K}(cid:21) =

d
dK

E(cid:20)e−rT Xm

x0

1{Xm≤K}(cid:21) .

Then setting h(x) = xm, g(x) = e−rT xm/x0, and applying Proposition 2, we have ¯Jm(z, t) =
1/tm−1, and

where Z , (Z1, . . . , Zm) = X/h(X), and for z = (z1, . . . , zm),

f1 (X1K/Xm|x0)(cid:21) ,

Xmx0

delta = E [ν(Z; K)] = E(cid:20)e−rT X1K
ν(z; ξ) = g(zξ)f (zξ)ξm−1(cid:30)Z ∞
= e−rT zmξ/x0f1(z1ξ)(cid:30)Z ∞

0

0

f (tz)tm−1dt

f1(z1t) dt

=

f1(z1ξ) =

f1(z1ξ),

e−rT zmz1ξ

x0

e−rT z1ξ

x0

where the last equality follows from zm = 1.

31

In a similar manner, it can be derived that

theta = E(cid:2)re−rT 1{Xm≥K}(cid:3) − E(cid:20)e−rT X1K
vega = E(cid:20)e−rT X1K

f1 (X1K/Xm|x0) (log(K/x0) − (µ + σ2)T )/σ(cid:21) ,

f1 (X1K/Xm|x0) (µT + log(K/x0))/(2T )(cid:21) ,
f1 (X1K/Xm|x0)(cid:0)log(X1K/x0Xm) − (µ + σ2)τ(cid:1)(cid:21) .

= E(cid:20) e−rT X1K

x2
0σ2τ Xm

gamma =

ddelta

Xm

Xm

dx0

where f1 is the conditional density of X1 given x0 as speciﬁed in (27).

• The Asian digital option.

¯X

¯Jm(z, t) = √m/tm−1. Applying Proposition 2, we have

The derivation is parallel to that for the digital option, except that h(x) =Pm
f1(cid:0)X1K/ ¯X|x0(cid:1)(cid:21) ,
delta = E(cid:20) e−rT X1K
theta = Ehre−rT 1{ ¯X≥K}i − E" e−rT X1K
vega = E" e−rT X1K
f1(cid:0)X1K/ ¯X|x0(cid:1) mXi=1
gamma = E(cid:20) e−rT X1K
f1(cid:0)X1K/ ¯X|x0(cid:1)(cid:18)log

f1(cid:0)X1K/ ¯X|x0(cid:1) mXi=1
¯X (cid:18)log
x0 ¯X − (µ + σ2)τ(cid:19)(cid:21) .

¯X (cid:18)log
x0 ¯X − (µ + σ2)iτ(cid:19)# ,

0σ2τ ¯X
x2

2mT ¯X

mσ ¯X

X1K

XiK

Xi

Xi

i=1 xm/m, and

XiK

x0 ¯X − µbiτ(cid:19)# ,

• The barrier call option.

Note that dXi/dx0 = Xi/x0 for i = 1, . . . , m. By Theorem 1 of Liu and Hong (2011),

delta =

d
dx0

Ehe−rT (Xm − K)+1{ bX≤κ}i
1{Xm≥K}1{ bX≤κ}(cid:21) −

d
dκ

= E(cid:20)e−rT Xm

x0

E"e−rT (Xm − K)+ bX

x0

1{ bX≤κ}# .

Similar to the derivation for the digital option, we apply Proposition 2 to derive change-

of-variables estimator for the second term on the right-hand-side of the above equation. In

d
dκ

Jacobian ¯Jm(z, t) = 1/tm−1, and Proposition 2 leads to

particular, we set h(x) = bx , max(x1, . . . , xm), and i∗ = argmax(X1, . . . , Xm). Then the
f1(cid:16)X1κ/bX|x0(cid:17)(cid:16)Xmκ/bX − K(cid:17)+(cid:21) ,
f1(cid:16)X1κ/bX|x0(cid:17)(cid:16)Xmκ/bX − K(cid:17)+(cid:21) .

E"e−rT (Xm − K)+ bX
delta = E(cid:20)e−rT Xm

1{ bX≤κ}# = E(cid:20) e−rT κX1
x0bX

1{Xm≥K}1{ bX≤κ} −

e−rT κX1

and thus

x0

x0

x0bX

32

Similarly it can be derived that

+

e−rT
2T

e−rT
2T

EhXm1{Xm≥K}1{ bX≤κ}(log(Xm/x0) + µT )i

(log(κ/x0) + µi∗τ )(cid:21) ,

E(cid:20) X1κ
bX
−E(cid:20)e−rT X1κ
σbX

theta = Ehre−rT (Xm − K)+1{ bX≤κ}i −
vega = Ehe−rT Xm1{Xm≥K}1{ bX≤κ}(cid:0)log(Xm/x0) − (µ + σ2)T(cid:1) /σi
gamma = E(cid:20) e−rT K 2X1f1(X1K/Xm|x0)

f1(cid:16)X1κ/bX|x0(cid:17)(cid:16)Xmκ/bX − K(cid:17)+
f1(cid:16)X1κ/bX|x0(cid:17)(cid:16)Xmκ/bX − K(cid:17)+(cid:0)log(κ/x0) − (µ + σ2)i∗τ(cid:1)(cid:21) ,
1{ bXK/Xm≤κ}(cid:21)
−E" e−rT X1Xmκ2
f1(X1κ/bX|x0)1{Xmκ/ bX≥K}#
0bX 2
−E" e−rT X1κ
f1(X1κ/bX)(Xmκ/bX − K)+(cid:18)log
bXx2

x0bX − (µ + σ2)τ(cid:19)# .

Xmx2
0

0σ2τ

X1κ

x2

The Likelihood Ratio Approach

Note that the likelihood ratios with respect to S0, σ and T are

L1(x) =

L2(x) =

L3(x) =

L4(x) =

log f (x) =

d
dx0
1

f (x)

d2
dx2
0

f (x) = L2

d
dT

d
dσ

log f (x) =

log f (x) =

1

1

x1

x2

1(x) −

x0 − µτ(cid:19) ,
x0σ2τ (cid:18)log
0σ2τ (cid:18)log
m−1Xi=0 " 1
2σ2τ 2(cid:18)log
m−1Xi=0 " 1
σ3τ (cid:18)log

1
m

xi+1

x1

x0 − µτ + 1(cid:19) ,
xi − µτ(cid:19)2

+

xi+1

xi − µτ(cid:19)2

−

1

σ(cid:18)log

1

2τ# ,

µ

xi+1

σ2τ (cid:18)log
xi − µτ(cid:19) −

xi − µτ(cid:19) −
σ# .

xi+1

1

• The digital option.

delta =

d
dx0
d
theta = −
dx0
d
dσ
d2
dx2
0

gamma =

vega =

E(cid:2)e−rT 1{Xm≥K}(cid:3) = E(cid:2)e−rT L1(X)1{Xm≥K}(cid:3) ,
E(cid:2)e−rT 1{Xm≥K}(cid:3) = E(cid:2)re−rT 1{Xm≥K}(cid:3) − E(cid:2)e−rT L3(X)1{Xm≥K}(cid:3) ,
E(cid:2)e−rT 1{Xm≥K}(cid:3) = E(cid:2)e−rT L4(X)1{Xm≥K}(cid:3) ,
E(cid:2)e−rT 1{Xm≥K}(cid:3) = E(cid:2)e−rT L2(X)1{Xm≥K}(cid:3) .

33

• The Asian digital option.

delta =

d
dx0
d
theta = −
dx0
d
dσ
d2
dx2
0

gamma =

vega =

Ehe−rT 1{ ¯X≥K}i = Ehe−rT L1(X)1{ ¯X ≥K}i ,
Ehe−rT 1{ ¯X≥K}i = Ehre−rT 1{ ¯X≥K}i − Ehe−rT L3(X)1{ ¯X≥K}i ,
Ehe−rT 1{ ¯X≥K}i = Ehe−rT L4(X)1{ ¯X≥K}i ,
Ehe−rT 1{ ¯X≥K}i = Ehe−rT L2(X)1{ ¯X ≥K}i .

• The barrier call option.

delta =

d
dx0
d
theta = −
dx0

Ehe−rT (Xm − K)+1{ bX≤κ}i = Ehe−rT L1(X)(Xm − K)+1{ bX≤κ}i ,
Ehe−rT (Xm − K)+1{ bX≤κ}i
= Ehre−rT (Xm − K)+1{ bX≤κ}i − Ehe−rT L3(X)(Xm − K)+1{ bX≤κ}i ,
Ehe−rT (Xm − K)+1{ bX≤κ}i = Ehe−rT L4(X)(Xm − K)+1{ bX≤κ}i ,
Ehe−rT (Xm − K)+1{ bX≤κ}i = Ehe−rT L2(X)(Xm − K)+1{ bX≤κ}i .

d
dσ
d2
dx2
0

vega =

gamma =

Conventional Conditional Monte Carlo Approach

• The digital option.

Conditioning on Xm−1 yields

E(cid:2)e−rT 1{Xm≥K}(cid:3) = E(cid:0)E(cid:2)e−rT 1{Xm≥K}|Xm−1(cid:3)(cid:1) = E(cid:20)e−rT(cid:18)1 − Φ(cid:18) 1

σ√τ (cid:18)log

where Φ denotes the standard normal distribution function.

K

Xm−1 − µτ(cid:19)(cid:19)(cid:19)(cid:21) ,

Using the pathwise method on the right-hand-side (RHS) of the above equation, we have

x0

(cid:21) ,
E(cid:2)e−rT Kfm(K|Xm−1) (log(K/x0) + µT )(cid:3) ,

delta = E(cid:20) e−rT Kfm(K|Xm−1)
theta = E(cid:2)re−rT 1{Xm≥K}(cid:3) −
# ,
vega = E" e−rT Kfm(K|Xm−1)(cid:0)log(K/x0) − (µ + σ2)T(cid:1)
gamma = E(cid:20) e−rT Kfm(K|Xm−1)(log(K/Xm−1) − (µ + σ2)τ )
(cid:21) .

x2
0σ2τ

1
2T

σ

• The Asian digital option.

34

i=1 Xi and Sm = mK − S−m. Conditioning on {X1, . . . , Xm−1}, we have

Let S−m =Pm−1
Ehe−rT 1{ ¯X≥K}i = EhEhe−rT 1{ ¯X≥K}|X1, . . . , Xm−1ii

= E(cid:20)e−rT(cid:18)1 − Φ(cid:18) 1

σ√τ (cid:18)log

mK − S−m

Xm−1

− µτ(cid:19)(cid:19)(cid:19) 1{S−m≤mK}(cid:21) .

Using the pathwise method on the RHS of the above equation, we have

delta = E(cid:20) e−rT mKfm(Sm|Xm−1)
theta = Ehre−rT 1{ ¯X≥K}i −

1
2T

x0

Sm(log(Sm/x0) + µT ) +

1{S−m<mK}(cid:21) ,
E(cid:2)e−rT fm(Sm|Xm−1)1{S−m<mK}
Xi(log(Xi/x0) + iµτ )# ,
m−1Xi=1

vega = E(cid:20) e−rT fm(Sm|Xm−1)

σ

1{S−m<mK}

 Sm(log(Sm/x0) − (µ + σ2)T ) +

Xi(log(Xi/x0) − i(µ + σ2)τ )!# ,

m−1Xi=1

gamma = E(cid:20) e−rT mKfm(Sm|Xm−1)

Smx2
0

1{S−m<mK}(cid:18)S−m − Sm +

mK(log (Sm/Xm−1) − µτ )

σ2τ

(cid:19)(cid:21) .

B.1.2 Derivation under the Variance Gamma Model

Under the variance gamma (VG) model,

Xi+1 = Xi exp(µβτ + θGi+1 + σpGi+1Ni+1),

i = 0, . . . , m − 1,

where τ = T /m, {G1, . . . , Gm} are independent gamma random variables with scale parameter
τ /β and shape parameter β, {N1, . . . , Nm} are independent standard normal random variables,
and µβ = r + 1/β log(1 − θβ − σ2β/2).

Note that for i = 0, . . . , m − 1, the conditional density of Xi+1 given Gi+1 and Xi = xi is

fi+1(xi+1|xi, Gi+1) =

1

xi+1σ√Gi+1

φ(cid:18)

1

σ√Gi+1(cid:18)log(cid:18) xi+1

xi (cid:19) − µβτ − θGi+1(cid:19)(cid:19) ,

and thus the conditional density of X = (X1, . . . , Xm) given G = (G1, . . . , Gm) is

f (x|G) =

m−1Yi=0

fi+1(xi+1|xi, Gi+1) =

1

xi+1σ√Gi+1

φ(cid:18)

m−1Yi=0

where φ denotes the standard normal density function.

The Change-of-Variables Approach

1

σ√Gi+1(cid:18)log(cid:18) xi+1

xi (cid:19) − µβτ − θGi+1(cid:19)(cid:19) ,

The derivation of the change-of-variables estimators is similar to that under the Black-Scholes

model, except that we are working with the conditional density of X given G, instead of the

35

unconditional density. Speciﬁcally, for a parameter η that X may depend on and a function l(·), it
holds that

d
dη

E [l(X)] =

d
dη

E (E [l(X)|G]) = E(cid:18) d

dη

E [l(X)|G](cid:19) ,

(28)

provided that E [l(X)|G] is a smooth function of G. For estimating price sensitivities under the
VG model, it can be veriﬁed that E [l(X)|G] is indeed smooth for the function l of concern, and
therefore (28) is justiﬁed. Then, Proposition 2 can be applied to dE [l(X)|G] /dη, leading to

d
dη

E [l(X)|G] = E [ψ(X, G)|G] ,

(29)

for an appropriate function ψ, and thus combining with (28) yields

d
dη

E [l(X)] = E (E [ψ(X, G)|G]) = E [ψ(X, G)] .

Note that conditional on G, the VG model has the same structure as the BS model. Therefore,

under the VG model, the way of applying Proposition 2 to derive ψ in (29) for various options is

exactly parallel to that under the BS model in Section B.1.1. The details are thus omitted.

i=1 Gi, γ = 1 − θβ − σ2β/2, and

¯X (cid:12)(cid:12)(cid:12)(cid:12) x0, G1(cid:19) , φ1,b = f1(cid:18) X1κ

• The digital option.

To further simplify notations, we let SG =Pm
Xm (cid:12)(cid:12)(cid:12)(cid:12) x0, G1(cid:19) , φ1,a = f1(cid:18) X1K
φ1,d = f1(cid:18) X1K
delta = E(cid:20) e−rT KX1
theta = E(cid:2)re−rT 1{Xm≥K}(cid:3) − E(cid:20) e−rT KX1φ1,d
vega = E(cid:20) e−rT KX1φ1,d
gamma = E(cid:20) e−rT KX1φ1,d

0σ2G1 (cid:18)log

φ1,d(cid:21) ,

Xmx2

2T Xm

x0Xm

Xm

(cid:18) log(K/x0) − µβT − θSG

(cid:18)θSG + µβT + log
γ (cid:19)(cid:21) ,
x0Xm − (µβτ + θG1 + σ2G1)(cid:19)(cid:21) .

KX1

−

T σ

σ

bX (cid:12)(cid:12)(cid:12)(cid:12) x0, G1(cid:19) .
x0(cid:19)(cid:21) ,

K

• The Asian digital option.

x0 ¯X

φ1,a(cid:21) ,

delta = E(cid:20) e−rT KX1
mXi=1 log
theta = Ehre−rT 1{ ¯X≥K}i − E" e−rT KX1φ1,a
vega = E" e−rT KX1
x0 ¯X − iµβτ − θSG(cid:19) −
¯X (cid:18) 1
σ(cid:18)log
x0 ¯X − (µβτ + θG1 + σ2G1)(cid:19)(cid:21) .
gamma = E(cid:20) e−rT KX1

mXi=1
φ1,a(cid:18)log

KXi
x0 ¯X

2mT ¯X

0σ2G1

¯Xx2

KX1

KXi

m ¯X

φ1,a

Xi

Gkθ!# ,

+ iµβτ +

Xi
¯X

iXk=1
γ (cid:19)# ,

iστ

36

x0

2T

Xm
x0

• The barrier call option. Let i∗ = argmax{X1, . . . , Xm}.
delta = E(cid:20) e−rT Xm
1{Xm≥K}1{ bX≤κ}(cid:21) − E(cid:20) e−rT κX1φ1,m
(Xmκ/bX − K)+(cid:21) ,
x0bX
+ µβT + θSG(cid:19) 1{Xm≥K}1{ bX≤κ}(cid:21)
(cid:18)log
theta = Ehre−rT (Xm − K)+1{ bX≤κ}i − E(cid:20) e−rT Xm
Gj ,
(Xmκ/bX − K)+log
i∗Xj=1
vega = E(cid:20)e−rT Xm(cid:18) log(Xm/x0) − µβT − θSG
γ (cid:19) 1{Xm≥K}1{ bX≤κ}(cid:21)
(Xmκ/bX − K)+  log(κ/x0) − µβi∗τ − θPi∗
1{κXm/ bX≥K}#
1{K bX/Xm≤κ}(cid:21) − E" e−rT κ2X1Xmφ1,b
0bX 2
x0bX − (µβτ + θG1 + σ2G1)(cid:19)# .
(Xmκ/bX − K)+(cid:18)log

+E e−rT κX1φ1,b
2T bX
−E"e−rT κX1φ1,m
gamma = E(cid:20) e−rT K 2X1φ1,d
−E"e−rT κX1φ1,b

γ !# ,

bXx2

+ µβi∗τ + θ

0σ2G1

x2
0Xm

j=1 Gj

bX

κ
x0

σi∗τ

κX1

−

−

σT

x2

σ

σ

The Likelihood Ratio Approach

For any function l, and a market parameter η, note that

d
dη

E [l(X)] =

d
dη

E (E [l(X)|G])

=

d
dη

E(cid:20)Z l(x)f (x|G) dx(cid:21) = E(cid:18)E(cid:20)l(X)

(cid:21) ,
because the integrationR l(x)f (x|G) dx is usually continuous in η even when l is discontinuous.

By elementary algebra, the conditional likelihood ratios with respect to S0, σ and T are

where the interchange of expectation and diﬀerentiation in the third equality is usually valid,

|G(cid:21)(cid:19) = E(cid:20)l(X)

d log f (X|G)

d log f (X|G)

dη

dη

L1(x|G) =

L2(x|G) =

L3(x|G) =

d
log f (x|G) =
dx0
1

d2
dx2
0

f (x|G)
d
dT

log f (x|G)

1

x0σ2G0(cid:18)log

f (x|G) = L2

1(x|G) −

= −

m
2T

+

mXi=1

(cid:16)log xi

2σ2T Gi

xi−1 − (µβτ + θGi)(cid:17)2
(cid:16)log xi

m
σ

+

mXi=1

L4(x|G) =

d
dσ

log f (x|G) = −

1

x1

x1

x2

x0 − (µβτ + θG1)(cid:19) ,
0σ2G1(cid:18)log
+(cid:16)log xi

x0 − (µβτ + θG1) + 1(cid:19) ,
xi−1 − (µβτ + θGi)(cid:17)(cid:16) µβ

x2
0σ2G1

m + θGi

T (cid:17)

,

xi−1 − (µβτ + θGi)(cid:17)2

σ3Gi

−(cid:16)log xi

xi−1 − (µβτ + θGi)(cid:17) τ

σGi(1 − θβ − σ2β/2)

.

37

• The digital option.

• The Asian digital option.

delta = E(cid:2)e−rT L1(X|G)1{Xm≥K}(cid:3) ,
theta = E(cid:2)re−rT 1{Xm≥K}(cid:3) − E(cid:2)e−rT L3(X|G)1{Xm≥K}(cid:3) ,
vega = E(cid:2)e−rT L4(X|G)1{Xm≥K}(cid:3) ,
gamma = E(cid:2)e−rT L2(X|G)1{Xm≥K}(cid:3) .
delta = Ehe−rT L1(X|G)1{ ¯X ≥K}i ,
theta = Ehre−rT 1{ ¯X≥K}i − Ehe−rT L3(X|G)1{ ¯X ≥K}i ,
vega = Ehe−rT L4(X|G)1{ ¯X ≥K}i ,
gamma = Ehe−rT L2(X|G)1{ ¯X ≥K}i .

• The barrier call option.

delta = Ehe−rT L1(X|G)(Xm − K)+1{ bX≤κ}i ,
theta = Ehre−rT (Xm − K)+1{ bX≤κ}i − Ehe−rT L3(X|G)(Xm − K)+1{ bX≤κ}i ,
vega = Ehe−rT L4(X|G)(Xm − K)+1{ bX≤κ}i ,
gamma = Ehe−rT L2(X|G)(Xm − K)+1{ bX≤κ}i .

Conventional Conditional Monte Carlo Approach

Deﬁne φm = fm ( K| Xm−1, Gm) in the following context.
• The digital option.

Conditioning on G and Xm−1 and then applying the pathwise method lead to

x0

i=1 Gi + µβT + log(K/x0)) Kφm

delta = E(cid:20) e−rT Kφm
(cid:21) ,
theta = E(cid:2)re−rT 1{Xm≥K}(cid:3)
−E(cid:20) e−rT (θPm
vega = E(cid:20)e−rT Kφm(cid:18) log(K/x0) − µβT − θPm
e−rT Kφm(cid:16)log K
gamma = E

(cid:21) ,
1 − θβ − σ2β/2(cid:19)(cid:21)
Xm−1 − (µβτ + θGm + σ2Gm)(cid:17)
 .

i=1 Gi

−

T σ

2T

σ

x2
0σ2Gm

38

i=1 Gi. To further simplify notation,

Conditioning on G and (X1, . . . , Xm−1) and then applying the pathwise method lead to

• The Asian digital option.

Let S−m =Pm−1
we let γ = 1 − θβ − σ2β/2, and

i=1 Xi, Sm = mK − S−m, and SG =Pm
bφm = fm(Sm|Xm−1, Gm).

x0

 Sm

log(Sm/x0) + µβT + θSG

1{S−m<mK}# ,

delta = E" e−rT mKbφm
theta = Ehre−rT 1{ ¯X≥K}i − Ehe−rTbφm1{S−m<mK}
Xi log
m−1Xi=1
vega = Ehe−rTbφm1{S−m<mK}
 Sm  log Sm
γ ! +
1{S−m<mK}−1 +
gamma = E e−rT mKbφm

x0 − µβT − θSG

m−1Xi=1

S−m
Sm

−

x2
0

T σ

2T

σ

+

Xi
x0

+ iµβτ + θ

Gk!!# ,

iXk=1

j=1 Gj

Xi  log Xi
mK(cid:16)log Sm

x0 − iµβτ − θPi
Xm−1 − (µβτ + θGm)(cid:17)

σ2GmSm

σ

+

−

iστ

γ !!# ,
 .

B.2 Gradient Estimators for the Chance Constrained Program Example

B.2.1 Multivariate Normal Distribution

Suppose that X = (X1, . . . , Xm) follows a multivariate normal distribution with mean zero and a

covariance matrix Σ. Its density function is

f (x) =

1

(2π)m/2 |Σ|−1/2e− 1

2 xT Σ−1x.

We ﬁrst derive a conventional CMC estimator. Let Σ1 denote the covariance matrix of (X1, t2X2+

. . . + tmXm), and set A , (aij)1≤i≤2,1≤j≤2 = Σ−1

1 . Conditioning on (X2, . . . , Xm), we have

Pr(cid:0)tT X ≤ b(cid:1)
= E"Φ √a11 b −Pm

i=2 tiXi
t1

+

a12
a11

mXk=2

tkXk!!# ,

where Φ denotes the standard normal distribution function. Then,

∂t1 Pr(cid:0)tT X ≤ b(cid:1)
= −E"√a11(b −Pm

t2
1

i=2 tiXi)

φ √a11  b −Pm

i=2 tiXi
t1

+

a12
a11

mXk=2

tkXk!!# ,

39

where φ denotes the standard normal density function.

Next we consider the change-of-variables estimator by (21). Deﬁne Z = X/(tT X). By (21),

−∞ |y|m−1e− 1

2 ZT Σ−1Zy2

dy(cid:21) .

2 ZT Σ−1Zb2.Z ∞
dy = 2Z ∞
ZT Σ−1Z(cid:19)m/2Z ∞
ZT Σ−1Z(cid:19)m/2

2

2

0

0

Γ(m/2),

ym−1e− 1

2 ZT Σ−1Zy2

dy

e−uum/2−1 du

2 ZT Σ−1Zy2

Note that

∂t1 Pr(cid:0)tT X ≤ b(cid:1)
= −E(cid:20)b|b|m−1Z1e− 1
Z ∞
−∞ |y|m−1e− 1
= (2π)−m/2|Σ|−1/2(cid:18)
= (2π)−m/2|Σ|−1/2(cid:18)
gamma function is deﬁned by Γ(t) =R ∞
b|b|m−1

Therefore,

∂t1 Pr(cid:0)tT X ≤ b(cid:1) = −

Γ(m/2)2m/2

0 yt−1e−y dy.

EhZ1(cid:0)ZT Σ−1Z(cid:1)m/2

e− 1

2 ZT Σ−1Zb2i

where the second to last equality follows from a change of variables u = ZT Σ−1Zy2/2, and the

B.2.2 Multivariate Student’s t-Distribution

Suppose that X = (X1, . . . , Xm) follows a multivariate Student’s t-distribution with mean zero, a

covariance matrix Σ, and v degrees of freedom. Its density function is

where

f (x) =

C =

C

(1 + xT Σ−1x/v)(v+m)/2

,

Γ((v + m)/2)

Γ(v/2)vm/2πm/2|Σ|1/2

.

Equivalently, one may generate a multivariate normal distribution Y = (Y1, . . . , Ym) with mean
zero and covariance Σ and a chi square random variable χ2
v with v degrees of freedoms, and set

X = Ypv/χ2

v.

We ﬁrst derive a conventional CMC estimator. Let Σ1 denote the covariance matrix of (Y1, t2Y2+

. . . + tmYm), and set A , (aij)1≤i≤2,1≤j≤2 = Σ−1

1 . Conditioning on (Y2, . . . , Ym) and χ2

v, we have

v(cid:17)
Pr(cid:0)tT X ≤ b(cid:1) = Pr(cid:16)tT Y ≤ b/pv/χ2
= E"Φ √a11  b/pv/χ2
v −Pm

t1

i=2 tiYi

+

a12
a11

mXk=2

tkYk!!# .

40

Then,

∂t1 Pr(cid:0)tT X ≤ b(cid:1)
= −E"√a11(b/pv/χ2

t2
1

v −Pm

i=2 tiYi)

φ √a11  b/pv/χ2

v −Pm

t1

i=2 tiYi

+

a12
a11

mXk=2

tkYk!!# .

Next we consider the new CMC estimator by (21). Deﬁne Z = X/(tT X). By (21),

∂t1 Pr(cid:0)tT X ≤ b(cid:1)
= −E"

Note that

Z ∞

−∞

=

=

=

−∞

|y|m−1

|y|m−1

b|b|m−1Z1

(1 + ZT Σ−1Zy2/v)(v+m)/2 dy# .

(1 + ZT Σ−1Zb2/v)(v+m)/2,Z ∞
dy = 2Z ∞
2Z ∞
(1 + w2/(v + m − 1))(v+m)/2
C1Γ(m/2)Γ(v/2)(v + m − 1)(m−1)/2
√πΓ((v + m − 1)/2)

(1 + ZT Σ−1Zy2/v)(v+m)/2

wm−1

0

0

vm/2

vm/2

dy

dw

ym−1

(1 + ZT Σ−1Zy2/v)(v+m)/2

(v + m − 1)m/2 (ZT Σ−1Z)m/2

(v + m − 1)m/2 (ZT Σ−1Z)m/2

vm/2Γ(m/2)Γ(v/2)

Γ((v + m)/2) (ZT Σ−1Z)m/2

,

where C1 = Γ((v + m)/2).hp(v + m − 1)πΓ((v + m − 1)/2)i , and the second to last equality

follows from the closed form formula of the raw moment for t-distribution.

Therefore,

∂t1 Pr(cid:0)tT X ≤ b(cid:1) = −E"

(1 + ZT Σ−1Zb2/v)(v+m)/2 vm/2Γ(m/2)Γ(v/2)# .
b|b|m−1Z1Γ((v + m)/2)(cid:0)ZT Σ−1Z(cid:1)m/2

41

