Predicting Coronal Mass Ejections Using Machine Learning Methods

W.W. Hansen Experimental Physics Laboratory, Stanford University, Stanford, CA 94305

M. G. Bobra and S. Ilonidis

ABSTRACT

Of all the activity observed on the Sun, two of the most energetic events are ﬂares and
Coronal Mass Ejections (CMEs). Usually, solar active regions that produce large ﬂares
will also produce a CME, but this is not always true (Yashiro et al., 2005). Despite
advances in numerical modeling, it is still unclear which circumstances will produce a
CME (Webb & Howard, 2012). Therefore, it is worthwhile to empirically determine
which features distinguish ﬂares associated with CMEs from ﬂares that are not. At
this time, no extensive study has used physically meaningful features of active regions
to distinguish between these two populations. As such, we attempt to do so by using
features derived from [1] photospheric vector magnetic ﬁeld data taken by the Solar
Dynamics Observatory’s Helioseismic and Magnetic Imager instrument and [2] X-ray
ﬂux data from the Geostationary Operational Environmental Satellite’s X-ray Flux
instrument. We build a catalog of active regions that either produced both a ﬂare and
a CME (the positive class) or simply a ﬂare (the negative class). We then use machine-
learning algorithms to [1] determine which features distinguish these two populations,
and [2] forecast whether an active region that produces an M- or X-class ﬂare will also
produce a CME. We compute the True Skill Statistic, a forecast veriﬁcation metric,
and ﬁnd that it is a relatively high value of ∼0.8±0.2. We conclude that a combination
of six parameters, which are all intensive in nature, will capture most of the relevant
information contained in the photospheric magnetic ﬁeld.

6
1
0
2

 
r
a

 

M
1
1

 
 
]

.

R
S
h
p
-
o
r
t
s
a
[
 
 

1
v
5
7
7
3
0

.

3
0
6
1
:
v
i
X
r
a

– 2 –

1.

Introduction

A Coronal Mass Ejection (CME) is a rapid ejection of magnetic ﬂux and plasma from the
Sun into interplanetary space (Lin & Forbes, 2000). They are typically observed in coronagraphs,
instruments that block out the solar disk to observe the corona, as an arc of bright light streaking
through space. Over the last couple decades, there has been a general consensus that CMEs and
ﬂares are in fact related as part of “a single magnetically-driven event” (Webb & Howard, 2012),
wherein a ﬂare unassociated with a CME is often referred to as a conﬁned or compact ﬂare. In
general, the more energetic a ﬂare, the more likely it is to be associated with a CME (Yashiro et
al., 2005) — but this is not, by any means, a rule. For example, Sun et al. (2015) found that the
largest active region in the last 24 years, which produced 6 X-class ﬂares, did not produce a single
CME.

It is unclear why some solar activity triggers both a ﬂare and a CME and other activity simply
triggers a conﬁned ﬂare. Though many numerical models describe the physical processes by which
ﬂares and CMEs are produced, outstanding questions remain about the magnetic topologies of
these eruptive events and how they release so much energy so fast (Webb & Howard, 2012). In
the absence of a deﬁnitive physical theory, it is worthwhile to empirically determine which features
distinguish ﬂares associated with CMEs from ﬂares that are not.

One way to distinguish between the two is by use of a machine-learning algorithm. One
speciﬁc class of machine-learning algorithms, known as supervised binary classiﬁers, will, given a
set of features characterizing an event, predict which of two classes an event belongs to. In other
words, the classiﬁer learns from some data for which it already knows the answer, and applies this
knowledge to predict an answer for other, similar data. One such algorithm is called a Support
Vector Machine (SVM), which is advantageous for its ability to predict which class an event falls into
even in cases where the number of features is large. SVMs can also identify non-linear relationships
between features (by using kernels to map these features into a higher dimensional space) and thus
may potentially unearth previously unknown properties in the data.

Various studies report correlations between features of solar active regions and CME produc-
tivity. Qahwaji et al. (2008) used a non-linear SVM to predict whether or not a ﬂare will initiate
a CME. However, their study only used the duration of these events, rather than physically mean-
ingful quantities, to distinguish between the populations. Falconer et al. (2008) report correlations
that are linear within their feature space between physically meaningful parameterizations of the
line-of-sight component of the magnetic ﬁeld, as measured by the Michelson Doppler Imager (MDI)
on the Solar and Heliospheric Observatory (SoHO), near active region neutral lines and used these
to predict CMEs with a 75% success rate. Falconer et al. (2006) reached a similar conclusion using
slightly diﬀerent parameterizations of active region neutral lines and vector magnetic ﬁeld data
from the Marshall Space Flight Center vector magnetograph. However, these studies limit their
exploration to solely CME-productive active regions. As far as we can tell, no study has used
physically meaningful features derived from the photospheric magnetic ﬁeld to distinguish between

– 3 –

ﬂares that are CME productive and those that are not.

To this end, we use a [1] SVM and features derived from photospheric vector magnetic ﬁeld
data taken by the Solar Dynamics Observatory’s (SDO) Helioseismic and Magnetic Imager (HMI)
instrument to forecast whether an active region that produces an M1.0-class ﬂare or higher will
also produce a CME, and [2] feature selection algorithm to determine which features distinguish
these two populations. This paper is organized as follows: in Section 2, we describe the solar data
and the event catalog; in Section 3, we describe the application of the machine-learning algorithms
to these data; and, in Section 4, we present our results.

The code used to conduct this study is publicly and permanently available in the Stanford
Digital Repository1; the data used to conduct this study are all public and their repositories are
detailed in Section 2. Both the feature selection and SVM implementations are from the python
scikit-learn library (Pedregosa et al., 2011), which is open-source, widely-used, and well-established.

2. Data

In May 2010, the SDO HMI instrument began producing full-disk vector magnetic ﬁeld data
It is the ﬁrst space-based instrument to continuously measure the full-disk
every 720 seconds.
photospheric vector magnetic ﬁeld (Schou et al., 2012). In 2014, the HMI team released a data
product, called Space-weather HMI Active Region Patches (SHARPs), which is publicly available
at the Joint Science Operations Center (JSOC)2. The SHARP data contains a vector magnetic ﬁeld
map of every single solar active region observed since SDO’s launch, each of which are automatically
tracked for its entire lifetime, and 18 keywords that parameterize the vector magnetic ﬁeld within
these regions (Bobra et al., 2014).

The SHARP parameterizations include measurements of physical quantities, such as the mag-
netic ﬂux contained in an active region; they also include proxies of physical quantities, such as
the current helicity, which cannot be measured in its entirety within a single plane. Further details
about the deﬁnition of and motivation behind each parameter can be found in Bobra et al. (2014).
A list of all 18 SHARP features is included in Table 1. Between 2010 May 1 and 2015 May 1, the
HMI data processing pipeline automatically detected 3023 active regions and computed 18 param-
eterizations of each active region at every 720-second interval throughout its lifetime, resulting in
∼36 million unique data points.

We use data from the SoHO Large Angle and Spectrometric Coronagraph Experiment (LASCO)
instrument and both coronagraphs on the Solar Terrestrial Relations Observatory (STEREO) Sun
Earth Connection Coronal and Heliospheric Investigation (SECCHI) instruments to determine

1https://purl.stanford.edu/wt605kh4712

2http://jsoc.stanford.edu

– 4 –

s
r
u
o
h

8
4

s
r
u
o
h

4
2

g
n
i
k
n
a
R
e
r
u
t
a
e
F

g
n

i
l
a
c
S

3
1

2

5

8
1

2
1

9

6

1
1

0
1

3

4
1

7

1

5
1

7
1

4

9
1

8

6
1

1

2

3

4

5

6

7

8

9

0
1

1
1

2
1

3
1

4
1

5
1

6
1

7
1

8
1

9
1

I

I

I

I

I

I

I

E

I

E

I

E

E

E

E

E

r
e
h
t
i
e
n

E

I

(cid:12)(cid:12)(cid:12)(cid:12)

2
(cid:17)
z
B
∂
(cid:16)

A
d
z
J

y
∂

+

x
∂

+

2
(cid:17)
z
B
∂
(cid:16)
(cid:114)

A
d
z
J
(cid:80)

t
o
P
B

t
o
P
(cid:126)B

·
s
b
O
(cid:126)B

|

|

◦

2
(cid:17)
t
o
P
(cid:126)B
−

s
b
O
B

|

s
b
O
(cid:126)B

s
o
c
c
r
a
(cid:16)
(cid:80)
∝

1N

(cid:12)(cid:12)(cid:12)(cid:12)

i

B

y
∂

z
B

A
d

−

−z(cid:88)

k
s
a
m
R
n
h
t
i
w
A
d
A
s
z
l
|
e
d
B
z
A
x
J
|
|
+z(cid:88)
S
i
|
d
P
(cid:80)
o
(cid:80)
|
L
z
M
(cid:80)
B
B
∝
=
∝
C
=
(cid:80)
(cid:80)
l
=
a
a
t
=
=
o
e
C
t
r
z
A
Φ
Φ
F
J

l
a
t
o
T
(cid:17)
/
x
(cid:17)
B
5
∂
h
4
B
>
(cid:16)
y
r
a
B
e
∂
h
(cid:16)
S
(cid:80)
h
t
i
w
a
e
r
A

z
B
(cid:80)

(cid:80)
1N

∝
z
J

|
z
J
·

s
b
a
c
H

n
a
t
c
r
a

l
a
t
o
t
c

t
o
t
ρ

=
γ

1N

H

x
∂

B

|

|

|

∝

m
u
s
z
J

|
z
(cid:12)(cid:12)(cid:12)(cid:12)
J
·

(cid:12)(cid:12)(cid:12)(cid:12)

2
(cid:17)
h
B
∂
(cid:16)

y
∂

+

2
(cid:17)
B
∂
(cid:16)

+

a
e
r
A

(cid:19)
y
∂

|

2
(cid:17)
h
a
B
l
u
∂
m
(cid:16)
r
(cid:114)
o
F

(cid:80)

x
∂

2
(cid:17)
t
o
P
(cid:126)B
−

2
(cid:17)
B
∂
(cid:16)
(cid:114)
z
J
·

x
∂

z
B
(cid:80)

(cid:18)

s
b
O
(cid:126)B

2z
B
z
B
(cid:80)
(cid:80)

·
z
J
(cid:80)

1N

1N

(cid:16)
(cid:80)

=

(cid:80)

=

1N

∝

|
h
B
∇

|

∝
c

H

l
a
t
o
t
α

|
t
o
t
B
∇

|

1N

∝
ρ

=
Γ

.
s
e
r
o
c
s
-
F

d
n
a

s
e
r
u
t
a
e
F

.
1

e
l

b
a
T

)
n
o
i
t
u
b
i
r
t
n
o
c

z
B
(

y
t
i
c
i
l
e
h

t
n
e
r
r
u
c

n
a
e
M

α

,
r
e
t
e
m
a
r
a
p

t
s
i
w
t

c
i
t
s
i
r
e
t
c
a
r
a
h
c

n
a
e
M

d
l
e
ﬁ

l
a
t
n
o
z
i
r
o
h

f
o

t
n
e
i
d
a
r
g

n
a
e
M

y
g
r
e
n
e

e
e
r
f

c
i
t
e
n
g
a
m
c
i
r
e
h
p
s
o
t
o
h
p

n
a
e
M

d
l
e
ﬁ

l
a
t
o
t

f
o

t
n
e
i
d
a
r
g

n
a
e
M

◦

5
4
>
r
a
e
h
S

h
t
i
w
a
e
r
A

f
o

n
o
i
t
c
a
r
F

e
l
g
n
a

r
a
e
h
s

n
a
e
M

y
t
i
s
n
e
d

y
g
r
e
n
e

e
e
r
f

c
i
t
e
n
g
a
m
c
i
r
e
h
p
s
o
t
o
h
p

l
a
t
o
T

n
o
i
g
e
r

e
v
i
t
c
a

e
h
t

n

i

s
l
e
x
i
p

d
l
e
ﬁ

g
n
o
r
t
s

f
o

a
e
r
A

y
t
i
c
i
l
e
h

t
n
e
r
r
u
c

t
e
n

e
h
t

f
o

e
u
l
a
v

e
t
u
l
o
s
b
A

e
n

i
l

n
o
i
s
r
e
v
n

i

y
t
i
r
a
l
o
p

r
a
e
n

x
u
ﬂ

f
o
m
u
S

y
t
i
c
i
l
e
h

t
n
e
r
r
u
c

d
e
n
g
i
s
n
u

l
a
t
o
T

s
s
a
l
C
e
r
a
l
F

y
t
i
s
n
e
d

t
n
e
r
r
u
c

l
a
c
i
t
r
e
v

n
a
e
M

x
u
ﬂ

d
e
n
g
i
s
n
u

l
a
t
o
T

l
a
i
d
a
r

m
o
r
f

d
l
e
ﬁ

f
o

e
l
g
n
a

n
a
e
M

t
n
e
r
r
u
c

l
a
c
i
t
r
e
v

d
e
n
g
i
s
n
u

l
a
t
o
T

h
b
g
n
a
e
m

h
z
j
n
a
e
m

p
l
a
n
a
e
m

t
b
g
n
a
e
m

t
o
p
n
a
e
m

r
h
s
n
a
e
m

5
4
t
g
r
h
s

t
o
p
t
o
t

d
z
j
n
a
e
m

x
u
l
f
s
u

m
a
g
n
a
e
m

z
j
s
u
t
o
t

h
z
j
n
s
b
a

r
c
a

a
e
r
a

e
u
l
a
v

r

h
j
s
u
t
o
t

y
t
i
r
a
l
o
p

r
e
p

t
n
e
r
r
u
c

t
e
n

e
h
t

f
o

l

s
u
u
d
o
m
e
h
t

f
o
m
u
S

p
p
c
n
v
a
s

n
o
i
t
p
i
r
c
s
e
D

d
r
o
w
y
e
K

1N

=

|
z
B
∇

|

d

l
e
ﬁ

l
a
c
i
t
r
e
v

f
o

t
n
e
i
d
a
r
g

n
a
e
M

z
b
g
n
a
e
m

s
e
t
a
c
i
d
n

i

n
m
u
l
o
c

g
n
i
l

a
c
S

e
h
T

.
s
e
i
r
e
s

a
t
a
d

P
R
A
H
S

e
h
t

n

i

d
r
o
w
y
e
k

r
e
d
a
e
h

S
T
I
F

e
h
t

f
o

e
m
a
n

e
h
t

s
e
t
a
c
i
d
n

i

n
m
u
l
o
c

d
r
o
w
y
e
K

e
h
T
—

.
e
t
o
N

e
r
a
l
F

e
h
t

d
e
l
l
a
c

,
e
r
u
t
a
e
f

e
n
O

.
)
9
0
0
2
(

.
l
a

t
e

h
c
s
l
e

W

r
e
p

r
e
n
n
a
m

)
I
(

e
v
i
s
n
e
t
n

i

r
o

)
E
(

e
v
i
s
n
e
t
x
e

n
a

n

i

s
e
l
a
c
s

n
o
i
t
s
e
u
q

n

i

r
e
t
e
m
a
r
a
p

e
h
t

r
e
h
t
e
h
w

o
t

t
p
e
c
n
o
c

n

i

r
a
l
i

m

i
s

s
i

s
s
a
l
c

e
r
a
ﬂ

e
h
T

.
a
t
a
d

S
E
O
G
e
h
t

m
o
r
f

d
e
t
u
p
m
o
c

s
i

s
i
h
t

,
r
e
h
t
a
r

;

d
r
o
w
y
e
k

r
e
d
a
e
h

S
T
I
F
P
R
A
H
S

a

s
a

e
l
b
a
l
i
a
v
a

t
o
n

s
i

,
s
s
a
l
C

e
r
e
h
w

,

M
C
=
C
F

:
s
w
o
l
l
o
f

s
a

C
F

,
s
s
a
l
c

e
r
a
ﬂ

e
h
t

e
n
ﬁ
e
d

e

W

.
e
r
a
ﬂ

e
n
o

r
o
f

d
e
t
a
l
u
c
l
a
c

s
i

t
i

t
p
e
c
x
e

,
)
6
9
9
1
(

´a
v
o
l
a
t
n
A
n

i

d
e
b
i
r
c
s
e
d

,
x
e
d
n

i

e
r
a
ﬂ

e
h
t

.
e
r
a
ﬂ

e
h
t

f
o

e
d
u
t
i
n
g
a
m
e
h
t

s
i

M
d
n
a

s
e
r
a
ﬂ

s
s
a
l
c
-

M

f
o

e
s
a
c

e
h
t

n

i

0
.
1

d
n
a

s
e
r
a
ﬂ

s
s
a
l
c
-
X

f
o

e
s
a
c

e
h
t

n

i

0
.
0
1

s
l
a
u
q
e

t
a
h
t

t
n
a
t
s
n
o
c

a

s
i

C

– 5 –

whether or not an active region produced a CME. These data are compiled into a NASA Space
Weather Research Center database called Space Weather Database Of Notiﬁcation, Knowledge,
Information (DONKI)3. This database contains a table of solar ﬂares and, within this table, a
column indicating whether or not any given ﬂare produced a CME. We query this database for
events between 2010 May 1 and 2015 May 1 to build our positive class; as such, the positive class is
deﬁned as events for which a ﬂaring active region also produced a CME. We do not consider ﬂares
below the M1.0-class, as they release a limited amount of energy and thus are unlikely to disturb
the near-Earth environment. Some ﬂares in this database are not associated with an active region.
As it is not possible to compute features for events unassociated with an active region, we reject
these ﬂares from our sample.

We use data from the X-ray Flux instrument aboard the Geostationary Operational Environ-
mental Satellite (GOES) to determine whether or not an active region produced an X- or M-class
ﬂare. The GOES ﬂare list can be queried from the Heliophysics Events Knowledgebase4 (Hurlburt
et al., 2012) using a python library called SunPy5 (SunPy Community et al., 2015). We use this
database to build our negative class by compiling a list of all the M1.0-class ﬂares that occurred
between 2010 May 1 and 2015 May 1 and then removing from this list the events in the positive
class. As such, the negative class is deﬁned as events for which a ﬂaring active region did not
produce a CME. We also include an additional feature from this database called the ﬂare class.
The formula for the ﬂare class is described in Table 1. Some ﬂares in the GOES ﬂare list are not
associated with an active region. We reject these ﬂares from our sample. We also reject all ﬂares
that are not within ±70◦ of central meridian during the GOES X-ray Flux peak time as the the
signal-to-noise in the HMI vector magnetic ﬁeld data decreases signiﬁcantly beyond this longitude.

We then compile all 18 SHARP active region features that characterize every event in both the
positive and negative class at exactly t hours before the GOES X-ray ﬂare peak time, where t ranges
from from 4 to 48 hours in 4 hour intervals. In other words, we use features from exactly t hours
before an event to predict the event. These prediction times are somewhat arbitrary, but the t=24
and t=48 hour cases are common in the literature (e.g. Ahmed et al., 2013) and may be a useful
lead time in an operational setting. We further restrict the number of events to those where the [1]
absolute value of the radial velocity of SDO is less than 3500 m/s (see Section 7.1.2 of Hoeksema
et al. (2014) for a discussion about the periodicity in magnetic ﬁeld strength due to the orbital
velocity of SDO) and [2] HMI data are of high quality (see Section Appendix A of Hoeksema et
al. (2014) for a discussion about HMI data quality; we essentially throw out observables produced
during bad conditions) at this time. As such, we are left with 56 events in the positive class and
364 events in the negative class for the time period between 2010 May 1 and 2015 May 1.

3http://kauai.ccmc.gsfc.nasa.gov/DONKI/

4http://www.lmsal.com/hek

5http://docs.sunpy.org/en/stable/code ref/instr/goes.html

– 6 –

In the case of predicting which ﬂaring active regions produce a CME, we expect that the
number of events in the positive class is much smaller than the number in the negative class.
Yashiro et al. (2005) showed that while more than 80% of X-class ﬂares are associated with CMEs,
this number drops as a power law with decreasing ﬂare class. On average, about 60% of M-class
ﬂares produce a CME; even so, there is a great disparity across the M-class, where M1.0-class to
M1.8-class ﬂares are only ∼44% likely to produce a CME. As such, we have ∼6.5 times more events
in the negative class than in the positive one: our negative class includes 364 events, 230 of which
are within the M1-range, whereas our positive class includes 56, where 7 are within the M1-range.

3. Algorithms

3.1. Feature Selection

Some of the features within a dataset may be powerful in distinguishing between the positive
and negative class, whereas others may be redundant or irrelevant. To identify features in the
former category, we use a univariate feature selection method, which is implemented in the feature
selection module of the scikit-learn library, for feature scoring. This involves computing an F-score,
which is a ratio between two diﬀerent measures of variance within the data (e.g. Section 4.3 of of
Rietveld & Hout, 2005). For two groups (i.e. the positive and negative class), the F-score for any
given feature i is given by:

(cid:104)(cid:80)n+

n+(¯x+
j=1(x+

i − ¯xi)2 + n−(¯x−
j,i − ¯x+

i )2 +(cid:80)n−

i − ¯xi)2
j=1(x−

i )2(cid:105)

j,i − ¯x−

F (i) =

1

N−2

(1)

is the average of the values of feature i over the positive-class examples, ¯x−

where ¯x+
is the average
i
of the values over the negative-class examples, ¯xi is the average of the values over the entire dataset,
n+ is the total number of positives examples in the dataset, n− is the number of negative examples,
and N is the total number of examples. The numerator measures the variance between each class
for a given feature, and the denominator measures the variance within each class for a given feature.
If this ratio is small, both groups have a similar population mean. If this ratio is large, both groups
have diﬀerent population means. The univariate feature selection method ranks features according
to their F-score.

i

3.2. Classiﬁcation

Our predictive model is based on a SVM, which has been used in many ﬂare prediction studies
(e.g. Li et al., 2007; Ahmed et al., 2013; Bobra & Couvidat, 2015). The SVM algorithm is a
non-probabilistic binary classiﬁer that determines whether the events in the positive and negative
class are separable. More speciﬁcally, given a set of N observations

– 7 –

(cid:16)

(cid:17)

(2)
with i ∈ 1, 2, ..., N , where d is an integer representing the number of features, and a corresponding
set of labels y(i) = ±1, where +1 is a positive example and −1 is a negative one, the SVM algorithm
tries to ﬁnd a hypersurface that separates the two classes.

d

x(i) =

x(i)
1 , x(i)

2 , ..., x(i)

This can be accomplished by enlarging the original d-dimensional feature space (cid:126)x, in which the
decision boundary between the two classes may be non-linear, into a higher dimensional feature
space φ((cid:126)x), in which the decision boundary between the two classes may be linear, i.e. the decision
boundary is the hyperplane ωT φ((cid:126)x) + b = 0. Here, φ((cid:126)x) is the function that maps the original data

into the higher dimensional space. One way to do this is by use of kernel functions K(cid:0)(cid:126)x(i), (cid:126)x(j)(cid:1)

deﬁned as:

(cid:126)x(i), (cid:126)x(j)(cid:17)
(cid:16)

K

= exp

(cid:32)

−γ

d(cid:88)

(cid:16)

k=1

(cid:17)(cid:33)

k − x(j)
x(i)

k

(3)

where γ is a positive tuning parameter that controls how far the inﬂuence of a single training
observation extends to aﬀect the classiﬁcation of a new observation.

variables εi, the sum(cid:80)N

If the observations of the positive and negative class can be perfectly separated with a hyper-
plane, the SVM algorithm will ﬁnd the maximal margin hyperplane, which is the hyperplane that
has the farthest minimum distance to the observations in the dataset. However, if there exists no
hyperplane that perfectly separates the positive class from the negative one, the SVM algorithm
will allow a certain number of observations, called support vectors, to be on the wrong side of
the margin or hyperplane. Each one of these observations is associated with a non-negative slack
variable εi that measures the degree of misclassiﬁcation of the ith observation. If there are N slack
i=1 is the total error of the support vector observations. Generally, there is
a tradeoﬀ between the total error of the support vectors and the width of the margin between the
positive and negative class. If the margin is wide, many observations violate the margin and the
total error is too large. If, on the other hand, a solution includes only a small number of support
vectors with low total error, the margin between the positive and negative class will be too narrow.
This can be expressed as an optimization problem, where the vectors ω and ε in the cost function,
deﬁned as:

N +(cid:88)

N−(cid:88)



cost =

ωT ω + C1

εi + C2

εi

i=1

i=1

2

 1
y(i)(cid:16)

are minimized according to the following constraint:

(cid:17) ≥ 1 − εi

ωT φ((cid:126)x(i)) + b

(4)

(5)

– 8 –

where C1 and C2 are non-negative tuning parameters for the positive and negative class, and N +
and N− are the number of observations in the positive and negative class. The ﬁrst term in
Equation 4 is the inverse of the margin and the remaining terms are the total error of the support
vectors. The SVM algorithm solves this optimization problem until the cost function of Equation
4 varies by less than a speciﬁed tolerance level. Once this optimization problem is solved, a new
observation can be classiﬁed by determining on which side of the hyperplane it lies.

In the case that N + (cid:39) N−, C1 can be set equal to C2.

If, as in our case, N + (cid:28) N−,
the algorithm is more likely to favor the majority class. To favor both classes equally under
these circumstances, we assign C1 to be greater than C2. In this way, the algorithm is receives a
greater penalty for misclassifying minority class events than misclassifying majority class ones, and
therefore does not neglect the minority class.

4. Results

4.1. Feature Selection Results

Table 1 lists the ranking for each feature, for a prediction both 24 and 48 hours before the
GOES X-ray ﬂare peak time. The features are ordered such that the feature with the highest
predictive power in the t=24 hour case, which is the mean gradient of the horizontal ﬁeld, is listed
ﬁrst, and the remaining features are arranged in order of decreasing rank. The absolute value of
the net current helicity has the highest predictive power for the t=48 hour case.

Of all of the features listed in Table 1, the ﬂare class, which we know from a priori constructing
a dataset that only includes ﬂaring active regions, is the only one that does not characterize an
active region at a time t before an event. Rather, the ﬂare class can only be determined after an
event. However, including the ﬂare class in our feature selection algorithm allows us the opportunity
to determine whether eruption potential (i.e. the probability of a ﬂaring active region producing
an associated CME) is simply a function of ﬂare size.

We ﬁnd that the ﬂare class is ranked 17 in the t=24 hour case and 19 in the t=48 hour case.
We surmise that there are two reasons for such a result:
[1] that our dataset is heavily biased
by M-class ﬂares (in the positive class, 73% of our examples include M-class ﬂares; this number
is 98% for the negative class), and [2] eruption potential is not solely a function of more energy
within an active region system. Because the ﬂare class is the only feature that cannot be used in
an operational setting, we do not include the ﬂare class in our predictive model.

We also ﬁnd that some of the features change rank depending on the prediction time. For
example, the feature which is highest ranked in the t=24 case ends up with a rank of 13 in the t=48
hour case. We ﬁnd that this feature is the highest-performing for all prediction times less than 24
hours and is one of the lowest-performing for all prediction times between 24 and 48 hours. This
may be a signature of diﬀerent physical processes occurring within an active region over time.

– 9 –

Though the mean gradient of the horizontal ﬁeld and the absolute value of the net current
helicity are highest ranked for the t=24 and t=48 hour cases, respectively, this does not necessarily
mean that they have any predictive power on their own. In order to determine which, if any, of
the features can distinguish ﬂaring active regions that produce CMEs from those that do not, we
use a classiﬁcation algorithm. If the classiﬁcation algorithm performs well for any given feature or
combination of features, the features are useful. Otherwise, the features are unable to distinguish
between the two populations.

4.2. Classiﬁcation Results

We train the SVM on a subset of events, and then test it on the remaining events. In general,
it is good practice to select only high-ranking features, as identiﬁed by the univariate feature
selection algorithm, to train the SVM. However, this does not necessarily mean that the information
contained in a low-ranking feature is absolutely useless. Features that are individually useless
can, when combined, signiﬁcantly improve the performance of a predictive model (Guyon and
Elisseeﬀ, 2003). On the other hand, features that are absolutely useless can signiﬁcantly worsen
the performance of a predictive model by forcing the model to ﬁt to noise. This process is called
overﬁtting.
In particular, overﬁtting becomes a concern when the number of features is large
compared with the number of examples (Guyon and Elisseeﬀ, 2003). Since this is not the case here
(we have 18 SHARP features and 420 examples), we use all 18 SHARP features to train the SVM
(we do not use the ﬂare class feature to train or test the SVM).

One way to test whether a set of features are absolutely versus individually useless is by
computing a classiﬁcation metric, which quantiﬁes an algorithm’s performance, on the testing set.
Since the algorithm has no knowledge of the examples in the testing set, a model that has overﬁt
the data in the training set will produce a low classiﬁcation metric when applied to the data in the
testing set.

Many classiﬁcation metrics exist in the literature; in the case of a highly imbalanced class
ratio, some of these metrics are useful and some are not. For an in-depth review of the impact of
class imbalance on classiﬁcation metrics, see Section 4 of Bobra & Couvidat (2015) and Bloomﬁeld
et al. (2012). We prefer the True Skill Score (TSS) to all the other metrics as it is insensitive to
the class imbalance ratio and thus best for comparison with other groups. The TSS is deﬁned as
follows:

TSS =

TP × TN − FP × FN

P × N

=

TP

TP + FN

−

FP

FP + TN

(6)

where TP is the number of true positives, FN the number of false negatives, TN the number of
true negatives, and FP the number of false positives. The TSS is symmetrically distributed about
0: i.e., it ranges from [-1, 1] where 0 represents no skill and a negative value represents a perverse

– 10 –

Fig. 1.— TSS per number of SHARP features. For the red points, the features are arranged from
lowest to highest F-score; for the blue points, this arrangement is reversed. These TSS values are
computed for a prediction exactly 24 hours before the GOES X-ray ﬂare peak time and with the
parameters detailed in Section 4.2.

181716151413121110987654321number of features0.40.20.00.20.40.60.81.0TSSTSS per number of featureslowest to highest F-scorehighest to lowest F-score– 11 –

Fig. 2.— TSS as a function of prediction time. These TSS values are computed using all 18 SHARP
features and the parameters detailed in Section 4.2.

4844403632282420161284prediction time0.30.40.50.60.70.80.91.0TSSTSS per time step– 12 –

prediction.

To train the SVM, we use the following parameters: γ = 0.075, C1 = 26.0, C2 = 4.0. We
determined these values through trial and error and tailored them to maximize the TSS. There are
many diﬀerent ways to separate a dataset into training and testing sets. A common way to do it is
simply to divide the data into two groups and assign a smaller sample, e.g. 30%, to the testing set,
and then use the remaining sample for the testing set. However, since the positive sample size is
quite small (both objectively and compared with the negative sample size) in our case, it is diﬃcult
to get an adequate number of events in both the training and testing sets.

To solve this problem, we use the stratiﬁed k-folds cross-validation method, which makes k
partitions of the data set and uses k-1 folds for training and 1 fold for testing. The stratiﬁcation
preserves the ratio of positive to negative examples per fold. We then permute over the partitions
such that each partition eventually makes its way into the testing set. For each individual testing
set, we calculate the TSS. We report the average of the TSS over the total number of testing sets
as the ﬁnal TSS; we report the standard deviation of these TSS values as the error in the ﬁnal TSS.

In order to use the stratiﬁed k-folds cross-validation method, we must ﬁrst select a value of
k. k can be arbitrarily deﬁned and take any value between 2 and the total number of examples.
As k approaches the total number of examples, the k-fold method reduces to the Leave One Out
method, in which only one example is in the testing set and all other examples are in the training
set. Kohavi et al. (1995) and other studies recommend the stratiﬁed 10-fold cross-validation to
reduce variance and bias. Here, we test their recommendation by computing the TSS using 50 k
values, ranging from 2 to 52, and conﬁrm that high k-values result in a high variance. As such, we
use ﬁnd it reasonable to use the stratiﬁed k-fold cross-validation method, which is implemented in
the cross validation module of the scikit-learn library, for k=10.

Figure 1 illustrates the TSS per number of features for a prediction exactly 24 hours before the
GOES X-ray ﬂare peak time. For the red points, the features are arranged from lowest to highest
F-score; for the blue points, this arrangement is reversed. In other words, Feature 1 represents the
highest ranked feature, or the mean value of the horizontal ﬁeld gradient, for the red points; for the
blue points, Feature 1 represents the lowest ranked feature, or the mean value of the vertical ﬁeld
gradient. We ﬁnd that any combination of 12 parameters yields the highest TSS, of approximately
0.8±0.2, whereas using fewer than 6 parameters will produce a signiﬁcantly lower TSS. Based on
this relatively high TSS value, we conﬁrm that our predictive model has not overﬁt the data and
that at least 12 of the 18 features are useful in combination. We also ﬁnd that the single best
feature performs marginally better on its own, producing a perverse TSS of -0.02±0.16, than the
single worst feature. As such, the highest-ranked feature in our dataset does not have any predictive
power on its own. Therefore, we conﬁrm that a combination of features is necessary to distinguish
between events in the positive and negative class.

Figure 2 illustrates the TSS as a function of prediction time t, where t is the number of hours
before the GOES X-ray ﬂare peak time, using all 18 features for every t. In this case, we predict

– 13 –

whether or not a ﬂaring active region will produce a CME for a t that ranges from 4 to 48 hours
in 4 hour intervals. We ﬁnd that a t of 28 to 24 hours will produce a marginally higher TSS than
a t of 48 or 4 hours, though the lowest TSS value is within the error for the highest one.

From Figures 1 and 2, we ﬁnd that our prediction algorithm exhibits a fairly high performance,
with a TSS equal to ∼0.8±0.2, yet there are no other, similar studies with which to compare this
value. However, since our positive class has only 56 examples, the TSS has a high error and may
not generalize well to other studies.

5. Conclusion

In this study, we [1] used features derived from maps of photospheric vector magnetic ﬁeld data
taken by the HMI instrument aboard SDO to forecast whether an active region that produces an
M1.0-class ﬂare or higher will also produce a CME, and [2] determined which features distinguish
ﬂaring active regions that produce CMEs from those that do not. Prior to this one, no extensive
study has used physically meaningful features of active regions to distinguish between these two
populations. Though we sampled from a database of 3023 active regions, we found only 56 events
that satisﬁed the criteria deﬁned for the positive class and 364 events for the negative one. This is
due to the uncharacteristically quiet nature of Solar Cycle 24, during which time the Sun produced
fewer ﬂares and CMEs than usual.

Through a feature selection process, we ﬁnd that all the relatively high-performing features are
independent of system size. In other words: as an active region increases in size, these parameters
do not necessarily increase in value. Parameters that obey this property have been dubbed as
intensive by Welsch et al. (2009); they call the opposite case, where the value of the parameter
increases with system size, extensive. Intensive parameters are generally comprised of population
means, whereas extensive ones are generally comprised of population sums.

Welsch et al. (2009) argue that the distinction between extensive and intensive properties is
relevant to CME processes. On one hand, many numerical models show that instabilities in small-
scale structures, like current sheets, form the trigger mechanism for CMEs (Chen, 2011). Changes
on such a scale would be captured by an intensive parameter. On the other hand, large-scale
non-potential magnetic structures, like ﬁlaments and sigmoids, have been observationally linked
to CMEs (Webb & Howard, 2012). Changes in such features would be captured by an extensive
parameter.

We ﬁnd that the 6 highest-ranking features, which are deﬁned by the univariate feature se-
lection algorithm as those that best distinguish between the positive and negative class, are all
intensive in nature. Further, we ﬁnd that there is a signiﬁcant diﬀerence in TSS between using this
group of 6 features versus using the 6 lowest-ranking features. This is illustrated in Figure 1. The
TSS for the 6 highest-ranking features is 0.68±0.10, whereas the TSS for the 6 lowest-ranking fea-
tures is 0.19±0.19. We therefore conclude that intensive features are a better choice for predicting

– 14 –

CMEs.

In their observational study, Sun et al. (2015) also found that intensive parameters show the
greatest change between ﬂaring active regions that are CME-productive and those that are not.
However, they found that the conﬁguration of the overlying ﬁeld may also play a strong role in CME
productivity. As such, they suggest developing features that quantify both the relative change in
intensive parameters and the topology of the overlying ﬁeld. T¨or¨ok & Kliem (2005) found similar
results using numerical models: changes in the overlying ﬁeld as a function of height strongly
aﬀects whether an eruption will lead to a conﬁned ﬂare or CME. Since we do not consider the
magnetic ﬁeld at any height above the photosphere in our study, we surmise that we lack some of
the information necessary to predict CMEs.

It turns out that there is not much added value in using more than 6 features: the maximum
TSS we obtain is within the error for the TSS obtained using 6 features. We compute Pearson cor-
relation coeﬃcients between each pair of features. While some features have almost no correlation
(the correlation coeﬃcient for the two top-ranking features in the 24-hour case is 0.06), we ﬁnd
that several features contain nearly identical information. For example, both of the shear-related
features (ranked 6 and 7, respectively, in the 24-hour case) have a correlation coeﬃcient of 0.99,
and the total and vertical ﬁeld gradients (ranked 4 and 18, respectively, in the 24-hour case) have a
correlation coeﬃcient of 0.95. Only one of each of these pairs retains a spot in the top 6 parameters.
However, the second and third ranked parameters in the 24-hour case are also strongly correlated
(0.98), yet both retain a spot in the top 6 parameters. This is because a high feature correlation
does not indicate an absence of feature complementarity (Guyon and Elisseeﬀ, 2003). Rather, it
appears that there is a limited amount of information contained in the photospheric magnetic ﬁeld
and the 6 most highly-ranked features capture most of this information. As such, we conclude that
only a handful of features are necessary to obtain a relatively high TSS and that a combination of
such features contains more predictive capacity than any single feature alone.

We also ﬁnd that all the relatively high-performing features characterize the degree to which
the magnetic ﬁeld deviates from a potential conﬁguration. We ﬁnd that the mean gradient of the
horizontal ﬁeld holds the most predictive power of all the assessed features. Sun et al. (2015) found
that the inclination of the horizontal ﬁeld plays a strong role in whether a ﬂaring active region
produces a CME. They learned that NOAA active region 12192, which hosted the largest sunspot
group in the last 24 years and produced several X-class ﬂares yet no CMEs, contained a relatively
weak horizontal ﬁeld compared with other CME-producing active regions; further, active regions
that exhibit a steep decrease in the magnitude of the horizontal component of the magnetic ﬁeld as
a function of height may be more likely to produce a CME than those that decrease much slower
with height. We also ﬁnd that the twist parameter, α, is relatively high-performing. Falconer
et al. (2002) and Falconer et al. (2006) found that the magnetic twist is correlated with CME
productivity. They also suggest that combined features, such as α combined with active region
size, may have an even stronger correlation with CME productivity.

– 15 –

When analyzing high-performing features, it is important investigate whether or not they
scale with ﬂare size. Features that do may be useful in a purely predictive sense, but they are
simply a proxy of the amount of energy that is available in an active region system (Kahler, 1982).
Features that do not scale with ﬂare size, however, can provide an additional relationship with
CME productivity. We ﬁnd that ﬂare class shows little diﬀerence in its distribution between the
positive and negative class; further, we ﬁnd no correlation between ﬂare class and any of the
SHARP features. Therefore, we conclude that the SHARP features are not simply measuring how
much energy is available in an active region system but rather highlighting a physically diﬀerent
relationship with CME productivity.

From our study, it is unclear that the physical mechanism responsible for triggering a CME
has a strong photospheric signature. Perhaps our features are too noisy; Sun et al. (2015) showed
that local features, which parameterize the vector magnetic ﬁeld within the core of an active region,
show a stronger pre-CME signature than the global features we use, which parameterize the vector
magnetic ﬁeld within the entire active region. Or perhaps it is a better idea to also study coronal
features to capture such a signature. Canﬁeld et al. (1999) clearly showed that the presence of a
sigmoidal shape in the X-ray corona that is simultaneous with a delta-class sunspot region on the
photosphere is likely to be associated with eruptive phenomena. As such, a natural extension of
this work would be to include coronal features of active regions in addition to photospheric ones.

This work was supported by NASA Grant NAS5-02139 (HMI). The data used here are courtesy
of NASA/SDO and the HMI science team, as well as the GOES, STEREO, and SoHO teams. The
codes used here are courtesy of the SunPy and scikit-learn teams. The authors would like to thank
S´ebastien Couvidat, Xudong Sun, Sanjiv Tiwari, and Yihua Zheng for their guidance.

REFERENCES

Antalov´a, A. 1996, Contributions of the Astronomical Observatory Skalnate Pleso, 26, 98

Ahmed, O.W., Qahwaji, R., Colak, T. et al., 2013, Sol. Phys., 283, 157

Bloomﬁeld, D.S., Higgins, P.A., McAteer, R.T.J., & Gallagher, P.T. 2012, ApJ, 747, L41

Bobra, M. G., Sun, X., Hoeksema, J. T., et al. 2014, Sol. Phys., 289, 3549

Bobra, M. G., & Couvidat, S. 2015, ApJ, 798, 135

Canﬁeld, R. C., Hudson, H. S., & McKenzie, D. E. 1999, Geophys. Res. Lett., 26, 627

Webb, D. F., & Howard, T. A. 2012, Living Reviews in Solar Physics, 9, 3

Chen, P. F. 2011, Living Reviews in Solar Physics, 8, 1

Falconer, D. A., Moore, R. L., & Gary, G. A. 2002, ApJ, 569, 1016

– 16 –

Falconer, D. A., Moore, R. L., & Gary, G. A. 2006, ApJ, 644, 1258

Falconer, D. A., Moore, R. L., & Gary, G. A. 2008, ApJ, 689, 1433

Guyon, I., & Elisseeﬀ, A. 2003, Journal of Machine Learning Research, 3, 1157

Hoeksema, J. T., Liu, Y., Hayashi, K., et al. 2014, Sol. Phys., 289, 3483

Hurlburt, N., Cheung, M., Schrijver, C., et al. 2012, Sol. Phys., 275, 67

Kahler, S. W. 1982, J. Geophys. Res., 87, 3439

Kohavi, R. 1999, International Joint Conference on Artiﬁcial Intelligence, 14, 2

Li, R., Wang, H.-N., He, H., Cui, Y.-M., & Du, Z.-L. 2007, Chin. J. Astron. Astrophys., 7, 441

Lin, J., & Forbes, T. G. 2000, J. Geophys. Res., 105, 2375

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Pret-
tenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher,
M., Perrot, M., & Duchesnay, E. 2011, Journal of Machine Learning Research, 12, 2825

Qahwaji, R., Colak, T., Al-Omari, M., & Ipson, S. 2008, Sol. Phys., 248, 471

Rietveld, T., Hout, R. v., 2010, Statistics in Language Research: Analysis of Variance, Mouton de

Gruyter, Berlin, New York.

Schou, J., Scherrer, P.H., Bush, R.I.; Wachter, R., Couvidat, S., Rabello-Soares, M.C., et al. 2012,

Sol. Phys., 275, 229

Sun, X., Bobra, M. G., Hoeksema, J. T., et al. 2015, ApJ, 804, L28

SunPy Community, T., Mumford, S. J., Christe, S., et al. 2015, Computational Science and Dis-

covery, 8, 014009

T¨or¨ok, T., & Kliem, B. 2005, ApJ, 630, L97

Webb, D. F., & Howard, T. A. 2012, Living Reviews in Solar Physics, 9, 3

Welsch, B. T., Li, Y., Schuck, P. W., & Fisher, G. H. 2009, ApJ, 705, 821

Yashiro, S., Gopalswamy, N., Akiyama, S., Michalek, G., & Howard, R. A. 2005, Journal of Geo-

physical Research (Space Physics), 110, A12S05

This preprint was prepared with the AAS LATEX macros v5.2.

