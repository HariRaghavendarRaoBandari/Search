6
1
0
2

 
r
a

 

M
5
1

 
 
]

G
L
.
s
c
[
 
 

1
v
8
1
9
4
0

.

3
0
6
1
:
v
i
X
r
a

Data Clustering and Graph Partitioning via Simulated Mixing

Shahzad Bhatti∗

Carolyn Beck∗

Angelia Nedi´c∗

Abstract

Spectral clustering approaches have led to well-accepted algorithms for ﬁnding accurate clus-
ters in a given dataset. However, their application to large-scale datasets has been hindered by
computational complexity of eigenvalue decompositions. Several algorithms have been proposed
in the recent past to accelerate spectral clustering, however they compromise on the accuracy
of the spectral clustering to achieve faster speed. In this paper, we propose a novel spectral
clustering algorithm based on a mixing process on a graph. Unlike the existing spectral cluster-
ing algorithms, our algorithm does not require computing eigenvectors. Speciﬁcally, it ﬁnds the
equivalent of a linear combination of eigenvectors of the normalized similarity matrix weighted
with corresponding eigenvalues. This linear combination is then used to partition the dataset
into meaningful clusters. Simulations on real datasets show that partitioning datasets based on
such linear combinations of eigenvectors achieves better accuracy than standard spectral clus-
tering methods as the number of clusters increase. Our algorithm can easily be implemented in
a distributed setting.

1 Introduction

Data clustering is a fundamental problem in pattern recognition, data mining, computer vision,

machine learning, bioinformatics and several other related disciplines. It has a long history and

researchers in various ﬁelds have proposed numerous solutions. Several spectral clustering algo-

rithms have been proposed [1, 2, 3, 4], which have enjoyed great success and have been widely used

to cluster data. However, spectral clustering does not scale well to large-scale problems due to its

considerable computational cost. In general, spectral clustering algorithms seek a low-dimensional

embedding of the dataset by computing the eigenvectors of a Laplacian or similarity matrix. For
a dataset with n instances, eigenvector computation has time complexity of O(n3) [5], which is
signiﬁcant for large-scale problems.

In the past few years, eﬀorts have been focused towards addressing scalability of spectral clustering.

A natural way to achieve scalability is to perform spectral clustering on a sample of the given

∗ISE & CSL, University of Illinois at Urbana Champaign, IL Email:{bhatti2, beck3, angelia}@illinois.edu

1

dataset and then generalize the result to the rest of data. For example, Fowlkes et al. [6] ﬁnd an

approximate solution by ﬁrst performing spectral clustering on a small random sample from the

dataset and then using the Nystrom method; they extrapolate the solution to all the dataset. In

[7], Sakai and Imiya also ﬁnd an approximate spectral clustering by clustering a random sample

of the dataset. They also reduce the dimension of the dataset using random projections. Another

approach proposed by Yan et al. [8] works by ﬁrst determining a smaller set of representative points

using k-means (each centroid is a representative point) and then performing spectral clustering on

the representative points. Finally, the original dataset is clustered by assigning each point to the

cluster of its representative. In [9], Chen et al. deal with large-scale data by parallelizing both

computation and memory use on distributed computers.

These methods sacriﬁce the accuracy of spectral clustering to achieve fast implementation.

In

this paper, we perform spectral clustering without explicitly calculating eigenvectors. Rather we

compute a linear combination of the right-eigenvectors weighted with corresponding eigenvalues.

Moreover, unlike many traditional algorithms, our algorithm does not require a predeﬁned number

of clusters, k, as input. This algorithm can automatically detect and adapt to any number of

clusters, based on a preselected tolerance. We apply our algorithm to large size stochastic block

models to illustrate the scalability and demonstrate that it can handle large datasets where tra-

ditional spectral algorithms result in memory errors. We compare the accuracy and speed of our

algorithm to the normalized cut algorithm [1] on real datasets and show that our approach achieves

similar accuracy but at a faster speed. We also show that our algorithm is faster and more accu-

rate than both the Nystrom method for spectral clustering [6] and the fast approximate spectral

clustering [8].

Notation. Throughout this paper we use boldface to distinguish between vectors and scalars. For
example, vi is in boldface to identify a vector, while xi, a scalar, is not boldface. We use 1 to denote
the vector of ones. We denote matrices by capital letters, such as A, and use aij to represent the
entries of A. Calligraphic font is used to denote sets with the single exception that G is reserved to
denote graphs. The norm (cid:107) · (cid:107) denotes (cid:107) · (cid:107)2 for vectors and for matrices it denotes spectral norm.

2 Problem Statement

. . . , vn} in a d-dimensional space. We will often use
Consider a set of n data points V = {v1, v2,
a short-hand notation i to denote the vector vi. The goal is to ﬁnd clusters in the dataset such that
points in the same cluster are similar to each other, while points in diﬀering clusters are dissimilar

under some predeﬁned notion of similarity.
In particular, suppose pairwise similarity between
points is given by some similarity function s(vi, vj) often abbreviated by s(i, j), where usually it is
assumed that the function s is symmetric. Also, the similarity function s is non-negative if vi (cid:54)= vj

2

and is equal to zero if vi = vj. A similarity matrix is an n × n symmetric matrix W such that the
entry wij is equal to the value of the similarity function s(vi, vj) between points vi and vj.

The data points together with the similarity function form a weighted undirected similarity graph
G = (V,E); V is the set of nodes (vertices) of the graph, E is the set of edges. We note that the
problem of ﬁnding clusters in a dataset can be converted into a graph partitioning problem. We
will make this more precise in the sequel. In particular, in our case each data point vi represents
a vertex/node in the graph. Two vertices i and j are connected by an edge if their similarity

s(i, j) is positive and the edge weight is given by s(i, j). Diﬀerent similarity measures lead to

diﬀerent similarity graphs. The objective of constructing a similarity graph is to model the local

neighborhood relationships to capture the geometric structure of the dataset using the similarity

function. Some commonly used similarity graphs for spectral clustering are noted below.

a) Gaussian similarity graphs: Gaussian similarity graphs are based on the distance between

points. Typically every pair of vertices is connected by an edge and the edge weight is determined
by the Gaussian function (radial basis function) s(i, j) = exp(−(cid:107)vi−vj(cid:107)2/2σ2), where parameter
σ controls how quickly the similarity fades away as the distance between the points increases.

It is often helpful to remove the edges for edge weights below a certain threshold (say δ) to

sparsify the graph. Thus the similarity function for this type of graph is

 e

0

s(i, j) =

− (cid:107)vi−vj(cid:107)2

2σ2

if (cid:107)vi − vj(cid:107) > δ,
otherwise.

b) p-nearest neighbor graphs: As the name suggests each vertex vi is connected to its p nearest
neighbors, where the nearness between i and j is measured by the distance (cid:107)vi − vj(cid:107). This
similarity measure results in a graph which is not necessarily symmetric in its similarity function,
since the nearness relationship is not symmetric. In particular, if vj is among the p nearest
neighbors of vi then it is not necessary for vi to be among the p nearest neighbors of vj. We
make the similarity measure symmetric by placing an edge between two vertices vi and vj if
either vi is among the p nearest neighbors of vj or vj is among the p nearest neighbors of vi,
that is

(cid:40)

s(i, j) =

1 if either i or j is one of the p nearest neighbors of the other,

0 otherwise.

c) -neighborhood graphs: In an -neighborhood graph, we connect two vertices vi and vj if

3

the distance (cid:107)vi − vj(cid:107) is less than  giving us the following similarity function.

(cid:40)

s(i, j) =

1 if (cid:107)vi − vj(cid:107) ≤ ,
0 otherwise.

Thus ﬁnding clusters in a dataset is equivalent to ﬁnding partitions in the similarity graph such

subgraphs. The degree of each vertex i of the graph is given by di =(cid:80)n

that the sum of edge weights between partitions is small and the partitions themselves are dense
j=1 wij. A degree matrix
D is a diagonal matrix with its diagonal elements given by di for i = 1, . . . , n. We assume that the
degree of each vertex is positive. This in turn allows us to deﬁne the normalized similarity matrix
by W = D−1W .

3 Mixing processes

As a visualization of our clustering approach, consider a mixing process in which one imagines that

every vertex in the graph moves towards (mixes with) other vertices in discrete time steps. At each

time step, vertex i moves towards (mixes with) vertex j by a distance proportional to the similarity

s(i, j). Thus the larger the similarity s(i, j), the larger the distance vertices i and j move towards
each other i.e., the greater the mixing. Moreover, a point vi will move away from the points which
have weak similarity with it. Thus, similar points will move towards each other making dense

clusters and dissimilar points will move away from each other increasing the separability between

clusters. Clusters in this transformed distribution of points then can easily be identiﬁed by the

k-means algorithm.

To describe the above idea more precisely, consider the following model, where each point vi moves
according to the following equation, starting at its original position at time t = 0:

n(cid:88)

j − vt
i)
n(cid:88)

wij vt
j.

vt+1
i = vt

i + α

wij(vt

j=1

= (1 − α)vt

i + α

(1)

j=1

The parameter α ∈ [0, 1] is the step size, which controls the speed of movement (or mixing rate)
in each time interval. Observe that if the underlying graph has a bipartite component and α = 1,

then in each time step all points on one side of this component would move to the other side and

vice versa. Therefore, points in this component would not actually mix even after a large number

of iterations (for details see [10]). For such graphs we must have α bounded away from 1. We can
use α = 1 for graphs without a bipartite component. Assuming each point vi is a row vector, we

4

express equation (1) in a matrix form:

V t+1 = ((1 − α)I + αW )V t

= M V t.

(2)

The matrix V t is a n × d matrix with row i representing the position of point vi at time t. I is a
n× n identity matrix, and we deﬁne M = (1− α)I + αW . Note that the matrix M is essentially the
transition matrix of a lazy random walk with probability of staying in place given by 1 − α. Since
M also captures the similarity of the data points, one would expect that for t large enough, the

process in equation (2) would reveal the data clusters, since M will mix the data points according to

their similarities. Using this intuition, one can expect that a heuristic algorithm based on equation

(2) can be constructed to determine the clusters, as given in the following algorithm.

Algorithm 1 Point-Based Resource Diﬀusion (PRD)
1: Input: Set of data points V, number of clusters k
2: Represent the data points in the matrix V with point vi being the ith row.
3: Compute M = (1 − α)I + W .
4: Loop over t
5:
6: until Stopping criteria is met.
7: Find k clusters from rows of V t+1 using k-means algorithm.
8: Output: Clustering obtained in the ﬁnal iteration.

V t+1 ← M V t

Algorithm 1 has two limitations: a) it does not scale well with the dimension d of the data points,
because the number of computations in each iteration is O(n2d), and b) it fails to identify clusters
contained within other clusters. For example, in the case of two concentric circular clusters, points in

both clusters will move towards the center and become one cluster, losing the geometric structure

inherent in the data. Hence it becomes impossible to discern these clusters using the k-means
algorithm in Step 7. To overcome these limitations, we associate an agent xi to each point vi and
carry out calculations in the agent space. Agents are generated by choosing n points uniformly at
random from a bounded interval [0, b]1. We rewrite equation (2) in the agent space as:

xt+1 = ((1 − α)I + αW )xt = M xt.

(3)

We refer to this iterative equation as the Mixing Process. In the following section we analyze this

process using the properties of the random walk matrix M .

1Note, b is a scaling parameter and does not change the resulting clustering. For the sake of simplicity we use a

probability vector for the analysis in the subsequent section.

5

4 Analysis of the Mixing Process

The matrix M captures the similarity structure of the data, and the idea behind using the iterative
process (3) is that, after some suﬃcient number of iterations, the entries of the vector xt+1 will
reveal clusters on a real line, which will be representative of the clusters in the data. The fact is

that the process (2) and (3) both mix with the same speed, which is governed by the random walk

M . Thus, the hope is that through the process in (3) we determine the weakly coupled components
in the matrix M , which can lead us to the data clusters of the points v1, . . . , vn.

The Mixing Process shares a resemblance to the power iteration. Unlike the power iteration, we

here use the mixing process to discover strongly coupled components of M , which translate to data

clusters.

4.1 Properties of the matrix M

We ﬁrst show that the matrix M is diagonalizable, which allows for a more straightforward analysis.

By deﬁnition,

M = (1 − α)I + αW

= I − α(I − D−1W )
= I − αD−1/2LD1/2
= D−1/2(I − αL)D1/2,

(4)

where L = I − D−1/2W D−1/2 is the normalized Laplacian of the graph G. Let φi be a right
eigenvector of L with eigenvalue λi, then D−1/2φi is a right eigenvector of M with eigenvalue
µi = 1 − αλi, that is

M D−1/2φi = (I − αD−1/2LD1/2)D−1/2φi

= (1 − αλi)D−1/2φi.

This gives us a useful relationship between the spectra of the random walk matrix M and the

normalized Laplacian L. It is well known that the eigenvalues of a normalized Laplacian lie in the
interval [0, 2], see for example [10]. Thus, if 0 = λ1 ≤ λ2 ≤ . . . ≤ λn ≤ 2 are the eigenvalues of
L, then the corresponding eigenvalues of M are 1 = µ1 ≥ µ2 ≥ . . . ≥ µn ≥ 1 − 2α. It is worth
noting that we are considering the right eigenvector of the random walk matrix M . One should

not confuse this with the left eigenvector.

Although the matrix M is not symmetric, L is a symmetric positive semi-deﬁnite matrix, thus its
normalized eigenvectors form an orthonormal basis for Rn and we can express L in the following

6

form:

Using (4) and (5), we obtain

(cid:16)

M t =

n(cid:88)

i=i

L =

λiφiφT
i .

D−1/2(I − αL)D1/2(cid:17)t
(cid:32) n(cid:88)

(cid:33)

D1/2.

(5)

(6)

= D−1/2

(1 − αλi)tφiφT

i

We will exploit this relationship in our proofs later.

i=i

4.2 The ideal case

i=1Vi = V and n =(cid:80)k

For the sake of analysis, it is worthwhile to consider the ideal case, in which all points form tight
clusters that are well-separated. By well-separated, we mean that if points vi and vj lie in diﬀerent
clusters, then their similarity wij = 0. Suppose that the data consists of k clusters V1, V2, . . . , Vk
with n1, n2, . . . , nk points, respectively, such that ∪k
i=1 ni. For the ease of
exposition, we also assume that the vi’s are numbered in such a way that points v1, v2, . . . , vn1 are
in cluster V1, the points vn1+1, vn1+2, . . . , vn1+n2 are in cluster V2 and so on.
The underlying graph in the ideal case consists of k connected components G1, G2, . . . , Gk, where
each component Gj consists of vertices in the corresponding cluster Vj. We represent this ideal
graph by G∗, its normalized Laplacian by L∗ and its similarity matrix by W ∗. The n-dimensional
characteristic vector χj of the jth component Gj is deﬁned as
if vi ∈ Gj,
otherwise.

χj(i) =

(cid:40)

1

0

The ideal similarity matrix W ∗ and, consequently the ideal normalized Laplacian L∗ of a graph with
k connected components, are both block-diagonal with the jth block representing the component
Gj, i.e.,

W ∗ = diag(W1, W2, . . . , Wk),
L∗ = diag(L1, L2, . . . , Lk).

Since L∗ is block-diagonal, its spectrum is the union of spectra of L1, L2, . . . , Lk. The eigenvalue
λ1 = 0 of L∗ has multiplicity k with k linearly independent normalized eigenvectors φ∗
2, . . . , φ∗
k.
j = D1/2χj/(cid:107)D1/2χj(cid:107). In the following theorem, we prove
Each of these eigenvectors is given by φ∗
that if we have an ideal graph then the iterate sequence {xt} generated by the Mixing Process (3)

1, φ∗

7

converges to a linear combination of characteristic vectors χj’s of the k components of the graph.
The χj’s are also eigenvectors of M∗, where M∗ = (1 − α)I − D−1W ∗, corresponding to ﬁrst k
eigenvalues µ1 = µ2 = ... = µk = 1.

Theorem 1. Suppose that we have an ideal dataset which consists of k clusters as deﬁned previously
and let x0 be any vector such that each x0

i > 0 and (x0)T 1 = 1, then

(cid:107)M∗tx0 − k(cid:88)

i=1

ciχi(cid:107) ≤ max

i>k

|1 − αλi|t maxj
minj

(cid:112)dj
(cid:112)dj

,

where ci = χT

i Dx0
1T Dχi

and dj is the degree of the jth node.

Proof. We begin by noting that the iterative process (3) is equivalent to xt = M tx0. Using (6), we
have

(cid:107)M∗tx0 − k(cid:88)

(cid:32) n(cid:88)

(cid:33)

D1/2x0 − k(cid:88)

ciχi(cid:107).

ciχi(cid:107) = (cid:107)D−1/2

(1 − αλi)tφ∗

i φ∗

i

T

i=1

i=1

i=1

Separating the ﬁrst k terms in the sum and using the fact that eigenvector φ∗
i = 1, . . . , k, the above equation can be simpliﬁed as

i = D1/2χi

(cid:107)D1/2χi(cid:107) , for

(cid:33)

(cid:33)

(cid:33)

ciχi(cid:107)

D1/2x0 − k(cid:88)
(cid:32) n(cid:88)

i=1

i=k+1

(1 − αλi)tφ∗

i φ∗

i

(cid:33)

T

D1/2x0 − k(cid:88)

i=1

ciχi(cid:107)

+ D−1/2

(1 − αλi)tφ∗

i φ∗

i

T

i=k+1

D−1/2φ∗

i φ∗

i

T D1/2x0 + D−1/2

= (cid:107) k(cid:88)
= (cid:107) k(cid:88)

i=1

i=1

D−1/2 D1/2χi
(cid:107)D1/2χi(cid:107)

(D1/2χi)T
(cid:107)D1/2χi(cid:107) D1/2x0

(cid:32) n(cid:88)

+ D−1/2

(1 − αλi)tφ∗

i φ∗

i

T

D1/2x0 − k(cid:88)

ciχi(cid:107).

(cid:107)M∗tx0 − k(cid:88)

i=1

ciχi(cid:107) = (cid:107)D−1/2

(1 − αλi)tφ∗

i φ∗

i

T

D1/2x0

(cid:32) k(cid:88)
(cid:32) n(cid:88)

i=1

We can simplify the ﬁrst term in the norm as

i=k+1

i=1

D−1/2 D1/2χi
(cid:107)D1/2χi(cid:107)

(D1/2χi)T
(cid:107)D1/2χi(cid:107) D1/2x0 =

=

8

χi χT

i D1/2D1/2x0
(cid:107)D1/2χi(cid:107)2
χT
i Dx0
(cid:107)D1/2χi(cid:107)2

χi,

where the term (cid:107)D1/2χi(cid:107)2 is equal to the sum of the degrees of vertices in component Gi (commonly
called volume of Gi) which can also be expressed by 1T Dχi, to have

(cid:33)

D1/2x0 − k(cid:88)

ciχi(cid:107)

(cid:107)M∗tx0 − k(cid:88)

ciχi(cid:107) = (cid:107) k(cid:88)

i=1

i=1

(cid:32) n(cid:88)

i=k+1

(cid:33)

χi + D−1/2

(1 − αλi)tφ∗

i φ∗

i

T

= (cid:107)D−1/2

(1 − αλi)tφ∗

i φ∗

i

T

D1/2x0(cid:107).

i=1

χT
i Dx0
1T Dχi

(cid:32) n(cid:88)

i=k+1

In the last equation we have used ci = χT
separate the terms, giving us

i Dx0
1T Dχi

. Now using the properties of the norm, we can

(cid:107)M∗tx0 − k(cid:88)

n(cid:88)

ciχi(cid:107) ≤ (cid:107)D−1/2(cid:107) (cid:107)

(1 − αλi)tφ∗

i φ∗

i

T(cid:107) (cid:107)D1/2(cid:107) (cid:107)x0(cid:107)

i=1

i=k+1

≤ max

i>k

|1 − αλi|t maxj
minj

(cid:112)dj
(cid:112)dj

since (cid:107)x0(cid:107) ≤ 1.

,

Note that we can always choose α ∈ [0, 1] such that λk+1 = arg maxi>k |1−αλi|. Thus the preceding
inequality can be written as

(cid:107)M∗tx0 − k(cid:88)

i=1

ciχi(cid:107) ≤ (1 − αλk+1)t maxj
minj

≤ e−αtλk+1

maxj
minj

(cid:112)dj
(cid:112)dj
(cid:112)dj
(cid:112)dj

.

For any ξ > 0, there exists some t > 0 such that

e−αtλk+1

maxj
minj

Speciﬁcally, taking the log and simplifying, we have

1

αλk+1

ln

4.3 The general case

(cid:112)dj
(cid:112)dj
(cid:112)dj
(cid:112)dj

≤ ξ.

(cid:33)

≤ t.

(cid:32)

maxj
ξ minj

(7)

In practice, the graph under consideration may not have k connected components, but rather k

nearly connected components i.e., k dense subgraphs sparsely connected by bridges (edges). We can

9

obtain k connected components from such a graph by removing only a small fraction of edges. This

means that matrices W and L have non-zero oﬀ-diagonal blocks, but both matrices have dominant

blocks on the diagonal. The general case is thus a perturbed version of the ideal case.
Let W = W ∗ + E be the similarity matrix for a dataset V, where W ∗ is the similarity matrix corre-
sponding to the true clusters (an ideal similarity matrix), which is block-diagonal and symmetric.
We obtain W ∗ by replacing the oﬀ-diagonal block elements of W with zeros and adding the sum of
the oﬀ-diagonal block weights in each row to the diagonal elements. This results in the matrices W
and W ∗ having the same degree matrix D. The matrix E is then a symmetric matrix with row and
j=1 eij. The oﬀ-diagonal

column sums equal to zero with the ith diagonal entry given by eii = −(cid:80)n

entries of E are the same as the entries in the oﬀ-diagonal blocks of W . For example, for



W =

 ,

0

20

50

1

2

1

20

0

30

0

1

1

50

30

0

1

0

1

1

0

1

0

25

40

2

1

0

25

0

30

1

1

1

40

30

0

the matrices W ∗ and E satisfying the above constraints are given by



W ∗ =

4

20

20 50

2

30

50 30

0

0

0

0

0

0

2

0

0

0

0

0

0

2

0

0

0

0

0

0

25 40

25

3

40 30

30

3

 , and E =



−4
0
0 −2
0

1

2

1

0

1

0
0 −2
0

1
1 −2
0

1

1

1

0

2

1

0

0
0 −3
0

0
0 −3

 .

1

1

1

0

Using the deﬁnition of L, one can then show that the following result holds.
Lemma 1. If W = W ∗ + E is a similarity matrix for a dataset V, then the normalized Laplacian
of the corresponding graph is L = L∗ − D−1/2ED−1/2, where L∗ is the normalized Laplacian of the
ideal graph corresponding to the true clusters.

Since eigenvalues and eigenvectors are continuous functions of entries of a matrix, the eigenvalues
λi’s of L can be written as

λi = λ∗

i + ˜λi,

where λ∗

i

is the eigenvalue of the ideal normalized Laplacian L∗, and ˜λi depend continuously on

10

the entries of ¯E (cid:44) D−1/2ED−1/2. Similarly the eigenvectors φi’s of L can be expressed as

φi = φ∗

i + ˜φi,

where φ∗
i is the eigenvector of the ideal normalized Laplacian L∗ and ˜φi depend continuously on
the entries of ¯E. Note that the pair (˜λi, ˜φi) is not necessarily an eigenvalue/eigenvector pair of ¯E.
We assume that (cid:107)E(cid:107) and consequently (cid:107) ¯E(cid:107) are small enough so that |˜λi| and (cid:107) ˜φi(cid:107) are also small.

Theorem 2. Suppose that we have a dataset which consists of k clusters and let x0 be any vector
such that each x0

i > 0 and (x0)T 1 = 1, then we have

(cid:107)M tx0 − k(cid:88)

ciχi(cid:107) ≤

i=1

i=1

(cid:32) k(cid:88)

(cid:16)

2(cid:107) ˜φi(cid:107) + (cid:107) ˜φi(cid:107)2(cid:17)

|1 − αλ(cid:96)|t

+ max
(cid:96)>k

(cid:33)

(cid:112)dj
(cid:112)dj

,

maxj
minj

ciχi(cid:107)

where ci = (1 − α˜λi)t χT

i Dx0
1T Dχi

and dj is the degree of the jth node.

Proof. Using (6) and separating the ﬁrst k terms, we get

(cid:107)M tx0 − k(cid:88)

i=1

ciχi(cid:107) = (cid:107)D−1/2

= (cid:107)D−1/2

(1 − αλi)tφiφT

i D1/2x0 − k(cid:88)

i=1

(1 − αλi)tφiφT

i D1/2x0

i=1

n(cid:88)
k(cid:88)
n(cid:88)

i=1

i D1/2x0 − k(cid:88)

ciχi(cid:107).

+ D−1/2

(1 − αλi)tφiφT

i=k+1

i=1

Substituting φi = φ∗
for i = 1, . . . , k, we have

(cid:107)M tx0 − k(cid:88)

i=1

i + ˜φi and λi = λ∗
i + ˜λi in the above equation and using the fact that λ∗
(1 − α˜λi)t(cid:16)
k(cid:88)
n(cid:88)

ciχi(cid:107) = (cid:107)D−1/2

i + ˜φiφ∗
˜φT

T + ˜φi ˜φT
i

T + φ∗

i D1/2x0 − k(cid:88)

(1 − αλi)tφiφT

+ D−1/2

φ∗
i φ∗

ciχi(cid:107)

D1/2x0

(cid:17)

i=1

i

i

i

i = 0

i=k+1

= (cid:107) k(cid:88)
(1 − α˜λi)tD−1/2 D1/2χi(D1/2χi)T
(1 − α˜λi)tD−1/2(cid:16)
k(cid:88)

(cid:107)D1/2χi(cid:107)2

i + ˜φiφ∗
˜φT

φ∗

i=1

+

i

i

i=1

D1/2x0

T + ˜φi ˜φT
i

(cid:17)

D1/2x0

i=1

11

n(cid:88)

+ D−1/2

i D1/2x0 − k(cid:88)

(1 − αλi)tφiφT

i=k+1

i=1

ciχi(cid:107),

(8)

where

(1 − α˜λi)tD−1/2 D1/2χi(D1/2χi)T

(cid:107)D1/2χi(cid:107)2

D1/2x0 = (1 − α˜λi)t χi χT

i D1/2D1/2x0
(cid:107)D1/2χi(cid:107)2

= (1 − α˜λi)t χT

i Dx0
1T Dχi

χi

= ciχi.

Substituting the above expression in (8) and using the triangle inequality, we have

(cid:107)M tx0 − k(cid:88)

ciχi(cid:107) = (cid:107) k(cid:88)

i=1

i=1

(1 − α˜λi)tD−1/2(cid:16)
n(cid:88)
(1 − α˜λi)tD−1/2(cid:16)

i=k+1

n(cid:88)

≤ (cid:107) k(cid:88)

i=1

i=1

+ (cid:107)D−1/2(cid:107) (cid:107)

n(cid:88)

i=k+1

φ∗

i

i + ˜φiφ∗
˜φT

i

T + ˜φi ˜φT
i

D1/2x0

+ D−1/2

(1 − αλi)tφiφT

i D1/2x0(cid:107)

φ∗

i

i + ˜φiφ∗
˜φT

i

T + ˜φi ˜φT
i

(cid:17)

(cid:17)

D1/2x0(cid:107)

(1 − αλi)tφiφT

i D1/2x0(cid:107)

+ (cid:107)D−1/2

≤ k(cid:88)

i=k+1

|1 − α˜λi|t(cid:107)D−1/2(cid:107)(cid:16)(cid:107)φ∗

i (cid:107) + (cid:107) ˜φiφ∗
˜φT

i

i

i (cid:107)(cid:17)(cid:107)D1/2(cid:107) (cid:107)x0(cid:107)

T(cid:107) + (cid:107) ˜φi ˜φT

(1 − αλi)tφiφT

i (cid:107) (cid:107)D1/2(cid:107) (cid:107)x0(cid:107).

(9)

Since (cid:107)x0(cid:107) ≤ 1, (cid:107)φ∗

i(cid:107) = 1,

|1 − α˜λi| ≤ 1, we can further simplify inequality (9) as

(cid:107)M tx0 − k(cid:88)

ciχi(cid:107) ≤ k(cid:88)

|1 − α˜λi|t(cid:107)D−1/2(cid:107)(cid:16)

i=1

i=1

n(cid:88)

2(cid:107) ˜φi(cid:107) + (cid:107) ˜φi(cid:107)2(cid:17)(cid:107)D1/2(cid:107)
(cid:112)dj
(cid:112)dj

i (cid:107) (cid:107)D1/2(cid:107)

+ max
(cid:96)>k

|1 − αλ(cid:96)|t

+ max
(cid:96)>k

maxj
minj

|1 − αλ(cid:96)|t maxj
minj

(cid:33)

(cid:112)dj
(cid:112)dj

(cid:112)dj
(cid:112)dj

.

(10)

(1 − αλi)tφiφT

+ (cid:107)D−1/2(cid:107) (cid:107)

i=k+1

(cid:16)
2(cid:107) ˜φi(cid:107) + (cid:107) ˜φi(cid:107)2(cid:17) maxj
≤ k(cid:88)
(cid:32) k(cid:88)
2(cid:107) ˜φi(cid:107) + (cid:107) ˜φi(cid:107)2(cid:17)
(cid:16)

minj

i=1

=

i=1

12

Note that we can always choose α such that (1−αλk+1) = max(cid:96)>k |1−αλ(cid:96)| and the above expression
then becomes

(cid:107)M tx0 − k(cid:88)

ciχi(cid:107) ≤

i=1

i=1

(cid:32) k(cid:88)

(cid:16)

2(cid:107) ˜φi(cid:107) + (cid:107) ˜φi(cid:107)2(cid:17)

(cid:33)

(cid:112)dj
(cid:112)dj

.

maxj
minj

+ (1 − αλk+1)t

Observe that λi = ˜λi for i = 1, . . . , k. Thus, assuming that the perturbation is small, the ﬁrst k
eigenvalues of the Laplacian are close to zero. If the eigengap (λk+1 − λk) is large enough, then for
some t > 0, we will have both (1− αλk) = (1− α˜λk)t ≥ 1− δ for a small δ > 0, and (1− αλk+1)t ≤ 
for a small  > 0. This results in the eﬀective vanishing of the term (1 − αλk+1)t in the above
expression after a suﬃcient number of iterations and ci’s being bounded away from zero. According
to Theorem 2, we will then have an approximate linear combination of the k characteristic vectors
i=1 ciχi(cid:107) will be small. Small perturbation assumption also leads to
(cid:107) ˜φi(cid:107) being relatively small. Note that this eigengap condition is equivalent to Assumption A1 in
[3].

of the graph i.e., (cid:107)M tx0 −(cid:80)k

It is worth noting that as the number of clusters k grows, it becomes increasingly diﬃcult to
distinguish the clusters from the vector M tx0 using the classical k-means algorithm because the
perturbation ˜φi will accompany the k eigenvectors in M tx0. Thus, we devise a recursive bi-
partitioning mechanism to ﬁnd the clusters.

4.4 Clustering algorithm

Our analysis in the previous section suggests that points in the same cluster mix quickly whereas

points in diﬀerent clusters mix slowly. Simon and Ando’s [11] theory of nearly completely de-

composable systems also demonstrates that states in the same subsystem achieve local equilibria

long before the system as a whole attains a global equilibrium. Therefore, an eﬃcient clustering

algorithm should stop when a local equilibrium is achieved. We can then distinguish the clusters

based on mixing of the points. The two clusters in this case correspond to aggregation of elements
of xt. Thus a simple search for the largest gap in the sorted xt can reveal the clusters. This cluster
separating gap is directly proportional to b, since we initialize x0 by choosing n points uniformly at
random from an interval [0, b]. Furthermore, it is inversely proportional to the size n of the dataset.
Thus we deﬁne the gap between two consecutive elements of sorted xt as:

(cid:40)

gap(i) =

i+1 − xt
xt

i

0

i+1 − xt
if xt
otherwise.

i ≥ b
2n ,

(11)

13

In each recursive call, the algorithm terminates upon ﬁnding the largest gap and bi-partitions

the data based on this gap. If the algorithm fails to ﬁnd a nonzero gap in a recursive call, then

the indexing set of x in this call corresponds a cluster. This leads us to Algorithm 2 (RARD -

Theoretical).

Algorithm 2 Recursive Agent-Based Resource Diﬀusion (RARD) - Theoretical
1: Input: Matrix M = (1 − α)I + αD−1W and a tolerance 
2: procedure C = RARD(M, )
3:

n ← rowsize(M )
Initialized x0 by choosing n points uniformly at random from [0, b].
repeat
until (1 − αλk+1)t ≤ 
Sort(xt+1); ﬁnd the largest gap using Equation (11).
if gap is not found then

xt+1 ← M xt

return

end if
bi-partition the indexing set of xt+1 based on largest gap into i1 and i2.
C ← RARD(M (i1, i1), ),

C ← RARD(M (i2, i2), )

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:
14: end procedure
15: Output: Clustering C.

In practice, we do not know the eigenvalues of M . Thus, in our implementation we start the
procedure with an initial tolerance 0 on mixing of xt. When the tolerance is achieved, we search
for a nonzero gap in the vector sort(xt+1). If a gap is found the dataset is bi-partitioned based
on the largest gap. In a recursive fashion, the bi-partitioning procedure is then applied to both

resulting partitions. On the other hand, if the gap is not found we decrease the tolerance and

reevaluate for a gap after the new tolerance is attained. A cluster is formed if the procedure can
not ﬁnd a gap using either a maximum number of iterations tmax or a minimum tolerance min.
This algorithm is Algorithm 3 (RARD - Implemented). Since our algorithm ﬁnds clusters in a

recursive fashion by bi-partitioning the dataset in each recursive step, Theorem 2 can be reduced

to the following corollary.

Corollary 1. Suppose the dataset contains 2 clusters and let x0 be any vector such that each x0
and (x0)T 1 = 1, then we have

i > 0

(cid:107)M tx0 − 2(cid:88)

(cid:32) 2(cid:88)

ciχi(cid:107) ≤

2(cid:107) ˜φi(cid:107) + (cid:107) ˜φi(cid:107)2 + (1 − αλ3)t

(cid:33)

(cid:112)dj
(cid:112)dj

,

maxj
minj

i=1

i=1

where ci = (1 − α˜λi)t χT

i Dx0
1T Dχi

and dj is the degree of the jth node.

14

Algorithm 3 Recursive Agent-Based Resource Diﬀusion (RARD) - Implemented
1: Input: Matrix M = (1 − α)I + αD−1W and initial tolerance 0
2: procedure C = RARD(M, 0)
3:

n ← rowsize(M )
Initialized x0 by choosing n points uniformly at random from [0, b].
Initialize  ← 0
repeat

repeat

yt+1 ← (cid:107)xt+1 − xt(cid:107)

xt+1 ← M xt,

|yt+1 − yt| ≤ 

until
Sort(xt+1); ﬁnd the largest gap using Equation (11).
if  ≤ min or t ≥ tmax then

return

end if
 ← /2

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

until A nonzero gap is found
bi-partition the indexing set of xt+1 based on largest gap into i1 and i2.
C ← RARD(M (i1, i1), 0),

C ← RARD(M (i2, i2), 0)

17:
18: end procedure
19: Output: Clustering C.

4.5 Time complexity

Each iteration of Algorithm 1 involves multiplication of two matrices of size n× n and n× d, which
requires O(¯nd) operations, where ¯n is the number of nonzero entries in the matrix W . Algorithm 1

also calls the k-means algorithm, whose running time for one iteration is O(nkd). So the complexity
of Algorithm 1 is O(¯ndtmax) + O(nkd), where tmax is the maximum number of iterations.

Call 1

C1

Call 2

C2

Call 3

C3

...

Call k

Ck−1

Ck

Figure 1: A depiction of the worst case.

Algorithm 3 is a recursive algorithm. For the sake of intuitive analysis, we assume that n = 2j for
j ≥ 1 and that all clusters are of equal size i.e., each cluster has n/k points. we further assume that

15

k = 2(cid:96) for (cid:96) ≥ 1. In each recursive call, the algorithm performs a maximum of tmax sparse matrix-
vector multiplications which require O(¯ntmax) operations. It also makes two more recursive calls
and sorts n numbers except for the base case. Thus each non-base call takes O(¯ntmax) + O(n log n)
operations, where O(n log n) is the complexity of sorting n numbers. Let T (n, k, tmax) be the time
required to ﬁnd k clusters in a dataset of size n by Algorithm 3, then we have

T (n, k, tmax) = 2T

= 4T
...

2

(cid:16) n
(cid:16) n
(cid:16) n

4

(cid:17)
(cid:17)

(cid:17)

, k, tmax

+ c¯ntmax + n log n

, k, tmax

+ c¯ntmax + c

¯n
2

tmax + n log n +

n
2

log

n
2

= 2(cid:96)T

2(cid:96) , k, tmax

+ c¯ntmax

1
2s + n

= 2(cid:96)c

¯n
2(cid:96) tmax + c¯ntmax

1
2s + n log n

(cid:96)−1(cid:88)

s=0

(cid:96)−1(cid:88)

s=0

(cid:96)−1(cid:88)
1
2s log
(cid:96)−1(cid:88)
2s − n
(cid:96)−1(cid:88)

s=0

1

n
2s

(cid:96)−1(cid:88)

s=0

s
2s

(cid:96)−1(cid:88)

s=0

= c¯ntmax + (c¯ntmax + n log n)
= O(¯ntmax) + O(¯ntmax) + O(n log n) − O(n)
= O(¯ntmax) + O(n log n),

s=0

s=0

1

2s − n

s
2s

where c is a constant and we have used the fact that(cid:80)(cid:96)

s=0 s/2s ≤ 2. Similarly,
if we assume a diﬀerent split of the data in each recursive call, such as 1/3 and 2/3, or 1/4 and

s=0 1/2s ≤ 2 and(cid:80)(cid:96)

3/4 etc., it easily follows from the above analysis that the running time of the Algorithm 3 remains
O(¯ntmax)+O(n log n). The worst case arises when the dataset comprises a big cluster Vk of size (say)
n/2 and rest of the dataset constitutes the other k− 1 clusters V1,V2, . . .Vk−1. In such a scenario, if
a smaller cluster from one of the k−1 clusters, say V1, splits from the data in the ﬁrst recursive call,
and in the second call on the dataset containing the big cluster, another smaller cluster V2 splits
from the data and so on. Continuing in this way, suppose that the big cluster is the only cluster left

in the last recursive call as shown in Figure 1, then we have made k recursive calls on a dataset of
size at least n/2 making the running time of the algorithm O(¯nktmax) + O(nk log n). However, if Vk
splits from the dataset early on in the recursion, the running time remains O(¯ntmax) + O(n log n).
Thus, unless the dataset contains a big cluster encompassing a dominant fraction of the dataset, the
running time of the Algorithm 3 is O(¯ntmax) + O(n log n). We use p nearest neighbors to compute
the similarities between points, which results in O(pn) nonzero entries in W . Since p is a constant

typically between 4 and 10, we conclude that number of nonzero entries in W is O(n). Thus for
the p-nearest neighbor similarity function, the complexity of Algorithm 3 is O(ntmax) + O(n log n).

16

5 Simulation results

We begin with a toy example to illustrate the mechanics of Algorithm 3. Suppose that we have

the following normalized similarity matrix for a dataset with three clusters. The intra-cluster

similarities of the three clusters are represented by red, green and blue colors.





0

.4

.3

0

0

0

.01

0

0

0

.5 .45 .025 .025

0

.7

.01

0

0

0

.01

0

0

.55

0

0

0

.1

0

0

0

0

0

0

0

.4

.25

.4

0

.02

0

0

0

.3

0

.25

.3

0

0

0

0

0

0

0

.05

0

.4 .28

.3

0

.27

0

0

0

.3

.4

0

0

0

0

0

0

0

0

0

0

.02

0

0

0

.01

0

0

0

0

0

0

0

0

0

0

0

0.5 0.49

0.49 00 0.49

0.7 0.3

00

The ﬁrst call to the RARD procedure in Algorithm 3 starts by picking 10 points uniformly at

random from [0, 100]. Stopping criterion is met after 27 iterations giving us the following vector.

A bipartition of the dataset based on the gap criteria separates the blue cluster from the other two

clusters.



31.92

32.15

31.92

32.64

32.60

32.55

32.71

39.57

39.53

39.63



Algorithm 3 then makes two recursive calls to the RARD procedure on the re-normalized sub-

matrices corresponding to the two partitions. The recursive call on the ﬁrst partition containing

blue and green clusters results in a bipartition of the data into two clusters giving us the following

vector, whereas the recursive call on the second partition containing the blue cluster does not ﬁnd

any gap satisfying the stopping criteria, so the algorithm identiﬁes it as a cluster and returns out

17

of the recursion.





77.63

78.03

78.07

67.06

66.91

67.78

66.98

 21.12

20.98

21.28



The recursive call on the red and green clusters, then makes two further calls to the RARD proce-

dure, one on the red and one on the green cluster. These two calls do not ﬁnd a bipartition in the

data and return out of the recursion identifying red and green clusters.

In the following sections, we demonstrate the eﬃciency of the proposed algorithm on a variety

of synthetic and real datasets. In particular, we show that Algorithm 3 can identify clusters of

complex shapes and varying sizes in Section 5.2 and compare its accuracy with the normalized cut

algorithm [1]. We have used p-neighbor similarity measure to construct the similarity graph for all

the experiments. In Section 5.3, we portray the scalability and speed of Algorithm 3 by applying

it to large-scale stochastic block models. Finally in Section 5.4, we run Algorithm 3 on two real

datasets and compare its accuracy and speed with normalized cut algorithm, fast approximate

spectral clustering [8] and Nystrom method [6]. We implement all the algorithms in MATLAB

8.4.0 and conduct experiments on a machine with Intel Core i7 3.40GHz CPU and 16GB memory.

5.1 Performance evaluation

Mutual information is a symmetric measure to quantify the information shared between two distri-

butions. It is widely used as a measure to calculate the shared information between two clusterings.
Let V denote the cluster labels and V(cid:48) be the clustering obtained by an algorithm. Their mutual
information is deﬁned as follows:

(cid:32) P r(Vi,V(cid:48)

j)
P r(Vi)P r(V(cid:48)
j)

(cid:33)

M I(V, V(cid:48)) =

P r(Vi,V(cid:48)

j) log

(cid:88)

Vi∈V, V(cid:48)

j∈V(cid:48)

where P r(Vi) and P r(V(cid:48)
clustering V and V(cid:48)

j in clustering V(cid:48) respectively, i.e.,

j) are the probabilities that an arbitrary point belongs to cluster Vi in

P r(Vi) =

|Vi|
n

and P r(V(cid:48)

j) =

|V(cid:48)
j|
n

18

(a) Mixture of Gaussians

(b) Clustering aggregation

(c) Two crescents

(d) Half ellipses

Figure 2: Clustering result of Algorithm 3 on synthetic datasets.

Table 1: Results of 50 runs of Algorithm 3 on synthetic datasets. Parameter settings α = 1 and
0 = 10−2, 10−3, 10−3, 10−4 top to bottom. NCut shows the average normalized cut deﬁned
in Equation (12) over 50 simulations with standard error. NMI is the mean normalized mutual
information in 50 runs with standard error. Runtime shows the average computation time of a
simulation.

Datasets

NCut

0.0052 ± 0.0000
Mixture of Gaussians
Clustering aggregation 0.0323 ± 0.0000
Two crescents

0.0052 ± 0

Half ellipses

0 ± 0

NMI

97.12 ± 0.03
99.58 ± 0.03

100 ± 0
100 ± 0

Runtime

0.687

0.106

0.092

0.969

j) is the joint probability that an arbitrary point lies in both clusters Vi and V(cid:48)

j in clusterings

P r(Vi,V(cid:48)
V and V(cid:48) respectively, i.e.,

P r(Vi,V(cid:48)

j) =

|Vi ∩ V(cid:48)
j|

n

For the ease of interpretation, we use normalized mutual information deﬁned as:

N M I(V, V(cid:48)) =

M I(V, V(cid:48))

(cid:112)H(V) H(V(cid:48))

where H(V) is the entropy of V given by:

H(V) = −(cid:88)

Vi∈V

P r(Vi) log(P r(Vi))

It is easy to verify that 0 ≤ N M I(V, V(cid:48)) ≤ 1. NMI is 1 when the two clusterings are identical and
0 when the clusterings are independent.

19

-10-50510-10-8-6-4-2024685101520253035510152025510152025303540051015202530-30-20-1001020-20-15-10-505101520Table 2: Computation time (in seconds) of Algorithm 3 on SBMs with p = 0.5 and q = 0.01. The
time shown is averaged over 50 simulations on diﬀerent SBMs. All partitions are exactly recovered.

n

k

15, 000

30, 000

60, 000

5

10

15

5

10

15

5

10

15

RARD

4.19

3.51

3.23

14.47

11.59

10.46

78.21

53.27

45.44

Normalized Cut

69.46

121.14

250.07

Out of Memory

5.2 Synthetic datasets

Four two-dimensional synthetic datasets have been used to compare Algorithm 3 to the normalized

cut algorithm [1]. The details of the datasets are given in Appendix A. Table 1 portrays the results

of RARD algorithm. These results depict that our recursive implementation is very accurate in

identifying clusters of complex shapes and diﬀerent sizes. Small standard errors emphasize that
RARD algorithm has little dependence on the initial vector x0.

5.3 Scalability

We apply Algorithm 3 to stochastic block model (SBM) graphs to illustrate its scalability to large

datasets with many clusters. We also compare the runtime with normalized cut algorithm [1]. In

the basic form, a SBM with same size blocks (clusters) is deﬁned by four parameters; n, the number

of vertices; k, the number of blocks (clusters); p, the probability of an edge between two points in

the same cluster and q, the probability of an edge between two points in diﬀerent clusters. Running

time of the algorithm on various size SBMs is shown in Table 2. Runtimes shown are average times

for 50 diﬀerent SBMs. Observe that RARD algorithm is signiﬁcantly faster than normalized cut. It

also consumes less memory as compared to normalized cut. For each model, the RARD algorithm

recovers all the clusters exactly in each run with p = 0.5 and q = 0.01. Each node shares roughly
pn/k edges within the cluster and q(n−n/k) edges across the cluster. For example with n = 30, 000
and k = 10, a node approximately has an edge with 1500 nodes in its cluster and 270 edges with
nodes in other clusters. Total edges in this graph are roughly 0.5(1770 × 30, 000) = 26.55 million.
We also show the ability of the algorithm to recover correct clusters as we increase the number of

edges across clusters. Figure 3 shows the number of simulations where RARD recovered correct

clusters in 50 diﬀerent SBMs while varying q, the probability of placing an edge between two nodes

in diﬀerent clusters.

20

Figure 3: Correct clusterings recovered in 50 runs vs. q (probability of an edge between two points
in diﬀerent clusters). Parameters n = 15000 and p = 0.5.

Table 3: Comparison of Average NMI with standard error and runtime in seconds (in parenthesis)
over 50 simulations on real datasets. k denotes the number of clusters. For each k, 50 runs are
conducted on randomly chosen clusters (except for k = 10 for USPS and k = 20 for COIL20).
N-cut represents the normalized cut algorithm [1]. FASC is KASP algorithm as deﬁned in [8].
FASC 1 and 2 are implemented with 10% and 5% representative points respectively. Runtime of
FASC is signiﬁcantly more than our algorithm because it uses k-means to get the representative
points which has linear time complexity in terms of dimension of the dataset. NYSTROM 1 and
2 are approximations of Normalized cut algorithm as deﬁned in [6] with a random sample of 50%
and 20% respectively.

(a) USPS

NCUT

FASC 1

k
4 90.50 ± 2.53(0.48) 84.80 ± 1.26(4.46) 84.21 ± 1.09(1.23) 74.36 ± 1.47(1.57) 68.66 ± 1.53(0.32) 88.07 ± 1.26(0.37)
6 85.76 ± 0.93(1.13) 80.88 ± 0.87(6.28) 79.43 ± 0.92(2.39) 73.06 ± 1.02(2.21) 66.46 ± 0.69(0.77) 85.23 ± 0.87(0.58)
8 85.21 ± 0.42(2.31) 79.28 ± 0.67(8.93) 78.67 ± 0.66(5.31) 69.93 ± 0.63(4.54) 62.93 ± 0.65(1.88) 84.56 ± 0.41(0.82)
10 81.47 ± 0.00(6.71) 78.18 ± 0.42(12.30) 76.95 ± 0.51(8.05) 67.20 ± 0.27(6.60) 61.94 ± 0.30(1.79) 82.33 ± 0.22(1.14)

NYSTROM 1 NYSTROM 2

FASC 2

RARD

(b) COIL20

NCUT

FASC 1

k
4 98.68 ± 0.96(0.15) 74.44 ± 2.24(0.94) 72.82 ± 2.04(0.41) 84.86 ± 2.05(0.02) 77.31 ± 2.51(0.01) 97.32 ± 1.79(0.13)
8 97.15 ± 0.90(0.38) 73.72 ± 1.16(1.30) 71.35 ± 1.27(0.82) 83.82 ± 1.15(0.05) 73.25 ± 1.53(0.04) 94.03 ± 0.73(1.17)
12 94.58 ± 0.79(0.72) 73.35 ± 0.69(2.77) 71.01 ± 0.69(1.84) 80.29 ± 0.99(0.11) 69.88 ± 1.12(0.07) 94.36 ± 0.92(0.41)
16 92.28 ± 0.69(1.23) 74.46 ± 0.61(4.14) 70.66 ± 0.45(2.44) 76.17 ± 0.71(0.20) 67.05 ± 0.62(0.11) 92.35 ± 0.28(0.55)
20 91.93 ± 0.00(1.98) 73.81 ± 0.43(6.53) 70.92 ± 0.41(3.59) 73.31 ± 0.43(0.32) 63.61 ± 0.46(0.16) 92.90 ± 0.09(0.68)

NYSTROM 1 NYSTROM 2

FASC 2

RARD

5.4 Real datasets

We empirically compare the accuracy and speed of our RARD algorithm with normalized cut

algorithm [1], fast approximate spectral clustering [8] and Nystrom method [6] on two real datasets.

21

q00.020.040.060.080.1Correct recovery instances01020304050k = 5k = 10k = 20(a) Ncut with 30 segments

(b) RARD with 30 segments

(c) Ncut with 30 segments

(d) RARD with 30 segments

Figure 4: Comparison of Ncut and RARD on image segmentation.

The USPS dataset has 7291 instances and length of the feature vector is 256 [12]. It has a total of

10 clusters. The COIL20 dataset consists of 1140 examples and has 1024 features with 20 clusters

[13]. We use a p-nearest neighbor graph to construct the similarity matrix. For both dataset we
use p = 4. 0 is set to 10−3 and 10−2 for USPS and COIL20 datasets respectively. We compare
normalized mutual information and computation time of RARD algorithm with other algorithms.

As demonstrated earlier RARD algorithm has the same accuracy as normalized cut algorithm. Our

algorithm does not sacriﬁce accuracy as opposed to FASC and Nystrom method which also claim

to improve the speed of spectral clustering. For large datasets RARD is faster than both FASC

and Nystrom and does not compromise on the accuracy.

5.5 Image segmentation

We also test our RARD algorithm against normalized cut algorithm [1] on image segmentation on

some standard images. We show that our algorithm performs signiﬁcantly better than normalized

cut algorithm when the number of segments is large. We follow the feature selection of Shi and

Malik [1]. In particular, normalized cut fails to detect big segments and thus divides them into

smaller sub-segments. On the other hand RARD can detect segments of varying sizes correctly.

22

(a) Mixture of Gaussians

(b) Clustering aggregation

(c) Two crescents

(d) Half ellipses

Figure 5: 0 vs Ncut and number of clusters in Algorithm 3

The resulting segmentations are shown in Figure 4.

5.6 Parameter selection

Our algorithm proposed in this paper require only one parameter 0. We start with some initial
value of 0 and monitor the number of clusters produced by RARD algorithm. We then decrease
0 by a constant factor and observe the number clusters detected by RARD algorithm.
In this
fashion, we search for two consecutive 0 with the same number of clusters, i.e., we set 0, when
the number of clusters become stable. Figure 5 exhibits this procedure for synthetic datasets. If
we already know the number of clusters k in the dataset, then the value 0 is chosen corresponding
to the value of k. Gaussian similarity measure require a parameter σ which is diﬃcult to tune.

Automatically selecting σ is a challenging problem and has received interest in some recent studies.

More detailed analysis of this subject is beyond the scope this work. Interested readers can refer

to [14].

6 Connection to normalized cuts

Shi and Malik proposed a graph partitioning criteria known as normalized cut (NCut) [1]. A k-way

k(cid:88)

i=1

cut(Vi,V − Vi)

a(Vi)

,

(12)

i∈A,j∈V wij. They showed that minimizing the 2-way
normalized cut to obtain a bipartition of the graph is equal to minimizing the following Rayleigh

NCut is deﬁned as

where cut(A,B) =(cid:80)

NCut(V1,V2, . . . ,Vk) =

i∈A,j∈B wij and a(A) =(cid:80)

quotient.

yT (D − W )y

yT Dy

min

y

23

ǫ010110-110-210-310-410-510-610-7Number of Clusters510152025303540455055ǫ010110-110-210-310-410-510-610-7Number of Clusters510152025303540ǫ010110-110-210-310-410-510-610-7Number of Clusters0246810121416ǫ010110-110-210-310-410-510-610-7Number of Clusters2468101214161820where D is the degree matrix. The vector y of length n satisﬁes yT D1 = 0 and yi ∈ {1,−b}
with b some constant in (0, 1). This problem is NP-Hard [15] and is approximated by relaxing y

to take on real values. This approximation leads to solving the generalized eigenvalue problem
(D − W )y = λDy for the second smallest eigenvalue also known as the Fiedler value. Since D is
invertible the generalized eigenvalue problem is equivalent to solving

(I − D−1W )y = λy.

(13)

Thus, minimizing a 2-way Ncut is approximated by ﬁnding the second smallest eigenvector of
I − D−1W . In this paper, we are looking for a linear combination of the eigenvectors of D−1W .
Observe that eigenvectors of both these systems are same, however the eigenvalues are translated

by 1.

7 Conclusions

We have proposed a fast spectral clustering algorithm based on a mixing process, which does not

explicitly compute the eigenvectors of a similarity matrix. It rather ﬁnds an eigenvalue weighted

linear combination of eigenvectors of the normalized similarity matrix. Our algorithms are simple

to implement and computationally eﬃcient. We have demonstrated the scalability and accuracy of

the RARD algorithm by implementing it on large stochastic block models with tens of thousands

of nodes and hundreds of millions of edges. We have also shown that our RARD algorithm has the

same accuracy as normalized cut algorithm. Thus our algorithm does not compromise on accuracy

to achieve faster speed unlike other algorithms which claim to speed-up spectral clustering such as

fast approximate spectral clustering [8] and Nystrom method [6].

Acknowledgments

This research is supported in part by NSF grant CNS 13-30077 and DMS 1312907.

References

[1] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions

on Pattern Analysis and Machine Intelligence, 22(8):888–905, 2000.

[2] Chris HQ Ding, Xiaofeng He, Hongyuan Zha, Ming Gu, and Horst D Simon. A min-max cut

algorithm for graph partitioning and data clustering. In Proceedings of the IEEE International

Conference on Data Mining, pages 107–114, 2001.

24

[3] Andrew Y Ng, Michael I Jordan, and Yair Weiss. On spectral clustering: Analysis and an

algorithm. In Advances in Neural Information Processing Systems 14, pages 849–856. 2002.

[4] Francis R. Bach and Michael I. Jordan. Learning spectral clustering. In Advances in Neural

Information Processing Systems 16, pages 305–312. 2004.

[5] Peter Arbenz and Daniel Kressner. Lecture notes on solving large scale eigenvalue problems.

D-MATH, EHT Zurich, 2012.

[6] Charless Fowlkes, Serge Belongie, Fan Chung, and Jitendra Malik. Spectral grouping us-

ing the Nystrom method. IEEE Transactions on Pattern Analysis and Machine Intelligence,

26(2):214–225, 2004.

[7] Tomoya Sakai and Atsushi Imiya. Fast spectral clustering with random projection and sam-

pling. In Machine Learning and Data Mining in Pattern Recognition, volume 5632 of Lecture

Notes in Computer Science, pages 372–384. 2009.

[8] Donghui Yan, Ling Huang, and Michael I Jordan. Fast approximate spectral clustering. In

Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and

Data Mining, pages 907–916, 2009.

[9] Wen-Yen Chen, Yangqiu Song, Hongjie Bai, Chih-Jen Lin, and Edward Y Chang. Paral-

lel spectral clustering in distributed systems.

IEEE Transactions on Pattern Analysis and

Machine Intelligence, 33(3):568–586, 2011.

[10] Fan RK Chung. Spectral graph theory, volume 92. American Mathematical Soc., 1997.

[11] Herbert A Simon and Albert Ando. Aggregation of variables in dynamic systems. Economet-

rica: Journal of the Econometric Society, pages 111–138, 1961.

[12] Yann Le Cun, Bernhard E Boser, John S Denker, Donnie Henderson, Richard E Howard,

Wayne E. Hubbard, and Lawrence D Jackel. Handwritten digit recognition with a back-

propagation network. In Advances in Neural Information Processing Systems 2, pages 396–404.

1990.

[13] Sameer A Nene, Shree K Nayar, and Hiroshi Murase. Columbia object image library (coil-20).

Technical report, CUCS-005-96, 1996.

[14] Lihi Zelnik-Manor and Pietro Perona. Self-tuning spectral clustering. In Advances in Neural

Information Processing Systems 17, pages 1601–1608. 2005.

[15] Charu C Aggarwal and Chandan K Reddy. Data clustering: algorithms and applications. CRC

Press, 2013.

25

[16] Aristides Gionis, Heikki Mannila, and Panayiotis Tsaparas. Clustering aggregation. ACM

Transactions on Knowledge Discovery from Data, 1(1):4, 2007.

26

A Details of Datasets

A.1 Synthetic Datasets

• Mixture of Gaussians: We consider a mixture of ﬁve Gaussians random variables X1, X2, X3, X4

and X5 with diﬀerent densities. The ﬁve mean vectors and covariance matrices are

, µ2 =

, µ3 =

, µ4 =

, µ5 =

(cid:35)

6
−6

(cid:34)
(cid:35)
(cid:35)

(cid:35)

(cid:34) −6
(cid:35)

6

,

2

0

0

2

(cid:34)

(cid:34)

(cid:35)

5

5

−5

(cid:34) −5
(cid:34)
(cid:34)

.5

(cid:35)

(cid:35)
(cid:35)

0

0 .5

1 0

0 1

µ1 =

Σ1 =

Σ4 =

(cid:34)

0

(cid:35)
(cid:34)
(cid:34)

0

, Σ2 =

, Σ3 =

3.5

0

0
3.5
1 −.5
1.5

−.5

, Σ5 =

A sample of 2000 points is taken from these distributions with 100, 1000, 300, 200 and 400
samples from X1, X2, X3, X4 and X5 respectively. We have used σ = 0.5 for RBF similarity.
• Clustering aggregation: This dataset set is taken from Clustering aggregation paper [16]. The
authors show that single link, complete link, average link, ward’s method and k-means fail to

recover the correct clusters in this data set. It has 7 clusters and 788 total points. Parameter

σ is kept at 1 for RBF similarity function.

• Two crescents: This dataset has two clusters with a total of 384 points. The value of param-

eter σ in the RBF similarity function is kept equal to 1.5.

• Half ellipses: This dataset contains a total of 2000 points with 1000 points in each cluster.

Parameter σ = 2.5 is used for RBF similarity.

A.2 Real Datasets

• USPS [12]: This dataset contains 7291 grayscale images of digits 0−9 scanned from envelopes
by United States Postal Service. Each feature vector consists of normalized grayscale values
of 16 × 16 = 256 pixels.

• COIL20 [13]: This dataset consists of 32 × 32 grayscale images of 20 diﬀerent objects. 72
diﬀerent images of each object are taken from 72 diﬀerent angles as the objects are rotated

on a table, i.e., after every 5 degree rotation an image is taken. A feature vector consists of

1024 normalized grayscale values i.e., one value for each pixel.

27

Figure 6: A sample of USPS dataset

Figure 7: A sample of COIL20 dataset

28

