Distribution Free Learning with Local Queries∗
Shai Shalev-Shwartz§
Galit Bary-Weisberg†

Amit Daniely‡

March 14, 2016

Abstract

The model of learning with local membership queries interpolates between the PAC
model and the membership queries model by allowing the learner to query the label of
any example that is similar to an example in the training set. This model, recently pro-
posed and studied by Awasthi et al. [5], aims to facilitate practical use of membership
queries.
We continue this line of work, proving both positive and negative results in the
distribution free setting. We restrict to the boolean cube {−1, 1}n, and say that a
query is q-local if it is of a hamming distance ≤ q from some training example. On
the positive side, we show that 1-local queries already give an additional strength, and
allow to learn a certain type of DNF formulas. On the negative side, we show that

even (cid:0)n0.99(cid:1)-local queries cannot help to learn various classes including Automata,
algorithm that uses(cid:0)log0.99(n)(cid:1)-local queries would lead to a breakthrough in the best

DNFs and more. Likewise, q-local queries for any constant q cannot help to learn
Juntas, Decision Trees, Sparse Polynomials and more. Moreover, for these classes, an

known running times.

6
1
0
2

 
r
a

 

M
1
1

 
 
]

G
L
.
s
c
[
 
 

1
v
4
1
7
3
0

.

3
0
6
1
:
v
i
X
r
a

discussion, as well as experiments.

∗This paper is based on the M.Sc. thesis [6] of the ﬁrst author. The thesis oﬀers a more elaborated
†Matiﬁc inc. Most work was done while the author was an M.Sc. student at the Hebrew University,
‡Google inc. Most work was done while the author was a Ph.D. student at the Hebrew University,
§School of Computer Science and Engineering, The Hebrew University, Jerusalem, Israel

Jerusalem, Israel

Jerusalem, Israel

1

Introduction

A child typically learns to recognize a cat based on two types of input. The ﬁrst is given by
her parents, pointing at a cat and saying “Look, a cat!”. The second is given as a response to
the child’s frequent question “What is that?”. These two types of input were the basis for the
learning model originally suggested by Valiant [21]. Indeed, in Valiant’s model, the learner
can randomly sample labelled examples from “nature”, but it can also make a membership
query (MQ) for the label of any unseen example. Today, the acronym PAC stands for
the restricted model in which MQ are forbidden, while the full model is called PAC+MQ.
Much work has been done investigating the limits and the strengths of MQ. In particular,
membership queries were proven stronger than the vanilla PAC model [1, 10, 12, 16]. Yet,
MQ are rarely used in practice. This is commonly attributed to the fact that MQ algorithms
query very artiﬁcial examples, that are uninterpretable by humans (e.g., [8]).

Awasthi et al. [5] suggested a solution to the problem of unnatural examples. They
considered a mid-way model that allows algorithms to make only local queries, i.e., query
examples that are close to examples from the sample set. Hopefully, examples which are
similar to natural examples will appear natural to humans. Awasti et al. considered the
case where the instance space is {−1, 1}n and the distance between examples is the Hamming
distance. They proved positive results on learning sparse polynomials with O(log(n))-local
queries under what they deﬁned as locally smooth distributions1. They also proposed an
algorithm that learns DNF formulas under the uniform distribution in quasi-polynomial time
using O(log(n))-local queries.

Our work follows Awasthi et al. and investigates local queries in the distribution free
setting, in which no explicit assumptions are made on the underlying distribution, but only
on the learned hypothesis. We prove both positive and negative results in this context:

• One of the strongest and most beautiful results in the MQ model shows that automata
are learnable with membership queries [1]. We show that unfortunately, this is probably
not the case with local queries. Concretely, we show that even (n0.99)-local queries
cannot help to learn automata. Namely, such an algorithm will imply a standard PAC
algorithm for automata. As learning automata is hard under several assumptions [17,
13], our result suggests that it is hard to learn automata even with (n0.99)-local queries.
• We prove a similar result for several additional classes. Namely, we show that (n0.99)-
local queries cannot help to learn DNFs, intersection of halfspaces, decision lists, depth-
d circuits for any d ≥ 2, and depth-d threshold circuits for any d ≥ 2. Likewise, for
any constant q, q-local queries cannot help to learn Juntas, Decision Trees, Sparse
polynomials, and Sparse polynomial threshold functions. In fact, we show that even

(cid:0)log0.99(n)(cid:1)-local queries are unlikely to lead to polynomial time algorithms. Namely,
any algorithm that uses (cid:0)log0.99(n)(cid:1)-local queries will result with a PAC algorithm

whose running time signiﬁcantly improves the state of the art in these well studied
problems.

• On the positive side we show that already 1-local queries are probably stronger than
the vanilla PAC model. Concretely, we show that a certain simplistic but natural

1A distribution is locally α-smooth for α ≥ 1 if its density function is log(α)-Lipschitz.

1

learning problem, which we term learning DNFs with evident examples, is learnable
with 1-local queries. Furthermore, we show that without queries, this problem is at
least as hard as learning decision trees, that are conjectured to be hard to learn.

2 Previous Work

log(n)

Membership Queries Several concept classes are known to be learnable only if mem-
bership queries are allowed: Deterministic Finite Automatons [1], k-term DNF for k =
log(log(n)) [10], decision trees and k-almost monotone-DNF formulas [12], intersections of k-
halfspaces [7] and DNF formulas under the uniform distribution [16]. The last result builds
on Freund’s boosting algorithm [15] and the Fourier-based technique for learning using mem-
bership queries due to [18]. We note that there are cases in which MQ do not help. E.g., in
the case of learning DNF and CNF formulas [2], assuming that one way functions exist, and
in the case of distribution free agnostic learning [14].

Local Membership Queries Awasthi et al.
focused on learning with O(log(n))-local
queries. They showed that t-sparse polynomials are learnable under locally smooth distri-
butions using O (log(n) + log(t))-local queries, and that DNF formulas are learnable under
the uniform distribution in quasi-polynomial time (nO(log log n)) using O(log(n))-local queries.
They also presented some results regarding the strength of local MQ. They proved that un-
der standard cryptographic assumptions, (r + 1)-local queries are more powerful than r-local
queries (for every 1 ≤ r ≤ n − 1). They also showed that local queries do not always help.
They showed that if a concept class is agnostically learnable under the uniform distribution
using k-local queries (for constant k) then it is also agnostically learnable (under the uniform
distribution) in the PAC model.

We remark that besides local queries, there were additional suggestions to solve the
problem of unnatural examples. For example, to allow “I don’t know” answers, or to be
tolerant to some incorrect answers [3, 4, 11, 20, 9].

3 Setting
We consider binary classiﬁcation where the instance space is X = Xn = {−1, 1}n and the
label space is Y = {0, 1}. A learning problem is deﬁned by a hypothesis class H ⊂ {0, 1}X .
The learner receives a training set

S = {(x1, h(cid:63)(x1)), (x2, h(cid:63)(x2)), . . . , (xm, h(cid:63)(xm))} ∈ (X × Y)m

where the xi’s are sampled i.i.d. from some unknown distribution D on X and h(cid:63) : X → Y is
some unknown hypothesis. The learner is also allowed to make membership queries. Namely,
to call an oracle, which receives as input some x ∈ X and returns h(cid:63)(x). We say that a
membership query for x ∈ X is q-local if there exists a training example xi whose Hamming
distance from x is at most q.
The learner returns (a description of) a hypothesis ˆh : X → Y. The goal is to approximate
h(cid:63), namely to ﬁnd ˆh : X → Y with loss as small as possible, where the loss is deﬁned as

2

(cid:16)ˆh(x) (cid:54)= h(cid:63)(x)
(cid:17)

LD,h(cid:63)(ˆh) = Prx∼D
. We will focus on the so-called realizable case where h(cid:63) is
assumed to be in H, and will require algorithms to return a hypothesis with loss <  in time
that is polynomial in n and 1

 . Concretely,

Deﬁnition 3.1 (Membership-Query Learning Algorithm) We say that a learning
algorithm A learns H with q-local membership queries if

• There exists a function mA (n, ) ≤ poly(cid:0)n, 1

(cid:1), such that for every distribution D over

X , every h(cid:63) ∈ H and every  > 0, if A is given access to q-local membership queries,
and a training sequence



S = {(x1, h(cid:63)(x1)), (x2, h(cid:63)(x2)), . . . , (xm, h(cid:63)(xm))}

where the xi’s are sampled i.i.d. from D and m ≥ mA(n, ), then with probability of at
least2 3

4 (over the choice of S), the output ˆh of A satisﬁes LD,h(cid:63)(ˆh) < .

• Given a training set of size m

– A asks at most poly(m, n) membership queries.
– A runs in time poly(m, n).
– The hypothesis returned by A can be evaluated in time poly(m, n).

We remark that learning with 0-local queries is equivalent to PAC learning, while learning
with n-local queries is equivalent to PAC+MQ learning.

4 Learning DNFs with Evident Examples

Intuitively, when evaluating a DNF formula on a given example, we check a few conditions
corresponding to the formula’s terms, and deem the example positive if one of them holds. We
will consider the case that for each of these conditions, there is a chance to see a “prototype
example”, that satisﬁes it in a strong or evident way.
In the sequel, we denote by hF :
{−1, 1}n → {0, 1} the function induced by a DNF formula F over n variables.
Deﬁnition 4.1 Let F = T1 ∨ T2 ∨ . . . ∨ Td be a DNF formula. We say that an example
x ∈ {−1, 1}n satisﬁes a term Ti (with respect to the formula F ) evidently and denote
Ti(x) ≡ 1 if :

• It satisﬁes Ti. (In particular, hF (x) = 1)
• It does not satisfy any other term Tk (for k (cid:54)= i) from F.
• No coordinate change will turn Ti False and another term Tk True. Concretely, if for
j ∈ [n] we denote x⊕j = (x1, . . . , xj−1,−xj, xj+1, . . . , xn), then for every coordinate
j ∈ [n], if x⊕j satisﬁes F (i.e. if hF (x⊕j) = 1) then x⊕j satisﬁes Ti and only Ti.

2The success probability can be ampliﬁed to 1 − δ by repetition.

3

Deﬁnition 4.2 Let F = T1 ∨ T2 ∨ . . . ∨ Td be a DNF formula. We say that h(cid:63) : {−1, 1}n →
{0, 1} is realized by F with evident examples w.r.t. a distribution D if h(cid:63) = hF and for
every term3 Ti, Prx∼D (Ti(x) ≡ 1|Ti(x) = 1) ≥ 1
n.

For example, the deﬁnition holds whenever h(cid:63) is realized by a DNF in which any pair of
diﬀerent terms contains two opposite literals. Also, functions that are realized by a decision
tree can be also realized by a DNF in which every positive example satisﬁes a single term,
corresponding to root-to-leaf path that ends with a leaf labelled by 1. Hence, the assumption
holds for decision trees provided that for every such path, there is a chance to see an example
satisfying it evidently, in the sense that ﬂipping each variable in the path will turn the
example negative.

4.1 An algorithm

Algorithm 1
Input: S1, S2 ∈ ({−1, 1}n × {0, 1})m
Output: A DNF formula H

Start with an empty DNF formula H
for all positive examples (x, y) ∈ S1 do

Deﬁne T = x1 ∧ x1 ∧ x2 ∧ x2 ∧ . . . ∧ xn ∧ xn
for 1 ≤ j ≤ n do

Query x⊕j to get h(cid:63)(x⊕j)
if h(cid:63)(x⊕j) = 1 then

Remove xj and xj from T

if h(cid:63)(x⊕j) = 0 then

if xj = 1 then

Remove xj from T

if xj = 0 then

Remove xj from T

H = H ∨ T
if T (x) = 1 but y = 0 for some (x, y) ∈ S2 then

for all T in H do

Remove T from H

Return H

Theorem 4.3 Algorithm 1 learns with 1-local-queries poly-sized DNFs with evident exam-
ples.

Idea The algorithm is based on the following claim that follows easily from deﬁnition 4.1.

3We note that the quantity 1

c > 0.

n in the following equation is arbitrary, and can be replaced by 1

nc for any

4

Claim 1 Let F = T1∨T2∨. . .∨Td be a DNF formula over {−1, 1}n. For every x ∈ {−1, 1}n
that satisﬁes a term Ti evidently (with respect to F ), and for every j ∈ [n] it holds that:

hF (x⊕j) = 1 ⇐⇒ the term Ti does not contain the variable xj

By this claim, if x is an evident example for a certain term T , one can easily reconstruct T .
Indeed, by ﬂipping the value of each variable and checking if the label changes, one can infer
which variables appear in T . Furthermore, the sign of these variables can be inferred from
their sign in x. Hence, after seeing an evident example for all terms, one can have a list of
terms containing all terms in the DNF. This list might have terms that are not part of the
DNF. Yet, such terms can be thrown away later by testing if they make wrong predictions.

Proof
(of Theorem 4.3) We will show that algorithm 1 learns with 1-local-queries any
function that is realized by a DNF of size ≤ n2 with evident examples. Adapting the proof
to hold with nc instead of n2, for any c > 0, is straight-forward. First, it is easy to see that
this algorithm is eﬃcient. Now, ﬁx a distribution D and let h(cid:63) : {−1, 1}n → {0, 1} be a
hypothesis that is realized, w.r.t. D, by a DNF formula F = T1 ∨ T2 ∨ . . . ∨ Td, d ≤ n2
with evident examples. Let  > 0, and suppose we run the algorithm on two indepent
samples from D, denoted S1 = {(xi, h(cid:63)(xi)}m1
i, h(cid:63)(x(cid:48)
i=1. We will show that
if m1 ≥ 32n3
then with probability of at least
4, the algorithm will return a function ˆh with LD,h(cid:63)(ˆh) < . Let H be the DNF formula
3
returned by the algorithm, and let ˆh be the function induced by H. We have that

 and m2 ≥ 32m1

i=1 and S2 = {(x(cid:48)

i)}m2

log 32m1



log 32n2

log 32d





LD,h(cid:63)(ˆh) = Pr
x∼D

h(cid:63)(x) (cid:54)= ˆh(x)

h(cid:63)(x) = 1 and ˆh(x) = 0

(cid:17)

(cid:16)

+ Pr
x∼D

(cid:17)

h(cid:63)(x) = 0 and ˆh(x) = 1



 ≥ 32nd
(cid:16)
(cid:16)

= Pr
x∼D

(cid:17)

The proof will now follow from claims 2 and 3.

Claim 2 With probability at least 7

8 over the choice of S1, S2 we have

(cid:16)

Pr
x∼D

(cid:17) ≤ 

2

h(cid:63)(x) = 1 and ˆh(x) = 0

(1)

Proof We ﬁrst note that if there is an evident example x for a term Ti in S1, then Ti will
be in the output formula. Indeed, in the for-loop that go over the examples in S1, when
processing the example (x, h(cid:63)(x)), it is not hard to see that Ti will be added. We furthermore
claim that the term Ti won’t be removed at the for loop that tests the terms collected in the
ﬁrst loop. Indeed, if for some (x, h(cid:63)(x)) ∈ S2 we have Ti(x) = 1, it must be the case that
h(cid:63)(x) = 1. Now, we say that the term Ti is revealed if we see an evident example for this
term in S1. We also denote pi = Prx∼D (Ti(x) = 1). We have

(cid:16)

Pr
x∼D

h(cid:63)(x) = 1 and ˆh(x) = 0

(cid:17)

Ti(x) = 1 and ˆh(x) = 0

pi

(cid:17) ≤ d(cid:88)

i=1

≤

(cid:16)
(cid:88)

Pr
x∼D

i:Ti is not revealed

5

n

E

(cid:104)

(cid:35)

(cid:16)

Pr
x∼D

S1∼Dm1

pi · 1Ai

h(cid:63)(x) = 1 and ˆh(x) = 0

event that Ti is not revealed, we have

Now, by the assumption that h(cid:63) is realized with evident queries, the probability (over the

(cid:1)m1. Hence, if we denote by Ai the
choice of S1, S2) that Ti is not revealed is at most(cid:0)1 − pi
(cid:34) d(cid:88)
(cid:17)(cid:105) ≤ ES1∼Dm1
d(cid:88)
d(cid:88)
d(cid:88)
(cid:88)
≤ (cid:88)

(cid:17)m1
(cid:17)m1
(cid:88)

1 − pi
n
1 − pi
n

(cid:88)
(cid:17)m1

piES1∼Dm1 [1Ai]

piPrS1∼Dm1 [Ai]

1 − pi
n

(cid:17)m1

i|pi< 

(cid:16)

(cid:16)

+

(cid:16)

=

=

(cid:16)

=

=

pi

pi

pi

i=1

i=1

i=1

i=1

32d

32d

i|pi≥ 
1 − pi
n



32d

+

+

(cid:88)

i|pi≥ 

32d

e− m1pi

n

i|pi< 


32d

≤ d ·

32d
i|pi≥ 
+ d · e− m1

32dn

32d

≤ 
32

Since m1 ≥ 32dn
conclude that the probabilty over the coice of S1, S2 that (1) does not lold is less than 1

16. By Markov’s inequality we
8. 2

the last expression is bounded by 

log 32d




Claim 3 With probability at least 7

8 over the choice of S1, S2 we have

h(cid:63)(x) = 0 and ˆh(x) = 1

(2)

Proof Let ˆT1, . . . , ˆTr be the terms that were added to H at the ﬁrst for-loop. Denote
qi = Prx∼D

. We have

(cid:16)
(cid:16) ˆTi(x) = 1 and h(cid:63)(x) = 0

Pr
x∼D

(cid:17)

(cid:16)

Pr
x∼D

(cid:17) ≤ 

2

(cid:88)

(cid:17) ≤

h(cid:63)(x) = 0 and ˆh(x) = 1

qi

i : ˆTi is not removed

Now, the probability that ˆTi is not removed is (1 − qi)m2. Hence, using an argument similar
to the one used in the proof of claim 2, and since m2 ≥ 32m1
 , the claim
2
follows.
2

 ≥ 32r

log 32m1

log 32r





6

4.2 A matching Lower Bound

In this section we provide evidence that the use of queries in our algorithm is crucial. We will
show that the problem of learning poly-sized decision trees can be reduced to the problem
of learning DNFs with evident examples. As learning decision trees is conjectured to be
intractable, this reduction serves as an indication that learning DNFs with strongly evident
examples is hard without membership queries. In fact, we will show that learning decision
trees can be even reduced to the case that all positive examples are evident.

Theorem 4.4 Learning poly-sized DNFs with evident examples is as hard as learning poly-
sized decision trees.

We denote by hT the function induced by a decision tree T . The proof will use the following
claim:
Claim 4 There exists a mapping (a reduction) ϕ : {−1, 1}n → {−1, 1}2n, that can be eval-
uated in poly(n) time so that for every decision tree T over {−1, 1}n there exists a DNF
formula F over {−1, 1}2n such that the following holds:

1. The number of terms in F is upper bounded by the number of leaves in T
2. hT = hF ◦ ϕ
3. ∀x such that hT (x) = 1 , ϕ(x) satisﬁes some term in F evidently.

Proof Deﬁne ϕ as follows:

∀x = (x1, x2, . . . , xn) ∈ Xn

ϕ(x1, x2, . . . , xn) = (x1, x1, x2, x2, . . . , xn, xn)

Now, for every tree T , we will build the desired DNF formula F as follows: First we build
a DNF formula F (cid:48) over {−1, 1}n . Every leaf labeled ’1’ in T will deﬁne the following term-
take the path from the root to that leaf and form the logical AND of the literals describing
the path. F (cid:48) will be a disjunction of these terms. Now, for every term T in F (cid:48) we will
deﬁne a term φ(T ) over X2n in the following way: Let PT = {i ∈ [n] : xi appear in T} and
NT = {i ∈ [n] : xi appear in T}. So

T =

xj

xj

(cid:94)
(cid:94)

j∈PT

(cid:94)
(cid:94)

j∈NT

(cid:94)

j∈NT

x2j

x2j−1

x2j−1

x2j

j∈PT

j∈NT

Deﬁne

(cid:94)

j∈PT

φ(T ) =

Finally, deﬁne F to be the DNF formula over X2n given by

F =

φ(T )

(cid:95)

T∈F (cid:48)

7

We will now prove that ϕ and F satisfy the required conditions. First, ϕ can be evaluated in
linear time in n. Second, it is easy to see that hT = hF ◦ ϕ, and as every term in F matches
one of T ’s leaves, the number of terms in F cannot exceed the number of leaves in T . It
is left to show that the third requirement holds. Let there be an x such that hT (x) = 1,
then x is matched to one and only one path from T ’s root to a leaf labeled ’1’. From the
construction of F , x satisﬁes one and only one term in F (cid:48). Regarding the last requirement,
that no coordinate change will make one term from F False and another one True, we made
sure this will not happen by “doubling” each variable. By this construction, in order to
2
change a term from False to True at least two coordinate must change their value.

We are now ready to prove theorem 4.4.

[of theorem 4.4] Suppose that A learns size-n DNFs with evident examples. Using
Proof
the reduction from claim 4 we will build an eﬃcient algorithm B that learns size-n decision
trees. For every training set

S = {(x1, h(cid:63)(x1)), (x2, h(cid:63)(x2)), . . . , (xm, h(cid:63)(xm))} ∈ (Xn × {0, 1})m

we deﬁne

ϕ(S) := {(ϕ(x1), h(cid:63)(x1)), (ϕ(x2)), h(cid:63)(x2)), . . . , (ϕ(xm), h(cid:63)(xm))} ∈ (X2n × {0, 1})m

The algorithm B will work as follows: Given a training set S, B will run A on ϕ(S), and will
return ˆh ◦ ϕ, where ˆh is the hypothesis returned by A. Since ϕ can be evaluated in poly(n)
time and A is eﬃcient, B is also eﬃcient. We will prove that B learns size-n trees. Since A
such that if A is given a training sequence

learns size-n DNFs with evident examples, there exists a function mA (n, ) ≤ poly(cid:0)n, 1

(cid:1),



S = {(x1, h(cid:63)(x1)), (x2, h(cid:63)(x2)), . . . , (xm, h(cid:63)(xm))} ∈ (Xn × {0, 1})m

where the xi’s are sampled i.i.d. from a distribution D, h(cid:63) is realized by a poly-sized DNF
with evident examples, and m ≥ mA(n, ), then with probability of at least 3
4 (over the
choice of S), the output ˆh of A satisﬁes LD,h(cid:63)(ˆh) ≤ . Let D be a distribution on Xn and let
hT be a hypothesis that can be realized by a tree with ≤ n leafs. Deﬁne a distribution ˜D on
X2n by,

if ∃x ∈ Xn such that z = ϕ(x)
otherwise

(cid:40)D(x)

0

˜(D)(z) =

Now, since hT is realized by T , from the conditions that ϕ satisﬁes, we get that hT = h ◦ ϕ,
where h is realized by a DNF of size ≤ n with evident examples w.r.t. ˜D. Now if S ∈
(Xn × {0, 1})m is an i.i.d. sample with m ≥ mA(2n, ) we have that with probability of at
least 3

4 it holds that

8

LD,hT (B(S)) = LD,hT (ˆh ◦ ϕ)

x∼D[hT (x) (cid:54)= ˆh ◦ ϕ(x)]
x∼D[h ◦ ϕ(x) (cid:54)= ˆh ◦ ϕ(x)]
= Pr
z∼ ˜D

[h(z) (cid:54)= ˆh(z)]

= Pr

= Pr

= L ˜D,h(ˆh)
= L ˜D,h(A(ϕ(S))) < 

2

5 Lower Bounds

We ﬁrst present a general technique to prove lower bounds on learning with local queries.
For A ⊂ Xn and q > 0 denote

B(A, q) = {x ∈ Xn | ∃a ∈ A, d(x, a) ≤ q}

We say that a mapping ϕ : {−1, 1}n → {−1, 1}n(cid:48)
to a class H(cid:48) if the following holds:

is a q-reduction of type A from a class H

1. ϕ is eﬃciently computable.
2. For every h ∈ H there is h(cid:48) ∈ H(cid:48) such that

(a) h = h(cid:48) ◦ ϕ
(b) The restriction of h(cid:48) to B(ϕ(Xn), q) \ ϕ(Xn) is the constant function 1.

We say that a mapping ϕ : {−1, 1}n → {−1, 1}n(cid:48)
is a q-reduction of type B if the same
requirements hold, except that (2b) is replaced by the following condition: For every z ∈
B(ϕ(Xn), q) there is a unique x ∈ Xn satisfying d(z, ϕ(x)) ≤ q, and furthermore, h(cid:48)(z) =
h(x).
Lemma 5.1 Suppose that there is a q-reduction ϕ from H to H(cid:48). Then, learning H(cid:48) with
q-local queries is as hard as PAC learning H.

(sketch) Suppose that A(cid:48)

learns H(cid:48) with q-local queries. We need to show
Proof
that there is an algorithm that learns H.
Indeed, by an argument similar to the
it is not hard to verify that the following algorithm learns H.
one in theorem 4.4,
Given a sample {(x1, y1), . . . , (xm, ym)} ⊂ {−1, 1}n × {0, 1}, run A(cid:48) on the sample
{(ϕ(x1), y1), . . . , (ϕ(xm), ym)} ⊂ {−1, 1}n(cid:48) × {0, 1}. Whenever A(cid:48) makes a q-local query
for z ∈ {−1, 1}n(cid:48)
, respond 1 if ϕ is of type A. If ϕ is of type B, respond yi, where xi is the
unique training sample satisfying d(z, ϕ(xi)) ≤ q. Finally, if A(cid:48) returned the hypothesis h(cid:48),
return h(cid:48) ◦ ϕ.

2

9

(cid:17)

(cid:16)

We next use Lemma 5.1 to prove that for several classes, local queries are not useful. Namely,
if the class is learnable with local queries then it is also learnable without queries. We will
use the following terminology. We say that q-local queries cannot help to learn a class H if
H is learnable if and only if it is learnable with q-local queries.
Corollary 5.2 For every 0 > 0, (n1−0)-local queries cannot help to learn poly-sized DNFs,
intersection of halfspaces, decision lists, depth-d circuits for any d = d(n) ≥ 2, and depth-d
threshold circuits for any d = d(n) ≥ 2.
Proof We will only prove the corollary for DNFs. The proofs for the remaining classes
are similar. Also, for simplicity, we will assume that 0 = 1
3. Consider the mapping ϕ :
{−1, 1}n → {−1, 1}n3 that replicates each coordinate n2 times. To establish the corollary,
-reduction of type A from poly-sized DNF to poly-sized
we show that ϕ is an
DNF.
Indeed, let F = T1 ∨ . . . ∨ Td be a DNF formula on n variables, consider the following
formula on the n3 variables {xi,j}1≤i≤n,1≤j≤n2. Let,
t ({xi,j}i,j) = Tt(x1,1, . . . , xn,1)
T (cid:48)
G(cid:48)({xi,j}i,j) = ∨n
F (cid:48) = (T (cid:48)

j=1 (xi,j ∧ ¬xi,j+1) ∨ (xi,j+1 ∧ ¬xi,j)

i=1 ∨n2−1
1 ∨ . . . ∨ T (cid:48)

It is not hard to verify that hF = hF (cid:48)◦ϕ. Moreover, if x = {xi,j}i,j ∈ B(ϕ(Xn), n2−1)\ϕ(Xn)
then xi,j (cid:54)= xi,j+1 for some i, j, meaning that hG(x) = 1 and therefore also hF (x) = 1.
2
Corollary 5.3 For every 0 > 0, (n1−0)-local queries cannot help to learn poly-sized au-
tomata.

d) ∨ G(cid:48)

3 − 1

(n(cid:48)) 2

(cid:16)

(cid:17)

(n(cid:48)) 2

3 − 1

Proof Again, for simplicity, we assume that 0 = 1
3, and consider the mapping ϕ :
{−1, 1}n → {−1, 1}n3 that replicates each coordinate n2 times. To establish the corol-
-reduction of type A from poly-sized automata to
lary, we show that ϕ is an
poly-sized automata.
Indeed, let A be an automaton on n variables. It is enough to show that there is an
automaton A(cid:48) on the n3 variables {xi,j}1≤i≤n,1≤j≤n2 such that (i) the size of A(cid:48) is polynomial,
(ii) A(cid:48) accepts any string in B(ϕ(Xn), n2 − 1)\ ϕ(Xn), and (iii) A(cid:48) accepts ϕ(x) if and only if
A accepts x. Now, by the product construction of automatons [19], if A(cid:48)
2 are automata
that induce the functions hA(cid:48)
can
2|. Hence, it is enough to show that there are
be induced by an automaton of size |A(cid:48)
poly-sized automata A(cid:48)

: {−1, 1}n3 → {0, 1}, then the function hA(cid:48)

2 that satisﬁes (ii) and (iii) respectively.

1| · |A(cid:48)

∨ hA(cid:48)

1, A(cid:48)

1, A(cid:48)

, hA(cid:48)

A construction of a size O(n2) automaton that satisﬁes (ii) is a simple exercise. We next
explain how to construct a poly-sized automaton A(cid:48)
2 satisfying (iii). The states of A(cid:48)
2 will
be the S(A) × [n2] (here, S(A) denotes the set of states of A). The start state of A(cid:48)
2 will
be (α0, 1), where α0 is the start state of A, and the accept states of A(cid:48)
2 will be the cartesian
product of the accept states of A with [n2]. Finally if δ : S(A) × {−1, 1} → S(A) is the
transition function of A, then the transition function of A(cid:48)

2 is deﬁned by

1

2

1

2

δ(cid:48)((α, i), b) =

(α, i + 1)
(δ(α, b), 1)

1 ≤ i < n2
i = n2

(cid:40)

10

2 satisﬁes (iii).

It is not hard to verify that A(cid:48)
2
In the next corollary, a Junta of size t is a function h : {−1, 1}n → {0, 1} that depends
on ≤ log(t) variables. The rational behind this deﬁnition of size, is the fact that in order
to describe a general function that depends on K variables, at least 2K bits are required.
Likewise, the sample complexity of learning Juntas is proportional to t rather than log(t)

Corollary 5.4 For every constant q0 > 0, q0-local queries cannot help to learn poly-sized
Juntas.
Proof Consider the mapping ϕ : {−1, 1}n → {−1, 1}(2q0+1)n that replicates each coordinate
2q0 + 1 times. To establish the corollary, we show that ϕ is a q0-reduction of type B from
poly-sized Juntas to poly-sized Juntas. Indeed, let h : {−1, 1}n → {0, 1} be a function that
depends on K variables. It is enough to show that there is a function h(cid:48) on the variables
{zi,j}i∈[n],j∈[2q0+1] that satisﬁes (i) h = h(cid:48) ◦ ϕ, (ii) for every x ∈ X , if z ∈ {−1, 1}(2q0+1)n
is obtained from ϕ(x) by modifying ≤ q0 coordinates, then h(cid:48)(z) = h(cid:48)(ϕ(x)) and (iii) h(cid:48)
depends on (2q0 + 1)K variables. It is not hard to check that the following function satisﬁes
these requirements:

h(cid:48)(z) = h (MAJ(z1,1, . . . , z1,2q0+1), . . . , MAJ(zn,1, . . . , zn,2q0+1))

2
We remark that by taking q0 = log1−0(n) for some 0 > 0, and using a similar argument, it

can be shown that an eﬃcient algorithm for learning poly-sized Juntas with(cid:0)log1−0(n)(cid:1)-local

queries would imply a PAC algorithm for poly-sized Juntas that runs in time nO(log1−0 (n)).

Corollary 5.5 For every constant q0 > 0, q0-local queries cannot help to learn poly-sized
decision tress.
Proof As with Juntas, consider the mapping ϕ : {−1, 1}n → {−1, 1}(2q0+1)n that replicates
each coordinate 2q0 + 1 times. To establish the corollary, we show that ϕ is a q0-reduction of
type B from poly-sized decision trees to poly-sized decision trees. Indeed, let h : {−1, 1}n →
{0, 1} be a function that is realized by a decision tree T . It is enough to show that there
is a function h(cid:48) on the variables {zi,j}i∈[n],j∈[2q0+1] that satisﬁes (i) h = h(cid:48) ◦ ϕ, (ii) for every
x ∈ X , if z ∈ {−1, 1}(2q0+1)n is obtained from ϕ(x) by modifying ≤ q0 coordinates, then
h(cid:48)(z) = h(cid:48)(ϕ(x)) and (iii) h(cid:48) can be realized by a tree of size |T|2q0+1. As we explain, this
will hold for the following function:

h(cid:48)(z) = MAJ (h(z1,1, . . . , zn,1), . . . , h(z1,2q0+1, . . . , zn,2q0+1))

It is not hard to check that h(cid:48) satisﬁes (i) and (ii). As for (iii), consider the following tree T (cid:48).
First replicate T on the variables z1,1, . . . , zn,1. Then, on the obtained tree, replace each leaf
by a replica of T on the variables z1,2, . . . , zn,2. Then, again, replace each leaf by a replica
of T on the variables z1,3, . . . , zn,3. Continues doing so, until the leafs are replaced by a
replica on the variables z1,2q0+1, . . . , zn,2q0+1. Now, each leaf in the resulted tree corresponds
to (2q0 + 1) root-to-leaf paths in the original tree T . The label of such leaf will be the
2
majority of the labels of these paths.

11

As with Juntas, we remark that by taking q0 = log1−0(n) for some 0 > 0, and using a
similar argument, it can be shown that an eﬃcient algorithm for learning poly-sized trees

with(cid:0)log1−0(n)(cid:1)-local queries would imply a PAC algorithm for poly-sized trees that runs

in time nO(log1−0 (n)).

In the sequel, we say that a function h : {−1, 1}n → {0, 1} is realized by a poly-sized
polynomial, if it is the function induced by a polynomial with polynomially many non-
zero coeﬃcient and degree O (log(n)). A similar convention applies to the term poly-sized
polynomial threshold function. We remark that the following corollary and its proof remain
correct also if in our deﬁnition, we replace the number of coeﬃcients with the (cid:96)1 norm of the
coeﬃcients vector.

Corollary 5.6 For every constant q0 > 0, q0-local queries cannot help to learn poly-sized
polynomials, as well as poly-sized polynomial threshold functions.

Proof We will prove the corollary for polynomials. The proof for polynomial threshold
functions is similar. Consider the mapping ϕ : {−1, 1}n → {−1, 1}(2q0+1)n that replicates
each coordinate 2q0 + 1 times. To establish the corollary, we show that ϕ is a q0-reduction
of type B from poly-sized polynomials to poly-sized polynomials. Indeed, let h : {−1, 1}n →
{0, 1} be a poly-sized polynomial. It is enough to show that there is a poly-sized polynomial
h(cid:48) on the variables {zi,j}i∈[n],j∈[2q0+1] that satisﬁes (i) h = h(cid:48) ◦ ϕ, and (ii) for every x ∈ X ,
if z ∈ {−1, 1}(2q0+1)n is obtained from ϕ(x) by modifying ≤ q0 coordinates, then h(cid:48)(z) =
h(cid:48)(ϕ(x)). As we explain next, this will hold for the following polynomial:

h(cid:48)(z) = h (MAJ(z1,1, . . . , z1,2q0+1), . . . , MAJ(zn,1, . . . , zn,2q0+1))

It is not hard to check that h(cid:48) satisﬁes (i) and (ii). It remains to show that h(cid:48) is poly-sized.
Indeed, the majority function on 2q0 + 1 coordinates is a polynomial of degree ≤ 2q0 + 1 with
at most 22q0+1 non zero coeﬃcients (this is true for any function on 2q0 + 1 coordinates).
Hence, if we replace each variable in h by a polynomial of the form MAJ(zi,1, . . . , zi,2q0+1),
the degree is multiplied by at most (2q0 + 1), while the number of non zero coeﬃcients is
multiplied by at most 22q0+1.
2
As with Juntas and decision trees, by taking q0 = log1−0(n) for some 0 > 0, and using a
similar argument, it can be shown that an eﬃcient algorithm for learning poly-sized polyno-

mials or polynomial threshold functions with(cid:0)log1−0(n)(cid:1)-local queries would imply a PAC

algorithm for the same problem that runs in time nO(log1−0 (n)).

6 Conclusion and Future Work

We have shown that in the distribution free setting, for many hypothesis classes, local queries
are not useful. As our proofs show, this stems from the fact that learning these classes
without queries can be reduced to a case where local queries are pointless, in the sense that
the answer to them is either always 1, or the label of the closest training example. On the
other hand, the learning problem of DNFs with evident examples circumvents this property.
Indeed, the underlying assumption enforces that local changes can change the label in a

12

non-trivial manner. While this assumption might be intuitive in some cases, it is certainly
very restrictive. Therefore, a natural future research direction is to seek less restrictive
assumptions, that still posses this property.

that(cid:0)log0.99(n)(cid:1)-local queries are unlikely to lead to eﬃcient algorithms. We conjecture that

More concrete direction arising from our work concern classes for which we have shown

for some a > 0 even (na)-local queries won’t lead to eﬃcient distribution free algorithms for
these classes.

References

[1] Dana Angluin. Learning regular sets from queries and counterexamples. Information

and computation, 75(2):87–106, 1987.

[2] Dana Angluin and Michael Kharitonov. When won t membership queries help? Journal

of Computer and System Sciences, 50(2):336–355, 1995.

[3] Dana Angluin and Donna K Slonim. Randomly fallible teachers: Learning monotone

dnf with an incomplete membership oracle. Machine Learning, 14(1):7–26, 1994.

[4] Dana Angluin, M¯arti¸nˇs Krik¸is, Robert H Sloan, and Gy¨orgy Tur´an. Malicious omissions
and errors in answers to membership queries. Machine Learning, 28(2-3):211–255, 1997.

[5] Pranjal Awasthi, Vitaly Feldman, and Varun Kanade. Learning using local membership

queries. arXiv preprint arXiv:1211.0996, 2012.

[6] Galit Bary.

Learning using 1-local membership queries.

arXiv preprint

arXiv:1512.00165, 2015.

[7] Eric B Baum. Neural net algorithms that learn in polynomial time from examples and

queries. Neural Networks, IEEE Transactions on, 2(1):5–19, 1991.

[8] Eric B Baum and Kenneth Lang. Query learning can work poorly when a human oracle

is used. In International Joint Conference on Neural Networks, volume 8, 1992.

[9] Laurence Bisht, Nader H Bshouty, and Lawrance Khoury. Learning with errors in
answers to membership queries. Journal of Computer and System Sciences, 74(1):2–15,
2008.

[10] Avrim Blum and Steven Rudich. Fast learning of k-term dnf formulas with queries.
In Proceedings of the twenty-fourth annual ACM symposium on Theory of computing,
pages 382–389. ACM, 1992.

[11] Avrim Blum, Prasad Chalasani, Sally A Goldman, and Donna K Slonim. Learning
In Proceedings of the eighth annual conference on

with unreliable boundary queries.
Computational learning theory, pages 98–107. ACM, 1995.

[12] Nader H Bshouty. Exact learning boolean functions via the monotone theory. Informa-

tion and Computation, 123(1):146–153, 1995.

13

[13] Amit Daniely and Shai Shalev-Shwatz. Complexity theoretic limitations on learning

dnf’s. arXiv preprint arXiv:1404.3378, 2014.

[14] Vitaly Feldman. On the power of membership queries in agnostic learning. The Journal

of Machine Learning Research, 10:163–182, 2009.

[15] Yoav Freund. Boosting a weak learning algorithm by majority. Information and com-

putation, 121(2):256–285, 1995.

[16] Jeﬀrey Jackson. An eﬃcient membership-query algorithm for learning dnf with respect
to the uniform distribution. In Foundations of Computer Science, 1994 Proceedings.,
35th Annual Symposium on, pages 42–53. IEEE, 1994.

[17] Michael Kearns and Leslie Valiant. Cryptographic limitations on learning boolean for-

mulae and ﬁnite automata. Journal of the ACM (JACM), 41(1):67–95, 1994.

[18] Eyal Kushilevitz and Yishay Mansour. Learning decision trees using the fourier spec-

trum. SIAM Journal on Computing, 22(6):1331–1348, 1993.

[19] Michael Sipser. Introduction to the Theory of Computation, volume 2.

[20] Robert H Sloan and Gy¨orgy Tur´an. Learning with queries but incomplete information.
In Proceedings of the seventh annual conference on Computational learning theory, pages
237–245. ACM, 1994.

[21] Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):

1134–1142, 1984.

14

