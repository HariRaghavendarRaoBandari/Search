Learning Word Segmentation Representations to Improve

Named Entity Recognition for Chinese Social Media

Nanyun Peng and Mark Dredze

Human Language Technology Center of Excellence

Center for Language and Speech Processing

Johns Hopkins University, Baltimore, MD, 21218
npeng1@jhu.edu, mdredze@cs.jhu.edu

6
1
0
2

 
r
a

M
2

 

 
 
]
L
C
.
s
c
[
 
 

1
v
6
8
7
0
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

Named entity recognition, and other in-
formation extraction tasks, frequently use
linguistic features such as part of speech
tags or chunkings. For languages where
word boundaries are not readily identiﬁed
in text, word segmentation is a key ﬁrst
step to generating features for an NER
system. While using word boundary tags
as features are helpful,
the signals that
aid in identifying these boundaries may
provide richer information for an NER
system. New state-of-the-art word seg-
mentation systems use neural models to
learn representations for predicting word
boundaries. We show that these same rep-
resentations, jointly trained with an NER
system, yield signiﬁcant improvements in
NER for Chinese social media. In our ex-
periments, jointly training NER and word
segmentation with an LSTM-CRF model
yields nearly 5% absolute improvement
over previously published results.

Introduction

1
Entity mention detection, and more speciﬁcally
named entity recognition (NER) (Collins and
Singer, 1999; McCallum and Li, 2003; Nadeau
and Sekine, 2007; Jin and Chen, 2008; He et al.,
2012), has become a popular task for social media
analysis (Finin et al., 2010; Liu et al., 2011; Ritter
et al., 2011; Fromreide et al., 2014; Li et al., 2012;
Liu et al., 2012a). Many downstream applications
that use social media, such as relation extraction
(Bunescu and Mooney, 2005) and entity linking
(Dredze et al., 2010; Ratinov et al., 2011), rely
on ﬁrst identifying mentions of entities. Not sur-
prisingly, accuracy of NER systems in social me-
dia trails state-of-the-art systems for news text and

other formal domains. While this gap is shrinking
in English (Ritter et al., 2011; Cherry and Guo,
2015), it remains large in other languages, such as
Chinese (Peng and Dredze, 2015; Fu et al., 2015).
One reason for this gap is the lack of robust
up-stream NLP systems that provide useful fea-
tures for NER, such as part-of-speech tagging or
chunking. Ritter et al. (2011) annotated Twitter
data for these systems to improve a Twitter NER
tagger, however, these systems do not exist for so-
cial media in most languages. Another approach
has been that of Cherry and Guo (2015) and Peng
and Dredze (2015), who relied on training unsu-
pervised lexical embeddings in place of these up-
stream systems and achieved state-of-the-art re-
sults for English and Chinese social media, respec-
tively. The same approach was also found helpful
for NER in the news domain (Collobert and We-
ston, 2008; Turian et al., 2010; Passos et al., 2014)
In Asian languages like Chinese, Japanese and
Korean, word segmentation is a critical ﬁrst step
(Gao et al., 2005; Zhang et al., 2006; Mao et al.,
2008). Peng and Dredze (2015) showed the value
of word segmentation to Chinese NER in social
media by using character positional embeddings.
In this paper, we investigate better ways to in-
corporate word boundary information into an NER
system for Chinese social media. We combine
state-of-the-art Chinese word segmentation (Chen
et al., 2015) with the best Chinese social media
NER system (Peng and Dredze, 2015). Since both
systems used learned representations, we propose
an integrated model that allows for joint training of
learned representations, providing more informa-
tion to the NER system about what is learned from
word segmentation, as compared to features based
on segmentation output. Our integrated model
achieves nearly a 5% absolute improvement over
the previous best results on both NER and nominal
mention for Chinese social media.

2 Model
We propose a model that integrates the best Chi-
nese word segmentation system (Chen et al., 2015)
using an LSTM neural model that learns represen-
tations, with the best NER model for Chinese so-
cial media (Peng and Dredze, 2015), that supports
training neural representations by a log-bilinear
CRF. We begin with a brief review of each system.

2.1 LSTM for Word Segmentation
Chen et al. (2015) proposed a single layer, left
to right LSTM for Chinese word segmentation.
An LSTM is a recurrent neural network (RNN)
which uses a series of gates (input, forget and out-
put gate) to control how memory is propagated in
the hidden states of the model. For the Chinese
word segmentation task, each Chinese character
is initialized as a d dimensional vector, which the
LSTM will modify during its training. Besides,
For each input character, the model learns a hid-
den vector h. These vectors are then used with a
biased-linear transformation to predict the output
labels, which in this case are Begin, Inside, End,
and Singleton. A prediction for position t is given
as:

y(t) = Woh(t) + bo

(1)
where Wo are the transformation parameters, bo
the bias parameter, and h(t) the hidden state at po-
sition t. To model the tag dependencies, they intro-
duced the transition score Aij to measure the prob-
ability of jumping from tag i ∈ T to tag j ∈ T .

We used the same model as Chen et al. (2015)
trained on the same data (segmented Chinese
news article) We employed a different training
objective. Chen et al. (2015) employed a max-
margin, however, while they found this objective
yielded better results, we observed that maximum-
likelihood yielded better segmentation results in
our experiments1. Additionally, we sought to
integrate their model with a log-bilinear CRF,
which uses a maximum-likelihood training objec-
tive. For consistency, we trained the LSTM with
a maximum-likelihood training objective as well.

1We were unable to match the results reported by Chen
et al. (2015). Our implementation achieved nearly identical
results on development data (as inferred from their published
ﬁgure), but lagged in test accuracy by 2.4%. Conversations
with the authors strongly points to differences in data pre-
processing, which can have a big impact on system accuracy.
However, we were unable to conduct the same data prepro-
cessing due to some unpublished resources, and we note that
better pre-processing for segmentation is not a key point for
this paper.

The maximum-likelihood CRF objective function
for predicting segmentations is:

(cid:88)

(cid:2) log

k

Ls(ys; xs, Θ) =

1
K

(cid:88)

(cid:16)

Z(xs)k

1

(cid:17)(cid:3)

(2)

+

Ts(yk

i−1, yk

i ) + s(yk

i ; xk

s , Λs)

i

Ts(yk

i−1, yk

Example pairs (ys, xs) are word segmented
sentences, k indexes examples, and i indexes
positions in examples.
i ) are stan-
dard transition probabilities learned by the CRF2.
The LSTM parameters Λs are used to produce
s , Λs), the emission probability of the la-
s(yk
bel at position i for input sentence k, which is ob-
tained by taking a soft-max of (1). We use a ﬁrst-
order Markov model.

i ; xk

2.2 Log-bilinear CRF for NER
Peng and Dredze (2015) proposed a log-bilinear
model for Chinese social media NER. They used
standard NER features along with additional fea-
tures based on lexical embeddings. By ﬁne-tuning
these embeddings, and jointly training them with
a word2vec (Mikolov et al., 2013) objective, the
resulting model is log-bilinear.

Typical lexical embeddings provide a single
embedding vector for each word type. However,
Chinese text is not word segmented, making the
mapping between input to embedding vector un-
clear. Peng and Dredze (2015) explored several
types of representations for Chinese,
including
pre-segmenting the input to obtain words, using
character embeddings, and a combined approach
that learned embeddings for characters based on
their position in the word. This ﬁnal representa-
tion yielded the largest improvements.

We use the same idea but augmented it with
LSTM learned representations, and we enables in-
teraction between the CRF and the LSTM param-
eters. More details are described in (§2.3).

2.3 Using Segmentation Representations to

Improve NER

The improvements provided by character position
embeddings demonstrated by Peng and Dredze
(2015) indicates that word segmentation informa-
tion can be helpful for NER. Embeddings aside, a
simple way to include this information in an NER
system would be to add features to the CRF using
the predicted segmentation labels as features.

2The same functionality as Aij in Chen et al. (2015)

model.

However,

these features alone may overlook
useful information from the segmentation model.
Previous work showed that jointly learning differ-
ent stages of the NLP pipeline helped for Chinese
(Liu et al., 2012b; Zheng et al., 2013). We thus
seek approaches for deeper interaction between
word segmentation and NER. The LSTM word
segmentation learns two different types of repre-
sentations: 1) embeddings for each characters and
2) hidden vectors for predicting segmentation tags.
Compressing these rich representations down to a
small feature set imposes a bottleneck on using
richer segmentation related information for NER.
We experiment with including both of these infor-
mation sources directly into the NER model.

Since the log-bilinear CRF already supports
joint training of lexical embeddings, we can also
incorporate the LSTM output hidden vectors as
dynamic features using a joint objective function.
First, we augment the CRF with the LSTM pa-

rameters as follows:

(cid:88)

(cid:2) log
n, ew, hw)(cid:3),

Z(xn)k

1

k
n, xk
ΛjFj(yk

Ln(yn; xn, Θ) =

1
K

(cid:88)

j

+

(3)

where k indexes instances, j positions, and

n(cid:88)

Fj(yk, xk, ew, hw) =

fj(yk

i−1, yk

i , xk, ew, hw, i)

i=1

represents the feature functions. These features
now depend on the embeddings learned by the
LSTM (ew) and the LSTM’s output hidden vectors
(hw). Note that by including hw alone we create a
dependence on all LSTM parameters on which the
hidden states depend (i.e.
the weight matrixes).
We experiment with including input embeddings
and output hidden vectors independently, as well
as both parameters together.
Joint Training In our integrated model,
the
LSTM parameters are used for both predicting
word segmentations and NER. Therefore, we con-
sider a joint training scheme. We maximize a
(weighted) joint objective:

Ljoint(Θ) = λLs(ys; xs, Θ) + Ln(yn; xn, Θ)

(4)

where λ trades off between better segmentations
or better NER, and Θ includes all parameters used
in both models. Since we are interested in improv-
ing NER we consider settings with λ < 1.

3 Parameter Estimation

We train all of our models using stochastic gradi-
ent descent (SGD.) We train for up to 30 epochs,
stopping when NER results converged on dev data.
We use a separate learning rate for each part of
the joint objective, with a schedule that decays the
learning rate by half if dev results do not improve
after 5 consecutive epochs. Dropout is introduced
in the input layer of LSTM following Chen et al.
(2015). We optimize two hyper-parameters using
held out dev data: the joint coefﬁcient λ in the in-
terval [0.5, 1] and the dropout rate in the interval
[0, 0.5]. All other hyper-parameters were set to the
values given by Chen et al. (2015) for the LSTM
and Peng and Dredze (2015) for the CRF.

We train the joint model using an alternating op-
timization strategy. Since the segmentation dataset
is signiﬁcantly larger than the NER dataset, we
subsample the former at each iteration to be the
same size as the NER training data, with different
subsamples in each iteration. We found subsam-
pling critical and it signiﬁcantly reduced training
time and allowed us to better explore the hyper-
parameter space.

character-positional

We initialized LSTM input embeddings with
pre-trained
embeddings
trained on 112,971,734 Weibo messages to ini-
tialize the input embeddings for LSTM. We used
word2vec (Mikolov et al., 2013) with the same
parameter settings as Peng and Dredze (2015) to
pre-train the embeddings.

4 Experiments and Analysis
4.1 Datasets
We use the same training, development and test
splits as Chen et al. (2015) for word segmentation
and Peng and Dredze (2015) for NER.

Word Segmentation The segmentation data is
taken from the SIGHAN 2005 shared task. We
used the PKU portion, which includes 43,963
word sentences as training and 4,278 sentences as
test. We did not apply any special preprocessing.

NER This dataset contains 1,890 Sina Weibo
messages annotated with four entity types (per-
son, organization, location and geo-political en-
tity), including named and nominal mentions. We
note that the word segmentation dataset is signiﬁ-
cantly larger than the NER data, which motivates
our subsampling during training (§3).

Method

1 CRF with baseline features
2 + Segment Features
3 P & D best NER model
4 + Segment Features
5 P & D w/ Char Embeddings
6 + Segment Features
7 Pipeline Seg. Repr. + NER
8 Jointly LSTM w/o feat.
9 Jointly Train Char. Emb.
10 Jointly Train LSTM Hidden
11 Jointly Train LSTM + Emb.

Named Entity

Dev

Precision Recall F1
25.43 35.77
27.75 38.40
35.84 44.13
42.20 44.65
32.95 42.22
40.46 45.31
38.14 48.00
35.26 44.20
35.26 45.52
34.68 44.44
38.73 46.85

60.27
62.34
57.41
47.40
58.76
51.47
64.71
59.22
64.21
61.86
5929

Test

Precision Recall F1
25.77 35.59
27.84 37.63
35.57 44.09
38.66 42.86
34.02 42.86
37.11 43.50
36.08 46.20
35.57 44.66
37.11 46.75
38.66 47.92
39.18 48.41

57.47
58.06
57.98
48.08
57.89
52.55
64.22
60.00
63.16
63.03
63.33

Nominal Mention

Dev

Precision Recall F1
32.56 44.85
38.87 46.71
36.88 48.90
36.54 49.44
35.55 46.42
40.86 50.31
39.87 50.63
39.53 47.70
37.87 50.00
39.53 49.79
43.19 50.78

72.06
58.50
72.55
76.38
66.88
65.43
69.36
60.10
73.55
67.23
61.61

Test

Precision Recall F1
23.55 33.80
26.77 34.23
29.45 40.38
26.77 37.64
29.35 38.32
32.58 40.64
33.55 42.11
31.94 40.91
31.61 42.61
33.87 43.30
37.42 45.67

59.84
47.43
63.84
63.36
55.15
54.01
56.52
56.90
65.33
60.00
58.59

Table 1: NER results for named and nominal mentions on dev and test data.

4.2 Results and Analysis
Table 1 shows results for NER in terms of preci-
sion, recall and F1 for named (left) and nominal
(right) mentions on both dev and test sets. The
hyper-parameters are tuned on dev data and then
applied on test. We now explain the results.

We begin by establishing a CRF baseline (#1)
and show that adding segmentation features helps
(#2). However, adding those features to the full
model (with embeddings) in Peng and Dredze
(2015) (#3) did not improve results (#4). This is
probably because the character-positional embed-
dings already carry segmentation information. Re-
placing the character-positional embeddings with
character embeddings (#5) gets worse results but
beneﬁts from adding segmentation features (#6).
This demonstrates both that word segmentation
helps and that character-positional embeddings ef-
fectively convey word boundary information.

We now consider our model of jointly train
the character embeddings (#9), the LSTM hid-
den vectors (#10) and both (#11). They all im-
prove over the best published results (#3). Jointly
train the LSTM hidden vectors (#10) does better
than the embeddings (#9) probably because they
carry richer word boundary information. Using
both representations achieves the single best result
(#11): 4.3% improvement on named and 5.3% im-
provement on nominal mentions.

Finally, we examine how much of the gain is
from joint training versus from pre-trained seg-
mentation representations. We ﬁrst train an LSTM
for word segmentation, then use the trained em-
beddings and hidden vectors as inputs to the log-
bilinear CRF model for NER, and ﬁne tunes these
representations. This (#7) improved test F1 by
2%, about half of the overall improvements from
joint training.

5 Discussion

Different interpretations of our results suggest di-
rections for future work.

First, we can view our method as multi-task
learning (Caruana, 1997; Collobert and Weston,
2008), where we are using the same learned rep-
resentations (embeddings and hidden vectors) for
two tasks: segmentation and NER, which use dif-
ferent prediction and decoding layers. Result #8
shows the result of excluding the additional NER
features and just sharing jointly trained LSTM.
While this does not perform as well as adding the
additional NER features (#11), it is impressive that
this simple architecture achieved similar F1 as the
best results in Peng and Dredze (2015). While we
may expect both NER and word segmentation re-
sults to improve, we found the segmentation per-
formances of the best joint model tuned for NER
lose to the stand alone word segmentation model
(F1 90.7% v.s. 93.3%). This lies in the fact that
tuning λ means choosing between the two tasks;
no single setting achieved improvements for both,
which suggests further work is needed on better
model structures and learning.

Second, our segmentation data is from the news
domain, whereas the NER data is from social me-
dia. While it is well known that segmentation sys-
tems trained on news do worse on social media
(Duan et al., 2012), we still show large improve-
ments in applying our model to these different do-
mains. It may be that we are able to obtain better
results in the case of domain mismatch because we
integrate the representations of the LSTM model
directly into our CRF, as opposed to only using the
predictions of the LSTM segmentation model. We
plan to consider expanding our model to explicitly
include domain adaptation mechanisms (Yang and
Eisenstein, 2015).

References
[Bunescu and Mooney2005] Razvan C Bunescu and
Raymond J Mooney. 2005. A shortest path de-
In Em-
pendency kernel for relation extraction.
pirical Methods in Natural Language Processing
(EMNLP), pages 724–731. Association for Compu-
tational Linguistics.

[Caruana1997] Rich Caruana. 1997. Multitask learn-

ing. Machine learning, 28(1):41–75.

[Chen et al.2015] Xinchi Chen, Xipeng Qiu, Chenxi
Zhu, Pengfei Liu, and Xuanjing Huang.
2015.
Long short-term memory neural networks for chi-
nese word segmentation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP).

[Cherry and Guo2015] Colin Cherry and Hongyu Guo.
2015. The unreasonable effectiveness of word rep-
resentations for twitter named entity recognition. In
North America Chapter of Association for Compu-
tational Linguistics (NAACL). Association for Com-
putational Linguistics.

[Collins and Singer1999] Michael Collins and Yoram
Singer. 1999. Unsupervised models for named en-
In Empirical Methods in Natu-
tity classiﬁcation.
ral Language Processing (EMNLP), pages 100–110.
Citeseer.

[Collobert and Weston2008] Ronan Collobert and Ja-
son Weston. 2008. A uniﬁed architecture for natu-
ral language processing: Deep neural networks with
multitask learning. In International Conference on
Machine Learning (ICML), pages 160–167. ACM.

[Dredze et al.2010] Mark Dredze, Paul McNamee,
Delip Rao, Adam Gerber, and Tim Finin. 2010. En-
tity disambiguation for knowledge base population.
In Conference on Computational Linguistics (Col-
ing).

[Duan et al.2012] Huiming Duan, Zhifang Sui, Ye Tian,
and Wenjie Li. 2012. The cips-sighan clp 2012 chi-
neseword segmentation onmicroblog corpora bake-
off. In Second CIPS-SIGHAN Joint Conference on
Chinese Language Processing, pages 35–40, Tian-
jin, China, December. Association for Computa-
tional Linguistics.

[Finin et al.2010] Tim Finin, William Murnane, Anand
Karandikar, Nicholas Keller, Justin Martineau, and
Mark Dredze. 2010. Annotating named entities in
twitter data with crowdsourcing. In NAACL Work-
shop on Creating Speech and Language Data With
Mechanical Turk.

[Fromreide et al.2014] Hege Fromreide, Dirk Hovy,
and Anders Søgaard. 2014. Crowdsourcing and an-
notating NER for Twitter# drift. In LREC.

[Fu et al.2015] JinLan Fu, Jie Qiu, Yunlong Guo, and
2015. Entity linking and name disam-
In

Li Li.
biguation using SVM in chinese micro-blogs.

11th International Conference on Natural Computa-
tion, ICNC 2015, Zhangjiajie, China, August 15-17,
2015, pages 468–472.

[Gao et al.2005] Jianfeng Gao, Mu Li, Andi Wu, and
Chang-Ning Huang. 2005. Chinese word segmen-
tation and named entity recognition: A pragmatic
approach. Comput. Linguist., 31(4):531–574, De-
cember.

[He et al.2012] Zhengyan He, Houfeng Wang, and Su-
jian Li. 2012. The task 2 of cips-sighan 2012 named
entity recognition and disambiguation in chinese
bakeoff. In Second CIPS-SIGHAN Joint Conference
on Chinese Language Processing, pages 108–114,
Tianjin, China, December. Association for Compu-
tational Linguistics.

[Jin and Chen2008] Guangjin Jin and Xiao Chen. 2008.
The fourth international chinese language process-
ing bakeoff: Chinese word segmentation, named en-
In Sixth
tity recognition and chinese pos tagging.
SIGHAN Workshop on Chinese Language Process-
ing, page 69. Citeseer.

[Li et al.2012] Chenliang Li, Jianshu Weng, Qi He,
Yuxia Yao, Anwitaman Datta, Aixin Sun, and Bu-
Sung Lee. 2012. Twiner: Named entity recogni-
tion in targeted twitter stream. In SIGIR Conference
on Research and Development in Information Re-
trieval, SIGIR ’12, pages 721–730, New York, NY,
USA. ACM.

[Liu et al.2011] Xiaohua Liu, Shaodian Zhang, Furu
Wei, and Ming Zhou. 2011. Recognizing named
entities in tweets. In Association for Computational
Linguistics (ACL), pages 359–367. Association for
Computational Linguistics.

[Liu et al.2012a] Xiaohua Liu, Ming Zhou, Furu Wei,
Zhongyang Fu, and Xiangyang Zhou. 2012a. Joint
inference of named entity recognition and normal-
In Association for Computa-
ization for tweets.
tional Linguistics (ACL), ACL ’12, pages 526–535,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

[Liu et al.2012b] Xiaohua Liu, Ming Zhou, Furu Wei,
Zhongyang Fu, and Xiangyang Zhou. 2012b. Joint
inference of named entity recognition and normal-
In Proceedings of the 50th An-
ization for tweets.
nual Meeting of the Association for Computational
Linguistics: Long Papers - Volume 1, ACL ’12,
pages 526–535, Stroudsburg, PA, USA. Association
for Computational Linguistics.

[Mao et al.2008] Xinnian Mao, Yuan Dong, Saike He,
Sencheng Bao, and Haila Wang. 2008. Chinese
word segmentation and named entity recognition
In IJCNLP,
based on conditional random ﬁelds.
pages 90–93.

[McCallum and Li2003] Andrew McCallum and Wei
Li. 2003. Early results for named entity recogni-
tion with conditional random ﬁelds, feature induc-
tion and web-enhanced lexicons. In North America

Chapter of Association for Computational Linguis-
tics (NAACL).

[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases
In Neural Information
and their compositionality.
Processing Systems (NIPS), pages 3111–3119.

[Nadeau and Sekine2007] David Nadeau and Satoshi
Sekine. 2007. A survey of named entity recogni-
tion and classiﬁcation. Lingvisticae Investigationes,
30(1):3–26.

[Passos et al.2014] Alexandre Passos, Vineet Kumar,
and Andrew McCallum. 2014. Lexicon infused
phrase embeddings for named entity resolution.
CoRR, abs/1404.5367.

[Peng and Dredze2015] Nanyun

and Mark
Dredze. 2015. Named entity recognition for chi-
nese social media with jointly trained embeddings.
September.

Peng

[Ratinov et al.2011] Lev Ratinov, Dan Roth, Doug
Downey, and Mike Anderson. 2011. Local and
global algorithms for disambiguation to wikipedia.
In Association for Computational Linguistics (ACL),
pages 1375–1384. Association for Computational
Linguistics.

[Ritter et al.2011] Alan Ritter, Sam Clark, Oren Et-
zioni, et al. 2011. Named entity recognition in
tweets: an experimental study. In Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 1524–1534. Association for Computational
Linguistics.

[Turian et al.2010] Joseph Turian, Lev Ratinov, and
Yoshua Bengio. 2010. Word representations: a sim-
ple and general method for semi-supervised learn-
In Association for Computational Linguistics
ing.
(ACL), pages 384–394. Association for Computa-
tional Linguistics.

[Yang and Eisenstein2015] Yi Yang and Jacob Eisen-
stein. 2015. Unsupervised multi-domain adaptation
with feature embeddings.

[Zhang et al.2006] Suxiang Zhang, Ying Qin, Juan
Wen, and Xiaojie Wang. 2006. Word segmentation
and named entity recognition for sighan bakeoff3.
In Fifth SIGHAN Workshop on Chinese Language
Processing, pages 158–161, Sydney, Australia, July.
Association for Computational Linguistics.

[Zheng et al.2013] Xiaoqing Zheng, Hanyang Chen,
and Tianyu Xu. 2013. Deep learning for Chinese
In Proceed-
word segmentation and POS tagging.
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 647–657,
Seattle, Washington, USA, October. Association for
Computational Linguistics.

