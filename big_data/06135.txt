6
1
0
2

 
r
a

 

M
9
1

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
5
3
1
6
0

.

3
0
6
1
:
v
i
X
r
a

Cauchy diﬀerence priors for edge-preserving

Bayesian inversion with an application to X-ray

tomography

Markku Markkanen1, Lassi Roininen2, Janne M J Huttunen3

and Sari Lasanen4

1 Eigenor Corporation

2 University of Warwick, Department of Statistics

3Nokia Technologies, and,

University of Eastern Finland, Department of Applied Physics

4 University of Oulu, Department of Mathematical Sciences

March 22, 2016

Abstract

We study Cauchy-distributed diﬀerence priors for edge-preserving Bayesian

statistical inverse problems. On the contrary to the well-known total vari-
ation priors, one-dimensional Cauchy priors are non-Gaussian priors also
in the discretization limit. Cauchy priors have independent and identically
distributed increments. One-dimensional Cauchy and Gaussian random
walks are special cases of L´evy α-stable random walks with α = 1 and
α = 2, respectively. Both random walks can be written in closed-form,
and as priors, they provide smoothing and edge-preserving properties. We
brieﬂy discuss also continuous and discrete L´evy α-stable random walks,
and generalize the methodology to two-dimensional priors.

We apply the developed algorithm to one-dimensional deconvolution
and two-dimensional X-ray tomography problems. We compute condi-
tional mean estimates with single-component Metropolis-Hastings and
maximum a posteriori estimates with Gauss-Newton-type optimization
method. We compare the proposed tomography reconstruction method
to ﬁltered back-projection estimate and conditional mean estimates with
Gaussian and total variation priors.

Keywords: Bayesian statistical inverse problems, Cauchy distribution, L´evy α-
stable random walk, a priori information, X-ray tomography

1

1 Introduction

In Bayesian statistical inverse problems, the objective is to estimate posterior
distribution of an unknown object X , given noise-perturbed indirect measure-
ments. In this paper, we consider only linear inverse problems. The measure-
ments are formally given with an operator equation

m = AX + e,

(1)

where m ∈ Rn is a known measurement vector, and A is a known linear operator
from some function space to a ﬁnite-dimensional vector space. We assume a
Gaussian-distributed measurement noise e ∼ N (0, Σ), where Σ ∈ Rn×n is a
known covariance matrix. We furthermore assume that e is independent of X .
For computational purposes, we discretize the continuous observation model

in Equation (1) and denote the discretized observation model as

m = AX + e,

(2)

where A ∈ Rn×k, and X ∈ Rk. A posteriori probability distribution is the
solution of a ﬁnite-dimensional Bayesian statistical inverse problem. Via the
Bayes’ formula, we give the posterior as an unnormalized probability density

D(X|m) =

D(X)D(m|X)

D(m)

∝ D(X)D(m|X).

(3)

D(X) is the a priori probability density, which reﬂects our information of the
unknown before any actual measurement is done. Prior is, in practice, the
only tunable parameter in the estimation algorithm. D(m|X) is the likelihood
density, which we construct from the discretized observation model (2). D(m)
is simply a normalization constant, and hence we may drop it and use the
unnormalized density.

In our previous papers (Roininen et al. 2011, 2013 and 2014 [10, 11, 12]),
our main result has been that the prior should be modeled as a continuous-
parameter random ﬁeld, and for practical computations we discretize the prior in
some computationally eﬃcient way. In the previous papers, we have considered
Gaussian Markov random ﬁelds within the framework of Bayesian statistical
inverse problems through, and we modeled the prior covariance via the sparse
Cholesky decomposition of the inverse covariance matrix. In [9], Lindgren et al.
2011 considered spatial interpolation in a similar setting. Such constructions
promote discretization-invariant inversion, which means, in practice, that the
posterior distributions are essentially the same, and hence also point estimators
are same on dense enough meshes. For the considered Gaussian priors, it is
enough to show that the discrete priors converge to continuous priors in the
discretization limit, and more speciﬁcally, we only need to show that the discrete
covariance matrix converges to a continuous covariance in the discretization
limit. This would guarantee the convergence of the posterior distributions, and
hence discretization-invariance, as shown by Lasanen 2012 [4, 5].

2

Gaussian priors are known to smooth the edges in the reconstructions.
Therefore, common alternative is to use total variation (TV) priors. However,
TV prior is not discretization-invariant as shown by Lassas and Siltanen 2004
[7]. This means that when we make the computational mesh denser and denser,
the posterior distributions and estimates are not invariant with respect to the
discretization of the unknown. For low-dimensional TV priors, we may get
edge-preserving inversion, however, high-dimensional TV priors start to loose
their edge-preserving property when the random walk presenting the prior tends
to its continuous limit. From Donsker’s theorem, Lindeberg-L´evy theorem and
Doeblin’s theorem, we see that the problem originates from the distributions
of the random walk. Namely, the jumps have ﬁnite moments, which always
leads to normally distributed limits. When one allows other distributions for
jumps, it may become possible to maintain the edge-preserving properties for
high-dimensional priors.

Lassas, Saksman and Siltanen 2009 [8] proposed to use non-Gaussian Besov
space priors which are constructed on wavelet basis. These can be shown to
promote discretization-invariant inversion, and the method has been e.g. applied
to X-ray tomography by V¨ansk¨a et al. 2009 [14]. In this paper, we shall tackle
the issue of edge-preserving inversion by constructing Cauchy diﬀerence priors
starting from continuous Cauchy random walks. We apply the methodology to
deconvolution and X-ray tomography problems. Our discussion lies mostly on
the numerics and we leave mathematically rigorous discussion on discretization-
invariance to the future papers.

The rest of this paper is organized as follows: In Section 2, we consider
one-dimensional Cauchy priors. We start by considering L´evy α-stable random
walks and consider why Cauchy random walks promote edge-preserving inver-
sion. We then review single-component Metropolis-Hastings algorithm, and use
the developed methodology to a one-dimensional deconvolution problem. In Sec-
tion 3, we consider two-dimensional Cauchy priors and apply the methodology
to a two-dimensional X-ray tomography. We compare the developed algorithm
against conditional mean estimates with Gaussian and TV priors as well as ﬁl-
tered back-projection reconstructions. In Section 4, we conclude the study and
make some notes on future research.

2 One-dimensional Cauchy priors in Bayesian

inversion

In this section, we ﬁrst consider L´evy α-stable random walks, and obtain the
Cauchy and Gaussian random walks as special cases. We study edge-promoting
inversion idea with Cauchy priors via unimodality versus bimodality of the dis-
tributions derived from the Cauchy distributions in a speciﬁc way. We carry
out a comparison with other common priors. Finally, we apply the methodol-
ogy to one-dimensional deconvolution problem by obtaining conditional mean
estimates with single-component Metropolis-Hastings.

3

2.1 Deﬁning random walks

Let {X (t), t ∈ I ⊂ R+} be a stochastic process. We call it a continuous L´evy
α-stable process starting from zero, if X (0) = 0, X has independent increments
and

X (t) − X (s) ∼ Sα(cid:16)(t − s)1/α , β, 0(cid:17)

(4)

for any 0 ≤ s < t < ∞ and for some 0 < α ≤ 2, −1 ≤ β ≤ 1 (for stable
distributions, see e.g. [13]).

We can simply construct stable processes by using independently scattered
measures. A random measure M : Ω → L0(P ) is called independently scattered
if M (A1), . . . , M (An) are independent random variables whenever the sets Ai,
i = 1, . . . , n, n ∈ N, are pairwise disjoint. We require that

M (A) ∼ Sα(cid:16)|A|

1

α , β, 0(cid:17)

with constant skewness β. Then α-stable L´evy motion starting from zero is

X (t) = M (χ([0, t])) ,

(5)

(6)

where χA is the characteristic function of the set A.

For the continuous limit of the L´evy α-stable random walk, we apply inde-
pendently scattered measures. Let us denote the discrete random walk at t = jh
by Xj, where j ∈ Z+ and h > 0 is the discretization step. We obtain a random
walk approximation (See Chapter 3.3. in [13])

Xj − Xj−1 ∼ Sα(cid:16)h

1

α , β, 0(cid:17) .

(7)

It is easy to see that such a random walk approximation converges to the con-
tinuous L´evy α-stable random walk as h → 0 in distribution in the Skorokhod
space of functions that are right-continuous and have left limits.

Our special interest is the Cauchy random walk. Given Equation (7) and
choosing α = 1, β = 0, we can write the Cauchy random walk increments as a
probability density

D(X) = C

J

Yj=1

(cid:18)

(λj h)2 + (Xj − Xj−1)2(cid:19) ,

λjh

(8)

where λj is the regularizing term. For the sake of simplicity, let us assume
λj = λ to be a constant. C is a normalization constant. We can apply this
probability density directly as an a priori probability density in Bayes’ formula
in Equation (3).

In Figure 1, we have plotted realizations of Cauchy and Gaussian random
walks. The latter is the L´evy α-stable random walk with α = 2. The Cauchy
random walk realizations have either small perturbations or big jumps. Hence,
intuitively it feels plausible that using Cauchy distributions for diﬀerences lead
to edge-preserving inversion. Gaussian random walk realizations do not prefer
jumps, but continuous paths, hence the tendency for smoothing in Bayesian
inversion.

4

4

2

0

-2

-4

-6

-8

0

0.2

0.4

0.6

0.8

1

5

4

3

2

1

0

-1

-2

-3

0

0.2

0.4

0.6

0.8

1

(a) Cauchy random walk

(b) Gaussian random walk

Figure 1: Realizations of Cauchy and Gaussian random walks.

2.2 Edge-preserving inversion

To our best knowledge, for diﬀerence priors there has not been any simple cri-
teria which would indicate, whether the prior favors edge-preserving solutions
instead of smooth ones. The intuitive idea is to construct a prior, which pro-
motes discontinuities, i.e. jumps. Hence, via a negation, we may say that in
edge-preserving inversion, the priors do not promote smooth properties.

Here, we will consider a one-dimensional case, as the situation becomes more
complicated in higher dimensions, and look at three consecutive coordinates
Xj−1, Xj, Xj+1 of the random vector X. We could then say that X prefers
smoothness at point j, if Xj is (most probably) in the middle of Xj−1 and
Xj+1, and that there is edge at point j if Xj is (most probably) either at
Xj−1 or at Xj+1. The above mentioned idea means that we are interested in
the conditional prior distribution of Xj on the condition that Xj−1 and Xj+1
are known. Without loss of generality we can then assume that Xj−1 = −a
and Xj+1 = a. The Cauchy diﬀerence prior for Xj can then be written as a
probability density

D(Xj ) ∝

1

1 + (Xj − a)2

1

1 + (Xj + a)2 .

(9)

This is simply a product of two Cauchy probability density functions, one from
each neighbor of Xj. In order to see the properties of these functions, consider
the second derivative of the probability density function with respect to Xj at
zero:

D′′(0) < 0, when, |a| < 1 : maximum at 0, unimodal
D′′(0) = 0, when, |a| = 1 : as ﬂat as possible at 0
D′′(0) > 0, when, |a| > 1 : minimum at 0, bimodal.

(10)

In Figure 2, we have plotted these probability density functions. With |a| < 1
we have a unimodal function, hence regularization promotes smooth solution

5

around Xj = 0. With |a| > 1 we have a bimodal function, hence regulariza-
tion promotes solutions around Xj = ±a with smooth variations around these
two values. This means that this kind of prior promotes jumps or small local
oscillations, and this is how we understand edge-preserving prior in this paper.

1

0.8

0.6

0.4

0.2

0

-5

1

0.8

0.6

0.4

0.2

0

-5

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

-5

0

5

0

5

0

5

(a) Unimodal Cauchy a < 1

(b) Flat Cauchy with |a| = 1

(c) Bimodal Cauchy a > 1

1

0.8

0.6

0.4

0.2

0

-5

1

0.8

0.6

0.4

0.2

0

-5

0

5

0

5

(d) Gaussian

(e) Total variation

Figure 2: Upper panel: Cauchy probability density function for X2, given ﬁxed
X1 = −a and X3 = a. Bottom panel: The same case for Gaussian diﬀerence
prior and total variation prior.

We can make similar plots for Gaussian diﬀerence prior and total varia-
tion prior. The Gaussian diﬀerence prior is simply a Gaussian-shaped function
with maximum at Xj = 0. Hence, providing smoothing properties. The to-
tal variation priors is constant for [−a, a] and decays exponentially outside this
region. Hence, total variation neither punishes discontinuities or promotes edge-
preserving inversion as it promotes all values equally between these two points.
This is also the case for the Cauchy prior with |a| = 1, which is, in a sense,
similar to the total variation prior.

2.3 Single-component Metropolis-Hastings

In order to get estimators for Bayesian statistical inverse problems with Cauchy
diﬀerence priors, we need to compute either the maximum a posteriori (MAP)
estimate or conditional mean (CM) estimates. For the MAP estimation, we use
Gauss-Newton type optimization methods. However, our interest lies mostly in
the CM estimate, which is the mean of the posterior density (see Equation (3)).
In higher dimensional problems, the explicit computation of the CM estimates,
the integration over the posterior density, is rarely possible and numerical inte-

6

gration of posterior density typically leads to an infeasible approach. Therefore,
CM estimates are usually computed using Markov Chain Monte Carlo (MCMC)
sampling methods, such as Metropolis-Hastings (MH) algorithm or Gibbs sam-
pler (see e.g. [3, 2]). The idea of the MCMC methods is to generate samples

(cid:8)X (s)(cid:9) from the posterior distribution and approximate the CM estimate using

the ensemble mean.

In this paper, we test the proposed approach using one–dimensional decon-
volution problem and X-ray tomography problem. Both of these problems have
a speciﬁc property that, when evaluating the posterior density, an update of a
single component in the parameter vector is computationally very cheap. The
commonly known algorithms that take advantage of the property are single-
component Gibbs sampler and single-component Metropolis-Hastings (MH) al-
gorithm; see for example [3]. Disadvantage of the Gibbs sampler is that the
explicit form of the conditional distributions is known, but the inverse cumula-
tive is not known, and that would be needed for random number generation.

The idea of the single-component MH sampling is similar to the single com-
ponent Gibbs sample, except the component-wise sampling is carried out using
MH-type sampling instead of the direct sampling from the conditional distribu-
tions. We start with some initial value

X (0) = (cid:16)X (0)

1 , X (0)

n (cid:17)
2 , . . . , X (0)

(11)

and set s = 0. First, we consider one–dimensional conditional distribution of
X1 given data m and all other parameters:

D(cid:16)X1|m, X (0)

2 , X (0)

3 , . . . , X (0)

n (cid:17) = C1D(cid:16)X1, X (0)

2 , X (0)

3 , . . . , X (0)

n |m(cid:17) .

(12)

where C1 is a normalization constant. We apply the MH algorithm to this
distribution: We draw a candidate sample ˜X1 from a proposal distribution
Q1(cid:16)·|X (0)

1 (cid:17), which is accepted with a probability
2 , . . . , X (0)
α1 = min
1,
2 , . . . , X (0)
1 = ˜X1. Otherwise we set X (1)

D(cid:16) ˜X1|m, X (0)
D(cid:16)X (0)
1 |m, X (0)

n (cid:17) q1(cid:16)X (0)
1 | ˜X1(cid:17)
1 (cid:17)
n (cid:17) q1(cid:16) ˜X1|X (0)
1 = X (0)


 .

(13)

If accepted, we set X (1)
the next parameter and consider the distribution

1 . We continue to

1 , X (0)

3 , . . . , X (0)

D(cid:16)X2|m, X (1)
where C2 is a normalization constant. We draw a candidate sample ˜X2 from a
proposal distribution q2(·|X (0)

n (cid:17) = C2D(cid:16)X (1)

2 ) and accept the sample with probability

n |m(cid:17) .

1 , X2, X (0)

3 , . . . , X (0)

(14)

α2 = min
1,

D(cid:16) ˜X2|m, X (1)
D(cid:16)X (0)
2 |m, X (1)

1 , X (0)
1 , X (0)

3 , . . . , X (0)
3 , . . . , X (0)

2 | ˜X2(cid:17)
n (cid:17) q2(cid:16)X (0)
2 (cid:17)
n (cid:17) q2(cid:16) ˜X2|X (0)


 .

(15)

7

Ns times to obtain a set of samples (cid:8)X (s)(cid:9)Ns

The procedure is continued until all parameters have been updated one at a time
and we have X (s+1). We increase s ← s + 1 and repeat the sample generation
s=1. See e.g. [3] for more details.
i (cid:17) =
In this paper the proposal distribution is chosen to be Gaussian: Qi(cid:16)·|X (0)

The algorithm is called also Metropolis-within-Gibbs [1].

i

N (X (0)
, σ2). The variance σ2 is chosen such that 25-50% of samples are ac-
cepted. It is common to ignore a number of samples at the beginning (burn-in
period) due to the fact that it takes a time to converge to the stationary distri-
bution D(X|m). In this paper the burn-in period is chosen to be the half of the
sample size. We leave optimization of the burn-in period to later studies.

2.4 Numerical example: One-dimensional deconvolution

In order to demonstrate the edge-preserving and discretization-invariance prop-
erties, in Figure 3, we have a synthetic deconvolution example with Cauchy
priors on diﬀerent grids. The unknown is assumed to be a piecewise constant
function (upper left panel) and it is convolved with a constant cosine-shaped
convolution kernel.
In the measurements, we have used additive white noise
with 1% noise level.

We have made the reconstructions with number of unknowns being n =

66, 131, 261, 521. In the single-component Metropolis-Hastings, we use 1,000,000
long chain.
In order to save memory, we use only every 10th sample (as 10
consecutive samples are still nearly the same). This is because we want the
samples to be essentially independent. Burn-in period is chosen to be 500,000.
The MCMC chain could be optimized, but as in this paper our goal is to simply
demonstrate the Cauchy prior, we postpone the MCMC optimization studies
for subsequent papers.

The reconstructions on diﬀerent grids clearly provide edge-preserving fea-
tures. The shapes vary between the diﬀerent reconstructions, but this is mostly
due to the MCMC chain, and, characteristically they are all rather similar.
This kind of behavior of estimators cannot be obtained with total variation pri-
ors, as they are not discretization-invariant prior. For the total variation prior
examples, see Lassas and Siltanen 2004 [7].

3 Application to two-dimensional X-ray tomog-

raphy

Let us consider a two-dimensional lattice (hj, h′j ′), where (j, j ′) ∈ I2 ⊂ Z2
and discretization steps to each coordinate directions h, h′ > 0. A Gaussian
diﬀerence prior can be constructed via diﬀerence equations of type

Xj,j ′ − Xj−1,j ′ ∼ N (0, σ2h/h′)
Xj,j ′ − Xj,j ′ −1 ∼ N (0, σ2h′/h).

(16)

8

Original unknown

Noisy measurements

3

2

1

0

3

2

1

0

3

2

1

0

0

0.2

0.4

0.6

0.8

1

n=66

0

0.2

0.4

0.6

0.8

1

n=261

0

0.2

0.4

0.6

0.8

1

n=131

0

0.2

0.4

0.6

0.8

1

n=521

10

5

0

3

2

1

0

3

2

1

0

0

0.2

0.4

0.6

0.8

1

0

0.2

0.4

0.6

0.8

1

Figure 3: Top left panel: The original unknown. Top right panel: Noisy decon-
volved measurement vector. Middle and bottom panels: Reconstructions with
Cauchy priors on diﬀerent grids with n = 66, 131, 261, 521.

9

The term σ2 > 0 is the regularization parameter. The prior is then constructed
through products of conditional normal distributions

Dpr(X) ∝ exp(cid:0)−(X T LT

(17)

1 C −1

1 L1X + X T LT

2 C −1

2 L2X)(cid:1) ,

where L1 and L2 are diﬀerence matrices to two diﬀerent coordinate directions,
and, C1 = σ2h/h′I and C2 = σ2h′/hI, where I is an identity matrix. These
constructions are rather well-known, see for example [6]. We emphasize that
Equation (16) is not explicitely solvable.

Let us consider with a simple example why we scale the variances by h/h′
and h′/h. It is enough to consider only one diﬀerence equation, as the other
one is obtained through same arguments. We consider the grid densiﬁcation
in two steps: 1) We consider denser discretization along the ’regularization
coordinate direction’, and, 2) Denser discretization along the ’other coordinate
direction’. Let Xj,j ′ − Xj−1,j ′ ∼ N (0, σ2). If we densify the lattice along the
regularization coordinate direction j, this would correspond to one-dimensional
Gaussian random walk and hence we need to add the discretization step in the
variance, hence Xj,j ′ − Xj−1,j ′ ∼ N (0, σ2h).

The second coordinate direction is a bit trickier. Let us consider a simpliﬁed

situation as depicted in Figure 4. Thus, we have

X1 =

X11 + X12

2

, and, X2 =

X21 + X22

2

.

Now the increments

X2 − X1 =

(X21 + X22) − (X11 + X12)

2

=

(X21 − X11) + (X22 − X12)

2

(18)

.

(19)

If we choose X2 − X1 ∼ N (0, σ2), then we may choose X21 − X11 ∼ X22 − X12 ∼
N (0, 2σ2). Hence, from this follows the scaling by 1/h′ in Equation (16).

Grid densiﬁcation

X12

X22

X1

X2

X11

X21

Figure 4: Grid densiﬁcation to the second coordinate direction.

Similarly for the Cauchy diﬀerences we may write

Xj,j ′ − Xj−1,j ′ ∼ Cauchy(λh)
Xj,j ′ − Xj,j ′ −1 ∼ Cauchy(λh′).

(20)

10

The result is obtained through similar argumentation as in the Gaussian case.
If we make the grid denser along the regularization direction, this corresponds
again to one-dimensional Cauchy random walk, i.e. we simply scale with dis-
cretization step h.

In the second coordinate direction we take again the same division as in
Figure 4. If X2 − X1 ∼ Cauchy(λ), then X21 − X11 ∼ X22 − X12 ∼ Cauchy(λ)
(note independent linear combination of Cauchy distributed random variable).
Through similar arguments, we can actually see that the scaling of the gen-

eral L´evy α-stable diﬀerence prior is

Xj,j ′ − Xj−1,j ′ ∼ Sα(cid:16)h′(α−1)/α)h1/α, β, 0(cid:17)
Xj,j ′ − Xj,j ′−1 ∼ Sα(cid:16)h(α−1)/α)h′1/α, β, 0(cid:17) .

(21)

This, naturally, implies similar constructions in the higher dimensions.

In Figure 5, we have plotted realizations of two-dimensional Cauchy, Gaus-
sian and TV priors. We have used zero-boundary conditions, but we have
cropped the image in order to demonstrate blocky or smooth structures of the
realizations. A notable feature in the Cauchy prior realization is that the re-
alization does not look isotropic. There seems to be more edges in vertical
and horizontal directions than in other directions. We may ask if this is a
problem? We suppose not, but this should be studied in more detail in subse-
quent papers. If we consider the edge-preserving features of the two-dimensional
Cauchy prior, we can study processes along either of the coordinate directions.
These one-dimensional processes look practically similar to the one-dimensional
Cauchy random walks. For comparison purposes, we have also plotted realiza-
tion of a Gaussian prior obtained in a similar manner, which shows that the
prior promotes smoothness, not jumps. The Gaussian prior is also isotropic.
Realization of a TV prior is not as blocky as the Cauchy prior, but resembles
more the Gaussian prior. However, the TV prior realization is not isotropic,
as we use |Xj,j ′ − Xj−1,j ′ | + |Xj,j ′ − Xj,j ′−1|, inside the exponential function

of the TV prior, and we should use p|Xj,j ′ − Xj−1,j ′ |2 + |Xj,j ′ − Xj,j ′−1|2 for

isotropic TV prior.

3.1 Two-dimensional X-ray tomography

Now we will apply the developed method to X-ray tomography. We take the
Shepp-Logan phantom to be the original unknown. The measurements corre-
spond to two-dimensional fan-beam geometry such that the distance between
the source and the detector to the rotation axis is 4 and 2, respectively. The
width of the detector is 3 and the detector element contains 200 pixels.
In
Figure 6, we have depicted the geometry of the measurements.

We make measurements at 41 diﬀerent angles (from −10◦ to 190◦ with 5◦
steps). We add white measurement noise with approximately 0.1% noise level.
In order to get convergence of the MCMC chain, we use 5 million as the chain
length, and 2.5 million as burn-in period.
In order to save memory, we use

11

(a) Realization of 2D Cauchy prior

(b) Realization of 2D Gaussian prior

(c) Realization of 2D TV prior

Figure 5: Realizations of two-dimensional Cauchy, Gaussian and priors showing
blocky and smooth structures.

12

only every 10th sample in the ensemble mean. The computations are carried
with a PC computational server with dual Xeon E5-2680 CPUs and 128 GB
memory. The core parts of the algorithm are implemented using the MEX/C
interface. Computation time for a reconstruction with 200 × 200 resolution is
approximately 24 hours and, for a 400×400 resolution, the time is approximately
96 hours. This indicates that the algorithm’s computational cost is (nearly)
proportional to the number of unknowns, as expected.

Y

DETECTOR

detectorHalfSize = 1.5

SOURCE

rSource = 4.0

rDetector = 2.0

nPixels = 200

X

Figure 6: Measurement geometry for the synthetic two-dimensional tomography
example.

In Figure 7, we have plotted the original unknown and reconstructions with
ﬁve diﬀerent methods. The ﬁltered back-projection estimate recovers most of
the features, but it has severe artifacts. For the Cauchy prior, we have two
diﬀerent estimates, the MAP and CM estimates obtained with Gauss-Newton
optimization methods and single-component Metropolis-Hastings, respectively.
Both methods capture the features well. According to our experiments, the
beneﬁt of the MAP estimate is lower computational time, while the CM estimate
has less artifacts in the reconstructions. MAP estimates may have e.g. peaks
in the reconstructions, which are not visible in the CM estimates. The CM
estimate with TV prior captures also the features well, but it is not as sharp
as the Cauchy prior estimates. Also, the Cauchy estimate seems to be more
robust, when we decrease the number of measurements or increase the noise.
The estimate with Gaussian prior recovers the main structures, but it has both
bad artifacts and also smooth structures.

In Figures 8 and 9, we have CM estimates on diﬀerent size lattices. The
discretization has been taken into account as described earlier in this section,
and hence the reconstructions look essentially same.

13

(a) Unknown:
phantom

the Shepp-Logan

(b) Filtered back-projection estimate

(c) MAP estimate with Cauchy prior (d) CM estimate with Cauchy prior

(e) CM estimate with TV prior

(f) CM estimate with Gaussian prior

Figure 7: Comparison of diﬀerent estimates on lattice sized 400 × 400 pixels.

14

(a) 200 × 200

(b) 400 × 400

(c) 800 × 800

Figure 8: Conditional mean estimates of the X-ray tomography problem on
three diﬀerent lattices.

150

100

50

0

0

150

100

50

0

0

true
400x400
800x800

true
400x400
800x800

50

45

40

35

30

25

20

0.2

0.4

0.6

0.8

1

0.42

0.44

0.46

0.48

0.5

0.52

0.54

0.56

0.58

(a) Horizontal

(b) Horizontal zoom-in

true
400x400
800x800

true
400x400
800x800

30

25

20

15

10

5

0

0.2

0.4

0.6

0.8

1

0.45

0.5

0.55

0.6

0.65

(c) Vertical

(d) Vertical zoom-in

Figure 9: Cross-sections of the conditional mean estimates from the middle of
the computational domain.

15

4 Conclusion and discussion

We have considered the construction of edge-preserving Bayesian inversion algo-
rithms with Cauchy priors. We drew estimators with single-component Metropolis-
Hastings. When compared to Bayesian inversion with total variation prior, the
methodology proposed promotes edge-preserving Bayesian inversion and gives
good reconstructions for the tomography problem also when we have low num-
ber of measurements, the measurement noise starts to grow or when we have
dense computational mesh.

In the future studies, we should consider the discretization-invariance with
mathematical rigor. Also, an exhaustive numerical study of L´evy α-stable ran-
dom walks should be made. While the Gaussian random walk with α = 2 is
obviously some kind of a high end of stable distributions, the Cauchy random
with α = 1 is in the middle of values 0 < α ≤ 2. For 0 < α < 1 we probably have
even more constant behavior between jumps, than we have for the Cauchy prior,
whereas the cases for 1 ≤ α ≤ 2 between Cauchy and Gaussian should provide
a continuity from smoothing inference to edge-preserving inference. Also, we
could try more general prior models through stochastic partial diﬀerential of
form PX = W, where P is some linear diﬀerential operator, X is unknown and
W L´evy noise. This is similar to e.g. Mat´ern ﬁeld, i.e. a Gaussian Markov ran-
dom ﬁeld construction, except that we would have general L´evy noise instead
of the special case α = 2 Gaussian noise.

Acknowledgments

This work has been funded by Engineering and Physical Sciences Research
Council, United Kingdom (EPSRC Reference: EP/K034154/1 – Enabling Quan-
tiﬁcation of Uncertainty for Large-Scale Inverse Problems), European Research
Council (ERC Advanced Grant 267700 - Inverse Problems) and Academy of Fin-
land (application number 250215, Finnish Programme for Centre of Excellence
in Research 2012-2017).

References

References

[1] Dunlop M M, Iglesias M A and Stuart A M 2016 Hierarchical Bayesian

level set inversion arXiv:1601.03605

[2] Gelman A, Carlin J B, Stern H S and Rubin D B 2003 Bayesian Data

Analysis, 2nd edn. (Chapman & Hall/CRC, Boca Raton, FL)

[3] Gilks W R, Richardson S and Spiegelhalter D J 1996 Markov Chain Monte

Carlo in Practice (Chapmann & Hall)

16

[4] Lasanen S 2012 Non-Gaussian statistical inverse problems. Part I: Posterior

distributions Inverse Problems and Imaging 6 215–266

[5] Lasanen S 2012 Non-Gaussian statistical inverse problems. Part II: Poste-
rior convergence for approximated unknowns Inverse Problems and Imaging
6 267–287

[6] Lasanen S and Roininen L 2005 Statistical inversion with Green’s priors

5th Int. Conf. Inv. Prob. Eng. Proc. 11 (S09):3–32

[7] Lassas M and Siltanen S 2004 Can one use total variation prior for edge

preserving Bayesian inversion? Inverse Problems 20 1537–1564

[8] Lassas M, Saksman E and Siltanen S 2009 Discretization-invariant Bayesian
inversion and Besov space priors Inverse Problems and Imaging 3 87–122

[9] Lindgren F, Rue H and Lindstr¨om J 2011 An explicit link between Gaussian
ﬁelds and Gaussian Markov random ﬁelds: the stochastic partial diﬀerential
equation approach Journal of the Royal Statistical Society: Series B 73
423–498

[10] Roininen L, Lehtinen M, Lasanen S, Orisp¨a¨a M and Markkanen M 2011

Correlation priors Inverse Problems and Imaging 5 167–184

[11] Roininen L, Piiroinen P and Lehtinen M 2013 Constructing continuous
stationary covariances as limits of the second-order stochastic diﬀerence
equations Inverse Problems and Imaging 7 611–647

[12] Roininen L, Huttunen J and Lasanen S 2014 Whittle-Mat´ern priors for
Bayesian statistical inversion with applications to electrical impedance to-
mography Inverse Problems and Imaging 8 561–586

[13] Samorodnitsky G and Taqqu M S 1994 Stable non-Gaussian Random Pro-

cesses (Chapman & Hall, New York)

[14] V¨ansk¨a S, Lassas M and Siltanen S 2009 Statistical X-ray tomography

using empirical Besov priors Int. J. Tomog. Stat. 3–32

17

