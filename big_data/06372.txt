Evaluating the Impact of Treating the Optimal

Subgroup

Alexander R. Luedtke and Mark J. van der Laan

Abstract

Suppose we have a binary treatment used to inﬂuence an outcome. Given data from
an observational or controlled study, we wish to determine whether or not there exists
some subset of observed covariates in which the treatment is more effective than the
standard practice of no treatment. Furthermore, we wish to quantify the improvement
in population mean outcome that will be seen if this subgroup receives treatment and
the rest of the population remains untreated. We show that this problem is surprisingly
challenging given how often it is an (at least implicit) study objective. Blindly applying
standard techniques fails to yield any apparent asymptotic results, while using existing
techniques to confront the non-regularity does not necessarily help at distributions where
there is no treatment effect. Here we describe an approach to estimate the impact of
treating the subgroup which beneﬁts from treatment that is valid in a nonparametric
model and is able to deal with the case where there is no treatment effect. The approach is
a slight modiﬁcation of an approach that recently appeared in the individualized medicine
literature.

6
1
0
2

 
r
a

 

M
1
2

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
2
7
3
6
0

.

3
0
6
1
:
v
i
X
r
a

1

1 Introduction

Traditionally, statisticians have evaluated the efﬁcacy of a new treatment using an average

treatment effect which compares the population mean outcomes when everyone versus no

one is treated. While analyses of marginal effect often successfully identify whether or not

introducing a treatment into the population is beneﬁcial, these analyses underestimate the

overall beneﬁt of introducing treatment into the population when treatment is on average

harmful in some strata of covariates. A treatment need not have adverse physiological side

effects for the treatment effect to be negative: it will be negative if the administration of an

inferior treatment under study precludes the administration of a superior treatment. To avoid

this problem, researchers often perform subgroup analyses to see if the treatment effect varies

between different strata of covariates [1, 2]. Many investigators consider subgroups deﬁned

by a single covariate at a time [3, 4], though there is a growing trend toward deﬁning these

subgroups using multiple baseline covariates [5, 6].

Subgroup analyses have led to much disagreement between clinicians and statisticians.

As has been highlighted elsewhere [2], A R Feinstein eloquently described this controversy

as follows:

“The essence of tragedy has been described as the destructive collision of two

sets of protagonists, both of whom are correct. The statisticians are right in

denouncing subgroups that are formed post hoc from exercises in pure data

dredging. The clinicians are also right, however, in insisting that a subgroup

is respectable and worthwhile when established a priori from pathophysiological

2

principles.” [7]

While learning about subgroup speciﬁc effects is clearly important when they exist, the con-

cerns of statisticians are understandable. When the analysis is not prespeciﬁed, statistical

signiﬁcance procedures tend not to be reliable [2, 3, 8]. To exemplify the concerns on this

issue, P Sleight shows that the strong marginal effect of aspirin for preventing myocardial

infarction changes to a negative effect in two subgroups when these subgroups are deﬁned

by astrological sign [9]. While it is clearly unlikely in practice that astrological sign yields

heterogeneous subgroups, one would hope that a statistical procedure would be sufﬁciently

robust to inform the user that astrological sign is not in fact associated with efﬁcacy. Some

have argued that sample splitting methods would help robustify a procedure to “data dredg-

ing” by either humans or overﬁtting by an algorithm [10, 11, 12].

In a related literature, optimal individualized treatment strategy have been developed to

formalize the process of allowing treatment decisions to depend on baseline covariates in

a rigorous manner [13]. An individualized treatment strategy is a treatment strategy that

makes a treatment decision based on a patient’s covariates. Often the objective for such

treatments is to optimize the population mean outcome under the given treatment strategy [14,

15]. For binary treatment decisions in a single time point setting, an optimal individualized

treatment strategy is any individualized treatment strategy which treats all individuals for

which the average treatment effect is positive in their strata of covariates and does not treat

anyone for whom the average treatment is negative in their strata of covariates. Estimating the

population mean outcome under the optimal individualized treatment rule has been shown to

3

be non-regular when the optimal treatment strategy is not unique [16, 17]. This non-regularity

causes standard semiparametric estimation approaches to fail. Despite the complexity of this

estimation problem, [18] shows that one can develop a slower than root-n rate conﬁdence

interval for the mean outcome under the estimated optimal individualized treatment rule using

a bootstrap procedure, and [17] shows how to obtain a root-n rate conﬁdence interval for the

actual optimal individualized treatment strategy. Often one can use the same conﬁdence

interval for these two estimation problems because one can estimate the optimal treatment

strategy consistently (in terms of the strategy’s mean outcome) at a faster than root-n rate

[17].

As the reader may have noticed, two literatures are analogous – if one deﬁnes the optimal

subgroup as the subgroup of covariate strata in which the treatment effect is positive, then

the optimal subgroup is (up to covariate strata for which there is no treatment effect) equal

to the group of individuals for which an optimal treatment rule suggests treatment. However,

the subgroup literature has not confronted the problem of developing high powered inference

when an arbitrary algorithm is used to develop the subgroup – while the sample splitting

procedure described in [12] is valid for a single sample split provided the optimal subgroup

is not empty, subsequently averaging across sample splits will not yield valid inference (see

the discussion of the use of cross-validation for individualized treatment rules in [19]). Thus

there is a signiﬁcant loss of statistical power in such a procedure.

In this work, we aim to satisfy the desires of both statisticians and clinicians – we seek

a statistically valid subgroup analysis procedure which allows the incorporation of both the

4

subject matter knowledge of physicians and the agnostic ﬂexibility of modern statistical learn-

ing techniques. Our subgroup analysis procedure will return an estimate of the population

level effect of treating everyone in a stratum of covariates with positive treatment effect ver-

sus treating no one. This succinctly characterizes the effect of optimally introducing a given

treatment into a population. To estimate this quantity, we modify an estimator from the in-

dividualized treatment literature which overcomes a statistical challenge that typically arises

when trying to estimate quantities involving individualized treatment rules [17]. We will

show that an additional statistical challenge arises when trying to use a variant of this estima-

tor in the subgroup setting. We will then show how to overcome this challenge.

2 Statistical formulation

Suppose we observe baseline covariates W , an indicator of binary treatment A, and an out-
come Y occuring after treatment and covariates. Let P0 be some distribution for O ≡
(W, A, Y ) in a nonparametric statistical model M that at most places restrictions on the
probability of treatment given covariates. We observe n independent individuals O1, . . . , On
drawn from P0. Deﬁne b0(W ) = EP0[Y |A = 1, W ] − EP0[Y |A = 0, W ]. Under causal as-
sumptions not elaborated here, b0(W ) can be identiﬁed with the additive effect of treatment

on outcome if everyone versus no one in a strata of covariates W receives treatment [20]. We

use sg to denote any (measurable) subset of the support of W . Deﬁne

Ψsg(P0) ≡

b0(w)dP0(w).

(cid:90)

sg

5

Under causal assumptions, Ψsg(P0) is identiﬁed with the difference (i)-(ii) between (i) the av-

erage outcome if the only individuals receiving treatment in the population are those whose

covariates fall in sg and (ii) the average outcome if no one in the population receives treat-

ment. Drawing parallels to optimal individualized treatment strategies, Ψsg(P0) is maximized

at sg if and only if sg includes precisely those individuals with covariate w such that b(w) > 0

and does not include those with b(w) < 0 [14, 15]. The maximizer is non-unique at so-called

“exceptional laws”, i.e. distributions for which b(W ) = 0 with positive P0 probability [15].
Deﬁne Ψ(P0) ≡ maxsg Ψsg(P0). Throughout we deﬁne bP , Ψsg(P ), and Ψ(P ) at arbitrary
P ∈ M analogously to b0, Ψsg(P0), and Ψ(P0).

3 Breakdown of “standard” estimators

We now describe how the standard semiparametric estimation roadmap seems to suggest we

estimate Ψ(P0). A function known as the efﬁcient inﬂuence function (EIF) often plays a key

role in this estimation procedure. While we avoid a formal presentation of the derivation of

EIFs here, the key result about EIFs is that they typically yield the expansion

(cid:90)

n1/2[Ψ( ˆPn) − Ψ(P0)] = −n1/2

IF ˆPn

(o)dP0(o) + n1/2 Rem( ˆPn, P0),

(1)

where ˆPn is an estimate of P0, IF ˆPn
is the EIF of Ψ at ˆPn, and Rem( ˆPn, P0) is a remainder
that plausibly converges to zero faster than n−1/2. We will present an explicit expression for
at the end of this section, but for now we state that the corresponding remainder term is
IF ˆPn

6

given by

Rem( ˆPn, P0) =

(cid:90)

(2˜a − 1)

1(cid:88)
+ Ψsgn(P0) − Ψsg0(P0),

˜a=0

I(w ∈ sgn)

(cid:34)

1 − P0(˜a|w)
ˆPn(˜a|w)

(cid:35)(cid:0)E ˆPn

[Y |˜a, w] − EP0[Y |˜a, w](cid:1) dP0(o)

where sgn is the optimal subgroup under ˆPn. The ﬁrst term on the right is a double robust term
[21] that shrinks to zero faster than n−1/2 if the outcome regression and treatment mechanism
are estimated well. The second term requires that the optimal subgroup can be estimated

well and is plausible if the stratum speciﬁc treatment effect function does not concentrate

too much mass near zero (mass at zero is not problematic since any subgroup decision for

these strata is optimal). See Theorem 8 of [17] for precise conditions under which this term

is small. In principle sgn need not be an optimal subgroup under ˆPn, i.e. one can replace
Ψ( ˆPn) on the left-hand side of (1) with Ψsgn( ˆPn) without changing the expansion. We ignore
such considerations here for brevity, though discussion in a closely related problem is given

in [19].

A one-step estimator of the form ψn ≡ Ψ( ˆPn) + 1

n

i=1 IF ˆPn

(Oi) aims to correct the bias

on the right-hand side of (1) by adding an estimate of that expectation, yielding

(cid:80)n

(cid:90)

√

n [ψn − Ψ(P0)] ≈ 1√
n

(cid:18)

n(cid:88)

i=1

(cid:19)

(Oi) −

IF ˆPn

IF ˆPn

(o)dP0(o)

,

(2)

where the above approximation is valid provided n1/2 Rem( ˆPn, P0) converges to zero in prob-

7

ability. For a general parameter Ψ, targeted minimum loss-based estimators (TMLEs) can

be seen to follow the above prescribed formula, with the estimate ˆPn carefully chosen so

that the empirical mean of IF ˆPn
ψn = Ψ( ˆPn) [22]. A detailed exposition of efﬁciency theory is given in [23].

is zero, and thus the ﬁnal estimator is the plug-in estimator

We cannot apply the central limit theorem to the right-hand side of (2) without further

conditions because the right-hand side is a root-n empirical mean over functions which de-

pend on the data. We now give sufﬁcient conditions. Suppose that IF ˆPn
sense that

has a limit IF∞ in the

(cid:0)IF ˆPn

− IF∞(cid:1)2 has P0 expectation converging to zero.

(Lim)

Typically IF∞ = IFP0. If ˆPn is not allowed to heavily overﬁt the data, the instances of IF ˆPn
on the right-hand side of (2) can be replaced with IF∞. The conditions on ˆPn which prevent

overﬁtting are given by the empirical process conditions presented in Part 2 of [24]. If

VarP0[IF∞(O)] > 0,

(V+)

then the central limit theorem can be used to see that n1/2[ψn − Ψ(P0)] converges to a normal
distribution with mean zero and variance VarP0[IF∞(O)]. Under these conditions, Ψ(P0)
falls in ψn± 1.96 σn√
n is the empirical variance of
applied to the data. If VarP0[IF∞(O)] is zero, then n1/2[ψn − Ψ(P0)] converges to zero
IF ˆPn
in probability, but there is no guarantee that ψn ± 1.96 σn√
n contains Ψ(P0) with probability

n with probability approaching 0.95, where σ2

8

approaching 0.95: both

√
n[ψn−Ψ(P0)] and σn are converging to zero, but coverage depends

on the relative rate of convergence of the two quantities.

We now argue that it is unlikely that both (Lim) and (V+) hold. When ˆPn is non-

exceptional,

IF ˆPn

(o) = I[b ˆPn

(w) > 0]

(cid:34)

2a − 1
ˆPn(a|w)

(Y − E ˆPn

[Y |A = a, W = w]) + b ˆPn

(cid:35)

(w)

− Ψ( ˆPn).

If ˆPn is exceptional, the above deﬁnition of IF ˆPn
theorem result about (2) holds under (Lim) and (V+), though in truth Ψ is not smooth enough

can still be used and the same central limit

at ˆPn for an efﬁcient inﬂuence function to be well-deﬁned [16, 17]. In light of the above

expression, the validity of (Lim) will typically require I[b ˆPn
limit. Suppose the data is drawn from an exceptional law where the treatment effect is zero

(W ) > 0] to have a mean-square

on some set S0. In that case we do not expect I[b ˆPn
since likely b ˆPn
which b0(w) = 0.

(w) does not converge to 0 strictly from above or below at any given w for

(W ) > 0] to converge to anything on S0

Now consider the case where treatment is always harmful, i.e. b0(W ) < 0 with proba-
bility 1. In this case b0(W ) ≥ 0 with probability 0, and so we would expect the indicator
(w) is positive converges to zero if ˆPn is a good estimate of P0. But in this case the
that b ˆPn
subgroup that should be treated is empty so that if the limit IF∞ exists then it is zero almost

surely and (V+) does not hold.

Finally, consider the intermediate case where there is no additive treatment effect within

9

any strata of covariates, i.e. b0(w) = 0 for all w. If on any positive probability set convergence

occurs from both above and below, then we do not expect (Lim) to hold. If b ˆPn
to zero from below for all w, then we expect (Lim) to hold with IF∞ equal to the constant

(w) converges

function zero and (V+) not to hold. If, for each w, b ˆPn
strictly above or strictly below and the set of covariates for which the convergence occurs

(w) converges to zero from either

from above happens with positive probability, then we expect (Lim) and (V+) to hold.

4 Avoiding the need for (Lim) and (V+)

In this section, we present an estimator which overcomes both (Lim) and (V+). We ﬁrst

present an estimator which does not require (Lim), and then argue that a simple extension of

this estimator also does not require (V+).

This estimation strategy is similar to the one-step estimator presented in the previous

section, but designed to estimate the parameter in an online fashion which eliminates the

need for convergence. The online one-step estimator was originally presented in [25], and

was reﬁned in [17] to deal with cases where the convergence of the sort required by (Lim)

fails to hold. The method will be presented in full generality in a forthcoming paper.

Let ˆP i

n represent an estimate of P0 based on observations O1, . . . , Oi. The stabilized

online one-step estimator for Ψ(P0) is given by

n−1(cid:88)

n ≡ ¯σn
ψst
n − (cid:96)n

i=(cid:96)n

ˆσi

Ψ( ˆP i

n) + IF ˆP i

n

(Oi+1)

.

(3)

where (cid:96)n is some user-deﬁned quantity that may or may not grow to inﬁnity but must sat-

10

isfy n − (cid:96)n → ∞, ˆσ2
vations O1, . . . , Oi, and ¯σn is the harmonic mean 1/[

i represents an estimate of the variance of IFdi
ˆP i
n
ˆσ−1
] of ˆσ(cid:96)n, . . . , ˆσn−1. We

(O) based on obser-

1

n−(cid:96)n

i=(cid:96)n

i

(cid:80)n−1

now wish to apply the martingale central limit theorem [26] to understand the behavior of
√
n − (cid:96)n¯σ−1
n − Ψ(P0)]. As each term in the sum deﬁning ψn has variance converging to
1 due to the stabilization by ˆσi, the validity of our central limit theorem argument does not

n [ψst

rely on an analogue of (Lim). It does, however, rely on an analogue of (V+). The primary

(cid:104)

(cid:105)

condition we would use to establish the validity of the central limit theorem argument is that

the set of ˆσ2

i are consistent for VarP0

IF ˆP i

n

(Oi+1)

as i gets large and that there exists some

δ > 0 such that

(cid:104)

(cid:105)

(Oi+1)

> δ2 > 0 for all i with probability approaching 1.

(V+’)

VarP0

IF ˆP i

n

The former condition holds under a Glivenko-Cantelli condition which is discussed in Theo-

rem 7 of [17]. Under these conditions, Section 7 of [17] (especially Lemma 6) shows that

√

n − Ψ(P0)]
n − (cid:96)n [ψst
¯σn

≈

1√
n − (cid:96)n

n−1(cid:88)

IF ˆP i

n

(Oi+1) −(cid:82) IF ˆP i

n

i=(cid:96)n

ˆσi

(o)dP0(o)

provided the same conditions needed for (2) hold. The above approximation is accurate up

to a term that goes to zero in probability. The martingale central limit theorem can now be

applied to establish the validity of the 95% conﬁdence interval CIst ≡ (cid:104)

(cid:105)

n ± 1.96 ¯σn√
ψst
n−(cid:96)n

.

We refer the reader to Theorem 2 in [17] for a sense of the formal conditions needed to prove

this result. If the treatment effect is negative for all strata of covariates, then (V+’) will not

11

hold if ˆP i

n is a reasonable estimate of P0 in the sense that the estimated optimal subgroup
converges to the empty set. Similarly, we have no guarantee that (V+’) will hold if b0(W ) is

zero almost surely.

Suppose that we are not willing to assume that (V+’) holds. A natural approach is to
redeﬁne the inverse weights to equal ˆσi(δ) ≡ ˆσi ∨ δ for some ﬁxed δ > 0. We then deﬁne
¯σn(δ) to be equal to the harmonic mean of ˆσ(cid:96)n(δ), . . . , ˆσn−1(δ) and ψst
n(δ) as in (3) but with
ˆσi and ¯σn replaced by ˆσi(δ) and ¯σn(δ). Clearly ¯σn(δ) ≥ ¯σn, and thus the conﬁdence interval
is no wider than that CIst, though it may have a different

CIst(δ) ≡ (cid:104)

(cid:105)

n(δ) ± 1.96 ¯σn(δ)√
ψst
n−(cid:96)n

midpoint. We conjecture that this conﬁdence interval is conservative when the truncation

scheme is active so that ¯σn(δ) > ¯σn, though proving this result has proven challenging.

To give the reader a sense of why we have hope that this conjecture will hold, we show

in the appendix that adding normal noise (with random variance depending on the sample)

to ψn(δ) can yield a valid 95% conﬁdence interval for Ψ(P0).

It then seems reasonable

that removing the noise can only improve coverage. Readers whose primary concern is the

theoretical soundness of an inferential procedure can apply the estimator in the appendix

and rest assured that their conﬁdence interval will be valid provided the optimal subgroup is

estimated sufﬁciently well and a double robust term is small. Nonetheless, the type I error

gains by not using this noised estimator, which we will see provided the conjecture holds,

would seem to imply that our original unnoised conﬁdence interval performs better than the

noised interval.

12

5 Simulation study

5.1 Methods

We now present a simulation study conducted in R [27]. Our simulation uses a four-dimensional

covariate W drawn from a mean zero normal distribution with identity covariance matrix.

Treatment A is drawn according to a Bernoulli random with probability of success 1/2, in-

dependent of baseline covariates. The outcome Y is Bernoulli, and the outcome regressions

considered in our primary analysis are displayed in Table 1.
Simulation logit E[Y |A, W ]
N1
N2
N3
A1
A2
A3

W1 + W2
−0.2A[(W1 − z0.8)+]2
−0.25A
0.8A
AW +
AW1

1
0.80
0
0
0.25
0

Ψ(P0) P0 (b0(W ) = 0) P0 (b0(W ) < 0)
0
0
0
0.19
0.06
0.09

0
0.20
1
0
0.38
0.50

1 − AW +

2

Table 1: Data generating distributions for simulation. Decimals rounded to the nearest hun-
dredth. For N2, z0.8 ≈ 0.84 is the 80th percentile of a standard normal distribution. We use
x+ to denote the positive part of a real number x.

We compare two estimators of Ψ(P0). The ﬁrst is the stabilized one-step estimator. We
truncate the inverse weights at 0.1, 0.001, and 10−20. The results were essentially identical
for truncations of 10−3 and 10−20, and thus we only display the results for the truncation of
10−3. We set (cid:96)n = n/10, and to speed up the computation time we estimated the subgroup
and outcome regression using observations O1, . . . , Ok(i)n/10 for all i ≥ (cid:96)n, where k(i) is the
largest integer such that k(i)n/10 < i (see Section 6.1 of [17] for more details). The second

13

estimator is a ten-fold cross-validated TMLE (CV-TMLE). This estimator is analogous the

CV-TMLE for the mean outcome under an optimal treatment rule as presented in [19], but is

modiﬁed to account for the fact that Ψ(P0) is equal to this quantity minus the mean outcome

when no one in the population is treated. We truncate the variance estimates for this esti-

mator at the same values as considered for the stabilized one-step estimator. Following the

theoretical results in [28], we can formally show that this estimator is asymptotically valid

when cross-validated analogues of (Lim) and (V+) hold.

We estimate the blip function using the super-learner methodology as described in [29].

Super-learner is an ensemble algorithms with an oracle guarantee ensuring that the resulting

blip function estimate will perform at least as well as the best candidate in the library up to a

small error term. We use a squared error loss to estimate the blip function, and use as candi-

date algorithms SL.gam, SL.glm, SL.glm.interaction, SL.mean, and SL.rpart
in the R package SuperLearner [30]. The outcome regression E[Y |A, W ] is estimated
using this same super-learner library but with the log-likelihood loss to respect the bounds

on the outcome. The probability of treatment given covariates was treated as known and the

known value was used by all of the estimators.

We also compare our estimators to the oracle estimator which a priori knows the optimal

subgroup. In particular, we use a CV-TMLE for Ψsg0(P0) in which we treat sg0 as known.
The estimation problem is regular in this case so that we expect the corresponding conﬁdence

intervals to have proper coverage at exceptional laws. We truncate the variance estimate at

the same values as for the other methods.

14

5.2 Results

Figure 1 displays the coverage of the conﬁdence interval lower bounds of the various esti-

mation strategies. All methods appear to achieve proper 97.5% lower bound coverage, with

the stabilized one-step and the non-oracle CV-TMLE estimators generally being conserva-

tive. This conservative behavior is to be expected given that both of these sample splitting

procedures need to estimate the optimal subgroup, and thus in any ﬁnite sample are expected

to be negatively biased due to the resulting suboptimal subgroup used from the estimate.

The oracle CV-TMLE attains the nominal coverage rate for alternative distributions, and for

non-alternatives is also conservative.

We now verify the tightness of the lower bounds for the alternative distributions A1, A2,

and A3. Figure 2 shows the power for the test of H0 : Ψ(P0) = 0 against H1 : Ψ(P0) >

0, where the test was conducted using the duality between hypothesis tests and conﬁdence

intervals. We see that the stabilized one-step is slightly less powerful than the non-oracle

CV-TMLE. This is likely due to the online nature of the stabilized one-step estimator relative

to the CV-TMLE. Nonetheless, the power loss is not large and the fact that we have actual

theoretical results for this estimator even at exceptional laws should make up for this slight

loss of power.

Figure 3 displays the two-sided coverage of the 95% conﬁdence intervals. One could

argue that upper bound coverage is not interesting for the null distributions, given that any

failure of the upper bound of the conﬁdence interval to cover Ψ(P0) = 0 requires this upper

bound to be negative. Hence we can always obtain proper upper bound coverage at null dis-

15

Figure 1: Coverage of 97.5% lower conﬁdence intervals for the impact of treating the optimal
subgroup versus sample size. Horizontal axis on a log scale. All methods (approximately)
achieve nominal coverage, with the stabilized one-step and non-oracle CV-TMLE generally
being conservative.

tributions by ensuring that the upper bound of our conﬁdence interval respects the parameter

space of Ψ, i.e. is nonnegative. Nonetheless, the coverage of the uncorrected two-sided con-

ﬁdence intervals (upper bound may be negative) is useful for detecting a lack of asymptotic

normality of the estimator sequence. While the stabilized one-step has two-sided coverage

above 0.95 for all distributions at all sample sizes of at least 500, the coverage for the unad-

justed non-oracle CV-TMLE conﬁdence interval falls at or below 0.90 for N1 and N2 at all

sample sizes. This is in line with our lack of asymptotic results for the non-oracle CV-TMLE

16

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.950.97510.950.97510.950.97510.950.97510.950.97510.950.9751N1N2N3A1A2A3250500100020004000Standardized WidthLower Bound CoverageMethodlllNon−Oracle CV−TMLEOracle CV−TMLEStabilized One−StepTruncationl0.10.001Figure 2: Power for rejecting a test of H0 : Ψ(P0) = 0 versus H1 : Ψ(P0) > 0 at the
0.025 level at different sample sizes. Horizontal axis on a log scale. All methods have power
increasing to 1. The stabilized one-step has lower power than the non-oracle CV-TMLE,
though its power is typically more comparable to the stabilized one-step relative than the
oracle CV-TMLE which knew the optimal subgroup from the outset.

at exceptional laws.

We now consider the two-sided coverage for the alternative distributions A1, A2, and A3.

The stabilized one-step conﬁdence intervals have coverage that improves with sample size,

though the improvement appears slow. The coverage is near nominal at a sample size of 4000

for all three simulations. In light of Figure 1, essentially all of the coverage deﬁciency is a

result of a failure of the upper bound. This makes sense given that our estimator relies on

a second-order term measuring a linear combination of the difference in impact of treating

the estimated subgroups (estimated on increasing chunks of data) versus treating the optimal
subgroup. While this term often reasonably shrinks to zero faster than n−1/2, in ﬁnite samples

17

lllllllllllllllllllllllllllllllllllllllllllll00.5100.5100.51A1A2A3250500100020004000Sample SizePowerMethodlllNon−Oracle CV−TMLEOracle CV−TMLEStabilized One−StepTruncationl0.10.001Figure 3: Coverage of 95% two-sided conﬁdence intervals for the impact of treating the
optimal subgroup versus the average conﬁdence interval width (standardized so that the max-
imum width in a given sample size-data generating distribution pair is 1). The stabilized one-
step conﬁdence intervals have nominal coverage for all null distributions, and have coverage
approaching nominal for all alternative distributions. The non-oracle CV-TMLE achieves
near nominal coverage for all alternative distributions, though has below nominal coverage
of approximately 90% for all of the null distributions.

this term can hurt the upper bound coverage. The non-oracle CV-TMLE conﬁdence intervals,

on the other hand, attain near nominal coverage at large sample sizes. This is to be expected

at the non-exceptional laws A1 and A3 given that we can prove asymptotic normality in

this case. We do not have an asymptotic result supporting the method’s proper coverage for

exceptional law A2, though this is an interesting area for future work.

18

250500100020004000llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.9000.9250.9500.9751.0000.900.951.000.850.900.951.000.70.80.90.40.60.80.50.60.70.80.9N1N2N3A1A2A30.00.51.00.00.51.00.00.51.00.00.51.00.00.51.0Standardized WidthTwo−Sided CoverageMethodlllNon−Oracle CV−TMLEOracle CV−TMLEStabilized One−StepTruncationl0.10.0016 Discussion

We have studied the statistical challenges associated with estimating the additive effect of

treating the subgroup of individuals with positive stratum speciﬁc additive treatment effect.

We showed that these challenges are similar to those arising when estimating the mean out-

come under an optimal individualized treatment strategy. Indeed, the individuals treated by

the strategy which maximizes the population mean outcome are the same individuals who

belong to the optimal subgroup. An additional challenge arises when one wishes to consider

the relative measure giving the additive effect of treating only those individuals in the optimal

subgroup versus treating no one in the population. In this case the parameter of interest is es-

timable at a faster than root-n rate for some data generating distributions. Procedures which

yield root-n rate conﬁdence intervals tend to fail in this setting due to the need to estimate

both the (in truth empty) optimal subgroup and the variance of the estimate of the impact of

treating this subgroup: generally the subgroup estimate will converge to the empty set and

the variance estimate will converge to zero, but there is no guarantee that the relative rate of

convergence of the two will yield valid inference.

Despite this added inferential challenge, we argue that obtaining a conﬁdence interval

for the impact of treating the optimal subgroup requires only minor modiﬁcation to the con-

ﬁdence interval for the mean outcome when only the optimal subgroup is treated. In par-

ticular, we propose truncating the estimated variance in the martingale sum used in [17] at

some constant δ > 0. If the truncation is not active, which will typically be true for al-

ternative distributions under which there exists a subgroup for which the treatment effect is

19

positive and is arguably true for many null distributions as well, then, under standard regu-

larity conditions, we obtain root-n rate inference with coverage approaching 0.95. If the data

is generating according to an alternative distribution for which there is a non-null (positive or

negative) treatment effect within all strata of covariates, then our estimator is asymptotically

efﬁcient and, provided the truncation is not active, our conﬁdence interval is asymptotically

equivalent to a standard Wald-type conﬁdence interval (see Corollary 3 in [17]). We expect

our conﬁdence interval to be conservative when the truncation is active, though we leave this

as a conjecture. We have instead shown that adding noise to our estimator yields a conﬁ-

dence interval with proper 95% coverage, though we suggest using the unnoised estimator in

practice.

One could imagine several alternative solutions to the described inferential challenge.

One such solution is to ensure that the variance of our estimator minus the truth, scaled by

root-n, is positive as sample size grows. This can be accomplished by changing the deﬁnition

of the optimal subgroup to ensure that this subgroup is not too small, e.g. contains at least

10% of the population. One can show via a change of variables that estimating the mean

outcome under such a constrained subgroup is equivalent to estimating the mean outcome

under an optimal rule which can treat at most 90% of the population, see [31]. Estimating

this alternative constrained parameter is still difﬁcult when the optimal subgroup is non-

unique, though there is little risk of degenerate ﬁrst-order behavior in this case. To construct

conﬁdence intervals despite the non-uniqueness of the optimal subgroup, one can combine

the results in [31] with the stabilized one-step estimator presented in [17].

20

A cross-validated TMLE, closely related to that presented in [28], outperformed the

method proposed in this paper in many simulation settings. Nonetheless, we do not have

any asymptotic results about the CV-TMLE at exceptional laws, in contrast to the estimator

presented in this paper for which we do have such results. This estimator’s lack of asymp-

totic normality at such laws was evident in our simulation. We view a careful study of this

estimator’s behavior at exceptional laws to be an important area for future research. In a

forthcoming work, we will present a stabilized TMLE that has the same desirable asymp-

totic properties of the stabilized one-step estimator but, like the CV-TMLE, is a substitution

estimator (thereby forcing the estimate to respect the parameter space).

One could imagine considering other parameters relating to the optimal subgroup that

we have presented in this paper. For example, investigators may be interested in estimating

the impact of treating everyone in the optimal subgroup on some secondary outcome ˜Y .

Each such parameter yields a new estimation problem and, in our experience, many of these

problems still face at least one of the two primary challenges that we faced in this paper. In

particular, these problems are often non-regular when the optimal subgroup is non-unique,

and may have degenerate ﬁrst-order behavior when the optimal subgroup is empty.

Acknowledgement

This research was supported by NIH grant R01 AI074345-06. Alex Luedtke was supported

by a Berkeley Fellowship. The authors thank Tyler VanderWeele for valuable discussions.

21

References
[1] Assmann SF, Pocock SJ, Enos LE, Kasten LE. Subgroup analysis and other (mis) uses of baseline

data in clinical trials. Lancet. 2000;355(9209):1064–1069.

[2] Rothwell PM. Subgroup analysis in randomised controlled trials: importance, indications, and

interpretation. Lancet. 2005;365(9454):176–186.

[3] Yusuf S, Wittes J, Probstﬁeld J, Tyroler HA. Analysis and interpretation of treatment effects in

subgroups of patients in randomized clinical trials. Jama. 1991;266(1):93–98.

[4] VanderWeele TJ. On the distinction between interaction and effect modiﬁcation. Epidemiology.

2009;20(6):863–871.

[5] Kent DM, Hayward RA. Limitations of applying summary results of clinical trials to individual

patients: the need for risk stratiﬁcation. Jama. 2007;298(10):1209–1212.

[6] Abadie A, Chingos MM, West MR. Endogenous stratiﬁcation in randomized experiments. Na-

tional Bureau of Economic Research; 2013. NBER Working Paper No. 19742.

[7] Feinstein AR. The problem of cogent subgroups: a clinicostatistical tragedy. J Clin Epidemiol.

1998;51(4):297–299.

[8] Lagakos SW. The challenge of subgroup analyses-reporting without distorting. N Engl J Med.

2006;354(16):1667.

[9] Sleight P. Debate: Subgroup analyses in clinical trials: fun to look at-but dont believe them.

Curr Control Trials Cardiovasc Med. 2000;1(1):25–27.

[10] Lipkovich I, Dmitrienko A, Denne J, Enas G. Subgroup identiﬁcation based on differential
effect searchA recursive partitioning method for establishing response to treatment in patient
subpopulations. Stat Med. 2011;30(21):2601–2621.

[11] Dmitrienko A, Muysers C, Fritsch A, Lipkovich I. General Guidance on Exploratory and Con-
ﬁrmatory Subgroup Analysis in Late-Stage Clinical Trials. J Biopharm Stat. 2015;26(1):71–98.

[12] Malani A, Bembom O, der Laan MJ. Accounting for Differences Among Patients in the FDA

Approval Process. U Chicago Law & Econ Olin Work Pap. 2009;(488).

[13] Chakraborty B, Moodie EE. Statistical Methods for Dynamic Treatment Regimes. Berlin Hei-

delberg New York: Springer; 2013.

[14] Murphy SA. Optimal dynamic treatment regimes. J R Stat Soc Ser B. 2003;65(2):331–336.

22

[15] Robins JM. Optimal structural nested models for optimal sequential decisions. In: Lin DY, P H,

editors. Proc. Second Seattle Symp. Biostat.. vol. 179; 2004. p. 189–326.

[16] Robins JM, Rotnitzky A. Discussion of “Dynamic treatment regimes: Technical challenges and

applications”. Electron J Stat. 2014;8(1):1273–1289.

[17] Luedtke AR, van der Laan MJ. Statistical inference for the mean outcome under a possibly

non-unique optimal treatment strategy. Ann Statist. 2016;44(2):713–742.

[18] Chakraborty B, Laber EB, Zhao YQ. Inference about the expected performance of a data-driven

dynamic treatment regime. Clin Trials. 2014;11(4):408–417.

[19] van der Laan MJ, Luedtke AR. Targeted learning of the mean outcome under an optimal dynamic

treatment rule. J Causal Inference. 2014;3(1):61–95.

[20] Robins JM. A new approach to causal inference in mortality studies with a sustained exposure
periodapplication to control of the healthy worker survivor effect. Math Model. 1986;7(9):1393–
1512.

[21] van der Laan MJ, Robins JM. Uniﬁed methods for censored longitudinal data and causality.

New York Berlin Heidelberg: Springer; 2003.

[22] van der Laan MJ, Rose S. Targeted Learning: Causal Inference for Observational and Experi-

mental Data. New York: Springer, New York; 2011.

[23] Bickel PJ, Klaassen CAJ, Ritov Y, Wellner JA. Efﬁcient and adaptive estimation for semipara-

metric models. Baltimore: Johns Hopkins University Press; 1993.

[24] van der Vaart AW, Wellner JA. Weak convergence and empirical processes. Berlin Heidelberg

New York: Springer; 1996.

[25] van der Laan MJ, Lendle SD. Online Targeted Learning. Division of Biostatistics, University of
California, Berkeley; 2014. 330, available at http://www.bepress.com/ucbbiostat/.

[26] Brown BM. Martingale central limit theorems. Ann Math Stat. 1971;42(1):59–66.

[27] R Core Team. R: a language and environment for statistical computing. Vienna, Austria; 2014.

Available from: http://www.r-project.org/.

[28] van der Laan MJ, Luedtke AR. Targeted learning of an optimal dynamic treatment, and statistical
inference for its mean outcome. Division of Biostatistics, University of California, Berkeley;
2014. 329, available at http://www.bepress.com/ucbbiostat/.

23

[29] Luedtke AR, van der Laan MJ. Super-learning of an optimal dynamic treatment rule. Division
of Biostatistics, University of California, Berkeley, under review at JCI; 2014. 326, available at
http://www.bepress.com/ucbbiostat/.

[30] Polley E, van der Laan MJ. SuperLearner: super learner prediction; 2013. Available from:

http://cran.r-project.org/package=SuperLearner.

[31] Luedtke AR, van der Laan MJ. Optimal dynamic treatments in resource-limited settings. 2015;.

Appendix
Let {Zi : i = 1, . . . ,∞} be a sequence of i.i.d. normal random variable independent of all
other sources of randomness under consideration. Let

n−1(cid:88)

(Oi+1) +(cid:112)(δ2 − ˆσ2

i )+Zi

,

˜ψst

n(δ) =

¯σn
n − (cid:96)n

Ψ( ˆP i

n) + IF ˆP i

n

i=(cid:96)n

ˆσi(δ)

where x+ is the positive part of a real number x. Observe that each term in the sum on the

right-hand side above has variance approximately 1 (approximately because ˆσi is only an

n

estimate of VarP0[IF ˆP i
ply the martingale central limit theorem appearing in [26] to show that

(O)]). It will then follow that, under regularity conditions, we can ap-
n(δ) ± 1.96 ¯σn(δ)√
n−(cid:96)n
has coverage approaching 95% for Ψ(P0). These regularity conditions are the same as those
used to establish the validity of CIst, except that they do not require (V+’) to hold. Note the

(cid:104) ˜ψst

(cid:105)

similarity to CIst(δ), though the above conﬁdence interval is centered about ˜ψn(δ) rather than

ψn(δ).

We now relate the noised ˜ψst
n(δ) + ˜σn(δ)√
n−(cid:96)n

in distribution to ψst

n to the unnoised ψst

n. Conditional on the data, ˜ψst

(cid:80)n−1

i=(cid:96)n

n(δ) is equal
n(δ) is

. That is, ˜ψst

(δ2−σ2
i )+
i ∨δ2
ˆσ2

Z1, where ˜σ2

n(δ) ≡ 1
n−(cid:96)n

24

equal to ψn(δ) plus normal noise, where the variance of the normal noise depends on the data.

If the truncation is not active, then the variance of this noise is zero. Otherwise, the variance

is positive, but the sign of the noise is independent of the data. Thus it seems reasonable to

expect that the unnoised ψst

n. It is for this reason
that we expect the unnoised conﬁdence interval CIst(δ) to have coverage at least 0.95 in large

n provides a better estimate of Ψ(P0) than ˜ψst

samples.

25

