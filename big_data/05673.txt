6
1
0
2

 
r
a

 

M
7
1

 
 
]
L
C
.
s
c
[
 
 

1
v
3
7
6
5
0

.

3
0
6
1
:
v
i
X
r
a

Predicting health inspection results from online

restaurant reviews

Samantha Wong1, Hamidreza Chinaei1, and Frank Rudzicz2

1 The G. Raymond Chang School of Continuing Education, Ryerson University,

Toronto, ON, Canada

samantha.y.wong@ryerson.ca, hrchinaei@ryerson.ca

2 Department of Computer Science, University of Toronto,

Toronto, ON, Canada
frank@cs.toronto.edu

Abstract. Informatics around public health are increasingly shifting
from the professional to the public spheres. In this work, we apply lin-
guistic analytics to restaurant reviews, from Yelp, in order to automati-
cally predict oﬃcial health inspection reports. We consider two types of
feature sets, i.e., keyword detection and topic model features, and use
these in several classiﬁcation methods. Our empirical analysis shows that
these extracted features can predict public health inspection reports with
over 90% accuracy using simple support vector machines.

1

Introduction

Typically, oﬃcial Health and Safety inspections of restaurants are only conducted
one to three times a year, and involve ﬁxed checklists related to food safety3.
Meanwhile, consumer reviews of these restaurants are posted publicly on sites
like Yelp and TripAdvisor much more frequently. Naturally, this leads to the
question of whether restaurant reviews can support the public health inspection
process by ﬂagging potentially high risk restaurants that may need more frequent
inspections.

To address this question, we compare oﬃcial restaurant health inspection
results from the Kitchener-Waterloo region against text comments found in Yelp
reviews of the relevant restaurants. In particular, we extract series of keywords
and topic model features and apply these to a few classiﬁcation methods. Our
extracted features can predict the public health inspection with 90% accuracy
when used with a support vector machine (SVM).

1.1 Related Work

Kang et al. [2013] matched keywords and phrases (unigram and bigram) from
text restaurant reviews and other Yelp metadata for to results from the Depart-
ment of Health’s research results, with the goal of allocating inspectors more

3 http://chd.region.waterloo.on.ca/en/healthylivinghealthprotection/

resources/sample_restaurant_inspection.pdf

Fig. 1. Can text restaurant reviews predict health inspection results?

eﬃciently. That study used liblinear’s SVM with L1 regularization and 10-fold
cross-validation, which was able to achieve 82.68% accuracy when classifying
text reviews with problematic restaurants. Kang et al. suggested that the high-
est accuracy can be achieved using only contentful unigrams and bigrams. They
also presented cue features for diﬀerent topics/categories such as Hygienic (e.g.,
gross, mess, sticky, smell, dirty), Cuisines (Vietnamese, Dim Sum, Thai), and
Sentiment (cheap, never). Our work extends theirs in a few ways, including our
use of topic modelling.

Blei et al. [2003] proposed a Bayesian topic modeling approach called la-
tent dirichlet allocation (LDA) which is used for discovering hidden topics of
documents. In this model, a document can be represented as a mixture of the
learned hidden topics, where each hidden topic is represented by a distribu-
tion over words occurred in the document. The authors also applied their LDA
method to text classiﬁcation on Reuters-21578 dataset. They trained a SVM on
the topic model representations applied by LDA and compared this SVM to an
SVM trained on all the word features. They concluded that the former SVM
can potentially produce better accuracy results. In this work, we use both topic
model features and keyword features in an SVM.

Sahami et al. [1998] studied junk mail ﬁltering using na¨ıve Bayes and message
text, along with several other non-textual features. They also described build-
ing a corpus of keywords from the text of junk emails, and how some of those
keywords were removed as part of feature selection based loosely on Zipf’s law.
They were able to achieve 97.1% precision and 94.3% recall for classifying junk
mail. In our work, we use also na¨ıve Bayes and text reviews, but for a diﬀerent
purpose.

Nsoesie et al. [2014] addressed the spread of food-borne illness through Yelp
reviews using keyword selection (according to symptoms) and clustering to pre-
dict the geographical location where a food-borne outbreak originated. They also
highlighted limitations in using Yelp data to support health-related studies,
speciﬁcally with regards to review bias, and issues related to timing. Nonethe-
less, Nsoesie et al. conﬁrmed that it is possible to crowd-source food and health
reports, given careful analysis.

Other work has applied analysis of textual signals in social media to public
health surveillance more generally, especially with regards to the spread of dis-
ease [Aramaki et al., 2011; Sadilek et al., 2012, 2013; Lamb et al., 2013; Dredze
et al., 2013; Von Etter et al., 2010]. The increase of recent work in this area
suggests at a shift towards ‘crowd-sourced’ public health informatics.

2 Data

In our study, we use Yelp4 and Waterloo Public Health (WPH)5 data. We ex-
tract facility information from WPH, as well as oﬃcial inspection results (date,
type, and recommended actions). From Yelp, we extract business information
(matched with WPH facility information) and tokenize text reviews. These to-
kens are used in a document/term matrix, and act as features in this work. The
other set of features are learned topic models, i.e., the distribution of topics for
each learned topic. We use binary prediction labels: “action” indicating non-
compliance or other major problem with the restaurant, and “no action” if the
restaurant passes health inspection.

3 Approach

Figure 2 summarizes our approach, summarized here.

Import row data First, we ﬁlter business data to those in the Kitchener-Waterloo-
Cambridge area. We then extract the data for the region from WPH. Finally, we
merge the two datasets, resulting in a list of 126 restaurants, their Yelp Business

4 https://www.yelp.ca/dataset_challenge/dataset
5 http://www.regionofwaterloo.ca/en/regionalGovernment/

FoodPremiseDataset.asp

Fig. 2. High-level approach to automatic, crowd-sourced health inspection method.

IDs, and WPH Facility IDs. This allows us to merge WPH health inspection re-
sults and textual Yelp reviews. A preliminary qualitative analysis of documents
labeled with “action” suggested that relatively few of the associated reviews
included negative terms (e.g., dirty, sick, bad). Interestingly, many of those re-
views focused on poor service, food temperature, or the restaurant environment
generally, rather than direct references to public health issues.

Build a document-term matrix (DTM) We build a tf-idf weighted document-
term matrix using all text reviews. First, we create a regular document-term
matrix with plain text (stripped of whitespaces, punctuation, stopwords, and
stems). Then, we weigh each term with tf-idf. Speciﬁcally, we calculate term
frequencies for each word using a function based on:

TF(t,d) =

Number of times term t appears in a document d

Total number of terms in document d

We then calculate the inverse document frequencies using:

(cid:18)

.

(cid:19)

IDF(t,D) = log

Total number of documents in set D

Number of documents in D containing term t

.

Finally, the overall score for term-document pair (t, d) is T F (t, d) · IDF (t, D).
We use the built-in functionality of the tm package in R6, which includes
further optimizations for tf-idf, and we use the resulting matrix for all of our
calculations below.

Applying na¨ıve Bayes (NB) classiﬁcation to the DTM We apply NB to the full
DTM created above but obtained poor results. NB had diﬃculty classifying “No
Action” reviews, with only 11.78% accuracy and 10% sensitivity.

Therefore, we reduce the number of features in the DTM by using only the N
most frequent few contentful terms, identiﬁed by sums of the weighted values for
each column of the DTM. Optimizing this process gives N = 300, with which
we achieve an accuracy of 47.13% and a sensitivity of 13.22%, which is still
sub-optimal.

In an eﬀort to improve the results, we add latent Dirichlet analysis (LDA) [Blei
et al., 2003] to perform topic modelling, and incorporate these topics into a SVM
classiﬁer.

Build a topic model using LDA First, we apply term frequency weighting to
the original document term matrix (as opposed to tf-idf), prior to LDA. We
empirically select 20 topics and, similar to previous work [Gruber et al., 2007],
we set the Dirichlet prior on the per-document topic distributions to α = 1+50/k
(where k is the number of topics), giving α = 3.5. Topics appeared to be mostly
divided by the type of cuisine (e.g., Chinese, wings, burgers, tea/coﬀee, breakfast).
We then extract the posterior topic distribution for each document, along
with the document term matrix, to perform SVM classiﬁcation. Table 1 shows
a sample of learned topics.

6 http://www.inside-r.org/packages/cran/tm/docs/weightTfIdf

Table 1. All learned topics, represented by their top keywords. Those keywords oc-
curring in negative contexts are in red.

order,food,service,time,experience room,local,amazing,food,love
food,place,just,can,service
food,great,menu,drink,night
food,burrito,buﬀet,Chinese,place taco,chicken,pork,try,order
sushi,place,ayc,good,roll
coﬀee,place,tea,great,friend
lunch,restaurant,time,service,like pizza,food,good,place,great
burger,fries,good,place,like
sandwich,pasta,food,fresh,place Thai,food,curries,order,service
wing,good,like,night,great
excel,food,really,great,place
meal,good,egg,breakfast,food

good,love,place,bake,store
good,place,really,pretty,food
great,food,good,place,friend
place,poutine,food,just,restaurant

Combining DTM and LDA topics in SVM classiﬁcation As discussed above, we
create the DTM with TF weighting, and bind the LDA posterior topic distri-
butions, as well as document classes (‘Action’ or ‘No Action’). As before, we
optimize the number of features from the DTM within an SVM classiﬁer. To
avoid issues of imbalanced classes, we use SMOTE [Chawla et al., 2002] to over-
sample the minority class, resulting in approximately 900 samples for each class.
The performance of diﬀerent methods, in a 10-fold cross-fold validation, are

shown in Table 2.

Table 2. Averages (and standard deviations) of accuracy, κ, sensitivity, and speciﬁcity
using 10-fold cross-validation. The κ statistic is an unweighted measure of concordance
for categorical data that measures agreement relative to what would be expected by
chance [Kuhn, 2008]. Legend: all keywords = all 6000 terms, top keywords = top
200 keywords, topics = learned 20 topics, NB = na¨ıve Bayes, SVM = support vector
machine.

accuracy

kappa

sensitivity

speciﬁcity

00.24% (00.63%) 10.45% (00.60%) 96.87% (08.83%)
NB, all keywords
08.70%(04.66%) 14.74% (02.54%) 93.76% (02.12%)
NB, top keywords
39.82%(05.22%) 99.67% (01.02%) 63.40%(01.96%)
SVM, all keywords
41.95%(15.29%) 93.36%(19.87%) 66.27%(12.05%)
SVM, top keywords
32.86%(05.57%) 100.00% (0.00%) 60.86% (02.04%)
SVM, all keywords + topics
SVM, topics
57.75%(08.64%) 74.64% (03.91%) 84.27%(05.56%)
SVM, top keywords + topics 90.82% (13.46%) 81.44%(27.53%) 93.46% (07.92%) 90.07%(13.46%)

12.06% (01.65%)
55.81% (04.15%)
70.36% (02.53%)
69.18% (02.79%)
66.98% (02.70%)
78.81%(04.33%)

4 Conclusion

Textual analysis of Yelp reviews can clearly be used to predict oﬃcial health
inspection results, as evidenced by over 90% accuracy in predicting oﬃcial WPH
actions using learned keywords and topic model features in an SVM classiﬁer.
In both the NB and SVM approaches, reducing the total number of keywords
to an optimized subset provides a signiﬁcant improvement in performance, most
likely to possible overﬁtting in the full set.

Given that both sensitivity and sensitivity are very high in our optimum
system, it is strongly suggested that this tool, or one very much like it, can be
used by governments to optimize their resources. Speciﬁcally, inspection sched-
ules should be automatically generated from crowd-sourced data, as in this work,
and standard regulated procedures. The derivation of such schedules is the sub-
ject of future work. Other possible avenues of exploration include interactive
online databases combining oﬃcial health inspector reports and crowd-sourced
information. Artiﬁcial intelligence is helping to shift public health informatics to
the public sphere, and it will be increasingly important for that shift to incor-
porate new approaches to machine learning.

Bibliography

Aramaki, E., Maskawa, S., and Morita, M. (2011). Twitter catches the ﬂu:
detecting inﬂuenza epidemics using Twitter. In Proceedings of the conference
on empirical methods in natural language processing (EMNLP’11), Edinburgh,
United Kingdom.

Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent Dirichlet allocation.

Journal of Machine Learning Research, 3:993–1022.

Chawla, N. V., Bowyer, K. W., Hall, L. O., and Kegelmeyer, W. P. (2002).
SMOTE: Synthetic minority over-sampling technique. Journal of Artiﬁcial
Intelligence Research, 16:321–357.

Dredze, M., Paul, M. J., Bergsma, S., and Tran, H. (2013). Carmen: A Twitter
geolocation system with applications to public health. In AAAI Workshop on
Expanding the Boundaries of Health Informatics Using AI (HIAI’13).

Gruber, A., Rosen-Zvi, M., and Weiss, Y. (2007). Hidden topic Markov models.
In Artiﬁcial Intelligence and Statistics (AISTATS’07), San Juan, Puerto Rico.
Kang, J. S., Kuznetsova, P., Luca, M., and Choi, Y. (2013). Where not to
eat? Improving public policy by predicting hygiene inspections using online
reviews. In Proceedings of the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP’13, Seattle, Washington, USA.

Kuhn, M. (2008). Building predictive models in R using the caret package.

Journal of Statistical Software, 28(5):1–26.

Lamb, A., Paul, M. J., and Dredze, M. (2013). Separating fact from fear: Track-
ing ﬂu infections on Twitter. In Proceedings of Human Language Technologies:
Conference of the North American Chapter of the Association of Computa-
tional Linguistics (HLT-NAACL’13), Atlanta, Georgia, USA.

Nsoesie, E. O., Kluberg, S. A., and Brownstein, J. S. (2014). Online reports
of foodborne illness capture foods implicated in oﬃcial foodborne outbreak
reports. Preventive medicine, 67:264–269.

Sadilek, A., Brennan, S., Kautz, H., and Silenzio, V. (2013). nEmesis: Which
restaurants should you avoid today? In Proceedings of the First AAAI Confer-
ence on Human Computation and Crowdsourcing, HCOMP’13, Palm Springs,
CA, USA.

Sadilek, A., Kautz, H. A., and Silenzio, V. (2012). Modeling spread of disease
from social interactions. In Proceedings of the Sixth International Conference
on Weblogs and Social Media (ICWSM’12), Dublin, Ireland.

Sahami, M., Dumais, S., Heckerman, D., and Horvitz, E. (1998). A Bayesian
approach to ﬁltering junk e-mail. In Learning for Text Categorization: Papers
from the 1998 workshop, volume 62, pages 98–105.

Von Etter, P., Huttunen, S., Vihavainen, A., Vuorinen, M., and Yangarber, R.
(2010). Assessment of utility in web mining for the domain of public health.
In Proceedings of Human Language Technologies: Conference of the North
American Chapter of the Association of Computational Linguistics (HLT-
NAACL’10), Los Angeles, CA, USA.

