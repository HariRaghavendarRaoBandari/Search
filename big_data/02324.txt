Constant Approximation for Capacitated k-Median with

(1 + )-Capacity Violation

G¨okalp Demirci∗

Shi Li†

Abstract

We study the Capacitated k-Median problem for which existing constant-factor approxima-
tion algorithms are all pseudo-approximations that violate either the capacities or the upper
bound k on the number of open facilities. Using the natural LP relaxation for the problem, one
can only hope to get the violation factor down to 2. Li [SODA’16] introduced a novel LP to go
beyond the limit of 2 and gave a constant-factor approximation algorithm that opens (1 + )k
facilities.

We use the conﬁguration LP of Li [SODA’16] to give a constant-factor approximation for
the Capacitated k-Median problem in a seemingly harder conﬁguration: we violate only the
capacities by 1 + . This result settles the problem as far as pseudo-approximation algorithms
are concerned.

6
1
0
2

 
r
a

M
7

 

 
 
]
S
D
.
s
c
[
 
 

1
v
4
2
3
2
0

.

3
0
6
1
:
v
i
X
r
a

∗
†

Department of Computer Science, University of Chicago, demirci@cs.uchicago.edu
Department of Computer Science and Engineering, University at Buﬀalo, shil@buffalo.edu

1

1

Introduction

In the capacitated k-median problem (CKM), we are given a set F of facilities together with their
capacities ui ∈ Z>0 for i ∈ F , a set C of clients, a metric d on F ∪ C, and a number k. We are
asked to open some of these facilities F (cid:48) ⊆ F and give an assignment σ : C → F (cid:48) connecting each
client to one of the open facilities so that the number of open facilities is not bigger than k, i.e.
|F (cid:48)| ≤ k (cardinality constraint), and each facility i ∈ F (cid:48) is connected to at most ui clients, i.e.

(cid:12)(cid:12)σ−1(i)(cid:12)(cid:12) ≤ ui (capacity constraint). The goal is to minimize the sum of the connection costs, i.e.
(cid:80)

j∈C d(σ(j), j).
Without the capacity constraint, i.e. ui = ∞ for all i ∈ F , this is the famous k-median problem
(KM). The ﬁrst constant-factor approximation algorithm for KM is given by Charikar et al. [9],
guaranteeing a solution within 6 2
3 times the cost of the optimal solution. Then the approximation
ratio has been improved by a series of papers [13, 8, 3, 12, 17, 5]. The current best ratio for KM is
2.675 +  due to Byrka et al. [5], which was obtained by improving a part of the algorithm given
by Li and Svensson [17].

On the other hand, we don’t have a true constant approximation for CKM. All known constant-
factor results are pseudo-approximations which violate either the cardinality or the capacity con-
straint. Aardal et al. [1] gave an algorithm which ﬁnds a (7 + )-approximate solution to CKM
by opening at most 2k facilities, i.e. violating the cardinality constraint by a factor of 2. Guha
[11] gave an algorithm with approximation ratio 16 for the more relaxed uniform CKM, where
all capacities are the same, by connecting at most 4u clients to each facility, thus violating the
capacity constraint by 4. Li [14] gave a constant-factor algorithm for uniform CKM with capacity
violation of only 2 +  by improving the algorithm in [9]. For non-uniform capacities, Chuzhoy and
Rabani [10] gave a 40-approximation for CKM by violating the capacities by a factor of 50 using a
mixture of primal-dual schema and lagrangian relaxations. Their algorithm is for a slightly relaxed
version of the problem called soft CKM where one is allowed to open multiple collocated copies of
a facility in F . CKM deﬁnition we gave above is sometimes referred to as hard CKM as opposed to
this version. Recently, Byrka et al. [4] gave a constant-factor algorithm for hard CKM by keeping
capacity violation factor under 3 + .

All these algorithms for CKM use the basic LP relaxation for the problem which is known to
have an unbounded integrality gap even when we are allowed to violate either the capacity or the
cardinality constraint by 2 − . In this sense, results of [1] and [14] can be considered as reaching
the limits of the basic LP relaxation in terms of restricting the violation factor. In order to go
beyond these limits, Li [15] introduced a novel LP called rectangle LP and presented a constant-
factor approximation algorithm for soft uniform CKM by opening (1 + )k facilities. This was later
generalized by the same author to non-uniform CKM [16], where he introduced an even stronger LP
relaxation called conﬁguration LP. Very recently, independently of the work in this paper, Byrka
et al. [6] used this conﬁguration LP to give a similar algorithm for uniform CKM violating the
capacities by 1 + .

1.1 Our Result

In this paper, we use the conﬁguration LP of [16] to give an O(1/5)-approximation algorithm for
non-uniform hard CKM which respects the cardinality constraint and connects at most (1 + )ui
clients to any open facility i ∈ F . The running time of our algorithm is nO(1/). Thus, with
this result, we now have settled the CKM problem from the view of pseudo-approximation algo-

2

rithms: either (1 + )-cardinality violation or (1 + )-capacity violation is suﬃcient for a constant
approximation for CKM.

For facility location problems, in general, global constraints in the problems are harder to satisfy
than local constraints and, for CKM, the upper bound on the total number of open facilities is more
of a global constraint than satisfying the capacity limit of a facility. Analogously, the known results
for the CKM problem have suggested that designing algorithms with capacity violation (satisfying
the cardinality constraint) is harder than designing algorithms with cardinality violation. Note, for
example, that the best known cardinality violation factor for non-uniform CKM among algorithms
using only the basic LP relaxation (a factor of 2 in [1]) matches the smallest possible cardinality
violation factor dictated by the gap instance. In contrast, the best capacity-violation factor is 3 + 
due to [4], but the gap instance for the basic LP with the largest known gap eliminates only the
algorithms with capacity violation smaller than 2.

Furthermore, we can argue that, for algorithms based on the basic LP and the conﬁguration
LP, a β-capacity violation can be converted to a β-cardinality violation, suggesting that allowing
capacity violation is more restrictive compared to allowing cardinality violation. Suppose we have
an α-approximation algorithm for CKM that violates the capacity constraint by a factor of β, based
on the basic LP relaxation. Given a solution (x, y) to the basic LP for a given CKM instance I,
we construct a new instance I(cid:48) by scaling k by a factor of β and scaling all capacities by a factor
of 1/β (in a valid solution, we allow connections to be fractional, thus fractional capacities do not
cause issues). Then it is easy to see that (x, βy) is a valid LP solution to I(cid:48) (with soft capacities).
A solution to I(cid:48) that only violates the capacity constraint by a factor of β is a solution to I that
only violates the cardinality constraint by a factor of β. Thus, by considering the new instance, we
conclude that for algorithms based on the basic LP relaxation, violating the cardinality constraint
gives more power. The same argument can be made for algorithms based on the conﬁguration
LP: one can show that a valid solution to the conﬁguration LP for I yields a valid solution to
the conﬁguration LP for I(cid:48). However, this reduction in the other direction does not work: due to
constraint (3), scaling y variables by a factor of 1/β does not yield a valid LP solution.

Our algorithm uses the framework of [16] that creates a two-level clustering of facilities. The
second-level clustering gives a set of “black components”. The algorithm in [16] then handles
each black component separately and the analysis works in a component-by-component fashion.
However, our algorithm needs one more level of clustering: we further merge black components to
form so-called “groups”. For each group, we ﬁnd a distribution over partial solutions restricted
to this group; then we choose partial solutions from these distributions dependently. The total
number of open facilities in these partial solutions may be larger than k. Our algorithm will then
shut down some facilities and we argue that the incurred cost is not very big.

2 The Basic LP and the Conﬁguration LP

In this section, we give the conﬁguration LP of [16] for CKM. We start with the following basic LP
relaxation:

min

i∈F,j∈C d(i, j)xi,j

s.t.

(Basic LP)

(cid:80)

3

(cid:80)
(cid:80)
i∈F yi ≤ k;
i∈F xi,j = 1,
xi,j ≤ yi,

∀j ∈ C;
∀i ∈ F, j ∈ C;

(1)

(2)

(3)

(cid:80)
j∈C xi,j ≤ uiyi,

0 ≤ xi,j, yi ≤ 1,

∀i ∈ F ;
∀i ∈ F, j ∈ C.

(4)

(5)

where yi indicates whether a facility i ∈ F is open, and xi,j indicates whether client j ∈ C is
connected to facility i ∈ F . Constraint (1) is the cardinality constraint assuring that the number
of open facilities is no more than k. Constraint (2) says that every client must be fully connected
to facilities. Constraint (3) requires a facility to be open in order to connect clients. Constraint (4)
is the capacity constraint.
It is well known that the basic LP has unbounded integrality gap, even if we are allowed to
violate the cardinality constraint or the capacity constraint by a factor of 2− . In the gap instance
for the capacity-violation setting, each facility has capacity u, k is 2u − 1, and the metric consists
of u isolated groups each of which has 2 facilities and 2u− 1 clients that are all collocated. In other
words, the distances within a group are all 0 but the distances between groups are nonzero. Any
integral solution for this instance has to have a group with at most one open facility. Therefore,
even with (2 − 2/u)-capacity-violation, we have to connect 1 client in this group to open facilities
in other groups. On the other hand, a fractional solution to the basic LP relaxation opens 2 − 1/u
facilities in each group and serves the demand of each group using only the facilities in that group.
Note that the gap instance disappears if we allow a capacity violation of 2.1

In order to overcome the gap in the cardinality-violation setting, Li [16] introduced a novel
S = {S ⊆ B : |S| ≤ (cid:96)1} and (cid:101)S = S ∪{⊥}, where ⊥ stands for “any subset of B with size more than
LP for CKM called the conﬁguration LP. We formally state the conﬁguration LP below. Let us
ﬁx a set B ⊆ F of facilities. Let (cid:96) = Θ(1/) and (cid:96)1 = Θ((cid:96)) be a suﬃciently large integers. Let
(cid:96)1”; for convenience, we also treat ⊥ as a set such that i ∈ ⊥ holds for every i ∈ B. For S ∈ S,
let zB
S indicate the event that the set of open facilities in B is exactly S and zB⊥ indicate the event
For every S ∈ (cid:101)S and i ∈ S, zB
that the number of open facilities in B is more than (cid:96)1.
S = 1 and i is open. (If i ∈ B but
keep both variables for notational purposes. For every S ∈ (cid:101)S, i ∈ S and client j ∈ C, zB
i /∈ S, then the event will not happen.) Notice that when i ∈ S (cid:54)= ⊥, we always have zB
S,i = zB
S ; we
S,i,j indicates
the event that zB
S,i = 1 and j is connected to i. In an integral solution, all the above variables are
{0, 1} variables. The following constraints are valid. To help understand the constraints, it is good
to think of zB

S,i indicates the event that zB

S,i as zB

S · yi and zB

S,i,j as zB

S · xi,j.

1A similar instance can be given to show that the gap is still unbounded when the cardinality constraint is violated,

instead of the capacity constraint, by less than 2: let k = u + 1 and each group have 2 facilities and u + 1 clients.

4

zB
S = 1;

zB
S,i = yi,

∀i ∈ B;

(6)

(7)

(cid:88)
(cid:88)
(cid:88)

j∈C

i∈S

zB
S,i = zB
S ,
S,i,j ≤ zB
zB
S ,
S,i,j ≤ uizB
zB
S,i,
zB⊥,i ≥ (cid:96)1zB⊥ .

∀S ∈ S, i ∈ S;

∀S ∈ (cid:101)S, j ∈ C;
∀S ∈ (cid:101)S, i ∈ S;

(10)

(11)

(12)

(cid:88)
(cid:88)
S∈(cid:101)S
(cid:88)
S∈(cid:101)S:i∈S
S∈(cid:101)S:i∈S
S,i,j ≤ zB

zB
S,i,j = xi,j,

∀i ∈ B, j ∈ C;

(8)

0 ≤ zB

S,i ≤ zB
S ,

∀S ∈ (cid:101)S, i ∈ S, j ∈ C; (9)
S = 1 for exactly one S ∈ (cid:101)S. Constraint (7) says that if i is open

i∈B

(13)

Constraint (6) says that zB

then there is exactly one S ∈ (cid:101)S such that zB
i then there is exactly one S ∈ (cid:101)S such that zB

S,i = 1. Constraint (8) says that if j is connected to
S,i,j = 1. Constraint (9) is by the deﬁnition of the
variables. Constraint (10) holds as we mentioned earlier. Constraint (11) says that if zB
S = 1 then
j can be connected to at most 1 facility in S. Constraint (12) is the capacity constraint. Finally,
Constraint (13) says that if zB⊥ = 1, then at least (cid:96)1 facilities in B are open.
The conﬁguration LP is obtained from the basic LP by adding the z variables and Constraints (6)
to (13) for every B ⊆ F . Since there are exponentially many subsets B ⊆ F , we don’t know how to
solve this LP eﬃciently. However, note that there are only polynomially many (nO((cid:96)1)) zB variables
for a ﬁxed B ⊆ F . Given a fractional solution (x, y) to the basic LP relaxation, we can construct the
values of zB variables and check their feasibility for Constraints (6) to (13) in polynomial time as
in [16]. Our rounding algorithm either constructs an integral solution with the desired properties,
or outputs a set B ⊆ F such that Constraints (6) to (13) are infeasible. In the latter case, we can
ﬁnd a constraint in the conﬁguration LP that (x, y) does not satisfy. Then we can run the ellipsoid
method and the rounding algorithm in an iterative way (see, e.g., [7, 2]).

3 Representatives, Black Components, and Groups

In this section, we deﬁne some basic notations and use standard techniques to bundle facilities
into big groups. Let ({xi,j : i ∈ F, j ∈ C} ,{yi : i ∈ F}) be a fractional solution to the basic LP.
i∈F xi,jd(i, j) to be the connection cost of j, for every client j ∈ C. Let
i∈S Di for every S ⊆ F . We

We deﬁne dav(j) := (cid:80)
Di := (cid:80)
denote the value of the solution (x, y) by LP :=(cid:80)
DF = (cid:80)
any set F (cid:48) ⊆ F of facilities and any set C(cid:48) ⊆ C of clients, we shall let xF (cid:48),C(cid:48) := (cid:80)
we simply write xi,C(cid:48) for x{i},C(cid:48) and xF (cid:48),j for xF (cid:48),{j}. For any F (cid:48) ⊆ F , let yF (cid:48) := (cid:80)

j∈C xi,j (d(i, j) + dav(j)) for every i ∈ F , and DS := (cid:80)
i∈F,j∈C xi,jd(i, j) +(cid:80)
i∈F,j∈C xi,j (d(i, j) + dav(j)) = (cid:80)

i∈F xi,j = 2LP. For
i∈F (cid:48),j∈C(cid:48) xi,j;
i∈F (cid:48) yi. Let
d(A, B) := mini∈A,j∈B d(i, j) denote the minimum distance between A and B, for any A, B ⊆ F ∪C;
we simply use d(i, B) for d({i} , B). Recall that (cid:96) = Θ(1/).

i∈F,j∈C xi,jd(i, j)(cid:0) =(cid:80)
j∈C dav(j)(cid:80)

j∈C dav(j)(cid:1). Note that

After the set of open facilities is decided, the optimum connection assignment from clients to
facilities can be computed by solving the minimum cost b-matching problem. Due to the integrality
of the matching polytope, we may allow the connections to be fractional. That is, if there is a
good fractional assignment, then there is a good integral assignment. So we can use the following
Initially there is one unit of demand at each client j ∈ C. During our algorithm,
framework.

5

we move demands fractionally inside F ∪ C; moving α units of demand from i to j incurs a cost
of αd(i, j). At the end, all the demands are moved to F and each facility i ∈ F has at most
(1 + O( 1
(cid:96) ))ui units of demand. Our goal is to bound the total moving cost and the number of open
facilities.

Our algorithm starts with bundling facilities together with a three-phase process each of which
creates bigger and bigger groups. At the end, we have a nicely formed network of suﬃciently big
clusters of facilities. Proofs of lemmas in this section can be found in Appendix A. See Figure 1 at
the end of this section for an illustration of the three-level clustering.

3.1 Representatives and Initial Moving of Demands

We ﬁrst use a standard approach to facility location problems ([18, 19, 9, 16]) to partition the
facilities into clusters such that each cluster has a total oppening that is not too small. Each cluster
has an associated center from the set of clients. We call the set of centers client representatives
and denote it by R ⊆ C. Any two representatives are suﬃciently far away from each other.
Let the set of representatives be R = ∅ initially. Repeat the following process until C becomes
empty: we select the client v ∈ C with the smallest dav(v) and add it to R; then we remove all
clients j such that d(j, v) ≤ 4dav(j) from C (thus, v itself is removed). We use v and its variants
to index representatives, and j and its variants to index general clients.
We partition the set F of locations according to their nearest representatives in R. Let Uv = ∅
for every v ∈ R initially. For each location i ∈ F , we add i to Uv for v ∈ R that is closest to i.
Thus, {Uv : v ∈ R} forms a Voronoi diagram of F with R being the centers. For any subset V ⊆ R
v∈V Uv to denote the union of Voronoi regions with centers V .

of representatives, we use U (V ) :=(cid:83)

Lemma 3.1. The following statements hold:
(3.1a) for all v, v(cid:48) ∈ R, v (cid:54)= v(cid:48), we have d(v, v(cid:48)) > 4 max{dav(v), dav(v(cid:48))}
(3.1b) for all j ∈ C, there exists v ∈ R, such that dav(v) ≤ dav(j) and d(v, j) ≤ 4dav(j);
(3.1c) yUv ≥ 1/2 for every v ∈ R;
(3.1d) for any v ∈ R, i ∈ Uv, and j ∈ C, we have d(i, v) ≤ d(i, j) + 4dav(j).

Next lemma shows that moving demands from facilities to their corresponding representative

doesn’t cost much.

Lemma 3.2. For every v ∈ R, we have(cid:80)
Corollary 3.3. (cid:80)

v∈R,i∈Uv

xi,Cd(i, v) ≤ O(1)LP.

Since {Uv : v ∈ R} forms a partition of F , we get the following corollary.

xi,Cd(i, v) ≤ O(1)DUv .

i∈Uv

With this corollary, we can describe our initial moving of demands. First for every j ∈ C and
i ∈ F , we move xi,j units of demand from j to i. The moving cost of this step is exactly LP. After
this step, all the demand is at the facilities and every facility i has xi,C units of demand. Then,
for every v ∈ R and i ∈ Uv, we move the xi,C units of demand at i to V . The moving cost for this
step is O(1)LP. Thus, after the initial moving, all demands are at the set R of representatives: a
representative v has xUv,C units of demand.

6

3.2 Black Components

Next, we employ minimum spanning tree construction of [16] to form bigger components (called
black components) by combining representatives. The construction produces a forest of rooted
trees over the black components. The forest in [16] can have arbitrary degrees, while our algorithm
requires the forest to have degree 2: every component has at most two child components. We
extend construction in [16] to satisfy this requirement.

First, we run the classic Kruskal’s algorithm to ﬁnd the minimum spanning tree MST of the
metric (R, d), and then color the edges in MST in black, grey or white. In Kruskal’s algorithm, we
maintain the set EMST of edges added to MST so far and a partition P of R. Initially, we have

EMST = ∅ and P = {{v} : v ∈ R}. The length of an edge e ∈ (cid:0)R
two endpoints of e. We sort all edges in(cid:0)R

(cid:1) is the distance between the
(cid:1) in the ascending order of their lengths, breaking ties

2

2

arbitrarily. For each pair (v, v(cid:48)) in this order, if v and v(cid:48) are not in the same partition in P, we add
the edge (v, v(cid:48)) to EMST and merge the two partitions containing v and v(cid:48) respectively.
We now color the edges in EMST. For every v ∈ R, we say the weight of v is yUv ; so every repre-
sentative v ∈ R has weight at least 1/2 by Property (3.1c). For a subset J ⊆ R of representatives,
we say J is big if the weight of J is at least (cid:96), i.e, yU (J) ≥ (cid:96); we say J is small otherwise. For any
edge e = (v, v(cid:48)) ∈ EMST, we consider the iteration in Kruskal’s algorithm in which the edge e is
added to MST. After the iteration we merged the partition Jv containing v and the partition Jv(cid:48)
containing v(cid:48) into a new partition Jv ∪ Jv(cid:48). If both Jv and Jv(cid:48) are small, then we call e a black edge.
If Jv is small and Jv(cid:48) is big, we call e a grey edge, directed from v to v(cid:48); similarly, if Jv(cid:48) is small
and Jv is big, e is a grey edge directed from v(cid:48) to v. If both Jv and Jv(cid:48) are big, we say e is a white
edge. So, we treat black and white edges as undirected edges and grey edges as directed edges.
We deﬁne a black component of MST to be a maximal set of vertices connected by black edges.
Let J be the set of all black components. Thus J indeed forms a partition of R. We contract
all the black edges in MST and remove all the white edges. The resulting graph is a forest ΥJ of
trees over black components in J . Each edge is a directed grey edge. For every component J ∈ J ,
we deﬁne L(J) := d(J, R \ J) to be the shortest distance between any representative in J and any
representative not in J.

A component J in the forest ΥJ may have many child-components. To make it binary, we
use the left-child-right-sibling binary-tree representation of trees. To be more speciﬁc, for every
component J(cid:48), we sort all its child-components J according to non-decreasing order of L(J). We add
a directed edge from the ﬁrst child to J(cid:48) and a directed edge between every two adjacent children
in the ordering, from the child appearing later in the ordering to the child appearing earlier. Let
J be the new forest. Υ∗
Υ∗
J naturally deﬁnes a new child-parent relationship between components.
Lemma 3.4. J and Υ∗
J satisfy the following properties:
(3.4a) for every J ∈ J , there is a spanning tree over the representatives in J such that for every

edge (v, v(cid:48)) in the spanning tree we have d(v, v(cid:48)) ≤ L(J);

yU (J) < (cid:96);

(3.4b) every root component J ∈ J has yU (J) ≥ (cid:96) and every non-root component J ∈ J has
(3.4c) every root component J ∈ J has either yU (J) < 2(cid:96) or |J| = 1;
(3.4d) for any non-root component J and its parent J(cid:48) we have L(J) ≥ L(J(cid:48));
(3.4e) for any non-root component J and its parent J(cid:48), we have d(J, J(cid:48)) ≤ O((cid:96))L(J);
(3.4f) every component J has at most two children.

7

(3.5b) if G ∈ G is not a root group, then(cid:80)
(3.5c) if G ∈ G is a non-leaf group, then(cid:80)

any representative in(cid:83)

J∈G yU (J) < 2(cid:96);
J∈G yU (J) ≥ (cid:96);

J(cid:48)∈G(cid:48) J(cid:48) is at most O((cid:96)2)L(J),

component of T . While(cid:80)

3.3 Groups
We will further group black components in J together similar to [4, 6] so that each group has total
weight bigger than or equal to (cid:96), unless the group is a leaf-group.
The algorithm for constructing groups is straightforward. For each rooted tree T = (JT , ET )
in Υ∗
J , we construct a group G of black components as follows. Initially, let G contain the root
J∈G yU (J) < (cid:96) and G (cid:54)= JT , repeat the following procedure. Choose the
component J ∈ JT \ G that is adjacent to G in T , with the smallest L-value. We add J to G.
Thus, by the construction G is connected in T . After we have constructed the group G, we add
G to G. We remove all black components in G from T . Then, each T is broken into many rooted
trees; we apply the above procedure recursively for each rooted tree.
So, we have constructed a partition G for the set J of components. If for every G ∈ G, we
J over J becomes a rooted
contract all components in G into a single node, then the rooted forest Υ∗
forest ΥG over the set G of groups. ΥG naturally deﬁnes a parent-child relationship over G. The
following lemma uses Properties (3.4a) to (3.4f) of J and the way we construct G.
Lemma 3.5. The following statements hold for the set G of groups and the parent-child relationship
deﬁned by ΥG over G:
(3.5a) any root group G ∈ G contains a single root component J ∈ J ;

(3.5d) let G ∈ G, G(cid:48) ∈ G be the parent of G, J ∈ G and v ∈ J, then the distance between v and

(3.5e) any group G has at most O((cid:96)) children.

Figure 1: On the left: An initial tree (in ΥJ ) of black components of arbitrary degrees joined
together by grey edges. Numbers on the edges are the edge lengths. In the middle: Final binary
tree (in Υ∗
J ) of black components and groupings. On the right: Internal structure of a black
component.

8

162569347:blackcomponent:greyedges:grouping:representative:blackedges:facility4 Constructing Distributions for Black Components

In this section, we shall construct a distribution over pairs (cid:0)S ⊆ U (V ), β ∈ RU (V )≥0

V ⊆ R which is the union of some black components. S is the facilities we shall open in U (V )
and βi, i ∈ U (V ) is the amount of demand that will be satisﬁed by i. We distinguish between
concentrated black components and non-concentrated black components. We shall construct a
distribution for each concentrated component J; to do this, we require Constraints (6) to (13) to
be feasible for B = U (J). We may construct a distribution for the union of many non-concentrated
components; indeed, this distribution contains a single pair (S, β) in the support.

(cid:1) for a given

The deﬁnition of concentrated black component is the same as that of [16], except that we

choose the parameter (cid:96)2 diﬀerently.

Deﬁnition 4.1. Deﬁne πJ =(cid:80)

j∈C xU (J),j(1− xU (J),j), for every black component J ∈ J . A black
component J ∈ J is said to be concentrated if πJ ≤ xU (J),C/(cid:96)2, and non-concentrated otherwise,
where (cid:96)2 = Θ((cid:96)3) is large enough.

We shall use J C to denote the set of concentrated black components and J N to denote the set
of non-concentrated black components. The next lemma from [16] shows the importance of πJ . For
the completeness of the paper, we give its proof in Appendix B.
Lemma 4.2. For any J ∈ J , we have L(J)πJ ≤ O(1)DU (J).

Recall that L(J) = d(J, R \ J) and xU (J),C is the total demand in J after the initial moving.
Thus, according to Lemma 4.2, if J is not concentrated, we can use DU (J) to charge the cost for
moving all the xU (J),C units of demand out of J, provided that the moving distance is not too
big compared to L(J). This gives us freedom for handling non-concentrated components. If J is
concentrated, the amount of demand that is moved out of J must be comparable to πJ ; we shall
use the conﬁguration LP to guarantee this.
In Section 4.1, we construct distributions for components in J C, while in Section 4.2, we con-
struct (S, β) pairs for components in J N. In order to describe the properties of the constructions,
we deﬁne the earth-mover-distance.
Deﬁnition 4.3 (Earth Mover Distance). Given a set V ⊆ R with B = U (V ), a demand vector
i∈B βi, the earth mover distance
v∈V,i∈B f (v, i)d(v, i), where f is over all functions

v∈V αv ≤ (cid:80)

α ∈ RV≥0 and a supply vector β ∈ RB≥0 such that (cid:80)
(cid:80)
• (cid:80)
• (cid:80)

i∈B f (v, i) = αv for every v ∈ V ;
v∈V f (v, i) ≤ βi for every i ∈ B.

from α to β is deﬁned as EMDV (α, β) := inf f
from V × B to R≥0 such that

Notice that in the deﬁnition we need to satisfy all demands; however some fraction of a supply
may be unmatched. From now on, we shall use αv = xUv,C to denote the amount of demand at v
after the initial moving. For any set V ⊆ R of representatives, we use α|V to denote the vector α
restricted to the coordinates in V .

9

4.1 Distributions for Components in J C
In this section, we construct distributions for components in J C, by proving:
Theorem 4.4. Let J ∈ J C and let B = U (J). Assume Constraints (6) to (13) are satisﬁed for
B. Then, we can ﬁnd a distribution (φS,β)S⊆B,β∈RB≥0
(4.4a) sφ := E
and for every (S, β) in the support of φ, we have
(4.4b) |S| ∈ {(cid:98)sφ(cid:99) ,(cid:100)sφ(cid:101)};
(4.4c) βi ≤ (1 + O(1/(cid:96)))ui if i ∈ S and βi = 0 if i ∈ B \ S;

of pairs (S, β), such that
(S,β)∼φ |S| ∈ [yB, yB(1 + 2(cid:96)πJ /xB,C)], and sφ = yB if yB > 2(cid:96),

(4.4d) (cid:80)

i∈S βi = xB,C =(cid:80)

v∈J αv.
Moreover, the distribution φ satisﬁes
(4.4e) the support of φ has size at most nO((cid:96));
(4.4f) E

(S,β)∼φ EMDJ (α|J , β) ≤ O((cid:96)4)DB.

To prove the theorem, we ﬁrst construct a distribution ψ that satisﬁes most of the properties;
then we modify it to obtain the ﬁnal distribution φ. Notice that a typical black component J has
yB ≤ 2(cid:96); however, when J is a root component containing a single representative, yB might be very
large. For now, let us just assume yB ≤ 2(cid:96). We deal with the case where |J| = 1 and yB > 2(cid:96) at
the end of this section.
Since Constraints (6) to (13) are satisﬁed for B, we can use the zB variables satisfying these
constraints to construct a distribution ζ over pairs (χ ∈ [0, 1]B×C, µ ∈ [0, 1]B), where µ indicates
Let S = {S ⊆ B : |S| ≤ (cid:96)1} and (cid:101)S = S ∪ {⊥} as in Section 2. For simplicity, for any µ ∈ [0, 1]B,
we shall use µB to denote(cid:80)
the set of open facilities in B and χ indicates how the clients in J are connected to facilities in B.
denote(cid:80)
j∈C χi,j, χB,j to denote(cid:80)
i∈B µi. For any χ ∈ [0, 1]B×C, i ∈ B and j ∈ C, we shall use χi,C to
[0, 1]B. For each S ∈ (cid:101)S such that zB
The distribution ζ is deﬁned as follows. Initially, let ζχ,µ = 0 for all χ ∈ [0, 1]B×C and µ ∈
S for the χ, µ satisfying χi,j =
S for every i ∈ B, j ∈ C. So, for every pair (χ, µ) in the support of ζ, we have
(cid:80)
zB
S,i,j/zB
χi,j ≤ µi, χi,C ≤ uiµi for every i ∈ B, j ∈ C. Moreover, either µ is integral, or µB ≥ (cid:96)1. Since
S∈(cid:101)S zB
(χ,µ)∼ζ χi,j = xi,j for
every i ∈ B, j ∈ C and E

S , µi = zB
S = 1, ζ is a distribution over pairs (χ, µ). It is not hard to see that E

i∈B χi,j, and χB,C to denote(cid:80)

(χ,µ)∼ζ µi = yi for every i ∈ B. The support of ζ has nO((cid:96)) size.

i∈B χi,C =(cid:80)

S > 0, increase ζχ,µ by zB

j∈C χB,j.

S,i/zB

Deﬁnition 4.5. We say a pair (χ, µ) is good if
(4.5a) µB ≤ yB/(1 − 1/(cid:96));
(4.5b) χB,C ≥ (1 − 1/(cid:96))xB,C.

We are only interested in good pairs in the support of ζ. We show that the total probabil-
ity of good pairs in the distribution ζ is large. Let Ξa denote the set of pairs (χ, µ) satisfying
Property (4.5a) and Ξb denote the set of pairs (χ, µ) satisfying Property (4.5b). Notice that
ζχ,µ ≥ 1/(cid:96). The proof of the following
E
lemma uses elementary mathematical tools, and is deferred to Appendix B.

(χ,µ)∼ζ µB = yB. By Markov inequality, we have(cid:80)
Lemma 4.6. (cid:80)

ζχ,µ ≤ (cid:96)πJ /xB,C.

(χ,µ)∈Ξa

(χ,µ) /∈Ξb

10

Overall, we have Q :=(cid:80)

(χ,µ) good ζχ,µ =(cid:80)

(χ,µ)∈Ξa∩Ξb

ζχ,µ ≥ 1/(cid:96) − (cid:96)πJ /xB,C ≥ 1/(cid:96) − 1/(2(cid:96)) =
1/(2(cid:96)), where the second inequality used the fact that πJ ≤ xB,C/(2(cid:96)2) for J ∈ J C.
Now focus on each good pair (χ, µ) in the support of ζ. Since J ∈ J C and (χ, µ) ∈ Ξa,
we have µB ≤ yB/(1 − 1/(cid:96)) ≤ 2(cid:96)/(1 − 1/(cid:96)) < (cid:96)1 (since we assumed yB ≤ 2(cid:96)), if (cid:96)1 is large
enough. So, µ ∈ {0, 1}B. Then, let S = {i ∈ B : µi = 1} be the set indicated by µ, and
(cid:80)
βi = χi,C/(1 − 1/(cid:96)) for every i ∈ B. For this (S, β), Property (4.4c) is satisﬁed, and we have
i∈B βi = χB,C/(1− 1/(cid:96)) ≥ xB,C. We then set ψS,β = ζχ,µ/Q. Thus, ψ indeed forms a distribution
over pairs (S, β). Moreover, the support of ζ has size nO((cid:96)), so does the support of ψ. Thus
we have(cid:80)
Property (4.4e) holds.

(cid:3) ≤ yB/(1 − (cid:96)πj/xB,C). Since
(cid:12)(cid:12)(χ, µ) ∈
(cid:3) can only be smaller. Thus, we have that sψ ≤ yB/(1 − (cid:96)πJ /xB,C) ≤ yB(1 + 2(cid:96)πJ /xB,C).
(cid:2) ·(cid:12)(cid:12)(χ, µ) good(cid:3), and a = 1/(1 − 1/(cid:96)) to denote the

the condition (χ, µ) ∈ Ξa requires µB to be upper bounded by some threshold, E
Ξb ∩ Ξa
For simplicity, we use ˆE[·] to denote E
scaling factor we used to deﬁne β. Indeed, we shall lose a factor O((cid:96)2) later and thus we shall prove
Property (4.4f) for ψ with the O((cid:96)2) term on the right:
Lemma 4.7. ˆE[EMDJ (α|J , β)] ≤ O((cid:96)2)DB, where β depends on χ as follows: βi = aχi,C for every
i ∈ B.

(cid:12)(cid:12)(χ, µ) good(cid:3). Notice that E
(cid:12)(cid:12)(χ, µ) ∈ Ξb

The proof of Property (4.4f) for ψ is long and tedious, thus we leave it to the Appendix B.

ζχ,µ ≤ (cid:96)πJ /xB,C. Thus, E

(cid:2)µB

(cid:2)µB

Let sψ := E

(S,β)∼ψ |S| = E

(χ,µ)∼ζ

(cid:2)µB

(χ,µ) /∈Ξb

(χ,µ)∼ζ

(χ,µ)∼ζ µB = yB. By Lemma 4.6,

(χ,µ)∼ζ

(χ,µ)∼ζ

At this point, we may have sψ < yB. We can apply the following operation repeatedly. Take a
pair (S, β) with ψS,β > 0 and S (cid:40) B. We then shift some ψ-mass from the pair (S, β) to (B, β) so
as to increase sψ. Thus, we can assume Property (4.4a) holds for ψ.
i∈B βi ≥ xB,C for every (S, β) in the support
i∈B βi > xB,C.
i ≤ βi
v∈J αv = xB,C, and EMD(α|J, β(cid:48)) = EMD(α|J, β). We then shift all

Property (4.4d) may be unsatisﬁed: we only have(cid:80)
of ψ. To satisfy the property, we focus on each (S, β) in the support of ψ such that(cid:80)
for every i ∈ B,(cid:80)

By considering the matching that achieves EMD(α|J , β), we can ﬁnd a β(cid:48) ∈ RB≥0 such that β(cid:48)
the ψ-mass at (S, β) to (S, β(cid:48)).

i =(cid:80)

i∈B β(cid:48)

To sum up what we have so far, we have a distribution ψ over (S, β) pairs, that satisﬁes
Properties (4.4a), (4.4c), (4.4d), (4.4e) and Property (4.4f) with O((cid:96)4) replaced with O((cid:96)2). The
only Property that is missing is Property (4.4b); to satisfy the property, we shall apply the following
lemma to massage the distribution ψ.
Lemma 4.8. Given a distribution ψ over pairs (S ⊆ B, β ∈ [0, 1]B) satisfying sψ := E
(cid:96)1, we can construct another distribution ψ(cid:48) such that
(4.8a) ψ(cid:48)
(4.8b) every pair (S, β) in the support of ψ(cid:48) has |S| ≤ (cid:100)sψ(cid:101);
(4.8c) E

S,β ≤ O((cid:96)2)ψS,β for every pair (S, β);

(S,β)∼ψ(cid:48) max{|S|,(cid:98)sψ(cid:99)} ≤ sψ.

(S,β)∼ψ |S| ≤

Property (4.8a) requires that the probability that a pair (S, β) happens in ψ(cid:48) can not be too
large compared to the probability it happens in ψ. Property (4.8b) requires |S| ≤ (cid:100)sψ(cid:101) for every
(S, β) in the support of ψ(cid:48). Property (4.8c) corresponds to requiring |S| ≥ (cid:98)sψ(cid:99): even if we count
the size of S as (cid:98)sψ(cid:99) if |S| ≤ (cid:98)sψ(cid:99), the expected size is still going to be at most sψ. The proof of
the lemma is straightforward and thus we defer it to the Appendix B.

11

With Lemma 4.8 we can ﬁnish the proof of Theorem 4.4 for the case yB ≤ 2(cid:96). We apply the
lemma to ψ to obtain the distribution ψ(cid:48). By Property (4.8a), Properties (4.4c), (4.4d) and (4.4e)
remain satisﬁed for ψ(cid:48); Property (4.4f) also holds for ψ(cid:48), as we lost a factor of O((cid:96)2) on the expected
cost.
To obtain our ﬁnal distribution φ, initially we let φS,β = 0 for every pair (S, β). For every
(S, β) in the support of ψ(cid:48), we apply the following procedure. If |S| ≥ (cid:98)sψ(cid:99), then we increase φS,β
S,β; otherwise, take an arbitrary set S(cid:48) ⊆ B such that S ⊆ S(cid:48) and |S(cid:48)| = (cid:98)sψ(cid:99) and increase
by ψ(cid:48)
φS(cid:48),β by ψS,β. Due to Property (4.8b), every pair (S, β) in the support of φ has |S| ∈ {(cid:98)sψ(cid:99) ,(cid:100)sψ(cid:101)}.
(S,β)∼φ |S| ≤ sψ ∈ [yB, (1 + 2(cid:96)π)yB]. If sφ < sψ, we increase
Property (4.8c) implies that sφ := E
sφ using the following operation. Take an arbitrary pair (S, β) in the support of φ such that
|S| = (cid:98)sψ(cid:99), let S(cid:48) ⊇ S be a set such that S(cid:48) ⊆ B and |S(cid:48)| = (cid:100)sψ(cid:101), we decrease φS,β and increase
φS(cid:48),β. Eventually, we can guarantee sφ = sψ; thus Properties (4.4a) and (4.4b) are satisﬁed. This
ﬁnishes the proof of Theorem 4.4 when yB ≤ 2(cid:96).

Now we handle the case where yB > 2(cid:96). By Properties (3.4b) and (3.4c), J is a root black
component that contains a single representative v and yUv=B > 2(cid:96). First we ﬁnd a nearly integral
solution with at most 2(cid:96) + 2 open facilities. Then we close two facilities serving the minimum
amount of demand and spread their demand among the remaining facilities. Since there is at least
2(cid:96) open facilities remaining, we increase the amount of demand at any open facility by no more
than a factor of O(1/(cid:96)).
Consider the following LP with variables {λi}i∈B:
u(cid:48)
iλid(i, v)

i by a factor of 1+O(1/(cid:96)) during the course of the algorithm.

≤ ui. We may scale u(cid:48)

i = xi,C
yi

Let u(cid:48)

(14)

min

s.t.

(cid:88)
(cid:88)

i∈Uv

i∈B

(cid:88)

i∈B

u(cid:48)
iλi = xB,C;

λi = yB; λi ∈ [0, 1],

∀i ∈ B.

By setting λi = yi, we obtain a solution to LP(14) of value (cid:80)

xi,Cd(i, v) ≤ O(1)DUv , by
Lemma 3.2. So, the value of LP(14) is at most O(1)DUv . Fix on such an optimum vertex-point
solution λ of LP(14). Since there are only two non-box-constraints, λ has at most two fractional
λi. Moreover, as yUv ≥ 2(cid:96), there are at least 2(cid:96) facilities in the support of λ.
Consider the i∗ in the support with the smallest λi∗u(cid:48)
i∗ value. Let a := λi∗u(cid:48)
we then scale u(cid:48)

We shall reduce the size of the support of λ by 2, by repeating the following procedure twice.
i ≤ O( 1
(cid:96) ),
i by a factor of 1/(1 − a) ≤ 1 + O(1/(cid:96)) for every i ∈ Uv \ i∗ and change λi∗ to 0.
u(cid:48)
iλi = xUv,C. The value of the objective function is scaled by a factor of at
i for every i ∈ Uv. So, |S| ≤ yUv Properties (4.4c)
βid(i, v) ≤ O(1)DUv since the value of

So, we still have(cid:80)
and (4.4d) are satisﬁed. Moreover, EMDJ (α|J , β) ≤(cid:80)

Let S = {i ∈ Uv : λi > 0} and let βi = λiu(cid:48)

i∗/(cid:80)

most 1 + O(1/(cid:96)).

λiu(cid:48)

i∈Uv

i∈Uv

i∈Uv

i∈Uv

LP(14) is O(1)DUv and we have scaled each βi by at most a factor of 1 + O(1/(cid:96)).

If we let φ contains the single pair (S, β) with probability 1, then all properties from (4.4c) to
(4.4f) are satisﬁed. To satisfy Properties (4.4a) and (4.4b), we can manually add facilities to S with
some probability, as we did before for the case yB ≤ 2(cid:96). This ﬁnishes the proof of Theorem 4.4.

12

4.2 Pairs for Non-Concentrated Black Components

In this section, we construct a (S, β)-pair for the union V of some non-concentrated black compo-
nents satisfying some properties.

J∈J (cid:48) J and
B = U (V ). Assume there exists v∗ ∈ R such that d(v, v∗) ≤ O((cid:96)2)L(J) for every J ∈ J (cid:48) and
v ∈ J. Then, we can ﬁnd a pair (S ⊆ B, β ⊆ RB≥0) such that

Lemma 4.9. Let J (cid:48) ⊆ J N be a set of non-concentrated black components, V = (cid:83)
(4.9a) |S| ∈(cid:8)(cid:100)yB(cid:101) ,(cid:100)yB(cid:101) + 1(cid:9);
(4.9c) (cid:80)

(4.9b) βi ≤ ui if i ∈ S and βi = 0 if i ∈ B \ S;

(4.9d) EMDV (α|V , β) ≤ O((cid:96)2(cid:96)2)DB.
Proof. We shall use an algorithm similar to the one we used for handling the case where yB > 2(cid:96) in
≤ ui to be the “eﬀective capacity” of i. Consider
Section 4.1. Again, for simplicity, we let u(cid:48)
the following LP with variables {λi}i∈B:

i = xi,C
yi

i∈S βi = xB,C.

(15)

(16)

O(1)DUv + (cid:96)2 (cid:88)

J∈J (cid:48)

(cid:96)2πJ L(J)

By setting λi = yi, we obtain a valid solution to the LP with the objective value

s.t.

∀i ∈ B.

(cid:88)

(cid:0)d(i, v) + (cid:96)2L(J)(cid:1)

u(cid:48)
iλi

u(cid:48)
iλi = xB,C;

λi = yB; λi ∈ [0, 1],

min

(cid:88)

i∈B

i∈B

J∈J (cid:48),v∈J,i∈Uv

(cid:88)
(cid:0)d(i, v) + (cid:96)2L(J)(cid:1)
(cid:88)

xi,C

xi,Cd(i, v) + (cid:96)2 (cid:88)

xU (J),CL(J) ≤(cid:88)

J∈J (cid:48)

v∈V
O(1)DU (J) = O((cid:96)2(cid:96)2)DB,

(cid:88)
≤ (cid:88)

v∈V,i∈Uv

J∈J (cid:48),v∈J,i∈Uv

≤ O(1)DB + (cid:96)2(cid:96)2

J∈J (cid:48)

by Lemma 3.2 and Lemma 4.2. So, the value of LP(15) is at most O((cid:96)2(cid:96)2)DB.

constraints, every vertex-point λ of the polytope has at most two fractional λi.

Fix such an optimum vertex-point solution λ of LP (15). Since there are only two non-box-
Let S = {i ∈ B : λi > 0} and let βi = λiu(cid:48)
i for every i ∈ B. So, Properties (4.9a), (4.9b) and
Now we prove Property (4.9d). To compute EMDV (α|V , β), we move all demands in α|V and

(4.9c) are satisﬁed.
all supplies in β to v∗. The cost is

αvd(v, v∗) +

βid(i, v∗)

(cid:88)

i∈B

(cid:88)
≤ (cid:88)

v∈V

J∈J (cid:48),v∈J

xUv,CO((cid:96)2)L(J) +

J∈J (cid:48),v∈J,i∈Uv

(cid:88)

(cid:0)d(i, v) + O((cid:96)2)L(J)(cid:1) ≤ O((cid:96)2(cid:96)2)DB,

βi

where the O((cid:96)2(cid:96)2)DB for the ﬁrst term was proved in (16) and the bound O((cid:96)2(cid:96)2)DB for the
second term is due to the fact that γ is an optimum solution to LP(15). This ﬁnishes the proof of
Lemma 4.9.

13

5 Rounding Algorithm
In this section we describe the rounding algorithm. For every group G ∈ G, we use ΛG to denote the
set of child-groups of G. We construct a partition JC of J C as follows. For each root group G ∈ G,
(G(cid:48) ∩ J C)
to JC, if it is not empty. We construct the partition JN for J N in the same way, except that we
J∈J (cid:48) J

we add G ∩ J C to JC if it is not empty. For each non-leaf group G ∈ G, we add(cid:83)
consider components in J N. We also deﬁne a set V N as follows: for every J (cid:48) ∈ JN, we add(cid:83)
to V N; thus, V N forms a partition for(cid:83)

G(cid:48)∈ΛG

J∈J N J.
5.1 Constructing Initial S∗, α∗ and β∗
During the algorithm, we shall maintain a set S∗ of open facilities, a supply vector β∗ ∈ RF≥0 such
i = 0 if i /∈ S∗, and a demand vector α∗ ∈ RR≥0. Initially, let α∗ = α (recall that αv = xUv,C
that β∗
is the demand at v after the initial moving, for every v ∈ R).
To deﬁne our initial S∗ and β∗, we ﬁrst consider non-concentrated components. Focus on a
J∈J (cid:48) J ∈ V N. Notice that J (cid:48) may contain a single root
black component J, or contain all the non-concentrated black components in the child-groups of
some group G. In the former case, the diameter of J is at most O((cid:96))L(J) by Property (3.4a); in
J(cid:48)∈G J(cid:48) and then any representative
v ∈ J, J ∈ J (cid:48) has d(v, v∗) ≤ O((cid:96)2)L(J) by Property (3.5d). Thus, we can apply Lemma 4.9 to

J (cid:48) ∈ JN and the correspondent V = (cid:83)
the latter case, we let v∗ be an arbitrary representative in (cid:83)
obtain a pair(cid:0)S ⊆ U (V ), β ∈ RU (V )≥0
otherwise we apply Theorem 4.4 to each component J to obtain a distribution(cid:0)φJ
For notational convenience, we shall use a ≈ b to denote a ∈(cid:2)(cid:98)b(cid:99) ,(cid:100)b(cid:101)(cid:3). Consider the following

Then we focus on concentrated components. For every J ∈ J C, we check if Constraints (6) to
(13) are satisﬁed for B = U (J). If not, we return a separation plane for the fractional solution;
.
S⊆U (J),β∈RU (J)≥0

(cid:1)
(S,β)∼φJ |S| be the expectation of |S| according to distribution φJ .

(cid:1) satisfying all the properties in the lemma. Then, we add S

i = βi for every i ∈ U (V ).

We let sJ := sφJ := E
polytope P deﬁned by variables {ψJ

to S∗ and let β∗

S,β

S,β}J∈J C,S,β and {qJ}J∈J C.2
(cid:88)
S,β|S| − qJ ≈ yU (J),
ψJ

qJ ≤ 1,

J∈J (cid:48)

(17)

(18)

S,β|S| − qJ
ψJ

S,β|S| − qJ
ψJ

yU (J),

∀J (cid:48) ∈ JC;

yU (J).

∀J (cid:48) ∈ JC;

∀J ∈ J C;

(19)

(20)

(21)

(22)

S,β, pJ ∈ [0, 1] ∀J ∈ J C, S, β;
(cid:88)
ψJ

∀J ∈ J C;

ψJ

S,β = 1,

S,β

(cid:88)
(cid:88)

J∈J (cid:48)

(cid:16)(cid:88)
(cid:16)(cid:88)

S,β

J∈J C

S,β

(cid:88)
(cid:17) ≈ (cid:88)
(cid:17) ≈ (cid:88)

J∈J (cid:48)

S,β

J∈J C

In the above LP, ψJ will indicate the set of open facilities in J and the amount of demand that
will be satisﬁed by each of these open facilities. qJ will indicate whether J has opened 1 more
facility than the budget. Later, if qJ = 1, we shall call remove(J) to remove a facility.

2For every J ∈ J C, we only consider the pairs (S, β) in the support of φJ ; thus the total number of variables is

nO((cid:96)).

14

In the appendix, we shall show that any vertex point of P is deﬁned by two laminar families of

tight constraints and thus P is integral:
Lemma 5.1. P is integral.

J∈J (cid:48) q∗

We set ψ∗J

S,β = φJ

S,β and q∗

J ≤ 2(cid:96)(cid:80)

S,β|S| − q∗

J = sJ − q∗

J∈J (cid:48) yU (J)πJ /xU (J),C ≤ 2(cid:96)(cid:80)

Then, we can randomly output a vertex point (ψ, q) of P such that E[ψJ

S,β ψ∗J
is a point in P.
every J ∈ J C, (S, β), and E[qJ ] = q∗

J = sJ − yU (J) for every J ∈ J C and (S, β). We show that (ψ∗, q∗)
is a point in P. Notice that sJ is the expected size of S according to the distribution φJ , while
yU (J) is the budget for the number of open facilities open in U (J). So q∗
J is the expected number of
facilities that go beyond the budget. It is easy to see that Constraints (17) and (18) hold for ψ∗ and
J∈J (cid:48) yU (J)/(cid:96)2 ≤
2(cid:96) × O((cid:96)2)/(cid:96)2 due to Properties (3.5e) and (4.4a). This at most 1 if (cid:96)2 = Θ((cid:96)3) is large enough. (If
J = 0.) Thus, Constraint (19)
J = yU (J). So, Constraints (20), (21) and (22) hold. So, (ψ∗, p∗)

q∗. For every J (cid:48) ∈ JC, we have that(cid:80)
J (cid:48) contains a root component J which has yU (J) > 2(cid:96) then(cid:80)
holds. (cid:80)
J ∈ J , there is a unique(cid:0)S ⊆ U (J), β ∈ RU (J)≥0
(5.2a) (cid:80)
(5.2b) (cid:80)
Property (5.2a) is due to Properties (4.4d) and (4.9c). Property (5.2b) holds since (cid:80)
(cid:80)
v∈R xUv,C = xF,C = |C|.

S,β for
J = sJ − yU (J) for every J ∈ J C. Since ψ is integral, for every
S,β = 1; for this unique (S, β), we add
This ﬁnishes the deﬁnition of the initial S∗ and β∗. The initial α∗ and (S∗, β∗) satisfy the

following properties. We shall maintain these properties as we run the rounding algorithm.

(cid:1) such that ψJ

v∈V β∗

v for every V ∈ J C ∪ V N;

i = βi for every i ∈ U (J).

S,β] = ψ∗J

S,β = φJ

S to S∗ and let β∗

v =(cid:80)

v = |C|.

v∈V α∗
v∈R α∗

v∈R α∗

v =

J∈J (cid:48) q∗

5.2 The remove procedure
The initial S∗ may contain more than k facilities. Thus, we shall shutdown some facilities in S∗,
update α∗ and β∗, by deﬁning a procedure called remove. The procedure takes a set V ∈ J C ∪ V N
as input. If V is a root black component, then we let G = {V } be the root group containing V ;
if V is a non-root concentrated component, let G be the parent group of the group containing V ;
otherwise V is the union of non-concentrated components in all child-groups of some group, and
J(cid:48)∈G J(cid:48). Before calling remove(V ), we require the following

we let G be this group. We let V (cid:48) =(cid:83)
(5.3b) (cid:12)(cid:12)S∗ ∩ U (V (cid:48))(cid:12)(cid:12) ≥ (cid:96) − 6.

properties to hold:
(5.3a) |S∗ ∩ U (V )| ≥ 1;

In remove(V ), we shall make changes to S∗, α∗ and β∗ so that Properties (5.2a) and (5.2b) remain
satisﬁed. Moreover, after each call to remove(V ), following properties hold
(5.4a) exactly one facility is removed from S∗ and this facility is in S∗ ∩ U (V ∪ V (cid:48));
(5.4b) if v /∈ V ∪ V (cid:48), then α∗
(5.4c) if v ∈ V ∪ V (cid:48), then α∗

v is not changed and if i /∈ U (V ∪ V (cid:48)) then β∗
v is increased by at most a factor of 1 + O(1/(cid:96)), and if i ∈ U (V ∪ V (cid:48))

is not changed;

i

then β∗

i

is increased by at most a factor of 1 + O(1/(cid:96));

(5.4d) the moving cost for converting the old α∗ to the new α∗ is at most O((cid:96)2)β∗

black component J ⊆ V and facility i∗ ∈ U (J);

i∗L(J) for some

15

i /(cid:80)

i(cid:48) value.

v∈V fv,i = β∗

i(cid:48) ≤ O(1)β∗

If a := β∗

v(cid:48)∈V (cid:48) α∗

v(cid:48) ≤ 1

(cid:0)α∗|V , β∗|U (V )

(cid:1). (Due to Property (5.2a), the

from v to V (cid:48). The total amount of demand moved from V to V (cid:48) is exactly(cid:80)

We shall ﬁrst highlight the ideas used to implement remove(V ). Assume V is not a root
component. We choose an arbitrary facility i ∈ S∗ ∩ U (V ). Notice that there are Ω((cid:96)) facilities
in S∗ ∩ U (V (cid:48)). If the β∗
v(cid:48)/(cid:96), then we can shut down i and send the demands that
should be sent to i to V (cid:48). We only need to increase the supplies in U (V (cid:48)) by a factor of 1 + O(1/(cid:96)).
Otherwise, we shall shut down the facility i(cid:48) ∈ S∗ ∩ U (V (cid:48)) with the smallest βi(cid:48) value. Since there
are at least Ω((cid:96)) facilities in U (V (cid:48)), we can satisfy the βi(cid:48) units of unsatisﬁed demands using other
facilities in S∗ ∩ U (V (cid:48)). For this i(cid:48), we have β∗
i . Thus, the total amount of demands that
should be moved is comparable to β∗
i ; this can be used to prove Property (5.4d). When V is a root
component, we shall shut down the facility i(cid:48) ∈ S∗ ∩ U (V ) with the smallest β∗
We now formally describe the procedure remove(V ). We ﬁrst consider the case that V is not a
root component. So, V is either a non-root component in J C, or the union of all non-concentrated
components in all child-groups of the group G. In this case, V ∩ V (cid:48) = ∅. Let i ∈ U (V ) ∩ S∗ be an
arbitrary facility; due to Property (5.3a), we can ﬁnd such an i. Let J ⊆ V be the component that
contains i.
(cid:96) , then we shall shutdown i. Consider the matching f : V × U (V ) →
R≥0 between α∗|V and β∗|U (V ) that achieves EMDV
total supply equals the total demand.) For every v ∈ V , we shall move f (v, i) units of demand
i . Every
v(cid:48) ∈ V (cid:48) will receive aα∗
v(cid:48) units of demand. We update α∗ to be the new demand vector: decrease α∗
v
by f (v, i) for every v ∈ V and scale α∗
v(cid:48) by a factor of (1 + a) for every v(cid:48) ∈ V (cid:48). By Property (3.5d),
the cost of moving the demands is at most β∗
i to 0, and for every i(cid:48) ∈ U (V (cid:48)), scale β∗
We remove i from S∗, change β∗
i(cid:48) by a factor of (1 + a).
For this new α∗ and β∗ vector, EMDV (α∗, β∗) will not increase. α∗|V (cid:48)(cid:48) and β∗|U (V (cid:48)(cid:48)) are scaled by
a factor of 1 + a ≤ 1 + 1/(cid:96) for every V (cid:48)(cid:48) ∈ J C ∪ V N such that V (cid:48)(cid:48) ⊆ V (cid:48). Thus Properties (5.4c)
and (5.4e) are satisﬁed. Properties (5.4a) and (5.4b) are trivially true. Moreover, we maintained
Properties (5.2a) and (5.2b).
Now consider the case a > 1/(cid:96). In this case, we shall remove the facility i(cid:48) ∈ S∗ ∩ U (V (cid:48)) with
i(cid:48) value from S∗. Notice that we have |S∗ ∩ U (V (cid:48))| ≥ (cid:96) − 6 before we run remove(V )
the smallest β∗
i(cid:48)(cid:48); so, we have a(cid:48) ≤ 1/((cid:96) − 6). To remove the
facility i(cid:48), we consider the function f that achieves EMDV (cid:48)(α∗|V (cid:48), β∗|U (V (cid:48))). We shall redistribute
i(cid:48)(cid:48)∈U (V (cid:48))\{i(cid:48)} f (v(cid:48), i(cid:48)(cid:48)). We
i(cid:48)(cid:48) for all other i(cid:48)(cid:48) ∈ U (V (cid:48))\{i(cid:48)} by (1 + a(cid:48)). Then,
remove i(cid:48) from S∗, change β∗
the total cost for redistributing the demands in this procedure will be at most β∗
i(cid:48)O((cid:96))L(J), due to
i L(J) since a > 1/(cid:96) and a(cid:48) ≤ 1/((cid:96)− 6). So, Properties (5.4a)
Property (3.4d). This is at most O((cid:96))β∗
to (5.4e) are satisﬁed and Properties (5.2a) and (5.2b) are maintained.
The case where V is a root component can be handled in a similar way. In this case, we have
G = {V } and V (cid:48) = V . By Property (5.3b), there are at least (cid:96) − 6 facilities in S∗ ∩ U (V ). Then
we can remove the facility i ∈ U (V )∩ S∗ with the smallest β∗
i . Using the same argument as above,
we can guarantee Properties (5.4a) to (5.4e) and maintain Properties (5.2a) and (5.2b).

due to Property (5.3b). Let a(cid:48) := βi(cid:48)/(cid:80)
the demands in V (cid:48) so that the new demand at v(cid:48) ∈ V (cid:48) will be (1 + a(cid:48))(cid:80)

i O((cid:96)2)L(J); thus, Property (5.4d) holds.

i(cid:48) to 0 and scale up β∗

(5.4e) for every V (cid:48)(cid:48) ∈ J C ∪V N such that V (cid:48)(cid:48) ⊆ V ∪ V (cid:48), EMDV (cid:48)(cid:48)(cid:0)α∗|V (cid:48)(cid:48), β∗|U (V (cid:48)(cid:48))

(cid:1) will be increased

by at most a factor of 1 + O(1/(cid:96)).

i ≤ (cid:80)

v(cid:48)∈V (cid:48) α∗

i(cid:48)(cid:48)∈U (V (cid:48)) β∗

16

5.3 Obtaining the Final Solution

Let V = (cid:83)

G(cid:48)∈ΛG,J∈G(cid:48)∩J N J. Repeat the following procedure twice:

Now, we describe how we call the procedure remove. We consider each group G using the top-
to-bottom order. That is, before we consider a group G, we have already considered its parent
group. If G is a root group, then it contains a single root component J. If J ∈ J N, repeat the
if there is some facility in S∗ ∩ U (J) then we call remove(J). If
the following procedure twice:
J ∈ J C and qJ = 1 then we call remove(J). Now if G is a non-leaf group, then do the following.
if there is some facility in
S∗ ∩ U (V ) then we call remove(V ). For every G(cid:48) ∈ ΛG and J ∈ G(cid:48) ∩ J C such that qJ = 1 we call
remove(J).
We ﬁrst show that after this procedure, we always have |S∗| ≤ yF = k. To show that, we
prove that whenever we call remove(V ), Properties (5.3a) and (5.3b) hold. For any concentrated
component J with qJ = 1, we have called remove(J). Notice that if qJ = 1, then initially we have
|S∗ ∩ U (J)| ≥ 1 due to Constraint (20). Due to the top-down order of considering components,
and Property (5.4a), we have never removed a facility in S∗ ∩ U (J) before calling remove(J). Thus,
Property (5.3a) holds. For V ∈ V N, we check if |S∗ ∩ U (V )| ≥ 1 before we call remove(V ) and thus
Property (5.3a) holds.

Now consider Property (5.3b). For any non-leaf group G, initially, we have(cid:12)(cid:12)S∗ ∩(cid:83)

J∈G U (J)(cid:12)(cid:12) ≥
(cid:5) ≥ (cid:96) where the ﬁrst inequality is due to Property (4.9a) and Constraint (21) and the

(cid:4)(cid:80)

J∈G yU (J)

(cid:16)(cid:80)

J∈G U (J)(cid:12)(cid:12) ≥(cid:80)

second is due to Property (3.5c). We may remove a facility from the set when we call remove(V ) for
V satisfying one of the following conditions: (a) V is a concentrated component in G or in a child
group of G, (b) V is the union of the non-concentrated components in the child-groups of G or (c)
V contains the non-concentrated components in G. For case (a), we removed at most 2 facilities
due to Constraint (19). For each (b) and (c), we remove at most 2 facilities. Thus, we shall remove

at most 6 facilities from(cid:12)(cid:12)S∗ ∩(cid:83)
remove(J) once. For each V ∈ V N, initially we have |S∗ ∩ U (V )| ∈(cid:8)(cid:6)yU (V )
Thus, every call of remove is successful. For a concentrated component J with qJ = 1, we called
calling remove(V ), we have never removed a facility from S∗ ∩ U (V ). Thus, the number of times
facilities in S∗ after the removing procedure is at most(cid:80)
we call remove(V ) is at least the initial value of |S∗ ∩ U (V )| minus yU (V ). Overall, the number of
J∈J C yU (J)+1+(cid:80)
(cid:80)
V ∈V N yU (V ) <
V ∈V N yU (V ) = yF +1 = k+1, where the ﬁrst inequality is due to Constraint (22).

(cid:7) ,(cid:6)yU (V )
(cid:7) + 1(cid:9). Before
(cid:17)
+(cid:80)

J∈G yU (J). Thus, Property (5.3b) holds.

i ≤ (1 + O(1/(cid:96)))ui for every i ∈ F .

i for every i ∈ V . Finally we have β∗

By Properties (5.4b) and (5.4c), and Constraint (19), our ﬁnal β∗
is at most 1 + O(1/(cid:96)) times
Now we bound the expected cost of the solution S∗, by bounding the cost for transferring our

Since |S∗| is an integer, we have that |S∗| ≤ k.
the initial β∗
original α∗ to our ﬁnal α∗, as well as the cost for matching our new α∗ and β∗.
We ﬁrst focus on the transferring cost. By Property (5.4e), when we call remove(V ), the
i∗L(J) for some black component J ⊆ V and i∗. Notice that
transferring cost is at most O((cid:96)2)β∗
i∗ ≤ (1 + O(1/(cid:96)))αU (J),C. So,
β∗
i∗ is scaled by at most a factor of (1 + O(1/(cid:96))), we always have β∗
the cost is at most O((cid:96)2)xU (J),CL(J). If V is the union of some non-concentrated components,
then this quantity is at most O((cid:96)2)(cid:96)2πJ L(J) ≤ O((cid:96)2(cid:96)2)DU (J) ≤ O((cid:96)2(cid:96)2)DU (V ). We call remove(V )
at most twice, thus the contribution of V to the transferring cost is at most O((cid:96)2(cid:96)2)DU (V ). If V
is a concentrated component J, then the quantity might be large. However, the probability we
J = sJ − yU (J) ≤ 2(cid:96)yU (J)πJ /xU (J),C if yU (J) ≤ 2(cid:96) and it is 0 otherwise
call remove(J) is E[qJ ] = q∗
(by Property (4.4a)). So, the expected contribution of this V to the transferring cost is at most

J∈J C

S,β ψJ

S,β − qJ

i

17

O((cid:96)2)xU (J),CL(J)× 2(cid:96)yU (J)πJ /xU (J),C ≤ O((cid:96)4)πJ L(J) ≤ O((cid:96)4)DU (J) by Lemma 4.2. Thus, overall,
the expected transferring cost is at most O((cid:96)5)DF = O((cid:96)5)LP.

is bounded by (cid:80)
β∗, the expectation of this quantity is at most(cid:80)

Then we consider the matching cost. Since we maintained Property (5.2a), the matching cost
V ∈J C∪V N EMDV (α∗|V , β∗|U (V )). Due to Property (5.4e), this quantity has only
increased by a factor of 1 + O(1/(cid:96)) during the course of removing facilities. For the initial α∗ and
V ∈V N O((cid:96)2(cid:96)2)DU (V ) due to

J∈J C O((cid:96)4)DU (J) +(cid:80)

We have found a set S∗ of at most k facilities and a vector β∗ ∈ RF≥0 such that β∗

Properties (4.4f) and (4.9d). This is at most O((cid:96)5)DF = O((cid:96)5)LP.
i = 0 for every
i /∈ S∗ and β∗
i ≤ (1 + )ui. The
cost for matching the α-demand vector and the β∗ vector is at most O((cid:96)5)LP = O(1/5)LP. Thus,
we obtained a O(1/5)-approximation for CKM with (1 + )-capacity violation.

i ≤ (1 + O(1/(cid:96)))ui. If we set (cid:96) = Θ(1/) to be large enough, then β∗

References

[1] Karen Aardal, Pieter L. van den Berg, Dion Gijswijt, and Shanfei Li. Approximation al-
gorithms for hard capacitated k-facility location problems. European Journal of Operational
Research, 242(2):358 – 368, 2015.

[2] Hyung-Chan An, Mohit Singh, and Ola Svensson. LP-based algorithms for capacitated facility
location. In Proceedings of the 55th Annual IEEE Symposium on Foundations of Computer
Science, FOCS 2014.

[3] V. Arya, N. Garg, R. Khandekar, A. Meyerson, K. Munagala, and V. Pandit. Local search
heuristic for k-median and facility location problems. In Proceedings of the thirty-third annual
ACM symposium on Theory of computing, STOC ’01, pages 21–29, New York, NY, USA, 2001.
ACM.

[4] Jarosaw Byrka, Krzysztof Fleszar, Bartosz Rybicki, and Joachim Spoerhase. Bi-factor approx-
imation algorithms for hard capacitated k-median problems. In Proceedings of the 26th Annual
ACM-SIAM Symposium on Discrete Algorithms (SODA 2015).

[5] Jarosaw Byrka, Thomas Pensyl, Bartosz Rybicki, Aravind Srinivasan, and Khoa Trinh. An
improved approximation for k-median, and positive correlation in budgeted optimization. In
Proceedings of the 26th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2015).

[6] Jarosaw Byrka, Bartosz Rybicki, and Sumedha Uniyal. An approximation algorithm for uni-

form capacitated k-median problem with 1 +  capacity violation, 2015. arXiv:1511.07494.

[7] Robert D. Carr, Lisa K. Fleischer, Vitus J. Leung, and Cynthia A. Phillips. Strengthening
integrality gaps for capacitated network design and covering problems. In Proceedings of the
Eleventh Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’00, pages 106–115,
Philadelphia, PA, USA, 2000. Society for Industrial and Applied Mathematics.

[8] M. Charikar and S. Guha. Improved combinatorial algorithms for the facility location and
k-median problems. In In Proceedings of the 40th Annual IEEE Symposium on Foundations
of Computer Science, pages 378–388, 1999.

18

[9] M. Charikar, S. Guha, . Tardos, and D. B. Shmoys. A constant-factor approximation algorithm
for the k-median problem (extended abstract). In Proceedings of the thirty-ﬁrst annual ACM
symposium on Theory of computing, STOC ’99, pages 1–10, New York, NY, USA, 1999. ACM.

[10] Julia Chuzhoy and Yuval Rabani. Approximating k-median with non-uniform capacities. In

SODA 05, pages 952–958, 2005.

[11] Sudipto Guha. Approximation Algorithms for Facility Location Problems. PhD thesis, Stan-

ford, CA, USA, 2000.

[12] K. Jain, M. Mahdian, and A. Saberi. A new greedy approach for facility location problems. In
Proceedings of the thiry-fourth annual ACM symposium on Theory of computing, STOC ’02,
pages 731–740, New York, NY, USA, 2002. ACM.

[13] K Jain and V. V. Vazirani. Approximation algorithms for metric facility location and k-median
problems using the primal-dual schema and Lagrangian relaxation. J. ACM, 48(2):274–296,
2001.

[14] Shanfei Li. An improved approximation algorithm for the hard uniform capacitated k-median
problem. In APPROX ’14/RANDOM ’14: Proceedings of the 17th International Workshop on
Combinatorial Optimization Problems and the 18th International Workshop on Randomization
and Computation, APPROX ’14/RANDOM ’14, 2014.

[15] Shi Li. On uniform capacitated k-median beyond the natural LP relaxation. In Proceedings

of the 26th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2015).

[16] Shi Li. Approximating capacitated k -median with (1 + )k open facilities. In Proceedings of the
27th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2016), pages 786–796,
2016.

[17] Shi Li and Ola Svensson. Approximating k-median via pseudo-approximation. In Proceedings
of the Forty-ﬁfth Annual ACM Symposium on Theory of Computing, STOC ’13, pages 901–910,
New York, NY, USA, 2013. ACM.

[18] J. Lin and J. S. Vitter. -approximations with minimum packing constraint violation (ex-
tended abstract). In Proceedings of the 24th Annual ACM Symposium on Theory of Computing
(STOC), Victoria, British Columbia, Canada, pages 771–782, 1992.

[19] D. B. Shmoys, . Tardos, and K. Aardal. Approximation algorithms for facility location prob-
lems (extended abstract). In STOC ’97: Proceedings of the twenty-ninth annual ACM sympo-
sium on Theory of computing, pages 265–274, New York, NY, USA, 1997. ACM.

A Missing Proofs from Section 3
Proof of Lemma 3.1. First consider Property (3.1a). Assume dav(v) ≤ dav(v(cid:48)). When we add v
to R, we remove all clients j satisfying d(v, j) ≤ 4dav(j) from C. If v(cid:48) ∈ R, then it must have
been d(v, v(cid:48)) > 4dav(v(cid:48)). For Property (3.1b), just consider the iteration in which j is removed
from C. The representative v added to R in this iteration satisfy the property. Then consider
Property (3.1c). By Property (3.1a), we have B := {i ∈ F : d(i, v) ≤ 2dav(v)} ⊆ Uv. Since dav(v) =

19

(cid:80)
i∈F xi,vd(i, v) and (cid:80)

i∈F xi,v = 1, we have dav(v) ≥ (1 − xB,v)2dav(v), implying yUv ≥ yB ≥
xB,v ≥ 1/2, due to Constraint (3).
Then we consider Property (3.1d). By Property (3.1b), there is a client v(cid:48) ∈ R such that
dav(v(cid:48)) ≤ dav(j) and d(v(cid:48), j) ≤ 4dav(j). Since d(i, v) ≤ d(i, v(cid:48)) as v(cid:48) ∈ R and i was added to Uv, we
have d(i, v) ≤ d(i, v(cid:48)) ≤ d(i, j) + d(j, v(cid:48)) ≤ d(i, j) + 4dav(j).
Proof of Lemma 3.2. By Property (3.1d), we have d(i, v) ≤ d(i, j) + 4dav(j) for every i ∈ Uv and

j ∈ C. Thus, (cid:88)

xi,Cd(i, v) ≤ (cid:88)

(cid:0)d(i, j) + 4dav(j)(cid:1) ≤ (cid:88)

xi,j

i∈Uv

i∈Uv,j∈C

4Di = 4DUv .

i∈Uv

Proof of Lemma 3.4. We show that all the black edges between the representatives in J are con-
sidered before all the edges in J × (R \ J) in the Kruskal’s algorithm. Assume otherwise. Consider
the ﬁrst edge e in J × (R \ J) we considered. Before this iteration, J is not connected yet. Then
we add e to the minimum spanning tree; since J is a black component, e is gray or white. In either
case, the new partition J(cid:48) formed by adding e will have weight more than (cid:96). This implies all edges
in J(cid:48) × (R \ J(cid:48)) added later to the MST are not black. Moreover, J \ J(cid:48), J(cid:48) \ J and J ∩ J(cid:48) are all
non-empty. This contradicts the fact that J is a black component. Therefore, all black edges in J
has length at most L(J), implying Property (3.4a) .
Focus on a tree T in the initial forest ΥJ and any small black component J in T . All black
edges among the representatives in J are added to MST before any edge in J × (R \ J). The ﬁrst
edge in J × (R \ J) added to MST is a grey edge directed from J to some other black component:
it is not white because J is small; it is not black since J is a black component. So, this is a grey
edge in T .

So, the growth of the tree T in the Kruskal’s algorithm must be as follows. The ﬁrst grey edge
in T is added between two black components, one of them is big and the other is small. We deﬁne
the root of T to be the big component. At each time, we add a new small black component J to
the existing tree via a grey edge directed from J to the existing tree. (During this process, white
edges incident to the existing tree T may be added.) So, the tree T is a rooted tree with grey edges,
where all edges are directed towards the root. So, Property (3.4b) holds for ΥJ . Moreover, notice
the length of the grey edge between J and its parent J(cid:48) is d(J, J(cid:48)) = L(J), implying Property (3.4e)
for ΥJ . Since d(J, J(cid:48)) ≥ d(J(cid:48), R \ J(cid:48)) = L(J(cid:48)), we have Property (3.4d) again for the initial forest
ΥJ .
The root J of T is a big black component. Suppose it contains two or more representatives and
at least one black edge connecting them; so it’s not a singleton. Consider the last black edge (v, v(cid:48))
added between Jv and Jv(cid:48) to make J = Jv ∪ Jv(cid:48). Since (v, v(cid:48)) is a black edge, both Jv and Jv(cid:48) are
small, i.e. yU (Jv), yU (Jv(cid:48) ) < (cid:96). Therefore, we have yU (J) = yU (Jv) + yU (Jv(cid:48) ) < 2(cid:96) proving Property
(3.4c).
J . Thus , Prop-
erty (3.4f) holds for Υ∗
J . Property (3.4a) is independent of the forest and thus still holds for the
ﬁnal forest Υ∗
J . Thus, properties (3.4b)
and (3.4c) are maintained for Υ∗
J . Since we sorted the children of a component according to L
values before constructing the left-child-right-sibling binary tree, Property (3.4d) is satisﬁed for
Υ∗
J .

We use left-child-right-sibling binary-tree representation of ΥJ to obtain Υ∗

J . A component is a root in ΥJ if and only if it is a root in Υ∗

20

For every component J and its parent J(cid:48) in the forest Υ∗

J , we have L(J) = d(J, R\J) = d(J, J(cid:48)(cid:48)),
where J(cid:48)(cid:48) is the parent of J in the initial forest ΥJ . J(cid:48) is either J(cid:48)(cid:48), or a child of J(cid:48)(cid:48) in ΥJ . In
the former case, we have d(J, J(cid:48)) = d(J, J(cid:48)(cid:48)) = L(J). In the later case, we have that d(J(cid:48), J(cid:48)(cid:48)) =
L(J(cid:48)) ≤ L(J) = d(J, J(cid:48)(cid:48)). Due to Property (3.4a), we have a path connecting some representative
in J to some representative in J(cid:48), with internal vertices being representatives in J(cid:48)(cid:48), where all edges
in the path have length at most L(J). Moreover, there are at most 4(cid:96) representatives in J(cid:48)(cid:48) due
to Properties (3.4b), (3.4c), and (3.1c). Thus, we have d(J, J(cid:48)) ≤ O((cid:96))d(J, J(cid:48)(cid:48)) = O((cid:96))L(J). Thus,
Property (3.4e) holds for Υ∗
J .

Proof of Lemma 3.5. For a root component J, we have yU (J) ≥ (cid:96) by Property (3.4b). Thus, any
root group G contains a single root component J, which is exactly Property (3.5a).
When constructing the group G from the tree T = (JT , ET ), the terminating condition is
J∈G yU (J) ≥ (cid:96). Thus, if G is not a leaf-group, then the condition G = JT does not

G = JT or(cid:80)
hold; thus we have(cid:80)
last black component was added to it. Then we have(cid:80)

J∈G yU (J) ≥ (cid:96), implying Property (3.5c).

By Property (3.4b), any non-root component J has yU (J) < (cid:96). Thus, if G is not a root group, the
terminating condition constructing G implies that G had total weight less than (cid:96) right before the
J∈G yU (J) < 2(cid:96), implying Property (3.5b).
Now, consider Property (3.5d). From Property (3.4d), it is easy to see that the group G
constructed from the tree T = (JT , ET ) has the following property: the L value of any component
in G is at most the L-value of any component in JT \ G. Let G be a non-root group and G(cid:48) be
its parent; let J ∈ G and J(cid:48) ∈ G(cid:48) be black components. Thus, there is a path in Υ∗
J from J to
J(cid:48), where components have L-values at most L(J). The edges in the path have length at most
O((cid:96))L(J) by Property (3.4e). Moreover, Property (3.4a) implies that the representatives in each
component in the path are connected by edges of length at most L(J). Thus, we can ﬁnd a path
J(cid:48)(cid:48)∈G∪G(cid:48) J(cid:48)(cid:48), and every edge in the path has length
at most O((cid:96))L(J) = O((cid:96))d(J, R\J). By Property (3.1c), (3.4b) and (3.4c), the total representatives
in the components contained in G (as well as in G(cid:48)) is at most 4(cid:96). Thus, the distance between v
and v(cid:48) is at most O((cid:96)2)L(J), which is exactly Property (3.5d).
J is binary and every group G ∈ G contains at most O((cid:96)) components,

from v to v(cid:48) that go through representatives in(cid:83)

Finally, since the forest Υ∗

we have that every group G contains at most O((cid:96)) children, implying Property (3.5e).

B Missing Proofs from Section 4
Proof of Lemma 4.2. Let B = U (J). For every i ∈ B, j ∈ C, we have d(i, J) ≤ d(i, j) + 4dav(j) by
Property (3.1d) and the fact that i ∈ Uv for some v ∈ J. Thus,

L(J)π(J) = L(J)

xB,j(1 − xB,j) = L(J)

xi,jxi(cid:48),j · 2d(i(cid:48), J) ≤ 2

(cid:88)

j∈C

i∈B,j∈C,i(cid:48)∈F\B

(cid:16)

≤ (cid:88)
(cid:88)
(cid:88)

i∈B,j∈C

= 2

= 2

i∈B

(5Di) = 10DB.

(cid:88)
(cid:88)
(cid:88)

i(cid:48)∈F

i∈B,j∈C

j∈C,i∈B,i(cid:48)∈F\B

xi,j

xi(cid:48),j

(cid:88)
(cid:17) ≤ 2

i∈B,j∈C

xi,jxi(cid:48),j

(cid:0)d(i(cid:48), j) + d(j, i) + d(i, J)(cid:1)
(cid:16)

(cid:17)

xi,j

dav(j) + d(j, i) + d(i, J)

xi,j

2d(i, j) + 5dav(j)

21

j∈C gt,j = χB,C.
Fix a client j ∈ C, we have

xB,j(1 − xB,j) =

(cid:90) xB,j
0 1t<xB,j dt = xB,j =(cid:82) 1

0

(cid:90) 1

(1 − 2t)dt =

1t<xB,j (1 − 2t)dt ≥

(cid:90) 1

0

gt,j(1 − 2t)dt,

0

function of t.

fact that(cid:82) 1
Summing up the inequality over all j ∈ C, we have πJ ≥(cid:82) 1
(cid:80)
for every t ∈ [t∗, 1). As gt is a non-increasing function of g and (cid:82) 1
to see that (cid:82) 1

where 1t<xB,j is the indicator variable for the event that t < xB,j. The inequality comes from the
0 gt,jdt, gt,j ∈ [0, 1] for every t ∈ [0, 1), and 1 − 2t is a decreasing
0 gt(1−2t)dt. By our assumption that
ζχ,µ > (cid:96)πJ /xB,C, there exists a number t∗ < 1 − (cid:96)πJ /xB,C such that gt ≤ (1 − 1/(cid:96))xB,C
0 gtdt = xB,C, it is not hard
0 gt(1 − 2t)dt is minimized when gt = (1 − 1/(cid:96))xB,C for every t ∈ [t∗, 1) and gt =

(χ,µ) /∈Ξb

xB,C−(1−1/(cid:96))xB,C (1−t∗)

t∗

xB,C for every t ∈ [0, t∗). We have

The ﬁrst inequality is by L(J) ≤ 2d(i(cid:48), J) for any i(cid:48) ∈ F \ B = UR\J : d(i(cid:48), R \ J) ≤ d(i(cid:48), J) implies
L(J) = d(R\J, J) ≤ d(R\J, i(cid:48))+d(i(cid:48), J) ≤ 2d(i(cid:48), J). The second inequality is by triangle inequality
and the third one is by d(i, J) ≤ d(i, j) + 4dav(j). All the equalities are by simple manipulations of
notations.

Proof of Lemma 4.6. The idea is to use the property that J is concentrated. To get some intuition,
consider the case where πJ = 0. For every j ∈ C, either xB,j = 0 or xB,j = 1. Thus, all pairs (χ, µ)
in the support of ζ have χB,j = xB,j for every j ∈ C; thus χB,C = xB,C.
ζχ,µ > (cid:96)πJ /xB,C. We sort all pairs (χ, µ) in
the support of ζ according to descending order of χB,C. For any t ∈ [0, 1), and j ∈ C, deﬁne
gt,j ∈ [0, 1] as follows. Take the ﬁrst pair (χ, µ) in the ordering such that the total ζ value of the
pairs (χ(cid:48), µ(cid:48)) before (χ, µ) in the ordering plus ζχ,µ is greater than t. Then, deﬁne gt,j = χB,j and

Assume towards contradiction that (cid:80)
deﬁne gt =(cid:80)

(χ,µ) /∈Ξb

(cid:90) 1

(cid:33)

(1 − 2t)dt +

(1 − 1/(cid:96))(1 − 2t)dt

(cid:0)t∗ − (t∗)2(cid:1) − (1 − 1/(cid:96))(cid:0)t∗ − (t∗)2(cid:1)(cid:19)

t∗

xB,C

xB,C

πJ ≥

=

= 1/(cid:96)+t∗−t∗/(cid:96)

t∗
1/(cid:96) + t∗ − t∗/(cid:96)

(cid:32)(cid:90) t∗
(cid:18) 1/(cid:96) + t∗ − t∗/(cid:96)
(cid:0)t∗ − (t∗)2(cid:1) xB,C =

t∗

t∗

0

1
(cid:96)t∗

1 − t∗

(cid:96)πJ /xB,C

(cid:96)

(cid:96)

=

xB,C >

(χ,µ) /∈Ξb

xB,C = πJ ,

ζχ,µ ≤ (cid:96)πJ /xB,C. This ﬁnishes the proof

of Lemma 4.6.
Proof of Lemma 4.7. Focus on a good pair (χ, µ) and the β it deﬁned: βi = aχi,C for every i ∈ B.
i∈B βi = aχB,C ≥

leading to a contradiction. Thus, we have that(cid:80)
We call α the demand vector and β the supply vector. Since (χ, µ) is good, (cid:80)
xB,C =(cid:80)
demands; each color is correspondent to a client j ∈ C. Notice that αv = (cid:80)
a(cid:80)

We satisfy the demands in two steps.
In the ﬁrst step, we give colors to the supplies and
j∈C xUv,j and βi =
j∈C χi,j. For every v ∈ J, j ∈ C, xUv,j units of demand at v has color j; for every i ∈ B, j ∈ C,
aχi,j units of supply at i have color j. In this step, we match the supply and demand using the
following greedy rule: while for some j ∈ C, i, i(cid:48) ∈ B, there is unmatched demand of color j at v

v∈J αv. Thus we can satisfy all the demands and EMD(α, β) is not ∞.

22

(cid:88)
≤ (cid:88)

v∈J,i∈Uv

v∈J,i∈Uv,j∈C

(cid:88)

v∈J,i∈Uv

(cid:88)

(cid:88)

j∈C

and there is unmatched supply of color j at i, we match them as much as possible. The cost for
this step is at most the total cost of moving all supplies and demands of color j to j, i.e,

xi,j(d(v, i) + d(i, j)) + a

χi,jd(i, j)

(cid:88)

i∈B,j∈C

xi,Cd(v, i) +

(cid:88)
j∈C max{xB,j − aχB,j, 0} ≤ (cid:80)
After this step, we have (cid:80)

≤ O(1)DB +

(xi,j + aχi,j)d(i, j),

(xi,j + aχi,j)d(i, j)

i∈B,j∈C

i∈B,j∈C

unmatched demand.

j∈C max{xB,j − χB,j, 0} units of
In the second step, we match remaining demand and the supply. For every v ∈ J, i ∈ Uv, we
move the remaining supply at i to v. After this step, all the supplies and the demands are at J;
then we match them arbitrarily. The total cost is at most

by Lemma 3.2.

aχi,Cd(i, v) +

max{xB,j − χB,j, 0} × diam(J),

(23)

where diam(J) is the diameter of J.

Notice that ˆE[χi,j] ≤ 2(cid:96)xi,j since Pr(χ,µ)∼ζ[(χ, µ) good] ≥ 1/(2(cid:96)) and E

expected cost of the ﬁrst step is at most O(1)DB + O((cid:96))(cid:80)
the expected value of the ﬁrst term of (23) is at most O((cid:96))(cid:80)

(χ,µ)∼ζ χi,j = xi,j. The
j∈C,i∈B xi,jd(i, j) = O((cid:96))DB. Similarly,
xi,Cd(i, v) ≤ O((cid:96))DB by

v∈J,i∈Uv

Lemma 3.2.

Consider the second term of (23). Notice that ˆE[max{xB,j − χB,j, 0}] ≤ xB,j. Also,

ˆE[max{xB,j − χB,j, 0}] = ˆE[max{(1 − χB,j) − (1 − xB,j), 0}]

≤ ˆE[1 − χB,j] ≤ 2(cid:96)(1 − xB,j).

j∈C max{xB,C − χB,C, 0}(cid:3) ≤

Summing up the inequality over all clients j ∈ C, we have ˆE(cid:2)(cid:80)
(cid:2)|S| ≤ (cid:98)sψ(cid:99)(cid:3) and we deﬁne ψ(cid:48)

So, ˆE max{xB,j−χB,j, 0} ≤ min{xB,j, 2(cid:96)(1−xB,j)} ≤ 3(cid:96)xB,j(1−xB,j): if xB,j ≥ 1−1/(2(cid:96)) ≥ 2/3,
then we have 2(cid:96)(1 − xB,j) ≤ 2(cid:96)(1 − xB,j) · (3xB,j/2) = 3(cid:96)xB,j(1 − xB,j); if xB,j < 1 − 1/(2(cid:96)), then
1 − xB,j > 1/(2(cid:96)), implying xB,j ≤ 2(cid:96)xB,j(1 − xB,j).
O((cid:96))πJ . So, the expected value of the second term of (23) is at most O((cid:96))πJ·diam(J) ≤ O((cid:96)2)πJ L(J) ≤
O((cid:96)2)DB, by Lemma 4.2. This ﬁnishes the proof of Lemma 4.7.
Proof of Lemma 4.8. If sψ − (cid:98)sψ(cid:99) ≤ 1 − 1/(cid:96), then we shall throw away the pairs with |S| > (cid:98)sψ(cid:99).
S,β = ψS,β/Q if |S| ≤ (cid:98)sψ(cid:99) and
More formally, let Q = Pr(S,β)∼ψ
S,β = 0 if |S| ≥ (cid:98)sψ(cid:99) + 1. So, Property (4.8b) is satisﬁed. By Markov inequality, we have that
ψ(cid:48)
Q ≥ 1 − sψ/((cid:98)sψ(cid:99) + 1) = ((cid:98)sψ(cid:99) − sψ + 1)/((cid:98)sψ(cid:99) + 1) ≥ (1/(cid:96))/((cid:98)sψ(cid:99) + 1) ≥ 1/((cid:96)(cid:96)1 + (cid:96)) since
sψ ≤ (cid:96)1 = O((cid:96)). Thus, ψ(cid:48)
S,β ≤ O((cid:96)2)ψS,β for every pair (S, β), implying Property (4.8a). Every
pair (S, β) in the support of ψ(cid:48) has |S| ≤ (cid:98)sψ(cid:99) and thus Property (4.8c) holds.
Now, consider the case where sψ − (cid:98)sψ(cid:99) > 1 − 1/(cid:96). In this case, sψ is a fractional number. Let
ψ(cid:48)(cid:48) be the distribution obtained from ψ by conditioning on pairs (S, β) with |S| ≤ (cid:100)sψ(cid:101). By Markov
inequality, we have Pr(S,β)∼ψ
(S,β)∼ψ(cid:48)(cid:48) |S| ≤ sψ
sψ ≤ (cid:96)1 = O((cid:96)). So, ψ(cid:48)(cid:48)

(cid:2)|S| ≤ (cid:100)sψ(cid:101)(cid:3) ≥ 1 − sψ/((cid:100)sψ(cid:101) + 1) ≥ 1 − sψ/(sψ + 1) ≥ 1/((cid:96)1 + 1) as

S,β ≤ O((cid:96))ψS,β for every pair (S, β). Moreover, we have E

23

since we conditioned on the event that |S| is upper-bounded by some-threshold; all pairs (S, β) in
the support of ψ(cid:48)(cid:48) have |S| ≤ (cid:100)sψ(cid:101).
Then we modify ψ(cid:48)(cid:48) to obtain the ﬁnal distribution ψ(cid:48). Notice that for a pair (S, β) with
|S| ≤ (cid:98)sψ(cid:99), we have sψ − |S| ≤ sψ ≤ 2(cid:96)1(sψ − (cid:98)sψ(cid:99)). Thus,
(cid:88)
(cid:88)

S,β(sψ − (cid:98)sψ(cid:99)) ≥ 1
ψ(cid:48)(cid:48)
2(cid:96)1

S,β(sψ − |S|)
ψ(cid:48)(cid:48)

(S,β):|S|≤(cid:98)sψ(cid:99)

(S,β):|S|≤(cid:98)sψ(cid:99)

(cid:88)

S,β((cid:100)sψ(cid:101) − sψ),
ψ(cid:48)(cid:48)

≥ 1
2(cid:96)1

(S,β):|S|=(cid:100)sψ(cid:101)

S,β = ψ(cid:48)(cid:48)

(S,β)∼ψ(cid:48)(cid:48) |S| ≤ sψ.

where the second inequality is due to E
we deﬁne ψ(cid:48)

For every pair (S, β) with |S| ≤ (cid:98)sψ(cid:99), let ψ(cid:48)
(cid:80)
the ψ(cid:48) vector so that we have(cid:80)
(S,β):|S|=(cid:100)sψ(cid:101) ψ(cid:48)

S,β/(2(cid:96)1). Due to the above inequality, we have(cid:80)
S,β(|S| − sψ), implying(cid:80)

S,β. For every pair (S, β) such that |S| = (cid:100)sψ(cid:101),
S,β(sψ−(cid:98)sψ(cid:99)) ≥
S,β max{|S| − sψ,(cid:98)sψ(cid:99) − sψ} ≤ 0. Finally, we scale
(S,β) ψ(cid:48)
S,β = 1; Properties (4.8b) and (4.8c) hold. The scaling factor
S,β ≤ O((cid:96)2)ψS,β for every pair (S, β) and Property (4.8a)

is at most 2(cid:96)1 = O((cid:96)). Overall, we have ψ(cid:48)
holds. This ﬁnishes the proof of Lemma 4.8.

(S,β):|S|≤(cid:98)sψ(cid:99) ψ(cid:48)

S,β = ψ(cid:48)(cid:48)

(S,β) ψ(cid:48)

equivalent to(cid:80)

C Missing Proofs from Section 5
J = 1 − qJ and focus on ψ and
Proof of Lemma 5.1. To avoid negative coeﬃcients, we shall let q(cid:48)
q(cid:48) variables. Consider the set of tight constraints that deﬁne a vertex point. The tight constraints
from (17), (18) and (19) deﬁne a matroid base polytope for a laminar matroid.
For each J ∈ J , every pair (S, β) in the support of φJ has |S| ≈ sJ . Thus, Constraint (20) is
J ≈ yU (J) + 1−(cid:98)sJ(cid:99). This is true since Constraint (18) holds
and ψJ is a distribution. We do the same transformation for Constraints (21) and (22). It is easy
to see that the tight constraints from (20), (21) and (22) also deﬁne the matroid-base-polytope for
a laminar matroid.
P is integral.

Thus, by the classic matroid theory, the set of tight constraints deﬁne a integral solution; thus

S,β(|S|−(cid:98)sJ(cid:99)) + q(cid:48)

S,β ψJ

24

