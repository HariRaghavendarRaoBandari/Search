6
1
0
2

 
r
a

 

M
4
2

 
 
]

.

C
O
h
t
a
m

[
 
 

2
v
2
4
6
5
0

.

3
0
6
1
:
v
i
X
r
a

OptimalBlack-BoxReductionsBetweenOptimizationObjectivesZeyuanAllen-Zhuzeyuan@csail.mit.eduPrincetonUniversityEladHazanehazan@cs.princeton.eduPrincetonUniversityﬁrstcirculatedonFebruary5,2016∗AbstractThediverseworldofmachinelearningapplicationshasgivenrisetoaplethoraofalgorithmsandoptimizationmethods,ﬁnelytunedtothesmoothness,convexity,andotherparameteri-zationsoftheobjective.Inthispaperweattempttosimplifyandreducethecomplexityofalgorithmdesignformachinelearningbyreductions:wedevelopreductionsthattakeamethoddevelopedforonesettingandapplyittotheentirespectrumofsmoothnessandstrong-convexityfoundinpractice.Weshowhowthesenewreductionsgiverisetofasterrunningtimesontraininglinearclas-siﬁersforcertainfamiliesoflossfunctions,andthatourreductionsareoptimalandcannotbeimprovedingeneral.Weconcludewithexperimentsshowingourreductionssuccessfullytransformmethodsbetweendomainsandachievethedesiredperformancepredictedbytheory.1IntroductionThebasicmachinelearningoptimizationproblemofminimizingalossfunctionandregularizeroverasetofexamplesforagivenhypothesisclasscomesinnumerousdiﬀerentvariationsandnames.Examplesincluderidgeregression,lasso,supportvectormachines,logisticregressionandmanymore.Inrecentyearsmultitudeofoptimizationmethodswereintroduced,manytimesspecializedtotheparticularconvexityandsmoothnesspropertiesofthecorrespondingoptimizationformulation.Witheverincreasingspecializedmethodsandsettings,thetasksofcomparingdiﬀerentmeth-odsandchoosingtheappropriateone,orevenkeepingtrackofthediﬀerentmethods,becomescomplicatedevenfortheexpert.Inthispaperweexplorereductionsthattakeanalgorithmdevisedforonesetting,andapplyittotheentirespectrumofstrong-convexityandsmoothnessparameters.Theadvantagesofourapproachare:•shavingoﬀanon-optimallog(1/ε)factorincurredfromclassicalreductionsinmostsettings;•simplerandfastercomparisonofdiﬀerentmethodsforanydomain;•allowthealgorithmdesignertofocusonlyononecase,andimmediatelyobtainresultsmorebroadly;•byapplyingourreductionstoexistingmethodssuchasSVRG,SDCA,orevenacceleratedonessuchasAPCG,weobtainfaster,directmethodfortrainingERMproblemsinsettingsthatthecitedmethodwerenotoriginallydesignedfor.∗FirstappearedonArXivonMarch17,2016.Correctedafewtyposinthismostrecentversion.1Formally,ourfocusisminimizingacompositeobjectivefunctionminx∈Rd(cid:8)F(x)def=f(x)+ψ(x)(cid:9)(1.1)where,f(x)isadiﬀerentiableconvexfunctionandψ(x)isarelativelysimple(butpossiblynon-diﬀerentiable)convexfunction,sometimesreferredtoastheproximalfunction.Weconsidertheproblemofﬁndinganapproximateminimizerx∈RdsatisfyingF(x)≤F(x∗)+ε,wherex∗isaminimizerofF(x).RecallthatthereareafewinterestingcategoriesofthefunctionF(x)andeachofthemcorre-spondtosomewell-knowntrainingproblemsinmachinelearning.Supposewearegivenntrainingexamples{(a1,b1),...(an,bn)},whereeachai∈Rdisthefeaturevectorofexampleiandeachbi∈Risthelabelofexamplei.Case1:ψ(x)isσstronglyconvexandf(x)isL-smooth.Examples:•ridgeregression:f(x)=12nPni=1(hai,xi−bi)2andψ(x)=σ2kxk22.•elasticnet:f(x)=12nPni=1(hai,xi−bi)2andψ(x)=σ2kxk22+λkxk1.Case2:ψ(x)isnotstronglyconvexandf(x)isL-smooth.Examples:•Lasso:f(x)=12nPni=1(hai,xi−bi)2andψ(x)=λkxk1.•‘1logisticregression:f(x)=1nPni=1log(1+exp(−bihai,xi))andψ(x)=λkxk1.Case3:ψ(x)isσstronglyconvexandf(x)isnon-smooth(butLipschitzcontinuous).Examples:•SVM:f(x)=1nPni=1max{0,1−bihai,xi}andψ(x)=σkxk22.Case4:ψ(x)isnotstronglyconvexandf(x)isnon-smooth(butLipschitzcontinuous).Examples:•‘1-SVM:f(x)=1nPni=1max{0,1−bihai,xi}andψ(x)=λkxk1.Inmanymachinelearningapplications,thefunctionf(x)iscanbewrittenasanaverageofnfunctionsfi(hai,xi)wherefi:R→R,thatis,f(x)=1nPni=1fi(hx,aii).Werefertothisastheﬁnite-sumcaseof(1.1).Case1isperhapsthemoststudiedcategoryinconvexoptimization.Forinstance,itisfamouslyknownthatbyapplying(proximal)gradientdescentstartingfromvectorx0,onecanobtainanε-approximateminimizerofF(x)inT=O(cid:0)LσlogF(x0)−F(x∗)ε(cid:1)iterations[17].ThiscanbeimprovedtoT=O(√L√σlogF(x0)−F(x∗)ε(cid:1)iftheso-called(proximal)acceleratedgradientdescentisused,see[16]orConstantStepSchemein[17].Mostﬁrst-orderalgorithmsforCase1enjoyalinearconvergencerate,thatis,alogarithmicdependenceonlog1εintherunningtime.However,manyalgorithmsforCase1donotdirectlyapplytotheotherthreecases,andthisforcesresearcherstotakeoneofthefollowingtwoapproaches.Theﬁrstapproachistodesignanon-stronglyconvexvariantoranon-smoothvariantwhichusuallyrequiresnon-trivialchangesofthealgorithm.Forinstance,theacceleratedgradientdescentmethodhasitsnon-stronglyconvexvariant(seeGeneralSchemein[17])anditsnon-smoothvariant[18].Thesecondapproachistoapplyareductionfromnon-stronglyconvexornon-smoothobjectivesintoastronglyconvexandsmoothone,viatheclassicalregularizationorsmoothingtechniques:(see[7]Section2.1foramoredetailedtreatment)•ClassicalRegularizationReduction.Givenanon-stronglyconvexobjectiveF(x)andsupposethedesiredtrainingerrorisε,onecandeﬁneanewobjectiveF0(x)def=F(x)+σ2kx0−xk2inwithσontheorderofε.Then,onecanapplyamethodthatminimizesthisstronglyconvexobjectiveF0(x)instead.2Forinstance,ifF(x)isL-smoothandoneappliesacceleratedgradientmethodtominimizeF0(x)thatconvergesinO(pL/σlog(1/ε)iterations,thisyieldsanalgorithmthatconvergesinO(pL/εlog1ε)iterationsforminimizingF(x).•ClassicalSmoothingReduction(finitesum).Givenanon-smoothobjectiveF(x)wheref(x)=1nPni=1fi(hx,aii)isofaﬁnite-sumform,1onecandeﬁneasmoothedvariantofbfi(α)foreachfi(α)andsetF0(x)=1nPni=1bfi(hai,xi)+ψ(x).Informally,weneedthissmoothedvarianttosatisfy|bfi(α)−fi(α)|≤εforallαsominimizingF0(x)isapproximatelythesameasminimizingF(x).Thiscanbedoneatleastintwoclassicalwaysifbfi(α)isLipschitzcontinuous.2Inbothcases,bfi(α)willbe(1/ε)smooth.Forinstance,ifF(x)isσ-stronglyconvexandoneappliesanacceleratedgradientmethodonF0(x)thatconvergesinO(cid:0)pL/σlog(1/ε)(cid:1)iterations,thisyieldsanalgorithmthatconvergesinO(p1/σεlog1ε)iterationsforminimizingF(x).Unfortunately,neithertheaboveregularizationorsmoothingistight.Forinstance,theoptimaldependenceonεshouldbe1/√εforﬁrst-ordermethodsonsmoothbutnon-stronglyconvexobjec-tives(seeGeneralSchemeof[17]),ascomparedtolog(1/ε)/√εobtainedviaregularization.Theoptimaldependenceonεshouldbe1/√εforﬁrst-ordermethodsonﬁnite-sum,non-smoothbutstronglyconvexobjectives[3,18],ascomparedtolog(1/ε)/√εobtainedviasmoothing.Inthispaper,weprovetheoremsthattightenboththesereductions.Inaddition,forexperimentalists,applyingtheabovereductiongivesonlybiasedalgorithms.Onehastotunetheregularizationorsmoothingparameter,andthealgorithmonlyconvergestotheminimumoftheregularizedorsmoothedproblem,whichcanbeawayfromtheoriginalminimizerofF(x)byadistanceproportionaltotheparameter.Thismakesthealgorithmhardtoapplyinpractice.Forthisreason,manyalgorithmdesignersinventdirectalgorithmstosolveCase2,3,or4.ThisincludesSAG[20],SAGA[4],andMISO[14]fortheﬁnitesumformofCase2;thealgorithms[8,10,19]forsolvingthegeneralformofCase3;aswellasthesaddle-pointbasedalgorithms[3,15,18]forsolvingtheﬁnitesumformofCase4.OurBlack-BoxOracle.Inthispaper,weassumethatwearegivenablack-boxalgorithmAlg(F,x0)forsolvingsolving(1.1)ofCase1thatsatisﬁesthehomogenousobjectivedecrease(HOOD)property.Thatis,itproducesanoutputx0=Alg(F,x0)satisfyingF(x0)−F(x∗)≤F(x0)−F(x∗)4intimeTime(L,σ).Inotherwords,AlgcandecreasetheobjectivevaluedistancetotheminimumbyaconstantfactorintimeTime(L,σ),regardlessofhowlargeorsmallF(x0)−F(x∗)is.WegiveafewexamplealgorithmsthatsatisfyHOOD:•ThefullgradientdescentmethodsolvesCase1andsatisﬁesHOODwithTime(L,σ)=O(L/σ)·CwhereCisthetimeneededtocomputeagradient∇f(x)andperformingaproximalgradientupdate[17].•TheacceleratedgradientdescentmethodofNesterov[16,17],whenputsinitsproximalform,solvesCase1andsatisﬁesHOODwithTime(L,σ)=O(√L/√σ)·C.ManysubsequentworksinthislineofresearchalsosatisfyHOOD,including[2,11,12].1Smoothingreductionistypicallyappliedtotheﬁnitesumformonlybecause,forageneralhighdimensionalfunctionf(x),itssmoothedvariantbf(x)maynotbeeﬃcientlycomputable.2Oneistodeﬁnebfi(α)=Ev∈[−1,1][fi(α+εv)]asanintegraloffoverthescaledunitinterval,seeforinstanceChapter2.3of[7],andtheotheristodeﬁnebfi(α)=maxβ(cid:8)β·α−f∗i(β)−ε2α2}usingtheFencheldualf∗i(β)offi(α),seeforinstance[18].3•TheSVRG[9]methodanditsproximalversion[25]solvetheﬁnite-sumformofCase1andsatisfyHOODwithTime(L,σ)=O(cid:0)n+Lσ(cid:1)·C1whereC1isthetimeneedtocomputeastochasticgradient∇fi(x)andperformingaproximalgradientupdate.ItcanbeveriﬁedthatAdaGrad[5]alsosatisﬁesHOOD.AdaptReg:OurNewRegularizationReduction.ForobjectivesF(x)thatarenotstronglyconvexbutonlyL-smooth,ourAdaptRegcallstheanoraclesatisfyingHOODalogarithmicnumberoftimes.Eachtimewecalltheoracle,weapplyitonF(x)+σ2kx−x0k2foranexponentiallydecreasingvalueofσ.Intheend,AdaptRegproducesanoutputbxsatisfyingF(bx)−F(x∗)≤O(ε)withatotalrunningtimeP∞t=0Time(L,ε·2t).Sincemostalgorithmshaveaninversepolynomialdependenceonσinitsrunningtimeforsolv-ingCase1,whensummingupTime(L,ε·2t)forpositivevaluest,wedonotincurtheadditionalfactorlog1εasopposedtotheoldreduction.Inaddition,AdaptRegisanunbiasedandanytimealgorithm.F(bx)convergestoF(x∗)asthetimegoeswithoutthenecessityofchangingtheparam-eters.Therefore,AdaptRegcanbeinterruptedatanytimeandthelongeritrunsthecloseritgetstotheminimum.Applications.SomeacceleratedgradientmethodsonlyapplytoCase1anddonotdirectlyapplytoCase2[2,11,12].Theyincuranon-optimaloverheadoflog(1/ε)factorforminimizing(weakly)convexandsmoothfunctions,seeforinstanceSection5.4of[12].ApplyingAdaptRegonthesemethods,weobtainnewalgorithmsthatconvergeintimeO(√L/√ε)·CforCase2,andthismatchestheoptimumdependenceonεforﬁrst-ordermethods.TheSVRG[9]methoddoesnotprovideanytheoreticalguaranteefortheﬁnite-sumformofCase2.ByapplyingAdaptRegonSVRG,weobtainanewalgorithmthatrunsintimeO(cid:0)nlog1ε+Lε(cid:1)·C1fortheﬁnite-sumformofCase2.Thisimprovesonthebestknowntheoreticalrunningtimeobtainedbynon-acceleratedmethods,includingO(cid:0)nlog1ε+Lεlog1ε(cid:1)·C1throughtheoldreduction,aswellasO(cid:0)n+Lε(cid:1)·C1throughdirectmethodssuchasSAGA[4]andSAG[20].AdaptSmooth:OurNewSmoothingReduction.ForobjectivesF(x)thatisﬁnite-sum,σ-stronglyconvex,butnotsmooth,ourAdaptSmoothcallsanoraclesatisfyingHOODalogarithmicnumberoftimes.Eachtimewecalltheoracle,weapplyitonasmoothedversionofF(x)(usingFenchelduality)withasmoothingparameterλthatisexponentiallydecreasing.Intheend,AdaptSmoothproducesanoutputbxsatisfyingF(bx)−F(x∗)≤O(ε)withatotalrunningtimeP∞t=0Time(1ε·2t,σ).SincemostalgorithmshasapolynomialdependenceonLinitsrunningtimeforsolvingCase1,whensummingupTime(1ε·2t,σ)forpositivevaluest,wedonotincuranadditionalfactoroflog1εasopposedtotheoldreduction.Inaddition,AdaptSmoothisalsoanunbiasedandanytimealgorithmforthesamereasonasAdaptReg.WealsodemonstratethatAdaptRegandAdaptSmoothcaneﬀectivelyworktogether,providingareductionfromtheﬁnite-sumformofCase4toCase1.WecallthisitJointAdaptRegSmooth.Applications.WecanapplyAdaptSmoothorJointAdaptRegSmoothtotheaforementionedalgo-rithms[2,11,12]inordertosolvetheﬁnite-sumformsofCase3or4.Onecancarefullyverifythat,theresultingalgorithmsareoptimalﬁrst-ordermethods(intermsofthedependenceonε)forsolvingthesetwoclassesofproblemsrespectively.Incontrast,toobtainsuchoptimalmethods,oneusuallyneedsverydiﬀerentprimal-dualapproachessuchas[15]or[3].Theoryvs.Practice.Intheory,notallalgorithmssolvingCase1satisfyHOOD.SomemachinelearningalgorithmssuchasAPCG[13],SPDC[26],AccSDCA[24]andSDCA[23]arenotknowntosatisfyHOOD.Forexample,APCGsolvestheﬁnite-sumformofCase1andproducesanoutputxsatisfyingF(x)−F(x∗)≤εintimeO(cid:0)(cid:0)n+√nL√σ(cid:1)·log(cid:0)Lσε(cid:1)(cid:1)·C1.Thisrunningtimedoesnot4havealogarithmicdependenceonεthathastheformlog(cid:0)F(x0)−F(x∗)ε(cid:1).Inotherwords,APCGmightinprincipletakeamuchlongerrunningtimeinordertodecreasetheobjectivedistancetotheminimumfrom1to1/4,ascomparedtothetimeneededtodecreasefrom10−10to10−10/4.Fortunately,althoughwithouttheoreticalguarantee,thesemethodsalsobeneﬁtfromournewreductions,andweincludeexperimentsinthispapertoconﬁrmsuchﬁndings.AnOpenQuestion.Focusingonﬁrst-orderstochasticgradientmethods,thebestknownde-pendenceonεremainstobelog(1/ε)/√εfortheﬁnite-sumsettingsofCase2and3,andtobelog(1/ε)/εfortheﬁnite-sumsettingofCase4.ThesewereobtainedbymethodssuchasAPCG[13],SPDC[26],andAccSDCA[24].Suchdependencecanbeimprovedto1/√εor1/εrespectivelyiffullgradientmethodsareused;buttothebestofourknowledge,itremainsopenhowtotightensuchεdependenceamongstochasticgradientmethods.Ourpresentpapershedslightononepo-tentialpathtowardsansweringallthethreeopenquestionsatonce:ifonecandesignanacceleratedmethodfortheﬁnite-sumsettingofCase1thathasrunningtimeO(cid:0)(cid:0)n+√nL√σ(cid:1)·log(cid:0)F(x0)−F(x∗)ε(cid:1)·C1,ournewreductionsimmediatelytightentheεdependenceforCase2,3and4.Roadmap.WeincludethedescriptionandanalysisofAdaptReginSection3.WeonlyincludethedescriptionofAdaptSmoothinSection4andleaveitsanalysistoAppendixA.WeleaveboththedescriptionandanalysisofJointAdaptRegSmoothtotheappendix.WeincludeexperimentalresultsinSection6.2PreliminariesInthispaperwedenoteby∇f(x)thefullgradientvectoroffunctionfifitisdiﬀerentiable,orthesubgradientvectoriffisonlyLipschitzcontinuous.Recallsomeclassicaldeﬁnitionsonstrongconvexityandsmoothness.Deﬁnition2.1(Smoothnessandstrongconvexity).Foraconvexfunctionf:Rn→R,•Wesayfisσ-stronglyconvexif∀x,y∈Rn,itsatisﬁesf(y)≥f(x)+h∇f(x),y−xi+σ2kx−yk2.•WesayfisL-smoothif∀x,y∈Rn,itsatisﬁesk∇f(x)−∇f(y)k≤Lkx−yk.Considertheminimizationproblem(1.1)inCase1:letLbethesmoothnessparameteroff(·)andσthestrongconvexityparameterofF(·).Deﬁnition2.2.WesaythatanalgorithmAlg(F,x0)solvingCase1satisﬁesthehomogenousobjectivedecrease(HOOD)propertywithtimeTime(L,σ),ifforeverystartingpointx0,itproducesanoutputx0←Alg(F,x0)suchthatF(x0)−minxF(x)≤F(x0)−minxF(x)4intimeTime(L,σ).3InthispaperwedenotebyCthetimeneededforcomputingafullgradient∇f(x)andperform-ingaproximalgradientupdateoftheformx0←argminx(cid:8)12kx−x0k2+η(h∇f(x),x−x0i+ψ(x))(cid:9).Fortheﬁnite-sumcaseofproblem(1.1),wedenotebyC1thetimeneededforcomputingastochastic(sub-)gradient∇fi(hai,xi)andperformingaproximalgradientupdateoftheformx0←argminx(cid:8)12kx−x0k2+η(h∇fi(hai,xi)ai,x−x0i+ψ(x))(cid:9).Forﬁnite-sumformsof(1.1),Cisusuallyonthemagnitudeofn×C1.3Althoughourdeﬁnitionisonlyfordeterministicalgorithms,iftheguaranteeisprobabilistic,i.e.,E(cid:2)F(x0)(cid:3)−minxF(x)≤F(x0)−minxF(x)4,alltheresultsofthispaperremaintrue.5Algorithm1TheAdaptRegReductionInput:anobjectiveF(·)inCase2(smoothandnotnecessarilystronglyconvex);x0astartingvector,σ0aninitialregularizationparameter,Tthenumberofepochs;analgorithmAlgthatsolvesCase1ofproblem(1.1).Output:bxT.1:bx0←x0.2:fort←0toT−1do3:DeﬁneF(σt)(x)def=σt2kx−x0k+F(x).4:bxt+1←Alg(F(σt),bxt).5:σt+1←σt/2.6:endfor7:returnbxT.3AdaptReg:ReductionfromCase2toCase1Inthissection,wefocusonsolvingCase2ofproblem(1.1):thatis,f(·)isL-smooth,butψ(·)isnotnecessarilystronglyconvex.WeachievesobyreducingtheproblemtoanalgorithmAlgsolvingCase1thatsatisﬁesHOOD.AdaptRegworksasfollows(seeAlgorithm1).AtthebeginningofAdaptReg,wesetbx0toequalx0,anarbitrarygivenstartingvector.AdaptRegconsistsofTepochs.Ateachepocht=0,1,...,T−1,wedeﬁneaσt-stronglyconvexobjectiveF(σt)(x)def=σt2kx−x0k2+F(x).Here,theparameterσt+1=σt/2foreacht≥0andσ0isaninputparametertoAdaptRegthatwillbespeciﬁedlater.WerunAlgonF(σt)(x)withstartingvectorbxtineachepoch,andlettheoutputbebxt+1.AfterallTepochsareﬁnished,AdaptRegsimplyoutputsbxT.(Alternatively,ifonesetsTtobeinﬁnity,AdaptRegcanbeinterruptedatanarbitrarymomentandoutputbxtofthecurrentepoch.)WestateourmaintheoremforAdaptRegbelowandproveitinSection3.1.Theorem3.1.Supposethatinproblem(1.1)f(·)isL-smooth.Letx0beastartingvectorsuchthatF(x0)−F(x∗)≤∆andkx0−x∗k2≤Θ.Then,AdaptRegwithσ0=∆/ΘandT=log2(∆/ε)producesanoutputbxTsatisfyingF(bxT)−minxF(x)≤O(ε)inatotalrunningtimeofPT−1t=0Time(L,σ0·2−t).4Example3.2.WhenAdaptRegisappliedongradientdescentwhichsatisﬁesHOODwithTime(L,σ)=O(L/σ)·C,wesolveCase2withatotalrunningtimePT−1t=0Time(L,σ0·2−t)=O(Time(L,σT))=O(L/σT)·C=O(LΘ/ε)·C.ThismatchestherunningtimeofproximalgradientdescentwhendirectlyappliedtoCase2.WhenAdaptRegisappliedtoanacceleratedgradientdescentmethoddesignedforCase1suchas[1,2,11,12],wesolveCase2withatotalrunningtimePT−1t=0Time(L,σ0·2−t)=O(Time(L,σT))=O(pL/σT)·C=O(pLΘ/ε)·C.ThismatchesthebestknownrunningtimetosolveCase2usingfull-gradientﬁrst-ordermethods,withoutthenecessitytochangethealgorithm.Incontrast,oneusuallyneedstoperformverynon-trivialchangesonanacceleratedgradientmethodinordertotuneittoworkforCase2.WhenAdaptRegisappliedtoSVRG,wesolvetheﬁnite-sumcaseofCase2withatotalrunningtimePT−1t=0Time(L,σ0·2−t)=PT−1t=0O(n+L2tσ0)·C1=O(nlog(∆/ε)+LΘ/ε)·C1.Thisisfaster4IftheHOODpropertyisonlysatisﬁedprobabilisticallyasperFootnote3,ourerrorguaranteealsobecomesprobabilistic,i.e.,E(cid:2)F(bxT)(cid:3)−minxF(x)≤O(ε).Thisisalsotruefortheotherreductiontheoremsofthispaper.6thanapplyingtheoldreductionwhichgivesarunningtimeO(cid:0)(cid:0)n+Lε(cid:1)logLΘε(cid:1)·C1,andalsofasterthanusingdirectmethodssuchasSAGA[4]andSAG[20]whichgivearunningtimeO(cid:0)n+LΘε(cid:1)·C1.3.1ConvergenceAnalysisforAdaptRegForanalysispurposeonly,wedeﬁnext+1tobetheexactminimizerofF(σt)(x).TheHOODpropertyofthegivenoracleAlgensuresthatF(σt)(bxt+1)−F(σt)(xt+1)≤F(σt)(bxt)−F(σt)(xt+1)4.(3.1)Wedenotebyx∗anarbitraryminimizerofF(x),andthefollowingclaimstatesasimplepropertyabouttheminimizersofF(σt)(x):Claim3.3.Wehavekxt+1−x∗k≤kx0−x∗kforeacht≥0.Proof.BythestrongconvexityofF(σt)(x)andthefactthatxt+1isitsexactminimizer,wehaveF(σt)(xt+1)−F(σt)(x∗)≤−σt2kxt+1−x∗k2.UsingthefactthatF(σt)(xt+1)≥F(xt+1),aswellasthedeﬁnitionF(σt)(x∗)=σt2kx∗−x0k2+F(x∗),weimmediatelyhaveσt2kx0−x∗k2−σt2kxt+1−x∗k2≥F(xt+1)−F(x∗)≥0.(cid:3)DeﬁneDtdef=F(σt)(bxt)−F(σt)(xt+1)tobetheinitialobjectivedistancetotheminimumonfunctionF(σt)(·)beforewecallAlginepocht.Atepoch0,wesimplyhavetheupperboundD0=F(σ0)(bx0)−minxF(σ0)(x)≤F(x0)−F(x∗).Foreachepocht≥1,wecomputethatDtdef=F(σt)(bxt)−F(σt)(xt+1)x=F(σt−1)(bxt)−σt−1−σt2kx0−bxtk2−F(σt−1)(xt+1)+σt−1−σt2kx0−xt+1k2y≤F(σt−1)(bxt)−σt−1−σt2kx0−bxtk2−F(σt−1)(xt)−σt−12kxt−xt+1k2+σt−1−σt2kx0−xt+1k2≤F(σt−1)(bxt)−F(σt−1)(xt)+σt−1−σt2kx0−xt+1k2z≤F(σt−1)(bxt)−F(σt−1)(xt)+σt−1−σt2(cid:0)2kx0−x∗k2+2kxt+1−x∗k2(cid:1){≤F(σt−1)(bxt)−F(σt−1)(xt)+2(σt−1−σt)kx0−x∗k2|≤Dt−14+2(σt−1−σt)kx0−x∗k2}=Dt−14+2σtkx0−x∗k2.Above,xfollowsfromthedeﬁnitionofF(σt)(·)andF(σt−1)(·);yfollowsfromthestrongconvexityofF(σt−1)(·)aswellasthefactthatxtisitsminimizer;zfollowsbecauseforanytwovectorsa,bitsatisﬁeska−bk2≤2kak2+2kbk2;{followsfromClaim3.3;|followsfromthedeﬁnitionofDt−1and(3.1);and}usesthechoicethatσt=σt−1/2fort≥1.7Bytelescopingtheaboveinequality,wehaveDT≤D04T+kx0−x∗k2·(cid:0)2σT+2σT−14+···(cid:1)≤14T(F(x0)−F(x∗))+4σTkx0−x∗k2,(3.2)wherethesecondinequalityusesourchoiceσt=σt−1/2again.Insum,weobtainavectorbxTsatisfyingF(bxT)−F(x∗)x≤F(σT)(bxT)−F(σT)(x∗)+σT2kx0−x∗k2y≤F(σT)(bxT)−F(σT)(xT+1)+σT2kx0−x∗k2z=DT+σT2kx0−x∗k2{≤14T(F(x0)−F(x∗))+4.5σTkx0−x∗k2.(3.3)Above,xusesthefactthatF(σT)(x)≥F(x)foreveryx;yusesthedeﬁnitionthatxT+1istheminimizerofF(σT)(·);zusesthedeﬁnitionofDT;and{uses(3.2).Finally,afterappropriatelychoosingσ0andT,(3.3)directlyimpliesTheorem3.1.Remark3.4.WhenapplyingAdaptReg,iftheoriginalproblemF(x)isalreadyσ-stronglyconvex(andthusinCase1),thetotalrunningtimeofthealgorithmbecomesPT−1t=0Time(L,σ+σt)≤Time(L,σ)·O(log∆ε).ThismatchestherunningtimeoftheoracleitselfonsolvingCase1uptoconstantfactors.Inotherwords,AdaptRegnotonlyturnstheoracleintoanunbiasedsolverforCase2,italsosolvesCase1withthesameasymptoticrunningtimecomparingtotheoracleitself.4AdaptSmooth:ReductionfromCase3to1Inthissection,wefocusonsolvingtheﬁnite-sumformofCase3forproblem(1.1).Thatis,minxF(x)=1nnXi=1fi(hai,xi)+ψ(x),whereψ(x)isσ-stronglyconvexandeachfi(·)maynotbesmooth(butisLipschitzcontinuous).Withoutlossofgenerality,weassumekaik=1foreachi∈[n].WesolvethisproblembyreducingittoanalgorithmAlgsolvingtheﬁnite-sumformofCase1thatsatisﬁesHOOD.RecallthefollowingdeﬁnitionusingFenchelconjugate:Deﬁnition4.1.Foreachfunctionfi:R→R,letf∗i(β)def=maxα{α·β−fi(α)}beitsFenchelconjugate.Then,wedeﬁnethefollowingsmoothedvariantoffiparameterizedbyλ>0:f(λ)i(α)def=maxβnβ·α−f∗i(β)−λ2β2o.Accordingly,wedeﬁneF(λ)(x)def=1nnXi=1f(λ)i(hai,xi)+ψ(x).8Algorithm2TheAdaptSmoothReductionInput:anobjectiveF(·)inﬁnite-sumformofCase3(stronglyconvexandnotnecessarilysmooth);x0astartingvector,λ0aninitialsmoothingparameter,Tthenumberofepochs;analgorithmAlgthatsolvestheﬁnite-sumformofCase1forproblem(1.1).Output:bxT.1:bx0←x0.2:fort←0toT−1do3:DeﬁneF(λt)(x)def=1nPni=1f(λt)i(hai,xi)+ψ(x)usingDeﬁnition4.1.4:bxt+1←Alg(F(λt),bxt).5:λt+1←λt/2.6:endfor7:returnbxT.FromthepropertyofFenchelconjugate(seeforinstancethetextbook[21]),weknowthatf(λ)i(·)isa(1/λ)-smoothfunctionandthereforetheobjectiveF(λ)(x)fallsintotheﬁnite-sumformofCase1forproblem(1.1)withL=1/λ.AdaptSmoothworksasfollows(seeAlgorithm2).AtthebeginningofAdaptSmooth,wesetbx0toequalx0,anarbitrarygivenstartingvector.AdaptSmoothconsistsofTepochs.Ateachepocht=0,1,...,T−1,wedeﬁnea(1/λt)-smoothobjectiveF(λt)(x)usingDeﬁnition4.1above.Here,theparameterλt+1=λt/2foreacht≥0andλ0isaninputparametertoAdaptSmooththatwillbespeciﬁedlater.WerunAlgonF(λt)(x)withstartingvectorbxtineachepoch,andlettheoutputbebxt+1.AfterallTepochsareﬁnished,AdaptSmoothsimplyoutputsbxT.(Alternatively,ifonesetsTtobeinﬁnity,AdaptSmoothcanbeinterruptedatanarbitrarymomentandoutputbxtofthecurrentepoch.)WestateourmaintheoremforAdaptSmoothbelowandproveitinAppendixA.Theorem4.2.Supposethatinproblem(1.1),ψ(·)isσstronglyconvexandeachfi(·)isG-Lipschitzcontinuous.Letx0beastartingvectorsuchthatF(x0)−F(x∗)≤∆.Then,AdaptSmoothwithλ0=∆/G2andT=log2(∆/ε)producesanoutputbxTsatisfyingF(bxT)−minxF(x)≤O(ε)inatotalrunningtimeofPT−1t=0Time(2t/λ0,σ).5JointAdaptRegSmooth:FromCase4to1WeshowinAppendixBthatAdaptRegandAdaptSmoothcanworktogethertoreducetheﬁnite-sumformofCase4toCase1.WecallthisreductionJointAdaptRegSmoothanditreliesonajointlyexponentiallydecreasingsequenceof(σt,λt),whereσtistheweightoftheconvexityparameterthatweaddontopofF(x),andλtisthesmoothingparameterthatdetermineshowwechangeeachfi(·).TheanalysisisanalogoustoacarefulcombinationoftheproofsforAdaptRegandAdaptSmooth.6ExperimentsWeperformexperimentstoconﬁrmourtheoreticalspeed-upsobtainedfortheregularizationandsmoothingreductions.Inparticular,weworkonempiricalriskminimizationsforthefollowingthreedatasetsthatcanbepubliclydownloadedfromtheLibSVMwebsite[6]:•thecovtype(binary.scale)dataset(581,012samplesand54features).•themnist(class1)dataset(60,000samplesand780features).91E-101E-091E-081E-071E-061E-051E-041E-031E-021E-011E+00020406080100(a)covtype,λ=10−61E-081E-071E-061E-051E-041E-031E-021E-01020406080100(b)mnist,λ=10−51E-051E-041E-031E-021E-011E+00020406080100(c)rcv1,λ=10−5Figure1:PerformanceComparisonforLassoonthe‘1regularizer.Theyaxisrepresentstheobjectivedistancetominimum,andthexaxisrepresentsthenumberofpassestothedataset.ThebluesolidcurvesrepresentAPCGundertheoldregularizationreduction,andthereddashedcurverepresentsAPCGunderAdaptReg.Forothervaluesofλ,ortheresultsonSDCA,pleaserefertoFigure3and4intheappendix.•thercv1(train.binary)dataset(20,242samplesand47,236features).Tomakeeasiercomparisonacrossdatasets,foreachdataset,wescaleeveryvectorbytheaverageEuclideannormofallthevectors.Inotherwords,weensurethatthedatavectorshaveanaverageEuclideannorm1.Thisstepisforcomparisononlyandnotnecessaryinpractice.6.1ExperimentsonAdaptRegTotesttheperformanceofAdaptReg,inthissubsectionweconsidertheLassotrainingproblemwhichisnotstronglyconvexbutsmooth(i.e.,ﬁnite-sumformofCase2).WeapplyAdaptRegtoreduceittoCase1andapplyeitherAPCG[13],anacceleratedmethod,or(Prox-)SDCA[22,23],anon-acceleratedmethod.Letusmakeafewremarks:•APCGandSDCAarebothindirectsolversfornon-stronglyconvexobjectivesandthereforeregularizationisintrinsicallyrequiredinordertorunthemforLASSOormoregenerallyCase2.5•APCGandSDCAdonotsatisfyHOODintheory.However,theystillbeneﬁtfromAdaptRegasweshallsee,demonstratingthepracticalstrengthofAdaptReg.Weusethedefaultstep-lengthchoiceforAPCGwhichrequiressolvingaquadraticunivariatefunctionperiteration;forSDCA,toavoidtheissuefortuningsteplengths,weusethesteepestdescent(i.e.,automatic)choicewhichisOptionIforSDCA(see[22]).APracticalImplementation.Inprinciple,onecanimplementAdaptRegbysettingthetermi-nationcriteriaoftheoracleintheinnerloopaspreciselysuggestedbythetheory,suchassettingthenumberofiterationsforSDCAtobeexactlyT=O(n+Lσt)forthet-thepoch.However,inpractice,itismoredesirabletoautomaticallyterminatetheoraclewhenevertheobjectivedistancetotheminimumhasbeensuﬃcientlydecreased,say,byafactorof4.Unfortunately,theoracle(SDCAorAPCG)doesnotknowtheexactminimizerandcannotcomputetheexactobjectivedistancetotheminimum(i.e.,Dt).SincebothSDCAandAPCGareprimal-dualmethods,inourexperiments,wecomputeinsteadthedualitygapwhichgivesareasonableapproximationonDt.Morespeciﬁcally,forbothexperiments,wecomputethedualitygapeveryn/3iterationsinsidetheimplementationofAPCG/SDCA,andterminateitwhenever5Notethatsomeothermethods,suchasSVRG,althoughonlyprovidingtheoreticalresultsforstronglyconvexandsmoothobjectives(Case1),inpracticeworksforCase2directly.Therefore,itisnotneededtoapplyAdaptRegonsuchmethodsatleastinpractice.101E-071E-061E-051E-041E-031E-021E-011E+00020406080100(a)covtype,σ=10−51E-071E-061E-051E-041E-031E-021E-011E+00020406080100(b)mnist,σ=10−41E-041E-031E-021E-011E+00020406080100(c)rcv1,σ=10−4Figure2:PerformanceComparisonforL2-SVMonthe‘2regularizer.Theyaxisrepresentstheobjectivedistancetominimum,andthexaxisrepresentsthenumberofpassestothedataset.ThebluesolidcurvesrepresentSVRGundertheoldsmoothingreduction,andthereddashedcurverepresentsSVRGunderAdaptSmooth.Forothervaluesofσ,pleaserefertoFigure5intheappendix.thedualitygapisbelow1/4timesthelastrecordeddualitygapofthepreviousepoch.Althoughonecanfurthertunethisparameter1/4forabetterperformance,toperformafaircomparison,wesimplysetittobeidentically1/4acrossallthedatasetsandanalysistasks.ExperimentalResults.Foreachdataset(covtype/mnist/rcv1),weconsiderthreediﬀerentmagnitudesofregularizationweightsforthe‘1regularizerintheLassoobjective.Thistotals9analysistasksforeachalgorithm.Foreachsuchatask,weﬁrstimplementtheoldreductionbyaddinganadditionalσ2kxk2termtotheLassoobjectiveandthenapplyAPCGorSDCA.Weconsiderdiﬀerentvaluesofσintheset{10k,3·10k:k∈Z}andshowthemostrepresentativesixofthemintheplots(bluesolidcurvesinFigure3andFigure4).Notethatforalargervalueofσ,theoldreductionconvergesfasterbutconvergestoapointthatisfartherfromtheexactminimizer.WethenimplementAdaptRegwherewechoosetheinitialregularizationparameterσ0alsofromtheset{10k,3·10k:k∈Z}andpresentthebestoneineachof18plots(reddashedcurvesinFigure3andFigure4).Duetospacelimitations,weprovideonly3ofthe18plotsformedium-sizedλandforAPCGinthemainbodyofthispaper(seeFigure1),andincludeFigure3andFigure4onlyintheappendix.ItisclearfromourexperimentsthatAdaptRegismoreeﬃcientthantheoldregularizationreduction.Perhapsmoreimportantly,AdaptRegisunbiasedandgreatlysimpliﬁestheparametertuningprocedure.Itiseasytoﬁndthebestσ0forAdaptRegwhichworksforalldesiredtrainingerrors.Thisisincontrasttotheoldreductionwhereifthedesirederrorissomehowchangedfortheapplication,onehastoselectadiﬀerentσandrestartthealgorithm.6.2ExperimentsonAdaptSmoothTotesttheperformanceofAdaptSmooth,inthissubsectionweconsidertheL2SVMtrainingprob-lemwhichissmoothbutstronglyconvex(i.e.,ﬁnite-sumformofCase3).WeapplyAdaptSmoothtoreduceittoCase1andapplySVRG[9].WeemphasizethatSVRGisanindirectsolverfornon-smoothobjectivesandthereforeregularizationisintrinsicallyrequiredinordertorunitforL2SVMormoregenerallyCase3.66Notethatsomeothermethods,suchasAPCGorSDCA,althoughonlyprovidingtheoreticalguaranteesforstronglyconvexandsmoothobjectives(Case1),inpracticeworkforCase2directlywithoutsmoothing(seeforinstancethediscussionin[22]).Therefore,itisunnecessarytoapplyAdaptSmoothtosuchmethodsatleastinpractice.11APracticalImplementation.Inprinciple,onecanimplementAdaptSmoothbysettingtheterminationcriteriaoftheoracleintheinnerloopaspreciselysuggestedbythetheory,suchassettingthenumberofiterationsforSVRGtobeexactlyT=O(n+1σλt)forthet-thepoch.However,inpractice,itismoredesirabletoautomaticallyterminatetheoraclewhenevertheobjectivedistancetotheminimumhasbeensuﬃcientlydecreased,say,byafactorof4.Unfortunately,SVRGdoesnotknowtheexactminimizerandcannotcomputetheexactobjec-tivedistancetotheminimum(i.e.,Dtinouranalysis).NeithercanwecomputethedualitygaplikeinSection6.2becauseSVRGisaprimal-onlymethod.Inourexperiment,wecomputeinsteadtheEuclideannormofthefullgradientoftheobjective(i.e.,k∇f(x)k)whichgivesareasonableapproximationonDt.Morespeciﬁcally,weusethedefaultsettingofSVRGOptionIthatistocomputea“gradientsnapshot”every2niterations.Whenagradientsnapshotiscomputed,wecanalsocomputeitsEuclideannormalmostforfree.Ifthisnormisbelow1/3timesthelastnorm-of-gradientofthepreviousepoch,weterminateSVRGforthecurrentepoch.Notethatonecanfurthertunethisparameter1/3forabetterperformance;however,toperformafaircomparisoninthispaper,wesimplysetittobeidentically1/3acrossallthedatasetsandanalysistasks.ExperimentalResults.Foreachdataset(covtype/mnist/rcv1),weconsiderthreediﬀerentmagnitudesofregularizationweightsforthe‘2regularizerintheL2SVMobjective.Thistotals9analysistasks.Foreachsuchatask,weﬁrstimplementtheoldreductionbysmoothingthehingelossfunctions(usingDeﬁnition4.1)withparameterλ>0andthenapplySVRG.Weconsiderdiﬀerentvaluesofλintheset{10k,3·10k:k∈Z}andshowthemostrepresentativesixofthemintheplots(bluesolidcurvesinFigure5).Foralargerλ,theoldreductionconvergesfasterbutconvergestoapointthatisfartherfromtheexactminimizer.WethenimplementAdaptSmoothwherewechoosetheinitialsmoothingparameterλ0alsofromtheset{10k,3·10k:k∈Z}andpresentthebestoneineachofthe9plots(reddashedcurvesinFigure5).Duetospacelimitations,weprovideonly3ofthe9plotsforsmall-sizedσinthemainbodyofthispaper(seeFigure2,andincludeFigure5onlyintheappendix.ItisclearfromourexperimentsthatAdaptSmoothismoreeﬃcientthantheoldone,especiallywhenthedesiredtrainingerrorissmall.Perhapsmoreimportantly,AdaptSmoothisunbiasedandgreatlysimpliﬁestheparametertuningprocedure.Itiseasytoﬁndthebestλ0forAdaptSmoothwhichworksforalldesiredtrainingerrors.Thisisincontrasttotheoldreductionwhereifthedesirederrorissomehowchangedfortheapplication,onehastoselectadiﬀerentλandrestartthealgorithm.AcknowledgementsWethankYangYuanforveryenlighteningconversations,andAlonGonenforcatchingafewtyposinanearlierversionofthispaper.References[1]ZeyuanAllen-ZhuandLorenzoOrecchia.Linearcoupling:Anultimateuniﬁcationofgradientandmirrordescent.ArXive-prints,abs/1407.1537,July2014.[2]S´ebastienBubeck,YinTatLee,andMohitSingh.AgeometricalternativetoNesterov’sacceleratedgradientdescent.ArXive-prints,abs/1506.08187,June2015.12[3]AntoninChambolleandThomasPock.Aﬁrst-orderprimal-dualalgorithmforconvexproblemswithapplicationstoimaging.JournalofMathematicalImagingandVision,40(1):120–145,2011.[4]AaronDefazio,FrancisBach,andSimonLacoste-Julien.SAGA:AFastIncrementalGradientMethodWithSupportforNon-StronglyConvexCompositeObjectives.InAdvancesinNeuralInformationProcessingSystems,NIPS2014,2014.[5]JohnDuchi,EladHazan,andYoramSinger.AdaptiveSubgradientMethodsforOnlineLearn-ingandStochasticOptimization.J.Mach.Learn.Res.,12:2121–2159,2011.[6]Rong-EnFanandChih-JenLin.LIBSVMData:Classiﬁcation,RegressionandMulti-label.Accessed:2015-06.[7]EladHazan.DRAFT:Introductiontoonlineconvexoptimimization.FoundationsandTrendsinMachineLearning,XX(XX):1–168,2015.[8]EladHazanandSatyenKale.Beyondtheregretminimizationbarrier:Optimalalgorithmsforstochasticstrongly-convexoptimization.TheJournalofMachineLearningResearch,15(1):2489–2512,January2014.[9]RieJohnsonandTongZhang.Acceleratingstochasticgradientdescentusingpredictivevari-ancereduction.InAdvancesinNeuralInformationProcessingSystems,NIPS2013,pages315–323,2013.[10]SimonLacoste-Julien,MarkW.Schmidt,andFrancisR.Bach.Asimplerapproachtoob-tainingano(1/t)convergenceratefortheprojectedstochasticsubgradientmethod.ArXive-prints,abs/1212.2002,2012.[11]YinTatLeeandAaronSidford.Eﬃcientacceleratedcoordinatedescentmethodsandfasteralgorithmsforsolvinglinearsystems.InFoundationsofComputerScience(FOCS),2013IEEE54thAnnualSymposiumon,pages147–156.IEEE,2013.[12]LaurentLessard,BenjaminRecht,andAndrewPackard.Analysisanddesignofoptimizationalgorithmsviaintegralquadraticconstraints.CoRR,abs/1408.3595,2014.[13]QihangLin,ZhaosongLu,andLinXiao.AnAcceleratedProximalCoordinateGradientMethodanditsApplicationtoRegularizedEmpiricalRiskMinimization.InAdvancesinNeuralInformationProcessingSystems,NIPS2014,pages3059–3067,2014.[14]JulienMairal.IncrementalMajorization-MinimizationOptimizationwithApplicationtoLarge-ScaleMachineLearning.SIAMJournalonOptimization,25(2):829–855,April2015.PreliminaryversionappearedinICML2013.[15]ArkadiNemirovski.Prox-MethodwithRateofConvergenceO(1/t)forVariationalInequalitieswithLipschitzContinuousMonotoneOperatorsandSmoothConvex-ConcaveSaddlePointProblems.SIAMJournalonOptimization,15(1):229–251,January2004.[16]YuriiNesterov.AmethodofsolvingaconvexprogrammingproblemwithconvergencerateO(1/k2).InDokladyANSSSR(translatedasSovietMathematicsDoklady),volume269,pages543–547,1983.13[17]YuriiNesterov.IntroductoryLecturesonConvexProgrammingVolume:ABasiccourse,vol-umeI.KluwerAcademicPublishers,2004.[18]YuriiNesterov.Smoothminimizationofnon-smoothfunctions.MathematicalProgramming,103(1):127–152,December2005.[19]AlexanderRakhlin,OhadShamir,andKarthikSridharan.Makinggradientdescentoptimalforstronglyconvexstochasticoptimization.InProceedingsofthe29thInternationalConferenceonMachineLearning,ICML’12,2012.[20]MarkSchmidt,NicolasLeRoux,andFrancisBach.Minimizingﬁnitesumswiththestochasticaveragegradient.arXivpreprintarXiv:1309.2388,pages1–45,2013.PreliminaryversionappearedinNIPS2012.[21]ShaiShalev-Shwartz.OnlineLearningandOnlineConvexOptimization.FoundationsandTrendsinMachineLearning,4(2):107–194,2012.[22]ShaiShalev-ShwartzandTongZhang.ProximalStochasticDualCoordinateAscent.arXivpreprintarXiv:1211.2717,pages1–18,2012.[23]ShaiShalev-ShwartzandTongZhang.Stochasticdualcoordinateascentmethodsforregular-izedlossminimization.JournalofMachineLearningResearch,14:567–599,2013.[24]ShaiShalev-ShwartzandTongZhang.AcceleratedProximalStochasticDualCoordinateAs-centforRegularizedLossMinimization.InProceedingsofthe31stInternationalConferenceonMachineLearning,ICML2014,pages64–72,2014.[25]LinXiaoandTongZhang.AProximalStochasticGradientMethodwithProgressiveVarianceReduction.SIAMJournalonOptimization,24(4):2057—-2075,2014.[26]YuchenZhangandLinXiao.StochasticPrimal-DualCoordinateMethodforRegularizedEmpiricalRiskMinimization.InProceedingsofthe32ndInternationalConferenceonMachineLearning,ICML2015,2015.141E-101E-091E-081E-071E-061E-051E-041E-031E-021E-011E+00020406080100(a)covtype,λ=10−51E-101E-091E-081E-071E-061E-051E-041E-031E-021E-011E+00020406080100(b)covtype,λ=10−61E-091E-081E-071E-061E-051E-041E-031E-021E-011E+00020406080100(c)covtype,λ=10−71E-091E-081E-071E-061E-051E-041E-031E-021E-01020406080100(d)mnist,λ=10−41E-081E-071E-061E-051E-041E-031E-021E-01020406080100(e)mnist,λ=10−51E-071E-061E-051E-041E-031E-021E-01020406080100(f)mnist,λ=10−61E-071E-061E-051E-041E-031E-021E-011E+00020406080100(g)rcv1,λ=10−41E-051E-041E-031E-021E-011E+00020406080100(h)rcv1,λ=10−51E-041E-031E-021E-011E+00020406080100(i)rcv1,λ=10−6Figure3:PerformanceComparisonforLassowithweightλonthe‘1regularizer.Theyaxisrepresentstheobjectivedistancetominimum,andthexaxisrepresentsthenumberofpassestothedataset.ThebluesolidcurvesrepresentAPCGundertheoldregularizationreduction,andthereddashedcurverepresentsAPCGunderAdaptReg.AppendixAConvergenceAnalysisforAdaptSmoothWeﬁrstrecallthefollowingpropertythatboundsthediﬀerencebetweenfiandf(λ)iasafunctionofλ:LemmaA.1.Ifeachfi(·)isG-Lipschitzcontinuous,itsatisﬁesfi(α)−λG22≤f(λ)i(α)≤fi(α).Proof.Lettingβ∗def=argmaxβ{β·α−f∗i(β)},wehaveβ∗∈[−G,G]becausethedomainoff∗i(·)equalstherangeof∇fi(·)whichisasubsetof[−G,G]duetotheLipschitzcontinuityoffi(·).As151E-081E-071E-061E-051E-041E-031E-021E-011E+00020406080100(a)covtype,λ=10−51E-081E-071E-061E-051E-041E-031E-021E-011E+00020406080100(b)covtype,λ=10−61E-091E-081E-071E-061E-051E-041E-031E-021E-011E+00020406080100(c)covtype,λ=10−71E-071E-061E-051E-041E-031E-021E-01020406080100(d)mnist,λ=10−41E-071E-061E-051E-041E-031E-021E-01020406080100(e)mnist,λ=10−51E-061E-051E-041E-031E-021E-01020406080100(f)mnist,λ=10−61E-061E-051E-041E-031E-021E-011E+00020406080100(g)rcv1,λ=10−41E-051E-041E-031E-021E-011E+00020406080100(h)rcv1,λ=10−51E-041E-031E-021E-011E+00020406080100(i)rcv1,λ=10−6Figure4:PerformanceComparisonforLassowithweightλonthe‘1regularizer.Theyaxisrepresentstheobjectivedistancetominimum,andthexaxisrepresentsthenumberofpassestothedataset.ThebluesolidcurvesrepresentSDCAundertheoldregularizationreduction,andthereddashedcurverepresentsSDCAunderAdaptReg.aresult,wehavefi(α)=maxβ{β·α−f∗i(β)}=β∗·α−f∗i(β∗)−λ2(β∗)2+λ2(β∗)2≤maxβ{β·α−f∗i(β)−λ2β2}+λ2(β∗)2=f(λ)i(α)+λ2(β∗)2≤f(λ)i(α)+λG22.Theotherinequalityisobvious.(cid:3)WealsonotethatFactA.2.Forλ1≥λ2,wehavef(λ1)i(α)≤f(λ2)i(α)foreveryα∈R.Foranalysispurposeonly,wedeﬁnext+1tobetheexactminimizerofF(λt)(x).TheHOODpropertyofthegivenoracleAlgensuresthatF(λt)(bxt+1)−F(λt)(xt+1)≤F(λt)(bxt)−F(λt)(xt+1)4.(A.1)161E-071E-061E-051E-041E-031E-021E-011E+00020406080100(a)covtype,σ=10−51E-071E-061E-051E-041E-031E-021E-011E+00020406080100(b)covtype,σ=10−61E-071E-061E-051E-041E-031E-021E-011E+00020406080100(c)covtype,σ=10−71E-071E-061E-051E-041E-031E-021E-011E+00020406080100(d)mnist,σ=10−41E-051E-041E-031E-021E-011E+00020406080100(e)mnist,σ=10−51E-041E-031E-021E-011E+00020406080100(f)mnist,σ=10−61E-041E-031E-021E-011E+00020406080100(g)rcv1,σ=10−41E-041E-031E-021E-011E+00020406080100(h)rcv1,σ=10−51E-041E-031E-021E-011E+00020406080100(i)rcv1,σ=10−6Figure5:PerformanceComparisonforL2-SVMwithweightσonthe‘2regularizer.Theyaxisrepresentstheobjectivedistancetominimum,andthexaxisrepresentsthenumberofpassestothedataset.ThebluesolidcurvesrepresentSVRGundertheoldsmoothingreduction,andthereddashedcurverepresentsSVRGunderAdaptSmooth.Wedenotebyx∗theminimizerofF(x),anddeﬁneDtdef=F(λt)(bxt)−F(λt)(xt+1)tobetheinitialobjectivedistancetotheminimumonfunctionF(λt)(·)beforewecallAlginepocht.Atepoch0,wesimplyhavetheupperboundD0=F(λ0)(x0)−F(λ0)(x1)≤F(x0)−F(x1)+λ0G22≤F(x0)−F(x∗)+λ0G22.Above,theﬁrstinequalityisbyLemmaA.1andFactA.2,andthesecondinequalityisbecausex∗istheminimizerofF(·).Next,foreachepocht≥1,wecomputethatDtdef=F(λt)(bxt)−F(λt)(xt+1)≤F(λt−1)(bxt)+λt−1G22−F(λt−1)(xt+1)≤F(λt−1)(bxt)+λt−1G22−F(λt−1)(xt)≤Dt−14+λt−1G22.Above,theﬁrstinequalityisbyLemmaA.1andFactA.2,andthesecondinequalityisbecausextistheminimizerofF(λt−1)(·).17Therefore,bytelescopingtheaboveinequalityandthechoiceλt=λt−1/2,wehavethatDT≤F(x0)−F(x∗)4T+G2·(cid:16)λT−12+λT−28+···(cid:17)≤F(x0)−F(x∗)4T+2λTG2.Insum,weobtainavectorbxTsatisfyingF(bxT)−F(x∗)≤F(λT)(bxT)−F(λT)(x∗)+λTG22≤F(λT)(bxT)−F(λT)(xT+1)+λTG22=DT+λTG22≤14T(F(x0)−F(x∗))+2.5λTG2(A.2)Finally,afterappropriatelychoosingλ0andT,(A.2)directlyimpliesTheorem4.2.BJointAdaptRegSmooth:ReductionfromCase4toCase1Inthissection,weshowthatAdaptRegandAdaptSmoothcanworktogethertosolvetheﬁnite-sumformofCase4.Thatis,minxF(x)=1nnXi=1fi(hai,xi)+ψ(x),whereψ(x)isnotnecessarilystronglyconvexandeachfi(·)maynotbesmooth(butisLipschitzcontinuous).Withoutlossofgenerality,weassumekaik=1foreachi∈[n].WesolvethisproblembyreducingittoanalgorithmAlgsolvingtheﬁnite-sumformofCase1thatsatisﬁesHOOD.Followingthesamedeﬁnitionoff(λ)i(·)inDeﬁnition4.1,inthissection,weconsiderthefollowingregularizedsmoothedobjectiveF(λ,σ)(x):DeﬁnitionB.1.Givenparametersλ,σ>0,letF(λ,σ)(x)def=1nnXi=1f(λ)i(hai,xi)+ψ(x)+σ2kx−x0k2.FromthisdeﬁnitionweknowthatF(λ,σ)(x)fallsintotheﬁnite-sumformofCase1forproblem(1.1)withL=1/λandσbeingthestrongconvexityparameter.JointAdaptRegSmoothworksasfollows(seeAlgorithm3).Atthebeginningofthereduction,wesetbx0toequalx0,anarbitrarygivenstartingvector.JointAdaptRegSmoothconsistsofTepochs.Ateachepocht=0,1,...,T−1,wedeﬁnea(1/λt)-smoothσt-stronglyconvexobjectiveF(λt,σt)(x)usingDeﬁnitionB.1above.Here,theparametersλt+1=λt/2andσt+1=σt/2foreacht≥0,andλ0,σ0aretwoinputparameterstoJointAdaptRegSmooththatwillbespeciﬁedlater.WerunAlgonF(λt,σt)(x)withstartingvectorbxtineachepoch,andlettheoutputbebxt+1.AfterallTepochsareﬁnished,JointAdaptRegSmoothsimplyoutputsbxT.(Alternatively,ifonesetsTtobeinﬁnity,JointAdaptRegSmoothcanbeinterruptedatanarbitrarymomentandoutputbxtofthecurrentepoch.)WestateourmaintheoremforJointAdaptRegSmoothbelowandproveitinSectionC.18Algorithm3TheJointAdaptRegSmoothReductionInput:anobjectiveF(·)inﬁnite-sumformofCase4(notnecessarilystronglyconvexorsmooth);x0startingvector,λ0,σ0initialsmoothingandregularizationparams,Tnumberofepochs;analgorithmAlgthatsolvestheﬁnite-sumformofCase1forproblem(1.1).Output:bxT.1:bx0←x0.2:fort←0toT−1do3:DeﬁneF(λt,σt)(x)def=1nPni=1f(λt,σt)i(hai,xi)+ψ(x)+σt2kx−x0k2usingDeﬁnitionB.1.4:bxt+1←Alg(F(λt),bxt).5:σt+1←σt/2,λt+1←λt/2.6:endfor7:returnbxT.TheoremB.2.Supposethatinproblem(1.1),eachfi(·)isG-Lipschitzcontinuous.Letx0beastartingvectorsuchthatF(x0)−F(x∗)≤∆andkx0−x∗k2≤Θ.Then,JointAdaptRegSmoothwithλ0=∆/G2,σ0=∆/ΘandT=log2(∆/ε)producesanoutputbxTsatisfyingF(bxT)−minxF(x)≤O(ε)inatotalrunningtimeofPT−1t=0Time(2t/λ0,σ0·2−t).ExampleB.3.WhenJointAdaptRegSmoothisappliedtoanacceleratedgradientdescentmethodsuchas[1,2,11,12,16–18],wesolvetheﬁnite-sumformofCase4withatotalrunningtimePT−1t=0Time(2t/λ0,σ0·2−t)=O(Time(1/λT,σT))=O(G√Θ/ε)·C.Thismatchesthebestknownrunningtimeoffull-gradientﬁrst-ordermethodsonsolvingCase4,whichusuallyisobtainedviasaddle-pointbasedmethodssuchasChambolle-Pock[3]orthemirrorproxmethodofNe-mirovski[15].CConvergenceAnalysisforJointAdaptRegSmoothForanalysispurposeonly,wedeﬁnext+1tobetheexactminimizerofF(λt,σt)(x).TheHOODpropertyofthegivenoracleAlgensuresthatF(λt,σt)(bxt+1)−F(λt,σt)(xt+1)≤F(λt,σt)(bxt)−F(λt,σt)(xt+1)4.Wedenotebyx∗anarbitraryminimizerofF(x).ThefollowingclaimstatesasimplepropertyabouttheminimizersofF(λt,σt)(x)whichisanalogoustoClaim3.3:ClaimC.1.Wehaveσt2kxt+1−x∗k2≤σt2kx0−x∗k2+λtG22foreacht≥0.Proof.BythestrongconvexityofF(λt,σt)(x)andthefactthatxt+1isitsexactminimizer,wehaveF(λt,σt)(xt+1)−F(λt,σt)(x∗)≤−σt2kxt+1−x∗k2.UsingthefactthatF(λt,σt)(xt+1)≥F(λt,0)(xt+1)≥F(xt+1)+λtG22(wherethesecondinequalityfollowsfromLemmaA.1),aswellasthedeﬁnitionF(λt,σt)(x∗)=F(λt,0)(x∗)+σt2kx∗−x0k2≤F(x∗)+σt2kx∗−x0k2(wherethesecondinequalityagainfollowsfromLemmaA.1),weimmediatelyhaveλtG22+σt2kx0−x∗k2−σt2kxt+1−x∗k2≥F(xt+1)−F(x∗)≥0.(cid:3)19LetDtdef=F(λt,σt)(bxt)−F(λt,σt)(xt+1)betheinitialobjectivedistancetotheminimumonfunctionF(λt,σt)(·)beforewecallAlginepocht.Atepoch0,wesimplyhavetheupperboundD0=F(λ0,σ0)(x0)−F(λ0,σ0)(x1)x≤F(0,σ0)(x0)−F(λ0,0)(x1)y≤F(x0)−F(x1)+λ0G22z≤F(x0)−F(x∗)+λ0G22.Above,xusesF(λ0,σ0)(x0)≤F(0,σ0)(x0)whichisaconsequenceofFactA.2;yusesF(0,σ0)(x0)=F(x0)fromthedeﬁnitionandF(x1)≤F(λ0,0)(x1)+λ0G22fromLemmaA.1;andzusesthemini-malityofx∗.Next,foreachepocht≥1,wecomputethatDtdef=F(λt,σt)(bxt)−F(λt,σt)(xt+1)x≤F(λt,σt−1)(bxt)−F(λt,σt−1)(xt+1)+σt−1−σt2kxt+1−x0k2y≤F(λt−1,σt−1)(bxt)+λt−1G22−F(λt−1,σt−1)(xt+1)+σt2kxt+1−x0k2z≤F(λt−1,σt−1)(bxt)+λt−1G22−F(λt−1,σt−1)(xt+1)+σtkxt+1−x∗k2+σtkx0−x∗k2{≤F(λt−1,σt−1)(bxt)+λt−1G22−F(λt−1,σt−1)(xt)+2σtkx0−x∗k2+λtG2|≤Dt−14+2σtkx0−x∗k2+2λtG2.Above,xfollowsfromthedeﬁnition;yfollowsfromLemmaA.1,FactA.2aswellasthechoiceσt−1=2σt;zfollowsbecauseforanytwovectorsa,bitsatisﬁeska−bk2≤2kak2+2kbk2;{followsfromClaimC.1;|followsfromthedeﬁnitionofDt−1,from(3.1),andfromthechoiceλt−1=2λt.Bytelescopingtheaboveinequality,wehaveDT≤F(x0)−F(x∗)4T+G2·(cid:16)2λT+2λT−14+···(cid:17)+kx0−x∗k2·(cid:16)2σT+2σT−14+···(cid:17)≤14T(F(x0)−F(x∗))+4λTG2+4σTkx0−x∗k2,wherethesecondinequalityusesourchoiceλt=λt−1/2andσt=σt−1/2again.Insum,weobtainavectorbxTsatisfyingF(bxT)−F(x∗)x≤F(λT,0)(bxT)−F(0,σT)(x∗)+λTG22+σT2kx0−x∗k2y≤F(λT,σT)(bxT)−F(λT,σT)(x∗)+λTG22+σT2kx0−x∗k2z≤F(λT,σT)(bxT)−F(λT,σT)(xT+1)+λTG22+σT2kx0−x∗k2{≤14T(F(x0)−F(x∗))+4.5λTG2+4.5σTkx0−x∗k2.(C.1)Above,xusesLemmaA.1andthedeﬁnition;yusesthemonotonicityandFactA.2,zusesthedeﬁnitionthatxT+1istheminimizerofF(λT,σT)(·);and{usesthedeﬁnitionofDTandourderivedupperbound.Finally,afterappropriatelychoosingσ0,λ0andT,(C.1)immediatelyimpliesTheoremB.2.20