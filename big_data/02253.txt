Blur Robust Optical Flow using Motion Channel

Wenbin Lia,e, Yang Chenb, JeeHang Leec, Gang Rend, Darren Coskere

aDepartment of Computer Science, University College London, UK

bHamlyn Centre, Imperial College London, UK

cDepartment of Computer Science, University of Bath, UK

dSchool of Digital Art, Xiamen University of Technology, China

eCentre for the Analysis of Motion, Entertainment Research and Applications (CAMERA), University of Bath, UK

6
1
0
2

 
r
a

M
7

 

 
 
]

V
C
.
s
c
[
 
 

1
v
3
5
2
2
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

It is hard to estimate optical ﬂow given a realworld video sequence with camera shake and other motion blur. In this paper, we
ﬁrst investigate the blur parameterization for video footage using near linear motion elements. We then combine a commercial
3D pose sensor with an RGB camera, in order to ﬁlm video footage of interest together with the camera motion. We illustrates
that this additional camera motion/trajectory channel can be embedded into a hybrid framework by interleaving an iterative blind
deconvolution and warping based optical ﬂow scheme. Our method yields improved accuracy within three other state-of-the-art
baselines given our proposed ground truth blurry sequences; and several other realworld sequences ﬁlmed by our imaging system.

Keywords: Optical Flow, Computer Vision, Image Deblurring, Directional Filtering, RGB-Motion Imaging

1. Introduction

Optical ﬂow estimation has been widely applied to computer
vision applications, e.g. segmentation, image deblurring and
stabilization, etc. In many cases, optical ﬂow is often estimated
on the videos captured by a shaking camera. Those footages
may contain a signiﬁcant amount of camera blur that bring ad-
ditional difﬁculties into the traditional variational optical ﬂow
framework. It is because such blur scenes often lead to a fact
that a pixel may match multiple pixels between image pair. It
further violates the basic assumption – intensity constancy – of
the optical ﬂow framework.

In this paper, we investigate the issue of how to precisely esti-
mate optical ﬂow from a blurry video footage. We observe that
the blur kernel between neighboring frames may be near linear,
which can be parameterized using linear elements of the camera
motion. In this case, the camera trajectory can be informatic to
enhance the image deblurring within a variational optical ﬂow
framework. Based on this observation, our major contribution
in this paper is to utilise an RGB-Motion Imaging System – an
RGB sensor combined with a 3D pose&position tracker – in or-
der to propose: (A) an iterative enhancement process for camera
shake blur estimation which encompasses the tracked camera
motion (Sec. 3) and a Directional High-pass Filter (Sec. 4 and
Sec. 7.2); (B) a Blur-Robust Optical Flow Energy formulation
(Sec. 6); and (C) a hybrid coarse-to-ﬁne framework (Sec. 7) for
computing optical ﬂow in blur scenes by interleaving an itera-
tive blind deconvolution process and a warping based minimisa-
tion scheme. In the evaluation section, we compare our method
to three existing state-of-the-art optical ﬂow approaches on our
proposed ground truth sequences (Fig. 1, blur and baseline blur-
free equivalents) and also illustrate the practical beneﬁt of our
algorithm given realworld cases.

Preprint submitted to Neurocomputing

Figure 1: Visual comparison of our method to Portz et al. [1] on our ground
truth benchmark Grove2 with synthetic camera shake blur. First Column: the
input images; Second Column: the optical ﬂow ﬁelds calculated by our method
and the baseline; Third Column: the RMS error maps against the ground truth.

2. Related Work

Camera shake blur often occurs during fast camera move-
ment in low-light conditions due to the requirement of adopt-
ing a longer exposure. Recovering both the blur kernel and
the latent image from a single blurred image is known as Blind
Deconvolution which is an inherently ill-posed problem. Cho
and Lee [2] propose a fast deblurring process within a coarse-
to-ﬁne framework (Cho&Lee) using a predicted edge map as a
prior. To reduce the noise effect in this framework, Zhong et
al. [3] introduce a pre-ﬁltering process which reduces the noise
along a speciﬁc direction and preserves the image information
in other directions. Their improved framework provides high
quality kernel estimation with a low run-time but shows difﬁ-
culties given combined object and camera shake blur.

To obtain higher performance, a handful of combined hard-
ware and software-based approaches have also been proposed

March 8, 2016

036(Pix.)1I2IOursError Map, OursPortz et al.Error Map, Portz et al.Figure 2: RGB-Motion Imaging System. (a): Our system setup using a combined RGB sensor and 3D Pose&Position Tracker. (b): The tracked 3D camera motion
in relative frames. The top-right box is the average motion vector – which has similar direction to the blur kernel. (c): Images captured from our system. The
top-right box presents the blur kernel estimated using [2]. (d): The internal process of our system where the ∆t presents the exposure time.

for image deblurring. Tai et al. [4] introduce a hybrid imag-
ing system that is able to capture both video at high frame rate
and a blurry image. The optical ﬂow ﬁelds between the video
frames are utilised to guide blur kernel estimation. Levin et
al. [5] propose to capture a uniformly blurred image by con-
trolling the camera motion along a parabolic arc. Such uniform
blur can then be removed based on the speed or direction of
the known arc motion. As a complement to Levin el al.’s [5]
hardware-based deblurring algorithm, Joshi et al. [6] apply in-
ertial sensors to capture the acceleration and angular velocity
of a camera over the course of a single exposure. This extra
information is introduced as a constraint in their energy optimi-
sation scheme for recovering the blur kernel. All the hardware-
assisted solutions described provide extra information in addi-
tion to the blurry image, which signiﬁcantly improves overall
performance. However, the methods require complex electronic
setups and the precise calibration.

Optical ﬂow techniques are widely studied and adopted
across computer vision because of dense image correspon-
dences they provide. Such dense tracking is important for
other fundamental research topics e.g. 3D reconstruction [7]
and visual effects [8, 9], etc.
In the last two decades, the
optical ﬂow model has evolved extensively – one landmark
work being the variational model of Horn and Schunck [10]
where the concept of Brightness Constancy is proposed. Un-
der this assumption, pixel intensity does not change spatio-
temporally, which is, however, often weakened in realworld
images because of natural noise. To address this issue, some
complementary concepts have been developed to improve per-
formance given large displacements [11], taking advantage of
feature-rich surfaces [12] and adapting to nonrigid deforma-
tion in scenes [13, 14, 15, 16, 17, 18]. However, ﬂow ap-
proaches that can perform well given blurred scenes – where the
Brightness Constancy is usually violated – are less common.
Of the approaches that do exist, Schoueri et al. [19] perform
a linear deblurring ﬁlter before optical ﬂow estimation while
Portz et al. [1] attempt to match un-uniform camera motion
between neighbouring input images. Whereas the former ap-
proach may be limited given nonlinear blur in realworld scenes;
the latter requires two extra frames to parameterise the motion-
induced blur. Regarding non optical-ﬂow based methods, Yuan
et al. [20] align a blurred image to a sharp one by predeﬁning

an afﬁne image transform with a blur kernel. Similarly HaCo-
hen et al. [21] achieve alignment between a blurred image and
a sharp one by embedding deblurring into the correspondence
estimation. Li et al. [16] present an approach to solve the image
deblurring and optical ﬂow simultaneously by using the RGB-
Motion imaging.

3. RGB-Motion Imaging System

Camera shake blur within video footage is typically due to
fast camera motion and/or long exposure time. In particular,
such blur can be considered as a function of the camera trajec-
tory supplied to image space during the exposure time ∆t. It
therefore follows that knowledge of the actual camera motion
between image pairs can provide signiﬁcant information when
performing image deblurring [6, 5].

In this paper, we propose a simple and portable setup
(Fig. 2(a)), combining an RGB sensor and a 3D pose&position
tracker (SmartNav by NaturalPoint Inc.)
in order to capture
continuous scenes (video footage) along with real-time camera
pose&position information. Note that the RGB sensor could be
any camera or a Kinect sensor – A Canon EOS 60D is applied in
our implementation to capture 1920 × 1080 video at frame rate
of 24 FPS. Furthermore, our tracker is proposed to provide the
rotation (yaw, pitch and roll), translation and zoom information
within a reasonable error range (2 mm). To synchronise this
tracker data and the image recording, a real time collaboration
(RTC) server [22] is built using the instant messaging protocol
XMPP (also known as Jabber1) which is designed for message-
oriented communication based on XML, and allows real-time
responses between different messaging channels or any sig-
nal channels that can be transmitted and received in message
form. In this case, a time stamp is assigned to the received mes-
sage package by the central timer of the server. Those message
packages are synchronised if they contain nearly the same time
stamp. We consider the Jabber for synchronisation because of
its opensource nature and the low respond delay (around 10
ms).

Assuming objects have similar depth within the same scene
(a common assumption in image deblurring which will be dis-

1http://www.jabber.org/

2

Blur KernelGT Camera TrajectoryTimeTracked Camera Motiontttt0I2I3I0I1I2I3I1M2M1I12M1I2I1M2M(c) Image Channel and Close Up(b) Tracked Camera Motion and Close Up(d) RGB-Motion Data Capture Process(a) System SetupRGB Sensor3D Pose&Position Trackercussed in our future work), the tracked 3D camera motion in
image coordinates can be formulated as:

(cid:88)

x

M j =

1
n

(cid:16)
[R|T] X j+1 − X j

(cid:17)

K

(1)

where M j represents the average of the camera motion vec-
tors from the image j to image j + 1. X denotes the 3D position
of the camera while x = (x, y)T is a pixel location and n rep-
resents the number of pixels in an image. K represents the 3D
projection matrix while R and T denote the rotation and transla-
tion matrices respectively of tracked camera motion in the im-
age domain. All these information K, R and T is computed
using Optitrack’s Camera SDK2 (version 1.2.1). Fig 2(b,c)
shows sample data (video frames and camera motion) captured
from our imaging system. It is observed that blur from the re-
alworld video is near linear due to the relatively high sampling
rate of the camera. The blur direction can therefore be approx-
imately described using the tracked camera motion. Let the
tracked camera motion M j = (r j, θ j)T be represented in polar
coordinates where r j and θ j denote the magnitude and direc-
j is a sharing index between
tional component respectively.
tracked camera motion and frame number. In addition, we also
consider the combined camera motion vector of neighbouring
images as shown in Fig 2(d), e.g. M12 = M1 + M2 where
M12 = (r12, θ12) denotes the combined camera motion vector
from image 1 to image 3. As one of our main contributions,
these real-time motion vectors are proposed to provide addi-
tional constraints for blur kernel enhancement (Sec. 7) within
our framework.

4. Blind Deconvolution

The motion blur process can commonly be formulated:

I = k ⊗ l + n

(2)

where I is a blurred image and k represents a blur kernel w.r.t.
a speciﬁc Point Spread Function. l is the latent image of I; ⊗
denotes the convolution operation and n represents spatial noise
within the scene. In the blind deconvolution operation, both k
and l are estimated from I, which is an ill-posed (but extensively
studied) problem. A common approach for blind deconvolution
is to solve both k and l in an iterative framework using a coarse-
to-ﬁne strategy:

k = argmink{(cid:107)I − k ⊗ l(cid:107) + ρ(k)},
l = argminl{(cid:107)I − k ⊗ l(cid:107) + ρ(l)}.

(3)
(4)

where ρ represents a regularization that penalizes spatial
smoothness with a sparsity prior [2], and is widely used in re-
cent state-of-the-art work [23, 12]. Due to noise sensitivity,
low-pass and bilateral ﬁlters [24] are typically employed before
deconvolution. Eq. 5 denotes the common deﬁnition of an op-
timal kernel from a ﬁltered image.

2http://www.naturalpoint.com/optitrack

k f = argmink f{(cid:13)(cid:13)(cid:13)(k ⊗ l + n) ⊗ f − k f ⊗ l
(cid:13)(cid:13)(cid:13) = k ⊗ f

(cid:13)(cid:13)(cid:13)l ⊗ (k ⊗ f − k f )

≈ argmink f

(cid:13)(cid:13)(cid:13) + ρ(k f )}

(5)

where k represents the ground truth blur kernel, f is a ﬁl-
ter, and k f denotes the optimal blur kernel from the ﬁltered im-
age I ⊗ f . The low-pass ﬁltering process improves deconvolu-
tion computation by removing spatially-varying high frequency
noise but also results in the removal of useful information which
yields additional errors over object boundaries. To preserve this
useful information, we introduce a directional high-pass ﬁlter
that utilises our tracked 3D camera motion.

5. Directional High-pass Filter

Detail enhancement using directional ﬁlters has been proved
effective in several areas of computer vision [3]. Here we deﬁne
a directional high-pass ﬁlter as:
fθ ⊗ I(x) = m

g(t)I(x + tΘ)dt

(cid:90)

(6)

g(t)dt

(cid:16)(cid:82)

(cid:17)−1. The

where x = (x, y)T represents a pixel position and g(t) =
1− exp{−t2/2σ2} denotes a 1D Gaussian based high-pass func-
tion. Θ = (cos θ, sin θ)T controls the ﬁltering direction along θ.
m is a normalization factor deﬁned as m =
ﬁlter fθ is proposed to preserve overall high frequency details
along direction θ without affecting blur detail in orthogonal di-
rections [25]. Given a directionally ﬁltered image bθ = fθ⊗I(x),
the optimal blur kernel is deﬁned (Eq 5) as kθ = k ⊗ fθ. Fig. 3
demonstrates that noise or object motion within a scene usu-
ally results in low frequency noise in the estimated blur kernel
(Cho&Lee [2]). This low frequency noise can be removed by
our directional high-pass ﬁlter while preserving major blur de-
tails. In our method, this directional high-pass ﬁlter is supple-
mented into the Cho&Lee [2] framework using a coarse-to-ﬁne
strategy in order to recover high quality blur kernels for use in
our optical ﬂow estimation (Sec. 7.2).

6. Blur-Robust Optical Flow Energy

Within a blurry scene, a pair of adjacent natural images may
contain different blur kernels, further violating Brightness Con-
stancy. This results in unpredictable ﬂow error across the dif-
ferent blur regions. To address this issue, Portz et al. pro-
posed a modiﬁed Brightness Constancy term by matching the
un-uniform blur between the input images. As one of our main
contributions, we extend this assumption to a novel Blur Gradi-
ent Constancy term in order to provide extra robustness against
illumination change and outliers. Our main energy function is
given as follows:

E(w) = EB(w) + γES (w)

(7)

A pair of consecutively observed frames from an image se-
quence is considered in our algorithm. I1(x) represents the cur-
rent frame and its successor is denoted by I2(x) where I∗ =

3

Figure 3: Directional high-pass ﬁlter for blur kernel enhancement. Given the blur direction θ, a directional high-pass ﬁlter along θ + π/2 is applied to preserve blur
detail in the estimated blur kernel.

k∗ ⊗ l∗ and {I∗, l∗ : Ω ⊂ R3 → R} represent rectangular im-
ages in the RGB channel. Here l∗ is latent image and k∗ denotes
the relative blur kernel. The optical ﬂow displacement between
I1(x) and I2(x) is deﬁned as w = (u, v)T . To match the un-
uniform blur between input images, the blur kernel from each
input image is applied to the other. We have new blur images
b1 and b2 as follows:

Our energy term encompassing Brightness and Gradient

Constancy relates to b1 and b2 as follows:

b1 = k2 ⊗ I1 ≈ k2 ⊗ k1 ⊗ l1
b2 = k1 ⊗ I2 ≈ k1 ⊗ k2 ⊗ l2
(cid:90)
+ α(cid:107)∇b2(x + w) − ∇b1(x)(cid:107)2)dx

φ((cid:107)b2(x + w) − b1(x)(cid:107)2

Ω

EB(w) =

(8)
(9)

(cid:90)

(10)
The term ∇ = (∂xx, ∂yy)T presents a spatial gradient and
α ∈ [0, 1] denotes a linear weight. The smoothness regulariser
penalizes global variation as follows:

ES (w) =

φ((cid:107)∇u(cid:107)2 + (cid:107)∇v(cid:107)2)dx

Ω

(11)

where we apply the Lorentzian regularisation φ(s) = log(1 +
s2/22) to both the data term and smoothness term. In our case,
the image properties, e.g. small details and edges, are broken
by the camera blur, which leads to additional errors in those
regions. We suppose to apply strong boundary preservation
even the non-convex Lorentzian regularisation may bring the
extra difﬁculty to the energy optimisation (More analysis can
be found in Li et al. [14]). In the following section, our optical
ﬂow framework is introduced in detail.

7. Optical Flow Framework

Our overall framework is outlined in Algorithm 1 based on
an iterative top-down, coarse-to-ﬁne strategy. Prior to minimiz-
ing the Blur-Robust Optical Flow Energy (Sec. 7.4), a fast blind
deconvolution approach [2] is performed for pre-estimation of
the blur kernel (Sec. 7.1), which is followed by kernel reﬁne-
ment using our Directional High-pass Filter (Sec. 7.2). All
these steps are detailed in the following subsections.

2

: A image pair I1, I2 and camera motion θ1, θ2, θ12

2 ← Ii
2 ← 0, wi ← (0, 0)T

Algorithm 1: Blur-Robust Optical Flow Framework
Input
Output : Optimal optical ﬂow ﬁeld w
1: A n-level top-down pyramid is built with the level index i
i ← 0
2:
1 ← Ii
li
1, li
3:
1 ← 0, ki
4: ki
for coarse to ﬁne do
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16: endfor

ki∗ ← IterBlindDeconv ( li∗, Ii∗ )
ki∗ ← DirectFilter ( ki∗, θ1, θ2, θ12 )
li∗ ← NonBlindDeconvolve ( ki∗, Ii∗ )
endfor
1 ⊗ ki
1 ← Ii
bi
2, bi
dwi ← Energyoptimisation ( bi
wi ← wi + dwi

i ← i + 1
Resize ki{1,2}, li{1,2}, Ii{1,2} and wi with the ith scale
foreach ∗ ∈ {1, 2} do

2, wi )

2 ← Ii

2 ⊗ ki

1, bi

1

7.1. Iterative Blind Deconvolution

Cho and Lee [2] describe a fast and accurate approach
(Cho&Lee) to recover the unique blur kernel. As shown in Al-
gorithm 1, we perform a similar approach for the pre-estimation
of the blur kernel k within our iterative process, which involves
two steps of prediction and kernel estimation. Given the la-
tent image l estimated from the consecutively coarser level, the
gradient maps ∆l = {∂xl, ∂yl} of l are calculated along the hor-
izontal and vertical directions respectively in order to enhance
salient edges and reduce noise in featureless regions of l. Next,
the predicted gradient maps ∆l as well as the gradient map of
the blurry image I are utilised to compute the pre-estimated blur
kernel by minimizing the energy function as follows:
ω∗ (cid:107)I∗ − k ⊗ l∗(cid:107)2 + δ(cid:107)k(cid:107)2
k = argmink
(I∗, l∗) ∈ {(∂xI, ∂xl), (∂yI, ∂yl), (∂xxI, ∂xxl),
(∂yyI, ∂yyl), (∂xyI, (∂x∂y + ∂y∂x)l/2)}

(cid:88)

(12)

I∗,l∗

where δ denotes the weight of Tikhonov regularization and
ω∗ ∈ {ω1, ω2} represents a linear weight for the derivatives
in different directions. Both I and l are propagated from the
nearest coarse level within the pyramid. To minimise this en-
ergy Eq. (12), we follow the inner-iterative numerical scheme
of [2] which yields a pre-estimated blur kernel k.

4

Blur KernelBlur DirectionFilter DirectionBlur PatternClear PatternGT KernelCho&LeeCho&Lee + Directional Filter7.2. Directional High-pass Filtering

Once the pre-estimated kernel k is obtained, our Directional
High-pass Filters are applied to enhance the blur information
by reducing noise in the orthogonal direction of the tracked
camera motion. Although our RGB-Motion Imaging System
provides an intuitively accurate camera motion estimation, out-
liers may still exist in the synchronisation. We take into ac-
count the directional components {θ1, θ2, θ12} of two consecu-
tive camera motions M1 and M2 as well as their combination
M12 (Fig. 2(d)) for extra robustness. The pre-estimated blur
kernel is ﬁltered along its orthogonal direction as follows:

(cid:88)

k =

β∗k ⊗ fθ∗+π/2

(13)

β∗,θ∗

where β∗ ∈ {1/2, 1/3, 1/6} linearly weights the contri-
bution of ﬁltering in different directions. Note that
two
consecutive images I1 and I2 are involved in our frame-
work where the former accepts the weight set (β∗, θ∗) ∈
{(1/2, θ1), (1/3, θ2), (1/6, θ12)} while the other weight
set
(β∗, θ∗) ∈ {(1/3, θ1), (1/2, θ2), (1/6, θ12)} is performed for the
latter. This ﬁltering process yields an updated blur kernel k
which is used to update the latent image l within a non-blind
deconvolution [3]. Note that the convolution operation is com-
putationally expensive in the spatial domain, we consider an
equivalent ﬁltering scheme in the frequency domain in the fol-
lowing subsection.

7.3. Convolution for Directional Filtering

Our proposed directional ﬁltering is performed as convolu-
tion operation in the spatial domain, which is often highly ex-
pensive in computation given large image resolutions. In our
implementations, we consider a directional ﬁltering scheme in
the frequency domain where we have the equivalent form of
ﬁltering model Eq. (6) as follows:

KΘ(u, v) = K(u, v)FΘ(u, v)

(14)

where KΘ is the optimal blur kernel in the frequency domain
while K and FΘ present the Fourier Transform of the blur ker-
nel k and our directional ﬁlter fθ respectively. Thus, the opti-
mal blur kernel kθ in the spatial domain can be calculated as
kθ = IDFT[KΘ] using Inverse Fourier Transform. In this case,
the equivalent form of our directional high-pass ﬁlter in the fre-
quency domain is deﬁned as follows:

FΘ(u, v) = 1 − exp

(cid:110)−L2(u, v)/2σ2(cid:111)

(15)

where the line function L(u, v) = u cos θ + v sin θ controls
the ﬁltering process along the direction θ while σ is the stan-
dard deviation for controlling the strength of the ﬁlter. Please
note that other more sophisticated high-pass ﬁlters could also
be employed using this directional substitution L. Even though
this consumes a reasonable proportion of computer memory,
convolution in the frequency domain O(N log2 N) is faster than
equivalent computation in the spatial domain O(N2).

5

Having performed blind deconvolution and directional ﬁlter-
ing (Sec. 7.1, 7.2 and 7.3), two updated blur kernels ki
1 and ki
2
on the ith level of the pyramid are obtained from input images
Ii
1 and Ii
2 respectively, which is followed by the uniform blur
1 and bi
image bi
2 computation using Eq. (9). In the following
subsection, Blur-Robust Optical Flow Energy optimisation on
bi
1 and bi
7.4. Optical Flow Energy optimisation

1 is introduced in detail.

As mentioned in Sec. 6, our blur-robust energy is continu-
ous but highly nonlinear. minimisation of such energy function
is extensively studied in the optical ﬂow community.
In this
section, a numerical scheme combining Euler-Lagrange Equa-
tions and Nested Fixed Point Iterations is applied [11] to solve
our main energy function Eq. 7. For clarity of presentation, we
deﬁne the following mathematical abbreviations:

bx = ∂xb2(x + w)
byy = ∂yyb2(x + w)
bz = b2(x + w) − b1(x)
by = ∂yb2(x + w)
bxx = ∂xxb2(x + w) bxz = ∂xb2(x + w) − ∂xb1(x)
byz = ∂yb2(x + w) − ∂yb1(x)
bxy = ∂xyb2(x + w)

At the ﬁrst phase of energy minimization, a system is built
based on Eq. 7 where Euler-Lagrange is employed as follows:

(cid:48){b2
φ

z + α(b2

xz + b2

(cid:48){b2
φ

z + α(b2

xz + b2

yz)} · {bxbz + α(bxxbxz + bxybyz)}

−γφ

(cid:48)((cid:107)∇u(cid:107)2 + (cid:107)∇v(cid:107)2) · ∇u = 0

yz)} · {bybz + α(byybyz + bxybxz)}

−γφ

(cid:48)((cid:107)∇u(cid:107)2 + (cid:107)∇v(cid:107)2) · ∇v = 0

(16)

(17)

An n-level image pyramid is then constructed from the top
coarsest level to the bottom ﬁnest level. The ﬂow ﬁeld is initial-
ized as w0 = (0, 0)T on the top level and the outer ﬁxed point
iterations are applied on w. We assume that the solution wi+1
converges on the i + 1 level. We have:

(cid:48){(bi+1
φ
z
·{bi
xbi+1
−γφ
(cid:48)(

)2 + α(bi+1
z + α(bi

(cid:13)(cid:13)(cid:13)∇ui+1(cid:13)(cid:13)(cid:13)2 +
(cid:13)(cid:13)(cid:13)∇ui+1(cid:13)(cid:13)(cid:13)2 +

)2 + α(bi+1
z + α(bi

xz + bi

yz )2}
xz )2 + α(bi+1
yz )}
xybi+1
xxbi+1

(cid:13)(cid:13)(cid:13)∇vi+1(cid:13)(cid:13)(cid:13)2) · ∇ui+1 = 0
(cid:13)(cid:13)(cid:13)∇vi+1(cid:13)(cid:13)(cid:13)2) · ∇vi+1 = 0

yz )2}
xz )2 + α(bi+1
xz )}
xybi+1
yybi+1

yz + bi

(cid:48){(bi+1
φ
z
·{bi
ybi+1
−γφ
(cid:48)(

(18)

(19)

Because of the nonlinearity in terms of φ(cid:48), bi+1∗

, the system
(Eqs. 18, 19) is difﬁcult to solve by linear numerical methods.
We apply the ﬁrst order Taylor expansions to remove these non-
linearity in bi+1∗

, which results in:

z ≈ bi
bi+1
xz ≈ bk
bi+1
yz ≈ bk
bi+1

z + bi
xz + bi
yz + bi

xdui + bi
xxdui + bi
xydui + bi

ydvi,
xydvi,
yydvi.

Based on the coarse-to-ﬁne ﬂow assumption of Brox et
al. [11] w.r.t. ui+1 ≈ ui + dui and vi+1 ≈ vi + dvi where the
unknown ﬂow ﬁeld on the next level i + 1 can be obtained using
the ﬂow ﬁeld and its incremental from the current level i. The
new system can be presented as follows:

(20)

(21)

2

: A image pair I1, I2 Without camera motion

2 ← Ii
2 ← 0, wi ← (0, 0)T , θi = 0

Algorithm 2: Auto Blur-Robust Optical Flow Framework
Input
Output : Optimal optical ﬂow ﬁeld w
1: A n-level top-down pyramid is built with the level index i
i ← 0
2:
1 ← Ii
li
1, li
3:
1 ← 0, ki
4: ki
for coarse to ﬁne do
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17: endfor

ki∗ ← IterBlindDeconv ( li∗, Ii∗ )
ki∗ ← DirectFilter ( ki∗, θi )
li∗ ← NonBlindDeconvolve ( ki∗, Ii∗ )
endfor
1 ⊗ ki
1 ← Ii
2, bi
bi
dwi ← EnergyOptimisation ( bi
1, bi
wi ← wi + dwi
θi ← CameraMotionEstimation(wi)

i ← i + 1
Resize ki{1,2}, li{1,2}, Ii{1,2} and wi with the ith scale
foreach ∗ ∈ {1, 2} do

2 ← Ii

2 ⊗ ki

2, wi )

1

7.5. Alternative Implementation with Automatic Camera Mo-

tion θ∗ Estimation

Alternative to using our assisted tracker, we also provide an
additional implementation by using the camera motion θ∗ esti-
mated generically from the ﬂow ﬁeld. As shown in Algorithm
2, the system does not take the camera motion (θ∗) as input
but computes it (CameraMotionEstimation) generically
at every level of the image pyramid.

Ai ← AffineEstimation(x, x + wi)
θi ← AffineToMotionAngle(Ai)

(24)

On each level, we calculate the Afﬁne Matrix from Ii

2 us-
ing the correspondences x → x + wi and RANSAC. The trans-
lation information from Ai is then normalized and converted to
the angle format θi. In this case, our DirectionalFilter is also
downgraded to consider one direction θi for each level. In the
next section, we quantitatively compare our method to other
popular baselines.

1 to Ii

8. Evaluation

In this section, we evaluate our method on both synthetic and
realworld sequences and compare its performance against three
existing state-of-the-art optical ﬂow approaches of Xu et al.’s
MDP [12], Portz et al.’s [1] and Brox et al.’s [11] (an imple-
mentation of [26]). MDP is one of the best performing opti-
cal ﬂow methods given blur-free scenes, and is one of the top
3 approaches in the Middlebury benchmark [27]. Portz et al.’s
method represents the current state-of-the-art in optical ﬂow es-
timation given object blur scenes while Brox et al.’s contains a
similar optimisation framework and numerical scheme to Portz
et al.’s, and ranks in the midﬁeld of the Middlebury benchmarks
based on overall average. Note that all three baseline methods

B · {bi
(cid:48))i
(φ
xx(bi
+αbi
+αbi
xy(bi

xdui + bi
z + bi
x(bi
xxdui + bi
xz + bi
xydui + bi
yz + bi
−γ(φ
S · ∇(ui + dui) = 0
(cid:48))i

ydvi)
xydvi)
yydvi)}

B · {bi
(cid:48))i
(φ
+αbi
yy(bi
xy(bi
+αbi

ydvi)
yydvi)
xydvi)}

xdui + bi
y(bi
z + bi
xydui + bi
yz + bi
xxdui + bi
xz + bi
−γ(φ
S · ∇(vi + dvi) = 0
(cid:48))i
B and (φ(cid:48))i

where the terms (φ(cid:48))i

S contained φ provide robust-
In addi-
ness to ﬂow discontinuity on the object boundary.
tion, (φ(cid:48))i
S is also regularizer for a gradient constraint in motion
space. Although we ﬁxed wi in Eqs. 20 and 21, the nonlinear-
ity in φ(cid:48) leads to the difﬁculty of solving the system. The inner
ﬁxed point iterations are applied to remove this nonlinearity:
dui, j and dvi, j are assumed to converge within j iterations by
initializing dui,0 = 0 and dvi,0 = 0. Finally, we have the linear
system in dui, j+1 and dvi, j+1 as follows:

B · {bi
(cid:48))i, j
(φ
xx(bi
+αbi
+αbi
xy(bi

x(bi
z + bi
xz + bi
yz + bi
−γ(φ

ydvi, j+1)
xydvi, j+1)
yydvi, j+1)}

xdui, j+1 + bi
xxdui, j+1 + bi
xydui, j+1 + bi
S · ∇(ui + dui, j+1) = 0
(cid:48))i, j

(φ

B · {bi
(cid:48))i, j
yy(bi
+αbi
+αbi
xy(bi

z + bi
y(bi
yz + bi
xz + bi
−γ(φ

xdui, j+1 + bi
xydui, j+1 + bi
xxdui, j+1 + bi

ydvi, j+1)
yydvi, j+1)
xydvi, j+1)}

S · ∇(vi + dvi, j+1) = 0
(cid:48))i, j

(22)

(23)

where (φ(cid:48))i, j

B denotes a robustness factor against ﬂow discon-
S represents

tinuity and occlusion on the object boundaries. (φ(cid:48))i, j
the diffusivity of the smoothness regularization.

(cid:48))i, j

B

(φ

(cid:48))i, j

S

(φ

(cid:48){(bi
z + bi
= φ
xz + bi
+ α(bi
yz + bi
+ α(bi

xdui, j + bi, j
xxdui, j + bi
xydui, j + bi

(cid:48){(cid:13)(cid:13)(cid:13)∇(ui + dui, j)
(cid:13)(cid:13)(cid:13)2 +

= φ

y dvi, j)2
xydvi, j)2
yydvi, j)2}

(cid:13)(cid:13)(cid:13)2}
(cid:13)(cid:13)(cid:13)∇(vi + dvi, j)

In our implementation, the image pyramid is constructed
with a downsampling factor of 0.75. The ﬁnal linear system
in Eq. (22,23) is solved using Conjugate Gradients within 45
iterations.

6

Figure 4: The synthetic blur sequences with the blur kernel, tracked camera motion direction and ground truth ﬂow ﬁelds. From Top To Bottom: sequences of
RubberWhale, Urban2, Hydrangea and Urban2.

are evaluated using their default parameters setting; all exper-
iments are performed using a 2.9Ghz Xeon 8-cores, NVIDIA
Quadro FX 580, 16Gb memory computer.

In the following subsections, we compare our algorithm
(moBlur) and four different implementations (auto, nonGC,
nonDF and nonGCDF) against the baseline methods. auto
denotes the implementation using the automatic camera mo-
tion estimation scheme (Algorithm 2); nonGC represents the
implementation without the Gradient Constancy term while
nonDF denotes an implementation without the directional ﬁl-
tering process. nonGCDF is the implementation with neither of
these features. The results show that our Blur-Robust Optical
Flow Energy and Directional High-pass Filter signiﬁcantly im-
prove algorithm performance for blur scenes in both synthetic
and realworld cases.

8.1. Middlebury Dataset with camera shake blur

One advance for evaluating optical ﬂow given scenes with
object blur is proposed by Portz et al. [1] where synthetic
Ground Truth (GT) scenes are rendered with blurry moving ob-
jects against a blur-free static/ﬁxed background. However, their
use of synthetic images and controlled object trajectories lead to
a lack of global camera shake blur, natural photographic proper-
ties and real camera motion behaviour. To overcome these lim-
itations, we render four sequences with camera shake blur and
corresponding GT ﬂow-ﬁelds by combining sequences from the
Middlebury dataset [27] with blur kernels estimated using our
system.

In our experiments we select the sequences Grove2, Hy-
drangea, RubberWhale and Urban2 from the Middlebury
dataset. For each of them, four adjacent frames are selected
as latent images along with the GT ﬂow ﬁeld wgt (supplied by

Middlebury) for the middle pair. 40 × 40 blur kernels are then
estimated [2] from realworld video streams captured using our
RGB-Motion Imaging System. As shown in Fig. 4, those ker-
nels are applied to generate blurry images denoted by I0, I1, I2
and I3 while the camera motion direction is set for each frame
based on the 3D motion data. Although the wgt between latent
images can be utilised for the evaluation on relative blur im-
ages I∗ [28, 29], strong blur can signiﬁcantly violate the origi-
nal image intensity, which leads to a multiple correspondences
problem: a point in the current image corresponds to multi-
ple points in the consecutive image. To remove such multi-
ple correspondences, we sample reasonable correspondence set
{ ˆw | ˆw ⊂ wgt,|I2(x + ˆw) − I1(x)| < } to use as the GT for the
blur images I∗ where  denotes a predeﬁned threshold. Once
we obtain ˆw, both Average Endpoint Error (AEE) and Average
Angle Error (AAE) tests [27] are considered in our evaluation.
The computation is formulated as follows:

(cid:112)

(cid:88)
(cid:88)

x

x

AEE =

AAE =

1
n
1
n

(u − ˆu)2 + (v − ˆv)2
(cid:32)

cos−1

√

1.0 + u × ˆu + v × ˆv

√

1.0 + u2 + v2

1.0 + ˆu2 + ˆv2

(cid:33)

(25)

(26)

where w = (u, v)T and ˆw = (ˆu, ˆv)T denotes the baseline
ﬂow ﬁeld and the ground truth ﬂow ﬁeld (by removing multiple
correspondences) respectively while n presents the number of
ground truth vectors in ˆw. The factor 1.0 in AAE is an arbitrary
scaling constant to convert the units from pixels to degrees [27].
Fig. 5(a) Left shows AEE (in pixel) and AAE (in degree) tests
on our four synthetic sequences. moBlur and nonGC lead both
AEE and AAE tests in all the trials. Both Brox et al. and MDP
yield signiﬁcant error in Hydrangea, RubberWhale and Urban2

7

1I0I2I3I1I0I2I3I1I0I2I3I1I0I2I3I(a) Left: Quantitative Average Endpoint Error (AEE), Average Angle Error (AAE) and Time Cost (in second) comparisons on our synthetic sequences
where the subscripts show the rank in relative terms. Right: AEE measure on RubberWhale by ramping up the noise distribution.

(b) Visual comparison on sequences RubberWhale, Urban2, Hydrangea and Urban2 by varying baseline methods. For each sequence, First Row: optical
ﬂow ﬁelds from different methods. Second Row: the error maps against the ground truth.

Figure 5: Quantitative evaluation on four synthetic blur sequences with both camera motion and ground truth.

because those sequences contain large textureless regions with
blur, which in turn weakens the inner motion estimation pro-
cess as shown in Fig. 5(b). Fig. 5(a) also illustrates the aver-
age time cost (second per frame) of the baseline methods. Our
method gives reasonable performance (45 sec. per frame) com-
paring to the state-of-the-art Portz et al. and MDP even an in-
ner image deblurring process is involved. Furthermore, Fig 5(a)
Right shows the AEE metric for RubberWhale by varying the

distribution of Salt&Pepper noise. It is observed that a higher
noise level leads to additional errors for all the baseline meth-
ods. Both moBlur and nonGC yield the best performance while
Portz et al. and Brox et al. show a similar rising AEE trend
when the noise increases.

Fig. 6 shows our quantitative measure by comparing our two
implementations which use the RGB-Motion Imaging (moBlur)
and automatic camera motion estimation scheme (auto, see

8

AEE Test on RubberWhale Seq.Salt&Pepper Noise Level (%)AEE (pix.)0510152025303540451234567 moBlurnonDFnonGCnonDFGCPortz et al.Brox et al.MDPGrove2AEEHydrangeaRub.WhaleUrban2AAEAEEAAEAEEAAEAEEAAEAEE/AAEPortzetal.nonDFGC1.1444.114moBlur-nonGCmoBlur-nonDFBroxetal.1.6275.1470.4922.3821.5264.9661.2454.5352.6263.5562.2853.2140.9522.2321.8333.0032.2643.4753.1268.1861.2547.7140.6423.7121.1236.4532.4457.9853.4465.1042.9855.4461.5423.0322.5035.1952.9244.603Ours,moBlur0.4712.3410.6712.1910.6213.6711.3612.871Xuetal.,MDP1.0633.4633.4073.5563.7068.2175.6276.827TimeCost852833392745466036(Pix.)moBlurnonGCnonDFnonDFGCPortz et al.Brox et al.Xu et al. MDP036(Pix.)Grove2GT Flow FieldRub.WhaleGT Flow Field036(Pix.)036(Pix.)Urban2GT Flow FieldHydrangeaGT Flow FieldFigure 6: Quantitative comparison between our implementations using RGB-Motion Imaging (moBlur); and automatic camera motion estimation scheme (auto, see
Sec. 7.5). From Left To Right: AEE and AAE tests on all sequences respectively; the angular error of camera motion estimated by auto by varying the pyramidal
levels of the input images.

Figure 7: AEE measure of our method (moBlur) by varying the input motion directions. (a): the overall measure strategy and error maps of moBlur on sequence
Urban2. (b): the quantitative comparison of moBlur against nonDF by ramping up the angle difference λ. (c): the measure of moBlur against Portz et al. [1].

Figure 8: The realworld sequences captured along the tracked camera motion. From Top To Bottom: sequences of warrior, chessboard, LabDesk and shoes.

Sec. 7.5) respectively. For better observation, we also give the
Portz et al.
in this measure. We observe that both our im-

plementations outperform Portz et al.
in the AEE and AAE
tests. Especially the moBlur gives the best accuracy in all tri-

9

01234AEE (pix.)AEE Test on All Seqs.moBlurautoPortz0246810AAE (degrees)AAE Test on All Seqs.moBlurautoPortzGrove2Hydra.Rub.Wh.Urban2Grove2Hydra.Rub.Wh.Urban2012345678910Index of Pyramidal Level0369121518Angular Error (degrees)Auto Camera Motion ErrorGrove2HydrangeaRub.WhaleUrban20102030405060708090AEE Test of moBlur by Varying λAEE (pix.)Varying Angle Di(cid:31). λ (°)0102030405060708090Varying Angle Di(cid:31). λ (°)GT Blur DirectionInput Direction036(Pix.)= 0= 60= 90oooAEE Comparison by Varying λ0.511.522.533.50.511.522.533.5 moBlur.Gro2moBlur.HydrmoBlur.RubbmoBlur.Urb2nonDF.Gro2nonDF.HydrnonDF.RubbnonDF.Urb2Portz.Gro2Portz.HydrPortz.RubbPortz.Urb2(a) sample sequence and error maps(b) Comparion of on/off filter(c) Comparion to Portz et al.AEE (pix.)0I1I2I3I0I1I2I3I0I1I2I3I0I1I2I3Ials. The implementation auto yields the less accurate results
than the moBlur. It may be because the auto camera motion
estimation is affected by ambiguous blur that often caused by
multiple moving objects. To investigate this issue, we plot the
angular error by comparing the auto-estimated camera motion
to the ground truth on all the sequences (Fig. 6, right end). We
observe that our automatic camera motion estimation scheme
leads to higher errors on the upper/coarser level of the image
pyramid. Even the accuracy is improved on the ﬁner levels but
the error may be accumulated and affect the ﬁnal result.

In practice,

the system may be used in some challenge
scenes, e.g. fast camera shaking, super high frame rate capture,
or even infrared interference, etc.
In those cases, the wrong
tracked camera motion may be given to some speciﬁc frames.
To investigate how the tracked camera motion affects the ac-
curacy of our algorithm, we compare moBlur to nonDF (our
method without directional ﬁltering) and Portz et al. by vary-
ing the direction of input camera motion. As shown in Fig. 7(a),
we rotate the input camera motion vector with respect to the
GT blur direction by an angle of λ degrees. Here λ = 0 rep-
resents the ideal situation where the input camera motion has
the same direction as the blur direction. The increasing λ sim-
ulates more errors in the camera motion estimation. Fig. 7(b,c)
shows the AEE metric by increasing the λ. We observe that
the AEE increases during this test. moBlur outperforms the
nonDF (moBlur without the directional ﬁlter) in both Grove2
and RubberWhale while nonDF provides higher performance
in Hydrangea when λ is larger than 50◦. In addition, moBlur
outperforms Portz et al. in all trials except Hydrangea where
Portz et al. shows a minor advantage (AEE 0.05) when λ = 90◦.
The rationale behind this experiment is that the wrong camera
motion may yield signiﬁcant information loss in the directional
high-pass ﬁltering. Such information loss harms the deblurring
process and consequently leads to errors in the optical ﬂow esti-
mation. Thus, obtaining precise camera motion is the essential
part of this system, as well as a potential future research.

8.2. Realworld Dataset

To evaluate our method in the realworld scenes, we capture
four sequences warrior, chessboard, LabDesk and shoes with
tracked camera motion using our RGB-Motion Imaging Sys-
tem. As shown in Fig. 8, both warrior and chessboard con-
tain occlusions, large displacements and depth change while the
sequences of LabDesk and shoes embodies the object motion
blur and large textureless regions within the same scene. Fig. 9
shows visual comparison of our method moBlur against Portz et
al. on these realworld sequences. It is observed that our method
preserves appearance details on the object surface and reduce
boundary distortion after warping using the ﬂow ﬁeld. In addi-
tion, our method shows robustness given cases where multiple
types of blur exist in the same scene (Fig.9(b), sequence shoes).

9. Conclusion

In this paper, we introduce a novel dense tracking framework
which interleaves both a popular iterative blind deconvolution;

10

as well as a warping based optical ﬂow scheme. We also in-
vestigate the blur papameterization for the video footages. In
our evaluation, we highlight the advantages of using both the
extra motion channel and the directional ﬁltering in the optical
ﬂow estimation for the blurry video footages. Our experiments
also demonstrated the improved accuracy of our method against
large camera shake blur in both noisy synthetic and realworld
cases. One limitation in our method is that the spatial invariance
assumption for the blur is not valid in some realworld scenes,
which may reduce accuracy in the case where the object depth
signiﬁcantly changes. Finding a depth-dependent deconvolu-
tion and deep data-driven model would be a challenge for future
work as well.

10. Acknowledgements

We thank Ravi Garg and Lourdes Agapito for providing
their GT datasets. We also thank Gabriel Brostow and the
UCL Vision Group for their helpful comments. The authors
are supported by the EPSRC CDE EP/L016540/1 and CAM-
ERA EP/M023281/1; and EPSRC projects EP/K023578/1 and
EP/K02339X/1.

References

References

[1] T. Portz, L. Zhang, H. Jiang, Optical ﬂow in the presence of spatially-
varying motion blur, in: IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR’12), 2012, pp. 1752–1759.

[2] S. Cho, S. Lee, Fast motion deblurring, ACM Transactions on Graphics

(TOG’09) 28 (5) (2009) 145.

[3] L. Zhong, S. Cho, D. Metaxas, S. Paris, J. Wang, Handling noise in single
image deblurring using directional ﬁlters, in: IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR’13), 2013, pp. 612–619.

[4] Y.-W. Tai, H. Du, M. S. Brown, S. Lin, Image/video deblurring using
a hybrid camera, in: IEEE Conference on Computer Vision and Pattern
Recognition (CVPR’08), 2008, pp. 1–8.

[5] A. Levin, P. Sand, T. S. Cho, F. Durand, W. T. Freeman, Motion-invariant
photography, ACM Transactions on Graphics (TOG’08) 27 (3) (2008) 71.
[6] N. Joshi, S. B. Kang, C. L. Zitnick, R. Szeliski, Image deblurring using
inertial measurement sensors, ACM Transactions on Graphics (TOG’10)
29 (4) (2010) 30.

[7] C. Godard, P. Hedman, W. Li, G. J. Brostow, Multi-view reconstruction
of highly specular surfaces in uncontrolled environments, in: 3D Vision
(3DV), 2015 International Conference on, IEEE, 2015, pp. 19–27.

[8] Z. Lv, A. Tek, F. Da Silva, C. Empereur-Mot, M. Chavent, M. Baaden,
Game on, science-how video game technology may help biologists tackle
visualization challenges, PloS one 8 (3) (2013) e57990.

[9] Z. Lv, A. Halawani, S. Feng, H. Li, S. U. R´ehman, Multimodal hand and
foot gesture interaction for handheld devices, ACM Transactions on Mul-
timedia Computing, Communications, and Applications (TOMM) 11 (1s)
(2014) 10.

[10] B. Horn, B. Schunck, Determining optical ﬂow, Artiﬁcial intelligence

17 (1-3) (1981) 185–203.

[11] T. Brox, A. Bruhn, N. Papenberg, J. Weickert, High accuracy optical ﬂow
estimation based on a theory for warping, in: European Conference on
Computer Vision (ECCV’04), 2004, pp. 25–36.

[12] L. Xu, S. Zheng, J. Jia, Unnatural l0 sparse representation for natural
image deblurring, in: IEEE Conference on Computer Vision and Pattern
Recognition (CVPR’13), 2013, pp. 1107–1114.

[13] W. Li, D. Cosker, M. Brown, An anchor patch based optimisation frame-
work for reducing optical ﬂow drift in long image sequences, in: Asian
Conference on Computer Vision (ACCV’12), Springer, 2012, pp. 112–
125.

(a) Visual comparison on realworld sequences of warrior and chessboard.

Figure 9: Visual comparison of image warping on realworld sequences of warrior, chessboard, LabDesk and shoes, captured by our RGB-Motion Imaging System.

(b) Visual comparison on realworld sequences of LabDesk and shoes.

[14] W. Li, D. Cosker, M. Brown, R. Tang, Optical ﬂow estimation using lapla-
cian mesh energy, in: IEEE Conference on Computer Vision and Pattern
Recognition (CVPR’13), IEEE, 2013, pp. 2435–2442.

[15] W. Li, D. Cosker, M. Brown, Drift robust non-rigid optical ﬂow enhance-
ment for long sequences, Journal of Intelligent and Fuzzy Systems 0 (0)
(2016) 12.

[16] W. Li, Y. Chen, J. Lee, G. Ren, D. Cosker, Robust optical ﬂow estimation
for continuous blurred scenes using rgb-motion imaging and directional
ﬁltering, in: IEEE Winter Conference on Application of Computer Vision
(WACV’14), IEEE, 2014, pp. 792–799.

[17] R. Tang, D. Cosker, W. Li, Global alignment for dynamic 3d morphable
model construction, in: Workshop on Vision and Language (V&LW’12),
2012.

[18] W. Li, Nonrigid surface tracking, analysis and evaluation, Ph.D. thesis,

University of Bath (2013).

[19] Y. Schoueri, M. Scaccia, I. Rekleitis, Optical ﬂow from motion blurred
color images, in: Canadian Conference on Computer and Robot Vision,
2009.

[20] L. Yuan, J. Sun, L. Quan, H.-Y. Shum, Progressive inter-scale and intra-

scale non-blind image deconvolution 27 (3) (2008) 74.

[21] Y. HaCohen, E. Shechtman, D. B. Goldman, D. Lischinski, Non-rigid
dense correspondence with applications for image enhancement, ACM
Transactions on Graphics (TOG’11) 30 (4) (2011) 70.

[22] J. Lee, V. Baines, J. Padget, Decoupling cognitive agents and virtual en-
vironments, in: Cognitive Agents for Virtual Environments, 2013, pp.
17–36.

[23] Q. Shan, J. Jia, A. Agarwala, High-quality motion deblurring from a sin-

gle image, ACM Transactions on Graphics (TOG’08) 27 (3) (2008) 73.

[24] Y.-W. Tai, S. Lin, Motion-aware noise ﬁltering for deblurring of noisy
and blurry images, in: IEEE Conference on Computer Vision and Pattern
Recognition (CVPR’12), 2012, pp. 17–24.

[25] X. Chen, J. Yang, Q. Wu, J. Zhao, X. He, Directional high-pass ﬁlter
for blurry image analysis, Signal Processing: Image Communication 27
(2012) 760–771.

[26] C. Liu, Beyond pixels: exploring new representations and applications
for motion analysis, Ph.D. thesis, Massachusetts Institute of Technology
(2009).

[27] S. Baker, D. Scharstein, J. Lewis, S. Roth, M. Black, R. Szeliski, A
database and evaluation methodology for optical ﬂow, International Jour-
nal of Computer Vision (IJCV’11) 92 (2011) 1–31.

[28] D. J. Butler, J. Wulff, G. B. Stanley, M. J. Black, A naturalistic open
source movie for optical ﬂow evaluation, in: European Conference on
Computer Vision (ECCV’12), 2012, pp. 611–625.

[29] J. Wulff, D. J. Butler, G. B. Stanley, M. J. Black, Lessons and insights
from creating a synthetic optical ﬂow benchmark, in: ECCV Workshop on
Unsolved Problems in Optical Flow and Stereo Estimation (ECCVW’12),

11

1I2I1I2ImoBlurPortz et al.Warping DirectionFlow FieldCloseups of Warping ResultsFlow FieldCloseups of Warping ResultsWarping DirectionCloseups of Warping ResultsFlow FieldCloseups of Warping Results1I2ImoBlurPortz et al.Flow FieldWarping Direction1I2IWarping Direction2012, pp. 168–177.

12

