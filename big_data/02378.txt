6
1
0
2

 
r
a

M
 
8

 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
8
7
3
2
0

.

3
0
6
1
:
v
i
X
r
a

Disaggregated Benders Decomposition for solving a Network Maintenance

Scheduling Problem.

Robin H. Pearce & Michael Forbes

School of Mathematics and Physics, University of Queensland, Australia

Abstract

We consider a problem concerning a network and a set of maintenance requests to be undertaken. We

wish to schedule the maintenance in such a way as to minimise the impact on the total throughput of the

network. We apply disaggregated Benders cuts and lazy constraints to solve the problem to optimality, as

well as exploring the strengths and weaknesses of the technique. We prove that our Benders cuts are pareto

optimal. Solutions to the LP relaxation also provide further valid inequalities to reduce total solve time.

We implement these techniques on simulated data presented in previous papers, and compare our solution

technique to previous methods and a direct MIP formulation. We prove optimality in many problem instances

that have not previously been proven.

Keywords: Network ﬂows, Scheduling, Maintenance planning, Benders Decomposition, Lazy constraints

1. Introduction

Network design and scheduling problems are an important area of study, particularly as they have

widespread practical applications. Examples of these problems include minimising the cost of maintaining a

network [1], restoring a damaged network [2] or extending an existing network [3]. In practice, networks are

often large, and optimising their design can be diﬃcult and time-consuming. Industry is always interested

in any improvements to operations that result in reduced costs.

Benders decomposition is a powerful technique for breaking a diﬃcult mixed integer program (MIP) into

smaller, easier to solve problems [4]. It has been successfully applied to a number of problems, particularly

network design and facility location problems. This technique is especially powerful when the sub-problems

can be disaggregated to allow us to add stronger disaggregated Benders cuts. This can make their solutions

easier to obtain. Magnanti and Wong (1984) [10] showed that the use of pareto optimal cuts with Benders

decomposition can improve convergence time by up to (or exceeding) 50 times. In 2008 Camargo, Miranda

and Luna [5] applied disaggregated Benders decomposition to the design of hub-and-spoke networks. Tang

and Saharidis [3] used disaggregated Benders decomposition for solving a capacity facility location prob-

lem with existing facilities which could be removed or extended, and Lusby, Muller and Petersen [6] used

disaggregated Benders decomposition for scheduling the maintenance of power plants in France.

Preprint submitted to

March 9, 2016

Network design and scheduling problems are perfect candidates for Benders decomposition, because they

can be separated into sub- and master problems. The sub-problem is ﬁnding the maximum ﬂow through

the network, and the master problem is designing the network to have the greatest ﬂow subject to design

constraints. Disaggregated Benders decomposition can be applied if the ﬂow at any time is independent of

the ﬂow at other times. This often makes the solution of large network design problems easier.

We consider a problem of performing maintenance on a network to minimise the impact on total ﬂow

through the network. Boland et. al.

[1] solved this problem to near-optimality using heuristics. In this

paper we show that this problem can be solved faster using disaggregated Benders decomposition and lazy

constraints. The problem formulation we use is similar to that of Boland et. al, with diﬀerent notation and

a necessary change to one constraint. We then decompose the problem into a master problem and a set of

sub-problems, and apply disaggregated Benders decomposition to solve it.

The networks considered by Boland et. al. [1] are rail networks for moving coal from the Hunter Valley to

shipping terminals in Newcastle. The arcs of the network represent railway lines and the nodes are junctions,

where it is possible to choose which direction to take. In the real network the shipping terminal is comprised

of diﬀerent machines and railways that also are modelled as being part of the same network, and similarly

have maintenance jobs assigned to them.

The rest of this section will be a short description of the max-ﬂow min-cut theorem which is useful in

solving this problem. In Section 2 we deﬁne the problem and present the formulation. Section 3 is where we

describe the use of Benders decomposition and lazy constraints to separate and solve the problem. We also

look at a scenario which makes this problem more diﬃcult to solve, and prove the pareto optimality of our

Benders cuts. In Section 4 we present our results and compare them to those found by Boland et. al. [1], as

well as to the direct MIP implementation in our version of Gurobi. Section 5 contains concluding remarks.

1.1. Max-ﬂow min-cut theorem

The max-ﬂow min-cut theorem was discovered and proven by Elias, Feinstein and Shannon [7], and

independently also by Ford and Fulkerson [8], in 1956. We will use the nomenclature from Elias, Feinstein

and Shannon in talking about the max-ﬂow min-cut theorem.

A cut-set of a network is deﬁned as a set of arcs which, when removed, prevents all ﬂow from the source

to the sink. This does not necessarily have to make the graph disconnected, as arcs are allowed to ﬂow

backwards, but there will be no complete path ﬂowing forwards from the source to the sink. A simple cut-

set is a cut-set which would no longer be a cut-set if any arc was omitted from it, i.e. it has no unnecessary

arcs. The value of a cut-set is the sum of the capacities of all arcs in the set. A minimal cut-set is the cut-set

with the smallest value of all possible cut-sets of the network.

With these deﬁnitions, we can state the max-ﬂow min-cut theorem, which we have paraphrased from

Elias et. al. [7]:

2

Theorem 1. The maximum possible ﬂow from the source to the sink through a network is equal to the

minimum value among all simple cut-sets.

For proof of this theorem we refer the reader to the original paper by Elias et. al.

[7]. We use this

theorem to place bounds on the ﬂow of the network based on the availability of arcs which are in a simple

cut-set, especially the minimum cut-set. Another result from this theorem is that when solving network ﬂow

problems without weighted demand, the dual variables associated with the capacity constraints will either

be 1 if the arc is in the minimum cut-set, since removing the arc reduces the total ﬂow by the capacity of

the arc, or zero otherwise.

2. Problem deﬁnition

We start with a network G = (V, A) where V is the set of nodes and A is the set of directed arcs. Without

loss of generality, assume the network has only one source and one sink, and that there is a directed arc

from the sink to the source. This can be achieved by adding two new nodes - one source and one sink - and

then adding directed arcs from the new source to the original sources, and from the original sinks to the new

sink. The arc that connects the sink to the source now measures the total ﬂow through the network. It has

suﬃciently high capacity that it does not restrict ﬂow through the network, and is denoted arc 0.

Every arc a ∈ A has capacity Capa. For every node v ∈ V , we deﬁne δ−(v) and δ+(v) as the set of arcs
entering and leaving node v respectively. Next we set up the notation for the maintenance requests which
take place over a set of time periods t ∈ T . For every maintenance request r ∈ R, we denote the duration of
the request as Durr ∈ N, the release time as Rer ∈ T and the deadline as Der ∈ T . Let Ra ⊆ R be the set
of requests to be performed on arc a.

As in [1], we make the assumption that no two jobs in Ra for any a can overlap. This allows us to

strengthen the main scheduling constraint which now says if any maintenance job on a particular arc has

started less than Durr time periods previously (i.e. it is still running), then the arc is closed, otherwise it

is open. If maintenance is being performed on an arc, then the ﬂow along that arc must be 0. If the arc is

open, then the ﬂow must not exceed Capa.

We can now deﬁne the problem as a mixed integer program which we will use as a basis for our solution

techniques. The decision variables are:

• xat is the ﬂow over arc a at time t.
• yat is 1 if arc a is operational at time t and 0 if it is undergoing maintenance.
• Startrt is 1 if maintenance request r starts at time t and 0 otherwise.

The problem maximum total ﬂow with ﬂexible arc outages (MaxTFFAO) is now as follows:

3

xat = 0

a∈δ+(v)

Maximise

(cid:88)
xat − (cid:88)
Subject to:(cid:88)

t∈T

x0t

a∈δ−(v)
xat ≤ Capayat

Der−Durr+1(cid:88)
(cid:88)

t=Rer

yat +

r∈Ra

xat ≥ 0,

Startrt = 1

min{t,Der}(cid:88)

Startrt(cid:48) = 1

t(cid:48)=max{Rer,t−Durr+1}
yat ∈ {0, 1},

Startrt ∈ {0, 1}

∀v ∈ V,∀t ∈ T

∀a ∈ A,∀t ∈ T

∀r ∈ R

∀a ∈ A,∀t ∈ T

∀a ∈ A,∀t ∈ T,∀r ∈ R

(1)

(2)

(3)

(4)

(5)

(6)

The objective (1) is the sum of ﬂow across the arc connecting the sink to the source, which measures the

total ﬂow of the network, over the considered time periods. Constraints (2-3) ensure that ﬂow into and out

of a node are the same, and that ﬂow along any arc does not exceed the capacity. Constraint (4) ensures that

every maintenance job is performed exactly once, and (5) is the modiﬁed constraint to set the yat variable

appropriately.

3. Disaggregated Benders decomposition and lazy constraints

In this problem, the linear variables xat, and integer variables yat and Startrt, only occur together in

one constraint: the capacity constraint (3). This allows us to apply Benders decomposition by separating

out the linear variables into a sub-problem, and approximating the solution to the sub-problem with a new

variable θ. The result of this is a smaller, more relaxed problem which can be explored faster. The physical

interpretation of the parts of the decomposition is that the sub-problem is maximising the ﬂow through

the network and the master problem is the maintenance scheduling. Because the ﬂow through the network
at each time t ∈ T is independent of the ﬂow at other times, we can further break up the problem by
disaggregating the sub-problems in t, so we solve one ﬂow problem for each time period.

Traditional Benders decomposition is useful because it results in a smaller, easier to solve master prob-

lem. Because the variables associated with the sub-problems are being approximated rather than computed

exactly, the master problem is more relaxed than the direct implementation which could result in a larger

branch-and-bound tree to ﬁnd the optimal solution. While this means more nodes need to be explored, the

simplicity of the problem means they can be explored more quickly, and the whole process should take less

time. Disaggregation of the sub-problems serves to magnify this eﬀect.

4

Assuming we have a feasible solution for yat, denoted by y∗

at, we can solve the sub-problems for each time

period. Each sub-problem is of the form:

Maximise x0t(cid:48)

Subject to:(cid:88)

xat(cid:48) − (cid:88)

a∈δ−(v)
xat(cid:48) ≤ Capay∗
at(cid:48)
xat(cid:48) ≥ 0

a∈δ+(v)

xat(cid:48) = 0

∀v ∈ V

∀a ∈ A
∀a ∈ A

(7)

(8)

(9)

(10)

for every t(cid:48) ∈ T . These are very small linear programs which can be easily solved by any good optimisation
software package. Having solved the sub-problems, the dual variables associated with constraint (9) provide

us with an estimate of the impact on the total ﬂow of switching each arc on or oﬀ, which will allow us to add

will consist of minimising (cid:80)

Capay∗

Benders cuts to the master problem. We denote these dual variables as uat(cid:48), and hence the dual problem

at(cid:48)uat(cid:48). We approximate the solutions to the sub-problems with new
variables in the master problem, θt(cid:48). For each t(cid:48) ∈ T , a valid upper bound on the objective of the dual
problem, and hence the objective of the primal problem, is given by:

a∈A

θt(cid:48) ≤(cid:88)
θt(cid:48) ≤(cid:88)

a∈A

Or

a∈A

(cid:16)

(cid:17)

Capau∗k
at(cid:48)

yat(cid:48) − y∗k
at(cid:48)

Capay∗k

at(cid:48)u∗k

at(cid:48) +

Capayat(cid:48)u∗k
at(cid:48)

∀k ∈ {1...K},∀t(cid:48) ∈ T

(11)

where K is the number of times we have added Benders cuts. In the traditional Benders decomposition,

Capayatu∗

at. Since this is just the sum of the disaggregated

we would instead have one constraint θ ≤ (cid:80)

(cid:80)

t∈T

a∈A

cuts, it cannot be as strong, which is why disaggregated Benders is much more powerful. We now present

the master problem for the disaggregated Benders decomposition formulation for the MaxTFFAO problem:

5

(cid:88)

t∈T

Maximise

θt

a∈A

Subject to:

θt ≤(cid:88)
Der−Durr+1(cid:88)
(cid:88)

t=Rer

yat +

r∈Ra

θt ≥ 0,

t(cid:48)=max{Rer,t−Durr+1}
yat ∈ {0, 1},

Startrt ∈ {0, 1}

Startrt(cid:48) = 1

Capayatu∗k

at

Startrt = 1

min{t,Der}(cid:88)

∀k ∈ {1...K},∀t ∈ T

∀r ∈ R

∀a ∈ A,∀t ∈ T

∀a ∈ A,∀t ∈ T,∀r ∈ R

(12)

(13)

(14)

(15)

(16)

Another advantage stemming from the disaggregation of the ﬂow problems is that they now only depend
on the conﬁguration of arcs yat(cid:48) for each t(cid:48) ∈ T . We keep solutions to these ﬂow problems in a Python
dictionary, where the key is the vector (yat(cid:48)|∀a ∈ A). If we have already solved a ﬂow problem, we can recall
it from memory rather than calculating it again. The number of times we solve and recall ﬂow problems will

be shown in the results section for some cases. There are several improvements we have made to increase

the eﬀectiveness of the solver.

During the solve process, whenever we encounter an integer solution, we check to see if it violates

constraint (11). If it does, we need to add another cut to the problem, which we add as a lazy constraint.

This constraint is added to the pool of lazy constraints used by Gurobi for solving the master problem. This

is the most eﬃcient way of implementing Benders decomposition. Our implementation of the base case with

only what has been described above will be referred to as DBD.

3.1. Initial cuts

We begin by considering the case where all arcs are turned on, i.e. yat = 1 ∀a ∈ A,∀t ∈ T . This
gives us an upper bound on the ﬂow through the network in any case, because turning an arc oﬀ cannot

possibly increase the total ﬂow. The set of arcs which have a non-zero dual variable associated with their

capacity constraints is a “minimum cut-set” from the max-ﬂow min-cut theorem [7]. In other words, they

are bottlenecks of the network, since turning any of them oﬀ will directly aﬀect the total ﬂow through the

network.

Consider the trivial case in Figure 1. To start with, the arc with capacity 10 is the bottleneck. It is the
only arc with a non-zero dual variable, so the initial cut will be θt ≤ 10y2,t for every t. Next, we change the
capacity of this arc to be larger than that of the total ﬂow arc (i.e. 100). When we solve the ﬂow problem

again, we see that arc 1 will be the bottleneck. Since it is the only arc with a non-zero dual variable, we can

6

Figure 1: Example of placing cuts on bottlenecks. Numbers next to arcs are of the form: ArcID - capacity (ﬂow). Dashed lines

are arcs whose associated dual variables are non-zero.

add another cut θt ≤ 20y1,t for every t. We increase the capacity of this arc as before, calculate the solution
to the new ﬂow problem, and ﬁnd that the total ﬂow arc is now the bottleneck. When this occurs, we are

ﬁnished adding initial cuts.

Both of these initial cuts are valid, since turning oﬀ either of these arcs will restrict all ﬂow through the

network and θt = 0. For larger problems, the bottlenecks will consist of multiple arcs, and the cuts provide

information about how closing arcs in those bottlenecks aﬀects the ﬂow.

Consider now the less-trivial example in Figure 2. This is a layered network, where the sum of the

capacities of arcs 3-6 is 10 and of arcs 7-10 is 20. In this case the initial cut-set will be arcs 3-6 since they

form the minimum cut-set. The cuts we add will be:

θt ≤ 2y3,t + 3y4,t + 4y5,t + y6,t

∀t ∈ T

(17)

Because this is a minimum cut-set by the max-ﬂow min-cut theorem, cutting any of these arcs will reduce

the total ﬂow through the network. When we increase these capacities and solve the maximum ﬂow problem

again, we ﬁnd the second cut-set of arcs 7-10. We then add the cuts:

θt ≤ 4y7,t + 8y8,t + 5y9,t + 3y10,t

∀t ∈ T

(18)

which are also valid.

With these initial cuts, we can ﬁnd a solution to the master problem which satisﬁes our scheduling

constraints. We then solve the problem as before, except now we start with a tighter LP bound. As we

encounter nodes of the branch-and-bound tree, we check to see if more cuts are needed, and add them as

7

lazy constraints. This implementation will be labelled as Pre-cuts.

Another potential improvement is, while the MIP solver

is exploring nodes, the cuts we add may cut oﬀ the current

values of θt. When this happens, we can construct a feasible

solution with the same values of yat and Startrt, but set θt

equal to the solutions to the sub-problems that were solved,

and suggest this to Gurobi as a heuristic solution.

In our

experiments, it has in some cases led to signiﬁcant jumps in

reducing the optimality gap. This implementation is called

MAIN.

Finally, we can start by relaxing the integrality con-

straints and running the main algorithm to add Benders cuts

as in (11). We can do this repeatedly until the objective

value found by the relaxed problem stops decreasing. Once

this occurs, we restore the integrality constraints and solve

the problem once more. This results in a tighter LP bound

and often leads to great improvements in the solve time of

the MIP. However, the time it takes to solve the relaxed

problem multiple times must be taken into account. The full

implementation with all features is called LP-R.

Figure 2: A layered network. The edges are labelled

by ArcID (capacity). The two layers have capacities

of 10 and 20.

3.2. Strength of the LP-Relaxation

All jobs r ∈ R can start during the time window [Rer, Der − Durr + 1]. If the size of this window is
larger than the duration of the job, then the LP-relaxation of the problem provides a weaker bound. This is

because it is possible to fractionally assign values to Startrt and thus have arcs fractionally open for more

than the duration of the maintenance. The result of this is a weaker LP bound on the objective value and

thus a longer solve time, which applies to all MIP implementations.

Consider the example of a job r where Rer = 0, Durr = 10 and Der = 29. This job could start at
times t(cid:48) ∈ [0, 20]. We are only considering the scheduling constraints (4-5) here. When the variables yat and
Startrt are allowed to be linear, it is possible for Startrt to take values of 1
3 at times 0, 10 and 20, and 0
elsewhere. Because in constraint (5) we sum the Startrt over values of t(cid:48) within Durr time periods previous
to t, at every t ∈ [Rer, Der],

(cid:88)

min{t,Der}(cid:88)

r∈Ra

t(cid:48)=max{Rer,t−Durr+1}

Startrt(cid:48) =

1
3

8

This implies that

yat +

1
3

= 1

yat =

2
3

∀a ∈ A,∀t ∈ T
∀a ∈ A,∀t ∈ T

This means that the arc the job is being performed on will be fractionally closed for the almost the entire

time between Rer and Der, whereas in the integer program it must be fully closed for Durr. If closing this

arc results in a change in the minimum cut-set of the network, but fractionally closing the arc does not,

then the relaxed problem will not properly reﬂect the impact on the objective value from closing this arc.

When exploring the branch-and-bound tree, we give branching priority to the yat over the Startrt variables

to allow more balanced branching to occur. That is, the impact of setting a branch variable to 1 is similar

to the impact of setting it to 0.

3.3. Algorithm details

We have included pseudo-code for our algorithms to give a brief idea of how our implementations are

set up. The ﬁrst is the main procedure, which includes potential calls to the sub-routines PRE-CUTS and

LP-RELAX, depending on which implementation is being used. When we talk about building models, we

are referring to creating a Model object in Gurobi.

Algorithm 1 Main Procedure

Initialise information about Network and Jobs: N, A, Capa, R, Ra, T

Create empty dictionary Y to hold solutions to Sub-Problems

Build Sub-Model and Master Model

Initialise Sub-Model variables xa and Master Model variables yat, Startrt
Set yat = 1 ∀a ∈ A, ∀t ∈ T
OPTIMISE Sub-Model for one time value
Add constraints: θt ≤ x0 ∀t ∈ T

if Pre-cuts then

Run procedure PRE-CUTS

if LP-Relax then

Run procedure LP-RELAX

Run procedure OPTIMISE MASTER MODEL

9

Because we have disaggregated the sub-models in time, and they are all identical, we only need to build

one model and use it to solve the ﬂow sub-problems for all time periods. The results of these sub-problems
are stored in a dictionary Y. For any time t(cid:48), yat(cid:48) ∀a ∈ A will be the conﬁguration of arcs of the network,
i.e. which arcs are open and which are closed.

Algorithm 2 OPTIMISE MASTER MODEL

OPTIMISE Master Model with callback MMCB

MMCB:

if Found new incumbent solution then

for all t(cid:48) ∈ T do

Retrieve values of yat(cid:48) and pass to Sub-Model

if not yat(cid:48) in Y then

OPTIMISE Sub-Model
Y[yat(cid:48)] ← (x0, xa ∀a ∈ A, ua ∀a ∈ A)

else

(x0, xa ∀a ∈ A, ua ∀a ∈ A) ← Y[yat(cid:48)]

t ← x0
¯θ(cid:48)
if θ(cid:48)

if (cid:80)

θt > (cid:80)

t ≤ (cid:80)

a∈A

t > x0 then
Add lazy constraint θ(cid:48)

Capayat(cid:48)ua

¯θt then

t∈T
Suggest θt = ¯θt ∀t ∈ T as heuristic solution

t∈T

The conﬁguration yat(cid:48) is the key to a dictionary entry which holds a tuple. The ﬁrst value is the total
ﬂow through the network and the second is a vector ua ∀a ∈ A of the dual variables associated with the
capacity constraints of each arc.

When we need the solution to a sub-problem for a certain conﬁguration of arcs, we ﬁrst check to see if

we have already solved it. If yat(cid:48) is a valid key to the dictionary we simply recall the tuple stored in that

entry. If that particular conﬁguration has not been solved, we pass the values yat(cid:48) to the sub-problem model

and solve the max-ﬂow problem. We then store the results of this in the dictionary under the key yat(cid:48).

The last “if” statement of Algorithm 2 is the addition we made to the MAIN implementation that is not

present in DBD or Pre-cuts. After all new lazy constraints have been added, it is possible that the current

values of θt may violate them, so we can suggest a heuristic solution to the solver instead. This solution uses
the same conﬁguration of arcs yat ∀a ∈ A, ∀t ∈ T , however we set the values of θt to the solutions of the
sub-problems, which we know do not violate the new constraints.

10

Algorithm 3 PRE-CUTS

Retrieve dual variables from Sub-Model ua ∀a ∈ A
while not u0 > 0 do

Add constraints θt ≤ (cid:80)

Capayatua ∀t ∈ T

a∈A

for all a ∈ A do
if ua > 0 then

Capa = Cap0 + 1

OPTIMISE Sub Model
Retrieve dual variables from Sub-Model ua ∀a ∈ A

Reset values of Capa ∀a ∈ A

Algorithm 4 LP-RELAX

Relax integrality constraints for yat and Startrt

while True do

Run procedure OPTIMISE MASTER MODEL
for all t(cid:48) ∈ T do

Retrieve values of yat(cid:48) ∀a ∈ A and pass to Sub-Model
OPTIMISE Sub-Model
if θ(cid:48)

Capayat(cid:48)ua

t > x0 then
Add constraint θ(cid:48)

t ≤ (cid:80)

a∈A

if Objective has not improved, time limit expired or max iterations reached then

Exit While

Enforce integrality constraints for yat and Startrt

11

3.4. Proof of pareto optimality of Benders cuts

It has been shown that the use of pareto optimal cuts can greatly improve the convergence rate of Benders

decomposition [3, 9, 10]. Pareto optimal cuts are especially powerful when there is degeneracy in the sub-

problems of the Benders decomposition, which is the case in network design problems [10]. The deﬁnitions

of dominating and pareto optimal cuts we use come from Magnanti and Wong (1981) [9], but have been

modiﬁed to match our problem. The Benders cuts we add are particularly useful because they are pareto

optimal. Before we prove this, we ﬁrst must make some deﬁnitions and statements.

Since these cuts are disaggregated in time, we will omit all t parameters for simplicity. This means we will

consider θ instead of θt, and likewise xa, ya, ua. Since our Benders cuts involve a subset of our y variables,

we can write them in a general form θ ≤ ¯θk(y), where ¯θk(y) = (cid:80)

Capayauk
a.

a∈A

Deﬁnition 1. A Benders cut θ ≤ ¯θk(y) dominates another Benders cut θ ≤ ¯θl(y) if ¯θk(y) ≤ ¯θl(y) for all
feasible y and is a strict inequality for at least one feasible y.

The contrapositive of this is that if there exists a feasible solution y∗ such that ¯θk(y∗) > ¯θl(y∗), then

θ ≤ ¯θk(y) does not dominate θ ≤ ¯θl(y).
Deﬁnition 2. A Benders cut θ ≤ ¯θk(y) is considered pareto optimal if it is not dominated by any other
Benders cuts.

That is to say, if for any other Benders cut θ ≤ ¯θl(y) you can ﬁnd a feasible solution y∗ such that

¯θk(y∗) < ¯θl(y∗), then θ ≤ ¯θk(y) is pareto optimal.

Observation 1. For a given network conﬁguration ya and its primal and dual ﬂow solutions xa and uk
set Ak = {a ∈ A|uk

a > 0} ⊂ A must constitute a simple cut-set of the original network.

a, the

This comes from the max-ﬂow min-cut theorem. Because our network has uniform demand weighting,
a = 1} ⊂ A. If the set is not a simple cut-set, then the Benders
we can redeﬁne this set as Ak = {a ∈ A|uk
cut generated by the set will not be pareto optimal. If Ak is not a cut-set, then we choose the network
conﬁguration y∗ where all arcs in Ak are closed and all other arcs are open. Since Ak is not a cut-set, it
is still possible for ﬂow between the source and the sink to occur, so θ > 0, however we also have that

a = 0. This means the constraint generated by Ak is invalid.

a∈Ak
If the set Ak is a cut-set but not a simple cut-set, then there exists an arc a(cid:48) ∈ Ak such that Ak \ {a(cid:48)} is
still a cut-set. Let Al = Ak \ {a(cid:48)}, which means Al ∪ {a(cid:48)} = Ak. Now the Benders cuts generated by these
two sets can be compared:

θ ≤ (cid:80)

Capay∗

θ ≤ (cid:88)

a∈Al

(cid:88)

a∈Al

(cid:88)

a∈Al∪{a(cid:48)}

(cid:88)

a∈Ak

Capaya <

Capaya =

Capaya

(19)

Capaya + Capa(cid:48)ya(cid:48) =

12

So the Benders cut θ ≤ ¯θl(y) dominates θ ≤ ¯θk(y), and thus the Benders cut generated by Ak cannot be

pareto optimal. Using this we can show that all the Benders cuts we generate are pareto optimal.

Theorem 2. Given a simple cut-set Ak, the Benders cut θ ≤ (cid:80)

Capaya = ¯θk(y), is pareto optimal.

a∈Ak

Proof. Let Ak be a set of arcs which form a simple cut-set of the network, and θ ≤ ¯θk(y) is the Benders cut
generated by this set. This cut-set does not have to be minimal in the original network. Now, for any other
a > 0} ⊂ A, and Ak (cid:54)= Al. If we compare
Benders cut θ ≤ ¯θl(y), we have another cut-set Al = {a ∈ A|ul
these two Benders cuts, we get

θ ≤ (cid:88)
θ ≤ (cid:88)

a∈Ak

a∈Al

Capaya = ¯θk(y)

Capaya = ¯θl(y)

(20)

(21)

For θ ≤ ¯θk(y) to be pareto optimal, we need to ﬁnd a solution y∗ such that ¯θk(y∗) < ¯θl(y∗). Because

Ak (cid:54)= Al, we can choose an arc a(cid:48) such that a(cid:48) ∈ Al and a(cid:48) /∈ Ak. Now we can take the solution

which is to turn oﬀ all arcs in Ak and open all other arcs. Our Benders cuts now look like:

y∗
a =

1, otherwise

0, a ∈ Ak
θ ≤ (cid:88)

Capay∗

a = 0

a∈Ak
Capay∗

a ≥ Capa(cid:48) > 0

θ ≤ (cid:88)

a∈Al

(22)

(23)

(24)

So θ ≤ ¯θl(y) does not dominate θ ≤ ¯θk(y). Since Al is arbitrary, we have that θ ≤ ¯θk(y) is pareto

optimal.

4. Results and comparison

We tested our implementation on the same data as Boland et. al [1], using several implementations. We

started with disaggregated Benders decomposition using lazy constraints (DBD), added the tighter initial

cuts (Pre-cuts), suggested new solutions as heuristics (MAIN) and solved the LP relaxation beforehand (LP-

R). Our program is implemented in Python 2.7.10 (64-bit) and uses the Gurobi 6.5 (64-bit on 8 threads)

solver package [11], running on a machine with Windows 8.1 Enterprise (64-bit). The machine has an Intel

Core i7-3770 (3.40GHz) with 8GB of RAM.

13

Data we collected included the total run time of the program, the optimality gap, the number of lazy

cuts added, branch-and-bound nodes explored and how many times the sub-problem was solved and recalled

from memory. Total run time was recorded as an integer number of seconds. For the LP-R implementation,

we also recorded how many times the relaxed problem was solved.

Because Capa ∈ N for our data sets, all feasible solutions will have integer objective values. This allows
us to set a termination condition for the MIP gap, since an absolute gap of less than one is suﬃcient to prove

optimality. Without this condition, some programs ﬁnd the optimal solution in less than 10 minutes, then

spend vast amounts of time trying to close the gap by exploring hundreds of thousands of nodes. Since this

is unnecessary, we will terminate the program if the absolute gap is less than 0.999.

Comparing our results with those of Boland et. al.

is not a simple task. As they used a straight MIP

formulation in CPLEX and a number of heuristics, it is diﬃcult to report optimality gaps. For the heuristics,

the optimality gap is computed using the best upper bound found by the CPLEX implementation, which is

not proving optimality in many cases. This means that their optimality gaps are over-estimates, and their

heuristics may be closer to optimality than reported.

4.1. Simulated Data

Each of the three constructed data sets from Boland et. al. has eight networks of increasing size, and

each network has 10 randomly-generated lists of maintenance requests. For all instances, the number of jobs

per arc is between ﬁve and 15, and the duration is between 10 and 30 time steps. For the ﬁrst instance, the

number of possible starting times for each job ranges between one and 35, whereas in the second instance

set, each job has between 25 and 35 potential start times. The second instance set is thus more diﬃcult to

solve in general, because there is a much higher chance of having jobs where the potential starting window

is larger than the duration of the job, which causes the problem discussed in section 3.2. Finally, there is a

third instance set where the number of possible starting times for each job is between one and 10. This is

an especially easy case as there will almost never be a job with the aforementioned problem.

Table 1 shows the average, minimum and maximum optimality gaps achieved while solving the ﬁrst

instance set using our four implementations. It also includes the number of maximum ﬂow sub-problems

solved, number of times solutions were recalled from memory, and number of lazy constraints added to the

problem. The ﬁrst three networks were all solved to optimality in less than ﬁve minutes, and even a number

of the instances for the larger networks found optimal solutions in the time frame. The average optimality

gap for MAIN and LP-R always was less than 0.4%, and the maximum gap for LP-R never exceeded 0.61%.

The second instance set was more diﬃcult to solve, as expected. Table 2 shows that in many cases the

problems did not solve to optimality within ﬁve minutes, though the gap was closed signiﬁcantly more than

in Boland et. al. [1]. Many instances closed to within 5% of optimality in the ﬁve-minute time window, and

the average gap for MAIN was less than 3% in all cases.

14

Table 1: Average, minimum and maximum optimality gap, number of schedules solved to optimality, number of ﬂow problems

solved and recalled and number of lazy constraints generated for instance set 1 after 5 minutes

avg. gap (%)

DBD

min gap (%)

max gap (%)

num solved

1

0.000

0.000

0.000

10

2

0.000

0.000

0.000

10

3

0.000

0.000

0.000

10

4

0.027

0.000

0.101

3

5

0.416

0.167

0.583

0

6

0.058

0.000

0.180

5

7

0.202

0.000

0.843

1

8

0.088

0.000

0.757

7

# ﬂow solved

3114

6587

3631

18141

20272

19186

18885

16248

# ﬂow recalled

27757

37300

14064

38396

39500

43362

29798

25722

# lazy gen.

avg. gap (%)

Pre-cuts

min gap (%)

max gap (%)

num solved

3189

0.000

0.000

0.000

10

4348

0.000

0.000

0.000

10

2657

0.000

0.000

0.000

10

4502

0.027

0.000

0.085

3

4964

0.308

0.223

0.368

0

6516

0.025

0.000

0.098

6

4386

0.234

0.000

1.474

2

4281

0.120

0.000

1.162

7

# ﬂow solved

2352

5024

2950

16015

16215

13952

21081

12211

# ﬂow recalled

22124

31866

13449

40018

35549

34483

32914

24449

# lazy gen.

avg. gap (%)

MAIN

min gap (%)

max gap (%)

num solved

1033

0.000

0.000

0.000

10

1583

0.000

0.000

0.000

10

1053

0.000

0.000

0.000

10

1834

0.036

0.000

0.130

3

1743

0.344

0.159

0.465

0

2163

0.025

0.000

0.097

7

1838

0.191

0.000

1.151

1

1341

0.034

0.000

0.277

6

# ﬂow solved

2352

4973

2950

17227

15660

13918

20346

13754

# ﬂow recalled

22324

31015

13449

42412

31096

34519

28337

25111

# lazy gen.

avg. gap (%)

LP-R

min gap (%)

max gap (%)

num solved

1033

0.000

0.000

0.000

10

1541

0.000

0.000

0.000

10

1053

0.000

0.000

0.000

10

1820

0.041

0.000

0.101

3

1768

0.387

0.238

0.543

0

1955

0.037

0.000

0.165

6

1764

0.143

0.000

0.609

2

1413

0.031

0.000

0.226

7

# ﬂow solved

2213

4243

2424

14297

13419

13887

15000

13200

# ﬂow recalled

21562

27245

11572

35927

26629

35149

23766

21957

# lazy gen.

avg. LP solves

781

3.5

1071

3.9

700

4.0

1507

1358

1751

1228

1043

3.9

4.5

4.3

3.7

3.4

15

Table 2: Average, minimum and maximum optimality gap, number of schedules solved to optimality, number of ﬂow problems

solved and recalled and number of lazy constraints generated for instance set 2 after 5 minutes

avg. gap (%)

DBD

min gap (%)

max gap (%)

num solved

1

0.000

0.000

0.000

10

2

0.529

0.053

1.586

0

3

0.000

0.000

0.000

10

4

0.291

0.000

0.913

2

5

2.771

0.158

5.609

0

6

4.227

2.014

9.121

0

7

0.621

0.006

2.800

0

8

0.424

0.000

1.135

3

# ﬂow solved

5932

13573

4353

17512

20770

28459

26945

24295

# ﬂow recalled

43280

54534

13340

42770

30478

48951

37984

26501

# lazy gen.

avg. gap (%)

Pre-cuts

min gap (%)

max gap (%)

num solved

3944

0.000

0.000

0.000

10

5984

0.442

0.116

1.166

0

2800

0.000

0.000

0.000

10

4572

0.622

0.000

2.630

2

5524

3.300

0.316

7.417

0

7879

2.819

0.482

6.165

0

4794

0.994

0.002

3.093

0

4965

0.302

0.000

1.516

6

# ﬂow solved

3853

11808

2860

13058

14770

16064

21034

16490

# ﬂow recalled

28257

53700

13334

36906

26575

33510

31870

27193

# lazy gen.

avg. gap (%)

MAIN

min gap (%)

max gap (%)

num solved

1357

0.000

0.000

0.000

10

2693

0.486

0.092

1.368

0

966

0.000

0.000

0.000

10

1626

0.836

0.000

2.615

2

1587

2.646

0.089

5.123

0

2075

2.819

0.695

6.165

0

1842

0.638

0.072

1.259

0

1757

0.308

0.000

1.516

6

# ﬂow solved

3687

11087

2860

12434

11740

14687

19735

17752

# ﬂow recalled

27921

51920

13334

30628

22189

29880

25855

23726

# lazy gen.

avg. gap (%)

LP-R

min gap (%)

max gap (%)

num solved

1298

0.000

0.000

0.000

10

2462

0.508

0.078

1.598

0

966

0.000

0.000

0.000

10

1728

0.723

0.000

3.327

2

1469

2.826

0.201

6.867

0

2080

4.130

0.602

9.209

0

1807

0.561

0.122

1.455

0

1851

0.337

0.000

2.369

6

# ﬂow solved

3683

10773

2494

14180

10696

14411

18712

13693

# ﬂow recalled

28336

52240

11400

29075

19633

26552

24373

21373

# lazy gen.

1075

2202

avg. LP solves

2.0

2.0

746

2.0

16

1637

1387

1852

1672

1475

2.0

2.0

2.0

2.0

2.0

The number of times the relaxed LP was solved was interesting. Our algorithm ran the relaxed LP and

added cuts until the objective function stopped decreasing. For all cases in instance set 2, it solved the LP

twice, which means there was no improvement after the ﬁrst set of cuts. This implies that there was no

direct beneﬁt to adding cuts based on the solution to the relaxed problem. This was also be seen from the

fact that in many cases the average gap of LP-R was similar to or greater than that of MAIN.

We ran only the MAIN and LP-R programs for 30 minutes. Table 3 shows that for instance set 1, 70%

of all schedules were solved to optimality. Even though none of the schedules for network ﬁve were solved

to optimality, every schedule we tested was solved to within 0.3% by both methods. LP-R solved one more

schedule in total than MAIN, however MAIN was more consistent in closing the optimality gap. The extra

run time resulted in a tighter gap and four extra schedules being solved to optimality in instance set 2, as

seen in Table 4. The average gap was now less than 1.6% for LP-R and 1.3% for MAIN. The maximum gap

had dropped from 9.2% to 3.3% for LP-R, and all solutions for MAIN achieved a gap of less than 2.6%.

While not always the best, our MAIN implementation performed consistently well compared to other

methods, especially for the large networks. We also compared our method to the direct MIP formulation

in Gurobi. This is for comparison with state-of-the-art general purpose solvers. Gurobi is eﬀective for

solving linear programs and small integer programs, though larger integer problems may beneﬁt from a

tailored approach, such as the one we describe. What is clear is that our method has better scalability than

the direct MIP implementation. For the ﬁrst four networks, Gurobi performs comparably, however as the

networks get larger, the direct MIP fails to solve even the linear relaxation in 30 minutes.

Table 3 shows that the direct implementation had diﬃculty in solving many of the cases. With the

exception of network 3, the average gap for the MIP was always at least six times larger than for LP-R.

Table 4 shows how the MIP struggled with the larger networks, and again with the exception of network 3,

fewer cases were solved to optimality than by MAIN and LP-R. In instance set 2, the direct implementation

failed to solve any of the linear relaxations of the job schedules of network 8.

Network 3 is an interesting case because of the number of nodes explored. In general, Benders decompo-

sition yields a more relaxed problem. This means that the branch-and-bound tree should be larger, which is

acceptable because the nodes can be explored much more quickly. However, in the case of network 3, espe-

cially for instance set 2, the number of nodes explored by our implementations is fewer than those explored

by the direct MIP. This is because the objective value of the solution to the relaxed problem is equal to or

very close to the IP solution’s objective value. The result of this is that Gurobi must extract an integer

solution with a similar objective value, and it is easier to do this for smaller models, which is why fewer

nodes are explored by our implementations. For instance set 2, the solver-added cuts are suﬃcient to ﬁnd a

solution at the root node, so that branch-and-bound is not required.

Moreover, in most instances we found that Gurobi is able to add cuts at the root node much more

17

Table 3: Average and maximum run times, average, minimum and maximum optimality gap and number of schedules solved

to optimality for instance set 1 after 30 minutes

avg. time (s)

MAIN

max time (s)

avg. gap (%)

max gap (%)

num solved

1

13.3

31.0

0.000

0.000

10

2

41.3

58.0

0.000

0.000

10

3

12.3

23.0

0.000

0.000

10

4

5

6

7

8

1169.3

1800.7

357.9

1371.3

542.6

1800.0

1803.0

1482.0

1801.0

1801.0

0.011

0.048

4

0.172

0.292

0

0.000

0.000

10

0.047

0.223

3

0.004

0.024

8

avg. nodes

1333.0

2528.0

912.7

79959.9

24851.9

21291.0

25772.4

11528.5

avg. time (s)

LP-R

max time (s)

avg. gap (%)

max gap (%)

num solved

avg. nodes

avg. time (s)

14.1

30.0

0.000

0.000

10

586.6

854.2

38.8

62.0

0.000

0.000

10

1957.0

1804.1

13.2

18.0

0.000

0.000

10

74.0

44.7

1070.6

1801.9

457.3

1346.8

524.9

1800.0

1819.0

1800.0

1827.0

1800.0

0.011

0.059

6

0.182

0.293

0

0.000

0.003

9

0.049

0.266

3

0.007

0.055

8

58234.4

23513.9

31531.1

20548.7

11093.7

1808.1

1813.8

1808.5

1816.1

1822.4

Direct MIP

max time (s)

1803.0

1805.0

221.0

1809.0

1837.0

1810.0

1817.0

1824.0

avg. gap (%)

max gap (%)

num solved

0.020

0.106

6

0.369

0.604

0

0.000

0.000

10

0.661

1.172

0

1.370

1.885

0

2.222

3.868

0

0.306

0.628

0

0.187

0.589

0

avg. nodes

85535.7

41985.7

1202.6

20318.8

7407.8

20615.8

7226.5

5877.6

num. LR Solved

10.0

10.0

10.0

10.0

10.0

10.0

10.0

10.0

18

Table 4: Average and maximum run times, average, minimum and maximum optimality gap and number of schedules solved

to optimality for instance set 2 after 30 minutes

1

2

avg. time (s)

40.8

1656.6

MAIN

max time (s)

avg. gap (%)

max gap (%)

num solved

103.0

0.000

0.000

10

3

12.7

14.0

4

5

6

7

8

1470.7

1802.0

1800.7

1663.0

651.4

1845.0

1811.0

1804.0

1863.0

1800.0

1800.0

0.163

0.000

0.188

1.298

1.067

0.462

0.000

0.502

2.525

1.842

2

2

0

0

0.369

1.022

0

0.074

0.514

8

avg. nodes

2731.0

58544.3

avg. time (s)

46.1

1734.6

1800.0

17895.8

7161.6

8203.7

15450.2

3731.5

1459.6

1800.6

1800.9

1803.9

610.5

1803.0

1804.0

1805.0

1825.0

1871.0

LP-R

max time (s)

avg. gap (%)

max gap (%)

num solved

109.0

0.000

0.000

10

0.170

0.000

0.209

1.565

0.971

0.512

0.000

0.639

3.321

1.711

2

0

0

0.275

1.279

0

0.078

0.537

8

avg. nodes

3357.1

59355.0

avg. time (s)

1629.5

1804.3

Direct MIP

max time (s)

1803.0

1806.0

19754.0

7798.0

8691.1

19436.4

2780.5

1709.9

1811.9

1809.2

1817.3

1847.0

1812.0

1810.0

1818.0

DNS

DNS

10

0.0

14.0

16.0

10

0.0

30.4

43.0

2

0

avg. gap (%)

max gap (%)

num solved

0.100

0.381

1

avg. nodes

97099.2

17865.8

num. LR Solved

10.0

10.0

2.244

0.000

0.761

3.505

5.968

153.276

100.000

4.179

0.000

1.828

6.746

8.526

1365.990

100.000

10

84.4

10.0

1

0

0

0

8530.0

1308.6

5127.8

376.4

10.0

10.0

10.0

9.0

0

0.0

0.0

19

Table 5: Average and maximum run times and average number of branch-and-bound nodes for instance set 3

avg. time (s)

DBD

max time (s)

1

3.8

4

2

5.4

6

avg. nodes

251.1

61.5

avg. time (s)

Pre-cuts

max time (s)

avg. nodes

avg. time (s)

MAIN

max time (s)

avg. nodes

avg. time (s)

max time (s)

avg. nodes

avg. LP solves

LP-R

4.0

4

0.4

4.0

4

0.4

6.0

7

0.2

3.9

6.5

7

0.4

6.4

7

0.4

8.8

9

0.0

4.2

3

6.1

7

0.6

7.2

8

1.0

7.2

8

1.0

13.9

15

15.2

14.1

15

15.1

10.1

18.37

11

0.0

4.1

19

3.7

4.8

19.4

22

125.0

19.4

22

125.3

25.4

29

4.3

5.0

4

12.7

20

5

18.5

20

416.0

191.8

6

12.7

14

41.3

16.5

19

0.0

16.6

19

0.0

20.3

23

0.0

4.1

7

36.2

58

1760.9

30.6

42

652.9

30.7

42

591.7

38.2

42

110.7

5.1

8

37.9

44

97.1

41.2

44

6.7

41.5

45

6.7

54.0

60

9.3

5.4

eﬀectively in the MAIN or LP-R models, compared to the Direct MIP. For example, in one instance the

direct implementation started the Branch and Bound process after 900s with an optimality gap of 3.51%,

while the MAIN implementation started after only 300s with a gap of 2.07%. This means that not only do we

generate speed improvements by being able to search the Branch and Bound tree more quickly, the number

of nodes in the Branch and bound tree also decreases due to the reduced model size and the improved bounds

found by Gurobi’s cutting algorithm.

This can be seen in Tables 3 and 4 from the average number of Branch and Bound nodes explored. In cases

where all problems solved to optimality, our implementations required fewer nodes to be explored due to the

tighter bounds and solver-added cuts. In the larger cases where no problems were solved to optimality, our

implementations explored more nodes due to the increased eﬃciency resulting from disaggregated Benders

decomposition.

Table 5 shows the average and maximum run times for the problems in instance set 3. All instances in

this set were solved to optimality in less than one minute. Because these instances are relatively easy to

solve, there is not much diﬀerence between our implementations. Solving the linear relaxation ﬁrst does not

add much to these cases because they are easy enough to solve without it. The biggest diﬀerence is in the

number of nodes explored. The pre-processing of bottlenecks greatly improves the initial bounds and results

in fewer nodes being explored.

Another common diﬀerence is the number of lazy constraints added during the solution process. Typically

20

Table 6: Comparison between MAIN and LP-R, with and without solver-added cuts, with regular or objective-value-focussed

solve priorities on 2010 data after two hours. All numbers are in Mt.

Standard

Focussed

Objective

Bound

Objective

Bound

Cuts

No Cuts

MAIN

LP-R

MAIN

LP-R

132.3

132.5

133.0

135.0

141.1

141.1

145.8

143.8

135.7

131.5

135.4

135.3

143.1

142.2

144.2

143.3

DBD has the most and LP-R has the fewest. This is because when we add the initial cuts, as in Pre-cuts,

we no longer have to add them later as lazy constraints. The same applies to LP-R: more cuts are added as

hard constraints before we begin the branch-and-bound process, so fewer lazy constraints are added.

4.2. Real-world Instances

We also tested our techniques on the instance sets derived from real-world data, provided by Boland

et. al.

[1]. The network is representative of the Hunter Valley Coal Chain, and there are two lists of jobs

designed to span one year each, for 2010 and 2011. The time is discretised into hours, so there are 8761 time

periods for each problem (since 2010 and 2011 are not leap years). The unrestricted ﬂow over one year is

161.3 Mt, and the success of an algorithm is measured in the minimisation of the impact on the network.

The majority of jobs have durations between 1 and 18 hours, while the potential time window is set at two

weeks for each job.

The fact that the potential job window is signiﬁcantly larger than the duration of the job in all cases

leads to the problem described in Section 3.2. The solver tends to start with a very weak LP bound and
takes more time to converge to a solution. After two hours, the optimality gap is 7.4% ± 2% in all cases. It
is possible to tell the solver to focus on ﬁnding better solutions rather than lowering the bound or proving

optimality.

In this case, the optimality gap is similar to before, however the objective values are higher

and comparable to the results in Boland et. al.

It is also possible to tell the solver not to add its own

cuts and to instead use only the constraints we have added. The main beneﬁt of this is that we enter the

branch-and-bound process much earlier and thus can explore more nodes. The drawback is that the upper

bound is not as tight as it would have been had the solver been allowed to add cuts.

When looking across years and solver focus, in three out of four cases LP-R found the highest objective

value, which is within 1.6 Mt of the best found solution in Boland et. al. for both instance sets. In most

cases focussing the solver towards ﬁnding better solutions does yield better objective values, however the

upper bound suﬀers as a result. Removing solver-added cuts does not yield much improvement when using

MAIN, whereas LP-R ﬁnds objective values between 1 and 4 Mt higher in every case, regardless of the solver

21

Table 7: Comparison between MAIN and LP-R, with and without solver-added cuts, with regular or objective-value-focussed

solve priorities on 2011 data after two hours. All numbers are in Mt.

Standard

Focussed

Objective

Bound

Objective

Bound

Cuts

No Cuts

MAIN

LP-R

MAIN

LP-R

138.0

136.3

136.9

139.0

145.5

145.6

149.2

147.4

139.0

138.9

139.0

139.9

146.3

146.1

148.0

147.4

focus. This is likely because the initial cuts added during the pre-cuts stage and the additional constraints

added from the relaxed problem are suﬃcient, and less time is spent at the root node adding cuts, so the

branch and bound process is started sooner.

While these methods cannot prove optimality in two hours, neither can a heuristic or a direct implemen-

tation in CPLEX or Gurobi. The heuristics used in Boland et. al. are very good and our methods return

similar objective values to theirs. If the solver focus were switched to providing a better upper bound, it

may make a good bounding tool for checking the heuristic solutions against.

Indeed, one can imagine a hybrid solution technique where two algorithms are run simultaneously, on

separate threads or separate computers. The current best solution found from the heuristic algorithm is

used to update the lower bound in the Benders decomposition algorithm, which in turn focuses on reducing

the upper bound.

5. Conclusion

We have shown that disaggregated Benders decomposition is an eﬀective technique for solving the MaxTF-

FAO problem. In many simulated cases optimal solutions can be found in a short enough time to be practi-

cally useful. The amount of choice in the real world problems makes it diﬃcult to prove optimality, however

reasonable solutions can be returned in a short amount of time. In conjunction with the advanced capa-

bilities of solvers such as Gurobi, disaggregated Benders decomposition can result in smaller problems with

tighter bounds and smaller branch-and-bound trees. In future we would like to consider more broadly the

class of integrated network design and scheduling problems to which this technique also applies. It would

also be interesting to look at other problems to which disaggregated Benders decomposition can be applied,

and see if similar beneﬁts can be obtained.

22

References

References

[1] H. W. N. Boland, T. Kalinowski, L. Zheng, Scheduling arc maintenance jobs in a network to maximize

total ﬂow over time, Discrete Applied Mathematics 163 (2014) 34–52. doi:10.1016/j.dam.2012.05.

027.

[2] J. M. T. S. S.G. Nurre, B. Cavdaroglu, W. Wallace, Restoring infrastructure sytems: An integrated

network design and scheduling (inds) problem, European Journal of Operational Research 223 (2012)

794–806. doi:10.1016/j.ejor.2012.07.010.

[3] W. J. L. Tang, G. Saharidis, An improved benders decomposition algorithm for the logistics facility

location problem with capacity expansions, Annals of Operations Research 210 (2013) 165–190. doi:

10.1007/s10479-011-1050-9.

[4] J. Benders, Partitioning procedures for solving mixed-variables programming problems, Numerische

Mathematik 4 (1962) 238–252.

[5] G. M. J. R.S. de Camargo, H. Luna, Benders decomposition for the uncapacitated multiple allocation

hub location problem, Computers and Operations Research 35 (2008) 1047–1064. doi:10.1016/j.cor.

2006.07.002.

[6] L. M. R. Lusby, B. Petersen, A solution approach based on benders decomposition for the preventive

maintenance scheduling problem of a stochastic large-scale energy system, Journal of Scheduling 16

(2013) 605–628. doi:10.1007/s10951-012-0310-0.

[7] A. F. P. Elias, C. Shannon, A note on the maximum ﬂow through a network, IRE Transactions on

Information Theory IT-2 (1956) 117–119.

[8] L. F. Jr., D. Fulkerson, Maximal ﬂow through a network, Canadian Journal of Mathematics 8 (1956)

399–404.

[9] T. Magnanti, R. Wong, Accelerating benders decomposition: Algorithmic enhanvement and model

selection criteria, Operations Research 29 (3) (1981) 464–484.

[10] T. Magnanti, R. Wong, Network design and transportation planning: Models and algorithms, Trans-

portation Science 18 (1) (1984) 1–55.

[11] I. Gurobi Optimization, Gurobi optimizer reference manual (2015).

URL http://www.gurobi.com

23

