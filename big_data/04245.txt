6
1
0
2

 
r
a

 

M
4
1
 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
5
4
2
4
0

.

3
0
6
1
:
v
i
X
r
a

A Variational Perspective on Accelerated Methods

in Optimization

Andre Wibisono

Ashia C. Wilson

Michael I. Jordan

wibisono@berkeley.edu

ashia@berkeley.edu

jordan@cs.berkeley.edu

March 15, 2016

Abstract

Accelerated gradient methods play a central role in optimization, achieving optimal rates in
many settings. While many generalizations and extensions of Nesterov’s original acceleration
method have been proposed, it is not yet clear what is the natural scope of the acceleration
concept. In this paper, we study accelerated methods from a continuous-time perspective. We
show that there is a Lagrangian functional that we call the Bregman Lagrangian which generates
a large class of accelerated methods in continuous time, including (but not limited to) accelerated
gradient descent, its non-Euclidean extension, and accelerated higher-order gradient methods.
We show that the continuous-time limit of all of these methods correspond to traveling the same
curve in spacetime at diﬀerent speeds. From this perspective, Nesterov’s technique and many
of its generalizations can be viewed as a systematic way to go from the continuous-time curves
generated by the Bregman Lagrangian to a family of discrete-time accelerated algorithms.

1

Introduction

The phenomenon of acceleration plays an important role in theory and practice of convex optimiza-
tion. Introduced by Nesterov in 1983 [24] in the context of gradient descent, the acceleration idea
has been extended to a wide range of other settings, including composite optimization [27, 33, 7],
stochastic optimization [12, 18], nonconvex optimization [11, 21], and conic programming [19].
There have been generalizations to non-Euclidean optimization [26, 17] and higher-order algo-
rithms [28, 6], and there have been numerous applications that further extend the reach of the
idea [14, 13, 15, 22]. On the theoretical front, acceleration often improves the convergence rate of
the underlying gradient-based procedure, and, under certain conditions, yields an optimal rate [23].
Despite this compelling evidence of the value of the idea of acceleration, it remains something
of a conceptual mystery. Derivations of accelerated methods do not ﬂow from a single under-
lying principle, but tend to rely on case-speciﬁc algebra [16]. The basic Nesterov technique is
often explained intuitively in terms of momentum, but this intuition does not easily carry over
to non-Euclidean settings [1]. In recent years, the number of explanations and interpretations of
acceleration has increased [1, 3, 10, 20, 8], but these explanations have been focused on restrictive
instances of acceleration, such as ﬁrst-order algorithms, the Euclidean setting, or cases in which

1

the objective function is strongly convex or quadratic. It is not yet clear what the natural scope of
the acceleration concept is and indeed whether it is a single phenomenon.

In this paper we study acceleration from a continuous-time, variational point of view. We build
on recent work by [32], who show that the continuous-time limit of Nesterov’s accelerated gradient
descent is a second-order diﬀerential equation, and we take inspiration from continuous time analysis
of mirror descent [23]. In our approach, rather than starting from existing discrete-time accelerated
gradient methods and deriving diﬀerential equations by taking limits, we take as our point of
departure a variational formulation in which we deﬁne a functional on continuous-time curves
that we refer to as a Bregman Lagrangian. Next, we calculate and discretize the Euler-Lagrange
equation corresponding to the Bregman Lagrangian. It turns out that naive discretization (the Euler
method) does not yield a stable discrete-time algorithm that retains the rate of the underlying
diﬀerential equation; rather, a more elaborate discretization involving an auxiliary sequence is
necessary. This auxiliary sequence is essentially that used by Nesterov in his constructions of
accelerated mirror descent [26] and accelerated cubic-regularized Newton’s method [28], and later
generalized by Baes [6]. Thus, from our perspective, Nesterov’s approach can be viewed as a
methodology for the discretization of a certain class of diﬀerential equations. Given the complexities
associated with the discretization of diﬀerential equations, it is perhaps not surprising that it has
been diﬃcult to perceive the generality and scope of the acceleration concept in a discrete-time
framework.

The Bregman-Lagrangian framework permits a systematic understanding of the matching rates
In the case of
associated with higher-order gradient methods in discrete and continuous time.
gradient descent, Su et al. show that the discrete and continuous-time dynamics have convergence
rates of O(1/(ǫk)) and O(1/t), respectively, and that these match using the identiﬁcation t = ǫk; for
accelerated gradient descent, the convergence rates are O(1/(ǫk2)) and O(1/t2) respectively, which
match using the identiﬁcation t = √ǫk [32]. This result has been extended to the non-Euclidean
case by Krichene et al. [17]. Higher-order gradient descent is a descent method which minimizes a
regularized (p− 1)-st order Taylor approximation of the objective function f , generalizing gradient
descent (p = 2) and Nesterov and Polyak’s cubic-regularized Newton’s method (p = 3) [29]. The
p-th order gradient algorithm with a constant step size ǫ has convergence rate O(1/(ǫkp−1)) when
∇p−1f is (1/ǫ)-Lipchitz and, in continuous time, as ǫ → 0, this algorithm corresponds to the p-
th rescaled gradient ﬂow, which is a ﬁrst-order diﬀerential equation with a matching convergence
rate O(1/tp−1). Thus, the p-th order gradient algorithm can be seen as a discretization t = δk
of the rescaled gradient ﬂow with time step δ = ǫ1/(p−1). Similarly, we show that the accelerated
higher-order gradient algorithm achieves an improved convergence rate O(1/(ǫkp)) under the same
assumption (i.e., ∇p−1f is (1/ǫ)-Lipschitz). In continuous time, as ǫ → 0, this corresponds to the
second-order Euler-Lagrange curve of the Bregman Lagrangian with a matching convergence rate
O(1/tp). Thus, the p-th order accelerated algorithm can be seen as a discretization t = δk of the
Euler-Lagrange equation of the Bregman Lagrangian with time step δ = ǫ1/p.

In addition to its value in relating continuous-time and discrete-time acceleration, the study of
the Bregman Lagrangian can provide further insights into the nature of acceleration. For instance,
it is noteworthy that the Bregman Lagrangian is closed under time dilation. This means that if

2

we take an Euler-Lagrange curve of a Bregman Lagrangian and reparameterize time so we travel
the curve at a diﬀerent speed, then the resulting curve is also the Euler-Lagrange curve of another
Bregman Lagrangian, with appropriately modiﬁed parameters. Thus, the entire family of acceler-
ated methods correspond to a single curve in spacetime and can be obtained by speeding up (or
slowing down) any single curve. Another insight is obtained by noting that from the discrete-time
point of view, an interpretation of acceleration starts with a base algorithm, which we can acceler-
ate by coupling with a suitably weighted mirror descent step. From the continuous-time point of
view, however, it is the weighted mirror descent step that is important since the base gradient algo-
rithm operates on a smaller time scale. Thus, Nesterov’s accelerated gradient methods are but one
possible implementation of second-order Bregman-Lagrangian curves as a discrete-time algorithm.
In Section 2, we introduce the general
In Section 3, we demonstrate how to
family of Bregman Lagrangians and study its properties.
discretize the Euler-Lagrange equations corresponding to the polynomial subfamily of Bregman
Lagrangians to obtain discrete-time accelerated algorithms. In particular, we introduce the family
of higher-order gradient methods which can be used to complete the discretization. In Section 4,
we discuss additional properties of the Bregman Lagrangian, including gauge-invariance properties,
connection to classical gradient ﬂows, and the correspondence with a functional that we refer to as
a Bregman Hamiltonian. Finally, we end in Section 5 with a brief discussion.

The remainder of the paper is organized as follows.

1.1 Problem setting

We consider the optimization problem

min
x∈X

f (x),

where X ⊆ Rd is a convex set and f : X → R is a continuously diﬀerentiable convex function.
To simplify the presentation in this paper we focus on the case X = Rd. We also assume f has
a unique minimizer, x∗ ∈ X , satisfying the optimality condition ∇f (x∗) = 0. We use the inner
product norm kxk = hx, xi1/2.

We consider the general non-Euclidean setting in which the space X is endowed with a distance-
generating function h : X → R that is convex and essentially smooth (i.e., h is continuously diﬀer-
entiable in X , and k∇h(x)k∗ → ∞ as kxk → ∞). The function h can also be used to deﬁne an
alternative measure of distance in X via its Bregman divergence:

Dh(y, x) = h(y) − h(x) − h∇h(x), y − xi,

which is nonnegative since h is convex. When x and y are nearby the Bregman divergence is an
approximation to the Hessian metric,

Dh(y, x) ≈

1
2hy − x,∇2h(x)(y − x)i :=

1
2ky − xk2

∇2h(x).

The Euclidean setting is obtained when h(x) = 1
Hessian metric coincide since ∇2h(x) is the identity matrix.

2kxk2, in which case the Bregman divergence and

3

In continuous time, the Hessian metric is generally studied rather than the more general Breg-
man divergence; this is the case, for instance, in the case of natural gradient ﬂow, which is the
continuous-time limit of mirror descent [2, 31]. By way of contrast, we shall see that our continuous-
time, Lagrangian framework crucially employs the Bregman divergence.

In this paper we denote a discrete-time sequence in lower case, e.g., xk with k ≥ 0 an integer.
We denote a continuous-time curve in upper case, e.g., Xt with t ∈ R. An over-dot means derivative
with respect to time, i.e.,

˙Xt = d

dt Xt.

2 The Bregman Lagrangian

We deﬁne the Bregman Lagrangian

L(X, V, t) = eαt+γt(cid:16)Dh(X + e−αtV, X) − eβtf (X)(cid:17)

(2.1)

which is a function of position X ∈ X , velocity V ∈ Rd, and time t ∈ T, where T ⊆ R is an interval
of time. The functions α, β, γ : T → R are arbitrary smooth (continuously diﬀerentiable) functions
of time that determine the weighting of the velocity, the potential function, and the overall damping
of the Lagrangian. We also deﬁne the following ideal scaling conditions:

˙βt ≤ eαt
˙γt = eαt ;

(2.2a)

(2.2b)

these conditions will be justiﬁed in the following section.

2.1 Convergence rates of the Euler-Lagrange equation

In this section we show that—under the ideal scaling assumption (2.2)—the Bregman Lagrangian (2.1)
deﬁnes a variational problem the solutions to which minimize the objective function f at an expo-
nential rate.

Given a general Lagrangian L(X, V, t), we deﬁne a functional on curves {Xt : t ∈ T} via inte-
˙Xt, t)dt. From the calculus of variations, a necessary

condition for a curve to minimize this functional is that it solve the Euler-Lagrange equation:

gration of the Lagrangian: J(X) = RT L(Xt,

d

dt(cid:26) ∂L

∂V

(Xt,

˙Xt, t)(cid:27) =

∂L
∂X

(Xt,

˙Xt, t).

Speciﬁcally, for the Bregman Lagrangian (2.1), the partial derivatives are

∂L
∂X
∂L
∂V

(X, V, t) = eγt+αt(cid:16)∇h(X + e−αtV ) − ∇h(X) − e−αt∇2h(X) V − eβt∇f (X)(cid:17)
(X, V, t) = eγt(cid:0)∇h(X + e−αtV ) − ∇h(X)(cid:1) .

4

(2.3)

(2.4a)

(2.4b)

Thus, for general functions αt, βt, γt, the Euler-Lagrange equation (2.3) for the Bregman La-
grangian (2.1) is a second-order diﬀerential equation given by

¨Xt + (eαt − ˙αt) ˙Xt + e2αt+βth∇2h(Xt + e−αt ˙Xt)i−1
+ eαt ( ˙γt − eαt )h∇2h(Xt + e−αt ˙Xt)i−1

∇f (Xt)

(2.5)

(∇h(Xt + e−αt ˙Xt) − ∇h(Xt)) = 0.

We now impose the ideal scaling condition (2.2b). In this case the last term in (2.5) vanishes,

so the Euler-Lagrange equation simpliﬁes to

¨Xt + (eαt − ˙αt) ˙Xt + e2αt+βth∇2h(Xt + e−αt ˙Xt)i−1

∇f (Xt) = 0.

(2.6)

In (2.6), we have assumed the Hessian matrix ∇2h(Xt + e−αt ˙Xt) is invertible. But we can also
write the equation (2.6) in the following way, which only requires that ∇h be diﬀerentiable,

d
dt∇h(Xt + e−αt ˙Xt) = −eαt+βt∇f (Xt).

(2.7)

To establish a convergence rate associated with solutions to the Euler-Lagrange equation—
under the ideal scaling conditions—we take a Lyapunov function approach. Deﬁning the following
energy functional:

Et = Dh(cid:16)x∗, Xt + e−αt ˙Xt(cid:17) + eβt(f (Xt) − f (x∗)),

we immediately obtain a convergence rate, as shown in the following theorem.

(2.8)

Theorem 2.1. If the ideal scaling (2.2) holds, then solutions to the Euler-Lagrange equation (2.7)
satisfy

Proof. The time derivative of the energy functional is

f (Xt) − f (x∗) ≤ O(e−βt).

˙Et = −(cid:28) d

dt∇h(Xt + e−αt ˙Xt), x∗ − Xt − e−αt ˙Xt(cid:29) + ˙βteβt(f (Xt) − f (x∗)) + eβth∇f (Xt),

˙Xti.

If Xt satisﬁes the Euler-Lagrange equation (2.7), then the time derivative simpliﬁes to

˙Et = −eαt+βtDf (x∗, Xt) + ( ˙βt − eαt)eβt (f (Xt) − f (x∗))

where Df (x∗, Xt) = f (x∗) − f (Xt) − h∇f (Xt), x∗ − Xti is the Bregman divergence of f . Note
that Df (x∗, Xt) ≥ 0 since f is convex, so the ﬁrst term in ˙Et is nonpositive. Furthermore, if the
ideal scaling condition (2.2a) holds, then the second term is also nonpositive, so ˙Et ≤ 0. Since
Dh(x∗, Xt + e−αt ˙Xt) ≥ 0, this implies that for any t ≥ t0 ∈ T, eβt(f (Xt) − f (x∗)) ≤ Et ≤ Et0.
Thus, f (Xt) − f (x∗) ≤ Et0e−βt = O(e−βt), as desired.

5

setting ˙βt = eαt , resulting in convergence rate O(e−βt) = O(exp(−R t

For a given αt, which determines γt by (2.2a), the optimal convergence rate is achieved by
In Section 3 we
study a subfamily of Bregman Lagrangians that have a polynomial convergence rate, and we show
how we can discretize the resulting Euler-Lagrange equations to obtain discrete-time methods
that have a matching, accelerated convergence rate. In Section 4 we study another subfamily of
Bregman Lagrangians that have an exponential convergence rate, and discuss its connection to a
generalization of Nesterov’s restart scheme. In the Euclidean setting, our derivations simplify. We
present these derivations in Appendix B.7, and comment on the insight that they provide into the
question posed by Su et al. [32] on the signiﬁcance of the value 3 in the damping coeﬃcient for
Nesterov’s accelerated gradient descent.

eαs ds)).

t0

2.2 Time dilation

A notable property of the Bregman Lagrangian family is that it is closed under time dilation.
This means if we take the Euler-Lagrange equation (2.5) of the Bregman Lagrangian (2.1) and
reparameterize time to travel the curve at a diﬀerent speed, the resulting curve is also the Euler-
Lagrange equation of a Bregman Lagrangian with a suitably modiﬁed set of parameters.

Concretely, let τ : T → T′ be a smooth (twice-continuously diﬀerentiable) increasing function,
where T′ = τ (T) ⊆ R is the image of T. Given a curve X : T′ → X , we consider the reparameterized
curve Y : T → X deﬁned by

Yt = Xτ (t).

(2.9)

That is, the new curve Y is obtained by traversing the original curve X at a new speed of time
determined by τ . If τ (t) > t, then we say that Y is the sped-up version of X, because the curve Y
at time t has the same value as the original curve X at the future time τ (t).

For clarity, we let Lα,β,γ denote the Bregman Lagrangian (2.1) parameterized by α, β, γ. Then

we have the following result whose proof is provided in Appendix A.1.

Theorem 2.2. If Xt satisﬁes the Euler-Lagrange equation (2.5) for the Bregman Lagrangian Lα,β,γ,
then the reparameterized curve Yt = Xτ (t) satisﬁes the Euler-Lagrange equation for the Bregman
Lagrangian L ˜α, ˜β,˜γ, with modiﬁed parameters

˜αt = ατ (t) + log ˙τ (t)
˜βt = βτ (t)
˜γt = γτ (t).

(2.10a)

(2.10b)

(2.10c)

Furthermore, α, β, γ satisfy the ideal scaling (2.2) if and only if ˜α, ˜β, ˜γ do.

We note that in general, when we reparameterize time by a time-dilation function τ (t), the

Lagrangian functional transforms to ˜L(X, V, t) = ˙τ (t)L(cid:16)X, 1

stating the result in Theorem 2.2 is to claim that

˙τ (t) V, τ (t)(cid:17) . Thus, another way of
V, τ (t)(cid:19) ,

(2.11)

L ˜α, ˜β,˜γ(X, V, t) = ˙τ (t)Lα,β,γ(cid:18)X,

1

˙τ (t)

6

which we can easily verify by directly substituting the deﬁnition of the Lagrangian (2.1) and the
modiﬁed parameters ˜α, ˜β, ˜γ (2.10).

In Section 3, we show that the Bregman Lagrangian generates the family of higher-order ac-
celerated methods in discrete time. Thus, the time-dilation property means that the entire family
of curves for accelerated methods in continuous time corresponds to a single curve in spacetime,
which is traveled at diﬀerent speeds. This suggests that the underlying solution curve has a more
fundamental structure that is worth exploring further.

3 Polynomial convergence rates and accelerated methods

In this section, we study a subfamily of Bregman Lagrangians (2.1) with the following choice of
parameters, indexed by a parameter p > 0,

αt = log p − log t
βt = p log t + log C

γt = p log t,

(3.1a)

(3.1b)

(3.1c)

where C > 0 is a constant. The parameters α, β, γ satisfy the ideal scaling condition (2.2) (with
an equality on the ﬁrst condition (2.2a)). The Euler-Lagrange equation (2.6) is given by

¨Xt +

p + 1

t

˙Xt + Cp2tp−2(cid:20)∇2h(cid:18)Xt +

t
p

˙Xt(cid:19)(cid:21)−1

∇f (Xt) = 0

(3.2)

and, by Theorem 2.1, it has an O(1/tp) rate of convergence. As direct result of the time-dilation
property (Theorem 2.2), the entire family of curves (3.2) can be obtained by speeding up the curve
in the case p = 2 by the time-dilation function τ (t) = tp/2. In Appendix A.2 we discuss the issue
of the existence and uniqueness of the solution to the diﬀerential equation (3.2).

The case p = 2 of the equation (3.2) is the continuous-time limit of Nesterov’s accelerated
mirror descent [26], and the case p = 3 is the continuous-time limit of Nesterov’s accelerated cubic-
regularized Newton’s method [28]. The case p = 2 has also been derived independently in a recent
work of Krichene et al. [17]; in the Euclidean case, when the Hessian ∇2h is the identity matrix,
we recover the diﬀerential equation of Su et al. [32].

3.1 Naive discretization

We now turn to the challenge of discretizing the diﬀerential equation in (3.2), with the goal of ob-
taining a discrete-time algorithm whose convergence rate matches that of the underlying diﬀerential
equation. As we show in this section, a naive Euler method is not able to match the underlying
rate. To match the rate a more sophisticated approach is needed, and it is at this juncture that
Nesterov’s three-sequence idea makes its appearance.

7

We ﬁrst write the second-order equation (3.2) as the following system of ﬁrst-order equations:

Zt = Xt +

t
p

˙Xt

d
dt∇h(Zt) = −Cptp−1∇f (Xt).

(3.3a)

(3.3b)

Now we discretize Xt and Zt into sequences xk and zk with time step δ > 0. That is, we make the
identiﬁcation t = δk and set xk = Xt, xk+1 = Xt+δ ≈ Xt+δ ˙Xt and zk = Zt, zk+1 = Zt+δ ≈ Zt+δ ˙Zt.
Applying the forward-Euler method to (3.3a) gives the equation zk = xk + δk
δ (xk+1 − xk), or
p
equivalently,

1

xk+1 =

p
k

zk +

k − p

k

xk.

(3.4)

Similarly, applying the backward-Euler method to equation (3.3b) gives 1
δ (∇h(zk) − ∇h(zk−1)) =
−Cp(δk)p−1∇f (xk), which we can write as the optimality condition of the following weighted mirror
descent step:

zk = arg min

z (cid:26)Cpkp−1h∇f (xk), zi +

1
ǫ

Dh(z, zk−1)(cid:27) ,

(3.5)

with step size ǫ = δp. In principle, the two updates (3.4), (3.5) deﬁne an algorithm that implements
the dynamics (3.3) in discrete time. However, we cannot establish a convergence rate for the
algorithm (3.4), (3.5); indeed, empirically, we ﬁnd that the algorithm is unstable. Even for the
simple case in which f is a quadratic function in two dimensions, the iterates of the algorithm
initially approach and oscillate near the minimizer, but eventually the oscillation increases and the
iterates shoot oﬀ to inﬁnity.

3.2 A rate-matching discretization

We now discuss how to modify the naive discretization scheme (3.4), (3.5) into an algorithm
whose rate matches that of the underlying diﬀerential equation. Our approach is inspired by
Nesterov’s constructions of accelerated mirror descent [26] and accelerated cubic-regularized New-
ton’s method [28], which maintain three sequences in the algorithms and use the estimate sequence
technique to prove convergence. Indeed, from our point of view, Nesterov’s methodology can be
viewed as a rate-matching discretization methodology.

Speciﬁcally, we consider the following scheme, in which we introduce a third sequence yk to

replace xk in the updates,

xk+1 =

p

k + p

zk +

k

k + p

yk

zk = arg min

z (cid:26)Cpk(p−1)h∇f (yk), zi +

1
ǫ

Dh(z, zk−1)(cid:27) ,

(3.6a)

(3.6b)

8

where k(p−1) := k(k + 1)··· (k + p − 2) is the rising factorial. A suﬃcient condition for the al-
gorithm (3.6) to have an O(1/(ǫkp)) convergence rate is that the new sequence yk satisfy the
inequality

h∇f (yk), xk − yki ≥ M ǫ

p−1k∇f (yk)k

1

p

p−1
∗

,

(3.7)

for some constant M > 0. Note that in going from (3.4) to (3.6a) we have replaced the weight p
k
by p
k+p ; this is only for convenience in the proof given below, and does not change the asymptotics
since p
k+p ) as k → ∞. Similarly, we replace kp−1 in (3.5) by the rising factorial k(p−1)
in (3.6b) to make the algebra easier, but we still have k(p−1) = Θ(kp−1).

k = Θ( p

The following result also requires a uniform convexity assumption on the distance-generating
function h. Recall that h is σ-uniformly convex of order p ≥ 2 if its Bregman divergence is lower
bounded by the p-th power of the norm,

Dh(y, x) ≥

σ
pky − xkp.

(3.8)

The case p = 2 is the usual deﬁnition of strong convexity. An example of a uniformly convex
function is the p-th power of the norm, h(x) = 1
pkx − wkp for any w ∈ X , which is σ-uniformly
convex of order p with σ = 2−p+2 [28, Lemma 4].

Theorem 3.1. Assume h is 1-uniformly convex of order p ≥ 2, and the sequence yk satisﬁes the
inequality (3.7) for all k ≥ 0. Then the algorithm (3.6) with the constant C ≤ M p−1/pp and initial
condition z0 = x0 ∈ X has the convergence rate

f (yk) − f (x∗) ≤

Dh(x∗, x0)

Cǫk(p)

= O(cid:18) 1

ǫkp(cid:19) .

(3.9)

The proof of Theorem 3.1 uses a generalization of Nesterov’s estimate sequence technique, and
can be found in Appendix A.3. We note that with the scaling ǫ = δp as in the previous section,
the convergence rate O(1/(ǫkp)) matches the O(1/tp) rate in continuous time for the diﬀerential
equation (3.2). We also note that the result in Theorem 3.1 does not require any assumptions on
f beyond the ability to construct a sequence yk satisfying (3.7). In the next section, we will see
that we can satisfy (3.7) using the higher-order gradient method, which requires a higher-order
smoothness assumption on f ; the resulting algorithm is then the accelerated higher-order gradient
method.

3.3 Higher-order gradient method

We study the higher-order gradient update, which minimizes a regularized higher-order Taylor
approximation of the objective function f .

Recall that for an integer p ≥ 2, the (p − 1)-st order Taylor approximation of f centered at

x ∈ X is the (p − 1)-st degree polynomial

fp−1(y; x) =

p−1

Xi=0

1
i!∇if (x)(y − x)i = f (x) + h∇f (x), y − xi + ··· +

1

(p − 1)!∇p−1f (x)(y − x)p−1.

9

We say that f is L-smooth of order p − 1 if f is p-times continuously diﬀerentiable and ∇p−1f is
L-Lipschitz, which means for all x, y ∈ X ,

k∇p−1f (y) − ∇p−1f (x)k∗ ≤ Lky − xk.

(3.10)

For a constant N > 0 and step size ǫ > 0, we deﬁne the update operator Gp,ǫ,N : X → X by

Gp,ǫ,N (x) = arg min

y (cid:26)fp−1(y; x) +

N

ǫpky − xkp(cid:27) .

(3.11)

When f is smooth of order p − 1, the operator Gp,ǫ,N has the following property, which generalizes
[28, Lemma 6]. We provide the proof in Appendix A.4.

Lemma 3.2. Let x ∈ X , y = Gp,ǫ,N (x), and N > 1. If f is L = (p−1)!

ǫ

-smooth of order p − 1, then

h∇f (y), x − yi ≥

(N 2 − 1)
2N

p−2
2p−2

ǫ

1

p−1k∇f (y)k

p

p−1
∗

Furthermore,

(N 2 − 1)
2N

p−2
2p−2

ǫ

1

1

p−1

p−1k∇f (y)k

∗ ≤ kx − yk ≤

.

(3.12)

1

p−1
∗

.

(3.13)

1
(N − 1)

ǫ

1

p−1

1

p−1k∇f (y)k

The inequality (3.12) means that we can use the update operator Gp,ǫ,N to produce a sequence
yk satisfying the requirement (3.7) under a higher-order order smoothness condition on f . We state
the resulting algorithm in the next section.

Higher-order gradient method.
algorithm deﬁned by the update operator Gp,ǫ,N :

In this section, we study the following higher-order gradient

xk+1 = Gp,ǫ,N (xk).

(3.14)

The case p = 2 is the usual gradient descent algorithm, and the case p = 3 is Nesterov and Polyak’s
cubic-regularized Newton’s method [29].

If f is smooth of order p − 1, then the algorithm (3.14) is a descent method. Furthermore, we
can prove the following rate of convergence, which generalizes the results for gradient descent and
the cubic-regularized Newton’s method. We provide the proof in Appendix A.5.

Theorem 3.3. If f is (p−1)!
and initial condition x0 ∈ X has the convergence rate

ǫ

-smooth of order p− 1, then the algorithm (3.14) with constant N > 0

f (xk) − f (x∗) ≤

pp−1(N + 1)Rp

ǫkp−1

= O(cid:18) 1

ǫkp−1(cid:19) ,

(3.15)

where R = supx : f (x)≤f (x0) kx − x∗k is the radius of the level set of f from the initial point x0.

10

Rescaled gradient ﬂow. We can take the continuous-time limit of the higher-order gradient
algorithm as the step size ǫ → 0. The resulting curve is a ﬁrst-order diﬀerential equation that is a
rescaled version of gradient ﬂow. We show that it minimizes f with a matching convergence rate.
In the following, we take N = 1 in (3.14) for simplicity (the general N simply scales the vector ﬁeld
by a constant). We provide the proof of Theorem 3.4 in Appendix A.6.

Theorem 3.4. The continuous-time limit of the algorithm (3.14) is the rescaled gradient ﬂow

˙Xt = − ∇f (Xt)
k∇f (Xt)k

p−2
p−1
∗

,

(3.16)

where we deﬁne the right-hand side to be the zero if ∇f (Xt) = 0. Furthermore, the rescaled gradient
ﬂow has convergence rate

f (Xt) − f (x∗) ≤

(p − 1)p−1Rp

tp−1

= O(cid:18) 1

tp−1(cid:19) ,

(3.17)

where R = supx : f (x)≤f (X0) kx − x∗k is the radius of the level set of f from the initial point X0.

1
p−1 , so t = δk = ǫ

Equivalently, we can interpret the higher-order gradient algorithm (3.14) as a discretization of
1
the rescaled gradient ﬂow (3.16) with time step δ = ǫ
p−1 k. With this identi-
ﬁcation, the convergence rates in discrete time, O(1/(ǫkp−1)), and in continuous time, O(1/tp−1),
match. The convergence rate for the continuous-time dynamics does not require any assumption
beyond the convexity and diﬀerentiability of f (as in the case of the Lagrangian ﬂow (2.6)), whereas
the convergence rate for the discrete-time algorithm requires the higher-order smoothness assump-
tion on f . We note that the limiting case p → ∞ of (3.16) is the normalized gradient ﬂow, which
has been shown to converge to the minimizer of f in ﬁnite time [9]. We also note that unlike the
Lagrangian ﬂow, the family of rescaled gradient ﬂows is not closed under time dilation.

3.4 Accelerated higher-order gradient method

By the result of Lemma 3.2, we see that we can use the higher-order gradient update Gp,ǫ,N
to produce a sequence yk satisfying the inequality (3.7), to complete the algorithm (3.14) that
implements the polynomial family of the Bregman-Lagrangian ﬂow (3.2). Explicitly, the resulting
algorithm is as follows,

xk+1 =

p

k + p

zk +

k

k + p

yk

yk = arg min

zk = arg min

N

ǫpky − xkkp(cid:27)

y (cid:26)fp−1(y; xk) +
z (cid:26)Cpk(p−1)h∇f (yk), zi +

1
ǫ

Dh(z, zk−1)(cid:27) .

(3.18a)

(3.18b)

(3.18c)

By Theorem 3.1 and Lemma 3.2, we have the following guarantee for this algorithm.

11

Corollary 3.5. Assume f is (p−1)!
p. Then the algorithm (3.18) with constants N > 1 and C ≤ (N 2 − 1)
conditions z0 = x0 ∈ X has an O(1/(ǫkp)) convergence rate.

-smooth of order p − 1, and h is 1-uniformly convex of order
2 /((2N )p−1pp) and initial

ǫ

p−2

The resulting algorithm (3.18) and its convergence rate recovers the results of Baes [6], who
studied a generalization of Nesterov’s estimate sequence technique to higher-order algorithms. We
note that the convergence rate O(1/(ǫkp)) of algorithm (3.18) is better than the O(1/(ǫkp−1))
rate of the higher-order gradient algorithm (3.14), under the same assumption of the (p − 1)-st
order smoothness of f . This gives the interpretation of the algorithm (3.18) as “accelerating” the
higher-order gradient method.
Indeed, in this view the “base algorithm” that we start with is
the higher-order gradient algorithm in the y-sequence (3.18b), and the acceleration is obtained by
coupling it with a suitably weighted mirror descent step in (3.18a) and (3.18c).

However, from the continuous-time point of view, where our starting point is the polynomial
Lagrangian ﬂow (3.2), we see that the algorithm (3.18) is only one possible implementation of the
ﬂow as a discrete-time algorithm. As we saw in Section 3.2, it is only the x- and z-sequences (3.18a)
and (3.18c) that play a role in the correspondence between the continuous-time dynamics and
its discrete-time implementation, and the requirement (3.7) in the y-update is only needed to
complete the convergence proof. Indeed, the higher-order gradient update (3.18b) does not change
1
the continuous-time limit, since from (3.13) in Lemma 3.2 we have that kxk − ykk = Θ(ǫ
p−1 ),
1
which is smaller than the δ = ǫ
p time step in the discretization of (3.2). Therefore, the x and y
sequences in (3.18) coincide in continuous time as ǫ → 0. Thus, from this point of view, Nesterov’s
accelerated methods (for the cases p = 2 and p = 3) are one of possibly many discretizations of
the polynomial Lagrangian ﬂow (3.2). For instance, in the case p = 2, Krichene et al. [17, Section
4.1] show that we can use a general regularizer in the gradient step (3.18b) under some additional
smoothness assumptions. If there are other implementations, it would be interesting to see if the
higher-gradient methods have some distinguishing property, such as computational eﬃciency.

4 Further explorations of the Bregman Lagrangian

In addition to providing a unifying framework for the generation of accelerated gradient-based
algorithms, the Bregman Lagrangian has mathematical structure that can be investigated directly.
In this section we brieﬂy discuss some of the additional perspective that can be obtained from the
Bregman Lagrangian. See Appendices B.1–B.6 for technical details of the results discussed here.

It is important to note the presence of the Bregman diver-
Hessian vs. Bregman Lagrangian.
gence in the Bregman Lagrangian (2.1). In the non-Euclidean setting, intuition might suggest using
the Hessian metric ∇2h to measure a “kinetic energy,” and thereby obtain a Hessian Lagrangian.
This approach turns out to be unsatisfying, however, because the resulting diﬀerential equation
does not yield a convergence rate and the Euler-Lagrange equation involves the third-order deriva-
tive ∇3h, posing serious diﬃculties for discretization. As we have seen, the Bregman Lagrangian,
on the other hand, readily provides a rate of convergence via a Lyapunov function; moreover, the

12

resulting discrete-time algorithm in (3.18) involves only the gradient ∇h via the weighted mirror
descent update.

In the Euclidean case, it is known classically that we can view
Gradient vs. Lagrangian ﬂows.
gradient ﬂow as the strong-friction limit of a damped Lagrangian ﬂow [34, p. 646]. We show that
the same interpretation holds for natural gradient ﬂow and rescaled gradient ﬂow. In particular,
we show in Appendix B.3 that we can recover natural gradient ﬂow as the strong-friction limit of
a Bregman Lagrangian ﬂow with an appropriate choice of parameters. Similarly, we can recover
the rescaled gradient ﬂow (3.16) as the strong-friction limit of a Lagrangian ﬂow that uses the p-th
power of the norm as the kinetic energy. Therefore, the general family of second-order Lagrangian
ﬂows is more general, and includes ﬁrst-order gradient ﬂows in its closure. From this point of view,
a particle with gradient-ﬂow dynamics is operating in the regime of high friction. The particle
simply rolls downhill and stops at the equilibrium point as soon as the force −∇f vanishes; there
is no oscillation since it is damped by the inﬁnitely strong friction. Thus, the eﬀect of moving from
a ﬁrst-order gradient ﬂow to a second-order Lagrangian ﬂow is to reduce the friction from inﬁnity
to a ﬁnite amount; this permits oscillation [30, 32, 17], but also allows faster convergence.

Bregman Hamiltonian. One way to understand a Lagrangian is to study its Hamiltonian, which
is the Legendre conjugate (dual function) of the Lagrangian. Typically, when the Lagrangian takes
the form of the diﬀerence between kinetic and potential energy, the Hamiltonian is the sum of the
kinetic and potential energy. The Hamiltonian is often easier to study than the Lagrangian, since
its second-order Euler-Lagrangian equation is transformed into a pair of ﬁrst-order equations. In
our case, the Hamiltonian corresponding to the Bregman Lagrangian (2.1) is the following Bregman
Hamiltonian,

H(X, P, t) = eαt+γt(cid:16)Dh∗(cid:0)∇h(X) + e−γtP, ∇h(X)(cid:1) + eβtf (X)(cid:17)

which indeed has the form of the sum of the kinetic and potential energy. Here the kinetic energy
is measured using the Bregman divergence of h∗, which is the convex dual function of h. See
Appendix B.4 for further discussion.

Gauge invariance. The Euler-Lagrange equation of a Lagrangian is gauge-invariant, which
means it does not change when we add a total time derivative to the Lagrangian. For the Bregman
Lagrangian with the ideal scaling condition (2.2b), this property implies that we can replace the
Bregman divergence Dh(X + e−αtV, X) in (2.1) by its ﬁrst term h(X + e−αtV ). This might suggest
a diﬀerent interpretation of the role of h in the Lagrangian.

Natural motion. The natural motion of the Bregman Lagrangian (i.e., the motion when there
is no force, −∇f ≡ 0) is given by Xt = ae−γt + b, for some constants a, b ∈ X . Notice that even
though the Bregman Lagrangian still involves the distance-generating function h, its natural motion
is actually independent of h. Thus, the eﬀect of h is felt only via its interaction with f —this can
also be seen in (2.6) where h and f only appear together in the ﬁnal term. Furthermore, assuming

13

eγt → ∞, the natural motion always converges to a limit point, which a priori can be anything.
However, as we see from Theorem 2.1, as soon as we introduce a convex potential function f , all
motions converge to the minimizer x∗ of f .

Exponential convergence rate via uniform convexity In addition to the polynomial family
in Section 3, we can also study the subfamily of Bregman Lagrangians that have exponential
convergence rates O(e−ct), c > 0. As we discuss in Appendix B.1, in this case the link to discrete-
time algorithms is not as clear. Using the same discretization technique as in Section 3 suggests
that to get a matching convergence rate, constant progress is needed at each iteration.

From the discrete-time perspective, we show that the higher-order gradient algorithm (3.14)
achieves an exponential convergence rate when the objective function f is uniformly convex. Fur-
thermore, we show that a restart scheme applied to the accelerated method (3.18) achieves a
better dependence on the condition number; this generalizes Nesterov’s restart scheme for the case
p = 3 [28, Section 5].

It is an open question to understand if there is a better connection between the discrete-time
restart algorithms and the continuous-time exponential Lagrangian ﬂows.
In particular, it is of
interest to consider whether a restart scheme is necessary to achieve exponential convergence in
discrete time; we know it is not needed for the special case p = 2, since a variant of Nesterov’s
accelerated gradient descent [25] that incorporates the condition number also achieves the optimal
convergence rate.

5 Discussion

In this paper, we have presented a variational framework for understanding accelerated methods
from a continuous-time perspective. We presented the general family of Bregman Lagrangian, which
generates a family of second-order Lagrangian dynamics that minimize the objective function at
an accelerated rate compared to gradient ﬂows. These dynamics are related to each other by the
operation of speeding up time, because the Bregman Lagrangian family is closed under time dilation.
In the polynomial case, we showed how to discretize the second-order Lagrangian dynamics to obtain
an accelerated algorithm with a matching convergence rate. The resulting algorithm accelerates a
base algorithm by coupling it with a weighted mirror descent step. An example of a base algorithm
is a higher-order gradient method, which in continuous time corresponds to a ﬁrst-order rescaled
gradient ﬂow with a matching convergence rate. Our continuous-time perspective makes clear
that it is the mirror descent coupling that is more important for the acceleration phenomenon
rather than the base algorithm. Indeed, the higher-order gradient algorithm operates on a smaller
timescale than the enveloping mirror descent coupling step, so it makes no contribution in the
continuous-time limit, and in principle we can use other base algorithms.

Our work raises many questions for further research. First, the case p = 2 is worthy of further
investigation. In particular, the assumptions needed to show convergence of the discrete-time al-
gorithm (∇p−1f is Lipschitz) are diﬀerent than those required to show existence and uniqueness
of solutions of the continuous-time dynamics (∇f is Lipschitz). In the case p = 2 however, these

14

assumptions match. This suggests a strong link between the discrete- and continuous-time dynam-
ics that might help us understand why several results seem to be unique to the special case p = 2.
Second, in discrete time, Nesterov’s accelerated methods have been extended to various settings,
for example to the stochastic setting. An immediate question is whether we can extend our La-
grangian framework to these settings. Third, we would like to understand better the transition
from continuous-time dynamics to discrete-time algorithms, and whether we can establish general
assumptions that preserve desirable properties (e.g., convergence rate). In Section 3 we saw that
the polynomial convergence rate requires a higher-order smoothness assumption in discrete time,
and in Section 4 we discussed whether the exponential case requires a uniform convexity assump-
tion. Finally, our work to date focuses on the convergence rates of the function values rather than
the iterates. Recently there has been some work extending [32] to study the convergence of the
iterates [5] and some perturbative aspects [4]; it would be interesting to extend these results to the
general Bregman Lagrangian.

At an abstract level, the general family of Bregman Lagrangian has a rich mathematical struc-
ture that deserves further study; we discussed some of these properties in Section 4. We hope
that doing so will give us new insights into the nature of the optimization problem in continuous
time, and help us design better dynamics with matching discrete-time algorithms. For example, we
can study how to use some of the appealing properties of the Hamiltonian formalism (e.g., volume
preservation in phase space) to help us discretize the dynamics. We also wish to understand where
the Bregman Lagrangian itself comes from, why it works so well, and whether there are other
Lagrangian families with similarly favorable properties.

15

References

[1] Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear coupling: An ultimate uniﬁcation of gradient

and mirror descent. ArXiv preprint arXiv:1407.1537, 2014.

[2] Felipe Alvarez, J´erˆome Bolte, and Olivier Brahic. Hessian Riemannian gradient ﬂows in convex

programming. SIAM Journal on Control and Optimization, 43(2):477–501, 2004.

[3] Yossi Arjevani, Shai Shalev-Shwartz, and Ohad Shamir. On lower and upper bounds for
smooth and strongly convex optimization problems. ArXiv preprint arXiv:1503.06833, 2015.

[4] Hedy Attouch and Zaki Chbani. Fast inertial dynamics and FISTA algorithms in convex

optimization: Perturbation aspects. ArXiv preprint arXiv:1507.01367, 2015.

[5] Hedy Attouch, Juan Peypouquet, and Patrick Redont. On the fast convergence of an inertial

gradient-like system with vanishing viscosity. ArXiv preprint arXiv:1507.04782, 2015.

[6] Michel Baes. Estimate sequence methods: Extensions and approximations. Manuscript, avail-
able at http://www.optimization-online.org/DB_FILE/2009/08/2372.pdf, August 2009.

[7] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear

inverse problems. SIAM Journal on Imaging Sciences, 2(1):183–202, March 2009.

[8] S´ebastien Bubeck, Yin Tat Lee, and Mohit Singh. A geometric alternative to Nesterov’s

accelerated gradient descent. ArXiv preprint arXiv:1506.08187, 2015.

[9] Jorge Cort´es. Finite-time convergent gradient ﬂows with applications to network consensus.

Automatica, 42(11):1993–2000, 2006.

[10] Nicolas Flammarion and Francis R. Bach. From averaging to acceleration, there is only a

step-size. In Proceedings of the 28th Conference on Learning Theory (COLT), 2015.

[11] Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear

and stochastic programming. Mathematical Programming, 156(1):59–99, 2015.

[12] Chonghai Hu, James T. Kwok, and Weike Pan. Accelerated gradient methods for stochastic
In Advances in Neural Information Processing Systems

optimization and online learning.
(NIPS) 22, 2009.

[13] Shuiwang Ji, Liang Sun, Rong Jin, and Jieping Ye. Multi-label multiple kernel learning. In

Advances in Neural Information Processing Systems (NIPS) 21, 2009.

[14] Shuiwang Ji and Jieping Ye. An accelerated gradient method for trace norm minimization. In

Proceedings of the 26th International Conference on Machine Learning (ICML), 2009.

[15] Vladimir Jojic, Stephen Gould, and Daphne Koller. Accelerated dual decomposition for MAP
inference. In Proceedings of the 27th International Conference on Machine Learning (ICML),
2010.

16

[16] Anatoli Juditsky. Convex Optimization II: Algorithms. Lecture notes, 2013.

[17] Walid Krichene, Alexandre Bayen, and Peter Bartlett. Accelerated mirror descent in contin-
uous and discrete time. In Advances in Neural Information Processing Systems (NIPS) 29,
2015.

[18] Guanghui Lan. An optimal method for stochastic composite optimization. Mathematical

Programming, 133(1-2):365–397, 2012.

[19] Guanghui Lan, Zhaosong Lu, and Renato Monteiro. Primal-dual ﬁrst-order methods with
O(1/ǫ) iteration-complexity for cone programming. Mathematical Programming, 126(1):1–29,
2011.

[20] Laurent Lessard, Benjamin Recht, and Andrew Packard. Analysis and design of optimization
algorithms via integral quadratic constraints. SIAM Journal on Optimization, 26(1):57–95,
2016.

[21] Huan Li and Zhouchen Lin. Accelerated proximal gradient methods for nonconvex program-

ming. In Advances in Neural Information Processing Systems (NIPS) 28, 2015.

[22] Indraneel Mukherjee, Kevin Canini, Rafael Frongillo, and Yoram Singer. Parallel boosting with

momentum. In Machine Learning and Knowledge Discovery in Databases. Springer, 2013.

[23] Arkadi Nemirovskii and David Yudin. Problem Complexity and Method Eﬃciency in Opti-

mization. John Wiley & Sons, 1983.

[24] Yurii Nesterov. A method of solving a convex programming problem with convergence rate

O(1/k2). Soviet Mathematics Doklady, 27(2):372–376, 1983.

[25] Yurii Nesterov.

Introductory Lectures on Convex Optimization: A Basic Course. Applied

Optimization. Kluwer, Boston, 2004.

[26] Yurii Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming,

103(1):127–152, 2005.

[27] Yurii Nesterov. Gradient methods for minimizing composite objective function. CORE Dis-

cussion Papers 2007076, Universit´e Catholique de Louvain, 2007.

[28] Yurii Nesterov. Accelerating the cubic regularization of Newton’s method on convex problems.

Mathematical Programming, 112(1):159–181, 2008.

[29] Yurii Nesterov and Boris T. Polyak. Cubic regularization of Newton’s method and its global

performance. Mathematical Programming, 108(1):177–205, 2006.

[30] Brendan O’Donoghue and Emmanuel Cand`es. Adaptive restart for accelerated gradient

schemes. Foundations of Computational Mathematics, 15(3):715–732, 2015.

17

[31] Garvesh Raskutti and Sayan Mukherjee. The information geometry of mirror descent. IEEE

Transactions on Information Theory, 61(3):1451–1457, 2015.

[32] Weijie Su, Stephen Boyd, and Emmanuel J. Cand`es. A diﬀerential equation for modeling Nes-
terov’s accelerated gradient method: Theory and insights. In Advances in Neural Information
Processing Systems (NIPS) 27, 2014.

[33] Paul Tseng. On accelerated proximal gradient methods for convex-concave optimization. SIAM

Journal on Optimization, 2008.

[34] Cedric Villani. Optimal Transport, Old and New. Springer, 2008.

18

A Proofs of results

A.1 Proof of Theorem 2.2

The velocity and acceleration of the reparameterized curve Yt = Xτ (t) are given by

˙Yt = ˙τ (t) ˙Xτ (t)
¨Yt = ¨τ (t) ˙Xτ (t) + ˙τ (t)2 ¨Xτ (t).

Inverting these relations, we get

˙Xτ (t) =

¨Xτ (t) =

˙Yt

1

˙τ (t)
1

˙τ (t)2

¨Yt −

¨τ (t)
˙τ (t)3

˙Yt.

(A.1a)

(A.1b)

By assumption, the original curve Xt satisﬁes the Euler-Lagrange equation (2.5) for the Bregman

Lagrangian Lα,β,γ. At time τ (t), this equation reads
¨Xτ (t) + (eατ (t) − ˙ατ (t)) ˙Xτ (t) + e2ατ (t)+βτ (t)h∇2h(Xτ (t) + e−ατ (t) ˙Xτ (t))i−1
+ eατ (t)( ˙γτ (t) − eατ (t))h∇2h(Xτ (t) + e−ατ (t) ˙Xτ (t))i−1

∇f (Xτ (t))

We now use the relations (A.1). After multiplying by ˙τ (t)2 and collecting terms, we get

(∇h(Xτ (t) + e−ατ (t) ˙Xτ (t)) − ∇h(Xτ (t))) = 0.

¨τ (t)

¨Yt +(cid:18) ˙τ (t)eατ (t) − ˙τ (t) ˙ατ (t) −
+ ˙τ (t)2eατ (t)( ˙γτ (t) − eατ (t))(cid:20)∇2h(cid:18)Yt +

˙τ (t)(cid:19) ˙Yt + ˙τ (t)2e2ατ (t)+βτ (t)(cid:20)∇2h(cid:18)Yt +
˙Yt(cid:19)(cid:21)−1(cid:18)∇h(cid:18)Yt +

e−ατ (t)
˙τ (t)

e−ατ (t)
˙τ (t)

e−ατ (t)
˙τ (t)

˙Yt(cid:19)(cid:21)−1
∇f (Yt)
˙Yt(cid:19) − ∇h(Yt)(cid:19) = 0.

Finally, with the deﬁnition of the modiﬁed parameters ˜α, ˜β, ˜γ (2.10), we can write this equation as

¨Yt + (e ˜αt − ˙˜αt) ˙Yt + e2 ˜αt+ ˜βth∇2h(Yt + e− ˜αt ˙Yt)i−1
+ e ˜αt ( ˙˜γt − e ˜αt )h∇2h(Yt + e− ˜αt ˙Yt)i−1

∇f (Yt)

(∇h(Yt + e− ˜αt ˙Yt) − ∇h(Yt)) = 0,

which we recognize as the Euler-Lagrange equation (2.5) for the Bregman Lagrangian L ˜α, ˜β,˜γ.

Furthermore, suppose α, β, γ satisfy the ideal scaling (2.2). Then

˙˜βt =

˙˜γt =

d
dt
d
dt

βτ (t) = ˙τ (t) ˙βτ (t)

γτ (t) = ˙τ (t) ˙γτ (t)

(2.2a)

≤ ˙τ (t)eατ (t) = eατ (t)+log ˙τ (t) = e ˜αt
= ˙τ (t)eατ (t) = eατ (t)+log ˙τ (t) = e ˜αt ,

(2.2b)

which means that the modiﬁed parameters ˜α, ˜β, ˜γ also satisfy the ideal scaling (2.2). The converse
follows by considering the inverse function τ −1(t) in place of τ (t).

19

A.2 Existence and uniqueness of solution to the polynomial family

In this section we discuss the existence and uniqueness of solution to the diﬀerential equation (3.2)
arising from the polynomial family of Bregman Lagrangian. We begin by writing the second-order
equation (3.2) as the pair of ﬁrst-order equations (3.3). We also write Wt = ∇h(Zt), so we can
write (3.3) as

p
t

˙Xt =
(∇h∗(Wt) − Xt)
˙Wt = −Cptp−1∇f (Xt).

Here h∗ : X ∗ → R is the Legendre conjugate function of h, deﬁned by

h∗(w) = sup

z∈X {hw, zi − h(z)} ,

(A.2a)

(A.2b)

(A.3)

where X ∗ is the dual space of X , i.e., the space of all linear functionals over X . Under the
assumption that h be essentially smooth, the supremum in (A.3) is achieved by z = ∇h∗(w), and
we have the relation that ∇h and ∇h∗ are inverses of each other, i.e., z = ∇h∗(w) ⇔ w = ∇h(z).
Thus, with the deﬁnition Wt = ∇h(Zt), we can write Zt = ∇h∗(Wt), which gives us (A.2).
Now assume ∇f and ∇h∗ are Lipschitz continuous functions. Then over any bounded time
intervals [t0, t1] with 0 < t0 < t1, the right-hand side of (A.2) is a Lipschitz continuous vector ﬁeld.
Thus, by the Cauchy-Lipschitz theorem, for any given initial conditions (Xt0 , Wt0) = (x0, w0) at
time t = t0, the system of diﬀerential equations (A.2) has a unique solution over the time interval
[t0, t1]. Furthermore, the solution does not blow up in any ﬁnite time, since from Theorem 2.1 we
know that the energy functional Et (2.8) is non-increasing, so in particular, the Bregman divergence
˙Xt) is bounded above by a constant. Since t1 is arbitrary, this shows that (A.2) has
Dh(x∗, Xt + t
p
a unique maximal solution, i.e., t1 can be extended to t1 → +∞.

In the above argument we have started at time t0 > 0, because the vector ﬁeld in (A.2) has a
singularity at t = 0. For p = 2, Su et al. [32] and Krichene et al. [17] treat the case when we start
˙X0 = 0. In that case, they show
at t = 0 with initial condition (X0, W0) = (x0,∇h(x0)), so that
that the system (A.2) still has a unique solution for all time [0,∞), by replacing the p/t coeﬃcient
by the approximation p/ max{t, δ} for δ > 0 and letting δ → 0. We can adapt this technique to the
more general case (A.2); alternatively, we can appeal to the time dilation property and state that
since the general system (A.2) is the result of speeding up the p = 2 case by time dilation function
τ (t) = tp/2, once we know a unique solution exists for p = 2, we can also conclude that it exists for
all p > 0.

A.3 Proof of Theorem 3.1

We deﬁne the following function, which is a generalization of Nesterov’s estimate function from [28],

ψk(x) = Cp

k

Xi=0

i(p−1)(cid:2)f (yi) + h∇f (yi), x − yii(cid:3) +

1
ǫ

Dh(x, x0).

(A.4)

20

The estimate function ψk arises as the objective function that the sequence zk is optimizing in (3.6b).
Indeed, the optimality condition for the zk update (3.6b) is

By unrolling the recursion, we can write

∇h(zk) = ∇h(zk−1) − ǫCpk(p−1)∇f (yk).

∇h(zk) = ∇h(z0) − ǫCp

k

Xi=0

i(p−1)∇f (yi),

and since x0 = z0, we can write this equation as ∇ψk(zk) = 0. Since ψk is a convex function, this
means zk is the minimizer of ψk. Thus, we can equivalently write the update for zk as

For proving the convergence rate for the algorithm (3.6), we have the following property.

zk = arg min

z

ψk(z).

Lemma A.1. For all k ≥ 0, we have

ψk(zk) ≥ Ck(p)f (yk).

(A.5)

(A.6)

Since h is 1-uniformly convex of order p, the rescaled Bregman divergence 1

Proof. We proceed via induction on k ≥ 0. The base case k = 0 is true since both sides equal zero.
Now assume (A.6) holds for some k ≥ 0; we will show it also holds for k + 1.
ǫ Dh(x, x0) is ( 1
uniformly convex. Thus, the estimate function ψk (A.4) is also ( 1
Since zk is the minimizer of ψk, ∇ψk(zk) = 0, so for all x ∈ X we have

ǫ )-
ǫ )-uniformly convex of order p.

1
ǫpkx − zkkp.
Applying the inductive hypothesis (A.6) and using the convexity of f gives us

ψk(x) = ψk(zk) + Dψk (x, zk) ≥ ψk(zk) +

ψk(x) ≥ Ck(p)(cid:2)f (yk+1) + h∇f (yk+1), yk − yk+1i(cid:3) +

1
ǫpkx − zkkp.

We now add Cp(k + 1)(p−1)[f (yk+1) +h∇f (yk+1), x− yk+1i] to both sides of the equation to obtain
ψk+1(x) ≥ C(k + 1)(p)(cid:2)f (yk+1) +(cid:10)∇f (yk+1), xk+1 − yk+1 + τk(x − zk)(cid:11)(cid:3) +
(A.7)

1
ǫpkx − zkkp,

k+p , and where we have also used the deﬁnition of xk+1 as a convex

where τk = p(k+1)(p−1)
combination of yk and zk with weight τk (3.6a).

(k+1)(p) = p

Note that the ﬁrst term in (A.7) gives our desired inequality (A.6) for k + 1. So to ﬁnish the
proof, we have to prove the remaining terms in (A.7) are nonnegative. We do so by applying two
inequalities. We ﬁrst apply the inequality (3.7) to the term h∇f (yk+1), xk+1 − yk+1i, so from (A.7)
we have

ψk+1(x) ≥ C(k + 1)(p)f (yk+1) + C(k + 1)(p)M ǫ

p−1 k∇f (yk+1)k

1

+ Cp(k + 1)(p−1)h∇f (yk+1), x − zki +

21

p

p−1
∗

1
ǫpkx − zkkp.

(A.8)

Next, we apply the Fenchel-Young inequality [28, Lemma 2]

hs, ui +

1
pkukp ≥ −

p − 1

p ksk

p

p−1
∗

(A.9)

1

with the choices u = ǫ− 1

p Cp(k + 1)(p−1)∇f (yk+1). Then from (A.8), we obtain
p (x − zk) and s = ǫ
ψk+1(x) ≥ C(k + 1)(p)"f (yk+1) + M −
∗ # .
p − 1
p−1 ≤ (k + 1)(p). Then from the assumption C ≤ M p−1/pp, we see that
Notice that {(k + 1)(p−1)}
the second term inside the parentheses is nonnegative. Hence we conclude the desired inequality
ψk+1(x) ≥ C(k + 1)(p)f (yk+1). Since x ∈ X is arbitrary, it also holds for the minimizer x = zk+1 of
ψk+1, ﬁnishing the induction.

p−1 {(k + 1)(p−1)}
(k + 1)(p)

p−1 k∇f (yk+1)k

! ǫ

p−1

p

p−1 C

p−1

p

1

p

p

1

p

p

With Lemma A.1 in hand, we can complete the proof of Theorem 3.1.

Proof of Theorem 3.1. Since f is convex, we can bound the estimate sequence ψk by

ψk(x) ≤ Cp

i(p−1)f (x) +

1
ǫ

Dh(x, x0) = Ck(p)f (x) +

1
ǫ

Dh(x, x0).

k

Xi=0

This holds for all x ∈ X , and in particular for the minimizer x∗ of f . Combining the bound with
the result of Lemma A.1, and recalling that zk is the minimizer of ψk, we get

Ck(p)f (yk) ≤ ψk(zk) ≤ ψk(x∗) ≤ Ck(p)f (x∗) +

1
ǫ

Dh(x∗, x0).

Rearranging and dividing by Ck(p) gives us the desired convergence rate (3.9).

A.4 Proof of Lemma 3.2

We follow the approach of [28, Lemma 6]. Since y solves the optimization problem (3.11), it satisﬁes
the optimality condition

p−1

Xi=1

1

(i − 1)!∇if (x) (y − x)i−1 +

N
ǫ ky − xkp−2 (y − x) = 0.

(A.10)

Furthermore, since ∇p−1f is (p−1)!
order Taylor expansion of ∇f ,

ǫ

-Lipschitz, we have the following error bound on the (p − 2)-nd

Substituting (A.10) to (A.11) and writing r = ky − xk, we obtain
rp−1

∇f (y) −

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

p−1

Xi=1
(cid:13)(cid:13)(cid:13)(cid:13)

1

(i − 1)!∇if (x) (y − x)i−1(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)∗
(y − x)(cid:13)(cid:13)(cid:13)(cid:13)∗ ≤

N rp−2

ǫ

∇f (y) +

22

1
ǫky − xkp−1.

≤

(A.11)

.

(A.12)

ǫ

Squaring both sides, expanding, and rearranging the terms, we get the inequality

h∇f (y), x − yi ≥

ǫ

2N rp−2k∇f (y)k2

∗ +

(N 2 − 1)rp

2N ǫ

.

(A.13)

Note that if p = 2, then the ﬁrst term in (A.13) already implies the desired bound (3.12). Now
assume p ≥ 3. The right-hand side of (A.13) is of the form A/rp−2 + Brp, which is a convex
Bo
function of r > 0 and minimized by r∗ =n (p−2)
2p−2 "(cid:18) p
p − 2(cid:19)

(r∗)p−2 + B(r∗)p = A

2p−2 , yielding a minimum value of

+(cid:18) p − 2
p (cid:19)

p−2# ≥ A

p−2
2p−2 .

2p−2 B

p−2
2p−2

p

2p−2 B

p

p−2

A

A

p

1

p

Substituting the values A = ǫ

2N k∇f (y)k2

∗ and B = 1

2N ǫ (N 2 − 1) from (A.13), we obtain

h∇f (y), x − yi ≥ (cid:16) ǫ

∗(cid:17)
2N k∇f (y)k2

which proves (3.12).

p

2p−2 (cid:18) 1

2N ǫ

(N 2 − 1)(cid:19)

p−2
2p−2

=

(N 2 − 1)
2N

p−2
2p−2

ǫ

1

p−1k∇f (y)k

p

p−1
∗

To obtain the ﬁrst inequality of (3.13), we use Cauchy-Schwarz inequality on (3.12),

(N 2 − 1)
2N

p−2
2p−2

ǫ

1

p

p−1

p−1k∇f (y)k

∗ ≤ h∇f (y), x − yi ≤ k∇f (y)k∗ kx − yk

and cancel out k∇f (y)k∗ from both sides. For the second inequality of (3.13), we use triangle
inequality on the left hand side of (A.12),

N rp−1

ǫ − k∇f (y)k∗ ≤ (cid:13)(cid:13)(cid:13)(cid:13)

∇f (y) +

N rp−2

ǫ

rp−1

ǫ

.

(y − x)(cid:13)(cid:13)(cid:13)(cid:13)∗ ≤

Rearranging the terms and taking the (p − 1)-st root of both sides gives us the result (3.13).

A.5 Proof of Theorem 3.3

This proof follows the approach in the proof of [28, Theorem 1]. We ﬁrst prove the following lemma.
Here δk = f (xk) − f (x∗) ≥ 0 denotes the residual value at iteration k.
Lemma A.2. Under the setting of Theorem 3.3, we have

δk+1 ≤ δk −

(p − 1)

p

·(cid:18)

ǫδp
k

(N + 1)Rp(cid:19)

1

p−1

.

(A.14)

Proof. Since f is (p−1)!

ǫ

-smooth of order p− 1, by the Taylor remainder theorem we have the bound

|fp−1(x; xk) − f (x)| ≤

1
ǫpkx − xkkp.

23

Then from the deﬁnition of xk+1 (3.14), we have

f (xk+1) = min

x∈X(cid:26)fp−1(x; xk) +

N

ǫpkx − xkkp(cid:27) ≤ min

x∈X(cid:26)f (x) +

N + 1

ǫp kx − xkkp(cid:27) .

(A.15)

Plugging in x = xk on the right-hand side of (A.15) shows that f (xk+1) ≤ f (xk); that is, the
algorithm (3.14) is a descent method. In particular, for all k ≥ 0 we have kxk − x∗k ≤ R, where
R = supx : f (x)≤f (x0) kx − x∗k is the radius of the level set as deﬁned in Theorem 3.3. Moreover,
plugging in x = x∗ on the right-hand side of (A.15) gives us

f (xk+1) − f (x∗) ≤

N + 1
ǫp kxk − x∗kp ≤

N + 1

ǫp

Rp.

(A.16)

Now for any λ ∈ [0, 1], consider the midpoint

xλ = x∗ + (1 − λ)(xk − x∗) = λx∗ + (1 − λ)xk.

By Jensen’s inequality, f (xλ) ≤ λf (x∗) + (1− λ)f (xk). We also have kxλ − xkk = λkxk − x∗k ≤ λR.
Plugging in the point xλ to the right-hand side of (A.15) gives

f (xk+1) ≤ f (xλ) +

N + 1
ǫp kxλ − xkkp ≤ λf (x∗) + (1 − λ)f (xk) +
With the notation δk = f (xk) − f (x∗), we can write the last inequality as

δk+1 ≤ (1 − λ)δk +

N + 1

ǫp

Rpλp.

N + 1

ǫp

Rpλp.

(A.17)

1

p−1 . Note that

.

(A.18)

p−1

p−2−i

k+1 (cid:19)
· δ

p−2
p−1
k

.

The right-hand side is a convex function of λ, which is minimized at λ∗ =n ǫ
λ∗ ∈ [0, 1] by (A.16). Plugging in λ∗ to (A.17) yields the desired bound (A.14).

N +1

δk

Rpo

With Lemma A.2, we can complete the proof of Theorem 3.3.

− 1
Proof of Theorem 3.3. Deﬁne the energy functional ek = δ
k

p−1

. We can write

1

1

ek+1 − ek =

1
1

p−1
δ
k+1

−

1
1

p−1
δ
k

=

p−1

δ
k − δ
k+1 · δ
δ

p−1

1

p−1
k+1
1

p−1
k

=

1

δk − δk+1
k+1 · δ
δ

p−1
k

p−1

1

·

1

i

p−1
i=0 δ
k

(cid:18)Pp−2

Since δk+1 ≤ δk, we can upper bound the summation in the denominator of (A.18) by (p − 1)δ
We use Lemma A.2 to lower bound δk − δk−1, obtaining

ek+1 − ek ≥

(p − 1)

p

·(cid:18)

ǫδp
k

(N + 1)Rp(cid:19)

1

p−1

·

1
2

p−1
δ
k

·

1

p−2
p−1
k

(p − 1)δ

=

1

p ·(cid:18)

ǫ

(N + 1)Rp(cid:19)

1

p−1

.

(A.19)

Summing (A.19) and telescoping the terms, we get

1

1

p−1

= ek ≥ ek − e0 ≥

1

p−1

k

p ·(cid:18)

ǫ

(N + 1)Rp(cid:19)

(f (xk) − f (x∗))

which gives us the desired conclusion (3.15).

24

A.6 Proof of Theorem 3.4

We write the higher-order gradient algorithm (3.14) (with N = 1) as

xk+1 − xk = arg min

u (cid:26)f (xk) + h∇f (xk), ui + ··· +

1

(p − 1)!∇p−1f (xk)up−1 +

1

ǫpkukp(cid:27) .

(A.20)

Our goal is to express the sequence xk as a discretization xk = Xt, xk+1 = Xt+δ ≈ Xt + δ ˙Xt of
some continuous-time curve Xt with time step δ > 0, which will be a function of ǫ. To that end,
we write u = δv, so (A.20) becomes

xk+1 − xk

δ

= arg min

v (cid:26)f (xk) + δh∇f (xk), vi + ··· +

δp−1

(p − 1)!∇p−1f (xk)vp−1 +

δp

ǫpkvkp(cid:27) .

Eliminating the constant term f (xk) from the right-hand side, which does not change the minimizer,
and canceling a factor of δ, we get

xk+1 − xk

δ

= arg min

δ
2∇2f (xk)v2 + ··· +

v (cid:26)h∇f (xk), vi +

ǫp kvkp(cid:27) .
We see that the ﬁrst term in the objective function does not depend on δ. As ǫ → 0, for the equation
to have a meaningful limit, we have to set δp−1 = ǫ, so the last term in the objective function
1
p−1 , so as
becomes a constant. On the other hand, the middle terms all have dependence on δ = ǫ
ǫ → 0, those terms vanish. Thus, the limit as ǫ → 0 is

(p − 1)!∇p−1f (xk)vp−1 +

δp−2

δp−1

˙Xt = arg min

v (cid:26)h∇f (Xt), vi +

1

pkvkp(cid:27) .

Equivalently,

˙Xt satisﬁes the optimality condition

This gives us the relation k∇f (Xt)k∗ = k ˙Xtkp−1, so we can also write (A.22) as

∇f (Xt) + k ˙Xtkp−2 ˙Xt = 0.

(A.21)

(A.22)

˙Xt = − ∇f (Xt)
k ˙Xtkp−2

= − ∇f (Xt)
k∇f (Xt)k

p−2
p−1
∗

,

which is the rescaled gradient ﬂow as claimed in (3.16).

We note that the rescaled gradient ﬂow (3.16) is a descent method, since

d
dt

f (Xt) = h∇f (Xt),

˙Xti = −k∇f (Xt)k

p−1

∗ ≤ 0.

p

Now to establish the convergence rate of the rescaled gradient ﬂow (3.16), we consider the energy
functional

Et = (f (Xt) − f (x∗))− 1

p−1

25

(A.23)

which is the same energy functional as in the discrete-time convergence proof in Appendix A.5.
The energy functional Et has time derivative
1

.

p

p−1

˙Et = −

(p − 1)

h∇f (Xt),

˙Xti
(f (Xt) − f (x∗))

If Xt satisﬁes the rescaled gradient ﬂow equation (3.16), then ˙Et simpliﬁes to

˙Et =

1

(p − 1)(cid:18) k∇f (Xt)k∗

f (Xt) − f (x∗)(cid:19)

p

p−1

.

(A.24)

By the convexity of f and the Cauchy-Schwarz inequality, we have

0 ≤ f (Xt) − f (x∗) ≤ h∇f (Xt), Xt − x∗i ≤ k∇f (Xt)k∗ kXt − x∗k.

Since the rescaled gradient ﬂow is a descent method, we have kXt−x∗k ≤ R. Therefore, from (A.24)
we get the bound

˙Et ≥

1

1

(p − 1)

kXt − x∗k

p

p−1 ≥

1

(p − 1)R

.

p

p−1

This means that Et increases at least linearly, so

1

(f (Xt) − f (x∗))

1

p−1

= Et ≥ E0 +

t

(p − 1) R

p

p−1 ≥

t

(p − 1) R

,

p

p−1

which gives us the desired result (3.17).

Remark: From the proof above, we see that rescaled gradient ﬂow (3.16) is a generalization of
the usual gradient ﬂow (the case p = 2) which is obtained by replacing the squared norm by the
p-th power of the norm in the variational formulation (A.21). It turns out that when the objective
function is the p-th power of the norm, f (x) = 1
pkxkp, the rescaled gradient ﬂow (3.16) reduces to
an explicit equation. Speciﬁcally, in this case we have ∇f (x) = kxkp−2x, so k∇f (x)k∗ = kxkp−1.
Therefore, the rescaled gradient ﬂow equation (3.16) becomes

kXtkp−2 = −Xt,
which is now independent of p, and has an explicit solution Xt = e−tX0.

˙Xt = − ∇f (Xt)
k∇f (Xt)k

= −kXtkp−2Xt

p−2
p−1
∗

Alternative proof of convergence rate.
alternative energy functional,

In the proof above, we can also use the following

˜Et = tp(f (Xt) − f (x∗)).

(A.25)

26

Its time derivative is

˙˜Et = ptp−1(f (Xt) − f (x∗)) + tph∇f (Xt),
˙Xti
≤ ptp−1h∇f (Xt), Xt − x∗i + tph∇f (Xt),
= ptp−1h∇f (Xt), Xt − x∗i − tpk∇f (Xt)k

˙Xti

p

,

(A.26a)

(A.26b)

p−1
∗

where (A.26a) follows from the convexity of f , and in (A.26b) we have substituted the rescaled gra-
dient ﬂow dynamic (3.16). We now apply the Fenchel-Young inequality (A.9) with s = tp−1∇f (Xt)
and u = −(p − 1)(Xt − x∗), to obtain

˙˜Et ≤

1

p − 1k(p − 1)(Xt − x∗)kp ≤ (p − 1)p−1Rp,

(A.27)

where in the last step we have used the fact that kXt − x∗k ≤ R since rescaled gradient ﬂow is a
descent method. Integrating (A.27) and plugging in the deﬁnition of ˜Et (A.25), we obtain

f (Xt) − f (x∗) ≤

(p − 1)p−1 Rp

tp−1

,

which is exactly the same bound as claimed in (3.17).

B Further properties

B.1 Exponential convergence rate via uniform convexity

Similar to the polynomial case in Section 3, in this section we study the subfamily of Bregman
Lagrangian (2.1) with the following choice of parameters, parameterized by c > 0,

αt = log c

βt = ct

γt = ct.

(B.1a)

(B.1b)

(B.1c)

The parameters (B.1) satisfy the ideal scaling condition (2.2), with an equality on the ﬁrst condi-
tion (2.2a). The Euler-Lagrange equation (2.6) in this case is given by

¨Xt + c ˙Xt + c2ect(cid:20)∇2h(cid:18)Xt +

1
c

˙Xt(cid:19)(cid:21)−1

∇f (Xt) = 0,

(B.2)

and by Theorem 2.1, it has an O(e−ct) rate of convergence. Thus, whereas the polynomial La-
grangian ﬂow (3.2) has a polynomial rate of convergence, the exponential Lagrangian ﬂow (B.2)
has an exponential rate of convergence. Furthermore, from the time-dilation property in Theo-
rem 2.2, we see that we can obtain the exponential curve (B.2) by speeding up the polynomial
curve (3.2) using a time-dilation function τ (t) = ect/p.

27

However, unlike the polynomial Lagrangian ﬂow (3.2), the process of discretizing the exponential
Lagrangian ﬂow (B.2) is not as straightforward. Following the same approach as the polynomial
family, we write the second-order equation (B.2) as the following pair of ﬁrst-order equations:

Zt = Xt +

1
c

˙Xt

d
dt∇h(Zt) = −cect∇f (Xt).

(B.3a)

(B.3b)

Now we discretize Xt and Zt into sequences xk and zk with time step δ > 0, so that t = δk as
before. In doing so, we can write (B.3) as the following discrete-time equations similar to (3.4) and
(3.5):

xk+1 = cδzk + (1 − cδ)xk
zk+1 = arg min

z (cid:26)cecδkh∇f (xk), zi +

1
δ

Dh(z, zk)(cid:27) .

(B.4a)

(B.4b)

Note that the weight in (B.4a) is independent of time, but depends on δ, and (B.4b) suggests the
step size ǫ = δ in the algorithm. If our analogy between continuous and discrete-time convergence
holds, then given the O(e−ct) convergence rate in continuous time, we expect a matching O( 1
ǫ e−ck)
convergence rate in discrete time. However, it is not clear how to obtain that rate via (B.4). If
we try to adapt the proof of Theorem 3.1, we ﬁnd that in order to conclude a convergence rate
O(δe−cδk), we need to introduce a sequence yk satisfying the following analog of inequality (3.7)
(with the ideal choice p = ∞):

h∇f (yk), xk − yki ≥ Mk∇f (yk)k∗.

(B.5)

Notice that the rates are consistent if we set ǫ = δ = 1. However, the condition (B.5) means we
need to make a constant improvement in each iteration from xk to yk, although we are also free on
how we choose to construct yk and impose any assumptions on f .

In the remainder of this section, we approach this problem from a discrete-time perspective,
and study the performance of the higher-order gradient algorithm (3.14) and its accelerated vari-
ant (3.18) when f is uniformly convex.

B.1.1 Exponential convergence rate of higher-order gradient algorithm

In this section we show that the higher-order gradient algorithm (3.14) has an exponential conver-
gence rate when the objective function f is uniformly convex of order p ≥ 2; this generalizes the
results in [28, Section 5] for the case p = 3, and the classical result of gradient descent for the case
p = 2 [25].

Speciﬁcally, we have the following result. Recall the deﬁnition of smoothness in (3.10), and the

deﬁnition of uniform convexity in (3.8).

28

Theorem B.1. Suppose f is (p−1)!
the p-th order gradient algorithm (3.14) with N > 1 has convergence rate

-smooth of order p−1, and σ-uniformly convex of order p. Then

ǫ

f (xk+1) − f (x∗) ≤

(N + 1)kx0 − x∗kp
p−1(cid:1)k
ǫp(cid:0)1 + Lκ

1

= O(cid:18) 1

ǫ

exp(−Lκ

1

p−1 k)(cid:19) ,

(B.6)

p−2
2p−2 /(2N ), and κ = ǫσ is the inverse condition number (which we assume is

where L = (N 2 − 1)
small).

Proof. By inequality (3.12) from Lemma 3.2, we know that since f is (p−1)!

ǫ

-smooth of order p − 1,

h∇f (xk+1), xk − xk+1i ≥ Lǫ

p−1k∇f (xk+1)k

1

p

p−1
∗

,

where L = (N 2−1)
Furthermore, since f is σ-uniformly convex of order p, from [28, Lemma 3] we also have

2p−2 /(2N ). Since f is convex, we have f (xk)−f (xk+1) ≥ h∇f (xk+1), xk−xk+1i.

p−2

p

k∇f (xk+1)k

p−1

∗ ≥

p

p − 1

σ

1

p−1 (f (xk+1) − f (x∗)) ≥ σ

1

p−1 (f (xk+1) − f (x∗)).

(B.7)

Combining these inequalities and recalling the deﬁnition κ = ǫσ gives us

f (xk) − f (xk+1) ≥ Lκ

1

p−1 (f (xk+1) − f (x∗)),

or equivalently,

f (xk+1) − f (x∗) ≤

f (xk) − f (x∗)
1 + Lκ

p−1 ≤

1

1

f (x1) − f (x∗)
p−1(cid:1)k
(cid:0)1 + Lκ

.

(B.8)

Note that by the smoothness of f , as in (A.15), we can write f (x1) ≤ minx{f (x)+ N +1
f (x∗) + N +1

ǫp kx−x0kp} ≤
ǫp kx0 − x∗kp. Furthermore, since we assume the inverse condition number κ = ǫσ is
1
p−1 ). Therefore, (B.8) yields the desired convergence

1

small, we can write 1 + Lκ
rate (B.6).

p−1 ≈ exp(Lκ

Notice that the result of Theorem B.1 matches the desired convergence rate O( 1

ǫ e−ck) discussed

in Appendix B.1, with c = Lκ

1
p−1 .

Exponential convergence rate of rescaled gradient ﬂow. As a side remark, we note that
the rescaled gradient ﬂow also has an exponential convergence rate when the objective function
f is uniformly convex. However, notice that the following continuous-time convergence rate only
depends on the uniform convexity constant of f , whereas the discrete-time convergence rate above
also depends on the Lipschitz constant for the higher-order smoothness of f .

Theorem B.2. If f is σ-uniformly convex of order p, then the rescaled gradient ﬂow (3.16) has
convergence rate

f (Xt) − f (x∗) ≤ (f (X0) − f (x∗)) exp(cid:16)−σ

1

p−1 t(cid:17) .

(B.9)

29

yk = arg min

zk = arg min

2

y (cid:26)fp−1(y; xk) +
z ( p

ǫpky − xkkp(cid:27)
i(p−1)h∇f (yi), zi +

(4p)p

k

Xi=0

(B.10a)

(B.10b)

(B.10c)

2p−2

ǫp kz − x0kp) .

Proof. As we saw in (B.7), the uniform convexity of f implies the inequality

p

k∇f (Xt)k

p−1

∗ ≥ σ

1

p−1 (f (Xt) − f (x∗)).

Using this inequality and plugging in the rescaled gradient ﬂow equation (3.16), we have

d
dt

(f (Xt) − f (x∗)) = h∇f (Xt),

˙Xti = −k∇f (Xt)k

p

p−1

∗ ≤ −σ

1

p−1 (f (Xt) − f (x∗)).

Dividing both sides by f (Xt)−f (x∗) and integrating, we get the desired convergence rate (B.9).

B.1.2 Exponential convergence rate of accelerated method with restart scheme

We now show that a variant of the accelerated gradient method (3.18) with a restart scheme also
attains an exponential convergence rate, with a better dependence on the condition number κ than
the higher-order gradient method as in Appendix B.1.1.

Speciﬁcally, we consider the following variant of the accelerated gradient method (3.18),

xk+1 =

p

k + p

zk +

k

k + p

yk

In (B.10), for simplicity we have explicitly set the constant N in (3.18b) to be N = 2, and set
2 /((2N )p−1pp).
C in (3.18c) to be C = 1/(4p)p, which satisﬁes the condition C ≤ (N 2 − 1)
Furthermore, for the z-update (B.10c) we have used the equivalent version (A.5) where we unroll
the recursion, and we have also replaced the Bregman divergence in the z-update (3.18c) by the
rescaled p-th power dp(z) = 2p−2
p kz − x0kp, which is 1-uniformly convex of order p. The proof of
Theorem 3.1 still holds in this case, so we have the guarantee

p−2

f (yk) − f (x∗) ≤

(4p)p · 2p−2kx0 − x∗kp

ǫpk(p)

≤

23p−2pp−1kx0 − x∗kp

ǫkp

.

(B.11)

Then we deﬁne the following restart scheme, which proceeds by running the accelerated method (B.10)

for some number of iterations at each step,

ˆxk = (cid:0)the output ym of running (B.10) for m iterations with input x0 = ˆxk−m(cid:1).

Our main result is the following.
Theorem B.3. Suppose f is (p−1)!
-smooth of order p − 1, and σ-uniformly convex of order p.
1
Let ˆxk be the output of running the restart scheme (B.12) for k/m times with m = 8p/κ
p , where
κ = ǫσ is the inverse condition number, and let ˆyk = Gp,ǫ,2(ˆxk) be the output of running one step
of the gradient update (3.11) with input ˆxk. Then we have the convergence rate

(B.12)

ǫ

f (ˆyk) − f (x∗) ≤

3kˆx0 − x∗kp

ǫp ek/m

= O  1

ǫ

exp −

30

κ

1
p k

8p !! .

(B.13)

Proof. Since f is σ-uniformly convex of order p, and by the bound (B.11), we have

σ
pkˆxk − x∗kp ≤ f (ˆxk) − f (x∗) ≤

23p−2pp−1kˆxk−m − x∗kp

ǫmp

σ
pekˆxk−m − x∗kp,

≤

(B.14)

where the last inequality follows from our choice of m. Thus, an execution of (B.12) with m
iterations of the accelerated method reduces the distance to optimum by a factor of at least 1/e.
Iterating (B.14), we obtain kˆxk − x∗kp ≤ e−k/mkˆx0 − x∗kp. To convert this into a bound on the
function value, we use the smoothness of f . As noted in (A.15), since ˆyk is the output of one step
of the gradient update (3.11) with input ˆxk, we have f (ˆyk) − f (x∗) ≤ 3
ǫpkˆxk − x∗kp. This gives the
desired bound (B.13).

The result of Theorem B.3 matches the desired convergence rate O( 1

ǫ e−ck) as discussed in
1
Appendix B.1 with c = 1
p . Note that this convergence rate has a better dependence on the
inverse condition number κ = ǫσ than the higher-order gradient algorithm as in Theorem B.1,
1
because κ
p−1 for small κ. This generalizes the conclusion of [28, Section 5] for the case p = 3.
However, as noted previously, the link to continuous time is not as clear as that of the polynomial
family.

1
p > κ

8p κ

B.2 Hessian vs. Bregman Lagrangian
In a Hessian manifold, the metric is generated by the Hessian ∇2h of the distance-generating
˙Xt = −∇f (Xt),
function h. So for example, the gradient ﬂow equation in the Euclidean case,
which can be written as

˙Xt = arg min

V (cid:26)h∇f (Xt), V i +

1

2kV k2(cid:27) ,

in general becomes the natural gradient ﬂow ˙Xt = −[∇2h(Xt)]−1∇f (Xt), or equivalently,

˙Xt = arg min

V (cid:26)h∇f (Xt), V i +

1
2kV k2

∇2h(Xt)(cid:27) ,

which is obtained by replacing the Euclidean squared norm kvk2 = hv, vi by the Hessian metric

kvk2

∇2h(x) := h∇2h(x)v, vi.

At the Lagrangian level, recall that a starting point of our work is the diﬀerential equation
˙Xt + ∇f (Xt) = 0 for accelerated gradient descent [32], which we observe is the Euler-

¨Xt + 3
t
Lagrange equation for the damped Lagrangian

L(X, V, t) = t3(cid:18) 1

2kV k2 − f (X)(cid:19) .

(B.15)

How should we generalize this Lagrangian to the non-Euclidean case? From our discussion on
natural gradient ﬂow, a natural guess is to replace the Euclidean metric in (B.15) by the Hessian
metric. Thus, we are led to consider the following family of Hessian Lagrangians:

LHess(X, V, t) = eγt(cid:18) 1

2kV k2

∇2h(X) − eβtf (X)(cid:19)

(B.16)

31

where we have also introduced arbitrary weighting functions βt, γt ∈ R ((B.15) is the Euclidean
case with βt = 0, γt = 3 log t). However, the Hessian Lagrangian (B.16) turns out to be unsuit-
able for our optimization purposes. This is because the Euler-Lagrange equation for the Hessian
Lagrangian (B.16),

1
2∇3h(Xt) ˙Xt

˙Xt + ∇2h(Xt)(cid:16) ¨Xt + ˙γt

˙Xt(cid:17) + eβt∇f (Xt) = 0,

(B.17)

involves the third-order derivative ∇3h (which comes from being the derivative of the metric tensor
∇2h). This makes the analysis diﬃcult, preventing us from obtaining a convergence rate for (B.17).
Furthermore, the presence of ∇3h in the equation makes it diﬃcult to implement as an eﬃcient
discrete-time algorithm.

On the other hand, our work shows that the “correct” way to generalize (B.15) to the non-
Euclidean case is to use the Bregman divergence, rather than Hessian metric. This results in the
general Bregman Lagrangian family (2.1), which requires an additional parameter αt controlling
the amount of interaction between the position X and velocity V . When the parameters are
coupled in an ideal scaling, the Bregman Lagrangian produces dynamics that converge at a provable
rate. This is achieved via the design of a corresponding Lyapunov function (the energy functional
Et (2.8)), whose form is intimately tied to the use of the Bregman divergence in the Lagrangian.
Furthermore, for the polynomial family, we can discretize the resulting dynamics as a discrete-time
algorithm (3.18) that does not require the Hessian ∇2h, but only the gradient ∇h.

It is interesting to consider whether the Hessian Lagrangian (B.16) has useful properties,
and how it relates to the Bregman Lagrangian. For a small displacement ε > 0 we know that
Bregman divergence approximates the Hessian metric, i.e., D(x + εv, x) ≈ ε2
∇2h(x). Setting
ε = e−αt, this suggests that the Bregman Lagrangian (2.1) is approximating the Hessian La-
grangian LHess(X, V, t) = eγt−αt(cid:16) 1

∇2h(X) − e2αt+βtf (X)(cid:17). However, this argument assumes ǫ

is small, whereas in our particular case of interest (the polynomial subfamily in Section 3) the value
of ǫ = e−αt = t

p is growing over time.

2kV k2

2 kvk2

B.3 Gradient vs. Lagrangian ﬂows

In the Euclidean case, we can think of gradient ﬂow as describing the behavior of a damped
Lagrangian system “in an asymptotic regime in which dissipative eﬀects play such an important
role, that the eﬀects of forcing and dissipation compensate each other” [34, p. 646]. That is, the
gradient ﬂow equation ˙Xt = −∇f (Xt) can be seen as the strong-friction limit λ → ∞ of the
equation ¨Xt + λ ˙Xt + λ∇f (Xt) = 0.

This is perhaps more apparent if we deﬁne m = 1/λ to be the “mass” of the ﬁctitious particle,

so the equation of motion becomes

which is the Euler-Lagrange equation of the damped Lagrangian

m ¨Xt + ˙Xt + ∇f (Xt) = 0,

L(X, V, t) = et/m(cid:16) m

2 kV k2 − f (X)(cid:17) ,

32

(B.18)

(B.19)

However, notice that in all these cases, the momentum variable P = ∂L

where the damping factor et/m also scales with m. In the massless limit m → 0, we indeed recover
gradient ﬂow from (B.18). In the following, we show that this result also holds more generally, both
for natural gradient ﬂow (as the massless limit of a Bregman Lagrangian ﬂow) and for the rescaled
gradient ﬂow (as the massless limit of a Lagrangian ﬂow which uses the p-th power of the norm).
∂V becomes inﬁnite as
m → 0. For instance, P = met/mV for (B.19), and met/m → ∞. This means as m → 0, the
particle also becomes more massive and has more inertia. Thus, gradient ﬂow is the limiting case
where the inﬁnitely massive particle simply rolls downhill and stops at the minimum x∗ as soon as
the force −∇f vanishes, without oscillation (which is damped by the inﬁnitely strong friction). In
this view, moving from a ﬁrst-order gradient algorithm to a second-order Lagrangian (accelerated)
algorithm does not amount to preventing oscillation; rather, it is the opposite, by unwinding the
curve to ﬁnite momentum where it can travel faster, albeit with some oscillation.

Natural gradient ﬂow as massless limit. Consider the following Lagrangian

L(X, V, t) =

et/m
m

(Dh(X + mV, X) − mf (X)) ,

(B.20)

which is the Bregman Lagrangian (2.1) with parameters αt = − log m, βt = log m, and γt = t/m
(which satisfy the ideal scaling (2.2)). Note that (B.20) recovers (B.19) in the Euclidean case. The
Euler-Lagrange equation (2.6) for the Lagrangian (B.20) is given by

mh∇2h(Xt + m ˙Xt)i−1
Multiplying the equation by m and letting m → 0, we recover

˙Xt +

¨Xt +

1

1
m

∇f (Xt) = 0.

which is the natural gradient ﬂow equation. In this case the momentum variable is P = ∂L
et/m(∇h(X + mV ) − ∇h(X)) ≈ met/m∇2h(X)V , so we still have P → ∞ as m → 0.

∂V =

˙Xt +(cid:2)∇2h(Xt)(cid:3)−1

∇f (Xt) = 0,

Rescaled gradient ﬂow as massless limit. Consider the following Lagrangian

L(X, V, t) = et/m(cid:18) m

p kV kp − f (X)(cid:19) ,

(B.21)

where we use the p-th power of the norm to measure the kinetic energy. Note that (B.21) recov-
ers (B.19) in the case p = 2. The Euler-Lagrange equation is

So as m → 0, this equation recovers

k ˙Xtkp−2(cid:0)m ¨Xt + ˙Xt(cid:1) + m(p − 2)k ˙Xtkp−4h ¨Xt,
k ˙Xtkp−2 ˙Xt + ∇f (Xt) = 0

˙Xti ˙Xt + ∇f (Xt) = 0.

(B.22)

which is equivalent to the rescaled gradient ﬂow (3.16). In this case the momentum variable is
P = ∂L

∂V = met/mkV kp−2V , which still goes to inﬁnity as m → 0.

33

B.4 Bregman Hamiltonian

In this section we deﬁne and compute the Bregman Hamiltonian corresponding to the Bregman
Lagrangian. In general, given a Lagrangian L(X, V, t), its Hamiltonian is deﬁned by

H(X, P, t) = hP, V i − L(X, V, t)
∂V is the momentum variable conjugate to position.

where P = ∂L

For the Bregman Lagrangian (2.1), the momentum variable is given by

(B.23)

(B.24)

(B.25)

We can invert this equation to solve for the velocity V ,

P =

∂L
∂V

= eγt(cid:0)∇h(X + e−αt V ) − ∇h(X)(cid:1) .
V = eαt(cid:0)∇h∗(∇h(X) + e−γtP ) − X(cid:1) ,

where h∗ is the conjugate function to h (recall the deﬁnition in (A.3)), and we have used the
property that ∇h∗ = [∇h]−1. So for the ﬁrst term in the deﬁnition (B.23) we have

hP, V i = eαt(cid:10)P, ∇h∗(∇h(X) + e−γtP ) − X(cid:11).

Next, we write the Bregman Lagrangian L(X, V, t) in terms of (X, P, t). We can directly substi-
tute (B.25) to the deﬁnition (2.1) and calculate the result. Alternatively, we can use the property
that the Bregman divergences of h and h∗ satisfy Dh(y, x) = Dh∗(∇h(x),∇h(y)). Therefore, we
can write the Bregman Lagrangian (2.1) as
L(X, V, t) = eαt+γt(cid:16)Dh∗(cid:0)∇h(X), ∇h(X + e−αtV )(cid:1) − eβtf (X)(cid:17)
= eαt+γt(cid:16)Dh∗(cid:0)∇h(X), ∇h(X) + e−γt P(cid:1) − eβtf (X)(cid:17)
= eαt+γt(cid:16)h∗(∇h(X)) − h∗(∇h(X) + e−γtP ) + e−γth∇h∗(∇h(X) + e−γt P ), Pi − eβtf (X)(cid:17) ,
where in the second step we have used the relation ∇h(X + e−αtV ) = ∇h(X) + e−γt P from (B.24),
and in the last step we have expanded the Bregman divergence.

Substituting these calculations into (B.23) and simplifying, we get the Hamiltonian

H(X, P, t) = eαt+γt(cid:16)h∗(∇h(X) + e−γt P ) − h∗(∇h(X)) − hX, e−γt Pi + eβtf (X)(cid:17) .

Since X = ∇h∗(∇h(X)), we can also write this result in terms of the Bregman divergence of h∗,
(B.26)

H(X, P, t) = eαt+γt(cid:16)Dh∗(∇h(X) + e−γt P, ∇h(X)) + eβtf (X)(cid:17) .

We call the Hamiltonian (B.26) the Bregman Hamiltonian. Notice that whereas the Bregman
Lagrangian takes the form of the diﬀerence between the kinetic and potential energy, the Bregman
Hamiltonian takes the form of the sum of the kinetic and potential energy. (However, note that
the kinetic energy is slightly diﬀerent: it is Dh∗(∇h(X) + e−γt P, ∇h(X)) = Dh(X, X + e−αt V ) in
the Hamiltonian (B.26), while it is Dh(X + e−αtV, X) in the Lagrangian (2.1).)

34

Hamiltonian equations of motion. The second-order Euler-Lagrange equation of a Lagrangian
can be equivalently written as a pair of ﬁrst-order equations

˙Xt =

∂H
∂P

(Xt, Pt, t),

˙Pt = −

∂H
∂X

(Xt, Pt, t).

For the Bregman Hamiltonian (B.26), the equations of motion are given by

(B.27)

(B.28a)

(B.28b)

˙Xt = eαt(cid:0)∇h∗(∇h(Xt) + e−γtPt) − Xt(cid:1)
˙Pt = −eαt+γt∇2h(Xt)(cid:0)∇h∗(∇h(Xt) + e−γtPt) − Xt(cid:1) + eαt Pt − eαt+βt+γt∇f (Xt).

Notice that the ﬁrst equation (B.28a) recovers the deﬁnition of momentum (B.24). Furthermore,
when ˙γt = eαt , by substituting (B.28a) to (B.28b) we can write (B.28) as

d

dt(cid:8)∇h(Xt) + e−γt Pt(cid:9) = ∇2h(Xt) ˙Xt − ˙γte−γt Pt + e−γt ˙Pt = −eαt+βt∇f (Xt).

Since ∇h(Xt) + e−γt Pt = ∇h(Xt + e−αt ˙Xt) by (B.28a), this indeed recovers the Euler-Lagrange
equation (2.7).

A Lyapunov function for the Hamiltonian equations of motion (B.28) is the following, which is

simply the energy functional (2.8) written in terms of (Xt, Pt, t),

Et = Dh∗(cid:0)∇h(Xt) + e−γtPt, ∇h(x∗)(cid:1) + eβt(f (Xt) − f (x∗)).

The Hamiltonian formulation of the dynamics has appealing properties that seem worthy of
further exploration. For example, Hamiltonian ﬂow preserves volume in phase space (Liouville’s
theorem); this property has been used in the context of sampling to develop the technique of
Hamiltonian Markov chain Monte-Carlo, and may also be useful to help us design better algorithms
for optimization. Furthermore, the Hamilton-Jacobi-Bellman equation (which is a reformulation of
the Hamiltonian dynamics) is a central object of study in the ﬁeld of optimal control theory, and
it would be interesting to study the Bregman Hamiltonian framework from that perspective.

B.5 Gauge invariance

The Euler-Lagrange equation of a Lagrangian is gauge-invariant, which means it does not change
when we transform the Lagrangian by adding a total time derivative,

L′(Xt,

˙Xt, t) = L(Xt,

˙Xt, t) +

d
dt

G(Xt, t)

(B.29)

for any smooth function G. We can show this by directly checking that the Euler-Lagrange equation
of L′ is the same as that of L. Alternatively, this follows from the formulation of the principle of least
action, where we ﬁx two points (x0, t0) and (x1, t1), and ask for a curve X joining the two endpoints
(Xt0 = x0 and Xt1 = x1) that minimizes the action J(X) = R t1
˙Xt, t)dt. Thus, when the
Lagrangian transforms as (B.29), the action only changes to J ′(X) = J(X) +R t1
d
dt G(Xt, t)dt =
J(X) + G(x1, t1) − G(x0, t0). Since (x0, t0) and (x1, t1) are ﬁxed, this means the new action only

t0 L(Xt,

t0

35

diﬀers from the old action by a constant; this implies that the optimal least action curve—namely,
the Euler-Lagrange equation—does not change.

In our case, under the ideal scaling condition ˙γt = eαt (2.2b), this property implies that the

Bregman Lagrangian (2.1) is equivalent to the following Lagrangian

L′(X, V, t) = eγt+αt(cid:16)h(X + e−αt V ) − eβtf (X)(cid:17) ,

(B.30)

where we have replaced the Bregman divergence Dh(X + e−αtV, X) by its ﬁrst term h(X + e−αtV ).
Indeed, we can check that the diﬀerence between the Bregman Lagrangian (2.1) and the reduced
form (B.30) is a total time derivative,

L′(Xt,

˙Xt, t) − L(Xt,

˙Xt, t) = eγt+αt(cid:16)h(Xt) + h∇h(Xt), e−αt ˙Xti(cid:17) =

where the last step follows from the ideal scaling eαt = ˙γt.

d
dt {eγt h(Xt)}

The reduced Lagrangian (B.30) is slightly simpler than the Bregman Lagrangian (2.1), and in
a sense it makes the roles of h and f more symmetric. It also suggests that the role of h is not
so much as measuring the distance via the Hessian metric or Bregman divergence, but rather, as
evaluating the extrapolated future point Xt + e−αt ˙Xt.

B.6 Natural motion

A natural motion is the motion of a particle when it experiences no force. In the physical world, the
natural motion of a particle is a straight-line motion with constant velocity. But for the Bregman
Lagrangian, which describes a dissipative system, the natural motion always converges.

Speciﬁcally, the Bregman Lagrangian (2.1) in the case of zero (or constant) potential function
f ≡ 0 is L(X, V, t) = eαt+γtDh(X + e−αtV, X). Assuming the ideal scaling ˙γt = eαt (2.2b), its
Euler-Lagrange equation is given by (2.7), which in this case is

d
dt∇h(Xt + e−αt ˙Xt) = 0.

(B.31)

This means ∇h(Xt +e−αt ˙Xt) is a constant, say ∇h(Xt +e−αt ˙Xt) = ∇h(b) for some b ∈ X . Applying
∇h∗ = [∇h]−1 to both sides gives us Xt + e−αt ˙Xt = b. Since eαt = ˙γt, we can write this as

d
dt {eγt (Xt − b)} = eαt+γt(Xt − b) + eγt ˙Xt = 0.

This means eγt (Xt − b) is a constant, say eγt (Xt − b) = a for some a ∈ X . Thus, we conclude that
the natural motion of the Bregman Lagrangian is

Xt = ae−γt + b.

(B.32)

Notice that the natural motion is independent of h, although the Lagrangian still depends on h.
Furthermore, in contrast with the straight-line motion, the natural motion (B.32) always converges;
in particular, if we assume eγt → ∞ as t → ∞, then Xt → b.

36

The natural motion (B.32) has simple explicit invariance and symmetry properties. Indeed, (B.31)
states that ∇h(Xt + e−αt ˙Xt) is a conserved quantity, which is always equal to ∇h(b). By Noether’s
theorem, any conservation law corresponds to a symmetry of the Lagrangian.
In our case, the
corresponding symmetry is the transformation

X ′

t = Xt + e−γt u,

(B.33)
t = ˙Xt − ˙γte−γt u. Since ˙γt = eαt , this
for any u ∈ X . Under this transformation,
˙Xt, t) = eγt+αth(Xt+
implies X ′
e−αt ˙Xt) is invariant under the transformation (B.33). Therefore, the Bregman Lagrangian (which
is gauge-equivalent to the reduced Lagrangian) is also invariant. So indeed (B.33) is a symmetry
of the Bregman Lagrangian when f = 0.

t = Xt+e−αt ˙Xt. This means the reduced Lagrangian L(Xt,

˙Xt changes to ˙X ′

t+e−αt ˙X ′

B.7 The Euclidean case

In the Euclidean case many of our results and equations simplify, as we summarize in this section.
When h is the squared Euclidean norm, h(x) = 1
2kxk2, the Bregman divergence is also the squared
norm and it coincides with the Hessian metric, Dh(y, x) = 1
∇2h(x). Furthermore,
h∗ = h and both ∇h,∇h∗ are the identity function.

2ky−xk2 = 1

2ky−xk2

In the Euclidean case, the Bregman Lagrangian (2.1) becomes

L(X, V, t) = eγt−αt(cid:18) 1

2kV k2 − e2αt+βtf (X)(cid:19) .

For general αt, βt, γt, the Euler-Lagrange equation (2.5) is given by
¨Xt + ( ˙γt − ˙αt) ˙Xt + e2αt+βt∇f (Xt) = 0.
When the ideal scaling ˙γt = eαt (2.2b) holds, this equation becomes
¨Xt + (eαt − ˙αt) ˙Xt + e2αt+βt∇f (Xt) = 0,

which we can equivalently write as d
for proving the rate of convergence becomes

(B.34)
dt (Xt + e−αt ˙Xt) = −eαt+βt∇f (Xt). The energy functional (2.8)

Et =

1
2kXt + e−αt ˙Xt − x∗k2 + eβt(f (Xt) − f (x∗)).

The Bregman Hamiltonian (B.26) becomes

H(X, P, t) = eαt−γt(cid:18) 1

2kPk2 + e2γt+βtf (X)(cid:19) ,

where the momentum variable (B.24) is given by P = eγt−αtV . The Hamiltonian equations of
motion (B.28) simplify to

˙Xt = eαt−γt Pt
˙Pt = −eαt+βt+γt∇f (Xt).

37

(B.35)

(B.36)

In particular, for the polynomial case with the parameters (3.1), the Euler-Lagrange equa-

tion (B.34) is given by

¨Xt +

p + 1

t

˙Xt + Cp2tp−2∇f (Xt) = 0,

(B.37)

Su et al. [32] observed that the generalized equation ¨Xt + r
t

with an O(1/tp) rate of convergence. For p = 2, this recovers the diﬀerential equation ¨Xt + 3
∇f (Xt) = 0 corresponding to Nesterov’s accelerated gradient descent, as derived in [32].
˙Xt +∇f (Xt) = 0 still has convergence
rate O(1/t2) whenever r ≥ 3, and they posed the question on the signiﬁcance of the threshold r = 3.
˙Xt+∇f (Xt) = 0 is the case of (B.34)
Our results give the following perspective: The equation ¨Xt+ r
t
with parameters αt = log(r − 1) − log t, γt = (r − 1) log t, and βt = 2 log t − 2 log(r − 1). These
parameters satisfy the ideal scaling condition (2.2) when r ≥ 3, so Theorem 2.1 guarantees a
convergence rate of O(e−βt) = O(1/t2). However, for a ﬁxed r > 3, the choice of βt = 2 log t −
2 log(r − 1) is suboptimal, since from the ideal scaling condition ˙βt ≤ eαt we know we can increase
βt up to (r − 1) log t. This will introduce a factor of tr−3 on the force term, as in (B.37), but it will
also yield a faster convergence rate of O(1/tr−1).

t +

38

