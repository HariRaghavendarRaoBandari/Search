Stochastic dual averaging methods using variance
reduction techniques for regularized empirical risk

minimization problems

Tomoya Murata†1 and Taiji Suzuki‡1, 2

1Department of Mathematical and Computing Sciences, Graduate School of

Information Science and Engineering, Tokyo Institute of Technology

2PRESTO, Japan Science and Technology Agency (JST)

6
1
0
2

 
r
a

M
8

 

 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
2
1
4
2
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

We consider a composite convex minimization problem associated with regularized em-
pirical risk minimization, which often arises in machine learning. We propose two new
stochastic gradient methods that are based on stochastic dual averaging method with vari-
ance reduction. Our methods generate a sparser solution than the existing methods because
we do not need to take the average of the history of the solutions. This is favorable in terms
of both interpretability and generalization. Moreover, our methods have theoretical sup-
port for both a strongly and a non-strongly convex regularizer and achieve the best known
convergence rates among existing nonaccelerated stochastic gradient methods.

1

Introduction

We consider the following composite convex minimization problem:

{P (x) def= F (x) + R(x)},

min
x∈Rd

(1)

where F (x) = 1
i=1 fi(x). Here each fi : Rd → R is an Li-smooth convex function and R :
Rd → R is a relatively simple and (possibly) nondifferentiable convex function. Problems of this
form often arise in machine learning and are known as regularized empirical risk minimization.

nPn

†Email: murata.t.ab@m.titech.ac.jp
‡Email: suzuki.t.ct@m.titech.ac.jp

1

A traditional method for solving (1) is the (proximal) gradient descent (GD) method. The
GD algorithm is very simple and intuitive and achieves a linear convergence rate for a strongly
convex regularizer. However, in typical machine learning tasks, the number n can be very large,
and then the iteration cost of GD can be quite expensive.

A popular alternative for solving (1) is the stochastic gradient descent (SGD) method [17, 6,
14]. Since the iteration cost of SGD is very cheap, SGD is suitable to many machine learning
tasks. However, SGD only achieves a sublinear convergence rate and is ultimately slower than
GD.

Recently, a number of (ﬁrst-order) stochastic gradient methods using variance reduction tech-
niques, which utilize the ﬁnite sum structure of problem (1), have been proposed [11, 12, 7, 21,
10, 4, 1]. The iteration costs of these methods are the same as that of SGD, and, moreover, they
achieve a linear convergence rate for a strongly convex objective.

The stochastic average gradient (SAG) method [11, 12] can be used to treat the special case
of problem (1) with R = 0. To the best of our knowledge, SAG is the ﬁrst variance reduction
algorithm that achieves a linear convergence rate for a strongly convex objective. SAGA [4] is a
modiﬁed SAG algorithm that not only achieves a linear convergence rate for a strongly convex
objective but also can handle a nondifferentiable and non-strongly convex regularizer. However,
for a non-strongly convex regularizer, SAGA needs to output the average of the whole history of
the solutions for a convergence guarantee whereas SAG and SAGA do not for a strongly convex
objective.

In contrast, the stochastic variance reduced gradient (SVRG) method [7, 21] adopts a dif-
ferent variance reduction scheme from SAG and SAGA, and in Acc-SVRG [10] a momen-
tum scheme is applied to SVRG. These methods do not have theoretical support for a non-
strongly convex regularizer but they achieve a linear convergence rate for a strongly convex
objective. (SVRG needs to output the average of the generated solutions in the last stage for
a convergence guarantee whereas Acc-SVRG does not.) UniVR [1] is an extension of SVRG

ε

and can handle a non-strongly convex regularizer and achieves an O(cid:16)nlog 1
faster than the O(cid:0) n+Lmax

the O notation means the order of the necessary number of the gradient evaluations), which is
i=1 Li, and
Lmax = max{L1, . . . , Ln}. However, UniVR also needs to output the average of the generated
solutions in the last stage for convergence guarantees for both strongly and non-strongly convex
regularizers.

(cid:1) rate of SAGA for E[P (x) − P (x∗)] ≤ ε, ¯L = (1/n)Pn

ε + ¯L

ε(cid:17) rate (where

In summary, the algorithms used in these methods often need to output the average of the
history of the solutions as a ﬁnal solution for convergence guarantees (and, especially, for a non-
strongly convex regularizer, all of these methods need to take the average). This requirement is
unsatisfactory for a sparsity-inducing regularizer because the average of the previous solutions
could be nonsparse.

In this paper, we propose two new stochastic gradient methods using variance reduction tech-
niques: the stochastic variance reduced dual averaging (SVRDA) method and the stochastic av-
erage dual averaging (SADA) method. Compared to previous stochastic optimization methods,

2

the main advantages of our algorithms are as follows:

• Nice sparsity recovery performance: Our algorithms do not need to take the average of the
history of the solutions whereas the existing ones do. This property often leads to sparser
solutions than the existing methods for sparsity-inducing regularizers.

• Fast convergence: Our algorithms achieve the best known convergence rates among the
existing nonaccelerated stochastic gradient methods for both strongly and non-strongly
convex regularizers. Experimentally, our algorithms show comparable or superior conver-
gence speed to that of the existing methods.

2 Assumptions and notation

We make the following assumptions for our theory:

Assumption 1. Each fi is convex and differentiable, and its gradient is Li-Lipschitz continuous,
i.e.,

||∇fi(x) − ∇fi(y)||2 ≤ Li||x − y||2

(∀x, y ∈ Rd).

(2)

Condition (2) is equivalent to the following conditions (see [9]):

fi(y) ≤ fi(x) + hy − x, ∇fi(x)i +

Li
2

||x − y||2
2

(∀x, y ∈ Rd)

and

fi(x) + hy − x, ∇fi(x)i +

1
2Li

||∇fi(x) − ∇fi(y)||2

2 ≤ fi(y)

(∀x, y ∈ Rd).

Assumption 2. The regularization function R is µ-strongly convex (and it is possible that µ = 0),
i.e.,

R(y) ≥ R(x) + ξT (y − x) +

||y − x||2
2

(∀x, y ∈ Rd, ∀ξ ∈ ∂R(x)),

µ
2

where ∂R(x) denotes the set of the subgradients of R at x.

Observe that, if the regularization function R is µ-strongly convex, then the objective func-
tion P is also µ-strongly convex. It is well known that a strongly convex function with µ > 0
has a unique minimizer.

Assumption 3. The regularization function R is relatively simple, which means that the proximal
mapping of R,

proxR(y) = argmin

||x − y||2

x∈Rd (cid:26) 1

2

2 + R(x)(cid:27) ,

can be efﬁciently computed.

3

Since the function (1/2)||x−y||2

2 + R(x) is 1 + µ-strongly convex, the function proxR is well

deﬁned regardless of the strong convexity of R. Note that R is not necessarily differentiable.

Assumption 4. There exists a minimizer x∗ of problem (1).

In addition, we deﬁne ¯L = (1/n)Pn
Q on the set {1, 2, . . . , n} by Q = {qi}i∈{1,2,...,n} =(cid:8) Li

i=1 Li. Moreover, we deﬁne the probability distribution
. This probability distribution
is used to randomly pick up a data point in each iteration. By employing nonuniform distribution,
we can improve the convergence as in [21].

n ¯L(cid:9)i∈{1,2,...,n}

Many regularized empirical risk minimization problems in machine learning satisfy these
assumptions. For example, given a set of training examples (a1, b1), (a2, b2), . . . , (an, bn), where
ai ∈ Rd and bi ∈ R, if we set fi(x) = (1/2)(a⊤
i x − bi)2 and R(x) = λ||x||1, we get Lasso
regression. Then the above assumptions are satisﬁed with Li = ||ai||2, µ = 0, and proxR(y) =
i x)) and R(x) = λ1||x||1 +
(sign(yj)max{|yj| − λ, 0})d
2, we get logistic elastic net regression. Then the above assumptions are satisﬁed with
(λ2/2)||x||2
2/4, µ = λ2, and proxR(y) = (1/(1 + λ2))(sign(yj)max{|yj| − λ1, 0})d
Li = ||ai||2

j=1. If we set fi(x) = log(1 + exp(−bia⊤

j=1.

3 Related work and our contribution

In this section, we comment on the relationships between our methods and several closely related
methods.

Standard methods for solving problem (1) are the GD method and the dual averaging (DA)

method [8]. These methods take the following update rules:

xt =prox 1

xt =prox 1

η R(cid:18)xt−1 −
η tR x0 −

1
η

1
η

(GD),

∇F (xt−1)(cid:19)
∇F (xτ −1)! (DA),
tXτ =1

where x0 is an initial vector and 1/η is a constant step size. GD and DA achieve linear conver-
gence rates for a strongly convex regularizer (where, for DA, we need to borrow a multistage
scheme as in [2]). However, when the number of data n is very large, these methods can be quite
expensive because they require O(nd) computation for each update.

Effective alternatives are the SGD method [17, 6, 14] and the regularized dual averaging
(RDA) method [20]. These methods randomly draw i in {1, 2, . . . , n} and use ∇fi as an estima-
tor of the full gradient ∇F in each iteration:

xt =prox 1
ηt

xt =prox 1
ηt

R(cid:18)xt−1 −
tR x0 −

1
ηt

1
ηt

(SGD),

∇fit(xt−1)(cid:19)
∇fiτ (xτ −1)! (RDA),
tXτ =1

4

where 1/ηt is a decreasing step size. These methods only require O(d) computation for each
iteration and are suitable for large-scale problems in machine learning. However, though ∇fi is
an unbiased estimator of ∇F , it generally has a large variance, which causes slow convergence.
As a result, these methods only achieve sublinear convergence rates even when the regularizer is
strongly convex. One of simple solutions of this problem is to use a mini-batch strategy [3, 5].
However, a mini-batch strategy still gives sublinear convergence.

In recent years, a number of (ﬁrst-order) stochastic gradient methods using variance re-
duction techniques, which utilize the ﬁnite sum structure of problem (1), have been proposed
[11, 12, 7, 21, 10, 4, 1]. These methods apply a variance reduction technique to SGD. For
example, SVRG [7, 21] takes the following update rules:

for t = 1 to m

Draw it randomly from {1, 2, . . . , n}

ex =exs−1
vt = ∇fit(xt−1) − ∇fit(ex) + ∇F (ex)
exs =

η R(cid:18)xt−1 −

1
η

vt(cid:19)

xt = prox 1

1
m

mXt=1

xt.

vt is an unbiased estimator of ∇F (xt−1) and one can show that its variance is “reduced”:

E||vt − ∇F (xt−1)||2 ≤ 4 ¯L[P (xt−1) − P (x∗) + P (ex) − P (x∗)].

This means that the variance of the estimator vt converges to zero as xt andex to x∗. In this sense,

vt is a better estimator of ∇F (xt−1) than the simple estimator ∇fit(xt−1). Indeed, these methods
achieve linear convergence rates for a strongly convex regularizer.

However, these methods often need to take the average of the previous solutions for conver-
gence guarantee. For example, SVRG and UniVR [1] require taking the average of the history
of the solutions in the last stage. SAGA [4] also requires taking the average of all previous solu-
tions for a non-strongly convex regularizer, though it does not for a strongly convex regularizer.
For a sparsity-inducing regularizer, this requirement is unsatisfactory because taking the average
could cause a nonsparse solution even though the optimal solution is sparse.

In contrast, our proposed methods have theoretical convergence guarantees without taking
the average of the previous solutions for both strongly and non-strongly convex regularizers.
The basic idea of our methods is simple: We apply a variance reduction technique to RDA rather

5

Strongly convex

Non-strongly convex

Gradient complexity

Final output

Gradient complexity

Final output Memory cost

µ (cid:17) log 1+Lmax/n

ε

(cid:17)

¯L

ε(cid:17)
µ (cid:17) log 1
µ , nq ¯L
µ o(cid:17) log 1
ε(cid:17)
(cid:17)
µ (cid:17) log 1+n/Lmax

ε

SAG [11, 12]

O (cid:16)(cid:16)n + Lmax
O (cid:16)(cid:16)n +
SVRG [7, 21]
Acc-SVRG [10] O (cid:16)(cid:16)n + minn ¯L
O (cid:16)(cid:16)n + Lmax
O (cid:16)(cid:16)n +
O (cid:16)(cid:16)n +
O (cid:16)(cid:16)n + Lmax

UniVR [1]

SAGA [4]

SVRDA

SADA

¯L

¯L

µ (cid:17) log 1
ε(cid:17)
µ (cid:17) log 1
ε(cid:17)
µ (cid:17) log 1
ε(cid:17)

Prox

Avg

Prox

Prox

Avg

Prox

Prox

O (cid:16) n+Lmax

ε

(cid:17)

No direct analysis

No direct analysis

¯L

O (cid:16) n+Lmax
ε
O (cid:16)nlog 1
ε +
O (cid:16)nlog 1
ε +
O (cid:16)nlog 1

(cid:17)
ε (cid:17)
ε (cid:17)
ε (cid:17)
ε + Lmax

¯L

Avg

Avg

Avg

Prox

Prox

O(nd)

O(d)

O(d)

O(nd)

O(d)

O(d)

O(nd)

Table 1: Summary of different stochastic gradient methods that use variance reduction tech-
niques

than to SGD. For example, using an analogy to SVRG, we naturally get the following algorithm:

for t = 1 to m

Draw it randomly from {1, 2, . . . , n}

ex =exs−1
vt = ∇fit(xt−1) − ∇fit(ex) + ∇F (ex)

xt = prox 1

η tR x0 −

vτ!

1
η

tXτ =1

1
m

mXt=1

xt.

exs =

However, this algorithm is not sufﬁcient because the ﬁnal solution has to be the average of the
previous solutions for convergence guarantees (a situation that is similar to RDA). Hence we
borrow a momentum scheme and an additional SGD step.
(For more detail, see Section 4.)
Then the algorithm does not need to take the average of the previous solutions for convergence
guarantees even when the regularizer is non-strongly convex. We call this algorithm SVRDA.
Similarly, we can apply the dual averaging scheme to SAGA and we call this algorithm SADA.
Comparisons of the properties of these methods are summarized in Table 1. “Gradient com-
plexity” indicates the order of the number of the necessary gradient evaluations for E[P (x) −
P (x∗)] ≤ ε (or E||x − x∗||2
2 ≤ ε). “Final output” indicates whether the (theoretically guaran-
teed) ﬁnal solution is generated from the (weighted) average of previous iterates (Avg) or from
the proximal mapping (Prox). For sparsity-inducing regularizers, the solution generated from the
proximal mapping is often sparser than the averaged solution. As we can see from Table 1, the
proposed SVRDA and SADA both possess good properties in comparison with state-of-the-art
stochastic gradient methods.

6

4 Algorithm description

In this section, we illustrate the proposed methods.

4.1 The SVRDA method

We provide details of the SVRDA method in Algorithm 1. The SVRDA method adopts a multi-
stage scheme. Step (4) generates a variance reduced estimator of the full gradient with nonuni-
form sampling and is the same as SVRG [7, 21]. Update rules (5) and (6) are the dual averaging
update and the gradient descent update, respectively. The SVRDA method combines these two
update rules. This idea is similar to the ORDA method [2]. As in (3), for a non-strongly convex
regularizer, we have to exponentially increase the iteration number in each inner loop whereas
we can use a common ﬁxed iteration number in each inner loop for a strongly convex regular-
izer. Note that the computational cost of each iteration in the inner loop of the SVRDA method is
O(d) rather than O(nd). Also note that SVRDA outputs the solution generated from the proximal
mapping rather than the average of previous iterates. For a strongly convex regularizer, SVRDA

can output bothexS (the gradient descent step’s output) andevS (the dual averaging step’s output)
as a ﬁnal solution. This is because the convergence of E||evs − x∗||2
convergence rate whereas the theoretical convergence of E[P (evs) − P (x∗)] is not guaranteed.
OutputtingevS experimentally leads to better sparsity recovery performance than outputtingexS

2 is guaranteed with a linear

(see Section 6).

4.2 The SADA method

We provide details of the SADA method in Algorithm 2. The algorithm is similar to SVRDA
(Algorithm 1). The main difference from the SVRDA method is the update rule (7). This step
reduces the variance of the approximation of the full gradient using a SAGA [4] type variance
reduction technique rather than SVRG. Note that SADA is a multistage algorithm like SVRG
and SVRDA whereas SAGA is a single-stage algorithm. To the best of our knowledge, there
exists no single-stage dual averaging algorithm that achieves a linear convergence rate for a
strongly convex regularizer. This is probably because of the limitations of the single-stage dual
averaging algorithms. Also note that we adopt uniform sampling for SADA. Schmidt et al. [13]
have considered a nonuniform sampling scheme for SAGA on the special setting R = 0 in (1),
but their methods require two gradient evaluations in one iteration and it is not satisfactory. For
this reason, we do not adopt nonuniform sampling schemes for SADA in this paper. SADA has
theoretically similar properties to SVRDA except for the difference of the sampling scheme, and
experimentally SADA sometimes outperforms SVRDA (see Section 6).

7

Algorithm 1 SVRDA

Input: ex0 ∈ Rd, η > 0, m1 ∈ N, S ∈ N.
ev0 =ex0
α =( 1

(µ > 0)
(µ = 0)
for s = 1 to S do

4
0

x0 =exs−1, v0 = (1 − α)evs−1 + αexs−1, u0 = v0, ¯g0 = 0

ms =(m1

2s−1m1

(µ > 0)
(µ = 0)

for t = 1 to ms do

pick it ∈ {1, 2, . . . , n} randomly according to Q

gt = (∇fit(ut−1) − ∇fit(x0))/nqit + ∇F (x0)

¯gt =(cid:18)1 −

1

vt = argmin

= prox 1

xt = argmin

1
η

1
t

gt

t¯gt(cid:19)

t(cid:19) ¯gt−1 +
x∈Rd nh¯gt, xi + R(x) +
η tR(cid:18)v0 −
x∈Rd (cid:26)hgt, xi + R(x) +
ηt R(cid:18)ut−1 −
gt(cid:19)
t + 1(cid:19) xt +

1
ηt

1

vt

t + 1

1

η
2t

ηt
2

= prox 1

ut =(cid:18)1 −

(3)

(4)

(5)

(6)

||x − v0||2

||x − ut−1||2

2o
2(cid:27)

end for

end for

exs = xms,evs = vms

Output: exS orevS (µ > 0),exS (µ = 0).

8

Algorithm 2 SADA

Input: ex0 ∈ Rd, η > 0, m1 ∈ N, S ∈ N
ev0 =ex0
α =( 1

(µ > 0)
(µ = 0)
for s = 1 to S do

4
0

x0 =exs−1, v0 = (1 − α)evs−1 + αexs−1, u0 = v0, ¯g0 = 0, φ0

ms =(m1

2s−1m1

(µ > 0)
(µ = 0)

i = x0 (i = 1, 2, . . . , n)

for t = 1 to ms do

pick it ∈ {1, 2, . . . , n} uniformly at random

φt
it = ut−1, φt

i = φt−1

i

(i 6= it)

gt = ∇fit(φt

it) − ∇fit(φt−1

it ) +

1
n

∇fi(φt−1

i

)

nXi=1

(7)

¯gt =(cid:18)1 −

vt = argmin

= prox t

xt = argmin

= prox 1

ut =(cid:18)1 −

1

1
t

gt

t
η

¯gt(cid:19)

t(cid:19) ¯gt−1 +
x∈Rd nh¯gt, xi + R(x) +
η R(cid:18)v0 −
x∈Rd (cid:26)hgt, xi + R(x) +
ηt R(cid:18)ut−1 −
gt(cid:19)
t + 1(cid:19) xt +

1
ηt

t + 1

1

vt

1

η
2t

ηt
2

2o
2(cid:27)

||x − v0||2

||x − ut−1||2

end for

end for

exs = xms,evs = vms

Output: exS orevS (µ > 0),exS (µ = 0).

9

5 Convergence analysis

Now we give a convergence analysis of our algorithms. In this section, all norms || · || mean the
L2-norm || · ||2.

5.1 Convergence analysis of SVRDA

In this subsection, we give the convergence analysis of SVRDA.

Theorem 5.1. Suppose that Assumptions 1, 2, 3, and 4 hold (and it is possible that µ = 0). Let
x0 ∈ Rd, η = 4 ¯L, m1 ∈ N, and 0 ≤ α ≤ 1. Then the SVRDA algorithm satisﬁes

η + msµ

≤

1
2

2ms

E [P (exs) − P (x∗)] +
E[P (exs−1) − P (x∗)] +(cid:18) αη

2ms

µ

E||evs − x∗||2
4(cid:19) E||exs−1 − x∗||2 +

−

(1 − α)η

2ms

E||evs−1 − x∗||2.

Remark. On inequality (8) in Appendix A, we can apply a tighter bound and η can be smaller
than 4 ¯L for satisfying Theorem 5.1. This means that we can get a larger step size 1
4 ¯L and
have a theoretically tighter bound. However, practically, if we tune η, it makes little difference
and thus we omit it in this paper.

η than 1

The proof of Theorem 5.1 is given in Appendix A. Using this theorem, we derive recursive

inequalities relative to E[P (exs) − P (x∗)] and E||evs − x∗||2. Based on Theorem 5.1, we obtain

the linear convergence of SVRDA for µ > 0.

Corollary 5.2 (for a strongly convex regularizer). Suppose that Assumptions 1, 2, 3, and 4 hold.
Moreover, assume that µ > 0. Let x0 ∈ Rd, η = 4 ¯L, m1 = η
4. Then the
SVRDA algorithm satisﬁes

2µ, S ∈ N, and α = 1

E[P (exS) − P (x∗)] +

3µ
2

In addition, the SVRDA algorithm has a gradient complexity of

3µ
2

||ex0 − x∗||2(cid:21) .

E||evS − x∗||2 ≤
O(cid:18)(cid:18)n +
O(cid:18)(cid:18)n +

1

¯L

2S(cid:20)P (ex0) − P (x∗) +
ε(cid:19)
µ(cid:19) log
µε(cid:19)
µ(cid:19) log

1

1

¯L

for E[P (exS) − P (x∗)] ≤ ε and
for E||evS − x∗||2 ≤ ε.

These gradient complexities are essentially the same as the ones obtained by [11, 12, 7, 21,
10, 4, 1] and are the best known ones among the existing nonaccelerated stochastic gradient

10

methods. Note that the gradient complexity of GD is O(cid:16)n ¯L
In a typical empirical risk minimization task, we require that ε be O(cid:0) 1
µ(cid:17) logn(cid:17), O(cid:16)n ¯L
complexities of SVRDA, GD, and SGD are O(cid:16)(cid:16)n + ¯L

ε(cid:17) and that of SGD is O(cid:16) 1
µε(cid:17).
n(cid:1). Then the gradient
µ(cid:17), re-
µ logn(cid:17), and O(cid:16) n

spectively. Hence, SVRDA signiﬁcantly improves upon the gradient complexities of GD and
SGD for µ > 0.
Proof. By Theorem 5.1 and the deﬁnitions of η, ms, and α, we obtain

µ log 1

≤ · · ·

≤

≤

=

3µ
2

1

1

3µ
2

E||evS − x∗||2
E||evS−1 − x∗||2(cid:21)
||ev0 − x∗||2(cid:21)
||ex0 − x∗||2(cid:21) .

E[P (exS) − P (x∗)] +
2(cid:20)E[P (exS−1) − P (x∗)] +
2S(cid:20)P (ex0) − P (x∗) +
2S(cid:20)P (ex0) − P (x∗) +
ε(cid:1) and the order of the necessary number of outer iterations for
µε(cid:17). Finally, since SVRDA computes S times the full gradient ∇F

3µ
2
3µ
2

1

E[P (exS) − P (x∗)] ≤ ε is O(cid:0)log 1
E||evS − x∗||2 ≤ ε is O(cid:16)log 1
and 2m1 = O(cid:16) ¯L

µ(cid:17) times the gradient ∇fi, the total gradient complexity is

By this inequality, we can see that the order of the necessary number of outer iterations for

Next, we derive the convergence rate for µ = 0 from Theorem 5.1 as follows.

Corollary 5.3 (for a non-strongly convex regularizer). Suppose that Assumptions 1, 2, 3, and 4
hold (and it is possible that µ = 0). Let x0 ∈ Rd, η = 4 ¯L, m1, S ∈ N, and α = 0. Then the
SVRDA algorithm satisﬁes

E[P (exS) − P (x∗)] ≤

In addition, if m1 = O( ¯L), then the SVRDA algorithm has a gradient complexity of

4 ¯L
m1

||ex0 − x∗||2(cid:21) .

1

2S(cid:20)P (ex0) − P (x∗) +
O(cid:18)nlog
ε(cid:19)

1
ε

¯L

+

11

O(cid:18)(cid:18)n +
O(cid:18)(cid:18)n +

1

¯L

ε(cid:19)
µ(cid:19) log
µε(cid:19)
µ(cid:19) log

¯L

1

for E[P (exS) − P (x∗)] ≤ ε and
for E||evS − x∗||2 ≤ ε.

The gradient complexity of SVRDA for a non-strongly convex regularizer is the same as that
of UniVR [1] and is the best known among the existing stochastic gradient methods. Note that

for E[P (exS) − P (x∗)] ≤ ε.
the gradient complexities of GD, SGD, and SAGA [4] are O(cid:16) ¯Ln
(cid:1),
ε2(cid:1), and O(cid:0) n+Lmax
n(cid:1). Then the
respectively. In a typical empirical risk minimization task, we require that ε be O(cid:0) 1
gradient complexities of SVRDA, GD, SGD, and SAGA are O(cid:0)nlogn + ¯Ln(cid:1), O(cid:0) ¯Ln2(cid:1), O (n2),

and O (n2 + Lmaxn), respectively. Hence, SVRDA signiﬁcantly improves upon the gradient
complexities of GD, SGD, and SAGA for µ = 0.

ε (cid:17), O(cid:0) 1

ε

Proof. By Theorem 5.1 and the deﬁnitions of η, ms, and α, we obtain

1

=

η

η

≤

≤

≤ · · ·

2mS

mS+1

η
mS

E[P (exS) − P (x∗)] +
=E[P (exS) − P (x∗)] +
2(cid:20)E[P (exS−1) − P (x∗)] +
2S(cid:20)P (ex0) − P (x∗) +
2S(cid:20)P (ex0) − P (x∗) +

E||evS − x∗||2
E||evS − x∗||2
E||evS−1 − x∗||2(cid:21)
||ev0 − x∗||2(cid:21)
||ex0 − x∗||2(cid:21) ,
2S(cid:20)P (ex0) − P (x∗) +

η
m1
4 ¯L
m1

4 ¯L
m1

1

1

and therefore

1

E[P (exS) − P (x∗)] ≤

||ex0 − x∗||2(cid:21) .
ε(cid:1).
Thus the order of the necessary number of outer iterations for E[P (exS)−P (x∗)] ≤ ε is O(cid:0)log 1
s=1 2ms(cid:17) = O(cid:0)2Sm1(cid:1)
Finally, since SVRDA computes S times the full gradient ∇F and O(cid:16)PS
2ms! = O(cid:0)nS + 2Sm1(cid:1) = O(cid:18)nlog
ε(cid:19) .

times the gradient ∇fi, the total gradient complexity is

O nS +

SXs=1

1
ε

¯L

+

5.2 Convergence analysis of SADA

In this subsection, we give the convergence analysis of SADA.

Theorem 5.4. Suppose that Assumptions 1, 2, 3, and 4 hold (and it is possible that µ = 0). Let

12

x0 ∈ Rd, η = 5Lmax, m1 ∈ N, and 0 ≤ α ≤ 1. Then the SADA algorithm satisﬁes

η + msµ

≤

1
2

2ms

E[P (exs) − P (x∗)] +
E [P (exs−1) − P (x∗)] +(cid:18) αη

2ms

µ

E||evs − x∗||2
4(cid:19) E||exs−1 − x∗||2 +

−

(1 − α)η

2ms

E||evs−1 − x∗||2.

The proof of Theorem 5.4 is given in Appendix B. Using this theorem, we derive recursive

the linear convergence of SADA for µ > 0.

inequalities relative to E[P (exs) − P (x∗)] and E||evs − x∗||2. Based on Theorem 5.4, we obtain

Corollary 5.5 (for a strongly convex regularizer). Suppose that Assumptions 1, 2, 3, and 4 hold.
Moreover, assume that µ > 0. Let x0 ∈ Rd, η = 5Lmax, m1 = η
4. Then the
SADA algorithm satisﬁes

2µ, S ∈ N, and α = 1

E[P (exS) − P (x∗)] +

In addition, the SADA algorithm has a gradient complexity of

3µ
2

E||evS − x∗||2 ≤
O(cid:18)(cid:18)n +

1

2S(cid:20)P (ex0) − P (x∗) +
µ (cid:19) log

ε(cid:19)

1

Lmax

3µ
2

||ex0 − x∗||2(cid:21) .

for E[P (exS) − P (x∗)] ≤ ε and

O(cid:18)(cid:18)n +

Lmax

µ (cid:19) log

1

µε(cid:19)

These gradient complexities are essentially same as the ones obtained by [11, 12, 7, 21, 10, 4,
1] and the ones of SVRDA and are the best known among the existing nonaccelerated stochastic

for E||evS − x∗||2 ≤ ε.
gradient methods. Note that the gradient complexity of GD is O(cid:16)n ¯L
ε(cid:17) and that of SGD is
O(cid:16) 1
µε(cid:17). In a typical empirical risk minimization task, we require that ε be O(cid:0) 1
n(cid:1). Then the
µ (cid:17) logn(cid:17), O(cid:16)n ¯L
µ logn(cid:17), and
gradient complexities of SADA, GD, and SGD are O(cid:16)(cid:16)n + Lmax
O(cid:16) n
µ(cid:17), respectively. Hence, SADA signiﬁcantly improves upon the gradient complexities of

GD and SGD for µ > 0.
Proof. The proof of Corollary 5.2 is identical to that of Corollary 5.2 and we omit it.

µ log 1

Corollary 5.6 (for non-strongly convex cases). Suppose that Assumptions 1, 2, 3, and 4 hold
(and it is possible that µ = 0). Let x0 ∈ Rd, η = 5Lmax, m1, S ∈ N, and α = 0. Then the SADA
algorithm satisﬁes

E[P (exS) − P (x∗)] ≤

1

2ShP (ex0) − P (x∗) +

5Lmax

m1

||ex0 − x∗||2i.

13

In addition, if m1 = O(Lmax), then the SADA algorithm has a gradient complexity of

O(cid:16)nlog

1
ε

+

Lmax

ε (cid:17)

The gradient complexity of SADA for a non-strongly convex regularizer is the same as
that of UniVR [1] and SVRDA and is the best known among the existing stochastic gradi-

for E[P (exS) − P (x∗)] ≤ ε.
ε (cid:17),
ent methods. Note that the gradient complexities of GD, SGD, and SAGA [4] are O(cid:16) ¯Ln
(cid:1), respectively.
ε2(cid:1), and O(cid:0) n+Lmax
O(cid:0) 1
quire that ε be O(cid:0) 1
n(cid:1). Then the gradient complexities of SVRDA, GD, SGD, and SAGA are
O (nlogn + Lmaxn), O(cid:0) ¯Ln2(cid:1), O (n2), and O (n2 + Lmaxn) , respectively. Hence, SADA signif-

icantly improves upon the gradient complexities of GD, SGD, and SAGA for µ = 0.

In a typical empirical risk minimization task, we re-

ε

Proof. The proof is the same as that of Corollary 5.3 and we omit it.

6 Numerical experiments

In this section, we provide numerical experiments to demonstrate the performances of SVRDA
and SADA. We compare our methods with several state-of-the-art stochastic gradient methods:
SVRG [7, 21], SAGA [4], and UniVR [1]. For a fair comparison, we compare all different meth-
ods using solutions that are theoretically guaranteed. We used nonuniform sampling for SVRG
[7, 21], UniVR [1], and SVRDA. (Zhu et al. [1] have not considered a nonuniform sampling
scheme for UniVR, but because there is theoretical justiﬁcation of nonuniform sampling for
UniVR, we adopted nonuniform sampling for UniVR.) However, we used uniform sampling for
SAGA [4] and SADA. (Schmidt et al. [13] considered nonuniform sampling for SAGA on the
special setting R = 0 in (1), but their algorithm require two gradient evaluations in one iteration

for a gradient complexity of O(cid:16)(cid:16)n + ¯L

sampling for SAGA and SADA.)

µ(cid:17) log 1

ε(cid:17), and thus in our experiment we adopted uniform

In this experiments, we focus on the regularized logistic regression problem for binary clas-
siﬁcation: Given a set of training examples (a1, b1), (a2, b2), . . . , (an, bn), where ai ∈ Rd and
bi ∈ {+1, −1}, we ﬁnd the optimal classiﬁer x ∈ Rd by solving

min
x∈Rd

1
n

nXi=1

log(1 + exp(−bia⊤

i x)) + λ1||x||1 +

λ2
2

||x||2
2,

where λ1 and λ2 are regularization parameters.

We used three publicly available data sets in the experiments. Their sizes n and dimensions
d are listed in Table 2. Each continuous feature vector in these data sets has been normalized to
zero mean and unit variance.

14

Data sets

n

covertype1

581, 012

d

54

Reuters-215782

5,964

18,933

sido03

12, 678

4, 932

Table 2: Summary of the data sets used in our numerical experiments

We performed our experiments on a desktop computer (a Windows 7 64-bit machine with an
Intel i7-4790 CPU operating at 3.60 GHz and 8 GB of RAM) and implemented all algorithms in
MATLAB 2015a.

Figures 1a and 1b show the comparison of SVRDA and SADA with the different methods
described above on the covertype data set for different setups of λ1 and λ2 (the strongly convex
case λ2 = 10−6 > 0 (Figure 1a) and the non-strongly convex case λ2 = 0 (Figure 1b)). Ob-
jective Gap (left) means P (x) − P (x∗) for the output solution x and NNZs (right) means the
number of nonzeros in the output solution. SVRDA-x and SADA-x output the solution gener-

ated by the gradient descent updateexs, and SVRDA-v and SADA-v output the one generated
by the dual averaging updateevs. We do not report SVRDA-v and SADA-v for a non-strongly

convex regularizer, because it has no theoretical convergence guarantee. For a strongly convex
regularizer (top), UniVR, SVRDA-x, and SVRDA-v outperform other methods, as indicated by
the theories (see Table 1). Observe that the objective gaps of SVRDA-x and SVRDA-v (respec-
tively SADA-x and SADA-v) are very close though SVRDA-v (respectively SADA-v) has no
theoretical guarantee for convergence of the objective gap. Note that SVRDA-v (respectively
SADA-v) gives sparser solutions than SVRDA-x (respectively SADA-x) and the other methods.
For a non-strongly convex regularizer (bottom), SVRDA-x and SADA-x converge more quickly
than both UniVR and SAGA. The sparsity pattern of the output solutions of UniVR is unstable
and that of SAGA is very poor, because UniVR and SAGA need to average the history of the
solutions and the averaged solutions could be nonsparse. In contrast, SVRDA-x and SADA-x
show a nice sparsity recovery performance.

Figures 2a and 2b show the comparison of different methods on the Reuters-21578 data set
for different setups of λ1 and λ2 (the strongly convex case λ2 = 10−4 > 0 (Figure 2a) and the
non-strongly convex case λ2 = 0 (Figure 2b)). For a strongly convex regularizer, SVRG type al-
gorithms (SVRG, UniVR, SVRDA-x, and SVRDA-v) show nice convergence behavior whereas
SAGA type algorithms (SAGA, SADA-x, and SADA-v) show a slightly unstable behavior. Note
that the sparsity pattern of the output solution of SVRG is poor. For a non-strongly convex reg-

1Available at https://archive.ics.uci.edu/ml/datasets/Covertype.
2Available at http://www.cad.zju.edu.cn/home/dengcai/Data/TextData.html. We con-

verted the 65 class classiﬁcation task into a binary classiﬁcation.

3Available at http://www.causality.inf.ethz.ch/data/SIDO.html.

15

(a) λ1 = 10−6, λ2 = 10−6

(b) λ1 = 10−6, λ2 = 0

Figure 1: Comparison of different methods on covertype data set

ularizer, SVRDA-x and SADA-x converge more quickly but a bit more unstably than the other
methods. Observe that, when a new stage starts, SVRDA-x and SADA-x lead to a sharp increase
in the objective gap followed by a quick drop. This behavior can also be seen in the Multi-stage
ORDA [2]. We can see that the sparsity recovery performances of SVRDA-x and SADA-x are
very nice whereas that of UniVR is unstable and poor and that of SAGA is quite poor.

Figures 3a and 3b show the comparison of different methods on the sido0 data set for differ-
ent setups of λ1 and λ2 (the strongly convex case λ2 = 10−4 > 0 (Figure 3a) and the non-strongly
convex case λ2 = 0 (Figure 3b)). For a strongly convex regularizer, the performances of SAGA,
SADA-x, and SADA-v are among the best. Especially, SADA-v shows the best sparsity recovery
performance. Note that the sparsity recovery performance of SVRG is very poor. We can see
that the convergence of the NNZs of SVRDA-v (respectively SADA-v) is superior to SVRDA-x
(respectively SADA-x). For a non-strongly convex regularizer, SVRDA-x and SADA-x outper-
form both UniVR and SAGA. Especially, SVRDA-x and SADA-x show nice sparsity recovery

16

(a) λ1 = 10−4, λ2 = 10−4

(b) λ1 = 10−4, λ2 = 0

Figure 2: Comparison of different methods on Reuters-21578 data set

performances though the solutions of UniVR and SAGA are not sparse at all.

7 Conclusion and future work

In this paper, we proposed two stochastic gradient methods for regularized empirical risk min-
imization problems: SVRDA and SADA. We have shown that SVRDA and SADA achieve

µ(cid:17) log 1

O(cid:16)(cid:16)n + ¯L
regularizer and O(cid:16)nlog 1

convex regularizer.

ε(cid:17) and O(cid:16)(cid:16)n + Lmax

µ (cid:17) log 1
ε(cid:17) and O(cid:0)nlog 1

ε + ¯L

ε(cid:17)complexity, respectively, for a strongly convex
ε (cid:1) complexity, respectively, for a non-strongly

ε + Lmax

In numerical experiments, our methods led to better sparsity recovery than the existing meth-
ods for sparsity-inducing regularizers and showed nice convergence behaviors, especially for
non-strongly convex regularizers.

17

(a) λ1 = 10−4, λ2 = 10−4

(b) λ1 = 10−4, λ2 = 0

Figure 3: Comparison of different methods on sido0 data set

An interesting future work is to extend our methods to the alternating directional multiplier
method (ADMM) framework. In this paper, we assumed that the proximal mapping of R can be
efﬁciently computed. However, for structured regularization problems (for example, overlapped
group lasso, graph lasso, etc.), this assumption is generally not satisﬁed and our methods cannot
be directly applied. In contrast, ADMM can be applied to these problems without this assump-
tion. Suzuki [18] has proposed regularized dual averaging-ADMM (RDA-ADMM), which is
RDA [20] for ADMM in an online setting. Furthermore, Suzuki [19] has proposed stochastic
dual coordinate ascent-ADMM (SDCA-ADMM), which is SDCA [16, 15] for ADMM in regu-
larized an empirical risk minimization setting, and has shown that it converges exponentially for
a strongly convex regularizer. Applying SVRDA to the ADMM framework and showing linear
convergence for a strongly convex regularizer would be promising future work.

18

Acknowledgement

This work was partially supported by MEXT Kakenhi (25730013, 25120012, and 26280009),
JST-PRESTO and JST-CREST.

References

[1] Z. Allen-Zhu and Y. Yuan. Univr: A universal variance reduction framework for proximal

stochastic gradient method. arXiv preprint arXiv:1506.01972, 2015.

[2] X. Chen, Q. Lin, and J. Pena. Optimal regularized dual averaging methods for stochastic
In Advances in Neural Information Processing Systems, pages 395–403,

optimization.
2012.

[3] A. Cotter, O. Shamir, N. Srebro, and K. Sridharan. Better mini-batch algorithms via ac-
celerated gradient methods. In Advances in neural information processing systems, pages
1647–1655, 2011.

[4] A. Defazio, F. Bach, and S. Lacoste-Julien. Saga: A fast incremental gradient method with
support for non-strongly convex composite objectives. In Advances in Neural Information
Processing Systems, pages 1646–1654, 2014.

[5] O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction

using mini-batches. The Journal of Machine Learning Research, 13(1):165–202, 2012.

[6] E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex opti-

mization. Machine Learning, 69(2-3):169–192, 2007.

[7] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive vari-
ance reduction. In Advances in Neural Information Processing Systems, pages 315–323,
2013.

[8] Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical pro-

gramming, 120(1):221–259, 2009.

[9] Y. Nesterov.

Introductory lectures on convex optimization: A basic course, volume 87.

Springer Science & Business Media, 2013.

[10] A. Nitanda. Stochastic proximal gradient descent with acceleration techniques.

In Ad-

vances in Neural Information Processing Systems, pages 1574–1582, 2014.

[11] N. L. Roux, M. Schmidt, and F. R. Bach. A stochastic gradient method with an exponential
convergence rate for ﬁnite training sets. In Advances in Neural Information Processing
Systems, pages 2663–2671, 2012.

19

[12] M. Schmidt, N. L. Roux, and F. Bach. Minimizing ﬁnite sums with the stochastic average

gradient. arXiv preprint arXiv:1309.2388, 2013.

[13] M. Schmidt, R. Babanezhad, M. O. Ahmed, A. Defazio, A. Clifton, and A. Sarkar. Non-
uniform stochastic average gradient method for training conditional random ﬁelds. arXiv
preprint arXiv:1504.04406, 2015.

[14] S. Shalev-Shwartz and Y. Singer. Logarithmic regret algorithms for strongly convex re-

peated games. The Hebrew University, 2007.

[15] S. Shalev-Shwartz and T. Zhang. Accelerated mini-batch stochastic dual coordinate ascent.

In Advances in Neural Information Processing Systems, pages 378–385, 2013.

[16] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized

loss. The Journal of Machine Learning Research, 14(1):567–599, 2013.

[17] Y. Singer and J. C. Duchi. Efﬁcient learning using forward-backward splitting. In Advances

in Neural Information Processing Systems, pages 495–503, 2009.

[18] T. Suzuki. Dual averaging and proximal gradient descent for online alternating direction
multiplier method. In Proceedings of the 30th International Conference on Machine Learn-
ing (ICML-13), pages 392–400, 2013.

[19] T. Suzuki. Stochastic dual coordinate ascent with alternating direction method of multipli-
ers. In Proceedings of the 31st International Conference on Machine Learning (ICML-14),
pages 736–744, 2014.

[20] L. Xiao. Dual averaging method for regularized stochastic learning and online optimiza-

tion. In Advances in Neural Information Processing Systems, pages 2116–2124, 2009.

[21] L. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance

reduction. SIAM Journal on Optimization, 24(4):2057–2075, 2014.

20

A Proof of Theorem 5.1

In this section, we give the proof of Theorem 5.1. First we prove the following two easy lemmas.

Lemma A.1.

¯gt =

1
t

tXτ =1

gτ (t ≥ 1).

Proof. For t = 1, ¯g1 = g1 = 1
Assume that the claim holds for some t ≥ 1. Then

τ =1 gτ .

1

1

1P1
t + 1(cid:19) ¯gt +
t + 1(cid:19) 1
tXτ =1
t+1Xτ =1

gτ .

t

¯gt+1 =(cid:18)1 −
=(cid:18)1 −

=

1

t + 1

1

t + 1

gt+1 (by the deﬁnition)

gτ +

1

t + 1

gt+1 (by the assumption of the induction)

This ﬁnishes the proof for Lemma A.1.

Lemma A.2. For every x, u ∈ Rd,

F (u) + h∇F (u), x − ui + R(x) ≤ P (x) −

Proof. Since fi is Li-smooth, we have (see [9])

1
2 ¯L

1
n

1
nqi

nXi=1

||∇fi(x) − ∇fi(u)||2.

fi(u) + h∇fi(u), x − ui ≤ fi(x) −

1
2Li

||∇fi(x) − ∇fi(u)||2.

Summing this inequality from i = 1 to n and dividing it by n results in

F (u) + h∇F (u), x − ui ≤ F (x) −

1
2 ¯L

1
n

1
nqi

nXi=1

||∇fi(x) − ∇fi(u)||2.

Adding R(x) gives the desired result.

Next we prove the following main lemma.

21

Lemma A.3. For the sth stage of SVRDA,

E[P (xms) − P (x∗)] +

E||vms − x∗||2

η + msµ

2ms

t

ηt − ¯L

E||gt − ∇F (ut−1)||2 −

≤

1

2ms

msXt=1"

η

+

2ms

||v0 − x∗||2,

E" 1

n

1
¯L

nXi=1

||∇fi(ut−1) − ∇fi(x∗)||2##

1
nqi

where the expectations are conditioned on all previous stages.

Proof. First note that ut =(cid:0)1 − 1

t+1(cid:1) xt + 1

t+1vt for t ≥ 0 by the deﬁnition of u0. We deﬁne

ℓt(x) = F (ut−1) + h∇F (ut−1), x − ut−1i + R(x),
ˆℓt(x) = F (ut−1) + hgt, x − ut−1i + R(x).

Observe that ℓt ≤ P . For t ≥ 1, by Lemma A.1, we have

ˆℓτ (x) =

tXτ =1

tXτ =1

F (ut−1) +

tXτ =1

hgτ , x − uτ −1i +

R(x)

=ht¯gt, xi + tR(x) +

hgτ , uτ −1i

tXτ =1
tXτ =1

F (ut−1) −

tXτ =1
2 ||x − v0||2o.

and thus we have vt = argmin

ˆℓτ (x) + η

Also note that xt = argmin

τ =1

x∈Rd nPt
x∈Rd nˆℓτ (x) + ηt

2 ||x − ut−1||2o. Since F is ¯L-smooth, we have (see [9])

F (xt) ≤ F (ut−1) + h∇F (ut−1), xt − ut−1i +

¯L
2

||xt − ut−1||2,

and thus

P (xt) ≤ℓt(xt) +

=ˆℓt(xt) +

¯L
2
ηt
2

||xt − ut−1||2

||xt − ut−1||2 −

ηt − ¯L

2

||xt − ut−1||2 − hgt − ∇F (ut−1), xt − ut−1i.

Since xt is the minimizer of ˆℓt(x) + ηt

2 ||x − ut−1||2, we have

ˆℓt(xt)+

||xt − ut−1||2

ηt
2

≤ˆℓt(cid:18)(cid:18)1 −

1

t(cid:19) xt−1 +

1
t

vt(cid:19) +

ηt

2 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:18)1 −

1

t(cid:19) xt−1 +

1
t

2

vt − ut−1(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

22

and hence

P (xt) ≤ˆℓt(cid:18)(cid:18)1 −

1

t(cid:19) xt−1 +

1
t

vt(cid:19) +

ηt − ¯L

−

2

1

t(cid:19) xt−1 +

1
t

2

vt − ut−1(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

||xt − ut−1||2 − hgt − ∇F (ut−1), xt − ut−1i.

ηt

2 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:18)1 −
t(cid:1) xt−1 + 1

Using the convexity of ˆℓt and the facts that(cid:0)1 − 1

||xt − ut−1||2−hgt − ∇F (ut−1), xt − ut−1i ≤

t vt − ut−1 = 1

t (vt − vt−1) and

1

2(ηt − ¯L)

||gt − ∇F (ut−1)||2,

ηt − ¯L

2

−

we get

P (xt) ≤(cid:18)1 −
=(cid:18)1 −
≤(cid:18)1 −

+

1

1

t(cid:19) ˆℓt(xt−1) +
t(cid:19) ℓt(xt−1) +
t(cid:19) P (xt−1) +

1

1

2(ηt − ¯L)

1

+

2(ηt − ¯L)

ˆℓt(vt) +

ˆℓt(vt) +

1
t
1
t

η
2t
η
2t

||vt − vt−1||2 +

1

2(ηt − ¯L)

||gt − ∇F (ut−1)||2

||vt − vt−1||2

||gt − ∇F (ut−1)||2 +(cid:18)1 −
||gt − ∇F (ut−1)||2 +(cid:18)1 −

ˆℓt(vt) +

η
2t

1
t

1

t(cid:19) hgt − ∇F (ut−1), xt−1 − ut−1i
t(cid:19) hgt − ∇F (ut−1), xt−1 − ut−1i.

1

||vt − vt−1||2

Multiplying both sides of the above inequality by t, we have

tP (xt) ≤(t − 1)P (xt−1) + ˆℓt(vt) +

η
2

||vt − vt−1||2

+

t

2(ηt − ¯L)

||gt − ∇F (ut−1)||2 + (t − 1)hgt − ∇F (ut−1), xt−1 − ut−1i.

ˆℓτ (x) + η

2 ||x − v0||2 is η-strongly convex and vt−1 is the minimizer of

2 ||x − v0||2 for t ≥ 2, we have

By the fact thatPt−1
Pt−1

ˆℓτ (x) + η

τ =1

τ =1

ˆℓτ (vt−1) +

t−1Xτ =1

η
2

||vt−1 − v0||2 +

η
2

||vt − vt−1||2 ≤

ˆℓτ (vt) +

η
2

||vt − v0||2

t−1Xτ =1

23

τ =1 = 0). Using this inequality, we obtain

for t ≥ 1 (and, for t = 1, we deﬁneP0

tP (xt) −

ˆℓτ (vt) −

||vt − v0||2

tXτ =1

η
2

t−1Xτ =1

+ (t − 1)hgt − ∇F (ut−1), xt−1 − ut−1i.

Summing the above inequality from t = 1 to ms results in

≤(t − 1)P (xt−1) −

ˆℓτ (vt−1) −

η
2

||vt−1 − v0||2 +

t

2(ηt − ¯L)

||gt − ∇F (ut−1)||2

msP (xms)−

≤

ˆℓt(vms) −

t

2(ηt − ¯L)

η
2

||vms − v0||2

||gt − ∇F (ut−1)||2

msXt=1
msXt=1
msXt=1

+

(t − 1)hgt − ∇F (ut−1), xt−1 − ut−1i.

Using η + msµ-strongly convexity of the functionPms

of vms, we have

t=1

ˆℓt(x) + η

2 ||x − v0||2 and the optimality

ˆℓt(vms) +

η
2

||vms − v0||2 ≤

msXt=1

ˆℓt(x∗) +

η
2

msXt=1

||v0 − x∗||2 −

η + msµ

2

||vms − x∗||2

and hence

msP (xms)

≤

=

+

msXt=1
msXt=1
msXt=1
msXt=1
msXt=1

+

+

ˆℓt(x∗) +

η
2

||v0 − x∗||2 −

η + msµ

2

||vms − x∗||2

t

2(ηt − ¯L)

||gt − ∇F (ut−1)||2 +

ℓt(x∗) +

η
2

||v0 − x∗||2 −

η + msµ

2

(t − 1)hgt − ∇F (ut−1), xt−1 − ut−1i

msXt=1

||vms − x∗||2

t

2(ηt − ¯L)

||gt − ∇F (ut−1)||2 +

msXt=1

hgt − ∇F (ut−1), x∗ − ut−1i.

(t − 1)hgt − ∇F (ut−1), xt−1 − ut−1i

24

By Lemma A.2 with x = x∗ and u = ut−1, we have

ℓt(x∗) ≤ P (x∗) −

1
2 ¯L

1
n

1
nqi

nXi=1

||∇fi(x∗) − ∇fi(ut−1)||2.

Applying this inequality to the above inequality yields

msP (xms)

≤msP (x∗) +

+

+

1
2

msXt=1"
msXt=1

1
nqi

nXi=1
msXt=1

η
2

||v0 − x∗||2 −

η + msµ

2

||vms − x∗||2

t

ηt − ¯L

||gt − ∇F (ut−1)||2 −

1
¯L

1
n

||∇fi(x∗) − ∇fi(ut−1)||2#

(t − 1)hgt − ∇F (ut−1), xt−1 − ut−1i +

hgt − ∇F (ut−1), x∗ − ut−1i.

Dividing this inequality by ms results in

P (xms)

≤P (x∗) +

2ms

msXt=1"
msXt=1

1

2ms

1
ms

+

+

η

||v0 − x∗||2 −

η + msµ

2ms

||vms − x∗||2

t

ηt − ¯L

||gt − ∇F (ut−1)||2 −

1
¯L

1
n

(t − 1)hgt − ∇F (ut−1), xt−1 − ut−1i +

1
nqi

nXi=1

1
ms

||∇fi(x∗) − ∇fi(ut−1)||2#
msXt=1

hgt − ∇F (ut−1), x∗ − ut−1i.

Taking the expectation on both sides yields

E[P (xms) − P (x∗)] +

E||vms − x∗||2

η + msµ

2ms

≤

1

2ms

t

ηt − ¯L

E||gt − ∇F (ut−1)||2 −

msXt=1"

η

+

2ms

||v0 − x∗||2.

E" 1

n

1
¯L

nXi=1

||∇fi(x∗) − ∇fi(ut−1)||2##

1
nqi

Here we used the fact that E[gt − ∇F (ut−1)] = 0 for t = 1, . . . , ms. This ﬁnishes the proof of
Lemma A.3.

Now we need the following lemma.

Lemma A.4. For every x ∈ Rd,

1
n

nXi=1

1
nqi

||∇fi(x) − ∇fi(x∗)||2 ≤ 2 ¯L(P (x) − P (x∗) −

µ
2

||x − x∗||2).

25

Proof. From the argument of the proof of Lemma A.2, we have

1
n

nXi=1

1
nqi

||∇fi(x) − ∇fi(x∗)||2 ≤ 2 ¯L(F (x) − h∇F (x∗), x − x∗i − F (x∗)).

By the optimality of x∗, there exists ξ∗ ∈ ∂R(x∗) such that ∇F (x∗) + ξ∗. Then using µ-strong
convexity of R, we get

−h∇F (x∗), x − x∗i = hξ∗, x − x∗i ≤ R(x) − R(x∗) −

and hence

1
n

nXi=1

1
nqi

||∇fi(x) − ∇fi(x∗)||2 ≤ 2 ¯L(P (x) − P (x∗) −

µ
2

µ
2

||x − x∗||2

||x − x∗||2).

Proof of Theorem 5.1. We bound the term E||gt − ∇F (ut−1)||2:

E||gt − ∇F (ut−1)||2

=E(cid:2)Eit(cid:2)||(∇fit(ut−1) − ∇fit(x0))/nqit + ∇F (x0) − ∇F (ut−1)||2|i1, . . . , it−1(cid:3)(cid:3)
≤E(cid:2)Eit(cid:2)||(∇fit(ut−1) − ∇fit(x0))/nqit||2|i1, . . . , it−1(cid:3)(cid:3)
=E" 1
nXi=1
≤3E" 1
nXi=1

||∇fi(ut−1) − ∇fi(x0)||2#
||∇fi(ut−1) − ∇fi(x∗)||2# +

||∇fi(x0) − ∇fi(x∗)||2# .

nXi=1

3

2" 1

n

n

n

1
nqi

1
nqi

1
nqi

Combining this inequality with Lemme A.4, we get

E||gt − ∇F (ut−1)||2 ≤3E" 1

n

||∇fi(ut−1) − ∇fi(x∗)||2#

1
nqi

nXi=1

+ 3 ¯L(P (x0) − P (x∗) −

µ
2

||x0 − x∗||2).

Since η = 1

4 ¯L, using the inequality

t

ηt − ¯L

≤

1
3 ¯L

(∀t ≥ 1),

(8)

26

by Lemma A.3 we obtain

E[P (xms) − P (x∗)] +

η + msµ

2ms

E||vms − x∗||2

≤

1
2

(P (x0) − P (x∗) −

µ
2

||x0 − x∗||2) +

η

2ms

||v0 − x∗||2.

Since xms =exs, vms =evs, x0 =exs−1, and v0 = (1 − α)evs−1 + αexs−1, we have

η + msµ

≤

1
2

2ms

E[P (exs) − P (x∗)] +
(P (exs−1) − P (x∗)) +(cid:18) αη

2ms

µ

E||evs − x∗||2
4(cid:19) ||exs−1 − x∗||2 +

−

(1 − α)η

2ms

||evs−1 − x∗||2.

Finally, taking expectations with respect to all previous stages gives the desired result.

B Proof of Theorem 5.4

In this section, we give the proof of Theorem 5.4.

Lemma B.1. For the sth stage of SADA,

E[P (xms) − P (x∗)] +

E||vms − x∗||2

η + msµ

2ms

≤

1

2ms

t

ηt − Lmax

E||gt − ∇F (ut−1)||2 −

msXt=1"

η

+

2ms

||v0 − x∗||2.

E" 1

n

1

Lmax

nXi=1

||∇fi(ut−1) − ∇fi(x∗)||2##

The proof of Lemma B.1 is identical to the proof of Lemma A.3 and we omit it.

Proof of Theorem 5.4. First we bound the term E||gt − ∇f (ut−1)||2:

1
n

it ) +

E||gt − ∇F (ut−1)||2 =E"Eit"||∇fit(ut−1) − ∇fit(φt−1
≤E(cid:2)Eit(cid:2)||∇fit(ut−1) − ∇fit(φt−1
=E" 1
nXi=1
≤2E" 1
nXi=1

it )||2|i1, . . . , it−1(cid:3)(cid:3)
||(∇fi(ut−1) − ∇fi(x∗))||2# + 2E" 1

nXi=1
))||2#

||(∇fi(ut−1) − ∇fi(φt−1

n

n

n

i

nXi=1

∇fi(φt−1

i

) − ∇F (ut−1)||2|i1, . . . , it−1##

||(∇fi(φt−1

i

) − ∇fi(x∗))||2# .

27

) − ∇fi(x∗))||2(cid:21)#

i

||(∇fi(φt−2

i

n

) − ∇fi(x∗))||2#

i

) − ∇fi(x∗))||2(cid:3) for t ≥ 1:

n

i

i

n

i

n

n

= · · ·

=

1

1

1

1

n

n

=

1
n

||(∇fi(φt−1

||(∇fi(φt−1

i=1 ||(∇fi(φt−1

) − ∇fi(x∗))||2|i1, . . . , it−2##
) − ∇fi(x∗))||2|i1, . . . , it−2(cid:3)#
n(cid:19) ||(∇fi(φt−2
n(cid:19) E" 1
nXi=1
||(∇fi(uj−1) − ∇fi(x∗))||2#
i ) − ∇fi(x∗))||2# .

Next we bound the term E(cid:2) 1
nPn
E" 1
) − ∇fi(x∗))||2#
nXi=1
=E"Eit−1" 1
nXi=1
=E" 1
nXi=1
Eit−1(cid:2)||(∇fi(φt−1
=E" 1
nXi=1(cid:20) 1
||(∇fi(ut−2) − ∇fi(x∗))||2 +(cid:18)1 −
||(∇fi(ut−2) − ∇fi(x∗))||2# +(cid:18)1 −
E" 1
nXi=1
E" 1
n(cid:19)t−1−j
n(cid:18)1 −
t−1Xj=1
n(cid:19)t−1" 1
+(cid:18)1 −
nXi=1
Here we deﬁnedP0
msXt=1"
msXt=1" 1

Using these inequalities and the deﬁnition of η, we have

j=1 = 0 for t = 1.

||(∇fi(φt−1

E||gt − ∇F (ut−1)||2 −

E" 1
nXi=1
) − ∇fi(x∗))||2# −
E" 1
nXi=1
E" 1
msXt=1" t−1Xj=1
n(cid:18)1 −
n(cid:19)t−1−j
n(cid:19)t−1" 1
msXt=1(cid:18)1 −
nXi=1
||∇fi(ut−1) − ∇fi(x∗)||2# .
E" 1
nXi=1
msXt=1

nXi=1

||(∇fi(φ0

nXi=1

||(∇fi(φ0

ηt − Lmax

+

−

2Lmax

2Lmax

2Lmax

2Lmax

=

=

Lmax

n

1

1

1

t

n

1

i

1

1

n

n

n

n

n

1

1

||∇fi(ut−1) − ∇fi(x∗)||2##

E" 1

1

n

2Lmax

nXi=1
||(∇fi(uj−1) − ∇fi(x∗))||2##
i ) − ∇fi(x∗))||2#

||∇fi(ut−1) − ∇fi(x∗)||2##

28

Observe that

=

≤

n

1

1

1

1

1

1

n

+

||(∇fi(uj−1) − ∇fi(x∗))||2##

msXt=1" t−1Xj=1
n(cid:18)1 −
msXt=2
n(cid:18)1 −
msXt=3
n(cid:18)1 −
msXt=ms
E" 1
msXt=1
nXi=1

E" 1
n(cid:19)t−1−j
n(cid:18)1 −
nXi=1
||(∇fi(u0) − ∇fi(x∗))||2#
E" 1
n(cid:19)t−2
nXi=1
||(∇fi(u1) − ∇fi(x∗))||2#
E" 1
n(cid:19)t−3
nXi=1
||(∇fi(ums−2) − ∇fi(x∗))||2#
E" 1
n(cid:19)t−ms
nXi=1
||∇fi(ut−1) − ∇fi(x∗)||2# .

+ · · ·

+

n

1

n

n

1

By Lemma A.4 and the deﬁnition of φ0

i , we get

msXt=1(cid:18)1 −

1

n(cid:19)t−1" 1

n

nXi=1

=2msLmax(P (x0) − P (x∗) −

||(∇fi(φ0

i ) − ∇fi(x∗))||2#

µ
2

||x0 − x∗||2).

Hence we get

t

ηt − Lmax

msXt=1"
≤ms(cid:16)P (x0) − P (x∗) −

Combining Lemma B.1 with this result yields

E||gt − ∇F (ut−1)||2 −

µ
2

||x0 − x∗||2(cid:17) .

E" 1

n

1

Lmax

nXi=1

||∇fi(ut−1) − ∇fi(x∗)||2##

E[P (xms) − P (x∗)] +

E||vms − x∗||2

η + msµ

2ms

≤

1

2(cid:16)P (x0) − P (x∗) −

µ
2

||x0 − x∗||2(cid:17) +

η

2ms

||v0 − x∗||2.

29

η + msµ

Since xms =exs, vms =evs, x0 =exs−1, and v0 = (1 − α)evs−1 + αexs−1, we obtain
E||evs − x∗||2
4(cid:19) ||exs−1 − x∗||2

E[P (exs) − P (x∗)] +
(P (exs−1) − P (x∗)) +(cid:18) αη
||evs−1 − x∗||2.

Finally, taking expectations with respect to all previous stages gives the desired result.

≤

1
2

µ

−

+

(1 − α)η

2ms

2ms

2ms

30

