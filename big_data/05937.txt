How to Combine a Billion Alphas

Zura Kakushadze§†1 and Willie Yu♯2

§ Quantigicr Solutions LLC

1127 High Ridge Road #135, Stamford, CT 06905 3

† Free University of Tbilisi, Business School & School of Physics

240, David Agmashenebeli Alley, Tbilisi, 0159, Georgia

♯ Centre for Computational Biology, Duke-NUS Medical School

8 College Road, Singapore 169857

(February 27, 2016)

Abstract

We give an explicit algorithm and source code for computing optimal
weights for combining a large number N of alphas. This algorithm does
not cost O(N 3) or even O(N 2) operations but is much cheaper, in fact, the
number of required operations scales linearly with N . We discuss how in
the absence of binary or quasi-binary “clustering” of alphas, which is not
observed in practice, the optimization problem simpliﬁes when N is large.
Our algorithm does not require computing principal components or inverting
large matrices, nor does it require iterations. The number of risk factors it
employs, which typically is limited by the number of historical observations,
can be sizably enlarged via using position data for the underlying tradables.

6
1
0
2

 
r
a

 

M
8
1

 
 
]

.

M
P
n
i
f
-
q
[
 
 

1
v
7
3
9
5
0

.

3
0
6
1
:
v
i
X
r
a

1 Zura Kakushadze, Ph.D., is the President of Quantigicr Solutions LLC, and a Full Professor

at Free University of Tbilisi. Email: zura@quantigic.com

2 Willie Yu, Ph.D., is a Research Fellow at Duke-NUS Medical School. Email: willie.yu@duke-

nus.edu.sg

3 DISCLAIMER: This address is used by the corresponding author for no purpose other than
to indicate his professional aﬃliation as is customary in publications. In particular, the contents
of this paper are not intended as an investment, legal, tax or any other such advice, and in no
way represent views of Quantigicr Solutions LLC, the website www.quantigic.com or any of their
other aﬃliates.

1

Introduction and Summary

Now that machines have taken over alpha4 mining, the number of available alphas is
growing exponentially. On the ﬂip side, these “modern” alphas are ever fainter and
more ephemeral. To mitigate this eﬀect, among other things, one combines a large
number of alphas and trades the so-combined “mega-alpha”. And this is nontrivial.
Why? It is important to pick the alpha weights optimally, i.e., to optimize the
return, Sharpe ratio and/or other performance characteristics of this alpha portfolio.
The commonly used techniques in optimizing alphas are conceptually similar to the
mean-variance portfolio optimization [Markowitz, 1952] or Sharpe ratio maximiza-
tion [Sharpe, 1994] for stock portfolios. However, there are some evident diﬀerences.5
The most prosaic diﬀerence is that the number of alphas can be huge, in hundreds
of thousands, millions or even billions. The available history (lookback), however,
naturally is much shorter. This has implications for determining the alpha weights.
Let us look at vanilla Sharpe ratio maximization of the alpha portfolio with
weights wi, i = 1, . . . , N, where N is the number of alphas.6 The optimal weights
are given by

wi = η

C−1

ij Ej

(1)

NXj=1

where Ej are the expected returns for our alphas, C−1
ij
return covariance matrix Cij, and η is the normalization coeﬃcient such that

is the inverse of the alpha

NXi=1

|wi| = 1

(2)

If we compute Cij as a sample covariance matrix based on a time series of realized
returns (see (3)), it is badly singular as the number of observations is much smaller
than N. This also happens in the case of stock portfolios. In that case one either
builds a proprietary risk model to replace Cij or opts for a commercially available
(multifactor) risk model. In the case of alphas the latter option is simply not there.
So, what is one to do? We can try to build a risk model for alphas following a
rich experience with risk models for stocks. In the case of stocks a more popular
approach is to combine style risk factors (i.e., those based on measured or estimated
properties of stocks, such as size, volatility, value, etc.) and industry risk fac-
tors (i.e., those based on stocks’ membership in sectors, industries, sub-industries,

4 Here “alpha” – following the common trader lingo – generally means any reasonable “expected
return” that one may wish to trade on and is not necessarily the same as the “academic” alpha.
In practice, often the detailed information about how alphas are constructed may not even be
available, e.g., the only data available could be the position data, so “alpha” then is a set of
instructions to achieve certain stock (or some other instrument) holdings by some times t1, t2, . . .
5 In the olden days the alpha weights would have to be nonnegative. In many practical appli-

cations this is no longer the case as only the “mega-alpha” is traded, not the individual alphas.
6 With no position bounds, trading costs, etc. – these do not aﬀect the point we make here.

1

etc., depending on the nomenclature used by a particular industry classiﬁcation
employed). The number of style factors is limited, of order 10 for longer-horizon
models, and about 4 for shorter-horizon models. In the case of stocks, at least for
shorter-horizon models, it is the ubiquitous industry risk factors (numbering in a
few hundred for a typical liquid trading universe) that add most value. However,
there is no analog of the (binary or quasi-binary) industry classiﬁcation for alphas.
In practice, for many alphas it is not even known how they are constructed, only the
(historical and desired) positions are known. Even formulaic alphas [Kakushadze,
2015d] are mostly so convoluted that realistically it is impossible to classify them
in any meaningful way, at least not such that the number of the resulting (binary
or quasi-binary)7 “clusters” would be numerous enough to compete with principal
components (see below).8 And there are only a few a priori relevant style factors for
alphas [Kakushadze, 2014] to compete with the principal components.

Just as in the case of stocks, we can resort to the principal components of the
sample covariance (or correlation) matrix to build our multifactor risk model for
alphas. This is where one of our key observations comes in. As we discuss in detail
below, irrespective of how a factor model for Cij is built, in the absence – which
we assume based on our discussion above – of (binary or quasi-binary) “clustering”,
when the number of alphas N is large, the optimization (1) invariably reduces to
a (weighted) regression! This also holds for any reasonably realistic deformation
(e.g., shrinkage [Ledoit and Wolf, 2004]) of the sample covariance matrix for such
deformations can be viewed as multifactor risk models.
I.e., there is no need to
construct a full-blown risk model and compute the factor covariance matrix or even
the speciﬁc (idiosyncratic) risk.9 So, as the simplest variant, we construct the factor
loadings matrix from the ﬁrst M principal components of the sample covariance
matrix corresponding to its positive eigenvalues and regress expected returns for
alphas over this factor loadings matrix with the regression weights given by the
inverse sample variances Cii = σ2
i . However, it turns out that we do not even need
to calculate the principal components thereby further reducing computational cost.
Here is a simple prescription for obtaining the weights wi. Start with a time se-
ries Ris of realized alpha returns (s = 1, . . . , M +1 labels the times ts). Calculate the
sample variances Cii = σ2
i (but not the sample correlations). This costs O(MN) op-
and serially. Let us call the so-demeaned returns Yis. Take only M − 1 columns in
Yis (e.g., the ﬁrst M − 1 columns). Take the expected returns Ei for the alphas and
7 By binary “clusters” we mean that each alpha would belong to one and only one “cluster”.
By quasi-binary clusters we mean that this would be mostly the case but a (small) fraction of
alphas could possibly belong to multiple (at most several) “clusters”. This is analogous to binary
and quasi-binary (i.e., where we have some conglomerates belonging to multiple industries, sub-
industries, etc., depending on the naming conventions) industry classiﬁcations in the case of stocks.
8 There is also the issue of stability. Stocks rarely, if ever, jump industries rendering well-
constructed industry classiﬁcations quite stable. However, alphas being ephemeral objects make
poor candidates for being classiﬁed into any kind of stable binary or quasi-binary “clusters”.

erations. Normalize the returns via eRis = Ris/σi. Demean eRis both cross-sectionally

9 More precisely, we can do that, but in the 0th – and very good – approximation we need not.

2

normalize them via eEi = Ei/σi. Regress eEi over the N × (M − 1) matrix Yis with
residuals eεi of this regression. Then the optimal weights wi = ηeεi/σi, where η is

unit weights and no intercept. This regression costs O(M 2N) operations. Take the
ﬁxed via (2). If the reader is only interested in the prescription and the source code,
then the reader can go straight to Appendix A, where we give R source code for
this algorithm,10 and skip the rest of the paper. However, if the reader would like
to understand why this algorithm makes sense and also, among other things, how
to potentially increase the number of risk factors from M (for a generous 1 year
lookback M ≈ 250) to a few thousand, then the reader may wish to keep reading.
To summarize, calculating the weights wi does not require computing any prin-
cipal components or inverting any large matrices, and it costs only O(M 2N) oper-
ations, so it is linear in N. Furthermore, the algorithm is not iterative, so there are
no convergence issues to worry about. Perhaps somewhat ironically, the simplicity
of this algorithm is rooted in the very nature of this problem, that we have a small
number of observations compared with the large (in fact, huge) number of alphas.
As we discuss in more detail in the subsequent sections, there is simply not much
else one can do that would be out-of-sample stable with the exception of enlarging
the number of risk factors provided there is additional information available to us.
In Section 2 we set up
our framework and notations.
In Section 3 we discuss why, absent “clustering”,
optimization using a factor model reduces to a regression in the large N limit. In
Section 4 we discuss why this also applies to deformations of the sample covariance
matrix as they too reduce to factor models. We also discuss why style factors add
little value and how to enlarge the number of risk factors based on more detailed
position data (as opposed to the historical alpha return data). In Section 5 we discuss
a reﬁnement whereby the analog of the “market” mode for stocks is factored out to
improve performance of the alpha portfolio. We also give the detailed algorithm for
obtaining the optimal alpha weights and discuss the computational cost (including
why it is cheaper than doing principal components). We brieﬂy conclude in Section
6. Appendix A contains the source code for our algorithm. Appendix B contains
some legalese. Parts of Sections 3 and 4 are based on [Kakushadze and Yu, 2016b].

The remainder of this paper is organized as follows.

2 Sample Covariance Matrix

So, we have N time series of returns. A priori these returns can correspond to stocks
or some other instruments, alphas, etc. Here we will be general and refer to them
simply as returns, albeit we will make some assumptions about these returns below.
Each time series contains M + 1 observations corresponding to times ts, and we will

10 R Package for Statistical Computing, http://www.r-project.org. The source code given in
Appendix A is not written to be “fancy” or optimized for speed or in any other way. Its sole purpose
is to illustrate the algorithms described in the main text in a simple-to-understand fashion. Some
legalese relating to this code is given in Appendix B.

3

denote our returns as Ris, where i = 1, . . . , N and s = 1, . . . , M, M + 1 (t1 is the
most recent observation). The sample covariance matrix (SCM) is given by11

Cij =

1
M

M +1Xs=1

Xis Xjs

(3)

s=1 Ris.

M +1PM +1

where Xis = Ris − Ri are serially demeaned returns; Ri = 1

We are interested in cases where M < N, in fact, M ≪ N. When M < N, Cij is
s=1 Xis = 0, so only M columns of the matrix Xis are linearly
s=1 Xis. Then we can

singular: we havePM +1
independent. Let us eliminate the last column: Xi,M +1 = −PM

express Cij via the ﬁrst M columns:

Cij =

MXs,s′=1

Xis φss′ Xjs′

(4)

Here φss′ = (δss′ + usus′) /M is a nonsingular M × M matrix (s, s′ = 1, . . . , M);
us ≡ 1 is a unit M-vector. Note that φss′ is a 1-factor model (see below).
The challenge is to either deform Cij such that it is nonsingular, or to replace it
with a constructed nonsingular matrix Γij such that it reasonably approximates Cij
in-sample and predicts it out-of-sample. Let us ﬁrst discuss the latter approach.

3 Factor Models

A popular method – at least in the case of equities – for constructing a nonsingular
replacement Γij for Cij is via a factor model:12

Γij = ξ2

i δij +

KXA,B=1

ΩiA ΦAB ΩjB

(5)

Here: ξi is the speciﬁc (a.k.a. idiosyncratic) risk for each return; ΩiA is an N × K
factor loadings matrix; and ΦAB is a K×K factor covariance matrix (FCM), A, B =
1, . . . , K. The number of factors K ≪ N to have FCM more stable than SCM.
The nice thing about Γij is that it is positive-deﬁnite (and therefore invertible)
if FCM is positive-deﬁnite and all ξ2
i > 0. For our purposes here it is convenient to
rewrite Γij via Γij = ξi ξj γij, where

γij = δij +

βiA βjA

KXA=1

(6)

11 The overall normalization of Cij does not aﬀect the weights wi in (1), so the diﬀerence between
the unbiased estimate with M in the denominator vs. the maximum likelihood estimate with M +1
in the denominator is immaterial for our purposes here. Also, in most applications M ≫ 1.
12 For equity multifactor models, see, e.g., [Grinold and Kahn, 2000] and references therein. A
multifactor model approach for alphas was set forth and discussed in detail in [Kakushadze, 2014].

4

i=1 β2

i=1 β2

and

(7)

(8)

wi =

η
ξi

γ−1
ij

η

=

Ej
ξj

ξi −

Ej

ξj#

NXj=1

KXA,B=1

βiA Q−1

AB βjB

i=1 zi eβiA eβiB

and βiA = eβiA/ξi. Here (in matrix notations) eβ = Ω eΦ, and eΦ is the Cholesky
decomposition of Φ, so eΦ eΦT = Φ. The weights (1) (with Cij replaced by Γij) are

given by

where εi are the residuals of the cross-sectional weighted regression13 of Ei over

i=1 βiA βiB. The diagonal elements of this
iA ≫ 1,

ξi"Ei
NXj=1
where QAB = δAB + qAB, and qAB =PN
iA. It then follows that, if all qAA = PN
matrix are QAA = 1 +PN
which we will argue to be the case momentarily, then QAB ≈ qAB =PN
AB eβjB zj Ej# = η zi εi
wi ≈ η zi"Ei −
NXj=1
KXA,B=1eβiA q−1
eβiA with the weights zi = 1/ξ2
i , or, equivalently, eεi = εi/ξi are the residuals of the
unit-weighted regression of eEi = Ei/ξi over βiA. So, (1) reduces to a regression.
The question is why – or, more precisely, when – all qAA ≫ 1. This is the case
when: i) N is large, and ii) there is no “clustering” in the vectors βiA. That is, we
do not have vanishing or small values of β2
iA for most values of the index i with only
a small subset thereof having β2
iA ∼> 1. Without “clustering”, to have qAA ∼< 1, we
would have to have β2
iA ≪ 1, i.e., γij and consequently Γij would be almost diagonal.
E.g., consider a 1-factor model (K = 1) with uniform βi ≡ β. In this model we
have uniform pair-wise correlations ρ = β2/(1 + β2). For these correlations not to be
small, we must have β2
∼> 1. Now, Q = 1 + q, where q = Nβ2. For large N we have
q ≫ 1, Q ≈ q, and in this case we have a weighted regression over the intercept.
So, absent “clustering”, when N is large, the factor model is only useful to the
extent of deﬁning the regression weights zi via the speciﬁc risks ξi. Thus, FCM does
not aﬀect the regression residuals: they are invariant under linear transformations
of βiA, (in matrix notations) β → β U, where UAB is a general nonsingular matrix.
What about “clustering”? For a large number of alphas trading largely overlap-
ping universes (e.g., top 2,500 most liquid U.S. stocks) there is no “clustering” so
long as they cannot be classiﬁed in a binary fashion as in industry classiﬁcations for
stocks, and such a classiﬁcation of alphas usually is not possible. Any risk factors
then lack “clustering” and are analogous to style factors or principal components.

4 Deformed Sample Covariance Matrix

Let us now discuss deforming – or regularizing – SCM such that it is nonsingular.
One method often used in this regard is the so-called shrinkage [Ledoit and Wolf,

13 Without the intercept unless it is subsumed in a linear combination of the columns of eβiA.

5

2004]. It is often regarded as an “alternative” to multifactor risk models. However,
as was recently discussed in [Kakushadze, 2016], shrunk SCM is also a factor model.
In fact, shrinkage is a special case of more general deformations, where instead

of SCM Cij given by (4), one uses

eCij = ∆ij +

MXs,s′=1

Xis eφss′ Xjs′

(9)

Here the matrix ∆ij is assumed to be positive-deﬁnite and (relatively) stable out-

The issue with (9) is that:

i) in practice ∆ij must have some relevance to the
underlying returns whose covariance matrix we are attempting to model; and ii) a

of-sample. We must have eCii = Cii, so this imposes N conditions on ∆ii. A priori
∆ij can be otherwise arbitrary. The matrix eφss′ is some deformation of φss′ in (4).14
priori it is unclear what the deformed matrix eφss′ should be. The available data is

limited to the N × M matrix Xis, and the matrix φss′, which is ﬁxed. To go beyond
this data, we invariably must introduce some additional input. As a 12th century
Georgian poet Shota Rustaveli put it, “What’s in the jar is what will ﬂow out.”15

However, not all is lost. The fact that we have large N (and no “clustering”)
simpliﬁes things. In practice, the matrix ∆ij cannot be arbitrary. It must be some-
how – be it directly or indirectly – related to the returns whose covariance matrix
we are after. The simplest choice is a diagonal matrix ∆ij = Di δij. More generally,
we can take ∆ij to be a K-factor model of the form (5), ∆ij = Γij, with a properly
chosen diagonal Γii. In fact, realistically, what else can ∆ij be in practice? If we
knew how to write down a non-factor-model covariance matrix that approximates
SCM well and is out-of-sample stable, this paper would have been very diﬀerent!

So, assuming ∆ij is a K-factor model (with K = 0 corresponding to a diagonal

∆ij) given by (5), the deformed matrix eCij is also a factor model. Indeed,

(10)

i δij +

eCij = ξ2

K+MXα,β=1bΩiα bΦαβ bΩjβ

Here: the index α = (A, s) takes K + M values; bΩiA = ΩiA;bΩis = Xis, s = 1, . . . , M;
bΦAB = ΦAB, A, B = 1, . . . , K; bΦss′ = eφss′, s, s′ = 1, . . . , M; and bΦAs ≡ 0.
14 In shrinkage, when translated into our language here, one simply takes eφss′ = (1 − ζ) φss′

and (“shrinkage target”) ∆ij = ζ Γij , where the weight (“shrinkage constant”) 0 ≤ ζ ≤ 1, and Γij
(usually chosen as a diagonal matrix or a K-factor model with low K) is such that Γii = Cii.
15 This is ZK’s own translation of an aphorism from a stanza in Rustaveli’s sole known epic
poem whose title is erroneously translated as “The Knight in the Panther’s Skin” (or similar). In
ZK’s humble opinion, not only is it the greatest masterpiece of the Georgian literature, but one
of the greatest literary writings of all time. It consists of over 1,600 perfectly rhymed shairi or
Rustavelian quatrains all containing 16 = 8+8 syllables per line with a caesura between the 8th and
9th syllables. How a human brain can come up with such perfection is mind-boggling, especially
considering that this poem tells an extremely complex story complete with dialogs, aphorisms, etc.

6

Now we are in good shape. Indeed, assuming large N and no “clustering”, we

know that optimization using eCij (instead of Cij) in (1) – a factor model – reduces to
matrix bΩiα, irrespective of FCM ΦAB or the deformed matrix eφss′, and the latter we

a weighted regression of the expected returns over the N × (K + M) factor loadings
do not even have a (constrained enough) guiding principle for computing. All we
need is to somehow compute the regression weights zi = 1/ξ2
i , i.e., the speciﬁc risks.

4.1 What about Regression Weights?

The speciﬁc risks follow from (10). However, to compute them, we do need to know

ΦAB and eφss′. Indeed, recalling that eCii = Cii, we have16

ξ2
i = Cii −

ΩiA ΦAB ΩiB −

KXA=1

MXs=1

Xis eφss′ Xis′

(11)

i = Cii is equivalent to setting ξ2

unknowns. This system may appear to be overconstrained for large enough N, but

parameters to play with and not much guidance to play the game. In fact, there is
no magic bullet here. Simplicity is essentially the only beacon we can follow...

Since at the end we have a weighted regression, which in itself does not require
i = Cii.
This might appear to contradict (11), but it does not. This is because the residuals
εi are invariant under the rescalings of the weights zi → λ zi, where λ > 0. So,
setting ξ2
i = ζ Cii, where 0 < ζ < 1, which simply

So, as far as the deformed matrix eφss′ is concerned, a priori we have M(M + 1)/2
knowing ΦAB or eφss′ provided we know the weights, we can simply take ξ2
puts N conditions on K(K + 1)/2 (from ΦAB) plus M(M + 1)/2 (from eφss′) a priori
there always exists a “solution”:17 we can simply take K = 0 andeφss′ = (1 − ζ) φss′.
answer is yes. Here is a simple prescription. As above we can set eφss′ = (1 − ζ) φss′,

but take Γij to be a nontrivial factor model (K > 0). We must have Γii = ζ Cii.
Generally, ξ2
i need not equal rescaled Cii. There is a notable exception: if we take18
Γij to have a uniform correlation matrix. Let the correlation be ρ. Then we have
In this case
Γij = ζσiσj [(1 − ρ) δij + ρuiuj], where ui ≡ 1 is the unit N-vector.
we have ξ2
i = ζ (1 − ρ) Cii. So, in this 1-factor model the weights are the same as
the inverse sample variances, albeit the regression is over M + 1 columns.19 If we
take a diﬀerent factor model (even a 1-factor model with nonuniform correlations),
generally ξ2

So, can we have weights other than the inverse sample variances? A priori the

i do not equal rescaled Cii. So, what should/can the risk factor(s) be?

16 Nontrivial algorithms are required to ensure that all ξ2

i so computed are positive and consistent
with FCM. Such algorithms and source code are given in [Kakushadze and Yu, 2016a] (see below).

17 This is shrinkage with a diagonal “shrinkage target” (see footnote 14).
18 As in [Ledoit and Wolf, 2004].
19 To wit, the M columns in Xis, s = 1, . . . , M , plus a single column equal σi. Usually, there is
a high correlation between the latter and a linear combination of the former (see below). In fact,
we will argue below that the factor proportional to σi should be taken out altogether, i.e., removed

from the factor loadings matrix bΩiα, irrespective of how the latter is constructed (see Section 5).

7

4.2 Candidates for Additional Risk Factors

Since we are assuming no “clustering”, i.e., there is no binary classiﬁcation we
can construct for our returns,20 a priori there are two evident choices for what
the additional K risk factors can be: i) principal components and ii) style factors
(analogous to those in equity risk models). Below we will discuss a 3rd possibility.

4.2.1 Principal Components

i

i

i

λ(a) V (a)

i δij + σiσj

KXa=1

The idea is to take the ﬁrst K < M principal components of SCM Cij as ΩiA. More
precisely, there is another choice, to wit, to take ΩiA = σi V (A)
, A = 1, . . . , K,
where V (a)
, a = 1, . . . , N, are the principal components of the sample correlation
matrix Ψij = Cij/σiσj. Typically, the diﬀerence between the two choices is not
make-it-or-break-it, with the latter preferred (and usually producing better results)
as it factors out the (skewed, quasi-log-normally distributed) volatility σi and deals
with the principal components of Ψij, whose oﬀ diagonal elements take values in the
interval (−1, 1) and have a tight distribution. So, we will adapt this approach here.

As above, eφss′ = (1 − ζ) φss′, we take ΦAB = ζ λ(A) δAB, so our deformed SCM21
eCij = ξ2

V (a)
j + σiσj (1 − ζ)

MXa=K+1

a=K+1 λ(a) [V (a)

Here: λ(a) are the eigenvalues corresponding to the principal components V (a)
(λ(1) ≥
]2.
λ(2) ≥ · · · ≥ λ(M ), and λ(a) ≡ 0 for a > M); and ξ2
In terms of the regression, this construction only aﬀects the regression weights
Indeed, the regression over the ﬁrst M principal components V (a)
zi = 1/ξ2
i .
,
a = 1, . . . , M, is the same as the regression over Xis, s = 1, . . . , M, as these two
matrices are related to each other via a linear transformation V (a)
s=1 Xis Usa,
where Usa is a nonsingular M × M matrix. As to ξ2
i , calculating it requires com-
puting the ﬁrst K principal components.22 For suﬃciently low K ≪ M we can use
the power iterations method [Mises and Pollaczek-Geiringer, 1929] (see Subsection
4.1 for the algorithm and Appendix B for R source code in [Kakushadze and Yu,
2016b]).23 If K ∼ M, then we can use a no-iterations method (see Subsection 4.2 for
the algorithm and Appendix C for R source code in [Kakushadze and Yu, 2016b]).24
E.g., we can calculate the regression weights by computing the ﬁrst few principal

iPM
i =PM

λ(a) V (a)

i = ζσ2

V (a)

(12)

i

j

i

i

i

20 Such a classiﬁcation has a factor loadings matrix (or a subset of its columns) of the form
ΩiA = ωi δG(i),A, where G : {1, . . . , N} → {1, . . . , K} maps our N returns to K “clusters”. Note,
however, that the “weights” ωi (not to be confused with the portfolio weights wi in (1)) can be
arbitrary (including negative) and need not be proportional to the unit N -vector ui ≡ 1.

21 Recall that eCii = Cii, and Cij = σiσjPM
]2; the a > 1 terms are weighted by smaller eigenvalues.
22 Note that ξ2
23 This costs O(niterM N ) operations, where the number of iterations niter ≫ K. As K in-
creases, at some point ntot ∼> M and it makes more sense to use the next method.
24 This costs O(M 2N ) operations.

i = 1−PK

a=1 λ(a) [V (a)

a=1 λ(a) V (a)

i /ζσ2

V (a)

.

j

i

i

8

components (which are diﬀerent from the inverse sample variances) and then run a
weighted regression of Ei over Xis. Note, however, that for N ≫ 1 the 1st principal
component typically has a large cross-sectional correlation with the intercept, i.e.,
i are close to rescaled Cii.
Furthermore, higher principal component terms are subleading (see footnote 22).25

is close to 1, so if we take K = 1, ξ2

θ = 1√NPN

i=1 V (1)

i

4.2.2 Style Factors

Style factors are based on measured or estimated properties of our returns. Even
in the case of stocks, their number is at most of order 10. In the case of alphas
the a priori possible style factors are logs of volatility, turnover, momentum26 and,
possibly, capacity27 [Kakushadze, 2014]. We will discuss the ﬁrst three below.

There are two parts to the story here. First and foremost, if M ≫ 1, then on
general grounds it is clear that adding a few style factors to the regression cannot
make a big diﬀerence provided that we keep the regression weights ﬁxed. Second,
the 3 style factors above turn out to be poor predictors for pair-wise correlations.
For turnover this was argued based on empirical evidence in [Kakushadze, 2015d]. A
similar analysis for volatility yields the same conclusion, that log of volatility is not
a good predictor for pair-wise correlations.28 Momentum is deﬁned as an average
realized return over some period of time. The expected return is also deﬁned as
an average realized return over some – possibly other – period of time. Allocating
capital into alphas inherently is a “momentum” strategy: usually one does not bet
against alphas that have performed well in the past.29 Including momentum in the
factor loadings matrix in the regression (partly) “kills” alpha.30

One place where the style factors can make a diﬀerence is in computing the
regression weights.
I.e., we do not include them in the regression, but take the
weights to be zi = 1/ξ2
i , where the speciﬁc risks ξi are for a factor model based on
the style factors only. That is, we model the correlation matrix Ψij via a factor

25 I.e., in the 0th approximation ξ2
i based on principal components are still close to rescaled Cii.
26 Assuming momentum is positive; otherwise, we can use momentum over volatility, so its
distribution is not too skewed. If momentum equals realized return, then this is the Sharpe ratio.

27 However, capacity is diﬃcult to implement and it is unclear if it adds value.
28 Following [Kakushadze, 2015d], we deﬁne νi = ln(σi/µ), where µ is such that νi has zero
mean. We deﬁne three symmetric tensor combinations xij = uiuj, yij = uiνj + ujνi, and zij = νiνj
(ui ≡ 1 is the unit N -vector). We further deﬁne a composite index {a} = {(i, j)|i > j}, which
takes L = N (N − 1)/2 values, i.e., we pull the oﬀ-diagonal lower-triangular elements of a general
symmetric matrix Gij into a vector Ga. This way we can construct four L-vectors Ψa, xa, ya and
za. Now we can run a linear regression of Ψa over xa, ya and za. Note that xa ≡ 1 is simply the
intercept (the unit L-vector), so this is a regression of Ψa over ya and za with the intercept. The
results based on the same data as in [Kakushadze, 2015d] are summarized in Table 1 and Figure
1 conﬁrming our conclusion above.

29 This does not necessarily mean that “hockey-stick” alphas (i.e., those that have performed

well in the past but have “ﬂat-lined”) are not used in constructing a portfolio of alphas.

30 If we replace volatility by momentum deﬁned as the realized return over the entire period
of the data sample in [Kakushadze, 2015d], log of momentum too turns out a poor predictor for
pair-wise correlations. Table 2 and Figure 2 summarize the regression results.

9

model based on all or some of the four factors, the intercept, log of volatility, log of
turnover, and log of momentum. As mentioned above, the intercept by itself yields
rescaled sample variances, but makes a diﬀerence in combination with other factors.
We can always simply take the inverse sample variances as the regression weights
and not bother with computing the speciﬁc risks based on the style factors. Without
a detailed analysis using real-life alphas it is unclear if style factor based regression
weights add value. Regardless, we need not include the style factors in the regression.

4.2.3 How to Compute Speciﬁc Risk?

Here we discuss the simplest – albeit neither the only nor necessarily the best –
way to compute the speciﬁc risks. As above, instead of modeling SCM Cij via a
factor model, it is convenient to model the sample correlation matrix Ψij. Let the
corresponding factor model be

i δij +

eΓij =eξ2

KXA,B=1eΩiA ΦAB eΩjB

is the K × K identity matrix: HAB = δAB. Indeed, we can always ensure orthonor-

whereeΓij = Γij/σiσj,eξi = ξi/σi, andeΩiA = ΩiA/σi. First, without loss of generality
we can assume that the columns of eΩiA are linearly independent. Second, we can
assume that they form an orthonormal basis, i.e., the matrix HAB =PN
i=1eΩiA eΩiB
mality via the transformation (in matrix notations) eΩ →eΩ (eH T )−1, where eH is the
Cholesky decomposition of H, so eH eH T = H. Furthermore, we haveeΓii = Ψii ≡ 1.
With orthonormaleΩiA, FCM ΦAB is simply a projection of the sample correlation
matrix onto the K-dimensional hyperplane deﬁned by the columns ofeΩiA in the N-

dimensional space:31

(13)

(14)

(15)

The speciﬁc risks then follow from (13):

ΦAB =

NXi,j=1eΩiA Ψij eΩjB
KXA,B=1eΩiA ΦAB eΩiB = 1 −

eξ2
i =eΓii −

KXA,B=1eΩiA ΦAB eΩiB

However, there is a caveat in this approach. For a generic matrix eΩiA there is no
guarantee that the so-deﬁned eξ2
nontrivial restrictions on eΩiA. As an illustration, let us discuss the K = 1 case.
For K = 1 things simplify. Let us denote the sole column of eΩiA via βi. Then
PN

i are positive, which they should be. This imposes

i = 1 and

i=1 β2

(16)

31 See, e.g., [Kakushadze, 2015c] or [Kakushadze and Yu, 2016a] for a more detailed discussion.

eξ2
i = 1 − κ β2

i

10

i,j=1 βi Ψij βj ≤ λ(1), and λ(1) is the largest eigenvalue of Ψij. So, a
i < 1/λ(1). We can replace
this condition with a stronger one that avoids computing the largest eigenvalue:
a suﬃcient condition is that all β2
i,j=1 Ψij. Note that

where κ =PN
suﬃcient condition for having all eξ2
βi ≡ 1/√N, which corresponds to the intercept as the sole risk factor, satisﬁes this
condition. Violations of this condition usually occur for βi with a skewed distribu-
tion, e.g., if βi ∝ σi; however, for, e.g., βi ∝ ln(σi) such violations are either absent
altogether or rarer and can be dealt with by “reigning” in the few violating elements.
See the R function qrm.fr() in Appendix A of [Kakushadze and Yu, 2016a] for such
an algorithm, which is built for a general K-factor model (not just K = 1).

i ≤ 1/λ∗, where λ∗ = 1

i > 0 is that all β2

NPN

i > 0 in

When we have multiple factors, however, things get trickier. Even if eξ2
all K 1-factor models based on the individual columns of a multifactor eΩiA, in the
K-factor model we can still have some eξ2
speciﬁc risks even if they all come out to be positive. Computingeξ2
using the inverse sample variances (i.e., eξ2

i < 0. See Section 4 for the algorithm and
Appendix B for R source code in [Kakushadze and Yu, 2016a] for circumventing this
issue (even for K = 1). However, let us note one issue associated with computing the
i via (15) involves
FCM ΦAB, which in turn involves the sample correlation matrix Ψij via (14). While
ΦAB is expected to be more out-of-stable than Ψij as we have K ≪ N, using FCM
in (15) still adds some noise to the regression weights. This is to be contrasted with
i ≡ 1), which are relatively stable out-of-

sample, as the regression weights. This remark equally applies to all K values.

4.3 Can We Increase the Number of Factors?

So, while we can try to play with the regression weights, it is not all that clear that
this would make it or break it, especially that we are still limited to the M risk
factors, which are equivalent to the ﬁrst M principal components – albeit we never
have to compute the principal components in the ﬁrst instance. The question is, can
we increase the number of columns in the factor loadings matrix in the regression as
this presumably would cover more directions in the risk space and improve the out-
of-sample performance. Prosaically, the answer is that we need more information,
i.e., additional input, to achieve this. Here is one approach [Kakushadze, 2014].

The idea here is that, assuming all alphas have essentially overlapping trad-
ing universes, we can treat exposure to each underlying tradable – for the sake of
deﬁniteness let us assume we are dealing with the U.S. equities as the underlying
tradables – as a risk factor. This makes sense, but the question is what should the

factor loadings matrix eΩiA be? In this case A simply labels stocks in the trading

universe, which is, say, top 2,500 most liquid tickers, so K is large, much larger than
the typical value of M, which for a (generous)32 1-year lookback is only about 250.
Historical stock position data for each alpha must be available to us if we are
to backtest them. Let this position data be PiAs, which is the dollar holding of the

32 Many alphas can be more ephemeral than that.

11

alpha labeled by i in the stock labeled by A at time labeled by ts, normalized such

s=1 PiAs
does not work as the sign of PiAs ﬂips over time frequently assuming alphas have

thatPA |PiAs| = 1 for each given pair i, s. We can try to constructeΩiA from PiAs by
M +1PM +1
getting rid of the time series index s. The most obvious choiceeΩiA = 1
reasonably short holding periods. We need an unsigned quantity to deﬁne eΩiA. We

can use33

1

(17)

eΩiA =

M + 1

M +1Xs=1

|PiAs|

This is simply average relative exposure of the i-th alpha to the stock labeled by A.
One potential “shortcoming” of this deﬁnition is that, if the position bounds –
call them BiA – are imposed at the level of individual alphas, for some, mostly less
liquid, stocks |PiAs| could be saturating these bounds. On general risk management
grounds, these bounds can very well be uniform across all alphas. E.g., one may
wish to cap the positions as the smaller of: i) a small percentile of the total dollar
investment – this is a diversiﬁcation bound; and ii) a (generally, diﬀerent) small
percentile of ADDV (average daily dollar volume) – this is a liquidity bound (in
case positions need to be liquidated). If the bounds are saturated most of the time,
this can eﬀectively reduce the number of independent risk factors, and the deﬁnition

ofeΩiA may have to be modiﬁed for such stocks (see [Kakushadze, 2014]). However, if

the bounds are imposed at the level of the combined alpha, then this is a non-issue.
Assuming N ≫ K, even with the larger number of risk factors (17), our opti-
mization reduces to a weighted regression. We can simply choose the weights as
the inverse sample variances. Alternatively, we can attempt to compute speciﬁc

risks. For this we need FCM ΦAB. With appropriately normalized eΩiA, FCM ΦAB

is simply the covariance matrix for the stocks. In the zeroth approximation we can
set it to ΦAB ≈ σ2
A are sample variances for stocks. Alternatively,
we can either use commercial risk models or construct them organically, as, e.g.,
in [Kakushadze, 2015c] and [Kakushadze and Yu, 2016a]. Also see Section 6 hereof.

A δAB, where σ2

5 A Reﬁnement

So, to summarize, our optimization (1) reduces to a regression of the normalized

returns eEi = Ei/ξi over the factor loadings matrix eΩiA = ΩiA/ξi. For simplicity,

let us take ξi = σi. Further, let us take ΩiA to be the M demeaned returns Xis,
s = 1, . . . , M, i.e., we are not using any additional style or other risk factors. Then
the sample correlation matrix is given by (we identify the index A with the index s)

Ψij =

MXs,s′=1eΩis φss′ eΩjs′

33 The overall normalization factor is immaterial and included for aesthetic reasons.

(18)

12

i

i

unit weights, or, equivalently, over the principal components V (a)

, a = 1, . . . , M, of Ψij. For large N the ﬁrst
is close to the appropriately normalized unit N-vector:

the ﬁrst M principal components V (a)
principal component V (1)
V (1)

It is evident that the columns of eΩis are nothing but some linear combinations of
i ≈ 1/√N . Recall from (8) (see the discussion right thereafter) that the weights
wi ≈ ηeεi/σi, whereeεi are the residuals of the regression of eEi = Ei/σi over eΩis with
thatPN
eεi = 0 and, therefore, many weights wi are negative (assuming the

expected returns Ei are all positive). I.e., the vanilla optimization forces us to take
bets against many positive expected returns because it hedges against all alphas
simultaneously losing money. This is overkill and literally “kills” alpha:
if alphas
are not highly correlated on average, most alphas losing money all at once is possible
but highly unlikely. Assuming tolerance for drawdowns, we can relax this hedge.

. This implies

i=1 V (1)

i

i

5.1 A 1-factor Example

To further illustrate this point, let us consider a simple example. Let us hypothet-
ically assume that the true correlation matrix has uniform oﬀ-diagonal elements:
Ψij = (1 − ρ) δij + ρuiuj, where ui ≡ 1 is the unit N-vector. Then the weights are
given by (note that this is exact):

wi =

η

ξi"eEi −

ρ

1 + (N − 1) ρ

NXi=1 eEi#

(19)

expression in the square brackets on the r.h.s. of (19) is approximately equal cross-

where eEi = Ei/ξi, and ξi = √1 − ρ σi. So, if ρ > 0 and N ≫ 1/ρ, then the
sectionally demeaned eEi and many weights will be negative.34 How can we ﬁx this?

5.2 Removing the “Overall” Mode

In the case of equity portfolios with a large number of tickers we also have the
same behavior, that the ﬁrst principal component of the sample correlation matrix
is close to the rescaled intercept. This is known as the “market” mode and describes
the overall, in-sync movement of all stocks, i.e., of the broad market as a whole.35
However, in the case of equity portfolios the issue of the “market” mode is subdued.
Thus, for dollar-neutral portfolios roughly half of the weights are negative by the
dollar-neutrality constraint. In long-only portfolios one typically does not optimize
the raw expected returns directly but against a broad benchmark. In this regard,
alpha portfolio optimization is analogous to long-only portfolio optimization for

34 The distribution of σi is skewed and roughly log-normal, and so is that of Ei, so the distri-

bution of eEi is not very skewed and is roughly normal; see [Kakushadze and Tulchinsky, 2016].

35 See, e.g., [Bouchaud and Potters, 2011], which reviews applications of random matrix theory

to modeling a sample correlation matrix for equities, and references therein.

13

equities. So, here too we could optimize against a benchmark alpha portfolio (as
opposed to (1)). E.g., we can take

wbenchmark

i

= η

Ei
σ2
i

(20)

which corresponds to taking the diagonal part of SCM Cij. There are other choices.
Alternatively, we can simply remove the “overall” mode (i.e., the analog of the
“market” mode for equity portfolios). One way is to take the M principal compo-
nents and simply remove the ﬁrst principal component, i.e., to run the regression
, a = 2, . . . , M. However, this would

over the factor loadings matrix eΩ′ia = V (a)
require computing the principal components. There is a simpler way. We take eΩis
in (18) and demean its columns. Let the resulting matrix be eΩ∗is. Only M − 1 of
its columns are linearly independent. So, we can regress over eΩ∗is, s = 1, . . . , M − 1.

This removes the “overall” mode and while some weights may still turn out to be
negative, their number will be relatively limited (not roughly half of the weights).
The backtested Sharpe ratio will go down, and the portfolio return will go up.

i

5.3 Summary of Regression Procedure

For clarity, let us put all the pieces together into a step-by-step summary:36

s=1 Ris.

• 1) Start with a time series of alpha returns37 Ris, i = 1, . . . , N, s = 1, . . . , M +1.
• 2) Calculate the serially demeaned returns Xis = Ris − 1
s=1 X 2
• 3) Calculate sample variances38 σ2
is.
• 4) Calculate the normalized demeaned returns Yis = Xis/σi.
• 5) Keep only the ﬁrst M columns in Yis: s = 1, . . . , M.
• 6) Cross-sectionally demean39 Yis: Λis = Yis − 1
j=1 Yjs.
• 7) Keep only the ﬁrst M − 1 columns in Λis: s = 1, . . . , M − 1.

MPM +1
NPN

M +1PM +1

i = Cii = 1

• 8) Take the alpha expected returns Ei and normalize them: eEi = Ei/σi.
• 9) Calculate the residuals eεi of the unit-weighted regression40 of eEi over Λis.
• 10) Set the alpha portfolio weights to wi = η eεi/σi.
• 11) Set the normalization coeﬃcient η such thatPN

Source code in R for the above procedure is given in Appendix A. If we wish to
use the underlying tradables as the risk factors as in (17), then we simply replace Λis
in step 9) above by the corresponding factor loadings matrix (see Appendix A).41
Also, in steps 4)-11) above instead of using sample variances σ2
i , we can use speciﬁc
variances ξ2
i computed based on principal components or style factors (see above).

i=1 |wi| = 1.

36 Here we deliberately use slightly diﬀerent notations than above.
37 As before, i = 1, . . . , N labels the alphas; s = 1, . . . , M + 1 labels the times ts.
38 Their normalization is immaterial in what follows.
39 This step removes the “overall” mode and can be skipped if so desired. Then we would also

skip the next step 7) below.
40 Without the intercept.
41 Which can be, e.g., (17) or a union thereof with Yis deﬁned in step 5) above (with any linearly

dependent columns removed).

14

5.4 What about Computational Cost?

It is evident than none of the steps above cost more than O(MN) operations except
perhaps for step 9), the regression. The regression residuals are given by

NXj=1

M−1Xs,s′=1

Λis Υ−1

ss′ Λjs′

(21)

eεi = eEi −

where Υss′ =PN

i=1 Λis Λis′. Calculating this (M−1)×(M−1) matrix costs O(M 2N)
operations. Straightforwardly inverting it costs O(M 3) operations.42 The rest (sums
over s, s′, j, etc.) costs O(M 2N) operations, and therefore so does the regression.

5.5

Is This Related to Principal Components?

PM

The answer is yes. As we discussed above, the sample correlation matrix Ψij =
s,s′=1 Yis φss′ Yjs′ (where φss′ = (δss′ + usus′) /M; us ≡ 1 is the unit M-vector), so
the M columns of Yis are just linear combinations of the ﬁrst M principal compo-
nents V (a)
, a = 1, . . . , M, of Ψij. So, we could use the principal components instead
of Yis in the above steps and get the same result43 for the weights wi. However,
computing the ﬁrst M principal components costs additional O(M 2N) operations.

i

6 Conclusions

As we discussed above, in the absence of “clustering”, when the number of alphas
is large, optimization (via maximizing the Sharpe ratio) reduces to a (weighted) re-
gression irrespective of whether we start from a constructed factor model, or deform
the sample covariance matrix. This is because such deformations themselves are
nothing but factor models. We also argued that in most cases the factor loadings,
over which the expected returns are regressed, are given by the (properly demeaned,
normalized and trimmed) time series matrix of historical returns based on which the
(singular) sample covariance matrix is computed. The regression weights, which can
be recast as the normalizations of the expected returns, the factor loadings matrix
and the alpha weights, can be taken as inverse sample variances or, alternatively,
speciﬁc variances in some factor model. However, computation of these speciﬁc
variances via (15) requires computing the factor covariance matrix via (14) (thereby
adding noise to the regression weights) not needed to compute sample variances.

There is a notable exception to this, to wit, if we use the underlying tradables
(stocks) themselves as risk factors via (17). In this case the factor covariance matrix
ΦAB need not be computed via a sample covariance matrix of linear combinations
42 The fact that Υss′ has a factor form does not help as N ≫ M , in fact, in practice N ≫ M 2.
43 We can keep step 6) above; alternatively, we can simply drop the ﬁrst principal component,

which will give a slightly diﬀerent set of wi.

15

of the alpha returns. Instead, it can be taken to be the covariance matrix for stocks.
The latter need not be computed as a sample covariance matrix of stock returns,
which would be out-of-sample unstable or, even worse, singular. Instead, we can use
a constructed covariance matrix for stocks, e.g., via a factor model [Kakushadze,
2014]. A priori we could use commercially available risk models (albeit they are not
necessarily expected to be out-of-sample stable), or build them organically via, e.g.,
heterotic risk models [Kakushadze, 2015c] or heterotic CAPM [Kakushadze and Yu,
2016a]. In fact, in the zeroth approximation we can set ΦAB ≈ σ2
A δAB, where σA
are sample (historical) stock volatilities or implied volatilities from options.44 And
once we nail down ΦAB, we can compute the speciﬁc variances via (15). However, in
reality there is a caveat here. The caveat is that if we identify ΦAB with the stock
covariance matrix, then the factor loadings matrix is given by (17) only up to an
overall normalization constant which is a priori unknown. So we can treat it as a
free parameter and consider a 1-parameter family of speciﬁc variances. The value
of this parameter then can be ﬁxed by optimizing for realized performance with the
caveat that it need not be out-of-sample stable and may have to be recomputed
frequently based on short lookbacks. This provides a well-deﬁned prescription for
computing speciﬁc variances. Alternatively, we can use sample variances, which are
relatively stable out-of-sample and simple to compute. Yet another alternative is
to use speciﬁc variances computed based on principal components (see [Kakushadze
and Yu, 2016b] and above) or style factors. The latter case generally requires using
more sophisticated methods such as those discussed in [Kakushadze and Yu, 2016a].
A nice thing about our optimization reducing to a regression, which is compu-
tationally cheap, is that it can be readily modiﬁed to incorporate bounds on the
alpha weights wi. Indeed, since wi are proportional to the regression residuals, we
can simply use the bounded regression discussed in [Kakushadze, 2015b]. Similarly,
we can incorporate trading costs via the method discussed in [Kakushadze, 2015a].
Finally, let us mention that if we use the algorithms of [Kakushadze and Yu,
2016b] for building statistical risk models, which include ﬁxing the number of risk
factors (i.e., principal components) K together with the speciﬁc risks, we are going
to end up with a regression over K < M principal components with the regression
weights equal the inverse speciﬁc variances. In the case of stocks this works better
than regressing over M principal components with the regression weights ad hoc set
equal the inverse speciﬁc variances for a K-factor model with K < M [Kakushadze
and Yu, 2016b]. Same may or may not hold for, e.g., N ∼ 100, 000 real-life alphas.

A R Code for Alpha Weights

In this appendix we give the R source code for calculating the alpha weights based
on a regression. The code below is essentially self-explanatory and straightforward

44 Albeit not all stocks in the trading universe may be optionable and have impled volatilities

readily available. Also, it is unclear whether implied volatilities add value [Kakushadze, 2015c].

16

as it simply follows the algorithm and formulas in Subsection 5.3. It consists of a
single function calc.opt.weights(e.r, ret, y = 0, s = 0, rm.overall = T); e.r
is an N-vector of expected returns we wish to optimize; N is the number of the un-
derlying returns (e.g., alphas); ret is an N × (M + 1) matrix of returns; M + 1 is the
number of data points in the time series (e.g., days); y is an N × K factor loadings
= 0 is used, the code computes the factor loadings matrix Yis (s = 1, . . . , M − 1 or
s = 1, . . . , M depending on whether rm.overall = T or rm.overall = F – see below)
based on the time series ret via the algorithm of Subsection 5.3; s is an N-vector of

matrix eΩiA, A = 1, . . . , K, pre-computed, e.g., via (17); otherwise, if the default y
speciﬁc risks eξi pre-computed, e.g., via (15) or (16); otherwise, if the default s = 0
normalized such thatPN
bine (via cbind()) the pre-computed factor-loadings matrix eΩiA as in (17) with the

is used, the code computes s as the square root of the sample variances; rm.overall,
if TRUE (default), implies that the “overall” mode is taken out; otherwise, it is kept
(see Subsection 5.3). The output is an N-vector wi of the optimized alpha weights
i=1 |wi| = 1. The code can be easily modiﬁed, e.g., to com-

factor loadings matrix Yis it already computes based on the time series ret.

calc.opt.weights <- function(e.r, ret, y = 0, s = 0, rm.overall = T)
{

if(length(s) == 1)

s <- apply(ret, 1, sd)

if(length(y) == 1)
{

x <- ret - rowMeans(ret)
y <- x / s
y <- y[, -ncol(x)]

}

if(rm.overall)
{

y <- t(t(y) - colMeans(y))
y <- y[, -ncol(y)]

}

e.r <- matrix(e.r / s, length(e.r), 1)
w <- t(y) %*% e.r
w <- solve(t(y) %*% y) %*% w
w <- e.r - y %*% w
w <- w / s
w <- w / sum(abs(w))
return(as.vector(w))

}

17

B DISCLAIMERS

Wherever the context so requires, the masculine gender includes the feminine and/or
neuter, and the singular form includes the plural and vice versa. The author of this
paper (“Author”) and his aﬃliates including without limitation Quantigicr Solu-
tions LLC (“Author’s Aﬃliates” or “his Aﬃliates”) make no implied or express
warranties or any other representations whatsoever, including without limitation
implied warranties of merchantability and ﬁtness for a particular purpose, in con-
nection with or with regard to the content of this paper including without limitation
any code or algorithms contained herein (“Content”).

The reader may use the Content solely at his/her/its own risk and the reader
shall have no claims whatsoever against the Author or his Aﬃliates and the Author
and his Aﬃliates shall have no liability whatsoever to the reader or any third party
whatsoever for any loss, expense, opportunity cost, damages or any other adverse
eﬀects whatsoever relating to or arising from the use of the Content by the reader
including without any limitation whatsoever: any direct, indirect, incidental, spe-
cial, consequential or any other damages incurred by the reader, however caused
and under any theory of liability; any loss of proﬁt (whether incurred directly or
indirectly), any loss of goodwill or reputation, any loss of data suﬀered, cost of pro-
curement of substitute goods or services, or any other tangible or intangible loss;
any reliance placed by the reader on the completeness, accuracy or existence of the
Content or any other eﬀect of using the Content; and any and all other adversities
or negative eﬀects the reader might encounter in using the Content irrespective of
whether the Author or his Aﬃliates is or are or should have been aware of such
adversities or negative eﬀects.

The R code included in Appendix A hereof is part of the copyrighted R code
of Quantigicr Solutions LLC and is provided herein with the express permission of
Quantigicr Solutions LLC. The copyright owner retains all rights, title and interest
in and to its copyrighted source code included in Appendix A hereof and any and
all copyrights therefor.

References

Bouchaud, J.-P. and Potters, M. “Financial applications of random matrix
theory: a short review.” In: Akemann, G., Baik, J. and Di Francesco, P. (eds.)
The Oxford Handbook of Random Matrix Theory. Oxford, United Kingdom:
Oxford University Press, 2011.

Grinold, R.C. and Kahn, R.N. “Active Portfolio Management.” New York, NY:
McGraw-Hill, 2000.

18

Kakushadze, Z. “Factor Models for Alpha Streams.” The Journal of Investment
Strategies, 4(1) (2014), pp. 83-109.
Available online: http://ssrn.com/abstract=2449927.

Kakushadze, Z. “Combining Alpha Streams with Costs.” The Journal of Risk,
17(3) (2015a), pp. 57-78. Available online: http://ssrn.com/abstract=2438687.

Kakushadze, Z. “Combining Alphas via Bounded Regression.” Risks, 3(4)
(2015b), pp. 474-490. Available online: http://ssrn.com/abstract=2550335.

Kakushadze, Z. “Heterotic Risk Models.” Wilmott Magazine, 2015(80) (2015c),
pp. 40-55. Available online: http://ssrn.com/abstract=2600798.

Kakushadze, Z. “101 Formulaic Alphas.” Wilmott Magazine (forthcoming).
Available online: http://ssrn.com/abstract=2701346 (2015d).

Kakushadze, Z. “Shrinkage = Factor Model.” Journal of Asset Management,
(17)(2) (2016), pp.69-72. Available online: http://ssrn.com/abstract=2685720.

Kakushadze, Z. and Tulchinsky, I. “Performance v. Turnover: A Story by
4,000 Alphas.” The Journal of Investment Strategies, 5(2) (2016) (forthcom-
ing). Available online: http://ssrn.com/abstract=2657603.

Kakushadze, Z. and Yu, W. “Multifactor Risk Models and Heterotic CAPM.”
The Journal of Investment Strategies, 5(4) (2016a) (forthcoming). Available
online: http://ssrn.com/abstract=2722093.

Kakushadze, Z. and Yu, W. “Statistical Risk Models.” Working Paper. Avail-
able online: http://ssrn.com/abstract=2732453 (2016b).

Ledoit, O. and Wolf, M. “Honey, I Shrunk the Sample Covariance Matrix.” The
Journal of Portfolio Management, 30(4) (2004), pp. 110-119.

Markowitz, H. “Portfolio selection.” The Journal of Finance, 7(1) (1952), pp.
77-91.

Mises, R.V. and Pollaczek-Geiringer. H. “Praktische Verfahren der Gle-
ichungsauﬂ¨osung.” ZAMM – Zeitschrift f¨ur Angewandte Mathematik und
Mechanik, 9(2) (1929), pp. 152-164.

Sharpe, W.F. “The Sharpe Ratio.” The Journal of Portfolio Management, 21(1)
(1994), pp. 49-58.

19

Table 1: Summary for the cross-sectional regression of Ψa over ya and za with the
intercept, where ya and za are based on log of volatility. See Subsection 4.2.2 for
details. Also see Figure 1.

Intercept
ya
za
Mult./Adj. R-squared
F-statistic

Estimate Standard error
0.1588
0.0331
0.1354

0.0016
0.0029
0.0106

t-statistic Overall
97.28
11.37
12.82

0.0540 / 0.0536
144.0

Table 2: Summary for the cross-sectional regression of Ψa over ya and za with the
intercept, where ya and za are based on log of momentum. See Subsection 4.2.2 for
details. Also see Figure 2.

Intercept
ya
za
Mult./Adj. R-squared
F-statistic

Estimate Standard error
0.1587
0.0158
0.1389

0.0017
0.0033
0.0137

t-statistic Overall
95.74
4.74
10.14

0.0238 / 0.0234
61.58

20

6
0

.

n
o

i
t

4
0

.

l

 

a
e
r
r
o
C
d
e
n
a
e
m
e
D

2

.

0

0

.
0

2
.
0
−

−0.1

0.0

0.2

0.3

0.1

w

Figure 1. Horizontal axis: wa = 0.0331 ya + 0.1354 za; vertical axis: Ψa − Mean(Ψa). See
Table 1 and Subsection 4.2.2. The numeric coeﬃcients in wa are the regression coeﬃcients
in Table 1.

21

6
0

.

n
o

i
t

4
0

.

l

 

a
e
r
r
o
C
d
e
n
a
e
m
e
D

2

.

0

0

.
0

2
.
0
−

−0.1

0.0

0.2

0.3

0.1

w

Figure 2. Horizontal axis: wa = 0.0158 ya + 0.1389 za; vertical axis: Ψa − Mean(Ψa). See
Table 2 and Subsection 4.2.2. The numeric coeﬃcients in wa are the regression coeﬃcients
in Table 2.

22

