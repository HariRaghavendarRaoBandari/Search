6
1
0
2

 
r
a

 

M
5
1

 
 
]

V
C
.
s
c
[
 
 

1
v
5
2
5
4
0

.

3
0
6
1
:
v
i
X
r
a

Pushing the Limits of Deep CNNs for Pedestrian Detection

Qichang Hu1,2, Peng Wang1, Chunhua Shen1, Anton van den Hengel1, Fatih Porikli2

1The University of Adelaide, Australia

2NICTA, Australia

Contents

1 Introduction

1.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.1 Convolutional Feature Maps (CFMs) . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.2
Segmentation for Object Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Datasets, Evaluation Metric and Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Boosted Decision Forests with Multi-layer CFMs

2.1 Fine-tuning DCNNs with Bootstrapped Data . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Ensemble of Boosted Decision Forests
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Pixel Labelling Improves Pedestrian Detection

4 Fusing Models

4.1 Using Complimentary Hand-crafted Features
. . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Pixel Labelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3 Ablation Studies
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.4 Fast Ensemble Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.5 Comparison to State-of-the-art Approaches
. . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Conclusions

Abstract

Compared to other applications in computer vision, convolutional neural networks have under-performed

on pedestrian detection. A breakthrough was made very recently by using sophisticated deep CNN mod-
els, with a number of hand-crafted features [1], or explicit occlusion handling mechanism [2].
In this
work, we show that by re-using the convolutional feature maps (CFMs) of a deep convolutional neural
network (DCNN) model as image features to train an ensemble of boosted decision models, we are able to
achieve the best reported accuracy without using specially designed learning algorithms. We empirically
identify and disclose important implementation details. We also show that pixel labelling may be simply
combined with a detector to boost the detection performance. By adding complementary hand-crafted
features such as optical ﬂow, the DCNN based detector can be further improved. We set a new record
on the Caltech pedestrian dataset, lowering the log-average miss rate from 11.7% to 8.9%, a relative im-
provement of 24%. We also achieve a comparable result to the state-of-the-art approaches on the KITTI
dataset.

1
2
2
3
3
3

4
4
4

6

8
8
9
9
9
10

12

1 Introduction

The problem of pedestrian detection has been intensively studied in recent years. Prior to the very recent
work in deep convolutional neural networks (DCNNs) based methods [1, 2], the top performing pedestrian
detectors are boosted decision forests with carefully hand-crafted features, such as histogram of gradients
(HOG) [3], self-similarity (SS) [4], aggregate channel features (ACF) [5], ﬁltered channel features [6] and
optical ﬂow [7].

1

Recently, DCNNs have signiﬁcantly outperformed comparable methods on a wide variety of vision prob-
lems [8–15]. A region-based convolutional neural network (R-CNN) [11] achieved excellent performance for
generic object detection, for example, in which a set of potential detections (object proposals) are evaluated
by a DCNN model. CifarNet [16] and AlexNet [8] have been extensively evaluated in the R-CNN detection
framework in [17] for pedestrian detection. In their work, the best performance (23.3%) was achieved by
AlexNet pre-trained on the ImageNet [18] classiﬁcation dataset. Note that this result is still inferior to con-
ventional pedestrian detectors such as [6] and [7]. The DCNN models in [17] under-perform mainly because
the network design is not optimal for pedestrian detection. The performance of R-CNNs for pedestrian detec-
tion has further improved to 16.43% in [2] through the use of a deeper GoogLeNet model which is ﬁne-tuned
using Caltech pedestrian dataset.

To explicitly model the deformation and occlusion, another line of research for object detection is part-
based models [19–22] and explicit occlusion handling [23–25]. DCNNs have also been incorporated along this
stream of work for pedestrian detection [26–28], but none of these approaches has achieved better results
than the best hand-crafted features based method of [6] on the Caltech dataset.
The performance of pedestrian detection is improved over hand-crafted features by a large margin (a ∼ 5%
gain on Caltech), by two very recent approaches relying on DCNNs: CompACT-Deep [1] combines hand-
crafted features and ﬁne-tuned DCNNs into a complexity-aware cascade. Tian et al.
[2] ﬁne-tuned a pool
of part detectors using a pre-trained GoogLeNet, and the resulting ensemble model (refer to as DeepParts)
delivers similar results as CompACT-Deep. Both approaches are much more sophisticated than the standard
R-CNN framework: CompACT-Deep involves the use of a variety of hand-crafted features, a small CNN
model and a large VGG16 model [9]. DeepParts contains 45 ﬁne-tuned DCNN models and needs a set of
strategies (including bounding-box shifting handling and part selection) to arrive at the reported result. Note
that the high complexity of DCNN models can lead to practical diﬃculties. For example, it can be too costly
to load all 45 DCNN models into a GPU card.

Here we ask a question: Is a complex DCNN based learning approach really a must for achieving the
state-of-the-art performance? Our answer to this question is negative. In this work, we propose alternative
methods for pedestrian detection, which are simpler in design, with comparable or even better performance.
Firstly, we extensively evaluate the CFMs extracted from convolutional layers of a ﬁne-tuned VGG16 model
for pedestrian detection. Using only a CFM of a single convolutional layer, we train a boosted-tree-based
detector and the resulting model already signiﬁcantly outperforms all previous methods except the above
two sophisticated DCNN frameworks. This model can be seen as a strong baseline for pedestrian detection
as it is very simple in terms of implementation.

We show that the CFMs from multiple convolutional layers can be used for training eﬀective boosted
decision forests. These boosted decision forests are combined altogether simply by score averaging. The
resulting ensemble model beats all competing methods on the Caltech pedestrian dataset. We further improve
the detection performance by incorporating a pixel labelling model. Next we review some related work.

1.1 Related Work

1.1.1 Convolutional Feature Maps (CFMs)

It has been shown in [29–31] that CFMs have strong representation abilities for many tasks. Long et al.
[32]
cast all fully-connected layers in DCNNs as convolutions for semantic image segmentation. In [30], the CFMs
from multiple layers are stacked into one vector and used for segmentation and localization. Ren et al.
[29]
learn a network on the CFMs (pooled to a ﬁxed size) of a pre-trained model.

The work by Yang et al. [31] is close to ours, which trains a boosted decision forest for pedestrian detection
with the CFM features from the Conv3-3 layer of the VGG16 model [9], and the performance (17.32%) on
Caltech is comparable to checkerboards [6]. It seems that there is no signiﬁcant superiority of the CFM used
in [31] over hand-crafted features on the task of pedestrian detection. The reason may be two-fold. First,
the CFM used in [31] are extract from the pre-trained VGG16 model which is not ﬁne-tuned on a pedestrian
dataset; Second, CFM features are extracted from only one layer and the multi-layer structure of DCNNs is
not fully exploited. We show in this work that both of these two issues are critically important in achieving
good performance.

2

1.1.2 Segmentation for Object Detection

The cues used by segmentation approaches are typically complementary to those exploited by top-down
[33] propose to perform generic object detection by labelling super-pixels,
methods. Recently, Yan et al.
which results in an energy minimization problem with data term learned by DCNN models.
In [34, 13],
segmented image regions (not bounding boxes) are generated as object proposals and then used for object
detection.

In contrast to the above region (or super-pixel) based methods, we here exploit at an even ﬁner level of
information, that is, pixel labelling. In particular, in this work we demonstrate that we can improve the
detection performance by simply re-scoring the proposals generated by a detector, using pixel-level scores.

1.2 Contributions

We revisit pedestrian detection with DCNNs by studying the impact of a few training details and design
parameters. We show that ﬁne-tuning of a DCNN model using pedestrian data is critically important. Proper
bootstrapping has a considerable impact too. Besides these ﬁndings, other main contributions of this work
can be summarized as follows.

1. The use of multi-layer CFMs for training a state-of-the-art pedestrian detector. We show that it is
possible to train an ensemble of boosted decision forests using multi-layer CFMs that outperform all
previous methods. For example, with CFM features extracted from two convolutional layers, we can
achieve an log-average miss rate of 10.7% on Caltech, which already perform better than all previous
methods, including the two sophisticated DCNNs based methods [1, 2].

2. Incorporating semantic pixel labelling. We also propose a combination of sliding-window detectors and
semantic pixel-labelling, which performs on par with the best of previous methods. To keep the method
simple, we use the learned weighted sum of pixel-labelling scores in a proposal region.

3. The best reported pedestrian detector. A new performance record for Caltech is set by exploiting a
DCNN as well as two complimentary hand-crafted features: ACF and optical-ﬂow features. This shows
that some types of hand-crafted features are complementary to deep convolutional features.

Before we present our methods, we brieﬂy describe the datasets, evaluation metric and boosting models.

1.3 Datasets, Evaluation Metric and Models

Caltech pedestrian dataset The Caltech dataset [35] is one of the most popular datasets for pedestrian
detection. It contains 250k frames extracted from 10 hours of urban traﬃc video. There are in total 350k
annotated bounding boxes with 2300 unique pedestrians. The standard training set and test set consider one
out of each 30 frames. In our experiments, the training images are increased to one out of each 4 frames.
Note that many competing methods [6, 31, 17] have used the same extended training set or even more data
(every third frame).

For Caltech dataset, we evaluate the performance of various detectors using the log-average miss rate (MR)
which is computed by averaging the miss rate at false positive rates spaced evenly between 0.01 to 1 false-
positives-per-image (FPPI) range. Unless otherwise speciﬁed, the detection performance on our experiments
shown in the remainder of the paper is the MR on the Caltech Reasonable test set.
KITTI pedestrian dataset The KITTI dataset [36] consists of 7481 training images and 7518 test images,
comprising more than 80 thousands of annotated objects in traﬃc scenes. The KITTI dataset provides a
large number of pedestrians with diﬀerent sizes, viewpoints, occlusions, and truncations. Due to the diversity
of these objects, the dataset has three subsets (Easy, Moderate, Hard) with respect to the diﬃculty of object
size, occlusion and truncation. We use the Moderate training subset as the training data in our experiments.
For KITTI dataset, average precision (AP) is used to evaluate the detection performance. The average
precision summaries the shape of the precision-recall curve, and is deﬁned as the mean precision at a set of
evenly spaced recall levels. All methods are ranked based on the Moderate diﬃcult results.
Boosted decision forest Unless otherwise speciﬁed, we train all our boosted decision forests using the
following parameters. The boosted decision model consists of 4096 depth-5 decision trees, trained via the
shrinkage version of real-Adaboost [37]. The size of model is set to 128 × 64 pixels, and one bootstrapping

3

Table 1: Performance improvements with diﬀerent ﬁne-tuning strategy and shrinkage (on Reasonable). All boosted decision
forests are trained with the CFM extracted from the Conv3-3 layer of VGG16. CFM3a: the original VGG16 model pre-trained
on ImageNet is used to extract features. CFM3b: the VGG16 model is ﬁne-tuned with the data collected by an ACF [5] detector.
CFM3c and CFM3: the ﬁne-tuning data is obtained by bootstrapping with CFM3b. With the same ﬁne-tuning data, setting the
shrinkage parameter of Adaboost to 0.5 brings an additional 1% reduction on the MR

Fine-tuning data

Shrinkage Miss rate (%)

No ﬁne-tuning

Model
CFM3a
CFM3b
CFM3c Bootstrapping with CFM3b
CFM3
Bootstrapping with CFM3b

Collected by ACF

−
−
−
0.5

18.71
16.42
14.54
13.49

iteration is implemented to collect hard negatives and re-trains the model. The sliding window stride is set
to 4 pixels.

2 Boosted Decision Forests with Multi-layer CFMs

In this section, we ﬁrstly show that the performance of boosted decision forests with CFMs can be signiﬁcantly
improved by simply ﬁne-tuning DCNNs with hard negative data extracted through bootstrapping. Then
boosted decision forests are trained with diﬀerent layers of CFMs, and the resulting ensemble model is able
to achieve the best reported result on the Caltech dataset.

2.1 Fine-tuning DCNNs with Bootstrapped Data

In this work, the VGG16 [9] model is used to extract CFMs. As we know, the VGG16 model was originally
pre-trained on the ImageNet data with image-level annotations and was not trained speciﬁcally for the
pedestrian detection task. It is expected that the detection performance of boosted decision forests trained
with CFMs ought to be improved by ﬁne-tuning the VGG16 model with Caltech pedestrian data.

To adapt the pre-trained VGG16 model to the pedestrian detection task, we modify the structure of the
model. We replace the 1000-way classiﬁcation layer with a randomly initialized binary classiﬁcation layer
and change the input size from 224 × 224 to 128 × 64 pixels. We also reduce the number of neurons in
fully connected layers from 4096 to 2048. We ﬁne-tune all layers of this modiﬁed VGG16, except the ﬁrst
4 convolutional layers since they correspond to low-level features which are largely universal for most visual
objects. The initial learning rate is set to 0.001 for convolutional layers and 0.01 for fully connected layers.
The learning rate is divided by 10 at every 10000 iterations. For ﬁne-tuning, 30k positive and 90k negative
examples are collected by diﬀerent approaches. The positive samples are those overlapping with a ground-
truth bounding box by [0.5, 1], and the negative samples by [0, 0.25]. At each SGD iteration, we uniformly
sample 32 positive samples and 96 negative samples to construct a mini-batch of size 128.

We train boosted decision forests with the CFM extracted from the Conv3-3 layer of diﬀerently ﬁne-
tuned VGG16 models and the results are shown in Table 1. Note that all the VGG16 models in this table
are ﬁne-tuned from the original model pre-trained on ImageNet data. It can be observed that the miss rate
is reduced from 18.71% to 16.42% by replacing the pre-trained VGG16 model with the one ﬁne-tuned on
data collected by applying an ACF [5] detector on the training dataset. The detection performance is further
improved to 14.54% miss rate if it is ﬁne-tuned on the bootstrapped data using the previous trained model
CFM3b. Another 1% performance gain is obtained by applying shrinkage to the coeﬃcients of weak learners,
with shrinkage parameter being 0.5 (see [38]). The last model (corresponding to row 4 in Table 1) is refer to
as CFM3 from now on.

2.2 Ensemble of Boosted Decision Forests

In the last experiment, we only use a CFM from a single layer of the VGG16 model. In this section, we
intensively explore the deep structure of the VGG16 model which consists of 13 convolutional layers, 2 fully
connected layers, and 1 classiﬁcation layer. These 13 convolutional layers are organized into 5 convolutional

4

Table 2: Comparison of detection performance (on Reasonable) of boosted decision forests trained on individual CFMs. Note
that models with Conv3-x features works as sliding-window detectors, and models with Conv4-x and Conv5-x features are
applied to the proposals generated by CFM3. The top performing layers in each convolutional stack are Conv3-3, Conv4-3 and
Conv5-1 respectively. The models trained with these three layers are denoted as CFM3, CFM4, and CFM5 respectively

Convolutional # Channels Down-sampling Miss rate (%)

layer

ratio

Conv3-1
Conv3-2
Conv3-3 (CFM3)

Conv4-1
Conv4-2
Conv4-3 (CFM4)

Conv5-1 (CFM5)
Conv5-2
Conv5-3

256
256
256

512
512
512

512
512
512

4
4
4

8
8
8

16
16
16

19.15
16.25
13.49

12.95
12.68
12.21

14.17
14.56
18.24

Figure 1: The spatial distribution of CFMs selected by boosting algorithms. For a 128 × 64 input image, the size of feature
maps are 32 × 16, 16 × 8, 8 × 4 respectively. Red pixels indicate that a large number of features are selected in that region and
blue pixels correspond to low frequency regions. The most important region correspond to the head, shoulder, waist and feet of
a human.

stacks, convolutional layers in the same stack have the same down-sampling ratio. We ignore the CFMs of
the ﬁrst two convolutional stacks (each one contains 2 layers) since they are universal for most visual objects.
We train boosted decision forests with CFMs from individual convolutional layers of the VGG16 model
which is the one ﬁne-tuned with bootstrapped data (same as row 4 in Table 1). All boosted decision forests
are trained with the same data as CFM3. For models with Conv3-x features, the input image are directly
applied on the convolutional layers and resulting in a feature map with the down-sampling ratio of 4. The
corresponding boosted decision forests works as a sliding-window detector with step-size of 4. For models
with Conv4-x and Conv5-x features, they are applied to proposals generated by CFM3 model. This is due to
the large downsampling ratio of Conv4-x and Conv5-x. If the step size of the sliding-window detector is too
large, it will hurt the detection performance.

Table 2 shows the comparison of detection performance of these boosted decision forests on Caltech
Reasonable setting. We can observe that the MR is relatively high for the Conv3-1 layer and the Conv5-
3 layer. We conjecture that the Conv3-1 layer provides relatively low-level features which result in an
under-ﬁtting training.
In contrast, the semantic information in the Conv5-3 layer may be too coarse for
pedestrian detection. According to Table 2, the best performing layer in each convolutional stack, are from
the inner layers of Conv3-3 (CFM3), Conv4-3 (CFM4), and Conv5-1 (CFM5) respectively. Fig. 1 shows the spatial
distribution of convolutional features, which are frequently selected by above three CFM models. We observe
that most active regions correspond to important human-body parts (such as head and shoulder).

5

(a) conv3-3(b) conv4-3(c) conv5-1Figure 2: The framework of an ensemble of boosted decision forests with multi-layer CFMs (CFM3+CFM4+CFM5), which obtain
a 10.46 MR on the Caltech Reasonable test set.

Table 3: The comparison of performance (on Reasonable) of diﬀerent ensemble models. DCNN: the entire VGG16 model ﬁne-
tuned by data collected by CFM3. The combination of multi-layer CFM models improves the detection performance of single-layer
CFM models signiﬁcantly (3%)

Model combination
CFM3+CFM4
CFM3+CFM5
CFM3+CFM4+CFM5
CFM3+CFM4+CFM5+DCNN

Avg. miss rate (%)

10.68
10.88
10.46
10.07

The boosted decision forests trained with CFMs of these three layers are further combined together simply
through score averaging. Fig. 2 shows the framework of the resulting ensemble model. Firstly, CFM3 model
works as a sliding-window detector, which rejects the majority of negative examples and pass region proposals
to CFM4 and CFM5. Both CFM4 and CFM5 generate the conﬁdence score for each incoming proposal. The ﬁnal
score is computed by averaging over the scores output by these three boosted decision forests. This model
delivers the best reported log-average miss rate (10.46%) on Caltech Reasonable setting without using any
sophisticatedly designed algorithm.

We also evaluate other combinations of the ensemble models. Furthermore, a VGG16 model is ﬁne-tuned
with another round of bootstrapping (using CFM3) and its ﬁnal output is also combined to improve the
detection performance. The corresponding results can be found in Table 3 We can see that combining two
layers already beats all existing approaches on Caltech, and adding the entire large VGG16 model also gives
a small improvement.

3 Pixel Labelling Improves Pedestrian Detection

In this section, the sliding-window based detectors are enhanced by semantic pixel labelling. By incorporat-
ing DCNNs, the performance of pixel labelling (semantic image segmentation) methods have been recently
improved signiﬁcantly [32, 39, 30, 40, 41]. In general, we argue that pixel labelling models encode information
complementary to the sliding-window based detectors. Empirically, we show that consistent improvements
are achieved over diﬀerent types of detectors.

The segmentation method proposed in [39] is used here for pixel labelling, in which a DCNN model
(VGG16) is trained on the Cityscapes dataset [42]. The prediction map is reﬁned by a fully-connected
conditional random ﬁeld (CRF) [43] with DCNN responses as unary terms. The Cityscapes dataset that we
use for training is similar to the KITTI dataset which contains dense pixel annotations of 19 semantic classes
such as road, building, car, pedestrian, sky, etc. There is a large imbalance in their frequencies. Note that
our models that exploiting pixel labelling have used extra data for training on top of the Caltech dataset.
However, most deep learning based methods [1,2] have used extra data, at least the ImageNet dataset for pre-

6

Rejected  negatives VGG Conv3-3 Boosted Forest (CFM3) VGG Conv4-3 Boosted Forest (CFM4)  Boosted Forest (CFM5) Final Score Passed Proposals Sliding window Score 2 Score 3 Score 1 Score Averaging Input image VGG Conv5-1 Score1>0?	  Figure 3: The framework for pedestrian detection with pixel-labelling. The region proposals and pixel-level score maps are
obtained by individually applying the sliding-window detector and the pixel labelling model. Next, the weighted sum of pixel
scores within a proposal region is aggregated with the detector score of the same proposal region.

Table 4: Performance improvements by aggregating pixel labelling models with sliding-window detectors (on Reasonable). All
the three detectors achieve performance gains, which shows that pixel labelling can be used to help detection. Note that the
performance of our model ‘CFM3 with Pixel labelling’ is already on par with the previously best reported result of [1]

Method
ACF [5]
ACF+Pixel label.
Checkerboards [6]
Checkerboards+Pixel label.
CFM3 (ours)
CFM3+Pixel label.

Avg. miss rate (%)

Improve. (%)

22.23
18.33
18.25
15.09
13.49
11.96

3.90

3.16

1.53

training the deep model. Pedestrian detection may beneﬁt from the semantic pixel labelling in the following

trained with two-class data, the pixel labelling model carries richer object-level information.

aspects:− Multi-class information: Learning from multiple classes, in contrast to the object detectors typically
− Long-range context: Using CRFs (especially fully-connected CRFs) as post-processing procedure, many
models (for example, [39, 41, 40]) have the ability to capture long-range context information. In contrast,
sliding-window detectors only extract features from ﬁxed-sized bounding boxes.
− Object parts: The trained pixel labelling model may cater for more ﬁne-grained details, such that they

are more insensitive to deformation and occlusion to some extent.

However, it is not straightforward to apply pixel labelling models to pedestrian detection problems. One
of the main impediments is that it is diﬃcult to estimate the object bounding boxes from the pixel score
map, especially for people in crowds.

To this end, we propose to bring the pedestrian detector and pixel labelling model together.

In our
framework (see Fig. 3), a sliding-window detector is responsible for providing region proposals and a pixel
labelling model is applied to the input image at the same time to generate a score map for the “person”
class. Next, a weighted mask is applied to the region of a proposal of the “person” score map to generate the
weighted sum of pixel scores. Finally, the weighted sum and the detector score for the proposal are aggregated
together as the ﬁnal score. The weighted mask is computed by averaging the pixel scores of ground truth
region on the training images. To match the mask and the input proposals, we resize both ground truth and
test proposals to 100 × 41 pixels (no surrounding pixels). Note that, there are more sophisticated methods
for exploiting the labelling scores. For example, one can use the pixel labelling scores as the image features,
similar to ‘object bank’ [44], and train a linear model. In this work, we show that even simply weighted sum
of the pixel scores considerably improves the results.

7

DCNN + CRF (Pixel Labeling) Detector	  Weighted sum Aggregating Proposals Score Final  Score Score Map Input image Figure 4: Examples of some region proposals on the original images and the associated pixel score maps. A strong comple-
mentary relationship can be found in the generated proposals and the pixel score maps.

Table 5: Comparison of detection results of diﬀerent variants of the CFM3 detector (on Reasonable). The convolutional features
of the Conv3-3 layer are combined with diﬀerent types of hand-crafted features, and used to train a boosted decision forest.
Both the performance of the variants and the ensemble models is improved with these additional features. Flow: optical ﬂow
features. DCNN: the entire VGG16 model ﬁne-tuned by data collected by CFM3

Method

CFM3 only

CFM3+ACF

CFM3+ACF+Flow

(CFM3+ACF)+CFM4+CFM5+DCNN

(CFM3+ACF+Flow)+CFM4+CFM5+DCNN

Avg. miss rate (%)

13.49

12.38

11.11

9.37

9.32

Table 4 shows the detection performance of diﬀerent sliding-window detectors enhanced by pixel labelling.
Boosted decision forests are trained here with three types of features, which are ACF [5], checkerboard
features [6] and the CFM from the Conv3-3 layer of VGG16 model (CFM3). We can see that the performances
of all the three detectors are improved by aggregating pixel labelling models. Fig. 4 presents some region
proposals on the original images and the associated pixel score maps. Some of the false proposals generated by
pedestrian detectors (CFM3) can be removed by considering the context of a larger region (the largest bounding
box in the ﬁrst column in Fig. 4). Some occluded pedestrians have responses on the pixel score map (the
rightmost bounding box in the fourth column in Fig. 4). This clearly illustrates why this combination works.

4 Fusing Models

4.1 Using Complimentary Hand-crafted Features

The detection performance of the CFM3 model is critical in the proposed ensemble model, since later compo-
nents often reply on the detection results of this model. In order to enhance the detection performance of
the CFM3 model, we make two variants of it by combining two hand-crafted features: the ACF and optical
ﬂow. We augment the CFM3 features with the ACF and optical ﬂow features to train an ensemble of boosted
decision forests. Optical ﬂow features are extracted the same way as in [7].

Table 5 shows the detection results of diﬀerent variants of CFM3 model. With adding the ACF features, the
MR of CFM3 detector is reduce by 1.11%. With the extra optical ﬂow features, the MR is further reduced to
11.11%. These experimental results demonstrate that hand-crafted features carry complimentary information

8

(a) proposals by detector(b) pixel score mapTable 6: Comparison of detection performance (on Caltech Reasonable) of diﬀerent ensemble models with pixel labelling.
DCNN: the entire VGG16 model ﬁne-tuned by hard negative data collected by CFM3; Label.: pixel labelling model; Flow: optical
ﬂow. The pixel labelling model consistently improves all the considered models in this table. The All-in-one model set a new
record on the Caltech pedestrian benchmark

Method

CFM3+Pixel label.

CFM3+CFM4+CFM5+Pixel label.

CFM3+CFM4+CFM5+DCNN+Pixel label.

(CFM3+ACF)+CFM4+CFM5+
DCNN+Pixel label.

(CFM3+ACF+Flow)+CFM4+CFM5+
DCNN+Pixel label. (All-in-one)

Avg. miss rate (%)

11.96

10.06

9.65

9.07

8.93

Table 7: Ablation studies of the All-in-one model on the Caltech Reasonable test set

Model

CFM3a

CFM3

CFM3+CFM4

CFM3+CFM4

CFM3+CFM4

+CFM5

+CFM5+DCNN

CFM3+CFM4+CFM5
+DCNN+Label.

All-in-one

Pipeline

CFM3a

ﬁne-tuning

Add CFM4

Add CFM5

Add DCNN

Add Pixel label.

Use (CFM3+ACF+Flow)

Miss rate (%)

Improve. (%)

18.71
−

13.49

+5.22

10.68

+2.81

10.46

+0.22

10.07

+0.39

9.65

+0.32

8.93

+0.72

which can further improve the DCNN convolutional features. This is easy to understand: the ACF features
may be viewed as lower-level features, compared with the middle-level features in CFM3. The optical ﬂow
clearly encodes motion information which is not in CFM3 features. By adding the other components of the
proposed ensemble model, our detector can achieve 9.32% MR. The MR is slightly increased to 9.37% by
removing motion information.

4.2 Pixel Labelling

As shown in Section 3, the pixel labelling model is also complementary to convolutional features. Table 6
shows the detection performance of diﬀerent ensemble models enhanced by pixel labelling model. The best
result is achieved by combining the most number of diﬀerent types of models (which is refer to as All-in-one),
which reduces the MR on the Caltech Reasonable test set from the previous best 11.7% to 8.9%. Note that
the combination rule used by our methods is simple, which implies a potential for further improvement.

4.3 Ablation Studies

We investigate the overall pipeline of the All-in-one model by adding each component step by step, which
is shown in Table 7. As the start point, the CFM3a model with the original VGG16 model pre-trained on
ImageNet data achieves a miss rate of 18.71%. A 5.22% performance gain can be obtained by ﬁne-tuning
the VGG16 model with bootstrapped data. The detection results can be improved to 10.46% (better than
all previous methods) by adding CFM4 and CFM5 models to construct an ensemble model. We obtain 0.39%
performance improvement if we use the entire VGG16 model (ﬁne-tuned by bootstrapped data with CFM3)
as a component of our ensemble model. Combining the pixel labelling information to predicted bounding
boxes can further reduce the miss rate by 0.32%. By replacing the CFM3 model to CFM3+ACF+Flow model,
the MR of our ensemble mode can eventually achieve 8.93% on the Caltech Reasonable test set.

4.4 Fast Ensemble Models

In this section, we investigate the speed issue of the proposed detector. Our All-in-one model takes about 8s
for processing one 640 × 480 image on a workstation with one octa-core Intel Xeon 2.30GHz processor and

9

Table 8: Comparison of detection performance (on Reasonable) between the original ensemble model and fast ensemble models

Method

Avg. miss rate (%)

runtime (s)

CFM3 (proposals)+CFM4+CFM5+Pixel label.

ACF (proposals)+CFM3+CFM4+CFM5+DCNN+ Pixel label.

Checkerboards (proposals)+CFM3+CFM4+CFM5+DCNN+ Pixel label.

9.65

12.68

11.42

8

0.75

1.25

one Nvidia Tesla K40c GPU. Most of time (about 7s) is spent on the extraction of the CFMs on a multi-scale
image pyramid. The remaining components of the ensemble model takes less than 1s to process the passed
region proposals. The pixel labelling model only uses about 0.25s to process one image since it only need
to be applied on one scale. It can be easily observed that the current bottleneck of the proposed detector
is the CFM3 which is used to extract region proposals. The speed of our detector can be accelerated using a
light-weight proposal method at the start of the pipeline in Fig. 2.
We use two pedestrian detectors ACF [5] and checkerboards [6] as the proposal methods. Our ACF detec-
tor consists of 4096 depth-4 decision trees, trained via real-Adaboost. The model has size 128× 64 pixels, and
is trained via four rounds of bootstrapping. The sliding window stride is 4 pixels. The checkerboards detector
is trained using almost identical parameters as for ACF. The only diﬀerence is that the feature channels are
the results of convolving the ACF channels with a set of checkerboard ﬁlters. In our implementation, we
adopt a set of 12 binary 2× 2 ﬁlters to generate checkerboard feature channels. To limit the number of region
proposals, we set the threshold of the above two detectors to generate about 20 proposals per each image.

Table 8 shows the detection performance of the original ensemble model and fast ensemble models on
Caltech Reasonable test set. We can observe that the quality of proposals are enhanced by a large margin
using both ensemble models and the pixel labelling model. The best result of fast ensemble models is
achieved by using proposals generated by the checkerboards detector. With a negotiable performance loss
(e.g., 1.77%), it’s about 6 times faster than the original ensemble model. Note that the fast ensemble model
(with checkerboard proposals) has also outperformed the state-of-the-art.

4.5 Comparison to State-of-the-art Approaches

We compare the detection performance of our models with existing state-of-the-art approaches on the Caltech
dataset. Table 9 compares our models with a wide range of detectors, including boosted decision trees trained
on hand-crafted features, RCNN-based methods and the state-of-the-art methods on the Caltech Reasonable
test set. The performance of the ﬁrst two types are quite close to each other. Using only one single layer of
convolutional feature map, our CFM3 model has outperformed all other methods expect the two sophisticated
methods [2, 1]. Note that the RCNN based methods are based on larger models than CFM3. As feature
representation, the CFM from the Conv3-3 layer of our ﬁne-tuned model signiﬁcantly outperforms all other
hand-crafted features. The CFM3+Pixel labelling model is comparable to the state-of-the-art performance
achieved by sophisticated methods [2, 1]. Our CFM3+CFM4+CFM5 model performs even better. Without using
hand-crafted features, our model can achieve 9.65% MR. The best result is achieved by the All-in-one model
which combines a number of features and CFM models.

Fig. 5 shows a more complete evaluation of the proposed detection framework on various Caltech test
settings, including Reasonable, Medium scale, Partial occlusion, and Overall. We can observe that our ensemble
model achieves the best results on most test subsets (including Reasonable). On the Partial occlusion set, our
models are only outperformed by DeepParts [2], which is speciﬁcally trained for handling occlusions.

10

Table 9: Detection performance on Caltech Reasonable setting for diﬀerent types of detectors. Three types of approaches
are compared in this table, including boosted decision trees trained on hand-crafted features, RCNN-based methods and the
state-of-the-art sophisticated methods. All of our models outperform the ﬁrst three types of models, and our All-in-one set a
new recorded MR on Caltech pedestrian benchmark. † indicates the methods trained with optical ﬂow features

Type

Method

Miss Rate (%)

Hand-crafted Features

RCNN based

State-of-the-arts

Ours

SpatialPooling [38]
SpatialPooling+ [7]†
LDCF [45]
Checkerboards [6]
Checkerboards+ [6]†

AlexNet [17]
GoogLeNet [2]

DeepParts [2]
CompACT-Deep [1]

CFM3
CFM3+Label.
CFM3+CFM4+CFM5
CFM3+CFM4+CFM5+DCNN+Label.
All-in-one†

29.24
21.89
24.80
18.47
17.10

23.32
16.43

11.89
11.75

13.49
11.96
10.46
9.65
8.93

Figure 5: Comparison to state-of-the-art on various Caltech test settings.

11

(d) Overall (b) Medium scale10−310−210−1100101.05.10.20.30.40.50.64.801false positives per imagemiss rate99.53% VJ90.36% HOG74.04% SpatialPooling71.25% LDCF71.11% SpatialPooling+68.60% CCF+CF67.70% Checkerboards+66.73% CCF64.78% DeepParts64.63% Ours(CFM3+Label.)64.44% CompACT−Deep63.41% Ours(CFM3+CFM4+CFM5)62.60% Ours(All−in−one)10−310−210−1100101.05.10.20.30.40.50.64.801false positives per imagemiss rate98.67% VJ84.47% HOG52.52% SpatialPooling43.19% LDCF40.57% CCF39.25% SpatialPooling+37.69% CCF+CF31.35% Ours(CFM3+Label.)31.31% Checkerboards+29.91% Ours(CFM3+CFM4+CFM5)25.14% CompACT−Deep24.61% Ours(All−in−one)19.93% DeepParts10−310−210−1100101.05.10.20.30.40.50.64.801false positives per imagemiss rate99.38% VJ87.39% HOG65.49% SpatialPooling63.38% SpatialPooling+61.82% LDCF59.56% CCF+CF57.96% Checkerboards+56.42% DeepParts56.29% CCF55.48% Ours(CFM3+Label.)53.54% Ours(CFM3+CFM4+CFM5)53.23% CompACT−Deep52.70% Ours(All−in−one)(c) Partial occlusion(a) Reasonable10−310−210−1100101.05.10.20.30.40.50.64.801false positives per imagemiss rate94.73% VJ68.46% HOG29.24% SpatialPooling24.80% LDCF21.89% SpatialPooling+18.71% CCF17.32% CCF+CF17.10% Checkerboards+11.96% Ours(CFM3+Label.)11.89% DeepParts11.75% CompACT−Deep10.46% Ours(CFM3+CFM4+CFM5)8.93% Ours(All−in−one)Table 10: Comparison to state-of-the-art on the KITTI
dataset. Note: ∗ indicates the methods trained with
stereo images

Method

3DOP∗ [46]
CFMs (Ours)
Reionlets [47]

CompACT-Deep [1]

DeepParts [2]
FilteredICF [6]
pAUCEnsT [7]

R-CNN [17]

Moderate(%)

Easy(%) Hard(%)

67.47
63.26
61.15
58.74
58.67
56.75
54.49
50.13

81.78
74.22
73.14
70.69
70.49
67.65
65.26
61.61

64.70
56.44
55.21
52.71
52.78
51.12
48.60
44.79

Figure 6: Comparison to state-of-the-art on the KITTI
Moderate test set.

Table 10 shows the results on the KITTI dataset [36]. Since images of KITTI are larger than Caltech,
the feature extraction of CFM3 model is time-consuming. In our experiments, only the fast ensemble model
(Checkerboards (proposals) + CFM3 + CFM4 + CFM5 + DCNN + Label.) is used for testing on KITTI. Our
model achieves competitive results, 74.22%, 63.26%, and 56.44% AP on Easy, Moderate, and Hard subsets
respectively. Fig. 6 presents the comparison of detection performance on the KITTI Moderate test subset.
It can be observed that the proposed detector outperforms all published monocular-based methods. Note
that the 3DOP [46] is based on stereo images. The proposed ensemble model is the best-performing detector
based on DCNN, and surpasses CompACT-Deep [1] and DeepParts [2] by 4.52% and 4.59% respectively.

5 Conclusions

In this work, we have built a simple-yet-powerful pedestrian detector, which re-uses inner layers of convolu-
tional features extracted by a properly ﬁne-tuned VGG16 model. This ‘vanilla’ model has already achieved
the best reported results on the Caltech dataset, using the same training data as previous DCNN approaches.
With a few simple modiﬁcations, its variants have achieved even more signiﬁcant results.

We have presented extensive and systematic empirical evaluations on the eﬀectiveness of DCNN features
for pedestrian detection. We show that it is possible to build the best pedestrian detector, yet avoiding
complex custom designs. We also show that a pixel labelling model can be used to improve performance by
simply incorporating the labelling scores with the detection scores of a standard pedestrian detector. Note
that simple combination rules are used here, which leaves potentials for further improvement. For example
the ROI pooling for further speed and performance improvement.

References

[1] Cai, Z., Saberian, M., Vasconcelos, N.: Learning complexity-aware cascades for deep pedestrian detec-

tion. In: Proc. IEEE Int. Conf. Comp. Vis. (2015)

[2] Tian, Y., Luo, P., Wang, X., Tang, X.: Deep learning strong parts for pedestrian detection. In: Proc.

IEEE Int. Conf. Comp. Vis. (2015)

[3] Dalal, N., Triggs, B.: Histograms of oriented gradients for human detection. In: Proc. IEEE Conf.

Comp. Vis. Patt. Recogn. (2005)

[4] Shechtman, E., Irani, M.: Matching local self-similarities across images and videos. In: Proc. IEEE

Conf. Comp. Vis. Patt. Recogn. (2007)

[5] Doll´ar, P., Appel, R., Belongie, S., Perona, P.: Fast feature pyramids for object detection. IEEE Trans.

Pattern Anal. Mach. Intell. 36(8) (2014) 1532–1545

12

00.250.50.75100.250.50.751RecallPrecisionKITTI Pedestrian (moderate)  3DOP 67.47%Ours−CFM 63.26%Regionlets 61.15%CompACT−Deep 58.74%DeepParts 58.67%FilteredICF 56.75%pAUCEnsT 54.49R−CNN 50.13%[6] Zhang, S., Benenson, R., Schiele, B.: Filtered channel features for pedestrian detection. Proc. IEEE

Conf. Comp. Vis. Patt. Recogn. (2015)

[7] Paisitkriangkrai, S., Shen, C., Hengel, A.v.d.: Pedestrian detection with spatially pooled features and

structured ensemble learning. IEEE Trans. Pattern Anal. Mach. Intell. (2015)

[8] Krizhevsky, A., Sutskever, I., Hinton, G.E.:

Imagenet classiﬁcation with deep convolutional neural

networks. In: Proc. Adv. Neural Inf. Process. Syst. (2012)

[9] Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. In:

Proc. Int. Conf. Learning Representations. (2015)

[10] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabi-

novich, A.: Going deeper with convolutions. In: Proc. IEEE Conf. Comp. Vis. Patt. Recogn. (2015)

[11] Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection

and semantic segmentation. In: Proc. IEEE Conf. Comp. Vis. Patt. Recogn. (2014)

[12] Tompson, J.J., Jain, A., LeCun, Y., Bregler, C.:

Joint training of a convolutional network and a

graphical model for human pose estimation. In: Proc. Adv. Neural Inf. Process. Syst. (2014)

[13] Hariharan, B., Arbel´aez, P., Girshick, R., Malik, J.: Simultaneous detection and segmentation.

In:

Proc. Eur. Conf. Comp. Vis. (2014)

[14] Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action recognition in videos. In:

Proc. Adv. Neural Inf. Process. Syst. (2014)

[15] Branson, S., Van Horn, G., Belongie, S., Perona, P.: Bird species categorization using pose normalized

deep convolutional nets. In: Proc. Bri. Conf. Mach. Vis. (2014)

[16] Krizhevsky, A., Hinton, G.: Learning multiple layers of features from tiny images. Technical report,

University of Toronto (2009)

[17] Hosang, J., Omran, M., Benenson, R., Schiele, B.: Taking a deeper look at pedestrians. Proc. IEEE

Conf. Comp. Vis. Patt. Recogn. (2015)

[18] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image

database. In: Proc. IEEE Conf. Comp. Vis. Patt. Recogn. (2009)

[19] Enzweiler, M., Eigenstetter, A., Schiele, B., Gavrila, D.M.: Multi-cue pedestrian classiﬁcation with

partial occlusion handling. In: Proc. IEEE Conf. Comp. Vis. Patt. Recogn. (2010) 990–997

[20] Felzenszwalb, P.F., Girshick, R.B., McAllester, D., Ramanan, D.: Object detection with discriminatively

trained part-based models. IEEE Trans. Pattern Anal. Mach. Intell. 32(9) (2010) 1627–1645

[21] Lin, L., Wang, X., Yang, W., Lai, J.H.: Discriminatively trained and-or graph models for object shape

detection. IEEE Trans. Pattern Anal. Mach. Intell. 37(5) (2015) 959–972

[22] Girshick, R., Iandola, F., Darrell, T., Malik, J.: Deformable part models are convolutional neural

networks. Proc. IEEE Conf. Comp. Vis. Patt. Recogn. (2015)

[23] Mathias, M., Benenson, R., Timofte, R., Van Gool, L.: Handling occlusions with franken-classiﬁers. In:

Proc. IEEE Int. Conf. Comp. Vis. (2013) 1505–1512

[24] Ouyang, W., Wang, X.: Single-pedestrian detection aided by multi-pedestrian detection.

In: Proc.

IEEE Conf. Comp. Vis. Patt. Recogn. (2013)

[25] Tang, S., Andriluka, M., Schiele, B.: Detection and tracking of occluded people. Int. J. Comp. Vis.

110(1) (2014) 58–69

[26] Ouyang, W., Wang, X.: A discriminative deep model for pedestrian detection with occlusion handling.

In: Proc. IEEE Conf. Comp. Vis. Patt. Recogn. (2012)

13

[27] Ouyang, W., Wang, X.: Joint deep learning for pedestrian detection. In: Proc. IEEE Int. Conf. Comp.

Vis. (2013)

[28] Luo, P., Tian, Y., Wang, X., Tang, X.: Switchable deep network for pedestrian detection. In: Proc.

IEEE Conf. Comp. Vis. Patt. Recogn. (2014)

[29] Ren, S., He, K., Girshick, R., Zhang, X., Sun, J.: Object detection networks on convolutional feature

maps. arXiv:1504.06066 (2015)

[30] Hariharan, B., Arbel´aez, P., Girshick, R., Malik, J.: Hypercolumns for object segmentation and ﬁne-

grained localization. Proc. IEEE Conf. Comp. Vis. Patt. Recogn. (2015)

[31] Yang, B., Yan, J., Lei, Z., Li, S.Z.: Convolutional channel features. Proc. IEEE Int. Conf. Comp. Vis.

(2015)

[32] Long, J., Shelhamer, E., Darrell, T.:

Fully convolutional networks for semantic segmentation.

arXiv:1411.4038 (2014)

[33] Yan, J., Yu, Y., Zhu, X., Lei, Z., Li, S.Z.: Object detection by labeling superpixels. In: Proc. IEEE

Conf. Comp. Vis. Patt. Recogn. (2015)

[34] Fidler, S., Mottaghi, R., Urtasun, R., et al.: Bottom-up segmentation for top-down detection. In: Proc.

IEEE Conf. Comp. Vis. Patt. Recogn. (2013)

[35] Dollar, P., Wojek, C., Schiele, B., Perona, P.: Pedestrian detection: An evaluation of the state of the

art. IEEE Trans. Pattern Anal. Mach. Intell. 34(4) (2012) 743–761

[36] Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? the kitti vision benchmark

suite. In: Proc. IEEE Conf. Comp. Vis. Patt. Recogn. (2012)

[37] Hastie, T., Tibshirani, R., Friedman, J., Franklin, J.: The elements of statistical learning: data mining,

inference and prediction. The Mathematical Intelligencer 27(2) (2005) 83–85

[38] Paisitkriangkrai, S., Shen, C., van den Hengel, A.: Strengthening the eﬀectiveness of pedestrian detection

with spatially pooled features. In: Proc. Eur. Conf. Comp. Vis. (2014)

[39] Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Semantic image segmentation
with deep convolutional nets and fully connected CRFs. In: Proc. Int. Conf. Learning Representations.
(2015)

[40] Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang, C., Torr, P.:

Conditional random ﬁelds as recurrent neural networks. arXiv:1502.03240 (2015)

[41] Lin, G., Shen, C., Reid, I., et al.: Eﬃcient piecewise training of deep structured models for semantic

segmentation. arXiv:1504.01013 (2015)

[42] Cordts, M., Omran, M., Ramos, S., Scharw¨achter, T., Enzweiler, M., Benenson, R., Franke, U., Roth,
S., Schiele, B.: The cityscapes dataset. In: Proc. IEEE Conf. Comp. Vis. Patt. Recogn. Workshops.
(2015)

[43] Kr¨ahenb¨uhl, P., Koltun, V.: Eﬃcient inference in fully connected CRFs with Gaussian edge potentials.

In: Proc. Adv. Neural Inf. Process. Syst. (2011)

[44] Li, L.J., Su, H., Lim, Y., Fei-Fei, L.: Object bank: An object-level image representation for high-level

visual recognition. Int. J. Comput. Vision 107(1) (2014) 20–39

[45] Nam, W., Doll´ar, P., Han, J.H.: Local decorrelation for improved pedestrian detection. In: Proc. Adv.

Neural Inf. Process. Syst. (2014)

[46] Chen, X., Kundu, K., Zhu, Y., Berneshawi, A.G., Ma, H., Fidler, S., Urtasun, R.: 3d object proposals

for accurate object class detection. In: Proc. Adv. Neural Inf. Process. Syst. (2015) 424–432

14

[47] Wang, X., Yang, M., Zhu, S., Lin, Y.: Regionlets for generic object detection. In: Proc. IEEE Int. Conf.

Comp. Vis. (2013) 17–24

15

