End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF

Xuezhe Ma and Eduard Hovy
Language Technologies Institute

Carnegie Mellon University
Pittsburgh, PA 15213, USA

xuezhem@cs.cmu.edu, ehovy@cmu.edu

Abstract

State-of-the-art sequence labeling systems
traditionally require large amounts of task-
speciﬁc knowledge in the form of hand-
crafted features and data pre-processing.
In this paper, we introduce a novel neu-
tral network architecture that beneﬁts from
both word- and character-level representa-
tions automatically, by using combination
of bidirectional LSTM, CNN and CRF.
Our system is truly end-to-end, requir-
ing no feature engineering or data pre-
processing, thus making it applicable to
a wide range of sequence labeling tasks
on different languages. We evaluate our
system on two data sets for two sequence
labeling tasks — Penn Treebank WSJ
corpus for part-of-speech (POS) tagging
and CoNLL 2003 corpus for named en-
tity recognition (NER). We obtain state-
of-the-art performance on both datasets
— 97.55% accuracy for POS tagging and
91.21% F1 for NER.

Introduction

1
Linguistic sequence labeling, such as part-of-
speech (POS) tagging and named entity recogni-
tion (NER), is one of the ﬁrst stages in deep lan-
guage understanding and its importance has been
well recognized in the natural language processing
community. Natural language processing (NLP)
systems, like syntactic parsing (Nivre and Scholz,
2004; McDonald et al., 2005; Koo and Collins,
2010; Ma and Zhao, 2012a; Ma and Zhao, 2012b;
Chen and Manning, 2014; Ma and Hovy, 2015)
and entity coreference resolution (Ng, 2010; Ma
et al., 2016), are becoming more sophisticated,
in part because of utilizing output information of
POS tagging or NER systems.

Most traditional high performance sequence la-
beling models are linear statistical models, includ-
ing Hidden Markov Models (HMM) and Condi-
tional Random Fields (CRF) (Ratinov and Roth,
2009; Passos et al., 2014; Luo et al., 2015), which
rely heavily on hand-crafted features and task-
speciﬁc resources. For example, English POS tag-
gers beneﬁt from carefully designed word spelling
features; orthographic features and external re-
sources such as gazetteers are widely used in NER.
However, such task-speciﬁc knowledge is costly
to develop (Ma and Xia, 2014), making sequence
labeling models difﬁcult to adapt to new tasks or
new domains.

In the past few years, non-linear neural net-
works with as input distributed word representa-
tions, also known as word embeddings, have been
broadly applied to NLP problems with great suc-
cess. Collobert et al. (2011) proposed a simple but
effective feed-forward neutral network that inde-
pendently classiﬁes labels for each word by us-
ing contexts within a window with ﬁxed size. Re-
cently, recurrent neural networks (RNN) (Goller
and Kuchler, 1996), together with its variants such
as long-short term memory (LSTM) (Hochreiter
and Schmidhuber, 1997; Gers et al., 2000) and
gated recurrent unit (GRU) (Cho et al., 2014), have
shown great success in modeling sequential data.
Several RNN-based neural network models have
been proposed to solve sequence labeling tasks
like speech recognition (Graves et al., 2013), POS
tagging (Huang et al., 2015) and NER (Chiu and
Nichols, 2015; Lample et al., 2016), achieving
competitive performance against traditional mod-
els. However, even systems that have utilized dis-
tributed representations as inputs have used these
to augment, rather than replace, hand-crafted fea-
tures (e.g. word spelling and capitalization pat-
terns). Their performance drops rapidly when the
models solely depend on neural embeddings.

6
1
0
2

 
r
a

 

M
4
1

 
 
]

G
L
.
s
c
[
 
 

4
v
4
5
3
1
0

.

3
0
6
1
:
v
i
X
r
a

In this paper, we propose a neural network ar-
chitecture for sequence labeling. It is a truly end-
to-end model requiring no task-speciﬁc resources,
feature engineering, or data pre-processing be-
yond pre-trained word embeddings on unlabeled
corpora. Thus, our model can be easily applied
to a wide range of sequence labeling tasks on dif-
ferent languages and domains. We ﬁrst use con-
volutional neural networks (CNNs) (LeCun et al.,
1989) to encode character-level information of a
word into its character-level representation. Then
we combine character- and word-level represen-
tations and feed them into bi-directional LSTM
(BLSTM) to model context information of each
word. On top of BLSTM, we use a sequential
CRF to jointly decode labels for the whole sen-
tence. We evaluate our model on two linguistic
sequence labeling tasks — POS tagging on Penn
Treebank WSJ (Marcus et al., 1993), and NER
on English data from the CoNLL 2003 shared
task (Tjong Kim Sang and De Meulder, 2003).
Our end-to-end model outperforms previous state-
of-the-art systems, obtaining 97.55% accuracy for
POS tagging and 91.21% F1 for NER. The con-
tributions of this work are (i) proposing a novel
neural network architecture for linguistic sequence
labeling. (ii) giving empirical evaluations of this
model on benchmark data sets for two classic NLP
tasks. (iii) achieving state-of-the-art performance
with this truly end-to-end system.

2 Neural Network Architecture

In this section, we describe the components (lay-
ers) of our neural network architecture. We intro-
duce the neural layers in our neural network one-
by-one from bottom to top.

2.1 CNN for Character-level Representation
Previous studies (Santos and Zadrozny, 2014;
Chiu and Nichols, 2015) have shown that CNN
is an effective approach to extract morphological
information (like the preﬁx or sufﬁx of a word)
from characters of words and encode it into neural
representations. Figure 1 shows the CNN we use
to extract character-level representation of a given
word. The CNN is similar to the one in Chiu and
Nichols (2015), except that we use only character
embeddings as the inputs to CNN, without char-
acter type features. A dropout layer (Srivastava et
al., 2014) is applied before character embeddings
are input to CNN.

Figure 1: The convolution neural network for ex-
tracting character-level representations of words.
Dashed arrows indicate a dropout layer applied be-
fore character embeddings are input to CNN.

2.2 Bi-directional LSTM
2.2.1 LSTM Unit
Recurrent neural networks (RNNs) are a powerful
family of connectionist models that capture time
dynamics via cycles in the graph. Though, in the-
ory, RNNs are capable to capturing long-distance
dependencies, in practice, they fail due to the gra-
dient varnishing/exploding problems (Bengio et
al., 1994; Pascanu et al., 2012).

LSTMs (Hochreiter and Schmidhuber, 1997)
are a variant of RNNs designed to cope with
these gradient varnishing problems. Basically, a
LSTM unit is composed of three multiplicative
gates which control the proportions of information
to forget and to pass on to the next time step. Fig-
ure 2 gives the basic structure of an LSTM unit.

Figure 2: Schematic of LSTM unit.

PlayingPaddingPaddingChar EmbeddingConvolutionMax PoolingChar RepresentationFormally, the formulas to update an LSTM unit

at time t are:

it = σ(W iht−1 + U ixt + bi)
ft = σ(W f ht−1 + U f xt + bf )
˜ct = tanh(W cht−1 + U cxt + bc)
ct = ft (cid:12) ct−1 + it (cid:12) ˜ct
ot = σ(W oht−1 + U oxt + bo)
ht = ot (cid:12) tanh(ct)

where σ is the element-wise sigmoid function
and (cid:12) is the element-wise product. xt is the
time
input vector (e.g. word embedding) at
t, and ht is the hidden state (also called out-
put) vector storing all the useful information at
(and before) time t. U i, U f , U c, U o denote the
weight matrices of different gates for input xt,
and W i, W f , W c, W o are the weight matrices
for hidden state ht. bi, bf , bc, bo denote the bias
vectors. It should be noted that we do not include
peephole connections (Gers et al., 2003) in the our
LSTM formulation.

2.2.2 BLSTM
For many sequence labeling tasks it is beneﬁ-
cial to have access to both past (left) and future
(right) contexts. However, the LSTM’s hidden
state ht takes information only from past, know-
ing nothing about the future. An elegant solution
whose effectiveness has been proven by previous
work (Dyer et al., 2015) is bi-directional LSTM
(BLSTM). The basic idea is to present each se-
quence forwards and backwards to two separate
hidden states to capture past and future informa-
tion, respectively. Then the two hidden states are
concatenated to form the ﬁnal output.

2.3 CRF
For sequence labeling (or general structured pre-
diction) tasks, it is beneﬁcial to consider the cor-
relations between labels in neighborhoods and
jointly decode the best chain of labels for a given
input sentence. For example, in POS tagging an
adjective is more likely to be followed by a noun
than a verb, and in NER with standard BIO2 an-
notation (Tjong Kim Sang and Veenstra, 1999)
I-ORG cannot follow I-PER. Therefore, we model
label sequence jointly using a conditional random
ﬁeld (CRF) (Lafferty et al., 2001), instead of de-
coding each label independently.
Formally, we use z = {z1,··· , zn} to repre-
sent a generic input sequence where zi is the input

vector of the ith word. y = {y1,··· , yn} rep-
resents a generic sequence of labels for z. Y(z)
denotes the set of possible label sequences for z.
The probabilistic model for sequence CRF deﬁnes
a family of conditional probability p(y|z; W, b)
over all possible label sequences y given z with
the following form:

p(y|z; W, b) =

n(cid:81)
(cid:80)

i=1

n(cid:81)

ψi(yi−1, yi, z)
i−1, y(cid:48)

ψi(y(cid:48)

i, z)

y(cid:48)∈Y(z)

i=1

where ψi(y(cid:48), y, z) = exp(WT
y(cid:48),yzi + by(cid:48),y) are
potential functions, and WT
y(cid:48),y and by(cid:48),y are the
weight vector and bias corresponding to label pair
(y(cid:48), y), respectively.

For CRF training, we use the maximum con-
ditional likelihood estimation. For a training set
{(zi, yi)}, the logarithm of the likelihood (a.k.a.
the log-likelihood) is given by:

L(W, b) =

log p(y|z; W, b)

(cid:88)

i

Maximum likelihood training chooses parameters
such that the log-likelihood L(W, b) is maxi-
mized.
Decoding is to search for the label sequence y∗

with the highest conditional probability:
p(y|z; W, b)

y∗ = argmax
y∈Y(z)

For a sequence CRF model (only interactions be-
tween two successive labels are considered), train-
ing and decoding can be solved efﬁciently by
adopting the Viterbi algorithm.

2.4 BLSTM-CNNs-CRF
Finally, we construct our neural network model by
feeding the output vectors of BLSTM into a CRF
layer. Figure 3 illustrates the architecture of our
network in detail.
For each word,

the character-level represen-
tation is computed by the CNN in Figure 1
with character embeddings as inputs. Then the
character-level representation vector is concate-
nated with the word embedding vector to feed into
the BLSTM network. Finally, the output vectors
of BLSTM are fed to the CRF layer to jointly de-
code the best label sequence. As shown in Fig-
ure 3, dropout layers are applied on both the in-
put and output vectors of BLSTM. Experimen-
tal results show that using dropout signiﬁcantly

We also run experiments on two other sets
of published embeddings, namely Senna 50-
dimensional embeddings2 trained on Wikipedia
and Reuters RCV-1 corpus (Collobert et al., 2011),
and Google’s Word2Vec 300-dimensional embed-
dings3 trained on 100 billion words from Google
News (Mikolov et al., 2013). To test the effec-
tiveness of pretrained word embeddings, we ex-
perimented with randomly initialized embeddings
with 100 dimensions, where embeddings are uni-
dim ]
where dim is the dimension of embeddings (He
et al., 2015). The performance of different word
embeddings is discussed in Section 4.4.
Character Embeddings.
Character embed-
dings are initialized with uniform samples from

formly sampled from range [−(cid:113) 3

(cid:113) 3

dim , +

dim ], where we set dim = 30.

[−(cid:113) 3
(cid:113) 3
samples from [−(cid:113) 6

dim , +

(cid:113) 6

r+c , +

Weight Matrices and Bias Vectors. Matrix pa-
rameters are randomly initialized with uniform
r+c ], where r and c
are the number of of rows and columns in the
structure (Glorot and Bengio, 2010). Bias vec-
tors are initialized to zero, except the bias bf for
the forget gate in LSTM , which is initialized to
1.0 (Jozefowicz et al., 2015).

3.2 Optimization Algorithm
Parameter optimization is performed with mini-
batch stochastic gradient descent (SGD) with
batch size 10 and momentum 0.9. We choose an
initial learning rate of η0 (η0 = 0.01 for POS tag-
ging, and 0.015 for NER, see Section 3.3.), and the
learning rate is updated on each epoch of training
as ηt = η0/(1 + ρt), with decay rate ρ = 0.05 and
t is the number of epoch completed. To reduce the
effects of “gradient exploding”, we use a gradient
clipping of 5.0 (Pascanu et al., 2012). We explored
other more sophisticated optimization algorithms
such as AdaDelta (Zeiler, 2012), Adam (Kingma
and Ba, 2014) or RMSProp (Dauphin et al., 2015),
but none of them meaningfully improve upon SGD
with momentum and gradient clipping in our pre-
liminary experiments.
Early Stopping. We use early stopping (Giles,
2001; Graves et al., 2013) based on performance
on validation sets. The “best” parameters appear at
around 50 epochs, according to our experiments.

2http://ronan.collobert.com/senna/
3https://code.google.com/archive/p/

word2vec/

Figure 3: The main architecture of our neural
network. The character representation for each
word is computed by the CNN in Figure 1. Then
the character representation vector is concatenated
with the word embedding before feeding into the
BLSTM network. Dashed arrows indicate dropout
layers applied on both the input and output vectors
of BLSTM.

improve the performance of our model (see Sec-
tion 4.5 for details).

3 Network Training
In this section, we provide details about training
the neural network. We implement the neural net-
work using the Theano library (Bergstra et al.,
2010). The computations for a single model are
run on a GeForce GTX TITAN X GPU. Using the
settings discussed in this section, the model train-
ing requires about 12 hours for POS tagging and 8
hours for NER.

3.1 Parameter Initialization
Word Embeddings. We use Stanford’s pub-
licly available GloVe 100-dimensional embed-
dings1 trained on 6 billion words from Wikipedia
and web text (Pennington et al., 2014)

1http://nlp.stanford.edu/projects/

glove/

WeWordEmbeddingareplayingsoccerCharRepresentationForwardLSTMBackwardLSTMLSTMLSTMLSTMLSTMLSTMLSTMLSTMLSTMPRPVBPNNVBGCRFLayerLayer

CNN

LSTM

Dropout

Hyper-parameter POS NER
window size
3
30
number of ﬁlters
200
state size
initial state
0.0
no
peepholes
0.5
dropout rate
10
batch size
initial learning rate
0.015
0.05
decay rate
gradient clipping
5.0

3
30
200
0.0
no
0.5
10
0.01
0.05
5.0

Table 1: Hyper-parameters for all experiments.

Fine Tuning. For each of the embeddings, we
ﬁne-tune initial embeddings, modifying them dur-
ing gradient updates of the neural network model
by back-propagating gradients. The effectiveness
of this method has been previously explored in se-
quential and structured prediction problems (Col-
lobert et al., 2011; Peng and Dredze, 2015).
Dropout Training. To mitigate overﬁtting, we ap-
ply the dropout method (Srivastava et al., 2014) to
regularize our model. As shown in Figure 1 and 3,
we apply dropout on character embeddings before
inputting to CNN, and on both the input and out-
put vectors of BLSTM. We ﬁx dropout rate at 0.5
for all dropout layers through all the experiments.
We obtain signiﬁcant improvements on model per-
formance after using dropout (see Section 4.5).

3.3 Tuning Hyper-Parameters
Table 1 summarizes the chosen hyper-parameters
for all experiments. We tune the hyper-parameters
on the development sets by random search. Due
to time constrains it is infeasible to do a ran-
dom search across the full hyper-parameter space.
Thus, for the tasks of POS tagging and NER we
try to share as many hyper-parameters as possible.
Note that the ﬁnal hyper-parameters for these two
tasks are almost the same, except the initial learn-
ing rate. We set the state size of LSTM to 200.
Tuning this parameter did not signiﬁcantly impact
the performance of our model. For CNN, we use
30 ﬁlters with window length 3.

4 Experiments
4.1 Data Sets
As mentioned before, we evaluate our neural net-
work model on two sequence labeling tasks: POS
tagging and NER.

Dataset

Train

Dev

Test

WSJ
SENT
38,219
TOKEN 912,344
SENT
5,527
TOKEN 131,768
SENT
5,462
TOKEN 129,654

CoNLL2003

14,987
204,567
3,466
51,578
3,684
46,666

Table 2: Corpora statistics. SENT and TOKEN
refer to the number of sentences and tokens in each
data set.

POS Tagging. For English POS tagging, we use
the Wall Street Journal (WSJ) portion of Penn
Treebank (PTB) (Marcus et al., 1993), which con-
tains 45 different POS tags.
In order to com-
pare with previous work, we adopt the standard
splits — section 0–18 as training data, section 19–
21 as development data and section 22–24 as test
data (Manning, 2011; Søgaard, 2011).
NER. For NER, We perform experiments on
the English data from CoNLL 2003 shared
task (Tjong Kim Sang and De Meulder, 2003).
This data set contains four different
types of
named entities: PERSON, LOCATION, ORGA-
NIZATION, and MISC. We use the BIOES tag-
ging scheme instead of standard BIO2, as pre-
vious studies have reported meaningful improve-
ment with this scheme (Ratinov and Roth, 2009;
Dai et al., 2015; Lample et al., 2016).

The corpora statistics are shown in Table 2. We
did not perform any pre-processing for data sets,
leaving our system truly end-to-end.

4.2 Main Results
We ﬁrst run experiments to dissect the effective-
ness of each component (layer) of our neural net-
work architecture by ablation studies. We com-
pare the performance with three baseline systems
— BRNN, the bi-direction RNN; BLSTM, the bi-
direction LSTM, and BLSTM-CNNs, the combi-
nation of BLSTM with CNN to model character-
level information. All these models are run using
Stanford’s GloVe 100 dimensional word embed-
dings and the same hyper-parameters as shown in
Table 1. According to the results shown in Ta-
ble 3, BLSTM obtains better performance than
BRNN on all evaluation metrics of both the two
tasks. BLSTM-CNN models signiﬁcantly outper-
form the BLSTM model, showing that character-
level representations are important for linguistic
sequence labeling tasks. This is consistent with

POS

Dev
Model
Acc.
96.56
BRNN
96.88
BLSTM
BLSTM-CNN
97.34
BRNN-CNN-CRF 97.46

Test
Acc.
96.76
96.93
97.33
97.55

Dev
Prec. Recall
89.13
92.04
92.31
90.85
93.64
92.52
94.85
94.63

NER

Test
Prec. Recall
83.88
87.05
87.77
86.23
90.21
88.53
91.35
91.06

F1
90.56
91.57
93.07
94.74

F1
85.44
87.00
89.36
91.21

Table 3: Performance of our model on both the development and test sets of the two tasks, together with
three baseline systems.

Model
Gim´enez and M`arquez (2004)
Toutanova et al. (2003)
Manning (2011)
Collobert et al. (2011)‡
Santos and Zadrozny (2014)‡
Shen et al. (2007)
Sun (2014)
Søgaard (2011)
This paper

Acc.
97.16
97.27
97.28
97.29
97.32
97.33
97.36
97.50
97.55

Table 4: POS tagging accuracy of our model on
test data from WSJ proportion of PTB, together
with top-performance systems. The neural net-
work based models are marked with ‡.

results reported by previous work (Santos and
Zadrozny, 2014; Chiu and Nichols, 2015). Fi-
nally, by adding CRF layer for joint decoding we
achieve signiﬁcant improvements over BLSTM-
CNN models for both POS tagging and NER on
all metrics. This demonstrates that jointly decod-
ing label sequences can signiﬁcantly beneﬁt the ﬁ-
nal performance of neural network models.

4.3 Comparison with Previous Work
4.3.1 POS Tagging
Table 4 illustrates the results of our model for
POS tagging, together with seven previous top-
performance systems for comparison. Our model
signiﬁcantly outperform Senna (Collobert et al.,
2011), which is a feed-forward neural network
model using capitalization and discrete sufﬁx fea-
tures, and data pre-processing. Moreover, our
model achieves 0.23% improvements on accu-
racy over the “CharWNN” (Santos and Zadrozny,
2014), which is a neural network model based on
Senna and also uses CNNs to model character-
level representations. This demonstrates the effec-
tiveness of BLSTM for modeling sequential data

Model
Chieu and Ng (2002)
Florian et al. (2003)
Ando and Zhang (2005)
Collobert et al. (2011)‡
Huang et al. (2015)‡
Chiu and Nichols (2015)‡
Ratinov and Roth (2009)
Lin and Wu (2009)
Passos et al. (2014)
Lample et al. (2016)‡
Luo et al. (2015)
This paper

F1
88.31
88.76
89.31
89.59
90.10
90.77
90.80
90.90
90.90
90.94
91.20
91.21

Table 5: NER F1 score of our model on test data
set from CoNLL-2003. For the purpose of com-
parison, we also list F1 scores of previous top-
performance systems. ‡ marks the neural models.

and the importance of joint decoding with struc-
tured prediction model.

Comparing with traditional statistical models,
our system achieves state-of-the-art accuracy, ob-
taining 0.05% improvement over the previously
best reported results by Søgaard (2011). It should
be noted that Huang et al. (2015) also evaluated
their BLSTM-CRF model for POS tagging on
WSJ corpus. But they used a different splitting of
the training/dev/test data sets. Thus, their results
are not directly comparable with ours.

4.3.2 NER
Table 5 shows the F1 scores of previous models
for NER on the test data set from CoNLL-2003
shared task. For the purpose of comparison, we
list their results together with ours. Similar to the
observations of POS tagging, our model achieves
signiﬁcant improvements over Senna and the other
three neural models, namely the LSTM-CRF pro-
posed by Huang et al. (2015), LSTM-CNNs pro-

Embedding Dimension
Random
Senna
Word2Vec
GloVe

100
50
300
100

POS NER
97.13
80.76
90.28
97.44
84.91
97.40
97.55
91.21

Table 6: Results with different choices of word
embeddings on the two tasks (accuracy for POS
tagging and F1 for NER).

posed by Chiu and Nichols (2015), and the LSTM-
CRF by Lample et al. (2016). Huang et al. (2015)
utilized discrete spelling, POS and context fea-
tures, Chiu and Nichols (2015) used character-
type, capitalization, and lexicon features, and all
the three model used some task-speciﬁc data pre-
processing, while our model does not require any
carefully designed features or data pre-processing.
We have to point out that the result (90.77%) re-
ported by Chiu and Nichols (2015) is incompa-
rable with ours, because their ﬁnal model was
trained on the combination of the training and de-
velopment data sets4.

To our knowledge, the previous best F1 score
(91.20)5 reported on CoNLL 2003 data set is by
the joint NER and entity linking model (Luo et
al., 2015). This model used many hand-crafted
features including stemming and spelling features,
POS and chunks tags, WordNet clusters, Brown
Clusters, as well as external knowledge bases such
as Freebase and Wikipedia. Our end-to-end model
slightly improves this model by 0.01%, yielding a
state-of-the-art performance.

4.4 Word Embeddings
As mentioned in Section 3.1, in order to test the
importance of pretrained word embeddings, we
performed experiments with different sets of pub-
licly published word embeddings, as well as a ran-
dom sampling method, to initialize our model. Ta-
ble 6 gives the performance of three different word
embeddings, as well as the randomly sampled one.
According to the results in Table 6, models using
pretrained word embeddings obtain a signiﬁcant
improvement as opposed to the ones using random
embeddings. Comparing the two tasks, NER relies

4We run experiments using the same setting and get

91.37% F1 score.

5Numbers are taken from the Table 3 of the original pa-
per (Luo et al., 2015). While there is clearly inconsistency
among the precision (91.5%), recall (91.4%) and F1 scores
(91.2%), it is unclear in which way they are incorrect.

Train
98.46
97.86

No
Yes

POS
Dev
97.06
97.46

Test
97.11
97.55

Train
99.97
99.63

NER
Dev
93.51
94.74

Test
89.25
91.21

Table 7: Results with and without dropout on two
tasks (accuracy for POS tagging and F1 for NER).

POS

NER

Dev

127,247
2,960
659
902

Test

125,826
2,412
588
828

Dev
4,616
1,087

44
195

Test
3,773
1,597

8
270

IV
OOTV
OOEV
OOBV

Table 8: Statistics of the partition on each corpus.
It lists the number of tokens of each subset for POS
tagging and the number of entities for NER.

more heavily on pretrained embeddings than POS
tagging. This is consistent with results reported
by previous work (Collobert et al., 2011; Huang et
al., 2015; Chiu and Nichols, 2015).

For different pretrained embeddings, Stanford’s
GloVe 100 dimensional embeddings achieve best
results on both tasks, about 0.1% better on POS
accuracy and 0.9% better on NER F1 score than
the Senna 50 dimensional one.
This is dif-
ferent from the results reported by Chiu and
Nichols (2015), where Senna achieved slightly
better performance on NER than other embed-
dings. Google’s Word2Vec 300 dimensional em-
beddings obtain similar performance with Senna
on POS tagging, still slightly behind GloVe. But
for NER, the performance on Word2Vec is far be-
hind GloVe and Senna. One possible reason that
Word2Vec is not as good as the other two embed-
dings on NER is because of vocabulary mismatch
— Word2Vec embeddings were trained in case-
sensitive manner, excluding many common sym-
bols such as punctuations and digits. Since we do
not use any data pre-processing to deal with such
common symbols or rare words, it might be an is-
sue for using Word2Vec.

4.5 Effect of Dropout
Table 7 compares the results with and without
dropout layers for each data set. All other hyper-
parameters remain the same as in Table 1. We
observe a essential improvement for both the two
tasks. It demonstrates the effectiveness of dropout
in reducing overﬁtting.

IV
LSTM-CNN
97.57
LSTM-CNN-CRF 97.68

POS

Dev

OOTV OOEV OOBV
93.75
80.27
82.71
93.65

90.29
91.05

IV
97.55
97.77

NER

Test

OOTV OOEV OOBV
93.45
80.07
82.49
93.16

90.14
90.65

Dev

Test

IV
94.83
LSTM-CNN
LSTM-CNN-CRF 96.49

OOTV OOEV OOBV
87.28
82.90
86.91
88.63

96.55
97.67

IV
90.07
92.14

OOTV OOEV OOBV
89.45
78.44
80.60
90.73

100.00
100.00

Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).

4.6 OOV Error Analysis
To better understand the behavior of our model,
we perform error analysis on Out-of-Vocabulary
words (OOV). Speciﬁcally, we partition each
data set into four subsets — in-vocabulary words
(IV), out-of-training-vocabulary words (OOTV),
out-of-embedding-vocabulary words (OOEV) and
out-of-both-vocabulary words (OOBV). A word is
considered IV if it appears in both the training
and embedding vocabulary, while OOBV if nei-
ther. OOTV words are the ones do not appear in
training set but in embedding vocabulary, while
OOEV are the ones do not appear in embedding
vocabulary but in training set. For NER, an en-
tity is considered as OOBV if there exists at lease
one word not in training set and at least one word
not in embedding vocabulary, and the other three
subsets can be done in similar manner. Table 8 in-
forms the statistics of the partition on each corpus.
The embedding we used is Stanford’s GloVe with
dimension 100, the same as Section 4.2.

Table 9 illustrates the performance of our model
on different subsets of words, together with the
baseline LSTM-CNN model for comparison. The
largest improvements appear on the OOBV sub-
sets of both the two corpora. This demonstrates
that by adding CRF for joint decoding, our model
is more powerful on words that are out of both the
training and embedding sets.

5 Related Work

In recent years, several different neural network
architectures have been proposed and successfully
applied to linguistic sequence labeling such as
POS tagging, chunking and NER. Among these
neural architectures, the three approaches most
similar to our model are the BLSTM-CRF model
proposed by Huang et al. (2015),
the LSTM-

CNNs model by Chiu and Nichols (2015) and the
BLSTM-CRF by Lample et al. (2016).

Huang et al. (2015) used BLSTM for word-level
representations and CRF for jointly label decod-
ing, which is similar to our model. But there
are two main differences between their model
and ours. First, they did not employ CNNs to
model character-level information. Second, they
combined their neural network model with hand-
crafted features to improve their performance,
making their model not an end-to-end system.
Chiu and Nichols (2015) proposed a hybrid of
BLSTM and CNNs to model both character- and
word-level representations, which is similar to the
ﬁrst two layers in our model. They evaluated their
model on NER and achieved competitive perfor-
mance. Our model mainly differ from this model
by using CRF for joint decoding. Moreover, their
model is not truly end-to-end, either, as it utilizes
external knowledge such as character-type, capi-
talization and lexicon features, and some data pre-
processing speciﬁcally for NER (e.g. replacing all
sequences of digits 0-9 with a single “0”). Re-
cently, Lample et al. (2016) proposed a BLSTM-
CRF model for NER, which utilized BLSTM to
model both the character- and word-level infor-
mation, and use data pre-processing the same as
Chiu and Nichols (2015). Instead, we use CNN to
model character-level information, achieving bet-
ter NER performance without using any data pre-
processing.

There are several other neural networks previ-
ously proposed for sequence labeling. Labeau et
al. (2015) proposed a RNN-CNNs model for Ger-
man POS tagging. This model is similar to the
LSTM-CNNs model in Chiu and Nichols (2015),
with the difference of using vanila RNN instead
of LSTM. Another neural architecture employing

CNN to model character-level information is the
“CharWNN” architecture (Santos and Zadrozny,
2014) which is inspired by the feed-forward net-
work (Collobert et al., 2011). CharWNN obtained
near state-of-the-art accuracy on English POS tag-
ging (see Section 4.3 for details). A similar model
has also been applied to Spanish and Portuguese
NER (dos Santos et al., 2015)
6 Conclusion
In this paper, we proposed a neural network archi-
tecture for sequence labeling. It is a truly end-to-
end model relying on no task-speciﬁc resources,
feature engineering or data pre-processing. We
achieved state-of-the-art performance on two lin-
guistic sequence labeling tasks, comparing with
previously state-of-the-art systems.

There are several potential directions for future
work. First, our model can be further improved
by exploring multi-task learning approaches to
combine more useful and correlated information.
For example, we can jointly train a neural net-
work model with both the POS and NER tags to
improve the intermediate representations learned
in our network. Another interesting direction is
to apply our model to data from other domains
such as social media (Twitter and Weibo). Since
our model does not require any domain- or task-
speciﬁc knowledge, it might be effortless to apply
it to these domains.

References
[Ando and Zhang2005] Rie Kubota Ando and Tong
Zhang. 2005. A framework for learning predic-
tive structures from multiple tasks and unlabeled
data. The Journal of Machine Learning Research,
6:1817–1853.

[Bengio et al.1994] Yoshua Bengio, Patrice Simard,
and Paolo Frasconi. 1994. Learning long-term de-
pendencies with gradient descent is difﬁcult. Neural
Networks, IEEE Transactions on, 5(2):157–166.

[Bergstra et al.2010] James Bergstra, Olivier Breuleux,
Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu,
Guillaume Desjardins, Joseph Turian, David Warde-
Farley, and Yoshua Bengio. 2010. Theano: a cpu
and gpu math expression compiler. In Proceedings
of the Python for scientiﬁc computing conference
(SciPy), volume 4, page 3. Austin, TX.

[Chen and Manning2014] Danqi Chen and Christopher
Manning. 2014. A fast and accurate dependency
In Proceedings of
parser using neural networks.
EMNLP-2014, pages 740–750, Doha, Qatar, Octo-
ber.

[Chieu and Ng2002] Hai Leong Chieu and Hwee Tou
Ng. 2002. Named entity recognition: a maximum
entropy approach using global information. In Pro-
ceedings of CoNLL-2003, pages 1–7.

[Chiu and Nichols2015] Jason PC Chiu and Eric
Named entity recognition
arXiv preprint

lstm-cnns.

Nichols.
2015.
with bidirectional
arXiv:1511.08308.

Cho,

[Cho et al.2014] Kyunghyun

Bart
van
Merri¨enboer, Dzmitry Bahdanau,
and Yoshua
Bengio. 2014. On the properties of neural machine
translation: Encoder–decoder approaches. Syntax,
Semantics and Structure in Statistical Translation,
page 103.

[Collobert et al.2011] Ronan Collobert, Jason Weston,
L´eon Bottou, Michael Karlen, Koray Kavukcuoglu,
and Pavel Kuksa. 2011. Natural language process-
ing (almost) from scratch. The Journal of Machine
Learning Research, 12:2493–2537.

[Dai et al.2015] Hong-Jie Dai, Po-Ting Lai, Yung-Chun
Chang, and Richard Tzong-Han Tsai. 2015. En-
hancing of chemical compound and drug name
recognition using representative tag scheme and
ﬁne-grained tokenization. Journal of cheminformat-
ics, 7(S1):1–10.

[Dauphin et al.2015] Yann N Dauphin, Harm de Vries,
Junyoung Chung, and Yoshua Bengio.
2015.
Rmsprop and equilibrated adaptive learning rates
arXiv preprint
for non-convex optimization.
arXiv:1502.04390.

[dos Santos et al.2015] Cıcero

dos Santos, Victor
Guimaraes, RJ Niter´oi, and Rio de Janeiro. 2015.
Boosting named entity recognition with neural
In Proceedings of NEWS
character embeddings.
2015 The Fifth Named Entities Workshop, page 25.

[Dyer et al.2015] Chris Dyer, Miguel Ballesteros,
Wang Ling, Austin Matthews, and Noah A. Smith.
2015. Transition-based dependency parsing with
In Proceedings
stack long short-term memory.
of ACL-2015 (Volume 1: Long Papers), pages
334–343, Beijing, China, July.

[Florian et al.2003] Radu Florian, Abe Ittycheriah,
Hongyan Jing, and Tong Zhang. 2003. Named en-
tity recognition through classiﬁer combination.
In
Proceedings of HLT-NAACL-2003, pages 168–171.

[Gers et al.2000] Felix A Gers, J¨urgen Schmidhuber,
and Fred Cummins. 2000. Learning to forget: Con-
tinual prediction with lstm. Neural computation,
12(10):2451–2471.

[Gers et al.2003] Felix A Gers, Nicol N Schraudolph,
and J¨urgen Schmidhuber. 2003. Learning precise
timing with lstm recurrent networks. The Journal of
Machine Learning Research, 3:115–143.

[Giles2001] Rich Caruana Steve Lawrence Lee Giles.
2001. Overﬁtting in neural nets: Backpropagation,
conjugate gradient, and early stopping. In Advances
in Neural Information Processing Systems 13: Pro-
ceedings of the 2000 Conference, volume 13, page
402. MIT Press.

[Gim´enez and M`arquez2004] Jes´us Gim´enez and Llu´ıs
M`arquez. 2004. Svmtool: A general pos tagger
generator based on support vector machines. In In
Proceedings of LREC-2004.

[Glorot and Bengio2010] Xavier Glorot and Yoshua
2010. Understanding the difﬁculty of
Bengio.
In In-
training deep feedforward neural networks.
ternational conference on artiﬁcial intelligence and
statistics, pages 249–256.

[Goller and Kuchler1996] Christoph Goller and An-
dreas Kuchler. 1996. Learning task-dependent dis-
tributed representations by backpropagation through
In Neural Networks, 1996., IEEE Inter-
structure.
national Conference on, volume 1, pages 347–352.
IEEE.

[Graves et al.2013] Alan Graves, Abdel-rahman Mo-
hamed, and Geoffrey Hinton. 2013. Speech recog-
nition with deep recurrent neural networks. In Pro-
ceedings of ICASSP-2013, pages 6645–6649. IEEE.

[He et al.2015] Kaiming He, Xiangyu Zhang, Shaoqing
Ren, and Jian Sun. 2015. Delving deep into recti-
ﬁers: Surpassing human-level performance on ima-
genet classiﬁcation. In Proceedings of the IEEE In-
ternational Conference on Computer Vision, pages
1026–1034.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter
and J¨urgen Schmidhuber. 1997. Long short-term
memory. Neural computation, 9(8):1735–1780.

[Huang et al.2015] Zhiheng Huang, Wei Xu, and Kai
Yu. 2015. Bidirectional lstm-crf models for se-
quence tagging. arXiv preprint arXiv:1508.01991.

[Jozefowicz et al.2015] Rafal

Jozefowicz, Wojciech
Zaremba, and Ilya Sutskever. 2015. An empirical
exploration of recurrent network architectures.
In
Proceedings of the 32nd International Conference
on Machine Learning (ICML-15), pages 2342–2350.

[Kingma and Ba2014] Diederik Kingma and Jimmy
Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980.

[Koo and Collins2010] Terry Koo and Michael Collins.
2010. Efﬁcient third-order dependency parsers. In
Proceedings of ACL-2010, pages 1–11, Uppsala,
Sweden, July.

[Labeau et al.2015] Matthieu Labeau, Kevin L¨oser,
Alexandre Allauzen, and Rue John von Neumann.
2015. Non-lexical neural architecture for ﬁne-
In Proceedings of the 2015
grained pos tagging.
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 232–237.

[Lafferty et al.2001] John Lafferty, Andrew McCallum,
and Fernando CN Pereira. 2001. Conditional ran-
dom ﬁelds: Probabilistic models for segmenting and
In Proceedings of ICML-
labeling sequence data.
2001, volume 951, pages 282–289.

[Lample et al.2016] Guillaume Lample, Miguel Balles-
teros, Sandeep Subramanian, Kazuya Kawakami,
Neural architectures
and Chris Dyer.
In Proceedings of
for named entity recognition.
NAACL-2016, San Diego, California, USA, June.

2016.

[LeCun et al.1989] Yann LeCun, Bernhard Boser,
John S Denker, Donnie Henderson, Richard E
Howard, Wayne Hubbard, and Lawrence D Jackel.
1989.
Backpropagation applied to handwrit-
Neural computation,
ten zip code recognition.
1(4):541–551.

[Lin and Wu2009] Dekang Lin and Xiaoyun Wu. 2009.
In

Phrase clustering for discriminative learning.
Proceedings of ACL-2009, pages 1030–1038.

[Luo et al.2015] Gang Luo, Xiaojiang Huang, Chin-
Yew Lin, and Zaiqing Nie.
Joint entity
In Proceedings
recognition and disambiguation.
of EMNLP-2015, pages 879–888, Lisbon, Portugal,
September.

2015.

[Ma and Hovy2015] Xuezhe Ma and Eduard Hovy.
2015. Efﬁcient inner-to-outer greedy algorithm for
In Pro-
higher-order labeled dependency parsing.
ceedings of the EMNLP-2015, pages 1322–1328,
Lisbon, Portugal, September.

[Ma and Xia2014] Xuezhe Ma and Fei Xia.

2014.
Unsupervised dependency parsing with transferring
distribution via parallel guidance and entropy reg-
In Proceedings of ACL-2014, pages
ularization.
1337–1348, Baltimore, Maryland, June.

[Ma and Zhao2012a] Xuezhe Ma and Hai Zhao. 2012a.
Fourth-order dependency parsing. In Proceedings of
COLING 2012: Posters, pages 785–796, Mumbai,
India, December.

[Ma and Zhao2012b] Xuezhe Ma

and Hai Zhao.
Probabilistic models for high-order pro-
Technical Report,

2012b.
jective dependency parsing.
arXiv:1502.04174.

[Ma et al.2016] Xuezhe Ma, Zhengzhong Liu, and Ed-
uard Hovy. 2016. Unsupervised ranking model
for entity coreference resolution. In Proceedings of
NAACL-2016, San Diego, California, USA, June.

[Manning2011] Christopher D Manning. 2011. Part-
is it time
of-speech tagging from 97% to 100%:
In Computational Linguis-
for some linguistics?
tics and Intelligent Text Processing, pages 171–189.
Springer.

[Marcus et al.1993] Mitchell Marcus, Beatrice San-
torini, and Mary Ann Marcinkiewicz. 1993. Build-
ing a large annotated corpus of English:
the Penn
Treebank. Computational Linguistics, 19(2):313–
330.

[McDonald et al.2005] Ryan McDonald, Koby Cram-
mer, and Fernando Pereira. 2005. Online large-
margin training of dependency parsers. In Proceed-
ings of ACL-2005, pages 91–98, Ann Arbor, Michi-
gan, USA, June 25-30.

[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases
In Advances in neural
and their compositionality.
information processing systems, pages 3111–3119.

[Ng2010] Vincent Ng. 2010. Supervised noun phrase
coreference research: The ﬁrst ﬁfteen years. In Pro-
ceedings of ACL-2010, pages 1396–1411, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.

[Nivre and Scholz2004] Joakim Nivre

and Mario
Scholz. 2004. Deterministic dependency parsing
of English text. In Proceedings of COLING-2004,
pages 64–70, Geneva, Switzerland, August 23-27.

[Pascanu et al.2012] Razvan Pascanu, Tomas Mikolov,
and Yoshua Bengio. 2012. On the difﬁculty of
training recurrent neural networks. arXiv preprint
arXiv:1211.5063.

[Passos et al.2014] Alexandre Passos, Vineet Kumar,
and Andrew McCallum. 2014. Lexicon infused
phrase embeddings for named entity resolution. In
Proceedings of CoNLL-2014, pages 78–86, Ann Ar-
bor, Michigan, June.

[Srivastava et al.2014] Nitish Srivastava, Geoffrey Hin-
ton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: A simple way to
prevent neural networks from overﬁtting. The Jour-
nal of Machine Learning Research, 15(1):1929–
1958.

[Sun2014] Xu Sun. 2014. Structure regularization for
structured prediction. In Advances in Neural Infor-
mation Processing Systems, pages 2402–2410.

[Tjong Kim Sang and De Meulder2003] Erik F. Tjong
Kim Sang and Fien De Meulder.
Intro-
duction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceed-
ings of CoNLL-2003 - Volume 4, pages 142–147,
Stroudsburg, PA, USA.

2003.

[Tjong Kim Sang and Veenstra1999] Erik

F. Tjong
Kim Sang and Jorn Veenstra. 1999. Representing
In Proceedings of EACL’99, pages
text chunks.
173–179. Bergen, Norway.

[Toutanova et al.2003] Kristina Toutanova, Dan Klein,
Christopher D Manning, and Yoram Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic de-
pendency network. In Proceedings of NAACL-HLT-
2003, Volume 1, pages 173–180.

[Zeiler2012] Matthew D Zeiler.

2012. Adadelta:
an adaptive learning rate method. arXiv preprint
arXiv:1212.5701.

[Peng and Dredze2015] Nanyun

and Mark
Dredze. 2015. Named entity recognition for chi-
nese social media with jointly trained embeddings.
In Proceedings of EMNLP-2015, pages 548–554,
Lisbon, Portugal, September.

Peng

[Pennington et al.2014] Jeffrey Pennington, Richard
Socher, and Christopher Manning. 2014. Glove:
In Pro-
Global vectors for word representation.
ceedings of EMNLP-2014, pages 1532–1543, Doha,
Qatar, October.

[Ratinov and Roth2009] Lev Ratinov and Dan Roth.
2009. Design challenges and misconceptions in
In Proceedings of
named entity recognition.
CoNLL-2009, pages 147–155.

[Santos and Zadrozny2014] Cicero D Santos

and
Bianca Zadrozny. 2014. Learning character-level
representations for part-of-speech tagging.
In
Proceedings of ICML-2014, pages 1818–1826.

[Shen et al.2007] Libin Shen, Giorgio Satta, and Ar-
avind Joshi. 2007. Guided learning for bidirec-
In Proceedings of
tional sequence classiﬁcation.
ACL-2007, volume 7, pages 760–767.

2011.

[Søgaard2011] Anders Søgaard.

Semi-
supervised condensed nearest neighbor for part-of-
speech tagging. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 48–
52, Portland, Oregon, USA, June.

