6
1
0
2

 
r
a

 

M
1
1

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
6
1
5
3
0

.

3
0
6
1
:
v
i
X
r
a

An (cid:96)∞ Eigenvector Perturbation Bound and Its
Application to Robust Covariance Estimation

Jianqing Fan∗, Weichen Wang and Yiqiao Zhong

Department of Operations Research and Financial Engineering, Princeton University

Abstract

In statistics and machine learning, people are often interested in the eigenvectors

(or singular vectors) of certain matrices (e.g. covariance matrices, data matrices, etc).

However, those matrices are usually perturbed by noises or statistical errors, either

from random sampling or structural patterns. One usually employs Davis-Kahan sin θ

theorem to bound the diﬀerence between the eigenvectors of a matrix A and those of a

perturbed matrix (cid:101)A = A + E, in terms of (cid:96)2 norm. In this paper, we prove that when

A is a low-rank and incoherent matrix, the (cid:96)∞ norm perturbation bound of singular
vectors (or eigenvectors in the symmetric case) is smaller by a factor of
d2 for
left and right vectors, where d1 and d2 are the matrix dimensions. The power of this new
perturbation result is shown in robust covariance estimation, particularly when random

d1 or

√

√

variables have heavy tails. There, we propose new robust covariance estimators and

establish their asymptotic properties using the newly developed perturbation bound.

Our theoretical results are veriﬁed through extensive numerical experiments.

Keywords: Matrix perturbation theory, Incoherence, Low-rank matrices, Sparsity, Ro-

bust covariance estimation, Approximate factor model.

∗Address: Department of ORFE, Sherrerd Hall, Princeton University, Princeton, NJ 08544, USA, e-mail:
jqfan@princeton.edu, yiqiaoz@princeton.edu, weichenw@princeton.edu. The research was partially supported
by NSF grants DMS-1206464 and DMS-1406266 and NIH grants R01-GM072611-10 and NIH R01GM100474-
04.

1

1

Introduction

The perturbation of matrix eigenvectors (or singular vectors) has been well-studied in
matrix perturbation theory (Wedin, 1972; Stewart, 1990). The best known result of eigenvec-
tor perturbation is the classic Davis-Kahan theorem (Davis and Kahan, 1970). It originally
emerged as a powerful tool in numerical analysis, but soon found its widespread use in other
ﬁelds, such as statistics and machine learning. Its popularity continues to surge in recent
years, which is largely attributed to the omnipresent data analysis, where it is a common
practice, for example, to employ PCA (Jolliﬀe, 2002) for dimension reduction, feature ex-
traction, and data visualization.

The eigenvectors of matrices are closely related to the underlying structure in a variety
of problems. For instance, principal components often capture most information of data
and extract the latent factors that drive the correlation structure of the data (Bartholomew
et al., 2011); in classical multidimensional scaling (MDS), the centered squared distance
matrix encodes the coordinates of data points embedded in a low dimensional subspace
(Borg and Groenen, 2005); and in clustering and network analysis, spectral algorithms are
used to reveal clusters and community structure (Ng et al., 2002; Rohe et al., 2011).
In
those problems, the low dimensional structure that we want to recover, is often ‘perturbed’
by observation uncertainty or statistical errors. Besides, there might be a sparse pattern
corrupting the low dimensional structure, as in approximate factor models (Chamberlain
and Rothschild, 1982; Stock and Watson, 2002) and robust PCA (De La Torre and Black,
2003; Cand`es et al., 2011).

A general way to study these problems is to consider

(cid:101)A = A + S + N,

(1.1)

where A is a low rank matrix, S is a sparse matrix, and N is a random matrix regarded as
random noise or estimation error, all of which have the size d1 × d2. Usually A is regarded
as the ‘signal’ matrix we are primarily interested in, S is some sparse contamination whose
eﬀect we want to separate from A, and N is the noise or estimation error when estimating
the covariance matrix.

The decomposition (1.1) forms the core of a ﬂourishing literature on robust PCA (Chan-
drasekaran et al., 2011; Cand`es et al., 2011), structured covariance estimation (Fan et al.,
2008), multivariate regression (Yuan et al., 2007) and so on. Among these works, a stan-
dard condition on A is matrix incoherence (Cand`es et al., 2011). Let the singular value

2

decomposition be

r(cid:88)

σiuivT
i ,

A = U ΣV T =

where r is the rank of A, the singular values are σ1 ≥ σ2 ≥ . . . ≥ σr > 0, and the matrices
U = (u1, . . . , ur), V = (v1, . . . , vr) consist of the singular vectors. The coherence µ(U ), µ(V )
is deﬁned as

i=1

µ(U ) =

d1
r

max

i

u2
ij,

µ(V ) =

d2
r

max

i

v2
ij,

(1.2)

r(cid:88)

j=1

where uij and vij are the (i, j) element of U and V , respectively. It is usually expected that
µ = max{µ(U ), µ(V )} is small. Note that we do not need any incoherence condition on
U V T , which is diﬀerent from Cand`es et al. (2011) and is arguably unnecessary (Chen, 2015).
On the contrary, the incoherence condition (1.2) is necessary for us to disentangle the sparse
eﬀect S from the low rank component A. Otherwise A and S are not identiﬁable.

Now we denote the eigengap γ0 := min{σi − σi+1 : i = 1, . . . , r} where σr+1 = 0 for
notational convenience, and let E = S + N , viewing it as a perturbation matrix to the

matrix A in (1.1). Deﬁne τ0 = max{(cid:112)d2/d1(cid:107)E(cid:107)1,(cid:112)d1/d2(cid:107)E(cid:107)∞}, where

(cid:107)E(cid:107)1 = max

j

|Eij|,

(cid:107)E(cid:107)∞ = max

i

|Eij|,

d2(cid:88)

j=1

r(cid:88)

j=1

d1(cid:88)

i=1

which are commonly used norms gauging sparsity (Bickel and Levina, 2008). As will be clear
soon, we do not need to additionally impose sparse patterns on N , and in fact the only thing
that matters is τ0/γ0, i.e. the magnitude of the perturbation E relative to its eigengap γ0.
We shall present a new matrix perturbation result similar to the well-known Davis-Kahn
theorem, but instead of (cid:96)2 norm or Frobenius norm, we will state our result in terms of (cid:96)∞
norm, that is (cid:107)x(cid:107)∞ := maxi |xi|. The following theorem gives a ﬂavor of our result.

Theorem 1.1. Consider the perturbation (cid:101)A = A + E, where A is a rank r matrix and
E is the perturbation matrix. Let the singular value decomposition be A = U ΣV T , (cid:101)A =
(cid:101)U(cid:101)Σ(cid:101)V T , where U = (u1, . . . , ur), V = (v1, . . . , vr) be singular vectors of A, and (cid:101)U =
((cid:101)u1, . . . ,(cid:101)ur), V = ((cid:101)v1, . . . ,(cid:101)vr) be similar quantities for (cid:101)A. Let µ = max{µ(U ), µ(V )} and
τ0 = max{(cid:112)d2/d1(cid:107)E(cid:107)1,(cid:112)d1/d2(cid:107)E(cid:107)∞}. Then there exist η1, . . . , ηr that are either 1 or −1,

and constants C1, C2 that depends on r and µ, such that

(cid:107)uk − ηk(cid:101)uk(cid:107)∞ ≤ C1

max
1≤k≤r

(cid:107)vk − ηk(cid:101)vk(cid:107)∞ ≤ C1

√
τ0

,

d2

γ0

(1.3)

when γ0 > C2τ0.

√
τ0

d1

γ0

and max
1≤k≤r

3

From now on, to facilitate the presentation, we will drop ηk in the expressions such as
(1.3), with understanding such an ambiguity correction is needed. When A is a square matrix,
say a covariance matrix, the condition on the eigengap is simply γ0 > C2 max{(cid:107)E(cid:107)1,(cid:107)E(cid:107)∞}.
It naturally holds for a variety of applications, where the low rank structure emerges as a
consequence of a few factors driving the data matrix. For example, in Fama-French factor
models, the excess returns of a stock market are driven by a few common factors (Fama and
French, 1993); in collaborative ﬁltering, the ratings of users are mostly determined by a few
common preferences (Rennie and Srebro, 2005); in video surveillance, A is associated with
the stationary background across image frames (Oliver et al., 2000). We shall have a more
elaborate discussion in Section 2.2.

To see how our result departs from Davis-Kahan theorem, we state a result that follows
easily from the classical Davis-Kahan theorem (see Stewart (1990)), which was, though,
originally proved for symmetric matrices. The (cid:96)2 perturbation bound of singular vectors is
given by

(cid:8)(cid:107)vk −(cid:101)vk(cid:107)2 ∨ (cid:107)uk −(cid:101)uk(cid:107)2

(cid:9) ≤

√

2(cid:107)E(cid:107)
δ

(1.4)
where (cid:107) · (cid:107) is the matrix 2-norm (cid:107) · (cid:107)2,2 (or matrix norm induced by (cid:96)2 norm), and δ is the

max
1≤k≤r

,

eigengap between A and (cid:101)A:

δ := min{|σj−1 −(cid:101)σj| ∧ |σj+1 −(cid:101)σj| : j = 1, . . . , r},

By Wely’s inequality, |(cid:101)σj − σj| ≤ (cid:107)E(cid:107); and it is known that (cid:107)E(cid:107)2 ≤ (cid:107)E(cid:107)1(cid:107)E(cid:107)∞ ≤ τ 2

when γ0 ≥ 2(cid:107)E(cid:107), Davis-Kahan theorem or (1.4) gives

σ0 = σr+1 := 0.

0 . Then

(cid:8)(cid:107)vk −(cid:101)vk(cid:107)2 ∨ (cid:107)uk −(cid:101)uk(cid:107)2

(cid:9) ≤ 2

√

2τ0
γ0

max
1≤k≤r

.

(1.5)

√

d1 oﬀ for uk and

Yu et al. (2014) also has the similar bound with tight constant as in (1.5). If we naively
use the relationship (cid:107)x(cid:107)∞ ≤ (cid:107)x(cid:107)2, we would have a bound, compared with (1.3), with a
√
d2 of for vk in the denominator. In other words, converting the
factor
(cid:96)2 bound from Davis-Kahan theorem directly to the (cid:96)∞ bound does not give a sharp result,
in the presence of incoherent and low rank structure. Actually, for square matrices, our
(cid:96)∞ bound (1.3) implies the (cid:96)2 bound (1.5) (ignoring some constant). Thus, our result is a
stronger perturbation result when the matrix has an incoherence and low rank structure.

To appreciate how matrix incoherence helps, let us consider a simple example with no ma-
trix incoherence, in which (1.4) is tight up to a constant. Let A = d(1, 0, . . . , 0)T (1, 0, . . . , 0)
be a d dimensional square matrix, and E = d(0, 1/2, 0, . . . , 0)T (1, 0, . . . , 0) of the same size. It

4

is apparent that γ0 = d, τ0 = d/2, and that v1 = (1, 0, . . . , 0)T ,(cid:101)v1 = (2/

√
√
5, 0, . . . , 0)T
5, 1/
up to sign. Clearly, the perturbation of the leading eigenvector is not vanishing in this ex-
ample, and thus, there is no hope of a strong upper bound as in (1.3) without incoherence
condition.

d2 comes into play in (1.4) is that, the error uk −(cid:101)uk

The reason that the factor

√
d1 or

√

(and similarly for vk) spreads out evenly in d1 (or d2) coordinates, so that the (cid:96)∞ error is
far smaller than the (cid:96)2 error. This, of course, hinges on the incoherence condition, which in
essence precludes eigenvectors from concentrating along any coordinate. In a rough sense,
the perturbation that E brings into A dissipates along all coordinates.

Our result stands in contrast with the sparse PCA literature, in which it is usually
assumed that the leading eigenvectors are sparse. In Johnstone and Lu (2009), it is proved

that there is a threshold for p/n, above which PCA performs poorly, in the sense that (cid:104)(cid:101)v, v(cid:105)

is approximately 0. This means that the principal component computed from the sample
covariance matrix reveals nothing about the true eigenvector. In order to mitigate this issue,
in Johnstone and Lu (2009) and subsequent papers (Vu and Lei, 2012; Ma, 2013; Berthet and
Rigollet, 2013), sparse leading eigenvectors are assumed. However, our result is diﬀerent,
in the way that we require a stronger eigengap condition (i.e. stronger signal), whereas
in Johnstone and Lu (2009), the eigengap of the leading eigenvectors does not scale with
the dimensionality. This explains why it is plausible to have a strong uniform eigenvector
perturbation bound in this paper.

We will illustrate the power of this perturbation result using robust covariance estimation
as one application. In the approximate factor model, the true covariance matrix admits a
decomposition into a low rank part A and a sparse part S. Such models have been widely
applied in ﬁnance, economics and genomics and health to explore correlation structure.
However, in many studies, especially ﬁnancial genomics applications, it is well known that
the observations exhibit heavy tails (Gupta et al., 2013). This problem can be resolved with
the aid of recent results of concentration bounds in robust estimation (Catoni, 2012; Hsu
and Sabato, 2014; Fan et al., 2016a), which produces the estimation error N with optimal
element-wise bound. It nicely ﬁts our perturbation result, and we can tackle it easily by
following the spirit of Fan et al. (2013).

(cid:80)d1
Here are a few notations in this paper. For a generic d1 by d2 matrix, the matrix max
norm is denoted as (cid:107)M(cid:107)max = maxi,j |Mij|. The matrix operator norm induced by vector (cid:96)p
(cid:80)d2
norm is (cid:107)M(cid:107)p = sup(cid:107)x(cid:107)p=1 (cid:107)M x(cid:107)p for 1 ≤ p ≤ ∞. In particular, (cid:107)M(cid:107)1 = maxj
i=1 |Mij|;
j=1 |Mij|; and (cid:107) · (cid:107) denotes matrix 2-norm (cid:107) · (cid:107)2 for shorthand. We use
(cid:107)M(cid:107)∞ = maxi
σj(M ) to denote the jth largest singular value. For a symmetric matrix M , denote λj(M )
as its jth largest eigenvalue. If M is a positive deﬁnite matrix, then M 1/2 is the square root

5

of M , and M−1/2 is the square root of M−1.

2 The perturbation result and its conditions

2.1 The perturbation theorem

We shall present a theorem for the symmetric matrices (d1 = d2 = d), and give a precise
form of how the constant in (1.1) depends on r and µ. This symmetric case does not lose
its generality, since the proof of Theorem 1.1 depends on the Hermitian dilation, which
transform any real matrix into a symmetric one.

Suppose that a d-dimensional symmetric matrix A is of rank r. The perturbation matrix
E is d-dimensional and symmetric. The eigen-decomposition of A is A = V ΛV T , where

Λ = diag{λ1, . . . , λr}, and the columns of V are eigenvectors correspondingly. Let(cid:101)v1, . . . ,(cid:101)vr
be analogous quantities for (cid:101)A := A + E. Let µ(V ) be the matrix coherence, deﬁned in (1.2).

Deﬁne τ = (cid:107)E(cid:107)∞ as a speciﬁc case of τ0 or equivalently

d(cid:88)

j=1

|Eij|

τ := max
i≤d

and

√
d(cid:107)PV (E)(cid:107)max,

κ :=

where PV (E) := V T E is a linear operator. PV (E) are the coeﬃcient vectors of projection
of E onto span{v1, . . . , vr} in a column-wise way.
It is also worth mentioning that κ ≤

τ(cid:112)rµ(V ), and thus we can get rid of κ in the bound in the following theorem, but the price
we have to pay is the factor(cid:112)rµ(V ).

Assuming eigenvalues are ordered in the decreasing order, the eigengap for a square

matrix is simply

γ = min{λi − λi+1 : 1 ≤ i ≤ r} ∧ min{|λi| : 1 ≤ i ≤ r} ,

with convention λr+1 = −∞. If A is positive deﬁnite, then γ = γ0.

Now we are in a position to state the main theorem. Recall our convention to drop ηk in

the expressions such as (1.3) and (1.4) to facilitate the presentation.

Theorem 2.1. Suppose that γ > 5rµ(V )(τ + 2rκ). Then,

(cid:107)vk −(cid:101)vk(cid:107)∞ ≤ C(r, µ(V ))

max
1≤k≤r

√
τ + κ
γ
d

,

(2.1)

6

where

C(r, µ(V )) = 45 r5/2(cid:112)µ(V )(1 + rµ(V )).

In particular, when r and µ(V ) are bounded by a constant, we have

(cid:107)vk −(cid:101)vk(cid:107)∞ ≤ C(cid:48) τ

γ

max
1≤k≤r

√
d

,

as long as γ > C0τ for some constant C0, C(cid:48) > 0.

Here we make no endeavors to pursue the optimal bound in terms of r and µ(V ), partly

because of simplicity of proof, and partly because of suﬃciency for many applications.

2.2 Examples: which matrices have such structure?

In many problems, low-rank structure naturally arises due to the impact of pervasive
latent factors that inﬂuence most observed data. Since observations are imperfect, the
low-rank structure is often ‘perturbed’ by an additional sparse component, gross errors,
measurement noises, or the idiosyncratic components that can not be captured by the latent
factors. We give some motivating examples with such structure, which our result is applicable
to.

Panel data in stock markets. Consider the excess return data from a stock market over
a period of time. The driving factors in the market are reﬂected in the covariance matrix as
a low rank component A. The residual covariance of the idiosyncratic components is often
modeled by a sparse component S. Statistical analysis including PCA is usually conducted

based on the estimated covariance matrix (cid:101)A =(cid:98)Σ, which is perturbed from the true covariance

Σ = A + S by the estimation error N (Stock and Watson, 2002; Fan et al., 2013). In Section
3.1, we will develop a robust estimation method in the presence of heavy-tail distributed
data.

Video surveillance. In image processing and computer vision, it is often desired to sep-
arate moving objects from static background before further modeling and analysis (Oliver
et al., 2000; Hu et al., 2004). The static background corresponds to the low rank component
A in the data matrix, which is a collection of video frames, each consisting of many pixels
represented as a long vector in the data matrix. Moving objects and noise correspond to
the sparse matrix S and noise matrix N . Since the background is global information and
reﬂected by many pixels of a frame, it is natural for the incoherence condition to hold.

Wireless sensor network localization. In wireless sensor networks, we are usually inter-
ested in determining the location of sensor nodes with unknown position based on a few

7

(noisy) measurements between neighboring nodes (Doherty et al., 2001; Biswas and Ye,
2004). Let X be an r by n matrix such that each column xi gives the coordinates of each
node in a plane (r = 2) or a space (r = 3). Assume the center of the sensors has been
relocated at origin. Then the low rank matrix A = X T X, encoding the true distance infor-
mation, has to satisfy distance constraints given by the measurements. The noisy distance

matrix (cid:101)A after centering, equals to the sum of A and a matrix N consisting of measurement

errors. Suppose that each node is a random point uniformly distributed in a rectangular
region. It is not diﬃcult to see that with high probability, the top r eigenvalues of X T X
and their eigengap scales with the number of sensors n and the leading eigenvectors have a
bounded coherence.

3 Application to robust covariance estimation

We will study robust covariance estimation and show the strength of our result. Through-
out this section, we assume that r is bounded by a constant, though this assumption is not
critical.

3.1 PCA in spiked covariance model

To initiate our discussion, we ﬁrst consider sub-Gaussian random variables. Let X be a n
by d matrix, whose columns are independent samples drawn from a zero mean p dimensional
distribution with the covariance matrix

Σ =

λivivT

i + σ2Id := Σ1 + Σ2,

(λ1 ≥ . . . ≥ λr > 0).

(3.1)

r(cid:88)

i=1

This is the spiked covariance model that has received intense study in recent years. When
λi/σ2 is bounded, it has been shown by Johnstone and Lu (2009) that the empirical eigenvec-
tor is not a consistent estimate of the true eigenvector, unless we assume low dimensionality
(p (cid:28) n). In order to achieve the fast rate of Theorem 2.1, we will impose more stringent

conditions. Let the empirical covariance matrix be (cid:98)Σ = X T X/n. Viewing the empirical

covariance matrix as the true one plus estimation error, we have the decomposition

(cid:98)Σ = Σ1 +

(cid:16) 1

n

(cid:17)

X T X − Σ

+ Σ2.

Assume that µ(V ) is bounded by a constant. It is easy to see Var(Xi) ≤ σ2 + Cλ1/d for
some constant C. It follows from the well-known result (e.g., Vershynin (2010)) that if rows

8

of X are i.i.d sub-Gaussian distributed, then with probability greater than 1 − d−1,

X T X − Σ(cid:107)max ≤ C(cid:0)σ2 +

(cid:107) 1
n

(cid:1)(cid:114)

λ1
d

log d

.

(3.2)

n
To apply Theorem 2.1, set E = X T X/n − Σ + Σ2, and observe

X T X − Σ(cid:107)max ≤ C(cid:0)dσ2 + λ1

(cid:1)(cid:114)

log d

n

.

τ ≤ σ2 + d(cid:107) 1
n

An application of (2.1) yields

max
1≤k≤r

(cid:107)vk −(cid:98)vk(cid:107)∞ = O(r7/2τ /(γ

√

d)).

(3.3)

where

In particular, when r is bounded, λ1 (cid:16) λr (cid:16) γ, and γ (cid:29) σ2d(cid:112)log d/n, we have

γ = min{λi − λi+1 : 1 ≤ i ≤ r},

λr+1 := 0.

(cid:107)vk −(cid:98)vk(cid:107)∞ = oP
where vk,(cid:98)vk are eigenvectors of Σ,(cid:98)Σ respectively.

max
1≤k≤r

(cid:16) 1√

(cid:17)

,

d

3.2 PCA for robust covariance estimation

The strength of Theorem 2.1 is more pronounced when the random variables are heavy-
tail distributed. Consider again the covariance matrix Σ with structure (3.1). Instead of
assuming sub-Gaussian distribution, we assume there exists a constant C > 0 such that
maxi EX 4

i < C, i.e. the fourth moments of the random variables are uniformly bounded.

Unlike sub-Gaussian variables, there is no concentration bound similar to (3.2) for the
empirical covariance matrix. Fortunately, thanks to recent advances in robust statistics (e.g.,
Catoni (2012)), robust estimate of Σ with guaranteed concentration property indeed exists.
We shall use the method proposed in Fan et al. (2016a). Motivated by the classic M-estimator

of Huber (1964), Fan et al. (2016a) proposed a robust estimator for each element of (cid:98)Σ, by

solving a Huber loss based minimization problem

lα(XtiXtj − µ),

(3.4)

(cid:98)Σij = argminµ

n(cid:88)

t=1

9

where lα is the Huber loss

The parameter α is suggested to be α =(cid:112)nv2/ log(−1) for  ∈ (0, 1), where v is assumed

x2,

to satisfy v ≥ maxij

lα(x) =

|x| ≥ α,
|x| ≤ α.

2α|x| − α2,
(cid:114)
(cid:16)|(cid:98)Σij − Σij| ≤ 4v

(cid:112)Var(XiXj). If log(−1) ≤ n/8, we have
(cid:17) ≥ 1 − 2.

log(−1)

P

n

From this result, the next proposition is immediate by taking  = d−3.

Proposition 3.1. Suppose that there is a constant C with maxi EX 4
i < C. Then there exists
a constant C1 such that with probability greater than 1− d−1(1 + d−1), the robust estimate of

covariance matrix with α =(cid:112)3nv2 log(d) satisﬁes
(cid:114)
(cid:107)(cid:98)Σ − Σ(cid:107)max ≤ 4v

3 log d

n

,

(cid:112)Var(XiXj).

where v is a pre-determined parameter assumed to be no less than maxij

According to this result, it is apparent that the convergence rate in the previous section
is still valid under the same regime. To be more speciﬁc, suppose µ(V ) is bounded. Then,
(3.3) holds for the PCA based on the robust covariance estimation. When λ1 (cid:16) γ and

γ (cid:29) σ2d(cid:112)log d/n, we again have

(cid:107)vk −(cid:98)vk(cid:107)∞ = oP

max
1≤k≤r

(cid:16) 1√

(cid:17)

.

d

The above discussion can be easily extended to a general Σ2 with bounded spectrum
rather than a diagonal matrix. So we have shown that if V is incoherent and the spiked

eigenvalues grow faster than σ2d(cid:112)log d/n, then every element in the spiked eigenvectors

√
can be estimated with errors far less than 1/
d, which implies consistency of the empirical
eigenvectors. The minimum signal strength (or eigenvalue) for (in)consistency is shown to be
σ2d/n under sub-Gaussian assumption (Fan and Wang, 2015), which is smaller compared to
our signal strength by a factor of
n log d. However, our result is derived from a nonasymp-
totic regime, holds for general distributions, and shows the (cid:96)∞ error bound for eigenvector
estimates.

√

10

3.3 Robust Covariance estimation via factor models

We now apply Theorem 2.1 to estimating large covariance matrices in approximate factor
models. With the theorem, we are able to extend the assumption of noise distribution
beyond sub-Gaussian, to include heavy-tailed variables, which are a generalization of Fan
et al. (2013).

Suppose the observation yit admits a decomposition

yit = bT

i ft + uit,

i ≤ d, t ≤ n,

(3.5)

where bi ∈ Rr is the unknown loading vector, ft ∈ Rr the unobserved factor vector, and uit
the idiosyncratic noise. Let yt = (y1t, . . . , ydt)T and ut = (u1t, . . . , udt)T , so yt = Bft + ut,
where B = (b1, . . . , bd)T . Suppose that ft and ut are uncorrelated and centered random
vectors. We also assume temporal independence, i.e. {ft, ut} are independent for t, for the
sake of simplicity, although it is possible to allow for weak temporal dependence as in Fan
et al. (2013). To circumvent the identiﬁability issue common for latent variable models, we
also assume Cov(ft) = Ir and that BT B is a diagonal matrix with distinct values. The merit
of (3.5) lies in the decomposition of Σ = Cov(yt) into a low rank part and a residual part:

Σ = BBT + Σu,

where Σu = Cov(ut). In addition, we assume (cid:107)Σu(cid:107)2 is bounded from above and below.

It has been observed by Chamberlain and Rothschild (1982) and Bai (2003) that in
the application of ﬁnance end economics that the factors are pervasive, in the sense that
the factors have impacts on a large fractions of the outcomes.
In other words, the ﬁrst
component in the right hand side of (3.5) has predominant contribution to the covariance
matrix Σ = Cov(yt). We formally give the following deﬁnition of pervasiveness.
Deﬁnition 3.1. In the factor model (3.5), the factors are called pervasive if ∃ C1, C2, C3
such that C1d ≤ λr(BT B) ≤ λ1(BT B) ≤ C2d and (cid:107)B(cid:107)max ≤ C3.

Let {λi, vi}r

i=1 be the top r eigenvalues and eigenvectors of Σ, and similarly, {λi, vi}r
i=1
for BBT . For notational convenience denote λ0 = λ0 = 0. We provide one explanation of
the pervasiveness condition in the following proposition.
Proposition 3.2. Suppose there exists a constant C0 such that (cid:107)Σu(cid:107)2 ≤ C0. The factors
ft are pervasive if and only if the coherence µ(V ) for Vd×r = (v1, . . . , vr) is bounded and
λi(Σ) (cid:16) d for i ≤ r.

11

j=1 v2

Proof. We ﬁrst prove the necessary condition. By Wely’s inequality, |λi − λi| ≤ (cid:107)Σu(cid:107) ≤ C0.
So this implies that λi = λi(BT B) (cid:16) d if and only if λi = λi(Σ) (cid:16) d for i ≤ r. Also
(cid:80)r
λi(Σ) ≤ C0 for i ≥ r + 1.
If µ(V ) is bounded, Σii must also be bounded, since Σii ≤
ijλj(Σ) + λr+1(Σ) ≤ C(µ(V ) + 1). Therefore (cid:107)bi(cid:107)2 ≤ (cid:107)bi(cid:107)2 + (Σu)ii = Σii implies
B = ((cid:101)b1, . . . ,(cid:101)br). Then obviously λi = (cid:107)(cid:101)bi(cid:107)2 (cid:16) d and vi =(cid:101)bi/(cid:107)(cid:101)bi(cid:107). Without loss of generality,
assume ¯λi’s are decreasing. So (cid:107)vi(cid:107)∞ ≤ (cid:107)B(cid:107)max/(cid:107)(cid:101)bi(cid:107) ≤ C/

(cid:107)B(cid:107)max is bounded. Namely, the factors are pervasive.

On the contrary, if pervasiveness holds, we need to prove that µ(V ) is bounded. Let

√
d and µ(V ) ≤ C where V =

(v1, . . . , vr). By Theorem 2.1,

(cid:107)vi − vi(cid:107)∞ ≤ C

(cid:107)Σu(cid:107)∞
d
γ

√

,

where γ = min{λi − λi+1 : 1 ≤ i ≤ r} (cid:16) d with convention λi+1 = 0. Hence, we have
√
(cid:107)vi(cid:107)∞ ≤ C/
d, which implies bounded coherence µ(V ).

Notice from the above proof that as long as (cid:107)Σu(cid:107)∞ (cid:28) d and

γ = min{λi − λi+1 : 1 ≤ i ≤ r}

is of the order d as implied by the pervasiveness condition, by Theorem 2.1, (cid:107)vi − vi(cid:107)∞ (cid:28)
√
d. This perturbation bound only accounts for the eﬀect of residual matrix. Next, we will
1/
consider perturbation caused by noise and illustrate how the results can be used for robust
covariance estimation for Σ when we further assume Σu has a sparse representation.

Let (cid:98)Σ be an initial estimate of covariance matrix, which is either the sample covariance

matrix, when the noise is known to be sub-Gaussian, or the robust estimate constructed in
(3.4). We can then readily apply Theorem 2.1 to

i=1

where (cid:98)Σ is viewed as a perturbed version of the low rank part. The following results are a
Proposition 3.3. Let wn =(cid:112)log d/n + 1/

√
d. Assume that there are constants C0 > 0 such
that (cid:107)Σu(cid:107)2 ≤ C0. If the factors are pervasive, then with probability greater than 1 − d−1, we

natural corollary of Theorem 2.1.

r(cid:88)

(cid:98)Σ =

i + ((cid:98)Σ − Σ) + Σu,

λivivT

12

have

(cid:107)(cid:98)Σ − Σ(cid:107)max ≤ C(cid:112)log d/n,
|(cid:98)λi/λi − 1| ≤ C(cid:112)log d/n,
(cid:107)(cid:98)vi − vi(cid:107)∞ ≤ C wn/

√
d,

(3.6)

(3.7)

(3.8)

for i ≤ r and some constant C > 0, where(cid:98)λi,(cid:98)vi are eigenvalues and eigenvectors of (cid:98)Σ.
by Wely’s inequality and Theorem 2.1 (noting that (cid:107)Σu(cid:107)∞ ≤ √

Conclusion (3.6) is implied from Proposition 3.1 and (3.7), (3.8) are derived from (3.6)
d(cid:107)Σu(cid:107)). We omit the details.
Finally, in order to see how the above results are related with robust covariance estimation, we
review a generic principal orthogonal complement thresholding (POET) procedure provided
in Fan et al. (2015) on factor model based covariance matrix estimation.
We further assume Σu is sparse, whose sparsity level is measured by

(cid:88)

j≤d

md := max
i≤d

|Σu,ij|q

(3.9)

for some q ∈ [0, 1]. In particular, with q = 0, md corresponds to the maximum number of
nonzero elements in each row of Σu. The generic POET procedure follows three steps:

(1) Given three initial estimators (cid:98)Σ,(cid:98)Λ = diag((cid:98)λ1, . . . ,(cid:98)λr),(cid:98)V = ((cid:98)v1, . . . ,(cid:98)vr) for true co-
(v1, . . . , vr) respectively, the principal orthogonal complement(cid:98)Σu is computed by elim-

variance Σ, leading eigenvalues Λ = diag(λ1, . . . , λr) and leading eigenvectors V =

inating the leading low-rank estimate, i.e.,

(cid:98)Σu =(cid:98)Σ −(cid:98)V(cid:98)Λ(cid:98)V T ;

In this step, ˆΣ, ˆΛ and ˆV can even be obtained from diﬀerent methods.

(2) The adaptive thresholding is applied to (cid:98)Σu to obtain (cid:98)Σ(cid:62)
(cid:40) (cid:98)Σu,ij,
sij((cid:98)Σu,ij)I(|(cid:98)Σu,ij| ≥ τij),

(cid:98)Σ(cid:62)

u,ij =

u , that is,

i = j
i (cid:54)= j

,

(3.10)

(3.11)

where sij(·) is the generalized shrinkage function (Antoniadis and Fan, 2001; Roth-
man et al., 2009) and τij = τ (ˆσu,iiˆσu,jj)1/2 is an entry-dependent threshold. τ will be
determined later in Theorem 3.1. This step explores the sparsity of Σu.

(3) The low-rank structure is added back to obtain the ﬁnal estimator (cid:98)Σ(cid:62) = (cid:98)V(cid:98)Λ(cid:98)V T +(cid:98)Σ(cid:62)

u .

13

Fan et al. (2015) provided a high level suﬃcient condition for this simple procedure to

be eﬀective in covariance estimation. For completeness, we recast it here:

Theorem 3.1. Let wn =(cid:112)log d/n + 1/
model (3.5), if ∃ C > 0 such that C−1 ≤ (cid:107)Σu(cid:107)2 ≤ C and if estimators (cid:98)Σ,(cid:98)Λ,(cid:98)V satisfy (3.6) -

√
d. Under the pervasiveness condition of the factor

n = o(1), the following rates of convergence hold with the

(3.8), then with τ (cid:16) wn, if mdw1−q
generic POET procedure:

and

(cid:16)

(cid:17)

n

mdw1−q

(cid:107)(cid:98)Σ(cid:62)
u − Σu(cid:107)2 = OP
(cid:16)
(cid:107)(cid:98)Σ(cid:62) − Σ(cid:107)max = OP
(cid:16)√
(cid:107)(cid:98)Σ(cid:62) − Σ(cid:107)Σ = OP
(cid:107)((cid:98)Σ(cid:62))−1 − Σ−1(cid:107)2 = OP

wn

n

= (cid:107)((cid:98)Σ(cid:62)
(cid:17)
(cid:16)

,

d log d

mdw1−q

n

,

u )−1 − Σu
(cid:17)

+ mdw1−q

n

,

(cid:17)

−1(cid:107)2 ,

(3.12)

(3.13)

where (cid:107)A(cid:107)Σ = d−1/2(cid:107)Σ−1/2AΣ−1/2(cid:107)F is the relative Frobenius norm.

Since the proposed robust covariance estimator (3.4) induced the quantities that satisfy

(3.6) – (3.8), its induced POET estimator(cid:98)Σ(cid:62) then satisﬁes properties given by Theorem 3.1.
the ellipsity assumptions. In this sense, our newly proposed POET estimator(cid:98)Σ(cid:62), while more

This robust estimator has the same properties as that in Fan et al. (2013) without the sub-
Gaussianity assumption, and also the same properties as that in Fan et al. (2015) without

complicated, is more genuinely robust. The key to the successful technical derivations are
due to the newly developed (cid:96)∞ eigenvector perturbation bound.

√
d in wn is the price we need to pay for estimating the unknown factors. But
The term 1/
in the high dimensional regime, this term can easily be negligible when the dimensionality
is suﬃciently high (d/ log d (cid:29) n) and the rates are optimal. Therefore, out method achieves
simultaneously the optimality and robustness.

4 Simulations

4.1 Simulation: the perturbation result

In this subsection, we implement numerical simulations to verify the perturbation bound
in Theorem 2.1. We will show that the error behaves in the same way as indicated by our
theoretical bound.

In the experiments, we let the matrix size d run from 200 to 2000 by an increment
of 200. We ﬁx the rank of A to be 3 (r = 3). To generate an incoherence low rank

14

Figure 1: The left plot shows the perturbation error of eigenvectors against matrix size d
ranging from 200 to 2000, with diﬀerent eigengap γ. The right plot shows log(err) against
log(d). The slope is around −0.5. Blue lines represent γ = 10; red lines γ = 50; green lines
γ = 100; and black lines γ = 500. We report the largest error over 100 runs.

matrix, we sample a d × d random matrix with iid standard normal variables, perform
singular value decomposition, and extract the ﬁrst r right singular vectors v1, v2, . . . , vr. Let
V = (v1, . . . , vr) and D = diag(rγ, (r− 1)γ, . . . , γ) where γ is the eigengap parameter. Then,
the rank r matrix A is set to be A = V DV T . By orthogonal invariance, vi is uniform
distributed on the unit sphere Sd−1. It is easy to see that with probability 1 − O(d−1), the
√
coherence of V is µ(V ) = O(

We consider two types of sparse perturbation matrices E: (a) construct a d × d matrix
E0 by randomly selecting s entries for each row, and sampling a uniform number in [0, L] for
each entry, and then symmetrize the perturbation matrix by setting E = (E0 + ET
0 )/2; (b)
pick ρ ∈ (0, 1), L(cid:48) > 0, and let Eij = L(cid:48)ρ|i−j|. Note that in (b) we have (cid:107)E(cid:107)∞ ≤ 2L(cid:48)/(1 − ρ),
and thus we can choose suitable L(cid:48) and ρ to control the norm of E. This covariance structure
is common in cases where correlation between random variables depends on their ‘distance’
|i − j|, which usually arises from autoregressive models.

log d).

The perturbation of eigenvectors is measured by the element-wise error:

(cid:107)ηi(cid:101)vi − vi(cid:107)∞,

err := max
1≤i≤r

min
ηi∈{±1}

15

20060010001400180000.050.10.150.20.250.30.350.4derr5.566.577.5−6.5−5.6−4.7−3.8−2.9−2−1.1logdlog(err)√
Figure 2: The left plot shows the perturbation error of eigenvectors against matrix size d
√
ranging from 200 to 2000, when γ
d is kept ﬁxed, with diﬀerent values. The right plot
√
d against d. Blue lines represent γ
d = 2000; red lines
shows the error multiplied by γ
d = 5000. We report the largest
γ
error over 100 runs.

√
d = 4000; and black lines γ

d = 3000; green lines γ

where {(cid:101)vi}r

i=1 is the eigenvectors of (cid:101)A = A + E in the descending order.

√

√

To investigate how the error depends on γ and d, we generate E according to mechanism
(a) with s = 10, L = 3, and run simulations in diﬀerent parameter conﬁgurations: (1) let
the matrix size d range from 200 to 2000, and choose the eigengap γ in {10, 50, 100, 500}
d to be one of {2000, 3000, 4000, 5000}, and let the matrix
(Figure 1); (2) ﬁx the product γ
size d run from 200 to 2000 (Figure 2).

√

To ﬁnd how the errors behave for E generated from diﬀerent methods, we run simulations
as in (1) but generate E diﬀerently. We construct E through mechanism (a) with L =
10, s = 3 and L = 0.6, s = 50, and also through mechanism (b) with L(cid:48) = 1.5, ρ = 0.9 and
L(cid:48) = 7.5, ρ = 0.5 (Figure 3). The parameters are chosen in a way such that (cid:107)E(cid:107)∞ is about
30, since we can always rescale the eigenvalues.

In Figure 1 – 3, we report the largest error based on 100 runs. Figure 1 shows that the
√
error decreases as d increases (the left plot); and moreover, the logarithm of the error is linear
in log(d), with a slope −0.5, that is, err ∝ 1/
d (the right plot). We can take the eigengap
γ into consideration and characterize the relationship in a more reﬁned way. In Figure 2, it is
clear that err almost falls on the same horizontal line for diﬀerent conﬁguration of d and γ,

16

20060010001400180000.0050.010.0150.020.0250.03derr200600100014001800515253545556575derr×γ√dFigure 3: These plots show log(err) aginst log(d), with matrix size d ranging from 200 to
2000 and diﬀerent eigengap γ. The perturbation E is generated from diﬀerent ways. Top
left: L = 10, s = 3; top right: L = 0.6, s = 50; bottom left: L(cid:48) = 1.5, ρ = 0.9; bottom right:
L(cid:48) = 7.5, ρ = 0.5. The slopes are around −0.5. Blue lines represent γ = 10; red lines γ = 50;
green lines γ = 100; and black lines γ = 500. We report the largest error over 100 runs.

d ﬁxed. The right panel clearly indicates that err× γ
√
d is a constant, and therefore
d. In Figure 3, we ﬁnd that the errors behave almost the same regardless of how

√

√
with γ
err ∝ 1/γ
E is generated.

These simulation results provide stark evidence for supporting Theorem 2.1, in terms of

the rates of convergence in

d.

√
d and γ

√

4.2 Simulation: robust covariance esitmation

We consider the performance of the general POET procedure in robust covariance es-
timation in this subsection. Note that the procedure is ﬂexible in employing any initial

estimator (cid:98)Σ,(cid:98)Λ,(cid:98)V satisfying the conditions (3.6) – (3.8).

We simulated n samples of (f T

t )T from two distribution settings: (a) a multivariate
t-distribution with covariance matrix diag{Ir, 5Id} and various degrees of freedom (ν = 3
for very heavy tail, ν = 5 for medium heavy tail and ν = ∞ for Gaussian tail), which

t , uT

17

5.566.577.5−5.8−5−4.2−3.4−2.6−1.8−15.566.577.5−7.3−6.3−5.2−4.2−3.2−2.2−1.15.566.577.5−7−6−5.1−4.1−3.1−2.2−1.25.566.577.5−6.6−5.7−4.9−4−3.1−2.3−1.4log(err)logdis an elliptical distribution (Fang et al., 1990); (b) an element-wise iid one-dimensional t
distribution with the same covariance matrix and degree of freedom ν = 3, 5 and ∞, which
is a non-elliptical heavy-tailed distribution.

Each row of coeﬃcient matrix B is independently sampled from a standard normal dis-
tribution, so that with high probability, the pervasiveness condition holds with (cid:107)B(cid:107)max =
√
log d). The data is then generated by yt = Bft + ut and the true population covariance
O(
matrix is Σ = BBT + 5Id.

We implemented the robust estimator with four diﬀerent initial trios: (1) the sample

covariance (cid:98)ΣS with its leading r eigenvalues and eigenvectors as (cid:98)ΛS and (cid:98)V S; (2) the general
Huber’s robust estimator (cid:98)ΣR given in (3.4) and its top r eigen-structure estimators (cid:98)ΛR and
(cid:98)V R; (3) the marginal Kendall’s tau estimator (cid:98)ΣK with its corresponding (cid:98)ΛK and (cid:98)V K; (4)
instead of the marginal Kendall’ tau, so (cid:98)V K in (3) is replaced with (cid:101)V K. We need to brieﬂy
(cid:98)ΣK and (cid:101)V K.

lastly, we use the multivariate Kendall’s tau estimator to estimate the leading eigenvectors

review the two types of Kendall’s tau estimators here, and speciﬁcally give the formula for

Kendall’s tau correlation coeﬃcient, for estimating pairwise comovement correlation, is

deﬁned as

ˆτjk :=

2

n(n − 1)

sgn((ytj − yt(cid:48)j)(ytk − yt(cid:48)k)) .

It is related to the Pearson correlation via the transform ˆrjk = sin

distributions. Letting (cid:98)R = (ˆrjk) and (cid:98)D = diag((cid:98)ΣR), we deﬁne the marginal Kendall’s tau

for elliptical

2 ˆτjk

estimator as

(cid:88)

t<t(cid:48)

(cid:98)ΣK = (cid:98)D (cid:98)R(cid:98)D .

(cid:16) π

(cid:17)

(4.1)

(4.2)

In the above deﬁnition, we still use the robust variance estimators to construct the estimated
covariance matrix.

The multivariate Kendall’s tau estimator is a second-order U-statstic, deﬁned as

(cid:101)ΣK :=

2

n(n − 1)

(yt − yt(cid:48))(yt − yt(cid:48))T

(cid:107)yt − yt(cid:48)(cid:107)2

2

(cid:88)

.

(4.3)

t<t(cid:48)

Then (cid:101)V S is naturally the matrix whose r columns correspond to the top r eigenvectors of
(cid:101)ΣK. It has been shown by Fan et al. (2015) that under elliptical distribution,(cid:98)ΣK and its top
r eigenvalues (cid:98)ΛK satisfy (3.6) and (3.7) while (cid:101)V S suﬃces to conclude (3.8). Hence Method
4 indeed provides good initial estimators. However, since (cid:98)ΣK attains (3.6) for elliptical
bound, the leading eigenvectors (cid:98)V K of (cid:98)ΣK is also valid for general POET. For more details

distribution, by similar argument for deriving Proposition 3.3 based on our (cid:96)∞ pertubation

18

Figure 4: Error ratios of robust estimates against varying dimension. Blue lines represent
errors of Method 2 over Method 1 under diﬀerent norms; black lines errors of Method 3 over
Method 1; red lines errors of Method 4 over Method 1. (f T
t ) is generated by multivariate
t-distribution with df = 3 (solid), 5 (dashed) and ∞ (dotted). The median errors and their
IQR over 100 simulations are reported.

t , uT

about the two types of Kendall’s tau, we refer the readers to Fang et al. (1990); Choi and
Marden (1998); Han and Liu (2014); Fan et al. (2015) and references therein.

For d running from 200 to 900 and n = d/2, we calculated errors of the four robust
procedure in diﬀerent norms. The tuning for α in minimization (3.4) is discussed more

throughly in Fan et al. (2016b). For the thresholding parameter, we used τ = 2(cid:112)log d/n.
The estimation errors are gauged in the following norms: (cid:107)(cid:98)Σ(cid:62)
u − Σu(cid:107), (cid:107)((cid:98)Σ(cid:62))−1 − Σ−1(cid:107) and

19

2004006008000.00.51.01.52.02.53.0Spectral norm error of Sigma_u (Median)perror ratio2004006008000.00.51.01.52.02.53.0Spectral norm error of Sigma_u (IQR)perror ratio2004006008000.00.51.01.52.02.53.0Spectral norm error of inverse Sigma (Median)perror ratio2004006008000.00.51.01.52.02.53.0Spectral norm error of inverse Sigma (IQR)perror ratio2004006008000.00.40.81.2Relative Frobenius norm error of Sigma (Median)perror ratio2004006008000.00.40.81.2Relative Frobenius norm error of Sigma (IQR)perror ratio(cid:107)(cid:98)Σ(cid:62) − Σ(cid:107)Σ as shown in Theorem 3.1. The two diﬀerent settings are separately plotted in
Figures 4 and 5. The estimation errors of applying sample covariance matrix (cid:98)ΣS in Method
measure performance, (cid:107)((cid:98)Σ(cid:62))(k) − Σ(cid:107)Σ/(cid:107)((cid:98)Σ(cid:62))(1) − Σ(cid:107)Σ will be depicked for k = 2, 3, 4, where
((cid:98)Σ(cid:62))(k) are generic POET estimators based on Method k. Therefore if the ratio curve moves

1 are used as the baseline for comparison. For example, if relative Frobenius norm is used to

below 1, the method is better than naive sample estimator (Fan et al., 2013) and vice versa.
The more it gets below 1, the more robust the procedure is against heavy-tailed randomness.
The ﬁrst setting (Figure 4) represents a heavy-tailed elliptical distribution, where we
expect Methods 2, 3, 4 all outperform the POET estimator based on the sample covariance,
i.e. Method 1, especially in the presence of extremely heavy tails (solid lines for ν = 3).
As expected, all three curves under various measures show error ratios visibly smaller than
1. On the other hand, if data are indeed Gaussian (dotted line for ν = ∞), Method 1 has
better behavior under most measures (error ratios are greater than 1). Nevertheless, our
robust Method 2 still performs comparably well with Method 1, whereas the median error
ratios for the two Kendall’s tau methods are much worse than the proposed robust method.
A plausible explanation is that the variance reduced compensates for the bias incurred in
our procedure.
In addition, the IQR plots reveals Method 2 is indeed more stable than
two Kendall’s tau Methods 3 and 4. It is also noteworthy that Method 4, which leverages
the advantage of multivariate Kendall’s tau, performs more robustly than Method 3, which
solely base its estimation of eigen-structure on marginal Kendall’s tau.

The second setting (Figure 5) provides an example of non-elliptical distributed heavy-
tailed data. We can see that the performance of the general robust Method 2 dominates the
other three methods, which veriﬁes the beneﬁts of robust estimation for a general heavy-
tailed distribution. However, Kendall’s tau method does not apply to distributions outside
the elliptical family, excluding even the element-wise iid t distribution in this setting. Note
that even in the ﬁrst setting where the data are indeed elliptical, with proper tuning, the
proposed robust method can still outperform Kendall’s tau by a clear margin.

5 Proof of Theorem 1.1 and Theorem 2.1

Proof of Theorem 2.1. The proof we present is motivated by the proof of classic Davis-Kahn
theorem (Davis and Kahan, 1970), but to obtain the sharp bound, we shall render a much
more reﬁned analysis.

We begin with an analysis of the eigenvalues. By Weyl’s theorem,

|(cid:101)λj − λj| ≤ (cid:107)E(cid:107) ≤ (cid:107)E(cid:107)∞ = τ,

max
1≤j≤p

20

Figure 5: Error ratios of robust estimates against varying dimension. Blue lines represent
errors of Method 2 over Method 1 under diﬀerent norms; black lines errors of Method 3 over
Method 1; red lines errors of Method 4 over Method 1. (f T
t ) is generated by element-wise
iid t-distribution with df = 3 (solid), 5 (dashed) and ∞ (dotted). The median errors and
their IQR over 100 simulations are reported.

t , uT

where we used symmetry of E and the well-known result (cid:107)E(cid:107)2 ≤ (cid:107)E(cid:107)1(cid:107)E(cid:107)∞. From the

condition on γ, we know γ > 2τ . Therefore, the multiplicity of(cid:101)λi remains one, and thus, its
associated eigenvector(cid:101)vi is unambiguous up to a sign.

21

2004006008000.00.51.01.52.02.53.0Spectral norm error of Sigma_u (Median)perror ratio2004006008000.00.51.01.52.02.53.0Spectral norm error of Sigma_u (IQR)perror ratio2004006008000.00.51.01.52.02.53.0Spectral norm error of inverse Sigma (Median)perror ratio2004006008000.00.51.01.52.02.53.0Spectral norm error of inverse Sigma (IQR)perror ratio2004006008000.00.40.81.2Relative Frobenius norm error of Sigma (Median)perror ratio2004006008000.00.40.81.2Relative Frobenius norm error of Sigma (IQR)perror ratioSuppose the eigen-decomposition of A is given by

(V, Z)T A(V, Z) =

(cid:33)

,

L1 0
0
0

(cid:32)

(cid:32)

where L1 = diag{λ1, . . . , λr}. By assumption, the top r eigenvalues have multiplicity of one,
and thus the columns of V are unique up to signs. Expressing E in terms of the orthogonal
basis (V, Z), we have

(V, Z)T E(V, Z) :=

E11 E12
E21 E22

(cid:33)

.

Note that E12 = ET

matrix (cid:101)A are close to V , and they diﬀer in some small linear combination of Z. This intuition

21 since E is symmetric. Conceptually, the eigenvectors of the perturbed

leads us to consider

and

V = (V + ZQ)(Ir + QT Q)−1/2,

Z = (Z − V QT )(Id−r + QQT )−1/2,

where Q ∈ R(d−r)×r is to be determined. Note that the second components in V and Z serve

to make (V , Z) an orthogonal basis. If we wish (V , Z)T(cid:101)A(V , Z) to be diagonal, a necessary

T(cid:101)AV = 0. Recalling E12 = ET

21 and expanding the terms gives

condition is Z

E21 − QE12Q = Q(L1 + E11) − E22Q.

(5.1)

Instead of studying the system (5.1), we multiply two sides by Z T Z, which is an identity
matrix, for reasons to be explained below. Now (5.1) is equivalent to

Z T(cid:0)ZE21 − ZQ(ZE21)T ZQ(cid:1) = Z T(cid:0)ZQ(L1 + E11) − ZE22Z T ZQ(cid:1) ,

(5.2)

where we used symmetry E12 = ET
21 again. For notational convenience, we write H =
ZE21, Q = ZQ, L1 = L1 + E11, L2 = ZE22Z T , and we deﬁne a linear operator T : Rd×r →
Rd×r as

T (Q) := Q L1 − L2Q.

If we can ﬁnd a solution Q to the equation

T (Q) = H − QH T Q,

(5.3)

and there exists Q such that Q = ZQ, then it is easy to check that Q is a solution to

22

orthogonal transformation, as will be seen below.

(5.1). To see this, multiplying the two sides of (5.3) by Z T , we will recover (5.1), which then
implies Z

T(cid:101)AV = 0. In other words, under the basis (V , Z), (cid:101)A is a block diagonal matrix.
The caveat is that V is not the perturbed eigenvector (cid:101)V yet; actually, they are up to an
Notice that V is almost the eigenvectors we are looking for, since (cid:101)A is a block diago-
T(cid:101)AV , V R diagonalizes (cid:101)A. Let us denote (cid:101)V = V R, whose columns are
orthonormal. Breaking (cid:101)V − V into two parts,

nal matrix under the basis formed by (V , Z). In fact, for any r × r orthogonal matrix R
that diagonalizes V

(cid:101)V − V = V (R − Ir) + (V − V ),

we only need to study the bounds of the two components.

Bounding (cid:107)V − V (cid:107)max: By Lemma 6.4, if γ > 4rµ(V )(τ +2rκ), there indeed exists a solution
Q to (5.3), and Q ∈ R(d−r)×r satisfying Q = ZQ. Moreover, we have the following bound

(cid:107)Q(cid:107)max ≤ ω0√
d

,

where

ω0 =

We will assume that the condition r2ω2
able to evaluate the deviation of V from V . Decomposing V − V into two parts,

2(1 + rµ(V ))κ

γ − 3rµ(V )(τ + rκ)
0 < 1/2 holds for the rest of the proof. Now we are

.

V − V = V (cid:0)(Ir + QT Q)−1/2 − Ir

T
(Ir + Q

Q)−1/2 − Ir

(cid:16)

= V

(cid:1) + ZQ(Ir + QT Q)−1/2
(cid:17)

T
+ Q(Ir + Q

Q)−1/2,

(5.4)

(5.5)

where we used identity Z T Z = Id−r. This decomposition provides motivation for studying
the bound of Q in order to understand the perturbation V − V .
Q(cid:107) ≤ r(cid:107)Q

Q(cid:107)max ≤ rω2

Q(cid:107)max ≤ ω2

0 and (cid:107)Q

0. It can be

T

T

T

An obvious bound gives (cid:107)Q
0 < 1/2, then

shown that if rω2

(cid:107)(Ir + Q
(cid:107)(Ir + Q

T

T

Q)−1/2 − Ir(cid:107)max ≤ rω2
0,
Q)−1/2(cid:107)max ≤ 3/2,

(5.6)

(5.7)

the details of which are left to Lemma 6.5 in the Appendix.

23

By Cauchy-Schwarz inequality and the deﬁnition of µ(V ),

(Ir + Q

T

Q)−1/2 − Ir

≤

r2µ(V )

d

(cid:107)(Ir + Q

T

Q)−1/2 − Ir(cid:107)max .

(cid:16)

(cid:13)(cid:13)(cid:13)V

This yields

(cid:114)

(cid:17)(cid:13)(cid:13)(cid:13)max
(cid:114)

(cid:107)V − V (cid:107)max ≤

r2µ(V )

d

rω2

0 +

3
2

r(cid:107)Q(cid:107)max ≤ ω1√
d

,

(5.8)

where ω1 :=(cid:112)µ(V ) ω2

0r2 + 3ω0r/2.

Bounding (cid:107)V (R − Ir)(cid:107)max: An apparent bound for V is

(cid:107)V (cid:107)max ≤ (cid:107)V − V (cid:107)max + (cid:107)V (cid:107)max ≤ ω1 +(cid:112)µ(V )r

√

Observe that ω1 =(cid:112)µ(V )r2ω2

0 + 3rω0/2 ≤ 2(cid:112)rµ(V ). Therefore, (cid:107)V (cid:107)∞ is neatly bounded

d

.

by

(cid:114)

(cid:107)V (cid:107)max ≤ 3

µ(V )r

d

.

(5.9)

To study how close R is to Ir, we will express R in terms of familiar quantities. By deﬁnition
of R, we have V

V R = R, which gives

Q)−1/2(V + Q)T(cid:101)V .

d(cid:107)Q(cid:107)max ≤ ω0, (cid:107)(cid:101)V T V (cid:107)max ≤ 1 and (cid:107)(Ir + Q

T

Q)−1/2 − Ir(cid:107)max ≤

T

T(cid:101)V = V
T(cid:101)V (cid:107)max ≤ √
(cid:107)R − (V + Q)T(cid:101)V (cid:107)max ≤ r(cid:107)(Ir + Q

T
R = (Ir + Q

Notice that (cid:107)Q
rω2

0, we have

Q)−1/2 − Ir(cid:107)max (cid:107)(V + Q)T(cid:101)V (cid:107)max

T

Furthermore, by Davis-Kahn theorem (Davis and Kahan, 1970), for any i ≤ r,

where sep(A, (cid:101)A) := min{|λi(A)− λi+1((cid:101)A)|∧|λi(A)− λi−1((cid:101)A)| : 1 ≤ i ≤ r} (as usual λ0(A) =
λr+1(A) := 0). By Weyl’s theorem and the deﬁnition of γ, we have sep(A, (cid:101)A) ≥ γ − (cid:107)E(cid:107).

(5.10)

(5.11)

≤ r2ω2

0(1 + ω0) .

(cid:112)1 − (cid:104)vi,(cid:101)vi(cid:105)2 ≤ (cid:107)E(cid:107)
sep(A, (cid:101)A)

.

24

Using (cid:107)E(cid:107)2 ≤ (cid:107)E(cid:107)1(cid:107)E(cid:107)∞ = τ 2, (5.11) becomes

(cid:112)1 − (cid:104)vi,(cid:101)vi(cid:105)2 ≤ τ

γ − τ

When the sign of(cid:101)vj is appropriated chosen,

.

(5.12)

In other words, the diagonal elements of V T(cid:101)V − Ir are bounded by τ 2/(γ − τ )2. By pairwise
i(cid:48)(cid:54)=i(cid:104)vi,(cid:101)vi(cid:48)(cid:105)2 ≥ (cid:104)vi,(cid:101)vj(cid:105)2 for any j (cid:54)= i. So
orthogonality of {(cid:101)vi}r

(5.13)

(cid:112)1 − (cid:104)vi,(cid:101)vi(cid:105) ≤(cid:112)1 − (cid:104)vi,(cid:101)vi(cid:105)2 ≤ τ
i=1, we have 1 − (cid:104)vi,(cid:101)vi(cid:105)2 ≥(cid:80)
(cid:107)V T(cid:101)V − Ir(cid:107)max ≤ max

(cid:110) τ

(cid:111)

τ 2

=

γ − τ

,

(γ − τ )2

γ − τ

.

τ

γ − τ

≤ 2τ
γ

,

(5.14)

where the equation and last inequality hold due to the assumption that γ ≥ 4rµ(V )(τ +
2rκ) ≥ 2τ . Thus, from (5.10) and (5.14),

(cid:107)R − Ir(cid:107)max ≤ (cid:107)R − (V + Q)T(cid:101)V (cid:107)max + (cid:107)V T(cid:101)V − Ir(cid:107)max + (cid:107)Q

T(cid:101)V (cid:107)max

≤ r2ω2

0(1 + ω0) +

2τ
γ

+ ω0 .

(5.15)

Before producing the ﬁnal bound, we make a few simpliﬁcation of the above bound, to

√
0 < 1/2, it implies ω0 ≤ rω0 < 1/

2, and thus,

make it less messy. Since r2ω2

r2ω2

0(1 + ω0) + ω0 ≤ (

1√
2

+

1
2

+ 1)rω0 ≤ 9
4

rω0 .

Similarly,

ω1 =(cid:112)µ(V )r2ω2

0 + 3rω0/2 ≤(cid:0) 1√

(cid:1)r(cid:112)µ(V )ω0 ≤ 9

4

r(cid:112)µ(V ) ω0 ,

+

3
2

2

ω0 =

2(1 + rµ(V ))κ

γ − 3rµ(V )(τ + rκ)

≤ 5(1 + rµ(V ))κ

γ

,

(5.16)

(5.17)

where in the last inequality we used condition γ > 5rµ(V )(τ + 2rκ).

Bounding (cid:107)(cid:101)V − V (cid:107)max: Using (5.4), (5.8), (5.9), and (5.15) – (5.17), we now put the pieces

25

together to produce the ﬁnal bound.

(cid:107)(cid:101)V − V (cid:107)max = (cid:107)V (R − Ir) + (V − V )(cid:107)max ≤ r(cid:107)V (cid:107)max(cid:107)R − Ir(cid:107)max + (cid:107)V − V (cid:107)max

(cid:114)

≤ 3r

≤ 72 r5/2

µ(V )r

(cid:17)

≤(cid:16)
(cid:16)9
(cid:112)µ(V )(1 + rµ(V ))(κ + τ )

ω1√
d

rω0 +

2τ
γ

+

d

4

√

γ

d

(cid:114)

(cid:17) ·

9r5/2 ω0 + 6r3/2 τ
γ

µ(V )

d

under the condition γ > 5rµ(V )(τ + 2rκ), which also implies r2ω2
the proof.

0 < 1/2. This completes

Proof of Theorem 1.1. Let Ad, Ed be d1 + d2 square matrices deﬁned as

(cid:32)

(cid:33)

(cid:33)

.

Ad :=

,

Ed :=

0 A
AT
0

0 E
ET
0

Also denote (cid:101)Ad := Ad + Ed. Here the superscript d means Hermition dilation. We use this
notation to denote quantities corresponding to Ad and (cid:101)Ad.
(cid:32)
From the observation that(cid:32)

(cid:33)(cid:32)

(cid:33)

(cid:33)

0 A
AT
0

ui
± vi

= ± σi

ui
± vi

,

we know that Ad is of rank 2r with nonzero eigenvalues ± σi. Its corresponding eigenvectors
are given by (uT

i )T . Thus, the normalized eigenvector matrix V d is written as

i ,± vT

,

(cid:32)

(cid:33)

.

(cid:33)

(cid:32)

V d =

1√
2

U
U
V −V

The coherence of V d is not bounded if d1 and d2 are not of the same order. In order to deal
with the diﬀerent scales of U and V , we need to deﬁne the weighted max norm for any M
with d1 + d2 rows,

(cid:13)(cid:13)(cid:13)(cid:32) √

(cid:107)M(cid:107)w =

d1Id1
0

√

0
d2Id2

(cid:13)(cid:13)(cid:13)max

M

,

(5.18)

and go through the proof of Theorem 2.1 again with this norm. Conceptually, the proof
is almost identical, although the derivation can be still tedious. We only highlight the
modiﬁcations needed in this proof. Note that the eigengap of A0 is still γ0, and recall

26

τ0 :=(cid:112)d1/d2(cid:107)E(cid:107)∞ ∨(cid:112)d2/d1(cid:107)E(cid:107)1 and
(cid:101)V d − V d = V

d

(Rd − I2r) + (V

d − V d).

(5.19)

Bounding (cid:107)V
d
Q

d − V d(cid:107)w: By Lemma 6.8, if γ0 > 5rµ(τ0 + 3rκ0), there indeed exists a solution

to (5.3), and Qd ∈ R(d1+d2−2r)×(2r) satisfying Q

d

= Z dQd such that

(cid:107)Q

d(cid:107)w ≤ ω0 ,

where

√

d1 (cid:107)EV (cid:107)max ∨ √
and κ0 =
γ0 > 5rµ(τ0 + 3rκ0). Since (cid:107)(Q
similar to Lemma 6.5, we have

ω0 =

2(1 + rµ)κ0

γ0 − 3rµ(τ0 + rκ0)
√
d2(cid:107)ET U(cid:107)max ≤ τ0
0 and (cid:107)(Q
d(cid:107)max ≤ 2ω2

)T Q

d

≤ 5(1 + rµ)κ0

,

γ0

rµ. Note that r2ω2

d(cid:107) ≤ 2r(cid:107)(Q

0 < 1/8 is implied by
d(cid:107)max ≤ 4rω2
0,

)T Q

d

d

)T Q

(cid:107)(I2r + (Q
(cid:107)(I2r + (Q

d

d

d
)T Q
d
)T Q

)−1/2 − I2r(cid:107)max ≤ 4rω2
0,
)−1/2(cid:107)max ≤ 3/2.

This yields

(cid:107)V

d − V d(cid:107)w = (cid:107)V d(cid:16)
≤(cid:112)

d
(I2r + (Q

d
)T Q
0 + 3r(cid:107)Q

)−1/2 − I2r
d(cid:107)w ≤ ω1 ,

2r2µ 4rω2

(cid:17)

d
+ Q

d
(I2r + (Q

d
)T Q

)−1/2(cid:107)w

(5.20)

where ω1 :=

√
32µ ω2

0r2 + 3 ω0r ≤ 5rω0

√

µ.

Bounding (cid:107)V
Recall that

d

(Rd − I2r)(cid:107)w: It is not hard to show (cid:107)V

µr similar to Theorem 2.1.

Notice that (cid:107)(Q

d

(cid:107)Rd − (V d + Q

d

d
)T Q

Rd = (I2r + (Q

)T(cid:101)V d(cid:107)max ≤ √
d(cid:107)w ≤ √
)T(cid:101)V d(cid:107)max ≤ 2r(cid:107)(I2r + (Q

2(cid:107)Q

d

≤ 8r2ω2

0(1 +

d

d
)T Q

√
2 ω0) .

√
d(cid:107)w ≤ 3

)T(cid:101)V d .

d

)−1/2(V d + Q

2 ω0, (cid:107)((cid:101)V d)T V d(cid:107)max ≤ 1 and

)−1/2 − I2r(cid:107)max (cid:107)(V d + Q

d

)T(cid:101)V d(cid:107)max

27

Following the same derivations as in Theorem 2.1, we again get

)T(cid:101)V d(cid:107)max + (cid:107)(V d)T(cid:101)V d − I2r(cid:107)max + (cid:107)(Q

d

)T(cid:101)V d(cid:107)max

(cid:107)Rd − I2r(cid:107)max ≤ (cid:107)Rd − (V d + Q

d

≤ 8r2ω2

0(1 +

√
2 ω0) +

2τ0
γ0

√
√
2 ω0 ≤ 4
2 rω0 +

+

2τ0
γ0

.

(5.21)

Bounding (cid:107)(cid:101)V d − V d(cid:107)w: Using (5.19), (5.20) and (5.21), we produce the ﬁnal bound.
(cid:107)(cid:101)V d − V d(cid:107)w = (cid:107)V

d(cid:107)w(cid:107)Rd − I2r(cid:107)max + (cid:107)V

d − V d(cid:107)w

(cid:16)

d

(Rd − I2r) + (V
√
rµ

√
4
2 rω0 +
√

(cid:17)

+ ω1 ≤(cid:16)

d − V d)(cid:107)w ≤ 2r(cid:107)V
2τ0
γ0

√

µ(1 + rµ)(1 +

rµ)τ0

,

≤ 200 r5/2

≤ 6r

γ0

(cid:17) · √

µ

40r5/2 ω0 + 12r3/2 τ0
γ0

under the condition γ0 > 5rµ(τ0 + 3rκ0). This completes the proof.

6 Appendix

6.1 Lemmas for Theorem 2.1

We will adopt the same notational system as that in Theorem 2.1. Let nonzero eigenvalues
of A be λ1, . . . , λr, and their corresponding eigenvectors be v1, . . . , vr. An orthogonal basis
in the orthogonal complement of span{v1, . . . , vr} is z1, . . . , zd−r. Denote V = (v1, . . . , vr)
and Z = (z1, . . . , zd−r). Suppose that E is represented as

(cid:32)

(cid:33)

(V, Z)T E(V, Z) :=

E11 E12
E21 E22

under the orthogonal basis (V, Z). Let L1 = diag{λ1, . . . , λr}, L1 = L1 + E11 and L2 =
ZE22Z T . The ﬁrst lemma bounds the magnitude of H = ZE21.

Lemma 6.1. The max norm of H has the following bound
√
(cid:107)H(cid:107)max ≤ (1 + rµ(V ))κ/

d ,

where κ =

√

d(cid:107)PV (E)(cid:107)max.

Proof. By deﬁnition, E21 = Z T EV . We shall study the upper bound of H = ZZ T EV .

28

Notice that the columns of (V, Z) consist of an orthogonal basis of Rd, and thus

V V T + ZZ T = Id .

(6.1)

By Cauchy-Schwarz inequality and the deﬁnition of µ(V ), for any i, j ≤ d,

r(cid:88)

|(V V T )ij| =

|VikVjk| ≤

k=1

k=1

k=1

(cid:118)(cid:117)(cid:117)(cid:116) r(cid:88)

V 2
ik

(cid:118)(cid:117)(cid:117)(cid:116) r(cid:88)

jk ≤ rµ(V )
V 2

d

.

Since E is symmetric, we have the identity EV = V T E = PV (E). Therefore,

(cid:107)H(cid:107)max ≤ (cid:107)EV (cid:107)max + (cid:107)V V T EV (cid:107)max

√
≤ (1 + d(cid:107)V V T(cid:107)max)(cid:107)EV (cid:107)max ≤ (1 + rµ(V ))κ/

d ,

which completes the proof.

Lemma 6.2. If γ > κr(cid:112)µ(V ), then L1 is a non-degenerate matrix. Furthermore, we have

the following bound

(cid:107)Q0L1 − L2Q0(cid:107)max ≥ γ − 3rµ(V )(τ + rκ) ,

(6.2)

inf

(cid:107)Q0(cid:107)max=1

where Q0 is a d by r matrix.
Proof. For any d by r matrix Q0 with (cid:107)Q0(cid:107)max = 1, we will ﬁrst bound Q0E11 and L2Q0.

By deﬁnition, E11 = V T EV , and PV (E) = V T E = (EV )T by symmetry. Thus,

(cid:107)Q0E11(cid:107)max ≤ d(cid:107)Q0V T(cid:107)max(cid:107)PV (E)(cid:107)max .

By Cauchy-Schwarz inequality and the deﬁnition of µ(V ),

|(Q0V T )ij| ≤ r(cid:88)

k=1

|(Q0)ikVjk| ≤

(cid:118)(cid:117)(cid:117)(cid:116) r(cid:88)

(cid:118)(cid:117)(cid:117)(cid:116) r(cid:88)
(cid:107)Q0E11(cid:107)max ≤ κr(cid:112)µ(V ) .

(Q0)2
ik

k=1

k=1

√
for any i, j ≤ d. Recall that (cid:107)PV (E)(cid:107)max = κ/
d. It shows that

(cid:114)

jk ≤ √

V 2

r

rµ(V )

d

,

29

To bound L2Q0, observe that

L2Q0 = ZZ T EZZ T Q0 = (Id − V V T )E(Id − V V T )Q0 ,

where we used (6.1). Notice that (cid:107)EQ0(cid:107)max ≤ (cid:107)E(cid:107)∞(cid:107)Q0(cid:107)max = (cid:107)E(cid:107)∞ and (cid:107)V T Q0(cid:107)max ≤
(cid:107)V T(cid:107)∞(cid:107)Q0(cid:107)max ≤ √

d. From the two observations, we have

(cid:107)E(Id − V V T )Q0(cid:107)max ≤ (cid:107)EQ0(cid:107)max + r(cid:107)EV (cid:107)max(cid:107)V T Q0(cid:107)max
√
d ≤ τ + rκ .

≤ (cid:107)E(cid:107)∞ + r(cid:107)PV (E)(cid:107)max

In the proof of Lemma 6.1, we have shown (cid:107)V V T(cid:107)max ≤ rµ(V )/d. Thus,

(cid:107)L2Q0(cid:107)max ≤ (1 + rµ(V ))(τ + rκ) .

Moreover, (cid:107)Q0L1(cid:107)max ≥ λr(cid:107)Q0(cid:107)max ≥ γ. This lower bound, together with the upper

bounds of (cid:107)Q0E11(cid:107)max and (cid:107)L2Q0(cid:107)max, gives

(cid:107)Q0L1 − L2Q0(cid:107)max ≥ inf

inf

(cid:107)Q0(cid:107)max=1

(cid:107)Q0(cid:107)max=1

(cid:107)Q0L1(cid:107)max − sup

≥ γ − κr(cid:112)µ(V ) − (1 + rµ(V ))(τ + rκ)

(cid:107)Q0(cid:107)max=1

(cid:107)Q0E11(cid:107)max − sup

(cid:107)L2Q0(cid:107)max

(cid:107)Q0(cid:107)max=1

≥ γ − 3rµ(V )(τ + rκ) ,

which is the desired inequality in the lemma. To show nondegeneracy of L1, notice

inf

(cid:107)Q0L1(cid:107)max ≥ (cid:107)Q0L1(cid:107)max − (cid:107)Q0E11(cid:107)max

(cid:107)Q0(cid:107)max=1

≥ γ − κr(cid:112)µ(V ) .
If γ − κr(cid:112)µ(V ) > 0, the matrix L1 must be non-degenerate.

Having established the bounds of (cid:107)H(cid:107)max and the induced norm of linear operator T , we
now show a critical lemma, which is crucial to our upper bound of Q, and subsequently that
of the perturbation of the eigenvectors. This lemma is similar to Stewart (1990); however,
it has to be modiﬁed and tailored for our purposes. For the sake of completeness, we also
provide a proof for this lemma.
Lemma 6.3. Let T be a bounded linear operator on a Banach space B equipped with a norm
(cid:107) · (cid:107). Assume that T has a bounded inverse, and set β = (cid:107)T −1(cid:107)−1. Let ϕ : B → B be a
function that satisﬁes

(cid:107)ϕ(x)(cid:107) ≤ η(cid:107)x(cid:107)2

30

and

(cid:107)ϕ(x) − ϕ(y)(cid:107) ≤ 2η max{(cid:107)x(cid:107),(cid:107)y(cid:107)}(cid:107)x − y(cid:107)

for some η ≥ 0. Suppose that B0 is a closed subspace of B such that T −1(B0) ⊆ B0 and
ϕ(B0) ⊆ B0. For any g ∈ B0 that satisﬁes

αη
β2 < 1,
where α = (cid:107)g(cid:107), the sequence deﬁned by x0 = 0 and

4

xk+1 = T −1(g + ϕ(xk)),

k ≥ 0

(6.3)

converges to a solution x(cid:63) of

Moreover, x(cid:63) ⊆ B0, and

T x = g + ϕ(x).

(cid:107)x(cid:63)(cid:107) ≤ 2

α
β

.

Proof. We ﬁrst construct an upper bound on (cid:107)xk(cid:107). From (6.3),

(cid:107)xk+1(cid:107) ≤ (cid:107)T −1(cid:107)((cid:107)g(cid:107) + (cid:107)ϕ(xk)(cid:107)) ≤ α
β

+

η
β

(cid:107)xk(cid:107)2.

Consequently if we set ξ0 = 0 and

ξk+1 =

α
β

+

η
β

ξ2
k,

k ≥ 0,

then (cid:107)xk(cid:107) ≤ ξk. Now the sequence ξ0, ξ1, . . . is clearly increasing. Moreover, if 4αη < β2, the
function

φ(ξ) =

+

ξ2,

α
β

η
β

β +(cid:112)β2 − 4ηα

2α

<

2α
β

.

has a smallest ﬁxed point

ξ(cid:63) =

If ξk < ξ(cid:63), then ξk+1 = φ(ξk) ≤ φ(ξ(cid:63)) = ξ(cid:63). Hence all the ξ(cid:63) are bounded by ξ(cid:63), and the
sequence {ξk} must converge to ξ(cid:63). Thus,

(cid:107)xk(cid:107) ≤ ξ(cid:63) < 2

α
β

.

31

We next show that the sequence {xk} converges. We have

(cid:107)xk+1 − xk(cid:107) ≤ (cid:107)T −1(cid:107)(cid:107)ϕ(xk) − ϕ(xk−1)(cid:107)

≤ 2β−1η max{(cid:107)xk(cid:107),(cid:107)xk−1(cid:107)}(cid:107)xk − xk−1(cid:107)
≤ 4

β2 (cid:107)xk − xk−1(cid:107).

αη

The convergence follows since 4αη/β2 < 1. Finally, {xk} ⊆ B0 by assumption, and therefore,
x(cid:63) ∈ B0 since B0 is closed. The proof is complete.

With all the preparations, we are now able to present the key lemma that supports the
Theorem 2.1. Recall that T : Rd×r → Rd×r is a linear operator. We are looking for a solution
Q to the quadratic system

T (Q) = H − QH T Q

(6.4)

with guaranteed properties. Deﬁne

B0 := {Q ∈ Rd×r : Q = ZQ for some Q ∈ R(d−r)×r}.

We now state the lemma.
Lemma 6.4. There is a solution Q ∈ B0 to the system (6.4) such that if γ > 4rµ(V )(τ +2rκ),
then

(cid:107)Q(cid:107)max ≤

(cid:0)γ − 3rµ(V )(τ + rκ)(cid:1)√

2(1 + rµ(V ))κ

.

d

Proof. We will show that Lemma 6.3 applies to the quadratic system (6.4).

Let B be Rd×r equipped with matrix max norm (cid:107) · (cid:107)max. Then T is a linear operator in

B. By Lemma 6.2, T has bounded inverse, with β = (cid:107)T −1(cid:107)−1

max bounded from below by

β ≥ γ − 3rµ(V )(τ + rκ) .

Let ϕ be given by ϕ(Q) = QH T Q. Observe that

(cid:107)ϕ(Q)(cid:107)max ≤ rd(cid:107)Q(cid:107)max(cid:107)H(cid:107)max(cid:107)Q(cid:107)max
d(cid:107)Q(cid:107)2

≤ (1 + rµ(V ))κr

√

max

32

by Lemma 6.1, and moreover

(cid:107)ϕ(Q1) − ϕ(Q2)(cid:107)max = (cid:107)Q1H T (Q1 − Q2) + (Q1 − Q2)H T Q2(cid:107)max

≤ rd(cid:107)Q1(cid:107)max(cid:107)H(cid:107)max(cid:107)Q1 − Q2(cid:107)max + rd(cid:107)Q1 − Q2(cid:107)max(cid:107)H(cid:107)max(cid:107)Q2(cid:107)max
≤ (1 + rµ(V ))κr

d max{(cid:107)Q1(cid:107)max,(cid:107)Q2(cid:107)max}(cid:107)Q1 − Q2(cid:107)max.

√

Thus, η = (1 + rµ(V ))κrd1/2 suﬃces for the condition in Lemma 6.3. For any Q ∈ B0, it is
apparent that ϕ(Q) ∈ B0. To show Q0 := T −1(Q) ∈ B0, observe that

L2 = ZE22Z T ∈ B0 by deﬁnition, which leads to Q0L1 ∈ B0. If γ > κr(cid:112)µ(V ), then L1 is

invertible by Lemma 6.2, and thus Q0 ∈ B0. To ensure 4αη/β2 < 1, we only need to require

Q0L1 − L2Q0 = Q ∈ B0,

(cid:2)γ − 3rµ(V )(τ + rκ)(cid:3)2 < 1.

4(1 + rµ(V ))2κ2r

The above inequality holds when γ > 4rµ(V )(τ + 2rκ). Under this condition, we have, by
Lemma 6.3,

(cid:107)Q(cid:107)max ≤ 2

(cid:0)γ − 3rµ(V )(τ + rκ)(cid:1)√

(1 + rµ(V ))κ

,

d

which completes the proof.

In the next lemma, we shall verify (5.6) and (5.7). Recall that (cid:107)Q

T

Q(cid:107)2 ≤ rω2
0.

Lemma 6.5. If rω2

0 < 1/2, then

(cid:107)(Ir + Q
(cid:107)(Ir + Q

T

T

Q)−1/2 − Ir(cid:107)max ≤ rω2
0,
Q)−1/2(cid:107)max ≤ 3
2

.

Proof. The second inequality is immediate from the ﬁrst one and the condition. To prove
T
the ﬁrst inequality, suppose the eigendecomposition of Q
,
where U is orthogonal matrix and Σ diagonal matrix. Let Σ = diag{λ1, . . . , λr}, where
λ1 ≥ . . . λr ≥ 0 since Q

Q (cid:23) 0. The PSD version of (Ir + Q

Q)−1/2 is given by

T
Q is given by Q

Q = U Σ U

T

T

T

T
(Ir + Q

Q)−1/2 = U diag{(1 + λ1)−1/2, . . . , (1 + λr)−1/2} U

T

.

33

Let U = (u1, u2, . . . , ur), where ui ∈ Rd (i = 1, . . . , r) is a unit vector. Since

r(cid:88)

(cid:0)(1 + λi)−1/2 − 1(cid:1)uiuT

i ,

T
(Ir + Q

Q)−1/2 − Ir =

we only need to bound (1 + λi)−1/2 − 1. Notice that (cid:107)Q
Furthermore, it is easy to check that

T

Q(cid:107) ≤ rω2

0 ≤ 1/2 implies λ1 ≤ 1/2.

i=1

1 + |x| ≥ (1 + x)−1/2 ≥ 1 − |x|,

holds for |x| < 1/2. From this fact, we know |(1 + λi)−1/2 − 1| ≤ (cid:107)Q
Cauchy-Schwarz inequality, we ﬁnally arrive at

T

Q(cid:107) ≤ rω2

0. Using

(cid:107)(Ir + Q

T

Q)−1/2 − Ir(cid:107)max ≤ rω2
0.

6.2 Lemmas for Theorem 1.1

Lemma 6.6. Parallel to Lemma 6.1, we have

(cid:107)H d(cid:107)w ≤ (1 + rµ)κ0 ,

√
d1 (cid:107)EV (cid:107)max ∨ √

d2(cid:107)ET U(cid:107)max.

where κ0 =
Proof. Recall H d = EdV d − V d(V d)T EdV d. Note V d(V d)T = diag(U U T , V V T ) and
(cid:107)U U T(cid:107)max ≤ rµ(U )/d1, (cid:107)V V T(cid:107)max ≤ rµ(V )/d2. Therefore,

(cid:107)H d(cid:107)w ≤ (cid:107)EdV d(cid:107)w + (cid:107)V d(V d)T EdV d(cid:107)w

≤ (1 + d1(cid:107)U U T(cid:107)max ∨ d2(cid:107)V V T(cid:107)max)(cid:107)EdV d(cid:107)w ≤ (1 + rµ)κ0 .

Lemma 6.7. Parallel to Lemma 6.2, if γ0 > κ0r
Furthermore, we have the following bound

√

µ, then L

d
1 is a non-degenerate matrix.

(cid:107)Qd
d
0L

1 − L

d
2Qd

0(cid:107)w ≥ γ0 − 3rµ(τ0 + rκ0) ,

inf
0(cid:107)w=1

(cid:107)Qd

(6.5)

where Qd

0 is a d1 + d2 by 2r matrix.

34

Proof. Following similar derivations with Lemma 6.2, we have bound for Qd

0Ed

11 and L

d
2Qd
0.

(cid:107)Qd

where τ0 =(cid:112)d1/d2(cid:107)E(cid:107)∞ ∨(cid:112)d2/d1(cid:107)E(cid:107)1. Moreover, (cid:107)Qd

d
2Qd

0Ed

11(cid:107)w ≤ κ0r

√
µ and (cid:107)L

0(cid:107)w ≤ (1 + rµ)(τ0 + rκ0) ,

0Ld

1(cid:107)w ≥ λr(cid:107)Qd
0(cid:107)w ≥ γ0 − 3rµ(τ0 + rκ0) ,

0(cid:107)w ≥ γ0. So

(cid:107)Qd
d
0L

1 − L

inf
0(cid:107)w=1

(cid:107)Qd

d
2Qd

√

which is the desired inequality in the lemma.
κ0r

µ > 0.

In addition, L1 is non-degenerate if γ0 −

Lemma 6.8. Parallel to Lemma 6.4, there is a solution Q
that if γ0 > 4rµ(τ0 + 3rκ0), then

d ∈ B0 to the system (6.4) such

(cid:107)Q

d(cid:107)w ≤

2(1 + rµ)κ0

γ0 − 3rµ(τ0 + rκ0)

.

Proof. We again apply Lemma 6.3. Let B be Rd×r equipped with matrix max norm (cid:107) · (cid:107)w.
First notice from Lemma 6.7, T is a linear operator with bounded inverse, i.e. β = (cid:107)T −1(cid:107)−1
is bounded from below by

w

β ≥ γ0 − 3rµ(τ0 + rκ0) .

d
Let ϕ be given by ϕ(Q

d
) = Q

(H d)T Q

d

. Observe that

(cid:107)ϕ(Q

d

)(cid:107)w ≤ 4r(cid:107)H d(cid:107)w(cid:107)Q

d(cid:107)2
w ≤ 4r(1 + rµ)κ0 (cid:107)Q

d(cid:107)2

w

by Lemma 6.6, and moreover

(cid:107)ϕ(Q

d

1) − ϕ(Q

d

d

2)(cid:107)w ≤ 4r(cid:107)Q

1(cid:107)w(cid:107)H d(cid:107)w(cid:107)Q

1 − Q
≤ 4r(1 + rµ)κ0 max{(cid:107)Q

2(cid:107)w + 4r(cid:107)Q
2(cid:107)w}(cid:107)Q
1(cid:107)w,(cid:107)Q

1 − Q
1 − Q

2(cid:107)w(cid:107)H d(cid:107)w(cid:107)Q
2(cid:107)w.

d

d

d

d

d

d

d

d

d

2(cid:107)w

Thus, η = 4r(1 + rµ)κ0 suﬃces for the condition in Lemma 6.3. To ensure 4αη/β2 < 1, we
only need to require

(cid:2)γ0 − 3rµ(τ0 + rκ0)(cid:3)2 < 1.

16(1 + rµ)2κ2
0r

The above inequality holds when γ0 > 4rµ(τ0 + 3rκ0). Under this condition, we have, by
Lemma 6.3,

(cid:107)Q

d(cid:107)w ≤ 2

(1 + rµ)κ0

γ0 − 3rµ(τ0 + rκ0)

,

35

which completes the proof.

References

Antoniadis, A. and Fan, J. (2001). Regularization of wavelet approximations. Journal

of the American Statistical Association 96.

Bai, J. (2003). Inferential theory for factor models of large dimensions. Econometrica 71

135–171.

Bartholomew, D. J., Knott, M. and Moustaki, I. (2011). Latent variable models and

factor analysis: A uniﬁed approach, vol. 904. John Wiley & Sons.

Berthet, Q. and Rigollet, P. (2013). Optimal detection of sparse principal components

in high dimension. The Annals of Statistics 41 1780–1815.

Bickel, P. J. and Levina, E. (2008). Covariance regularization by thresholding. The

Annals of Statistics 2577–2604.

Biswas, P. and Ye, Y. (2004). Semideﬁnite programming for ad hoc wireless sensor network
localization. In Proceedings of the 3rd international symposium on Information processing
in sensor networks. ACM.

Borg, I. and Groenen, P. J. (2005). Modern multidimensional scaling: Theory and

applications. Springer Science & Business Media.

Cand`es, E. J., Li, X., Ma, Y. and Wright, J. (2011). Robust principal component

analysis? Journal of the ACM (JACM) 58 11.

Catoni, O. (2012). Challenging the empirical mean and empirical variance: a deviation
study. In Annales de l’Institut Henri Poincar´e, Probabilit´es et Statistiques, vol. 48. Institut
Henri Poincar´e.

Chamberlain, G. and Rothschild, M. (1982). Arbitrage, factor structure, and mean-

variance analysis on large asset markets.

Chandrasekaran, V., Sanghavi, S., Parrilo, P. A. and Willsky, A. S. (2011).
Rank-sparsity incoherence for matrix decomposition. SIAM Journal on Optimization 21
572–596.

36

Chen, Y. (2015). Incoherence-optimal matrix completion. IEEE Transactions on Informa-

tion Theory 61 2909–2923.

Choi, K. and Marden, J. (1998). A multivariate version of kendall’s τ . Journal of

Nonparametric Statistics 9 261–293.

Davis, C. and Kahan, W. M. (1970). The rotation of eigenvectors by a perturbation. iii.

SIAM Journal on Numerical Analysis 7 1–46.

De La Torre, F. and Black, M. J. (2003). A framework for robust subspace learning.

International Journal of Computer Vision 54 117–142.

Doherty, L., Pister, K. S. and El Ghaoui, L. (2001). Convex position estimation in
wireless sensor networks. In INFOCOM 2001. Twentieth Annual Joint Conference of the
IEEE Computer and Communications Societies. Proceedings. IEEE, vol. 3. IEEE.

Fama, E. F. and French, K. R. (1993). Common risk factors in the returns on stocks

and bonds. Journal of Financial Economics 33 3–56.

Fan, J., Fan, Y. and Lv, J. (2008). High dimensional covariance matrix estimation using

a factor model. Journal of Econometrics 147 186–197.

Fan, J., Li, Q. and Wang, Y. (2016a). Robust estimation of high-dimensional mean

regression. Journal of Royal Statistical Society, Series B .

Fan, J., Liao, Y. and Mincheva, M. (2013). Large covariance estimation by thresholding
principal orthogonal complements. Journal of the Royal Statistical Society: Series B 75
1–44.

Fan, J., Liu, H. and Wang, W. (2015). Large covariance estimation through elliptical

factor models. arXiv preprint arXiv:1507.08377 .

Fan, J. and Wang, W. (2015). Asymptotics of empirical eigen-structure for ultra-high

dimensional spiked covariance model. arXiv preprint arXiv:1502.04733 .

Fan, J., Wang, W. and Zhong, Y. (2016b). Robust covariance estimation for approximate

factor models. arXiv preprint arXiv:1602.00719 .

Fang, K.-T., Kotz, S. and Ng, K. W. (1990). Symmetric multivariate and related

distributions. Chapman and Hall.

Gupta, A. K., Varga, T. and Bodnar, T. (2013). Elliptically contoured models in

statistics and portfolio theory. Springer.

37

Han, F. and Liu, H. (2014). Scale-invariant sparse PCA on high-dimensional meta-elliptical

data. Journal of the American Statistical Association 109 275–287.

Hsu, D. and Sabato, S. (2014). Heavy-tailed regression with a generalized median-of-
means. In Proceedings of the 31st International Conference on Machine Learning (ICML-
14).

Hu, W., Tan, T., Wang, L. and Maybank, S. (2004). A survey on visual surveillance of
object motion and behaviors. Systems, Man, and Cybernetics, Part C: IEEE Transactions
on Applications and Reviews 34 334–352.

Huber, P. J. (1964). Robust estimation of a location parameter. The Annals of Mathe-

matical Statistics 35 73–101.

Johnstone, I. M. and Lu, A. Y. (2009). On consistency and sparsity for principal
components analysis in high dimensions. Journal of the American Statistical Association
104 682–693.

Jolliffe, I. (2002). Principal component analysis. Wiley Online Library.

Ma, Z. (2013). Sparse principal component analysis and iterative thresholding. The Annals

of Statistics 41 772–801.

Ng, A. Y., Jordan, M. I., Weiss, Y. et al. (2002). On spectral clustering: Analysis

and an algorithm. Advances in neural information processing systems 2 849–856.

Oliver, N. M., Rosario, B. and Pentland, A. P. (2000). A bayesian computer vision
IEEE Transactions on Pattern Analysis and

system for modeling human interactions.
Machine Intelligence 22 831–843.

Rennie, J. D. and Srebro, N. (2005). Fast maximum margin matrix factorization for
collaborative prediction. In Proceedings of the 22nd international conference on Machine
learning. ACM.

Rohe, K., Chatterjee, S. and Yu, B. (2011).

Spectral clustering and the high-

dimensional stochastic blockmodel. The Annals of Statistics 1878–1915.

Rothman, A. J., Levina, E. and Zhu, J. (2009). Generalized thresholding of large

covariance matrices. Journal of the American Statistical Association 104 177–186.

Stewart, G. W. (1990). Matrix perturbation theory .

38

Stock, J. and Watson, M. (2002). Forecasting using principal components from a large

number of predictors. Journal of the American Statistical Association 97 1167–1179.

Vershynin, R. (2010). Introduction to the non-asymptotic analysis of random matrices.

arXiv preprint arXiv:1011.3027 .

Vu, V. Q. and Lei, J. (2012). Minimax rates of estimation for sparse pca in high dimensions.

arXiv preprint arXiv:1202.0786 .

Wedin, P.-˚A. (1972). Perturbation bounds in connection with singular value decomposi-

tion. BIT Numerical Mathematics 12 99–111.

Yu, Y., Wang, T. and Samworth, R. J. (2014). A useful variant of the davis–kahan

theorem for statisticians. arXiv preprint arXiv:1405.0680 .

Yuan, M., Ekici, A., Lu, Z. and Monteiro, R. (2007). Dimension reduction and
coeﬃcient estimation in multivariate linear regression. Journal of the Royal Statistical
Society: Series B (Statistical Methodology) 69 329–346.

39

