Model Selection for Graphical Log-linear

Models: A Forward Model Selection Algorithm

based on Mutual Conditional Independence

Niharika Gauraha

Systems Science and Informatics Unit

Indian Statistical Institute

8th Mile, Mysore Road Bangalore, India

niharika.gauraha@gmail.com

Abstract. Model selection and learning the structure of graphical mod-
els from the data sample constitutes an important ﬁeld of probabilistic
graphical model research, as in most of the situations the structure is
unknown and has to be learnt from the given dataset. In this paper,
we present a new forward model selection algorithm for graphical log-
linear models. We use mutual conditional independence check to reduce
the search space which also takes care of the evaluation of the joint ef-
fects and chances of missing important interactions are eliminated. We
illustrate our algorithm with a real dataset example.

Keywords: Mutual Conditional Independence, Graphical Log-linear Mod-
els, Model Selection, Markov Networks

1

Introduction

Graphical models are a way of representing relationships among random vari-
ables using a graph and are adopted by a variety of research ﬁelds such as
data mining, natural language processing and bioinformatics etc. The graphical
models may be based on directed acyclic graphs, undirected graph and mixed
graphs(see [1] for details). We mainly focus on graphical log-linear models that
are undirected graphical models and are used for visual representation of log-
linear models, in particular for higher dimensional tables. For further information
on log-linear models we refer from [2] to [6].

Graphical Log Linear Models(GLLM) are a subclass of hierarchical log-linear
models. Selecting an optimal model from the class of graphical models is known
to be an intractable problem. Most of all existing model selection algorithms are
based on forward selection, backward elimination or combination of the both.
For detailed discussion on graphical log-linear model selection we refer [2], [3],
[7], [8] and [13].

Current model selection algorithms mostly use conditional independence
property as model selection criteria. We propose a forward model selection algo-
rithm which is based on the concept of equivalence between mutual conditional

6
1
0
2

 
r
a

 

M
1
1

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
9
1
7
3
0

.

3
0
6
1
:
v
i
X
r
a

2

Niharika Gauraha

independence in probability and independent set in graph theory. We use mutual
conditional independence check to reduce the search space for the local search
algorithm. We also use the concept that, there is a one-to-one relationship be-
tween graphs and All Maximal Independent set (AMIS). Our main focus is to
ﬁnd the AMIS for the underlying graphical model based on the data sample.
The main drawback of the traditional forward selection algorithm is that some
joint eﬀects of the group of factors may not be evaluated thus many important
interactions can be missed(see [2] for details). As we perform mutual conditional
independence check at every step for a group of factors that takes care of the
evaluation of the joint eﬀects and chances of missing important interactions are
eliminated.

At a high level, our algorithm works as follows. Suppose we have a p-factor
contingency table and a graph G = (V, E) consists of p vertices V = {X1, ..., Xp}
that corresponds to the factors of the contingency table. Let the vertex set
{Xi, Xj, Xk} be a maximal independent set of G. The set {Xi, Xj, Xk} forms an
independent set if and only if the factors {Xi, Xj, Xk} are mutually independent
conditioned on the remaining factors. The algorithm starts with the null model(a
model with complete independence) and then we use rules based on mutual
conditional independence for adding edges to arrive at a smallest graphical model
that ﬁts the data. We note that here model selection is inferring the AMIS or
equivalently the edge set E from the data sample. We use the minimization of
the deviance from the saturated model as a model selection criteria but it can
be extended to many other metrics as well.

The remainder of the paper is arranged as follows. In section 2 we brieﬂy
review the required concepts such as graph theory, conditional independence,
GLLMs, model testing and model comparison criterion. Section 3 introduces
concept of mutual conditional independence. In section 4 we present new for-
ward model selection algorithm with illustrated example. In section 5 we give
computational details that we used for model selection. In Section 6 we conclude
and discuss future scope.

2 Background and Notation

In this section, we brieﬂy review the required concepts.

2.1 Graph Theory

Here we list and deﬁne fundamental principles of graph theory that we will be
using in later sections. For details on graph theory we refer to [9].

Deﬁnition 1 (Undirected Graphs). A graph G, is a pair G = (V, E), where
V is a set of vertices and E is a set of edges. A graph is said to be an undirected
graph if its vertices are connected by undirected edges.

Deﬁnition 2 (Maximal Independent set). An independent set of a graph
G is a subset S of vertices such that every pair of vertices are non-adjacent. An

Model Selection for Graphical Log-linear Models

3

independent set is said to be maximal if no vertex can be added to S without
violating independent set property.

Deﬁnition 3 (Maximal Cliques). A clique of a graph G is a subset S of
mutually adjacent vertices. A clique is said to be maximal if no vertex can be
added to S without violating clique property.

Deﬁnition 4 (Chordal Graphs). A graph is said to be chordal graph if every
cycle of length at least four has a chord.

2.2 Conditional Independence and Markov Networks

In this section we deﬁne conditional independence in probability(see [10] for
more details), Markov network graphs and Markov networks.

Deﬁnition 5 (Conditional Independence). If X, Y, Z are random variables
with joint distribution P. Random variables X and Y are said to be conditionally
independent given the random variable Z if and only if following holds.

X ⊥⊥ Y | Z ⇐⇒ P (X, Y | Z) = P (X | Z)P (Y | Z)

⇐⇒ P (X | Y, Z) = P (X | Z)

Deﬁnition 6 (Markov Network Graph). A Markov network graph is an
undirected graph G = ( V, E ) where V = {X1, X2, .., Xn} corresponds to random
variables of a multivariate distribution.

Deﬁnition 7 (Markov Networks). A Markov network is a tuple M = (G, ψ)
where G is a Markov network graph, ψ = {ψ1, ψ2, ..., ψm} is a set of non negative
functions for each maximal clique Ci ∈ G ∀i = 1 . . . m and the joint pdf can be
decomposed into factors as

P (x) =

1
Z Y

a∈Cm

ψa(x)

where Z is a normalizing constant.

2.3 Graphical and Decomposable Log-linear Models

In this section, we deﬁne a class of LLMs that can be represented by a graph.

Deﬁnition 8 (Graphical Log-linear Models(GLLMs)). A model is said
to be graphical if it contains all lower order terms which can be derived from
variables contained in a higher-order term, then the model also contains the
higher order interaction.

For example, Let us consider a three-factor table. If a model includes all two
factor interactions([12][23][13]) then it also contains the three factor interaction
[123]. We usually represent a graphical model as a set of maximal cliques, which
is [123] in this case.

Deﬁnition 9 (Decomposable log-linear Models). A model is decomposable
if it is both graphical and chordal.

4

Niharika Gauraha

2.4 Model Checking and Model Comparison

The goal of model selection is to choose a smallest graphical model from a class of
graphical models under consideration that which best ﬁts the data and has least
number of edges(number of interaction terms). We use following test statistics
for model testing and model comparison(see [3] and [4] for further details).

– Pearson’s χ2 Statistic: It is deﬁned as

χ2 = X

i

(Oi − Ei)2

Ei

where O denotes observed cell count, E as expected cell count.

– The Deviance statistics: In the generalized likelihood test, the test statistic is
called deviance when we are comparing one model against saturated models
otherwise for nested model its called the deviance diﬀerence. It is deﬁned as
follows.

G2 = −2 X

Oi log

i

Ei
Oi

Under null hypotheses the deviance is also distributes as χ2 with appropriate
degrees of freedom.

3 Mutual Conditional Independence in Markov networks

Theorem 1. Let P be a positive distribution over V and let G be a Markov
network graph over V, then elements of an independent set I of G are mutually
conditionally independent given the rest {V \ I}.

Here is an informal proof. Let us consider a simplest example a star graph
as shown in ﬁgure(1) Since the set I = {B, C, D, E} forms an independent
set they belong to separate cliques [AB],
[AC] [AD], [AE] respectively. From
deﬁnition(7) the joint probability P factorizes as follows.

P = ψ1(A, B) ψ2(A, C) ψ3(A, D)ψ4(A, E)

The conditional probability P ({B, C, D, E} | A) can be expressed as

P (I | A = a) = ψ1(a, B) ψ2(a, C) ψ3(a, D) ψ4(a, E)

=⇒ φ1(B) φ2(C) φ3(D) φ4(E)

Hence the factors {B, C, D, E} are mutually conditionally independent given the
factor A as given in ﬁgure (2). See [11] for details on conditional independence
for graphical models.

Model Selection for Graphical Log-linear Models

5

C

B

E

A

D

C

B

E

D

Fig. 1. A Grphical Model

Fig. 2. B,C,D,E are Mutually Separated by A

4 Model Selection using Mutual Conditional

Independence

Finding an optimal graphical model from data sample is important mainly for
prediction of future observables and describing the association among factors.
As mentioned previously our goal is to ﬁnd a simplest graphical model that ﬁts
the data. Now we present a new forward model selection procedure exploiting
Mutual Conditional Independence Property(MCIP).

In this approach, we start with the null model (a complete independence
model). We maintain two lists tempAMIS and AMIS. The tempAMIS contains
a list of subset of factors to be tested for MCIP and the AMIS list contains
MISs(for which the data supports MCIP). At each step we pick a set from
tempAMIS, if its elements are mutually conditionally independent given the
remaining factors then we remove it from tempAMIS and move it to the AMIS
otherwise we ﬁnd the most signiﬁcant edge between them and add the required
two factor and higher order terms into the present model. All sets in tempAMIS
containing the end points of newly added edge gets split into two subsets. We
repeat the process until there is no maximal independent set left to be tested
for MCIP. The algorithm returns AMIS that determines the graph structure
uniquely.

Let us consider a ﬁve-factor contingency table. Initial model assumption is the
null model(all factor form an MIS), tempAM IS = {{1, 2, 3, 4, 5}} and AM IS =
{∅}. Let us suppose that the complete independence model does not ﬁt and also
assume that the edge (1, 2) is the most signiﬁcant edge. We add this edge into the
complete independence model. Since we had assumed that the set {1, 2, 3, 4, 5}
is an MIS after adding the edge (1, 2) it is no longer an MIS, in fact now the
assumption for the MISs are tempAM IS = { {1, 3, 4, 5}, {2, 3, 4, 5} }. Now
let us suppose that the data supports the MCI condition for the set {1, 3, 4, 5},
we remove it from the tempAMIS and add it to the AMIS. At each step the
procedure continues in this way until tempAMIS becomes empty.

Since it is more notational than conceptual, we begin the development with

an example (1).

6

Niharika Gauraha

Algorithm 1: Forward Selection Algorithm

Input: A p-factor Contingency table, factors are labelled as {X1, X2, ..., Xp}
Output: AMIS:= All Maximal Independent Set
Initialize:
AM IS = φ ;
tempAM IS = {{X1, X2, ..., Xp}}
for

each s ∈ tempM IS do

if MCI relation holds amongst factors in s then

tempM IS = tempM IS − s ;
AM IS = AM IS ∪ s ;

else

ﬁnd the most signiﬁcant edge (v1, v2) ∈ s
for each t ∈ tempM IS do

if

(v1, v2) ∈ t then
t1 = t − v1 ;
t2 = t − v2 ;
tempM IS = {tempM IS − t} ∪ t1 ∪ t2 ;

end

end

end

end
return AM IS

Example 1 (Forward Model Selection for Rienis Dataset). We now illustrate the
algorithm using real data Rienis dataset, for details on Reinis dataset see [12].
The Reinis data is shown in the table (1).

Table 1: Reinis data

Family Protein Systol Phys Mental no yes

Smoke

no

neg

pos

< 3 < 140 no
yes
≥ 140 no
yes
≥ 3 < 140 no
yes
≥ 140 no
yes
< 3 < 140 no
yes
≥ 140 no
yes
≥ 3 < 140 no
yes
≥ 140 no
yes

44 40
129 145
35 12
109 67
23 32
50 80
24 25
51 63
5
7
17
9
4
3
14 17
3
7
9
16
0
4
5
14

yes
no yes

112 67
12 23
80 33
7
9
70 66
7
13
73 57
16
7
21
9
4
1
8
11
5
2
14 14
2
3
13 11
4
4

Model Selection for Graphical Log-linear Models

7

We begin by ﬁtting the complete independence model, as given in ﬁgure(3).

The vertices A,B,C,D,E,F correspond to the factors smoke,mental,phys,systol,protein,family
respectively. The G2 statistic for the model is 843.957(df:57, p-value:0), hence
we conclude that the data fails to support the complete independence model.

A

C

E

B

D

F

Fig. 3. The model of complete independence

The data structures are initialized as follows.

currM odel = [A][B][C][D][E][F ]
tempAM IS = { {A, B, C, D, E, F } }

AM IS = {∅}

As mentioned before, at each step we add the most signiﬁcant edge as long
as the signiﬁcance level is below a cutoﬀ value(we use cutoﬀ of α = 0.05). As
a ﬁrst step we compare all the models with a single edge added to the model
of complete independence. Table(2) gives the model ﬁt and table(3) summarizes
the test results.

Table 2: Model Fitting

Ad. Edge

Model

d.f.

G2

[A][B][C][D][E][F] 57 843.9570
56 834.2932
[AB][C][D][E][F]
56 816.4759
[AC][B][D][E][F]
[AD][B][C][E][F]
56 832.9246
56 826.5566
[AE][B][C][D][F]
56 842.8883
[AF][B][C][D][E]
56 157.9852
[A][BC][D][E][F]
56 843.4569
[A][BD][C][E][F]
[A][BE][C][D][F]
56 826.0277
56 839.2254
[A][BF][C][D][E]
56 843.8615
[A][B][CD][E][F]
56 827.2845
[A][B][CE][D][F]
[A][B][CF][D][E]
56 843.7867
56 831.1477
[A][B][C][DE][F]
56 842.8332
[A][B][C][DF][E]
[A][B][C][D][EF]
56 840.9532

AB
AC
AD
AE
AF
BC
BD
BE
BF
CD
CE
CF
DE
DF
EF

Table 3: Model Comparision

Ad. Edge d.f.

G2

p-value

AB
AC
AD
AE
AF
BC
BD
BE
BF
CD
CE
CF
DE
DF
EF

9.6637 0.0018
1
27.4810 0.0000
1
11.0323 0.0008
1
17.4003 0.0000
1
1
1.0686 0.3012
1 685.9717 0.0000
0.5000 0.4794
1
1
17.9292 0.0000
4.7315 0.0296
1
0.0954 0.7573
1
16.6724 0.0000
1
1
0.1702 0.6799
12.8092 0.0003
1
1.1237 0.2891
1
1
3.0037 0.0830

8

Niharika Gauraha

The model with edge (B, C) has the largest diﬀerence in G2 (or smallest p-value),
we choose this models as the current model. Also the set containing the factors
(B, C) gets separated as follows.

currM odel = [A][BC][D][E][F ]
tempAM IS = { {A, B, D, E, F }, {A, C, D, E, F } }

AM IS = {∅}

As a next step, we consider the set{A, B, D, E, F }. We perform the MCI test
for the set and get G2 statistic as 113.566(df:52, p-value:0), which suggests that
the data does not support the MCIP for the set. We compare the current model
with all the models with an additional edge from the set {A, B, D, E, F }. It is
described in the table(4) and table(5).

Table 4: Model Fitting

Ad. Edge

Model

d.f.

G2

[A][BC][D][E][F]
56 157.9852
[AB][BC][D][E][F] 55 148.3215
55 146.9529
[AD][BC][E][F]
55 140.5849
[AE][BC][D][F]
[AF][BC][D][E]
55 156.8614
[A][BC][BD][E][F] 55 157.4851
[A][BC][BE][D][F] 55 140.0559
[A][BC][BF][D][E] 55 153.2537
55 145.1760
[A][BC][DE][F]
55 156.9166
[A][BC][DF][E]
[A][BC][D][EF]
55 154.9815

AB
AD
AE
AF
BD
BE
BF
DE
DF
EF

Table 5: Model Comparision

Ad. Edge d.f. G2

p-value

AB
AD
AE
AF
BD
BE
BF
DE
DF
EF

9.6637 0.0018
1
1 11.0324 0.0008
1 17.4003 0.0000
1.0686 0.3012
1
1
0.5000 0.4794
1 17.9292 0.0000
1
4.7315 0.02961
1 12.8092 0.0003
1.1237 0.2891
1
1
3.0037 0.0830

The term (B, E) is added to the current model. The data structures are updated
as

currM odel = [A][BC][BE][D][F ]
tempAM IS = { {A, B, D, F }, {A, D, E, F }, {A, C, D, E, F } }

AM IS = {∅}

Next we consider the set {A, B, D, F }. The MCI test for the set gives the
G2 statistic as 67.5(df:44, p-value:0.013), we conclude that the data fails to
support the MCI relation for the set. We consider an additional edge from the
set {A, B, D, F }, table(6) gives the model ﬁt and table(7) summarizes the test
results.

Model Selection for Graphical Log-linear Models

9

Table 6: Model Fitting

Ad. Edge

Model

d.f.

G2

[A][BC][BE][D][F]
55 140.0559
[AB][BC][BE][D][F] 54 130.3922
54 129.0236
[AD][BC][BE][F]
[AF][BC][BE][D]
54 138.9873
[A][BC][BD][BE][F] 54 139.5558
[A][BC][BE][BF][D] 54 135.3244
54 138.9321
[A][BC][BE][DF]

AB
AD
AF
BD
BF
DF

Table 7: Model Comparision

Ad. Edge d.f. G2

p-value

AB
AD
AF
BD
BF
DF

1
9.6637 0.0018
1 11.0323 0.0008
1.0686 0.3012
1
0.5000 0.4795
1
1
4.7315 0.0296
1.1237 0.2891
1

The term (A, D) is added to the current model. The data structures get modiﬁed
as

currM odel = [AD][BC][BE][F ]
tempAM IS = { {A, B, F }, {B, D, F }, {A, E, F }, {D, E, F }, {A, C, E, F } , {C, D, E, F } }

AM IS = {∅}

The G2 statistics 38.915(df:32, p-value:0.186) and 39.271(df:32, p-value:0.176)
of the MCI tests for the sets {A, B, F } and {B, D, F } respectively indicates that
the data supports the MCI relations for the sets. We remove them from tem-
pAMIS and add to the AMIS.

currM odel = [AD][BC][BE][F ]
tempAM IS = { {A, E, F }, {D, E, F }, {A, C, E, F } , {C, D, E, F } }

AM IS = { {A, B, F }, {B, D, F } }

Now we perform the MCI test for the set {A, E, F }. The G2 59.043(df:32,
p-value:0.002) statistic indicates that the data fails to support the MCI relation.
We look for a signiﬁcant edge in the set, the details are given in table(8) and
table(9).

Table 8: Model Fitting

Ad. Edge

Model

d.f.

G2

[AD][BC][BE][D][F]
54 129.0236
[AD][AE][BC][BE][D][F] 53 111.6233
53 127.9550
[AD][AF][BC][BE][D]
[AD][BC][BE][D][EF]
53 126.0199

AE
AF
EF

Table 9: Model Comparision

Ad. Edge d.f. G2

p-value

AE
AF
EF

1 17.4003 0.0000
1.0686 0.3012
1
1
3.0037 0.0830

The term (A, E) is added to the current model. The data structures get updated
as follows(It must be noted that since {A, F } ⊂ {A, B, F } it is subsumed in it
and its removed from tempAMIS).

currM odel = [AD][AE][BC][BE][F ]
tempAM IS = { {E, F }, {D, E, F }, {A, C, F } , {C, E, F }, {C, D, E, F } }

AM IS = { {A, B, F }, {B, D, F } }

10

Niharika Gauraha

In the next step, we perform the MCI test for the set {E, F }. We get G2 statistics
as 18.316(df: 16, p-value:0.305), hence we conclude that the data supports MCI
relation for the set. The tempAMIS and AMIS gets updated as follows.

currM odel = [AD][AE][BC][BE][D][F ]
tempAM IS = { {D, E, F }, {A, C, F } , {C, E, F }, {C, D, E, F } }

AM IS = { {E, F }, {A, B, F }, {B, D, F } }

Now we perform the MCI test for the set {D, E, F }, the G2 statistic is 49.428(df:32,
p-value:0.025), hence the data fails to support the MCI relation. We look for most
signiﬁcant edge between them. Test details are given in table(10) and table(11).

Table 10: Model Fitting

Ad. Edge

Model

d.f.

G2

[AD][AE][BC][BE][F]
53 111.6233
[AD][AE][BC][BE][DE][F] 52 93.3047
[AD][AE][BC][BE][DF]
52 110.4995
52 108.6195
[AD][AE][BC][BE][EF]

DE
DF
EF

Table 11: Model Comparision

Ad. Edge d.f. G2

p-value

DE
DF
EF

1 18.3185 0.0000
1
1.1237 0.2891
3.0037 0.0830
1

We select the model with an additional edge (D, E). The set {D, E, F } gets
divided into the subsets {D, F } and {E, F }. Since {D, F } ⊂ {B, D, F } and
{E, F } are already members of AMIS , hence the data supports MCI for them
and therefore it is removed from the tempAMIS.

Similarly we proceed further and we ﬁnd that the data fails to support the
MCI relation for the set {A, C, F }. We compute following statistics for choosing
the most signiﬁcant edge between them.

Table 12: Model Fitting

Ad. Edge

Model

d.f.

G2

[ADE][BC][BE][F]
51 93.30472
[AC][ADE][BC][BE][F] 50 63.0128
50 92.2360
[ADE][AF][BC][BE]
[ADE][BC][BE][CF]
50 93.1345

AC
AF
CF

Table 13: Model Comparision

Ad. Edge d.f. G2

p-value

AC
AF
CF

1 30.2918 0.0000
1.0686 0.3012
1
1
0.1702 0.6799

We choose the model with the edge (A, C). The data structures get modiﬁed

as

currM odel = [AC][ADE][BC][BE][F ]
tempAM IS = { {C, F } , {C, E, F }, {C, D, E, F } }

AM IS = { {E, F }, {A, B, F }, {B, D, F } }

On performing the MCI tests for the sets (C, F ) , {C, E, F } and {C, D, F } we
get G2 statistics as 22.153(df:16, p-value:0.138), 42.534(df:16, p-value:0.100) and
35.475(df:16, p-value:0.307) respectively. It indicates that the data supports the

Model Selection for Graphical Log-linear Models

11

MCI relations for them. The sets are removed from the tempAMIS and added
to the AMIS. After removing redundant sets from the AMIS, ﬁnally we get the
data structures as

currM odel = [AC][ADE][BE][BC][F ]
tempAM IS = {∅}

AM IS = { {C, E, F }, {C, D, F } , {A, B, F }, {B, D, F } }

Since tempAMIS becomes empty, we stop with the model [AC][ADE][BC][BE][F ].

The algorithm returns the { {A, D, F }, {C, E, F }, {B, E, F } } AMIS. A graph
structure can be determined uniquely from the AMIS as given in ﬁgure(4).

A

C

B

D

E

F

Fig. 4. A Graphical Model for Reinis Data set

5 Computational details

All the experimental results in this paper were carried out using R 3.1.3. We
implemented the new forward selection algorithm in R(Our implementation is
available on request). We also used the existing packages gRim and MASS. All
packages used are available at http://CRAN.R-project.org/.

6 Conclusion and Future Scope

In this paper, we have discussed mutual conditional independence property
amongst the factors of a maximal independent set. We have presented an ef-
ﬁcient forward model selection algorithm for the graphical log-linear models
exploiting MCIP. Unlike traditional forward selection algorithm, our algorithm
takes care of the evaluation of the joint eﬀects since we perform mutual con-
ditional independence check at every step for a group of factors therefore the
important interactions can not be missed.

We conclude with a couple of open problems as follows:
(i). The search space can be further reduced by considering only decomposable
models(see [14] for more details on stepwise selection in decomposable models).
(ii). The MCIP can be also used in backward elimination or in other stepwise
selection procedures.

12

Niharika Gauraha

References

1. S. L Lauritzen. Graphical Models. Oxford University Press Inc., New York, 2nd

edition edition, 1996.

2. Y.M.M. Bishop, S.E. Fienberg, and P.W. Holland. Discrete Multivariate Analysis:

Theory and Practice. The MIT Press, Cambridge, MA, 1989 edition.

3. R. Christensen. Log-Linear Models and Logistic Regression. Springer, 2nd edition

edition, 1997.

4. J. Whittaker. Graphical Models in Applied Multivariate Statistics. Chichester: Wi-

ley, 2nd edition edition, 1990.

5. A. Agresti. Categorical Data Analysis. Wiley-Interscience, New York, 2nd edition

edition, 2002.

6. A. H. Andersen. Multidimensional contingency tables. Scand. J. Statist., 3:115–127,

1974.

7. Corinne Dahinden, Markus Kalisch, and Peter Buhlmann. Decomposition and model

selection for large contingency tables. Biometrical Journal, 52:233–252, 2010.

8. L. A. Goodman. The analysis of multidimensional contingency tables: Stepwise
procedures and direct estimation methods for building models for multiple classiﬁ-
cations. Technometrics, 13:31–66, 1971a.

9. Douglas B. West. Introduction to Graph Theory. The MIT Press, Cambridge, MA,

2nd edition edition, 2000.

10. A. P. Dawid. Conditional independence in statistical theory. Journal of the Royal

Statistical Society, 41(1):1–31, 1979.

11. Dan Geiger and Judea Pearl. Logical and algorithmic properties of conditional
independence and graphical models. The Annals of Statistics, 24(4):2001–2021,
1993.

12. Z. Reinis, J. Pokorny, V. Basika, J. Tiserova, K. Gorican, D. Horakova, E. Stuch-
likova, T.and Havranek, and F Hrabovsky. Prognostic signiﬁcance of the risk proﬁle
in the prevention of coronary heart disease. Bratis. lek. Listy, 76:137–150, 1981.

13. Nanny Wermuth. Model search among multiplicative models. Biometrics, 32:

253–263, 1976.

14. Amol Deshpande, Minos Garofalakis and Michael I. Jordan Eﬃcient stepwise
selection in decomposable models. Morgan Kaufmann Publishers, 32:128–135, 2001.

