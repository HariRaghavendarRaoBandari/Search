The 1-Laplacian Cheeger Cut: Theory and Algorithms

K.C. Chang†,

Sihong Shao†,

Dong Zhang†

Abstract

This paper presents a detailed review of both theory and algorithms for the Cheeger
cut based on the graph 1-Laplacian. In virtue of the cell structure of the feasible set, we
propose a cell descend (CD) framework for achieving the Cheeger cut. While plugging
the relaxation to guarantee the decrease of the objective value in the feasible set, from
which both the inverse power (IP) method and the steepest descent (SD) method can
also be recovered, we are able to get two speciﬁed CD methods. A compare study of all
these methods are conducted on several typical graphs.

6
1
0
2

 
r
a

M
5

 

.

 
 
]
P
S
h
t
a
m

[
 
 

1
v
7
8
6
1
0

.

3
0
6
1
:
v
i
X
r
a

1

Introduction

Graph cut, partitioning the vertices of a graph into two or more disjoint subsets, is a
fundamental problem in graph theory [1]. It is a very powerful tool in data clustering with
wide applications ranging from statistics, computer learning, image processing, biology to
social sciences [2]. There exist several kinds of balanced graph cut [3–5]. The Cheeger cut [6],
which has recently been shown to provide excellent classiﬁcation results [7–9], is one of them
and its deﬁnition is as follows. Let G = (V, E) denote a undirected and unweighted graph
with vertex set V = {1, 2,··· , n} and edge set E. Each edge e ∈ E is a pair of vertices {i, j}.
For any vertex i, the degree of i, denoted by di, is deﬁned to be the number of edges passing
through i. Let S and T be two nonempty subsets of V and use

E(S, T ) = {{i, j} ∈ E : i ∈ S, j ∈ T}

to denote the set of edges between S and T . The edge boundary of S is ∂S = E(S, Sc) (Sc

is the complement of S in V ) and the volume of S is deﬁned to be vol(S) := (cid:80)i∈S di. The

number

is called the Cheeger constant, and a partition (S, Sc) of V is called a Cheeger cut of G if

h(G) =

min

S⊂V,S(cid:54)∈{∅,V }

|∂S|

min{vol(S), vol(Sc)}

|∂S|

min{vol(S), vol(Sc)}

= h(G),

where |∂S| is the cardinality of the set ∂S.

†LMAM and School of Mathematical Sciences, Peking University, Beijing 100871, China.

1

However, solving analytically the Cheeger cut problem is combinatorially NP-hard [7]. Ap-
proximate solutions are required. The most well-known approach to approximate the Cheeger
cut solutions is the spectral clustering method, which relaxes the original discrete combina-
tion optimization problem into a continuous function optimization problem through the graph
Laplacian [10]. The standard graph Laplacian (i.e. the 2-Laplacian) is deﬁned as L := D − A,
where D = diag(d1,··· , dn) is a diagonal matrix and A the adjacency matrix of G. According
to the linear spectral graph thoery, the eigenvalues of L satisfy 0 = λ1 ≤ λ2 ≤ ··· ≤ λn ≤ 2
and the second eigenvalue λ2 can be used to bound the Cheeger constant as follows

λ2

2 ≤ h(G) ≤(cid:112)2λ2,

which is nothing but the Cheeger inequality [1]. Furthermore, the corresponding second eigen-
vector is also used to approximate the Cheeger cut, i.e. the 2-spectral clustering or the (cid:96)2
relaxation. It should be noted that this second eigenvector is not the Cheeger cut, but only
an approximation [10].

In order to achieve a better cut than the (cid:96)2 relaxation, a spectral clustering based on the

graph p-Laplacian deﬁned by

(∆px)i =(cid:88)j∼i |xi − xj|p−1 sign(xi − xj)

with small p ∈ (1, 2) was proposed, in view of the fact that the cut by threshold the second
eigenvector of the graph p-Laplacian tends to the Cheeger cut as p → 1+ [11]. Here j ∼ i
vertices adjacent to vertex i, and sign(t) is the standard sign function which equals to 1 if
t > 0, 0 if t = 0, and −1 if t < 0. The resulting (cid:96)p relaxation

denotes vertex j is adjacent to vertex i, (cid:80)j∼i means the summation is with respect to all

(1.1)

(1.2)

(cid:80)i∼j |xi − xj|p
(cid:80)n

i=1 di|xi|p

is diﬀerentiable but nonconvex, so that standard Newton-like methods can be applied, but only
local minimizers are obtained. In actual calculations, multiple runs with random initializations
are taken to approximate the global minimizer.

All above mentioned p-spectral clustering for any p ∈ (1, 2] are indirect methods.
Since the second (the ﬁrst non-zero) eigenvalue of 1-Laplacian (see Deﬁnition 1) for con-
nected graphs equals to the Cheeger constant, and the corresponding eigenvectors provide
exact solutions of the Cheeger cut problem [7, 12]. We study the numerical solution of the
second eigenvector of the graph 1-Laplacian.

However, the (cid:96)1 nonlinear eigenvalue problem (the corresponding object function is ob-
tained by setting p = 1 in Eq. (1.2)) is not only nonconvex but also nondiﬀerentiable. Three
types of algorithms have been proposed to minimize the 1-spectral clustering problem. They
are: the Split-Bregman like ratio minimization algorithm [7], the inverse power (IP) method [8],
and the steepest descent (SD) algorithm [13]. Unfortunately, all these methods fail to give
global minimizers.

Motivated by a recently developed nonlinear spectral graph theory of 1-Laplacian [12] by
the ﬁrst author of this paper, we propose a cell descend (CD) algorithm framework. The main

2

idea is based on the fact that the feasible set consists of a collection of cells, and the objective
function on each cell is convex. At each step, one obtains the minimum on the initial cell, and
ﬁnding out a suitable direction transfer to a new cell by descending the value of the objective
function. This CD algorithm framework combines the advantages of both the original discrete
combination optimization and its equivalent continuous optimization.

Preliminary numerical results on several typical graphs demonstrate that the proposed
In order
CD algorithm framework could provide better cut than the IP and SD methods.
to provide a solid theoretical foundation for algorithms, we also present a uniﬁed framework
for designing algorithms for the 1-Laplacian Cheeger cut problem, in which both IP and SD
algorithms can be easily recovered but from a totally diﬀerent angle.

The paper is organized as follows. In the rest of this section we list the notations, deﬁnitions
and some known results. A brief review of the spectral theory of 1-Laplacian and Cheeger
cut is in Section 2. A new CD framework for solving the Cheeger cut problem is proposed in
Section 3. The corresponding numerical experiments on several typical graphs are presented
with discussions in Section 4. The conclusion with a few remarks is presented in Section 5.

1.1 Notations and deﬁnitions

In the following, we list some notations which will be used in this paper.
• To the edge e ∈ E, we assign an orientation, let i be the head, and j be the tail, they
are denoted by i = eh, and j = et respectively. Under this orientation, one deﬁnes the
incidence matrix B = (bei)l×n where

bei =

1,
−1,
0,

if
if
if

i = eh,
i = et,
i /∈ e,

(1.3)

with e ∈ E, i ∈ V and l being the number of edges in E.

• For any x ∈ Rn, we denote

D+(x) = {i ∈ V |xi > 0}, D0(x) = {i ∈ V |xi = 0}, D−(x) = {i ∈ V |xi < 0}, (1.4)

δ±(x) = (cid:88)i∈D±(x)
n(cid:88)i,j=1

I(x) =

1
2

di,

δ0(x) = (cid:88)i∈D0(x)

di,

aij|xi − xj|,

(cid:107)x(cid:107)1,d =

,

2

i(cid:33) 1

x2

(cid:107)x(cid:107)2 =(cid:32) n(cid:88)i=1
n(cid:88)i=1

di|xi|, F (x) =

I(x)
(cid:107)x(cid:107)1,d

.

• Let

X = {x ∈ Rn : (cid:107)x(cid:107)1,d = 1},
π = {x ∈ X : |δ+(x) − δ−(x)| ≤ δ0(x)}.

• For given S ⊂ Rn, deﬁne

cone(S) = {kx : x ∈ S, k > 0}.

3

(1.5)

(1.6)

(1.7)
(1.8)

(1.9)

• Given a subset T ⊂ X, let x, y ∈ T , we say that x is equivalent to y in T , denoted by
x (cid:39) y in T , if there is a path γ connecting x and y in T , i.e. ∃ a continuous γ : [0, 1] → T
such that γ(0) = x, γ(1) = y.

Deﬁnition 1. Given a graph G = (V, E), for any x ∈ Rn, the set valued map:

∆1 : x → {BT z|z : E → R1 is a Rl vector satisfying ze(Bx)e = |(Bx)e|, ∀ e ∈ E}

is called the graph 1-Laplacian on G. Namely, it can be rewritten as

∆1x = BT Sgn(Bx),

(1.10)

where Sgn : Rn → (2R)n is a set valued mapping:

Sgn(y) = (Sgn(y1), Sgn(y2),··· , Sgn(yn)),

∀y = (y1, y2,··· , yn) ∈ Rn,

and

For convenience, we often write Eq. (1.10) in the coordinate form for i = 1, 2,··· , n:

if
if
if

t > 0,
t < 0,
t = 0.

1,
−1,
[−1, 1],

Sgn(t) =
zij(x) ∈ Sgn(xi − xj), zji(x) = −zij(x), ∀j ∼ i(cid:41) .

(1.11)

(∆1x)i = (BT Sgn(Bx))i

=(cid:40)(cid:88)j∼i

zij(x)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Obviously, from Eq. (1.11), the graph 1-Laplacian ∆1 is a nonlinear set valued mapping, which
is independent to the special choice of orientation.
Deﬁnition 2. Given a graph G = (V, E), a pair (µ, x) ∈ R1 × X is called an eigenpair of the
1-Laplacian ∆1 on G if
(1.12)

µD Sgn(x)(cid:92) ∆1x (cid:54)= ∅.

In the coordinate form, Eq. (1.12) is equivalent to the system: ∃zij(x) ∈ Sgn(xi−xj) satisfying
zji(x) = −zij(x),∀j ∼ i, and

zij(x) ∈ µdi Sgn(xi),

i = 1, 2,··· , n.

(1.13)

(cid:88)j∼i

The set of all solutions of Eq. (1.12) is denoted by S(G).

Remark 1. According to Theorem 10 (vide post), one can choose either S = D+(x) or
S = D−(x) based on an eigenvector x with respect to the second eigenvalue to produce a
Cheeger cut (S, Sc).

4

Deﬁnition 3. Given a graph G = (V, E) and x ∈ Rn, let σ : {1, 2,··· , n} → {1, 2,··· , n} be
a permutation satisfying xσ(1) ≤ xσ(2) ≤ ··· ≤ xσ(n). Then there exists unique k0 ∈ {1,··· , n}
such that

−

n(cid:88)i=1

The set

dσ(i) < ··· <

dσ(i) −

k0−1(cid:88)i=1
n(cid:88)i=k0
(cid:40)[xσ(k0), xσ(k0+1)],

{xσ(k0)},

is said to be the median of x, denoted by median(x).

dσ(i) < 0 ≤

k0(cid:88)i=1

dσ(i) −

n(cid:88)i=k0+1

dσ(i) < ··· <

dσ(i).

n(cid:88)i=1

if (cid:80)k0
if (cid:80)k0

i=1 dσ(i) −(cid:80)n
i=1 dσ(i) −(cid:80)n

i=k0+1 dσ(i) = 0,
i=k0+1 dσ(i) > 0,

Deﬁnition 4. Let m : {1,··· , n} → {−1, 0, 1} be a nonzero map. The set

(cid:52)m := {x ∈ X :

n(cid:88)i=1

is called a cell.

dim(i)xi = 1 with m(i)xi > 0 for m(i) (cid:54)= 0}

We have several remarks about the cell as follows.

• (cid:52)m are piecewise linear manifolds. The center of gravity of (cid:52)m is

center((cid:52)m) =

1
δ

n(cid:88)i=1

dim(i)ei.

The dimension of (cid:52)m is

dim(cid:52)m = Card{m(i) : m(i) (cid:54)= 0} − 1,

and the total number of k-dimensional cells is C k+1

n 2k+1 for k = 0, 1,··· , n − 1.

• It is evident that sign(x) := (sign(x1),··· , sign(xn)) = m if and only if x ∈ (cid:52)m.
Consequently, the set of all cells having nonempty intersection with π can be denoted
by Π:

Π = {(cid:52)m : ∃x ∈ π such that sign(x) = m}.

(1.14)

• Hereafter, we sometimes use [i · m(i) : m(i) (cid:54)= 0] to denote the cell (cid:52)m for simplicity.

2 Theory of 1-Laplacian Cheeger cut

In this section, we present a detailed review of the theory of 1-Laplacian Cheeger cut,
including the spectrum of ∆1, the property of the feasible set and the connection between the
Cheeger constant h(G) and the second eigenvalue µ2 of ∆1.

5

2.1 Spectrum of ∆1

The function I(x) deﬁned in Eq. (1.6) is Lipschitzian on Rn. Let ˜I = I|X. A characteri-

zation of the subgradient vector ﬁeld on X has been studied in [12], by which one proves

Theorem 1 (Theorem 4.11 in [12]). x ∈ S(G) if and only if x is a critical point of ˜I.

I(·)
(cid:107)·(cid:107)1,d

Remark 2. It must be pointed out that, the deﬁnition (see Deﬁnition 4.4 in [12]) of the critical
point of ˜I adopted in Theorem 1 is diﬀerent from the usual one which studies the critical point
in the
of
sense of the Clarke derivative must be the eigenvectors of ∆1, but the inverse is not true. To
clarify this, we present an example in Appendix A.

through the so-called Clarke derivative [8]. In fact, the critical points of

I(·)
(cid:107)·(cid:107)1,d

Let K denote the set of critical points of ˜I, and let Kc denote the subset of K with critical

value c.

The Liusternik-Schnirelmann theory is extended to study the multiplicity of the critical
points for the even function ˜I. The notion of genus due to Krasnoselski is introduced, see for
instance, [14, 15]. Let T ⊂ Rn\{0} be a symmetric set, i.e. −T = T satisfying 0 /∈ T . An
integer valued function, which is called the genus of T , γ : T → Z+ is deﬁned to be:

γ(T ) =(cid:40)0,

if T = ∅,
min{k ∈ Z+ : ∃ odd continuous h : T → Sk−1}, otherwise.

Obviously, the genus is a topological invariant.

Let us deﬁne

ck = inf

γ(T )≥k

max
x∈T⊂X

I(x),

k = 1, 2,··· n.

It can be proved that these ck are critical values of ˜I. One has

(2.1)

(2.2)

c1 ≤ c2 ≤ ··· ≤ cn,

c = ck+1 = ··· = ck+l, 0 ≤ k ≤ k + l ≤ n,

and if

then γ(Kc) ≥ l.

A critical value c is said of multiplicity l, if γ(Kc) = l.

Theorem 2 (Theorem 4.10 in [12]). There are at least n critical points φk, k = 1, 2,··· , n of
˜I such that φk ∈ Kck. Moreover, counting multiplicity, ˜I has at least n critical values.
Theorem 3 (Corollary 5.5 in [12]). If a graph G consists of r connected components G1, G2,··· , Gr,
then the eigenvalue µ = 0 has multiplicity r, i.e. all the eigenvectors with respect to µ = 0
form a critical set K0 with γ(K0) = r.

Lemma 1. Assume that x, y ∈ X satisfy:
(P1) sign preserving property: xi = 0 ⇒ yi = 0; xi < 0 ⇒ yi ≤ 0; xi > 0 ⇒ yi ≥ 0,
(P2) order preserving property: xi = xj ⇒ yi = yj; xi < xj ⇒ yi ≤ yj,

6

∀ i, j ∈ {1, 2,··· , n}. If (µ, x) is an eigenpair of ∆1, then (µ, tx + (1− t)y) is also an eigenpair
of ∆1, ∀ t ∈ [0, 1].
Proof. Since (µ, x) is an eigenpair, we have: ∃zij(x) ∈ Sgn(xi − xj) satisfying

zji(x) = −zij(x), ∀ i ∼ j and (cid:88)j∼i

zij(x) ∈ µdi Sgn(xi), i = 1,··· , n.

By (P1) and (P2), we deduce that Sgn(xi) ⊂ Sgn(yi) and Sgn(xi − xj) ⊂ Sgn(yi − yj). If one
takes zij(y) = zij(x),∀ i, j ∈ {1, 2,··· , n}, then zij(y) ∈ Sgn(yi − yj) also satisﬁes
zij(y) ∈ µdi Sgn(yi), i = 1,··· , n.

zji(y) = −zij(y), ∀ i ∼ j and (cid:88)j∼i

These mean (µ, y) is an eigenpair, too. More generally, for any given t ∈ [0, 1], we let (cid:101)y =
tx + (1 − t)y. It is easy to check that x and y :=(cid:101)y satisfy (P1) and (P2). In the same way,
we have (µ,(cid:101)y) is also an eigenpair. This completes the proof.

A vector ˆx = (ˆx1, ˆx2,··· , ˆxn) ∈ X is called a binary valued vector, if ˆx = c(a1,··· , an) for

some c > 0, where a1, a2,··· , an are either 1 or 0. By the above lemma, it follows
Theorem 4. For any eigenpair (µ, x) of ∆1, there exists a binary valued vector ˆx such that
|∂A|
(µ, ˆx) is an eigenpair and x (cid:39) ˆx in S(G)∩I−1(µ). Consequently, ∃A ⊂ V such that µ =
vol(A).
Proof. We only need to prove the last conclusion. In fact, let A = {i ∈ V | ˆxi = 1}. From the
system:

after summation on both sides, we have µ =

(cid:88)j∼i

zij(ˆx) = µdi ˆxi,

∀ i,

|∂A|
vol(A).

Theorem 5. The following facts are readily observed.
(1) For any eigenvalue µ of ∆1, we have 0 ≤ µ ≤ 1.
(2) The distance of two diﬀerent eigenvalues are at least

are only ﬁnite eigenvalues of ∆1.

4

n2(n−1)2 , where n = |V |. So there

(3) If (µ, x) is an eigenpair of ∆1, then I(x) = µ. Moreover, if µ (cid:54)= 0, then 0 ∈(cid:80)n
(4) If x ∈ X with x1 = x2 = ··· = xn, then x = ± 1
δ (1, 1,··· , 1).

i=1 di Sgn(xi).
δ (1, 1,··· , 1) is an eigenvector with
eigenvalue µ = 0. Conversely, if G is connected, then the eigenvector x corresponding
to the eigenvalue µ = 0 must be x = ± 1

Proof. We only prove here the fact (2). Other facts follow from [12].

For given two diﬀerent critical values µ, ˜µ, by Theorem 4, we have: ∃A, B ⊂ V such that
|∂A|
vol(A) and ˜µ =

|∂B|
vol(B). Accordingly, we obtain

µ =

|µ − ˜µ| =(cid:12)(cid:12)(cid:12)(cid:12)

|∂A|
vol(A) − |∂B|

vol(B)(cid:12)(cid:12)(cid:12)(cid:12) = ||∂A| vol(B) − |∂B| vol(A)|

vol(A) vol(B)

≥

7

1

vol(A) vol(B) ≥(cid:18) 1
n(cid:19)2

C 2

.

Since the number of eigenvalues are ﬁnite, all eigenvalues can be ordered as follow:

On the other hand, the subset of critical values (at least n if counting multiplicity) {ck}, is
ordered by the topological genus, i.e.,

0 = µ1 ≤ µ2 ≤ ··· ,

0 = c1 ≤ c2 ≤ ··· ≤ cn.

But we do not know whether these two sequences are identical.

Theorem 6. Any eigenvector x of ∆1 with eigenvalue µ (cid:54)= 0 lies on π.

According to Theorem 1, µ is a eigenvalue of ∆1 if and only if µ is a critical value of (cid:101)I(x).
Proof. By Theorem 5, we have 0 ∈ (cid:80)n
(cid:80)n
|δ+(x) − δ−(x)| =(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:88)i∈D+(x)

i=1 di Sgn(xi). So there exist θi ∈ Sgn(xi) such that

i=1 diθi = 0, i = 1, 2,··· , n. Then

di − (cid:88)i∈D−(x)

θidi(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:88)i∈D0(x)

which means that x ∈ π.

di(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

≤ δ0(x),

2.2 Properties of π

According to Theorems 6 and 10 (vide post), π is the feasible set for searching the Cheeger
cut and some useful properties of π will be shown in this section. By simple observation, we
have

Theorem 7. π is compact and π = (cid:83)(cid:52)∈Π(cid:52).
Lemma 2. x ∈ π if and only if δ+(x) ≤ δ
Proof. Without loss of generality, we assume δ+(x) ≥ δ−(x), then

2 and δ−(x) ≤ δ

2 with δ =(cid:80)n

i=1 di.

x ∈ π ⇔ δ+(x) − δ−(x) ≤ δ0(x)

⇔ δ+(x) − δ−(x) ≤ δ − δ+(x) − δ−(x)
⇔ 2δ+(x) ≤ δ
δ
⇔ δ+(x) ≤
2

and δ−(x) ≤

δ
2

.

Theorem 8. π is connected when n ≥ 3.
Proof. For any x ∈ π, δ+(x) ≤ δ
x1 (cid:54)= 0. On one hand, consider

2, δ−(x) ≤ δ

2. Without loss of generality, we may assume

γ(t) =

(x1, tx2,··· , txn)
i=2 di|xi|
8

d1|x1| + t(cid:80)n

,

t ∈ [0, 1].

d1

2 and δ−(γ(t)) ≤ δ−(x) ≤ δ

, 0,··· , 0). On the other hand, when n ≥ 3, we can prove (± 1

It is easily checked that both δ+(γ(t)) ≤ δ+(x) ≤ δ
have γ([0, 1]) ⊂ π. This means that the path γ connecting x and ( sign(x1)
i.e. x (cid:39) ( sign(x1)
(0,± 1
, 0,··· , 0). Actually, for ( 1
d1
iﬁed by taking the following path
y(t) =(cid:18) t

, 0,··· , 0(cid:19) ⊂ π, ∀ t ∈ [0, 1].

2 hold, then we
, 0,··· , 0) lies in π,
, 0, 0,··· , 0) (cid:39)
, 0,··· , 0), this can be readily ver-

, 0, 0,··· , 0) and (0,− 1

t − 1
d2

d1

d2

d1

d1

d2

,

For ( 1
d1

, 0, 0,··· , 0) and (0, 1

d2

, 0,··· , 0), we then have

(

1
d1

, 0, 0,··· , 0) (cid:39) (0, 0,−

1
d3

1
d2

,··· , 0) (cid:39) (0,

, 0,··· , 0).
, 0, 0,··· , 0) (cid:39) (0,··· , 1

Hence, for any y ∈ π with yi (cid:54)= 0, we have x (cid:39) ( 1
Remark 3. When n = 2, it can be easily checked that π consists of two disjoint connected
subsets.

,··· , 0) (cid:39) y.

d1

di

Lemma 3. y ∈ median(x) if and only if y = arg min

i=1 di|t − xi|.

t∈R (cid:80)n

−

di −

i=1 di − h(cid:80)n

i=k+1 di ≤ 0, then f (t + h) − f (t) ≤ 0, so f is decreasing on [xk, xk+1].
i=k+1 di ≥ 0, then f (t + h) − f (t) ≥ 0, so f is increasing on [xk, xk+1].

Without loss of generality, we assume x1 ≤ x2 ≤ ··· ≤ xn and x1 < xn, then there exists
i=k+1 di.

i=1 di|t − xi|.

k ∈ {1, 2,··· , n − 1} such that xk < xk+1.

Since there exists k0 ∈ {1,··· , n} such that

Proof. Let f (t) =(cid:80)n
Assume xk ≤ t < t + h < xk+1, then we have f (t + h) − f (t) = h(cid:80)k
i=1 di −(cid:80)n
If(cid:80)k
i=1 di −(cid:80)n
If(cid:80)k
n(cid:88)i=1
i=1 di −(cid:80)n
When (cid:80)k0
When(cid:80)k0
i=1 di −(cid:80)n

we deduce that f (t) is decreasing on (−∞, xk0] and increasing on [xk0+1, +∞).
median(x). Consequently, the set of minimal points of f equals to median(x).

n(cid:88)i=k0+1

di < ··· <

di < ··· <

i=k0+1 di > 0, we have median(x) = {xk0} and xk0 is the only minimal

i=k0+1 di = 0, we have median(x) = [xk0, xk0+1], and f is constant on

di,

n(cid:88)i=1

k0−1(cid:88)i=1

di < 0 ≤

n(cid:88)i=k0

point of f .

k0(cid:88)i=1

di −

Theorem 9. For any x ∈ X, we have

0 ∈ median(x) ⇔ x ∈ π.

In other word,

π = {x ∈ X : 0 ∈ median(x)}.

(2.3)

9

Proof. Suppose x1 ≤ ··· ≤ xn.

⇒:

i=1 di −(cid:80)n

If there exists k such that (cid:80)k
D−(x) ⊂ {1, 2,··· , k}, D+(x) ⊂ {k + 1,··· , n}. Then we have δ−(x) ≤ (cid:80)k
δ+(x) ≤ (cid:80)n
i=1 di −(cid:80)n
(cid:80)k
i=1 di −(cid:80)n
D−(x) ⊂ {1, 2,··· , k − 1}, D+(x) ⊂ {k + 1,··· , n}. We still have δ−(x) ≤(cid:80)k−1
δ+(x) ≤(cid:80)n

i=k+1 di = 0, then median(x) = [xk, xk+1] and
i=1 di = δ
2 and
i=k+1 di = δ
i=k di < 0 <
i=k+1 di, then median(x) = {xk}. Since 0 ∈ median(x), xk = 0. Therefore,
2 and
i=k+1 di < δ

2. Otherwise, there exists k such that (cid:80)k−1

2. In either case, by Lemma 2, we always have x ∈ π.

⇐:
We suppose the contrary that 0 /∈ median(x)
When min median(x) > 0, let xk = min median(x), then we have

i=1 di < δ

k−1(cid:88)i=1

di −

n(cid:88)i=k

di < 0,

and δ+(x) ≥(cid:80)n

2 implying that x /∈ π, which is a contradiction.
When max median(x) < 0, let xk+1 = min median(x), then we have

i=k di > δ

0 ≤

k(cid:88)i=1

di −

n(cid:88)i=k+1

di <

k+1(cid:88)i=1

di −

n(cid:88)i=k+2

di,

and δ−(x) ≥(cid:80)k+1

i=1 di > δ

2 implying that x /∈ π. Again, it is a contradiction.

In summary, the compactness of the feasible set π in Theorem 7 ensures the existence
of a minimizer of the continuous function I(x) and the connectivity in Theorem 8 provides a
possibility of ﬁnding the minimizer via a descend method. Theorem 9 shows that the projection
of x ∈ X onto π can be realized by a simple operation x − median(x). In Section 3, new cell
descent algorithms for seeking the Cheeger cut will be designed according to the cell structure
of π (Theorem 7).

2.3 Connection between h(G) and µ2

Now let’s study the relationship between Cheeger’s constant and the eigenvalue of ∆1. In

[8, 12], it is proved:

Theorem 10 (Proposition 4.1 in [8]; Theorem 5.15 in [12]). We have

µ2 = min
x∈π

I(x) = h(G).

(2.4)

Combining Theorems 9 and 10 leads to [1, 7]

h(G) =

min

x(cid:54)=0, 0∈median(x)

I(x)
(cid:107)x(cid:107)1,d

=

x nonconstant α∈median(x)

min

I(x)

(cid:107)x − α1(cid:107)1,d

.

(2.5)

Theorem 10 means that we could minimize the original discrete graph cut problem through
solving an equivalent continuous function optimization problem. As mentioned in Remark 1,

10

based on corresponding eigenvector x, via x (cid:39) ˆx (i.e. ﬁnding the equivalent binary vector ˆx) by
Theorem 4, we are able to produce a unique Cheeger cut (A, Ac) by setting either A = D+(x)
or A = D−(x). On the other hand, for any Cheeger cut (A, Ac), we can also determine at
most two binary eigenvectors ˆx = 1A/ vol(A) or ˆx = 1Ac/ vol(Ac). Moreover, starting from
an eigenvector x (i.e. a Cheeger cut), Theorem 11 below implies that it is possible for us to
obtain a new binary eigenvector (i.e. a new Cheeger cut) by investigating the connectedness of
resulting subgraphs from the pre-existing Cheeger cut. Actually, the connectedness of those
subgraphs is closely related to the nodal domains of eigenvectors [12].

Theorem 11. Let G = (V, E) be connected, the following statements hold.

(1) If (A, Ac) is a Cheeger cut and A is a disjoint union of two nonempty sets A1, A2 satisfying
2) are also Cheeger cuts with

1), (A2, Ac

E(A1, A2) = ∅, then vol(A) ≤ vol(Ac), and (A1, Ac
|∂Ai|
vol(Ai) = h(G), i = 1, 2.

(2) If (A, Ac) is a Cheeger cut, then one of A, Ac is connected.

(3) There exists a Cheeger cut (A, Ac) such that A, Ac are both connected.

Proof. (1) One can easily check that ∂A is the disjoint union of ∂A1 and ∂A2, it follows
|∂A| = |∂A1| + |∂A2| and vol(A) = vol(A1) + vol(A2). Therefore,

|∂A|
vol(A)

= |∂A1| + |∂A2|

vol(A1) + vol(A2) ≥ min(cid:26) |∂A1|

vol(A1)

,

|∂A2|

vol(A2)(cid:27) .

(2.6)

Meanwhile, |∂Ac

1| = |∂A1| < |∂A| = |∂Ac| and A1 (cid:38) A implies vol(Ac) < vol(Ac

1). Then

We claim:

|∂Ac
1|
vol(Ac
1)

< |∂Ac|
vol(Ac)

.

|∂A1|
vol(A1)

= |∂A2|
vol(A2)

= |∂A|
vol(A)

.

(2.7)

For otherwise, according to Eq. (2.6), we may assume without loss of generality that

Then we have

It follows from Eq. (2.7),

max(cid:26) |∂A1|

vol(A1)

,

This is a contradiction.

vol(A1) ≤ |∂A2|
|∂A1|

vol(A2)

.

|∂A1|
vol(A1)

< |∂A|
vol(A)

.

|∂Ac
1|
vol(Ac

1)(cid:27) < max(cid:26) |∂A|

vol(A)

,

|∂Ac|

vol(Ac)(cid:27) = h(G).

11

Now we turn to prove: vol(A) ≤ vol(Ac). If not, i.e. vol(A) > vol(Ac), then combining

with Eq. (2.7) yields

max(cid:26) |∂A1|

vol(A1)

,

|∂Ac
1|
vol(Ac

1)(cid:27) = max(cid:26) |∂A|

vol(A)

,

|∂Ac
1|
vol(Ac

Again, it is a contradiction. Thus

max(cid:26) |∂A1|

vol(A1)

,

|∂Ac
1|
vol(Ac

1)(cid:27) = max(cid:26) |∂A|

vol(A)

,

|∂Ac
1|
vol(Ac

1)(cid:27) = |∂A|

vol(A)

= h(G).

vol(Ac)

1)(cid:27) < |∂Ac|
= max(cid:26) |∂A|

vol(A)

,

|∂Ac|

vol(Ac)(cid:27) = h(G),

i.e. (A1, Ac

1) is a Cheeger cut. Similarly, (A2, Ac

2) is also a Cheeger cut.

(2) Suppose the contrary. There is a Cheeger cut (A, Ac) such that A = A1(cid:116)A2, Ac = B1(cid:116)
B2, where A1, A2, B1, B2 are four disjoint nonempty sets satisfying E(A1, A2) = ∅, E(B1, B2) =
∅. By (1), we know vol(A) = vol(Ac). That is, vol(A1) + vol(A2) = vol(B1) + vol(B2). The
sketch for their possible connection is shown below.

According to (1), (A1, Ac

1), (A2, Ac

2), (B1, Bc

1), (B2, Bc

2) are all Cheeger cuts. Since ∂A1 =

E(A1, B1) (cid:116) E(A1, B2), we have

Similarly, we obtain

|∂A1| = |E(A1, B1)| + |E(A1, B2)|.

|∂A2| = |E(A2, B1)| + |E(A2, B2)|,
|∂B1| = |E(B1, A1)| + |E(B1, A2)|,
|∂B2| = |E(B2, A1)| + |E(B2, A2)|.

|∂A1|
vol(A1) = h(G) leads to

The fact

|E(A1, B1)| + |E(A1, B2)| = |∂A1| = h(G) vol(A1).

Analogously, we have

|E(B1, A1)| + |E(B1, A2)| = h(G) vol(B1).

12

A1B1A2B2Since G is connected, at least three of E(A1, B1), E(A1, B2), E(A2, B2), E(A2, B1) are

nonempty. Then we may assume that |E(A1, B1)| > 0, |E(A2, B2)| > 0.

Let D = A1 (cid:116) B1 and Dc = A2 (cid:116) B2. Then we have

|∂D| = |E(A1, B2)| + |E(A2, B1)|,

and

Thus,

vol(D) = vol(A1) + vol(B1) =

1

h(G)

(2|E(A1, B1)| + |E(A1, B2)| + |E(A2, B1)|) .

|∂D|
vol(D)

= h(G)

|E(A1, B2)| + |E(A2, B1)|

2|E(A1, B1)| + |E(A1, B2)| + |E(A2, B1)|

< h(G).

Similarly,

|∂Dc|
vol(Dc) < h(G), we obtain a contradiction.

(3) We suppose that A is not connected, then by (2) we know that Ac is connected. There
i=1 Ai with k ≥ 2. Each
subgraph (Ai, E(Ai)) is connected. By noting that |E(Ai, Ac)| > 0, i = 1, 2,··· , k, we have
Ac
1 are connected. Further by (1), we
know that (A1, Ac

are ﬁnite disjoint nonempty sets A1, A2, ··· , Ak such that A = (cid:70)k
1 = Ac ∪(cid:70)k

i=2 Ai is connected. That is, both A1 and Ac

1) is a Cheeger cut.

Example 1. The Cheeger value of the following graph is 1
{11, 12, 13}, A = A1 (cid:116) A2. It can be easily veriﬁed that (A, Ac) is a Cheeger cut.

5. Let A1 = {1, 2, 3}, A2 =

It is seen that, Ac is connected but A is not. One can also easily check that, vol(A) = 10 <
vol(Ac) = 14; both (A1, Ac
1))
are connected subgraphs. Theorem 11 is veriﬁed on this graph.

2) are Cheeger cuts; both (A1, E(A1)) and (Ac

1) and (A2, Ac

1, E(Ac

Theorem 10 is the cornerstone of our noval cell descent algorithms delineated in Section 3.
Theorem 11 gives us some hint for choosing an initial cut. That is, it is better to start from
the initial cut each branch of which is connected.

13

12345678910111213A1A23 Algorithms for 1-Laplacian Cheeger cut

According to Theorems 7 and 10, we have

h(G) = µ2 = min
x∈π

I(x) = min
∆∈Π

min
x∈ ¯∆

I(x),

(3.1)

from which there are two diﬀerent approaches in treating the Cheeger cut problem. The
ﬁrst one is to regard it as a discrete combination optimization problem. By Theorem 7, the
minimizer of I(x) in π exists, it can be found by solving a convex programming on each of the
cells, and then by choosing the smallest one among them. However, the number of cells in Π
2 ] − 1, which is reminiscent of NP-hardness of the
Cheeger cut problem. The other one is to regard it as a continuous optimization problem on
the feasible set π. Both IP (see Eq. (3.7)) and SD (see Eqs. (3.9) and (3.10)) methods have
been used, but usually only local minimizers are obtained.

must be larger than(cid:80)[ n−1

n 2k+1 = 3[ n+1

2 ]

k=0 C k+1

In this section, we propose a new CD framework in solving the combinatorial optimization
problem. Since the objective function is convex on each cell, we produce a sequence of cells
(cid:52)k in π, on which the minimizers {xk} are decreasing.
transfer the search from one cell to the other.

In each step, we search a decreasing direction of I in π at the minimizer xk in order to

We are going to study the strategy for generating a decreasing direction.

Lemma 4. Let T ⊂ Rn − {0} be a bounded closed set. We have

x0 = arg min

x∈T

where λ0 = I(x0)
(cid:107)x0(cid:107)1,d

.

I(x)

(cid:107)x(cid:107)1,d ⇔ x0 = arg min

x∈T {I(x) − λ0(cid:107)x(cid:107)1,d},

Proof. ⇒:

Since x0 = arg min

x∈T

I(x)
(cid:107)x(cid:107)1,d

, we have

I(x)

(cid:107)x(cid:107)1,d ≥

I(x0)
(cid:107)x0(cid:107)1,d

= λ0, ∀ x ∈ T.

Therefore, I(x)−λ0(cid:107)x(cid:107)1,d ≥ 0. Now I(x0)−λ0(cid:107)x0(cid:107)1,d = 0, i.e. x0 = arg min

x∈T {I(x)−λ0(cid:107)x(cid:107)1,d}.

⇐:
Since x0 = arg min

x∈T {I(x) − λ0(cid:107)x(cid:107)1,d}, we have

I(x) − λ0(cid:107)x(cid:107)1,d ≥ I(x0) − λ0(cid:107)x0(cid:107)1,d = 0, ∀ x ∈ T.

Therefore I(x)

(cid:107)x(cid:107)1,d ≥ λ0 = I(x0)
(cid:107)x0(cid:107)1,d

, then x0 = arg min

x∈T

I(x)
(cid:107)x(cid:107)1,d

.

Lemma 5. Let T ⊂ Rn − {0} be a bounded closed set and x0 ∈ T be a given point. Assume

x1 = arg min

x∈T {I(x) − λ0(cid:107)x(cid:107)1,d},

then λ1 ≤ λ0, where λ0 = I(x0)
(cid:107)x0(cid:107)1,d

and λ1 = I(x1)
(cid:107)x1(cid:107)1,d

.

14

Proof. Since x1 = arg min

x∈T {I(x) − λ0(cid:107)x(cid:107)1,d}, we have

Taking x = x0 in the above equation, we have

I(x) − λ0(cid:107)x(cid:107)1,d ≥ I(x1) − λ0(cid:107)x1(cid:107)1,d, ∀ x ∈ T.

Therefore, λ0 ≥ I(x1)
(cid:107)x1(cid:107)1,d

0 = I(x0) − λ0(cid:107)x0(cid:107)1,d ≥ I(x1) − λ0(cid:107)x1(cid:107)1,d.
= λ1.

Based on Lemmas 4 and 5, we obtain

Theorem 12. Let T ⊂ {x ∈ Rn − {0} : 0 ∈ median(x)} be a bounded closed set containing
π. The two-step iterative scheme from any initial point x0 ∈ π
I(x) − λk(cid:107)x(cid:107)1,d,

xk+1 = arg min

(3.2)

λk+1 =

,

(3.3)



x∈T
I(xk+1)
(cid:107)xk+1(cid:107)1,d

produces a sequence λk convergent to the global minimum of I(x) in π.

Proof. According to Lemma 5, λk ≥ λk+1 ≥ 0 for any k ∈ N+, so there exists λ∗

k→+∞ λk = λ∗. By the deﬁnition of T , we know that x ∈ T implies

≥ 0 such
x(cid:107)x(cid:107)1,d ∈ π, and

that

lim

. Next we will prove that λ∗ = min
x∈T
I(x) − λ(cid:107)x(cid:107)1,d. It is easy to see that g(λ) is continuous on R since T

I(x)
(cid:107)x(cid:107)1,d

.

min
x∈π

I(x) = min
x∈T

I(x)
(cid:107)x(cid:107)1,d

Denote g(λ) = min
x∈T

is compact. Hence, we have

g(λ∗) = lim

k→+∞ g(λk) = lim

k→+∞(cid:107)xk+1(cid:107)1,d(λk+1 − λk) = 0,

which implies that

I(x) − λ∗

(cid:107)x(cid:107)1,d ≥ 0 ⇔ I(x/(cid:107)x(cid:107)1,d) = I(x)/(cid:107)x(cid:107)1,d ≥ λ∗

holds for all x ∈ T .

However, in solving the minimization problem (3.2), the ﬁrst diﬃculty is the nonlinear
constraint 0 ∈ median(x) (i.e. x ∈ π by Theorem 9). Taking the complicated geometry of π
into account, one often performs the minimization in the larger space X, and then projects
the resulting solution into π through a simple operation: x − α1, where α ∈ median(x) [7, 8].
The second diﬃculty comes from the non-diﬀerentiability of the objective function in
Eq. (3.2). The following results show that explicit subgradient methods hardly work.
Lemma 6. Let f be a Lipschitz function and x0 ∈ int(dom(f )). Then the subgradient ∂f (x0)
is a nonempty compact convex set. Let

v0 = arg min{(cid:107)v(cid:107) : v ∈ ∂f (x0)}.

Then we have (1) f (x0 − tv0) ≤ f (v0) for any suﬃciently small t > 0; (2) if 0 (cid:54)∈ ∂f (x0),
f (x0 − tv0) < f (v0) for any suﬃciently small t > 0.

15

Theorem 13. Let x0 ∈ π and λ0 = F (x0), where the function F is deﬁned in (1.6). Assume

z0
ij − λ0dic0

v0 =(cid:32)(cid:88)j
i(cid:33)i
zij +zji=0(cid:88)i (cid:32)(cid:88)j
zij − λ0dici(cid:33)2

= arg min

: zij ∈ Sgn(xi − xj), ci ∈ Sgn(xi)

,

(3.4)

then we have F (x0 − tv0) ≤ F (x0) for any suﬃciently small t > 0. Moreover, if x0 is not a
critical point of F , then we have F (x0 − tv0) < F (x0) for any suﬃciently small t > 0.

From Theorem 13, it seems that the decreasing direction −v0 should be our choice if the
minimization problem (3.4) is solved (In fact, it is also diﬃcult) and then we expect that
F (x0 − tv0) < F (x0) for a steplength t0 > 0 if x0 is not a critical point. However, the
direction −v0 may not be acceptable. The reason is twofold: First, x1 = x0 − t0v0 ∈ π is not
guaranteed; Second, after projecting x1 onto π, i.e. ˜x1 = x1−median(x1), F (˜x1) ≤ F (x0) may
not hold. In view of this, we turn to the implicit subgradient methods. The basic requirement
is to ensure F (˜x1) ≤ F (x0). To this end, a clever relaxation introduced in [8] is used.
Lemma 7. Let (·,·) denote the standard inner product in Rn. Then we have (1) (u, x) =
(cid:107)x(cid:107)1,d, ∀u ∈ ∂(cid:107)x(cid:107)1,d, ∀x ∈ Rn; (2) (u, y) ≤ (cid:107)y(cid:107)1,d, ∀u ∈ ∂(cid:107)x(cid:107)1,d, ∀x, y ∈ Rn.

The term (cid:107)x(cid:107)1,d is replaced by the inner produce (vk, x), where vk ∈ ∂(cid:107)xk(cid:107)1,d (this relax-
ation enlarges the objective function), the two-step iterative scheme (3.2)-(3.3) turns out to
be

xk+1 =

arg min

x(cid:54)=0,0∈median(x)

I(x) − λk(vk, x), vk ∈ ∂(cid:107)xk(cid:107)1,d,

λk+1 = F (xk+1),

and then we can easily recover the four-step iterative scheme [8] as follow:




(3.5)

(3.6)

(3.7)

xk+1 = arg min
(cid:107)x(cid:107)2≤1

I(x) − λk(vk, x), vk ∈ ∂(cid:107)xk(cid:107)1,d,

xk+1 = xk+1 − α · 1, α ∈ median(xk+1),
Choose vk+1 ∈ ∂(cid:107)xk+1(cid:107)1,d such that (vk+1, 1) = 0,
λk+1 = F (xk+1).

Theorem 14 (Theorem 4.1 in [8]). For given x0 ∈ π and v0 ∈ ∂(cid:107)x0(cid:107)1,d satisfying (v0, 1) = 0,
x1 is generated by the scheme (3.7). Then we have x1 ∈ π and F (x1) ≤ F (x0).
Remark 4.

• In [8], the iterative scheme (3.7) is called the inverse power method, which is an extension
of the inverse power method for linear systems. While a diﬀerent view presented here is
to regard it as some kind of relaxation introduced in Lemma 7.

16

• In the scheme (3.7), it should be noted that the 2-norm is only chosen for algorithmic
convenience and does not make any eﬀect on the ﬁnal results because the function F (x)
is homogeneous of degree zero.

• Furthermore, we can also add an extra positive term to make the induced optimization
problem bounded from below. This raises the following generalized steepest descent (GSD)
method.

Theorem 15. For given x0 ∈ π and v0 ∈ ∂(cid:107)x0(cid:107)1,d satisfying (v0, 1) = 0, let Gx0(x) be a
convex function on Rn and x0 is the minimum of Gx0(x). Assume
I(x) − λ0(v0, x) + λ0Gx0(x),

x1 = arg min
x∈Rn

(3.8)

then F (x1) ≤ F (x0). Moreover, F (˜x1) ≤ F (x0) holds for ˜x1 = x1 − α1 with α ∈ median(x1).
Proof. By the conditions, we know that x1 is a critical point of I(x) − λ0(v0, x) + λ0Gx0(x),
i.e.

so λ0(v0 − ∂Gx0(x1)) ∩ ∂I(x1) (cid:54)= ∅. Let g1 ∈ ∂Gx0(x1) such that λ0(v0 − g1) ∈ ∂I(x1), then
we have

0 ∈ ∂I(x1) − λ0v0 + λ0∂Gx0(x1),

I(x0) − I(x1) ≥ λ0(v0 − g1, x0 − x1)

= λ0(v0, x0 − x1) − λ0(g1, x0 − x1)
≥ λ0(v0, x0 − x1) − (Gx0(x0) − Gx0(x1))
= λ0(v0, x0 − x1) + Gx0(x1) − Gx0(x0)
≥ λ0(v0, x0 − x1).

Then taking x1 = x1 − α1 with α ∈ median(x1) leads to
I(x0) − I(x1) ≥ λ0(v0, x0 − x1)

= λ0(v0, x0) − λ0(v0, x1)
≥ λ0(cid:107)x0(cid:107)1,d − λ0(cid:107)x1(cid:107)1,d
= I(x0) − λ0(cid:107)x1(cid:107)1,d
(cid:107)x1(cid:107)1,d ≤ λ0.

Therefore, we get I(x1) − λ0(cid:107)x1(cid:107)1,d ≤ 0, i.e. λ1 = I(x1)

Theorem 15 implies that a further relaxation to enlarge the objective function can be

allowed and the following iterative scheme, i.e. the GSD method, should also work,



Remark 5.

I(x) − λk(vk, x) + λkGxk(x), vk ∈ ∂(cid:107)xk(cid:107)1,d,

xk+1 = arg min
x∈Rn
xk+1 = xk+1 − α1, α ∈ median(xk+1),
Choose vk+1 ∈ ∂(cid:107)xk+1(cid:107)1,d such that (vk+1, 1) = 0,
λk+1 = F (xk+1).

(3.9)

17

• Taking Gxk(x) =

(cid:107)x−xk(cid:107)2

2+c2(cid:107)vk(cid:107)2

2+2c(xk,vk)

2c

algorithm [13]. Then the updatation of xk+1 becomes

for c > 0, the scheme (3.9) reduces to the SD

xk+1 = arg min
x∈Rn

I(x) +

λk
2c(cid:107)x − (xk + cvk)(cid:107)2
2,

(3.10)

which is nothing but the so-called proximal gradient often used in implicit subgradient
methods [13, 16, 17].

• The relaxation from Eq. (3.2) to Eq. (3.5) is more essential than that from Eq. (3.5) to

Eq. (3.8).

In summary, by exploiting the idea of relaxing the objective function for searching the
descending direction in π, adopted by either IP or SD, we obtain two kinds of CD methods:
CD1 and CD2. In CD1, the inner problem appeared in the ﬁrst line of Eq. (3.7) is devised in
choosing a descending direction, and then the next cell, while the inner problem (3.10) is for
CD2. The skeleton of CD methods is given in Algorithm 1, where the CD1 method is taken
as an example.

Algorithm 1: The skeleton of cell descent algorithms.
Input : The initial cell (cid:52)1 ∈ Π.
Output: The cell (cid:52)

traversed cells #(cid:52).

∗ containing the minimizer x∗ with the minimum I∗, the number of

1 Set k = 1 and I(x0) = 1;
2 while True do
3

Solve xk = arg min
I(x);
x∈(cid:52)k
if I(xk) > I(xk−1) then
← (cid:52)k−1, x∗

return (cid:52)
Exit the loop;

∗

end if

← xk−1, I∗

← I(xk−1), #(cid:52) ← k;
i =(cid:40)di sign(xk

δ0(xk)

i ),

δ−(xk)−δ+(xk)

if xk
if xk

i (cid:54)= 0
i = 0

;

,

4

5

6

7

8

9

10

11

12

13

14

15

Choose vk ∈ ∂(cid:107)xk(cid:107)1,d such that (vk, 1) = 0, i.e. vk
Solve yk = arg min
(cid:107)x(cid:107)2≤1

I(x) − F (xk)(vk, x);

Set yk ← yk − median(yk);
Set (cid:52)k+1 = (cid:52)sign(yk);
if (cid:52)k+1 ∈ {±(cid:52)1,±(cid:52)2,··· ,±(cid:52)k} then

∗

← (cid:52)k, x∗

← xk, I∗

return (cid:52)
Exit the loop;

← I(xk), #(cid:52) ← k;

end if
Set k ← k + 1;

16
17 end while

18

Theorem 16. The sequence {I(xk)} produced by the CD1 method is decreasing and convergent
to a local minimum. Furthermore, the sequence {xk} produced by the CD1 method converges
to an eigenvector of the 1-Laplacian with eigenvalue λ∗

∈ [h(G), I(x0)].

Proof. Since

yk−1
(cid:107)yk−1(cid:107)1,d ∈ (cid:52)k, we obtain

I(x) = I(xk).

(3.11)

F (yk−1) = I(cid:18) yk−1

(cid:107)yk−1(cid:107)1,d(cid:19) ≥ min

x∈(cid:52)k

For any α ∈ median(yk), we can readily arrive at

I(yk − α · 1) − F (xk)(vk, yk − α · 1) = I(yk) − F (xk)(vk, yk)
≤ I(xk) − F (xk)(vk, xk)
= I(xk) − F (xk)(cid:107)xk(cid:107)1,d = 0,

since yk is a minimizer of I(x)− F (xk)(vk, x) and (vk, 1) = 0, where the translation invariance
of I(x) and Lemma 7 are applied. Accordingly, after the updation of yk ← yk− α· 1 in Line 10
of Algorithm 1, we have

I(yk) ≤ F (xk)(vk, yk) ≤ F (xk)(cid:107)yk(cid:107)1,d ⇔ F (xk) ≥ I(yk)/(cid:107)yk(cid:107)1,d = F (yk).

Combining Eqs. (3.11) and (3.12) leads to

(3.12)

I(x0) ≥ ··· ≥ F (yk−1) ≥ I(xk) ≥ F (yk) ≥ I(xk+1) ≥ ··· ≥ 0,

Because there are only ﬁnite cells in Π, the decreasing sequence {I(xk)}

where the fact that I(xk) = F (xk) holds for xk ∈ (cid:52)k ⊂ X has been used. That is, I(xk)
converges to λ∗ and λ∗
≥ h(G) due to Theorem 10. Below we will further show this λ∗ is
actually an eigenvalue of ∆1.
∞
k=1 can only take
ﬁnite diﬀerent values. That means there exists K ∈ Z+ such that I(xk) = F (yk) = λk = λ∗
for any k ≥ K. Furthermore, we claim that any xk(k ≥ K) must be the minimizer of
I(x) − F (xk)(vk, x) in the open ball {x : (cid:107)x(cid:107)2 < 1}. The reason is, (1) we have (cid:107)xk(cid:107)2
2 ≤
(cid:80)n
1,d = 1, i.e. (cid:107)xk(cid:107)2 < 1; (2) I(xk) − F (xk)(vk, xk) = 0
holds due to Lemma 7; (3) for any x ∈ {x : (cid:107)x(cid:107)2 < 1}, we have

i |2 < ((cid:80)n
I(x) − F (xk)(vk, x) ≥ I(yk) − F (xk)(vk, yk) ≥ I(yk) − F (xk)(cid:107)yk(cid:107)1,d = 0,

i |)2 = (cid:107)xk(cid:107)2

i=1 di|xk

i=1 di|xk

where we have used Line 9 of Algorithm 1 and Lemma 7. Therefore, xk is a critical point of
I(x) − F (xk)(vk, x) and then

0 ∈ ∂I(x) − F (xk)∂(vk, x) =(cid:32)(cid:88)j∼i

i(cid:33)n
Sgn(xi − xj) − λkvk

.

i=1

Consequently, it follows from vk
have λ∗ = λk is an eigenvalue.

i ∈ di Sgn(xi) that (xk, λk) is an eigenpair of ∆1. Hence we

For the CD2 method, we still have a similar local convergence. In the CD1 method as
shown in Algorithm 1, the main cost is spent on two inner minimization problems. The ﬁrst
one is shown in Line 3 and the second one is shown in Line 9, both of which are convex
and then can be solved by any standard solver for convex optimization, e.g. the alternating
direction method [17, 18] and the MOSEK solver. The same situation applies to the CD2
method.

19

4 Numerical experiments

Applying the idea of relaxing the objective function in π, a CD algorithm framework
in solving the 1-Laplacian Cheeger cut problem is proposed and two speciﬁed CD methods
(i.e. CD1 and CD2) are presented. The CD1 method adopts the inner problem Eq. (3.7) to
search the next cell, while the CD2 method uses (3.10). Actually, we have shown that both
IP and SD methods can be readily recovered from such relaxation.

(a) A Petersen graph on 10 vertices.

(b) A path graph on 10 vertices P10.

(c) A roach Graph on 20 vertices R20.

Figure 1: Several typical graphs.

In this section, we conduct a compare study of the CD1, CD2, IP and SD methods in terms
of performance on several typical graphs, e.g. the Petersen graph, the path graph (e.g. P10),
the roach graph and the complete graph (e.g. K10). The Petersen graph has 10 vertices (see
Fig. 1(a)) and the exact Cheeger value is h(G) = µ2 = 1
3 while the second eigenvalue of the
standard graph Laplacian is λ2 = 2µ2 = 2
3. That is, the Cheeger value takes the lower bound
of inequality (1.1) on the Petersen graph, implying that the Petersen graph should be one
of the most typical graphs on which the 2-spectral clusering fails to determine the Cheeger
cut. Another interesting graph is the so-called roach graph [10], which has 4k vertices (see
Fig. 1(c) for k = 5, i.e. a roach graph R20) and is a typical graph that the 2-spectral clusering
fails, too. Four roach graphs of diﬀerent size (i.e. R4k with k = 2, 3, 4, 5) are tested. That is, a

20

16257348910123456789101234567891011121314151617181920Table 1: Performance of the IP, SD, CD1 and CD2 methods on typical graphs. Avergae number of
iterations and the percent for the numerical Cheeger value ˜h reaching the exact Cheeger value h.

Graph

Average No. of iterations

Percent for ˜h = h

IP

SD

CD1 CD2

IP

SD CD1 CD2

Petersen 2.889

4.925

2.746

2.789

33.1

32.4

37

31

P10
K10
R8
R12
R16
R20

3.984

7.438

2.774

3.474

79.8

76.5

98.6

86.1

2.435

4.277

3.191

3.128

61.1

85.3

100

3.216

5.794

2.753

2.727

43.2

44.6

82.4

3.481

6.521

2.722

2.989

97.3

95.9

100

3.887

9.957

2.843

2.826

49.1

52.1

4.239

10.782

3.087

3.02

52.7

55.7

52.7

57.7

100

51.7

98.9

42.3

47.9

comparison between the 1- and 2-Laplacian Cheeger cuts in the roach graphs makes no sense.
Moreover, this work focuses on the 1-Laplacian Cheeger cut, while its comparison with the
p-spectral clusering (1 < p < 2) methods can be found in [7, 8].

To solve all envolved convex inner problems as well as to make a fair comparison, we use the
same MOSEK solver with CVX, a package for specifying and solving convex programs [19, 20].
We set c = 1 in the inner problem (3.10) as in [13].

9, 5

9, 1

3, 1

5, 1

3, 1

8, 1

During all the experiments, we start from the same 1000 initial random cuts and record
the numbers of iterations as well as the resulting numerical Cheeger values. Let ˜h denote
the numerical Cheeger value. Table 1 shows the percent for ˜h equalling the exact Cheeger
value h (h equals to 1
11 for the Petersen graph, P10, K10, R8, R12, R16 and R20,
respectively) and the average number of iterations required in those four methods. From there
it is easily seen that: (a) the SD method needs more iterations than the other and its average
number of iterations increases quickly on the roach graph as the graph size increases; (b) both
CD1 and CD2 methods require almost the same average numbers of iterations on all seven
graphs, which are also less than those for the IP method; (c) the CD1 method has the highest
percent to reach the exact Cheeger value. Table 2 further presents the pairwise comparison of
the numerical Cheeger values obtained by the IP, SD, CD1 and CD2 methods. Overall, the
CD1 method provides the best cut among those four methods. For example, on the Petersen
graph, the percent for ˜hIP < ˜hCD1 is far less than that for ˜hIP > ˜hCD1, which means, within
the 1000 numerical Cheeger values from the same 1000 initial random cuts, the CD1 method
produces much more smaller numerical Cheeger values than the IP method. In such sense,
we have the order on the Petersen graph: CD1 > SD ≈ CD2 > IP. In summary, the CD1
method has the better performance in terms of both the number of iterations and the quality
of cut, but the CD2 method does not show clear advantage in the quality of cut. That is, while
using the the CD framework to design a speciﬁed algorithm, as we expected, the strategy for
searching next cell plays a key role. The numerical results here show that the strategy using
the inner problem in Eq. (3.7) is better than that using the inner problem (3.10).

21

Graph

Petersen

P10
K10
R8
R12
R16
R20

Graph

Petersen

P10
K10
R8
R12
R16
R20

Graph

Petersen

P10
K10
R8
R12
R16
R20

Table 2: Comparison of the numerical Cheeger values.
SD v.s. CD2

IP v.s. CD1

Percent for ˜hIP (cid:83) ˜hCD1

Percent for ˜hSD (cid:83) ˜hCD2

˜hIP < ˜hCD1

˜hIP > ˜hCD1

˜hIP = ˜hCD1

˜hSD < ˜hCD2

˜hSD > ˜hCD2

˜hSD = ˜hCD2

0.8

0.4

0

0.5

0

9.1

10.3

11

19.2

38.9

39.8

2.7

14.4

16

88.2

80.4

61.1

59.7

97.3

76.5

73.7

7.6

8

0

13.7

0.8

20.1

19.9

7.5

18.8

14.7

28.2

3.8

11.90

13.3

84.9

81.4

85.3

58.1

95.4

68

66.8

SD v.s. CD1

Percent for ˜hSD (cid:83) ˜hCD1

CD1 v.s. CD2

Percent for ˜hCD1 (cid:83) ˜hCD2

˜hSD < ˜hCD1

˜hSD > ˜hCD1

˜hSD = ˜hCD1

˜hCD1 < ˜hCD2

˜hCD1 > ˜hCD2

˜hCD1 = ˜hCD2

7.3

0.4

0

2.9

0

12.2

11.7

9

22.5

14.7

40.5

4.1

15.7

15.1

83.7

77.1

85.3

56.6

95.9

72.1

73.2

8.4

12.5

0

32.7

1.1

13.8

14.4

6.4

0

0

2.2

0

1.8

4

85.2

87.5

100

65.1

98.9

84.4

81.6

SD v.s. IP

Percent for ˜hSD (cid:83) ˜hIP

IP v.s. CD2

Percent for ˜hIP (cid:83) ˜hCD2

˜hSD < ˜hIP

˜hSD > ˜hIP

˜hSD = ˜hIP

˜hIP < ˜hCD2

˜hIP > ˜hCD2

˜hIP = ˜hCD2

12.4

11.4

30.2

13.7

0.6

10.6

10

3.6

15

6

12.2

2

8.6

8.1

84

73.6

63.4

74.1

97.4

80.8

81.9

5.9

8.8

0

10.2

0.8

17.4

16.7

14.6

16.2

38.9

19

2.4

10.7

12.5

79.5

75

61.1

70.8

96.8

71.9

70.8

22

5 Conclusions and discussions

In this paper, we present a systematic review of the theory of the 1-Laplacian Cheeger
cut, where the spectrum of the 1-Laplacian ∆1, the property of the feasible set π and the
conncetion between the Cheeger value h(G) and the second eigenvalue µ2 of ∆1 are studied in
details. Taking advantage of the cell structure of π, we propose a cell descend (CD) algorithm
framework for achieving the Cheeger cut. Combined with the relaxation to guarantee the
descrease of the objective value in π, we obtain two speciﬁed CD methods. Moreover, we also
show that the inverse power (IP) method and the steepest descent (SD) method can also be
generally recovered from such relaxation. A compare study of all these methods are conducted
on several typical graphs. We ﬁnd that, the CD1 method, which adopts the inner problem in
Eq. (3.7) to search the next cell, performs better than both IP and SD methods. We must
admit that all the numerical results presented in this work are preliminary since the size of the
graphs is small, but precisely because of small size of those typical graphs the exact cuts can
be obtained conveniently for comparison. The performance of our CD algorithm framework
on graphs with relatively large size is still going on. Furthermore, developping theory and
algorithms for multi-class (≥ 3) cut and designing other strategies than the relaxation for
searching next cell will also be our furture projects.

Acknowledgement

This research was supported by grants from the National Natural Science Foundation of

China (Nos. 11371038, 11471025, 11421101, 61121002).

Appendix A: An example

The critical points of

in the sense of the Clarke derivative [8] are actually not equiv-
alent to the eigenvectors of ∆1. In this Appendix, we give an example. Let G = (V, E) be the
following graph.

I(·)
(cid:107)·(cid:107)1,d

Consider the point

x0 =

1
12

(0, 0, 1, 1, 1,−1,−1,−1) ∈ X.

We can easily check that x0 is an eigenvector of ∆1 and the corresponding eigenvalue is µ = 1
3.
However, for the direction

h0 = (1,−1, 0, 0, 0, 0, 0, 0) ∈ R8,

23

1LetG=(V,E)beagraphasfollows.43512768Letx=112(0,0,1,1,1,−1,−1,−1)beapointinX.PartI:xisaneigenvectorandµ=I(x)=13isaneigenvalue.Takez31=z51=z26=z27=z12=1,z35=z43=z76=z68=13,(1)andc03=c04=c05=c02=1,c06=c07=c08=c01=−1.(2)Wecaneasilyverifythatzij∈Sgn(xi−xj)andc0i∈Sgn(xi)andz12+z13+z15=1−1−1=−1=13·3·(−1)=µd1c01,z21+z26+z27=−1+1+1=1=13·3·1=µd2c02,z34+z35+z31=−13+13+1=1=13·3·1=µd3c03,z43=13=13·1·1=µd4c04,z53+z51=−13+1=23=13·2·1=µd5c05,z62+z67+z68=−1−13+13=−1=13·3·(−1)=µd6c06,z72+z76=−1+13=−23=13·2·(−1)=µd7c07,z86=−13=13·1·(−1)=µd8c08(3)Soxisaneigenvector.PartII:xisnotacriticalpointofI(y)kykinthesenseofClarkederivative.Leth=(1,−1,0,0,0,0,0,0)∈R8.Wewillprovethatlimsupw→0,t→0+1t(cid:18)I(x+w+th)kx+w+thk−I(x+w)kx+wk(cid:19)≤−2.(4)Itisequivalenttolimsupw→0,t→0+I(x+w+th)kx+wk−I(x+w)kx+w+thkt≤−2.(5)we have

1

t(cid:18) I(x0 + w + th)
(cid:107)x0 + w + th(cid:107)1,d −
That is, x0 is not a critical point I(·)
(cid:107)·(cid:107)1,d

lim sup
w→0,t→0+

I(x0 + w)

(cid:107)x0 + w(cid:107)1,d(cid:19) ≤ −2.

in the sense of the Clarke derivative.

References

[1] F. R. K. Chung. Spectral Graph Theory. American Mathematical Society, USA, revised

edition, 1997.

[2] A. K. Jain, M. N. Murty, and P. J. Flynn. Data clustering: A review. ACM Computing

Surveys, 31:264–323, 1999.

[3] J. B. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Trans. Pattern

Anal. Mach. Intell., 22:888–905, 2000.

[4] M. Hein and S. Setzer. Beyond spectral clustering - tight relaxations of balanced graph
In Advances in Neural Information Processing Systems 24 (NIPS 2011), pages

cuts.
2366–2374, 2011.

[5] T. B¨uhler, S. S. Rangapuram, S. Setzer, and M. Hein. Constrained fractional set programs
and their application in local clustering and community detection. In Proceedings of the
30th International Conference on Machine Learning, pages 624–632, 2013.

[6] J. Cheeger. A lower bound for the smallest eigenvalue of the laplacian. In R. C. Gunning,

editor, Problems in Analysis, pages 195–199. Princeton University Press, 1970.

[7] A. Szlam and X. Bresson. Total variation and Cheeger cuts. In Proceedings of the 27th

International Conference on Machine Learning, pages 1039–1046, 2010.

[8] M. Hein and T. B¨uhler. An inverse power method for nonlinear eigenproblems with
applications in 1-spectral clustering and sparse PCA. In Advances in Neural Information
Processing Systems 23, pages 847–855, 2010.

[9] X. Bresson, T. Laurent, D. Uminsky, and J. H. von Brecht. Multiclass total variation
clustering. In Advances in Neural Information Processing Systems 26 (NIPS 2013), pages
1421–1429, 2013.

[10] U. von Luxburg. A tutorial on spectral clustering. Stat. Comput., 17:395–416, 2007.

[11] T. B¨uhler and M. Hein. Spectral clustering based on the graph p-laplacian. In Proceedings

of the 26th International Conference on Machine Learning, pages 81–88, 2009.

[12] K. C. Chang. Spectrum of the 1-laplacian and Cheeger’s constant on graphs. Preprint,

2014.

[13] X. Bresson, T. Laurent, D. Uminsky, and J. H. von Brecht. Convergence and energy
In Advances in Neural Information Processing

landscape for Cheeger cut clustering.
Systems 25 (NIPS 2012), pages 1385–1393, 2012.

24

[14] K. C. Chang. Critical Point Theory and Its Applications (in Chinese). Shanghai Science

and Technology Press, 1985.

[15] P. H. Rabinowitz. Minimax Methods in Critical Point Theory with Applications in Dif-

ferential Equations. American Mathematical Society, 1986.

[16] A. Chambolle and T. Pock. A ﬁrst-order primal-dual algorithm for convex problems with

applications to imaging. J. Math. Imaging Vis., 40:120–145, 2011.

[17] S. Q. Ma. Alternating proximal gradient method for convex minimization. Preprint, 2012.

[18] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and
statistical learning via the alternating direction method of multipliers. Found. Trends
Mach. Learning, 3:1–122, 2011.

[19] M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming,

version 2.1. http://cvxr.com/cvx, March 2014.

[20] M. Grant and S. Boyd. Graph implementations for nonsmooth convex programs.

In
V. Blondel, S. Boyd, and H. Kimura, editors, Recent Advances in Learning and Con-
trol, Lecture Notes in Control and Information Sciences, pages 95–110. Springer-Verlag
Limited, 2008.

25

