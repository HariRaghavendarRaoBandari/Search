6
1
0
2

 
r
a

M
6

 

 
 
]

.

C
N
o
i
b
-
q
[
 
 

1
v
0
8
8
1
0

.

3
0
6
1
:
v
i
X
r
a

Noise dynamically suppresses chaos in neural networks

Sven Goedeke∗,1 Jannis Schuecker∗,1 and Moritz Helias1, 2

1Institute of Neuroscience and Medicine (INM-6) and Institute for Advanced Simulation

(IAS-6) and JARA BRAIN Institute I, Jülich Research Centre, Jülich, Germany
2Department of Physics, Faculty 1, RWTH Aachen University, Aachen, Germany

(Dated: March 8, 2016)

∗ These authors contributed equally

Noise is ubiquitous in neural systems due to intrinsic stochasticity or external drive. For deter-
ministic dynamics, neural networks of randomly coupled units display a transition to chaos at a
critical coupling strength. Here, we investigate the eﬀect of additive white noise on the transition.
We develop the dynamical mean-ﬁeld theory yielding the statistics of the activity and the max-
imum Lyapunov exponent. A closed form expression determines the transition from the regular
to the chaotic regime. Noise suppresses chaos by a dynamic mechanism, shifting the transition to
signiﬁcantly larger coupling strengths than predicted by local stability analysis. The decay time of
the autocorrelation function does not diverge at the transition, but peaks slightly above the critical
coupling strength.

PACS numbers: 87.19.lj, 05.45.-a, 05.40.-a

Collective phenomena that emerge in systems with
many interacting degrees of freedom present a central
quest in physics.
In nature and technology, these sys-
tems operate in the presence of noise or time-dependent
drive, rendering their dynamics stochastic or nonau-
tonomous. Large random networks of neuron-like units
are a prominent example, which exhibit chaotic dynam-
ics [1–3]. Their information processing capabilities have
been a focus in neuroscience [4] and in machine learning
[5].
In particular, the capacity is optimal close to the
onset of chaos [6–8].

In the absence of noise or inputs, networks of ran-
domly coupled rate neurons display a transition from a
ﬁxed point to deterministic chaos at a critical coupling
strength [1]. The transition is well understood by dy-
namical mean-ﬁeld theory, originally developed for spin
glasses [9]. In particular, the onset of chaos is equivalent
to the emergence of a decaying autocorrelation function,
the dynamical order parameter of the transition. The de-
cay of the autocorrelation function has been used in sev-
eral subsequent studies as a criterion for chaotic dynam-
ics [10–12]. It was further demonstrated that the tran-
sition happens precisely when the ﬁxed point becomes
linearly unstable. This identiﬁes the spectral radius of
the random connectivity matrix [13, 14] as the parame-
ter controlling the transition. In stochastic systems, a de-
caying autocorrelation function does not indicate chaotic
dynamics. The stochasticity causes perpetual decorrela-
tion also for regular dynamics. Thus the transition to
chaos, if existent at all, must be qualitatively diﬀerent to
the deterministic case.

It is controversially discussed, whether the determin-
istic picture explains a transition to chaotic dynamics in
spiking neural networks [15], since the realization noise of
the spiking dynamics is missing. It was shown that the

spiking noise smooths the transition [16] and that the
correlation time in spiking simulations does not peak at
the point where the transition is predicted by the spec-
tral radius of the Jacobian [17, 18]. However, the funda-
mental mechanism by which noise aﬀects the maximum
Lyapunov exponent and the location of the transition are
not understood.

The eﬀect of noise on the transition to chaos has so far
only been studied for a time-discrete system [19], whose
order parameter is a single number, the variance of the
mean-ﬁeld. Therefore these results can not be transferred
to time-continuous systems whose order parameter, in
the deterministic case, is a function of time-lag.

To focus on the generic inﬂuence of noise on the tran-
sition to chaos, we subject the seminal model by Som-
polinsky et al. [1] to white noise and develop the dynami-
cal mean-ﬁeld theory for the resulting stochastic system.
The autocorrelation function can be understood as the
motion of a classical particle in a potential and we ﬁnd
that the noise amounts to the initial kinetic energy. We
show that the loss of local stability is only a necessary
condition for the transition from regular to chaotic dy-
namics. We derive an exact condition showing that noise
suppresses the transition signiﬁcantly stronger than ex-
pected from its eﬀect on local stability. This is explained
by a dynamic eﬀect of the noise:
it sharpens the auto-
correlation function decreasing the maximum Lyapunov
exponent. The Lyapunov exponent becomes positive pre-
cisely at the point when the curvature of the autocorre-
lation function at time-lag zero becomes negative. This
is equivalent to a parity between the strength of the in-
coming ﬂuctuations and the variance of a single unit.
Furthermore the decay time of the autocorrelation func-
tion does not diverge at the transition. Its maximum is
reduced by noise and occurs slightly above the critical

(a)

0

4
0
1
·

)
c
(

V

−20

0.0

)
τ
(
c

0.5

0.0

2

Ekin

c0

1.0

0.5

c

(b)

0

3
0
1
·

)
c
(

V

−15

1.0

0.0

(d)

c0

0.8

)
τ
(
c

0.5
c

0

20

τ

0

0

20

τ

Figure 1: Solution of the mean-ﬁeld theory. Upper row:
Self-consistent potential (7) for noise-free case σ = 0 (a) and
noisy case σ = √0.125 (b). Coupling strength g encoded in
grayscale from black to light gray with g ∈ [0, 1.2, 1.48, 1.7]
and in red for g = 1.48 indicating the critical coupling (12)
in the noisy case. Dashed line shows the initial kinetic en-
ergy Ekin = σ4/2. Lower row: Corresponding autocorrelation
function (solid line) in comparison to simulations (crosses) for
noise free (c) and noisy case (d). The largest c-values of the
curves in panel (a),(b) mark c0 and thus correspond to the
values at zero in (c),(d), as indicated for g = 1.7 (gray dotted
lines). Network size in simulations is N = 10000.

Our goal is to determine the mean-ﬁeld autocorrelation
function hx(t)x(s)i, which also describes the population-
averaged autocorrelation function due to self-averaging.
Assuming that x(t) is a stationary process, c(τ ) := hx(t+
τ )x(t)i obeys the diﬀerential equation

¨c =

d2c
dτ 2 = c − g2fφ(c, c0) − 2σ2δ(τ )

(4)

with c0 = c(0) = hx(t)2i. The Dirac δ inhomogeneity
originates from the white noise autocorrelation function
and is absent in [1]. In (4) we write hφ(x(t+τ ))φ(x(t))i =
fφ(c(τ ), c0), introducing the notation

fu(c(τ ), c0)

= ZZ u(cid:18)qc0 − c(τ )2

c0

z1 + c(τ )
√c0

z2(cid:19) u (√c0z2) Dz1Dz2

(5)

with the Gaussian integration measure Dz

exp(−z2/2)/√2π dz and an arbitrary function u(x).

This representation holds since x(t) is itself a Gaussian
process. Note that (5) reduces to one-dimensional
integrals for fu(c0, c0) = hu(x)2i and fu(0, c0) = hu(x)i2,
where x is Gaussian distributed with zero mean and
variance c0.

=

The Jij are independent and identically Gaussian dis-
tributed random coupling weights with zero mean and
variance g2/N , where the intensive gain parameter g con-
trols the recurrent coupling strength or, equivalently, the
weight heterogeneity of the network. The ξi(t) are inde-
pendent Gaussian white noise processes with autocorre-
lation function hξi(t)ξi(s)i = 2σ2δ(t − s). We choose the
sigmoidal transfer function φ(x) = tanh(x), such that
without noise, i.e., σ = 0, the model agrees with the one
studied in [1].

The dynamical system (1) contains two sources of ran-
domness: quenched disorder due to the random coupling
weights and temporal ﬂuctuations by the noise. A par-
ticular realization of the random couplings Jij deﬁnes
a ﬁxed network conﬁguration and its dynamical prop-
erties usually vary between diﬀerent realizations. For
large network size N , however, certain quantities are self-
averaging meaning that their values for a typical realiza-
tion can be obtained by an average over network conﬁg-
urations [20]. An important example is the population-
averaged autocorrelation function.

We study the dynamical mean-ﬁeld theory, which, in
the limit of N → ∞, describes the statistical properties
of the system under the joint distribution of disorder,
noise and possibly random initial conditions. The theory
can be derived via a heuristic “local chaos” assumption
[21] or using a generating functional formulation [9, 22],
while a rigorous proof employs large deviation techniques
[23]. The general idea is that for large network size N
j=1 Jij φ(xj ) in (1) approaches
a Gaussian process η(t) with self-consistently determined
statistics. As a result we obtain the dynamical mean-ﬁeld
equation characterizing the statistics of a typical neuron:

the local recurrent input PN

dx
dt

= −x + η(t) + ξ(t) .

(2)

Here, ξ(t) is a Gaussian white noise process as in (1),
independent of η(t). Because the random couplings Jij
have zero mean, the Gaussian process η(t) is centered,
hη(t)i = 0, and thus fully speciﬁed by its autocorrelation
function

coupling strength.

We study the continous-time dynamics of a random
network of N neurons, whose states xi(t) ∈ R, i =
1, . . . , N, evolve according to the system of stochastic dif-
ferential equations

dxi
dt

= −xi +

N

Xj=1

Jij φ(xj ) + ξi(t) .

(1)

(c)

hη(t)η(s)i = g2hφ(x(t))φ(x(s))i .

(3)

We reformulate (4) as a one-dimensional motion of a

classical particle in a potential, i.e.,

The expectation on the right-hand side is taken with re-
spect to the distribution of x(t).

¨c = −V ′(c) − 2σ2δ(τ ) ,

(6)

where we deﬁne

V (c) = V (c; c0) = −

1
2

c2 + g2fΦ(c, c0) − g2fΦ(0, c0) .
(7)

with Φ(x) = R x

0 φ(y) dy and ∂/∂c fΦ(c, c0) = fφ(c, c0) fol-
lowing from Price’s theorem [24]. The potential (7) de-
pends on the initial value c0, which has to be determined
self-consistently. We obtain c0 from classical energy con-
servation 1
2 ˙c2 + V (c) = constant. Considering τ ≥ 0 and
the symmetry of c(τ ), the noise term in (6) amounts to
an initial velocity ˙c(0+) = −σ2 and thus to the kinetic
energy 1
2 ˙c2(0+) = σ4/2. Considering the shape of the
potential (Figure 1a,b) and that |c(τ )| ≤ c0, the solu-
tion c(τ ) and its ﬁrst derivative must approach zero as
τ → ∞. Thus we obtain the self-consistency condition
for c0 as

1
2

σ4 + V (c0; c0) = V (0; c0) = 0.

(8)

For the noiseless case Figure 1a,c shows the resulting po-
tential and the corresponding self-consistent autocorrela-
tion function c(τ ), whose decay is equivalent to chaotic
dynamics [1]. Approaching the transition from above,
g → gc = 1, the decay time diverges due to the transition
from chaos to a ﬁxed point. This picture breaks down in
the stochastic system (Figure 1 b,d), where c(τ ) is always
decaying and has a kink at zero. The analytical predic-
tions are in excellent agreement with the population av-
eraged autocorrelation function obtained from numerical
simulations showing that the self-averaging property is
fulﬁlled. In the following we will derive a condition for
the transition from regular to chaotic dynamics in the
noisy case.

The maximum Lyapunov exponent quantiﬁes how sen-
sitive the dynamics depends on the initial conditions [25].
It measures the asymptotic growth rate of inﬁnitesimal
perturbations. For stochastic dynamics the stability of
the solution for a ﬁxed realization of the noise is also
characterized by the maximum Lyapunov exponent [26]:
If it is negative, trajectories with diﬀerent initial condi-
tions converge to the same time-dependent solution, i.e.,
the dynamics is stable. If it is positive, the distance be-
tween two initially arbitrary close trajectories grows ex-
ponentially in time, i.e., the dynamics exhibits sensitive
dependence on the initial conditions and is hence chaotic.
The linear stability of the dynamical system (1) is an-

alyzed via the variational equation

d
dt

yi(t) = −yi(t) +

N

Xj=1

Jijφ′(xj (t)) yj(t) ,

(9)

i = 1, . . . , N, describing the temporal evolution of an
inﬁnitesimal deviation yi(t) about a reference trajectory
xi(t). The maximum Lyapunov exponent can then be

3

(a)

0

)
τ
(

W

-0.2

0.15

)
τ
(
2
ψ

0

(b)

0

-0.3

0.15

0

−30

0
τ

30

−30

0
τ

30

Figure 2: Solution of Schrödinger equation. Upper part
of vertical axis: Quantum potential W (solid line) and ground
state energy E0 (dashed line) for noise free case (a) and noisy
case (b). Lower part of vertical axis: Corresponding ground
state wave functions. Parameters and color code same as in
Figure 1 (noisy case for g = 0 left out here).

deﬁned as

λmax = lim
t→∞

1
2t

ln

N

Xi=1

y2
i (t) ,

(10)

if a generic initial perturbation yi(0) with PN

is chosen.

i=1 y2

i (0) = 1

Typically, Lyapunov exponents cannot be calculated
analytically. However, for large network size N one
can obtain a mean-ﬁeld description of the variational
equation (9) and introduce a dynamical mean-ﬁeld vari-
able y(t).
Self-averaging suggests that the squared
Euclidean norm appearing in the deﬁnition of the
maximum Lyapunov exponent (10) can be approxi-
mated as PN
i (t) ≈ N K(t, t). Here, K(t, s) =
hy(t)y(s)i is the mean-ﬁeld autocorrelation function,
which obeys the linear partial diﬀerential equation
(∂t + 1) (∂s + 1) K(t, s) = g2fφ′(c(t − s), c0) K(t, s). A
separation ansatz in the coordinates τ = t − s and
T = t + s then yields an eigenvalue problem in the form
of a time-independent Schrödinger equation [1, 16]

i=1 y2

(cid:2)−∂2

τ − V ′′(c(τ ))(cid:3) ψ(τ ) = E ψ(τ ),

(11)

where now τ plays the role of a spatial coordinate. The
ground state energy E0 of (11) determines the asymp-
totic growth rate of K(t, t) as t → ∞ and, hence, the
maximum Lyapunov exponent via λmax = −1+√1 − E0.
Therefore, the dynamics is predicted to become chaotic
if E0 < 0. The negative second derivative of the clas-
sical potential V (c) evaluated along the self-consistent
autocorrelation function c(τ ) yields the quantum poten-
tial W (τ ) = −V ′′(c(τ )) = 1− g2fφ′(c(τ ), c0). It is shown
together with the ground state energy and wave function
in Figure 2. The latter are obtained as solutions of a
ﬁnite diﬀerence discretization of (11).

For the noiseless case Sompolinsky et al. [1] showed
that a decaying autocorrelation function corresponds to
a positive maximum Lyapunov exponent. This follows
from the observation that for g > 1 the derivative of the
self-consistent autocorrelation function ˙c(τ ) solves the
Schrödinger equation with E = 0. Because ˙c(τ ) is an
eigenfunction with a single node, there exists a ground
state with energy E0 < 0. Hence, the dynamics is chaotic
and the maximum Lyapunov exponent crosses zero at
g = 1 (Figure 3a).

In the presence of noise the maximum Lyapunov ex-
ponent becomes positive at a critical coupling strength
gc > 1; depending on the noise intensity the transition
is shifted to larger values (Figure 3a). The mean-ﬁeld

prediction λmax = −1 + √1 − E0 shows excellent agree-

ment with the maximum Lyapunov exponents obtained
in simulations using a standard algorithm [25]. Since the
ground state energy E0 must be larger than the mini-
mum W (0) = 1 − g2h[φ′(x)]2i of the quantum potential,
an upper bound for λmax is provided by −1+gph[φ′(x)]2i
leading to a necessary condition for a positive maximum
Interestingly, gph[φ′(x)]2i is also
Lyapunov exponent.
the spectral radius of the Jacobian matrix (shifted by
one) in (9) as estimated by random matrix theory [13, 14].
Therefore, the dynamics are expected to become locally
unstable if this spectral radius exceeds unity. However,
close to the critical coupling gc, the maximum Lyapunov
exponent is clearly smaller than the upper bound, which
is a good approximation only for small g (Figure 3a, in-
set). This conﬁrms the condition being only necessary;
the actual transition occurs at substantially larger cou-
pling strengths (Figure 3b).

We now set out to derive the exact transition curve,
i.e., the curve (gc, σc) in parameter space at which the
system becomes chaotic. To this end we determine a
ground state with vanishing energy E0 = 0. As in the
noiseless case, ˙c(τ ) solves (11) for E = 0, except at τ = 0
where it exhibits a jump caused by the noise. However,
due to linearity | ˙c(τ )| would be a continuous and symmet-
ric solution with zero nodes. Therefore, if its derivative
is continuous as well, requiring ¨c(0+) = 0, it constitutes
the searched for ground state. This is in contrast to the
noiseless case, where ˙c(τ ) corresponds to the ﬁrst excited
state. Consequently, with (4) we ﬁnd the transition con-
dition

g2
chφ2(x)i − c0 = 0 ,

(12)

in which c0 is determined by the self-consistency con-
dition (8) resulting in the transition curve (gc, σc)
(Figure 3b).

At the transition the classical self-consistent potential
has a horizontal tangent at c0, while in the chaotic regime
a minimum emerges (Figure 1b). This implies that the
curvature ¨c(0+) of the autocorrelation function at zero
changes sign from positive to negative (Figure 1d). Fur-
thermore, according to (12) the system becomes chaotic

(a)

0

x
a
m
λ

−1

1

0

0
E

4

(b)

2

g

chaos

0

-0.3

1

1
0.0

(c)

10

2

∞
τ

0.5
σ

1.0

0

1

2

3

g

0

0

1

2

3

g

Figure 3: Transition to chaos.
(a) Upper part of ver-
tical axis: Maximal Lyapunov exponent λmax as a func-
tion of the coupling strength g for diﬀerent noise levels
encoded in grayscale from black to light gray with σ ∈
[0, √0.0125, √0.125, √0.25]. Numerical solution (solid line)
and simulation (diamonds). Network size in simulations is
N = 5000. Comparison to upper bound for maximum Lya-

punov exponent −1 + gph[φ′(x)]2i (dashed) for σ = √0.25 in

inset. Zero crossings marked with dot and diamond. Lower
part of vertical axis: Ground state energy E0 as function of
g. (b) Phase diagram with transition curve (solid red curve)
obtained from (12) and necessary condition g2hφ′
(x)i > 1
(gray dashed curve). Dot and diamond correspond to zero
crossings in inset in (a). (c) Asymptotic decay time constant
of autocorrelation function. Vertical dashed lines mark the
transition to chaos. Color code as in (a).

2

precisely when the variance c0 = hx2i of a typical single
unit equals the variance g2hφ2(x)i of the recurrent input.
Close to the transition a standard perturbative approach
shows that λmax is proportional to g2hφ2(x)i−c0, indicat-
ing a self-stabilizing eﬀect: Since both terms grow with
increasing coupling g, the growth of their diﬀerence is at-
tenuated, qualitatively explaining the reduction of slope
of the maximum Lyapunov exponent in the vicinity of
the transition (Figure 3a).

The condition (12) predicts the transition to happen
at signiﬁcantly larger coupling strengths compared to the

necessary condition gph[φ′(x)]2i > 1 (Figure 3b). This

is surprising as the necessary condition also implies lo-
cally unstable dynamics, as noted above. This diﬀer-
ence is explained in an illustrative manner, considering
that the eﬀect of noise is twofold in the continuous-time
system. First, because φ′(x) is maximal at the origin,
noise reduces the averaged squared slope in g2h[φ′(x)]2i,
thereby stabilizing the dynamics. We interpret this as an
essentially static eﬀect since this can be fully attributed
to the increase of the instantaneous variance c0 due to
the noise. However, in addition noise reduces the decay
time of the autocorrelation function (Figure 1c,d), which
sharpens the quantum potential (Figure 2). This shifts
the ground-state energy to larger values resulting in a

further decreased Lyapunov exponent. Thus we identify
a second stabilization mechanism, which depends on the
shape of the autocorrelation function and thus consti-
tutes a dynamical eﬀect.

Analytical results for the transition to chaos in stochas-
tic networks so far only exist for time-discrete dynamics
[19]. Their mean-ﬁeld theory corresponds to static solu-
tions of the time-continuous system [1, 21]. The absence
of temporal correlations greatly simpliﬁes the analysis,
reducing the order parameter to a single real number,
the variance of the mean-ﬁeld. The necessary condition
in the time-continuous stochastic system found here is
therefore also suﬃcient for the transition to chaos in those
systems [19, their eq. 13].

Finally, we consider the eﬀect of the noise on the

= p1 − ghφ′(x)i2 of the au-

asymptotic decay time τ−1
∞
tocorrelation function (Figure 3d). For low noise the de-
cay constant shows a sharp peak at the transition, re-
ﬂecting the diverging time scale in the noise-free case.
For larger noise amplitudes this peak is strongly sup-
pressed. Remarkably, the decay time does not have its
maximum at the transition, but rather peaks at larger
coupling values. This observations suggest that the ab-
sence of a diverging time scale in spiking neural networks
[17] is due to the spiking noise.

This work was partially supported by Helmholtz young
investigator’s group VH-NG-1028, Helmholtz portfolio
theme SMHB, Jülich Aachen Research Alliance (JARA),
EU Grant 269921 (BrainScaleS), and EU Grant 604102
(Human Brain Project, HBP).

[1] H. Sompolinsky, A. Crisanti, and H. J. Sommers, Phys.

Rev. Lett. 61, 259 (1988).

[2] C. van Vreeswijk and H. Sompolinsky, Science 274, 1724

(1996).

[3] M. Monteforte and F. Wolf, Phys. Rev. Lett. 105, 268104

(2010).

5

[4] W. Maass, T. Natschläger, and H. Markram, Neural

Comput. 14, 2531 (2002).

[5] H. Jaeger and H. Haas, Science 304, 78 (2004).
[6] R. Legenstein and W. Maass, Neural Networks 20, 323

(2007).

[7] D. Sussillo and L. F. Abbott, Neuron 63, 544 (2009).
[8] T. Toyoizumi and L. Abbott, Phys. Rev. E 84, 051908

(2011).

[9] H. Sompolinsky and A. Zippelius, Phys. Rev. B 25, 6860

(1982).

[10] K. Rajan, L. Abbott, and H. Sompolinsky, Phys. Rev. E

82, 011903 (2010).

[11] J. Aljadeﬀ, M. Stern, and T. Sharpee, Phys. Rev. Lett.

114, 088101 (2015).

[12] O. Harish and D. Hansel, PLoS Comput Biol 11,

e1004266 (2015).

[13] H. Sommers, A. Crisanti, H. Sompolinsky, and Y. Stein,

Phys. Rev. Lett. 60, 1895 (1988).

[14] K. Rajan and L. Abbott, Phys. Rev. Lett. 97, 188104

(2006).

[15] S. Ostojic, Nat. Neurosci. 17, 594 (2014).
[16] J. Kadmon and H. Sompolinsky, Phys. Rev. X 5, 041030

(2015).

[17] R. Engelken, F. Farkhooi, D. Hansel, C. van Vreeswijk,

and F. Wolf, bioRxiv p. 017798 (2015).

[18] S. Ostojic, bioRxiv p. 020354 (2015).
[19] L. Molgedey, J. Schuchhardt, and H. Schuster, Phys. Rev.

E 69, 3717 (1992).

[20] K. Fischer and J. Hertz, Spin glasses (Cambridge Uni-

versity Press, 1991).

[21] S.-I. Amari, Systems, Man and Cybernetics, IEEE Trans-

actions on pp. 643–657 (1972).

[22] A. Crisanti and H. Sompolinsky, Phys. Rev. A 36, 4922

(1987).

[23] T. Cabana and J. Touboul, J. statist. Phys. 153, 211

(2013).

[24] A. Papoulis, Probability, Random Variables, and Stochas-
(McGraw-Hill, Boston, Massachusetts,

tic Processes
1991), 3rd ed.

[25] J.-P. Eckmann and D. Ruelle, Reviews of modern physics

57, 617 (1985).

[26] G. Lajoie, K. K. Lin, and E. Shea-Brown, Phys. Rev. E

87, 052901 (2013).

