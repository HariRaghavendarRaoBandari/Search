Strong Converse Exponent for State Dependent

Channels With Full State Information at the Sender

and Partial State Information at the Receiver

Yasutada Oohama

University of Electro-Communications, Tokyo, Japan

Email: oohama@uec.ac.jp

6
1
0
2

 
r
a

M
 
1
2

 
 
]
T
I
.
s
c
[
 
 

1
v
4
4
3
6
0

.

3
0
6
1
:
v
i
X
r
a

Abstract—We consider the state dependent channels with full
state information with at the sender and partial state information
at the receiver. For this state dependent channel, the channel
capacity under rate constraint on the state information at the
decoder was determined by Steinberg. In this paper, we study
the correct probability of decoding at rates above the capacity.
We prove that when the transmission rate is above the capacity
this probability goes to zero exponentially and derive an explicit
lower bound of this exponent function.

Index Terms—State dependent channels, strong converse the-

orem, exponent of correct probability of decoding

I. CODING PROBLEM FOR STATE DEPENDENT CHANNELS
We consider the classical problem of channel coding with
noncausal state information at the encoder, also known as the
Gel’fand-Pinsker problem. In this problem, we would like to
send a uniformly distributed message over a state-dependent
channel W n : X n × Sn, where S, X and Y, respectively, are
the state, input and output alphabets. We assume that X , Y,
S are ﬁnite sets. The state-dependent channel(SDC) we study
in this paper is deﬁned by a stationary discrete memoryless
channel speciﬁed by the following stochastic matrix:

W

△
= {W (y|x, s)}(s,x,y)∈S×X ×Y.

(1)

Let X n be a random variable taking values in X n. We write
an element of X n as xn = x1x2· · · xn. Suppose that X n
has a probability distribution on X n denoted by pX n =
{pX n(xn)}xn∈X n. Similar notations are adopted for other
random variables. Let Y n ∈ Y n be a random variable obtained
as the channel output by connecting X n to the input of channel
under the random state Sn. We write a conditional distribution
of Y n on Y n given X n and Sn as

W n = {W n(yn|xn, sn)}(sn,xn,yn)∈S n×X n×Y n .

Since the channel is memoryless, we have

n

W n(yn|xn, sn) =

W (yt|xt, st).

(2)

Yt=1

We assume that the state information of Sn is an output of a
stationary discrete memoryless source {St}t=1 speciﬁed by a
probability distribution pS = {pS(s)}s∈S on S. Transmission
of messages via the state dependent channel is shown in Fig.
1. The random variable Kn is a message sent to the receiver.
The random variable Sn represent a random state. Under Sn, a

sender transforms Kn into a transmitted sequence X n using an
encoder function ϕ(n) and sends it to the receiver. In this paper
we consider the case where the receiver is provided with a rate
limited state information. In this case encoded data φ(n)(Sn)
of the random state information Sn is available at the decoder.
Here φ(n) is an encoder function of the state information a
formal deﬁnition of which is deﬁned by

φ(n) : Sn → Mn = {1, 2, · · · |Mn|}.

Set Mn = φ(n)(Sn). In this paper we assume that the encoder
function ϕ(n) is a stochastic encoder. In this case, ϕ(n) is a
stochastic matrix given by

ϕ(n) = {ϕ(n)(xn|k, sn)}(k,sn,xn)∈Kn×S n×X n,

where ϕ(n)(xn|k, sn) is a conditional probability of xn ∈ X n
given k ∈ Kn and non-causal random state sn ∈ Sn. The joint
probability mass function on Kn × Mn ×Sn ×X n ×Y n is
given by

Pr{(Kn, Mn, St, X n, Y n) = (k, m, xn, yn, zn)}

=

1

|Kn|

ϕ(n)(xn|k, sn)pMnSn (m, sn)

W (yt |xt, st )

where |Kn| is a cardinality of the set Kn. The decoding
function at the receiver 1 is denoted by ψ(n). Those functions
are formally deﬁned by

ψ(n) : Y n × Mn → Kn.

The average error probability of decoding on the receiver is
deﬁned by

P(n)

e = P(n)

e

(ϕ(n), φ(n), ψ(n))

△
= Pr{ψ(n)(Y n, Mn) 6= Kn}

For k ∈ Kn and m ∈ Mn, set

D(k|m)

△
= {yn : ψ(n)

1

(yn) = (k, m)}.

A family of sets {D(k|m)}(k,m)∈Kn×Mn is called the decod-
ing region. Using the decoding region, P(n)
can be written
as

e

P(n)

e =

1

|Kn| X(k,l)∈Kn×Mn

X

(sn,xn,yn)∈S n×X n×Y n:

yn∈Dc(k|m)

×ϕ(n)(xn|k, sn)W n(yn|xn, sn)pMn,Sn(m, sn).

n

Yt=1

S n

n

X

S

S n

n

W

Kn

n
( )

φ

ϕ
n

Y

( )n Mn

^
n
( ) Kn

ψ

Sender

Receiver 

Fig. 1. Coding for state dependent channels with rate limited side Information
at the receiver

We can show that the above functions and sets satisfy the
following property.

Property 1:
a) The region C(W ) is a closed convex subset of R2

+, where

R2
+

△
= {(Rd, R) : Rd ≥ 0, R ≥ 0}.

b) The region C(W ) can be expressed with two families of
supporting hyperplanes. To describe this result we deﬁne
the set of probability distribution p = pUV SXY of (U, V,
S, X, Y ) ∈ U ×V ×S ×X ×Y by

Psh(W )

Set
c = P(n)

P(n)

c

(ϕ(n), φ(n), ψ(n))

△
= 1 − P(n)

e

(ϕ(n), φ(n), ψ(n)).

△
= {p : |V| ≤ min{|S||X |, |S| + |Y| − 1},

The quantity P(n)
decoding. This quantity has the following form:

is called the average correct probability of

c

P(n)

c =

1

|Kn| X(k,m)∈Kn×Mn

X

(sn,xn,yn)∈S n×X n×Y n:

yn∈D(k|m)

×ϕ(n)(xn|k, sn)W n(yn|xn, sn)pMn,Sn (m, sn).

For ﬁxed ε ∈ [0, 1), a pair (Rd, R) is ε-achievable if there
n=1 such that
exists a sequence of triples {(ϕ(n), φ(n), ψ(n))}∞

|U| ≤ min{|V||S||X |, |S| + |Y| − 1},
pY |XS = W, (U, V ) ↔ (S, X) ↔ Y }.

Furthermore, we deﬁne the set of probability distribution
q = qUV SXY of (U, V, S, X, Y ) ∈ U ×V ×S ×X ×Y
by

Q

△
= {q = qUV SXY : |U|, |V| ≤ |S| + |Y| − 1}.

P(n)

e ≤ ε,

We set

lim sup
n→∞
1
n

lim inf
n→∞

log |Kn| ≥ R, lim sup
n→∞

1
n

log |Mn| ≤ Rd.

The set that consists of all achievable rate pair is denoted
by C(ε|W ), which is called the capacity region of the state
dependent channels. Furthermore, set

C(ε, Rd|W )

△
=

max

(Rd,R)∈C(ε|W )

R.

It is obvious that the determination problem of C(ε|W ) is
equivalent to that of C(ε, Rd|W ) for ﬁxed Rd > 0.

To describe previous works on C(ε|W ), we introduce a
pair of auxiliary random variables (U, V ) taking values in
a ﬁnite set U ×V. We assume that the joint distribution of
(U, V, S, X, Y ) is

pUV XY Z(u, v, s, x, y)

= pUV (u, v)pSX|UV (s, x|u, v)W (y|x, s).

The above condition is equivalent to (U, V ) ↔ (X, S) ↔ Y .
Deﬁne the set of probability distribution p = pUV SXY of (U,
V, S, X, Y ) ∈ U ×V ×S ×X ×Y by

P(W )

△
= {p : |V| ≤ |S||X | + 1, |U| ≤ |V||S||X |,

pY |XS = W, (U, V ) ↔ (S, X) ↔ Y }.

Set

C(p)

△
= {(Rd, R) : R, Rd ≥ 0 ,

Rd ≥ Ip(V ; S) − Ip(V ; Y ),
R ≤ Ip(U ; Y |V ) − Ip(U ; S|V )},

C(W ) = [p∈P(W )

C(p).

C(µ)(W )

△
= max

p∈Psh(W )

{Ip(U ; Y |V ) − Ip(U ; S|V )

−µ[Ip(V ; S) − Ip(V ; Y )]},

˜C(α,µ)(W )

△
= max

q∈Q (cid:8)−αD(qY |XSUV ||W |qXSUV )

+Iq(U ; Y |V ) − Iq(U ; S|V )
−µ[Iq(V ; S) − Iq(V ; Y )]},

{(Rd, R) : R − µRd ≤ C(µ)(W )},

Csh(W )

˜Csh(W )

△

= \µ>0
= \α,µ>0

△

Then, we have the following:

{(Rd, R) : R − µRd ≤ ˜C(α,µ)(W )}.

C(W ) = Csh(W ) = ˜Csh(W ).

Property 1 part a) is a well known result. Proof of Property
1 part a) is omitted. Proof of Property 1 part b) is given in
the Appendix B. Typical shape of the region C(W ) is shown
in Fig. 2.

Coding problem in the case of Rd = 0 is called Gelfand
and Pinsker problem, which was posed and investigated by
Gelfand and Pinsker [1]. They determined C(0, 0|W ). Their
result is the following:

Theorem 1 (Gelfand and Pinsker [1]): For any state de-

pendent channel W ,

C(0, 0|W ) =

max
pU X|S :

|U |≤|S||X |+1

{I(U ; Y ) − I(U ; S)}.

Strong converse theorem is proved by Tyagi and Narayan

[2]. Their result is the following:

R

(

I

YX ;

|S

)

max
pX |S

G (

Rd,

R

|W

)

R

[
max I
pUX |S

(
U;

Y

)

(
I U;

S

)
]

0

Rd

H (

S |Y

)

Fig. 2. Shape of C(W ). In this ﬁgure C(Rd|W ) is deﬁned by C(Rd|W )
max{R : (Rd, R) ∈ C(W )}.

0

Rd

Fig. 3. Typical shape of G(Rd, R|W ).

R
d

By time sharing we have that {G(n)(Rd, R|W )}n≥1 satisﬁes
the following subadditivity property:

△
=

Theorem 2 (Tyagi and Narayan [2]): For each ε ∈ [0, 1),

and for any state dependent channel W , we have

Hence we have

C(ε, 0|W ) = C(0, 0|W ).

lim
n→∞

G(n)(Rd, R|W ) = inf
n≥1

G(n)(Rd, R|W ).

G(n+m)(Rd, R|W )
nG(n)(Rd, R|W ) + mG(m)(Rd, R|W )

.

≤

n + m

C(0|W ) ⊇ C(W ).

≤

To prove this theorem they used a method of image size

characterization introduced by Csisz´ar and K¨orner [3].

On the determination problem of C(0|W ) posed and inves-
tigated by Heegard and El Gamal [4], they proved that C(W )
serves as an inner bound of the capacity region C(0|W ). That
is, we have the following:

Theorem 3 (Heegard and El Gamal [4]): For any state

dependent channel W , we have

Subsequently, Steinberg [5] proved that the inner bound

C(W ) is tight, thereby establishing the following theorem:

Theorem 4 (Steinberg [5]): For any state dependent chan-

nel W , we have

C(0|W ) = C(W ).

In this paper we shall prove that

the strong converse
theorem holds for the state dependent channel with full state
information and partial state information at the encoder, i.e.,
we have C(0|W ) = C(W ) for each ε ∈ [0, 1).

Capacity theorems for the state dependent channel in the
case of general noisy channels was obtained by Tan [6]. To
derive those capacity results he used the information spectrum
method introduced by Han [7].

To examine an asymptotic behavior of P(n)

c

for rates outside

the capacity region C(W ) we deﬁne the following quantity.

G(n)(Rd, R|W )
△
=

min

(ϕ(n),φ(n),ψ(n)):

(1/n) log |Mn|≤Rd,

(1/n) log |Kn|≥R

(cid:18)−

1

n(cid:19) log P(n)

c

(ϕ(n), φ(n), ψ(n)).

Set

G(Rd, R|W ) = inf
n≥1

G(n)(Rd, R|W ),

R(W )

△
= {(Rd, R, G) : G ≥ G(Rd, R|W )}.

The exponent function G(Rd, R|W ) is a convex function of
(R, Rd). In fact, by time sharing we have that
nR + mR′

G(n+m)(cid:18) nRd + mR′

n + m

d

,

nG(n)(Rd, R|W ) + mG(m)(R′

W(cid:19)

n + m (cid:12)(cid:12)(cid:12)(cid:12)

d, R′|W )

,

n + m

from which we have that for any α ∈ [0, 1]

G(αRd + ¯αR′

d, αR + ¯αR′|W )

≤ αG(Rd, R|W ) + ¯αG(R′

d, R′|W ).

The region R(W ) is also a closed convex set. Shape of
G(Rd, R|W ) is shown in Fig. 3. Our main aim is to ﬁnd
an explicit characterization of R(W ). In this paper we derive
an explicit outer bound of R (W ) whose section by the plane
G = 0 coincides with C(W ).

II. MAIN RESULT

In this section we state our main result. Deﬁne

ω(α,µ)

q

(s, x, y|u, v)

△
= α log

+ log

W (y|x, s)

qY |XSUV (y|x, s, u, v)
qY |UV (y|u, v)qS|V (s|v)
qS|UV (y|u, v)qY |V (y|v)

Λ(α,µ,λ)

q

(SXY |U V )

− µ log

qS|V (s|v)qY (y)
qY |V (y|v)qS(s)

,

△

= X(u,v,s,x,y)

∈U ×V×S×X ×Y

qUV SXY (u, v, s, x, y)

× expnλω(α,µ)

△
= log Λ(α,µ,λ)

(s, x, y|u, v)o ,

(SXY |U V ),

q

q

Ω(α,µ,λ)

q

(SXY |U V )

Ω(α,µ,λ)(W )

△
= max
q∈Q

F (α,µ,λ)(Rd, R|q)

Ω(α,µ,λ)

q

(SXY |U V ),

△
=

λ(cid:0)R − µRd(cid:1) − Ω(α,µ,λ)

1 + λ(4 + α + 3µ)

q

F (α,µ,λ)(Rd, R|W )

(SXY |U V )

III. PROOF OF THE MAIN RESULTS

We ﬁrst prove the following lemma.
Lemma 1: For any η > 0 and for any (ϕ(n), φ(n), ψ(n))

satisfying

1
n

log |Kn| ≥ R,

1
n

log |Mn| ≤ Rd

,

we have

P(n)

c

(ϕ(n), φ(n), ψ(n)) ≤ pKnMnSnX nY n(cid:26)

W n(Y n|X n, Sn)

△
= min
q∈Q

F (α,µ,λ)(Rd, R|q) =

λ(cid:0)R − µRd(cid:1) − Ω(α,µ,λ)(W )

1 + λ(4 + α + 3µ)

,

F (Rd, R|W )

△
= sup

α,µ,λ>0

F (α,µ,λ)(Rd, R|W ),

R(W )

△
= {(Rd, R, G) : G ≥ F (Rd, R|W )} .

0 ≤

log

1
n

0 ≤

1
n

log

We can show that the above functions and sets satisfy the

following property.

R ≤

1
n

log

(Y n|X n, Sn, Kn, Mn)

q(i)
Y n|X nSnKnMn
pSn|KnMn (Sn|Kn, Mn)
q(ii)
(Sn|Kn, Mn)
Sn|KnMn
pY n|KnMn (Y n|Kn, Mn)

q(iii)
Y n|Mn

(Y n|Mn)

+ η,

+ η,

+ η, (5)

(6)

(7)

(8)

Property 2:
a) Ω(α,µ,λ)
b) For every q ∈ Q, we have

q

(SXY |U V ) is a convex function of λ > 0.

Ω(α,µ,λ)

q

(SXY |U V )

λ

lim
λ→+0

= −αD(qY |XSUV ||W |qXSUV )
+Iq(U ; Y |V ) − Iq(U ; S|V )
−µ[Iq(V ; S) − Iq(V ; Y )].

c) If (Rd, R) /∈ C(W ), then we have F (Rd, R|W ) > 0.
Proof of Property 2 is given in Appendix C. Our main

result is the following.

Theorem 5: For any state dependent channel W , we have

G(Rd, R|W ) ≥ F (Rd, R|W ),

R(W ) ⊆ R(W ).

(3)
(4)

Proof of this theorem will be given in Section III. It follows
from Theorem 5 and Property 2 part c) that
if (Rd, R)
is outside the capacity region, then the error probability of
decoding goes to one exponentially and its exponent is not
below F (Rd, R|W ).

From this theorem we immediately follows from the fol-

lowing corollary:

Corollary 1: For each ε ∈ [0, 1), we have

C(ε|W ) = C(0|W ).

Outline of the proof of Theorem 5 will be given in the next
section. The exponent function at rates outside the channel
capacity was derived by Arimoto [8] and Dueck and K¨orner
[9]. The techniques used by them are not useful to prove
Theorem 5. Some novel techniques based on the information
spectrum method introduced by Han [7] are necessary to prove
this theorem.

Rd ≥

1
n

log

q(iv)
Sn|Mn

(Sn|Mn)

pSn(Sn)

− η(cid:27) + 4e−nη.

In (5), we can choose any conditional distribution q(i)
Y n|X n
SnKnMn on Y n given (X n, Sn, Kn, Mn). In (6), we can
choose any conditional distribution q(ii)
on Sn given
(Kn,Mn). In (7), we can choose any conditional distribution
q(iii)
on Y n given Mn. In (8) we can choose any distribu-
Y n|Mn
tion q(iv)

Sn|M n on Sn given Mn.

Proof of this lemma is given in Appendix D. From this

Sn|Kn,Mn

lemma we immediately obtain the following lemma.

Lemma 2: For any η > 0 and for any (ϕ(n), φ(n), ψ(n))

satisfying

we have

1
n

log |Kn| ≥ R,

1
n

log |Mn| ≤ Rd,

(Y n|X n, Sn, Kn, Mn)

+ η, (9)

(Sn|Mn)

Sn|Mn

Y n|Mn

(Y n|Mn)

P(n)

c

(ϕ(n), φ(n), ψ(n)) ≤ pKnMnSnX nY n(cid:26)

W n(Y n|X n, Sn)

0 ≤

log

1
n

R ≤

1
n

log

+

+

1
n

1
n

Rd ≥

1
n

+

1
n

log

log

log

log

(Y n|Kn, Mn)q(iii)
(Sn|Kn, Mn)q(iii)

q(i)
Y n|X nSnKnMn
q(ii)
Y n|KnMn
q(ii)
Sn|KnMn
pY n|KnMn (Y n|Kn, Mn)
q(ii)
(Y n|Kn, Mn)
Y n|KnMn
pSn|Mn (Sn|Mn)
q(iii)
(Sn|Mn)
Sn|Mn
(Sn|Mn)q(v)
q(iv)
Sn|Mn
(Y n|Mn)q(vi)
q(iv)
Y n|Mn
Sn (Sn)q(iv)
q(vi)

Y n (Y n)
Sn (Sn)
(Y n|Mn)

+ 2η,

Y n|Mn
pSn(Sn)q(v)

Y n (Y n)

− η(cid:27) + 4e−nη.(14)

(10)

(11)

(12)

(13)

, q(ii)

Y n|KnMn

Y n|X nSnKnMn

In (9), the choice of q(i)
is the same as (5) in
Lemma 1. In (10) and (11), we can choose any pair of
(q(ii)
). In (10) and (12), we can choose any
pair of (q(iii)
). In (13) we can choose any pair
Sn|Mn
Y n|Mn
, q(iv)
of (q(iv)
). In (13) and (14), we can choose any
Sn|Mn
distribution q(v)
Y n on Y n. In (13) and (14), we can choose any
distribution q(vi)
Sn on Sn.

Sn|KnMn
, q(iii)

Proof: From Lemma 1, we have the following chain of

Y n|Mn

inequalities:

P(n)

c

(ϕ(n), φ(n), ψ(n)) ≤ pKnMnSnX nY n(cid:26)

W n(Y n|X n, Sn)

0 ≤

log

1
n

R ≤

1
n

log

+2η,

(Y n|X n, Sn, Kn, Mn)

q(i)
Y n|X nSnKnMn
pY n|KnMn (Y n|Kn, Mn)pSn|Mn (Sn|Mn)
q(ii)
(Y n|Mn)
Sn|KnMn

(Sn|Kn, Mn)q(iii)

Y n|Mn

+ η,

W n(Y n|X n, Sn)

(Y n|X n, Sn, Kn, Mn)

+ η,

(Sn|Mn)

Sn|Mn

Y n|Mn

(Y n|Mn)

Rd ≥

1
n

log

q(iv)
Sn|Mn

(Sn|Mn)

pSn (Sn)

− η(cid:27) + 4e−nη

= pKnMnSnX nY n(cid:26)

0 ≤

log

1
n

(Y n|Kn, Mn)q(iii)
(Sn|Kn, Mn)q(iii)

q(i)
Y n|X nSnKnMn
q(ii)
Y n|KnMn
q(ii)
Sn|KnMn
pY n|KnMn (Y n|Kn, Mn)
q(ii)
(Y n|Kn, Mn)
Y n|KnMn
pSn|Mn (Sn|Mn)
q(iii)
(Sn|Mn)
Sn|Mn
(Sn|Mn)q(v)
q(iv)
Sn|Mn
(Y n|Mn)q(vi)
q(iv)
Y n|Mn
Sn (Sn)q(iv)
q(vi)

Y n(Y n)
Sn (Sn)
(Y n|Mn)

+ 2η,

Y n|Mn
pSn (Sn)q(v)

Y n (Y n)

R ≤

1
n

log

+

+

1
n

1
n

Rd ≥

1
n

+

1
n

log

log

log

log

− η(cid:27) + 4e−nη,

completing the proof.

For t = 1, 2, · · · , n, set

Ut

Vt

vt
ˆVt

ˆvt
ˇVt

ˇvt

t+1) ∈ Vt,

△
△
= Kn ∈ Ut,
= Kn, Ut
△
= Mn × Y t−1 × Sn
△
= (m, yt−1, sn
△
= Mn × Y t−1, ˆVt
△
= (m, yt−1) ∈ ˆVt,
△
t+1, ˇVt
= Mn × Sn
△
t+1) ∈ ˇVt.
= (m, sn

△
= (Mn, Y t−1) ∈ ˆVt,

△
= (Mn, Sn

t+1) ∈ ˇVt,

From Lemma 2, we have the following.

Lemma 3: For any η > 0 and for any (ϕ(n), φ(n), ψ(n))

satisfying

we have

1
n

log |Kn| ≥ R,

1
n

log |Mn| ≤ Rd,

(Yt|Xt, St, Ut, Vt)

+ η,

(St|Vt)

(Yt|Vt)

P(n)

c

0 ≤

R ≤

Rd ≥

n

n

log

n

n

log

log

1
n

1
n

W (Yt|Xt, St)

(Yt|Ut, Vt)q(iii)
St|Vt
(St|Ut, Vt)q(iii)
Yt|Vt

(ϕ(n), φ(n), ψ(n)) ≤ pKnMnSnX nY n(cid:26)
Xt=1
Xt=1
Xt=1
Xt=1
Xt=1
Xt=1

q(i)
Yt|XtStUtVt
q(ii)
Yt|UtVt
q(ii)
St|UtVt
pYt|Ut ˆVt
q(ii)
Yt|Ut ˆVt
pSt| ˇVt
q(iii)
St| ˇVt
q(iv)
St|Vt
q(iv)
Yt|Vt
q(vi)
St

(St| ˇVt)
(St| ˇVt)
(St|Vt)q(v)
Yt
(Yt|Vt)q(vi)
(St)
St
(St)q(iv)
(Yt| ˆVt)
Yt| ˆVt
pSt (St)q(v)
Yt

(Yt|Ut ˆVt)
(Yt|Ut ˆVt)

+ 2η,

(Yt)

(Yt)

1
n

1
n

1
n

1
n

log

+

+

log

log

n

n

+

− η(cid:27) + 4e−nη,(15)

where for each t = 1, 2, · · · , n, the following probability and
conditional probability distributions:
, q(ii)
, q(iv)
Yt|Vt

, q(ii)
, q(iii)
St| ˇVt

St|UtVt
q(iv)
,
St|Vt

, q(iv)
Yt| ˆVt

, q(ii)

Yt|Ut ˆVt

Yt|UtVt

(16)

,

q(i)
Yt|XtStUtVt
, q(iii)
q(iii)
St|Vt
Yt|Vt
q(v)
Yt

, q(vi)
St

appearing in the ﬁrst term in the right members of (15) have
a property that we can choose their values arbitrary.

Proof: On the probability distributions appearing in the right
members of (14), we take the following choices. In (9), we
choose qY n|X nSnKnMn so that

q(i)
Y n|X nSnKnMn
n

(Y n|X n, Sn, Kn, Mn)




q(i)
Yt|X tY t−1Sn

t+1KnMn

(Yt|Xt, Y t−1, Sn

t+1, Kn, Mn)

q(i)
Yt|X tUtVt

(Yt|Xt, St, Ut, Vt).

(17)

=

=

n

Yt=1
Yt=1

q(ii)
Y n|KnMn
q(ii)
Sn|KnMn

(Y n|Kn, Mn)

(Sn|Kn, Mn)

=

=

n

Yt=1
Yt=1

n

q(ii)
Yt|Y t−1Sn
q(ii)
St|Y t−1Sn
q(ii)
Yt|Ut,Vt
q(ii)
St|Ut,Vt

t+1KnMn

t+1KnMn

(Yt|Ut, Vt)

(St|Ut, Vt)

(Yt|Y t−1, Sn

t+1, Kn, Mn)

(St|Y t−1, Sn

t+1, Kn, Mn)

.

(18)

t+1, Vt

△
= (Mn, Y t−1, Sn

t+1) ∈ Vt,

In (10), we have the following identity:

(Yt|Y t−1, Sn

t+1, Mn)

(St|Y t−1, Sn

t+1, Mn)

+

1
n

n

Xt=1

log

q(vi)
St

(St)q(iv)
Yt| ˆVt
pSt (St)q(v)
Yt

(Yt| ˆVt)

(Yt)

− η(cid:27) + 4e−nη,

completing the proof.

For each t = 1, 2, · · · , n, let Q(Ut ×Vt ×S× X × Y) be a

set of all probability distributions on

.

(19)

Ut × Vt × S × X × Y = Kn × Mn × Sn−t+1 × X × Y t

In (13), we have the following identity:

(Y n|Mn)

(Sn|Mn)

n

q(iii)
Y n|Mn
q(iii)
Sn|Mn
q(iii)
Yt|Y t−1Sn
q(iii)
St|Y t−1Sn
q(iii)
Yt|Vt
q(iii)
St|Vt

Yt=1
Yt=1

n

(Yt|Vt)

(St|Vt)

t+1Mn

t+1Mn

=

=

(Y n|Mn)

(Sn|Mn)

n

q(iv)
Y n|Mn
q(iv)
Sn|Mn
q(iv)
Yt|Y t−1Sn
q(iv)
St|Y t−1Sn
q(iv)
Yt|Vt
q(iv)
St|Vt

Yt=1
Yt=1

n

(Yt|Vt)

(St|Vt)

t+1Mn

t+1Mn

=

=

(Yt|Y t−1, Sn

t+1, Mn)

(St|Y t−1, Sn

t+1, Mn)

.

(20)

For t = 1, 2, · · · , n, we simply write Qt=Q(Ut ×Vt ×S ×
X × Y). Similarly, for t = 1, 2, · · · , n, we simply write
qt =qUtVtStXtYt ∈ Qt. Set
n

n

Qt =

Q(Ut × Vt × S × X × Y),

Qn △
=

Yt=1

Yt=1

qn △

= {qt}n

t=1 ∈ Qn.

From Lemma 3, we immediately obtain the following lemma.
Lemma 4: For any η > 0 and for any (ϕ(n), φ(n), ψ(n))

satisfying

we have

1
n

log |Kn| ≥ R,

1
n

log |Mn| ≤ Rd,

+ η,

(St|Vt)
(Yt|Vt)

n

n

n

+

log

log

log

1
n

1
n

1
n

W (Yt|Xt, St)

qYt|XtStUtVt (Yt|Xt, St, Ut, Vt)
qYt|UtVt (Yt|Ut, Vt)qSt|Vt
qSt|UtVt
(St|Ut, Vt)qYt|Vt

(ϕ(n), φ(n), ψ(n)) ≤ pKnMnSnX nY n(cid:26)
Xt=1
Xt=1
Xt=1
Xt=1
Xt=1
Xt=1

pYt|Ut ˆVt
q
Yt|Ut ˆVt
pSt| ˇVt(St| ˇVt)
(St| ˇVt)
qSt| ˇVt
(St|Vt)qYt
qSt|Vt
qYt|Vt
(Yt|Vt)qSt
qSt

(Yt)
(St)
(Yt| ˆVt)
(Yt)

(Yt|Ut ˆVt)
(Yt|Ut ˆVt)

Yt| ˆVt
pSt(St)qYt

(St)q

+ 2η,

1
n

1
n

1
n

log

log

log

+

+

n

n

n

− η(cid:27) + 4e−nη, (24)

where for each t = 1, 2, · · · , n, the following probability and
conditional probability distributions:

qYt|XtStUtVt , qYt|Ut ˆVt
qYt|UtVt , qYt|Vt , qYt| ˆVt
qSt|UtVt, qSt|Vt, qSt| ˇVt

,
, qYt ,
, qSt




(25)

appearing in the ﬁrst term in the right members of (24) are
chosen so that they are induced by the joint distribution qt =
qUtVtStXtYt ∈ Qt.

To evaluate an upper bound of (24) in Lemma 4. We use
the following lemma, which is well known as the Cram`er’s
bound in the large deviation principle.

P(n)

c

0 ≤

R ≤

(21)

(22)

q(vi)
St

(St). (23)

Rd ≥

In (11), we have the following:

q(ii)
Y n|KnMn
n

(Y n|Kn, Mn)

q(ii)
Yt|Y t−1KnMn

(Yt|Y t−1, Kn, Mn)

q(ii)
Yt|Ut ˆVt

(Yt|Ut, ˆVt).

=

=

n

Yt=1
Yt=1

In (12), we have the following:

q(iii)
Sn|Mn

(Sn|Mn) =

(St| ˇVt).

n

q(iii)
St| ˇVt

Yt=1
Y n and q(vi)

In (13) and (14), we choose q(v)

Sn so that

From Lemma 2 and (17)-(23), we have

P(n)

c

0 ≤

R ≤

n

n

q(v)
Y n(Y n) =

(Yt|Xt, St, Ut, Vt)

n

n

log

log

1
n

1
n

q(v)
Yt

Yt=1

(Yt), q(vi)

W (Yt|Xt, St)

Sn (Sn) =

Yt=1
(ϕ(n), φ(n), ψ(n)) ≤ pKnMnSnX nY n(cid:26)
Xt=1
Xt=1
Xt=1
Xt=1
Xt=1

q(i)
Yt|XtStUtVt
q(ii)
Yt|UtVt
q(ii)
St|UtVt
pYt|Ut ˆVt
q(ii)
Yt|Ut ˆVt
pSt| ˇVt (St| ˇVt)
q(iii)
(St| ˇVt)
St| ˇVt
(St|Vt)q(v)
q(iv)
Yt
St|Vt
(Yt|Vt)q(vi)
q(iv)
St
Yt|Vt

(Yt|Ut, Vt)q(iii)
St|Vt
(St|Ut, Vt)q(iii)
Yt|Vt

(Yt|Ut ˆVt)
(Yt|Ut ˆVt)

+ 2η,

(St)

(Yt)

1
n

1
n

1
n

log

log

log

+

+

n

n

n

(St|Vt)

(Yt|Vt)

Rd ≥

+ η,

Lemma 5: For any real valued random variable A and any

we have

θ > 0, we have

Pr{A ≥ a} ≤ exp [− (θa − log E[exp(θA)])] .

Here we deﬁne a quantity which serves as an exponential
upper bound of P(n)
). Let P (n)(W ) be a set
of all probability distributions pKnMnSnX nY n on Kn ×Mn
×Sn ×X n ×Y n having the form:

(ϕ(n), ψ(n)

, ψ(n)

1

2

c

pKnM nSnX nY n (k, m, sn, xn, yn) = pKn (k)pMnSn (m, sn)

n

pXt|KnX t−1 (xt|k, xt−1, sn)W (yt|xt, st).

×

Yt=1

For simplicity of notation we use the notation p(n)
for
pKnMnSnX nY n ∈ P (n) (W ). We assume that pUtVtStXtYt =
t XtY t is a marginal distribution induced by p(n). For
pKnMnSn
t = 1, 2, · · · , n, we simply write pt = pUtVtStXtYt. For p(n)
∈ P (n)(W ) and qn ∈ Qn, we deﬁne
Ω(α,µ,θ)
p(n)||qn (SnX nY n|KnMn)

W αθ(Yt|Xt)

(Yt|Xt, St, Ut, Vt))

Yt|Vt

St|Vt

(St|Vt)

qαθ
Yt|XtStUtVt
(Yt|Ut, Vt)qθ
(Yt|Ut, Vt)qθ
(Yt|Ut ˆVt)

(Yt|Vt))
(Yt|Ut ˆVt)
( n
Yt=1

(Yt))

pθ
St| ˇVt
qθ
St| ˇVt

qµθ
St
qµθ
Yt

(St)

(Yt|Vt)

(St|Vt)

(St| ˇVt)

(St| ˇVt))

n

△

= log Ep(n)"( n
Yt=1
×( n
Yt=1
×
Yt=1

×( n
Yt=1
×( n
Yt=1

qθ
Yt|UtVt
qθ
St|UtVt
pθ
Yt|Ut ˆVt
qθ
Yt|Ut ˆVt
qµθ
Yt|Vt
qµθ
St|Vt
pµθ
St
qµθ
St

n

(St)

(St))
Yt=1


qµθ
Yt

qµθ
Yt| ˆVt


 ,

(Yt)

(Yt| ˆVt)




where for each t = 1, 2, · · · , n, the following probability and
conditional probability distributions:

(26)

,
qYt|XtStUtVt , qYt|Ut ˆVt
, qYt ,
qYt|UtVt , qYt|Vt , qYt| ˆVt
qSt|UtVt , qSt|Vt , qSt| ˇVt
, qSt
appearing in the deﬁnition of Ω(α,µ,θ)
p(n)||qn (SnX nY n|KnMn) are
chosen so that
they are induced by the joint distribution
qt = qUtVtXtYtSt ∈ Qt. Here we give a remark on an essential
difference between p(n) ∈ P (n)(W ) and qn ∈ Qn. For the
former the n probability distributions pt, t = 1, 2, · · · , n,
are consistent with p(n), since all of them are marginal
distributions of p(n). On the other hand, for the latter, qn
is just a sequence of n probability distributions. Hence, we
may not have the consistency between the n elements qt,
t = 1, 2, · · · , n, of qn.

By Lemmas 4 and 5, we have the following proposition.
Proposition 1: For any α, µ, θ > 0, any qn ∈ Qn, and any

(ϕ(n), φ(n), ψ(n)) satisfying

1
n

log |Kn| ≥ R,

1
n

log |Mn| ≤ Rd,

P(n)

c

(ϕ(n), φ(n), ψ(n))

≤ 5 exp(cid:26)−n [1 + θ(2 + α + µ)]−1
×(cid:20)θ(R − µRd) −

Ω(α,µ,θ)

1
n

p(n)||qn (SnX nY n|KnMn)(cid:21)(cid:27) .

Proof: We deﬁne three random variables Ai,i = 1, 2, 3 by

A1

△
=

A2

△
=

1
n

1
n

+

1
n

log

log

log

n

n

n

Xt=1
Xt=1
Xt=1
Xt=1
Xt=1
Xt=1

1
n

1
n

n

n

n

W (Yt|XtSt)

,

qYt|XtStUtVt(Yt|Xt, St, Ut, Vt)
(St|Vt)
qYt|UtVt (Yt|Ut, Vt)qSt|Vt
qSt|UtVt
(St|Ut, Vt)qYt|Vt
(Yt|Vt)
(Yt|Ut ˆVt)
pYt|Ut ˆVt
(Yt|Ut ˆVt)
q
Yt|Ut ˆVt
pSt| ˇVt(St| ˇVt)
(St| ˇVt)
qSt| ˇVt
(Yt|Vt)qSt
(St|Vt)qYt
pSt(St)qYt
(St)q

(Yt)
(Yt| ˆVt)

qYt|Vt
qSt|Vt

(St)
(Yt)

+ Rd.

− R,

qSt

Yt| ˆVt

+

log

A3

△
=

1
n

log

+

log

Then by Lemma 4, for any (ϕ(n), φ(n), ψ(n)) satisfying

1
n

log |Kn| ≥ R,

1
n

log |Mn| ≤ Rd,

we have

P(n)

c

(ϕ(n), φ(n), ψ(n))

= pKnMnSnX nY n {A1 ≥ −η, A2 ≥ −2η, A3 ≥ −η}

+4e−nη

≤ pKnMnSnX nY n {A ≥ a} + 4e−nη,

(27)

where we set

A

△
= αA1 + A2 + µA3, a

△
= −η[2 + α + µ].

Applying Lemma 5 to the ﬁrst term in the right member of
(27), we have

P(n)

c

(ϕ(n), φ(n), ψ(n))

≤ exp(cid:2)−(cid:0)nθa − log Ep(n)[exp(nθA)](cid:1)(cid:3) + 4e−nη
= exp(cid:20)n(cid:26)θ(2 + α + µ)η − θ(cid:2)R − µRd(cid:3)

+

1
n

Ω(α,µ,θ)(SnX nY n|KnMn)(cid:27)(cid:21) + 4e−nη.(28)

We choose η so that

− η = θ(2 + α + µ)η − θ(cid:2)R − µRd(cid:3)
Ω(α,µ,θ)
p(n)||qn (SnX nY n|KnMn).

+

1
n

(29)

For each t = 1, 2, · · · , n, we deﬁne a conditional probability
distribution of (X t, Y t) given (Kn, Sn) by

.

p(α,µ,θ;qt)
X tY t|KnSn

△

=np(α,µ,θ;qt)

X tY t|KnSn (xt, yt|k, sn)o(xt,yt,k,sn)∈X t×Y t×Kn×S n

p(α,µ,θ;qt)
X tY t|KnSn (xt, yt|k, sn)

,

△
= C−1

t

(k, sn)pX tY t|KnSn (xt, yt|k, sn)

t

f (α,µ,θ)
pi||qi

(si, xi, yi|ui, vi),

Solving (29) with respect to η, we have

η =

θ(cid:2)R − µRd(cid:3) −

1
n

Ω(α,µ,θ)
p(n)||qn(SnX nY n|KnMn)

1 + θ(2 + α + µ)

For this choice of η and (28), we have

P(n)

c

(ϕ(n), φ(n), ψ(n)) ≤ 5e−nη

= 5 exp(cid:20)−n[1 + θ(2 + α + µ)]−1
×(cid:26)θ(cid:2)R − µRd(cid:3) −

Ω(α,µ,θ)

1
n

completing the proof.

p(n)||qn (SnX nY n|KnMn)(cid:27)(cid:21),

Set

(α,µ,θ)

Ω

(W )

△
= sup
n≥1

max

p(n)∈P (n)(W )

min
qn∈Qn

1
n

Ω(α,µ,θ)
p(n)||qn (SnX nY n|KnMn).

Then we have the following corollary from Proposition 1.

Corollary 2: For any positive R, Rd and for any positive

α, µ, and θ, we have

G(Rd, R|W ) ≥

θ[R − µRd] − Ω

(α,µ,θ)

1 + θ(2 + α + µ)

(W )

.

Proof: By the deﬁnition of Ω

(α,µ,θ)

(W ), the deﬁnition of

G(n)( Rd, R|W ), and Proposition 1, we have

G(n)(Rd, R|W ) ≥

θ [R − µRd] − Ω

(α,µ,θ)

1 + θ(2 + α + µ)

(W )

−

1
n

log 5,

(α,µ,θ)

We shall call Ω

(W ) the communication potential. The
above corollary implies that the analysis of Ω
( W ) leads
to an establishment of a strong converse theorem for the state
depedent channels treated in this paper.

(α,µ,θ)

(α,µ,θ)

In the following argument we drive an explicit upper bound
of Ω
(W ). We use a new novel techique we call the
recursive method. The recursive method is a powerfull tool
to drive a single letterized exponent function for rates below
the rate distortion function. This method is also applicable
to prove the exponential strong converse theorem for other
network information theory problems [10], [11], [12].

For each t = 1, 2, · · · , n, deﬁne a function of (ut, vt,

st, xt, yt) ∈ Ut ×Vt ×S ×X ×Y by

f (α,µ,θ)
pt||qt

(st, xt, yt|ut, vt)
W αθ(yt|xt, st)

△
=

(yt|xt, st, ut, vt)

×

qαθ
Yt|XtStUtVt
qθ
Yt|UtVt
qθ
St|UtVt
pθ
Yt|Ut ˆVt
qθ
Yt|Ut ˆVt

×

(yt|ut, vt)qθ
(yt|ut, vt)qθ

(yt|ut, ˆvt)

(yt|ut, ˆvt)

(st|vt)
(yt|vt)

(st|ˇvt)

(st|ˇvt)

St|Vt

Yt|Vt
pθ
St| ˇVt
qθ
St| ˇVt

(yt|vt)

(st|vt)

(st)

(yt)

qµθ
St
qµθ
Yt
qµθ
Yt

(yt)

qµθ
Yt|Vt
qµθ
St|Vt
pµθ
St
qµθ
St

(st)

(st)

qµθ
Yt| ˆVt

(yt|ˆvt)

from which we have Corollary 2.

Furthermore, we have

×

where

Ct(k, sn)

Yi=1
= Xxt,yt

△

pX tY t|KnSn (xt, yt|k, sn)

t

f (α,µ,θ)
pi||qi

(si, xi, yi|ui, vi)

×

Yi=1

are constants for normalization. For t = 1, 2, · · · , n, deﬁne

Φ(α,µ,θ)

t,qt

(k, sn)

△
= Ct(k, sn)C−1

t−1(k, sn),

(30)

where we deﬁne C0(k, sn) = 1 for (k, sn) ∈ Kn ×Sn. Then
we have the following lemma.

Lemma 6: For each t = 1, 2, · · · , n, and for any (k, sn

xt, yt) ∈ Kn ×Sn ×X t ×Y t, we have

p(α,µ,θ;qt)
X tY t|KnSn (xt, yt|k, sn) = (Φ(α,µ,θ)
×p(α,µ,θ;qt−1)
X t−1Y t−1|KnSn (xt−1, yt−1|k, sn)
×f (α,µ,θ)

(st, xt, yt|ut, vt).

t,qt

pt||qt

(k, sn))−1

Φ(α,µ,θ)

t,qt

(k, sn)
p(α,µ,θ;qt−1)
X t−1Y t−1|KnSn (xt−1, yt−1|k, sn)

= Xxt,yt

×pXtYt|KnX t−1Sn (xt, yt|k, xt−1, yt−1, sn)
×f (α,µ,θ)

(st, xt, yt|ut, vt),

pt||qt

p(n)||qn (SnX nY n|KnMn)o

n

expnΩ(α,µ,θ)
= Xk,sn

pKnSn(k, sn)

Yt=1

Φ(α,µ,θ)

t,qt

(k, sn).

Proof of this lemma is given in Appendix E. Next we deﬁne
a probability distribution of random variable KnSn taking
values in Kn ×Sn by
p(α,µ,θ;qt)
KnSn

(k, sn)

= ˜C−1

t pKnSn (k, sn)

Φ(α,µ,θ)

i,qi

(k, sn),

(31)

t

Yi=1

where ˜Ct is a constant for normalization given by

.

pKnZn (k, sn)

˜Ct = Xk,sn

t

Yi=1

Φ(α,µ,θ)

i,qi

(k, sn).

For t = 1, 2, · · · , n, deﬁne

Furthermore, set

Λ(α,µ,θ)

t,qt

△
= ˜Ct ˜C−1
t−1,

(32)

where we deﬁne ˜C0 = 1. Then we have the following.

Lemma 7:

Ω(α,µ,θ)
p(n)||qn (SnX nY n|KnMn) =

Λ(α,µ,θ)

t,qt

log Λ(α,µ,θ)

t,qt

,

(33)

n

Xt=1

p(α,µ,θ;qt−1)
KnMnSn
= p(α,µ,θ;qt−1)
UtVtStXtYt

t XtY t (k, m, sn

t , xt, yt)

(ut, vt, st, xt, yt)

△

= Xxt−1 Xst−1:φ(n)(sn)=m

p(α,µ,θ;qt−1)
KnSn

(k, sn)

X t−1Y t−1|KnSn (xt−1, yt−1|k, sn)

×p(α,µ,θ;qt−1)
×pXtYt|X t−1Y t−1KnSn (xt, yt|xt−1, yt−1, k, sn). (39)

p(α,µ,θ;qt−1)
KnSn

(k, sn)Φ(α,µ,θ)

t,qt

(k, sn)

Then by Lemma 7, we have

p(α,µ,θ;qt−1)
KnSn

(k, sn)

Λ(α,µ,θ)

t,qt

= Xut,vt,st,xt,yt

= Xk,sn
= Xk,sn Xxt,yt

p(α,µ,θ;qt−1)
UtVtStXtYt

(ut, vt, st, xt, yt)

×f (α,µ,θ)

pt||qt

(st, xt, yt|ut, vt).

X t−1Y t−1|KnSn (xt−1, yt−1|k, sn)

×p(α,µ,θ;qt−1)
×pXtYt|X t−1Y t−1KnSn(xt, yt|xt−1, yt−1, k, sn)
×f (α,µ,θ)

(st, xt, yt|ut, vt).

pt||qt

(34)

Proof: By Lemma 6, we have

n

expnΩ(α,µ,θ)

p(n)||qn (SnX nY n|KnMn)o
Yt=1

˜Ct ˜C−1
t−1

Λ(α,µ,θ)

Yt=1

(a)
=

t,qt

n

.

= ˜Cn =

(35)

Step (a) follows from the deﬁnition (32) of Λ(α,µ,θ)
. From
(35), we have (33) in Lemma 7. We next prove (34) in Lemma
7. Multiplying Λ(α,µ,θ)
= ˜Ct/ ˜Ct−1 to both sides of (31), we
have

t,qt

t,qt

Λ(α,µ,θ)

t,qt

p(α,µ,θ;qt)
KnSn

(k, sn)

t

(36)

= ˜C−1

t−1pKnSn (k, sn)

Φ(α,µ,θ)

i,qi

(k, sn),

= p(α,µ,θ;qt−1)

KnSn

(k, sn)Φ(α,µ,θ)

t,qt

(k, sn).

(37)

Yi=1

For each t = 1, 2, · · · , n, we choose qt = qUtVtStXtYt so that
qUtVtStXtYt(ut, vt, st, xt, yt) = p(α,µ,θ;qt−1)
and choose the following probability and conditional proba-
bility distributions:

(ut, vt, st, xt, yt)

UtVtStXtYt

qYt|XtStUtVt , qYt|Ut ˆVt
, qYt ,
qYt|UtVt , qYt| ˆVt
qSt|UtVt , qSt| ˇVt
, qSt

,




appearing in
f (α,µ,θ)
pt||qt

=

(st, xt, yt|ut, vt)
W αθ(yt|xt, st)

(yt|xt, st, ut, vt)

×

qαθ
Yt|XtStUtVt
qθ
Yt|UtVt
qθ
St|UtVt
pθ
Yt|Ut ˆVt
qθ
Yt|Ut ˆVt

×

(yt|ut, vt)qθ
(yt|ut, vt)qθ

(yt|ut, ˆvt)

(yt|ut, ˆvt)

(st|vt)

(yt|vt)

(st|ˇvt)

(st|ˇvt)

St|Vt

Yt|Vt
pθ
St| ˇVt
qθ
St| ˇVt

(yt|vt)

(st|vt)

(st)

(yt)

qµθ
St
qµθ
Yt
qµθ
Yt

(yt)

qµθ
Yt|Vt
qµθ
St|Vt
pµθ
St
qµθ
St

(st)

(st)

qµθ
Yt| ˆVt

(yt|ˆvt)

Taking summations of (36) and (37) with respect to (k, sn),
we have (34) in Lemma 7.

The following proposition is a mathematical core to prove

such that they are the distributions induced by qUtVtStXtYt.
Then for each t = 1, 2, · · · , n, we have the following chain
of inequalities:

our main result.

Proposition 2: For θ ∈ (0, (2 + 2µ)−1), set

λ =

θ

1 − 2(1 + µ)θ

⇔ θ =

λ

1 + 2(1 + µ)λ

.

(38)

Then, for any positive α, µ, and any θ ∈ (0, (2 + 2µ)−1), we
have

(α,µ,θ)

Ω

(W ) ≤

Ω(α,µ,λ)(W )
1 + 2(1 + µ)λ

.

Proof: Set

ˆQn

△
= {q = qUV SXY : |U| ≤ |Kn|,

|V| ≤ |Mn||Sn−1||Y n−1|},

ˆΩ(α,µ,λ)

n

(W )

△
= min
q∈ ˆQn

Ω(α,µ,λ)

q

(SXY |U V ).

Λ(α,µ,θ)

t,qt

= Eqt"(

×

×

qθ
Yt|UtVt
qθ
St|UtVt
qµθ
Yt|Vt
qµθ
St|Vt

Yt|Ut
qθ
Yt|Ut

×( pθ
×( pµθ

St
qµθ
St

W αθ(Yt|Xt)

(Yt|Xt, St, Ut, Vt)

qαθ
Yt|XtStUtVt
(Yt|Ut, Vt)qθ
(Yt|Ut, Vt)qθ

(St|Vt)

(Yt|Vt)

St|Vt

Yt|Vt

(St)

(St|Vt)

(Yt|Vt)

qµθ
St
qµθ
Yt
(Yt|Ut ˆVt)

(Yt))
(Yt|Ut ˆVt))( pθ
(St))


qµθ
Yt| ˆVt

qµθ
Yt

(St)

St| ˇVt
qθ
St| ˇVt

(Yt)

(Yt| ˆVt)


(St| ˇVt)

(St| ˇVt))



(a)

≤  Eqt"(

W αθ(Yt|Xt)

(Yt|Xt, St, Ut, Vt)

qαθ
Yt|XtStUtVt
(Yt|Ut, Vt)qθ
(Yt|Ut, Vt)qθ

qθ
Yt|UtVt
qθ
St|UtVt

(St|Vt)

(Yt|Vt)

St|Vt

Yt|Vt

×

×

qµθ
Yt|Vt
qµθ
St|Vt

(Yt|Vt)qµθ
St
(St|Vt)qµθ
Yt

1

1−2(1+µ)θ

(St)

1−2(1+µ)θ




(Yt))
(Yt|Ut ˆVt)# Eqt" pSt| ˇVt(St| ˇVt)

(Yt|Ut ˆVt)

qSt| ˇVt

(St| ˇVt)#)θ

qYt|Ut ˆVt

×(Eqt" pYt|Ut ˆVt
qSt(St)(cid:21) Eqt"
×(Eqt(cid:20) pSt(St)
= exp(cid:26)[1 − 2(1 + µ)θ] Ω
= exp( Ω(α,µ,λ)
≤ exp( ˆΩ(α,µ,λ)

1 + 2(1 + µ)λ) (d)

1 + 2(1 + µ)λ

(W )

qt

n

(c)

(b)

(StXtYt|UtVt)

)

qYt(Yt)

(Yt| ˆVt)#)µθ

θ

1−2(1+µ)θ )

qYt| ˆVt
(α,µ,
qt

(StXtYt|UtVt)(cid:27)

= exp(cid:26) Ω(α,µ,λ)(W )

1 + 2(1 + µ)λ(cid:27) . (40)

n

Step (a) follows from H¨older’s inequality. Step (b) follows
from (38). Step (c) follows from qt ∈ ˆQn and the deﬁnition of
ˆΩ(α,µ,λ)
(W ). Step (d) follows from Lemma 10 in Appendix
A. To prove this lemma we bound the cardinalities |U| and
|V| appearing in the deﬁnition of ˆΩ(α,µ,λ)
(W ) to show that
the bounds |U|, |V| ≤ |S| +|Y| − 1 are sufﬁcient to describe
ˆΩ(α,µ,λ)

(W ). Hence we have the following:

n

n

min
qn∈Qn

1
n

Ω(α,µ,θ)
p(n)||qn(SnX nY n|KnMn)
Xt=1

Ω(α,µ,θ)
p(n)||qn (SnX nY n|KnMn)

(a)
=

1
n

n

1
n

≤

(b)
≤

Ω(α,µ,λ)(W )
1 + 2(1 + µ)λ

.

(41)

Step (a) follows from (33) in Lemma 7. Step (b) follows from
(40). Since (41) holds for any n ≥ 1 and any p(n) ∈ P (n)
(W ), we have

(α,µ,θ)

Ω

(W ) ≤

Ω(α,µ,λ)(W )
1 + 2(1 + µ)λ

,

completing the proof.

Proof of Theorem 5: For θ ∈ (0, (2 + 2µ)−1), set

θ

λ =

1 − 2(1 + µ)θ
Then we have the following:

⇔ θ =

λ

1 + 2(1 + µ)λ

.

(42)

G(Rd, R|W )

(a)
≥

(b)
≥

θ(R − µRd) − Ω

(α,µ,θ)

(W )

1 + θ(2 + α + µ)

λ(R − µRd) − Ω(α,µ,λ)(W )

1 + 2(1 + µ)λ

1 +

λ(2 + α + µ)
1 + 2(1 + µ)λ

=

λ(R − µRd) − Ω(α,µ,λ)(W )

1 + λ(4 + α + 3µ)

= F (α,µ,λ)(Rd, R|W ).

(43)

Step (a) follows from Corollary 2. Step (b) follows from
Proposition 2 and (42). Since (43) holds for any positive α,
µ, and λ, we have

G(Rd, R|W ) ≥ F (Rd, R|W ).

Thus (3) in Theorem 5 is proved. The inclusion R(W ) ⊆
R(W ) is obvious from this bound.

IV. CONCLUSIONS

We have dealt with the state dependent discrete memoryless
channels with full state information at the sender and partial
state information at the receiver. We have proved that for rates
outside the capacity region the correct probability of decoding
tends to zero exponentially and derived an explicit lower bound
of its exponent function.

A. Cardinality Bound on Auxiliary Random Variables

APPENDIX

Deﬁne

Q(W )

△
= {qUV SXY : |U|, |V| ≤ |S| + |Y| − 1,
qY |XS = W, (U, V ) ↔ (X, S) ↔ Y }.

We ﬁrst prove the following lemma.

Lemma 8:
(µ)

C

(W )

△
= max

p∈P(W )

{Ip(U ; Y |V ) − Ip(U ; S|V )

−µ[Ip(V ; S) − Ip(V ; Y )]}

Proof: We ﬁrst observe that

Ip(U ; Y |V ) − Ip(U ; S|V ) − µ[Ip(V ; S) − Ip(V ; Y )]

pV (v)ζ(α,µ,λ)

1

(pUSXY |V (·|v), pS, pY ),

(44)

= Xv∈V

where we set

ζ(α,µ,λ)
1

(qUSXY |V (·|v), pS, pY )

pUSXY |V (u, s, x, y|v)

△
=

X

(u,s,x,y)∈U ×S×X ×Y

× expnλω(α,µ)

p

(s, x, y|u, v)o .

For each v ∈ V, ζ(α,µ,λ)
uous function of pUSXY |V (·|v). We further observe that

(pUSXY |V (·|v), pS, pY ) is a contin-

1

pS(s) = Xv∈V
pY (y) = Xv∈V

qV (v)qS|V (s|v),

qV (v)qY |V (s|v).




(45)

log Λ(α,µ,θ)

t,qt

≤ ˆC(µ)(W )

△
= max

p∈Q(W )

{Ip(U ; Y |V ) − Ip(U ; S|V )

−µ[Ip(V ; S) − Ip(V ; Y )]}

Then by the support lemma,

|V| ≤ |S| + |Y| − 2 + 1 = |S| + |Y| − 1

(46)

is sufﬁcient to express |S| + |Y| − 2 values of (45) and one
value of (44). We next bound the cardinality of U on the
conditional distribution pU|V = {pU|V (u|v)}(u,v)∈U ×V. We
ﬁrst replace the set V in (58) and (57) with ˜V so that it satisﬁes
the cardinality bound | ˜V| ≤ |S| + |Y| − 1 and preserves the
values of the right members of (45) and (44). Then we have

Ip(U ; Y |V ) − Ip(U ; S|V ) − µ[Ip(V ; S) − Ip(V ; Y )]

2

pU (u)ζ(µ)

= Xu∈U
pV SX (v, s, x) = Xu∈U

where ζ(µ)
the support lemma,

2

is sufﬁcient to express one value of (52) and |S||X | − 1 values
of (51). Next we derive an upper bound of |U|. Observe that

Ip(U ; Y |V ) − Ip(U ; S|V ) − µ[Ip(V ; S) − Ip(V ; Y )]

(cid:0)pV SX , pV SX|U (·|u)(cid:1) ,

pU (u)pV SX|U (v, s, x|u),

(54)

(55)

is a continuous function of pV SX|U (·|u). Then by

pV (v)ζ(α,µ,λ)

1

(pUXY Z|V (·|v), pS, pY ).

(47)

|U| ≤ 1 + |V||S||X | − 1 = |V||S||X |

(56)

= Xv∈ ˜V

Furthermore for each v ∈ ˜V, ζ(µ)
can be written as

1

( pUSXY |V (·|v), pS, pY )

ζ(α,µ,λ)
1

×ζ(α,µ,λ)

2

(cid:0)pUSXY |V (·|v), pS, pY(cid:1) = Xu∈U
(cid:0)pSXY |UV (·|u, v), pS|V (·|v), pY |V (·|v), pS, qY(cid:1) .

qU|V (u|v)

(48)

Note that for each v ∈ ˜V, ζ(α,µ,λ)
qSXY |UV (·|u, v). For each v ∈ ˜V, we have

2

is a continuous function of

pS|V (s|v) = Xu∈U
pY |V (y|v) = Xu∈U

pU|V (u|v)qS|UV (s|u, v),

pU|V (u|v)qY |UV (y|u, v).

Then by the support lemma,




|U| ≤ |S| + |Y| − 2 + 1 = |S| + |Y| − 1

(50)

is sufﬁcient to express |S| + |Y| − 2 values of (62) and one
value of (48).

We next prove the following lemma.
Lemma 9:

ˆC(µ)(W )

△
= max

p∈Q(W )

{Ip(U ; Y |V ) − Ip(U ; S|V )

= C(µ)(W )

△
= max

p∈Psh(W )

−µ[Ip(V ; S) − Ip(V ; Y )]}

{Ip(U ; Y |V ) − Ip(U ; S|V )

−µ[Ip(V ; S) − Ip(V ; Y )]} .

Proof: We ﬁrst bound the cardinality |V| of V to show that
the bound |V| ≤ |S||X | is sufﬁcient to describe ˆC(µ)(W ).
Observe that

Ip(U ; Y |V ) − Ip(U ; S|V ) − µ[Ip(V ; S) − Ip(V ; Y )]

1

pV (v)ζ(µ)

= Xv∈V
pSX (s, x) = Xv∈V

(cid:0)pSX , pUSX|V (·|v)(cid:1) ,

pV (v)pSX|V (s, x|v),

(51)

(52)

is sufﬁcient to express one value of (54) and |V||S||X | − 1
values of (55).

Finally we prove the following lemma.
Lemma 10: For each integer n ≥ 2, we deﬁne

ˆΩ(α,µ,λ)

n

(W )

△
=

△
=

max

q=qU V SXY :

|V|≤|Mn||S|n−1|Y|n−1,

Ω(α,µ,λ)

q

(SXY |U V ),

|U |≤|Kn|

Ω(α,µ,λ)(W )

max

q=qU V SXY :

Ω(α,µ,λ)

q

(SXY |U V ).

|U |,|V|≤|S|+|Y|−1

(49)

Then we have

ˆΩ(α,µ,λ)(W ) = Ω(α,µ,λ)(W ).

Proof: We ﬁrst bound the cardinality |V| of V to show
that the bound |V| ≤ |S|+|Y| − 1 is sufﬁcient to describe
ˆΩ(α,µ,λ)

(W ). We ﬁrst observe that

n

Λ(α,µ,λ)

q

(SXY |U V )

qV (v)ζ(α,µ,λ)

3

(qUSXY |V (·|v), qS, qY ),

(57)

= Xv∈V

where we set

ζ(α,µ,λ)
3

(qUSXY |V (·|v), qS, qY )

qUSXY |V (u, s, x, y|v)

△
=

X

(u,s,x,y)∈U ×S×X ×Y

× expnλω(α,µ)

q

(s, x, y|u, v)o .

For each v ∈ V, ζ(α,µ,λ)
uous function of qUSXY |V (·|v). We further observe that

(qUSXY |V (·|v), qS, qY ) is a contin-

3

qS(s) = Xv∈V
qY (y) = Xv∈V

qV (v)qS|V (s|v),

qV (v)qY |V (s|v).

Then by the support lemma,

(58)




where ζ(µ)
the support lemma,

1

is a continuous function of pUSX|V (·|v). Then by

|V| ≤ |S| + |Y| − 2 + 1 = |S| + |Y| − 1

(59)

|V| ≤ 1 + |S||X | − 1 = |S||X |

is sufﬁcient to express one value of (57) and |S| + |Y| − 2
values of (58). We next bound the cardinality of U on the

(53)

conditional distribution qU|V = {qU|V (u|v)}(u,v)∈U ×V. We
ﬁrst replace the set V in (58) and (57) with ˜V so that it satisﬁes
the cardinality bound | ˜V| ≤ |S| + |Y| − 1 and preserves the
values of the right members of (58) and (57). Then we have

Λ(α,µ,λ)

q

(SXY |U V )

qV (v)ζ(α,µ,λ)

3

(qUXY Z|V (·|v), qS, qY ).

(60)

= Xv∈ ˜V

Furthermore for each v ∈ ˜V, ζ(µ)
be written as

3

( pUSXY |V (·|v), qS, qY ) can

ζ(α,µ,λ)
3

×ζ(α,µ,λ)

4

(cid:0)qUSXY |V (·|v), qS, qY(cid:1) = Xu∈U
(cid:0)qSXY |UV (·|u, v), qS|V (·|v), qY |V (·|v), qS, qY(cid:1) .

qU|V (u|v)

(61)

Note that for each v ∈ ˜V, ζ(α,µ,λ)
qSXY |UV (·|u, v). For each v ∈ ˜V, we have

4

is a continuous function of

qS|V (s|v) = Xu∈U
qY |V (y|v) = Xu∈U

qU|V (u|v)qS|UV (s|u, v),

qU|V (u|v)qY |UV (y|u, v).

Then by the support lemma,




|U| ≤ 1 + |S| + |Y| − 2 = |S| + |Y| − 1

(63)

is sufﬁcient to express one value of (61) and |S| + |Y| − 2
values of (62).

B. Proof of Property 1

In this appendix we prove Property 1. Property 1 part a) is
a well known property. Proof of this property is omitted here.
From Property 1 part a), we have the following lemma.

Lemma 11: Suppose that ( ˆRd, ˆR) does not belong to C(W ).
Then there exists ǫ, µ∗ > 0 such that for any (Rd, R) ∈ C(W )
we have

(R − ˆR) − µ∗(Rd − ˆRd) + ǫ ≤ 0.

Proof of this lemma is omitted here. Lemma 11 is equivalent
to the fact that if the region C(W ) is a convex set, then for any
point ( ˆRd, ˆR) outside C(W ) there exits a line which separates
the point ( ˆRd, ˆR) from the region C(W ). The line separating
the point ( ˆRd, ˆR) /∈ C(W ) from the region C(W ) is shown in
Fig. 4. Lemma 11 will be used to prove Property 1 part b).
To prove the parts b) and c),

Proof of Property 1 part b): We ﬁrst recall the following

deﬁnitions of P(W ) and Psh(W ):

P(W )

△
= {pUV SXY : |V| ≤ |S||X | + 1, |U| ≤ |V||S||X |,

pY |XS = W, (U, V ) ↔ (X, S) ↔ Y },

Psh(W )

△
= {pUV SXY : |V| ≤ min{|S||X |, |S| + |Y| − 1},

|U| ≤ min{|V||S||X |, |S| + |Y| − 1},
pY |XS = W, (U, V ) ↔ (S, X) ↔ Y }.

(62)

(R − ˆR) − µ∗(Rd − ˆRd) + ǫ ≤ 0.

R

^
R

0

R^R

=

μ*

(

Rd

^
Rd

)

ε

ε

ε

C (

)W

^
Rd

R
d

Fig. 4. An existence of a line that separates the point ( ˆRd, R) from the
region C(W ).

We ﬁrst prove Csh(W ) ⊆ C (W ). We assume that ( ˆRd, ˆR) /∈
C(W ). Then by Lemma 11, there exist ǫ, µ∗ > 0 such that for
any (Rd, R) ∈ C(W ) we have

Then we have

ˆR − µ∗ ˆRd

≥

max

(Rd,R)∈C(W )

{R − µRd} + ǫ

(a)
= max

p∈P(W )

{Ip(U ; Y |V ) − Ip(U ; S|V )

−µ[Ip(V ; S) − Ip(V ; Y )]} + ǫ

≥ max

p∈Psh(W )

{Ip(U ; Y |V ) − Ip(U ; S|V )

−µ[Ip(V ; S) − Ip(V ; Y )]} + ǫ

= C(µ∗)(W ) + ǫ.

(64)

Step (a) follows from the deﬁnition of C(W ). The bound (64)
implies that ( ˆRd, ˆR) /∈ Csh(W ). Thus Csh(W ) ⊆ C(W ) is
proved. We next prove C( W ) ⊆ ˜Csh(W ). We assume that
(Rd, R) ∈ C(W ). Then there exists p ∈ P (W ) such that

Rd ≥ Ip(V ; S) − Ip(V ; Y ),

R ≤ Ip(U ; Y |V ) − Ip(U ; S|V ).(cid:27)

(65)

Deﬁne

Q(W )

△
= {qUV SXY : |U|, |V| ≤ |S| + |Y| − 1,
qY |XS = W, (U, V ) ↔ (X, S) ↔ Y }.

Then, for (Rd, R) ∈ C(W ), we have the following chain of
inequalities:

R − µRd

(a)
≤ Ip(U ; Y |V ) − Ip(U ; S|V ) − µ[Ip(V ; S) − Ip(V ; Y )]
≤ max

{Ip(U ; Y |V ) − Ip(U ; S|V )
−µ[Ip(V ; S) − Ip(V ; Y )]}

p∈P(W )

(b)
≤ max

p∈Q(W )

{Ip(U ; Y |V ) − Ip(U ; S|V )
−µ[Ip(V ; S) − Ip(V ; Y )]}

By deﬁnition, we have ˆqα,µ ∈ Q(W ). Computing D(q∗
ˆqα,µ), we have the following:

α,µ||

(c)
= max

p∈Q(W )

≤ max
p∈Q

{−αD(pY |XSUV ||W |pXSUV )
+Ip(U ; Y |V ) − Ip(U ; S|V )
−µ[Ip(V ; S) − Ip(V ; Y )]}
{−αD(pY |XSUV ||W |pXSUV )
+Ip(U ; Y |V ) − Ip(U ; S|V )
−µ[Ip(V ; S) − Ip(V ; Y )]}

= ˜C(α,µ)(W ).

Step (a) follows from (65). Step (b) follows from Lemma 8
stated in Appendix A. Step (c) follows from that when p ∈
P(W ), we have

D(pY |XSUV ||W |pXSUV ) = D(pY |XS||W |pXSUV ) = 0.

Hence we have C(W ) ⊆ ˜Csh(W ). Finally we prove ˜Csh(W ) ⊆
Csh(W ). We assume that ( ˜Rd, ˜R) ∈ ˜Csh(W ). Then we have

˜R − µ ˜Rd ≤ ˜C(α,µ)(W )

= max
q∈Q

{−αD(qY |XSUV ||W |qXSUV )

D(q∗

α,µ||ˆqα,µ)

= D(q∗

Y |XSUV,α,µ||W |q∗

XSUV,α,µ)

(a)
≤

∆(µ)

α

.

(70)

Step (a) follows from (68). From (70), we have

lim
α→∞

D(q∗

α,µ||ˆqα,µ) = 0,

from which we obtain

q∗
α,µ → ˆqα,µ, α → ∞.

(71)

By (71) and the continuity of Iq(U ; Y |V ), Iq(U ; S|V ),
Iq(V ; S), and Iq(V ; Y ), with respect to the distribution q =
qUV SXY , we have that for any µ > 0 and any sufﬁciently
large α, we have

Iq∗
−µ[Iq∗

α,µ (U ; Y |V ) − Iq∗
α,µ (V ; S) − Iq∗

α,µ (U ; S|V )
α,µ (V ; Y )]
≤ Iˆqα,µ (U ; Y |V ) − Iˆqα,µ (U ; S|V )

+Iq(U ; Y |V ) − Iq(U ; S|V )
−µ[Iq(V ; S) − Iq(V ; Y )]}
Y |XSUV,α,µ||W |q∗

XSUV,α,µ)

= −αD(q∗

+Iq∗
−µ[Iq∗

α,µ (U ; Y |V ) − Iq∗
α,µ (V ; S) − Iq∗

α,µ (U ; S|V )
α,µ (V ; Y )]

−µ[Iˆqα,µ (V ; S) − Iˆqα,µ (V ; Y )] + τ (α, µ),

(72)

where τ (α, µ) is a positive number that satisﬁes

(66)

lim
α→∞

τ (α, µ) = 0.

α,µ = q∗

where q∗
UV SXY,α,µ ∈ Q is a probability distribution
which attains the maximum in the deﬁnition of ˜C(α,µ)(W ).
XSUV,α,µ appearing in the
The quantities q∗
ﬁrst term in the right members of (66) is the conditional
distributions induced by q∗

Y |XSUV,α,µ and q∗

α,µ. We set

∆(µ) △

= Iq∗

α,µ (U ; Y |V ) − Iq∗
α,µ (V ; S) − Iq∗

α,µ (U ; S|V )
α,µ (V ; Y )] − [ ˜R − µ ˜Rd]

−µ[Iq∗

From (66), we must have

0 ≤ αD(q∗

Y |XSUV,α,µ||W |q∗

XSUV,α,µ) ≤ ∆(µ),

for any α, µ > 0. From (67), we have

0 ≤ D(q∗

Y |XSUV,α,µ||W |q∗

XSUV,α,µ) ≤

∆(µ)

α

.

From (68), we have

˜R − µ ˜Rd ≤ Iq∗

α,µ (U ; Y |V ) − Iq∗
α,µ (V ; S) − Iq∗

α,µ (U ; S|V )
α,µ (V ; Y )]

−µ[Iq∗

(67)

(68)

(69)

for any α, µ > 0. Let ˆqα,µ = ˆqUV SXY,α,µ be a probability
disitribution with the form

Then we have the following chain of inequalties:

˜R1 − µ ˜Rd

(a)
≤ Iq∗

α,µ (U ; Y |V ) − Iq∗
α,µ (V ; S) − Iq∗

α,µ (U ; S|V )
α,µ (V ; Y )]

−µ[Iq∗

(b)
≤ Iˆqα,µ (U ; Y |V ) − Iˆqα,µ (U ; S|V )

−µ[Iˆqα,µ (V ; S) − Iˆqα,µ (V ; Y )] + τ (α, µ)

(c)
≤ max

{Iq(U ; Y |V ) − Iq(U ; S|V )

q∈Q(W )
−µ[Iq(V ; S) − Iq(V ; Y )]} + τ (α, µ)

(d)
= max

{Iq(U ; Y |V ) − Iq(U ; S|V )

q∈Psh(W )
−µ[Iq(V ; S) − Iq(V ; Y )]} + τ (α, µ)

= C(µ)(W ) + τ (α, µ).

(73)

Step (a) follows from (69). Step (b) follows from (72). Step (c)
follows from that ˆqα,µ ∈ P(pXY ). Step (d) follows from that
by Lemma 9 stated in Appendix A. Since in (73) the quantity
τ (α, µ) can be made arbitrary close to zero, we conclude that
( ˜Rd, ˜R) ∈ Csh (W ). Thus ˜Csh(W ) ⊆ Csh(W ) is proved.

C. Proof of Property 2

ˆqUV SXY,α,µ(u, v, s, x, y) = q∗

UV SX,α,µ(u, v, s, x)W (y|x, s).

In this appendix we prove Property 2.

Proof of Property 2: We ﬁrst prove parts a) and b). For

and for every q ∈ Q, we have

simplicity of notations, set

Ω(α∗,µ∗,λ)

q

(SXY |U V )

a

△
= (u, v, s, x, y), A
△
= U × V × S × X × Y,

△
= (U, V, S, X, Y ),

A
ω(α,µ,λ)

q

(s, x, y|u, v)

△
= ρ(a),
△
= ξ(λ).

Ω(α,µ,λ)

q

(SXY |U V )

Then we have

Ω(α,µ,λ)

q

qA(a)eλρ(a)
(U V SXY ) = ξ(λ) = log
Xa∈A
 .
qA(a)ρ(a)eλρ(a)

By simple computations we have

−1
qA(a)eλρ(a)
Xa

qA(a)eλρ(a)


−2

{ρ(a) − ρ(b)}2

qA(a)qA(b)

2

ξ′(λ) =
Xa
ξ′′(λ) =
Xa
×
 Xa,b∈A

eλ{ρ(a)+ρ(b)}
 .

(75)

From (75), it is obvious that ξ′′(λ) is nonnegative. Hence
Ωq(SXY |U V ) is a convex function of λ. From (74), we have

ξ′(0) =Xa

qA(a)ρ(a)

= −αD(qY |XSUV ||W |qXSUV )
+Iq(U ; Y |V ) − Iq(U ; S|V )
−µ[Iq(V ; S) − Iq(V ; Y )].

≤ λn−α∗D(qY |XSUV ||W |qXSUV )
2o.

+Iq(U ; Y |V ) − Iq(U ; S|V )
−µ∗[Iq(V ; S) − Iq(V ; Y )] +

ǫ

From (79), we have that for any λ ∈ (0, κ(ǫ)],

Ω(α∗,µ∗,λ)(W ) = max
q∈Q

Ω(α∗,µ∗,λ)

q

(SXY |U V )

≤ λ max

q∈Qn−α∗D(qY |XSUV ||W |qXSUV )
2o

+Iq(U ; Y |V ) − Iq(U ; S|V )
−µ∗[Iq(V ; S) − Iq(V ; Y )] +

ǫ

ǫ

= λh ˜C(α∗,µ∗)(W ) +

2i .

 ,(74)

Under (77) and (80), we have the following chain of inequal-
ities:

(79)

(80)

F (Rd, R|W )

= sup

α,µ,λ>0

F (α,µ,λ)(Rd, R|W )

F (α∗,µ∗,λ)(Rd, R|W )

λ(R − µ∗Rd) − Ω(α∗,µ∗,λ)(W )

1 + λ(4 + α∗ + 3µ∗)

λh(R − µ∗Rd) − ˜C(α∗,µ∗)(W ) −

1 + λ(4 + α∗ + 3µ∗)

ǫ

2i

≥ sup

λ∈(0,κ(ǫ)]

= sup

λ∈(0,κ(ǫ)]

(a)
≥ sup

λ∈(0,κ(ǫ)]

(b)
≥ sup

λ∈(0,κ(ǫ)]
1
2

·

(76)

=

1
2

·

λǫ

1 + λ(4 + α∗ + 3µ∗)
κ(ǫ)ǫ

> 0.

1 + κ(ǫ)(4 + α∗ + 3µ∗)

Hence we have the part b). Next we prove the part c). We
assume that (R, Rd, R) /∈ C(W ), then by Property 1 part c),
there exist positive α∗, µ∗, and ǫ such that

Step (a) follows from (80). Step (b) follows from (77).

D. Proof of Lemma 1

R − µ∗Rd ≥ ˜C(α∗,µ∗)(W ) + ǫ.

(77)

In this appendix we prove Lemma 1. For (k, m) ∈ Ln ×

Mn, set

Set

ζ(λ)

△
= Ω(α∗,µ∗,λ)

q

(SXY |U V )

+Iq(U ; Y |V ) − Iq(U ; S|V )

−λh−α∗D(qY |XSUV ||W |qXSUV )
2i.

−µ[Iq(V ; S) − Iq(V ; Y )] +

ǫ

Then we have the following:

ζ(0) = 0, ζ′(0) = −
ζ′′(λ) = ξ′′(λ) ≥ 0.

ǫ
2

< 0,

)

It follows from (78) that there exists κ(ǫ) > 0 such that we
have ζ(λ) ≤ 0 for λ ∈ (0, κ(ǫ)]. Hence for any λ ∈ (0, κ(ǫ)]

A1(k, m)

(sn, xn, yn) :

△

=


1
n

1
n

=

log

pY n|X nSnKnMn (yn|xn, sn, k, m)
q(i)
(yn|xn, sn, k, m)
Y n|X nSnKnMn

log

W (yn|xn, sn)

q(i)
Y n|X nSnKnMn

(yn|xn, sn, k, m)

(78)

˜A2(k, m)

sn :

△

=


1
n

log

pSn|KnMn (sn|k, m)
q(ii)
(sn|k, m)
Sn|KnMn

,

≥ −η


,

≥ −η


A2(k, m)

△
= ˜A2(k, m) × X n × Y n.

Furthermore, for (k, m) ∈ Kn × Mn, set

A3(k, m)

A4(k, m)

△
= {(sn, xn, yn) : pY n|KnMn (yn|k, m)

≥ |Kn|e−nηq(iii)

Y n|Mn

(yn|m)},

△

=(cid:26)(sn, xn, yn) : pSn (sn)

≥

e−nη
|Mn|

q(iv)
Y n|Mn

(yn|m)(cid:27),

A(k, m)

△
=

Ai(k, m).

4

\i=1

Proof of Lemma 1: We have the following:

P(n)

c = X(k,m)∈Kn×Mn

X(sn,xn,yn)∈A(k,m),

yn∈D(k|m),

1

×pKnMnSnX nY n (k, m, sn, xn, yn)

+ X(k,m)∈Kn×Mn

X(sn,xn,yn)∈Ac(k,m):

yn∈D(k|m),

1

×pKnMnSnX nY n (k, m, sn, xn, yn)
4

∆i,

≤

Xi=0

×pKnMnSnX nY n (k, m, sn, xn, yn),

△

= X(k,m)∈Kn×Mn
= X(k,m)∈Kn×Mn

△

1

X(sn,xn,yn)∈A(k,m)
X(sn,xn,yn)∈Ac

i (k,m)

1

×pKnMnSnX nY n (k, m, sn, xn, yn),
for i = 1, 2, 4,

where

∆0

∆i

∆3

△

= X(k,m)∈Kn×Mn

X(sn,xn,yn)∈Ac

yn∈D(k|m)

3(k,m),

1

×pKnMnSnX nY n (k, m, sn, xn, yn)

=

1

|Kn| X(k,m)∈Kn×Mn

X(sn,xn,yn)∈Ac

yn∈D(k|m)

3(k,m),

1

×ϕ(n)(xn|k, sn)W n(yn|xn, sn)pMnSn (m, sn).

By deﬁnition we have

∆0 = pKnMnSnX nY n


0 ≤

log

1
n
1
n

0 ≤

log

W n(Y n|X nSn)

+ η,

qY n|X nSnKnMn (Y n|X n, Sn, Kn, Mn)
pSn|KnMn (Sn|Kn, Mn)
q(ii)
(Sn|Kn, Mn)
Sn|KnMn

+ η,

− η


1
n

1
n

log |Kn| ≤ log

pY n|KnMn (Y n|Kn, Mn)

+ η,

log |Mn| ≥

1
n

log

q(iii)
Y n|Mn
q(iv)
Sn|Mn

(Y n|Mn)

(Sn|Mn)

pSn(Sn)

.

(81)

From (81), it follows that if (ϕ(n), φ(n), ψ(n)) satisﬁes

1
n

log |Kn| ≥ R,

1
n

log |Mn| ≤ Rd,

then the quantity ∆0 is upper bounded by the ﬁrst term in the
right members of (8) in Lemma 1. Hence it sufﬁces to show
∆i ≤ e−nη, i = 1, 2, 3, 4 to prove Lemma 1. We ﬁrst prove
∆i ≤ e−nη for i = 1, 2. We have the following chains of
inequalities:

∆1

pKnMnSnX nY n (k, m, sn, xn, yn)

∈Ac

1(k,m)

= X(k,m)∈Kn×Mn X(sn,xn,yn)
≤ e−nη X(k,m)∈Kn×Mn
× X(sn,xn,yn)

1

1(k,m)

∈Ac
≤ e−nη,

∆2

∈Ac

2(k,m)

= X(k,m)∈Kn×Mn X(sn,xn,yn)
= X(k,m)∈Kn×Mn Xsn∈ ˜Ac
≤ e−nη X(k,m)∈Kn×Mn
× Xsn∈ ˜Ac

q(ii)
Sn|KnMn

2(k,m)

1

2(k,m)

≤ e−nη.

q(i)
Y n|X nSnKnMn
×pX nSnKnMn (xn, sn, k, m)

(yn|xn, sn, k, m)

pKnMnSnX nY n (k, m, sn, xn, yn)

pKnMnSn (k, m, sn)

(sn|k, m)pKnMn (k, m)

Next, we prove ∆3 ≤ e−nη. We have the following chain of
inequalities:

∆3

= X(k,m)∈Kn×Mn

X(sn,xn,yn):

yn∈D(k|m),

1

pY n |Kn ,Mn (yn|k,m)<e−nη

(yn|m)
×pKnMnSnX nY n (k, m, sn, xn, yn)

×|Kn|q(iii)

Y n |Mn

=

1

|Kn| X(k,m)∈Kn×Mn

Xyn∈D(k|m),

pY n |Kn ,Mn (yn|k,m)

<e−nη|Kn|q(iii)

Y n|Mn

(yn|m)

1

×pMn (m)pY n|KnMn (yn|k, m)

≤ e−nη X(k,m)∈Kn×Mn Xyn∈D(k|m)

pMn (m)q(iii)

Y n|Mn

(yn|m)

= e−nη Xm∈Mn
≤ e−nη Xm∈Mn

pMn (m)q(iii)

Y n|Mn  [k∈Kn

pMn (m) = e−nη.

m!

D(k|m)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Finally, we prove ∆4 ≤ e−nη. We have the following chain
of inequalities:

∆4 = X(k,m)∈Kn×Mn X(sn,xn,yn):

pSn (sn)< e−nη
|Ln|
×q(iv)
(sn|m)

Sn|Mn

1

×pKnMnSnX nY n (k, m, sn, xn, yn)

pSn (sn)

= Xm∈Mn Xsn:φ(n)(sn)=m,

pSn (sn)< e−nη
|Mn|
×q(iv)
(sn|m)

Sn |Mn

q(iv)
Sn|Mn

(sn|m)

≤

≤

e−nη

|Mn| Xm∈Mn Xsn:φ(n)(sn)=m
|Mn| Xm∈Mn

1 = e−nη.

e−nη

Thus Lemma 1 is proved.

E. Proof of Lemma 6

In this appendix we prove Lemma 6.
Proof of Lemma 6: By the deﬁnition of p(α,µ,θ;qt)

X tY t|KnSn

(xt, yt|k, sn), for t = 1, 2, · · · , n, we have

p(α,µ,θ;qt)
X tY t|KnSn (xt, yt|k, sn)

= C−1
t

t

(k, sn)pX tY t|KnSn (xt, yt|k, sn)

f (α,µ,θ)
pi||qi

(si, xi, yi|ui, vi).

(82)

×

Yi=1

Then we have the following chain of equalities:

p(α,µ,θ;qt)
X tY t|KnSn (xt, yt|k, sn)

(a)
= C−1
t

t

(k, sn)pX tY t|KnSn (xt, yt|k, sn)

f (α,µ,θ)
pi||qi

(si, xi, yi|ui, vi)

×

Yi=1

= C−1
t
t−1

×

Yi=1

(k, sn)pX t−1Y t−1|KnSn (xt−1, yt−1|k, sn)

f (α,µ,θ)
pi||qi

(si, xi, yi|ui, vi)

(b)
=

pt||qt

×pXtYt|X t−1Y t−1KnSn (xt, yt|xt−1, yt−1, k, sn)
×f (α,µ,θ)
(st, xt, yt|ut, vt)
Ct−1(k, sn)
p(α,µ,θ;qt−1)
X t−1Y t−1|KnSn(xt−1, yt−1|k, sn)
Ct(k, sn)
×pXt|Yt|X t−1Y t−1KnSn(xt, yt|xt−1, yt−1, k, sn)
×f (α,µ,θ)
pt||qt
= (Φ(α,µ,θ)

(st, xt, yt|ut, vt)

(k, sn))−1

t,qt

X t−1Y t−1|KnSn (xt−1, yt−1|k, sn)

×p(α,µ,θ;qt−1)
×pXtYt|X t−1Y t−1KnSn(xt, yt|xt−1, yt−1, k, sn)
×f (α,µ,θ)

(st, xt, yt|ut, vt).

pt||qt

Steps (a) and (b) follow from (82). From (83), we have

Φ(α,µ,θ)

t,qt

(k, sn)p(α,µ,θ;qt)

X tY t|KnSn(xt, yt|k, sn)

= p(α,µ,θ;qt−1)

X t−1Y t−1|KnSn (xt−1, yt−1|k, sn)
×pXtYt|X t−1Y t−1KnSn (xt, yt|xt−1, yt−1, k, sn)
×f (α,µ,θ)

(st, xt, yt|ut, vt).

pt||qt

(83)

(84)

(85)

Taking summations of (84) and (85) with respect to xt, yt, we
obtain

Φ(α,µ,θ)

t,qt

(k, sn)

p(α,µ,θ;qt−1)
X t−1Y t−1|KnSn (xt−1, yt−1|k, sn)

= Xxt,yt

×pXtYt|X t−1Y t−1KnSn (xt, yt|xt−1, yt−1, k, sn)
×f (α,µ,θ)

(st, xt, yt|ut, vt),

pt||qt

completing the proof.

REFERENCES

[1] S. Gel’fand and M. Pinsker, “Coding for channels with random param-
eters”, Prob. of Control and Infomration Theory, vol.9, no.1, pp. 19-31,
1980.

[2] H. Tyagi and P. Narayan, “The Gel’fand Pinsker channel: Strong converse
and upper bound for the reliability function”, Proceedings of the IEEE
International Symposium on Information Theory, Soul, Korea, pp.1954-
1957, June 2009.

[3] I. Csisz´ar and J. K¨orner, Information Theory : Coding Theorems for

Discrete Memoryless Systems. Academic Press, New York, 1981.

[4] C. Heegard and A. El Gamal, “On the capacity of computer memory with
defect”, IEEE Trans. Inform Theory, vol. IT-29, no. 5, pp.731-739, Sept.
1983.

[5] Y. Steinberg, “Coding for channels with rate-limited side information at
the decoder and applications,” IEEE Trans. Inform. Theory, vol. 54, no.
9, pp. 4283-4295, Sept. 2008.

[6] V. Y. F. Tan, “A formula for the capacity of the general Gel’fand-
Pinsker channel,”Proceedings of the IEEE International Symposium on
Information Theory, Istanbul, Turkey, pp.2458-2462, July 2013.

[7] T. S. Han,

Information-Spectrum Methods

in Information Theory.
Springer-Verlag, Berlin, New York, 2002. The Japanese edition was
published by Baifukan-publisher, Tokyo, 1998.

[8] S. Arimoto, “On the converse to the coding theorem for discrete mem-
oryless channels,” IEEE Trans. Inform. Theory, vol. IT-19, no. 3, pp.
357-359, May 1973.

[9] G. Dueck and J. K¨orner, “Reliability function of a discrete memoryless
channel at rates above capacity,” IEEE Trans. Inform. Theory, vol. IT-25,
no. 1, pp. 82-85, Jan. 1979.

[10] Y. Oohama, “Exponent function for one helper source coding problem at
rates outside the rate region,” Proceedings of the 2015 IEEE International
Symposium on Information Theory, pp. 1575-1579, Hong Kong, China,
June 14-19, 2015.

[11] Y. Oohama, “Strong converse exponent for degraded broadcast channels
at rates outside the capacity region,” Proceedings of the 2015 IEEE
International Symposium on Information Theory, pp. 939 - 943, Hong
Kong, China, June 14-19, 2015.

[12] Y. Oohama, “Strong converse theorems for degraded broadcast channels
with feedback,” Proceedings of the 2015 IEEE International Symposium
on Information Theory, pp. 2510 - 2514, Hong Kong, China, June 14-19,
2015.

