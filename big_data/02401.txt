6
1
0
2

 
r
a

M
8

 

 
 
]

.

R
P
h
t
a
m

[
 
 

1
v
1
0
4
2
0

.

3
0
6
1
:
v
i
X
r
a

ON THE EXPECTATION OF OPERATOR NORMS OF RANDOM

MATRICES

OLIVIER GU´EDON, AICKE HINRICHS, ALEXANDER E. LITVAK,

AND JOSCHA PROCHNO

Abstract. We prove estimates for the expected value of operator norms of
Gaussian random matrices with independent and mean-zero entries, acting as
operators from ℓn

q , 1 ≤ p∗ ≤ 2 ≤ q ≤ ∞.

p∗ to ℓn

1. Introduction and main results

Random matrices and their spectra are under intensive study in Statistics since
the work of Wishart [27] on sample covariance matrices, in Numerical Analysis since
their introduction by von Neumann and Goldstine [24] in the 1940s, and in Physics
as a consequence of Wigner’s work [25, 26] since the 1950s. His Semicircle Law,
a fundamental theorem in the spectral theory of large random matrices describing
the limit of the empirical spectral measure for what is nowadays known as Wigner
matrices, is among the most celebrated results of the theory.

In Banach Space Theory and Asymptotic Geometric Analysis, random matrices
appeared already in the 70s (see e.g.
In [2], the authors obtained
asymptotic bounds for the expected value of the operator norm of a random matrix
B = (bij )m,n
2 to ℓm
q ,
2 ≤ q < ∞. To be more precise, they proved that

i,j=1 with independent mean-zero entries with |bij| ≤ 1 from ℓn

[2, 3, 10]).

2 → ℓm

E(cid:13)(cid:13)B : ℓn

q (cid:13)(cid:13) ≤ Cq · max{m1/q,√n},

where Cq depends only on q. This was then successfully used to characterize (p, q)-
absolutely summing operators on Hilbert spaces. Ever since, random matrices
are extensively studied and methods of Banach spaces have produced numerous
deep and new results. In particular, in many applications the spectral properties
of a Gaussian matrix, whose entries are independent identically distributed (i.i.d.)
standard Gaussian random variables, were used. Moreover, Seginer [22] proved that
for an m×n random matrix with i.i.d. symmetric random variables the expectation
of its spectral norm (that is, the operator norm from ℓn
2 ) is of the order of
the expectation of the largest Euclidean norm of its rows and columns. He also
proved an optimal result in the case of random matrices with entries εijaij where
εij are independent Rademacher random variables and aij are ﬁxed numbers. We
refer the interested reader to the survey [8, 7] and references therein.

2 to ℓm

It is natural to ask similar questions about general random matrices, in particular
about Gaussian matrices whose entries are still independent centered Gaussian

2000 Mathematics Subject Classiﬁcation. Primary 60B20; Secondary 46B09.
Key words and phrases. Gaussian random matrix, operator norm, moments of random vectors.
J.P. was supported in part by the Austrian Science Fund grant FWFM 1628000 “Local Theory

of Banach Spaces and Convex Geometry”.

1

2 OLIVIER GU ´EDON, AICKE HINRICHS, ALEXANDER E. LITVAK, AND JOSCHA PROCHNO

random variables, but with diﬀerent variances.
In this case, where we drop the
assumption of identical distributions, very little is known. It is conjectured that
the expected spectral norm of such a Gaussian matrix is as in Seginer’s result, that
is, of the order of the expectation of the largest Euclidean norm of its rows and
columns. A big step toward the solution was made by Lata la in [16], who proved a

bound involving fourth moments, which is of the right order max{√m,√n} in the

i.i.d. setting. On one hand, in view of the classical Bai-Yin theorem, the presence
of fourth moments is not surprising, on the other hand they are not needed if the
conjecture is true.

Later in [20], Riemer and Sch¨utt proved the conjecture up to a log n factor. The
two results are incomparable – depending on the choice of variances, one or another
gives a better bound. The Riemer-Sch¨utt estimate was used recently in [21].

Another big step toward the solution was made a short while ago by Bandeira

and Van Handel [1]. In particular, they proved that

(1.1)

E(cid:13)(cid:13)(aijgij) : ℓm

2 → ℓn

2(cid:13)(cid:13) ≤ C(cid:16)|||A||| +plog max(n, m) max

ij

|aij|(cid:17)

where |||A||| denotes the largest Euclidean norm of the rows and columns of (aij ),
C > 0 is a universal constant, and gij are independent standard Gaussian random
variables. This was further developed by Van Handel [23] to verify the conjecture
up to a log log n factor. In fact, more was proved in [23]. He computed precisely the
expectation of the largest Euclidean norm of the rows and columns using Gaussian
concentration. And, while the moment method is at the heart of the proofs in [22]
and [1], he proposed a very nice approach based on the comparison of Gaussian
processes to improve the result of Lata la. However, his approach is based on a trick
using symmetric matrices and this cannot be generalized to study other operator
norms of the matrix.

The purpose of this work is to provide some bounds for operator norms of such

matrices considered as operators from ℓm

p to ℓn
q .

In what follows, by gi, gij, i ≥ 1, j ≥ 1 we always denote independent standard
Gaussian random variables. Let n, m ∈ N and A = (aij )m,n
i,j=1 ∈ Rm×n. We write
i,j=1. For r ≥ 1, we denote by γr ≈ √r the Lr-norm of a
G = GA = (aij gij)m,n
standard Gaussian random variable. The notation f ≈ h means that there are two
absolute positive constants c and C (that is, independent of any parameters) such
that cf ≤ h ≤ Cf .

Our main result is the following theorem.

p∗ → ℓn

Theorem 1.1. For every 1 < p∗ ≤ 2 ≤ q < ∞ one has
E(cid:13)(cid:13)G : ℓm

q(cid:17)1/q
q(cid:13)(cid:13) ≤(cid:16)E(cid:13)(cid:13)G : ℓm
p∗ → ℓn
q(cid:13)(cid:13)
≤ C p5/q (log m)1/q(cid:20) γp max
i≤m k(aij )n
j≤n k(aij)m
i=1kq,

+ 21/q γq max

j=1kp + γq E max

i≤m
j≤n

|aij gij|(cid:21)

where C is a positive absolute constant.

We conjecture the following bound.

OPERATOR NORMS OF RANDOM MATRICES

3

Conjecture 1.2. For every 1 ≤ p∗ ≤ 2 ≤ q ≤ ∞ one has
j≤n k(aij)m

j=1kp + max

i≤m k(aij)n

p∗ → ℓn

E(cid:13)(cid:13)G : ℓm

q(cid:13)(cid:13) ≈p,q max

i=1kq + E max

i≤m
j≤n

|aijgij|.

The notation f ≈p,q h means that there are two positive constants c(p, q) and
C(p, q), which depend only on the parameters p and q, such that c(p, q)f ≤ h ≤
C(p, q)f , and, as usual 1/p + 1/p∗ = 1. This conjecture extends the corresponding
conjecture for the case p = q = 2 and m = n.
In this case, Bandeira and Van

Handel [1] (see (1.1)) proved an estimate with plog max(m, n) max|aij| instead
of E max |aijgij|, while in [23] the corresponding bound is proved with log log n in
front of the right hand side.
Remark 1.3. Note that if 1 ≤ p∗ ≤ 2 ≤ q ≤ ∞, in the case of matrices of tensor
i,j=1, with x, y ∈ Rn, Chevet’s
i,j=1 = x ⊗ y = (xj · yi)n
structure, that is, (aij )n
theorem [4, 3] and a direct computation show that

If the matrix is diagonal, that is, (aij )n
ately obtain

i,j=1 = diag(a11, . . . , ann), then we immedi-

p∗ → ℓn

E(cid:13)(cid:13)G : ℓn
q(cid:13)(cid:13) = Ek(aiigii)n

q(cid:13)(cid:13) ≈p,q kykqkxk∞ + kyk∞kxkp.
i≤n pln(i + 3) · a∗

i=1k∞ ≈ max

ii ≈ k(aii)n

i=1kMg ,

p∗ → ℓn

E(cid:13)(cid:13)G : ℓn

where (a∗
function given by

ii)i≤n is the decreasing rearrangement of (|aii|)i≤n and Mg is the Orlicz

(see Lemma 2.2 below and [11, Lemma 5.2] for the Orlicz norm expression).

Mg(s) =r 2

π Z s

0

e− 1

2t2 dt

Slightly diﬀerent estimates of the same ﬂavour can be also obtained in the case

1 ≤ q ≤ 2 ≤ p∗ ≤ ∞.

2. Notation and Preliminaries

By c, C, C1, ... we always denote positive absolute constants, whose values may
change from line to line, and we write cp, Cp, ... if constants depend on some pa-
rameter p.

Given p ∈ [1,∞], p∗ denotes its conjugate, that is 1/p + 1/p∗ = 1. For x =
(xi)i≤n ∈ Rn, kxkp denotes its ℓp-norm, that is kxk∞ = maxi≤n |xi| and, for
p < ∞,

n

|xi|p(cid:17)1/p
The corresponding space (Rn,k · kp) is denoted by ℓn
ball. The modulus of convexity of E is deﬁned for any ε ∈ (0, 2) by

kxkp =(cid:16)

Xi=1

.

If E is a normed space, then E∗ denotes its dual space and BE its closed unit

p , its unit ball by Bn
p .

δE(ε) := infn1 −(cid:13)(cid:13)(cid:13)

x + y

2 (cid:13)(cid:13)(cid:13)E

: kxkE = 1, kykE = 1, kx − ykE > εo.

We say that E has modulus of convexity of power type 2 if there exists a positive
constant c such that for all ε ∈ (0, 2), δE(ε) ≥ cε2.
It is well known that this
property (see e.g. [9] or [19, Proposition 2.4]) is equivalent to the fact that
E ≤ kxk2

E + kyk2

x − y

x + y

2

E

E

2

2

(cid:13)(cid:13)(cid:13)

2 (cid:13)(cid:13)(cid:13)

+ λ−2(cid:13)(cid:13)(cid:13)

2 (cid:13)(cid:13)(cid:13)

4 OLIVIER GU ´EDON, AICKE HINRICHS, ALEXANDER E. LITVAK, AND JOSCHA PROCHNO

In that
holds for all x, y ∈ E, where λ > 0 is a constant depending only on c.
case, we say that E has modulus of convexity of power type 2 with constant λ. We
clearly have δE(ε) ≥ ε2/(2λ2). It follows from Clarkson’s inequality [6] that for
p > 2 the space ℓn

p∗ has modulus of convexity of power type 2 with

λ−2 =

p∗(p∗ − 1)

8

1
p

.

≈

Recall that a Banach space E is of Rademacher type r for some 1 ≤ r ≤ 2 if there
is C > 0 such that for all n ∈ N and for all x1, . . . , xn ∈ E,
kxikr!1/r

n

,

2(cid:19)1/2

≤ C  n
Xi=1

where (εi)∞
i=1 is a sequence of independent random variables deﬁned on some prob-
ability space (Ω, P) such that P(εi = 1) = P(εi = −1) = 1
2 for every i ∈ N. The
smallest C is called type-r constant of E, denoted by Tr(E). This concept was in-
troduced into Banach space theory by Hoﬀmann-Jørgensen [15] in the early 1970s
and the basic theory was developed by Maurey and Pisier [18].

(cid:18)Eε(cid:13)(cid:13)(cid:13)

Xi=1

εixi(cid:13)(cid:13)(cid:13)

We will need the following theorem, which follows from [14] in combination with

an improvement that can be found in [13].

Theorem 2.1. Let E be a Banach space with modulus of convexity of power type 2
with constant λ. Let X1, . . . , Xm ∈ E∗ be independent random vectors. For q ≥ 2,

if

B := Cλ4T2(E∗)r log m

m (cid:16)E max

1≤i≤mkXikq

E∗(cid:17)1/2

,

where T2(E∗) is the type 2 constant of E∗, and

σ := sup

y∈BE  1

m

E|hXi, yi|q!1/q

,

m

Xi=1

then

1
m

m

Xi=1

E sup

y∈BE(cid:12)(cid:12)(cid:12)(cid:12)

|hXi, yi|q − E|hXi, yi|q(cid:12)(cid:12)(cid:12)(cid:12)

≤ A2 + A · σq/2.

We also recall known facts about Gaussian random variables. The next lemma

is well-known (see e.g. Lemmas 2.3, 2.4 in [23]).
Lemma 2.2. Let a = (ai)i≤n ∈ Rn and (a∗
of (|ai|)i≤n. Then

E max

i≤n |aigi| ≈ max

i≤n pln(i + 3) · a∗

i .

i )i≤n be the decreasing rearrangement

Note that in general the maximum of i.i.d. random variables weighted by coordi-
nates of a vector a is equivalent to a certain Orlicz norm kakM , where the function
M depends only on the distribution of random variables (see [12, Corollary 2] and
Lemma 5.2 in [11]).

The following theorem is the classical Gaussian concentration inequality (see e.g.

[5] or inequality (2.35) and Proposition 2.18 in [17]).

OPERATOR NORMS OF RANDOM MATRICES

5

(2.1)

Theorem 2.3. Let n ∈ N and (Y,k·kY ) be a Banach space. Let y1, . . . , yn ∈ Y
and X =Pn

i=1 giyi. Then, for every t > 0,

t2

P(cid:16)(cid:12)(cid:12) kXkY − EkXkY (cid:12)(cid:12) ≥ t(cid:17) ≤ 2 exp(cid:18)−

1

2σY (X)2(cid:19) ,

where σY (X) = supkξkY ∗ =1(cid:16)Pn
Remark 2.4. Let p > 2 and p∗ ≤ q ≤ ∞. Let a = (aj )j≤n ∈ Rn and X =
(ajgj)j≤n. Then we clearly have

i=1 |ξ(yi)|2(cid:17)

.

2

Thus, Theorem 2.3 implies for X = (aj gj)j≤n

σℓn

p (X) = max

j≤n |aj|.

Note also that

(2.2)

(2.3)

P(cid:16)(cid:12)(cid:12)kXkp − EkXkp(cid:12)(cid:12) > t(cid:17) ≤ 2 exp(cid:18) −
|aj|p|gj|p(cid:19)1/p

EkXkp ≤(cid:18)E

n

Xj=1

t2

2 maxj≤n |aj|2(cid:19).

= γpkakp.

3. Proof of the main result

We will apply Theorem 2.1 with E = ℓn

the rows of the matrix G = (aij gij)m,n
estimate the quantities σ and the expectation, appearing in that theorem.
Lemma 3.1. Let m, n ∈ N, 1 < p∗ ≤ 2 ≤ q, and for i ≤ m let Xi = (aij gij)n

p∗ , 1 < p∗ ≤ 2 and X1, . . . , Xm being
i,j=1. We start with two lemmas in which we

j=1.

Then

γq
m1/q max

j≤n k(aij)m

i=1kq.

j=1 aijyjgij, is a Gaussian random variable

2q−2p∗

q−2 ≤ 1.

|yj|

n

Xj=1

m

q(cid:19)1/q

p∗(cid:18) 1

=

m

Xi=1

σ = sup
y∈Bn

E(cid:12)(cid:12)hXi, yi(cid:12)(cid:12)
Proof. For every i ≤ m, hXi, yi = Pn
with variance k(aijyj)n
j=1k2. Hence,
1
Xi=1
E|hXi, yi|q =
m
Xi=1(cid:18) n
Xj=1

Taking standard unit vectors e1, . . . , en ∈ Bn
|aijyj|2(cid:19)q/2

σq = sup
y∈Bn
p∗

sup
y∈Bn
p∗

m

m

γq
q
m

sup
y∈Bn
p∗

m

Xi=1(cid:18) n
Xj=1

|aijyj|2(cid:19)q/2

.

p∗ , we immediately obtain

≥ max

j≤n k(aij)m

i=1kq
q.

n

n

which proves the lower bound for σ. The corresponding upper bound is a conse-
quence of H¨older’s inequality. Indeed, since q ≥ 2,
Xj=1
Since p∗ ≤ 2, we have 2q−2p∗

|aij|q|yj|p∗(cid:19)2/q
q−2 ≥ p∗. Therefore, for y ∈ Bn
p∗ ,

q−2 (cid:19)(q−2)/q

≤(cid:18) n
Xj=1

·(cid:18) n
Xj=1

ij y2p∗/q
a2

y2−2p∗/q

Xj=1

ijy2
a2

|yj|

j =

2q−2p∗

.

j

j

6 OLIVIER GU ´EDON, AICKE HINRICHS, ALEXANDER E. LITVAK, AND JOSCHA PROCHNO

m

p∗ one has

This implies that for every y ∈ Bn
Xj=1
Xi=1
k(aij)m
i=1kq

|aijyj|2(cid:19)q/2
|yj|p∗

Xi=1(cid:18) n
Xj=1

≤

=

m

n

n

n

|aij|q|yj|p∗

|yj|p∗

=

Xj=1
j≤n k(aij)m
i=1kq
q,

q ≤ max

Xj=1

which completes the proof.

m

Xi=1

|aij|q

(cid:3)

Now we estimate the expectation from Theorem 2.1. The proof is based on the
Gaussian concentration, Theorem 2.3, and is similar to Theorem 2.1 and Remark 2.2
in [23].
Lemma 3.2. Let m, n ∈ N, 1 < p∗ ≤ 2 ≤ q, and for i ≤ m let Xi = (aij gij)n

j=1.

Then

i≤m kXikq

(cid:16)E max

p(cid:17)1/q

i≤m

≤ 2 max
≤ 2 γp max

EkXikp + C γq E max
i≤m k(aij)n

|aij gij|
j=1kp + C γq E max

i≤m
j≤n

i≤m
j≤n

|aijgij|,

where C is a positive absolute constant.

Proof. For any a, b > 0 and q ≥ 1, we have aq ≤ 2q−1(cid:0)|a − b|q + bq(cid:1). Thus,
EkXikp(cid:17)q(cid:21).

p ≤ 2q−1(cid:20) max

1≤i≤mkXikq
max

1≤i≤m

q

Taking expectation and then the q-th root, we obtain

1≤i≤m(cid:12)(cid:12)(cid:12)kXikp − EkXikp(cid:12)(cid:12)(cid:12)
≤ 2(cid:18)E max
p(cid:17)1/q
1≤i≤m(cid:12)(cid:12)(cid:12)kXikp − EkXikp(cid:12)(cid:12)(cid:12)
P(cid:16)(cid:12)(cid:12)kXikp − EkXikp(cid:12)(cid:12) > t(cid:17) ≤ 2 exp(cid:18) −

(cid:16)E max
1≤i≤mkXikq
For all i ≤ m and t > 0 by (2.2) we have
(3.1)

By permuting the rows of (aij )m,n

+(cid:16) max
q(cid:19)1/q

t2

2 maxj≤n |aij|2(cid:19).

+ 2 max
1≤i≤m

EkXikp.

i,j=1, we can assume that
j≤n |anj|.

max
j≤n |a1j| ≥ ··· ≥ max

For each i ≤ m, choose j(i) ≤ n such that |aij(i)| = maxj≤n |aij|. Clearly,

max

i≤m
j≤n

|aijgij| ≥ max

i≤m |aij(i)| · |gij(i)|

and hence, by independence of gij’s and Lemma 2.2,

B := E max

i≤m
j≤n

|aij gij| ≥ E max

i≤m |aij(i)| · |gi| ≥ c max

i≤mplog(i + 3) · |aij(i)|,

where the latter inequality follows since |a1j(1)| ≥ ··· ≥ |anj(n)|. Thus, for i ≤ m,

j≤n |aij|2 = a2
max

ij(i) ≤

c log(i + 3)

.

B2

OPERATOR NORMS OF RANDOM MATRICES

7

By (3.1) we observe for every t > 0,

m

Xi=1

(cid:19)

2B2

ct2 log(i + 3)

exp(cid:18) −
dx ≤ 6 · 3−ct2/2B 2

,

x−ct2/2B 2

m

= 2

i + 3(cid:19)ct2/2B 2

1≤i≤m(cid:12)(cid:12)kXikp − EkXikp(cid:12)(cid:12) > t(cid:17) ≤ 2
P(cid:16) max
≤ 2 Z ∞
Xi=1(cid:18) 1
(cid:18)E max
1≤i≤m(cid:12)(cid:12)(cid:12)kXikp − EkXikp(cid:12)(cid:12)(cid:12)

q(cid:19)1/q

3

whenever ct2/B2 ≥ 4. Integrating the tail inequality proves that
≤ C1√q B ≤ C2 γq E max

|aij gij|.

i≤m
j≤n

By the triangle inequality we obtain the ﬁrst desired inequality, the second one
follows by (2.3).
(cid:3)

We are now ready to present the proof of the main theorem.

Proof of Theorem 1.1. First observe that

p∗ → ℓm

E(cid:13)(cid:13)G : ℓn
Xi=1(cid:12)(cid:12)hXi, yi(cid:12)(cid:12)

m

We have

E sup
y∈Bn
p∗

q(cid:17)1/q
q (cid:13)(cid:13) ≤(cid:16)E(cid:13)(cid:13)G : ℓn
p∗ → ℓm
q (cid:13)(cid:13)
p∗" m
Xi=1(cid:12)(cid:12)hXi, yi(cid:12)(cid:12)
p∗" 1
= m · E sup

≤ E sup

y∈Bn

y∈Bn

m

m

q

q

Hence, Theorem 2.1 applied with E = ℓn

=(cid:18)E sup

q(cid:19)1/q

.

m

m

y∈Bn
p∗

Xi=1(cid:12)(cid:12)hXi, yi(cid:12)(cid:12)
q# + sup
Xi=1
− E(cid:12)(cid:12)hXi, yi(cid:12)(cid:12)

y∈Bn
p∗

q# + m · σq.

q

E(cid:12)(cid:12)hXi, yi(cid:12)(cid:12)

− E(cid:12)(cid:12)hXi, yi(cid:12)(cid:12)

q

Xi=1(cid:12)(cid:12)hXi, yi(cid:12)(cid:12)

p∗ implies

where B and σ are deﬁned in that theorem. Therefore,

E(cid:13)(cid:13)G : ℓn

q

p∗ → ℓm
≤ m ·(cid:2)B2 + Bσq/2(cid:3) + m · σq ≤ 2m(cid:0)B2 + σq(cid:1),
q (cid:13)(cid:13)
q(cid:17)1/q
(cid:16)E(cid:13)(cid:13)G : ℓn
p∗ → ℓm
q (cid:13)(cid:13)
p ) ≈ √p and that Bn

≤ 21/qm1/q (cid:16)B2/q + σ(cid:17) .

p∗ has modulus of convexity of power type

Now, recall that T2(ℓn
2 with λ−2 ≈ 1/p. Therefore,

p(cid:17)1/q

(cid:16)E max
1≤i≤m kXikq
p(cid:17)1/q
1≤i≤mkXikq

.

2

(ℓn

B2/q = C2/qλ8/q T 2/q

p )(cid:18) log m

Applying Lemma 3.1, we obtain

m (cid:19)1/q
= C2/qp5/q(log m)1/qm−1/q(cid:16)E max
q(cid:17)1/q
p∗ → ℓm
q (cid:13)(cid:13)

(cid:16)E(cid:13)(cid:13)G : ℓn
≤ (2C2)1/q · p5/q · (log m)1/q(cid:16)E max

1≤i≤m kXikq
The desired bound follows now from Lemma 3.2.

p(cid:17)1/q

+ 21/qγq · max

1≤j≤n k(aij)m

i=1kq.

(cid:3)

8 OLIVIER GU ´EDON, AICKE HINRICHS, ALEXANDER E. LITVAK, AND JOSCHA PROCHNO

Acknowledgment. Part of this work was done while A.L. visited J.P. at the
Johannes Kepler University in Linz (supported by FWFM 1628000). We would
also like to thank our colleague R. Adamczak for helpful comments.

References

[1] A. Bandeira and R. van Handel. Sharp nonasymptotic bounds on the norm of random matrices

with independent entries. Ann. Probab., to appear.

[2] G. Bennett, V. Goodman, and C.M. Newman. Norms of random matrices. Paciﬁc J. Math.,

59(2):359–365, 1975.

[3] Y. Benyamini and Y. Gordon. Random factorization of operators between banach spaces. J.

Analyse Math., 39(45-74), 1981.

[4] S. Chevet. S´eries de variables al´eatoires gaussiennes `a valeurs dans E ˆ⊗εF . Application aux
produits d’espaces de Wiener abstraits. In S´eminaire sur la G´eom´etrie des Espaces de Banach
(1977–1978), pages Exp. No. 19, 15. ´Ecole Polytech., Palaiseau, 1978.

[5] B. S. Cirel′son, I. A. Ibragimov, and V. N. Sudakov. Norms of Gaussian sample functions. In
Proceedings of the Third Japan-USSR Symposium on Probability Theory (Tashkent, 1975),
pages 20–41. Lecture Notes in Math., Vol. 550. Springer, Berlin, 1976.

[6] James A. Clarkson. Uniformly convex spaces. Trans. Amer. Math. Soc., 40(3):396–414, 1936.
[7] K.R. Davidson and S.J. Szarek. Addenda and corrigenda to: ”Local operator theory, random
matrices and Banach spaces”. Handbook of the geometry of Banach spaces, Vol. 2. North-
Holland, North-Holland, Amsterdam, 2003.

[8] K.R. Davidson and S.J. Szarek. Local operator theory, random matrices and Banach spaces.
Handbook of the geometry of Banach spaces, Vol. 1. North-Holland, North-Holland, Amster-
dam, 2003.

[9] T. Figiel. On the moduli of convexity and smoothness. Studia Math., 56(2):121–155, 1976.

[10] Y. Gordon. Some inequalities for gaussian processes and applications. Israel J. Math., 50:265–

289, 1985.

[11] Y. Gordon, A.E. Litvak, C. Sch¨utt, and E. Werner. Uniform estimates for order statistics

and Orlicz functions. Positivity, 16(1):1–28, 2012.

[12] Yehoram Gordon, Alexander Litvak, Carsten Sch¨utt, and Elisabeth Werner. Orlicz norms of

sequences of random variables. Ann. Probab., 30(4):1833–1853, 2002.

[13] O. Gu´edon, S. Mendelson, A. Pajor, and N. Tomczak-Jaegermann. Majorizing measures and
proportional subsets of bounded orthonormal systems. Revista Matemtica Iberoamericana,
24(3):1075–1095, 2008.

[14] O. Gu´edon and M. Rudelson. Moments of random vectors via majorizing measures. Adv.

Math., 208(2):798 – 823, 2007.

[15] J. Hoﬀmann-Jørgensen. Sums of independent Banach space valued random variables. Studia

Math., 52:159–186, 1974.

[16] R. Lata la. Some estimates of norms of random matrices. Proc. Amer. Math. Soc.,

133(5):1273–1282 (electronic), 2005.

[17] M. Ledoux. The concentration of measure phenomenon. Mathematical Surveys and Mono-

graphs, 89. American Mathematical Society, Providence, RI, 2001.

[18] Bernard Maurey and Gilles Pisier. S´eries de variables al´eatoires vectorielles ind´ependantes et

propri´et´es g´eom´etriques des espaces de Banach. Studia Math., 58(1):45–90, 1976.

[19] Gilles Pisier. Martingales with values in uniformly convex spaces. Israel J. Math., 20(3-4):326–

350, 1975.

[20] S. Riemer and C. Sch¨utt. On the expectation of the norm of random matrices with non-

identically distributed entries. Electron. J. Probab., 18:no. 29, 13, 2013.

[21] M. Rudelson and O. Zeitouni. Singular values of gaussian matrices and permanent estimators.

Random Structures and Algorithms, to appear.

[22] Y. Seginer. The expected norm of random matrice. Combin. Probab. Comput., 9:149–166,

2000.

[23] R. Van Handel. On the spectral norm of gaussian random matrices. Trans. Amer. Math. Soc.,

to appear.

[24] J. von Neumann and H. H. Goldstine. Numerical inverting of matrices of high order. Bull.

Amer. Math. Soc., 53(11):1021–1099, 11 1947.

OPERATOR NORMS OF RANDOM MATRICES

9

[25] E. P. Wigner. Characteristic Vectors of Bordered Matrices With Inﬁnite Dimensions. The

Annals of Mathematics, 62(3):548–564, 1955.

[26] E. P. Wigner. On the Distribution of the Roots of Certain Symmetric Matrices. The Annals

of Mathematics, 67(2):325–327, March 1958.

[27] J. Wishart. The generalised product moment distribution in samples from a normal multi-

variate population. Biometrika, 20A(1/2):32–52, July 1928.

(Olivier Gu´edon) Laboratoire d’Analyse et de Math´ematiques Appliqu´ees, Universit´e

Paris-Est, 77454 Marne-la-Vall´ee, Cedex 2, France

E-mail address: olivier.guedon@u-pem.fr

(Alexander E. Litvak) Department of Mathematical and Statistical Sciences, Univer-

sity of Alberta, Edmonton, AB, T6G 2G1, Canada

E-mail address: alitvak@ualberta.ca

(Aicke Hinrichs) Institut f¨ur Analysis, Johannes Kepler Universit¨at Linz, Altenberg-

erstrasse 69, 4040 Linz, Austria

E-mail address: aicke.hinrichs@jku.at

(Joscha Prochno) Department of Mathematics, University of Hull, Cottingham Road,

Hull, HU6 7RX, United Kingdom

E-mail address: j.prochno@hull.ac.uk

