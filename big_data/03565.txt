Matrix factoring by fraction-free reduction

Johannes Middeke

RISC Linz

Johannes Kepler University

Linz, Austria

jmiddeke@risc.uni-linz.ac.at

David J. Jeffrey

ORCCA and Dept Applied Mathematics

The University of Western Ontario
London, Ontario, Canada N6A 5B7

djeffrey@uwo.ca

6
1
0
2

 
r
a

 

M
1
1

 
 
]

C
S
.
s
c
[
 
 

1
v
5
6
5
3
0

.

3
0
6
1
:
v
i
X
r
a

ABSTRACT
We consider exact matrix decomposition by Gauss-Bareiss
reduction. We investigate two aspects of the process: com-
mon row and column factors and the inﬂuence of pivoting
strategies. We identify two types of common factors: sys-
tematic and statistical. Systematic factors depend on the
process, while statistical factors depend on the speciﬁc data.
We show that existing fraction-free QR (Gram–Schmidt) al-
gorithms create a common factor in the last column of Q.
We relate the existence of row factors in LU decomposi-
tion to factors appearing in the Smith normal form of the
matrix. For statistical factors, we identify mechanisms and
give estimates of the frequency. Our conclusions are tested
by experimental data. For pivoting strategies, we compare
the sizes of output factors obtained by diﬀerent strategies.
We also comment on timing diﬀerences.

Keywords
LU Decomposition; Fraction free; QR factors; Common fac-
tor removal; pivoting strategy

1.

INTRODUCTION

Although known earlier, fraction-free methods for exact
matrix computations became popular after Bareiss’s study
of Gaussian elimination [1]. Extensions to related topics,
such as LU factoring, were considered in [9, 10, 15]. Gram–
Schmidt orthogonalization and QR factoring were studied
by [3], under the more descriptive name of exact division.
Recent studies have looked at extending fraction-free LU fac-
toring to non-invertible matrices [7] and rank proﬁling [2],
and more generally to areas such as the Euclidean algorithm,
and the Berlekamp–Massey algorithm [8]. We consider ma-
trices over an integral domain D. For the purposes of giv-
ing illustrative examples and conducting computational ex-
periments, matrices over Z and Q[x] are used, because the
metrics associated with these domains are well established
and familiar to readers. We emphasize, however, that the

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
ISSAC’16 July 19–22, 2016, Waterloo, ON, Canada
c(cid:13) 2016 ACM. ISBN 123-4567-24-567/08/06. . . $15.00
DOI: 10.475/123 4

methods here apply for all integral domains, as opposed to
methods that target speciﬁc domains, such as [5, 12].

The starting point for this paper is the fraction-free form
for LU decomposition [7]: given a matrix A over an integral
domain D,

A = PwLD−1U Pc ,

(1)

where L, D and U are over D. L and U are lower and up-
per triangular and their diagonals contain the pivots of the
Gaussian elimination; D is diagonal and contains products of
the pivots. The permutation matrices Pw and Pc ensure that
the decomposition is always a full-rank decomposition, even
if A is rectangular or rank deﬁcient. In addition to the usual
indeterminacy due to varying pivot choices, the columns of
L and the rows of U can be multiplied by common factors,
which then appear also in D. We show in section 3 that this
form can cover QR decomposition also.

Our ﬁrst main result is for QR factoring. In this context,
the orthonormal Q matrix used in ﬂoating point calculations
is replaced by a Θ matrix, which is left-orthogonal, i.e. ΘtΘ
is diagonal, but ΘΘt is not. We show that for a square
matrix A, the last column of Θ, as calculated by existing al-
gorithms, is subject to an exact division by the determinant
of A, with a signiﬁcant reduction in size. This is an example
of a systematic factor, being one inherent to the algorithm.
Systematic factors occur in several ways. The Bareiss al-
gorithm uses exact division precisely to remove systematic
factors; the Gram–Schmidt algorithm from [3] is another,
where exact division removes systematic factors during the
reduction. In addition to these, we add a diﬀerent type of
systematic factor: we show a relation between GCDs exist-
ing for the rows in matrices obtained from LU factoring, and
entries in the Smith normal form of the same initial matrix.
We next consider statistical factors: ones which depend
on the initial data. When LU and QR matrices are com-
puted using current standard fraction-free algorithms, their
rows and columns may contain common factors. We discuss
their origins and show we can predict a signiﬁcant propor-
tion of them from simple considerations. Their presence
inﬂuences aspects such as uniqueness. Speciﬁcally, for the
basic decomposition (1), we show how common factors can
be moved between the three matrices. We discuss when this
is beneﬁcial.

We next consider the role of pivoting in Gaussian reduc-
tion. Geddes et al. [4] comment “We also mention that when
the entries of A(k) are not of uniform size, it may be worth-
while to interchange rows in order to obtain a smaller pivot
at the next step”. It is often said that whereas for ﬂoating-
point Gaussian elimination the largest pivot should be cho-

Ratio
2

1

10 20 30 40 50 60 70 80 90 100 110 120

Size

❍

❍

❍s

n

❍
3
7
13
23
37
53
67
89
109

11

19

31

53

73

97

107

1.00
0.97
0.94
0.93
0.89
0.93
0.90
0.89
0.87

0.85
0.88
0.84
1.33
0.81
0.80
1.32
0.53
0.77

0.91
0.83
0.82
0.85
0.77
0.74
0.73
0.72
0.73

0.78
0.71
0.68
0.66
0.63
0.62
0.60
0.61
0.60

0.72
0.65
0.61
0.59
0.56
0.55
0.54
0.53
0.52

0.63
0.59
0.55
0.51
0.49
0.47
0.46
0.45
0.44

0.62
0.56
0.52
0.49
0.47
0.45
0.44
0.43
0.42

Figure 1: Comparison of the output size of Gaussian
Elimination vs. LD−1U . We show the ratio of the number
of digits in the output of Gaussian elimination divided by the
number of digits in the output of the LD−1U decomposition
for random square integer matrices of various sizes.

Table 1: Timings of LD−1U decomposition vs. Gaus-
sian elimination for integer matrices. The table shows
the run times for LD−1U divided by those for Gauss for
random n-by-n matrices with maximal entry size s.

sen, in the setting of exact computation the smallest pivot is
best. Although within the ﬂoating-point literature, pivoting
has been studied over an extended period, much less atten-
tion has been paid to the question in the context of exact
computation. We consider a number of strategies empiri-
cally, and show that selecting the smallest pivot, suitably
deﬁned, leads to smaller output matrices.

The paper will start with a brief discussion of fraction-free
methods, then present results for QR factoring, LU factor-
ing, and ﬁnally pivoting.

2. FRACTION-FREE METHODS

Fraction-free methods are based on the assumption that it
is more eﬃcient to compute with the elements of the input
domain of a matrix than to compute with elements from
the quotient ﬁeld. Since the solutions usually sought require
the quotient ﬁeld, fraction-free methods can be regarded as
delaying for as long as possible the ultimate fall from grace of
the computation. Here, some measurements are reported to
supply empirical evidence to support fraction-free methods.
Our ﬁrst point of comparison is between the LU decom-
position oﬀered by Maple, through LUDecomposition(A),
and our own implementation of the LD−1U decomposition
based on [7]. The built-in Maple command returns matri-
ces L and U such that all diagonal elements of L are 1, and
both L and U contain elements from the quotient ﬁeld of
D. The procedure which we implemented has the output
format described in [7, Theorem 2].

Figure 1 shows the ratio of average storage requirements
for the decomposition of integer matrices. Here, we mea-
sure the total number of digits needed to represent the ﬁ-
nal output. Note that this metric does not depend on the
internal implementation of the two functions, nor does it
depend on the particular computer algebra system. As ﬁg-
ure 1 illustrates, fraction-free methods require roughly half
the storage.

Table 1 compares timings for random integer matrices,
as functions of matrix size and initial data size. For this
experiment we used our own implementation of Gaussian
elimination, since we do not know the details of Maple’s
built-in procedure, which may well use compiled code. By
writing our own programme we make sure that every com-
mon part, for example pivot searching, uses exactly the same
code and only the reduction steps diﬀer. As table 1 reveals,
the advantages of the fraction-free method are clear, while
not spectacular.

3. COMMON FACTORS IN QR

A fraction-free (exact division) algorithm for Gram–Schmidt

orthogonalization was described by [3]. An algorithm based
on LU factoring has been described in [13, 15]. The two
approaches yield the same results. We denote the decom-
position by A = ΘD−1R, because Q usually denotes an
orthonormal matrix, and Θ is not orthonormal. We give a
new statement of the basic theorem.

Theorem 1. Given a square, full-rank matrix A over an
integral domain D, the partitioned matrix (AtA, At) has a
fraction-free LU decomposition

(AtA, At) = RtD−1(R, Θt) ,

where ΘtΘ = D and A = ΘD−1R.

Proof. We can apply LU factoring, to get

(AtA, At) = ˆLD−1( ˆU , Θ) ,

where the notation ˆL, ˆU emphasizes that the matrices refer
not to a factoring of A, but of AtA. Since this matrix is
symmetric we obtain

ˆLD−1 ˆU = AtA = ˆU tD−1 ˆLt.

Because A has full rank, so do L and U and we can rewrite
the equation as

ˆU ( ˆLt)−1D = D ˆL−1 ˆU t.

Examination of the matrices on the left hand side reveals
that they and therefor also their product are all upper tri-
angular. Similarly, the left hand side is a lower triangular
matrix and the equality of the two implies that they must
both be diagonal. Cancelling D and rearranging the equa-
tion yields ˆU = ( ˆL−1 ˆU t) ˆLt where ˆL−1 ˆU t is diagonal. This
shows that the rows of ˆU are just multiples of the rows of
ˆLt. However, we know that the diagonal entries of ˆU and ˆL
are the same. Thus, ˆL−1 ˆU t is the identity and ˆL = ˆU t.

We now write R = ˆLt = ˆU . The proof of [15, Theo-
rem 8] shows A = ΘD−1R and ΘtΘ = ˆU (D ˆL−1)t. Expand-
ing the last expression and using the deﬁnition of R gives
then ΘtΘ = RR−1D = D.

We now give an explicit expression of the last column of

Θ, showing the common factor of det A.

Theorem 2. With A ∈ Dn×n and Θ as in theorem 1, we

have for all i = 1, . . . , n that

Θin = (−1)n+i det Ain det A

where det Ain is the (i, n) minor of A.

Proof. We use the notation from the proof of theorem 1.

From ΘD−1 ˆLt = A we obtain

ΘtA = D ˆL−1AtA = D ˆL−1( ˆLD−1 ˆU ) = ˆU .

Thus, since A has full rank, Θt = ˆU A−1 or, equivalently,

Θ = ( ˆU A−1)t = (A−1)t ˆU t = (det A)−1(adj A)t ˆU t

where adj A is the adjugate matrix of A. Since ˆU t is a lower
triangular matrix with det AtA = (det A)2 at position (n, n),
the claim follows.

Theorem 3. Given a square matrix A, a reduced fraction-
free QR decomposition is given by A = ˆΘ ˆD−1 ˆR, where
S = diag(1, 1, . . . , det A) and ˆΘ = ΘS−1, and ˆR = S−1R.
In addition, ˆD = S−1DS−1 = ˆΘt ˆΘ.

Proof. By theorem 2, ΘS−1 is an exact division. The

theorem follows from A = ΘS−1SD−1SS−1R.

As an example we consider the 4-by-4 integer matrix

A =

64 −96
21
−62
31
56
18
38
2
−59 −86
19
40 −91 −62
9




.




Computing the QR decomposition with theorem 1 yields

Θ =




268341
−62
3112061098992
38
155634
−59 −843590
14218033256642
40 −976219 −81659738 −18215371009147

2658137038 −23374523883001
8243861790
2460946816




D = diag(10369, 1760876458298, 81089877269400184044,

1090005501728694354954965838) ,

R =

10369

816

169821242

−6391

66495846

8322

−27518383

0
0

477501379182

210662060582

0

2282727441742609




0
0
0

.




We can now check that indeed det A = 47777897 divides the
last column of Θ.

Cancelling det A from the last column of Θ and the last
entry of R as well as reducing D accordingly leads to the
much simpler output

Θ =

−62
268341
65136
38
155634
−59 −843590
297586
40 −976219 −81659738 −381251

2658137038 −489233
8243861790
2460946816




,




D = diag(10369, 1760876458298, 81089877269400184044,

477501379182) and

R =

10369




0
0
0

816

169821242

−6391

66495846

8322

−27518383

0
0

477501379182

210662060582

0

47777897




4. COMMON FACTORS IN LU

Given a matrix A over an integral domain D, we consider
the fraction-free decomposition A = LD−1U .
It is clear
that if the elements in a column of L or a row of U possess
a common GCD, then that factor can be removed, reducing
the size of the matrix elements. We identify 3 sources of
common GCDs.

4.1 Input data

The initial matrix may contain one or more rows hav-
ing a common GCD, usually because of modelling choices
made by the user. Standard Gaussian elimination will then
transfer the common factor into all subsequent rows. If sev-
eral rows have diﬀerent GCDs, then all GCDs accumulate
in subsequent rows.
4.2 LU and the Smith Form

The following theorem links the Smith normal form of a
given matrix with factors appearing in the LU decomposi-
tion.

Theorem 4. Let A ∈ Dn×n have the Smith normal form
S = diag(d1, . . . , dn) where d1, . . . , dn ∈ D. Moreover, let
A = LD−1U be an LD−1U decomposition of A. Then for
k = 1, . . . , n

d∗
k =

k

Yj=1

dj | Uk,∗

and d∗

k | L∗,k.

Remark 1. The values d∗

1, . . . , d∗

n are known as the de-

terminantal divisors of A.

1 = d1 and dk = d∗

Proof. According to [11, II.15], the diagonal entries of
the Smith form are quotients of the determinantal divisors,
i. e., d∗
k−1 for k = 2, . . . , n. Moreover,
d∗
k is the greatest common divisor of all k-by-k minors of A
for each k = 1, . . . , n. Thus, we only have to prove that the
entries of the kth row of U are k-by-k minors of A. However,
this follows from [4, Eqns (9.8), (9.12)], since the kth row of
U are just

k/d∗

det


A11
...
Ak1

· · · A1k A1j
...
· · · Akk Akj

...




where

j = 1, . . . , k.

Similarly, following the algorithm in [7], we see that the
columns of L are just made up by copying entries from the
columns of U during the reduction. More precisely, the kth
column of L will have the entries a(k−1)
(using
the notation of [4]). But these are again just k-by-k minors
of A.

, . . . , a(k−1)

nk

1k

We give an example using the domain Q[x]. Let A be the

polynomial matrix

2

x2 + x

2 −x3 + 5x2 + 3x − 9

2 x3 − x2
− 3
−3 −2x3 + 10x2 + 5x − 9 2x2 + 2x x3 − 2x2
2 x3
− 1
1
2
− 1
1
2 x
2

x3 + 3
2
−x − 3
2

0

0

1





.





The Smith normal form S of A is

diag(1, x, x(x + 1), x(x + 1)(x − 1))

and thus its determinantal divisors are d∗
2 = x,
4 = x3(x + 1)2(x − 1). Computing the
3 = x2(x + 1) and d∗
d∗
LD−1U decomposition of A yields A = LD−1U where L is

1 = 1, d∗

0

3
2

x

0

0

3 + 1

2

2

x

0

0

0

1
2

−x
2 − 1

2

− 1

x

3 − 5
3 + 5

2

x

2

x

2 − 3
2 + 3x − 1

1
2

x

x

2

x

3 − 1

2

x

2 − 1

4

x

6 − 1

4

x

5 + 1

4

x

4 + 1

4

3

x

2

− 3

2

−3




,




D = diag(−3/2, −9/4x, 3/4x4 + 3/4x3, −1/8x9 − 1/4x8 +
1/4x6 + 1/8x5), U is

could detect about 40.17% of all the common prime row
factors occurring in U .1

− 3

2 −x

3 + 5x

2 + 3x − 9

2

2 + x

x

0

0

0

3
2

x

0

0

0

1
2

x

3 + 1

2

2

x

0

− 1

4

x




1
2

3 − x

2

x

0

2

− 1
6 − 1

4

x

x

4 − 1
5 + 1

2

4

3

x

x

4 + 1

4

3

x

.




Computing the column factors of L and the row factors of
U yields 1, x, x2(x + 1) and x3(x − 1)(x + 1)2, i. e., exactly
the determinantal divisors. In general, there could be other
factors as well.
4.3 Statistical effects

Suppose that during Bareiss’s algorithm after k − 1 itera-

tions we have reached the following state

A(k−1) =

U ∗ ∗
p ∗
0
0 a
0
0
0
0 0 ∗

∗
∗
v
b w
∗







,

where U is an upper triangular matrix, p, a, b ∈ D, v, w ∈
D1×n−k−1 and the other overlined quantities are row vectors
and the underlined quantities are column vectors. Assume
that a 6= 0 and that we choose it as a pivot. Continuing the
computations we now eliminate b (and the entries below) by
cross-multiplication

A(k−1)

❀

U ∗
∗
p ∗
0
0 a
0
0
0
0 0




∗
∗
v

.




0 aw − bv
0

∗

Here, we can see that any common factor of a and b will be
a factor of every entry in that row, i. e., gcd(a, b) | aw − bv.
However, we still have to carry out the exact division step.
This leads to

A(k−1)

❀

U ∗ ∗
p ∗
0
0
0 a
0
0 0
0 0 0





∗
∗
v

1
p (aw − bv)

∗





= A(k).

The division by p is exact. Some of the factors in p might be
factors of a or b while others are hidden in v or w. However,
every common factor of a and b which is not also a factor
of p will still be a common factor of the resulting row. In
other words,

gcd(a, b)

gcd(a, b, p) (cid:12)(cid:12)(cid:12)

1
p

(aw − bv).

In fact, the factors do not need to be tracked during the
LD−1U reduction but can be computed afterwards: All the
necessary entries a, b and p of A(k−1) will end up as entries
of L. More precisely, we will have p = Lk−1,k−1, a = Lk,k
and b = Lk+1,k.

If D are the integers, then the probability that the quotient
gcd(a, b)/ gcd(p, a, b) 6= 1, i.e. nontrivial, for random a, b, p
equals 1 − 6ζ(3)/π2 ≈ 26.92% [6, 14]. Thus, for integer
matrices these factors occur with a high enough frequency to
suggest we care about them. In our experiments we saw that
independently of the size of the input matrix this method

As an example consider the matrix

−18 −92 −25 −60

A =




0
49 −77
31
18
−58
41
−77 −52

8
66
45
51
69 −81
37 −97
22
48 −19 −10




This matrix has a LD−1U decomposition with

0
0

134076

0
0
0

8

0

−10 −126
51 −2355
−97
−60




L =

and

.

0
0
0
0




4289 −233176 −28490930
2940 −148890 −53377713

11988124645

U =

49

8
0 −126
0
0
0

0
0
0




45
298

−77
−1186

134076 −414885

66

1044
351648

0
0

−28490930

55072620

0

11988124645

.




The method outlined above correctly predicts the common
factor 2 in the second row, the factor 3 in the third row and
the factor 2 in the fourth row. However, it does not detect
the additional factor 5 in the fourth row.

There is another way in which common factors in integer
matrices can arise: Let d be any number. Then for random
a, b the probability that d | a + b is 1/d. That means that
if v, w ∈ Z1×n are vectors, then d | v + w with a probability
of 1/dn. This eﬀect is noticable in particular for small num-
bers like d = 2, 3 and in the last iterations of the LD−1U
decomposition when the number of non-zero entries in the
rows has shrunk. For instance, in the second last iterations
we only have three rows with at most three non-zero entries
each. Moreover, we know that the ﬁrst non-zero entries of
the rows cancel during cross-multiplication. Thus, a factor
of 2 appears with a probability of 25% in one of those rows,
a factor of 3 with a probability of 11.11%.
In the exam-
ple above, the probability for the factor 5 to appear in the
fourth row was 4%.

In a manner similar to theorem 3, we can cancel all factors

which we ﬁnd from the ﬁnal output:

Theorem 5. Given a matrix A ∈ Dm×n with rank r and
its decomposition A = PwLD−1U Pc, if DU = diag(d1, . . . , dr)
is a diagonal matrix with dk | gcd(Uk,∗), then setting ˆU =
D−1
U where both matrices are fraction-free
we have the decomposition A = PwL ˆD−1 ˆU Pc.

U U and ˆD = DD−1

Proof. By [7, Theorem 2] the diagonal entries of U are
the pivots chosen during the decomposition and they also
divide the diagonal entries of D. Thus, any common divisor
of Uk,∗ will also divide Dkk and therefor both ˆU and ˆD are
fraction-free. We can easily check that A = PwLD−1DU D−1
PwL ˆD−1 ˆU Pc.
1This experiment was carried out with random square ma-
trices A of sizes between 5-by-5 and 125-by-125. We decom-
posed A into PwLD−1U Pc and then computed the number
of predicted prime factors in U and related that to the num-
ber of actual prime factors. We did not consider the last
row of U since this contains only the determinant.

U U =

Remark 2. If we can ﬁnd common column factors of L
we can cancel them in the same way. However, if we have
already cancelled factors from U , then there is no guarantee
that d | L∗,k implies d | ˆDkk. Thus, in general we can only
cancel gcd(d, ˆDkk) from L∗,k.

5. PIVOTING STRATEGIES FOR LU

Our pivoting strategies are all based on full pivoting, which
is already implied by the deﬁnition of the form. We deﬁne
a number of pivoting strategies.

Largest We select the largest pivot according to an ap-
propriate metric. Metrics were the absolute value for
integer matrices and the degree as well as the height
for matrices univariate polynomials.

Smallest Here we select the smallest pivot according to the

same metrics as above.

First We select the ﬁrst non-zero pivot.

Factors With this strategy we select the pivot which has
the least number of prime factors counted with multi-
plicity.

Of course, the “factors” strategy is not viable in practice
since the factorisation is much too costly. However, it does
provide interesting theoretical insight.

In contrast to ﬂoating point calculations, accuracy of the
result is not an issue, and we consider instead the size of
the elements in the matrices generated, and any impact on
the eﬃciency of the computation. By size we examine the
following

Digits For integer matrices or matrices we count the total
number of base-10 digits needed to represent it. We
also use this measurement for matrices with rational
number entries where we simply add up the digits of
the numerators and the denominators.

Terms For univariate polynomial matrices we count the to-
tal number of non-zero terms in the fully expanded
representation of the entries.

Height As another metric for polynomial matrices we use

the maximal height of its entries.

Factors For both integer and polynomial matrices we mea-
sure the total number of row factors. Here, we compute
the greatest common divisor of each row and count the
number of prime factors with multiplicity. The number
of factors for ech row is then added up.

Note that the measured quantities do solely depend on the
output. In particular do they not depend on how the pro-
gramme handles its memory during the computations. Also
note that the measurements are chosen in such a way that
they are independent of the internal representation of the
data. For instance, every programme has to store all the
digits of the output matrices somehow.

The experiments included in this paper were all carried
out with Maple. We use our own implementation of the
LD−1U decomposition which closely follows [7]. For each ex-
periment we generated random matrices A of diﬀerent sizes
and then performed the decomposition A = PwLD−1U Pc
using the strategies described above. That is, each random

n

5
10
15
20
25

smallest
78.13
503.72
1625.08
3832.33
7533.28

digits
largest
101.74
678.40
2130.83
4888.83
9365.39

factors
85.13
569.40
1833.94
4297.05
8316.27

row factors

smallest
7.58
11.65
17.17
21.38
26.06

largest
8.01
12.80
17.95
22.88
27.92

factors
5.74
6.44
7.77
7.98
8.26

Table 2: Output sizes for diﬀerent pivoting strate-
gies for integer matrices. The table compares the aver-
age number of digits and the number of row common factors
of U for random n-by-n integer matrices A as input using
the “smallest”, “largest” and “factors” pivoting strategies.

n smallest degree
5
83.07
532.45
10
15
1696.09
20
3932.09

largest degree
106.80
698.15
2154.53
4860.95

height
91.91
609.13
1946.16
4504.71

Table 3: Output sizes for diﬀerent pivoting strate-
gies for polynomial matrices. The table compares the
number of terms of U for random n-by-n input matrices A
using the “smallest degree”, “largest degree” and “smallest
height” pivoting strategies.

matrix A was decomposed with each of the strategies. We
then applied the applicable measurements. In the end we
computed the mean value of all the results. More precise
description of the experiments follow below.

For table 2 we generated three hundred integer matrices
for each size. The entries where between −113 and 113. Also
in order to be closer to real world problems, we made sure
that the sizes of the entries in our matrices varied widely
with less than 25% of the entries reaching maximal size.
Table 2 shows the number of digits and the number of row
factors of U where the decompositions are done using the
“smallest”, “largest” and “factors” strategies described at the
beginning of this section.

Table 3 shows a similar experiment for matrices of univari-
ate polynomials. We compare the strategies of choosing the
pivot with the smallest degree versus choosing the largest
degree and choosing the smallest height. The matrices A
contained random polynomials with integer coeﬃcients be-
tween −100 and 100 and degree at most 3. During the same
experiment we also measured the number of row factors and
the height of U but we did not ﬁnd a signiﬁcant diﬀerence
between the diﬀerent strategies.

6. SOLVING

In this section we detail a method for solving linear sys-
tems in such a way that fractions are delayed until the ﬁnal
output.

Let A ∈ Dm×n and b ∈ Dm. We wish to solve the system
Ax = b, seeking solutions x with entries in the ﬁeld of frac-
tions of D. First, apply the LD−1U decomposition as in [7].
We obtain

DL−1P t

0(cid:19) Pc

and Pcx = (cid:18)y

z(cid:19) ,

w A = (cid:18) V

W(cid:19) A = (cid:18)U B

0

where all (sub) matrices have entries in D, U is an r-by-r,
regular and upper triangular matrix, r is the rank of A and
where y has dimension r. Then Ax = b if and only if W b = 0

and U y + Bz = V b.

Now, perform a second LD−1U decomposition on U (piv-
oting is not needed as all diagonal entries of U are non-zero),
working from the bottom to the top, and from right to left2.
This will compute a regular X ∈ Dr×r such that XU = ∆
is a diagonal matrix. Then Ax = b if and only if W b = 0
and ∆y + XBz = XV b.

Assume now that the compatibility condition W b = 0 is
fulﬁlled. In order to compute a particular solution x0 of the
system Ax = b, we can simply choose

x0 = P −1

c (cid:18)∆−1XV b

0 (cid:19)
(cid:19) = ˜∆−1Sb where S = Pc(cid:18)XV
and where ˜∆ = Pc diag(∆, 1)Pc is a diagonal matrix with
entries in D.

0

Moreover, we can compute the nullspace of A in the fol-

lowing way: If

x ∈ colspace P −1

c (cid:18)−∆−1 XB

1n−r (cid:19) ,

then we can easily check that Ax = 0. Since the n − r
columns of the matrix spanning the space are clearly lin-
early independent, it follows that this is already the entire
nullspace of A. Thus, setting

K = Pc(cid:18)−XB

1 (cid:19) ,

we see the nullspace of A is colspace ˜∆−1K, with ˜∆ as de-
ﬁned above.

Note that S and K are both matrices over D. Thus, the
particular solution and the nullspace are both computed in a
fraction-free way. Moreover, neither of the matrices depends
on the right hand side b. Consequently, after computing W ,
S, ˜∆ and K, we can solve the system Ax = b for arbitrary
b by just checking whether W b = 0 and then computing
x0 = ˜∆−1Sb.

We summarise the method as follows:

Algorithm 1. Input: A matrix A ∈ Dm×n.

Output: Matrices W , S, and K with entries in D and a
diagonal matrix ˜∆ with entries in D such that for any
b ∈ Dm if the compatibility condition W b = 0 is met,
then the system Ax = b has the solution set ˜∆−1Sb +
colspace ˜∆−1K.

Steps:

1. Apply the LD−1U decomposition to obtain

DL−1P t

wA = (cid:18) V

W(cid:19) A = (cid:18)U B
0(cid:19)

0

where U is upper triangular.

2. Use a backwards LD−1U decomposition on U to
obtain a matrix X such that diagonal XU = ∆ is
a diagonal matrix.

3. Let

S = Pc(cid:18)XV

1 (cid:19)
0 (cid:19) , K = Pc(cid:18)−XB

and ˜∆ = Pc diag(∆, 1)Pc.

2More formally,
let Π be the matrix of the permutation
which maps i to r + 1 − i and decompose ΠU Π in the normal
way applying the same permutations to the result.

As an example we consider the matrix

A =




−370 −62 −101 −3
−708 −120 −193 −5
−304 −50 −83 −3
−1962 −336 −534 −12




and examine the two systems below for solutions.

Ax =




1
0
0
1




= b1

and Ax =




0
0
1
1




= b2.

Following algorithm 1, we ﬁrst compute




1
3

0
0

0
−3

0
0
110 −36 −50 0
1

−6 −1

7




A = 


−3 −62 −101 −370
0 −36 −54 −198
0
−12 −12
0

0
0

0

0

Pc




where Pc represents the permutation (1
deﬁne the matrices V , W , U and B. Next, we compute

4); and use this to

X = 


432 −744 −288

0
0

−12

0

54
1




and XU = diag(−1296, 432, −12) = ∆. This leads to

S = 


0

5904
110

−33480

0

0

0
−1944 −2664 0
0
0

−50
16632

−36
10368




, K = 


1

−1728

12
9072




and ˜∆ = diag(1, 432, −12, −1296).

We can check that W b1 = 8 6= 0. Consequently, the
system Ax = b1 does not have a solution. On the other
hand, W b2 = 0 and the solution set for Ax = b2 is

˜∆−1Sb + colspace ˜∆−1K =

0

−37/6
25/6
−77/6




+ colspace







1
−4
−1
−7




.

7. CONCLUSIONS

We have shown that fraction-free LU and QR decomposi-
tions can contain signiﬁcant common factors, and we have
shown how these can be beneﬁcially removed to obtain more
compact decompositions. Moreover, their removal makes
the decomposition unique.

We considered removing the common factors as soon as
they can be detected during the computation of the decom-
positions. This would require either discovering the GCDs
by direct computation, or by predicting them by diﬀerent,
preferably simpler, computations. Although we have dis-
played here mechanisms that generate common factors, and
which lend themselves to predictions through relatively sim-
ple calculations, there are other mechanisms which we have
not discussed. These require more extensive computations
to predict, and quickly leave the realm of reasonable strate-
gies. Therefore we have concluded that it is most sensible
to leave common factor identiﬁcation to the ﬁnal stage of
decomposition.

We hope that reduced decompositions can be implemented

as the standard form in future computer-algebra systems.

8. ACKNOWLEDGMENTS

This work was supported in part by the Austrian Science

Fund (FWF) grant SFB50 (F5009-N15).

We would like to thank Prof. Kevin G. Hare and Univ.-

Doz. Dr. Arne Winterhof for helpful.

9. REFERENCES
[1] E. H. Bareiss. Sylvester’s identity and multistep

integer-preserving Gaussian elimination. Mathematics
of Computation, 22(103):565 – 578, 1968.

[2] J.-G. Dumas, C. Pernet, and Z. Sultan. Computing

the rank proﬁle matrix. In D. Robertz, editor,
Proceedings of the 2015 International Symposium on
Symbolic and Algebraic Computation, ISSAC’15,
pages 149–156. ACM, ACM Press, 2015.

[3] ´U. Erlingsson, E. Kaltofen, and D. Musser. Generic
Gram—Schmidt orthogonalization by exact division.
In International Symposium on Symbolic and Algebraic
Computation, pages 275–282. ACM press, 1996.

[4] K. Geddes, G. Labahn, and S. Czapor. Algorithms for

Computer Algebra. Kluwer, 1992.

[5] M. W. Giesbrecht and A. Storjohann. Computing

rational forms of integer matrices. Journal of Symbolic
Computation, 34(3):157–172, 2002.

[6] K. G. Hare. Personal Communication.
[7] D. J. Jeﬀrey. LU factoring of non-invertible matrices.

Comm. Comp. Alg., 44(171):1–8, 2010.

[8] E. Kaltofen and G. Yuhasz. A fraction free matrix
Berlekamp/Massey algorithm. Linear Algebra and
Applications, 439(9):2515–2526, 2013.

[9] H. R. Lee and B. D. Saunders. Fraction free Gaussian

elimination for sparse matrices. J. Symbolic
Computation, 19:393–402, 1995.

[10] G. C. Nakos, P. R. Turner, and R. M. Williams.

Fraction-free algorithms for linear and polynomial
equations. SIGSAM Bull., 31(3):11–19, 1997.

[11] M. Newman. Integral Matrices, volume 45 of Pure and

Applied Mathematics. Academic Press, New York,
1972.

[12] C. Pauderis and A. Storjohann. Computing the

invariant structure of integer matrices: fast algorithms
into practice. In M. Kauers, editor, Proceedings of the
International Symposium on Symbolic and Algebraic
Computation, ISSAC’13. ACM Press, 2013.
[13] L. Pursell and S. Y. Trimble. Gram-Schmidt

orthogonalization by Gaussian elimination. American
Math. Monthly, 98(6):544–549, 1991.

[14] A. Winterhof. Personal Communication.
[15] W. Zhou and D. J. Jeﬀrey. Fraction-free matrix

factors: new forms for LU and QR factors. Frontiers
of Computer Science in China, 2(1):67–80, 2008.

