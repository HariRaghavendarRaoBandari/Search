6
1
0
2

 
r
a

M
2

 

.

 
 
]
E
P
o
i
b
-
q
[
 
 

1
v
2
0
8
0
0

.

3
0
6
1
:
v
i
X
r
a

Digital Evolution Unravels Selective Pressures to Avoid Collision in Drosophila

Flies as Ship Captains?

Ali Tehrani-Saleh1,2 and Christoph Adami2,3,4

1Computer Science and Engineering, Michigan State University

2BEACON Center for the Study of Evolution in Action

3Microbiology and Molecular Genetics, Michigan State University

4Physics and Astronomy, Michigan State University, East Lansing, MI 48824

adami@msu.edu

Abstract

Flies that walk in a covered planar arena on straight paths
avoid colliding with each other, but which of the two ﬂies
stops is not random. High-throughput video observations,
coupled with dedicated experiments with controlled robot
ﬂies have revealed that ﬂies utilize the type of optic ﬂow on
their retina as a determinant of who should stop, a strategy
also used by ship captains to determine which of two ships
on a collision course should throw engines in reverse. We use
digital evolution to test whether this strategy evolves when
collision avoidance is the sole penalty. We ﬁnd that the strat-
egy does indeed evolve in a narrow range of cost/beneﬁt ra-
tios, for experiments in which the “regressive motion” cue is
error free. We speculate that these stringent conditions may
not be sufﬁcient to evolve the strategy in real ﬂies, pointing
perhaps to auxiliary costs and beneﬁts not modeled in our
study.

Introduction

How animals make decisions has always been an interesting,
yet controversial, question to scientists (McFarland, 1977)
and philosophers alike. Animals obtain various types of
sensory information from the environment and then process
these information streams so as to take actions that beneﬁt
them in survival and reproduction. Visual system plays an
important role in providing animals with information about
their environment, for example when foraging for food, de-
tecting predators or prey, and when searching for potential
mates. One of the primary components of visual information
is motion detection. Motion is a fundamental perceptual di-
mension of visual systems (Borst and Egelhaaf, 1989) and is
a key component in decision making in most animals. Here,
we study a very particular type of motion detection and
concomitant behavior (collision avoidance) in Drosophila
melanogaster (the common fruit ﬂy), and attempt to unravel
the selective (evolutionary) pressures that might have given
rise to this behavior.

D. melanogaster shows a striking difference in behavior
when exposed to two different types of optical ﬂow. Bran-
son and colleagues recorded the interaction of groups of fruit
ﬂies in a planar covered arena (so that they could only walk,
not ﬂy) and used machine-vision algorithms to analyze the

walking trajectories in order to study ﬂy behavior (Bran-
son et al., 2009). Their analysis revealed that female fruit
ﬂies stop walking when they perceive another ﬂy’s motion
from back-to-front in their visual ﬁeld (an optical ﬂow re-
ferred to as “regressive motion”) whereas they keep walking
when perceiving conspeciﬁcs moving from front-to-back in
their visual ﬁeld, referred to as “progressive motion” (see
ﬁgure 1). Zabala and colleagues further investigated this be-
havior and tested the “regressive motion saliency” hypothe-
sis, suggesting that ﬂies stop walking when perceiving re-
gressive motion (Zabala et al., 2012). They used a pro-
grammable ﬂy-sized robot interacting with a real ﬂy to ex-
clude other sensory cues such as image expansion (“loom-
ing”, see Schiff et al. 1962) and pheromones. Their results
provide rigorous support for the regressive motion saliency
hypothesis.

Subsequently, Chalupka and colleagues (2015) coined
the term “generalized regressive motion” for optic ﬂows
in which images move clockwise on the left eye and con-
versely, counterclockwise on the right eye, see Fig 1. They
presented a geometric analysis for two ﬂies moving on
straight, intersecting trajectories with constant velocities and
showed that the ﬂy that reaches the intersection ﬁrst always
perceives progressive motion on its retina, whereas the one
that reaches the intersection later perceives regressive mo-
tion at all times before the other ﬂy reaches the intersection.
They went on to suggest that this behavior is a strategy to
avoid collisions during locomotion similar to the rules that
ship captains use when moving on intersecting paths (see,
e.g., Maloney 1989). As intriguing as this hypothesis may
seem, it is not clear a priori which selective pressures or en-
vironmental circumstances could give rise to this behavior.
For example, it is unclear whether collision avoidance pro-
vides a signiﬁcant enough ﬁtness beneﬁt. As a consequence,
it is possible in principle that the behavior has its origin in
a completely different cognitive constraint that is fundamen-
tally unrelated to collision avoidance, or to the rules that ship
captains use to navigate the seas. While such questions tra-
ditionally must be relegated to ﬁre-side discussions amongst
behavioral biologists, Artiﬁcial Life offers unique opportu-

Figure 1: An illustration of regressive (back-to-front, left)
and progressive (front-to-back, right) optic ﬂows in a ﬂy’s
retina.

nities to test them directly.

In this study, we tested whether collision avoidance can be
a sufﬁcient selective pressure for the described behavior to
evolve. We also investigated the environmental conditions
under which this behavior could have evolved, in terms of
the varying costs and beneﬁts involved. By using an agent-
based computational model (described in more detail be-
low), we studied how the interplay (and trade-offs) between
the necessity to move and the avoidance of collisions can re-
sult in the evolution of regressive motion saliency in digital
ﬂies.

Digital evolution is currently the only technique that can
study hypotheses concerning the selective pressures neces-
sary (or even sufﬁcient) for the emergence of animal behav-
iors, as experimental evolution with animal lines of thou-
sands of generations is infeasible. In digital evolution, we
can study the interplay between multiple factors such as se-
lective pressures, environmental conditions, population size
and structure, etc. For example, Olson et al. (2013) used dig-
ital evolution to show that predator confusion is a sufﬁcient
condition to evolve swarming behavior, but they also found
that collective vigilance can give rise to gregarious foraging
behavior in group-living organisms (Olson et al., 2015). In
principle, any one hypothesis favoring the emergence of be-
havior can be tested in isolation, or in conjunction (Olson
et al., 2015).

Methods

Markov Networks
We use an agent-based model to simulate the interaction
of walking ﬂies with moving objects (here, potentially con-
speciﬁcs) in a two-dimensional virtual world. Agents have
sensors to perceive their surrounding world (details below)
and have actuators that enable them to move in the envi-
ronment. Agent brains in our experiment have altogether
twelve sensors, three internal processing nodes, and one out-
put node (the actuator). The brain controlling the agent is
a “Markov network brain (MNB)”, which is a probabilis-
tic controller that makes decisions based on sensory inputs

Figure 2: An illustration of probabilistic logic gates in
Markov network brains. This gate has three inputs and two
outputs. One of the outputs writes into one of the inputs of
this gate, so its output is “hidden”. Because after ﬁring all
Markov neurons automatically return to the quiescent state,
values can only be stored (kept in memory) by actively main-
taining them. Probability table shows the probability of each
output given input values.

and internal nodes (Edlund et al., 2011) Each node in the
network (i.e., sensors, internal nodes, and actuators) can
be thought of as a digital (binary) neuron that either ﬁres
(value=1), or is quiescent (value=0). Nodes of the network
are connected via hidden Markov gates (HMG) that func-
tion as probabilistic logic gates. Each HMG is speciﬁed by
its inputs, outputs, and a state transition table that speciﬁes
the probability of each output state based on input states
(ﬁgure 2). For example, in the transition table of ﬁgure 2
(a three-input, two-output gate), the probability p73 controls
the likelihood that the output state is 3 (the decimal equiv-
alent of the binary pattern 11, that is, both output neurons
ﬁre) given that the input happened to be state 7 (the decimal
translation of 111, i.e., all inputs are active). Markov brain
networks can consist of any number of HMGs with any pos-
sible connection arrangement, given certain constrains (see
for example Edlund et al. 2011).

The number of gates, their connections, and how they
work is subject to evolution and changes across individu-
als and through generations. For this purpose, the agent’s
brains are encoded in a genome, which is an ordered se-
quence of integers, each in the range [0,255], i.e. one byte.
Each integer (or byte) is a locus in the genome and speciﬁc
sequences of loci construct genes, where each gene codes
for one gate. The “start” codon for a gene (the sequence
that determines the beginning of the gene) in our encod-
ing is the pair (42,213) (these numbers are arbitrary). Each
gene encodes exactly one HMG, for example as shown in
ﬁgure 3. The gene speciﬁes the number of inputs/outputs
in each HMG, which nodes it reads from and writes to (the
connectivity) and the probability table that determines the
gates’ function. As shown in ﬁgure 3, the ﬁrst two bytes are
the start codon, followed by one byte that speciﬁes the num-

Figure 3: An illustration of a portion of genome containing two genes that encode two HMGs. The ﬁrst two loci represent start
codon (red blocks), followed by two loci that determine the number of inputs and outputs respectively (green blocks). The next
four loci specify which nodes are inputs of this gate (blue blocks) and the following four specify output nodes (yellow blocks).
The remaining loci encode the probabilities of HMG’s logic table (cyan blocks).

ber of inputs and one byte for the number of outputs. The
bytes are modulated so as to encode the number of inputs
and outputs unambiguously. For example, the bytes encod-
ing the number of inputs is an integer in [0,255] whereas a
Markov gate can take a maximum of four inputs, thus we use
a mapping function that generates a number ∈ [1,4] from the
value of this byte. The next four bytes specify the inputs of
the HMG, followed by another four bytes specifying where
it writes to. The remaining bytes of the gene are mapped
to construct the probabilistic logic gate table. The genomes
are initialized with 5000 random integers and they contain
four start codons to speed up the evolution at the beginning
of the experiment. We can also force the gates to be deter-
ministic rather than probabilistic (all values in the logic table
are 0 or 1), which turns our HMGs into classical logic gates.
Markov network brains have been used extensively in the
last ﬁve years to study the evolution of navigation (Edlund
et al., 2011; Joshi et al., 2013), the evolution of active cate-
gorical perception (Marstaller et al., 2013; Albantakis et al.,
2014), the evolution of swarming behavior as noted earlier,
as well as how visual cortices (Chapman et al., 2013) and
hierarchical groups (Hintze and Miromeni, 2014) form.
Experimental Conﬁgurations
We construct an initial population of 100 agents, i.e., digital
ﬂies, each with a genome initialized with 5,000 random in-
tegers containing four start codons (to accelerate early evo-
lution). Agents (and by proxy the genomes that determine
them) are scored based on how they perform in their liv-
ing environment. The population of genomes is updated
via a standard Genetic Algorithm (GA) for 50,000 gener-
ations, where the next generation of genomes is constructed
via roulette wheel selection combined with mutations (de-
tailed GA speciﬁcations are listed in table 1). There is no
crossover or immigration in our GA implementation.

Each digital ﬂy is put in a virtual world for 25,000 time
steps where it can prove its prowess. During each time step
in the simulation, the agent perceives its surrounding en-
vironment, and makes movement decisions. The sensory

system of a digital ﬂy is designed such that it can see sur-
rounding objects within a limited distance of 250 units, in a
280 degrees pixelated retina shown in ﬁgure 4. The state of
each sensor node is 0 (inactive) when it does not sense any-
thing within the radius, and turns to 1 (active) if an object
is projected at that position in the retina. Agents in this ex-
periment have one actuator node that enables them to move
ahead or stop, for active (ﬁring) and non-active (quiescent)
states respectively. In our experiment, digital ﬂies exist in an

GA Parameters

Population size
Generations
Point mutation rate
Gene deletion rate
Gene duplication rate
Initial genome length
Initial start codons
Crossover
Immigration

100
50,000
0.5%
2%
5%
5000
4
None
None

Environment Parameters

Vision range
Field of vision
Collision range
Agent velocity
Event time steps
Number of events
Moving reward
Collision penalty
Replicates

250
280 deg
60
15
250
100
0.004
1,2,3,5,10
20

Table 1: Conﬁgurations for GA and Environmental setup

environment where they should move to gain ﬁtness, repre-
senting the fact that organisms should forage for resources,
mates, and avoiding predators. Thus, the ﬁtness function
is set so that agents are rewarded for moving ahead at each
update of the world, and are penalized for colliding with ob-
jects. The amount of ﬁtness they gain for moving (the ben-
eﬁt) is characteristic of the environment, and we change it
in different treatments. The penalty for collisions represents
the importance of collision avoidance for their survival and
reproduction, and we vary this cost also. Each digital ﬂy
sees 100 moving objects (one at a time) during its lifetime,
and we say that it experiences 100 “events”. The penalty-
reward ratio (PR) determines the amount of penalty of col-
lision divided by the reward for moving during the entirety
of an event. So for example, PR=1 means the agent loses all
the rewards it gained by walking during the whole event if it

Figure 4: An illustration of a digital ﬂy and its visual ﬁeld
in the model. Flies have a pixelated (12 pixels) retina that is
able to sense surrounding objects in 280 degrees and within
a limited distance (250 units). The red circle is an exter-
nal object that can be detected by the agent within its vision
ﬁeld. Activated sensors are shown in red, while inactive sen-
sors are blue. In (a) the object is outside of the visual ﬁeld
of the agent, in (b) the object is detected in one sensor, and
in (c) the object activates two sensors.

(cid:88)

collides with the object in that event:

ﬁtness =

(reward − PR × collision) ,

(1)

events

where reward ∈ [0, 1] reﬂects how many time steps the
agent moved during the event. Our experiments are con-
structed such that all objects that produce regressive motion
in the digital retina will collide with the ﬂy if it keeps mov-
ing. The reason for biasing our experiments in this manner
is explained in the following section.
Collision Probability in Events with Regressive
Optic Flow
As mentioned earlier, Chalupka et al. (2015) showed that for
two ﬂies moving on straight, intersecting trajectories with
constant velocities, the ﬂy that reaches the intersection ﬁrst
always perceives progressive motion on its retina while the
counterpart that reaches the intersection later perceives re-
gressive motion at all times before the ﬁrst ﬂy reaches the
intersection. However, this does not imply that all objects
that produce a regressive motion on a ﬂy’s retina will nec-
essarily collide with it. In this section we present a mathe-
matical analysis to discover how often objects that produce
regressive motion in the ﬂy’s retina will eventually collide
with the ﬂy if the latter does not stop.

Suppose a ﬂy moves on a straight line with constant ve-
locity Vﬂy and an object is also moving on a straight line
with constant velocity Vobj (ﬁgure 5-a). The ﬂy is able to
perceive objects within distance Rvis, its vision range (ﬁg-
ure 5-a). The object is assumed to be a point in the plane and
the distance between this point and the center of the visual
ﬁeld of the ﬂy is deﬁned to be the distance between them.
We deﬁne “the onset of the event” as the ﬁrst time the object
is detected by the ﬂy. At the onset of the event, the object
is at the distance Rvis of the ﬂy at relative azimuthal angle
α ∈ [0, π
2 ] (ﬁgure 5-a). We assume that the object can be

Figure 5: An illustration of a moving ﬂy at the onset of the
event.

at any relative position Rvis = (Rvis, α)1 with equal prob-
abilities (the probability distribution of α is uniform around
the ﬂy). The velocity of the object can be represented as
Vobj = (Vobj, θ) where θ ∈ [−π
2 ] (note that Vobj is con-
stant). We also assume that the velocity of the object can
point in all directions with equal probabilities (the probabil-
ity distribution of θ is uniform). The relative velocity of the
object with respect to the ﬂy is Vrel = Vobj − Vﬂy (ﬁgure
5). Since both Vobj and Vﬂy are constant, Vrel is also a
constant vector.

2 , π

Proposition 1. A moving object produces regressive mo-
tion on a ﬂy’s retina if:

θ > −α + arcsin(

Vﬂy
Vobj

cos α) .

(2)

Proof.
In order for the object to produce regressive motion
on the retina, the relative velocity should be pointed above
the center point O. The relative velocity direction γ can be
found awith Vrel = (Vrel, γ), as

(cid:18) Vobj sin θ − Vﬂy

(cid:19)

Vobj cos θ

.

(3)

(cid:18) Vrely

(cid:19)

Vrelx

γ = arctan

= arctan

The angle γ should be greater than the central angle (ﬁgure
5-b), that is, γ > −α. Replacing γ and simplifying, we
obtain:

θ > −α + arcsin(ν cos α),

ν =

Vﬂy
Vobj

.

(4)

For smaller values of θ, the object produces progressive op-
tic ﬂow. We thus deﬁne θmin = −α + arcsin(ν cos(α)) as
the minimum angle θ that produces regressive motion on the
retina.

Deﬁnition 1. The object remains “observable” to the ﬂy
after the onset of the event if its relative velocity is directed
toward the inside of the ﬂy’s vision ﬁeld (to the left of the
tangent line δ1 in ﬁgure 5-b).

1Here and below, we represent vectors either in boldface or by
the parameters that determine them within a planar polar coordinate
system. Thus the vector R is represented by (|R|, φ), where Rx =
R cos φ and Ry = R sin φ.

Proposition 2. The object remains observable to the ﬂy if:

θ < arccos(− Vﬂy
Vobj

sin α) − α .

(5)

Proof. According to the deﬁnition the sufﬁcient condition
for observability is that γ should be less than the tangent line
δ1 angle: γ < −α + π
2 . Replacing γ and simplifying we
obtain

θ < arccos(−ν sin α) − α .

(6)

For greater values of θ, the object will be out of vision range
of the ﬂy. Thus the maximum value that θ can take on is:

θmax = arccos(−ν sin α) − α .

(7)

In order for the object to produce regressive motion on ﬂy’s
retina and also remain observable to the ﬂy, relative velocity
should be within the arc ψ (ﬁgure 5-b).

Deﬁnition 2. The object collides with the ﬂy if its distance
with the ﬂy is less than “collision range” Rcoll (ﬁgure 5-b).

Proposition 3. An object that creates regressive optic ﬂow
on the ﬂy’s retina and remains observable will collide with
it if:

θ < φ + arcsin(ν cos φ), φ = arcsin(

) − α .

Rcoll
Rvis

(8)

Proof. The relative velocity of such object is within arc ψ.
This object will collide with the ﬂy if its relative velocity is
in the range β, i.e. lower than tangent line to collision circle
(ﬁgure 5-b). This condition holds true if:

γ < β − α,

β = arcsin(

(9)
and φ = β − α. Replacing γ and rearranging

Rcoll
Rvis

) .

Let ρ = Rcoll
Rvis
gives:

θ < φ + arcsin(ν cos φ) .

(10)

For greater values of θ, the object produces regressive mo-
tion on the ﬂy’s retina but does not collide with it. So the
threshold collision angle is given by:

θcol = φ + arcsin(ν cos φ) .

(11)

As mentioned, we assume that the probability distribution of
the direction of the object velocity, θ is uniform.

Deﬁnition 3. For an object at initial position α, the proba-
bility Πcoll is the range of velocity directions θ such that the
object collides with the ﬂy divided by the range of directions
with which it creates regressive optic ﬂow on ﬂy’s retina (see
ﬁgure 5-b):

Πcoll(α, ν, ρ) =

θcol − θmin
θmax − θmin

.

(12)

We conducted experiments with ﬁve different ﬁtness func-
tions representing different environments. Environments

Figure 6: Probability of collision Πcoll(ν, ρ) with an object
that creates regressive motion on the retina as a function of
the ratio of vision radius to collision radius ρ, for different
ﬂy-object velocity ratios ν.

Integrating this function over the range of possible initial
relative positions, the probability that an event results in a
collision given that the object produces regressive motion
on an ﬂy’s retina can be found as:

αmax(cid:90)

Πcoll(ν, ρ) =

Πcoll(α, ν, ρ)dα ,

(13)

αmin

where αmin is either 0 or the minimum value of α for which
there exists a θ with which the object can produce a regres-
sive motion on ﬂy’s retina, and αmin is either 90 or maxi-
mum value of α for which there exists a θ with which the
object remains observable to the ﬂy.

We calculated the integral (13) numerically and show the
results in ﬁgure 6 for different values of ﬂy-object veloc-
ity ratios ν and different collision range-vision range ratios
ρ. As can be seen from ﬁgure 6, for Rvis=60 mm (Za-
bala et al., 2012) and Rcoll=15 mm (our assumption), the
collision probability is around 0.2-0.3. This implies that if
encounters are created randomly, regressive motion on the
retina is not predictive of collision, and as a consequence it
is unreasonable to expect that digital evolution will produce
collision avoidance in response, as only 1 in 5 to 1 in 3 re-
gressive motions actually lead to collisions. This was borne
out in experiments, and we thus decided to bias the events in
such a manner that all events that leave a regressive motion
signature in the retina will lead to collision. Note that this
is not necessarily an unrealistic assumption, as we have not
analyzed a distribution of realistic “events” (such as is avail-
able in the data set of (Branson et al., 2009)). It could very
well be that the way real ﬂies approach each other differs
from the uniform distributions that went into the mathemat-
ical analysis presented here.

Results

differ in the amount of ﬁtness individuals gain when moving
and in the penalty incurred by a collision. Evolved agents
use various strategies to avoid collisions and maximize the
travelled distance, but one of the most successful strategies
they use is indeed to categorize visual cues into regressive
and progressive optic ﬂows. We ﬁnd that agents categorize
these visual cues only in some regions of the retina: the re-
gions in which collisions take place more frequently. They
then use this information to cast a movement decision: they
keep moving when seeing an object creating progressive op-
tic ﬂow on their retina, and stop when the object creates re-
gressive optic ﬂow on their retina. However, they do not
stop for the entire duration of the event, i.e., the whole time
they perceive regressive optic ﬂow. Rather they stop during
only a portion of the event, which helps the agent to avoid
a collision with the object while maximizing their walking
duration and hence gaining higher ﬁtness.

The strategy of using regressive motion as a cue for colli-
sion (Chalupka et al., 2015), similar to the observed behav-
ior in fruit ﬂies (Zabala et al., 2012) thus evolves in our ex-
perimental setup under some environmental circumstances
(discussed below). We refer to this strategy as regressive-
collision-cue (RCC) and we deﬁne it in our experimental
setup as follows:
1) if the moving object produces regressive motion on
agent’s retina during an event and the agent stops at least
for some amount of time during that event, or
2) if the moving object produces progressive motion on
agent’s retina during an event and the agent does not stop
at all during that event. The number of events (out of 100)
in which the agent uses this strategy is termed the “RCC
value”.

We now discuss the results of an experiment in which
the RCC strategy has evolved. We take the most successful
agent at the end of that experiment and analyze its behavior.
This agent evolved in an environment with penalty-reward
ratio of 2, meaning the penalty of each collision equals twice
the maximum reward the agent can gain in 2 events. Fig-
ure 7 shows whether the agent stopped during an event, stop
probability (in blue), as a function of the angular velocity
of the image on the agent’s retina for 100 events. In that
ﬁgure, the angular velocity of the image on agent’s retina is
positive for regressive optic ﬂow and negative for progres-
sive events. Simulations units are converted to plotted values
(in deg/s and mm/s) by equalizing dimensionless values ν,
and ρ in simulation and actual values: Rvis=60 mm (Zabala
et al., 2012), Vﬂy=20 mm/s (Zabala et al., 2012), Rcoll=15
mm (our assumption). It can be seen from the ﬁgure that
out of all 100 events, during one event with a regressive mo-
tion it did not stop at all and for two progressive events it
stopped. In the remaining events it uses the RCC strategy
(RCC value=97). The average velocity of the agent during
each event is also shown (in grey), which reﬂects the num-
ber of time steps the agent moves during that event, and thus

Figure 7: The stop probability of the evolved agent vs. the
angular velocity of the image on its retina for 100 events.
Positive values of angular velocity show progressive motion
events and negative angular velocities stand for regressive
motion events. The average velocity of the agent is also
shown during each event.

indirectly how often it stops. For progressive motions, the
stop probability is zero (the agent does not stop during the
event) and thus the velocity of the agent is maximal during
that event. For regressive optic ﬂow (negative angular ve-
locities), the average velocity during each event is less than
maximum and for extreme angular velocities, it needs to stop
for shorter durations to avoid collisions. In order to quantita-
tively analyze how using regressive motion as a collision cue
beneﬁts agents to gain more ﬁtness, we traced this particular
agent’s evolutionary line of descent (LOD) by following its
lineage backward for 20,000 generations mutation by muta-
tion until we reached the random agent that we used to see
the initial population (see Lenski et al. 2003 for more de-
tails on how to construct evolutionary lines of descent for
digitals). Figure 8 shows the ﬁtness and the RCC value vs.
generation for this agent’s LOD. It is evident from these re-
sults that evolving this strategy beneﬁts agents in gaining
ﬁtness compared to the rest of the population in this envi-
ronment as high peaks of ﬁtness occur at high RCC values
and conversely, the ﬁtness drops as the RCC value decreases.
Nevertheless, this strategy does not evolve all the time. Fig-
ure 9 shows the ﬁtness and RCC for all 20 replicates in the
environment with penalty-reward ratio of 2. We can see that
the mean ﬁtness of all 20 replicates is around 20% less than
the ﬁtness of the agent that evolved the RCC strategy. The
mean RCC value for all 20 replicates is also 20% less than
that of an agent that evolved the RCC strategy. The difﬁculty
to evolve the RCC strategy is not limited to the number of
runs in which this behavior evolved out of all replicates in
some environment (we also tried running the experiment for
longer evolutionary times but the results do not change sig-
niﬁcantly). Environmental conditions also play a key role
in the evolution of this behavior. Figure 10 shows the RCC
value distribution for 20 replicates in ﬁve different environ-
ments. In order to calculate the RCC value in each replicate,

Discussion

We used an agent-based model of ﬂies equipped with
Markov brain networks that evolve via a GA to study the se-
lective pressures and environmental conditions that can lead
to the evolution of collision avoidance strategies based on
visual information. We speciﬁcally tested cognitive mod-
els that invoke “regressive motion saliency” and “regressive
motion as a cue for collision” to understand how ﬂies avoid
colliding with each other in two-dimensional walks. We
showed that it is possible to conﬁgure the experiment in such
a manner that “regressive-collision-cue” (RCC) evolves as
a strategy to avoid collisions. However, the conditions un-
der which the RCC strategy evolved in our experiments are
limited: the strategy only evolved in a narrow range of en-
vironmental conditions and even in those environments, it
does not evolve all the time. In addition, we showed that
from general principles, only a small percentage of events in
which an agent perceives regressive optical ﬂow eventually
leads to a collision, so that RCC as sole strategy is expected
to have a large false positive rate, leading to unnecessary
stops.

As discussed in the methods section, our experimental im-
plementation is biased in such a way that all regressive mo-
tion events lead to a collision if the agent does not stop dur-
ing that event. If the moving object’s velocity direction is
distributed uniformly randomly in all directions, the proba-
bility that a regressive event ends up in a collision is rather
low (≈ 20% in our implementations). Because the false pos-
itive rate of using regressive optical ﬂow only as a predictor
of collisions is liable to thwart the evolution of an RCC strat-
egy, we biased our setup in such a way that the false-positive
rate is zero. We now argue that this bias in our implemen-
tations does not signiﬁcantly inﬂuence the outcome of our
experiments. Consider an environment in which only a per-
centage of events with regressive motion end up in collision.
This is similar to an environment with a lower penalty for
collisions (as long as the strategy evolves at all) since the
agent’s ﬁtness is scored at the end of its lifetime (all 100
events) not during each event.

However, there is a difference a between lower percentage
of collisions in regressive events and lower penalty for col-
lisions, namely a lower probability of collision in regressive
motion events is equivalent to a higher amount of noise in
the cue that the agent takes from the environment, compared
to the case of lower penalties for collision. In other words,
if 100% of all regressive motion events lead to collisions,
the agent associates regressive motion events with collisions
with certainty. Thus, implementing the experiments with
100% collisions in regressive motion events is tantamount
to eliminating the noise in sensory information, which gen-
erally aids evolution. Compensating for noise in sensory
information could also be achieved if we scored agents in
every single event, and informed them about their perfor-
mance in that event (feedback learning). We did not use

Figure 8: Fitness and regressive-collision-cue (RCC) value
on the line of descent for an agent that evolved RCC as a
strategy to avoid collisions. Only the ﬁrst 20,000 genera-
tions are shown for every 150 generations.

Figure 9: Mean values of ﬁtness and regressive-collision-
cue (RCC) over all 20 replicates vs evolutionary time in the
line of descent in the environment with penalty-reward ra-
tio of 2. Standard error lines are shown with shaded areas
around mean values. Only the ﬁrst 20,000 generations are
shown for every 150 generations.

we took the average of the RCC value in the last 1,000 gener-
ations on the line of descent to compensate for ﬂuctuations.
We observe that the RCC strategy only evolves in a narrow
range of penalty-reward ratio, namely for PR=2 and PR=3.
According to ﬁgure 10, higher values of penalty on the one
hand discourage the agents from walking in the environment
(they simply choose to remain stationary), and therefore pre-
vent them from exploring the ﬁtness landscape. Lower val-
ues for the penalty, on the other hand, result in indifference
to collisions and thus, the optimal strategy (probably the lo-
cal optimum) in these environments is to keep walking and
ignore all collisions. In lower values of penalty, the RCC
value is 55% which means they evolve to stop in obvious
cases that end up in collision (if they keep moving without
stopping at all, the RCC value should be 50).

Branson, K., Robie, A. A., Bender, J., Perona, P., and Dick-
inson, M. H. (2009). High-throughput ethomics in large
groups of drosophila. Nature Methods, 6:451–457.

Chalupka, K., Dickinson, M., and Perona, P. (2015). Gen-
eralized regressive motion: A visual cue to collision.
arXiv preprint arXiv:1510.07573.

Chapman, S., Knoester, D., Hintze, A., and Adami, C.
(2013). Evolution of an artiﬁcial visual cortex for im-
age recognition. In Advances in Artiﬁcial Life, ECAL
12, pages 1067–1074.

Edlund, J. A., Chaumont, N., Hintze, A., Koch, C., Tononi,
G., and Adami, C. (2011). Integrated information in-
creases with ﬁtness in the evolution of animats. PLoS
Comput Biol, 7:e1002236.

Hintze, A. and Miromeni, M. (2014). Evolution of au-
tonomous hierarchy formation and maintenance. In AL-
IFE 14: The Fourteenth Conference on the Synthesis
and Simulation of Living Systems, pages 366–367.

Joshi, N. J., Tononi, G., and C., K. (2013). The minimal
complexity of adapting agents increases with ﬁtness.
PLoS Comput Biol, 9:e1003111.

Lenski, R. E., Ofria, C., Pennock, R. T., and Adami, C.
(2003). The evolutionary origin of complex features.
Nature, 423(6936):139–144.

Maloney, E. S. (1989). Chapman Piloting, Seamanship and

Small Boat Handling. Hearst Marine Books.

Marstaller, L., Hintze, A., and Adami, C. (2013). The evo-
lution of representation in simple cognitive networks.
Neural Comput, 25:2079–2107.

McFarland, D. J. (1977). Decision making in animals. Na-

ture, 269:15–21.

Olson, R. S., Haley, P. B., Dyer, F. C., and Adami, C. (2015).
Exploring the evolution of a trade-off between vigi-
lance and foraging in group-living organisms. Royal
Society open science, 2:150135.

Olson, R. S., Hintze, A., Dyer, F. C., Knoester, D. B., and
Adami, C. (2013). Predator confusion is sufﬁcient to
evolve swarming behaviour. Journal of The Royal So-
ciety Interface, 10:20130305.

Schiff, W., Caviness, J. A., and Gibson, J. J. (1962). Per-
sistent fear responses in rhesus monkeys to the optical
stimulus of “looming”. Science, 136:982–3.

Zabala, F., Polidoro, P., Robie, A., Branson, K., Perona, P.,
and Dickinson, M. H. (2012). A simple strategy for de-
tecting moving objects during locomotion revealed by
animal-robot interactions. Current Biology, 22:1344–
1350.

Figure 10: RCC value distribution in environments with
different penalty-reward ratios. Each box-plot shows RCC
value (averaged over last 1000 generations on the line de-
scent) for 20 replicates.

feedback learning in the present experiments, but Markov
gates equipped with such abilities exist, and we plan to use
them in future experiments.

We conclude that the evolution of “regressive motion
saliency” is unlikely to have happened only due to colli-
sion avoidance as the selective pressure. It is important to
remember that walking is not the most frequent activity in
fruit ﬂies and furthermore that ﬂies do not usually live in
high density colonies and hence, do not ﬁnd themselves on
collision courses very often. It may be the case that com-
ponents of this strategy (namely categorizing the optic ﬂow
as regressive or progressive) have evolved under different
selective pressures entirely unrelated to the present test sit-
uation, and was further evolved to enhance collision avoid-
ance with conspeciﬁcs while moving (a type of exaptation).
For example, detecting predators is a strong selective pres-
sure in the evolution of visual motion detection, including
the categorization of that cue so as to take appropriate ac-
tions.
It may be interesting to study the behavior of ﬂies
interacting with animals or objects that are not perceived as
conspeciﬁcs.

This research has been supported in part by the National
Science Foundation (NSF) BEACON Center under Coop-
erative Agreement DBI-0939454. We gratefully acknowl-
edge the support of the Michigan State University High Per-
formance Computing Center and the Institute for Cyber-
Enabled Research (iCER).

References

Albantakis, L., Hintze, A., Koch, C., Adami, C., and Tononi,
G. (2014). Evolution of integrated causal structures in
animats exposed to environments of increasing com-
plexity. PLoS Comput Biol, 10:e1003966.

Borst, A. and Egelhaaf, M. (1989). Principles of visual mo-

tion detection. Trends Neurosci, 12:297–306.

