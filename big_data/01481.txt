Gibbs Random Fields and Markov Random Fields

1

with Constraints

Levent Onural

Abstract

6
1
0
2

 
r
a

M
4

 

 
 
]

It was shown many times in the literature that a Markov random ﬁeld is equivalent to a Gibbs random ﬁeld
when all realizations of the ﬁeld have non-zero probabilities; the proofs are rather complicated. A simpler proof,
which is based directly on simple probability theory, is presented. Furthermore, it is shown that the equivalence is
still valid when there are constraints (zero probability realizations) of any type. The equivalence extends to inﬁnite
size random ﬁelds, as well.

Markov random ﬁelds, Gibbs random ﬁelds, constrained Markov random ﬁelds, constrained Gibbs random ﬁelds

Index Terms

.

R
P
h
t
a
m

[
 
 

1
v
1
8
4
1
0

.

3
0
6
1
:
v
i
X
r
a

I. INTRODUCTION

It is well known that a Gibbs random ﬁeld has an equivalent Markov random ﬁeld, and vice versa, provided
that all realizations have nonzero probabilities; this requirement is known as the “positivity condition” [1]–[4].
There are, however, many open questions when there are constraints, which in turn, impose zero probabilities to
some outcomes (i.e., when there are impossible outcomes) [5]. Local constraints in the form of zero conditional
probabilities are already incorporated into the theory [6]. Some cases with global constraints are also demonstrated
[7], [8]. The positivity condition is sufﬁcient but not necessary; indeed, there are hints that it might not be needed
for the equivalence. Here in this paper, the basic steps and the associated proofs to construct Gibbs or Markov
random ﬁelds are revisited. Simpler alternative proofs are provided for many cases. It is also proven that Gibbs
random ﬁelds and Markov random ﬁelds are equivalent even if there are impossible (zero probability, forbidden)
outcomes.

II. PRELIMINARIES

Let us start by the set of all outcomes Ω, with its elements ai having all nonzero probabilities: P (ai) = pi > 0,
for all ai. Let us assume that these probabilities of elements, P (ai) = pi may not be known, but all ratios,
P (ai) /P (aj) = pi/pj = rij of such probabilities are known, for all i and j. It is easy to show that these ratios
uniquely specify the unknown P (ai)’s for all i. To show this, let us choose an arbitrary i, and consider all ratios
pj/pi, for all j. Clearly,

(1)

(2)

(3)

which implies

and therefore,

pj =

pi
rij

=

=

1
pi

,

,

pj
pi

Xj

pi =

pj
pi

1

Pj

∀j

,

and for any i

,

pj
pi

1

rijPj

where the sum over j indicates that the summation is running over all elements aj ∈ Ω. Therefore, all unknown
probabilities can be found from the known ratios of those probabilities. The reason to explicitly include Eq.(3) is to

L. Onural is with the Dept. of Electrical and Electronics Eng. of Bilkent University.

stress the fact that only one arbitrary element ai may be chosen as the reference, and all ratios of probabilities are
given with respect to pi as pj/pi, for all j. Indeed, it is sufﬁcient to know only such a set of ratios of probabilities,
instead of all ratios, to ﬁnd probabilities, pj, for all j.

2

Any constraint partitions the sample space Ω into two sets, Ck, and its complement Cc

any ai ∈ C satisﬁes all constraints, and again, the sample space is partitioned into C and Cc.

constraints, labeled by the index k, and in such a case, we are interested in the intersection C = Tk
Now, let us concentrate on the conditional probabilities P (ai(cid:12)(cid:12) C). Simply, from the deﬁnition of the conditional

k. There may be many such
Ck. Therefore,

probabilities,

P (C) = P (ai)

P (C)

P (ai(cid:12)(cid:12) C) = ( P (ai∪C)

0

if ai ∈ C
else

.

Clearly, P (C) > 0 provided that C is not the empty set, since all ai’s have positive probabilities.

Now, let us focus only on those outcomes in C; by the way, conditional probabilities are probabilities, i.e., they

satisfy all axioms and properties of a probability structure. Therefore, we simply state that, i) P (ai(cid:12)(cid:12) C) > 0 for

ai ∈ C, and, ii)

(4)

(5)

=

=

P (aj )
P (C)
P (ai)
P (C)

P (aj)
P (ai)

=

pj
pi

= rij

.

P (aj(cid:12)(cid:12) C)
P (ai(cid:12)(cid:12) C)

Therefore, given two outcomes where both satisfy the constraints, the ratio of their conditional probabilities is still
the same as the ratio of their (unconditional) probabilities, as expected. This observation leads us to the conclusion
that for any subset C, if all the ratios of probabilities (or equivalently, conditional probabilities, given that both
realizations are in C), rij’s, are known, then the conditional probabilities P (aj | C) for all j (cid:127) aj ∈ C are also
known as they are induced by these ratios, following the same steps are in Eqs.(1-3); and furthermore, this will
also induce the (unconditional) probabilities of those elements, as well, if needed.

III. GRF-MRF EQUIVALENCE IN THE PRESENCE OF CONSTRAINTS

Indeed, the equivalence of a GRF to its corresponding MRF, and vice versa, is proven using these ratios of
probabilities; the invariance of these ratios, whether for the unconditional case or the conditional case, assures the
GRF-MRF equivalence even if there are also constraints. And this is true for any constraint, as long as the set of
outcomes satisfying the constraints form a non-empty set. As already stated, constraints mean zero probabilities as
also indicated by Eq.(4).

Starting from the general, and simple, observations above, we can now turn our attention to GRFs and MRFs.
(We will use undertilde to describe random variables; no undertilde will be used for numerical values that these
= x) means the
random variables take. We will use bold fonts to represent vectors (arrays). For example, P (x
“probability that the vector random variable x
(x)
for the same purpose, whenever we feel this notation is more appropriate. Indeed, whenever there is no ambiguity
in the meaning, we will also use the shortened notation P (x) to represent the same probability as described above.
There is no speciﬁc meaning attached to lower case or upper case symbols.) As usual, we assume a set of indexed
random variables, x
l; the number of elements in the set may be ﬁnite of inﬁnite. The index could be called the
“site”, but actually it may or may not be associated with a physical location. The collection of all of those random
variables x
as the “random pattern”. A realization of X
,
is denoted by X which is a pattern over all sites.

takes the speciﬁc vector value x. We will also use the notation Px

l for all l, is a vector random variable X

; we will call X

e

e

e

e

e

As proven many times, conditional probabilities given for the MRF induce joint probabilities for all outcomes
= X, under the positivity condition [1]–[5]. However, the provided proofs are unnecessarily complicated and
X
lengthy. Instead, a simple proof is a direct consequence of the trivial discussion on the ratios of probabilites, as
presented above:

Let us ﬁrst prove that an MRF is also a GRF. As before, the set C contains all outcomes that satisfy the constraints;
therefore, the elements in C all have nonzero probabilities. So, we can safely form the ratios of such probabilities,
as,

e

e

e

e

= Xj(cid:12)(cid:12) C(cid:1)
= Xi(cid:12)(cid:12) C(cid:1) =

P(cid:0)X
P(cid:0)X
e
e

= Xj)
= Xi)

= rij ∀ i, j

.

(6)

P (X
P (X

e
e

3

e

,

(7)

Let us choose patterns X from C such that their values at all sites, except the speciﬁc but arbitrary site l, are the
same. The number of such patterns is at least one; we are interested in cases where this number is greater than one
since otherwise the rest of the discussion is trivial. By the way, the number of such patterns could be quite small
as a consequence of the constraints, and each such distinct pattern has a different realization for x
l. Assuming that
there are more than one such patterns, we can write,

where vj is the realization (value) of the random variable x
Eq.(6) becomes,

l at site l within the pattern Xj. Therefore, the ratio in

=

=

e

m = xm ∀m (cid:127) m 6= l, C)

= Xj(cid:12)(cid:12) C(cid:1) = P(cid:0)x
l = vj(cid:12)(cid:12) x
P(cid:0)X
m = xm ∀m (cid:127) m 6= l, C(cid:1) P (x
e
e
e
e
m = xm ∀m (cid:127) m 6= l, C(cid:17) P(cid:16)x
m = xm ∀m (cid:127) m 6= l, C(cid:17)
P(cid:16)X
= Xj(cid:12)(cid:12) C(cid:17)
P(cid:16)x
l = vj(cid:12)(cid:12) x
P(cid:16)x
P(cid:16)X
= Xi(cid:12)(cid:12) C(cid:17) =
m = xm ∀m (cid:127) m 6= l, C(cid:17) P(cid:16)x
m = xm ∀m (cid:127) m 6= l, C(cid:17)
e
e
e
e
l = vi(cid:12)(cid:12) x
P(cid:16)x
m = xm ∀m (cid:127) m 6= l, C(cid:17)
e
e
e
e
l = vj(cid:12)(cid:12) x
P(cid:16)x
m = xm ∀m (cid:127) m 6= l, C(cid:17)
e
e
l = vi(cid:12)(cid:12) x
P(cid:16)x
m = xm for m ∈ ηl, C(cid:17)
e
e
l = vj(cid:12)(cid:12) x
P(cid:16)x
m = xm for m ∈ ηl, C(cid:17)
e
e
l = vi(cid:12)(cid:12) x
P(cid:16)x
m = xm for m ∈ ηl, C(cid:17) P (C)
e
e
l = vj(cid:12)(cid:12) x
P(cid:16)x
m = xm for m ∈ ηl, C(cid:17) P (C)
e
e
l = vi(cid:12)(cid:12) x
P(cid:16)x
m = xm for m ∈ ηl(cid:17)
e
e
l = vj(cid:12)(cid:12) x
P(cid:16)x
m = xm for m ∈ ηl(cid:17) = rij
e
e
l = vi(cid:12)(cid:12) x
e
e
l = vl(cid:12)(cid:12) x
m = xm for m ∈ ηl, C(cid:1)
P(cid:0)x
e
e
l = vl(cid:12)(cid:12) x
m = xm ∀m (cid:127) m 6= l(cid:1) = P(cid:0)x
e
e

m = xm for m ∈ ηl(cid:1)

l = vl(cid:12)(cid:12) x
e

=

=

,

,

for all l and for all allowed vaules vl for that location are known when there are constraints, or no constraints (i.e.,
when C = Ω, the sample space), and if the Markovianity as indicated by

holds for all l and vl (i.e., if the ﬁeld is a MRF), we can go backwards through the arguments and obtain the result:
probabilities
Given

P(cid:0)x
e
l = vl(cid:12)(cid:12) x
P(cid:0)x
m = xm for m ∈ ηl(cid:1) and the set indicated by constraints C, all ratios rij for all Xi and Xj in
C are known; and from that (due to Eqs.(1-3)) all conditional probabilities P(cid:0)Xi(cid:12)(cid:12) C(cid:1) for all Xi ∈ C are known
e
e
and they are positive; and therefore, we can always write P(cid:0)Xi(cid:12)(cid:12) C(cid:1) = ke−U (Xi) due to nonzero value of P(cid:0)Xi(cid:12)(cid:12) C(cid:1),

where k is just a normalization constant to have the sum of all probabilities equal to one, i.e.,

conditional

(9)

all

where we used the Markovianity, and ηl is the neighborhood associated with site l (see, for example, [5], for the
deﬁnition and the properties of the neighborhood). Therefore, if all conditional probabilities,

(8)

Furthermore, noting that,

k = Xall i (cid:127) Xi ∈ C
P (C) = Xall i (cid:127) Xi ∈ C

e−U (Xi)

.

(10)

ke−U (Xi)

,

(11)

we also know P (C), and therefore, P (Xi) = P(cid:0)Xi(cid:12)(cid:12) C(cid:1) P (C) is also known, and is equal to kP (C)e−U (Xi), for

all Xi, where kP (C) is just the new normalization constant to have the sum of all probabilities equal to one. By
the way, U (X) is called the “energy” of the pattern X. Therefore, a conditional (constraint satisfying) MRF is
also a conditional GRF. The terms “conditional Markov random ﬁelds (CMRF)” and “conditional Gibbs random

ﬁelds (CGRF)” are also used in [7], [8]. Please note that the proof presented above is also valid for the case where
there are no constraints, i.e., when C = Ω, and therefore, we have also provided an alternative proof that a MRF
is equivalent to a GRF under the positivity condition, as well. We believe that this proof is a lot simpler and
straightforward than other known proofs, as given, for example in [1]–[5].

4

Now we will prove that every conditional GRF is also a conditional MRF. If X

is a GRF, then each Xi ∈ C
has a probability in the form ke−U (Xi) as a consequence of the deﬁnition of a GRF; we have shown above that the
(unconditional) probabilities of these patterns are also in the form kP (C)e−U (Xi), where kP (C) is just the related
normalization constant. We can always decompose any function U (Xi) into an additive form as,

e

U (Xi) = Xc ∈ Q

Vc(Xi)

(12)

where c is a “clique” which simply means a subset of indices (“sites”) l. The set Q is the set of all cliques.
The number of elements in a clique ranges from zero up to the maximum number of indices (sites) in a pattern;
therefore, that maximum is inﬁnity for inﬁnite size patterns. Therefore, the ratio of the probabilities of two patterns,
Xj and Xi becomes,

Now, let us assume that the patterns Xi, and Xj differ only at one speciﬁc site l, and they have the same values
at all other sites other than l. In that case, the above ratio becomes,

=

e−U (Xj )
e−U (Xi)

=

Vc(Xj )

Vc(Xi)

− P
e

c ∈ Q

− P
e

c ∈ Q

.

(13)

P(cid:0)X
P(cid:0)X
e
e

Vc(Xj )

Vc(Xi)

− P
e

c ∈ Q

− P
e

c ∈ Q

= Xj(cid:12)(cid:12) C(cid:1)
= Xi(cid:12)(cid:12) C(cid:1) =
= (cid:18)e
(cid:18)e

− P

c ∈ Dl

− P

c ∈ Dl

P (X
P (X

= Xj)
= Xi)

e
e
Vc(Xj)(cid:19) e
Vc(Xi)(cid:19) e

− P

c ∈ Dc
l

− P

c ∈ Dc
l

Vc(Xj)!
Vc(Xi)! =

− P
e

c ∈ Dl

− P
e

c ∈ Dl

Vc(Xj )

Vc(Xi)

,

(14)

where Dl is the set of only those cliques which have different realizations for Xi, and Xj over them, for a given
l. In other words,

where xmi and xmj are the realizations at site m of the patterns Xi, and Xj, respectively. Since Xi, and Xj may
differ only at site l, as a consequence of the above assumption, an equivalent deﬁnition of Dl can be given by

Dl = (cid:8)c (cid:12)(cid:12) xmi 6= xmj for at least one m ∈ c(cid:9)

,

(15)

Please note that Dc

.

P(cid:16)X
P(cid:16)X
e
e

l is the complement of Dl in Q: Dc

l = Q \ Dl. Noting, from Eq.(14), that

Dl = (cid:8)c(cid:12)(cid:12) l ∈ c(cid:9)
= Xj(cid:12)(cid:12) C(cid:17)
P(cid:16)x
m = xm ∀m (cid:127) m 6= l, C(cid:17)
l = vj(cid:12)(cid:12) x
P(cid:16)x
= Xi(cid:12)(cid:12) C(cid:17) =
m = xm ∀m (cid:127) m 6= l, C(cid:17)
e
e
l = vi(cid:12)(cid:12) x
Vc(Xj)!
Vc(Xj )(cid:19) e
e
e
= (cid:18)e
Vc(Xi)! =
Vc(Xi)(cid:19) e
(cid:18)e
l = vj(cid:12)(cid:12) x
P(cid:0)x
m = xm for m ∈ ηl, C(cid:1)
l = vi(cid:12)(cid:12) x
P(cid:0)x
m = xm for m ∈ ηl, C(cid:1)
e
e
e
e

l = vj(cid:12)(cid:12) x
P(cid:0)x
m = xm ∀m (cid:127) m 6= l, C(cid:1)
l = vi(cid:12)(cid:12) x
P(cid:0)x
m = xm ∀m (cid:127) m 6= l, C(cid:1) =
e
e
e
e

− P
e

c ∈ Dl

− P
e

c ∈ Dl

Vc(Xj )

Vc(Xi)

− P

c ∈ Dc
l

− P

c ∈ Dc
l

− P

c ∈ Dl

− P

c ∈ Dl

.

(16)

(17)

,

(18)

and complete the proof that every GRF is also a MRF. Please note that the proof is also valid if there are no
constraints; i.e., when C = Ω.

We conclude that xm, m ∈ ηl are sufﬁcient to compute each term of the last expression above, to yield,

5

IV. OBSERVATIONS, REMARKS AND CONCLUSIONS

Based on the discussions above, we can conclude that,
* The only requirement to write a probability in exponential form is the positivity of that probability; therefore,

any ﬁeld with positive probability realizations is a GRF.

* A GRF has an equivalent MRF, and vice versa, when all the realizations (outcomes) have positive probabilities.
* At this point, one could get the impression that MRFs, or equivalently GRFs, are mathematically so simple:
provided that outcomes (patterns) in a set of outcomes all have non-zero probabilities, the set forms a MRF
(GRF). This is true. However, the MRF model becomes useful only when the neighborhoods ηl are simple
(small in size; i.e., have few indices (sites) in it for every l); equivalently, GRF model becomes useful when
the set of cliques Q contains only simple and few components. An equivalent statement is that the clique
potentials, Vc(X)’s, are zero for most cliques (i.e., for most subsets c of sites). In other words, the beneﬁts
prevail only when the direct statistical interactions among the random variables at different sites are sparse.
Obviously, every ﬁeld with nonzero probability outcomes is a MRF when the sizes of the neighborhoods ηl’s
reach the size of the entire ﬁeld; or equivalently, every ﬁeld with nonzero probability outcomes is a GRF when
the sizes of c’s in Q are allowed to reach the size of the entire ﬁeld (i.e., Dc = ∅). In such cases, one may
still call the ﬁeld as a MRF with the neighborhood size equal to the size of the entire ﬁeld; or may choose to
leave those extreme cases out of the deﬁnition and say that those cases are not MRFs (GRFs); we prefer the
ﬁrst alternative in this paper.

* The neighborhood of a site is not necessarily near (here we assume that the sites form a lattice, and therefore,
a distance measure is applicable) to that site; indeed, the simplicity (sparsity) of a neighborhood scheme is
based on the number of sites in a neighborhood, and not where they are located.

* Extension of the proofs for the Markov case where the “interior” is no longer a single site, but more than one

site, is straightforward.

* The discussion is valid both for causal ﬁelds, as well as non-causal ones; the causality is a direct consequence
of the neighborhood shape (see, for example [9] for the deﬁnition of “causal” and, “non-causal” random ﬁelds).

* GRFs are usually simpler to handle than MRFs.
* Inclusion of constraints, which in turn means zero probability outcomes, does not violate the MRF-GRF
equivalence or the basic structures of GRFs and the MRFs. This is a consequence of the observation that the
constraints result in a smaller set of non-zero probability realizations, and within that smaller set, all other
features that leads to the equivalence is still valid and the same as the unconstrained case.

* These conclusions are valid both for the ﬁnite size or inﬁnite size random ﬁelds.
* Simple proofs are given for above statements; the proofs are direct consequences of elementary probability

theory, and techniques.

The presented discussions clarify and provide answers to those long open questions related to constraints in

MRFs or GRFs.

REFERENCES

[1] J. M. Hammersley and P. Clifford, “Markov random ﬁelds on ﬁnite lattices and graphs” Unpublished, 1971.

[Online]

Available:http://www.statslab.cam.ac.uk/∼grg/books/hammfest/hamm-cliff.pdf

[2] J. Besag, “Spatial

interaction and the statistical analysis of lattice systems” Journal of

the Royal Statistical Society Series B

(Methodological), vol. 36, no. 2, pp 192–236, 1974. [Online] Available: http://www.jstor.org/stable/2984812

[3] F. Spitzer, “Markov random ﬁelds and Gibbs ensembles” The American Mathematical Monthly, vol. 78, no. 2, pp 142–154, 1971.

[Online] Available: http://www.jstor.org/stable/2317621

[4] V. Isham, “An introduction to spatial point processes and Markov random ﬁelds” International Statistical Review / Revue Internationale

de Statistique, vol. 49, no. 1, pp 21–43, 1981. [Online] Available: http://www.jstor.org/stable/1403035

[5] H. Derin and P. A. Kelly, “Discrete-index Markov-type random processes” Proceedings of the IEEE, vol. 77, no. 10, pp 1485–1510,

1989. [Online] Available: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=40665

[6] J. Moussouris, “Gibbs and Markov random systems with constraints” Journal of Statistical Physics, vol. 10, no. 1, pp 11–33 1974.

[Online] Available: http://dx.doi.org/10.1007/BF01011714

[7] L. Onural, “Conditional Markov random ﬁelds applied to textured fractal pattern generation” In Preprint Booklet, The First International

Conference on Statistical Computing, pp. 666, C¸ es¸me, ˙Izmir, Turkey, 1987. (Published abstract, unpublished presented paper).

[8] L. Onural, “Generating connected textured fractal patterns using Markov random ﬁelds” Pattern Analysis and Machine Intelligence, IEEE
Transactions on, vol. 13, no. 8, pp 819–825 1991. [Online] Available: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=85673

[9] C. S. Won and R. M. Gray, Stochastic Image Processing, Springer, 2004.

