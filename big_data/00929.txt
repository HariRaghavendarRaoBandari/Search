6
1
0
2

 
r
a

M
8

 

 
 
]
L
M

.
t
a
t
s
[
 
 

2
v
9
2
9
0
0

.

3
0
6
1
:
v
i
X
r
a

A Kernel Test for Three-Variable Interactions with Random Processes

Paul K. Rubenstein123, Kacper P. Chwialkowski3, Arthur Gretton4

1Machine Learning Group, University of Cambridge

2Empirical Inference, MPI for Intelligent Systems, Tübingen, Germany

3Department of Computer Science, University College London

4Gatsby Computational Neuroscience Unit, University College London

pkr23@cam.ac.uk, kacper.chwialkowski@gmail.com, arthur.gretton@gmail.com

Abstract

We apply a wild bootstrap method to the Lan-
caster three-variable interaction measure in or-
der to detect factorisation of the joint distribution
on three variables forming a stationary random
process, for which the existing permutation boot-
strap method fails. As in the i.i.d. case, the Lan-
caster test is found to outperform existing tests in
cases for which two independent variables indi-
vidually have a weak inﬂuence on a third, but that
when considered jointly the inﬂuence is strong.
The main contributions of this paper are twofold:
ﬁrst, we prove that the Lancaster statistic satis-
ﬁes the conditions required to estimate the quan-
tiles of the null distribution using the wild boot-
strap; second, the manner in which this is proved
is novel, simpler than existing methods, and can
further be applied to other statistics.

1

INTRODUCTION

Nonparametric testing of independence or interaction be-
tween random variables is a core staple of machine learn-
ing and statistics. The majority of nonparametric sta-
tistical tests of independence for continuous-valued ran-
dom variables rely on the assumption that the observed
data are drawn i.i.d. Feuerverger [1993], Gretton et al.
[2007], Székely et al. [2007], Gretton and Gyorﬁ [2010],
Heller et al. [2013]. The same assumption applies to
tests of conditional dependence, and of multivariate inter-
action between variables Zhang et al. [2011], Kankainen
and Ushakov [1998], Fukumizu et al. [2008], Sejdinovic
et al. [2013], Patra et al. [2015]. For many applications
in ﬁnance, medicine, and audio signal analysis, however,
the i.i.d. assumption is unrealistic and overly restrictive.
While many approaches exist for testing interactions be-
tween time series under strong parametric assumptions
Kirchgässner et al. [2012], Ledford and Tawn [1996], the
problem of testing for general, nonlinear interactions has

seen far less analysis: tests of pairwise dependence have
been proposed by Gaisser et al. [2010], Besserve et al.
[2013], Chwialkowski et al. [2014], Chwialkowski and
Gretton [2014], where the ﬁrst publication also addresses
mutual independence of more than two univariate time se-
ries. The two ﬁnal works use as their statistic the Hilbert-
Schmidt Indepenence Criterion, a general nonparametric
measure of dependence [Gretton et al., 2005], which ap-
plies even for multivariate or non-Euclidean variables (such
as strings and groups). The asymptotic behaviour and cor-
responding test threshold are derived using particular as-
sumptions on the mixing properties of the processes from
which the observations are drawn. These kernel approaches
apply only to pairs of random processes, however.
The Lancaster interaction is a signed measure that can be
used to construct a test statistic capable of detecting depen-
dence between three random variables [Lancaster, 1969,
Sejdinovic et al., 2013].
If the joint distribution on the
three variables factorises in some way into a product of a
marginal and a pairwise marginal, the Lancaster interaction
is zero everywhere. Given observations, this can be used to
construct a statistical test, the null hypothesis of which is
that the joint distribution factorises thus. In the i.i.d. case,
the null distribution of the test statistic can be estimated
using a permutation bootstrap technique: this amounts to
shufﬂing the indices of one or more of the variables and re-
calculating the test statistic on this bootstrapped data set.
When our samples instead exhibit temporal dependence,
shufﬂing the time indices destroys this dependence and thus
doing so does not correspond to a valid resample of the test
statistic.
Provided that our data-generating process satisﬁes some
technical conditions on the forms of temporal dependence,
recent work by Leucht and Neumann [2013], building on
the work of Shao [2010], can come to our rescue. The wild
bootstrap is a method that correctly resamples from the null
distribution of a test statistic, subject to certain conditions
on both the test statistic and the processes from which the
observations have been drawn.
In this paper we show that the Lancaster interaction test

statistic satisﬁes the conditions required to apply the wild
bootstrap procedure; moreover, the manner in which we
prove this is signiﬁcantly simpler than existing proofs in the
literature of the same property for other kernel test statis-
tics [Chwialkowski et al., 2014, Chwialkowski and Gretton,
2014]. Previous proofs have relied on the classical theory
of V -statistics to analyse the asymptotic distribution of the
kernel statistic. In particular, the Hoeffding decomposition
gives an expression for the kernel test statistic as a sum of
other V -statistics. Understanding the asymptotic properties
of the components of this decomposition is then conceptu-
ally tractable, but algebraically extremely painful. More-
over, as the complexity of the test statistic under analysis
grows, the number of terms that must be considered in this
approach grows factorially.1 We conjecture that such anal-
ysis of interaction statistics of 4 or more variables would in
practice be unfeasible without automatic theorem provers
due to the sheer number of terms in the resulting computa-
tions.
In contrast, in the approach taken in this paper we explic-
itly consider our test statistic to be the norm of a Hilbert
space operator. We exploit a Central Limit Theorem for
Hilbert space valued random variables Dehling et al. [2015]
to show that our test statistic converges in probability to
the norm of a related population-centred Hilbert space op-
erator, for which the asymptotic analysis is much simpler.
Our approach is novel; previous analyses have not, to our
knowledge, leveraged the Hilbert space geometry in the
context of statistical hypothesis testing using kernel V -
statistics in this way.
We propose that our method may in future be applied to
the asymptotic analysis of other kernel statistics.
In the
appendix, we provide an application of this method to the
Hilbert Schmidt Independence Criterion (HSIC) test statis-
tic, giving a signiﬁcantly shorter and simpler proof than
that given in Chwialkowski and Gretton [2014]
The Central Limit Theorem that we use in this paper makes
certain assumptions on the mixing properties of the ran-
dom processes from which our data are drawn; as further
progress is made, this may be substituted for more up-to-
date theorems that make weaker mixing assumptions.

OUTLINE:
In Section 2, we detail the Lancaster interac-
tion test and provide our main results. These results justify
use of the wild bootstrap to understand the null distribu-
tion of the test statistic.
In Section 3, we provide more
detail about the wild bootstrap, prove that its use correctly
controls Type I error and give a consistency result. In Sec-

1See for example Lemma 8 in Supplementary material A.3
of Chwialkowski and Gretton [2014]. The proof of this lemma
requires keeping track of 4! terms; an equivalent approach for the
Lancaster test would have 6! terms. Depending on the precise
structure of the statistic, this approach applied to a test involving
4 variables could require as many as 8! = 40320 terms.

tion 4, we evaluate the Lancaster test on synthetic data to
identify cases in which it outperforms existing methods, as
well as cases in which it is outperformed. In Section 6, we
provide proofs of the main results of this paper, in particu-
lar the aforementioned novel proof. Further proofs may be
found in the Supplementary material.

2 LANCASTER INTERACTION TEST

2.1 KERNEL NOTATION

Throughout this paper we will assume that the kernels
k, l, m, deﬁned on the domains X , Y and Z respectively,
are characteristic [Sriperumbudur et al., 2011], bounded
and Lipschitz continuous. We describe some notation rel-
evant to the kernel k; similar notation holds for l and m.
Recall that µX := EX k(X,·) ∈ Fk is the mean embed-
(cid:80)n
ding [Smola et al., 2007] of the random variable X. Given
observations Xi, an estimate of the mean embedding is
i=1 k(Xi,·). Two modiﬁcations of k are used
˜µX = 1
n
in this work:

¯k(x, x(cid:48)) = (cid:104)k(x,·) − µX , k(x(cid:48),·) − µX(cid:105),
˜k(x, x(cid:48)) = (cid:104)k(x,·) − ˜µX , k(x(cid:48),·) − ˜µX(cid:105)

(1)
(2)

These are called the population centered kernel and empir-
ically centered kernel respectively.

2.2 LANCASTER INTERACTION

The Lancaster interaction on the triple of random vari-
ables (X, Y, Z) is deﬁned as the signed measure ∆LP =
PXY Z − PXY PZ − PXZPY − PXPY Z + 2PXPY PZ. This
measure can be used to detect three-variable interactions.
It is straightforward to show that if any variable is indepen-
dent of the other two (equivalently, if the joint distribution
PXY Z factorises into a product of marginals in any way),
then ∆LP = 0. That is, writing HX = {X ⊥⊥ (Y, Z)} and
similar for HY and HZ, we have that

HX ∨ HY ∨ HZ ⇒ ∆LP = 0

(3)

The reverse implication does not hold, and thus no con-

clusion about the veracity of the H· can be drawn when

∆LP = 0. Following Sejdinovic et al. [2013], we can con-
sider the mean embedding of this measure:

(cid:90)

µL =

k(x,·)l(y,·)m(z,·)∆LP

(4)

Given an i.i.d. sample (Xi, Yi, Zi)n
i=1, the norm of the
mean embedding µL can be empirically estimated using
empirically centered kernel matrices. For example, for the
kernel k with kernel matrix Kij = k(Xi, Xj), the empiri-
cally centered kernel matrix ˜K is given by

˜Kij = (cid:104)k(Xi,·) − ˜µX , k(Xj,·) − ˜µX(cid:105),

By Sejdinovic et al. [2013], an estimator of the norm of the
mean embedding of the Lancaster interaction for i.i.d. sam-
ples is

Our ﬁrst contribution is to show that the (difﬁcult) study of
the asymptotic null distribution of (cid:107)ˆµL(cid:107)2 can be reduced to
studying population centered kernels

(cid:16) ˜K ◦ ˜L ◦ ˜M

(cid:17)

(cid:107)ˆµL(cid:107)2 =

1
n2

(5)
where ◦ is the Hadamard (element-wise) product and

++

A++ =(cid:80)

ij Aij, for a matrix A.

(cid:107)ˆµ(Z)

L,2(cid:107)2 =

1
n2

where e.g.

(cid:16)

K ◦ L ◦ M

(cid:17)

++

2.3 TESTING PROCEDURE

In this paper, we construct a statistical test for three-
variable interaction, using n(cid:107)ˆµL(cid:107)2 as the test statistic to
distinguish between the following hypotheses:
H0 : HX ∨ HY ∨ HZ
H1 : PXY Z does not factorise in any way
The null hypothesis H0 is a composite of the three ‘sub-
hypotheses’ HX, HY and HZ. We test H0 by testing each
of the sub-hypotheses separately and we reject if and only if
we reject each of HX, HY and HZ. Hereafter we describe
the procedure for testing HZ; similar results hold for HX
and HY .
Sejdinovic et al. [2013] show that, under HZ, n(cid:107)ˆµL(cid:107)2 con-
verges to an inﬁnite sum of weighted χ-squared random
variables. By leveraging the i.i.d. assumption of the sam-
ples, any given quantile of this distribution can be estimated
using simple permutation bootstrap, and so a test procedure
is proposed.
In the time series setting this approach does not work. Tem-
poral dependence within the samples makes study of the
asymptotic distribution of n(cid:107)ˆµL(cid:107)2 difﬁcult; in Section 4.2
we verify experimentally that the permutation bootstrap
used in the i.i.d case fails. To construct a test in this set-
ting we will use asymptotic and bootstrap results for mix-
ing processes.
Mixing formalises the notion of the temporal structure
within a process, and can be thought of as the rate at which
the process forgets about its past. For example, for Gaus-
sian processes this rate can be captured by the autocorrela-
tion function; for general processes, generalisations of au-
tocorrelation are used. The exact assumptions we make
about the mixing properties of processes in this paper are
discussed in Section 3, and we will refer to them as suit-
able mixing assumptions for brevity in statements of results
throughout this paper.

2.4 MAIN RESULTS

It is straightforward to show that the norm of the mean em-
bedding (5) can also be written as

(cid:18)(cid:94)˜K ◦ ˜L ◦ ˜M

(cid:19)

++

(cid:107)ˆµL(cid:107)2 =

1
n2

K ij = (cid:104)k(Xi,·) − µX , k(Xj,·) − µX(cid:105),

Speciﬁcally, we prove the following:
Theorem 1. Suppose that (Xi, Yi, Zi)n
i=1 are drawn from
a random process satisfying suitable mixing assumptions.
L,2(cid:107)2 − n(cid:107)ˆµL(cid:107)2) = 0 in proba-
Under HZ, limn→∞(n(cid:107)ˆµ(Z)
bility.

Our proof of Theorem 1 relies crucially on the following
Lemma which we prove in Supplementary material A.1
Lemma 1. Suppose that (Xi)n
i=1 is drawn from a random
process satisfying suitable mixing assumptions and that k
is a bounded kernel on X . Then (cid:107)ˆµX − µX(cid:107)k = OP (n− 1
2 )

Proof. (Theorem 1) We provide a short sketch of the proof
here; for a full proof, see Section 6.
The key idea is to note that we can rewrite n(cid:107)ˆµL(cid:107)2 in terms
of the population centred kernel matrices K, L and M.
Each of the resulting terms can in turn be converted to an
inner product between quantities of the form ˆµ − µ, where
ˆµ is an empirical estimator of µ, and each µ is a mean em-
bedding or covariance operator.
By applying Lemma 1 to the ˆµ − µ, we show that most of
these terms converge in probability to 0, with the residual
terms equaling n(cid:107)ˆµ(Z)

L,2(cid:107)2.

As discussed in Section 1, the essential idea of this proof
is novel and the resulting proof is signiﬁcantly more con-
cise than previous approaches [Chwialkowski and Gretton,
2014, Chwialkowski et al., 2014].
Theorem 1 is useful because the statistic (cid:107)ˆµ(Z)
L,2(cid:107)2 is much
easier to study under the non-i.i.d. assumption than (cid:107)ˆµL(cid:107)2.
Indeed, it can expressed as a V -statistic (see Section 3.2)

(cid:88)

1≤i,j≤n

Vn =

1
n2

k ⊗ l ⊗ m(Si, Sj)

where Si = (Xi, Yi, Zi). The crucial observation is that

h := k ⊗ l ⊗ m

is well behaved in the following sense.
Theorem 2. Suppose that k, l and m are bounded, symmet-
ric, Lipschitz continuous kernels. Then h is also bounded
symmetric and Lipschitz continuous, and is moreover de-
generate under HZ i.e ESh(S, s) = 0 for any ﬁxed s.

Algorithm 1 Test HZ with Wild Bootstrap

n

(cid:33)

Input: ˜K, ˜L, ˜M, each size n × n, N= number of boot-
straps, α = p-value threshold
n(cid:107)ˆµL(cid:107)2 = 1
samples = zeros(1,N)
for i = 1 to N do

(cid:32) (cid:94)(cid:16) ˜K ◦ ˜L
(cid:17) ◦ ˜M
(cid:32) (cid:94)(cid:16) ˜K ◦ ˜L
(cid:17) ◦ ˜M

Draw random vector W according to Equation 6

samples[i] = 1

(cid:33)

++

(cid:124)

n W

end for
if sum(n(cid:107)ˆµL(cid:107)2 > samples)> α

N then

Reject HZ
Do not reject HZ

else

end if

Proof. See Section 6

The asymptotic analysis of such a V -statistic for non-
i.i.d. data is still complex, but we can appeal to prior work:
Leucht and Neumann [2013] showed a way to estimate any
given quantile of such a V -statistic under the null hypoth-
esis using a method called the wild bootstrap. This, com-
bined with analysis of the V -statistic under the alternative
hypothesis provided in Theorem 2 of Chwialkowski et al.
[2014]2, results in statistical test (see Algorithm 1).
In Section 3 we discuss the wild bootstrap and provide re-
sults regarding consistency and Type I error control.

2.5 MULTIPLE TESTING CORRECTION

In the Lancaster test, we reject the composite null hypoth-
esis H0 if and only if we reject all three of the compo-
nents. In Sejdinovic et al. [2013], it is suggested that the
Holm-Bonferroni correction be used to account for multi-
ple testing [Holm, 1979]. We show here that more relaxed
conditions on the p-values can be used while still bounding
the Type I error, thus increasing test power.
Denote by A∗ the event that H∗ is rejected. Then

P(A0) = P(AX ∧ AY ∧ AZ)

≤ min{P(AX ), P(AY ), P(AZ)}

If H0 is true, then so must one of the components. Without
loss of generality assume that HX is true. If we use signiﬁ-
cance levels of α in each test individually then P(AX ) ≤ α
and thus P(A0) ≤ α.
Therefore rejecting H0 in the event that each test has p-
value less than α individually guarantees a Type I error

2Note that similar results are presented in Leucht and Neu-

mann [2013] as speciﬁc cases.

In contrast, the Holm-Bonferonni
overall of at most α.
method requires that the sorted p-values be lower than
2 , α] in order to reject the null hypothesis overall. It
[ α
3 , α
is therefore more conservative than necessary and thus has
worse test power compared to the ‘simple correction’ pro-
posed here. This is experimentally veriﬁed in Section 4.

3 THE WILD BOOTSTRAP

W

In this section we discuss the wild bootstrap and provide
consistency and Type I error results for the proposed Lan-
caster test.

3.1 TEMPORAL DEPENDENCE

There are various formalisations of memory or ‘mixing’ of
a random process [Doukhan, 1994, Bradley et al., 2005,
Dedecker et al., 2007]; of relevance to this paper is the fol-
lowing :
Deﬁnition 1. A process (Xt)t is β-mixing (also known as
absolutely regular) if β(m) −→ 0 as m −→ ∞, where

β(m) =

1
2

sup

sup

n

|P(Ai ∩ Bj) − P(Ai)P(Bj)|

I(cid:88)

J(cid:88)

i=1

j=1

where the second supremum is taken over all ﬁnite par-
titions {A1, . . . , AI} and {B1, . . . , BJ} of the sample
space such that Ai ∈ F n
b =
σ(Xb, Xb+1, . . . , Xc)

1 and Bj ∈ F∞

n+m and F c

A related notion is that of τ-mixing. This is a property re-
quired to apply the wild bootstrap method of Leucht and
Neumann [2013], but we do not discuss τ-mixing here
since it is implied by β-mixing under the assumption that
Xi has ﬁnite p-th moment for any p > 1.

SUITABLE MIXING ASSUMPTIONS

We assume that the random process Si = (Xi, Yi, Zi)
is β mixing with mixing coefﬁcients satisfying β(m) =
o(m−6). Throughout this paper we refer to this assump-
tion as suitable mixing assumptions.

3.2 V -STATISTICS

A V -statistic of a 2-argument, symmetric function h given
observations Sn = {S1, . . . , Sn} is [Serﬂing, 2009]:

(cid:88)

Vn =

1
n2

h(Si, Sj)

1≤i,j≤n

We call nVn a normalised V -statistic. We call h the
core of V and we say that h is degenerate if, for any s1,
ES2∼P[h(s1, S2)] = 0, in which case we say that V is a

degenerate V -statistic. Many kernel test statistics can be
viewed as normalised V -statistics which, under the null hy-
pothesis, are degenerate. As mentioned in the previous sec-
L,2(cid:107)2 is a V -statistic. Theorems 1 and 2 together
tion, (cid:107)ˆµ(Z)
imply that, under HZ, it can be treated as a degenerate V -
statistic.

3.3 WILD BOOTSTRAP

If the test statistic has the form of a normalised V -statistic,
then provided certain extra conditions are met, the wild
bootstrap of Leucht and Neumann [2013] is a method to
directly resample the test statistic under the null hypothe-
sis. These conditions can be categorised as concerning: (1)
appropriate mixing of the process from which our observa-
tions are drawn; (2) the core of the V -statistic.
The condition on the core that is of crucial importance to
this paper is that it must be degenerate. Theorem 2 justiﬁes
our use of the wild bootstrap in the Lancaster interaction
test.
Given the statistic nVn, Leucht and Neumann [2013] tells
us that a random vector W of length n can be drawn such
that the bootstrapped statistic3

(cid:88)

i,j

nVb =

1
n

Wih(Si, Sj)Wj

is distributed according to the null distribution of nVn.
By generating many such W and calculating nVb for each,
we can estimate the quantiles of nV .

3.4 GENERATING W

The process generating W must satisfy conditions (B2)
given on page 6 of Leucht and Neumann [2013] for nVb to
correctly resample from the null distribution of nVn. For
brevity, we provide here only an example of such a pro-
cess; the interested reader should consult Leucht and Neu-
mann [2013] or Appedix A of Chwialkowski et al. [2014]
for a more detailed discussion of the bootstrapping process.
The following bootstrapping process was used in the exper-
iments in Section 4:

1 − e−2/lnt

Wt = e−1/ln Wt−1 +

(6)
where W1, 1, . . . , t are independent N (0, 1) random vari-
ables. ln should be taken from a sequence {ln} such that
limn−→∞ ln = ∞; in practice we used ln = 20 for all of
the experiments since the values of n were roughly compa-
rable in each case.

3Note that for ﬁxed Sn, nVb is a random variable through the

randomness introduced by W

(cid:112)

3.5 CONTROL OF TYPE I ERROR

The following theorem shows that by estimating the quan-
tiles of the wild bootstrapped statistic nVb we correctly
control the Type I error when testing HZ.
Theorem 3. Suppose that (Xi, Yi, Zi)n
i=1 are drawn from
a random process satisfying suitable mixing conditions,
and that W is drawn from a process satisfying (B2) in
Leucht and Neumann [2013]. Then asymptotically, the
quantiles of

(cid:124)(cid:16)(cid:0) ¯K ◦ ¯L(cid:1) ◦ ¯M

(cid:17)

W

nVb =

1
n

W

converge to those of n(cid:107)ˆµL(cid:107)2.

Proof. See Supplementary material A.3

3.6

(SEMI-)CONSISTENCY OF TESTING
PROCEDURE

Note that in order to achieve consistency for this test, we
would need that H0 ⇐⇒ ∆LP = 0. Unfortunately
this does not hold - in Sejdinovic et al. [2013] examples
are given of distributions for which H0 is false, and yet
∆LP = 0.
However, the following result does hold:
Theorem 4. Suppose that ∆LP (cid:54)= 0. Then as n −→ ∞,
the probability of correctly rejecting H0 converges to 1.

Proof. See Supplementary material A.4

At the time of writing, a characterisation of distributions
for which H0 is false yet ∆LP = 0 is unknown. There-
fore, if we reject H0 then we conclude that the distribution
does not factorise; if we fail to reject H0 then we cannot
conclude that the distribution factorises.

4 EXPERIMENTS

The Lancaster test described above amounts to a method to
test each of the sub-hypotheses HX ,HY ,HZ. Rather than
using the Lancaster test statistic with wild bootstrap to test
each of these, we could instead use HSIC. For example, by
considering the pair of variables (X, Y ) and Z with ker-
nels k ⊗ l and m respectively, HSIC can be used to test
HZ. Similar grouping of the variables can be used to test
HX and HY . Applying the same multiple testing correc-
tion as in the Lancaster test, we derive an alternative test of
dependence between three variables. We refer to this HSIC
based procedure as 3-way HSIC.
In the case of i.i.d. observations, it was shown in Sejdinovic
et al. [2013] that Lancaster statistical test is more sensi-
tive to dependence between three random variables than the

above HSIC-based test when pairwise interaction is weak
but joint interaction is strong. In this section, we demon-
strate that the same is true in the time series case on syn-
thetic data.

4.1 WEAK PAIRWISE INTERACTION, STRONG

JOINT INTERACTION

This experiment demonstrates that the Lancaster test has
greater power than 3-way HSIC when the pairwise interac-
tion is weak, but joint interaction is strong.
Synthetic data were generated from autoregressive pro-
cesses X, Y and Z according to:

Xt =

Yt =

Zt =

1
2
1
2
1
2

Xt−1 + t

Yt−1 + ηt
Zt−1 + d|θt|sign(XtYt) + ζt

where X0, Y0, Z0, t, ηt, θt and ζt are i.i.d. N (0, 1) random
variables and d ∈ R, called the dependence coefﬁcient, de-
termines the extent to which the process (Zt)t is dependent
on (Xt, Yt)t.
Data were generated with varying values of d. For each
value of d, 300 datasets were generated, each consisting
of 1200 consecutive observations of the variables. Gaus-
sian kernels with bandwidth parameter 1 were used on each
variable, and 250 bootstrapping procedures were used for
each test on each dataset.
Observe that the random variables are pairwise indepen-
dent but jointly dependent. Both the Lancaster and 3-way
HSIC tests should be able to detect the dependence and
therefore reject the null hypothesis in the limit of inﬁnite
data. In the ﬁnite data regime, the value of d affects dras-
tically how hard it is to detect the dependence. The results
of this experiment are presented in Figure 1, which shows
that the Lancaster test achieves very high test power with
weak dependence coefﬁcients compared to 3-way HSIC.
Note also that when using the simple multiple testing cor-
rection a higher test power is achieved than with the Holm-
Bonferroni correction.

4.2 FALSE POSITIVE RATES

This experiment demonstrates that in the time series case,
existing permutation bootstrap methods fail to control the
Type I error, while the wild bootstrap correctly identiﬁes
test statistic thresholds and appropriately controls Type I
error.
Synthetic data were generated from autoregressive pro-

Figure 1: Results of experiment in Section 4.1. (S) refers
to the simple multiple correction; (HB) refers to Holm-
Bonferroni. The Lancaster test is more sensitive to de-
pendence than 3-way HSIC, and test power for both tests
is higher when using the simple correction rather than the
Holm-Bonferroni multiple testing correction.

cesses X, Y and Z according to:

Xt = aXt−1 + t
Yt = aYt−1 + ηt
Zt = aZt−1 + ζt

where X0, Y0, Z0, t, ηt and ζt are i.i.d. N (0, 1) random
variables and a, called the dependence coefﬁcient, deter-
mines how temporally dependent the processes are. The
null hypothesis in this example is true as each process is
independent of the others.
The Lancaster test was performed using both the Wild
Bootstrap and the simple permutation bootstrap (used in the
i.i.d. case) in order to sample from the null distributions of
the test statistic. We used a ﬁxed desired false positive rate
α = 0.05 with sample of size 1000, with 200 experiments
run for each value of a. Figure 2 shows the false positive
rates for these two methods for varying a. It shows that
as the processes become more dependent, the false positive
rate for the permutation method becomes very large, and is
not bounded by the ﬁxed α, whereas the false positive rate
for the Wild Bootstrap method is bounded by α.

4.3 STRONG PAIRWISE INTERACTION

This experiment demonstrates a limitation of the Lancaster
test. When pairwise interaction is strong, 3-way HSIC has

00.020.040.060.080.10.120.140.160.180.200.10.20.30.40.50.60.70.80.91Dependence coefficientPowerPower of HSIC and Lancaster joint factorisation tests  Lancaster (S)Lancaster (HB)3−way HSIC (S)3−way HSIC (HB)that in this case the 3-way HSIC test is more sensitive to
the dependence than the Lancaster test.

Figure 2: Results of experiment in section 4.2. Whereas
the wild bootstrap succeeds in controlling the Type I error
across all values of the dependence coefﬁcient, the permu-
tation bootstrap fails to control the Type I error as it does
not sample from the correct null distribution as temporal
dependence between samples increases.

greater test power than Lancaster.
Synthetic data were generated from autoregressive pro-
cesses X, Y and Z according to:

Xt =

Yt =

Zt =

1
2
1
2
1
2

Xt−1 + t

Yt−1 + ηt

Zt−1 + d(Xt + Yt) + ζt

where X0, Y0, Z0, t, ηt and ζt are i.i.d. N (0, 1) random
variables and d ∈ R, called the dependence coefﬁcient, de-
termines the extent to which the process (Zt)t is dependent
on Xt and Yt.
Data were generated with varying values for the depen-
dence coefﬁcient. For each value of d, 300 datasets were
generated, each consisting of 1200 consecutive observa-
tions of the variables. Gaussian kernels with bandwidth
parameter 1 were used on each variable, and 250 bootstrap-
ping procedures were used for each test on each dataset.
In this case Zt is pairwise-dependent on both of Xt and Yt,
in addition to all three variables being jointly dependent.
Both the Lancaster and 3-way HSIC tests should be capable
of detecting the dependence and therefore reject the null
hypothesis in the limit of inﬁnite data. The results of this
experiment are presented in Figure 3, which demonstrates

Figure 3: Results of experiment in Section 4.3. (S) refers
to the simple multiple correction; (HB) refers to Holm-
Bonferroni. The Lancaster test is less sensitive to depen-
dence than 3-way HSIC, and test power in both cases is
higher when using the simple correction rather than the
Holm-Bonferroni multiple testing correction.

4.4 FOREX DATA

Exchange rates between three currencies (GBP, USD,
EUR) at 5 minute intervals over 7 consecutive trading days
were obtained. The data were processed by taking the re-
turns (difference between consecutive terms within each
t = xt − xt−1) which were then normalised
time series, xr
(divided by standard deviation). We performed the Lan-
caster test, 3-way HSIC and pairwise HSIC on using the
ﬁrst 800 entries of each processed series. All tests rejected
the null hypothesis. The Lancaster test returned p-values of
0 for each of HX, HY and HZ with 10000 bootstrapping
procedures.
We then shifted one of the time series and repeated the tests
(i.e. we used entries 1 to 800 of two of the processed series
and entries 801 to 1600 of the third). In this case, pairwise
HSIC still detected dependence between the two unshifted
time series, and both Lancaster and 3-way HSIC did not re-
ject the null hypothesis that the joint distribution factorises.
The Lancaster test returned p-values of 0.2708, 0.2725 and
0.1975 for HX, HY and HZ respectively.
In both cases, the Lancaster test behaves as expected. Due
to arbitrage, any two exchange rates should determine the
third and the Lancaster test correctly identiﬁes a joint de-
pendence in the returns. However, when we shift one of

0.10.20.30.40.50.60.70.80.900.10.20.30.40.50.60.70.8Dependence coefficientType I errorFalse positives − Wild Bootstrap vs Permutation  Lancaster with Wild BootstrapLancaster with Permutation BootstrapDesired type I error bound (0.05)00.10.20.30.40.500.10.20.30.40.50.60.70.80.91Dependence coefficientPowerPower of HSIC and Lancaster joint factorisation tests  Lancaster (S)Lancaster (HB)3−way HSIC (S)3−way HSIC (HB)the time series, we break the dependence between it and
the other series. Lancaster correctly identiﬁes here that the
underlying distribution does factorise.

5 DISCUSSION AND FUTURE

RESEARCH

We demonstrated that the Lancaster test is more sensitive
than 3-way HSIC when pairwise interaction is weak, but
that the opposite is true when pairwise interaction is strong.
It is curious that the two tests have different strengths in
this manner, particularly when considering the very similar
forms of the statistics in each case. Indeed, to test HZ using
the Lancaster statistic, we bootstrap the following:

(cid:32) (cid:94)(cid:16) ˜K ◦ ˜L
(cid:17) ◦ ˜M

(cid:33)

++

n(cid:107)∆L ˆP(cid:107)2 =

1
n

while for the 3-way HSIC test we bootstrap:

(cid:16) (cid:94)(K ◦ L) ◦ ˜M

(cid:17)

++

nHSICb =

1
n

These two quantities differ only in the centring of K and L,
amounting to constant shifts in the respective feature spaces
of the kernels k and l. This difference has the consequence
of quite drastically changing the types of dependency to
which each statistic is sensitive. A formal characterisation
of the cases in which the Lancaster statistic is more sensi-
tive than 3-way HSIC would be desirable.

6 PROOFS

An outline of the proof of Theorem 1 was given in Sec-
tion 2; here we provide the full proof, as well as a proof of
Theorem 2.

Proof. (Theorem 1)
By observing that

φX (Xi) − 1
n

(cid:88)

k

φX (Xk)

(cid:88)

k

¯φX (Xk)

(cid:88)

k

= (φX (Xi) − µX ) − 1
n

(φX (Xk) − µX )

= ¯φX (Xi) − 1
n

we can therefore expand ˜K in terms of ¯K as

(cid:88)
(cid:88)

k

˜Kij
= (cid:104)φX (Xi) − 1
n
= (cid:104) ¯φX (Xi) − 1
n
¯Kik − 1
n

= ¯Kij − 1
n

(cid:88)

k

k

(cid:88)

k

φX (Xk), φX (Xj) − 1
n
¯φX (Xk), ¯φX (Xj) − 1
n

(cid:88)

kl

¯Kjk +

1
n2

(cid:88)
(cid:88)

k

k
¯Kkl

φX (Xk)(cid:105)

¯φX (Xk)(cid:105)

+

and expanding ˜L and ˜M in a similar way, we can rewrite
the Lancaster test statistic as
( ¯K ◦ ¯L ◦ ¯M )++
n(cid:107)ˆµL(cid:107)2 =
1
n
n2 (( ¯K ◦ ¯M ) ¯L)++
− 2
n3 ( ¯K ◦ ¯L)++ ¯M++
1
n3 ( ¯L ◦ ¯M )++ ¯K++
2
n3 ( ¯K ¯L ¯M )++
n3 tr( ¯K+ ◦ ¯L+ ◦ ¯M+) − 4
4
+
− 4
− 4
n4 ( ¯K ¯M )++ ¯L++
4
¯K++ ¯L++ ¯M++
n5

n2 (( ¯K ◦ ¯L) ¯M )++
− 2
n2 (( ¯M ◦ ¯L) ¯K)++
− 2
n3 ( ¯K ◦ ¯M )++ ¯L++
1
2
n3 ( ¯M ¯K ¯L)++
2
n3 ( ¯K ¯M ¯L)++
n4 ( ¯K ¯L)++ ¯M++
n4 ( ¯L ¯M )++ ¯K++

+

+

+

+

+

+

1

i

(cid:80)

We denote by CXY Z = EXY Z[ ¯φX (X)⊗ ¯φY (Y )⊗ ¯φZ(Z)]
the population centred covariance operator with empirical
¯φX (Xi)⊗ ¯φY (Yi)⊗ ¯φZ(Zi). We
estimate ¯CXY Z = 1
n
deﬁne similarly the quantities CXY , CY ZX , . . . with cor-
responding empirical counterparts ¯CXY , ¯CY ZX , . . . where
for example CY Z = EY Z[ ¯φY (Y ) ⊗ ¯φZ(Z)]
Each of the terms in the above expression for n(cid:107)ˆµL(cid:107)2 can
be expressed as inner products between empirical estimates
of population centred covariance operators and tensor prod-
ucts of mean embeddings. Rewriting them as such yields:

n(cid:107)ˆµL(cid:107)2 = n(cid:104) ¯CXY Z, ¯CXY Z(cid:105)

− 2n(cid:104) ¯CXY Z, ¯CXY ⊗ ¯µZ(cid:105)
− 2n(cid:104) ¯CXZY , ¯CXZ ⊗ ¯µY (cid:105)
− 2n(cid:104) ¯CY ZX , ¯CY Z ⊗ ¯µX(cid:105)
+ n(cid:104) ¯CXY ⊗ ¯µZ, ¯CXY ⊗ ¯µZ(cid:105)
+ n(cid:104) ¯CXZ ⊗ ¯µY , ¯CXZ ⊗ ¯µY (cid:105)
+ n(cid:104) ¯CY Z ⊗ ¯µX , ¯CY Z ⊗ ¯µX(cid:105)
+ 2n(cid:104)¯µZ ⊗ ¯CXY , ¯CZX ⊗ ¯µY (cid:105)
...

+ 2n(cid:104)¯µX ⊗ ¯CY Z, ¯CXY ⊗ ¯µZ(cid:105)
+ 2n(cid:104)¯µX ⊗ ¯CZY , ¯CXZ ⊗ ¯µY (cid:105)
+ 4n(cid:104) ¯CXY Z, ¯µX ⊗ ¯µY ⊗ ¯µZ(cid:105)
− 4n(cid:104) ¯CXY ⊗ ¯µZ, ¯µX ⊗ ¯µY ⊗ ¯µZ(cid:105)
− 4n(cid:104) ¯CXZ ⊗ ¯µY , ¯µX ⊗ ¯µZ ⊗ ¯µY (cid:105)
− 4n(cid:104) ¯CY Z ⊗ ¯µX , ¯µY ⊗ ¯µZ ⊗ ¯µX(cid:105)
+ 4n(cid:104)¯µX ⊗ ¯µY ⊗ ¯µZ, ¯µX ⊗ ¯µY ⊗ ¯µZ(cid:105)

By assumption, PXY Z = PXY PZ and thus the expecta-
tion operator also factorises similarly. As a consequence,
CXY Z = 0. Indeed, given any A ∈ FX ⊗ FY ⊗ FZ, we
can consider A to be a bounded linear operator FZ −→
FX ⊗ FY. It follows that4
EXY Z(cid:104)A, ¯CXY Z(cid:105)

EXY EZ(cid:104)A, ¯φX (Xi) ⊗ ¯φY (Yi) ⊗ ¯φZ(Zi)(cid:105)

EXY EZ(cid:104) ¯φX (Xi) ⊗ ¯φY (Yi), A ¯φZ(Zi)(cid:105)FX⊗FY

EXY (cid:104) ¯φX (Xi) ⊗ ¯φY (Yi), AEZ

¯φZ(Zi)(cid:105)FX⊗FY

(cid:88)
(cid:88)
(cid:88)

i

i

i

=

=

=

1
n

1
n

1
n

= 0
We conclude that CXY Z = EXY Z ¯CXY Z = 0.
Similarly, CXZY , CY ZX, CXZ, CY Z are all 0 in their re-
spective Hilbert spaces. Lemma 2 tells us that each sub-
process of (Xi, Yi, Zi) satisﬁes the same β-mixing condi-
tions as (Xi, Yi, Zi), thus by applying Lemma 1 it follows
that (cid:107) ¯CXZY (cid:107), (cid:107) ¯CY ZX(cid:107), (cid:107) ¯CXZ(cid:107), (cid:107) ¯CY Z(cid:107), (cid:107)¯µX(cid:107), (cid:107)¯µY (cid:107),
(cid:107)¯µZ(cid:107) = OP

. Therefore

n− 1

(cid:16)

(cid:17)

2

n(cid:107)ˆµL(cid:107)2 OP (n

−−−−−−→ n(cid:104) ¯CXY Z, ¯CXY Z(cid:105)

2 )

− 1

− 2n(cid:104) ¯CXY Z, ¯CXY ⊗ ¯µZ(cid:105) − 2n(cid:104) ¯CXZY , ¯CXZ ⊗ ¯µY (cid:105)

(( ¯K ◦ ¯L) ◦ ¯M )++
1
=
n
n2 (( ¯K ◦ ¯L) ¯M )++ +
− 2

1

n3 ( ¯K ◦ ¯L)++ ¯M++

since all the other terms decay at least as quickly as
n ). This is shown here for n(cid:104)¯µX⊗ ¯CY Z, ¯CXY ⊗¯µZ(cid:105);
OP ( 1√
the proofs for the other terms are similar.
n(cid:104)¯µX ⊗ ¯CY Z, ¯CXY ⊗ ¯µZ(cid:105)
≤ n(cid:107)¯µX ⊗ ¯CY Z(cid:107)(cid:107) ¯CXY ⊗ ¯µZ(cid:107)

(cid:113)(cid:104)¯µX ⊗ ¯CY Z, ¯µX ⊗ ¯CY Z(cid:105)(cid:113)(cid:104) ¯CXY ⊗ ¯µZ, ¯CXY ⊗ ¯µZ(cid:105)

= n

4We can bring the EZ inside the inner product in the penulti-
mate line due to the Bochner integrability of ¯φZ (Z), which fol-
lows from the conditions required for µZ to exist [Steinwart and
Christmann, 2008].

(cid:113)(cid:104)¯µX , ¯µX(cid:105)(cid:104) ¯CY Z, ¯CY Z(cid:105)(cid:113)(cid:104) ¯CXY , ¯CXY (cid:105)(cid:104)¯µZ, ¯µZ(cid:105)
(cid:17)
(cid:16) 1√
(cid:16) 1√

= n
= n(cid:107)¯µX(cid:107)(cid:107) ¯CY Z(cid:107)(cid:107) ¯CXY (cid:107)(cid:107)¯µZ(cid:107)
= nOP

(cid:16) 1√

(cid:16) 1√

OP (1)OP

= OP

(cid:17)

(cid:17)

(cid:17)

OP

n

n

n

n

It can be shown that ¯K ◦ ¯L in the above expression can be
replaced with ¯K ◦ ¯L while preserving equality. That is, we
can equivalently write
n(cid:107)∆L ˆP(cid:107)2 −→ 1
(( ¯K ◦ ¯L) ◦ ¯M )++
n
− 2
n2 (( ¯K ◦ ¯L) ¯M )++ +

n3 ( ¯K ◦ ¯L)++ ¯M++
This is equivalent to treating ¯k ⊗ ¯l as a kernel on the single
variable T := (X, Y ) and performing another recentering
trick as we did at the beginning of this proof. By rewrit-
ing the above expression in terms of the operator ¯CT Z and
mean embeddings µT and µZ, it can be shown by a simi-
lar argument to before that the latter two terms tend to 0 at
least as OP (n− 1
2 ), and thus, substituting for the deﬁnition
of (cid:107)ˆµ(Z)

1

L,2(cid:107)2,

n(cid:107)ˆµL(cid:107)2

as required.

OP ( 1√
−−−−−→ n(cid:107)ˆµ(Z)
L,2(cid:107)2

n

)

Proof. (Theorem 2)
Note that EXY Z = EXY EZ under HZ. Therefore, ﬁxing
any sj = (xj, yj, zj) we have that
ESih(Si, sj) = EXiYi

¯k ⊗ ¯l ⊗ ¯m(Si, sj)

EZi

= (cid:104)EXiYi

¯φ(Xi) ⊗ ¯φ(Yi) − CXY , ¯φ(xj) ⊗ ¯φ(yj) − CXY (cid:105)
× (cid:104)EZi

¯φ(Zi), ¯φ(zj)(cid:105)
= (cid:104)0, ¯φ(xj) ⊗ ¯φ(yj) − CXY (cid:105)

× (cid:104)0, ¯φ(zj)(cid:105) = 0

Therefore h is degenerate. Symmetry follows from the
symmetry of the Hilbert space inner product.
For boundedness and Lipschitz continuity, it sufﬁces to
show the two following rules for constructing new kernels
from old preserve both properties (see Supplementary ma-
terials A.5 for proof):

• k (cid:55)→ ¯k
• (k, l) (cid:55)→ k ⊗ l

It then follows that h = ¯k ⊗ ¯l ⊗ ¯m is bounded and Lips-
chitz continuous since it can be constructed from k, l and
m using the two above rules.

characteristic function. Journal of Mathematical Scien-
cies, 89:1582–1589, 1998.

G. Kirchgässner, J. Wolters, and U. Hassler. Introduction to
modern time series analysis. Springer Science & Busi-
ness Media, 2012.

H. O. Lancaster. Chi-Square Distribution. Wiley Online

Library, 1969.

A. W. Ledford and J. A. Tawn. Statistics for near indepen-
dence in multivariate extreme values. Biometrika, 83(1):
169–187, 1996.

A. Leucht and M. H. Neumann. Dependent wild bootstrap
for degenerate u-and v-statistics. Journal of Multivariate
Analysis, 117:257–280, 2013.

R. Patra, B. Sen, and G. Szekely. On a nonparametric
notion of residual and its applications. Statist. Probab.
Lett., 106:208–213, 2015.

D. Sejdinovic, A. Gretton, and W. Bergsma. A kernel test
for three-variable interactions. In Advances in Neural In-
formation Processing Systems, pages 1124–1132, 2013.
R. J. Serﬂing. Approximation theorems of mathematical

statistics, volume 162. John Wiley &amp; Sons, 2009.

X. Shao. The dependent wild bootstrap.

Journal of
the American Statistical Association, 105(489):218–
235, 2010.

A. Smola, A. Gretton, L. Song, and B. Schölkopf. A
hilbert space embedding for distributions. In Algorith-
mic Learning Theory, pages 13–31. Springer, 2007.

B. K. Sriperumbudur, K. Fukumizu, and G. R. Lanckriet.
Universality, characteristic kernels and rkhs embedding
of measures. The Journal of Machine Learning Re-
search, 12:2389–2410, 2011.

I. Steinwart and A. Christmann. Support vector machines.

Springer Science &amp; Business Media, 2008.

G. Székely, M. Rizzo, and N. Bakirov. Measuring and test-
ing dependence by correlation of distances. Annals of
Statistics, 35(6):2769–2794, 2007.

K. Zhang, J. Peters, D. Janzing, and B. Schoelkopf. Kernel-
based conditional independence test and application in
causal discovery. In Proceedings of the Conference on
Uncertainty in Artiﬁcial Intelligence (UAI), pages 804–
813, 2011.

References
M. Besserve, N. Logothetis, and B. Schölkopf. Statistical
analysis of coupled time series with kernel cross-spectral
density operators. In NIPS, pages 2535–2543, 2013.

R. C. Bradley et al. Basic properties of strong mixing con-
ditions. a survey and some open questions. Probability
surveys, 2(2):107–144, 2005.

K. Chwialkowski and A. Gretton. A kernel indepen-
arXiv preprint

dence test for random processes.
arXiv:1402.4501, 2014.

K. P. Chwialkowski, D. Sejdinovic, and A. Gretton. A
wild bootstrap for degenerate kernel tests. In Advances
in neural information processing systems, pages 3608–
3616, 2014.

J. Dedecker, P. Doukhan, G. Lang, L. R. J. Rafael,
S. Louhichi, and C. Prieur. Weak dependence. In Weak
Dependence: With Examples and Applications, pages 9–
20. Springer, 2007.

H. Dehling, O. S. Sharipov, and M. Wendler. Bootstrap for
dependent hilbert space-valued random variables with
application to von mises statistics. Journal of Multivari-
ate Analysis, 133:200–215, 2015.
P. Doukhan. Mixing. Springer, 1994.
A. Feuerverger. A consistent test for bivariate dependence.

International Statistical Review, 61(3):419–433, 1993.

K. Fukumizu, A. Gretton, X. Sun, and B. Schölkopf. Ker-
nel measures of conditional dependence. In NIPS, pages
489–496, Cambridge, MA, 2008. MIT Press.

S. Gaisser, M. Ruppert, and F. Schmid. A multivariate ver-
sion of hoeffding’s phi-square. Journal of Multivariate
Analysis, 101(10):2571–2586, 2010.

A. Gretton and L. Gyorﬁ. Consistent nonparametric tests of
independence. Journal of Machine Learning Research,
11:1391–1423, 2010.

A. Gretton, O. Bousquet, A. Smola, and B. Schölkopf.
Measuring statistical dependence with hilbert-schmidt
In Algorithmic learning theory, pages 63–77.
norms.
Springer, 2005.

A. Gretton, K. Fukumizu, C. H. Teo, L. Song,
B. Schölkopf, and A. J. Smola. A kernel statistical test of
independence. In Advances in Neural Information Pro-
cessing Systems, pages 585–592, 2007.

R. Heller, Y. Heller, and M. Gorﬁne. A consistent multi-
variate test of association based on ranks of distances.
Biometrika, 100(2):503–510, 2013.

S. Holm. A simple sequentially rejective multiple test pro-
cedure. Scandinavian journal of statistics, pages 65–70,
1979.

A. Kankainen and N. G. Ushakov. A consistent modiﬁca-
tion of a test for independence based on the empirical

A SUPPLEMENTARY MATERIAL

This supplementary section contains proofs omitted from the main paper and includes a proof that the HSIC statistic
asymptotically satisﬁes the hypothesis of the Wild Bootstrap.

A.1 HILBERT SPACE RANDOM VARIABLE CLT

In this paper we exploit a Central Limit Theorem for Hilbert space valued random variables that are functions of random
processes [Dehling et al., 2015]. One of the conditions required to apply this theorem concerns appropriate β-mixing of
the underlying processes. This theorem is used as a black-box, and it is hoped by the authors that as further theorems
concerning CLT-properties of Hilbert space random variables are developed, the conditions required of the processes may
be weakened.

Proof. (Lemma 1) We exploit Theorem 1.1 from Dehling et al. [2015]. Using the language of this paper, ¯φ(Xi) is a 1-
approximating functional of (Xi)i, following straightforwardly from the deﬁnition of 1-approximating functionals given.
Since our kernels are bounded, ∃C : (cid:107) ¯φ(Xi)(cid:107) < C and so

E(cid:107) ¯φ(X1)(cid:107)2+δ < C 2+δ < ∞ ∀δ > 0

Thus condition (1) is satisﬁed.
We can take fm = ¯φ(X0) ∀m and so achieve am = 0 ∀m, thus condition (2) is satisﬁed.
By assumption on the time series, condition (3) is satisﬁed.
Thus, by Theorem 1.1 in Dehling et al. [2015]

√

n(˜µX − µX ) n−→∞∼ N

where N is a Hilbert space valued Gaussian random variable and convergence is in distribution. Thus

(cid:107)˜µX − µX(cid:107) = OP (

1√
n

)

A.2 SUB-PROCESSES OF β-MIXING PROCESSES ARE β-MIXING

Lemma 2. Suppose that the process (Xt, Yt, Zt)t is β-mixing. Then any ‘sub-process’ is also β-mixing (for example
(Xt, Yt)t or (Xt)t)

Proof. (Lemma 2)
Let us consider (Xt, Yt)t. Let us call βXY Z(m) the coefﬁcients for the process (Xt, Yt, Zt)t, and βXY (m) the coefﬁcients
for the process (Xt, Yt)t.
Observe that for A ∈ σ((Xb, Yb), . . . , (Xc, Yc)), it is the case that A × Z ∈ σ((Xb, Yb, Zb), . . . , (Xc, Yc, Zc)) and
PXY (A) = PXY Z(A × Z).
Thus

βXY (m) =

=

1
2

1
2

sup

n

sup

n

≤ 1
2

sup

n

sup
},{BXY

j

{AXY

i

sup
},{BXY

j

{AXY

i

}

}

{AXY Z

i

sup
},{BXY Z

j

i=1

j=1

I(cid:88)
J(cid:88)
I(cid:88)
J(cid:88)
I(cid:88)
J(cid:88)

j=1

i=1

}

i=1

j=1

|PXY (AXY

i

∩ BXY

j

) − PXY Z(AXY

i

)PXY Z(BXY

j

)|

|PXY Z((AXY
− PXY Z(AXY

i × Z) ∩ (BXY
i × Z)PXY Z(BXY

j × Z))

|PXY Z(AXY Z

i

∩ BXY Z

j

)PXY Z(BXY Z

j

)|

j × Z)|
) − PXY Z(AXY Z

i

= βXY Z(m)

Thus we have shown that βXY Z(m) −→ 0 =⇒ βXY (m) −→ 0. That is, if (Xt, Yt, Zt)t is β-mixing then so is (Xt, Yt)t
A similar argument holds for any other sub-process.

A.3 CONTROL OF TYPE I ERROR

Theorem 3 shows that the quantiles of the bootstrapped statistic nVb (which we can estimate by drawing a large number
of samples) converge to those of the test statistic (cid:107)ˆµL(cid:107)2 under the null hypothesis. Therefore, we can estimate rejection
thresholds to appropriately control Type I error.

Proof. (Theorem 3)
We use Theorem 3.1 from Leucht and Neumann [2013]. By assumption, condition (B2) is satisﬁed by the random matrix
W . (A2) is satisﬁed due to Theorem 2. (B1) is satisﬁed due to the suitable mixing assumptions.
Therefore, Theorem 3.1 implies that nVb converges in probability to the null distribution of n(cid:107)ˆµ(Z)
converges in probability to n(cid:107)ˆµ(Z)
Convergence in distribution implies that the quantiles converge.

L,2(cid:107)2. Since n(cid:107)µL(cid:107)2 also
L,2(cid:107)2, it follows that nVb converges to n(cid:107)µL(cid:107)2 in probability, and thus also in distribution.

A.4 SEMI-CONSISTENCY
Theorem 4 provides a consistency result: if ∆LP (cid:54)= 0, then we correctly reject H0 with probability 1 in the limit n −→ ∞.

Proof. By Theorem 2 from Chwialkowski et al. [2014], nVb converges to some random variable with ﬁnite variance, while
n(cid:107)ˆµL(cid:107)2 −→ ∞. Thus if Qα is the α-quantile of nVb, then P (n(cid:107)ˆµL(cid:107)2 > Qα) −→ 1 for any α.

A.5 PROOF THAT BOUNDEDNESS AND LIPSCHITZ CONTINUITY IS PRESERVED
Recall that a kernel k deﬁned on X is Lipschitz continuous iff ∃Ck : ∀w |k(x, w) − k(x(cid:48), w)| ≤ CkdX (x, x(cid:48)) where dX
is the metric on X with respect to which k is Lipschitz continuous.
Claim 1. k bounded and Lipschitz continuous =⇒ ¯k is bounded and Lipschitz continuous
Proof. k bounded implies there exists Bk such that |k(x, w)| ≤ Bk ∀x, w ∈ X . It follows that

|¯k(x, w)| = |k(x, w) − EX [k(X, w)] − EW [k(x, W )] + EXW [k(X, W )]|
≤ |k(x, w)| + EX|k(X, w)| + EW|k(x, W )| + EXW|k(X, W )|
≤ 4Bk

And thus ¯k is bounded. For Lipschitz continuity, observe that for any w ∈ X

|¯k(x, w) − ¯k(x(cid:48), w)| = |k(x, w) − EX [k(X, w)] − EW [k(x, W )] + EXW [k(X, W )]

− k(x(cid:48), w) + EX [k(X, w)] + EW [k(x(cid:48), W )] − EXW [k(X, W )]|

= |k(x, w) − k(x(cid:48), w) + EW [k(x(cid:48), W )] − EW [k(x, W )]|
≤ |k(x, w) − k(x(cid:48), w)| + |EW [k(x(cid:48), W )] − EW [k(x, W )]|
≤ |k(x, w) − k(x(cid:48), w)| + EW|k(x(cid:48), W ) − k(x, W )|
≤ 2CkdX (x, x(cid:48))

and thus ¯k is Lipschitz continuous.

Claim 2. k and l bounded and Lipschitz continuous with respect to the metrics dX and dY respectively =⇒ k ⊗ l is
bounded and Lipschitz continuous with respect to any metric on X × Y equivalent to d ((x, y), (x(cid:48), y(cid:48))) = dX (x, x(cid:48)) +
dY (y, y(cid:48))
Note that all norms on ﬁnite dimensional vector spaces are equivalent, and so if X and Y are ﬁnite dimensional vector
spaces then k ⊗ l is Lipschitz continuous with respect to any norm on X × Y

Proof. Let k and l be bounded by Bk and Bl respectively. Then

|k ⊗ l ((x, y), (w, z))| = |k(x, w)l(y, z)|
= |k(x, w)||l(y, z)|
≤ BkBl

Let k and l have Lipschitz constants Ck and Cl respectively. Then, for any (w, z) ∈ X × Y

|k ⊗ l ((x, y), (w, z)) − k ⊗ l ((x(cid:48), y(cid:48)), (w, z))|

= |k(x, w)l(y, z) − k(x(cid:48), w)l(y(cid:48), z)|
= |k(x, w)l(y, z) − k(x(cid:48), w)l(y, z) + k(x(cid:48), w)l(y, z) − k(x(cid:48), w)l(y(cid:48), z)|
≤ |l(y, z)||k(x, w) − k(x(cid:48), w)| + |k(x(cid:48), w)||l(y, z) − l(y(cid:48), z)|
≤ BlCkdX (x, x(cid:48)) + BkCldY (y, y(cid:48))
≤ max(BlCk, BkCl) d ((x, y), (x(cid:48), y(cid:48)))

A.6 PROOF THAT HSIC CAN BE WILD BOOTSTRAPPED
Given samples {(Xi, Yi)}n
deﬁned to be the squared RKHS distance between the empirical embeddings of the distributions PXY and PXPY :

i=1, and taking all notation involving kernels and base spaces as before, the HSIC statistic is

HSICb = (cid:107) 1
n

φX (Xi) ⊗ φY (Yi) −

φX (Xi)

⊗

n3 (KL)++ +

1
n4 K++L++

(cid:32)

(cid:88)

i

1
n

(cid:33)

(cid:32)

(cid:33)

(cid:107)2

(cid:88)

i

1
n

φY (Yi)

(cid:88)
n2 (K ◦ L)++ − 2
n2 ( ˜K ◦ ˜L)++

1

i

1

=

=

where the last equality can be shown easily by expanding ˜K (and ˜L similarly) as

(cid:88)

φX (Xk), φX (Xj) − 1
n

φX (Xk)(cid:105)

˜Kij = (cid:104)φX (Xi) − 1
n
Kik − 1
n

= Kij − 1
n

(cid:88)

k

k

(cid:88)

k

Kjk +

1
n2

(cid:88)

k

Kkl

(cid:88)

kl

(cid:80)∞

probability.

Theorem 5. Suppose that (Xi, Yi)n

2+δ < ∞ for some δ > 0. Under H0 = {PXY = PXPY }, limn→∞(nHSICb − 1

i=1 are drawn from a process that is β-mixing with coefﬁcients β(m) satisfying
n ( ¯K ◦ ¯L)++) = 0 in

m=1 β(m)

δ

Similar to the case with the Lancaster statistic, 1
sumption. It can be written as a normalised V -statistic as:

n ( ¯K ◦ ¯L)++ is much easier to study than nHSICb under the non-i.i.d. as-

(cid:88)

1≤i,j≤n

nVn =

1
n

¯k ⊗ ¯l(Si, Sj)

where Si = (Xi, Yi). Again, the crucial observation is that

h = ¯k ⊗ ¯l

is well behaved in the following sense
Theorem 6. Suppose that k and l are bounded symmetric Lipschitz contentious kernels. Then h is also bounded symmetric
and Lipschitz continuous, which is moreover degenerate under H0.

Together, Theorems 5 and 6 justify use of the Wild Bootstrap in estimating the quantiles of the null distribution of the test
statistic nHSICb.

Proof. (Theorem 5) We can equivalently write HSICb as the norm of the empirically centred covariance operator, which
is invariant to population centering the feature maps:

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

n

n

φX (Xi) − 1
 ¯φX (Xi) − 1

n

n

(cid:88)
(cid:88)

i

i

(cid:88)
(cid:88)

j

j

HSICb =

=

 ⊗ 1
 ⊗ 1

n

n

φY (Yi) − 1
 ¯φY (Yi) − 1

n

n

(cid:88)
(cid:88)

i

i

(cid:88)
(cid:88)

j

j

φX (Xj)

¯φX (Xj)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

φY (Yj)

¯φY (Yj)

Expanding this, we can rewrite the above in terms of inner products involving the population centred covariance operator
and the population centred mean embeddings:

nHSICb = n(cid:107) ¯CXY (cid:107)2 − 2n(cid:104) ¯CXY , ¯µX ⊗ ¯µY (cid:105) + n(cid:107)¯µX ⊗ ¯µY (cid:107)2

The ﬁrst term in this expression can be written as n(cid:107) ¯CXY (cid:107)2 = 1
that the remaining two terms decay to zero in probability.

n

¯k(Xi, Xj)¯l(Yi, Yj) = 1
n

ij

(cid:80)

(cid:80)

ij h(Si, Sj). We show

By assumption, PXY = PXPY and thus the expectation operator factorises similarly. Therefore, for any A ∈
HS(FY ,FX ),

(cid:88)
(cid:88)
(cid:88)

i

i

i

EXY (cid:104)A, ¯CXY (cid:105) =

=

=

1
n

1
n

1
n

= 0

EXEY (cid:104)A, (φX (Xi) − µX ) ⊗ (φY (Yi) − µY )(cid:105)HS

EXEY (cid:104)φX (Xi) − µX , A (φY (Yi) − µY )(cid:105)FX

EY (cid:104)EX (φX (Xi) − µX ) , A (φY (Yi) − µY )(cid:105)FX

where the commutativity of EX with the inner product in the penultimate line follows from the Bochner integrability of
the quantity φX (X) − µX, which in turn follows from the conditions under which µX exists [Steinwart and Christmann,
2008]. It follows that EXY ¯CXY = 0.
Thus by Lemma 1 as before, it follows that (cid:107) ¯CXY (cid:107),(cid:107)¯µX(cid:107),(cid:107)¯µY (cid:107) = OP (n− 1
2 ).
It thus follows that the two latter quantities in the above expression for nHSICb decay to 0 in probability.

n(cid:104) ¯CXY , ¯µX ⊗ ¯µY (cid:105) ≤ n(cid:107) ¯CXY (cid:107)(cid:107)¯µX(cid:107)(cid:107)¯µY (cid:107)

= OP (n− 1
2 )

(cid:107)¯µX ⊗ ¯µY (cid:107)2 = n(cid:107)¯µX(cid:107)2(cid:107)¯µY (cid:107)2

= nOP (n−2)
= OP (n−1)

It follows that nHSICb

− 1

O(n

−−−−−→= n(cid:107) ¯CXY (cid:107)2 = 1

2 )

n ( ¯K ◦ ¯L)++, as required.

Proof. (Theorem 6)
To show degeneracy, ﬁx any si and observe that

ESh(si, S) = EXEY (cid:104) ¯φ(xi), ¯φ(X)(cid:105)(cid:104) ¯φ(yi), ¯φ(Y )(cid:105)
¯φ(Y )(cid:105)

¯φ(X)(cid:105)(cid:104) ¯φ(yi), EY

= (cid:104) ¯φ(xi), EX
= (cid:104) ¯φ(xi), 0(cid:105)(cid:104) ¯φ(yi), 0(cid:105) = 0

Symmetry is inherited from symmetry of k and l. Boundedness and Lipschitz continuity are implied by application of the
claims in Section A.5.

