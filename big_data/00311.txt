Why does Monte Carlo fail to work properly
in high-dimensional optimization problems?

6
1
0
2

 
r
a

M
1

 

 
 
]

B. Polyak and P. Shcherbakov∗

March 2, 2016

Abstract

.

C
O
h
t
a
m

[
 
 

1
v
1
1
3
0
0

.

3
0
6
1
:
v
i
X
r
a

The paper proposes an answer to the question formulated in the title.

1

Introduction

After the invention of the Monte Carlo (MC) paradigm by S. Ulam in the late 1940s, it has
become extremely popular in numerous application areas such as physics, biology, economics,
social sciences, etc. As far as mathematics is concerned, Monte Carlo methods showed them-
selves exceptionally eﬃcient in the simulation of various probability distributions, numerical
integration, estimation of the mean values of the parameters, global optimization (e.g., sim-
ulated annealing methods), [1, 2, 3], to name just a few. The salient feature of this approach
to solution of various problems of this sort is that “often,” it is dimension-free in the sense
that, given N samples, the accuracy of the result does not depend on the dimension of the
problem. However, direct use of the Monte Carlo approach for ﬁnding the minimum value of
a function over a set by straightforward sampling the argument set and picking the minimum
value of the function at these points, faces serious numerical problems. This phenomenon
has ﬁrst been observed as early as in the monograph [4] by Nemirovskii and Yudin, both in
deterministic and stochastic optimization setups, as applied to global optimization. Speciﬁ-
cally, they considered the minimax approach for the class of Lipschitz functions and proved
that, no matter what is the optimization method, it is possible to construct a problem which
will require exponential (in the dimension) number of function evaluations.

By way of fairness, to rehab the potentials of the Monte Carlo methods in optimization
problems, it should be stressed that alternative randomization MC-based techniques (for
instance, random walks, such as Hit-and-Run, shake-and-bake, billiards, etc., e.g., see [5],
[6] for the theory and [7] for implementation) are extremely eﬃcient if applied in a more
sophisticated fashion.

∗The authors are with The Institute of Control Science, Russian Academy of Sciences, Moscow, Russia.

E-mails: boris@ipu.ru; cavour118@mail.ru

1

There are still quite a number of research papers in this direction; for instance, in [8],
versions of the Markov Chain Monte Carlo methods were used to arrange randomized cutting
plane optimization algorithms with nice convergence properties. To facilitate the analysis,
statistical properties of the empirical minimum of a function over an N-sample of a random
vector in a convex body Q ⊂ Rn were studied in [8]. In particular, it was shown that the
worst-case geometry of Q that provides guaranteed probabilistic estimates of the minimum
was a cone.

In this paper we consider the case where both the sets and the functions to be optimized
have the simplest form; namely, Q is the unit Euclidean ball, and the functions are linear;
other regular shapes of Q, such as the unit cube are also discussed. One might believe that,
for convex functions and sets (in contrast with the setup in [4]) which diﬀer from the worst-
case cones, direct use of Monte Carlo is a success. However, we show that, for n large, such
an approach exhibits poor performance; i.e., the empirical min/max is very far from the true
optimal value. An explanation of this phenomenon and ﬁnding closed-form estimates is the
subject of this paper; numerical illustrations are presented.

A preliminary version of this paper is [9]. Several reﬁnements are performed in the current
text; ﬁrst, we changed the overall structure of the exposition; then, we provide a new result
on the probability of the empirical maximum of a linear function on a ball (Section 3.1), next,
we add a result on the expected value (end of Section 3.2), present closed-form results for the
l∞- and l1-norm balls (Section 4.1), discuss deterministic grids over a box (Section 4.2), and
accompany these new results with numerical illustrations. Finally, the introduction section
and the bibliography list are extended and various typos and inaccuracies are corrected.

2 Statement of the problem

In this section, we formulate the problems of interest and present two known facts which
form the basis for deriving the new results in Section 3.

2.1 The two optimization settings

Let Q ⊂ Rn denote a unit ball in one or another norm and let ξ(i)(cid:12)(cid:12)N

multisample of size N from the uniform distribution ξ ∼ U(Q).
We are targeted at solving the problems of the following sort.

1 =(cid:8)ξ(1), . . . , ξ(N )(cid:9) be a

I. Scalar optimization: Given the scalar-valued linear function

g(x) = c⊤x,

c ∈ Rn,

deﬁned on the unit ball Q ⊂ Rn, estimate its maximum value from the multisample.

More speciﬁcally, let η∗ be the true maximum of g(x) on Q and let

η = max{g(1), . . . , g(N )},

g(i) = g(ξ(i)),

i = 1, . . . , N,

2

(1)

(2)

be the empirical maximum; we say that η approximates η∗ with accuracy at least δ if

η∗ − η
η∗ ≤ δ.

Then the problem is: Given a probability level p ∈ (0, 1) and accuracy δ ∈ (0, 1),
determine the minimal length Nmin of the multisample such that, with probability at least p,
the accuracy of approximation is at least δ (i.e., with high probability, the empirical maximum
nicely evaluates the true one).

These problems are the subject of discussion in Sections 3.1 and 4.1.

II. Multiobjective optimization: Consider now 1 < m < n scalar functions gj(x), j =
1, . . . , m, and the image of Q under these mappings. The problem is to “characterize” the

boundary of the image set g(Q) ⊂ Rm via the multisample ξ(i)(cid:12)(cid:12)N

In rough terms, the problem is: Determine the minimal sample size Nmin which guaran-
tees, with high probability, that the image of at least one sample fall close to the boundary
of g(Q).

1 from Q.

For the case where Q is the Euclidean ball, the mappings gj(x) are linear, and m = 2, this
problem is discussed in Section 3.2; various statistics (such as the cumulative distribution
function, mathematical expectation, mode) of a speciﬁc random variable associated with
image points are evaluated.

2.2 Supporting material

The results presented in Section 3 below are based on the following two facts established
in [10]; they relate to the probability distribution of a speciﬁc linear or quadratic function
of the random vector uniformly distributed on the Euclidean ball.

Fact 1 [10]. Let the random vector ξ ∈ Rn be uniformly distributed on the unit Euclidean
ball Q ⊂ Rn. Assume that a matrix A ∈ Rm×n has rank m ≤ n. Then the random variable

ρ

.

=(cid:16)(AAT)−1Aξ, Aξ(cid:17)

has the beta distribution B( m

2 , n−m

2 + 1) with probability density function

Γ( n
2 + 1)
2 )Γ( n−m

2 + 1)

Γ( m

m
2

x

−1(1 − x)

n−m

2

0

for 0 ≤ x ≤ 1,
otherwise,

(3)

fρ(x) =


where Γ(·) is the Euler gamma function.

Alternatively, the numerical coeﬃcient in (3) writes

where B(·,·) is the beta function.

The second fact is an asymptotic counterpart of Fact 1.

3

Γ( n

2 +1)
2 )Γ( n−m

2 +1) = 1/B( m

Γ( m

2 , n−m

2 + 1),

Fact 2 [10]. Assume that for every n ≥ m, the matrix A(n) ∈ Rm×n has rank m, and ξ(n)
is a random vector uniformly distributed over the unit ball Q in Rn. Then, as n → ∞, the
random vector

ρ(n) = n1/2(cid:0)A(n)A⊤

(n)(cid:1)−1/2A(n)ξ(n)

tends in distribution to the standard Gaussian vector N (0, Im), where Im is the identity
m × m-matrix.

Note that for n ﬁxed, we have

i.e., Facts 2 and 1 characterize the asymptotic distribution of the vector ρ(n) and exact
distribution of its squared norm (normalized by the dimension).

kρ(n)k2 = nρ;

(4)

3 Main results: Ball-shaped sets

In this section we analyse the two optimization settings formulated in Section 2.1 for Q being
the n-dimensional unit l2-ball.

3.1 Scalar optimization

We consider the scalar case (1) and discuss ﬁrst a qualitative result that follows immediately
from Fact 1.

Without loss of generality, let c = (1, 0, . . . , 0)⊤, so that the function g(x) = x1 takes
its values on the segment [−1, 1], and the true maximum of g(x) on Q is equal to 1 (re-
spectively, −1 for the minimum) and is attained with x = c (respectively, x = −c). Let us
compose the random variable

ρ = g2(ξ),

which is the squared ﬁrst component ξ1 of ξ. By Fact 1 with m = 1 (i.e., A = c⊤), for the
probability density function (pdf) of ρ we have

fρ(x) =

Γ( n
2 + 1)
2)Γ( n+1
Γ( 1
2 )

x− 1

2 (1 − x)

n−1

2

.
= βn x− 1

2 (1 − x)

n−1

2 .

(5)

Straightforward analysis of this function shows that, as dimension grows, the mass of the
distribution tends to concentrate closer the origin, meaning that the random variable (r.v.) ρ
is likely to take values which are far from the maximum, equal to unity. To illustrate, Fig. 1
depicts the plot of the pdf (5) for n = 20.

We next formulate the following rigorous result.

Theorem 1 Let ξ be a random vector uniformly distributed over the unit Euclidean ball Q
and let g(x) = x1, x ∈ Q. Given p ∈ (0, 1) and δ ∈ (0, 1), the minimal sample size Nmin

4

25

20

15

10

5

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Figure 1: The probability density function (5) for n = 20.

that guarantees, with probability at least p, for the empirical maximum of g(x) to be at least
a δ-accurate estimate of the true maximum, is given by

Nmin =

lnh 1

2 + 1

ln(1 − p)
2I(cid:0)(1 − δ)2; 1

2 , n+1

2 (cid:1)i ,

(6)

where I(x; a, b) is the regularized incomplete beta function with parameters a and b.

Clearly, a correct notation should be Nmin = ⌈·⌉, i.e., rounding toward the next integer;

we omit it, but it is implied everywhere in the sequel.

Proof: We specify sample size N, and let ξ(i)(cid:12)(cid:12)N

distribution on Q; also introduce the random variable

1 be a multisample from the uniform

η = max
1≤i≤N

g(ξ(i)),

(7)

1

2, n+1

2 and n+1

the empirical maximum of the function g(x) = a⊤x, a = (1, 0, . . . , 0)⊤, from this multisam-
ple. We now estimate the probability P{η > 1 − δ}.
By Fact 1, the pdf of the r.v. ρ = g2(ξ) is given by (5), and its cumulative distri-
bution function (cdf) is known to be referred to as the regularized incomplete beta func-
tion I(cid:0)x; 1
2 , [13]. Due to the symmetry of the distri-
bution of ξ1, we have P{ρ > (1 − δ)2} = 2P{ξ1 > 1 − δ}, so that P{ξ1 > 1 − δ} =
2 (cid:1) and
2 I(cid:0)(1 − δ)2; 1
2 − 1
P{η ≤ 1 − δ} =h 1

2I(cid:0)(1 − δ)2; 1
(cid:1)iN
Letting P{η > 1 − δ} = p and inverting the last relation, we arrive at (6).

2 (cid:1) with parameters 1
2 (cid:1). Respectively, P{ξ1 ≤ 1 − δ} = 1
2 (cid:1)iN
2 I(cid:0)(1 − δ)2; 1
P{η > 1 − δ} = 1 −h 1

I(cid:0)(1 − δ)2;

2 , n+1
2 + 1

, so that ﬁnally

2 + 1

1
2

+

2

2 , n+1

2 , n+1

1
2

,

n + 1

2

.

(8)

(cid:4)

5

Numerical values of the function I(x; a, b) can be computed via use of the Matlab
routine betainc. For instance, with modest values n = 10, δ = 0.05, and p = 0.95, this
gives Nmin ≈ 8.9 · 106, and this quantity grows quickly as the dimension n increases.

Since we are interested in small values of δ, i.e., in x close to unity, a “closed-form” lower

bound for Nmin can be computed as formulated below.

Corollary 1 In the conditions of Theorem 1

Nmin > Nappr =

lnh1 − βn

n+1

1

ln(1 − p)
1−δ(cid:0)2δ − δ2(cid:1)(n+1)/2i ,

where βn = Γ( n

2 +1)
2 )Γ( n+1

2 ) = 1/B( 1

Γ( 1

2, n+1

2 ) .

Proof: We have

I(x; 1

2 , n+1

t−1/2(1 − t)(n−1)/2dt
t−1/2(1 − t)(n−1)/2dt − γnZ 1

x

x−1/2(1 − t)(n−1)/2dt

0

0

2 ) = βnZ x
= βnZ 1
> 1 − βnZ 1
= 1 − βnx−1/2Z 1−x
= 1 − βn

n + 1

2

x

0

v(n−1)/2dv

[ v = 1 − t ]

x−1/2(1 − x)(n+1)/2,

t−1/2(1 − t)(n−1)/2dt

[ since t−1/2 < x−1/2 for x < t < 1 ]

so that from (8) we obtain

and

1

n+1

1−δ(cid:0)2δ − δ2(cid:1)(n+1)/2iN

P{η > 1 − δ} > 1 −h1 − βn
ln(1 − p)
1−δ(cid:0)2δ − δ2(cid:1)(n+1)/2i < Nmin.

lnh1 − βn

n+1

1

Nappr =

(cid:4)

Further simpliﬁcation of the lower bound can be obtained:

Nappr > eNappr = −

p2π(n + 1) 1

ln(1 − p)
1−δ(cid:0)2δ − δ2(cid:1)(n+1)/2 .

This is doable by noting that ln(1 − ε) ≈ −ε for small ε > 0 and using the approximation
B(a, b) ≈ Γ(a)b−a for the beta function with large b. These lower bounds are quite accurate;
for instance, with n = 10, δ = 0.05, and p = 0.95, we have Nmin ≈ 8.8694 · 106, while
Nappr ≈ 8.7972 · 106 and eNappr = 8.5998 · 106.

6

3.2 Multiobjective optimization
Consider a (possibly nonlinear) mapping g : Rn → Rm, n ≫ m > 1; the goal is to characterize
the boundary of the image of a set Q ⊂ Rn under the mapping g. Apart from being
of independent interest, this problem emerges in numerous applications. In particular, if a
special part of the boundary, the Pareto front [11], is of interest, we arrive at a multiobjective
optimization problem. Numerous examples (e.g., see [12]) show that, for n large, the images
of the points sampled randomly uniformly in Q may happen to fall deep inside the true
image set, giving no reasonable description of the boundary and the Pareto front of g(Q).

We ﬁrst present a qualitative explanation of this phenomenon by using the setup of

Fact 2; i.e., the set Q is the unit Euclidean ball and the mappings are linear.

Since the squared norm of a standard Gaussian vector in Rm has the χ2-distribution

C(m) with m degrees of freedom [13], from Fact 2 and (4) we obtain

nρ → C(m)

in distribution as n → ∞. This is in compliance with the well-known result in the probability
theory, namely, ν2B(ν1, ν2) → C(ν1) in distribution as ν2 → ∞, [13]; here, B(ν1, ν2) stands for
the r.v. having the beta distribution with shape parameters ν1, ν2. For ν1 = 1 (i.e., m = 2,
the case most relevant to applications), Fig. 2 depicts the plots of the cumulative distribution
functions B(ν1, ν2) (see (11) below for the explicit formula) for ν2 = 1, 2, 5, 10, 20, 40 (i.e.,
n = 2, 4, 10, 20, 40, 80).

ν
2 = 40

ν
2 = 20

ν
2 = 10

ν
2 = 5
ν
2 = 2
ν
2 = 1

1

0.8

0.6

0.4

0.2

0

−0.2

0

0.2

0.4

0.6

0.8

1

1.2

Figure 2: Cumulative distribution functions B(1, ν2) for ν2 = 1, 2, 5, 10, 20, 40.

Hence, Fact 2 immediately implies the following important conclusion: Linear transfor-
mations essentially change the nature of the uniform distribution on a ball. Namely, as
the dimension of the vector ξ grows, with the rank of the transformation matrix A being
unaltered, the distribution of the vector Aξ tends to “concentrate closer the center” of the
image set.

7

We now turn to Fact 1 and provide quantitative estimates; to this end, consider the

simple case where m = 2 and the two mappings are linear:

g1(x) = c⊤

1 x,

kc1k = 1,

g2(x) = c⊤

2 x,

kc2k = 1,

c⊤
1 c2 = 0

(9)

2 (cid:19) in the notation of Fact 1); for instance, c1, c2 may be any two diﬀerent
(i.e., A = (cid:18) c⊤
unit coordinate vectors, so that g1(x) = xi and g2(x) = xj, i 6= j, are the two diﬀerent
components of x. Then the image of Q is the unit circle centered at the origin.

1
c⊤

Introduce now the random variable

ρ = g2

1(ξ) + g2

2(ξ),

(10)

the squared norm of the image of ξ ∼ U(Q) under mapping (9) (i.e., ρ = ξ2
j ). Then,
by Fact 1 with m = 2, we have the closed-form expressions for the cdf Fρ and pdf fρ of the
r.v. ρ:

i + ξ2

Fρ(x) =

fρ(x) =(cid:26) n

1 − (1 − x)

n

2

2 (1 − x) n

2

−1

0

1

0

for x < 0,
for 0 ≤ x ≤ 1,
for x > 1;

for 0 < x < 1,
otherwise.

(11)

(12)

With these in mind, let us evaluate the minimal length N of the multisample that guaran-
tees a given accuracy with a given probability. To this end, recall that, given a multisample

1 from the scalar cdf Fζ(x) with pdf fζ(x), the random variable

ζ (i)(cid:12)(cid:12)N

η = max{ζ (1), . . . , ζ (N )}

has the cumulative distribution function Fη(x) = F N

ζ (x) with pdf

fη(x) = F ′

η(x) = Nfζ(x)F N −1

ζ

(x),

which is, in our case (11)–(12) writes

fη(x) =

Nn
2

(1 − x)n/2−1(cid:16)1 − (1 − x)n/2(cid:17)N −1

.

(13)

We next evaluate several statistics of the r.v. η = max{ρ(1), . . . , ρ(N )}.
Probability: The theorem below determines the minimal sample size Nmin that guaran-
tees, with high probability, that a random vector ξ ∼ U(Q) be mapped close to the boundary
of the image set.

Theorem 2 Letting ξ be the random vector uniformly distributed over the unit Euclidean
ball Q ⊂ Rn, consider the linear mapping g(·) as in (9). Given p ∈ (0, 1) and δ ∈ (0, 1),

8

the minimal sample size Nmin that guarantees, with probability at least p, that at least one
sample be mapped at least δ-close to the boundary of the image set, is given by

For small δ we have

Nmin =

ln(1 − p)

ln(cid:16)1 − (2δ − δ2)n/2(cid:17) .

Nmin ≈ −

ln(1 − p)
(2δ − δ2)n/2 .

Proof: Let us specify sample size N and estimate the probability for a sample to be
mapped close to the boundary of the image set. To this end, denote the image of ξ ∼ U(Q)
under mapping (9) by g(ξ) =(cid:0)g1(ξ), g2(ξ)(cid:1)⊤
the maximum of kg(ξ)k over the multisample ξ(i)(cid:12)(cid:12)N

κ = max{kg(1)k, . . . ,kg(N )k},

1 . Also, consider the r.v. ρ = ρ(ξ) (10)

and introduce the r.v.

for which we have

Fρ(x2) = P{ρ ≤ x2} = P{kg(ξ)k2 ≤ x2} = 1 − (1 − x2)

n
2

and the r.v. η = max{ρ(1), . . . , ρ(N )}, the maximum of ρ over the multisample ξ(i)(cid:12)(cid:12)N

which we have

n

1 , for

P{η ≤ x2} = F N

ρ (x2) =(cid:16)1 − (1 − x2)

2(cid:17)N

.

(14)

Hence, noting that η = κ2, for a small δ > 0 (i.e., letting x = 1 − δ), we see that the
probability for at least one sample ξ(i) to be mapped at least δ-close to the boundary is equal
to

P{κ > 1 − δ} = 1 −(cid:16)1 − (2δ − δ2)

n

2(cid:17)N

.

Let p ∈ (0, 1) be a desired conﬁdence level; then, letting P{κ > 1− δ} = p and inverting
the last relation, we obtain the minimal required length of the multisample. The simple
approximation for Nmin follows from the fact that ln(1 − ε) ≈ −ε for small ε > 0.

(cid:4)

To illustrate, for modest dimension n = 10, accuracy δ = 0.05, and probability p =
0.95, one has to generate approximately N = 3.4· 105 random samples to obtain, with
probability 95%, a point which is δ-close to the boundary of the image set. A sharper
illustration of this phenomenon for n = 50 is given in Fig. 3, which depicts the images of
N = 100, 000 samples of ξ ∼ U(Q) under mapping (9). None of them falls closer than 0.35
to the boundary of the image set.

Mode: The pdf (13) can be shown to be unimodular, and we ﬁnd its mode by straight-

forward diﬀerentiating. Letting z = 1 − x, for the pdf we have
zn/2−1(cid:16)1 − zn/2(cid:17)N −1

Nn
2

f (z) =

.

9

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

−1

−0.5

0

0.5

1

Figure 3: The 2D image of the 50-dimensional ball and the result of the Monte Carlo sampling

Then f ′(z) = 0 writes

(n/2 − 1)zn/2−2(1 − zn/2)N −1 = zn/2−1(N − 1)(1 − zn/2)N −2 n

2

zn/2−1.

Simplifying, we obtain

hence,

,

zn/2 =

n − 2
nN − 2
xmax = 1 −(cid:16) n − 2

We thus arrive at the following result.

nN − 2(cid:17)2/n

.

Theorem 3 Letting ξ be the random vector uniformly distributed over the unit Euclidean
ball Q ⊂ Rn, consider the linear mapping g(·) as in (9) and the random variable

η = max
i=1,...,N

ρ(i),

the empirical maximum of the function ρ(x) = kg(x)k2 from the multisample ξ(i)(cid:12)(cid:12)N

The mode of the distribution of η is given by

1 of size N.

xmax = 1 −(cid:16) n − 2

nN − 2(cid:17)2/n

.

For large n we have an approximation

xmax ≈ 1 −

1
N 2/n .

The quantity xmax is seen to be essentially less than unity for large n, even if the sample
size N is huge. This means the r.v. η takes values far from the boundary of the image. For

10

40

35

30

25

20

15

10

5

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Figure 4: Probability density functions (13) for n = 20 and N = 10k, k = 2, . . . , 10

instance, let n = 20; then, to “obtain a point in the 0.1 vicinity of the boundary,” one has
to generate N ≈ 1010 random samples in Q. The family of the pdfs (13) is plotted in Fig. 4.
Expectation: We now estimate the mathematical expectation E of the empirical maxi-

mum.

Theorem 4 Under the conditions of Theorem 3 we have

E (η) = 1 −

2
n

B(cid:16) 2

n

, N + 1(cid:17),

(15)

where B(·,·) is the beta function.

Proof: If a r.v. ζ is positive and deﬁned on D = [0, d], then the expectation

E (ζ) =

dZ

0

(cid:16)1 − Fζ(x)(cid:17)dx,

where Fζ(x) is the cdf of ζ. Hence, having N samples ξ(i) of ξ ∼ U(Q) and the respective
r.v. ρ = ρ(ξ) (10) with support [0, 1], for the r.v. η = max{ρ(1), . . . , ρ(N )} we have

E (η) =

1Z
0 (cid:0)1 − Fη(x)(cid:1)dx,

where Fη(x) is given by (14). By change of variables z = (1− x)n/2, we arrive at the desired
result.

(cid:4)

For large n and N, numerical values of the expectation are close to those observed for
the mode; this is seen from the shape of the pdf (13), Fig. 4. More formally, having the

11

n , N + 1(cid:17) ≈ Γ( 2

approximation B(cid:16) 2
E (η) ≈ 1 −

n )(N + 1)−2/n for large N, from (15) we obtain

2
n

Γ( 2

n)(N + 1)−2/n = 1 − Γ( 2

n + 1)(N + 1)−2/n ≈ 1 − N −2/n.

For instance, with n = 20 and N = 109, we have E (η) = 0.8802 for the expectation and

xmax = 0.8754 for the mode.

4 Main results: Box-shaped sets

In this section, we consider the scalar optimization problem, however, for box-shaped sets,
i.e,, not related to Facts 1 and 2. We consider the scalar setup described in Section 2.1 along
with the deterministic approach based on use of regular grids.

4.1 A direct Monte Carlo approach
Consider the linear scalar optimization problem for the case where Q = [−1, 1]n. Clearly,
the results heavily depend on the vector c in the optimized function g = c⊤x; we consider
two extreme cases.

Case 1. First, let c = (1, 0, . . . , 0)⊤ and consider the empirical maximum

η = max{g(1), . . . , g(N )},

where g(i) is the ﬁrst component of the random vector ξ ∼ U(Q). Specifying δ ∈ (0, 1), we
obtain

P{η ≤ 1 − δ} = (1 − δ/2)N .

This quantity is seen to be independent of the dimension (which is obvious as it is). Now,
specifying a probability level p ∈ [0, 1], we obtain that the minimal required sample size
that guarantees accuracy δ with probability p is equal to

Nmin =

ln(1 − p)
ln(1 − δ/2)

.

For instance, with p = 0.95 and δ = 0.1, one has to generate just 59 points to obtain a 10%-
accurate estimate of the maximum with probability 95%, independently of the dimension.

Case 2. Now let c = (1, 1, . . . , 1)⊤; i.e., the optimized function is g(x) =Pn
i xi, so that
the maximum is attained at x = c⊤ and is equal to η∗ = n. In contrast to Case 1, Monte
Carlo sampling exhibits a totally diﬀerent behavior. Below, Vol(·) stands for the volume of
a set.

Theorem 5 Letting ξ be the random vector uniformly distributed over the unit l∞-norm ball
i xi. Given p ∈ (0, 1) and δ ∈ (0, 1),

Q = [−1, 1]n, consider the linear function g(x) = Pn

12

δ ≤ 1/n, the minimal sample size Nmin that guarantees, with probability at least p, for the
empirical maximum of g(x) to be at least a δ-accurate estimate of the true maximum, is
given by

For small δ and large n we have

Nmin < −

Nmin =

ln(1 − p)
ln(cid:0)1 − nnδn
2nn!(cid:1) .
√2πn
(δe/2)n ln(1 − p).

(16)

(17)

Proof: Let us specify a small δ ∈ (0, 1) and deﬁne

Qδ = {x ∈ Q :

nXi

xi ≥ n(1 − δ)},

so that the maximum of g(x), over Q \ Qδ is equal to n(1 − δ). For δ ≤ 1/n, the set Qδ is
seen to be the simplex with n + 1 vertices at the points v0 = (1, . . . , 1)⊤ and

vj = (1, . . . , 1, 1 − nδ
| {z }j

, 1, . . . , 1),

j = 1, . . . , n,

with Vol(Qδ) = | 1
we have

n!det(cid:0)v1 − v0; . . . ; vn − v0(cid:1)| = δnnn/n!. Since Vol(Q) = 2n, for ξ ∼ U(Q)
P{ξ ∈ Qδ} =

P{ξ ∈ Q \ Qδ} = 1 −

δnnn
2nn!

δnnn
2nn!

and

,

so that

P{η > n(1 − δ)} = 1 −(cid:16)1 −

δnnn

2nn!(cid:17)N

.

Equating this probability to p and inverting this relation leads to (16).

The lower bound (17) follows immediately from Stirling’s formula and the fact that

ln(1 − ε) ≈ −ε for small ε > 0.

(cid:4)

For n = 10 and the same values δ = 0.1 and p = 0.95, we obtain a huge Nmin ≈ 1.12·1010.

Even for n = 2, an “unexpectedly” large number Nmin ≈ 600 of samples are to be drawn.

l1-norm ball: The setup of Case 2 is of the same ﬂavor as the one where the set Q is
the unit l1-norm ball, and the optimized function is g(x) = c⊤x with c = (1, 0, . . . , 0)⊤. We
have a result similar to those in Theorems 1 and 5.

Theorem 6 Letting ξ be the random vector uniformly distributed over the unit l1-norm ball
Q = {x ∈ Rn : Pn
i=1 |xi| ≤ 1}, consider the linear function g(x) = x1. Given p ∈ (0, 1)
and δ ∈ (0, 1), the minimal sample size Nmin that guarantees, with probability at least p, for
the empirical maximum of g(x) to be at least a δ-accurate estimate of the true maximum, is
given by

(18)

Nmin =

ln(1 − p)
2δn(cid:1) .
ln(cid:0)1 − 1

13

n

1

2

3

4

5

10

15

l2
119
l∞ 119
119
l1

449
2.4 · 103
2.4 · 103

1.6 · 103
4.3 · 104
4.8 · 104

5.7 · 103
7.2 · 105
9.6 · 105

2 · 104
1.2 · 107
1.9 · 107

8.9 · 106
1.1 · 1013
6.1 · 1013

3.6 · 109
1019
2 · 1020

Table 1: lp-balls: Minimal required number of samples for δ = 0.05 and p = 0.95.

For small δ we have

Nmin ≈ −

ln(1 − p)
0.5 δn

.

Proof: The true maximum of g(x) on Q is equal to unity; we specify accuracy δ ∈ (0, 1)

and consider the set

Qδ = {x ∈ Q : x1 ≥ 1 − δ}.

We then have

Vol(Q) =

2n
n!

,

so that for ξ ∼ U(Q) we obtain

Vol(Qδ) =

(2δ)n
2·n!

,

P{ξ ∈ Q \ Qδ} =

Vol(Q \ Qδ)

Vol(Q)

= 1 −

1
2

δn,

and the rest of the proof is the same as that of the previous theorem.

(cid:4)

To compare complexity associated with evaluating the optimum of a linear function
over the l2-, l∞-, and l1-balls, we present a table showing the minimal required number of
samples for δ = 0.05, p = 0.95 and various dimensions, as per formulae (6), (16), and (18),
respectively.

These results are in consistence with intuition, since the l1-norm ball is “closer” in shape
to the worst-case conic set (see [8]), while the l∞-norm ball with c = (1, 1, . . . , 1)⊤ “takes an
intermediate position” between l2 and l1 (obviously, closer to l1).

4.2 Deterministic grids

In this section we brieﬂy discuss a natural alternative to random sampling the set Q. A
belief is that use of various deterministic grids might outperform straightforward Monte
Carlo. Again, we consider Q being the unit box [−1, 1]n, and the scalar function to be
optimized is g(x) = Pn
i=1 xi, so that the maximum is equal to n. We show that, even in
such a simple setting, deterministic grids happen to be computationally intensive in high
dimensions.

Uniform grid: Consider a positive integer M > 1 and the uniform mesh M on Q, with
cell-size ∆ = 2/(M + 1); the mesh is assumed not to cover the boundary of Q. The total

14

n; true max

2

3

4

5

10

15

20

Uniform grid 1.9960
1.9999
1.9974

Monte Carlo

LPτ

2.9406
2.9792
2.9676

3.7576
3.8373
3.8731

4.4118
4.6844
4.6981

6.0000
7.9330
7.8473

7.5000
10.2542
10.0796

6.6667
10.9470
11.8560

Table 2: l∞-box: Empirical maxima for the three methods with N = 106.

amount of points in the mesh is cardM = M n and the maximum of g(x) over M is equal
to gM = n(1 − ∆). To guarantee relative accuracy δ of approximation, i.e., gM = n·(1 − δ),
one needs cell-size to be ∆ = δ, hence, the overall number of mesh points is equal to

card M =(cid:16)2

δ − 1(cid:17)n

,

(19)

which amounts to a huge M ≈ 6.13 · 1012 for modest n = 10 and δ = 0.1. Interestingly,
to obtain the same accuracy with probability p = 0.99, Monte Carlo requires “just” Nmin =
1.7 · 1010 samples!

Sobol sequences: Another type of grids that can be arranged over boxes are low-
discrepancy point sets or quasi-random sequences [14]. In practice, they share many proper-
ties of pseudorandom numbers, e.g., such as those produced by the rand routine in Matlab.
Among the variety of quasi-random sequences, so-called LPτ sequences introduced by
I.M. Sobol in 1967, [15] (also see [16], [17] for recent developments) are widely used in
various application areas. This sophisticated mechanism heavily exploits the box shape of
the set; it is much more eﬃcient than purely deterministic uniform grids and may outperform
straightforward Monte Carlo.

In the experiments, we considered the function g(x) = Pn

i=1 xi deﬁned on Q = [−1, 1]n
and computed its maximum value over the points of an LPτ sequence of length N = 106
for various dimensions; this was performed by using the Matlab routine sobolset. The
corresponding results are given in the row “LPτ ” of Table 2. The row “Monte Carlo” presents
empirical maxima obtained by using Monte Carlo sampling with the same sample size N
(averaged over 100 realizations), and the row “Uniform grid” relates to the uniform mesh of
cardinality N.

It is seen that the uniform mesh exhibits a very poor relative performance as dimension
grows, while the results of LPτ and Monte Carlo approaches are much better and similar to
each other. Clearly, the absolute values are very far from the true maxima equal to n, since
N = 106 samples are not suﬃcient to obtain reasonable accuracy for dimensions n > 5.

Instead of computing the sample size for ﬁxed values of the accuracy δ as in Table 1,
here we ﬁx the sample size and compute the empirical maxima. The reason for such an
“inversion” is that, given δ and the speciﬁc linear function g(x), it is not quite clear how to
estimate the required length of the LPτ -sequence. To overcome this problem, one might ﬁx
a reasonable value of accuracy, compute the minimal sample size Nmin required for Monte

15

Carlo, run the LPτ -approach with this length, and compare the results. However, for large
dimensions n, the values of Nmin are huge, leading to very large computation times or even
memory overﬂow.

The values obtained for the uniform mesh were computed by inverting relation (19) and
using the actual grid with cardinality card M = ⌈N 1/n⌉n ≥ N, so the quantities presented
in row “Uniform grid” of the table are overly optimistic.

More importantly, the routine sobolset has several parameters, such as s = skip (choice
of the initial point in the sequence) and ℓ = leap (selection of every ℓth point). These play
the similar role as the seed value in Matlab pseudorandom number generators does, and
may happen to be crucial for the quality of the resulting LPτ sequence. In the experiments,
we used the default values s = ℓ = 1 (i.e., the ﬁrst N points), though for diﬀerent values of
s, ℓ, the respective estimates of the empirical maximum may diﬀer a lot. For instance, adopt-
ing ℓ = 133, for n = 5 we obtain a much better estimate 4.8653, while taking (intentionally
bad) ℓ = 128 leads to very poor 2.8734.

Finally, note that applications of uniform grids and LPτ sequences are limited to box-
shaped sets. Sets diﬀerent from boxes can in principle be embedded in tight enclosing
boxes with subsequent use of rejection techniques; however, the rejection rate usually grows
dramatically as the dimension increases.

5 Conclusions

The main contribution of the paper is an explanation of the reason why does a direct Monte
Carlo approach show itself ineﬃcient in high-dimensional optimization problems when esti-
mating the maximum value of a function from a random sample in the domain of deﬁnition.
First, attention was paid to linear functions and ball-shaped sets; using known results on the
uniform distribution over the ball, we characterized the accuracy of the estimates obtained
via a speciﬁc random variable associated with the function value. Also, a multiobjective
optimization setup was discussed. The results obtained testify to a dramatic growth of
computational complexity (required number of samples) as the dimension of the ball in-
creases. Same ﬂavor results are obtained for box-shaped sets; these also include analysis of
deterministic grids.

Conclusions analogous to those presented in this paper can be drawn from the estimates
of the mean value of the maximum of a function from a sample; using the technique from [8],
similar results can be obtained for all regularly shaped sets (such as balls, boxes, simplices,
etc.), and the minimal required sample size can be evaluated which guarantees, in the mean,
that the empirical maximum provides an estimate of the true maximum with a given accu-
racy.

16

References

[1] R. Tempo, G. Calaﬁore, F. Dabbene, Randomized Algorithms for Analysis and Control

of Uncertain Systems: with Applications, Springer, 2013.

[2] Handbook of Global Optimization, Kluwer, vol. 1, 1995, R. Horst and P. Pardalos (Eds.);

vol. 2, 2002, P.M. Pardalos and H.E. Romeijn (Eds.).

[3] A. Zhigljavsky, A. ˘Zhilinskas, Stochastic Global Optimization, New York: Springer Sci-

ence+Business Media, 2008).

[4] A. Nemirovski, D.B. Yudin, Problem Complexity and Method Eﬃciency in Optimization

(Wiley-Interscience Series in Discrete Mathematics), John Wiley, 1983.

[5] D.P. Kroese, T. Taimre, Z.I. Botev, Handbook of Monte Carlo Methods (Wiley Series

in Probability and Statistics), New York: John Wiley and Sons, 2011.

[6] B.T. Polyak, E.N. Gryazina, Billiard walk – a new sampling algorithm for control and
optimization, in: Proc. 19th World Congress of IFAC , Aug. 2014, Cape Town, South
Africa, pp. 6123–6128.

[7] G. van Valkenhoef, “Hit and Run” and “Shake and Bake” for sampling uniformly from

convex shapes, 2015, URL: http://github.com/gertvv/hitandrun.

[8] F. Dabbene, P. Shcherbakov, B. T. Polyak, A randomized cutting plane method with

probabilistic geometric convergence, SIAM J. Optimiz., 2010, 20, 6, 3185–3207.

[9] B.T. Polyak, P.S. Shcherbakov, Failure of the Monte Carlo methods in optimiza-
tion problems of high dimensions, in: “Stochastic Optimization in Informatics,” O.N.
Granichin (Ed.), St.-Petersburg State University, 2014, 10, 1, 89–100 (in Russian).

[10] B.T. Polyak, P.S. Shcherbakov, Random spherical uncertainty in estimation and robust-

ness, IEEE Transactions on Automatic Control , 2000, 45, 11, 2145–2150.

[11] K. Deb, Multiobjective optimization, in: E.K. Burke and G. Kendall (Eds.), Search
Introductory Tutorials in Optimization and Decision Support Tech-

Methodologies:
niques, New York: Springer Science+Business Media, 2014, pp. 444-463.

[12] B. Polyak, P. Shcherbakov, M. Khlebnikov, Quadratic image of a ball: Towards eﬃcient
description of the boundary, in: Proc. Int. Conf. Syst. Theory, Control, Computing
(ICSTCC 2014), Oct. 2014, Sinaia, Romania, pp. 105–110.

[13] S.S. Wilks, Mathematical Statistics, New York: John Wiley and Sons, 1962.

[14] L. Kuipers, H. Niederreiter, Uniform Distribution of Sequences, New York: John Wiley,

2005.

17

[15] I.M. Sobol, On the distribution of points in a cube and the approximate evaluation of

integrals, USSR Computat. Maths. Math. Phys., 1967, 7, 4, p. 86–112.

[16] R. Ststnikov, J. Matusov, Multicriteria Analysis in Engineering Using the PSI Method

with MOVI 1.0 , Kluwer Academic Publishers, 2002.

[17] R. Statnikov, J. Matusov, A. Statnikov, Multicriteria engineering optimization prob-
lems: Statement, solution and applications, Journal of Optimization Theory and Ap-
plications, 2012, 155, 2, p. 355–375.

18

