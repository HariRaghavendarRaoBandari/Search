6
1
0
2

 
r
a

 

M
4
1

 
 
]

V
C
.
s
c
[
 
 

1
v
9
3
1
4
0

.

3
0
6
1
:
v
i
X
r
a

SSSC-AM: A Uniﬁed Framework for Video Co-Segmentation by

Structured Sparse Subspace Clustering with Appearance and

Motion Features

Junlin Yao, and Frank Nielsen
´Ecole Polytechnique, France

Frank.Nielsen@acm.org

March 15, 2016

Abstract

Video co-segmentation refers to the task of jointly segmenting common objects appearing in
a given group of videos. In practice, high-dimensional data such as videos can be conceptually
thought as being drawn from a union of subspaces corresponding to categories rather than from
a smooth manifold. Therefore, segmenting data into respective subspaces — subspace clustering
— ﬁnds widespread applications in computer vision, including co-segmentation. State-of-the-art
methods via subspace clustering seek to solve the problem in two steps: First, an aﬃnity matrix
is built from data, with appearance features or motion patterns. Second, the data are segmented
by applying spectral clustering to the aﬃnity matrix. However, this process is insuﬃcient to
obtain an optimal solution since it does not take into account the interdependence of the aﬃnity
matrix with the segmentation. In this work, we present a novel uniﬁed video co-segmentation
framework inspired by the recent Structured Sparse Subspace Clustering (S3C) based on the
self-expressiveness model. Our method yields more consistent segmentation results. In order to
improve the detectability of motion features with missing trajectories due to occlusion or tracked
points moving out of frames, we add an extra-dimensional signature to the motion trajectories.
Moreover, we reformulate the S3C algorithm by adding the aﬃne subspace constraint in order
to make it more suitable to segment rigid motions lying in aﬃne subspaces of dimension at most
3. Experiments on MOViCS dataset demonstrate the eﬀectiveness of our approaches and its
robustness to heavy noise.

Keywords: Object video co-segmentation, subspace clustering, self-expressiveness model, sparse

representation, aﬃne motion subspaces.

1

Introduction, prior work and contributions

The past decades, we have witnessed an explosion in the availability of video data. Just to give a
ﬁgure, let us say that the amount of new videos uploaded to YouTube every minute is estimated
to be 300 hours! Segmenting videos into multiple spatio-temporal areas, and extracting useful
information eﬀectively and computationally eﬃciently from them becomes a crucial issue and has
broad applications, such as 3D reconstruction, action recognition, etc. Various methods have
been proposed to deal with this challenging video segmentation problem. For example, subspace

1

clustering based methods [1] seek to discover low-dimensional representation of high-dimensional
data points.

Co-segmentation [2, 3, 4] consists in segmenting simultaneously multiple images or videos into
several subregions denoting common objects. Thus co-segmentation searches for objects jointly
appearing in videos, and provides additional information in the absence of supervisory informa-
tion [4].

1.1 Previous work

1.1.1 Subspace clustering

Many subspace clustering algorithms have already been proposed, including algebraic algorithms [5],
iterative methods [6], statistical methods [7], and spectral clustering based methods [8, 9, 10].
Among these algorithms, the Structured Sparse Subspace Clustering (S3C for short) algorithm [11]
(2015) outperforms all others due to its framework promoting coherence between the aﬃnity matrix
and segmentation, as well as its robustness with respect to noise and outliers. However, S3C only
considers linear subspaces while it is frequent to encounter data lying in a union of aﬃne rather than
linear subspaces for dynamic data-sets. For instance, the motion segmentation problem consists in
clustering data drawn from 3-dimensional aﬃne subspaces. Though there exists a naive way that
data coming from aﬃne subspaces can be treated as if they lie in linear subspaces, this method may
be potentially unable to distinguish subspaces from each other [12]. Since video co-segmentation
involves motion segmentation and thus aﬃne subspaces, it is necessary for the subspace clustering
method to have the ability to deal with data points lying in aﬃne structures.

1.1.2 Video co-segmentation

Several methods [13, 14] have been proposed to tackle the video co-segmentation problem. In [13],
Chiu et al.
(2013) formulate a generative multi-video model to enable multi-class video co-
segmentation. In the presence of noisy motion information, such as objects moving together or
existence of similar motion, its segmentation performance degrades quickly. In [14], Wang et al.
(2014) propose a co-segmentation algorithm given facts exploited by subspace clustering based
method called Low-Rank Representation (LRR). However, their algorithm suﬀers from limitations
that it is unable to tackle defocus blur, noise and outliers. Our work is inspired by those recents
works, and seeks to better cope with videos corrupted by noise and outliers, as demonstrated by
our experiments.

1.2 Paper contributions

In this paper, we propose a uniﬁed framework for video co-segmentation that enables to obtain
better coherent segmentation results. We summarize our contributions as follows: First, we im-
prove the detectability of motion trajectories corrupted by missing data using extra-dimensional
signatures. Second, in order to deal with motion features lying in 3-dimensional aﬃne subspaces,
we reformulate the structured sparse subspace clustering by adding the aﬃne subspace constraint,
and solve this novel optimization problem eﬃciently using the Alternating Direction Method of
Multipliers (ADMM) method. By integrating this subspace clustering approach, we propose a uni-
ﬁed optimization framework, dubbed Structured Sparse Subspace Clustering with Appearance and
Motion Features (S3C − AM).

2

Figure 1: Three subspaces in R3: the ﬁrst and second group of 50 data points belong to two one-
dimensional subspaces, and the remaining 150 data points belong to a two-dimensional subspace.

1.3 Paper organization

In Section 2, we introduce the subspace clustering problem and the S3C algorithm for clustering
data points lying in a union of linear subspaces. In Section 3, we motivate and formulate the uniﬁed
framework for video co-segmentation. In Section 4, we study experimentally the eﬀectiveness our
framework on real video data sets. In Section 5, we discuss on some possible extensions of our work.
Finally, Section 5 concludes the paper. The notations used throughout the paper are concisely
recalled in Appendix A.

2 Subspace Clustering

2.1 Overview

Subspace Clustering consists in ﬁnding low-dimensional representation of high-dimensional data.
For example, given data points drawn from a three-dimensional space, we recover their subspace
structures via subspace clustering as depicted in Figure 1.

The subspace clustering problem can be framed mathematically as follows.

Problem 1 (Subspace clustering). Let X ∈ Rn×N be a real-valued matrix whose columns are N
data points drawn from a union of k subspaces Sj of Rn, of dimensions dj < min{n, N}, for
j = 1, . . . , k. Given X, the subspace clustering seeks to ﬁnd: a) the subspace to which each data

3

point belongs to. That is, the segmentation of the data points; b) the number of subspaces and their
intrinsic dimensions.

Note that ﬁnding the respective basis of subspaces can be obtained as a by-product of solving
the subspace clustering problem. Recent approaches to solving problem 1 are based on the self-
expressiveness model [10, 11], which states that each data point in a union of subspaces can be
expressed as a linear or an aﬃne combination of other points. When data points lie in a linear
subspace Sj of dimension dj, each data point can be written as a linear combination of dj other
points from Sj. Similarly, when data points lie in an aﬃne subspace Sj of dimension dj, each data
point can be written as an aﬃne combination of dj + 1 other points from Sj. The combination can
be represented using matrix notation by X = XZ, where X denotes the data matrix and Z the
matrix of coeﬃcients.

Although this combination is in general not unique, a sparse representation of a data point
ideally corresponds to a combination of a few points belonging to its own subspace [12]. By
solving a global sparse optimization problem, and obtaining the matrix of coeﬃcients, the spectral
clustering technique is then applied to infer the clustering of data points, whose theoretical bound
can be guaranteed by the normalized cut [15].
In addition, ideally, the number of the nonzero
elements in the sparse representation of a data point corresponds to the dimension of its subspace.
As for the number of subspaces, it can be found either by analyzing the eigenspectrum of the
Laplacian matrix of the graph constructed by Z [8], or by model selection techniques [16]. Finally,
the respective basis of subspaces can be readily obtained. Therefore, with the sparse representation
framework based on self-expressiveness, we can solve thoroughly the subspace clustering problem 1.
Compared to local spectral clustering-based algorithms, we can furthermore overcome challenges
such as dealing with points near the intersection of subspaces.

2.2 Structured sparse subspace clustering (S3C)

State-of-the-art subspace clustering methods based on the self-expressiveness model follow a two-
step approach to solve the subspace clustering problem 1: learning an aﬃnity matrix from data in
the ﬁrst step and segmenting data based on the aﬃnity matrix in the second step. This two-step
approach is generally suboptimal since it does not consider the interdependence of the aﬃnity with
the segmentation. In [11] (2015), Liu et al. seeks to combine these two steps by building a uniﬁed
optimization framework, dubbed Structured Sparse Subspace Clustering (SSSC or S3C) in which
obtaining aﬃnity matrix and segmentation of data are merged together. More speciﬁcally, this
approach considers the S3C problem as follows:

(cid:107)Z(cid:107)1,Q + λ(cid:107)E(cid:107)(cid:96)
s.t. X = XZ + E, diag(Z) = 0, Q ∈ Q.

l min
Z,E,Q

where X is the n × N data matrix and Z the coeﬃcient matrix. The norm (cid:107)·(cid:107)(cid:96) on the error
term E depends upon the prior knowledge about the pattern of noise. Q = [q1,··· , qk] refers to an
N × k binary segmentation matrix indicating the membership of each data point to each subspace.
That is, qi,j = 1 if the i-th column of X lies in subspace Sj and qi,j = 0 otherwise. The space of
segmentation matrices is deﬁned as Q = {Q ∈ {0, 1}N×k : Q1 = 1 and rank(Q) = k}. λ and α are
two trade-oﬀ parameters. The subspace structured (cid:96)1 norm of Z is deﬁned as follows:

4

rCl (cid:107)Z(cid:107)1,Q

(cid:88)
·
= (cid:107)Z(cid:107)1 + α(cid:107)Θ (cid:12) Z(cid:107)1

(cid:18)

|Zi,j|

(cid:13)(cid:13)(cid:13)q(i) − q(j)(cid:13)(cid:13)(cid:13)2(cid:19)

=

i,j

1 +

α
2

(cid:13)(cid:13)q(i) − q(j)(cid:13)(cid:13)2

where Θi,j = 1
2

with q(i) and q(i) the i-th and j-th row of matrix Q, respectively.

The optimization problem (1) ensures a sparse representation of data points by means of the
(cid:96)1 norm. The constraint X = XZ + E seeks to capture the self-expressiveness while diag(Z) = 0
prevents data points representing themselves. The introduction of (1) merges the aﬃnity and
segmentation together. By formulating the subspace clustering problem in this way, S3C is able to
yield more coherent clustering results. We build our uniﬁed framework for video co-segmentation
based on S3C.

3 A uniﬁed framework for video co-segmentation

In this section, we introduce our uniﬁed framework for video co-segmentation with both appearance
and motion features. Appearance feature and motion feature are both crucial in object segmentation
and recognition. On one hand, the appearance feature captures color or texture information of
objects. It emphasizes the salience of an object by which it stands out relative to its neighbors. On
the other hand, the motion feature shows how objects move. By exploiting together the appearance
and motion information, we simulate thus how human beings recognize and diﬀerentiate objects.
We given an overview of our framework as follows:

First, we build temporal superpixels in the preprocessing stage in order to reduce computa-
tional costs. We extract motion and appearance features from those superpixels. Furthermore, we
improve the detectability of motion features by adding an extra signature to the conventional mo-
tion trajectory matrix. Then, we formulate the uniﬁed optimization framework with the additional
aﬃne subspace constraint, and solve our global optimization problem eﬃciently using the Alter-
nating Direction Method of Multipliers (ADMM) method. Finally, we construct a uniﬁed aﬃnity
matrix from the optimal solution of the optimization problem, and obtain segmentation results by
applying spectral clustering to the aﬃnity matrix.

3.1 Preprocessing: Temporal superpixels

In order to reduce the computational complexity, we represent videos using Temporal SuperPixels
(TSPs) [17] which help build superpixel-wise spatiotemporal correspondences. TSPs are diﬀerent
from common over-segmentation techniques because those superpixel representations maintain the
most salient features of an image. In addition, we integrate SIFT ﬂow [18, 14] into the TSP frame-
work so as to improve the robustness of correspondences. We then extract motion and appearance
features at the level of TSPs.

3.2 Appearance features

We choose the appearance feature to be 10D color features in HSV color space. Gaussian mixture
model and discretization are then applied to the color features to obtain the color histogram [19].

5

3.3 Motion feature using an extra-dimensional signature

In order to capture motion information of objects, we need a function called a camera model to map
the three-dimensional world onto a two-dimensional image plane. In the case of imaging objects far
away from a camera, small diﬀerences in depth, or the perspective eﬀect, becomes less apparent [20].
The perspective projection can thus be approximated by an aﬃne projection with the aﬃne camera
model. Thus we construct the motion feature under the aﬃne projection model to ﬁt the aﬃne
subspace clustering method.

Traditionally [1], motion feature matrix is represented by (1).

 x11

...
xF 1

lM =



. . . x1N
...
. . . xF N

2F×N

where {xfj ∈ R2}f =1,...,F
j=1,...,N denotes the 2D projection of N 3D points on rigidly moving objects onto F
frames of a moving camera. However, in real-world problems, there often exist missing trajectories
caused by tracked points moving into or out of frames, or by occlusion. Conventionally, nearest
motion values in the same data column are padded to the missing entries [14]. While this simple
treatment sometimes results in acceptable segmentation, generally it reduces the detectability of
motion trajectories, making subspace clustering method unable to perform motion segmentation.
To overcome this limitation, in addition to padding motion values, we propose a new way to form
motion feature matrix by adding an extra-dimension signature so that {xfj ∈ R3}f =1,...,F
j=1,...,N with
xfj = (x y s)(cid:62). s denotes the signature added to the original motion trajectories. Diﬀerent values
of s indicate diﬀerent status of motion trajectories, such as missing trajectories caused by newly
appearing points or by dead points as shown in Table 1. The values of the signature, si, i = 1, 2, 3
are speciﬁed so as to suﬃciently distinguish one status from another. Here, we set si, i = 1, 2, 3 to
be 1, 3, 5, respectively.

Status of motion trajectories
Original complete trajectories

Value
s1 = 1
s2 = 3 Missing trajectories caused by new points
s3 = 5 Missing trajectories caused by dead points

Table 1: Meaning of signature s to status of motion trajectories: Depending on newly appearing
or disappearing feature points, we encode the event using an extra dummy dimension to get better
and more consistent subspace clustering results.

By marking the diﬀerence by means of the signature, motion features are more detectable in the
presence of missing trajectories, which is demonstrated experimentally in Figure 2. The subspace
clustering method succeeded in segmenting motions with signature (2b) while it fails in the absence
of signature (2c).

3.4 Uniﬁed optimization framework

We propose the following uniﬁed optimization framework (1). In order to make it work on aﬃne
subspaces, we add an aﬃne subspace constraint Z(cid:62)
k 1 = 1 to the original S3C framework (1), and

6

(a) Original frame

(b) With signature

(c) Without signature

Figure 2: Comparison of motion feature with and without signature.

formulate the optimization problem to solve as follows:

((cid:107)Zk(cid:107)1,Q + λk(cid:107)Ek(cid:107)1) + β(cid:107)Z(cid:107)2,1

(1)

where

K(cid:88)

k=1

min

Z1,Z2,...,ZK
E1,E2,...,EK

Z = (cid:0) vec(Z(cid:62)
 (Z1)11

(Z2)11

=

...

s.t. Xk = XkZk + Ek, diag(Zk) = 0, Z(cid:62)

k 1 = 1

1 ) vec(Z(cid:62)
2 )
(Z1)12
(Z2)12

··· vec(Z(cid:62)
···
(Z1)NN
···
(Z2)NN

K) (cid:1)(cid:62)


...

...

···

(ZK)11

(ZK)12

(ZK)NN

K×N 2

and (cid:107)Zk(cid:107)1,Q = (cid:107)Zk(cid:107)1 + α(cid:107)Θ (cid:12) Zk(cid:107)1. 0 and 1 are two vectors of appropriate dimensions ﬁlled with
0 and 1, respectively. Xk refers to diﬀerent feature data points. Here, with K = 2, X1 denotes
appearance features and X2 denotes motion features. The constraint Z(cid:62)
k 1 = 1 generalizes the
framework to aﬃne subspaces by writing each data point as an aﬃne combination of a few other
points. Since the linear subspace is a special case of the aﬃne subspace, our framework works
eﬀectively also on data points lying in linear subspaces.
(cid:107)Z(cid:107)2,1 is a penalty term which encourages a uniﬁed aﬃnity matrix inferred from appearance

and motion features since the (cid:96)2,1 norm can induce column sparsity of the matrix [14].

To solve the optimization problem (1), we notice that it is equivalent to the following problem

(2) which can be solved more eﬃciently.

K(cid:88)

C min

Z1,Z2,...,ZK
E1,E2,...,EK

((cid:107)Jk(cid:107)1,Q + λk(cid:107)Ek(cid:107)1) + β(cid:107)Z(cid:107)2,1

k=1
s.t. Xk = XkCk + Ek, Jk = Zk,
k 1 = 1

Ck = Jk − diag(Jk), C(cid:62)

We can solve this problem using the Alternating Direction Method of Multipliers (ADMM)
method [21], which is a powerful yet simple algorithm well suited to problems arising in applied

7

Algorithm 1 Uniﬁed framework for solving problem (1) with aﬃne subspace constraint

Input: Data matrix X, α, β, λ, ρ, and the number of subspaces.
1. Initialize: Θ0 = 0,  = 10−6
2. while not converged do
3. Given ΘT , initialize E = 0, C = Z = J = 0, Y (1) = 0, Y (2) = Y (3) = 0, Y (4) = 0
4. while not converged do
5.

, Y (4)

, Y (3)

, Y (2)

Update Jt, Ct, Et, and Zt according to (2) (2) (3) (5);
Update Y (1)
Update µt+1 ← ρµt;
Check the condition (cid:107)X − XC − E(cid:107)∞ < ; if not converged, then set t ← t+1; if converged,
obtain Z
end while

according to (5);

10. Given Z, construct the aﬃnity graph G given by W = |Z| + |Z|T.
11. Apply the spectral clustering technique to the aﬃnity graph with the input number of sub-

6.

7.

8.

9.

t

t

t

t

spaces as k and obtain ΘT +1.
Check the convergence condition (cid:107)ΘT +1 − ΘT(cid:107)∞ < 1; if not converged, T + 1 ← T

12.
13. end while
Output: Coeﬃcient matrix Z and the label l indicating subspaces where data points lie in

statistics and machine learning. The augmented Lagrangian is given by:

K(cid:88)

k=1

=

lL(Jk, Ck, Ek, Z, Y (1)

k

k

k

k

k

, Y (2)

, Y (3)
(cid:107)Jk(cid:107)1,Q + λk(cid:107)Ek(cid:107)1 + (cid:104)Y (1)
+(cid:104)Y (2)
+(cid:104)Y (4)

k

, Y (4)
; k = 1, . . . , K)
, Xk − XkCk − Ek(cid:105)
, Jk − Zk(cid:105)
((cid:107)Xk − XkCk − Ek(cid:107)2
F + (cid:107)Jk − Zk(cid:107)2
F ) + β(cid:107)Z(cid:107)2,1,

k 1 − 1(cid:105) +

, Ck − Jk + diag(Jk)(cid:105) + (cid:104)Y (3)
, C(cid:62)
+(cid:107)Ck − Jk + diag(Jk)(cid:107)2
k 1 − 1(cid:107)2

+(cid:107)C(cid:62)

µ
2

F

F

k

k

k

k

k

k

, Y (4)

, Y (3)

, Y (2)

where Y (1)
are Lagrange multipliers. ADMM allows us to update variables alter-
natively. In addition, since Jk, Ck, Ek, Y (1)
k with k = 1, . . . , K are separable in the
optimization problem, we can update them separately given k (using OpenMP for multi-cores pro-
cessors).
Update Jk. We update Jk by solving the following problem:

, Y (2)

, Y (3)

, Y (4)

k

k

k

lllJk,t+1 = arg min

Jk

1
µt

(cid:107)Jk(cid:107)1,Q +

1
2

(cid:107)Jk − diag(Jk) − Uk,t(cid:107)2

F

k,t /µt(cid:107)2
F ,
where (cid:107)Jk(cid:107)1,Q = (cid:107)Jk(cid:107)1 + α(cid:107)Θ (cid:12) Jk(cid:107)1 and Uk,t = Ck,t + 1

(cid:107)Jk − Zk,t + Y (3)

1
2

+

Y (2)
k,t .

µt

8

The closed-form solution for Jk is given by

 1

lJ i,j

k,t+1 =

2 S 1
µt

(1+αΘi,j
k )

S 1
µt

(1+αΘi,j
k )

(cid:19)
k,t − Y (3),i,j

k,t
µt

U i,j
t + Zi,j
k,t − Y (3),i,j
Zi,j

k,t
µt

(cid:18)
(cid:18)

(cid:19)

i (cid:54)= j,

i = j.

µ Y (2)

k,t . Sτ (·) is the shrinkage thresholding operator acting on every element
where Uk,t = Ck,t + 1
of a matrix. It is deﬁned as: Sτ (ν) = (|ν| − τ )+ sgn(ν) with the operator (ν)+ returning ν if it is
non-negative and returning zero otherwise.
Update Ck. We update Ck by solving the following problem:

The solution is given by

lllCk,t+1 = arg min

k

Ck

(cid:104)Y (1)
k,t , Xk − XkCk − Ek,t(cid:105)
, Ck − Jk,t+1 + diag(Jk,t+1)(cid:105)
k 1 − 1(cid:107)2

+ (cid:104)Y (2)
+ (cid:104)Y (4)
+ (cid:107)Ck − Jk,t+1 + diag(Jk,t+1)(cid:107)2
+ (cid:107)Xk − XkCk − Ek,t(cid:107)2
F )

k 1 − 1(cid:105) +

k,t , C(cid:62)

((cid:107)C(cid:62)

µt
2

F

F

(cid:20)

(cid:18)

Xk − Ek,t +

lllCk,t+1 = (X(cid:62)

k

k Xk + I + 11(cid:62))−1

X(cid:62)
+ Jk,t+1 − diag(Jk,t+1) − 1
µt
− 1
µt

) + 11(cid:62)(cid:21)

(1Y (4)(cid:62)

k,t

Y (2)
k,t

(cid:19)

1
µt

Y (1)
k,t

(2)

(3)

(4)

Update Ek. We update Ek by solving the following problem:

Ek,t+1 = arg min

Ek

λk
µt

(cid:107)Ek(cid:107)1 +

1
2

(cid:107)Ek − Vk,t(cid:107)2
F ,

where Vk,t = Xk − XkCk,t+1+ 1
Then the solution is given by

µt

Y (1)
k,t .

Ek,t+1 = S λ
µt

(Vk,t)

where Vk,t = Xk − XkCk,t+1+ 1
Update Z. We solve Z by solving the following problem:
(cid:107)Z(cid:107)2,1 + 1

Zt+1 = arg min

Y (1)
k,t .

µt

β
µt

2(cid:107)Z − Qt(cid:107)2
F ,

where

Z

Qt = (cid:2) vec(Q(cid:62)
 (Q1)11

(Q2)11

=

...

K) (cid:3)(cid:62)


1 ) vec(Q(cid:62)
2 )
(Q1)12
(Q2)12

··· vec(Q(cid:62)
···
(Q1)NN
···
(Q2)NN

...

...

···

(QK)NN

K×N 2

(QK)11

(QK)12

9

(a) Original frame

(b) Segmentation result

(c) motion only

(d) appearance only

Figure 3: Segmentation results on the video sequences

with Qk,t = Jk,t+1 +
Then the problem can be solved via Lemma 4.1 of [22] as follows:

Y (3)
k,t
µt

(cid:40) (cid:107)[Qt]:,i(cid:107)2− β

l[Zt+1]:,i =

µt
(cid:107)[Qt]:,i(cid:107)2
0

[Qt]:,i

if (cid:107)[Qt]:,i(cid:107)2 > β
otherwise .

µt

,

where [Zt+1]:,i and [Qt]:,i refer to the ith column of respective matrix. Zk,t+1 can then be obtained
from Zt+1.
Update Y (1)
step.

. We update the Lagrange multipliers by a simple gradient ascent

, Y (2)

, Y (3)

, Y (4)

k

k

k

k

lclY (1)
k,t+1 = Y (1)
Y (2)
k,t+1 = Y (2)
Y (3)
k,t+1 = Y (3)
Y (4)
k,t+1 = Y (4)

k,t + µt(Xk − XkCk,t+1 − Ek,t+1)
k,t + µt(Ck,t+1 − Jk,t+1 + diag(Jk,t+1))
k,t + µt(Jk,t+1 − Zk,t+1)
k,t + µt(C(cid:62)

k,t+11 − 1)

Finally, we update µ by µt+1 ← ρµt. The uniﬁed framework is summarized in Algorithm 1. For
the details of the derivation, we refer the readers to [21].

3.5 Co-segmentation algorithm

We construct the aﬃnity matrix S given by



(cid:118)(cid:117)(cid:117)(cid:116) K(cid:88)

(cid:118)(cid:117)(cid:117)(cid:116) K(cid:88)

(Zk)2
ji



l(S)ij =

1
2

(Zk)2

ij +

k=1

k=1

Then, we apply the spectral clustering technique [8] to the aﬃnity matrix S to obtain segmentation
results in the form of an array of label l indicating corresponding subspace of each data point.
For clarity, we summarize the S3C − AM algorithm in Algorithm 2. The overall processing is
summarized in Figure 4.

10

Start

Videos

Preprocessing to obtain TSPs

Extract appearance and motion information

Solve the optimization problem (1)

Segmentation results

Stop

Figure 4: Flow chart of the overall processing

Algorithm 2 S3C − AM
Input: Feature matrices Xk, k = 1, . . . , K, α, β, λ, ρ, and the number of subspaces.
1. Obtain Zk, k = 1, . . . , K via Algorithm 1;
2. Form the aﬃnity matrix S according to (5);
3. Apply the spectral clustering technique to the aﬃnity matrix S with the input number of

subspaces;

Output: The label l indicating subspaces where TSPs lie in

4 Experiments

In this section, we conduct experiments on the dataset Multi-Object Video Co-Segmentation
(MOViCS)1 proposed by [13] to evaluate the eﬀectiveness of the S3C − AM framework. This
dataset is based on real videos with several challenges including diﬀerent lighting conditions, simi-
lar appearance between objects and background, and motion blur.

4.1 Parameter setting
In S3C − AM, the parameters λk and α need to be set properly as in [12]. Parameters ρ and β are
set to be 1.2 and 2, respectively, which work well in practice.

1The MOViCS dataset can be found at http://www.d2.mpi-inf.mpg.de/datasets

11

Table 2: Results of co-segmentation on videos chicken on turtle and chicken. Columns 1, 2, and 3
correspond to frames 1, 9, and 17. Rows 1 and 4 correspond to the original video frames, rows 2
and 5 are multi-class segmentation, rows 3 and 6 correspond to the ﬁnal co-segmentation results.

4.2 Experiments on MOViCS

Some experiment results are shown in Figure 3.
In the example of a group of video imaging a
chicken on a turtle, we notice that our method successfully extract the common foreground, namely,
a chicken with a turtle, from the background. In order to better illustrate the eﬀect of combining
appearance and motion features in the framework, we compare segmentation only with the motion
feature (3c) to that solely with the appearance features (3d). It can be noticed that, with only
appearance features, objects with similar colors in the background may be segmented to the same
class of the foreground. On the other hand, segmentation without appearance information may
result in incompletely segmented objects. The combination of appearance and motion features gives
a satisfactory segmentation even in the presence of noise and blur in the original videos. Figure 6
displays the segmentation results for a few frames of the chicken on turtle video.

When one video of the video group provides more information, we can reﬁne the results and
obtain a more detailed foreground. For example, given a video of a chicken on a turtle and the
other video of only a chicken, our framework succeeded to extract the common foreground, i.e., the
chicken and thus obtain a reﬁned result as shown in Table 2. See also Figure 7 for performance
results on several frames of the two videos.

The corresponding computational cost is shown in Table 3. We obtain the runtime under the
following computer conﬁguration: Intel(R) Core(TM) i7-3610QM CPU @2.30GHz with 8 GB RAM.
Finally, we tested the robustness of our framework under the condition of heavy noise by adding
gaussian-distributed additive noise with variance of 0.25, or 5% amount of salt-and-pepper noise

12

Video

chicken on turtle

chicken

chicken+turtle

Preprocessing Clustering
35 minutes
30 minutes

1 hour
1 hour
1 hour

1 hour

Table 3: Computational cost (Python implementation on Intel(R) Core(TM) i7-3610QM CPU
@2.30GHz with 8 GB RAM).

Original

(a)
frame
corrupted by gaussian-
distributed noise

(b) Segmentation result (c)

Original

frame
corrupted by salt-and-
pepper noise

(d) Segmentation result

Figure 5: Segmentation results on corrupted videos

to the original video. It is shown that our framework is eﬀective even with heavy data corruption
as illustrated in Figure 5.

5 Conclusion and perspectives

We described a novel uniﬁed framework for video co-segmentation problem. By adding an extra-
dimensional signature to motion trajectories, we improve the detectability of motion features in
the subspace clustering framework. Moreover, we reformulated the S3C with the additional aﬃne
subspace constraint to successfully segment data points drawn from a union of aﬃne subspaces.
We then formulated the uniﬁed optimization framework by integrating the newly proposed sub-
space clustering algorithm and applied it to appearance and motion features. Experiments on the
MOViCS benchmark demonstrated the eﬀectiveness of our method. In future work, we would like
to extend our approach by generalizing the framework to objects with articulated motion. In ad-
dition, we plan also exploit a distributed optimization based on ADMM to improve the running
times. We concisely hint at those extensions as follows:

5.1 Object with articulated motion

As shown by the experiments, our approach segments eﬀectively common foreground from videos by
jointly making full use of the appearance and motion information. As for the motion information,
it is assumed that each object has a rigid-body motion which forms an aﬃne subspace. Though
this assumption works well in general, it is diﬃcult to handle objects with articulated motion. To
generalize our work, one possible solution is to encode motion information in a diﬀerent way so as
to express subspace structure yet to preserve properties of articulated motion. Another approach
consists in exploiting the inherent hierarchical structure of the articulated motion.

13

Frame 1

Frame 10

Frame 20

Frame 30

Figure 6: Results of video chicken on turtle. Columns 1 ,2, 3 and 4 correspond to frames 1, 10, 20,
and 30. Row 1 corresponds to the original video frames, row 2 is binary segmentation (foreground
mask), row 3 corresponds to the ﬁnal segmentation result (extracted foreground).

#fr

original

multi-class seg.

co-segmentation

original

multi-class seg.

co-segmentation

1

5

9

14

17

Figure 7: Results of co-segmentation on videos chicken on turtle and chicken. Rows 1, 2, 3, 4 and
5 correspond to frames 1, 5, 9, 14 and 17, respectively. Columns 1 and 4 correspond to the original
video frames, columns 2 and 5 are multi-class segmentation, columns 3 and 6 correspond to the
ﬁnal co-segmentation results.

14

5.2 Distributed optimization

Another possible extension of this work consists in taking advantage of the inherent parallel com-
puting structure of ADMM. As demonstrated in the Algorithm 1, ADMM takes the form of a
decomposition-coordination procedure, coordinating solutions to local subproblems to ﬁnd a solu-
tion to a global problem. Therefore, it is natural to develop parallel and distributed optimization
algorithm based on ADMM to solve eﬃciently the large-scale video co-segmentation problem.

A complete source code implementation of SSSC-AM in Python for reproducible research is

available from the Authors at request.

References

[1] Ren Vidal, “A tutorial on subspace clustering,” IEEE Signal Processing Magazine, vol. 28,

no. 2, pp. 52–68, 2010.

[2] Jiaming Guo, Loong-Fah Cheong, Robby T. Tan, and Steven Zhiying Zhou, “Consistent

foreground co-segmentation,” Computer Vision – ACCV, pp. 241–257, 2014.

[3] Wenguan Wang, Jianbing Shen, Xuelong Li, and Fatih Porikli, “Robust video object coseg-

mentation,” IEEE Transactions on Image Processing, vol. 24, no. 10, pp. 3137–3148, 2015.

[4] Armand Joulin, Francis Bach, and Jean Ponce, “Discriminative clustering for image co-
segmentation,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2010, pp. 1943–1950.

[5] R. Vidal, Yi Ma, and S. Sastry, “Generalized principal component analysis (gpca),” Pattern
Analysis and Machine Intelligence, IEEE Transactions on, vol. 27, no. 12, pp. 1945–1959,
2005.

[6] Pankaj K. Agarwal and Nabil H. Mustafa, “K-means projective clustering,” in Proceedings
of the Twenty-third ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database
Systems. 2004, PODS ’04, pp. 155–165, ACM.

[7] Martin A. Fischler and Robert C. Bolles, “Random sample consensus: A paradigm for model
ﬁtting with applications to image analysis and automated cartography,” Commun. ACM, vol.
24, no. 6, pp. 381–395, 1981.

[8] Ulrike von Luxburg, “A tutorial on spectral clustering,” Statistics and Computing, vol. 17,

no. 4, pp. 395–416, 2007.

[9] Baohua Li, Ying Zhang, Zhouchen Lin, and Huchuan Lu, “Subspace clustering by mixture
of gaussian regression,” in Proceedings of IEEE Conference on Computer Vision and Pattern
Recognition, 2015.

[10] Mahdi Soltanolkotabi and Emmanuel J.Cands, “A geometric analysis of subspace clustering

with outliers,” The Annals of Statistics, vol. 40, no. 4, pp. 2195–2238, 2012.

[11] Chun-Guang Li and Ren Vidal, “Structured sparse subspace clustering: A uniﬁed optimization
framework,” in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,
2015.

15

[12] Ehsan Elhamifar and Ren Vidal, “Sparse subspace clustering: Algorithm, theory, and appli-
cations,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 11,
pp. 2765–2781, 2013.

[13] Wei-Chen Chiu and M. Fritz, “Multi-class video co-segmentation with a generative multi-video
model,” in Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on,
June 2013, pp. 321–328.

[14] Chuan Wang, Yanwen Guo, Jie Zhu, Linbo Wang, and Wenping Wang, “Video object co-
segmentation via subspace clustering and quadratic pseudo-boolean optimization in an mrf
framework,” Multimedia, IEEE Transactions on, vol. 16, no. 4, pp. 903–916, June 2014.

[15] Jianbo Shi and J. Malik, “Normalized cuts and image segmentation,” IEEE Transactions on

Pattern Analysis and Machine Intelligence, vol. 22, no. 8, pp. 888–905, Aug 2000.

[16] Thomas Brox and Jitendra Malik, “Object segmentation by long term analysis of point tra-

jectories,” European Conference on Computer Vision, 2010.

[17] J. Chang, Donglai Wei, and J.W. Fisher, “A video representation using temporal superpixels,”
in Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, June 2013,
pp. 2051–2058.

[18] C. Liu, J. Yuen, and A. Torralba, “Sift ﬂow: Dense correspondence across scenes and its

applications,” in IEEE Trans. Pattern Anal. Mach. Intell., 2011.

[19] Dengsheng Zhang, Md. Monirul Islam, and Guojun Lu, “A review on automatic image anno-

tation techniques,” Pattern Recognition, vol. 45, no. 1, pp. 346362, 2012.

[20] Bryan Poling, “A tutorial on camera models,” .

[21] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein, “Distributed
optimization and statistical learning via the alternating direction method of multipliers,” Foun-
dations and Trends in Machine Learning, vol. 3, no. 1, pp. 1–122, 2010.

[22] Guangcan Liu, Zhouchen Lin, Shuicheng Yan, Ju Sun, Yong Yu, and Yi Ma, “Robust recovery

of subspace structures by low-rank representation,” CoRR, vol. abs/1010.2955, 2010.

16

A Notation

A, B
ai,j
ai
vec(A)

(cid:107)A(cid:107)1
(cid:107)A(cid:107)2,1
X
Z
diag(Z)
E
0
1
Q

q(i)
Q

Θ
(cid:12)
(cid:107)Z(cid:107)1,Q

Sτ (ν)
Aﬃne
space

M

sub-

Arbitrary m × n matrices
the (i, j)-th element of matrix A
The i-th column of A
mn × 1
columns of A on top of one another:
[a1,1, . . . , am,1, a1,2, . . . , am,2, . . . , a1,n, . . . , am,n](cid:62)

column vector

(cid:107)A(cid:107)1 =(cid:80)m
(cid:107)A(cid:107)2,1 =(cid:80)n

(cid:80)n
j=1 (cid:107)aj(cid:107)2 =(cid:80)n
j=1 |aij|

i=1 |aij|2(cid:1)1/2
(cid:0)(cid:80)m

j=1

i=1

obtained by stacking

the
vec(A) =

n × N data matrix. Each column refers to a data point.
N × N coeﬃcient matrix
diag(Z) ∈ RN , vector of the diagonal elements of Z
n × N noise matrix
Vector of appropriate dimensions ﬁlled with 0
Vector of appropriate dimensions ﬁlled with 1
N ×k segmentation matrix with k the number of subspaces.
Q = [q1,··· , qk] indicates the membership of each data
point to each subspace. qi,j = 1 if the i-th column of X lies
in subspace Sj and qi,j = 0 otherwise.
i-th row of matrix Q
Space of segmentation matrices: Q = {Q ∈ {0, 1}N×k :
Q1 = 1 and rank(Q) = k}.
N × N matrix with Θi,j = 1
Hadamard product or element-wise product: (A (cid:12) B)i,j =
(A)i,j · (B)i,j, A, B ∈ Rm×n
·
= (cid:107)Z(cid:107)1 +
Subspace structured (cid:96)1 norm:
), where α > 0

(cid:13)(cid:13)q(i) − q(j)(cid:13)(cid:13)2
(cid:13)(cid:13)q(i) − q(j)(cid:13)(cid:13)2

α(cid:107)Θ (cid:12) Z(cid:107)1 =(cid:80)

i,j |Zi,j|(1 + α

(cid:107)Z(cid:107)1,Q

2

is trade-oﬀ parameter.
Sτ (ν) = (|ν| − τ )+ sgn(ν)
A subset U ⊂ V of a vector space V is an aﬃne subspace if
there exists a u ∈ U such that U − u = {x − u|x ∈ U} is a
linear subspace of V.
Motion matrix:

2

 x11

...
xF 1

lM =



. . . x1N
...
. . . xF N

2F×N

where {xfj ∈ R2}f =1,...,F
rigidly moving objects onto F frames of a moving camera.

j=1,...,N denotes the 2D projection of N 3D points on

17

