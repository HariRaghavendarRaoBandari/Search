Semi-Supervised Veriﬁed Feedback Generation

Shalini Kaleeswaran Anirudh Santhiar Aditya Kanade

Indian Institute of Science, Bangalore

{shalinik,anirudh_s,kanade}@csa.iisc.ernet.in

6
1
0
2

 
r
a

 

M
5
1

 
 
]
E
S
.
s
c
[
 
 

1
v
4
8
5
4
0

.

3
0
6
1
:
v
i
X
r
a

ABSTRACT
Students have enthusiastically taken to online programming lessons
and contests. Unfortunately, they tend to struggle due to lack of
personalized feedback when they make mistakes. The overwhelm-
ing number of submissions precludes manual evaluation. There is
an urgent need of program analysis and repair techniques capable
of handling both the scale and variations in student submissions,
while ensuring quality of feedback.

Towards this goal, we present a novel methodology called semi-
supervised veriﬁed feedback generation. We cluster submissions
by solution strategy and ask the instructor to identify or add a cor-
rect submission in each cluster. We then verify every submission
in a cluster against the instructor-validated submission in the same
cluster. If faults are detected in the submission then feedback sug-
gesting ﬁxes to them is generated. Clustering reduces the burden on
the instructor and also the variations that have to be handled during
feedback generation. The veriﬁed feedback generation ensures that
only correct feedback is generated.

We have applied this methodology to iterative dynamic program-
ming (DP) assignments. Our clustering technique uses features of
DP solutions. We have designed a novel counter-example guided
feedback generation algorithm capable of suggesting ﬁxes to all
faults in a submission.
In an evaluation on 2226 submissions to
4 problems, we could generate veriﬁed feedback for 1911 (85%)
submissions in 1.6s each on an average. Our technique does a good
job of reducing the burden on the instructor. Only one submission
had to be manually validated or added for every 16 submissions.

1.

INTRODUCTION

Programming has become a much sought-after skill for superior
employment in today’s technology-driven world [1]. Students have
enthusiastically taken to online programming lessons and contests,
in the hope of learning and improving programming skills. Unfor-
tunately, they tend to struggle due to lack of personalized feedback
when they make mistakes. The overwhelming number of student
submissions precludes manual evaluation. There is an urgent need
of automated program analysis and repair techniques capable of
handling both the scale and variations in student submissions, while
ensuring quality of feedback.

A promising direction is to cluster submissions, so that the in-
structor provides feedback for a representative from each cluster
which is then propagated automatically to other submissions in
the same cluster [20, 41]. This provides scalability while keep-
ing the instructor efforts manageable. Many novel solutions have
been proposed in recent times to enable clustering of programs.
These include syntactic or test-based similarity [15, 44, 20, 13, 12],
co-occurrence of code phrases [36] and vector representations ob-
tained by deep learning [40, 41, 33]. However, clustering can be

Sumit Gulwani

Microsoft Research, Redmond
sumitg@microsoft.com

Clusters of submissions

Student submissions

?

?

?

?
?
?

?

?

?

Clustering by

solution strategy

?
??

?

?

?

?

??
Instructor-
validated

submissions

?

X

?

X

?

X

X

X

X

X

Veriﬁed feedback

X

?

generation

? ?

X

?

Submissions with feedback

Clusters with validated submissions

Figure 1: Semi-supervised veriﬁed feedback generation: ✓
is a
submission veriﬁed to be correct, ✗
is a faulty submission for
which feedback is generated and ? is an unlabeled submission
for which feedback is not generated.

correct only in a probabilistic sense. Thus, these techniques cannot
guarantee that the feedback provided manually by the instructor,
by looking only at some submissions in a cluster, would indeed be
suitable to all the submissions in that cluster. As a result, some
submissions may receive incorrect feedback. Further, if submis-
sions that have similar mistakes end up in different clusters, some
of them may not receive the suitable feedback. Instead of helping,
these drawbacks can cause confusion among the students.

To overcome these drawbacks, we propose a novel methodology
in which clustering of submissions is followed by an automated
feedback generation phase grounded in formal veriﬁcation. Fig-
ure 1 shows our methodology, called semi-supervised veriﬁed feed-
back generation. Given a set of unlabeled student submissions,
we ﬁrst cluster them by similarity of solution strategies and ask
the instructor to identify1 a correct submission in each cluster. If
none exists, the instructor adds a correct solution similar to the
submissions in the cluster; after which we do clustering again. In
the next phase, each submission in a cluster is veriﬁed against the
instructor-validated submission in the same cluster. If any faults
are detected in the submission then feedback suggesting ﬁxes to
them is generated. Because program equivalence checking is an un-
decidable problem, it may not be possible to generate feedback for
every submission. We let the instructor evaluate such submissions
manually. This is better than propagating unveriﬁed or incorrect
feedback indiscriminately.

The proposed methodology has several advantages. First, it uses
unsupervised clustering to reduce the burden on the instructor. The
supervision from the instructor comes in the form of identifying
a correct submission per cluster. Second, the veriﬁcation phase

1To minimize instructor’s efforts further, we discuss strategies to
suggest potentially correct submissions to the instructor.

✗
✗
1 void main() {
2

int i, j, n, max;
scanf("%d", &n); // Input
int m[n][n], dp[n][n]; // dp is the DP array
for (i = 0; i < n; i++)

for (j = 0; j <= i; j++)

scanf("%d", &m[i][j]); // Input
dp[0][0] = m[0][0]; // Initialization
for (i = 1; i < n; i++) {

for (j = 0; j <= i; j++) {

if (j == 0)

dp[i][j] = dp[i-1][j] + m[i][j]; // Update

else if (j == i)

dp[i][j] = dp[i-1][j-1] + m[i][j]; // Update

else if (dp[i-1][j] > dp[i-1][j-1])

dp[i][j] = dp[i-1][j] + m[i][j]; // Update

else dp[i][j] = dp[i-1][j-1] + m[i][j]; // Update

}

}
max = dp[n-1][0];
for (i = 1; i < n; i++)

if (dp[n-1][i] > max) max = dp[n-1][i];

printf("%d", max); // Output

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23
24 }

Figure 2: A correct submission for the matrix path problem.

complements clustering by providing certainty about correctness
of feedback. Clustering helps by reducing the variations required
to be handled during feedback generation. It would be difﬁcult to
check equivalence of very dissimilar submissions, e.g., an iterative
solution against a recursive solution. Since only submissions that
are similar (i.e., belong to the same cluster) are compared, there is
a higher chance of making equivalence checking work in practice.
Third, feedback is generated for each submission separately so it
is personalized. This is superior to propagating a manually-written
feedback indiscriminately to all the submissions in a cluster.

To demonstrate the effectiveness of this methodology, we apply
it to iterative dynamic programming assignments. Dynamic pro-
gramming (DP) [8] is a standard technique taught in algorithms
courses. Shortest-path and subset-sum are among the many prob-
lems that can be solved efﬁciently by DP. We design features that
characterize the DP strategy and extract them from student submis-
sions by static analysis and pattern matching. The features include
the types of the DP arrays used for memoizing solutions to sub-
problems, and how the sub-problems are solved and reused iter-
atively. The set of features we need is small and all features are
discrete valued. Therefore, our clustering approach is very simple
and works directly by checking equality of feature values.

We also propose a novel feedback generation algorithm called
counter-example guided feedback generation. The equivalence be-
tween two submissions is checked using syntactic simpliﬁcations
and satisﬁability-modulo-theories (SMT) based constraint solving.
If an equivalence check fails, our algorithm uses the counter-example
generated by the SMT solver to reﬁne the equivalence query. This
process terminates when our algorithm proves the equivalence, or
is unable to reﬁne the query. A trace of reﬁnements leading to a
logically valid equivalence query constitutes the feedback.

As an example, consider the matrix path problem2 taken from
a popular programming contest site CodeChef. A lower triangular
matrix of n rows is given. Starting at a cell, a path can be traversed
in the matrix by moving either directly below or diagonally below
to the right. The objective is to ﬁnd the maximum weight among the
paths that start at the cell in the ﬁrst row and ﬁrst column, and end in
any cell in the last row. The weight of a path is the sum of all cells

2http://www.codechef.com/problems/SUMTRIAN

2
3 }
4

1

5

6

7

8

int max(int a, int b) {
return a > b ? a : b;

int max_arr(int arr[]) {

int i, max;
max = arr[0];
for (i = 0; i < 100; i++)

if (arr[i] > max) max = arr[i];

return max;

9
10 }
11

int main() {

int n, i, j, A[101][101], D[101][101]; // D is the DP array
scanf("%d", &n);// Input
for (i = 0; i < n; i++)

for (j = 0; j <= i; j++)

scanf("%d", &A[i][j]); // Input
D[0][0] = A[0][0] ; // Initialization
for (i = 1; i < n; i++)

for (j = 0; j <= i; j++)

D[i][j] = A[i][j] + max(D[i-1][j], D[i-1][j-1]); // Update

int ans = max_arr(D[n-1]);
printf("%d", ans); // Output
return 0;

12

13

14

15

16

17

18

19

20

21

22

23
24 }

Figure 3: A faulty submission belonging to the same cluster as
the correct submission in Figure 2.

In the declaration: 1) Types of A and D should be int[n][n]
In the update:
2) Under guard j == 0,

compute D[i][j] = D[i-1][j] + A[i][j]
instead of D[i][j] = A[i][j] + (D[i-1][j]>D[i-1][j-1] ?

D[i-1][j] : D[i-1][j-1]).

3) Under guard (j != 0 && j == i),

compute D[i][j] = D[i-1][j-1] + A[i][j]
instead of D[i][j] = A[i][j] + (D[i-1][j]>D[i-1][j-1] ?

In the output: 4) Under guard true, compute maximum

D[i-1][j] : D[i-1][j-1]).

over D[n-1][0],...,D[n-1][n-1] instead of D[n-1][0],...,D[n-1][99].

Figure 4: The auto-generated feedback for the submission in
Figure 3 by verifying it against the submission in Figure 2.

along that path. Figure 2 shows an example correct submission
to this problem and Figure 3 shows a faulty submission. The two
programs are syntactically and structurally quite different but both
use 2D integer arrays for memoization and iterate over them in the
same manner. Our clustering technique therefore puts them into the
same cluster. This avoids unnecessarily creating many clusters but
requires a more powerful feedback generation algorithm that can
handle stylistic variations between submissions, e.g., Figure 3 uses
multiple procedures whereas Figure 2 uses only one.

Our algorithm automatically generates the feedback in Figure 4
for the faulty submission by verifying it against the correct submis-
sion. The ﬁrst correction suggests that the submission should use
array sizes as int[n][n] instead of hardcoded value of int[101][101].
The update to the DP array D at line 20 in Figure 3 misses some cor-
ner cases for which our algorithm generates corrections #2 and #3
above. The computation of output at line 22 should use the correct
array bounds as indicated by correction #4. This is a comprehen-
sive list of changes to correct the faulty submission.

We have implemented our technique for C programs and evalu-
ated it on 2226 student submissions to 4 problems from CodeChef.
On 1911 (85%) of them, we could generate feedback by either ver-
ifying them to be correct, or identifying faults and ﬁxes for them.
In addition to faults in wrong answers, we also found faults in
265 submissions accepted by CodeChef as correct answers! Like
most online contest sites, CodeChef uses test-based evaluation. Our

static veriﬁcation technique has a qualitative advantage over the
test-based approach of online judges. The submissions come from
1860 students from over 250 different institutes and are therefore
representative of diverse backgrounds and coding styles. Even then,
the number of clusters ranged only from 2–80 across the problems.
Our technique does a good job of reducing the burden on the in-
structor. On an average, using one manually validated or added
submission, we generated veriﬁed feedback on 16 other submis-
sions. We had to add only 7 correct solutions manually. While our
technique generated feedback automatically for 1911 submissions,
the remaining 315 (15%) submissions require manual evaluation.
Our technique is fast and on an average, took 1.6s to generate feed-
back for each submission.

Work on feedback generation so far has focused on introductory
programming assignments [46, 17, 47, 21]. In comparison, we ad-
dress the challenging class of algorithmic assignments, in particu-
lar, that of dynamic programming. The program repair approaches
for developers [27, 37, 24, 31, 29] deal with one program at a time.
We work with all student submissions simultaneously. To do so, we
propose a methodology inspired by both machine learning and veri-
ﬁcation. Unlike the developer setting, we have the luxury of calling
upon the instructor to identify or add correct solutions. We exploit
this to give complete and correct feedback but then our technique
must solve the challenging (and in general, undecidable) problem
of checking semantic equivalence of programs.

The salient contributions of this work are as follows:
• We present a novel methodology of clustering of submissions
followed by program equivalence checking within each clus-
ter. This methodology can pave way for practical feedback
generation tools capable of handling both the scale and vari-
ations in student submissions, while minimizing the instruc-
tor’s efforts and ensuring quality of feedback.

• We demonstrate that this methodology is effective by apply-
ing it to the challenging class of iterative DP solutions. We
design a clustering technique and a counter-example guided
feedback generation algorithm for DP solutions.

• We experimentally evaluate the technique on 2226 submis-
sions to 4 problems and generate veriﬁed feedback for 85%
of them. We show that our technique does not require many
inputs from the instructor and runs efﬁciently.

2. DETAILED EXAMPLE

We now explain in details how our technique handles the moti-

vating example from the previous section.
2.1 Clustering Phase

The two submissions in Figure 2 and Figure 3 are syntactically
and structurally quite different. Our technique extracts features of
the solution strategy in a submission. These features are more ab-
stract than low-level syntactic or structural features and put the su-
perﬁcially dissimilar submissions into the same cluster.

The solution strategy of a DP program is characterized by the
DP recurrence being solved [8]. The DP recurrence for the correct
submission in Figure 2 is as follows:

m[0][0]

dp[i-1][j]+m[i][j]

dp[i-1][j-1]+m[i][j]

max(dp[i-1][j],dp[i-1][j-1])+m[i][j]

if i = 0, j = 0
if i 6= 0, j = 0
if j = i 6= 0
otherwise




dp[i][j] =

where dp is the DP array, m is the input matrix of n rows, and i goes
from 0 to n-1, j goes from 0 to i and max returns the maximum of

the two numbers. The DP recurrence of the submission in Figure 3
is similar but misses the second and third cases above.

Comparing the recurrences directly would be ideal but extracting
them is not easy. Students can implement a recurrence formula in
different imperative styles. They may use multiple procedures (as
in Figure 3) and arbitrary temporary variables to hold intermediate
results. Rather than attempting to extract the precise recurrence,
we extract some features of the solution to ﬁnd submissions that
use similar solution strategies. For this, our analysis identiﬁes and
labels the DP arrays used in each submission. It also identiﬁes and
labels key statements that 1) read inputs, 2) initialize the DP array,
3) update the DP array elements using previously computed array
elements, and 4) generate the output. The comments in Figure 2
and Figure 3 identify the DP arrays and key statements.

We call a loop which is not contained within any other statement
as a top-level loop. For example, the loop at lines 9–19 in Figure 2
is a top-level loop but the loop at lines 10–18 is not. More gener-
ally, a statement which is not contained within any other statement
is a top-level statement. The features extracted by our technique
and their values for the submission in Figure 2 are as follows:

1. Type and the number of dimensions of the DP array: hint, 2i
2. Whether the input array is reused as the DP array: No
3. The number of top-level loops which contain update state-

ments for the DP array: 1

4. For each top-level loop containing updates to the DP array,

(a) The loop nesting depth: 2
(b) The direction of loop indices: h+, +i (indicating that
the respective indices are incremented by one in each
iteration of the corresponding loops)

(c) The DP array element updated inside the loop: dp[i][j]

Extracting these features requires static analysis and syntactic
pattern matching. The most challenging part is identifying which
array serves as a DP array. An array which is deﬁned in terms of
itself is identiﬁed as a DP array since in the DP recurrence, the
DP array appears on both sides. However, a student may use some
temporary variables to store intermediate results of DP computation
and pass values across procedures. As explained in Section 3.1, we
track data dependences inter-procedurally. In Figure 3, the array
elements D[i-1][j] and D[i-1][j-1] are passed to the procedure max.
Through inter-procedural analysis, our technique infers that the re-
turn value of max is indeed deﬁned in terms of these arguments
and hence, D is deﬁned in terms of itself at line 20. Thereby, it
discovers that D is a DP array.

The submission in Figure 3 yields similar feature values and is
clustered along with the submission in Figure 2. Note that our ob-
jective is not to use clustering to distinguish between correct and
incorrect submissions. We therefore do not encode the exact na-
ture of the initialization or update of the DP array in the features.
Analyzing these is left to the veriﬁcation and feedback phase.
2.2 Veriﬁcation and Feedback Phase

After clustering, suppose the instructor identiﬁes the submission
in Figure 2 as correct. Our objective is to verify the submission in
Figure 3 against it and suggest ﬁxes if any faults are found. For
brevity, we will refer to the submission in Figure 2 as the reference
and the submission in Figure 3 as the candidate.

By analyzing the sequence in which inputs are read, our tech-
nique infers that the candidate uses two input variables: an integer
variable n and a 2D integer array A, where n is read ﬁrst (line 13)
and A second (line 16). Their types respectively match the types
of the input variables n and m of the reference except that the can-
didate uses a hardcoded array size for A. Both submissions use 2D

Is ψ1
valid?

Counter-example E1

Yes

Is ψ2
valid?

Counter-example E2

Yes

Is ψ3
valid?

Counter-example

Yes

ψ1 ≡ pre ∧ ϕ1 ∧ ϕ2 =⇒ post

Using the counter-example E1, the algorithm dis-
covers that when j = 0, ϕ2 computes statement
s1 (line 20 in Figure 3) but according to ϕ1, it
should compute s2: D[i][j] = D[i-1][j]+A[i][j].
Let ϕ′
2 ≡ (j 6= 0 =⇒ s1) ∧ (j = 0 =⇒ s2).

ψ2 ≡ pre ∧ ϕ1 ∧ ϕ′

2 =⇒ post

Using the counter-example E2, the algorithm dis-
covers that when j = i ∧ j 6= 0, ϕ′
2 computes
s1 but according to ϕ1, it should compute s3:
D[i][j] = D[i-1][j-1]+A[i][j].
Let ϕ′′
i =⇒ s3) ∧ (j = 0 =⇒ s2).

2 ≡ (j 6= 0 ∧ j 6= i =⇒ s1) ∧ (j 6= 0 ∧ j =

ψ3 ≡ pre ∧ ϕ1 ∧ ϕ′′

2 =⇒ post

Correction #2

Correction #3

a) Successive equivalence queries and results

b) Reﬁnement steps and corrections suggested

Figure 5: Steps of the veriﬁed feedback generation algorithm for the DP update in the faulty submission in Figure 3.

integer DP arrays but the candidate hardcodes array size of the DP
array D also. While the reference can handle arbitrarily large in-
put matrices, the candidate can handle input matrices only up to
size 101 × 101. For smaller input matrices, the reference is more
space efﬁcient than the candidate. Our technique therefore emits
correction #1 in Figure 4 suggesting size declarations for A and D.
We check equivalence of matching code fragments of the two
submissions one-by-one. The matching code fragments are easy to
identify given the statement labels computed during feature extrac-
tion. For our example, line 20 of the candidate is an “update” state-
ment and lines 12, 14, 16 and 17 of the reference are also “update”
statements. Therefore, the top-level loop (say L2) at lines 18–20 of
the candidate matches the top-level loop (say L1) at lines 9–19 of
the reference. The question is whether they are equivalent.

We check equivalence of the loop headers ﬁrst. The input vari-
ables n in both submissions correspond to each other and are not
re-assigned before they are used in the respective loop headers.
Therefore, the loop headers of L1 and L2 are equivalent. Thus,
the corresponding loop indices are equal in each iteration.

To check equivalence of loop bodies, our algorithm formulates
an equivalence query ψ1 which asserts that in an (i,j)th iteration, if
the two DP arrays are equal at the beginning then they are equal at
the end of the iteration. The equivalence query is of the form:

ψ1 ≡ pre ∧ ϕ1 ∧ ϕ2 =⇒ post

where 1) pre encodes the equality of DP arrays, loop indices and
input variables at the beginning of the iteration, and the lower and
upper bounds on the loop index variables in the reference, 2) post
encodes the equality of DP arrays at the end of the iteration (we
syntactically check that input variables are not changed), 3) ϕ1 is a
formula encoding the statements in the loop body of the reference,
and 4) ϕ2 is a formula encoding the statements in the loop body of
the candidate. Converting a loop-free sequence of statements into
a formula is straightforward. For example, an if-statement such
as if(p) x = e is converted to a guarded equality constraint p :
x′ = e where x′ is a fresh variable. The predicates in an if-else
statement are propagated so as to make the guards mutually disjoint
and ﬁnally, the conjunction of all guarded equality constraints is
taken. We defer other technical details to Section 3.2.

As shown in Figure 5.a, the algorithm checks whether ψ1 is
a logically valid formula. The SMT solver ﬁnds the following

counter-example E1 which shows that the formula is not valid:

j = 0, dp[i][j] = 1, D[i][j] = 2, m[i][j] = A[i][j] = 1,

dp[i-1][j] = D[i-1][j] = 0, dp[i-1][j-1] = D[i-1][j-1] = 1

Following the usual convention, we use = as equality in formulae
and use == as equality in code. Similarly, 6= and != denote dise-
quality symbols in formulae versus code.

Our algorithm, called counter-example guided feedback genera-
tion algorithm, uses E1 to localize the fault in the candidate. It ﬁrst
identiﬁes which guards are satisﬁed by the counter-example in the
candidate and the reference, and whether they are equivalent. The
guard j = 0 is satisﬁed in ϕ1 and the implicit guard true for line 20
is satisﬁed in ϕ2. Since they are not equivalent, the algorithm infers
that the faulty submission is missing a condition. On the contrary, if
the guards turn out to be equivalent, the fault is localized to the as-
signment statement. It then derives a formula ϕ′
2 given in Figure 5.b
which lets the candidate compute line 20 under the guard j!=0 and
makes it compute D[i][j] = D[i-1][j]+A[i][j] under j==0. This
assignment statement is obtained from the assignment at line 12
under the guard j==0 of the reference in Figure 2 by substituting
the variables from the candidate. The algorithm records this reﬁne-
ment in the form of correction #2 of Figure 4.

As shown in Figure 5.a, it checks validity of ψ2 ≡ pre ∧ ϕ1 ∧
ϕ′
2 =⇒ post obtained by replacing ϕ2 (the encoding of candi-
date’s loop body) by ϕ′
2 (deﬁned in Figure 5.b). This results in a
counter-example E2 using which the algorithm discovers the miss-
ing case of j==i and generates correction #3 of Figure 4. For
brevity, we do not show the counter-example E2. A reﬁned equiv-
alence query ψ3 shown in Figure 5.b is computed. As shown in
Figure 5.a, this formula is valid and establishes that the faults in the
candidate can be ﬁxed using the synthesized feedback.

The input and initialization parts of the two submissions are found
to be equivalent. In our experiments, we observed certain repeat-
ing iterative patterns. The computation of a maximum over an
array in lines 6–8 in Figure 3 is one such example. We encode
syntactic patterns to lift these to certain predeﬁned functions. We
deﬁne _max which takes the ﬁrst and the last elements of a con-
tiguous array segment as arguments and returns the maximum over
the array segment. In Figure 3, the output expression in terms of
_max is _max(D[n-1][0], D[n-1][99]) and in Figure 2, the output
is _max(dp[n-1][0], dp[n-1][n-1]). A syntactic comparison between
the two leads to correction #4 in Figure 4.

3. TECHNICAL DETAILS

We now explain our approach for clustering submissions, and the

algorithm for veriﬁed feedback generation.
3.1 Clustering by Solution Strategy

The ﬁrst phase of our technique is to cluster submissions by the

solution strategy so that each cluster can be analyzed separately.

3.1.1 Feature Design

Section 2.1 has already introduced the features of a submission
that we use. Typically, in machine learning, a large number of fea-
tures are obtained and then the learning algorithm ﬁnds the impor-
tant ones (called feature selection). In our case, since the domain
is well-understood, we design a small number of suitable features
that provide enough information about the solution strategy.

In particular, we cluster two submissions together if 1) they use
the same type and dimensions for the DP arrays, 2) either both use
DP arrays distinct from the input arrays or not, and 3) there is a
one-to-one correspondence between top-level loops which contain
DP update statements — the loops should have the same depth, di-
rection and the DP array element being updated. Two submissions
in the same cluster can differ in all other aspects.

The rationale behind these features is simple: Checking equiv-
alence of two submissions which use the same types of DP arrays
and similar DP update loops is easier than if they do not share these
properties. For example, the subset-sum problem can be solved by
using either a boolean DP array or an integer DP array, but the two
implementations are hard to compare algorithmically. Recall the
matrix path problem stated in the Introduction. Consider a sub-
mission which traverses the matrix from top-to-bottom and another
which traverses it from bottom-to-top. Using one to validate the
other is difﬁcult and perhaps, even undesirable. The features of DP
update loops will prevent these submissions from being part of the
same cluster. Imposing further restrictions (by adding more fea-
tures) can make veriﬁcation simpler but will increase the burden on
the instructor by creating additional clusters.

The feature 4.c described in Section 2.1 requires a bit more ex-
planation. We want to get the DP array element being updated
inside each loop containing a DP update statement.
If two sub-
missions use different names for DP arrays and loop indices, we
cannot compare them. To compare them across submissions, our
technique uses canonical names for them: dp for the DP array and
loop indices i, j, k, etc. from the outer to inner loops. If a submis-
sion uses multiple DP arrays then we assign subscripts to dp.

3.1.2 Feature Extraction

Identifying input statements and variables is simple. We look
for the common C library functions like scanf. The case of out-
put statements is similar. A variable x is identiﬁed as a loop index
variable if 1) x is a scalar variable, 2) x is initialized before the
loop is entered, 3) x updated inside the loop and 4) x is used in
the loop guard. Identifying DP arrays requires more subtle analy-
sis discussed below. We call DP arrays, input variables and loop
indices in a submission as DP variables. All other variables are
called temporary variables.

To eliminate the use of a temporary variable x at a control loca-

tion l, we compute a set of guarded expressions

{g1 : e1, . . . , gn : en}

where the guards and expressions are deﬁned only over DP vari-
ables, and the guards are mutually disjoint. We denote this set by
Σ(l, x) and call Σ the substitution store. Semantically, if gk : ek ∈
Σ(l, x) then x and ek evaluate to the same value at l whenever gk

evaluates to true at l. The substitution store Σ is lifted in a nat-
ural manner to expressions and statements. For instance, for an
assignment statement s ≡ x = e, Σ(l, s) = {g1 : x = e1, . . . , gn :
x = en} where {g1 : e1, . . . , gn : en} = Σ(l, e).

Gulwani and Juvekar [16] developed an inter-procedural back-
ward symbolic execution algorithm to compute symbolic bounds
on values of expressions. While we are not interested in the bounds,
the equality mode of their algorithm sufﬁces to compute substitu-
tion stores. We refer the reader to [16] for the details.

To determine whether a statement s at location l is an initial-
ization or an update statement, we perform pattern matching over
Σ(l, s). If the same array appears on both sides of an assignment
statement then the array is identiﬁed as a DP array and the statement
is labeled as an update statement. A statement where the LHS is a
DP array and RHS is an input variable or a constant is labeled as
an initialization statement. In Σ(l, s), the temporary variables in s
are replaced by the guarded expressions from the substitution store.
This makes the labeling part of our tool robust even in presence of
temporaries and procedure calls. For example, suppose we have t
= x[i-1]; x[i] = t;. The second statement can be identiﬁed as an up-
date statement through pattern matching only if we substitute x[i-1]
in place of t on the RHS of the statement. In general, Σ(l, s) may
contain multiple guarded statements. If Σ(l, s) = {s1, . . . , sn}, we
require that all of s1, . . . , sn satisfy the same pattern and get the
same label. Extracting the feature values is now straightforward.

3.1.3 Clustering and Identifying Correct Submissions
All our features are discrete valued. Therefore, our clustering al-
gorithm is very simple and works directly by checking equality of
feature values. Once the clustering is done, we ask the instructor to
identify a correct submission from each cluster. To reduce instruc-
tor’s efforts, we can employ some heuristics to rank candidates in a
cluster and present them one-by-one to the instructor. For example,
we can use a small set of tests or majority voting on some other
features of submissions like the loop bounds of update loops.

The instructor can accept a submission as correct or add a mod-
iﬁed version of an existing submission. If none of this is possible,
the instructor can write a correct solution similar to the solutions
in the cluster. If a new submission is added, we perform clustering
again. The instructor may have correct solutions from a previous
offering of the course if the assignment is repeated from a previous
offering. The instructor can add them to the dataset even before we
apply clustering to the submissions.

3.2 Veriﬁed Feedback Generation

Once the submissions are clustered and the instructor has identi-
ﬁed a valid submission for each cluster, we proceed to the veriﬁed
feedback generation phase. We check semantic equivalence of a
submission from a cluster (called the candidate) with the instructor-
validated submission from the same cluster (called the reference).

3.2.1 Variable and Control Correspondence

Program equivalence checking is an undecidable problem.

In
practice, a major difﬁculty is establishing correspondence between
variables and control locations of the two programs [34]. We ex-
ploit the analysis information computed during feature extraction
to solve this problem efﬁciently.

Let σ be a one-to-one function, called a variable map. σ maps
the input variables and DP arrays of the reference to the corre-
sponding ones of the candidate. To obtain a variable map, the in-
put variables of the two submissions are matched by considering
the order in which they are read and their types. The DP arrays
are matched based on their types. If there are multiple DP arrays

with the same type in both submissions then all type-compatible
pairs are considered. This generates a set of potential variable maps
and equivalence checking is performed for each variable map sepa-
rately. The one which succeeds and produces the minimum number
of corrections is used for communicating feedback to the student.
In equivalence checking, we eliminate the occurrences of tempo-
rary variables using the substitution store computed during feature
extraction. We therefore do not need to derive correspondence be-
tween temporary variables – which simpliﬁes the problem greatly.
The feature extraction algorithm labels the input, initialization,
update and output statements of a submission. We refer to these
statements as labeled statements. The labeled statements give an
easy way to establish control correspondence between the submis-
sions. We now use the notion of top-level statements deﬁned in
Section 2.1. Let ˆR = [s1
1] be the list of all top-level state-
ments of the reference such that 1) each statement in ˆR contains at
least one labeled statement and 2) the order of statements in ˆR is
consistent with their order in the reference submission. It is easy to
see that the top-level statements in a submission are totally ordered.
Let ˆC = [s1
2 ] be the similar list for the candidate submis-
sion. Without loss of generality, from now on, we assume that there
is only one DP array in a submission and the top-level statements
are (possibly nested) loops.

2, . . . , sn

1, . . . , sk

A (top-level) loop in ˆR or

ˆC may contain multiple statements
which have different labels. For example, a loop may read the in-
put and also update the DP array. We call it a heterogeneous loop.
If a loop reads two different input variables then also we call it
a heterogeneous loop. Heterogeneous loops make it difﬁcult to es-
tablish control correspondence between the statement lists ˆR and ˆC.
Fortunately, it is not difﬁcult to canonicalize the statement lists us-
ing semantics-preserving loop transformations, well-known in the
compilers literature [3]. Our algorithm ﬁrst does loop splitting to
split a heterogeneous loop into different homogeneous loops.
It
then does loop merging to coalesce different loops operating on the
same variable. Speciﬁcally, it merges two loops reading the same
input array. It also merges loops performing initialization to the
same DP array. During merging, we ensure that there is no loop
in between the merged loops such that it reads from or writes to
the same variable or array as the merged loops. In our experience,
in most cases, these transformations work because loops reading
inputs or performing initialization of DP arrays do not have loop-
carried dependences or ad-hoc dependences between loops.

In contrast, by deﬁnition, loops performing DP updates do have
loop-carried dependences. We therefore do not attempt loop merg-
ing for such loops. The feature 3 in Section 2.1 tracks the number
of loops containing DP updates. Therefore, two submissions in the
same cluster already have the same number of loops containing DP
updates. Thus, clustering helps in reducing the variants that need
to be considered during feedback generation.

Let R and C be the resulting statement lists for the reference and
candidate submissions respectively. If they have the same length
and at each index i, the ith loops in the two lists 1) operate on the
variables related by a variable map σ, 2) the statements operating
on the variables carry the same labels and 3) the loops have the
same nesting depth and directions then we get the control corre-
spondence π : R → C. If our algorithm fails to compute variable
or control correspondence for the candidate then it exits without
generating feedback, implicitly delegating it to the instructor.

3.2.2 Equivalence Queries

Let s′

1 and s′

2 be the top-level loops from the reference and the
candidate such that π(s′
2. We ﬁrst use the substitution map
computed during feature extraction to eliminate temporary vari-

1) = s′

1 and s′
2.

ables and procedure calls in s′
2 by equivalent guarded ex-
pressions over only DP arrays, loop indices and input variables.
Let s1 = Σ(l1, s′
2) where l1 and l2 are control
locations of s′

1) and s2 = Σ(l2, s′

1 and s′

We formulate an equivalence query Φ for the iteration spaces
of s1 and s2. Let corr be the correspondence between the input
variables, DP arrays, and loop indices of s1 and s2 at the matching
nesting depths. We deﬁne iter1 to be the range of the loop indices
in s1 and guards1 to be the disjunction of all guards present in the
loop body of s1. Similarly, we have iter2 and guards2 for s2. The
equivalence query Φ is deﬁned as follows:

Φ ≡ corr =⇒ (iter1 ∧ guards1 ⇐⇒ iter2 ∧ guards2)

This query provides more ﬂexibility than using direct syntactic
checking between the loop headers. For example, suppose s1 is
for(i=1, i<=n, i++){true: s} and s2 is for(i′=0, i′<=n, i′++){i′
> 0: s′}. s1 executes s for 1 ≤ i ≤ n and s2 also executes s′ for 1 ≤
i′ ≤ n. A syntactic check will end up concluding that s2 executes
one additional iteration when i′ is 0. But our equivalence query
establishes equivalence between the iteration spaces as desired.

The formulation of the query Ψ to establish equivalence between
loop bodies of s1 and s2 is as discussed in Section 2.2. Even though
the submissions use arrays, we eliminate them from the queries. A
loop body makes use of only a ﬁnite number of symbolic array ex-
pressions. We substitute each unique array expression in a query by
a scalar variable while encoding correspondence between the scalar
variables in accordance with the variable map σ. We overcome
some stylistic variations when the order of operands of a commu-
tative operation differs between the two submissions. For example,
say s1 uses x[i+j] and s2 uses y[b+a] such that σ(x) = y, σ(i) = a
and σ(j) = b. The expressions i+j and b+a are not identical under
renaming but are equivalent due to commutativity. To take care of
this, we force a ﬁxed ordering among variables in the two submis-
sions for commutative operators. Sometimes, the instructor may
include some constraints over input variables as part of the prob-
lem statement. In the equivalence queries, our algorithm takes in-
put constraints into account and also adds array bounds checks. We
omit these details due to space limit.

3.2.3 Counter-Example Guided Feedback Generation
Algorithm 1 is our counter-example guided feedback generation
algorithm. Its input is a list Q of equivalence queries where each
query (Φi, Ψi) corresponds to the ith statements in the two submis-
sions. Φi encodes the equivalence of iteration spaces and Ψi of the
loop bodies. If the ith statements are not loops, Φi is true and Ψi
just checks equivalence of the loop-free statements. The output of
the algorithm is a list of corrections to the candidate submission.

Algorithm 1 iterates over the query list (line 1). For a query
(Φi, Ψi), it ﬁrst checks whether Φi is (logically) valid or not. If it
is not then the algorithm suggests a correction to make the iteration
spaces of the ith statements (loops) of the two submissions equal
(lines 2-4). It then enters a reﬁnement loop for Ψi at lines 7-23.

During each iteration of the reﬁnement loop, it checks whether
Ψi is valid.
If yes, it exits the loop (line 9). Otherwise, it gets
a counter-example α from the SMT solver and ﬁnds the guarded
statements that are satisﬁed by α. Let g1 : s1 ∈ ϕ1 and g2 :
s2 ∈ ϕ2 be those statements (line 11). The formulae ϕ1 and ϕ2
correspond to the encodings of the loop bodies of the reference and
the candidate respectively. Note that the conversion of statements
to guarded equality constraints (Section 2.2) ensures that the guards
within ϕ1 and within ϕ2 are pairwise disjoint.

Let ˆσ be the variable map which is same as the variable corre-
spondence σ but augmented with the correspondence between loop

Algorithm 1: Algorithm GENFEEDBACK
Input: A list Q = [(Φ1, Ψ1), . . . , (Φk, Ψk)] of equivalence queries
Output: A list of corrections to the candidate submission

1 foreach (Φi, Ψi) ∈ Q do
2
3

if ∃α 6|= Φi then

Suggest corrections to make the iteration spaces of the ith
statements of the two submissions equal

4
5
6
7
8
9
10
11
12
13

14
15

16
17

18

19
20

21

22

end
Let Ψi ≡ pre ∧ ϕ1 ∧ ϕ2 =⇒ post
k ← 0
repeat

k ← k + 1
if |= Ψi then break else

Let α 6|= Ψi be a counter-example
Let g1 : s1 ∈ ϕ1 and g2 : s2 ∈ ϕ2 s.t. α |= g1 and α |= g2
if |= pre =⇒ (g1 ⇐⇒ g2) then

ϕ′
2 ← ϕ2[g2 : s2/g2 : ˆσ(s1)]
Ψi ← Ψi[ϕ2/ϕ′
2]
Suggest computation of ˆσ(s1) instead of s2 under g2

else

h2 ← g2 ∧ ˆσ(g1); h′
ϕ′
2 ← ϕ2[g2 : s2/h2 : ˆσ(s1) ∧ h′
Ψi ← Ψi[ϕ2/ϕ′
2]
Suggest computation of ˆσ(s1) instead of s2 under h2

2 ← g2 ∧ ˆσ(¬g1)
2 : s2]

end

end

until k < δ
if k = δ then Suggest a correction to replace ϕ2 by ˆσ(ϕ1)

23
24
25 end

indices at the same nesting depths for the ith statements. The func-
tion ˆσ is lifted in a straightforward manner to expressions and as-
signments. The algorithm checks whether the guards g1 and g2 are
equivalent (line 12). If they are then the fault must be in the as-
signment statement s2. It therefore deﬁnes ϕ′
2 by substituting s2 by
ˆσ(s1) in ϕ2 (line 13) and reﬁnes Ψi by replacing ϕ2 by ϕ′
2 (line 14).
It suggests an appropriate correction for the candidate submission
(line 15). The other case when the guards are not equivalent leads
to the other branch (lines 16-21). The algorithm now splits the
guarded assignment g2 : s2 to make it conform to the reference
under h2 ≡ g2 ∧ ˆσ(g1), whereas, for h′
2 ≡ g2 ∧ ˆσ(¬g1), the can-
2 by
didate can continue to perform s2 (line 17).
replacing g2 : s2 by h2 : ˆσ(s1) and h′
2 : s2 (line 18). It then reﬁnes
Ψi by replacing ϕ2 by ϕ′
2 (line 19) and suggests an appropriate cor-
rection for the candidate submission (line 20). The reﬁnement loop
terminates when no more counter-examples can be found (line 9)
and thus, progressively ﬁnds all semantic differences between ith
statements of the two submissions.

It computes ϕ′

Each iteration of the reﬁnement loop eliminates a semantic dif-
ference between a pair of statements from the two submissions and
the loop terminates after a ﬁnite number of iterations. In practice,
giving a long list of corrections might not be useful to the student if
there are too many mistakes in the submission. A better alternative
might be to stop generating corrections after a threshold is reached.
We use a constant δ to control how many reﬁnements should be
attempted (line 23). If this threshold is reached then the algorithm
suggests a total substitution of ˆσ(ϕ1) in place of ϕ2 (line 24). In
our experiments, we used δ = 10.

Due to the explicit veriﬁcation of equivalence queries, our algo-
rithm only generates correct feedback. The feedback for the dec-
larations of the candidate are obtained by checking dimensions of
the corresponding variables according to σ.

4.

IMPLEMENTATION

Table 1: Summary of submissions and clustering results.

Problem

Total Clusters with
subs.

Clusters with
correct sub. manually added
correct sub.

SUMTRIAN 1983
144
MGCRNK
58
MARCHA1
41
PPTEST

Total

2226

78
23
4
2

107

2
3
2
0

7

We consider C programs for experimental evaluation. We have
implemented the source code analysis using the Clang front-end of
the LLVM framework [26] and use Z3 [10] for SMT solving. We
presently do not support pointer arithmetic.

In the pre-processing step, our tool performs some syntactic trans-
formations. It rewrites compound assignments into regular assign-
ments. For example, x += y is rewritten to x = x + y. A code
snippet of the form: scanf("%d", &a[0]); for (i = 1; i < n; i++)
scanf("%d", &a[i]);, where the input array is read in multiple state-
ments is transformed to use a single read statement. The above
snippet will be rewritten to for (i = 0; i < n; i++) scanf("%d",
&a[i]);. Sometimes, students read a scalar variable and then assign
it to an array element. Our tool eliminates the use of the scalar vari-
able and rewrites the submission so that the input is read directly
into the array element. Another common pattern is to read a se-
quence of input values into a scalar one-by-one and then use it in
the DP computation. For example, consider the code snippet: for (i
= 0; i < n; i++) for (j = 0; j < n; j++) { scanf("%d", &x); dp[i][j]
= dp[i-1][j] + x; }. It does not use an array to store the sequence
of input values. We declare an array and rewrite the snippet to use
it. When feedback is generated for the submission, an explanatory
note about the input array is added. In each of the syntactic trans-
formations, we ensure that the program semantics is not altered.

Many students, especially beginners, write programs with con-
voluted conditional control ﬂow, and unnecessarily complex ex-
pressions. In addition, the reﬁnement steps of our counter-example
guided feedback generation algorithm may generate complex guards.
To present clear and concise feedback even in the face of these pos-
sibilities, in the post-processing step, our tool simpliﬁes guards in
the feedback using the SMT solver. We use Z3’s tactics to remove
redundant clauses, evaluate sub-expressions to Boolean constants
and simplify systems of inequalities.

5. EXPERIMENTAL EVALUATION

To assess the effectiveness of our technique, we collected sub-

missions to the following 4 DP problems3 on CodeChef:

1. SUMTRIAN – Described in the Introduction section.
2. MGCRNK – Find a path from (1,1) to (N,N) in an N × N
matrix, so that the average of all integers in cells on the path,
excluding the end-points, is maximized. From each cell, the
path can extend to cells to the right or below.

3. MARCHA1 – The subset sum problem.
4. PPTEST – The knapsack problem.

We selected submissions to these problems that implemented an
iterative DP strategy in the C language. A user can submit solu-
tions any number of times. We picked the latest submissions from
individual users. These represent their best efforts and can beneﬁt

3http://www.codechef.com/problems/<problem-name>

Table 2: Results of feedback generation.

Problem

Veriﬁed as
correct (X)

Corrections
suggested (✗ )

Average Unlabeled
(?)

corrections

SUMTRIAN
MGCRNK
MARCHA1
PPTEST

Total

1049
61
9
3

1122

659
66
35
29

789

3.3
6.8
10.3
12.7

4.3

275
17
14
9

315

from feedback. We do not consider submissions that either do not
compile or crash on CodeChef’s tests. To enable automated test-
ing on CodeChef, the submissions had an outermost loop to iterate
over test cases – we identiﬁed and removed this loop automatically
before further analysis.

Table 1 shows the number of submissions for each problem.
SUMTRIAN had the maximum number of submissions (1983) and
PPTEST had the minimum (41). There were a total of 2226 sub-
missions from 1860 students representing over 250 institutions.
These submissions employ a wide range of coding idioms and many
possible solution approaches, both correct and incorrect. This is a
fairly large, diverse and challenging set of submissions.
5.1 Effectiveness of Clustering

Our features were quite effective in clustering submissions by
their solution strategies. Since we do not include features repre-
senting low-level syntactic or structural aspects of submissions, the
clustering resulted in only a few clusters for each problem, without
compromising our ability to generate veriﬁed feedback. Table 1
gives the number of clusters. The number of clusters increased
gracefully from the smallest problem (by the number of submis-
sions) to the largest one. The smallest problem PPTEST yielded
only 2 clusters for 41 submissions, whereas, the largest problem
SUMTRIAN yielded 80 clusters for 1983 submissions. Our manual
evaluation revealed that in each cluster, the solutions were actually
following the same DP strategy.

The small number of clusters reduces the burden on the instruc-
tor signiﬁcantly. Instead of evaluating 2226 submissions separately,
the instructor is required to look at representatives from only 114
clusters. CodeChef uses test suites to classify problems into cor-
rect and incorrect. As a simple heuristic, we randomly picked one
of the submissions marked as correct by CodeChef in each cluster
and manually validated it. As shown in Table 1, this gave us cor-
rect representatives for 107/114 clusters across the problems. The
remaining 7 clusters seemed to follow some esoteric strategies and
we manually added a correct solution to each of them.

Clustering also helps the instructor get a bird’s eye view of the
multitude of solution strategies. For example, it can be used to
ﬁnd the most or least popular strategy used in student submissions.
In SUMTRIAN, the most popular strategy (with 677 submissions)
was the one that traverses the matrix rows bottom up, traverses the
columns left to right and updates the element (i, j).
5.2 Effectiveness of Feedback Generation

Our tool veriﬁes a submission from a cluster against the manu-
ally validated or added correct submission from the same cluster.
Table 2 shows the number of 1) submissions veriﬁed as correct
(X), 2) submissions for which faults were identiﬁed and correc-
tions suggested (✗ ) and 3) submissions which our algorithm could
not handle (?). Across the problems, 1122 submissions amounting
to 50% were veriﬁed to be correct, with the maximum at 53% for
SUMTRIAN and the minimum at 7% for PPTEST. For a total of
789 submissions amounting to 35%, some corrections were sug-

U only

I&U&O

I&U

7.9%

7.9%

16.4%

Correct

7.9%

2.9%
0.7%

O only
I only

26.4%

30%

U&O

Others

Figure 6: Distribution of submissions in a cluster of SUM-
TRIAN by the type of feedback.

gested by our tool. The maximum percentage of submissions with
corrections were for PPTEST at 71% and the minimum was 33%
for SUMTRIAN. Many submissions had multiple faults. Table 2
shows the average number of corrections over faulty submissions
for each problem. PPTEST required the maximum number of cor-
rections of 12.7 on average.
In all, our tool succeeded in either
verifying or generating veriﬁed feedback for 85% submissions.

For the remaining 315 (15%) submissions, our tool could neither
generate feedback nor verify correctness. These submissions need
manual evaluation. MARCHA1 had the maximum percentage of
unlabeled submissions at 24% and MGCRNK had the minimum
at 12%. These arise either because the SMT solver times out (we
kept the timeout of 3s for each equivalence query), or due to the
limitations of the veriﬁcation algorithm or the implementation.

These results on the challenging set of DP submissions are en-
couraging and demonstrate effectiveness of our methodology and
technique. Even if we assume that all 315 unhandled submissions
are faulty, we could generate veriﬁed feedback for 71% faulty sub-
missions. In comparison, on a set of introductory programming as-
signments, Singh et al. [46] report that 64% of faulty submissions
could be ﬁxed using manually provided error models. Our counter-
example guided feedback generation technique guarantees correct-
ness of the feedback.
In addition, we would have liked to com-
municate the feedback to the students and assess their responses.
Unfortunately, their contact details were not available to us.

Diversity of Feedback and Personalization.

The feedback propagation approaches [20, 41] suggest that the
same feedback text written by the instructor can be propagated to
all submissions within a cluster. We found that this is not practical
and the submissions within the same cluster require heterogeneous
feedback. Figure 6 shows the distribution of submissions in a clus-
ter of SUMTRIAN by the type of feedback. We only highlight feed-
back over the logical components of a submission: initialization (I),
update (U) and output (O). Feedback related to type declarations
and input statements (possibly, in conjunction with feedback on the
logical components) is summarized under the category “Others”.

While only 7.9% submissions were veriﬁed to be correct, 20%
submissions had faults in one of the logical components of the DP
strategy: initialization (0.7%), update (16.4%) and output (2.9%).
As shown in Figure 6, a large percentage of submissions had faults
in two logical components, and 7.9% had them in all three compo-
nents. 30% of the submissions were in the others category. Clearly,
it would be difﬁcult for the instructor to predict faults in other sub-
missions in a cluster by looking only at some submissions in the
cluster and write feedback applicable to all. We do admit that Fig-
ure 6 is based on our clustering approach and other approaches may
yield different clusters. Even then, the clusters would be correct

Table 3: Submissions by faulty components.

Faulty comp.

SUMTRIAN MGCRNK MARCHA1 PPTEST

36
229
31
29
10
97
30
197

659

15
7
1
18
0
1
0
24

66

0
5
6
2
1
2
11
8

35

0
2
0
8
0
0
0
19

29

Before Simpliﬁcation
After Simpliﬁcation

I only
U only
O only
I&U
I&O
U&O
I&U&O
Others

Total

e
z
i
S
k
c
a
b
d
e
e
F

600

500

400

300

200

100

0

0

5

10

15

30

35

40

45

20

25
Submission IDs

Figure 7: Effect of simpliﬁcation on feedback size for MGCRNK

only in a probabilistic sense and the veriﬁcation phase, we suggest,
would add certainty about correctness of feedback.

Our technique generated personalized feedback depending on
which components of a submission were faulty. Table 3 shows
the number of submissions by the faulty components. Across the
problems, PPTEST had the maximum percentage 53.7% of sub-
missions requiring corrections to multiple logical components and
SUMTRIAN had the minimum percentage 17.5%. The most com-
mon faulty components varied across problems.

Types of Faults Found and Corrected.

Our tool found a wide range of faults and suggested appropriate
corrections for them. This is made possible by availability of a cor-
rect submission to verify against and the ability of our veriﬁcation
algorithm to reﬁne the equivalence queries to ﬁnd all faults. The
faults found and corrected include: incorrect loop headers, initial-
ization mistakes including missing or spurious initialization, miss-
ing cases in the DP recurrence, errors in expressions and guards,
incorrect dimensions, etc.

Conciseness of Feedback.

To reduce the size of formulae in the generated feedback, we
perform simpliﬁcations outlined in Section 4. We measure the ef-
fectiveness of the simpliﬁcations by disabling them and using the
sum of AST sizes (#nodes in the AST) of the guards in our feed-
back text as feedback size. Figure 7 shows the impact of the sim-
pliﬁcations on feedback size in the case of MGCRNK by plotting
submission IDs versus feedback size. The ﬁgure excludes cases
where simpliﬁcation had no impact on feedback size. Simpliﬁca-
tions ensured that the feedback size was at most 150, and 42.1 on
average. Without simpliﬁcations, the maximum feedback size was

599. Simpliﬁcations, where applicable, reduced feedback size by
63.1% on an average across the problems.

5.3 Comparison with CodeChef

Our tool was able to verify 12 submissions as correct that were
tagged by CodeChef as incorrect. This was surprising because
CodeChef uses tests which should not produce such false positives.
On investigation, we found that the program logic was indeed cor-
rect, as veriﬁed by our tool. The faults were localized to output
formatting, or in custom input/output functions. Understandably,
black-box testing used by CodeChef cannot distinguish between
formatting and logical errors. However, being able to distinguish
between these types of faults would save time for the students. Our
tool ﬁnds logical faults but not formatting errors.

Due to the incompleteness of testing, CodeChef did not iden-
tify all faulty submissions (false negatives). This can hurt students
since they may not realize their mistakes. We checked the cases
when CodeChef tagged a submission as correct but our tool is-
sued some corrections. For 64 submissions, our tool identiﬁed that
the submissions were making spurious initializations to the DP ar-
ray. For 112 submissions, our tool identiﬁed that the DP udpate
was performed for additional iterations than required and generated
feedback to ﬁx the bounds of loops containing update statements.
Importantly, our tool detected out-of-bounds array accesses in 99
submissions, and suggested appropriate corrections. In 265 distinct
submissions, our tool was able to identify one or more of the faults
described above, whereas CodeChef tagged them as correct! Thus,
our static technique has a qualitative advantage over the test-based
approach of online judges.

5.4 Performance

We ran our experiments on an Intel Xeon E5-1620 3.60 GHz ma-
chine with 8 cores and 24GB RAM. Out tool runs only on a single
core. On an average, our tool generated feedback in 1.6s includ-
ing the time for clustering and excluding the time for identifying
correct submissions manually.

5.5 Limitations and Threats to Validity

Our technique fails for submissions that have loop-carried de-
pendencies over scalar variables apart from the loop index vari-
ables, submissions that use auxiliary arrays and submissions for
which pattern matching fails to label statements. We inherit the lim-
itations of SMT solvers in reasoning about non-linear constraints
and program expressions with undeﬁned semantics, such as divi-
sion by 0. Most of the unhandled cases arise from these limitations.
Our approach cannot suggest feedback for errors in custom in-
put/output functions, output formatting, typecasting, etc. Our ap-
proach may provide spurious feedback enforcing stylistic confor-
mance with the instructor-validated submission. For example, if
a submission starts indexing into arrays from position 1 but the
instructor-validated submission indexes from position 0, our tool
generates feedback requiring the submission to follow 0 based in-
dexing. This may correct some misconception about array index-
ing that the student may have. Nevertheless, these differences can
be either compiled away during pre-processing or through SMT
solving with additional annotations. We will investigate these in
future. Finally, our implementation currently handles only a fre-
quently used subset of C constructs and library functions.

There can be faults in our implementation that might have af-
fected our results. To address this threat, we manually checked the
feature values and feedback obtained, and did not encounter any
error. Threats to external validity arise because our results may not
generalize to other problems and submissions. We mitigated this

threat by drawing upon submissions from more than 1860 students
on 4 different problems. While our technique is able to handle most
constructs that introductory DP coursework employs, further stud-
ies are required to validate our ﬁndings in the case of other prob-
lems. In Section 5.3, we compared our tool with the classiﬁcation
available on CodeChef. The tests used by CodeChef are not public
and hence, we cannot ascertain their quality. By improving the test
suites, some false negatives of CodeChef may disappear but black-
box testing will not be able to distinguish between logical faults
and formatting errors (discussed in Section 5.3).

6. RELATED WORK

Program Representations and Clustering.

In order to cluster submissions effectively, we need strategies to
represent both the syntax and semantics of programs. Many clus-
tering approaches use only edit distance between submissions [15,
44], while others use edit distance along with test-based similar-
ity [20, 36, 12]. We use neither of these. Glassman et al. [13]
advocate a hierarchical technique where the submissions are ﬁrst
clustered using high-level (abstract) features and then using low-
level (concrete) features. An interesting recent direction is to use
deep learning to compute and use vector representations of pro-
grams [40, 41, 33]. Peng et al. [40] propose a pre-training tech-
nique to automatically compute vector representations of different
AST nodes which is then fed to a tree-based convolution neural net-
work [33] for a classiﬁcation task. Piece et al. [41] propose a recur-
sive neural network to capture both the structure and functionality
of programs. The functionality is learned using input-output exam-
ples. But the class of programs considered in [41] is very simple.
It only handles programs which do not have any variables.

Since our experiments were focused on iterative DP solutions,
we designed features that capture the DP strategy. The above ap-
proaches are more general but unlike us, they may not put the sub-
missions in Figure 2 and 3 in the same cluster. Our algorithm ex-
tracts features in the presence of temporary variables and proce-
dures, and might be useful in other contexts as well.

Feedback Generation and Propagation.

The idea of comparing instructor provided solutions with student
submissions appears in [2]. It uses graph representation and trans-
formations for comparison of Fortran programs. Xu and Chee [49]
use richer graph representations for object-oriented programs. Rivers
and Koedinger [44] use edit distance as a metric to compare graphs
and generate feedback. Gross et al. [15] cluster student solutions
by structural similarity and perform syntactic comparisons with a
known correct solution to provide feedback. Feedback generated
by pattern matching may not always be correct.
In contrast, we
generate veriﬁed feedback but for the restricted domain of DP.

Alur et al. [4] develop a technique to automatically grade au-
tomata constructions using a pre-deﬁned set of corrections. Singh
et al. [46] apply sketching based synthesis to provide feedback for
introductory programming assignments. In addition to a reference
implementation, the tool takes as input an error model in the form
of correction rules. Their error model is too restrictive to be adapted
to our setting that requires more sophisticated repairs and that too
for a more challenging class of programs. Gulwani et al. [17] ad-
dress the orthogonal issue of providing feedback to address perfor-
mance issues, while Srikant and Aggarwal [47] use machine learn-
ing to assess coding quality of prospective employees and do not
provide feedback on incorrect solutions.

The idea of exploiting the common patterns in DP programs

has been used by Pu et al. [43] but for synthesis of DP programs.
The clustering-based approaches [20, 41] propagate the instructor-
provided feedback to all submissions in the same cluster, whereas
we generate personalized and veriﬁed feedback for each submis-
sion in a cluster separately. OverCode [12] also performs clustering
of submissions and provides a visualization technique to assist the
instructor in manually evaluating the submissions.

Program Repair and Equivalence Checking.

Genetic programming has been used to automatically generate
program repairs [5, 11, 27]. These approaches are not directly ap-
plicable in our setting as the search space of mutants is very large.
Further, GenProg [27] relies on redundancy present in other parts
of the code for ﬁxing faults. This condition is not met in our set-
ting. Software transplantation [18, 6] transfers functionality from
one program to another through genetic programming and slicing.
Prophet [30] learns a probabilistic, application independent model
of correct code from existing patches, and uses it to rank repair
candidates from a search space. These are generate-and-validate
approaches which rely on a test suite to validate the changes. In
comparison, we derive corrections for a faulty submission by pro-
gram equivalence checking with a correct submission.

Konighopher et. al. [25] present a repair technique using ref-
erence implementations. Their fault model is restrictive and only
considers faulty RHS. Many approaches rely on program speciﬁ-
cations for repair, including contracts [39, 48], LTL [23], asser-
tions [45] and pre-post conditions [14, 28, 19]. Recent approaches
that use tests to infer speciﬁcations and propose repairs include
SemFix [37], MintHint [24], DirectFix [31] and Angelix [32]. These
approaches use synthesis [22], symbolic execution [9] and partial
MaxSAT [10] respectively. Both DirectFix and Angelix use partial
MaxSAT but Angelix extracts more lightweight repair constraints
to achieve scalability. SPR [29] uses parameterized transformation
schemas to search over the space of program repairs. In contrast,
we use instructor-validated submissions and a combination of pat-
tern matching, static analysis and SMT solving.

Automated equivalence checking between a program and its op-
timized version has been studied in translation validation [42, 35,
7]. Partush and Yahav [38] design an abstract interpretation based
technique to check equivalence of a program and its patched ver-
sion.
In comparison, our technique performs equivalence check
between programs written by different individuals independently.

All these approaches are designed for developers and deal with
only one program at a time. Our tool targets iterative DP solutions
written by students and works on a large number of submissions
simultaneously. It combines clustering and veriﬁcation to handle
both the scale and variations in student submissions.

7. CONCLUSIONS AND FUTURE WORK

We presented semi-supervised veriﬁed feedback generation to
deal with both scale and variations in student submissions, while
minimizing the instructor’s efforts and ensuring feedback quality.
We also designed a novel counter-example guided feedback gener-
ation algorithm. We successfully demonstrated the effectiveness of
our technique on 2226 submissions to 4 DP problems.

Our results are encouraging and suggest that the combination
of clustering and veriﬁcation can pave way for practical feedback
generation tools. There are many possible directions to improve
clustering and veriﬁcation by designing sophisticated algorithms.
We plan to investigate these for more problem domains.

8. REFERENCES
[1] www.acm.org/public-policy/education-policy-committee.

[2] A. Adam and J.-P. Laurent. LAURA, a system to debug

student programs. Artiﬁcial Intelligence, 15(1-2):75 – 122,
1980.

[3] A. V. Aho, M. S. Lam, R. Sethi, and J. D. Ullman.

Compilers: Principles, Techniques, and Tools. Pearson, 2nd
edition, 2006.

[4] R. Alur, L. D’Antoni, S. Gulwani, D. Kini, and

M. Viswanathan. Automated grading of DFA constructions.
In IJCAI, pages 1976–1982, 2013.

[5] A. Arcuri. On the Automation of Fixing Software Bugs. In

ICSE Companion, pages 1003–1006, 2008.

[6] E. T. Barr, M. Harman, Y. Jia, A. Marginean, and J. Petke.

Automated software transplantation. In ISSTA, pages
257–269, 2015.

[7] C. Barrett, Y. Fang, B. Goldberg, Y. Hu, A. Pnueli, and
L. Zuck. TVOC: A translation validator for optimizing
compilers. In CAV, pages 291–295. Springer-Verlag, 2005.
[8] R. E. Bellman. Dynamic Programming. Dover Publications,

Incorporated, 2003.

[9] C. Cadar, D. Dunbar, and D. R. Engler. KLEE: Unassisted

and Automatic Generation of High-Coverage Tests for
Complex Systems Programs. In OSDI, pages 209–224, 2008.
[10] L. De Moura and N. Bjørner. Z3: An efﬁcient SMT solver. In

TACAS, pages 337–340, 2008.

[11] V. Debroy and W. E. Wong. Using Mutation to

Automatically Suggest Fixes for Faulty Programs. In ICST,
pages 65–74, 2010.

[12] E. L. Glassman, J. Scott, R. Singh, P. J. Guo, and R. C.

Miller. OverCode: Visualizing variation in student solutions
to programming problems at scale. ACM Trans.
Comput.-Hum. Interact., 22(2):7:1–7:35, Mar. 2015.
[13] E. L. Glassman, R. Singh, and R. C. Miller. Feature

engineering for clustering student solutions. In Proceedings
of the First ACM Conference on Learning @ Scale
Conference, L@S ’14, pages 171–172. ACM, 2014.

[14] D. Gopinath, Z. M. Malik, and S. Khurshid.

Speciﬁcation-based program repair using SAT. In TACAS,
pages 173–188, 2011.

[15] S. Gross, X. Zhu, B. Hammer, and N. Pinkwart. Cluster

based feedback provision strategies in intelligent tutoring
systems. In Intelligent Tutoring Systems, pages 699–700.
Springer, 2012.

[16] S. Gulwani and S. Juvekar. Bound analysis using backward

symbolic execution. Technical report, October 2009.

[17] S. Gulwani, I. Radiˇcek, and F. Zuleger. Feedback generation

for performance problems in introductory programming
assignments. In FSE, pages 41–51, 2014.

[18] M. Harman, W. B. Langdon, and W. Weimer. Genetic

programming for reverse engineering. In WCRE, pages 1–10,
2013.

[19] H. He and N. Gupta. Automated debugging using path-based

weakest preconditions. In FASE, pages 267–280, 2004.

[20] J. Huang, C. Piech, A. Nguyen, and L. J. Guibas. Syntactic
and functional variability of a million code submissions in a
machine learning MOOC. In AIED, 2013.

[21] P. Ihantola, T. Ahoniemi, V. Karavirta, and O. Seppälä.
Review of recent systems for automatic assessment of
programming assignments. In Proceedings of the 10th Koli
Calling International Conference on Computing Education
Research, Koli Calling ’10, pages 86–93, 2010.
[22] S. Jha, S. Gulwani, S. A. Seshia, and A. Tiwari.

Oracle-guided Component-based Program Synthesis. In
ICSE, pages 215–224, 2010.

[23] B. Jobstmann, A. Griesmayer, and R. Bloem. Program

Repair as a Game. In CAV, pages 287–294, 2005.

[24] S. Kaleeswaran, V. Tulsian, A. Kanade, and A. Orso.

MintHint: Automated synthesis of repair hints. In ICSE,
pages 266–276, 2014.

[25] R. Konighofer and R. Bloem. Automated Error Localization
and Correction for Imperative Programs. In FMCAD, pages
91–100, 2011.

[26] C. Lattner and V. Adve. LLVM: A Compilation Framework
for Lifelong Program Analysis & Transformation. In CGO,
Palo Alto, California, Mar 2004.

[27] C. Le Goues, T. N., S. Forrest, and W. Weimer. Genprog: A

Generic Method for Automatic Software Repair. IEEE Trans.
on Software Engineering, pages 54 –72, 2012.

[28] F. Logozzo and T. Ball. Modular and Veriﬁed Automatic

Program Repair. In OOPSLA, pages 133–146, 2012.
[29] F. Long and M. Rinard. Staged program repair with

condition synthesis. In ESEC/FSE, 2015.

[30] F. Long and M. Rinard. Automatic patch generation by
learning correct code. In POPL, pages 298–312, 2016.

[31] S. Mechtaev, J. Yi, and A. Roychoudhury. DirectFix:
Looking for simple program repairs. In ICSE, 2015.

[32] S. Mechtaev, J. Yi, and A. Roychoudhury. Angelix: Scalable

Multiline Program Patch Synthesis via Symbolic Analysis.
In ICSE, 2016.

[33] L. Mou, G. Li, L. Zhang, T. Wang, and Z. Jin. Convolutional

neural networks over tree structures for programming
language processing. In AAAI, 2016.

[34] I. Narasamdya and A. Voronkov. Finding basic block and

variable correspondence. In Static Analysis, pages 251–267.
Springer, 2005.

[35] G. C. Necula. Translation validation for an optimizing

compiler. In PLDI, pages 83–94. ACM, 2000.

[36] A. Nguyen, C. Piech, J. Huang, and L. Guibas. Codewebs:

Scalable homework search for massive open online
programming courses. In WWW, pages 491–502, 2014.

[37] H. D. Nguyen, D. Qi, A. Roychoudhury, and S. Chandra.
SemFix: Program Repair via Semantic Analysis. In ICSE,
2013.

[38] N. Partush and E. Yahav. Abstract semantic differencing via
speculative correlation. In OOPSLA, pages 811–828. ACM,
2014.

[39] Y. Pei, Y. Wei, C. A. Furia, M. Nordio, and B. Meyer.

Code-based Automated Program Fixing. In ASE, pages
392–395, 2011.

[40] H. Peng, L. Mou, G. Li, Y. Liu, L. Zhang, and Z. Jin.

Building program vector representations for deep learning.
In Knowledge Science, Engineering and Management, pages
547–553. Springer, 2015.

[41] C. Piech, J. Huang, A. Nguyen, M. Phulsuksombati,

M. Sahami, and L. J. Guibas. Learning program embeddings
to propagate feedback on student code. In ICML, pages
1093–1102, 2015.

[42] A. Pnueli, M. Siegel, and E. Singerman. Translation

validation. In TACAS, pages 151–166. Springer-Verlag, 1998.

[43] Y. Pu, R. Bodik, and S. Srivastava. Synthesis of ﬁrst-order

dynamic programming algorithms. In OOPSLA, pages
83–98, 2011.

[44] K. Rivers and K. Koedinger. Automatic generation of

programming feedback: A data-driven approach. In AIED,
pages 4:50–4:59, 2013.

[45] R. Samanta, O. Olivo, and E. Emerson. Cost-aware

automatic program repair. In Static Analysis, Lecture Notes
in Computer Science. 2014.

[46] R. Singh, S. Gulwani, and A. Solar-Lezama. Automated

feedback generation for introductory programming
assignments. In PLDI, pages 15–26, 2013.

[47] S. Srikant and V. Aggarwal. A system to grade computer

programming skills using machine learning. In KDD, pages
1887–1896, 2014.

[48] Y. Wei, Y. Pei, C. A. Furia, L. S. Silva, S. Buchholz,

B. Meyer, and A. Zeller. Automated Fixing of Programs with
Contracts. In ISSTA, pages 61–72, 2010.

[49] S. Xu and Y. S. Chee. Transformation-based diagnosis of

student programs for programming tutoring systems.
Software Engineering, IEEE Transactions on,
29(4):360–384, April 2003.

