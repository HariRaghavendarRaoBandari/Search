6
1
0
2

 
r
a

 

M
7
1

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
4
9
6
5
0

.

3
0
6
1
:
v
i
X
r
a

Semiparametric two-component mixture models under linear

constraints

Diaa AL MOHAMAD∗ and Assia BOUMAHDAF†

Laboratoire de Statistique Th´eorique et Appliqu´ee, Universit´e Pierre et Marie Curie,France

March 21, 2016

Abstract

Estimation of a two-component mixture model with an unknown component is very dif-
ﬁcult when no particular assumption is made on the structure of the unknown component.
A symmetry assumption was used in the literature to simplify the estimation. Such method
has the advantage of being consistent and asymptotically normal, and identiﬁability becomes
tractable. Still, the method has its limits when the two components of the mixture can hardly
be distinguished or the proportion of the parametric part is high. Moreover, it is not applica-
ble on positive-support distributions or in multivariate situations. We propose in this paper
a method to incorporate a prior linear information about the distribution of the unknown
component in order to better estimate the model when existing estimation methods fail. The
new method is based on ϕ−divergences and has an original form since the minimization is
carried over both arguments of the divergence. The new method is proved to be consistent
and asymptotically normal under standard assumptions. It has a linear complexity resulting
in a fast implementation in the case of the Pearson’s χ2. Simulations on univariate and
multivariate mixtures demonstrate the viability and the interest of our novel approach.
linear constraint; Fenchel Duality; ϕ−divergence;
Keywords: Semiparametric mixture;
signed measures.

Introduction

A two-component mixture model with an unknown component is deﬁned by:

f (x) = λf1(x|θ) + (1 − λ)f0(x),

for x ∈ Rs

(0.1)
for λ ∈ (0, 1) and θ ∈ Rd to be estimated as the density f0 is unknown. Such a model appears
in the study of gene expression data coming from microarray analysis. An application to two
bovine gestation mode comparison is performed in Bordes et al. [2006]. The authors suppose
that θ is known, f0 is symmetric around an unknown µ and that r = 1. Xiang et al. [2014]
studied a more general setup by considering θ unknown and applied model (0.1) on the Iris data
by considering only the ﬁrst principle component for each observed vector. Another application
of model (0.1) in genetics can be found in Ma et al. [2011].
Robin et al. [2007] used the semiparametric model (supposing that θ is known) in multiple test-
ing procedures in order to estimate the posterior population probabilities and the local false
rate discovery. Song et al. [2010] studied a similar setup where θ is unknown without further
assumptions on f0. They applied the semiparametric model in sequential clustering algorithms
as a second step after having calculated the centers of the clusters. The model of the current
cluster is f1 (Gaussian for example) with known location and unknown scale, whereas the dis-
tribution of the remaining clusters is represented by f0 and supposed to be unknown. Finally,

∗Corresponding author. E-mail: diaa.almohamad@gmail.com
†E-mail: assia.boumahdaf@gmail.com

1

Model (0.1) can also be regarded as a contamination model, see Titterington et al. [1985] or
McLachlan and Peel [2005] for further applications.

Estimation methods used in the aforementioned papers can be divided into two categories;
direct and iterative procedures. We mean by direct here that the estimator is obtained using an
optimization tool one time without the need to repeat the optimization. The method proposed
in Bordes et al. [2006] and its improved version by Bordes and Vandekerkhove [2010] are direct
methods and assume that the unknown density is symmetric around an unknown value µ and
that the parametric component is fully known, i.e. θ is known. The idea behind their procedure
is to calculate f0 as a function of other terms in equation (0.1), then use the symmetry of f0 to
write F0(x) = 1− F0(−x), where F0 is the cumulative distribution function of f0. We then use a
suitable distance to compare between F0(x) and 1−F0(−x). The method as it is cannot be used
in multivariate cases. Besides, in univariate ones it is not possible to use such method when
any of the two components of the mixture has a nonnegative support. In comparison to other
proposed methods, this one gives the best performance when the proportion of the parametric
component λ is low. It is worth to mention that the method was proved to be consistent and
asymptotically normal under mild conditions. Another direct method was proposed by Song
et al. [2010] and was called as π−maximizing procedure. The procedure is based on the identiﬁ-
ability of model (0.1) when f1 is a scale Gaussian model. Asymptotic properties of this method
were not studied and theoretical justiﬁcation is only done in the Gaussian case and it is not
evident how to generalize it. The method is still adaptable to multivariate situations.
On the other hand, there are several iterative procedures dedicated to the estimation of model
(0.1). All these methods are EM-type procedures except for the method of Xiang et al. [2014].
The later is a Hellinger-based two-step directional optimization procedure; a ﬁrst step mini-
mizes the divergence over f0 and the second step minimizes over the parameters (λ, θ). Their
method seems to give good results, but the algorithm is very complicated and no explanation
on how to do the calculus is given. Properties of the iterative procedure are not studied either.
EM-type methods are more intuitive, but have lower performance when the two components of
the mixture are close enough. Besides, it is diﬃcult to prove their convergence and study their
asymptotic behavior especially that they are not really based on the optimization of an objec-
tive function similarly to the EM algorithm. They are only based on the calculus of a vector of
weights (w1,··· , wn) which plays the same role as in the EM algorithm. The diﬀerence appears
in its calculus method, see Robin et al. [2007],Ma et al. [2011], Song et al. [2010] and Bordes
et al. [2007].

All above methods were proved or illustrated to work but only in speciﬁc situations and the
only simulated example were a Gaussian mixture. The paper of Xiang et al. [2014] provides
a comparison of the method of Bordes and Vandekerkhove [2010], the π−maximizing and the
EM-type algorithm of Song et al. [2010], and ﬁnally their Hellinger-based algorithm. No method
performs uniformly good on all simulated mixtures. They still give satisfactory results in most
simulations. Still, these simulations consider θ to be given. As we add θ to the set of unknown
parameters, things become diﬀerent. The method of Bordes and Vandekerkhove [2010] does
not perform well unless the proportion of the unknow component 1 − λ is high enough. This
is not surprising since this method is based on the properties of the unknown component;
hence it should be well estimated. On the contrary, other methods (EM-type methods and
the π−maximizing) performs well when the proportion of the parametric component λ is high
enough.
It is important for the method to be used in other applications that it can take into account
possible unknown parameters in the parametric component f1. The failure of the existing
methods comes from the degree of diﬃculty of the semiparametric model. The use of a symmetric
assumption made the estimation better in some contexts, but such assumption is still restrictive.
We need to incorporate other prior information about f0 in a way that we stay in between a

2

fully parametric settings and a fully semiparametric one. We thus propose a method which
permits to add relatively general priori information. Such information needs to apply linearly
on the distribution function of the unknown component such as moment-type information. For
example, we may have an information relating the ﬁrst and the second moments of f0 such as

(cid:82) xf0(x) = α and(cid:82) x2f0(x)dx = m(α), see Broniatowski and Keziou [2012] and the references

therein. Such information adds some structure to the model without precising the value of the
moments. More examples will be discussed later on.
Unfortunately, the incorporation of linear constraints on the distribution function cannot be done
directly in existing methods because the optimization will be carried over a (possibly) inﬁnite
dimensional space, and we need a new tool. Convex analysis oﬀers a way using Fenchel-Legendre
duality to transform an optimization problem over an inﬁnite dimensional space to the space of
Lagrangian parameters (ﬁnite dimensional one). φ−divergences oﬀer a way by their convexity
properties to use the duality of Fenchel-Legendre. The paper of Broniatowski and Keziou [2012]
gives a complete study of this problem in the non mixture case, see also Decurninge [2015] Chap.
1 and Keziou [2003] Chap. 3. We will exploit such results to build a new estimation procedure
which takes into account linear information over the unknown component’s distribution.
The paper is organized as follows. Section 1 presents the general context of semiparametric
models under linear constraints. Section 2 presents ϕ−divergences and some of their general
properties. Section 3 introduces the semiparametric mixture model (0.1) under linear constraints
and the methodology we adapt to estimate the model through ϕ−divergences. We also present
the duality technique and how we can apply it in our context. We end this section with a
plug-in estimator which permits to estimate the parameters of the model as soons as we have
an sample from the whole mixture. Section (4) shows that the new estimator is consistent
and asymptotically normal under standard assumptions. Finally, Section (5) is devoted to the
demonstration of the method on several mixture models in univariate and multivarite contexts
and a comparison with existing methods which permits to show how the prior information can
improve the estimation.

1 Models deﬁned through linear constraints on the distribution

function

Let us consider n i.i.d. copies, X1, . . . , Xn of random variables taking values in Rr drawn from
the unknown distribution PT . Semiparametric models are often deﬁned through a ﬁnite number

of linear constraints of the form(cid:90)

g(x)dP (x) = EP [g(X)] = m(α),

(1.1)
where the unknown parameter α belongs to A ⊂ Rs, g : Rr → R(cid:96) is a speciﬁed vector-valued
function and m : A → R(cid:96) is also a speciﬁed vector-valued function, see Broniatowski and Keziou
[2012], Decurninge [2013] (with dP replaced by dP −1 the quantile measure), Owen [1990] (in
the empirical likelihood context) and Jiahua Chen [1993] (for ﬁnite population problems). It is
also possible to make g depend on the parameter vector α, but we stay for the sake of simplicity
with the assumption that g does not depend on α. The theoretical approach we present in this
paper remains valid with slight modiﬁcation on the assumptions and more technicalities at the
level of the proofs.
Denote by M + the set of all probability measures (p.m.) deﬁned on the same measurable space
as PT , i.e. (Rr, B(Rr)). If the true distribution PT veriﬁes the set of (cid:96) constraints (1.1) for some
α∗, then the set

(cid:90)

(cid:27)

g(x)dQ(x) = m(α∗)

(1.2)

(cid:26)

M+

α∗(PT ) =

Q ∈ M + such that Q (cid:28) PT ,

3

constitutes a ”neighborhood” of probability measures of PT . Generally, one would rather con-
sider the larger ”neighborhood” deﬁned by

(cid:91)

α∈A

M =

Mα,

because the value of α∗ is unknown and needs to be estimated. The estimation procedure is
generally done by either solving the set of equations (1.1) deﬁning the constraints or by mini-
mizing a suitable distance-like function between the set M and some estimator of PT based on
an observed sample. In other words, we search for the ”projection” of PT on M.
Solving the set of equations (1.1) is in general a diﬃcult task since it is a set of non linear equa-
tions. We therefore prefer to use distance-like functions and estimate the parameter vector α∗
by minimizing some distance between the set M and an estimator of PT . We use ϕ−divergences
because they oﬀer a strong tool, as we will see later, which permits to simplify the task of
minimizing the distance between the set M and a probability measure. The next paragraph
introduces these divergences and presents a notion of a distance between a set M and some
ﬁnite measure.

Example 1.1. A simple and a standard example is a model deﬁned through moment constraints.
Let PT be the Weibull distribution with scale a∗ and shape b∗. We deﬁne Mα with α = (a, b) ∈
(0,∞)2 to be the set of all probability measures whose ﬁrst three moments are given by:

(cid:90)

xidP (x) = aiΓ(1 + i/b),

i = 1, 2, 3.

This set is a ”neighborhood” of probability measures of the Weibull distribution; it contains all
probability measures absolutely continuous with respect to the Weibull mixture PT and which
share the ﬁrst three moments with it.

2 ϕ-divergences: Deﬁnitions and properties

2.1 Deﬁnitions and properties

ϕ-divergences were introduced independently by Csisz´ar [1963] (as ”f -divergences”) and Ali and
Silvey [1966]. Let P and Q be two ﬁnite signed measures deﬁned on (Rr, B(Rr)) such that Q is
absolutely continuous (a.c.) with respect to (w.r.t.) P . Let ϕ : R (cid:55)→ [0, +∞] be a proper convex
function with ϕ(1) = 0 and such that its domain domϕ = {x ∈ R such that ϕ(x) < ∞} :=
(aϕ, bϕ) with aϕ < 1 < bϕ. The ϕ-divergence between Q and P is deﬁned by:

(cid:90)

(cid:19)

(cid:18) dQ

dP

Dϕ(Q, P ) =

ϕ

Rr

(x)

dP (x),

dQ
dP

is the Radon-Nikodym derivative. When Q is not a.c.w.t. P , we set Dϕ(Q, P ) = +∞.
where
When, P = Q then Dϕ(Q, P ) = 0. Furthermore, if the function x (cid:55)→ ϕ(x) is strictly convex on
a neighborhood of x = 1, then

Dϕ(Q, P ) = 0 if and only if P = Q.

(2.1)
Several standard statistical divergences can be expressed as ϕ−divergences; the Hellinger, the
Pearson’s and the Neymann’s χ2, and the (modiﬁed) Kullback-Leibler. They all belong to the
class of Cressie-Read (also known as ”power divergences”) deﬁned by:

ϕγ(x) :=

xγ − γx + γ − 1

γ(γ − 1)

,

(2.2)

4

Dϕ (M, P ) := inf

Q∈M Dϕ(Q, P ).

(2.3)

2 , 2,−2, 0, 1 respectively1. More details and properties can be found in Liese and Vajda

for γ = 1
[1987] or Pardo [2006].
Estimators based on such a tool were developed in the parametric (see Beran [1977],Lindsay
[1994],Park and Basu [2004],Broniatowski and Keziou [2009]) and the semiparametric setups (see
Broniatowski and Keziou [2012] and Decurninge [2013]). In all these methods, the ϕ−divergence
is calculated between a model Q and a true distribution PT . We search our estimators by making
the model approaches2 the true distribution PT . In this paper, we provide an original method
where the minimization is done over both arguments of the divergence in a way that the two
arguments approach one another for the sake of ﬁnding a suitable estimator3.
In completly
nonparametric setup, we may mention the work of Karunamuni and Wu [2009] on two component
mixture models when both components are unknown. The authors use the Hellinger divergence,
and assume that we have in hand a sample from each component and a sample drawn from the
whole mixture. For regression of nonparametric mixture models using the Hellinger divergence,
see the paper of Tang and Karunamuni [2013].
The following deﬁnitions concern the notion of ϕ-projection of ﬁnite signed measures over a set
of ﬁnite signed measures.
Deﬁnition 2.1. Let M be some subset of M , the space of ﬁnite signed measures. The ϕ-
divergence between the set M and some ﬁnite signed measure P , noted as Dϕ(M, P ), is given
by

Furthermore, we deﬁne the ϕ−divergence between two subsets of M , say M and N by:

Dϕ (M,N ) := inf

Q∈M inf

P∈N Dϕ(Q, P ).

Deﬁnition 2.2. Assume that Dϕ(M, P ) is ﬁnite. A measure Q∗ ∈ M such that

Dϕ(Q∗, P ) ≤ Dϕ(Q, P ),

for all Q ∈ M

is called a ϕ-projection of P onto M. This projection may not exist, or may not be deﬁned
uniquely.

The essential tool we need from a ϕ−divergence is its characterization of the projection of
some ﬁnite signed measure P onto a set M of ﬁnite signed measures. Such characterization will
permit to transform the search of a projection in an inﬁnite dimensional space to the search of
a vector ξ in R(cid:96)+1.
2.2 The duality technique of ϕ−divergences
In this section, we recall the Fenchel duality technique in order to give a practicable repre-
sentation of the quantity Dϕ(M, P ) when M is deﬁned through (cid:96) + 1 linear constraints in a
similar way to (1.2). The duality is a standard tool in optimization theory that transforms
a constrained problem (called the primal ) into an unconstrained one (the dual ). Let ϕ be a
strictly convex function which veriﬁes the same properties as in the previous paragraph 2.1.
The Fenchel-Legendre transform of ϕ, say ψ is deﬁned by:

ψ(t) = sup
x∈R

{tx − ϕ(x)} ,

∀t ∈ R.

We are concerned with the convex optimization problem
1For γ ∈ {0, 1}, the limit is calculated since it is not well-deﬁned.
2More accurately, we project the true distribution on the model.
3There still exists some work in computer vision using ϕ−divergences where the minimization is done over
both arguments of the divergence, see El Gheche [2014]. The work concerns a parametric setup in discrete models.

5

gi(x)dQ(x) = mi(α), i = 1, . . . , (cid:96)

.

(2.5)

(cid:90)
(cid:90)

ψ(cid:0)ξtg(x)(cid:1) dP0(x).

(cid:27)

(2.6)

(2.7)

(P)

(2.4)
where P0 is some ﬁnite singed measure and Mα is the subspace deﬁning the linear constraints.
Mα here will be a larger subset than the one considered in Section 1 since we are considering
all ﬁnite signed measures and not only positive ones. The set Mα will be deﬁned as follows:

Dϕ(Q, P0),

inf
Q∈Mα

(cid:26)

Mα =

Q ∈ M s.t. Q (cid:28) P0, Q(X ) = 1 and
We associate to (P) the following dual problem

(P∗)

ξtm(α) −

sup
ξ∈R(cid:96)+1

that (cid:82) |gi(x)||dP0|(x) < ∞ for all i = 1, . . . , (cid:96) + 1 and there exists some measure Q0 a.c.w.r.t.

We assume that ϕ is a proper closed convex function and diﬀerentiable. . Assume furthermore
P0 such that Dϕ(Q0, P0) < ∞. We consider the problem of strong duality which state that
(P) = (P∗). According to Proposition 1.4 in Decurninge [2015] (see also Proposition 4.2 in
Broniatowski and Keziou [2012]) we have:

inf
Q∈Mα

Dϕ (Q, P0) = sup
ξ∈R(cid:96)+1

ξtm(α) −

(cid:90)

ψ(cid:0)ξtg(x)(cid:1) dP0(x).

Thus, in the context of models deﬁned through linear constraints (Section 1), if we are looking
for the value of α for which the ϕ−projection of P0 on Mα has a minimum distance, then we
can calculate such α by the following criterion:

α∗ = arg inf

α

Dϕ(Mα, P0)

= arg inf

α

sup
ξ∈R(cid:96)+1

ξtm(α) −

(cid:90)

ψ(cid:0)ξtg(x)(cid:1) dP0(x).

This is indeed a feasible procedure since we only need to optimize a real function. The real chal-
lenge is of course how to implement such optimization eﬃciently and how to study asymptotic
properties of plug-in estimates. Examples of such procedures can be found in Broniatowski and
Keziou [2012], Decurninge [2013], Newey and Smith [2004] and the references therein.

Now that all notions and analytical tools are presented, we proceed to the objective of
this paper; semiparametric mixtures models. The following section deﬁnes such models and
presents our proposed method to estimate them using ϕ−divergences. It follows then the plug-
in estimates and their asymptotic properties.

3 Semiparametric mixture models under linear constraints

3.1 Deﬁnition and estimating methodology
Deﬁnition 3.1. Let X be a random variable taking values in Rr distributed from a probability
measure P . We say that P (.|φ) with φ = (λ, θ, α) is a two-component semiparametric mixture
model subject to linear constraints if it can be written as follows:

P (.|φ) = λP1(.|θ) + (1 − λ)P0

s.t.

P0 ∈ Mα =

Q ∈ M s.t.

dQ(x) = 1,

Rr

(cid:90)

Rr

(cid:26)

(cid:90)

gi(x)dQ(x) = mi(α),

i ∈ {1,··· , (cid:96)}

(3.1)

(cid:27)

for λ ∈ (0, 1) the proportion of the parametric component, θ ∈ Θ ⊂ Rd a set of parameters
deﬁning the parametric component, α ∈ A ⊂ Rs is the constraints parameter and ﬁnally m(α) =
(m1(α),··· , m(cid:96)(α)) is a vector-valued function determining the value of the constraints.

6

We have seen in the previous paragraph that it is possible to use ϕ−divergences to estimate
such models as long as the constraints apply over P (.|φ), i.e. the whole mixture. In our case,
the constraints apply only on a component of the mixture. It is thus reasonable to consider a
”model” expressed through P0 instead of P . We have:
P (.|φ) − λ
1 − λ

P1(.|θ).

1 − λ

P0 =

1

Denote PT = P (.|φ∗) with φ∗ = (λ∗, θ∗, α∗) to the distribution which generates the observed
data. Denote also P ∗
0 to the true semiparametric component of the mixture PT . The only
0 is that it belongs to a set Mα∗ for some (possibly unknown)
information we hold about P ∗
α∗ ∈ A. Besides, it veriﬁes:

P ∗
0 =

1

1 − λ∗ PT − λ∗

1 − λ∗ P1(.|θ∗).

(3.2)
We would like to retrieve the value of the vector φ∗ = (λ∗, θ∗, α∗) provided a sample X1,··· , Xn
drawn from PT and that P ∗

0 ∈ ∪Mα. Let’s start by considering the set of signed measures:

N =

Q =

1

1 − λ

PT − λ
1 − λ

P1(.|θ),

λ ∈ (0, 1), θ ∈ Θ

.

(3.3)

Notice that P ∗
for simplicity, to belong to the union ∪α∈AMα. We may now write,

0 belongs to this set for λ = λ∗ and θ = θ∗. On the other hand, P ∗

0 is supposed,

P ∗

If we suppose now that the intersection N(cid:84)∪α∈AMα contains only one element which would

be a fortiori P ∗
0 , then it is very reasonable to consider an estimation procedure by calculating
some ”distance” between the two sets N and ∪α∈AMα. Such distance can be measured using
a ϕ−divergence by (see Deﬁnition 2.1):

0 ∈ N(cid:92)∪α∈AMα.

(cid:26)

(cid:27)

Dϕ(M,N ) = inf

Q2∈N inf

Q1∈M Dϕ(Q1, Q2).

We may reparametrize this distance using the deﬁnition of N . We have :

Dϕ(∪αMα,N ) = inf
Q∈N

inf

P0∈∪αMα

Dϕ(P0, Q)

= inf
λ,θ

inf

α,P0∈Mα

Dϕ

P0,

1

1 − λ

P (.|φ∗) − λ
1 − λ

P1(.|θ)

If we still have P ∗
argument of the inﬁmum in (3.5) is non other than (λ∗, θ∗, α∗), i.e.

0 as the only signed measure which belongs to both N and ∪αMα. Thus, the

(λ∗, θ∗, α∗) = inf

λ,θ

inf

α,P0∈Mα

Dϕ

P0,

1

1 − λ

P (.|φ∗) − λ
1 − λ

P1(.|θ)

.

(3.6)

3.2 Uniqueness of the solution under the model

1

By a unique solution we mean that only one measure, which can be written in the form of

1−λ P1(.|θ), veriﬁes the constraints with a unique triplet (λ∗, θ∗, α∗). The existence of
1−λ PT − λ
a unique solution is essential in order to ensure that the procedure is a reasonable estimation
0 ∈ Mα∗. The idea
method. By under the model we mean that there exists α∗ ∈ A such that P ∗
is based on the identiﬁcation of the intersection of the set N ∩ M.
Proposition 3.1. Assume that P ∗

0 ∈ M = ∪αMα. Suppose also that:

7

(cid:18)

(cid:18)

(3.4)

(3.5)

(cid:19)

.

(cid:19)

1. the system of equations:

(cid:90)

gi(x) (dP (x|φ∗) − λdP1(x|θ)) = (1 − λ)mi(α),

i = 1,··· , (cid:96)

(3.7)

has a unique solution (λ∗, θ∗, α∗);
2. function α (cid:55)→ m(α) is one-to-one;
3. there exists x0 ∈ Rr (with possible inﬁnite norm) such that for any θ ∈ Θ we have :

dP1(x|θ)
dPT (x)

lim
x→x0

= c,

with c ∈ [0,∞) \ {1};

4. the parametric component is identiﬁable, i.e. if P1(.|θ) = P1(.|θ(cid:48)) dPT−a.e. then θ = θ(cid:48),
then, there exists a unique vector (λ∗, θ∗, α∗) such that PT = λ∗P1(.|θ∗) + (1− λ)P ∗
0 where P ∗
0 is
given by (3.2) and belongs to Mα∗. Moreover, provided assumptions 2-4, the conclusion holds
if and only if assumption 1 is fulﬁlled.
Proof. Let P0 be some signed measure which belongs to the intersection N ∩ M. Since P0
belongs to N , there exists a couple (λ, θ) such that:
PT − λ
1 − λ

P1(.|θ).

1 − λ

P0 =

(3.8)

1

This couple is unique by virtue of assumptions 3 and 4. Indeed, let (λ, θ) and (˜λ, ˜θ) bet two
couples such that:

1

1 − λ

PT − λ
1 − λ

P1(.|θ) =

1

1 − ˜λ

PT − ˜λ
1 − ˜λ

P1(.|˜θ) dPT − a.e.

(3.9)

1

This entails that:

1 − λ

− λ
1 − λ

dP1(x|θ)
dPT (x)
Taking the limit as x tends to x0 results in:
1 − cλ
1 − λ

=

1

1 − ˜λ

− ˜λ
1 − ˜λ

dP1(x|˜θ)
dPT (x)

.

˜1 − cλ
1 − ˜λ

.

=

Note that function z (cid:55)→ (1 − cz)/(1 − z) is strictly monotone as long as c (cid:54)= 1. Hence, it is a
one-to-one map. Thus λ = ˜λ. Inserting this result in equation (3.9) entails that:

P1(.|θ) = P1(.|˜θ)

dPT − a.e.

Using the identiﬁability of P1 (assumption 4), we get θ = ˜θ which proves the existence of a
unique couple (λ, θ) in (3.8).
On the other hand, since P0 belongs to M, there exists a unique α such that P0 ∈ Mα.
Uniqueness comes from the fact that function α (cid:55)→ m(α) is one-to-one (assumption 2). Thus,
P0 veriﬁes the constraints

(cid:90)

dP0(x) = 1,

gi(x)dP0(x) = mi(α),

∀i = 1,··· , (cid:96).

(cid:90)
(cid:19)

(cid:90) (cid:18) 1

Combining these two facts, we get:

1 − λ

dPT − λ
1 − λ

dP1(x|θ)

= 1,

(cid:19)

dP1(x|θ)

dPT − λ
1 − λ

= mi(α),

(3.10)

(cid:18) 1

1 − λ

(cid:90)

gi(x)

8

for all i = 1,··· , (cid:96). This is a non linear system of equations with (cid:96) + 1 equations. The ﬁrst
one is veriﬁed for any couple (λ, θ) since both dP (.|φ∗) and dP1 are probability measures. This
reduces the system to (cid:96) non linear equations. We may now conclude that, if a signed measure P0
belongs to the intersection N ∩M, then it has the representation (3.8) for a unique couple (λ, θ)
and there exists a unique α such that the triplet (λ, θ, α) is a solution to the non linear system
(3.10). Conversely, if there exists a triplet (λ, θ, α) which solves the non linear system (3.10),
1−λ P1(.|θ) belongs to the intersection
then the signed measure P0 deﬁned by P0 = 1
N ∩ M. This is because on the one hand, it clearly belongs to N by its deﬁnition and on the
other hand, it belongs to Mα since it veriﬁes the constraints and thus belongs to M.
It is now reasonable to conclude that the intersection N ∩ M includes a unique signed measure
P0 if and only if the set of (cid:96) non linear equations (3.10) has a unique solution (λ, θ, α).

1−λ P (.|φ∗)− λ

There is no general result for a non linear system of equations to have a unique solution;
still, it is necessary to ensure that (cid:96) ≥ d + s + 1, otherwise there would be an inﬁnite number of

signed measures in the intersection N(cid:84)∪α∈AMα.

(cid:16) 1
1−λ PT − λ

(cid:17)
1−λ P1(.|θ)

λ,θ

Remark 3.1. Assumptions 3 and 4 of Proposition 3.1 are used to prove the identiﬁability of the
. Thus, one is not restricted with such assumption and may
”model”

ﬁnd simpler ones for particular cases (or even for the general case). Our assumptions remain
suﬃcient but not necessary for the proof.

Example 3.1. One of the most popular models in clustering is the Gaussian multivariate
mixture (GMM). Suppose that we have two classes. Linear discriminant analysis (LDA) is
based on the hypothesis that the covariance matrix of the two classes is the same. Let X be
a random variable which takes its values in R2 and is drawn from a mixture model of two
components. In the context of LDA, the model has the form:

f (x, y|λ, µ1, µ2, Σ) = λf1(x, y|µ1, Σ) + (1 − λ)f1(x, y|µ2, Σ),

with:

f1(x, y|µ1, Σ) =

2π(cid:112)|det(Σ)| exp

1

(cid:20)

− 1
2

((x, y)t − µ1)tΣ((x, y)t − µ1)

(cid:21)

, Σ =

(cid:18) σ2

ρ

(cid:19)

.

ρ
σ2

We would like to relax the assumption over the second component by keeping the fact that
the covariance matrix is the same as the one of the ﬁrst component. We will start at ﬁrst by
imposing the very natural constraints on the second component.

(cid:90)
(cid:90)
(cid:90)
(cid:90)
(cid:90)

xf0(x, y)dxdy = µ2,1,

yf0(x, y)dxdy = µ2,2,

x2f0(x, y)dxdy = σ2,

y2f0(x, y)dxdy = σ2,

xyf0(x, y)dxdy = ρ + µ2,1µ2,2.

These constraint concerns only the fact that the covariance matrix Σ is the same as the one of
the Gaussian component (the parameteric one). In order to see whether this set of constraints
is suﬃcient for the existence of a unique measure in the intersection N ∩ M, we need to write

9

the set of equations corresponding to (3.7) in Proposition 3.1.

(cid:90)
(cid:90)
(cid:90)
(cid:90)
(cid:90)

x

y

x2

y2

xy

1 − λ

1 − λ

(cid:20) 1
(cid:20) 1
(cid:20) 1
(cid:20) 1
(cid:20) 1

1 − λ

1 − λ

1 − λ

(cid:21)
(cid:21)
(cid:21)
(cid:21)
(cid:21)

f (x, y) − λ
1 − λ
f (x, y) − λ
1 − λ
f (x, y) − λ
1 − λ
f (x, y) − λ
1 − λ
f (x, y) − λ
1 − λ

f1(x, y|µ1, σ, ρ)

f1(x, y|µ1, σ, ρ)

f1(x, y|µ1, σ, ρ)

f1(x, y|µ1, σ, ρ)

f1(x, y|µ1, σ, ρ)

dxdy = µ2,1,

dxdy = µ2,2,

dxdy = σ2,

dxdy = σ2,

dxdy = ρ + µ2,1µ2,2,

The number of parameters is 7, and we only have 5 equations. In order for the problem to have
a unique solution, it is necessary to either add two other constraints or to consider for example
µ1 = (µ1,1, µ1,2) to be known4. Other solutions exist, but depend on the prior information. We
may imagine an assumption of the form µ1,1 = aµ1,2 and µ2,1 = bµ2,2 for given constants a and
b.
The gain from relaxing the normality assumption on the second component is that we are
building a model which is not constrained to a Gaussian form for the second component, but
rather to a form which suits the data. Indeed, we are placed between a parametric Gaussian
hypothesis on the second component and a total nonparametric assumption over the second
component.

3.3 Estimation procedure and plug-in estimates

The Fenchel-Legendre duality permits to transform the problem of minimizing under linear
constraints in a possibly inﬁnite dimensional space into a simply unconstrained optimization
problem in the space of Lagrangian parameters over Rl, where l is the number of constraints.
We will apply the duality result presenter earlier in paragraph 2.2 on the inner optimization in
equation (3.5). We have:

(cid:18)

(cid:19)

inf
Q∈Mα

Dϕ

Q,

1

λ − 1

PT − λ
1 − λ

P1(.|θ)

= sup
ξ∈Rl+1

ξtm(α) − 1

1 − λ

(cid:90)
(cid:90)

ψ(cid:0)ξtg(x)(cid:1) (dPT (x) − λdP1(x|θ))
ψ(cid:0)ξtg(x)(cid:1) dPT (x)

= sup
ξ∈Rl+1
λ
1 − λ

+

(cid:90)

ξtm(α) − 1

1 − λ

ψ(cid:0)ξtg(x)(cid:1) dP1(x|θ),

(cid:19)
ψ(cid:0)ξtg(x)(cid:1) dPT (x)

P1(.|θ)

PT − λ
1 − λ

(cid:18)

inf
Q∈Mα

sup
ξ∈Rl+1

(cid:90)

1

Q,

Dϕ

λ − 1
ξtm(α) − 1

(cid:90)
ψ(cid:0)ξtg(x)(cid:1) dP1(x|θ).

1 − λ

Inserting this result in (3.6) gives that:

(λ∗, θ∗, α∗) = arg inf

φ

= arg inf

φ

+

λ
1 − λ

The right hand side can be estimated on the basis of an n−sample drawn from PT , say
X1,··· , Xn, by a simple plug-in of the empirical measure Pn. The resulting procedure can

4or estimated by another procedure such as k−means.

10

now be written as:

(ˆλ, ˆθ, ˆα) = arg inf
λ,θ,α

sup
ξ∈Rl+1

n(cid:88)

ψ(cid:0)ξtg(Xi)(cid:1)
ψ(cid:0)ξtg(x)(cid:1) dP1(x|θ).

1
n

1 − λ

i=1

(cid:90)

ξtm(α) − 1

+

λ
1 − λ

(3.11)

This is a feasible procedure in the sens that we only need that data, the set of constraints and
the model of the parametric component.
Example 3.2 (Chi square). Let’s take the case of the χ2 divergence for which ϕ(t) = (t− 1)2/2.
The Convex conjugate of ϕ is given by ψ(t) = t2/2 + t. For (λ, θ, α) ∈ Φ ⊂ Rd. We have:

(cid:18)

inf
Q∈Mα

Dϕ

Q,

1

λ − 1

PT − λ
1 − λ

P1(.|θ)

= sup
ξ∈Rl+1

(cid:21)

(cid:0)ξtg(x)(cid:1)2 + ξtg(x)

(cid:90) (cid:20) 1
(cid:21)
(cid:0)ξtg(x)(cid:1)2 + ξtg(x)

2

dP1(x|θ).

dPT (x)

ξtm(α)− 1

1 − λ

(cid:90) (cid:20) 1

+

λ
1 − λ

2

(cid:90)

It is interesting to note that the supremum over ξ can be calculated explicitly. Clearly, the
optimized function is a polynomial of ξ and thus inﬁnitely diﬀerentiable. The Hessian matrix is
equal to −Ω where:

Ω =

g(x)g(x)t

1 − λ

dP (x) − λ
1 − λ

dP1(x|θ)

.

(3.12)

1

1−λ dP − λ

1−λ dP1(.|θ) is positive, then Ω is symmetric deﬁnite positive (s.d.p)

If the measure
and the Hessian matrix is symmetric deﬁnite negative. Consequently, the supremum over ξ is
ensured to exist. If it is a signed measure, it is possible, then, that the supremum is inﬁnity. We
may now write:

(cid:19)

(cid:19)(cid:19)

ξ(φ) = Ω−1

m(α) −
= ∞ otherwise.

g(x)

1 − λ

dP (x) − λ
1 − λ

dP1(x|θ)

,

if Ω is s.d.p

(cid:19)

(cid:18) 1

(cid:18) 1

n(cid:88)

i=1

(cid:90)

n(cid:88)

i=1

(cid:18)

(cid:32)

For the empirical criterion, we deﬁne similarly Ωn by:

Ωn =

1
n

1

1 − λ

g(Xi)g(Xi)t − λ
1 − λ

The solution to the corresponding supremum over ξ is given by:

ξn(φ) = Ω−1

n

m(α) − 1
n
= ∞ otherwise.

1

1 − λ

g(Xi) +

λ
1 − λ

g(x)dP1(x|θ)

(cid:90)

g(x)g(x)tdP1(x|θ).

(3.13)

(cid:90)

(cid:33)

,

if Ωn is s.d.p

4 Asymptotic properties

4.1 Consistency
The double optimization procedure deﬁning the estimator ˆφ deﬁned by (3.11) does not permit
us to use M-estimates methods to prove consistency.
In Keziou [2003] Proposition 3.7 and
in Broniatowski and Keziou [2009] Proposition 3.4, the authors propose a method which can
simply be generalized to such circumstances as the idea of the proof slightly depends on the
form of the optimized function. In order to restate this result here and give an exhaustive and a

11

general proof, suppose that our estimator ˆφ is deﬁned through the following double optimization
procedure:

ˆφ = arg inf

sup

Hn(φ, ξ),

φ

ξ

such that Hn(φ, ξ) → H(φ, ξ) in probability. Let φ∗ be the true argument of inﬁmum, i.e.

φ∗ = arg inf

sup

H(φ, ξ)

φ

ξ

We adapt the following notation:

ξ(φ) = arg sup

H(φ, t),

t

ξn(φ) = arg sup

t

Hn(φ, t)

The following theorem provides suﬃcient conditions for consistency of ˆφ towards φ∗. This result
will then be applied to the case of our estimator.
Assumptions:
A1. the estimate ˆφ exists (even if it is not unique);
A2. supξ,φ |Hn(φ, ξ) − H(φ, ξ)| tends to 0 in probability;
A3. for any φ, the supremum of H over ξ is unique and isolated, i.e. ∀ε > 0,∀ ˜ξ such that

(cid:107) ˜ξ − ξ(φ)(cid:107) > ε, then there exists η > 0 such that H(φ, ξ(φ)) − H(φ, ˜ξ) > η;

A4. the inﬁmum of φ (cid:55)→ H(φ, ξ(φ)) is unique and isolated, i.e. ∀ε > 0,∀φ such that (cid:107)φ− φ∗(cid:107) >

ε, there exists η > 0 such that H(φ, ξ(φ)) − H(φ∗, ξ(φ∗)) > η;

A5. for any φ in Φ, function ξ (cid:55)→ H(φ, ξ) is continuous.
In assumption A4, we suppose the existence and uniqueness of φ∗. It does not, however, imply
the uniqueness of ˆφ. This is not a problem for our consistency result. The vector ˆφ may be
any point which veriﬁes the minimum of function φ (cid:55)→ supξ Hn(φ, ξ). Our consistency result
shows that all vectors verifying the minimum of φ (cid:55)→ supξ Hn(φ, ξ) converge to the unique vector
φ∗. We also prove an asymptotic normality result which in its turn shows that even if ˆφ is not
unique, all possible values should be in a neighborhood of radius O(n−1/2) centered at φ∗.
The following lemma establishes a uniform convergence result for the argument of the supremum
over ξ of function Hn(φ, ξ) towards the one of function H(φ, ξ). It constitutes a ﬁrst step towards
the proof of convergence of ˆφ towards φ∗.
Lemma 4.1. Assume A2 and A3 are veriﬁed, then
(cid:107)ξn(φ) − ξ(φ)(cid:107) → 0,

in probability.

sup

φ

Proof. The proof is based partially on the proof of Proposition 3.7 part (ii) in Keziou [2003].
We proceed by contradiction. Let ε > 0 be such that supφ (cid:107)ξn(φ) − ξ(φ)(cid:107) > ε. Then, there
exists a sequence ak ∈ Φ such that (cid:107)ξn(ak) − ξ(ak)(cid:107) > ε. By assumption A3, there exists η > 0
such that:

H(ak, ξ(ak)) − H(ak, ξn(ak)) > η.

Thus,

P

(cid:32)

sup

φ

(cid:33)

(cid:107)ξn(φ) − ξ(φ)(cid:107) > ε

≤ P (H(ak, ξ(ak)) − H(ak, ξn(ak)) > η) .

(4.1)

Let’s prove that the right hand side tends to zero as n goes to inﬁnity which is suﬃcient to
accomplish our claim.
By deﬁnition of ξn(ak) and assumption A2, we can write:

Hn(ak, ξn(ak)) ≥ Hn(ak, ξ(ak))

≥ H(ak, ξ(ak)) − oP (1)

12

where oP (1) does not depend upon ak by virtue of A2. Now we have:

H(ak, , ξ(ak)) − H(ak, ξn(ak)) ≤ Hn(ak, ξn(ak)) − H(ak, , ξn(ak)) + oP (1)

≤ sup

|Hn(φ, ξ) − H(φ, ξ)| + oP (1).

ξ,φ

Last but not least, assumption A2 permits to conclude that the right hand side tends to zero
in probability. Since the left hand side is already nonnegative by deﬁnition of ξ(ak), then by
the previous result we conclude that H(ak, , ξ(ak)) − H(ak, ξn(ak)) tends to zero in probability.
Employing this ﬁnal result in inequality (4.1), we get that supφ (cid:107)ξn(φ) − ξ(φ)(cid:107) tends to zero in
probability.

We proceed now to announce our consistency theorem.

Theorem 4.1. Let ξ(φ) be the argument of the supremum of ξ (cid:55)→ H(φ, ξ) for a ﬁxed φ. Assume
that A1-A5 are veriﬁed, then ˆφ tends to φ∗ in probability.
Proof. We proceed by contradiction in a similar way to the proof of Lemma 4.1. Let κ > 0 be
such that (cid:107)φ∗ − ˆφ(cid:107) > κ, then by assumption A4, there exists η > 0 such that :

This can be rewritten as:

P(cid:16)(cid:107)φ∗ − ˆφ(cid:107) > κ

(cid:17) ≤ P(cid:16)

H( ˆφ, ξ( ˆφ)) − H(φ∗, ξ(φ∗)) > η

(cid:17)

.

(4.2)

H( ˆφ, ξ( ˆφ)) − H(φ∗, ξ(φ∗)) > η.

We now demonstrate that the right hand side tends to zero as n goes to inﬁnity. Let ε > 0 be
such that for n suﬃciently large, we have supξ,φ |H(φ, ξ) − Hn(φ, ξ)| < ε. This is possible by
virtue of assumption A2. The deﬁnition of ˆφ together with assumption A2 will now imply:

Hn( ˆφ, ξn( ˆφ)) ≤ Hn(φ∗, ξn(φ∗))

≤ H(φ∗, ξn(φ∗)) + sup
≤ H(φ∗, ξn(φ∗)) + ε.

ξ,φ

|H(φ, ξ) − Hn(φ, ξ)|

(4.3)
We use now the continuity assumption A5 of function ξ (cid:55)→ H(φ∗, ξ) at ξ(φ∗). For the ε chosen
earlier, there exists δ(φ∗, ε) such that if (cid:107)ξ(φ∗) − ξn(φ∗)(cid:107) < δ(φ∗, ε), then:

|H(φ∗, ξn(φ∗)) − H(φ∗, ξ(φ∗))| < ε.

This is possible for suﬃciently large n since supφ (cid:107)ξ(φ∗) − ξn(φ∗)(cid:107) tends to zero in probability
by Lemma 4.1. Inserting this result in (4.3) gives:

Hn( ˆφ, ξn( ˆφ)) ≤ H(φ∗, ξ(φ∗)) + 2ε.

We now have:
H( ˆφ, ξ( ˆφ)) − H(φ∗, ξ(φ∗)) ≤ H( ˆφ, ξ( ˆφ)) − Hn( ˆφ, ξn( ˆφ)) + 2ε

≤ H( ˆφ, ξ( ˆφ)) − H( ˆφ, ξn( ˆφ)) + H( ˆφ, ξn( ˆφ)) − Hn( ˆφ, ξn( ˆφ)) + 2ε.
Continuity assumption of H implies that for ε > 0, there exists δ( ˆφ, ε) > 0 such that if (cid:107)ξ( ˆφ) −
ξn( ˆφ)(cid:107) < δ( ˆφ, ε), then:

(cid:12)(cid:12)(cid:12)H( ˆφ, ξ( ˆφ)) − H( ˆφ, ξn( ˆφ))
(cid:12)(cid:12)(cid:12) ≤ ε.

This is again possible for suﬃciently large n since supφ (cid:107)ξ(φ∗) − ξn(φ∗)(cid:107) tends to zero in proba-
bility by Lemma 4.1. This entails that:

H( ˆφ, ξ( ˆφ)) − H(φ∗, ξ(φ∗)) ≤ H( ˆφ, ξn( ˆφ)) − Hn( ˆφ, ξn( ˆφ)) + 3ε

|H(φ, ξ) − Hn(φ, ξ)| + 3ε

≤ sup
≤ 4ε

ξ,φ

We conclude that the right hand side in (4.2) goes to zero and the proof is completed.

13

Let’s now go back to our optimization problem (3.11) in order to simplify the previous
assumptions. First of all, we need to specify functions H and Hn. Deﬁne function h as follows.
Let φ = (λ, θ, α),

ψ(cid:0)ξtg(z)(cid:1) +

λ
1 − λ

(cid:90)

ψ(cid:0)ξtg(x)(cid:1) dP1(x|θ).

h(φ, ξ, z) = ξtm(α) − 1

1 − λ

Functions H and Hn can now be deﬁned through h by:

{ψ(cid:0)ξtg(z)(cid:1) , ξ ∈ R(cid:96)} is a Glivenko-Cantelli class of functions, then the estimator deﬁned by

Theorem 4.2. Assume that A1, A4 and A5 are veriﬁed. Suppose also that the class of functions

Hn(φ, ξ) = Pnh(φ, ξ, .).

H(φ, ξ) = PT h(φ, ξ, .),

(3.11) is consistent.

Proof. We will use Theorem 4.1. We need to verify assumptions A2 and A3. By assumption
B2, the class of functions {(φ, ξ) (cid:55)→ h(φ, ξ, .)} is a Glivenko-Cantelli class of functions and thus
assumption A2 is fulﬁlled by the Glivenko-Cantelli theorem. Finally, assumption A3 can be
checked by strict concavity of function ξ (cid:55)→ H(φ, ξ). Indeed, for any η ∈ (0, 1) and any ξ1, ξ2,
we have by strict convexity of ψ :

2g(x)(cid:1) < ηψ(cid:0)ξt

1g(x)(cid:1) + (1 − η)ψ(cid:0)ξt

2g(x)(cid:1) .

1g(x) + (1 − η)ξt

ψ(cid:0)ηξt

If the measure dP/(1 − λ) − λdP1(.|θ)/(1 − λ) is positive5, we may write:

(cid:90)
(cid:90)

η

ψ(cid:0)ηξt
1g(x)(cid:1)(cid:18) 1
ψ(cid:0)ξt

1g(x) + (1 − η)ξt

2g(x)(cid:1)(cid:18) 1

1 − λ

dP (x) − λ
1 − λ

1 − λ
dP1(x|θ)

dP (x) − λ
1 − λ
+(1−η)

(cid:19)

(cid:90)

dP1(x|θ)

(cid:19)
2g(x)(cid:1)(cid:18) 1
ψ(cid:0)ξt

<

1 − λ

(cid:19)

,

dP (x) − λ
1 − λ

dP1(x|θ)

which entails that

H(φ, ηξ1 + (1 − η)ξ2) > ηH(φ, ξ1) + (1 − η)H(φ, ξ2),

and function ξ (cid:55)→ H(φ, ξ) becomes strictly concave. However, the measure dP/(1 − λ) −
λdP1(.|θ)/(1−λ) is in general a signed measure and the previous implication does not hold. This
is not dramatic because function ξ (cid:55)→ H(φ, ξ) has only two choices; it is either strictly convex or
strictly concave. In case function ξ (cid:55)→ H(φ, ξ) is strictly convex, then its supremum is inﬁnity
and the corresponding vector φ does not count in the calculus of the inﬁmum after all. This
means that the only vectors φ ∈ Φ which interest us are those for which function ξ (cid:55)→ H(φ, ξ)
is strictly concave. In other words, the inﬁmum in (3.11) can be calculated over the set:

Φ+ = Φ ∩ {φ :

ξ (cid:55)→ H(φ, ξ) is strictly concave}

instead of over Φ. All assumptions of Theorem 4.1 are now fulﬁlled and ˆφ converges in probability
to φ∗.
Assumption A5 could be handled using Lebesgue’s continuity theorem if one ﬁnds a PT−integrable

function ˜h such that |ψ(cid:0)ξtg(z)(cid:1)| ≤ ˜h(z). This is, however, not possible in general unless we

restrain ξ to a compact set. Otherwise, we need to verify this assumption according the situa-
tion we have in hand, see example 4.1 below for more details. Assumption B2 can be treated
according to the divergence and the constraints which we would like to impose. A simple case
of a Gliveko-Cantelli class of function in the parametric settings, is when the set Φ is compact
and there exists a function U which upper bounds all members of the parametric family. In our
case, the ”parametric family” is deﬁned by (h(φ, ξ, .))φ,ξ. We still need to ﬁnd a way to restrain
the values of ξ to a compact set. For more details and other possibilities, see Van Der Vaart
[1998] Chap. 19 Section 2 and the examples therein.

5This measure can never be zero since it integrates to one, thus we do not need to suppose that it is nonnegative.

14

Remark 4.1. Under suitable diﬀerentiability assumptions, the set Φ+ deﬁned earlier in the
proof of Theorem 4.1 can be rewritten as:

JH(φ,.) is deﬁnite negative(cid:9) ,

(cid:19)
where JH(φ,.) is the Hessian matrix of function ξ (cid:55)→ H(φ, ξ) and is given by:

(cid:18) 1

JH(φ,.) = −

g(x)g(x)tψ(cid:48)(cid:48)(ξtg(x))

1 − λ

dPT − λ
1 − λ

dP1

(x).

(4.4)

Φ+ = Φ ∩(cid:8)φ :
(cid:90)

The problem with using the set Φ+ is that if we take a point φ in the interior of Φ, there is
no guarantee that it would be an interior point of Φ+. This is an important thing needed for
the study of the regularity of function φ (cid:55)→ ξ(φ) and the asymptotic normality later on. In the
following two results, we prove however, that this is not the case for φ∗. Moreover, we prove that
int(Φ+) is not empty. On the other hand, we give a way to verify assumption B2 in Theorem
4.2.
Proposition 4.1. Assume that function ξ (cid:55)→ H(φ, ξ) is of class C2 for any φ ∈ Φ. Suppose that
φ∗ is an interior point of Φ, then there exists a neighborhood V of φ∗ such that for any φ ∈ V,
JH(φ,.) is deﬁnite negative and thus ξ(φ) exists and is ﬁnite. Moreover, function φ (cid:55)→ ξ(φ) is
continuously diﬀerentiable on V.
Proof. We already have:

1

1 − λ∗ PT − λ∗

1 − λ∗ P1(.|θ∗) = P ∗

0 ,

and since P ∗
0 is supposed to be a probability measure, the matrix JH(φ∗,.) is deﬁnite negative.
Thus φ∗ ∈ Φ+. Since the set of negative deﬁnite matrices is an open set (see for example
page 36 in Lange [2013]), there exists a ball U of negative deﬁnite matrices centered at JH(φ∗,.).
Continuity of φ (cid:55)→ JH(φ,.) permits6 to ﬁnd a ball B(φ∗, ˜r) such that the subset {JH(φ,.) : φ ∈
B(φ∗, ˜r)} is inside U. Now the neighborhood we are looking at is the ball B(φ∗, ˜r).
For the second part of the proposition, the existence and ﬁniteness of ξ(φ) for φ ∈ V = B(φ∗, ˜r)
is immediate since function ξ (cid:55)→ H(φ, ξ) is strictly concave. Besides the the diﬀerentiability of
the function φ (cid:55)→ ξ(φ) is a direct result of the implicit function theorem applied on the equation
ξ (cid:55)→ ∇H(φ, .). Notice that the Hessian matrix of H(φ, .) is invertible since it is symmetric
deﬁnite negative.
Corollary 4.1. If Φ is bounded, then ξ(int(Φ+)) is bounded and {h(φ, ξ), φ ∈ int(Φ+), ξ ∈
ξ(int(Φ+))} is a Glivenko-Cantelli class of functions.
Proof. The ﬁrst part of the corollary is an immediate result of the continuity of function φ (cid:55)→ ξ(φ)
over int(Φ+), and the second part is an immediate result of Example 19.7 page 271 from Van
Der Vaart [1998].

Remark 4.2.

1

1−λ dPT − λ

• There is a great diﬀerence between the set where JH is deﬁnite negative
1−λ dP1 is a probability measure. There is a
and the set where the measure
1−λ dP1 is a probability measure,
strict inclusion in the sense that if the measure
then JH is deﬁnite negative, but the inverse is not right. Figure (1) shows this diﬀerence.
Moreover, it is clearly simpler to check for a vector φ if the matrix JH is deﬁnite negative.
It suﬃces to calculate the integral7 (even numerically) and then use some rule such as
Sylvester’s rule to check if it is deﬁnite negative. However, in order to check if the measure
1−λ dPT − λ

1−λ dP1 is positive, we need to verify it on all Rs.

1

1

1−λ dPT − λ

6To see this, consider Sylvester’s rule which is based on a test using the determinant of the sub-matrices
of JH . Each determinant needs to be negative. The continuity of the determinant function together with the
continuity of φ (cid:55)→ JH(φ,.) will imply that we may move around J(H(φ∗, .)) in a small neighborhood in a way that
the determinants of the sub-matrices stay negative.

7If function g is a polynomial, i.e. moment constraints, then the integral is a mere subtractions between the

moments of PT and the ones of P1.

15

Figure 1: Diﬀerences between the set where
Weibull–Lognormal mixture.

1

1−λ dPT − λ

1−λ dP1 is positive and the set Φ+ in a

• The previous point shows the interest of adapting a methodology based on signed mea-
sures and not only positive ones. We have a better space to search inside for the triplet
(λ∗, θ∗, α∗). Even if the algorithm gives a triplet (λ, θ, α) for which the semiparametric
component P0 = 1
1−λ dP1 is not a positive measure, it should not mean that the
procedure failed. This is because we are looking for the parameters and not to estimate
P0 in such case, a thresholding can be applied to make P0 positive. In many models, a
slight translation from the true set of parameters results in P0 which is not positive such
as the Weibull–lognormal mixture considered in ﬁgure (1).

1−λ dPT − λ

Example 4.1 (χ2 case). Consider the case of a two-component semiparapetric mixture where
P0 is deﬁned through its ﬁrst three moments. In other words, the set of constraints Mα is given
by:

(cid:26)

(cid:90)

Mα =

(cid:90)

(cid:90)

Q :

dQ(x) = 1,

xdQ(x) = m1(α),

x2dQ(x) = m2(α),

x3dQ(x) = m3(α)

.

(cid:27)

(cid:90)

(cid:19)(cid:19)

We have already seen in example ??? that if ψ(t) = t2/2 + t, the Pearson’s χ2 convex conjugate,
then the optimization over ξ can be solved and the solution is given by:

where,

ξ(φ) = Ω−1

Ω =

=

(cid:90)

g(x)

(cid:18) 1
(cid:18) 1

m(α) −

(cid:18)
(cid:90)
(cid:20) 1

g(x)g(x)t

1 − λ
Mi+j−2 − λ
1 − λ

1 − λ

dP (x) − λ
1 − λ

(cid:21)

M (1)

i+j−2

i,j∈{1,··· ,4}

1 − λ

dP (x) − λ
1 − λ

dP1(x|θ)

(cid:19)

dP1(x|θ)

The solution holds for any φ ∈ int(Φ+). Otherwise ξ(φ) = ∞. Continuity and diﬀerentiability

16

assumptions A5 over ξ (cid:55)→ H(φ, ξ) are simpliﬁed here because function H can be rewritten as:

H(φ, ξ) = ξtm(α) −

+ (ξ2

2/2 + ξ1ξ2 + ξ3)

+ (ξ2

3/2 + ξ2ξ4)

(cid:20) 1
(cid:18) 1
(cid:18) 1

1 − λ

2

ξ2
1 + ξ1 + (ξ1ξ2 + ξ2)

M (1)

M2 − λ
1 − λ
M4 − λ
1 − λ

1 − λ

2

(cid:18) 1
(cid:19)
(cid:19)

1 − λ

M (1)

4

M (1)

1

(cid:19)
(cid:18) 1
(cid:18) 1

M1 − λ
1 − λ

+ (ξ1ξ4 + ξ2ξ3 + ξ4)

+ ξ3ξ4

1 − λ

(cid:18) 1

+ξ2

4/2

(cid:19)

M3 − λ
1 − λ

1 − λ
M5 − λ
1 − λ
M6 − λ
1 − λ

1 − λ

M (1)

5

M (1)

6

M (1)

3

(cid:19)
(cid:19)(cid:21)

where:

Mi = EPT [X i],

M (1)

i = EP1(.|θ)[X i].

i

Regularity of function φ (cid:55)→ ξ(φ) is directly tied by the regularity of the moments of P1(.|θ) with
respect to θ. If M (1)
is continuous with respect to θ and m(α) is continuous with respect to α,
then the existence of φ∗ becomes immediate as soon as the set Φ is compact.
If φ∗ is an interior point of Φ, then Proposition 4.1 and Corollary 4.1 apply. Thus int(Φ+) is
non void and the class {h(φ, ξ), φ ∈ Φ+, ξ ∈ ξ(int(Φ+))} is a Glivenko-Cantelli class of functions.
What remains for the consistency is the veriﬁcation of assumption A4. This remains speciﬁc to
the model we have in hand. All other calculus shown above in this example remain valid for any
modeling problem with the three ﬁrst moments.

4.2 Asymptotic normality
We will, of course, suppose that the model pφ is C2(int(Φ)) and that ψ is C2(R). In order to
simplify the formula below, we suppose that ψ(cid:48)(0) = 1 and ψ(cid:48)(cid:48)(0) = 1. These are not restrictive
assumptions and can be relaxed. Recall that they are both veriﬁed in the class of Cressie-Read
functions.
Deﬁne the following matrices:

(cid:17)
1−λ∗(cid:82) g(x)∇θp1(x|θ∗)dx,∇m(α∗)

(cid:16)
Σ = (cid:0)J t

Jφ∗,ξ∗ =
Jξ∗,ξ∗ = EP ∗

0

1

(1−λ∗)2

(cid:2)−EPT [g(X)] + EP1(.|θ∗) [g(X)](cid:3) , λ∗
(cid:2)g(X)g(X)t(cid:3) ;
φ∗,ξ∗Jξ∗,ξ∗Jφ∗,ξ∗(cid:1)−1 ;

H = ΣJ t
W = J−1

φ∗,ξ∗J−1
ξ∗,ξ∗;
ξ∗,ξ∗ − J−1
ξ∗,ξ∗Jφ∗,ξ∗ΣJ t
(cid:90)

(cid:18) 1

g(x)g(x)t

φ :

φ∗,ξ∗J−1

ξ∗,ξ∗.

(cid:26)
(cid:40)

Φ+ =

Φ+

n =

φ :

1
n

1

1 − λ

1 − λ

dPT − λ
1 − λ
g(Xi)g(Xi)t − λ
1 − λ

dP1

(cid:90)

n(cid:88)

i=1

We will need to deﬁne the following two sets:

; (4.5)

(4.6)

(4.7)

(4.8)

(4.9)

(cid:41)

(4.10)

. (4.11)

(cid:19)

(cid:27)

(x|θ) is s.p.d.

;

g(x)g(x)tdP1(x|θ) is s.p.d.

The set Φ+ is the same as before but with −JH instead. These two sets are the feasible sets of
parameters. In other words, outside the set Φ+, we have H(φ, ξ(φ)) = ∞. Similarly, outside the
set Φ+

n , we have Hn(φ, ξn(φ)) = ∞.

Theorem 4.3. Suppose that:

1. ˆφ is consistent and φ∗ ∈ int (Φ);

17

2. diﬀerentiability with respect to ξ whatever the value of φ in a neighborhood of φ∗, i.e.
∀φ ∈ B(φ∗, ˜r) and any ξ ∈ ξ(B(φ∗, ˜r)), there exist functions h1,1, h1,2 ∈ L1(p1(.|θ)) such

3. function α (cid:55)→ m(α) is C2;
4. diﬀerentiability with respect to θ in a neighborhood of θ∗ inside ξ(B(φ∗, ˜r)), i.e. ∀ξ ∈

that(cid:13)(cid:13)ψ(cid:48)(cid:0)ξtg(x)(cid:1) g(x)(cid:13)(cid:13) ≤ h1,1(x) and(cid:13)(cid:13)ψ(cid:48)(cid:48)(cid:0)ξtg(x)(cid:1) g(x)g(x)t(cid:13)(cid:13) ≤ h1,2(x);
ξ(B(φ∗, ˜r)), there exist functions h2,1, h2,2 ∈ L1(dx) such that (cid:13)(cid:13)ψ(cid:0)ξtg(x)(cid:1)∇θp1(x|θ)(cid:13)(cid:13) ≤
h2(x) and(cid:13)(cid:13)ψ(cid:0)ξtg(x)(cid:1) Jp1(.|θ)
a function h3 ∈ L1(dx) such that(cid:13)(cid:13)ψ(cid:48)(cid:0)ξtg(x)(cid:1) g(x)∇θp1(x|θ)t(cid:13)(cid:13) ≤ h3(x);

5. diﬀerentiability with respect to ξ and θ, i.e. for (φ, ξ) ∈ B(φ∗, ˜r)× ξ(B(φ∗, ˜r)), there exists

(cid:13)(cid:13) ≤ h2(x);

6. ﬁnite second order moment of g under PT , i.e. EPT [gi(X)gj(X)] < ∞ for i, j ≤ (cid:96);
7. matrices Jξ∗,ξ∗ and J t

φ∗,ξ∗Jξ∗,ξ∗Jφ∗,ξ∗ are invertible,

(cid:32) √

(cid:16) ˆφ − φ∗(cid:17)

(cid:33)

n
√

nξn( ˆφ)

(cid:18)

−→L N

0,

1

(1 − λ∗)2

(cid:18) H

W

(cid:19)

VarPT (g(X))(cid:0)H t W t(cid:1)(cid:19)

,

then

where H and P are given by formulas (4.8) and (4.9).

Proof. We follow the steps of Theorem 3.2 in Newey and Smith [2004]. The idea behind the
proof is a mean value expansion with Lagrange remainder of the estimating equations.
We need at ﬁrst to verify if ˆφ belongs to the interior of Φ+ in order to be able to diﬀerentiate
φ (cid:55)→ Hn(φ, ξ). This can be done similarly to Proposition 4.1. We also can prove (by replacing
H by Hn and ξ(φ) by ξn(φ)) that φ (cid:55)→ ξn(φ) is continuously diﬀerentiable in a neighborhood of
φ∗.
We may now proceed to the mean value expansion. By the very deﬁnition of ξn(φ), we have:

∂Hn
∂ξ

(φ, ξn(φ)) = 0

∀φ ∈ int(Φ+),

which also holds for φ = ˆφ, i.e.

∂Hn
∂ξ

( ˆφ, ξn( ˆφ)) = 0.

On the other hand, the deﬁnition of ˆφ implies that:

(cid:12)(cid:12)(cid:12)(cid:12)φ= ˆφ

∂
∂φ

Hn(φ, ξn(φ))

= 0.

Since function φ (cid:55)→ ξn(φ) is continuously diﬀerentiable. A simple chain rule would imply

(cid:12)(cid:12)(cid:12)(cid:12)φ= ˆφ

∂
∂φ

(Hn(φ, ξn(φ)))

=

=

∂
∂φ

∂
∂φ

Hn( ˆφ, ξn( ˆφ)) +

∂
∂ξ

Hn( ˆφ, ξn( ˆφ))

∂ξn
∂φ

( ˆφ)

Hn( ˆφ, ξn( ˆφ)).

The second line comes from the deﬁnition of ξn(φ) as the argument of the supremum of function
ξ (cid:55)→ Hn(φ, ξ). Now, the estimating equations are given simply by:

∂Hn
∂ξ
∂Hn
∂φ

( ˆφ, ξn( ˆφ)) = 0;

( ˆφ, ξn( ˆφ)) = 0.

18

We need to calculate these partial derivatives. We start by the derivative with respect to ξ:

ψ(cid:48)(cid:0)ξtg(xi)(cid:1) g(xi) +

n(cid:88)

i=1

(cid:90)

λ
1 − λ

ψ(cid:48)(cid:0)ξtg(x)(cid:1) g(x)p1(x|θ)dx (4.12)

∂Hn
∂ξ

(φ, ξ) = m(α) − 1

1 − λ

1
n

We calculate the partial derivatives with respect to α, λ and θ:

∂Hn
∂α
∂Hn
∂λ

∂Hn
∂θ

= ξt∇m(α)

= −

1

(cid:90)

(1 − λ)2
λ
1 − λ

=

n(cid:88)
ψ(cid:0)ξtg(xi)(cid:1) +
ψ(cid:0)ξtg(x)(cid:1)∇θp1(x|θ)dx

1
n

i=1

1

(1 − λ)2

(cid:90)

ψ(cid:0)ξtg(x)(cid:1) p1(x|θ)dx

(4.13)

(4.14)

(4.15)

Notice that by Lemma 4.1, the continuity of φ (cid:55)→ ξ(φ) and the consistency of ˆφ towards φ∗,
we have ξn( ˆφ) → ξ(φ∗) = 0 in probability. A mean value expansion of the estimating equation
between ( ˆφ, ξn( ˆφ)) and (φ∗, 0) implies that there exists ( ¯φ, ¯ξ) on the line between these two

points such that:(cid:32) ∂Hn

(cid:33)

=

(cid:33)

(cid:32) ∂Hn
∂φ (φ∗, 0)
∂ξ (φ∗, 0)

∂Hn

∂φ ( ˆφ, ξn( ˆφ))
∂ξ ( ˆφ, ξn( ˆφ))
∂Hn

(cid:18) ˆφ − φ∗

(cid:19)

ξn( ˆφ)

+ JHn( ¯φ, ¯ξ)

,

(4.16)

where JHn( ¯φ, ¯ξ) is the matrix of second derivatives of Hn calculated at the mid point ( ¯φ, ¯ξ). The
left hand side is zero, so we need to calculate the ﬁrst vector in the right hand side. We have
by simple substitution in formula (4.12):

∂Hn
∂ξ

(φ∗, 0) = m(α∗) − 1

1 − λ∗

1
n

g(xi) +

λ∗
1 − λ∗

g(x)p1(x|θ∗)dx.

n(cid:88)

i=1

(cid:90)

Using the assumption that the model (3.2) verify the set of constraints deﬁning Mα together
with the CLT, we write:
√

(cid:19)

(cid:18)

1

n

∂Hn
∂ξ

(φ∗, 0) −→L N

0,

(1 − λ∗)2 VarPT (g(X))

.

(4.17)

Using formulas (4.13), (4.14) and (4.15), we may write:

(φ∗, 0) = 0;
(φ∗, 0) = −

(φ∗, 0) =

∂Hn
∂α
∂Hn
∂λ
∂Hn
∂θ

1

(cid:90)

(1 − λ∗)2 +
λ∗
1 − λ∗

1

(1 − λ∗)2 = 0;

∇θp1(x|θ∗)dx =

λ∗
1 − λ∗∇θ

(cid:90)

p1(x|θ∗)dx = 0.

The ﬁnal line holds since by Lebesgue’s diﬀerentiability theorem using assumption 5 for ξ = 0,
we can change between the sign of integration and derivation. Combine this with the fact that
p1(x|θ∗) is a probability density function which integrates to 1, gives the result in the last line.
We need now to write an explicit form for the matrix JHn( ¯φ, ¯ξ) and study its limit in probability.
It contains the second order partial derivatives of function Hn with respect to its parameters.

19

We start by the double derivatives. Using formulas (4.12), (4.13), (4.14) and (4.15), we write:

(cid:90)

λ
1 − λ

ψ(cid:48)(cid:48)(cid:0)ξtg(x)(cid:1) g(x)g(x)tp1(x|θ)dx;

(cid:90)

ψ(cid:0)ξtg(x)(cid:1) p1(x|θ)dx;

(cid:90)

ψ(cid:48)(cid:0)ξtg(x)(cid:1) g(x)p1(x|θ)dx;

1

(1 − λ)2

2

i=1

1
n

n(cid:88)

ψ(cid:48)(cid:48)(cid:0)ξtg(xi)(cid:1) g(xi)g(xi)t +
n(cid:88)
ψ(cid:0)ξtg(xi)(cid:1) +
ψ(cid:0)ξtg(x)(cid:1) Jp1(x|θ)dx;
n(cid:88)
ψ(cid:48)(cid:0)ξtg(xi)(cid:1) g(xi) +
ψ(cid:48)(cid:0)ξtg(x)(cid:1) g(x)∇θp1(x|θ)tdx;

(1 − λ)3

1
n

i=1

i=1

1
n

∂2Hn
∂ξ2 = − 1
1 − λ
∂2Hn
∂α2 = ξtJm(α);
∂2Hn
∂λ2 = −
2
(cid:90)
∂2Hn
∂θ2 =
∂2Hn
∂ξ∂α
∂2Hn
∂ξ∂λ

(1 − λ)3
λ
1 − λ

= ∇m(α);

= −

1

(cid:90)

(1 − λ)2
λ
1 − λ

=

= 0;

= 0;

=

1

(1 − λ)2

(cid:90)

ψ(cid:0)ξtg(x)(cid:1)∇θp1(x|θ)dx.

∂2Hn
∂ξ∂θ
∂2Hn
∂α∂λ
∂2Hn
∂α∂θ
∂2Hn
∂λ∂θ

As n goes to inﬁnity, we have ¯ξ → 0 and ¯φ → φ∗. Then, under regularity assumptions of the
present theorem, we can calculate the limit in probability of the matrix JHn( ¯φ, ¯ξ). The blocks
limits are given by:

∂2Hn
∂α2

P→ 0,

∂2Hn
∂λ2

P→ 0,

(cid:90)

∂2Hn
∂θ2

P→ 0,

∂2Hn
∂ξ∂α

P→ ∇m(α∗)

g(x)p1(x|θ∗)dx

∂2Hn
∂ξ∂λ

P→ −

1

(1 − λ∗)2

EPT [g(X)] +

1

(1 − λ∗)2

(cid:2)g(X)g(X)t(cid:3) ,

∂2Hn
∂ξ2

P→ −EP ∗

0

(cid:90)

∂2Hn
∂ξ∂θ

P→ λ∗
1 − λ∗

g(x)∇θp1(x|θ∗)dx,

∂2Hn
∂α∂λ

P→ 0,

∂2Hn
∂α∂θ

P→ 0,

∂2Hn
∂λ∂θ

P→ 0,

taking into account that ψ(0) = 0, ψ(cid:48)(0) = 1 and ψ(cid:48)(cid:48)(0) = 1. The limit in probability of the
matrix JHn( ¯φ, ¯ξ) can be written in the form:

(cid:21)

,

(cid:20)

JH =

0

J t
φ∗,ξ∗
Jφ∗,ξ∗ Jξ∗,ξ∗

(cid:18) −Σ H

H t W

(cid:19)

,

J−1
H =

where Jφ∗,ξ∗ and Jξ∗,ξ∗ are given by (4.5) and (4.6). The inverse of matrix JH has the form:

where

Σ =(cid:0)J t

φ∗,ξ∗Jξ∗,ξ∗Jφ∗,ξ∗(cid:1)−1 , H = ΣJ t

φ∗,ξ∗J−1

ξ∗,ξ∗, W = J−1

ξ∗,ξ∗ − J−1

ξ∗,ξ∗Jφ∗,ξ∗ΣJ t

φ∗,ξ∗J−1

ξ∗,ξ∗.

20

0

=

Solving this equation in φ and ξ gives:

Going back to (4.16), we have:

(cid:18) 0
(cid:32) √

(cid:19)
(cid:18)
(cid:16) ˆφ − φ∗(cid:17)
Finally, using (4.17), we get that:(cid:32) √

nξn( ˆφ)

n
√

where

This ends the proof.

S =

1

(1 − λ∗)2

0

∂Hn

∂ξ (φ∗, 0)
(cid:33)

H

= J−1

(cid:18)
(cid:16) ˆφ − φ∗(cid:17)
(cid:19)
(cid:18) H

nξn( ˆφ)

W

n
√

(cid:19)

+ JHn( ¯φ, ¯ξ)

(cid:19)

.

(cid:18) ˆφ − φ∗
(cid:19)

ξn( ˆφ)

√

0
∂ξ (φ∗, 0)
n ∂Hn

+ oP (1).

(cid:33)
−→L N (0, S)
VarPT (g(X))(cid:0)H t W t(cid:1) .

Diﬀerentiability assumptions in Theorem 4.3 can be relaxed in the case of the Pearson’s χ2
since all integrals in functions Hn and H can be calculated. Our result cover the general result
and thus we need to ensure diﬀerentiability of the integrals using Lebesgue theorems which
requires the existence of integrable functions which upperbound the integrands.

Remark 4.3. It is important to notice that the variance of the estimator becomes higher as
the proportion of the parametric part becomes higher.

5 Simulation study-Moment constraints

We perform several simulations in univariate and multivariate situations and show how a prior
information about the distribution of the semiparametric component P0 can help us better
estimate the set of parameters (λ∗, θ∗, α∗) in regular examples, i.e.
the components of the
mixture can be clearly distinguished when we plot the probability density function. We also
show how our approach permits to estimate even in diﬃcult situations when the proportion of
the parametric component is very low; such cases could not be estimated using existing methods.
Another important problem in existing methods is that they highly depend on the number of
observations. For example, an EM-type method such as Robin et al. [2007]’s algorithm or its
stochastic version introduced by Bordes et al. [2007] needs at each iteration to calculate ﬁrst
a weighted kernel density estimator and calculate it at each observation of the sample. This
calculus has a complexity of order n2. We then need to calculate a vector of weights of length
n, and estimate the proportion by averaging this vector of weights. Finally, we need to estimate
the parameters of P1 by maximum likelihood which can be done at the best cases by averaging
n terms. This means that such an algorithm needs to do at least n2 + 3n operations in order to
complete a single iteration. An EM-type algorithm for semiparametric mixture model needs in
average 100 iterations to converge and may attain 1000 iterations8. To conclude, the estimation
procedure performs at least 100(n2 +3n) operations. In a signal-noise situations where the signal
has a very low proportion around 0.05, we need a greater number of observations say n = 105.
Such experiences cannot be performed using an EM-type method such as Robin et al. [2007]’s
algorithm or its stochastic version introduced by Bordes et al. [2007] unless one has a ”super
computer”. The method of Bordes and Vandekerkhove [2010] shares similar complexity because
one needs to calculate a cumulative density estimator on each observation, and thus a complexity

8This was the case of the Weibull mixture.

21

n2. There is then the optimization step which needs at least 100 iterations to converge9. Last
but not least, the EM-type method of Song et al. [2010] and their π−maximizing one have the
advantage over other methods, because we need only to calculate a kernel density estimator once
and for all, then use it at each iteration10. Nevertheless, the method has still a complexity of
order n2.
Our approach although has a double optimization procedure can be implemented in a way that
it has a linear complexity n. First of all, one needs to use the χ2 divergence. This way, the
optimization over ξ in (3.11) can be calculated directly. On the other hand, all integrals are mere
calculus of empirical moments and moments of the parametric part, see Example 4.1. Empirical
moments can be calculated once and for all whereas moments of the parametric part can be
calculated using direct formulas available for a large class of probability distributions. What
remains is the optimization over φ. In the simulations below, our method produced the estimates
instantly even for a number of observations of order 106 whereas other existing methods needed
from several hours (algorithms of Song et al. [2010]) to several days (for other algorithms).
Because of the very long execution time of existing methods, we retrained the comparison to
simulations in regular situations with n < 104. Experiments with greater number of observations
were only treated using our method and the methods in Song et al. [2010]. In all tables presented
hereafter, we performed 100 experiments and calculated the average of resulting estimators. We
provided also the standard deviation of the 100 experiment in order to get a measure of preference
in case the diﬀerent estimation methods gave close results.
Our experiments cover four principle models:

• A two-component Weibull mixture;
• A two-component Weibull - Lognormal mixture;
• A two-component Gaussian – Two-sided Weibull mixture;
• A two-component bivariate Gaussian mixture.

We have chosen a variety of values for the parameters especially the proportion. The third
model stem from a signal-noise applications where the signal is near or centered at zero whereas
the noise has a heavier tail and is to the right or at both sided. The second model covers
two situations. A signal-noise case and then a case which resembles to the study of queues is
studied in mixtures 1 and 3. The regular and normal customers are represented by the distant
component whereas the impatient customers are represented by the component near zero. The
fourth model appears in clustering and is uniquely presented to show how our method perform
in multivariate contexts.
In all our experiments, no numerical integration was used since they can be easily calculated as
functions of the empirical moment of the data and the moments of the parametric component.
Simulations were done using the R Core Team [2015]. Optimization was performed using the
Nelder-Mead algorithm, see Nelder and Mead [1965]. For the π−maximizing algorithm of Song
et al. [2010], we used the Brent’s method because the optimization was carried over one param-
eter.
For our procedure, we only used the χ2 divergence, because the optimization over ξ can be cal-
culated without numerical methods11. Recall that the optimized function over ξ is not always
strictly concave and the Hessian matrix may be deﬁnite positive, see remark 4.1.
It is thus
important to check for each vector φ = (λ, θ, α) if the Hessian matrix is still deﬁnite negative
for example using Sylvester’s criterion. If it is not, we set the objective function to a value such
as 102. Besides, since the resulting function φ (cid:55)→ Hn(φ, ξn(φ)) as a function of φ is not ensured

9we need more than 24 hours to estimate the parameters of one sample with 105 observations.
10We were able to perform simulations with n = 105 observations but needed about 5 days on an i7 laptop
clocked at 2.5 GHz with 8GB of RAM. For Robin et al. [2007]’s algorithm, a few iterations took about one day.
One can imagine the time needed to estimate 100 samples with 105 observations in each sample.

11We noticed no great diﬀerence when using a Hellinger divergence.

22

to be strictly convex, we used 10 random initial feasible points inside the set Φ+
n deﬁned by
(4.11). We then ran the Nelder-Mead algorithm and chose the vector of parameters for which
the objective function has the lowest value. We applied a similar procedure on the algorithm of
Bordes and Vandekerkhove [2010] in order to ensure a good optimization.

Remark 5.1. In the literature on the stochastic EM algorithm, it is advised that we iterate
the algorithm for some time until it reaches a stable state, then continue iterating long enough
and average the values obtained in the second part. The trajectories of the algorithm was very
erratic especially for the estimation of the proportion. For us, we iterated for the stochastic
EM-type algorithm of Bordes et al. [2007] 5000 times and averaged the 4000 ﬁnal iterations.

Remark 5.2. Initialization of both the EM-type algorithm of Song et al. [2010] and the SEM-
type algorithm of Bordes et al. [2007] was not very important, and we got the same results when
the vector of weights was initialized uniformly or in a ”good” way. The method of Robin et al.
[2007] was more inﬂuenced by such initialization and we used most of the time a good initials.

Remark 5.3. Matrix inversion was done manually using direct inversion methods, because the
function solve in the statistical program R produced errors sometimes because the matrix was
highly sensible at some point during the optimization. For matrices of dimension 4× 4 and 5× 5
we used block matrix inversion, see for example Lu and Shiou [2002]. The inverse of a 3× 3 was
calculated using a direct formula.

5.1 Weibull mixtures

We consider a mixture of two Weibull components with scales σ1 = 0.5, σ2 = 1 and shapes
ν1 = 2, ν2 = 1. The parametric component will be ”the one to the right”, i.e. the component
whose true set of parameters is (ν1 = 2, σ1 = 0.5). We illustrate several values of the proportion
λ ∈ {0.7, 0.3}. The mixture is plotted in ﬁgure (2). This constitutes a diﬃcult example for both
our method and existing methods such as EM-type methods or the π−maximizing algorithm
of Song et al. [2010]. We therefore, simulated 10000 samples and ﬁxed both scales during
estimation. We estimate the proportion and the shapes of both components. For our method,
the variance of the estimator of ν1 was high and we needed to use 4 moments to reduce it to an
acceptable rage. Of course, as the number of observations increases, the variance reduces. We,
however, avoided greater number of observations because methods such as Robin et al. [2007]
need very long execution time for even one sample. The method of Bordes and Vandekerkhove
[2010] cannot be applied here since the support of the mixture is R+.
Moments of the Weibull distribution are given by:

E[X i] = σiΓ(1 + i/ν),

∀i ∈ N.

The results of our method are clearly better than existing methods which practically failed and
could not see but one main component with shape in between the two shapes, see table (1).
Although our method presents an inconvenient greater variance for ν1, the Monte-Carlo mean
of the hundred experiences is still at its true value. We believe that the use of other types of
constraints would have resulted in better results without the need to add one more constraint.

5.2 Weibull-LogNormal mixtures

We consider a mixture of two components; a Weibull component with scale = 1 and shape = 1.5,
and a log-normal component with meanlog = 3 and scale = 0.5. The parametric part is the
lognormal in table (2). The parametric part is the Weibull in table (3).
The number of observations for each mixture depends on its complexity. As the proportion of
the parametric component becomes lower, we needed more observations to better estimate. We
tried to choose the number of observations in a way that the standard deviation of estimated

23

λ

Nb of observations

Robin

Song EM-type

Song π−maximizing

Mixture 1 : n = 104 λ∗ = 0.7, ν∗
Pearson’s χ2 3 moments
0.700
Pearson’s χ2 4 moments
0.701
0.654
0.907
0.782
Mixture 1 : n = 104 λ∗ = 0.3, ν∗
Pearson’s χ2 3 moments
0.304
Pearson’s χ2 4 moments
0.303
0.604
0.806
0.624

Song π−maximizing

Song EM-type

Robin

ν

µ

sd(λ)
1 = 2, σ∗
0.010
0.010
0.101
0.004
0.006
1 = 2, σ∗
0.016
0.016
0.029
0.005
0.007

2 = 1, σ∗
1.005
1.013

sd(ν)
1 = 0.5(ﬁxed), ν∗
0.217
0.086
0.085 —
0.020 —
0.012 —

2.006
2.014
1.591
1.675
1.443

2 = 1, σ∗
0.998
1.001

1 = 0.5(ﬁxed), ν∗
0.887
0.285
0.037 —
0.018 —
0.013 —

2.191
2.120
1.256
1.185
1.312

sd(µ)

0.024
0.024

—
—
—

0.013
0.013

—
—
—

2 = 1(ﬁxed)

2 = 1(ﬁxed)

Table 1: The mean value with the standard deviation of estimates in a 100-run experiment on
a two-component Weibull mixture.

parameters does not exceed 1.
We used the three ﬁrst moments such that we can estimate three parameters; the proportion
of the parameteric component, the shape of the Weibull component and the mean-parameter
of the Lognormal component. Thus, the scale of both components are supposed to be known
during the estimation procedure. The moments of these two distributions are given by:

Weibull:

Lognormal:

E[X i] = σiΓ(1 + i/ν);
E[X i] = eiµ+i2σ2/2.

Our new method seems to produce high variance of the shape of the Weibull component. This
should not be surprising, because the part which inﬂuences on the moments of the model is
the Lognormal component. Its moments have an exponential form and small diﬀerences in the
mean-parameter can compensate for a great diﬀerences in the shape of the Weibull component.
The results are still satisfactory since we get to estimate an information of the semiparametric
component at a great precision together with the proportion.

λ

µ

sd(µ)

Nb of observations
sd(λ)
Mixture 1 : λ∗ = 0.7, µ∗ = 3, σ∗
0.117
0.068
0.044
0.030
0.018

n = 102
n = 103
n = 104
n = 105
n = 106

0.384
0.518
0.605
0.651
0.682

2 = 0.5(ﬁxed), ν∗ = 1.5, σ∗
0.488
0.473
0.531
0.809
1.638

2.654
2.806
2.903
2.957
2.979

0.153
0.099
0.069
0.041
0.022

0.018
0.014
0.326
0.630
0.813

ν

sd(ν)

1 = 1(ﬁxed)

Table 2: The mean value with the standard deviation of estimates in a 100-run experiment on
a two-component Weibull-log normal mixture.

24

Figure 2: The Weibull mixtures, see table (1)

Nb of observations

λ

sd(λ)

ν

sd(ν)

µ

sd(µ)

Mixture 1 : n = 103, λ∗ = 0.3, ν∗ = 1.5, σ∗

SEM

Robin

Pearson’s χ2

Song EM-type

0.308
0.296
0.291
0.230
0.284

Song π−maximizing

0.017
0.015
0.015
0.022
0.041
Mixture 2 : n = 104, λ∗ = 0.1, ν∗ = 1, σ∗
0.006
0.003
0.004
0.004
0.005

Song π−maximizing
Mixture 3 : n = 104, λ∗ = 0.05, ν∗ = 1, σ∗

0.103
0.095
0.102
0.100
0.085

Pearson’s χ2

Robin

Song EM-type (true init)

Song EM-type (0.2,1.5 init)

Pearson’s χ2

Song EM-type (0.2,1.5 init)

Song π−maximizing

0.052
0.050
0.042

0.004
0.003
0.003

Mixture 4 : n = 5 × 104, λ∗ = 0.05, ν∗ = 0.4, σ∗

Pearson’s χ2
Song EM-type

Song π−maximizing

0.049
0.064
0.024

0.002
0.001
0.001

0.624
0.068 —
0.087 —
0.251 —
0.263 —

1 = 1(ﬁxed), µ∗ = 3, σ∗
1.484
3.002
1.557
1.614
1.662
1.570
1 = 1(ﬁxed), µ∗ = 3, σ∗
3.001
1.284
1.049
0.941
0.894
1.024
1 = 1(ﬁxed), µ∗ = 3, σ∗
1.312
3.001
0.855
1.013
1 = 1(ﬁxed), µ∗ = 3, σ∗
0.629
0.345
0.773

0.677
0.031 —
0.037 —
0.039 —
0.055 —

0.438
0.004 —
0.010 —

0.703
0.068 —
0.052 —

3.001

0.026

—
—
—
—

—
—
—
—

0.006

—
—

0.004

—
—

2 = 0.5(ﬁxed)

2 = 0.5(ﬁxed)
0.007

2 = 0.5(ﬁxed)

2 = 0.5(ﬁxed)

Table 3: The mean value with the standard deviation of estimates in a 100-run experiment on
a two-component Weibull-log normal mixture.

25

Figure 3: The Weibull – Lognormal mixtures, see tables (2,3)

26

5.3 Two-sided Weibull Gaussian mixture

The (symmetric) two-sided Weibull distribution can be considered as a generalization of the
Laplace distribution and can be deﬁned through either its density or its distribution function as
follows:

(cid:18)|x|

(cid:19)ν−1

(cid:17)ν

−(cid:16) |x|

σ

e

f (x|ν, σ) =

1
2

σ
ν

σ

(cid:40)

,

F(x|ν, σ) =

1 − 1

2 e−( x
σ )ν
e−( −x
σ )ν

x ≥ 0
x < 0

We can also deﬁne a skewed form of the two-sided Weibull by attributing diﬀerent scale and
shape parameters to the positive and negative parts and then normalizing in a suitable way so
that f (x) integrates to one; see Chen and Gerlach [2013]. Figure (4) shows several examples of
a mixture of a normal distribution N (µ = 0, σ = 0.5) and a two-sided Weibull distribution with
shape ν and a scale σ, see table (4) for the corresponding values of the scale and the shape of
each mixture. The moments of this symmetric form are given by:

E[X 2k] = σ2kΓ(1 + 2k/ν)

E[X 2k+1] = 0,∀k ∈ N.

We simulate diﬀerent samples from a two component mixture with a parametric component f1
a Gaussian N (µ = 0, σ = 0.5) and a semiparametric component f0 a (symmetric) two-sided
Weibull distribution with parameters ν ∈ {3, 2.5, 1.5} and a scale σ0 ∈ {1.5, 2}. We perform
diﬀerent experiments to estimate the proportion and the mean of the parametric part ( the
Gaussian) and the shape of the semiparametric component. The values of the scale of the two
components are considered to be known during estimation. We consider the following two sets
of constraints:
M1:3 =

f0(x)dx = 1, Ef0[X] = 0, Ef0[X 2] = σ2

0Γ(1 + 2/ν), Ef0[X 3] = 0, ν > 0

f0 :

M2:4 =

f0 :

f0(x)dx = 1, Ef0[X 2] = σ2

0Γ(1 + 2/ν), Ef0[X 3] = 0,

(cid:26)
(cid:26)

(cid:90)
(cid:90)

R

R

(cid:27)
0Γ(1 + 4/ν), ν > 0(cid:9) .

;

Ef0[X 4] = σ4

The ﬁrst set imposes that the semiparametric component is centered around zero whereas the
second one does not impose it.
The ﬁrst set of constraints is not really suitable for estimation especially when the number of
observations is high enough. The reason is simple and is based on the original idea behinde
our procedure, see paragraph 3.1. The ﬁrst and the third moment constraints are practically
1−λ f1(x|θ) verifying
the same constraint. Indeed, the number of models of the form 1
the constraints of M1:3 is inﬁnite because the ﬁrst and the third constraints give rise to the
following equations:

1−λ f (.) − λ

λµ(cid:0)µ2 + 3σ2

λµ = 0

(cid:1) = 0.

1

The zero in the right hand side comes from the fact that the ﬁrst and the third true moments
of the whole mixture are zero. These are two equations in λ and µ since σ1 is supposed to
be known. This implies that theoretically, there is an inﬁnite number of models of the form
1−λ f (.)− λ
1−λ f1(x|θ) in the intersection N ∩ M1:3. Still, the empirical version of these equations

1

is

n(cid:88)
n(cid:88)

i=1

i=1

Xi;

X 3
i .

λµ =

λµ(cid:0)µ2 + 3σ2

(cid:1) =

1

1
n

1
n

27

Figure 4: Mixtures of two-sided Weibull – Gaussian with low and high proportion of the para-
metric part. See table (4)

As the number of observations is very small, the right hand side of both equations is biased
enough from zero and it is highly possible that the number of solutions becomes not only ﬁnite
but reduced to one. As the number of observations increases, the law of large numbers implies
directly that the right hand side becomes arbitrarily close to zero and the set of solutions
becomes inﬁnite. This is exactly what happened in the simulation results in table (4) below.
The algorithm favored the value zero for the estimate of the proportion as the true proportion
of the parametric component became close to zero, whereas the estimates of the mean took
values very dispersed centered around zero but with a high standard deviation. It is clear in the
results that our method outperforms other semiparametric algorithms without prior information
especially when the proportion of the parameteric component is low.

28

Bordes symmetry Triangular Kernel
Bordes symmetry Gaussian Kernel

sd(µ2) =0.398
sd(µ2) =0.203

sd(µ2) =0.393
sd(µ2) =0.350

sd(µ2) =0.496
sd(µ2) =0.470

Pearson’s χ2 under M1:3
Pearson’s χ2 under M2:4

Bordes symmetry Triangular Kernel
Bordes symmetry Gaussian Kernel

EM-type Song et al.

π−maximizing Song et al.

EM-type Song et al.

π−maximizing Song et al.

Estimation method

Pearson’s χ2 under M1:3
Pearson’s χ2 under M2:4

λ

Robin et al.

Robin et al.

0.713
0.764
0.309
0.211
0.488
0.762
0.717
0.539

0.333
0.407
0.272
0.206
0.203
0.494
0.384
0.263

sd(λ)
Mixture 1 :n = 100 λ∗ = 0.7, µ∗ = 0, σ∗
0.064
0.067
0.226
0.133
0.137
0.040
0.156
Stochastic EM
0.083
Mixture 2 :n = 100 λ∗ = 0.3, µ∗ = 0, σ∗
0.079
0.077
0.119
0.104
0.078
0.035
0.129
Stochastic EM
0.040
Mixture 3 :n = 300 λ∗ = 0.2, µ∗ = 0, σ∗
0.058
0.055
0.108
0.096
0.068
0.023
0.020
Stochastic EM
0.057
Mixture 4 :n = 105 λ∗ = 0.2, µ∗ = 0, σ∗
0.010
0.004
0.012
0.002
Mixture 5 :n = 105 λ∗ = 0.2, µ∗ = 0, σ∗
0.030
0.013
0.002
Mixture 6 :n = 105 λ∗ = 0.1, µ∗ = 0, σ∗
0.013
0.014
Mixture 7 :n = 105 λ∗ = 0.05, µ∗ = 0, σ∗
0.033
0.013

0.200
0.252
0.439
0.414
0.278
0.461
0.362
0.292

0.161
0.203
0.325
0.251

0.015
0.213
0.397

0.0097
0.116

Robin et al.

0.005
0.066

Pearson’s χ2 under M1:3
Pearson’s χ2 under M2:4

EM-type Song et al.

Pearson’s χ2 under M1:3
Pearson’s χ2 under M2:4
Pearson’s χ2 under M1:3
Pearson’s χ2 under M2:4

Pearson’s χ2 under M1:3
Pearson’s χ2 under M2:4
π−maximizing Song et al.

EM-type Song et al.

EM-type Song et al.

π−maximizing Song et al.

Pearson’s χ2 under M1:3
Pearson’s χ2 under M2:4

Bordes symmetry Triangular Kernel
Bordes symmetry Gaussian Kernel

µ

sd(µ)

2 = 0.5(ﬁxed), ν∗ = 3, σ∗

ν

-0.0003
-0.012
0.240
0.106
-0.005
-0.005
-0.161
-0.005

0.001
0.012
0.773
0.855
-0.109
-0.132
0.014
-0.062

1 = 1.5(ﬁxed)
4.315
2.893

0.085
0.342
0.609 µ2 = −0.220
0.533 µ2 = −0.035
0.114
0.092
2.301
0.112

—
—
—
—

1 = 1.5(ﬁxed)
4.243
2.925

0.316
0.575
0.947 µ2 = −0.430
0.911 µ2 = −0.308
0.947
0.806
1.321
0.646

—
—
—
—

2 = 0.5(ﬁxed), ν∗ = 3, σ∗

2 = 0.5(ﬁxed), ν∗ = 3, σ∗

1 = 1.5(ﬁxed)
4.058
2.932

0.004
0.069
-0.972
-0.928
-0.062
0.162
0.025
0.118

0.215
0.573
0.328
µ2 =1.036
0.289 µ2 = −1.125
1.253
1.128
1.224
1.027

—
—
—
—
2 = 0.5(ﬁxed), ν∗ = 2.5, σ∗
1 = 2(ﬁxed)
3.874
2.492

—
—
2 = 0.5(ﬁxed), ν∗ = 1.5, σ∗
1 = 2(ﬁxed)
2.150
1.494

0.019
0.213
1.469
1.592

2.381
0.436
0.021

-0.002
-0.018
-0.061
-0.158

0.203
-0.004
0.001

—
2 = 0.5(ﬁxed), ν∗ = 1.5, σ∗
1 = 2(ﬁxed)
1.712
1.492

0.344
-0.004

2.754
0.666

2 = 0.5(ﬁxed), ν∗ = 1.5, σ∗

1 = 2(ﬁxed)

-0.105
-0.036

2.693
0.857

1.581
1.493

sd(ν)

0.118
0.731

—
—
—
—

0.442
0.454

—
—
—
—

0.684
0.200

—
—
—
—

0.661
0.012

—
—

0.138
0.009

—

0.042
0.008

0.056
0.008

Table 4: The mean value with the standard deviation of estimates in a 100-run experiment on
a two-component two-sided Weibull–Gaussian mixture.

29

5.4 Bivariate Gaussian mixture

We generate 1000 i.i.d. observations from a bivariate Gaussian mixture with proportion λ = 0.7
for the parametric component. The parametric component is a bivariate Gaussian with mean
(0,−1) and covariance matrix I2. The unknown component is a bivariate Gaussian with mean
(3, 3) and covariance matrix:

(cid:18) σ∗

2

2

ρ∗

(cid:19)

ρ∗
σ∗

2

2

Σ2 =

,

σ∗

2

2 = 0.5,

ρ∗ ∈ {0, 0.25}

In a ﬁrst experiment, we suppose that we know the whole parametric component, and that the
unknown component belongs to the set M1

(cid:26)(cid:90)

R2

M1 =

(cid:90)

R2

(cid:90)

R2

(cid:27)

f0(x, y)dxdy = 1,

xf0(x, y)dxdy =

yf0(x, y)dydx = θ,

θ ∈ R

.

We suppose that the only unknown parameters are the center of the unknown cluster (θ, θ) and
the proportion of the parametric component.
In a second experiment, we supposed that the center of the parametric component is unknown
but given by (µ, µ − 1) for some unknown µ ∈ R. The set of constraints is now replaced with
M2 given by

(cid:90)

(cid:90)

(cid:26)(cid:90)

M2 =

f0(x, y)dxdy = 1,

R2

xf0(x, y)dxdy =

R2

R2
xyf0(x, y)dxdy = θ2 + ρ∗, θ ∈ R

yf0(x, y)dydx = θ,

(cid:27)

.

(cid:90)

R2

The covariance between the two coordinates ρ∗ in the unknown component is supposed to be
known. We tested two values for ρ∗; 0 and 0.25, see ﬁgure (5) for a sample of points for each
value of the covariance parameter.
Although existing methods were only proposed for univariate cases, we see no problem in using
them in multivariate cases without any changes. The only method which cannot be used directly
is the method of Bordes and Vandekerkhove [2010] because it is based on the symmetry of the
density function, so it remained out of the competition.
For methods which use a kernel estimator, we used a kernel estimator for each coordinate of the
random observations, i.e. Kwx,wy (x, y) = Kwx(x)Kwy (y). The EM-type algorithm of Song et al.
[2010] performs as good as our algorithm. The SEM algorithm of Bordes et al. [2007] gives also
good results. The algorithm of Robin et al. [2007] and the π−maximizing algorithm of Song
et al. [2010] failed to give satisfactory results.

30

Figure 5: The two bivariate Gaussian mixtures.

Pearson’s χ2 under M1
Pearson’s χ2 under M2

SEM (unif init, no costr mu)

Robin

Song EM (unif init, no costr mu)

Song π−maximizing

Estimation method

λ

sd(λ)

µ

sd(µ)

θ

Mixture 2 : ρ∗ = 0 and µ1 = (µ, 1 − µ) is unknown
2.854
3.034

0.035
0.043 µ1,2 = -0.880
0.212 µ1,2 = -0.221
0.038 µ1,2 = -0.996
1.810 µ1,2 = -0.463
Mixture 3 : ρ∗ = 0.25 and µ1 = (µ, 1 − µ) is unknown

0.680
0.694
0.724
0.954
0.697
0.114

0.027 —
0.019
0.015
0.064
0.014
0.297

0.016
0.090
0.779
0.003
0.538

—

sd(θ)

0.233
0.045

sd(µ1,2) =0.053
sd(µ1,2) =0.218
sd(µ1,2) =0.039
sd(µ1,2) =1.810

Pearson’s χ2

SEM(unif init, no costr mu)

Robin

Song EM (unif init, no costr mu)

Song π−maximizing

0.704
0.730
0.890
0.704
0.095

0.026
0.016
0.025
0.015
0.268

0.033
0.083
0.566
0.016
0.564

3.071

0.060
µ1,2 =-0.878
0.052
0.117 µ1,2 = -0.434
0.047 µ1,2 = -0.973
1.606 µ1,2 = -0.436

0.101

sd(µ1,2) =0.055
sd(µ1,2) =0.117
sd(µ1,2) =0.040
sd(µ1,2) =1.606

Table 5: The mean value with the standard deviation of estimates in a 100-run experiment on
a two-component bivariate normal mixture.

31

References

S. M. Ali and S. D. Silvey. A General Class of Coeﬃcients of Divergence of One Distribution from
Another. Journal of the Royal Statistical Society. Series B (Methodological), 28(1):131–142,
1966.

Rudolf Beran. Minimum hellinger distance estimates for parametric models. Ann. Statist., 5(3):

445–463, 05 1977.

L. Bordes and P. Vandekerkhove. Semiparametric two-component mixture model with a known
component: An asymptotically normal estimator. Mathematical Methods of Statistics, 19(1):
22–41, 2010. ISSN 1066-5307.

Laurent Bordes, C´eline Delmas, and Pierre Vandekerkhove. Semiparametric estimation of a two-
component mixture model where one component is known. Scandinavian Journal of Statistics,
33(4):733–752, 2006.

Laurent Bordes, Didier Chauveau, and Pierre Vandekerkhove. A stochastic {EM} algorithm for
a semiparametric mixture model. Computational Statistics and Data Analysis, 51(11):5429 –
5443, 2007. ISSN 0167-9473. Advances in Mixture Models.

Michel Broniatowski and Amor Keziou. Parametric estimation and tests through divergences

and the duality technique. J. Multivariate Anal., 100(1):16–36, 2009.

Michel Broniatowski and Amor Keziou. Divergences and duality for estimation and test under
moment condition models. Journal of Statistical Planning and Inference, 142(9):2554 – 2573,
2012.

Qian Chen and Richard H. Gerlach. The two-sided weibull distribution and forecasting ﬁnancial

tail risk. International Journal of Forecasting, 29(4):527 – 540, 2013.

Imre Csisz´ar. Eine informationstheoretische Ungleichung und ihre Anwendung auf den Beweis
der Ergodizit¨at von Markoﬀschen Ketten. Magyar Tud. Akad. Mat. Kutat´o Int. K¨ozl., 8:
85–108, 1963.

Alexis Decurninge. Geometric Science of Information: First International Conference, GSI
2013, Paris, France, August 28-30, 2013. Proceedings, chapter Estimation and Tests Under
L-Moment Condition Models, pages 459–466. Springer Berlin Heidelberg, Berlin, Heidelberg,
2013.

Alexis Decurninge. Univariate and multivariate quantiles, probabilistic and statistical ap-
proaches; radar applications. Theses, Universit´e Pierre et Marie Curie, January 2015. URL
https://hal.inria.fr/tel-01129961.

Mireille El Gheche. Proximal methods for convex minimization of Phi-divergences : applica-
tion to computer vision. Theses, Universit´e Paris-Est, May 2014. URL https://pastel.
archives-ouvertes.fr/tel-01124306.

Jing Qin Jiahua Chen. Empirical likelihood estimation for ﬁnite populations and the eﬀective

usage of auxiliary information. Biometrika, 80(1):107–116, 1993.

R.J. Karunamuni and J. Wu. Minimum hellinger distance estimation in a nonparametric mixture

model. Journal of Statistical Planning and Inference, 139(3):1118 – 1133, 2009.

Amor Keziou. Utilisation des Divergences entre Mesures en Statistique Inf´erentielle. The-
ses, Universit´e Pierre et Marie Curie - Paris VI, November 2003. URL https://tel.
archives-ouvertes.fr/tel-00004069. Patrice Bertail (rapporteur), Denis Bosq (p´esident),
Michel Delecroix, Dominique Picard, Ya’acov Ritov (rapporteur), Christian P. Robert, Jean-
Michel Zakoian.

32

Kenneth Lange. Optimization. Springer Texts in Statistics. Springer-Verlag New York, 2 edition,

2013.

Friedrich Liese and Igor Vajda. Convex statistical distances, volume 95 of Teubner-Texte zur
Mathematik [Teubner Texts in Mathematics]. BSB B. G. Teubner Verlagsgesellschaft, Leipzig,
1987. ISBN 3-322-00428-7. With German, French and Russian summaries.

Bruce G. Lindsay. Eﬃciency versus robustness: The case for minimum hellinger distance and

related methods. Ann. Statist., 22(2):1081–1114, 06 1994.

Tzon-Tzer Lu and Sheng-Hua Shiou. Inverses of 2 × 2 block matrices. Computers & Mathematics
with Applications, 43(1-2):119 – 129, 2002. ISSN 0898-1221. doi: http://dx.doi.org/10.1016/
S0898-1221(01)00278-4.

Jun Ma, Sigurbjorg Gudlaugsdottir, and Graham Wood. Generalized em estimation for semi-
parametric mixture distributions with discretized non-parametric component. Statistics and
Computing, 21(4):601–612, 2011. ISSN 0960-3174.

Geoﬀrey McLachlan and David Peel. Finite Mixture Models. John Wiley & Sons, Inc., 2005.

J. A. Nelder and R. Mead. A simplex method for function minimization. Computer Journal, 7:

308–313, 1965.

Whitney K. Newey and Richard J. Smith. Higher order properties of gmm and generalized
empirical likelihood estimators. Econometrica, 72(1):219–255, 2004. ISSN 1468-0262. doi:
10.1111/j.1468-0262.2004.00482.x. URL http://dx.doi.org/10.1111/j.1468-0262.2004.
00482.x.

Art Owen. Empirical likelihood ratio conﬁdence regions. The Annals of Statistics, 18(1):90–120,

1990.

Leandro Pardo. Statistical inference based on divergence measures, volume 185 of Statistics:
ISBN 978-1-

Textbooks and Monographs. Chapman & Hall/CRC, Boca Raton, FL, 2006.
58488-600-6; 1-58488-600-5.

Chanseok Park and Ayanendranath Basu. Minimum disparity estimation : Asymptotic normal-

ity and breakdown point results. Bulletin of informatics and cybernetics, 36:19–33, 2004.

R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for

Statistical Computing, Vienna, Austria, 2015. URL http://www.R-project.org/.

St´ephane Robin, Avner Bar-Hen, Jean-Jacques Daudin, and Laurent Pierre. A semi-parametric
approach for mixture models: Application to local false discovery rate estimation. Computa-
tional Statistics and Data Analysis, 51(12):5483 – 5493, 2007. ISSN 0167-9473.

Seongjoo Song, Dan L. Nicolae, and Jongwoo Song. Estimating the mixing proportion in a
semiparametric mixture model. Computational Statistics and Data Analysis, 54(10):2276 –
2283, 2010. ISSN 0167-9473.

Qingguo Tang and Rohana J. Karunamuni. Minimum distance estimation in a ﬁnite mixture

regression model. Journal of Multivariate Analysis, 120:185 – 204, 2013.

D.M. Titterington, A.F.M. Smith, and U.E. Makov. Statistical Analysis of Finite Mixture Dis-

tributions. Wiley, New York, 1985.

A.W. Van Der Vaart. Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic

Mathematics, 3. Cambridge University Press, 1998.

Sijia Xiang, Weixin Yao, and Jingjing Wu. Minimum proﬁle hellinger distance estimation for a

semiparametric mixture model. Canadian Journal of Statistics, 42(2):246–267, 2014.

33

