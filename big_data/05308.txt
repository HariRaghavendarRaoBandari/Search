6
1
0
2

 
r
a

 

M
6
1

 
 
]

.

R
P
h
t
a
m

[
 
 

1
v
8
0
3
5
0

.

3
0
6
1
:
v
i
X
r
a

DEVIATION OF POLYNOMIALS FROM THEIR EXPECTATIONS AND

ISOPERIMETRY

LAVRENTIN M. ARUTYUNYAN AND EGOR D. KOSOV

Abstract. In the ﬁrst part we study deviation of a polynomial from its mathematical ex-
pectation. This deviation can be estimated from above by Carbery–Wright inequality, so we
investigate estimates of the deviation from below. We obtain such estimates in two diﬀerent
for Gaussian measures and a polynomial of an arbitrary degree and for an arbitrary
cases:
log-concave measure but only for polynomials of the second degree.
In the second part we
deals with isoperimetric inequality and the Poincar´e inequality for probability measures on
the real line that are images of the uniform distributions on convex compact sets in Rn under
polynomial mappings.

Polynomials on spaces with log-concave measures possess a number of important and useful
properties. These properties have been studied in many works, see for example [1, 4, 5, 13, 21].
Some authors (see [8, 9, 22, 23]) also consider the special case of Gaussian measures.

Results about polynomial distributions ﬁnd many applications in various ﬁelds. One of
such applications is concerned with geometrical properties of convex bodies, especially when
the dimension tends to inﬁnity. For example, Bourgain [13] proved an upper bound in the
hyperplane conjecture by using a Khinchin-type inequality for polynomials of a ﬁxed degree on
convex bodies (about this inequality see also [4, 5]). It should be remarked that the best known
upper bound in the hyperplane conjecture is due to Klartag (see [19]). On the other hand,
properties of polynomials and polynomial distributions play an important role in probabilistic
questions.
In particular, Gaussian measures are also logarithmically concave and there are
certain properties of Gaussian measures that were proved only in the framework of general
logarithmically concave measures. One of them is the following Carbery–Wright inequality
that holds for any polynomial f of degree d and any log-concave measure µ on Rn (see [14]):

kf k1/d

L1(µ)µ(x :

|f (x)| ≤ α) ≤ C(d)α1/d.

This inequality has already found interesting applications in probability theory (see, for exam-
ple, [22, 23]).

In the ﬁrst part of this work we discuss inequalities that are reverse in some sense to the

Carbery–Wright inequality, more precisely, inequalities of the form

µ(|f − mf | ≤ σf s) ≥ C(d)ϕ(s),

where f is a polynomial of degree d and mf and σ2
f are its expectation and variance, respectively,
and s ∈ [0, 1/2]. We prove inequalities of such a type in two diﬀerent cases. In the ﬁrst case,
the measure µ is Gaussian and ϕ(s) = s| ln s|−d/2 (see Theorem 2.3). In the second case, the
measure µ is an arbitrary log-concave measure and the degree of the polynomial f is at most two
while ϕ(s) = s (see Corollary 2.8). The main feature of our inequalities is their independence
of the dimension of the space and of the measure µ itself.

2010 Mathematics Subject Classiﬁcation. 60E05, 60E15, 52A20, 28C20.
Key words and phrases. Logarithmically concave measure, Gaussian measure, distribution of a polynomial,

Carbery–Wright inequality, isoperimetric inequality, Poincar´e inequality.

This work has been supported by the Russian Science Foundation Grant 14-11-00196 at Lomonosov Moscow

State University.

1

2

L.M. ARUTYUNYAN AND E.D. KOSOV

The second part of our work is devoted to the isoperimetric inequality and the Poincar´e
inequality for distributions of polynomials. It is well-known that for the standard Gaussian
measure on Rn both inequalities hold true (see [6, 24, 12]). For log-concave measures, inequal-
ities of these types have been studied in [3, 18]. In our work, these inequalities are proved for
probability measures on the real line that are polynomial images of the uniform distributions
on convex compact sets in Rn (see Theorem 3.5 and Corollaries 3.6 and 3.7).

One of the main tools used in this paper is the so-called localization technique (see [18, 20]).
The idea of localization of a problem was used in many papers as an approach for obtaining
estimates in multidimensional spaces. For example, it was used to study isoperimetric inequal-
ities for the uniform distributions on convex bodies in [18] and for the distributions on spheres
in [17]. Also it was used in [5, 4] in the proof of Khinchine-type inequalities for polynomials.
This technique allows to reduce some multidimensional inequalities to one-dimensional ones.
In the case of polynomials it is especially convenient, because a restriction of a polynomial to a
straight line is again a polynomial. A new approach to the ideas of localization was developed
in [16], where localization is interpreted as a property of extreme points of some special convex
sets in the space of all probability measures.

1. Preliminaries

First of all we introduce some notation and mention ceratin previously known results used

in our work.

Let µ be a probability Borel measure on Rn and let f be a µ-measurable function. We use

the following notation:

µf = µ ◦ f −1 is the image of the measure µ under the mapping f,

mf =Z f dµ is the expectation of the random variable f,
f =Z (f − mf )2dµ is the variance of the random variable f,

σ2

αf =Z |f − mf |dµ,

kf k0 = exp(cid:18)Z ln |f |dµ(cid:19) = lim

r→0

kf kr.

kf kp =(cid:18)Z |f |pdµ(cid:19)1/p

for p > 0,

Let IA denote the indicator function of a set A.
A probability Borel measure µ on Rn is called logarithmically concave (also log-concave or
convex) if it has a density of the form e−V with respect to Lebesgue measure on some aﬃne
subspace, where V is a convex function (possibly with inﬁnite values) on this subspace (see [7]).
This properety is equivalent (see [10, 11]) to the property that for every pair of Borel sets A, B
one has

A polynomial of degree d is a function f on Rn of the form

µ(tA + (1 − t)B) ≥ µ(A)tµ(B)1−t, ∀ t ∈ [0, 1].

f (x) =

dXm=0

Bm(x, . . . , x),

where B(x1, . . . , xm) is a symmetric m-linear function.

Let ν be a probability Borel measure on the real line. Deﬁne the ν-perimeter of a set A by

the following formula:

ν+(A) = lim inf
ε→0

ν(A + (−ε, ε)) − ν(A)

ε

.

The proofs of our main results use the following known facts.

DEVIATION OF POLYNOMIALS FROM THEIR EXPECTATIONS AND ISOPERIMETRY

3

Theorem 1.1 (see [21]). Let µ be a log-concave measure on Rn and let f be a polynomial of
degree d. Set k(f ) = inf{k : µ({|f | ≥ k}) ≤ 1/e}. Then for every t ≥ 1 one has

µ(x :

|f (x)| ≥ (4t)dk(f )) ≤ e−t.

Theorem 1.2 (see [4, 5]). Let µ be a log-concave measure on Rn, q ≥ 1. Then there is an
absolute constant c such that for every polynomial f of degree d the following inequalities hold
true:

kf kq ≤ (cqd)dkf k0,

kf kq ≤ (cq)dkf k1.

Some inﬁnite-dimensional analogues of these two theorems are presented in [1].

Theorem 1.3 (see [1]). Let µ be a log-concave measure on Rn and let U be a set of positive
µ-measure. Then there is an absolute constant C such that, for every polynomial f of degree d,
the following estimate holds:

µ(U)d+1Z |f |dµ ≤ (Cd)2dZU

|f |dµ.

Let us recall two localization lemmas. The ﬁrst one is concerned with log-concave measures.

Theorem 1.4 (see [16, 18, 20]). Let f1, f2 be a pair of two upper semi-continuous nonnegative
functions on Rn and let f3, f4 be a pair of two lower semi-continuous nonnegative functions
on Rn. Suppose that for every compact interval ∆ = [a, b] ⊂ Rn and every measure ν with a
density of the form eℓ with respect to Lebesgue measure on ∆, where ℓ is an aﬃne function on
∆, one has

Then the following inequality holds for every log-concave measure µ on Rn:

f1dν(cid:19)α(cid:18)Z∆

f2dν(cid:19)β
(cid:18)Z∆
(cid:18)Z f1dµ(cid:19)α(cid:18)Z f2dµ(cid:19)β

.

f3dν(cid:19)α(cid:18)Z∆

f4dν(cid:19)β
≤(cid:18)Z∆
≤(cid:18)Z f3dµ(cid:19)α(cid:18)Z f4dµ(cid:19)β

.

The second localization lemma is applicable in the case of uniform distributions on convex

bodies.

Theorem 1.5 (see [16, 18, 20]). Let f1, f2 be a pair of two upper semi-continuous nonnegative
functions on Rn and let f3, f4 be a pair of two lower semi-continuous nonnegative functions
on Rn. Suppose that for every compact interval ∆ = [a, b] ⊂ Rn and every measure ν with a
density of the form (αt + β)n−1 with respect to Lebesgue measure on ∆ one has

Then the following inequality holds for every convex body K in Rn:

(cid:18)Z∆
(cid:18)ZK

f1dν(cid:19)α(cid:18)Z∆
f1dλ(cid:19)α(cid:18)ZK

f2dν(cid:19)β

f2dλ(cid:19)β

≤(cid:18)Z∆
≤(cid:18)ZK

f3dν(cid:19)α(cid:18)Z∆
f3dλ(cid:19)α(cid:18)ZK

f4dν(cid:19)β

.

f4dλ(cid:19)β

,

where λ is Lebesgue measure.

2. Behavior of the distribution of a polynomial in a neighbourhood of its

expectation

In this section we estimate from below the measure of small deviations of a polynomial from
its mean. Firstly, we need to establish the following important property of the expectation of
a polynomial.

4

L.M. ARUTYUNYAN AND E.D. KOSOV

Lemma 2.1. Let µ be a log-concave measure on Rn. Then there are positive constants c(d) and
s(d) depending only on the degree d such that for every polynomial f of degree d with αf > 0
and mf = 0 the following estimate holds true:

µ(f ≥ εαf ) ≥ c(d)

for every number ε ∈ [0, s(d)].

Proof. Let δ = µ(f ≥ εαf ). Applying the inequality from Theorem 1.3 to the set U = {f <
εαf } we get

(1 − δ)d+1ZRn

|f |dµ = µ(f < εαf )d+1ZRn
≤ (Cd)2dZ{f <εαf }

|f |dµ

= (Cd)2d(cid:18)Z{0<f <εαf }

|f |dµ = (Cd)2d(cid:18)Z{0<f <εαf }
|f |dµ +Z{f <0}
|f |dµ(cid:19)
|f |dµ +Z{f >0}
= (Cd)2d(cid:18)2Z{0<f <εαf }

|f |dµ(cid:19)

|f |dµ +Z{f ≥εαf }

|f |dµ(cid:19).

|f |dµ from above. For this purpose we use Theorem 1.1. Let τ > 1

(this number will be chosen later). Then

We now estimate Z{f ≥εαf }
Z{f ≥εαf }

|f |dµ

≤Z{4dk(f )τ >f ≥εαf }

|f |dµ +Z{|f |≥4dk(f )τ }
= 4dk(f )τ δ + d4dk(f )Z ∞

τ 1/d

|f |dµ ≤ 4dk(f )τ δ +Z ∞

4dk(f )τ

µ{|f | > t}dt

λd−1µ{|f | > (4λ)dk(f )}dλ

≤ 4dk(f )τ δ + d4dk(f )Z ∞

τ 1/d

λd−1e−λdλ.

|f |dµ, the last expression can be estimated from above by

Since k(f ) ≤ eZE

Thus, using the estimate

we obtain the inequality

4deZRn
Z{0<f <εαf }

|f |dµ(τ δ + dZ ∞

τ 1/d

λd−1e−λdλ).

|f |dµ ≤ s(d)ZRn

|f |dµ,

(1 − δ)d+1 ≤ (Cd)2d(cid:18)2s(d) + e4d(cid:0)τ δ + dZτ 1/d

λd−1e−λdλ(cid:1)(cid:19).

Now set τ = δ−1/2. Then, for any δ close enough to 0, the left-hand side of the inequality is
close to 1 and the right-hand side is close to 2(Cd)2ds(d). So, if we take s(d) < 2−1(Cd)−2d, we
obtain that δ cannot be arbitrary small. Thus, the desired constant c(d) exists and the lemma
is proved.
(cid:3)

Corollary 2.2. Let µ be a log-concave measure on Rn. Then there is a constant c(d) ∈ (0, 1)
depending only on the degree d such that for every polynomial f of degree d with σf > 0 one
has

1 − c(d) ≥ µ(f < mf ) ≥ c(d).

DEVIATION OF POLYNOMIALS FROM THEIR EXPECTATIONS AND ISOPERIMETRY

5

A. The case of a Gaussian measure

First we consider the case of the standard Gaussian measure on Rn and a polynomial of an

arbitrary degree.

Below we need the following elementary estimate. Let f be a smooth function on Rn. Let

Dmf (x) denote the m-fold derivative of f , i.e., a multilinear function such that

dm
dtm f (x + th)|t=0 = Dmf (x)(h, . . . , h).

Let now f be a polynomial of degree d on Rn. Set

ϕ(t) = f (x + t(y − x)).

Then one has

f (y) = ϕ(1) = ϕ(0) +

dXm=1

(m!)−1ϕ(m)(0) = f (x) +

dXm=1

(m!)−1Dmf (x)(y − x, . . . , y − x),

thus,

f (y) − f (x) ≤

dXm=1

(m!)−1|Dmf (x)||y − x|m,

where |Dmf (x)|2 =Pi1,...,im |∂xi1 ,...,xim f (x)|2. Applying this estimate to the polynomial −f , we

get

(2.1)

|f (y) − f (x)| ≤

dXm=1

(m!)−1|Dmf (x)||y − x|m.

We also need the following estimate for the standard Gaussian measure γ on Rn:

(2.2)

kDmf k2

2 :=Z |Dmf |2dγ ≤ a(m, d)σ2

f ,

where the constant a(m, d) depends only on m and d and is independent of the dimension n.
This estimate follows from the equivalence of the Sobolev and Lp-norms on the space of all
polynomials of a ﬁxed degree (see [6]).

Let us recall the isoperimetric inequality for Gaussian measures (see [24, 12, 6]), which is
used in the proof of the next theorem. Let γ be the standard Gaussian measure on Rn, let C
be a measurable set in Rn, and let B be the closed unit ball centered at the origin. Then

(2.3)

γ(C + εB) ≥ Φ(a + ε),

where Φ is the distribution function of the standard Gaussian random variable on the real line
and the number a is chosen from the relation γ(C) = Φ(a).

For a Gaussian measure, there is a better estimate for the tails of a polynomial f of degree

d than in Theorem 1.1:

(2.4)

γ(|f | > kf k2td) ≤ R exp(cid:0)−rt2(cid:1)

for every t > 0 and some positive constants R and r (see [6, Corollary 5.5.7]).

Theorem 2.3. Let γ be the standard Gaussian measure on Rn. Then there is a number
L(d) > 0 depending only on the degree d such that for every polynomial f of degree d the
following estimate holds:

γ(|f − mf | ≤ σf s) ≥ L(d)s| ln s|−d/2

for 0 ≤ s ≤ 1/2.

6

L.M. ARUTYUNYAN AND E.D. KOSOV

Proof. Without loss of generality we can assume that mf = 0. Fix a number t > 1 (this number
will be chosen later). Let B be the unit ball in Rn centered at the origin. For ε < 1 consider
the set

A = ({f < 0} + εB)\({f > 0} + εB)\ d\m=1(cid:8)|Dmf (x)| ≤ td−mkDmf k2(cid:9).

Note that by (2.1) we have A ⊂(cid:8)|f | ≤ Pd

we have εm ≤ ε, td−m ≤ td, and inequality (2.2) yields the estimate

m=1(m!)−1td−mkDmf k2εm(cid:9). Since t > 1 and ε < 1,

dXm=1

(m!)−1td−mkDmf k2εm ≤ C(d)σf tdε,

where C(d) is a constant. Thus, we have the inclusion

A ⊂ {|f | ≤ C(d)σf tdε}.

Let af be the number such that γ(f < 0) = Φ(af ). Using the Gaussian isoperimetric inequality
(2.3), we can estimate the measure of the set A:

γ(A) ≥ 1 − (1 − γ({f < 0} + εB)) − (1 − γ({f > 0} + εB))

−

dXm=1

γ{|Dmf (x)| ≥ td−mkDmf k2}

≥ −1 + Φ(af + ε) + Φ(−af + ε) − R

dXm=1

e−rt2

= Φ(af + ε) − Φ(af ) + Φ(−af + ε) − Φ(−af ) − Rde−rt2

.

In the last inequality Theorem 1.1 is used.

By Corollary 2.2, there is a number c(d) such that

1 − c(d) ≥ γ(f < mf ) ≥ c(d),

so, there is a constant a(d) such that for every polynomial f of degree d with zero expectation
one has |af | ≤ a(d). Let q(d) be the minimal value of the standard Gaussian density on the
interval [−a(d) − 1, a(d) + 1]. Then Φ(a + ε) − Φ(a) ≥ q(d)ε and similarly Φ(−a + ε) − Φ(−a) ≥
q(d)ε. Therefore,

γ(|f | ≤ C(d)σf tdε) ≥ γ(A) ≥ 2q(d)ε − Rde−rt2
Let 0 < s < 1, ε = C(d)−12−drd/2s| ln s|−d/2, t = 2r−1/2| ln s|1/2. Then

.

γ(|f | ≤ σf s) ≥ 2q(d)C(d)−12−drd/2s| ln s|−d/2 − Rde−2| ln s|

= 2q(d)C(d)−12−drd/2s| ln s|−d/2 − Rds2.

There is a number s0(d) ∈ (0, e−1) such that for any s ≤ s0(d) one has

γ(|f | ≤ σf s) ≥ q(d)C(d)−12−drd/2s| ln s|−d/2.

Thus, there is a positive constant L(d) such that

γ(|f | ≤ σf s) ≥ L(d)s| ln s|−d/2,

whenever 0 ≤ s ≤ 1/2.

(cid:3)

DEVIATION OF POLYNOMIALS FROM THEIR EXPECTATIONS AND ISOPERIMETRY

7

B. The case of a log-concave measure and a polynomial of degree two

Here we obtain an estimate that is sharper than the previous one in the case of an arbitrary
log-concave measure, but applies only to polynomials of degree 2. The proof in this case relies
on the following lemma.

Lemma 2.4. Let s ≥ 0. Then there is a constant c such that the inequality

εZ s

0

e−tI{f ≤−ε}dtZ s

0

e−tI{f ≥ε}dt ≤ cZ s

0

e−tI{|f |<ε}dtZ s

0

e−t|f (t)|dt

holds for every polynomial f of degree two on the real line.

Proof. Without loss of generality we can assume that the coeﬃcient at the highest degree term
of the polynomial f is 1. Suppose ﬁrst that s ≥ 1.

If the right-hand side is not zero, then there is a root of the polynomial f on the interval
(0, s). Let τ denote the minimal root in (0, s) and let σ be the second root. Then on the interval
(0, τ ) the polynomial f is either strictly positive or strictly negative, hence,

Thus, we have to obtain the estimate

Z s

0

e−tI{f ≤−ε}dtZ s
εe−τ ≤ cZ s

e−tI{f ≥ε}dt. ≤Z ∞
e−tI{|f |<ε}dtZ s

0

τ

0

0

e−t|f (t)|dt.

e−tdt = e−τ

Let us consider the case ε < 1

2(1 + |τ − σ|). By Theorems 1.2 and 1.3, given a polynomial g

of degree d, linear functions ℓ1, ℓ2 and a log-concave measure ν, we have

kgkL1(ν) ≥ cdkgkL2(ν),

c1kℓ1k2

L2(ν)kℓ2k2

L2(ν) ≥ kℓ1ℓ2k2

L2(ν) ≥ c2kℓ1k2

L2(ν)kℓ2k2

L2(ν),

ZA

|g|dν ≥ c(d)ν(A)1+dZ |g|dν.

Applying this inequalities to the functions g = f , ℓ1(t) = t − τ , ℓ2(t) = t − σ and to the measure
e−tI{t>0}dt, we obtain that

Z s

0

e−t|f (t)|dt ≥ C(1 − e−s)3Z ∞
≥ eC(1 − e−s)3(cid:18)Z ∞

0

0

e−t|f (t)|dt

(t − σ)2e−tdtZ ∞

0

(t − τ )2e−tdt(cid:19)1/2
≥ bC(cid:0)1 + (1 − τ )2(cid:1)1/2(cid:0)1 + (1 − σ)2(cid:1)1/2

.

In the last estimate we have used the inequality 1 − e−s > 1 − e−1 for s ≥ 1. Let us pick a
point u such that τ ∈ [u, u + 1] ⊂ [0, s]. Then |t − σ| ≤ |τ − σ| + 1 on [u, u + 1]. Thus,

Z s

0

e−tI{|f |<ε}dt ≥ e−τ −1Z u+1

u

I{|t−τ |<ε(|τ −σ|+1)−1}dt ≥ e−τ −1ε(|τ − σ| + 1)−1.

Since

τ,σ (cid:0)1 + (1 − τ )2(cid:1)1/2(cid:0)1 + (1 − σ)2(cid:1)1/2

|τ − σ| + 1

inf

≥ 1/2 > 0,

the desired estimate is proved in this case.

Now let us consider the case ε ≥ 1

2(1 + |τ − σ|). If

ε ≤ 2(1 − e−1)−1Z ∞

0

e−t|f (t)|dt,

8

L.M. ARUTYUNYAN AND E.D. KOSOV

then

Z s

0

Using the estimate

u

I{|t−τ |<ε(|τ −σ|+1)−1}dt ≥ e−τ −1Z u+1
e−tI{|f |<ε}dt ≥ e−τ −1Z u+1
Z s
e−t|f (t)|dt ≥ CZ ∞
ε ≥ 2(1 − e−1)−1Z ∞

e−t|f (t)|dt,

e−t|f (t)|dt,

u

0

0

0

we obtain that the desired inequality is valid in this case too. If

I{|t−τ |<1/2}dt ≥ e−τ −1/2.

then, by Chebyshev’s inequality

e−tI{|f |≥ε}dt ≤ ε−1Z ∞

0

e−t|f (t)|dt.

On the other hand,

Using the above mentioned estimate

Z s

0

0

0

Z s
e−tI{|f |≥ε}dt ≤Z ∞
e−tI{|f |<ε}dt ≥ 1 − e−1 − ε−1Z ∞
e−t|f (t)|dt ≥ CZ ∞

Z s

0

0

0

e−t|f (t)|dt ≥ (1 − e−1)2−1.

e−t|f (t)|dt

and estimating one of the integrals on the left-hand side by one, we conclude that the desired
estimate is also valid in the considered case.

Now let s < 1. In this case e−1 ≤ e−t ≤ 1 on [0, s] and our estimate in this case is equivalent

to the following one:

εZ s

0

I{f ≤−ε}dtZ s

0

I{f ≥ε}dt ≤ cZ s

0

I{|f |<ε}dtZ s

0

|f (t)|dt.

Moreover, after a linear change of variables, we can assume s = 1, i.e., the desired inequality
takes the form

εZ 1

0

I{f ≤−ε}dtZ 1

0

I{f ≥ε}dt ≤ cZ 1

0

I{|f |<ε}dtZ 1

0

|f (t)|dt.

It can be easily seen that if we multiply the integrand by e−t in each integral, then each integral
will diﬀer from the former one by the factor which belongs to [1, e]. So, this case follows from
the previous one for s = 1.
(cid:3)

Remark 2.5. The estimate from Lemma 2.4 does not extend to polynomials of the third
degree. It is suﬃcient to take

s = ∞,

f (t) = (t + 1)2(t − a).

Remark 2.6. If the estimate from Lemma 2.4 were true for

Z s

0

e−t|f (t) − r|dt

instead of

e−t|f (t)|dt

Z s

0

for every r > 0, the isoperimetric inequality in the Cheeger form would be true for the distribu-
tion of any polynomial, see Lemma 3.1 and Theorem 3.5. However, this inequality cannot be
true for an arbitrary log-concave measure. Indeed, this inequality for an arbitrary polynomial
of degree two would have implied the exponential integrability of such polynomials with respect
to an arbitrary log-concave measure (see [2]).

DEVIATION OF POLYNOMIALS FROM THEIR EXPECTATIONS AND ISOPERIMETRY

9

Theorem 2.7. There is a constant c such that, for every polynomial of the second degree on
Rn and every log-concave measure µ, the following inequality holds:

εZ I{f ≤−ε}dµZ I{f ≥ε}dµ ≤ cZ I{|f |<ε}dµZ |f |dµ.

Proof. The functions f1 = εI{f ≤−ε}, f2 = I{f ≥ε} are upper semi-continuous and the functions
f3 = cI{|f |<ε} and f4 = |f | are lower semi-continuous. Hence one can apply the localization
lemma. Thus, it is suﬃcient to prove the following inequality:

εZ r

s

eℓ(t)I{f ≤−ε}dtZ r

s

eℓ(t)I{f ≥ε}dt ≤ cZ r

s

eℓ(t)I{|f |<ε}dtZ r

s

eℓ(t)|f |dt,

where ℓ is an aﬃne function. By a linear change of variables this estimate can be reduced to
the estimate from Lemma 2.4. Hence the theorem is proved.
(cid:3)

Corollary 2.8. There is a constant C > 0 such that for every polynomial of degree 2 on Rn
and every log-concave measure µ the following estimate holds:

µ{|f − mf | < ε}Z |f − mf |dµ ≥ Cε

for ε < αf

Proof. This estimate follows from the previous theorem and Lemma 2.1.

(cid:3)

3. The isoperimetric and Poincar´e inequalities

The following lemma is an analog of Lemma 2.4 for polynomials of an arbitrary degree, but

for measures with a density of the form tn on some interval instead of e−t.

Lemma 3.1. Let s ≥ 0, n ∈ N. Then there is a constant c(d, n) depending only on d and n
such that for every polynomial f of degree d on the real line one has

εZ s+1

s

tnI{f ≤−ε}dtZ s+1

s

tnI{f ≥ε}dt ≤ c(d, n)Z s+1

s

tnI{|f |<ε}dtZ s+1

s

tn|f (t)|dt.

Proof. Without loss of generality we can assume that the coeﬃcient at the highest degree term
of the polynomial f is 1, i.e., f is of the form

Let

Let

f (t) =

(t − ti).

dYi=1
µs(dt) :=(cid:18)Z s+1
tndt(cid:19)−1
ms :=Z tµs(dt),
s :=Z t2µs(dt) − m2

I[s,s+1]tndt,

σ2

s

s.

c1(n) = inf
s

σ2
s .

Note that c1(n) > 0, since the limit at inﬁnity of σ2

s is not zero. First we consider the case

ε < 2R |f |dµs. Let

τ := max{ti : ti ∈ [s, s + 1]}.

Then on the interval (τ, s + 1) the polynomial f is either strictly positive or strictly negative.
Hence,

Z s+1

s

tnI{f ≤−ε}dtZ s+1

s

tnI{f ≥ε}dt ≤Z s+1

s

tndtZ τ

s

tndt.

10

L.M. ARUTYUNYAN AND E.D. KOSOV

Thus, it is suﬃcient to prove the estimate

εZ τ

s

tndt ≤ c(d, n)Z s+1

s

tnI{|f |<ε}dtZ |f |dµs.

Applying Theorem 1.1 to the polynomial f and the measure µs, we obtain

kf k2

L1(µs) ≥ c−2dkf k2

L2(µs) ≥ c−2dkf 2kL0(µs) = c−2d

dYi=1

k(t − ti)2kL0(µs)

≥ c−2d(2c)−2d

dYi=1

k(t − ti)2kL1(µs) = c−2d(2c)−2d

dYi=1

(σ2

s + |ms − ti|2).

Let tj be a root of the polynomial f such that αj := inf

|tj − t| > 1. Then

|f (t)| ≤ (αj + 1)Yi6=j

t∈[s,s+1]

|t − ti|

on [s, s + 1], while σ2
αk := inf

s + |ms − tj|2 ≥ α2

|tk − t| ≤ 1. Then |f (t)| ≤ 2Qi6=k

t∈[s,s+1]

Applying these estimates several times, we obtain that

j . Let tk be a root of the polynomial f such that
s + |ms − tk|2 ≥ c1(n).

|t − ti| on [s, s + 1], while σ2

|f (t)| ≤ 2d(cid:16)Yαj >1

(αj + 1)(cid:17)|t − τ | ≤ 4d(cid:16)Yαj >1

αj(cid:17)|t − τ |

on [s, s + 1], while

Let R := Qαj >1

If ε4−dR−1 > 1/4, then

dYi=1

(σ2

s + |ms − ti|2) ≥ (c1(n))d Yαj>1

α2
j .

αj. Then it is suﬃcient to prove the estimate

s

εZ τ
εZ τ

tndt ≤ c(d, n)Z s+1
tnI{|t−τ |<ε4−dR−1}dtZ |f |dµs.
tndt ≤ 2Z |f |dµs(n + 1)−1((s + 1)n+1 − sn+1),

s

s

Z s+1

s

Obviously, inf
s

tnI{|t−τ |<ε4−dR−1}dtZ |f |dµs ≥ (n + 1)−1((s + 1/4)n+1 − sn+1)Z |f |dµs.

((s + 1/4)n+1 − sn+1)((s + 1)n+1 − sn+1)−1 > 0, thus, in the case where ε4−dR−1 >

1/4 the desired inequality is proved.

Let now ε4−dR−1 ≤ 1/4. It is suﬃcient to prove the inequality

tnI{|t−τ |<ε4−dR−1}dt,

since R |f |dµs ≥ δ(d, n)R. If τ < s + 3/4, then

s

εZ τ
tndt ≤ c(d, n)RZ s+1
Z s+1

s

s

since [τ, τ + ε4−dR−1] ⊂ [s, s + 1], while

tnI{|t−τ |<ε4−dR−1}dt ≥ τ nε4−dR−1,

Z τ

s

tndt ≤ τ n(τ − s) ≤ τ n.

DEVIATION OF POLYNOMIALS FROM THEIR EXPECTATIONS AND ISOPERIMETRY

11

Thus, in this case the desired inequality is also proved. Let now τ ≥ s + 3/4. Then

Z s+1

s

tnI{|t−τ |<ε4−dR−1}dt ≥ (τ − ε4−dR−1)nε4−dR−1,

since [τ, τ − ε4−dR−1] ⊂ [s, s + 1]. The last expression can be estimated by

(τ − 1/4)nε4−dR−1 ≥ (s + 1/2)nε4−dR−1.

Moreover,

Z τ

s

tndt = (n + 1)−1(τ n+1 − sn+1) ≤ (s + 1)n

and, since s + 1/2 ≥ 1/2(s + 1), the desired inequality is proved in this case too.

It remains to consider the case where ε ≥ 2R |f |dµs. Applying Chebyshev’s inequality, we

obtain

µs(|f | ≥ ε) ≤ ε−1Z |f |dµs ≤ 1/2,

hence

Thus, we have

µs(|f | < ε) ≥ 1 − 1/2 = 1/2.

εµs{f ≤ −ε}µs{f ≥ ε} ≤ c(d, n)µs{|f | < ε}Z |f |dµs,

which is equivalent to the announced estimate.

(cid:3)

Lemma 3.2 (The reverse Poincar´e inequality). Let f be a polynomial of degree d on the real
line and let µ be a log-concave measure on the real line. Then the following inequality holds:

σ(µ)kf ′kL2(µ) ≤ (Cd)dkf kL2(µ),

where C is an absolute constant, σ2(µ) is the variance of the measure µ.

Proof. Due to homogeneity we can assume that the polynomial f is of the form

f (t) =

(t − ti).

dYi=1

the L1-norm of a polynomial via its L0-norm from Theorem 1.2, using multiplicativity of the
L0-norm and estimating the L0-norm by the L1-norm, we obtain

Also, without loss of generality we can assume that R tµ(dt) = 0. Applying the estimate of
Z t2µ(dt)Z (f ′(t))2µ(dt) ≤ d

µ(dt)

2

dXi=1 Z t2µ(dt)Z (cid:12)(cid:12)(cid:12)Yj6=i
dXi=1 Z t2µ(dt)Yj6=i
dXi=1 Z t2µ(dt)Yj6=i

(t − tj)(cid:12)(cid:12)(cid:12)
Z |t − tj|2µ(dt)
(cid:18)Z t2µ(dt) + |tj|2(cid:19)

≤ d(2cd)2d

= d(2cd)2d

≤ d2(2cd)2d

(cid:18)Z t2µ(dt) + |ti|2(cid:19) =
dYi=1

d2(2cd)2d

dYi=1Z |t − ti|2µ(dt) ≤ d2(4c2d)2dZ f 2µ(dt),

as announced.

(cid:3)

12

L.M. ARUTYUNYAN AND E.D. KOSOV

Lemma 3.3. Let f be a polynomial of degree d on R. Suppose that f vanishes at some point
τ ∈ [s, s + 1], where s ≥ 0. Then

Z s+1

s

|f (t)|tndt ≤ C(d, n)Z s+1

s

|f (t) − r|tndt

for every number r, where the constant C(d, n) depends only on d and n. In particular, C(d, n)
is independent of r.

Proof. Let µs, σs be deﬁned as in the proof of Lemma 3.1. Note that it is suﬃcient to prove
the inequality

Z |f |dµs ≤ C(d, n)Z |f − λ|dµs.

It can be easily veriﬁed that

kf kL∞(µs) ≤ kf ′kL∞(µs).

Recall the following inequality (see Theorem 1 and Corollary after it in [14]) that holds with
some universal constant C for any probability measure on some interval with a density of the
form ctn and for any polynomial g of degree d:

kgk∞ ≤(cid:18)C max(cid:26)1,

n + 1

d (cid:27)(cid:19)d

kf ′k2

Applying this estimate to our measure µs and the polynomial f ′, we obtain

kf ′kL∞(µs) ≤(cid:18)C max(cid:26)1,

n + 1

d − 1(cid:27)(cid:19)d−1

kf ′kL2(µs).

We now use Lemma 3.2:

.
σs > 0 and kf kL1(µs) ≤ kf kL∞(µs), the lemma is proved.

kf ′kL2(µs) ≤ (Cd)dkf − λkL2(µ)σ−1
s

Since min

s

(cid:3)

Corollary 3.4. Let f be a polynomial of degree d on the real line, s ≥ 0, n ∈ N. Then there
is a constant c(d, n) depending only on d and n such that for every number r one has

εZ s+1

s

tnI{f ≤−ε}dtZ s+1

s

tnI{f ≥ε}dt ≤ c(d, n)Z s+1

s

tnI{|f |<ε}dtZ s+1

s

tn|f (t) − r|dt.

Proof. If f has no zeros in [s, s + 1], then the left-hand side vanishes and the inequality is
obvious. If f has a root in [s, s + 1], then we can apply Lemmas 3.1 and 3.3.
(cid:3)

Theorem 3.5. Let K be a convex compact set in Rn, let λK be the normalized Lebesgue measure
on K, and let f be a polynomial of degree d. Let R = J1⊔J2⊔J3, where Ji are disjoint measurable
sets, and let the distance between J1 and J3 be ε > 0, i.e.,

inf

x1∈J1,x2∈J2

|x1 − x2| = ε > 0.

Then there is a constant c(d, n) depending only on d and n such that

ελK(f ∈ J1)λK(f ∈ J3) ≤ c(d, n)λK(f ∈ J2)ZK

|f − mf |dλK,

where mf =Z f dλK.

Proof. Obviously, it is suﬃcient to prove the estimate with an arbitrary number m in place of
mf . We will prove exactly this statement.

If we take the closures of the sets J1 and J3 and replace J2 with R \ (J1 ∪ J3), the left-hand
side of the inequality does not decrease, while the right-hand side does not increase. Hence we

DEVIATION OF POLYNOMIALS FROM THEIR EXPECTATIONS AND ISOPERIMETRY

13

can assume that J1 and J3 are closed and J2 is open. First we consider the case where J2 is an
interval. Note that our inequality can be written in the form

εZK

I{f ∈J1}(x)dxZK

I{f ∈J3}(x)dx ≤ c(d, n)ZK

I{f ∈J2}(x)dxZK

|f (x) − m|dx.

Since J1 and J3 are closed and J2 is open, the functions f1 = εI{f ∈J1}, f2 = I{f ∈J3} are upper
semi-continuous and the functions f3 = c(d, n)I{f ∈J2}, f4 = |f − λ| are lower semi-continuous.
Hence we can apply Theorem 1.5. Thus, it is suﬃcient to prove the inequality

εZ r

s

ℓ(t)n−1I{f ∈J1}dtZ r

s

ℓ(t)n−1I{f ∈J3}dt ≤ c(d, n)Z r

s

ℓ(t)n−1I{f ∈J2}dtZ r

s

ℓ(t)n−1|f − λ|dt,

where ℓ is an aﬃne function that is nonnegative on [s, r]. By a linear change of variables we
arrive at the inequality

εZ s+1

s

tn−1I{f ∈J1}dtZ s+1

s

tn−1I{f ∈J3}dt ≤ c(d, n)Z s+1

s

tn−1I{f ∈J2}dtZ s+1

s

tn−1|f − λ|dt,

where s ≥ 0 and f is some (possibly, diﬀerent) polynomial. Thus, the case where J2 is an open
interval follows from Corollary 3.4.

Recall that µf = λK ◦ f −1 is the image of the uniform distribution on the compact set K
under the polynomial mapping f . Note that the support of the measure µf is a bounded set
on the real line. Let now J2 be the union of countably many disjoint intervals. The proof in
this case is similar to the one from [18]. For completeness, we present this argument. If there
is an interval which length is less than ε, then its both end points belong either to J1 or to J3
and we can add this interval to J1 or J3, respectively, and prove the estimate for exactly these
(ai, bi) (the union of disjoint intervals), moreover, bi − ai ≥ ε

sets. Thus, we assume that J2 =Fi

and there are ﬁnitely many such intervals, since the support of the measure µf is bounded. We
have already proved that

Summing these inequalities in i, we obtain

εµf(cid:0)(−∞, ai](cid:1)µf(cid:0)[bi, ∞)(cid:1) ≤ c(d, n)µf(cid:0)(ai, bi)(cid:1)αf .
Xi

µf(cid:0)(−∞, ai](cid:1)µf(cid:0)[bi, ∞)(cid:1) ≤

c(d, n)αf

µf (J2).

ε

Since every point of J1 and every point of J3 are separated by at least one interval (ai, bi), we
have

Xi

µf(cid:0)(−∞, ai](cid:1)µf(cid:0)[bi, ∞)(cid:1) ≥ µf (J1)µf (J3),

which completes the proof.

(cid:3)

Corollary 3.6 (The isoperimetric inequality in Cheeger’s form). Let K be a convex compact
set in Rn, let λK be the normalized Lebesgue measure on K, and let f be a polynomial of degree
d. Then there is a constant δ(d, n) depending only on d and n such that

µ+
f (A) ≥

δ(d, n)

αf

µf (A)µf (R \ A).

Proof. It is suﬃcient to apply the previous theorem to the sets

J1 = A, J3 = R \(cid:0)A + (−ε, ε)(cid:1), J2 = R \ (J1 ∪ J2)

and let ε tend to zero.

(cid:3)

As it is known (see [15], a proof can also be found in [2]), the previous assertion implies the

following result.

14

L.M. ARUTYUNYAN AND E.D. KOSOV

Corollary 3.7 (The Poincar´e inequality). Let K be a convex compact set in Rn, let λK be the
normalized Lebesgue measure on K, and let f be a polynomial of degree d. Then there is a
constant C(d, n) depending only on d and n such that, for every smooth function ϕ, one has

(cid:13)(cid:13)(cid:13)(cid:13)ϕ −Z ϕdµf(cid:13)(cid:13)(cid:13)(cid:13)L2(µf )

≤ C(d, n)αf kϕ′kL2(µf ).

References

[1] L. M. Arutyunyan, E. D. Kosov, Estimates for integral norms of polynomials on spaces with convex

measures, Sbornik: Mathematics 206:8 (2015), 1030–1048.

[2] S. G. Bobkov, Isoperimetric Problems in the Theory of Inﬁnite Dimensional Probability Distributions,

Doctoral dissertation, Saint Petersburg (1997).

[3] S. G. Bobkov, Isoperimetric and analytic inequalities for log-concave probability measures, The Annals of

Probability 27:4 (1999), 1903–1921.

[4] S. G. Bobkov, Remarks on the growth of Lp-norms of polynomials, Geometric Aspects of Functional

Analysis, Lecture Notes in Math. 1745 (2000), 27–35.

[5] S. G. Bobkov, Some generalizations of Prokhorov’s results on Khinchin-type inequalities for polynomials,

Theory Probab. Appl. 45:4 (2001), 644–647.

[6] V. I. Bogachev, Gaussian measures, Amer. Math. Soc., Providence, Rhode Island (1998).
[7] V. I. Bogachev, Diﬀerentiable measures and the Malliavin calculus, Amer. Math. Soc., Providence, Rhode

Island, (2010).

[8] V. I. Bogachev, E. D. Kosov, I. Nourdin, G. Poly, Two properties of vectors of quadratic forms in Gaussian

random variables, Theory Probab. Appl. 59:2 (2015), 208–221.

[9] V. I. Bogachev, G. I. Zelenov, On convergence in variation of weakly convergent multidimensional distri-

butions, Doklady Mathematics 91:2 (2015), 138–141.

[10] C. Borell, Convex measures on locally convex spaces, Ark. Math. 12 (1974), 239–252.
[11] C. Borell, Convex set functions in d-space, Periodica Mathematica Hungarica 6:2 (1975), 111–136.
[12] C. Borell, The Brunn–Minkowski inequality in Gauss space, Invent. Math. 30:2 (1975), 207-–216.
[13] J. Bourgain, On the distribution of polynomials on high dimensional convex sets, Geometric aspects of

functional analysis, Lecture Notes in Math. 1469 (1991), 127–137.

[14] A. Carbery, J. Wright, Distributional and Lq norm inequalities for polynomials over convex bodies in Rn,

Math. Res. Lett. 8:3 (2001), 233–248.

[15] J. Cheeger, A lower bound for the smallest eigenvalue of the Laplacian, Problems in analysis 625 (1970),

195–199.

[16] M. Fradelizi, O. Guedon, The extreme points of subsets of s-concave probabilities and a geometric local-

ization theorem, Discrete and Comput. Geom. 31:2 (2004), 327–335.

[17] M. Gromov, V. Milman, Generalization of the spherical isoperimetric inequality to uniformly convex Banach

spaces, Compositio Math. 62 (1987), 263–282.

[18] R. Kannan, L. Lovasz, M. Simonovits, Isoperimetric problems for convex bodies and a localization lemma,

Discrete and Comput. Geom. 13 (1995), 541–559.

[19] B. Klartag, On convex perturbations with a bounded isotropic constant, Geometric and Functional Analysis

16:6 (2006), 1274–1290.

[20] L. Lovasz, M. Simonovits, Random walks in a convex body and an improved volume algorithm, Random

Structures and Algorithms 4:4 (1993), 359–412.

[21] Nazarov F., Sodin M., Volberg A. The geometric Kannan-Lovasz-Simonovits lemma, dimension-free esti-
mates for the distribution of the values of polynomials, and the distribution of the zeros of random analytic
functions, St. Petersburg Math. 14:2 (2003), 351–366.

[22] I. Nourdin, D. Nualart, G. Poly, Absolute continuity and convergence of densities for random vectors on

Wiener chaos, Electron. J. Probab 18:22 (2013), 1–19.

[23] I. Nourdin, G. Poly, Convergence in total variation on Wiener chaos, Stochastic Processes and their Ap-

plications 123:2 (2013), 651–674.

[24] V. N. Sudakov, B. S. Tsirel’son, Extremal properties of semi-spaces for spherically symmetrical measures,

Zap. Nauchn. Sem. LOMI 41 (1974), 14–24.

DEVIATION OF POLYNOMIALS FROM THEIR EXPECTATIONS AND ISOPERIMETRY

15

Faculty of Mechanics and Mathematics, Moscow State University, Moscow, 119991 Russia
E-mail address: lavrentin@yandex.ru

Faculty of Mechanics and Mathematics, Moscow State University, Moscow, 119991 Russia
E-mail address: ked 2006@mail.ru

