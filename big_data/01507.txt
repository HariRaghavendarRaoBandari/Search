6
1
0
2

 
r
a

M
4

 

 
 
]
T
I
.
s
c
[
 
 

1
v
7
0
5
1
0

.

3
0
6
1
:
v
i
X
r
a

Improved Sufﬁcient Conditions for Sparse Recovery

with Generalized Orthogonal Matching Pursuit

Jinming Wen, Zhengchun Zhou, Dongfang Li and Xiaohu Tang

1

Abstract

The generalized orthogonal matching pursuit (gOMP), also called the orthogonal multi-matching pursuit (OMMP), is a natural
generation of the well-known orthogonal matching pursuit (OMP) in the sense that multiple N (N ≥ 1) indices are identiﬁed per
iteration. Sufﬁcient conditions for sparse recovery with OMP and gOMP under restricted isometry property of a sensing matrix
have received much attention in recent years. In this paper, we show that if the restricted isometry constant δNK+1 of the sensing
matrix A satisﬁes δNK+1 < 1/pK/N + 1, then under some conditions on the signal-to-noise ratio (SNR), gOMP recovers at
least one index in the support of the K-sparse signal x from an observation vector y = Ax + v (where v is a noise vector) in
each iteration. Surprisingly, this condition do not require N ≤ K which is needed in Wang, et al 2012 and Liu, et al 2012. Thus,
N can have more choices. When N = 1, this sufﬁcient condition turns to be a sufﬁcient condition for support recovery with
OMP. We show that it is weaker than that in Wang 2015 in terms of both SNR and RIP. Moreover, in the noise free case, we
obtain that δNK+1 < 1/pK/N + 1 is a sufﬁcient condition for recovering the K-sparse signal x with gOMP in K iterations
which is better than the best known one in terms of δNK+1. In particular, this condition is sharp when N = 1.

Compressed sensing, restricted isometry constant, orthogonal matching pursuit, orthogonal multi-matching pursuit, support

recovery.

Index Terms

In compressed sensing (CS) setting, we usually observe the following linear model [4], [9], [6]:

I. INTRODUCTION

y = Ax + v,

(1)
where x ∈ Rn is a K-sparse unknown signal (i.e., |supp(x)| ≤ K, where supp(x) = {i : xi 6= 0} is the support of x, and
|supp(x)| is the cardinality of supp(x)), y ∈ Rm is an observation vector, A ∈ Rm×n (with m << n) is a known sensing
matrix and v ∈ Rm is a noise vector.
One of the central goals in CS is to recover the unknown sparse vector x in (1) using some efﬁcient algorithms based on A
and y. The state-of-the-art CS theory (see, e.g., [4], [3], [9]) show that this can be done under appropriate conditions on A. A
commonly used framework for characterizing ing such conditions is the so-called restricted isometry property (RIP) [4]. For
any m × n matrix A and any integer K with 1 ≤ K ≤ m, the restricted isometry constant (RIC) δK of order K is deﬁned
as the smallest constant such that
(2)

(1 − δK)kxk2

2 ≤ kAxk2

2 ≤ (1 + δK)kxk2

2

for all K-sparse vectors x.

The orthogonal matching pursuit (OMP) [18] is a well-known greedy algorithm for recovering sparse signals. The generalized
orthogonal matching pursuit (gOMP) [20], also called orthogonal multi-matching pursuit (OMMP) in [12], is a natural generation
of OMP in the sense that multiple N (N ≥ 1) indices are identiﬁed per iteration. Simulations in [20] and [12] indicate that,
compared with the original OMP, the gOMP has better sparse recovery performance with less CPU time. The gOMP is described
in Algorithm 1, where AS denote the submatrix of A that only contains the columns indexed by set S ⊂ {1, 2, . . . , n}, and
xS denote the subvector of x that only contains the entries indexed by S. Note that when N = 1, gOMP turns to OMP.
Many RIC-based conditions have been proposed to guarantee exact recovery of K-sparse signals with gOMP in the noise

free case (i.e., v = 0) for general N . It were respectively shown in [20] and [12] that δN K < 1/(pK/N + 3) and δN K <
1/((2 + √2)pK/N ) are sufﬁcient conditions for gOMP to recover x in K iterations. Later, the condition was improved to
δN K < 1/(pK/N + 2) and δN K+1 < 1/pK/N + 1 in [15]. Recently, it was further improved to δN K < 1/(pK/N + 1.27)

J. Wen is with ENS de Lyon, Laboratoire LIP (U. Lyon, CNRS, ENSL, INRIA, UCBL), Lyon, France, 69007, (e-mail: jwen@math.mcgill.ca)
Z. Zhou is with the School of Mathematics, Southwest Jiaotong University, Chengdu 610031, China, (e-mail: zzc@home.swjtu.edu.cn).
D. Li is with the School of Mathematics and Statistics, Huazhong University of Science and Technology, Wuhan 430074, China, and Department of

Mathematics, City University of Hong Kong, Kowloon, Hong Kong, (e-mail: dﬂi@hust.edu.cn).

X. Tang is with the Information Security and National Computing Grid Laboratory, Southwest Jiaotong University, Chengdu 610031, China (e-mail:

xhutang@swjtu.edu.cn).

This research was supported by ”Programme Avenir Lyon Saint-Etienne de l’Universit´e de Lyon” in the framework of the programme ”Inverstissements
d’Avenir” (ANR-11-IDEX-0007), ANR through the HPAC project under Grant ANR 11 BS02 013, NSFC (No. 11571128), a grant from the Research Grants
Council of the Hong Kong Special Administrative Region, China (Project No. CityU 11302915)

Manuscript received; revised .

2

Algorithm 1 gOMP
Input: measurements y ∈ Rm, sensing matrix A ∈ Rm×n, sparsity K, number of indexes to be chosen per iteration N .
Initialize: k = 0, r0 = y, S0 = ∅.
1: while k < K and krkk2 > 0 do

2:
3:
4:
5:
6:
7: end while

k = k + 1
Choose indexes i1, . . . , iN corresponding to the N largest magnitude of |AT rk−1|,
Sk = Sk−1S{i1, . . . , iN},
ˆxSk = arg min ky − ASk xk2,
rk = y − ASk ˆxSk
min

Output: ˆx = arg

x:supp(x)=Sk ky − Axk2.

in [16]. It is worthwhile pointing out that there are more results on sparse recovery with OMP (the special case of gOMP for
N = 1), see, e.g., [8], [12], [21] and [5]. To the best of our knowledge, the best sufﬁcient condition for recovering K-sparse
signals in K iterations is δK+1 < 1/√K + 1 in [13]. On the other hand, it was proved in [23] that for any given positive
integer K ≥ 2 and any given t satisﬁes 1/√K + 1 ≤ t < 1, there always exist a K-sparse x and a matrix A satisﬁes the
RIP of order K + 1 with δK+1 = t such that OMP may fail to recover x in K iterations (here we want to point out that this
necessary condition also works for K = 1). Thus, δK+1 < 1/√K + 1 is a sharp condition for exact recovery of K-sparse
signals with OMP in K iterations.

Sufﬁcient conditions of support recovery of K-sparse signals in the presence of noise with gOMP have also been widely
studied (see [2], [1], [7], [11], [17], [5], and the references therein). As one of the latest results, it was proved in [11] that under

some conditions on the minimum magnitude of the nonzero elements x, δN K+1 < 1/(pK/N + 1) is a sufﬁcient condition
for support recovery under the l2 (kvk2 ≤ ǫ for some constant ǫ, see, e.g. [2]) and l∞ (kAT vk∞ ≤ ǫ, see, e.g. [1]) bounded
noises.
In this paper, we investigate the RIP based sufﬁcient conditions for support recovery in the presence of noise with gOMP.
Instead of considering l2 and and l∞ bounded noises separately (see, e.g, [7], [11], [17] and [5]), we follow [10] and [19] which
use signal-to-noise ratio (SNR) and minimum-to-average ratio (MAR) to measure v and x, which are respectively deﬁned by

and

2

SNR =( kAxk2

v 6= 0
kvk2
+∞ v = 0

,

(3)

(4)

MAR =

mini∈Ω |xi|2
kxk2
2/K

.

We show that under some conditions on SNR, gOMP is ensured to recover at least one index in the support of x in each

iteration if δN K+1 < 1/pK/N + 1. As consequences, we have the following contributions:
• Unlike [20] and [12], which require N ≤ min(K, m/K), our condition on N is only N ≤ (m − 1)/K which ensures
that the assumption δN K+1 < 1/pK/N + 1 makes sense. This allows more choices of N for gOMP.
• The sufﬁcient condition for support recovery with gOMP turns to the condition for OMP when N = 1, and it is weaker
• In the noise free case, we obtain that δN K+1 < 1/pK/N + 1 is a sufﬁcient condition for recovering K-sparse signals
with gOMP in K iterations. This improves the best known condition δN K+1 < 1/(pK/N + 1) in [15]. In particular,
when N = 1, our sufﬁcient condition δK+1 < 1/√K + 1 is exactly the one in [13]. As aforementioned, this condition
is sharp according to the result in [23]

than that in [19] in terms of both SNR and RIP.

The rest of the paper is organized as follows. We give some useful notation and lemmas in section II. We present our main

results in section III. Finally, this paper is summarized in section IV.

A. Notation

II. NOTATION AND USEFUL LEMMAS

Throughout this paper, we adopt the following notations unless otherwise stated. Let R be the real ﬁeld. Boldface lowercase
letters denote column vectors, and boldface uppercase letters denote matrices, e.g., x ∈ Rn and A ∈ Rm×n. Let 0 denote a
zero column vector. Let Ω be the support of X and |Ω| be the cardinality of Ω. Let set S ⊂ {1, 2, . . . , n}, and Ω\ S = {i|i ∈
Ω, i 6∈ S}. Let Ωc and Sc be the complement of Ω and S, i.e., Ωc = {1, 2, . . . , n} \ Ω, and Sc = {1, 2, . . . , n} \ S. Let AS
be the submatrix of A that only contains the columns indexed by S, and xS be the subvector of x that only contains the
entries indexed by S, and AT
S and
P ⊥S = I − P S denote the projector and orthogonal complement projector on the column space of AS, respectively.

S be the transpose of AS. For full column rank matrix AS, let P S = AS(AT

S AS)−1AT

B. Useful lemmas

3

We now introduce some lemmas that will be useful in the sequel.
Lemma 1 ([4]): If a matrix A ∈ Rm×n satisﬁes the RIP of orders K1 and K2 with K1 < K2, then δK1 ≤ δK2.
Lemma 2 ([16]): Let S1, S2 be two subsets of {1, 2, . . . , n} with |S2 \ S1| ≥ 1. If a matrix A ∈ Rm×n satisﬁes the RIP of

order |S1 ∪ S2|, then for any vector x ∈ R|S2\S1|,
(1 − δ|S1∪S2|)kxk2

2 ≤ kP ⊥S1 AS2\S1 xk2

2 ≤ (1 + δ|S1∪S2|)kxk2
2.

Lemma 3 ([14]): Let A satisfy the RIP of order K and S be a subset of {1, 2, . . . , n} with |S| ≤ K, then for any x ∈ Rm,

kAT

S xk2

2 ≤ (1 + δK)kxk2
2.

The following lemma is the key to the proof of our main results in the next section.
Lemma 4: Suppose that x in (1) is a sparse vector with support Ω. Let set S ⊆ {1, 2, . . . , n} satisfy |S| = kN and
|Ω ∩ S| = l for some integers N , k and l with 0 ≤ k ≤ l ≤ |Ω| − 1 and N (k + 1) + |Ω| − k ≤ m. Let W ⊆ Ωc be a set with
|W| = N and W ∩ S = ∅. If the matrix A in (1) satisﬁes the RIP of order N (k + 1) + |Ω| − l with 0 ≤ δN (k+1)+|Ω|−l < 1.
Then

≥

Proof: See Appendix A.

1

max

i P ⊥S AΩ\S xΩ\S| −

i∈Ω\S |AT
(1 −p(|Ω| − l)/N + 1δN (k+1)+|Ω|−l)kxΩ\Sk2

N Xj∈W

|AT

j P ⊥S AΩ\SxΩ\S|

p|Ω| − l

.

(5)

makes sense.

Remark 1: The condition N (k+1)+|Ω|−k ≤ m in Lemma 4 is to ensure that A satisﬁes the RIP of order N (k+1)+|Ω|−l
If N = 1, then from Lemma 4, l = k. Hence, we have the following result which is Lemma 1 in [22].
Corollary 1: Suppose that x in (1) is a sparse vector with support Ω. Let S be a subset of Ω with |S| < |Ω|. If A in (1)

satisﬁes the RIP of order |Ω| + 1 with 0 ≤ δ|Ω|+1 < 1. Then,

Remark 2: By Lemma 4 and Lemma 1, it is easy to verify that the righthand side of (5) is positive if the matrix A in (1)

satisﬁes the RIP of order N (k + 1) + |Ω| − k with

Ω\S P ⊥S AΩ\S xΩ\Sk∞ − kAT
kAT
(1 −p|Ω| − |S| + 1δ|Ω|+1)kxΩ\Sk2

.

≥

Ωc P ⊥S AΩ\S xΩ\Sk∞

p|Ω| − |S|

δN (k+1)+|Ω|−k <

1

.

p(|Ω| − k)/N + 1

III. MAIN RESULTS

In this section, we will show that if A in (1) satisﬁes the RIP of order N K + 1 with

δN K+1 <

,

(6)

1

pK/N + 1

then gOMP is ensured to recover at least one index in the support of K-sparse signals in each iteration under some conditions
on SNR and MAR. In particular, in the noise free case (i.e., v = 0), we will show that (6) is a sufﬁcient condition for recovering
K-sparse signals in K steps with gOMP. To achieve these goals, we need the following result.

with

Theorem 1: Suppose that x in (1) is a sparse vector with support Ω, and A in (1) satisﬁes the RIP of order N (k+1)+|Ω|−k
(7)

1

for some integers k and N satisfying 0 ≤ k ≤ |Ω| − 1 and N (k + 1) + |Ω| − k ≤ m. Then gOMP selects at least one index in
Ω in each iteration of the ﬁrst k + 1 ones before all the indexes in Ω have been selected and before gOMP terminates provided
that

√SNR >

Proof: See Appendix B.

√2K(1 + δN (k+1)+|Ω|−k)

(1 −p|Ω|/N + 1δN (k+1)+|Ω|−k)√MAR

.

(8)

δN (k+1)+|Ω|−k <

p|Ω|/N + 1

If x in (1) is a K-sparse vector with support Ω, then by Lemma 1, (7) also holds if (6) holds. Thus, by Theorem 1, we can

easily obtain the following result.

4

Theorem 2: Suppose that x in (1) is a K-sparse vector with support Ω, and A in (1) satisﬁes the RIP of order N K + 1
with δN K+1 satisfying (6) for an integer N with 1 ≤ N ≤ (m − 1)/K. Then gOMP either recovers at least k0 indexes in Ω
if gOMP terminates after performing k0 iterations with 1 ≤ k0 < K or recovers Ω in K iterations provided that

√SNR >

√2K(1 + δN K+1)

(1 −p|Ω|/N + 1δN K+1)√MAR

.

(9)

When N = 1, gOMP turns to OMP. In this case, the following result for OMP can be directly obtained from Theorem 2

which signiﬁcantly improves a known one in [19].

Corollary 2: Suppose that x in (1) is a K-sparse vector with support Ω, and A in (1) satisﬁes the RIP of order K + 1 with

δK+1 <

1

√K + 1

.

(10)

Then OMP either recovers at least k0 indexes in Ω if it terminates after performing k0 iterations with 1 ≤ k0 < K or it
recovers Ω in K iterations provided that

Remark 3: With the same notations as in Corollary 2, it was shown in [19, Theorem 3.1] that OMP either recovers at least
k0 indexes in Ω if OMP terminates after performing k0 iterations with 1 ≤ k0 < K or recovers Ω in K iterations provided
that

.

(11)

√SNR >

√2K(1 + δK+1)

(1 −p|Ω|/N + 1δK+1)√MAR

δK+1 <

1

√K + 1

2√K(1 + δK+1)

(1 −p|Ω|/N + 1δK+1)√MAR

.

and

√SNR >

It is easy to see that our sufﬁcient condition given in Corollary 2 is weaker than that in [19, Theorem 3.1] in terms of both
RIC and SNR. Furthermore, by the necessary condition on SNR given by [19, Theorem 3.2], the constraint on SNR (see (11))
is very tight. But the necessary condition is our of the scope of this paper.

Notice that gOMP may terminate before performing K iterations, and in this case Ω is not guaranteed to be recovered by
gOMP by Theorem 2 under (6) and (9). However, in the noise-free case, (6) is a sufﬁcient condition for recovering x with
gOMP in K iterations. Speciﬁcally, we have the following result.

Theorem 3: Suppose that v in (1) satisﬁes v = 0, x in (1) is a K-sparse vector with support Ω, and A in (1) satisﬁes the
RIP of order N K + 1 with δN K+1 satisfying (6) for integer N with 1 ≤ N ≤ (m − 1)/K. Then gOMP recovers x in K
iterations.

Proof: The result follows directly from Theorem 2 and Lemma 5 below.

Remark 4: In the noise free case, the best known condition on δN K+1 such that gOMP recovers x in K iterations is

δN K+1 < 1/(pK/N + 1) in [15]. Obviously, our sufﬁcient condition given in Theorem 3 is better.
Lemma 5: Suppose that x in (1) is a sparse vector with support Ω, and A in (1) satisﬁes the RIP of order N (k + 1)+|Ω|− k
with δN (k+1)+|Ω|−k satisfying (7) for some integers k and N with 1 ≤ k ≤ |Ω| − 1 and 1 ≤ N ≤ (m − 1)/K. If there exists
an integer k0 with 0 < k0 ≤ k and |Ω ∩ Sk0| ≥ k0 such that ||rk0||2 = 0 (see Algorithm 1 for the deﬁnitions of Sk0 and rk0
). Then Ω ⊆ Sk0.
Proof: We prove this lemma by contradiction. Suppose that Ω 6⊆ Sk0 and let Γ = Ω∪ Sk0. Let ¯x, ˜x ∈ R|Γ| satisfy ¯xi = xi
is the vector generated by
for i ∈ Ω and ¯xi = 0 for i /∈ Ω, and ˜xi = (ˆxSk0 )i for i ∈ Sk0 and ˜xi = 0 for i /∈ Sk0, where ˆxSk0
Algorithm 1. Since krk0k2 = 0, by line 6 of Algorithm 1, ASk0 ˆxSk0 = y, we have

(12)

AΓ ¯x = AΩxΩ = Ax = y = ASk0 ˆxSk0 = AΓ ˜x.

Note that |Ω ∩ Sk0| ≥ k0 and Γ = Ω ∪ Sk0. It then follows that

Combing with (12) and (7), we obtain ¯x = ˜x.

|Γ| = |Ω| + |Sk0| − |Ω ∩ Sk0| ≤ |Ω| + N k − k ≤ N (k + 1) + |Ω| − k.

¯xj 6= 0 and ˜xj = 0. This implies that ¯x 6= ˜x and leads to contradiction with ¯x = ˜x. Completing the proof.

On the other hand, by the deﬁnitions of ¯x and ˜x, and the assumption that Ω 6⊆ Sk0, there exists j ∈ (Ω \ Sk0 ) such that
When N = 1, we immediately get the following result for OMP from Theorem 3 which is [13, Theorem III.1].
Corollary 3: Suppose that v in (1) satisﬁes v = 0, x in (1) is a K-sparse vector with support Ω, and A in (1) satisﬁes the

RIP of order K + 1 with δK+1 < 1√K+1

. Then OMP recovers x in K iterations.

5

Remark 5: It was shown in [23] that for any given positive integer K ≥ 1 and any 1/√K + 1 ≤ t < 1, there always exist
a K-sparse x and a matrix A satisﬁes the RIP of order K + 1 with δK+1 = t such that OMP may fail to recover x in K
iterations. Thus, in the noise free case, δK+1 < 1√K+1
given in Corollary 3 is a sharp condition for OMP to recover K-sparse
given in Theorem 3 is sharp or
signals in K iterations. However, for general N , it is still open whether δN K+1 <
not for gOMP to recover K-sparse signals in K iterations. The reader is kindly invited to attack this problem.

1√K/N +1

IV. CONCLUSION

In this paper, we have studied the sufﬁcient conditions for sparse recovery with gOMP based on the RIP of the sensing

matrix. We have shown that under some conditions on SNR, δN K+1 < 1/pK/N + 1 is a sufﬁcient condition for support
recovery of a K-sparse signal x with gOMP. Surprisingly, our sufﬁcient condition do not require N ≤ K (which is needed
in [20] and [12]), which provides more choices for N . When N = 1, the sufﬁcient condition is for support recovery with
OMP and it is better than that in [19] in terms of both RIC and SNR. In the noise free case, we have also showed that

δN K+1 < 1/pK/N + 1 is a sufﬁcient condition for recovering x with gOMP in K iterations, which is much better than the

best known one in terms of δN K+1 in [15]. Moreover, the condition is sharp when N = 1.

PROOF OF LEMMA 4
Since |Ω ∩ S| = l for 0 ≤ l ≤ |Ω| − 1, kxΩ\Sk1 6= 0. Thus, we have

APPENDIX A

=

(a)

≥

(b)

≥

≥

(c)
=

=

1

max

|xj|) max

i∈Ω\S |AT
kxΩ\Sk1
1

i P ⊥S AΩ\S xΩ\S|
( Xj∈Ω\S
i∈Ω\S |AT
|xj|) max
( Xj∈Ω\S
p|Ω| − lkxΩ\Sk2
p|Ω| − lkxΩ\Sk2 Xj∈Ω\S(cid:0)|xjAT
p|Ω| − lkxΩ\Sk2 Xj∈Ω\S(cid:0)xjAT
p|Ω| − lkxΩ\Sk2 Xj∈Ω\S(cid:0)xjAT
p|Ω| − lkxΩ\Sk2kP ⊥S AΩ\S xΩ\Sk2
i∈Ω\S |AT

i P ⊥S AΩ\S xΩ\S| ≥ AT

max

2,

1

1

1

1

i P ⊥S AΩ\S xΩ\S|

i P ⊥S AΩ\S xΩ\S|

i∈Ω\S |AT
l P ⊥S AΩ\S xΩ\S|(cid:1)
l P ⊥S AΩ\S xΩ\S(cid:1)
l (P ⊥S )T P ⊥S AΩ\S xΩ\S(cid:1)

j P ⊥S AΩ\SxΩ\S;

where (a) follows from |supp(xΩ\S)| = |Ω| − l and Cauchy-Schwarz inequality; (b) is because for each j ∈ Ω \ S,

And (c) is from

Thus,

Let

then by some simple calculations, we obtain

(P ⊥S )T P ⊥S = P ⊥S P ⊥S = P ⊥S .

kP ⊥S AΩ\S xΩ\Sk2
≤p|Ω| − lkxΩ\Sk2 max

2

i∈Ω\S |AT

i P ⊥S AΩ\S xΩ\S|.

α = −p(|Ω| − l)/N + 1 − 1

p(|Ω| − l)/N

,

2α

1 − α2 = −p(|Ω| − l)/N ,

1 + α2

1 − α2 =p(|Ω| − l)/N + 1.

(13)

(14)

(15)

(16)

6

(17)

(18)

(19)

(20)
(21)

To simplify notation, let W = {j1, j2, . . . , jN} (note that |W| = N ), and deﬁne eW ∈ RN with

for 1 ≤ i ≤ N . Then, it is easy to see that
W AT
eT

Furthermore, we deﬁne

where (a) is because W ∩ Ω = ∅. Then,

and

Thus,

ei =(1

if AT
−1 if AT

ji P ⊥S AΩ\S xΩ\S ≥ 0
ji P ⊥S AΩ\S xΩ\S < 0

W P ⊥S AΩ\S xΩ\S = Xj∈W

|AT

j P ⊥S AΩ\SxΩ\S|.

(a)

B =P ⊥S A(Ω\S)∪W
u =(cid:2)xΩ\S 0(cid:3)T
w =h0

αkxΩ\Sk2

√N

= P ⊥S A(Ω∪W )\S,
∈ R|Ω\S|+N
eWiT

∈ R|Ω\S|+N ,

Bu = P ⊥S AΩ\SxΩ\S,

ku + wk2
kα2u − wk2

2 = (1 + α2)kxΩ\Sk2
2,
2 = α2(1 + α2)kxΩ\Sk2
2.

wT BT Bu

(a)
= wT AT
(b)
= wT AT

(Ω\S)∪W (P ⊥S )T P ⊥S AΩ\S xΩ\S
(Ω\S)∪W P ⊥S AΩ\S xΩ\S

=

(c)
=

W AT
eT

αkxΩ\Sk2

√N
αkxΩ\Sk2
√N Xj∈W

W P ⊥S AΩ\SxΩ\S
j P ⊥S AΩ\SxΩ\S|,
|AT

where (a) follows from (18) and (19); and (b) follows from (13), and (c) is from (17). Therefore, we have

2

kB(u + w)k2
=kBuk2
=kBuk2

2 + kBwk2
2 + kBwk2

2 + 2wT BT Bu
2αkxΩ\Sk2

2 +

√N Xj∈W

and

kB(α2u − w)k2

2 =α4kBuk2

2

2α3kxΩ\Sk2

2 + kBwk2
√N Xj∈W

−

|AT

j P ⊥S AΩ\S xΩ\S|.

|AT

j P ⊥S AΩ\S xΩ\S|.

Thus, by the aforementioned equations, we have
kB(u + w)k2
=(1 − α4)kBuk2
+ 2α(1 + α2)kxΩ\Sk2√N Xj∈W
=(1 − α4)

2

2 − kB(α2u − w)k2

2

|AT

j P ⊥S AΩ\S xΩ\S|

|AT

j P ⊥S AΩ\S xΩ\S|

× (kBuk2

=(1 − α4)

× (kBuk2

2 +

2α
1 − α2

kxΩ\Sk2√N Xj∈W
2 − p|Ω| − lkxΩ\Sk2

N

Xj∈W

where the last equality follows from the ﬁrst equality in (16).

It is not hard to check that

|AT

j P ⊥S AΩ\SxΩ\S|),

(22)

7

(23)

(24)

(a)

kB(u + w)k2
2 − kB(α2u − w)k2
≥ (1 − δN (k+1)+|Ω|−l)k(u + w)k2

2

2

(b)

2

2

2

− (1 + δN (k+1)+|Ω|−l)k(α2u − w)k2
= (1 − δN (k+1)+|Ω|−l)(1 + α2)kxΩ\Sk2
2(cid:2)(1 − δN (k+1)+|Ω|−l)
2(cid:2)(1 − α2) − δN (k+1)+|Ω|−l(1 + α2)(cid:3)
2(cid:0)1 −
2(cid:0)1 −p(|Ω| − l)/N + 1δN (k+1)+|Ω|−l(cid:1),

− (1 + δN (k+1)+|Ω|−l)α2(1 + α2)kxΩ\Sk2
=(1 + α2)kxΩ\Sk2
− (1 + δN (k+1)+|Ω|−l)α2(cid:3)
=(1 + α2)kxΩ\Sk2
=(1 − α4)kxΩ\Sk2
= (1 − α4)kxΩ\Sk2

1 − α2 δN (k+1)+|Ω|−l(cid:1)

1 + α2

(c)

2

kP ⊥S AΩ\SxΩ\Sk2
−p|Ω| − lkxΩ\Sk2
N
≥kxΩ\Sk2

Xj∈W

|AT

j P ⊥S AΩ\SxΩ\S|

2(cid:0)1 −p(|Ω| − l)/N + 1δN (k+1)+|Ω|−l(cid:1).

where (a) follows from Lemma 2 and (18), (b) follows from (20) and (21), and (c) follows from the second equality in (16).

By (19), (22), (23) and the fact that 1 − α4 > 0, we have

Thus, combining with (14), we obtain

p|Ω| − lkxΩ\Sk2
i∈Ω\S |AT
×(cid:0) max
≥kxΩ\Sk2

1

i P ⊥S AΩ\S xΩ\S| −
2(cid:0)1 −p(|Ω| − l)/N + 1δN (k+1)+|Ω|−l(cid:1).

N Xj∈W

|AT
j P ⊥S AΩ\SxΩ\S|(cid:1)

Therefore, (5) holds. (cid:3)

APPENDIX B

PROOF OF THEOREM 1

We prove the result by induction. Suppose that gOMP selects at least one correct indexes in the ﬁrst ¯k−1 (with 1 ≤ ¯k ≤ k+1)
iterations, then l = |S¯k−1 ∩ Ω| ≥ ¯k − 1. We assume Ω 6⊆ S¯k−1 and Algorithm 1 performs at least ¯k iterations. Then, we need
to show that |(S¯k \ S¯k−1) ∩ Ω| ≥ 1. Since S0 = ∅, the induction assumption |Ω| > |S¯k−1 ∩ Ω| ≥ ¯k − 1 holds with ¯k = 1. So,
the proof for the ﬁrst iteration is contained in the case that ¯k = 1.

Let j1, j2 . . . , jn ∈ Ωc such that

|AT
j1 r

where we deﬁne

¯k−1| ≥ |AT
j2 r
≥ |AT
j∈(Ωc)\W r

¯k−1| ≥ . . . ≥ |AT

¯k−1|,

jN r

¯k−1|

W = {j1, j2, . . . , jN}.

Then to show |(S¯k \ S¯k−1) ∩ Ω| ≥ 1, we only need to show
i r

max

i∈Ω |AT

¯k−1| > |AT

jN r

¯k−1|.

By (25), it sufﬁces to show

By lines 4 and 5 of Algorithm 1, we have

max

i∈Ω |AT
i r

¯k−1| >

1

N Xj∈W

|AT
j r

¯k−1|.

r

¯k−1 = y − AS¯k−1 ˆxS¯k−1
=(cid:0)I − AS¯k−1 (AT

(Ax + v)

(a)

S¯k−1

AS¯k−1)−1AT

S¯k−1(cid:1)y

(b)

= P ⊥S¯k−1
= P ⊥S¯k−1
= P ⊥S¯k−1
= P ⊥S¯k−1

(c)

(AΩxΩ + v)
(AΩ∩S¯k−1 xΩ∩S¯k−1 + AΩ\S¯k−1
AΩ\S¯k−1 xΩ\S¯k−1 + P ⊥S¯k−1
v,

xΩ\S¯k−1 + v)

, the fact that Ω = supp(x) and P ⊥S¯k−1
where (a), (b) and (c) follows from the deﬁnition of P ⊥S¯k−1
¯k−1| = 0. Thus, by (28)
By lines 3 and 4 of Algorithm 1, for each i ∈ S¯k−1, |AT
i r

max

i∈Ω |AT
≥ max
i∈Ω\S¯k−1

¯k−1|
i r
i P ⊥S¯k−1
(|AT

AΩ\S¯k−1 xΩ\S¯k−1| − |AT

i P ⊥S¯k−1

v|).

Similarly, by (28),

≤

To simplify notation, we deﬁne

1

N Xj∈W
N Xj∈W

1

|AT
j r

¯k−1|

j P ⊥S¯k−1

|AT

AΩ\S¯k−1 xΩ\S¯k−1| + max

j∈W |AT

j P ⊥S¯k−1

v|.

β1 = max

AΩ\S¯k−1 xΩ\S¯k−1|

1

i P ⊥S¯k−1

i∈Ω\S¯k−1 |AT
N Xj∈W
|AT
−
i P ⊥S¯k−1
i∈Ω\S¯k−1 |AT

j P ⊥S¯k−1

β2 = max

AΩ\S¯k−1 xΩ\S¯k−1|,
j P ⊥S¯k−1

j∈W |AT

v| + max

v|.

Then, by (29)-(32), to show (27), it sufﬁces to show

β1 > β2.

8

(25)

(26)

(27)

(28)

AS¯k−1 = 0, respectively.

(29)

(30)

(31)

(32)

(33)

In the following, we give an upper bound on β2. Clearly there exist i0 ∈ Ω \ S¯k−1 and j0 ∈ W such that

max

i∈Ω\S¯k−1 |AT
j∈W |AT

max

i P ⊥S¯k−1

j P ⊥S¯k−1

v| = |AT
v| = |AT

i0 P ⊥S¯k−1

j0 P ⊥S¯k−1

v|;
v|.

Therefore

where (a) is because AT

i0∪j0 P ⊥S¯k−1

vk2

9

(34)

(a)

i0∪j0 P ⊥S¯k−1

√2kAT
β2 =kAT
≤q2(1 + δN (k+1)+|Ω|−k)kvk2

vk1

≤

(b)

i0∪j0 P ⊥S¯k−1

v is a 2 × 1 vector, (b) follows from Lemma 3, and

kP ⊥S¯k−1

vk2 ≤ kP ⊥S¯k−1k2kvk2 ≤ kvk2 ≤ ǫ.

In the following, we give a lower bound on β1. By line 3 of Algorithm 1, |S¯k−1| = (¯k − 1)N . By the induction assumption
(35)

0 ≤ ¯k − 1 ≤ |Ω ∩ S¯k−1| = l ≤ |Ω| − 1.

By the deﬁnition of set W , W ⊂ Ωc and |W| = N . Thus, by Lemma 1 and 4, and (31), we obtain

β1 ≥

≥

≥

(1 −p(|Ω| − l)/N + 1δN ¯k+|Ω|−l)kxΩ\S¯k−1k2
(1 −p(|Ω| − l)/N + 1δN ¯k+|Ω|−¯k+1)kxΩ\S¯k−1k2
(1 −p|Ω|/N + 1δN (k+1)+|Ω|−k)kxΩ\S¯k−1k2

,

p|Ω| − l
p|Ω| − l
p|Ω| − l
|Ω| − l

where the second and third inequality respectively follows from (35) and the fact that ¯k ≤ k + 1.

By [19, 25], we have

kxΩ\S¯k−1k2 ≥s

In fact, by (3) and (4), we have

K(1 + δN (k+1)+|Ω|−k)

√MAR · SNRkvk2.

(36)

(37)

kxΩ\S¯k−1k2 ≥p|Ω| − l min

i∈Ω |xi|

(a)

(b)

√K

≥ p|Ω| − l √MARkxk2
≥ s
≥ s

K(1 + δN (k+1)+|Ω|−k)

K(1 + δN (k+1)+|Ω|−k)

|Ω| − l

|Ω| − l

(c)

!
√MARkAxk2
√MAR · SNRkvk2,

where (a) is from (4), (b) is from

And (c) follows from (3).

By (36) and (37), we have

Thus, by (34), (33) can be guaranteed by

kAxk2 = kAΩxΩk2 ≤q1 + δN (k+1)+|Ω|−kkxΩk2

=q1 + δN (k+1)+|Ω|−kkxk2.

β1 ≥

pK(1 + δN (k+1)+|Ω|−k)

(1 −p|Ω|/N + 1δN (k+1)+|Ω|−k)√MAR · SNRkvk2
(1 −p|Ω|/N + 1δN (k+1)+|Ω|−k)√MAR · SNRkvk2
>q2(1 + δN (k+1)+|Ω|−k)kvk2,

pK(1 + δN (k+1)+|Ω|−k)

,

which is equivalent to (8). By induction, the theorem holds. (cid:3)

10

REFERENCES

[1] T. CAI AND L. WANG, Orthogonal matching pursuit for sparse signal recovery with noise, IEEE Trans. Inf. Theory, 57 (2011), pp. 4680–4688.
[2] E. J. CAND ´ES, The restricted isometry property and its implications for compressed sensing, C. R. Acad. Sci. Paris, Ser. I, 346 (2008), pp. 589–592.
[3] E. J. CAND ´ES, J. ROMBERG, AND T. TAO, Stable signal recovery from incomplete and inaccurate measurements, Comm. Pure Appl. Math, 59 (2006),

pp. 1207–1223.

[4] E. J. CAND ´ES AND T. TAO, Decoding by linear programming, IEEE Trans. Inf. Theory, 51 (2005), pp. 4203–4215.
[5] L.-H. CHANG AND J.-Y. WU, An improved RIP-based performance guarantee for sparse signal recovery via orthogonal matching pursuit, IEEE Trans.

Inf. Theory, 60 (2014), pp. 707–710.

[6] A. COHEN, W. DAHMEN, AND R. DEVORE, Compressed sensing and best k-term approximation, J. Amer. Math. Soc., 22 (2009), pp. 211–231.
[7] W. DAN, Analysis of orthogonal multi-matching pursuit under restricted isometry property, Science China Mathematics, 57 (2014), pp. 2179–2188.
[8] M. DAVENPORT AND M. WAKIN, Analysis of orthogonal matching pursuit using the restricted isometry property, IEEE Trans. Inf. Theory, 56 (2010),

pp. 4395–4401.

[9] D. L. DONOHO, Compressed sensing, IEEE Trans. Inf. Theory, 52 (2006), pp. 1289–1306.
[10] A. K. FLETCHER AND S. RANGAN, Orthogonal matching pursuit: A brownian motion analysis, Signal Processing, IEEE Transactions on, 60 (2012),

pp. 1010–1021.

[11] B. LI, Y. SHEN, Z. WU, AND J. LI, Sufﬁcient conditions for generalized orthogonal matching pursuit in noisy case, Signal Processing, 108 (2015),

pp. 111–123.

[12] E. LIU AND V. TEMLYAKOV, The orthogonal super greedy algorithm and applications in compressed sensing, IEEE Trans. Inf. Theory, 58 (2012),

pp. 2040–2047.

[13] Q. MO, A sharp restricted isometry constant bound of orthogonal matching pursuit, arXiv:1501.01708.
[14] D. NEEDEL AND J. A. TROPP, CoSaMP: Iterative signal recovery from incomplete and inaccurate samples, Applied and Computational Harmonic

Analysis, 26 (2009), pp. 301–321.

[15] S. SATPATHI, R. L. DAS, AND M. CHAKRABORTY, Improving the bound on the rip constant in generalized orthogonal matching pursuit, Signal

Processing Letters, IEEE, 20 (2013), pp. 1074–1077.

[16] Y. SHEN, B. LI, W. PAN, AND J. LI, Analysis of generalized orthogonal matching pursuit using restricted isometry constant, Electron. Lett., 50 (2014),

pp. 1020–1022.

[17] Y. SHEN AND S. LI, Sparse signals recovery from noisy measurements by orthogonal matching pursuit, Inverse Problems and Imaging, 9 (2015),

pp. 231–238.

[18] J. TROPP, A. C. GILBERT, ET AL., Signal recovery from random measurements via orthogonal matching pursuit, Information Theory, IEEE Transactions

on, 53 (2007), pp. 4655–4666.

[19] J. WANG, Support recovery with orthogonal matching pursuit in the presence of noise, IEEE Trans. Signal Process., 63 (2015), pp. 5868–5877.
[20] J. WANG, S. KWON, AND B. SHIM, Generalized orthogonal matching pursuit, Signal Processing, IEEE Transactions on, 60 (2012), pp. 6202–6216.
[21] J. WANG AND B. SHIM, On the recovery limit of sparse signals using orthogonal matching pursuit, IEEE Trans. Signal Process., 60 (2012), pp. 4973–

4976.

[22] J. WEN, Z. ZHOU, J. WANG, X. TANG, AND Q. MO, Sharp conditions for exact support recovery of sparse signals with noise via OMP, arXiv preprint

arXiv:1512.07248, (2015).

[23] J. WEN, X. ZHU, AND D. LI, Improved bounds on the restricted isometry constant for orthogonal matching pursuit, Electronics Letters, 49 (2013),

pp. 1487–1489.

