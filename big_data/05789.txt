6
1
0
2

 
r
a

 

M
8
1

 
 
]

O
L
.
s
c
[
 
 

1
v
9
8
7
5
0

.

3
0
6
1
:
v
i
X
r
a

Stuttering equivalence is too slow!

David N. Jansen†
dnjansen@cs.ru.nl

Jeroen J. A. Keiren*,†

Jeroen.Keiren@ou.nl

*Open University of the Netherlands, School of Computer Science, Netherlands
†Radboud Universiteit, Institute for Computing and Information Sciences, Nijmegen, Netherlands

Abstract

Groote and Wijs recently described an algorithm for deciding stuttering equivalence and
branching bisimulation equivalence, acclaimed to run in O (m log n) time. Unfortunately,
the algorithm does not meet the acclaimed running time.
In this paper, we present two
counterexamples where the algorithms uses Ω (md) time. In order to analyse the problem we
present pseudocode of the algorithm, and indicate the time that can be spent on each part
of the algorithm in order to meet the desired bound. We also propose ﬁxes to the algorithm
such that it indeed runs in O (m log n) time.

1

Introduction

It has long been an open problem whether the algorithm by Groote and Vaandrager [2] for com-
puting stuttering equivalence [1] and branching bisimulation [5] was optimal. Recently, Groote
and Wijs [4, 3], presented an improvement. They describe an algorithm for deciding stuttering
equivalence in time O (m log n) and space O (m), where n is the number of states, and m the
number of transitions of the Kripke structure at hand, with m ≥ n. This is an improvement over
the previous running time of O (mn).

Unfortunately, the algorithm [3] falls short of the stated goal. In this paper we introduce two
counterexamples where the algorithm will use more time than O (m log n), namely Ω (md), where
d is the maximal outdegree of a state in the Kripke structure.

Since the original description of the algorithm relies heavily on auxiliary data structures and
pointers in order to ensure that all information is available quickly when needed, without making
the data structures any bigger than strictly necessary, the problem with the algorithm is hard to
detect in this original description. We therefore ﬁrst present our understanding of the algorithm
as presented in [3] by giving a high-level description in pseudocode, which leaves out as much
of the detailed data structures as possible. This also allows us to assign time budgets to its
parts, that are satisﬁed locally and that together allow us to determine the overall time bound.
Subsequently, we identify two problems in the original algorithm, by giving counterexamples that
lead to running times higher than the desired bound of O (m log n). For each of these problems,
we indicate how to ﬁx the algorithm, so that ultimately we can conﬁrm the main result of [3]
that stuttering equivalence and branching bisimulation can be computed in O (m log n) time and
O (m) space.

Throughout this paper we assume the reader is familiar with the deﬁnitions of Kripke structure
and (divergence-blind) stuttering equivalence and with the auxiliary notions introduced in [3].
The note is best read while having a copy of [3] within reach for reference. We focus our analysis
on deciding stuttering equivalence for Kripke structures. The results carry over directly to the
computation of branching bisimulation.

1

Algorithm 1 The main algorithm for stuttering equivalence. Closely follows [3]
1.1 Initialise all temporary data.
1.2 while there is a nontrivial constellation do
1.3

O (m log n)

O (1) per splitter Sp

Choose a nontrivial constellation Sp
and a splitter Sp ⊂ Sp that is small (i. e. |Sp| ≤ 1
Create a new constellation New and move Sp from Sp to New .
for all s ∈ Sp do {Find predecessors of Sp (except Sp itself)}

2 |Sp|).

for all s′ ∈ in \τ (s) do

Mark the block of s′ as reﬁnable.
Mark s′ as predecessor of Sp.
Register that s′ → s goes to New (instead of Sp).

end for

end for
Prepare Sp to be reﬁned (i. e., mark states, register transitions).
begin {Stabilise the partition again:}

for all reﬁnable blocks Ref do

O(cid:0)|in \τ (Sp)|(cid:1)

O (|out(Sp)|)

Result := TrySplit(Ref , New , marked states ∈ Ref ,
unmarked bottom states ∈ Ref )

TrySplit′(Result, Sp, states ∈ Result with a transition to Sp,

bottom states ∈ Result without transition to Sp)

PostprocessNewBottom()

end for

1.4

1.5
1.6

1.7

1.8
1.9

1.10

1.11
1.12

1.13
1.14

1.15

1.16

1.17
1.18

end
Destroy all temporary data (i. e., markings of states and blocks).

1.19
1.20
1.21 end while

O (|in(Sp)|+|out (Sp)|)

2 Pseudocode for Groote/Wijs 2016

We ﬁrst present the main part of the algorithm from [3] as we understand it in terms of pseudocode,
while separating out a routine TrySplit that tries to split a reﬁnable block into states that can
reach the splitter (called red states) and those that cannot (called blue states). There is a small
diﬀerence between the calls to TrySplit in lines 1.15 and 1.16, which we will explain later. The
high-level structure of the algorithm is presented in Algorithm 1. It maintains the invariant:

Invariant 1. The current blocks are stable with respect to the constellations, i. e. all states in a
block can reach the same constellations through a (weak) transition.

A nontrivial constellation, i. e. a constellation containing multiple blocks, indicates that the
current blocks are not yet stable with respect to themselves. The main loop separates a block Sp
from a nontrivial constellation Sp, moving it to a new constellation New , and then restores the
invariant by reﬁning blocks with respect to these new constellations, if needed. The latter is done
in TrySplit.

In the algorithms we assigned a time budget to some steps to facilitate the analysis of the

algorithms’ complexity. We generally use the abbreviations |in(B)| = Ps∈B max{1, |in(s)|} and
|out (B)| = Ps∈B|out(s)| (note that in the latter case, it is not necessary to take the maximum,

as the transition relation is total, i. e., |out(s)| > 0 for all s ∈ B.). Similarly, in τ (s) are the inert
incoming transitions and out τ (s) the inert outgoing transitions of s.

2.1 Splitting blocks

The most important new idea from [3] is used in the step when a block is actually being reﬁned.
They start to ﬁnd both the red and the blue states, spending the same amount of work on either
part, until it becomes clear which one is the smaller. In other words, they use the idea “Process
the smaller half” not only when looking for splitters, but also when reﬁning blocks. The work

2

Algorithm 2 Reﬁne a block into red and blue states, called in Line 1.15. Slightly improved
2.1 function TrySplit(Ref , Sp, Red , Blue)
2.2 {Try to reﬁne block Ref , depending on whether states have transitions to the splitter constel-
lation Sp. Red contains all states in Ref with a strong transition to Sp, and Blue contains
all bottom states in Ref without transition to Sp.}

2.3 begin {Spend the same amount of work on either process:}
2.4 whenever |Blue| > 1
2.5

whenever |Red | > 1
Stop this process.

Stop this process.

2 |Ref | then

end whenever

2.6
2.7 while Blue contains

end whenever
while Red contains

2 |Ref | then O (1) per assign-
ment to Blue or
Red , respectively.

unvisited states do
Choose an unvisited s ∈ Red.
Mark s as visited.
for all s′ ∈ in τ (s) do

Red := Red ∪ {s′}

end for
end while
Stop the other process.
Move Red to a new block New .
Destroy all temporary data.
for all s ∈ New do

for all s′ ∈ out τ (s) \ New do

s → s′ is no longer inert.

end for
if |out τ (s)| = 0 then

s is a new bottom state.

end if
end for
Result := New

O (|in τ (New )|)

O (|out (New )|)

as Lines 2.7–2.19

O (|in τ (New )|)
or
O (|out τ (New )|)

O (|out (NBot )|)

2.35
2.36

Register that (Result, constellation of s′) needs postprocessing.
Register that s can reach the constellation of s′.

unvisited states do

Choose an unvisited s ∈ Blue.
Mark s as visited.
for all s′ ∈ in τ (s) \ Red do

if nottoblue(s′) undeﬁned then

nottoblue(s′) := |out τ (s′)|

end if
nottoblue(s′) := nottoblue(s′)−1
if nottoblue(s′) = 0 then

Blue := Blue ∪ {s′}

end if
end for
end while
Stop the other process.

2.20
2.21 Move Blue to a new block New .
2.22 Destroy all temporary data.
2.23
2.24

for all s′ ∈ in τ (s) \ New do

for all s ∈ New do

2.8

2.9

2.10
2.11

2.12

2.13
2.14

2.15
2.16

2.17

2.18
2.19

2.25

2.26
2.27

2.28

2.29
2.30

s′ → s is no longer inert.
if |out τ (s′)| = 0 then

s′ is a new bottom state.

end if
end for

end for
Result := Ref

2.31
2.32 end
2.33 for all new bottom states s ∈ Result do
2.34

for all s′ ∈ out \τ (s) do

end for

2.37
2.38 end for
2.39 return Result

3

Algorithm 3 Reﬁne as required by new bottom states, called in Line 1.17
3.1 function PostprocessNewBottom()
3.2 while there is a pair ( ˆB, B) that needs postprocessing do
3.3

Choose a pair ( ˆB, B) that needs postprocessing.
Delete ( ˆB, B) from the pairs that need postprocessing.
if some, but not all new bottom states can reach B then

O (1) per pair ( ˆB, B)

3.4

3.5
3.6

3.7

3.8

3.9
3.10

3.11

TrySplit′( ˆB, B , states ∈ ˆB with a transition to B ,

new bottom states ∈ ˆB without transition to B)

for all constellations C that New can reacha do

if ( ˆB, C ) still needs postprocessing then

Register that (New , C ) needs postprocessing.

O (|out(New )|)

end if

end for

end if

3.12
3.13 end while
3.14 Destroy all temporary data.
3.15 return

aNew is the new block created in TrySplit′, Line 3.6.

spent on the reﬁnement can then be bounded: state s is involved in such a reﬁnement at most
O (log n) times. Upon every such reﬁnement, at most O (|in(s)| + |out(s)|) time is spent for state
s. So, overall O ((|in(s)| + |out (s)|) log n) time is spent on state s. Summing over all states then
gives the desired time complexity O (m log n).

Note that this may require detailed bookkeeping of the amount of work. One may balance
the work by using an auxiliary variable, which stores the amount of work done on the red states
minus the amount of work done on the blue states. Every time a state or transition is checked
(i. e. every time the loop in Lines 2.7 or 2.10 is entered), the balance is increased or reduced by 1.
We deviate from [3] slightly: ibid. uses a priority queue to keep track of nottoblue, but actually
nothing queue-like is needed, as the order of states is irrelevant for the correctness, time and
memory bounds. Note that the data structure should allow to test for membership in time O (1).1
They use the priority only to store the value nottoblue(s′) and check whether this value is deﬁned
by a test for membership in the priority queue. We propose instead to set nottoblue(s′) to some
special value (e. g. 0) to indicate that it was not yet calculated. We also need a list or similar of
all states whose nottoblue(s′) is deﬁned, to destroy the temporary data later.

2.2 New bottom states

While reﬁning a block, it may happen that some states become bottom states because all their
inert transitions become transitions from a red state to a blue state and therefore are no longer
inert. Here, we also added a slight improvement (Lines 2.23–2.30): we only look for new bottom
states in Red.

It may happen that a new bottom state can no longer reach all the constellations that were
reachable from the original bottom states. To repair Invariant 1, we may have to split some new
bottom states oﬀ the block. Lines 2.33–2.38 prepare some temporary data for these splits.

The further splitting itself is shown in Algorithm 3. The basic idea is to check, for each
constellation that is reachable from some new bottom state, whether the block has to be split.
Of course, as soon as a block is split, both parts have to be checked for the remaining reachable
constellations. The algorithm in [3] constructs, for each block ˆB and constellation B, a list SB ⊆ ˆB
of states that can reach B (see Line 2.36). These lists are used in Line 3.6 to decide which states
are blue, namely the new bottom states that are not in SB . The time budget O (|out(NBot )|) can
be met if the list SB follows the same order as the list NBot .

1Note that priority queues typically have longer access times.

4

Algorithm 3 generally follows [3], except in Lines 3.7–3.11, where we tried to ﬁnd a formulation

that ﬁts in the time budget.

3 Counterexamples to time complexity

The algorithm contains two problems that cause it to be too slow. First, the second call to
TrySplit, here referred to as TrySplit′, takes too long. We give a counterexample, and im-
prove TrySplit′ to improve the complexity to the required bound. As indicated above, also the
postprocessing of new bottom states as carried out in [3] is too slow. In section 3.3 we analyse
the problem in the original algorithm; the proposed improvement has already been incorporated
in Algorithm 3.

3.1 TrySplit′ is too slow
In the call to TrySplit′ (in Line 1.16), the initial set of red states is given implicitly, through
a list of transitions. As a consequence, the test whether the potentially blue state s′ has a non-
inert transition to Sp (this is why we require s′ 6∈ Red in Line 2.10ℓ) in the variant TrySplit′
is executed in a diﬀerent way compared to the one in TrySplit. Groote and Wijs [3] add this
test just before Line 2.12ℓ. If s′ is marked (i. e. it has a transition to Sp), they can access one of
their many auxiliary variables, but otherwise, “it can be checked by walking over the transitions
s′ → s′′ ∈ s′.Ttgt ” (obviously, this is meant instead of the original “. . . ∈ s.Ttgt ” – see Section 5.3,
item 1.(b).ii.A.second bullet.ﬁrst dash of [3]). So they execute a loop to verify out \τ (s′) ∩ Sp = ∅,
namely:

for all s′′ ∈ out(s′) do

if s′′ ∈ Sp \ Ref then continue to Line 2.10ℓ

end for

This loop makes their algorithm slower than promised: the test uses time O (|out(s′)|), but

should take at most O (1). Our ﬁrst counterexample illustrates this time budget overrun.

As the algorithm looks through the predecessors of all

Assume that the partition shown in Figure 1 has been reached. Then, we reﬁne Sp: we
select Sp, ﬁnd its weak predecessors (the whole block Ref , so nothing is reﬁned) and the weak
predecessors of Sp\Sp (the right half of block Ref ). Note that we do the latter without walking over
the states in Sp \ Sp: it is ok to spend O (|in(Sp)|) time here. We also save time by calculating
-states in New , because it is smaller than
the complement of the weak predecessors, i. e. the
Ref \ New : we are allowed to spend an additional O (|in(New )| + |out(New )|) time on this task.
-state for
-state). The cited passage
inclusion in New . This happens k times (once for each transition to a
-state is in Sp \Sp.
then requires that we check each time whether some immediate successor of a
-state, we have to check Θ(d) non-inert
If the transitions to the
transitions to diﬀerent constellations (plus possibly some inert ones), and in the end we ﬁnd that
-state should not be considered further.2 The problem is that we spend (much) time on a
the
transition to New from a state that is possibly in New but in the end not actually in New . The
part of the algorithm in Section 5.3, 1.(b).ii.A.second bullet, is only allowed to spend O (1) time
on each such transition.

-states, it considers the

-states are checked before the

Overall, observe that the number of states and the number of transitions both are O (k + d). So,
the algorithm is allowed to spend O ((k + d) log(k + d)) time, but it spends Ω (kd) time. Variants
-state show that the checks can cost O (md)
of this Kripke structure with several copies of the
time.

Note that we are not allowed to concentrate on the red states (the weak predecessors of Sp \Sp,

the

-states and

-state) themselves instead of the complement, as this set is larger.

2In particular, [3] does not deﬁne nottoblue(
that does not help if there are multiple copies of

). An ad-hoc solution would be to set it to some value > d, but

.

5

Figure 1: TrySplit′ is too slow.

Sp

D

k states
. . .

m

Sp

k states
. . .

Ref

E1

E 1

E2

E 2

. . .

Ed

E d

d constellations

Lemma 2. Reﬁning in Lines 1.16 and 3.6 as described in [3] has a worst-case time complexity of
Ω (md).

We tried (in vain) to ﬁnd a recursive counterexample, which should increase the time complexity
to Ω (md log n), but every of our ideas led to a counterexample with so many additional transitions
that it still ﬁt the bound of Lemma 2.

3.2 A faster TrySplit′

We propose to solve this problem as follows: Execute the slow test at the latest possible moment,
namely immediately before a state is inserted in Blue. This is shown in Algorithm 4. Here, we
also present the formal parameters list according to the implicit representation of the red and blue
states: a set FromRed of transitions from red states, a set MaybeBlue of possibly blue bottom
states with a predicate isBlueTest that indicates which bottom states are actually blue. The
overall time budget is still met: If the red states are the smaller part, then FromRed is a subset
of out (New ). If the blue states are the smaller part, then states in MaybeBlue \ New are marked
(i. e. they have a transition to Sp, see Line 1.8); we are allowed to walk over them one more time.
When the test is executed in Line 4.24ℓ, all inert transitions of s′ point to blue states. If s′ has
a non-inert transition to Sp, it is actually a red state, and in particular, a red new bottom state.
(It may happen that we do not ﬁnd all new bottom states here; therefore, we still have to execute
Line 4.33ℓ.) As every state becomes a bottom state at most once, we are allowed to spend time
O ((|in(s′)| + |out(s′)|) log n) then, which is abundant. If no such transition to Sp is found, s′ is
a blue state and we have to account for the time diﬀerently. It is O (|out(s′)|) per unmarked blue
state s′. Every time s′ becomes an unmarked blue state, the test is executed exactly once, which
ﬁts in the general bound per time that s′ is involved in a reﬁnement.

6

Algorithm 4 Reﬁne a block w. r. t. Sp \ Sp (corrected), called in Line 1.16
4.1 function TrySplit′(Ref , Sp, FromRed , MaybeBlue , isBlueTest )
4.2 {Try to reﬁne block Ref , depending on whether states have transitions to the splitter constel-
lation Sp. FromRed contains all transitions from Ref to Sp, MaybeBlue contains all bottom
states that may be initially blue states, isBlueTest is a predicate that determines whether a
candidate in MaybeBlue is deﬁnitely blue.}

Blue := ∅

4.3 begin {Spend the same amount of work on either process:}
4.4
4.5 whenever |Blue| > 1
4.6

Red := ∅
whenever |Red | > 1
Stop this process.

Stop this process.

2 |Ref | then

end whenever

4.7
4.8 while Blue or MaybeBlue contain

O (1)

2 |Ref | then O (1) per assign-
ment to Blue or
Red , respectively.

end whenever
while Red or FromRed contain
unvisited elements do
Choose an unvisited s ∈ Red
or s → s′′ ∈ FromRed.

if s → s′′ is chosen then
Mark s → s′′ as visited.
if s was visited earlier then

continue to Line 4.8r

end if
Red := Red ∪ {s}

end if
Mark s as visited.
for all s′ ∈ in τ (s) do

Red := Red ∪ {s′}

|in τ (New )|+
|MaybeBlue|+
|out (New )|+
|out(NBot )|


O


|FromRed| !
O |in τ (New )|+

and

unvisited states do

Choose an unvisited s ∈ Blue

or s ∈ MaybeBlue.

Mark s as visited.
if s 6∈ Blue then

if ¬isBlueT est(s) then
continue to Line 4.8ℓ

end if
Blue := Blue ∪ {s}

end if

for all s′ ∈ in τ (s) \ Red do

if nottoblue(s′) undeﬁned then

nottoblue(s′) := |out τ (s′)|

end if
nottoblue(s′) := nottoblue(s′)−1
if nottoblue(s′) = 0 then

if out \τ (s′) ∩ Sp = ∅ then

Blue := Blue ∪ {s′}

end if

4.9

4.10

4.11
4.12

4.13
4.14

4.15

4.16
4.17

4.18

4.19
4.20

4.21

4.22
4.23

4.24
4.25

4.26

4.27
4.28

4.29

end if
end for
end while
Stop the other process.

end for
end while
Stop the other process.
Move Red to a new block New .
Destroy all temporary data.

4.30
4.31 Move Blue to a new block New .
4.32 Destroy all temporary data.
4.33
4.34 end
4.35 Register new bottom states for postprocessing (as Lines 2.33–2.38).
4.36 return

Find new non-inert transitions and bottom states (as Lines 2.23–2.30).

O (|out (New )|)

as Lines 4.8–4.29

7

Figure 2: PostprocessNewBottom is too slow.

Sp

Sp

D

Ref

n states

. . .

. . .

E1

E 1

E2

E 2

En

E n

n constellations

3.3 PostprocessNewBottom is too slow
In Algorithm 3, we already included an improvement in Lines 3.7–3.11. If some block ˆB is split
here, it should not take longer than O (|in(New )| + |out(New )|). The original formulation did
not take this into account; it always walked over all lists SB to separate them into the part that
belongs to New and the part that belongs to what remains in ˆB. With our terminology, it did:

for all constellations B such that ( ˆB, B) still needs postprocessing do

for all new bottom states s ∈ the original ˆB with a transition to B do

if s ∈ New then

Register that (New , B) needs postprocessing.
Move s from SB for ˆB to the corresponding list for New .
if SB for ˆB is empty then

Register that ( ˆB, B) no longer needs postprocessing.

end if

end if

end for

end for

If New is much smaller than ˆB, a budget overrun may result because the loop still spends (a
little) time for each state in ˆB \ New with a transition to B. This is illustrated in our second
counterexample.

Assume that the partition in Figure 2 has been reached. Then, we choose Sp as splitter. the
-state is a (weak) predecessor of Sp, but not of Sp \ Sp, and therefore is split oﬀ from the
remainder of Ref . This turns all
-states into new bottom states. First, it is registered that
(Ref , E 1), . . . , (Ref , E n) all need postprocessing. (Also (Ref , Sp) needs postprocessing, but we
will disregard it in the lower bound for timing.) Then, one for one, these pairs are handled.
Suppose it starts with (Ref , E 1). The algorithm will ﬁnd that it has to split Ref into two parts,

8

-state that is a predecessor of E 1 and the n − 1 other

namely the
-states. Than, it walks over
the n − 1 pairs (Ref , E i) that still need postprocessing and their lists SE i, containing altogether
2 n(n − 1) − 1 list
2 + 3 + · · · + n states; from each list, it will remove the ﬁrst
entries are read or removed. After that, the algorithm may handle (Ref , E 2), split oﬀ one more
2 (n − 1)(n − 2) − 1
list entries are read or removed. For all the pairs up to (Ref , E n), the algorithm reads and ﬁnally

-state from the rest, and walk over the n − 2 remaining pairs (Ref , E i). Here, 1

-state. In total, 1

removes Θ(cid:0)n3(cid:1) list entries.
The Kripke structure in Figure 2 has O (n) states and O(cid:0)n2(cid:1) transitions. Therefore, the whole
algorithm should run in time O(cid:0)n2 log n(cid:1). However, it takes at least Ω(cid:0)n3(cid:1) time. When we think

of variants of this Kripke structure (e. g. with reduced outdegree of the
states having a transition to the same
Θ (m) states.

-
-state), we ﬁnd that there are actually d iterations over

-state and multiple

Lemma 3. Postprocessing new bottom states as described in [3] has a worst-case time complexity
of Ω (md).

3.4 A faster PostprocessNewBottom

The main idea for correcting the time bound was already hinted at earlier: Lines 3.7–3.11 try to
distribute SB over New and what remains of ˆB in time proportional to the outgoing transitions
of New , while keeping the order of SB in line with the order of NBot . This can be achieved if
one distributes SB simultaneously with distributing the states in ˆB and their outgoing transitions
themselves in Line 4.31. If all else fails, even constructing SB for New from scratch can ﬁt the
time bound O (|out(New )|).

4 Conclusion

We showed that the algorithm of [3] for computing stuttering equivalence does not meet the
acclaimed bound of O (m log n); instead, there are examples showing it required Ω (md) time with
d the maximum outdegree in the Kripke structure.

Presentation of the algorithm in pseudocode enabled us to identify the parts of the algorithm
that are responsible for the overrun of the time bound. We are convinced that, after correcting
TrySplit′ and PostprocessNewBottom, the time budgets (as indicated in the pseudocode)
are met. Therefore, we are emboldened to conﬁrm the main result of [3]:

Theorem 4. It is possible to calculate the stuttering equivalence of a Kripke structure in O (m log n)
time and O (m) memory (by using the corrected Algorithm 1).

References

[1] M.C. Browne, E.M. Clarke, and O. Gr¨umberg. Characterizing ﬁnite Kripke structures in

propositional temporal logic. Theoretical Computer Science, 59(12):115–131, July 1988.

[2] J. F. Groote and F. W. Vaandrager. An eﬃcient algorithm for branching and stuttering
equivalence. In Proc. 17th Int. Colloq. Autom. Lang. Program., volume 443 of LNCS, pages
626–638. Springer Berlin / Heidelberg, 1990.

[3] J. F. Groote and A. Wijs. An O(m log n) algorithm for stuttering equivalence and branching

bisimulation. arXiv e-prints 1601.01478, January 2016. http://arxiv.org/abs/1601.01478.

[4] J. F. Groote and A. Wijs. An O(m log n) algorithm for stuttering equivalence and branching

bisimulation. 2016. Accepted for publication in TACAS 2016.

[5] R.J. van Glabbeek and W.P. Weijland. Branching time and abstraction in bisimulation se-

mantics. Journal of the ACM, 43(3):555–600, 1996.

9

