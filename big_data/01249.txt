6
1
0
2

 
r
a

M
3

 

 
 
]

V
C
.
s
c
[
 
 

1
v
9
4
2
1
0

.

3
0
6
1
:
v
i
X
r
a

HyperFace: A Deep Multi-task Learning Framework for Face Detection,

Landmark Localization, Pose Estimation, and Gender Recognition

Rajeev Ranjan

University of Maryland
College Park, MD 20742

rranjan1@umd.edu

Abstract

Vishal M. Patel

Rutgers University

New Brunswick, NJ 08901
vishal.m.patel@rutgers.edu

Rama Chellappa

University of Maryland
College Park, MD 20742

rama@umiacs.umd.edu

We present an algorithm for simultaneous face detection,
landmarks localization, pose estimation and gender recog-
nition using deep convolutional neural networks (CNN).
The proposed method called, Hyperface, fuses the interme-
diate layers of a deep CNN using a separate CNN and trains
multi-task loss on the fused features.
It exploits the syn-
ergy among the tasks which boosts up their individual per-
formances. Extensive experiments show that the proposed
method is able to capture both global and local information
of faces and performs signiﬁcantly better than many com-
petitive algorithms for each of these four tasks.

1. Introduction

Detection and analysis of faces is a challenging problem
in computer vision, and has been actively researched for
its extensive applications on face veriﬁcation, face track-
ing, person identiﬁcation, etc. Although recent methods
based on deep Convolutional Neural Networks (CNN) have
achieved remarkable results for face detection tasks [9],
[32], [47], it is still difﬁcult to obtain facial landmark lo-
cations, head pose estimates and gender information from
face images containing extreme pose, illumination and res-
olution variations. The tasks of face detection, landmark
localization, pose estimation and gender classiﬁcation have
generally been solved as separate problems. Recently it has
been shown that learning correlated tasks simultaneously
can boost the performance of the individual tasks [52] ,[51],
[4].

In this paper, we present a novel framework based on
CNNs for simultaneous face detection, facial landmark lo-
calization, head pose estimation and gender recognition
from a given image (see Figure 1). We design a CNN
architecture to learn common features for these tasks and
build the synergy among them. We exploit the fact that in-
formation contained in features is hierarchically distributed

Figure 1. Our method can simultaneously detect the face, localize
landmarks, estimate the pose and recognize the gender. The blue
boxes denote detected male faces, while pink boxes denote female
faces. The green dots provide the landmark locations. Pose esti-
mates for each face are shown on top of the boxes in the order of
roll, pitch and yaw.

throughout the network as demonstrated in [48]. Lower lay-
ers respond to edges and corners, and hence contain bet-
ter localization features. They would be more suitable for
learning landmark localization and pose estimation tasks.
On the other hand, higher layers are class-speciﬁc and suit-
able for learning complex tasks which is desired for face
detection and gender recognition. It is evident that we need
to make use of all the intermediate layers of a deep CNN in
order to train different tasks under consideration. We refer
the set of intermediate layer features as hyperfeatures. We
borrow this term from [1] which uses it to denote a stack of
local histograms for multilevel image coding.

Since a CNN architecture contains multiple layers with
hundreds of feature maps in each layer, the overall dimen-
sion of hyperfeatures is too large to be efﬁcient for learn-
ing multiple tasks. Moreover, the hyperfeatures must be
associated in a way that they efﬁciently encode the features
common to the multiple tasks. This can be handled using
feature fusion techniques. Features fusion aims to trans-
form the features to a common subspace where they can

1

5°,−3°,14°1°,−11°,1°−6°,−1°,21°4°,−4°,10°6°,−10°,7°−9°,−11°,13°−15°,−8°,−15°−30°,1°,−15°12°,0°,−1°8°,8°,12°Figure 2. The architecture of the proposed Hyper-net. The network is able to classify a given image region as face or non-face, estimate
the head pose, locate face landmarks and recognize gender.

be combined linearly or non-linearly. Recent advances in
deep learning have shown that CNNs are capable of esti-
mating any complex function. Hence, we construct a sepa-
rate fusion-CNN to fuse the hyperfeatures. In order to learn
the tasks, we train them simultaneously using multiple loss
functions. In this way, the features get better understanding
of faces, which leads to improvements in the performances
of the individual tasks. The deep CNN combined with the
fusion-CNN can be learned together end-to-end. Since the
network performs hyperfeatures fusion, we name it Hyper-
net. This paper makes the following key contributions.

1. We propose a novel CNN architecture that performs
the multiple tasks of face detection, landmarks local-
ization, pose estimation and gender recognition.

2. We propose two post-processing methods: recursive
region proposal and landmark-based non-maximum
suppression, which leverage the multitask information
obtained from the CNN to improve the overall perfor-
mance.

3. We achieve new state-of-the-art performances on chal-
lenging unconstrained datasets for all of these four
tasks.

This paper is organized as follows. Section 2 reviews

related work. Section 3 describes the proposed Hyperface
framework in detail. Section 4 provides the multitask results
on challenging datasets. Finally, Section 5 concludes the
paper with a brief summary and discussion.

2. Related Work

One of the earlier approaches for jointly addressing the
tasks of face detection, pose estimation, and landmark local-
ization was proposed in [51] and later extended in [52]. This
method is based on a mixture of trees with a shared pool
of parts in the sense that every facial landmark is modeled
as a part and uses global mixtures to capture topological
changes due to viewpoint changes. A joint cascade-based
method was recently proposed in [4] for simultaneously de-
tecting faces and landmark points on a given image. This
method yields improved detection performance by incorpo-
rating a face alignment step in the cascade structure. Multi-
task learning using CNNs has also been studied lately. [50]
learns gender and other attributes to improve landmark lo-
calization, while [12] trains a CNN for person pose estima-
tion and action detection, using features only from the last
layer. The intermediate layer features have been used for
solving the tasks of image segmentation [13] and pedestrian
detection [34].

Face detection: Viola-Jones detector [41] is a classic
method which uses cascaded classiﬁers on Haar-like fea-
tures to detect faces. This method provides realtime face
detection, but works best for full, frontal, and well lit faces.
Deformable Parts Model (DPM) [10]-based face detection
methods have also been proposed in the literature where a
face is essentially deﬁned as a collection of parts [51], [29].
It has been shown that in unconstrained face detection, fea-
tures like HOG or Haar wavelets do not capture the dis-
criminative facial information at different illumination vari-
ations or poses. To overcome this limitations, various deep
CNN-based face detection methods have been proposed in
the literature [32], [24], [47], [9], [46]. These methods have
produced state-of-the-art results on many challenging pub-
licly available face detection datasets. Some of the other
recent face detection methods include NDFaces [27], PEP-
Adapt [23], and [4].

Landmark localization: Fiducial point extraction or
landmark localization is one of the most important steps in
face recognition. Several approaches have been proposed in
the literature. These include both regression-based [3], [44],
[43], [42] and model-based [5] ,[30], [26]. While the for-
mer learns the shape increment given a mean initial shape,
the latter trains an appearance model to predict the key-
point locations. CNN-based landmark localization methods
have also been proposed in recent years [37], [50] and have
achieved remarkable performance.

Pose estimation: The task of head pose estimation is to
infer the orientation of person’s head relative to the camera
view. It is extremely useful in face veriﬁcation for matching
face similarity in different orientations. However, not much
research has gone into pose estimation from unconstrained
images. Non-linear manifold-based methods have been pro-
posed in [2], [14], [35] to classify face images based on
pose. A survey of various head pose estimation methods is
provided in [31].

Gender recognition: Previous works on gender recog-
nition have focused on ﬁnding good discriminative features
for classiﬁcation. Most previous methods use one or a com-
bination of features such as LBP, SURF, HOG or SIFT. In
recent years, attribute-based methods for face recognition
have gained a lot of traction. Binary classiﬁers were used
in [22] for each attribute such as male, long hair, white etc.
Different features were computed for different features and
they were used to train a different SVM for each attribute.
CNN-based methods have also been proposed for learning
attribute-based representations [28], [49].

3. HyperFace

We propose a single CNN model called Hyper-net for si-
multaneous face detection, landmark localization, pose es-
timation and gender classiﬁcation. The network architec-
ture is deep in both vertical and horizontal directions and

In this section, we provide a brief
is shown in Figure 2.
overview of the system pipeline and then discuss the differ-
ent components in this pipeline in detail.

The proposed framework called Hyperface consists of
three modules. The ﬁrst one generates class independent
region-proposals from the given image and scales them to
227× 227 pixels. The second module is a CNN which takes
in the resized candidate regions and classiﬁes them as face
or non-face. If a region gets classiﬁed as a face, the net-
work additionally provides the facial landmarks locations,
estimated head pose and the gender information for it. The
third module is a post-processing step which involves recur-
sive region-proposals and landmarks-based k-NMS to boost
the face detection score and improve the performance of in-
dividual tasks.

3.1. Hyper-net Architecture

We start with the Alexnet[21] network for image classi-
ﬁcation. The network consists of ﬁve convolutional layers
along with three fully connected layers. We initialize the
network with the Imagenet Challenge (ILSVRC2012 [6])
pre-trained weights distributed with the Caffe[18] imple-
mentation. All the fully connected layers are removed as
they encode image-classiﬁcation task speciﬁc information,
which is not desirable for face related tasks. We utilize the
following two observations to create our network. 1) The
features in CNN are distributed hierarchically in the net-
work. While the lower layer features are informative for lo-
calization tasks such as landmarks and pose estimation, the
higher layer features are suitable for more complex tasks
such as detection or classiﬁcation[48]. 2) Learning mul-
tiple correlated tasks simultaneously builds a synergy and
improves the performance of individual taska as shown in
[4, 50]. Hence, in order to simultaneously learn face de-
tection, landmarks, pose and gender, we need to fuse the
features from the intermediate layers of the network (hy-
perfeatures), and learn multiple tasks on top of it. Since the
adjacent layers are highly correlated, we do not consider all
the intermediate layers for fusion.

We fuse the max1, conv3 and pool5 layers of Alexnet,
using a separate network. A naive way for fusion is directly
concatenating the features. Since the feature maps for these
layers have different dimensions 27×27×96, 13×13×384,
6 × 6 × 256, respectively, they cannot be concatenated di-
rectly. We therefore add conv1a and conv3a convolutional
layers to pool1, conv3 layers to obtain consistent feature
maps of dimensions 6× 6× 256 at the output. We then con-
catenate the output of these layers along with pool5 to form
a 6 × 6 × 768 dimensional feature maps. The dimension
is still quite high to train a multi-task framework. Hence,
a 1 × 1 kernel convolution layer (convall) is added to re-
duce the dimensions [38] to 6 × 6 × 192. We add a fully
connected layer (f call) to convall, which outputs a 3072

dimensional feature vector. At this point, we split the net-
work into ﬁve separate branches corresponding to the dif-
ferent tasks. We add f cdetection, f clandmarks, f cvisibility,
f cpose and f cgender fully connected layers, each of dimen-
sion 512, to f call. Finally, a fully connected layer is added
to each of the branch to predict the individual task labels.
After every convolution or fully connected layer, we deploy
Rectiﬁed Layer Unit (ReLU) non-linearity. We did not in-
clude any pooling operation in the fusion network as they
provide local invariance which is not desired for the face
landmark localization task. Task-speciﬁc loss functions are
then used to learn the weights of the network.
3.2. Training

We use AFLW[20] dataset for training the Hyper-net.
It contains 25, 993 faces in 21, 997 real-world images with
full pose, expression, ethnicity, age and gender variations. It
provides annotations for 21 landmark points per face, along
with the face bounding-box, face pose (yaw, pitch and roll)
and gender information. We randomly select 1000 images
for testing, and keep the rest for training the network. We
formulate different loss functions for training the tasks of
face detection, landmark localization, pose estimation and
gender classiﬁcation.

Face Detection: We use the Selective Search[40] algo-
rithm in a similar manner as RCNN [11] to generate region
proposals for faces in an image. A region having an over-
lap of more than 0.5 with the ground truth bounding box is
considered a positive sample (l = 1). The candidate regions
with overlap less than 0.35 are treated as negative instance
(l = 0). All the other regions are ignored. We use the soft-
max loss function given by (1) for training the face detection
task.

where p is the probability that the candidate region is a face.
The probability values p and 1−p are obtained from the last
fully connected layer for the detection task.

Landmark Localization: We use the 21 point markup
for face landmark location as provided in the AFLW[20]
dataset. Since the faces have full pose variations, some of
the landmark points are invisible. The dataset provides the
annotations for the visible landmarks. We consider the re-
gions with overlap greater than 0.35 with the ground truth
for learning this task, while ignore the rest. A region can be
characterized by {x, y, w, h} where (x, y) is the co-ordinate
of the center of the region and w,h is the width and height
of the region respectively. Each visible landmark point is
shifted with respect to the region center (x, y), and normal-
ized by (w, h) as given by (2)

(cid:18) xi − x

w

(cid:19)

yi − y

h

,

(ai, bi) =

lossD = −(1 − l) · log(1 − p) − l · log(p),

(1)

lossP =

.

(2)

where losst is the individual loss corresponding to the tth
task. The weight parameter λt is decided based on the

N(cid:88)

i=1

N(cid:88)

i=1

where (xi, yi)’s are the given ground truth ﬁducial co-
ordinates. The (ai, bi)’s are treated as labels for train-
ing the landmark localization task using the Euclidean loss
weighted by the visibility factor. The labels for landmarks
which are not visible are taken to be (0, 0). The loss in pre-
dicting the landmark location can be computed from (3)

lossL =

1
2N

vi(( ˆxi − ai)2 + (( ˆyi − bi)2),

(3)

where ( ˆxi, ˆyi) is the ith landmark location predicted by the
network, relative to a given region, N is the total number of
landmark points (21 for AFLW[20]). The visibility factor
vi is 1 if the ith landmark is visible in the candidate region,
else it is 0. This implies that there is no loss corresponding
to invisible points and hence they do not take part during
back-propagation.

Learning Visibility: We also learn the visibility factor
in order to test the presence of the predicted landmark. For a
given region with overlap higher than 0.35, we use a simple
Euclidean loss to train the visibility as shown in (4)

lossV =

1
N

( ˆvi − vi)2 ,

(4)

where ˆvi is the predicted visibility of ith landmark. The
true visibility vi is 1 if the ith landmark is visible in the
candidate region, else it is 0.

Pose Estimation: We use the Euclidean loss to train the
head pose estimates of roll(p1), pitch(p2) and yaw(p3). We
compute the loss for a candidate region having an overlap
more than 0.5 with the ground truth, from (5)

( ˆp1 − p1)2 + ( ˆp2 − p2)2 + ( ˆp3 − p3)2

3

,

(5)

where ( ˆp1, ˆp2, ˆp3) are the estimated pose labels.

Gender Recognition: Predicting gender is a two class
problem similar to face detection. For a candidate region
with overlap of 0.5 with the ground truth, we compute the
softmax loss given in (6)

lossG = −(1 − g) · log(1 − p0) − g · log(p1),

(6)

where g = 0 if the gender is male, or else g = 1. (p0, p1) is
the two dimensional probability vector computed from the
network.

The total loss is computed as the weighted sum of the

ﬁve individual losses as shown in (7)

lossf ull =

λtlosst,

(7)

t=5(cid:88)

t=1

importance of the task in the overall loss. We choose
(λD = 1, λL = 5, λV = 0.5, λP = 5, λG = 2) for our
experiments. Higher weights are assigned to landmark lo-
calization and pose estimation tasks as they need spatial ac-
curacy.
3.3. Testing

From a given test image, we ﬁrst extract the candidate
region proposals using[40]. For each of the regions, we
predict the task labels by a forward-pass through Hyper-
net. Only those regions with detection scores above a cer-
tain threshold are classiﬁed as face and considered for the
subsequent tasks. The predicted landmark points are scaled
and shifted to the image co-ordinates using (8)

(xi, yi) = ( ˆxiw + x, ˆyih + y),

(8)

where ( ˆxi, ˆyi) is the predicted location of ith landmark from
the network, and {x, y, w, h} are the region parameters de-
ﬁned in (2). Points obtained with predicted visibility less
than a certain threshold are marked invisible. The pose la-
bels obtained from the network are the estimated roll, pitch
and yaw for the face region. The gender is assigned accord-
ing to the label with maximum predicted probability.

The fact that we obtain the landmark locations along
with the detections, enables us to improve the post-
processing step so that all the tasks beneﬁt from it. We
propose two novel methods: recursive region proposals and
landmark-based k-NMS to improve the recall performance.
Recursive Region Proposals: We use a fast version of
Selective Search[40] which extracts around 2000 regions
from an image. It is quite possible that some faces with poor
illumination or small size fail to get captured by any candi-
date region with a high overlap. The network would fail to
detect that face due to low score. In these situations, it is
desirable to have a candidate box which precisely captures
the face. Hence, we generate a new candidate bounding box
from the predicted landmark points using the FaceRectCal-
culator provided by [20]. The new region, being more local-
ized yields a higher detection score and the corresponding
tasks output, thus increasing the recall. The usefulness of
this step can be appreciated from Figure 3.

Landmarks-based k-NMS: Traditional way of non-
maximum suppression involves selecting the top scoring
region and discarding all the other regions with overlap
more than a certain threshold. This method can fail in
the following two scenarios: 1) If a region correspond-
ing to the same detected face has less overlap with the
highest scoring region, it can be detected as a separate
face. 2) The highest scoring region might not always be
localized well for the face, which can create some dis-
crepancy if two faces are close together. To overcome
these issues, we perform NMS on a new region whose
bounding box is deﬁned by the boundary co-ordinates as

Figure 3. Candidate face region (red box on left) obtained using
Selective Search gives a low score for face detection, while land-
marks are correctly localized. We generate a new face region (red
box on right) using the landmarks information and feed it through
the network to increase the detection score.

[mini xi, mini yi, maxi xi, maxi yi]. In this way, the candi-
date regions would get close to each other, thus decreasing
the ambiguity of the overlap and improving the face local-
ization.

We apply landmarks-based k-NMS to keep the top k
boxes, based on the detection scores. The detected face cor-
responds to the region with maximum score. The landmark
points, pose estimates and gender classiﬁcation scores are
decided by the median of the top k boxes obtained. Hence,
the predictions do not rely only on one face region, but con-
siders the votes from top k regions for generating the ﬁnal
output. From our experiments, we found that best results
are obtained with the value of k being 5.

4. Experimental Results

We evaluated the proposed HyperFace method on six
challenging datasets: Annotated Face in-the-Wild (AFW)
[51] for evaluating face detection, landmark localization,
and pose estimation; Annotated Facial Landmarks in the
Wild (AFLW) [20] for evaluating landmark localization and
pose estimation; Face Detection Dataset and Benchmark
(FDDB) [17] and PASCAL faces [45] for evaluating face
detection; Large-scale CelebFaces Attributes (CelebA) [28]
and LFWA [16] for evaluating gender recognition. Our
method was trained on the AFLW dataset using Caffe [18].
4.1. Face Detection

We show face detection results for AFW, FDDB and
PASCAL datasets. The AFW dataset [51] was collected
from Flickr and the images in this dataset contain large vari-
ations in appearance and viewpoint. In total there are 205

Figure 4. Performance evaluation on (a) the AFW dataset, (b) the FDDB dataset, (c) the PASCAL faces dataset. The numbers in the legend
are the mean average precision for the corresponding datasets.

(c)

(a)

(b)

images with 468 faces in this dataset. The FDDB dataset
[17] consists of 2,845 images containing 5,171 faces col-
lected from news articles on the Yahoo website. This dataset
is the most widely used benchmark for unconstrained face
detection. The PASCAL faces dataset [45] was collected
from the test set of PASCAL person layout dataset, which
is a subset from PASCAL VOC [7]. This dataset contains
1335 faces from 851 images with large appearance varia-
tions. For improved face detection performance, we learn
a SVM classiﬁer on top of f cdetection features using the
training splits from the FDDB dataset.

Some of the recent published methods compared in our
evaluations include DP2MFD [32], Faceness [47], Head-
Hunter [29], JointCascade [4], CCF [46], SquaresChnFtrs-5
[29], CascadeCNN [24], Structured Models [], DDFD [9],
NDPFace [27], PEP-Adapt [23], TSM [51], as well as three
commercial systems Face++, Picasa and Face.com.

The precision-recall curves of different detectors cor-
responding to the AFW and the PASCAL faces datasets
are shown in Figures 4 (a) and (c), respectively. Figure 4
(b) compares the performance of different detectors using
the Receiver Operating Characteristic (ROC) curves on the
FDDB dataset. As can be seen from these ﬁgures, our
method outperforms all the academic and commercial de-
tectors on the AFW and PASCAL datasets and performs
comparably to recently published deep learning-based face
detection methods such as DP2MFD [32] and faceness [47]
on the FDDB dataset 1.
4.2. Landmark Localization

We evaluate the performance of different landmark lo-
calization algorithms on the AFW [51] and AFLW [20]
datasets. Some of the methods compared include Multiview
Active Appearance Model-based method (Multi. AAM)
[51], Constrained Local Model (CLM) [33], Oxford facial
landmark detector [8], Zhu [51], FaceDPL [52], JointCas-
cade [4], Convolutional Latent Variable Models (CLVM)

1http://vis-www.cs.umass.edu/fddb/results.html

[15], Ensemble of Regression Trees (ERT) [19], Gauss-
Newton Deformable Part Models (GN-DPM) [39], and
Tasks-Constrained Deep Convolutional Network (TCDCN)
[50].

Figure 5 compares the performance of different land-
mark localization methods. In this ﬁgure, (*) indicates that
models that are evaluated on near frontal faces or use hand-
initialization [51]. We compute the error as the mean dis-
tance between the predicted and ground truth keypoints,
normalized by the face size. For AFLW, we calculate the
error using all the visible keypoints. To evaluate on AFW,
we adopt the same protocol as deﬁned in [52]. The number
in the legend indicates the fraction of test faces with less
than 5% error. As can be seen from this ﬁgure, our method
outperforms many recent state-of-the-art landmark localiza-
tion methods including FaceDPL [52], TCDCN [50], ERT
[19], GN-DPM [39] and CLVM [15] on both datasets. This
clearly shows that while most of the methods work well on
frontal faces, the proposed method is able to predict land-
marks for faces with full pose variations.

(a)

(b)

Figure 5. Cumulative error distribution curves for landmark local-
ization on (a) the AFLW dataset and (b) the AFW dataset. The
numbers in the legend are the fraction of testing faces that have
average error below (5%) of the face size.

0.50.60.70.80.91Recall0.50.60.70.80.91PrecisionESVM (ap=77.2%)Multi. HoG (ap=81.6%)DPM re4 (ap=88.6%)Shen et al. (ap=89.0%)face++face.comDP2MFD (ap=93.3%)Google PicasaJoint cascade (ap=95.6%)Structured models (ap=95.1%)Headhunter (ap=97.1%)Faceness (ap=97.2%) Hyperface (ap=97.5%)False positive02004006008001000True positive rate0.40.50.60.70.80.91HyperFace (0.893)DP2MFD (0.913)Faceness (0.903)HeadHunter (0.871)DPM(HeadHunter) (0.864)JointCascade(0.863)CCF (0.859)SquaresChnFtrs-5 (0.858)CascadeCNN (0.857)StructuredModels (0.846)DDFD (0.84)NDPFace (0.817)PEP-Adapt (0.809)TSM (0.766)0.00.20.40.60.81.0Recall0.00.20.40.60.81.0PrecisionHyperFace (AP 92.46)DPM(Headhunter) (AP 90.29)Headhunter (AP 89.63)SquaresChnFtrs-5 (AP 85.57)Structured Models (AP 83.87)TSM (AP 76.35)Sky Biometry (AP 68.57)OpenCV (AP 61.09)W.S. Boosting (AP 59.72)Picasa Face++ 00.050.10.150.20.25Average Error Normalized by Face Size00.10.20.30.40.50.60.70.80.91Fraction of number of test images Zhu (50.3%) CLVM (81.7%) ERT (66.7%) GN-DPM (36%) TCDCN (63.6%) Hyperface (88.5%)00.050.10.15Average localization error as fraction of face size00.20.40.60.81Fraction of the num. of testing facesMulti. AAMs (15.9%)CLM (24.1%)Oxford (57.9%)face.com (69.6%)Face DPL (76.7%)Joint cascade (87.4%) Hyperface (85.2%)4.3. Gender Recognition

We show the gender recognition performance of our
method on the CelebA [28] and LFWA [16] datasets since
these datasets come with gender information. The CelebA
and LFWA datasets contain labeled images selected from
the CelebFaces [36] and LFW [16] datasets, respectively
[28]. CelebA dataset contains 10,000 identities and there
are 200,000 images in total. The LFWA dataset has 13,233
images of 5,749 identities. We compare our approach with
FaceTracer [22], PANDA-w [49], PANDA-1 [49], [25] with
ANet and [28]. The gender recognition performance of dif-
ferent methods is reported in Table 1. On the LFWA dataset,
our method outperforms [28] as well as PANDA [49] and
FaceTracer [22]. On the CelebA dataset, our method per-
forms comparably to [28]. Unlike [28] which uses 180,000
images for training, we only use 20,000 images from valida-
tion dataset to train a SVM classiﬁer on f cgender features.

Method
FaceTracer [22]
PANDA-w [49]
PANDA-1 [49]
[25]+ANet
LNets+ANet [28]
HyperFace

CelebA LFWA

91
93
97
95
98
96.33

84
86
92
91
94
96.25

Table 1. Performance comparison (in %) of gender recognition on
CelebA and LFWA datasets.

4.4. Pose Estimation

We evaluate our method on the AFW [51] and AFLW
[20] datasets. For AFW dataset, we compare our approach
with Multi. AAM [51], Multiview HoG [51], FaceDPL2
[52] and face.com. Note that multiview AAMs are initial-
ized using the ground truth bounding boxes (denoted by *).
Figures 6 (a) and (b) show the cumulative error distribu-
tion curves on AFW and AFLW, respectively. These curves
show the fraction of faces for which the estimated pose is
within some error tolerance. As can be seen from this ﬁg-
ure, the HyperFace method achieves the best performance
on both of these datasets and beats FaceDPL by a large
margin. For AFLW dataset, we show the performance of
our method for different pose angles: roll, pitch and yaw. It
can be seen from the results that the network is able to learn
roll, and pitch information better than yaw. This experiment
clearly shows that the hyperfeatures do contain enough in-
formation to estimate pose reliably.

Several qualitative results of our method on the AFW,
AFLW and FDDB dataset are shown in Figure 7. As can be
seen from this ﬁgure, our method is able to simultaneously

2Available

at:

http://www.ics.uci.edu/˜dramanan/

software/face/face_journal.pdf

(a)

(b)

Figure 6. Cumulative error distribution curves for pose estimation
on (a) AFLW dataset. The numbers in legend show the mean error
in degrees while estimating roll, pitch and yaw. (b) AFW dataset.
The numbers in the legend are the percentage of faces that are
labeled within ±15◦ error tolerance.

perform all the four tasks on images containing extreme
pose, illumination, and resolution variations with cluttered
background.
4.5. Runtime

The Hyperface method was tested on a machine with 8
cores and GTX TITAN-X gpu. The overall time taken to
perform all the four tasks was 4s per image. The limita-
tion was not because of CNN, but due to selective search
which takes approximately 3s to generate candidate region
proposals. One forward pass through the Hyper-net takes
only 0.2s.

5. Conclusions

In this paper, we presented a multi-task deep learn-
ing method called HyperFace for simultaneously detecting
faces, localizing landmarks, estimating head pose and iden-
tifying gender. Extensive experiments using various pub-
licly available unconstrained datasets demonstrate the ef-
fectiveness of our method on all four tasks. In future, we
will evaluate the performance of our method on other appli-
cations such as simultaneous human detection and human
pose estimation, object recognition and pedestrian detec-
tion.

Acknowledgments

This research is based upon work supported by the Of-
ﬁce of the Director of National Intelligence (ODNI), In-
telligence Advanced Research Projects Activity (IARPA),
via IARPA R&D Contract No. 2014-14071600012. The
views and conclusions contained herein are those of the au-
thors and should not be interpreted as necessarily represent-
ing the ofﬁcial policies or endorsements, either expressed
or implied, of the ODNI, IARPA, or the U.S. Government.
The U.S. Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwithstanding
any copyright annotation thereon.

051015202530Pose Estimation Error in degrees00.10.20.30.40.50.60.70.80.91Fraction of Number of Face ImagesRoll (4.27°)Pitch (6.55°)Yaw (7.64°)051015202530Pose estimation error (in degrees)00.20.40.60.81Fraction of the num. of testing facesMulti. HoG (74.6%)Multi. AAMs (36.8%)face.com (64.3%)FaceDPL (89.4%) Hyperface continuous (92.6%) Hyperface discrete (97.9%)Figure 7. The blue boxes denote detected male faces, while pink boxes denote female faces. The green dots provide the landmark locations.
Pose estimates for each face are shown on top of the boxes in the order of roll, pitch and yaw.

References
[1] A. Agarwal and B. Triggs. Multilevel image coding with hy-
perfeatures. International Journal of Computer Vision, pages
15–27, 2008. 1

[2] V. Balasubramanian, J. Ye, and S. Panchanathan. Biased
manifold embedding: A framework for person-independent
In Computer Vision and Pattern
head pose estimation.
Recognition, 2007. CVPR ’07. IEEE Conference on, pages
1–7, June 2007. 3

[3] X. Cao, Y. Wei, F. Wen, and J. Sun. Face alignment by ex-
plicit shape regression. International Journal of Computer
Vision, 107(2):177–190, 2014. 3

[4] D. Chen, S. Ren, Y. Wei, X. Cao, and J. Sun.

Joint cas-
cade face detection and alignment.
In D. Fleet, T. Pajdla,
B. Schiele, and T. Tuytelaars, editors, European Conference
on Computer Vision, volume 8694, pages 109–122. 2014. 1,
2, 3, 6

[5] T. F. Cootes, C. J. Taylor, D. H. Cooper, and J. Graham.
Active shape models&mdash;their training and application.
Comput. Vis. Image Underst., 61(1):38–59, Jan. 1995. 3

[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei.
Imagenet: A large-scale hierarchical image database.
In Computer Vision and Pattern Recognition, 2009. CVPR
2009. IEEE Conference on, pages 248–255. IEEE, 2009. 3
[7] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and
A. Zisserman. The pascal visual object classes (voc) chal-
lenge. International Journal of Computer Vision, 88(2):303–
338, June 2010. 6

[8] M. R. Everingham, J. Sivic, and A. Zisserman. Hello! my
name is... buffy” – automatic naming of characters in tv
In Proceedings of the British Machine Vision Con-
video.
ference, pages 92.1–92.10, 2006. 6

[9] S. S. Farfade, M. Saberian, and L.-J. Li. Multi-view face
detection using deep convolutional neural networks. In In-

ternational Conference on Multimedia Retrieval, 2015. 1, 3,
6

[10] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ra-
manan. Object detection with discriminatively trained part-
based models. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 32(9):1627–1645, Sept 2010. 3

[11] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. In Computer Vision and Pattern Recognition,
2014. 4

[12] G. Gkioxari, B. Hariharan, R. B. Girshick, and J. Malik.
R-cnns for pose estimation and action detection. CoRR,
abs/1406.5212, 2014. 2

[13] B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik. Hyper-
columns for object segmentation and ﬁne-grained localiza-
tion. In Computer Vision and Pattern Recognition (CVPR),
2015. 2

[14] N. Hu, W. Huang, and S. Ranganath. Head pose estimation
by non-linear embedding and mapping. In Image Process-
ing, 2005. ICIP 2005. IEEE International Conference on,
volume 2, pages II–342–5, Sept 2005. 3

[15] P. Hu and D. Ramanan. Bottom-up and top-down rea-
soning with convolutional latent-variable models. CoRR,
abs/1507.05699, 2015. 6

[16] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller.
Labeled faces in the wild: A database for studying face
recognition in unconstrained environments. Technical Re-
port 07-49, University of Massachusetts, Amherst, Oct.
2007. 5, 6, 7

[17] V. Jain and E. Learned-Miller. Fddb: A benchmark for face
detection in unconstrained settings. Technical Report UM-
CS-2010-009, University of Massachusetts, Amherst, 2010.
5, 6

[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolu-

2°,-15°,-10°2°,-13°,-48°-6°,-14°,59°11°,-9°,-54°2°,-4°,14°3°,-3°,50°2°,-24°,-43°13°,-9°,-74°11°,-1°,-82°-6°,11°,-25°3°,-5°,24°-29°,-30°,4°7°,-20°,-26°-4°,-4°,-18°1°,-19°,32°-6°,8°,91°11°,1°,-69°-4°,-3°,-20°-1°,-16°,-25°0°,-19°,-41°3°,6°,-11°8°,-12°,26°9°,-9°,16°1°,-2°,80°tional architecture for fast feature embedding. arXiv preprint
arXiv:1408.5093, 2014. 3, 5

[19] V. Kazemi and J. Sullivan. One millisecond face alignment
with an ensemble of regression trees. In IEEE Conference
on Computer Vision and Pattern Recognition, pages 1867–
1874, June 2014. 6

[20] M. Kostinger, P. Wohlhart, P. Roth, and H. Bischof. Anno-
tated facial landmarks in the wild: A large-scale, real-world
In IEEE Inter-
database for facial landmark localization.
national Conference on Computer Vision Workshops, pages
2144–2151, Nov 2011. 4, 5, 6, 7

[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
F. Pereira, C. Burges, L. Bottou, and K. Weinberger, edi-
tors, Advances in Neural Information Processing Systems 25,
pages 1097–1105. Curran Associates, Inc., 2012. 3

[22] N. Kumar, P. N. Belhumeur, and S. K. Nayar. FaceTracer: A
Search Engine for Large Collections of Images with Faces.
In European Conference on Computer Vision (ECCV), pages
340–353, Oct 2008. 3, 7

[23] H. Li, G. Hua, Z. Lin, J. Brandt, and J. Yang. Probabilis-
tic elastic part model for unsupervised face detector adapta-
tion. In IEEE International Conference on Computer Vision,
pages 793–800, Dec 2013. 3, 6

[24] H. Li, Z. Lin, X. Shen, J. Brandt, and G. Hua. A convolu-
In IEEE
tional neural network cascade for face detection.
Conference on Computer Vision and Pattern Recognition,
pages 5325–5334, June 2015. 3, 6

[25] J. Li and Y. Zhang. Learning surf cascade for fast and ac-
In IEEE Conference on Computer
curate object detection.
Vision and Pattern Recognition, pages 3468–3475, 2013. 7
[26] L. Liang, R. Xiao, F. Wen, and J. S. 0001. Face alignment via
component-based discriminative search.
In D. A. Forsyth,
P. H. S. Torr, and A. Zisserman, editors, ECCV (2), volume
5303 of Lecture Notes in Computer Science, pages 72–85.
Springer, 2008. 3

[27] S. Liao, A. Jain, and S. Li. A fast and accurate unconstrained
IEEE Transactions on Pattern Analysis and

face detector.
Machine Intelligence, 2015. 3, 6

[28] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face
attributes in the wild. In International Conference on Com-
puter Vision, Dec. 2015. 3, 5, 6, 7

[29] M. Mathias, R. Benenson, M. Pedersoli, and L. Van Gool.
Face detection without bells and whistles. In European Con-
ference on Computer Vision, volume 8692, pages 720–735.
2014. 3, 6

[30] I. Matthews and S. Baker. Active appearance models revis-

ited. Int. J. Comput. Vision, 60(2):135–164, Nov. 2004. 3

[31] E. Murphy-Chutorian and M. Trivedi. Head pose estimation
in computer vision: A survey. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 31(4):607–626, April
2009. 3

[32] R. Ranjan, V. M. Patel, and R. Chellappa. A deep pyramid
deformable part model for face detection. In International
Conference on Biometrics Theory, Applications and Systems,
2015. 1, 3, 6

[33] J. Saragih, S. Lucey, and J. Cohn. Deformable model ﬁtting
by regularized landmark mean-shift. International Journal
of Computer Vision, 91(2):200–215, 2011. 6

[34] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. Lecun.
Pedestrian detection with unsupervised multi-stage feature
learning. In Proceedings of the 2013 IEEE Conference on
Computer Vision and Pattern Recognition, CVPR ’13, pages
3626–3633, Washington, DC, USA, 2013. IEEE Computer
Society. 2

[35] S. Srinivasan and K. Boyer. Head pose estimation using
view based eigenspaces. In Pattern Recognition, 2002. Pro-
ceedings. 16th International Conference on, volume 4, pages
302–305 vol.4, 2002. 3

[36] Y. Sun, Y. Chen, X. Wang, and X. Tang. Deep learning
face representation by joint identiﬁcation-veriﬁcation.
In
Advances in Neural Information Processing Systems, pages
1988–1996. 2014. 7

[37] Y. Sun, X. Wang, and X. Tang. Deep convolutional network
cascade for facial point detection. In Proceedings of the 2013
IEEE Conference on Computer Vision and Pattern Recogni-
tion, CVPR ’13, pages 3476–3483, Washington, DC, USA,
2013. IEEE Computer Society. 3

[38] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. CoRR, abs/1409.4842,
2014. 3

[39] G. Tzimiropoulos and M. Pantic. Gauss-newton deformable
In IEEE Con-
part models for face alignment in-the-wild.
ference on Computer Vision and Pattern Recognition, pages
1851–1858, June 2014. 6

[40] K. E. A. van de Sande, J. R. R. Uijlings, T. Gevers, and
A. W. M. Smeulders. Segmentation as selective search for
object recognition. In Proceedings of the 2011 International
Conference on Computer Vision, ICCV ’11, pages 1879–
1886, Washington, DC, USA, 2011. IEEE Computer Society.
4, 5

[41] P. A. Viola and M. J. Jones. Robust real-time face detec-
tion. International Journal of Computer Vision, 57(2):137–
154, 2004. 3

[42] X. Xiong and F. D. la Torre. Global supervised descent

method. In CVPR, 2015. 3

[43] Xuehan-Xiong and F. De la Torre.

Supervised descent
In IEEE
method and its application to face alignment.
Conference on Computer Vision and Pattern Recognition
(CVPR), 2013. 3

[44] J. Yan, Z. Lei, D. Yi, and S. Z. Li. Learn to combine multiple
hypotheses for accurate face alignment. In Proceedings of
the 2013 IEEE International Conference on Computer Vision
Workshops, ICCVW ’13, pages 392–396, Washington, DC,
USA, 2013. IEEE Computer Society. 3

[45] J. Yan, X. Zhang, Z. Lei, and S. Z. Li. Face detection by
structural models. Image and Vision Computing, 32(10):790
– 799, 2014. 5, 6

[46] B. Yang, J. Yan, Z. Lei, and S. Z. Li. Convolutional chan-
nel features. In IEEE International Conference on Computer
Vision, 2015. 3, 6

[47] S. Yang, P. Luo, C. C. Loy, and X. Tang. From facial parts
responses to face detection: A deep learning approach. In

IEEE International Conference on Computer Vision, 2015.
1, 3, 6

[48] M. D. Zeiler and R. Fergus. Visualizing and understanding

convolutional networks. CoRR, abs/1311.2901, 2013. 1, 3

[49] N. Zhang, M. Paluri, M. Ranzato, T. Darrell, and L. Bourdev.
Panda: Pose aligned networks for deep attribute modeling. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 1637–1644, 2014. 3, 7

[50] Z. Zhang, P. Luo, C. Loy, and X. Tang. Facial landmark de-
tection by deep multi-task learning. In European Conference
on Computer Vision, pages 94–108, 2014. 2, 3, 6

[51] X. Zhu and D. Ramanan. Face detection, pose estimation,
and landmark localization in the wild. In IEEE Conference
on Computer Vision and Pattern Recognition, pages 2879–
2886, June 2012. 1, 2, 3, 5, 6, 7

[52] X. Zhu and D. Ramanan. FaceDPL: Detection, pose estima-
tion, and landmark localization in the wild. preprint 2015. 1,
2, 6, 7

