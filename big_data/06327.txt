Deep Self-Convolutional Activations Descriptor

for Dense Cross-Modal Correspondence

Seungryong Kim1(cid:63), Dongbo Min2, Stephen Lin3, and Kwanghoon Sohn1

1Yonsei University, 2Chungnam National Univerisity, 3Microsoft Research

Abstract. We present a novel descriptor, called deep self-convolutional
activations (DeSCA), designed for establishing dense correspondences
between images taken under diﬀerent imaging modalities, such as dif-
ferent spectral ranges or lighting conditions. Motivated by descriptors
based on local self-similarity (LSS), we formulate a novel descriptor by
leveraging LSS in a deep architecture, leading to better discriminative
power and greater robustness to non-rigid image deformations than state-
of-the-art cross-modality descriptors. The DeSCA ﬁrst computes self-
convolutions over a local support window for randomly sampled patches,
and then builds self-convolution activations by performing an average
pooling through a hierarchical formulation within a deep convolutional
architecture. Finally, the feature responses on the self-convolution acti-
vations are encoded through a spatial pyramid pooling in a circular con-
ﬁguration. In contrast to existing convolutional neural networks (CNNs)
based descriptors, the DeSCA is training-free (i.e., randomly sampled
patches are utilized as the convolution kernels), is robust to cross-modal
imaging, and can be densely computed in an eﬃcient manner that sig-
niﬁcantly reduces computational redundancy. The state-of-the-art per-
formance of DeSCA on challenging cases of cross-modal image pairs is
demonstrated through extensive experiments.

6
1
0
2

 
r
a

 

M
1
2

 
 
]

V
C
.
s
c
[
 
 

1
v
7
2
3
6
0

.

3
0
6
1
:
v
i
X
r
a

1 Introduction

In many computer vision and computational photography applications, images
captured under diﬀerent imaging modalities are used to supplement the data
provided in color images. Typical examples of other imaging modalities include
near-infrared [1,2,3] and dark ﬂash [4] photography. More broadly, photos taken
under diﬀerent imaging conditions, such as diﬀerent exposure settings [5], blur
levels [6,7], and illumination [8], can also be considered as cross-modal [9,10].

Establishing dense correspondences between cross-modal image pairs is es-
sential for combining their disparate information. Although powerful global opti-
mizers may help to improve the accuracy of correspondence estimation to some
extent [11,12], they face inherent limitations without help of suitable match-
ing descriptors [13]. The most popular local descriptor is scale invariant feature
transform (SIFT) [14], which provides relatively good matching performance

(cid:63) This work is done while Seungryong Kim was an intern at Microsoft Research.

2

S. Kim et al.

(a) cost function in A (b) cost function in B (c) cost function in C

Fig. 1. Examples of matching cost proﬁles, computed with diﬀerent descriptors along
the scan lines of A, B, and C for image pairs under severe non-rigid deformations and
illumination changes. Unlike other descriptors, DeSCA yields reliable global minimum.

when there are small photometric variations. However, conventional descriptors
such as SIFT often fail to capture reliable matching evidences in cross-modal
image pairs due to their diﬀerent visual properties [9,10].

Recently, convolutional neural networks (CNNs) based features [15,16,17,18,19]

have emerged as a robust alternative with high discriminative power. However,
CNN-based descriptors cannot satisfactorily deal with severe cross-modality ap-
pearance diﬀerences, since they use shared convolutional kernels across images
which lead to inconsistent responses similar to conventional descriptors [19,20].
Furthermore, they do not scale well for dense correspondence estimation due
to their high computational complexity. Though recent works [21] propose an
eﬃcient method that extracts dense outputs through the deep CNNs, they do
not extract dense CNN features for all pixels individually. More seriously, their
methods were usually designed to perform a speciﬁc task only, e.g., semantic
segmentation, not to provide a general purpose descriptor like ours.

To address the problem of cross-modal appearance changes, feature descrip-
tors have been proposed based on local self-similarity (LSS) [22], which is mo-
tivated by the notion that the geometric layout of local internal self-similarities
is relatively insensitive to imaging properties. The state-of-the-art descriptor for
cross-modal dense correspondence, called dense adaptive self-correlation (DASC)
[10], makes use of LSS and has demonstrated high accuracy and speed on cross-
modal image pairs. However, DASC suﬀers from two signiﬁcant shortcomings.
One is its limited discriminative power due to a limited set of patch sampling
patterns used for modeling internal self-similarities. In fact, the matching per-
formance of DASC may fall well short of CNN-based descriptors on images that
share the same modality. The other major shortcoming is that the DASC descrip-
tor does not provide the ﬂexibility to deal with non-rigid deformations, which
leads to lower robustness in matching.

In this paper, we introduce a novel descriptor, called deep self-convolutional
activations (DeSCA), that overcomes the shortcomings of DASC while providing

−15−10−5051015Ground truth→search rangeMatching costSIFTCNNDASCDSCN−15−10−5051015Ground truth→search rangeMatching costSIFTCNNDASCDSCN−15−10−5051015Ground truth→search rangeMatching costSIFTCNNDASCDSCNDeep Self-Convolutional Activations Descriptor

3

dense cross-modal correspondences. This work is motivated by the observation
that local self-similarity can be formulated in a deep convolutional architecture
to enhance discriminative power and gain robustness to non-rigid deformations.
Unlike the DASC descriptor that selects patch pairs within a support window
and calculates the self-similarity between them, we compute self-convolutional
activations that more comprehensively encode the intrinsic structure by calculat-
ing the self-similarity between randomly selected patches and all of the patches
within the support window. These self-convolutional responses are aggregated
through spatial pyramid pooling in a circular conﬁguration, which yields a rep-
resentation less sensitive to non-rigid image deformations than the ﬁxed patch
selection strategy used in DASC. To further enhance the discriminative power
and robustness, we build hierarchical self-convolutional layers resembling a deep
architecture used in CNN, together with nonlinear and normalization layers. For
eﬃcient computation of DeSCA over densely sampled pixels, we calculate the
self-convolutional activations through fast edge-aware ﬁltering.

DeSCA resembles a CNN in its deep, multi-layer, and convolutional struc-
ture. In contrast to existing CNN-based descriptors, DeSCA requires no training
data for learning convolutional kernels, since the convolutions are deﬁned as the
local self-similarity between pairs of image patches, which yields its robustness to
cross-modal imaging. Fig. 1 illustrates the robustness of DeSCA for image pairs
across non-rigid deformations and illumination changes. In the experimental re-
sults, we show that DeSCA outperforms existing area-based and feature-based
descriptors on various benchmarks.

2 Related Work

Feature Descriptors Conventional gradient-based descriptors, such as SIFT
[14] and DAISY [23], as well as intensity comparison-based binary descriptors,
such as BRIEF [24], have shown limited performance in dense correspondence
estimation between cross-modal image pairs. Besides these handcrafted features,
several attempts have been made using machine learning algorithms to derive
features from large-scale datasets [15,25]. A few of these methods use deep con-
volutional neural networks (CNNs) [26], which have revolutionized image-level
classiﬁcation, to learn discriminative descriptors for local patches. For designing
explicit feature descriptors based on a CNN architecture, immediate activations
are extracted as the descriptor [15,16,17,18,19], and have been shown to be ef-
fective for this patch-level task. However, even though CNN-based descriptors
encode a discriminative structure with a deep architecture, they have inherent
limitations in cross-modal image correspondence because they are derived from
convolutional layers using shared patches or volumes [19,20]. Furthermore, they
cannot in practice provide dense descriptors in the image domain due to their
prohibitively high computational complexity.

To estimate cross-modal correspondences, variants of the SIFT descriptor
have been developed [27], but these gradient-based descriptors maintain an in-
herent limitation similar to SIFT in dealing with image gradients that vary dif-
ferently between modalities. For illumination invariant correspondences, Wang

4

S. Kim et al.

(a)

(b)

(c)

Fig. 2. Illustration of (a) LSS [22] using center-biased dense max pooling, (b) DASC
[10] using patch-wise receptive ﬁeld pooling, and (c) our DeSCA. Boxes, formed by
solid and dotted lines, depict source and target patches. DeSCA incorporates a circular
spatial pyramid pooling on hierarchical self-convolutional activations.

et al. proposed the local intensity order pattern (LIOP) descriptor [28], but se-
vere radiometric variations may often alter the relative order of pixel intensities.
Simo-Serra et al. proposed the deformation and light invariant (DaLI) descriptor
[29] to provide high resilience to non-rigid image transformations and illumina-
tion changes, but it cannot provide dense descriptors in the image domain due
to its high computational time.

Schechtman and Irani introduced the LSS descriptor [22] for the purpose of
template matching, and achieved impressive results in object detection and re-
trieval. By employing LSS, many approaches have tried to solve for cross-modal
correspondences [30,31,32]. However, none of these approaches scale well to dense
matching in cross-modal images due to low discriminative power and high com-
plexity. Inspired by LSS, Kim et al. recently proposed the DASC descriptor to
estimate cross-modal dense correspondences [10]. Though it can provide satis-
factory performance, it is not able to handle non-rigid deformations and has
limited discriminative power due to its ﬁxed patch pooling scheme.

Area-Based Similarity Measures A popular measure for registration of
cross-modal medical images is mutual information (MI) [33], based on the en-
tropy of the joint probability distribution function, but it provides reliable per-
formance only for variations undergoing a global transformation [34]. Although
cross-correlation based methods such as adaptive normalized cross-correlation
(ANCC) [35] produce satisfactory results for locally linear variations, they are
less eﬀective against more substantial modality variations. Robust selective nor-
malized cross-correlation (RSNCC) [9] was proposed for dense alignment be-
tween cross-modal images, but as an intensity based measure it can still be
sensitive to cross-modal variations. Recently, DeepMatching [36] was proposed
to compute dense correspondences by employing a hierarchical pooling scheme
like CNN, but it is not designed to handle cross-modal matching.

3 Background
Let us deﬁne an image as fi : I → R for pixel i, where I ⊂ N2 is a discrete
image domain. Given the image fi, a dense descriptor Di : I → RL with a
feature dimension of L is deﬁned on a local support window Ri of size MR.

Deep Self-Convolutional Activations Descriptor

5

i

ldLSS
(l) = max
j∈Bi(l)

dLSS
i

{exp(−S(Fi,Fj)/σc)},

Unlike conventional descriptors, relying on common visual properties across
images such as color and gradient, LSS-based descriptors provide robustness to
diﬀerent imaging modalities since internal self-similarities are preserved across
cross-modal image pairs [22,10]. As shown in Fig. 2(a), the LSS discretizes the
correlation surface on a log-polar grid, generates a set of bins, and then stores
the maximum correlation value of each bin. Formally, it generates an LLSS × 1
feature vector DLSS
(l) computed as

(l) for l ∈ {1, ..., LLSS}, with dLSS

i =(cid:83)

i

(1)
where log-polar bins are deﬁned as Bi = {j|j ∈ Ri, ρr−1 < |i − j| ≤ ρr, θa−1 <
∠(i − j) ≤ θa} with a log radius ρr for r ∈ {1,··· , Nρ} and a quantized angle
θa for a ∈ {1,··· , Nθ} with ρ0 = 0 and θ0 = 0. S(Fi,Fj) is a correlation surface
between a patch Fi and Fj of size MF , computed using sum of square diﬀerences.
Each pair of r and a is associated with a unique index l. Though LSS provides
robustness to modality variations, its signiﬁcant computation does not scale well
for estimating dense correspondences in cross-modal images.
Inspired by the LSS [22], the DASC [10] encodes the similarity between patch-
wise receptive ﬁelds sampled from a log-polar circular point set Pi as shown in
Fig. 2(b). It is deﬁned such that Pi = {j|j ∈ Ri,|i − j| = ρr, ∠(i − j) = θa},
which has a higher density of points near a center pixel, similar to DAISY [23].
The DASC is encoded with a set of similarities between patch pairs of sampling
patterns selected from Pi such that DDASC
(l) for l ∈ {1, ..., LDASC}:
(2)
where si,l and ti,l are the lth selected sampling pattern from Pi at pixel i. The
patch-wise similarity is computed with an exponential function with a bandwidth
of σc, which has been widely used for robust estimation [37]. C(Fsi,l ,Fti,l ) is
computed using an adaptive self-correlation measure. While the DASC descriptor
has shown satisfactory results for cross-modal dense correspondence [10], its
randomized receptive ﬁeld pooling has limited descriptive power and does not
accommodate non-rigid deformations.

(l) = exp(−(1 − |C(Fsi,l ,Fti,l )|)/σc),

=(cid:83)

ldDASC

dDASC
i

i

i

4 The DeSCA Descriptor
4.1 Motivation and Overview

Inspired by DASC [10], our DeSCA descriptor also measures an adaptive self-
correlation between two patches. We, however, adopt a diﬀerent strategy for
selecting patch pairs, and build self-convolutional activations that more com-
prehensively encode self-similar structure to improve the discriminative power
and the robustness to non-rigid image deformation (Sec. 4.2). Motivated by the
deep architecture of CNN-based descriptors [19], we further build hierarchical
self-convolution activations to enhance the robustness of the DeSCA descriptor
(Sec. 4.4). Densely sampled descriptors are eﬃciently computed over an entire
image using a method based on fast edge-aware ﬁltering (Sec. 4.3). Fig. 2(c)
illustrates the DeSCA descriptor, which incorporates a circular spatial pyramid
pooling on hierarchical self-convolutional activations.

6

S. Kim et al.

(a)

(b)

(c)

(d)

Fig. 3. Computation of single self-convolutional activation (SiSCA). (a) A local support
window Ri of size M 2R with NK random samples. (b) For each random patch, a self-
convolutional surface is computed using an adaptive self-correlation measure. (c) A
self-convolutional activation is then obtained through circular spatial pyramid pooling
(C-SPP). (d) The activation from C-SPP is concatenated as 1-D feature vector.

is NSB =(cid:80)NS

Fig. 4. Examples of the circular spatial pyramidal bins SBi. The total number of bins

(a) s = 1 (b) s = 2 (c) s = 3 (b) s = 4 (e) s = 5

s=2 2s + 1, where NS represents the pyramid level.

4.2 SiSCA: Single Self-Convolutional Activation

To simultaneously leverage the beneﬁts of self-similarity in DASC [10] and the
deep convolutional archiecture of CNNs while overcoming the limitations of each
method, our approach builds self-convolutional activations. Unlike DASC [10],
the feature response is obtained through circular spatial pyramid pooling. We
start by describing a single-layer version of DeSCA, which we denote as SiSCA.

Self-Convolutions To build a self-convolutional activation, we randomly select
NK points from a log-polar circular point set Pi deﬁned within a local support
window Ri. We convolve a patch Fri,k centered at the k-th point ri,k with all
patches Fj, which is deﬁned for j ∈ Ri and k ∈ {1, ..., NK} as Fig. 3(b). Similar
to DASC [10], the similarity C(Fri,k ,Fj) between patch pairs is measured using
an adaptive self-correlation, which is known to be eﬀective in addressing cross-
modality. With (i, k) omitted for simplicity, C(Fr,Fj) is computed as follows:

(cid:80)
r(cid:48),j(cid:48) ωr,r(cid:48)(fr(cid:48) − Gr,r)(fj(cid:48) − Gr,j)
r(cid:48) ωr,r(cid:48)(fr(cid:48) − Gr,r)
for r(cid:48) ∈ Fr and j(cid:48) ∈ Fj. Gr,r =(cid:80)
(cid:80)

r(cid:48),j(cid:48) ωr,r(cid:48)fj(cid:48) represent
weighted averages of fr(cid:48) ∈ Fr and fj(cid:48) ∈ Fj. Similar to DASC [10], the weight
ωr,r(cid:48) represents how similar two pixels r and r(cid:48) are, and is normalized, i.e.,
r(cid:48) ωr,r(cid:48) = 1. It may be deﬁned using any form of edge-aware weighting [38,39].

C(Fr,Fj) =

r(cid:48),j(cid:48) ωr,r(cid:48)(fj(cid:48) − Gr,j)

r(cid:48) ωr,r(cid:48)fr(cid:48) and Gr,j =(cid:80)

,

(3)

(cid:112)(cid:80)

(cid:113)(cid:80)

Circular Spatial Pyramid Pooling To encode the feature responses on the
self-convolutional surface, we propose a circular spatial pyramid pooling (C-SPP)

M,ikrirKNjjr1i3i2ii4iKSBNNiDeep Self-Convolutional Activations Descriptor

7

(a)

(b)

(c)

(d)

Fig. 5. Eﬃcient computation of self-convolutional activations on the image. (a) An
image fi with a doubled support window R∗
i and random samples. (b) 1-D vectorial self-
convolutional surface. (c) Self-convolutional activations. (d) Activations after C-SPP.
With an eﬃcient edge-aware ﬁltering and activation reformulation, self-convolutonal
activations are computed eﬃciently in a dense manner.

scheme, which pools the responses within each hierarchical spatial bin, similar
to a spatial pyramid pooling (SPP) [20,40,41] but in a circular conﬁguration.
Note that many existing descriptors also adopt a circular pooling scheme thanks
to its robustness based on a higher pixel density near a central pixel [22,23,24].
We further encodes more structure information with a C-SPP.
The circular pyramidal bins SBi(u) are deﬁned from log-polar circular bins
Bi, where u indexes all pyramidal level s ∈ {1, ..., NS} and all bins in each level
s as in Fig. 4. The circular pyramidal bin at the top of pyramid, i.e., s = 1,
ﬁrst encompasses all of bins Bi. At the second level, i.e., s = 2, it is deﬁned
by dividing Bi into quadrants. For further lower pyramid levels, i.e., s > 2, the
circular pyramidal bins are deﬁned diﬀerently according to whether s is odd or
even. For an odd s, the bins are deﬁned by dividing bins in upper level into two
parts along the radius. For an even s, they are deﬁned by dividing bins in upper
level into two parts with respect to the angle. The set of all circular pyramidal
u SBi(u) for u ∈ {1, ..., NSB}, where the

bins SBi is denoted such that SBi =(cid:83)
number of circular spatial pyramid bins is deﬁned as NSB =(cid:80)NS

s=2 2s + 1.

As illustrated in Fig. 3(c), the feature responses are ﬁnally max-pooled on
the circular pyramidal bins SBi(u) of each self-convolutional surface C(Fri,k ,Fj),
yielding a feature response

{C(Fri,k ,Fj)},

u ∈ {1, ..., NSB}.

(4)

hi(k, u) = max

j∈SBi(u)

tions ˆhi(l) =(cid:83){k,u} hi(k, u) where l indexes for all k and u.

This pooling is repeated for all k ∈ {1, ..., NK}, yielding accumulated activa-

Interestingly, LSS [22] also uses the max pooling strategy to mitigate the
eﬀects of non-rigid image deformation. However, max pooling in the 2-D self-
correlation surface of LSS [22] loses ﬁne-scale matching details as reported in
[10]. By contrast, DeSCA employs circular spatial pyramid pooling in the 3-
D self-correlation surface that provides a more discriminative representation of
self-similarities, thus maintaining ﬁne-scale matching details as well as providing
robustness to non-rigid image deformations.

*ii2MHfWf2M24Mi2KNMiKSBNNi8

S. Kim et al.

Fig. 6. Visualization of SiSCA and DeSCA descriptor. Our architecture consists of a
hierarchical self-convolutional layer, circular spatial pyramid pooling layer, non-linear
gating layer, and normalization layer.

Non-linear Gating and Nomalization The ﬁnal feature responses are passed
through a non-linear and normalization layer to mitigate the eﬀects of outliers.
With accumulated activations ˆhi, the single self-convolution activiation (SiSCA)
descriptor DSiSCA
(l) is computed for l ∈ {1, ..., LSiSCA} through a
non-linear gating layer:

=(cid:83)

ldSiSCA

i

i

dSiSCA
i

(l) = exp(−(1 − |ˆhi(l)|)/σc),

(5)

where σc is a Gaussian kernel bandwidth. The size of features obtained from
the SiSCA becomes LSiSCA = NKNSB. Finally, dSiSCA
(l) for each pixel i is
normalized with an L-2 norm for all l.

i

4.3 Eﬃcient Computation for Dense Description

The most time-consuming part of DeSCA is in constructing self-convolutional
surfaces C(Fri,k ,Fj) for k and j, where NKM 2R computations of (3) are needed
for each pixel i. Straightforward computation of a weighted summation using ω
in (3) would require considerable processing with a computational complexity
of O(IMF NKM 2R), where I = Hf Wf represents the image size (height Hf and
width Wf ). To expedite processing, we utilize fast edge-aware ﬁltering [38,39]
and propose a pre-computation scheme for convolutional surfaces.
Similar to DASC [10], we compute C(Fri,k ,Fj) eﬃciently by ﬁrst rearranging
the sampling patterns (ri,k, j) into reference-biased pairs (i, jr) = (i, i + ri,k − j).
C(Fi,Fjr ) can then be expressed as

i(cid:48) ωi,i(cid:48)f 2
i(cid:48).
C(Fi,Fjr ) can be eﬃciently computed using any form of fast edge-aware ﬁlter

ωi,i(cid:48)fi(cid:48)fj(cid:48)

ωi,i(cid:48)f 2
j(cid:48)

i(cid:48),j(cid:48)

r

i(cid:48),j(cid:48)

r

r

C(Fi,Fjr ) =

where Gi,ijr = (cid:80)

(cid:112)Gi,i2 − (Gi,i)2 ·(cid:113)Gi,j2

Gi,ijr − Gi,i · Gi,jr

r

= (cid:80)

, Gi,j2

r

r

(6)

,

− (Gi,jr )2

, and Gi,i2 = (cid:80)

.........HW12RHW4M2KRHWNMON2RHW2M2RHWM2KSPRHW(N+N)M2KRHWNMKSPSBHW(N+N)NkSBHWNNC-SPPNorm.C-SPPNon-linearDeSCAdescriptor:Aver.poolingimageSiSCAdescriptor:Non-linearNorm.Self-conv.Aver.poolingDeep Self-Convolutional Activations Descriptor

9

i

.

Algorithm 1: Deep Self-Convolutional Activations (DeSCA) Descriptor
Input : image fi, random samples ri,k.
Output : DeSCA descriptor DDeSCA
1 : Compute C(Fi,Fj) for a doubled support window R∗
2 : Estimate C(Fri,k ,Fj) from C(Fi,Fj) according to the index mapping process.

i by using (6).

for v = 1 : NSP do /∗ hierarchical aggregation using average pooling ∗/

Determine a circular pyramidal point SP i(v).
Compute C(Fv,Fj) by using an average pooling for SP i(v) on C(Fri,k ,Fj).
end for
for u = 1 : NSB do /∗ hierarchical spatial aggregation using C-SPP ∗/

Determine a circular pyramidal bin SBi(u).
Compute hi(k, u) and hi(v, u) by using C-SPP on each SBi(u)
from C(Fri,k ,Fj) and C(Fv,Fj), respectively.

3 :
4 :

6 :
7 :

end for

8 : Build hierarchical self-convolutional activations ˆhi(l) from hi(k, u) and hi(v, u).
8 : Compute a nonlinear response (5), followed by L-2 normalization.
9 : Build a DeSCA descriptor DDeSCA

(l).

ldDeSCA

i

i

=(cid:83)

[38,39] with the complexity of O(INKM 2R). C(Fri,k ,Fj) is then simply obtained
from C(Fi,Fjr ) by re-indexing sampling patterns.

Though we remove the computational dependency on patch size MF , NKM 2R
computations of (6) are still needed to obtain the self-convolutional activations,
where many sampling pairs are repeated. To avoid such redundancy, we ﬁrst
compute self-convolutional activation C(Fi,Fj) for j ∈ R∗
i with a doubled local
i of size 2MR × 2MR(= 4M 2R). A doubled local support win-
support window R∗
dow is used because (6) is computed with patch Fjr and the minimum support
window size for R∗
i to cover all samples within Ri is 2MR as shown in Fig. 5(b).
After the self-convolutional activation for R∗
i is computed once over the image
domain, C(Fri,k ,Fj) can be extracted through an index mapping process, where
the indexes for Ri−ri,k are estimated from R∗
i .

4.4 DeSCA: Deep Self-Convolutional Activations

So far, we have discussed how to build the self-convolutional activation on a single
level. In this section, we extend this idea by encoding self-similar structures at
multiple levels in a manner similar to a deep architecture widely adopted in the
CNNs [26]. DeSCA is deﬁned similarly to SiSCA, except that an average pooling
is executed before C-SPP (see Fig. 6). With self-convolutional activations, we
perform the average pooling on circular pyramidal point sets.

In comparison to the self-convolutions just from a single patch, the spatial
aggregation of self-convolutional responses is clearly more robust, and it requires
only marginal computational overhead over SiSCA. The strength of such a hi-
erarchical aggregation has also been shown in [36]. Compared to using only last
CNN layer activations, we use all intermediate activations from hierarchical av-
erage pooling, which yields better cross-modal matching quality.
To build the hierarchical self-convolutional volume using an average pooling,
we ﬁrst deﬁne the circular pyramidal point sets SP i(v) from log-polar circular

10

S. Kim et al.

(a)

(b)

(c)

(d)

Fig. 7. Component analysis of DeSCA on the Middlebury benchmark [42] for varying
parameter values, such as (a) support window size MR, (b) number of log-polar circular
point Nρ × Nθ, (c) number of random samples NK , and (d) level of circular spatial
pyramid NS. In each experiment, all other parameters are ﬁxed to the initial values.
point sets Pi, where v associates all pyramidal level o ∈ {1, ..., NO} and all points
in each level o. In the average pooling, the circular pyramidal bins SBi(u) used
in C-SPP is re-used such that SP i(v) = {j|j ∈ Pi, j ∈ SBi(u)}, thus NS = NO.
Deep self-convolutional activations are deﬁned by aggregating C(Fri,k ,Fj) for all
ri,k patches determined on each SP i(v) such that

C(Fv,Fj) =

C(Fri,k ,Fj)/Nv,

ri,k∈SP i(v)

(7)
which is deﬁned for all v, and Nv is the number of ri,k patches within SP i(v).
The hierarchical activations are sequentially aggregated using average pooling
from bottom to top of circular pyramidal point set SP i(v). After computing hi-
erarchical self-convolutional aggregations, similar to SiSCA, the DeSCA employs
C-SPP, non-linear, and normalization layer presented in Sec. 4.2. Hierarchical
self-convolutional activation hi(v, u) is computed using the C-SPP such that

(cid:88)

hi(v, u) = max

j∈SBi(u)

{C(Fv,Fj)}.

(8)

hi(v, u) in (8) such that ˆhi(l) =(cid:83){k,v,u} {hi(k, u), hi(v, u)}. Our DeSCA descrip-

Accumulated self-convolutional activations are built from hi(k, u) in (4) and

(l) is then passed through a non-linear layer. DDeSCA

tor dDeSCA
ldDeSCA
built for l ∈ {1, ..., LDeSCA} with LDeSCA = (NK + NSP )NSB. Finally, dDeSCA
for each pixel i is normalized with an L-2 norm for all l.

=(cid:83)

(l) is
(l)

i

i

i

i

5 Experimental Results and Discussion

5.1 Experimental Settings

In our experiments, the DeSCA descriptor was implemented with the following
ﬁxed parameter settings for all datasets: {σc, MF , MR, NK, NS} = {0.5, 5, 9, 32, 3},
and {Nρ, Nθ} = {4, 16}. We chose the guided ﬁlter (GF) for edge-aware ﬁltering
in (6), with a smoothness parameter of  = 0.032. We implemented the DeSCA
descriptor in C++ on an Intel Core i7-3770 CPU at 3.40 GHz. We will make our
code publicly available. The DeSCA descriptor was compared to other state-of-
the-art descriptors (SIFT [14], DAISY [23], BRIEF [24], LIOP [28], DaLI [29],

5791113151719Error in unoccluded areas (%)51015202530E. 0/1E. 0.2I. 1/2I. 1/31x42x83x124x165x206x247x288x32Error in unoccluded areas (%)51015202530E. 0/1E. 0.2I. 1/2I. 1/3816243240485664Error in unoccluded areas (%)5101520E. 0/1E. 0.2I. 1/2I. 1/301234Error in unoccluded areas (%)5101520253035E. 0/1E. 0.2I. 1/2I. 1/3Deep Self-Convolutional Activations Descriptor

11

(a) image 1 (b) image 2 (c) ANCC (d) SIFT
(f) DASC (g) DeSCA
Fig. 8. Comparison of disparity estimations for Moebius and Dolls image pairs across
illumination combination ‘1/3’ and exposure combination ‘0/2’, respectively. Compared
to other methods, DeSCA estimates more accurate and edge-preserved disparity maps.

(e) LSS

(a)

(b)

(c)

(d)

Fig. 9. Average bad-pixel error rate on the Middlebury benchmark [42] with illumina-
tion and exposure variations. Optimization was done by GC in (a), (b), and by WTA
in (c), (d). DeSCA descriptor shows the best performance with the lowest error rate.

LSS [22], and DASC [10]), as well as area-based approaches (ANCC [35] and
RSNCC [9]). Furthermore, to evaluate the performance gain with a deep archi-
tecture, we compared SiSCA and DeSCA.

5.2 Parameter Evaluation

The matching performance of DeSCA is exhibited in Fig. 7 for varying parameter
values, including support window size MR, number of log-polar circular points
Nρ × Nθ, number of random samples NK, and levels of the circular spatial pyra-
mid NS. Note that NO = NS. Especially, Fig. 7(c), (d) prove the eﬀectiveness of
self-convolutional activations and deep architectures of DeSCA. For a quantita-
tive analysis, we measured the average bad-pixel error rate on the Middlebury
benchmark [42]. With a larger support window MR, the matching quality im-
proves rapidly until about 9 × 9. Nρ × Nθ inﬂuences the performance of circular
pooling, which is found to plateau at 4 × 16. Using a larger number of ran-
dom samples NK yields better performance since the descriptor encodes more
information. The level of circular spatial pyramid NS also aﬀects the amount of
encoding in DeSCA. Based on these experiments, we set NK = 32 and NS = 3
in consideration of eﬃciency and robustness.

5.3 Middlebury Stereo Benchmark

We evaluated DeSCA on the Middlebury stereo benchmark [42], which contains
illumination and exposure variations. In the experiments, the illumination (ex-
posure) combination ‘1/3’ indicates that two images were captured under the

1/11/21/32/22/33/3Error in unoccluded areas (%)051015202530ANCCRSNCCSIFTBRIEFDAISYLSSLIOPDASCSiSCADeSCA0/00/10/21/11/22/2Error in unoccluded areas (%)0510152025301/11/21/32/22/33/3Error in unoccluded areas (%)0510152025300/00/10/21/11/22/2Error in unoccluded areas (%)05101520253012

S. Kim et al.

(a) image 1 (b) image 2 (c) BRIEF
(e) DASC (f) SiSCA (g) DeSCA
Fig. 10. Dense correspondence evaluations for (from top to bottom) RGB-NIR, ﬂash-
noﬂash, diﬀerent exposures, and blurred-sharp images. Compared to others, DeSCA
estimates more reliable dense correspondences for challenging cross-modal pairs.

(d) LSS

Methods

ANCC [35]
RSNCC [9]
SIFT [14]
DAISY [23]
BRIEF [24]
LSS [22]
LIOP [28]
DASC [10]
SiSCA
DeSCA

WTA optimization

SF optimization [11]

RGB-
NIR

ﬂash-
noﬂash

23.21
20.42
27.51
25.12
24.11
18.72
27.61
26.30
29.14
18.29
27.82
19.18
24.42
16.42
14.51
13.24
10.12 10.12
8.12
8.22

diﬀ.
expo.

25.19
18.21
19.42
20.72
17.13
18.21
14.22
10.32
8.22
6.72

blur-
sharp

26.14
27.91
27.18
27.41
26.43
26.14
20.42
16.42
14.22
13.28

RGB-
NIR

ﬂash-
noﬂash

18.45
13.41
18.51
20.42
17.54
16.14
15.32
13.42
9.12
7.62

14.14
15.87
11.06
10.84
9.21
11.88
11.42
7.11
6.18
5.12

diﬀ.
expo.

11.96
9.15
14.87
12.71
9.54
9.11
10.22
7.21
5.22
4.72

blur-
sharp

19.24
18.21
20.78
22.91
19.72
18.51
17.12
11.21
9.12
8.01

Table 1. Comparison of quantitative evaluation on cross-modal benchmark.

1st and 3rd illumination (exposure) conditions. For a quantitative evaluation, we
measured the bad-pixel error rate in non-occluded areas of disparity maps [42].
Fig. 8 shows the disparity maps estimated under severe illumination and
exposure variations with winner-takes-all (WTA) optimization. Fig. 9 displays
the average bad-pixel error rates of disparity maps obtained under illumina-
tion or exposure variations, with graph-cut (GC) [43] and WTA optimization.
Area-based approaches (ANCC [35] and RSNCC [9]) are sensitive to severe ra-
diometric variations, especially when local variations occur frequently. Feature
descriptor-based methods (SIFT [14], DAISY [23], BRIEF [24], LSS [22], and
DASC [10]) perform better than the area-based approaches, but they also provide
limited performance. Our DeSCA descriptor achieves the best results both quan-
titatively and qualitatively. Compared to SiSCA descriptor, the performance of
DeSCA descriptor is highly improved, where the performance beneﬁts of the
deep architecture are apparent.

Deep Self-Convolutional Activations Descriptor

13

(a) image 1 (b) image 2 (c) DAISY (d) BRIEF
(g) DeSCA
Fig. 11. Dense correspondence comparisons for images with diﬀerent illumination con-
ditions and non-rigid image deformations [29]. Compared to other approaches, DeSCA
provides more accurate dense correspondence estimates with reduced artifacts.

(f) DaLI

(e) LSS

Methods

def.

illum.

def./
illum.

aver.

SIFT [14]
DAISY [23]
BRIEF [24]
LSS [22]
LIOP [28]
DaLI [29]
DASC [10]
SiSCA
DeSCA

44.49
43.37

40

40.81
42.72
37.14
39.54
31.72
27.31
24.83

45.15
43.98
41.51
40.12
40.81
30.22
28.72
27.47
27.12
26.21
26.18
23.42 22.21 24.17 23.27
20.14 20.72 21.87 20.91

47.51
43.42
41.35
40.11
30.21
27.99
27.51

Table 2. Average error rates on the DaLI benchmark.

5.4 Cross-modal and Cross-spectral Benchmark

We evaluated DeSCA on a cross-modal and cross-spectral benchmark [10] con-
taining various kinds of image pairs, namely RGB-NIR, diﬀerent exposures,
ﬂash-noﬂash, and blurred-sharp. Optimization for all descriptors and similarity
measures was done using WTA and SIFT ﬂow (SF) with hierarchical dual-layer
belief propagation [11], for which the code is publicly available. Sparse ground
truths for those images are used for error measurement as done in [10].

Fig. 10 provides a qualitative comparison of the DeSCA descriptor to other
state-of-the-art approaches. As already described in the literature [9], gradient-
based approaches such as SIFT [14] and DAISY [23] have shown limited per-
formance for RGB-NIR pairs where gradient reversals and inversions frequently
appear. BRIEF [24] cannot deal with noisy regions and modality-based appear-
ance diﬀerences since it is formulated on pixel diﬀerences only. Unlike these
approaches, LSS [22] and DASC [10] consider local self-similarities, but LSS is
lacking in discriminative power for dense matching. DASC also exhibits limited
performance. Compared to those methods, the DeSCA displays better corre-

14

S. Kim et al.

image size
463 × 370

SIFT

DAISY

130.3s

2.5s

LSS

31s

DaLI

352.2s

DASC DeSCA* DeSCA†
2.7s

193.2s

9.2s

Table 3. Computation speed of DeSCA and other state-of-the-art local and global
descriptors. The brute-force and eﬃcient implementations of DeSCA are denoted by *
and †, respectively.

spondence estimation. We also performed a quantitative evaluation with results
listed in Table 1, which also clearly demonstrates the eﬀectiveness of DeSCA.

5.5 DaLI Benchmark

We also evaluated DeSCA on a recent, publicly available dataset featuring chal-
lenging non-rigid deformations and very severe illumination changes [29]. Fig.
11 presents dense correspondence estimates for this benchmark [29]. A quanti-
tative evaluation is given in Table 2 using ground truth feature points sparsely
extracted for each image, although DeSCA is designed to estimate dense corre-
spondences. As expected, conventional gradient-based and intensity comparison-
based feature descriptors, including SIFT [14], DAISY [23], and BRIEF [24], do
not provide reliable correspondence performance. LSS [22] and DASC [10] exhibit
relatively high performance for illumination changes, but are limited on non-
rigid deformations. LIOP [28] provides robustness to radiometric variations, but
is sensitive to non-rigid deformations. Although DaLI [29] provides robust cor-
respondences, it requires considerable computation for dense matching. DeSCA
oﬀers greater discriminative power as well as more robustness to non-rigid de-
formations in comparison to the state-of-the-art cross-modality descriptors.

5.6 Computational Speed

In Table 3, we compared the computational speed of DeSCA to state-of-the-art
local descriptor, namely DaLI [29], and dense descriptors, namely DAISY [23],
LSS [22], and DASC [10]. Even though DeSCA needs more computational time
compared to some previous dense descriptors, it provides signiﬁcantly improved
matching performance as described previously.

6 Conclusion

The deep self-convolutional activations (DeSCA) descriptor was proposed for es-
tablishing dense correspondences between images taken under diﬀerent imaging
modalities. Its high performance in comparison to state-of-the-art cross-modality
descriptors can be attributed to its greater robustness to non-rigid deformations
because of its eﬀective pooling scheme, and more importantly its heightened
discriminative power from a more comprehensive representation of self-similar
structure and its formulation in a deep architecture. DeSCA was validated on
an extensive set of experiments that cover a broad range of cross-modal diﬀer-
ences. In future work, thanks to the robustness to non-rigid deformations and
high discriminative power, DeSCA can potentially beneﬁt object detection and
semantic segmentation.

Deep Self-Convolutional Activations Descriptor

15

References

1. Brown, M., Susstrunk, S.: Multispectral sift for scene category recognition. In:

CVPR (2011)

2. Yan, Q., Shen, X., Xu, L., Zhuo, S.: Cross-ﬁeld joint image restoration via scale

map. In: ICCV (2013)

3. Hwang, S., Park, J., Kim, N., Choi, Y., Kweon, I.: Multispectral pedestrian detec-

tion: Benchmark dataset and baseline. In: CVPR (2015)

4. Krishnan, D., Fergus, R.: Dark ﬂash photography. In: SIGGRAPH (2009)
5. Sen, P., Kalantari, N.K., Yaesoubi, M., Darabi, S., Goldman, D.B., Shechtman, E.:
Robust patch-based hdr reconstruction of dynamic scenes. In: SIGGRAPH (2012)
6. HaCohen, Y., Shechtman, E., Lishchinski, E.: Deblurring by example using dense

correspondence. In: ICCV (2013)

7. Lee, H., Lee, K.: Dense 3d reconstruction from severely blurred images using a

single moving camera. In: CVPR (2013)

8. Petschnigg, G., Agrawals, M., Hoppe, H.: Digital photography with ﬂash and

no-ﬂash iimage pairs. In: SIGGRAPH (2004)

9. Shen, X., Xu, L., Zhang, Q., Jia, J.: Multi-modal and multi-spectral registration

for natural images. In: ECCV (2014)

10. Kim, S., Min, D., Ham, B., Ryu, S., Do, M.N., Sohn, K.: Dasc: Dense adaptive
self-correlation descriptor for multi-modal and multi-spectral correspondence. In:
CVPR (2015)

11. Liu, C., Yuen, J., Torralba, A.: Sift ﬂow: Dense correspondence across scenes and

its applications. IEEE Trans. PAMI 33(5) (2011) 815–830

12. Kim, J., Liu, C., Sha, F., Grauman, K.: Deformable spatial pyramid matching for

fast dense correspondences. In: CVPR (2013)

13. Pinggera, P., Breckon, T., Bischof, H.: On cross-spectral stereo matching using

dense gradient features. In: BMVC (2012)

14. Lowe, D.: Distinctive image features from scale-invariant keypoints. IJCV 60(2)

(2004) 91–110

15. Simonyan, K., Vedaldi, A., Zisserman, A.: Learning local feature descriptors using

convex optimisation. IEEE Trans. PAMI 36(8) (2014) 1573–1585

16. Gong, Y., Wang, L., Guo, R., Lazebnik, S.: Multi-scale orderless pooling of deep

convolutional acitivation features. In: ECCV (2014)

17. Fischer, P., Dosovitskiy, A., Brox, T.: Descriptor matching with convolutional

neural networks: A comparison to sift. arXiv:1405.5769 (2014)

18. Donahue, J., Jia, Y., Vinyals, O., Hoﬀman, J., Zhang, N., Tzeng, E., Darrell, T.:
Decaf: A deep convolutional activation feature for generic visual recognition. In:
ICML (2014)

19. Simo-Serra, E., Trulls, E., Ferraz, L., Kokkinos, I., Fua, P., Moreno-Noguer, F.:
Discriminative learning of deep convolutional feature point descriptors. In: ICCV
(2015)

20. Dong, J., Soatto, S.: Domain-size pooling in local descriptors: Dsp-sift. In: CVPR

(2015)

21. Long, J., Shelhamer, E., Darrell, T.: Fully conovlutional networks for semantic

segmentation. In: CVPR (2015)

22. Schechtman, E., Irani, M.: Matching local self-similarities across images and videos.

In: CVPR (2007)

23. Tola, E., Lepetit, V., Fua, P.: Daisy: An eﬃcient dense descriptor applied to wide-

baseline stereo. IEEE Trans. PAMI 32(5) (2010) 815–830

16

S. Kim et al.

24. Calonder, M.: Brief : Computing a local binary descriptor very fast. IEEE Trans.

PAMI 34(7) (2011) 1281–1298

25. Trzcinski, T., Christoudias, M., Lepetit, V.: Learning image descriptor with boost-

ing. IEEE Trans. PAMI 37(3) (2015) 597–610

26. Alex, K., Ilya, S., Geoﬀrey, E.H.: Imagenet classiﬁcation with deep convolutional

neural networks. In: NIPS (2012)

27. Saleem, S., Sablatnig, R.: A robust sift descriptor for multispectral images. IEEE

SPL 21(4) (2014) 400–403

28. Wang, Z., Fan, B., Wu, F.: Local intensity order pattern for feature description.

In: ICCV (2011)

29. Simo-Serra, E., Torras, C., Moreno-Noguer, F.: Dali: Deformation and light invari-

ant descriptor. IJCV 115(2) (2015) 136–154

30. Heinrich, P., Jenkinson, M., Bhushan, M., Matin, T., Gleeson, V., Brady, S., Schn-
abel, A.: Mind: Modality indepdent neighbourhood descriptor for multi-modal
deformable registration. MIA 16(3) (2012) 1423–1435

31. Torabi, A., Bilodeau, G.: Local self-similarity-based registration of human rois in

pairs of stereo thermal-visible videos. PR 46(2) (2013) 578–589

32. Ye, Y., Shan, J.: A local descriptor based registration method for multispectral
remote sensing images with non-linear intensity diﬀerences. JPRS 90(7) (2014)
83–95

33. Pluim, J., Maintz, J., Viergever, M.: Mutual information based registration of

medical images: A survey. IEEE Trans. MI 22(8) (2003) 986–1004

34. Heo, Y., Lee, K., Lee, S.: Joint depth map and color consistency estimation for
stereo images with diﬀerent illuminations and cameras. IEEE Trans. PAMI 35(5)
(2013) 1094–1106

35. Heo, Y., Lee, K., Lee, S.: Robust stereo matching using adaptive normalized cross-

correlation. IEEE Trans. PAMI 33(4) (2011) 807–822

36. Weinzaepfel, P., Revaud, J., Harchaoui, Z., Schmid, C.: Deepﬂow: Large displace-

ment optical ﬂow with deep matching. In: ICCV (2013)

37. Black, M.J., Sapiro, G., Marimont, D.H., Heeger, D.: Robust anisotropic diﬀusion.

IEEE Trans. IP 7(3) (1998) 421–432

38. Gastal, E., Oliveira, M.: Domain transform for edge-aware image and video pro-

cessing. In: SIGGRAPH (2011)

39. He, K., Sun, J., Tang, X.: Guided image ﬁltering. IEEE Trans. PAMI 35(6) (2013)

1397–1409

40. Seidenari, L., Serra, G., Bagdanov, A.D., Bimbo, A.D.: Local pyramidal descriptors

for image recognition. IEEE Trans. PAMI 36(5) (2014) 1033–1040

41. He, K., Zhang, X., Ren, S., Sun, J.: Spatial pyramid pooling in deep convolutional

networks for visual recognition. IEEE Trans. PAMI 37(9) (2015) 1904–1916

42. online.: http://vision.middlebury.edu/stereo/.
43. Boykov, Y., Yeksler, O., Zabih, R.: Fast approximation enermgy minimization via

graph cuts. IEEE Trans. PAMI 23(11) (2001) 1222–1239

