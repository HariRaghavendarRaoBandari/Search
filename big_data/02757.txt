6
1
0
2

 
r
a

M
9

 

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
7
5
7
2
0

.

3
0
6
1
:
v
i
X
r
a

arXiv: arXiv:0000.0000

Permutation p-value approximation via

generalized Stolarsky invariance

Hera Yu He, Kinjal Basu, Qingyuan Zhao, Art B. Owen

Department of Statistics

Sequoia Hall

Stanford University
Stanford, CA 94305

e-mail: (hera, kinjal, qyzhao, owen)@stanford.edu

Abstract: When it is necessary to approximate a small permutation p-
value p, then simulation is very costly. For linear statistics, a Gaussian ap-
proximation ˆp1 reduces to the volume of a spherical cap. Using Stolarsky’s
(1973) invariance principle from discrepancy theory, we get a formula for
the mean of (ˆp1 − p)2 over all spherical caps. From a theorem of Brauchart
and Dick (2013) we get such a formula averaging only over spherical caps
of volume exactly ˆp1. We also derive an improved estimator ˆp2 equal to the
average true p-value over spherical caps of size ˆp1 containing the original
data point x0 on their boundary. This prevents ˆp2 from going below 1/N
when there are N unique permutations. We get a formula for the mean of
(ˆp2 − p)2 and ﬁnd numerically that the root mean squared error of ˆp2 is
roughly proportional to ˆp2 and much smaller than that of ˆp1.

1. Introduction

Permutation methods are commonly used to obtain p-values in genomic applica-
tions. The exact permutation p-value is typically too expensive to compute and
one instead samples M − 1 random permutations and reports an approximate
p-value as the number of suitably extreme outcomes divided by M . The original
data are counted in both numerator and denominator, and the p-value will not
be smaller than 1/M .

Small p-values are often required in settings where a great many tests are
conducted. When a very small p-value  is required, then for reasonable power,
M must be at least a modest multiple of 1/. For instance in genome wide
association studies (GWAS) the customary threshold for signiﬁcance is  = 5 ×
10−8, making permutation methods prohibitively expensive for such problems.
Advances in multiple testing, such as the false discovery rate of Benjamini and
Hochberg (1995), mitigate but do not remove the need to establish small p-
values.

For linear test statistics, as we show below, the permutation p-value is the
fraction of permuted data vectors lying in a given spherical cap subset of a
d-dimensional sphere Sd = {z ∈ Rd+1 | zTz = 1}. A natural but crude approxi-
mation to that p-value is the fraction ˆp of the sphere’s surface volume contained
in that spherical cap.

1

He, Basu, Zhao & Owen/Stolarsky invariance for permutations

2

Stolarsky’s invariance principal gives a remarkable description of the accu-
racy of this approximation ˆp. For y ∈ Sd and t ∈ [−1, 1] we can deﬁne the
spherical cap of center y and height t via C(y; t) = {z ∈ Sd | (cid:104)y, z(cid:105) ≥ t}.
For x0, . . . , xN−1 ∈ Sd, let p(y, t) be the fraction of those N points that lie in
C(y; t) and let ˆp(y, t) = ˆp(t) = vol(C(y; t))/vol(Sd). The squared L2 spherical
cap discrepancy of these points is

L2(x0, . . . , xN−1)2 =

|ˆp(t) − p(z, t)|2 dσd(z) dt.

(cid:90) 1

(cid:90)

−1

Sd

Stolarsky (1973) shows that

(cid:90)

(cid:90)

dωd
ωd+1

× L2(·)2 =

(cid:107)x − y(cid:107) dσd(x) dσd(y) − 1
N 2

Sd

Sd

N−1(cid:88)

k,l=0

(cid:107)xk − xl(cid:107)

(1.1)

where σd is the uniform (Haar) measure on Sd and ωd is the (surface) volume
of Sd. Equation (1.1) relates the mean squared error of ˆp to the mean absolute
Euclidean distance among the N points. In our applications, the N points will
be the distinct permuted values of a data vector, but the formula holds for an
arbitrary set of N points.
The left side of (1.1) is, up to normalization, a mean squared discrepancy over
spherical caps. This average of (ˆp − p)2 includes p-values of all sizes between 0
and 1. It is not then a very good accuracy measure when ˆp turns out to be very
small. It would be more useful to get such a mean squared error taken over caps
of exactly the size ˆp, and no others.

Brauchart and Dick (2013) consider quasi-Monte Carlo (QMC) sampling in
the sphere. They generalize Stolarsky’s discrepancy formula to include a weight-
ing function on the height t. By specializing their formula, we get an expression
for the mean of (ˆp − p)2 over spherical caps of any ﬁxed size.

Discrepancy theory plays a prominent role in QMC (Niederreiter, 1992),
which is about approximating an integral by a sample average. The present
setting is in a sense the reverse of QMC: the discrete average over permutations
is the exact value, and the integral over a continuum is the approximation. A
second diﬀerence is that the QMC literature focusses on choosing N points to
minimize a criterion such as (1.1), whereas here the N points are determined
by the problem.
We present several results for the mean of (ˆp− p)2 under diﬀerent conditions.
In addition to ﬁxing the size of the caps we can restrict the mean squared error
to only be over caps centered on points y satisfying (cid:104)y, x0(cid:105) = (cid:104)y0, x0(cid:105) where x0
is the original (unpermuted) x vector and y0 is the observed y value. We can
obtain this result by further extending Brauchart and Dick’s generalization of
Stolarsky’s invariance. We call this the ‘ﬁner approximation’ and will show it
has advantages over constraining only the height of the caps. More generally, the
point xc could be any of the permuted x vectors, such as the one that happens
to be closest to y0.

Although we found these results via invariance we can also obtain them via
probabilistic arguments. As a consequence we have a probabilistic derivation

He, Basu, Zhao & Owen/Stolarsky invariance for permutations

3

of Stolarsky’s formula. Some of our results are for arbitrary x, but our best
computational formulas are for the case where the variable x is binary, as it
would be in experiments comparing treatment and control groups.

The rest of the paper is organized as follows. Section 2 presents some context
on permutation tests and gives some results from spherical geometry. In Sec-
tion 3 we use Stolarsky’s invariance principle as generalized by Brauchart and
Dick (2013) to obtain the mean squared error between the true p-value and its
continuous approximation ˆp1, taken over all spherical caps of volume ˆp1. This
section also has a probabilisitic derivation of that mean squared error. In Sec-
tion 4 we describe some ﬁner approximations ˜p for the p-value. These condition
on not just the volume of the spherical cap but also on its distance from the orig-
inal data point x0, or from some other point, such as the closest permutation of
x0 to y0. By always including the original point we ensure that ˜p (cid:62) 1/N . That
is a desirable property because the true permutation p-value cannot be smaller
than 1/N . In Section 5 we modify the proof in Brauchart and Dick (2013), to
further generalize their invariance results to include the mean squared error of
the ﬁner approximations. Section 6 extends our estimates to two-sided testing.
Section 7 illustrates our p-value approximations numerically. We see that an
RMS error in the ﬁner approximate p-values is of the same order of magnitude
as those p-values themselves. Section 8 makes a numerical comparison to sad-
dlepoint methods. Section 9 discusses the results and gives more details about
the bioinformatics problems that motivate the search for approximations to the
permutation distribution. Most of the proofs are in the Appendix, Section 10.

2. Background and notation

¯X = (1/n)(cid:80)n

X = (1/n)(cid:80)n

i=1 Xi, s2

The raw data contain points (Xi, Yi) for i = 1, . . . , n. The sample correla-
0 y0 where x0 has components (Xi − ¯X)/sX for
tion of these points is ˆρ = xT
i=1(Xi − ¯X)2 and ¯Y and sY are deﬁned sim-
ilarly. We assume that sX and sY are positive. Both x0 and y0 belong to Sn−1.
Moreover they belong to {z ∈ Sn−1 | zT1n = 0}. We can use an orthogonal
matrix to rotate the points of this set onto Sn−2 × {0}. As a result, we may
simply work with x0, y0 ∈ Sd where d = n − 2.

p-value is p = (1/n!)(cid:80)

The quantity ˆρ measures association between X and Y . It can be used as such
a measure if the Xi are ﬁxed and Yi observed conditionally, or vice versa, or if
the (Xi, Yi) pairs are independently sampled from some joint distribution. Let
π be a permutation of the indices 1, . . . , n. There are n! vectors xπ that result
from centering and scaling Xπ = (Xπ(1), Xπ(2), . . . , Xπ(n)). The permutation
0 y0). The justiﬁcation for this p-value
relies on the group structure of permutations (Lehmann and Romano, 2005).
For a cautionary tale on the use of permutation sets without a group structure,
see Southworth et al. (2009). For notational simplicity we assume ˆρ > 0 and
work with one-sided p-values. Negative ˆρ can be handled similarly, or simply by
switching the group labels. For two-sided p-values see Section 6.

(cid:62) xT

π 1(xT

πy0

Our proposals are computationally most attractive in the case where Xi takes

He, Basu, Zhao & Owen/Stolarsky invariance for permutations

4

on just two values, such as 0 and 1. Then ˆρ is a two-sample test statistic. If there
are m0 observations with Xi = 0 and m1 with Xi = 1 then x0 contains m0 com-
Some formulas involve the smaller sample size, m ≡ min(m0, m1).

ponents equal to −(cid:112)m1/(nm0) and m1 components equal to +(cid:112)m0/(nm1).
For this two-sample case there are only N =(cid:0)m0+m1
(cid:1) distinct permutations

m0

of x0. Calling these x0, x1, . . . , xN−1 we ﬁnd that

N−1(cid:88)

k=0

p =

1
N

1(xT

k y0

(cid:62) ˆρ).

(2.1)

Now suppose that there are exactly r indices for which xk is positive and xl
is negative. There are then r indices with the reverse pattern too. We say that
xk and xl are at ‘swap distance r’. In that case we easily ﬁnd that

u(r) ≡ xT

k xl = 1 − r

+

1
m1

m0

.

(2.2)

(cid:17)

We need some geometric properties of the unit sphere and spherical caps. The
surface volume of Sd is ωd = 2π(d+1)/2/Γ((d + 1)/2). We use σd for the volume
element in Sd normalized so that σd(Sd) = 1. The spherical cap C(y; t) = {z ∈
Sd | zTy (cid:62) t} has volume

(cid:40) 1

2 I1−t2
1 − 1

2 , 1
2 I1−t2

2

2 , 1

2

(cid:1) , −1 ≤ t < 0

0 ≤ t ≤ 1

σd(C(y; t)) =

where It(a, b) is the incomplete beta function

(cid:16) 1

(cid:1) ,
(cid:0) d

(cid:0) d

(cid:90) t

with B(a, b) =(cid:82) 1

It(a, b) =

1

B(a, b)

0

xa−1(1 − x)b−1 dx

0 xa−1(1 − x)b−1 dx. Obviously, this volume is 0 if t < −1 and
it is 1 if t > 1. This volume is independent of y so we may write σd(C(· , t)) for
the volume. By symmetry, 1(x ∈ C(y, t)) = 1(y ∈ C(x, t)).

Our ﬁrst approximation of the p-value is

ˆp1(ˆρ) = σd(C(y; ˆρ)).

(2.3)

This approximation has two intuitive explanations. First, the true p-value is the
proportion of permutations of x0 that lie in C(y0; ˆρ), and σd(C(y0, ˆρ)) is the
proportion of the volume of Sd in that set. Second, as we show in Proposition 2,
ˆp1 = E(p | (cid:104)x0, y(cid:105) = ˆρ) for y ∼ U(Sd) as y0 would if the original Yi were IID
Gaussian. In Theorem 4, we ﬁnd Var(ˆp1) under this assumption.
We frequently need to project y ∈ Sd onto a point x ∈ Sd. In this representa-
1 − t2y∗ where t = yTx ∈ [−1, 1] and y∗ ∈ {z ∈ Sd | zTx = 0}
tion y = tx +
which is isomorphic to Sd−1. The coordinates t and y∗ are unique. From equation
(A.1) in Brauchart and Dick (2013) we get

√

dσd(y) =

ωd−1
ωd

(1 − t2)d/2−1 dt dσd−1(y∗).

(2.4)

He, Basu, Zhao & Owen/Stolarsky invariance for permutations

5

In their case x was (0, 0, . . . , 1).

The intersection of two spherical caps of common height t is

C2(x, y; t) ≡ C(x; t) ∩ C(y; t).

We will need the volume of this intersection. Lee and Kim (2014) give a general
solution for spherical cap intersections without requiring equal heights. They
enumerate 25 cases, but our case does not correspond to any single such case
and so we obtain the formula we need directly, below. We suspect it must be
known already, but we were unable to ﬁnd it in the literature.
Lemma 1. Let x, y ∈ Sd and −1 (cid:54) t (cid:54) 1 and put u = xTy. Let V2(u; t, d) =
σd(C2(x, y; t)). If u = 1, then V2(u; t, d) = σd(C(x; t)). If −1 < u < 1, then

V2(u; t, d) =

where ρ(s) = (t − su)/(cid:112)(1 − s2)(1 − u2). Finally, for u = −1,

t

(1 − s2)

d

2 −1σd−1(C(y∗; ρ(s))) ds,

ωd−1
ωd

V2(u; t, d) =

(cid:82) |t|
−|t|(1 − s2) d

0,
ωd−1

ωd

2 −1 ds,

t (cid:62) 0
else.

(2.5)

(2.6)

Proof. Let z ∼ U(Sd). Then V2(u; t, d) = σd(C2(x, y; t)) = Pr(z ∈ C2(x, y; t)).
If u = 1 then x = y and so C2(x, y; t) = C(x; t). For u < 1, we project y and
z onto x, via z = sx +

1 − s2z∗ and y = ux +

1 − u2y∗. Now

√

√

(cid:90) 1

(cid:40)

If u > −1 then this reduces to (2.5). For u = −1 we get

V2(u; t, d) =

ωd−1
ωd

which reduces to (2.6).

−1

1(s (cid:62) t)1(−s (cid:62) t)(1 − s2)

d

2 −1 ds.

When we give probabilistic arguments and interpretations we do so for a
random center y of a spherical cap. We use Models 1 and 2 below. Model 1 is
illustrated in Figure 1. Model 2 is illustrated in Figure 2 of Section 4 where we
ﬁrst use it.
Model 1. The vector y is uniformly distributed on the sphere Sd. Expectation
under this model is denoted E1(·).

V2(u; t, d) =

=

Sd

(cid:90)
(cid:90) 1
(cid:90)

−1
×

1((cid:104)x, z(cid:105) ≥ t)1((cid:104)y, z(cid:105) ≥ t) dσ(z)

1(s (cid:62) t)

1(su +

Sd−1

d

2 −1

ωd−1
ωd

(1 − s2)

(cid:112)
1 − s2(cid:112)
(cid:90) 1

1 − u2 (cid:104)y∗, z∗(cid:105) ≥ t) dσd−1(z∗) ds.

He, Basu, Zhao & Owen/Stolarsky invariance for permutations

6

Fig 1: Illustration for Model 1. The point y is uniformly distributed over Sd. The
small open circles represent permuted vectors xk. The point y0 is the observed
value of y. The circle around it goes through x0 and represents a spherical cap of
height yT
0 x0. A second spherical cap of equal volume is centered at y = y1. We
study moments of p(y; ˆρ), the fraction of xk in the cap centered at random y.

some −1 ≤ ˜ρ ≤ 1, and c ∈ {0, 1, . . . , N − 1}. Then y = ˜ρxc +(cid:112)1 − ˜ρ2y∗ for

Model 2. The vector y is uniformly distributed on {z ∈ Sd | zTxc = ˜ρ}, for
y∗ uniformly distributed on a subset of Sd isometric to Sd−1. Expectation under
this model is denoted E2(·).

3. Approximation via spherical cap volume

Here we study the approximate p-value ˆp1(ˆρ) = σd(C(y; ˆρ)). First we ﬁnd the
mean squared error of this approximation over all spherical caps of the given
volume via invariance. Then we give a probabilistic interpretation which includes
the conditional unbiasedness result in Proposition 2 below. Then we give two
computational simpliﬁcations, ﬁrst for points obtained via permutation, and
second for permutations of a binary vector. We begin by restating the invariance
principle.

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllly0ly1SdHe, Basu, Zhao & Owen/Stolarsky invariance for permutations

7

N−1(cid:88)

k,l=0

1
N 2

Theorem 1. Let x0, . . . , xN−1 be any points in Sd. Then

(cid:12)(cid:12)(cid:12)(cid:12)σd(C(z; t)) − 1

N

N−1(cid:88)

k=0

(cid:90)

(cid:90) 1
(cid:90)

−1

Sd

(cid:107)x − y(cid:107) dσd(x) dσd(y)

Sd

Sd

(cid:107)xk − xl(cid:107) +

=

1
Cd

(cid:90)

(cid:12)(cid:12)(cid:12)(cid:12)2

1C(z;t)(xk)

dσd(z) dt

where Cd = ωd−1/(dωd).

Proof. Stolarsky (1973).

Brauchart and Dick (2013) gave a simple proof of Theorem 1 using repro-

ducing kernel Hilbert spaces. They generalized Theorem 1 as follows.
Theorem 2. Let x0, . . . , xN−1 be any points in Sd. Let v : [−1, 1] → (0,∞) be
any function with an antiderivative. Then

(cid:90) 1

−1

1
N 2

=

(cid:90)
N−1(cid:88)

v(t)

k,l=0

(cid:12)(cid:12)(cid:12)(cid:12)σd(C(z; t)) − 1
(cid:90)
(cid:90)

N

Sd

Kv(xk, xl) −

Sd

Sd

N−1(cid:88)

k=0

(cid:12)(cid:12)(cid:12)(cid:12)2

1C(z;t)(xk)

dσd(z) dt

(3.1)

Kv(x, y) dσd(x) dσd(y)

(cid:90) 1

−1

(cid:90)

Sd

where Kv(x, y) is a reproducing kernel function deﬁned by

Kv(x, y) =

v(t)

1C(z;t)(x)1C(z;t)(y) dσd(z) dt.

(3.2)

Proof. See Theorem 5.1 in Brauchart and Dick (2013)

If we set v(t) = 1 and K(x, y) = 1− Cd(cid:107)x− y(cid:107), then we recover the original
Stolarsky formula. Note that the statement of Theorem 5.1 in Brauchart and
Dick (2013) has a sign error in their counterpart to (3.1). The corrected state-
ment (3.1) can be veriﬁed by comparing equations (5.3) and (5.4) of Brauchart
and Dick (2013).
For ˆρ ∈ [−1, 1) and  = (1, 2) ∈ (0, 1)2, let

We would like a version of (3.1) just for one value of t such as t = ˆρ = xT

0 y0.

v(t) = 2 +

1(ˆρ ≤ t ≤ ˆρ + 1).

1
1

(3.3)

Each v satisﬁes the conditions of Theorem 2 making (3.1) an identity in . We
let 2 → 0 and then 1 → 0 on both sides of (3.1) for v = v yielding Theorem 3.
Theorem 3. Let x0, x1, . . . , xN ∈ Sd and t ∈ [−1, 1]. Then

|p(y, t) − ˆp1(t)|2 dσd(y) =

1
N 2

Sd

σd(C2(xk, xl; t)) − ˆp1(t)2.

(3.4)

N−1(cid:88)

N−1(cid:88)

k=0

l=0

(cid:90)

He, Basu, Zhao & Owen/Stolarsky invariance for permutations

8

Proof. See Section 10.1 of the Appendix which uses the limit argument described
above.

We now give a proposition that holds for all models, including our Model 1

and Model 2.
Proposition 1. For a random point y ∈ Sd,

E(p(y, t)) =

1
N

E(p(y, t)2) =

1
N 2

Pr(y ∈ C(xk; t)),

and

Pr(y ∈ C2(xk, xl; t)).

(3.5)

(3.6)

N−1(cid:88)
N−1(cid:88)

k=0

k,l=0

Proposition 1 provides a probabilistic interpretation for equation (3.4). When
y ∼ U(Sd), the double sum on right side of (3.4) is E(p(y, t)2). Additionally
ˆp1(t) has a probabilistic interpretation under Model 1.
Proposition 2. For any x0, . . . , xN−1 ∈ Sd and t ∈ [−1, 1], ˆp1(t) = E1(p(y, t)).
Proof. E1(p(y; t)) = E1

= σd(Cd(y; t)) = ˆp1(t).

N−1(cid:80)

(cid:104) 1

1C(y;t)(xk)

(cid:105)

N

k=0

Combining Proposition 2 and Theorem 3 we ﬁnd that if y ∼ U(Sd), as it
would for IID Gaussian Yi, then p(y, ˆρ) is a random variable with mean ˆp1(ˆρ)
and variance given by (3.4) with t = ˆρ.

The right hand side of (3.4) sums O(N 2) terms. In a permutation analysis

we might have N = n! or N =(cid:0)m0+m1

(cid:1) for binary Xi, and so the computational
N−1(cid:88)

σd(C2(x0, xk; t)) − ˆp1(t)2

cost could be high. The symmetry in a permutation set allows us to use

|p(y, t) − ˆp1(t)|2 dσd(y) =

m0

instead. But that costs O(N ), the same as the full permutation analysis.

When the Xi are binary, then for ﬁxed t, σd(C2(xk, xl; t)) just depends on

the swap distance r between xk and xl. Then

|p(y, t) − ˆp1(t)|2 dσd(y) =

NrV2(u(r); t, d) − ˆp1(t)2

Sd

r=0

for V2(u(r); t, d) given in Lemma 1 and Nr =(cid:80)N−1
be the N = (cid:0)m0+m1

pairs (xk, xl) at swap distance r.
Theorem 4. Let x0 ∈ Sd be the centered and scaled vector from an experiment
with binary Xi of which m0 are negative and m1 are positive. Let x0, x1, . . . , xN−1

(cid:1) distinct permutations of x0. If y ∼ U(Sd), then for

l=0 1(rk,l = r) counts

(cid:80)N−1

k=0

(3.7)

m0

(cid:90)

Sd

(cid:90)

1
N

k=0

m(cid:88)

1
N

He, Basu, Zhao & Owen/Stolarsky invariance for permutations

9

t ∈ [−1, 1], and with u(r) deﬁned in (2.2),

E(p(y; t)) = σd(C(y0; t)),

(cid:18)m0

(cid:19)(cid:18)m1

(cid:19)

and

m(cid:88)
(cid:1) permuted points xi at swap distance r from x0.

V2(u(r); t, d) − ˆp1(t)2.

1
N

r=0

r

Var(p(y, t)) =

Proof. There are(cid:0)m0

(cid:1)(cid:0)m1

r

r

r

4. A ﬁner approximation to the p-value

In the previous section, we studied the distribution of p-values with the spherical
cap centers y uniformly distributed on the sphere Sd. In this section, we give a
ﬁner approximation to p(y0, ˆρ) by studying the distribution of the p-values with
centers y satisfying the constraint (cid:104)y, xc(cid:105) = (cid:104)y0, xc(cid:105) = ˜ρ. The point xc may
be any permutation of x0. There are two special choices. The ﬁrst is to choose
c = 0 so that xc = x0 is the original unpermuted data. The second is to choose
xc to be the closest permutation of x0 to y0. That is c = arg maxi (cid:104)y0, xi(cid:105). We
will give a general formula that works for any choice of xc and compare the
performance of the above two choices.
The rationale for conditioning on all y satisfying (cid:104)y, xc(cid:105) = ˜ρ is as follows.
Since we want the exact p-value centered at y0 with radius ˆρ, the more tar-
geted set of p-values we study, the better our approximation should be. When
conditioning on (cid:104)y, xc(cid:105) = ˜ρ, we eliminate many irrelevant y. The approxima-
tion could be improved by conditioning on even more information, but the cost
would go up. If we condition on the order statistic of all inner products (cid:104)y0, xi(cid:105),
we get back the exact p-value.
For an index c ∈ {0, 1, . . . , N − 1} we propose ﬁner approximations to the

p-value based on Model 2 from Section 2. These are

˜pc = E2(p(y, ˆρ)) = E1(p(y, ˆρ) | yTxc = yT

0 xc).

We are interested in two special cases,

ˆp2 = ˜p0,

and ˆp3 = ˜pc, where

c = arg max
0(cid:54)i<N

(cid:104)y0, xi(cid:105).

(4.1)

(4.2)

For an illustration of Model 2 see Figure 2.
Notice that ˆp2 cannot go below 1/N because all of the points y that it includes
have x0 ∈ C(y; ˆρ). In fact x0 is on the boundary of this spherical cap. Since the
true value satisﬁes p (cid:62) 1/N , having ˆp2 (cid:62) 1/N is a desirable property. Similarly,
ˆp3 (cid:62) 1/N because then xc is in general an interior point of C(y, ˆρ). We expect
that ˆp3 should be more conservative than ˆp2 and we see this numerically in
Section 7.

From Proposition 1, we can get our estimate ˜pc and its mean squared error

by ﬁnding single and double inclusion probabilties for y.

He, Basu, Zhao & Owen/Stolarsky invariance for permutations

10

Fig 2: Illustration for Model 2. The original response vector is y0 with yT
0 x0 = ˆρ.
We consider alternative y uniformly distributed on the surface of C(x0; ˆρ) with
examples y1 and y2. Around each such yj there is a spherical cap of height ˆρ that
just barely includes xc = x0. We use ˆp2 = E2(p(y; ˆρ)) and ﬁnd an expression
for E2((ˆp2 − p(y; ˆρ))2).

To compute ˜pc we need to sum N values Pr(y ∈ C(xk; t) | yTxc = ˜ρ) and for
˜pc to be useful we must compute it in o(N ) time. The computations are feasible
in the binary case, which we now focus on.

on xc be y = ˜ρxc +(cid:112)1 − ˜ρ2y∗. Then the single and double point inclusion

j x0 for j = 1, 2, and let u3 = xT

1 x2. Let the projection of y

Let uj = xT

probabilities under Model 2 are

(cid:90)
(cid:90)

Sd−1

Sd−1

P1(u1, ˜ρ, ˆρ) =

P2(u1, u2, u3, ˜ρ, ˆρ) =

1((cid:104)y, x1(cid:105) ≥ ˆρ) dσd−1(y∗),
1((cid:104)y, x1(cid:105) ≥ ˆρ)1((cid:104)y, x2(cid:105) ≥ ˆρ) dσd−1(y∗)

and

(4.3)

(4.4)

where ˆρ = (cid:104)x0, y0(cid:105). If two permutations of x0 are at swap distance r, then their
inner product is u(r) = 1 − r(m−1

Lemma 2. Let the projection of x1 onto xc be x1 = u1xc +(cid:112)1 − u2

1 ) from equation (2.2).

0 + m−1

1. Then

1x∗

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllxcly0ly2ly1SdHe, Basu, Zhao & Owen/Stolarsky invariance for permutations

11

(cid:40)

1(˜ρu1 ≥ ˆρ),
σd−1(C(x∗

the single point inclusion probability from (4.3) is

u1 = ±1 or ˜ρ = ±1

P1(u1, ˜ρ, ˆρ) =

1, ρ∗)), u1 ∈ (−1, 1), ˜ρ ∈ (−1, 1)

where ρ∗ = (ˆρ − ˜ρu1)/(cid:112)(1 − ˜ρ2)(1 − u2
Proof. The projection of y onto xc is y = ˜ρxc +(cid:112)1 − ˜ρ2y∗. Now
(cid:40)
˜ρu1 +(cid:112)1 − ˜ρ2(cid:112)1 − u2

u1 = ±1 or ˜ρ = ±1

(cid:104)y, x1(cid:105) =

˜ρu1,

1 (cid:104)y∗, x∗

1(cid:105) , u1 ∈ (−1, 1), ˜ρ ∈ (−1, 1)

1).

(4.5)

and the result easily follows.

We can now give a computable expression for ˜pc and hence for ˆp2 and ˆp3.

Theorem 5. For −1 ≤ ˆρ ≤ 1, −1 ≤ ˜ρ ≤ 1,

˜pc = E2(p(y, ˆρ)) =

1
N

P1(u(r), ˜ρ, ˆρ)

(4.6)

m(cid:88)

(cid:18)m0

(cid:19)(cid:18)m1

(cid:19)

r

r

r=0

Proof. There are(cid:0)m0

where u(r) is given in equation (2.2), P1(u(r), ˜ρ, ˆρ) is given in equation (4.5)
and ˜ρ = xT

(cid:1) permutations of x0 at swap distance r from xc.

(cid:1)(cid:0)m1

c y0.

r

r

From (4.6) we see that ˜pc can be computed in O(m) work. The mean squared
error for ˜pc is more complicated and will be more expensive. We need the double
point inclusion probabilities and then we need to count the number of pairs
xk, xl forming a given set of swap distances among xk, xl, xc.

Lemma 3. For j = 1, 2, let xj be at swap distance rj from xc and let r3 be
the swap distance between x1 and x2. Let u1, u2, u3 be the corresponding inner
products given by (2.2). If there are equalities among x1, x2 and xc, then the
double point inclusion probability from (4.4) is



(cid:40)

P2(u1, u2, u3, ˜ρ, ˆρ) =

1(˜ρ ≥ ˆρ),
x1 = x2 = xc
1(˜ρ ≥ ˆρ)P1(u2, ˜ρ, ˆρ), x1 = xc (cid:54)= x2
1(˜ρ ≥ ˆρ)P1(u1, ˜ρ, ˆρ), x2 = xc (cid:54)= x1
x1 = x2 (cid:54)= xc.
P1(u2, ˜ρ, ˆρ),
If x1, x2 and xc are three distinct points with min(u1, u2) = −1, then

P2(u1, u2, u3, ˜ρ, ˆρ) =

1(−˜ρ ≥ ˆρ)P1(u2, ˜ρ, ˆρ), u1 = −1
1(−˜ρ ≥ ˆρ)P1(u1, ˜ρ, ˆρ), u2 = −1.

He, Basu, Zhao & Owen/Stolarsky invariance for permutations

12

+, +, +, +, +, · · · , +, +, +, +,

−, −, −, −, · · · , −, −, −, −, − )

+, +, +, · · · , +, −, −, −, · · · , −

,

+, +, +, · · · , +, +, +

, −, · · · , − )

xc = (

x1 = (

x2 = (

m1

m1

(cid:122)
(cid:122)
(cid:122)
+, · · · , +, −, −, · · · , −(cid:124) (cid:123)(cid:122) (cid:125)
(cid:125)

(cid:125)(cid:124)
(cid:125)(cid:124)
(cid:124)
(cid:125)(cid:124)
(cid:123)(cid:122)

(cid:124)

m1

δ1

r2

(cid:123)(cid:122)

r1

(cid:123)

(cid:122)

(cid:123)
(cid:125)

(cid:122)
(cid:124)

(cid:123)

(cid:122)

(cid:123)

(cid:123)

(cid:123)

(cid:123)(cid:122)

r1

(cid:125)(cid:124)

m0

(cid:125)(cid:124)

m0

(cid:125)(cid:124)
(cid:123)(cid:122)
(cid:123)(cid:122)

δ2

r2

(cid:124)
(cid:124)

(cid:125)
(cid:125)

(cid:125)

, +, · · · , +,

m0
−, · · · , −, +, +, · · ·

, −, · · · , − )

, +

Fig 3: Illustration of r1, r2, δ1 and δ2. The points xc, x1 and x2 each have m0
negative and m1 positive components. For j = 1, 2 the swap distance between
xj and xc is rj. There are δ1 positive components of xc where both x1 and x2
are negative, and δ2 negative components of xc where both xj are positive.

Otherwise −1 < u1, u2 < 1, and then

P2(u1, u2, u3, ˜ρ, ˆρ)

1(˜ρu1 ≥ ˆρ)1(˜ρu2 ≥ ˆρ),



(cid:82) 1
(cid:82) 1

−1

−1

=

(1 − t2)
(1 − t2)

ωd−2
ωd−1
ωd−2
ωd−1

d−1

2 −11(t ≥ ρ1)1(tu∗
2 −11(t ≥ ρ1)σd−2(C(x∗∗
2 ,

3 ≥ ρ2) dt,
√

d−1

√
ρ2−tu∗

3

1−t2

1−u∗2

3

˜ρ = ±1
˜ρ (cid:54)= ±1, u∗
˜ρ (cid:54)= ±1,|u∗

3 = ±1
3| < 1

)) dt,

where

u∗
3 =

(cid:112)1 − u2

(cid:112)1 − u2

u3 − u1u2

1

2

and ρj =

ˆρ − ˜ρuj

(cid:112)1 − ˜ρ2(cid:113)

1 − u2

j

, j = 1, 2

(4.7)

is the residual from the projection of x∗

2 on x∗
1.

and x∗∗
Proof. See Section 10.2.

2

Next we consider the swap conﬁguration among x1, x2 and xc. Let xj be
at swap distance rj from xc, for j = 1, 2. We let δ1 be the number of positive
components of xc that are negative in both x1 and x2. Similarly, δ2 is the
number of negative components of xc that are positive in both x1 and x2. See
Figure 3. The swap distance between x1 and x2 is then r3 = r1 + r2 − δ1 − δ2.
Let r = (r1, r2), δ = (δ1, δ2) and r = min(r1, r2). We will study values of

r1, r2, r3, δ1, δ2 ranging over the following sets:
r1, r2 ∈ R = {1, . . . , m}

δ1 ∈ D1(r) = {max(0, r1 + r2 − m0), . . . , r}
δ2 ∈ D2(r) = {max(0, r1 + r2 − m1), . . . , r},
r3 ∈ R3(r) = {max(1, r1 + r2 − 2r), . . . , min(r1 + r2, m, m0 + m1 − r1 − r2)}.

and

He, Basu, Zhao & Owen/Stolarsky invariance for permutations

13

Whenever the lower bound for one of these sets exceeds the upper bound, we
take the set to be empty, and a sum over it to be zero. Note that while r1 = 0
is possible, it corresponds to x1 = xc and we will handle that case specially,
excluding it from R.

The number of pairs (xl, xk) with a ﬁxed r and δ is

(cid:18)m0

(cid:19)(cid:18)m1

δ1

δ2

c(r, δ) =

(cid:19)(cid:18)m1 − δ2

(cid:19)(cid:18)m0 − r1

(cid:19)(cid:18)m1 − r1

(cid:19)

r1 − δ2

r2 − δ1

r2 − δ2

.

(4.8)

Then the number of conﬁgurations given r1, r2 and r3 is

c(r1, r2, r3) =

c(r, δ)1(r3 = r1 + r2 − δ1 − δ2).

(4.9)

(cid:19)(cid:18)m0 − δ1
(cid:88)
(cid:88)

r1 − δ1

δ1∈D1

δ2∈D2

We can now get an expression for the expected mean squared under Model 2
which combined with Theorem 5 for the mean provides an expression for the
mean squared error of ˜pc.
Theorem 6. For −1 ≤ ˆρ ≤ 1,−1 ≤ ˜ρ ≤ 1,

E2(p(y, ˆρ)2) =

+

+

1
N 2

1(˜ρ ≥ ˆρ) + 2

(cid:20)
(cid:18)m0
(cid:19)(cid:18)m1
(cid:19)
m(cid:88)
(cid:88)
(cid:88)
(cid:88)

r=1

r

r

r1∈R

r2∈R

r3∈R3(r)

m(cid:88)

(cid:18)m0

(cid:19)(cid:18)m1

(cid:19)

r

r

r=1

P2(1, u(r), u(r), ˜ρ, ˆρ)

P1(u(r), ˜ρ, ˆρ)

(4.10)

(cid:21)

c(r1, r2, r3)P2(u1, u2, u3, ˜ρ, ˆρ)

where P2(·) is the double inclusion probability in (4.4) and c(r1, r2, r3) is the
conﬁguration count in (4.9).

Proof. See Section 10.3 of the Appendix.

In our experience, the cost of computing E2(p(y, ˆρ)2) under Model 2 is domi-
nated by the cost of the O(m3) integrals required to get the P2(·) values in (4.10).
The cost also includes an O(m4) component because c(r1, r2, r3) is also a sum
of O(m) terms, but it did not dominate the computation at the sample sizes we
looked at (up to several hundred).

5. Generalized Stolarsky Invariance

Here we obtain the Model 2 results in a diﬀerent way, by extending the work
by Brauchart and Dick (2013). They introduced a weight on the height t of
the spherical cap in the average. We now apply a weight function to the inner
product (cid:104)z, xc(cid:105) between the center z of the spherical cap and a special point xc.

He, Basu, Zhao & Owen/Stolarsky invariance for permutations

14

Theorem 7. Let x0, . . . , xN−1 be arbitrary points in Sd and v(·) and h(·) be
positive functions in L2([−1, 1]). Then for any x(cid:48) ∈ Sd, the following equation

h((cid:104)z, x(cid:48)(cid:105))

1C(z;t)(xk)

dσd(z) dt

(cid:12)(cid:12)(cid:12)(cid:12)σd(C(z; t)) − 1
(cid:90)

(cid:90)

N

N−1(cid:88)

k=0

Sd

Sd

(cid:12)(cid:12)(cid:12)(cid:12)2

Kv,h,x(cid:48)(xk, xl) +

Kv,h,x(cid:48)(x, y) dσd(x) dσd(y)

(5.1)

holds,(cid:90) 1

(cid:90)

−1

=

1
N 2

v(t)

N−1(cid:88)
N−1(cid:88)

k,l=0

Sd

(cid:90)

− 2
N

Sd

k=0

where Kv,h,x(cid:48) : Sd × Sd → R is a reproducing kernel deﬁned by

Kv,h,x(cid:48)(x, y) =

v(t)

h((cid:104)z, x(cid:48)(cid:105))1C(z;t)(x)1C(z;t)(y) dσd(z) dt.

(5.2)

Kv,h,x(cid:48)(x, xk) dσd(x)

(cid:90) 1

−1

(cid:90)

Sd

Proof. See Section 10.4 of the Appendix.
Remark. We will use this result for x(cid:48) = xc, where xc is one of the N given
points. The theorem holds for general x(cid:48) ∈ Sd, but the result is computationally
and statistically more attractive when x(cid:48) = xc.

We now show that the second moment in Theorem 6 holds as a special limiting
case of Theorem 7. In addition to v from Section 3 we introduce η = (η1, η2) ∈
(0, 1)2 and

hη(s) = η2 +

η1( ωd−1
ωd

1

(1 − s2)d/2−1)

1(˜ρ ≤ s ≤ ˜ρ + η1)

(5.3)

Using these results we can now establish the following theorem, which pro-

vides the second moment of p(y, ˆρ) under Model 2.
Theorem 8. Let x0 ∈ Sd be the centered and scaled vector from an experiment
with binary Xi of which m0 are negative and m1 are positive. Let x0, x1, . . . , xN−1

(cid:1) distinct permutations of x0. Let xc be one of the xk and

be the N =(cid:0)m0+m1

(cid:90)

m0

let ˜pc be given by (4.1). Then

N−1(cid:88)
where y = ˜ρxc +(cid:112)1 − ˜ρ2y∗.

E2(p(y, ˆρ)2) =

1
N 2

k,l=0

Sd−1

1((cid:104)y, xk(cid:105) ≥ ˆρ)1((cid:104)y, xl(cid:105) ≥ ˆρ) dσd−1(y∗)

Proof. The proof uses Theorem 7 with a sequence of h deﬁned in (5.3) and v
deﬁned in (3.3). See Section 10.5 of the appendix.

This result shows that we can use the invariance principle to derive the second
moment of p(y, ˆρ) under Model 2. The mean square in Theorem 8 is consistent
with the second moment equation (3.6) in Proposition 1.

He, Basu, Zhao & Owen/Stolarsky invariance for permutations

15

6. Two sided p-values

In statistical applications it is more usual to report two-sided p-values. A con-
servative approach is to use 2 min(p, 1 − p) where p is a one-sided p-value. A
sharper choice is

N−1(cid:88)

k=0

p =

1
N

1(|xT

k y0| ≥ |ˆρ|).

(6.1)

This choice changes our Model 2 estimate. It also changes the second moment
of our Model 1 estimate.
The two-sided version of the estimate ˆp1(ˆρ) is 2σd(C(y;|ˆρ|)), the same as if
we had doubled a one-tailed estimate. Also E1(p) = ˆp1 in the two tailed case. We
now consider the mean square for the two-tailed estimate under Model 1. For
x1, x2 ∈ Sd with u = xT
1 x2, the two-tailed double inclusion probability under
Model 1 is

˜V2(u; t, d) =

1(|zTx1| ≥ |t|)1(|zTx2| ≥ |t|) dσd(z).

(cid:90)

Sd

Writing 1(|zTxi| ≥ |t|) = 1(zTxi ≥ |t|) + 1(zT(−xi) ≥ |t|) for i = 1, 2 and
expanding the product, we get

˜V2(u; t, d) = 2V2(u;|t|, d) + 2V2(−u;|t|, d).

By replacing V2(u, t, d) with ˜V2(u, t, d) and ˆp1(t) with 2σd(C(y;|t|)) in Theo-
rem 4, we get the variance of two-sided p-values under Model 1.

To obtain corresponding formulas under Model 2, we use the usual notations.
1 x2. Let the projection of y on xc

j x0 for j = 1, 2, and let u3 = xT

be y = ˜ρxc +(cid:112)1 − ˜ρ2y∗. Now

Let uj = xT

˜P1(u1, ˜ρ, ˆρ) =

˜P2(u1, u2, u3, ˜ρ, ˆρ) =

1(|(cid:104)y, x1(cid:105)| ≥ |ˆρ|) dσd−1(y∗),
1(|(cid:104)y, x1(cid:105)| ≥ |ˆρ|)1(|(cid:104)y, x2(cid:105)| ≥ |ˆρ|) dσd−1(y∗) (6.3)

(6.2)

and,

(cid:90)
(cid:90)

Sd−1

Sd−1

are the appropriate single and double inclusion probabilities.

After writing 1(|(cid:104)y, xi(cid:105)| ≥ |ˆρ|) = 1((cid:104)y, xi(cid:105) ≥ |ˆρ|) + 1((cid:104)y,−xi(cid:105) ≥ |ˆρ|) for

i = 1, 2 and expanding the product, we get

˜P1(u1, ˜ρ, ˆρ) = P1(u1, ˜ρ,|ˆρ|) + P1(−u1, ˜ρ,|ˆρ|),

and

˜P2(u1, u2, u3, ˜ρ, ˆρ) = P2(u1, u2, u3, ˜ρ,|ˆρ|) + P2(−u1, u2,−u3, ˜ρ,|ˆρ|)

+ P2(u1,−u2,−u3, ˜ρ,|ˆρ|) + P2(−u1,−u2, u3, ˜ρ,|ˆρ|).

Changing P1(u1, ˜ρ, ˆρ) and P2(u1, u2, u3, ˜ρ, ˆρ) to ˜P1(u1, ˜ρ, ˆρ) and ˜P2(u1, u2, u3, ˜ρ, ˆρ)

respectively in Theorem 5 and 6, we get the ﬁrst and second moments for two-
sided p-values under Model 2.
For a two-sided p-value, ˆp3 is calculated with xc where ˜c = arg maxi|(cid:104)y0, xi(cid:105)|.
For m0 = m1, ˜c = c = arg maxi (cid:104)y0, xi(cid:105), but the result may diﬀer signiﬁcantly
for unequal sample sizes.

He, Basu, Zhao & Owen/Stolarsky invariance for permutations

16

7. Numerical Results

We consider two-sided p-values in this section. First we evaluate the accuracy
of ˆp1, the simple spherical cap volume approximate p value. We considered
m0 = m1 in a range of values from 5 to 200. The values ˆp1 ranged from just
below 1 to 2 × 10−30. We judge the accuracy of this estimate by its root mean
squared error. Under Model 1 this is (E(ˆp1(ρ) − p(y, ρ))2)1/2 for y ∼ U(Sd).
Figure 4a shows this RMSE decreasing towards 0 as ˆp1 goes to 0 with ρ going
to 1. The RMSE also decreases with increasing sample size, as we would expect
from the central limit theorem.

As seen in Figures 4a and 4b, the RMSE is not monotone in ˆp1. Right at
ˆp1 = 1 we know that RMSE = 0 and around 0.1 there is a dip. The practically
interesting values of ˆp1 are much smaller than 0.1, and the RMSE is monotone
for them.
A problem with ˆp1 is that it can approach 0 even though p (cid:62) 1/N . The
Model 1 RMSE does not reﬂect this problem. By studying E2((ˆp1(ρ)−p(y, ρ))2)1/2,
we get a diﬀerent result. In Figure 4c, the RMSE of ˆp1 under Model 2 reaches a
plateau as ˆp1 goes to 0. The Model 2 RMSE reveals the ﬂaw in ˆp1 going below
1/N .

The estimator ˆp2 = ˜p0 performs better than ˆp1 because it makes more use
of the data, and it is never below 1/N . As seen in Figure 4d, the RMSE of ˆp2
very closely matches ˆp2 itself as ˆp2 decreases to zero. That is, the relative error
|ˆp2− p|/ˆp2 is well behaved for small p-values. Also as ˆp2 drops to the granularity
limit 1/N , its RMSE drops to 0.
The estimators ˆp1 and ˆp2, do not diﬀer much for larger p-values as seen in
Figure 5a. But in the limit as ˆρ → 1 we see that ˆp1 → 0, while ˆp2 approaches
the granularity limit 1/N instead.

Figure 5b compares the RMSE of the two estimators under Model 2. As
expected, ˆp2 is more accurate. It also shows that the biggest diﬀerences occur
only when ˆp1 goes below 1/N .

To examine the behavior of ˆp2 more closely, we plot its coeﬃcient of vari-
ation in Figure 6. We see that the relative uncertainty in ˆp2 is not extremely
large. Even when the estimated p-values are as small as 10−30 the coeﬃcient of
variation is below 5.

In Section 4, we mentioned another choice for xc. It was ˆp3 = ˜pc, where xc is
the closest permutation of x0 to y0. We compare ˆp3 to ˆp2 in Figures 7 and 8. We
ﬁxed the observed x0 and ρ, and then randomly sampled 100 vectors y0 with
(cid:104)y0, x0(cid:105) = ρ. All 100 of the y0 lead to the same value for ˆp2 and its standard
deviation. We get 100 diﬀerent estimates for ˆp3 and its standard deviation. We
varied m0 and m1, choosing ρ so that the values of ˆp2 are comparable at diﬀerent
sample sizes. Figure 7 shows the estimates ˆp3 with reference points for ˆp2. As
expected ˆp3 tends to be larger than ˆp2. Figure 8 shows the sample RMSEs for ˆp3
with reference points for the RMSE for ˆp2. The top row of plots has m0 = m1
while the bottom row has m1 = 2m0. The left column of plots are at larger
p-values than the rightmost column. We see that neither choice always has the
smaller RMSE, but ˆp2 is usually more accurate.

He, Basu, Zhao & Owen/Stolarsky invariance for permutations

17

(a) RMSE1(ˆp1).

(b) RMSE1(ˆp1) zoomed.

(c) RMSE2(ˆp1).

(d) RMSE2(ˆp2).

Fig 4: RMSEs for ˆp1 and ˆp2 under Models 1 and 2. The x-axis shows the estimate
ˆp as ρ varies from 1 to 0. Here m0 = m1. Plots with m0 (cid:54)= m1 are similar.

8. Comparison to saddlepoint approximation

Many approximation methods have been proposed for permutation tests. Zhou
et al. (2009) ﬁt approximations by moments in the Pearson family. Larson
and Owen (2015) ﬁt Gaussian and beta approximations to linear statistics and
gamma approximations to quadratic statistics for gene set testing problems.
Knijnenburg et al. (2009) ﬁt generalized extreme value distributions to the tails
of sampled permutation values.

These approximations do not come with an all inclusive p-value that accounts
for both numerical and sampling uncertainty. The sampling method does come
with such a p-value if we add one to numerator and denominator as Barnard

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−30−20−10−30−20−100log10(phat1)log10(RMSE1)(m0, m1)lllllllll5,510,1015,1520,2030,3040,4050,50100,100200,200llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−8−6−4−2−2.0−1.5−1.0−0.50.0log10(phat1)log10(RMSE1)(m0, m1)lllllllll5,510,1015,1520,2030,3040,4050,50100,100200,200lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−60−40−200−120−90−60−300log10(phat1)log10(RMSE1UNDER2)(m0, m1)lllllllll5,510,1015,1520,2030,3040,4050,5070,70100,100lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−60−40−200−60−40−200log10(phat2)log10(RMSE2)(m0, m1)lllllllll5,510,1015,1520,2030,3040,4050,5070,70100,100He, Basu, Zhao & Owen/Stolarsky invariance for permutations

18

(a)

(b)

Fig 5: Comparison of ˆp1 and ˆp2. In (a), log10(ˆp2) is plotted against log10(ˆp1) for
varying ρ’s. The black line is the 45 degree line. In (b), the ratio of RMSEs for
ˆp1 and ˆp2 is plotted against log10(ˆp1). The x-axis is log10(ˆp1).

Fig 6: The coeﬃcient of variation for ˆp2 with varying ρ’s.

(1963) suggests. But that method cannot attain very small p-values. Reasonable
power to attain p (cid:54)  requires a sample of somewhere between 3/ and 19/
random permutations (Larson and Owen, 2015).

The strongest theoretical support for approximate p-values comes from sad-
dlepoint approximations. Reid (1988) surveys saddlepoint approximations and
Robinson (1982) develops them for permutation tests of the linear statistics we
have considered here. When the true p-value is p, the saddlepoint approximation

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−60−40−200−120−90−60−300log10(phat1)log10(phat2)(m0, m1)lllllllll5,510,1015,1520,2030,3040,4050,5070,70100,100lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.000.250.500.751.00−120−90−60−300log10(phat1)RMSE2/RMSE1UNDER2(m0, m1)lllllllll5,510,1015,1520,2030,3040,4050,5070,70100,100lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0510−60−40−200log10(phat2)RMSE2/phat2(m0, m1)lllllllll5,510,1015,1520,2030,3040,4050,5070,70100,100He, Basu, Zhao & Owen/Stolarsky invariance for permutations

19

Fig 7: Comparison of ˆp3 versus ˆp2. For a given triple (m0, m1, ˆρ), we randomly
sample 100 vectors y0 with xT
0 y0 = ˆρ. By symmetry, x0 can be any permutation.
We get 100 diﬀerent ˆp3 and a common ˆp2 for each triple (m0, m1, ˆρ). In the two
panels on the left, ˆρ’s are chosen to give two-sided ˆp1(ˆρ) = 2 × 10−10 with
various dimensions (m0, m1). The two panels on the right correspond to two-
sided ˆp1(ˆρ) = 2 × 10−20. Estimates for two-sided p-values are plotted on the
y-axis, with ˆp3 plotted as black dots with distributions and ˆp2 as red crosses.

ˆps satisﬁes ˆps = p(1 + O(1/n)). Because we do not know the implied constant
in O(1/n) or the n at which it takes eﬀect, the saddlepoint approximation does
not provide a computable upper bound for the true permutation p-value p.

Our approximations ˆp2 and ˆp3 have an RMSE of the same order of magni-
tude as the estimate itself. In this, they are much more accurate than ˆp1 and
comparable to saddlepoint approximations. We can get an upper bound for the
p value with our method, under our model assumptions. Let µ = E(p(y, ˆρ) and
σ2 = Var(p(y, ˆρ)) for the observed value ˆρ = xT
0 y0 and for random y under
either Model 1 or 2. Then Pr(p (cid:62) µ + λσ) (cid:54) 1/(1 + λ2) for any λ > 0. Under
this model, p∗ = µ + λσ + 1/(1 + λ2) is a conservative p-value. Minimizing that
bound over λ reduces to solving 2λ = σ(1 + λ2)2. For small p we anticipate
λ (cid:29) 1 and hence λ(cid:48) = (2/σ)1/3 will be almost as good as the optimal λ we
could ﬁnd numerically. That choice leads to p∗ (cid:54) µ + (21/3 + 2−2/3)σ2/3. For
illustration, consider µ = 10−30 and σ = 3× 10−30, roughly describing the small
p-value estimates from the case m0 = m1 = 70. Then p∗ (cid:54) 4 × 10−20, much
larger than µ and yet still very small. Models 1 and 2 both hold for Gaussian
data Yi. In applications, the data may be nearly Gaussian, but not exactly so,
and hence there could be settings where our p∗ is too small. But this numerical

ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxoooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooox1e−091e−0720,2030,3040,4050,5060,6070,7080,8090,90100,100(m0, m1)phatEstimatexophat2phat3ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxoooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooox1e−181e−151e−1220,2030,3040,4050,5060,6070,7080,8090,90100,100(m0, m1)phatEstimatexophat2phat3ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxoooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooox1e−091e−0820,4030,6040,8050,10060,12070,14080,16090,180100,200(m0, m1)phatEstimatexophat2phat3ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxoooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooox1e−191e−171e−151e−1320,4030,6040,8050,10060,12070,14080,16090,180100,200(m0, m1)phatEstimatexophat2phat3He, Basu, Zhao & Owen/Stolarsky invariance for permutations

20

Fig 8: Comparison of RMSE(ˆp3) versus RMSE(ˆp2). The same simulation
setting as described in Figure 7. The red crosses and black dots are esti-
mated RMSE(ˆp2) and RMSE(ˆp3) under Model 2 with centers c = 0 and
c = arg max0(cid:54)i<N |(cid:104)y0, xi(cid:105)| respectively.

example uses a Chebychev inequality at λ(cid:48)
which ought to more than compensate for mild non-normality.

= 8.7 × 109 standard deviations,
.

Figures 9 to 12 compare our estimates to each other and those of the saddle-
point approximation, equation (1) from Robinson (1982). The sample sizes were
m0 = m1 = 10 making it feasible to compute the exact permutation p-value for
hundreds of examples. In each case we ran 500 independent simulations. Cases
with perfect separation were excluded: the saddlepoint approximation is numer-
ically unstable then, and one can easily detect that the minimum Y value in one
group is larger than the maximum in the other group, showing that p = 1/N .
Our estimates ˆp2 and ˆp3 are always at least as large as the granularity limit in
those cases. In every instance we compared two-sided p-values.
The simulated Yi values in group 0 come from the t(5), Exp(1), N (0, 1) and
U(0, 1) distributions in the four ﬁgures. The Yi values for group 1 are shifted
versions of those distributions. The naive spherical cap estimator ˆp1 is consis-
tently least accurate and is often much smaller than the true p. The saddlepoint
estimate is very accurate but tends to come out smaller than the true p. The
improved estimators ˆp2 and ˆp3 are less likely to be below p than the saddlepoint
estimate. We can also construct Z scores, Z2 = (p − ˆp2)/RMSE2 and a similar
Z3. If these take large values, then it means that ˆp is too small. The largest
Z scores we observed are in Table 1. The largest Z values arose for exponen-
.
= ˆp3. Such large p-values are not very
tial data with p

.
= 0.89 and ˆp2

.
= 0.78

ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxoooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooox1e−091e−0720,2030,3040,4050,5060,6070,7080,8090,90100,100(m0, m1)RMSE(phat)Estimatexophat2phat3ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxoooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooox1e−181e−151e−1220,2030,3040,4050,5060,6070,7080,8090,90100,100(m0, m1)RMSE(phat)Estimatexophat2phat3ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxoooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooox1e−101e−0820,4030,6040,8050,10060,12070,14080,16090,180100,200(m0, m1)RMSE(phat)Estimatexophat2phat3ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxoooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooox1e−191e−171e−151e−1320,4030,6040,8050,10060,12070,14080,16090,180100,200(m0, m1)RMSE(phat)Estimatexophat2phat3He, Basu, Zhao & Owen/Stolarsky invariance for permutations

21

Fig 9: Simulation results ˆp/p as described in the text, for Y0,i
Y1,i

iid∼ t(5) + 1.

iid∼ t(5), and

important and so maximal Z scores are also shown among estimated p-values
below 0.1.

9. Discussion

We have constructed approximations to the permutation p-value using proba-
bility and geometry derived from discrepancy theory. A rigorous upper bound
for p could be attained using L∞ spherical cap discrepancies instead of the L2
version, but computing such discrepancies is a major challenge. Narcowich et al.
(2010) give upper bounds for the L∞ spherical cap discrepancy, in terms of av-
erages of a great many harmonic functions at the points xi. For our application
we need bounds for spherical caps of a ﬁxed volume (under Model 1) and of
ﬁxed volume and constrained location (under Model 2) and those go beyond
what is in Narcowich et al. (2010).

phat1phat2phat3saddle.point05010015005010015011001100phat/p.exactcountHe, Basu, Zhao & Owen/Stolarsky invariance for permutations

22

Fig 10: Simulation results ˆp/p as described in the text, for Y0,i
Y1,i

iid∼ Exp(1) + 2.

iid∼ Exp(1), and

Our motivation came from gene set testing problems, where one tests whether
an ensemble of mutually correlated genes is associated with a binary variable.
Ackermann and Strimmer (2009) make a thorough numerical comparison of 261
gene set testing methods. There were two clear winners and they were partic-
ularly simple. One summed the individual genes’ t-statistics (the JG score of
Jiang and Gentleman (2007)) over the gene set and the other summed squared
t-statistics. Permutation methods were used because of correlations among the
genes within a gene set. Because the simulation was focussed on coordinated
small eﬀects of several genes, the results from the JG score were virtually iden-
tical to simply correlating the binary response with the average of the genes,
that is, to using a linear statistic like we study here.

To have reasonable power to obtain a p-value below  by permutation sam-
pling requires on the order of 1/ permutations each requiring O(n log(n)) com-
putation to generate and O(n) computation to evaluate the inner product. The

phat1phat2phat3saddle.point020406002040600.110.00.110.0phat/p.exactcountHe, Basu, Zhao & Owen/Stolarsky invariance for permutations

23

Fig 11: Simulation results ˆp/p as described in the text, for Y0,i
Y1,i

iid∼ N (2, 1).

iid∼ N (0, 1), and

cost to compute the standard errors in our method is dominated by a cost
proportional to m3 though there is a very small cost proportional to m4. In
the range where the ﬁrst cost dominates, our proposal is advantageous when
m3 = o(n/). Supposing that m0 and m1 are comparable, our advantage holds
when m2 = o(1/). If only the estimate and not the standard error is required,
then our ˆp2 and ˆp3 cost O(m) once the ˆρ (cost O(n)) has been computed. Then
the total cost is O(n) compared to O(n/) for sampling.

Acknowledgements

This work was supported by the US National Science Foundation under grants
DMS-1407397 and DMS-1521145. We thank John Robinson for helpful com-
ments.

phat1phat2phat3saddle.point025507510002550751000.11.010.00.11.010.0phat/p.exactcountHe, Basu, Zhao & Owen/Stolarsky invariance for permutations

24

Fig 12: Simulation results ˆp/p as described in the text, for Y0,i
Y1,i

iid∼ U(0, 1) + 1/2.

iid∼ U(0, 1), and

References

Ackermann, M. and Strimmer, K. (2009). A general modular framework for

gene set enrichment analysis. BMC Bioinformatics, 10:1–20.

Aronszajn, N. (1950). Theory of reproducing kernels. Transactions of the Amer-

ican Mathematical society, 68(3):337–404.

Barnard, G. A. (1963). Discussion of the spectral analysis of point processes
(by m. s. bartlett). Journal of the Royal Statistical Society, series B, 25:294.
Benjamini, Y. and Hochberg, Y. (1995). Controlling the false discovery rate:
a practical and powerful approach to multiple testing. Journal of the Royal
Statistical Society, Series B, pages 289–300.

Brauchart, J. and Dick, J. (2013). A simple proof of Stolarsky’s invariance
principle. Proceedings of the American Mathematical Society, 141(6):2085–
2096.

phat1phat2phat3saddle.point0501001500501001500.11.00.11.0phat/p.exactcountHe, Basu, Zhao & Owen/Stolarsky invariance for permutations

25

Dist’n Y0,i max Z2 max{Z2 | ˆp2 < 0.1} max Z3 max{Z3 | ˆp3 < 0.1}
t(5)
Exp(1)
U(0, 1)
N (0, 1)

1.91
7.55
2.45
2.78

3.87
7.76
2.61
2.78

26.7

7.55
3.49
3.07

31.5

7.76
5.87
3.07

Table 1

Maximal Z scores observed for ˆp2 and ˆp3.

Jiang, Z. and Gentleman, R. (2007). Extensions to gene set enrichment. Bioin-

formatics, 23(3):306–313.

Knijnenburg, T. A., Wessels, L. F. A., Reinders, M. J. T., and Shmulevich,
I. (2009). Fewer permutations, more accurate p-values. Bioinformatics,
25(12):i161–i168.

Larson, J. L. and Owen, A. B. (2015). Moment based gene set tests. BMC

Bioinformatics, 16(1):132.

Lee, Y. and Kim, W. C. (2014). Concise formulas for the surface area of the
intersection of two hyperspherical caps. Technical report, Korea advanced
institute of science and technology.

Lehmann, E. L. and Romano, J. P. (2005). Testing Statistical Hypotheses.

Springer, New York, 3rd edition.

Narcowich, F. J., Sun, X., Ward, J. D., and Wu, Z. (2010). Leveque type
inequalities and discrepancy estimates for minimal energy conﬁgurations on
spheres. Journal of Approximation Theory, 162(6):1256–1278.

Niederreiter, H. (1992). Random Number Generation and Quasi-Monte Carlo

Methods. SIAM, Philadelphia, PA.

Reid, N. (1988). Saddlepoint methods and statistical inference. Statistical Sci-

ence, pages 213–227.

Robinson, J. (1982). Saddlepoint approximations for permutation tests and
conﬁdence intervals. Journal of the Royal statistical society,Series B, pages
91–101.

Southworth, L. K., Kim, S. K., and Owen, A. B. (2009). Properties of balanced

permutations. Journal of Computational Biology, 16(4):625–638.

Stolarsky, K. B. (1973). Sums of distances between points on a sphere. II.

Proceedings of the American Mathematical Society, 41(2):575–582.

Zhou, C., Wang, H. J., and Wang, Y. M. (2009). Eﬃcient moments-based
In Advances in neural information processing systems,

permutation tests.
pages 2277–2285.

10. Appendix

Here we collect up some of the longer proofs.

He, Basu, Zhao & Owen/Stolarsky invariance for permutations

26

10.1. Proof of Theorem 3 (Limiting invariance)

Here we show that taking limits as  goes to zero in the formula of Brauchart
and Dick (2013) proves Theorem 3. We use three lemmas, one for each term in
Theorem 2. We use  → 0 as a shorthand for lim1→0+ lim2→0+.
Lemma 4. Let v be deﬁned as in (3.3). Then for ˆρ ∈ [−1, 1),

(cid:12)(cid:12)(cid:12)(cid:12)2

1C(z;t)(xk)

dσd(z) dt

Proof. Substituting v we get

(cid:90) 1

Sd

−1
|p(z, ˆρ) − ˆp(ˆρ)|2 dσd(z).

(cid:90)

N−1(cid:88)

k=0

N

(cid:12)(cid:12)(cid:12)(cid:12)σd(C(z; t)) − 1
(cid:17)(cid:90)

1(ˆρ ≤ t ≤ ˆρ + 1)

Sd

lim
→0

=

v(t)

Sd

(cid:90)
(cid:90) 1
(cid:16)
(cid:90) ˆρ+1

−1

2 +

1
1

(cid:90)

ˆρ

Sd

→ 1
1

(cid:90)

→

Sd

(ˆp(ˆρ) − p(z, ˆρ))2 dσd(z),

as 1 → 0+.

(ˆp(t) − p(z, t))2 dσd(z) dt

(ˆp(t) − p(z, t))2 dσd(z) dt,

as 2 → 0+

Lemma 5. Let v be as in (3.3) with ˆρ ∈ [−1, 1), and let Kv be given by (3.2).
Then for any x, x(cid:48) ∈ Sd,

Kv (x, x(cid:48)) = σd(C(x; ˆρ) ∩ C(x(cid:48); ˆρ)).

lim
→0

Proof. The argument is essentially the same as for Lemma 4.
Lemma 6. Let v be as in (3.3) with ˆρ ∈ [−1, 1), and let Kv be given by (3.2).
Then

(cid:90)

(cid:90)

Sd

lim
→0

Kv (x, y) dσd(x) dσd(y) = ˆp1(ˆρ)2.

(10.1)
Proof. For any x, y ∈ Sd, the kernel Kv (x, y) is nonnegative and upper bounded
by a constant. Therefore we can take our limit operations inside the double in-
Sd 1C(z; ˆρ)(x)1C(z; ˆρ)(y) dσd(z).

tegral over x and y. Now lim→0 Kv (x, y) = (cid:82)

Sd

Therefore the limit in (10.1) is

1C(z; ˆρ)(x)1C(z; ˆρ)(y) dσd(z) dσd(y) dσd(x) = ˆp1(ˆρ)2

after changing the order of the integrals.
Theorem 3 Let x0, x1, . . . , xN ∈ Sd and ˆρ ∈ [−1, 1]. Then

(cid:90)

|p(z, ˆρ) − ˆp1(ˆρ)|2 dσd(z) =

1
N 2

Sd

σd(C(xk; ˆρ) ∩ C(xl; ˆρ)) − ˆp1(ˆρ)2.

N−1(cid:88)

N−1(cid:88)

k=0

l=0

(cid:90)

(cid:90)

(cid:90)

Sd

Sd

Sd

He, Basu, Zhao & Owen/Stolarsky invariance for permutations

27

Proof. Theorem 2 gives us an identity and applying Lemmas 4, 5 and 6 to both
sides of it establishes (3.4) for ρ ∈ [−1, 1). For ˆρ = 1 we get the answer by
replacing v by 2 + (1/1)11−1(cid:54)t(cid:54)1 in the lemmas. Replacing z by y and ˆρ by
t above gives the version in the main body of the article.

10.2. Proof of Lemma 3 (Double inclusion for Model 2)

Proof. We split the proof into four cases and prove them individually. Recall
Sd−1 1((cid:104)y, x1(cid:105) (cid:62) ˆρ)1((cid:104)y, x2(cid:105) (cid:62) ˆρ) dσd−1(y∗) where

that P2(u1, u2, u3, ˜ρ, ˆρ) = (cid:82)
y = ˜ρxc +(cid:112)1 − ˜ρ2y∗.
(cid:90)

Case 1. x1 = x2 = xc, i.e., r1 = r2 = r3 = 0.

P2(1, 1, 1, ˜ρ, ˆρ) =
Case 2. x1 = xc (cid:54)= x2, i.e., r1 = 0, r2 > 0, r3 > 0.

Sd−1

1((cid:104)y, xc(cid:105) ≥ ˆρ)1((cid:104)y, xc(cid:105) ≥ ˆρ) dσd−1(y∗) = 1(˜ρ ≥ ˆρ).

P2(1, u2, u2, ˜ρ, ˆρ) =

Sd−1

1((cid:104)y, xc(cid:105) ≥ ˆρ)1((cid:104)y, x2(cid:105) ≥ ˆρ) dσd−1(y∗)

(cid:90)

= 1(˜ρ ≥ ˆρ)
= 1(˜ρ ≥ ˆρ)P1(u2, ˜ρ, ˆρ)

Sd−1

1((cid:104)y, x2(cid:105) ≥ ˆρ) dσd−1(y∗)

where the last step uses Lemma 2.

Case 3. x1 = x2 (cid:54)= xc, i.e., r1 = r2 > 0 = r3.

P2(u2, u2, 1, ˜ρ, ˆρ) =

=

Sd−1

Sd−1

1((cid:104)y, x1(cid:105) ≥ ˆρ)1((cid:104)y, x2(cid:105) ≥ ˆρ) dσd−1(y∗)
1((cid:104)y, x2(cid:105) ≥ ˆρ) dσd−1(y∗)

= P1(u2, ˜ρ, ˆρ).

Case 4. x1 (cid:54)= x2 (cid:54)= xc (cid:54)= x1, i.e., r1, r2, r3 > 0. We split this case into

subcases. First we assume u2 = −1, so

P2(u1, u2, u3, ˜ρ, ˆρ) =

Sd−1

1((cid:104)y, x1(cid:105) ≥ ˆρ)1((cid:104)y,−xc(cid:105) ≥ ˆρ) dσd−1(y∗)

(cid:90)

(cid:90)
(cid:90)

(cid:90)

= 1(−˜ρ ≥ ˆρ)
= 1(−˜ρ ≥ ˆρ)P1(u1, ˜ρ, ˆρ).

Sd−1

1((cid:104)y, x1(cid:105) ≥ ˆρ) dσd−1(y∗)

Similarly if u1 = −1, then

P2(u1, u2, u3, ˜ρ, ˆρ) = 1(−˜ρ ≥ ˆρ)P1(u2, ˜ρ, ˆρ).

Finally we assume u1 > −1 and u2 > −1, so now |u1| < 1 and |u2| <
j for j = 1, 2 and introduce

1. Recall the projections xj = ujcc +

1 − u2

j x∗

(cid:90)

(cid:113)

He, Basu, Zhao & Owen/Stolarsky invariance for permutations

28

further projections of y∗ and x∗
u∗
3x∗
isomorphic to Sd−2. Now we have

3x∗∗

2 . The residuals y∗∗ and x∗∗

2 onto x∗

1: y∗ = tx∗

1 − t2y∗∗ and x∗

2 =
2 belong to a subset of Sd that is

1 +

√

1((cid:104)y, x1(cid:105) ≥ ˆρ)1((cid:104)y, x2(cid:105) ≥ ˆρ) dσd−1(y∗)

(cid:90)
(cid:113)

1

Sd−2
1 − u2

(cid:16)

˜ρu1 +

1 − ˜ρ2

(cid:112)
(cid:112)

(cid:16)
2(tu∗

(cid:113)
(cid:113)
(cid:17)
(cid:112)1 − ˜ρ2(cid:112)1 − u2
(cid:112)

ˆρ − ˜ρu1

1 − t2

3 +

1

1 − u2

2(tu∗

3 +

d−1
2 −11

(cid:112)

1 − ˜ρ2

t ≥

(cid:113)

1 +(cid:112)1 − u∗
(cid:90)
(cid:90) 1

Sd−1

P2(u1, u2, u3, ˜ρ, ˆρ)

=

=

=

(1 − t2)

d−1
2 −1

(cid:112)

ωd−2
ωd−1

(cid:16)

˜ρu2 +

−1
× 1
× dσd−1(y∗∗) dt
(1 − t2)

(cid:90) 1

1 − ˜ρ2

ωd−2
ωd−1

(cid:90)

(cid:16)

1

Sd−2

−1
×
˜ρu2 +
× dσd−1(y∗∗) dt
1(˜ρu1 ≥ ˆρ)1(˜ρu2 ≥ ˆρ),
−1
−1
where u∗

(1 − t2)
(1 − t2)



ωd−2
ωd−1
ωd−2
ωd−1

(cid:82) 1
(cid:82) 1

d−1

d−1

=

(cid:17)
1t ≥ ˆρ
3 (cid:104)y∗∗, x∗∗

1 − u2

1 − u∗2

(cid:17)
2 (cid:105)) ≥ ˆρ

(cid:113)

(cid:17)
2 (cid:105)) ≥ ˆρ

1 − u∗2

3 (cid:104)y∗∗, x∗∗

1 − t2

2 −11(t ≥ ρ1)1(tu∗
2 −11(t ≥ ρ1)σd−2(C(x∗∗
2 ,

3 ≥ ρ2) dt,
√

√
ρ2−tu∗

3

1−t2

1−u∗2

3

˜ρ = ±1
˜ρ (cid:54)= ±1, u∗
˜ρ (cid:54)= ±1,|u∗

3 = ±1
3| < 1

)) dt,

3, ρ1, ρ2 are deﬁned in (4.7). Hence, the result follows.

10.3. Proof of Theorem 6 (Second moment under Model 2)

Proof. Without loss of generality we relabel the values xk so that c = 0. Any

other choice for c is reﬂected in the number (cid:101)ρ. The second moment is

E(p(y, ˆρ)2) =

1
N 2

N−1(cid:88)

N−1(cid:88)

k=0

l=0

P2(uk, ul, uk,l, ˜ρ, ˆρ)

(10.2)

where rk,l is the swap distance between points xk and xl. We will partition the
sum in (10.2) into the same four cases as in the proof of Lemma 3.

Case 1, xk = xl = xc, i.e., rk = rl = rk,l = 0. There is only one pair
of (xk, xl) for this condition. Hence, we get only one term corresponding to
P2(1, 1, 1, ˜ρ, ˆρ) = 1(˜ρ ≥ ˆρ).
Case 2, xk = xc (cid:54)= xl, i.e., rk = 0, rl = rk,l > 0. Consider all pairs of (xk, xl)
that satisfy this condition and let K2 denote their total contribution to (10.2).

He, Basu, Zhao & Owen/Stolarsky invariance for permutations

29

(cid:90)
N−1(cid:88)
(cid:18)m0
m(cid:88)

l=1

Sd−1

(cid:19)(cid:18)m1

(cid:19)

r

r

r=1

1((cid:104)y, xc(cid:105) ≥ ˆρ)1((cid:104)y, xl(cid:105) ≥ ˆρ) dσd−1(y∗)

P2(1, u(r), u(r), ˜ρ, ˆρ).

Case 3, xk = xl (cid:54)= xc, i.e., rk = rl > 0 = rk,l. The contribution from terms

1((cid:104)y, xk(cid:105) ≥ ˆρ) dσd−1(y∗) =

P1(u(r), ˜ρ, ˆρ).

Case 4, xk (cid:54)= xl (cid:54)= xc, i.e., rk, rl, rk,l > 0. The contribution of these cases

m(cid:88)

(cid:18)m0

(cid:19)(cid:18)m1

(cid:19)

r

r

r=1

Then

K2 = 2

= 2

of this form is

(cid:90)

N−1(cid:88)

K3 =

Sd−1

k=1

to the sum is

N−1(cid:88)
N−1(cid:88)
(cid:88)
(cid:88)

k=1

l=1

=

(cid:90)
(cid:88)

Sd−1

rk∈R

rl∈R

rk,l∈R3(r)

K4 =

1(l (cid:54)= k)

1((cid:104)y, xi(cid:105) ≥ ˆρ)1((cid:104)y, xj(cid:105) ≥ ˆρ) dσd−1(y∗)

c(rk, rl, rk,l)P2(u1, u2, u3, ˜ρ, ˆρ).

Then the second moment is (1(˜ρ (cid:62) ˆρ) + K2 + K3 + K4)/N 2.

10.4. Proof of Theorem 7 (Location weighted invariance)

Proof. We follow the technique in Brauchart and Dick (2013). We begin by
showing that Kv,h,x(cid:48) as deﬁned in (5.2) is a reproducing kernel. First, Kv,h,x(cid:48)
is symmetric: Kv,h,x(cid:48)(x, y) = Kv,h,x(cid:48)(y, x). Next, choose a0, . . . , aN−1 ∈ R and
x0, . . . , xN−1 ∈ Sd. Then

akalKv,h,x(cid:48)(xk, xl) equals

N−1(cid:80)

(cid:90) 1
(cid:90) 1

−1

(cid:90)
(cid:90)

Sd

=

−1

Sd

k,l=0

N−1(cid:88)

k,l=0

v(t)h((cid:104)z, x(cid:48)(cid:105))

(cid:12)(cid:12)(cid:12)(cid:12)N−1(cid:88)

k=0

akalv(t)h((cid:104)z, x(cid:48)(cid:105))1C(z;t)(xk)1C(z;t)(xl) dσd(z) dt

(cid:12)(cid:12)(cid:12)(cid:12)2

ak1C(z,t)(xk)

dσd(z) dt

which is nonnegative. Thus Kv,h,x(cid:48) is symmetric and positive deﬁnite, and so
by Aronszajn (1950), Kv,h,x(cid:48) is a reproducing kernel.
Aronszajn (1950) also shows that a reproducing kernel uniquely deﬁnes a
Hilbert space of functions with a speciﬁc inner product. Let Hv,h,x(cid:48) = H(Kv,h,x(cid:48), Sd)
denote the corresponding reproducing kernel Hilbert space of functions f : Sd →
R with reproducing kernel Kv,h,x(cid:48).

He, Basu, Zhao & Owen/Stolarsky invariance for permutations

30

We now consider functions f1, f2 : Sd → R which admit the representation

fi(x) =

gi(z; t)1C(z;t)(x) dσd(z) dt,

i = 1, 2

(10.3)

for functions gi ∈ L2(Sd×[−1, 1]). For any ﬁxed y ∈ Sd the function Kv,h,x(cid:48)(·, y)
has representation (10.3) via g(z; t) = v(t)h((cid:104)z, x(cid:48)(cid:105))1C(z,t)(y).

For functions with representation (10.3), we deﬁne an inner product by

(cid:104)f1, f2(cid:105)Kv,h,x(cid:48) =

v(t)
For y ∈ Sd and f1 ∈ Hv,h,x(cid:48),

−1

1

(cid:104)f1, Kv,h,x(cid:48)(·, y)(cid:105)Kv,h,x(cid:48) =

1

h((cid:104)z, x(cid:48)(cid:105))

g1(z, t)g2(z, t) dσd(z) dt.

(10.4)

(cid:90)

Sd

g1(z; t)v(t)
h((cid:104)z, x(cid:48)(cid:105))

h((cid:104)z, x(cid:48)(cid:105))1C(z;t)(y) dσd(z) dt

(cid:90) 1

(cid:90)

−1

Sd

(cid:90) 1

(cid:90)

Sd

1

(cid:90)

v(t)

(cid:90) 1
(cid:90) 1

−1

=

−1

Sd
= f1(y),

g1(z, t)1C(z,t)(y) dσd(z) dt

showing that the inner product (10.4) has the reproducing property. By Aron-
szajn (1950), the inner product in Hv,h,x(cid:48)
is unique. Functions fi satisfying
(10.3) with (cid:104)fi, fi(cid:105)Kv,h,x(cid:48) < ∞ are in Hv,h,x(cid:48), and (10.4) is the unique inner
product of Hv,h,x(cid:48).
We prove the theorem by equating two diﬀerent forms of (cid:107)R(Hv,h,x(cid:48);· )(cid:107)Kv,h,x(cid:48)

where

R(Hv,h,x(cid:48);· ) =

Kv,h,x(cid:48)(· , y) dσd(y) − 1
N

Sd

Kv,h,x(cid:48)(· , xk).

Although R(Hv,h,x(cid:48);· ) depends on our speciﬁc points xi we omit that from the
notation. The reproducing property of Kv,h,x(cid:48) yields

(cid:104)Kv,h,x(cid:48)(· , xk), Kv,h,x(cid:48)(· , xl)(cid:105)Kv,h,x(cid:48) = Kv,h,x(cid:48)(xk, xl)

(cid:90)

from which it follows that

(cid:28)(cid:90)
(cid:90)

=

Sd

(cid:90)

Sd

Sd

(cid:90)

Sd

Kv,h,x(cid:48)(· , y) dσd(y),

Kv,h,x(cid:48)(· , y(cid:48)) dσd(y(cid:48))

Kv,h,x(cid:48)(y, y(cid:48)) dσd(y) dσd(y(cid:48)).

Kv,h,x(cid:48)

(10.5)

N−1(cid:88)

k=0

(cid:29)

He, Basu, Zhao & Owen/Stolarsky invariance for permutations

31

Using (10.5) and the linearity of the inner product, we have

(cid:90)
(cid:104)R(Hv,h,x(cid:48);· ),R(Hv,h,x(cid:48);· )(cid:105)Kv,h,x(cid:48)

Kv,h,x(cid:48)(y, y(cid:48)) dσd(y) dσd(y(cid:48)) − 2
N

(cid:90)

N−1(cid:88)

Sd

k=0

Kv,h,x(cid:48)(y, xk) dσd(y)

(cid:90)
N−1(cid:88)

Sd

k,l=0

=

Sd

+

1
N 2

Kv,h,x(cid:48)(xk, xl).

For our second form of (cid:107)R(Hv,h,x(cid:48);· )(cid:107)Kv,h,x(cid:48) , we write
(cid:90)
R(Hv,h,x(cid:48);· )
(cid:90) 1
(cid:90) 1

Kv,h,x(cid:48)(· , y) dσd(y) − 1
N

N−1(cid:88)
(cid:20)(cid:90)
(cid:20)

1C(z;t)(x)h((cid:104)z, x(cid:105))

Kv,h,x(cid:48)(· , xk)

(cid:90)
(cid:90)

v(t)

−1

k=0

Sd

Sd

Sd

1C(z,t)(y) dσd(y) dt − 1
N

1C(z,t)(x)h((cid:104)z, x(cid:105))

σd(C(z, t)) − 1
N

N−1(cid:88)

k=0

=

v(t)

−1

Sd

=

=

1C(z,t)(xk)

(10.6)

(cid:21)

1C(z,t)(xk)

dσd(z) dt

dσd(z) dt.

N−1(cid:88)
(cid:21)

k=0

Hence using the deﬁnition of the inner product (cid:104)·,·(cid:105)Kv,h,x(cid:48) , we have

(cid:90)

(cid:90) 1
(cid:104)R(Hv,h,x(cid:48); x),R(Hv,h,x(cid:48); x)(cid:105)Kv,h,x(cid:48)

(cid:12)(cid:12)(cid:12)(cid:12)σd(C(x, t)) − 1

h((cid:104)z, x(cid:105))

v(t)

Sd

N

−1

=

(cid:12)(cid:12)(cid:12)(cid:12)2

1C(x;t)(xk)

N−1(cid:88)

k=0

(10.7)

dσd(x) dt.

Combining equations (10.6) and (10.7), we have the generalized location-weighted
version of the Stolarsky invariance principle.

10.5. Proof of Theorem 8 (Spatially weighed invariance)

As in Section 10.1, lim→0 means lim1→0+ lim2→0+ and similarly limη→0 de-
notes limη1→0+ limη2→0+. We prove a series of lemmas ﬁrst.
Lemma 7. For v(·) and hη(·) deﬁned by equations (3.3) and (5.3),

(cid:90) 1

lim
η→0

lim
→0

(cid:90)

=

Sd−1

v(t)

−1
|p(˜ρxc +

(cid:90)
(cid:112)

(cid:12)(cid:12)(cid:12)(cid:12)σd(C(z, t)) − 1

N−1(cid:88)

hη((cid:104)z, xc(cid:105))

Sd
1 − ˜ρ2y∗, ˆρ) − ˆp1(ˆρ)|2 dσd−1(y∗),

N

k=0

(cid:12)(cid:12)(cid:12)(cid:12)2

1C(z;t)(xk)

dσd(z) dt

where ˆp1(ˆρ) = σd(C(y; ˆρ)).

He, Basu, Zhao & Owen/Stolarsky invariance for permutations

32

Proof. This proof is similar to the others. First we take the limit  → 0 yielding

(cid:90)

hη((cid:104)z, xc(cid:105))

lim
η→0

Sd

(cid:12)(cid:12)(cid:12)(cid:12)σd(C(z, ˆρ)) − 1

N

N(cid:88)

k=1

√

1 − s2z∗ gives

Making the projection z = sxc +

(cid:12)(cid:12)(cid:12)(cid:12)2

1C(z; ˆρ)(xk)

dσd(z).

(cid:90)
(cid:90) 1
(cid:12)(cid:12)(cid:12)(cid:12)σd(C(sxc +

Sd−1

−1

ωd−1
ωd

(cid:112)
(cid:112)

lim
η→0

(cid:90)

(1 − s2)d/2−1hη(s)×

1 − s2z∗, ˆρ)) − 1
N

(cid:12)(cid:12)(cid:12)(cid:12)2

1−s2z∗; ˆρ)(xk)

√

1C(sxc+

dσd−1(z∗) ds

N(cid:88)

k=1

|p(ˆρxc +

1 − ˆρ2y∗, ˆρ) − ˆp1(ˆρ)|2 dσd−1(y∗).

=
Lemma 8. For v(·) and hη(·) deﬁned by equations (3.3) and (5.3) ,

Sd−1

N−1(cid:88)
(cid:90)

k,l=0

1
N 2

N−1(cid:88)

Sd−1

k,l=0

lim
η→0

lim
→0

=

1
N 2

N−1(cid:88)

k,l=0

1
N 2

(cid:90)

lim
η→0

Sd

Kv,hη,xc(xk, xl)

1((cid:104)y, xk(cid:105) ≥ ˆρ)1((cid:104)y, xl(cid:105) ≥ ˆρ) dσd−1(y∗).

Proof. First, limη→0 lim→0 N−2

Kv,hη,xc(xk, xl) equals

hη((cid:104)z, xc(cid:105))1C(z; ˆρ)(xk)1C(z; ˆρ)(xl) dσd(z).

Projecting z onto xc yields z = sxc +

1 − s2y∗ and then we have

N−1(cid:80)

k,l=0

√

(cid:90)

N−1(cid:88)
N−1(cid:88)

k,l=0

lim
η→0

(cid:90) 1
(cid:90)

−1

Sd−1

k,l=0

1
N 2

1
N 2

=

ωd−1
ωd

(1 − s2)d/2−1hη(s)

Sd−1

1C(z; ˆρ)(xk)1C(z; ˆρ)(xl) dσd−1(y∗)

1((cid:104)y, xk(cid:105) ≥ ˆρ)1((cid:104)y, xl(cid:105) ≥ ˆρ) dσd−1(y∗).

Lemma 9. For v(·) and hη(·) deﬁned by equations (3.3) and (5.3),

lim
η→0

lim
→0

Sd

Sd

Kv,hη,xc(x, y) dσd(x) dσd(y) = ˆp1(ˆρ)2

Proof. Because Kv,hη,xc is nonnegative and uniformly bounded we may take
the limit over  inside the integrals. Now

lim
→0

Kv,hη,xc(x, y) =

Sd

hη((cid:104)z, xc(cid:105))1C(z; ˆρ)(x)1C(z; ˆρ)(y) dσd(z),

(cid:90)

(cid:90)

(cid:90)

He, Basu, Zhao & Owen/Stolarsky invariance for permutations

33

and the limit becomes

(cid:90)

(cid:90)

(cid:90)

lim
η→0

Sd

Sd

Sd

hη((cid:104)z, xc(cid:105))1C(z; ˆρ)(x)1C(z; ˆρ)(y) dσd(z) dσd(x) dσd(y).

Integrating over z last we get limη→0

Sd hη((cid:104)z, xc(cid:105))ˆp2

1(ˆρ) dz = ˆp2

1(ˆρ).

Lemma 10. Under Model 2

lim
η→0

lim
→0

2
N

Kv,hη,xc(x, xk) dσd(x) = 2ˆp1(ˆρ)E(p(y, ˆρ)).

Proof. The argument here is similar to the one used for Lemma 9. Take the
limit over  inside the integral and change the order of integration to yield

(cid:90)

N−1(cid:88)

Sd

k=0

(cid:90)

N−1(cid:88)

k=0

(cid:82)

√

(cid:90)

lim
η→0

2ˆp1(ˆρ)

1
N

Sd

hη((cid:104)z, xc(cid:105))1C(z, ˆρ)(xk) dσd(z).

Substituting the projection z = txc +

1 − t2z∗ produces

(cid:90) 1

−1

(cid:90)

ωd−1
ωd

N−1(cid:88)

k=0

= 2ˆp1(ˆρ)

1
N
= 2ˆp1(ˆρ)E(p(y, ˆρ))

Sd−1

N−1(cid:88)

k=0

√

1

C( ˜ρxc+

1− ˜ρ2z∗, ˆρ)

(xk) dσd−1(z∗)

lim
η→0

2ˆp1(ˆρ)

(1 − t2)d/2−1hη(t)

1
N

Sd−1

√

1C(txc+

1−t2z∗, ˆρ)(xk) dσd−1(z∗) dt

for y under Model 2.

Proof of Theorem 8

Proof. The proof follows from using Lemmas 7 to 10 and Theorem 7.

