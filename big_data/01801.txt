6
1
0
2

 
r
a

M
6

 

 
 
]

V
C
.
s
c
[
 
 

1
v
1
0
8
1
0

.

3
0
6
1
:
v
i
X
r
a

Variational methods for Conditional Multimodal Learning:

Generating Human Faces from Attributes

Gaurav Pandey and Ambedkar Dukkipati

{gp88, ad}@csa.iisc.ernet.in

Indian Institute of Science

Bangalore, India

Abstract

Prior to this decade, the ﬁeld of computer vision was
primarily focused around hand-crafted feature extraction
methods used in conjunction with discriminative mod-
els for speciﬁc tasks such as object recognition, detec-
tion/localization, tracking etc. A generative image under-
standing was neither within reach nor the prime concern of
the period. In this paper, we address the following prob-
lem: Given a description of a human face, can we gener-
ate the image corresponding to it? We frame this problem
as a conditional modality learning problem and use vari-
ational methods for maximizing the corresponding condi-
tional log-likelihood. The resultant deep model, which we
refer to as conditional multimodal autoencoder (CMMA),
forces the latent representation obtained from the attributes
alone to be ’close’ to the joint representation obtained from
both face and attributes. We show that the faces generated
from attributes using the proposed model, are qualitatively
and quantitatively more representative of the attributes from
which they were generated, than those obtained by other
deep generative models. We also propose a secondary task,
whereby the existing faces are modiﬁed by modifying the
corresponding attributes. We observe that the modiﬁcations
in face introduced by the proposed model are representative
of the corresponding modiﬁcations in attributes. Hence, our
proposed method solves the above mentioned problem.

1. Introduction

In 2006, Hinton and his collaborators [10] made the ob-
servation that generative models are capable of learning
Gabor-like ﬁlters from images of characters. Not only was it
possible to sample MNIST-like characters from these mod-
els, the learned representations could also be discrimina-
tively ﬁne-tuned to improve the classiﬁcation performance
on MNIST characters. Though this spurred an interest in
models for unsupervised learning, the prime focus still re-

mained the achievement of improved performance on dis-
criminative tasks [20, 6]. It is only in the recent years, that,
achieving a deeper image understanding, which subsumes
the capability to generate realistic images, has started gain-
ing interest. Several generative models that build upon the
models for unsupervised learning of the previous decade
have been proposed[22, 1, 8, 15, 2].

In this paper, we attempt to solve the problem of gen-
erating faces given the set of attributes/tags that the face
must possess. One can think of images and attributes as
two modalities of the data [19, 23]. The ﬁrst instinct to ap-
proach this problem is to train a neural network from the
attributes to the faces. This will force the images gener-
ated from attributes to be close in image-space (in terms of
mean-squared error) to the actual image that corresponds to
the attributes. To keep things in perspective, assume that we
have only one attribute, say the ‘moustache’ attribute and
we train the model. If we generate a face with moustache
from the trained model, it will be an average in image-space
over all faces that contain a moustache. This is certainly
undesirable, since an average in image-space need not cor-
respond to a meaningful image at all. Another disadvantage
of this model is its unidirectional nature. For instance, if
we wish to modify the attributes of an already existing face
(say, add a moustache), we can not achieve the same using
a deterministic neural network.

Hence, we resort to a probabilistic model. Given enough
training examples, it is theoretically possible to learn a joint
distribution over the faces and the attributes. However,
since we are only interested in generating faces from at-
tributes, it is reasonable to learn a conditional distribution
on the faces given the attributes directly, rather than infer-
ring it from the joint distribution. Also referred to as con-
ditional learning [12], this method of learning has been im-
mensely successful for several tasks that include, but are
not limited to, natural language processing [3], computer
vision [9, 26] etc.
It is based on the principle that “one
should always solve the problem directly and not solve a
more general problem as an intermediate step” [25, 18].

4321

Since the conditional log-likelihood of the proposed
model is intractable, we use variational methods for train-
ing the model, whereby the posterior of the latent vari-
ables given the faces and the attributes is approximated by
a tractable distribution. While variational methods have
long been a popular tool for training graphical models,
their usage for deep learning became popular after the
reparametrization trick of [15, 21, 24]. Prior to that, mean-
ﬁeld approximation has been used for training deep Boltz-
mann machines (DBM) [22]. However, the training of
DBM involves solving a variational approximation prob-
lem for every instance in the training data. On the other
hand, reparametrizing the posterior allows one to single
parametrized variational approximation problem for all in-
stances in the training data simultaneously.

The proposed model is referred to as conditional multi-
modal autoencoder (CMMA). We use CMMA for the task
of generating faces from attributes, and to modify faces
in the training data, by modifying the corresponding at-
tributes. The dataset used is the cropped Labelled Faces
in the Wild dataset1 (LFW) [11]. We also compare the
qualitative and quantitative performance of CMMA against
other models that are capable of generating faces from
tags/attributes. These include conditional generative adver-
sarial networks (CGAN) [17, 7] and conditional variational
autoencoder (CVAE) [13]. More information about these
models is provided in Section 3.

The main contributions of this paper are as follows:

1. We frame the problem of generating faces from at-
tributes as a conditional modality learning problem,
whereby faces are generated by sampling a latent layer
conditioned on the attributes, and then sampling the
faces conditioned on the latent layer.

2. We derive the variational lower bound of the condi-
tional log-likelihood, and use stochastic backpropaga-
tion [21] (also known as SGVB [15]) to optimize this
lower bound.

3. We use the proposed model for generating faces from
attributes and modifying faces in the training data for
the Labelled Faces in the Wild (LFW) dataset.

4. We show that the proposed model is quantitatively and
qualitatively superior to CGAN [17, 7] and CVAE [13]
for the proposed task.

2. Problem Formulation and Proposed Solu-

tion
We formulate the problem as a multimodal learning
problem whereby the face corresponds to the modality that

Figure 1: A graphical representation of CMMA

we wish to generate and the attributes correspond to the
modality that we wish to condition on.

the problem is as

A formal description of

fol-
lows. We are given an i.i.d sequence of N datapoints
{(x(1), y(1)), . . . , (x(N ), y(N ))}.
For a ﬁxed datapoint
(x, y), let x be the modality that we wish to generate and
y be the modality that we wish to condition on. We as-
sume that x is generated by ﬁrst sampling a real-valued la-
tent representation z from the distribution p(z|y), and then
sampling x from the distribution p(x|z). The graphical rep-
resentation of the model is given in Figure 1. Furthermore,
we assume that the conditional distribution of the latent rep-
resentation z given y and the distribution of x given z are
parametric.

Given the above description of the model, our aim is
to ﬁnd the parameters so as maximize the conditional log-
likelihood of x given y for the given sequence of datapoints.
The computation of conditional log-likelihood requires the
marginalization of the latent variable z from the joint distri-
bution p(x, z|y).

(cid:90)
(cid:90)

p(x|y) =

=

p(x, z|y)dz

p(x|z)p(z|y)dz

(1)

(2)

For most choices of p(x|z) and p(z|y), the evaluation of
conditional log-likelihood is intractable. Hence, we resort
to the minimization of a variational lower bound to the con-
ditional log-likelihood. This is achieved by approximat-
ing the posterior distribution of z given x and y, that is
p(z|x, y) by a tractable distribution q(z|x, y). This is ex-
plained in more detail in the following section.
2.1. The variational bound

For

a

given

{(x(1), y(1)), . . . , (x(N ), y(N ))},
likelihood can be written as

i.i.d

collection

of

datapoints,
log-

conditional

the

log p

(cid:16)

x(1), . . . , x(N )|y(1), . . . , y(N )(cid:17)
N(cid:88)

log p(x(i)|y(i))

=

(3)

1The dataset is available at http://conradsanderson.id.au/lfwcrop/

i=1

4322

Distribution

p(z|y)
p(x|z)
q(z|x, z)

Parametric form
N (fµ(y), efσ(y))
N (gµ(z), egσ(z))

N (hµ(x, y), ehσ(x,y))

Representation

pf (z|y)
pg(x|z)
qh(z|x, z)

Table 1: Parametric forms for the distributions used in the
paper and their representations demonstrating the explicit
dependence on f, g and h.

Let q(z|x, y) be an approximation to the posterior distribu-
tion of the latent variables given x and y. For an individual
datapoint, this conditional log-likelihood can be rewritten
as

p(x, z|y)
p(z|x, y)

log p(x|y)
= Eq(z|x,y) log
= KL [q(.|x, y)||p(.|x, y)] + Eq(z|x,y) log
≥ Eq(z|x,y) log

p(x, z|y)
q(z|x, y)

,

p(x, z|y)
q(z|x, y)

(4)

(5)

where KL(p||q) refers to the KL-divergence between the
distributions p and q and is always non-negative. The term
in equation (5) is referred to as the variational lower bound
for the conditional log-likelihood for the datapoint (x, y)
and will be denoted by L(p, q; x, y).
It can further be
rewritten as

(6)

L(p, q; x, y)
p(z|y)
= Eq(z|x,y) log p(x|z) + Eq(z|x,y) log
q(z|x, y)
= Eq(z|x,y) log p(x|z) − KL [q(z|x, y)||p(z|y)]
From the last equation, we observe that the variational
lower bound can be written as the sum of two terms. The
ﬁrst term is the negative of reconstruction error of x, when
reconstructed from the joint encoding z of x and y. The
second term ensures that the joint encoding of x and y is
’close’ to the encoding of y alone, where closeness is de-
ﬁned in terms of KL-divergence between the corresponding
distributions.
2.2. The reparametrization

In order to simplify the computation of the variational
lower bound, we assume that conditioned on y, the latent
representation z is normally distributed with mean fµ(y)
and a diagional covariance matrix whose diagonal entries
are given by efσ(y). Moreover, conditioned on z, x is nor-
mally distributed with mean gµ(z) and a diagonal covari-
ance matrix whose diagonal entries are given by egσ(y). In

the rest of the paper, we assume fµ, fσ, gµ and gσ to be
multi-layer perceptrons. Furthermore, we approximate the
posterior distribution of z given x and y by a normal distri-
bution with mean hµ(x, y) and a diagonal covariance ma-
trix whose diagonal entries are given by ehσ(x,y), where
hµ and hσ are again multi-layer perceptrons. In order to
make the dependence of the distributions on f, g and h ex-
plicit, we represent p(z|y) as pf (z|y), p(x|z) as pg(x|z)
and q(z|x, y) as qh(z|x, y). For reference, the parametric
forms of the likelihood, prior and posterior distributions and
their representations demonstrating the explicit dependence
on f, g and h are given in Table 1.
The above assumptions simplify the calculation of KL-
divergence and log p(x|z). Let fj denote the jth component
of the function f and the size of the latent representation be
J. After ignoring the constant terms, the KL-divergence
term of the variational lower bound can be written as
KL(qh(z|x, y)||pf (z|y)) =
1
2

fjσ(y) − hjσ(x, y) + exp (hjσ(x, y) − fjσ(y))

J(cid:88)

(cid:20)

j=1

(cid:21)

(hjµ(x, y) − fjµ(y))2

exp(fjσ(y))

+

(7)

The negative reconstruction error term in the variational
lower bound in (6) can be obtained by generating samples
from the posterior distribution of z given x and y, and then
averaging over the negative reconstruction error. For a ﬁxed
z, the term can be written as

(cid:34) m(cid:88)

l=1

m(cid:88)

(glµ(z) − xl)2
2 exp(glσ)

+

log pg(x|z) = −

(cid:35)

exp(glσ(z))

l=1

(8)

The choice of the posterior allows us to sample z as follows:

 ∼ N (0, I)
z = hµ(x, y) +  (cid:12) hσ(x, y)

(9)
(10)
where (cid:12) denotes elementwise multiplication. Hence, the
negative reconstruction error can alternatively be rewritten
as

E∼N (0,I) log pg(x|hµ(x, y) +  (cid:12) hσ(x, y)) ,

(11)

where log pg(x|.) is as deﬁned in (8)

In order to train the model using ﬁrst-order methods,
we need to compute the derivative of the variational lower
bound with respect to the parameters of the model. Let
θf , θg and θh be the parameters of {fµ, fσ}, {gµ, gσ} and
{hµ, hσ} respectively. Note that the KL-divergence term in
(7) depends only on {fµ, fσ}, and {hµ, hσ}. Its derivatives
with respect to θf and θh can be computed via chain rule.

4323

From (11), the derivative of the negative reconstruction

error with respect to {θg, θh} is given by
E∼N (0,I)∇{θg,θh} log pg(x|hµ(x, y) +  (cid:12) hσ(x, y))
(12)
The term inside the expectation can again be evaluated us-
ing chain rule.
2.3. Implementation details

We use minibatch training to learn the parameters of
the model, whereby the gradient of the model with respect
to the model parameters {θf , θg, θh} is computed for ev-
ery minibatch and the corresponding parameters updated.
While the gradient of the KL-divergence can be computed
exactly from (7), the gradient of the negative reconstruction
error in (11) requires one to sample standard normal random
vectors, compute the gradient for each sampled vector, and
then take the mean. In practise, when the minibatch size is
large enough, it is sufﬁcient to sample one standard normal
random vector per training example, and then compute the
gradient of the negative reconstruction error with respect to
the parameters, for this vector. This has also been observed
for the case of variational autoencoder in [15].

A pictorial representation of the implemented model is
given in Figure 2. Firstly, x and y are fed to the neural net-
work h to generate mean and log-variance of the distribu-
tion qh(z|x, y). Moreover, y is fed to the neural network
f to generate the mean and log-variance of the distribu-
tion pf (z|y). The KL-divergence between qh(z|x, y) and
pf (z|y) is computed using (7), and its gradient is backprop-
agated to update the parameters θf and θh. Furthermore, the
mean and log-variance of qh(z|x, y) are used to sample z,
which is then forwarded to the neural network g to compute
the mean and log-variance of the distribution pg(x|z). Fi-
nally, the negative reconstruction error is computed using
equation (8) for the speciﬁc z and its gradient is backprop-
agated to update the parameters θg and θh.

3. Related Works

Over the past few years, several deep generative mod-
els have been proposed. They include deep Boltzmann
machines (DBM) [22], generative adversarial networks
(GAN) [8], variational autoencoders (VAE) [15] and gen-
erative stochastic networks (GSN) [1].

DBMs learn a Markov random ﬁeld with multiple la-
tent layers, and have been effective in modelling MNIST
and NORB data. However, the training of DBMs involves
a mean-ﬁeld approximation step for every instance in the
training data, and hence, they are computationally expen-
sive. Moreover, there are no tractable extensions of deep
Boltzmann machines for handling spatial equivariance.

All the other models mentioned above, can be trained us-
ing backpropagation or its stochastic variant, and hence can

Figure 3: A graphical representation of conditional VAE as well
as conditional GAN

incorporate the recent advances in training deep neural net-
works such as faster libraries and better optimization meth-
ods.
In particular, GAN learns a distribution on data, by
forcing the generator to generate samples that are ‘indistin-
guishable’ from training data. This is achieved by learning a
discriminator whose task is to distinguish between the gen-
erated samples and samples in the training data. The gen-
erator is then trained to fool the discriminator. Though this
approach is intuitive, it requires a careful selection of hyper-
parameters. Moreover, given the data, one can not sample
the latent variables from which it was generated, since the
posterior is never learnt by the model.

In a VAE, the posterior distribution of the latent vari-
ables conditioned on the data, is approximated by a nor-
mal distribution, whose mean and variance are the output
of a neural network (distributions other than normal can
also be used). This allows approximate estimation of varia-
tional log-likelihood which can be optimized using stochas-
tic backpropagation [21].

Both GAN and VAE are directed probabilistic models
with an edge from the latent layer to the data. Condi-
tional extensions of both these models for incorporating at-
tributes/labels have also been proposed [14, 7, 17]. The
graphical representation of a conditional GAN or condi-
tional VAE is shown in Figure 3. As can be observed, both
these models assume the latent layer to be independent of
the attributes/labels. This is in stark contrast with our model
CMMA, which assumes that the latent layer is sampled con-
ditioned on the attributes.

It is also informative to compare the variational lower
bound of conditional log-likelihood for a CVAE with (6).
The lower bound for a CVAE is given by
log p(x|y) ≥ Eq(z|x,y) log p(x|y, z)−KL(q(z|x, y)||p(z))
(13)
Note that while the lower bound in the proposed model
CMMA contains a KL-divergence term to explicitly force
the latent representation from y to be ’close’ to the latent
representation from both x and y, there is no such term in
the lower bound of CVAE. This proves to be a disadvantage
for CVAE as is reﬂected in the experiments section.

4324

Figure 2: A pictorial representation of the implemented model. The KL-divergence between qh(z|x, y) and pf (z|y)

is computed using (7) and backpropagated to update the parameters θh and θf . Similarly, the negative reconstruction error is

computed using equation (8) for the speciﬁc z and its gradient is backpropagated to update the parameters θg and θh.

4. Experiments

convolutional layer.

For our experiments, we use the cropped Labelled Faces
in the Wild dataset2 (LFW) [11], which consists of 13, 233
faces of 5749 people of which 4069 people have only one
image. The images are of size 64 × 64 and contain 3 chan-
nels (red green and blue). Of the 13, 233 faces, 13, 143 faces
have 73 attributes associated with them, obtained partially
using Amazon Mechanical Turk and partially using attribute
classiﬁers [16]. The attributes include ‘Male’, ‘Asian’, ‘No
eye-wear’, ‘Eyeglasses’, ‘Moustache’, ‘Mouth open’, ‘Big
nose’, ‘Pointy nose’, ‘Smiling’, ‘Frowning’, ‘Big lips’ etc.
The data also contains attributes for hair, necklace, earrings
etc., though these attributes are not visible in the cropped
images. We use the ﬁrst 10, 000 faces and the correspond-
ing 73 attributes for training the model, the next 1000 faces
for validation, and keep the remaining faces and their cor-
responding attributes for testing.
4.1. CMMA architecture

The MLP {fµ, fσ} of the CMMA used in this paper (re-
fer Figure 2) encodes the attributes, and is a neural network
with 800 hidden units, a soft thresholding unit of the form
log(1 + ex) and two parallel output layers, each comprising
of 500 units. The MLP {hµ, hσ} is a convolutional neural
network, and comprises of the following layers:
1. Three convolutional layers with 192 ﬁlters of size 3 ×
5× 5, 96 ﬁlters of size 192× 5× 5 and 48 ﬁlters of size
96 × 4 × 4 in the ﬁrst, second and third convolutional
layers respectively.

2. A 2 × 2 max-pooling layer after the ﬁrst and second
2The dataset is available at http://conradsanderson.id.au/lfwcrop/

3. A non-negative rectiﬁcation layer after every max-

pooling layer.

4. A non-negative rectiﬁcation layer after the last convo-

lutional layer.

5. Two parallel linear layers that maps the output of the
last rectiﬁcation layer to two output layers of size 500.

Note that, we do not explicitly feed the attributes to the
MLP {hµ, hσ}, since we did not observe any improvement
in performance when the attributes were fed to the MLP
{hµ, hσ}. The MLP {gµ, gσ} is a deconvolutional network
and comprises of the following layers:

1. A linear layer that maps the latent representation z to
4800 dimensions and a reshaping layer to reshape tit to
48 feature maps of size 10 × 10.

2. Two convolutional layers with 96 ﬁlters of size 48 ×
3 × 3 and 192 ﬁlters of size 96 × 3 × 3 in the ﬁrst and
second convolutional layers respectively.

3. A 2× 2 nearest neighbour upsampling layer before ev-

ery convolutional layer.

4. A A non-negative rectiﬁcation layer after the ﬁrst two

convolutional layer.

5. Two parallel convolutional layers at the output with
192 ﬁlters of size 3×5×5. The ﬁrst convolutional layer
at the output is followed by a sigmoid non-linearity.

4325

Model
CMMA
CVAE

Variational Lower Bound

17,973
14,356

Model
CMMA
CVAE
CGAN

Conditional Log-likelihood

9,487
8,714
8,320

Table 2: Variational lower bound to the conditional log-likelihood
for the models CMMA and CVAE (Higher means better).

4.2. Models used for comparison

We compare the quantitative and qualitative performance
of CMMA against conditional Generative Adversarial Net-
works [17, 7] (CGAN) and conditional Variational Autoen-
coders [15] (CVAE). We have tried to ensure that the ar-
chitecture of the models used for comparison is as close as
possible to the architecture of the CMMA used in our exper-
iments. Hence, the generator and discriminator of CGAN
and the encoder and decoder of CVAE closely mimic the
MLPs g and h of CMMA as described in the previous sec-
tion.
4.3. Training

We coded all the models in Torch [4] and trained each
of them for 500 iterations on a Tesla K40 GPU. For each
model, the training time was approximately 1 day. The ada-
grad optimization algorithm was used [5]. The proposed
model CMMA was found to be relatively stable to the se-
lection of initial learning rate, and the variance of the ran-
domly initialized weights in various layers. For CGAN,
we selected the learning rate of generator and discrimina-
tor and the variance of weights by verifying the conditional
log-likelihood on the validation set. Only the results from
the best hyperparameters have been reported. We found the
CGAN model to be quite unstable to the selection of hyper-
parameters.
4.4. Quantitative results

For the ﬁrst set of experiments, we compare the con-
ditional log-likelihood of the faces given the attributes on
the test set for the 3 models - CMMA, CGAN, and CVAE.
A direct evaluation of conditional log-likelihood is infeasi-
ble, and for the size of latent layer used in our experiments
(500), MCMC estimates of conditional log-likelihood are
unreliable.

For the proposed model CMMA, a variational lower
bound to the log-likelihood of the test data can be computed
as the difference between the negative reconstruction error
and KL-divergence (see (5)). The same can also be done for
the CVAE model using (13). The corresponding values are
provided in Table 2.

Since we can not obtain the variational lower bound
for the other models, we use Parzen-window based log-
likelihood estimation method for comparing the 3 models.

Table 3: Parzen window based estimates of conditional log-
likelihood for the test data (Higher means better).

In particular, for a ﬁxed test instance, we condition on the
attributes to generate samples from the 3 models. A Gaus-
sian Parzen window is ﬁt to the generated samples, and
the log-probability of the face in the test instance is com-
puted for the obtained Gaussian Parzen window. The σ-
parameter of the Parzen window estimator is obtained via
cross-validation on the validation set. The corresponding
log-likelihood estimates for the 3 models are given in Ta-
ble 3.

In both the cases, the proposed model CMMA was able
to achieve a better conditional log-likelihood than the other
models.
4.5. Qualitative results

While the quantitative results do convey a sense of su-
periority of the proposed model over the other models used
in comparison, it is more convincing to look at the actual
samples generated by these models. Hence, we compare
the three models CGAN, CVAE and CMMA for the task
of generating faces from attributes. We also compare the
two models CVAE and CMMA for modifying an existing
face by changing the attributes. CGAN can not be used for
modifying faces because of the uni-directional nature of the
model, that is, it is not possible to sample the latent layer
from an image in a generative adversarial network.

4.5.1 Generating faces from attributes

In our ﬁrst set of experiments, we generate samples from the
attributes using the 3 already trained models. In a CGAN,
the images are generated by feeding noise and attributes to
the generator. Similarly, in a CVAE, noise and attributes
are fed to the MLP that corresponds to p(x|z, y) (see (13))
to sample the images.
In order to generate images from
attributes in a CMMA, we prune the MLP {hµ, hσ} from
the CMMA model (refer Figure 2), and connect the MLP
{fµ, fσ} in its stead as shown in Figure 5.

We set/reset the ’Male’ and ’Asian’ attributes to gener-
ate four possible combinations. The faces are then gener-
ated by varying the other attributes one at a time. In order
to remove any bias from the selection of images, we set
the variance parameter of the noise level to 0 in CMMA,
CVAE and CGAN. The corresponding faces for our model
CMMA, and the other models (CVAE [14] and CGAN [7])

4326

(a) Faces generated from our model CMMA.

(b) Faces generated from CVAE [14].

(c) Faces generated from CGAN [7] using the hyperparameters
selected by us.

(d) Faces generated from CGAN [7] using the hyperparameters
used in [7].

Figure 4: Faces generated from the attributes using various models (Best viewed in color). For a ﬁxed model, the 4 rows correspond to
‘Female Asian’, ’Female Not-Asian’, ’Male Asian’ and ’Male Not-Asian’ in order. The remaining attributes are varied one at a time to
generate the 7 columns. In particular, for each model, the 7 columns of faces correspond to i) no change, ii) mouth open, iii) spectacles,
iv) bushy eyebrows, v) big nose ,vi) pointy nose and vii) thick lips. Note that, for our model CMMA, any change in attributes, such as
mouth open, spectacles etc., is clearly reﬂected in the corresponding face. For other models, this change is not very evident from the
corresponding faces.

cles, iv) bushy eyebrows, v) big nose ,vi) pointy nose and
vii) thick lips. As is evident from the ﬁrst image in Fig-
ure 4, CMMA can incorporate any change in attribute such
as ‘open mouth’ or ‘spectacles’ in the corresponding face
for each of the 4 rows. However, this does not seem to be
the case for the other models. We hypothesize that this is
because our model explicitly minimizes the KL-divergence
between the latent representation of attributes and the joint
representation of face and attributes.

4.5.2 Varying the attributes in existing faces

In our next set of experiments, we select a face from the
training data, and vary the attributes to generate a modiﬁed
face. For a CMMA, this can be achieved as follows (also
refer Figure 2):

1. Let attr orig be the original attributes of the face and
attr new be the new attributes that we wish the face to
possess.

Figure 5: The model used for generating faces from attributes
in CMMA is obtained by removing the MLP {hµ, hσ} from the
CMMA model (refer Figure 2), and connecting the MLP {fµ, fσ}
in its stead.

are listed in Figure 4. We have also presented the results
from the implementation of CGAN3 by the author of [7],
since the images sampled from CGAN trained by us were
quite noisy.

The 7 columns of images for each model correspond
to the attributes i) no change, ii) mouth open, iii) specta-

3https://github.com/hans/adversarial

4327

(a) Unchanged image

(b) After removal of spectacles with CMMA

(c) After removal of spectacles with CVAE

Figure 7: Removal of spectacles with CMMA and CVAE (Best
viewed in color). The resultant eyes look more natural in case of
CMMA than CVAE.

corresponding to our model CMMA and CVAE. The cor-
responding transformed faces are given in Figure 6a and
Figure 6b respectively. As can be observed, for most of
the attributes, our model, CMMA, is successfully able to
transform images by removing moustaches, adding specta-
cles and making the nose bigger or pointy etc. However,
some transformations such as addition of moustaches (4th
row of Figure 6a) also create the impression of an open
mouth, thereby suggesting that the latent representation of
an open-mouth is correlated with the latent representation
for a moustache.

Some more comparative experimental results are given

in the supplementary.

5. Concluding Remarks

In this paper, we proposed a model for generating faces
from attributes/tags, that forces the latent representation of
attributes to be ‘close’ to the joint representation for face
and attributes. Quantitative and qualitative results suggest
that our model is more suitable for this task than CGAN [7]
and CVAE [14]. Moreover, for the task of modifying faces,
by modifying the attributes, CMMA does an arguably better
job than CVAE.

References
[1] Y. Bengio, E. Thibodeau-Laufer, G. Alain, and J. Yosinski.
Deep generative stochastic networks trainable by backprop.
In Proceedings of The 31st International Conference on Ma-
chine Learning, pages 226–234.

[2] Y. Bengio, L. Yao, G. Alain, and P. Vincent. Generalized
denoising auto-encoders as generative models. In C. Burges,
L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger,
editors, Advances in Neural Information Processing Systems
26, pages 899–907. Curran Associates, Inc., 2013.

(a) Modifying faces with CMMA

(b) Modifying faces with CVAE

Figure 6: Modifying the faces in the training data by modifying
the corresponding attributes using CMMA and CVAE respectively
(Best viewed in color). The rows in each of the above ﬁgures cor-
respond to i) No change, ii) Big Nose, iii) Spectacles, iv) Mous-
tache and v) Big lips. Except for spectacles, any other change in
attributes is not reﬂected in the faces modiﬁed by CVAE.

2. Pass the selected face and the attr new through the

MLP {hµ, hσ}.

3. Pass attr orig and attr new through the MLP {fµ, fσ}

and compute the difference.

4. Add the difference to the output of MLP {hµ, hσ}.
5. Pass the resultant sum through the decoder {gµ, gσ}.

As in the previous case, we have the set the variance param-
eter of noise level to 0.

Note that, we can not use CGAN for this set of experi-
ments, since, given a face, it is not possible to sample the
latent layer in a CGAN. Hence, we only present the results

4328

[19] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng.
Multimodal deep learning. In Proceedings of the 28th inter-
national conference on machine learning (ICML-11), pages
689–696, 2011.

[20] M. A. Ranzato, F. J. Huang, Y.-L. Boureau, and Y. LeCun.
Unsupervised learning of invariant feature hierarchies with
applications to object recognition. In IEEE Conference on
Computer Vision and Pattern Recognition, pages 1–8. IEEE,
2007.

[21] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic
backpropagation and approximate inference in deep genera-
tive models. In T. Jebara and E. P. Xing, editors, Proceedings
of the 31st International Conference on Machine Learning
(ICML-14), pages 1278–1286. JMLR Workshop and Con-
ference Proceedings, 2014.

[22] R. Salakhutdinov and G. E. Hinton. Deep Boltzmann ma-
chines. In International Conference on Artiﬁcial Intelligence
and Statistics, pages 448–455, 2009.

[23] N. Srivastava and R. R. Salakhutdinov. Multimodal learn-
ing with deep Boltzmann machines. In Advances in Neural
Information Processing Systems, pages 2222–2230, 2012.

[24] A. Stuhlm¨uller, J. Taylor, and N. Goodman.

Learning
stochastic inverses.
In C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Weinberger, editors, Advances in
Neural Information Processing Systems 26, pages 3048–
3056. Curran Associates, Inc., 2013.

[25] V. N. Vapnik. Statistical learning theory, volume 1. Wiley

New York, 1998.

[26] S. B. Wang, A. Quattoni, L.-P. Morency, D. Demirdjian,
and T. Darrell. Hidden conditional random ﬁelds for ges-
ture recognition. In IEEE Conference on Computer Vision
and Pattern Recognition, volume 2, pages 1521–1527. IEEE,
2006.

[3] A. L. Berger, V. J. D. Pietra, and S. A. D. Pietra. A maximum
entropy approach to natural language processing. Computa-
tional linguistics, 22(1):39–71, 1996.

[4] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A
matlab-like environment for machine learning. In BigLearn,
NIPS Workshop, number EPFL-CONF-192376, 2011.

[5] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradi-
ent methods for online learning and stochastic optimization.
The Journal of Machine Learning Research, 12:2121–2159,
2011.

[6] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vin-
cent, and S. Bengio. Why does unsupervised pre-training
help deep learning? The Journal of Machine Learning Re-
search, 11:625–660, 2010.

[7] J. Gauthier. Conditional generative adversarial nets for con-

volutional face generation.

[8] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in Neural Information
Processing Systems, pages 2672–2680, 2014.

[9] X. He, R. S. Zemel, and M. ´A. Carreira-Perpi˜n´an. Multiscale
conditional random ﬁelds for image labelling. In IEEE Con-
ference on Computer Vision and Pattern Recognition, vol-
ume 2, pages II–695. IEEE, 2004.

[10] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learn-
ing algorithm for deep belief nets. Neural Computation,
18(7):1527–1554, 2006.

[11] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller.
Labeled faces in the wild: A database for studying face
recognition in unconstrained environments. Technical Re-
port 07-49, University of Massachusetts, Amherst, October
2007.

[12] T. Jebara. Discriminative, generative and imitative learning.

PhD thesis, Massachusetts Institute of Technology, 2001.

[13] D. P. Kingma, S. Mohamed, D. Jimenez Rezende, and
M. Welling. Semi-supervised learning with deep genera-
tive models.
In Z. Ghahramani, M. Welling, C. Cortes,
N. Lawrence, and K. Weinberger, editors, Advances in Neu-
ral Information Processing Systems 27, pages 3581–3589.
Curran Associates, Inc., 2014.

[14] D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling.
In
Semi-supervised learning with deep generative models.
Advances in Neural Information Processing Systems, pages
3581–3589, 2014.

[15] D. P. Kingma and M. Welling. Auto-encoding variational
International Conference on Learning Representa-

Bayes.
tions, 2014.

[16] N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar.
In In-
Attribute and simile classiﬁers for face veriﬁcation.
ternational Conference on Computer Vision, pages 365–372.
IEEE, 2009.

[17] M. Mirza and S. Osindero. Conditional generative adversar-

ial nets. arXiv preprint arXiv:1411.1784, 2014.

[18] A. Y. Ng and M. I. Jordan. On discriminative vs. genera-
tive classiﬁers: A comparison of logistic regression and naive
bayes. In T. Dietterich, S. Becker, and Z. Ghahramani, edi-
tors, Advances in Neural Information Processing Systems 14,
pages 841–848. MIT Press, 2002.

4329

