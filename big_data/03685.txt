6
1
0
2

 
r
a

 

M
1
1
 
 
]
n
a
-
a
t
a
d

.
s
c
i
s
y
h
p
[
 
 

1
v
5
8
6
3
0

.

3
0
6
1
:
v
i
X
r
a

Determination of the edge of criticality in echo state networks

through Fisher information maximization

Lorenzo Livi∗†1,2,4, Filippo Maria Bianchi‡3, and Cesare Alippi§1,2

1Dept. of Electronics, Information, and Bioengineering, Politecnico di Milano, Piazza

2Faculty of Informatics, Universit`a della Svizzera Italiana, Via G. Buﬃ 13, 6904

Leonardo da Vinci 32, 20133 Milano, Italy

3Dept. of Physics and Technology, UiT the Arctic University of Norway, Tromsø,

Lugano, Switzerland

4Dept. of Computer Science, Ryerson University, 350 Victoria Street, Toronto, ON

Norway

M5B 2K3, Canada

March 14, 2016

Abstract

It is a widely accepted fact that the computational capability of recurrent neural networks is
maximized on the so-called “edge of criticality”. Once in this conﬁguration, the network performs
eﬃciently on a speciﬁc application both in terms of (i) low prediction error and (ii) high short-term
memory capacity. Since the behavior of recurrent networks is strongly inﬂuenced by the particular
input signal driving the dynamics, a universal, application-independent method for determining the
edge of criticality is still missing. In this paper, we propose a theoretically motivated method based
on Fisher information for determining the edge of criticality in recurrent neural networks. It is proven
that Fisher information is maximized for (ﬁnite-size) systems operating in such critical regions. How-
ever, Fisher information is notoriously diﬃcult to compute and either requires the probability density
function or the conditional dependence of the system states with respect to the model parameters.
The paper exploits a recently-developed non-parametric estimator of the Fisher information matrix
and provides a method to determine the critical region of echo state networks, a particular class of
recurrent networks. The considered control parameters, which indirectly aﬀect the echo state net-
work performance, are suitably controlled to identify a collection of network conﬁgurations lying on
the edge of criticality and, as such, maximizing Fisher information and computational performance.
Keywords— Edge of criticality; Echo state network; Fisher information; Non-parametric estimation.

1

Introduction

A Recurrent Neural Network (RNN) can approximate any dynamic system under mild hypotheses (see
[28] and references therein). However, RNNs are diﬃcult to train [41] and the interpretability of their
modus operandi is still object of study [6, 51]. RNNs can generate complex dynamics characterized
by sharp transitions between ordered and chaotic regimes. Experimental results on a multitude of
application contexts suggest that RNNs achieve the highest information processing capabilities exactly
on the edge of this transition, resulting in high memory capacity (storage of past events) and good
performance on the modeling/prediction task at hand (low prediction errors) [1, 5, 25, 43, 56, 60].
Therefore, in order to determine such “critical” network conﬁgurations, RNNs require ﬁne tuning of

∗llivi@scs.ryerson.ca
†Corresponding author
‡ﬁlippo.m.bianchi@uit.no
§cesare.alippi@polimi.it

1

their controlling parameters. This general behavior is in agreement with the widely-discussed “criticality
hypothesis” associated with the functioning of many biological (complex) systems [18, 21, 35, 45, 46, 53],
including the brain [12, 20, 32, 36, 37, 54, 55]. In fact, it was noted [35] that such complex systems tend
to self-organize and operate in a critical regime. This still controversial issue has been supported by
experiments showing that, in such a regime, systems are highly responsive to external stimuli and hence
are capable of introducing any dynamics as requested by the speciﬁc task [35]. Investigating weather a
given complex system operates more eﬃciently in the critical regime or not requires, at ﬁrst, theoretically
sound methods for detecting the onset of criticality [47].

Determination of system conﬁgurations characterizing the edge of criticality can be then carried out
by means of appropriate sensitivity analyses. In this direction, Fisher information, and its multivariate
extension called Fisher information matrix (FIM) [3, 57, 58, 63], provide a way to quantify the sensi-
tivity of a (parametrized) probability density function with respect to its control parameters. Fisher
information is tightly linked with statistical mechanics and, in particular, with the ﬁeld of (continuous)
phase transitions. In fact, as shown in [42], it is possible to provide a thermodynamic interpretation of
Fisher information in terms of rate of change of the order parameter, quantities used to discriminate the
diﬀerent phases of a system. This fact provides an important bridge between the concept of criticality
and statistical modeling of complex systems. It emerges that the critical phase of a thermodynamic sys-
tem can be mathematically described as that region of the parameter space where the order parameters
vanish and their derivatives diverge. This implies that, on the critical region, Fisher information diverges
as well, hence providing a quantitative, well-justiﬁed tool for detecting the onset of criticality in both
theoretical models and computational simulations [61]. Nonetheless, Fisher information is notoriously
diﬃcult to compute and, in principle, it requires the analytical knowledge of the parametrized probability
density function describing the system behavior.

In this paper, we study a special class of RNNs called Echo State Networks (ESNs) [27]. Although
ESNs are typically initialized randomly, the network designer has access to a set of hyper-parameters,
which have an indirect eﬀect (when considering inputs) on the resulting ESN dynamics and their related
computational capability. We deﬁne the hyper-parameter conﬁgurations that bring an ESN in a state
where the computational capability is maximized as the critical region (or equivalently, edge of criticality)
of the ESN. Two hyper-parameters play a particularly important role in controlling the ESN behavior:
the spectral radius of the reservoir weight matrix and the scaling coeﬃcient of the input weights. Here
we demonstrate that the FIM can be used to determine the onset of criticality for a network designed
to solve a particular application. Notably, we provide an algorithm that exploits the determinant of the
FIM in order to determine the edge of criticality. In the proposed algorithm, we use a non-parametric
FIM estimator [4] that allows us to overcome some of the diﬃculties that arise when adopting a model-
based approach for computing the FIM (e.g., availability of the analytical model ruling the system).
Additionally, in order to robustly estimate the FIM, as recommended in [15] we follow an ensemble
approach and perform a number of independent trials.

RNNs, as well as ESNs, are driven by inputs. Therefore, their dynamics and related computational
capability depend on the type of input signal driving the network. During the last decade, many solutions
have been proposed to characterize the input-driven dynamics of the network and perform related tuning
of the (hyper-)parameters [38]. Among the many contributions, we can cite approaches based on mean-
ﬁeld approximation of the neuron activations [31], information-theoretic methods inspired by the concept
of intrinsic plasticity (based on the maximum entropy principle) borrowed from neuroscience [40, 48],
and methods for characterizing the onset of criticality with measures of (directional) information transfer
and information storage [10], together with related self-organized adaptation mechanisms [39].

To the best of our knowledge, FIM and related thermodynamic interpretations have not been inves-
tigated yet for studying criticality in ESNs. We stress that, in principle, our method can be extended to
account for additional (hyper-)parameters, such as teacher signal scaling, feedback scaling, percentage
of noise in state update, sparsity of the reservoir, etc [24]. Finally, it is worth noticing that, as a conse-
quence of the theoretical framework adopted here, we implicitly assume that the critical phase of ESNs
can be described by a continuous phase transition. This assumption is highly justiﬁable, since a system
can operate in a critical regime only if such a transition is continuous.

The novelty of our contribution can be summarized as:
• A method that, by exploiting the information coming from the neuron activations only, permits
to identify the edge of crticality. Since no assumption regarding the mathematical model of the

2

(input-driven) dynamic system is made, the method can handle any type of applications;

• The proposed method is independent of the particular reservoir topology, since it is conceived to
determine the critical ESN (hyper-)parameters. This allows the network designer to instantiate a
speciﬁc architecture based on problem-dependent design choices;

• The envisaged non-parametric FIM estimator [4] operates directly on data/observations: as such,
there is no need to estimate the high-dimensional densities underlying the neuron activations.
As a consequence, the number of reservoir neurons does not pose a serious technical issue from
the estimation viewpoint and therefore it can be chosen by the network designer according to
application requirements;

• The FIM estimator can be implemented in two diﬀerent ways, one of which requires a speciﬁc
formulation of the related optimization problem. In this paper, we propose our own formulation
for the constraints deﬁning such an optimization problem – see Appendix A for details.

The remainder of this paper is structured as follows. In Section 2 we introduce ESNs and the related
considerations on the characterization of the dynamics. Section 3 introduces Fisher information matrix
and the adopted non-parametric estimator. In Section 4, we present the method that we propose for
determining the ESN hyper-parameters. In order to support our methodological developments, Section
5 presents experimental results performed on both well-known benchmarks and a real-world application
involving the prediction of telephone call loads [8]. Conclusions and future directions follow in Section 6.

2 Echo state networks

ESNs [27] consist of a large recurrent layer of non-linear units with randomly generated weights and a
linear, memory-less read-out layer that is trainable by means of a simple regularized least-square opti-
mization. The recurrent layer acts as a non-linear kernel [19], mapping the input to a high-dimensional
space. The equations describing the ESN state-update and output are, respectively, deﬁned as

h[k] =φ(Wr
y[k] =Wo

rh[k − 1] + Wr
rh[k].

i x[k] + Wo

i x[k] + Wr

oy[k − 1]),

(1)

(2)
The reservoir contains Nr neurons characterized by a transfer/activation function φ(·), which is typically
implemented as a hyperbolic tangent (tanh) function. At time instant k, the network is driven by the
input signal x[k] ∈ RNi and produces the output y[k] ∈ RNo, being Ni and No the dimensionality of input
and output, respectively. The vector h[k] contains Nr components and describes the ESN (instantaneous)
i ∈ RNi×Nr (input-to-reservoir),
state. The weight matrices Wr
o ∈ RNo×Nr (output-to-reservoir feedback) contain real values in the [−1, 1] interval distributed
and Wr
according to a uniform distribution; additional options have been explored in the recent literature [2, 44].
Wo
r, instead, are optimized for the task at hand. A visual representation of the ESN architecture
is reported in Fig. 1

r ∈ RNr×Nr (reservoir connections), Wr

i and Wo

r, Wr

o, and Wr

A network designer can control Wr

i only by using suitable scaling coeﬃcients. For
instance, Wo
In this study, we remove the
output feedback connection by setting ωo = 0. The resulting ESN state-update (1) is hence modiﬁed as
follows:

r is typically scaled through a multiplicative constant ωo.

h[k] = φ(Wr

rh[k − 1] + Wr
while the output (2) remains unchanged. The input weights, Wr
i , are controlled by scalar parameter ωi
that determines the amount of non-linearity introduced by the neurons due to saturating eﬀects. The
spectral radius of Wr
r, denoted as ρ in the sequel, is known to inﬂuence both stability and computational
capability of the network. Both ρ and ωi act as ESN hyper-parameters and are typically tuned to
ﬁnd the best-performing conﬁguration for the task at hand. In this paper, we will focus on these two
hyper-parameters to control the network performances.

i x[k]),

(3)

In order to guarantee asymptotic stability, ESNs must satisfy the so-called echo state property [11, 30,
62], which requires the reservoir exhibiting short-term memory (exponential fading) [14, 52]. Recently,
in [34] the author investigated the eﬀects of criticality in ESN memory, showing that, under certain

3

Figure 1: Schematic depiction of an ESN. The circles represent input x, state, h, and output, y, respec-
tively. Solid squares Wo
i , are the trainable matrices, respectively, of the readout, while dashed
squares, Wr
i , are randomly initialized matrices. The polygon represents the non-linear
transformation performed by neurons and z-1 is the lag operator.

o, and Wr

r and Wo

r, Wr

conditions, the echo state property can still be veriﬁed even if the memory vanishes more slowly (i.e.,
following a power-law function).

The degree of network stability can be assessed in practice by analyzing the Jacobian matrix of
the reservoir state update (3). Notably, the maximal local Lyapunov exponent (MLLE) λ, used to
approximate the separation rate in phase space of trajectories having very similar initial states [59], is
typically computed from such a matrix. In autonomous systems, λ < 0 indicates that the system (here
an ESN) is stable; λ > 0 that is chaotic. A transition point between those two diﬀerent behaviors is
obtained when λ = 0. The sign of λ provides thus a criterion for detecting the onset of criticality in
reservoirs. Such a criterion is widely used also as a baseline to develop and compare novel criteria [10].
If reservoir neurons are implemented with a tanh activation function, the Jacobian at time k can be

conveniently expressed as

J(h[k]) =

 1 − (h1[k])2

0
...
0

 Wr

r ,

. . .
. . .
. . .
. . .

0
0
...

1 − (hNr [k])2

(4)

1 − (h2[k])2

0

...
0

where hl[k], l = 1, 2, ..., Nr, is the activation of the l-th reservoir neuron at time k. λ is then computed
by means of the geometric average:

λ = max

n=1,...,Nr

1
K

K(cid:88)

k=1

log (rn[k]) ,

(5)

where rn[k] is the module of n-th eigenvalue of J(h[k]) and K is the total number of samples in the
time-series under consideration.

In this paper, we will use the MLLE criterion for detecting the onset of criticality as a means of

numerical validation for the proposed method based on FIM.

3 Fisher information matrix and the non-parametric estimator

The Fisher information matrix [63] is a symmetric positive semi-deﬁnite (PD) matrix whose elements
are deﬁned as follows:

(cid:90)

(cid:18) ∂ ln pθ(u)

(cid:19)(cid:18) ∂ ln pθ(u)

(cid:19)

Fij(pθ(·)) =

(6)
where pθ(·) is a parametric probability density function (PDF), which depends on d parameters θ =
[θ1, θ2, ..., θd]T ∈ Θ ⊂ Rd. In (6), ln pθ(·) is the log-likelihood function and D ⊆ RD denotes the support

∂θj

∂θi

D

pθ(u)

du,

4

WorWoiWroWrrWriyhxz-1z-1of the PDF. For sake of simplicity, we denote F(pθ(·)) as F(θ). The FIM contains d(d + 1)/2 distinct
entries encoding the sensitivity of the PDF with respect to the parameters θ.

Elements of the FIM can be directly linked with the rate of change of the order parameters of a con-
trolled (thermodynamic) system [42]. An order parameter is a quantity that is used to discern the phases
of a controlled thermodynamic system. For instance, in the liquid–vapor (ﬁrst-order) transition of water,
temperature acts as a control parameter (at constant pressure), while the diﬀerence in density of the two
phases – liquid and gaseous states – is the order parameter. At the critical temperature, liquid water
turns into vapor and the order parameter varies discontinuously. The mathematical relationship between
Fisher information and order parameters is particularly interesting to provide a statistical description of
continuous, second-order phase transitions, and, as a consequence, of any complex system approaching
a critical transition.
In fact, during a continuous phase transition the order parameter changes con-
tinuously. Therefore, diﬀerently from ﬁrst-order transitions, a system can reside and operate in such a
critical state. A well-known example of continuous phase transition is the ferromagnetic–paramagnetic
transition of materials such as iron, where the magnetization (the order parameter) is non-zero for tem-
peratures lower than the critical (Curie) one and zero otherwise. However, second-order derivatives of
the observed thermodynamic variable (or, equivalently for continuous transitions, the ﬁrst-order deriva-
tives of the order parameter) are discontinuous and divergent in at least one dimension. This implies
that Fisher information diverges at criticality for inﬁnite systems, while it is maximized in the ﬁnite-size
system case [42]. This fact provides a clear mathematical justiﬁcation for explaining why the FIM (6)
can be used to detect criticality in complex systems in terms of maximum sensitivity with respect to
control parameter changes. Therefore, as we already mentioned, the critical region (edge of criticality)
is a region in parameter space where the Fisher information is maximized; hence we assume here to deal
with ﬁnite-size systems.

Calculation of the FIM (6) requires full analytical knowledge of the PDF. However, in many experi-
mental settings either (i) the PDF underlying the observed data is unknown or (ii) the relation linking
the variation of the control parameters θ and the resulting pθ(·) depends on an unknown function. In a
recent paper [4], a non-parametric estimator of the FIM based on divergence measure

was proposed, with α ∈ (0, 1); p(·) and q(·) are PDFs both supported on D. It is shown that (7) belongs
to the family of f -divergences, and, most importantly, that it can be computed directly without the need
to estimate the PDFs by means of an extension of the Friedman-Rafsky multi-variate two-sample test
statistic [13]. The test operates by using two datasets, Sp and Sq, each one containing samples extracted
from p(·) and q(·), respectively. Theorem 1 in [4] shows that, as the number of samples n = |Sp| and
m = |Sq| grows, we have:

1 − C(Sp,Sq)

n + m
2nm

a.s.−−→ Dα(p, q),

(8)
where C(Sp,Sq) is the outcome (expected to be normally distributed) of the Friedman-Rafsky test, which
basically provides a way to measure the similarity between two samples. Interestingly, such a test allows
to analyze also high-dimensional data, since it makes use of a graph-based representation of the samples;
a minimum spanning tree.

It is well-known [4, 21, 22] that the FIM can be approximated by using a proper f -divergence measure
computed between the parametric PDF of interest and a perturbed version of it. Notably, by expanding
Eq. 7 up to the second order we obtain:

(9)
where ˆθ = θ + r, being r ∼ N (0, σ2Id×d) a small normally distributed perturbation vector with standard
deviation σ.

2

Dα(pθ, p ˆθ) (cid:39) 1

rT F(θ)r,

In the following, for the sake of brevity we omit θ in most of the equations and refer to the estimated
FIM as ˆF. [4] proposes two diﬀerent approaches for estimating the FIM in Eq. 9. The ﬁrst one is based
on the well-known least-square optimization:

ˆFhvec = (RT R)

−1RT vθ,

(10)

5

Dα(p, q) =

1

4α(1 − α)

(cid:90)

D

(αp(u)(1 − α)q(u))2
αp(u)(1 − α)q(u)

du − (2α − 1)2,

(7)

), i = 1, ..., M , and Dα(·,·) is computed by
where vθ = [vθ(r1), ..., vθ(rM )]T , with vθ(ri) = 2Dα(pθ, p ˆθi
means of the left-hand side of Eq. 8. R is a matrix containing all M perturbation vectors ri arranged
as column vectors, and ˆFhvec is the half-vector representation of ˆF. Note that a vector representation
ˆFvec of ˆF reads as [f11, . . . , fm1, f12, . . . , fmn]T . Since ˆF is symmetric, it can be represented through
the half-vector representation, ˆFhvec, which is obtained by eliminating all superdiagonal elements of ˆF
from ˆFvec [29]. ˆFhvec in Eq. 10 is hence deﬁned as
, where the diagonal
elements are located in the ﬁrst components of the vector.

(cid:104) ˆf11, . . . , ˆfdd, ˆf12, . . . , ˆfd(d−1)

(cid:105)T

However, the least-square approach (10) does not guarantee to ﬁnd and approximation of the FIM
which is PD. A second approach consists in solving a semi-deﬁnite optimization problem, which instead
assures that the resulting FIM is PD:

(cid:107)RFhvec − vθ(cid:107)2

(cid:16)
mat
mat (Fhvec) (cid:23) 0d×d.

(cid:16)

(cid:17)(cid:17)

minimize

Fhvec

subject to Fhvec(i) = diag

ˆFhvec

, i ∈ {1, . . . , d},

(11)

The operator diag(·) returns the diagonal elements of a matrix and the mat(·) operator converts the
argument from a vector form into a square d × d matrix. The diagonal values of the FIM as expressed
by the ﬁrst constraint are computed through the LS optimization (10). The second constraint, instead,
guarantees the estimated matrix to be PD, i.e. all eigenvalues must be non-negative.

Such a convex optimization problem (11) can be implemented by using the framework provided in
[16, 17]. However, there, a non-trivial implementation in matrix form of the second constraint, i.e.,
mat (Fhvec) (cid:23) 0d×d, must be provided to deﬁne a proper semideﬁnite program. In this paper, we ﬁll
this gap and provide the details of the proposed formulation for (11) implementing mat (Fhvec) (cid:23) 0d×d
in matrix form (in Appendix A).

4 Critical region identiﬁcation for ESNs

Our goal is to ﬁnd the edge of criticality, i.e., parameter conﬁgurations in Θ that maximize the ESN
computational capability. To this end, we devised a method that takes into account the particular input
driving the network. We search in the space of two-dimensional vectors deﬁned as θ = [ρ, ωi]T ∈ Θ ⊂ R2.
That is, we consider the spectral radius ρ of the reservoir weight matrix and the input scaling ωi as
hyper-parameters controlling the behavior of the resulting ESN. Although it is possible to consider more
hyper-parameters, we decided to focus on these two, since they are known to be fundamental for the
In fact, ρ inﬂuences both stability and computational capability of the
resulting ESN dynamics [6].
network; ωi is linked to the degree of non-linearity introduced by the neurons due to saturating eﬀects,
which in turn inﬂuences the stability of the network. In addition, dealing with two parameters allows
for a better visualization and hence it provides interpretation of obtained results.

Fig. 2 shows a schematic description of the main stages involved in the procedure. Given an input
signal and an initial ESN hyper-parameter conﬁguration, we estimate the FIM from the neuron activa-
tions. We then exploit a geometric property (the determinant) of FIMs of critical systems in order to
determine the edge of criticality.

Figure 2: Schematic, high-level description of the proposed procedure.

Let us detail the proposed procedure. We deﬁne the edge of criticality of ESNs as a region K ⊂ Θ.
In order to determine K, here we introduce an algorithm exploiting the FIM properties of a system

6

undergoing a continuous phase transition. FIM deﬁnes a metric tensor for the smooth manifold of
parametric PDFs embedded in Θ [42], allowing thus also for a geometric characterization of the system
under analysis. It is possible to prove [33] that K corresponds to a region in Θ characterized by the largest
volume (high concentration of parametric PDFs). This geometric result can be exploited by using the
determinant det(F(θ)), which is monotonically related to the aforementioned volume in parameter space.
Therefore, considering that the FIM is a PD matrix, and hence its determinant is always non-negative,
we identify K with all those hyper-parameters θ∗ for which:

θ∗

= arg max

θ∈Θ

det(F(θ)).

(12)

Algorithm 1 delivers the pseudo-code of the proposed procedure. As said before, the impact provided
by the variation of the control parameters θ on the resulting ESN state cannot be described analytically
without making further assumptions [31]: the (unknown) input signal driving the network plays an
important role in the resulting ESN dynamics. Therefore, in order to calculate F(θ), in Algorithm 1 we
rely on the non-parametric FIM estimator described in Sec. 3. The estimation of the FIM for a given θ is
performed by analyzing the sequence Sθ = {h[k]}K
k=1 of reservoir neuron activations produced during the
processing of a given input x of size K. Since h[k] ∈ [−1, 1]Nr , the domain of the PDF in (6) is deﬁned as
D = [−1, 1]Nr . Additional sequences of activations, S ˆθj
, are considered (see line 7), which are obtained by
perturbing M times the current network conﬁguration θ under analysis, and processing the same input
x. Perturbations are introduced by means of a small zero-mean noise with spherical covariance matrix,
thus characterized by a single scalar parameter σ controlling the magnitude of the perturbation. In this
paper, we estimate the FIM by solving the optimization problem (11) according to our formulation as
described in Appendix A. In order to make the estimation more robust, as recommended in [15] we follow
an ensemble approach and perform a number of trials (see line 3). The determinant is computed only
once on the resulting average FIM, which is obtained by using T diﬀerent (and independent) random
realizations of the ESN architecture chosen for the experiment (see line 16).

In theory, the parameter space Θ is continuous. However, here we assume that the parameter space
Θ is quantized according to some user-deﬁned resolution. Although this is not a necessary assumption
for the proposed methodology, it allows us to disentangle the problems of deﬁning and ﬁnding the edge
of criticality.
In fact, our main goal here is to provide a principled deﬁnition of the critical region
characterizing the ESN (hyper-)parameter space and related behaviors. More eﬃcient and/or accurate
search schemes will be considered in future research studies. Accordingly, the criterion in (12) identiﬁes
a “quantized” critical region K in Θ represented by a single hyper-parameter conﬁguration, θ∗.

5 Experiments

We have seen that the computational capability (low prediction error and high short-term memory
capacity) of recurrent neural networks is maximized on the edge of criticality. Here we adopt the
proposed method based on FIM for determining ESN hyper-parameter conﬁgurations lying on the edge
of criticality. In order to provide a numerical comparison, we also consider the criterion based on the
sign of λ (5).

The proposed method is ﬁrstly validated on a set of benchmarks used in the ESN literature.

In
particular, we evaluate the short-term memory capacity (Sec. 5.1) and then consider a forecast task on
diﬀerent time-series models (Sec. 5.2).

Successively, in Section 5.3 we validate the proposed methodology on a real-world application involv-

ing the prediction of time-series related to phone calls load [8].

All experiments are performed by discretizing the hyper-parameter domain, Θ, with a 5x5 grid. Hence
we select 5 conﬁgurations for ρ and ωi, for a total of 25 conﬁgurations. This resolution has been chosen
in order to ﬁnd a compromise for the required computing time. Since the considered parameter space is
two-dimensional, the edges identiﬁed by the two methods are shown as lines. Accordingly, we show as a
line also the region where the monitored performance measure of the network is maximized. Therefore,
an edge is consistent with the network performance if the locations of the respective lines in parameter
space overlap. In the following, in order to improve visualization of results, lines are slightly displaced
on the vertical axis to avoid complete overlap.
We do not consider the output feedback (ωo = 0); weights are uniformly distributed in the [−1, 1]
r of 25%. The readout layer is trained

r; percentage of non-zero connections in Wr

interval for Wr

i and Wr

7

for t = 1 to T do

tions, number of trials T and perturbations M .

Algorithm 1 Procedure for determining an ESN conﬁguration on the edge of criticality.
Input: An ESN architecture, input x of K samples, quantized parameter space Θ, standard deviation σ for the perturba-
Output: A conﬁguration θ∗ ∈ K
1: Select an initial parameter conﬁguration, θ ∈ Θ; maximum η = 0
2: loop
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

Generate a perturbation vector rj ∼ N (0, σ2Id×d)
Randomly initialize the ESN weight matrices
Conﬁgure ESN with perturbed version ˆθj = θ + rj and process input x
= {h[i]}K
Collect the related activations S ˆθj

Randomly initialize the ESN weight matrices
Conﬁgure ESN with θ and process input x
Collect the related activations Sθ = {h[i]}K
for j = 1 to M do

Deﬁne S ˆθ = ∪M
Estimate the FIM F(t)(θ) of trial t using Sθ and S ˆθ with the non-parametric estimator introduced in Sec. 3

j=1S ˆθj

end for

i=1

i=1

end for
Compute the average FIM, F(θ), using all F(t)(θ), t = 1, ..., T
if det(F(θ)) > η then

Update η = det(F(θ)) and θ∗ = θ

end if
if Stop criterion is met then

return θ∗
Select a new θ ∈ Θ based on a suitable search scheme

14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
end if
25: end loop

else

by using a least-square (LS) regression and setting the regularization parameter to 0.1. In all tests on
synthetic data, we used a reservoir with Nr = 75 neurons; a standard drop-out procedure is adopted
[24], discarding the ﬁrst 100 states in order to get rid of the ESN transient.

Finally, in Algorithm 1 we always use M = 60 perturbations and T = 10 trials to compute the

ensemble average of the FIM estimate.

5.1 Memory capacity

This test, originally proposed in [23], is conceived to quantify the ability of ESNs to remember the past
by correlating past events in a i.i.d. input with the computed network output. It was proved that, for
any RNN with Nr neurons in the recurrent layer, MC is bounded by Nr if the network is driven by an
input. Given a time delay δ > 0, here we train an ESN to reproduce the input x[t − δ], after
i.i.d.
having seen the input up to time t. The short-term memory capacity is then measured as the squared
correlation coeﬃcient between the desired output, i.e., the input signal delayed by δ time steps, and the
observed network output y[t]:

δmax(cid:88)

δ=1

MC =

cov2 (u[t − δ], y[t])
var (u[t − δ]) var (y[t])

.

=

(13)

MC is computed by training several readout layers by considering delays δ ∈ {1, ..., δmax} and keeping
ﬁxed the same input and reservoir layers. δmax in our case is 100.

In Fig. 3 we show the results for the MC test: for each hyper-parameter conﬁguration, the computed
MC values, along with determinant of the FIM and λ are given. It is possible to recognize that the
determinant of the FIM is always consistently maximized along with MC. On the other hand, the edge
of criticality identiﬁed by using the sign of MLLE is signiﬁcantly diﬀerent from the information provided
by MC. In fact, they agree only when ωi ∈ [0.35, 0.5]. From this ﬁrst test, we conclude that the criterion
based on maximization of the FIM is useful to individuate ESN hyper-parameter conﬁgurations that
enhance the short-term memory capacity of the network.

8

Figure 3: Memory capacity, determinant of FIM, and MLLE λ for diﬀerent values of ρ and ωi.

5.2 Prediction accuracy

Here we evaluate the prediction accuracy achieved by an ESN on three standard prediction tasks of
increasing complexity: a sinusoidal input, the Mackey-Glass (MG) system, and the Non-Linear Auto-
Regressive Moving Average (NARMA) signal.

For each prediction problem, we set the forecast step τf > 0 by evaluating the independence of τf -
separated points in the time-series. In fact, one usually looks for the smallest τf that guarantees the
measurements in the interval to be uncorrelated. Hence, here we considered for τf the ﬁrst zero of the
autocorrelation function of the time-series.

The error measure that we adopt is the Normalized Root Mean Squared Error (NRMSE),

(cid:115) (cid:104)(cid:107)y[n] − d[n](cid:107)2(cid:105)

(cid:104)(cid:107)y[n] − (cid:104)d[n](cid:105)(cid:107)2(cid:105) ,

NRMSE =

(14)

where y[n] is the ESN prediction and d[n] the desired/teacher output. A measure of prediction accuracy,
denoted as γ, can be deﬁned by taking γ = max{1 − NRMSE, 0}.

Sinusoidal input For this test, the ESN is trained to perform a 25-step ahead prediction and is tested
on 1500 data points. The resulting prediction accuracy γ, determinant of the FIM, and MLLE are shown
in Fig. 4(a). Analogously to the MC test, here we expect to ﬁnd the edge of criticality in the same
region where the prediction accuracy γ is maximum. As it is possible to recognize in Fig. 4(a), in this
case MLLE criterion overestimates the location of the edge of criticality, while the determinant of the
FIM always suggests hyper-parameters conﬁgurations that are consistent with respect to γ.

NARMA The NARMA task, originally proposed in [24], consists in modeling the output of the fol-
lowing order-r system:

y[t + 1] =

0.3y[t] + 0.05y[t]

(cid:32)r−1(cid:88)

(cid:33)

+ 1.5x[t − r]x[t] + 0.1.

(15)

y[t − i]

i=0

The input x[t] is a uniform random noise distributed in [0, 1]. The NARMA task is known to require

a reservoir with memory of at least r past time-steps.

The results obtained on the NARMA signal are represented in Fig. 4(b). As it is possible to notice,
also in this case the edge of criticality suggested by MLLE is signiﬁcantly over-estimated. The edge
suggested by the FIM determinant, instead, is consistent with the accuracy, with an exception for the
case with ωi = 0.65, where the location of the edge is over-estimated also by the FIM based criterion.

9

ωi0.2 0.350.5 0.650.8 ρ1.61.31  0.70.4MCFIMλMG time-series The last input signal is given by a time-series generated from the Mackey-Glass (MG)
system, which is described by the following diﬀerential equation:

dx
dt

=

αx(t − τMG)
1 + x(t − τMG)10

− βx(t).

(16)

We generated a time-series of 150000 time-steps using τMG = 17, α = 0.2, β = 0.1, initial condition
x(0) = 1.2, and 0.1 as integration step for (16). For this prediction task, we set a forecast step τf = 84.
The MG time-series has been widely considered for benchmarking prediction system, since in fact the
underlying dynamic system is only mildly chaotic, hence allowing for some short-term predictions. ESNs
demonstrated to be particularly eﬀective in forecasting the MG and other chaotic time-series [26, 50].

Fig. 4(c) shows the results in the usual format, highlighting once again the correctness of the infor-

mation provided by FIM based criterion.

5.3 Prediction of telephone call load time-series

In this section, we analyze time-series of data related to nationwide mobile telephone loads. Such time-
series have been generated from the data collected in the Orange telephone dataset, published in the
Data for Development (D4D) challenge [9]. D4D is an open collection of call data records, containing
anonymized events of Orange’s mobile phone users in Ivory Coast, Africa. More detailed information on
the challenge is available on the related website1. The data considered here span from December 1, 2011
to April 28, 2012. It includes mobile phone calls and SMS that are arranged in four diﬀerent datasets.
In particular, the four datasets contain the following information: (i) antenna-to-antenna traﬃc on an
hourly basis, (ii) individual trajectories for 50000 customers for two week time windows with antenna
location information, (iii) individual trajectories for 50000 customers over the entire observation period
with sub-prefecture location information, and ﬁnally (iv) a sample representing a communication network
connecting 5000 customers. Here we analyze the ﬁrst dataset, i.e. the antenna-to-antenna traﬃc.
Each record in the dataset has the following structure: (cid:104)DateTime, IDa, IDb, NumCalls, TotTime(cid:105).
DateTime is the time (with hourly resolution) and date when an activity between the two antennas a
and b has been registered; IDa and IDb are the identiﬁers of the transmitting and receiving antenna,
respectively; NumCalls is the number of calls started from a and received by b in the time interval under
consideration; ﬁnally TotTime is the sum of the durations (in seconds) of all calls recorded in the interval.
We selected a speciﬁc antenna and we retrieved from the dataset all the records relative to the activity
involving that antenna. We have accordingly generated the following 7 distinct time-series:

– ts1: constant input (a time-series with all values set to 1). This is a standard practice in prediction
with neural networks, since a constant input acts as a bias for the individual neurons of the network
[24];

– ts2: number of incoming calls in the area covered by the antenna;

– ts3: volume in minutes of the incoming calls in the area covered by the antenna;

– ts4: number of outgoing calls in the area covered by the antenna;

– ts5: volume in minutes of the outgoing calls in the area covered by the antenna;

– ts6: hour of the day when the telephone activity was registered;

– ts7: day of the week when the telephone activity was registered.

All such 7 time-series are fed as input to an ESN; we predict the values relative only to ts2.
The dataset contains a small percentage of missing values and corrupted data. Missing values are
present each time there is no outgoing and/or incoming telephone activities for a speciﬁc antenna at a
given hour. This means that, for example, when we generate the ts4 time-series relative to the outgoing
calls for that antenna, we do not have any value in correspondence of that hour, which causes ts4 to
be shorter than the others. Since our speciﬁc experimental setting requires all time-series to have the
same length, we insert 0s in correspondence of missing values. Corrupted data are marked with a “-1”

1http://www.d4d.orange.com

10

(a) Sinusoidal input.

(b) NARMA.

(c) MG.

Figure 4: Prediction accuracy γ, determinant of FIM, and MLLE λ for diﬀerent values of ρ and ωi.

in the dataset, which represent periods where the telephone activity was not correctly registered by the
antenna. To address this issue, we used the technique described in [49]; the missing values were replaced
with the average value of the corresponding periods (i.e., same weekday and hour of the day) in two
adjacent weeks.

All data have been standardized by a z-score transformation prior to processing. This is successively
reversed when the forecast values must be provided and compared [7]. In Fig. 5 we show the proﬁle of
ts2 relatively to the load in the ﬁrst 300 time intervals (corresponding to 1 hour of activity).

11

ωi0.2 0.350.5 0.650.8 ρ1.61.31  0.70.4γFIMλωi0.2 0.350.5 0.650.8 ρ1.61.31  0.70.4γFIMλωi0.2 0.350.5 0.650.8 ρ1.61.31  0.70.4γFIMλFigure 5: The load proﬁle of ts2 for the ﬁrst 300 time intervals.

These time-series have been previously studied in [8], where ESNs and other standard methods
(ARIMA and Triple Exponential Smoothing) were adopted to perform both 1-step and 24-steps ahead
predictions. Diﬀerent optimization procedures were evaluated for training the readout: least-square (LS)
regression, elastic net penalty, linear and non-linear SVR. The hyper-parameters of both ESN and the
related learning methods were tuned using a genetic algorithm optimization scheme. In particular, the
following hyper-parameters were optimized for this speciﬁc prediction problem: the size of the reservoir
(Nr), the spectral radius (ρ), the sparsity of the connections in the reservoir, the scaling factors for
the input, output feedback and teacher signal (ωi, ωo, and ωt), the regularization parameter in the LS
regression (λls), and ﬁnally the hyper-parameters in elastic net and SVR training methods. It was shown
that ESN achieved, in general, higher prediction accuracy with respect to the other forecast methods.
For what concerns the training methods for the reservoir, slightly better results have been observed by
using non-linear SVR at the expense of a much higher computational cost; linear SVR and elastic net
penalty obtained similar or worse results than LS regression.
It is worth mentioning that, although
several hyper-parameters were tuned through the genetic algorithm, the solution is still black-box and
does not follow a mathematically motivated criterion to determine a critical region in parameter space.

Figure 6: Orange D4D time-series. Accuracy γ, determinant of FIM, and MLLE λ for diﬀerent values
of ρ and ωi.

Once again, here we focus our analysis on the two hyper-parameters considered here, ρ and ωi, while
In particular, we set
for the remaining ones we adopt the optimal conﬁguration as suggested in [8].
Nr = 680, reservoir sparsity equal to 25%, ωt = 0.1, λls = 0.04, and ωo = 0. The forecast step τf is
set to 1, meaning that the telephonic load of the next hour is predicted. Note that this diﬀers from the
other (synthetic) cases considered in the previous sections, where τf was set equal to the ﬁrst zero in the
autocorrelation function.

In Fig. 6, we report the accuracy γ (prediction accuracy on ts2), determinant of FIM, and MLLE λ.
Interestingly, by using the FIM-based criterion we ﬁnd a critical region of the (2-dimensional) ESN hyper-
parameter space containing the optimal values for ρ and ωi as reported in [8]. In fact, the quantized
area centered in [ρ = 1, ωi = 0.35]T that, according to the FIM-based criterion, belongs to the ESN
critical region, contains also the values ρ = 0.98 and ωi = 0.33, which were identiﬁed as optimal in [8]

12

Hour050100150200250300Callvolume0100200300400500ωi0.2 0.350.5 0.650.8 ρ1.61.31  0.70.4γFIMλwith a genetic algorithm. It is worth pointing out that, with the proposed methodology, we are able to
identify additional critical hyper-parameter conﬁgurations. As it is possible to note, for ωi ∈ [0.2, 0.5),
the conﬁgurations suggested by the MLLE criterion over-estimate the location of the edge.
In the
other cases, both criteria agree on the location of the edge. The maximum values of FIM determinant
suggests conﬁgurations which corresponds to optimal ESN prediction accuracy, with the only exception
for ωi = 0.65, where the suggested conﬁguration is the second-best.

6 Conclusions and future directions

Echo state networks, as a class of networks in reservoir computing, oﬀer a compromise between training
time and network performance in terms of prediction error and short-term memory capacity. Experiments
showed that such networks operate more eﬃciently when conﬁgured on the so-called edge of criticality, a
region in hyper-parameter space separating ordered and chaotic regimes. Hyper-parameters (indirectly)
aﬀecting the behavior of the network are hence tuned according to some criterion. In this paper, we
have proposed a principled approach for conﬁguring an echo state network on the edge of criticality.
The proposed method is based on the interplay between the theory of continuous phase transitions
and Fisher information.
In fact, it is possible to prove that, in the limit of inﬁnite systems, Fisher
information diverges on the critical region and hence can be used to determine the onset of criticality.
Nonetheless, Fisher information presumes analytic knowledge of the parametric distribution describing
the system/network; in addition its computation is known to be diﬃcult and prone to numerical errors.
In order to deal with these issues, here we have followed an ensemble estimation approach based on a
recently proposed non-parametric Fisher information matrix estimator. Such an estimator is applicable
to high-dimensional densities, since it is operates by means of a graph-based representation of the data.
This last aspect is very important in our case, since we analyze the network though a multivariate
sequence of neuron activations coming from the reservoir.

We have evaluated the proposed method on well-known benchmarks as well as on a real-world appli-
cation involving telephone call load prediction. The considered benchmarks were conceived to evaluate
both the short-term memory capacity (in terms of the squared correlation between past inputs and net-
work outputs) and the prediction accuracy (in terms of normalized root mean square error). In order to
compare our method with a well-established reference, we have taken into account also a criterion based
on the sign of the maximum local Lyapunov exponent computed on the activations. Results showed
that the proposed method based on Fisher information is more accurate than the criterion based on
maximum Lyapunov exponent (on both the benchmarks and the real-world application) in determining
critical ESN hyper-parameter conﬁgurations.

The methodology proposed here oﬀers a sound and appealing solution to determine the onset of
criticality in echo state networks. Nonetheless, our contribution comes with some technical diﬃculties
that we have only partially solved so far. First of all, potential non-stationarities and (short-term)
dependencies of the neuron activations distribution might aﬀect the estimation outcomes. Here we
have addressed this issue by following an ensemble approach to estimate the Fisher information matrix.
However, other approaches might be considered in the future, for instance by using a window-based
approach to assure stationarity on a local basis. Second, the non-parametric Fisher information matrix
estimator that we have used requires to set a parameter controlling the magnitude of the perturbations.
This parameter turned out to be very sensitive and diﬃcult to determine in practice, hence posing some
technical limitations when trying to automatize the procedure. Such issues will be object of future
research eﬀorts.

There are many possible routes that we intend to follow in the future. Among the many, we believe
it is worth focusing on (i) how to enable the output feedback and (ii) the application of the proposed
method as a unsupervised learning method for recurrent neural networks.

Acknowledgements

We wish to thank Prof. Visar Berisha and Prof. Alfred Olivier Hero III for providing us with part of the
code required to implement the non-parametric Fisher information matrix estimator used in this work.

13

Appendix A Proposed formulation of the semideﬁnite constraint
Here we provide the details of the formulation in matrix form of the mat(·) operator in Eq. 11. This is
a necessary step in order to implement the semideﬁnite constraint in matrix form.
First, we express the constraint with mat(·) using the inverse operator, vec(·), which transforms a
matrix into its vector representation. A matrix F ∈ Rm×n is converted into the vector representation as
follows:

n(cid:88)

Fvec =

BiFEi,

(17)

i=1

where Ei is the i-th canonical basis vector of an n-dimensional Euclidean space, i.e., Ei = [0, . . . , 0, 1, 0, . . . , 0]T
has a 1 in the i-th position and 0 elsewhere. Bi is a (mn) × m block matrix deﬁned as a stack of n
blocks, which are deﬁned as m× m zero-matrix with the exception of the i-th block, which is the identity
matrix:

Bi = [0m×m,··· , 0m×m, Im×m, 0m×m,··· , 0m×m]T .

(18)

Notice that, in our case, m = n = d, where d is the number of ESN hyper-parameters taken into
account. To convert the half-vector representation Fhvec in Eq. 11 into the vector form Fvec, we rely on
the following expression:

D (SFhvec) = Fvec,

where D and S are the multiplication and the shuﬄing matrices, respectively. These matrices cannot
be expressed in closed-form [29]. Therefore, in the following we provide the pseudo-code of Algorithms
2 and 3 that implement them.

γ ∪ {i + di, . . . , i + d(d − 1)}
δ ∪ {i + d(i − 1) + 1, . . . , i + d(i − 1) + d − i}

Algorithm 2 Duplication matrix computation.
Input: Dimensionality d of the hyper-parameter space
Output: Duplication matrix, D
1: D = I
d2×d2 = [d1, . . . , dd2 ]T
2: γ = δ = ∅
3: for i = 1, . . . , d − 1 do
4:
5:
6: end for
7: for i = 1, . . . , d(d − 1)/2 do
8:
9: end for
10: for i = 1, . . . , d(d − 1)/2 do
11:
12: end for

remove dγ(i) from D

dδ(i) = dγ(i) + dδ(i)

Algorithm 3 Shuﬄing matrix computation.
Input: Dimensionality d of the hyper-parameter space
Output: Shuﬄing matrix, S

1: S = 0d(d+1)/2×d(d+1)/2 =(cid:2)s1, . . . , sd(d+1)/2
(cid:3)T
(cid:3)T
2: I = Id(d+1)/2×d(d+1)/2 =(cid:2)i1, . . . , id(d+1)/2

γ = 1 + d(j − 1) − (j − 1)(j − 2)/2
sj = iγ
remove iγ from I

3: s1 = i1
4: for j = 2, . . . , d do
5:
6:
7:
8: end for
9: for j = d + 1, . . . , d(d + 1)/2 do
10:
11: end for

sj = ij

14

The optimization problem in (11) can be ﬁnally formalized as:

(cid:107)RFhvec − vθ(cid:107)2

Fhvec

minimize
subject to Fhvec(i) = ˆFhvec(i), i ∈ {1, . . . , d},

D (SFhvec) = Fvec,

d(cid:88)

BiFEi,

Fvec =
F (cid:23) 0d×d.

i=1

(19)

References

[1] J. Aljadeﬀ, M. Stern, and T. Sharpee. Transition to chaos in random networks with cell-type-speciﬁc connectivity.

Physical Review Letters, 114:088101, Feb. 2015. doi: 10.1103/PhysRevLett.114.088101.

[2] L. Appeltant, M. C. Soriano, G. Van der Sande, J. Danckaert, S. Massar, J. Dambre, B. Schrauwen, C. R. Mirasso,
and I. Fischer. Information processing using a single dynamical node as complex system. Nature Communications, 2:
468, 2011. doi: 10.1038/ncomms1476.

[3] J. Beck, V. R. Bejjanki, and A. Pouget. Insights from a simple expression for linear Fisher information in a recurrently
connected population of spiking neurons. Neural Computation, 23(6):1484–1502, 2011. doi: 10.1162/NECO a 00125.
[4] V. Berisha and A. O. Hero III. Empirical non-parametric estimation of the Fisher information. IEEE Signal Processing

Letters, 22(7):988–992, Jul. 2015. ISSN 1070-9908. doi: 10.1109/LSP.2014.2378514.

[5] N. Bertschinger and T. Natschl¨ager. Real-time computation at the edge of chaos in recurrent neural networks. Neural

Computation, 16(7):1413–1436, 2004. doi: 10.1162/089976604323057443.

[6] F. Bianchi, L. Livi, and C. Alippi. Investigating echo state networks dynamics by means of recurrence analysis. arXiv

preprint arXiv:1601.07381, 2016.

[7] F. M. Bianchi, E. De Santis, A. Rizzi, and A. Sadeghian. Short-term electric load forecasting using echo state networks

and PCA decomposition. IEEE Access, 3:1931–1943, 2015. doi: 10.1109/ACCESS.2015.2485943.

[8] F. M. Bianchi, S. Scardapane, A. Uncini, A. Rizzi, and A. Sadeghian. Prediction of telephone calls load using echo

state network with exogenous variables. Neural Networks, 71:204–213, 2015. doi: 10.1016/j.neunet.2015.08.010.

[9] V. D. Blondel, M. Esch, C. Chan, F. Cl´erot, P. Deville, E. Huens, F. Morlot, Z. Smoreda, and C. Ziemlicki. Data for

Development: the D4D Challenge on Mobile Phone Data. ArXiv preprint arXiv:1210.0137, 2012.

[10] J. Boedecker, O. Obst, J. T. Lizier, N. M. Mayer, and M. Asada. Information processing in echo state networks at

the edge of chaos. Theory in Biosciences, 131(3):205–213, 2012. doi: 10.1007/s12064-011-0146-8.

[11] M. Buehner and P. Young. A tighter bound for the echo state property. IEEE Transactions on Neural Networks, 17

(3):820–824, May 2006. ISSN 1045-9227. doi: 10.1109/TNN.2006.872357.

[12] L. De Arcangelis, F. Lombardi, and H. J. Herrmann. Criticality in the brain. Journal of Statistical Mechanics: Theory

and Experiment, 2014(3):P03026, 2014. doi: 10.1088/1742-5468/2014/03/P03026.

[13] J. H. Friedman and L. C. Rafsky. Multivariate generalizations of the Wald-Wolfowitz and Smirnov two-sample tests.

The Annals of Statistics, 7(4):697–717, 1979.

[14] S. Ganguli, D. Huh, and H. Sompolinsky. Memory traces in dynamical systems. Proceedings of the National Academy

of Sciences, 105(48):18970–18975, 2008. doi: 10.1073/pnas.0804451105.

[15] G. G´omez-Herrero, W. Wu, K. Rutanen, M. C. Soriano, G. Pipa, and R. Vicente. Assessing coupling dynamics from

an ensemble of time series. Entropy, 17(4):1958–1970, 2015. doi: 10.3390/e17041958.
[16] M. Grant and S. Boyd. Graph implementations for nonsmooth convex programs.

In V. Blondel, S. Boyd, and
H. Kimura, editors, Recent Advances in Learning and Control, Lecture Notes in Control and Information Sciences,
pages 95–110. Springer-Verlag Limited, 2008.

[17] M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming, version 2.1. Available at: http:

//cvxr.com/cvx, Mar. 2014.

[18] P. Grigolini. Emergence of biological complexity: Criticality, renewal and memory. Chaos, Solitons & Fractals, 2015.

doi: 10.1016/j.chaos.2015.07.025.

[19] M. Hermans and B. Schrauwen. Recurrent kernel machines: Computing with inﬁnite echo state networks. Neural

Computation, 24(1):104–133, 2012. doi: 10.1162/NECO a 00200.

[20] J. Hesse and T. Gross. Self-organized criticality as a fundamental property of neural systems. Frontiers in Systems

Neuroscience, 8:166, 2014. doi: 10.3389/fnsys.2014.00166.

[21] J. Hidalgo, J. Grilli, S. Suweis, M. A. Mu˜noz, J. R. Banavar, and A. Maritan. Information-based ﬁtness and the
emergence of criticality in living systems. Proceedings of the National Academy of Sciences, 111(28):10095–10100,
2014. doi: 10.1073/pnas.1319166111.

[22] J. Hidalgo, J. Grilli, S. Suweis, A. Maritan, and M. A. Mu˜noz. Cooperation, competition and the emergence of

criticality in communities of adaptive systems. arXiv preprint arXiv:1510.05941, 2015.

[23] H. Jaeger. Short term memory in echo state networks. GMD-Forschungszentrum Informationstechnik, 2001.
[24] H. Jaeger. Adaptive nonlinear system identiﬁcation with echo state networks. In S. Becker, S. Thrun, and K. Ober-

mayer, editors, Advances in Neural Information Processing Systems, pages 593–600. MIT Press, 2002.

[25] R. Legenstein and W. Maass. Edge of chaos and prediction of computational performance for neural circuit models.

Neural Networks, 20(3):323–334, 2007. doi: 10.1016/j.neunet.2007.04.017.

15

[26] D. Li, M. Han, and J. Wang. Chaotic time series prediction based on a novel robust echo state network. IEEE
Transactions on Neural Networks and Learning Systems, 23(5):787–799, May 2012. ISSN 2162-237X. doi: 10.1109/
TNNLS.2012.2188414.

[27] M. Lukoˇseviˇcius and H. Jaeger. Reservoir computing approaches to recurrent neural network training. Computer

Science Review, 3(3):127–149, 2009. doi: 10.1016/j.cosrev.2009.03.005.

[28] W. Maass, P. Joshi, and E. D. Sontag. Computational aspects of feedback in neural circuits. PLoS Computational

Biology, 3(1):e165, 2007. doi: 10.1371/journal.pcbi.0020165.eor.

[29] J. R. Magnus and H. Neudecker. Matrix Diﬀerential Calculus with Applications in Statistics and Econometrics. John

Wiley & Sons, New York, NY, 1995.

[30] G. Manjunath and H. Jaeger. Echo state property linked to an input: Exploring a fundamental characteristic of

recurrent neural networks. Neural Computation, 25(3):671–696, 2013. doi: 10.1162/NECO a 00411.

[31] M. Massar and S. Massar. Mean-ﬁeld theory of echo state networks. Physical Review E, 87(4):042809, 2013. doi:

10.1103/PhysRevE.87.042809.

[32] P. Massobrio, L. de Arcangelis, V. Pasquale, H. J. Jensen, and D. Plenz. Criticality as a signature of healthy neural

systems. Frontiers in Systems Neuroscience, 9:22, 2015. doi: 10.3389/fnsys.2015.00022.

[33] I. Mastromatteo and M. Marsili. On the criticality of inferred models. Journal of Statistical Mechanics: Theory and

Experiment, 2011(10):P10012, 2011. doi: 10.1088/1742-5468/2011/10/P10012.

[34] N. M. Mayer. Input-anticipating critical reservoirs show power law forgetting of unexpected input events. Neural

Computation, 27(5):1102–1119, 2015. doi: 10.1162/NECO a 00730.

[35] T. Mora and W. Bialek. Are biological systems poised at criticality? Journal of Statistical Physics, 144(2):268–302,

2011. doi: 10.1007/s10955-011-0229-4.

[36] T. Mora, S. Deny, and O. Marre. Dynamical criticality in the collective activity of a population of retinal neurons.

Physical Review Letters, 114(7):078105, 2015. doi: 10.1103/PhysRevLett.114.078105.

[37] P. Moretti and M. A. Mu˜noz. Griﬃths phases and the stretching of criticality in brain networks. Nature Communi-

cations, 4, 2013. doi: 10.1038/ncomms3521.

[38] O. Obst and J. Boedecker. Guided self-organization of input-driven recurrent neural networks. In M. Prokopenko,

editor, Guided Self-Organization: Inception, pages 319–340. Springer Berlin, Heidelberg, Germany, 2014.

[39] O. Obst, J. Boedecker, and M. Asada. Improving recurrent neural network performance using transfer entropy. In
Neural Information Processing. Models and Applications, volume 6444, pages 193–200. Springer Berlin Heidelberg,
2010.

[40] M. C. Ozturk, D. Xu, and J. C. Pr´ıncipe. Analysis and design of echo state networks. Neural Computation, 19(1):

111–138, 2007. doi: 10.1162/neco.2007.19.1.111.

[41] R. Pascanu, T. Mikolov, and Y. Bengio. On the diﬃculty of training recurrent neural networks. arXiv preprint

arXiv:1211.5063, 2012.

[42] M. Prokopenko, J. T. Lizier, O. Obst, and X. R. Wang. Relating Fisher information to order parameters. Physical

Review E, 84(4):041116, 2011. doi: 10.1103/PhysRevE.84.041116.

[43] K. Rajan, L. F. Abbott, and H. Sompolinsky. Stimulus-dependent suppression of chaos in recurrent neural networks.

Physical Review E, 82(1):011903, 2010. doi: 10.1103/PhysRevE.82.011903.

[44] A. Rodan and P. Tiˇno. Minimum complexity echo state network. IEEE Transactions on Neural Networks, 22(1):

131–144, Nov. 2011. doi: 10.1109/TNN.2010.2089641.

[45] A. Roli, M. Villani, A. Filisetti, and R. Serra. Dynamical criticality: overview and open questions. arXiv preprint

arXiv:1512.05259, 2015.

[46] M. Scheﬀer, J. Bascompte, W. A. Brock, V. Brovkin, S. R. Carpenter, V. Dakos, H. Held, E. H. Van Nes,
M. Rietkerk, and G. Sugihara. Early-warning signals for critical transitions. Nature, 461(7260):53–59, 2009. doi:
10.1038/nature08227.

[47] M. Scheﬀer, S. R. Carpenter, T. M. Lenton, J. Bascompte, W. Brock, V. Dakos, J. van De Koppel, I. A. van De
Leemput, S. A. Levin, E. H. van Nes, M. Pascual, and J. Vandermeer. Anticipating critical transitions. Science, 338
(6105):344–348, 2012. doi: 10.1126/science.1225244.

[48] B. Schrauwen, M. Wardermann, D. Verstraeten, J. J. Steil, and D. Stroobandt. Improving reservoirs using intrinsic

plasticity. Neurocomputing, 71(7):1159–1171, 2008. doi: 10.1016/j.neucom.2007.12.020.

[49] H. Shen and J. Z. Huang. Analysis of call centre arrival data using singular value decomposition. Applied Stochastic

Models in Business and Industry, 21(3):251–263, 2005. doi: 10.1002/asmb.v21:3.

[50] Z. Shi and M. Han. Support vector echo-state machine for chaotic time-series prediction. IEEE Transactions on

Neural Networks, 18(2):359–372, Mar. 2007. doi: 10.1109/TNN.2006.885113.

[51] D. Sussillo and O. Barak. Opening the black box: Low-dimensional dynamics in high-dimensional recurrent neural

networks. Neural Computation, 25(3):626–649, 2013. doi: 10.1162/NECO a 00409.

[52] P. Tiˇno and A. Rodan. Short term memory in input-driven linear dynamical systems. Neurocomputing, 112:58–63,

2013. doi: 10.1016/j.neucom.2012.12.041.

[53] G. Tkaˇcik and W. Bialek. Information processing in living systems. Annual Review of Condensed Matter Physics, 7

(1):89–117, 2016. doi: 10.1146/annurev-conmatphys-031214-014803.

[54] G. Tkaˇcik, T. Mora, O. Marre, D. Amodei, S. E. Palmer, M. J. Berry, and W. Bialek. Thermodynamics and signatures
of criticality in a network of neurons. Proceedings of the National Academy of Sciences, 112(37):11508–11513, 2015.
doi: 10.1073/pnas.1514188112.

[55] J. J. Torres and J. Marro. Brain performance versus phase transitions. Scientiﬁc Reports, 5, 2015. doi: doi:

10.1038/srep12216.

[56] T. Toyoizumi and L. F. Abbott. Beyond the edge of chaos: Ampliﬁcation and temporal integration by recurrent

networks in the chaotic regime. Physical Review E, 84(5):051908, 2011. doi: 10.1103/PhysRevE.84.051908.

16

[57] T. Toyoizumi, K. Aihara, and S-I. Amari. Fisher information for spike-based population decoding. Physical Review

Letters, 97(9):098102, 2006. doi: 10.1103/PhysRevLett.97.098102.

[58] M. K. Transtrum, B. B. Machta, K. S. Brown, B. C. Daniels, C. R. Myers, and J. P. Sethna. Perspective: Sloppiness
and emergent theories in physics, biology, and beyond. The Journal of Chemical Physics, 143(1):010901, 2015. doi:
10.1063/1.4923066.

[59] D. Verstraeten and B. Schrauwen. On the quantiﬁcation of dynamics in reservoir computing. In Artiﬁcial Neural

Networks–ICANN 2009, pages 985–994. Springer Berlin Heidelberg, 2009. doi: 10.1007/978-3-642-04274-4 101.

[60] G. Wainrib and J. Touboul. Topological and dynamical complexity of random neural networks. Physical Review

Letters, 110:118101, Mar. 2013. doi: 10.1103/PhysRevLett.110.118101.

[61] X. Wang, J. Lizier, and M. Prokopenko. Fisher information at the edge of chaos in random boolean networks. Artiﬁcial

Life, 17(4):315–329, Oct. 2011. ISSN 1064-5462. doi: 10.1162/artl a 00041.

[62] I. B. Yildiz, H. Jaeger, and S. J. Kiebel. Re-visiting the echo state property. Neural Networks, 35:1–9, 2012. doi:

10.1016/j.neunet.2012.07.005.

[63] P. Zegers. Fisher information properties. Entropy, 17(7):4918–4939, 2015. doi: 10.3390/e17074918.

17

