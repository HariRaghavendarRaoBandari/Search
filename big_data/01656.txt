Using the SAL technique for spatial veriﬁcation of cloud processes:

A sensitivity analysis

Michael Weniger∗ and Petra Friederichs†

Meteorological Institute, University of Bonn, Germany

6
1
0
2

 
r
a

M
4

 

 
 
]
h
p
-
o
a
.
s
c
i
s
y
h
p
[
 
 

1
v
6
5
6
1
0

.

3
0
6
1
:
v
i
X
r
a

∗Corresponding author address: Michael Weniger, Meteorological Institute, University of Bonn,

Auf dem Hügel 20, 53121 Bonn, Germany.

E-mail: mweniger@uni-bonn.de

†Current afﬁliation: Meteorological Institute, University of Bonn, Germany.

Generated using v4.3.2 of the AMS LATEX template

1

ABSTRACT

The feature based spatial veriﬁcation method SAL is applied to cloud data, i.e. two-dimensional spa-

tial ﬁelds of total cloud cover and spectral radiance. Model output is obtained from the COSMO-DE

forward operator SynSat and compared to SEVIRI satellite data. The aim of this study is twofold. First,

to assess the applicability of SAL to this kind of data, and second, to analyze the role of external object

identiﬁcation algorithms (OIA) and the effects of observational uncertainties on the resulting scores.

As a feature based method, SAL requires external OIA. A comparison of three different algorithms

shows that the threshold level, which is a fundamental part of all studied algorithms, induces high sensi-

tivity and unstable behavior of object dependent SAL scores (i.e. even very small changes in parameter

values can lead to large changes in the resulting scores). An in-depth statistical analysis reveals sig-

niﬁcant effects on distributional quantities commonly used in the interpretation of SAL, e.g. median

and interquartile distance. Two sensitivity indicators based on the univariate cumulative distribution

functions are derived. They allow to asses the sensitivity of the SAL scores to threshold level changes

without computationally expensive iterative calculations of SAL for various thresholds. The mathemat-

ical structure of these indicators connects the sensitivity of the SAL scores to parameter changes with

the effect of observational uncertainties.

Finally, the discriminating power of SAL is studied. It is shown, that – for large-scale cloud data –

changes in the parameters may have larger effects on the object dependent SAL scores (i.e. the S and

L2 scores) than a complete loss of temporal collocation.

2

1. Introduction

Veriﬁcation of numerical model output is essential in the development of successful models

for numerical weather prediction. Due to an increase in model resolution new techniques for the

evaluation of spatial ﬁelds have emerged during the last decade (see for instance Casati et al.

(2008); Gilleland et al. (2009); Ebert (2008)). Feature based methods are an important part of

this toolkit. These methods use score functions that are deﬁned on objects, and not on the spatial

ﬁeld itself (i.e. on a subset of the spatial data usually identiﬁed by some external algorithm). Most

feature based methods have been designed with a speciﬁc ﬁeld of application in mind. Veriﬁcation

of precipitation ﬁelds is the most prominent application, and various methods have been developed

for this kind of spatial data, e.g. Contiguous Rain Area (Ebert and McBride 2000; Ebert and

Gallus Jr 2009), Method for Object-based Diagnostic Evaluation (Davis et al. 2006a,b) and SAL

(Wernli et al. 2008).

SAL stands for its three score components: (S)tructure, (A)mplitude and (L)ocation. It was de-

veloped to measure the quality of a forecast using three distinct scores, which have direct physical

interpretations to allow for conclusions on potential sources of model errors. It does not require

matching individual objects in observations and forecasts, but compares the statistical characteris-

tics of those ﬁelds. The resulting scores are close to a subjective visual assessment of the accuracy

of the forecast for precipitation data. SAL was originally developed for the veriﬁcation of precip-

itation ﬁelds in a deﬁned area (e.g. river catchments) and today it is widely used in the evaluation

of quantitative precipitation forecasts (e.g. Zacharov et al. 2013; Leoncini et al. 2013; Zimmer

et al. 2011). Recently, efforts to employ SAL to different kinds of data have been made: Shi et al.

(2014) used SAL for the evaluation of a soil moisture model and Crocker and Mittermaier (2013)

applied SAL on binary cloud masks.

3

The aim of this work is twofold: ﬁrst, to assesses the beneﬁts and drawbacks of SAL applied

on cloud data. And second, to systematically study the role of the object identiﬁcation algorithms

(OIA) and their parameters. Spatial ﬁelds that describe CP processes such as total cloud cover or

spectral radiance may contain large scale structures. A focus of this study is thus to investigate

how well SAL is able to deal with large features, and to quantitatively analyze the effect of dif-

ferent OIA parameter settings. Wernli et al. (2008) investigated the so-called “camel cases” on a

qualitative level and showed that even small changes in the threshold level of the OIA can lead to

very different SAL scores. We follow this line of thought and conduct an extensive statistical anal-

ysis of large sets of spatial cloud data to quantify the sensitivity of SAL towards three parameters:

threshold ration, smoothing radius and minimal object size. Since substantially different threshold

levels correspond to different physical situations, we expect the resulting scores to be different as

well. This is true not only for SAL but virtually any threshold based veriﬁcation method, such

as the Fractions Skill Score (Roberts and Lean 2008) or the Intensity-Scale Skill Score (Casati

et al. 2004; Casati 2010). The interesting question is how the scores react to very small changes in

parameter values, i.e. whether the veriﬁcation score is numerically stable with respect to its OIA

parameters.

This stability, i.e. the effect of small perturbations in parameter values, is closely linked to the

effect of observational uncertainties, i.e. small perturbations in the data itself. Observational un-

certainties are generally ignored in spatial veriﬁcation methods (Ebert et al. (2013) and references

therein), which might be justiﬁed if observational errors are small compared to model errors. How-

ever, this assumption is not true for remotely sensed estimates of variables (e.g. estimates derived

from radar or satellite observations), particularly those related to cloud and precipitation (CP) pro-

cesses. While the instrument errors of direct satellite measurements such as spectral radiance or

brightness temperature are small, this is not true for derived quantities such as cloud fraction or

4

cloud masks (Zinner et al. 2005; Crocker and Mittermaier 2013). Additional uncertainties enter the

veriﬁcation process in form of spatial interpolation due to the discrepancy between the model grid

and the, usually irregular, observational grid. The evaluation of CP processes in high-resolution

model simulations strongly relies on this kind of remotely sensed observations (Evaristo et al.

2014; Steinke et al. 2015; Hammann et al. 2015; Nam et al. 2014; Eggert et al. 2015). Therefore,

it is important to understand the behavior of SAL with respect to observational uncertainties.

The article is structured as follows. We ﬁrst provide the mathematical deﬁnitions of SAL in Sec-

tion 2. Three different OIA and conceptional scenarios, which allow us to identify focal points for

the analysis of SAL’s parameter sensitivity, are discussed in Section 3. These points are explored

with exemplary cases and an in-depth statistical analysis using spatial data of total cloud cover

and spectral radiance in Section 4. The threshold parameter is of particular importance, since it is

the basis of all three OIA, is closely connected to observational uncertainties and impacts not only

SAL but other threshold based veriﬁcation techniques. A-priori and a-posteriori indicators, which

provide a computationally effective way to asses SAL’s sensitivity to varying thresholds, are dis-

cussed in Section 5. The insights gained from the mathematical formulation of these indicators are

used to establish the link between parameter sensitivity and observational uncertainties. Section

6 investigates the ability of the object dependent SAL scores to distinguish between two different

sets of cloud data.

2. Deﬁnition of SAL

Let us consider a two-dimensional domain D ⊂ R2 composed of N ∈ N grid points with a

maximal diameter

d := sup
(x,y)∈D

|x− y| > 0.

5

We now want to evaluate one set of spatial data R1 on the domain D with respect to a second set
of data R2. To this end, for each ﬁeld Ri, we deﬁne ni ∈ N objects Oi,k ⊂ D with k ∈ {1, . . . ,ni},
i ∈ {1,2} using some OIA. The OIAs are discussed later in Section 3. Based on the deﬁned objects

Oi,k the three components of SAL are deﬁned as follows:

• (A)mplitude

A =

(cid:104)R1(cid:105)D −(cid:104)R2(cid:105)D

0.5 ((cid:104)R1(cid:105)D +(cid:104)R2(cid:105)D )

∈ [−2,2],

where (cid:104).(cid:105)D denotes the average over the domain D. A perfect A-score of A = 0 indicates that

R1 is unbiased with respect to R2. In the case of A = 1, the spatial data R1 is overestimated
by a factor of 3, wheres A = −1 means that R1 is underestimated by a factor of 3.

• (L)ocation

Let us denote the center of total mass for the ﬁeld Ri by xi ∈ R2, for i ∈ {1,2}, the center of
mass for each object Oi,k by xi,k and the mass of each object by Mi,k ∈ R, for k ∈ {1, . . . ,ni},
i ∈ {1,2}. The L-score is deﬁned as

L1 =

ri =

|x1 − x2|
∈ [0,1]
d
k=1 Mi,k|xi − xi,k|
∑ni
∑ni
k=1 Mi,k
|r1 − r2|

∈ [0,1]
L2 = 2
L = L1 + L2 ∈ [0,2].

d

scattering of objects

The ﬁrst part of the L-score, L1, describes the relative distance between the centers of total

mass x1 and x2. The second part, L2, is a measure for the scattering of the identiﬁed objects.

Since both L-scores are fully deﬁned by the centers of total mass and centers of object’s mass,

L is rotation invariant, i.e. rotating the whole ﬁeld or an object around its center of mass does

not change the L score. For L = 0 we have a perfect location match of all centers of mass.

6

• (S)tructure

Vi,k =

Vi =

S =

Mi,k

scaled mass of object Oi,n

scaled, weighted total mass

maxx∈Oi,k Ri(x)
∑ni
k=1 Mi,kVi,k
∑ni
k=1 Mi,k
V1 −V2

0.5 (V1 +V2)

∈ [−2,2]

Vi,k is the mass of the object Oi,k after it has been rescaled to maximal height of 1. The scaled

total mass Vi is the weighted and normalized sum over the rescaled masses. The intent of the

rescaling is to remove, or at least dampen, the inﬂuence of total mass and concentrate on the

structure of the objects. An S-score of S = 0 is obtained for a perfect match for the structures

of all objects in both data sets. If S < 0 then the objects of R1 are too peaked compared to

those of R2, whereas for a positive S-score, S > 0, implies that they are too ﬂat.

For visualizations of the SAL properties the reader is referred to (Wernli et al. 2008).

3. Object Identiﬁcation Algorithms

One central component of SAL is the identiﬁcation of objects. While A and L1 scores are

independent of the object identiﬁcation, since they are directly deﬁned on the Ri ﬁelds, L2 and

S are deﬁned on the sets of objects Oi,k. As there exists a variety of OIA, which in turn require

the speciﬁcation of parameters, it is imperative to understand the sensitivity of the SAL scores to

the choice of the OIA. We start this study with the discussion of some conceptual cases. These

theoretical consideration show potential issues, which may lead to very unstable responses of SAL

with respect to small changes in parameter values. We will study the parameter sensitivity of three

different OIA, which are described in the following section, by looking at the behavior of the

object dependent scores S and L2.

7

In order to identify cohesive objects in spatial data, OIA typically use a threshold level and

deﬁne a continuous set of points of threshold exceedances as one object. In order to ﬁlter small

scale noise many methods apply a smoothing ﬁlter prior to object identiﬁcation, or ignore objects

smaller than a predeﬁned number of points. From the multitude of existing OIA we apply three

methods implemented in the R package SpatialVx (Gilleland 2014).

The OIA threshfac is the algorithm originally used with SAL (Wernli et al. 2009). It deﬁnes an
object as a cohesive set of threshold exceedances. The threshold is deﬁned as R∗

, where
is the 95%-quantile of the ﬁeld Ri, i ∈ {1,2} and f > 0 a threshold ratio with a default value

R95
i

i = f · R95

i

of f = 1/15. This simplistic approach has the advantage that its only parameter, the threshold

level, has a direct physical interpretation. The lack of smoothing or ﬁltering makes this method

susceptible to the effect of small scale noise, which might lead to a unrepresentative dominance of

very small, scattered objects. This issue is addressed by the following OIA.

The convolution threshold algorithm convthresh (Davis et al. 2006a,b) identiﬁes objects in two

steps. First, the data ﬁelds are convolved with a smoothing process, i.e.

the value at each grid

point is replaced by the mean value over a disc with a radius given by the parameter smoothpar.

Second, the convolved data is thresholded yielding a binary mask, which in turn is applied to the

original ﬁelds. The resulting objects thus have the original values at each grid point but smoothed

boundaries. The advantage is that the borders of the objects are smooth similar to those a human

would draw manually. The method ﬁlters out small scale noise (i.e. small scattered objects), which

is either isolated or located at the borders of large objects. The drawback is the introduction of an

additional parameter, the smoothing radius, which has no direct physical interpretation. Therefore

it is not obvious how to choose this parameter for a given set of data.

The algorithm threshsizer (Nachamkin 2009) deﬁnes objects as cohesive threshold exceedances,

where objects consisting of less than NContig grid points are omitted. This method is used to ﬁlter

8

out small isolated objects. The interpretation of the NContig parameter is more straightforward

than the smoothing radius in the previous OIA, but has no effect on the shape of large objects.

Hence, it is easier to foresee the consequences of a particular choice of parameter value, but the

objects do not look as natural as the ones provided by convthresh.

Let us now consider conceptual cases on a 4× 4 grid with values of different intensity. Fig. 1

a) shows the effect of a varying threshold level. Depending on the threshold level, objects of

lower intensity are either identiﬁed or ignored. The presence or absence of the lower intensity

objects inﬂuences L2 and S. On the one hand, this is a deliberate effect, since different threshold

levels concentrate the analysis to different physical situations. However, this effect might become

problematic when the threshold is close to the lower intensity value, since an arbitrarily small

change in the threshold level may cause the whole object to vanish and lead to a potentially large

change in the L2 and S scores.

The effect of the smoothing radius is illustrated in Fig. 1 b). Smoothing may cause a low intensity
bridge to fall below the threshold. In our example, smoothing is achieved by averaging over a 3×3

window. The resulting sets of objects differ in average spread and structure, which in turn leads to

changes in the L2 and S scores. If such a bridge is very narrow or its value is close to the threshold

level, then even small changes in the strength of the smoothing may have large effects on the L2

and S scores.

In contrast, the effect of the minimal object size parameter (i.e. the Ncontig of the threshsizer

algorithm) only affects small objects, regardless of the intensity of the values (Fig. 1 c)). Since all

object dependent SAL scores are weighted with the mass of the objects, the effect on the scores

should be small for small changes in the parameter value. Extreme cases, where e.g. only two

small objects are present and one of them vanishes due to a slightly raised Ncontig parameter, may

9

be thought of, but are very unlikely to occur for actual data with hundreds or thousands of grid

points.

Let us consider a conceptual setting (Fig. 2), which allows for an easy calculation of the L2

score. In this setting, the middle grid point has a value equal to the threshold. An arbitrarily small

change in the threshold level yields a change in the L2 score from the optimal to the worst possible

score. In the scenario A the L2 score amounts to 1, whereas in scenario B forecast and observation

are identical with a perfect score of L2 = 0. This example demonstrates that there exist situations

in which the L2 score is unstable, i.e. it changes from the best to the worst L2 value for arbitrarily

small parameter changes in the OIA.

4. Analysis of Parameter Sensitivity

To investigate the sensitivity of SAL regarding cloud processes, we use model data from the

“Synthetic Satellite Simulator” (SynSat, Keil et al. 2006) implemented in the operational regional

weather prediction model COSMO-DE at the Deutscher Wetterdienst (DWD), which computes

synthetic spectral radiances and brightness temperatures of eight MSG channels (Crewell et al.
2008). The model has a horizontal resolution of 2.8km and covers a domain with 421× 461 grid

points containing Germany, Switzerland and Austria. For each day, the forecast is initiated at

00UTC and yields synthetic satellite data with a temporal resolution of 15min. As observations

we use data from the SEVIRI satellite (Crewell et al. 2008; Reuter et al. 2009) for a domain of
302× 202 grid points with a maximal horizontal resolution of 3km.

Nine different variables are studied: total cloud cover (TClC) and eight channels of spectral radi-

ance. TClC is derived for the observational data as the fraction of cloudy pixels in a grid box using

the NWC SAF MSG v2010 algorithm, which is based on a multi-spectral thresholding technique

(Derrien and Le Gléau 2005, 2010). The COSMO model uses a parametrization based on relative

10

humidity in its radiation scheme and a statistical cloud scheme (Sommeria and Deardorff 1977)

within the turbulence model to parametrize boundary layer clouds (Schättler et al. 2013). We have

chosen spectral radiance over brightness temperature to study parameter sensitivity, since it allows

us to use SAL in its original formulation (implemented in the R package SpatialVx). For brightness

temperature the deﬁnition of threshold levels based on the 95% quantile is problematic, because

the minimal value of the ﬁelds is far greater than zero. Therefore, only a very small range of

threshold ratios around f = 1 would yield sensible thresholds. Thus, by using spectral radiances,

we avoid additional choices how to normalize the data or change the threshold routine, and can

concentrate on the effects of different OIA and their parameters. If one is primarily interested in

the direct veriﬁcation results (e.g. to evaluate a speciﬁc model setup) and not in a technical analysis

of the veriﬁcation method itself, this decision should be revisited. In this case it would be inter-

esting to compare veriﬁcation scores derived from spectral radiance and brightness temperature,

which essentially describe the same physical quantity. Due to the strictly monotone increasing

relation between brightness temperature and spectral radiance based on Planck’s law, a one to one

conversion of all data points and thresholds would not change the results of the sensitivity study.

We refer to the technical reports for SEVIRI for more details (EUMETSAT 2012a,b).

The results shown concentrate on the spectral radiance at IR6.2, i.e. the water vapor band at

a wavelength of 6.2µm. For each variable, we compare observed and synthetic IR6.2 radiance

values every 3 hours (starting at 00UTC) between 1 January 2012 and 19 February 2012, resulting

in 400 pairs of spatial ﬁelds. Note that each set of 400 spatial ﬁelds includes forecasts of eight

different lead times, which have a large impact on the veriﬁcation scores. However, we are not

interested in absolute SAL values, but rather in the difference of two SAL values calculated for

the same ﬁelds with different OIA parameter settings. Examples in form of two case studies are

stated below. The consideration of different lead times allows us to cover the whole range of small

11

and large SAL values for the study of parameter sensitivity. Since SAL requires all the data to be

on the same grid, the model output was interpolated onto the shared area (Fig. 3) of the coarser

observational grid using a straightforward nearest-neighbor method. While the effect of different

interpolation methods, e.g. bilinear, weighted, spline-based or via kriging (Li and Heap 2008),

may have very signiﬁcant effects on veriﬁcation scores, a systematic statistical analysis is out of

the scope of this work. In order to focus on the study of different OIA parameter settings, we use

the computationally least expensive interpolation method.

Before we explore the statistical consequences different OIA and parameter settings have on

SAL, let us consider two exemplary cases, where the conceptual processes from Section 3 can be

observed for meteorological data. Both cases exhibit large changes in S and L2 scores due to small

changes in the parameters of the OIA. Following the line of thoughts established in Section 3,

these object decomposition processes may occur for the convthresh and threshfac algorithm. We

distinguish between two different types of object decomposition, one where the (spatial) shape of

a large object is the deciding factor, and one where the intensity structure of an object is the most

important criterion.

The ﬁrst type, which is shown on a conceptual level in Fig. 1b, is responsible for most of the

large deviations when using the convthresh OIA. Fig 4 shows a case for IR6.2 using the convthresh

algorithm with a smoothing radius of 0 and 1, respectively. Without smoothing the OIA identiﬁes

one dominating object in both observation and forecast. Although these do not match perfectly,

they are very similar, which leads to small scores of S = 0.12 and L2 = 0.04. Using the smallest

possible smoothing radius of 1 grid point causes the small interconnecting bridge in the center of

the object in the observations to vanish. This leads to the decomposition of the dominant object

into two large ones. Since the dominant object in the forecast is unaffected, S and L2 scores

exhibit large changes:

the object in the forecast is too large, which results in a large positive

12

structure score (S = 0.72). The spread of objects is too small in the forecast resulting in a large L2

score (L2 = 0.44). In this case, the shape of the bridge is crucial, i.e. it has to be thin in order to

vanish due to smoothing, while its intensity values are only of secondary importance.

The second type of object decomposition is shown on a conceptual level in Fig. 1a. In real

meteorological data the situation is not as palpable most of the time, but the deﬁning aspect that a

large part of an objects mass falls below a varying threshold level can be clearly observed. Fig 5

shows a case for IR6.2 where the threshfac algorithm is applied with threshold ratios of 0.9 and

1, respectively. For the lower threshold the forecast is dominated by a large elongated object.

Raising the parameter causes most of this object to fall below the now higher threshold level. The

remaining mass is then identiﬁed as a cluster of smaller objects. The objects in the observations

become smaller but are otherwise unaffected. For the lower threshold the dominant object in the

forecast is too large and thus responsible for a large positive S score (S = 1.4). The situation is

reversed for the higher threshold: the structure of the clustered small objects in the forecast is
too small, which leads to a large negative S score (S = −1.11). While the effect on the L2 score

is small in this example (L2 f ac=0.9 = 0.05 and L2 f ac=1 = 0.18), we have observed other cases

where it exhibits large changes. The unpredictable behavior of the L2 score is one reason for the

low correlation between absolute changes in S and L2 scores for the threshfac OIA, which will be

discussed at the end of Section 4.

The exemplary cases show that object decomposition and the resulting large changes in SAL

scores can be caused by small changes in parameter values of the OIA. Let us now take closer

look at the statistical effects on the distribution of SAL scores over large data sets of N = 400 pairs
of spatial ﬁelds Ri, j, where i ∈ {1,2} denotes forecast and observation and j ∈ {1, . . . ,400} the

temporal index. We denote SAL as maximum-stable with respect to a parameter p of an OIA, if

13

small changes in the value of this parameter (∆p = p2 − p1) can only cause small changes in the
resulting SAL scores (∆SAL), i.e.

(cid:12)(cid:12)∆SAL(R1, j,R2, j, p1, p2)(cid:12)(cid:12) ≤ C|∆p| ,

max

j∈{1,...,N}

for a constant C > 0. SAL is mean-stable with respect to a parameter p, if there exists a constant

C > 0 with

1
N

N

∑

j=1

(cid:12)(cid:12)∆SAL(R1, j,R2, j, p1, p2)(cid:12)(cid:12) ≤ C|∆p| .

While maximal changes in SAL scores represent worst case scenarios, the mean value of score

changes presents a starting point to a distributional analysis of parameter sensitivity. We denote

SAL as maximum-unstable or mean-unstable with respect to a parameter of an OIA, if no

bounding constants exist, i.e. small changes in the parameter value can lead to large changes, or

even unbounded responses, in the resulting SAL scores.

Figs. 6-8 show the responses of L2 and S scores to parameter changes of the OIA for IR6.2. For

stable parameters we expect a linear decrease in (maximal and mean) absolute score differences

for decreasing differences in parameter value.

Fig. 6 shows that the threshold ratio, which varies for the threshfac algorithm, clearly induces

maximum-unstable and mean-unstable SAL scores. The maximum of absolute differences in L2

score is as high as 1.0. As discussed for the conceptual example this corresponds to the difference

between the best and the worst score possible. This holds true for the S score as well, with a max-

imum difference of 2.5. Naturally, the effect on the mean value is smaller but still very signiﬁcant

at about 0.2 for the L2 score and 0.4 for the S score.

The convthresh algorithm with a varying smoothing radius and a ﬁxed threshold ratio is studied

in Fig. 7. Here, the results are more complex: while the maxima of absolute score differences

14

indicate a maximum-unstable behavior, the mean values indicate mean-stable SAL scores. Largest

L2 and S score differences are of about 0.6. These are smaller than in Fig. 6 by a factor of 1.7 for

the L2 score and 4 for the S score. The differences in the mean values are smaller by a factor of

20 for both scores. Conclusively, the worst cases for varying smoothing radii are less severe and

occur less frequently than for the threshold ratio of the threshfac algorithm. Note, however, that a

different behavior might be obtained with another data set. Whether or not SAL can be regarded

as stable with respect to changes in the smoothing radius depends on two aspects: ﬁrst, the data,

and second, the question one wishes to address with the SAL veriﬁcation. If latter includes the

interpretation of quantiles other than the median (e.g., the interquartile range), a more elaborate

statistical analysis is necessary.

Fig. 8 shows the same stability analysis for the threshsizer algorithm, where the NContig pa-

rameter (i.e. the minimal size of objects) varies. Here, both maxima and mean values exhibit a

stable behavior of both scores. The differences are much smaller than for the previous cases.

Interestingly, even large changes in the parameter values lead to small changes in S and L2 scores.

In summary, we have an unstable behavior of SAL with respect to the threshold parameter

and a stable behavior with respect to the minimum object size. Varying smoothing radii seem to

induce mean-stable behavior, but may result in unstable SAL scores for some cases, which calls

for a closer look at the distributional properties of score changes.

In order to assess the sensitivity of the SAL scores to changes in the OIA parame-

ters on a distributional

level, we study the null hypothesis of equal distributions of the
threshold ratio f ∈
S and L2 scores.
{1/15,0.2,0.5,0.75,0.85,0.9,0.95,1}, smoothing radius smoothpar ∈ {0,1,2,5,10} and mini-
mal object size NContig ∈ {5,25,50,100,250,500,1000}. The null hypothesis is tested using

The following OIA parameters are considered:

15

three complementary hypothesis tests, notably the Kolmogorov-Smirnov (K, Kolmogorov 1933;

Smirnov 1939), median (M), and quantile (Q) test, where the test statistic is the interquartile dis-

tance (i.e. the distance between the 25% and 75% quantiles). The M and Q tests are permutation

tests (Good 2000) with 10,000 iterations and are particularly interesting for the study of SAL,

since both median and interquartile distance are important quantities for the interpretation of SAL

scores (Wernli et al. 2008).

For large variations of the threshold ratio in the threshfac algorithm one potentially looks at

different physical situations. Thus, signiﬁcant differences in the distributions of the S and L2 scores

are expected for large differences ∆ f > 0.1. Table 1 summarizes the results of all hypothesis tests

and all combinations of threshold ratios for 400 ﬁelds of IR6.2 data. The upper right triangular

shows results for the comparison of two S score distributions (italic font), while the lower left

triangular shows the results for L2 (bold font), e.g. for the two S distributions derived with f = 0.75

and f = 0.85 only the quantile test (Q) indicates signiﬁcant differences, while the L2 distributions

with the same parameter settings differ in all three test statistics (KMQ). Table 1 conﬁrms our

expectations for IR6.2 for all but the largest threshold ratios of the threshfac algorithm. These

results are consistent with the stability analysis for the threshold ratio in the previous section.

The Q test is tailored to detect changes in spread and therefore well suited for a two-sided score.

Accordingly, only the Q test is able to distinguish between S distributions for threshold ratios 0.75

and 0.85 (Table 1). Note that changes in S scores due to object decomposition are symmetric,

since the decomposition can happen in both observations and/or forecasts.

Both the convthresh and threshsizer algorithms show signiﬁcant differences only in the distri-

bution of the L2 but not the S scores (Tables 2 and 3). The reason for this is twofold: ﬁrst, S

is a two-sided score and changes in the score may cancel out in accumulated statistics like mean

or median. Therefore, the M test is less powerful to detect changes in the S score distribution.

16

Second, while – in principle – the K test is able to detect changes in spread, it has issues when

only the outer tails of the distributions are affected (e.g. Mason and Schuenemeyer 1983).

In

summary we can identify symmetric changes in distributions for the S score only if they affect

the interquartile distance. By deﬁnition, the interquartile distance is largely unaffected by small

variation of individual values. Hence, changes in the S score are expected to be small for the less

critical parameters, which is consistent with the results observed in Tables 2 and 3.

The distributional analysis has conﬁrmed the unstable behavior of the SAL scores with respect

to the threshold ratio in the threshfac algorithm. Whether or not smoothing radius and minimum

object size can be considered as uncritical parameters depends on the interpretation of the SAL

scores: if one is interested in the statistical quantities mean, median and interquartile distance both

can be considered as giving fairly stable SAL scores for IR6.2. However, for a varying smoothing

radius this highly depends on the data. Table 4 shows the results for TClC, where both median

and interquartile distance change signiﬁcantly for many parameter pairings.

It is often stated that the components of SAL are independent (e.g. Früh et al. 2007; Zimmer

et al. 2008). However, this is not true in the sense of the mathematical deﬁnition of statistical

independence. Quite contrarily, Fig. 9 shows signiﬁcant Pearson correlation coefﬁcients between

the absolute L2 and S differences of each of the 400 ﬁelds for all algorithms and standardized

parameter changes (i.e. the difference of two parameter values is divided by the maximum differ-

ence we investigated for this parameter). The correlation coefﬁcients between the absolute L2 and

S differences are even close to one for the convthresh algorithm, where most signiﬁcant changes

are due to object decomposition (see Section 3). The decomposition of a large object into two,

or the emergence of more small objects impacts the structure (S) and the spread of objects (L2),

simultaneously.

17

The correlation coefﬁcients for the threshsizer algorithm are slightly lower with values between

0.7 and 0.9. Here, an increasing Ncontig parameter implies that objects of increasing size are

omitted. Since only the small-scale objects are affected, the structure is shifted towards larger

objects with more mass. At the same time the spread is reduced. For threshfac we observe a large

spread in the correlation coefﬁcients varying between 0.4 and 0.8. This underlines the unstable

behavior with respect to a varying threshold level, since many different effects can occur: small

changes can lead to object decomposition, while large changes can cause objects of arbitrary size

to vanish. Latter can lead to vastly different S scores, but nearly unchanging L2 scores, which in

turn leads to lower correlation coefﬁcients. This behavior can be observed for the exemplary case

in Figure 5.

5. Indicators for Parameter Sensitivity and Observational Uncertainty

Of the three investigated OIA parameters the threshold ratio is most important one for two rea-

sons. First, all three OIA depend on the threshold level. Second, the SAL scores show the largest

sensitivity to changes in the threshold ratio. Therefore, we concentrate solely on the threshfac al-

gorithm in this section. Since the calculation of SAL scores for a multitude of different threshold

ratios rapidly becomes computationally expensive, it is useful for practical applications to ﬁnd a

quantity that indicates whether or not a given set of data exhibits a high sensitivity towards small

variations in threshold ratios, i.e. whether SAL is mean-stable with respect to threshold ratio. To

derive such a sensitivity indicator we concentrate on changes in the L2 score, which is mathemat-

ically more accessible. Section 4 shows that the results hold true also for the S score.

Since we are interested in the response to small parameter changes, we vary each of the
eight threshold ratios f ∈ {1/15,0.2,0.5,0.75,0.85,0.9,0.95,1} additionally by ±0.05, and cal-

culate L2 scores for all 24 resulting parameter values. Lets denote the original threshold level

18

by fi, the perturbed level by f −
for i ∈ {1, ...,8}. The L2 sensitivity at threshold level

and f +
i

i

fi for a single ﬁeld-to-ﬁeld compari-

, and the resulting L2 scores by L2−

i , L2i and L2+
i ,

son (R f cst vs. Robs) is given by the diameter (i.e.
{L2−

the maximum pairwise distance) of the set
i (R f cst,Robs)}. Taking the mean value over N = 400 ﬁeld-
to-ﬁeld comparisons then yields the L2 sensitivity (δL2) at threshold level fi for the complete set

i (R f cst,Robs),L2i(R f cst,Robs),L2+

of data:

δL2( fi) =

1
N

N

∑

l=1

diam

(cid:16)(cid:110)

L2−

i (R f cst

l

,Robs

l

),L2i(R f cst

l

,Robs

l

),L2+

i (R f cst

l

(cid:111)(cid:17)

)

.

,Robs

l

Largest sensitivity is expected in cases where a slight increase in the threshold ratio causes a large

number of grid points to fall below the threshold. We are therefore interested in the ratio of grid

points that vanish for a given ﬁeld due to an increase in threshold ratio. This quantity can be

approximated via the univariate empirical cumulative distribution function (ECDF) of the total set

of spatial ﬁelds (see Fig. 10 a), which describes the probability that IR6.2 is below a threshold.

The ECDF of observational data is deﬁned as

ecdf1(t) =

1

N #(D)

N

∑

l=1

(cid:16)(cid:110)
x ∈ D(cid:12)(cid:12) Robs

l

#

(cid:111)(cid:17)

,

(x) ≤ t

where #(·) denotes the number of elements in a set. The ECDF of the set of forecasts ﬁelds is

denoted by ecdf2 and deﬁned analogously. The ECDF-ratios, which are functions of the parameter

value fi, are given by

pk
i =

The lowest threshold f −
i

(cid:0) f +

i

(cid:1)− ecdfk
(cid:0) f −
(cid:1)
(cid:0) f −

i

i

ecdfk

1− ecdfk

(cid:1)

,

k ∈ {1,2}, i ∈ {1, ...,8}.

is used in the denominator to ensure that the ratio has an upper bound

equal to one. This is necessary to allow the interpretation of pk

i as a ﬁrst order approximation of a

“decomposition-probability”, i.e. the probability for the event that a large object decomposes into

two or more smaller objects. This interpretation is intuitive as seen for the following two extreme

19

cases. First, if no point vanishes when raising the threshold, the probability that a large object

decomposes is zero and pk

i = 0. Second, if all points vanish, the probability that a large object

vanishes is one and pk

i = 1. Therefore, pk

i approximates the decomposition probability while

ignoring any effects of spatial correlation. This helps us to estimate the sensitivity of L2, since we

can now quantify the probability that object decomposition occurs and hence sensitivity is high.

It may seem overly simpliﬁed to use a single ECDF for a whole set of 400 spatial ﬁelds instead

of 400 independent ECDFs. However, we are not interested in single worst case scenarios, but

in an indicator for mean-stability, which is a statistical quantity depending on the average score

deviations in the data set. This justiﬁes the above approach as a reasonable and computational

effective ﬁrst guess1.

We further need to quantify the effect that a process of object decomposition has on L2. Recall

that the L2 score at threshold level fi is deﬁned as
|r1( fi)− r2( fi)|

L2i = 2

d

i ∈ {1, ...,8},

,

where r1 and r2 describe the scattering of objects (see Section 2). L2i as well as r1( fi) and r2( fi)

are statistical estimators. The variance – or standard deviation – of a statistical estimator is closely

related to its robustness. We therefore use the empirical standard deviation σ over the whole set of

spatial ﬁelds of the three following quantities as a measure of the effect an object decomposition

would have on the L2 score. The effect is quantiﬁed as

• 2r1/d, if object decomposition occurs only in the ﬁrst spatial ﬁeld.
• 2r2/d, if object decomposition occurs only in the second spatial ﬁeld.
• L2i, if object decomposition occurs in both spatial ﬁelds simultaneously.
1To be on the safe side, we have also calculated indicators analogous to SI and (cid:101)SI deﬁned in (1) and (2) but based on single-ﬁeld ECDFs.

However, no signiﬁcant improvements could be observed.

20

Note that all standard deviations are calculated based only on SAL values for the threshold fi and
do not use any SAL values or calculations for the perturbed thresholds f −

.

i and f +
i

Using the ECDF-ratios pk

i as ﬁrst order approximation for the “decomposition probabilities” we

can estimate the expected L2 sensitivity for threshold fi as follows

SIi = p1

i (1− p2

i ) σ

+ p2

i (1− p1

i ) σ

(cid:18)2r1( fi)

(cid:19)

d

(cid:18)2r2( fi)

(cid:19)

+ p1

i p2

i σ (L2i) ,

(1)

d

for i ∈ {1, ...,8}. Figure 10.b shows that SI is indeed a good indicator for L2 sensitivity: if SI <

0.05, the change of L2 is bounded by 0.1. Note that once L2 scores have been calculated for a

given threshold, SI can be computed with very little computational cost, since all its components

but the univariate ECDF-ratios have already been calculated for L2. For an a-priori indicator that

uses only the ECDF information and no SAL values, we set 2r1

i /d, 2r2

i /d and L2i equal one and

obtain

(cid:101)SIi = p1

i (1− p2

i ) + p2

i (1− p1

i ) + p1

i p2
i ,

i ∈ {1, ...,8}.

Figure 10.c shows that the correlation between the L2 sensitivity and (cid:101)SI is still strong. This

(2)

indicates that a large part of the stability issues is founded in the univariate ﬁeld-distributions, i.e.

the slopes of the univariate ECDFs, while spatial correlations only play a minor role.

There exists a close link between the sensitivity of SAL to varying threshold levels and the

effects of observational uncertainties: if we look at the thresholded ﬁeld, it makes no difference

whether we raise the threshold level by a certain amount or lower the intensity of the ﬁeld by

the same constant amount at each grid point. The latter is equivalent to the effect of observational

uncertainties with inﬁnite spatial correlation length, i.e. an additive constant. Therefore, the results

for the sensitivity of SAL to varying threshold levels carry over to its sensitivity to large scale

uncertainties.

21

Do the results also contain valuable information about the effect of small scale uncertainties?

The previous section shows that (cid:101)SI is a good indicator for the L2 sensitivity with inﬁnite spatial
correlation length. However, (cid:101)SI employs only univariate ECDF information of the ﬁelds, i.e. it

ignores any information regarding spatial correlations. Therefore, the correlation length of the

(observational) uncertainties can only play a minor role for the sensitivity of SAL. This strongly

suggests that the sensitivity of SAL to uncertainties of arbitrary correlation length is close to SAL’s

sensitivity to varying threshold levels. Consequently, SI and (cid:101)SI are good indicators not only for

the sensitivity of SAL towards varying threshold levels, but also for the sensitivity of SAL towards

observational uncertainties.

6. Discrimination power of SAL

The sensitivity of the SAL parameters is closely linked to the ability of SAL to discriminate

’good’ and ’bad’ forecasts. We have shown that the SAL parameters are sensitive to changes in

the OIA, and as argued in Section 5 a similar effect is expected for observational uncertainties. It

is thus of interest to investigate the sensitivity of SAL scores towards artiﬁcial changes in the data

itself. A somewhat savage way to produce an artiﬁcially ’bad’ set of forecasts is to destroy the

temporal collocation of the 400 pairs of forecasted and observed ﬁelds by a random permutation

of the ﬁelds in time, i.e. for each observation a random forecast is drawn (from the set of 400

available forecast ﬁelds). We then ask whether SAL is able to distinguish the quality of the original

forecast and the randomly permuted forecast. This is achieved by testing the null hypothesis that

the SAL scores over the 400 pairs of ﬁelds in the original and permuted data set follow the same

distribution.

Table 5 provides results of the different hypothesis tests on the SAL values using the threshfac

OIA with different threshold ratios for the two different variables TClC and IR6.2. We included

22

TClC in this analysis to demonstrate that the results are strongly dependent on the type of data.

For TClC and IR6.2 the default threshold ratio of 1/15 exhibits almost no differences between

both distributions (see Fig. 11). Only the quantile permutation test is able to signiﬁcantly detect

differences in the S score distribution of the IR6.2 data.

Fig. 12 shows the density of the L2 and S score distributions for IR6.2 for a higher threshold

ratio of 0.85. Although signiﬁcant statistical differences can be observed for this threshold, the

distributions do not look as different as one would expect for such a hyperbolic case.

The traditional mean square error (MSE) is able to clearly discriminate between the original and

the permuted set of IR6.2 data, as shown in Fig. 13. In the original data set the MSE is signiﬁcantly

smaller than in the permuted data set.

For TClC and threshold ratios of 1/15 or 0.2 none of the two object dependent scores are able

to discriminate between the original and the perturbed data set (Tab. 5). This changes for higher

thresholds between 0.5 and 0.95, which suggests that the loss of discriminating power is due

to very large objects at low thresholds. However, the situation is not as clear cut: the highest

studied threshold ratio of 1, which identiﬁed the smallest objects of all OIA settings, again yields

indistinguishable score distributions.

Conclusively, the ability of object dependent SAL scores to distinguish between the two data

sets largely depends on the data, and cannot be guaranteed a-priori. It is important to note, that

randomly permuting the observations is synonymous with a complete loss of temporal collocation

between forecast and observation, which is expected to lead to signiﬁcantly worse scores, as is the

case for the MSE (Fig. 13).

On its own, the inability to distinguish between the original and permuted data set is not nec-

essarily a disaster. SAL investigates the statistics of the objects, and one might conclude that this

is relatively homogeneous in time and thus insensitive to the loss of temporal collocation. How-

23

ever, in view of the large sensitivity the SAL scores show with respect to the choice of the OIA

parameters, caution is advised when interpreting the results of SAL.

7. Conclusions

The aim of this work is twofold: ﬁrst, to study the applicability of SAL for cloud processes;

second, to identify and understand the importance of OIA and their parameters on feature based

veriﬁcation methods and the link to observational uncertainties. Three different OIA have been

used for the comparison of COSMO-DE SynSat data with SEVIRI satellite observations. In this

process varying values have been used for the three parameters threshold ratio, smoothing radius

and minimal size of objects. On a conceptual level we have shown, that small changes in thresh-

old levels or smoothing radii can potentially lead to very large score differences due to object

decomposition, which is conﬁrmed by two exemplary case studies of IR6.2 data.

To study SAL’s parameter sensitivity on a distributional level, we denote SAL as stable with

respect to a parameter of an OIA if small changes in the value of this parameter can lead to large

changes, or even unbounded responses, in the resulting SAL scores. SAL is unstable with respect

to threshold ratio and stable with respect to minimal object size. With respect to varying smoothing

radii SAL is mean-stable but maximum-unstable, i.e. there are rare worst case scenarios where

large score deviations occur.

In-depth statistical analysis using three different hypothesis tests

conﬁrms these results. For varying threshold ratios the observed large score deviations translate

into signiﬁcant changes in the distribution of S and L2 scores. Consistent with the prior stability

assessment the statistical implications for varying smoothing radii are much weaker.

The threshold ratio is of particular interest, not only because it is the most sensitive parameter,

but because it links the ﬁeld of parameter sensitivity to observational uncertainties: in cases where

the intensity of a spatial ﬁeld is close to the threshold level, changes in the threshold ratio (param-

24

eter sensitivity) lead to similar results as changes in the intensity of the data itself (observational

uncertainties). An a-posteriori indicator (SI ) for the stability of SAL to the threshold ratio parame-

ter shows promising results to assess the sensitivity without the need of expensive computations of

multiple threshold levels (Section 5). The a-priori indicator (cid:101)SI is based solely on univariate ECDF

information of the spatial ﬁelds and can therefore be calculated with very little computational ef-

fort. Both quantities can also be employed to asses SAL’s sensitivity to observational uncertainties

(see Section 5).

Highly sensitive parameters are particularly problematic if the changes in scores due to varying

parameters outweighs score deviations caused by actual differences in the data. Such a case is

discussed in Section 6, where S and L2 scores where unable to reliably detect the complete loss of

temporal allocation between forecast and observation.

To summarize, the choice of OIA and its parameters has a signiﬁcant effect on the resulting SAL

scores. Therefore it is essential to explicitly state the algorithm and all parameter settings when

using SAL for veriﬁcation. The use of complementary hypothesis tests has shown that it is advis-

able to include statistical quantities beside median and interquartile range for the interpretation of

SAL scores. The high sensitivity towards the threshold level implies a potentially high impact of

observational uncertainties. This particularly true for SAL’s original ﬁeld of application, i.e. the

veriﬁcation of quantitative precipitation ﬁelds against radar observations. By deﬁning sensitivity

indicators (SI ) and (cid:101)SI, we were able to quantify the connection between parameter sensitivity and

the effect of observational uncertainties. The fact, that small changes in parameter values have a

larger impact than even drastic changes in data implies that SAL is not well equipped too verify

this speciﬁc kind of data.

On a more technical level, object decomposition in conjunction with non-continuous operations,

e.g. thresholding, during object identiﬁcation has been established as the major cause for unstable

25

behavior. Due to the similarity between threshold sensitivity and observational uncertainties, this

study is a step towards the construction of feature based veriﬁcation methods, that are robust with

respect to observational uncertainties. The importance of the univariate ECDFs in the deﬁnition

of (SI ) and (cid:101)SI suggests, that the normalization of continuous data or the application of thresh-

olds solely based on quantiles could signiﬁcantly reduce the high sensitivity to parameters and

uncertainties.

Acknowledgments. We gratefully acknowledge ﬁnancial funding by the project High Deﬁnition

Clouds and Precipitation for advancing Climate Prediction HD(CP)2 funded by the German Min-

istry for Education and Research (BMBF) under grant FK 01LK1209B. The authors thank Sonja

Reitter (Universität Köln) and the DWD, who provided data from the COPS/GOP project, which

was founded by the Deutsche Forschungsgemeinschaft under grant WU 356/4-2. We further ap-

preciate the help of Jennifer Slobodda and Justus Franke (Institut für Weltraumwissenschaften,

Freie Universität Berlin), who prepared SEVIRI data as part of the DFG-ICOS program.

References

Casati, B., 2010: New developments of the intensity-scale technique within the Spatial Veriﬁcation

Methods Intercomparison Project. Weather and Forecasting, 25 (1), 113–143.

Casati, B., G. Ross, and D. Stephenson, 2004: A newintensity-scale approach for the veriﬁcation

of spatial precipitation forecasts. Meteorological Applications, 11 (02), 141–154.

Casati, B., and Coauthors, 2008: Forecast veriﬁcation: current status and future directions. Mete-

orological applications, 15 (1), 3–18.

Crewell, S., and Coauthors, 2008: The general observation period 2007 within the priority program

on quantitative precipitation forecasting: Concept and ﬁrst results. Meteorologische Zeitschrift,

26

17 (6), 849–866.

Crocker, R., and M. Mittermaier, 2013: Exploratory use of a satellite cloud mask to verify NWP

models. Meteorological Applications, 20 (2), 197–205.

Davis, C., B. Brown, and R. Bullock, 2006a: Object-based veriﬁcation of precipitation forecasts.

Part I: Methodology and application to mesoscale rain areas. Monthly Weather Review, 134 (7),

1772–1784.

Davis, C., B. Brown, and R. Bullock, 2006b: Object-based veriﬁcation of precipitation forecasts.

Part II: Application to convective rain systems. Monthly Weather Review, 134 (7), 1785–1795.

Derrien, M., and H. Le Gléau, 2005: MSG/SEVIRI cloud mask and type from SAFNWC. Inter-

national Journal of Remote Sensing, 26 (21), 4707–4732.

Derrien, M., and H. Le Gléau, 2010: Improvement of cloud detection near sunrise and sunset

by temporal-differencing and region-growing techniques with real-time SEVIRI. International

Journal of Remote Sensing, 31 (7), 1765–1780.

Ebert, E., and J. McBride, 2000: Veriﬁcation of precipitation in weather systems: Determination

of systematic errors. Journal of Hydrology, 239 (1), 179–202.

Ebert, E., and Coauthors, 2013: Progress and challenges in forecast veriﬁcation. Meteorological

Applications, 20 (2), 130–139.

Ebert, E. E., 2008: Fuzzy veriﬁcation of high-resolution gridded forecasts: a review and proposed

framework. Met. Apps, 15 (1), 51–64, doi:10.1002/met.25, URL http://dx.doi.org/10.1002/met.

25.

Ebert, E. E., and W. A. Gallus Jr, 2009: Toward better understanding of the contiguous rain area

(CRA) method for spatial forecast veriﬁcation. Weather and Forecasting, 24 (5), 1401–1415.

27

Eggert, B., P. Berg, J. Haerter, D. Jacob, and C. Moseley, 2015: Temporal and spatial scaling im-

pacts on extreme precipitation. Atmospheric Chemistry and Physics Discussions, 15 (2), 2157–

2196.

EUMETSAT, 2012a: Effective Radiances and Brightness Temperature Relation Tables for

Meteosat Second Generation. EUM/OPS-MSG/TEN/08/0024, Issue v2, available through

http://www.eumetsat.int.

EUMETSAT, 2012b: The Conversion from Effective Radiances to Equivalent Brightness Temper-

atures. EUM/MET/TEN/11/0569, Issue v1, available through http://www.eumetsat.int.

Evaristo, R., X. Xie, S. Troemel, M. Diederich, J. Simon, and C. Simmer, 2014: A macrophysical

life cycle description for precipitating systems. EGU General Assembly Conference Abstracts,

EGU General Assembly Conference Abstracts, Vol. 16, 10322.

Früh, B., J. Bendix, T. Nauss, M. Paulat, A. Pfeiffer, J. W. Schipper, B. Thies, and H. Wernli, 2007:

Veriﬁcation of precipitation from regional climate simulations and remote-sensing observations

with respect to ground-based observations in the upper Danube catchment. Meteorologische

Zeitschrift, 16 (3), 275–293.

Gilleland, E., 2014: SpatialVx: Spatial Forecast Veriﬁcation. URL http://CRAN.R-project.org/

package=SpatialVx, r package version 0.2-0.

Gilleland, E., D. Ahijevych, B. G. Brown, B. Casati, and E. E. Ebert, 2009: Intercomparison

of spatial forecast veriﬁcation methods. Wea. Forecasting, 24 (5), 1416–1430, doi:10.1175/

2009waf2222269.1, URL http://dx.doi.org/10.1175/2009waf2222269.1.

Good, P., 2000: Permutation tests. Springer.

28

Hammann, E., A. Behrendt, F. Le Mounier, and V. Wulfmeyer, 2015: Temperature proﬁling of

the atmospheric boundary layer with rotational Raman lidar during the HD(CP)2 Observational

Prototype Experiment. Atmospheric Chemistry and Physics, 15 (5), 2867–2881, doi:10.5194/

acp-15-2867-2015, URL http://www.atmos-chem-phys.net/15/2867/2015/.

Keil, C., A. Tafferner, and T. Reinhardt, 2006: Synthetic satellite imagery in the Lokal-Modell.

Atmospheric research, 82 (1), 19–25.

Kolmogorov, A. N., 1933: Sulla determinazione empirica di una legge di distribuzione. na.

Leoncini, G., R. Plant, S. Gray, and P. Clark, 2013: Ensemble forecasts of a ﬂood-producing

storm: comparison of the inﬂuence of model-state perturbations and parameter modiﬁcations.

Quarterly Journal of the Royal Meteorological Society, 139 (670), 198–211.

Li, J., and A. D. Heap, 2008: A review of spatial interpolation methods for environmental scien-

tists. Australia Geoscience, 23, geoCat #68229.

Mason, D. M., and J. H. Schuenemeyer, 1983: A modiﬁed Kolmogorov-Smirnov test sensitive to

tail alternatives. The Annals of Statistics, 933–946.

Nachamkin, J. E., 2009: Application of the composite method to the spatial forecast veriﬁcation

methods intercomparison dataset. Weather and Forecasting, 24 (5), 1390–1400.

Nam, C. C. W., J. Quaas, R. Neggers, C. Siegenthaler-Le Drian, and F. Isotta, 2014: Evaluation

of boundary layer cloud parameterizations in the ECHAM5 general circulation model using

CALIPSO and CloudSat satellite data. Journal of Advances in Modeling Earth Systems, 6 (2),

300–314, doi:10.1002/2013MS000277, URL http://dx.doi.org/10.1002/2013MS000277.

Reuter, M., W. Thomas, P. Albert, M. Lockhoff, R. Weber, K. Karlsson, and J. Fischer, 2009: The

CM-SAF and FUB cloud detection schemes for SEVIRI: validation with synoptic data and ini-

29

tial comparison with MODIS and CALIPSO. Journal of Applied Meteorology and Climatology,

48 (2), 301–316.

Roberts, N. M., and H. W. Lean, 2008: Scale-selective veriﬁcation of rainfall accumulations from

high-resolution forecasts of convective events. Monthly Weather Review, 136 (1).

Schättler, U., G. Doms, and C. Schraf, 2013: A Description of the Nonhydrostatic Regional

COSMO-Model, Part VII: User’s Guide.

Shi, X., J. Liu, Y. Li, H. Tian, and X. Liu, 2014: Improved SAL method and its application to

verifying regional soil moisture forecasting. Science China Earth Sciences, 57 (11), 2657–2670.

Smirnov, N. V., 1939: On the estimation of the discrepancy between empirical curves of distribu-

tion for two independent samples. Bull. Math. Univ. Moscou, 2 (2).

Sommeria, G., and J. Deardorff, 1977: Subgrid-scale condensation in models of nonprecipitating

clouds. Journal of the Atmospheric Sciences, 34 (2), 344–355.

Steinke, S., S. Eikenberg, U. Löhnert, G. Dick, D. Klocke, P. Di Girolamo, and S. Crewell,

2015: Assessment of small-scale integrated water vapour variability during HOPE. Atmo-

spheric Chemistry and Physics, 15 (5), 2675–2692, doi:10.5194/acp-15-2675-2015, URL

http://www.atmos-chem-phys.net/15/2675/2015/.

Wernli, H., C. Hofmann, and M. Zimmer, 2009: Spatial forecast veriﬁcation methods intercompar-

ison project: Application of the SAL technique. Weather and Forecasting, 24 (6), 1472–1484.

Wernli, H., M. Paulat, M. Hagen, and C. Frei, 2008: SAL - A Novel Quality Measure for the

Veriﬁcation of Quantitative Precipitation Forecasts. Monthly Weather Review, 136 (11).

Zacharov, P., D. Rezacova, and R. Brozkova, 2013: Evaluation of the QPF of convective ﬂash

ﬂood rainfalls over the Czech territory in 2009. Atmospheric Research, 131, 95–107.

30

Zimmer, M., G. Craig, C. Keil, and H. Wernli, 2011: Classiﬁcation of precipitation events with a

convective response timescale and their forecasting characteristics. Geophysical Research Let-

ters, 38 (5).

Zimmer, M., H. Wernli, C. Frei, and M. Hagen, 2008: Feature-based veriﬁcation of deterministic

precipitation forecasts with SAL during COPS. Proceedings from the MAP D-PHASE Scientiﬁc

Meeting in Bologna, Italy, 116–121.

Zinner, T., L. Bugliaro, and B. Mayer, 2005: Remote sensing of inhomogeneous clouds with

MSG/SEVIRI. Proceedings of the EUMETSAT Meteorological Satellite Conference, 46.

31

.

.

33

.

.

34

.

.

35

.

. 36

.

.

37

List of Tables
Table 1.

Signiﬁcant differences in score distributions for different values of the threshold
ratio f in the threshfac algorithm for 400 ﬁelds of IR6.2. The capital letters
denote hypothesis tests (deﬁned in Section 4) detecting differences at a 5% level
of signiﬁcance. The upper right triangular shows results for the comparison of
two S score distributions (italic font), while the lower left triangular shows the
results for L2 (bold font).
.

.

.

.

.

.

.

.

.

.

.

.

.

.

Table 2.

Table 3.

Table 4.

Table 5.

Signiﬁcant differences in score distributions for different values of the smooth-
ing radius smoothpar in the convthresh algorithm for 400 ﬁelds of IR6.2. The
capital letters denote hypothesis tests (deﬁned in Section 4) detecting differ-
ences at a 5% level of signiﬁcance. The upper right triangular shows results for
the comparison of two S score distributions (italic font), while the lower left
triangular shows the results for L2 (bold font).
.

.

.

.

.

.

.

.

Signiﬁcant differences in score distributions for different values of the mini-
mum object size NContig in the threshsizer algorithm for 400 ﬁelds of IR6.2.
The capital letters denote hypothesis tests (deﬁned in Section 4) detecting dif-
ferences at a 5% level of signiﬁcance. The upper right triangular shows results
for the comparison of two S score distributions (italic font), while the lower left
triangular shows the results for L2 (bold font).
.

.

.

.

.

.

.

.

Signiﬁcant differences in score distributions for different values of the smooth-
ing radius smoothpar in the convthresh algorithm for 400 ﬁelds of TClC. The
capital letters denote hypothesis tests (deﬁned in Section 4) detecting differ-
ences at a 5% level of signiﬁcance. The upper right triangular shows results for
the comparison of two S score distributions (italic font), while the lower left
triangular shows the results for L2 (bold font).
.

.

.

.

.

.

.

.

.

Signiﬁcant differences between score distributions of 400 ﬁelds derived from
original observations vs. observations randomly permuted in time at various
threshold ratios f in the threshfac algorithm. The capital letters denote hypoth-
esis tests deﬁned in Section 4 that where able to detect differences with a 5%
level of signiﬁcance. Results are shown for TClC and IR6.2 for the S score
(italic font) and L2 score (bold font).
.

.

.

.

.

.

.

.

.

.

.

32

f

1/15

0.20

0.50

0.75

0.85

0.90

0.95

1

-

-

Q
Q

1/15
0.20
0.50 KMQ KMQ
0.75 KMQ KMQ KMQ
0.85 KMQ KMQ KMQ KMQ
0.90 KMQ KMQ KMQ KMQ
0.95 KMQ KMQ KMQ KMQ
KMQ KMQ KMQ KMQ
1

KMQ KMQ KMQ KMQ KMQ
KMQ KMQ KMQ KMQ KMQ
KMQ KMQ KMQ KMQ KMQ

Q

-
K
K

-
-

-
-

-
-
-

-

-
-
-
-

Table 1.

Signiﬁcant differences in score distributions for different values of the threshold ratio f in the

threshfac algorithm for 400 ﬁelds of IR6.2. The capital letters denote hypothesis tests (deﬁned in Section 4)

detecting differences at a 5% level of signiﬁcance. The upper right triangular shows results for the comparison

of two S score distributions (italic font), while the lower left triangular shows the results for L2 (bold font).

33

smoothpar

0

2

5

10

1

-

-
-

K

-
K
K
KM KM KM K

-
K

0
1
2
5
10

-
-
-

-
-
-
-

Table 2. Signiﬁcant differences in score distributions for different values of the smoothing radius smoothpar

in the convthresh algorithm for 400 ﬁelds of IR6.2. The capital letters denote hypothesis tests (deﬁned in

Section 4) detecting differences at a 5% level of signiﬁcance. The upper right triangular shows results for the

comparison of two S score distributions (italic font), while the lower left triangular shows the results for L2 (bold

font).

34

NContig

5

50

100

250

500

1000

-
-
-
-

-
-
-
-
-

-
-
-
-
-
-

25

-

-
-

-
-
-

K
K

-
K
K

-
K
K
K
K
K
KM KM K
KM KM KM KM KM K

K

5
25
50
100
250
500
1000

Table 3. Signiﬁcant differences in score distributions for different values of the minimum object size NContig

in the threshsizer algorithm for 400 ﬁelds of IR6.2. The capital letters denote hypothesis tests (deﬁned in

Section 4) detecting differences at a 5% level of signiﬁcance. The upper right triangular shows results for the

comparison of two S score distributions (italic font), while the lower left triangular shows the results for L2 (bold

font).

35

smoothpar

0
1
2
5
10

5

10

0

-

1

-

2

-
-

-
-
-

-
-
-
-

-

KM
KM KM KM
KMQ KMQ KMQ KM

Table 4. Signiﬁcant differences in score distributions for different values of the smoothing radius smoothpar

in the convthresh algorithm for 400 ﬁelds of TClC. The capital letters denote hypothesis tests (deﬁned in Sec-

tion 4) detecting differences at a 5% level of signiﬁcance. The upper right triangular shows results for the

comparison of two S score distributions (italic font), while the lower left triangular shows the results for L2

(bold font).

36

f

S(TClC)

S(IR6.2) L2(TClC) L2(IR6.2)

1/15
0.20
0.50
0.75
0.85
0.90
0.95

1

-
-
-
Q
Q
KQ
KQ
-

Q
Q
Q
KQ
KQ
Q
-
Q

-
-
KQ
KM
KMQ
KMQ
KMQ

-

-
-
K

KMQ
KMQ
KMQ
KM
KM

Table 5. Signiﬁcant differences between score distributions of 400 ﬁelds derived from original observations

vs. observations randomly permuted in time at various threshold ratios f in the threshfac algorithm. The capital

letters denote hypothesis tests deﬁned in Section 4 that where able to detect differences with a 5% level of

signiﬁcance. Results are shown for TClC and IR6.2 for the S score (italic font) and L2 score (bold font).

37

List of Figures
Fig. 1.

Conceptual OIA cases. The left part of each panel shows the data of a domain with 4× 4
grid points. Dark blue squares indicate points with high intensity, while light blue coloring
denotes an intensity value near the threshold level. The resulting object masks are plotted
in red. Panel (a) shows varying threshold levels that cause a large object to vanish, (b)
demonstrates object decomposition due to varying smoothing radii, and (c) shows varying
minimal object sizes that cause a small object to vanish.

.

.

.

.

.

.

.

.

.

Fig. 2.

Conceptual case demonstrating a potential effect of small changes in parameter value. Dark
blue squares indicate points with high intensity, while light blue coloring denotes an intensity
value near the threshold level. Obs. A yields the worst possible L2 = 1 score, while Obs. B
is a perfect match with L2 = 0.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

Fig. 3. Map of the area shared by observational data and model output.

.

.

.

.

.

.

.

.

.

40

.

.

. 41

.

42

Case study: object decomposition for the convthresh algorithm for IR6.2 (19 January 2012,
03UTC). With a smoothing parameter of 0 one large object is identiﬁed in the observations.
Raising the smoothing radius to 1 grid point causes the small interconnecting bridge in this
object to vanish, which leads to decomposition and vastly different S and L2 scores.
.

.

.

. 43

Case study: object decomposition for the threshfac algorithm for IR6.2 (21 January 2012,
12UTC). For a threshold ratio of 0.9 one large object dominates the forecast. Raising the
threshold ratio to 1 causes most of the objects mass to fall below the threshold level. Effec-
tively the object decomposes into many small ones, which leads to vastly different S scores
but nearly constant L2 scores.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

Fig. 6. Differences in (a) L2 and (b) S scores for IR6.2 with respect to changes in the threshold ratio
(fac) of the threshfac algorithm. The box-whiskers represent the score differences over the
400 spatial ﬁelds. Solid boxes indicate the interquartile range, while the dashed lines reach
out to the extremes.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

Same as Fig. 6 but with respect to changes in the smoothing radius (smoothpar) of the
convthresh algorithm.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

Same as Fig. 6 but with respect to changes in the minimum object size NContig) of the
threshsizer algorithm.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

Fig. 4.

Fig. 5.

Fig. 7.

Fig. 8.

Fig. 9.

.

.

44

.

. 45

.

.

.

.

46

47

The Pearson correlation coefﬁcients between absolute changes in S and L2 scores for IR6.2
are plotted against the normalized difference in parameter values. For the convthresh algo-
rithm we only vary the smoothing radius (smoothpar), for the threshsizer algorithm only the
minimal object size (NContig) and for the threshfac algorithm the threshold ratio (fac).
.

.

. 48

Fig. 10. Sensitivity Indicators: (a) Univariate ECDF of 400 ﬁelds of IR6.2. For a given threshold
ratio of 0.75 (black cross), varying threshold ratios ( f −
f +
i ) and their resulting ECDF-
i
values are marked with dashed lines. Panel (b) shows L2 sensitivity against the a-posteriori
sensitivity indicator SI, and (c) against the a-priori indicator(cid:93)SI.

.

.

,

.

.

.

.

.

Fig. 11. Density of (a) L2 and (b) S score of original and permuted TClC with the default threshold
.

ratio f = 1/15.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

Fig. 12. Density of (a) L2 and (b) S score of original and permuted IR6.2 with threshold ratio f =

0.85.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

. 49

. 50

.

.

51

38

Fig. 13. Density of MSE for IR6.2 data of forecast vs. original and randomly permuted observations.

.

.

.

This traditional score is able to distinguish both sets of data easily.

.

.

.

.

.

52

39

Figure 1. Conceptual OIA cases. The left part of each panel shows the data of a domain with 4×4 grid points.
Dark blue squares indicate points with high intensity, while light blue coloring denotes an intensity value near

the threshold level. The resulting object masks are plotted in red. Panel (a) shows varying threshold levels that

cause a large object to vanish, (b) demonstrates object decomposition due to varying smoothing radii, and (c)

shows varying minimal object sizes that cause a small object to vanish.

40

Figure 2. Conceptual case demonstrating a potential effect of small changes in parameter value. Dark blue

squares indicate points with high intensity, while light blue coloring denotes an intensity value near the threshold

level. Obs. A yields the worst possible L2 = 1 score, while Obs. B is a perfect match with L2 = 0.

41

Figure 3. Map of the area shared by observational data and model output.

42

Figure 4. Case study: object decomposition for the convthresh algorithm for IR6.2 (19 January 2012, 03UTC).

With a smoothing parameter of 0 one large object is identiﬁed in the observations. Raising the smoothing radius

to 1 grid point causes the small interconnecting bridge in this object to vanish, which leads to decomposition

and vastly different S and L2 scores.

43

Figure 5. Case study: object decomposition for the threshfac algorithm for IR6.2 (21 January 2012, 12UTC).

For a threshold ratio of 0.9 one large object dominates the forecast. Raising the threshold ratio to 1 causes most

of the objects mass to fall below the threshold level. Effectively the object decomposes into many small ones,

which leads to vastly different S scores but nearly constant L2 scores.

44

Figure 6. Differences in (a) L2 and (b) S scores for IR6.2 with respect to changes in the threshold ratio (fac)

of the threshfac algorithm. The box-whiskers represent the score differences over the 400 spatial ﬁelds. Solid

boxes indicate the interquartile range, while the dashed lines reach out to the extremes.

45

00.20.40.60.810.00.20.40.60.81.0Absolute Difference of L2 ScoresDifference in Threshold Ratio (fac)(a)00.20.40.60.810.00.51.01.52.02.5Absolute Difference of S ScoresDifference in Threshold Ratio (fac)(b)Figure 7. Same as Fig. 6 but with respect to changes in the smoothing radius (smoothpar) of the convthresh

algorithm.

46

02468100.00.10.20.30.40.50.6Absolute Difference of L2 ScoresDifference in Smoothing Radius (smoothpar)(a)02468100.00.10.20.30.40.50.6Absolute Difference of S ScoresDifference in Smoothing Radius (smoothpar)(b)Figure 8. Same as Fig. 6 but with respect to changes in the minimum object size NContig) of the threshsizer

algorithm.

47

020040060080010000.000.020.040.060.080.10Absolute Difference of L2 ScoresDifference in Min. Object Size (NContig)(a)020040060080010000.000.010.020.030.040.05Absolute Difference of S ScoresDifference in Min. Object Size (NContig)(b)Figure 9. The Pearson correlation coefﬁcients between absolute changes in S and L2 scores for IR6.2 are

plotted against the normalized difference in parameter values. For the convthresh algorithm we only vary the

smoothing radius (smoothpar), for the threshsizer algorithm only the minimal object size (NContig) and for the

threshfac algorithm the threshold ratio (fac).

48

llllllllllllll0.20.40.60.81.00.20.40.60.81.0llllllllllllllllllllllllllllllllllllllllllllllllCorrelation of Absolute Score−DifferencesNormalized Change in Parameter ValueslllconvthreshthreshsizerthreshfacFigure 10. Sensitivity Indicators: (a) Univariate ECDF of 400 ﬁelds of IR6.2. For a given threshold ratio of
0.75 (black cross), varying threshold ratios ( f −
i ) and their resulting ECDF-values are marked with dashed
i
lines. Panel (b) shows L2 sensitivity against the a-posteriori sensitivity indicator SI, and (c) against the a-priori

, f +

indicator (cid:101)SI.

49

Threshold RatioCumulative Probability(a)00.250.50.7511.251.50.00.20.40.60.81.0++fi −fi +llllllllllllllll0.000.050.100.150.200.000.050.100.150.200.25A−Posteriori Sensitivity IndicatorL2 Sensitivity(b)llllllllllllllllllllllllllllllll0.00.20.40.60.81.00.000.050.100.150.200.25A−Priori Sensitivity IndicatorL2 Sensitivity(c)llllllllllllllllFigure 11. Density of (a) L2 and (b) S score of original and permuted TClC with the default threshold ratio

f = 1/15.

50

L2 − ScoreDensity00.20.40.60.810123456OriginalPermuted(a)S − ScoreDensity−2−10120.00.20.40.60.81.0OriginalPermuted(b)Figure 12. Density of (a) L2 and (b) S score of original and permuted IR6.2 with threshold ratio f = 0.85.

51

L2 − ScoreDensity00.20.40.60.810.00.51.01.52.02.53.03.5OriginalPermuted(a)S − ScoreDensity−2−10120.00.20.40.60.8OriginalPermuted(b)Figure 13. Density of MSE for IR6.2 data of forecast vs. original and randomly permuted observations. This

traditional score is able to distinguish both sets of data easily.

52

Mean Square ErrorDensity00.511.5201234OriginalPermuted