Seed, Expand and Constrain: Three Principles

for Weakly-Supervised Image Segmentation

Alexander Kolesnikov

Christoph H. Lampert

akolesnikov@ist.ac.at

chl@ist.ac.at

IST Austria

Abstract. We introduce a new loss function for the weakly-supervised
training of semantic image segmentation models based on three guiding
principles: to seed with weak location cues, to expand objects based on
the information about which classes can occur, and to constrain the seg-
mentations to coincide with image boundaries. We show experimentally
that training a deep convolutional neural network using the proposed
loss function leads to substantially better segmentations than previous
state-of-the-art methods on the challenging PASCAL VOC 2012 dataset.
We furthermore give insight into the working mechanism of our method
by a detailed experimental study that illustrates how the segmentation
quality is aﬀected by each term of the proposed loss function as well as
their combinations.

Keywords: Weakly-supervised image segmentation, deep learning

1 Introduction

6
1
0
2

 
r
a

 

M
9
1

 
 
]

V
C
.
s
c
[
 
 

1
v
8
9
0
6
0

.

3
0
6
1
:
v
i
X
r
a

Computer vision research has recently made tremendous progress. Many chal-
lenging vision tasks can now be solved with high accuracy, assuming that suf-
ﬁciently much annotated data is available for training. Collecting large labeled
datasets is time consuming and typically requires substantial ﬁnancial invest-
ments. Therefore, the creation of training data has become a bottleneck for the
further development of computer vision methods. However, large amounts of
unlabeled visual data can be collected in a relatively fast and cheap manner.
Therefore, a promising direction in the computer vision research is to develop
methods that can learn from unlabeled or partially labeled data.

In this paper we focus on the task of semantic image segmentation. Image
segmentation is a prominent example of an important vision task, for which
creating annotations is especially costly: as reported in [26,4], manually produc-
ing segmentation masks requires several worker-minutes per image. Therefore, a
large body of previous research studies how to train segmentation models from
weaker forms of annotation.

A particularly appealing setting is to learn image segmentation models using
training sets with only per-image labels, as this form of weak supervision can
be collected very eﬃciently. However, there is currently still a large performance
gap between models trained from per-image labels and models trained from

2

Alexander Kolesnikov and Christoph H. Lampert

full segmentations masks. In this paper we demonstrate that this gap can be
substantially reduced compared to the previous state-of-the-art techniques.

We propose a new composite loss function for training convolutional neural
networks for the task of weakly-supervised image segmentation. Our approach
relies on the following three insights:

– Recognition neural networks, such as AlexNet [16] or VGG [30], can be used
to generate reliable object localization cues (seeds), but fail to predict the
exact spatial extent of the objects. We incorporate this aspect by using
a semi-supervised loss that only encourages a segmentation network to
match local cues but that is agnostic about the rest of the image.

– To train a segmentation network from per-image annotation, a global pool-
ing layer can be used that aggregates segmentation masks into classiﬁcation
scores. The choice of this layer has large impact on the quality of segmenta-
tions. For example, max-pooling tends to underestimate the size of objects
while average-pooling tends to overestimate it [23]. We propose a global
weighted rank pooling that acts as a force that expands the seeds to
object regions of a reasonable size. It generalizes max-pooling and average
pooling and outperforms them in our empirical study.

– Networks trained from image-level labels rarely precisely capture boundaries
of objects in an image. Postprocessing by fully-connected conditional ran-
dom ﬁelds (CRF) at test time is often insuﬃcient to overcome this eﬀect,
because once the networks have been trained they tend to be conﬁdent even
about misclassiﬁed regions. We propose a new boundary-aware loss that
alleviates the problem of imprecise boundaries already at training time. It
strives to constrain predicted segmentation masks to respect low-level im-
age information, in particular object boundaries.

We name our approach SEC, as it is based on three principles: Seed, Expand
and Constrain. We formally deﬁne and discuss the individual components of
the SEC loss function in Section 3. In Section 4 we experimentally evaluate
it on the PASCAL VOC 2012 image segmentation benchmark, showing that
it substantially outperforms the previous state-of-the-art techniques under the
same experimental settings. We also provide further insight by discussing and
evaluating the eﬀect of each of our contributions separately through additional
experiments.

2 Related work

Semantic image segmentation, i.e. assigning a semantic class label to each pixel
of an image, is a topic of relatively recent interest in computer vision research,
as it required the availablity of modern machine learning techniques, such as
discriminative classiﬁers [28,5] or probabilistic graphical models [25,18]. As the
creation of fully annotated training data poses a major bottleneck to the further
improvement of these systems, weakly supervised training methods were soon
proposed in order to save annotation eﬀort. In particular, competitive methods

SEC: Three Principles for Weakly-Supervised Image Segmentation

3

were developed that only require partial segmentations [34,11] or object bound-
ing boxes [17,49,8] as training data.

A remaining challenge is, however, to learn segmentation models from just
image-level labels [32,33]. Existing approaches fall into three broad categories.
Graph-based models infer labels for segments or superpixels based on their sim-
ilarity within or between images [44,45,43,40,24]. Variants of multiple instance
learning [1] train with a per-image loss function, while internally maintaining
a spatial representation of the image that can be used to produce segmenta-
tion masks [35,36,37]. Methods in the tradition of self-training [27] train a fully-
supervised model but create the necessary pixel-level annotation using the model
itself in an EM-like procedure [41,42,46]. Our SEC approach contains aspects of
the latter two approaches, as it makes use of a per-image loss as well as per-pixel
loss terms.

In terms of segmentation quality, currently only methods based on deep con-
volutional networks [16,30] are strong enough to tackle segmentation datasets
of diﬃculty similar to what fully-supervised methods can handle, such as the
PASCAL VOC 2012 [9], which we make use of in this work. In particular, MIL-
FCN [22], MIL-ILP [23] and the approaches of [4] train deep networks in a
multiple instance learning setting, diﬀering mainly in their pooling strategies,
i.e. how they convert their internal spatial representation to per-image labels.
EM-Adapt [20] and CCNN [21] rely on the self-training framework and diﬀer
in how they enforce the consistency between the per-image annotation and the
predicted segmentation masks. SN B [38] adds additional steps for creating and
combining multiple object proposals. As far as possible, we provide an experi-
mental comparison to these methods in Section 4.

3 Weakly supervised segmentation from image-level

labels

In this section we present a technical description of our approach. We denote
the space of images by X . For any image X ∈ X , a segmentation mask Y is a
collection, (y1, . . . , yn), of semantic labels at n spatial locations. The semantic
labels belong to a set C = C(cid:48) ∪ {cbg} of size k, where C(cid:48) is a set of all foreground
labels and cbg is a background label. We assume that the training data, D =
{(Xi, Ti)}N
i=1, consists of N weakly annotated images, Xi ∈ X , and label sets,
Ti ⊂ C(cid:48), that list all foreground labels occurring in the image. Our goal is to
train a deep convolutional neural network f (X; θ), parameterized by θ, that
models the conditional probability of observing any label c ∈ C at any location
u ∈ {1, 2, . . . , n}, i.e. fu,c(X; θ) = p(yu = c|X). For brevity we will often omit
the parameters θ in our notation and write f (X; θ) simply as f (X).

3.1 The SEC loss for weakly supervised image segmentation

Our approach for learning the parameters, θ, of the segmentation neural network
relies on minimizing a loss function that has three terms. The ﬁrst term, Lclass,

4

Alexander Kolesnikov and Christoph H. Lampert

Fig. 1. A schematic illustration of SEC that is based on minimizing a composite
loss function consisting of three terms: semi-supervised loss, classiﬁcation loss and
boundary-aware loss. See Section 3 for details.

penalizes the predicted segmentation masks if they does not match the weak
supervision. The second term, Lsemi, provides localization hints to the network,
and the third term, Lbound, encourages segmentations that respect the spatial
and color structure of the images. Overall, we propose to solve the following
optimization problem for parameter learning:

(cid:88)

min

θ

(X,T )∈D

[Lclass(f (X; θ), T ) + Lsemi(f (X; θ), T ) + Lbound(X, f (X; θ))]

(1)

In the rest of this section we explain each loss term in detail. A schematic

overview of the setup can be found in Figure 1.

Classiﬁcation loss with global weighted rank pooling. To measure if a seg-
mentation mask is consistent with the image-level labels one can aggregate seg-
mentation scores into classiﬁcation scores and apply the standard loss function
for multi-label image classiﬁcation. In the context of weakly-supervised segmen-
tation/detection various techniques were used by researches to aggregate score
maps into a classiﬁcation scores. The most prominent ones are global max-poling
(GMP) [19] that assigns any class c in any image X a score of max
u∈{1,...,n} fu,c(X)
fu,c(X).

and global average-pooling [47] that assigns it a score of

n(cid:80)

u=1

1
n

Both ways of aggregation have been successfully used in practice. However,
they have their own drawbacks. For classes which are present in an image GMP
only encourages the response for a single location to be high, while GAP en-
courages all responses to be high. Therefore, GMP results in a segmentation
network that often underestimates the sizes of objects, while networks trained
using GAP, in contrast, often overestimate them. Our experiments in Section 4
support this claim empirically.

In order to overcome these drawbacks we propose a global weighted rank-
pooling (GWRP), a new aggregation technique, which can be seen as a general-
ization of GMP and GAP. GWRP computes a weighted average score for each

SegmentationCNNClassiﬁcationLossGlobalWeightedRank-PoolingCRFBoundary-awareLossSemi-supervisedLossWeaklocalizationPersonCowPersonCowPersonCowDownscalePersonCowPersonCowBackgr.SEC: Three Principles for Weakly-Supervised Image Segmentation

5

class, where weights are higher for more promising locations. This way it encour-
ages objects to occupy a certain fraction of an image, but, unlike GAP, is less
prone to overestimating object sizes.
Formally, let an index set I c = {i1, . . . , in} deﬁne the descending order of
prediction scores for any class c ∈ C, i.e. fi1,c(x) ≥ fi2,c(x) ≥ ··· ≥ fin,c(x)
and let dc > 0 be a decay parameter for class c. Then we deﬁne the GWRP
classiﬁcation scores, Gc(X, dc), for an image X, as following:

Gc(X; dc) =

(dc)j−1fij ,c(X), where Z(dc) =

(dc)j−1.

(2)

n(cid:88)

1

Z(dc)

j=1

n(cid:88)

j=1

Note, that for dc = 0 GWRP turns into GMP (adopting the convention that
00 = 1), and for dc = 1 it is identical to GAP. Therefore, GWRP generalizes
both approaches and the decay parameter can be used to interpolate between
the behavior of both extremes.

In principle, the decay parameter could be set individually for each class and
each image. However, this would need prior knowledge about how large objects
of each class typically are, which is not available in the weakly supervised setting.
Therefore, we only distinguish between three groups: for object classes that occur
in an image we use a decay parameter d+, for object classes that do not occur
we use d−, and for background we use dbg. We will discuss how to choose their
values in Section 4.

In summary, the classiﬁcation loss term is

(cid:88)

c∈T

(cid:88)

+
c∈C(cid:48)\T

log(1−Gc(X;d−))

|C(cid:48)\T|

Lclass(f (X), T ) =

logGc(X;d+)

|T|

+logGcbg (X;dbg). (3)

Semi-supervised loss with localization cues. Image-level labels do not
explicitly provide any information about the position of semantic objects in an
image. Nevertheless, as was noted in many recent research papers [19,47,29,3],
deep recognition networks that were trained just from image-level labels, may
be successfully employed to retrieve cues on object localization. We call this
procedure weak localization and illustrate it in Figure 2.

Unfortunately, localization cues typically are not precise enough to be used as
full and accurate segmentation masks. However, these cues can be very useful to
guide the weakly-supervised segmentation network. We propose to use a semi-
supervised loss to encourage predictions of the neural network to match only
“landmarks” given by the weak localization procedure while ignoring the rest of
the image. Suppose that Sc is a set of locations that are labeled with class c by
the weak localization procedure. Then, the semi-supervised loss Lsemi has the
following form:

Lsemi(f (X), T ) =

log fu,c(X)

(4)

(cid:88)

(cid:88)

c∈T

u∈Sc

1(cid:80)

c∈T

|Sc|

6

Alexander Kolesnikov and Christoph H. Lampert

Fig. 2. A schematic illustration of the weak localization procedure.

Note that for computing Lsemi one needs the weak localization sets, Sc, so
that many existing techniques from the literature can be used, essentially, as
black boxes. In this work, we rely on [47] for weakly localizing foreground classes.
However, this method does not provide a direct way to select conﬁdent back-
ground regions, therefore we use the gradient-based saliency detection method
from [29] for this purpose. We provide more details on the weak localization
procedure in Section 4.

Boundary-aware loss. The high level idea of the boundary-aware loss is to
penalize the neural network for producing segmentations that are discontinuous
with respect to spatial and color information in the input image. Thereby, it
encourages the neural network to learn to produce segmentation masks that
match up with object boundaries.

Speciﬁcally, we construct a fully-connected CRF, Q(X, f (X)), as in [15], with
unary potentials given by the probability scores predicted by the segmentation
network, and pairwise potentials of ﬁxed parametric form that depend only on
the image pixels. We downscale the image X, so that it matches the resolution of
the segmentation mask, produced by the network. More details about the choice
of the CRF parameters are given in Section 4. We then deﬁne the boundary-
aware loss as the mean KL-divergence between the outputs of the network and
the outputs of the CRF, i.e.:

n(cid:88)

(cid:88)

u=1

c∈C

Lbound(X, f (X)) =

1
n

Qu,c(X) log

Qu,c(X)
fu,c(X)

.

(5)

This construction achieves the desired eﬀect, since it encourages the network
output to coincide with the CRF output, which itself is known to produce seg-
mentation that respect image boundaries. An illustration of this eﬀect can be
seen in Figure 1.

3.2 Numerical Optimization

The proposed approach for weakly-supervised learning is end-to-end diﬀeren-
tiable. We leverage back-propagation technique to compute gradients automat-
ically, assuming that code for computing gradients of every layer of the neural

PersonCowBackgroundPersonCowBackgroundExtractLocalizationCuesfromClassiﬁcationNetworkPersonCowSEC: Three Principles for Weakly-Supervised Image Segmentation

7

network is available. For computing gradients of the fully-connected CRF we
employ the procedure from [31], which was successfully used in the context of
semantic image segmentation. Figure 1 illustrates the ﬂow of gradients for the
backpropagation procedure with gray arrows.

4 Experiments

In this section we validate our proposed loss function experimentally, including
a detailed study of the eﬀects of its diﬀerent terms.

4.1 Experimental setup

Dataset and evaluation metric. We evaluate our method on the PASCAL
VOC 2012 image segmentation benchmark, which has 21 semantic classes, in-
cluding background [9]. The dataset images are split into three parts: training
(train, 1464 images), validation (val, 1449 images) and testing (test, 1456 im-
ages). Following the common practice we augment the training part by addi-
tional images from [10]. The resulting trainaug set has 10,582 weakly annotated
images that we use to train our models. We compare our approach with other
approaches on both val and test parts. For the val part, ground truth segmenta-
tion masks are available, so we can evaluate results of diﬀerent experiments. We
therefore use this data also to provide a detailed study of the inﬂuence of the
diﬀerent components in our approach. The ground truth segmentation masks
for the test part are not publicly available, so we use the oﬃcial PASCAL VOC
evaluation server to obtain quantitative results. As evaluation measure we use
the standard PASCAL VOC 2012 segmentation metric: mean intersection-over-
union (mIoU).

Segmentation network. As a particular choice for the segmentation ar-
chitecture, in this paper we use DeepLab-CRF-LargeFOV from [6], which is a
slightly modiﬁed version of the 16-layer VGG network [30]. The network has in-
puts of size 321x321 and produces segmentation masks of size 41x41, see [6] for
more details on the architecture. We initialize the weights for the last (predic-
tion) layer randomly from a normal distribution with mean 0 and variance 0.01.
All other convolutional layers are initialized from the publicly available VGG
model [30]. Note, that in principle, our loss function can be combined with any
deep convolutional neural network.

Localization networks. The localization networks for the foreground classes
and the background class are also derived from the standard VGG architecture.
In order to improve the localization performance, we ﬁnetune these networks for
solving a multilabel classiﬁcation problem on the trainaug data. Due to space
limitations we provide exact details on these networks and optimization param-
eters in the supplementary material.

Optimization. For training the network we use the batched stochastic gra-
dient descent (SGD) with parameters used successfully in [6]. We run SGD for

8

Alexander Kolesnikov and Christoph H. Lampert

Fig. 3. A schematic illustration of our approach at test time.

8000 iterations, the batch size is 15 (reduced from 30 to allow simultaneous train-
ing of two networks), the dropout rate is 0.5 and the weight decay parameter
is 0.0005. The initial learning rate is 0.001 and it is decreased by a factor of 10
every 2000 iterations. Overall, training on a GeForce TITAN-X GPU takes 7-8
hours, which is comparable to training times reported in [20,21].

Decay parameters. The GWRP aggregation requires specifying the decay
parameters, d−, d+ and dbg, that control the weights for aggregating the scores
produced by the network. We do so using the following rules-of-thumb:

– for semantic classes that are not present in the image we want to predict as
few pixels as possible. Therefore, we set d− = 0, which corresponds to GMP.
– for semantic classes that are present in the image we suggest that the top
10% scores represent 50% of the overall aggregated score. For our 41x41
masks this roughly corresponds to d+ = 0.996.

– for the background we suggest that the top 30% scores represent 50% of the

overall aggregated score, resulting in dbg = 0.999.

Fully-connected CRF at training time. In order to enforce the segmen-
tation network to respect the boundaries of objects already at training time we
use a fully-connected CRF [15]. As parameters for the pairwise interactions, we
use the default values from the authors’ public implementation, except that we
multiply all spatial distance terms by 12 to reﬂect the fact that we downscaled
the original image in order to match the size of the predicted segmentation mask.
Inference at test time. Our segmentation neural network is trained to
produce probability scores for all classes and locations, but the spatial resolution
of a predicted segmentation mask is lower than the original image. Thus, we
upscale the predicted segmentation mask to match the size of the input image,
and then apply a fully-connected CRF [15] to reﬁne the segmentation. This is
a common practice, which was previously employed, e.g., in [20,21,6]. Figure 3
shows a schematic illustration of our inference procedure at test time.

Reproducibility. In our experiments we rely on the caﬀe deep learning
framework [13] in combination with a python implementation of the SEC loss.
The source code of SEC and pretrained models will soon be publicly available.

4.2 Results

Numeric Results. Table 1 compares the performance of our weakly supervised
approach with previous approaches that are trained in the same setup, i.e. using
only images from PASCAL VOC 2012 and only image-level labels. It shows that

SegmentationCNNCRFUpscaleSEC: Three Principles for Weakly-Supervised Image Segmentation

9

Table 1. Results on PASCAL VOC 2012 (mIoU in %) for weakly-supervised semantic
segmentation with only per-image labels.

)
j
b
O
+
g
m

I
(

]
4
[

)
1
e
g
a
t
s
(

]
4
1
[

)
]
1
2
[

t
p
a
d
A
M
E

-

f
o

.
l

p
m

i
-
e
r
(

]
3
2
[

†
P
L
I
+
L
I
M

l
x
p
p
s
-
P
S
+

)
d
e
s
o
p
o
r
p
(
C
E
S

]
1
2
[

N
N
C
C

71.7∗ 67.2 68.5 77.2 82.2
30.7∗ 29.2 25.5 37.3 61.7
30.5∗ 17.6 18.0 18.4 26.0
26.3∗ 28.6 25.4 25.4 60.4
20.0∗ 22.2 20.2 28.2 25.6
24.2∗ 29.6 36.3 31.9 45.6
39.2∗ 47.0 46.8 41.6 70.9
33.7∗ 44.0 47.1 48.1 63.2
50.2∗ 44.2 48.0 50.7 72.2
17.1∗ 14.6 15.8 12.7 20.9
29.7∗ 35.1 37.9 45.7 52.9
22.5∗ 24.9 21.0 14.6 30.6
41.3∗ 41.0 44.5 50.9 62.8
35.7∗ 34.8 34.5 44.1 56.8
43.0∗ 41.6 46.2 39.2 63.5
36.0∗ 32.1 40.7 37.9 57.1
29.0∗ 24.8 30.4 28.3 32.2
34.9∗ 37.4 36.3 44.0 60.6
23.1∗ 24.0 22.2 19.6 32.3
33.2∗ 38.1 38.8 37.6 44.8
33.2∗ 31.6 36.9 35.0 42.3
32.2∗ 33.6∗ 33.8 35.3 36.6 50.7

PASCAL
VOC 2012

val set

background
aeroplane

bike
bird
boat
bottle

bus
car
cat
chair
cow

diningtable

dog
horse

motorbike

person
plant
sheep
sofa
train

tv/monitor

average

PASCAL
VOC 2012
test set

background
aeroplane

bike
bird
boat
bottle

bus
car
cat
chair
cow

diningtable

dog
horse

motorbike

person
plant
sheep
sofa
train

tv/monitor

average

]
3
2
[

†
P
L
I
+
L
I
M

l
x
p
p
s
-
P
S
+

)
d
e
s
o
p
o
r
p
(
C
E
S

]
1
2
[

N
N
C
C

]
2
2
[

N
C
F
-
L
I
M

≈71‡ 74.7 83.0
24.2 38.8 55.6
19.9 19.8 27.4
26.3 27.5 61.1
18.6 21.7 22.9
38.1 32.8 52.4
51.7 40.0 70.2
42.9 50.1 58.8
48.2 47.1 70.0
15.6 7.2 22.1
37.2 44.8 54.3
18.3 15.8 27.9
43.0 49.4 67.4
38.2 47.3 59.4
52.2 36.6 70.7
40.0 36.4 59.0
33.8 24.3 38.7
36.0 44.5 58.6
21.6 21.0 38.1
33.4 31.5 37.6
38.3 41.3 45.2
25.7 35.6 35.8 51.5

(∗ results from unpublished/not peer-reviewed manuscripts, †trained on ImageNet, ‡value inferred from average)

Table 2. Summary results (mIoU %) for other methods on PASCAL VOC 2012. Note:
the values in this table are not directly comparable to Table 1, as they were obtained
under diﬀerent experimental conditions.

test comments

method
DeepLab [6]
STC [39]
TransferNet [12]

val
67.6 70.3 fully supervised training
49.8∗ 51.2∗ trained on Flickr
52.1∗ 51.2∗ trained on MS COCO; additional supervision:
35.1∗
[4] (1Point)
[4] (AllPoints-weighted) 43.4∗
49.1∗
[4] (squiggle)
38.2 39.6 uses weak labels of multiple image crops
EM-Adapt [20]
41.9 43.2 uses MCG region proposals (see text)
SN B [38]
MIP+ILP+SP-seg [23]
42.0 40.6 trained on ImageNet, MCG proposals (see text)
37.8 37.0 trained on ImageNet, BING proposals (see text)
MIL+ILP+SP-bb [23]
( ∗results from manuscripts that are currently unpublished/not peer-reviewed)

from segmentation mask of other classes
additional supervision: 1 click per class
additional supervision: 1 click per instance
additional supervision: 1 squiggle per class

–
–
–

10

Alexander Kolesnikov and Christoph H. Lampert

SEC substantially outperforms the previous techniques when trained under the
same conditions. On the test data, where the evaluation is performed by an
independent third party, the PASCAL VOC evaluation server, it achieves 15.7%
higher mean intersection-over-union score than the state-of-the-art approaches
with new best scores on all 21 semantic classes. On the validation data, for which
researchers can compute scores themselves, SEC improves over the state-of-the-
art by 14.1%, and achieves new best scores on 20 out of the 21 classes.

Results of other weakly-supervised methods on PASCAL VOC and the fully-
supervised variant of DeepLab are summarized in Table 2. We provide these
results for reference but emphasize that they should not simply be compared to
Table 1, because the underlying methods were trained on diﬀerent (and larger)
training sets or were given additional forms of weak supervision, e.g. user clicks.
Some entries need further explanation in this regard: [20] reports results for the
EM-Adapt model when trained with weak annotation for multiple image crops.
The same model was reimplemented and trained with only per-image supervi-
sion in [21], so these are the values we report in Table 1. The results reported
for SN B [38] and the seg variant of the MIL+ILP+SP [23] are incomparable
to others because they were obtained with help of MCG region proposals [2]
that were trained in a fully supervised way on PASCAL VOC data. Similarly,
MIL+ILP+SP-bb makes use of bounding box proposals generated by the BING
method [7] that was trained using PASCAL VOC bounding box annotation.

Note that we do include the sppxl variant of MIL+ILP+SP in Table 1. While
it is trained on roughly 760.000 images of the ImageNet dataset, we do not con-
sider this an unfair advantage compared to other methods, because those implic-
itly beneﬁt from ImageNet images as well when using pretrained classiﬁcation
networks for initialization.

Qualitative Results. Figure 4 illustrates typical successful segmentations.
It shows that our method can produce accurate segmentations even for non-
trivial images and recover ﬁne details of the boundary. Figure 5 illustrates some
failure cases. As is typical for weakly-supervised systems, SEC has problems
segmenting objects that occur almost always in front of the same background,
e.g. boats on water, or trains on tracks. A second failure mode is that object
regions can be segmented correctly, but assigned wrong class labels. This is
actually quite rare for SEC, which we attribute to the fact that the DeepLab
network has a large ﬁeld-of-view and therefore can make use of the full image
when assigning labels. Finally, it can also happen that segmentations cover only
parts of objects. This is likely due to imperfections of the weak localization cues
that tend to reliably detect only the most discriminative parts of an object,
e.g. the face of a person. This might not be suﬃcient to segment the complete
object, however, especially when objects overlap each other or consist of multiple
components of very diﬀerent appearance.

4.3 Detailed Discussion

To provide additional insight into the working mechanisms of the SEC loss func-
tion, we performed two further sets of experiments on the val data. First, we

SEC: Three Principles for Weakly-Supervised Image Segmentation

11

Fig. 4. Examples of predicted segmentations (val set, successfull cases)

Fig. 5. Examples of predicted segmentations (val set, failure cases)

analyze diﬀerent global pooling strategies, and second, we perform an ablation
study that illustrates the eﬀect of each of the three terms in the proposed loss
function visually as well as numerically.

Eﬀect of global pooling strategies. As discussed before, the quality of
segmentations depends on which global pooling strategy is used to convert seg-
mentation mask into class labels. To quantify this eﬀect, we train three seg-
mentation networks from weak supervision, using either GMP, GAP or GWRP
as aggregation methods for all classes that are present in the image. For classes
that are not present we always use GMP, i.e. we penalize any occurrence of these

GroundTruthImagePredictionGroundTruthImagePredictionGroundTruthImagePredictionGroundTruthImagePrediction12

Alexander Kolesnikov and Christoph H. Lampert

classes. In Figure 6 we demonstrate visual results for every pooling strategy and
report two quantities: the fraction of pixels that are predicted to belong to a
foreground (fg) class, and the segmentation performance as measured by mean
IoU. We observe that GWRP outperforms the other method in terms of seg-
mentation quality and the fractions of predicted foreground pixels supports our
earlier hypothesis: the model trained with GMP tends to underestimate object
sizes, while the model trained with with GAP on average overestimates them.
In contrast, the model trained with GWRP, produces segmentations in which
objects are, on average, close to the correct size1.

Eﬀect of the diﬀerent loss terms. To investigate the contribution of
each term in our composite loss function we train segmentation networks with
loss functions in which diﬀerent terms of the SEC loss were omitted. Figure 7
provides numerical results and illustrates typical segmentation mistakes that
occur when certain loss terms are omitted. Best results are achieved when all
three loss terms are present. However, the experiments also allow us to draw to
interesting additional conclusions about the interaction between the loss terms.
Semi-supervised loss and large ﬁeld-of-view. First, we observe that
having Lsemi in the loss function is crucial to achieve competitive performance.
Without this loss term our segmentation network fails to reﬂect the localization
of objects in its predictions, even though the network does match the global
label statistics rather well. See the fourth column of Figure 7 for the illustration
of this eﬀect.

We believe that this eﬀect can be explained by the large (378x378) ﬁeld-of-
view of the segmentation network2: if an object is present in an image, then the
majority of the predicted scores may be inﬂuenced by this object, no matter
where object is located. This helps in predicting the right class labels, but can
negatively aﬀect the localization ability. Other researchers addressed this prob-
lem by explicitly changing the architecture of the network in order to reduce its
ﬁeld-of-view [20]. However, networks with a small ﬁeld-of-view are less powerful
and often fail to recognize which semantic labels are present on an image. We
conduct an additional experiment (see Supplementary material for details) that
conﬁrm that SEC with a small (211x211) ﬁeld-of-view network performs clearly
worse than with the large (378x378) ﬁeld-of-view network, see Figure 8 for nu-
meric results and visual examples. Thus, we conclude that the semi-supervised
loss provides the necessary localization guidance that enables the large ﬁeld-of-
view network to still reliably localize objects.

Interaction of the classiﬁcation loss and the boundary-aware losses.
We observe an interesting eﬀect when adding the classiﬁcation or the boundary-
aware loss to the semi-supervised loss. When added individually, neither of both
losses helps to improve the overall performance. When added together, however,
they lead to a measurable improvement. We explain this observation by the

1 Note that these experiments were done after the network architecture and parameters

were ﬁxed. In particular, we did not tune the decay parameters for this eﬀect.

2 We report the theoretical ﬁelds-of-view inferred from the network architecture. The

empirical ﬁeld-of-view that is actually used by the network can be smaller [48].

SEC: Three Principles for Weakly-Supervised Image Segmentation

13

pooling
fg mIoU
method fraction (val)
47.3
GMP
GAP
45.1
50.7

21.0
37.5
GWRP 26.7
ground
truth

27.1

–

Fig. 6. Results on the val set and examples of segmentation masks for models trained
with diﬀerent pooling strategies.

characteristic eﬀects of both losses. By construction, the boundary-aware loss
encourages nearby regions of similar color to have the same label. However, this
is often not enough to turn the weak localization cues into segmentation masks
that cover a whole object, especially if the object consists of visually dissimilar
parts, such as people wearing clothes of diﬀerent colors. See the third column of
Figure 7 (in particular rows 2 to 4) for an illustration of this eﬀect.

The classiﬁcation loss, based on GWRP, suppresses the prediction of classes
that are not meant to be in the image, and it encourages classes that are in the
image to have reasonable sizes. When combined with the semi-supervised loss,
the classiﬁcation loss actually results in a drop in performance. The ﬁfth column
of Figure 7 shows an explanation of this: objects sizes are generally increased,
but the additionally predicted regions do not match the image boundaries.

In combination, the semi-supervised loss provides reliable seed locations, the
classiﬁcation loss acts as a force to enlarge the segmentation masks to a reason-
able size, and the boundary-aware loss constrains the segmentation mask to line
up with image boundaries, thus integrating low-level image information. The
result are substantially improved segmentation masks as illustrated in the last
column of Figure 7.

5 Conclusion

We proposed a new loss function for training deep segmentation networks when
only image-level labels are available. We demonstrate that our approach outper-
forms previous state-of-the-art methods by a large margin when used under the
same experimental conditions. Additionally, we provided a detailed study that
sheds light on the reasons why our approach is successful.

We also identify potential directions that may help to further improve weakly-
supervised segmentation performance. Our experiments show that knowledge
about object sizes can dramatically improve the segmentation performance. SEC
readily allows incorporating size information through decay parameters, but a

GroundTruthImageGMPGAPGWRP14

Alexander Kolesnikov and Christoph H. Lampert

loss mIoU
function (val)
27.8
49.2

Lclass
Lsemi
Lsemi+
Lbound
Lclass+
Lbound
Lsemi+
Lclass
all terms 50.7

45.7

49.4

17.2

Fig. 7. Results on the val set and examples of segmentation masks for models trained
with diﬀerent loss functions.

ﬁeld mIoU
of view (val)
211x211 38.1
378x378 50.7

Fig. 8. Results on the val set and examples of segmentation masks for models with
small or large ﬁeld-of-views.

procedure for estimating object sizes automatically would be desirable. A second
way to improve the performance would be stronger segmentation priors, for ex-
ample about shape or materials. This could oﬀer a way to avoid mistakes that are
currently typical for weakly-supervised segmentation networks, including ours,
for example that boats are confused with the water in their background.

References

1. Andrews, S., Tsochantaridis, I., Hofmann, T.: Support vector machines for

multiple-instance learning. In: NIPS (2002)

2. Arbel´aez, P., Pont-Tuset, J., Barron, J., Marques, F., Malik, J.: Multiscale combi-

natorial grouping. In: CVPR (2014)

3. Bazzani, L., Bergamo, A., Anguelov, D., Torresani, L.: Self-taught object localiza-

tion with deep networks. In: WACV (2016)

4. Bearman, A., Russakovsky, O., Ferrari, V., Fei-Fei, L.: What’s the point: Semantic

segmentation with point supervision. arXiv preprint arXiv:1506.02106 (2015)

5. Carreira, J., Sminchisescu, C.: CPMC: Automatic object segmentation using con-

strained parametric min-cuts. IEEE T-PAMI 34(7) (2012)

GroundTruthImageLsemi+LboundLclass+LboundLclass+LsemiFullLossGroundTruthImageSmallFOVLargeFOVSEC: Three Principles for Weakly-Supervised Image Segmentation

15

6. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Semantic
image segmentation with deep convolutional nets and fully connected CRFs. In:
ICLR (2015)

7. Cheng, M.M., Zhang, Z., Lin, W.Y., Torr, P.H.S.: BING: Binarized normed gradi-

ents for objectness estimation at 300fps. In: CVPR (2014)

8. Dai, J., He, K., Sun, J.: BoxSup: Exploiting bounding boxes to supervise convolu-

tional networks for semantic segmentation. In: ICCV (2015)

9. Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The

PASCAL visual object classes (VOC) challenge. IJCV 88(2) (2010)

10. Hariharan, B., Arbelaez, P., Bourdev, L., Maji, S., Malik, J.: Semantic contours

from inverse detectors. In: ICCV (2011)

11. He, X., Zemel, R.S.: Learning hybrid models for image annotation with partially

labeled data. In: NIPS (2009)

12. Hong, S., Oh, J., Han, B., Lee, H.: Learning transferrable knowledge for se-
mantic segmentation with deep convolutional neural network. arXiv preprint
arXiv:1512.07928v1 (2015), http://arxiv.org/abs/1512.07928

13. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadar-
rama, S., Darrell, T.: Caﬀe: Convolutional architecture for fast feature embedding.
arXiv preprint arXiv:1408.5093v1 (2014)

14. Kim, H., Hwang, S.: Scale-invariant feature learning using deconvolutional
neural networks for weakly-supervised semantic segmentation. arXiv preprint
arXiv:1602.04984v2 (2016), http://arxiv.org/abs/1602.04984v2

15. Kr¨ahenb¨uhl, P., Koltun, V.: Eﬃcient inference in fully connected CRFs with gaus-

sian edge potentials. In: NIPS (2011)

16. Krizhevsky, A., Sutskever, I., Hinton, G.E.: ImageNet classiﬁcation with deep con-

volutional neural networks. In: NIPS (2012)

17. Liu, S., Yan, S., Zhang, T., Xu, C., Liu, J., Lu, H.: Weakly supervised graph

propagation towards collective image parsing. IEEE T-MM 14(2) (2012)

18. Nowozin, S., Gehler, P.V., Lampert, C.H.: On parameter learning in CRF-based

approaches to object class image segmentation. In: ECCV (2010)

19. Oquab, M., Bottou, L., Laptev, I., Sivic, J.: Is object localization for free? – weakly-
supervised learning with convolutional neural networks. In: CVPR. pp. 685–694
(2015)

20. Papandreou, G., Chen, L.C., Murphy, K.P., Yuille, A.L.: Weakly- and semi-
supervised learning of a deep convolutional network for semantic image segmenta-
tion. In: ICCV (2015)

21. Pathak, D., Kr¨ahenb¨uhl, P., Darrell, T.: Constrained convolutional neural networks

for weakly supervised segmentation. In: ICCV (2015)

22. Pathak, D., Shelhamer, E., Long, J., Darrell, T.: Fully convolutional multi-class

multiple instance learning. In: ICLR (2015)

23. Pinheiro, P.O., Collobert, R.: From image-level to pixel-level labeling with convo-

lutional networks. In: CVPR (2015)

24. Pourian, N., Karthikeyan, S., Manjunath, B.: Weakly supervised graph based se-

mantic segmentation by learning communities of image-parts. In: CVPR (2015)

25. Rabinovich, A., Vedaldi, A., Galleguillos, C., Wiewiora, E., Belongie, S.: Objects

in context. In: ICCV (2007)

26. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet large
scale visual recognition challenge. IJCV 115(3) (2015)

27. Scudder, H.J.: Probability of error of some adaptive pattern-recognition machines.

IEEE T-IT 11(3) (1965)

16

Alexander Kolesnikov and Christoph H. Lampert

28. Shotton, J., Winn, J., Rother, C., Criminisi, A.: Textonboost: Joint appearance,
shape and context modeling for multi-class object recognition and segmentation.
In: ECCV (2006)

29. Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks:

Visualising image classiﬁcation models and saliency maps. In: ICLR (2014)

30. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale

image recognition. In: ICLR (2015)

31. Toyoda, T., Hasegawa, O.: Random ﬁeld model for integration of local information

and global information. IEEE T-PAMI 30(8) (2008)

32. Vasconcelos, M., Vasconcelos, N., Carneiro, G.: Weakly supervised top-down image

segmentation. In: CVPR (2006)

33. Verbeek, J., Triggs, B.: Region classiﬁcation with Markov ﬁeld aspect models. In:

CVPR (2007)

34. Verbeek, J., Triggs, W.: Scene segmentation with CRFs learned from partially

labeled images. In: NIPS (2008)

35. Vezhnevets, A., Buhmann, J.M.: Towards weakly supervised semantic segmenta-

tion by means of multiple instance and multitask learning. In: CVPR (2010)

36. Vezhnevets, A., Ferrari, V., Buhmann, J.M.: Weakly supervised semantic segmen-

tation with a multi-image model. In: ICCV (2011)

37. Vezhnevets, A., Ferrari, V., Buhmann, J.M.: Weakly supervised structured output

learning for semantic segmentation. In: CVPR (2012)

38. Wei, Y., Liang, X., Chen, Y., Jie, Z., Xiao, Y., Zhao, Y., Yan, S.: Learning to

segment with image-level annotations. Pattern Recognition (2016)

39. Wei, Y., Liang, X., Chen, Y., Shen, X., Cheng, M., Zhao, Y., Yan, S.: STC: a
simple to complex framework for weakly-supervised semantic segmentation. arXiv
preprint arXiv:1509.03150v1 (2015), http://arxiv.org/abs/1509.03150

40. Xie, W., Peng, Y., Xiao, J.: Weakly-supervised image parsing via constructing

semantic graphs and hypergraphs. In: Multimedia (2014)

41. Xu, J., Schwing, A.G., Urtasun, R.: Tell me what you see and I will show you

where it is. In: CVPR (2014)

42. Xu, J., Schwing, A.G., Urtasun, R.: Learning to segment under various forms of

weak supervision. In: CVPR (2015)

43. Zhang, L., Gao, Y., Xia, Y., Lu, K., Shen, J., Ji, R.: Representative discovery
of structure cues for weakly-supervised image segmentation. IEEE T-MM 16(2)
(2014)

44. Zhang, L., Song, M., Liu, Z., Liu, X., Bu, J., Chen, C.: Probabilistic graphlet
cut: Exploiting spatial structure cue for weakly supervised image segmentation.
In: CVPR (2013)

45. Zhang, L., Yang, Y., Gao, Y., Yu, Y., Wang, C., Li, X.: A probabilistic associative

model for segmenting weakly supervised images. IEEE T-IP 23(9) (2014)

46. Zhang, W., Zeng, S., Wang, D., Xue, X.: Weakly supervised semantic segmentation

for social images. In: CVPR (2015)

47. Zhou, B., Khosla, A., A., L., Oliva, A., Torralba, A.: Learning deep features for

discriminative localization. In: CVPR (2016)

48. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Object detectors

emerge in deep scene CNNs. In: ICLR (2015)

49. Zhu, J., Mao, J., Yuille, A.L.: Learning from weakly supervised data by the expec-

tation loss SVM (e-SVM) algorithm. In: NIPS (2014)

SEC: Three Principles for Weakly-Supervised Image Segmentation

17

6 Supplementary material

In this section we provide technical details of the weak localization procedure
(section 3.1) and specify how the network with the small ﬁeld-of-view was derived
(section 4.3).

6.1 Localization networks

The semi-supervised loss SEC relies on weak localization cues. As was noted in
many recent papers [19,47,29,3], localization cues may be produced by leveraging
a deep convolutional neural network that is trained for solving an image classi-
ﬁcation task. We use two diﬀerent approaches, one for localizing the foreground
classes and the other for the background class.

Speciﬁcally, for localizing the foreground classes we employ the technique
from [47]. As an underlying classiﬁcation network we use the standard VGG
network, initialized from the publicly available model [30]. The VGG architecture
is slightly modiﬁed in order to make the methodology from [47] applicable. In
particular, we implement the following changes into VGG:

– the last two fully-connected layers, fc6 and fc7, are substituted with ran-
domly initialized convolutional layers, which have 1024 output channels and
kernels of size 3.

– the output of the last convolutional layer is followed by a global average
pooling layer and then by a fully-connected prediction layer with 20 outputs
(the number of foreground semantic classes in PASCAL VOC)

Additionally, in order to increase the spatial resolution of the last convolutional
layer of the network, we increase the input size to 321x321 and omit the two last
max-pooling layers, pool 4 and pool 5. The resulting network is ﬁnetuned with
multilabel logistic loss on the trainaug part of the PASCAL VOC 2012 dataset
(we use the same optimization parameters as in Section 4). Then the network is
used to provide class-speciﬁc localization heat maps. In order to produce local-
ization cues for every foreground class we threshold the corresponding heat map
by 20% of its maximum value, as was suggested in [47]. The resulted localization
cues are stacked together into a single weak localization mask, as shown on Fig-
ure 2. It may happen that localization cues are in conﬂict, when diﬀerent classes
are assigned to the same location. We use simple rule to resolve these conﬂicts:
during the stacking process classes that occupy a smaller fraction of an image
have priority over classes that occupy a bigger fraction of an image.

For localizing background we rely on the alternative technique from [29]. We
also use the VGG as the underlying network. It is modiﬁed to have input resolu-
tion of size 321x321 and to have a prediction layer with 20 outputs. Analogously
to [6], we also change the number of output channels in the fully-connected lay-
ers, fc6 and fc7, from 4096 to 1024. We ﬁnetune the network using the same
procedure as for the network for localizing the foreground classes. Following the
gradient-based procedure from [29], we utilize the ﬁnetuned network to produce
class-independent saliency maps. Finally, 10% of the least salient locations in
each image are selected as background cues.

18

Alexander Kolesnikov and Christoph H. Lampert

6.2 Small ﬁeld-of-view

The Deeplab-Large-FOV neural network achieves particularly wide ﬁeld-of-view
by utilizing convolutions with “holes”, which were recently suggested in the con-
text of semantic image segmentation in [20]. In order to derive the closest archi-
tecture, but with a small ﬁeld-of-view, we substitute convolutions with “holes”
by the standard convolutional layers. This leads to a measurable drop in the size
of the ﬁeld-of-view: from 378x378 to 211x211.

SEC: Three Principles for Weakly-Supervised Image Segmentation

19

Fig. 9. Random examples of segmentations produced by SEC (val set, part 1)

GroundTruthImagePredictionImagePredictionGroundTruth20

Alexander Kolesnikov and Christoph H. Lampert

Fig. 10. Random examples of segmentations produced by SEC (val set, part 2)

GroundTruthImagePredictionImagePredictionGroundTruth