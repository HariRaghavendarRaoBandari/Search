6
1
0
2

 
r
a

 

M
4
2

 
 
]

V
C
.
s
c
[
 
 

2
v
6
7
0
1
0

.

3
0
6
1
:
v
i
X
r
a

What is the right way to represent document images?

Gabriela Csurka∗, Diane Larlus, Albert Gordo, Jon Almaz´an

ﬁrstname.lastname@xrce.xerox.com

Xerox Research Centre Europe (XRCE), Grenoble, France

Abstract

In this article we explore and compare diﬀerent ways to represent document
images using visual cues. We propose a comprehensive experimental study
that covers the three main types of visual document image representations:
(1) traditional so-called shallow features, such as the run-length and the
Fisher-vector descriptors, (2) deep features based on convolutional neural
networks, and (3) features extracted from hybrid architectures that take
inspiration from the two previous ones.

We evaluate these features on multiple tasks, including document classi-
ﬁcation, clustering, and retrieval, and in several domain transfer scenarios,
using a large variety of public and in-house datasets. Our results show that
deep features generally outperform other types of features when there is no
domain shift or when the new task is closely related to the one considered
at train time. However, in presence of a large domain or task shift, docu-
ment images are best described by Fisher-vector-based shallow features that
generalize well and often obtain the best results.

Keywords: Document image representation, visual features, deep features,
feature transfer, benchmark

1. Introduction

In this work we focus on the problem of document image representation.
Given images of documents, we are interested in best representing these im-
ages to perform several prediction tasks related to document understanding
such as classiﬁcation, retrieval, clustering, etc. Document understanding is

∗Corresponding author.

Preprint submitted to Pattern Recognition

March 25, 2016

a key aspect of several applications including digital mail-room scenarios,
where the content of the documents is used to route incoming documents
to the right work-ﬂow, extract relevant data, annotate the documents with
additional information such as priority or relevance, etc.

Traditionally, there has been three main cues that are taken into ac-
count when representing and understanding a document image: visual cues,
structural cues, and textual cues [1]. The visual cues describe the overall
appearance of the document, and capture the information that would allow
one to diﬀerentiate documents “at a glance”. The structural cues explicitly
capture the relation between the diﬀerent elements of the documents, for ex-
ample by performing a layout analysis and encoding the diﬀerent regions in
a graph. Although visual descriptors can capture similar information implic-
itly, the representations based on structural cues focus on capturing them in
an explicit manner. Finally, textual cues capture the actual textual content
of the document, which can contain important semantic information.

In many cases, these cues contain complementary information, and their
combined use would be desired. Unfortunately, obtaining structural and
textual features is usually computationally expensive, and these costs be-
come prohibitive in large scale domains. For example, structural features
usually require a layout analysis of the document, which is slow and error-
prone. Similarly, textual cues usually require to perform optical character
recognition (OCR) on the entire document, which is once again slow and
error-prone. Moreover, these two kinds of features are very domain-speciﬁc,
and, in general, do not transfer well between diﬀerent domains and tasks.

On the other hand, visual features are usually fast to extract while being
quite generic. This has motivated their use in many document understanding
works [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]. Although not as expressive
as pure structural features, visual features can typically encode some coarse
structure of the image, while textual information can be added as an addi-
tional step depending on the speciﬁc domain [15].

Motivated by their success and advantages, in this work we focus on visual
features for document representation, and propose a comprehensive experi-
mental study. We compare handcrafted, shallow features with more recent,
learned features based on deep learning. In particular, although deep fea-
tures have shown outstanding performance in many computer vision tasks,
only a few works have focused on learning features for document images using
convolutional networks. They have showed promising results and have out-
performed shallow visual features in classiﬁcation and retrieval tasks [16, 17],

2

yet their comparison with other shallow methods has been limited. This re-
cent shift towards deep learning in document image understanding raises two
questions: ﬁrst, given a task and a dataset, do these deep methods outper-
form shallow features in all the cases? Second, how well do they transfer to
diﬀerent domains (i.e. datasets) and to diﬀerent tasks, if one wants to reduce
their training cost by reusing a pre-trained representation or model? These
crucial questions have not been addressed in detail yet.

Additionally, some hybrid architectures have recently been proposed for
natural image classiﬁcation [18]. Built on top of shallow features, they also
include several layers that allow them to be trained end-to-end similarly to
deep models. The underlying motivation is to combine the advantages of
shallow features (faster training and good generalization) with the expres-
siveness of deep architectures. In this paper we propose to evaluate them in
the context of document image understanding in comparison with shallow
features and deep convolutional networks.

Our contribution is therefore threefold. First, we benchmark several stan-
dard features against diﬀerent ﬂavors of recently proposed deep features on
a document classiﬁcation task. Second, we explore hybrid architectures as
an appealing comprise between reusable but weaker shallow features, and
specialized but high-performing deep features. Third, we evaluate the trans-
ferability of all these features across domains and across tasks.

Accordingly, this article is organized as follows. In Section 2, we review
related work. Section 3 describes the diﬀerent feature representations that
we consider for this work. Section 4 details the training procedure with and
without domain shift. Section 5 describes the datasets used in our bench-
mark. The comprehensive experimental study is reported in Section 6 and is
divided into two parts. The ﬁrst set of experiments (Section 6.2) compares
all the features with a standard protocol and tackles document image classi-
ﬁcation. The second set of experiments (Section 6.3) studies transferability
of the features across diﬀerent datasets and diﬀerent tasks. Finally, Section
7 concludes our study.

2. Related work

Traditional visual features for document images usually rely on simple
statistics computed directly from the image pixels. For example, Heroux et
al . [3] propose a multi-scale density decomposition of the page to produce

3

ﬁxed-length descriptors constructed eﬃciently from integral images. A simi-
lar idea is presented by Reddy and Govindaraju [19], where representations
based on low-level pixel density information are classiﬁed using adaptive
boosting. Cullen et al . [2] use a combination of features including densities
at interest points, histograms of the size and the density of the connected
components and vertical projection histograms. Bagdanov and Worring [5]
propose a representation based on density changes obtained with diﬀerent
morphological operations, while Sarkar [6] describes document images as a
list of salient Viola-Jones based features. Joutel et al . [20] use curvelets to
capture information about handwritten strokes in the image, tailored to the
speciﬁc task of retrieving images with similar handwriting styles. However,
in general, all these traditional features contain relatively limited amount of
information and while they might perform well on a speciﬁc dataset for the
task they were designed for, they are not generic enough to handle other
document class types, datasets and tasks. Some more elaborate representa-
tions, such as the run-length histograms [7, 8, 12], have shown to be more
generic and hence better suited for document image representation. More-
over, many of these representations can be combined with spatial pyramids
[21] to explicitly add a coarse structure, leading to higher accuracies at the
cost of higher-dimensional representations.

On a diﬀerent line of research, some recent works [13, 15, 22, 23] have
drawn inspiration from representations typically used for natural images, and
have shown that image features such as the bag-of-visual-words (BoV) [24] or
the Fisher-vector [25] built on top of densely-extracted local descriptors such
as SIFT [26] or SURF [27] lead to notable improvements. All these repre-
sentations are task-agnostic. They are fed to the right prediction algorithm,
such as a classiﬁer or a clustering method, depending on the application.
These shallow features were shown to generalize very well across tasks [23].
Recently, deep features, and convolutional neural networks (CNN) in par-
ticular, have been applied to document images and have shown improved
classiﬁcation and retrieval performances over some shallow features based on
BoV [28, 16, 17]. The characteristic of deep features is that they are learned
end-to-end. This means that the two previously distinct steps of i) feature
extraction and ii) prediction are done jointly in one single step.
In other
words, the feature and the classiﬁer are learned jointly and cannot be distin-
guished any more. Despite their recent success, they are highly specialized,
and their use as a generic feature extractor for document images has not
been studied in detail. Also, they are a lot more costly to train, as learning

4

can easily take several days on a GPU.

Hybrid architectures were recently introduced [18] to classify natural im-
ages. They also showed good transferability properties from classiﬁcation to
the image retrieval task. We are not aware of any work where these hybrid
models have been applied to document images.

3. Feature representations for document images

This benchmark considers a broad range of feature representations for
document images. First, we select two shallow features that were success-
fully used in various document image tasks [13, 15, 23]: the run-length his-
togram [7] and the Fisher-vector [25] representations, brieﬂy described in
Sections 3.1 and 3.2 respectively. We also experiment with deep features, and
more precisely with two diﬀerent convolutional neural network (CNN) archi-
tectures: the AlexNet [29] and GoogLeNet [30] networks, described in Section
3.3. Finally, Section 3.4 brieﬂy recalls the hybrid architecture from [18] that
is applied for the ﬁrst time to document images.

3.1. Run-length features

The run-length (RL) feature [7] encodes sequences of pixels that share
the same value and that are aligned vertically, horizontally or diagonally.
These sequences are called ”run-lengths” (see e.g. the green rectangles in the
Figure 1). While the RL feature can be extended to consider sequences of
similar gray-scale or even color values, in practice binary images are suﬃcient
to characterize document images [15, 12]. Therefore, we ﬁrst binarize the
document images using a simple thresholding1 and consider only runs of
black and white pixels. Optionally, one can resize the images so they share
the same resolution within the dataset.

The binarized images are described using histograms that collect the num-
ber of (black and white) pixel runs. To be more robust to noise and small
variations, these histograms use a logarithmic quantization of the lengths2
into Q bins as in [15, 12]. We compute these runs in four directions, hor-
izontal, vertical, diagonal and anti-diagonal, and build two histograms per

1Image pixel intensities are represented between 0 and 1 and we threshold at 0.5. For
[1], [2], [3 −

2The lengths of the runs are discretized according to the following bins:

color images we binarize the luminance channel.
4], [5 − 8], [9 − 16], . . . , [≥ (2Q−2 + 1)].

5

Figure 1: Left: Illustration of a vertical black run of length 7 (top) and a
horizontal white run of length 16 (bottom). Right: A 3-levels spatial pyramid
of sizes 1× 1, 2× 2, and 4× 4 captures the document layout (image courtesy
of [15]).

direction, one for the white pixels and one for the black pixels. An image
(or image region) is then represented by the concatenation of all these his-
tograms, resulting in a 4 × 2 × Q dimensional RL representation.

To better capture information about the page layout we use a spatial
pyramid [21] with several levels such that at a given level the image is divided
into n × n regions and the RL histograms computed on these regions are
concatenated to obtain the full image signature (see illustration in Figure 1).
To obtain the ﬁnal RL image feature, we (cid:96)1-normalize and apply component-
wise square-rooting as in [12].

3.2. Fisher-vector representations

The Fisher-vector (FV) [25] can be seen as an extension of the bag-of-
visual-words (BoV) representation [31, 24] that encodes higher order statis-
tics about the distribution of local descriptors assigned to visual words, as-
suming that the distribution of the descriptors can be modeled with a gen-
erative model, e.g. a Gaussian mixture model (GMM). More formally, the
FV characterizes the set of low-level features XI = {xt}T
t=1 extracted from
an image I by encoding the necessary modiﬁcations of the GMM in order
to best ﬁt this particular feature set. Assuming independence, this can be
written as the gradient of the data log-likelihood on the model:

T(cid:88)

(cid:40) N(cid:88)

(cid:41)

Gλ(I) =

1
T

∇λ log

wnN (xt|µn, Σn)

t=1

n=1

(1)

where wn, µn and Σn denote respectively the weight, the mean and the
covariance matrix of Gaussian n.

6

Figure 2: Illustration of the Fisher-vector representation [25]. Local patches
are extracted uniformly in the document image, and then described by low-
level descriptors (typically PCA-projected SIFT [26] descriptors). These are
encoded using a GMM-based visual codebook, and the patch-level encod-
ings are aggregated. Finally, the representation is square-rooted and (cid:96)2-
normalized.

To compare two images I and J, a natural kernel on these gradients
(cid:62)
F −1
is the Fisher Kernel K(I, J) = Gλ(I)
λ Gλ(J), where Fλ is the Fisher
information matrix. As F −1
is symmetric and positive deﬁnite, it has a
λ
Cholesky decomposition L(cid:62)
λ Lλ and K(I, J) can be rewritten as a dot-product
between normalized vectors Γλ where Γλ(I) = LλGλ(I) to which we refer as
the Fisher-vector (FV) of the image I.

It was shown in [25] that, assuming diagonal covariance matrices in the
GMM, these normalized gradients can be computed in closed form. Fur-
thermore, according to [25, 32], the gradients with respect to the weights
can be ignored as they bring marginal improvement. Hence, we consider
only the gradients with respect to the mean and the variance, and the ﬁ-
nal gradient vector Γλ(I) concatenates all Γµd
(I). This vector is
2N D-dimensional, where N is the number of Gaussians in the mixture, D
is the dimension of the low level features xt and sd
n are the elements of the
diagonal Σn. As proposed in [32], we apply a component-wise square-rooting
followed by (cid:96)2-normalization to produce the ﬁnal Fisher-vectors. The full
process is illustrated in Figure 2.

(I) and Γsd

n

n

3.3. Convolutional neural networks

Convolutional neural networks (CNNs) are composed of several layers
that combine linear and non-linear operators, and whose weights are jointly
learned in an end-to-end manner to solve a particular task. The most stan-
dard architectures consist of several stacked convolutional layers followed by

7

Figure 3: The AlexNet architecture (image courtesy of [29]), composed of ﬁve
convolutional layers, followed by three fully-connected layers. Max pooling
is applied after the ﬁrst, second and ﬁfth layers.

one or more fully-connected layers (i.e. linear projections). Between each
pair of layers one typically places a non-linear operator. For CNNs, recti-
ﬁed linear units ReLU have shown a great success [29]. Furthermore, max-
pooling layers are often inserted in-between successive convolutional layers
to progressively reduce the spatial size of the representation and to provide
invariance to translation.

Therefore, a feed-forward neural network can be thought of as the com-

position of a number of functions

F (x0) = FL(...F2(F1(x0, W1), W2), ..., WL),

(2)

where each function Fi takes an input xi−1 and a set of parameters Wi, and
produces xi as output, and where x0 is the input image.

Although the architecture of these networks, deﬁned by the hyper-parameters

and the arrangement of these blocks, are commonly handcrafted, the param-
eters set W1, . . . , WL of the network are learned in a supervised manner from
a set of labeled inputs using a suitable loss function for the task at hand.
To perform multiclass classiﬁcation, the most standard approach, which we
follow, is to have the output of the last fully-connected layer be the same
dimensionality as the number of classes and to apply a softmax at the out-
put of this layer to obtain the probability distribution of the classes given an
input image. To train the network, one typically uses a cross-entropy loss.
The weights are then updated by back-propagation using stochastic gradient
descent (see details of the training procedure in Section 4).

Since their introduction in the early 1990’s (LeNet [33]), and mostly since
their recent success in various challenges including the ImageNet large scale

8

Figure 4: Inception module as used in the GoogLeNet architecture [30].

visual recognition challenge (ILSVRC) [34], many diﬀerent CNN architec-
tures have been proposed [29, 30, 35, 36].
In this paper we focus on two
popular ones: AlexNet [29] and GoogLeNet [30].

AlexNet. The AlexNet architecture, proposed by Krizhevsky et al . [29], was
the ﬁrst successful CNN architecture for natural image classiﬁcation, outper-
forming by a large margin shallow methods in the ILSVRC 2012 competition.
This network is composed of eight layers with weights (see also Figure 3).
Five convolutional layers with respectively 96, 256, 384, 384, 256 kernels of
sizes 11×11, 5×5, 3×3, 3×3, and 3×3, are applied with a stride of 1 (except
for the ﬁrst layer that uses a stride of 4). A response normalization is applied
after layers 1 and 2, and a max pooling with size 2 and a stride of 2 are ap-
plied after layers 1, 2, and 5. This is followed by three fully-connected layers
of sizes 4096, 4096, and C respectively, where C is the number of classes.
The output of the last fully-connected layer is fed to a C-way softmax which
produces a distribution over the C class labels. ReLU non-linearities are
applied after every convolutional and fully-connected layer. The network is
fed with ﬁxed-size image of 224 × 224 × 3 pixels3.

GoogLeNet. Szegedy et al . [30] set a new state of the art in image classiﬁca-
tion on the ILSVRC 2014 dataset with a signiﬁcantly diﬀerent architecture,
the GoogLeNet.
It uses a deeper and wider architecture than traditional

3Note that this requires one to either distort the proportions of the input image, or to

use a subregion of the image [29].

9

1x1 convolutionsPrevious layer1x1 convolutions1x1 convolutions3x3 max poolingFilterconcatenation1x1 convolutions3x3 convolutions5x5 convolutionsInception layerFigure 5: Overview of the GoogLeNet architecture [30] (the layers composing
an inception module are detailed in Figure 4).

CNNs with 10 times fewer parameters. The main idea behind this model is
the inception architecture, which tries to approximate a sparsely connected
CNN architecture using readily available dense components. This allows one
to increase the size of the network without drastically increasing the com-
putational cost and without making it more prone to overﬁtting. For that,
GoogLeNet relies on several inception layers, where each of such layer uses
a series of trainable ﬁlters with sizes 1 × 1, 3 × 3 and 5 × 5 (see Figure 4).
In this way, there are multiple ﬁlter sizes per layer, and each layer has the
ability to target the diﬀerent feature resolutions that may occur in its input.
In order to reduce the computational cost, it also performs dimensionality
reduction by 1× 1 convolutions inserted before the expensive 3× 3 and 5× 5
convolutions. Finally, the inception module also includes a parallel pooling
path, which is concatenated along with the output of the convolutional layers
into a single output vector forming the ﬁnal output.

The GoogLeNet architecture is shown in Figure 5. Nine inception mod-
ules are stacked after two convolutional layers with ﬁlter sizes of 7 and 3 and
strides of 2 and 1. Max-pooling layers with size 3 and stride 2 are inserted
after the convolutional layers 1 and 2, and after the inception layers 3b, 4e,
and 5b. An average pooling layer with size 7 and stride 1 follows the last in-
ception layer 5b, whose output is fed to a single fully-connected layer and the
C-way softmax classiﬁer. All the convolutions use rectiﬁed linear activation
(ReLU ), including those inside the inception modules.

Besides its common use to solve a given task in an end-to-end manner,
it has also become a standard practice to use CNNs as generic feature ex-
tractors. Convolutional ﬁlters in the ﬁrst layers can be seen as detectors of
basic structures, like corners or straight lines, while deeper layers are able

10

InputConv 7x7LRNConv1x1MaxPool 3x3Conv3x3LRNInception3aInception3bMaxPool 3x3Inception4aInception4bInception4cInception4dInception4eMaxPool 3x3Inception5aInception5bAvgPool 7x7FCSoftmaxOutputFigure 6: Illustration of the hybrid architecture [18]. The unsupervised part
of the hybrid architecture is identical to the FV representation (see Figure 2),
but can be replaced by any shallow representation (e.g. RL). The supervised
part uses a set of fully-connected layers and ReLU non-linearities are applied
after every fully-connected layer. The last fully-connected layer is fed to a
softmax producing a distribution over the class labels.

to capture more complex structures and semantic information. Therefore,
a given image can be feed-forwarded through the CNN and the activations
of intermediate layers be used as mid-level features to represent it. These
features can then be used for a task diﬀerent than the one the CNN was
trained for. This ﬁnding was quantitatively validated for a number of tasks
including image classiﬁcation [37, 38, 35, 39, 40], image retrieval [40, 41],
object detection [42], and action recognition [38].

3.4. Hybrid descriptors

Our last representation is a hybrid descriptor [18] that draws inspiration
from both FVs and CNNs. This architecture combines an unsupervised part,
obtained by an image-level patch-based Fisher-vector encoding, and a super-
vised part composed of several fully-connected layers. The intuition behind
this model is to replace the convolutional layers of traditional CNN archi-
tectures with a FV representation and to learn subsequent fully-connected
layers in a supervised way, akin to a multi-layer perceptron (MLP), trained
with back-propagation. We provide details below for the resulting hybrid
architecture, illustrated in Figure 6.

The unsupervised part of the hybrid architecture is identical to the FV
In the
representation described in section 3.2. We consider two versions.
ﬁrst one, the FV representation is followed by a PCA projection and (cid:96)2-
normalization, as originally proposed by Perronnin and Larlus [18]. Alter-
natively, we consider a hybrid architecture that directly builds on the full

11

dimensionality FVs, in which case, the dimensional reduction is performed
implicitly by the ﬁrst fully-connected layer of the supervised part of the ar-
chitecture (i.e., in Figure 6, x0 would be the original FV without the PCA
and subsequent (cid:96)2 normalization).
The supervised part uses a set of L − 1 fully-connected layers S1, . . . SL−1
of size 4096 and a last layer SL of size C, where C is the number of classes,
and ReLU non-linearities are applied after every fully-connected layer. Like
in AlexNet and GoogLeNet, the output of the last fully-connected layer is
fed to a C-way softmax which produces a distribution over the C class labels.
As an alternative, we also replace the unsupervised part of this architecture
(i.e. the FV) by the run-length histogram (RL) described in section 3.1.

As in the case of CNNs, one can use the output of the fully-connected
layers as a transferable representation of the input images that can be used
for other tasks.

4. Training

As shallow, deep and hybrid features require diﬀerent learning paradigms,
this section details the training procedure for them in the context of the two
scenarios that we consider in the experimental part. In the ﬁrst one, training
and testing are done on the same dataset, for the same task (Section 4.1).
In the second one, both the dataset and the task can vary, and a transfer
mechanism is needed (Section 4.2).

4.1. Training for the task at end
Shallow features. To perform classiﬁcation using shallow features, we ﬁrst
extract the features from the training images and then we train a linear sup-
port vector machine (SVM) classiﬁer [43] using these features and their labels.
In the case of the RL descriptors, one does not need to perform any other
learning. However, to extract the FV descriptors, one needs to ﬁrst learn
the PCA for the SIFT descriptors and then learn GMM visual codebook in
an unsupervised manner. This is done by maximum likelihood estimation
(MLE) of the GMM parameters, where we impose diagonal covariance ma-
trices and the number of Gaussians. Beyond this unsupervised training step,
the FV does not depend on the data, and more importantly on the labels.
Thus, as the RL, it is independent from the task.

12

Deep features. Unlike the previous two representations, CNNs are deep
learning approaches that group feature extraction and prediction into a single
architecture. Consequently, features are learned to optimize the prediction
task, and the classiﬁer is already integrated in the architecture. Therefore,
at test time, the full architecture is used to predict the document label.

The set of parameters W of the CNN, which includes the weights and
biases of the convolutional ﬁlters and the fully-connected layers, are learned
in a supervised manner from a set of M labeled images {Im, ym} using a
suitable loss function for the task to be solved. In our case, we will train
both AlexNet and GoogLeNet for document image classiﬁcation. To learn
the parameters, we minimize the cross-entropy between the network output
and the ground-truth:

M(cid:88)

C(cid:88)

ym,c log(ˆym,c)

(3)

m=1

c=1

where ym,c is the ground-truth label, ˆym,c is the prediction of label c for the
image Im as output by the last layer, C is the number of classes and M is
the number of available training examples. We update the parameters W via
stochastic gradient descent (SGD) [44] by back-propagating the derivative of
the loss with respect to the parameters throughout the network. To avoid
over-ﬁtting, we use drop-out [29] at the input of the fully-connected layers.

Hybrid features. The hybrid descriptors share similarities both with tra-
ditional features (FV or RL depending on what is used in the unsupervised
part) and with deep features (fully-connected layers).
In the case of FV,
the unsupervised part requires learning the visual codebook on patches ex-
tracted from the dataset. The supervised part is trained end-to-end with
the classiﬁer integrated in the architecture. To learn the parameters of the
supervised part, as for the CNNs, we minimize the cross-entropy between the
label predicted by the last layer and the ground-truth labels. Weights are
updated using back-propagation. Again, we use drop-out at the input of the
fully-connected layers.

4.2. Training for a diﬀerent task

In the case of shallow features (RL or FV), descriptors can be used for a
diﬀerent task, and they only need to be combined with the right predictor
(new classiﬁer, clustering algorithm, etc.).

In the case of the CNN models, one can use the pre-trained model as
an oﬀ-the-shelf feature extractor. Given an input image, the activations of

13

dataset

# images

image size (pixels) # categ.

category description

RVL-CDIP

NIST
MARG
CLEF-IP

IH1
IH2
IH3

400000
5590
1553
38081
11252
884
7716

750M
8.4M
8.4M

1.5K - 4.5M

1.2M
1.4M

0.5M - 5M

16
12
9
9
14
72
63

document types

form types
layout types

patent image types

document types

document types and layout
ﬁne-grained document types

Table 1: Statistics of the seven datasets considered in our experiments.

diﬀerent layers can be used as its representation, as discussed in Section 3.3.
As with the shallow features, we can use these representations for other tasks
such as clustering or retrieval. Similarly, we also tested the activations of the
fully-connected layers of the hybrid model as oﬀ-the-shelf features for new
datasets, and for new tasks, by combining them with diﬀerent predictors.

5. Datasets

We conducted a broad experimental study comparing the feature rep-
resentations described in the previous section on seven diﬀerent datasets.
We used four publicly available datasets, namely RVL-CDIP, NIST, MARG,
and CLEF-IP. We also conﬁrm our conclusions on three in-house customer
datasets, that we refer to as IH1, IH2, and IH3. Statistics on the diﬀerent
datasets can be found in the Table 1 and some illustrations in Figures 7, 8
and 9. We detail their characteristics below.

RVL-CDIP. The Ryerson Vision Lab Complex Document Information Pro-
cessing (RVL-CDIP) dataset4 [17] is a subset of the IIT-CDIP Test collec-
tion [45]. It is composed of 400000 images labeled with one of the following
16 categories: letter, memo, email, ﬁlefolder, form, handwritten, invoice, ad-
vertisement, budget, news article, presentation, scientiﬁc publication, ques-
tionnaire, resume, scientiﬁc report, and speciﬁcation.

NIST. The NIST Structured Forms Reference Set5 [46] is a dataset of black-
and-white images that consists of 5590 pages of synthesized documents.

4http://scs.ryerson.ca/~aharley/rvl-cdip/
5http://www.nist.gov/srd/nistsd2.cfm

14

Figure 7: Illustration of the RVL-CDIP dataset. Each column corresponds
to one of the 16 classes (image courtesy of [17]).

These documents correspond to 12 diﬀerent tax forms from the IRS 1040
Package X for the year 1988. Class names are Forms 1040, 2106, 2441,
4562, 6251 and Schedules A, B, C, D, E, F, SE.

MARG. The Medical Article Records Ground-truth (MARG) dataset6 [47]
consists of 1553 documents, each document corresponding to the ﬁrst page
of a medical journal. The dataset is divided into 9 diﬀerent layout types.
These layouts vary in relative position of the title, the authors, the aﬃlia-
tion, the abstract and the text (see examples from four classes in Figure 8).
Within each layout type, the document can be composed of one, two or three
columns. This impacts visual similarity a lot and makes classiﬁcation and
clustering on this dataset very challenging.

CLEF-IP. The CLEF-IP dataset is the training set7 released for the Patent
Image Classiﬁcation task of the Clef-IP 2011 Challenge [48].
In the chal-
lenge, the aim was to categorize patent images (i.e. ﬁgures) into 9 cate-
gories: abstract drawing, graph, ﬂowchart, gene sequence, program listing,
symbol, chemical structure, table and mathematics. We show example im-
ages grouped by classes in Figure 8. The dataset contains between 300 and
6000 labeled images for each class, 38081 images in total, with a large varia-
tion of the image size (from as little as 1500 pixels to more than 4M pixels)
and aspect ratio (from 1 to more than 10).

IH1. The ﬁrst in-house dataset (IH1) regroups internal document images
from a single customer. It contains 11252 scanned documents from 14 dif-

6https://ceb.nlm.nih.gov/proj/marg/marg.php
7http://www.ifs.tuwien.ac.at/~clef-ip/download/2011/index.shtml

15

Figure 8: Example images from the NIST, MARG and CLEF-IP datasets
(clusters represent classes for MARG and for CLEF-IP).

ferent document categories such as invoices, contracts, IDs, coupons, hand-
written letters, etc.

IH2. The second in-house dataset (IH2) is a small dataset of 884 ﬁrst pages
of multi-page documents from a single customer, divided into 72 ﬁne-grained
categories representing both the document type such as invoice, mail, ta-
ble, map and the document layout (e.g. “a mail with an excel table on the
bottom”, “a table with black lines separating the rows”).

IH3. The third in-house dataset (IH3) contains 7716 documents collected
from several customers. We divided the dataset into 63 ﬁne-grained cat-
egories such as diverse types of forms, invoices, contracts, etc., where the
class labels were deﬁned by the generic document type but also by their
origin. Hence invoices, mails, handwritten or typed letter that belong to dif-
ferent customers were considered as independent classes. The aim with this
dataset was to go beyond generic document types or layout and simulate real
document image based applications where documents from several customers
should be processed all together (e.g. in a print or scan ﬂow).

6. Experiments

We ﬁrst provide some implementation details concerning the diﬀerent
representations used in the experiments (Section 6.1). Then we report our
experimental results. The experiments are divided into two main parts. In
the ﬁrst part, Section 6.2, we deal with a classiﬁcation task where training
and testing are done on the same dataset, i.e. the RVL-CDIP dataset. The
second part, Section 6.3, is devoted to the feature transfer experiments, where

16

Figure 9: Example images from the three in-house customer datasets IH1,
IH2 and IH3. The images were intentionally blurred for privacy reasons.
Here we show examples to illustrate the variability of documents within each
dataset.

we explore how transferable diﬀerent image representations, learned on the
RVL-CDIP dataset, are to new datasets and tasks without any extra learning
or ﬁne-tuning of the parameters.

6.1. Implementation details
Shallow features. To generate shallow features we ﬁrst downscale all im-
ages to a maximum number of pixels (250M) while preserving their aspect
ratio. For the RL rescaling is done after binarization.
For the RL representation we use a 5-level spatial pyramid of sizes 1 × 1,
2 × 2, 4 × 4, 6 × 6, and 8 × 8, and 11 quantization level ([1], [2], [3 − 4], [5 −
8], [9 − 16], . . . , [≥ 512], yielding a total of 121 × 8 × 11 = 10648 dimensions
per feature vector. This setup has been shown to perform best on various
tasks and datasets [23].
Our Fisher-vector descriptors are built on top of SIFT features [26] ex-
tracted at 5 diﬀerent scales varying the patch size from 24×24 to 96×96 in the
rescaled images. To add some basic spatial information to the local descrip-
tors we follow S´anchez et al . [49] and project the original 128-dimensional
SIFT features down to 77 dimensions with PCA. Then we concatenate the
position (x,y) and the scale (s) of the patch, obtaining 80-dimensional local
features used to build the visual vocabularies. We consider diﬀerent vocab-
ulary sizes and single layer spatial pyramid grids8. For a fair comparison we

8Initial experiments with multiple-level spatial pyramids yielded no improvements.

17

choose the vocabulary size and the pyramid grid to build FVs of the same di-
mension (40960). Hence, for a vocabulary with 4 Gaussians, we concatenate
FVs on an 8 × 8 grid, for a vocabulary with 16 Gaussians, we concatenate
FVs on a 4 × 4 grid, and for a vocabulary of size 256 we use the FV build
on the whole image. We denote the corresponding Fisher-vectors by FV4,
FV16 and FV256 respectively. Given a new dataset, we can either compute
new PCA and GMM models to build the FVs, or reuse the PCA and GMM
models pre-trained (in an unsupervised manner) on the RVL-CDIP dataset.
We opted for the second strategy for two reasons: ﬁrst, preliminary experi-
ments have shown very similar results, and second, our study aims at testing
the transferability of the models to new datasets.

Hybrid features. Both the RL and FV were considered in the unsuper-
vised part of the hybrid architecture (see Section 3.4). We refer to them
as FV+MLP and RL+MLP respectively. Note that for each feature type
(e.g. FV256 or FV16) we need to build a diﬀerent hybrid model. In addi-
tion to the model proposed in [18] where the size of the original FV is ﬁrst
reduced with PCA to 4096 dimensions, we also build hybrid models directly
on the FV without PCA reduction. In this case, we ﬁx the size of the ﬁrst
fully-connected layers to 4096, and we let the hybrid model learn the dimen-
sionality reduction. By default, results reported for our hybrid models do not
include PCA. When we do, this is mentioned explicitly (FV+PCA+MLP).
In the experiments exploring feature transferability (Section 6.3), we use the
activations of the fully-connected layers in the supervised part of the models
trained on the RVL-CDIP dataset (see Sections 3.4 and 4.2).

CNNs. We consider two popular CNN architectures that were successfully
used to classify natural images: AlexNet and GoogLeNet (see Section 3.3)
denoted by CNN-A and CNN-G respectively. For both models, we initialize
the CNN with the models (available online) trained on the ImageNet clas-
siﬁcation challenge dataset [34] (ILSVRC 2012), and ﬁne-tune them on the
RVL-CDIP dataset. We also conducted experiments where the models were
directly trained on RVL-CDIP, but the results were 1-2% below the ﬁne-
tuned version. As above, for the feature transferability experiments (Section
6.3) we considered activation features corresponding to the models ﬁne-tuned
on the RVL-CDIP dataset (see details in Sections 3.3 and 4.2).

18

Features RL+SVM FV+SVM RL+MLP FV+MLP CNN-A CNN-G

Top-1

75.6

85.1

84.8

89.3

90.1

90.7

Table 2: Overall comparison. Top-1 accuracy for diﬀerent descriptors on the
RVL-CDIP dataset.

6.2. Part 1: Classiﬁcation of documents from the same dataset

The ﬁrst part of our experimental analysis focuses on the document im-
age classiﬁcation task. We benchmark the diﬀerent feature representations
introduced in Section 3 on the RVL-CDIP dataset. We follow the experimen-
tal protocol (train/val/test split and evaluation measure) suggested in [17].
We used the validation set to choose both the classiﬁer’s parameters (the
regularization cost in the case of the SVM) as well as the hyper-parameters
of the CNN and the MLP (number of iterations, learning rate, and number
of layers for the MLP).

First, as a summary, we compare the best ﬂavor of each descriptor type,

then we provide a deeper analysis of the diﬀerent models.

6.2.1. Overall comparison

Table 2 summarizes top-1 accuracy on the RVL-CDIP dataset for the

best version of each ﬂavor of features that we consider in our benchmark.

First, we notice that both CNN-A and CNN-G outperform other descrip-
tors. The CNN-A results are consistent with state-of-the art results on the
RVL-CDIP dataset [17] that reports 89.9% top-1 for its holistic AlexNet-
based CNN. By using a better CNN architecture (GoogLeNet), we manage
to improve it and get 90.7% top-1 accuracy.

Second, the hybrid architecture based on Fisher-vectors (FV+MLP) yields
an accuracy very close to the CNN-A. This is an interesting observation as
these models are much faster to train than the CNNs, and no GPU is re-
quired. More generally, we can observe the strong performance gain (+9.2%
for RL and +4.2% for FV) brought by the hybrid architecture compared
to these features used in their shallow version and combined with an SVM
classiﬁer.

Last and not surprisingly, FV features outperform RL features both using
SVM and MLP on the document image classiﬁcation task, this conﬁrms
previous observations in [23].

19

Model FV4 FV16 FV256
+SVM 83.1
+MLP 88.3

85.1
89.3

85.0
89.1

Table 3: Top-1 accuracy on RVL-CDIP for FVs using diﬀerent vocabulary
sizes, combined with SVM or within the hybrid architecture (+MLP).

6.2.2. Deeper Analysis

In this section, we study the parameters of the representations, showing

that some of them play a crucial role in improving classiﬁcation accuracy.

The vocabulary size for the FV. In Table 3 we compare the 3 diﬀerent FV
representations, FV4, FV16 and FV256, of the same dimensions (40960).
These representations are combined either with an SVM classiﬁer or used
within the hybrid architecture (i.e. MLPs). We can see that while FV256
and FV16 are on par when we use SVM, the hybrid model built on FV16
performs slightly better. Also, we observe that FV4 performs worse than
the other descriptors in both cases, showing that the vocabulary needs to be
expressive enough. Therefore, we do not report further results with FV4.

Number of hidden layers. We ﬁrst look at the modiﬁed hybrid architecture
that we proposed, i.e. which does not apply PCA to the FV representations
in the unsupervised part. Table 4 compares several hybrid architectures.
On top of either FV256 or FV16 we use a varying number H of hidden
layers, yielding to models with increasingly deep architectures. We observe
that even with a small number of fully-connected layers, we obtain good
performances. Moreover, even a single layer already achieves better results
than the FV+SVM strategy. Note that all hidden layers have their size ﬁxed
to 4096, but we varied the level of drop out. Best results were obtained in
general with a drop out level of 30% or 40%.

Inﬂuence of PCA in the hybrid architecture. We modiﬁed the original hybrid
model of [18] by removing the PCA projection (x0 is FV instead of FV+PCA)
and learning the dimensionality reduction (the size of x1 is 4096) in the
supervised part of the architecture (see Figure 6). In the last row of Table 4
we compare the previous results with the original model (built on top of
FV+PCA), still varying the number of hidden layers H. We observe that
except for the single layer case (H = 1), the proposed hybrid architecture

20

Model

FV16+MLP
FV256+MLP

FV256+PCA+MLP

H = 1 H = 2 H = 3 H = 4
89.2
89.3
89.0
88.2
88.4
88.5

89.3
89.1
88.5

89.2
89.0
88.6

Table 4: Top-1 accuracy on RVL-CDIP for diﬀerent numbers of hidden layers
(H) in the hybrid architecture, for diﬀerent FV representations, optionally
followed by a PCA projection (last raw).

that does not apply PCA to the FV but learns the dimension reduction
performs better.

6.3. Part 2: Transfer of features to diﬀerent datasets and tasks

In this section we explore how transferable the models and the related
features are to new datasets and tasks. We target three diﬀerent tasks:
i) retrieval, ii) clustering and iii) (non parametric) classiﬁcation. For all
the experiments, we consider the best models9 trained on the RVL-CDIP
dataset and use them to extract features from the document images in the
six remaining datasets.

For all three tasks, we randomly split the datasets in halves, the ﬁrst set
for training, and the second set for testing. This is done ﬁves times, and we
report averaged results over the ﬁve splits. To asses the performance for a
given split we proceed as follows.

Retrieval. Each test example is considered in turn as query example and the
documents in the training set are ranked according to their similarity to it. As
our features are (cid:96)2-normalized, we use the dot product as similarity measure
for all features. To asses the retrieval performance, we use mean average
precision (mAP). We also averaged over all query examples precisions at 1
(P@1) and at 5 (P@5), but as they exhibited similar behavior, we only report
the mAP results.

Clustering. For each split, we cluster samples from the training set into as
many clusters as the number of classes we have in the dataset using hier-

9SIFT-PCA and GMM for FV, the deep models (CNN, hybrid) with the parameters

best performing on the RVL-CDIP validation set.

21

IH2
Retrieval (mAP) NIST MARG CLEF-IP IH1
78.8
75.1
76.8
72.7
65.7
59.1
78.3
73.4
77.1
80.5
78.5 81.8
78.0
80.3
79.2
75.6
63.0
64.6

CNN-A-p5
CNN-A-fc6
CNN-A-fc7
CNN-G-i3a
CNN-G-i3b
CNN-G-i4a
CNN-G-i4e
CNN-G-i5b
CNN-G-p5s1

40.9
38.6
28.9
38.4
36.9
42.4
43.9
39.0
32.2

100
99.9
93.6
100
100
100
100
99.9
78.9

38.7
37.0
30.5
33.6
34.1
36.4
36.2
35.2
27.8

IH3
67.7
63.6
43.8
66.2
67.0
73.4
73.8
67.9
44.7

Table 5: Retrieval task: mean Average Precision (mAP) for diﬀerent CNN
“oﬀ-the-shelf” descriptors on diﬀerent transfer datasets.

archical clustering with centroid-linkage10. To evaluate the quality of the
clustering we consider the adjusted mutual information [50] between true
class labels and cluster labels, the adjusted Random Index [51], and the
V-measure [52] (which is the weighted harmonic mean of homogeneity and
completeness). As we observe similar trends for these three measures, we
only report results with the adjusted mutual information (AMI).

Classiﬁcation. We consider the Nearest Classiﬁcation Mean (NCM) classi-
ﬁer [53] in our classiﬁcation experiments as it is a non-parametric classiﬁer,
i.e. each class is represented by the centroid (class mean) of its training ex-
amples and a test element is assigned to the closest class mean. We report
overall classiﬁcation accuracy (number of correctly classiﬁed test documents
divided by the number of test documents).

In what follows, we ﬁrst explore the best performing models and param-
eter conﬁgurations of each feature type (shallow, deep, and hybrid), then
we present overall comparisons, and we ﬁnally discuss the results for each
dataset individually.

10Other clustering algorithms and diﬀerent numbers of clusters could have lead to better
performances, however here we are not interested on the clustering algorithm itself, but
on comparing the diﬀerent features in a similar setting.

22

Clustering (AMI) NIST MARG CLEF-IP IH1
73.6
73.6
78.4

CNN-A-p5
CNN-G-i4a
CNN-G-i4e

31.8
38.5
40.2

100
98.8
100

7.9
8.5
8.9

IH2
73.7
71.5
66.8

IH3
59.5
65.4
59.3

Table 6: Clustering evaluated using adjusted mutual information (AMI) for
CNN activation features that yielded best retrieval performances.

Classiﬁcation (OA) NIST MARG CLEF-IP IH1
94.3
94.0
94.5

CNN-A-p5
CNN-G-i4a
CNN-G-i4e

65.7
63.3
60.4

75.0
75.8
74.1

100
100
100

IH2
93.6
92.6
92.6

IH3
91.3
93.6
93.8

Table 7: NCM-based classiﬁcation evaluated by overall accuracy (OA) for
CNN activation features that yielded best retrieval performances.

6.3.1. CNN features

In this section, we compare activation features extracted from diﬀerent
layers for both CNN architectures (see details in Sections 3.3 and 4.2). In
the case of AlexNet (CNN-A), we consider the 3 most popular layers for our
tasks: pool5, fc6 and fc7, that are activation features of the last pooling layer
and of the two ﬁrst fully-connected layers respectively. Both fc6 and fc7 have
4096 dimensions, and the pool5 feature is 9216-dimensional. In the case of
GoogLeNet, we consider activations from the diﬀerent inception layers i3a,
i3b and i4a, i4e, i5b (see Figure 4) which are features with their dimensions
equal to 200704, 376320, 100352, 163072 and 50176 respectively. We also
consider the activations from the average pooling layer that follows the last
inception layer, denoted by p5s1, which has 1024 dimensions.

We report retrieval results in Table 5. In addition, we also report cluster-
ing and classiﬁcation results in the Tables 6 and 7 for the activation features
best performing on the retrieval task. Based on these three tables we make
the following observations.

First, for all three tasks, the best results are in general obtained with
inception layers i4a and i4e of GoogLeNet except for MARG and IH2 where
the CNN-A-p5 of AlexNet outperforms in general the results obtained with
the diﬀerent GoogLeNet activation features. This can be explained by the

23

Retrieval (mAP)

RL

RL + MLP

FV256

FV256 + MLP

FV16

FV16 + MLP
FV256+PCA

FV256+PCA+MLP

IH3
NIST MARG CLEF-IP IH1
57.8
63.7
100
60.7
66.5
100
64.0
99.8
75.6
64.8
74.1
96.5
67.7
77.4
100
76.4
99.7
65.7
79.1 70.6 68.3
99.9
66.7
77.1
99.8

34.3
34.9
38.2
35.9
36.3
32.1
38.2
37.0

34.8
36.4
43.4
43.6
44.2
44.0
50.9
50.8

IH2
66.5
67.4
64.9
64.0
68.1
66.3

69.6

Table 8: Retrieval task: mean average precision (mAP) obtained by diﬀerent
shallow descriptors on diﬀerent datasets. We underline the cases where the
hybrid activation feature outperforms its corresponding shallow feature.

fact that CNN-G-i4a and CNN-G-i4e capture higher-level semantic informa-
tion11, that are well aligned with the diﬀerent categories these datasets are
composed of. On the other hand, the categories from MARG and IH2 are
more correlated with the layout than with the document semantics, and the
pool5 convolutional layer of the AlexNet (CNN-A-p5) better captures the lo-
cal geometry. Surprisingly, CNN-A-p5 outperforms signiﬁcantly CNN-A-fc6
and CNN-A-fc7 on all datasets, not only on MARG, meaning that the latter
features do not transfer well in the context of document images. One expla-
nation might be the low number of classes (16) in the RVL-CDIP dataset,
used to train the models.

6.3.2. Shallow and hybrid features

For these experiments, we consider the run-length descriptor with a 5-
level spatial pyramid (RL), and two Fisher-vector-based descriptors with
respectively 16 and 256 Gaussians (FV16 and FV256). We compare them
with activation features extracted from the corresponding hybrid architec-
tures (MLP). In addition, we consider the PCA-projected FV256 as a shallow
feature and the activation features from its hybrid model.

In all cases, we select the MLP model that performs best on the RVL-
CDIP validation set (see Section in 6.3) and use the activation values from
the fully-connected layers as feature representations, similarly to what is usu-

11But not too specialized such as CNN-G-p5s1 or CNN-A-fc7, which perform poorly

24

Clustering (AMI)

RL

RL + MLP

FV256

FV256 + MLP

FV16

FV16 + MLP
FV256+PCA

FV256+PCA+MLP

NIST MARG CLEF-IP IH1
18.0
98.9
58.4
99.2
99.5
62.3
67.7
94.2
71.1
100
74.6
97.7
99.4
72.5
71.2
98.6

28.5
26.7
30.2
35.6
40.3
39.1
44.9
42.7

4.8
6.2
7.7
6.5
7.5
9.1
4.8
1.6

IH2
45.7
52.5
43.7
45.0
49.7
54.9
60.3
59.6

IH3
19.6
45.5
58.8
63.3
63.1
63.5
47.4
40.9

Table 9: Clustering task: adjusted mutual information (AMI) for diﬀerent
shallow descriptors on diﬀerent datasets. We underline the cases where the
hybrid activation feature outperforms its corresponding shallow feature.

ally done when using CNN models as “oﬀ-the-shelf” features. By design, all
these descriptors are 4096-dimensional. When using them in our target appli-
cations, we observe that in most cases the activation features corresponding
to the ﬁrst fully-connected layer outperform the activation features of the
following layers. Therefore we decided to only show results obtained with the
ﬁrst fully-connected layer.

We show results both with the shallow features and the corresponding
hybrid activation features in Table 8 for retrieval, Table 9 for clustering and
Table 10 for categorization. Best results per dataset are shown in bold.

We observe that the advantage of hybrid architectures for feature trans-
fer is less obvious than in the initial classiﬁcation task. To make this easier
to observe from the tables, we underline the cases where the hybrid activa-
tion feature outperforms its corresponding shallow feature. The results are
somewhat mixed depending on features, tasks and datasets. The activation
feature of the hybrid model learned on RL is almost always better than the
original RL feature. In the case of the FV16 and FV256 the hybrid model
sometimes brings a gain (especially on clustering results) but in other cases
using directly the shallow features performs better. If we consider the PCA
reduced FV256, using the hybrid model most often degrades the performance.
Overall, the best retrieval results and most often the best NCM classiﬁ-
cation accuracies are obtained with FV256+PCA. Regarding the clustering
task, FV16+MLP signiﬁcantly outperforms FV256+PCA for three datasets
out of six.

25

RL

RL + MLP

FV256

FV256+MLP

Classiﬁcation (OA) NIST MARG CLEF-IP IH1
90.2
91.4
92.6
92.2
92.9
93.1
94.1
94.5

54.7
53.4
62.0
58.4
61.7
53.7
64.4
63.1

57.8
63.1
76.7
69.4
72.8
64.5
81.3
79.6

100
100
100
100
100
100
100
100

FV16

FV16+MLP
FV256+PCA

FV256+PCA+MLP

IH3
IH2
83.2
79.8
84.9
79.1
90.0
83.5
89.5
81.9
90.1
85.0
84.1
88.4
86.6 92.5
92.5
86.4

Table 10: NCM-based classiﬁcation: overall accuracy (OA) obtained by dif-
ferent shallow and hybrid descriptors on diﬀerent datasets. We underline
the cases where the hybrid activation feature outperforms its corresponding
shallow feature.

6.3.3. Comparing shallow and deep features

Finally, Table 11, summarizes all the best results obtained with shallow,

deep and hybrid features, and analyze them dataset per dataset.

NIST. This dataset is much easier than the other ones as the appearance is
very consistent within a given category, and categories are well-aligned with
speciﬁc templates. Consequently, most methods perform really well on all
tasks.

MARG. This dataset is much more challenging as category labels were
deﬁned based on speciﬁc aspects of the document layout (i.e. locations of
the aﬃliation or of the abstract), while other aspects of the layout are totally
ignored (e.g. the number of columns in the document). Consequently, there
is a large intra-class variation, and visually similar documents may belong to
diﬀerent categories. This is illustrated in Figure 10. The left part of the ﬁgure
displays visually dissimilar documents from the same category, while the right
part displays a cluster of visually similar documents that belong to diﬀerent
categories (each document represents one of the classes). This could explain
the very low clustering results obtained, independently of the visual feature
used. Regarding the retrieval and NCM classiﬁcation tasks, best results are
obtained with pool5 activation features from AlexNet (CNN-A-p5), however
the performance obtained with FV256+PCA are close to these results and
better than the results obtained with GoogLeNet activation features.

26

FV16 + MLP
FV256+PCA

CNN-A-p5
CNN-G-i4a
CNN-G-i4e

CNN-A-p5
CNN-G-i4a
CNN-G-i4e

NIST MARG CLEF-IP IH1
75.1
100
78.5
100
100
78.0
76.4
99.7
99.9
79.1
Clustering (AMI) NIST MARG CLEF-IP IH1
73.6
73.6
78.4
74.6
72.5
Classiﬁcation (OA) NIST MARG CLEF-IP IH1
94.3
94.0
94.5
93.1
94.1

CNN-A-p5
CNN-G-i4a
CNN-G-i4e

100
98.8
100
97.7
99.4

100
100
100
100
100

40.9
42.4
43.9
44.0
50.9

31.8
38.5
40.2
39.1
44.9

75.0
75.8
74.1
64.5
81.3

38.7
36.4
36.2
32.1
38.2

7.9
8.5
8.9
9.1
4.8

65.7
63.3
60.4
53.7
64.4

Retrieval (mAP)

FV16 + MLP
FV256+PCA

FV16 + MLP
FV256+PCA

IH2
78.8
81.8
80.3
66.3
70.6
IH2
73.7
71.5
66.8
54.9
60.3
IH2
93.6
92.6
92.6
84.1
86.6

IH3
67.7
73.4
73.8
65.7
68.3
IH3
59.5
65.4
59.3
63.5
47.4
IH3
91.3
93.6
93.8
88.4
92.5

Table 11: Summary table that compares best performing variants of the
diﬀerent descriptors for diﬀerent tasks and datasets.

Clef-IP. This dataset departs from the others in two aspects. First, the size
and aspect ratio of the images varies a lot, which might have a strong im-
pact on CNN representations that use a ﬁxed size and aspect ratio as input.
Second, there is a very large intra-class variability and the document layout
has small or even no importance in the category deﬁnition (see examples
in Figure 8). This might explain why FV256+PCA outperforms by a large
margin CNN activation features; FVs are explicitly designed to work with
geometry-less bags of local features, and consequently they better capture
local information disregarding its position (see e.g. ﬂowchart components or
mathematical symbols in formulas). Qualitative results are shown in Fig-
ure 11.

IH1. This dataset is probably the one most similar to the RVL-CDIP dataset,
on which the feature representations have been trained. Indeed, both datasets
share classes, such as invoices, contracts, etc. However, the IH1 dataset also
consists of sub-classes (e.g. invoice type 1 and invoice type 2 ). On this

27

Figure 10: Illustration of the MARG dataset. Top left: visually diﬀerent
documents that all share the same category label (Type C). Top right: doc-
uments from diﬀerent categories grouped automatically in the same cluster.
Bottom: visual deﬁnition of the nine layout types.

dataset, for the classiﬁcation task, the diﬀerent methods obtain similar ac-
curacies, CNN-G-i4e features being the best. This feature yields also the
best clustering performance but is outperformed on the retrieval task by
FV256+PCA. The relatively good and similar performances obtained with
the CNN and hybrid activation features is probably due to the closeness be-
tween the classes and images of the RVL-CDIP dataset used to train the
models, and those of the IH1 dataset.

IH2. This is a small ﬁne-grained dataset (72 categories) where the document
layout (e.g. ”page with two tables, one on the top and one on the bottom”),
plays a crucial role in the category deﬁnition. This property seems to have
been better captured by CNN activation features that keep geometric infor-
mation compared to FV256+PCA or FV16+MLP that are less dependent
on the layout. Tables 8, 9 and 10 show the importance of capturing the
geometry for this datasets: FV16 (with its spatial grid) and even RL (with
a 5-level spatial pyramid) often outperforms FV256 (no grid). In summary,
on this dataset, there is no obvious best performing feature: CNN-A-p5 per-
forms the best for clustering and NCM classiﬁcation, but it is outperformed
by both CNN-G-i4a and CNN-G-i4e on the retrieval task.

28

Figure 11: Randomly selected query examples from the Clef-IP dataset. For
each query (left) we show top results with FV256+PCA (upper row) and
top results obtained with CNN-G-i4e (bottom row). Relevant results are
shown with green borders and irrelevant ones with red borders (best viewed
in color).

IH3. The last in-house dataset contains 63 ﬁne-grained categories such as
diverse forms, invoices, contracts where each form/contract/invoice coming
from a diﬀerent customer corresponds to a diﬀerent class. It can be seen as a
mix between NIST (some classes are variations of templates) and IH1 (other
classes are more generic with intra-class variations, and we have also sub-
classes for several of them). On this dataset the best or close to best results
were obtained with the CNN-G-i4e activation features of GoogLeNet. We
show some retrieval examples in Figure 12, where we compare the top results
for this feature with the top results obtained with FV256+PCA. Note that
the class label diﬀerences often come from the fact that the document belongs
to diﬀerent customers, which explains that while most retrieved documents
are of the same generic type as the query (e.g. drawing, handwritten letter,
printed code) not all of them are considered as relevant to the query (provided
by diﬀerent customers they belong to diﬀerent classes).

7. Conclusions

This paper proposes a detailed benchmark that compares three types
of document image representation: so-called shallow features, such as the
run-length and the Fisher-vector descriptors, deep features based on convo-

29

Figure 12: Randomly selected examples from IH3. For each query (left),
we show top results with FV256+PCA (upper row) and with CNN-G-i4e
(bottom row).
(The images were intentionally blurred to keep the actual
content of the documents conﬁdential.

lutional neural networks, and features extracted from hybrid architectures
that take inspiration from the two previous ones. Our benchmark ﬁrst com-
pares these features on a classiﬁcation task where the training and testing
sets belong to the same domain. It also compares these features when used
to represent documents from other domains, for three diﬀerent tasks, in or-
der to quantify how much these diﬀerent document image representations
generalize across datasets and tasks.

We observed that without domain shift, CNN features perform better
than shallow and hybrid features, closely followed by hybrid architectures
that perform almost as well for a fraction of the training cost. This had
already been observed for natural images, and we conﬁrmed this observation
for document images.

In presence of a domain shift, the story changes quite signiﬁcantly. In-
dependently of the targeted task (we considered retrieval, clustering, and
classiﬁcation), the hybrid architectures do not transfer well in general across
datasets. Instead, deep or shallow features obtain the best performance, de-
pending on the dataset particularities. On one hand, CNNs seems to perform
best for target datasets that are not too diﬀerent from the source dataset,
and for datasets for which the global layout is important. On the other hand,

30

PCA reduced FVs appears to better deal with strong aspect-ratio changes
and very large intra-class variability on the document layout.

References

[1] N. Chen, D. Blostein, A survey of document image classiﬁcation: prob-
lem statement, classiﬁer architecture and performance evaluatio, IJDAR
10 (2007) 1–16.

[2] J. Cullen, J. Jonathan, P. Hart, Document image database retrieval and

browsing using texture analysis, in: ICDAR, 1997.

[3] P. Heroux, S. Diana, A. Ribert, E. Trupin, Classiﬁcation method study

for automatic form class identiﬁcation, in: ICPR, 1998.

[4] C. Shin, D. Doermann, A. Rosenfeld, Classiﬁcation of document pages

using structure-based features, IJDAR 3 (2001) 232–247.

[5] A. Bagdanov, M. Worring, Multiscale document description using rect-

angular granulometries, IJDAR 6 (2004) 181–191.

[6] P. Sarkar, Image classiﬁcation: Classifying distributions of visual fea-

tures, in: ICPR, 2006.

[7] Y.-K. Chan, C.-C. Chang,

Image matching using run-length feature,

Pattern Recognition Letters 22 (2001) 447–455.

[8] D. Keysers, F. Shafait, T. Breuel, Document image zone classiﬁcation -

a simple high-performance approach, in: VISAPP, 2007.

[9] A. Gordo, F. Perronnin, A bag-of-pages approach to unordered multi-

page document classiﬁcation, in: ICPR, 2010.

[10] A. Gordo, F. Perronnin, E. Valveny, Document classiﬁcation using mul-

tiple views, in: DAS, 2012.

[11] M. Rusnol, D. Karatzas, A. Bagdanov, J. Llados, Multipage document

retrieval by textual and visual representations, in: ICPR, 2012.

[12] A. Gordo, F. Perronnin, E. Valveny, Large-scale document image re-
trieval and classiﬁcation with runlength histograms and binary embed-
dings, PR 46 (2013) 1898–1905.

31

[13] A. Gordo, M. Rusinol, D. Karatzas, A. Bagdanov, Document classiﬁ-
cation and page stream segmentation for digital mailroom applications,
in: ICDAR, 2013.

[14] M. Rusnol, V. Frinken, D. Karatzas, A. Bagdanov, J. Llados, Mul-
timodal page classiﬁcation in administrative document image streams,
IJDAR 17 (2014) 331–341.

[15] A. Gordo, Document Image Representation, Classiﬁcation and Retrieval
in Large-Scale Domains, Ph.D. thesis, Computer Vision Center, Univer-
sitat Aut`onoma de Barcelona, 2013.

[16] L. Kang, J. Kumar, P.Ye, Y. Liy, D. Doermann, Convolutional neural

networks for document image classiﬁcation, in: ICPR, 2014.

[17] A. Harley, A. Ufkes, K. Derpanis, Evaluation of deep convolutional nets

for document image classiﬁcation and retrieval, in: ICDAR, 2015.

[18] F. Perronnin, D. Larlus, Fisher vectors meet neural networks: A hybrid

classiﬁcation architecture, in: CVPR, 2015.

[19] K. V. U. Reddy, V. Govindaraju, Form classiﬁcation, in: DRR, 2008.

[20] G. Joutel, V. Eglin, S. Bres, H. Emptoz, Curvelets based queries for

cbir application in handwriting collections, in: ICDAR, 2007.

[21] S. Lazebnik, C. Schmid, J. Ponce, Beyond bags of features: spatial
pyramid matching for recognizing natural scene categories, in: CVPR,
2006.

[22] J. Kumar, P. Ye, D. Doermann, Structural similarity for document

image classiﬁcation and retrieval, PRL 43 (2014) 119–126.

[23] G. Csurka, Document image classiﬁcation, with a speciﬁc view on ap-

plications of patent images, CoRR arXiv:1601.03295 (2016).

[24] G. Csurka, C. Dance, L. Fan, J. Willamowski, C. Bray, Visual cate-
in: ECCV Workshop on Statistical

gorization with bags of keypoints,
Learning for Computer Vision, 2004.

[25] F. Perronnin, C. Dance, Fisher kernels on visual vocabularies for image

categorization, in: CVPR, 2007.

32

[26] D. Lowe, Distinctive image features from scale-invariant keypoints,

IJCV 60 (2004) 91–110.

[27] H. Bay, T. Tuytelaars, L. Gool, SURF: Speeded Up Robust Features,

in: ECCV, 2006.

[28] Y. Rangoni, A. Bela¨ıd, S. Vajda, Labelling logical structures of doc-
ument images using a dynamic perceptive neural network, IJDAR 15
(2012).

[29] A. Krizhevsky, I. Sutskever, G. Hinton,

ImageNet classiﬁcation with

deep convolutional neural networks, in: NIPS, 2012.

[30] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er-
han, V. Vanhoucke, A. Rabinovich, Going deeper with convolutions, in:
CVPR, 2015.

[31] J. Sivic, A. Zisserman, Video google: A text retrieval approach to object

matching in videos, in: ICCV, 2003.

[32] F. Perronnin, J. S´anchez, T. Mensink, Improving the ﬁsher kernel for

large-scale image classiﬁcation, in: ECCV, 2010.

[33] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hub-
bard, L. D. Jackel, Backpropagation applied to handwritten zip code
recognition, Neural Computation 1 (1989) 541–551.

[34] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. Berg, L. Fei-Fei,
Imagenet large scale visual recognition challenge, IJCV 115 (2015) 211–
252.

[35] M. Zeiler, R. Fergus, Visualizing and understanding convolutional net-

works, in: ECCV, 2014.

[36] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-

scale image recognition, CoRR arXiv:1409.1556 (2014).

[37] J. Donahue, Y. Jia, O. Vinyals, J. Hoﬀman, N. Zhang, E. Tzeng, T. Dar-
rell, DeCAF: A deep convolutional activation feature for generic visual
recognition, in: ICML, 2014.

33

[38] M. Oquab, L. Bottou, I. Laptev, J. Sivic, Learning and transferring
mid-level image representations using convolutional neural networks, in:
CVPR, 2014.

[39] K. Chatﬁeld, K. Simonyan, A. Vedaldi, A. Zisserman, Return of the
in: BMVC,

devil in the details: delving deep into convolutional nets,
2014.

[40] A. S. Razavian, H. Azizpour, J. Sullivan, S. Carlsson, CNN features
in: CVPR Deep

oﬀ-the-shelf: an astounding baseline for recognition,
Vision Workshop, 2014.

[41] A. Babenko, A. Slesarev, A. Chigorin, V. S. Lempitsky, Neural codes

for image retrieval, in: ECCV, 2014.

[42] R. Girshick, J. Donahue, T. Darrell, J. Malik, Rich feature hierarchies
in: CVPR,

for accurate object detection and semantic segmentation,
2014.

[43] V. Vapnik, Statistical learning theory, Wiley-Interscience, 1998.

[44] L. Bottou, Large-scale machine learning with stochastic gradient de-

scent, in: COMPSTAT, 2010.

[45] D. Lewis, G. Agam, S. Argamon, O. Frieder, D. Grossman, J.Heard,
Building a test collection for complex document information processing,
in: SIGIR, 2006.

[46] D. L. Dimmick, M. D. Garris, C. L. Wilson, Structured Forms Database,
Technical Report, SFRS, National Institutte of Standards and Technol-
ogy, 1991.

[47] G. Ford, G. Thoma, Ground truth data for document image analysis, in:
Symposium on Document Image Understanding and Technology, 2003.

[48] F. Piroi, M. Lupu, A. Hanbury, V. Zenz, CLEF-IP 2011: Retrieval in
the Intellectual Property Domain, in: Intellectual Property Evaluation
Campaign (CLEF-IP), 2011.

[49] J. S´anchez, F. Perronnin, T. D. Campos, Modeling the spatial layout of

images beyond spatial pyramids, PRL (2012).

34

[50] N. Vinh, J. Epps, J. Bailey, Information theoretic measures for cluster-

ings comparison, in: ICML, 2009.

[51] L. Hubert, P. Arabie, Comparing partitions, Journal of Classiﬁcation 2

(1985) 193–218.

[52] A. Rosenberg, J. Hirschberg, V-measure: A conditional entropy-based

external cluster evaluation measure, in: EMNLP-CoNLL, 2007.

[53] T. Mensink, J. Verbeek, F. Perronnin, G. Csurka, Distance-Based Image
Classiﬁcation: Generalizing to new classes at near-zero cost, PAMI 35
(2013).

35

