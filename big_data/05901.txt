6
1
0
2

 
r
a

 

M
2
2

 
 
]

C
H
.
s
c
[
 
 

2
v
1
0
9
5
0

.

3
0
6
1
:
v
i
X
r
a

Emotion Classiﬁcation from Noisy Speech - A

Deep Learning Approach

Rajib Rana∗, Raja Jurdak†, Xue Li‡, Jeffrey Soar‡ Roland Goecke§ Julien Epps¶ and Michael Breakspear(cid:107)

∗Institute of Resilient Region (IRR)

University of Southern Queensland, Springﬁeld, QLD 4300

Email: rajib.rana@usq.edu.au

†Sensor Systems Group, CSIRO-Data61

‡School of Information Technology and Electrical Engineering, University of Queensland

Email: raja.jurdak@csiro.au

§Information Technology & Engineering, University of Canberra

xueli@itee.uq.edu.au

¶School of Electrical Engineering and Telecommunications, University of New South Wales

roland.goecke@canberra.edu.au

(cid:107)ISystems Neuroscience Group, Queensland Institute of Medical Research

j.epps@unsw.edu.au

Michael.Breakspear@qimrberghofer.edu.au

Abstract—Mood and emotion although used interchangeably,
mood is signiﬁcantly different from emotion in many aspects.
It reﬂects the internal state of a person compared to a rather
transient affective state shown by emotional expressions. Mood
inference from voice can provide better performance, however,
hardly any study exists that predicts mood from voice. In this
paper, we propose a solution to this problem and present our
work in progress.

I. INTRODUCTION

In the era of smartphones,

there is almost an app for
everything. Despite that smartphones are still unable to deter-
mine user’s mood unless manually entered. Automatic mood
extraction can be useful for many application, in particular for
monitoring mental wellness. We are interested in automatic
and continuous mood assessment using spontaneous speech.
Mood and emotion are often used interchangeably, but these
are different in several important aspects [1]. Emotion is rather
transient and discrete but mood is typically less intensely felt
by an individual and tends to last longer than emotion, e.g.,
persisting for days or hours instead of minutes or seconds.
Due to its long-lasting and private nature, mood reﬂects the
underlying feelings of people. Mood is usually described in
two dimensions: positive and negative. Mood inﬂuences the
expression of emotions [1]. In positive mood, people tend
to respond to emotion-eliciting probes normally, however, in
negative mood people often fail to express certain emotions;
such as smile control during watching comedy movie etc. [2].
We aim to use this phenomenon to infer mood. When the
user takes part in a phone conversation the system will use
the emotional construct of the speech of the person at the other
end, the listener, as the “contextual information”. If the listener
is talking about an exciting event the user is expected to be
excited or cheerful if he/she is in a positive mood, otherwise,

it would be assumed that the user is in a negative mood. We
call this concept “Context-aware mood mining”.

Context-aware mood mining is a completely new con-
cept. The closest “context-aware” approach that exists in the
literature [3] asks participants in a “controlled laboratory
environment” to read sentences containing emotionally arous-
ing “target” words (Example of an emotional sentence: The
parcel contained a bomb and example of a Neutral sentence:
The parcel contained a bowl). Then their vocal features are
manually analysed to evaluate the emotional response. In
this approach, the contextual information is predeﬁned as the
participants are given to read sentences with known affective
content. The proposed project will discover context dynam-
ically during phone conversation. Furthermore, the proposed
framework will conduct mood-mining seamlessly in an uncon-
trolled (home/ofﬁce/outdoor) natural environment. A number
of studies have emerged that have attempted to infer affective
states, especially “emotion”, on the smartphone e.g., [4], [5]
etc. While these studies open up avenues for new ways (which
yet need to be clinically validated) of inferring emotion, none
of them have considered the most crucial affective display -
human voice [6]. There exists a wealth of research that links
”voice evident changes in affect” directly to ”mental health
change” [3]. However, only recently, one study has determined
affective state from the voice on a mobile platform [7], but,
it has focused on emotion, not on mood. Also, robust audio
processing was not the focus of this study. Another recent
study has developed methods to infer mood [8] on a mobile
phone platform, but again this study did not consider voice.

II. PROBLEM DESCRIPTION AND METHODOLOGY

The underlying fundamental problem is to determine emo-
tion from speech from uncontrolled/unpredictable ambient
environment, where speech can be compounded with various

background noise (TV running in the background, multiple
people, children speaking in the background, trafﬁc noise etc.).
In addition, distance from the phone microphone cannot be
kept ﬁxed as well.

In order to address the above mentioned problems we
will develop a novel classiﬁcation framework which will
utilise Deep Belief Networks and Sparse Random Classiﬁer
jointly for robust and microphone location invariant emotion
classiﬁcation from the noisy audio signal.

A deep network is chosen due to its effectiveness in dis-
criminative feature learning. It is a directed graph, meaning
that each hidden unit is connected to many other hidden units
below it. So each hidden layer going further into the network
is a non-linear combination of the layers below it, because of
all the combining and recombining of the outputs from all the
previous units in combination with their activation functions.
Therefore, when the optimisation routine is applied to the net-
work, each hidden layer then becomes an optimally weighted,
non-linear combination of the layer below it. The latest study
with the best-reported results in emotion classiﬁcation from
the speech [9] has used a basic Deep Neural Network, which
has a limited capacity. In the proposed study a newer form
of Deep Neural Network (DNN) will be utilised, which is
known as Deep Belief Networks (DBN). A DBN is better than
a conventional DNN because it offers the layer-wise training
(known as “pretraining”) as opposed to the conventional “feed-
forward” training. The layer-wise training is unsupervised and
it does not require labeled data, therefore, a DBN can have a
deeper architecture compared to a DNN. A deeper architecture
offers a better representation of data compared to a shallow
architecture.

Apart from the capacity of producing a high level of dis-
criminative features, DBNs are also naturally robust to noise.
Deep network training can infer the most discriminative part of
the noise-corrupted acoustic features [10]. For instance, if the
training speech data corrupted by car noise, the deep network
training process will learn that the corruption is mainly in the
low-frequency part of the signal, and so the low-frequency
components of the speech features are deemphasised in the
car noise condition. Moreover, learning car noise may beneﬁt
other noise conditions where the corruption mainly resides in
low-frequency components (as the car noise), even though the
noise is not involved in the training.

The Sparse Random Classiﬁer (SRC) has been developed
extending the emerging theory of Compressive Sensing [?].
In his previous research,
the applicant has extended SRC
and showed that
it can outperform the powerful Support
Vector classiﬁers [?], [11], [12]. Consider that we have ni
training samples from emotion class i, i ∈ [1, 2, ..., k]. Also,
consider that each training sample is represented by an m
dimensional feature vector. Then the ni
training samples
from the i-th activity class can be represented by a matrix
Ai = [vi,1, vi,2, ..., vi,ni] ∈ Rm×ni. The underlying hypothesis
of SRC is that any test object y ∈ Rm from the same activity
class will approximately lie in the linear span of the training
samples associated with object i. Let deﬁne a new matrix A

for the entire training set as the concentration of the all n
training samples of k object classes: A = [A1, A2, ..., Ak] =
[v1,1, ..., v1,n1 , v2,1, ..., v2,n2 , vk,1, ..., vk,nk,].

Fig. 1. The joint DBN-SRC Classiﬁcation Framework for segment level
emotion classiﬁcation.

Then the linear representation of the training object y can be
rewritten in terms of training samples: y = Ax0 ∈ Rm, where
x0 = [0, ..., 0, αi,1, ..., αi,ni, 0, ..., 0]T ∈ Rn] is a coefﬁcient
vector whose entries needs to be zero except those associated
with i-th class. Finding such sparse solution (x0) is non-trivial
and only SRC could ﬁnd such solution. SRC uses an (cid:96)1-
minimiser to solve y = Ax0 ∈ Rm and it has been proven by
the theory of compressive sensing that (cid:96)1-norm can produce
the unique sparse solution, i.e., only entries related to the test
class are non-zero. When a sparse solution is sought, SRC
performs better than the powerful Support Vector Machine
(SVM) as, SVM employs an (cid:96)2-norm, which produces a rather
dense solution. For a classiﬁcation problem, a dense solution
will mean that the test class will be found associated with
multiple training classes.

As a very ﬁrst step, the utterance level speech signals are
divided into segments and then the segment-level features are
extracted for training a DBN. An optimal feature set will be
determined from the pool of available features including pitch,
energy and formant related features etc. For every instance
of the dataset, features would be extracted using the DBN
network and will be stored as a column of the “Training Matrix
A”. When a test speech arrives, similarly the features are ﬁrst
extracted and solved with regards to the “Training Matrix
A” to identify the test class. For the segment-level emotion
recognition, the training target is the label of the utterance. The
same labels are assigned to all the segments in one utterance.
Furthermore, since not all segments in an utterance contain
emotional information and it is reasonable to assume that
the segments with the highest energy contain most prominent
emotional information, segments with the highest energy in an
utterance will be chosen as the training samples [13].

[11] S. W. Chew, R. Rana, P. Lucey, S. Lucey, and S. Sridharan, “Sparse
temporal representations for facial expression recognition,” in Advances
in Image and Video Technology. Springer, 2012, pp. 311–322.

[12] B. Wei, M. Yang, Y. Shen, R. Rana, C. T. Chou, and W. Hu, “Real-
time classiﬁcation via sparse representation in acoustic sensor networks,”
in Proceedings of the 11th ACM Conference on Embedded Networked
Sensor Systems. ACM, 2013, p. 21.

[13] K. Han, D. Yu, and I. Tashev, “Speech emotion recognition using deep
neural network and extreme learning machine.” in Interspeech, 2014,
pp. 223–227.

[14] G.-B. Huang, Q.-Y. Zhu, and C.-K. Siew, “Extreme learning machine:
theory and applications,” Neurocomputing, vol. 70, no. 1, pp. 489–501,
2006.

Given the sequence of a probability distribution over the
emotion states generated from the above method, the ﬁnal
emotion recognition problem can be formed as a sequence
classiﬁcation problem, i.e., based on the unit (segment) infor-
mation, a decision needs to be made for the whole sequence
(utterance). Methods like the Extreme Learning Machine
(ELM) [14] will be investigated to determine utterance level
classiﬁcation. The output of this method needs to be for every
utterance a K-dimensional vector corresponding to the scores
of each emotion state. The emotion with the highest score can
be chosen as the recognition result for the utterance.

III. WORK IN PROGRESS

Currently, we are evaluating various integration strategies
of SRC and DBN and running simulation experiments to
ﬁnd a optimal feature set. As a next step we will impute
synthetic noise available from publicly available datasets
(http://parole.loria. fr/DEMAND/), which involves 18 types of
noises including white noise and noises at the cafeteria, car,
restaurant, train station, bus and park. We will then train the
classiﬁcation framework to handle noise. Signal-to-noise ratio
will be varied by varying the noise intensity (from the noise
dataset) in the speech signal and the optimal conﬁguration for
the DBN parameters (such as, number of layers, number of
hidden units per layer etc.) and optimal construction of the
training matrix for SRC will be learned through solving an
optimisation problem. A general model will be developed that
given the noisy intensity will be able to yield the optimal
conﬁguration of the DBN and SRC to maximise the accuracy.

REFERENCES

[1] D. Hume, “Emotions and moods,” Organizational behavior, pp. 258–

297, 2012.

[2] L. I. Reed, M. A. Sayette, and J. F. Cohn, “Impact of depression on
response to comedy: a dynamic facial coding analysis.” Journal of
abnormal psychology, vol. 116, no. 4, p. 804, 2007.

[3] N. Cummins, J. Epps, M. Breakspear, and R. Goecke, “An investigation
of depressed speech detection: Features and normalization.” in Inter-
speech, 2011, pp. 2997–3000.

[4] G. Bauer and P. Lukowicz, “Can smartphones detect stress-related
changes in the behaviour of individuals?” in Pervasive Computing
and Communications Workshops (PERCOM Workshops), 2012 IEEE
International Conference on.

IEEE, 2012, pp. 423–426.

[5] H. Lee, Y. S. Choi, S. Lee, and I. Park, “Towards unobtrusive emotion
recognition for affective social communication,” in Consumer Communi-
cations and Networking Conference (CCNC), 2012 IEEE.
IEEE, 2012,
pp. 260–264.

[6] Z. Zeng, M. Pantic, G. Roisman, and T. Huang, “A survey of affect
recognition methods: Audio, visual, and spontaneous expressions,” Pat-
tern Analysis and Machine Intelligence, IEEE Transactions on, vol. 31,
no. 1, pp. 39–58, Jan 2009.

[7] P. Georgiev, N. D. Lane, K. K. Rachuri, and C. Mascolo, “Dsp.
ear: leveraging co-processor support for continuous audio sensing on
smartphones,” in Proceedings of the 12th ACM Conference on Embedded
Network Sensor Systems. ACM, 2014, pp. 295–309.

[8] R. LiKamWa, Y. Liu, N. D. Lane, and L. Zhong, “Moodscope: building
a mood sensor from smartphone usage patterns,” in Proceeding of the
11th annual international conference on Mobile systems, applications,
and services. ACM, 2013, pp. 389–402.

[9] N. D. Lane and P. Georgiev, “Can deep learning revolutionize mobile

sensing?” 2015.

[10] S. Yin, C. Liu, Z. Zhang, Y. Lin, D. Wang, J. Tejedor, T. F. Zheng, and
Y. Li, “Noisy training for deep neural networks in speech recognition,”
EURASIP Journal on Audio, Speech, and Music Processing, vol. 2015,
no. 1, pp. 1–14, 2015.

