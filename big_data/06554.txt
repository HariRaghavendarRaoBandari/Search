6
1
0
2

 
r
a

 

M
1
2

 
 
]

V
C
.
s
c
[
 
 

1
v
4
5
5
6
0

.

3
0
6
1
:
v
i
X
r
a

Action-Aﬀect Classiﬁcation and Morphing
using Multi-Task Representation Learning

Timothy J. Shields(cid:63), Mohamed R. Amer(cid:63), Max Ehrlich, Amir Tamrakar

SRI International

Abstract. Most recent work focused on aﬀect from facial expressions,
and not as much on body. This work focuses on body aﬀect analysis.
Aﬀect does not occur in isolation. Humans usually couple aﬀect with
an action in natural interactions; for example, a person could be talk-
ing and smiling. Recognizing body aﬀect in sequences requires eﬃcient
algorithms to capture both the micro movements that diﬀerentiate be-
tween happy and sad and the macro variations between diﬀerent actions.
We depart from traditional approaches for time-series data analytics by
proposing a multi-task learning model that learns a shared representation
that is well-suited for action-aﬀect classiﬁcation as well as generation. For
this paper we choose Conditional Restricted Boltzmann Machines to be
our building block. We propose a new model that enhances the CRBM
model with a factored multi-task component to become Multi-Task Con-
ditional Restricted Boltzmann Machines (MTCRBMs). We evaluate our
approach on two publicly available datasets, the Body Aﬀect dataset and
the Tower Game dataset, and show superior classiﬁcation performance
improvement over the state-of-the-art, as well as the generative abilities
of our model.

Keywords: Body Aﬀect; Multi-Task Learning; Conditional Restricted
Boltzmann Machines; Deep Learning;

1 Introduction

There has been so much activity in the ﬁeld of aﬀective computing that it al-
ready contributed to the creation of new research directions in aﬀect analysis [1].
There are multiple research directions for analyzing human aﬀect, including face
data [2], audiovisual data [3], and body data [4]. One of the main challenges of
aﬀect analysis is that it does not occur in isolation. Humans usually couple aﬀect
with an action in natural interactions; for example, a person could be talking
and smiling, or knocking on a door angrily as shown in Fig. 1. To be able to
recognize body action-aﬀect pairs, eﬃcient temporal algorithms are needed to
capture the micro movements that diﬀerentiate between happy and sad as well
as capture the macro variations between the diﬀerent actions. The focus of our
work is on single-view, multi-task action-aﬀect recognition from skeleton data

(cid:63) Both authors equally contributed to this work

2

Timothy J. Shields

Fig. 1. Examples from the Body Aﬀect dataset [7] of a person Knocking with various
aﬀects: Neutral, Angry, Happy, and Sad. The trajectory color corresponds to time,
where black is the beginning of the sequence, reddish-black is the middle, and red is
the end of the sequence. (This ﬁgure is best visualized in color.)

captured by motion capture or Kinect sensors. Our work leverages the knowl-
edge and work done by the graphics and animation community [5,6,7] and uses
machine learning to enhance it and make it accessible for a wide variety of ap-
plications. We use the Body Aﬀect dataset produced by [7] and the Tower Game
[8] dataset as the test cases for our novel multi-task approach. Time series anal-
ysis is a diﬃcult problem that requires eﬃcient modeling, because of the large
amounts of data it introduces. There are multiple approaches that designed fea-
tures to reduce the data dimensionality, using mid-level features, and then use
a simpler model to do classiﬁcation [9,10]. We depart from these methods by
proposing a model that learns a shared representation using multi-task learning.
For this paper we choose Conditional Restricted Boltzmann Machines, which
are non-linear generative models for modeling time series data, as our building
block. They use an undirected model with binary latent variables connected to
a number of visible variables. A CRBM-based generative model enables model-
ing short-term phenomenon. We propose a new hybrid model that enhances the
CRBM model with multi-task, discriminative, components based on the work
of [11]. This work leads to a superior classiﬁcation performance, while also al-
lowing us to model temporal dynamics eﬃciently. We evaluate our approach on
the Body Aﬀect [7] and Tower Game [8] datasets and show how our results are
superior to the state-of-the-art.

Our contributions:

– Multi-task learning model for unimodal and multimodal time-series data.
– Method for applying aﬀect to a neutral skeleton (Sequence Morphing).

Action-Aﬀect Classiﬁcation and Morphing

3

– Evaluations on two multi-task public datasets [7,8].

Paper organization: In sec. 2 we discuss prior work. In sec. 3 we give a brief back-
ground of similar models that motivate our approach, followed by a description
of our model. In sec. 4 we describe the inference algorithm. In sec. 5 we specify
our learning algorithm. In sec. 6 we show quantitative results of our approach,
followed by the conclusion in sec. 7.

2 Prior work

In this section we ﬁrst review literature on activity recognition in RGB-D and
Motion Capture Sequences; second we review Multi-Task Learning approaches;
ﬁnally we review temporal, energy-based, representation learning.

Body Aﬀect Analysis: Initial work on activity recognition in RGB-D se-
quences has been popular in recent years with the availability of cheap depth
sensors. Since initial work [12], there have been an increasing number of ap-
proaches addressing the problem of activity recognition using skeletal data [9].
Prior to activity recognition in RGB-D sequences, datasets were captured using
motion capture sensors. During that time, research focused on graphics appli-
cations such as generating animation and transitions between animations using
signal processing techniques rather than machine learning or computer vision.
Their main goal was to generate natural looking skeletons for animation. Some
methods used knowledge of signal processing to transform a neutral skeleton
pose to reﬂect a certain emotion [5]. These methods were very constrained to
the type of motion and were engineered to reproduce the same motions. Other
work used a language based modeling of aﬀect [6] where they modeled actions
(verbs) and aﬀect (adverbs) using a graph. They were able to produce results us-
ing a combination of low level functions to interpolate between example motions.
More recent work [13] modeling non-stylized motion for aﬀect communication
used segmentation techniques which divided complex motions into a set of mo-
tion primitives that they used as dynamic features. Unlike our approach, their
mid-level features were hand engineered rather than learned, which is very lim-
ited, does not scale and is prone to feature design ﬂaws. More recent work such
as [7] collected natural body aﬀect datasets where they have varied identity,
gender, emotion, and actions of the actors but not used it for classiﬁcation.

Multi-Task Learning: Multi-task learning is a natural approach for problems
that require simultaneous solutions of several related problems [14]. Multi-task
learning approaches can be grouped into two main sets. The ﬁrst set focuses
on regularizing the parameter space. The main assumption is that there is an
optimal shared parameter space for all tasks. These approaches regularize the
parameter space by using a speciﬁc loss [15], methods that manually deﬁne re-
lationships [16], or more automatic ways that estimate the latent structure of
relationships between tasks [17,18,19,20,21]. The second set focuses on correlat-
ing relevant features jointly [22,23,24,25]. Other work focused on the schedule

4

Timothy J. Shields

of which tasks should be learned [26]. Multi-task learning achieved good re-
sults on vision problems such as: person re-identiﬁcation [27], multiple attribute
recognition [28], and tracking [29]. Recently, Deep Multi-Task Learning (DMTL)
emerged with the rise of deep learning. Deep Neural Networks (DNNs) were used
to address multi-task learning and were applied successfully to facial landmark
detection [30], scene classiﬁcation [31], object localization and segmentation [32]
and attribute prediction [33]. Other work used multi-task autoencoders [34] for
object recognition in a generalized domain [35], where the tasks were the dif-
ferent domains. Other work used multi-task RNNs for interaction prediction in
still images [36]. Most of the Deep Multi-task Learning approaches only focused
on using DNN-based models applied to still images. Our approach is the ﬁrst
DMTL for temporal and multimodal sequence analysis.

Representation Learning: Deep learning has been successfully applied to
many problems [37]. Restricted Boltzmann Machines (RBMs) form the building
blocks in energy-based deep networks [38,39]. In [38,39], the networks are trained
using the Contrastive Divergence (CD) algorithm [40], which demonstrated the
ability of deep networks to capture the distributions over the features eﬃciently
and to learn complex representations. RBMs can be stacked together to form
deeper networks known as Deep Boltzmann Machines (DBMs), which capture
more complex representations. Recently, temporal models based on deep net-
works have been proposed, capable of modeling a rich set of time series analysis
problems. These include Conditional RBMs (CRBMs) [41] and Temporal RBMs
(TRBMs) [42,43,44]. CRBMs have been successfully used in both visual and
audio domains. They have been used for modeling human motion [41], tracking
3D human pose [45], and phone recognition [46]. TRBMs have been applied for
transferring 2D and 3D point clouds [47], and polyphonic music generation [48].

3 Model

Rather than immediately deﬁning our Multi-Task CRBM (MT-CRBM) model,
we discuss a sequence of models, gradually increasing in complexity, such that the
diﬀerent components of our ﬁnal model can be understood in isolation. We start
with the basic RBM model (sec. 3.1), then we extend the RBM to the CRBM
model (sec. 3.2), then we further extend the CRBM to a new discriminative (D-
CRBM) model (sec. 3.3), then we extend the D-CRBM to our main multi-task
model (MT-CRBM) (sec. 3.4), and ﬁnally we deﬁne a multi-task multimodal
model (MTM-CRBM) (sec. 3.5).

3.1 Restricted Boltzmann Machines

RBMs [39], shown in Figure 2(a), deﬁne a probability distribution pR as a Gibbs
distribution (1), where v is a vector of visible nodes, h is a vector of hidden nodes,
ER is the energy function, and Z is the partition function. The parameters θ to
be learned are a and b, the biases for v and h respectively, and the weights W .

Action-Aﬀect Classiﬁcation and Morphing

5

(a) RBM

(b) CRBM

(c) D-CRBM

Fig. 2. The deep learning models described in sections 3.1, 3.2, and 3.3: (a) RBM
(b) CRBM (c) D-CRBM.

The RBM is fully connected between layers, with no lateral connections. This
architecture implies that v and h are factorial given one of the two vectors. This
allows for the exact computation of pR(v|h) and pR(h|v).

pR(h, v) = exp[−ER(h,v)]

,

Z(θ)

Z(θ) =(cid:80)

h,v exp[−ER(h, v)],

(cid:34){a, b}

(cid:35)

θ =

{W } -fully connected.

-bias,

(1)

In case of binary valued data vi is deﬁned as a logistic function. In case of real
valued data, vi is deﬁned as a multivariate Gaussian distribution with a unit
covariance. A binary valued hidden layer hj is deﬁned as a logistic function such
that the hidden layer becomes sparse [41,49]. The probability distributions over
v and h are deﬁned as in (2).

pR(vi = 1|h) = σ(ai +(cid:80)
pR(vi|h) = N (ai +(cid:80)
pR(hj = 1|v) = σ(bj +(cid:80)
−(cid:88)

(ai − vi)2

(cid:88)

ER(h, v) =

2

i

j hjwij),

Binary,

j hjwij, 1), Real,

(2)

i viwij),

Binary.

bjhj −(cid:88)

j

i,j

viwijhj

(3)

The energy function ER for the real valued v is deﬁned as in (3).

3.2 Conditional Restricted Boltzmann Machines

CRBMs [41] are a natural extension of RBMs for modeling short term tempo-
ral dependencies. A CRBM, shown in Figure 2(b), is an RBM which takes into
account history from the previous N time instances, t − N, . . . , t − 1, when con-
sidering time t. This is done by treating the previous time instances as additional
inputs. Doing so does not complicate inference. Some approximations have been
made to facilitate eﬃcient training and inference, more details are available in

6

Timothy J. Shields

[41]. A CRBM deﬁnes a probability distribution pC as a Gibbs distribution (4).

pC(ht, vt|v<t) = exp[−EC(vt,ht|v<t)]
,
h,v exp[−EC(ht, vt|v<t)],

Z(θ) =(cid:80)

Z(θ)

θ =

-bias,

{A, B} -auto regressive,
{W } -fully connected.

(4)

(cid:34) {a, b}

(cid:35)

The visible vectors from the previous N time instances, denoted as v<t, inﬂuence
the current visible and hidden vectors. The probability distributions are deﬁned
in (5).

pC(vi|h, v<t) = N (ci +(cid:80)
pC(hj = 1|v, v<t) = σ(dj +(cid:80)
ci = ai +(cid:80)
dj = bj +(cid:80)
EC(ht, vt|v<t) =(cid:80)

i(ci − vi,t)2/2 −(cid:80)

p Apivp,<t ,

j djhj,t −(cid:80)

j hjwij, 1),

i viwij),

p Bpjvp,<t.

The new energy function EC(ht, vt|v<t) in (6) is deﬁned in a manner similar to
that of the RBM (3).

i,j vi,twijhj,t,

(6)

(5)

Note that A and B are matrices deﬁning dynamic biases for vt and ht, consisting
of concatenated vectors of previous time instances of a and b.

3.3 Discriminative Conditional Restricted Boltzmann Machines

Z(θ) =(cid:80)

We extend the CRBMs to the D-CRBMs shown in Figure 2(c). D-CRBMs are
based on the D-RBM model presented in in [11], generalized to account for tem-
poral phenomenon using CRBMs. D-CRBMs deﬁne the probability distribution
pDC as a Gibbs distribution (7).
pDC(yt, ht, vt|v<t) = exp[−EDC(yt,ht,vt|v<t)]
,
y,h,v exp[−EDC(yt, ht, vt|v<t)],

{A, B} -auto regressive,
{W , U} -fully connected.
(7)
The probability distribution over the visible layer will follow the same distribu-
tions as in (5). The hidden layer h is deﬁned as a function of the labels y and
the visible nodes v. A new probability distribution for the classiﬁer is deﬁned to
relate the label y to the hidden nodes h (8).

(cid:34){a, b, s}

-bias,

(cid:35)

θ =

Z(θ)

pDC(vi,t|ht, v<t) = N (ci +(cid:80)
pDC(hj,t = 1|yt, vt, v<t) = σ(dj +(cid:80)
exp[sk+(cid:80)
k∗ exp[sk∗ +(cid:80)

pDC(yk,t|h) =

(cid:80)

j hjwij, 1),

k yk,tujk +(cid:80)

j ujkhj ]

j ujk∗ hj ] .

i vi,twij),

(8)

Action-Aﬀect Classiﬁcation and Morphing

7

(a) MT-CRBMs

Fig. 3. Models described in sections 3.4 and 3.5. (a) MT-CRBMs (b) MTM-CRBMs.
The MT-CRBMs learn a shared representation layer for all tasks. In addition to the
shared layer, the MTM-CRBMs learn an extra representation layer for each of the
modalities, which learn modality-speciﬁc representations.

(b) MTM-CRBMs

The new energy function EDC is deﬁned as in (9).

EDC(yt, ht, vt|v<t) = EC(ht, vt|v<t)

(cid:124)

(cid:123)(cid:122)

Generative

−(cid:88)
(cid:124)

k

skyk,t −(cid:88)
(cid:123)(cid:122)

j,k

(cid:125)

Discriminative

hj,tujkyk,t

(9)

(cid:125)

3.4 Multi-Task Conditional Restricted Boltzmann Machines

In the same way the CRBMs can be extended to the DC-RBMs by adding a
discriminative term to the model, we can extend the CRBMs to be multi-task
MT-CRBMs Figure 3(a). MTCRBMs deﬁne the probability distribution pMT as
a Gibbs distribution (10). The MT-CRBMs learn a shared representation layer
for all tasks.

pMT(yL

Z(θ) =(cid:80)

t , ht, vt|v<t) = exp[−EDC(yL

t ,ht,vt|v<t)]
Z(θ)

,

t , ht, vt|v<t)],

y,h,v exp[−EMT(yL

{A, B} -auto regressive,
{W , U L} -fully connected.
(10)
The probability distribution over the visible layer will follow the same distri-
butions as in (8). The hidden layer h is deﬁned as a function of the multi-task
labels yL and the visible nodes v. A new probability distribution for the multi-
task classiﬁer is deﬁned to relate the multi-task labels yL to the hidden nodes h

θ =

-bias,

(cid:34){a, b, sL}

(cid:35)

8

Timothy J. Shields

as shown in (11).

pMT(hj,t = 1|yL

pMT(yl

pMT(vi,t|ht, v<t) = N (ci +(cid:80)
t , vt, v<t) = σ(dj +(cid:80)
k+(cid:80)
k∗ +(cid:80)
−(cid:88)
(cid:124)

k,t|h) =

exp[sl
k∗ exp[sl

(cid:125)
t , ht, vt|v<t) = EC(vt, ht|v<t)

(cid:123)(cid:122)

(cid:124)

Generative

(cid:80)

k,l

The energy for the model shown in Figure 3(a), EMT, is deﬁned as in (12).

EMT(yL

sl
kyl

hj,tujkyl

k,t

j hjwij, 1),

jk +(cid:80)

l,k yl

k,tul

i vi,twij),

j ul

jkhj ]
j ul

jk∗ hj ] .

k,t −(cid:88)
(cid:123)(cid:122)

j,k,l

Multi-Task

(cid:125)

(11)

(12)

3.5 Multi-Task Multimodal Conditional Restricted Boltzmann

Machines

We can naturally extend MT-CRBMs to MTM-CRBMs. A MTM-CRBMs com-
bines a collection of unimodal MT-CRBMs, one for each visible modality. The
hidden representations produced by the unimodal MT-CRBMs are then treated
as the visible vector of a single fusion MT-CRBMs. The result is a MTMCRBM
model that relates multiple temporal modalities to multi-task classiﬁcation la-
bels. MTMCRBMs deﬁne the probability distribution pMTM as a Gibbs distribu-
tion (13). The MTM-CRBMs learn an extra representation layer for each of the
modalities, which learns a modality speciﬁc representation as well as the shared
layer for all the tasks.

pMTM(yL

t , ht, h1:M

t

t , ht, h1:M

t

|v1:M

<t )]/Z(θ),

, v1:M

Z(θ) =(cid:80)

t

|v1:M
<t ) = exp[−EMTM(yL
y,v,h exp[−EMTM(yL
(cid:34) {a1:M , b1:M , e, sL}

t , ht, h1:M

t

, v1:M

t

θ =

{A1:M , B 1:M , C 1:M}
-auto regressive,
{W 1:M , U 1:M , W , U L} -fully connected.

-bias,

t

, v1:M
|v1:M
<t ),

(cid:35)

(13)
Similar to the MT-CRBMs(11), the hidden layer h is deﬁned as a function of
the labels yL and the visible nodes v. A new probability distribution for the
classiﬁer is deﬁned to relate the label yL to the hidden nodes h is deﬁned as in

Action-Aﬀect Classiﬁcation and Morphing

9

(14).

pMTM(vm

pMTM(hm

i,t|hm
t , vm
j,t = 1|yL

pMTM(yl

k,t|hm

t ) =

<t) = N (cm

t , vm
t , vm
(cid:80)

exp[sl
l∗ exp[sl

pMTM(hn,t = 1|yL
t , h1:M
(cid:80)

k,t|h) =

pMTM(yl

t

exp[sl
k∗ exp[sl

ij , 1),

j hm

j wm

<t) = σ(dm

i +(cid:80)
j +(cid:80)
k+(cid:80)
k∗ +(cid:80)
<t ) = σ(fn +(cid:80)
k+(cid:80)
k∗ +(cid:80)

jk hm
j,t]
j um,l

nk∗ hn] .

nkhn]
n ul

jk∗ hm
j,t]

, h1:M

j um,l

j ul

,

where,

l,k yl

k,tul

i vm

i,twm

ij ),

jk +(cid:80)
nk +(cid:80)

l,k yl

k,tul

m,j hm

j,twm

jn),

cm
i = am

i +(cid:80)
j +(cid:80)
fn = en +(cid:80)

dm
j = bm

p Am

p,ivm

p,<t,

p Bm

p,jvp,<t,

m,r C m

r,nhm

r,<t.

(14)

(15)

(16)

The new energy function EMTM is deﬁned in (16) similar to that of the MT-
CRBMs (10).

EMTM(yL

t , ht, h1:M

t

, v1:M

t

|v1:M

<t ) =

EMT(yL

t , hm

t , vm

t |vm
<t)

m

(cid:88)
(cid:124)
−(cid:88)
(cid:124)

k,l

Unimodal

(cid:123)(cid:122)
k,t −(cid:88)
(cid:123)(cid:122)

n,k,l

Multi-Task

sl
kyl

hn,tul

nkyl

k,t

(cid:125)
(cid:125)

−(cid:88)
(cid:124)

j

fnhn,t − (cid:88)
(cid:123)(cid:122)

j,k,m

Fusion

hm
j,twjnhn,t

(cid:125)

4 Inference

We ﬁrst discuss inference for the MTM-CRBM since it is the most general case.
To perform classiﬁcation at time t in the MTM-CRBM given v1:M
we
use a bottom-up approach, computing the mean of each node given the activation
coming from the nodes below it; that is, we compute the mean of hm
t using vm
<t
and vm
<t , then
t
we compute the mean of yL
for each task using ht, obtaining the classiﬁcation
t
probabilities for each task. Figure 4 illustrates our inference approach. Inference
in the MT-CRBM is the same as the MTM-CRBM, except there is only one
modality, and inference in the D-CRBM is the same as the MT-CRBM, except
there is only one task.

for each modality, then we compute the mean of ht using h1:M

<t and v1:M

t

10

Timothy J. Shields

(a) Unimodal MTCRBM

(b) Fusion MTCRBM

Fig. 4. This ﬁgure speciﬁes the inference algorithm. We ﬁrst classify the unimodal
data by activating the corresponding hidden layers hm
t as shown in (a), followed by
classifying the multimodal data by activating the fusion layer ht as shown in (b).

5 Learning

Learning our model is done using Contrastive Divergence (CD) [40], where
(cid:104)·(cid:105)data is the expectation with respect to the data and (cid:104)·(cid:105)recon is the expec-
tation with respect to the reconstruction. The learning is done using two steps:
a bottom-up pass and a top-down pass using sampling equations from (8) for
D-CRBM, (11) for MT-CRBM, and (14) for MTM-CRBM. In the bottom-
up pass the reconstruction is generated by ﬁrst sampling the unimodal layers
p(hm
<t, yl) for all the hidden nodes in parallel. This is followed by
sampling the fusion layer p(ht,n = 1|yL
<t ). In the top-down pass the
unimodal layer is generated using the activated fusion layer p(hm
k,t).
t,i|hm
This is followed by sampling the visible nodes p(vm
<t) for all the visible
nodes in parallel. The gradient updates are described in (17). Similarly learning
of D-CRBM and MT-CRBM could be done.

t,j = 1|ht, yL

t,j = 1|vm

t , vm

k,t, h1:M

t

, h1:M

t , vm

− (cid:104)vm
i (cid:105)recon,
∝ (cid:104)vm
i (cid:105)data
j (cid:105)recon,
− (cid:104)hm
j (cid:105)data
∝ (cid:104)hm
− (cid:104)hn(cid:105)recon,
∝ (cid:104)hn(cid:105)data
− (cid:104)yl
k(cid:105)recon,
∝ (cid:104)yl
k(cid:105)data
i,t(cid:105)recon),
i,t(cid:105)data − (cid:104)vm
k,<t((cid:104)vm
p,i,<t ∝ vm
j,t(cid:105)recon),
j,t(cid:105)data − (cid:104)hm
i,<t((cid:104)hm
p,j,<t ∝ vm
j,<t((cid:104)hn,t(cid:105)data − (cid:104)hn,t(cid:105)recon),
r,n,<t ∝ hm
− (cid:104)vm
∝ (cid:104)vm
j (cid:105)data
j (cid:105)recon,
i hm
i hm
j hn(cid:105)recon,
∝ (cid:104)hm
j hn(cid:105)data
− (cid:104)hm
j (cid:105)recon,
− (cid:104)yl
j (cid:105)data
∝ (cid:104)yl
khm
khm
khn(cid:105)recon.
khn(cid:105)data
∝ (cid:104)yl
− (cid:104)yl

∆ai
∆bj
∆en
∆sl
k
∆Am
∆Bm
∆C m
∆wm
i,j
∆wj,k
∆ul,m
jk
∆uL
nk

(17)

Action-Aﬀect Classiﬁcation and Morphing

11

6 Experiments

We now describe the datasets in (sec 6.1), specify the implementation details in
(sec 6.2), and present our quantitative results in (sec 6.3).

6.1 Datasets

Our problem is very particular in that we focus on multi-task learning for body
aﬀect. In the literature [4,9] most of the datasets were either single task for
activity recognition, not publicly available, too few instances, or only RGB-D
without skeleton. We found two available datasets to evaluate our approach that
are multi-task. The ﬁrst dataset is the Body Aﬀect dataset [7], collected using
a motion capture sensor, which consists of a set of actors performing several ac-
tions with diﬀerent aﬀects. The second dataset is the Tower Game [8], collected
using a Kinect sensor, which consists of an interaction between two humans per-
forming a cooperative task, with the goal of classifying diﬀerent components of
entrainment. In the following subsections we describe the datasets.

Body Aﬀect Dataset: This dataset [7] consists of a library of human move-
ments captured using a motion capture sensor, annotated with actor, action,
aﬀect, and gender. The dataset was collected for studying human behavior and
personality properties from human movement. The data consists of 30 actors (15
female and 15 male) each performing four actions (walking, knocking, lifting, and
throwing) with each of four aﬀect styles (angry, happy, neutral, and sad). For
each actor, there are 40 data instances: 8 instances of walking (2 directions x 4
aﬀects), 8 instances of knocking (2 repetitions x 4 aﬀects), 8 instances of lifting
(2 repetitions x 4 aﬀects), 8 instances of throwing (2 repetitions x 4 aﬀects), and
8 instances of the sequences (2 repetitions x 4 aﬀects). For knocking and lifting
and throwing there were 5 repetitions per data instances. Thus, the 24 records of
knocking, lifting, and throwing contain 120 separate instances, yielding a total
of 136 instances per actor and a total of 4,080 instances. We split dataset into
50% training using 15 actors and 50% testing using the other 15 actors.

Tower Game Dataset: This dataset [8] is a simple game of tower building often
used in social psychology to elicit diﬀerent kinds of interactive behaviors from
the participants. It is typically played between two people working with a small
ﬁxed number of simple toy blocks that can be stacked to form various kinds of
towers. The data consists of 112 videos which were divided into 1213 10-second
segments indicating the presence or absence of these behaviors in each segment.
Entrainment is the alignment in the behavior of two individuals and it involves
simultaneous movement, tempo similarity, and coordination. Each measure was
rated low, medium, or high for the entire 10 seconds segment. 50% of that data
was used for training and 50% were used for testing. In this dataset we call
each person’s skeletal data a modality, where our goal is to model mocap-mocap
representations.

12

Timothy J. Shields

6.2 Implementation Details

For pre-processing the Tower Game dataset, we followed the same approach as
[50] by forming a body centric transformation of the skeletons generated by the
Kinect sensors. We use the 11 joints from the upper body of the two players since
the tower game almost entirely involves only upper body actions and gestures
are done using the upper body. We used the raw joint locations normalized
with respect to a selected origin point. We use the same descriptor provided by
[51,52]. The descriptor consists of 84 dimensions based on the normalized joints
location, inclination angles formed by all triples of anatomically connected joints,
azimuth angles between projections of the second bone and the vector on the
plane perpendicular to the orientation of the ﬁrst bone, bending angles between
a basis vector, perpendicular to the torso, and joint positions. As for the Body
Aﬀect dataset we decided to use the full body centric representation [53] for
motion capture sensors resulting in 42 dimensions per frame.
For the Body Aﬀect dataset we trained a three-task model for the following
three tasks: Action (AC) ∈ {Walking, Knocking, Lifting, Throwing}, Aﬀect (AF)
∈ {Neutral, Happy, Sad, Angry}, Gender (G) ∈ {Male, Female}. The data is split
into a training set consisting of 50% of the instances, and a test set consisting
of the remaining 50%. For the Tower Game dataset we trained a three-task
model for the following tasks,: Tempo Similarity (TS), Coordination (C), and
Simultaneous Movement (SM), each in {Low, Medium, High}. The data is split
into a training set consisting of 50% of the instances, and a test set consisting
of the remaining 50%.

We tuned our model parameters. For selecting the model parameters we
used a grid search. We varied the number of hidden nodes per layer in the
range of {10, 20, 30, 50, 70, 100, 200}, as well as the auto-regressive nodes in the
range of {5, 10}, resulting a total of 2744 trained models. The best performing
model on the Body Aﬀect dataset has the following conﬁguration v = 42, h =
30, v<t = 42 × 10 and the best performing model on the Tower Game dataset
<t = 10 × 84 for each of the
has the following conﬁguration vm = 84, hm = 30, vm
modalities and for the fusion layer in the Tower Game dataset h1:M = 60, h =
60, h1:M

needed for the hidden-label edges is H ·(cid:80)L
product, H·(cid:81)L

<t = 10 × 60.
Note that in our MT-CRBM model, the tasks are assumed conditionally
independent given the hidden representation. Thus the number of parameters
k=1 Yk, where H is the dimensionality
of the hidden layer and Yk is the number of classes for task k. Contrast this to
the number of parameters needed if instead the tasks are ﬂattened as a Cartesian
k=1 Yk. Our factored representation of the multiple tasks uses only
linearly many parameters instead of the exponentially many parameters needed
for the ﬂattened representation.

6.3 Quantitative Results

We ﬁrst deﬁne baselines and variants of the model, followed by the average
classiﬁcation accuracy results on the two datasets, and ﬁnally we provide some

Action-Aﬀect Classiﬁcation and Morphing

13

(a) MT-CRBMs-Deep

Fig. 5. Deep variants of the models presented in sections 3.4 and 3.5. (a) MT-CRBMs-
Deep (b) MTM-CRBMs-Deep. The Deep variants add an extra representation layer for
each of the tasks, which learns a task speciﬁc representation.

(b) MTM-CRBM-Deep

generative results, which we call Morphing, on the Body Aﬀect dataset.

Baselines and Variants: Since we compare our approach against the results
presented in [8] we decided to use the same baselines they used. They used SVM
classiﬁers on a combination of features. SVM+RAW: The ﬁrst set of features
consisted of ﬁrst order static and dynamic handcrafted skeleton features. The
static features are computed per frame. The features consist of relationships be-
tween all pairs of joints of a single actor, and the relationships between all pairs of
joints of both the actors. The dynamic features are extracted per window (a set
of 300 frames). In each window, they compute ﬁrst and second order dynamics
of each joint, as well as relative velocities and accelerations of pairs of joints per
actor, and across actors. The dimensionality of their static and dynamic features
is (257400 D). SVM+BoW100 and SVM+BoW300: To reduce their dimension-
ality they used, Bag-of-Words (BoW) (100 and 300 D) [54,52]. We also evaluate
our approach using HCRF [55]. We deﬁne our own model’s variants, D-CRBMs
which is our single-task model presented in Section 3.3, MT-CRBMs which is
our multi-task model presented in Section 3.4, MTM-CRBMs the multi-modal
multi-task model presented in Section 3.5 and DM-CRBMs an extension to the
D-CRBMs to be multimodal similar to MTM-CRBMs. We also add two new
variants1 MT-CRBMs-Deep and MTM-CRBMs-Deep shown in Fig.5, which are

1 This model is initially prototyped by [56] in the deep learning book.

14

Timothy J. Shields

a deeper version of the original models, by adding a task speciﬁc representation
layer.

Classiﬁcation: For the Body Aﬀect dataset, Table 1 shows the results of the
baselines as well as our model and its variants. For the Tower Game dataset,
Table 2 shows our average classiﬁcation accuracy using diﬀerent features and
baselines combinations as well as the results from our models. We can see that
the MT-CRBMs-Deep model outperforms all the other models for both cases,
thereby demonstrating its eﬀectiveness on predicting multi-task labels correctly.
Furthermore, the MTM-CRBMs-Deep model outperforms all the SVM variants
which used high dimensional handcrafted features, demonstrating its ability to
learn a rich representation starting from the raw skeleton features. Note that
only the MTM-CRBMs and MTM-CRBMs-Deep performed well on predicting
the diﬀerent tasks simultaneously with a relatively large margin better than the
other models, using a shared representation that uses less parameters than our
D-CRBMs model that treats all the labels ﬂat.

Morphing: Besides classifying action and aﬀect, the MT-CRBMs model trained
on the body aﬀect dataset is also capable of generation. We demonstrate this
by morphing a motion capture sequence of one aﬀect to the same sequence with
a diﬀerent aﬀect. For example, we could morph a Neutral Walk into a Happy
Walk. We morph a sequence by sweeping through its frames, updating the vt
vector of each frame in order. To update vt, we ﬁrst compute the expected value
of ht given v<t, vt and yL
t , then compute the expected value of vt given v<t
and ht. The v<t used is a linear blend of the original sequence and the newly
generated sequence, so that the generated sequence retains the general shape
of the original sequence. To evaluate the morphing process, we take a Neutral
sequence for each action and each actor; morph it to a Happy, Sad, or Angry
sequence of the same action type; and then compare the classiﬁer probability
of the target aﬀect for the original Neutral sequence and the generated Happy,
Sad, or Angry sequence. The average classiﬁer probabilities before and after the
morphing process for each action and target aﬀect are shown in Table 3. Most of
the classiﬁcation probabilities for target aﬀects increased as a result of morphing
the Neutral sequences toward them, which means that our model was able to
morph the sequences successfully.

7 Conclusion and Future Work

We have proposed a collection of hybrid models, both discriminative and gener-
ative, that model the relationships in and distributions of temporal, multimodal,
multi-task data. An extensive experimental evaluation of these models on two
diﬀerent datasets demonstrates the superiority of our approach over the state-
of-the-art for multi-task classiﬁcation of temporal data. This improvement in
classiﬁcation performance is accompanied by new generative capabilities and an
eﬃcient use of model parameters via factorization across tasks.

Action-Aﬀect Classiﬁcation and Morphing

15

Table 1. Average Classiﬁcation Accuracy on The Body Aﬀect Dataset.

Classiﬁer (labels) AC(4) AF(4) G(2)
25.0
25.0 50.0
Random Guess
32.2 65.1
35.6
SVM+Raw
34.1 71.4
SVM+BoW100
41.3
32.8 69.5
SVM+BoW300[52] 39.9
HCRF[55]
44.8
34.7 74.1
30.7 78.4
52.6
D-CRBMs
31.2 78.2
MT-CRBMs
53.5
MT-CRBMs-Deep 54.5
32.7 78.4

Table 2. Average Classiﬁcation Accuracy on The Tower Game Dataset.

TS (3) C (3) SM (3)

Classiﬁer (labels)
33.3
Random Guess
59.3
SVM+Raw [8]
65.6
SVM+BoW100 [8]
54.4
SVM+BoW300 [52]
67.2
HCRF[55]
76.5
DM-CRBMs
MTM-CRBMs
86.2
MTM-CRBMs-Deep 87.2

33.3
52.2
55.8
47.5
58.8
62.0
70.0
70.0

33.3
39.5
44.3
42.8
44.5
49.2
63.5
72.8

Table 3. Aﬀect Classiﬁcation Before and After Morphing

AC/AF Happy-Before Happy-After Sad-Before Sad-After Angry-Before Angry-After
Knock
Lift
Throw
Walk

32.0
18.0
12.9
36.7

15.5
14.7
8.6
8.0

33.3
19.7
13.9
46.9

14.6
11.3
8.1
4.3

15.2
21.1
13.3
4.3

15.2
9.1
6.9
4.1

The generative capabilities of our approach enable new and interesting appli-
cations, such as the demonstrated sequence morphing. A future direction of work
is to further explore and improve these generative applications of the models.

The factorization of tasks used in our approach means the number of param-
eters grows only linearly with the number of tasks and classes. This is seen to be
signiﬁcant when contrasted with a single-task model that uses a ﬂattened Carte-
sian product of tasks, where the number of parameters grows exponentially with
the number of tasks. Our factorized approach makes adding additional tasks a
trivial matter.

Acknowledgments

This research was partially developed with funding from the Defense Advanced
Research Projects Agency (DARPA) and the Air Force Research Laborotory

16

Timothy J. Shields

(AFRL). The views, opinions and/or ﬁndings expressed are those of the authors
and should not be interpreted as representing the oﬃcial views or policies of the
Department of Defense or the U.S. Government

Action-Aﬀect Classiﬁcation and Morphing

17

References

1. Picard, R.W.: Aﬀective Computing. MIT Press (1995)
2. Calvo, R., D’Mello, S., Gratch, J., Kappas, A., Cohn, J.F., Torre, F.D.L.: Auto-

mated face analysis for aﬀective computing (2014)

3. Zeng, Z., Pantic, M., Roisman, G.I., Huang, T.S.: A survey of aﬀect recognition
methods: Audio, visual, and spontaneous expressions. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence 31(1) (2009) 39–58

4. Kleinsmith, A., Bianchi-Berthouze, N.: Aﬀective body expression perception and
IEEE Transactions on Aﬀective Computing 4(1) (2013)

recognition: A survey.
15–33

5. Amaya, K., Bruderlin, A., Calvert, T.: Emotion from motion. In: GI. (1996)
6. Rose, C., Bodenheimer, B., Cohen, M.F.: Verbs and adverbs: Multidimensional
In: Computer Graphics and

motion interpolation using radial basis functions.
Applications. (1998)

7. MA, Y., PATERSON, H.M., POLLICK, F.E.: A motion capture library for the
study of identity, gender, and emotion perception from biological motion. In: BMR.
(2006)

8. Salter, D.A., Tamrakar, A., Behjat Siddiquie, M.R.A., Divakaran, A., Lande, B.,
Mehri, D.: The tower game dataset: A multimodal dataset for analyzing social
interaction predicates. In: ACII. (2015)

9. Zhang, J., Li, W., Ogunbona, P.O., Wang, P., Tang, C.: Rgb-d-based action recog-

nition datasets: A survey. In: arxiv. (2016)

10. Chaquet, J.M., Carmona, E.J., Fern´andez-Caballero, A.: A survey of video datasets

for human action and activity recognition. CVIU 117(6) (2013) 633 – 659

11. Larochelle, H., Bengio, Y.: Classiﬁcation using discriminative restricted boltzmann

machines. In: ICML. (2008)

12. Li, W., Zhang, Z., Liu, Z.: Action recognition based on a bag of 3d points. In: Com-
puter Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer
Society Conference on, IEEE (2010) 9–14

13. Bernhardt, D., Robinson, P.: Detecting aﬀect from non-stylised body motions. In:

ACII. (2007)

14. Caruana, R.: Multitask learning. In: Machine Learning. (1997)
15. Evgeniou, T., Pontil, M.: Regularized multi?task learning. In: KDD. (2004)
16. Evgeniou, T., Micchelli, C.A., Pontil, M.: Learning multiple tasks with kernel

methods. In: JMLR. (2005)

17. Ciliberto, C., Rosasco, L., Villa, S.: Learning multiple visual tasks while discovering

their structure. In: CVPR. (2015)

18. Maurer, A., Pontil, M., Romera-Paredes, B.: The beneﬁt of multitask representa-

tion learning. In: ArXiv. (2015)

19. Maurer, A., Pontil, M., Romera-Paredes, B.: Sparse coding for multitask and

transfer learning. In: ICML. (2013)

20. Kumar, A., III, H.D.: Learning task grouping and overlap in multi-task learning.

In: ICML. (2012)

21. Zhou, J., Chen, J., Ye, J.: Clustered multi-task learning via alternating structure

optimization. In: NIPS. (2011)

22. Argyriou, A., Evgeniou, T., Pontil., M.: Convex multi-task feature learning. In:

Machine Learning. (2008)

23. Kang, Z., Grauman, K.: Learning with whom to share in multi-task feature learn-

ing. In: ICML. (2011)

18

Timothy J. Shields

24. Romera-Paredes, B., Aung, H., Bianchi-Berthouze, N., Pontil, M.: Multilinear

multitask learning. In: ICML. (2013)

25. Yang, Y., Hospedales, T.M.: A uniﬁed perspective on multi-domain and multi-task

learning. In: ICLR. (2015)

26. Pentina, A., Sharmanska, V., Lampert, C.H.: Curriculum learning of multiple

tasks. In: CVPR. (2015)

27. Su, C., Yang, F., Zhang, S., Tian, Q., Davis, L.S., Gao, W.: Multi-task learning
with low rank attribute embedding for person re-identiﬁcation. In: ICCV. (2015)
28. Chen, L., Zhang, Q., Li, B.: Predicting multiple attributes via relative multi-task

learning. In: CVPR. (2014)

29. Zhang, T., Ghanem, B., Liu, S., Ahuja, N.: Robust visual tracking via structured

multi-task sparse learning. In: IJCV. (2012)

30. Zhang, Z., Luo, P., Loy, C.C., Tang, X.: Facial landmark detection by deep multi-

task learning. In: ECCV. (2014)

31. Lapin, M., Schiele, B., Hein, M.: Scalable multitask representation learning for

scene classiﬁcation. In: CVPR. (2014)

32. Wang, X., Zhang, L., Lin, L., Liang, Z., Zuo, W.: Deep joint task learning for

generic object extraction. In: NIPS. (2014)

33. Abdulnabi, A.H., Wang, G., Lu, J.: Multi-task cnn model for attribute prediction.

In: arXiv. (2016)

34. Zhuang, F., Luo, D., Jin, X., Xiong, H., Luo, P., He, Q.: Representation learning

via semi-supervised autoencoder for multi-task learning. In: ICML. (2015)

35. Ghifary, M., Kleijn, W.B., Zhang, M., Balduzzi, D.: Domain generalization for

object recognition with multi-task autoencoders. In: ICCV. (2015)

36. Chu, X., Ouyang, W., Yang, W., Wang, X.: Multi-task recurrent neural network

for immediacy prediction. In: ICCV. (2015)

37. Bengio, Y.: Learning deep architectures for ai. In: FTML. (2009)
38. Hinton, G.E., Osindero, S., Teh, Y.W.: A fast learning algorithm for deep belief

nets. In: NC. (2006)

39. Salakhutdinov, R., Hinton, G.E.: Reducing the dimensionality of data with neural

networks. In: Science. (2006)

40. Hinton, G.E.: Training products of experts by minimizing contrastive divergence.

In: NC. (2002)

41. Taylor, G.W., Hinton, G.E., Roweis, S.T.: Two distributed-state models for gen-
erating high-dimensional time series. In: Journal of Machine Learning Research.
(2011)

42. Sutskever, I., Hinton, G.E.: Learning multilevel distributed representations for

high-dimensional sequences. In: AISTATS. (2007)

43. Sutskever, I., Hinton, G., Taylor, G.: The recurrent temporal restricted boltzmann

machine. In: NIPS. (2008)

44. Hausler, C., Susemihl, A.: Temporal autoencoding restricted boltzmann machine.

In: CoRR. (2012)

45. Taylor, G.W., et. al.: Dynamical binary latent variable models for 3d human pose

tracking. In: CVPR. (2010)

46. Mohamed, A.R., Hinton, G.E.: Phone recognition using restricted boltzmann ma-

chines. In: ICASSP. (2009)

47. M. D. Zeiler, G. W. Taylor, L.S., Matthews, I., Fergus, R.: Facial expression
In: NIPS.

transfer with input-output temporal restricted boltzmann machines.
(2011)

Action-Aﬀect Classiﬁcation and Morphing

19

48. Lewandowski, N.B., Bengio, Y., Vincent, P.: Modeling temporal dependencies
in high-dimensional sequences: Application to polyphonic music generation and
transcription. In: ICML. (2012)

49. Salakhutdinov, R., Hinton, G.E.: Deep boltzmann machine. In: AISTATS. (2009)
50. Neverova, N., Wolf, C., Taylor, G., Nebout, F.: Moddrop: adaptive multi-modal

gesture recognition. In: PAMI. (2014)

51. Neverova, N., Wolf, C., Taylor, G.W., Nebout, F.: Multi-scale deep learning for

gesture detection and localization. In: ECCV-W. (2014)

52. Zanﬁr, M., Leordeanu, M., Sminchisescu, C.: The moving pose: An eﬃcient 3d
kinematics descriptor for low-latency action recognition and detection. In: ICCV.
(2013)

53. Zheng, Q., Hao, Z., Huang, H., Xu, K., Zhang, H., Cohen-Or, D., Chen, B.:
Skeleton-intrinsic symmetrization of shapes. In: Computer Graphics Forum. Vol-
ume 34. (2015) 275–286

54. Niebles, J., Wang, H., Fei-Fei, L.: Unsupervised learning of human action categories

using spatial-temporal words. IJCV 79(3) (2008) 299–318

55. Wang, S.B., Quattoni, A., Morency, L.P., Demirdjian, D., Darrell, T.: Hidden
conditional random ﬁelds for gesture recognition. In: Computer Vision and Pattern
Recognition, 2006 IEEE Computer Society Conference on. (2006)

56. Ian Goodfellow, Y.B., Courville, A.: Deep learning. Book in preparation for MIT

Press (2016)

