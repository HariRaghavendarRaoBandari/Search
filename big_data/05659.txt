6
1
0
2

 
r
a

M
 
7
1

 
 
]

.

C
N
o
i
b
-
q
[
 
 

1
v
9
5
6
5
0

.

3
0
6
1
:
v
i
X
r
a

Unveiling causal activity of complex networks

Rashid V. Williams-Garc´ıa,∗ John M. Beggs, and Gerardo Ortiz

Department of Physics, Indiana University, Bloomington, Indiana 47405, USA

(Dated: March 17, 2016)

We introduce a novel tool for analyzing complex network dynamics, allowing for cascades of
causally-related events, which we call causal webs (c-webs for short), to be separated from other
activity. In the context of neural networks, c-webs represent a new, emergent dynamical degree of
freedom which highlights the eﬀective network connectivity, in contrast with the oft-used degrees of
freedom, neuronal avalanches. By identifying non-causal activity, our method can also be used to
characterize dynamical disorder in complex networks. Using this method, we reveal causally-related
activity from experimental data with statistics which may support quasicriticality in the brain.

PACS numbers: 87.19.lc, 64.60.av, 87.19.lj, 64.60.aq

In systems consisting of many interacting elements, a
variety of methods (e.g., transfer entropy or Granger
causality) are often used to reveal hidden dynamical
causal links between them. This naturally leads to a com-
plex networks description [1], raising interesting ques-
tions. For example, what fraction of the activity in such a
network can be attributed to the hidden causal dynam-
ics, and what fraction is produced by other processes,
such as noise? Here we describe a new approach to this
problem and demonstrate its utility on neural networks.

Over the past twenty years, there have been a number
of theoretical [2–14] and experimental [15–27] attempts
to connect activity in living neural networks to critical
avalanches like those seen in the Bak-Tang-Wiesenfeld
(BTW) sandpile model [28, 29].
It has been hypothe-
sized that homeostatic mechanisms might tune the brain,
a complex neural network, towards optimality associated
with a critical point [30] which separates ordered (“su-
percritical”) and disordered (“subcritical”) phases, where
cascades of activity are ampliﬁed or damped, respectively
[14, 23, 26, 31, 32]. In the BTW model, grains of “sand”
are dropped one at a time at random lattice locations;
sites which reach a threshold height topple their grains
to their neighboring sites, potentially inducing further
topplings, together forming an emergent degree of free-
dom called an avalanche. Successive topplings are thus
causally related, with each new toppling having been in-
duced by another which happened before. The sand-
pile model eventually reaches a steady state in which the
probability distribution of avalanche sizes is a power law,
a potential indicator of criticality. It is important to note
that the grains are dropped at an inﬁnitesimally slow rate
such that the relaxation timescale, i.e., the duration of
the avalanches, is much shorter than the time between
grain drops. This separation of timescales is essential to
this concept of self-organized criticality (SOC) [2].

In real systems, however, this separation is not al-
ways achieved. Closer inspection of experimental neu-
ronal avalanche data has revealed possible confounds
with the SOC approach. For example, temporally dis-
tinct avalanches could be concatenated by spontaneous

activity occurring between them, or two spatially dis-
tinct avalanches could be concatenated if they occurred
at overlapping times. These confounding situations high-
light the need for a method to clearly separate causally
from non-causally related activity.

Recent work developed a framework in which there
is no separation of timescales [33]. It has been demon-
strated that access to criticality relates to the coupling
of the concerned network to an external environment, re-
sulting in a non-zero spontaneous activation probability;
the higher the probability, the further from criticality,
and thus the further from optimality [33]. Spontaneous
activity is not necessarily intrinsic to the network, but
perhaps originates due to a coupling with an environ-
ment. Indeed, spontaneous events such as these could be
caused by some unobserved inﬂuence, noise, (in which
case the network has been undersampled) or by intrinsic
properties of the neurons themselves, such as those neu-
rons which have a propensity to ﬁre spontaneously or are
tonically active, e.g., as in the case of pacemaker cells or
some inhibitory neurons [34, 35]. Vanishing spontaneous
activity is equivalent to a separation of timescales, as de-
scribed in SOC. Thus, achieving optimality by operating
at a critical point may not be feasible for a living, open
neural network, according to the quasicriticality hypoth-
esis introduced in [33], although a relative optimality may
still be achieved along a nonequilibrium Widom line. The
accessibility of this relatively-optimal quasicritical region
will likely depend on the character of the environment
and the fundamental properties of the neural network
itself. Our ability to apply the nonequilibrium Widom
line framework and test the quasicriticality hypothesis,
however, hinges on the ability to identify spontaneous
activity in a living neural network. In this paper, we in-
troduce a method to disentangle spontaneous neuronal
activity from that which is causally-related and primar-
ily governed by the network structure and dynamics. We
focus our attention on directed networks, such as those
found in living neural networks.

Here, we introduce a new dynamical degree of free-
dom, which we call causal webs (or c-webs for short),

whose properties contrast and complement those of neu-
ronal avalanches. Whereas neuronal avalanches are de-
ﬁned as spatiotemporal patterns of neuronal activation
spanning a number of adjacent time steps (deﬁned by
a particular choice of temporal bin width, typically 1-
4 ms), framed by time steps lacking activity [16, 36],
c-webs explicitly depend on the network structure and
synaptic delays, thus accommodating the non-Markovian
causal structure of living neural networks. Knowledge of
the network structure and delay information is key, as
it allows to distinguish between diﬀerent spatiotemporal
patterns of activation in a way which is not possible with
neuronal avalanches (see Fig. 1A).

We next formalize the concept of c-webs. Let us label
individual neuronal activations by x = (i, t), which repre-
sents the activation of neuron i at time t, or following the
notation used in [33], zi(t) = 1 (zi(t) = 0 meant quies-
cence). We write the set of all activations A = {xµ}, e.g.,
in Fig. 1B, A = {a, b, c, d, e, f, g}. Formally, we deﬁne a
ν , x(2)
c-web C as a set of correlated ordered pairs (x(1)
ν )
of network activations (i.e., spikes), which we call causal
pairs; quiescent neurons are not included in the set. The
ﬁrst and second entries, x(1)
ν , of the νth causal
ν
pair represent causally-related presynaptic and postsy-
naptic activations, respectively. We emphasize bivariate,
and do not address multivariate, interactions here, al-
though they are believed to play an important role in
cortical processing [37]. In the following, we show how
to determine those causal pairs.

and x(2)

A complete set of causal pairs X is constructed by

FIG. 1. (Color online) Causal webs (c-webs) are distinct from
neuronal avalanches in that they rely on the network struc-
ture and synaptic delays. A. A network produces a variety of
spatiotemporal activity patterns. B. Whereas only two neu-
ronal avalanches are detected, a richer structure is revealed
when spontaneous events (blue annuli) are separated from c-
webs (orange disks); acceptance windows Wij(t) are shaded
light-orange.

taking the Cartesian product of each activation xµ with
its corresponding dynamic postsynaptic activations
U(xµ), i.e.,

2

(cid:91)

xµ∈A

X =

xµ × U(xµ),

(1)

where U(x) ≡ U(i, t) is the set given by

U(i, t) = {(j, t(cid:48)) | j ∈ N (i) and t(cid:48) ∈ Wij(t)}.

(2)

N (i) refers to the set of all postsynaptic neurons j of neu-
ron i, and Wij(t) = [t + dij − ∆ij, t + dij + ∆ij] is a prede-
termined dynamical acceptance window: if a postsy-
naptic neuron j is active within the acceptance window,
then a causal link is inferred. The lower bound of the ac-
ceptance window is adjusted such that it is greater than
t. We write the set of activations in X as A(X) ⊆ A.

Synaptic delays dij associated with the connection
from a presynaptic neuron i to a postsynaptic neuron
j, are allowed to have some uncertainty ∆ij due to vari-
ability in the postsynaptic spike timing. We will later
present a method by which this information can be de-
termined from data; for the moment, we assume it is
given. In Fig. 1B, synaptic delays and their uncertain-
ties are given for the connections in Fig. 1A: d12 = 2,
d14 = 4, d31 = 2, and d42 = 1, with ∆12 = 1, ∆14 = 0,
∆31 = 1, and ∆42 = 1. This information can be used
to determine causal pairs, e.g., the activation a = (1, 2)
in Fig. 1B has U(a) = {c, d}, resulting in the causal
pairs a × U(a) = {(a, c), (a, d)}. The complete set of
causal pairs for the spacetime graph in Fig. 1B is
X = {(a, c), (a, d), (e, f )} and so A(X) = {a, c, d, e, f}.
Causal webs C are disjoint subsets of the set X, which
only contain causal pairs with activations not found in
any other subset. The example in Fig. 1B thus has two
c-webs, C1 = {(a, c), (a, d)} and C2 = {(e, f )}. Note that
spontaneous events initiate c-webs and may become part
of ongoing c-webs. The size s(C) of a c-web is deﬁned as
the total number of distinct activations within it. Deﬁn-
ing that set as A(C), the size s(C) is then given by its
cardinality: s(C) = |A(C)|. Note that A(C) ⊆ A(X).
For example, A(C1) = {a, c, d} and A(C2) = {e, f} in
Fig. 1B, with s(C1) = 3 and s(C2) = 2, respectively.

The duration D(C) of a c-web C can be deﬁned in
terms of its chord. The chord of a c-web K(C) is the
sequence of distinct time steps for which there are ac-
tivations belonging to that c-web, arranged in ascend-
ing order in time, with no repeated elements. That is,
K(C) = (t1, t2, ..., tn), where t1 and tn are the times of
the ﬁrst and last activations, respectively.
In contrast
to the deﬁnition of duration for avalanches, the length
of a c-web’s chord is not equal to the c-web duration.
Instead, we deﬁne the duration of a c-web as a measure
of its chord plus one, i.e., D(C) = 1 + λ(K(C)), where
λ(K(C)) = tn − t1. The chords of the c-webs in Fig. 1B,

ABNeuron IDTime steps, t123412345678910Network structurePossible activity propagation patterns,all consistent with the network structure1234gdAvalanche 1Avalanche 2cfeSpontaneous EventsCausal Websbafor example, are K(C1) = (2, 4, 6) and K(C2) = (7, 8),
with durations D(C1) = 5 and D(C2) = 2.

Finally, we deﬁne the branching fraction φ(C) of a c-
web C as the average number of postsynaptic activations
associated with each presynaptic activation:

φ(C) =

1

|β(C)|

δxµ(C),x(1)

ν (C),

(3)

|A(C)|(cid:88)

|C|(cid:88)

µ=1

ν=1

where δ is the Kronecker delta. The ﬁrst sum is evaluated
over all activations xµ(C) of c-web C, while the second
one is over all presynaptic activations x(1)
ν (C), given by
β(C) = {x(1)
ν (C)}. For example, in Fig. 1B, β(C1) = {a}
and β(C2) = {e}, with φ(C1) = 2 and φ(C2) = 1.

We performed tests of our method using simulations
of the cortical branching model (CBM) [33]. Neuronal
avalanches and c-webs should coincide as emergent de-
grees of freedom in the limits ps → 0 and dij = 1 for all
pairs of nodes (i, j). We simulated 106 avalanches on a
network of N = 243 nodes, whose structure and synaptic
weights were inspired by experimental data; all synaptic
delays were set to a single time step. To simulate the
ps → 0 limit (a separation of timescales), we initiated
avalanches at single, random nodes, only starting a new
avalanche when the previous one had ﬁnished; no sponta-
neous activations or concurrent avalanches were allowed.
The resulting avalanche and c-web size probability dis-
tributions were identical, as expected.

In another test, we constructed a random network of
N = 360 nodes, each with an in-degree of kin = 3, as
in [33]. The network was reducible and had a Perron-
Frobenius eigenvalue of κ = 0.23 [33]. Synaptic delays
(in time steps) were drawn from a uniform distribution
of integers in a closed interval, dij ∈ [1, 16]. Spontaneous

3

activation probabilities for each node were drawn from a
Gaussian distribution with mean and standard deviation
of 10−4; negative values were set to zero. The simula-
tion was performed over 3.6 × 106 time steps. Sponta-
neous events detected by our method were used to con-
struct a new spontaneous activation probability distri-
bution, which we compared with the initial distribution
using a Kolmogorov-Smirnov test: the distributions were
in agreement at a 5% signiﬁcance level with a p-value
of 0.9955 [38]. We note that as the overall connectiv-
ity of the network (which we quantify by κ, as in [33])
is increased, spontaneous events become less prominent
as c-webs begin to dominate the dynamics, leading to
more driven activations and refractory nodes, thus pre-
venting spontaneous activations: neural network dynam-
ics present a ﬂuctuating bottleneck to the inﬂuence of an
environment.

To determine the c-webs, we have so far assumed
knowledge of the network structure and delay informa-
tion (i.e., the dij and ∆ij’s), however in practice, this in-
formation must be learned from experimental data. We
now describe a method, based on delayed transfer en-
tropy (TE) [39, 40], by which this information can be es-
tablished from high temporal resolution multi-unit time-
series data. The use of TE is not absolutely necessary;
alternatives (e.g., computing conditional probabilities di-
rectly from data) may be used so long as they provide a
network structure and synaptic delays.

For a particular pair of neurons (i, j), TE is calculated
at various delays d, peaking at the appropriate synaptic
delay d = dij with a width of ∆ij [39]. At a given activity
transmission delay of d, the TE from neuron i to neuron
j, is given by

(cid:88)

zi→j (d)

Ti→j(d) =

p(zj(t − 1), zj(t), zi(t − d)) log2

(cid:18) p(zj(t)|zj(t − 1), zi(t − d))

(cid:19)

p(zj(t)|zj(t − 1))

,

(4)

where zi→j(d) = {zj(t− 1), zj(t), zi(t− d)} indicates that
the sum is performed over all possible conﬁgurations of
the binary variables zj(t − 1), zj(t), and zi(t − d). Joint
and conditional probabilities in Eq. 4 are estimated from
timeseries data [39]. Synaptic delay information from the
delayed TE analysis is then used to infer the causal struc-
ture of the recorded activity, as in [41]. The method could
be improved by converting TE values to activity trans-
mission probabilities, as described in the supplemental
materials of [42], and then stochastically accepting causal
pairings of activations depending on those probabilities.

We next demonstrate the utility of our method when
2). For our

applied to experimental data (see Fig.

demonstration, we have used data from [41], which were
collected in vitro from organotypic cultures of mouse
somatosensory cortex using a 512-microelectrode array
with a 60 µm electrode spacing and a 20 kHz sampling
rate over an hour-long recording. Data were spike-sorted
to identify individual neurons. Using our method, spon-
taneous events (dark blue) were disentangled from c-webs
(orange) to illustrate their qualitative diﬀerences. In Fig.
2A, we present an activity time raster (top panel) and
corresponding timeseries of the activity (bottom panel),
on which we have performed a moving average with a
t(cid:48)=0 x(t − t(cid:48))/∆t,
In Fig. 2B, we plot

∆t = 100 ms window: y(t) = (cid:80)∆t−1
where x(t) = (cid:80)N

i=1 δzi(t),1 [43].

4

FIG. 2. (Color online) Demonstration of the c-webs method on experimental data. A. Time raster of one minute of neural net-
work activity recorded from somatosensory cortex, processed to separate spontaneous events (dark blue) from c-webs (orange).
Note that tonically-active neurons mainly produce spontaneous events. B. Avalanches (black) and c-webs (orange) exhibit
diﬀerent statistical properties and scaling; the c-web size probability distribution qualitatively agrees with simulation results.

logarithmically-binned avalanche and c-web size proba-
bility distributions to demonstrate that while neuronal
avalanches may exhibit approximate power-law scaling,
thus suggesting underlying critical behavior, the use of c-
webs reveals potentially quasicritical, non-power-law be-
havior, as predicted in [33]. Although we have not yet
determined whether the network operates at or near the
nonequilibrium Widom line, the qualitative shape of the
c-web size distribution agrees with dij = 1 simulations
of the CBM performed at ps = 10−4, indicating that
the avalanches in Fig. 2B are not solely composed of
causally-related events (cf. Fig. 5 from [33]).

The ability to disentangle c-webs from noise allows
us to apply the nonequilibrium Widom line framework
and test the quasicriticality hypothesis [33], but also to
examine the dynamics of living neural networks in new
ways with signiﬁcant consequences in neuroscience. For
example, it has been suggested that a reduced signal-to-
noise ratio produces ineﬃcient hyperactivation in the pre-
frontal cortices of schizophrenics during successful cog-
nitive tasks [44]. This hypothesis has been supported
by computational models which have shown that neu-
ral network attractors become unstable with increased
noise [45]. Our method could be used to test this for
the ﬁrst time. A better understanding of neural network
noise may also provide further insight into the nature
of rapid eye movement (REM) sleep, which has recently
been shown to be induced by the activation of inhibitory
neurons [46]. Activation of inhibitory neurons may result
in a decreased signal-to-noise ratio, as in the portions of
Fig. 2A dominated by spontaneous activations.

Because inhibitory neurons exhibit diﬀerent ﬁring pat-

terns from excitatory neurons, namely fast-spiking and
tonic activation [34, 35], and because we have found tonic
activations to be mostly spontaneous (cf. Fig. 2A), dis-
tributions of spontaneous neuronal events may identify
inhibitory neurons. This method could be used to com-
plement previously-established methods [47].

An improved understanding of the heterogeneity of
noise in neural networks may lead to theoretical advance-
ments, such as the characterization of additional phases
or the appearance of tricritical points, as in the case of an
Ising spin system in the presence of a dichotomous exter-
nal magnetic ﬁeld [30]. Our method may also inform un-
derstanding of the neural network structure beyond the
capabilities of currently available methods. In the case
of network undersampling, hidden network cycles might
be discovered by employing our approach in conjunction
with population coupling methods [48]. Finally, because
c-webs emphasize timescales associated with each neu-
ronal activation (i.e., synaptic delays), their use bypasses
the need to choose a temporal bin width required to de-
ﬁne neuronal avalanches [16, 17]. These projects merit
further investigation.

Similar types of applications could be envisioned for
complex networks in general. For example, ﬁnancial net-
works could be decomposed into agents that directly in-
teract through exchanges as well as exogenous factors like
weather or inﬂation. Such an approach is likely to be use-
ful whenever considering interacting units, whether they
are people in social networks, species in ecological webs,
or protein molecules in a stochastic environment.

The authors would like to thank Hadi Haﬁzi, Emily B.
Miller, and Zachary C. Tosi for valuable discussions, as

50100150200(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)1.561.571.581.591.61.611.62(cid:9)(cid:10)(cid:11)(cid:2)(cid:12)(cid:1)(cid:13)(cid:11)(cid:14)(cid:15)(cid:1)106024(cid:2)(cid:13)(cid:1)(cid:15)(cid:36)100101102103(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:3)(cid:9)(cid:1)10-610-510-410-310-210-1100(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:3)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:13)(cid:7)(cid:15)(cid:7)(cid:5)(cid:16)(cid:17)(cid:7)(cid:6)(cid:5)(cid:11)(cid:7)(cid:13)(cid:18)(cid:5)(cid:7)(cid:12)(cid:4)(cid:9)(cid:2)(cid:19)(cid:1)(cid:20)391,263 avalanches726,505 causal webs(cid:2)(cid:19)(cid:1)(cid:20)(cid:1)(cid:1)(cid:1)(cid:21)(cid:3)(cid:22)(cid:23)(cid:2)(cid:19)(cid:1)(cid:20)(cid:1)(cid:1)(cid:1)(cid:24)(cid:3)(cid:21)(cid:25)(cid:37)well as Shinya Ito, Alan M. Litke, and Fang-Chin Yeh for
providing their in vitro data.

∗ Electronic address: rwgarcia@indiana.edu
[1] C. Gros, Complex and Adaptive Dynamical Systems

(Springer-Verlag, Berlin, 2013).

[2] P. Bak, How Nature Works: The Science of Self-
Organized Criticality (Springer-Verlag, New York, 1996).
[3] N. Bertschinger and T. Natschlager, Neural Comput. 16,

1413 (2004).

[4] D. R. Chialvo, Physica A 340, 756 (2004).
[5] C. Haldeman and J. M. Beggs, Phys. Rev. Lett. 94,

058101 (2005).

[6] O. Kinouchi and M. Copelli, Nat. Phys. 2, 348 (2006).
[7] A. Levina, J.M. Herrmann, and T. Geisel, Nat. Phys. 3,

857 (2007).

[8] M. G. Kitzbichler et al., PLoS Comput. Biol. 5, e1000314

(2009).

[9] W. Chen et al., BMC Neurosci. 11, 3 (2010).

[10] D. B. Larremore et al., Phys. Rev. Lett 106, 058101

(2011).

[11] T. Mora and W. Bialek, J. Stat. Phys. 144, 268 (2011).
[12] S. Pei et al., Phys. Rev. E 86, 021909 (2012).
[13] K. Manchanda et al., Phys. Rev. E 87, 012704 (2013).
[14] M. Rybarsh and S. Bornholdt, PLoS ONE 9, e93090

(2014).

[15] G. A. Worrell et al., Neuroreport 13, 2017 (2002).
[16] J. M. Beggs and D. Plenz, J. Neurosci. 23, 11167 (2003).
[17] V. Pasquale et al., Neuroscience 153, 1354 (2008).
[18] S. S. Poil, A. van Ooyen, and K. Linkenkaer-Hansen,

Hum. Brain Mapp. 29, 770 (2008).

[19] W. L. Shew et al., J. Neurosci. 29, 15595 (2009).
[20] T. L. Ribeiro et al., PLoS ONE 5, e0014129 (2010).
[21] W. L. Shew et al., J. Neurosci. 31, 55 (2011).
[22] G. Solovey et al., Front. Integr. Neurosci. 6, 44 (2012).
[23] V. Priesemann et al., PLoS Comput. Biol. 9, e1002985

(2013).

5

[24] O. Shriki et al., J. Neurosci. 33, 7079 (2013).
[25] S. Yu et al., Front. Sys. Neurosci. 7 (2013).
[26] L. de Arcangelis et al., J. Stat. Mech. 2014, P03026

(2014).

[27] W. L. Shew et al., Nat. Phys. 11, 659 (2015).
[28] P. Bak, C. Tang, and K. Wiesenfeld, Phys. Rev. Lett.

59, 381 (1987).

[29] K. Christensen and N. R. Moloney, Complexity and Crit-

icality (Imperial College Press, London, 2005).

[30] H. Nishimori and G. Ortiz, Elements of Phase Transi-

tions and Critical Phenomena (Oxford, 2011).

[31] D. Hsu and J. M. Beggs, Neurocomputing 69, 1134

(2006).

[32] B. A. Pearlmutter and C. J. Houghton, Neural Comput.

21, 1622 (2009).

[33] R. V. Williams-Garc´ıa, M. Moore, J. M. Beggs, and G.

Ortiz, Phys. Rev. E 90, 062714 (2014).

[34] T. F. Freund and G. Buzs´aki, Hippocampus 6, 347

(1996).

[35] H. Hu et al., Science 345, 1255263 (2014).
[36] D. Plenz et al., Criticality in Neural Systems (Wiley-

VCH, Weinheim, 2014).

[37] I. E. Ohiorhenuan et al., Nature Lett. 466, 617 (2010).
[38] F. James, Statistical Methods in Experimental Physics

(World Scientiﬁc, Singapore, 2006).

[39] S. Ito et al., PLoS ONE 6, e27431 (2011).
[40] M. Wibral et al., PLoS ONE 8, e55809 (2013).
[41] S. Nigam et al., J. Neurosci. 36, 670 (2016).
[42] N. Friedman, S. Ito, B. A. W. Brinkman, M. Shimono, R.
E. L. DeVille, K. A. Dahmen, J. M. Beggs, T. C. Butler,
Phys. Rev. Lett. 108, 208102 (2012).

[43] A. V. Oppenheim et al., Discrete-Time Signal Processing

(Prentice Hall, Englewood Cliﬀs, 1989).

[44] G. Winterer and D. R. Weinberger, Trends Neurosci. 27,

683 (2004).

[45] E. T. Rolls et al., Nature 9, 696 (2008).
[46] F. Weber et al., Nature 526, 435 (2015).
[47] P. Barth´o et al., J. Neurophysiol. 92, 600 (2004).
[48] M. Okun et al., Nature 521, 511 (2015).

