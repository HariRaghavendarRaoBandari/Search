6
1
0
2

 
r
a

 

M
0
2

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
7
7
1
6
0

.

3
0
6
1
:
v
i
X
r
a

Constrains and Conditions: the Lasso

Oracle-inequalities

Niharika Gauraha

Indian Statistical Institute

Abstract

We study various constraints and conditions on the true coeﬃcient vector
and on the design matrix to establish non-asymptotic oracle inequalities for
the prediction error, estimation accuracy and variable selection for the Lasso
estimator in high dimensional sparse regression models. We review results
from the literature and we provide simpler and detailed derivation for sev-
eral boundedness theorems.
In addition, we complement the theory with
illustrated examples.

Keywords: LASSO, Oracle-inequalities, Restricted Eigenvalue,
Irrepresentable Condition, Compatibility Constant

1. Introduction

We consider linear regression model

Y = Xβ0 + ǫ,

(1)

with response vector Yn×1, design matrix Xn×p, true underlying coeﬃcient
vector β0
p×1 and error vector ǫn×1. Mainly, we study the models where the
number of predictors (p) is much larger than the number of observations (n),
p >> n, and we assume sparsity in β0. A lot of research has been devoted to
penalized estimators over a more than decade, we refer to a few papers here
[1], [2], [3], [4], [5] and [6] etc. The Least Absolute Shrinkage and selection
operator (Lasso) is a popular penalized method for simultaneous estimation
and variable selection. We consider the Lasso for high dimensional sparse
regression models and we study various conditions on the design matrix re-
quired to establish non-asymptotic oracle inequalities.

The oracle property means that the penalized estimator is asymptotically
equivalent to the oracle estimator that is as good as the true underlying
model was given in advance, i.e. β0 = β0
S0. Oracle results for estimation and
prediction has been established assuming restricted eigenvalue conditions([7])
and its various forms, see [8], [9], [10], [11], [12] and [13]. For variable selection
consistency, the design matrix X must satisfy irrepresentable condition([14])
or neighborhood stability condition([15]).

Our aim is two fold: to review various results from the literature, and
to provide simpler and detailed derivation for several boundedness theorems.
We also give illustrated examples to support the theory.

The rest of this paper is organized as follows.

In Section 2, we state
notations and assumptions. Then we start with the sparse recovery in the
simplest model where where observations are noiseless, in section 3. Where
we study suﬃcient conditions required on the design matrix for the exact
recovery in a noiseless situation. In section 4, we provide theory with illus-
trated examples on the various conditions and constraints required for the
oracle-inequalities to hold. In Section 5, we (re)derive some useful bounds
which we require to prove bounds on various loss functions . In section 6,
we derive oracle inequalities for prediction accuracy and estimation error for
the Lasso for both noiseless and noisy case. We also (re)prove that the irrep-
resentable condition is almost necessary and suﬃcient for variable selection
for the noiseless Lasso problem. We shall provide conclusion in section 7.

2. Notations and Assumptions

In this section, we state notations and assumptions, and we also deﬁne

required concepts, which are applied throughout this paper.

We consider the usual linear regression model:

Y = Xβ0 + ǫ

where Y ∈ Rn is a response vector, X ∈ Rn×p is a design matrix. β ∈ Rp is
a vector of unknown true regression coeﬃcients, and the components of the
noise vector ǫ ∈ Rn are i.i.d. N(0, σ2). We assume that the design X is ﬁxed
and p >> n.

ℓ0-norm counts the number of non zero coeﬃcients, and is deﬁned as:

kβk0 = Pp

j=1(βj)0, if βj 6= 0

2

(2)

ℓ1-norm is deﬁned as:

ℓ2-norm squared is deﬁned as:

kβk1 = Pp

j=1 |βj|

The ℓ∞− norm is deﬁned as:

kβk2

2 = Pp

j=1 β2

j

kβk∞ = max1≤i≤n||βj|

(3)

(4)

(5)

The true active set denoted by S0 is the support of the subset selection
solution(S0 = supp(β0)) and deﬁned as(we may also use ”S” for the ﬁxed
active set)

S0 = {j; β0

j 6= 0}.

(6)

We denote a Lasso estimated parameter vector as ˆβ. Assume λ > 0 is the
regularization parameter, then the Lasso estimator is computed as:

ˆβ := ˆβ(λ) = argmin
β∈Rp {

1
nky − Xβk2

2 + λ ∗ kβk1}

and the Lasso estimated active set is denoted as ˆS and deﬁned as

ˆS = {j; ˆβj 6= 0}

(7)

(8)

For the noiseless case Y = Xβ0, the lasso problem for some ﬁxed λ > 0, is
deﬁned as:

β⋆ := argmin

{kXβ − Xβ0k2

2 + λkβk1}

The sign function is deﬁned as:

β

sign(x) = 


−1
0
1

if x < 0
if x = 0
if x > 0

Null space of a matrix X is deﬁned as null(X) = {β ∈ Rp|Xβ = 0}
ℓ1 norm can be upper bounded by ℓ2 norm as(see [10])

kβS0k1 ≤ √s0kβS0k2

3

(9)

(10)

(11)

The (scaled)Gram matrix(covariance matrix) is deﬁned as ˆΣ = X ′X
n . We use
Σ(without the “hat” superscript) for the empirical covariance matrix of the
ﬁxed design X.

The βS has zeroes outside the set S,as

βS = {βjI(j ∈ S)}

and β = βS + βSc
For the ﬁxed active set S, the covariance matrix can be partitioned for the
active and the redundant variables as

Σ = (cid:20) Σ11 = Σ(S)

Σ21(S)

Σ12(S)

Σ22 = Σ(Sc) (cid:21) .

(12)

We also assume that Σ11 is non-singular, that is Λmin(Σ11) > 0.

The Lasso error is denotes as (for simplicity we may omit the superscript

hat notation)

ˆ∆ = ˆβ − β0

Soft thresholding function is deﬁned as

Sλ(x) = (cid:26) sign(x)(|x| − λ)

0

if |x| ≥ λ
otherwise

Holder’s inequality for two real function f and g, is deﬁned as

kf gk1 ≤ kfkp kgkq,

where p, q ∈ [1,∞] and

1
p

+

1
q

= 1

(13)

3. Sparse Recovery in Noiseless Case

In order to get an idea of how some conditions on design matrix ensures
the exact(or an optimal) solution, we start with the simplest model, where
the observations are noiseless. We consider a linear equation as

Y = Xβ0

(14)

Where Y,X and β0 are deﬁned as in previous section. When p > n, this is
an under-determined linear system, and there is no unique solution(inﬁnitely
many solutions). We need to pick one out of many solutions that satisﬁes

4

certain property for example, sparsity assumption for the truth β0. More
concretely, we aim to obtain a solution to the linear equation 14, that has the
fewest number of non-zero entries in β0. This problem can be best described
as ℓ0-norm constrained optimization problem and deﬁned as

minimize

β

kβk0 such that Y = Xβ

(15)

But the above optimization problem is non-convex and NP-complete. Then,
we consider a nearest convex problem, which is ℓ1-norm constrained convex
optimization problem deﬁned as follows.

minimize

β

kβk1 such that Y = Xβ

(16)

It is also known as the Basis Pursuit Linear Program(BPLP), see [16]. Now,
we state the assumption under which the solution of BPLP (16) is equivalent
to the solution of ℓ0 non-convex problem (16).
Theorem 1. The BPLP estimates the true β0 exactly if and only if the
design matrix X satisﬁes the restricted null space property with respect to the
true active set S0.

For proof of the above theorem we refer to [16]. The restricted null space is
deﬁned as follows.
Deﬁnition 1 (Restrcited Null Spase(RNS)). For a ﬁxed set S ⊂ {1, 2, ..., p},
the design matrix X satisﬁes the restricted null space property with respect
to S, denoted as RN(S) if,

where the set C(S, 1) is deﬁned as in eq. (17).

C(S, 1) ∩ null(X) = 0

We illustrate the RNS property using a couple of examples as follows.

Example 1. We consider an under-determined linear system where p =2
and n =1, but the true support is S = {2}. Let us suppose X = {1, 2} , the
true coeﬃcient to be estimates is β0 = {0, b}.
The null(X) is the set of scaler multiples of { 2,-1 }.
Since for all vector that belong to null(X) also satisﬁes |2 ∗ α| ≥ | − α|,
hence a unique solution exists and the solution will be, where the ℓ1 ball
intersects the null(X) translated by (o,b), since the error vector has to satisfy
k∆Sck1 = |2∗α| ≤ |−α−b| = k∆Sk1. Therefore the solution of ℓ1 constrained
optimization problem is ˆβ⋆ = {0, b}, and thus it estimates the true β0 exactly.

5

Example 2. We consider another similar problem, where p =2, n =1 and
the true support is S = {2}. Given X = {2, 1}. The true coeﬃcient to be
estimated is β0 = {0, b}.
The null(X) is the set of scaler multiples of { 1,-2 }.
For all vectors that belong to null(X) does not satisfy k∆Sck1 = | − 2α| 6>
|α| = k∆Sk1, hence the design matrix does not satisfy the RNS property. The
the solution of BPLP is β⋆ = {b/2, 0}, and thus it can not estimate the true
β0.

3.1. Suﬃcient Conditions for RNS

It has been proven that the Restricted Isometry Property and Mutual
Incoherence Property are suﬃcient for the restricted nullspace property to
hold. The RIP is deﬁned as follows, see [17] and [18].

Deﬁnition 2 (Restricted Isometry Property(RIP)). For a ﬁxed s ≤
p, the design matrix X is said to satisfy the RIP of order s with isometry
constant δs if

(1 − δs)kuk2

2 ≤ kXsuk2

2 ≤ kuk2

2(1 + δs) ∀u ∈ Rs

or equivalently

2

kXsuk2
kuk2

2 ∈ [(1 − δs), (1 + δs)]

In words, the matrix X ′
subsets S of size at most s.

sXs has their eigenvalues in [(1 − δs), (1 + δs)], for all

RIP is a suﬃcient condition for the RNS to hold, as given by the following

proposition, see [17] and [18] for proof.

Proposition 1 (RIP implies RNS). If RIP (2s, δ2s) holds with δ2s < 1/3,
then the uniform RNP holds for all subsets S of size at most s.

Next, we deﬁne Mutual Incoherence Property(MIP) as follows.

Deﬁnition 3 (Mutual Incoherence Property(MIP)). Assuming that columns
of the design matrix X are centred, MIP is the maximum absolute pairwise
correlation, see [19].

M(X) = max(i6=j)|x

ixj|

′

6

A low pairwise incoherence is required for the exact estimation in noiseless
case and for stable estimation in noisy case.
Proposition 2 (MIP implies RNS). Suppose that for some s ≤ p, the
MIP satisﬁes the bound M(X) < 1
3s, then the uniform RNP of order s holds.
For proof of the above proposition, we refer to [18]. The main advantage of
MIP is that it can be easily computed in O(np2) time and a major disadvan-
tage is that it gives very conservative bound for ℓ1 penalized estimation.

4. Constraints and Conditions

In this section, we study various conditions required on design matrix X
to establish oracle results for the Lasso. We also describe cone constraints
on error vector ∆ and beta-min condition on true coeﬃcient vector β0.

4.1. Convex Cone Constraints

Here, we deﬁne a subset

C(S, L) = {∆ ∈ Rp| k∆Sck1 ≤ Lk∆Sk1}

(17)
for some L ≥ 1. This corresponds to the cone of vectors which is a subset of
Rp, and ∆ is restricted to lie in this subset.
It is easy to prove that the Lasso error ∆⋆ = β⋆ − β0, for the noiseless
case (eq. 9) always satisﬁes a cone constraint with L = 1. Suppose β⋆ is
a solution of the Lasso and β0 the vector of true parameters, then due to
optimality the following inequality holds.

1
nkXβ⋆ − Xβ0k2

2 + λkβ⋆k1 ≤

1
nkXβ0 − Xβ0k2

2 + λkβ0k1

k∆⋆

(18)
Similarly, it can be shown that for the noisy case, the Lasso error ∆ = ˆβ− β0
satisﬁes the following cone constraint for a suitable choice of regularization
parameter λ (see section 5.2 for proof).

In general, the following theorem holds for the error vector ∆.

k∆Sck1 ≤ 3k∆Sk1

(19)

7

1
nkXβ⋆ − Xβ0k2

2 ≤ kβ0k1 − kβ⋆k1
0 ≤ kβ⋆
≤ k∆⋆
Sck1 ≤ k∆⋆

S − β0
Sk1 − k∆⋆
Sk1

Sk1 − kβ⋆
Sck1

Sck1

(∵ β0

Sc = 0)

Theorem 2. The Lasso error, ∆ = ˆβ−β0 is restricted to the cone constraint
C(S, L) for some L ≥ 1.
4.2. Restricted Eigenvalue Condition

The least squares objective function f (β) = ky− Xβk2

2 is always convex.
n ≥ γ for some γ > 0,
n | = 0, hence f (β) can not be strongly convex. The

The function f (β) is strongly convex if ∇2f (β) = X ′X
see [18]. For p < n, | X ′X
strong convexity condition is relaxed for some subset C ⊂ Rp.
Deﬁnition 4 (Restricted Strong Convexity). A function f satisﬁes re-
stricted γ-strong convexity at β∗ with respect to C if

u′∇2f (β)u

≥ γ
for all β in neighbourhood of β∗.

kuk2

2

for all non zero u ∈ C,

Now, we deﬁne restricted eigenvalue on the gram matrix Σ using the cone
constraint on the lasso error, see [7] and [10].

Deﬁnition 5 (Restricted Eigenvalue Condition). For a set S with car-
dinality s = |S| and constant L > 0, the (L,S,s)-restricted eigenvalue is

φ2(L, S, s) := min(cid:26) 1

2

nkX∆k2
k∆Sk2

2

: k∆Sck1 ≤ Lk∆Sk1 6= 0(cid:27) .

The restricted eigenvalue condition is said to be met if φ2(L, S, s) > 0 for all
subsets S of size s.

RE-condition has been used to derive oracle results for the estimation and
prediction (see [7]). Since ℓ1 norm can be upper bounded by ℓ2 norm (11),
the adaptive RE condition can be deﬁned using ℓ2 norm as follows.

Deﬁnition 6 (Adaptive Restricted Eigenvalue). For a set S with car-
dinality s = |S| and constant L > 0, the (L,S,s)-adaptive restricted eigen-
value constant is

φ2

adap(L, S, s) := min(cid:26) 1

2

nkX∆k2
k∆Sk2

2

: k∆Sck1 ≤ L√sk∆Sk2 6= 0(cid:27) .

The adaptive restricted eigenvalue condition is said to be met if φ2
0 for all subsets S of size s.

adap(L, S, s) >

8

The adaptive restricted eigenvalue condition is useful in proving oracle results
for the adaptive Lasso(see [2]).

Finally we deﬁne a slightly stronger version of restricted eigenvalue con-
dition as given in [20], we use it to derive lower bound for ℓ2 estimation error
in later section.

Deﬁnition 7 ((strong)Restricted Eigenvalue Condition). For a set S
with cardinality s = |S| and constant L > 0, the (L,S,s)-strong restricted
eigenvalue is

φ2

str(L, S, s) := min(cid:26) 1

2

nkX∆k2
k∆k2

2

: k∆Sck1 ≤ Lk∆Sk1 6= 0(cid:27) .

The strong restricted eigenvalue condition is said to be met if φ2
for all subsets S of size s.

str(L, S, s) > 0

4.3. Compatibility Condition

For a ﬁxed active set S with cardinality s = |S| and constant L > 0, the

(L,S) restricted ℓ1 eigenvalue is

φ2

comp(L, S) := min(cid:26) 1

nkX∆k2
2s
k∆Sk2

1

: k∆Sck1 ≤ Lk∆Sk1 6= 0(cid:27) .

(20)

Deﬁnition 8 (Compatibility Condition). The (L,S) compatibility con-
dition is said to be satisﬁed for the set S, if φcomp(L, S) > 0.

We note that, the RE-condition implies that compatibility condition holds
for all subset S of size s. The compatibility condition depends on the set S
whereas the RE condition depends only on the cardinality s = |S|. It follows
that the RE condition is stronger than the compatibility condition. In later
section, we derive bounds for prediction accuracy and estimation error under
assumption of the least restrictive condition, the compatibility condition.

4.4. Beta-min Condition

Beta-min condition requires that the non-zero regression coeﬃcients are
suﬃciently large, see [10]. The beta-min, denoted by |β0|min is deﬁned as :

|β0|min = minj∈S0|β0
j|

9

In order to build an intuition, ﬁrst we consider the noiseless Lasso(9 ) problem
with orthonormal design matrix, where X ′X
Using KKT condition(see [21]) we get the following equality.

n = Ip.

2X′(X(β⋆ − β0))

n

= −λτ ⋆, where

τ ⋆ = sign(β⋆)

β⋆ − β0 = −λτ ⋆
2
β⋆ = β0 −
j = (cid:26) sign(β0)(|β0| − λ
2 )

λτ ⋆
2

=⇒ β⋆
=⇒ β⋆

0
(β0)

j = S λ

2

if |β0| ≥ λ
2
otherwise

Hence we conclude that, for the orthonormal design matrix, the Lasso(noieless
case) will select a predictor if its corresponding true regression coeﬃcient
j| > λ
|β0
2 . For the standard Lasso the beta-min condition is deﬁned as fol-
lows, see [10].

Deﬁnition 9 (Beta-min Condition). The beta-min condition is said to
be satisﬁes if the following holds

|β0|min ≥

4λs0

φ2

comp(L, S0)

(21)

In section 6.3, we derive beta-min condition under IR-condition and we
also show that the beta-min condition is important for exact variable selec-
tion.

4.5. Irrepresentability Condition

The irrepresentable condition depends on the covariance of the predictors
(gram matrix Σ) and the signs of the unknown true parameter β0, see [14].
For a ﬁxed set S ⊂ {1, 2, ..., p} and true regression coeﬃcient β, the IR

condition(weak) is deﬁned as

kΣ21Σ−1

11 sign(βS)k∞ < 1

Since we do not know sign(β) before hand, we need the Irrepresentable Con-
dition to hold for every possible combination of diﬀerent signs and placement
of zeros, so we use a modiﬁed version of IR conditions which involves only
the design matrix but not the true coeﬃcient, as given in [12].

10

Deﬁnition 10 (Irrepresentable Condition). The irrepresentable condi-
tion is said to be met for the set S, with cardinality s = |S|, if the following
holds:

kΣ12(S)Σ−1(S)τSk∞ < 1,

∀τS ∈ Rs such that kτSk∞ ≤ 1

The weak irrepresentable condition holds for a ﬁxed τS if

For some 0 < θ < 1, the θ-uniform irrepresentable condition holds if

kΣ12(S)Σ−1(S)τSk∞ ≤ 1

max

kτSk∞≤1kΣ12(S)Σ−1(S)τSk∞ ≤ θ

We note that the IR condition implies that false positives are not selected
by the Lasso, whereas for controlling the false negatives beta-min condition
is also required.

4.6. An Example

In this section we illustrate the various conditions deﬁned above using a

simple example.

Let S0 = {1, 2, 3, 4} be the active set, Σ = X ′X
n and is given as




Σ =

1 0 0 0 ρ
0 1 0 0 ρ
0 0 1 0 ρ
0 0 0 1 ρ
ρ ρ ρ ρ 1



,



where the active variables are uncorrelated and the ﬁfth variable is equally
correlated with all active covariates. We partition Σ as (12), and we analyse
for what values of ρ various conditions hold.

It is easy to check that for ρ ≥ 1

2, Σ is positive semi deﬁnite. First, we
consider ρ = 1
2, though the Σ is not invertible and Λmin(Σ) = 0, it satisﬁes
RIP of order 4 with constant δ4 = 0.866. Since RIP implies Restricted
eigenvalue(and compatibility condition), see [12], therefore for ρ = 1
2, the
Σ satisﬁes the RE and compatibility conditions. For a particular choice of
τS = {1, 1, 1, 1}, Σ does not satisfy the IR condition.
4, it also satisﬁes the IR condition. The RIP with small
constants implies the IR condition, for exact form and proof we refer to
[12]. To check compatibility using approximations and several examples(i.e.
toeplitz, block diagonal matrix etc.) where the compatibility condition holds
are given in [12].

For ρ < 1

s = 1

11

5. Some Useful Bounds

In this section, we derive important bounds which are required for proving

oracle bounds for the Lasso(eq. 7).

Lemma 1 (Basic Inequality). :

kX ˆβ − Xβ0k2

2/n + λk ˆβk1 ≤ 2ǫ

′

X( ˆβ − β0)/n + λkβ0k1

(22)

Proof. For an optimal solution ˆβ and a feasible solution β0 the following
holds.

kY − X ˆβk2
=⇒ kǫ − X( ˆβ − β0)k2

2/n + λk ˆβk1 ≤ kY − Xβ0k2
2/n + λk ˆβk1 ≤ kǫk2

2/n + λkβ0k1

2/n + λkβ0k1

′

2/n − 2ǫ

X( ˆβ − β0)/n + λk ˆβk1 ≤ λkβ0k1

=⇒ kX ˆβ − Xβ0k2
Rearranging the above inequality we get the basic inequality, see [10].
The term 2ǫ′X( ˆβ − β0)/n is called stochastic process part, and it can be
bounded by the ℓ1 norm of the Lasso error. Applying Holder’s inequality
(13), we get the following.

(cid:3)

′

2ǫ

X( ˆβ − β0)/n ≤ 2kǫ

′

X/nk∞ k( ˆβ − β0)k1

To overrule the stochastic process part the (regularization parameter) penalty
is deﬁned to be λ ≥ 2λ0 where λ0 ≥ 2kǫ′X/nk∞ with high probability.
Therefore we can write

′

2ǫ

X( ˆβ − β0)/n ≤

λ
2k( ˆβ − β0)k1

(23)

5.1. Choice of Regularization Parameter(Penalty)

In this section, we prove that a good choice for the regularization param-

eter λ is of order σq log p
n .
Let {x1, ..., xp} denote the columns of the design matrix X. Then the random
variable x′
jǫ is distributed as N(0, σ2/n). We have the following inequality.

′

P(|x

jǫ|/n ≥ t) ≤ 2e

2

−nt
2σ2

12

As kǫ′X/nk∞ corresponds to the maximum over p such variables, the

union bound gives the following inequality.

′

P(kǫ

2

−nt
2σ2

X/nk∞ ≥ t) ≤ 2pe
= 2e

2

−nt

2σ2 +log p

By setting t = σq τ log p

probability.

n

for some τ > 2, we get λ = const.σq log p

n with high

For rest of the article we choose λ to be the order of σq log p
n .

5.2. Choice of the Parameter L

In this section, we prove that a good choice for the parameter L, which
c−1, where c is the deﬁned as

is used for cone constraint, is of order L = c+1
follows.

′

′

λ > 2kǫ
λ = c(2kǫ
λ
= 2kǫ
c

X/nk∞
X/nk∞)
X/nk∞

′

Proof. Using the basic inequality, and let ∆ = ˆβ − β0

λ
c k∆k1 + λkβ0k1
λ
c k∆k1 + λk ˆβS − β0k1 − λk ˆβSck1
λ
c k∆Sk1 +
λ(c + 1)

λ
c k ˆ∆ck1 + λk∆Sk1 − λk∆Sck1

k ˆβS − β0k1 −

λ(c − 1)

c

k ˆβSck1

kX∆k2

2/n + λk ˆβk1 ≤
0 ≤
0 ≤
0 ≤
k∆Sck1 ≤

Or, equivalently

c
c + 1

c − 1k∆Sk1
kβSck1 ≤ Lk ˆβS − β0k1

13

We proved that L = c+1
L=3.

c−1. For rest of the article we choose c = 2, therefore

(cid:3)

6. Oracle Bounds for the Lasso

Usually, we assess quality of the Lasso estimates by measuring prediction
and estimation accuracy, and variable selection consistency. In this section,
we discuss various bounds for them, see [11],[7], [22], [13], [8], [12]) and for a
book length discussion we refer to [10].

6.1. Bounds on Prediction Loss

Prediction loss is also known as mean squared prediction error, and it is

deﬁned as (see [18]):

Lpred( ˆβ; β0) =

1
nkX ˆβ − Xβ0k2

2 =

1
nkX∆k2

2

(24)

We start with deriving prediction error bound for the noiseless Lasso for
building understanding, then it becomes easy to derive the bounds for the
noisy case.

6.1.1. Bounds on Prediction Loss in the noiseless case

First, we consider the noiseless Lasso problem and we derive oracle result

for the prediction without assuming any condition on the design matrix.

Theorem 3. If β⋆ is an optimal solution for the noiseless Lasso problem eq.
(9), then it satisﬁes the following bound.

1
nkX(β⋆ − β0)k2

2 ≤ λkβ0k1

(25)

Proof. If β⋆ is an optimal solution for the eq. (9), then the following basic
inequality holds.

1
nkX∆⋆k2

2 + λkβ⋆k1 ≤ λkβ0k1

1
nkXβ⋆ − Xβ0k2

2 ≤ λkβ0k1 − λkβ⋆k1
0 ≤ λkβ0k1 − λkβ⋆k1

kβ⋆k1 ≤ kβ0k1
2 ≤ λkβ0k1

1
nkX∆⋆k2

∴

14

(26)

(27)

(28)

(29)

Now we exploit compatibility constant to derive fast rate bounds for the

prediction error.

Theorem 4. If β⋆ is an optimal solution for the eq. (9) and φ(S) > 0 over
a ﬁxed set S, then β⋆ satisﬁes the following bounds.

1
nkX∆⋆k2 ≤
k∆⋆k1 ≤

λ2s
φ2
comp
2λs
φ2

comp

Proof. Once again we start from the basic inequality.

1
nkX∆⋆k2

1
nkX∆⋆k2

2 ≤ λkβ0k1 − λkβ⋆k1
≤ λkβS
2 ≤ λk∆⋆

⋆ − β0k1 − λkβ⋆
Sk1 − λk∆⋆
Sck1

Sck1

From eq. (34), we can also have

(30)

(31)

(32)

(33)

(34)

1
nkX∆⋆k2

2 + k∆⋆
1
nkX∆⋆k2

Sck1 ≤ k∆⋆
2 ≤ k∆⋆

∴

Sk1
Sk1
Sk1 from eq. (20)
λkX∆⋆k√s
√nφcomp(S, L)
λ2s
φ2

comp

substituting value for k∆⋆
2 ≤

1
nkX∆⋆k2
1
nkX∆⋆k2 ≤

15

Continuing from eq. (34), adding λk∆⋆k1 to both the sides we get
Sk1 + |k∆⋆

Sck1 + λ(k∆⋆

1
nkX∆⋆k2

Sck1)

2 + λk∆⋆k1 ≤ λk∆⋆
≤ 2λk∆⋆

Sk1 − λk∆⋆
Sk1

substituting value for k∆⋆
2
nkX∆⋆k2
2 + 2λk∆k1 ≤

Sk1 from eq. (20) and multiplying by 2
4λkX∆⋆k√s
√nφcomp(S, L)
using the inequality on r.h.s 4ab ≤ a2 + 4b2
+

2
nkX∆⋆k2
1
nkX∆⋆k2

2 + 2λk∆k1 ≤ kX∆⋆k2
2 + 2λk∆⋆k1 ≤

2

4λ2s
φ2

comp

n
4λ2s
φ2
comp
4λ2s
φ2

comp

=⇒ 2λk∆⋆k1 ≤

Therefore ℓ1- estimation error bound k ˆβ − β0k1 is

k∆⋆k1 ≤

2λs
φ2

comp

6.1.2. Bounds on Prediction Loss in the Noisy Case

It is known that, for p < n and the design matrix X with full column rank,
the prediction error for the Ordinary Least Square(OLS) estimate follows χ2
p
(see [23]):

(cid:3)

kX ˆβOLS − Xβ0k2

2σ2 ∼ χ2

p

It follows that,

E

1
nkX ˆβOLS − Xβ0k2

2 =

σ2
n

p

When p is ﬁxed and n → ∞, the above bound tends to 0 in probability.
where p > n and p ≥ n → ∞, the term p

We can not have similar bound for the Lasso, since for the situation
n does not converge. Under

16

sparsity assumption, we derive the slow rate(A) and fast rate(B) bounds for
the Lasso estimator as follows. For the noisy situation, a lower bound on
the regularization parameter λ is required, that is λ ≥ 2λ0, to overrule the
stochastic process part.

Theorem 5. (Prediction error bounds)
(A) An optimal solution ˆβ of the Lasso problem satisﬁes the following bound

1
nkX ˆβ − Xβ0k2

2 ≤

3λ

2 kβ0k1 = const.σkβ0k1r log p

n

(B) If the design matrix satisﬁes φcomp(3, S) compatibility condition over a
ﬁx set S , then an optimal solution ˆβ satisﬁes the following bound

1
nkX ˆβ − Xβ0k2

2 ≤

9λ2s
4φ2 = const.

σ2 log p

n

s
φ2

comp

Without assuming any restrictive condition on design matrix, we can derive
bounds for the prediction error(case (A)). (A) is known as slow rate since
prediction error is inversely proportional to the square root of the number
of observations, n, whereas the second condition (B) is known as fast rate
because it is inversely proportion to n. As the n grows the rate of convergence
for (B) will be faster.

Proof. (A): From basic inequality (22) we can get the following:

1
nkX ˆβ − Xβ0k2

2 ≤
0 ≤

λ
2k∆k1 + λ(kβ0k1 − k ˆβk1)
λ
2k∆k1 + λ(kβ0k1 − k ˆβk1)

applying the inequality k ˆβ − β0k1 ≤ k ˆβk1 + kβ0k1, we get
(k ˆβk1 + kβ0k1) + λ(kβ0k1 − k ˆβk1)

0 ≤

λ
2

=⇒ k ˆβk1 ≤ 3kβ0k1

17

Considering once again the basic inequality (22)

1
nkX ˆβ − Xβ0k2

λ
2k ˆβk1

λ
(k ˆβk1 + kβ0k1) + λ(kβ0k1 − k ˆβk1)
2
3λ
2 kβ0k1 −
3
2kβ0k1
3λ
2 kβ0k1

2 ≤
≤
λ
2k ˆβk1 ≤
2 ≤
= const.σkβ0k1r log p

n

(∵ λ = O(σrlog p

n

)

1
nkX ˆβ − Xβ0k2

2 +

∴

1
nkX ˆβ − Xβ0k2

We note that, consistency for prediction can be achieved only if kβ0k1 <<
q n
log p .

(cid:3)

Proof. (B) We start with the basic inequality (22)

1
nkX∆k2

2 ≤
≤

λ
2k∆k1 + λkβ0k1 − λk ˆβk1
λ
2k∆S + ∆Sck1 + λk∆Sk1 − λk∆Sck1

multiplying by 2 to both the sides we get

2
nkX∆k2

2 ≤ 3λk∆Sk1 − λk∆Sck1

(35)

Simplifying further

2
nkX∆k2

2 + λk∆Sck1 ≤ 3λk∆Sk1
λk∆Sk1

1
nkX∆k2

2 ≤

3
2

∴

18

√nφcomp(3,S) we get
3λ
2

substituting k∆Sk1 ≤ kX∆k2√s
1
nkX∆k2
1
√nkX∆k2 ≤
1
nkX∆k2

2 ≤

2 ≤

∴

kX∆k2√s
√nφcomp(3, S)
3λ√s

2φcomp(3, S)

9λ2s
comp(3, S)

4φ2

Continuing from eq.(35) and adding λk∆k1 both the sides we get

2
nkX∆k2

2 + λk∆k1 ≤ 3λk∆Sk1 − λk∆Sck1 + λ(k∆Sk1 + k∆Sck1)

≤ 4λk∆Sk1

substituting k∆Sk1 ≤ kX∆k2√s
2
nkX∆k2

√nφcomp(3,S)

2 + λk∆k1 ≤ 4λkX∆k2√s
√nφcomp

Applying the inequality 4ab ≤ a2 + 4b2

2
nkX∆k2
1
nkX∆k2

2 + λk∆k1 ≤

2 + λk∆k1 ≤

4sλ2
φ2

comp

2 +

1
nkX∆k2
4sλ2
φ2

comp

Basically the above inequality gives two bounds.
(1) The prediction error bound

1
nkX∆k2

2 ≤

4sλ2
φ2

comp

(2) ℓ1 - estimation error bound.

λk∆k1 ≤
k ˆβ − β0k1 ≤

4sλ2
φ2
comp
4sλ
φ2

comp

19

(cid:3)

(36)

The above result(and proof) is from [10].

6.2. Bounds on Parameter Estimation Loss

Parameter estimation loss is also known as ℓ2−error and is deﬁned as the
ℓ2-norm loss between a lasso estimated ˆβ a and the true regression vector β0.
(37)

L2( ˆβ; β0) = k ˆβ − β0k2

2

We have already computed bound for the ℓ1 estimation error eq. (36).

k ˆβ − β0k1 ≤

4sλ
φ2

comp

Diﬀerent bounds on estimation error with similar scaling have been discussed
in various papers,i.e., [7], [22], [4] and [13].

Now we obtain optimal bound on ℓ2−error under assumption of more

restrictive condition, so called the (strong)restricted eigenvalue condition.

6.2.1. ℓ2 Estimation Bound for Noiseless Lasso
Theorem 6. For a ﬁxed set S, L > 0 and φstr(L, S, s) > 0, the solution of
the noiseless Lasso eq.(9), β⋆, satisﬁes the following bound.

kβ⋆ − β0k2

2 ≤

sλ2
φ4
str

Proof. We start with the simpliﬁed basic inequality for the noiseless lasso
as in eq.(34) and atfer adding λk∆⋆

Sck1 to both the sides, we get

1
nkX∆⋆k2

2 + λk∆⋆

Sck1 ≤ λk∆⋆k1
2 ≤ λk∆⋆k1

1
nkX∆⋆k2

applying eq (11) we get

∴

1
nkX∆⋆k2

2 ≤ √sλk∆⋆k2
After applying (strong)restricted eigenvalue kX∆⋆k2
2 ≤ √sλk∆⋆k2
2 ≤

φ2
strk∆⋆k2
∴ k∆⋆k2

sλ2
φ4
str

2

n ≥ φ2

strk∆⋆k2

2, we get

20

(cid:3)

6.2.2. ℓ2 Estimation Bound for Noisy Case
str(L, S, s) > 0 and λ ≥ 2λ0, if ˆβ is
Theorem 7. For a ﬁxed set S, L > 1, φ2
a solution of the Lasso problem deﬁned in (7), then ˆβ satisﬁes the following
inequality.

k ˆβ − β0k2

2 ≤

9λ2s
4φ4

str

Proof. We start with the simpliﬁed basic inequality as in eq.(35) and after
adding λk∆Sck1 to both the sides, we get

1
nkX∆k2
1
nkX∆k2

2 + 2λk∆Sck1 ≤
2 + 2λk∆Sck1 ≤
2 ≤

1
nkX∆k2

∴

λ
2k∆k1 + λk∆Sk1 + λk∆Sck1
3λ
2 k∆k1
3λ
2 k∆k1

applying eq (11) we get

3λ
2

1
nkX∆k2

√sk∆k2
After applying (strong)restricted eigenvalue kX∆k2
√sk∆k2

2 ≤

2

n ≥ φ2

strk∆k2

2, we get

strk∆k2
φ2
∴ k∆k2

2 ≤
2 ≤

3λ
2
9λ2s
4φ4
str

The oracle results for prediction and estimation are summarized in the

table (1).

(cid:3)

21

Property

Design Condition

Lasso Method

Prediction Error(slow rate) No requirement

Prediction Error(fast rate) Compatibility condition

Estimation Error(ℓ1 norm) Compatibility condition

Estimation Error(ℓ2 norm)

(strong)Restricted eigenvalue

Noiseless Gaussian Noise
λkβ0k1

2 kβ0k1

3λ

λ2s

9λ2s

φ2

comp

2λs

φ2

comp

λ2s
φ4

str

4φ2

comp

4λs

φ2

comp

9λ2s
4φ4

str

Table 1: Oracle results and suﬃcient conditions required for prediction and estimation

6.3. Variable Selection

Variable selection is also known as support recovery, and the following

loss function is used:

Lvs( ˆβ; β0) = (cid:26) 0 if sign( ˆβi) = sign(β0

1 otherwise

i )

(38)

This checks whether the Lasso estimated ˆβ and the true regression vector β0
have the same sign support.

It has been proven that, the Lasso does not select the true active set S0,
unless we make the following assumptions:

1. Irrepresentable Condition

kΣ21Σ−1

11 sign(β0)k∞ ≤ 1

2. λ ≥ 2λ0
3. beta-min condition |β0|min >> λ

The last two conditions are required to address false negatives. In the next
two theorems, we show the variable selection consistency for noiseless case,
that is the irrepresentable condition is (almost)necessary and suﬃcient con-
dition for the Lasso to select only variables in the true active set. These
results and proofs are from [12].

Theorem 8 (IR is suﬃcient condition for variable selection). Let S⋆
be the variables selected by the Lasso(eq. 9). Suppose the irrepresentable con-

22

dition is met for a ﬁxed active set S, then S⋆ ⊂ S and
11 τSk∞/2

S − β0k∞ ≤ λ sup

kβ⋆

kτSk∞≤1 kΣ−1

Proof. We consider the noiseless case eq.(9). We compute the KKT con-
dition as follows.

1
n

2X′(Xβ − Xβ0) + λτ = 0

where kτk∞ ≤ 1 and τ′β = kβk1. Let β⋆, τ ⋆ be a solution of the Lasso, then
we get,

X′X

n

2(β⋆ − β0) = −λτ ⋆
=⇒ 2Σ(β⋆ − β0) = −λτ ⋆

Without loss of generality we can assume that the ﬁrst s variables are the
active variables, and we partition the Σ, β⋆ = (β1β2)′ and τ ⋆ = (τ1, τ2)′
accordingly (we omit the star superscript for simplicity). We get the following
equation.

Σ = (cid:20) Σ11 Σ12
2k1 = kβ0

Σ21 Σ22 (cid:21)(cid:18) β1 − β0
β2 − β0
Sck1 = 0, it is a null vector. We get the following

2 (cid:19) = (cid:18) τ1
τ2 (cid:19)

1

We note that kβ0
two equations after simpliﬁcation:

2Σ11(β1 − β0
2Σ21(β1 − β0

1) + 2Σ12β2 = −λτ1
1) + 2Σ22β2 = −λτ2

Multiplying the ﬁrst equation with Σ−1

11 we get
11 Σ12β2 = −λΣ−1
11 τ1
1) + 2Σ22β2 = −λτ2

1) + 2Σ−1

2(β1 − β0
2Σ21(β1 − β0

Multiplying the ﬁrst equality by−β ′

′

−2β

2Σ21(β1 − β0

′

′

1) − 2β
2Σ21(β1 − β0

−2β

2Σ21 and the second by −β ′
2.
2Σ21Σ−1
11 τ1
2τ2 = λkβ⋆

11 Σ12β2 = +λβ
2Σ22β2 = +λβ

2Σ21Σ−1
1) − 2β

′

′

′

Sck1

23

subtracting the ﬁrst equality with the second one we get.

2β

′

2(Σ22 − Σ21Σ−1

11 Σ12)β2 = λβ

′

2Σ21Σ−1

11 τ1 − λkβ⋆

Sck1

Using Holder inequality for the following expression.

′

2Σ21Σ−1
β

11 τ1 ≤ λkβ2k1kΣ21Σ−1

11 τ1k∞

≤ kβ⋆

Sck1

It follows that

′

2β

2(Σ22 − Σ21Σ−1

11 Σ12)β2 ≤ 0

Sc = 0.

11 Σ12) is a positive deﬁnite matrix it is a contradiction.

Since (Σ22 − Σ21Σ−1
Hence β2 = 0 =⇒ β⋆
Therefore S⋆ ⊂ S0, hence irrepresentable condition implies no false positive
selection.
Sc = 0), we get the following

Under irrepresentable condition, β2 = 0(β⋆

equation using the KKT condition.

Σ = (cid:20) Σ11 Σ12

Σ21 Σ22) (cid:21)(cid:18) β1 − β0

0

1

(cid:19) = (cid:18) τ1

τ2 (cid:19)

After simpliﬁcation we get

2Σ11(β1 − β0
2Σ21(β1 − β0

1) = −λτ1
1) = −λτ2

From ﬁrst equality, it follows that

(β1 − β0

1) = −λΣ−1
=⇒ k(βS − β0)k∞ ≤ λkΣ−1
j > λkΣ−1

j = 0 and β0

11 τ1/2
11 τSk∞/2
11 τSk∞/2, then we would have

Suppose for some j ∈ S, β⋆

kβS − β0k∞ > λkΣ−1

11 τSk∞/2

This is a contradiction, therefore IR condition and beta-min condition (minj∈S0 |β0
λkΣ−1

11 τSk∞/2) together implies variable selection S⋆ = S0 .

(cid:3)

j| >

24

Theorem 9 (IR is necessary condition for variable selection). Let S⋆
be the variables selected by the Lasso in (9). Suppose S⋆ ⊂ S then the irrep-
resentable condition is met for the true active set S.

Proof. Given S⋆ ⊂ S , it implies β2 = 0(β⋆
equation using the KKT condition.

Sc = 0). We get the following

Σ21 Σ22) (cid:21)(cid:18) β1 − β0
(cid:20) Σ11 Σ12

0

1

(cid:19) = (cid:18) τ1

τ2 (cid:19)

After simpliﬁcation we get

2Σ11(β1 − β0
2Σ21(β1 − β0
Multiplying the ﬁrst equality with Σ−1

1) = −λτ1
1) = −λτ2

11 we get

2(β1 − β0

1) = −λΣ−1
11 τ1
substituting 2(β1 − β0
1) in equality 2 we get
Σ21(λΣ−1
11 τ1) = −λτ2
kΣ21Σ−1
11 τ1k∞ = kβ⋆
kΣ21Σ−1
11 τSk∞ ≤ 1

Sck∞ ≤ 1

(cid:3)

6.3.1. The irrepresentable condition implies the compatibility condition

We have shown that the irrepresentable condition implies variable selec-
tion, now we show that it is more restrictive than the compatibility condition.
The following result(and proof) is from [12].

Theorem 10. For a ﬁxed set S, uniform θ-IR condition implies compatibil-
ity condition.

Proof. It is given that uniform θ-IR condition is satisﬁed by the design
matrix for the set S, hence it is implicit that Λmin(Σ11) > 0. Now Let us
suppose that compatibility condition does not hold for the set S, that is
φ2

comp(L, S) = 0.

25

The compatibility condition can also be deﬁned as:

φ2

comp(L, S) = minn s
1
nkX∆k2

2 = ∆′Σ∆

nkX∆k2

2 : k∆Sk1 = 1, k∆Sck1 ≤ Lo

W here

Suppose ˜∆ solves the above minimization problem, that is

˜∆ = argmin

∆∈Rp {∆′Σ∆ : k∆Sk1 = 1, k∆Sck1 ≤ L}

(39)

We assume that the ﬁrst s variables are the active variables, and we

partition the Σ and ˜∆ = ( ˜∆1, ˜∆2)′ accordingly.
2)(cid:20) Σ11 Σ12

˜∆′Σ ˜∆ = ( ˜∆

Σ21 Σ22) (cid:21) (

1, ˜∆

′

′

˜∆1
˜∆′
2

)

Under assumption of φ2

comp(L, S) = 0, the following equality holds.

˜∆

′

1Σ11 ˜∆1 + ˜∆

′

1Σ12 ˜∆2 + ˜∆

′

2Σ21 ˜∆1 + ˜∆

˜∆′Σ ˜∆ = 0
2Σ22 ˜∆2 = 0

′

(40)

(41)

We introduce a Lagrange multiplier λ ∈ R for the equality constraint
k∆1k1 = 1 in eq. (39). Then by the KKT conditions, there exists a vector
τ1 , such that kτ1k∞ ≤ 1, τ ′

1∆1 = k∆k1 and
Σ11 ˜∆1 + Σ12 ˜∆2 = −λτ1

(42)

(43)

By multiply by ( ˜∆1)′ we obtain
( ˜∆1)′Σ11 ˜∆1 + ( ˜∆1)′Σ12 ˜∆2 = −λk∆k1
( ˜∆1)′Σ11 ˜∆1 + ( ˜∆1)′Σ12 ˜∆2 = −λ
11 in eq (42) we get

By multiplying (τ1)′Σ−1

(τ1)′ ˜∆1 + (τ1)′Σ−1
k ˜∆1k1 + (τ1)′Σ−1

11 Σ12 ˜∆2 = −λ(τ1)′Σ−1
11 τ1
11 Σ12 ˜∆2 = −λ(τ1)′Σ−1
11 τ1

26

By substituting k ˜∆1k1 = 1 we get
1 = −(τ1)′Σ−1
We can simplify the term −(τ1)′Σ−1

−(τ1)′Σ−1

11 Σ12 ˜∆2 ≤ |(τ1)′Σ−1

11 Σ12 ˜∆2 − λ(τ1)′Σ−1
11 τ1
11 Σ12 ˜∆2 as follows
11 Σ12 ˜∆2| ≤ k(τ1)′Σ−1

∵ k(τ1)′Σ−1

11 Σ12k∞ ≤ θ and k∆2k1 ≤ L

11 Σ12k∞k∆2k1 ≤ Lθ

(44)

By substituting value for −(τ1)′Σ−1

11 Σ12 ˜∆2 in eq. (44) we obtain

Let us assume that θ < 1
λ < 0, and we get the following inequality.

=⇒ 1 − Lθ ≤ −λ(τ1)′Σ−1
11 τ1
L . Then 1 − Lθ > 0 and (τ1)′Σ−1

11 τ1 > 0, therefore

(1 − Lθ)Λmin(Σ11)

−λ ≥

s
11 τ1 ≤

Here, we used the inequality (τ1)′Σ−1
(τ1)′Σ−1
2 ≤
kτk2
By multiplying ( ˜∆2)′Σ21Σ−1

Λmin(Σ11).

11 τ1

1

11 in eq (42) we get

s

Λmin(Σ11), which is due to

(45)

( ˜∆2)′Σ21 ˜∆1 + ( ˜∆2)′Σ21Σ−1

11 Σ12 ˜∆2 = −λ( ˜∆2)′Σ21Σ−1
11 τ1

Here, we consider projecting (XSc∆2) to the space spanned by XS. The
projected vector is XSΣ−1

11 Σ12 ˜∆2. ℓ2 norm of the projected vector is

(XSΣ−1

11 Σ12 ˜∆2)′XSΣ−1

11 Σ12 ˜∆2 = ( ˜∆2)′Σ21Σ−1

11 Σ12 ˜∆2

and ℓ2 norm of (XSc∆2) can also be written as

( ˜∆2)′Σ22 ˜∆2
11 Σ12 ˜∆2 = ( ˜∆2)′Σ22 ˜∆2.

Therefore ( ˜∆2)′Σ21Σ−1
And we use the following fact:

−λ( ˜∆2)′Σ21Σ−1

λ( ˜∆2)′Σ21Σ−1
−λ( ˜∆2)′Σ21Σ−1

11 τ1 = |λ|( ˜∆2)′Σ21Σ−1
11 τ1
≤ |λ||( ˜∆2)′Σ21Σ−1
11 τ1|
≤ |λ|Lθ
11 τ1 ≤ −λLθ
11 τ1 ≥ λLθ

(∵ λ < 0)

(∵ |( ˜∆2)′Σ21Σ−1

11 τ1| ≤ Lθ)

27

Substituting value for −λ( ˜∆2)′Σ21Σ−1

11 τ1, we obtain

( ˜∆2)′Σ21 ˜∆1 + ( ˜∆2)′Σ22 ˜∆2 ≥ λLθ

(46)

ﬁnally after substituting from eq (42) and eq (46) values to eq (40), we

get the following inequality.

˜∆′Σ ˜∆ = −λ + λLθ ≥ 0
∴ ˜∆′Σ ˜∆ ≥ 0

which contradicts our assumption that φcomp(L, S) = 0, and continuing with
the above inequality, we get the bound for φcomp(L, S) as follows.

φcomp(L, S) = s ˜∆′Σ ˜∆
φcomp(L, S) ≥ −λs(1 − Lθ)

Substituting value of −λ from eq. (45), we get the following bound.

φcomp(L, S) ≥ (1 − Lθ)2Λ2

min(Σ11)

(cid:3)

7. Conclusion

We discussed various conditions required for Lasso oracle-inequalities.
The compatibility condition is the weakest among others and we used it
for deriving oracle results for prediction(fast rates) and estimation(ℓ1-norm).
The oracle results for the slow rates for prediction does not assume any
condition on the design matrix. We derived oracle results for ℓ2 estimation
error using a slightly stronger version of the restricted eigenvalue condition.
We have shown that the oracle result for variable selection requires irrepre-
sentable condition and beta-min conditions. We illustrated various design
conditions using simple examples. We also discussed that the irrepresentable
condition implies compatibility condition. For further details on how various
conditions for Lasso oracle results relate to each other, we refer to [12].

References

[1] R. Tibshirani, Regression shrinkage and selection via the lasso, J. R.

Statist. Soc 58 (1996) 267–288.

28

[2] H. Zou, The adaptive lasso and its oracle properties, Journal of the

American Statistical Association 101(476) (2006) 1418–1429.

[3] H. Zou, T. Hastie, Regularization and variable selection via the elastic

net, J. R. Statist. Soc 67 (2005) 301–320.

[4] E. Candes, T. Tao, The dantzig selector: statistical estimation when p

is much larger than n, Annals of Statistics 35 (2007) 2313–2351.

[5] N. Meinshausen, Relaxed lasso, Comput. Statist. Data Anal 52 (2007)

374–393.

[6] A. Belloni, V. Chernozhukov, L. Wang, Square-root lasso: pivotal re-
covery of sparse signals via conic programming, Biometrika 98 (2011)
791–806.

[7] P. Bickel, Y. Ritov, A. Tsybakov, Simultaneous analysis of lasso and

dantzig selector, Annals of Statistics 37 (2009) 1705–1732.

[8] S. van de Geer, The deterministic lasso, In JSM proceedings 140.

[9] C. Zhang, J. Huang, The sparsity and bias of the lasso selection in high-
dimensional linear regression, Annals of Statistics 36 (2008) 1567–1594.

[10] P. B¨uhlmann, S. van de Geer, Statistics for High-Dimensional Data:

Methods, Theory and Applications, Springer Verlag, 2011.

[11] van de Geer S., Sparsity oracle inequalities for the lasso, Annals of Statis-

tics 36 (2008) 614–645.

[12] S. van de Geer, P. B¨uhlmann, On the conditions used to prove oracle
results for the lasso, Electronic Journal of Statistics 3 (2009) 1360–1392.

[13] N. Meinshausen, B. Yu, Lasso-type recovery of sparse representations

for high-dimensional data, Annals of Statistics 37 (2009) 246–270.

[14] P. Zhao, B. Yu, On model selection consistency of lasso, Journal of

Machine Learning Research 7 (2006) 2541–2563.

[15] N. Meinshausen, P. B¨uhlmann, High-dimensional graphs and variable

selection with the lasso, Annals of Statistics 34 (2006) 1436–1462.

29

[16] Cohen, W. Dahmen, R. DeVore, Compressed sensing and best k-term
approximation, J. of. American Mathematical Society 22 (2009) 211–
231.

[17] E. Candes, The restricted isometry property and its implications for

compressed sensing, C. R. Acad. Sci. Paris 346 (2008) 589–592.

[18] T. Hastie, R. Tibshirani, M. Wainwright, Statistical Learning with Spar-

sity: The Lasso and Generalizations, CRC Press, 2015.

[19] E. Candes, J. Romberg, Sparsity and incoherence in compressive sam-

pling, Inverse Problems 23 (2006) 969–985.

[20] G. Raskutti, M. J. Wainwright, B. Yu, Restricted eigenvalue properties
for correlated gaussian designs, Journal of Machine Learning Research
11 (2010) 2241–2259.

[21] S. Boyd, L. Vandenberghe, Convex Optimization, Cambridge University

Press, 2004.

[22] F. Bunea, A. Tsybakov, M. Wegkamp, High-dimensional generalized
linear models and the lasso, Electronic Journal of Statistics 1 (2007)
169–194.

[23] G. A. F. Seber, A. J. Lee, Linear Regression Analysis, Wiley, 2003.

30

