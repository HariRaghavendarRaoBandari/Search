6
1
0
2

 
r
a

M
 
0
3

 
 
]

R

I
.
s
c
[
 
 

4
v
2
7
5
5
0

.

3
0
6
1
:
v
i
X
r
a

Supervised Matrix Factorization for Cross-Modality Hashing

Hong Liu†‡, Rongrong Ji†‡, Yongjian Wu♮, Wei Liu♯, Gang Hua♭

†Fujian Key Laboratory of Sensing and Computing for Smart City, Xiamen University, 361005, China

‡School of Information Science and Engineering, Xiamen University, 361005, China

♮BestImage, Tencent Technology (Shanghai) Co.,Ltd, China

♯Didi Research, Beijing, China

♭Microsoft Research Asia, Beijing, China

Corresponding author: rrji@xmu.edu.cn

Abstract

Matrix factorization has been recently utilized for
the task of multi-modal hashing for cross-modality
visual search, where basis functions are learned
to map data from different modalities to the same
Hamming embedding. In this paper, we propose
a novel cross-modality hashing algorithm termed
Supervised Matrix Factorization Hashing (SMFH)
which tackles the multi-modal hashing problem
with a collective non-matrix factorization across
the different modalities.
In particular, SMFH
employs a well-designed binary code learning
algorithm to preserve the similarities among
multi-modal original features through a graph
regularization. At the same time, semantic labels,
when available, are incorporated into the learning
procedure. We conjecture that all these would
facilitate to preserve the most relevant information
during the binary quantization process, and hence
improve the retrieval accuracy. We demonstrate
the superior performance of SMFH on three
cross-modality visual search benchmarks,
i.e.,
the PASCAL-Sentence, Wiki, and NUS-WIDE,
with quantitative comparison to various state-
[Kumar and Udupa, 2011;
of-the-art methods
Rastegari et al., 2013;
Zhang and Li, 2014;
Ding et al., 2014].

been

a

and

retrieval

has

several

emerging

search, machine

translation,

applications

[Bronstein et al., 2010;

1 Introduction
fundamental
Cross-modality
includ-
problem in
text
ing
visual
Masci et al., 2014;
mining
Rasiwasia et al., 2010; Costa Pereira et al., 2014].
In a
typical scenario of cross-modality retrieval, a query comes
from one modality, e.g.,
text, while the returned results
come from another modality, e.g., image. To achieve this
goal, a typical solution adopted by most existing works
is to embed data samples of different modalities into a
common low-dimensional space. By doing so, both the
query and returns can be well aligned to capture their cross-
modality similarities for retrieval [Costa Pereira et al., 2014;
Wang et al., 2014].

Recently, both unsupervised and supervised hashing
techniques have been investigated for cross-modality re-
trieval due to their prominent efﬁciency.
For instance,
Bronstein et al.
proposed a Cross-Modality Similar-
ity Search Hashing (CMSSH) algorithm by using eigen-
decomposition and boosting. Both Cross-View Hashing
(CVH) [Kumar and Udupa, 2011] and Inter-Media Hashing
(IMH) [Song et al., 2013] extended the classic Spectral Hash-
ing approach [Weiss et al., 2009] to the scenario of cross-
modality retrieval. For another instance, Co-Regularized
Hashing (CRH) [Zhen and Yeung, 2012] and Heterogeneous
Translated Hashing (HTH) [Wei et al., 2014] further deal
with the cross-modality hashing under a co-regularized boost-
ing framework.

In [Rastegari et al., 2013], Predictable Dual-View Hashing
(PDH) was proposed to learn the discriminative hash func-
tions via a max-margin formulation with an iterative opti-
mization algorithm. In [Ding et al., 2014], Collective Matrix
Factorization Hashing (CMFH) was proposed to formulate
the joint learning of cross-modality binary codes as a collec-
tive matrix factorization problem. In [Zhang and Li, 2014],
Supervised Multi-modal Hashing (SMH) was proposed to in-
tegrate semantic labels to improve the performance of hash
function learning in the respective modalities.

While promising progress has been made, it remains as
an open problem to capture the multi-modal similarities
among data samples, as well as to preserve such similar-
ities in a produced binary code (Hamming) space. As
mentioned before, collective factorization [Ding et al., 2014]
and supervised hashing [Zhang and Li, 2014] have demon-
strated outperformance on respective problems, a.k.a.,
for cross-modality retrieval and for single-modality bi-
nary code learning [Liu et al., 2015; Mukherjee et al., 2015;
Lin et al., 2015]. The intuition of our work is to com-
bine the merits of supervised hashing [Liu et al., 2012;
Zhang and Li, 2014] over the state-of-the-art cross-modality
retrieval schemes [Ding et al., 2014].

However, the integration of both approaches towards su-
In
pervised cross-modality hashing is not an easy task.
it is hard to optimize the discrete Hamming
one aspect,
distances.
In the other aspect, the complexity of the ex-
isting matrix factorization hashing and supervised cross-
modality hashing are very high, which are the square of
the training set size [Zhang and Li, 2014; Liu et al., 2015;

DŽĚĂůŝƚǇϭ

ĂƚĂďĂƐĞ

DŽĚĂůŝƚǇϮ

ƐŝŵŝůĂƌ













ϭ ϭ Ϭ
Ϭ
ϭϭ
ϭϬϬ



















,ĂƐŚ&ƵŶĐƚŝŽŶϭ

ŽůůĞĐƚŝǀĞDĂƚƌŝǆ

&ĂŽƚƌŝǌĂƚŝŽŶ

,ĂƐŚ&ƵŶĐƚŝŽŶϮ

>ĂďĞů'ƌĂƉŚ
ZĞŐƵůŝǌĂƚŝŽŶ

^D&,

yϭ

zϭ

ϭϭϭ

ϭϭϭ

ϭϭϭ

ϭϭϭ

yϮ

zϮ

yϯ

zϯ

ͲϭͲϭϭ

ͲϭͲϭϭ

DƵůƚŝͲŵŽĚĂůŝƚǇ

ďŝŶĂƌǇĐŽĚĞ

Figure 1: The Framework of our proposed Supervised Ma-
trix Factorization Hashing (SMFH).

Mukherjee et al., 2015] and cannot be easily scaled up to
massive training data.

In this paper, we propose a novel cross-modality hash-
ing method, dubbed the name Supervised Matrix Factoriza-
tion Hashing (SMFH), which addresses the above challenges
under a graph-regularized, collective non-matrix factoriza-
tion framework. The ﬁrst contribution we make is a hy-
brid regularization method to model the hash function learn-
ing, which integrates graph regularization into the collective
non-negative matrix factorization. The second contribution
we make is a supervised collective non-matrix factorization
scheme, which leverages semantic labels to reﬁne the graph
regularizer during the step of graph construction. It ensures
the learned binary codes to preserve the semantic similarities
among data within multiple modalities.

Besides the two contributions mentioned above, we also
propose an optimization algorithm to solve the objective
function designed for SMFH, which works under an iter-
ative updating procedure with stochastic sampling. This
strategy can reach a training time reduction of supervised
cross-modality hashing. Fig.1 shows the overall framework
of the proposed SMFH scheme. We conduct extensive ex-
periments in cross-modality visual search, i.e., using text
queries to retrieve relevant images and vice versa, on three
widely used benchmarks including, PASCAL-Sentence, Wiki
and NUS-WIDE. We demonstrate the superior performance
of SMFH over a group of state-of-the-art cross-modality
hashing methods including CVH [Kumar and Udupa, 2011],
PDH [Rastegari et al., 2013], SMH [Zhang and Li, 2014],
and CMFH [Ding et al., 2014].

The rest of this paper is organized as follows: in Section 2,
we present our SMFH approach in depth. Section 3 shows ex-
tensive experiments conducted on three benchmark datasets.
Finally, we draw our conclusions in Section 4 and discuss the
future work.

2 Supervised Matrix Factorization Hashing
In this section, we describe the proposed supervised cross-
modality hashing algorithm. Without loss of generality, we
take bi-modal hashing for instance, which can be easily ex-
tended to the scenario of multi-modality hashing.

2.1 Collective Factorization for Cross-Modality

Hashing

Non-negative Matrix Factorization (NMF) is a matrix de-
composition algorithm that focuses on learning low-rank rep-
resentation. We deﬁne a non-negative data matrix X =
[x1, ..., xN ] ∈ Rd×N , where N is the number of sam-
ples, d is the feature dimension, and xi is the i-th sam-
ple. Non-negative matrix factorization aims at ﬁnding two
non-negative factors U = [u1, ..., ur] ∈ Rd×r and Y =
[v1, ..., vN ] ∈ Rr×N , r << d, whose product can approx-
imate X, i.e.,

X ≈ UY.

(1)
The squared Frobenius norm of the difference between two
matrices is commonly used as the cost function to measure
the approximation quality [Cai et al., 2011; Liu et al., 2013],
which is deﬁned as:

min
U,Y

k X − UY k2

F , s.t. U > 0, Y > 0,

(2)

where k · kF is the Frobenius norm of the matrix.

However, when facing cross-modality data, it is expected
that the matrix Y is a new multi-modality representation of
the data matrices X1 and X2 in a common low-dimensional
space.
It aims to learn two base U1 ∈ Rd1×r and U2 ∈
Rd2×r to produce hash codes, which map bi-modal fea-
ture matrices into a r-dimensional binary code matrix Y ∈
{0, 1}r×N, where r is the number of hash bits. Ideally, Y
should reveal the hidden semantics shared by different modal-
ities.

Correspondingly, the learning of the hash code is done via:

min

Y,U1,U2

2

X

t=1

λt(cid:13)(cid:13)

Xt − UtY(cid:13)(cid:13)

2
F

,

(3)

where λt is the weight coefﬁcient of this modality satisfying
P2
t=1 λt = 1. Intuitively, for the 1-st modal data X1, the
matrix U1 and the matrix Y are learned by Eq. (3), which
is the same to X2. yji = 1 means that the corresponding
semantic representation ut
·i and vice
versa.

·j is related to data Xt

2.2 Learning Supervised Hash Function
Towards learning supervised hash function, our goal is to pre-
serve the semantic similarity among data points in the Ham-
ming space. That says, the hash functions should enforce
labeled data pair l(ei) = l(ej) to have identical or similar
binary codes, where ei = (x1
i ) is the i-th data sample on
both modalities from the training set, and function l(·) returns
the supervised labels of the data across different modalities.
Our task is to ﬁnd a graph matrix that preserves the intrin-
sic geometric structure of the similarity from bi-modal data.
To this end, we ﬁrst construct an afﬁnity graph to model the
aforementioned semantic information. This is done by calcu-
lating the pairwise similarity via the inner product among the
semantic labels.

i , x2

More speciﬁcally, the similarity between the i-th data sam-

ple and the j-th data sample is deﬁned as follow:

Aij = l(ei)T · l(ej).

(4)

Without
loss of generality, we deﬁne a matrix La ∈
{0, 1}dl×N, where each column of La is the label representa-
tion vector of each data sample, and each row represents each
sample’s category1. Subsequently, the similarity matrix can
be presented as A = La

T ·La.

Then, our goal can be formulated via a spectral graph learn-

ing problem from the label similarity matrix A as:

label similarity matrix, it is inefﬁcient to store and compute
it during optimization. To solve this problem, we propose a
random sampling method, which uses a sampled sub-graph
to replace the complete similarity matrix. The detailed opti-
mization procedure is presented as follows:

(1) Fix Wt and Y, then update Ut. The corresponding

sub-problem is

min

Y

1
2

n

X

i,j=1

kYi − Yjk2 Ai,j = T r(YT LY),

(5)

t=1 λtkXt − UtYk2
minU1,U2 P2
F
s.t. U1 ≥ 0, U2 ≥ 0, λ1 + λ2 = 1.

(9)

where L is the Laplacian matrix for A.

Given data in two modalities, a.k.a, X1 = {x1

i ∈ Rd1|i =
i ∈ Rd2|i = 1, ..., N }, and their pairwise
1, ..., N }, X2 = {x2
semantic similarity A, the goal of supervised hash function
learning is to learn two basis matrices U1 and U2, together
with the hash codes Y by the following objective function:

Here we learn two basis matrices U1 and U2, which turns
this sub-problem to be a traditional NMF problem for each
modality.

By directly optimizing Eq. (9) within each modality re-
spectively, we solve the new objective function through La-
grange multiplier Ψ = [ψik] with constant ut

ij ≥ 0:

min

Y,U1,U2

2

X

t=1

λt(cid:13)(cid:13)

Xt − UtY(cid:13)(cid:13)

2

F + αT r(YT LY),

(6)

where α is a balance parameter, which can be seen as a regu-
larizer for the above collective non-matrix factorization.

Intuitively, we learn coefﬁcients of the optimal projection
Wt by minimizing the error term between the linear hash
function Ht(Xt) = sgn(WtT
Xt) and the hash codes Y as
kY − Ht(Xt)k2. Inspired by [?], label graph can be used
as Laplacian Regularized in this error. Then, the objective
function of hash function learning is written as follows:

min

Ht(Xt)(cid:13)(cid:13)

Y − Ht(Xt)(cid:13)(cid:13)

2

+ ηT r(Ht(Xt)LHt(Xt)T ),

(7)

where η is the balance parameter. This term is integrated with
the term of supervised semantic similarity in the proposed
graph-regularized collective matrix factorization. Then, the
overall objective function is written as follows:

minY,Wt P2
t=1 (cid:8) kY − Ht(Xt)k2

+µP2

t=1 λtkXt − UtYk2

F + ηT r(Ht(Xt)LHt(Xt)T )(cid:9)(8)

F + αT r(YLYT )

s.t. Y ∈ {0, 1}r×N , YYT = I, Ut ≥ 0,

where µ is a tradeoff parameter to control the weights be-
tween hash function approximation and the proposed graph-
regularized collective non-matrix factorization scheme.

2.3 Optimization
Directly minimizing the objective function in Eq. (8) is in-
tractable due to the discrete constraint of Y. To tackle this
issue, we relax the discrete constraint from Y ∈ {0, 1}r×n
to 0 ≤ Y ≤ 1. After that, it is still non-convex with respect
to Y, Ut, and Wt jointly. This is further handled by using
an alternating optimization, i.e., updating one variable while
ﬁxing the rest two until convergence. Due to the scale of the

1In this paper, we assume that La is fully observed without miss-
ing labels. We can get the labels among points in many cases for
missing labeled datas, i.e. classiﬁcation technology, which makes
our assumption reasonable.

O1(Ut) = T r(XtXtT

+ T r(UtYYT UtT

) − 2T r(XtYT UtT
) + T r(ΨUtT

).

)

(10)

We derive the partial derivatives with respect to Ut as :

∂O1
∂Ut = −2XtYT + 2UtYYT + Ψ.

(11)

Then by Karush-Kuhn-Tucker (KKT) conditions, we update
Ut via the following procedure:

ut
ij ← ut

ij

(XtYT )ij
(UtYYT )ij

.

(12)

Intuitively, Ut is non-negative after updating.

(2) Fix Ut and Wt, then update Y. We then ﬁx Ut and

Wt, the updating of Y subsequently refers to:

t=1 λtkXt − UtYk2
minY P2
t=1 (cid:13)(cid:13)(cid:13)
Xt(cid:13)(cid:13)(cid:13)
+µP2

Y − WtT

F

2

s.t. 0 ≤ Y ≤ 1.

F + αT r(YT LY)

+ βkYYT − Ik2
F

(13)

However, the scale of the semantic matrix is extremely large,
which needs huge storage cost and makes the Eq. (12) hard
to optimize during each iteration. To tackle this problem, we
randomly sample parts of the original label similarity matrix
A, which can approximate the graph regularization. We de-
ﬁne a sampling matrix S ∈ {0, 1}N ×m during each iteration,
where m is the number of sampling points with m << N .
Then the hash code of sampled data can be represented by
ˆY = YT S ∈ {0, 1}dt×m, and the sampled label represen-
tation can be presented by ˆLa = La
T S ∈ {0, 1}dl×m. By
using the above sampling, this sub-problem of Eq. (12) can
be rewritten as:
t=1 λtkXt − UtYk2
minY P2
t=1 (cid:13)(cid:13)(cid:13)
Xt(cid:13)(cid:13)(cid:13)
+µP2

F + αT r( ˆYT ˆL ˆY)
+ βkYYT − Ik2
F

Y − WtT

(14)

s.t. 0 ≤ Y ≤ 1.

F

2

where ˆL is the Laplacian matrix for ˆA = ˆLa

T

· ˆLa.

Algorithm 1 Supervised Matrix Factorization Hashing
Input: Training data points in two modalities X1 and X2,
the corresponding pairwise semantic similarity matrix A,
the number of support samples m, and the number of hash
bits r.

Output: The hash codes Y for training data and the projec-

tion coefﬁcient matrix Wt.

1: Initialize Wt, Ut and Y by random matrices, t = 1, 2.
2: repeat
3:
4:

Fixing Wt and Y, update Ut by Eq. (12);
Uniformly and randomly select m sample pairs from
training data.
Fixing Ut and Wt, update Y by Eq. (17);
Fixing Ut and Y, update Wt by Eq. (18);

5:
6:
7: until convergence

Since solving such a constraint is not convenient, we fur-
ther relax it to Y ≥ 0 and normalize Y after factorization in
each iteration. Let Φ be the Lagrange Multiplier for the new
constraint. The Lagrange term could be written as follows:

O(cid:16)(cid:0)ndr + (n + d)(r2 + r) + m2r(cid:1)t(cid:17), where t is the num-
ber of iterations. Since r, m << n, the overall complexity is
O(cid:0)n(d + r)rt(cid:1), which is linear to the size of the training data.
In practice, the proposed SMFH is much faster than most
cross-modality hashing, i.e. PDH and CMFH, whose train-
ing time is rounded to the competing cross-moality hashing
method SMH. Although the hash codes of training data is ob-
tained by minimizing Eq. (8), it cannot be directly applied to
the case of the out-of-sample query. For such out-of-sample
query, we use the hash function learned by Eq. (8) to gener-
ate the corresponding binary code. Then in online search, the
time complexity for each modality is constant as O(dr).

2.4 Extension to Multi-Modality Search
It is quite intuitive to extend SMFH in Eq. (8) from bi-modal
to multiple modalities, that is:

minY,Wt Pnt

+µPnt

t=1 λtkXt − UtYk2

F + αT r(YLYT ) (19)

t=1{kY − Ht(Xt)k2

F + ηT r(Ht(Xt)LHt(Xt)T )}

s.t. Y ∈ {0, 1}r×n, Ut ≥ 0,

O2(Y) =

2

X

t=1

λt{T r(XtXtT

) − 2T r(XtYT UtT

)

+ T r(UtYYT UtT

)} + αT r( ˆY ˆL ˆYT )

+ µ

2

X

t=1

{T r(YYT ) − 2T r(YXtT

Wt)

where Pnt
t=1 λt = 1. It is convenient to adopt Algorithm 1
to minimize the objective function in Eq.(8). An alternating
optimization strategy can also be used here.
In particular,
the variable Ut and Wt can be directly got through Eq.(11)
and Eq.(17), respectively.And ﬁnally, the variable Y can be
learned by the new formulation as follows:

(15)

+ T r(WtT

XtXtT

Wt)} + T r(ΦYT ).

yij ← yij

Using the KKT conditions, we have:

t=1 λtUtT
WtT
(Pnt
t=1 λtUtT UtY + α ˆY ˆDST + µPnt
(Pnt

Xt + α ˆY ˆAST + µPnt

t=1

t=1

Xt)ij
Y)ij
(20)

.

∂O2
∂Y

= 2

2

X

t=1
2

λt{−UtT

Xt + UtT

UtY} + 2α ˆY ˆLST

+ 2µ

X

t=1

{Y − WtT

Xt} + Φ = 0.

s.t. ψij Yij = 0, Y ≥ 0.

(16)

Eq. (14) can be solved by the following updating rule:

t=1

t=1

WtT

yij ← yij

(3) Fix Ut and Y, then update Wt.

Xt + α ˆY ˆAST + µP2

t=1 λtUtT
(P2
t=1 λtUtT UtY + α ˆY ˆDST + µP2
(P2

Xt)ij
Y)ij
(17)
ˆD is the diagonal matrix with entries of column sums of ˆA.
This last sub-
problem ﬁnds the best projection coefﬁcient Wt by minimiz-
ing Eq.(7) for the t-th modality as the Laplacian Regularized
Least squares algorithm, resulting in a closed-form solution:
Wt = (cid:0)XtXtT
We summarize the whole procedure of the proposed SMFH

+ η(XtS)ˆL(ST XtT

) + γI(cid:1)−1

XtYT . (18)

in Algorithm 1.

Times Complexity: The main time consumption of the
proposed SMFH is the matrix factorization, its complexity is

3 Experiments
Quantitative experiments are conducted to validate the ad-
vantages of the proposed cross-modality hashing algorithm
on three widely-used benchmark, i.e., PASCAL-Sentence2,
Wiki3 and NUS-WIDE4.

The PASCAL-Sentence dataset contains 1,000 images
that are divided into 20 categories. Each image is represented
by a 269-dimensional visual feature extracted by a collec-
tions detectors. A 2,790-dimensional textual feature is ex-
tracted using the bag-of-words representation with WordNet
[Farhadi et al., 2010]. For this dataset, 800 image-sentence
pairs are randomly sampled as the training set and the remain-
ing for query testing.

The Wiki dataset contains 2,866 documents, where the
image-text pairs are fully annotated with 10 semantic cate-
gories. Each image is represented as a 128-dimensional bag-
of-visual-words feature. Each document is represented as a
10-dimensional topical feature using Latent Dirichlet Alloca-
tion [Blei et al., 2003]. For the Wiki dataset, we randomly
select 75% image-text pairs for training and the rest for query
testing.

.

2http://vision.cs.uiuc.edu/pascal-sentences/
3http : //www.svcl.ucsd.edu/projects/crossmodal/
4http : //lms.comp.nus.edu.sg/research/NUSWIDE.htm

Table 1: The mAP and Precision Comparison Using Hamming Ranking on Two Benchmark with Different Hash Bits.

Task Methods

32

Task 1

Task 2

0.2053
CVH
0.2034
PDH
0.5947
CMFH
0.3831
SMH
SMFH 0.6611
0.1660
CVH
0.2442
PDH
0.2081
CMFH
SMH
0.2301
SMFH 0.2518

mAP
64

0.1872
0.2047
0.6063
0.4032
0.6819
0.1479
0.2360
0.2111
0.2503
0.2701

Wiki

128

0.2039
0.2133
0.6131
0.4171
0.6880
0.1572
0.2685
0.2270
0.2570
0.2787

Precision@100

32

64

0.1504
0.1765
0.5492
0.3135
0.6342
0.1280
0.2058
0.1691
0.1953
0.2246

0.1324
0.1725
0.5649
0.3400
0.6602
0.1170
0.1945
0.1678
0.2186
0.2238

32

0.4480
0.5008
0.3807
0.5978
0.5462
0.4592
0.5129
0.3818
0.5823
0.5938

mAP
64

0.4184
0.5078
0.3787
0.6162
0.6633
0.4260
0.5260
0.3774
0.6020
0.6325

NUS-WIDE

128

0.4012
0.5336
0.3663
0.6195
0.6247
0.4021
0.5377
0.3664
0.6089
0.6175

Precision@100

32

64

0.4606
0.4885
0.3764
0.6007
0.5478
0.4700
0.4988
0.3787
0.5829
0.5985

0.4281
0.5018
0.3750
0.6194
0.6757
0.4333
0.5224
0.3766
0.6039
0.6380

Image Query to Text Database

Text Query to Image Database

 

CVH
SMH
PDH
CMFH
SMFH

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

P
A
m

CVH
SMH
PDH
CMFH
SMFH

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

P
A
m

 

64

96

128

0.05

 
8

16

24

32

48
number of bits

64

96

128

0.05

 
8

16

32

24

48
number of bits

(a) The mAP on Task 1.
(b) The mAP on Task 2.
Figure 2: The mAP curves on PASCAL-Sentence.

The NUS-WIDE dataset contains 269,648 images with 81
concepts crawled from Flickr. We select 186,577 labeled
image-text pairs according to the top 10 largest concepts
as adopted in [Hu et al., 2014; Zhang and Li, 2014]. In this
dataset, images are represented by a 500-dimensional bag-
of-visual-words feature, and its corresponding tags are rep-
resented by a 1,000-dimensional bag-of-words feature. We
choose 6,577 image-text pairs from this database as query and
the remaining to form the dataset for training.

(1)

task via:

the text-to-image side,

Compared Methods: We evaluate the cross-modality
termed
retrieval
termed Task 2.
Task 1, and (2) the image-to-text side,
the proposed SMFH is compared against
In both sides,
four state-of-the-art methods: Cross-View Hashing (CVH)
[Kumar and Udupa, 2011], Supervised MultiModal Hashing
(SMH) [Zhang and Li, 2014], Predictable Dual-view Hash-
ing (PDH) [Rastegari et al., 2013] and Collective Matrix
Factorization Hashing (CMFH) [Ding et al., 2014]. Except
CVH, the source codes of the rest methods are available pub-
licly, and all their parameters’ setting are used as what their
papers presented. All our experiments were run on a work-
station with a 3.60GHz Intel Core I5-4790 CPU and 16GB
RAM.

Evaluation Protocols: The quantitative performance is
evaluated by using the mean Average Precision (mAP) with
top 100 ranking list. We also consider other three evalu-
ation protocols, i.e., precision at top-100 positions (Preci-
sion@100), Recall curves at top-K and Precision curves at
top-K.

Parameter Settings: SMFH has ﬁve essential parameters
in Eq. (13), i.e., λ1, λ2, α, µ, η and m. The parameter λ1
and λ2 control the weights between two modalities, which
are found to have little inﬂuence. In our experiments, we em-

pirically set λ1 = 0.5 for the image modality and λ2 = 0.5
for the text modality. The parameter α holds the semantic
similarity of the original space, which is set as a large num-
ber of 2. µ is a trade-off parameter, which is set as 25 on
the two datasets. During each iteration, the number of sam-
pling points is set as 800 for PASCAL-Sentence, and 2, 000
for the other two datasets. At the last part of this section, we
will analyze the relation of the parameter m, and show the
convergence result. The three regularization parameters γ, β,
and η are set to a small number 0.001 in all the experiments.
The

Quantitative Results: Fig.2 shows the mAP results on
PASCAL-Sentence dataset with different bits on both re-
trieval tasks. SMFH has achieved remarkable mAP scores,
especially when hash bit is larger than 32. Comparing to
the second best scheme, SMFH has achieved 7.1% mAP im-
provement for the Task 1 and 6.5% improvement for the Task
2.

Then, we evaluate the proposed method on Wiki, as
shown in the ﬁrst row of Fig.3 and Tab.1, which demonstrate
that SMFH has achieved superior performance on this
benchmark for both text-to-image and image-to-text sides,
both with a performance gain of more than 6%. The mAP
results and Precision@100 results on Wiki are reported
in Tab.1 under the setting of 32, 64, and 128 bits respec-
tively. SMFH has achieved remarkable mAP and precision
scores. Comparing with the state-of-the-art alogrithms,
Rastegari et al., 2013;
i.e.,
Zhang and Li, 2014; Ding et al., 2014],
the task of
text-to-image retrieval, our SMFH has signiﬁcant advantage
on precision and mAP values with all bits, mainly due to the
fact that the matrix factorization can successfully ﬁnd better
latent topic concepts from text. Meanwhile SMFH fully uses
supervised label to improve the cross-modality retrieval.
Fig.3 shows the comparison of precision curves and recall
curves on Wiki when hash bit is 64.

[Kumar and Udupa, 2011;

for

Similar performance gains are observed on the large-
scale NUS-WIDE, especially in the text-to-image retrieval.
As shown in Tab.1 and the second row of Fig.3, SMFH
achieves highest search accuracy. When the hash bit
is 64,
the precision of the top-100 Hamming ranking
is over 60% by SMFH, which is much better than the
state-of-the-arts [Ding et al., 2014; Kumar and Udupa, 2011;
Rastegari et al., 2013; Zhang and Li, 2014]. Although, the
mAP is not the highest when hash bit is 32 for text-to-

Text Query to Image Database
CVH
SMH
PDH
CMFH
SMFH

 

Text Query to Image Database

 

CVH
SMH
PDH
CMFH
SMFH

0.8

0.7

0.6

0.5

0.4

0.3

0.2

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

l
l

a
c
e
R

i

i

n
o
s
c
e
r
P

Image Query to Text Database

 

CVH
SMH
PDH
CMFH
SMFH

0.24

0.22

0.2

0.18

0.16

0.14

0.12

0.7

0.6

0.5

0.4

0.3

0.2

0.1

l
l

a
c
e
R

Image Query to Text Database
CVH
SMH
PDH
CMFH
SMFH

 

i

i

n
o
s
c
e
r
P

0.1

 
11050100 200 300 400 500 600 700 800 900 1000

Number of retrieval points

0
 
11050100 200 300 400 500 600 700 800 900 1000

Number of retrieval points

0.1

 
11050100 200 300 400 500 600 700 800 900 1000

Number of retrieval points

0
 
11050100 200 300 400 500 600 700 800 900 1000

Number of retrieval points

(a) Precsion@K on Task 1.

(b) Recall@K on Task 1.

(c) Precsion@K on Task 2.

(d) Recall@K on Task 2.

Text Query to Image Database

 

(1) Wiki Dataset

Text Query to Image Database

Image Query to Text Database

 

Image Query to Text Database
CVH
SMH
PDH
CMFH
SMFH

 

0.02

0.018

0.016

0.014

0.012

0.01

0.008

0.006

0.004

0.002

l
l

a
c
e
R

CVH
SMH
PDH
CMFH
SMFH

0.75

0.7

0.65

0.6

0.55

0.5

0.45

0.4

0.35

i

i

n
o
s
c
e
r
P

0.02

0.018

0.016

0.014

0.012

0.01

0.008

0.006

0.004

0.002

l
l

a
c
e
R

 

CVH
SMH
PDH
CMFH
SMFH

0.7

0.65

0.6

0.55

0.5

0.45

0.4

0.35

i

i

n
o
s
c
e
r
P

CVH
SMH
PDH
CMFH
SMFH

 
11050100 200 300 400 500 600 700 800 900 1000

Number of retrieval points

0
 
11050100 200 300 400 500 600 700 800 900 1000

Number of retrieval points

 
11050100 200 300 400 500 600 700 800 900 1000

Number of retrieval points

0
 
11050100 200 300 400 500 600 700 800 900 1000

Number of retrieval points

(e) Precsion@K on Task 1.

(f) Recall@K on Task 1.

(g) Precsion@K on Task 2.

(h) Recall@K on Task 2.

Figure 3: The Precision curves and Recall curves of all the algorithms on both datasets when hash bit is 64.

(2) NUS-WIDE Dataset

Table 2: The Training Time (s) comparing with different al-
gorithms on both datasets.

Methods

CVH
PDH
CMFH
SMH
SMFH

PASCAL

Wiki

32
5.84
278.19
324.87
26.26
5.89

64
5.18
350.73
335.41
33.31
6.16

32
0.14
1.34
0.32
0.31
0.56

64
0.04
2.44
0.59
0.59
0.71

NUS-WIDE
64
32
3.31
3.25
736.81
1579.1
20.74
73.94

1461.71
1782.6
40.53
88.04

P
A
m

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

 

Task1
Task2

 

 

Wiki
NUS−WIDE

400

350

300

250

200

150

100

50

e
c
n
e
g
r
e
v
n
o
C

200 400 600 800 1000 1200 1400 1600 1800 2000

number of sampling pairs

0

 
0

10

20

number of iteration

30

40

50

image sides, SMFH maintain its the competitive advantage
on higher hash bits. Nevertheless, the performance of Task
2 is at the second place, which also have competitive perfor-
mance for cross-modality retrieval.

Tab.2 shows the results of training time comparing with
different algorithms on different hash bits on three bench-
marks, which contain the whole training set. PDH and CMFH
are much higher than that of our proposed SMFH, with larger
size of training data and higher feature dimensions. SMFH
can get the better performance comparing with others, by
fully use of the semantic information to enhance the perfor-
mance with less training time.

We further study the inﬂuence of different sizes of the
training set. As shown in Fig.4 (a) for Wiki, mAP results
are shown when hash bit is 64, in which we vary the size
of sampling from 200 to 2, 000. The performance of cross-
modality retrieval consistently improve with the increasing
of the sampling size for SMFH. Thus, we randomly choose
about 100 image-tag pairs for each concept as the training set
for convenience in optimization, which contain about 1, 000
pairs during each iteration. At last, we validate the conver-
gence according to the sampling during each iteration. As
shown in Fig.4 (b), when the size of sampling pairs is 1, 000,
and the hash bit is 64, SMFH can quickly converge by using

(a) The mAP curves vs. number of
sampling pairs.

(b) Convergence validation.

Figure 4: Parameter Analysis.

random sampling. The same conclusion holds on the NUS-
WIDE dataset.

4 Conclusions
In this paper, we propose a novel hashing method termed
Supervised Matrix Factorization Hashing (SMFH) for cross-
modality visual search. We employ graph regularization
to develop a collective matrix factorization based hashing
framework, which can preserve the similarities among origi-
nal features from different modalities into a produced Ham-
ming space. Meanwhile, SMFH incorporates supervised la-
bel information to enhance the quantization quality of the
learned binary codes. Furthermore, hashing and graph reg-
ularization are integrated into a uniﬁed framework by means
of joint hash function learning. In this framework, the given
supervised labels can be leveraged to construct a label ma-
trix, leading to more discriminative hash codes. Extensive ex-
periments conducted on PASCAL-Sentence, Wiki, and NUS-
WIDE benchmarks demonstrated the superior performance
of SMFH over several state-of-the-art cross-modality hash-
ing methods [Ding et al., 2014; Kumar and Udupa, 2011;

[Masci et al., 2014] Jonathan Masci, Michael M Bronstein,
Alexander M Bronstein, and J¨urgen Schmidhuber. Multi-
modal similarity-preserving hashing. TPAMI, 36(4):824–
830, 2014.

[Mukherjee et al., 2015] Lopamudra Mukherjee, Sathya N
Ravi, Vamsi K Ithapu, Tyler Holmes, and Vikas Singh.
An nmf perspective on binary hashing.
In Proceedings
of CVPR, pages 4184–4192, 2015.

[Rasiwasia et al., 2010] Nikhil

Jose
Costa Pereira, Emanuele Coviello, Gabriel Doyle,
Gert RG Lanckriet, Roger Levy, and Nuno Vasconcelos.
A new approach to cross-modal multimedia retrieval. In
Proceedings of MM, pages 251–260. ACM, 2010.

Rasiwasia,

[Rastegari et al., 2013] Mohammad Rastegari,

Jonghyun
Choi, Shobeir Fakhraei, Daume Hal, and Larry Davis.
Predictable dual-view hashing. In Proceedings of ICML,
pages 1328–1336, 2013.

[Song et al., 2013] Jingkuan Song, Yang Yang, Yi Yang,
Zi Huang, and Heng Tao Shen. Inter-media hashing for
large-scale retrieval from heterogeneous data sources. In
Proceedings of SIGMOD, pages 785–796. ACM, 2013.

[Wang et al., 2014] Wei Wang, Beng Chin Ooi, Xiaoyan
Yang, Dongxiang Zhang, and Yueting Zhuang. Effective
multi-modal retrieval based on stacked auto-encoders. In
Proceedings of VLDB, 2014.

[Wei et al., 2014] Ying Wei, Yangqiu Song, Yi Zhen, Bo Liu,
and Qiang Yang. Scalable heterogeneous translated hash-
ing. In Proceedings of SIGKDD, pages 791–800. ACM,
2014.

[Weiss et al., 2009] Yair Weiss, Antonio Torralba, and Rob
Fergus. Spectral hashing. In Proceedings of NIPS, pages
1753–1760, 2009.

[Zhang and Li, 2014] Dongqing Zhang and Wu-Jun Li.
Large-scale supervised multimodal hashing with semantic
correlation maximization. In Proceedings of AAAI, 2014.
[Zhen and Yeung, 2012] Yi Zhen and Dit-Yan Yeung. Co-
regularized hashing for multimodal data. In Proceeding of
NIPS, pages 1376–1384, 2012.

Rastegari et al., 2013; Zhang and Li, 2014].
In the future,
we would investigate large-scale discrete optimization tech-
niques for the proposed SMFH.

References
[Blei et al., 2003] David M Blei, Andrew Y Ng, and
JMLR,

Michael I Jordan. Latent dirichlet allocation.
3:993–1022, 2003.

[Bronstein et al., 2010] Michael M Bronstein, Alexander M
Bronstein, Fabrice Michel, and Nikos Paragios. Data
fusion through cross-modality metric learning using
similarity-sensitive hashing.
In Proceedings of CVPR,
pages 3594–3601. IEEE, 2010.

[Cai et al., 2011] Deng Cai, Xiaofei He, Jiawei Han, and
Thomas S Huang. Graph regularized nonnegative matrix
factorization for data representation. TPAMI, 33(8):1548–
1560, 2011.

[Costa Pereira et al., 2014] Jose Costa Pereira, Emanuele
Coviello, Gabriel Doyle, Nikhil Rasiwasia, Gert RG
Lanckriet, Roger Levy, and Nuno Vasconcelos. On the
role of correlation and abstraction in cross-modal multi-
media retrieval. TPAMI, 36(3):521–535, 2014.

[Ding et al., 2014] Guiguang Ding, Yuchen Guo, and Jile
Zhou. Collective matrix factorization hashing for multi-
modal data. In Proceedings of CVPR, pages 2083–2090.
IEEE, 2014.

[Farhadi et al., 2010] Ali Farhadi, Mohsen Hejrati, Moham-
mad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. Every picture tells a
story: Generating sentences from images. In Proceedings
of ECCV, pages 15–29. Springer, 2010.

[Hu et al., 2014] Yao Hu, Zhongming Jin, Hongyi Ren,
Deng Cai, and Xiaofei He.
Iterative multi-view hashing
for cross media indexing. In Proceedings of MM, pages
527–536. ACM, 2014.

[Kumar and Udupa, 2011] Shaishav Kumar and Raghaven-
dra Udupa. Learning hash functions for cross-view sim-
ilarity search. In Proceedings of IJCAI, volume 22, page
1360, 2011.

[Lin et al., 2015] Zijia Lin, Guiguang Ding, Mingqing Hu,
Semantics-preserving hashing for
In Proceedings of CVPR, pages

and Jianmin Wang.
cross-view retrieval.
3864–3872, 2015.

[Liu et al., 2012] Wei Liu, Jun Wang, Rongrong Ji, Yu-Gang
Jiang, and Shih-Fu Chang. Supervised hashing with ker-
nels. In Proceedings of CVPR, pages 2074–2081. IEEE,
2012.

[Liu et al., 2013] Jialu Liu, Chi Wang, Jing Gao, and Jiawei
Han. Multi-view clustering via joint nonnegative matrix
factorization. In Proceedings of SDM, volume 13, pages
252–260. SIAM, 2013.

[Liu et al., 2015] Li Liu, Mengyang Yu, and Ling Shao.
Multiview alignment hashing for efﬁcient image search.
TIP, 24(3):956–966, 2015.

