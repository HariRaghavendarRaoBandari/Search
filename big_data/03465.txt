6
1
0
2

 

b
e
F
 
7
2

 
 
]
T
I
.
s
c
[
 
 

1
v
5
6
4
3
0

.

3
0
6
1
:
v
i
X
r
a

Recovery of signals under the condition on RIC and

ROC via prior support information

Wengu Chen1 , Yaling Li2

1 Institute of Applied Physics and Computational Mathematics

Beijing, 100088, China

2 Graduate School, China Academy of Engineering Physics

Beijing, 100088, China

Email: chenwg@iapcm.ac.cn, leeyaling@126.com

March 15, 2016

Abstract In this paper, the suﬃcient condition in terms of the RIC and ROC for the stable
and robust recovery of signals in both noiseless and noisy settings was established via weighted l1
minimization when there is partial prior information on support of signals. An improved perfor-
mance guarantee has been derived. We can obtain a less restricted suﬃcient condition for signal
reconstruction and a tighter recovery error bound under some conditions via weighted l1 minimiza-
tion. When prior support estimate is at least 50% accurate, the suﬃcient condition is weaker than
the analogous condition by standard l1 minimization method, meanwhile the reconstruction error
upper bound is provably to be smaller under additional conditions. Furthermore, the suﬃcient
condition is also proved sharp.

Keywords Compressed sensing, Restricted isometry property, Restricted orthogonality con-

stant, Weighted l1 minimization, Sparse signal recovery

Mathematics Subject Classiﬁcation (2010) 94A12, 90C59, 94A15

1

Introduction

Compressed sensing shows that it is highly possible to reconstruct sparse signals from what was
previously believed to be incomplete information[10, 13]. The fundamental goal in compressed sens-
ing is to recover a high dimensional sparse signal based on a small number of linear measurements,
possibly corrupted by noise. This can be compactly described via

y = Ax + z,

1

(1.1)

where A is a given n × N sensing matrix with n ≪ N , i.e., using very few measurements, y ∈ Rn
is a vector of measurements, and z ∈ Rn is the measurement error (z = 0 means no noise). One
needs to reconstruct the unknown signal x ∈ RN based on A and y. In general, the solutions to the
underdetermined systems of linear equations (1.1) are not unique. In order to recover x uniquely,
additional assumptions on A such as restricted isometry property and x such as sparsity are needed.
A vector x ∈ RN is k−sparse if kxk0 = |supp(x)| ≤ k, where supp(x) = {i : xi 6= 0} is the support
of x. Then the most natural approach for solving this problem is to ﬁnd the sparsest solution in
the feasible set of possible solutions. In the noiseless case, it can be cast as the l0 minimization
problem as below [10, 13, 20, 26]:

minimize

x∈RN

kxk0

subject

to Ax = y.

(1.2)

It was proved that when measurements n > 2k and A is in general position (any collection of n
columns of A is linearly independent), then any k−sparse signals can be exactly recovered [14].
However, l0 minimization problem is a combinatorial problem which becomes intractable in the
high dimensional settings. Hence, solving it directly is NP-hard.

Cand`es and Tao [12] then proposed the following constrained l1 minimization method:

minimize

x∈RN

kxk1

subject

to ky − Axk2 ≤ ǫ.

(1.3)

It can be viewed as a convex relaxation of l0 minimization. To recover sparse signals via constrained
l1 minimization, Cand`es and Tao [12] also introduced the notion of Restricted Isometry Property
(RIP), which is one of the most commonly used frameworks for compressive sensing. The deﬁnition
of RIP is as follows.
Deﬁnition 1.1. Let A ∈ Rn×N be a matrix and 1 ≤ k ≤ N is an integer. The restricted isometry
constant (RIC) δk of order k is deﬁned as the smallest nonnegative constant that satisﬁes

2 ≤ (1 + δk)kxk2
2,
for all k−sparse vectors x ∈ RN . Note that for k1 ≤ k2, δk1 ≤ δk2.

(1 − δk)kxk2

2 ≤ kAxk2

Thus, l1 minimization has been proved an eﬀective way to recover sparse signals in many settings
[2, 3, 5–9, 12, 23]. Cand`es, Romberg and Tao ﬁrst gained the suﬃcient condition for stable recovery
by l1 minimization method [9]. In [6], Cai and Zhang applied the following l1 minimization

minimize

x∈RN

kxk1

subject

to

ky − Axk2 ∈ B,

(1.4)

where B is a bounded set determined by the noise structure. In particular, B is taken to be {0}
in the noiseless case. Here they considered the following l2 bounded noise and Dantzing Selector
noise settings

Bl2(ε) = {z : kzk2 ≤ ε}

2

(1.5)

and

BDS(ε) = {z : kAT zk∞ ≤ ε}.

(1.6)

Cai and Zhang [6] provided a sharp suﬃcient condition δtk < q t−1

t with t ≥ 4/3 which can
guarantee the exact recovery of all k−sparse signals in the noiseless case and stable recovery of
approximately sparse signals in the noise case by l1 minimization method (1.4) with (1.5) and
(1.6).

In addition, the restricted orthogonality constant is also important in compressed sensing [2, 3,

7].

Deﬁnition 1.2. Let A ∈ Rn×N be a matrix and 1 ≤ k1, k2 ≤ N be integers with k1 + k2 ≤ N , the
restricted orthogonality constant (ROC) θk1,k2 of order (k1, k2) is deﬁned as the smallest nonnegative
constant that satisﬁes

|hAu, Avi| ≤ θk1,k2kuk2kvk2,

for all k1−sparse vectors u ∈ RN and k2−sparse vectors v ∈ RN with disjoint supports. Note that
for k1 ≤ k2 and k′1 ≤ k′2, θk1,k′1 ≤ θk2,k′2

.

It also has been shown that l1 minimization can recover a sparse signal under various conditions
on δk and θk1,k2 [2–5, 7, 11, 12, 15, 17]. For example, δk + θk,k + θk,2k < 1 [12], δ2k + θk,2k < 1
[11], δ1.5k + θk,1.5k < 1 [5] and δ1.25k + θk,1.25k < 1 [2]. Cai and Zhang [7] also established a sharp
suﬃcient condition in terms of RIC and ROC to achieve the stable and robust recovery of signals in
both noiseless and noisy cases via l1 minimization method. In fact, Cai and Zhang [7] proved that
δa + Ca,b,kθa,b < 1 can ensure stable and robust recovery of signals via l1 minimization method (1.4)
with (1.5) and (1.6). Moreover, for any ε > 0, δa + Ca,b,kθa,b < 1 + ε is not suﬃcient to guarantee
the exact and stable recovery of all k−sparse signals via any methods.

It is worthy of noting that compressed sensing is a nonadaptive data acquisition technique
since A is independent of x, the signal being measured. The l1 minimization method (1.3) is
also itself nonadaptive as a result of no prior information on the signal x being used in (1.4). In
practical examples, however, the estimate of the support of the signal or of its largest coeﬃcients
may be possible to be drawn. Incorporating prior information is very useful for recovering signals
from compressive measurements. Thus, the following weighted l1 minimization method which
incorporates partial support information of the signals has been introduced to replace standard l1
minimization

minimize

x∈RN

kxk1,w subject

to

ky − Axk2 ≤ ǫ,

(1.7)

where w ∈ [0, 1]N and kxk1,w = Pi

wi|xi|. Reconstructing compressively sampled signals with
partially known support has been previously studied in the literature; see [1, 16, 18, 19, 21, 22, 24].

3

Borries, Miosso and Potes in [1], Khajehnejad et al. in [19], and Vaswani and Lu in [24] introduced
the problem of signal recovery with partially known support independently. The works by Borries
et al. in [1], Vaswani and Lu in [21, 24, 25] and Jacques in [18] incorporated known support
information using weighted l1 minimization approach with zero weights on the known support,

[16] extended weighted l1 minimization approach to nonzero weights. They allow the weights

namely, given a support estimate eT ⊂ {1, 2, . . . , N} of unknown signal x, setting wi = 0 whenever
i ∈ eT and wi = 1 otherwise, and derived suﬃcient recovery conditions. Friedlander et al. in
wi = ω ∈ [0, 1] if i ∈ eT . Since Friedlander et al. incorporated the prior support information

and consider the accuracy of the support estimate, they derived the stable and robust recovery
guarantees for weighted l1 minimization which generalize the results of Cand`es, Romberg and Tao
in [9]. They actually improved the recovery guarantees of l1 minimization problem (1.3) by using
weighted l1 minimization problem (1.7). Friedlander et al. [16] pointed out that once at least 50% of
the support information is accurate, a less conservative suﬃcient condition for guaranteeing stably
and robustly signal reconstruction as well as a tighter reconstruction error bound can be obtained.
Furthermore, they also pointed out suﬃcient conditions are weaker than those of [24] when ω = 0.

In this paper, we consider the following weighted l1 minimization method:

to y − Ax ∈ B

(1.8)

i ∈ eT c
i ∈ eT .

minimize

x∈RN

kxk1,w subject

with wi =( 1,

ω,

where 0 ≤ ω ≤ 1 and eT ⊂ {1, 2, . . . , N} is a given support estimate of unknown signal x. B is

also a bounded set determined by the noise settings (1.5) and (1.6). Our goal is to generalize
the results of Cai and Zhang [7] via the weighted l1 minimization method (1.8). We establish the
suﬃcient condition on RIC and ROC for the stable and robust recovery of signals with partially
known support information from (1.1). We also show that the recovery by weighted l1 minimization
method (1.8) is stable and robust under weaker suﬃcient conditions compared to the standard l1
minimization method (1.4) when we have the partial support information with accuracy better than
50%. Meanwhile, we obtain the smaller upper bounds on the reconstruction error under additional
conditions. By means of weighted l1 minimization method (1.8), that is to say, the requirement on
the RIC and ROC of the sensing matrix for guaranteeing stable and robust signal recovery can be
further relaxed if at least 50% of the support estimate is accurate; in addition, the reconstruction
error upper bound is provably to be smaller under additional conditions. Our result implies that
the achievable performance of signal recovery via weighted l1 minimization method (1.8) is actually
better than the works by Cai and Zhang [7] under some conditions.

The rest of the paper is organized as follows. In Section 2, we will introduce some notations
and some basic lemmas that will be used. The main results are given in Section 3, and the proofs

4

of our main results are presented in Section 4.

2 Preliminaries

Let us begin with basic notations. For arbitrary x ∈ RN , xmax(k) is deﬁned as x with all but the
largest k entries in absolute value set to zero, i.e. xmax(k) is the best k−term approximation of x,
and x− max(k) = x − xmax(k). Let T0 be the support of xmax(k), with T0 ⊆ {1, . . . , N} and |T0| ≤ k.

the size of the estimated support to the size of the actual support of xmax(k) (or the support of x if x

Let eT ⊆ {1, . . . , N} be the support estimate of x with |eT| = ρk, where ρ ≥ 0 represents the ratio of
0 ∩eT with |eTα| = α|eT| = αρk and |eTβ| = β|eT| = βρk,
is k− sparse). Denote eTα = T0∩eT and eTβ = T c
where α denotes the ratio of the number of indices in T0 that were accurately estimated in eT to
the size of eT and α + β = 1. For arbitrary nonnegative number ζ, we denote by [[ζ]] an integer

satisfying ζ ≤ [[ζ]] < ζ + 1. Moreover, for given set T ⊆ {1, . . . , N}, we denote by xT the vector
which equals to x on T and 0 on the component T c.

We ﬁrst state three key technical tools used in the proof of the main result. Lemma 2.1
was introduced by Cai and Zhang ([7], Lemma 5.1) which provides a way to estimate the inner
product by the ROC when only one component is sparse. Lemma 2.2 introduced by Cai and Zhang
([8], Lemma 5.3) provides an inequality between the sum of the αth power of two sequences of
nonnegative numbers based on the inequality of their sums. Cai, Wang and Xu ([2], Lemma 1)
supplied Lemma 2.3 that reveals the relationship between ROC’s of diﬀerent orders.

Lemma 2.1 ([7], Lemma 5.1). Let k1, k2 ≤ N and λ ≥ 0. Assume u, v ∈ RN have disjoint supports
and u is k1−sparse. If kvk1 ≤ λk2 and kvk∞ ≤ λ, then

|hAu, Avi| ≤ θk1,k2kuk2 · λpk2.

Lemma 2.2 ([8], Lemma 5.3). Assume m ≥ k, a1 ≥ a2 ≥ ··· ≥ am ≥ 0,
all α ≥ 1,

ai ≥

kPi=1

mPi=k+1

ai, then for

More generally, assume a1 ≥ a2 ≥ ··· ≥ am ≥ 0, λ ≥ 0 and

ai + λ ≥

mPi=k+1

ai, then for all α ≥ 1,

mXj=k+1

aα
j ≤

aα
i .

kXi=1

j ≤ k(cid:16) αsPk

aα

i=1 aα
i
k

mXj=k+1

5

kPi=1
k(cid:17)α

λ

+

.

Lemma 2.3 ([2], Lemma 1). For any τ ≥ 1 and positive integers k, k′ such that τ k′ is an integer,
then

θk,τ k′ ≤ √τ θk,k′.

As we mentioned in the introduction, Cai and Zhang [7] provided the sharp suﬃcient condition
for ensuring exact and stable sparse signals reconstruction via l1 minimization (1.4). Their main
result can be stated as below.

Theorem 2.1 ([7], Theorem 2.6). Let y = Ax + z with kzk2 ≤ ε and bxl2 is the minimizer of (1.4)

with B = Bl2(η) = {z : kzk2 ≤ η} for some η ≥ ε. If

δa + Ca,b,kθa,b < 1

for some positive integers a and b with 1 ≤ a ≤ k, where

Ca,b,k = max( 2k − a
√ab

a ) ,
,r 2k − a

then

where

kbxl2 − xk2 ≤ C0(ε + η) + C1 · 2kx− max (k)k1,

C0 = p2(1 + δa)k/a

1 − δa − Ca,b,kθa,b

, C1 =

√2kCa,b,kθa,b

(1 − δa − Ca,b,kθa,b)(2k − a)

(2.1)

(2.2)

(2.3)

(2.4)

+

1
√k

.

Theorem 2.2 ([7], Theorem 2.7). Let y = Ax + z with kAT zk∞ ≤ ε and bxDS is the minimizer of
(1.4) with B = BDS(η) = {z : kAT zk∞ ≤ η} for some η ≥ ε. If δa + Ca,b,kθa,b < 1 for some positive
integers a and b with 1 ≤ a ≤ k, where Ca,b,k = max(cid:26) 2k−a√ab

a (cid:27) , then
kbxDS − xk2 ≤ C′0(ε + η) + C′1 · 2kx− max (k)k1,

,q 2k−a

(2.5)

(2.6)

where

√2k

C′0 =

1 − δa − Ca,b,kθa,b

, C′1 = C1.

Cai and Zhang pointed out that the suﬃcient condition (2.1) is sharp in Theorem 2.8 (see [7]).
Namely, if δa + Ca,b,kθa,b = 1, there does not exist any method that can exactly recover all k−sparse
signals in noiseless case. Also, in noisy case, for any ε > 0, δa + Ca,b,kθa,b < 1 + ε can not guarantee
the stable recovery of all k−sparse signals.

6

3 Main results

Theorem 3.1. Let x ∈ RN be an arbitrary signal and its best k−term approximation support on
T0 ⊆ {1, . . . , N} with |T0| ≤ k. Let eT ⊆ {1, . . . , N} be an arbitrary set and denote ρ ≥ 0 and
0 ≤ α ≤ 1 such that |eT| = ρk and |eT ∩ T0| = αρk. Let y = Ax + z with kzk2 ≤ ε and bxl2 is the

minimizer of (1.8) with (1.5). If

δa + C α,ω

a,b,kθa,b < 1

for some positive integers a and b with 1 ≤ a ≤ k, where

C α,ω

a,b,k = max(cid:26) s
√ab

,r s
a(cid:27)

with

Then

where

s =hhk − a + ωk + (1 − ω)p(1 + ρ − 2αρ)k · max{p(1 + ρ − 2αρ)k,√a}ii .

0 k1 + (1 − ω)kxeT c∩T c

0 k1(cid:17) ,

kbxl2 − xk2 ≤ D0(2ε) + D1 · 2(cid:16)ωkxT c
D0 = p2(1 + δa)d/a
1 − δa − C α,ω
√2dC α,ω
(1 − δa − C α,ω

a,b,kθa,b
a,b,sθa,b

D1 =

,

+

1
√d

.

a,b,kθa,b)s

(3.1) holds. If

Let y = Ax + z with kAT zk∞ ≤ ε. Assume that bxDS is the minimizer of (1.8) with (1.6) and

a,b,kθa,b < 1
for some positive integers a and b with 1 ≤ a ≤ k, where

δa + C α,ω

(3.1)

(3.2)

(3.3)

(3.4)

(3.5)

(3.6)

(3.7)

where s is given in (3.3). Then

where

C α,ω

√ab

,r s
a,b,k = max(cid:26) s
a(cid:27) ,
kbxDS − xk2 ≤ D′0(2ε) + D′1 · 2(cid:16)ωkxT c

0 k1 + (1 − ω)kxeT c∩T c

0 k1(cid:17) ,

D′0 =

, D′1 = D1.

√2d

1 − δa − C α,ω

a,b,kθa,b

7

Here

d =(

k,

ω = 1,

max{k, (1 + ρ − 2αρ)k}, 0 ≤ ω < 1.

(3.8)

Remark 3.1. In Theorem 3.1, we observed that every signal x ∈ RN can be stably and robustly
recovered. And if B = {0} and x is a k−sparse signal, then Theorem 3.1 ensures exact recovery of
the signal x.

When the the measurement model (1.1) is with Gaussian noise, the above results on the bounded
noise case can be directly applicable to the case where the noise is Gaussian by using the same
argument as in [2, 5]. This is due to the fact Gaussian noise is essentially bounded. The concrete
content is stated as follows.

δa +C α,ω

a,b,kθa,b < 1 for some positive integers a and b with 1 ≤ a ≤ k, where C α,ω

Remark 3.2. Let x ∈ RN be an arbitrary signal and its best k−term approximation support
on T0 ⊆ {1, . . . , N} with |T0| ≤ k. Let eT ⊆ {1, . . . , N} be an arbitrary set and deﬁne ρ ≥ 0
and 0 ≤ α ≤ 1 such that |eT| = ρk and |eT ∩ T0| = αρk. Assume that z ∼ Nn(0, σ2I) in (1.1) and
ao
,p s
with s = hhk − a + ωk + (1 − ω)p(1 + ρ − 2αρ)k · max{p(1 + ρ − 2αρ)k,√a}ii. Let Bl2 = {z :
kzk2 ≤ σpn + 2√n log n} and BDS = {z : kAT zk∞ ≤ σ√2 log N}. bxl2 and bxDS is the minimizer

of (1.8) with Bl2 and BDS, respectively. Then, with probability at least 1 − 1/n,

a,b,k = maxn s√ab

and

kbxl2 − xk2 ≤ D0(2σqn + 2pn log n) + D1 · 2(cid:16)ωkxT c
kbxDS − xk2 ≤ D′0(2σp2 log N ) + D′1 · 2(cid:16)ωkxT c

0 k1(cid:17) ,
0 k1 + (1 − ω)kxeT c∩T c
0 k1(cid:17) ,

0 k1 + (1 − ω)kxeT c∩T c

with probability at least 1 − 1/√π log N .
Theorem 3.2. Let 1 ≤ a ≤ s ≤ k, a + s ≤ N and b ≥ 1, where s is deﬁned as (3.3). Then there
ao and
,p s
exists a sensing matrix A ∈ Rn×N satisfying δa + C α,ω
some k−sparse vector η ∈ RN such that the weighted l1 minimization method (1.8) fails to exactly
recover the k−sparse vector η in the noiseless case and stably recover the k−sparse vector η in the
noise case.

a,b,k = maxn s√ab

a,b,kθa,b = 1 where C α,ω

Remark 3.3. Theorem 3.2 implies that for arbitrarily ε > 0, δa + C α,ω
a,b,kθa,b < 1 + ε is not suﬃcient
to guarantee the exact recovery of all k−sparse vectors in noiseless case and the stable recovery of
all k−sparse vectors in noise case.
Proposition 3.1. Let s be deﬁned as (3.3) and d be deﬁned as (3.8).

8

(1) If ω = 1, then s = 2k − a, d = k. The suﬃcient condition (3.1) of Theorem 3.1 is identical to
that of Theorem 2.1 and Theorem 2.2 with (2.1), and D0 = C0, D1 = C1, D′0 = C′0, D′1 = C′1.
Moreover, the condition is sharp.

(2) If α = 1

2 , then s = 2k−a and d = k. The suﬃcient condition (3.1) of Theorem 3.1 is identical to
that of Theorem 2.1 and Theorem 2.2 with (2.1), and D0 = C0, D1 = C1, D′0 = C′0, D′1 = C′1.
Moreover, the condition is sharp.

(3) Assume 0 ≤ ω < 1. If α > 1

2 , then s < 2k − a and d = k. The suﬃcient condition (3.1)
in Theorem 3.1 is weaker than that of Theorem 2.1 and Theorem 2.2 with (2.1), and D0 <
C0, D′0 < C′0.

(4) Suppose 0 ≤ ω < 1. If α > 1
(5) Suppose 0 ≤ ω < 1. If α > 1

2 and b ≤ s, then D1 < C1.
2 and s < b ≤ 2k−a, then D1 < C1 if and only if 1−δa−C α,ω

a,b,kθa,b <

θa,b.

2k−a−√bs
√a(√b−√s)
q 2k−a

a θa,b.

(6) Suppose 0 ≤ ω < 1. If α > 1

2 and b > 2k − a, then D1 < C1 if and only if 1 − δa − C α,ω

a,b,kθa,b <

4 Proofs

Proof of Theorem 3.1. Firstly, we show the estimate (3.4). Let h =bxl2 − x, where x is the original
signal andbxl2 is the minimizer of (1.8) with (1.5). We can express h as h =

ciui, where {ci}N
are nonnegative and decreasing, i.e. c1 ≥ c2 ≥ ··· ≥ cN ≥ 0, {ui}N
i=1 are diﬀerent unit vectors with
one entry of ±1 and other entries of zeros. From the following inequality proved by Friedlander
et al. (see (21) in [16])

NPi=1

i=1

khT c

0 k1 ≤ ωkhT0k1 + (1 − ω)khT0∪eT\eTαk1 + 2(cid:16)ωkxT c

0 k1 + (1 − ω)kxeT c∩T c

0 k1(cid:17) ,

(4.1)

we have

ci = kh− max(k)k1 ≤ ωkhT0k1 + (1 − ω)khT0∪eT\eTαk1 + 2(cid:16)ωkxT c

NXi=k+1
Noting that |T0 ∪ eT \ eTα| = (1 + ρ − 2αρ)k, thus
aPi=1

kh− max(a)k∞ = ca+1 ≤

ci

a

= khmax(a)k1

a

≤ khmax(a)k2

√a

,

0 k1 + (1 − ω)kxeT c∩T c

0 k1(cid:17) .

9

kh− max(a)k1 =

a

k

≤

≤

ci

≤

ci +

k − a

k − a

k − a

kXi=a+1

NXi=k+1
ci + ωkhT0k1 + (1 − ω)khT0∪eT\eTαk1 + 2(cid:16)ωkxT c

0 k1(cid:17)
0 k1 + (1 − ω)kxeT c∩T c

0 k1(cid:17)
0 k1 + (1 − ω)kxeT c∩T c

kXi=1
a khmax(a)k1 + ω√kkhT0k2 + (1 − ω)p(1 + ρ − 2αρ)kkhT0∪eT\eTαk2
+ 2(cid:16)ωkxT c
√a khmax(a)k2 + ω√kr k
+ (1 − ω)p(1 + ρ − 2αρ)k · max(r (1 + ρ − 2αρ)k
+ 2(cid:16)ωkxT c
=(cid:16)k − a + ωk + (1 − ω)p(1 + ρ − 2αρ)k · max{p(1 + ρ − 2αρ)k,√a}(cid:17) khmax(a)k2
+ 2(cid:16)ωkxT c
≤skhmax(a)k2

, 1) khmax(a)k2

akhmax(a)k2

√a

√a

0 k1 + (1 − ω)kxeT c∩T c

0 k1(cid:17) ,

0 k1(cid:17)
0 k1 + (1 − ω)kxeT c∩T c
0 k1(cid:17)
0 k1 + (1 − ω)kxeT c∩T c
+ 2(cid:16)ωkxT c
2(cid:18)ωkxT c

√a

a, k2 = s, λ = khmax(a)k2
obtain

where s = hhk − a + ωk + (1 − ω)p(1 + ρ − 2αρ)k · max{p(1 + ρ − 2αρ)k,√a}ii . Taking k1 =
 .
|hAhmax(a), Ah− max(a)i| ≤θa,skhmax(a)k2√s ·khmax(a)k2

0 k1(cid:17)
0 k1 + (1 − ω)kxeT c∩T c

, from above inequalities and Lemma 2.1 , we

2(cid:16)ωkxT c

0 k1+(1−ω)kx eT c∩T c

0 k1(cid:19)

√a

+

+

s

s

Combining the deﬁnition of δk and the fact that

we have

kAhk2 = kAbxl2 − Axk2 ≤ ky − Abxl2k2 + kAx − yk2 ≤ 2ε,

|hAhmax(a), Ahi| ≤ kAhmax(a)k2kAhk2

≤p1 + δakhmax(a)k2 · (2ε).

10

(4.2)

(4.3)

Hence,

(2ε)p1 + δakhmax(a)k2 ≥ |hAhmax(a), Ahi|

2 − |hAhmax(a), Ah− max(a)i|

≥ kAhmax(a)k2

≥ (1 − δa)khmax(a)k2
=(cid:18)1 − δa −r s

a

2 − θa,skhmax(a)k2√s ·khmax(a)k2
2(cid:16)ωkxT c
θa,s(cid:19)khmax(a)k2

2 − θa,skhmax(a)k2

√a

+

0 k1(cid:17)
0 k1 + (1 − ω)kxeT c∩T c

2(cid:16)ωkxT c
0 k1(cid:17)
0 k1 + (1 − ω)kxeT c∩T c

√s

s

.



It follows from the above inequality that

khmax(a)k2 ≤

√1 + δa(2ε)
1 − δa −p s
a θa,s

+

Deﬁne

θa,s

1 − δa −p s

a θa,s

2(cid:16)ωkxT c

0 k1(cid:17)
0 k1 + (1 − ω)kxeT c∩T c

√s

.

d =(

k,

ω = 1,

max{k, (1 + ρ − 2αρ)k}, 0 ≤ ω < 1.

With (4.1), it is clear that

From Lemma 2.2, we have

kh− max(d)k1 ≤ khmax(d)k1 + 2(cid:16)ωkxT c
2(cid:16)ωkxT c

kh− max(d)k2 ≤ khmax(d)k2 +

0 k1(cid:17) .
0 k1 + (1 − ω)kxeT c∩T c
0 k1(cid:17)
0 k1 + (1 − ω)kxeT c∩T c

.

√d

11

Therefore,

khk2 =qkhmax(d)k2
≤vuuutkhmax(d)k2
≤q2khmax(d)k2
=vuut2
dXi=1
≤vuut 2d
aXi=1
=r 2d
≤ p2(1 + δa)d/a
1 − δa −p s

c2
i +

c2
i +

Since

a

a khmax(a)k2 +

θa,s = θa,

min{b,s}

where C α,ω

a,b,k = max{ s√ab
khk2 ≤ p2(1 + δa)d/a
1 − δa − C α,ω
So, (3.4) is obtained.

a,b,kθa,b

2 + kh− max(d)k2

2

2



2 +

√d

√d

2(cid:16)ωkxT c

0 k1(cid:17)
0 k1 + (1 − ω)kxeT c∩T c

2 +khmax(d)k2 +
√d
0 k1(cid:17)
2(cid:16)ωkxT c
0 k1 + (1 − ω)kxeT c∩T c
0 k1(cid:17)
2(cid:16)ωkxT c
0 k1 + (1 − ω)kxeT c∩T c
0 k1(cid:17)
2(cid:16)ωkxT c
0 k1 + (1 − ω)kxeT c∩T c
√d
0 k1(cid:17)
2(cid:16)ωkxT c
0 k1 + (1 − ω)kxeT c∩T c
√d! · 2(cid:16)ωkxT c
(2ε) + 
p2d/aθa,s
(1 − δa −p s
min{b,s} ≤r s
,p s
(2ε) + 

a θa,s)√s
θa,min{b,s} ≤ max{r s

min{b, s}

a,b,kθa,b

√d

+

a θa,s

1

b

s

0 k1 + (1 − ω)kxeT c∩T c

0 k1(cid:17) .

, 1}θa,b =r a

s

C α,ω

a,b,kθa,b,

a}, and the ﬁrst inequality follows from Lemma 2.3. Consequently,
0 k1(cid:17)
0 k1 + (1 − ω)kxeT c∩T c

√d! · 2(cid:16)ωkxT c

√2dC α,ω
(1 − δa − C α,ω

a,b,kθa,b)s

+

1

Next, we can prove (3.6) going along similar lines to that of (3.4). To prove(3.6), we only need

to use the following (4.4) and (4.5) instead of (4.2) and (4.3), respectively.

kAT Ahk∞ = kAT A(bxDS − x)k∞

≤ kAT (AbxDS − y)k∞ + kAT (y − Ax)k∞

≤ 2ε,

|hAhmax(a), Ahi| = |hhmax(a), AT Ahi|
≤ khmax(a)k1kAT Ahk∞
≤ √akhmax(a)k2 · (2ε).

12

(4.4)

(4.5)

This completes the proof of Theorem 3.1.

(cid:3)

Proof of Theorem 3.2. Firstly, let L = a + s, and

, 0, . . . , 0) ∈ RN ,

if L − k ≤ ρk,

Due to kξ1k2 = 1, we extend ξ1 into an orthonormal basis {ξ1, . . . , ξN} of RN . Next, we deﬁne the
linear map A : RN → RN such that for all x =

ξ1 =

or

ξ1 =

1
√L

1
√L

L

z }| {
1, . . . , 1, 0, . . . , 0) ∈ RN ,
(
| {z }

z }| {
{z
|

1, . . . , 1, 0, . . . , 0

(1, . . . , 1

k−αρk

L−k

ρk

,

if L − k > ρk,

, 1, . . . , 1

αρk

| {z }
}
NPi=1
ciξi ∈ RN ,
(x − hξ1, xiξ1) =r1 +
L + s(cid:19)(cid:0)kxk2

ciξi.

L − s
L + s

NXi=2
2 − |hξ1, xi|2(cid:1) ,
a
Lkxk2

2 ≤

Ax =r1 +

L − s
L + s

L − s

2 =(cid:18)1 +
|ξ1(i)|2 ≤ kxk2

kAxk2
2 · Xi∈supp(x)

Then for any a−sparse signal x, we can easily gain

and

|hξ1, xi|2 ≤ kxk2

2 · kξ1,max(a)k2

2 =

L − s
L kxk2
2.

Hence,

(cid:18)1 +

L − s

L + s(cid:19)kxk2

2 ≥ kAxk2

2 ≥(cid:18)1 +

L − s

L + s(cid:19) (1 −

L − s
L

)kxk2

2 =(cid:18)1 −

L − s

L + s(cid:19)kxk2

2,

which deduces

δa ≤

L − s
L + s

.

i=1 lidi.

(i) When b ≤ s, through a simple calculation, it can be concluded that

Finally, we estimate θa,b. For arbitrary a−sparse vector u ∈ RN and b−sparse vector v ∈ RN
with disjoint supports, we deﬁne u = PN
i=1 diξi. It follows immediately that
0 = hu, vi =PN
|l1| = |hξ1, ui| ≤ kuk2 · Xi∈supp(u)
|d1| = |hξ1, vi| ≤ kvk2 · Xi∈supp(v)

i=1 liξi and v = PN
|ξ1(i)|2
|ξ1(i)|2

≤ kuk2 · kξ1,max(a)k2 ≤r a
≤ kvk2 · kξ1,max(b)k2 ≤r b

Lkuk2,

Lkvk2.

and

1/2

1/2

13

It then follows that

Accordingly,

Therefore,

1

1 + L−s

L+s |hAu, Avi| = |

lidi| = | − l1d1| ≤

√ab
L kuk2kvk2.

NXi=2

θa,b ≤ (1 +

L − s
L + s

)

√ab
L

.

δa + C α,ω

a,b,kθa,b ≤

=

L − s
L + s
L − s
L + s

= 1.

+ max(cid:26) s
√ab
s
√ab

(1 +

+

,r s
a(cid:27) · (1 +

L − s
L + s

)

√ab
L

L − s
L + s

)

√ab
L

(ii) When b > s, without loss of generality, we can suppose that u and v are nonzero. If u = 0 or
v = 0, clearly hAu, Avi = 0 ≤ Ckuk2kvk2 holds for all C > 0. We normalize u and v such that
kuk2 = kvk2 = 1. Because u is a−sparse and v is b−sparse, and u, v have disjoint supports, we
conclude

and

,

s + a

L

|l1| = |hξ1, ui| ≤r a
Lkuk2 =r a
(cid:12)(cid:12)(cid:12)(cid:12)d1 ±r a
l1(cid:12)(cid:12)(cid:12)(cid:12) =(cid:12)(cid:12)(cid:12)(cid:12)(cid:28)ξ1, v ±r a
=rkvk2

=r a
u(cid:29)(cid:12)(cid:12)(cid:12)(cid:12) ≤(cid:13)(cid:13)(cid:13)(cid:13)v ±r a
2 =rs + a

a
skuk2

2 +

s

s

s

.

s

u(cid:13)(cid:13)(cid:13)(cid:13)2

14

In view of |l1| ≤q a

a+s and 1 ≤ a ≤ s,

1

1 + L−s

L+s |hAu, Avi| = |

l1(cid:12)(cid:12)(cid:12)(cid:12)(cid:27) −(cid:12)(cid:12)(cid:12)(cid:12)r a

s

l1(cid:12)(cid:12)(cid:12)(cid:12)(cid:19) · |l1|

s

s

lidi| = | − l1d1|

NXi=2
=(cid:18)max(cid:26)(cid:12)(cid:12)(cid:12)(cid:12)d1 +r a
l1(cid:12)(cid:12)(cid:12)(cid:12) ,(cid:12)(cid:12)(cid:12)(cid:12)d1 −r a
s|l1|!
≤ |l1| r s + a
s −r a
s |l1|2 −r s + a
a |l1|!
= −ra
s |l1| −
a !2
2r s + a
= −ra
s r a
a !2
2r s + a
≤ −ra

+

1

1

s + a −
√as
L

,

√as
s + a

=

=

s + a
4√as

+

s + a
4√as

which implies

Hence,

θa,b ≤ (1 +

L − s
L + s

)

√as
L

.

δa + C α,ω

a,b,kθa,b ≤

=

L − s
L + s
L − s
L + s

= 1.

√ab

+ max(cid:26) s
+r s

(1 +

a

,r s
a(cid:27) · (1 +

L − s
L + s

)

√as
L

L − s
L + s

)

√as
L

In a word, δa + C α,ω
Next, we deﬁne

a,b,kθa,b ≤ 1 has been proved.

η = (

γ = (0, . . . , 0

αρk

0, . . . , 0,
,−1, . . . ,−1,

k−αρk

k−αρk
1, . . . , 1,

z }| {
| {z }
| {z }

k−αρk

,

ρk

z }| {
|
{z
}|
z
|

ρk
L−k

αρk

0, . . . , 0

z }| {
1, . . . , 1, 0, . . . , 0) ∈ RN ,
, 0, . . . , 0) ∈ RN ,
,−1, . . . ,−1
| {z }
}
}
{
, 0, . . . , 0) ∈ RN ,
{z
}

|
{z
| {z }

L−k−ρk

, 0, . . . , 0

αρk

ρk

−1, . . . ,−1, 0, . . . , 0

or

γ = (0, . . . , 0

if L − k > ρk,

if L − k ≤ ρk.

15

From 1 ≤ a ≤ s ≤ k and L = a + s, we have L − k ≤ k. Hence η and γ are k−sparse. Moreover,
kηk1,w = k, kγk1,w ≤ L − k ≤ k. Note that kγk1,w ≤ kηk1,w and ξ1 = 1√L
(η − γ). Since Aξ1 = 0,
we obtain Aη = Aγ.

(i) kγk1,w < kηk1,w.
In the noiseless case y = Aη, if weighted l1 minimization method (1.8) can exactly recover η,

η with constraint B, i.e., lim

In the noise case y = Aη + z, suppose weighted l1 minimization method (1.8) can stable recover

namely, bη = η. Clearly, kbηk1,w = kηk1,w. It contradicts that kγk1,w < kηk1,w.
z→0bη = η. Due to y − A(bη − η + γ) = y − Abη ∈ B and the deﬁnition ofbη,
it follows immediately that kbηk1,w ≤ kbη − η + γk1,w. Thus, we have kηk1,w ≤ kγk1,w as z → 0. It

(ii) kγk1,w = kηk1,w. The weighted l1 method (1.8) does not distinguish k−sparse signals η and

contradicts that kγk1,w < kηk1,w.

γ based y and A.

Hence the weighted l1 method (1.8) does not exactly and stably recover the k−sparse signal η
a,b,kθa,b = 1. This completes the proof

based on A and y. Combining Theorem 3.1, we have δa + C α,ω
of the theorem.

(cid:3)

Proof of Proposition 3.1. For (1) and (2), when ω = 1 or α = 1
2 , by simple calculation, we have
s = 2k − a, d = k. Then, it is easy to imply (1) and (2) by comparing Theorem 3.1 with Theorem
2.1 and Theorem 2.2.

2 , by means of the deﬁnition of s in (3.3) and d in (3.8), it follows

immediately that s < 2k − a, d = k.
< 2k−a√ab

(3) Let 0 ≤ ω < 1. If α > 1
When b ≤ s, C α,ω
a,b,k = s√ab
a,b,k =p s
When s < b ≤ 2k − a, C α,ω
a,b,k =p s
When b ≥ 2k − a, C α,ω
For any positive integers a and b with 1 ≤ a ≤ k, in short, we obtain C α,ω

a <q 2k−a

= Ca,b,k.
a < 2k−a√ab

a = Ca,b,k.

= Ca,b,k.

implies δa + C α,ω

a,b,kθa,b < δa + Ca,b,kθa,b. Thus the condition δa + C α,ω
√2(1+δa)k/a
1−δa−Ca,b,kθa,b

√2(1+δa)k/a
1−δa−C α,ω
a,b,kθa,b

<

a,b,k < Ca,b,k, which
a,b,kθa,b < 1 in (3.1) is weaker than

= C0, D′0 =

√2k
1−δa−C α,ω

a,b,kθa,b

<

δa + Ca,b,kθa,b < 1 in (2.1) and D0 =

1−δa−Ca,b,kθa,b

√2k
(4) Assume 0 ≤ ω < 1. If α > 1

= C′0, which implies (3).

Combining the deﬁnition of C1 and D1, obviously, D1 =
C1.

(5) Since α > 1

we just need to prove
1 − δa < 2k−a−s
√a(√b−√s)

2 and s < b ≤ 2k − a, C α,ω
+ 1√k
<
θa,b, namely, 1 − δa − C α,ω

√2kq 1
(1−δa−√ s

a θa,b
a θa,b)√s

a,b,k = p s
√2kq 1
1−δa− 2k−a√ab
a,b,kθa,b < 2k−a−

a , Ca,b,k = 2k−a√ab
+ 1√k
θa,b.

θa,b
√bs
√a(√b−√s)

ab θa,b

.

16

2 and b ≤ s, we have d = k and δa + C α,ω

√2k 1√ab
1−δa−C α,ω

θa,b

a,b,kθa,b

+ 1√k

<

a,b,kθa,b < δa + Ca,b,kθa,b.

√2k 1√ab
1−δa−Ca,b,kθa,b

θa,b

+ 1√k

=

. Thus, to prove D1 < C1,

It is equal to prove that

(6) Due to α > 1

2 and b > 2k − a, we have C α,ω

√2kq 1
(1−δa−√ s

a θa,b
a θa,b)√s

is equal to prove that

1 − δa <

√2k−a+√s

√a

θa,b, i.e., 1 − δa − C α,ω

+ 1√k

<

a and Ca,b,k =q 2k−a
a,b,k =p s
√2kq 1
(1−δa−q 2k−a
a,b,kθa,b <q 2k−a
a θa,b.

a θa,b
a θa,b)√2k−a

+ 1√k

a . To show D1 < C1

. It suﬃces to prove

(cid:3)

Acknowledgments

This work was supported by the NSF of China (Nos.11271050, 11371183) and Beijing Center for
Mathematics and Information Interdisciplinary Sciences (BCMIIS).

References

[1] R. V. Borries, C. Miosso and C. Potes, Compressed sensing using prior information,

in
2nd IEEE Int. Workshop on Computational Advances in Multi-Sensor Adaptive Processing,
CAMPSAP 2077, 12-14, 2007, pp. 121-124.

[2] T. T. Cai, L. Wang and G. W. Xu, Shifting inequality and recovery of sparse signals, IEEE

Trans. Signal Process, 58(3), pp. 1300-1308, 2010.

[3] T. T. Cai, L. Wang and G. W. Xu, New bounds for restricted isometry constants, IEEE Trans.

Inform. Theory, 56(9), pp. 4388-4394, 2010.

[4] T. T. Cai, L. Wang and G. Xu, Stable recovery of sparse signals and an oracle inequality,

IEEE, Trans. Inform. Theory, 56(7), pp. 3516-3522, 2010.

[5] T. T. Cai, G. W. Xu, J. Zhang, On recovery of sparse signal via l1 minimization, IEEE Trans.

Inf. Theory, 55(7), pp. 3388-3397, 2009.

[6] T. T. Cai and A. Zhang, Spares representation of a polytope and recovery of sparse signals

and low-rank matrices, IEEE Trans. Inform. Theory, 60(1), pp. 122-132, 2014.

[7] T. T. Cai and A. Zhang, Compressed sensing and aﬃne rank minimization under restricted

isometry, IEEE Trans. Signal Process., 61(13), pp. 3279-3290, 2013.

[8] T. T. Cai and A. Zhang, Sharp RIP bound for sparse signal and low-rank matrix recovery,

Appl. Comput. Harmon. Anal., 35, pp. 74-93, 2013.

[9] E. J. Cand`e, J. Romberg and T. Tao, Stable signal recovery from incomplete and inaccurate

measurements, Commum. Pure Appl. Math., 59, pp. 1207-1223, 2006.

17

[10] E. J. Cand`e, J. Romberg and T. Tao, Robust uncertainty principles: Exact signal reconstruc-
tion from highly incomplete frequency information, IEEE Trans. Inform. Theory, 52(20), pp.
489-509, 2006.

[11] E. J. Cand`es and T. Tao, The Dantzing selector: Statistical estimation when p is much larger

than n (with discussion), Ann. Statist., 35, pp. 2313-2351, 2007.

[12] E. J. Cand`es and T. Tao, Decoding by linear programming, IEEE Trans. Inf. Theory, 51(12),

pp. 4203-4215, 2005.

[13] D. Donoho, Compressed sensing, IEEE Trans. Inform. Theory, 52, pp. 11289-1306, 2006.

[14] D. Donoho and M. Elad, Optimally sparse representation in general (nonorthogonal) dictio-

naries via l1 minimization, Proc. Natl. Acad. Sci. USA, 100(5), pp. 2197-2202, 2003.

[15] D. Donoho and X. Huo, Uncertainty principles and ideal atomic decomposition, IEEE, Trans.

Inform. Theory, 47(7), pp. 2845-2862, 2001.

[16] M. P. Friedlander, H. Mansour, R. Saab and O. Yilmaz, Recoverying compressively sampled
signals using partial support information, IEEE Transactions Information Theory, 58(2), pp.
1122-1134, 2012.

[17] J. J. Fuchs, On sparse representations in arbitrary redundant bases, IEEE, Trans. Inform.

Theory, 50(6), pp.1341-1344, 2004.

[18] L. Jacques, A short note compressed sensing with partially known signal support, Signal

Process., 90, pp. 3308-3312, 2010.

[19] M. A. Khajehnejad, W. Xu, A. S. Avestimehr and B. Hassibi, Weighted l1 minimization for
sparse recovery with prior information, in IEEE Int. Symp. Information Theory, ISIT 2009,
2009, pp. 483-487.

[20] S. Li and H. M. Wang, Signal recovery with partially known signal support (in Chinese), Sci.

Sin. Math., 42, pp. 313-319, 2012.

[21] W. Lu and N. Vaswani, Exact reconstruction conditions and error bounds for regularized

modiﬁed basis pursuit, in Proc. Asilomar Conf. on Signals, Systems and Computers, 2010.

[22] W. Lu and N. Vaswani, Modiﬁed basis pursuit denoising (modiﬁedbpdn) for noisy compressive
sensing with partially known signal support, in IEEE Int. Conf. Acoustics Speech and Signal
Processing (ICASSP), 2010, 14-19, 2010, pp. 3926-3929.

18

[23] Q. Mo and S. Li, New bounds on the restricted isometry constant δ2k, Appl. Comput. Harmon.

Anal., 31(3), pp. 3335460-468, 2011.

[24] N. Vaswani and W. Lu, Modiﬁed-CS: Modifying compressive sensing for problems with par-

tially known support, IEEE Trans. Signal Process., 58(9), pp. 4595-4607, 2010.

[25] N. Vaswani and W. Lu, Modiﬁed-CS: Modifying compressive sensing for problems with par-

tially known support, IEEE Int. Symp. Information Theory, ISIT 2009, pp. 488-492, 2009.

[26] Z. Q. Xu, Compressd sensing: A survey (in Chinese), Sci. Sin. Math., 42, pp. 865-877, 2012.

19

