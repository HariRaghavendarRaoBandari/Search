Stochastic optimal control using semideﬁnite programming

for moment dynamics

Andrew Lamperski, Khem Raj Ghusinga, and Abhyudai Singh

6
1
0
2

 
r
a

 

M
1
2

 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
9
0
3
6
0

.

3
0
6
1
:
v
i
X
r
a

Abstract— This paper presents a method to approximately
solve stochastic optimal control problems in which the cost
function and the system dynamics are polynomial. For stochas-
tic systems with polynomial dynamics, the moments of the
state can be expressed as a, possibly inﬁnite, system of de-
terministic linear ordinary differential equations. By casting
the problem as a deterministic control problem in moment
space, semideﬁnite programming is used to ﬁnd a lower bound
on the optimal solution. The constraints in the semideﬁnite
program are imposed by the ordinary differential equations for
moment dynamics and semideﬁniteness of the outer product of
moments. From the solution to the semideﬁnite program, an
approximate optimal control strategy can be constructed using
a least squares method. In the linear quadratic case, the method
gives an exact solution to the optimal control problem. In more
complex problems, an inﬁnite number of moment differential
equations would be required to compute the optimal control
law. In this case, we give a procedure to increase the size
of the semideﬁnite program, leading to increasingly accurate
approximations to the true optimal control strategy.

I. INTRODUCTION

Stochastic optimal control problems frequently arise in
a variety of settings such as engineering, management,
ﬁnance, ecology, etc. where a cost function is minimized
by choosing the inputs to a stochastic differential equation
[1]. The solution to a stochastic optimal control problem is
typically formulated using the value function approach [2].
It turns out that aside from a few special cases such as linear
quadratic control problems, it is generally not possible to ﬁnd
explicit solutions. For other problems, numerical methods
are employed which require discretization of the state-space
and time, incur signiﬁcant computational cost, and generally
suffer from the curse of dimensionality. Another prevalent
approach to study nonlinear stochastic control problems has
been to linearize the system to be able to apply the linear
stochastic control methods [3]–[5]. However, in these meth-
ods it is difﬁcult to get solutions with satisfactory accuracy
as the linearization is only valid for small deviations.

Alternatively, one can transform the problem to moment-
space wherein the cost function to be optimized is viewed
as a function of moments of the state and control variables.
Particularly when the cost function and the system dynamics

Andrew Lamperski is with the Department of Electrical and Computer
Engineering, University of Minnesota, Minneapolis, MN, USA 55455.
alampers@umn.edu

Khem Raj Ghusinga is with the Department of Electrical and Com-
puter Engineering, University of Delaware, Newark, DE, USA 19716.
khem@udel.edu

Abhyudai Singh is with the Faculty of Electrical

and Com-
puter Engineering, University of Delaware, Newark, DE, USA 19716.
absingh@udel.edu

are polynomial, the moment dynamics of the state is well
characterized by linear ordinary differential equations, which
allows one to study the problem as a deterministic optimal
control problem. In [6], [7], this approach has been used
for few examples considering the system dynamics to be
linear. In general, when the system dynamics is nonlinear,
the system of differential equations for moment dynamics
become inﬁnite dimensional.

Here, we provide a method to approximate the solution
to stochastic optimal control problems wherein the system
dynamics, and cost are polynomials. A lower bound for
the optimal value of the cost function is computed using a
semideﬁnite programming approach. Moreover, a polynomial
control policy with time-varying coefﬁcients can be extracted
from the semideﬁnite program, using which an upper bound
on the optimal value of the cost function can be computed
via Monte Carlo simulations. The idea here is that if the
difference between the upper and lower bounds is small,
the controller extracted from the semideﬁnite program must
be close to the optimal controller. Whereas for the systems
for which moment dynamics is ﬁnite dimensional, the lower
bound computed is unique; one can obtain an increas-
ing sequence of lower bounds for a system with inﬁnite-
dimensional moment dynamics by increasing the number
of moment equations and the number of constraints on the
moment dynamics in the semideﬁnite program. For details
on semideﬁnite programming, see [8], [9].

The paper is structured as follows. Section II discusses the
problem statement, introduces some examples that we use
to illustrate the method proposed in the paper, and brieﬂy
describes the results. In Section III, a system of ordinary
differential equations characterizing the moment dynamics
is derived for a stochastic differential equation. Section IV
presents the main results of the paper. These results are illus-
trated via examples in Section V. In section VI, conclusion
of the paper and some directions of future work are given.

Notation

Random variables will be denoted in bold: x, and u. Non-
random variables will be non-bold. For example, a speciﬁc
value taken by x would be denoted by x. The expected value
of a random variable, x is denoted by (cid:104)x(cid:105).

A. Setup

II. PROBLEM

This paper will provide a method to approximate the
solution to scalar polynomial optimal control problems of
the form:

(cid:42)(cid:90) T

0

minimize

u

(cid:43)

c(xt, ut)dt + h(x(T ))

(1a)

subject to

dxt = f (xt, ut)dt + g(xt, ut)dwt
bl(xt, ut) ≥ 0
for l = 1, . . . , B
x(0) = x0.

(1b)
(1c)
(1d)
Here xt ∈ R is the state and ut ∈ R is the control; f (x, u) :
R × R → R and g(x, u) : R × R → R are polynomials
that describe the system dynamics; bl(x, u) : R × R → RM
are polynomials describing constraints; and wt is a Weiner
process satisfying

(cid:10)dwt dw(cid:62)

t

(cid:11) = dt.

(cid:104)dwt(cid:105) = 0,

(2)

The control input u(t) is minimized over all Borel measure-
able functions that are adapted to the ﬁltration generated by
w(t). Typically, there is no loss of generality in restricting
the search to Markov control policies, i.e. policies of the
form, ut = γ(xt, t), where γ is a Borel-measurable function.
For details, see [1].

Since it is assumed that all of the functions, c, h, f, and
g are polynomials, without loss of generality they can be
expressed in the form:

cijxiuj,

h(x) =

nx(cid:88)
nx(cid:88)

i=0

hixi,

nu(cid:88)

i=0

j=0

g(x, u) =

i=0

j=0

nx(cid:88)
nx(cid:88)
nx(cid:88)

nu(cid:88)
nu(cid:88)
nu(cid:88)

j=0

i=0

i=0

j=0

c(x, u) =

f (x, u) =

bl(x, u) =

fijxiuj,

bl,ijxiuj.

(3a)

gijxiuj,

(3b)

(3c)

Example 1: A simple example where explicit solutions
can be calculated is the linear quadratic regulator problem.
For concreteness, a special case is given by

minimize

subject to

(cid:28)(cid:90) 1

0

(cid:0)x2

t + u2
t

(cid:29)
(cid:1) dt

dxt = utdt + dwt
x0 = 0.

(4a)

(4b)
(4c)

(5a)

(5b)
(5c)

See [10], [11] for more detailed discussion of this problem.
Example 2: Another problem, which will display some
of the more interesting aspects of the problem is given by:

minimize

subject to dxt =(cid:0)(1.5)2xt − x3

t + 0.1u2
t

0

(cid:28)(cid:90) 1

(cid:0)x2

x0 = 0.

(cid:1) dt + x2

1

t + ut

(cid:29)
(cid:1) dt + dwt

Unlike the linear quadratic regulator problem, it is not clear
how to compute exact solutions to this optimal control
problem.

Example 3: The previous examples had no constraints
constraints of the form (1c). Such constraints often arise in

applications. For example, consider the modiﬁed version of
the optimal ﬁsheries management from [12], [13]

(cid:42)(cid:90) T
(cid:43)
dxt =(cid:0)xt − γx2

utdt

0

t − ut

maximize

subject to

(cid:1) dt + σxtdwt

(6a)

(6b)
(6c)
(6d)
(6e)

x0 = x0
xt ≥ 0
ut ≥ 0.

Here xt models the population in a ﬁshery and ut models
the rate of harvesting. As in the earlier works, a constraint
that xt ≥ 0 is required to be physically meaningful. Also,
without this constraint, the optimal strategy would be to set
ut = +∞. In other words, the objective would be unbounded
without the constraint. The constraint that ut ≥ 0 encodes
the idea that ﬁsh are only being taken out, not put into the
ﬁshery.

The primary difference between this formulation and that
of [12] and [13], is that the cost is not discounted, and
operates over a ﬁxed, ﬁnite horizon.
equivalent to minimizing the objective multiplied by −1.

this is a maximization problem, but

Note that

this is

The systems are assumed to be scalars for notational
simplicity. Vector systems could be considered at the expense
of extra book-keeping.

B. Description of Results

Let vOPT be the optimal value of (1a). For polynomial
systems, this paper provides a method based on semideﬁnite
programming to compute a lower bound on the optimal
value, vSDP ≤ vOPT.

Furthermore, a control policy of the form

ut = p0(t) + p1(t)xt + p2(t)x2

t + ··· + pnp (t)xnp

t

(7)

can be computed from the result of the semideﬁnite program.
Let vp denote the value of (1a) resulting from this controller,
which can be computed or estimated by simulations. It
follows that

vSDP ≤ vOPT ≤ vp.

(8)

While the true optimal value, vOPT is typically unknown,
if vp − vSDP is small, the controller from (7) must be close
to optimal.

III. MOMENT DYNAMICS OF A CONTROLLED SDE
This paper will derive lower bounds for the problem (1)
by solving an optimal control problem for the moments of
xt. This section will derive differential equations for these
moments.

If q(x) is a twice-differentiable, real-valued function, then

the Itˆo formula implies that [14]

dq(xt) =

∂q(xt)

∂x

(f (xt, ut)dt + g(xt, ut)dwt) +

1
2

∂2q(x)
∂x2 g(xt, ut)2dt.

(9)

Taking expected values of both sides results in a deter-

ministic differential equation:

d
dt

(cid:104)q(xt)(cid:105) =

(cid:28) ∂q(xt)

∂x

f (xt, ut) +

(cid:29)

.

(10)

g(xt, ut)2

The moments of xt and ut will be denoted by:

1
2

(cid:68)

∂2q(xt)

∂x2

(cid:69)

µxiuj
t

=

tuj
xi

t

.

(11)

When q(x) is a monomial, q(x) = xk, and f and g are
polynomials, (10) becomes a linear differential equation with
respect to the moments.

Example 4: Recall the dynamics from (5b). Then (10) has

the form

d
dt

(cid:104)q(xt)(cid:105) =

(cid:28) ∂q(xt)

∂x

(cid:0)(1.5)2xt − x3

t + ut

(cid:1) +

1
2

∂2q(xt)

∂x2

(cid:29)

.

(12)

For q(x) = x, we have that ∂q(x)

∂x = 1 and ∂2q(x)

∂x2 = 0.

Thus, the following holds:

(cid:104)xt(cid:105) =

d
dt

d
dt

=(cid:10)(1.5)2xt − x3

µx
t

= (1.5)2µx

t + ut
t + µu
t .

t − µx3

(cid:11)

(cid:17)

A similar argument shows that

(cid:16)
(cid:16)

dµx2
t
dt
dµxk
t
dt

= 2

= k

(1.5)2µx2

t − µx4
t − µxk+2
(1.5)2µxk
µxk−2

t

t

k(k − 1)

+

2

for k ≥ 3.

t + µxu
t

+ 1

+ µxk−1u

t

(cid:17)

Another constraint arises from the fact that outer products
are positive semideﬁnite, and this semideﬁnite constraint is
perserved by taking expectations. For example:

(cid:3)(cid:43)

t ut

=

(cid:42) 1

xt
x2
t
ut

(cid:2)1 xt x2
 1

µx
t
µx2
t
µu
t

µx
t
µx2
t
µx3
t
µxu

t

µx2
t
µx3
t
µx4
t
µx2u
t

t

µu
t
µxu
µx2u
t
µu2
t

 (cid:23) 0.

(16)

Furthermore, if the system has inequality constraints of
the from (1c), then for any r ≥ 1, it must be the case that
(bl(xt, ut))r ≥ 0, and thus the following must hold

(cid:104)(bl(xt, ut))r(cid:105) ≥ 0.

(17)

Recall the notation for the polynomials from (3). The fol-
lowing theorem gives a lower bound of the original problem,
(1), based on continuous-time semideﬁnite programming.
Theorem 1: For any integers K ≥ 1, r1 ≥ 1, . . . rB ≥ 1,
dx ≥ 1, and du ≥ 1, the optimal value of (1) is always at
least as large as the optimal value of the following optimal
control problem:

nx(cid:88)

i=0

(cid:90) T

nx(cid:88)

0

i=0

nu(cid:88)
nx(cid:88)

j=0

= k

dµk
t
dt
k(k − 1)

i=0

+

2

nu(cid:88)
nx(cid:88)

j=0

cijµxiuj

t

dt +

hiµxi

T

(18a)

(18b)

fijµxi+k−1uj

t

nu(cid:88)

gijgrsµxi+r+k−2uj+s

t

j,s=0

i,r=0
for k = 1, . . . , K
µk
0 = xk
0
bl,i1j1 ··· bl,irjr µxi1+···+ir uj1+···+jr

for k = 1, . . . , K

t



1
µx
t
...
µxdx
t
µu
t
...
µudu
t

··· µxdx
··· µxdx+1

for l = 1, . . . , B and r = 1, . . . , rl
··· µudu
··· µxudu
...

µu
t
µxu
...

t

t

t

t

t

µx
t
µx2
t
...

...
··· µx2dx
··· µxdx u

t

t

...

··· µxdx udu

t

t

µxdx+1
t
µxu
...
µxudu
t

µxdx u
t
µu2
t
...

µudu+1
t

··· µxdx udu
··· µudu+1

t

t

...
··· µu2du

t

(18c)

≥ 0

(18d)

(cid:23) 0.



(18e)
In (18d), the sums over i1, . . . , irl range from 0 to nx, while
the sums over j1, . . . , jrl range from 0 to nu.

Proof: The cost function, (18a), is exactly, the original
cost, (1a), expressed in terms of the moments. Given any
policy for ut, the moments of xt must satisfy (18b) with
initial conditions given by (18c). If the system has inequality

(13)

(14)

(15)

minimize

subject to

(cid:88)

(cid:88)

i1,...,ir

j1,...,jr

. In this case, no moment, µxk
t

In this example, we see that the differential equation for
t depends on the third moment, µx3
t . More generally, the
µx
differential equation for µxk
t depends on the higher moment,
µxk+2
, can be described using
t
a ﬁnite set of differential equations. In this case, it is said that
the moments are not closed. Several moment closure methods
have been developed to approximate moment dynamics using
a ﬁnite number of differential equations both for discrete,
and continuous state Markov models [15]–[20]. Future work
will involve combining the work in this paper with moment
closure methods.

IV. RESULTS

A. Lower Bounds by Semideﬁnite Programming

The moment differential equations described in the previ-
ous section must be satisﬁed for any choice of input strategy
for ut. Thus, they form a natural candidate for constraints
in an optimization problem.

constraints, as in (1c), then (17) must hold. The constraint
from (18d) is exactly (17) written in terms of the moments.
The ﬁnal constraint on the the matrix of moments, from
(18e), expresses the following semideﬁniteness constraint on
the expected value of an outer product:

(cid:42)







1
xt
...
xdx
t
ut
...
udu
t

1
xt
...
xdx
t
ut
...
udu
t



T

(cid:43)

(cid:23) 0.

Thus, for every policy, the moments of xt and ut must satisfy
(18e). Thus, the constraints (18b), (18c),(18d) and (18e) are
satisﬁed by every feasible solution for (1). Since their cost
functions coincide, (18) is a relaxation of (1), and so its
optimal solution is a lower bound to the optimal solution of
(1).

Recall that in some systems, as seen in Example 6, the
moments are not closed. Thus, no ﬁnite number of moment
equations will be sufﬁcient to describe the dynamics exactly.
In this case, (18) can be used to construct a sequence
of increasing lower bounds by increasing the number of
moment equations, K, and increasing the size of the moment
matrix from (18e) correspondingly. The value of the lower
bound increases because the feasible set becomes more
constrained.

Remark 1: There is a large amount of ﬂexibility in choos-
ing the size of the semideﬁnite program in (18). One sensible
procedure for choosing the sizes is as follows. Fix dx and
du. This will constrain the moment matrix from (18e) to
be of size (1 + dx + du) × (1 + dx + du). Then choose the
number of moment differential equations, K, and the number
of inequality constraints, r1, . . . , rB to be the largest values
such that every moment in the corresponding constraints is
contained in the moment matrix. This procedure is used in
the examples in this paper.

As with deterministic continuous-time optimal control
problems [21], the cost integral, (18a), can be discretized as
a Riemann sum, and the dynamic constraints, (18b), can be
discretized using Euler integration. This results in a ﬁnite-
dimensional semideﬁnite program. Reasonably sized prob-
lems, can be handled with off-the-shelf tools for numerical
optimization [22], [23]. Future work will developed special-
ized methods for solving this problem that take advantage of
the specialized structure as an optimal control problem.

B. Constructing the Controller

This subsection will describe a procedure for computing
a control strategy of the form in (7) from a collection of
moments of xt and ut.

Assume that the ut is generated according to (7). Then

for every k ≥ 0, the following holds:

t =(cid:10)xk
=(cid:10)xk

t

µxku

t ut

(cid:11)
(cid:0)p0(t) + p1(t)xt + ··· + pnp (t)xnp

(cid:1)(cid:11)
+ ··· + pnp (t)µxk+np

t + p1(t)µxk+1

t

t

t

= p0(t)µxk

.
(19)

Given a collection of moments, as computed from (18),
coefﬁcients that approximately satisfy (19) may be computed
using least-squares optimization, with the following objective
function:

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)


µx
t
µx2
t
...

1
µx
t
...
µxm
t µxm+1

t

··· µxnp
··· µxnp+1

t

t

··· µxm+np

t

...





 −



(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)


2

.

t

µu
t
µxu
...
µxmu
t

p0(t)
p1(t)
...

pnp (t)

(20)
Of course, to pose this optimization problem, the number of
moment differential equations, and the size of the moment
matrix from (18) must be large enough so that all of the
moments required for (20) are computed.

In practice, the coefﬁcients, pi(t) are computed at the
discrete time points at which the moments of xt and ut
are computed.

The controller computed from (20) is a Markov policy,
and thus feasible. As discussed in Subsection II-B, the value
of the average cost produced by this controller will always
give an upper bound on the true optimal value.

V. EXAMPLES

Example 5: Recall the linear quadratic regulator problem
from Example 1. This problem can be cast in the form of
(18) as:

minimize

subject to

(cid:90) 1

0

(µx2

t + µu2

t )dt

t + 1

= µu
t

= 2µxu

dµx
t
dt
dµx2
t
dt
0 = µx2
µx
0 = 0
µx
t
µx2
t
µxu

 1

µx
t
µu
t

t

µu
t
µxu
t
µu2
t

 (cid:23) 0.

(21a)

(21b)

(21c)

(21d)

(21e)

In this example, the moment equations are closed, since
moments higher than 2 are not required to compute the
solution. The classical theory of optimal control shows that
the optimal solution to the original regulator problem is of
the form

ut = Ltxt,

where Lt is a gain computed from a Riccati differential
equation. Using the Pontryagin maximum principle, it can

be shown that the optimal solution to (21) is given by:

 1

µx
t
µu
t

 =

1

µx
t
µx2
t
µxu

t

0

0
µx2
0
t
0 Ltµx2
t

µu
t
µxu
t
µu2
t
˙µx2
t = 2Ltµx2

Ltµx2
t
t µx2
L2
t + 1, µx2
0 = 0.

t



Example 6: Recall the system from Examples 2 and 4.

The cost function, (5a) can be written in moment form as

µx2
t + µu2

t

dt + µx2
1 .

(cid:17)

(cid:17)

(cid:17)

(cid:90) 1

(cid:16)

0

(cid:90) 1

(cid:16)

t

The problem can be approximated using (18) using this
cost, as well as the moment dynamics from (13), (14), and
(15). Recall that these moment equations are not closed.
Thus, the optimal value can be approximated by solving
(18) for increasingly large numbers of moment equations
and increasingly large moment matrices.

For comparison purposes, Fig. 1 plots the cost-to-go

function,

µx2
τ + µu2

τ

dτ + µx2
1 ,

(22)

for various sizes of the optimization problem in (18). For
sufﬁciently large semideﬁnite programs, an optimal value of
0.60 is obtained.

Using the least-squares method from Subsection IV-B, an
order-3 controller was constructed from the solution to the
SDP with K = 4 moment equations, and moment matrix
dx = 3 and du = 1. (This corresponds to a moment matrix
of size (1 + dx + du) × (1 + dx + du) = 5 × 5.) Running
2000 trials with this controller resulted in an average cost
of 0.67. Thus, the true optimal value is likely to lie between
0.60 and 0.67. In contrast, with no control, the average value
obtained was 2.4.

Example 7: Recall Example 3 on optimal harvesting for

ﬁsheries. This problem has moment dynamics given by

t

(cid:16)
t − µu
t − γµx2
(cid:16)
t − γµx3
µx2
t − γµxk+1
µxk

t

˙µx
t = µx
˙µx2
t = 2

˙µxk
t = k

for k ≥ 3.

t − µxu

t

(cid:17)

+ σ2µx2
t
1
2

+

− µxk−1u

t

k(k − 1)µxk

t

As with the previous example, the moments are not closed.
Furthermore, this problem has inequality constraints, which
imply moment inequalities, (18d), of the form

t ≥ 0,
µxr

t ≥ 0,
µur

for r ≥ 1.

A plot of the results of the SDP, (18), and a controller
computed from the least squares procedure, (20), is shown
in Fig. 2. From these ﬁgures, we can see that a strategy
that emerges as the SDP size increases. Through most of the
horizon, harvesting is low and the population is kept near
a constant level. Then, near the end of the horizon, a large
harvest drives the population to 0.

Fig. 1: Values of the cost-to-go function, (22) are plotted.
In all cases, du = 1, which implies the ﬁrst two moments of
ut are included in the moment matrix. For the smallest SDP,
with K = 2 and dx = 2, the moments are too unconstrained,
and an optimal value of near 0 is obtained. However, with
K = 4 moment equations and dx = 3, an optimal value of
0.53 is obtained. Increasing the size to K = 10 and dx =
9 barely changes this value, or the cost-to-go function. An
order-3 controller was constructed from the K = 4 and dx =
3 SDP solution. The average cost-to-go of 2000 runs with this
controller is plotted. The cost-to-go function is quite close
to cost-to-go from the SDP, aside from a deviation near the
end of the trajectory. For comparison, the average cost-to-go
of 2000 runs of the uncontrolled system is also plotted. The
average controlled cost is 0.66, compared with an average
uncontrolled cost of 2.4.

VI. CONCLUSION

This paper studied a method of solving stochastic optimal
control using moment equations. The approach consists of
formulating a semideﬁnite program, with constraints and
cost function represented in terms of the moments. Several
extensions of this work are possible. For example, it would
be interesting to investigate whether by using an appropri-
ate moment-closure for nonlinear systems, we can get a
lower bound to the optimal value at low orders of moment
truncation. Furthermore, several other works have used mo-
ment approximations in conjugation with other techniques
for stochastic optimal control [24]–[28]. We would like to
compare the performance of different controllers, including
the computational cost of each.

ACKNOWLEDGMENTS

A.S. is supported by the National Science Foundation

Grant DMS–1312926.

REFERENCES

[1] W. H. Fleming and H. M. Soner, Controlled Markov Processes and

Viscosity Solutions. Springer, second ed., 2006.

0.00.20.40.60.81.0Time0.00.51.01.52.02.5Expected Cost-to-GoSDP, K=2, dx=2SDP, K=4, dx=3SDP, K=10, dx=9Controlled, K=4, dx=np=3Uncontrolled[2] J. Yong and X. Y. Zhou, Stochastic controls: Hamiltonian systems
and HJB equations, vol. 43 of Stochastic Modelling and Applied
Probability. Springer-Verlag New York, 1999.

[3] G. Young and R. Chang, “Optimal control of stochastic parametrically
and externally excited nonlinear control systems,” Journal of dynamic
systems, measurement, and control, vol. 110, no. 2, pp. 114–119, 1988.
[4] G. Jumarie, “Improvement of stochastic neighbouring-optimal control
using nonlinear gaussian white noise terms in the taylor expansions,”
Journal of the Franklin Institute, vol. 333, no. 5, pp. 773–787, 1996.
[5] L. Socha, “Linearization in analysis of nonlinear stochastic systems:
Recent results–part i: Theory,” Applied Mechanics Reviews, vol. 58,
no. 3, pp. 178–205, 2005.

[6] G. Jumarie, “A practical variational approach to stochastic optimal
control via state moment equations,” Journal of the Franklin Institute,
vol. 332, no. 6, pp. 761–772, 1995.

[7] G. Jumarie, “Improvement of stochastic neighbouring-optimal control
using nonlinear gaussian white noise terms in the taylor expansions,”
Journal of the Franklin Institute, vol. 333, no. 5, pp. 773–787, 1996.
[8] R. Cominetti, F. Facchinei, and J. B. Lasserre, Modern optimization

modelling techniques. Springer Science & Business Media, 2012.

[9] S. Boyd and L. Vandenberghe, Convex optimization. Cambridge

university press, 2004.

[10] A. E. Bryson and Y.-C. Ho, Applied Optimal Control: Optimization,
Estimation, and Control. Hemisphere Publishing Corporation, revised
printing ed., 1975.

[11] F. L. Lewis, L. Xie, and D. Popa, Optimal and Robust Estimation: With
an Introduction to Stochastic Control Theory. CRC Press, second ed.,
2008.

[12] L. H. Alvarez and L. A. Shepp, “Optimal harvesting of stochastically
ﬂuctuating populations,” Journal of Mathematical Biology, vol. 37,
no. 2, pp. 155–177, 1998.

[13] E. Lungu and B. Øksendal, “Optimal harvesting from a population in a
stochastic crowded environment,” Mathematical Biosciences, vol. 145,
no. 1, pp. 47 – 75, 1997.

[14] B. Øksendal, Stochastic differential equations. Springer, 2003.
[15] L. Socha, Linearization Methods for Stochastic Dynamic Systems.
Lecture Notes in Physics 730, Springer-Verlag, Berlin Heidelberg,
2008.

[16] C. Kuehn, Moment Closure–A Brief Review. Understanding Complex

Systems, Springer, 2016.

[17] I. N˚asell, “An extension of the moment closure method,” Theoretical

population biology, vol. 64, no. 2, pp. 233–239, 2003.

[18] A. Singh and J. P. Hespanha, “Lognormal moment closures for
biochemical reactions,” in Proceedings of the 45th Conference on
Decision and Control, pp. 2063–2068, 2006.

[19] A. Singh and J. P. Hespanha, “Approximate moment dynamics for
chemically reacting systems,” IEEE Transactions on Automatic Con-
trol, vol. 56, no. 2, pp. 414–418, 2011.

[20] M. Soltani, C. A. Vargas-Garcia, and A. Singh, “Conditional moment
closure schemes for studying stochastic dynamics of genetic circuits,”
Biomedical Circuits and Systems, IEEE Transactions on, vol. 9, no. 4,
pp. 518–526, 2015.

[21] A. V. Rao, “A survey of numerical methods for optimal control,”
Advances in the Astronautical Sciences, vol. 135, no. 1, pp. 497–528,
2009.

[22] S. Diamond and S. Boyd, “CVXPY: A Python-embedded modeling
language for convex optimization.” To appear in Journal of Machine
Learning Research, 2016.

[23] B. ODonoghue, E. Chu, N. Parikh, and S. Boyd, “Conic optimization
via operator splitting and homogeneous self-dual embedding.” To
appear in Journal of Optimization Theory and Applications, 2016.

[24] L. Crespo and J. Sun, “Stochastic optimal control of nonlinear systems
via short-time gaussian approximation and cell mapping,” Nonlinear
Dynamics, vol. 28, no. 3-4, pp. 323–342, 2002.

[25] L. G. Crespo and J.-Q. Sun, “Stochastic optimal control via bellman’s

principle,” Automatica, vol. 39, no. 12, pp. 2109–2114, 2003.

[26] Y. Xu and P. Vedula, “A moment-based approach for nonlinear
stochastic tracking control,” Nonlinear Dynamics, vol. 67, no. 1,
pp. 119–128, 2012.

[27] Y. Xu and P. Vedula, “Nonlinear stochastic control part i: A moment-
based approach,” in AIAA Guidance, Navigation, and Control Confer-
ence, p. 5627, 2009.

[28] S. Wojtkiewicz and L. Bergman, “A moment speciﬁcation algorithm
for control of nonlinear systems driven by gaussian white noise,”
Nonlinear Dynamics, vol. 24, no. 1, pp. 17–30, 2001.

(a)

(b)

Fig. 2: 2a The thick solid lines show the mean population,
(cid:104)xt(cid:105) computed for various sizes of the SDP. In each case
du = 1 and dx varies. The sizes of the SDP are chosen
based on the procedure from Remark 1. Using the moments
from the dx = 10 SDP, a controller was computed from the
procedure from Subsection IV-B. The thin red lines show the
population 20 runs of this controller. 2b The thick solid lines
show mean harvesting rate, (cid:104)ut(cid:105). Similar to 2a, the thin red
lines show the harvesting rates in 20 runs of the computed
controller. The optimal expected harvest for the various SDPs
is computed as 2.31, 2.17, and 2.11, for dx = 2, 5, and
10, respectively. Using 1000 runs of the controlled process
results in an average harvest of 1.62. Thus, the true optimal
harvest is likely to lie somewhere between 1.62 and 2.11.

0.00.20.40.60.81.0Time0.50.00.51.01.52.0PopulationK=3, dx=2K=6, dx=5K=11, dx=100.00.20.40.60.81.0Time505101520253035Harvesting StrategyK=3, dx=2K=6, dx=5K=11, dx=10