K O N S TA N T I N M . Z U E V

S TAT I S T I C A L I N F E R E N C E

6
1
0
2

 
r
a

 

M
6
1

 
 
]
P
A

.
t
a
t
s
[
 
 

1
v
9
2
9
4
0

.

3
0
6
1
:
v
i
X
r
a

A C M L E C T U R E N O T E S

Contents

Preface

9

Summarizing Data

12

Simple Random Sampling

21

Population Variance and the Bootstrap Method

1

2

3

4

5 Normal Approximation and Conﬁdence Intervals

6 Modeling and Inference: A Big Picture

35

26

30

7

8

9

Estimating the CDF and Statistical Functionals

42

The Jackknife Method

The Bootstrap Method

47

52

10 The Method of Moments

60

11 Maximum Likelihood: Intuition, Deﬁnitions, Examples

64

statistical inference

3

12 Properties of Maximum Likelihood Estimates

71

13 Hypothesis Testing: General Framework

79

14 The Wald test and t-test

86

15 P-values

90

16 The Permutation Test

95

17 The Likelihood Ratio Test

18 Testing Mendel’s Theory

19 Multiple Testing

109

98

104

20 Regression Function and General Regression Model

21 Scatter Plots and Simple Linear Regression Model

115

119

22 Ordinary Least Squares

123

23 Properties of the OLS Estimates

128

24 Hypothesis Testing & Interval Estimation

25 Prediction & Graphic Residual Analysis

134

140

List of Figures

1.1 Illustration by Larry Gonick, The Cartoon Guide to Statistics.
1.2 Illustration by Larry Gonick, The Cartoon Guide to Statistics.

9
11

12

2.1 Googled histograms.
2.2 Histograms of the normally distributed data x1, . . . , xn, n = 1000.
2.3 Normal bell-shaped histogram.
2.4 Boxplot. We can clearly see that the sample is skewed to the right.
2.5 Empirical CDF for sample of size n = 100 drawn from the uniform

13

13

16

17

17

distribution on [0, 1].

2.6 The standard normal quantile zq in term of the CDF Φ(z) (top) and

2.7 Normal-quantile plots for the data x1, . . . , xn sampled from (a) the

PDF φ(z) (bottom).
standard normal distribution, (b) uniform distribution on [−1/2, 1/2]
(short tails), (c) the Laplace distribution f (x) ∝ e−|x| (long tails), and
(d) a bimodal distribution (a mixture of two well-separated Gaus-
sians). Sample size in all examples is n = 100.

18

2.8 John Turky. Photo source: wikipedia.org.
2.9 The 68-95-99.7 rule for the standard normal curve.

19

19

3.1 By a small sample we may judge of the whole piece, Miguel de Cervantes

“Don Quixote.” Photo source: wikipedia.org.

21
3.2 Captain John Graunt. Photo source: goodreads.com
3.3 Pierre-Simon Laplace. Photo source: wikipedia.org.

21
21

4.1 Bradley Efron, the father of the bootstrap. Photo source: statweb.stanford.edu.
4.2 Boxplots of bootstrap estimates. Each boxplot is constructed based

on 100 bootstrap estimates. That is, we repeated (4.9) with P = Pboot,
(4.10), and (4.11) 100 times for each value of B.

29

28

5.1 Normal-quantile plot for the standardized sample means X(1)

n −µ
n −µ
se[Xn] , . . . , X(m)
se[Xn] .

The sampling distribution closely follows the standard normal curve.
31

5.2 90% conﬁdence intervals for the population mean µ = 3.39. Inter-

vals that don’t contain µ are shown in red.

33

6.1 Charles Darwin studying corn. Photo source: wikipedia.org.
6.2 The heights of corn plants for two different types of fertilization.
6.3 Sir Francis Galton, Darwin’s cousin. Among many other things, he
developed the concept of correlation. Photo source: wikipedia.org.
6.4 Wassily Hoeffding, one of the founders of nonparametric statistics.

37

Photo source: nap.edu

40

statistical inference

5

37

38

8.1 A Victorinox Swiss Army knife. Photo source: wikipedia.org.

47

53

9.1 Bootstrap at work. Image source: [jpg].
9.2 Enrollments (in thousands) of N = 354 large degree-granting in-
stitutions. Data: U.S. Department of Education. Available at enroll-
ment.xlsx. An “outlier” — University of Phoenix (14.8, 307.9) — is
not shown in Fig. 9.2.

9.3 Random samples of size n = 10 (left) and n = 100 (right). Out goal

56

is to estimate θ, compute the standard error of the estimate, and con-
struct conﬁdence intervals based on these samples.

57

9.4 The normal (blue) and pivotal (red) conﬁdence intervals at level 0.95.
The true value of θ (9.18) is shown by the dashed line. The estimates
58
ˆθn are marked by green circles (n = 100) and squares (n = 10).

9.5 Top panel illustrates the variability in the values of the plug-in es-
timates ˆθn. As expected, on average, the estimate for n = 100 (red
curve) is more accurate. The bottom panel shows the variability of

the bootstrap estimates (cid:98)seB[ ˆθn] for n = 10 (left) and n = 100 (right).

Green dashed lines represent the true values of seF[ ˆθ] computed us-
ing F (i.e. using full data).

9.6 Here we show approximate 0.95 conﬁdence intervals for θ. Four in-

58

tervals in Fig. 9.4 are ones of these. For n = 10, only 71 out of 100
normal intervals (top left) and 69 pivotal intervals (bottom left) con-
tain the true value θ = 1.41. This means that the approximation is
poor, since we expect about 95 out of 100 intervals to contain θ. For
n = 100, the intervals are more accurate: 83 normal (top right) and
86 pivotal (bottom right) contain θ.

58

11.1 Sir Ronald Fisher. Photo source: wikipedia.org.
11.2 A chocolate chip cookie (left) was invented in 1938 in Massachusetts

64

and a fortune cookie (right). The exact origin of fortune cookies is
unclear, though various immigrant groups in California claim to have
popularized them in the early 20th century. Photo source: wikipedia.org
and hufﬁngtonpost.com.
11.3 Three normalized (so that maxLn(p) = 1) likelihood functions for
the data X1, . . . , Xn ∼ Bernoulli(1/3) with n = 10, n = 100, and
n = 1000. Notice how the likelihood function becomes more and
more concentrated around the true value p = 1/3 as the sample
size n increases.

66

64

6 k. m. zuev

11.4 Top row: three likelihood functions for the data X1, . . . , Xn ∼ N (0, 1)

with n = 10, n = 100, and n = 1000. Bottom row: the correspond-
ing contour plots. Red color corresponds to higher values of the like-
lihood function. Notice how the likelihood function becomes more
and more concentrated around the true values θ1 = µ = 0 and θ2 =
σ2 = 1 as the sample size n increases.

67

11.5 The likelihood function (11.15) for the uniform model.

69

13.1 Alice and Bob are two archetypal characters commonly used in cryp-

tography, game theory, physics, and now.... in statistics. Comics source:
physicsworld.com.
13.2 Probabilities (13.2).
13.3 Unfortunately, the presumption of innocence does on always work

79
80

in real life.

81

82

13.4 The ideal power function.
13.5 The power function of a reasonably good test of size α.
13.6 The normal power function (13.18) for n = 10, σ = 1, and differ-
ent values of c. Notice that as c increases (rejection region shrinks),
the size of the test decreases, as expected.

83

84

13.7 The normal power function (13.18) with n and c deﬁned from (13.21)

and (13.19) with α = δ =  = 0.1. Notice the sample size increase!

85

14.1 Abraham Wald, a Hungarian statistician. Photo source: wikipedia.org.
14.2 The power function of the Wald test with size α.
14.3 William Sealy Gosset (aka “Student”). Photo source: wikipedia.org.
14.4 The PDF of t-distribution with k = 1, 2, and 3 degrees of freedom.

87

89

86

88

15.1 Binomial probabilities

Pi(k) =

(cid:18)n

(cid:19)

k

i (1 − pi)n−k,
pk

90

where n = 10, p0 = 0.5, and p1 = 0.7.
15.2 The concept of p-value. Here X is the observed data. If the size α is
too small, X /∈ Rα, and we accept H0. Gradually increasing α, we
reach the moment where we reject H0. The corresponding value of
α is the p-value. Reporting simply a particular size and the correspond-
ing decision (reject/accept), gives only a point on the bottom graph.
Reporting the p-value, determines the graph completely.
pected, α = 1 corresponds to R = Ω (kmin = 0) and α = 0 corre-
sponds to R = ∅ (kmin = 11).

15.3 The size α as a function of the rejection region boundary kmin. As ex-

92

91

16.1 Hot Chicken Wings. Photo source: losangeles.com.
16.2 Boxplots of hot wings consumption by Females and Males at the Williams

96

bar, MN.

97

statistical inference

7

16.3 Histogram of the obtained values of the test static (16.10) for K =

16.4 Histogram of the obtained values of the test static (16.11) for K =

105 data permutations.

105 data permutations.

97

97

17.1 Jerzy Neyman, Polish-American mathematician, and Egon Pearson,

leading British statistician.

99

17.2 The PDF of the χ2-distribution with q degrees of freedom.

101

18.1 Gregor Mendel, a scientist and a monk, the father of modern genet-

ics. Photo source: britannica.com

104

18.2 Mendel’s Principle of Independent Assortment. Picture source: wi-

ley.com.

105

19.1 Uncorrected testing vs. Bonferroni vs Benjamini-Hochberg.

113

21.1 Scatterplots of (a) the original Pearson’s data and (b) jittered data with

added small random noise.

119

21.2 Karl Pearson, English mathematician, one of the fathers of mathe-

matical statistics and the father of Egon Pearson. Photo source: wikipedia.org.

119

21.3 Panel (a): Examining three vertical slices of the data suggest that the
response Y indeed depends on the predictor X. Panel (b): Nonpara-
metric smoother suggests that the regression function can be reason-
ably well modeled by a liner function.

120

21.4 “Regression towards the mean.”

122

22.1 A cloud of green points (data) and a ﬁtted blue line ˆr(x) = ˆβ0 +

ˆβ1x. A couple of predicted values ((cid:98)Yi and (cid:98)Yj) are shown in blue. The

residuals are depicted by red dashed lines.

124

22.2 OLS in action: the regression line for the heights data (Lecture 21),

heights.xlsx.

124
22.3 Anscombe’s quartet.

125

23.1 The constrain minimization (23.18) is equivalent to ﬁnding the clos-
est to the origin point (red dot) in the search space of codimension
2, which is the intersection of two hyperplanes deﬁned by the con-
straints.

132

24.1 Two cases where the null hypothesis H0 : β1 = 0 is accepted.
24.2 Two cases where the null hypothesis H0 : β1 = 0 is rejected.
137
24.3 The OLS regression line for the heights data, heights.xlsx.
138
24.4 The PDF of the χ2-distribution with k degrees of freedom.
24.5 The OLS regression line (red) and random regression lines (green)

136
137

with slopes and intercepts chosen uniformly from the 95% conﬁdence
intervals (24.24).

138

8 k. m. zuev

24.6 The OLS regression line (red) and random regression lines (green)

with slopes and intercepts chosen uniformly from the 95% conﬁdence
intervals constructed for the data {(X(cid:48)

139

i, Yi)}.

25.1 Conﬁdence intervals for r(X∗) and prediction intervals for Y∗, for

X∗ = 56, 62.5, and 69.

142

25.2 Panel (a): both zero mean and constant variance assumptions appear

to be true; Panel (b): homoscedasticity is violated; Panel (c): linear
model for the mean response is doubtful.

143

25.3 Using the SLR model for extrapolation can lead to misleading pre-

dictions.

144

25.4 The simple linear regression would ﬁt almost perfectly here. Picture

source: wikipedia.org.

144

1
Preface

What is Statistics?

Opinions vary. In fact, there is a continuous spectrum of attitudes
toward statistics ranging from pure theoreticians, proving asymptotic
efﬁciency and searching for most powerful tests, to wild practition-
ers, blindly reporting p-values1 and claiming statistical signiﬁcance
for scientiﬁcally insigniﬁcant results. Even among most prominent
statisticians there is no consensus: some discuss the relative impor-
tance of the core goals of statistical inference2, others comment of the
differences between “mathematical” and “algorithmic” cultures of
statistical modeling3, yet others argue that mathematicians should
not even teach statistics4. The absence of a uniﬁed view on the sub-
ject led to different approaches and philosophies: there is frequentist
and Bayesian statistics, parametric and nonparametric, mathemati-
cal, computational, applied, etc. To complicate the matter, machine
learning, a modern subﬁeld of computer science, is bringing more
and more new tools and ideas for data analysis.

Here we view statistics as a branch of mathematical engineering,5
that studies ways of extracting reliable information from limited
data for learning, prediction, and decision making in the presence
of uncertainty. Statistics is not mathematics per se because it is inti-
mately related to real data. Mathematics is abstract, elegant, and can
often be useful in applications; statistics is concrete, messy, and al-
ways useful.6 As a corollary, although present, the proofs are not of
paramount importance in these notes. Their main role is to provide
intuition and rationale behind the corresponding methods. On the
other hand, statistics is not simply a toolbox that contains answers
for all data related questions. Almost always, as in solving engineer-
ing problems, statistical analysis of new data requires adjustment of
existing tools or even developing completely new methods7.

Figure 1.1: Illustration by Larry Gonick,
The Cartoon Guide to Statistics.

1 What is a p-value?

2 G. Shmueli (2010) “To explain or
to predict?” Statistical Science, 25(3):
289-310.
3 L. Breiman (2001) “Statistical model-
ing: the two cultures,” Statistical Science,
16(3): 199-231.
4 D.S. Moore (1988) “Should mathe-
maticians teach statistics?” The College
Mathematics Journal, 19(1): 3-7.

5 To the best of our knowledge, this
formulation is due to Cosma Shalizi.

6 The difference between statistics and
mathematics is akin to the difference
between a real man and the Vitruvian.

7 For example, recent years witnessed
an explosion of network data for which
most of the classical statistical methods
and models are simply inappropriate.

10 k. m. zuev

What are these Notes?

These ACM8 Lecture Notes are based on the statistical courses I
taught at the University of Southern California in 2012 and 2013, and
at the California Institute of Technology in 2016.

8 Applied & Computational Mathematics

What are the Goals?

The main goals of these notes are:

1. Provide a logical introduction to statistical inference,

2. Develop statistical thinking and intuitive feel for the subject,

3. Introduce the most fundamental ideas, concepts, and methods of
statistics, explain how and why they work, and when they don’t.

After working on these notes you should be able to read9 most10
contemporary papers that use statistical inference and perform basic
statistical analysis yourself.

What are the Prerequisites?

This is an introductory text on statistical inference. As such, no prior
knowledge of statistics is assumed. However, to achieve the afore-
mentioned goals, you will need a ﬁrm understanding of probabil-
ity11, which is — in the context of statistics — a language for de-
scribing variability in the data and uncertainty associated with the
phenomenon of interest.

Why Prerequisites are Important?

Because without knowing probability, the best you could hope for is
to memorize several existing concepts and methods without under-
standing why they work. This would increase the risk of an unfortu-
nate event of turning into a “wild practitioner” mentioned above.

How to read these Lecture Notes?

I would suggest to read each lecture note twice. First time: glancing
through, ignoring footnotes, examining ﬁgures, and trying to get
the main idea and understand a big picture of what is going on.
Second time: with a pencil and paper, working through all details,
constructing examples, counterexamples, ﬁnding errors and typos12,
and blaming me for explaining easy things in a complicated way.

9 And understand.
10 Admittedly not all.

11 Here is the list of concepts you should
know: random variable, cumulative
distribution function, probability mass
function, probability density function;
speciﬁc distributions, such as uniform,
Bernoulli, binomial, normal, χ2, t; ex-
pectation; variance, standard deviation;
joint and conditional distributions;
conditional expectations and variances;
independence; Markov’s inequality,
Chebyshev’s inequality; law of large
numbers, central limit theorem.

12 Please look for them. There are
many, I promise. Please, inform me of
those you ﬁnd by sending an email to
kostia@caltech.edu.

statistical inference

11

What is Missing?

A lot by any standards. Bayesian inference, causal inference, decision
theory, simulation methods are not covered at all. I hope to expand
these notes in the feature. This is simply the ﬁrst draft.

Acknowledgment

I wish to express my sincere thanks to Professor Mathieu Desbrun
of Caltech for granting me a teaching-free fall term in 2015. This
allowed me to bite the bullet and write these notes.

References

These notes, which I tried to make as self-contained as possible, are
heavily based on the following texts:

G. Casella & R.L. Berger (2002) Statistical Inference.
L. Chihara & T. Hesterberg (2011) Mathematical Statistics with Resampling and R.
A.C. Davison (2003) Statistical Models.
D. Freedman, R. Pisani, & R. Purves (2007) Statistics.
M. Lavine (2013) Introduction to Statistical Thought.
S.L. Lohr (2009) Sampling: Design and Analysis.

[CB]
[CH]
[D]
[FPP]
[La]
[Lo]
[MPV] D.C. Montgomery, E.A. Peck, & G.G. Vining (2012) Introduction to Linear Regression Analysis.
[NS]
[Wa]
[We]

D. Nolan & T. Speed (2000) Stat Labs: Mathematical Statistics Through Applications.
L.A. Wasserman (2005) All of Statisitcs: A Concise Course in Statistical Inference.
S. Weisberg (2005) Applied Linear Regression.

Konstantin M. Zuev
Pasadena, California,
March 15, 2016

Figure 1.2: Illustration by Larry Gonick,
The Cartoon Guide to Statistics.

2
Summarizing Data

Data is1 at the heart of statistics. The most basic element of data
is a single observation, x, a number. Usually real data comes in the
form of a (very long) list of numbers. Even if the original data is
more complex — a text, curve, or image — we will assume that we
can always convert it to a set of n numerical observations x1, . . . , xn,
called a sample.

To get a better feel for the data in hand, it is often useful (espe-
cially if the sample size n is large) to summarize it numerically or
graphically. This can bring some insights about the data. In this lec-
ture, we discuss several kinds of summary statistics2.

The Histogram

If you Google images for “statistics,” you will see something like this:

1 For “data is” vs "data are" see gram-
marist.com

2 In statistics, any (possibly vector
valued) quantity s = s(x1, . . . , xn) that
can be calculated from data is called a
statistic.

Figure 2.1: Googled histograms.

These graphs, called histograms, are perhaps the best-known sta-

tistical plots. To construct a histogram from data x1, . . . , xn:
1. Divide the horizontal axis into disjoint bins, the intervals I1, . . . , IK.
2. Denote the number of observation in Ik by nk, so that ∑K
k=1 nk = n.
3. For each bin, plot a column over it so that the area of the column
n of the data in the bin3. The height hk of the

is the proportion nk
column over Ik is therefore hk = nk/n|Ik| .

3 This makes the total area of the
histogram equal to 1. Such histograms
are called normalized. Sometimes not
normalized histograms are used, where
the area of a column over Ik is simply
the number of observations nk. In this
case, the total area of the histogram is n.

Question: How to chose bins?
There is no unique recipe for choosing bins: a good choice de-
pends on the data. Let us try to understand what “good” means.
The main purpose of the histogram is to represent the shape of the
sample: symmetry (bell-shaped? uniform?), skewness (right-skewed?
left-skewed?), modality (unimodal? multimodal?). Let us assume for
simplicity that all bins have equal width4 w:

I1 = [x(1), x(1) + w),
I2 = [x(1) + w, x(1) + 2w),

. . .

IK = [x(n) − w, x(n)],

(2.1)

statistical inference

13

4 Sometimes it might be better to vary
the bin width, with narrower bins in the
center of the data, and wider ones at
the tails.

where x(1) and x(n) are respectively the minimum and maximum of
the sample, x(1) = min{x1, . . . , xn} and x(n) = max{x1, . . . , xn}. In
this case, the total number of bins is

K =

.

(2.2)

x(n) − x(1)

w

The number of bins K in a histogram can drastically affect its ap-
pearance. If K is too large (w is too small), then the histogram looks
too rough. On the other hand, if K is too small, then the histogram
is too smooth. This effect is illustrated below with the normally dis-
tributed sample.

Thus, either too few or too many bins can obscure structure in
the data. There are several simple heuristic rules for the approxi-
mate number if bins. For example, if the sample x1, . . . , xn appears
to be approximately normally distributed5, then we can use Sturges’
formula:

K ≈ 1 + log2 n.

(2.3)

In general, exploring the data using histograms with different
numbers of bins and different cut points between bins is useful in
understanding the shape of the data, and heuristics like (2.3) can be
used as a starting point of exploration. But this exploration should
not be confused with manipulation of the data for presentation pur-
poses!

Figure 2.2: Histograms of the normally
distributed data x1, . . . , xn, n = 1000.

5 That is we expect the histogram is
bell-shaped, i. e. looks similar to this:

Figure 2.3: Normal bell-shaped his-
togram.

-4-202400.20.40.60.811.2Too rough (K=1000)-4-202400.10.20.30.4Too smooth (K=3)-4-202400.10.20.30.4Nearly optimal (K=11:1+log2n)14 k. m. zuev

Numerical Summaries

Numerical summaries provide quantitative information about the
data. Two basic features of a sample are its location and spread.

Measures of location

A measure of location is a statistic that represents the center of the
sample6. One such measure is the sample mean, which is simply the
average of the sample:

¯x =

1
n

n∑

i=1

xi.

(2.4)

The main drawback of the mean is that it is sensitive to outliers. An
outlier is an observation x∗ that is distant from other observations
7. An outlier may be due to variability in the
in the sample x1, . . . , xn
data8 or it may indicate measurement error. For example, by chang-
ing only the value of x1 we can make the mean ¯x arbitrary small or
large, and, in this case, it will be a poor measure of the sample center.

An alternative measure of location, which is robust9 to outliers, is
the median. The median ˜x is the point that divides the sample in half.
To calculate the median, we need to order the data. The order statistics
of x1, . . . , xn are their values put in increasing order, which we denote

6 If x1, . . . , xn are different measure-
ments of the same quantity (say, mea-
surements of temperature obtained
from different thermometers), a mea-
sure of location is often used as an
estimate of the quantity in the hope
that it is more accurate than any single
measurement.
7 Outliers are often easy to spot in
histograms.
8 For example, Bill Gates will be an
outlier in the study of people’s wealth.

9 Insensitive.

The median is then deﬁned as follows10:

x(1) ≤ x(2) ≤ . . . ≤ x(n).
x( n+1
(cid:16)

2 ),
x( n

(cid:17)

2 ) + x( n

2 +1)

1
2

˜x =

if n is odd,
if n is even.

,

(2.5)

(2.6)

10 Convince yourself that ˜x deﬁned this
way indeed splits the data in half.

The main drawback of the median is the opposite of the drawback
of the mean: it is too insensitive to the change in the sample values.
Suppose for simplicity, that the sample size is odd, n = 2k − 1, then
the median is the kth order statistic, ˜x = x(k). Making the values of
the right half of the sample x(k+1), . . . , x(n) arbitrary large does not
affect the median. Similar effect holds for the left half od the sample.

Question: Can we ﬁnd a compromise between ¯x and ˜x?
A compromise between the mean and the median is a trimmed

mean. The mean is the average of all observations. We can think of
the median as the average of the middle one or two observations as
if the rest observations were discarded. The α-trimmed mean ¯xα is
deﬁned as follows: discard 100α% of the observations on each side of
the ordered sample x(1) ≤ x(2) ≤ . . . ≤ x(n) and take the mean of the
remaining middle 100(1 − 2α)% of the observations. Mathematically,

xα =

x([nα]+1) + . . . + x(n−[nα])

n − 2[nα]

,

(2.7)

statistical inference

15

where [s] denotes the greatest integer less than or equal to s. Then 0-
trimmed mean is the standard sample mean, and the median can be
thought of as the 0.5-trimmed mean. If the trimmed mean is a slowly
varying function of α, then the sample has a well deﬁned center.

Measures of spread

A measure of location is often accompanied by a measure of spread
that gives an idea as to how far an individual value xi may vary from
the center of the data (“scatteredness” of the sample). The simplest
measure of spread is the range, which is the difference between the
largest and smallest values,

r = x(n) − x(1).

(2.8)

The range ignores most of the data and is very sensitive to outliers.

One of the most popular measures of spread in statistics is the

sample standard deviation11:

(cid:115)

sx =

1
n

n∑

i=1

(xi − ¯x)2.

(2.9)

(cid:113) 1

11 Sometimes it is deﬁned as sx =
i=1(xi − ¯x)2 for reasons we
∑n

n−1

discuss later. But if n is large the
difference between the two versions is
negligible.

Although it is also sensitive to outliers but it is more robust than the
range.

The measure of spread that typically accompanies the median
is the interquartile range (IQR), which is the difference between the
upper and lower quartiles of the sample,

IQR = Q3 − Q1,

(2.10)

where Q1 is is ﬁrst (lower) quartile that splits lowest 25% of the sam-
ple and Q3 is the third (upper) quartile that splits highest 25% of the
sample12.

12 What is the second quartile Q2?

Five-Number Summary

The ﬁve-number summary provides simultaneously a measure of
location and spread. The ﬁve numbers are: the minimum x(1), the
ﬁrst quartile Q1, the median ˜x = Q2, the third quartile Q3, and the
maximum x(n).

Boxplots

A boxplot is a graph that visualizes the ﬁve-number summary, gives
a good idea of the shape of the data, and shows potential outliers. To
create a boxplot from data x1, . . . , xn:

16 k. m. zuev

1. Draw a box with the bottom end placed at the ﬁrst quartile Q1

and the top end placed at the third quartile Q3. Thus about a half
of the data lie in the box, and its hight is IQR.

2. Draw a horizontal line through the box at the median ˜x = Q2.
3. Place a cap at the largest observation that is less than or equal to

Q3 + 1.5IQR. Similarly, place a cap at the smallest observation that
is greater than or equal to Q1 − 1.5IQR.

4. Extend whiskers (dashed lines in Fig. 2.4) from the edges of the
box to the caps. Observations that lie between the caps are not
considered as outliers.

5. The observations that fall outside the caps are considered as out-
liers13. Plot them individually with ·, +, ∗, or your favorite sym-
bol.

A box plot is a semi-graphical and semi-numerical summary of data.
It contains more information than a ﬁve-number summary, but it is
less informative than a histogram: from a boxplot it is not possible to
ascertain whether there are gaps in the data or multiple modes.

Boxplots are especially useful when comparing related samples.
For examples, household incomes in different states, lengths of the
ﬂight delays of different airlines, heights of males and females, etc.

Empirical CDF

The basic problem of statistical inference is: given the data x1, . . . , xn,
what can we say about the process that generated the data? Proba-
bilistically, we model the sample x1, . . . , xn as realizations of a random
variable X with (unknown and to be inferred) cumulative distribution
function (CDF) FX, which is the theoretical model for the data.
The empirical CDF (eCDF) is the “data analogue” of the CDF of
a random variable. Recall that FX(x) = P(X ≤ x). The eCDF of
x1, . . . , xn is deﬁned as follows:

number of observations less than or equal to x

Fn(x) =

=

1
n

n∑

i=1

H(x − xi),

n

(2.11)

where H(x) is the Heaviside function14 that puts mass one at zero:

0,

1,

H(x) =

x < 0,
x ≥ 1.

(2.12)

The eCDF is thus a step function that jumps by 1

n at each of the xi

15.

Figure 2.4: Boxplot. We can clearly see
that the sample is skewed to the right.
13 Outliers are often deﬁned as points
which fall more than k > 0 times the
interquartile range above Q3 or below
Q1 with k = 1.5 as a usual choice.

14 This is one of those “standard”
functions that you are likely to meet in
any math/science/engineering course.

15 If the value of xi appears k times in
the sample, then the eCDF jumps by k
n
at this value.

11.522.533.544.551Sample valuesSmallestobservation, notconsidered asan outlierQ2Q1Q3IQRLargestobservation, notconsidered asan outlierOutliersFigure 2.5 shows how it looks for the sample drawn from the uni-
form distribution on [0, 1]. Notice how closely the eCDF resembles
the true uniform CDF, Fn(x) ≈ FX(x)

16.

The eCDF is a graphical display that conveniently summarizes

the sample. It is more detailed than the histogram, but perhaps a bit
more difﬁcult to read and conveys less information about the shape
of the data. The eCDF plays an important role in estimating statistical
functionals, and we will come back to it in the future.

Q-Q plots

Remarkably, many histograms follow the normal curve17. Visually,
this means that a histogram has a single peak (mode), its mean and
median are approximately equal, and its symmetric about the center
(see Fig. 2.3). Examples include histograms of heights of people,
errors in measurements, and marks on a test.

Suppose we wish to check if the data x1, . . . , xn come the normal
distribution, that is if the standard normal CDF Φ(z) is a good theo-
retical model for the data. We could start from plotting the histogram
and see if it is bell-shaped. But the problem with this approaches is
that usually histograms do not allow to see clearly what happens in
the tails of the data distribution, i.e. around x(1) and x(n): do the tails
decay faster (“short” tails) or slower (“long” tails) than the normal
tails?18 Therefore, we need a more accurate procedure. A quantile-
quantile (Q-Q) plot is a graphical method that allows to assess the
normality of the sample, and, more generally, to compare the sample
x1, . . . , xn with any theoretical model FX.

The qth quantile19 of the standard normal distribution is a number

zq such that

Φ(zq) = q, where 0 < q < 1.

(2.13)

statistical inference

17

16 This is not a coincidence, and we will
make this statement more precise in the
subsequent lectures.

Figure 2.5: Empirical CDF for sample of
size n = 100 drawn from the uniform
distribution on [0, 1].
17 A quick reminder on the normal
distribution is given in Appendix.

18 Why do the tails matter? Think of
using the inferred FX for prediction.

19 Sometime the term “percentile” is
used in the literature.

In other words, zq is a number such that the probability mass sup-
ported by the interval (−∞, zq) is exactly q. Figure 2.6 clariﬁes this
deﬁnition. For example, the median, lower, and upper quartile are,
respectively, the 0.5, 0.25, and 0.75 quantiles.
then we expect that Fn(x) ≈ Φ(x), and, therefore, the the correspond-
ing quantiles should also match. Notice that

If the sample x1, . . . , xn is approximately normally distributed,

(cid:16)

(cid:17)

Fn

x(1)

=

1
n

, . . . , Fn(x(k)) =

k
n

, . . . , Fn(x(n)) =

n
n

= 1.

(2.14)

Therefore, the kth order statistics x(k) should be a good approxima-
tion for the ( k
. There is a little tech-
nical problem: if k = n, then z1 = +∞. There are may ways to get

n )th standard normal quantile z k

n

Figure 2.6: The standard normal quan-
tile zq in term of the CDF Φ(z) (top)
and PDF φ(z) (bottom).

00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91xFn(x)−4−202400.20.40.60.81Φ(z)−4−202400.10.20.30.4zφ(z)zqzqqq18 k. m. zuev

around this. One consists of taking z k
n+1
that q < 120.

The normal-quantile plot graphs the pairs

instead of z k
n

, to make sure

(cid:16)

(cid:17)

z k
n+1

, x(k)

,

for k = 1, . . . , n.

(2.15)

If the plotted points fall roughly on the line y = x, then it indicates
that the data have an approximate standard normal distribution. As
an illustration, Fig 2.7(a) shows the normal-quantile plot for the data
x1, . . . , xn sampled from the standard normal distribution.

20 Some software packages use z k−0.375
n+0.25

.

Figure 2.7: Normal-quantile plots for
the data x1, . . . , xn sampled from (a)
the standard normal distribution, (b)
uniform distribution on [−1/2, 1/2]
(short tails), (c) the Laplace distribution
f (x) ∝ e−|x| (long tails), and (d) a
bimodal distribution (a mixture of two
well-separated Gaussians). Sample size
in all examples is n = 100.

Question: What if the points fall on the line y = ax + b?
Departures from normality are indicated by systematic departures
from a straight line. Examples of different departures are illustrated
in Fig. 2.7(b), (c), and (d).

Q-Q plots can be made for any probability distribution, not nec-
essarily normal, which is considered as a theoretical model for the
process that generated the data. For example, we can construct a
uniform-quantile plot, exponential-quantile plot, etc. To compare two
different samples x1, . . . , xn and y1, . . . , yn, we can also create a Q-Q
21. Again,
plot by pairing their respective sample quantiles (x(k), y(k))
a departure from a straight line indicates a difference in the shapes of
the two samples.

21 What would you do if the samples
have different sizes x1, . . . , xn and
y1, . . . , ym, where m (cid:54)= n?

−4−2024−3−2−10123Sample Quantiles−4−2024−1−0.500.51−4−2024−6−4−20246Standard Normal QuantilesSample Quantiles−4−2024−15−10−5051015Standard Normal Quantiles(b)(c)(d)(a)Further Reading

1.

[FPP, Part II] gives a very intuitive description of histograms, the
mean, and the standard deviation. It is a lengthy but easy and
enjoyable read.

2. Summarizing data is a part of Exploratory Data Analysis (EDA), an
approach for data analysis introduced and promoted by John Tukey.
His seminal work Tukey (1977) “Exploratory Data Analysis” re-
mains one of the best texts on EDA.

What is Next?

We discussed how to summarize data, but how to get the data in
ﬁrst place? Perhaps the most popular way is to conduct a survey. In
the next three lectures we will discuss arguably the most classical
subjects of statistical inference: survey sampling.

Appendix: Normal Distribution

The standard normal curve, known as the bell curve or the Gaussian
curve22, is deﬁned as

(cid:18)

(cid:19)

φ(z) =

1√
2π

exp

− z2
2

.

(2.16)

The normal curve is unimodal, symmetric around zero, and follows
the so-called “68-95-99.7 rule”: approximately 68% of the area under
the curve is within 1 unit of its center23, 95% is within 2 units, and
99.7% is within 3 units.

statistical inference

19

Figure 2.8: John Turky. Photo source:
wikipedia.org.

22 V.I. Arnold’s principle states that
if a notion bears a personal name,
then this name is not the name of the
discoverer. The Arnold Principle is
applicable to itself as well as to the
Gaussian distribution: the standard
normal curve was discovered around
1720 by Abraham de Moivre.

23 Mathematically,(cid:82) 1−1 ϕ(z)dz ≈ 0.68.

Figure 2.9: The 68-95-99.7 rule for the
standard normal curve.

1√
2π

The coefﬁcient

does not have any sacral meaning, it is simply

a normalizing constant that guarantees that the full area under the
curve is exactly one,

φ(z)dz = 1.

(2.17)

(cid:90) +∞

−∞

This allows to interpret φ(z) as the probability density function (PDF)
of a random variable24. This random variable, often denoted by Z,

24 Recall that, any non-negative func-
tion p(x) that integrates to one can
be viewed as a PDF. The associated
random variable X is fully deﬁned by
p(x):

(cid:90)

P(X ∈ A) =

p(x)dx.

A

−4−3−2−10123400.10.20.30.4zφ(z)−4−3−2−10123400.10.20.30.4z−4−3−2−10123400.10.20.30.4z99.7%67%95%20 k. m. zuev

is called standard normal. Thanks to the Central Limit Theorem (CLT),
φ(z) is the single most important distribution in probability and
statistics25.

Traditionally, Φ(z) denotes the cumulative distribution function

(CDF), whose value at z is the area under the standard normal curve
to the left of z,

φ(z)dz.

(2.18)

(cid:90) z

−∞

The standard normal random variable Z has zero mean and unit

Φ(z) =

See the top panel of Fig. 2.6.

variance:

(cid:90) +∞

µ = E[Z] =
σ2 = V[Z] = E[(Z − µ)2] =

−∞

zφ(z)dz = 0,

(cid:90) +∞

−∞

z2φ(z)dz = 1.

(2.19)

The random variable X is called normal with mean µ and variance σ2,
(cid:18)
denoted X ∼ N (µ, σ2), if its PDF is26

(cid:19)

p(x) =

1√
2πσ

exp

− (x − µ)2
2σ2

.

(2.20)

Here are some useful facts:
1. If X ∼ N (µ, σ2), then Z = X−µ

2. If X ∼ N (µ, σ2), then P(a < X < b) = Φ(cid:16) b−µ

σ ∼ N (0, 1).

σ

(cid:17) − Φ(cid:16) a−µ

σ

(cid:17)

.

3. If X1 ∼ N (µ1, σ2

1 ), X2 ∼ N (µ2, σ2
then X = X1 + X2 ∼ N (µ1 + µ2, σ2

2 ), and X1 and X2 are independent,
1 + σ2
2 ).

25 Intuitively (and very roughly), CLT
states that the properly shifted and
scaled sum ∑N
i=1 Xi of more or less
any (!) random variables X1, . . . , Xn is
approximately standard normal. Many
phenomena in Nature can be accurately
modeled by sums of of random vari-
ables. This partially explains why many
histograms (which are can be viewed
as approximations for the underlying
PDFs) follow the normal curve.

26 The 68-95-99.7 rule holds for any
normal variable, we need just to replace
intervals [−1, 1], [−2, 2] and [−3, 3] with
[−σ, σ], [−2σ, 2σ], and [−3σ, 3σ].

3
Simple Random Sampling

Sample surveys are used to obtain information about a large popu-
lation. The purpose of survey sampling is to reduce the cost and the
amount of work that it would take to survey the entire population.
Familiar examples of survey sampling include taking a spoonful of
soup to determine its taste (a cook does not need to eat the entire
pot) and making a blood test to measure the red blood cell count
(a medical technician does not need to drain you of blood). In this
lecture we learn how to estimate the population average and how to
assess the accuracy of the estimation using simple random sampling,
the most basic rule for selecting a subset of a population.

A Bit of History

The ﬁrst known attempt to make statements about a population
using only information about part of it was made by the English mer-
chant John Graunt. In his famous tract (Graunt, 1662) he describes a
method to estimate the population of London based on partial infor-
mation. John Graunt has frequently been merited as the founder of
demography.

The second time a survey-like method was applied was more than

a century later. Pierre-Simon Laplace realized that it was important
to have some indication of the accuracy of the estimate of the French
population (Laplace, 1812).

Terminology

Let us begin by introducing some key terminology.

• Target population: The group that we want to know more about.

Often called “population” for brevity1.

Figure 3.1: By a small sample we may
judge of the whole piece, Miguel de
Cervantes “Don Quixote.” Photo
source: wikipedia.org.

Figure 3.2: Captain John Graunt. Photo
source: goodreads.com

Figure 3.3: Pierre-Simon Laplace. Photo
source: wikipedia.org.

1 Deﬁning the target population may be
nontrivail. For example, in a political
poll, should the target population be
all adults eligible to vote, all registered
voters, or all persons who voted in the
last election?

22 k. m. zuev

• Population unit: A member of the target population. In studying

human populations, observation units are often individuals.

• Population size: The total number of units in the population2. Usu-

ally denoted by N.

• Unit characteristic: A speciﬁc piece of information about each mem-

ber of the population3. For unit i, we denote the numerical value
of the characteristic by xi, i = 1, . . . , N.

• Population parameter: A summary of the characteristic for all units
in the population. One could be interested in various parameters,
but here are the four examples that are used most often:
1. Population mean (our focus in this lecture):

2 For very large populations, the exact
size is often not known.

3 For example, age, weight, income, etc.

2. Population total:

µ =

1
N

N∑

i=1

xi.

τ =

N∑

i=1

xi = Nµ.

3. Population variance (our focus in the next lecture):

σ2 =

1
N

N∑

i=1

(xi − µ)2.

4. Population standard deviation

(cid:118)(cid:117)(cid:117)(cid:116) 1

N

σ =

(xi − µ)2.

N∑

i=1

(3.1)

(3.2)

(3.3)

(3.4)

In an “ideal survey,” we take the entire target population, measure

the value of the characteristic of interest for all units, and compute
the corresponding parameter. This ideal (as almost all ideals) is rarely
met in practice: either population is too large, or measuring xi is
too expensive, or both. In practice, we select a subset of the target
population and estimate the population parameter using this subset.

• Sample: A subset of the target population.

• Sample unit: A member of the population selected for the sample.

• Sample size: The total number of units in the sample. Usually de-

noted by n. Sample size is often much less than the population
size, n (cid:28) N.
Let P = {1, . . . , N} be the target population and S = {s1, . . . , sn}
be a sample from P4. When it is not ambiguous, we will identify P

4 si ∈ {1, . . . , N} and si (cid:54)= sj.

and S with the corresponding values of the characteristic of interest,
that is

P = {x1, . . . , xN} and S = {xs1, . . . , xsn}.

(3.5)

To avoid cluttered notation, we denote xsi simply by Xi, and thus,

S = {X1, . . . , Xn} ⊂ {x1, . . . , xN} = P.

(3.6)

• Sample statistic: A numerical summary of the characteristic of the
sampled units5. The statistic estimates the population parameter.
For example, a reasonable sample statistic for the population mean
µ in (3.1) is the sample mean:

Xn =

1
n

n∑

i=1

Xi.

(3.7)

• Selection Rule: The method for choosing a sample from the target

population.

Many selection rules used in practice are probabilistic, meaning
that X1, . . . , Xn are selected at random according to some probability
method. Probabilistic selection rules are important because they
allow to quantify the difference between the population parameters
and their estimates obtained from the randomly chosen samples.
There is a number of different probability methods for selecting a
sample. Here we consider the simplest: simple random sampling6.

Simple Random Sampling

In simple random sampling (SRS), every subset of n units in the pop-
ulation has the same chance of being the sample7. Intuitively, we
ﬁrst mix up the population and then grab n units. Algorithmically, to
draw a simple random sample from P, we
1. Select s1 from {1, . . . , N} uniformly at random.
2. Select s2 from {1, . . . , N} \ {s1} uniformly at random.
3. Select s3 from {1, . . . , N} \ {s1, s2} uniformly at random.
4. Proceed like this till n units s1, . . . , sn are sampled.
In short, we draw n units one at a time without replacement8.

Questions: What is the probability that unit #1 is the ﬁrst to be

selected for the sample9? What is the probability that unit #1 is the
second to be selected for the sample? What is the probability that
unit #1 is selected for the sample? How about unit #k?

So, let X1, . . . , Xn be the SRS sample drawn from the population P,
and let us consider the sample mean Xn in (3.7) as an estimate of the
population mean µ in (3.1).

statistical inference

23

5 Essentially any function of X1, . . . , Xn.

6 More advanced methods include
stratiﬁed random sampling, cluster
sampling, and systematic sampling.

7 This chance is 1/(N
n ).

8 SRS with replacement is discussed in
S.L. Lohr Sampling: Design and Analysis.

9 i. e. what is P(s1 = 1), or, equivalently,
what is P(X1 = x1)?

24 k. m. zuev

Our goal: to investigate how accurately Xn approximates µ.
Before we proceed, let me reiterate a very important point: xi, and

therefore µ, are deterministic; Xi, and therefore Xn, are random.

Since Xn = 1

n ∑ Xi, it is natural to start our investigation from

the properties of a single sample element Xi. Its distribution is fully
described by the following Lemma.

Lemma 1. Let ξ1, . . . , ξm be the distinct values assumed by the population
units10. Denote the number of population units that have the value ξi by ni.
Then Xi is a discrete random variable with probability mass function

P(Xi = ξj) =

nj
N

,

j = 1, . . . , m,

and its expectation and variance are

E[Xi] = µ

and V[Xi] = σ2.

As an immediate corollary, we obtain the following result:

Theorem 1. With simple random sampling,

E[Xn] = µ.

(3.8)

(3.9)

(3.10)

Intuitively, this result tells us that “on average” Xn = µ

11. The

property of an estimator being equal to the estimated quantity on av-
erage is so important that it deserves a special name and a deﬁnition.
Deﬁnition 1. Let θ be a population parameter and ˆθ = ˆθ(X1, . . . , Xn)
be a sample statistic that estimates θ. We say that ˆθ is unbiased if

E[ ˆθ] = θ.

(3.11)

Thus, Xn is an unbiased estimate of µ. The next step is to investi-
gate how variable Xn is. As a measure of the dispersion of Xn about µ,
we will use the standard deviation of Xn

12

Let us ﬁnd the variance13:

V[Xn] = V

(cid:34)

1
n

n∑

i=1

(cid:35)

Xi

=

1
n2

V

se[Xn] =

V[Xn].

(3.12)

Cov(Xi, Xj).

(cid:35)

Xi

=

1
n2

n∑

n∑

i=1

j=1

(cid:113)
(cid:34) n∑

i=1

To continue, we need to compute the correlation.
Lemma 2. If i (cid:54)= j, then the covariance between Xi and Xj is

Cov(Xi, Xj) = − σ2
N − 1

.

(3.13)

(3.14)

10 For example, if x1 = 1, x2 = 1, x3 =
2, x4 = 3, and x5 = 3, then there are
m = 3 distinct values: ξ1 = 1, ξ2 =
2, ξ3 = 3.

11 This is good news and justiﬁes the
characteristic “reasonable estimate” of µ
that we gave to Xn above.

12 Standard deviations of estimators are
often called standard errors (se). Hence
the notation in Eq. (3.12).

13 If sampling were done with re-
placement then Xi would be in-
dependent, and we would have:
V[Xn] = 1
n2
n2 ∑n
1
i=1
In SRS, however, sampling is done with-
out replacement and this introduces
dependence between Xi.

V [∑n
V[Xi] = 1

i=1 Xi] =
n2 ∑n

i=1 σ2 = σ2
n .

And, therefore, we have:

(cid:18)

Theorem 2. The variance of Xn is given by

1 − n − 1
N − 1
A few important observations are in order:

V[Xn] =

σ2
n

(cid:16)

(cid:17)
approximately(cid:0)1 − n

1 − n−1
N−1

1. The factor

(cid:1). The ratio n

N

statistical inference

25

(cid:19)

.

(3.15)

is called ﬁnite population correction. It is

N is called the sampling fraction.

2. Finite population correction is always less than one. Therefore,

n . This means that SRS is more efﬁcient than sampling

V[Xn] < σ2
with replacement.

3. If the sampling fraction is small, that is if n (cid:28) N, then

V[Xn] ≈ σ2
n

se[Xn] ≈ σ√
n
4. To double the accuracy of approximation Xn ≈ µ

and

.

(3.16)

14, the sample

size n must be quadrupled.

5. If σ is small15, then a small sample will be fairly accurate. But if σ
is large, then a larger sample will be required to obtain the same
accuracy.

Further Reading

1. The history of survey sampling, in particular, how sampling be-
came an accepted scientiﬁc method, is described in a nice discus-
sion paper by J. Bethlehem (2009) “The rise of survey sampling.”

What is Next?

The result (3.15) and the above observations are nice, but we have a
serious problem: we don’t know σ! In the next lecture, we will learn
how to estimate the population variance using SRS.

14 i. e. to reduce se[Xn] by half.

15 i. e. the population values are not very
dispersed.

4
Population Variance and the Bootstrap Method

Estimating population variance σ is important because of at least
two reasons:

1) it is important population parameter by itself and
2) it appears in the formula for the standard error of the sample

mean Xn

1:

(cid:115)(cid:18)

(cid:19)

se[Xn] =

σ√
n

1 − n − 1
N − 1

.

(4.1)

1 Recall that Xn is an unbiased estimate
of the population mean µ, E[Xn] = µ.

If we want to compute se[Xn] or to determine the required sample

size n to achieve a prescribed value of of error, we must know σ. In
this lecture we learn two things:

1) how to estimate σ and
2) how to estimate se[Xn] ... without estimating σ!

Estimation of the Population Variance

Recall that the population variance is

σ2 =

1
N

N∑

i=1

(xi − µ)2.

It seems natural to use the following estimate:

ˆσ2
n =

1
n

n∑

i=1

(Xi − Xn)2.

However, this estimate is biased.

Theorem 3. The expected value of ˆσ2

n is given by
n] = σ2 Nn − N
Nn − n

.

E[ˆσ2

(4.2)

(4.3)

(4.4)

Since Nn−N

Nn−n < 1, we have that E[ˆσ2

n tends to
underestimate σ2. Theorem 3 helps to construct an unbiased estimate
for the population variance:
Corollary 1. An unbiased estimate for the population variance σ2 is

n] < σ2, and thus, ˆσ2

s2 = ˆσ2
n

Nn − n
Nn − N

=

1 − 1
N

n − 1

(Xi − Xn)2.

n∑

i=1

(4.5)

(cid:18)

Note that if both population size N and the sample size n are

n. Combining (4.1) with (4.5) gives the estimate of

large, then s2 ≈ ˆσ2
the standard error:

se[Xn] ≈ (cid:98)se[Xn] =

s√
n

1 − n − 1
N − 1

.

(4.6)

(cid:19)

(cid:19) 1

(cid:115)(cid:18)

Thus, in simple random sampling, we can estimate (Xn) not only the
unknown population parameter (µ), but also obtain the likely size of

the error of the estimate ((cid:98)se[Xn]). In other words, we can obtain the

estimate of a parameter as well as the estimate of the error of that
estimate.

The Bootstrap Method for Estimating se[Xn]

Let us take a step back and look at Eq. (4.1).

Question: Is there a way to estimate se[Xn] without estimating σ?2
Let us quickly refresh our minds. The sample mean Xn is a dis-
crete random variable which is obtained by averaging sample units
S = {X1, . . . , Xn} which are obtained from the target population
P = {x1, . . . , xN} by simple random sampling.

Now let us forget for the moment about SRS and consider the fol-

lowing problem. Suppose Y is a discrete random variable with the
probability mass function P. And suppose we can generate indepen-
dent realizations of Y, that is we can independently sample from P:

Y1, . . . , YB ∼ P.
(cid:32)

How can we estimate the variance of Y? Well, we can do this using
the law of large numbers3 (LLN). Namely,
B∑

V[Y] = E[(Y − E[Y])2] ≈ 1
B

Yi − 1
B

(cid:33)2

.

B∑

j=1

Yj

i=1

(4.8)

Now let us apply this to Y = Xn. To do this, we would have to

generate B simple random samples from P:
1 , . . . , X(1)

S (1) = {X(1)

n } ⊂ P,

statistical inference

27

2 — Why should we care? We already
know how to estimate σ!

— Because there are many cases

when we can construct an unbiased
estimate ˆθ of a population parameter
θ, but we don’t know the analytical
formula (like (4.1)) for its standard error
se[ ˆθ]. For example, s2 is an unbiased
estimate of σ2, but what is the standard
error se[s2]? In such cases, we need an
alternative way of estimating se.

3 The law of large numbers is one of
the main achievements in probability.
Intuitively, it says that if B is large, then
the sample average YB = 1
i=1 Yi is
a good approximation for E[Y]. More
formally, the weak (strong) LLN states
that YB converges to E[Y] in probability
(almost surely), as B → ∞.

B ∑B

(4.7)

(4.9)

. . .

S (B) = {X(B)

1

, . . . , X(B)

n } ⊂ P,

28 k. m. zuev

compute the corresponding sample means:

X(1)
n =

1
n

n∑

i=1

X(1)
i

,

. . .

X(B)
n =

1
n

n∑

i=1

X(B)
i

,

and, ﬁnally, estimate se[Xn] by analogy with (4.8):

se[Xn] ≈ (cid:98)se[Xn] =

(cid:32)

(cid:118)(cid:117)(cid:117)(cid:116) 1

B

B∑

i=1

n − 1
X(i)
B

B∑

j=1

X(j)
n

(cid:33)2

.

(4.10)

(4.11)

Looks good expect for one thing: the total sample size in (4.9) is
nB, which is much larger than our original sample size n, nB (cid:29) n.
Therefore, this straightforward method for estimating se[Xn] is not
really acceptable since we assume that sampling n population units
is the maximum we can afford4. Here is where the bootstrap principle
comes into play.

The bootstrap is a very general simulation-based method, intro-
duced by Bradley Efron, for measuring uncertainty of an estimate.
It requires no analytical calculations and often used in applications.
In Lecture 9, we will discuss the bootstrap in detail in different con-
texts. Here is our ﬁrst encounter with the bootstrap: in the context of
survey sampling.
The intuition behind the bootstrap is the following. In SRS, our
main underlying assumption is that our sample S represents the
target population P well. Based on S, we can then create a new
population of size N by simply creating N/n copies of each Xi
call it the bootstrap population:

5. We

(4.12)

(cid:124)

(cid:123)(cid:122)
(cid:125)
Pboot = {X1, . . . , X1

(cid:125)
Bootstrap principle: Use Pboot instead of P in (4.9).
In other words, instead of sampling the target population, boot-

, . . . , Xn, . . . , Xn

(cid:123)(cid:122)

(cid:124)

}.

N/n

N/n

strap6 says that we can “reuse” our original sample S = {X1, . . . , Xn}.
That is, for every b = 1, . . . , B, S (b) = {X(b)
n } is a simple ran-
dom sample from Pboot. We call S (b) a bootstrap sample. The rest is
exactly as before. The bootstrap estimate of the standard error se[Xn] is
given by (4.11).

1 , . . . , X(b)

Example: Gaussian Population

For illustrative purposes, let us consider a “Gaussian” population
P, where x1, . . . , xN are independently drawn from the normal dis-
tribution N (µ0, σ2
0 ) with µ0 = 0, σ0 = 10, and the population size
N = 1047. The resulting population mean is µ = 0.11 and standard

4 After all, if we could afford sampling
nB units, we would use XnB as an
estimate of µ instead of Xn!

Figure 4.1: Bradley Efron, the fa-
ther of the bootstrap. Photo source:
statweb.stanford.edu.
5 For simplicity, we assume here that
N/n is an integer. But if it is not,
we can always round off N/n to the
nearest integer.

6 This method derives its name from
the expression “to pull youself up by
your own bootstraps,” P. Diaconis and
B. Efron, “Computer-intensive meth-
ods in statistics,” Scientiﬁc American,
248(5):116-129, 1983.

7 In other words, we generate N realiza-
tions of Z ∼ N (µ, σ2), “freeze” them,
and denote the obtained values by xi.

deviation is σ = 10.13. As expected, they are close to 0 and 10, re-
spectively. Let S be a simple random sample from P of size n = 102.
The obtained value of the sample mean is Xn = 0.4. The exact value
of the standard error of Xn is given by (4.1)8:

se[Xn] = 1.01.

(4.13)

Figure 4.2 shows the boxplots of the bootstrap estimates (4.11) with
B = 102, 103, and B = 104 as well as the analytical estimate (4.6)
marked by a green star. The larger B, the smaller the dispersion of
the bootstrap estimates. Both analytical and bootstrap estimates

(cid:98)se[Xn] agree with the exact value (4.13).

statistical inference

29

8 In this example, we can compute
the exact value, since we know the
population variance σ2.

Figure 4.2: Boxplots of bootstrap
estimates. Each boxplot is constructed
based on 100 bootstrap estimates. That
is, we repeated (4.9) with P = Pboot,
(4.10), and (4.11) 100 times for each
value of B.

Further Reading

1. Intended for general readership, P. Diaconis and B. Efron (1983),
“Computer-Intensive Methods in Statistics,” Scientiﬁc American,
248(5):116-129 discusses different applications of bootstrap.

What is Next?

The sample mean Xn is a point estimate (single number) of the pop-
ulation mean µ. In the next lecture, we will learn how to construct
conﬁdence intervals for µ, which are random intervals that contain µ
with a prescribed probability.

B=100B=1000B=100000.750.80.850.90.9511.051.11.15 Analytical estimate (6)5
Normal Approximation and Conﬁdence Intervals

In the last two lectures we studied properties of the sample
mean Xn under SRS. We learned that it is an unbiased estimate of the
population mean,

derived the formula for its variance,

E[Xn] = µ,

(cid:18)

V[Xn] =

σ2
n

1 − n − 1
N − 1

(cid:19)

,

and learned how to estimate it analytically and using the bootstrap.
Ideally, however, we would like to know the entire distribution of
1, called sampling distribution, since it would tell us everything
Xn
about the accuracy of the estimation Xn ≈ µ. In this lecture, we
discuss the sampling distribution of Xn and show how it can be used
for constructing interval estimates for µ.

Normal Approximation for Xn

1 A random variable can’t be fully
described by only ﬁrst two moments.

(5.1)

(5.2)

(5.3)

(5.4)

First, let us recall one of the most remarkable results in probability:
the Central Limit Theorem (CLT). Simply put, the CLT says that if
Y1, . . . , Yn are independent and identically distributed (iid) with mean
µ and variance σ2, then Yn = 1
approximately normal with mean µ and variance σ2
n

i=1 Yi has a distribution which is

n ∑n

2:

(cid:18)

µ, σ2
n

(cid:19)

.

Yn ˙∼ N
(cid:33)

≤ z

(cid:32)

P

Yn − µ
√
n
σ/

Symbol

˙∼ means “approximately distributed.” More formally,

→ Φ(z),

as n → ∞,

where Φ(z) is the CDF of the standard normal N (0, 1).

n is trivial. The remarkable

2 The fact that E[Yn] = µ and
V[Yn] = σ2
part of the CLT is that the distribu-
tion of Yn is normal regardless of the
distribution of Yi.

Question: Can we use the CLT to claim that the sampling distribu-

tion of Xn under SRS is approximately normal?

Answer: Strictly speaking, no. Since in SRS, Xi are not independent3

(although identically distributed). Moreover, it makes no sense to
have n tend to inﬁnity while N is ﬁxed.

Nevertheless... it can be shown that if both n and N are large, then

Xn is approximately normally distributed:

Xn ˙∼ N (µ, V[Xn])

or

Xn − µ
se[Xn]

˙∼ N (0, 1).

(5.5)

The intuition behind this approximation is the following: if both
n, N (cid:29) 1, then Xi are nearly independent, and, therefore, the CLT
approximately holds.
The CLT result in (5.5) is very powerful: it says that for any popu-
lation, under SRS (for n (cid:29) 1 and n (cid:28) N), the sample mean has an
approximate normal distribution.

Example: Birth Weights
Let us consider the example from Lecture 2b, where the target pop-
ulation P is the set of all birth weights4. The population parame-
ters are: N = 1236, µ = 3.39, and σ = 0.52. Let n = 100, and let
S (1), . . . ,S (m) be the SRS samples from P, m = 103. Figure 5.1 shows
the normal-quantile plot for the corresponding standardized5 sample
means X(1)
well.

n −µ
n −µ
se[Xn] , . . . , X(m)
se[Xn] . The normal approximation (5.5) works

statistical inference

31

3 Recall Lemma 2 in Lecture 3.

4 The data is available at birth.txt

5 If X is a random variable with mean µ
and variance σ2, then X−µ
is called the
standardized variable; it has zero mean
and unit variance. This transformation
is often used in statistics.

σ

Figure 5.1: Normal-quantile plot
for the standardized sample means
n −µ
n −µ
se[Xn ] , . . . , X(m)
X(1)
se[Xn ] . The sampling dis-
tribution closely follows the standard
normal curve.

−4−3−2−101234−3−2−10123Standard Normal QuantilesQuantiles of Straight Line ApproximationQ−Q plot32 k. m. zuev

In this example, we know both population parameters µ and σ,
and, therefore, the exact standard error se[Xn] is also known from
(5.2). Suppose now that, in fact, we don’t know the entire population,
and we have only one simple random sample S from P.

Question: How to check in this case that the sampling distribution

of the sample mean Xn follows the normal curve?
Estimating the Probability P(|Xn − µ| ≤ )
The normal approximation of the sampling distribution (5.5) can be
used for various purposes. For example, it allows to estimate the
probability that the error made in estimating µ by Xn is less than
ε > 0. Namely,

P(|Xn − µ| ≤ ε) ≈ 2Φ

− 1,

(5.6)

(cid:18) ε

(cid:19)

se[Xn]

where the standard error can be estimated, for example, by the boot-
strap.

Conﬁdence Intervals

What is the probability that Xn exactly equals to µ? Setting  = 0 in
(5.6), gives an intuitively expected result: P(Xn = µ) ≈ 0. But given
a simple random sample X1, . . . , Xn, can we deﬁne a random region6
that contains the population mean µ with high probability? It turns
out that yes, the notion of a conﬁdence interval formalizes this idea.
Let 0 < α < 1. A 100(1 − α)% conﬁdence interval for a population
parameter θ is a random interval I calculated from the sample, which
contains θ with probability 1 − α,

P(θ ∈ I) = 1 − α.

(5.7)

The value 100(1 − α)% is called the conﬁdence level7.
proximation (5.5). Since Xn−µ

Let us construct a conﬁdence interval for µ using the normal ap-

(cid:18)

P

−z1− α

se[Xn] is approximately standard normal,
≤ Xn − µ
se[Xn]

≈ 1 − α,

≤ z1− α

2

2

(cid:19)

(5.8)

6 As opposed to random number Xn.

7 Usually 90% (α = 0.1) or 95% (α =
0.05) levels are used.

(cid:16)

where zq is the qth standard normal quantile, Φ(zq) = q. We can
rewrite (11.1) as follows:

P

Xn − z1− α

se[Xn] ≤ µ ≤ Xn + z1− α

(5.9)
se[Xn] is an approximate 100(1 − α)%
This means that I = Xn ± z1− α
conﬁdence interval for µ. Conﬁdence intervals often have this form:

se[Xn]

2

2

2

(cid:17) ≈ 1 − α.

I = statistic ± something,

(5.10)

and the “something” is called the margin of error8.
Conﬁdence intervals are often misinterpreted9. Suppose that we
got a sample S = {X1, . . . , Xn} from the target population P, set the
conﬁdence level to, say 95%, plugged in all the numbers in (5.9) and
obtained that the conﬁdence interval for µ is, for example, [0, 1]. Does
it mean that µ belongs to [0, 1] with probability 0.95? No, of course
not: µ is a deterministic (not random) parameter, it either belongs to
[0, 1] or it does not10.

The correct interpretation of conﬁdence intervals is the follow-
ing. First, it is important to realize that Eq. (6.15) is a probability
statement about the conﬁdence interval, not the population param-
eter11. It says that if we take many samples S (1), . . . , and compute
conﬁdence intervals I (1), . . . , for each sample, then we expect about
100(1 − α)% of these intervals to contain θ. The conﬁdence level
100(1 − α)% describes the uncertainty associated with a sampling
method, simple random sampling in our case.

Example: Birth Weights

Let us again consider the example with birth weights. Figure 5.2
shows 90% conﬁdence intervals for µ computed from m = 100 simple
random samples. Just as different samples lead to different sample
means, they also lead to different conﬁdence intervals. We expect that
about 90 out of 100 intervals would contain µ. In our experiment, 91
intervals do.

statistical inference

33

8 For the constructed interval for µ, the
margin of error is z1− α
2
9 Even by professional scientists.

se[Xn].

10 In other words, once a sample is
drawn and an interval is calculated, this
interval either covers µ or it does not, it
is no longer a matter of probability.

11 Perhaps, it would be better to rewrite
it as P(I (cid:51) θ) = 1 − α.

Figure 5.2: 90% conﬁdence intervals for
the population mean µ = 3.39. Intervals
that don’t contain µ are shown in red.

0102030405060708090100Confidence Intervals3.23.253.33.353.43.453.53.553.63.65Population mean 7=3.3934 k. m. zuev

Survey Sampling: Postscriptum

We stop our discussion of survey sampling here. The considered SRS
is the simplest sampling scheme and provides the basis for more ad-
vanced sampling designs, such as stratiﬁed random sampling, cluster
sample, systematic sampling, etc. For example, in stratiﬁed random
sampling (StrRS), the population is partitioned into subpopulations,
or strata, which are then independently sampled using SRS. In many
applications, stratiﬁcation is natural. For example, when studying
human populations, geographical areas form natural strata. SrtRs is
often used when, in addition to information about the whole popu-
lation, we are interested in obtaining information about each natural
subpopulation. Moreover, estimates obtained from StrRS can be con-
siderably more accurate than estimates from SRS if a) population
units within each stratum are relatively homogeneous and b) there
is considerable variation between strata. If the total sample size we
could afford is n and there are L strata, then we face an optimal re-
source allocation problem: how to chose the sample sizes nk for each
stratum, so that ∑L
k=1 nk = n and the variance of the corresponding
estimator is minimized? This leads to the so-called Neyman alloca-
tion scheme, but this is a different story.

Further Reading

1. A detailed discussion of survey sampling12 is given in the fun-
damental (yet accessible to students with diverse statistical back-
grounds) monograph by S.L. Lohr Sampling: Design and Analysis.

12 Which contains all the sampling
scheme mentioned in the Postscriptum.

What is Next?

Summarizing Data and Survey Sampling constitute the core of clas-
sical elementary statistics. In the next lecture, we will draw a big
picture of modern statistical inference.

6
Modeling and Inference: A Big Picture

Suppose we are interested in studying a certain phenomenon which
can be schematically represented as follows:

Furthermore, suppose we collected some data {(inputs, responses)}
by observation or experiment. The most basic question in statistics
is: what can we learn, or infer, about the phenomenon from data?
Generally, there two goals in analyzing the data:
1. Understanding. To extract some information on how Nature asso-

ciates the responses to the inputs.

2. Prediction. To be able to predict the response to the future input.

The main idea of statistical inference is to replace the Nature

“black box” (i.e. the unknown mechanism that Nature uses to as-
sociate the responses to the inputs) by a statistical model:

The key feature of a statistical model is that the observed variability
in the data is represented by probability distributions, which form the
building-blocks of the model. In other words, the data is treated as
the outcome of a random experiment, as the realization of random
variables. In this lecture, we discuss various statistical models and
consider several fundamental concepts of statistical inference.

Statistical Models

For simplicity, let us ﬁrst assume that the data consists only from
“responses” X1, . . . , Xn. A statistical model F is then simply a set of
probability distributions F (or probability density functions f ) for

00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91NatureInputResponse00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91InputStatistical ModelResponse36 k. m. zuev

Xi. The basic statistical inference problem can then be formulated
as follows: given data X1, . . . , Xn, we assume1 that it is an iid sample
from F,

X1, . . . , Xn ∼ F,

F ∈ F,

(6.1)

There are two big classes of statistical models: parametric and non-

and what to infer F or some properties of F (such as its mean).
parametric2. A statistical model F is called parametric if it can be
parameterized by a ﬁnite number of parameters. For example, if we
assume that the data comes from a normal distribution, then the
model is a two-dimensional parametric model:

 f (x; µ, σ2) =

F =

(cid:18)

(cid:19)

− (x−µ)2
2σ2

, µ, σ2 ∈ R

1√
2πσ

e

 .

In general, a parametric model takes the from
F = {F(x; θ), θ ∈ Θ},

where θ is an unknown parameter (or vector of parameters) that
takes values in the parameter space Θ ⊂ Rd. In parametric inference,
we thus want to learn about θ from the data.
Quite naturally, a statistical model F is called nonparametric if it
is not parametric, that is if it cannot be parametrized by a ﬁnite num-
ber of parameters. For example, if we assume that the data comes for
a distribution with zero mean, then the model is nonparametric3:

(cid:26)

(cid:90)

(cid:27)

F =

F :

xdF(x) = 0

.

(6.4)

instead.

1 It is very important to keep in mind
that (6.1) is an assumption, which, in
fact, can be wrong.

2 Which lead to two subﬁelds of statis-
tics: parametric and nonparametric
statistics.

let us take F = (cid:8) f :(cid:82) x f (x)dx = 0(cid:9)

3 If you feel uncomfortable with (6.4),

(6.2)

(6.3)

Taking this example to extreme and throwing away the zero mean
assumption, we obtain the most general statistical model,

F = {all CDFs},

(6.5)

which is of course nonparametric4.

Historically, parametric models were developed ﬁrst since most

nonparametric methods were not feasible in practice, due to limited
computing power. Nowadays, this has changed due to rapid develop-
ments in computing science.

Advantages of parametric models:

1. Parametric models are generally easier to work with.
2. If parametric model is correct5, then parametric methods are more

efﬁcient than their nonparametric counterparts.

3. Sometimes parametric models are easier to interpret.

4 At ﬁrst glance, this model may look
silly, but it is not. In fact, this model
is often used in practice when nothing
is really known about the mechanism
that generated the data. Essentially, the
model in (6.5) says that all we assume
is that X1, . . . , Xn is an iid sample from
some distribution. In the forthcoming
lectures we will see that we can learn
a lot from the data even under this
seemingly weak assumption.

5 This means that there exists the value
of θ0, often called the “true value,” such
that the corresponding distribution
F(x; θ0) ∈ {F(x; θ), θ ∈ Θ} indeed
adequately describes the data.

Advantages of nonparametric models:
1. Sometimes it is hard to ﬁnd a suitable parametric model.
2. Nonparametric methods are often less sensitive to outliers.
3. Parametric methods have a high risk of mis-speciﬁcation6.

The art of statistical modeling is based on a proper incorporation
of the scientiﬁc knowledge about the underlying phenomenon into
the model and on ﬁnding a balance between the model complexity
on one hand and the ability to analysis the model analytically or
numerically on the other hand. The choice of the model also depends
on the problem and the answer required, so that different models
may be appropriate for a single set of data.

Example: Darwin and Corn

Charles Darwin wanted to compare the heights of self-fertilized and
cross-fertilized corn plants. To this end, he planted n = 15 pairs of
self- and cross-fertilized plants in different pots, trying to make all
other characteristics of the plants in each pair the same (descended
from the same parents, planted at the same time, etc).

statistical inference

37

6 Mis-speciﬁcation is the choice of the
model F that in fact does not contain a
distribution that adequately describes
the modeled data.

Figure 6.1: Charles Darwin studying
corn. Photo source: wikipedia.org.
.Figure 6.2: The heights of corn plants
for two different types of fertilization.

Figure 6.2 summarizes the results in terms of boxplots. Cross-
fertilized plants seem generally higher then self-fertilized ones. At
the same time, there is a variation of heights within each group, and
one could model this variability in terms of probability distribu-
tions7. But if the spread of heights within each group is modeled by
random variability, the same cause will also generate variation be-
tween groups. So Darwin asked his cousin, Francis Galton, whether
the difference in heights between the types of plants was too large to

7 It might be possible, to construct a
mechanistic model for plant growth
that could explain all the variation in
such data. This would take into account
genetic variation, soil and moisture
conditions, ventilation, lighting, etc,
through a vast system of equations
requiring numerical solution. For most
purposes, however, a deterministic
model of this sort is unnecessary, and it
is simpler to express variability in terms
of probability distributions.

SelfCrossHeight (eights of an inch)10012014016018038 k. m. zuev

have occurred by chance, and was in fact due to the effect of fertiliza-
tion. If so, he wanted to estimate the average height increase.

Galton proposed an analysis based on the following model. The

height of a self-fertilized plant is modeled as

Xs = µ + σ,

where µ and σ are ﬁxed unknown parameters, and  is a random
variable with zero mean and unit variance. Thus, E[Xs] = µ and
V[Xs] = σ2. The height of a cross-fertilized plant is modeled as

Xc = µ + η + σ,

(6.6)

(6.7)

where η is another unknown parameter. Therefore, E[Xc] = µ + η
and V[Xc] = σ2. In the model (6.6) & (6.7), variation within the
groups is accounted for by the randomness of , whereas variation
between groups is modeled deterministically by η, the difference
between the means of Xc and X f . Under this model8, the questions
asked by Darwin are:
a) Is η (cid:54)= 0?
b) Can we estimate η and state the uncertainty of our estimate?

Fundamental Concepts in Inference

Many inferential problems can be identiﬁed as being one of the three
types: estimation, conﬁdence sets, or hypothesis testing9. In this
lecture we will consider all of these problems. Here we give a brief
introduction to the ideas and illustrate them with the iconic coin
ﬂipping example.

Point Estimation

Point estimation refers to providing a single “best guess” for some
quantity of interest, which could be a population parameter10, a
parameter in a parametric model, a CDF F, a probability density
function f , a regression function11 r, to name a few. By convention,
we denote a point estimate of θ by ˆθ.
Let X1, . . . , Xn be data which is modeled as an iid sample from a
distribution F(x; θ) ∈ F, where F is a parametric model. A point
estimate ˆθn of a parameter θ is some function of the data:

ˆθn = s(X1, . . . , Xn).

(6.8)

Thus, θ is a ﬁxed deterministic unknown quantity, and ˆθn is a random
variable. The distribution of ˆθn is called the sampling distribution. The
standard deviation of ˆθn is called the standard error12,

Figure 6.3: Sir Francis Galton, Darwin’s
cousin. Among many other things, he
developed the concept of correlation.
Photo source: wikipedia.org.

.

8 By the way, is this model parametric or
nonparametric?

9 For example, Darwin’s problems a)
and b) are, respectively, hypothesis
testing and estimation.

10 For instance, the sample mean Xn is a
point estimate for the population mean
µ.
11 See below.

12 Notice that these deﬁnitions mirror
the corresponding deﬁnitions for Xn
that we discussed in the context of
survery sampling.

statistical inference

39

(cid:113)

V[ ˆθn].

se[ ˆθn] =

(6.9)

To access how good a point estimate is on average, we introduce bias:

bias[ ˆθn] = E[ ˆθn] − θ.

(6.10)

We say that ˆθn is unbiased if bias[ ˆθn] = 0. Unbiasedness is a good
property of an estimator, but its importance should not be overstated:
an estimator could be unbiased, but at the same time it could have a
very large standard error. Such an estimator is poor since its realiza-
tions are likely to be far from θ, although, on average, the estimator
equals to θ. The overall quality of a point estimate is often assessed
by the mean squared error, or MSE,

MSE[ ˆθn] = E[( ˆθn − θ)2].

(6.11)

It is straightforward to check that MSE can be written it terms of bias
and standard error as follows:

MSE[ ˆθn] = bias[ ˆθn]2 + se[ ˆθn]2.

(6.12)

This is called the bias-variance decomposition for the MSE.

Example: Let us take a coin and ﬂip it n times. Let Xi = 1 if we get
“head” on the ith toss, and Xi = 0 if we get “tail”. Thus, we have
the data X1, . . . , Xn. Since we don’t know whether the coin is fair, it
is reasonable to model the data by the Bernoulli distribution, which
is the probability distribution of a random variable which takes the
value 1 with probability p and the value 0 with probability of 1 − p,
where p ∈ [0, 1] is a model parameter13. So, assume that

X1, . . . , Xn ∼ Bernoulli(p).

(6.13)

13 If the coin is fair, p = 1/2.

The goal is to estimate p from the data. It seems reasonable to esti-
mate p by

This estimate is unbiased, its standard error is se[ ˆpn] =(cid:112)p(1 − p)/n,

ˆpn = Xn =

and the means squared error is MSE[ ˆpn] = p(1 − p)/n.

(6.14)

(cid:3)

1
n

n∑

i=1

Xi.

Conﬁdence Sets

We have already encountered conﬁdence intervals in the context
of survey sampling. Here, they are deﬁned similarly. Suppose that
X1, . . . , Xn ∼ F(x; θ). A 100(1 − α)% conﬁdence interval for parameter θ
is a random interval I calculated from the data, which contains θ with
probability 1 − α,

P(θ ∈ I) = 1 − α.

(6.15)

40 k. m. zuev

If θ is a vector, then we an interval is replaced by a conﬁdence set,
which can be a cube, a sphere, an ellipsoid, or any other random set
that traps θ with probability 1 − α.

Example: Let us construct a conﬁdence interval for p in the coin ex-
ample. We can do this using Hoeffding’s inequality: if X1, . . . , Xn ∼
Bernoulli(p), then, for any  > 0,

P(|Xn − p| > ) ≤ 2e−2n2.

(cid:113) 1

If we set n,α =

2n log 2

α , then (6.16) is equivalent to
P(Xn − n,α < p < Xn + n,α) > 1 − α,

(6.16)

(6.17)

which means that Xn ± n,α is at least a (1 − α)100% conﬁdence
interval for p.

(cid:3)

Clearly, this method for constructing a conﬁdence interval can be
used only for data that can be modeled by the Bernoulli distribution.
In general, many point estimates turn out to have, approximately, a
normal distribution14,

ˆθn − θ
se[ ˆθn]

˙∼ N (0, 1).

(6.18)

This approximation can be used for constructing approximate conﬁ-
dence intervals.

Hypothesis Testing

While we discussed estimation and conﬁdence intervals in the con-
text of survey sampling, hypothesis testing is something new for us.
In hypothesis testing, we start with some default theory, called a
null hypothesis, and we ask if the data provides sufﬁcient evidence to
reject the theory. If yes, we reject it; if not, we accept it.

Example: Suppose we want to test if the coin is fair. Let H0 denote
the null hypothesis that the coin is fair, and let H1 denote the alterna-
tive hypothesis that the coin is not fair. Under the Bernoulli model, we
can write the hypothesis as follows:

H0 : p = 1/2

and H1 : p (cid:54)= 1/2.

(6.19)

It seems reasonable to reject H0 if |Xn − 1/2| is too large. When we
discuss hypothesis testing in detail, we will be more precise about
how large the statistic |Xn − 1/2| should be to reject H0.

(cid:3)

Figure 6.4: Wassily Hoeffding, one of
the founders of nonparametric statistics.
Photo source: nap.edu

.

14 Recall the normal approximation for
the sample mean Xn in SRS.

statistical inference

41

15 It is also called a predictor or regressor,
or feature, or independent variable.
16 It is also called an outcome variable or
dependent variable.
17 For example, X is the lab test results
and Y is the presence (Y = 1) or
absence (Y = 0) of a certain disease.

Prediction, Classiﬁcation, and Regression

Suppose now that, in accordance with the schemes in the abstract,
our data consists of pairs of observations: (X1, Y1), . . . , (Xn, Yn),
where Xi is an “input” and Yi is the corresponding “outcome”. For
example, Xi is a the father’s height, Yi in the son’s height, and i is the
family number.

The task of predicting the son’s height Y based on this father’s
height X for a new family is called prediction. In this context, X is
called a covariate15 and Y is called a response variable16. If Y is dis-
crete17, Y ∈ {1, . . . , K}, then prediction is called classiﬁcation since it
involves assigning the observation X to a certain class Y.

Regression is a method for studying the relationship between a
response variable Y and a covariate X. It is based on the so-called
regression function

r(x) = E[Y|X = x],

(6.20)

which is the expected value of the response given the value of the co-
variate. In regression, our goal is to estimate the regression function
which then can be used for prediction or classiﬁcation. If we assume
that r(x) is a linear function,

r(x) ∈ F = {r(x) = β0 + β1x, β0, β1 ∈ R},

(6.21)

then we have a liner regression model. We will discuss regression in the
last lectures.

Further Reading

1. A thought provoking and stimulating paper by Leo Breiman “Sta-

tistical modeling: the two cultures,” Statistical Science, 16(3): 199-
231., compares stochastic data modeling (which we discussed in
this lecture and which is a mainstream in statistical research and
practice) with algorithmic modeling which was developed outside
statistics (in particular, in computer science) and does not assume
any stochastic model for the data. See also the comments on the
paper by D.R. Cox, B. Efron, B. Hoadley, and E. Parzen as well as
the rejoinder by Breiman.

What is Next?

In the next lecture, we will start discussing the elements of nonpara-
metric inference.

7
Estimating the CDF and Statistical Functionals

The basic idea of nonparametric inference is to use data X1, . . . , Xn
to infer an unknown quantity of interest θ while making as few as-
sumptions as possible. Mathematically, “few assumptions” means
that the statistical model F used to model the data,

X1, . . . , Xn ∼ F, F ∈ F,

(7.1)

is large, inﬁnite-dimensional1. Here we take F = {all CDFs}.

In this lecture we will discuss one of the central problems in non-
parametric inference: estimation of a parameter θ of F2. Hold on. If
dimF = ∞, the model F can’t be parametrized by a ﬁnite number
of parameters. So what do we mean by a “parameter” of F? Let us
discuss this.

Functionals and Parameters

A statistical functional is any function of the CDF,

t : F → R,

F (cid:51) F (cid:55)→ t(F) ∈ R.

A parameter of a distribution F is the value of a functional t on F,

θ = t(F).

(7.2)

(7.3)

1 A better name for nonparametric
inference might be inﬁnite-dimensional
inference.
2 Other problems include density esti-
mation: given X1, . . . , Xn ∼ F, estimate
f (x) = F(cid:48)(x); and nonparametric re-
gression: given (X1, Y1), . . . , (Xn, Yn),
estimate the regression function
r(x) = E[Y|X = x]. See L.A. Wasser-
man, All of Nonparametric Statistics.

Examples of t and θ include:

1. t(F) =(cid:82) xdF(x) = µF, mean3,
2. t(F) =(cid:82) (x − µF)2dF(x) = σ2

F, variance,

((cid:82) (x−µF)2dF(x))1/2
(cid:82) xdF(x)
(cid:82) (x−µF)3dF(x)
((cid:82) (x−µF)3dF(x))3/2 = κF, skewness.

3. t(F) =
4. t(F) = F−1(1/2) = mF, median,
5. t(F) =

= σF
µF

= δF, coefﬁcient of variation,

3 Notation: if F is discrete with
probability mass function p, then

(cid:82) g(x)dF(x) = ∑ g(xi)p(xi); if
(cid:82) g(x)dF(x) =(cid:82) g(x) f (x)dx.

F is continuous with PDF f , then

So, the problem is the following: given the data X1, . . . , Xn ∼ F,
estimate a parameter of interest θ = t(F). In this context, the func-
tional t is known, but F, and therefore θ, are unknown. The basic
idea is, ﬁrst, to estimate the CDF, ˆF ≈ F, and then estimate the pa-
rameter θ by ˆθ = t( ˆF).

Estimating the CDF

We will estimate F with the empirical distribution function (eCDF)4.
Recall that the eCDF ˆFn of X1, . . . , Xn is the CDF that puts mass 1/n
at each data point Xi. More formally,
n∑

H(x − Xi),

ˆFn(x) =

(7.4)

1
n

i=1

where H(x) is the Heaviside step function. The basic properties of
the eCDF are described by the following theorem.

statistical inference

43

4 We have already encountered the
eCDF in lecture 1, in the context of
summarizing data. We saw that for the
uniform distribution, eCDF≈CDF, and
noticed that this is not a coincidence.
Here we will explain why this approx-
imation holds for any distribution and
why it is good for large n.

Theorem 4. For any ﬁxed value of x,

1.

ˆFn(x) is an unbiased estimate of F(x):

E[ ˆFn(x)] = F(x).

2. The standard error of ˆFn(x) is given by

(cid:114) F(x)(1 − F(x))

.

n

se[ ˆFn(x)] =

3. The mean squared error of ˆFn(x) goes to zero as n increases:

MSE[ ˆFn(x)] → 0,

as n → ∞.

(7.5)

(7.6)

(7.7)

An estimate ˆθn of a quantity of interest θ is said to be consistent, if

it converges to θ in probability5:
P−→ θ,

ˆθn

as n → ∞.

(7.8)

5 See the Appendix at the end of this
Lecture for a quick recap on different
types of convergence.

It turns out that if MSE[ ˆθn] → 0, then an unbiased estimate ˆθn is a
consistent estimate of θ
Theorem 5. For any x, ˆFn(x) is a consistent estimate of F(x).

6. Thus, we have:

Intuitively, this means that, for any x, if n is large enough, then
ˆFn(x) is very close to F(x) with large probability . This justiﬁes our
decision to estimate F(x) with ˆFn(x).

In fact, there are stronger results about the properties of ˆFn(x)
which make it even a more attractive estimate for F. First, as it di-
rectly follows from the strong law of large numbers, ˆFn(x) converges
to F(x) almost surely7,

6 This immediately follows from Cheby-
shev’s inequality.

7 Which is stronger than convergence in
probability. Again, see the Appendix.

44 k. m. zuev

ˆFn(x) a.s.−→ F(x),

as n → ∞.

(7.9)

The Glivenko-Cantelli theorem strengths this pointwise result by
proving the uniform convergence:

(cid:12)(cid:12) ˆFn(x) − F(x)(cid:12)(cid:12) a.s.−→ 0,

as n → ∞.

(7.10)

Finally, the Dvoretzky-Kiefer-Wolfowitz (DKW) inequality8 says that
the convergence in (7.10) is fast: for any  > 0,

(cid:12)(cid:12) ˆFn(x) − F(x)(cid:12)(cid:12) > 

(cid:33)

≤ 2e−2n2.

(7.11)

sup
x∈R

(cid:32)

P

sup
x∈R

The eCDF ˆFn(x) is a point estimate of F. The DKW inequality9

allows to construct a conﬁdence set for F. To construct a conﬁdence
set for F, we need to ﬁnd two functions Fl and Fu (construct them
from the data) such that

P(Fl(x) ≤ F(x) ≤ Fu(x) for all x) = 1 − α.

(7.12)

The DKW inequality implies that we can take

Fl(x) = max{ ˆFn(x) − n,α, 0},
Fu(x) = min{ ˆFn(x) + n,α, 1},

(7.13)

where n,α =
called a nonparametric (1 − α) conﬁdence band.

2n log 2

α . The set {y : Fl(x) ≤ y ≤ Fu(x), x ∈ R} is

(cid:113) 1

8 Which strengthens the GK theorem.

9 Notice the similarity with the Hoeffd-
ing inequality.

Plug-In Principle

The plug-in principle refers to replacing the unknown CDF F with
its empirical model ˆFn. This principle can be readily used for con-
structing the plug-in estimate of the parameter of interest θ = t(F):

For example, if the functional t has the following form:10

ˆθn = t( ˆFn).

(cid:90)

t(F) =

a(x)dF(x),

then the plug-in estimate of θ = t(F) is simply
n∑

a(x)d ˆFn(x) =

ˆθn =

(cid:90)

1
n

i=1

a(Xi).

10 Such functionals are called linear,
because t(αF + βG) = αt(F) + βt(G).

(7.14)

(7.15)

(7.16)

statistical inference

45

11 Nevertheless, it can still be used as a
benchmark.

12 This part of probability is called large
sample theory or limit theory or asymptotic
theory. It is very important for statistical
inference.

Example: The plug-in estimate for the mean is ˆµn = Xn. The plug-in
i=1(Xi − Xn)2. Note, that ˆσ2
n ∑n
n = 1
estimator for the variance is ˆσ2
i=1(Xi − Xn)2. But in
∑n
n = 1
is biased. The unbiased estimate is s2
n−1
n and s2
practice, there is little difference between ˆσ2
n.

n

(cid:3)

Note that using the plug-in principle may not be the best idea11
in situations where there is some additional information about F
other than provided by the sample X1, . . . , Xn. In such cases, a better
estimate of F may be available. For example, if F is a member of a
parametric family F = {F(x; β)}, replacing F(x) with F(x; ˆβ), where
ˆβ is an estimate of parameter β, may be better than replacing it with
ˆFn. In other words, the estimate ˆθ = t(F(x; ˆβ)) may be more accurate
than the plug-in estimate ˆθn = t( ˆFn).

Further Reading

1. A quick and nice introduction to nonparametric statistics is given

in L.A. Wasserman (2006), All of Nonparametric Statistics.

What is Next?

We learned how to estimate a parameter of interest non-parametrically
using the plug-in principle, ˆθn ≈ θ, but we saw that a plug-in esti-
mate may be biased (e.g. for the variance). In the next lecture, we will
learn how to reduce the bias using the jackknife method.

Appendix: Convergence of a Sequence of Random Variables

One of the most important questions in probability theory concerns
the behavior of sequences of random variables12. The basic question
is this: what can we say about the limiting behavior of a sequence
of random variables X1, . . . , Xn, . . .? In the statistical context, this
question can be reformulated as what happens as we gather more
and more data?

In Calculus, we say that a sequence of real numbers x1, x2, . . .

converges to a limit x if, for every  > 0, we can ﬁnd N such that
|xn − x| <  for all n > N. In Probability, convergence is more subtle.

Example: Suppose that xn = 1/n. Then trivially, limn→∞ xn =
0. Consider a probabilistic version of this example: suppose that
X1, X2, . . . are independent and Xn ∼ N (0, 1/n). Intuitively, Xn is
very concentrated around 0 for large n, and we are tempted to say
that Xn “converges” to zero. However, P(Xn = 0) = 0 for all n!

(cid:3)

46 k. m. zuev

There are several types of convergence of a sequence of random

variables. One is convergence in probability13. We say that a se-
quence {Xn} converges to a random variable X in probability, written
Xn

P−→ X, if for every  > 0,

P(|Xn − X| < ) = 1,

lim
n→∞

(7.17)

or, in more detail,

P({ω ∈ Ω : |Xn(ω) − X(ω)| < }) = 1,

(7.18)

lim
n→∞

where Ω is the sample space. Note that for every , the sequence
{P(|Xn − X| > )} is a sequence of numbers, and when we say it has
zero limit, we understand the limit in the calculus sense. Intuitively,
convergence in probability means that, when n is large, realizations
of Xn are very close to the realizations of X with high probability.
Another important type of convergence is convergence almost
surely14. We say that a sequence {Xn} converges to a random variable
X almost surely, written Xn

a.s.−→ X, if

P( lim

n→∞ Xn = X) = 1,

or, in more detail,

P({ω ∈ Ω : lim

n→∞ Xn(ω) = X(ω)) = 1.

(7.19)

(7.20)

13 This is the type of convergence used
in the weak law of large numbers: if
X1, . . . , Xn ∼ F, then Xn

P−→ µF.

14 This is the type of convergence used
in the strong law of large numbers: if
X1, . . . , Xn ∼ F, then Xn

a.s.−→ µF.

Almost sure convergence is stronger, meaning that it implies conver-
gence in probability15

15 By Fatou’s lemma.

a.s.−→ X ⇒ Xn

P−→ X.

Xn

(7.21)

There are other types of convergence, e.g. convergence in distri-

bution, convergence distribution, convergence in mean, in quadratic
mean, but we don’t need them here.

8
The Jackknife Method

The jackknife method was originally proposed by Maurice Que-
nouille1 (1949) for estimating the bias of an estimator. A bit later,
John Tukey (1956) extended the use the method by showing how to
use it for reducing the bias and estimating the variance, and coined
the name “jackknife.” As a pocket knife, this technique can be used
as a “quick and dirty” tool that can solve a variety of problems.

Estimating the Bias

So, let X1, . . . , Xn be the data which is modeled as a sample from a
distribution F, and let

ˆθn = s(X1, . . . , Xn)

(8.1)

be an estimate of a parameter of interest θ = t(F). For example, ˆθn
could be the plug-in estimate ˆθn = t( ˆFn)
are biased. Our “working” example is the plug-in estimate of the
variance.

2. In practice, many estimates

Example:

If θ = σ2

F, then the plug-in estimate is
(Xi − Xn)2,

ˆσ2
n =

1
n

n∑

i=1

and its bias3 is

B[ˆσ2

n] = − σ2
F
n

.

In general, the bias of an estimate ˆθn is

B[ ˆθn] = E[ ˆθn] − θ.

Question: How to estimate the bias?4

(8.2)

(8.3)
(cid:3)

(8.4)

1 Despite the remarkable inﬂuence of
the jackknife on the statistical com-
munity, I could not ﬁnd a photo of its
inventor on the Internet! Please let me
know if you do.

Figure 8.1: A Victorinox Swiss Army
knife. Photo source: wikipedia.org.

2 But not necessarily. It could be es-
sentially any statistic s that estimate a
quantity of interest θ.

3 For brevity, let’s denote the bias by B.

4 In stead of immediately giving you
a ready-to-use formula (like in most
textbooks), let me try to provide the
intuition behind the jackknife.

48 k. m. zuev

Well, we can estimate θ simply by ˆθn. And if we had m iid samples

from F:

n ∼ F,

X(1)
1 , . . . , X(1)
. . .
X(m)
1

, . . . , X(m)

n ∼ F,

(8.5)

we could, using the law of large numbers, estimate E[ ˆθn] by the sam-
ple mean:

E[ ˆθn] ≈ 1
m

m∑

i=1

(i)
ˆθ
n ,

(8.6)

(i)
n = s(X(i)

1 , . . . , X(i)

(i)
n = t( ˆF(i)

n ), where ˆF(i)
n

n ). In particular, if ˆθn is the plug-in esti-
is the eCDF constructed from
n . The problem is that we don’t have samples (8.5). We

where ˆθ
mator, then ˆθ
X(i)
1 , . . . , X(i)
have only one data set X1, . . . , Xn
samples of size n − 1 from the original data by leaving one data point
Xi out at a time6:

The key idea of the jackknife is to emulate (8.5) by cooking up n

5.

X2, . . . , Xn ∼ F,
. . .
X1, . . . , Xi−1, Xi+1, . . . , Xn ∼ F,
. . .
X1, . . . , Xn−1 ∼ F.

(8.7)

These samples are called the jackknife samples. Based on these sam-
ples, we compute the jackknife replications of ˆθn:

(−i)
ˆθ
n = s(X1, . . . , Xi−1, Xi+1, . . . , Xn).

(8.8)

Now, similar to (8.6), we can estimate E[ ˆθn] by the sample mean of
the jackknife replications

E[ ˆθn] ≈ ¯θ

J
n =

(−i)
ˆθ
n

.

1
n

n∑

i=1

The bias of ˆθn in (8.4) can then be estimated by

B[ ˆθn] ≈ ¯θ

J

n − ˆθn.

(8.9)

(8.10)

While intuitively this may feel correct, we have at least two con-

crete problems7:
n − 1, not n, and therefore ¯θ

a) the jackknife replications are based on the samples (8.7) of size

(−i)
b) more importantly, the jackknife replications ˆθ
n

J
n is more like an estimate of E[ ˆθn−1],
are not in-

dependent, in fact, they are very dependent since any two jackknife
samples differ only in two data points.

5 Recall that the same problem we
faced in lecture 4, where we discussed
the bootstrap method. The bootstrap
approach was to create multiple copies
of Xis to mimic the target population.
The jackknife also reuses the data, but
using a slightly different strategy.
6 That is why the jackknife is also called
a “leave one out” procedure.

7 In fact, (8.10), as it is, is wrong. To
make it a good approximation, we need
to slightly modify the right-hand side.

statistical inference

49

8 Here we go: [pdf]

9 Under the assumption that its bias
satisﬁes (6.1).
10 Careful with notation: ˆθ
different animals!

J
n and ¯θ

J
n are

(cid:18) 1

(cid:19)

n3

It turns out however, that this method will work if we make an

assumption about the bias of our estimate ˆθn: suppose that

B[ ˆθn] =

a
n

+

b
n2 + O

as n → ∞,

(8.11)

where a and b are some constants. In fact, many estimates have this
property, so this assumption is not very strong. For example, (8.11)
holds for the plug-in variance estimate (8.3) with a = −σ2

F and b = 0.

It is straightforward to show8 that under (8.11),

E[ ¯θ

J

n − ˆθn] =

a

n(n − 1)

+

(2n − 1)b
n2(n − 1)2 + O

as n → ∞.

(8.12)

(cid:18) 1

(cid:19)

n3

This means that we need just to properly rescale ¯θ
estimate of the bias of ˆθn. The jackknife estimate of the bias is

J

n − ˆθn to get an

B[ ˆθn] ≈ ˆBJ[ ˆθn] = (n − 1)( ¯θ

J

n − ˆθn).

It estimates the bias up to order O(n−2):

(cid:18) 1

(cid:19)

n2

(cid:18) 1

(cid:19)

n2

.

= B[ ˆθn] + O

E[ ˆBJ[ ˆθn]] =

a
n

+ O

Reducing the Bias

(8.13)

(8.14)

It is now clear how to reduce the bias of the estimate ˆθn
need to subtract from ˆθn its estimated bias10:

9: we just

J

n = ˆθn − ˆBJ[ ˆθn] = n ˆθn − (n − 1) ¯θ
J
ˆθ
n.

Using (8.14), we have:

B[ ˆθ

J

n] = E[ ˆθn] − E[ ˆBJ[ ˆθn]] − θ = O

(cid:18) 1

(cid:19)

n2

.

(8.15)

(8.16)

J
n is therefore an order magnitude smaller than that of

The bias of ˆθ
ˆθn.The jackknifed estimate (8.15) is also called the bias-corrected esti-
mate. An important remark: if the original estimate ˆθn is unbiased,
then so is the jackknifed estimate:
E[ ˆθn] = θ ⇒ E[ ˆθ

n] = nθ − (n − 1)θ = θ.

J

(8.17)

Example:
plug-in estimate of the variance ˆσ2
estimate:

It can be shown that the bias-corrected estimate of the
n is simply the usual unbiased

2,J
n = s2
ˆσ

n =

1
n − 1

(Xi − Xn)2.

n∑

i=1

(8.18)

(cid:3)

50 k. m. zuev

Let us look at the deﬁnition of the bias-corrected estimate (8.15).
It is a linear combination of the original estimate and the mean of
its jackknife replications. There is another way to think about the
jackknife.

Pseudo-Values

A straightforward manipulation with (8.15) leads to

J
ˆθ
n =

1
n

n∑

i=1

(i)
˜θ
n ,

where

(i)

n = n ˆθn − (n − 1) ˆθ
˜θ

(−i)
n

(8.19)

(8.20)

are called pseudo-values. The idea behind pseudo-values is that they
allow us to write the bias-corrected estimate as a mean of n “inde-
pendent” data values11. Let us consider a couple of examples.

Example:
ˆθn = Xn, then the pseudo-values are simply Xi:

If θ = µF and the plug-in estimate is the sample mean

(i)
˜θ
n =

n∑

i=1

Xi − n∑
j(cid:54)=i

Xj = Xi.

(8.21)
(cid:3)

11 Expect that in general, pseudo-values
are not independent.

(cid:82) a(x)dF(x), the plug-in estimate is ˆθn = s(X1, . . . , Xn) = 1

In a more general case of a linear functional t(F) =
n ∑n

Example:

i=1 a(Xi).

The pseudo-values are then
n∑

(i)
˜θ
n =

a(Xi) − n∑
j(cid:54)=i

i=1

a(Xj) = a(Xi).

(8.22)

This means, in particular, that for linear functionals, the jackknifed
estimated coincides with the plug-in estimate, ˆθ

12.

J
n = ˆθn

(cid:3)

In both examples, the pseudo-values are indeed independent.

Based on this, Tukey suggested that in general case, we can treat the
pseudo-values ˜θ

(i)
n as liner approximations to iid observations:

12 This is expected, since the plug-
in estimate ˆθn = 1
n ∑n
i=1 a(Xi) is an
unbiased estimate of θ.

if

ˆθn = s(X1, . . . , Xn) ≈ 1
n

n∑

i=1

Estimating the Variance

a(Xi) ⇒ ˜θ

(i)

n ≈ a(Xi).

(8.23)

Following Tukey’s idea of treating the pseudo-values as iid random
variables allows to estimate the variance of the bias-corrected esti-
mate ˆθ

(i)
n are iid, then from (8.19), we have:

J
n. Indeed, if ˜θ

V[ ˆθ

J
n] =

(i)
V[ ˜θ
n ]
n

≈ ˜s2
n
n

=: vJ,

(8.24)

where ˜s2

n is the sample variance of the pseudo-values,

˜s2
n =

1
n − 1

n∑

i=1

(i)

n − ˆθ
( ˜θ

J
n)2.

statistical inference

51

(8.25)

It turns out that under suitable conditions on statistic s, vJ consis-
tently estimates the variance of the original estimate ˆθn = s(X1, . . . , Xn),

P−→ V[ ˆθn].

vJ

(8.26)
However, there are cases where vJ is not a good estimate for the vari-
ance of an estimate. This happens when ˆθn is not a smooth function
of the data X1, . . . , Xn. For example, if ˆθn is the plug-in estimate for
the median, vJ is a poor estimate for its variance.

Further Reading

1. A brief description of the jackknife together with a summary of
its underlying theory, advantages, disadvantages, and its general
properties is given in Bissell & Fergusun (1975) “The Jackknife —
Toy, Tool or Two-edged Weapon?” The Statistician, 24(2): 79-100.

What is Next?

We learned how to estimate a parameter of interest non-parametrically
using the plug-in principle, ˆθn ≈ θ, how to reduce its bias and even
estimate its variance using the jackknife. On the other hand, we saw
that the jackknife works only under appropriate assumptions13. In
the next lecture, we will discuss the bootstrap method, which was in-
spired by the jackknife, and which is a superior technique and can be
used pretty much anywhere jackkniﬁng can be used. In some sense
the jackknife can be viewed as a linear approximation of the boot-
strap. We will learn how to estimate the standard error of an estimate
ˆθn and how to construct conﬁdence intervals for the parameter of
interest θ using the bootstrap.

13 We often hold in practice, but rarely
veriﬁable.

9
The Bootstrap Method

As before, let X1, . . . , Xn be data which we model nonparametri-
cally as a sample from a distribution F ∈ F, where the statistical
model F = {all CDFs}. Let ˆθn = s(X1, . . . , Xn) be an estimate of a
parameter of interest θ = t(F) calculated from the data (e.g. using the
plug-in principle), where t is a given functional. In this lecture, our
focus is on the following

Question: How accurate is ˆθn? What is its standard error? How to

construct a conﬁdence interval for θ?

These questions can be answered by using the bootstrap method1.

The bootstrap, introduced by Bradley Efron2, is a simulation-based
method for measuring uncertainty of an estimate, in particular, for
estimating standard errors and constructing conﬁdence intervals. Its
beauty lies in its simplicity and universality: the bootstrap is fully
automatic, requires no theoretical calculations, and always available.

Bootstrapping the Standard Error
If ˆθn = s(X1, . . . , Xn) is unbiased3, then the most common way to
assess its statistical accuracy is to compute the standard error of ˆθn:

seF[ ˆθn] =(cid:0)VF[ ˆθn](cid:1)1/2 ,

(cid:90)
(cid:90)

· · ·
· · ·

VF[ ˆθn] =

EF[ ˆθn] =

(cid:90) (cid:0)s(x1, . . . , xn) − EF[ ˆθn](cid:1)2 dF(x1) . . . dF(xn),
(cid:90)

s(x1, . . . , xn)dF(x1) . . . dF(xn).

(9.1)

We intentionally use the subscript F to emphasize that the standard
error, variance, and mean of ˆθn do depend on F, which is unknown.

Question: How to estimate seF[ ˆθn]?
Bootstrap: The ideal bootstrap estimate of seF[ ˆθn] is a plug-in esti-

mate that uses ˆFn in place of F:

(cid:98)seideal

B

[ ˆθn] := se ˆFn

[ ˆθn].

(9.2)

1 We already encountered the bootstrap
in Lecture 4 in the context of survey
sampling. Here we will discuss this
general method in detail.
2 Who was inspired by the success of
the jackknife, B. Efron (1979) “Bootstrap
methods: another look at the jackknife,”
The Annals of Statistics, 7: 1-26.

3 Recall that ˆθ is unbiased if E[ ˆθn] = θ.
Plug-in estimates ˆθ = t( ˆFn) are not
necessarily unbiased (what if the
corresponding functional is linear?),
but they tend to have small biases
compared to the magnitude of their
standard errors.

statistical inference

53

Example: Let the parameter of interest θ = t(F) be the mean µF,
and the estimate ˆθn be the plug-in estimate, ˆθn = t( ˆFn) = Xn. The
standard error is then
n∑

(cid:35)(cid:33)1/2

(cid:33)1/2

(cid:32)

(cid:32)

n∑

(cid:34)

seF[ ˆθn] =

VF [Xi]

VF

Xi

=

=

.

σF√
n

(9.3)

1
n2

i=1

The ideal bootstrap estimate is therefore
i=1(Xi − Xn)2
√

(cid:98)seF[ ˆθn] =

n ∑n

=

=

σ ˆFn√
n

n

(cid:32) n∑

i=1

1
n

(cid:33)1/2

(Xi − Xn)2

(9.4)

(cid:3)

i=1

1
n

(cid:113) 1

This example is very special because essentially only for ˆθn = Xn
explicit calculations in (9.3) are possible.4 Usually the ideal bootstrap
estimate (9.2) cannot be computed exactly like in (9.4) and some
approximations are needed.

4 Convince yourself by considering
other four functionals discussed in
Lecture 7.

Monte Carlo Simulation

We can readily compute the approximate numeric value of the boot-
[ ˆθn] by Monte Carlo simulation (i.e. using the law
strap estimate se ˆFn
of large numbers):
1. For b = 1, . . . , B, generate a bootstrap sample X(b)

n ∼ ˆFn
5
1 , . . . , X(b)
n ).
[ ˆθn] by the sample standard deviation of the B repli-

and compute the bootstrap replication of ˆθn, ˆθ

1 , . . . , X(b)

(b)
n = s(X(b)

2. Estimate se ˆFn

(cid:32)

cations:

 1

(cid:33)21/2
By the law of large numbers, when B is large, (cid:98)seB[ ˆθn] ≈ se ˆFn

(cid:98)seB[ ˆθn] =

n − 1
ˆθ
B

(b)
ˆθ
n

B∑

B∑

b=1

b=1

(b)

B

.

Schematic Illustration of Nonparametric Bootstrap

5 How to sample form ˆFn? If X1, . . . , Xn
are all distinct, how many distinct
bootstrap samples?

(9.5)

[ ˆθn]

6.

6 The ideal bootstrap estimate se ˆFn
and its Monte Carlo approximation

(cid:98)seB are called nonparametric bootstrap

[ ˆθn]

estimates.

Efron called this method “bootstrap” since using data to estimate

the uncertainty of an estimate computed from these same data is akin
to the Baron Munchausen’s method for getting himself out of a bog

Figure 9.1: Bootstrap at work. Image
source: [jpg].

𝑋𝑋1,…,𝑋𝑋𝑛𝑛𝜃𝜃=𝑡𝑡𝐹𝐹≈𝐹𝐹𝑡𝑡�𝐹𝐹𝑛𝑛parameter of interestdatâ𝜃𝜃𝑛𝑛=𝑡𝑡�𝐹𝐹𝑛𝑛=𝑠𝑠(𝑋𝑋1,…,𝑋𝑋𝑛𝑛)plug-ineCDF(𝑋𝑋1(1),…,𝑋𝑋𝑛𝑛(1)), …, (𝑋𝑋1(𝐵𝐵),…,𝑋𝑋𝑛𝑛(𝐵𝐵))bootstrap samplesGK, DKW𝑠𝑠̂𝜃𝜃𝑛𝑛1,…,̂𝜃𝜃𝑛𝑛𝐵𝐵bootstrap replicationsMonte Carlo𝑠𝑠𝑒𝑒𝐹𝐹[̂𝜃𝜃𝑛𝑛]≈𝑠𝑠𝑒𝑒�𝐹𝐹𝑛𝑛[̂𝜃𝜃𝑛𝑛]≈�𝑠𝑠𝑒𝑒𝐵𝐵plug-inLLN⇒plug-in estimatemeasure of uncertaintybootsrappedstandard error  54 k. m. zuev

by lifting himself by his bootstraps7. It is worth mentioning that in
his original paper, Efron was considering even more colorful names
such as “Swiss Army Knife” and “Shotgun.”

Errors

There are two sources of approximation error in bootstrap:

• Statistical error: the empirical distribution ˆFn is not exactly the

same as the true data-generating process F. But they get closer and
closer as we have more data (as n increases).

• Simulation error: occurs from using ﬁnitely many bootstrap replica-
(B)
n . It can be made arbitrary small simply by brute

(1)
n , . . . , ˆθ

tions ˆθ
force: take B very large.

Usually we have more control over the simulation error (it is up to
us what B to use) than the statistical error (typically data X1, . . . , Xn
are given and we cannot8 collect more). In complex models, however,
statistic s may be a complicated function of data, and its computation
may be time-consuming. It is essential then to reduce the number of
s-evaluations and the following question becomes relevant:

Question: How many replications B should we use?
This is thoroughly discussed by Efron and Tibshirani9. In Chapter

19, the formula for the coefﬁcient of variation of (cid:98)seB is derived which

leads to the following rule of thumb: it is very rare when more than
B = 200 replications are needed for estimating a standard error.10
A take-home message: the statistical error is larger than the sim-

ulation error, provided that the Monte Carlo sampling is properly
designed.

Bootstrap Conﬁdence Intervals

Recall that a conﬁdence interval for a parameter θ = t(F) is random
interval I calculated from the sample X1, . . . , Xn that contains θ with
high probability (conﬁdence level11) 1 − α,
P(θ ∈ I) = 1 − α.

(9.6)

A point and interval estimates of θ provide the guess for θ and how
far in error that guess might reasonably be.

Let ˆθn = t( ˆFn) be the plug-in estimate and (cid:98)seB be the bootstrap

estimate of its standard error. There are several ways to construct
bootstrap conﬁdence intervals. The simplest and most straightfor-
ward is the normal interval.12

7 In the original version of this tale,
Baron lifted himself and his horse by
pulling his own hair (Fig. 9.1).

8 Or it is very expensive.

9 B. Efron & R.J. Tibshirani (1993), An
Introduction to the Bootstrap.

10 For bootstrap conﬁdence intervals
much bigger values are required.

11 Conﬁdence level is also called coverage
probability.

12 Sometimes it is called standard conﬁ-
dence interval.

statistical inference

55

13 This large-sample result is often true
for general statistics ˆθ = s(X1, . . . , Xn),
not necessarily for plug-in estimates.

14 Recall, that zα = Φ−1(α), where Φ is
the standard normal CDF. Obvious, yet
useful property: z1−α = −zα.

15 A pivot is a random variable ζ(X, θ)
that depends on the sample X ∼ F
and the unknown parameter θ = t(F),
but whose distribution does not depend
on θ. The classical example is the so-
called z-score: if X ∼ N (µ, σ2), then
Z = X−µ
σ ∼ N (0, 1). It can be shown
that ˜θn = ˆθn − θ is an approximate
pivot under weak conditions on t(F):
the distribution of ˜θn (not necessarily
Gaussian) is approximately the same
for each value of θ.

Normal Interval

Suppose that the parameter of interest is the mean θ = µF. The
plug-in estimate of θ is then ˆθn = Xn. Thanks to the central limit
theorem, if the sample size n is large enough, the distribution of
the sample mean Xn is approximately normal with mean µF and
variance σ2

B. That is Xn ·∼ N (µF,(cid:98)se2

F[Xn] ≈ (cid:98)se2

n = se2

B).

F

It turns out that, for many reasonable distributions F and function-

als t, the distribution of ˆθn = t( ˆFn) is also approximately normal13,

ˆθn ·∼ N (θ,(cid:98)se2

B), or equivalently

·∼ N (0, 1).

(9.7)

Let zα denote the αth quantile of the standard normal distribution14.
Then (9.7) results into

ˆθn − θ(cid:98)seB
(cid:33)

(cid:32)

P

zα/2 ≤ ˆθn − θ(cid:98)seB

Therefore,

≤ z1−α/2

≈ Φ(z1−α/2) − Φ(zα/2) = 1 − α.

(9.8)

I = ˆθn ± zα/2(cid:98)seB.

(9.9)
is an approximate conﬁdence interval for θ at level 1 − α. This interval
is accurate only under assumption (9.7) that the distribution of ˆθn is
close to normal.

Question: Can we construct accurate intervals without making

normal theory assumptions like (9.7)? The answer is “yes.”

Pivotal Interval
Deﬁne the approximate pivot15

(9.10)
and let G be its CDF. We want to ﬁnd an interval I = (a, b), such that
P(a ≤ θ ≤ b) = 1 − α. Let us rewrite this probability in terms of G:

˜θn = ˆθn − θ,

P(a ≤ θ ≤ b) =P(a − ˆθn ≤ θ − ˆθn ≤ b − ˆθn)

(9.11)

=P( ˆθn − b ≤ ˜θn ≤ ˆθn − a)
=G( ˆθn − a) − G( ˆθn − b).
(cid:17)

Therefore, we can achieve the conﬁdence level 1 − α, by setting

a = ˆθn − G−1(cid:16)
The problem is that G, and thus its quantiles ξ1−α/2 = G−1(cid:0)1 − α
and ξα/2 = G−1(cid:0) α

and b = ˆθn − G−1(cid:16) α

(cid:1), are unknown.

1 − α
2

(cid:17)

2

2

.

(cid:1)

2

(9.12)

56 k. m. zuev

But we can estimate G using the bootstrap!

G(ξ) ≈ ˆGB(ξ) =

1
B

B∑

b=1

H

ξ − ˜θ
(b)
n

(cid:16)

(cid:17)

,

(9.13)

where

(b)
and ˆθ
n
ple quantile of ˜θ

(9.14)
is the bootstrap replication of ˆθn. Let ˜ξα denote the αth sam-

(b)
˜θ
n = ˆθ

(b)

n − ˆθn,

(1)
n , . . . , ˜θ

(B)
n ,
˜ξα = inf{ξ : ˆGB(ξ) ≥ α}.

Note that ˜ξα = ˆξα − ˆθn, where ˆξα is the αth sample quantile of
(B)
(1)
ˆθ
n , . . . , ˆθ
n . Therefore, the end points a and b of the conﬁdence
interval can be approximated as follows:

a ≈ ˆaB = ˆθn − ˜ξ1−α/2 = 2 ˆθn − ˆξ1−α/2,
b ≈ ˆbB = ˆθn − ˜ξα/2 = 2 ˆθn − ˆξα/2.

(9.15)

(9.16)

Under weak conditions on F and t, P(ˆaB ≤ θ ≤ ˆbB) → 1 − α as
n, B → ∞. The interval I = (ˆaB, ˆbB) is thus an approximate 1 − α
conﬁdence interval.

There are other ways to construct the bootstrap conﬁdence inter-
vals, e.g. the percentile interval16, studentized interval17, BCa interval18,
with many variations. Typically there is a trade-off between accuracy
of an interval and the amount of work needed for its construction.
Here we described only two basic intervals with the main goal to il-
lustrate the idea of bootstrap. For more advanced methods, see the
textbooks listed in section “Further Reading” below.

Example: Enrollment in the U.S. Universities

16 This interval is intuitive, but does
not have theoretical support. For
“semi-theoretical” justiﬁcation, see,
for example, Chapter 13 in B. Efron &
R.J. Tibshirani (1993), An Introduction to
the Bootstrap.
17 Also called “bootstrap-t.”
18 BCa stands for “bias-corrected and
accelerated.”

Figure 9.2 shows N = 354 data points, each corresponding to a large
university in the U.S. The x and y coordinates of each point are the
enrollment sizes of the corresponding university in 2000 and 2011.
Only universities with 2011 enrollment ≥ 15, 000 are considered.
Let the parameter of interest θ be the ratio of the means ¯y/ ¯x,

which is a good proxy19 for the total increase in university enroll-
ment in the country from 2000 to 2011. The distribution F in this case
is the bivariate CDF that puts probability 1/N at each of the data
points (xi, yi), and

(cid:82) ydF(x, y)
(cid:82) xdF(x, y)

=

¯y
¯x

.

θ = t(F) =

Given the data in Fig. 9.2, θ can be computed exactly20:

θ = 1.41.

(9.17)

(9.18)

Figure 9.2: Enrollments (in thousands)
of N = 354 large degree-granting
institutions. Data: U.S. Department of
Education. Available at enrollment.xlsx.
An “outlier” — University of Phoenix
(14.8, 307.9) — is not shown in Fig. 9.2.
19 We ignore small universities.
20 We will use this true value is a bench-
mark.

0102030405060010203040506070802000 Enrollment2011 Enrollment  Datay=xSuppose now that we don’t have access to the full data, and we
can only pick n < N universities at random, and record their en-
rollment sizes (X1, Y1), . . . , (Xn, Yn). Figure 9.3 shows the random
samples of sizes n = 10 and n = 100.

statistical inference

57

Figure 9.3: Random samples of size
n = 10 (left) and n = 100 (right).
Out goal is to estimate θ, compute the
standard error of the estimate, and
construct conﬁdence intervals based on
these samples.

Since there is no obvious parametric model for the joint distribu-

tion F, it is natural to stick to nonparametric estimation. The bivariate
eCDF ˆFn puts probability 1/n at each sampled pair (Xi, Yi). The
plug-in estimate of θ is therefore

ˆθn = t( ˆFn) =

Yn
Xn

.

(9.19)

The estimates computed from the samples depicted in Fig. 9.3 are:

ˆθ10 = 1.61 and ˆθ100 = 1.39.

Let us compute the bootstrap estimates (cid:98)seB[ ˆθn] of the standard
n ) ∼ ˆFn.21
n /X(b)
n . The

errors seF[ ˆθn]. To make the simulation error completely negligible, we
use B = 104 bootstrap samples (X(b)
The corresponding bootstrap replications are ˆθ
bootstrap estimates obtain from (9.5) are:22

), . . . , (X(b)

n , Y(b)
n = Y(b)
(b)

1 , Y(b)
1

(9.20)

(cid:98)seB( ˆθ10) = 0.14 and (cid:98)seB( ˆθ100) = 0.06.

(9.21)

Figure 9.4 shows the normal and pivotal conﬁdence intervals for

θ constructed from the samples in Fig. 9.3. As expected, intervals
constructed from the small sample (n = 10) are longer. Note also that
while the normal intervals are (by deﬁnition) symmetric about ˆθn, the
pivotal intervals are not.

It is important to highlight that plug-in estimates (9.20), boot-
strap estimates of their standard errors (9.21), and conﬁdence in-
tervals in Fig. 9.4 are computed based on the two speciﬁc samples

21 We can easily afford this large B since
computing the statistic (9.19) is very
fast.
22 Recall now that we actually know F in
this example. How would you compute
the true standard errors seF[ ˆθn]?

010203040506001020304050607080n=102000 Enrollment2011 Enrollment  010203040506001020304050607080n=1002000 Enrollment2011 Enrollment  Data (unknown)y=xSample (known)Data (unknown)y=xSample (known)58 k. m. zuev

Figure 9.4: The normal (blue) and
pivotal (red) conﬁdence intervals at
level 0.95. The true value of θ (9.18)
is shown by the dashed line. The
estimates ˆθn are marked by green circles
(n = 100) and squares (n = 10).

(X1, Y1), . . . , (Xn, Yn) shown in Fig. 9.3. The results of course will
change if we get another samples. It is interesting to see the variabil-
ity of the estimates. Let us repeat all computations for 200 indepen-
dent samples from the total population of N universities: 100 samples
of size n = 10 and 100 samples of size n = 100. Figures 9.5 and 9.6
show the simulation results.

Figure 9.5: Top panel illustrates the
variability in the values of the plug-in
estimates ˆθn. As expected, on average,
the estimate for n = 100 (red curve) is
more accurate. The bottom panel shows
the variability of the bootstrap estimates

(cid:98)seB[ ˆθn] for n = 10 (left) and n = 100

(right). Green dashed lines represent
the true values of seF[ ˆθ] computed
using F (i.e. using full data).

Figure 9.6: Here we show approximate
0.95 conﬁdence intervals for θ. Four
intervals in Fig. 9.4 are ones of these.
For n = 10, only 71 out of 100 normal
intervals (top left) and 69 pivotal
intervals (bottom left) contain the true
value θ = 1.41. This means that the
approximation is poor, since we expect
about 95 out of 100 intervals to contain
θ. For n = 100, the intervals are more
accurate: 83 normal (top right) and 86
pivotal (bottom right) contain θ.

1.31.41.51.61.71.81.92(1−α) confidence intervals, α=0.05 Pivotal intervalsθn=100n=100n=10n=10Normal intervalsTrue010203040506070809010011.522.53Plug−in estimates   True θn=10n=10002040608010000.511.52n=10Bootstrap estimates of se  02040608010000.050.10.150.2n=100Bootstrap estimates of se0123456020406080100n=10Normal intervals−20246020406080100n=10Pivotal intervals11.21.41.61.82020406080100n=100Normal intervals11.21.41.61.82020406080100n=100Pivotal intervalsstatistical inference

59

Further Readings

1. The original bootstrap paper B. Efron (1979) “Bootstrap methods:

another look at the jackknife,” The Annals of Statistics, 7: 1-26. is
classical. It is very readable and highly recommended.

2. A clear textbook-length treatment of the bootstrap is by Efron

and his former PhD student Tibshirani: B. Efron & R.J. Tibshirani
(1993) An Introduction to the Bootstrap. It focuses more on theory.

3. Another good textbook that focuses more on applications is

A.C. Davison & D.V. Hinkley (1997) Bootstrap Methods and their
Applications.

4. If you encounter a serious application, the review A.J. Canty et al

(2006) “Bootstrap diagnostics and remedies,” Canadian Journal of
Statistics, 34:5-27 might be useful. They describe typical problems
with bootstrap, provide workable diagnostics, and discuss efﬁcient
ways to implement the necessary computations.

5. For a more conceptual and somewhat philosophical discussion

of the bootstrap, see the beautiful essay by C. Shalizi (2010) “The
bootstrap,” American Scientist, 98: 186-190. For a much more com-
plete list of references on the bootstrap, go to [web].

What is Next?

Nonparametric inference is made under minimal assumptions on
the underlaying statistical model for the data in hand. In general, the
more assumptions we make, the more powerful methods are avail-
able for data analysis, and, as a result, the more we can infer from
the data23. In the next Lecture, we will start discussing the parametric
inference, which makes stronger assumptions about the data.

23 If the assumptions are correct!

10
The Method of Moments

Let us turn our attention to parametric inference, where the data
X1, . . . , Xn is modeled as a random sample from a ﬁnitely parametrized
distribution:

X1 . . . , Xn ∼ f ,

f ∈ F = { f (x; θ) : θ ∈ Θ},

(10.1)

where f is a probability density function (PDF)1, θ is the model pa-
rameter, and Θ is the parameter space. In general Θ ⊂ Rp and
θ = (θ1, . . . , θp) is a vector of parameters. Recall that whenever
p = dim Θ < ∞, we call the corresponding model F in (10.1) a para-
metric model. In this framework, the problem of inference reduces to
estimating θ from the data. But before we start talking about different
parametric methods, let us discuss the following conceptual question:
How could we possibly know that X1, . . . , Xn ∼ f (x; θ) ?
In other words, how would we ever know that there exists a family of
distribution F = { f (x; θ)} and a speciﬁc value of the parameter θ
such that our data X1 . . . , Xn is generated exactly from f (x; θ).

First of all, when we assume the parametric model (10.1), we don’t
really believe that the data is exactly generated from f (x; θ) for some
value of θ. Rather, we believe that there exists some value of θ in Θ
such that the distribution f (x; θ) does well (for all practical purposes)
in describing the randomness in the data. That is

2

1 Or, a probability mass function (PMF)
if the data is discrete.

2 This value is often called the “true”
value of the parameter.

X1, . . . , Xn ·∼ f (x; θ).

(10.2)

Ok, but still, how do we know what parametric model to chose?
Indeed, in many applications we would not have such knowledge3.
But there are cases where background knowledge and prior experi-
ence suggest that a certain parametric model provides a reasonable
approximation4.

3 In such cases, nonparametric inference
is preferable.

4 For example, it is known that counts
of trafﬁc accidents approximately
follow a Poisson model.

Finally, whenever we assume a parametric model (10.1), we can
always check this assumption. One possibility is to check (10.1) in-
formally by inspecting plots of the data5. A formal way to check a
parametric model is to use some formal test, for instance permutation
test, which will consider later in Lecture 16.

Parameters in Parametric Models

Recall that in the nonparametric setup, a parameter θ of a distribu-
tion F is the value of a certain (known) functional t on F, θ = t(F).
In the parametric setup (10.1), a parameter of interest can be θ, or a
component of θ, or, more generally6, any function of θ.

statistical inference

61

5 For example, if a histogram of the data
looks bimodal, then the assumption of
normality F = {N (µ, σ2)} would be at
least questionable.

6 And quite often in applications.

Example: Suppose our data is X1, . . . , Xn, where Xi is the outcome
of a blood test of subject i. And suppose we are interested in the
fraction τ of the entire population whose test score is larger than
a certain threshold α. Since many measurements taken on humans
approximately follow normal distribution, it is reasonable to model
the data as a sample X1, . . . , Xn ∼ N (µ, σ2). The parameter of interest
is then

(cid:18) α − µ
(cid:19)
(cid:17)
which can be estimated by ˆτ = 1 − Φ(cid:16) α− ˆµ

, where ˆµ and ˆσ are the

τ = 1 − Φ

(10.3)

,

σ

ˆσ

estimates of µ and σ obtained from the data7.

(cid:3)

The Method of Moments

The ﬁrst method for constructing parametric estimates that we will
consider is called the method of moments (MOM). MOM is perhaps
the oldest general method for ﬁnding point estimates, dating back
at least to Karl Pearson in the late 1800s. Its main advantage is that
MOM estimates are usually easy to compute for “standard” mod-
els8. The main drawback is that they often not optimal and better
estimates exist9. Nevertheless, MOM estimates are often useful as
starting values for other methods that require iterative numerical
routines.

So, suppose that we model the data parametrically

X1, . . . , Xn ∼ f (x; θ),

(10.4)

and that dim θ = k, that is θ = (θ1, . . . , θk). Recall that the qth moment
of a distribution f is the expected value E f [Xq]. Since, f depends on
θ, so do the moments10:

(cid:90)

mq(θ) = E f [Xq] =

xq f (x; θ)dx.

(10.5)

7 If we ignore the normal model for the
data, we can estimate τ simply by the
fraction of sample whose score is large
than α, ˇτ = |{Xi : Xi > α}|/n.

8 Normal, Bernoulli, Poisson, etc.
9 For example, maximum likelihood
estimates, which we will discuss in the
next Lecture.

10 If the data is discrete, then f is a
probability mas function and the
integral in (10.5) is replaced with the
sum.

11 Note that the sample moment ˆmq is
simply the plug-in estimate of the theo-

retical moment mq: ˆmq =(cid:82) xqd ˆFn(x).

62 k. m. zuev

Can we estimate these “theoretical” moments from the data? Yes, of
course. Let is deﬁne the qth sample moment as follows:

ˆmq =

1
n

n∑

i=1

Xq
i .

(10.6)

By the law of large numbers, mq(θ) ≈ ˆmq when n is large11. The
method of moments exploits this approximation. The MOM estimate
ˆθMOM of θ is the solution of the following system of k equations with
k unknowns:

mq(θ) = ˆmq,

q = 1, . . . , k.

Let us consider a couple of classical examples.

Example: Let X1, . . . , Xn ∼ Bernoulli(p), then ˆpMOM = Xn.
Example: Let X1, . . . , Xn ∼ N (µ, σ2), then

ˆµMOM = Xn

and

ˆσ2
MOM =

1
n

n∑

i=1

(Xi − Xn)2.

(10.7)

(cid:3)

(10.8)

(cid:3)

These examples show that, at least in these two speciﬁc cases,

MOM produces very reasonable estimates, which, in fact, coincide
with the corresponding plug-in estimates. This leads to a natural
question: are there examples where the MOM and plug-in estimates
are different? As expected, the answer is yes.

Example: Let X1, . . . , Xn ∼ U[0, θ]. Recall that the plug-in estimate
is ˆθn = X(n). The MOM estimate is ˆθMOM = 2Xn. Note that this esti-
mate, although unbiased, can give impossible results. For example, if
n = 3, X1 = X2 = 1 and X3 = 7, then ˆθMOM = 6, which makes X3 = 7
(cid:3)
impossible.

This example serves as a “red ﬂag:” we should be careful when

using MOM, it may give unreasonable estimates. On the other hand,
MOM estimates have a nice property.

Consistency of MOM Estimates
Let X1, . . . , Xn ∼ f (x; θ) and let ˆθn denote the MOM estimate. Under
certain regularity conditions on the model f , ˆθn is consistent:

P−→ θ.

ˆθn

(10.9)

A couple of remarks are in order:

statistical inference

63

12 Indeed, if we can’t get the true value
of the parameter even with inﬁnite data,
the estimator we are using is rubbish!
13 See, e. g. [CH, Proposition 6.6, p. 158].

14 If it is not, use MLE (Lecture 11)!

a) Consistency says roughly that ˆθn gives the right answer in the

long run, as the sample size n goes to inﬁnity. This is a very basic
test for a quality of an estimate: it is rare to use an estimate which
is not consistent12. Note that if E[ ˆθn] → θ and V[ ˆθn] → 0, then
ˆθn is consistent13. For all considered examples, this helps to check
the consistency of the corresponding MOM estimates. The general
proof of consistency is beyond our scope.

b) Finally, it us useful to keep in mind a scenario when MOM can
be preferable to other approaches. Namely: the chosen statistical
model F = { f (x; θ)} is in doubt14 and one wishes to make sure F
accurately ﬁts certain aspects of the data that can be expressed in
term of moments.

Further Reading

1. In complicated statistical models, theoretical moments mq(θ) are
generally expressed as intractable integrals, in which case match-
ing theoretical and sample moments requires solving a system of
integral equations. A. Gelman (1995) “Method of moments using
Monte Carlo simulation,” J. Comp and Graph. Statistics, 4(1), 36-54
presents a computation approach to MOM that efﬁciently resolves
this technical problem.

What is Next?

We will discuss one of the most fundamental and iconic methods of
parametric inference: maximum likelihood estimation.

11
Maximum Likelihood: Intuition, Deﬁnitions, Examples

Maximum likelihood is the most popular method for estimating
parameters in parametric models. It was strongly recommended by
Ronald Fisher, one of the greatest statisticians of all times. The max-
imum likelihood estimates (MLEs) are known to be very powerful,
especially with large samples. We start from describing the intuition
behind this method, then deﬁne the likelihood function and MLE,
and consider several classical examples.

Intuition

Suppose I tell you I have 100 cookies in my backpack. The cookies
are of two types: chocolate chip cookies and fortune cookies. More-
over, I tell you that the number of fortune cookies is either 10 or 90.
You draw a cookie out of my backpack at random and see that it is a
fortune cookie. Based on this “data,” what is more likely: there are

a) 10 fortune cookies and 90 chocolate chip cookies, or

b) 90 fortune cookies and 10 chocolate chip cookies?

Based solely on one sample (fortune cookie), b) is more likely.

This is exactly the idea behind maximum likelihood estimation.
The method asks: what value of the a parameter is most consistent
with the data. In other words, what value of a parameter makes the data
most likely?

Likelihood Function and MLE

Let us consider the discrete and continuous cases separately. The
discrete case is somewhat more intuitive, but at the end, we will see
that there is no much difference between the two cases.

Figure 11.1: Sir Ronald Fisher. Photo
source: wikipedia.org.

Figure 11.2: A chocolate chip cookie
(left) was invented in 1938 in Mas-
sachusetts and a fortune cookie (right).
The exact origin of fortune cookies is
unclear, though various immigrant
groups in California claim to have
popularized them in the early 20th
century. Photo source: wikipedia.org
and hufﬁngtonpost.com.

Discrete Models

Let X1 . . . , Xn be data modeled as a sample from a discrete distribu-
tion with the probability mass function (PMF) f (x; θ). What is the
probability of observing the data? Given the value of the parameter,
we can write it as follows1:

P(X1, . . . , Xn|θ) =

P(Xi|θ) =

n∏

i=1

n∏

i=1

f (Xi; θ).

(11.1)

The likelihood function is the joint probability of the data viewed as a
function of the parameter θ:

statistical inference

65

1 We are using independence of Xi in
the ﬁrst equality in (11.1).

L(θ|X1, . . . , Xn) =

f (Xi; θ).

(11.2)

n∏

i=1

In spite the fact that the likelihood L(θ|X1, . . . , Xn) is expressed in
therms of f (Xi; θ), the two functions are conceptually different.
When we consider the probability mass function f (x; θ), we con-
sider x to be variable and the parameter θ is ﬁxed. When we consider
the likelihood L(θ|X1, . . . , Xn), we consider the data to be ﬁxed (ob-
served) and θ to be variable: L(θ1|X1, . . . , Xn) is the probability of
observing the data if θ = θ1, L(θ2|X1, . . . , Xn) is the probability of
observing the data if θ = θ2, etc. Often, the likelihood function is
denoted simply by Ln(θ). This notation emphasizes the fact that
likelihood is a function of a parameter.

2 Assuming the model is correct!

The maximum likelihood method looks for the value of θ that
makes the data as likely as possible2. Technically, that means looking
for θ which maximizes the likelihood Ln(θ) ≡ Ln(θ|X1, . . . , Xn).
Thus, a maximum likelihood estimate (MLE) is a value ˆθMLE such that

Ln( ˆθMLE) ≥ Ln(θ)

for all θ ∈ Θ,

or, equivalently,

ˆθMLE = arg max
θ∈Θ

Ln(θ).

(11.3)

(11.4)

Notice that by construction, the range of the MLE coincides with the
range of the parameter Θ. Let us consider an example.
Example: Let X1 . . . , Xn ∼ Bernoulli(p). Let us ﬁnd the MLE of the
model parameter p ∈ [0, 1]. Since the PMF is f (x; p) = px(1 − p)1−x,
x = 0, 1, the likelihood function is

Ln(p|X1, . . . , Xn) =

n∏

pXi (1 − p)1−Xi
i=1 Xi (1 − p)n−∑n

i=1
= p∑n

i=1 Xi = pS(1 − p)n−S,
i=1 Xi. Figure 11.3 shows the likelihood function

where S = ∑n
(11.5) (up to a multiplicative constant) for the data generated from
Bernoulli(p) with p = 1/3 with n = 10, 100, and 1000.

(11.5)

66 k. m. zuev

Figure 11.3: Three normalized (so that
maxLn(p) = 1) likelihood functions for
the data X1, . . . , Xn ∼ Bernoulli(1/3)
with n = 10, n = 100, and n = 1000.
Notice how the likelihood function
becomes more and more concentrated
around the true value p = 1/3 as the
sample size n increases.

3 The same estimate is given by the
plug-in princimple and the method of
moments.

Now we need to ﬁnd the value of p that maximizes the likelihood.
n (p) < 0 at

To this end, we need to solve L(cid:48)
the solution. This leads to a very familiar estimate3:

n(p) = 0 and check that L(cid:48)(cid:48)

ˆpMLE =

S
n

= Xn.

(11.6)
(cid:3)

Continuous Models

Now let us turn to the continuous parametric case, where the data
is modeled as a sample X1, . . . , Xn ∼ f (x; θ) from a continuous
distribution with PDF f (x; θ). If we were to mimic the discrete case
exactly, we would fail since the probability of observing the data
P(Xi|θ) = 0 in continuous settings. But there
P(X1, . . . , Xn|θ) = ∏n
is a walk around this technical problem.

i=1

Recall that if X ∼ f (x; θ), then for small  (cid:28) 1

P(x −  < X < x + |θ) =

f (t; θ)dt ≈ 2 f (x; θ).

(11.7)

(cid:90) x+

x−

Therefore,

P(x1 −  < X1 < x1 + , . . . , xn −  < Xn < xn + |θ)

P(xi −  < Xi < xi + |θ)

(11.8)

n∏

i=1

n∏

=

=

i=1

2 f (xi; θ) = (2)n

f (xi; θ).

n∏

i=1

Of course if  → 0, both sides of (11.8) quickly converge to zero.
But for small non zero  the probability on the left hand side that we
want to maximize, is proportional to ∏n
i=1 f (xi; θ). This leads to the
following natural deﬁnition of the likelihood function:

Ln(θ) = L(θ|X1, . . . , Xn) =

n∏

i=1

f (Xi; θ).

(11.9)

Parameter p00.10.20.30.40.50.60.70.80.91Likelihood   Ln(p)00.20.40.60.81n=10n=100n=1000statistical inference

67

(cid:82)
4 It is important, so once again:
Ln(θ) is not a density, in particular,
Θ Ln(θ)dθ (cid:54)= 1.

5 Multiplication of Ln(θ) by some
positive constant c (not depending on
θ) does notchange the MLE. Hence, for
convenience, we will often drop some
irrelevant constants in the likelihood
function.

Figure 11.4: Top row: three likelihood
functions for the data X1, . . . , Xn ∼
N (0, 1) with n = 10, n = 100, and n =
1000. Bottom row: the corresponding
contour plots. Red color corresponds to
higher values of the likelihood function.
Notice how the likelihood function
becomes more and more concentrated
around the true values θ1 = µ = 0
and θ2 = σ2 = 1 as the sample size n
increases.

6 And then checking that this is indeed
the maximum, not the minimum!
7 At least for me :)

In words, the likelihood function is the joint density of the data,
excepts that we treat it is a function of the model parameter θ
discrete case, the MLE is the value of θ that maximizes Ln(θ).
Example: Let X1 . . . , Xn ∼ N (µ, σ2). Let us ﬁnd the MLEs of the
model parameters θ = (θ1, θ2), where θ1 = µ and θ2 = σ2. The
likelihood function (ignoring some multiplicative constants5) is

4. As in the

L(θ|X1, . . . , Xn) =

− (Xi − µ)2

exp

(cid:18)
(cid:32)

n∏

1
σ
i=1
− n
2
2

= θ

exp

(cid:19)

2σ2
n∑

i=1

− 1
2θ2

(Xi − θ1)2

(cid:33)

.

(11.10)

Figure 11.4 shows the likelihood function (11.10) for the data gener-
ated from N (0, 1) with n = 10, 100 and 1000.

∂θ1

= 0 and ∂Ln(θ)

The next step is to ﬁnd the MLEs by solving the system of two
equations ∂Ln(θ)
6. But differen-
tiating (11.10) is a daunting task7. Technically, it is much easier to
differentiate its logarithm ln(θ) = logLn(θ). Note that since log is an
increasing function, maximizing Ln(θ) is equivalent to maximizing
ln(θ).

= 0 for θ1 and θ2

∂θ2

ln(θ|X1, . . . , Xn) = − n
2

log θ2 − 1
2θ2

(Xi − θ1)2.

n∑

i=1

(11.11)

Solving ∂ln(θ)
∂θ1
familiar result:

= 0 and ∂ln(θ)
∂θ2

= 0 for θ1 = µ and θ2 = σ2 gives a

ˆµMLE = Xn

and

ˆσ2
MLE =

1
n

n∑

i=1

(Xi − Xn)2.

(11.12)

68 k. m. zuev

It can be veriﬁed8 that these values indeed deﬁne the global maxi-
mum of the likelihood.

(cid:3)

8 e. g. [CB], Example 7.2.11 on p.321.

Log-Likelihood

In the last example, we maximized the logarithm of the likelihood
function because it was theoretically equivalent9, but technically
much easier. This is often the case for many parametric models. The
log of the likelihood function is called, es expected, the log-likelihood:

ln(θ) = logLn(θ) =

n∑

i=1

log f (Xi; θ).

(11.13)

Sums are easier to differentiate than products.

Plug-In, MOM, and MLE

9 Maximizing h(x) is equivalent to
maximizing log h(x).

So far we consider two examples of maximum likelihood estima-
tion — Bernoulli and normal models — and in both cases MLE
agrees with the corresponding MOM and plug-in estimates. Recall
(lecture 8) that plug-in and MOM disagree in estimating the upper
bound of the uniform model U[0, θ]: ˆθplug-in = X(n) and ˆθMOM = 2Xn.
What about MLE?
Example: Let X1 . . . , Xn ∼ U[0, θ]. Let us ﬁnd the MLE of the model
parameter θ. Given θ, the PDF is

 1

θ ,
0,

f (x; θ) =

if x ∈ [0, θ],
if x /∈ [0, θ].

(11.14)

0,

1
θn ,

The likelihood function is then (keep in mind that in (11.15), X1, . . . , Xn
are ﬁxed, and θ is a variable)

L(θ|X1, . . . , Xn) =

n∏

i=1

f (Xi; θ) =

if θ < X(n),
if θ ≥ X(n).

(11.15)

The likelihood function (11.15) is shown schematically in Fig. 11.5.

The MLE of θ is therefore ˆθMLE = X(n), the same as the plug-in

estimate.

(cid:3)

Thus, MLE agrees with the plug-in estimate in all three examples.
It turns out however, that the above example can be slightly modiﬁed
in such a way that all three methods give different results. Namely,
let us consider the model U(0, θ), where the support of the uniform

statistical inference

69

Figure 11.5: The likelihood function
(11.15) for the uniform model.

distribution is the open interval (0, θ). The plug-in and MOM esti-
mates remain the same in this case, but it is easy to see that the MLE
simply does not exist, since the new likelihood function

0,

1
θn ,

L(θ|X1, . . . , Xn) =

if θ ≤ X(n),
if θ > X(n),

(11.16)

which does not have a maximum. But of course, this example is quite
artiﬁcial: for all practical purposes ˆθ = X(n) + , where  (cid:28) 1 will be
a good estimate for θ. Can you come up with a better example where
all three methods give substantially different estimates?

Final Remarks

In the next Lecture, we will discuss good properties of MLEs. So let
us list a few bad ones here. There are a few issues associated with the
general problem of ﬁnding the global maximum of a function, and,
therefore, with maximum likelihood estimation.

1. The MLE may not exist.

2. The MLE may not be unique.

3. Finding the global maximum can be a nontrivial task. In some
cases, e.g. for the Bernoulli and normal models, this problem re-
duces to a simple calculus problem. But in most applications (even
using common statistical models) MLEs can’t be found analytically
and some numerical optimization methods must be used.

4. The likelihood function may have several local maxima. In this
case, a maximum found by a numerical method may not be the
global maximum, and, therefore, some additional checks are re-
quired.

Parameter 3Likelihood  L(3)X(2)X(1)X(n)10 For example, I. Otkin et al (1981)
“A comparison of n estimators for the
binomial distribution,” Journal of the
American Statistical Association 76(375),
637-642 demonstrate this effect with the
binomial model Binomial(k, p). If the
data is X1 = 16, X2 = 18, X3 = 22, X4 =
25, X5 = 27, then ˆkMLE = 99, but if
X5 = 28, then ˆkMLE = 190!

70 k. m. zuev

5. Sensitivity to small changes in data: sometimes a slightly different

sample will produce a vastly different MLE10, making the use of
the method at least questionable. This happens when the likeli-
hood function is very ﬂat in the neighborhood of its maximum.

Further Reading
1. The MLEs for the multivariate normal model N (µ, Σ), where
µ ∈ Rk and Σ is a k × k symmetric, positive deﬁnite covariance
matrix are a straightforward generalization of the considered uni-
variate model. The expressions can be found in many textbooks,
for example, in [Wa], Sec. 14.3.

2. A tutorial exposition of maximum likelihood estimation is pro-

vided in J. Myung (2003) “Tutorial on maximum likelihood estima-
tion,” Journal of Mathematical Psychology 47: 90-100.

What is Next?

The popularity of MLEs stems from their attractive properties for
large sample sizes. We will discuss these properties in the next Lecture.

12
Properties of Maximum Likelihood Estimates

The method of maximum likelihood is, by far, the most pop-
ular method of parametric inference. Its popularity stems from
the nice asymptotic properties of MLEs: if the parametric model
F = { f (x; θ)} satisﬁes certain regularity conditions1, then the MLE is
consistent, asymptotically normal, asymptotically unbiased, asymp-
totically efﬁcient, and equivariant. Let us discuss these properties in
detail. So, assume that

X1, . . . , Xn ∼ f (x; θ),

(12.1)

let θ0 denote the true value of θ ∈ Θ, and let ˆθn be the MLE of θ.

Consistency

1 These are essentially smoothness
conditions.

Consistency is a basic “must have” property for any reasonable esti-
mate. As MOM, the MLE is consistent:
P−→ θ0.

ˆθn

(12.2)

In words, as we get more and more data, the MLE ˆθn becomes more
and more accurate, and gives the correct answer in the long run.

I will use the urge to give you a sketch of a proof of consistency as

an opportunity to introduce another important notion which is fre-
quently used in probability and information theory: Kullback-Leibler
(KL) distance which measures the difference between two probability
distributions f and g2,

(cid:90)

D( f , g) =

f (x) log

f (x)
g(x)

dx.

(12.3)

It can be shown that D( f , g) ≥ 0 and D( f , g) = 0 if and only if f = g.
However, it is not symmetric: D( f , g) (cid:54)= D(g, f ). Despite the name,
the KL distance is not really a distance in the formal sense3.

2 As usual, in the discrete settings,
where f and g are PMFs, the integral
sign in (12.3) is replaced by a sum.

3 That is why it is often called the KL
divergence.

72 k. m. zuev

Sketch of Proof of (12.2). To ﬁnd the MLE, we need to maximize the
log-likelihood:

ln(θ) =

n∑

i=1

log f (Xi; θ) → max .

(12.4)

The expression of the log-likelihood as a sum of iid quantities calls
for the use of the law of large numbers:

log f (Xi; θ)

P−→ E [log f (X; θ)]

n∑

ln(θ)

n

1
n

(cid:90)

=

=

i=1
f (x; θ0) log f (x; θ)dx.

(12.5)

The right hand side of (12.5) reminds us a bit the KL distance be-
tween f (x; θ0) and f (x; θ). It is not quite the distance, but it is straight-
forward to cook it up:

(cid:18) f (x; θ)

f (x; θ0)

(cid:19)

f (x; θ0)

dx

(cid:90)

(cid:90)

ln(θ)

n

f (x; θ0) log

P−→
(cid:90)
=
= −D(θ0, θ) + ξ(θ0),

f (x; θ0) log

f (x; θ)
f (x; θ0)

dx +

f (x; θ0) log f (x; θ0)dx

(12.6)

where D(θ0, θ) = D( f (x; θ0), f (x; θ)) and ξ(θ0) =(cid:82) f (x; θ0) log f (x; θ0)dx

is a function of θ0. Thus, for large n,

ln(θ) ≈ −nD(θ0, θ) + nξ(θ0).

(12.7)

Since D(θ0, θ) ≥ 0 and D(θ0, θ) = 0 if and only if θ = θ0
4, the
log-likelihood is maximized at ˆθn ≈ θ0, where the approximation
becomes the exact equality in the limit n → ∞.

(cid:3)

Asymptotic Normality

It turns out that for large n, the distribution of the MLE ˆθn is approx-
imately normal. Namely, under appropriate regularity conditions5,

where se is the standard error of MLE, se = se[ ˆθn] =
V[ ˆθn].
Moreover, the standard error can be approximated analytically:

ˆθn ·∼ N (θ0, se2),

se ≈

1(cid:112)nI(θ0)

,

(cid:113)

(12.8)

(12.9)

(12.10)

where I(θ0) is the Fisher information of a random variable X with
distribution f (x; θ0) from the family F = { f (x; θ), θ ∈ Θ}:

(cid:32)

I(θ0) = E

∂ log f (X; θ)

∂θ

(cid:33)2 .

(cid:12)(cid:12)(cid:12)(cid:12)θ=θ0

4 Strictly speaking, we must assume
that different values of the parameter
θ correspond to different distributions:
i f θ1 (cid:54)= θ2 ⇒ D(θ1, θ2) > 0. Such models
F are called identiﬁable.

ˆθn−θ0
se

5 More precisely,
converges
to the standard normal variable in
distribution.

statistical inference

73

Let us give an informal interpretation of the Fisher information.

The derivative6

(cid:12)(cid:12)(cid:12)θ=θ0

∂ f (x;θ)

∂θ
f (x; θ0)

=

(cid:12)(cid:12)(cid:12)(cid:12)θ=θ0

s(x; θ0) =

∂ log f (x; θ)

∂θ

6 It is called the score function.
In terms of the score function,
I(θ0) = E[s2(X; θ0)].

(12.11)

7 In fact, the expected value of the score
function (12.11) is zero. To see this,

differentiate(cid:82) f (x; θ)dx = 1 with

respect to θ at θ0.

8 This also follows from (12.9): the
larger I(θ0), the smaller the standard
error.

9 If I(θ0) is small, (12.9) tells us that we
need a lot of data, to get small standard
error.

10 The proof is straightforward compu-
tation of the integral in (12.12).

can be interpreted as a measure of how quickly the distribution f
will change at X = x when we change the parameter θ near θ0. To
get the measure of the magnitude of the change (we don’t care about
the sign7), we square the derivative (12.11). To get the average value
of the measure across different values of X, we take the expectation.
Thus, if I(θ0) is large, the distribution f (x; θ) will change quickly
when θ moves near θ0. This means that f (x; θ0) is quite different
from “neighboring” distributions, and we should be able to pin it
down well from the data. So, large I(θ0) is good8: θ0 is easier to
estimate. If I(θ0) is small, we have the opposite story: distribution
f (x; θ) are similar to f (x; θ0), and therefore the estimation of the true
value is troublesome9.

While (12.10) provides an intuitive interpretation of the Fischer
information, it is not convenient for actual computations. It can be
shown that10

(cid:34)
(cid:90) ∂2 log f (x; θ)

∂2 log f (X; θ)

∂θ2

∂θ2

(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)θ=θ0
(cid:12)(cid:12)(cid:12)(cid:12)θ=θ0

I(θ0) = −E

= −

(12.12)

f (x; θ0)dx.

To get a better feel for it, let us compute the Fischer information

for some particular models.

Example: For the Bernoulli(p), the Fisher information is

I(p) =

1

p(1 − p)

.

(12.13)

It agrees with our intuitive interpretation: the closer p to 0 or 1, the
larger the Fisher information, and the easier to infer p from the data.
The fair coin p = 1
information.
Example: For the normal model N (µ, σ2) with known σ2, the Fisher
information is constant:

2 provides the global minimum of the Fisher

(12.14)
Indeed, it is equally difﬁcult (or easy) to infer different values of µ. (cid:3)

I(µ) =

1
σ2 .

(cid:3)

Let us now come back to the MLE standard error approximation
(12.9). We now know how to compute and (most importantly) how

74 k. m. zuev

to think of I(θ0). How does the factor n appear in the denominator?
Since the expected value of the score function is zero11, the Fisher
information (12.10) is simply the variance of the score function:

11 See footnote 7.

I(θ0) = V[s(X; θ0)].

(12.15)
This is the Fisher information of a single random variable distributed
according to f (x; θ0). If we have an iid sample of size n, X1 . . . , Xn ∼
f (x; θ0), then the Fisher information of this sample is deﬁned as the
variance of the sum of score function:
n∑

(cid:34) n∑

V[s(Xi, θ0)] = nI(θ0).

In(θ0) = V

s(Xi, θ0)

(12.16)

(cid:35)

=

i=1

i=1

Thus, the denominator of (12.9) is the square root of the Fisher infor-
mation of the sample X1, . . . , Xn.

Sketch of Proof of asymptotic normality of the MLE:

ˆθn ·∼ N

Recall that by deﬁnition,

θ0,

1

nI(θ0)

(cid:18)

(cid:19)

ˆθn = arg max

θ∈Θ ln(θ),

.

(12.17)

(12.18)

where ln(θ) is the log-likelihood. Let us Taylor-expand the derivative
of ln(θ) at θ = θ0:
l(cid:48)
n(θ) = l(cid:48)

n(θ0) + (θ − θ0)l(cid:48)(cid:48)

n (θ0) + higher order terms

(12.19)
n( ˆθn) = 0, and dropping the higher order
ˆθn − θ0 = − l(cid:48)
n(θ0)
l(cid:48)(cid:48)
n (θ0)

(12.20)

.

Setting θ = ˆθn, noting that l(cid:48)
terms, we obtain:

• The nominator (using the central limit theorem):

1
n

l(cid:48)
n(θ0) =

1
n
·∼ N

n∑
(cid:18)

i=1

∂ log f (Xi; θ)

∂θ

E[s(X; θ0)],

(cid:12)(cid:12)(cid:12)(cid:12)θ=θ0

=

1
n
V[s(X; θ0)]

n

• The denominator (using the law of large numbers):

(cid:19)

.

(12.21)

I(θ0)

n

(12.22)

(12.23)
(cid:3)

12 Slutzky’s theorem allows to to that: if
D−→ X
Xn
a .

P−→ a, then Xn
Yn

D−→ X and Yn

0,

(cid:18)

s(Xi; θ0)

i=1
= N

n∑
(cid:19)
(cid:12)(cid:12)(cid:12)(cid:12)θ=θ0
(cid:35)
(cid:12)(cid:12)(cid:12)(cid:12)θ=θ0
= −I(θ0).
(cid:19)

1

1
n

l(cid:48)(cid:48)
n (θ0) =

1
n
≈ E

n∑
(cid:34)

i=1

∂2 log f (Xi; θ)

∂θ2

∂2 log f (X; θ)

∂θ2

(cid:18)

ˆθn − θ0 ·∼ N

0,

nI(θ0)

.

Combining12 (12.20), (12.21), and (12.22), we get what we need:

statistical inference

75

Remark. Why is this a sketch, not a proof? Because we need to make
an appropriate regularity conditions on the statistical model F =
{ f (x; θ)} to make sure that all considered derivatives exist, the Fisher
information is well deﬁned, the higher order terms in the Taylor
series go to zero, the conditions for the Law of Large Numbers and
Central Limit Theorem are satisﬁed, etc.

(cid:3)

Asymptotic Conﬁdence Intervals
The asymptotic normality (12.8) is a nice theoretical result, but how
to use it in practice if the standard error (4.1) is unknown since θ0 is
unknown. It can be shown that the standard error of the MLE can be
estimated by

,

(12.24)

and the asymptotic normality result will still hold: for large n,

(cid:98)se =

1(cid:113)

nI( ˆθn)

·∼ N (0, 1).

ˆθn − θ0(cid:98)se
In = ˆθn ± zα/2(cid:98)se,

We can use (12.25) for construction an asymptotic conﬁdence inter-

val for θ0. Indeed let

(12.25)

(12.26)

then the usual computation shows that
P(θ0 ∈ In) → 1 − α,

For example, if α = 0.05, then z α
2
mate 95% conﬁdence interval for θ0.

as n → ∞.

≈ −2, and ˆθn ± 2(cid:98)se is an approxi-

(12.27)

(cid:113) Xn(1−Xn)

Example: Supposed that X1, . . . , Xn ∼ Bernoulli(p), then ˆpMLE = Xn,
(cid:98)se =
, and an approximate 95% conﬁdence interval for p is

n

(cid:115)

In = Xn ± 2

Xn(1 − Xn)

n

.

(12.28)

(cid:3)

Asymptotic Unbiasedness

As a byproduct of asymptotic normality with vanishing variance, we
have that the MLE is asymptotically unbiased13:

lim
n→∞

E[ ˆθn] = θ0.

(12.29)

13 By the way, this is not a by-product
of consistency! Somewhat counterin-
P−→ a does not imply that
tuitive, Xn
E[Xn] → a. Can you give a counterex-
ample?

76 k. m. zuev

Example: Recall that for the normal N (µ, σ2) and uniform U[0, θ]
models the MLEs are:

ˆµMLE = Xn,

ˆσ2
MLE =

1
n

n∑

i=1

(Xi − Xn)2,

ˆθMLE = X(n).

(12.30)

The ﬁrst estimate is unbiased for any n, but the last two are biased:

E[ˆσ2

MLE] =

n − 1
n σ2, E[ ˆθMLE] =

n
n + 1 θ.

In both cases, the bias disappears as the sample size increases.

(12.31)

(cid:3)

Asymptotic Efﬁciency

We see that, when n is large, the MLE is approximately unbiased.
This leads to a natural question: what is the smallest possible value
of the variance of an unbiased estimate?

The answer is given by the Cramer-Rao inequality. Let ˜θn be any
unbiased estimate of the parameter θ whose true value is θ0, then14

V[ ˜θn] ≥ 1

nI(θ0)

.

(12.32)

An unbiased estimate whose variance achieves this lower bound is
said to be efﬁcient. In some sense, it is the best estimate15. Note that
the right-hand-side of (12.32) is exactly the asymptotic variance of the
MLE. Therefore, the MLE is asymptotically efﬁcient. Roughly speaking,
this means that among all well-behaved estimates, the MLE has the
smallest variance, at least for large samples.

Example: Assume that X1 . . . , Xn ∼ N (µ, σ) with known σ2. Then
the MLE ˆµMLE = Xn and its variance is σ2
Cramer-Rao lower bound. Another reasonable estimate of the mean
is the sample median ˜µn. It can be shown that its asymptotically un-
σ2
n . Thus, ˜µn converges to the right value,
biased and its variance is π
2
(cid:3)
but it has a larger variance than the MLE.

n . This is exactly the

Note, however, that

a) For a ﬁnite sample size n, MLE may not be efﬁcient,

14 Again, (12.32) is true if the underly-
ing statistical model satisﬁes certain
regularity conditions.

15 An efﬁcient estimate may not exist.

b) If the MLE is efﬁcient16, there may still exist a biased estimate

16 e. g. for the normal or Poisson models.

with a smaller mean squared error, and

c) MLEs are not the only asymptotically efﬁcient estimates.

statistical inference

77

Equivariance

This is a non-asymptotic17 “bonus” to the nice asymptotic properties
of MLEs. Suppose we are interested in estimating a parameter τ,
which is a function of θ which parametrizes the model (12.1)18, τ =
g(θ). It turns out that if we know ˆθMLE, then the MLE of τ is simply

17 Valid for all n.

18 Recall a blood test example from
Lecture 10.

ˆτMLE = g( ˆθMLE).

(12.33)

This property is called equivariance or transformation invariance.

Example: Recall that in Lecture 11, we found that the MLE of vari-
ance under the normal model N (µ, σ) is

ˆσ2
MLE =

1
n

n∑

i=1

(Xi − Xn)2.

(12.34)

19 We don’t need to solve the calculus
problem again with respect to θ2 = σ.

20 Theorem 7.2.10 in [CB].

Thanks to the equivariance of maximum likelihood estimation, we
can readily ﬁnd the MLE of the standard deviation19:

(cid:115)

ˆσMLE =

1
n

n∑

i=1

(Xi − Xn)2.

(12.35)
(cid:3)

Equivariance holds for arbitrary functions g20. If g is a one-to-

one correspondence, then it is easy to understand why. In this case,
there exists the inverse function θ = g−1(τ). This means that we can
parametrize the model using θ, as in (12.1), or using τ:

f (x; θ) = f (x; g−1(τ)) =: ˜f (x; τ),
F = { ˜f (x; τ), τ ∈ T}, T = g(Θ).
Let (cid:101)L denote the likelihood in the τ-parametrization. Then
(cid:101)L(τ) = ∏ ˜f (Xi; τ) = ∏ f (Xi; θ) = L(θ).
(cid:101)L(τ) = L(θ) ≤ L( ˆθMLE) = (cid:101)L(g( ˆθMLE)),

Therefore, for any τ:

(12.36)

(12.37)

(12.38)

which means exactly (12.33).

Final Remark

The considered nice properties explain the popularity of MLEs. But
we should always keep in mind, that if the statistical model F =
{ f (x; θ), θ ∈ Θ} is wrong, meaning that there is no θ in Θ that model
the data adequately, then the inference based on f (x; ˆθMLE) may be

21 Here we focused on a one parameter
case. Everything could be generalized
to the arbitrary number of parameters.
High-dimensional optimization is, in
general, a non-trivial task.

78 k. m. zuev

very poor. Moreover, even if the model is correct, it may not satisfy
the regularity conditions (which is often difﬁcult to check) required
for the MLE to have the above asymptotic properties. Finally, even
if the model is correct and satisﬁes the regularity conditions, ﬁnding
the MLE could be very challenging21: the (log) likelihood may not be
analytically tractable, may have many local maxima, be sensitive to
data, etc.

Further Reading

1. The regularity conditions, so often mentioned in these notes, are

discussed in detail in many advanced (and often unreadable) texts
on mathematical statistics. This [note] provides a good trade-off
between rigor and readability.

2. Computing the MLE using two standard numerical methods,

Newton-Raphson and the expectation-maximization algorithm, are
discussed in Sec. 9.13.4 of [Wa].

3. For extension to multiparameter models, see Sec. 9.10 of [Wa].

What is Next?

“To be, or not to be...” In the next Lecture, we will start discussing
move to hypothesis testing.

13
Hypothesis Testing: General Framework

In previous lectures, we discussed how to estimate parameters
in parametric and nonparametric settings. Quite often, however,
researchers are interesting in checking a certain statement about a
parameter, not its exact value. Suppose, for instance, that someone
developed a new drug for reducing blood pressure. Let θ denote the
average change in a patient’s blood pressure after taking a drug. The
big question is to test

H0 : θ = 0 versus H1 : θ (cid:54)= 0.

(13.1)

The hypothesis H0 is called the null hypothesis. It states that, on av-
erage, the new treatment has zero effect1 on blood pressure. The
alternative hypothesis2 states that there is some effect. In this context,
testing H0 against H1 is a primary problem. Even if we ﬁnd out that
θ (cid:54)= 03, estimating the value of θ is important, yet a secondary prob-
lem. A part of statics that deals with this sort of “yes/no” problems
is called hypothesis testing.

In this lecture, we discus a general framework of hypothesis test-

ing. To get started let us consider the following toy example, that will
help us to illustrate all main notions and ideas.

Two Coins Example

Suppose that Alice has two coins: fair and unfair, with the probabili-
ties of heads p0 = 0.5 and p1 = 0.7 respectively. Alice chooses one of
the coins, tosses it n = 10 times and tells Bob the number of heads,
but does not tell him what coin she tossed. Based of the number of
heads k, Bob has to decide which coin it was.

Intuitively, it is clear that the larger k = 0, 1, . . . , n, the more likely
it was an unfair coin. If Alice tossed coin i (i = 0 is fair and i = 1 is
unfair), then the probability of getting exactly k heads is given by the

1 Hence the name “null.”
2 Also sometimes called the research
hypothesis.

3 Hopefully, θ < 0!

Figure 13.1: Alice and Bob are two
archetypal characters commonly used
in cryptography, game theory, physics,
and now.... in statistics. Comics source:
physicsworld.com.

80 k. m. zuev

Binomial distribution Bin(n, pi):

(cid:18)n

(cid:19)

k

Pi(k) =

i (1 − pi)n−k,
pk

i = 0, 1.

(13.2)

Figure 13.2 shows the values of these probabilities for different k.

Figure 13.2: Probabilities (13.2).

Suppose that Bob observed only k = 2 heads. Then

P0(k = 2)
P1(k = 2)

≈ 30,

(13.3)

and, therefore, the fair coin is about 30 times more likely to produce
this result than the unfair one. On the other hand, if there were k = 8
heads, then

P0(k = 8)
P1(k = 8)

≈ 0.19,

which would favor the unfair coin. So, based on Fig. (13.2), Bob
should guess that the coin is unfair if

k ∈ {7, 8, 9, 10},

and unfair otherwise. This is the simplest example of testing.

General Framework

(13.4)

(13.5)

Suppose that data X1, . . . , Xn is modeled as a sample from a distribu-
tion f ∈ F 4. Let θ be the parameter of interest, and Θ be the set of all
its possible values, called the parameter space. Let Θ = Θ0 (cid:116) Θ1 be a
partition of the parameter space into two disjoint sets5. Suppose we
wish to test

H0 : θ ∈ Θ0 versus H1 : θ ∈ Θ1.

(13.6)

We call H0 the null hypothesis and H1 the alternative hypothesis.

4 The statistical model F can be either
parametric or nonparametric.
5 Recall that A = B (cid:116) C means that
A = B ∪ C and B ∩ C = ∅.

01234567891000.050.10.150.20.250.30.35Number of HeadsProbabiityCoin 0Coin 1Let Ω be the samples space, i.e. the range of data, X = (X1, . . . , Xn) ∈
Ω. We test a hypothesis by ﬁnding an appropriate subset of outcomes
R ⊂ Ω, called the rejection region:

If X ∈ R ⇒ reject H0,
If X /∈ R ⇒ accept H0.

Usually, the rejection region has the following form:

R = {X ∈ Ω : s(X) > c},

(13.7)

(13.8)

where s is a test statistic and c is a critical value. The problem of test-
ing is then boils down to ﬁnding

• an appropriate statistic s and

• an appropriate critical value c.

In the two coin example, the data is the total number of heads

X = k, which is modeled as a sample from the binomial distribution
Bin(n, θ), where n = 10 and θ ∈ Θ = {0.5, 0.7}. The hull hypothesis
is that the coin is fair: H0 : θ ∈ Θ0 = {0.5}, and the alternative is
that the coin is not fair, H1 : θ ∈ Θ1 = {0.7}. The sample space is
Ω = {0, . . . , 10}. Bob tested the hypothesis using the rejection region
R given by (13.5)6.

The Null and Alternative

Mathematically, the null and alternative hypotheses seem to play
symmetric roles. Traditionally, however, the null hypothesis H0 says
that “nothing interesting” is going on7, the current theory is correct,
no new effects, etc. The null hypothesis is a “status quo.” The alter-
native hypothesis, on the other hand, says that something interesting,
something unexpected is happening: the old theory needs to be up-
dated, new previously unseen effects are present, etc8.

It is useful to think of hypothesis testing is a legal trial. By default,

we assume that someone is innocent9 (the null hypothesis) unless
there is strong evidence that s/he is guilty (alternative).

Question: Suppose an engineer designed a new earthquake-resistant
building. Let pF be the failure probability of the building under
earthquake excitation. How would you formulate the null and al-
ternative hypotheses if you wish to test whether or not the failure
probability is smaller than a certain acceptable threshold p∗
F?

statistical inference

81

6 What is the test statistic and the
critical value in this example?

7 Recall the drag example from the
beginning. H0 says the new drag no
effect on the blood pressure.

8 This explains why we focus on the
rejection region and not the acceptance
region. The rejection region is where
the surprise is living.
9 Presumption of innocence.

Figure 13.3: Unfortunately, the pre-
sumption of innocence does on always
work in real life.

10 The unfair coin may produce 5 heads
in which case Bob will make in error by
accepting the hypothesis that the coin is
fair.

11 That is when we focus on equations
and forget about the context.

82 k. m. zuev

Errors in Testing

Can we guarantee that we make no errors when making conclusions
from data? Of course, not. Data provides some, but not full, infor-
mation about the unknown quantity of interest and helps to reduce
the uncertainty, but not completely illuminate it. The errors are thus
unavoidable10.

There are two types of errors in hypothesis testing with very bor-

ing names: type I error and type II error:

• Type I error: rejecting H0 when it is true.

• Type II error: accepting H0 when it is not true.
Purely mathematically11, making both errors are equally bad. But,
given the context discussed in the previous section, making a type
I error is much worse than making a type II error: declaring an in-
nocent person guilty is much worse than declaring a guilty person
innocent. Probabilities of both errors can be computed using the
so-called power function.

Power Function
If R is the rejection region, then the probability of a type I error is

P(Type I error) = P(X ∈ R|θ ∈ Θ0).

(13.9)

The probability of a type II error is

P(Type II error) = P(X /∈ R|θ ∈ Θ1)

= 1 − P(X ∈ R|θ ∈ Θ1).

(13.10)

From (13.9) and (13.10), we see that probabilities of both error are
determined by function on the parameter space P(X ∈ R|θ) . This
leads to the following deﬁnition.

Deﬁnition 2. The power function of a hypothesis test with rejection
region R is the function of θ deﬁned by

β(θ) = P(X ∈ R|θ).

In term of error probabilities:

β(θ) =

P(Type I error),

1 − P(Type II error),

(13.11)

(13.12)

if θ ∈ Θ0,
if θ ∈ Θ1.

The ideal test will thus have the power function which is zero on Θ0
and one on Θ1, see Fig. 13.4. This ideal is rarely (never) achieved in
practice.

Figure 13.4: The ideal power function.

Parameter space #Power function -(3)00.20.40.60.81Ideal Test#0#1statistical inference

83

Example:
In the two coin example, the parameter space is a two
point set Θ = {0.5, 0.7}, Θ0 = {0.5}, Θ1 = {0.7}, and the power
function is

β(θ) = P(k ∈ {7, 8, 9, 10}|θ)

(cid:18)10

(cid:19)

=

10∑

k=7

k

θk(1 − θ)10−k ≈

0.17,

0.65,

if θ = 0.5,
if θ = 0.7.

(13.13)

This power function is not exactly what Bob would like to have, but
in some sense (will discuss later) this is the best possible test.

(cid:3)

In reality, a reasonable test has power function near zero on Θ0
and near one on Θ1. So, qualitatively, the power function of a good
test looks like the one in Fig. 13.5.

Controlling Errors

Usually it is impossible to control both types of errors and make their
probabilities arbitrary small. Roughly, the reason behind this is the
following. Choosing a test is choosing the rejection region R ⊂ Ω. If
we want to make the type I error probability (13.9) smaller, we need
to shrink R. In the extreme case, we can completely exclude the type
I error by taking R = ∅. On the other hand, to make the type II error
probability (13.10) smaller, we need to inﬂate R. By taking R = Ω,
we can guaranty that the type II error will not be made. So, typically,
decrease in the probability of one error leads to the increase of the
probability of the other error12.

As we discussed previously, type I error is more dangerous, and
therefore, controlling its probability is more important. This leads to
the following deﬁnition.

Deﬁnition 3. The size of a test with power function β(θ) is

α = sup
θ∈Θ0

β(θ).

(13.14)

A test is said to have level α if its size is ≤ α

13.

In words, the size of the test is the largest possible probability
of the type I error (rejecting H0 when it is true). See Fig. 13.5. Re-
searchers usually specify the size of the test they wish to use14 (to
make sure that the type I error is under control), and then search for
the test with the highest power under H1 (i.e. on Θ1) among all test
with level α. Such a test, if it exists, is called most powerful. Finding
most powerful tests is hard and, in many cases, they don’t even ex-
ist. So in practice, researchers use a test with power which is high
enough.

Figure 13.5: The power function of a
reasonably good test of size α.

12 The provided intuition is “rough”
because instead of shrinking and
inﬂating R we could move it around.

13 In practice, the terms “size” and
“level” are often used interchangeably
because both are upper-bounds for the
type I error probability.

14 With typical choice being
α = 0.01, 0.05, and 0.1.

Parameter space #Power function -(3)00.20.40.60.81Reasonable Test#1#0Level ,84 k. m. zuev

Example: Let X1, . . . , Xn ∼ N (µ, σ2), where σ2 is known15. We want
to test

H0 : µ ≤ 0 versus H1 : µ > 0.

(13.15)
So, here Θ = R, Θ0 = (−∞, 0], and Θ1 = (0, ∞). It seems reasonable
to use the sample mean Xn as a test statistic and reject H0 if Xn is
large enough. The rejection region is thus

R = {(X1, . . . , Xn) : Xn > c} ⊂ Ω = Rn,

(13.16)

15 i. e. estimated.

where c is the critical value. Let us ﬁnd the power function of this
test.

Since Xn ∼ N(cid:16)

µ, σ2
n

(cid:17)

√

β(µ) = P(Xn > c|µ).
n(Xn−µ)
(cid:18)√
√

, we have that

σ

n(Xn − µ)
(cid:18)√
>
σ
n(c − µ)

(cid:19)

= 1 − Φ

σ

β(µ) = P

(13.17)

∼ N (0, 1). Therefore,
n(c − µ)

(cid:19)

σ

.

(13.18)

The power function is an increasing function of µ. It is shown

Figure 13.6: The normal power function
(13.18) for n = 10, σ = 1, and different
values of c. Notice that as c increases
(rejection region shrinks), the size of the
test decreases, as expected.

in Fig. 13.6 for n = 10, σ = 1, and different values of c. As ex-
pected, when the rejection region (13.16) shrinks (the critical value
c increases), the size of the test α decreases meaning that it becomes
less and less likely to make the type I error. On the other hand, the
type II error probability increases. To make a test with a speciﬁc size α,
we need to ﬁnd the corresponding critical value c. Thanks to mono-
tonicity of β, α = β(0). Together with (13.18), this give an equation
for c, whose solution is

c =

σΦ−1(1 − α)

√

n

.

(13.19)

A halfway summary: the test which rejects H0 whenever Xn > c,
where c is given by (13.19), has size α.

Parameter 7-1-0.500.511.5Power function -(7)00.10.20.30.40.50.60.70.80.91Normal model, sample size n=10c=0.1c=0.5c=0.75size ,size ,statistical inference

85

16 For example, we are designing an
experiment, and trying to determine
what sample size is appropriate.

Suppose now that we can also control the sample size n16. Note

that the power function does depend on the sample size, and by
choosing n large enough we can hope to reduce the type II error
probability. Since β is continuous and β(0) = α (cid:28) 1, β(µ) (cid:28) 1 in the
neighborhood of zero, and, therefore, the type II error probability is
large in this neighborhood. However, we may step apart from zero by
δ > 0, δ (cid:28) 1 and ask the power function to be large at δ:

β(δ) = 1 − ,

(13.20)

where  > 0,  (cid:28) 1 plays similar role to type II error as α plays
for the type I error. Combining (13.18), (13.19), and (13.20), gives an
equation for the sample size, whose solution is

(cid:16)Φ−1(1 − α) − Φ−1())

(cid:17)

√

n =

σ
δ

.

(13.21)

Figure 13.7: The normal power function
(13.18) with n and c deﬁned from
(13.21) and (13.19) with α = δ =  = 0.1.
Notice the sample size increase!

17 Sometimes after long analysis :)

18 Analytically if you are lucky, but most
likely numericlly.

19 The data comes from an observational
study, or experiments are too expensive.

Thus, the test which rejects H0 whenever Xn > c, where n and c
are given by (13.21) and (13.19), has size α and, moreover, the type
II error probability is at most  if µ ∈ [δ, ∞] ⊂ Θ1. If µ ∈ [0, δ],
this probability is, unfortunately, larger. Figure 13.7 shows the power
(cid:3)
function for α = δ =  = 0.1.

The strategy described in this example is often employed in other
cases. Namely, to design a test, we need to specify the rejection re-
gion R = {X ∈ Ω : s(X) > c} by choosing a test statistic s and its
critical value c. Choosing the test statistic is an art, but often reason-
able candidates are rather obvious17. After choosing s, the rejection
region is parametrized by c. We chose c to get the desired size α. To
this end, we need to solve18 for c the following equation:

P(X ∈ Rc|θ) = α,

sup
θ∈Θ0

(13.22)

where X = (X1, . . . , Xn). If we can’t control n19, then we are done.
If we can control n, then we may try to reduce the type II error prob-
ability by exploiting the fact that the power function depends on n.

Further Reading
1. The most complete book on testing is E. Lehmann & J. Romano

(2005) Testing Statistical Hypotheses.

What is Next?
In real applications, ﬁnding most powerful tests is a very hard prob-
lem which often does not have a solution. So, instead of focusing on
the theory of most powerful tests, we will consider several widely
used tests that often perform reasonably well.

Parameter 7-0.5-0.4-0.3-0.2-0.100.10.20.30.40.5Power function -(7)00.10.20.30.40.50.60.70.80.91Normal model, sample size n=657size ,/014
The Wald test and t-test

In this lecture, we will discuss two straightforward and often
used parametric tests: the Wald test and the t-test.

The Wald Test

The Wald test bridges the gap between three statistical inference
methods: estimation, conﬁdence sets, and hypothesis testing.

Let θ be the parameter of interest, and suppose we want to test1

Let ˆθ be an estimate of θ
of ˆθ

3. Assume that ˆθ is approximately normally distributed:

H0 : θ = θ0 versus H1 : θ (cid:54)= θ0.

(14.1)

2, and let (cid:98)se be the estimated standard error
ˆθ − θ(cid:98)se

·∼ N (0, 1).

(14.2)

Note that this assumption is not very strong: it holds for many rea-
sonable estimates4.

In this settings, if H0 is true, then

Therefore, it seems rational to reject the null hypothesis if

(cid:12)(cid:12)(cid:12) is likely to be small.
(cid:12)(cid:12)(cid:12) ˆθ−θ0(cid:98)se
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > c.
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ˆθ − θ0(cid:98)se

W =

(14.3)

As usual, we ﬁnd the critical value c from the upper-bound for the
probability of the type I error, i.e. the size of the test. Since H0 is a
simple hypothesis, the size

α = sup
θ∈Θ0

β(θ) = β(θ0).

(14.4)

Under H0,

ˆθ−θ0(cid:98)se

·∼ N (0, 1), and therefore,

β(θ0) = P (W > c|θ = θ0) = P

(cid:32)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ˆθ − θ0(cid:98)se

(cid:33)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > c

≈ 2Φ(−c).

(14.5)

Figure 14.1: Abraham Wald, a Hun-
garian statistician. Photo source:
wikipedia.org.

1 A hypothesis of the form θ = θ0 is
called a simple hypothesis, and a test of
the form (14.1) is called a two-sided test.

2 For example, ˆθ = ˆθMLE.
3 For example, we can estimate se using
the bootstrap; or, if ˆθ is the MLE, then

(cid:113)

(cid:98)se = 1/

nI( ˆθ).

4 For example, if ˆθ is the MLE, then
(14.2) holds, since the MLE is asymptot-
ically normal.

The critical value c is thus5

To summarize, the size α Wald test rejects H0 when

W =

2

2

.

2

= z1− α
2

= −z α

c = −Φ−1(cid:16) α
(cid:17)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > −z α
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ˆθ − θ0(cid:98)se
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > −z α
(cid:32)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ˆθ − θ0(cid:98)se
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) θ
(cid:33)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) θ
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) θ
(cid:33)
(cid:32) ˆθ − θ0(cid:98)se
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) θ
(cid:32) ˆθ − θ(cid:98)se
(cid:33)
θ0 − θ(cid:98)se
(cid:19)
(cid:18) θ0 − θ(cid:98)se
(cid:19)

< z α
2

+ z α
2

+ Φ

+ P

+ P

.

.

2

(cid:33)

< z α
2

How does the power function look?

β(θ) = P(W > −z α

|θ) = P

2

(cid:32) ˆθ − θ0(cid:98)se
(cid:32) ˆθ − θ(cid:98)se
(cid:18) θ0 − θ(cid:98)se

> −z α

2

> −z α

2

+

− z α

2

= P

= P

= 1 − Φ

statistical inference

87

5 Recall that Φ(zα) = α.

Figure 14.2: The power function of the
Wald test with size α.

(14.6)

(14.7)

(cid:33)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) θ

θ0 − θ(cid:98)se

+

(14.8)

The power function of the Wald test is shown schematically in
Recall that (cid:98)se often tends to zero as the sample size n increases. As a
Fig. 14.2. As expected, β(θ0) = α. Also, β(θ) → 1 as |θ − θ0| → ∞.
result, β(θ) → 1 as n → ∞ for all θ (cid:54)= θ0. We can therefore reduce the
probability of the type II error outside of a certain neighborhood of θ0
by choosing n sufﬁciently large.

In a nutshell, given a point estimate of the parameter of interest,
which is approximately normally distributed (14.2), the Wald test al-
lows to test simple hypothesis (14.1) essentially with zero intellectual
effort.

Connection to Conﬁdence Intervals
Given the approximate normality (14.2), we can immediately con-
struct an approximate (1 − α)100% conﬁdence interval for θ:

I = ˆθ ± z α

2(cid:98)se.

The size α Wald test

Rejects H0 : θ = θ0 ⇔ θ0 /∈ I.

In words, testing the hypothesis is equivalent to checking whether
the null value is in the conﬁdence interval.

(14.9)

(14.10)

Parameter space #Power function -(3)00.10.20.30.40.50.60.70.80.91Size , Wald Test Size ,30#1#188 k. m. zuev

Comparing Means of Two Populations

Let us ﬁnish the discussion of the Wald test with a nonparametric
example. Suppose we are interesting in comparing the unknown
means µ1 and µ2 of two populations6. In particular, we want to test

H0 : ∆µ = 0 versus H1 : ∆µ (cid:54)= 0,

(14.11)

6 For example, mean income of males
and females.

where ∆µ = µ1 − µ2. Let X1 . . . , Xn and Y1, . . . , Ym be two inde-
pendent samples from the populations. The plug-in estimates of the
means are: ˆµ1 = Xn and ˆµ2 = Ym. A nonparametric estimates of ∆µ
is thus

(cid:99)∆µ = Xn − Ym.

The standard error of (cid:99)∆µ is

se2 = se2[Xn] + se2[Ym] =

σ2
1
n

+

σ2
2
m

,

where σ2

1 and σ2

2 are the population variances. It can be estimated by

(14.12)

(14.13)

+

s2
2
m

,

(14.14)

where s2
H0 when

i are the sample variances. Thus, the size α Wald test rejects

.

(14.15)

s2
1
n

(cid:98)se =
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) Xn − Ym
(cid:113) s2

1
n +

s2
2
m

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > z1− α

2

The t-Test
When testing simple hypothesis (14.1), it is common to use the t-test7
instead of the Wald test if

1. The data is modeled as a sample from the normal distribution,

2. The sample size is small.

Let X1, . . . , Xn ∼ N (µ, σ2), where both µ and σ are unknown.

Suppose we want to test

H0 : µ = µ0 versus H1 : µ (cid:54)= µ0.

(14.16)

7 The t-test was introduced in 1908 by
William Gosset, an English statistician.
At that time he was an employee
of Guinness in Dublin. To prevent
disclosure of conﬁdential information
that could potentially be used by other
competitors, the brewery prohibited its
employees from publishing any papers.
Gosset published his results under a
pseudonym “Student.”

of ˆµ by (cid:98)se = sn/

√

Let us estimate µ by the sample mean ˆµ = Xn and the standard error

n, where s2

n is the sample variance. If n is large,

then, under H0, the random variable

T =

ˆµ − µ0(cid:98)se

Xn − µ0
√
n
sn/

=

·∼ N (0, 1),

(14.17)

Figure 14.3: William Sealy Gos-
set (aka “Student”). Photo source:
wikipedia.org.

statistical inference

89

8 In fact, if k < 30, the two distributions
are very close.

Figure 14.4: The PDF of t-distribution
with k = 1, 2, and 3 degrees of freedom.

and we can use the Wald test. It turns out however, that for any n, the
exact distribution of T under H0 is Student’s t-distribution with (n − 1)
degrees of freedom:

(14.18)

The formula for the PDF of this distribution looks rather complicated:

T ∼ tn−1.

Γ(cid:16) k+1
kπΓ(cid:16) k
(cid:17)(cid:16)

2

2

(cid:17)

ftk (x) =

√

(cid:17) k+1

2

.

(14.19)

1 + x2
k

But it has a couple of nice properties: a) the t-distribution is symmet-
ric about zero and, b) as k → ∞, it tends to the standard normal dis-
tribution (as expected)8. Figure 14.4 shows the PDF of t-distribution
for several values of k.

By analogy with the Wald test, the size α t-test rejects H0 when

(cid:12)(cid:12)(cid:12)(cid:12) Xn − µ0

√

sn/

n

(cid:12)(cid:12)(cid:12)(cid:12) > tn−1,1− α

2

,

(14.20)

where tn−1,α plays the same role as zα plays in the Wald test, i.e. con-
trols the size α. More precisely, tk,α is such point that the probability
that the t random variable with k degrees of freedom is less than tk,α
is exactly α:

ftk (x)dx = α.

(14.21)

(cid:90) tk,α

−∞

When n is moderately large (say, n ≈ 30), the t-test is essentially
identical to the Wald test.

Further Reading

1. “Sometimes the most important step in creative work is simply to
ask the right question.” J.F. Box (1987) “Guinness, Gosset, Fisher,
and small samples,” Statistical Science, 2(1), 45-52 is a nice story
about the two men, one of whom invented the t-test and the other
generalized it so greatly.

What is Next?

Can we be more informative than simply reporting “reject” or “ac-
cept” when testing a hypothesis? Yes, we can. This leads to the cor-
nerstone concept of statistical inference, the p-value.

x-3-2-10123f(x)00.050.10.150.20.250.30.350.4t1t2t3N(0,1)15
P-values

The p-value is an iconic concept in statistics. First computations of
p-values date back to the 1770s, where they were used by Laplace.
The modern use of p-values was popularized by Fisher in the 1920s.
Nowadays, most of the research papers that use statistical analysis of
data report p-values. Yet quite shamefully, many researchers using
the p-value can’t even explain what exactly the p-value means1.
In this lecture, we will provide a rigorous statistical deﬁnition

of the p-value, its intuitive meaning, geometrical and probabilistic
interpretations, and an analytical recipe for computing p-values.

Deﬁnition of the p-value

Reporting “reject” or “accept” a hypothesis is not very informative.
Recall the two coin example from the previous lecture, Fig. 15.1.
Accepting the hypothesis that the coin is fair observing k = 3 heads
is much more comfortable than accepting it observing k = 6 heads.
To start with, we can always report the size α of the test used and
the decision the reject H0 or accept H0. Can we be more informative?
It turns out we can, and this leads to the concept of p-value.
Recall that the rejection region R has the following form:

1 C. Aschwanden (2016) “Statisticians
found one thing they can agree on:
it’s time to stop misusing p-values”
http://ﬁvethirtyeight.com/.

Figure 15.1: Binomial probabilities
i (1 − pi)n−k,
pk

Pi(k) =

(cid:18)n

(cid:19)

k

R = {X ∈ Ω : s(X) > c},

(15.1)

where n = 10, p0 = 0.5, and p1 = 0.7.

where Ω is the sample space2, s is the test statistic, and c is its critical
value. The critical value c is determined by the test size α, c = cα.
By varying α ∈ (0, 1), we generally obtain a one-parameter family of
nested rejection regions Rα
Rα ⊂ Rα(cid:48)
Rα → ∅,
Rα → Ω,

for α
as α → 0 (never reject),
as α → 1 (always reject).

(cid:48) > α,

3:

(15.2)

2 Set of all possible outcomes of data
X = (X1, . . . , Xn).

3 The intuition behind this is the follow-
ing. The size α is the largest possible
probability to reject H0 when it is true:
α = P(X ∈ Rα|H0). Increasing α leads
to inﬂating Rα: it becomes easier to
reject. The size α, therefore, controls the
size of the rejection region.

01234567891000.050.10.150.20.250.30.35Number of HeadsProbabiityCoin 0Coin 1This means that if a test rejects at size α, it will also rejects at size
α(cid:48) > α. Therefore, given the observed data X ∈ Ω, there exits the
smallest α at which the test rejects. This number is called the p-value:

p(X) = inf
α∈(0,1)

{α : X ∈ Rα}.

(15.3)

Geometric Interpretation and Intuitive Meaning

Schematically, the picture looks as follows:

statistical inference

91

Figure 15.2: The concept of p-value.
Here X is the observed data. If the
size α is too small, X /∈ Rα, and we
accept H0. Gradually increasing α, we
reach the moment where we reject H0.
The corresponding value of α is the
p-value. Reporting simply a particular
size and the corresponding decision
(reject/accept), gives only a point on
the bottom graph. Reporting the p-
value, determines the graph completely.

Intuitively, the p-value is a measure of the evidence against H0 pro-

vided by the data: the smaller the p-value, the stronger the evidence
4. Typically, researchers use the following evidence scale:
against H0

very strong evidence agianst H0,
p(X) < 0.01
p(X) ∈ (0.01, 0.05)
strong evidence agianst H0,
p(X) ∈ (0.05, 0.1) weak evidence agianst H0,

p(X) > 0.1

little or no evidence agianst H0.

(15.4)

To get accustomed to the new notion, let us compute the p-values

for several examples.

4 Indeed, if the p-value α∗ is small, than
the test of size α∗ is a) very conservative
about rejecting H0, and yet b) it rejects
H0 based on the obrained data.

92 k. m. zuev

Examples
Two coins. Here we have the data k ∼ Bin(n, θ), where k is the
number of heads in n = 10 trials and θ ∈ Θ = {0.5, 0.7}. We wish to
test

H0 : θ = θ0 = 0.5 versus H1 : θ = θ1 = 0.7.

(15.5)

The test statistic that we used last time is the ratio of likelihoods

(cid:18)n

(cid:19)

k

i (1 − θi)n−k,
θk

(15.6)

s(k) =

P1(k)
P0(k)

, where Pi(k) =

and the rejection region is5

R = {k : s(k) > c}.

(15.7)

Since s(k + 1) > s(k) (see Fig. 13.2), if k ∈ R, then k + 1 ∈ R. This
means the all rejection regions have the following intuitive form:

R = {kmin, . . . , n}, where kmin = 0, . . . , n, n + 1.

(15.8)

Here kmin = n + 1 corresponds to the empty rejection region. Let us
compute the size of the test with rejection region (15.8).

α = P(k ∈ {kmin, . . . , n}|θ = θ0) =

n∑
i=kmin

P0(i).

(15.9)

Figure 15.3 shows the dependence of α on kmin. Notice that be-
cause of the discreteness, we can’t construct a test of arbitrary size:
only sizes appeared on the y-axes of Fig. 15.3 are available.

Given k, to ﬁnd the p-value, we need to ﬁnd the smallest size
at which the test rejects H0. This smallest size corresponds to the
smallest rejection region (15.8) that contains k. This smallest rejection
region is R = {k, . . . , n}. And, thus, the p-value is

p(k) =

n∑

i=k

P0(i).

(15.10)

Figure 15.3 shows the p-value as a function of k. According to the
classiﬁcation (15.4), k = 10 provides very strong evidence against H0;
k = 9 provides strong evidence; k = 8 corresponds to weak evidence;
and k ≤ 7 corresponds to little or no evidence.
(cid:3)

Normal model Last time we constructed a test of size α for testing

where X1, . . . , Xn ∼ N (µ, σ2), where σ2 is known. The rejection
region was

H0 : µ ≤ 0 versus H1 : µ > 0,
(cid:27)

(cid:26)

(15.11)

Rα =

X : Xn >

σz1−α√
n

.

(15.12)

5 We used c = 1, which led to R =
{7, . . . , 10} and size α = 0.17. To
compute p-value, we need to consider a
family of rejection regions and ﬁnd the
smallest that contain our data.

Figure 15.3: The size α as a function of
the rejection region boundary kmin. As
expected, α = 1 corresponds to R = Ω
(kmin = 0) and α = 0 corresponds to
R = ∅ (kmin = 11).

Threshold kmin01234567891011Size ,00.20.40.60.81statistical inference

93

Geometrically, this region is a half-space in Ω = Rn. To ﬁnd the
smallest rejection region that contains the data X ∈ Rn, we must
require that X lies on the boundary of that rejection region. That is
the equation for p-value α∗ = p(X) is

Xn =

which leads to

,

σz1−α∗√
n
(cid:18)√

(cid:19)

.

(15.13)

(15.14)

p(X) = 1 − Φ

nXn
σ

Qualitatively, large positive values of Xn (strong evidence against H0)
(cid:3)
corresponds to small p-values.

The Wald Test As we know, the rejection region of the size α Wald test
is

(cid:111)

(cid:110)
(cid:12)(cid:12)(cid:12) is the Wald statistic. Given the data X, the

X ∈ Ω : W(X) > z1− α

,

2

(15.15)

where W(X) =
p-value is that value of α for which X lies exactly on the boundary of
Rα. So, to ﬁnd the p-value, we need to solve W(X) = z1− α
for α. This
leads to

2

Rα =
(cid:12)(cid:12)(cid:12) ˆθ(X)−θ0
(cid:98)se(X)

p(X) = 2Φ(−W(X)).

(15.16)
(cid:3)

Computing p-values

In general, if the rejection region of a test with size α has the form

Rα = {X : s(X) > cα},

(15.17)

6, then to ﬁnd the p-value α∗,
where cα is a decreasing function of α
we need to solve s(x) = cα∗ for α∗, where x is actually observed data.

6 Which means that Rα inﬂates with α.

Probabilistic Interpretation
The p-value is a certain value α∗ of the test size. Recall that by deﬁni-
tion

α

∗ = sup
θ∈Θ0
= sup
θ∈Θ0

P(X ∈ Rα∗|θ)

β(θ) = sup
θ∈Θ0
P(s(X) > cα∗|θ) = sup
θ∈Θ0

P(s(X) > s(x)|θ).

(15.18)

Hence, the p-value is the probability (under H0) of observing a value
of the test statistic more extreme that was actually observed.

94 k. m. zuev

Misinterpretations

Let us ﬁnish with two main misinterpretation of the p-values, which
often appear even in published research papers in respected journals.

• A large p-value is not a strong evidence in favor of H0.

A large p-value can occur for two reasons. First, indeed H0 is true.
Second, H0 is false, but the probability of the type II error (accept
H0 when it is false) is high (i.e. the power of the test is low).

• The p-value is not the probability that the null hypothesis is true.

The p-value is merely a measure of the evidence against H0. It is
not meant to be the measure of whether or not H0 is true. Rather it
is a measure of whether or not the data should be taken seriously.

Further Reading

1. In 2015 the journal Basic and Applied Social Psychology has banned
the use of p-values. The editors argued that, in practice, the use
of p-values is more misleading than informative. In particular,
p-values encourage lazy thinking: if you reach the magical p-
value< 0.05 then the null is false. Steven Novella discusses this
issue in his recent post “Psychology journal bans signiﬁcance
testing” on www.sciencebasedmedicine.org.

2. Recent critical literature on p-values is reviewed in B. Vidgen
& T. Yasseri (2016) “P-values: misunderstood and misused,”
arXiv:1601.06805. In particular, the difference between the p-value
and the False Discovery Rate (Lecture 19) is discussed.

What is Next?

We will discuss the permutation test, which is a nonparametric
method for testing whether two samples were generated by the same
data generation process.

16
The Permutation Test

The Wald test for simple hypothesis H0 : θ = θ0 assumes that the
estimate ˆθ of the parameter of interest is approximately normally dis-
tributed. The t-test assumes that the data itself comes from a normal
distribution. But what if these normality assumptions do not hold?

For example, recall comparing the means µ1 and µ2 of two popula-
tions: given two independent samples X1 . . . , Xn and Y1, . . . , Ym from
the populations, we want to test

H0 : ∆µ = µ1 − µ2 = 0 versus H1 : ∆µ (cid:54)= 0.
If n and m are small, then (cid:99)∆µ = Xn − Ym may not be normal.

(16.1)

The permutation test is a general nonparametric method for test-
ing whether two distributions are the same. It is completely free of
any normality assumptions, or any other distributional assumptions.
In the spirit, it is very similar to the bootstrap. Like bootstrap, it has
been known for awhile1, but became popular only with availability of
cheap computing power.

Let X1 . . . , Xn ∼ FX and Y1 . . . , Ym ∼ FY be two independent
samples from two populations, and H0 is the hypothesis that two
populations are identical2. Namely, we want to test

H0 : FX = FY versus H1 : FX (cid:54)= FY.

(16.2)

Let s(X; Y) = s(X1 . . . , Xn; Y1 . . . , Ym) be some test statistic that
discriminates between the null and alternative3, and, as usual, we
reject H0 if s(X; Y) is large enough.

If H0 is true, then all N = n + m random variables that constitute
the data X1 . . . , Xn, Y1 . . . , Ym come from essentially one population:

Under H0 : X1 . . . , Xn, Y1 . . . , Ym ∼ F = FX = FY.

(16.3)

This means that, conditional on the observed values, any of the N!
permutations of the data has the same probability of being observed.

1 “the statistician does not carry out this
very simple and very tedious process,
but his conclusions have no justiﬁcation
beyond the fact that they agree with
those which could have been arrived at
by this elementary method,”

Fisher (1936).

2 This is the type of hypothesis we
would consider when testing wheter a
treatment differes from a placebo.

3 For example, s(X; Y) = |Xn − Ym|,
or the Kolmogorov-Smirnov statistic
s(X; Y) = supx | ˆFX,n(x) − ˆFY,m(x)|.

96 k. m. zuev

Let s1, . . . , sN! denote the values of the test statistic computed for all
permutations of the data4. Then, under the null hypothesis, all these
values are equally likely. The distribution P0 that puts mass 1
each si is called the permutation distribution of s.

N! on

4 One of these values is what we actu-
ally observe, sobs.

Recall that the p-value is the probability (under H0) of observing
a value of the test statistic more extreme that was actually observed.
The p-value of the permutation test is then
N!∑

p-value = P0(s > sobs) =

I(si > sobs).

1
N!

i=1

(16.4)

In most cases, N = n + m is large enough so that summing over

all permutations in (16.4) is infeasible. In this case we can simply
approximate the p-value using the Monte Carlo method, that is by
using a random sample of permutations. This leads to the following
algorithm for testing (16.2):
1. Compute the observed value of the test statistic

sobs = s(X1 . . . , Xn; Y1 . . . , Ym).

(16.5)

2. Randomly permute the data. That is pick a permutation5

π at

random and deﬁne

5 Racall that a permutation of N el-
ements is a one-to-one map from
{1, . . . , N} to itself.

Zπ = (Zπ(1), . . . , Zπ(N)), where
Z = (Z1, . . . , ZN) = (X1, . . . , Xn; Y1, . . . , Ym).

3. Compute the statistic for the permuted data:

sπ = s(Zπ).

4. Repeat the last two steps K times and let s1, . . . , sK denote the

resulting statistic values.
5. The estimated p-value is6

p-value ≈ 1
K

K∑

i=1

I(si > sobs).

(16.6)

(16.7)

(16.8)

The permutation test is especially useful for small samples, since

for large samples, the normality assumptions used in parametric tests
usually hold and the tests give similar results.

Example: Hot Wings
Carleton student Nicki Catchpole conducted a study of hot wings
consumption at the Williams bar in the Uptown area of Minneapolis.
She asked patrons at the bar to record the consumption of hot wings
during several hours. One of the questions she wanted to investigate
is whether or not gender had an impact on hot wings consumption.

6 The number of permutations K is a
trade-off between the accuracy and
computer time. The more permutation
the better. It is suggested to use K of the
order K ∼ 104, 105.

Figure 16.1: Hot Chicken Wings. Photo
source: losangeles.com.

statistical inference

97

7 Available at wings.xlsx.

Figure 16.2: Boxplots of hot wings
consumption by Females and Males at
the Williams bar, MN.

Figure 16.3: Histogram of the obtained
values of the test static (16.10) for
K = 105 data permutations.

Figure 16.4: Histogram of the obtained
values of the test static (16.11) for
K = 105 data permutations.

The data7 obtained in the course of this study consists of N = 30

observations: X1, . . . , Xn are Y1, . . . , Ym, where X’s and Y’s corre-
spond to males and females, and n = m = 30. The boxplots of the
data are shown in Fig. 16.2.

The sample means for males and females are clearly different:

µM = 14.53 and µF = 9.33. But could this difference arise by chance?
In other words, could it be the case that the male and female con-
sumptions are the same, and the difference that we observe is simply
due to high variability of the consumption? Let us test this using the
permutation test.

So, we assume that X1, . . . , Xn ∼ WM, Y1, . . . , Ym ∼ WF, and
our null hypothesis is that the two populations (male and female
consumptions of hot wings) are identical:

H0 : WM = WF versus H1 : WM (cid:54)= WF.

(16.9)

Let us use the absolute value of the sample means

s(X; Y) = |Xn − Ym|,

(16.10)

as a test statistic. The observed value is sobs = 5.2

Figure 16.3 shows the histogram of the statistic values (16.10)

obtained from K = 105 random permutations of the original data. As
we can see, the observed value of the statistic is quite extreme. The
corresponding estimated p-value is 0.0017, which means that the data
provided quite strong evidence against the hypothesis that males and
females consume hot wigs in equal amounts.

What if instead of (16.10), we will use the Kolmogorov-Smirnov

statistic:

s(X; Y) = sup
x

| ˆFX,n(x) − ˆFY,m(x)|.

(16.11)

Figure 16.4 shows the distribution of the statistic values in this

case. Again, the observed value sobs = 0.53 is extreme. The estimated
p-value 0.0062 is a bit larger than in the previous case, but still small
enough to reject the null.
Further Reading
1. Introductory texts on statistical inference rarely cover permutation

tests. And yet permutation procedures are the primary methods
for testing hypothesis in many application areas, especially in
biostatistics and genetics. The book P. Good (2005) Permutation,
Parametric, and Bootstrap Tests of Hypothesis is highly recommended.

What is Next?
We will discuss the maximum likelihood ratio test which plays the
same role in testing as the MLE plays in estimation.

FemaleMaleNumer of Hot Wigs Consumed5101520-10123456700.10.20.30.40.50.6Histogram of statistic values for permuted dataObservedvalue00.10.20.30.40.50.60.70.80123456Histogram of statistic values for permuted dataObservedvalue17
The Likelihood Ratio Test

In this lecture, we will discuss the likelihood ratio method for
testing hypotheses, which plays the same role as the maximum like-
lihood estimates play in point estimation. The likelihood ratio tests
are as widely applicable as MLEs and are one of the most popular
methods of testing in parametric settings.

Likelihood Ratio Test for Simple Hypotheses

Let us ﬁrst consider a simple case where both the null and alternative
hypotheses are simple. Namely, suppose that X1, . . . , Xn is modeled
as a sample from f (x; θ), where θ ∈ Θ = {θ0, θ1}, and we wish to test

H0 : θ = θ0 versus H1 : θ = θ1.

(17.1)

Recall that the likelihood function, which is the join probability/density
of the data viewed as a function of the parameter,

L(θ|X) =

n∏

i=1

f (Xi; θ),

(17.2)

measures the consistency of the parameter θ and the observed
data. If L(θ1|X) > L(θ0|X), then it is more likely that the data
X = {X1, . . . , Xn} was generated by f (x; θ1) and vice versa. This
motivates the likelihood ratio test (LRT):

Reject H0 ⇔ λ(X) =

L(θ1|X)
L(θ0|X)

> c,

(17.3)

where c is some critical value1. The statistic λ is called the likelihood
ratio statistic.

Note that the LRT is exactly the test Bob used in the two coin

example (Lecture 13). Let us consider one more classical example.

1 Which is, as usual, found from the
requirement for the size α of the test.

statistical inference

99

2 If the variance is unknown, then (17.4)
are no longer simple hypotheses.

(17.4)

(17.5)

Example: Normal LRT. Let X1, . . . , Xn ∼ N(µ, σ2), where σ2 is
known2, and let us test

where µ1 > µ0. The likelihood function is

H0 : µ = µ0 versus H1 : µ = µ1,

(cid:18)−(Xi − µ)2
(cid:18)

(cid:19)
2σ2
i=1(Xi − µ)2
− ∑n

n∏

i=1

exp

1√
2πσ

(cid:19)n

(cid:18) 1√
(cid:18) ∑n
i=1(Xi − µ0)2 − ∑n

2πσ

exp

2σ2

(cid:19)

.

(cid:19)

L(µ|X) =

=

L(µ1|X)
L(µ0|X)
(cid:40)

R =

The likelihood ratio statistic is then

= exp

λ(X) =
After some algebra, the rejection region of the LRT {X : λ(X) > c}
reduces to

2σ2

. (17.6)

X : Xn > c(cid:48) =

2σ2 log c + n(µ2
2n(µ1 − µ0)

(17.7)

i=1(Xi − µ1)2
(cid:41)

1 − µ2
0)

,

which looks intuitive: the test rejects H0 when Xn (which is supposed
to be around µ0 under H0) is large enough3. The critical value c(cid:48) is
determined from the condition on the size α:

n

>

σ/

√

(cid:19)

c(cid:48) − µ0
√
n
σ/

(cid:12)(cid:12)(cid:12)(cid:12) µ = µ0

(cid:18) Xn − µ0
(cid:17)
. The previous equations which
σz1−α√
n

.

(17.8)

(17.9)

µ0, σ2
n
c(cid:48) = µ0 +

(cid:18) c(cid:48) − µ0

(cid:19)
α =P(Xn > c(cid:48)|µ = µ0) = P
where we used that Xn ∼ N(cid:16)

= 1 − Φ

√

σ/

n

,

leads to

To sum up, the size α LRT4

Rejects H0 ⇔ Xn > µ0 +

σz1−α√
n

.

(17.10)
(cid:3)

We know that in general, ﬁnding the most powerful test is a

daunting task. It turns out however, that in the special of simple
null and simple alternative (17.1), the LRT is the most powerful test5.
Theorem 6 (Neyman-Pearson Lemma). Let X1 . . . , Xn is modeled as a
random sample from a distribution with parameter θ. Suppose we wish to
test H0 : θ = θ0 vesus H1 : θ = θ1. The size α LRT is the most powerful
test of size α. That is, among all tests with size α, the LRT has the largest
power β(θ1) (i.e. the smallest probability of the type II error).

In particular, the test (17.10) is the most powerful for testing (17.4),

and Bob did his best in testing the hypothesis that the coin is fair.

3 If µ1 < µ0, the rejection region would
be of the form {Xn < c(cid:48)(cid:48)}.

4 What is the p-value of this test?

5 J. Neyman & E.S. Pearson (1933) “On
the problem of the most efﬁcient tests
of statistical hypotheses,” Philosophical
Transactions of the Royal Society A:
Mathematical, Physical and Engineering
Sciences, 231 (694-706): 289-337.

Figure 17.1: Jerzy Neyman, Polish-
American mathematician, and Egon
Pearson, leading British statistician.

100 k. m. zuev

Likelihood Ratio Test: General Case

Let us now consider a general case, where the hypothesis are not
necessarily simple6. That is, suppose that X1, . . . , Xn is modeled as a
sample from f (x; θ), where θ ∈ Θ = Θ0 (cid:116) Θ1, and we wish to test

H0 : θ ∈ Θ0 versus H1 : θ ∈ Θ1.

(17.11)

We need to generalize the deﬁnition of the likelihood ratio statistic

(17.3) because now it does not makes sense: we have sets Θ0 and
Θ1 instead of points θ0 and θ1. In general case, the likelihood ratio
statistic is deﬁned as follows:

λ(X) =

supθ∈Θ L(θ|X)
L(θ|X)
supθ∈Θ0

.

(17.12)

6 In this case, they are often called
composite.

Looking at (17.3), you might have expected to see ˜λ(X) =

supθ∈Θ
supθ∈Θ
In practice, these two statistics often have similar values7. But theo-
retical properties of the statistic (17.12) are much simpler and nicer.

0

1

L(θ|X)
L(θ|X) .

7 Notice that λ = max{1, ˜λ}.

Large values of λ(X) provide evidence against the null hypothesis.

Indeed, if λ(X) is large, then the value of parameter θ most consis-
tent with the observed data does not lie in Θ0. So, the LRT8:

Rejects H0 ⇔ λ(X) > c.

(17.13)

8 Sometimes this test is called the
generalized likelihood ratio test.

Since λ(X) ≥ 1, the critical value c should be also c ≥ 1.

If we think of maximization over Θ and Θ0, then the close rela-

tionship between LRTs and MLEs become clear. Let ˆθ be the MLE of
θ, and ˆθ0 be the MLE when θ is required to lie in Θ0 (i.e. when we
consider Θ0 as the full parameter space). Then λ can be written as
follows:

L( ˆθ|X)
L( ˆθ0|X)

λ(X) =

.

(17.14)

The Neyman-Pearson Lemma says that the likelihood ratio tests
are optimal for simple hypotheses. For composite hypothesis, the
LRTs are generally not optimal9, but perform reasonable well10. This
explains the popularity of LRTs.

Example: Normal GLRT. Let X1, . . . , Xn ∼ N (µ, σ2), where variance
σ2 is known. Consider testing the following hypothesis:

H0 : µ = µ0 versus H1 : µ (cid:54)= µ0.

(17.15)

Here, Θ = R, Θ0 = {µ0}, and Θ1 = (−∞, µ0) ∪ (µ0, ∞). As in the
previous example, the likelihood is given by (17.5). The likelihood

9 But often the most powerful test
simply do not exist.
10 Just like MLEs.

=

L( ˆµMLE|X)
L(µ0|X)

(cid:0)(Xi − µ0)2 − (Xi − Xn)2(cid:1)

L(Xn|X)
(cid:33)
L(µ0|X)

=

ratio statistic is

i=1

= exp

= exp

λ(X) =

supµ∈Θ L(µ|X)
(cid:32) ∑n
L(µ|X)
supµ∈Θ0
(cid:18) n(Xn − µ0)2
(cid:12)(cid:12)(cid:12)(cid:12) Xn − µ0
Since under the null Xn ∼ N(cid:16)

we need to set c(cid:48) = z1− α
coincides with the Wald test11.

2σ2

√

σ/

n

2

.

2σ2

(cid:19)
(cid:12)(cid:12)(cid:12)(cid:12) > c(cid:48) =(cid:112)2 log c.

(cid:17)

Rejecting when λ(X) > c is equivalent to rejecting when

statistical inference

101

(17.16)

(17.17)

(cid:3)

11 How would you construct the size
α LRT test if σ is unknown? Does it
remind you any other test?

µ0, σ2
n

, to construct the size α test,
. So, in this example, the LRT essentially

Null Distribution of λ(X)

In order to construct the LRT of size α, we need to ﬁnd the critical
value c from the following equation:

α = sup
θ∈Θ0

P(λ(X) > c).

(17.18)

To compute the probability of the type I error on the right-hand side,
we need to know the null distribution12 of the LRT statistic λ(X). Let
us look at the previous example: under H0

(cid:18) Xn − µ0

√

σ/

n

(cid:19)2 ∼ χ2

1,

2 log λ(X) =

(17.19)

where χ2

1 is the χ2-distributions with 1 degree of freedom13.

It turns out that similar result holds in a more general case. As-

suming that the probability model f (x; θ) satisﬁes certain regularity
conditions, the null distribution of 2 log λ(X) tends to χ2-distribution
with q degrees of freedom as the sample size n → ∞:

2 log λ(X)

D−→ χ2

q, where q = dim Θ − dim Θ0,

(17.20)

where dim Θ and dim Θ0 are the numbers of free parameters in Θ
and Θ0. For instance, in the previous example, Θ = R, dim Θ = 1,
Θ0 = {µ0}, dim Θ0 = 0.

The result (17.20) may appear counter intuitive at the ﬁrst glance:
indeed, how come that regardless of the probability model for data
X (as long as it is smooth enough), the LRT statistic 2 log λ(X) con-
verges to the same χ2-distribution. So let us give

12 That is, the distribution under H0.

13 Recall, that if Z1, . . . , Zq are i.i.d.
standard normal variables, then the the
distribution of Q = ||Z||2 = Z2
1 + . . . Z2
q
is called the χ2-distribution with q
degrees of freedom.

Figure 17.2: The PDF of the χ2-
distribution with q degrees of freedom.

x012345678910PDF of the @2-distribution00.10.20.30.40.50.60.70.80.91q=1q=2q=3q=4q=5102 k. m. zuev

for a special case: Θ = R, Θ0 = {θ0}, q = 1.
Sketch of Proof of (17.20)
2 log λ(X) = 2 logL( ˆθMLE) − 2 logL(θ0) = 2l( ˆθMLE) − 2l(θ0), (17.21)

where l(θ) is the log-likelihood. Using the Taylor expansion of l(θ) at
θ = ˆθMLE, we have:
l(θ) ≈ l( ˆθMLE) + (θ − ˆθMLE)l(cid:48)( ˆθMLE) +
(θ − ˆθMLE)2
l(cid:48)(cid:48)( ˆθMLE).
(cid:32)

(θ − ˆθMLE)2

= l( ˆθMLE) +

l(cid:48)(cid:48)( ˆθMLE)

Therefore,

(cid:33)

(17.22)

2

2

(θ0 − ˆθMLE)2

l(cid:48)(cid:48)( ˆθMLE)

2 log λ(X) ≈ 2l( ˆθMLE) − 2

l( ˆθMLE) +
= −l(cid:48)(cid:48)( ˆθMLE)(θ0 − ˆθMLE)2.

2

(cid:17)

(17.23)

D−→ N(cid:16)
(cid:18)

Under H0, the true value of the parameter is θ0. Recall that the MLE
is asymptotically normal, ˆθMLE
2 log λ(X) ≈ − l(cid:48)(cid:48)( ˆθMLE)
nI(θ0)
= − l(cid:48)(cid:48)( ˆθMLE)
nI(θ0)

(θ0 − ˆθMLE)

. Therefore,

(cid:19)2

nI(θ0)

(cid:113)

nI(θ0)

Z2
n,

θ0,

1

(17.24)

D−→ N (0, 1). We are almost there. Since the MLE is consis-
where Zn
tent, it converges to the true value of the parameter, with is θ0 under
P−→ l(cid:48)(cid:48)(θ0). In lecture 10,
H0, i.e. ˆθMLE
while discussing the asymptotic normality of the MLE, we obtained
that − l(cid:48)(cid:48)(θ0)

nI(θ0) ≈ 1. Combining these results, we ﬁnally have that

P−→ θ0. Therefore, l(cid:48)(cid:48)( ˆθMLE)

2 log λ(X)

D−→ (N (0, 1))2 = χ2
1.

(17.25)

Thus, in essence, the null distribution of the LRT statistic is a conse-
quence of the nice analytical properties of the MLE.

(cid:3)

Approximate p-value of the LRT
Using the asymptotic result (17.20), it is straightforward to derive
the approximate p-value of the LRT. Indeed, if the sample size is
sufﬁciently large, then 2 log λ(X) ·∼ χ2

q, and the test size is therefore

α = P(λ(X) > c) = P(2 log λ(X) > 2 log c),

which means that

2 log c ≈ χ2

q,1−α,

(17.26)

(17.27)

q,α is such point that the probability that the χ2-random

where, χ2
variable with q degrees of freedom is lass than χ2
LRT with the rejection region

q,α is α

14. Thus, the

(cid:40)

(cid:33)(cid:41)

(cid:32) χ2

q,1−α
2

Rα =

X : λ(X) > exp

(17.28)

statistical inference

103

(cid:113)

14

q,α is the χ2 analog of zα.
χ2
1,α = −z 1−α
χ2
Check that

.

2

has approximate size α.

The p-value is smallest (inﬁmum) size α∗ at which the test rejects.

The approximate p-value is thus the solution of

(cid:32) χ2

q,1−α∗
2

(cid:33)

,

λ(X) = exp

which is

∗ = P(Y > 2 log λ(X)),

α

Y ∼ χ2
q,

where X is the actually observed data.

(17.29)

(17.30)

Further Reading

1. I encourage you to read (or at least to look through) the original
paper J. Neyman & E.S. Pearson (1933) “On the problem of the
most efﬁcient tests of statistical hypotheses,” Philosophical Trans-
actions of the Royal Society A: Mathematical, Physical and Engineering
Sciences, 231 (694-706): 289-337. This is a very good literature with
beautiful illustrations.15.

What is Next?

We will use the LRT to test Mendel’s theory of inheritance.

15 Notice the vintage terminology and
notation. They call “character” and
“elementary probability” what we now
call “test statistic” and “probability
density function”, and they spell
coordinates as “co-ordinates.”

18
Testing Mendel’s Theory

Here we will statistically test the theory of inheritance proposed by
Gregor Mendel, after he experimented with pea plants 1. We focus on
his third law: the principle of independent assortment, which states that
alleles for different traits are distributed uniformly at random to the
offspring. Using Mendel’s data, we will construct the likelihood ratio
test and compute the corresponding p-value.

Figure 18.1: Gregor Mendel, a scientist
and a monk, the father of modern
genetics. Photo source: britannica.com
1 A brief interactive introduction to
Mendelian inheritance is available at
wiley.com.

Mendel’s Peas

Suppose we are going to breed peas with round yellow seeds and
wrinkled green seeds. Then, according to the Mendel’s principle of
dominance, all of the offspring in the ﬁrst generation will be round
and yellow, since yellow trait is dominant to green and round trait
is dominant to wrinkled. If we now allow the offspring of the ﬁrst
generation to self-fertilize, then we will get all four types of progeny:
round yellow, round green, wrinkled yellow, and wrinkled green.
Moreover, Mendel’s principle of independent assortment predicts the
proportions of each type. Namely
3
16

(18.1)
respectively. The breeding process is schematically shown in Fig. 18.2.

9
16

3
16

,

1
16

,

,

,

The Data
In his original paper2, Mendel described the results of his exper-
iments, and, in particular, he reported that in N = 556 trials he
observed

round and yellow,
round and green,

n1 = 315
n2 = 108
n3 = 101 wrinkled and yellow,
n4 = 32
wrinkled and green.

(18.2)

2 G. Mendel (1866), “Versuche über
Pﬂanzen-Hybriden,” Verh. Naturforsch.
Ver. Brünn, 4: 3-47. For the English
translation, see: W. Bateson (1901).
“Experiments in plant hybridization.”

statistical inference

105

Figure 18.2: Mendel’s Principle of
Independent Assortment. Picture
source: wiley.com.

Let us test whether or not Mendel’s theoretical prediction (18.1) is
consistent with the observed data (18.2). Based on the data, should
we accept his theory or reject it?

Probability Model for the Data

In order to quantitatively answer the question on how plausible or
unlikely the observed data under Mendel’s theory, we need to inject
some stochasticity in the picture. Namely, we need to assume some
probability model for the data3. What is a natural model for the
observed numbers of peas of different types?

Multinomial Distribution

As the name suggests, the multinomial distribution is a straight-
forward generalization of the binomial distribution. Recall, that the
binomial distribution Bin(n, p) is the discrete probability distribu-
tion of the number of successes in a sequence of n independent suc-
cess/failure experiments, each of which has success with probability
p4. Multinomial distributions is a generalization for the case where
there are more than two possible outcomes and a “success-failure”
description is insufﬁcient to understand the underlying system or
phenomenon5.

Consider drawing a ball from a box which has balls with k differ-

ent colors labeled 1, 2 . . . , k. Let p = (p1, . . . , pk), where pi is is the

3 Without probability model, the data
are just numbers. With probability
model, these numbers are a sample
from probability distribution, which
allows to us to use the machinery of
probability theory to make quantitative
statements.

4 That is, Bin(n, p) is the distribution
if the sum on n i.i.d. Bernoulli trials
Bernoulli(p).

5 For example, temperature can be
“high,” “medium,” or “low,” or, as in
the Mendel experiments, the seeds can
be round yellow, round green, wrinkled
yellow, and wrinkled green.

6 Independent draws with replacement.

106 k. m. zuev

probability of drawing a ball of color i,
k∑

pi ≥ 0,

pi = 1.

i=1

(18.3)

Let us draw N times6, and let n = (n1, . . . , nk), where ni is the num-
ber of times that color i appeared,
k∑

ni = N.

(18.4)

We say that n has a multinomial distribution, n ∼ Mult(N, p), with
parameters N, number of trials, and p = (p1, . . . , pk), vector of proba-
bilities of k different outcomes.

Here is a couple of properties of the multinomial distribution:

• The probability mass function of Mult(N, p) is

f (n|N, p) =

N!

n1! . . . nk!

1 . . . pnk
pn1
k .

(18.5)

• The marginal distribution of ni is Bin(N, pi).

Problem Formulation

i=1

It seems very natural to model the observed data (18.2) as a sample
from the multinomial distribution:
n = (n1, . . . , nk) ∼ Mult(N, p), where k = 4 and N = 556.
The Mendel theory, the null hypothesis, is then
1
16

H0 : p = p∗ =

(cid:18) 9

3
16
and we want to test versus the alternative
H1 : p (cid:54)= p∗.

(cid:19)

16

3
16

,

,

,

,

The full parameter space is Θ = {(p1, . . . , pk) : pi ≥ 0, ∑ pi = 1},
which is geometrically a 3-simplex (i.e. tetrahedron), and Θ0 = {p∗}.

(18.6)

(18.7)

(18.8)

Constructing the LRT

The ﬁrst step in constructing the LRT is to ﬁnd the likelihood func-
tion, which is in this case is simply
L(p) = f (n|N, p) =

N!

1 . . . pnk
pn1
k .

(18.9)

n1! . . . nk!

The LRT statistic is then

λ(n) =

supp∈Θ L(p)
L(p)
supp∈Θ0

L( ˆpMLE)
L(p∗)

.

=

To proceed, we need to ﬁnd the MLE of p.

(18.10)

statistical inference

107

The MLE of p

The log-likelihood is

l(p) = logL(p) = log N! − k∑

i=1

log ni! +

k∑

i=1

ni log pi.

(18.11)

The MLE ˆpMLE is thus the solution of the following constrained opti-
mization problem:

ni log pi −→ max,

k∑

i=1

subject to

pi = 1.

i=1

k∑
(cid:16) n1

N

(cid:17)

.

nk
N

(18.12)

(18.13)

The solution is readily obtained by the method of Lagrange multipli-
ers:

ˆpMLE =

, . . . ,

Computing the p-value
Using (18.13), we can now compute the LRT statistic:

(cid:0) n1
(cid:1)n1 . . .(cid:0) nk
1)n1 . . . (p∗
n1!...nk! (p∗

n1!...nk!

N!

N!

N

N
k )nk

(cid:1)nk

(cid:19)ni

(cid:18) ni

Np∗
i

=

k∏

i=1

λ(n) =

.

(18.14)

Recall7 that for large N, the null distribution of 2 log λ(n) is approx-
imately the χ2-distribution with q = dim Θ − dim Θ0 degrees of
freedom. In our case, dim Θ = 3, dim Θ0 = 0, and, thus q = 3.

The p-value is then8

(cid:32)

(cid:33)

k∑

i=1

ni log

ni
Np∗
i

p(n) = P(Y > 2 log λ(n)) = P

Y > 2

For Mendel’s data (18.2), we have:

p(n) = P(Y > 0.475) = 0.92.

, Y ∼ χ2
3.

(18.15)

(18.16)

7 See Lecture 17.

8 See Lecture 17.

This is a huge p-value9. This means that the Mendel data does not
provide evidence for rejecting Mendel’s theory. As expected.

Further Reading

1. In 1866 Mendel published his seminal paper containing the foun-

dations of modern genetics, where he reported the data that we
analyzed in this lecture. In 1936 Fisher published a statistical anal-
ysis of Mendel’s data concluding that “the data of most, if not all,

9 In fact, this p-value is so large that
there is some controversy about
whether Mendel’s results are “too
good” to be true. See the reference in
the next section “Further Reading.”

108 k. m. zuev

of the experiments have been falsiﬁed so as to agree closely with
Mendel’s expectations.” A recent paper A.M. Pires & J.A. Branco
(2010) “A statistical model to explain the Mendel-Fisher contro-
versy,” Statistical Science, 25(4): 545-565 provides a brief history of
the controversy and offers a possible resolution, which suggests
that perhaps Mendel performed several experiments, but reported
only the results that best ﬁt his theory.

What is Next?

There are applications where we need to to test thousands or even
millions of hypotheses. For any one test, the chance of a false rejec-
tion10 may be small α (cid:28) 1, but the chance of at least one false rejec-
tion may still be large. This is called the multiple testing problem. In
the next lecture, we will discuss how to deal with it.

10 i. e. the probability of the type I error.

19
Multiple Testing

There are applications where one needs to perform multiple
testing, that is, to conduct several hypotheses tests simultaneously.
The basic paradigm for single-hypothesis testing dictates: ﬁx the
maximum acceptable value α for the the type I error probability1,
and then search for the test with the lowest type II error probabil-
ity2. When testing multiple hypotheses, the situation in more subtle
since each test has type I and type II errors, and it becomes unclear
how to measure the overall error rate and how to control it3. In this
lecture we will introduce two popular measures: the family-wise er-
ror rate (FWER) and the false discovery rate (FDR), and two methods
for controlling these measures: the Bonferroni correction and the
Benjamini-Hochberg algorithm.

Multiple Testing Problem

Let us ﬁrst fully appreciate the importance of the multiple testing
problem. Suppose a pharmaceutical company is testing a new drug
for efﬁcacy:

H0 : no effect versus H1 : effect

(19.1)

They performed a test with type I error probabiity α((cid:28) 1) on a
whole population and the data forced them to accepted the null: the
beneﬁt of the drug was not4 statistically signiﬁcant. Regardless of
this failure, they could repeat the test for several subpopulation5.
The probability of making at least one type I error among the family
of hypotheses tests is called the family-wise error rate (FWER). Let us
compute it assuming there are m tests, all tests are independent, and

1 That is, ﬁx the size of the test.

2 That is, the most powerful test.

3 In other words, it is not clear what is
the analog of α in multiple testing.

4 Unfortunately for the company.
5 For example, males, females, children,
students, etc.

110 k. m. zuev

have the same type I error probability α:

FWER = P(at least one type I error) = 1 − P(no type I errors)
= 1 − P(no type I error in test 1, . . . , no type I error in test m)
= 1 − m∏
= 1 − m∏

(1 − P(type I error in test i)) = 1 − (1 − α)m.

P(no type I error in test i)

i=1

i=1

(19.2)

Thus, even if α is small for each individual test, considering suf-
ﬁciently large number of tests m, it is possible to make FWER very
large. For example, if α = 0.05 and m = 100, then FWER ≈ 0.99,
meaning that almost certainly the company will obtain at least one
false rejection6. Purely by chance. The corresponding subpopulation
may be reported as the one for which the drug produces the desired
effect... This situation does not look good.

Bonferroni Correction

The ﬁrst idea that comes in mind is that instead of ﬁxing α for each
individual test, we need to ﬁx the overall FWER. Since α (cid:28) 1,

FWER ≈ mα.

(19.3)

6 This effect can be formulated in coin-
tossing language: if we toss a coin, no
matter how strongly biased against
heads, long enough, sooner a later we
will observe heads.

Therefore, if we wish to get FWER = α, the new value of α for each
individual test must be

(19.4)

α (cid:57)(cid:57)(cid:75) ˜α =

α
m

.

This method of controlling the overall error rate is called the Bonfer-
roni correction: if we run m tests and want the FWER to be α, then the
type I error for each test should be set to α
m .

The Bonferroni method of controlling the FWER is historically the

ﬁrst attempt to deal with the multiple testing problem. It has two
main drawbacks.
1. Technical: In practice, it is often too conservative: the corrected
sizes ˜α are much smaller than they need to be7. Let us explain
why. In practice, tests are rarely independent and

P(no type I error in test 1, . . . , no type I error in test m)
(cid:29) m∏

P(no type I error in test i).

i=1

This results into

FWER (cid:28) m˜α = α.

(19.5)

(19.6)

7 Especially if the number of tests m is
large.

Thus, the true FWER will be signiﬁcantly less then the prescribed
value α. The tests will be unwilling to reject raising the number of
false acceptances (type II errors).

2. Conceptual: In many applications, especially in exploratory analy-
sis, one is more interested in ﬁnding potentially interesting effects,
i.e. having mostly true rejections and maybe a few false ones,
rather than guarding against one or more false rejections8. This
led to a new measure, called false discovery rate (FDR), which is de-
signed to for this kind of applications and allows to maintain the
overall rate of false rejections (type I errors) without inﬂating the
rate of false acceptances (type II errors).

False Discovery Rate

statistical inference

111

8 For example, DNA microarrays mea-
sure the expression levels of thousands
of genes simultaneously. An important
problem is to identify genes that are
differently expressed in different bio-
logical conditions (e. g. different types
of cancer). In this context, failing to
identify truely differentially expressed
genes is a major concern.

Consider the problem of testing simultaneously m null hypotheses:
0 , . . . , H(m)
H(1)
ing tests. Suppose that we reject H(i)
0
The question is how to chose the threshold?

. Let p1, . . . , pm denote the p-values for the correspond-

0

if pi is below some threshold.

Let us introduce some notation:

• m0 is the number of true null hypotheses (unknown).
• m1 is the number of false null hypotheses9 (unknown).
• R f is the number of false rejections, i.e. the number of type I errors

(unobservable random variable).

• Rt is the number of true rejections (unobservable random vari-

able).

• R is the total number of rejections10 (observable random variable).
• A f is the number of false acceptances, i.e. the number of type II

errors (unobservable random variable).

• At is the number of true acceptances (unobservable random vari-

able).

9 m1 = m − m0.

10 R = R f + Rt.

• A is the total number of acceptances11 (observable random vari-

11 A = A f + At, A + R = m.

able)

The following table summarizes the error outcomes.

Accepted Rejected Total

True nulls

False nulls

Total

At

A f

A

R f

Rt

R

m0

m1

m

112 k. m. zuev

Note that in this notation, the FWER is simply

FWER = P(R f ≥ 1).

(19.7)

In the language of p-values, the Bonferroni method can be formu-
lated as follows: for all i = 1, . . . , m,

α
.
m
It can be shown that this guarantees FWER ≤ α

0 ⇔ pi <

Reject H(i)

12.

In their seminal paper13, Benjamini and Hochberg deﬁned the

false discovery rate as the expected value of the proportion of false
rejection among all rejections:

(19.8)

(cid:20) R f

(cid:21)

R

FDR = E

.

(19.9)

m is sometimes called the

12 The value α
Bonferroni threshold.
13 Y. Benjamini & Y.Hochberg (1995)
“Controlling the false discovery rate:
a practical and powerful approach to
multiple testing,” Journal of the Royal
Statistical Society. Series B (Methodologi-
cal), 57(1): 289-300.

This formula assumes R > 0. If R = 0, then obviously FDR = 0.
It should be clear why “false” and why “rate.” Why “discovery”?
A true rejection of the null hypothesis, which represents a current
theory or belief, is considered as a discovery.

To keep the FDR below a certain acceptable value α, the following

algorithm can be used.

The Benjamini-Hochberg Algorithm for controlling FDR

1. Let p(1) ≤ . . . ≤ p(m) be the ordered p-values, and denote H((i))

0

the null hypotheses corresponding to p(i).
2. Let i∗ be the largest i for which p(i) ≤ α
m
p-values are independent and βm = ∑m
i=1

(cid:26)

i
βm , where βm = 1 if the
1
i otherwise.

(cid:27)

i∗ = max

i = 1, . . . , m : p(i) ≤ α
m

i
βm

.

(19.10)

3. Reject all H((1))

for
which pi < p(i∗). The p-value p(i∗) is called the BH threshold.

. In other words, reject all H(i)
0

0

, . . . , H((i∗))

0

It can be shown14 that in this case,

FDR ≤ α.

(19.11)

14 The proof is out of our scope and can
be found in the original paper.

Example

Suppose that we performed m = 10 independent hypothesis tests and
obtained the following (ordered) p-values:

p(1) = 0.007, p(2) = 0.012, p(3) = 0.014, p(4) = 0.021, p(5) = 0.024,
p(6) = 0.033, p(7) = 0.04, p(8) = 0.065, p(9) = 0.073, p(10) = 0.08.

(19.12)

These p-values as well as the Benferroni and Benjamini-Hochberg
rejection thresholds are shown in Fig. 19.1.

If we tested at level α

statistical inference

113

Figure 19.1: Uncorrected testing vs.
Bonferroni vs Benjamini-Hochberg.

without doing any corrections for multiple testing, we would reject
all tests whose p-values are less then α. In this example, α = 0.05,
so we would reject seven null hypotheses with the smallest p-values.
The Bonferroni method rejects all nulls whose p-values are less than
α/m. In this example, α/m = 0.005 and none hypotheses are rejected.
The BH threshold corresponds to the last p-value that falls under
the line with slope α/m. Here, it is p(5). This leads to ﬁve hypothesis
being rejected.

Bottom line

When testing multiple hypothesis, uncorrected testing is simply un-
acceptable. The Bonferroni correction, which controls the FWER,
provides a simple solution, but it may be too conservative for cer-
tain applications. The Benjamini-Hochberg algorithm controls the
FDR. The main advantage of controlling the FDR instead of FWER
is that the former is better detecting true effects. The FDR control is
especially popular in genomics and neuroscience.

Further Reading

1. Y. Benjamini (2010) “Discovering the false discovery rate,” Journal

of the Royal Statistical Society, 72(4): 405-416 describes the back-
ground for the original paper Y. Benjamini & Y.Hochberg (1995)
“Controlling the false discovery rate: a practical and powerful ap-

m=10 tests12345678910p-values00.010.020.030.040.050.060.070.08Line y(i)=(,/m)*iOriginal level ,Bonferroni thresholdp-values,,/mBH threshold114 k. m. zuev

proach to multiple testing,” Journal of the Royal Statistical Society.
Series B (Methodological), 57(1): 289-300, and reviews the progress
made on the false discovery rate.

What is Next?

We’ll turn to regression, one of the most popular statistical techniques.

20
Regression Function and General Regression Model

1 In recent years many other methods
have been developed: neural networks,
support vector machines, tree-based
methods, no name but a few. These
methods often outperforms the old
good regression. This leads to a natural
question: why do we need to study re-
gression? The main reason is that most
of these fancy new methods are really
just modiﬁcations of regression. So,
understanding, say SVMs, is impossible
without understanding of regression.
Going for neural networks without un-
derstanding regression is like studying
string theory without knowing calculus.

2 In general, the question “how Y
depends on X” is one of the most
fundamental in Science.
3 Prediction is one of the main goals of
Applied Science and Engineering.

Regression is the study of dependence. It is one of the most impor-
tant and perhaps the most popular statistical technique1.

Recall the schematic picture of a certain phenomenon of interest

from Lecture 6:

So far, we have been ignoring the inputs and discussed the classical
methods of statistical inference tailored for analyzing responses. In
many applications, however, data comes in the form (X1, Y1), . . . , (Xn, Yn),
where Xi is an input and Yi is the corresponding response. Moreover,
inputs and responses are often depended, and ignoring inputs, when
trying to understand the phenomenon, is not wise.

Regression analysis explores the dependence of responses on

inputs with the following two major goals:
1. Understanding. How does Nature associate response Y to input

X?2

2. Prediction. Given a future input X what will be the response Y?3

Besides it is direct “mercantile” purpose, being able to make pre-
dictions also tests our understanding of the phenomena: if we mis-
understand, we might still be able to predict, but we are not able to
predict, then it is hard to claim that we understand.

Let us see how the attempt to predict the response naturally leads
to the regression function, the key element of the regression method-
ology.

Regression Function

Suppose that, given the data (X1, Y1), . . . , (Xn, Yn), we want to predict
the value of the response Y to future input X. For the moment, let us

00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91NatureInputResponse116 k. m. zuev

again forget about inputs4, and focus on the response data Y1, . . . , Yn.
Let r denote our prediction. What is the optimal value for r? The
answer, of course, depends on what we mean by “optimal.” Suppose
that we want to minimize the mean squared error:
MSE[r] = E[(Y − r)2] → min .

(20.1)

4 Assuming, for example, that don’t
actually affect responses, or, simply that
we don’t have access to the input data.

This is a well-deﬁned calculus problem that has an expected solution.
Using the bias-variance decomposition for MSE5, we have:

5 See Lecture 6.

MSE[r] = (E[Y] − r)2 + V[Y],

and, therefore, the MSE is minimized when

r = E[Y].

(20.2)

(20.3)

Given the response data, we can estimate r by ˆr = Yn = 1

i=1 Yi.
But if we have the input data and we believe that inputs and re-
sponses are depended, then it is natural to bring Xis in to the game.
Let r(X) denote our prediction of the response Y to input X, which
now explicitly depends on the input. What function should we use?
As above, let us use the MSE as a measure of goodness:

n ∑n

Using the law of total expectation6, we have:

MSE[r] = E[(Y − r(X))2] → min .
(cid:105)(cid:105)

(Y − r(X))2(cid:12)(cid:12)(cid:12) X

MSE[r] = E

(cid:104)

(cid:104)

E

.

(20.4)

(20.5)

6 E[Y] = E[E[Y|X]]. Here the inner
expectation is wrt Y and the outer
expectation is wrt X.

Since r(X) is a constant when conditioned on X, we can work with
the inner expectation as about, that is we can use the bias-variance
decomposition:

(cid:104)
(cid:104)

(cid:105)
(cid:105)
(E [Y − r(X)| X])2 + V[Y|X]
(E[Y|X] − r(X))2 + V[Y|X]

.

MSE[r] = E

= E

(20.6)

(20.7)

And, thus, the MSE is minimized when

r(X) = E[Y|X].

In other words, if we observe that input X = x, then our optimal7
prediction for the response should be

7 In the MSE sense.

r(x) = E[Y|X = x].

(20.8)
Note that if the response does not depend on the input, then E[Y|X] =
E[Y], and (20.8) reduces to (20.3).

The function r(x) in (20.8) is called the regression function. This is

what we want to know when we want to predict the response.

General Regression Model

Suppose that the regression function is known. It is important to
realize that the true response Y to input X = x typically will not
be exactly equal to our prediction r(x). Simply because there are
measurement errors and, most importantly, because often Y can take
a range of values for given x8. In other words, the observed response
is a sample from the conditional distribution Y|X = x and generally
does not equal to the expected value r(x) = E[Y|X = x],

Y (cid:54)= r(x).

(20.9)

statistical inference

117

8 For example, for different patients,
the improvement in blood cholesterol
(Y) due to the same dose of drug (x) is
different.

But we hope, especially if the variability of Y for a given X is small9,
that our prediction is not too bad and that approximately

Y ≈ r(x).

(20.10)

To account for this discrepancy between the observed data and the
expected value, we introduce a quantity called a statistical error10 :

9 For example, if we hang weight x on a
spring, then, according to Hooke’s law,
the length of the elongated spring is
Y = a + bx, where a and b are constants
that depend on the spring. If, however,
we repeat this experiment n times with
the same weight x, we will get slightly
different values Y1, . . . , Yn because of
the measurement error.

e = Y − r(x).

(20.11)

10 In engineering ﬁleds, it is often called
a prediction error or noise.

Note that, in general, the distribution of e depends on X (since Y
depends on X), but the mean is zero:

E[e|X = x] = E[Y|X = x] − r(x) = 0.

(20.12)

The response Y is thus a sum of a deterministic prediction term,

which is simply the conditional mean value of Y, and a random
statistical error11:

(cid:125)
Y = E[Y|X = x]

(cid:123)(cid:122)

+e.

(20.13)

(cid:124)

r(x)

This equation constitutes a general regression model of Y on X. Note
that so far we did not make any assumptions whatsoever and (20.13)
is always true12. Different speciﬁc regression models are obtained
when we start making certain assumption about the regression func-
tion and statistical error. In nonparametric regression, one tries to
estimate the regression function directly from the data, without mak-
ing any speciﬁc assumptions. In classical parametric regression13,
we assume a particular functional form of the regression function
r(x) ∈ F = { f (x; θ), θ ∈ Θ}14 and then try to obtain a good estimate
for θ.

Remark:
predictor, and the parameters are traditionally denotes by βs15

In regression analysis, the input variable X is often called

15 Rather than θs.

11 In some texts, the statistical error is
denoted by , which unconsciously
makes us to think about the error
as a small quantity, which, although
desirable, may not be the case.

12 We simply say: here is our prediction,
if we are off, we call the difference an
error.

13 The main focus of these notes.

14 For example, f (x; θ) = θ1 + sin(θ2x) +
exp(−θ3x2). You can pick your favorite.

118 k. m. zuev

Further Reading

1. Cosma Shalizi writes notebooks on various topics. Highly rec-
ommended. One of the notebooks is on regression, mostly on
nonparametric regression though. Highly recommended.

What is Next?

How to guess and choose a good functional form for the regression
function and a reasonable assumption about the statistical error? A
good approach for answering this question is examining the scatter
plot of the data, which is a starting point of any regression analysis.
In the next lecture, we will examine in detail a scatter plot of the
data collected by Karl Pearson and see how our observations will
naturally lead to the simple linear regression model.

21
Scatter Plots and Simple Linear Regression Model

Recall that in Lecture 2, we discussed how to summarize data
X1, . . . , Xn using graphical tools such as histograms, boxplots, and
Q-Q plots. A scatter plot, which is simply a plot of the response Y
versus the predictor X, is a fundamental graphic tool for looking at
the regression data (X1, Y1), . . . , (Xn, Yn).

Example: Inheritance of Height

Karl Pearson studied inheritance of different traits from generation
to generation. In 1893-1898, he collected n = 1375 heights of mothers
and their adult daughters in the UK1. Figure 21.1(a) shows the the
scatter plot of the original data, where we consider the mother’s
height Xi as a predictor, and her daughter’s height Yi as the response.

1 K. Pearson & A. Lee (1903) “On the
laws of inheritance in man,” Biometrika,
2, 357-463, Table 31.

Figure 21.1: Scatterplots of (a) the
original Pearson’s data and (b) jittered
data with added small random noise.

Let us discuss several important features of this scatter plot.

1. Size & Scale. The range of heights appears to be about the same for
mother and for daughters: between 55 and 74 inches. That is why

Figure 21.2: Karl Pearson, English
mathematician, one of the fathers
of mathematical statistics and the
father of Egon Pearson. Photo source:
wikipedia.org.

Mothers' heights X (in)5560657075Daughters' heights Y (in)5560657075Original Data6162636461626364Mothers' heights X (in)5560657075Daughters' heights Y (in)5560657075Jittered Data6162636461626364(a)(b)120 k. m. zuev

we make the lengths and the scale of the x- and y-axis the same.
In general, it is useful to play with the scatter plot by resizing and
changing scales, and see how the visual appearance of the data
changes.

2. Jittering. The original Pearson’s data were rounded: each height
was given to the nearest tenth of an inch. This is very well seen
in the zoomed portion of the scatter plot in Fig. 21.1(a). This may
lead to substantial overplotting: having many data points (Xi, Yi) at
exactly the same location. This is undesirable since by looking at
the scatter plot we will not know if one point represents one case
or many cases. This can be very misleading. The easiest solution is
2:
jittering: add a small uniform random number to each Xi and Yi
(21.1)

Xi → Xi + ui, Yi → Yi + vi, ui, vi ∼ U[−δ, δ].

In our case, δ = 0.05 seems to be a good choice: the jittered values
would round to the original numbers. The scatter plot of the jit-
tered data is shown in Fig. 21.1(b). In what follows, we work with
the jittered data3.

3. Dependence. One important function of the scatter plot is to decide
if we can reasonably assume that the response Y indeed depends
on the predictor X. This assumption is clearly reasonable for the
heights data: when X increases, the scatter of Ys shifts upwards4.
This effect is illustrated in Fig. 21.3(a).

4. Regression Function. It appears form in Fig. 21.3(a) that the mean of
Y increases when X increases, i.e. that the regression function r(x)
is an increasing function. Let us look into this in more detail. Let
us consider 8 vertical slices of data:

S1 = {(X, Y) : X ∈ (55, 57)}, . . . , S8 = {(X, Y) : X ∈ (69, 71)}.

(21.2)

2 The original data is likely to be noisy
anyway!

3 Available at heights.xlsx.

4 This is expected of course: higher
mothers tend to have higher daughters.

Figure 21.3: Panel (a): Examining three
vertical slices of the data suggest that
the response Y indeed depends on the
predictor X. Panel (b): Nonparametric
smoother suggests that the regres-
sion function can be reasonably well
modeled by a liner function.

Mothers' heights X (in)5560657075Daughters' heights Y (in)5560657075``Slices'' of Jittered DataMothers' heights X (in)5560657075Daughters' heights Y (in)5560657075Mean responses for 8 slices (a)(b)For each slice, we compute the mean response

Yk =

1

|Sk| ∑
Yi∈Sk

Yi,

k = 1, . . . , 8,

(21.3)

and plot points (56, Y1), . . . (70, Y8) with big red dots in Fig. 21.3(b)5.
The points almost perfectly lie on a straight line. This6 suggests
that a linear function is a very reasonable parametric model for the
regression function:

r(x) = E[Y|X = x] = β0 + β1x.

(21.4)

It has two parameters: an intercept β0 and a slope β1 that can be
estimated from the data7.

5. Statistical Errors. Let us look again at the slices in Fig. 21.3(a).

While the mean value E[Y|X = x] increases with x, the conditional
variance V[Y|X = x] seems to be constant: the spread of all three
slices looks the same8. Thus, it is reasonable to assume that

V[Y|X = x] = σ2,

(21.5)

where σ2 is some positive constant. In view of (20.13), this as-
sumption can be rewritten in terms of the statistical error:

V[e|X = x] = σ2.

(21.6)
The general regression equation (20.13) together with linear model
for the regression function (21.4) and properties of the statistical error
(20.12) and (21.6) gives the simple liner regression model, arguably the
most popular and widely used statistical model.

Simple Linear Regression Model

To sum up, given the data (X1, Y1), . . . , (Xn, Yn), where X is viewed
as the predictor (input) variable that affects the response variable Y,
the simple linear regression model is

Yi = β0 + β1Xi + ei,

where the errors ei are independent9 random variables with

E[ei|Xi] = 0,

and V[ei|Xi] = σ2.

(21.7)

(21.8)

The predictor variable can be either fully deterministic if we can
control and chose its values X1, . . . , Xn as we wish10, or X1, . . . , Xn
can be viewed as a sample from a certain distribution11. In either
case, (21.7) & (21.8) tell us that if Xi is known, then Yi is simply β0 +
β1Xi plus zero-mean “noise” with constant variance12. The model
has three parameters: β0, β1, and σ2.

statistical inference

121

5 Values 56, . . . , 70 are simply horizontal
“centers” of the slices.
6 This method of approximation of the
regression function — averaging the
observed responses for all values of
X close to x — is called nonparamet-
ric smoother. It is at the core of may
nonparametric regression methods.

7 We will learn how to do this in the
next lecture.

8 This appears not to be true for the
farmost left and farmost right slices
where X ≈ 56 and X ≈ 70. But
most likely these regions are simply
undersampled: very few very short
mothers and very few very tall mothers
in the data.

9 We assume that the knowing the error
ei made in case i, does not affect the
error ej made in case j.

10 For example, Xi can be the ith time we
measure a certain quantity, or Xi can be
a weight we attach to a spring in the ith
experiment.
11 For example, a simple random sample
from the populations of mothers in UK.
12 The property of constant variance is
called homoscedasticity.

122 k. m. zuev

• Why “simple”? The model is called simple because the predic-

tor is one-dimensional. In general, Xi can be a vector, Xi =
(Xi,1, . . . , Xi,p). In this case, the model is called multiple linear
regression13.

• Why “linear”? Because the regression function is assumed to be

linear in parameters:

13 This should not be confused with the
multivariate linear regression, where
there are several response variables.

r = β0 + β1◊ + β2(cid:54) + . . . + βp☼.

(21.9)
Whatever ◊,(cid:54), . . . ,☼ are, we can call them predictors. For exam-
model: in this case we could just redeﬁne (cid:101)Xi = exp(Xi) to obtain a
ple, Yi = β0 + β1 exp(Xi) + ei is also a simple linear regression

more familiar form (21.7); Yi = β0 + β1 exp(Xi,1) + β2 sin(Xi,2) + ei
is multiple linear regression; but Yi = β0 + exp(β1Xi) + ei is simple
non-linear regression.

14 The term was coined by Sir Francis
Galton.

Figure 21.4: “Regression towards the
mean.”

• Why “regression”14? Let us look at Fig. 21.4, where the estimated
values of the conditional expectation function r(x) = E[Y|X = x]
are plotted together with the the line y = x, where all data points
would like if all daughters would have exactly the same hight as
their mothers. Notice that the slope of r(x) is less than one. This
means that tall mothers tend to have tall daughters (the slope of
r(x) is positive), but not as tall as themselves (the slope is less than
one). Likewise, the short mothers tend to have short daughters,
but not as as short as themselves. This effect is observed in rela-
tionships between many attributes of parents and children and
was called “regression towards the mean.” The line relating the
mean attribute of children to that of their parents was called “’the
regression line.”

• Why “model”?... I leave this to you as a question for reﬂection :o)

Further Reading

1. Simple linear regression is just the ﬁrst little (yet very important)
step towards a major area of statistical inference, called regression
analysis. S. Weisberg (2014) Applied Linear Regression is recom-
mended as a gentle introduction.

What is Next?

We will discuss how to estimate the parameters of the simple linear
regression model using the method of ordinary least squares.

Mothers' heights X (in)5560657075Daughters' heights Y (in)5560657075Regression line vs y=x lineTall, but shorter than mothersShort,buttaller than mothers22
Ordinary Least Squares

The simple linear regression (SLR) model is

Yi = β0 + β1Xi + ei,

i = 1, . . . , n,

(22.1)

where Xi is the predictor variable, Yi is the corresponding response,
and ei is the random statistical error1. The model assumptions are:

1. ei are independent,
2. E[ei|Xi] = 0,
3. V[ei|Xi] = σ2.

(22.2)

The parameters β0 and β1 are called the regression coefﬁcients. There
are many methods for estimating the regression coefﬁcients, since
there are many reasonable ways to ﬁt a line to a cloud of points. In
this lecture, we will discus the most common method: ordinary least
squares (OLS).

1 Note that (22.1) is always true as long
as we do not make any assumptions of
the errors.

Ordinary Least Squares

Let ˆβ0 and ˆβ1 denote estimates of β0 and β1. The line

ˆr(x) = ˆβ0 + ˆβ1x

is then called the ﬁtted line, and

(cid:98)Yi = ˆr(Xi) = ˆβ0 + ˆβ1Xi

(22.3)

(22.4)

(22.5)

are called the ﬁtted or predicted values. The difference between the

actually observed data point Yi and the predicted value (cid:98)Yi is called

the residual:

ˆei = Yi −(cid:98)Yi = Yi − ˆβ0 − ˆβ1Xi.

Residuals ˆei can be viewed as realizations of random errors ei and
they play an important role in checking the model assumptions

124 k. m. zuev

(22.2). Geometrically, residuals are simply the (signed) vertical dis-
tances between the ﬁtted line and the actual Y-values. See Fig. 22.1.

Figure 22.1: A cloud of green points
(data) and a ﬁtted blue line ˆr(x) =
ˆβ0 + ˆβ1x. A couple of predicted values

((cid:98)Yi and (cid:98)Yj) are shown in blue. The

residuals are depicted by red dashed
lines.

The OLS method choses the estimates ˆβ0 and ˆβ1 to minimize the

quantity called residual sum of squares:

RSS =

n∑

i=1

ˆe2
i =

n∑

i=1

(Yi − ˆβ0 − ˆβ1Xi)2 −→ min .

(22.6)

This minimization criterion is very natural: RSS is a measure of the
overall prediction error, and we want to minimize it by choosing ˆβ0
and ˆβ1 appropriately. In the Appendex, we show that the solution, i.
e. the OLS estimates of β0 and β1 are

ˆβ0 = Y − ˆβ1X and ˆβ1 =

SXY
SXX

,

(22.7)

where X and Y are the sample means2, SXX is the sum of squares, and
SXY is the sum of cross-products:

2 To make the notation simpler, we drop
the usual subscript n, X ≡ Xn, Y ≡ Yn.

SXX =

n∑

i=1

(Xi − X)2 and SXY =

n∑

i=1

(Xi − X)(Yi − Y).

(22.8)

As an example, Fig. 22.2 shows the regression line estimated by

the OLS for the Pearson’s heights data. The estimated values are
ˆβ0 = 30.5 and ˆβ1 = 0.53.

Figure 22.2: OLS in action: the re-
gression line for the heights data
(Lecture 21), heights.xlsx.

Predictor X-2-101234Response Y-2-101234^r(x)=^-0+^-0xXjXiYjYibYjbYi^ej^eiMothers' heights X (in)565860626466687072Daughters' heights Y (in)565860626466687072Regression Lineestimaed by OLSAnscombe’s Quartet

statistical inference

125

Notice that the OLS estimates (22.7) depend on data only through the
statistics X, Y, SXX, and SXY. This means that any two data sets for
which these statistics are the same will have identical ﬁtted regres-
sion lines,

,

(22.9)

ˆr(x) = Y + (x − X)

SXY
SXX

even if a straight-line model is appropriate for one but not the other.
This effect was beautifully demonstrated by Frank Anscombe3.

3 F.J. Anscombe (1973) “Graphs in
statistical analysis,” The American
Statistician, 27(1): 17-21.

Figure 22.3: Anscombe’s quartet.

Anscombe came up with four artiﬁcial data sets for which the OLS

method ﬁts the same regression line, but the visual impression of
the scatter plots is very different. Anscombe’s “quartet” is shown
in Fig. 22.3. For the ﬁrst data set in Fig. 22.3(a) the SLR model is
appropriate. The scatter plot of the second data set in Fig. 22.3(b)
suggests that ﬁtting a line is incorrect, ﬁtting a quadratic polynomial
would be more natural. In Fig. 22.3(c), we see that the SLR model
maybe correct for most of the data, but one of the cases is too far
away from the ﬁtted regression line. This is called the outlier problem4.
Finally, the scatter plot in Fig. 22.3(d) is different from the previous
three in that there is not enough data to make a judgment regarding
the regression function r(x) = E[Y|X = x]. Essentially, we have
information about r(x) only at two points. Moreover, there is only
one response value for the larger input. Without that point, we would
not be able even estimate the slope. Even if the SRL model is correct
here, we can’t trust the regression line whose slope is so heavily
dependent on a single case. More data is needed.

The moral is clear: don’t ﬁt the regression line blindly, check the
scatter plot ﬁrst.

4 In this case, the outlier is often re-
moved from the data and the regression
line is reﬁt from the remaining data.
This of course assumes that the outlier
is not a true response to the corre-
sponding input: it occurred because of
the noise in the measurement or some
other error.

(a)(b)(c)(d)126 k. m. zuev

OLS and MLE

The OLS method is very intuitive. It turns out that the OLS estimates
can be (at ﬁrst glance unexpectedly) justiﬁed purely statistically if we
assume that the errors ei are normally distributed5:

ei|Xi ∼ N (0, σ2).

(22.10)

Note that this assumption is much stronger than (22.2): whereas
(22.2) speciﬁes only the ﬁrst two moments, E[Yi|Xi] = β0 + β1Xi and
V[Yi|Xi] = σ2, the normality assumption (22.10) speciﬁes the exact
form of the distribution of the response variable:
Yi|Xi ∼ N (β0 + β1Xi, σ2).
(cid:18)
n∏
i=1(Yi − β0 − β1Xi)2

Under the conditional normal SLR model, the likelihood function

of model parameters is the joint density of the data:

L(β0, β1, σ2|{(Xi, Yi)}) =
(cid:18)
− ∑n
(cid:18)
2σ2
− RSS(β0, β1)

2πσ2(cid:17)− n
2πσ2(cid:17)− n

− (Yi − β0 − β1Xi)2

1√
2πσ

(cid:16)
(cid:16)

2 exp

2 exp

=

=

(cid:19)

(cid:19)

(cid:19)

2σ2

exp

i=1

(22.11)

2σ2

.

(22.12)

This means that the MLEs of β0 and β1 are exactly those values that
minimize RSS(β0, β1). Thus, under the assumption of normality
(22.10), the OLS are also the MLEs:

ˆβ0 = ˆβ0,MLE and ˆβ1 = ˆβ1,MLE

(22.13)

To ﬁnd the MLE of σ2, we need to substitute ˆβ0 and ˆβ1 in (22.12)
and maximize the likelihood over σ2. Maximizing the log-likelihood
is more convenient. Dropping non relevant terms,

l( ˆβ0, ˆβ1, σ2) = −n log σ − 1

2σ2 RSS( ˆβ0, ˆβ1).

(22.14)

Differentiating with respect to σ, setting the derivative to zero, and
solving the corresponding equations gives:

ˆσ2
MLE =

1
n

RSS( ˆβ0, ˆβ1) =

1
n

n∑

i=1

ˆe2
i ,

(22.15)

which is a natural estimate if you think about it. It is natural, but
biased. An unbiased6 estimate of σ2 is7
n∑

ˆσ2 =

(22.16)

1
n − 2

ˆe2
i ,

i=1

You might expect to see 1
the residuals are not independent8. The dependence is low though,
and the factor

n−1 as in the case of the sample variance, but

1
n−2 takes care of it.

5 The model (22.1) & (22.10) is called
the conditional normal model. It is the
most common SLR model and the most
straightforward to analyze.

6 Even under the original weaker
assumptions (22.2).
7 The proof is a long calculation.
See, for example, Appendix C.3 in
D.C. Montgomery et al (2006) Introduc-
tion to Linear Regression Analysis.
8 In the next lecture, we will show (you
are welcome to check this now) that
∑n

i=1 ˆei is zero, hence the dependence.

Appendix: Proof of (22.7)

The least squares estimates ˆβ0 and ˆβ1 are deﬁned as those values that
minimize the RRS

RSS( ˆβ0, ˆβ1) =

(Yi − ˆβ0 − ˆβ1Xi)2.

n∑

i=1

(22.17)

This function of two variables can be minimized in the following way.
For any ﬁxed ˆβ1, the value of ˆβ0 that minimizes

is given by9

RSS( ˆβ0, ˆβ1) =

((Yi − ˆβ1Xi) − ˆβ0)2

n∑

i=1

ˆβ0 = Y − ˆβ1X = Y − ˆβ1X.

Thus, for a given value of ˆβ1, the minimum value of RSS is

RSS(Y − ˆβ1X, ˆβ1) =

n∑

i=1

(Yi − Y + ˆβ1X − ˆβ1Xi)2

=

n∑

i=1

((Yi − Y) − ˆβ1(Xi − X))2 = SYY − 2 ˆβ1SXY + ˆβ2

1SXX.

(22.18)

(22.19)

(22.20)

statistical inference

127

9 We use elementary fact that

arg min

a

n∑

i=1

(xi − a)2 = ¯x.

To show this, simply add and subtract ¯x
inside the brackets, and then expand.

Since SXX > 0, the value of ˆβ1 that gives the overall minimum of RSS
is

.

(22.21)

ˆβ1 =

SXY
SXX

Note that the OLS method, strictly speaking, is not a method of
statistical inference. It does not use any model assumptions (22.2). It
simply ﬁts the line to the data using using the RSS → min criterion,
and RSS is one of many reasonable ways of measuring the distance
from the line ˆr(x) = ˆβ0 + ˆβ1x to the data points. But we will see that
under (22.2), the OLS estimates have nice optimality properties.

Further Reading

1. The original F.J. Anscombe (1973) “Graphs in statistical analysis,”

The American Statistician, 27(1): 17-21 is worth reading.

What is Next?

We will discuss several important properties of the OLS estimates, in
particular, their color :)

23
Properties of the OLS Estimates

Recall that the simple linear regression (SLR) model is:

Yi = β0 + β1Xi + ei,

i = 1, . . . , n,

(23.1)

where Xi is the predictor variable, Yi is the corresponding response,
and ei is the random statistical error. The model assumptions are:

1. ei are independent,
2. E[ei|Xi] = 0,
3. V[ei|Xi] = σ2

(23.2)

Last time we discussed the ordinary least squares (OLS) estimates of
the regression coefﬁcients β0 and β1:

ˆβ0 = Y − ˆβ1X and ˆβ1 =

SXY
SXX

.

(23.3)

The OLS estimates have several important properties, some of which
we will derive in this lecture.

OLS and Data Centroid

The point (X, Y) is called the centroid of the data. It is straightforward
to check that the least-squares regression line always passes through
the centroid. Indeed:

ˆr(X) = ˆβ0 + ˆβ1X = Y − SXY
SXX

X +

SXY
SXX

X = Y.

(23.4)

This is somewhat expected: our prediction for the average input is
the average response.

OLS and the Sum of the Residuals

statistical inference

129

The sum of the residuals is always zero:
n∑

n∑

n∑

ˆei =

i=1

(Yi −(cid:98)Yi) =
(cid:18)
Yi − Y +

(Yi − Y) +

i=1

n∑

i=1

n∑

=

=

(Yi − ˆβ0 − ˆβ1Xi)
(cid:19)

X − SXY
SXX
(X − Xi) = 0.

n∑

Xi

i=1
SXY
SXX
SXY
SXX

This property is also natural: on average the ﬁtted value (cid:98)Yi neither

i=1

i=1

overestimates nor underestimates the true response Yi.

OLS is Linear

(23.5)

There many possible estimates of the regression coefﬁcients1. Let us
restrict our attention to the class of liner estimates. An estimate ˆˆβ of a
regression coefﬁcient β is called linear if it as a linear combination of
the responses:

ˆˆβ =

n∑

i=1

αiYi,

αi ∈ R.

The OLS estimates are linear:

i=1(Xi − X)(Yi − Y)
∑n

ˆβ1 =

=

SXY
SXX
n∑

i=1

=
Xi − X
SXX

SXX
n∑

i=1

Yi − Y
SXX

(Xi − X) =

n∑

i=1

Xi − X
(cid:124) (cid:123)(cid:122) (cid:125)
SXX
αi

Yi.

(23.6)

(23.7)

1 For example, one may chose to min-
imize (instead of RSS) the sum of
squared Euclidean distances from data
points to the ﬁtted line, or some other
measure of the overall ﬁt.

2 Recall that we think of {Xi} as either
being fully deterministic or being
an observed sample from a certain
distribution. The context is: observing
X we want to predict Y.

The estimate ˆβ0 is also linear since both terms Y and ˆβ1 are linear.

OLS is Unbiased

In regression problems, we always focus on properties conditional on
the values {Xi} of predictor variable2. Assuming the SLR model is
correct and using representation (23.7) and model assumption 2 in
(23.2), we have:
E[ ˆβ1|{Xi}] = E

(cid:34) n∑

n∑

(cid:35)

Yi

=

Xi − X
SXX

i=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) {Xi}

n∑

i=1
β1X
SXX

E[Yi|Xi]
n∑

i=1

Xi(Xi − X)

Xi − X
SXX

i=1

(Xi − X) +
n∑

β1
SXX
(Xi − X) = β1.

i=1

(23.8)

n∑

=

Xi − X
SXX
n∑

i=1

i=1
β1
SXX

=

(β0 + β1Xi) =

β0
SXX
(Xi − X)(Xi − X) +

130 k. m. zuev

Using the unbiasedness of ˆβ1,

E[ ˆβ0|{Xi}] = E[Y − ˆβ1X|{Xi}] =

1
n

n∑

i=1

E[Yi|Xi] − β1X

=

1
n

n∑

i=1

(β0 + β1Xi) − β1X = β0.

(23.9)

Variance of OLS

To quantify the variability of the OLS estimates, let us compute their
variances. We will also need this result later on when we will dis-
cuss the prediction based on the OLS regression line. Using model
assumptions 1 and 3 in (23.2), we have:
Xi − X
(cid:19)2
SXX

(cid:34) n∑
(cid:18) Xi − X

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) {Xi}

V[ ˆβ1|{Xi}] = V

(cid:35)

i=1

Yi

(23.10)

=

n∑

i=1

SXX

V[Yi|Xi] =

σ2
SXX

.

Let’s now look at this expression and let’s suppose that we can

control the inputs X1, . . . , Xn, that is we can choose them as we wish.
Then (23.10) suggests that we must chose them such that SXX is as
large as possible. This would make the variance small. For example,
if all Xi must be in an interval [x0, x1], then the choice of Xi that
maximizes SXX is to take half3 of them equal to x0 and the other half
equal to x1. This would be the best4 design if we are certain that the
SLR model is correct. In practice, however, this two-point design is
almost never used, since researchers are rarely certain of the model.
If the regression function r(x) = E[Y|X = x] is, in fact, non-linear,
it could never be detected from data obtained using the two-point
design5.

Computing the variance of ˆβ0 is a bit more involved.
V[ ˆβ0|{Xi}] = V[Y − ˆβ1X|{Xi}]

= V[Y|{Xi}] + X2V[ ˆβ1|{Xi}] − 2XCov[Y, ˆβ1|{Xi}].

(23.11)

Since Y1, . . . , Yn are independent6, the ﬁrst term is simply σ2
n . The
second term has been just computed in (23.10). The sample mean
response and the OLS estimate ˆβ1 constitute an example of two de-
pendent but uncorrelated random variables:
Xj − X
SXX
σ2
nSXX

Cov[Y, ˆβ1|{Xi}] = Cov

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) {Xi}

Cov[Yi, Yi|Xi] =

(Xi − X) = 0.

Xi − X
SXX

n∑

n∑

n∑

n∑

j=1

(cid:34)

(cid:35)

=

1
n

Yi,

Yj

1
n

i=1

i=1

i=1

(23.12)

3 Assume for simplicity that n is even.
4 “Best” in the sense that it would give
the most precise estimate of the slope
β1 of the regression line.

5 Recall also the 4th example in
Anscombe’s quartet.

6 Assumption 1 in (23.2).

(cid:32)

(cid:33)

.

1
n

+

X2
SXX

V[ ˆβ0|{Xi}] = σ2

(23.13)

Thus,

OLS is BLUE

So, we have shown that the OLS estimates are linear, unbiased, and
computed their variances. It turns out that ˆβ0 and ˆβ1 are the best liner
unbiased estimates (BLUE). Here “best” means that the estimate has
the smallest variance among all linear and unbiased estimates. This
result, called the Gauss-Markov theorem, is valid not only for the SLR
model, but also for a more general multiple regression model.

Let us show that ˆβ1 is BLUE. To start, let us describe in more detail

the class of estimates we consider. First, the estimate must be linear:

ˆˆβ1 =

n∑

i=1

αiYi,

αi ∈ R.

(23.14)

Second, it must be unbiased. That is E[ ˆˆβ1|{Xi}] = β1. This condition
induces the following requirement7:

(cid:34) n∑

i=1

(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) {Xi}

αiYi

=

αi(β0 + β1Xi) = β0

i=1

β1 = E[ ˆˆβ1|{Xi}] = E
n∑

=

n∑

i=1

αiE[Yi|Xi]
n∑
n∑

αi + β1

i=1

i=1

(23.15)

αiXi.

statistical inference

131

7 We use assumption 2 in (23.2).

The RHS must be equal to the LHS for any β0 and β1. This is possi-
ble if and only if

n∑

i=1

αi = 0 and

n∑

i=1

αiXi = 1.

(23.16)

So, we consider the estimates of the form (23.14) with coefﬁcients
satisfying (23.16).

The variance of ˆˆβ1 is8

8 We use assumptions 1 and 3 in (23.2).

V[ ˆˆβ1|{Xi}] = σ2

n∑

i=1

α2
i .

To ﬁnd the BLUE, we thus need to minimize

n∑

i=1

i −→ min
α2
n∑

i=1

subject to

αi = 0 and

n∑

i=1

αiXi = 1.

(23.17)

(23.18)

This constrained minimization problem can be solved, for instance,
by the method of Lagrange multipliers. But before we delve into

132 k. m. zuev

Figure 23.1: The constrain minimization
(23.18) is equivalent to ﬁnding the
closest to the origin point (red dot)
in the search space of codimension
2, which is the intersection of two
hyperplanes deﬁned by the constraints.

computations, let us see what is going on geometrically. Figure 23.1
shows what we are trying to ﬁnd.

From this visualization it is clear that there exists the unique criti-
cal point which is the global minimum. Let us now go back to work.
The Lagrangian is

L(α1, . . . , αn; λ1, λ2) =

n∑

i=1

α2
i + λ1

n∑

i=1

αi + λ2

αiXi − 1

.

(23.19)

To ﬁnd the critical points of the Lagrangian we need to set all its
partial derivatives to zero:

(cid:33)

(cid:32) n∑

i=1



i = 1, . . . , n,

2αi + λ1 + λ2Xi = 0,
∑n
i=1 αi = 0,
i=1 αiXi − 1 = 0.
∑n

(23.20)

(23.21)

It is readily veriﬁable that this system has the unique solution:

λ1 =

2X
SXX

, λ2 = − 2
SXX

, αi =

Xi − X
SXX

.

It remains to observe that αi in (23.1) are exactly the same as in (23.7),
which proves that ˆβ1 is the BLUE of the slope β1. A similar analysis
shows that ˆβ0 is also the BLUE of the intercept β0.

Thus, if you believe the SLR model correctly describes your data

and want to use linear unbiased estimates for β0 and β1, the OLS
estimates are the ones to use.

Further Reading

1. But what if we don’t care about the unbiasedness and linear-
ity, and simply want (possibly biased) estimates of β0 and β1
with small MSE? Can we beat OLS? A short answer is “yes.” See

statistical inference

133

T.L. Burr & H.A. Fry (2005) “Biased regression: the case for cau-
tious application,” Technometrics, 47(3), 284-296 for a review of
biased estimates that have lower MSE than OLS.
2. See also a discussion on stats.stackexchange.com.

What is Next?

We will see how hypothesis testing and interval estimation work in
the context of simple linear regression.

24
Hypothesis Testing & Interval Estimation

The OLS method allows us to construct point estimates for the re-
gression parameters. In many applications, however, we are often
interested in testing hypothesis about the parameters and construct-
ing conﬁdence intervals for them. In previous lectures, we discussed
hypothesis testing and intervals in general settings. Here, we will see
how these methods of statistical inference work for the SLR model.

To both test hypothesis and construct conﬁdence intervals, we
need to make a parametric assumption about the statistical errors.
Namely, we assume that ei are normally distributed, and thus we will
work with the conditional normal model1:

Yi = β0 + β1Xi + ei,
ei|Xi ∼ N (0, σ2),

i = 1, . . . , n.

(24.1)

In the next lecture, we will discuss how this assumption can be
checked using the residual analysis.

The t-Test for the Regression Parameters

Suppose we want to test the hypothesis (“current theory”) that the
slope β1 equals to some constant β∗
1:

H0 : β1 = β

∗
1 versus H1 : β1 (cid:54)= β

∗
1.

(24.2)

1 We introduced in Lecture 22, where
we have shown that, in this model, the
OLS estimates are simply the MLEs.

Thanks to the normality assumption in (24.1), the responses are inde-
pendently and normally2 distributed:

2 But, of course, not identically.

Yi|Xi ∼ N (β0 + β1Xi, σ2).

(24.3)

Recall that the OLS estimate ˆβ1 is a liner combination of responses,

ˆβ1 =

Xi − X
SXX

n∑

i=1

Yi,

(24.4)

statistical inference

135

3 Lecture 22.

4 So, in principle, we can use the Wald
test.
5 For example, see Appendix C.3 in
D.C. Montgomery et al (2006) Introduc-
tion to Linear Regression Analysis.

6 In (24.12), as usual, (cid:98)se( ˆβ0) denotes the

estimated standard error.

and, therefore, it is also normal. We also found its mean (E[ ˆβ1|{Xi}] =
β1) and variance (V[ ˆβ1|{Xi}] = σ2

SXX ) last time. Thus,

ˆβ1|{Xi} ∼ N

β1, σ2
SXX

(cid:19)
(cid:18)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12){Xi} ∼ N (0, 1).

This means that, under H0,

ˆβ1 − β∗

1(cid:113) σ2

SXX

.

(24.5)

(24.6)

The parameter σ2 is unknown, but recall3 that we know its unbiased
estimate:

ˆσ2 =

1
n − 2

n∑

i=1

ˆe2
i .

The random variable

T1 =

ˆβ1 − β∗

1(cid:113) ˆσ2

SXX

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12){Xi} ·∼ N (0, 1).

(24.7)

(24.8)

is then approximately normally distributed4. It can be shown5 how-
ever, that the exact distribution of T1 under H0 is t-distribution with
(n − 2) degrees of freedom:

T1|{Xi} ∼ tn−2.

So, the size α t-test rejects H0 when

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ˆβ1 − β∗

1
SXX

√

ˆσ/

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > tn−2,1− α

2

.

To ﬁnd the p-value of the test, we need to solve |T1| = tn−2, α

2

Similarly, we can construct a t-test for the intercept:
∗
∗
0 versus H1 : β0 (cid:54)= β
0.

H0 : β0 = β

(24.9)

(24.10)

for α.

(24.11)

The test statistic in this case is6

T0 =

(cid:98)se( ˆβ0)
ˆβ0 − β∗

0

=

ˆσ

(cid:114)
ˆβ0 − β∗
n + X2
SXX

1

0

,

(24.12)

and, as before, the size α t-test rejects H0 when

|T0| > tn−2,1− α

2

,

(24.13)

and the p-value is p = 2Fn−2(−|T0|), where Fn−2 is the CDF of the
t-distribution with (n − 2) degrees of freedom.

136 k. m. zuev

Testing Signiﬁcance of Linear Regression

A very important special case of (24.2) is

H0 : β1 = 0 versus H1 : β1 (cid:54)= 0.

(24.14)

Here we test the existence of linear relationship between the predic-
tor and response. Accepting the null hypothesis implies that we have
one of the following two scenarios:
1. The response Y does not really depend on the input X, and the

best prediction (cid:98)Y of the response to any future input X is sim-
ply the sample mean, (cid:98)Y = Y. This situation is illustrated in

Fig. 24.1(a).

2. The response Y does depend on X, but the true relationship is not

linear, Fig. 24.1(b).

Thus, accepting H0 is equivalent to saying that there is no linear rela-
tionship between Y and X.

Figure 24.1: Two cases where the null
hypothesis H0 : β1 = 0 is accepted.

On the other hand, rejecting H0 means that the predictor indeed

inﬂuences the response, but the relationship is not necessarily linear.
Namely, two cases are possible:
1. The SLR model (24.1) is an accurate model for the data, Fig. 24.2(a).
2. There is a linear trend Y = β0 + β1X, but the data is more
accurately modeled with the addition of higher order terms
Y = β0 + β1X + β2X2 + . . ., Fig.24.2(b).
The procedure for testing signiﬁcance of linear regression is ob-
1 to zero. Namely, we claim the liner

tained from (24.10) by setting β∗
regression signiﬁcant at level α if
ˆβ1
√
SXX

ˆσ/

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

The corresponding p-value is p(X, Y) = 2Fn−2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > tn−2, α

2

.

(24.15)

(cid:16)−(cid:12)(cid:12)(cid:12)

ˆβ1
√
SXX

ˆσ/

(cid:12)(cid:12)(cid:12)(cid:17)

.

Pridictor XResponse YPridictor X(a)(b)statistical inference

137

Figure 24.2: Two cases where the null
hypothesis H0 : β1 = 0 is rejected.

Example: Heights of Mothers and Daughters
Recall that in Lecture 20, we found the OLS regression line for the
Pearson’s data, mothers’ heights vs. daughters’ heights, Fig. 24.3. The
estimated valued of the slope is ˆβ1 = 0.53. The p-value is essentially
zero,

p(X, Y) = 9.7 × 10−81,

(24.16)

which means that Pearson’s data provides extremely strong evidence
against H0. As expected.

Conﬁdence Intervals for β0, β1, and σ2

Figure 24.3: The OLS regression line for
the heights data, heights.xlsx.

Now let us turn to constructing conﬁdence intervals for the regres-
sion parameters, which can be used as a measure of the overall qual-
ity of the regression line.

In the previous section, we have already established that, under

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12){Xi} ∼ tn−2,

(cid:98)se( ˆβ1)
ˆβ1 − β1

(24.17)

and (cid:98)se( ˆβ1) =

ˆσ√
SXX

,

(24.18)

normality assumption (24.1),

(cid:98)se( ˆβ0)
ˆβ0 − β0

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12){Xi} ∼ tn−2 and
(cid:115)
(cid:98)se( ˆβ0) = ˆσ

+

1
n

X2
SXX

where the estimated standard errors are

and ˆσ is given by (24.7). Therefore, a 100(1 − α)% conﬁdence interval
for βi is given by

ˆβi ± tn−2, α

2(cid:98)se( ˆβi).

(24.19)

The interpretation of this interval is the following: if we

1. ﬁx X1, . . . , Xn,

Predictor XResponse YPredictor X(a)(b)Mothers' heights X (in)565860626466687072Daughters' heights Y (in)565860626466687072Regression Lineestimaed by OLS138 k. m. zuev

2. measure , for each Xi, the corresponding response m times:

Xi (cid:55)−→ Y(1)

i

, . . . , Y(m)

i

,

(24.20)

3. construct m intervals (24.19) from data {(Xi, Y(k)
)}, k = 1, . . . , m,
then approximately (1 − α)m intervals will contain the true value of
βi (assuming the the SLR model is correct).

To construct a conﬁdence interval for the variance σ2, we need to

i

use the following technical result7:
(n − 2)ˆσ2

∼ χ2

n−2,

(24.21)

σ2

q is the χ2-distribution with q degrees of freedom8. Fig-

where χ2
ure 24.4 shows the density of this distribution.

Introducing the standard notation, χ2

q,α, for the point that deﬁnes

q,α) that contains probability mass α, and using

(cid:19)

the interval (0, χ2
(24.21), we have:

(cid:18)

P

χ2
n−2,1− α
2

<

(n − 2)ˆσ2

σ2

< χ2

n−2, α
2

= 1 − α.

(24.22)

This gives a 100(1 − α)% conﬁdence interval for σ2:

(n − 2)ˆσ2
χ2
n−2, α
2

< σ2 <

(n − 2)ˆσ2
χ2
n−2,1− α
2

.

(24.23)

Example: Heights of Mothers and Daughters
The 95% (α = 0.05) conﬁdence intervals for β0, β1, and σ2 are

27.3 < β0 < 33.7,

0.48 < β1 < 0.58,

4.9 < σ2 < 5.7.

(24.24)

To visualize the uncertainty about the regression line, in Fig. 24.5
we show the OLS regression line and 10 regression lines with slopes
and intercepts chosen uniformly at random from the corresponding
conﬁdence intervals. Notice that the variability of the regression lines
is quite large. It would be better to have green lines ﬂuctuating more
closely to the red line. Let us investigate what is going on.

High variability stems form the large size of the conﬁdence inter-
vals for β0 and β1 in (24.24), computed from (24.19). The size of the
conﬁdence intervals is 2tn−2,1− α
are

2(cid:98)se( ˆβi). The estimated standard errors

= 0.03,

(cid:98)se( ˆβ1) =
(cid:98)se( ˆβ0) = ˆσ

ˆσ√
SXX

(cid:115)

(24.25)

1
n

+

X2
SXX

≈ ˆσX√
SXX

= 1.6.

7 For example, see Appendix C.3 in
D.C. Montgomery et al (2006) Introduc-
tion to Linear Regression Analysis.

8 Recall the deﬁnition: if Z1, . . . , Z1 are
iid standard normal, then

Q =

i ∼ χ2
Z2

q

q∑

i=1

Figure 24.4: The PDF of the χ2-
distribution with k degrees of freedom.

Figure 24.5: The OLS regression line
(red) and random regression lines
(green) with slopes and intercepts cho-
sen uniformly from the 95% conﬁdence
intervals (24.24).

x012345678910@2k density00.10.20.30.40.50.6k=1k=2k=3k=4k=5k=6k=7k=8k=9k=10Mothers' heights X (in)54565860626466687072Daughters' heights Y (in)54565860626466687072statistical inference

139

Figure 24.6: The OLS regression line
(red) and random regression lines
(green) with slopes and intercepts
chosen uniformly from the 95% conﬁ-
dence intervals constructed for the data
{(X(cid:48)

i, Yi)}.

This shows that the high uncertainty is caused primarily by the high
value of X = 62.5, which makes the conﬁdence interval for β0 large.
If the average mothers’ height were smaller, the uncertainty in the
regression line were also smaller. To conﬁrm this observation, let us
formally modify the data by reducing the mothers heights:

Xi (cid:32) X(cid:48)

i = Xi − min{Xi}.

(24.26)

The smallest mother in the new data has zero height (at least it is not
i, Yi)}. As
negative!). Fig.24.6 is the same as Fig.24.5, but for data {(X(cid:48)
expected, the uncertainty about the regression line is much smaller.

Further Reading

1. The analysis-of-variance (ANOVA) approach for testing signiﬁ-

cance of regression is an important alternative to the t-test consid-
ered in this lecture. For simple linear regression, the two methods
are equivalent. ANOVA is especially useful in multiple regression.
It is covered in depth in D.C. Montgomery et al (2006) Introduction
to Linear Regression Analysis.

What is Next?

We will discuss the response prediction using the regression model.
We will conclude these notes with a brief discussion of the residual
plots, a useful tool or checking model assumptions.

Tiny Mothers' heights X' (in)024681012141618Daughters' heights Y (in)5456586062646668707225
Prediction & Graphic Residual Analysis

For the last time in these notes, let us assume that we model data
(X1, Y1), . . . , (Xn, Yn) using the conditional normal SLR model:

Yi = β0 + β1Xi + ei,
ei|Xi ∼ N (0, σ2),

i = 1, . . . , n,

(25.1)

1 See Lecture 20.
2 In the MSE sense.

and let ˆβ0 and ˆβ1 denote the OLS estimates of the regression param-
eters. One of the main goals of regression is to predict the response
to the future input. Let X∗ denote the future value of the predictor
variable. The response, according to the model, is then
e∗|X∗ ∼ N (0, σ).

+e∗,

(25.2)

(cid:123)(cid:122)
(cid:125)
Y∗ = β0 + β1X∗

(cid:124)

r(X∗)

We know1 that the optimal2 prediction for the response Y∗ is

r(X∗) = E[Y∗|X∗] = β0 + β1X∗.

(25.3)

If instead of normality assumption in (25.1) for the statistical errors,
we make a weaker set of assumptions (25.18), then pretty much all
we can do is to report the point estimate of the optimal prediction,
namely the ﬁtted (or predicted value):

(cid:98)Y∗ = ˆr(X∗) = ˆβ0 + ˆβ1X∗.

(25.4)

Under the normality assumption, we can do more:

(a) Construct a conﬁdence interval for r(X∗), which is a parameter of

the model, and

(b) Construct a prediction interval for Y∗, which is an unobserved

random variable.

According to the model,

Y∗|X∗ ∼ N (r(X∗), σ2).

(25.5)
So, (a) can be considered as the inference on the mean of the distribu-
tion, and (b) as the inference on the actual value.

statistical inference

141

Conﬁdence Interval for r(X∗)
Thanks to the normality assumption, the ﬁtted value (cid:98)Y∗ = ˆr(X∗) is

normally distributed, since it is a liner combination of Yi, which are
normal. Since the OLS estimates are unbiased, so is the ﬁtted value:

E[ˆr(X∗)|{Xi}, X∗] = E[ ˆβ0 + ˆβ1X∗|{Xi}, X∗]

= β0 + β1X∗ = r(X∗).

(25.6)

Let us compute the variance:

V[ˆr(X∗)|{Xi}, X∗] = V[ ˆβ0 + ˆβ1X∗|{Xi}, X∗]
= V[ ˆβ0|{Xi}] + (X∗)2V[ ˆβ1|{Xi}] + 2X∗Cov[ ˆβ0, ˆβ1|{Xi}].

(25.7)

Variance of both ˆβ0 and ˆβ1 we found in Lecture 233. To ﬁnd the
covariance, we use ˆβ0 = Y − ˆβ1X and that Cov[Y, ˆβ1|{Xi}] = 04.

Cov[ ˆβ0, ˆβ1|{Xi}] = Cov[Y − ˆβ1X, ˆβ1|{Xi}]
= −XV[ ˆβ1|{Xi}] = − σ2X
SXX

.

(25.8)

3 V[ ˆβ1|{Xi}] = σ2
SXX

V[ ˆβ0|{Xi}] = σ2(cid:16) 1

and
n + X2
SXX

(cid:17)

.

4 Also, see Lecture 23.

Thus,

V[ˆr(X∗)|{Xi}, X∗] = σ2

= σ2

(cid:32)
(cid:18) 1

1
n

n

and

(cid:113)
ˆr(X∗) − r(X∗)
n + (X∗−X)2
σ

SXX

1

(cid:33)

σ2(X∗)2
SXX

X2
+
SXX
(X∗ − X)2

(cid:19)

,

+

+

SXX

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12){Xi}, X∗ ∼ N (0, 1).

− 2σ2XX∗
SXX

(25.9)

(25.10)

If we replace σ with its unbiased estimate ˆσ, then the distribution will
be approximately normal. The exact distribution, as in the previous
lecture, is t with (n − 2) degrees of freedom. Consequently, a 100(1 −
α)% conﬁdence interval for r(X∗), mean response at X∗, is

 ˆβ0 + ˆβ1X∗ ± tn−2, α

ˆσ

2

(cid:115)

r(X∗) ∈

 .

1
n

+

(X∗ − X)2

SXX

(25.11)

Note that the width of the conﬁdence interval for r(X∗) is, as ex-
pected, a function of X∗. The interval width is minimal if X∗ = X
and it increases as X∗ goes away from X. Intuitively, this is reason-
able: our prediction is most accurate near the center of the data, and
as |X∗ − X| increases, the prediction degenerates.

142 k. m. zuev

Prediction Interval for Y∗

Now let us derive an interval estimate for the unobserved random
response Y∗. Uncertainty in the value of the mean response r(X∗)
stems from the uncertainty in the regression coefﬁcients β0 and β1.
In the case of Y∗, we have an additional source of uncertainty coming
from the random statistical error e∗:

Let ˆe∗ be the unobserved residual

Y∗ (cid:54)= r(X∗), but Y∗ = r(X∗) + e∗.
ˆe∗ = Y∗ −(cid:98)Y∗ = (β0 − ˆβ0) + (β1 − ˆβ1)X∗ + e∗.

Since both Y∗ and (cid:98)Y∗ are normal according to the model, so is ˆe∗. Its

(25.13)

(25.12)

(25.14)

(25.15)

(25.16)

V[ ˆe∗|{Xi}, X∗] = V[ ˆβ0 + ˆβ1X∗|{Xi}, X∗] + V[ ˆe∗|X∗]

mean is zero:

and the variance is

The rest is as above:

(cid:18)

E[ ˆe∗|{Xi}, X∗] = E[ ˆe∗|X∗] = 0,
(cid:19)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12){Xi}, X∗ ∼ tn−2,

(X∗ − X)2

1 + 1

= σ2

1 +

+

SXX

1
n

.

(cid:113)

ˆσ

Y∗ −(cid:98)Y∗
n + (X∗−X)2
(cid:115)

SXX

and a 100(1 − α)% prediction interval for the future response Y∗ to
input X∗ is

 ˆβ0 + ˆβ1X∗ ± tn−2, α

2

Y∗ ∈

 .

ˆσ

1 +

1
n

+

(X∗ − X)2

SXX

(25.17)

Notice that the intervals for the mean response r(X∗) in (25.11) and
the response Y∗ in (25.17) have the same center, the ﬁtted value ˆβ0 +
ˆβ1X∗, but the width of the second interval is larger, since there is
more uncertainty in the value of the response than in the value of its
mean.

Example: Heights Data
Figure 25.1 shows six intervals: three for the future daughters’
heights Y∗ (dashed) born by the mothers of heights X∗ = 56, 62.5, 69
and three for the mean daughters’ height r(X∗) born by the mothers
of the same heights. In all cases, α = 0.05. In this example, it is espe-
cially clear that the uncertainty associated with future response Y∗ is
much higher than that for the mean response.

Figure 25.1: Conﬁdence intervals for
r(X∗) and prediction intervals for Y∗,
for X∗ = 56, 62.5, and 69.

Mothers' heights X (in)565860626466687072Daughters' heights Y (in)565860626466687072Checking Model Assumptions using the Residual Plots

There are two standard sets of assumptions in the SLR model: “semi-
parametric” assumptions

1. ei are independent,
2. E[ei|Xi] = 0,
3. V[ei|Xi] = σ2,

and a stronger parametric assumption

ei|Xi ∼ N (0, σ2),

(25.18)

(25.19)

which, in addition to point estimates, allows to test hypothesis and
construct conﬁdence intervals.

The above assumptions are the statements about statistical errors,

ei = Yi − β0 − β1Xi,

(25.20)

which are unobserved5 random variables. Strictly speaking, the
residuals,

ˆei = Yi − ˆβ0 − ˆβ1Xi,

(25.21)

are not realizations of errors, because the betas are hatted. Never-
theless, since ˆβ0 ≈ β0 and ˆβ1 ≈ β1, it is convenient to think of the
residuals as the observed approximate realizations of random errors.
Therefore, any departure from the assumptions on the errors should
show up in the residuals. Plotting residuals is a very effective way to
investigate how well the assumptions hold for the data in hand.

Checking the independence assumption in (25.18) using residuals
is a bit tricky since the residuals are necessarily dependent6. But if
the statistical error are independent, then the dependence among the
residuals is very low (especially if n is large). One way to check the
independence assumption is to use the lag plot of the residuals, con-
structed by plotting residual ˆei against residual ˆei−1, for i = 2, . . . , n.
If the statistical errors are independent, there should be no pattern or
structure in the lag plot and the point {( ˆei−1, ˆei)} will appear to be
randomly scattered.

The second and third assumptions in (25.18) calls for plotting
the residuals versus the predictor. A plot of ˆei against Xi is called
a residual plot. If all the assumptions are satisﬁed, then the resid-
ual plot should look like in Fig. 25.2(a), where all residuals are ap-
proximately contained in a horizontal band centered at y-axes. The
pattern in Fig. 25.2(b) indicates that the assumption of constant vari-
ance (homoscedasticity) is violated. The presence of the curvature
in Fig. 25.2(c) signals for nonlinearity: E[ei|Xi] (cid:54)= 0, and, therefore,
E[Yi|Xi] (cid:54)= β0 + β1Xi. Nonlinearity can also be detected on the

statistical inference

143

5 Since β0 and β1 are unknown.

6 Recall than ∑n

i=1 ˆei = 0.

Figure 25.2: Panel (a): both zero mean
and constant variance assumptions ap-
pear to be true; Panel (b): homoscedas-
ticity is violated; Panel (c): linear model
for the mean response is doubtful.

Residuals^eiResiduals^eiPredictorXiResiduals^ei(a)(b)(c)144 k. m. zuev

scatterplot of the original data, but residual plots often give a better
“resolution” since a linear trend is removed.

Finally, to check the normality assumption (25.19), we can use a

normal Q-Q plot, where we plot the ordered residual ˆe(i) against the
corresponding normal quantiles7. Under the normality assumption,
the resulting points should lie approximately on a straight line.
Please, keep in mind, however, that the discussed diagnostics
are much better at indicating when the model assumption does
not hold than when it does. For example, if you see Fig. 25.2(b),
there is a problem with the constant variance assumption, but if you
see Fig. 25.2(a), it does not automatically mean that errors are ho-
moscedastic.

Final Remarks on Simple Linear Regression

Regression model is one of the most popular and widely used sta-
tistical models. As a result, often it is misused. Here is a list of most
common mistakes.

1. Regression model is often used for extrapolation: predicting the re-
sponse to the input which lies outside of the range of the values of
the predictor variable used to ﬁt the model. The danger associate
with extrapolation is illustrated in Fig. 25.3. The regression model
is “by construction” an interpolation model, and should not be
used for extrapolation, unless this is properly justiﬁed.

2. Outliers can strongly affect the OLS regression line8, and yet they

are often ignored and not taken care of. They can be detected
either on the scatter plot of the original data or on the residual
plot9. If you see outliers, ﬁrst, check that they are correct, i.e. a
true part of the system/phenomenon you study, and not a result
of some measurement error. If this is indeed the case, set them for
a separate study, which could be very interesting and rewarding.

3. Regression model studies dependence between input and re-

sponse. Strong (linear or nonlinear) dependence suggests but does
not imply that the variables are related in any causal sense. For ex-
ample, see Fig. 25.4: is the lack of pirates the real cause of global
warming? Correlation between the predictor and response is nec-
essary for causation, but not sufﬁcient.

Further Reading

1. For other abuses of regression see G.E.P. Box (1966) “Use and

abuse of regression,” Technometrics, 8(4): 625-629.

7 See Lecture 2.

Figure 25.3: Using the SLR model for
extrapolation can lead to misleading
predictions.
8 Recall the third examples of
Anscombe’s quartet in Lecture 23.
9 The corresponding residuals are much
larger in magnitude than all the others.

Figure 25.4: The simple linear regres-
sion would ﬁt almost perfectly here.
Picture source: wikipedia.org.

Predictor XResponse YRegression lineTrue regression functionr(x)=E[Y|X=x]New input X*Predictionstatistical inference

145

2. A comprehensive exposition of modern analysis of causation is
given in a highly cited monograph by J. Pearl (2009) Causality:
Models, Reasoning and Inference.

3. Finally, if your statistical analysis does not bring your the desired
results, I would recommend to use some techniques described in
D.Huff (2007) How to Lie with Statistics.

What is Next?

Many important areas of statistical inference — Bayesian inference,
causal inference, decision theory, simulation methods, to name but
a few — are not covered in these notes (but they are covered well in
the texts listed in the Preface). I hope to ﬁnd time in the future to
extend the notes and make them more coherent. Any feedback10 on
the current version would be greatly appreciated.

10 Emailed to kostia@caltech.edu.

