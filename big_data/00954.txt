6
1
0
2

 
r
a

M
4

 

 
 
]

G
L
.
s
c
[
 
 

2
v
4
5
9
0
0

.

3
0
6
1
:
v
i
X
r
a

Training Input-Output Recurrent Neural Networks

through Spectral Methods

Hanie Sedghi∗ Anima Anandkumar†

Abstract

We consider the problem of training input-output recurrent neural networks (RNN) for sequence
labeling tasks. We propose a novel spectral approach for learning the network parameters. It is based
on decomposition of the cross-moment tensor between the output and a non-linear transformation of
the input, based on score functions. We guarantee consistent learning with polynomial sample and
computational complexity under transparent conditions such as non-degeneracy of model parameters,
polynomial activations for the neurons, and a Markovian evolution of the input sequence. We also
extend our results to Bidirectional RNN which uses both previous and future information to output the
label at each time point, and is employed in many NLP tasks such as POS tagging.

Keywords: Recurrent neural networks, sequence labeling, spectral methods, tensor algebra, score func-
tion.

1 Introduction

Learning with sequence data is widely encountered in domains such as natural language processing, ge-
nomics, speech recognition, video processing, ﬁnancial time series analysis, and so on. Recurrent neural
networks (RNN) are a ﬂexible class of sequence models which can memorize past information, and selec-
tively pass it on across sequence steps on multiple scales. However, training RNNs is challenging in practice,
and backpropagation suffers from exploding and vanishing gradients as the length of the training sequence
grows. To overcome this, either RNNs are trained over short sequences or incorporate more complex ar-
chitectures such as long short-term memories (LSTM). For a detailed overview of RNNs, see (Lipton et al.,
2015). Figure 1 contrasts the RNN with a feedforward neural network which has no memory.

On the theoretical front, our understanding of RNNs is at best rudimentary. With the current techniques,
it is not tractable to analyze the highly non-linear state evolution in RNNs. Analysis of backpropagation is
also intractable due to non-convexity of the loss function, and in general, reaching the global optimum is
hard. We take the ﬁrst steps towards addressing these challenging issues in this paper.

We consider the class of input-output RNN or IO-RNN models, where each input in the sequence xt also
has an output label yt. These are useful for sequence labeling tasks, which has many applications such as
parts of speech (POS) tagging and named-entity recognition (NER) in NLP (Manning and Sch¨utze, 1999),
motif ﬁnding in protein analysis (Ben-Hur and Brutlag, 2006), action recognition in videos (Karpathy et al.,
2014), and so on.

∗University of California, Irvine. Email: sedghih@uci.edu
†University of California, Irvine. Email: a.anandkumar@uci.edu

1

Output

Output

Hidden Layer

Hidden Layer

Input

any t

(a) NN

Input

yt−1

Output

yt

yt+1

Hidden Layer

zt−1
Input

t = 1

t = 2

(b) IO-RNN

xt−1
(c) BRNN

xt

xt+1

Figure 1: Graphical representation of a Neural Network (NN) versus an Input-Output Recurrent Neural Network
(IO-RNN) and a Bidirectional Recurrent Neural Network (BRNN)

In addition, we also consider an extension of IO-RNN, viz., the bi-directional RNN or BRNN, ﬁrst pro-
posed by Schuster and Paliwal (1997). This incorporates information from both sides of the input sequence
for hidden state evolution, i.e. it includes two classes of hidden neurons: the ﬁrst class receives recurrent
connection from previous states, and the second class receives it from next steps. See Figure 1(c). BRNN is
useful in NLP tasks such as POS tagging, where both previous and next words in a sentence have an effect
on labeling the current word.

In this paper, we develop novel spectral methods for training IO-RNN and BRNN models. Spectral
methods have previously been employed for unsupervised learning of a range of latent variable models such
as hidden Markov models (HMM), topic models, network community models, and so on (Anandkumar et al.,
2014c). The idea is to decompose moment matrices and tensors using computationally efﬁcient algorithms.
The recovered components of the tensor decomposition yield consistent estimates of the model parameters.
However, a direct application of these techniques is ruled out due to non-linearity of the activations in the
RNN.

Recently, Janzamin et al. (2014) derived a new framework for training input-output models in the super-
vised framework. It is based on spectral decomposition of moment tensors, obtained after certain non-linear
transformations of the input. These non-linear transformations take the form of score functions, which
depend on generative models of the input. This provides a new approach for transferring generative infor-
mation, obtained through unsupervised learning, into discriminative training on labeled samples.

Based on the above approach, Janzamin et al. (2015) provided guaranteed risk bounds for training two-
layer feedforward neural network models with polynomial sample and computational bounds. The con-
ditions for obtaining the risk bounds are mild: a small approximation error for the target function under
the given class of neural networks, a generative input model with a continuous distribution, and general
sigmoidal activations at the neurons.

In this paper, we consider new spectral approaches for training IO-RNNs. The previous score function
approach for training feedforward networks (as described above) does not immediately extend, and there
are some non-trivial challenges: (i) Non-linearity in a RNN is propagated along multiple steps in the se-
quence, while in the two-layer feedforward network, non-linearity is applied only once to the input. It is
not immediately clear on how to “untangle” all these non-linearities and obtain guaranteed estimates of the
network weights. (ii) Learning bidirectional RNNs is even more challenging since recursive non-linearities
are applied in both the directions, and (iii) Assumption of i.i.d.
input and output training samples is no
longer applicable, and analyzing concentration bounds for samples generated from a RNN with non-linear
state evolution is challenging. We address all these challenges concretely in this paper.

2

1.1 Summary of Results

Our main contributions are: (i) novel approaches for training input-output RNN and bidirectional RNN
models using tensor decomposition methods, (ii) guaranteed recovery of network parameters with polyno-
mial computational and sample complexity, and (iii) transparent conditions for successful recovery based on
non-degeneracy of model parameters and bounded evolution of hidden states.

Score function transformations:
Training input-output neural networks under arbitrary input is compu-
tationally hard. On the other hand, we show that training becomes tractable through spectral methods when
the input is generated from a probabilistic model on a continuous state space. We assume knowledge of
score function forms, which correspond to normalized derivatives of the input probability density function
(p.d.f). For instance, if the input is standard Gaussian, score functions are given by the Hermite polynomials.
There are many unsupervised approaches for estimating the score function efﬁciently, see Appendix C.1.
To estimate the score function, one does not need to estimate the density, and this distinction is especially
crucial for models where the normalizing constant or the partition function is intractable to compute. Guar-
antees have been derived for estimating score functions of many ﬂexible model classes such as inﬁnite
dimensional exponential families (Sriperumbudur et al., 2013).

In this work, we assume a Markovian model for the input sequence {x1, . . . , xn} on a continuous state
space, e.g. autoregressive process of order one or their kernel counterparts (Kallas et al., 2011). For a
Markovian model, the score function only depends on the Markov kernel, and has a compact representation,
as seen in Section 2.5.

Tensor decomposition: We form cross-moments between the output label and score functions of the in-
put. For a vector input, the ﬁrst order score function is a vector, second order is a matrix, and higher orders
corresponds to tensors. Hence, the empirical moments are tensors, and we perform a CP tensor decompo-
sition to obtain the rank-1 components. Efﬁcient algorithms for tensor decomposition have been proposed
before, based on simple iterative updates such as tensor power method (Anandkumar et al., 2014c). After
some simple manipulations on the components, we provide estimates of the network parameters of RNN
models. The overall algorithm involves simple linear and multilinear steps, is embarrassingly parallel, and
is practical to implement. See (Huang et al., 2014a; Wang et al., 2015) for more discussion on algorithmic
issues.

Recovery guarantees: We guarantee consistent recovery under (a low order) polynomial and computa-
tional complexity. We consider the realizable setting when samples are generated by a IO-RNN or a BRNN
under the following transparent conditions: (i) one hidden layer of neurons with a polynomial activation
function, (ii) Markovian input sequence, (iii) full rank weight matrices on input, hidden and output layers,
and (iv) spectral norm bounds on the weight matrices to ensure bounded state evolution.

The polynomial activations are a departure from the usual sigmoidal units, but they can also capture
non-linear signal evolution, and have been employed in different applications, e.g. (Chen and Manning,
2014). The Markovian assumption on the input limits the extent of dependence and allows us to derive
concentration bounds for our empirical moments. Our results can be extended to general order Markov
models and latent space models such as HMMs.

The full rank conditions on the weight matrices imply non-degeneracy in the neural representation:
the weights for any two neurons cannot linearly combine to generate the weight of another neuron. Such

3

conditions have previously been derived for spectral learning of HMMs and other latent variable mod-
els (Anandkumar et al., 2014c). Moreover, it can be easily relaxed by considering higher order moment
tensors, and is relevant when we want to have more neurons than the input dimension in our network. The
rank assumption on the output weight matrix implies a vector output of sufﬁcient dimensions, i.e. sufﬁcient
number of output classes. This can be relaxed to a scalar output, the details are given in Appendix C.2.

The spectral norm condition on the weight matrices arises in the analysis of concentration bounds for the
empirical moments. Since we assume polynomial state evolution, it is important to ensure bounded values
of the hidden states, and this entails a bound on the spectral norm of the weight matrices. We employ con-
centration bounds for functions of Markovian input from (Kontorovich et al., 2008; Kontorovich and Weiss,
2014) and combine it with matrix Azuma’s inequality (Tropp, 2012) to obtain concentration of empirical
moment tensors. This implies learning RNNs with polynomial sample complexity.

Proof techniques: We rely on the property of score functions as derivative operators. Janzamin et al.
(2014) show that by computing the cross-moment E[f (x) · Sm(x)] between the input score function Sm(x)
and any function f (x), we obtain access to mth order derivative: E[∇m
x f (x)]. As an example, consider a
generalized linear model, where f (x) = σ(hw, xi), for some non-linear activation σ(·) and weight vector
w. Here, the ﬁrst order score function yields E[∇xf (x)] = µ · w, where µ := E[σ′(hw, xi) is a scaling
factor. Thus, in this simple example, by computing the cross-moment E[f (x) · S1(x)], we can estimate the
weight vector up to scaling.

It is immediately not clear how to extend the above idea to more complicated functions such as input-
output mappings in a RNN. There are multiple challenges: the complexity of forming score functions for
sequence data grows for general input. However, we show that this is not the case for Markovian input, and
the score functions only depend on the Markov kernel. The next challenge is: what form of cross moments
are useful in recovering network parameters?

Note that in an IO-RNN, all outputs y1, . . . , yn are dependent on the entire input sequence x1, . . . , xn,
and thus, we can form different partial derivatives E[∇m
yt]. We engineer an efﬁcient selection of
partial derivatives of low order which are informative of the network weights. For recovering the net-
work weights from input to hidden layer, we consider E[∇2
xtyt]. This yields a third order tensor for a
vector valued output yt, and the components of the decomposition correspond to input and output network
weights. By the full rank assumption, we are guaranteed to recover the weights consistently from earlier
results (Anandkumar et al., 2014c).

τt1 ...xτm

The more difﬁcult challenge is to deal with the recursive non-linearity of hidden state evolution.

In
general, this makes analysis of the above partial derivatives intractable. However, for polynomial activations
of degree l, we can use the property that by taking the lth order derivative, only the terms corresponding to
the highest degree survive. Unfortunately, this approach degrades as the degree of the polynomial activation
grows, and we currently do not know of a framework to overcome this. Finally, for a bidirectional RNN,
the problem of “unmixing” the contributions of neurons from both the directions appears to be intractable at
ﬁrst. However, by careful manipulation of the moments, we are able to guarantee recovery of all the network
parameters.

Outlook:
This work is a ﬁrst step towards answering challenging questions in sequence modeling. Many
of the assumptions can be relaxed, e.g. Markovian assumption can be extended to general order Markov
models and latent space models such as HMMs. Here, in IO-RNNs we have aligned inputs and outputs
which is not applicable to tasks such as machine translation, and we can relax this assumption to obtaining
more general RNNs. Architectures such as long short-term memory (LSTM) have much more complicated

4

non-linear dependencies, and it is unclear on how to analyze them effectively. We have assumed polynomial
activation functions at the neurons and our computational and sample complexity degrade exponentially in
the degree of the polynomial. It is an open question to develop strategies to avoid this exponential blowup,
and to expand it to the usual sigmoidal units in RNNs.

We have assumed the realizable setting where samples are generated from a RNN. For general se-
quences, it is unclear on how to analyze the approximation bounds. While a solid theoretical framework
exists for function approximation in the i.i.d. setting (Barron, 1994), the question of approximation bounds
by a RNN with a ﬁxed number of neurons is not satisfactorily resolved (Hammer, 2000). Analysis under
non-stationary inputs is another challenging open problem.

We have provided a framework to transfer feature representations from unsupervised learning to dis-
criminative training through the score function forms. It has been argued and empirically observed that in a
multi-layer neural network, lower layers of neurons perform feature transformations that are not specialized
to any task and are easily transferrable (Yosinski et al., 2014). Only the last few layers of the neural network
are discriminative, and specialized to speciﬁc task. With this viewpoint, our assumption of knowledge of
score function forms implies success in training these feature transformations in the lower layers. Indeed,
further study is needed to analyze this aspect, and our focus here is on training input-output mapping in a
RNN, given the score function forms.

1.2 Related work

The following works are directly relevant to this paper.

Spectral approaches for sequence learning:
Previous guaranteed approaches for sequence learning
mostly focus on the class of hidden Markov models (HMM). Anandkumar et al. (2014c) provide a tensor
decomposition method for learning the parameters under non-degeneracy conditions, similar to ours. This
framework is extended to more general HMMs in (Huang et al., 2014b). While in a HMM, the relationship
between the hidden and observed variables can be modeled as a linear one1, in a RNN it is non-linear.
However, in a IO-RNN, we have both inputs and outputs, and that is helpful in handling the non-linearities.

Input-output sequence models:
A rich set of models based on RNNs have been employed in prac-
tice in a whole range of applications. Lipton et al. (2015) provides a nice overview of these various mod-
els. Balduzzi and Ghifari (2016) recently apply physics based principles to design RNNs for stabilizing
gradients and getting better training error. However, a rigorous analysis of these techniques is lacking.

2 Preliminaries

2.1 Notation

Let [n] := {1, 2, . . . , n}, and kuk denote the ℓ2 or Euclidean norm of vector u, and hu, vi denote the inner
product of vectors u and v. For sequence of n vectors v1, . . . , zn, we use the notation z[n] to denote the
whole sequence. For vector v, v∗m refers to elementwise mth power of v. For matrix C ∈ Rd×k, the j-th
column is referred by Cj or cj, j ∈ [k], the jth row is referred by C (j) or c(j), j ∈ [d] and kCk denotes

1Although the linear relationship is not obvious in a discrete model, this is clear under one hot encoding of both hidden variable
h and observed variable y, here E[y|h] = Ah, where A is the conditional probability table, and h selects a column of the table
depending on the hidden state.

5

the spectral norm of matrix C. Throughout this paper, ∇(m)
variable x.

x

denotes the mth order derivative operator w.r.t.

Tensor: A real mth order tensor T ∈Nm Rd is a member of the outer product of Euclidean spaces Rd.

The different dimensions of the tensor are referred to as modes. For instance, for a matrix, the ﬁrst mode
refers to columns and the second mode refers to rows.

Tensor matricization: For a third order tensor T ∈ Rd×d×d, the matricized version along ﬁrst mode
denoted by M ∈ Rd×d2 is deﬁned such that

T (i, j, l) = M (i, l + (j − 1)d),

i, j, l ∈ [d].

(1)

Tensor Reshaping:
T2 = Reshape(T1, v1, . . . , vl) means that T2 is a tensor of order l that is made by
reshaping tensor T1 such that the ﬁrst mode of T2 includes modes of T1 that are shown in v1, the second
mode of T2 includes modes of T1 that are shown in v2 and so on. For example if T1 is a tensor of order 5,
T2 = Reshape(T1, [1 2], 3, [4 5]) is a third order tensor, where its ﬁrst mode is made by concatenation of
modes 1, 2 of T1 and so on.

Tensor rank: A 3rd order tensor T ∈ Rd×d×d is said to be rank-1 if it can be written in the form

T = w · a ⊗ b ⊗ c ⇔ T (i, j, l) = w · a(i) · b(j) · c(l),

(2)

where ⊗ represents the outer product, and a, b, c ∈ Rd are unit vectors. A tensor T ∈ Rd×d×d is said to
have a CP (Candecomp/Parafac) rank k if it can be (minimally) written as the sum of k rank-1 tensors

T = Xi∈[k]

wiai ⊗ bi ⊗ ci, wi ∈ R, ai, bi, ci ∈ Rd.

(3)

Note that v⊗p = v ⊗ v ⊗ v · · · ⊗ v, where v is repeated p times.

Deﬁnition 1 (Khatri-Rao product) For matrices A, B ∈ Rd×k, the Khatri-Rao product is deﬁned below

a1
a2
...
ak





⊙

b1
b2
...
bk



=

a1 ⊗ b1
a2 ⊗ b2

...

ak ⊗ bk

,



where ai, bi are rows of A, B respectively. Note that our deﬁnition is different from usual deﬁnition of KR
product which is performed on columns of matrices.

Tensor as multilinear form: We view a tensor T ∈ Rd×d×d as a multilinear form. Consider matrices
Ml ∈ Rd×dl, l ∈ {1, 2, 3}. Then tensor T (M1, M2, M3) ∈ Rd1 ⊗ Rd2 ⊗ Rd3 is deﬁned as

T (M1, M2, M3)i1,i2,i3 := Xj1,j2,j3∈[d]

Tj1,j2,j3 · M1(j1, i1) · M2(j2, i2) · M3(j3, i3).

(4)

6

In particular, for vectors u, v, w ∈ Rd, we have 2

T (I, v, w) = Xj,l∈[d]

vjwlT (:, j, l) ∈ Rd,

(5)

which is a multilinear combination of the tensor mode-1 ﬁbers. Similarly T (u, v, w) ∈ R is a multilinear
combination of the tensor entries, and T (I, I, w) ∈ Rd×d is a linear combination of the tensor slices.

Derivative: For function g(x) : Rd → R with vector input x ∈ Rd, the m-th order derivative w.r.t.
variable x is denoted by ∇(m)

x g(x) ∈Nm Rd (which is a m-th order tensor) such that
h∇(m)
x g(x)ii1,...,im

∂xi1∂xi2 · · · ∂xim

∂g(x)

:=

,

i1, . . . , im ∈ [d].

(6)

When it is clear from the context, we drop the subscript x and write the derivative as ∇(m)g(x).

Derivative of product of two functions We frequently use the following gradient rule.

Lemma 1 (Product rule for gradient (Janzamin et al., 2014)) For tensor-valued functions F (x) : Rn →

Np1 Rn, G(x) : Rn →Np2 Rn, we have

∇x(F (x) ⊗ G(x)) = (∇xF (x) ⊗ G(x))hπi + F ⊗ ∇xG(x),

where the notation hπi denotes permutation of modes of the tensor for permutation vector π = [1, 2, . . . , p1, p1+
2, p1 + 3, . . . , p1 + p2 + 1, p1 + 1]. This means that the (p1 + 1)th mode is moved to the last mode.

2.2 Problem Formulation

We consider a two-layer input-output RNN modeled below

E[yt|ht] = A⊤2 ht,

ht = polyl(A1xt + U ht−1),

where polyl(·) denotes an element-wise polynomial of order l, The input sequence x consists of the vectors
xt ∈ Rdx, ht ∈ Rdh, yt ∈ Rdy and hence A1 ∈ Rdh×dx, U ∈ Rdh×dh and A2 ∈ Rdh×dy . We can learn
the parameters of the model using our method. Our algorithm is called GLOREE (Guaranteed Learning Of
Recurrent nEural nEtworks) and is shown in Algorithm 1.

Throughout the paper, we assume that the probability distribution of the input sequence vanishes in the

boundary. This is also the assumption in (Janzamin et al., 2014).

We consider the case where input is a Markov chain. Then in order to have mixing and assure ergodicity

for the output, we need to impose additional constraints on the model.

2Compare with the matrix case where for M ∈ Rd×d, we have M (I, u) = M u := Pj∈[d] uj M (:, j) ∈ Rd.

7

2.3 Review of Score functions

As mentioned in the introduction, our method builds on the method introduced by Janzamin et al. (2014)
called FEAST (Feature ExtrAction using Score function Tensors). The goal of FEAST is to extract discrim-
inative directions using the cross-moment between the label and score function of input. Score function is
the normalized (higher order) derivative of probability distribution of the input.

Let p(x) denote the joint probability density function of random vector x ∈ Rd. Janzamin et al. (2014)

denote Sm(x) as the mth order score function, given by

Sm(x) = (−1)m ∇(m)

x p(x)
p(x)

,

(7)

where ∇(m)
recursive form

x

denotes the mth order derivative operator w.r.t. variable x. It can also be derived using the

S1(x) = −∇x log p(x),
Sm(x) = −Sm−1(x) ⊗ ∇x log p(x) − ∇xSm−1(x).

(8)

The importance of score function is that it provides a derivative operator. Janzamin et al. (2014) proved
that the cross-moment between the label and the score function of the input yields the information regarding
derivative of the label w.r.t. the input.

Theorem 2 (Yielding differential operators (Janzamin et al., 2014)) Let x ∈ Rdx be a random vector
with joint density function p(x). Suppose the mth order score function Sm(x) deﬁned in (7) exists. Consider

any continuously differentiable tensor function G(x) : Rdx →Nr Rdy . Then, we have

E [G(x) ⊗ Sm(x)] = Eh∇(m)

x G(x)i .

2.4 Score function form for RNN

We now extend the notion of score function to handle sequence data with non i.i.d. samples. We denote the
score function at each time step t in the sequence as Sm(z[n], t), where z[n] := z1, z2, . . . , zn, and it is de-
ﬁned below. The score function for the entire sequence as Sm(z[n], [n]) := [Sm(z[n], 1)⊤, Sm(z[n], 2)⊤, . . .]⊤
is the concatenation of the scores at each time step.

Theorem 6 in (Janzamin et al., 2014) can be readily modiﬁed to

Lemma 3 (Score function form for partitioned input sequence) For vector sequence z[n] = {z1, z2, . . . , zn},
let p(z1, z2, · · · , zn) and Sm(z[n], [n]) respectively denote the joint density function and the corresponding
mth order score function. Then, under some mild regularity conditions, for all functions G(z1, z2, . . . , zn),
we have

E [G(z1, z2, . . . , zn) ⊗ Sm(z[n], t)] = Eh∇(m)

zt G(z1, z2, . . . , zn)i ,

where ∇(m)

zt denotes the mth order derivative operator w.r.t. zt,

Sm(z[n], t) = (−1)m ∇ztp(z1, z2, . . . , zs)
p(z1, z2, . . . , zs)

.

(9)

8

2.5 Score function form for Markov chains

We assume a Markovian model for the input sequence, and derive compact score function forms for (9).
Note that this form can be readily expanded to higher-order Markov chains.

Lemma 4 (Score function form for ﬁrst-order Markov Chains) Let the input sequence {xi}i∈[n] be a
ﬁrst-order Markov chain. The score function in (9) simpliﬁes as

xi [p(xi+1|xi)p(xi|xi−1)]
p(xi+1|xi)p(xi|xi−1)
The proof follows the deﬁnition of ﬁrst-order Markov chain and Equation 9.

Sm(x[n], i) = (−1)m ∇m

.

(10)

Score function form for Gaussian processes The Gaussian AR-1 process is a special case of the Markov
chain: we have xt = φxt−1 + ǫt,
|φ| < 1, where index t represents the time and |φ| < 1.
For simplicity, we have considered scalar x, the form can be easily extended to vector x.

ǫt ∼ N (0, 1),

The conditional distribution is given by xt|x1, · · · , xt−1 ∼ N (φxt−1, 1),

t = 2, · · · , n. and

1

(2φ)n/2

|J|1/2 exp(cid:18)−

1
2

x[n]⊤Jx[n](cid:19) ,

−φ

1
−φ 1 + φ2 −φ
. . .

p(x[n]) =

J =



. . .

.. .
−φ 1 + φ2 −φ
1

−φ

,



with zero entries outside the diagonal and ﬁrst off-diagonals (Rue and Held, 2005).

Expanding Equation (8) for any Gaussian Markov Random Fields(GMRF) model, we have that

S1(x[n], [n]) = x[n]⊤J,
Sm(x[n], [n]) = Sm−1(x[n], [n]) ⊗ (x[n]⊤J) − ∇x[n]Sm−1(x[n], [n]).

This simpliﬁes as

S1(x[n], [n]) =



3 Algorithm and Guarantees

xt − φxt−1

−φxt + (1 + φ2)xt−1 − φxt−2

...

−φx3 + (1 + φ2)x2 − φx1

−φx2 + x1

.



(11)

In this paper, we have functions which map input sequence x1, . . . xn to an output sequence y1, . . . , yn. By
assuming a structured form of function mapping in terms of IO-RNN, we can hope to recover the function
parameters efﬁciently. We exploit the score function forms derived in the previous section to compute partial
derivatives of the form E[∇m

yt]. We ﬁrst start with some simple intuitions.

xτ1 ...xτm

9

3.1 Preliminary insights

Generalized linear model: Before considering the RNN, consider a simple generalized linear model with
i.i.d. samples: E[y|x] = A⊤2 σ(A1x), where A1 is the weight matrix and σ(·) is element-wise activation.
Here, the partial derivative of y w.r.t. x has a linear relationship with the weight matrices A1 and A2, i.e.

E[∇xy] = E[∇xσ(A1x)] = A⊤2

E[σ′(A1x)]A1.

(12)

The partial derivative E[∇xy] is obtained by forming the cross moment E[y⊗S1(x)], as given by Theorem 2.
The form in (12) yields A1 and A2 up to a linear transformation. However, by computing second order
derivatives, we can recover A1 and A2, up to scaling of their rows. The second-order derivative has the form

E[y ⊗ S2(x)] = E[∇2

xy] =Xi∈dh

µiA(i)

2 ⊗ A(i)

1 ⊗ A(i)
1 .

(13)

The tensor decomposition in (Anandkumar et al., 2014c) uniquely recovers A1, A2 up to scaling of rows,
under full row rank assumptions.

Recovering input-output weight matrices in IO-RNN:
over to IO-RNN. Recall that the IO-RNN has the form

The above intuition for GLM readily carries

E[yt|ht] = A⊤2 ht,

ht = polyl(A1xt + U ht−1),

where polyl denotes any polynomial function of degree at most l.

Suppose we have access to partial derivatives E[∇xtyt] and E[∇2

xtyt], then they have the same forms as
(12) and (13). This is because ht does not depend on xt given x[t − 1]. Thus, the weight matrices A1 and
A2 can be easily recovered by forming E[yt ⊗ S2(x[n], t)], as given by (9), and it has an compact form for
Markovian input in (10). Note that this intuition holds for any non-linear element-wise activation function,
and we do not require it to be a polynomial at this stage.

Recovering hidden state transition matrix in IO-RNN: Recovering the transition matrix U is much
more challenging since we do not have access to hidden state sequence h[n]. Thus, we cannot readily form
partial derivatives of the form E[∇m
h[n]y[n]]. Moreover, the non-linearities get recursively propagated along
the chain. Here, we provide an algorithm that works for any polynomial activation function of ﬁxed degree
l.

The main idea is that we attempt to recover U by considering partial derivatives E[∇m

xt−1yt], i.e., how
the previous input xt−1 affects current output yt. At ﬁrst glance, this appears complicated and indeed,
the various terms are highly coupled and we do not have a nice CP tensor decomposition form as before.
However, we can prove that when the derivative order m is sufﬁciently large, a nice CP tensor form emerges
out, and this m depends on the degree l of the polynomial activation.

For simplicity, we provide intuitions for the quadratic activation function (l = 2). Now, yt is a quartic or
a degree-4 polynomial function of xt−1, since the activation function is applied twice. By considering fourth
order derivative E[∇4
xt−1yt], many coupled terms vanish since they correspond to polynomial functions of
degree less than 4. The surviving term has a nice CP tensor form, and we can recover U efﬁciently from it.
Note that this fourth order partial derivative can be computed efﬁciently using fourth order score function
and forming the cross moment E[yt ⊗ S4(x[n], t − 1)]. The precise algorithm is given in Algorithm 1.

10

Algorithm 1 GLOREE (Guaranteed Learning Of Recurrent nEural nEtworks) for vector input
input Labeled samples {(xi, yi) : i ∈ [n]} from IO-RNN model in Figure 1(b), polynomial order l for

activation function.

1: Compute 2nd-order score function S2(x[n], i) of the input sequence as in Equation (10).

5: Compute l2th-order score function Sl2(x[n], i) of the input sequence as in Equation (10).

2: Compute bT := bE [yi ⊗ S2(x[n], i)], bD := bE [yi ⊗ S1(x[n], i)]. bD will be used for whitening (Proce-
3: { ˆw, ˆR1, ˆR2, ˆR3} = tensor decomposition(bT , bD); see Algorithm 3 in the Appendix.
4: ˆA2 = ˆR1, ˆR1 =(cid:16) ˆR2 + ˆR3(cid:17) /2.
6: Compute bT = ˆE(cid:2)yt ⊗ Reshape(Sl2(x[n], t − 1), 1, [1 . . . l], . . . , [l2 − l + 1 . . . l2](cid:3).
7: Contract the moment-tensor to a 3-rd order tensor as bT2 = T (I, I, I, θ, . . . , θ) with some random vector
8: { ˆw, ˆR1, ˆR2, ˆR3} = tensor decomposition(bT2); see Algorithm 3 in the Appendix.
9: ˜R =(cid:16) ˆR2 + ˆR3(cid:17) /2.
10: ˆU = ˜Rh ˆA1 ⊙ ˆA1i−1

, Khatri-Rao product ⊙ is deﬁned in Deﬁnition 1.

11: return ˆA1, ˆA2, ˆU.

dure 4).

θ.

3.2 IO-RNN with quadratic activation functions

We now provide our algorithm for training IO-RNNs. It is called GLOREE (Guaranteed Learning Of Re-
current nEural nEtworks) and is shown in Algorithm 1. Here we consider vector output and polynomial
activation functions of order l ≥ 2. We handle the linear case separately in Appendix C.3. We also cover
the case where output y is a scalar (e.g. a binary label) in Appendix C.2.

Our framework can deal with polynomial activation functions of any order l ≥ 2. We ﬁrst start with
quadratic activation function to provide a step-by-step introduction to GLOREE for better explanation. story
telling, start from linear, there are tricks to do but we use it as a building block to show our method, toeplitz,
easier or harder? but we expand to higher order polynomials

Now, we consider an RNN with quadratic activation function and vector output. Let

E[yt|ht] = A⊤2 ht,

ht = poly2(A1xt + U ht−1),

where xt ∈ Rdx, ht ∈ Rdh, yt ∈ Rdy and hence A1 ∈ Rdh×dx , U ∈ Rdh×dh, A2 ∈ Rdh×dy . We can learn
the parameters of the model using GLOREE. Let n be the window size for RNN. In order to learn A1 we
form the cross-moment tensor E[yt ⊗ S2(xt)]. We have the following result
Theorem 5 (Learning parameters of RNN for quadratic activation function) We have the following prop-
erties for an IO-RNN:

E [yt ⊗ S2(x[n], t)] = 2Xi∈dh

2 ⊗ A(i)
A(i)

1 ⊗ A(i)
1 .

(14)

Hence, we can recover A1, A2 via tensor decomposition, assuming that they are full row rank.

In order to learn U we form the tensor E [yt ⊗ S4(xt−1)], and under quadratic activations, we have
2 ⊗ [U (A1 ⊙ A1)](i) ⊗ [U (A1 ⊙ A1)](i).

E [yt ⊗ Reshape(S4(x[n], t − 1), [1 2], [3 4])] =Xi∈dh

A(i)

11

Hence, we can recover U (A1 ⊙ A1) via tensor decomposition. Since A1 is previously recovered using
(14), and (A1 ⊙ A1)† exists due to full rank assumption, we can recover U. Thus, Algorithm 1 (GLOREE)
consistently recovers the parameters of IO-RNN under quadratic activations.

Proof:

The underlying idea behind the proof comes from Theorem 2. Hence

xtyt(cid:3) ,
E [yt ⊗ Reshape(S4(x[n], t − 1), [1 2], [3 4])] = EhReshape(∇4
1 ⊗ Xm∈dh

E [yt ⊗ S2(x[n], t)] = E(cid:2)∇2
2 ⊗ Xk∈dh

xt−1yt = 2Xj∈dh

1 ⊗ A(k)

UjkA(k)

T = ∇4

A(j)

xt−1yt, [1 2], [3 4])i .

UjmA(m)

1 ⊗ A(m)

1

,

By reshaping the above tensor to Reshape(∇4
A1)](j) ⊗ [U (A1 ⊙ A1)](j). For detailed proof, see Appendix B.1.

xt−1yt, [1 2], [3 4]) we have the formPj∈dh

A(j)

2 ⊗ [U (A1 ⊙
(cid:3)

3.3 IO-RNN with polynomial activation functions

Next, we consider an RNN with polynomial activation function of order l, i.e.

E[yt|ht] = A⊤2 ht,

ht = polyl(A1xt + U ht−1).

(15)

We have the following properties.

Theorem 6 (Learning parameters of RNN for general polynomial activation function) The following is
true:

E [yt ⊗ S2(x[n], t)] =Xi∈dh

µiA(i)

2 ⊗ A(i)

1 ⊗ A(i)
1 .

(16)

Hence, we can recover A1, A2 via tensor decomposition assuming that they are full row rank.

In order to learn U we form the tensor E [yt ⊗ Sl2(xt−1)]. Then we have

E(cid:2)yt ⊗ Reshape(Sl2(x[n], t − 1), 1, [1 . . . l], . . . , [l2 − l + 1 . . . l2](cid:3) =Xi∈dh

A(i)

2 ⊗(cid:20)hU (A⊙l

1 )i(i)(cid:21)⊗l

.

(17)

Hence, we can recover U (A⊙l
1 ) via tensor decomposition under full row rank assumption. Since A1 is pre-
viously recovered, U can be recovered. Thus, Algorithm 1(GLOREE) consistently recovers the parameters
of IO-RNN with polynomial activations.

Remark on coefﬁcients µi:
For the cross-moment tensor in (6), the coefﬁcients µi are the expected values
of derivatives of activation function. More concretely if activation is a polynomial of degree l, we have

that µi = Ehpolyl−2(cid:16)hA(i)

1 , xti + hU (i), ht−1i(cid:17)i, where polyl−2 denotes a polynomial of degree l − 2.

Similarly, the coefﬁcients of the tensor decomposition in (17) correspond to expectations over derivatives
of (recursive) activation functions. We assume that these coefﬁcients are non-zero in order to recover the
weight matrices.

12

Proof: By Theorem 2, we have that

E [yt ⊗ S2(x[n], t)] = E(cid:2)∇2
E(cid:2)yt ⊗ Reshape(Sl2(x[n], t − 1), [1 . . . l], . . . , [l2 − l + 1 . . . l2])(cid:3)
= EhReshape(∇l2

xtyt, 1, [2 . . . l + 1], . . . , [l2 . . . l2 + 1])i .

xtyt(cid:3) ,

The form follows directly using the derivative rule as in Lemma 1.

(cid:3)
Thus, we provide an efﬁcient framework for recovering all the weight matrices of an input-output recur-

rent neural network using tensor decomposition methods.

3.4 Bidirectional RNN with polynomial activation functions

Bidirectional Recurrent Neural network was ﬁrst proposed by Schuster and Paliwal (1997). Here there two
groups of hidden neurons.The ﬁrst group receives recurrent connections from previous time steps while the
other from the next time steps. The following equations describe a BRNN

E[yt|ht] = A⊤2 (cid:20) ht
zt (cid:21) ,

ht = f (A1xt + U ht−1),

zt = g(B1xt + V zt+1),

(18)

where ht and zt denote the neurons that receive forward and backward connections respectively. Note that
BRNNs cannot be used in online settings as they require knowing the future steps. However, in various
natural language processing applications such as part of speech (POS) BRNNs are effective models since
they consider both past and future words in a sentence.

We can learn the parameters of bidirectional RNN by modifying our earlier algorithm, and the new
algorithm is in Algorithm 2. For simplicity, we limit ourselves to quadratic activation functions for f (·) and
g(·) here. The general polynomial function is shown in Algorithm 8 in Appendix D.

Before getting into details of the algorithm, let us provide some intuitions. If we only had forward or
backward connections, we would directly apply our previous method in GLOREE. For backward connec-
tions, the only difference would be to use derivatives of yt w.r.t. xt+1 to learn the transition matrix V .

Now that we have both hidden neurons mixing to yield the output vector y, the cross-moment tensor
T = E[yi ⊗ S2(x[n], i)] = E[∇2
xiyi] has a CP decomposition where the factor matrix for the ﬁrst mode is
A2. Hence, under full rank assumption, as before, we can recover A2. Now the tensor has a speciﬁc form:

Tz (cid:21), where Th corresponds to the tensor incorporating columns of A1 and Tz incorporates

columns of B1. Since, A2 is already estimated, we can invert it to recover Th and Tz. We decompose them
to recover A1 and B1.

T = A⊤2 (cid:20) Th

The steps for recovering U and V remains the same as before in GLOREE. The only difference is to use
derivatives of yt w.r.t. xt+1 to learn V . Note that in order to recover U and V , we can either decompose a
tensor of order l + 1 or contract the tensor to a third-order tensor, and then decompose the contracted tensor.
For simplicity, we show the latter in our algorithm.

Theorem 7 (Training BRNN) Consider the BRNN model as in (18). Algorithm 2 correctly recovers pa-
rameters of this model.

For proof, see Appendix B.2.

13

3.5 Sample and Computational Complexity Analysis

3.5.1 Sample complexity analysis

We now analyze the sample complexity for GLOREE. We ﬁrst start with the concentration bound for the mo-
ment tensor and then use analysis of tensor decomposition to show that our method has a sample complexity
that is a polynomial of the the model parameters.

We ﬁrst derive concentration bounds for the empirical moment tensors. This is a bit more involved
since we have a non-i.i.d. sequence. Moreover, since we assume a polynomial activation function, the
hidden state ht can grow in an unbounded manner. To avoid this, without loss of generality, we assume
kxik2 < 1, ∀i ∈ [n] (with high probability).

Concentration bounds for functions over Markov chains: Our input sequence x[n] is a Markov chain.
We can think of the empirical moment ˆE[yt ⊗ Sm(x[n], t)] as functions over the samples x[n] of the Markov
chain. Note that this assumes h[n] and y[t] as deterministic functions of x[t], and our analysis can be
extended when there is additional randomness.

We now recap concentration bounds for general functions on Markov chains. For any ergodic Markov
chain with the stationary distribution ω, denote f1→t(xt|x1) as state distribution given initial state x1. The
inverse mixing time is deﬁned as follows

For an ergodic Markov chain,

ρmix(t) = sup
x1

kf1→t(xt|x1) − ωk.

ρmix(t) ≤ Gθt−1,

where 1 ≤ G < ∞ is geometric ergodicity and 0 ≤ θ < 1 is the contraction coefﬁcient of the Markov
chain.

In the IO-RNN (and BRNN) model, the output is a nonlinear function of the input. Hence, the next step
is to deal with this non-linearity. Kontorovich and Weiss (2014) analyze the mixing of a (scalar) nonlinear
function through its Lipschitz property. In order to analyze how the empirical moment tensor concentrates,
we deﬁne the Lipschitz constant for matrix valued functions.

Deﬁnition 2 (Lipschitz constant for a matrix-valued function of a sequence) A matrix-valued function Φ :
Rndx → Rd1×d2 is c-Lipschitz with respect to the spectral norm if

sup

x[n],˜x[n]

kΦ(x[n]) − Φ(˜x[n])k2

kx[n] − ˜x[n]k2

≤ c,

where x[n], ˜x[n] are any two possible sequences of observations. Here k · k denotes the spectral norm and
Rndx is the state space for a sequence of n observations x[n].

Concentration of empirical moments of IO-RNN and BRNN:
moment tensor has a bounded Lipschitz constant, we need the following assumptions.

In order to ensure that the empirical

14

Assumptions:

• Without loss of generality, we assume that the input sequence is bounded by 1 with high probability,

kxik < 1 ∀i ∈ [n]

• Assume that kA1 + U k ≤ 1.

• If the activation function is a polynomial of order l, we need kU k ≤ 1/l.

• max
i∈[n]

kS2(x[n], i)k, max
i∈[n]

Under the above assumptions, we have that

{kS3(x[n], i) − S2(x[n], i) ⊗ S1(x[n], i)k} are bounded.

Lemma 8 (Lipschitz property for the Empirical Moment Tensor) For the IO-RNN discussed in (15), if

the above assumptions hold, the matricized tensor Mat(cid:16)ˆE[yi ⊗ S2(x[n], i)](cid:17) is a function of the input se-

quence with Lipschitz constant

c ≤

1
n

kA2kkA1k
1 − lkU k

max
i∈[n]

kS2(x[n], i)k + kA2kk max
i∈[n]

For proof, see Appendix B.3.

{kS3(x[n], i) − S2(x[n], i) ⊗ S1(x[n], i)k} ,

(19)

Remark:
have that

For a Gaussian AR-1 process, form of the score function is shown in (11). Since |φ| ≤ 1, we

kS2(x[n], i)k ≤ (1 + |φ|)4 + (1 + |φ|)2 ≤ 20,

{kS3(x[n], i) − S2(x[n], i) ⊗ S1(x[n], i)k} ≤ (1 + |φ|)4 ≤ 16.

max
i∈[n]
max
i∈[n]

In this case, the Lipschitz constant is

Given this Lipschitz constant, we can now apply the following concentration bound.

c ≤

20
n

kA2kkA1k
1 − lkU k

+ 16kA2k.

Theorem 9 (Concentration bound for RNN) For the IO-RNN discussed in (15), let z[n] be the sequence

of matricized empirical moment tensors Mat(cid:16)ˆE[yi ⊗ S2(x[n], i)](cid:17) for i ∈ [n]. Then,
(cid:19),

1 − θ s8c2n log(cid:18) dy + d2

1 + 1√8cn1.5

kz − E(z)k ≤ G

δ

x

with probability at least 1 − δ, where E(z) is expectation over samples of Markov chain when the initial
distribution is the stationary distribution and c is speciﬁed in Equation (19).

For proof see Appendix B.4.
Considering the analysis of tensor decomposition analysis in (Janzamin et al., 2015), Theorem 9 implies

polynomial sample complexity for GLOREE. The sample complexity is

poly(dx, dy, dh, G,

1

1 − θ

, σ−1

min(A1), σ−1

min(A2), σ−1

min(U )).

Note that with similar analysis we can prove polynomial sample complexity for GLOREE-B.

15

Algorithm 2 GLOREE-B (Guaranteed Learning Of Recurrent nEural nEtworks-Bidirection case) for
quadratic activation function (general case is shown in Algorithm 8
input Labeled samples {(xi, yi) : i ∈ [n]}, both activation functions in the forward and backward direction

are quadratic.

input 2nd-order score function S2(x[n], [n]) of the input x; see Equation (7) for the deﬁnition.

3: ˆA2 = ˆR1.

5: {( ˆw, ˆR1, ˆR2, ˆR3} = tensor decomposition( ˜T );
6: ˆC = ( ˆR2 + ˆR3)/2.

1: ComputebT := ˆE[yi⊗S2(x[n], i)], bD := ˆE[yi⊗S1(x[n], i)]. bD will be used for whitening (Procedure 4).
2: {( ˆw, ˆR1, ˆR2, ˆR3} = tensor decomposition(bT ); see Algorithm 3 in the Appendix.
4: Compute ˜T = bT ((( ˆA2)⊤)−1, I, I). For deﬁnition of multilinear form see Section 2.1.
7: ˆC =(cid:20) ˆA1
ˆB1 (cid:21).
9: Compute bT = ˆE [yt ⊗ Reshape(S4(x[n], t − 1), 1, [1 2], . . . , [3 4]].
10: Contract the moment-tensor to a 3-rd order tensor as bT2 = T (I, I, I, θ, θ) with some random vector θ.
11: { ˆw, ˆR1, ˆR2, ˆR3} = tensor decomposition(bT2); see Algorithm 3 in the Appendix.
12: ˜R =(cid:16) ˆR2 + ˆR3(cid:17) /2.
13: ˆU = ˜Rh ˆA1 ⊙ ˆA1i−1

8: Compute 4th-order score function S4(x[n], t − 1) of the input sequence as in Equation (10).

, Khatri-Rao product ⊙ is deﬁned in Deﬁnition 1.

14: Repeat lines (8)-(13) with S4(x[n], t + 1) instead of S4(x[n], t − 1) to recover ˆV .
15: return ˆA1, ˆA2, ˆB1, ˆU , ˆV .

3.5.2 Computational Complexity

The computational complexity of our method is related to the complexity of the tensor decomposition meth-
ods. See (Anandkumar et al., 2014c; Janzamin et al., 2015) for a detailed discussion. It is popular to perform
the tensor decomposition in a stochastic manner by splitting the data into mini-batches and reducing compu-
tational complexity. Starting with the ﬁrst mini-batch, we perform a small number of tensor power iterations,
and then use the result as initialization for the next mini-batch, and so on. We assume that score function is
given to us in an efﬁcient form. Note that if we can write the cross-moment tensor in terms of summation
of rank-1 components, we do not need to form the whole tensor explicitly. As an example, if input follows
Gaussian distribution, the score function has a simple polynomial form, and the computational complexity
of tensor decomposition is O(ndhdxR), where n is the number of samples and R is the number of initial-
izations for the tensor decomposition. Similar argument follows when the input distribution is mixture of
Gaussian distributions.

4 Conclusion

This work is a ﬁrst step towards answering challenging questions in sequence modeling. Many of the as-
sumptions can be relaxed, e.g., here we assumed IO-RNNs with aligned inputs and outputs which is not
applicable to tasks such as machine translation, and we can relax this assumption to obtain more general
RNNs. We have assumed polynomial activation functions at the neurons and our computational and sample

16

complexity degrade exponentially in the degree of the polynomial. It is an open question to develop strate-
gies to avoid this exponential blowup, and to extend it to the usual sigmoidal units in RNNs. Architectures
such as long short-term memory (LSTM) have much more complicated non-linear dependencies, and it is
unclear on how to analyze them effectively. Analysis under non-stationary inputs is another challenging
open problem.

We have assumed the realizable setting where samples are generated from a RNN. For general se-
quences, it is unclear on how to analyze the approximation bounds. While a solid theoretical framework
exists for function approximation in the i.i.d. setting (Barron, 1994), the question of approximation bounds
by a RNN with a ﬁxed number of neurons is not satisfactorily resolved (Hammer, 2000).

Acknowledgment

The authors thank Majid Janzamin for detailed discussions on sample complexity and constructive com-
ments on the draft. H. Sedghi is supported by NSF Career award FG15890. A. Anandkumar is supported in
part by Microsoft Faculty Fellowship, NSF Career award CCF-1254106, ONR award N00014-14-1-0665,
ARO YIP award W911NF-13-1-0084, and AFOSR YIP award FA9550-15-1-0221.

References

Guillaume Alain and Yoshua Bengio. What regularized auto-encoders learn from the data generating distri-

bution. arXiv preprint arXiv:1211.4246, 2012.

A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. Tensor decompositions for learning latent

variable models. J. of Machine Learning Research, 15:2773–2832, 2014a.

Anima Anandkumar, Yi-kai Liu, Daniel J Hsu, Dean P Foster, and Sham M Kakade. A spectral algorithm
for latent dirichlet allocation. In Advances in Neural Information Processing Systems, pages 917–925,
2012.

Anima Anandkumar, Rong Ge, and Majid Janzamin. Sample Complexity Analysis for Learning Overcom-

plete Latent Variable Models through Tensor Methods. arXiv preprint arXiv:1408.0553, Aug. 2014b.

Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor decom-
positions for learning latent variable models. Journal of Machine Learning Research, 15:2773–2832,
2014c.

Animashree Anandkumar, Rong Ge, and Majid Janzamin. Guaranteed Non-Orthogonal Tensor Decompo-

sition via Alternating Rank-1 Updates. arXiv preprint arXiv:1402.5180, Feb. 2014d.

David Balduzzi and Muhammad Ghifari.

Strongly-typed recurrent neural networks.

arXiv preprint

arXiv:1602.02218, 2016.

Andrew R Barron. Approximation and estimation bounds for artiﬁcial neural networks. Machine Learning,

14:115–133, 1994.

Asa Ben-Hur and Douglas Brutlag. Sequence motifs: highly predictive features of protein function.

In

Feature Extraction, pages 625–645. Springer, 2006.

17

Danqi Chen and Christopher D Manning. A fast and accurate dependency parser using neural networks. In

EMNLP, pages 740–750, 2014.

Barbara Hammer. On the approximation capability of recurrent neural networks. Neurocomputing, 31(1):

107–123, 2000.

F. Huang, U.N. Niranjan, M. Hakeem, and A. Anandkumar. Online Tensor Methods for Learning Latent

Variable Models. Accepted to JMLR, 2014a.

Qingqing Huang, Rong Ge, Sham Kakade, and Munther Dahleh. Minimal realization problem for hidden
In Communication, Control, and Computing (Allerton), 2014 52nd Annual Allerton

markov models.
Conference on, pages 4–11. IEEE, 2014b.

Aapo Hyv¨arinen. Estimation of non-normalized statistical models by score matching. In Journal of Machine

Learning Research, pages 695–709, 2005.

M. Janzamin, H. Sedghi, and A. Anandkumar. Beating the Perils of Non-Convexity: Guaranteed Training

of Neural Networks using Tensor Methods. Preprint available on arXiv:1506.08473, June 2015.

Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Score Function Features for Discriminative Learn-

ing: Matrix and Tensor Frameworks. arXiv preprint arXiv:1412.2863, Dec. 2014.

Maya Kallas, Paul Honeine, C´edric Richard, Clovis Francis, and Hassan Amoud. Kernel-based autoregres-
sive modeling with a pre-image technique. In Statistical Signal Processing Workshop (SSP), 2011 IEEE,
pages 281–284. IEEE, 2011.

Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei.
In Proceedings of the IEEE con-

Large-scale video classiﬁcation with convolutional neural networks.
ference on Computer Vision and Pattern Recognition, pages 1725–1732, 2014.

George Konidaris and Finale Doshi-Velez. Hidden parameter markov decision processes: An emerging

paradigm for modeling families of related tasks. In 2014 AAAI Fall Symposium Series, 2014.

Aryeh Kontorovich and Roi Weiss. Uniform chernoff and dvoretzky-kiefer-wolfowitz-type inequalities for

markov chains and related processes. Journal of Applied Probability, 51(4):1100–1113, 2014.

Leonid Aryeh Kontorovich, Kavita Ramanan, et al. Concentration inequalities for dependent random vari-

ables via the martingale method. The Annals of Probability, 36(6):2126–2158, 2008.

Zachary C Lipton, John Berkowitz, and Charles Elkan. A critical review of recurrent neural networks for

sequence learning. arXiv preprint arXiv:1506.00019, 2015.

Christopher D Manning and Hinrich Sch¨utze. Foundations of statistical natural language processing, vol-

ume 999. MIT Press, 1999.

Havard Rue and Leonhard Held. Gaussian Markov random ﬁelds: theory and applications. CRC Press,

2005.

Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks. Signal Processing, IEEE

Transactions on, 45(11):2673–2681, 1997.

18

Hanie Sedghi and Anima Anandkumar. Provable methods for training neural networks with sparse connec-

tivity. NIPS workshop on Deep Learning and Representation Learning, Dec. 2014.

L. Song, A. Anandkumar, B. Dai, and B. Xie. Nonparametric estimation of multi-view latent variable

models. arXiv preprint arXiv:1311.3287, Nov. 2013.

Daniel A Spielman, Huan Wang, and John Wright. Exact recovery of sparsely-used dictionaries. In Confer-

ence on Learning Theory, 2012.

Bharath Sriperumbudur, Kenji Fukumizu, Revant Kumar, Arthur Gretton, and Aapo Hyv¨arinen. Density

estimation in inﬁnite dimensional exponential families. arXiv preprint arXiv:1312.3516, 2013.

Kevin Swersky, David Buchman, Nando D Freitas, Benjamin M Marlin, et al. On autoencoders and score
In Proceedings of the 28th International Conference on Machine

matching for energy based models.
Learning (ICML-11), pages 1201–1208, 2011.

Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational

Mathematics, 12(4):389–434, 2012.

Yining Wang, Hsiao-Yu Tung, Alexander Smola, and Animashree Anandkumar. Fast and guaranteed tensor

decomposition via sketching. In Proc. of NIPS, 2015.

Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural

networks? In Advances in Neural Information Processing Systems, pages 3320–3328, 2014.

A Spectral Decomposition Algorithm

As part of GLOREE, we need a spectral/tensor method to decompose the higher order derivative tensor
E[∇(m)
x[n]yi] to its rank-1 components. There exist different tensor decomposition frameworks, but the most
popular one is the CP decomposition. Note that tensor decomposition for various scenarios is extensively an-
alyzed in the literature (Song et al., 2013), (Anandkumar et al., 2014a),(Anandkumar et al., 2014b),(Anandkumar et al.,
2014d), (Janzamin et al., 2014), (Janzamin et al., 2015). We follow the method in (Janzamin et al., 2015).

The only difference between our tensor decomposition algorithm and (Janzamin et al., 2015) is that they
have a symmetric tensor. In GLOREE, we have two tensor decomposition procedure. The ﬁrst one is to
decompose the asymmetric tensor E[yi ⊗ S2(xi) to learn A2, A1 and the seocnd one is to decompose a sym-
metric tensor. In order to decompse the asymmetric tensor, we ﬁrst symmetrize the tensor we combine the
symmetrization and whitening into one Procedure 4. We use the rule presented in (Anandkumar et al., 2012)
to form the symmetrization tensor. After the unwhitening procedure (Procedure 5) we reverse the effect of
symmetrization. For a more compact represnetation, we use the same tensor decomposition algorithm for
both cases. Note that the symmetric case can be considered as a speciﬁc form of asymmetric case where the
symmetrization matrix is identity.

The second tensor decomposition we need to do is on the reshaped tensor

(cid:2)yt ⊗ Reshape(Sl2(x[n], t − 1), 1, [1 . . . l], . . . , [l2 − l + 1 . . . l2](cid:3) .

Note that this is a tensor of order l + 1, where the modes 2, . . . , l2 are similiar. In order to reduce computa-
tional complexity, we contract the tensor such that we get a symmetric-third order tensor.

19

Input: Tensor T =Pi∈[k] λiu⊗3

Whitening procedure (Procedure 4)

i

SVD-based Initialization (Procedure 7)

Tensor Power Method (Algorithm 6 )

Output: {ui}i∈[k]

Figure 2: Overview of tensor decomposition algorithm for third order tensor (without tensorization). (Janzamin et al.,
2015)

Algorithm 3 Tensor Decomposition Algorithm Setup
input Asymmetric tensor T with symmetrization matrix D or Symmetric tensor T (D is the identity matrix

in this case).

Calculate T =Whiten(T , D); see Procedure 4.

1: if Whitening then
2:
3: end if
4: for j = 1 to k do
5:
6: end for
7: (A1)j = Un-whiten(vj), j ∈ [k]; see Procedure 5.
8: A2 = D−1A1
9: return A2, A1, A1.

(vj, µj, T ) = tensor power decomposition(T ); see Algorithm 6.

The rest of the tensor decomposition procedure is the same as (Janzamin et al., 2015). For more details,

see (Janzamin et al., 2015).

The steps we take for tensor decomposition are shown in Figure 2 and the Algorithm 3 provides the

details.

B Proofs

B.1 Proof Theorem 5

Proof 1 By Theorem 2 we have that

E [yt ⊗ S2(xt)] = E(cid:2)∇2

xtyt(cid:3) .

20

Procedure 4 Whitening
input Tensor T ∈ Rd×d×d, symmetrization matrix D.
1: Second order moment M2 ∈ Rd×d is constructed such that it has the same decompositon form as target

tensor T :

• computed as M2 := T (T (θ, I, I)D−1), I, I) ∈ Rd×d, where θ ∼ N (0, Id) is a random standard

Gaussian vector. With this form whitening also symmetrized the tensor

2: Compute the rank-k SVD, M2 = U Diag(γ)U⊤, where U ∈ Rd×k and γ ∈ Rk.
3: Compute the whitening matrix W := U Diag(γ−1/2) ∈ Rd×k.
4: return T (W, W, W ) ∈ Rk×k×k.

Procedure 5 Un-whitening
input Orthogonal rank-1 components vj ∈ Rk, j ∈ [k].
1: Consider matrix M2 which was exploited for whitening in Procedure 4, and let ˜λj, j ∈ [k] denote the

corresponding coefﬁcients as M2 = A1 Diag(˜λ)A⊤1 .

2: Compute the rank-k SVD, M2 = U Diag(γ)U⊤, where U ∈ Rd×k and γ ∈ Rk.
3: Compute

(A1)j =

4: return {(A1)j}j∈[k].

1

q˜λj

U Diag(γ1/2)vj,

j ∈ [k].

In order to show the derivative form more easily, let us look at derivative of each entry i ∈ [dy] of the vector
yt.

(yt)i = h(A2)(i), (A1xt + U ht−1)∗2i =Xi∈dh

1 , xti + hU (i), ht−1i(cid:17)2

,

(A2)ji(cid:16)hA(i)
1 , xti + hU (j), ht−1i(cid:17)2
1 , xti + hU (j), ht−1i(cid:17) A(j)

1

∇2

xt(yt)i = ∇2

xtXj∈dh
(A2)ji(cid:16)hA(j)
(A2)ji∇xt(cid:16)hA(j)
= 2Xj∈dh
= 2Xj∈dh
xtyt = 2Xj∈dh

A(j)
2 ⊗ A(j)

(A2)jiA(j)

1 ⊗ A(j)
1 ,

1 ⊗ A(j)
1 ,

∇2

and hence the form follows. By Theorem 2 we have that

E [yt ⊗ S4(xt−1)] = Eh∇4

xt−1yti

21

Algorithm 6 Robust tensor power method (Anandkumar et al., 2014c)
input symmetric tensor ˜T ∈ Rd′×d′×d′, number of iterations N, number of initializations R.
output the estimated eigenvector/eigenvalue pair; the deﬂated tensor.
1: for τ = 1 to R do
2:
3:
4:

0 with SVD-based method in Procedure 7.

Compute power iteration update

Initializebv(τ )

for t = 1 to N do

t

:=

end for

t−1)
t−1)k

5:
6: end for

˜T (I,bv(τ )
k ˜T (I,bv(τ )
7: Let τ∗ := arg maxτ∈[R]{ ˜T (bv(τ )
8: Do N power iteration updates (20) starting frombv(τ ∗)
N to obtainbv, and set ˆµ := ˜T (bv,bv,bv).
9: return the estimated eigenvector/eigenvalue pair (bv, ˆµ); the deﬂated tensor ˜T − ˆµ · ˆv⊗3.

bv(τ )
N ,bv(τ )

t−1,bv(τ )
t−1,bv(τ )

N ,bv(τ )

N )}.

Procedure 7 SVD-based initialization (Anandkumar et al., 2014d)
input Tensor T ∈ Rd′×d′×d′.
1: for τ = 1 to log(1/ˆδ) do
2:

Draw a random standard Gaussian vector θ(τ ) ∼ N (0, Id′ ).
Compute u(τ )
1

as the top left singular vector of T (I, I, θ(τ )) ∈ Rd′×d′.

3:
4: end for

5: bv0 ← maxτ∈[log(1/ˆδ)](cid:16)u(τ )
1 (cid:17)min
6: return bv0.

.

In order to show the derivative form more easily, let us look at each entry i ∈ [dy] of the vector yt.

(20)

1 , xt−1i + hU (l), ht−2i(cid:17)2

(ht−1)k = hA(k)
(yt)i = Xk∈[dh]

Ukl(cid:16)hA(l)
1 , xti + Xl∈[dh]

1 , xti + Xl∈[dh]
(A2)kihA(k)
xt−1yt = 2Xj∈dh
2 ⊗ Xk∈dh

A(j)

T = ∇4

Ukl(cid:16)hA(l)

2

.

1 , xt−1i + hU (l), ht−2i(cid:17)2
1 ⊗ Xm∈dh

1 ⊗ A(m)

UjmA(m)

1

,

UjkA(k)

1 ⊗ A(k)

The form follows directly using the derivative rule as in Lemma 1.

Now when we reshape the above tensor T to Reshape(∇4
[U (A1 ⊙ A1)](j) ⊗ [U (A1 ⊙ A1)](j).

xt−1yt, [1 2], [3 4]) we have the formPj∈dh

A(j)

2 ⊗

22

B.2 Proof of Theorem 7

Hence,

yt = A⊤2 (cid:20) ht
zt. (cid:21)
xtzt. (cid:21)
xtyt = A⊤2 (cid:20) ∇2
ei ⊗ (B1)(i) ⊗ (B1)(i). #
= A⊤2 " Pi∈dh
Pi∈dh

ei ⊗ (A1)(i) ⊗ (A1)(i)

xtht

∇2

T = ∇2

The second Equation is direct result of Lemma 5. Therefore, if we decompose the above tensor, the ﬁrst
mode yields the matrix A2. Next we remove the effect of A2 by multiplying its inverse to the ﬁrst mode of
the moment tensor T . By the above Equations, we readily see that

T ((A2)−1, I, I) =" Pi∈dh
Pi∈dh

ei ⊗ (A1)(i) ⊗ (A1)(i)

ei ⊗ (B1)(i) ⊗ (B1)(i). #

This means T ((A2)−1, I, I) =Pi∈dh

recovers A2, A1, B1. Recovery of U, V directly follows Lemma 6.

ei ⊗ ci ⊗ ci, where ci =(cid:20) (A1)(i)

(B1)(i). (cid:21) Hence, Algorithm 2 correctly

B.3 Proof Lemma 8

In order to prove the result, we ﬁrst need to show that the matricized cross-moment tensor is a Lipschitz
function of the input sequence and ﬁnd the Lipschitz constant.

Without loss of generality, we assume that the input sequence is bounded by 1, kxik < 1 ∀i ∈ [n] (note
that this constant in bound of the input sequence is transferable to a bound on norm of A1) and kA1 +U k ≤ 1
(This constant is transferable to the bound on spectral norm of U. These two assumptions ensure that
∀t ∈ [n], khtk ≤ 1. Then, if the activation function is a polynomial of order l, we need kU k ≤ 1/l. We
have that

Lemma 10 (Lipschitz property for the Output of IO-RNN) For the IO-RNN discussed in (15), if the above
assumptions hold, then the output is a Lipschitz function of the input sequence with Lipschitz constant
n kA2kkA1k
1
1−lkUk

w.r.t. Hamming metric.

Proof 2 This follows directly from the deﬁnition. In order to ﬁnd the Lipschitz constant, we need to sum
over all possible changes in the input sequence. Therefore, we bound derivative of the function w.r.t. each
input entry and then take the average the results to provide an upper bound on the Lipschitz constant. With
the above assumptions, it is straightforward to show that

k∇xiytk ≤ lt−i+1kA2kkA1kkU kt−i.

Taking the average of this geometric series for t ∈ [n] and large sample sequence, we get 1

n kA2kkA1k
1−lkUk

as

the Lipschitz constant.

23

Next we want to ﬁnd the Lipschitz constant for the matricized tensor T = E[yi ⊗ S2(x[n], i)] which
is a function of the input sequence. Considering the rule for derivative of product of two functions as in
Lemma lem:prodrule we have that

∇xiyi ⊗ S2(x[n], i) = S2(x[n], i) ⊗ ∇xiyi + yi ⊗ ∇xiS2(x[n], i)

Note that by deﬁnition of score function in (9), we have that

∇xiS2(x[n], i) = S3(x[n], i) − S2(x[n], i) ⊗ S1(x[n], i).

Hence,

k∇xiyi ⊗ S2(x[n], i)k = kS2(x[n], i) ⊗ ∇xiyi + yi ⊗ ∇xiS2(x[n], i)k

≤ max
i∈[n]

S2(x[n], i)kA2k + k∇xiyik max
i∈[n]

{kS3(x[n], i) − S2(x[n], i) ⊗ S1(x[n], i)k}

{kS3(x[n], i) − S2(x[n], i) ⊗ S1(x[n], i)k} are bounded values, it

and assuming that max
i∈[n]

S2(x[n], i), max
i∈[n]

is straightforward to show that Mat (E[yi ⊗ S2(x[n], i)]) is Lipschitz with Lipschitz constant

c =

1
n

kA2kkA1k
1 − lkU k

kmax
i∈[n]

S2(x[n], i)k + kA2kkmax
i∈[n]

∇xiS2(x[n], i)k

B.4 Proof of Theorem 9

In order to get the complete concentration bound in Theorem 9. We need Lemma 8 in addition to the
following Theorem.

Theorem 11 (Concentration bound for a matrix-valued function of a Markov chain] Consider a Markov
chain with observation samples x[n] = (x1, . . . , xn) ∈ Sn, geometric ergodicity G, contraction coefﬁcient
θ and an arbitrary initial distribution. For any c-Lipschitz matrix-valued function Φ(·) : Sn → Rd1×d2, we
have

kΦ − E[Φ]k ≤ G

1 + 1√8cn1.5

1 − θ s8c2n log(cid:18) d1 + d2
δ (cid:19),

with probability at least 1 − δ, where E(Φ) is expectation over samples of Markov chain when the initial
distribution is the stationary distribution.

The proof follows result of (Kontorovich et al., 2008), (Konidaris and Doshi-Velez, 2014) and the anal-

ysis of (Tropp, 2012) for sum of random matrices.

C Discussion

C.1 Score Function Estimation

According to (Janzamin et al., 2014), there are various efﬁcient methods for estimating the score function.
The framework of score matching is popular for parameter estimation in probabilistic models (Hyv¨arinen,

24

2005; Swersky et al., 2011), where the criterion is to ﬁt parameters based on matching the data score func-
tion. Swersky et al. (2011) analyze the score matching for latent energy-based models. In deep learning, the
framework of auto-encoders attempts to ﬁnd encoding and decoding functions which minimize the recon-
struction error under added noise; the so-called Denoising Auto-Encoders (DAE). This is an unsupervised
framework involving only unlabeled samples. Alain and Bengio (2012) argue that the DAE approximately
learns the ﬁrst order score function of the input, as the noise variance goes to zero. Sriperumbudur et al.
(2013) propose non-parametric score matching methods that provides the non-parametric score function
form for inﬁnite dimensional exponential families with guaranteed convergence rates. Therefore, we can
use any of these methods for estimating S1(x[n], [n]) and use the recursive form (Janzamin et al., 2014).

Sm(x[n], [n]) = −Sm−1(x[n], [n]) ⊗ ∇x[n] log p(x[n]) − ∇x[n]Sm−1(x[n], [n])

to estimate higher order score functions.

C.2 Training IO-RNN and BRNN with scalar output

In the main text, we discussed training IO-RNNs and BRNNs with vector outputs. Here we expand the
results to training IO-RNNs and BRNNs with scalar outputs. Note that in order to recover the parameters
uniquely, we need the cross-moment to be a tensor of order at least 3. This is due to the fact that in general
matrix decomposition does not provide unique decomposition for non-orthogonal components. In order to
obtain a cross-moment tensor of order at least 3, since the output is scalar, we needs its derivative tensors
of order at least 3. In order to have a non-vanishing gradient, the activation function to be a polynomial of
order l ≥ 3.

Hence, our method can also be used for training IO-RNN and BRNN with scalar output if the activation

function is a polynomial of order l ≥ 3, i.e., Let yt be the output of

E[yt|ht] = ha2, hti,

ht = polyl(A1xt + U ht−1),

where xt ∈ Rdx, ht ∈ Rdh, yt ∈ R and hence A1 ∈ Rdh×dx , U ∈ Rdh×dh, a2 ∈ Rdh. We can learn the
parameters of the model using GLOREE with guarantees.

Then we have

Lemma 12 (Learning parameters of RNN for general activation function, scalar output)

E [yt ⊗ S3(xt)] = Xi∈dh
µi = E(cid:20)(cid:16)hA(i)

µia2iA(i)

1 ⊗ A(i)

1 ⊗ A(i)
1 ,

1 , xti + hU i, ht−1i(cid:17)∗(l−3)(cid:21)

Then we have

In order to learn U we form the tensor ˆE(cid:2)yt ⊗ Reshape(Sl2(xt−1), 1, [1 . . . l], . . . , [l2 − l + 1 . . . l2](cid:3).
E(cid:2)yt ⊗ Reshape(Sl2(xt−1), 1, [1 . . . l], . . . , [l2 − l + 1 . . . l2](cid:3) =Xi∈dh

(a2)i ⊗(cid:20)hU (A⊙l

1 )i(i)(cid:21)⊗l

.

Hence, since we know A1, we can recover U via tensor decomposition.

We can prove that we can learn the parameters of a BRNN with scalar output and polynomial activation

functions of order l ≥ 3 using the same trend as for Lemma 7.

25

C.3 Training Linear IO-RNN

Sedghi and Anandkumar (2014) provide a method to train ﬁrst layer of neural networks using the ﬁrst-
order score function of the input. For a NN with vector output, their formed cross-moment is a matrix of
the form E[y ⊗ S1(x)] = BA1, where A1 is the weight matrix for the ﬁrst layer and B is a matrix that
includes the rest of the derivative matrix. Then they argie that if A1 is sparse, the problem of recovering
A1 is a sparse dictionary learning problem that can be solved efﬁciently using Sparse Dictionary Learning
Algorithm (Spielman et al., 2012).

Here we show that for IO-RNN with linear activation function, we can expand the result of (Sedghi and Anandkumar,

2014) to the non-i.i.d. input sequence. Let

yt = A⊤2 ht,

ht = A1xt + U ht−1,

where xt ∈ Rdx, ht ∈ Rdh, yt ∈ Rdy , A⊤2 ∈ Rdh×dy and hence A1 ∈ Rdy×dx , U ∈ Rdy×dh.

Let ˜y[n] = [y1, y2, . . . , yn], ˜x[n] = [x1, x2, . . . , xn]. Similar to our earlier analysis, we have

E[˜y[n] ⊗ S(˜x[n], [n]) = ∇ ˜x[n] ˜y[n]

For our linear model the derivative has a Toeplitz form. Assuming that A1 is sparse, we can use this structure
and Sparse Dictionary Learning Algorithm (Spielman et al., 2012) to recover the model parameters.

Let ˜y[n] = [y1, y2, . . . , yn], ˜x[n] = [x1, x2, . . . , xn]. Similar to our earlier analysis, we have

E[˜y[n] ⊗ S(˜x[n], [n]) = ∇ ˜x[n] ˜y[n]

For our linear model the derivative has a Toeplitz form. Assuming that A1 is sparse, we can use this structure
and Sparse Dictionary Learning Algorithm (Spielman et al., 2012) to recover the model parameters.

Below we write the cross-moment Topelitz form for n = 4 for simplicity.

A⊤2 A1
A⊤2 U A1
A⊤2 A1
A⊤2 U 2A1 A⊤2 U A1
A⊤2 U 3A1 A⊤2 U 2A1 A⊤2 U A1 A⊤2 A1

A⊤2 A1

0
0
0

0

0
0



If we recover the Toeplitz structure, we have access to the following matrices: A2A1, A2U A1, . . . , A2U nA1.
Next, we put these matrices in a new matrix C as below.

E[˜y[n] ⊗ S(˜x[n], [n]) =
C =
B =




A2A1
A2U A1

...

A2U nA1

A2A1
A2U A1

...

A2U nA1

26

It is easy to see that C = BA1 for matrix B as

Procedure 8 GLOREE-B (Guaranteed Learning Of Recurrent nEural nEtworks-Bidirection case) for gen-
eral activation function
input Labeled samples {(xi, yi) : i ∈ [n]}, polynomial order lh for activation function in the forward

direction, polynomial order lz for activation function in the backward direction.

input 2nd-order score function S2(x[n], [n]) of the input x; see Equation (7) for the deﬁnition.

5: {( ˆw, ˆR1, ˆR2, ˆR3} = tensor decomposition( ˜T );
6: ˆC = ( ˆR2 + ˆR3)/2.

1: ComputebT := ˆE[yi⊗S2(x[n], i)], bD := ˆE[yi⊗S1(x[n], i)]. bD will be used for whitening (Procedure 4).
2: {( ˆw, ˆR1, ˆR2, ˆR3} = tensor decomposition(bT ); see Algorithm 3 in the Appendix.
4: Compute ˜T = bT ((( ˆA2)⊤)−1, I, I). For deﬁnition of multilinear form see Section 2.1.
7: ˆC =(cid:20) ˆA1
ˆB1 (cid:21).
9: Compute bT = ˆE(cid:2)yt ⊗ Reshape(Sl2(x[n], t − 1), 1, [1 . . . l], . . . , [l2 − l + 1 . . . l2](cid:3).
10: Contract the moment-tensor to a 3-rd order tensor as bT2 = T (I, I, I, θ, . . . , θ) with some random vector
11: { ˆw, ˆR1, ˆR2, ˆR3} = tensor decomposition(bT2); see Algorithm 3 in the Appendix.
12: ˜R =(cid:16) ˆR2 + ˆR3(cid:17) /2.
13: ˆU = ˜Rh ˆA1 ⊙ ˆA1i−1

8: Compute l2th-order score function Sl2(x[n], t − 1) of the input sequence as in Equation (10).

14: Repeat lines (8)-(13) with S4(x[n], t + 1) instead of S4(x[n], t − 1) to recover ˆV .
15: return ˆA1, ˆA2, ˆB1, ˆU , ˆV .

, Khatri-Rao product ⊙ is deﬁned in Deﬁnition 1.

3: ˆA2 = ˆR1.

θ.

Now assuming that A1 is sparse and B is full column-rank, we can recover A1 using Sparse Dictionary
Learning Algorithm (Spielman et al., 2012). Let U = V ΛV ⊤ be the singular-value decomposition of
It is easy to show that, to ensure that B is full column-rank, we need the
U, where Λ = Diag(λi).
singular-values of U to satisfy λi ∼ 1√dh
and
U = A−1

. Once we recover A1, we can recover A2 = A2A1A−1
1

2 A2U A1A−1
1 .

D BRNN Algorithm for general polynomial activation function

In Algorithm 8, we show the complete algorithm for training BRNN when the activation functions are
polynomials of order l.

27

