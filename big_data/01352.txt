6
1
0
2

 
r
a

M
4

 

 
 
]

C
D
.
s
c
[
 
 

1
v
2
5
3
1
0

.

3
0
6
1
:
v
i
X
r
a

Cloud Server Benchmarks for Performance
Evaluation of New Hardware Architecture

1

Hao Wu, Fangfei Liu, and Ruby B. Lee

Princeton University, Princeton, NJ, 08544 USA
E-mail: haow.princeton@gmail.com, {fangfeil, rblee}@princeton.edu

Abstract—Adding new hardware features to a cloud computing server requires testing both the functionalities and the performance of
the new hardware mechanisms. However, commonly used cloud computing server workloads are not well-represented by the SPEC
integer and ﬂoating-point benchmark and Parsec suites typically used by the computer architecture community. Existing cloud
benchmark suites for scale-out or scale-up computing are not representative of the most common cloud usage, and are very difﬁcult to
run on a cycle-accurate simulator that can accurately model new hardware, like gem5. In this paper, we present PALMScloud, a suite of
cloud computing benchmarks for performance evaluation of cloud servers, that is ready to run on the gem5 cycle-accurate simulator.
We demonstrate how our cloud computing benchmarks are used in evaluating the cache performance of a new secure cache called
Newcache as a case study. We hope that these cloud benchmarks, ready to run on a dual-machine gem5 simulator or on real
machines, can be useful to other researchers interested in improving hardware micro-architecture and cloud server performance.

Index Terms—Cloud Computing, Benchmarks Performance Evaluation, Gem5, Dual System, Simulation.

!

1 INTRODUCTION

S Imulation of new hardware or new architecture is a

necessary stage in computer hardware design, to test
both its functionalities and its performance. Ideally, we
would like to evaluate the functionalities and performance
on a detailed simulation platform at the design stage of the
new systems before committing to expensive chip fabrica-
tion and prototype systems. There are open-source cycle-
accurate simulation platforms that have been worked on for
a long time (e.g., gem5 [19], PTLsim [30], MARSSx86 [28],
etc), that simulate hardware at a very detailed level, and
provide good performance data. They can also be leveraged
to add new hardware features into the detailed hardware
models.

However, demonstrating performance of different hard-
ware conﬁgurations requires representative benchmarks for
the performance evaluation of today’s computing envi-
ronments. Frequently used benchmarks for performance
may not be representative of current or future computing
paradigms. Currently, the computer architecture community
uses SPEC Integer and Floating-point Benchmarks [17] for
general-purpose computing and some PARSEC benchmarks
[11] for parallel computing workloads. These are mostly
compute-intensive benchmarks and do not represent to-
day’s Cloud Computing scenarios, e.g., public Cloud Com-
puting infrastructures like Amazon EC2 [1]. The purpose of
Cloud Computing is to provide different IT resources (e.g.
computing resources, storage resources, software develop-
ment, system testing, etc. ) as services on demand to cus-
tomers. So the most critical attribute of Cloud Computing is
resource virtualization.

• The authors are with the Princeton Architecture Laboratory for
of Electrical

Multimedia
(PALMS), Department
Engineering, Princeton University, Princeton, NJ 08544

and Security

For example a company can buy real hardware resources
(e.g. servers, etc.), or instead, buy Virtual Machines (VMs)
from cloud infrastructure providers. The company may
deploy web servers and application servers in several of
these VMs, and deploy data servers (e.g. MySQL) in other
VMs. In this paper, we did a study of the most common
cloud computing services and workloads, and selected a
suite of representative cloud computing benchmarks that
can be run on a detailed hardware simulation platform.
With this paper, we will also make our Cloud benchmark
suite, compiled to run on the gem5 simulator, available to
the computer architecture research community.

To run these Cloud benchmarks under gem5, we had to
get the x86 dual-computer simulation to work, to simulate
both the server side and the client side of these benchmarks.
Then to demonstrate a concrete performance evaluation
example, we use Newcache as a case study. Newcache is
a secure cache that uses dynamic randomized memory-
to-cache mapping to prevent software cache side-channel
attacks [26], [29]. By modifying the cache sub-system under
gem5, we compared performance of different cache conﬁg-
urations. The contributions of this paper are:

• A general-purpose and representative suite of cloud

computing benchmarks, ready to run on gem5.

• Open availability of the security simulation frame-
work and cloud computing benchmarks, for com-
puter architecture researchers.

Section 2 discusses representative cloud computing
server benchmarks, and how they can be parameterized.
Section 3 describes how we set up the x86 dual system in
gem5. Section 4 describes our case study of a secure cache
design, Newcache, and gives sample performance testing
results on Newcache used in place of conventional set-
associative (SA) caches. Section 5 concludes the paper.

2 CLOUD COMPUTING SERVER BENCHMARKS

Cloud computing services can be categorized into several
fundamental models [22]: Infrastructure as a service (IaaS),
Platform as a service (PaaS), Software as a service (SaaS),
etc. Rackspace [14], the leader in hybrid cloud and founder
of OpenStack, put together a Top-10 list [20] of the most
common cloud computing use cases in 2012: File Storage
and Sharing, Cloud Database, Email, PaaS for Web Appli-
cations, Web Site Hosting, etc. Some obsolete benchmarks
(e.g., SPECweb [16], SPECmail [16], etc.) and currently
in-use benchmarks (e.g., SPECjbb [16], TPC-C [21], TPC-
W [21], etc.) only cover some of these. Also, since they
target speciﬁc commercial purposes and include too many
features, they run way too slow under a cycle-accurate
simulator like gem5. CloudSuite [6] is a recent benchmark
suite for scale-out workloads which covers many of the Top-
10 list. However, strictly speaking, most of them are big data
applications (e.g. Hadoop Mapreduce for Data Analytics,
Memcached for Data Caching, Cassandra NoSQL for Data
Serving, etc. ), and they are not designed to work under a
Cloud environment (virtual machines), so they are not really
cloud server benchmarks. Virt-LM [24] on the other hand
is a suite of benchmarks used to evaluate the performance
of live VM migration strategies among different software
and hardware environments in a data center scenario, and
Virt-LM chooses 5 popular cloud computing workloads,
comprising 5 categories from Rackspace’s Top-10 list.

Based on Virt-LM and Rackspace’s Top-10 list, we care-
fully selected an initial set of 6 representative cloud comput-
ing workloads, including the workloads for a Web Server,
Database Server, Mail Server, File Server, and Application
Server, and also Streaming Server, which is not in Virt-
LM or Rackspace’s list, that are suitable to run on gem5.
Below, we describe possible programs for each server cat-
egory, the programs we selected , and the parameters we
use for each program. We also select a popular general
purpose compute-intensive benchmark and an Idle Server
benchmark for comparison. Table 1 summarizes our server
benchmarks and the client-side driving tools.

TABLE 1: Cloud Server Benchmarks in PALMScloud Suite

Web Server
Database Server
Mail Server
File Server
Streaming Server
Application Server
Compute Server
Idle Server

Server-side Benchmarks
Apache httpd
MySQL
Postﬁx
Samba smbd
ffserver
Tomcat
libsvm
-

Client-side Driving Tool
Apache ab
SysBench
Postal
DBench
openRTSP
Apache ab
a1a.t
-

Web Server and Client: Apache HTTP server (httpd)
[3] has been the most popular web server on the Internet
since 1996. The project aims to develop and maintain an
open-source HTTP server for modern operating systems
including UNIX and Windows NT. For the client side, we
choose Apache Benchmark Tool (ab) [2], which is a single-
threaded command line benchmarking tool well suited for
a non-GUI testing environment under gem5. This bench-
mark allows picking the number of total requests and the
number of concurrent requests. For example, to send 1000
HTTP requests to our Apache server with a concurrency

2

of 10 requests at the same time, we type ab -n 1000 -c 10
http://10.0.0.1:8080/.

Database Server and Client: MySQL [9] is a well-
known open-source relational Database Management Sys-
tem (DBMS). To start MySQL server, we need to ﬁrst create
a user and group for the main program mysqld to run. Then
we run script mysql install db to set up initial grant tables,
and ﬁnally we start the mysqld service. For the client side,
we choose SysBench [18]. It is a modular, cross-platform
and multi-threaded benchmark tool for evaluating OS pa-
rameters that are important for a system running a database
under intensive load. In SysBench, we use the OLTP test
mode, which benchmarks a real database’s performance.
In the preparation phase, we create test tables with 100
records, while for the running phase, we do 200 advanced
transactions.

Mail Server and Client: We mainly focus on Simple
Mail Transfer Protocol (SMTP) under gem5, and we choose
postﬁx [13] to act as the SMTP server. We set the server side
10.0.0.1 to link with domain: domain1.com. Postﬁx is on the
server side, and is responsible for delivering all the emails.
The receivers of email are local users in domain1.com.
During the server initialization phase, we add 20 local users:
user0 ∼ user19 for our testing purposes.

For the client, we use postal [12] to send messages
to the server. Postal aims at benchmarking mail server
performance. It shows how fast the system can process
incoming email. We use command: postal -t X -m Y -c Z
-f send-list 10.0.0.1 rcpt-list to stress the server, where X
is the number of threads attempting separate connections,
Y is the maximum-message-size (in KB), and Z indicates
messages-per-connection. Also, send-list contains sender
email addresses, and rcpt-list contains receiver email ad-
dresses (user0 ∼ user19@domain1.com). In our experiment,
we ﬁx X=1, Y=1 and Z=3, and the stressing lasts 10 seconds
(the time is controlled by linux terminal command wait()).
File Server and Client: SMB provides ﬁle sharing and
printing services to Windows clients as well as Linux clients.
We use the SMB protocol and choose Samba smbd [15] as
the ﬁle server in our test. On the client side, we choose
Dbench [7] as the workload generator. It can generate
different I/O workloads to stress either a ﬁle system or a
networked server. We can ﬁrst choose dbench’s stressing
backend (smb, nfs, iscsi, socketio, etc.) by specifying the -
B option. Since the server is Samba smbd, we choose the
backend to be smb. Then we need to specify the shared
ﬁle server folder and the user-password pair through the
–smb-share option and the –smb-user option. The shared
folder and the user-password pair are already set up by the
smbd server. However in our experiments, we don’t use a
user-password pair. Moreover, dbench has a key concept
of a “loadﬁle”, which is a sequence of operations to be
performed on the ﬁle server’s shared folder. The operations
could be “Open ﬁle 1.txt”, “Read XX bytes from offset XX
in ﬁle 2.txt”, “Close the ﬁle”, etc. In our experiments, we
generate two different “loadﬁles”, one is a write-intensive
load (smb-writeﬁles.txt), another is a read-intensive load
(smb-readﬁles.txt). Finally, we can add a number n at the
end of the dbench command to specify the total clients
simultaneouly performing the load.

shown in section 4, we

type

In the

results

./dbench -B smb –smb-share=//10.0.0.1/share –smb-user=% –
loadﬁle=smb-writeﬁles.txt –run-once –skip-cleanup 3 to generate
smbd.write, which means launching 3 clients (simulated as
processes), and each client opens and writes ﬁve 64kB ﬁles
(the sequence of operations are speciﬁed in loadﬁle=smb-
writeﬁles.txt). By replacing loadﬁle with smb-readﬁles.txt,
we generate smbd.read, which launches 3 clients and each
client opens and reads ﬁve 64kB ﬁles.

Streaming Server and Client: We use ffserver [8]
as our streaming server. It is a streaming server for both
audio and video, which can stream mp3, mpg, wav, etc.
ffserver is part of the ffmpeg package. It is small and robust.
Before starting the server, we need to register server side
media-ﬁles at ffserver.conf ﬁle. On the streaming client side,
we choose openRTSP [10]. RTSP protocol can basically
control the streaming. Clients issue VCR-like commands,
such as play and pause, to facilitate real-time control of
playback of media ﬁles from the server. In the experi-
ment, ffserver registered several mp3 audio ﬁles on the
server side, and it uses port 7654 for streaming. On the
client side, we use command: ./openRTSP -r -p [port-number]
rtsp://10.0.0.1:7654/*.mp3 to generate a streaming request,
where ‘-r’ indicates playing the RTP streams without receiv-
ing them, and ‘-p’ option indicates the local port to stream
the ﬁle. In section 4, ffserver.sX means using openRTSP
to send X different remote client connection requests to
the ffserver for mp3 ﬁles streaming. We generate 3 differ-
ent streaming workloads (X=1, X=3 and X=30) to test the
streaming server.

Application Server and Client: For web applications,
the application server components’ main job is to support
the construction of dynamic pages [5]. Application servers
differ from web servers by dynamically generating html
pages each time a request is received, while most http
servers just fetch static web pages. Application servers can
utilize server-side scripting languages (PHP, ASP, JSP, etc.)
and Servlets to generate dynamic content. We use Tomcat
[4] on the server side. It is a small, robust application server
that also provides us with a lot of useful small jsp and
servlet examples. For the testing, we use Apache ab [2]
to send HTTP requests to Tomcat (Apache ab is described
previously as a web server client). We use command: ab -
n Y -c Z http://10.0.0.1:8080/(X URLs), where X represents
X different URLs, Y is the number of requests to perform
for each URL, and Z is the requesting concurrency for each
URL. These different URLs contain different jsp and servlet
examples provided by Tomcat. In our experiments, we ﬁx
Y=10 and Z=2, and choose X to be 1, 3 and 11, respectively,
for three different workloads, ranging from light to heavy
work.

Computation Machine Learning Server: We also want
to use some computing benchmarks to represent compute
servers. We choose LIBSVM [23] to do Support Vector
Machine (SVM) classiﬁcation. On the server side, we have
already trained an SVM model using data set a1a’s training
data from UCI’s machine learning repository [25]. a1a is
called Aduilt Data Set, which can be used to predict whether
a person’s income exceeds $50K/yr based on his census
data. We use this model to do SVM classiﬁcation on a1a’s
testing data: a1a.t. The testing data can either be placed on
the server side in advance, or can be transfered from the

3

client side (using netcat, php ﬁle upload, ﬁle server, ssh,
etc.) during runtime.

Idle Server and Client: We run a clean server without

any client-side stressing as our baseline benchmark.

3 SIMULATING CLIENT-SERVER DUAL SYSTEM IN
GEM5
The original gem5 simulator had a python script for con-
ﬁguring the dual system for ALPHA, ARM and MIPS,
but didn’t provide a python script for setting up the dual
system for x86, due to the complexity of the x86 system. The
python script in gem5 is used to deﬁne the connection of
the system and will be used to automatically generate C++
code. Basically, to set up a dual system, we need to add an
Ethernet interface to each system and connect them using
an Ethernet link. The Ethernet interface is a PCI device and
will be connected to the Intel chip-set’s North-Bridge chip.
The conﬁguration not only speciﬁes how the Ethernet device
is connected to the system, but also the proper assignment
of the interrupt number which is deﬁned in the MP Table
(Multiprocessor Conﬁguration Table), as well as the address
range of the MMIO (Memory-Mapped I/O). In addition to
the python script, we need to patch the gem5 source code
so that the cpuid can support long address mode and the
access to the Ethernet device is uncacheable.

In Figure 1, after these x86 dual-system conﬁguration
settings under gem5, we can simulate both server side (Test
system) and client side (Drive system), and they are able
to communicate via the Ethernet link. The server has an IP
address 10.0.0.1, and the client has an IP address 10.0.0.2.
By taking advantage of the dual system, we can adjust
the driving workload from the client side and measure the
performance on the server side, instead of using a server-
side ﬁxed input script. Also, the dual system simulates a
real two-node network environment. The server applica-
tions provide services and monitor input requests on their
speciﬁed TCP ports, while the client programs request for
services (HTTP requests, streaming requests, etc.) through
these server-side ports.

Fig. 1: New x86 Dual System in gem5

Both server side and client side use the basic Linux
system installed on gem5’s image disk. The server has dif-
ferent server-side applications installed. When booting the
dual system, both server and client conﬁgure their network
interface. The server then initiates the server-side services
and sends a ready signal to the client, so that the client
can drive the server side with requests. Some Server-side
benchmarks actually require some time to start up before
their services are available to the client under gem5. We use
a daemon program to test periodically if the service ports
are opened up by the benchmark. Only at that time will the
server send a ’ready’ signal to the client.

4

(a)

(b)

Fig. 2: Newcache as L1 D-cache: (a) Data Cache Miss Rate, and (b) Instructions Per Cycle (IPC) for Different Associativity
and Newcache extra index bits, k. Here, the cache size is ﬁxed at 32kB

4 EXAMPLE: SECURE CACHES
Recently, much interest in the security world focused on
using ”Moving Target Defense” (MTD) to thwart the at-
tacker, by having the system appear to change continuously.
We notice with interest that Newcache [29], a proposal for
a secure cache that thwarts cache side-channel attacks, is
an example of using MTD in hardware design. However,
the performance of Newcache was only tested with SPEC
benchmarks [29]. Hence, we use Newcache as a concrete
example for performance evaluation with current Cloud
Computing server benchmarks (section 2). You can refer
to [26], [27] to see both security testing and performance
testing of Newcache. We model Newcache in a modular
way, so that it can replace any of the caches in the cache
hierarchy in gem5.

TABLE 2: Gem5 Base CPU and Cache Conﬁgurations

Parameter
L1 data cache (private) associativity, and size
L1 instruction cache (private) associativity, and size
L2 cache (private) associativity, and size
L3 cache (shared) associativity, and size
Cache line size
L1 hit latency
L2 hit latency
L3 hit latency
Memory size, and latency

Value
8-way SA, 32 KB
4-way SA, 32 KB
8-way, 256 kB
16-way, 2MB
64 B
4 cycles
10 cycles
35 cycles
2 GB, 200 cycles

Newcache as L1 Data Cache: We replace the L1 data
cache with Newcache, and compare the performance results
against a baseline x86 system with Set-Associative (SA)
caches, with parameters shown in Table 2, like those of the
Intel core i7. We run the benchmark services on the server
side and driving tools on the client side. The results for
Instructions Per Cycle (IPC) and Data Cache Miss Rate for
all the benchmarks are shown in Figure 2. In the ﬁgure,
k is the number of extra index bits for Newcache to have
better security and performance. We also study many other
different conﬁguration parameter values. For our case study
of Newcache, we found that it does not degrade overall IPC
performance when used as a D-cache, for our cloud server
benchmarks, when compared to conventional SA caches.

5 CONCLUSIONS
We have deﬁned a new Cloud benchmark suite that rep-
resents the common workloads of today’s public cloud
computing usage, and we described how to run such client-
server benchmarks in x86 dual-system mode on gem5. We
hope that this paper and the Cloud benchmark suite under
the enhanced gem5 simulator can expedite cloud perfor-
mance evaluation for other researchers when new hardware
features are simulated on gem5. Since security testing can
also be done on gem5 [26], [27], we also hope to enable better
security-performance tradeoff studies, and thereby speed up
the design of better hardware-software security mechanisms
and architectures.

REFERENCES

[1]
[2]

[3]
[4]
[5]

“Amazon EC2,” http://aws.amazon.com/ec2/.
“Apache HTTP Server Benchmarking Tool,” http://httpd.apache.
org/docs/2.2/programs/ab.html.
“Apache HTTP Server Project,” http://httpd.apache.org/.
“Apache Tomcat,” http://tomcat.apache.org/.
“Application Server Deﬁnition,” http://en.wikipedia.org/wiki/
Application server.
“CloudSuite,” http://parsa.epﬂ.ch/cloudsuite/cloudsuite.html.
“Dbench Workloads Generator,” http://dbench.samba.org/.
“ffserver,” http://www.ffmpeg.org/ffserver.html.
“MySQL,” http://www.mysql.com/.

[6]
[7]
[8]
[9]
[10] “openRTSP,” http://www.live555.com/openRTSP/.
[11] “PARSEC,” http://parsec.cs.princeton.edu/.
[12] “Postal,” http://doc.coker.com.au/projects/postal/.
[13] “Postﬁx,” http://www.postﬁx.org/.
[14] “Rackspace,” http://www.rackspace.com/about/.
[15] “SMBD File Server,” http://www.samba.org/.
[16] “SPEC Benchmarks,” http://www.spec.org/.
[17] “SPEC CPU 2006,” http://www.spec.org/cpu2006/.
[18] “SysBench,” http://sysbench.sourceforge.net/.
[19] “The gem5 Simulator System,” http://www.gem5.org.
[20] “Top
the

Common

10

Uses

Cloud
http://www.rackspace.com/blog/

for

for
top-10-common-uses-for-the-cloud-for-2012/.

2012,”

[21] “Transactions Processing Council,” http://www.tpc.org/.
[22] R. Buyya, J. Broberg, and A. Goscinski, Cloud Computing: Principles

and Paradigms. Wiley Press, 2011.

[23] C.-C. Chang and C.-J. Lin, “LIBSVM: A library for support vector

machines,” ACM TIST, vol. 2, 2011.

[24] D. Huang, D. Ye, Q. He, J. Chen, and K. Ye, “Virt-lm: A benchmark
for live migration of virtual machine,” in ICPE, 2011, pp. 307–316.

0.080.10.12Miss RateSA‐assoc2SA‐assoc4SA‐assoc8New(k=3)New(k=4)New(k=5)New(k=6)00.020.040.06Data Cache M0.50.60.70.8CSA‐assoc2SA‐assoc4SA‐assoc8New(k=3)New(k=4)New(k=5)New(k=6)0.10.20.30.4IPC[25] M. Lichman, “UCI machine learning repository,” 2013. [Online].

Available: http://archive.ics.uci.edu/ml

[26] F. Liu and R. B. Lee, “Security Testing of a Secure Cache Design,”

in HASP, 2013.

[27] F. Liu, H. Wu, and R. B. Lee, “Can randomized mapping secure

instruction caches from side-channel attacks?” in HASP, 2015.

[28] A. Patel, F. Afram, S. Chen, and K. Ghose, “Marss: A full system

simulator for multicore x86 cpus,” in DAC, 2011, pp. 1050–1055.

[29] Z. Wang and R. B. Lee, “A Novel Cache Architecture with En-

hanced Performance and Security,” in MICRO, 2008, pp. 83–93.

[30] M. Yourst, “PTLsim: A Cycle Accurate Full System x86-64 Microar-

chitectural Simulator,” in IEEE ISPASS, 2007, pp. 23–34.

5

