Improved Bounds on the Epidemic Threshold of Exact SIS Models on

Complex Networks

Navid Azizan Ruhi, Christos Thrampoulidis and Babak Hassibi

6
1
0
2

 
r
a

 

M
6
1

 
 
]
I
S
.
s
c
[
 
 

1
v
5
9
0
5
0

.

3
0
6
1
:
v
i
X
r
a

Abstract— The SIS (susceptible-infected-susceptible)

epi-
demic model on an arbitrary network, without making approx-
imations, is a 2n-state Markov chain with a unique absorbing
state (the all-healthy state). This makes analysis of the SIS
model and, in particular, determining the threshold of epidemic
spread quite challenging. It has been shown that the exact
marginal probabilities of infection can be upper bounded by
an n-dimensional linear time-invariant system, a consequence
of which is that the Markov chain is “fast-mixing” when the
LTI system is stable, i.e. when β
λmax(A) (where β is the
infection rate per link, δ is the recovery rate, and λmax(A)
is the largest eigenvalue of the network’s adjacency matrix).
This well-known threshold has been recently shown not to
be tight in several cases, such as in a star network. In this
paper, We provide tighter upper bounds on the exact marginal
probabilities of infection, by also taking pairwise infection
probabilities into account. Based on this improved bound, we
derive tighter eigenvalue conditions that guarantee fast mixing
(i.e., logarithmic mixing time) of the chain. We demonstrate the
improvement of the threshold condition by comparing the new
bound with the known one on various networks and epidemic
parameters.

δ <

1

I. INTRODUCTION

The mathematical modeling and analysis of epidemic
spread is of great importance in understating dynamical pro-
cesses over complex networks (e.g. social networks) and has
attracted signiﬁcant interest from different communities in
recent years. The study of epidemics plays a key role in many
areas beyond epidemiology [1], such as viral marketing [2],
[3], network security [4], [5], and information propagation
[6], [7]. Although there is a huge body of work on epidemic
models, classical ones mostly neglect the underlying network
structure and assume a uniformly mixed population, which
is obviously far from reality. However, in recent years more
realistic networked models have been introduced, and many
interesting results are now known [8], [9].

In the simplest case (the binary-state or SIS model)
each node is in one of two different states: susceptible (S)
or infected (I). During any time interval, each susceptible
(healthy) node has a chance of being independently infected
by any of its infected neighbors (with probability β). Further,
during any time interval, each infected node has a chance
of recovering (with probability δ) and becoming susceptible

This work was supported in part by the National Science Foundation un-
der grants CNS-0932428, CCF-1018927, CCF-1423663 and CCF-1409204,
by a grant from Qualcomm Inc., by NASAs Jet Propulsion Laboratory
through the President and Directors Fund, by King Abdulaziz University,
and by King Abdullah University of Science and Technology.

The

authors

are with the California

Pasadena,
hassibi}@caltech.edu

91125,

CA

USA

Institute of Technology,
{azizan, cthrampo,

again. For a network with n nodes, this yields a Markov
chain with 2n states, which is referred to as the exact
or “stochastic” model. Since analyzing this model is quite
challenging, most researchers have resorted to n-dimensional
linear and nonlinear approximations (the most common being
the mean-ﬁeld approximation), which are sometimes called
“deterministic” models. This paper focuses on improving
known bounds on the exact model.

The spreading process can be considered either as a
discrete-time Markov chain or a continuous-time one. Al-
though the discrete-time model is sometimes argued to be
more realistic [10], [11], there is no fundamental difference
between the two, and similar results have been shown for
both. We focus on the discrete-time Markov chain here.

It is known that these epidemic models exhibit a phase
transition behavior at a certain threshold [12], [13] , i.e., once
the effective infection rate τ = β
δ approaches a critical value
τc [8] the epidemic appears not to die out. We should remark
that the Markov chain has a unique absorbing state, which is
the all-healthy state, because once the system reaches this
state it remains there forever since there are no infected
nodes to propagate infections. This means that if we wait
long enough the epidemic will eventually die out, which may
seem to be odd at ﬁrst. However, what this means is that the
question of the epidemic dying out is not interesting; what
is interesting is the question of how long it takes for the
epidemic to die out? In particular, if the mixing time of the
Markov chain is exponentially large one will not see it die out
in any reasonable time. Therefore the right question to ask is
what is the mixing time of the Markov chain (or, equivalently,
its mean-time-to-absorption); it turns out that the threshold τc
corresponds to the phase transition between “slow mixing”
(exponential time) and “fast mixing” (logarithmic time) of
the MC [14], [15], [16].

The epidemic threshold (critical value) of general networks
is still an open problem. However, lower- and upper-bounds
have been found using different techniques [14], [16]. The
best known lower-bound is 1/λmax(A), i.e. the inverse of the
leading eigenvalue of the adjacency matrix, which is derived
by upper-bounding the marginal probabilities of infection
and using a linear dynamical system. In fact, this method
relies on keeping track of n variables which are upper bounds
on the marginal probability of infection for any of the nodes.
In this paper, we focus on improving this upper-bound on
the infection probabilities and ultimately the lower-bound
on the epidemic threshold. The key idea, is to maintain the
“pairwise” probabilities of nodes’ infections, in addition to
the marginals. This comes at the cost of increased, yet still

Fig. 1: State diagram of a single node in the SIS model,
and the transition rates. Wavy arrow represents exogenous
(neighbor-based) transition. β : probability of infection per
infected link, δ : probability of recovery.

perfectly feasible, computation. There is a trade-off between
the tightness of the bound and the complexity, and in theory
if one takes into account all marginals, pairs, triples, and
higher order terms, we get back to the original 2n-state
Markov chain.

We ﬁrst brieﬂy review the known bound with marginals,
and show a simple alternative approach for deriving it. We
then move on to pairs and use the machinery developed in
Section II to derive tighter bounds on the probabilities and
connect it with the mixing time of the Markov chain (Sec-
tions III and IV). Finally, we demonstrate the improvement
of the bounds through extensive simulations (Section V), and
conclude with future directions.

II. THE MARKOV CHAIN AND MARGINAL

PROBABILITIES OF INFECTION

from the disease with probability δ. That is
P(ξi(t + 1) = Yi|ξ(t) = X)

(1 − β)mi
1 − (1 − β)mi
δ
1 − δ

if (Xi, Yi) = (0, 0),|Ni ∩ S(X)| = mi,
if (Xi, Yi) = (0, 1),|Ni ∩ S(X)| = mi,
if (Xi, Yi) = (1, 0),|Ni ∩ S(X)| = mi,
if (Xi, Yi) = (1, 1),|Ni ∩ S(X)| = mi,



=

(2)
where S(X) is the support of X ∈ {0, 1}n, i.e. S(X) = {i :
Xi = 1}, and Ni is the set of neighbors of node i.
Eqs. (1, 2) completely deﬁne the 2n× 2n transition matrix
of the Markov chain, which determines the evolution of
the 2n states over time. Of course with this, we have the
joint probability of all the nodes, and we can compute the
probability of any desired combination by marginalizing
out the rest. In particular, one can compute the probability
of each node i being infected at time t + 1 (denoted by
pi(t + 1)), which is a function of all joint probabilities of
the states at time t. Since there are only n such variables,
the dimension would be signiﬁcantly reduced if one could
“bound” or “approximate” that function by something that
includes marginals pi(t) only. This way we obtain a recursion
which relates the marginals at time t + 1 to those of time t,
and indeed we have a system with only n states rather that
2n states. Approximations per se are not very interesting
because they do not provide any guarantee on the behavior
of the exact Markov chain model. What is more important is
whether one can obtain a bound on these true probabilities,
which can guarantee for example fast extinction of disease.
The most common upper-bound, which has been shown to
be the tightest linear upper-bound with marginals only (using
a linear programming technique) [17], [18] is:

pi(t + 1) ≤ (1 − δ)pi(t) + β

pj(t)

(3)

(cid:88)

j∈Ni

Let G = (V, E) be an arbitrary connected undirected
network with n nodes, and with adjacency matrix A. Each
node can be in a state of health, represented by “0”, or
a state of infection, represented by “1”. The state of the
entire network can be represented by a binary n-tuple ξ(t) =
(ξ1(t),··· , ξn(t)) ∈ {0, 1}n, where each of the entries
represents the state of a node at time t, i.e. i is infected
if ξi(t) = 1 and it is healthy if ξi(t) = 0.

for all i = 1, . . . , n. Deﬁning p(t) = (p1(t), . . . , pn(t))T ,
this can be written in a matrix form as
p(t + 1) (cid:22) M p(t),

(4)

where

M = (1 − δ)In + βA.

(5)

A. An Alternative Bounding Technique

Given the current state ξ(t), the infection probability of
each node in the next step is determined independently, and
therefore the transition matrix S of this Markov Chain has
elements SX,Y = P(ξ(t+1) = Y |ξ(t) = X) of the following
form:

P(ξ(t+1) = Y |ξ(t) = X) =

P(ξi(t+1) = Yi|ξ(t) = X),
(1)

for any two state vectors X, Y ∈ {0, 1}n.

As mentioned before, a healthy node can receive infection
from any of its infected neighbors independently with prob-
ability β per infected link, and an infected node can recover

n(cid:89)

i=1

The derivation of (3) in [17], [18] involves a linear pro-
gramming technique. In this paper, we provide an alternative
technique to bound the infection probabilities using indicator
variables and conditional expectation, which is more intuitive
and more direct. Importantly, as will be shown later, this
technique can be used to obtain tighter bounds on the
exact probabilities of infections using pairwise inﬂectional
probabilities. Before that, it is instructive to derive (3) using
this alternative approach.
Let i ∈ V . We start by conditioning on the state of the

same node i at time t, as follows:
pi(t + 1) =P(Xi(t + 1) = 1|Xi(t) = 1)P(Xi(t) = 1)

+ P(Xi(t + 1) = 1|Xi(t) = 0)P(Xi(t) = 0).

The probability that an infected node remains infected is
1 − δ, and the probability that a susceptible node does not
receive infection from an infected neighbor is 1 − β. We
denote j neighbor of i by j ∼ i. The expression above can
be written as
pi(t + 1) = (1 − δ)pi(t)+
EX−i(t)|Xi(t)=0

(cid:21)
(1 − β1Xj (t))

(cid:20)
1 −(cid:89)

(6)

P(Xi(t) = 0).

j∼i

The conditional expectation is on the joint probability of all
nodes other than i (denoted by X−i) given node i being
healthy (Xi = 0). Of note, this expression is still exact, and
we have not done any approximation yet. It can be easily

checked that(cid:89)

j∼i

(cid:88)

j∼i

1Xj (t).

Combining this with (6), yields the desired upper bound
pi(t + 1) ≤ (1 − δ)pi(t) + β

P(Xi(t) = 0, Xj(t) = 1)

(7)

(1 − β1Xj (t)) ≥ 1 − β
(cid:88)
(cid:88)

j∼i

≤ (1 − δ)pi(t) + β

pj(t).

j∼i

B. Connection to Mixing Time of the Markov Chain

Up to this point, we just

talked about bounding the
marginal probabilities of infection, and it is not clear how
a bound on the marginal probabilities relates to the mixing
time of the Markov chain. To establish this connection, let
us start from the deﬁnition of mixing time [19]:

tmix() = min{t : sup

µ

(cid:107)µSt − π(cid:107)T V ≤ },

(8)

where µ is any initial probability distribution deﬁned on the
state space and π is the stationary distribution; (cid:107)µ−µ(cid:48)(cid:107)T V is
the total variation distance of any two probability measures
µ and µ(cid:48), and is deﬁned by
(cid:107)µ − µ(cid:48)(cid:107)T V =
1
2

|µ(x) − µ(cid:48)(x)|,

(cid:88)

x

where x is any possible state in the probability space. In fact
tmix() is the minimum time instant for which the distance
between the stationary distribution and the probability distri-
bution at time t from any initial distribution is smaller than or
equal to . Roughly speaking, the mixing time measures how
fast the initial distribution converges to the limit distribution,
which in our case means how quickly the epidemic dies out.
Since in the stationary distribution the all-healthy state has

probability 1, it can be shown [17] that

(cid:107)µSt − π(cid:107)T V = P(cid:0)some nodes are infected at time t|
(cid:1)

all nodes were infected at time 0

(9)

sup

µ

which highlights the fact that the worst initial distribution
(i.e. the µ that maximizes above quantity) is the all-infected

state. Now for any t < tmix() we have

(cid:1)
 < P(cid:0)some nodes are infected at time t|
≤ n(cid:88)
P(cid:0)node i is infected at time t|

all nodes were infected at time 0

all nodes were infected at time 0

(cid:1)

i=1
= 1T
n p(t)

given that p(0) = 1n,

(10)

where we have used the union bound, and 1n denotes the
all-ones vector of size n.
Back to the upper-bound on the marginals (4), we get
n p(t) ≤ 1T
n M p(t − 1). Furthermore, since M has non-
1T
negative entries (we write this as M ≥ 0) we can “propagate”
the bound to ﬁnd that
n p(t) ≤ 1T
1T
As a result, for any t < tmix()

n M 2p(t−2) ≤ ··· ≤ 1T

n M p(t−1) ≤ 1T

n M tp(0).

 < 1T

n M t1n ≤ n(ρ(M ))t,

(11)





since M is non-negative and symmetric, and λmax(M ) =
ρ(M ), where ρ(M ) is the spectral radius of M.
When ρ(M ) < 1 (or equivalently 1− δ + βλmax(A) < 1),
it follows that t < log n
− log ρ(M ) for all t < tmix(). This implies
the well-known result that when β/δ < 1/λmax(A) then
tmix() ≤ log n

− log ρ(M ) = O(log n).

We should note here that if M was not symmetric (as
we will encounter such instances in the next section), it can
be shown by an appeal to the Lyapunov equation that if
ρ(M ) < 1 then for all t < tmix() there exists 0 < η < 1
such that  ≤ ηtO(poly(n)), from which it follows directly
that the mixing time is logarithmic in n. To see that, note that
ρ(M ) < 1 implies that there exists a positive deﬁnite matrix
P (cid:31) 0 such that M T P M − P ≺ 0. Letting P 1/2 denote the
unique positive square root of P and N := P 1/2M P −1/2,
it follows easily that N T N ≺ Id, or equivalently η :=
(cid:107)N(cid:107)2 < 1. (Here, (cid:107)N(cid:107)2 denotes the spectral norm of N.)
Deﬁning y := P 1/21n and x := P −1/21n we get 1T
n M t1n =
xT N ty ≤ (cid:107)x(cid:107)2ηt(cid:107)y(cid:107)2 ≤ nηt(cid:107)P 1/2(cid:107)2(cid:107)P −1/2(cid:107)2.
III. PAIRWISE PROBABILITIES (pij)

In section II we showed how a bound on marginal prob-
abilities of infection can be obtained, and how this bound
translates to the threshold condition for fast mixing of the
Markov chain. As mentioned before, the bound (4) has been
proved to be the tightest linear bound one can get with
marginals; A natural idea to improve this bound is to go
to higher order terms (i.e. pairs, triples, etc.). In principle,
maintaining higher order terms is advantageous because it
means keeping more information from the original chain,
but of course at the cost of increased complexity. We deﬁne
the pairwise probability of infection of two nodes, in addition
to the marginals, as follows. For (i, j) ∈ E,

Note that out of the (cid:0)n

pij(t) := P(Xi(t) = 1, Xj(t) = 1).

(cid:1) possible pairs of nodes, we only

consider the ones that correspond to edges in the graph.

2

Based on this deﬁnition, P(Xi(t) = 0, Xj(t) = 1) =
pj(t) − pij(t) and it follows easily from (7) that
pi(t + 1) ≤ (1 − δ)pi(t) + β

pj(t) − β

pij(t). (12)

(cid:88)

(cid:88)

j∼i

j∼i

Of course, this bound is at least as tight as the one in (3).
Now, in order to strictly improve upon the latter, we need a
lower bound on the pairwise infection probabilities at time
t + 1 in terms of marginals and pairwise probabilities at time
t, which is derived next.

A. A Lower Bound on the pij’s

To construct a lower bound on the pairwise marginal
probabilities pij(t + 1), we use the same approach as was
introduced in Section II-A, but this time applied to pairwise
infection probabilities.
Let (i, j) ∈ E and t ≥ 0. We ﬁrst expand pij(t + 1) as

P(Xi(t + 1) = 1, Xj(t + 1) = 1, Xi(t) = x, Xj(t) = y).

follows(cid:88)

x∈{0,1}
y∈{0,1}

(cid:21)

(cid:22) M(cid:48)(cid:20) p(t)−pE(t)
(cid:21)

B. Back to the Mixing Time

In order to express Eqs. (12) and (13) for all i and
j’s together in a matrix form, recall the deﬁnition p(t) =
(p1(t), . . . , pn(t))T . Further, let us deﬁne pE(t) ∈ R|E| as
the vector of pairwise infection probabilities, i.e., pE(t) =
vec(pij(t) : (i, j) ∈ E). Note that pij(t) = pji(t), so for
each edge we only keep track of one of the two terms. Now
we can write Eqs. (12), (13) as

.

−pE(t + 1)

(14)
The matrix M(cid:48), after a little bit of thought, can be expressed
in the following way.
M(cid:48) =
, (15)
where B ∈ R|V |×|E| happens to be the incidence matrix of
G, which is formally deﬁned as

(1 − δ)(1 − δ − 2β)I|E|

(cid:21)

βB

(cid:20) p(t + 1)
(cid:20)(1 − δ)In + βA
(cid:40)

−(1 − δ)βBT

Bi,e =

1
0

if i is an endpoint of e,
otherwise,

(cid:21)

.

(16)

for all i ∈ V and e ∈ E.

By accounting for pairwise infection probabilities,

the
bound derived in (14) is tighter when compared to the one
in (4). In order to connect this to the the mixing time of the
underlying Markov chain, observe that

(cid:104)

(cid:105)(cid:20) p(t)−pE(t)
(cid:21)
M(cid:48)(cid:20) p(t − 1)

.

−pE(t − 1)

(cid:105)

Applying (14) to this gives,

1T
n p(t) =

1T
n

0T|E|

n p(t) ≤(cid:104)
(cid:105)

1T

1T
n

0T|E|

(cid:104)

0T|E|

This step is possible because the entries of the vector
1T
are all non-negative, which guarantees that the
n
signs of all the n + |E| inequalities in (14) are preserved.
With this note, it becomes clear that in order to be able
to propagate the bounds for the remaining time instances
t − 2, t − 3, . . . , 0, a sufﬁcient condition would be
for all t ≥ 1.

(M(cid:48))t ≥ 0,

(17)

Provided that (17) holds, we can continue with the se-

quence of bounds after (16), which results in

1T
n

0T|E|

(cid:104)
(cid:105)
n p(t) ≤(cid:104)

1T

(cid:105)

(cid:20) p(0)−pE(0)
(cid:21)

1T
n

0T|E|

(M(cid:48))t

.

(18)

Subsequently, the same argument as in II-B concludes the
following result.
Theorem 1. Assume that (17) holds. If ρ(M(cid:48)) < 1, then the
mixing time of the Markov chain whose transition matrix S
is described by Eqs. (1) and (2) is O(log n).

From the old bound it was known that when ρ(M ) < 1
then the Markov chain is fast-mixing. Now in addition to that,
this theorem states that also when ρ(M(cid:48)) < 1 the Markov
chain mixes fast. Of course, this is informative only when

For convenience, denote each one of the summands above
by sxy. Also, let cxy represent the corresponding conditional
probability P(Xi(t + 1) = 1, Xj(t + 1) = 1|Xi(t) =
x, Xj(t) = y). In what follows, we lower bound each
one of sxy’s. We write Exy for the conditional expectation
EX−i,−j (t)|{Xi(t)=x,Xj (t)=y}.
• (x=0,y=0): Trivially, s00 ≥ 0.
• (x=0,y=1): As before, the probability of not getting infected
from each infected neighbor is (1 − β), and the probability
that an infected node remains infected is 1 − δ. Therefore

(1 − β1Xk(t)))(1 − δ)(cid:3).
Since j ∼ i and 0 ≤ β ≤ 1, it follows that (cid:81)

k∼i(1 −
β1Xk(t)) ≤ (1− β1Xj (t)). Hence c01 ≥ β(1− δ)E011Xj (t),
which eventually gives

(cid:2)(1 −(cid:89)

c01 = E01

k∼i

s01 ≥ β(1 − δ)P(Xj(t) = 1, Xi(t) = 0)

≥ β(1 − δ)pj(t) − β(1 − δ)pij(t).

• (x=1,y=0): By symmetry, the exact same argument as above
implies

s10 ≥ β(1 − δ)pi(t) − β(1 − δ)pij(t).
• (x=1,y=1): Clearly c11 = (1 − δ)2, which gives

s11 ≥ (1 − δ)2pij(t).

Adding up all the above terms yields the following lower

bound for all (i, j) ∈ E and t ≥ 0:

pij(t + 1) ≥ (1 − δ)β(pi(t) + pj(t))

+ (1 − δ)(1 − δ − 2β)pij(t).

(13)

TABLE I: Performance of the proposed bounds M(cid:48) and M(cid:48)(cid:48)
in comparison with the old bound M. Boldface values show
an improvement over the old bound. The signs next to ρ(M(cid:48))
indicate whether the non-negativity condition (required for
the proof) holds.

ρ(M ) =

ρ(M(cid:48))

ρ(M(cid:48)(cid:48))

Star

Cycle

Star-line

Clique

Erd˝os-R´enyi

δ=0.750, β=0.078
δ=0.500, β=0.053
δ=0.250, β=0.028
δ=0.750, β=0.390
δ=0.500, β=0.265
δ=0.250, β=0.140
δ=0.750, β=0.174
δ=0.500, β=0.118
δ=0.250, β=0.063
δ=0.750, β=0.008
δ=0.500, β=0.005
δ=0.250, β=0.003
δ=0.750, β=0.070
δ=0.500, β=0.048
δ=0.250, β=0.026
δ=0.750, β=0.077
δ=0.500, β=0.053
δ=0.250, β=0.028

1 + 
1.030
1.030
1.030
1.030
1.030
1.030
1.030
1.030
1.030
1.003
1.003
1.003
1.030
1.030
1.030
1.030
1.030
1.030

0.968

0.945
0.942

0.958
0.955

0.903
0.828
0.840
0.817
0.720
0.882
0.872
0.693
0.856
0.999
0.998
0.999
0.993
0.977
0.984
0.991
0.974
0.982

+
+
-
+
-
-
+
-
-
+
+
+
+
+
+
+
+
+

there is a case where ρ(M ) > 1 but ρ(M(cid:48)) < 1. As it will
be shown in the Section V this is indeed the case.

Note that in the proof of Theorem 1 we used the assump-
tion that (17) holds. As it will be shown in the simulations
section,
in many cases this is a reasonable assumption.
However, when the assumption does not hold we cannot
appeal to this theorem. For this reason we propose another
bound using an alternative pairwise probability, which does
not require such a condition.

IV. AN ALTERNATIVE PAIRWISE PROBABILITY (qij)
As it was discussed above, when the assumption (17) does

not hold, we seek an alternative bound. Let us deﬁne

qij(t) := P(Xi(t) = 0, Xj(t) = 1).

We can use the same approach as before to obtain bounds
for pi, qij’s. Intuitively, lower bounding pij(t + 1) is equiv-
alent with upper bounding qij(t + 1), and it turns out that
it is what we need. The next lemma summarizes the bounds
on these probabilities.
Lemma 2. For all i, j ∈ V , (i, j) ∈ E and t ≥ 0, it holds
that

pi(t + 1) ≤ (1 − δ)pi(t) + β
qij(t + 1) ≤ δ(1 − δ)pj(t) + (1 − δ)(1 − δ − β)qij(t)

qi(cid:96)(t)

(cid:96)∼i

(cid:88)

+ βδqji(t) + β(1 + δ)

qj(cid:96)(t) (20)

(cid:88)

(cid:96)∼j
(cid:96)(cid:54)=i

Proof. Observe that (19) is nothing but (12) expressed in
terms of qij’s. Now let (i, j) ∈ E. We ﬁrst expand qij(t + 1)

as follows(cid:88)

P(Xi(t + 1) = 0, Xj(t + 1) = 1, Xi(t) = x, Xj(t) = y).

x∈{0,1}
y∈{0,1}
For convenience, denote each one of the summands above
by sxy. Also, let cxy denote the corresponding conditional
probabilities P(Xi(t + 1) = 0, Xj(t + 1) = 1|Xi(t) =
x, Xj(t) = y). In what follows, we upper bound each one of
the sxy’s; this will immediately yield (20). All expectations
below are conditional on the events {Xi(t) = x, Xj(t) = y}
which is omitted for the sake of convenience.
(cid:89)
c00 = E(cid:2) (
• (x=0,y=0): Using the fact that
(1 − β1Xk(t)))
(cid:124)
(cid:123)(cid:122)
(cid:125)
we ﬁnd that s00 ≤ β(cid:80)
0, X(cid:96)(t) = 1) ≤ β(cid:80)
(cid:89)
c01 = E(cid:2)(

(1 −(cid:89)
(1 − β1X(cid:96)(t)))
(cid:123)(cid:122)
(cid:125)
(cid:124)
≤β(cid:80)

(1 − β1Xk(t)))(1 − δ)(cid:3) ≤ 1 − β1Xj ,

P(Xi(t) = 0, Xj(t) =

• (x=0,y=1): From

(cid:96)∼j
qj(cid:96)(t).

(cid:3),

(cid:96)∼j
(cid:96)(cid:54)=i

k∼i

(cid:96)∼j

≤1

(cid:96)∼j

1X(cid:96)

k∼i

it follows that s01 ≤ (1 − β)(1 − δ)qij(t).

(19)

Watts-Strogatz

(cid:88)

(cid:96)∼j

1X(cid:96),

(1 − β1X(cid:96)(t)))(cid:3) ≤ βδ

• (x=1,y=0): Using the fact that

c10 = E(cid:2)δ(1 −(cid:89)
we ﬁnd that s10 ≤ βδ(cid:80)
0, X(cid:96)(t) = 1) ≤ βδ(cid:80)

(cid:96)∼j

(cid:22) M(cid:48)(cid:48)(cid:20) p(t)

(cid:21)

(cid:96)∼j
(cid:96)∼j qj(cid:96)(t).

P(Xi(t) = 1, Xj(t) =
• (x=1,y=1): Using the fact that c11 = δ(1− δ), we ﬁnd that
s11 = δ(1−δ)P(Xi(t) = 1, Xj(t) = 1) = δ(1−δ)(pj−qij).
Similar as before, by deﬁning a vector qE(t) ∈ R2|E| as
qE(t) = vec(qij(t) : (i, j) ∈ E), we can express (19), (20)
as

(cid:20) p(t + 1)

(cid:21)

,

qE(t)

qE(t + 1)

(21)
for some appropriately deﬁned square matrix M(cid:48)(cid:48) of size
n + 2|E|. It is easy to see that if 1−δ−β ≥ 0, then M(cid:48)(cid:48) ≥ 0,
i.e. all entries of M are nonnegative. In particular, this implies
that M(cid:48)(cid:48) satisﬁes (17) and it only takes repeating the same
argument as in (18) to conclude with the following theorem.
Theorem 3. If ρ(M(cid:48)(cid:48)) < 1 and 1 − δ − β ≥ 0, then the
mixing time of the Markov chain whose transition matrix S
is described by Eqs. (1) and (2) is O(log n).

V. EXPERIMENTAL RESULTS

In this section, we demonstrate the performance of the
proposed bounds by evaluating them on a variety of networks
such as clique, Erd˝os-R´enyi, Watts-Strogatz, star graph, line
graph, cycle, and star-line graph, with various parameters

was shown to be tighter than the so-called βλmax(A)
condition.

δ

< 1

Clearly one possible extension of this work would be to
construct a bound consisting of marginals, pairs, and triples,
which in theory should result in an improvement. In fact,
keeping all higher order terms eventually takes us back to the
2n-state Markov chain. Therefore there is a trade-off between
the complexity and the accuracy of the model. We should
however note that going to triples may still be tractable, and
one advantage of that would be not only to gain by improving
the bound on the probability (same as here) but also to get
an improvement in the fast-mixing condition of the chain in
i pi−
i,j,k pijk, which naturally includes all terms
rather than just the marginals. As a last comment, based
on the simulations we conjecture that condition (17) may be
relaxed, and other future work may concern its proof.

the sense that the bound (10) can be replaced by  <(cid:80)
(cid:80)
i,j pij +(cid:80)

VII. ACKNOWLEDGMENT

The authors would like to thank Ahmed Douik, Anatoly Khina

and Ehsan Abbasi for insightful discussions on the subject.

REFERENCES

[1] N. T. Bailey et al., The mathematical theory of infectious diseases and its
Charles Grifﬁn & Company Ltd, 5a Crendon Street, High

applications.
Wycombe, Bucks HP13 6LE., 1975.

[2] J. E. Phelps, R. Lewis, L. Mobilio, D. Perry, and N. Raman, “Viral marketing
or electronic word-of-mouth advertising: Examining consumer responses and
motivations to pass along email,” Journal of advertising research, vol. 44, no. 04,
pp. 333–348, 2004.

[3] M. Richardson and P. Domingos, “Mining knowledge-sharing sites for viral
marketing,” in Proceedings of the eighth ACM SIGKDD international conference
on Knowledge discovery and data mining. ACM, 2002, pp. 61–70.

[4] T. Alpcan and T. Bas¸ar, Network security: A decision and game-theoretic

approach. Cambridge University Press, 2010.

[5] D. Acemoglu, A. Malekian, and A. Ozdaglar, “Network security and contagion,”

National Bureau of Economic Research, Tech. Rep., 2013.

[6] P. Jacquet, B. Mans, and G. Rodolakis, “Information propagation speed in mobile
and delay tolerant networks,” Information Theory, IEEE Transactions on, vol. 56,
no. 10, pp. 5001–5015, 2010.

[7] M. Cha, A. Mislove, and K. P. Gummadi, “A measurement-driven analysis of
information propagation in the ﬂickr social network,” in Proceedings of the 18th
international conference on World wide web. ACM, 2009, pp. 721–730.

[8] C. Nowzari, V. M. Preciado, and G. J. Pappas, “Analysis and control of
epidemics: A survey of spreading processes on complex networks,” Control
Systems, IEEE, vol. 36, no. 1, pp. 26–46, 2016.

[9] R. Pastor-Satorras, C. Castellano, P. Van Mieghem, and A. Vespignani, “Epi-
demic processes in complex networks,” Reviews of modern physics, vol. 87,
no. 3, p. 925, 2015.

[10] A. Arenas, J. Borge-Holthoefer, S. Meloni, Y. Moreno et al., “Discrete-time
markov chain approach to contact-based disease spreading in complex networks,”
EPL (Europhysics Letters), vol. 89, no. 3, p. 38009, 2010.

[11] H. J. Ahn, “Random propagation in complex systems: nonlinear matrix recur-
sions and epidemic spread,” Ph.D. dissertation, California Institute of Technol-
ogy, 2014.

[12] C. Castellano and R. Pastor-Satorras, “Thresholds for epidemic spreading in

networks,” Physical review letters, vol. 105, no. 21, p. 218701, 2010.

[13] A. Barrat, M. Barthelemy, and A. Vespignani, Dynamical processes on complex

networks. Cambridge University Press, 2008.

[14] M. Draief and L. Massouli, Epidemics and rumours in complex networks.

Cambridge University Press, 2010.

[15] P. Van Mieghem, “Decay towards the overall-healthy state in SIS epidemics on

networks,” arXiv preprint arXiv:1310.3980, 2013.

[16] P. Van Mieghemy, F. D. Sahnehz, and C. Scoglioz, “An upper bound for the
epidemic threshold in exact Markovian SIR and SIS epidemics on networks,” in
Decision and Control (CDC), 2014 IEEE 53rd Annual Conference on.
IEEE,
2014, pp. 6228–6233.

[17] H. J. Ahn and B. Hassibi, “On the mixing time of the SIS markov chain model
for epidemic spread,” in Decision and Control (CDC), 2014 IEEE 53rd Annual
Conference on.

IEEE, 2014.

[18] N. A. Ruhi and B. Hassibi, “SIRS epidemics on complex networks: Concurrence
of exact markov chain and approximated models,” in 2015 54th IEEE Conference
on Decision and Control (CDC), Dec 2015, pp. 2919–2926.

[19] D. A. Levin, Y. Peres, and E. L. Wilmer, Markov chains and mixing times.

American Mathematical Soc., 2009.

Fig. 2: Evolution of the SIS epidemic over a star graph with
n = 2000 nodes, with two values of ρ(M(cid:48)) below and above
1. When ρ(M(cid:48)) = 0.99 < 1 we observe fast extinction of
the epidemic (blue curve). It seems that the condition is also
very tight since for ρ(M(cid:48)) = 1.05 > 1 the epidemic does
not die out (red curve).

δ

β and δ. As mentioned before, in order for any of the two
threshold conditions proposed in sections III and IV to be an
improvement, we need to check if there are cases where the
spectral radius of M(cid:48) or M(cid:48)(cid:48) is less than 1, while the spectral
radius of M is greater than 1 (or equivalently βλmax(A)
> 1).
Indeed extensive simulations on our ﬁrst bound (M(cid:48)) suggest
that not only are there such cases, but interestingly always
ρ(M(cid:48)) ≤ ρ(M ).
In order to compare M(cid:48) with M, we set ρ(M ) = 1 +
 > 1 for some small value of , and observe the value of
ρ(M(cid:48)). Table I lists the values of spectral radii for the three
matrices. The positive sign next to ρ(M(cid:48)) indicates that the
non-negativity condition (17) holds. For the cases that the
condition holds (+), we can conclude that M(cid:48) has clearly
improved. For the cases where the condition does not hold
(-) we evaluate the second proposed bound M(cid:48)(cid:48), which again
shows clear improvement over the old bound.

In order to demonstrate how tight the new condition is,
Fig. 2 plots the evolution of the epidemic over a star graph,
for which the old bound is known not to be tight. The
parameters in the two cases are δ = 0.3, β = 0.0130, and
δ = 0.3, β = 0.0157. It can be seen that while the value of
βλmax(A)
is not informative (it is 1.93 > 1 for the ﬁrst case,
and 2.33 > 1 for the second one), the ρ(M(cid:48)) condition is
quite tight.

δ

VI. CONCLUSION AND FUTURE WORK

In this paper, we ﬁrst proposed a simple technique using
conditional expectations to systematically construct bounds
on the exact probabilities of infection, up to any desired
order. Using this approach, we showed that keeping higher
order terms (such as pairs) indeed helps in obtaining tighter
bounds; speciﬁcally we derived a bound composed of both
marginals and pairwise probabilities which has improved
over the well-known bounds. Based on this new bound, we
provided a new condition for fast mixing of the Markov chain
to the all-healthy state, which through extensive simulations

01002003004005006007008009001000Time Step01002003004005006007008009001000Number of Infected Nodesρ(M′)=1.05>1ρ(M′)=0.99<1