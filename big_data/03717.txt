6
1
0
2

 
r
a

 

M
1
1

 
 
]
h
p
-
t
n
a
u
q
[
 
 

1
v
7
1
7
3
0

.

3
0
6
1
:
v
i
X
r
a

The Asymptotics of Quantum Max-Flow Min-Cut

Matthew B. Hastings

1Station Q, Microsoft Research, Santa Barbara, CA 93106-6105, USA

2Quantum Architectures and Computation Group, Microsoft Research, Redmond, WA 98052, USA

The quantum max-ﬂow min-cut conjecture relates the rank of a tensor network to the minimum
cut in the case that all tensors in the network are identical1. This conjecture was shown to be false
in Ref. 2 by an explicit counter-example. Here, we show that the conjecture is almost true, in that
the ratio of the quantum max-ﬂow to the quantum min-cut converges to 1 as the dimension N of the
degrees of freedom on the edges of the network tends to inﬁnity. The proof is based on estimating
moments of the singular values of the network. We introduce a generalization of “rainbow diagrams”4
to tensor networks to estimate the dominant diagrams. A direct comparison of second and fourth
moments lower bounds the ratio of the quantum max-ﬂow to the quantum min-cut by a constant.
To show the tighter bound that the ratio tends to 1, we consider higher moments. In addition, we
show that the limiting moments as N → ∞ agree with that in a diﬀerent ensemble where tensors in
the network are chosen independently; this is used to show that the distributions of singular values
in the two diﬀerent ensembles weakly converge to the same limiting distribution. We present also a
numerical study of one particular tensor network, which shows a surprising dependence of the rank
deﬁcit on N mod 4 and suggests further conjecture on the limiting behavior of the rank.

PACS numbers:

I.

INTRODUCTION

The quantum max-ﬂow min-cut conjecture was introduced in Ref. 1. This conjecture relates the rank of a tensor
network for a generic choice of tensor to a maximal classical ﬂow (or minimal cut) on the graph corresponding to
the tensor network. In Ref. 2, various forms of the quantum max-ﬂow min-cut conjecture were considered, and the
conjectures were in fact shown to be false. Here, we consider a particular version of the conjecture, called version
2 in that paper. Even though the conjecture is not true, we show that the ratio of the actual rank to the rank
predicted by the conjecture converges to 1 in the limit of large dimension N of the degrees of freedom on the edges
of the network. The proof is statistical in nature, relying on a random choice of tensor in the network in a particular
Gaussian ensemble.

We begin by reviewing that particular form of the conjecture (in fact, we consider a special case of the conjecture in
that paper, in which all the vertices in the graph have the same degree and all edges have the same capacity; however,
our results can be fairly straightfowardly extended to the general case of the conjecture). We slightly modify the
notation in that paper.

Consider a tensor network. The tensor network is deﬁned by several pieces of data. First, there is a graph G, with
some open edges (i.e., edges that attach to only one vertex inside the graph) and some closed edges (edges which
attach to two vertices). Second, for each edge, there is an integer, called the capacity in Ref. 2. Finally, for each
vertex, there is a tensor; the number of indices of the tensor is equal to the degree of that vertex and each index of
the tensor corresponds to a distinct edge attached to that vertex; each index ranges over a number of possible values
equal to the capacity of that edge. The entries of the tensor are complex numbers (one could also consider the case
that they are real numbers; this would require some modiﬁcations of the techniques here).

By contracting the tensor network, the tensor network assigns a complex number for each choice of indices on the
open edges. We partition the open edges into two sets, called the input set and output set. Deﬁne DS to equal the
product of the capacities of all input edges and deﬁne DT to equal the product of the capacities of all output edges.
This contraction of the tensor network deﬁnes a linear operator L from a complex vector space of dimension DS to
one of dimension DT .

For a given tensor network, the rank of this linear operator is termed the quantum ﬂow of the network. We deﬁne
two sets, S and T ; these sets are the open ends of the input and output open edges, respectively. We let V be the
set of vertices in the graph, not including S and T . We let V = S ∪ T ∪ V ; below when we refer to vertices, we only
mean vertices in V . A cut is a partition of V into S ∪ T , where S ⊂ S and T ⊂ T . The cut set of the cut is the set of
edges (v, w) with v ∈ S and w ∈ T . For a given graph, and a given cut set separating S from T , deﬁne DC to equal
the product of the capacities of the edges in the cut set. Then, deﬁne the quantum min-cut of the network to be the
minimum of DC over all cuts.

For a given graph and given capacities, for any choice of tensors, the quantum ﬂow is bounded by the quantum
min-cut. We brieﬂy sketch the proof2. Given a cut set C, cutting the edges in the cut set separates the graph into
two graphs, and similarly the tensor network can be split into two tensor networks, one with input S and output C

2

and one with input C and output T . Letting L1 and L2 be the linear operators deﬁned by these networks, the linear
operator L can be written as a product L = L2L1. Clearly, since L1 is a map to a space of dimension DC, it has rank
at most DC and hence so does L.

This leaves the question: under what circumstances will the quantum ﬂow equal the quantum min-cut? Suppose
that all edges have the same capacity. We denote this capacity by N (c was used in Ref. 2; here is one place where
we change notation to be more suggestive of “large N ” limits in physics). Then, suppose that all vertices have the
same degree d. Choose a single tensor T which has d indices, each ranging from 1 to N . We then use the same
tensor T on all vertices of the graph. For each vertex, choose some ordering assigning indices of the tensor to edges
attached to that vertex. Then, for a given d, N , graph, and choice of ordering, deﬁne the “quantum max-ﬂow” to be
the maximum of the rank over all tensors T .

Then, the second version of the quantum max-ﬂow min-cut conjecture is that the quantum max-ﬂow is equal to

the quantum min-cut. This conjecture was shown to be false.

In fact, the conjecture considered in Ref. 2 is slightly more general, as edges are allowed to have diﬀerent capacities.
Then any two vertices which have the same degree and which have the same sequence of capacities of edges attached
to the vertex (with the sequence of edges ordered by the assignment of edges to indices of the tensor) are said to have
the same “valence type” and any two vertices with the same valence type have the same tensor. Our results can be
extended to this case.

Our main result is that the conjecture is “asynptotically true”, in that as N becomes large, the ratio of the quantum
max-ﬂow to the quantum min-cut converges to 1. Write QM C(G, N ) to denote the quantum min-cut for a given
graph G and given capacity N . Write QM F (G, N, O) to denote the quantum max-ﬂow for a given graph G and
ordering O (the symbol L was used for the ordering in Ref. 2 but we use O instead to avoid confusion with L for the
linear operator). For a graph G, let M C(G) denote the minimum cut of G, where each edge is assigned capacity 1 to
determine the min cut. Then,

We show that

Theorem 1. For all G, O,

QM C(G, N ) = N M C(G).

QM F (G, N, O) = QM C(G, N ) · (1 − o(1)).

(I.1)

(I.2)

Here, we use a big-O notation where we consider asymptotic behavior as a function of N . The constant factors
hidden by the big-O notation may depend on G, O. Of course, the fact that the ﬂow is bounded by the quantum
min-cut implies that QM F (G, N, O) ≤ QM C(G, N ). The new result will be to lower bound QM F (G, N, O).

Before proving theorem 1, we will prove a weaker theorem:

(cid:16)

(L†L)2(cid:17)

(I.3)

Theorem 2. For all G, O,

QM F (G, N, O) = Θ(QM C(G, N )).

The proof of this will rely on estimating the expectation value of tr(L†L) and tr

, and using a relation
between moments of an operator and its rank. Most of the work of the paper will be developing tools to estimate
moments.

Before stating the next theorem about moments, we make some deﬁnitions:

Deﬁnition 1. Given two tensor networks, N1,N2 with corresponding graphs G1, G2, we deﬁne their product to be a
tensor network with graph G which is the union of G1, G2. We write the product as a network N1 · N2. The capacity
of an edge in G is given by the capacity of the corresponding edge in G1 or G2 and the ordering of indices of a vertex
in G is given by the ordering of the indices of the corresponding vertex in G1 or G2. If N1,N2 correspond to linear
operators L1, L2 respectively, then their product corresponds to linear operator L1 ⊗ L2; if N1,N2 both have no open
edges, so that the corresponding linear operators are scalars, then the linear operator corresponding to the product is
simply the product of these scalars.
Deﬁnition 2. We say that a tensor network N with open edges is “connected” if every vertex is connected to some
open edge (either input or output) by a path in the graph corresponding to that network.

Note that a tensor network may be connected and yet the graph corresponding to that network may be disconnected.
See Fig. I.1.
Suppose a tensor network N is not connected, with N corresponding to graph G and linear operator L. Then, let
W be the set of vertices which are not connected to an input or output edge. Then, we can write N as a the product

3

FIG. I.1: Example of a tensor network that is connected in the sense of deﬁnition 2. This network has |V | = 2 vertices, labelled
by solid circles, and |E| = 6 edges, all of which are open. There are |S| = 3 input edges, shown at the left of the ﬁgure and
|T| = 3 output edges, shown at the right of the ﬁgure.

of two networks, N1,N2, with linear operators L1, L2 and graphs G1, G2, where the vertices in N2 correspond to the
vertices in W and where N1 is connected. Then, L2 is a scalar which is nonzero for generic choice of T , so that
rank(L) = rank(L1). Further, M C(G) = M C(G1). So, to prove results about the rank of L in terms of M C(G), we
can assume, without loss of generality, that N is connected. So, unless explicitly stated otherwise, all tensor networks
with open edges that we consider will be connected and all linear operators L will correspond to connected tensor
networks.
In general, we use |E| to denote the number of edges of a tensor network (including open edges) and use |V |
to denote the number of vertices.For notational simplicity, throughout the paper we label closed edges by the pair
of vertices (v, w) to which they are attached; the results also apply to the case in which there are multiple edges
connecting a pair of vertices in which case an additional label must be given to the edge to indicate for which edge
it is; for notational simplicity, we do not write this extra label. We will later deﬁne a Gaussian ensemble to choose
the tensors. For most of the paper, we consider the case that all the tensors in the network are identical, given by
some ﬁxed tensor T drawn from this ensemble. However, we sometimes also consider the case that the tensors in the
network are not identical, and instead are chosen independently from the Gaussian ensemble. If we need to distinguish
these two cases, we refer to them as the identical ensemble and independent ensemble. If not otherwise stated, we
are referring to the indentical ensemble. We use E[. . .] to denote an expectation value in the identical ensemle and
Eind[. . .] to denote an expectation value in the independent ensemble.

We can now state the following theorem proven later.

Theorem 3. Consider a tensor network N (assumed connected as explained above) with corresponding graph G.
Then for the linear operator L corresponding to the tensor network, for any k ≥ 1, we have

E[tr

] = c(G, k) · N k|E|−(k−1)M C(G) + O(N k|E|−(k−1)M C(G)−1),

(I.4)

(cid:16)

(L†L)k(cid:17)
(cid:16)
(L†L)k(cid:17)

where where c(G, k) denotes a positive integer that depends upon the graph G and on k (the constant c(G, k) does not
depend on the ordering O). Further,

Eind[tr

] = c(G, k) · N k|E|−(k−1)M C(G) + O(N k|E|−(k−1)M C(G)−1),

(I.5)

with the same constant c(G, k) in Eq. (I.4) as in Eq. (I.5).

Further, for any graph G, the constant c(G, k)is bounded by an exponential in k, i.e., C(G, k) ≤ c1 · ck

2, where the

constants c1, c2 depend upon G.

This theorem requires that the network be connected. As a trivial example to show how this is necessary, consider
a network which computes a scalar. Suppose that the tensors in the network all are also scalars; i.e., the vertices
have degree 0. In this trivial example, suppose that |V | = 2. This network is not connected. If the two “tensors”
at the two diﬀerent vertices are the scalars x, y, then the operator L is equal to the scalar xy.
If one picks x, y
π exp(−|z|2), then one can readily compute
independent complex Gaussians with probability distribution function 1
Eind[tr(L†L)] = Eind[|x|2|y|2] = 1. However, if we choose x Gaussian and set y = x, then E[|x|2|y|2] = E[|x|4] = 2
and so the expectation values would be diﬀerent in the independent and identical ensembles. This example might
seem strange (having degree 0), so the reader can also consider, for example, a tensor network with 2 vertices and d
edges, with no open edges so that all edges connect one vertex to the other; using the techniques later to evaluate
expectation values, the reader can check that the expectation values will diﬀer.

Thus, if we deﬁne

K =

(cid:16)

N|E|−M C(G)(cid:17)−1/2

L

and deﬁne

we have

and

E[av

av(. . .) =

(K†K)k(cid:17)
(cid:16)
(K†K)k(cid:17)
(cid:16)

Eind[av

] = c(G, k) + O(1/N ).

1

N M C(G)

tr(. . .),

] = c(G, k) + O(1/N ),

4

(I.6)

(I.7)

(cid:16)

]. Let µind

(K†K)k(cid:17)

The notation av(. . .) is intended to be suggestive as follows: we know2 that if the tensors are chosen independently
for generic choice of tensors then L (and hence K) has rank N M C(G). Hence, for given K, the expectation value, of
the 2k-th moment of a randomly chosen non-zero singular value of a random tensor in the independent ensemble is
N be the distribution function of a randomly chosen singular value for a randomly chosen
Eind[av(
tensor. By the fact that c(G, k) are bounded by an exponential in k, the distributions µind
N converge weakly to a
limit3. Suppose instead we choose the tensors all identically from the Gaussian ensemble and then randomly choose
one of the largest N M C(G) singular values (i.e., if there are rank(L) ≤ N M C(G) non-zero singular values, then with
probability rank(L)/N M C(G) we choose one of the non-zero singular values and otherwise the result we choose a zero
singular value). Let this distribution be µN . Then, the moments of µN are the same as the moments of µind
N up to
O(1/N ) corrections and so we have the further corollary that

Corollary 1. µN converges weakly to the same limit as µind

N . The limiting distribution has compact support

Also,

Corollary 2. For all  > 0, for all n > 0, there is a constant c such that for all suﬃciently large N the probability
that the largest singular value of L is greater than or equal to x
Proof. Let λ be the largest singular value of L. E[λ2k] ≤ c(G, k)(1 + O(1/N ))N k|E|−(k−1)M C(G). So, the probability
that λ ≥ x

N|E|−M C(G)+ is bounded by cx−n.

N|E|−M C(G)+ is bounded by

√

√

E[λ2k]

x2kN k|E|−kM C(G)+k

= c(G, k)x−2k(1 + O(1/N ))N M C(G)−k,

(I.8)

and choosing k > M C(G))/ and k ≥ n/2, this is bounded by a constant times x−n for all suﬃciently large N .

Since if the tensors are chosen independently, one has QM C(G, N ) = QM F (G, N ) generically, one might naively
guess that corollary 1 implies theorem 1 as follows:
for independent choice of tensors, the linear operator L will
generically have QM C(G, N ) nonzero singular values and so one might expect that when the tensors are chosen
identically the linear operator L will have nearly QM C(G, N ) singular values. The trouble with this naive argument
is that it is conceivable that in the independent ensemble the linear operator K will have QM C(G, N ) nonzero singular
values but that with high probability a constant fraction of them will have magnitude which is o(1) so that µind
N will
converge to, for example, a sum of a smooth function plus a δ-function at the origin. So, to prove theorem 1 we
instead give a more detailed analysis of higher moments.
We begin in section II by lower bounding the rank in terms of traces of moments of the linear operator L. We also
deﬁne the appropriate Gaussian ensemble for T in this section, and give a combinatorial method for computing these
traces for this ensemble. Then, in section III, we use these methods to estimate the expectation value of tr(L†L). In
sections IV,V, we show how to estimate expectation values of traces of higher moments of L†L, as well as expectation
values of products of such traces; this is done by combining a lower bound in section IV for these expectation values
with an upper bound in section V. The techniques for computing the expectation value show that the results are
indeed the same, up to O(1/N ), for the two ensembles. In section VI we complete the proof of theorem 1. In section
VII we collect some results on variance that are not needed elsewhere but may be of independent interest. In section
VIII we present some numerical simulations.

II. MOMENTS BOUND AND DEFINITION OF ENSEMBLE

Let rank(L) denote the rank of a linear operator L. We have the following bound:

Lemma 1. For any linear operator L, and any integer k > 1,

rank(L)k−1 ≥ tr(L†L)k
(L†L)k

tr

(cid:16)

(cid:17) ,

assuming that the denominator of the right-hand side is nonzero. In the special case k = 2 used later we have

rank(L) ≥ tr(L†L)2
(L†L)2

tr

(cid:16)

(cid:17) .

5

(II.1)

(II.2)

Proof. Let the non-zero singular values of L be λ1, ..., λrank(L). Let the vector v be (λ2
be (1, 1, ..., 1). Then, by H¨older’s inequality applied to vectors v, w,

1, ..., λ2

rank(L)). Let the vector w

rank(L)(cid:88)

i=1

(cid:17)1/p

λ2

λ2p
i

i ≤(cid:16)rank(L)(cid:88)
(cid:16)
(L†L)k(cid:17)

i=1

rank(L)1/q

(II.3)

for any p, q with 1/p + 1/q = 1. Choosing p = k, q = k/(k − 1), and raising the above equation to the k-th power, we
get

tr(L†L)k ≤ tr

rank(L)k−1,

(II.4)

as claimed (in the special case of k = 2, we can use Cauchy-Schwarz instead of H¨older).

We will prove Theorem 2 by estimating the expected value of the numerator and denominator of Eq. (II.1) for a
particular random ensemble of tensors. The ensemble that we choose is that the entries of the tensor T will be chosen
independently and indentically distributed, using a Gaussian distribution with probability density

so that E[|z|2] = 1.

exp(−|z|2)

1
π

We estimate the expectation value of the numerator and denominator of Eq. (II.1) independently, rather than

] be given and nonzero for k > 1. Then there must exist some tensor

estimating the expectation value of the ratio, and use the following lemma:
Lemma 2. Let E[tr(L†L)k] and E[tr
T0 for which the corresponding linear operator L0 obeys

(cid:16)

(L†L)k(cid:17)
(cid:17)
rank(L0)k−1 ≥ E[tr(L†L)k]
(L†L)k

(cid:16)

E[tr

Further, since E[tr(L†L)k] ≥ E[tr(L†L)]k,

.

]

.

]

(cid:17)
rank(L0)k−1 ≥ E[tr(L†L)]k
(L†L)k

(cid:16)

E[tr

], there must exist some T0 for which

tr(L

tr

(L

†
0L0)k
†
0L0)k

(cid:17) ≥ E[tr(L†L)]k
(cid:17)

(cid:16)

(L†L)k

E[tr

.

]

(L†L)k(cid:17)
(cid:16)

Proof. Given E[tr(L†L)k] and E[tr

(cid:16)

The result then follows from Eq. (II.1).

(II.5)

(II.6)

(II.7)

6

FIG. II.1: Closed tensor network for computing tr(L†L) for the linear operator corresponding to the tensor network shown in
Fig. I.1. Closed circles represent tensors T , while open circles represent tensors T . The dashed lines on the 4 open edges are
used to indicate that the edges on the right-hand side of the ﬁgure should be joined to the edges on the left-hand side of the
ﬁgure so that the tensor network is closed, computing the trace. Without these dashed lines, the tensor network would have 4
output edges and would compute the linear operator L†L with the four output edges on the right and the four input edges on
the left (while a mathematical expression such as L†L has its “input on the right” and its “output on the left”, tensor networks
are conventionally read from left to right instead.

FIG. II.2: Closed tensor network for computing tr
in Fig. I.1.

(cid:16)

(L†L)2(cid:17)

for the linear operator corresponding to the tensor network shown

For a given tensor T , traces such as tr(L†L) and tr(L†LL†L) are also given by tensor networks; they are tensor

networks with no open edges so that their contraction yields a scalar, and this scalar is equal to the desired trace.

Deﬁnition 3. We refer to such networks with no open edges as closed tensor networks.

See Figs. II.1,II.2. The notation in Fig. II.1 with closed and open circles to denote T and T and dashed lines to
denote edges that should be joined to compute a trace will be used in ﬁgures from here on. We use N to denote the
network with open edges that is used to deﬁne L and we use Nc to denote various diﬀerent tensor networks with no
open edges; the networks Nc that we consider will correspond to traces such as tr(L†L), tr(L†LL†L), and so on; so,
Nc is derived from N and from the choice of the particular trace.
Deﬁnition 4. We introduce notation: Nc(tr(L†L)) indicates the closed tensor network corresponding to tr(L†L),
while Nc(tr(L†LL†L)) indicates the tensor network corresponding to tr(L†LL†L), and so on. Additionally, we consider
closed tensor networks which correspond to products of traces, so that Nc(tr(L†L)tr(L†LL†L)) indicates the tensor
network corresponding to tr(L†L)tr(L†LL†L). Such a tensor network Nc(tr(L†L)tr(L†LL†L)) is the product of the
tensor networks Nc(tr(L†L)) and Nc(tr(L†LL†L)).

We now explain how to compute the expectation value of the contraction of a closed tensor network Nc; for brevity,
we will simply refer to this as “the expectation value of the tensor network”, rather than “the expectation value of
the contraction of the tensor network”. The tensor network Nc is a polynomial in the entries of T and T , where
the overline denotes the complex conjugate. Hence, we can use Wick’s theorem to compute the expectation value.
Suppose that the tensor network has M diﬀerent tensors T and M(cid:48) diﬀerent tensors T . The expectation value is
nonzero only if M = M(cid:48). Wick’s theorem computes the expectation value by summing over all possible pairings of a
tensor T with a tensor T , so that every T is paired with a unique T ; that is, there are M ! diﬀerent pairings to sum
over. Given a tensor T with d indices, with entries of the tensor written Ti1,...,id , we have

E[Ti1,...,idT j1,...,jd ] = δi1,j1 ...δid,jd .

(II.8)

These δ-functions can be represented graphically as follows. For each pairing, we deﬁne a new graph by removing
every vertex (leaving all edges with both ends open) and then for each pair of vertices v, w which are paired with
each other, for each a ∈ {1, ..., d}, we attach the end of the edge which corresponded to the a-th index of the tensor
at vertex v to the end of the edge which corresponded to the a-th index of the tensor at vertedx w. Then, the tensor
network breaks into a set of disconnected closed loops. See Fig. II.3.

Deﬁnition 5. A closed loop created by a pairing consists of a sequence of edges (v1, w1), (v2, w2), ..., (vl, wl); no
repetition of edges is allowed in a closed loop, but vertices may be repeated. The closed loops created by the pairing

7

FIG. II.3: A possible pairing of the closed tensor network in Fig. II.1. Dotted lines are used to connect edges which end at
vertices which are paired; each vertex in a pair has d = 3 diﬀerent edges: we connected the edges which have the same index for
the local ordering. This pairing has 6 diﬀerent closed loops. There is one other possible pairing for this network; that pairing
would have only 3 closed loops.

are such that wi is paired with vi+1 (identiyﬁng vl+1 with v1) with the local ordering assigning the same index of the
tensor to the edge (vi, wi) at wi as it assigns to edge (vi+1, wi+1) at vi+1. The choice of starting edge in the sequence
is irrelevant as is the direction in which the edges are traversed; i.e., the sequences (v1, w1), (v2, w2), ..., (vl, wl) and
(v2, w2), . . . , (vl, wl), (v1, w1) and (wl, vl), . . . , (w2, v2), (w1, v1) all denote the same closed loop.

Suppose for a given pairing π that the number of closed loops is equal to C(π). Then, the sum over indices on the
edges for the given pairing is N C(π). Thus, for a tensor network Nc with M = M(cid:48), the expectation value is equal to

(cid:88)

E[Nc] =

Note that every term in Eq. (II.9) is positive. So, once we have found that there exists some pairing π0 with some
given C(π0), we have established that E[Nc] ≥ N C(π0). Further, the pairing or pairings with the largest C(π0) give
the dominant contribution in the large N limit. We deﬁne

π pairing T with T

N C(π).

(II.9)

and deﬁne nmax to be the number of distinct pairings π with C(π) = Cmax; then

Cmax = maxπC(π),

and

E[Nc] =

1 − O(1/N )

E[Nc] = Θ(N Cmax).

(cid:16)

(cid:17) · nmaxN Cmax.

(II.10)

(II.11)

(II.12)

III. ESTIMATING FIRST MOMENT

We now estimate E[tr(L†L)].

Lemma 3. Given a tensor network N obtained from a graph G with |V | vertices and |E| edges, with corresponding
linear operator L, for Nc(tr(L†L)) we have

Cmax = |E|,

(III.1)

and

nmax = 1.

(III.2)
Proof. First, we explicitly give a pairing π for which C(π) = |E|. Note that the number of vertices in the graph G(cid:48) for
tensor network Nc(tr(L†L)) equals 2|V |. There are |V | vertices with tensor T and |V | vertices with tensor T . There
is an obvious pairing π of the vertices in G(cid:48) exempliﬁed in Fig II.3 for one particular network. For this pairing π, we
have C(π) = |E|. We can deﬁne this pairing formally for arbitrary G as follows. Suppose G has vertex set V, and
edge set E. Then, G(cid:48) has vertices labelled by a pair (v; σ) where v ∈ V and σ ∈ {1, 2}. If σ = 1 then the vertex has
tensor T , and if σ = 2 then the vertex has tensor T . If (v, w) is an edge in G, then ((v; σ), (w; σ)) is an edge of G(cid:48) for
σ ∈ {1, 2}. Additionally, for every open edge in G there is an edge in G(cid:48); for each open edge attached to a vertex v,
then there is an edge ((v; 1), (v; 2)) in G(cid:48). There are no other edges in G(cid:48) other than those given by these rules. The
ordering of edges attached to vertices in G(cid:48) is deﬁned in the obvious way: if an edge e in G attached to a vertex v

8

corresponds to the j-th index of the tensor, then the edge in G(cid:48) obtained from e in the above rules attached to (v; σ)
also corresponds to the j-th index of the tensor. This pairing π is then the pairing which pairs each vertex (v; 1) with
(v; 2). This pairing gives one closed loop for every edge of G so that C(π) = |E|.
We next show that there is no pairing π for which C(π) > |E|. A closed loop corresponds to a sequence of edges
in G(cid:48) which we write as (v1, w1), (v2, w2), ..., (vl, wl) for some l; we say that such a loop “has l edges”. The pairing
is such that wi is paired with vi+1 for i < l and wl is paired with v1. If l = 1, then v1 has a tensor T and w1 has a
tensor T and so this edge in G(cid:48) is obtained from an open edge in G. If l > 1, then vertices vi and wi for odd i have
tensors T while for even i they have tensors T . So, if l > 1, then l is even.
Let n1 be the number of closed loops in the pairing with l = 1; n1 is bounded by the number of open edges in
G, which is equal to |S| + |T|. The number of closed loops with l > 1 is then bounded by (1/2)(|E(cid:48)| − n1), where
|E(cid:48)| is the number of edges in G(cid:48), since every closed loop with l > 1 must have at least two edges. Note that
|E(cid:48)| = 2|E| − |S| − |T|.

So, the number of closed loops is bounded by

C(π) ≤ n1 + (1/2)(|E(cid:48)| − n1),

(III.3)
which is maximal when n1 is as large as possible, i.e., n1 = |S| + |T|. In this case, the maximum number of closed
loops equals |E|. So, Cmax = |E|.
We now show that nmax = 1. Consider a pairing π with C(π) = |E| so that n1 = |S| + |T|. Thus, for every open
edge in G, there must be a closed loop containing just the edge in G(cid:48) corresponding to that open edge in G. Further,
to have C(π) = |E|, we must have that no loops have more than two edges, as otherwise C(π) < n1 + (1/2)(|E(cid:48)|− n1).
A loop with two edges corresponds to a sequence (v1, w1), (v2, w2) with w1, v2 paired and v1, w2 paired. This then
allows us to show that nmax = 1 as follows. Since for every open edge in G, there is a closed loop of length l = 1
containing just the corresponding edge in G(cid:48), any vertex (v; 1) which is attached to an open edge must be paired with
(v, 2). That is, for all vertices v ∈ G attached to an open edge, we pair (v; 1) and (v; 2). Now consider a vertex w ∈ G
which neighbors some vertex v ∈ G such that we pair (v; 1) and (v; 2). Then, there is some edge ((w; 1), (v; 1)) and
this edge must be in a closed loop with two edges. Since we pair (v; 1) with (v; 2), there must be a loop containing
edges ((w; 1), (v; 1)) and ((v; 2), ((w; 2)). Then, since this loop must have length 2, we must pair (w; 1) with (w; 2).
Let P be the set of vertices v ∈ G such that we pair (v; 1) with (v; 2); we have shown that P contains all vertices
attached to an open edge and P contains all vertices connected to a vertex in P by an edge. So, since the network is
connected, P must contain all vertices and so there is indeed only one such pairing.

IV. LOWER BOUND FOR HIGHER MOMENTS

We now lower bound E[tr(L†LL†L)] and other higher moments. First let G(cid:48) denote the graph for tensor network
Nc(tr(L†LL†L)). Let us formally deﬁne G(cid:48), in a way similar to that in which a diﬀerent graph (also called G(cid:48)) was
deﬁned in lemma 3. Suppose G has vertex set V, and edge set E. Then, G(cid:48) has vertices labelled by a pair (v; σ) where
v ∈ V and σ ∈ {1, 2, 3, 4}. If σ ∈ {1, 3} then the vertex has tensor T , and if σ ∈ {2, 4} then the vertex has tensor T .
If (v, w) is an edge in G, then ((v; σ), (w; σ)) is an edge of G(cid:48) for σ ∈ {1, 2, 3, 4}. Additionally, for every open edge in
G there is are two edges in G(cid:48); for each open edge attached to a vertex v, if the edge is an input edge then there are
edges ((v; 1), (v; 2)) and ((v; 3), (v; 4)) in G(cid:48) while if the edge is an output edge then there are edges ((v; 2), (v; 3)) and
((v; 4), (v; 1)). There are no other edges in G(cid:48) other than those given by these rules. The ordering of edges attached
to vertices in G(cid:48) is deﬁned in the obvious way: if an edge e in G attached to a vertex v corresponds to the j-th index
of the tensor, then the edge in G(cid:48) obtained from e in the above rules attached to (v; σ) also corresponds to the j-th
], we deﬁne a graph G(cid:48) for tensor network
index of the tensor. If we instead consider a higher moment E[tr
) similarly. Now G(cid:48) has vertices labelled by a pair (v; σ) where v ∈ V and σ ∈ {1, 2, . . . , 2k}. If σ
Nc(tr
is odd then the vertex has tensor T , and if σ is even then the vertex has tensor T . If (v, w) is an edge in G, then
((v; σ), (w; σ)) is an edge of G(cid:48) for all σ. Additionally, for every open edge in G there is are k edges in G(cid:48); for each
open edge attached to a vertex v, if the edge is an input edge then there are edges ((v; σ), (v; σ + 1)) for σ odd while
if the edge is an output edge then there are edges ((v; σ), (v; σ + 1)) for σ even. We regard σ as periodic mod 2k, so
that σ = 2k + 1 is the same as σ = 1. The ordering of edges attached to vertices in G(cid:48) is deﬁned in the obvious way:
if an edge e in G attached to a vertex v corresponds to the j-th index of the tensor, then the edge in G(cid:48) obtained
from e in the above rules attached to (v; σ) also corresponds to the j-th index of the tensor.

(L†L)k(cid:17)
(cid:16)

(cid:16)

(L†L)k(cid:17)

Thus, for example, for the closed tensor network in Fig. II.2, the four vertices in the top row correspond to
σ = 1, 2, 3, 4 from left to right (recall that the input of the tensor network is on the left), as do the four vertices in
the bottom row.

We now show the lower bound

9

FIG. IV.1: Example of the pairing deﬁned in lemma 4. The network is the same as in Fig. II.2. The thin curving vertical
lines represent minimum cuts of the network. There are four such lines, each cutting one of the cases σ = 1, 2, 3, 4. The lines
for odd σ are reﬂected in the horizontal direction compared to those for even σ. The vertical dotted lines represent reﬂection
planes: reﬂecting the region between any two neighboring vertical curved lines about one of these vertical dashed lines leaves
the region unchanged except for interchanging tensors T and T (we have drawn one of these lines at the right-hand side of the
ﬁgure, indicating a reﬂection relating vertices at the right-hand side to those at the left-hand side). The pairing in lemma 4
pairs vertices related by such a reﬂection.

.

Lemma 4. Let linear operator L correspond to a tensor network N obtained from a graph G with |V | vertices and
|E| edges. Then, for Nc(tr

) for k ≥ 1 we have

(cid:16)

(L†L)k(cid:17)

Proof. The case k = 1 is already given above. So, assume k > 1.

Suppose the lemma does not hold, so that Cmax < k|E|−(k−1)M C(G) and hence Cmax ≤ k|E|−(k−1)M C(G)−1.

Cmax ≥ k|E| − (k − 1)M C(G).

(IV.1)

Then from lemma 3, for suﬃciently large N we would have

(cid:16)

E[tr(L†L)]k
(L†L)k
E[tr

(cid:17)

]

≥ c · N (k−1)M C(G)+1,

(IV.2)

(cid:16)
(L†L)k(cid:17)

for some positive constant c (the ratio of expectation values would asymptotically tend to 1/nmax, where here nmax
refers to the number of pairings with maximal C(π) for tensor network Nc(tr
). Then, from lemma 2, for some
choice of tensor T0 the corresponding linear operator L0 obeys rank(L0) ≥ c·N M C(G)+1/(k−1), which is asymptotically
larger than QM C(G, N ), contradicting the fact that rank(L) ≤ QM C(G, N ).
In addition to this proof, we give an alternative proof by explicitly giving a pairing π for which C(π) = k|E| −
(k − 1)M C(G). We exemplify this pairing for a particular network in Fig. IV.1. Consider a minimum cut, with
corresponding sets S, T . For v ∈ T , we pair (v; σ) with (v; σ + 1) for odd σ, while for v ∈ S we pair (v; σ) with
(v; σ + 1) for even σ. Then, for every edge (v, w) for vertices v, w ∈ T \ T there are k closed loops, corresponding
to edges ((v; σ), (w; σ)) and ((w; σ + 1), (v; σ + 1)) for odd σ; similarly, for every edge (v, w) for vertices v, w ∈ S \ S
there are k closed loops, corresponding to edges ((v; σ), (w; σ)) and ((w; σ + 1), (v; σ + 1)) for even σ. These edges
(v, w) for v, w ∈ T \ T or v, w ∈ S \ S are not in the cut set. Similarly, for every edge in the output set or in the input
set (assuming that the edge is not in the cut set) there are k closed loops. However, for each edge (v, w) in the cut
set, there is only one closed loop, corresponding to edges ((v; 1), (w; 1)), ((w; 2), (v; 2)), . . . , ((w; 2k), (v; 2k)).
edges in the cut set, which equals k|E| minus k − 1 times the number of edges in the cut set.

Thus, the number of closed loops is equal to k times the number of edges not in the cut set, plus the number of

V. UPPER BOUND FOR HIGHER MOMENTS AND ITS REALIZATION BY “DIRECT PAIRINGS”

The pairing in the lemma 4 has a certain structure, which we now deﬁne (that pairing is not the only pairing

consistent with this structure).
Deﬁnition 6. Consider an arbitrary closed tensor network, Nc, with some vertices having tensor T and some having
tensor T . Suppose that there are two vertices, v, w with v having tensor T and w having tensor T . Further, suppose
that there is at least one edge from v to w which has the property that the local ordering of indices makes that edge

10

FIG. V.1: Example of a direct pairing that does not have maximal C(π). The notation is the same as in Fig. IV.1, except that
the thin curving vertical lines are cuts which are not min cuts.

correspond to the same index for tensor T as it does for tensor T . Then, deﬁne a new closed tensor network, N (cid:48)
c by
removing the vertices v, w from Nc; every edge from v to w is removed, while for every pair of edges (u, v) and (w, x)
for which the local ordering gives the same tensor index at v as at w, we replace that pair with a single edge (u, x),
deﬁning the local ordering in the obvious way, so that the edge (u, x) corresponds to the index of the tensor at u that
(u, v) did and corresponds to the index of the tensor at x that (w, x) did. Then, we say that a network N (cid:48)
c constructed
in this fashion is a “one-step direct subnetwork” of Nc; more speciﬁcally, we call it a “one-step direct subnetwork
made by pairing v, w in N ” to indicate how it is constructed. Any network N (cid:48)
c constructed by zero or more steps of
the above procedure is termed a “direct subnetwork” of Nc.
Given a sequence of direct subnetworks, Nc(1) pairing (v(1), w(1)) in Nc and Nc(2) pairing (v(2), w(2)) in Nc(1),
and so on, we deﬁne a partial pairing of the original network Nc. This partial pairing pairs v(1) with w(1), pairs v(2)
with w(2), and so on. This is a partial pairing as it may pair only a subset of the vertices. Such a partial pairing
is termed a direct partial pairing. If all vertices are paired (so that the last direct subnetwork in the sequence has no
vertices), then this is termed a direct pairing.
For each direct partial pairing there are several possible sequences of direct subnetworks Nc(1), . . ., but the last
network in the sequence is unique, and we say that this is the direct subnetwork determined by that direct partial
pairing.

One way to understand direct pairings is to consider the case that the graph has degree d = 2 and has one input
and one output edge. Then, if the graph has a single vertex, the problem of the singular values of L reduces to a well-
studied problem in random matrix theory, studying the singular values of a random square matrix with independent
entries. This is the so-called chiral Gaussian Unitary Ensemble5,6. In this case, it is well-known that the dominant
diagrams in the large N limit for any moment are the so-called “rainbow diagrams” (these are also called “planar
diagrams”)4. Even if there is one input and one output edge, but more than one vertex (so that we now study the
singular values of a power of a random matrix) the dominant diagrams are still rainbow diagrams. These rainbow
diagrams are precisely the direct pairings in this case. If we still stick to the case d = 2, with |S| = |T| = M C(G),
but allow |M C(G)| > 1, dominant diagrams can still be understood as rainbow diagrams: for each of the M C(G)
distinct paths from input to output, we draw a rainbow diagram, pairing only vertices which are both in the same
path (i.e., given a power k ≥ 1 so that we have k copies of a given path with tensors T and k copies with tensors T ,
we pair vertices between copies of that path, but only within a given path, not between paths). Again, these are the
direct pairings.

Later, we will use this understanding in terms of rainbow diagrams to better understand the case d > 2. Suppose
d > 2 and we have a direct pairing. We can construct M C(G) edge disjoint paths from input to output. We will
show that the direct pairing has the properties that if we consider only the vertices and edges in one of these paths,
the result is a rainbow diagram. Since the edge-disjoint paths might share vertices (if d ≥ 4) this can impose some
relationship between the diﬀerent rainbow diagrams for each paths:
i.e., it is not the case that we can choose a
distinct rainbow diagram for each path independently. Further, while every pairing that gives rainbow diagrams when
restricted to each of these paths will be a direct pairing, not every such direct pairing will having maximal C(π); see
Fig. V.1.
Deﬁnition 7. Consider an arbitrary closed tensor network Nc, and a one-step direct subnetwork N (cid:48)
c made by pairing
c. Then, we say that π(cid:48) induces a partial pairing of Nc which
vertices v, w in Nc. Let π(cid:48) be a partial pairing of N (cid:48)
pairs v with w and pairs all other vertices as they are paired in π(cid:48) (every vertex in Nc other than v, w corresponds to
a vertex in N (cid:48)
c and any pairing π(cid:48) of
N (cid:48)
c we deﬁne a partial pairing on Nc induced by π(cid:48). We write Π(π(cid:48), θ) to denote this partial pairing.
Deﬁnition 8. Given a partial pairing π of a network, we deﬁne C(π) the number of closed loops created by the partial

c). Inductively, for any direct partial pairing θ deﬁning a direct subnetwork N (cid:48)

11

FIG. V.2: Example of paths Q(i) as constructed in lemma 8. The two thickened solid lines each represent such a path. For
this network, the choice of paths P (i) is non-unique.

pairing in the obvious way: it is the number of distinct closed loops, where as in Deﬁnition 5 each closed loop contains
edges (v1, w1), . . . , (vl, wl) such that wi is paired with vi+1 (identiyﬁng vl+1 with v1) with the local ordering assigning
the same index of the tensor to the edge (vi, wi) at wi. Note that for a partial pairing, some edges might not be in a
closed loop.
Lemma 5. Consider direct partial pairing θ determining direct subnetwork N (cid:48)

c and let π(cid:48) be a pairing of N (cid:48)

c. Then

(V.1)
c is a one-step direct subnetwork, C(θ) ≤ NE(v, w), where NE(v, w) is the number of edges
In the special case that N (cid:48)
from v to w, with v, w as in deﬁnition 6; this is an equality if the ordering is such that all of these edges correspond
to the same index at v as they do at w.

C(Π(π(cid:48), θ)) = C(π(cid:48)) + C(θ).

When applying this lemma 5, we will say below that the pairing of vertices v, w “creates NE(v, w) closed loops” or

that the pairing θ “creates C(θ) closed loops”.
Lemma 6. Let π be a pairing of a closed tensor network Nc. Assume that there is no one-step direct subnetwork N (cid:48)
with pairing π(cid:48) such that π is induced by π(cid:48). Then,

c

C(π) ≤ NE(Nc)/2,

(V.2)

(cid:16)

(L†L)k(cid:17)

Further, (*) if a pair of vertices (x; µ) and (y; ν) in N (cid:48)

). Let G be the graph corresponding to this tensor network. Let N (cid:48)

where NE(Nc) is the number of edges in tensor network Nc.
Proof. Every closed loop for pairing π must be composed of at least two edges (if it is composed of one edge, then
pairing those vertices deﬁnes a one-step direct subnetwork).
Lemma 7. Let Nc = Nc(tr
c be a direct
subnetwork of Nc. Let π(cid:48) be the partial pairing deﬁned by N (cid:48)
c consists only of closed loops, then π(cid:48) is a pairing).
Then, labelling the vertices of Nc by pairs (v; σ) with σ ∈ {1, ..., 2k} as above, the partial pairing π only pairs (v; σ)
with (w; τ ) for v = w and σ odd and τ even.
c, then either x = y and
µ (cid:54)= ν mod 2 and the local ordering is such that the edge is assigned the same index of the tensor at (x; µ) as it is at
(y; ν), or µ = ν mod 2 and there is an edge (x, y) in G and the local orderings agree: if the ordering assigns the j-th
index of the tensor to edge (x, y) at x it also assigns the j-th index of the tensor to the edge ((x; µ), (y; ν)) at (x; µ)
and similarly for y and (y; ν).
Proof. The fact that σ; τ have diﬀerent parity mod 2 follows because the odd and even vertices correspond to T †,T
respectively.
is that N (cid:48)
subnetwork, one can check case-by-case that N (cid:48)(cid:48)

The statement (*) in the last paragraph of the claim of the lemma can be established inductively. The base case
c is a one-step direct

c are connected by an edge in N (cid:48)

c = Nc, where (*) follows trivially. If N (cid:48)

c is a direct subnetwork obeying(*) and N (cid:48)(cid:48)

c (if N (cid:48)

c obeys (*).

Once (*) is established, the fact that the pairing only pairs (v; σ) with (w; τ ) for v = w and σ odd and τ even

follows inductively since one only pairs vertices connected by an edge.

Lemma 8. Let Nc = Nc(tr

(cid:16)

(L†L)k(cid:17)

). Then, for any direct pairing π,

C(π) ≤ k|E| − (k − 1)M C(G).

For any pairing π that is not a direct pairing,

C(π) ≤ k|E| − (k − 1)M C(G) − 1.

Finally, for any graph G, nmax is bounded by an exponentially growing function of k.

(V.3)

(V.4)

12

Proof. First consider the case that π is a direct pairing. Consider a maximal ﬂow on the graph G, giving each edge
of the graph capacity 1. By the max-ﬂow/min-cut theorem, the ﬂow is equal to the min cut8,9. Further, the ﬂow on
each edge in a maximal ﬂow can be chosen to be an integer, and hence equals 0 or 1 on every edge. Consider the set
of edges on which the ﬂow equals 1. This set deﬁnes M C(G) edge-disjoint paths from input to output. Call these
paths P (1), P (2), . . . , P (M C(G)).

Let vi,1, vi,2, . . . , vi,l(i) denote the sequence of vertices in P (i), as the path is traversed from input to output,
where l(i) is the total number of vertices in the path P (i). Each path in the graph deﬁnes a closed path Q(i) in
the network: the path is traversed backwards for each network corresponding to L† and forward for each network
corresponding to L so that it forms a closed path traversing kl(i) vertices; i.e., the path Q(i) traverses vertices
(vi,1; 1), (vi,2; 1), . . . , (vi,l(i); 1), (vi,l(i); 2), . . . , (vi,2; 2), (vi,1; 2), . . .. See Fig. V.2 for an example. If P (i) has ei edges in
the graph, including 1 input edge and 1 output edge, then the number of edges Ei in Q(i) is equal to 2k(ei − 1).
By lemma 7, for a direct pairing, for every such path, vertex (vi,b; σ) is paired with (vi,b; σ(cid:48)) for some σ(cid:48). So, π
pairs vertices in Q(i) with other vertices in Q(i). As noted above, in the case of a graph with degree d = 2, a direct
pairing gives a rainbow diagram. Here, if we consider the subgraph containing only vertices in Q(i), we have a graph
with degree 2 and again pairing π deﬁnes a “rainbow diagram”. Hence, the number of closed loops formed by edges
on the path is equal to

Ei + 1 = kei − (k − 1).

1
2

(One can derive this number of closed loops using the known result for rainbow diagrams, or one can also derive it
inductively: given a graph with degree 2 containing 1 closed loop with ei > 2 edges, pairing two vertices v, w gives a
one-step direct subnetwork with 2 fewer edges and creates NE(v, w) = 1 edges between them, while for ei = 2, the
pairing gives two closed loops.) Hence, the total number of closed loops formed in all such paths in the network is
equal to

M C(G)(cid:88)

i=1

k

ei − (k − 1)M C(G).

edges in G which are not in a path P (i), so |P ⊥| = |E| =(cid:80)

Let QE be the set of edges in Nc which are in some path Q(i) and QV be the set of vertices in Nc which are in
some path Q(i). We now count the number of closed loops which do not contain any edges in QE; note that for a
direct pairing, every closed loop either contains only edges from QE or contains no edge in QE. Let P ⊥ be the set of
i ei. Note that S − M C(G) of the edges in P ⊥ are input
edges and T − M C(G) of such edges are output edges so that there are |P ⊥| − (S − M C(G)) − (T − M C(G)) edges
in G which are not in a path P (i) and which are closed edges. Every edge in P ⊥ which is closed corresponds to 2k
edges in the network, while every edge in P ⊥ which is open corresponds to only k edges in the network. The number
of closed loops which can be formed by these edges is at most 1/2 the number of edges which connect T to T (i.e.,
corresponding to the closed edges in G) or T to T (i.e., also corresponding to closed edges), plus the number of edges
connecting T to T (i.e., corresponding to the open edges in G) so that the total number of such closed loops is at
most (1/2)(2k)(|P ⊥| − (S − M C(G)) − (T − M C(G))) + k(S − M C(G)) + (T − M C(G)) = k|P ⊥|. Hence, the total
number of closed loops including edges in QE and edges not in QE is at most

ei + (k − 1)M C(G) + k|P ⊥| = k|E| − (k − 1)M C(G).

(V.5)

(cid:88)

i

Now, suppose that π is not a direct pairing. We show that C(π) ≤ k|E| − (ki − 1)M C(G) − 1. Before giving the
proof, we give some motivation. The basic idea is that if we consider the edges not in QE then no pairing can do
better than the direct pairing: the direct pairing has one loop of length 1 for each edge in QE corresponding to an
output edge in G and has one loop of length 2 for each edge in QE not corresponding to an output edge in G, and that
is the shortest such a loop can be. On the other hand, when we consider the edges in QE, we know that a rainbow
diagram gives the optimal pairing for a network with degree d = 2 and so any other pairing must be worse. To do
this in detail, we will use lemma 6.
The θ be a direct partial pairing of Nc determining direct subnetwork N (cid:48)
c such that π is induced by a pairing π(cid:48) of
N (cid:48)
c (i.e., π = Π(π(cid:48), θ)) and such that there is no one-step direct subnetwork N (cid:48)(cid:48)
c with pairing π(cid:48)(cid:48) such that π(cid:48)
is induced by π(cid:48)(cid:48). By lemma 6, C(π(cid:48)) ≤ NE(N (cid:48)
c)/2 + C(θ). We wish to estimate
C(θ) and to estimate NE(Nc) − NE(N (cid:48)
c). Recall that when we deﬁne a one-step direct subnetwork, the number of
edges changes for two reasons: we remove NE(v, w) edges (and create NE(v, w) loops) but we also combine other
pairs of edges into a single edge.

c)/2, so by lemma 5, C(π) ≤ NE(N (cid:48)

c of N (cid:48)

Let C be the set of these closed loops created by θ. Let CE be the set of edges in a loop in C. Each closed loop
in C contains either only edges in QE or contains no edges in QE. Consider the loops in C containing edges not in

QE. Let Ro be the set of edges in these loops which correspond to output edges in G and let Ri be the set of edges
in these loops which do not correspond to output edges in G so that the number of loops in C containing edges not
in QE is bounded by |Ro| + (1/2)|Ri|. f
Consider the loops in C containing edges in QE. Each such loop contains edges from at most one path Q(i). In
such a path, there are 2k(ei − 1) total edges in Nc. For each path Q(i) we deﬁne a path in N (cid:48)
c in the obvious way,
c which are in the path in Nc. If all edges in Q(i) are in a loop
so that the path in N (cid:48)
in C then there are k(ei − 1) + 1 closed loops in C containing edges in Q(i). On the other hand, if not all edges in
Q(i) are in such a loop, then there are less than k(ei − 1) + 1 closed loops and the number of closed loops is equal to
(1/2)(Ei − E(cid:48)
i is the number of edges in the corresponding path in
N (cid:48)
c. So,

i), where Ei is the number of edges in Q(i) and E(cid:48)

c consists of the vertices in N (cid:48)

NE(Nc) − NE(N (cid:48)

(Ei − E(cid:48)
i).

(cid:88)
c) ≥ |Ro| + |Ri| +
(cid:88)

i

13

(V.6)

(V.7)

Let q be the number of paths Q(i) such that π(cid:48) pairs all vertices in Q(i). So,
(Ei − E(cid:48)

C(θ) = |Ro| + (1/2)|Ri| + (1/2)

i) + q

≤ NE(Nc) − NE(N (cid:48)

c)

2

i

+ |Ro|/2 + q.

So,

(V.8)
The number of open edges in G is |S| + |T| of which |S| + |T| − 2M C(G) edges are not in a path Q(i), so |Ro| ≤
k(|S| + |T| − 2M C(G)). So,

NE(N (cid:48)

c)/2 + C(θ) ≤ NE(Nc)/2 + |Ro|/2 + q.

NE(N (cid:48)

c)/2 + C(θ) ≤ NE(Nc)/2 + k(|S + |T|)/2 − kM C(G) + q

= k|E| − kM C(G) + q.

(V.9)

So, unless q = M C(G), we have established the desired bound on C(π).
So, suppose that q = M C(G) and suppose that C(π) = Cmax. We will show that π is a direct pairing. We will use
the assumption that the network is connected. Since C(π) = Cmax, every edge in Nc which is not in a loop created
by θ must either be in a loop of length 1 (if corresponds to an open edge in G) or to a loop of length 2 (otherwise).
Consider a vertex (v; σ) which is not paired by θ and which is attached to an open edge. Since this edge must be in a
closed loop of length 1, (v; σ) must be paired with σ ± 1 depending on σ mod 2 and on whether it is an input edge or
an output edge. Thus, we can pair (v; σ) with (v; σ ± 1) in N (cid:48)
c such
that a pairing π(cid:48)(cid:48) on N (cid:48)(cid:48)
c induces π(cid:48). So, we can assume that no such vertices exist. Consider instead a vertex (v; σ)
which is not paired by θ and which neighbors a vertex (w; σ) which is paired by θ. The pairing θ is a direct partial
pairing so it pairs (w; σ) with (w; τ ) for some τ . Then, since the edge ((v; σ), (w; σ)) must be in a loop of length 2 in
π, the pairing π must pair (v; σ), (v; τ ). So, pairing these vertices would deﬁne a one-step direct subnetwork N (cid:48)(cid:48)
c . So,
there can be no vertices not paired by θ which are attached to output edges or which neighbor a vertex paired by θ;
so, since the network is connected, all vertices are paired by θ and π is a direct pairing.

c giving a further one-step direct subnetwork N (cid:48)(cid:48)

We now bound nmax to bound the c(G, k). Let π be a direct pairing with C(π) = Cmax. In each path Q(i), the
direct pairing must deﬁne a rainbow diagram. Further, since every edge not in QE is either in a loop of length 1 (if
it is an open edge) or a loop of length 2 (otherwise), the pairing of the vertices in QV fully determines π. So, we
can bound nmax by bounding the number of possible pairings of the vertices in QV . For each path Q(i), the pairings
of the vertices in that path deﬁne some rainbow diagram. If P (i) has l(i) vertices, than Q(i) has 2kl(i) vertices.
There is some restriction on the pairing of these vertices, as one can only pair vertices corresponding to T to those
corresponding to T . Ignoring this restriction to obtain an upper bound, we ask for the number of possible rainbow
diagrams pairing 2kl(i) vertices (this is a problem that arises in estimating the 2kl(i)-th moment in the Gaussian
Orthogonal Ensemble where one consider random real symmetric matrices). The number of such rainbow diagrams
is at most exponential7 in 2kl(i) and so the product over paths Q(i) of the number of such rainbow diagrams for each
i l(i) ≤ 2k|V |, showing the desired result. The number nmax may be less than
the product of these for two reasons: ﬁrst, not all direct pairings have C(π) = Cmax and second, if two paths share a
vertex then the imposes some relation between the pairings on those two paths.

path is at most exponential in 2k(cid:80)

Proof of Theorem 2 By lemmas 4,8, for Nc = Nc(tr

), we have Cmax = k|E|− (k − 1)M C(G). This implies
Eq. (I.4) in theorem 3 with c(G, k) = nmax (we show below that c(G, k) does not depend on O; we will not need that

(cid:16)

(L†L)k(cid:17)

to prove theorem 2). Using Eq. (I.4) for k = 2 to estimate E[tr
we ﬁnd that

(cid:16)
(L†L)k(cid:17)

] and using lemma 3 to estimate E[tr(L†L)],

14

(cid:16)

E[tr(L†L)]2
(L†L)2
E[tr

(cid:17)

≥

]

1

c(G, 2)

N M C(G) − O(1/N ).

(V.10)

(cid:90)

So, by lemma 2, theorem 2 follows.

Proof of Theorem 3 We have shown Eq. (I.4) in theorem 3. To show that Eq. (I.5) holds for the independent
ensemble, note that in that ensemble, the only pairings allowed are those in which we pair (v; σ) with (w; τ ) for
v = w. However, by lemma 7 all direct pairings have that property and by lemma 8 the only pairings π with
C(π) = Cmax are direct pairings. To show that c(G, k) indeed does not depend on O, one can either note that
the possible direct pairings do not depend on O or one can note that in the enbsemble in which tensors are chosen
independently, one can freely change the ordering at any vertex without altering the expectation values. The bound
on the c(G, k) follows follows from the bound on nmax in lemma 8.

VI. PROOF OF THEOREM 1

We now prove theorem 1. Let f (x) be some smooth function deﬁned for 0 ≤ x < ∞ with 0 ≤ f (x) ≤ 1,
f (x) = 0 for x ≥ 2, and f (x) = 1 for 0 ≤ x ≤ 1 and let f (x) decrease monotonically with increasing x. Let

PN () = (cid:82) f (x/)dµN , with  chosen later. We choose f (x) to be smooth so that as N → ∞, PN () converges to
some limit P r(, G) =(cid:82) f (x/)dµ, where we explicitly put the dependence on G in parentheses as we will deal with

this probability for several diﬀerent graphs below. We have

f (x/)dµN ≥ 1 − E[rank(L)]
QM C(G, N )

,

(VI.1)

for all  > 0. So, P r(, G) ≥ lim inf N→∞ 1− E[rank(L)]
then this establishes theorem 1. For a given graph G, let P r(G) denote the limit of P r(, G) as  → 0+.

QM C(G,N ) for all  > 0. So, if we can show that P r(, G) → 0 as  → 0+,

We now prove that P r(G) = 0 using induction on the number of vertices in the graph. The base case, a graph with
no vertices, obviously has P r(G) = 0 since all edges must be identity edges. Otherwise, given a general graph, we
will either ﬁnd a min cut which cuts it into two graphs with fewer vertices and apply the inductive assumption, or, if
no such cut exists, we will prove that P r(G) = 0 by estimating moments.

We need the following:

Lemma 9. Let A be an N1-by-N2 matrix and B be an N2-by-N3 matrix. Assume that A has at least rA singular
values which are greater than or equal to A for some A. Assume that B has at least rB singular values which are
greater than or equal to B for some B. Then, AB has at least rA + rB − N2 singular values which are greater than
or equal to AB.
Proof. Let PB project onto the eigenspace of B†B with eigenvalue greater than or equal to 2
has rank at least rB. We have

B. By assumption, PB

A†B†BA ≥ 2

BA†PBA.

The non-zero eigenvalues of A†PBA are the same as the nonzero eigenvalues of PBAA†PB. Let PA project onto the
eigenspace of AA† with eigenvalue greater than or equal to 2
PBAA†PB ≥ 2

A so that

APBPAPB.

By assumption, PB has rank at least rB. The operator PBPAPB must have at least rA + rB − N2 eigenvalues equal to
1 (this can be shown by using Jordan’s lemma to bring both projectors PA, PB to a block 2-by-2 form). So, PBAA†PB
A and so A†B†BA has at least rA + rB − N2 eigenvalues
has at least rA + rB − N2 eigenvalues greater than or equal to 2
greater than or equal to 2

A2
B.

Consider a tensor network N and corresponding graph G and linear operator L. Let C be a min cut. This cut splits
the tensor network into two tensor networks N1,N2 with corresponding linear operators L1, L2 so that L = L2L1 and
splits G into two graphs, G1, G2. Recall that

(cid:16)

N|E|−M C(G)(cid:17)−1/2

L

K =

15

FIG. VI.1: A min cut of the network shown in Fig. I.1. Thin curving vertical line shows the cut.

and similarly

(cid:16)

N|E1|−M C(G1)(cid:17)−1/2

K1 =

L1, K2 =

(cid:16)

N|E2|−M C(G2)(cid:17)−1/2

L2.

Since M C(G) = M C(G1) = M C(G2) and |E1| + |E2| = |E| + M C(G) (this holds because the M C(G) edges in the
cut set become both input edges of G2 and output edges of G1), K = K1K2. If both G1, G2 have at least 1 vertex,
then G1, G2 both have fewer vertices than G and so by the inductive assumption, P r(G1) = P r(G2) = 0.

Lemma 10. In this case (i.e., where P r(G1) = P r(G2) = 0 and where the cut splitting G into G1, G2 is a min cut),
P r(G) = 0.

Proof. Since P r(G1) = 0, for any δ > 0, there is an 1 > 0 such that P r(1, G1) ≤ δ/2. Since(cid:82) f (1)dµN converges
to P r(1, G), there is an N0 such that for all N ≥ N0, the diﬀerence |(cid:82) f (1)dµN − P r(1, G)| is bounded by δ/2. So,

there is an N0 such that for all N ≥ N0, the probability that a randomly chosen singular value of L1 for a random
choice of tensors will be smaller than 1 is bounded by δ. So, the probability that, for a random choice of tensors in
N1, the operator L1 will have more than
δ for all
suﬃciently large N .
Since P r(G2) = 0 also, there is an 2 > 0 such that the probability that, for a random choice of tensors in N2, the
δ for all suﬃciently

δQM C(G, N ) singular values smaller than 2 is bounded by
√
So, with probability at least 1 − 2
has at least (1 − √
√
δ)QM C(G, N ) singular values larger than 1 and L2
δ)QM C(G, N ) singular values larger than 2. So by lemma 9, with probability at least 1 − 2
√
δ,
L has at least (1 − 2
δ)QM C(G, N ) singular values larger than 12. Since for any δ > 0 such 1, 2 > 0 exist, the
result follows.

operator L2 will have more than
large N .

√

√

δQM C(G1, N ) singular values smaller than 1 is bounded by

√

√

δ, L1 has at least (1 − √

Note that when we cut a graph, the new graph might have some open edges which connect to no vertices inside the
graph. Such edges are open edges with one open end in S and one in T . For example, the graph in Fig. I.1 can be
cut into two graphs, as shown in Fig. VI.1. We call these edges “identity edge”, because the linear operator for such
a graph is the identity operator (on the degree of freedom on that edge) tensored with the linear operator for the rest
of the graph. Such identity edges count as a single edge when determing |E|, and Eq. (I.4) still holds for a network
with identity edges; one can verify this by noting that adding an identity edge multiplies the trace of any moment of
L†L by N and it increases both M C(G) and |E| by 1, leaving |E| − M C(G) unchanged.
Suppose instead that G has no min cuts other than possibly the cuts S = S, T = V \ S or S = V \ T, T = T ;
that is, the only min cuts will cut G into two graphs, one of which has no vertices. We consider three cases: (i)
|S| = M C(G) < |T|; (ii) |T| = M C(G) < |S|; (iii) |S| = |T| = M C(G). Case (i) and (ii) can be related by
interchanging L and L† so we only do cases (i,iiii) and prove in both cases that P r(G) = 0.

For both cases, we will use a method of deﬁning a new network by removing vertices from a network.

Deﬁnition 9. Consider a tensor network N . We deﬁne a new network by “removing a vertex v as input from the
network” as follows. The vertex v is removed. The input edges previously attached to v are also removed. The edges
attached to v which were not input edges are not removed; if they were not open edges, then they become input edges
and the end attached to v becomes the open end of the edge. If they were output edges, then they become identity edges.
Similarly, we deﬁne “removing a vertex v as output from the network”; this deﬁnition is the same as above except

“input” and “output” are interchanged everywhere.

In case (i), we now show that the constant c(G, k) in Eq. (I.4) is equal to 1. Thus, in this case, the limiting

distribution µ is a δ-function at 1 and so P r(G) = 0.

We ﬁrst need the following lemma which we will also use in case (iii). Remark: even though we are proving a
combinatoric result (the value of a certain constant counting pairings), we do this in a linear algebraic fashion, by
estimating the trace of a certain product of operators.

FIG. VI.2: An example network for tr(
one does not pair (v; σ) with (v; σ − 1) for any odd σ, necessarily the pairing is the same as that constructed in lemma 4.

), for a graph with |S| = M C(G) = 1 and |T| = 4. For a direct pairing for which

L†L

(cid:16)

(cid:17)2

16

Lemma 11. Let N (1),N (2), . . . ,N (l) be tensor networks with open edges, with N (i) having the same number of
output edges as N (i + 1) has input edges (identifying Nc(l + 1) with Nc(1)). Let L1, L2, . . . , Ll be the corresponding
linear operators and let G1, . . . be the corresponding graphs, with edge sets E(1), . . .. Consider the tensor network Nc
which computes the trace tr(LlLl−1 . . . L1). This tensor network has

Cmax ≤ l(cid:88)

(cid:16)|E(l)| − M C(G(l))

(cid:17)

i=1

+ miniM C(G(i)).

(VI.2)

(cid:80)

(cid:80)

Proof. By corollary 2, for all n, i > 0, for all suﬃciently large N , there is a ci such that the probability that the
largest singular value of L(i) is ≥ xN|E(i)|−M C(G(i))+i is bounded by cix−n.

(cid:80)
i(|E(i)|−M C(G(i)))+ is bounded by cz−m.

The largest singular value of LlLl−1 . . . L1 is bounded by the product over i of the largest singular values of
Li, so for any m there is a constant c such that the probability that the largest singular value of LlLl−1 . . . L1 is
≥ zN
i(|E(i)|−M C(G(i))+i) is bounded by cz−m. (To show this bound, one must pick each n suﬃciently large above)
Choosing i = /l, it follows that for all  > 0, for all m, there is a constant c such that the probability the largest
singular value of LlLl−1 . . . L1 is ≥ zN
is ≥ zN
E[tr(LlLl−1 . . . L1)] is O(N

The operator LlLl−1 . . . L1 has rank at most miniM C(G(i)) so the probability that the trace tr(LlLl−1 . . . L1)
(cid:80)
i(|E(i)|−M C(G(i)))+miniM C(G(i))+ is bounded by cz−m. Choosing m suﬃciently large, this implies that
i(|E(i)|−M C(G(i)))+miniM C(G(i))+) for all  > 0. Choosing  < 1, this implies the result.
(cid:17)k

Lemma 12. For a graph G in case (i), c(G, k) = 1.
Proof. Let Nc = Nc(tr(
). We show that nmax = 1 for all k. We only need to consider the case k > 1 (for
k = 1, lemma 3 shows nmax = 1). Lemma 4 constructs a direct pairing π with C(π) = Cmax = k|E| − (k − 1)M C(G).
There is only one min cut of the graph, so this lemma constructs only one such direct pairing. Let π(cid:48) be a direct
pairing diﬀerent from that π. Since π(cid:48) (cid:54)= π, there must be some vertex (v; σ) for odd σ which is attached to an input
edge such that we pair (v; σ) with (v; σ − 1). For example, see Fig. VI.2.
Consider the sum over all pairings π(cid:48)(cid:48) which pair (v; σ) with (v; σ − 1), weighted by N C(π(cid:48)(cid:48)). This is the expectation
value of the one-step direct subnetwork made by pairing (v; σ) with (v; σ − 1). Let L be the linear operator deﬁned
above, and let M be the linear operator corresponding to the network with vertex v removed as input.

L†L

(cid:16)

Then, the contraction of this one-step direct subnetwork is equal to tr(

. The graph GM corre-
sponding to M has M C(GM ) ≥ M C(G)+1 (if not, one has a min cut of G with partition V = S∪T with S = S∪{v}).
Let there be NE edges connecting (v; σ) with (v; σ − 1) so that GM has |E| − NE edges. Hence by lemma 11, the
c has Cmax ≤ k|E| − (k − 1)M C(G) − NE − 1 and so, all pairings π(cid:48) of Nc which pair (v; σ) with (v; σ − 1)
network N (cid:48)
have C(π(cid:48)) ≤ k|E| − (k − 1)M C(G) − 1.

(cid:16)

L†L)k−1M†M

(cid:17)

In case (iii), we show that the constant c(G, k) in Eq. (I.4) is independent of the particular graph chosen so long
as the graph has at least 1 vertex.
In particular, the constant is the same as that if we consider a graph with
|S| = |T| = M C(G) = 1, with one vertex of degree 2 and |E| = 2, with both edges open. This case is the well-
known case of random matrix theory studying the eigenvalues of L†L where L is a random N -by-N matrix with
independent Gaussian entries; the matrix L is drawn from the chiral Gaussian unitary ensemble. In this case, the
limiting distribution is known5 and is smooth near 0 so again P r(G) = 0.

(cid:16)

(L†L)k(cid:17)

Lemma 13. For a graph G in case (iii), c(G, k) is independent of G so long as G has at least 1 vertex.
Proof. Let Nc = Nc(tr
). Consider the sum of all pairings π in which for some odd σ, for some vertex v
attached to an input edge, (v; σ) is paired with (v; σ − 1) and for some vertex w attached to an output edge (w; σ)
is paired with (w; σ + 1). This is the expectation value of the direct subnetwork N (cid:48)
c made by pairing (v; σ) with

17

FIG. VI.3: A network with |S| = |T| = M C(G) = 2 and with no min cuts other than those which divide G into two graphs,
one of which has no vertices. Input edges are on left and output edges are on right.

(v; σ − 1) and (w; σ) with (w; σ + 1). Let L be the linear operator deﬁned above, and let M1 be the linear operator
corresponding to the network with vertex v removed as output and let M2 be the linear operator corresponding to the
network with vertices v removed as input and w removed as output and let M3 be the linear operator corresponding
to the network with vertex w removed as input. Then, the contraction of N (cid:48)
. Let
GM1 , GM2, GM3 be the graphs corresponding to M1, M2, M3, respectively. We have M C(GM2) ≥ M C(G) + 1 (if not,
then there is a set of M C(G) edges that one can remove from GM2 to disconnect the graph; removing this same set of
edges from G will disconnect G giving a min cut which cuts G into two graphs, each of which has at least one vertex).
Let there be NE(v) edges connecting (v; σ) with (v; σ − 1) and NE(w) edges connecting (v; σ) with (v; σ + 1),
so GM1 has |E| − NE(v) edges, GM2 has |E| − NE(v) − NE(w) edges, and GM3 has |E| − NE(w) edges. Hence,
c has Cmax ≤ k|E| − (k − 1)M C(G) − NE(v) − NE(w) − 1, and so all such pairings π of Nc have
by lemma 11, N (cid:48)
C(π) ≤ k|E| − (k − 1)M C(G) − 1.

L†L)k−2M

c equals tr(

†
1 M2M

(cid:16)

(cid:17)

†
3 L

The above proof was for σ odd, but it works with σ even also, if one takes the adjoint of all linear operators

So, for a maximal pairing, there is no σ such that for some vertex v attached to an input edge, (v; σ) is paired with

L, M1, M2, M3.
(v; σ − 1) and for some vertex w attached to an output edge (w; σ) is paired with (w; σ + 1).
For a direct pairing, there must be some σ such that for some vertex v attached to an input edge, (v; σ) is paired
with (v; σ − 1) or for some vertex w attached to an output edge (w; σ) is paired with (w; σ + 1). So, for that σ, either
for all v, (v; σ) is paired with (v; σ − 1) are for all all v, (v; σ) is paired with (v; σ + 1). Suppose, without loss of
generality, the ﬁrst case holds, so that for all v, (v; σ) is paired with (v; σ − 1) for that σ. Deﬁne a direct subnetwork
of Nc by pairing such vertices v in that way; this direct subnetwork is precisely the network Nc(tr(
), so
one can apply the result above and and for some σ, either for all v, (v; σ) is paired with (v; σ − 1) are for all all v,
(v; σ) is paired with (v; σ + 1). Iterating this k times, one is left with a network with no vertices. The possible choices
(which σ and whether one pairs with σ − 1 or σ + 1) are in one-to-one corresponding with rainbow diagrams and do
not depend upon G.

(cid:17)k−1

L†L

(cid:16)

To better understand this case |S| = |T| = M C(G), consider the network shown in Fig. VI.3 as an example of such
a network. Imagine generalizing the networks that we consider, so that the 4 open edges have capacity N but the
vertical edge in the ﬁgure has some capacity N(cid:48) ≤ N . In the case N(cid:48) = 1, the tensor network factors into a product
of two tensor networks; the linear operator L factors as L = L1 ⊗ L2, where L1 maps the upper input edge to the
upper output edge and L2 maps the lower input edge to the lower output edge (upper and lower refer to the position
of the edge in the ﬁgure). Then, the singular value spectrum of L is the product of the singular value spectrums of
L1, L2 and the limiting distribution of the singular value spectrum of L is not the same as in the chiral Gaussian
unitary ensemble. (The case with N(cid:48) = 1 is equivalent to a case with degree d = 2 but where there is a min cut which
splits the graph into two networks each with only 1 vertex; i.e., simply redraw the graph by removing the vertical
edge with N = 1.) For general N(cid:48), the entries of L can be obtained by summing N(cid:48) independent matrices of the form
L1 ⊗ L2. As N(cid:48) increases, the correlations between entries of L are reduced until eventually when N(cid:48) is large enough,
the singular value spectrum of L is the same as in the chiral Gaussian unitary ensemble.

VII. VARIANCE

Here we collect some results on variance of moments.
The ﬁrst result bounds the variance

Lemma 14. For any linear operator L obtained from a tensor network Nc with graph G with |E| edges, for any

18

(VII.1)

positive integer c

and

E[tr

(L†L)k(cid:17)c
(cid:16)
(L†L)k(cid:17)c
(cid:16)

(cid:16)

] − E[tr

(L†L)k(cid:17)
(L†L)k(cid:17)
(cid:16)

Eind[tr

] − Eind[tr

]c = O(N 2(k|E|−(ki−1)M C(G))−1,

]c = O(N c(k|E|−(ki−1)M C(G))−1,

(cid:16)

(L†L)k(cid:17)c

(cid:16)

(VII.2)

(M†M )k(cid:17)

Proof. Let M = L ⊗ L . . ., with a total of c copies of L in the tensor product so that tr
.
Then, M is the linear operator associated to the tensor network N (cid:48)
c which is the product Nc · Nc · . . . and has graph
G(cid:48) with min cut M C(G(cid:48)) = cM C(G). We can label vertices in G(cid:48) by a pair {v; d} for d = 1, 2, . . . , c where d labelto
the two diﬀerent tensor factors in M (we use diﬀerent notation {v; f} to distinguish this from the notation (v; σ)
used above). Then, vertices in N (cid:48)
c are labelled by triples ({v; d}, σ). We can use lemma 8 to show that the pairings
of N (cid:48)
c only pairs ({v; d}, σ) with
({v; d}, τ ), so that the direct pairings of N (cid:48)
c are in one-to-one correspondance to the product of the direct pairings of
Nc with itself.

c with maximal number of closed loops are direct pairings. Every direct pairing of N (cid:48)

= tr

This has the corollary using the case c = 2:

Corollary 3. With high probability, for either choice of ensemble, av

(cid:16)

(K†K)k(cid:17)

is within o(1) of its expectation value.

To study expectation values of other products, we deﬁne a “product of pairings”.

Deﬁnition 10. Let Nc(1),Nc(2), . . . ,Nc(a) be a sequence of closed tensor networks, a ≥ 1. Let Nc = Nc(1)·. . .·Nc(a)
be the product of these networks and label each vertex in the product network by a pair [v; i], for 1 ≤ i ≤ a, where
v labels a vertex in tensor network Nc(i). Let π(1), π(2), . . . , π(n) be pairings of these networks. Then, deﬁne the
pairing π which is a product of these pairings to be the pairing which pairs [v; i] with [w; i] is π(i) pairs v with w.

Note that C(π) =(cid:80)

i C(π(i)).

Then, once we lower bound a trace, we have an immediate lower bound for products of traces:

Lemma 15. Let Nc(1),Nc(2), ...Nc(a) be a sequence of closed tensor networks, a ≥ 1. Let Cmax(i) be the maximum
number of loops in a pairing of Nc(i) and let nmax(i) be the number of distinct pairings π of Nc(i) with C(π) = Cmax(i).
Then, for the network Nc deﬁned to be the product network Nc(1) · Nc(2) · . . . · Nc(a) we have

and if Cmax =(cid:80)

i Cmax(i) then

Further,

Cmax ≥(cid:88)
nmax ≥(cid:89)

i

Cmax(i),

nmax(i).

i

E[Nc] ≥ E[Nc(1)] · E[Nc(2)] · . . . · E[Nc(a)],

(VII.3)

(VII.4)

(VII.5)

where E[Nc] denotes the expectation value of the contraction of the network.
Proof. Let πi(j), for 1 ≤ j ≤ nmax(i) label the pairings of Nc(i) with C(π) = Cmax(i). Then, for each sequence
j1, . . . , ja, consider the product pairing of Nc. This shows Eqs. (VII.3,VII.4). To show Eq. (VII.5), note that each
expectation value E[Nc(i)] is a sum over pairings of that network, weighted by the number of closed loops; the sum
over product pairings of Nc, weighted by the number of closed loops, is equal to E[Nc(1)] · . . . · E[Nc(a)].

We now show:

(cid:16)

(cid:17)

(cid:16)

(cid:17)

Lemma 16. The expectation value of any product of traces is close to the product of the averages:

E[tr

(L†L)k1

(L†L)k2

tr

. . .] = (1 + O(1/N )) · E[tr

(L†L)k1

]E[tr

(L†L)k2

] . . .

(VII.6)

(cid:16)

(cid:17)

(cid:16)

(cid:17)

(cid:16)

Proof. By lemma 15, E[tr

(cid:17)

(cid:16)

tr

(L†L)k2

(cid:17)

(L†L)k1

] . . . ≥ (1 − O(1/N )) · E[tr

(cid:16)

(L†L)k1

(cid:17)

Consider the case of a product of only two traces. Then, by Cauchy-Schwarz, E[tr

need to upper bound the left-hand side of Eq. (VII.6).

(cid:114)

(cid:16)

(cid:17)2

(cid:16)

E[tr

(L†L)k1

]E[tr

(L†L)k2

]. By lemma 14, and Eq. (I.4),

(cid:16)
(cid:16)

(cid:17)
(cid:17)

(cid:16)

]E[tr

(L†L)k2

] . . .. So, we just

(L†L)k1

(L†L)k2

tr

] ≤

19

(cid:17)

(cid:17)2
(cid:114)
(cid:17)

(cid:16)

(cid:16)

(cid:16)

(cid:17)

(cid:17)

(cid:16)

(cid:17)2

E[tr

(L†L)k

] ≤ (1 + O(1/N )) · E[tr

(cid:17)

(cid:17)

(cid:17)

(cid:114)

(cid:16)

(cid:16)

(cid:16)
(cid:114)

(cid:17)2

(cid:17)2

(cid:16)

(L†L)k(cid:17)
(cid:17)

].

(cid:16)

(cid:17)

].

(L†L)k2

(VII.7)

(VII.8)

(cid:16)

(cid:17)2

(cid:16)

. . . tr

(L†L)ka

(cid:17)2

(L†L)k2

].

(VII.9)

(cid:16)

(cid:17)4
(cid:16)

(cid:16)
(cid:17)

]E[tr

(L†L)k2

] . . .

(cid:17)8

]1/8 . . .

(VII.10)

(VII.11)

(L†L)k1

(L†L)k2

] ≤ (1 + O(1/N )) · E[tr

(L†L)k1

tr

]E[tr

So, the claim follows for the case of two traces.

Now, consider a case with a traces for some a > 2. By Cauchy-Schwarz,

E[tr

(L†L)k1

(L†L)k2

tr

. . . tr

(L†L)ka

] ≤

E[tr

(L†L)k1

]

E[tr

So,

(cid:16)

E[tr

(cid:16)

(cid:16)

(cid:17)

(cid:16)

(cid:17)

(cid:16)

Applying Cauchy-Schwarz again to the second term in this product on the right-hand side, and continuing in this
fashion, we ﬁnd that

E[tr

(L†L)k1

(L†L)k2

tr

. . . tr

(L†L)ka

] ≤ . . . E[tr

(L†L)k1

]1/2E[tr

(L†L)k2

]1/4E[tr

(L†L)k3

By lemma 14, and Eq. (I.4),
(L†L)k1

E[tr

(cid:16)

(cid:17)

(cid:16)

tr

(L†L)k2

(cid:17)

. . .] ≤ (1 + O(1/N )) · E[tr

(cid:16)

(cid:17)

(L†L)k1

VIII. NUMERICS

We performed a numerical study of the network shown in Fig. VIII.1. This network was used in Ref. 2 as an
example of a network for which QM F < QM C for N = 2. Since this network has |S| = |T| = M C(G) = 2 and there
is no min cut that cuts the graph into two graphs which each have at least one vertex, the results above predict that
N|E|−M C = N 3, the same as that for
for large N , the limiting distribution of singular values is, up to dividing by
random matrix theory in the chiral Gaussian unitary ensemble. In Fig. VIII.2 we show the singular values both for a
randomly chosen example of this ensemble for N = 20 and for a randomly chosen chiral Gaussian unitary matrix of
size N 2-by-N 2. In Fig. VIII.3, we show the same for N = 40. One can see the convergence.
We have also considered the smallest singular values to obtain numerical evidence for the rank of L. Studying a
few random samples for each N with 2 ≤ N ≤ 40, we found that there was exactly one very small singular value
(smaller than 10−14) for N = 2, 3 mod 4 and no very small singular values otherwise. In both cases, all the other
singular values were larger than 6 ∗ 10−4. While this is only numerical evidence, it supports a conjecture that for this
network, QM F (G, N, O) = QM C(G, N ) − 1 if N = 2, 3 mod 4 and QM F (G, N, O) = QM C(G, N ) otherwise.

√

The quantum max-ﬂow/min-cut conjecture of Ref. 1 was shown to be false in Ref. 2. If the conjecture above for
the network shown in Fig. VIII.1 is true, then it is not even the case that “for all G, O, for all suﬃciently large N ,
QM F (G, N, O) = QM C(G, N )”. We may still hope for the weaker conjecture that “for all G, O, for inﬁnitely many
N , QM F (G, N, O) = QM C(G, N )”. Indeed, to show this weaker conjecture, it suﬃces to show that “for all G, O, for
0 ) for all integer k ≥ 0 due
some N0 > 1, QM F (G, N0, O) = QM C(G, N0)” as then QM F (G, N k
to the following:
Lemma 17. For all G, QM F (G, N1N2, O) ≥ QM F (G, N1, O)QM F (G, N2, O).
Proof. Let T1,T2 be tensors whose indices range from 1 to N1 or 1 to N2 respectively, such that the network with
tensor T1 gives a linear operator L1 with rank QM F (G, N1, O) and with tensor T2 gives a linear operator L2 with
rank QM F (G, N2, O). Then, T = T1 ⊗ T2 is a tensor whose indices range from 1 to N1N2 and gives a linear operator
L = L1 ⊗ L2 with rank QM F (G, N1, O)QM F (G, N2, O). Here, the product T1 ⊗ T2 is deﬁned as follows:
label
the each index of T by a pair (i, j). Write the entries of T as T(i1,j1),(i2,j2),...,(id,jd). Set T(i1,j1),(i2,j2),...,(id,jd) =
(T1)i1,i2,...,id (T2)j1,j2,...,jd .

0 , O) = QM C(G, N k

20

FIG. VIII.1: A tensor network from Ref. 2 that we studied numerically. Numbers indicate local ordering.

FIG. VIII.2: Plot of the value of the singular values (divided by N 3) for one sample of the tensor network shown in Fig. VIII.1
for N = 20 shown in blue and for a chiral Gaussian unitary matrix of size N 2-by-N 2 shown in green. The largest singular value
shown on the blue curve is larger than the largest on the green curve.

1 D. Calegari, M. Freedman, and K. Walker, “Positivity of the universal pairing in 3 dimensions”, Jour. Amer. Math. Soc. 23,

no. 1, 107-188 (2010).

1
2k

the

condition (cid:80)∞

a

2 S. X. Cui, M. H. Freedman, O. Sattath, R. Stong, and G. Minton, “Quantum Max-ﬂow/Min-cut”, arXiv:1508.04644.
3 For

distributions where

sequence

c(G, k)

to

of

k=1 c(G, 2k)

= ∞,

the moments
distributions

converge
a
converge weakly

obeying Carleman’s
See
http://mathoverﬂow.net/questions/230794/converging-to-moments-obeying-carlemans-condition . Sketch: since the second
moment is bounded, the sequence is tight; by Prokhorov’s theorem, there is a subsequence which converges to a limit µ;
the moments of the limiting distribution µ are the limits of the moments (to show that µ has the correct k-th moment,
use a bound on a higher moment to establish uniform integrability) so by Carleman’s condition, all subsequences which
converge must converge to the same limit; so by Prokhorov’s theorem, the sequence converges without needing to pass to
a subsequence. In this particular case, since the 2k-th moments of µind
2 for some constants
N (x) has a limit as N → ∞ for bounded Lipschitz functions f (x), approximate
f (x) on an interval [−c, +c] by a polynomial p(x) for any c > c2 and use a bound on higher moments to bound the integral
|x|>c p(x)dµind

c1, c2, it is simpler. To show that(cid:82) f (x)dµind
(cid:82)

N (or µN ) are bounded by c1 · ck

limit
to

a

limiting

distribution.

N (x).

4 “Rainbow diagrams” for random matrix theory are another term for “planar diagrams”. This is a class of diagrams that
appear in computing expectation values of traces of powers of a random matrix. See, for example, A. Zee, Quantum Field

1223113123320501001502002503003504000.00.51.01.52.02.521

FIG. VIII.3: Plot of the value of the singular values (divided by N 3) for one sample of the tensor network shown in Fig. VIII.1
for N = 40 shown in blue and for a chiral Gaussian unitary matrix of size N 2-by-N 2 shown in green. The largest singular value
shown on the blue curve is slightly larger than the largest on the green curve but the discrepancy is smaller than in the case
N = 20.

Theory in a Nutshell, Second Edition, pp 396-400, Princeton University Press (Princeton, NJ 2010).

5 E.V. Shuryak and J.J.M. Verbaarschot, Nucl. Phys. A 560, 306 (1993).
6 J.J.M. Verbaarschot, Phys. Rev. Lett. 72, 2531 (1994); Phys. Lett. B 329, 351 (1994).
7 Since the limiting distribution in the Gaussian orthogonal ensemble is the Wigner semi-circle, which is bounded, one can
deduce that the expected trace of the k-th moment of a matrix drawn from this ensemble is at most exponential in k. See also
E. Brezin, C. Itzykson, G. Parisi, and J. B. Zuber, “Planar Diagrams”, Commun. Math. Phys. 59, 35 (1978). Alternatively,
one can reduce the problem of counting rainbow diagrams in this case to the problem of counting parenthesizations, which
are bounded by an exponential.

8 P. Elias, A. Feinstein, and C. E Shannon, “A note on the maximum ﬂow through a network”, Information Theory, IRE

Transactions on, 2(4),117 119 (1956).

9 L. R Ford and D. R Fulkerson, “Maximal ﬂow through a network”, Canadian journal of Mathematics, 8(3), 399404 (1956).

020040060080010001200140016000.00.51.01.52.02.5