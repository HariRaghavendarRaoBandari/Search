6
1
0
2

 
r
a

M
3

 

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
4
1
2
1
0

.

3
0
6
1
:
v
i
X
r
a

Network modularity in the presence of covariates

Beate Franke1

Patrick J. Wolfe1,2

1Department of Statistical Science, University College London
2Department of Computer Science, University College London

Abstract

We characterize the large-sample properties of network modularity in
the presence of covariates, under a natural and ﬂexible nonparametric null
model. This provides for the ﬁrst time an objective measure of whether
or not a particular value of modularity is meaningful. In particular, our
results quantify the strength of the relation between observed community
structure and the interactions in a network. Our technical contribution
is to provide limit theorems for modularity when a community assign-
ment is given by nodal features or covariates. These theorems hold for
a broad class of network models over a range of sparsity regimes, as well
as weighted, multi-edge, and power-law networks. This allows us to as-
sign p-values to observed community structure, which we validate using
several benchmark examples in the literature. We conclude by applying
this methodology to investigate a multi-edge network of corporate email
interactions.

Key words: central limit theorems, degree-based network models, net-
work community structure, nonparametric statistics, statistical network
analysis

A fundamental challenge in modern science is to understand and explain net-
work structure: in particular, the tendency of nodes in a network to connect in
communities based on shared characteristics or function. Scientists inevitably
observe not only network nodes and their connections, but also additional in-
formation in the form of covariates. Most analysis methods fail to exploit this
information when attempting to explain network structure, and instead assign
communities based solely on the network itself. This leads to a loss of inter-
pretability and presents a barrier to understanding. We solve this problem, by
showing how to decide whether communities deﬁned by covariates lead to a
valid summary of network structure. In the student friendship network shown
in Fig. 1, for example, this means we can evaluate whether communities based
on common gender, race, or year in school can explain the observed structure
of the friendships.

The strength of community structure in networks is most often measured
by modularity [1], which is intuitive and practically eﬀective but until now has
lacked a sound theoretical basis. We derive modularity from ﬁrst principles,

1

give it a formal statistical interpretation, and show why it works in practice.
Moreover, by acknowledging that diﬀerent community assignments may explain
diﬀerent aspects of a network’s observed structure, we extend the applicability of
modularity beyond its typical use to ﬁnd a single “best” community assignment.
We use covariates to deﬁne community assignments, and then prove that
modularity quantiﬁes how well these covariates explain network structure. We
show a fundamental limit theorem for modularity in this context: in the pres-
ence of covariates, it behaves like a Normal random variable for large networks
whenever there is a lack of community structure. This allows us to translate
modularity into a probability (a p-value), enabling for the ﬁrst time its use to
draw defensible, repeatable conclusions from network analysis.

Our main technical contribution is a ﬂexible, nonparametric approach to
quantify the strength of observed community structure. Most work assumes a
single unobserved or latent community assignment (e.g., stochastic block models
[2] and latent space models [3]). Hoﬀ et al. [3] and Zhang et al. [4] both estimate
latent community structure, while adjusting for the varying eﬀects of covariates.
Fosdick and Hoﬀ [5] simultaneously model covariates and latent structure, pro-
viding a test for independence. In contrast, we derive limit theorems to evaluate
observed community structure implied by the covariates themselves.

The existing statistical literature on modularity has focused on more basic
parametric approaches. For example, the authors of [6] and [7] model all edges as
equally likely Bernoulli random variables. In contrast, we take a nonparametric
approach: using a single parameter per node, we model only the expectation
of each edge [8]. This allows for individual node-speciﬁc diﬀerences but avoids
speciﬁc distributional assumptions on the edges. Our results apply to a broad
class of network models, allowing us to treat (among others) power-law networks,
weighted networks, and those with multiple edges.

1 Network modularity in the presence of covari-

ates

Two essential ingredients are necessary to understand modularity in the pres-
ence of covariates: ﬁrst, a framework to allow for a formal interpretation of
modularity as a measure of statistical signiﬁcance; and second, the use of this
framework to evaluate a covariate-based community assignment. We now de-
scribe each of these ingredients in turn.
First, to interpret modularity as a measure of statistical signiﬁcance, we
must recognize it as an estimator of a population quantity. Let g(·) denote an
assignment of nodes into groups (i.e., communities), and write δg(i)=g(j) = 1
when nodes i and j are assigned to the same group, and 0 otherwise. Denote by
Aij the strength of an edge (e.g., a count or a weight) between nodes i and j,
j(cid:54)=i Aij the degree of the ith node. Then, modularity as deﬁned

and by di =(cid:80)

2

(a) Race

(b) Year in school

(c) Gender

(d) Randomized

Figure 1: A student friendship network illustrated for four diﬀerent community
assignments, each deﬁned by a covariate [5, 9].

in [1] is

n(cid:88)

(cid:20)
(cid:88)
Aij − didj(cid:80)n

(cid:98)Q =

(cid:21)

Modularity contrasts an observed edge Aij with the ratio didj/(cid:80)
ever nodes i and j are in the same community. Now consider replacing didj/(cid:80)

by E Aij, the expected value of an edge under a given model:

l=1 dl

l dl when-
l dl

j=1

i<j

δg(i)=g(j).

(1)

n(cid:88)

(cid:88)

Q =

[Aij − E Aij]δg(i)=g(j).

(2)

j=1

i<j

We recognize Q in Eq. (2) as a sum of signed residuals (observed minus ex-
pected values) Aij − E Aij. If the model for each E Aij posits the absence of
community structure, then a large positive value of Q indicates the presence of
such structure (more within-group edges than expected). Figure 1 illustrates
this eﬀect: the visible community structure in Figs. 1a–c is obscured in Fig. 1d
l dl as a

when communities are assigned at random. Moreover, using didj/(cid:80)
proxy for E Aij, we see that modularity (cid:98)Q as deﬁned in Eq. (1) is an estimator

of Q in Eq. (2). We will return to this point in the next section.

Second, to interpret covariate-based community structure, we must recognize
that diﬀerent community assignments reveal diﬀerent structural aspects of a
network. Figures 1a–c illustrate this point using a student friendship network

3

grouped by gender, race, and year in school. Covariates such as these deﬁne
distinct community assignments, each of which relates the covariate in question
to the observed network structure.

A key insight is that rather than maximizing modularity to obtain a sin-
gle “best” community assignment, we may instead use modularity to measure
the strength of an observed community structure. If a particular community
assignment is given by a covariate, then modularity allows us to quantify the
explanatory value of this covariate for the observed structure of the network.

2 Main result: A limit theorem for modularity

Our main result is a practical tool to understand objectively whether a covariate
captures the structure of the interactions in a network. Technically, we derive
a theorem quantifying the large-sample behavior of modularity in the setting
above. In particular, if the null model of Deﬁnition 1 below is in force, then
modularity in the presence of covariates behaves like a Normal random variable.
This enables us to associate a p-value with any observed community structure,
quantifying how unlikely it is (under the null) to observe a community structure
at least as extreme as the one we observe.

Theorem 1 (Central limit theorem for modularity). Suppose the null model of
Deﬁnition 1 below is in force, and consider a sequence of networks where for
each n we observe a ﬁxed (non-random) group assignment g(1), g(2), . . . , g(n).
Then as long as the number of groups grows strictly more slowly than n, there
exist constants b and s for each n such that as n → ∞,

(cid:98)Q − b

s

d→ Normal(0, 1).

Proof. Proofs of all results are given in the Appendices.

Thus, when appropriately shifted and scaled, modularity converges in dis-
tribution to a standard Normal random variable. In the sequel we explain this
result and give explicit formulations for b and s2 (Eqs. (4) and (5) below).

3 The network model underlying modularity

To understand Theorem 1, we must establish a technical foundation for mod-
ularity in the presence of covariates. Diﬀerent models for the network edges

Aij will imply diﬀerent estimators for Q in Eq. (2). Estimating Q using (cid:98)Q in

Eq. (1), we indirectly assume a model for the absence of community structure,
where nodes connect independently based on the product of their individual
propensities to form connections [8, 10, 11].

Deﬁnition 1 (The network model underlying modularity). Consider an undi-
rected, random graph on n nodes without self-loops. We model its (possibly

4

weighted) edges Aij ≥ 0 as independent random variables with expectations given
by the product of node-speciﬁc parameters π1, π2, . . . , πn > 0:

E Aij = πiπj,

1 ≤ i < j ≤ n.

Furthermore, considering a sequence of such networks as n grows, we assume
they are well behaved asymptotically:

1. No single node dominates the network: maxi πi/¯π, with ¯π = 1
n

l=1 πl, is

bounded asymptotically;

2. The network is not too sparse: mini πi · √
3. The expectation of each edge E Aij does not diverge too quickly as n grows:

n diverges as n grows;

√
maxi πi/

n goes to 0;

(cid:80)n

E(cid:104)
(Aij − E Aij)3(cid:105)

4. The variance of each edge does not vary too much from its expectation:
Var Aij/ E Aij is bounded from above and away from 0 asymptotically; and

5. The skewness of each edge Aij is controlled:

the third central moment
divided by the variance Var Aij is bounded asymptoti-

cally.

We make no further assumptions on the distribution of Aij, and so our
results apply in many settings, including weighted networks and those with
multiple edges. Assumptions 1–3 are structural: the ﬁrst excludes star-like net-
works; the second ensures that the network is not too sparse; and the third
controls the growth of E Aij with n in the weighted or multi-edge setting. As-
sumptions 4 and 5 are technical; they exclude extreme behavior of the edge
variables. For instance, both are fulﬁlled whenever Aij ∼ Bernoulli(πiπj) or
Aij ∼ Poisson(πiπj).

Each parameter πi describes the relative popularity of node i. Thus, to ﬁt
the degree-based model of Deﬁnition 1 to a network, we estimate the parameters
πi using the node’s degrees di as follows [8, 10, 11]:

di(cid:112)(cid:80)n

l=1 dl

ˆπi =

1 ≤ i ≤ n.

,

(3)

The estimator ˆπi is both more natural and more computationally eﬃcient
than the corresponding maximum-likelihood estimator for πi, which follows from
the theory of generalized linear models and cannot be written explicitly in closed
form. In many settings the diﬀerence between these estimators is provably small
[10], and so properties of maximum likelihood estimation can also be expected
to hold for Eq. (3).

Most importantly, we show that any ﬁnite collection of estimators deﬁned
by Eq. (3) tends toward a multivariate Normal distribution when n is large and
Deﬁnition 1 is in force. This generalizes a univariate result in [11] which assumes
Bernoulli(πiπj) edges and a power law degree distribution.

5

Theorem 2 (Multivariate central limit theorem for Eq. (3)). Assume the model
of Deﬁnition 1 and any ﬁnite set of estimators from Eq. (3). Relabeling the
indices of these estimators from 1 to r without loss of generality, we have that

as n → ∞, (cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)
(cid:18) ˆπ1 − π1
Furthermore, (cid:112)n Var di/(cid:80)n

E dl

Var d1

√

l=1

(cid:19)

, . . . ,

ˆπr − πr
√
Var dr

d→ Normal(0, Ir).

E dl is bounded asymptotically, and can be con-
sistently estimated if Aij ∼ Bernoulli(πiπj) or Poisson(πiπj) by substituting ˆπ
for π in Var di and E di.

l=1

From Deﬁnition 1 and Eq. (3), it is natural to deﬁne

(cid:100)E Aij = ˆπi ˆπj =

didj(cid:80)n

1 ≤ i < j ≤ n.

,

l=1 dl

Substituting (cid:100)E Aij for E Aij in Eq. (2), we immediately recognize modularity
(cid:98)Q as deﬁned in Eq. (1). Thus, modularity implicitly assumes the degree-based
Moreover, (cid:100)E Aij − E Aij converges in probability to zero under the model of
a central limit theorem for (cid:100)E Aij.

Deﬁnition 1 (see Appendices). As a consequence of Theorem 2, we then obtain

model of Deﬁnition 1.

Corollary. As n → ∞ under the model of Deﬁnition 1,

(cid:100)E Aij − E Aij

(cid:113)(cid:0)π2
(cid:113)
[n/ E Aij] ·(cid:0)π2

j Var di + π2

(cid:1)/(cid:80)n

i Var dj

E dl

l=1

(cid:1)/(cid:80)n

d→ Normal(0, 1).

E dl is bounded asymp-
Furthermore,
totically, and can be consistently estimated if Aij ∼ Bernoulli(πiπj) or Aij ∼
Poisson(πiπj) by substituting ˆπ for π.

j Var di + π2

i Var dj

l=1

This result leads to the ﬁrst of two key insights as to why modularity, when
appropriately shifted and scaled, behaves like a Normal(0, 1) random variable.

Recall that (cid:98)Q (Eq. (1)) is an estimator for its population counterpart Q (Eq. (2)),
in which(cid:100)E Aij estimates E Aij. Comparing Eqs. (1) and (2), and approximating
(cid:100)E Aij by E didj/(cid:80)n

E dl, we obtain:

l=1

E((cid:98)Q − Q) ≈ n(cid:88)

j=1

n(cid:88)

(cid:88)

j=1

i<j

b =

E Aij

i<j

(cid:88)

(cid:18)
E Aij − E didj(cid:80)n
(cid:0)E di + E dj −(cid:80)n
(cid:80)n

l=1

E dl

l=1

l=1 π2
l

(cid:19)

(cid:1)

E dl

δg(i)=g(j).

δg(i)=g(j).

(4)

Under the model of Deﬁnition 1, this diﬀerence cancels to ﬁrst order (see Ap-
pendices), yielding an approximate bias term of

This is precisely the shift term appearing in Theorem 1.

6

Figure 2: Within- and between-group edges in a network of political books
frequently purchased together, where groups are deﬁned by political alignment
[12]. Note that only within-group edges appear in Q (Eq. (2)); by contrast, both

types of edges contribute to modularity (cid:98)Q (Eq. (1)).

4 Modularity reﬂects within- and between-group

edges

Figure 2 illustrates the second main insight into the limiting behavior of modu-
larity: its variability reduces asymptotically to that of a centered sum of within-
and between-group edges.

More speciﬁcally, every network degree di = (cid:80)
(cid:88)

(cid:88)

within- and between-group components:

j(cid:54)=i Aij decomposes into

di = dw

i + db
i ;
db
i =

dw
i =

Aijδg(i)=g(j),

j(cid:54)=i

Aijδg(i)(cid:54)=g(j).

j(cid:54)=i

This decomposition is surprisingly powerful, in part because the model of
Deﬁnition 1 asserts that dw
i are statistically independent for any ﬁxed
group assignment g(1), g(2), . . . , g(n). After separating the systematic bias term
b in modularity from its random variation, we obtain the following decomposi-
tion.

i and db

Theorem 3 (Bias–variance decomposition for modularity). Under the null
model of Deﬁnition 1 and for a ﬁxed (non-random) group assignment g(1), g(2),
. . . , g(n), it holds that

n(cid:88)

(cid:98)Q − b =

n(cid:88)

(cid:2)db

(cid:3) + ,

αi[dw

i − E dw

i ] +

i − E db

i

βi

i=1

i=1

7

Dataset (no. nodes)

Covariate (no. groups)

Degree percentiles
25% 50% 75%

((cid:98)Q − ˆb)/ˆs

Simulated under the null
p-value

Data as observed

((cid:98)Q − ˆb)/ˆs

p-value

Books (105) [12]
Jazz bands (198) [13]
Weblogs (1224) [14]
Co-authors (36297) [15]

Political alignment (3)
Recording location (17)
Political alignment (2)
Subject category (7)

5
16
3
2

6
25
13
5

9
39
36
10

0.02
0.01
0.01
0.00

1.01
1.02
1.04
1.00

0.51
0.51
0.50
0.50

mean

std. mean

std.

0.29
0.29
0.30
0.29

21 < 10−6
29 < 10−6
118 < 10−6
472 < 10−6

Table 1: Analysis of four benchmark network datasets, using modularity de-
rived from covariate-based community assignments.

where  is a random error term, αi = 1/2 + βi, and

(cid:20) 1

2

(cid:80)n
(cid:80)n

l=1

l=1

βi =

(cid:21)

,

E dw
E dl

l

− E dw
iE di

1 ≤ i ≤ n.

Theorem 3 quantiﬁes the random variability inherent in modularity under
the model of Deﬁnition 1. It establishes that a main term contributing to the

variability of (cid:98)Q−b in this setting is a linear combination of centered within- and

i , db

between-group degrees (dw
i ), which for each i are statistically independent.
The weights αi and βi associated with this linear combination are determined
by the global proportion of expected within-group edges in the network, relative
to the local proportion of expected within-group edges speciﬁc to node i.

Combining these two insights, we ﬁrst shift modularity (cid:98)Q by its approximate
bias b and then scale it by the standard deviation s of (cid:80)n
(cid:2)db
(cid:80)n
i − E dw

i=1 αi[dw

i ] +

i − E db

i

i=1 βi

Var Aij.

(5)

(cid:3), with
n(cid:88)

s2 =

j=1

i<j

(cid:88)
(cid:2)δg(i)=g(j) + βi + βj

(cid:3)2

Recalling Theorem 3, we then know that we are left with a linear combination
of centered within- and between-group degrees that are now also scaled by s.

This leads directly to a central limit theorem for modularity (cid:98)Q as stated in

Theorem 1:

(cid:98)Q − b

s

d→ Normal(0, 1).

5 Applying the limit theorem to benchmark ex-

amples

Having established a central limit theorem for modularity in the presence of
covariates, we now show how to apply this result in practice. To turn our
theory into a methodology suitable for a speciﬁc network dataset, we ﬁrst need
to elicit a model for the data based on Deﬁnition 1. We then ﬁt this model,
leading ultimately to a p-value based on Theorem 1. We now illustrate the
complete analysis procedure for four binary networks which, along with their

8

covariates, frequently serve as benchmarks for community detection [12, 16].
Table 1 summarizes all data and results.

1. First, we must further specify the null model of Deﬁnition 1, so that
the parameter s2 in Eq. (5) can be estimated. This can be done either
by assuming sets of the variances Var Aij to be equal, or by assuming a
distribution for the edges Aij. Since the benchmark networks we consider
here are binary (Aij ∈ {0, 1}), we model their edges as

Aij ∼ Bernoulli(πiπj).

(maxi πi/¯π bounded) and 2 (mini πi · √
tuting ˆπi for πi, noting that maxi ˆπi/¯ˆπ = maxi di/ ¯d and mini ˆπi · √

2. Second, we must assess whether the ﬁve asymptotic assumptions of Deﬁ-
nition 1 appear to hold for our data. Assumptions 3–5 are automatically
satisﬁed for Bernoulli edges, and so we are left to assess Assumptions 1
n growing). We do this by substi-
√
n =
¯d. Replacing mini di, ¯d, and maxi di respectively by the ﬁrst,
mini di/
second and third degree quartiles as shown in Table 1, we observe that for
all four benchmark networks, these ratios are of order one. This indicates
that these networks are neither too star-like nor too sparse for Theorem
1 to apply.

3. Third, we estimate the parameters b and s necessary to shift and scale (cid:98)Q

in accordance with Theorem 1. To obtain an estimator ˆb, we substitute
ˆπ for π in Eq. (4). The estimator ˆs depends on the assumption added in
Step 1 above. Here, with Aij ∼ Bernoulli(πiπj), we have

Var Aij = πiπj(1 − πiπj).

Then, ˆs follows directly by substituting ˆπ for π in Eq. (5).

4. Finally, we compute and interpret the resulting approximate p-value. We
ﬁrst deﬁne community assignments g(1), g(2), . . . , g(n) based on a covari-

ate, and calculate (cid:98)Q as per Eq. (1). We next estimate ((cid:98)Q − b)/s using

ˆb and ˆs. Then, by Theorem 1, we compute an approximate one-sided
p-value as follows:

(cid:32)
Z ≥ (cid:98)Q − ˆb

(cid:33)

Pr

ˆs

, Z ∼ Normal(0, 1).

(6)

A small p-value implies that the observed value of modularity (or any
larger value) is unlikely under the null.

Table 1 shows the results of applying this procedure to four benchmark
datasets: a network of books [12] where books are connected if they have fre-
quently been purchased together, categorized by political aﬃliation (Fig. 2); a
network of jazz bands [13] where bands are connected if they have at least one

9

band member in common, categorized by recording location; a network of po-
litical commentary websites (weblogs) [14] where weblogs are connected if they
refer to each other, categorized by political aﬃliation; and a network of physi-
cists [15] where physicists are connected if they have co-authored a manuscript,
categorized by manuscript subject category.

The ﬁrst conclusion of our benchmark analysis is as follows: when we ﬁt the
null model of Deﬁnition 1 to each of these four networks, and then simulate from
the ﬁtted model (parametric bootstrap), each simulated network results in (via
√
Eq. (6)) a p-value with empirical mean near 1/2 and standard deviation near
12. This empirical result aligns with Theorem 1, which predicts the p-values
1/
to be uniformly distributed with exactly that mean and standard deviation in
the limit.

Our second conclusion is that, when using the observed data rather than
simulated data under the null, each of the covariates leads (again via Eq. (6))
to a very small p-value (< 10−6; see Table 1). This suggests that the data as
observed are extremely unlikely under the null. Furthermore, since the null itself
cannot explain any community structure, the conclusion we obtain agrees with
the use of these covariates by other researchers as ground truth in community
detection settings.

6 Evaluating communities in a multi-edge email

network

We now illustrate how our methodology can identify covariates that reﬂect a
network’s community structure. This analysis goes beyond the four benchmark
examples considered above, where we validated our methodology but did not
reach any new data-analytic conclusions. Here we evaluate the eﬀects of em-
ployee seniority, gender, and company department on community structure in a
multi-edge corporate email network (see Fig. 3). Table 2 summarizes all results,
showing that each of these covariates results in a small p-value, while covariates
based on grouping the ﬁrst- or last-name initials of the employees do not. We
will return to this analysis in more detail below, after describing the data and
eliciting a suitable model.

This network and its covariates form a substantially richer dataset than those
treated above. The data come from the Enron corporation [18]: as part of a
U.S. government investigation following allegations of fraud, the email activities
of senior employees from 1998–2002 were made public. Following the analysis
in [18], we exclude all emails that have been sent en masse (to more than ﬁve
recipients), leading to 32261 pairwise email exchanges between 153 employees.
To model this network we will use the full ﬂexibility aﬀorded by Deﬁnition 1,
following the four steps described in the previous section to determine a p-value
corresponding to each covariate.

Step 1: To construct a suitable model for the observed multi-edges Aij, we
compare four diﬀerent distributions satisfying the assumptions of Deﬁnition 1:

10

(cid:124)
(cid:123)
(cid:122)
(cid:125)

l
e
g
a

(cid:124)

(cid:123)
(cid:122)

(cid:125)

d

(cid:124)

t
r
a
d
i
n
g

t
r
a
d
i
n
g

l
e
g
a
l

t
r
a
d
i
n
g

o
t
h
e
r
s

(cid:123)
(cid:122)

d
e
r

lega(cid:124)(cid:123)(cid:122)(cid:125)

(cid:124)

(cid:125)

s
e
n
i
o
r

j
u
n
i
o
r

(cid:123)
(cid:122)

(cid:125)
(cid:124)

(cid:123)
(cid:122)

g
e
n
d
e
r
i
t
r
a
d
i
t
i
o
n
a
l

(cid:124)

(cid:124)

m
a
l
e

≥

hello echo he

trading d

trading der

(cid:123)(cid:122)

(cid:125)

(cid:124)

(cid:123)(cid:122)

(cid:125)

e
male genderi
x
i

(cid:125)

traditional exi

(cid:123)(cid:122)

(cid:124)
(cid:125)

(cid:123)(cid:122)

(cid:125)

legal

trading

others

senior

junior

(a) By department

(b) By seniority

(cid:124)

m
a
l
e

g
e
n
d
e
r

(cid:123)
(cid:122)

m
a
l
e

h
a
h
a
h

(cid:125)

h

f
e
m
a
l
e

(cid:124)

(cid:123)
(cid:122)

t
r
a
d
i
t
male gender hahah
i

(cid:125)

(cid:124)

(cid:123)(cid:122)

male

(cid:125)

(cid:124)

traditi

(cid:123)(cid:122)

(cid:125)

female

(cid:124)

(cid:123)
(cid:122)

(cid:125)
(cid:124)

(cid:123)
(cid:122)

l
e
g
a
l
t
r
a
d
i
t
r
a
d
i
n
g

-

A
H

I
-

Q

R
-
Z

(cid:125)

(cid:124)

(cid:123)
(cid:122)

t
r
a
d
i
n
g
legaltradi
a

(cid:125)

(cid:123)(cid:122)

(cid:124)

(cid:125)

(cid:124)

(cid:123)(cid:122)

(cid:125)

(cid:124)

(cid:123)(cid:122)

(cid:125)

trading

trading a

A-H

I-Q

R-Z

(c) By gender

(d) By last name initial

(cid:98)Q − ˆb

Figure 3: Multi-edges Aij in the Enron corporate email dataset (153 employees,
32261 pairwise email exchanges), grouped according to four diﬀerent covariate-
based community assignments. Shading indicates the number of emails ex-
changed.

Covariate (no. groups)

p-value

ˆs

Eq. (6) Bootstrap

Department (3)
Seniority (3)
Gender (2)
First name initial (17)
Last name initial (3)

6.17
3.14
2.36
0.74
−0.46

< 10−6
9 × 10−4
9 × 10−3
2 × 10−1
7 × 10−1

< 10−6
8 × 10−6
2 × 10−3
2 × 10−1
7 × 10−1

Table 2: Analysis of the data of Fig. 3, using modularity derived from multiple
covariate-based community assignments.

Poisson(πiπj), NegativeBinomial(πiπj, r) with common shape parameter r, and
zero-inﬂated versions of both. Figure 4 shows how well these distributions model
the multi-edges. Even without zero-inﬂation, the negative Binomial distribution
yields a good ﬁt, particularly in the right tail. A formal model comparison via
suitable likelihood ratio tests [19] conﬁrms this: as Table 3 shows, the negative
Binomial achieves the best balance between ﬁtting the observed data (residual

11

  123451050>=100  123451050>=100  123451050>=100  123451050>=100  123451050>=100Data
Cover upper
Poisson

Zero-inﬂated Poisson
Cover lowert
Negative Binomial (NB)
and zero-inﬂated NB

s
e
c
n
e
r
r
u
c
c
o

f
o

r
e
b
m
u
N

Multi-edge value

Figure 4: Observed versus expected email counts for maximum-likelihood ﬁts
of four diﬀerent models satisfying Deﬁnition 1.

Model for the
multi-edges Aij

Degrees

of freedom deviance

Residual Relative
change

Poisson
Zero-inﬂated Poisson
Negative Binomial (NB)
Zero-inﬂated NB

153
154
154
155

142031
57070
12671
12671

−39%
−37%
−19%
0%

Table 3: Goodness-of-ﬁt versus model complexity for the models in Fig. 4 (start-
ing from the 1-parameter model Poisson(λ), relative to a saturated negative
Binomial model with r → ∞).

deviance) and model complexity (degrees of freedom). We thus choose the model

Aij ∼ NegativeBinomial(πiπj, r).

(7)

√

Step 2: To verify the assumptions of Deﬁnition 1 for our data, we ﬁrst as-
√
sess Assumptions 1 and 2 exactly as before. Computing quartiles Q1–Q3 of
√
Q2 are both of order
the degrees—68, 200, 564—we see that Q3/Q2 and Q1/
n shrinking) can be analogously assessed via
one. Assumption 3 (maxi πi/
Q2). Assumptions 4 and 5 require Var Aij/ E Aij = 1 + πiπj/r and
Q3/(n
/ Var Aij = 1 + 2πiπj/r to be bounded. To assess this, we
observe that a maximum-likelihood estimate of r [19] yields ˆr = 0.047, while

E(cid:104)
(Aij − E Aij)3(cid:105)
the ﬁrst three quartiles of (cid:100)E Aij are respectively 0.16, 0.59, 2.1.

Step 3: To estimate b and s in Theorem 1, we substitute ˆπi for πi in Eqs. (4)
and (5) exactly as before. Recall, however, that to estimate s we also require an

12

Histogram for model comparison for Enron dataEdge valueEdge count01247194912531579319940110100100010000DataPoissonZero−infl PoissonNegativeBinomialHistogram for model comparison for Enron dataEdge valueEdge count012471219314978199500125831610110100100010000DataPoissonZero−infl PoissonNegativeBinomialHistogram for model comparison for Enron dataEdge valueEdge count012471219314978199500125831610110100100010000DataPoissonZero−infl PoissonNegativeBinomialHistogram for model comparison for Enron dataEdge valueEdge count012471219314978199500125831610110100100010000DataPoissonZero−infl PoissonNegativeBinomialHistogram for model comparison for Enron dataEdge valueEdge count01247194912531579319940110100100010000DataPoissonZero−infl PoissonNegativeBinomialHistogram for model comparison for Enron dataEdge valueEdge count01247194912531579319940110100100010000DataPoissonZero−infl PoissonNegativeBinomialHistogram for model comparison for Enron dataEdge valueEdge count01247194912531579319940110100100010000DataPoissonZero−infl PoissonNegativeBinomialestimate of Var Aij in Eq. (5). Under the parametrization of Eq. (7), it follows
that

Var Aij = πiπj(1 + πiπj/r).

(8)

Thus, Var Aij can be estimated by substituting ˆπi for πi and ˆr for r in (8). This
yields the required estimators ˆb and ˆs.

Step 4: To calculate p-values, we must ﬁrst compute ((cid:98)Q − ˆb)/ˆs for each

covariate. In advance of our analysis, we would expect that employee gender,
seniority, and department might reﬂect aspects of community structure in email
interactions. In contrast, we would expect covariates based on the ﬁrst or last
name of each individual to be non-informative. Figure 3 illustrates, in decreasing

order of ((cid:98)Q−ˆb)/ˆs, the observed structure of our data when grouped by covariate.
shows the limiting distribution of ((cid:98)Q − ˆb)/ˆs under the assumed model to be

Table 2 reports two approximate p-values per covariate, in contrast to the
previous section. The ﬁrst of these derives (via Eq. (6)) from Theorem 1, which

a standard Normal. The second is based on 107 replicates of the parametric
bootstrap, whereby we ﬁt a negative Binomial model to the data and then
simulate from the ﬁtted values to obtain an empirical ﬁnite-sample distribution.
Table 2 indicates that our asymptotic theory is somewhat conservative in this
setting, leading as it does here to larger p-values than the bootstrap.

Finally, considering these p-values in more detail, we see from Table 2 that
for the covariates of department, gender, and seniority, all p-values fall below
1% (leading to a corrected total of 5% after adjusting for multiple comparisons).
In contrast, we obtain large p-values for ﬁrst- and last-name covariates. This
matches our expectations that department, gender, and seniority are likely to
have an impact on email interactions, while there is no obvious reason why this
should hold for name-related covariates.

7 Discussion

Networks have richer and more varied structure than can be described by a
single “best” community assignment. To reﬂect this, we have introduced an
approach which exploits the structural information captured by covariates, each
of which may describe diﬀerent aspects of community structure in the data. In
contrast to community detection per se, this approach allows us to assess the
signiﬁcance of a given, interpretable community assignment with respect to the
observed network structure. As described in the data analysis examples above,
our method leads to the identiﬁcation of structurally signiﬁcant community
assignments, ultimately yielding a better understanding of the network under
study.

In technical terms, we have established a central limit theorem for modularity
under a nonparametric null model, yielding p-values to assess the signiﬁcance of
observed community structure. The model we introduce shows explicitly how
modularity measures variability in the data that cannot be explained solely
by node-speciﬁc propensities for connection. What is more, modularity has

13

by aggregating the estimated signed residuals Aij − didj/(cid:80)

more explanatory power than a classical (chi-squared) goodness-of-ﬁt statistic:
l dl within every
network community, it measures the global tendency of a given community
assignment to explain the observed network structure.

To advance the state of the art in network analysis, we as a research com-
munity must use this explanatory power to understand the eﬀects of multiple
observed communities on network structure. Our work here represents a ﬁrst
step in this direction: we use the explanatory power of modularity to assess
the signiﬁcance of observed community structure relative to a null model. This
opens the door to more advanced uses of multiple observed community assign-
ments within formal statistical modeling frameworks. This is an important
next step, since we see clear evidence here that multiple groupings may explain
diﬀerent aspects of a network’s community structure.

Acknowledgments

The authors thank Dr. Leon Danon for sharing the data on jazz musicians
from [13] and Mar´ıa Dolores Alfaro Cuevas for producing Fig. 2. This work was
supported in part by the US Army Research Oﬃce under Multidisciplinary Uni-
versity Research Initiative Award 58153-MA-MUR; by the US Oﬃce of Naval
Research under Award N00014-14-1-0819; by the UK Engineering and Physi-
cal Sciences Research Council under Mathematical Sciences Established Career
Fellowship EP/K005413/1; by the UK Royal Society under a Wolfson Research
Merit Award; and by Marie Curie FP7 Integration Grant PCIG12-GA-2012-
334622 within the 7th European Union Framework Program.

A Notation and assumptions

For the following proofs we will always consider an undirected random graph
on n nodes with no self-loops. We model the edges Aij as independent random
variables with expectation

E Aij = πiπj, Aij ≥ 0;

1 ≤ i < j ≤ n

di =(cid:80)

where π = (π1, . . . , πn) ∈ Rn

>0. We will denote the degree of node i as di; i.e.,
j(cid:54)=i Aij. The remaining ﬁve assumptions of Deﬁnition 1 of the degree-
based model are not all needed at all times and will therefore be mentioned
explicitly. For convenience we restate the assumptions below, all of which ref-
erence a sequence of networks where n → ∞.

1. No node dominates the network; i.e, n maxi πi/(cid:107)π(cid:107)1 = O(1);
2. The network is not too sparse; i.e., mini πi = ω(1/

√

n);

3. The expectation of each edge does not diverge too quickly; i.e., maxi πi =

√

o(

n);

14

4. The ratio of variance to expectation of each edge is controlled; i.e., ∀i, j :

5. The skewness of each edge Aij is controlled; i.e., ∀i, j :

Var Aij/ E Aij = Θ(1); and

E(cid:104)
(Aij − E Aij)3(cid:105)

/ Var(Aij) = O(1).

We use bold letters to denote vectors.

B Proof of Theorem 2

di/(cid:112)(cid:107)d(cid:107)1. We then extend this result to the multivariate case, applying the

We ﬁrst show a univariate central limit theorem for the scalar estimator ˆπi =

Cram´er–Wold theorem.

Preliminaries: Since the edges Aij, i < j are independent, it follows as shown

in [11] that for ﬁnite n

E di = πi((cid:107)π(cid:107)1 − πi),
Var di =

Var Aij,

(cid:88)
(cid:40)

i(cid:54)=j

cov(di, dj) =

Var Aij,
Var di,
n(cid:88)
E(cid:107)d(cid:107)1 = (cid:107)π(cid:107)2
1 − (cid:107)π(cid:107)2
2,
Var(cid:107)d(cid:107)1 = 2

Var di.

i (cid:54)= j
i = j

(9)

(10)

(11)

(12)

(13)

i=1

ˆπi − πi

Theorem B.1 (Central limit theorem for ˆπi). Consider Assumptions 1–5. De-

d→ Normal(0, 1).
√

ﬁne ˆπi = di/(cid:112)(cid:107)d(cid:107)1 as an estimator of πi. Then as n → ∞,
(cid:112)Var di/ E(cid:107)d(cid:107)1
Furthermore, (cid:112)Var di/ E(cid:107)d(cid:107)1 = O(1/
 di − E di
(cid:125)
(cid:123)(cid:122)
(cid:124)

(cid:112)(cid:107)d(cid:107)1
(cid:125)
(cid:123)(cid:122)

ˆπi − πi

(cid:112)Var di/ E(cid:107)d(cid:107)1

n), and can be consistently estimated
using a plug-in estimator for Aij ∼ Bernoulli(πiπj) and Aij ∼ Poisson(πiπj).
Proof. The proof is a generalization of the proof of Theorem 3.2 in [11], which
assumes Bernoulli edges and a power law degree distribution. We write

(cid:115)E(cid:107)d(cid:107)1
(cid:125)
(cid:123)(cid:122)
(cid:124)

(cid:107)d(cid:107)1

E di − πi

+

(cid:124)

√

=



.

(14)

√

Var di

T2

Var di

T1

T3

15

To deduce the required result, we show that T1 converges in distribution to a
Normal(0, 1) random variable and T2 and T3 go in probability to 0 and 1, re-
spectively. Slutsky’s theorem enables us to combine the results and to obtain
the claimed convergence in distribution.

Term T1: Each degree di = (cid:80)

j(cid:54)=i Aij is a sum of independent random
variables. From Assumption 2 (⇒ E di → ∞) and Assumption 4 (E Aij =
Θ(Var Aij)), it follows that Var di → ∞. Since in addition, the skewness of
each edge Aij is asymptotically bounded (Assumption 5), the Lyapunov condi-
tion for exponent 1 is satisﬁed; i.e.,

(cid:80)

(Aij − E Aij)3(cid:105)
E(cid:104)
(cid:104)(cid:80)
(cid:105)3/2

j(cid:54)=i

j(cid:54)=i Var Aij

→ 0.

Hence, the Lindeberg–Feller Central Limit Theorem allows us to conclude

that T1

d→ Normal(0, 1).

Term T2: We write

E di − πi

√

Var di
E di − πi
√

(cid:112)(cid:107)d(cid:107)1
(cid:112)E(cid:107)d(cid:107)1
(cid:123)(cid:122)
(cid:125)

Var di

a)

(cid:124)

T2 =

=

(cid:112)(cid:107)d(cid:107)1 − πi
(cid:123)(cid:122)

√

(cid:112)E(cid:107)d(cid:107)1
(cid:125)

Var di

b)

− πi

(cid:124)

.

(15)

Term T2 converges in probability to 0 since both a) the ﬁrst ratio converges to
0 and b) the second ratio converges to 0 in probability.
Eqs. (9) and (12)) while Var di → ∞. More precisely,

a) This convergence is driven by the fact that E di − πi

(cid:112)E(cid:107)d(cid:107)1 = O(1) (see
(cid:21)

(cid:112)E(cid:107)d(cid:107)1

E di − πi
√

Var di

πi(cid:107)π(cid:107)1

=

(cid:20)
1 −(cid:113)

2/(cid:107)π(cid:107)2

1

− π2

i

1 − (cid:107)π(cid:107)2
√

Var di

.

(16)

Considering ˜π = π/ maxj πj, we can conclude from (cid:107)˜π(cid:107)2

2 ≤ (cid:107)˜π(cid:107)1 that

(cid:107)π(cid:107)2
(cid:107)π(cid:107)2

2

1

=

(maxj πj)2(cid:107)˜π(cid:107)2
(maxj πj)2(cid:107)˜π(cid:107)2

2

1

≤ 1
(cid:107)˜π(cid:107)1

=

maxj πj
(cid:107)π(cid:107)1

.

Assumption 1 implies that maxj πj/(cid:107)π(cid:107)1 = O(1/n), and thus we conclude

(cid:18) 1

(cid:19)

.

n

(cid:107)π(cid:107)2
(cid:107)π(cid:107)2

2

1

= O

16

(17)

(18)

This allows us to apply a convergent Taylor expansion of

=

πi

E di − πi
√
Var di
πi(cid:107)π(cid:107)1

1 − (cid:107)π(cid:107)2

2/2(cid:107)π(cid:107)1 + o
√

2/2(cid:107)π(cid:107)2
√
Var di
2/(cid:107)π(cid:107)1

(cid:112)E(cid:107)d(cid:107)1
(cid:104)
1 −(cid:16)
(cid:104)(cid:107)π(cid:107)2
≤ πi[maxj πj/2 + o(maxj πj)] − π2
(cid:18) πi(maxj πj − πi)
√
(cid:32)√
√E di
(cid:112)(cid:107)π(cid:107)1 − πi
(cid:19)
(cid:18) maxj πj − πi

(cid:16)(cid:107)π(cid:107)2
(cid:19)

πi(maxj πj − πi)

(cid:33)

Var di

Var di

= Θ

= Θ

=

i

= O
√

√

n

.

(Assumption 1)

√

1 − x at 0 in Eq. (16):

(cid:17)(cid:17)(cid:105) − π2

i

(19)

1 + o

2/(cid:107)π(cid:107)2

1

(cid:16)(cid:107)π(cid:107)2
(cid:17)(cid:105) − π2

i

(see Eq. (17))

(Assumption 4)

Since πj = o(
Eq. (19) converges to 0 in n.

n) for all j (Assumption 3), it follows that the left-hand side of

b) We show below that the second ratio

√
Eq. (15) converges in probability to 0; this follows since πi/

der Assumptions 1 and 4 (see c) below) and (cid:112)(cid:107)d(cid:107)1 −(cid:112)E(cid:107)d(cid:107)1 = OP (1) (see

Var di in
Var di → 0 un-

πi

/

(cid:16)

(cid:112)(cid:107)d(cid:107)1 − πi

(cid:112)E(cid:107)d(cid:107)1

(cid:17)

√

Lemma B.1 below).

c) From Assumption 4 it follows that

(cid:18) πi√E di
(cid:19)
n(cid:1).
= O(cid:0)1/

= Θ

√

(cid:18)(cid:114) πi

(cid:19)

= Θ

(cid:107)π(cid:107)1 − πi
(Assumption 1)

πi√
Var di

Lemma B.1. Consider Assumptions 2–5. Then,(cid:112)(cid:107)d(cid:107)1 −(cid:112)E(cid:107)d(cid:107)1 = OP (1).
1. A Taylor expansion in probability of (cid:112)(cid:107)d(cid:107)1/ E(cid:107)d(cid:107)1 about 1 requires in

Proof. Observe that the square root function has one continuous derivative at

(20)

addition [20, p. 201] that

I. ∃a ∈ R : (cid:107)d(cid:107)1/ E(cid:107)d(cid:107)1 = a + OP (rn); with
II. rn → 0 as n → ∞.

I. It follows from Chebyshev’s inequality that

(cid:107)d(cid:107)1/ E(cid:107)d(cid:107)1 = 1 + OP

(cid:16)(cid:112)Var(cid:107)d(cid:107)1/ E(cid:107)d(cid:107)1

(cid:17)

.

(21)

17

II. As a consequence of I., rn = (cid:112)Var(cid:107)d(cid:107)1/ E(cid:107)d(cid:107)1. From Eq. (12) and

Assumption 2 (⇒ E di → ∞) it follows that E(cid:107)d(cid:107)1 → ∞. Since Aij are inde-
pendent for i < j, and since we assume Var Aij/ E Aij = Θ(1) (Assumption 4),
it holds that

We now can apply a convergent Taylor expansion in probability:

j=1

j=1

=

=

j=1

Var

i<j Aij

i<j Aij

(cid:17)
(cid:17)

(cid:80)
(cid:80)

Var(cid:107)d(cid:107)1
E(cid:107)d(cid:107)1

i<j Var(Aij)
E(Aij)

(cid:16)
2(cid:80)n
E(cid:16)
2(cid:80)n
4(cid:80)n
(cid:80)
(cid:80)
2(cid:80)n
It follows that the ratio(cid:112)Var(cid:107)d(cid:107)1/ E(cid:107)d(cid:107)1 → 0.
(cid:19)
(cid:32)(cid:107)d(cid:107)1 − E(cid:107)d(cid:107)1
(cid:112)Var(cid:107)d(cid:107)1

(cid:115) (cid:107)d(cid:107)1
⇔ (cid:112)(cid:107)d(cid:107)1 −(cid:112)E(cid:107)d(cid:107)1 =
Since the term (cid:107)d(cid:107)1/2 =(cid:80)n

E(cid:107)d(cid:107)1

E(cid:107)d(cid:107)1

= Θ(1).

− 1

+ oP

1
2

j=1

i<j

1
2

= 1 +

(cid:18) (cid:107)d(cid:107)1
(cid:34)
(cid:112)Var(cid:107)d(cid:107)1
(cid:112)E(cid:107)d(cid:107)1
(cid:80)
(cid:112)Var(cid:107)d(cid:107)1

(cid:107)d(cid:107)1 − E(cid:107)d(cid:107)1

d→ Normal(0, 1).

i<j Aij is a sum of independent random vari-
ables, we apply the Lindeberg–Feller central limit theorem analogously to Term
T1: From Assumptions 2–5, it follows that

j=1

(22)

(cid:33)

(cid:35)

(cid:32)(cid:112)Var(cid:107)d(cid:107)1

E(cid:107)d(cid:107)1

(cid:33)

+ oP (1)

.

(23)

of Lemma B.1; i.e.,(cid:112)(cid:107)d(cid:107)1 −(cid:112)E(cid:107)d(cid:107)1 = OP (1).

Since Var(cid:107)d(cid:107)1/ E(cid:107)d(cid:107)1 = Θ(1) by Eq. (22), we conclude from Eq. (23) the result

As a consequence of Lemma B.1, we now know that the numerator of
√
√
term b) in Eq. (15) is bounded in probability. Since we show in Eq. (20) that
Var di = O(1/
πi/

n), it follows that

(cid:112)(cid:107)d(cid:107)1 − πi

(cid:112)E(cid:107)d(cid:107)1

√

Var di

P→ 0.

πi

b) =

In turn, this completes the proof of the convergence of Term 2 (see Eq. (15));
i.e.,

(cid:112)E(cid:107)d(cid:107)1
(cid:125)
(cid:123)(cid:122)

Var di

a)

(cid:112)(cid:107)d(cid:107)1 − πi
(cid:123)(cid:122)

√

(cid:112)E(cid:107)d(cid:107)1
(cid:125)

Var di

b)

− πi

(cid:124)

E di − πi
√

T2 =

(cid:124)

P→ 0.

(24)

18

Term T3

Combining Eqs. (21) and (22), we know that

(cid:32)

(cid:33)

.

1(cid:112)E(cid:107)d(cid:107)1

(cid:107)d(cid:107)1
E(cid:107)d(cid:107)1

= 1 + OP

Applying the continuous mapping theorem, leads to (cid:112)(cid:107)d(cid:107)1/ E(cid:107)d(cid:107)1

This converges in probability to 1 because of Assumption 2 (⇒ E(cid:107)d(cid:107)1 → ∞).
P→ 1.
The inverse of a random variable which converges in probability to a constant
c, must in turn converge to 1/c, as long as c (cid:54)= 0 [21, Theorem 2.1.3]. Thus,

(cid:115)E(cid:107)d(cid:107)1

(cid:107)d(cid:107)1

T3 =

P→ 1.

(25)

Slutsky’s Theorem enables us to combine the results on the convergence of

terms T1–T3 to obtain that

(cid:112)Var di/ E(cid:107)d(cid:107)1

ˆπi − πi

→ Normal(0, 1).

To complete the proof of Theorem B.1, it remains to show that Var di/ E(cid:107)d(cid:107)1 =
O(1/n), and that it can be consistently estimated using a plug-in estimator for
Aij ∼ Bernoulli(πiπj) and Aij ∼ Poisson(πiπj).

Since Var Aij/ E Aij = Θ(1) (Assumption 4), we know that

(cid:115)

(cid:115)
(cid:115)
(cid:115)

n Var di
E(cid:107)d(cid:107)1

=

=

n Θ(E di)
E(cid:107)d(cid:107)1
n Θ(πi((cid:107)π(cid:107)1 − πi))

(cid:107)π(cid:107)2

1 − (cid:107)π(cid:107)2

(cid:18) 1 − πi/(cid:107)π(cid:107)1

2

1 − (cid:107)π(cid:107)2

2/(cid:107)π(cid:107)2

1

(cid:19)

.

=

nπi
(cid:107)π(cid:107)1

Θ

1 = O(1/n) (also from Assumption 1). Hence,(cid:112)Var di/ E(cid:107)d(cid:107)1

We know that nπi/(cid:107)π(cid:107)1 = O(1) (Assumption 1) and we have seen in Eq. (18)
that (cid:107)π(cid:107)2
2/(cid:107)π(cid:107)2
√
= O(1/
n).
We defer the proof of consistency of the plug-in estimator of Var di/ E(cid:107)d(cid:107)1
for Aij ∼ Bernoulli(πiπj) and Aij ∼ Poisson(πiπj) to Theorem D.1, where we
show a more general statement.

Having shown a univariate central limit theorem for each ˆπi, we are now
ready to extend this result to the multivariate case. The Corollary below is
identical to Theorem 2 in the main text.

tions 1–5. Estimate πi by ˆπi = di/(cid:112)(cid:107)d(cid:107)1 for all i and ﬁx a set of r positive

Corollary B.1 (Multivariate central limit theorem for ˆπis). Consider Assump-

19

integers as indices, with r ﬁnite. Relabeling the indices from 1 to r without loss

(cid:18) ˆπ1 − π1

(cid:19)(cid:48)
of generality,(cid:113)E(cid:107)d(cid:107)1
Furthermore for all i, (cid:112)Var di/ E(cid:107)d(cid:107)1 = O(1/

ˆπr − πr
√
Var dr

Var d1

, . . . ,

√

d→ Normal(0, Ir).
√

n), and can be consistently
estimated for Aij ∼ Bernoulli(πiπj) and Aij ∼ Poisson(πiπj) using a plug-in
estimator.

Proof. This proof is the multidimensional equivalent of the proof of Theo-
rem B.1. It is analogously driven by the fact that the vector

(cid:18) d1 − E d1

√

Var d1

m1 =

, . . . ,

dr − E dr
√
Var dr

(cid:19)(cid:48)

can be reduced to a sum of independent but not identically distributed ran-
dom vectors. These in turn converge in distribution to a multivariate standard
Normal random vector; as we now show. In direct analogy to the univariate
case of Eq. (14),

(cid:19)(cid:48)

(cid:33)

√

, . . . ,

ˆπr − πr
√
Var dr

Var d1
1√
Var d1

(cid:18) ˆπ1 − π1
(cid:113)E(cid:107)d(cid:107)1
(cid:32)
(cid:32)
(cid:113)E(cid:107)d(cid:107)1
d1(cid:112)(cid:107)d(cid:107)1
(cid:115)E(cid:107)d(cid:107)1
(cid:32)(cid:18) d1 − E d1
(cid:123)(cid:122)
(cid:124)
(cid:123)(cid:122)
(cid:125)
(cid:124)
(cid:107)d(cid:107)1
(cid:32)E d1 − π1
(cid:112)(cid:107)d(cid:107)1
(cid:123)(cid:122)
(cid:124)

Var d1

Var d1

, . . . ,

, . . . ,

√

=

=

·

√

m3

+

m1

m2

− π1

, . . . ,

1√
Var dr

(cid:19)(cid:48)
(cid:125)
(cid:112)(cid:107)d(cid:107)1

dr − E dr
√
Var dr

E dr − πr

√

Var dr

(cid:33)(cid:33)(cid:48)

(cid:32)

dr(cid:112)(cid:107)d(cid:107)1

− πr

(cid:33)
(cid:33)(cid:48)
(cid:125)

.

(26)

Each component of the vector m2 converges in probability to 0 (see Eq. (24)
in the proof of Theorem B.1).
In addi-
tion, the scalar m3 converges in probability to 1 (see Eq. (25) in the proof of
Theorem B.1).
d→ Normal(0, Ir). In order to apply a multivariate
central limit theorem, we rearrange m1 such that we extract a sum of indepen-

It follows that the vector m2

We now prove that m1

P→ 0.

20

dent random vectors (m12):

m1 =

= diag

(cid:19)(cid:48)

, . . . ,

(cid:123)(cid:122)

D11

l=r+1 Al1)

dr − E dr
√
Var dr

√

√

, . . . ,

Var d1

Var d1

(cid:18) d1 − E d1
(cid:113)

Var((cid:80)n
(cid:124)
(cid:80)n
(cid:113)
Var((cid:80)n
l=r+1(Al1 − E Al1)
(cid:124)
l=r+1 Al1)
(cid:18)(cid:80)r
(cid:124)

l=1(Al1 − E Al1)

Var d1

√

+

·

, . . . ,

m12

(cid:123)(cid:122)
(cid:123)(cid:122)

m13

, . . . ,

(cid:113)
Var((cid:80)n

√

l=r+1 Alr)

Var dr

(cid:80)n
(cid:113)
Var((cid:80)n
l=r+1(Alr − E Alr)
l=r+1 Alr)
(cid:80)r
(cid:19)(cid:48)
l=1(Alr − E Alr)
(cid:125)

Var dr

√

.


(cid:125)
(cid:48)
(cid:125)

(27)

We will show three things: that the matrix D11 converges to the identity matrix
Ir; that m12

d→ Normal(0, Ir); and that the term m13

P→ 0.

For the term D11, it holds for all i that

l=r+1 Ali)

=

l=1 Ali)
l=1 Ali)

.

Furthermore, from Assumption 4 (Var Aij = Θ(E Aij)) we conclude for all i that

Var((cid:80)r
Var((cid:80)n

It follows further from Assumption 1 that

In turn,

(cid:113)

Var((cid:80)n

l=1 Ali)
l=1 Ali)
√
l=r+1 Ali)/
d→ Normal(0, Ir), as we will now show by applying the
Cram´er–Wold theorem. The term m12 is a random vector depending on n,
where each component is a sum of independent random variables. We will show

Var di → 1 for all i. Hence, the diagonal

matrix D11 converges to the identity matrix Ir in the operator norm.

The term m12

(28)

n

(cid:115)

Var di

Var((cid:80)n
Var((cid:80)r
Var((cid:80)n

l=1 Ali)
l=1 Ali)

l=1

(cid:115)
1 − Var((cid:80)r
Var((cid:80)n
(cid:18)(cid:80)r
(cid:80)n
E Ali
(cid:80)r
(cid:18) πi
E Ali
(cid:32) r(cid:88)
l=1 πl
πi(cid:107)π(cid:107)1
πl(cid:107)π(cid:107)1
(cid:17) → 0.
= O(cid:16) r

l=1

l=1

(cid:19)
(cid:19)
(cid:33)

.

= Θ

= Θ

= Θ

21

now that, as a consequence, each component converges marginally in distri-
bution to a Normal(0, 1) random variable (by the same argument as in Theo-
rem B.1 for Term T1). From Assumption 2 (⇒ E di → ∞) and Assumption 4
(Var Aij/ E Aij = Θ(1)), it follows that Var di → ∞. Since in addition we
assume the skewness of each edge Aij to be bounded asymptotically (Assump-
tion 5), the Lyapunov condition (for δ = 1) is satisﬁed for each component.
Hence, the Lindeberg–Feller central limit theorem lets us conclude that each
component converges marginally in distribution to a Normal(0, 1) random vari-
able [22, p. 362].
each (c1, . . . , cr) ∈ Rr and Yu

Furthermore, the components of m12 are independent. It follows that for

iid∼ Normal(0, 1) for u = 1, . . . , r, it holds that

r(cid:88)

u=1

cu

(cid:80)n
(cid:113)
Var((cid:80)n
l=r+1(Alu − E Alu)
l=r+1 Alu)

d→ r(cid:88)

u=1

cuYu.

Applying the Cram´er–Wold theorem, we conclude that m12
P→ 0, since by Chebyshev’s inequality

Finally, term m13

(cid:80)r
l=1(Ali − E Ali)

√

Var di

(cid:115)

Var((cid:80)r

= OP

l=1 Ali)

Var di

,

d→ Normal(0, Ir).

which in turn goes to 0 for all i, as seen in Eq. (28).

By Slutsky’s theorem, we can combine the results on the convergence of

D11, m12, and m13 to conclude (see Eq. (27)) that

m1 = D11 m12 + m13

d→ Normal(0, Ir).

In turn, we deduce the required result (see Eq. (26)) that

(cid:113)E(cid:107)d(cid:107)1

(cid:18) ˆπ1 − π1

√

Var d1

, . . . ,

ˆπr − πr
√
Var dr

(cid:19)(cid:48)

= m3m1 + m2

d→ Normal(0, Ir).

To complete the proof we need to show consistency of the plug-in estimator
of Var di/ E(cid:107)d(cid:107)1 for Aij ∼ Bernoulli(πiπj) and Aij ∼ Poisson(πiπj). We defer
this to Theorem D.1, where we show a more general statement.

C Proof of the Corollary of Theorem 2

As a reminder to the reader, the Corollary in the main text is as follows.

Corollary (Central limit theorem for (cid:100)E Aij). Consider Assumptions 1–5. De-
ﬁne the estimator (cid:100)E Aij = didj/(cid:107)d(cid:107)1 for E Aij. Then as n → ∞,

(cid:100)E Aij − E Aij

j Var di + π2

i Var dj

(cid:113)(cid:0)π2

(cid:1)/ E(cid:107)d(cid:107)1

22

d→ Normal(0, 1).

(cid:113)(cid:0)π2

(cid:1)/ E(cid:107)d(cid:107)1 = O(cid:16)(cid:112)E Aij/n

(cid:17)

j Var di + π2

Furthermore for all i, j,
, and
can be consistently estimated using a plug-in estimator for Aij ∼ Bernoulli(πiπj)
and Aij ∼ Poisson(πiπj).

Proof. We show that (cid:100)E Aij = ˆπi ˆπj, once appropriately standardized, converges

i Var dj

j Var di + π2

i Var dj

in distribution to a Normal(0, 1) random variable. It can easily be seen that

(cid:100)E Aij = πiπj + πj(ˆπi − πi) + πi(ˆπj − πj) + (ˆπi − πi)(ˆπj − πj).

πj). As a consequence, we standardize all quantities in Eq. (29) by the factor

of the standard deviation of πj(ˆπi− πi) + πi(ˆπj − πj). Then, we can use Eq. (29)

(29)
Under the hypothesis that (ˆπi − πi)(ˆπj − πj) is asymptotically negligible, the

asymptotic behavior of (cid:100)E Aij − πiπj will be dominated by πj(ˆπi − πi) + πi(ˆπj −
(cid:113)E(cid:107)d(cid:107)1/(cid:0)π2
(cid:1), which can be interpreted as an approximation
to write(cid:113)E(cid:107)d(cid:107)1
(cid:113)
(cid:115)
(cid:124)
(cid:115)
(cid:124)

π2
j Var di + π2
E(cid:107)d(cid:107)1
π2
j Var di + π2

(cid:18) ˆπi − πi
(cid:123)(cid:122)

E(cid:107)d(cid:107)1
π2
j Var di + π2

(cid:100)E Aij − πiπj

(cid:112)Var dj

· (ˆπi − πi)(ˆπj − πj)

ˆπj − πj

(cid:112)Var dj

(cid:33)(cid:35)
(cid:125)

√

T1

πj

Var di

+ πi

Var di

i Var dj

i Var dj

.

(cid:125)

i Var dj

(cid:34)

(cid:112)

=

+

(cid:32)

(cid:19)

(cid:123)(cid:122)

T2

To deduce the required result, we will show that T1

d→ Normal(0, 1) and
T2 = oP (T1). Slutsky’s theorem will then enable us to combine these results
and obtain the claimed convergence in distribution.

Term T1: Recall from Corollary B.1 that under Assumptions 1–5 it holds
d→ Normal(0, I2). Applying the Cram´er–Wold

, ˆπj−πj√

ˆπi−πi
√
Var di

Var dj

that(cid:112)E(cid:107)d(cid:107)1

(cid:18)

(cid:19)(cid:48)

theorem and Slutsky’s theorem, we can conclude that

d→ Normal(0, 1).

T1

Term T2: It remains to show that T2 = oP (T1); i.e., that

(ˆπi − πi)(ˆπj − πj) = o(πj(ˆπi − πi) + πi(ˆπj − πj)).

23

We now use Lemma C.1 that we will show immediately below.

T2
T1

=

=

=

=

(cid:21)−1

(ˆπi − πi)(ˆπj − πj)

πj(ˆπi − πi) + πi(ˆπj − πj)

(ˆπi − πi)(ˆπj − πj)
πj

(cid:20) πj(ˆπi − πi) + πi(ˆπj − πj)
(cid:20)
(cid:21)−1
(cid:16)(cid:112)E dj
(cid:17)(cid:105)−1
(cid:17)
(cid:16)(cid:112)E di
(cid:104)
(cid:33)
(cid:32)
√E di +(cid:112)E dj

ˆπj − πj

ˆπi − πi

+ Ω

πi

+

Ω

1

= OP

(see Lemma C.1)

(30)

√
From Assumption 2 (πi = ω(1/

hence that T2/T1

P→ 0.

n)), it follows that mini E di diverges, and

Lemma C.1. Consider Assumptions 1, 2 and 4. Then,

(cid:19)

(cid:18) πi√E di

ˆπi − πi = OP

Proof. First, we appeal to a Taylor expansion in probability of ˆπi = di/(cid:112)(cid:107)d(cid:107)1.

Let A = di/ E di and B = ((cid:107)d(cid:107)1 − 2di)/ E((cid:107)d(cid:107)1 − 2di). Observe that the func-
tion

.

ˆπi = f (A, B) =

(31)

(cid:112)2 E di A + E((cid:107)d(cid:107)1 − 2di) B

E di A

has continuous partial derivatives at (1, 1)(cid:48). A Taylor expansion in probability

[20, p. 201] of f requires in addition that (cid:112)(A − 1)2 + (B − 1)2 P→ 0. By
(cid:112)(A − 1)2 + (B − 1)2 =

Chebyshev’s inequality, we know that

− 1

− 1

+

(cid:19)2
(cid:18) di
(cid:35)

E di

(cid:18) (cid:107)d(cid:107)1 − 2di
(cid:19)(cid:21)

E((cid:107)d(cid:107)1 − 2di)
+ Op

(cid:20)

(cid:19)2
(cid:18) (cid:107)d(cid:107)1 − 2di
(cid:35)

E((cid:107)d(cid:107)1 − 2di)

(cid:34)

Var

Var di
(E di)2

+ Op

Var((cid:107)d(cid:107)1 − 2di)
(E((cid:107)d(cid:107)1 − 2di))2

.

Var

(cid:19)(cid:21)

(cid:115)(cid:18) di
(cid:115)
(cid:20)
(cid:118)(cid:117)(cid:117)(cid:116)Op
(cid:34)
that(cid:112)(A − 1)2 + (B − 1)2 P→ 0.

E di
Op

=

=

From Assumptions 2 and 4 (⇒ E di → ∞, Var Aij/ E Aij = Θ(1)), it follows

24

We now can expand the function f (A, B) in Eq. (31) in a convergent Taylor

series around (1, 1)(cid:48). In combination with Assumptions 2 and 4 we obtain

(cid:20)

E di(cid:112)E(cid:107)d(cid:107)1

1 + OP

(cid:19)(cid:21)

(cid:18) 1√E di

.

(32)

Furthermore, we conclude that

E di(cid:112)E(cid:107)d(cid:107)1

=

=

di(cid:112)(cid:107)d(cid:107)1
(cid:112)1 − (cid:107)π(cid:107)2
(cid:20)
(cid:18) 1
(cid:20)
(cid:18) 1
(cid:18) 1
(cid:20)

πi(1 − πi/(cid:107)π(cid:107)1)
(cid:19)(cid:21)(cid:20)
2/(cid:107)π(cid:107)2
(cid:19)(cid:21)(cid:20)
(cid:19)(cid:21)

1 + O

1 + O

n

n

1

1 + O

n

= πi

= πi

= πi

Combining Eqs. (32) and (34), it follows that

(cid:21)−1/2
1 − (cid:107)π(cid:107)2
(cid:18)(cid:107)π(cid:107)2
(cid:19)(cid:21)
(cid:107)π(cid:107)2
1 + O

1

2

2

(cid:107)π(cid:107)1

(33)

(Assumption 1)

(Taylor expansion)

.

(see Eq. (18))

(34)

We conclude immediately the result of Lemma C.1; i.e.,

(cid:19)(cid:21)

.

ˆπi =

(cid:20)

= πi

1 + OP

di(cid:112)(cid:107)d(cid:107)1

(cid:18) 1√E di
(cid:19)
(cid:18) πi√E di
(cid:1)/ E(cid:107)d(cid:107)1 = O(cid:16)(cid:112)E Aij/n

ˆπi − πi = OP

.

(cid:113)(cid:0)π2

Having established the claimed central limit theorem, we now show that
j Var di + π2

i Var dj

:

(35)

(cid:17)

= O(cid:16)(cid:112)πiπj/n
(cid:17)
 (Assumption 4)
 (Assumption 1)

E di

(cid:115)

(cid:115)
(cid:115)
(cid:18)(cid:114)

= Θ

= Θ

· π2

n

πiπj

j Var di

i Var dj + π2
E(cid:107)d(cid:107)1
· π2

n

i

E dj + π2
E(cid:107)d(cid:107)1

j

πiπj

n · πi + πj
(cid:107)π(cid:107)1

= Θ
= O(1).

n

πiπj

· π2

i πj + π2
(cid:107)π(cid:107)1

j πi

(cid:19)

(Assumption 1)

25

(cid:113)

n(cid:0)π2

(cid:1)/(cid:80)n

j Var di + π2

i Var dj

To complete the proof of the Corollary, we need to show consistency of
E dl for networks with
the plug-in estimator of
edges Aij ∼ Bernoulli(πiπj) or Aij ∼ Poisson(πiπj). We defer this to Theo-
rem D.1, where we show a more general statement.

Recall that modularity (cid:98)Q (Eq. [1] in main text) is an empirical quantity that
E Aij is estimated using (cid:100)E Aij. For each individual (cid:100)E Aij, we show now that
(cid:100)E Aij − E Aij

estimates its population counterpart Q (Eq. [2] in main text), in the sense that

P→ 0 at a rate no slower than (πi + πj)/

n (Assumption 3). More

√

l=1

precisely we have the following.

Lemma C.2. Consider Assumptions 1, 2, and 4. Then,

(cid:100)E Aij − E Aij = OP

(cid:18) πi + πj√

(cid:19)

.

n

√
From Assumption 3, we know that (πi + πj)/

n = oP (1).

Proof. Recall from Eq. (29) that

(cid:100)E Aij − E Aij = πj(ˆπi − πi) + πi(ˆπj − πj) + (ˆπi − πi)(ˆπj − πj).

Furthermore, we know from Eq. (30) that

= (πj(ˆπi − πi) + πi(ˆπj − πj))

(cid:32)

(cid:34)

1 + OP

(cid:33)(cid:35)

√E di +(cid:112)E dj

1

From Lemma C.1 and Assumptions 1, 2 and 4, it follows that

+

(cid:32)
πiπj√E di
(cid:18)(cid:114) πi(cid:107)π(cid:107)1
(cid:18) πj√

+

(cid:33)
πiπj(cid:112)E dj
(cid:114) πj(cid:107)π(cid:107)1
(cid:19)

πj +

πi√
n

= OP

= OP

= OP

n
(Assumption 3).

= oP (1)

(Assumption 1)

(cid:19)

πi

(Assumption 1)

D Consistency of the plug-in estimator for

(cid:112)Var di/ E(cid:107)d(cid:107)1
that (cid:112)Var di/ E(cid:107)d(cid:107)1 can be consistently estimated using a plug-in estimator

Throughout the Theorem and Corollaries in the main text (and above), we state

26

Each edge distribution leads to a diﬀerent variance Var di, each of which

for Aij ∼ Bernoulli(πiπj) and Aij ∼ Poisson(πiπj). In fact, this is true more
generally, as we show below.

can be consistently estimated by a plug-in estimator, as long as Var di can
be consistently estimated by a plug-in estimator. More precisely, we have the
following.

is Θ(E di) by Assumption 4. We now show that the term (cid:112)Var di/ E(cid:107)d(cid:107)1
Theorem D.1 (Consistency of plug-in estimator for (cid:112)Var di/ E(cid:107)d(cid:107)1). Con-
exchanging each πi in Var di and E(cid:107)d(cid:107)1 by ˆπi = di/(cid:112)(cid:107)d(cid:107)1. In addition, assume
Then,(cid:112)Var di/ E(cid:107)d(cid:107)1 can be estimated consistently using the plug-in estimator
(cid:113)(cid:92)Var di/ (cid:92)E(cid:107)d(cid:107)1; i.e.,

sider Assumptions 1, 2 and 4. Deﬁne plug-in estimators (cid:92)Var di and (cid:92)E(cid:107)d(cid:107)1 by

(cid:91)Var di
Var di

P→ 1.

that

Proof. We ﬁrst write(cid:113)(cid:92)Var di/ (cid:92)E(cid:107)d(cid:107)1
(cid:112)Var di/ E(cid:107)d(cid:107)1

(cid:113)(cid:92)Var di/ (cid:92)E(cid:107)d(cid:107)1
(cid:112)Var di/ E(cid:107)d(cid:107)1
(cid:115) (cid:91)Var di
(cid:115) (cid:91)Var di
(cid:115) (cid:91)Var di

Var di

=

=

P→ 1.

(cid:107) ˆπ(cid:107)2

(cid:115) E(cid:107)d(cid:107)1
(cid:115) E(cid:107)d(cid:107)1
1 − (cid:107) ˆπ(cid:107)2
(cid:115)E(cid:107)d(cid:107)1
(cid:34)
1 − (cid:107)d(cid:107)2

(cid:107)d(cid:107)2

2

Var di

(cid:107)d(cid:107)1
(cid:35)− 1
1 − (cid:107)d(cid:107)2
(cid:107)d(cid:107)2
Assumption 4 (Var Aij = Θ(E Aij)) it holds that(cid:112)E(cid:107)d(cid:107)1/(cid:107)d(cid:107)1

From term T3 (Eq. (25)) in the proof of Theorem B.1, we know that under
P→ 1 . Since we

.

(36)

(cid:107)d(cid:107)1

Var di

=

2

1

2

2

assume (cid:91)Var di/ Var di

P→ 1, it remains to show that

First, from Chebyshev’s inequality, and from Assumption 4, we know that

 1(cid:113)E(cid:107)d(cid:107)2

2

(cid:107)d(cid:107)2
E(cid:107)d(cid:107)2

2

2

= 1 + OP

 1(cid:113)E(cid:107)d(cid:107)2

1

.

(37)

(cid:107)d(cid:107)2
E(cid:107)d(cid:107)2

1

1

= 1 + OP

P→ 0.

(cid:107)d(cid:107)2
(cid:107)d(cid:107)2

2

1

 and

27

We may apply a convergent Taylor expansion of f (x) = (1 + x)−1 at 1, since
x = 1/

1 = o(1). It follows that

1

 1(cid:113)E(cid:107)d(cid:107)2
 1(cid:113)E(cid:107)d(cid:107)2

1

−1

.



1 + OP

1 + OP
.
(cid:16)

(cid:17)

(38)

since (cid:107)d(cid:107)2

2 ≤ (cid:107)d(cid:107)2

1

In return, it follows that

(cid:107)d(cid:107)2
(cid:107)d(cid:107)2

2

1

=

E(cid:107)d(cid:107)2
E(cid:107)d(cid:107)2

2

1

(cid:113)E(cid:107)d(cid:107)2

=

=

E(cid:107)d(cid:107)2
E(cid:107)d(cid:107)2

2

1

E(cid:107)d(cid:107)2
E(cid:107)d(cid:107)2

2

1

2

1 + OP

 1(cid:113)E(cid:107)d(cid:107)2
1 + OP
 1(cid:113)E(cid:107)d(cid:107)2
1 + OP
 1(cid:113)E(cid:107)d(cid:107)2
(cid:88)
(cid:88)
(cid:88)
(cid:88)

2

2

i

Via straightforward algebraic computations, we obtain

E(cid:107)d(cid:107)2

2 =

E(AijAil)

l(cid:54)=i

j(cid:54)=i
E di E di · (1 + o(1))
1(cid:107)π(cid:107)2
2 · (1 + o(1)),

=

i

= (cid:107)π(cid:107)2

and

(Assumption 1)

(39)

E(cid:107)d(cid:107)2

1 = Var(cid:107)d(cid:107)1 + (E(cid:107)d(cid:107)1)2
(cid:104)
(E(cid:107)d(cid:107)1)2(cid:105)
= Θ(E(cid:107)d(cid:107)1) + (E(cid:107)d(cid:107)1)2
= Θ

.

(Assumption 4)

(40)

(41)

Combining Eqs. (39) and (41) and applying Assumption 1, it then follows that

We know from Eq. (38) that

(cid:107)d(cid:107)2
(cid:107)d(cid:107)2

2

1

=

E(cid:107)d(cid:107)2
E(cid:107)d(cid:107)2

2

1

(Assumption 2)

1 + OP
 1(cid:113)E(cid:107)d(cid:107)2
.
(cid:19)(cid:21)
(cid:18)

2

1 + OP

(cid:19)

1

(cid:107)π(cid:107)1(cid:107)π(cid:107)2

.

(see Eq. (18))

28

(cid:20)
(cid:18) 1

2

1

n

(cid:107)π(cid:107)2
(cid:107)π(cid:107)2

=

= OP

Finally, we know from Eq. (36) that

(cid:113)(cid:92)Var di/ (cid:92)E(cid:107)d(cid:107)1
(cid:112)Var di/ E(cid:107)d(cid:107)1

=

(cid:115) (cid:91)Var di

Var di

(cid:34)

(cid:115)E(cid:107)d(cid:107)1

(cid:107)d(cid:107)1

1 − (cid:107)d(cid:107)2
(cid:107)d(cid:107)2

2

1

(cid:35)− 1

2

.

The inverse of a random variable which converges in probability to a constant c
must in turn converge to 1/c, as long as c (cid:54)= 0 [21, Theorem 2.1.3]. Applying this
fact and the continuous mapping theorem, we obtain the claimed convergence
in probability; i.e.,

(cid:113)(cid:92)Var di/ (cid:92)E(cid:107)d(cid:107)1
(cid:112)Var di/ E(cid:107)d(cid:107)1

P→ 1.

Having established Theorem D.1, we now show for Aij ∼ Bernoulli(πiπj)
P→ 1. This allows us to apply The-

orem D.1 to conclude that (cid:112)Var di/ E(cid:107)d(cid:107)1 can be estimated consistently via

and Aij ∼ Poisson(πiπj) that (cid:91)Var di/ Var di

its plug-in estimator.

Aij ∼ Poisson(πiπj): For Poisson-distributed edges, E Aij = Var Aij for all

i, j. Hence, we obtain

(cid:21)

(Chebyshev’s inequality)

(cid:33)(cid:35)(cid:20)
1 − di(cid:107)d(cid:107)1
(cid:19)(cid:21)(cid:20)
(cid:21)
1 − di(cid:107)d(cid:107)1

.

=

(42)
Furthermore, from Assumptions 1 (nπi/(cid:107)π(cid:107)1 = O(1)), 2 (⇒ E di → ∞),
P→ 1, as we will now show.

and 4 (Var Aij = Θ(E Aij)), it follows that

(Assumption 4)

(cid:107)π(cid:107)1
πi

di(cid:107)d(cid:107)1

We write

(cid:91)Var di
Var di

=

=

=

=

=

i

− d2
i(cid:107)d(cid:107)1

di
E di
1 + OP

(cid:99)E di
E di
ˆπ1(cid:107)ˆπ(cid:107)1 − ˆπ2
E di
di√(cid:107)d(cid:107)1
(cid:107)d(cid:107)1√(cid:107)d(cid:107)1
(cid:21)
(cid:20)
E di
(cid:34)
(cid:32)(cid:115)
1 − di(cid:107)d(cid:107)1
(cid:18) 1√E di
(cid:20)
(cid:18)(cid:107)π(cid:107)1
(cid:124)
(cid:123)(cid:122)

Var di
(E di)2

(cid:107)π(cid:107)1
πi

1 + OP

di(cid:107)d(cid:107)1

πi

=

cn

(cid:19)
(cid:125)

(cid:18) (cid:107)d(cid:107)1
(cid:124)
(cid:123)(cid:122)

(cid:107)π(cid:107)2

1

En

(cid:19)
−1(cid:18) di
(cid:19)
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:125)

E di

Fn

E di
(cid:107)π(cid:107)2

1

29

.

(43)

By Chebyshev’s inequality and from Assumptions 2 and 4, we know that

Fn =

= 1 + OP

di
E di

For En, we will ﬁrst establish the equivalence

(cid:107)d(cid:107)1
E(cid:107)d(cid:107)1
⇔ (cid:107)d(cid:107)1
(cid:107)π(cid:107)2

1

=

=

(cid:107)d(cid:107)1
(cid:20)
1 − (cid:107)π(cid:107)2
1 − (cid:107)π(cid:107)2
(cid:107)π(cid:107)2

2

2

1

(cid:21)

.

=

(cid:107)π(cid:107)2
(cid:107)d(cid:107)1
E(cid:107)d(cid:107)1

.

(cid:19)

(cid:18) 1√E di
(cid:20)
1 − (cid:107)π(cid:107)2
(cid:107)π(cid:107)2

(cid:107)d(cid:107)1
(cid:107)π(cid:107)2

2

1

1

(cid:21)−1

By Eq. (18), we know that from Assumption 1 it follows that (cid:107)π(cid:107)2
1 =
O(1/n). Furthermore, by Chebyshev’s inequality and from Assumptions 2 and
4, (cid:107)d(cid:107)1/ E(cid:107)d(cid:107)1
(cid:107)d(cid:107)1
(cid:107)π(cid:107)2

(cid:20)
1 − (cid:107)π(cid:107)2
(cid:107)π(cid:107)2

P−→ 1. Thus, it follows that

(cid:16)
n,(cid:112)E(cid:107)d(cid:107)1

(cid:107)d(cid:107)1
E(cid:107)d(cid:107)1

= 1 + OP

.

2/(cid:107)π(cid:107)2



En =

(cid:17)

(cid:21)

(44)

min

=

1

2

1

1

For the non-random sequence {cn; n ∈ N} in Eq. (43) it holds that

(cid:107)π(cid:107)1
πi
(cid:107)π(cid:107)1
(cid:20)
πi
1 + O

(cid:21)

1

E di
(cid:20)
(cid:107)π(cid:107)2
πi(cid:107)π(cid:107)1
(cid:19)(cid:21)
1 − πi(cid:107)π(cid:107)1
(cid:107)π(cid:107)2

(cid:18) 1

1

cn =

=

=

.

(Assumption 1)

n

The inverse of a random variable which converges in probability to a con-
stant c must in turn converge to 1/c, as long as c (cid:54)= 0 [21, Theorem 2.1.3].
Furthermore, the product of two random variables, converging in probability to
a constant c and a constant d respectively, itself converges to the product of the
constants cd [21, Theorem 2.1.3]. Thus, it follows that

(cid:32)

(cid:0)√E di, n(cid:1)(cid:33)

1

.

mini

(45)

(cid:107)π(cid:107)1
πi

⇔ di(cid:107)d(cid:107)1

di(cid:107)d(cid:107)1
= OP

= cnE−1

n Fn = 1 + OP

.

(Assumption 1)

(cid:19)

(cid:18) 1
(cid:20)

n

Recall from Eq. (42) that
(cid:91)Var di
Var di

1 + OP

=

(cid:19)(cid:21)(cid:20)

(cid:18) 1√E di

1 − di(cid:107)d(cid:107)1

(cid:21)

.

30

In turn, we obtain the required result; i.e.,

(cid:32)

(cid:0)√E di, n(cid:1)(cid:33)

(cid:91)Var di
Var di

= 1 + OP
√
n)), it follows that mini E di diverges. Hence,
From Assumption 2 (πi = ω(1/
we have shown the required result that Var di can be consistently estimated by
its plug-in estimator (cid:91)Var di.

mini

1

.

Aij ∼ Bernoulli(πiπj): For Bernoulli-distributed edges, we obtain Var di =

E di − π2

i (cid:107)π(cid:107)2

2 + π4

i [11]. We write

i − ˆπ2
i − π2
It can easily been seen that ˆπi(cid:107)ˆπ(cid:107)1 = di and (cid:107)ˆπ(cid:107)2

ˆπi(cid:107)ˆπ(cid:107)1 − ˆπ2
πi(cid:107)π(cid:107)1 − π2

i (cid:107)ˆπ(cid:107)2
2 + ˆπ4
i
i (cid:107)π(cid:107)2
2 + π4
i
2 = (cid:107)d(cid:107)2

(cid:91)Var di
Var di

=

.

2/(cid:107)d(cid:107)1. It follows that

=

=

=

di − d2

di − d2

i /(cid:107)d(cid:107)1 − d2
πi(cid:107)π(cid:107)1 − π2
i /(cid:107)d(cid:107)1 − d2

i(cid:107)d(cid:107)2
i − π2
i(cid:107)d(cid:107)2
πi(cid:107)π(cid:107)1 − π2

2/(cid:107)d(cid:107)2
i (cid:107)π(cid:107)2
2/(cid:107)d(cid:107)2
i (cid:107)π(cid:107)2

2

1 + d4
2 + π4
i
1 + d4

i /(cid:107)d(cid:107)2

1

i /(cid:107)d(cid:107)2

1

di[1 − di/(cid:107)d(cid:107)1] − d2

i

1 + d2

i /(cid:107)d(cid:107)2

1

2/(cid:107)d(cid:107)2
i (cid:107)π(cid:107)2

2

πi(cid:107)π(cid:107)1 − π2

(cid:104)(cid:107)d(cid:107)2

· [1 + o(1)]

(cid:105)

(Assumption 1)

· [1 + o(1)].

(46)

We have seen in Eq. (32) that Assumptions 2 and 4 imply that

di(cid:112)(cid:107)d(cid:107)1

=

E di(cid:112)E(cid:107)d(cid:107)1
(cid:20)

E di
E(cid:107)d(cid:107)1

(cid:20)

1 + OP

(cid:19)(cid:21)

(cid:18) 1√E di
(cid:19)(cid:21)
(cid:18) 1√E di

di(cid:107)d(cid:107)1

=

1 + OP

.

.

(47)

It follows from identical arguments that

From Assumption 1, we conclude that

E di
E(cid:107)d(cid:107)1

=

πi(1 − πi/(cid:107)π(cid:107)1)
(cid:19)(cid:21)
2/(cid:107)π(cid:107)2
1)

(cid:18) 1

(cid:107)π(cid:107)1(1 − (cid:107)π(cid:107)2
(cid:18) 1
πi(cid:107)π(cid:107)1
= O

1 + O

(cid:20)
(cid:19)

=

n

.

n

(see Eq. (34))

(Assumption 1)

(48)

31

Combining Eqs. (47) and (48), it follows that

(cid:19)

.

(cid:18) 1

n

di(cid:107)d(cid:107)1

= OP

It follows in turn that in combination with Eq. (46), we obtain

(cid:91)Var di
Var di

=

=

Term Rn:

1

2/(cid:107)d(cid:107)2
i (cid:107)π(cid:107)2

di − d2
i(cid:107)d(cid:107)2
πi(cid:107)π(cid:107)1 − π2
· 1 − di/(cid:107)d(cid:107)1(cid:107)d(cid:107)2
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124)
(cid:123)(cid:122)
πi(cid:107)π(cid:107)1

· [1 + oP (1)]
2/(cid:107)d(cid:107)1
(cid:125)

1 − πi/(cid:107)π(cid:107)1 (cid:107)π(cid:107)2

di

2

2

Rn

Sn

·[1 + oP (1)].

(49)

Rn =

di

=

πi(cid:107)π(cid:107)1
E di
πi(cid:107)π(cid:107)1
E di
πi(cid:107)π(cid:107)1
= 1 + OP

=

(cid:32)(cid:115)
(cid:18) 1√E di
(cid:19)

1 + OP

(cid:34)
(cid:20)
(cid:18) 1√E di

1 + OP

Var di
(E di)2

(cid:19)(cid:21)

(Assumption 1)

(cid:33)(cid:35)

(Chebyshev’s inequality)

(Assumption 4)

= 1 + oP (1).

(Assumption 2)

Term Sn: We show the convergence of Sn from Eq. (49) in two steps:

1.

(cid:107)π(cid:107)1
πi

(cid:16)(cid:107)π(cid:107)2

di(cid:107)d(cid:107)1

P−→ 1;

(cid:17)−1 (cid:107)d(cid:107)2

P→ 1.

2

2

(cid:107)d(cid:107)1

2.
Step 1: This step follows analogously to Eq. (45) for Aij ∼ Poisson(πiπj).
Step 2: We write the ratio of interest as

(cid:16)(cid:107)π(cid:107)2

2

(cid:17)−1(cid:107)d(cid:107)2

2

(cid:107)d(cid:107)1

=

(cid:33)−1

(cid:32) (cid:107)d(cid:107)1

(cid:107)π(cid:107)2

1

(cid:18) (cid:107)d(cid:107)2

2

E(cid:107)d(cid:107)2

2

·

(cid:19)

·

(cid:33)

(cid:32) E(cid:107)d(cid:107)2

2

(cid:107)π(cid:107)2

2(cid:107)π(cid:107)2

1

= L−1

n Mntn.

Now, we analyze Ln, Mn and tn in consecutive order. Under Assumptions 1, 2
P−→ 1 (see Eq. (44)). Furthermore, com-
and 4, we know that Ln = (cid:107)d(cid:107)1/(cid:107)π(cid:107)2
P−→ 1
bining Eqs. (37) and (39) enables us to conclude that Mn = (cid:107)d(cid:107)2
(under Assumptions 1 and 4). From Eq. (39), we know that under Assumption
1, the sequence {tn; n ∈ N} converges to 1.

2/ E(cid:107)d(cid:107)2

1

2

32

The inverse of a random variable which converges in probability to a con-
stant c, must in turn converge to 1/c, as long as c (cid:54)= 0 [21, Theorem 2.1.3].
Furthermore, the product of two random variables, converging in probability to
a constant c and a constant d respectively, itself converges to the product of the
constants cd [21, Theorem 2.1.3]. Thus, Step 2 follows.

Returning now to Eq. (49) and following the same argument, we conclude
P→ 1 and in turn, (cid:91)Var di/ Var di = RnSn[1 + oP (1)] P→ 1 for Bernoulli-

that Sn
distributed edges (Aij ∼ Bernoulli(πiπj)).

E Proof of Theorem 1

We now state and prove Theorem E.1, which is identical to Theorem 1 in the
main text, except for the formulation of the weights βj, j = 1, . . . , n. In Corol-
lary F.1 below, we introduce the formulation for βj used in Theorem 1 to improve
interpretability and show that both formulations are asymptotically equivalent.
The proof below expands on the proof sketch given in the main text.

Theorem E.1 (Central limit theorem for modularity). In addition to Assump-
tions 1–5, suppose that the number K of communities grows strictly more slowly
than n(; i.e., K/n → 0). Then, as n → ∞,

s

d→ Normal(0, 1),

(cid:98)Q − b
(cid:16)E di + E dj − (cid:107)π(cid:107)2
(cid:3)2

(cid:88)
(cid:88)
(cid:2)δg(i)=g(j) + βi + βj

E(cid:107)d(cid:107)1

E Aij

i<j

(cid:17)

δg(i)=g(j),

2

Var(Aij).

where

n(cid:88)
n(cid:88)

j=1

b =

s2 =

j=1

i<j

The βi are deﬁned in Eq. (52) in Lemma E.1 below and are non-random.

Proof. The proof consists of two main steps. First, in Lemma E.1, we will
relate modularity to a linear combination of within-group degrees (dw
in Eq.
i
(50) below) and between-group degrees (db
in Eq. (50) below). Second, in
i
Lemma E.2, we will show that this linear combination, when appropriately
standardized, converges in distribution to a Normal(0, 1) random variable.

Let us ﬁrst note some preliminaries. Recall from the main text:

Aijδg(i)=g(j)

and db

j =

Aijδg(i)(cid:54)=g(j).

(50)

dw
j =

Let us denote

(cid:107)π(cid:107)g(j),j

1

=

(cid:88)
(cid:88)

i(cid:54)=j

i(cid:54)=j

(cid:88)

i(cid:54)=j

n(cid:88)

i=1

πi δg(i)(cid:54)=g(j).

πi δg(i)=g(j)

and (cid:107)π(cid:107)¬g(j)

1

=

33

(cid:3) +

n(cid:88)

j=1

(cid:2)db
(cid:35)

E dl
E(cid:107)d(cid:107)1

− (cid:107)π(cid:107)g(j),j

1

1(cid:112)E(cid:107)d(cid:107)1

,

(cid:98)Q = b +

βj =

αj =

 =

(cid:34)

j=1

αj

1
2

(cid:107)π(cid:107)g(l),l

(cid:2)dw

 n(cid:88)
n(cid:88)
(cid:80)
(cid:80)n
min(n,(cid:107)π(cid:107)1) minl
(cid:98)Q =

+ βj,

1
2

j=1

l=1

1

√

n),

(52)

(53)

(54)

We obtain

E dw

j =πj(cid:107)π(cid:107)g(j),j

1

and E db

j = πj(cid:107)π(cid:107)¬g(j)

1

.

(51)

We are now ready to proceed with our analysis. The following Lemma is

identical to Lemma 1 in the main document.
Lemma E.1. Consider Assumptions 1–4 (πi/(cid:107)π(cid:107)1 = O(1/n), πi = ω(1/
√
πi = o(

n), E Aij = Θ(Var Aij)). Then, the following identity holds:

j − E dw

j

βj

j − E db

j

(cid:3) + OP (),

where the non-random quantities αj, βj, and  are deﬁned as follows:

i<j πiπjδg(i)=g(j)

√E dl

.

(cid:88)

n(cid:88)

Proof. Since (cid:100)E Aij = didj/(cid:107)d(cid:107)1, modularity can be written as
(cid:100)E Aijδg(i)=g(j).

Aijδg(i)=g(j) − n(cid:88)
(cid:88)
1. Write (cid:100)E Aij in terms of ˆπj = dj/(cid:112)(cid:107)d(cid:107)1;
2. Expand the denominator(cid:112)(cid:107)d(cid:107)1 around its mean in a convergent Taylor
(cid:1) into the lower-order terms of the Taylor

3. Substitute dj = E dj +OP

We will show this lemma in six steps. We

(cid:0)(cid:112)E dj

series;

(55)

j=1

j=1

i<j

i<j

expansion of Step 2;

4. Apply the decomposition dj = dw

j + db

j, and center dw

j and db

j about their

respective means E dw

5. Collect all higher-order non-random terms in (cid:98)Q into b; and

j and E db
j;

6. Show that the remaining lower-order random and non-random terms can

be absorbed into .

34

Step 1: Recall from Eq. (29) that

(cid:100)E Aij = ˆπi ˆπj

= πiπj + πj(ˆπi − πi) + πi(ˆπj − πj) + (ˆπi − πi)(ˆπj − πj),

and from Eq. (30) that, given Assumptions 1, 2, and 4, it holds that

(cid:33)

.

(cid:32)

1

√E di +(cid:112)E dj
(cid:18)

(cid:18)

1 + OP

minl

1

√E dl

(cid:19)(cid:19)

.

(56)

As a consequence, we may combine these two results to write

(ˆπi − πi)(ˆπj − πj)

πj(ˆπi − πi) + πi(ˆπj − πj)

= OP

(cid:100)E Aij = πiπj + [πj(ˆπi − πi) + πi(ˆπj − πj)] ·
n(cid:88)

(cid:88)

i<j

(cid:100)E Aijδg(i)=g(j) − n(cid:88)
(cid:88)
 n(cid:88)
(cid:88)
(cid:18)

(cid:18)

j=1

j=1

i<j

·

1 + OP

j=1

=

πiπjδg(i)=g(j)

i<j

n(cid:88)

(cid:88)

j=1

i<j

1

√E dl

minl

Focusing on the rightmost sum in Eq. (55), we then obtain from Eq. (56)

πj(ˆπi − πi)δg(i)=g(j) +

πi(ˆπj − πj)δg(i)=g(j)



=

j=1

i(cid:54)=j

(cid:18)

(cid:18)

(cid:19)(cid:19)

1 + OP

Renaming the indices in the ﬁrst summand from i to j and vice versa leads to

πi(ˆπj − πj)δg(i)=g(j)

 n(cid:88)
(cid:88)
(cid:80)
Hence,(cid:80)n
i<j(cid:100)E Aijδg(i)=g(j) can be substituted into Eq. (55) as follows:
Aijδg(i)=g(j) − n(cid:88)
n(cid:88)
(cid:88)
(cid:98)Q =
− n(cid:88)
(cid:88)

πi(ˆπj − πj)δg(i)=g(j) ·

πiπjδg(i)=g(j)

√E dl

1 + OP

(cid:19)(cid:19)

(cid:18)

(cid:18)

minl

j=1

i<j

j=1

i<j

j=1

1

.

.

1

√E dl

minl

j=1

i(cid:54)=j

.

(cid:19)(cid:19)
 ·
(cid:88)

35

We now change from a relative error term to an absolute error. In addition,

i<j Aijδg(i)=g(j) = 1
2

j=1 dw
j ,

(cid:80)n

ˆπj = dj/(cid:112)(cid:107)d(cid:107)1 and

:

1

j=1

i(cid:54)=j πiδg(i)=g(j) = (cid:107)π(cid:107)g(j),j
(cid:88)
(cid:32)

(cid:80)
we substitute (cid:80)n
(cid:80)
n(cid:88)
j − n(cid:88)
 n(cid:88)


(cid:107)π(cid:107)g(j),j

+ OP

−

dw

1
2

j=1

j=1

j=1

i<j

=

1

minl

1

√E dl
n(cid:88)

We will show in Step 6 below that

πiπjδg(i)=g(j)

(cid:33)

− n(cid:88)

j=1

(cid:88)

i(cid:54)=j

dj(cid:112)(cid:107)d(cid:107)1
n(cid:88)

j=1

(cid:107)π(cid:107)g(j),j

(ˆπj − πj)

1

πiπjδg(i)=g(j)

.



(57)

(ˆπj − πj) = OP (),

where  is the error term deﬁned in Eq. (54). Thus,

1

1

j=1

(cid:107)π(cid:107)g(j),j

√E dl
πiπjδg(i)=g(j) − n(cid:88)
(cid:88)

minl

n(cid:88)

n(cid:88)

(cid:98)Q =

1
2

dw
j +

j=1

j=1

i<j

j=1

(cid:107)π(cid:107)g(j),j

1

dj(cid:112)(cid:107)d(cid:107)1

+ OP ().

(58)

Step 2: In this step we focus on the penultimate term in Eq. (58). We
−1/2 = f (x) = x−1/2 at 1, and
appeal to a Taylor expansion of ((cid:107)d(cid:107)1/ E(cid:107)d(cid:107)1)
then control the remainder using Chebyshev’s inequality. As a consequence, we
obtain from Assumption 4 (Var Aij = Θ(E Aij)) that

n(cid:88)
n(cid:88)

j=1

=

j=1

(cid:107)π(cid:107)g(j),j

1

(cid:107)π(cid:107)g(j),j

1

dj(cid:112)(cid:107)d(cid:107)1
dj(cid:112)E(cid:107)d(cid:107)1
n(cid:88)

(cid:107)π(cid:107)g(j),j

1

j=1

dj(cid:112)E(cid:107)d(cid:107)1

− 1
2

(cid:20)

·

1 − 1
2

(cid:18) (cid:107)d(cid:107)1E(cid:107)d(cid:107)1
dj(cid:112)E(cid:107)d(cid:107)1
n(cid:88)

(cid:107)π(cid:107)g(j),j

·

1

j=1

36

We will show in Step 6 below that

Continuing Eq. (59), we have that

n(cid:88)

j=1

=

(cid:107)π(cid:107)g(j),j

1

(cid:19)

− 1

+ OP

(cid:18) 1

E(cid:107)d(cid:107)1

(cid:19)(cid:21)

.

(59)

(60)

1

E(cid:107)d(cid:107)1

= OP ().

dj(cid:112)E(cid:107)d(cid:107)1

(cid:18) (cid:107)d(cid:107)1E(cid:107)d(cid:107)1

(cid:19)

− 1

+ OP ().

(61)

Step 3: From Chebyshev’s inequality and Assumption 4, we know that dj =

(cid:1)(cid:3). Inserting this result into the second (i.e., lower-order)

E dj
term of the Taylor expansion in Eq. (61), we obtain

(cid:2)1 + OP
n(cid:88)

(cid:0)1/(cid:112)E dj
dj(cid:112)E(cid:107)d(cid:107)1
E dj(cid:112)E(cid:107)d(cid:107)1
n(cid:88)

1

1

=

(cid:107)π(cid:107)g(j),j

j=1

j=1

− 1

− 1
2

n(cid:88)

(cid:107)π(cid:107)g(j),j

(cid:19)(cid:34)
(cid:18) (cid:107)d(cid:107)1E(cid:107)d(cid:107)1
(cid:18) (cid:107)d(cid:107)1E(cid:107)d(cid:107)1
E dj(cid:112)E(cid:107)d(cid:107)1
Applying Eq. (63) and then substituting(cid:80)n
n(cid:88)
E dj(cid:112)E(cid:107)d(cid:107)1

1
2
= OP ().

dj(cid:112)(cid:107)d(cid:107)1

(cid:107)π(cid:107)g(j),j

(cid:107)π(cid:107)g(j),j

(cid:107)π(cid:107)g(j),j

(Step 6 below)

1
2

j=1

=

1

1

1

j=1

(62)

+ OP ().

1 + OP

(cid:32)
(cid:33)(cid:35)
1(cid:112)E dj
(cid:19) 1(cid:112)E dj

− 1

(63)
j=1 dj for (cid:107)d(cid:107)1 in Eq. (62), we have

Applying Chebyshev’s inequality and then Assumption 4, we next obtain

n(cid:88)
(cid:34)
− n(cid:88)

j=1

1
2
+ OP ().

j=1

n(cid:88)

l=1

j=1

1
2

i + db

n(cid:88)
(cid:34)
− n(cid:88)
(cid:34)
− n(cid:88)

j=1

1

n(cid:88)
n(cid:88)

l=1

1
2

1
2

j=1

l=1

(cid:107)π(cid:107)g(l),l

1

E dl
E(cid:107)d(cid:107)1

− (cid:107)π(cid:107)g(j),j

1

(cid:107)π(cid:107)g(l),l

1

(cid:107)π(cid:107)g(l),l

1

E dl
E(cid:107)d(cid:107)1
E dl
E(cid:107)d(cid:107)1

− (cid:107)π(cid:107)g(j),j

1

− (cid:107)π(cid:107)g(j),j

1

(cid:35)

(cid:35)
(cid:35)

dj(cid:112)E(cid:107)d(cid:107)1

(64)

dw

j(cid:112)E(cid:107)d(cid:107)1
j(cid:112)E(cid:107)d(cid:107)1

db

(65)

.

Step 4: Applying di = dw

i leads to the identity

=

(cid:107)π(cid:107)g(j),j

+ OP ()

E dj(cid:112)E(cid:107)d(cid:107)1

(cid:35)

(cid:34)

1
2

n(cid:88)

We deﬁne non-random factors βj and αj as in Eqs. (52) and (53); i.e.,

(cid:107)π(cid:107)g(l),l

− (cid:107)π(cid:107)g(j),j

βj =

and αj =

+ βj.

1

1

l=1

Combining the results from Eqs. (58) and (65), we may rewrite (cid:98)Q in terms of
(cid:98)Q =

(cid:107)π(cid:107)g(j),j

αj and βj as

j + OP ().

n(cid:88)

(cid:88)

αjdw

βjdb

j +

πiπjδg(i)=g(j) − 1
2

E dj(cid:112)E(cid:107)d(cid:107)1

j=1

j=1

j=1

i<j

+

1

E dl
E(cid:107)d(cid:107)1
n(cid:88)

1
2

1(cid:112)E(cid:107)d(cid:107)1
n(cid:88)

37

After centering dw

j and db

j about their respective means, we obtain

(cid:2)dw

j − E dw

j

(cid:3) +

n(cid:88)
(cid:2)db
n(cid:88)

βj

j=1

j=1

n(cid:88)
(cid:3) +
E dj(cid:112)E(cid:107)d(cid:107)1

j=1

πiπjδg(i)=g(j) − 1
2

(cid:107)π(cid:107)g(j),j

1

j − E db

j

αj E dw

j +

βj E db

j

j=1

(66)

+ OP ().

n(cid:88)

Step 5 We now address the non-random terms in modularity. We treat the

non-random terms in the two lines of Eq. (66) separately; i.e.,

j +(cid:80)n

j=1 αj E dw
i<j πiπjδg(i)=g(j) − 1

(cid:80)n
j=1 βj E db
j;
j=1(cid:107)π(cid:107)g(j),j

1

2

E dj√E(cid:107)d(cid:107)1

.

Term a) :
From the deﬁnition of αj and βj, we obtain

n(cid:88)

E dw

j +

βj E dj

j=1

πiπjδg(i)=g(j) +

πiπjδg(i)=g(j) +

(cid:107)π(cid:107)g(j),j

1

E dj(cid:112)E(cid:107)d(cid:107)1

πiπjδg(i)=g(j) − 1
2

(cid:34)
n(cid:88)
(cid:34)
n(cid:88)

1
2

j=1

1
2

l=1

(cid:35) E dj(cid:112)E(cid:107)d(cid:107)1

n(cid:88)

(cid:107)π(cid:107)g(l),l

1

E dl
E(cid:107)d(cid:107)1

E dl(cid:112)E(cid:107)d(cid:107)1

1

− (cid:107)π(cid:107)g(j),j

(cid:35)(cid:80)n
E dj
E(cid:107)d(cid:107)1

j=1

l=1

(cid:107)π(cid:107)g(l),l

1

n(cid:88)

(cid:107)π(cid:107)g(j),j

1

j=1

E dj(cid:112)E(cid:107)d(cid:107)1

(cid:98)Q =

n(cid:88)
(cid:88)

j=1

αj

+

i<j

a) (cid:80)n
b) (cid:80)

j=1

j=1

1
2

n(cid:88)
n(cid:88)
(cid:88)
(cid:88)
n(cid:88)
− n(cid:88)
n(cid:88)
(cid:88)

j=1

j=1

i<j

i<j

a) =

=

=

=

j=1

i<j

= b).

b) =

=

n(cid:88)
n(cid:88)

j=1

j=1

i<j

(cid:88)
(cid:88)
n(cid:88)

i<j

− 1
2

Term b) :
Via straightforward calculations, one can show that

πiπjδg(i)=g(j) − 1
2

n(cid:88)

(cid:88)

j=1

i(cid:54)=j

(cid:0)πj(cid:107)π(cid:107)1 − π2
(cid:114)

j

(cid:1)

1 − (cid:107)π(cid:107)2
(cid:107)π(cid:107)2

2

1

πi
(cid:107)π(cid:107)1

δg(i)=g(j)

πiπjδg(i)=g(j)

(cid:88)

(cid:34)
πiπj − πiπ2
(cid:107)π(cid:107)1

j

(cid:35)(cid:32)

1 − (cid:107)π(cid:107)2
(cid:107)π(cid:107)2

2

1

(cid:33)− 1

2

δg(i)=g(j).

j=1

i(cid:54)=j

38

(67)

(68)

We know from Eq. (18) that from Assumption 1 it follows that (cid:107)π(cid:107)2
1 =
O(1/n). As a consequence, we can apply a convergent Taylor expansion to
f (x) = (1 − x)−1/2 at 0 to obtain

2/(cid:107)π(cid:107)2

(cid:33)− 1

2

(cid:32)
1 − (cid:107)π(cid:107)2
(cid:107)π(cid:107)2

2

1

(cid:32)(cid:107)π(cid:107)2

(cid:107)π(cid:107)2

2

1

(cid:33)2.

= 1 +

1
2

(cid:107)π(cid:107)2
(cid:107)π(cid:107)2

2

1

+ O

As a consequence, it follows that we may express Eq. (68) as

b) =

πiπjδg(i)=g(j) − 1
2

i<j

j=1

n(cid:88)
(cid:88)
(cid:34)
− n(cid:88)
(cid:88)
(cid:34)
(cid:88)
n(cid:88)

1
2

j=1

i<j

+

1
2

j=1

i(cid:54)=j

n(cid:88)

j=1

(cid:88)
(cid:34)(cid:32)(cid:107)π(cid:107)2

i(cid:54)=j

2

(cid:107)π(cid:107)2

1

πiπjδg(i)=g(j)

(cid:33)2(cid:35)(cid:35)
(cid:34)(cid:32)(cid:107)π(cid:107)2

δg(i)=g(j)

(cid:33)2(cid:35)(cid:35)

+

1
2

πiπ2
j
(cid:107)π(cid:107)1

(cid:107)π(cid:107)2
(cid:107)π(cid:107)2

2

1

+

πiπ2
j
(cid:107)π(cid:107)1

O

2

(cid:107)π(cid:107)2

1

πiπj

(cid:107)π(cid:107)2
(cid:107)π(cid:107)2

2

1

+ πiπj O

(69)

(70)

δg(i)=g(j).

(71)

We identify the ﬁrst terms in Eqs. (70) and (71) as the terms of leading

order. We will show in Step 6 that the remaining terms satisfy

πiπ2
j
(cid:107)π(cid:107)1
(cid:34)
(cid:88)
(cid:34)
(cid:88)
n(cid:88)

i<j

j=1

i(cid:54)=j

πiπj O

− n(cid:88)

j=1

+

1
2
= O(),

(cid:34)(cid:32)(cid:107)π(cid:107)2

2

(cid:33)2(cid:35)(cid:35)

1

(cid:107)π(cid:107)2
(cid:107)π(cid:107)2
(cid:107)π(cid:107)2

2

1

1
2

πiπ2
j
(cid:107)π(cid:107)1

δg(i)=g(j)

+

πiπ2
j
(cid:107)π(cid:107)1

O

(cid:34)(cid:32)(cid:107)π(cid:107)2

2

(cid:107)π(cid:107)2

1

(cid:33)2(cid:35)(cid:35)

(72)

δg(i)=g(j)

where we remind the reader that  is the error term deﬁned in Eq. (54).

Finally, considering the leading-order terms in Eqs. (70) and (71), it then

follows from the identity

n(cid:88)

(cid:88)

j=1

i(cid:54)=j

n(cid:88)

(cid:88)

j=1

i<j

πiπ2

j δg(i)=g(j) =

πiπj(πi + πj)δg(i)=g(j)

that

n(cid:88)

(cid:88)

j=1

i<j

1
2

b) =

(cid:34)

πiπj

πi + πj
(cid:107)π(cid:107)1

− (cid:107)π(cid:107)2
(cid:107)π(cid:107)2

2

1

(cid:35)

δg(i)=g(j) + O().

(73)

We may then combine terms a) and b) using Eqs. (67) and (73), whence

(cid:34)

n(cid:88)

(cid:88)

j=1

i<j

a) + b) =

πiπj

(cid:35)

− (cid:107)π(cid:107)2
(cid:107)π(cid:107)2

2

1

πi + πj
(cid:107)π(cid:107)1

39

δg(i)=g(j) + O().

In order to gain interpretability, we rearrange the term a) + b) even further:

(cid:34)
(cid:34)
(cid:34)

(cid:35)
(cid:35)(cid:34)
(cid:35)

E Aij

E Aij

πi(cid:107)π(cid:107)1 + πj(cid:107)π(cid:107)1 − (cid:107)π(cid:107)2

2

(cid:107)π(cid:107)2

1

πi(cid:107)π(cid:107)1 + πj(cid:107)π(cid:107)1 − (cid:107)π(cid:107)2

2

E(cid:107)d(cid:107)1

2

E(cid:107)d(cid:107)1

πi(cid:107)π(cid:107)1 + πj(cid:107)π(cid:107)1 − (cid:107)π(cid:107)2
(cid:34)
(cid:16)E di + E dj − (cid:107)π(cid:107)2

(cid:17)
E(cid:107)d(cid:107)1

πi(cid:107)π(cid:107)1 + πj(cid:107)π(cid:107)1 − (cid:107)π(cid:107)2

E Aij

2

2

E Aij

(cid:88)

i<j

E Aij

δg(i)=g(j)

2

(cid:35)

δg(i)=g(j) + O()
1 − (cid:107)π(cid:107)2
(cid:107)π(cid:107)2
(cid:35)(cid:107)π(cid:107)2

δg(i)=g(j)

1

2

(cid:107)π(cid:107)2

1

(cid:88)
(cid:88)

i<j

i<j

E Aij(πi + πj)

δg(i)=g(j)

E(cid:107)d(cid:107)1
(cid:34)
E(cid:107)d(cid:107)1

E Aij

πi(cid:107)π(cid:107)1 + πj(cid:107)π(cid:107)1 − (cid:107)π(cid:107)2

2

E(cid:107)d(cid:107)1

δg(i)=g(j) + O()

δg(i)=g(j) + O()

(cid:35)(cid:107)π(cid:107)2

2

(cid:107)π(cid:107)2

1

(cid:35)(cid:107)π(cid:107)2

2

(cid:107)π(cid:107)2

1

(74)

δg(i)=g(j) + O().

δg(i)=g(j)

(75)

=

=

=

=

i<j

i<j

i<j

j=1

j=1

j=1

j=1

n(cid:88)
(cid:88)
n(cid:88)
(cid:88)
n(cid:88)
(cid:88)
− n(cid:88)
(cid:88)
n(cid:88)
n(cid:88)
− n(cid:88)
n(cid:88)
(cid:88)
− n(cid:88)

j=1

j=1

j=1

j=1

i<j

i<j

+

We will show in Step 6 that
E Aij(πi + πj)

(cid:34)
E(cid:107)d(cid:107)1
(cid:88)

E Aij

δg(i)=g(j)

πi(cid:107)π(cid:107)1 + πj(cid:107)π(cid:107)1 − (cid:107)π(cid:107)2

2

E(cid:107)d(cid:107)1

i<j

j=1

= O().

Recall from the deﬁnition of b in Eq. (4) that

n(cid:88)

(cid:88)

j=1

i<j

b =

(cid:16)E di + E dj − (cid:107)π(cid:107)2

2

E Aij

(cid:17)

δg(i)=g(j).

E(cid:107)d(cid:107)1

Then, as a consequence of Eqs. (74) and (75), we see that

a) + b) =b + O().

(76)

Inserting the results from Eq. (76) into Eq. (66) and under the assumption that
all error terms are controlled (see Step 6 below), we obtain the result of this

lemma; i.e., (cid:98)Q =

n(cid:88)

(cid:2)dw

αj

j − E dw

j

(cid:3) − n(cid:88)

(cid:2)db

βj

j − E db

j

(cid:3) + b + O().

(77)

j=1

j=1

40

Step 6: We now deﬁne and address the ﬁve error terms cited above; we call

these (1), (2), . . . , (5).

Term (1): Recalling Eq. (57), we deﬁne

n(cid:88)
n(cid:88)

j=1

1

√E dl
√E dl

(1) =

minl

(cid:107)π(cid:107)g(j),j

(ˆπj − πj)

1

(cid:32)

1

=

dj(cid:112)(cid:107)d(cid:107)1
First, we apply a Taylor expansion to ((cid:107)d(cid:107)1/ E(cid:107)d(cid:107)1)
(cid:32)(cid:115)
leading to

(cid:107)π(cid:107)g(j),j

(cid:34)

minl

j=1

1

(cid:33)

.

− πj

(cid:33)(cid:35)

−1/2 = f (x) = x−1/2 at 1,

(1) =

minl

(cid:107)π(cid:107)g(j),j

and then control the remainder using Chebyshev’s inequality. As a consequence,
we obtain from Assumption 4 (Var Aij = Θ(E Aij)) that

From Chebyshev’s inequality and Assumptions 2 and 4, we know that dj =
E dj + OP

=

1

1(cid:112)(cid:107)d(cid:107)1
n(cid:88)
√E dl
(cid:0)(cid:112)E dj
(cid:1) = E dj
n(cid:88)
√E dl
n(cid:88)

j=1

j=1

1

1

√E dl

j=1

,

.

1

dj

(cid:33)

− πj

1 + OP

(cid:2)1 + OP

1(cid:112)E(cid:107)d(cid:107)1
(cid:32)

Var(cid:107)d(cid:107)1
(E(cid:107)d(cid:107)1)2
(cid:0)1/(cid:112)E(cid:107)d(cid:107)1
(cid:1)(cid:3)
(cid:112)E(cid:107)d(cid:107)1
(cid:2)1 + OP (1/(cid:112)E dj)(cid:3). It follows that
(cid:32)E dj
(cid:33)
(cid:0)1/(cid:112)E dj
(cid:2)1 + OP
(cid:1)(cid:3)
(cid:112)E(cid:107)d(cid:107)1
πj
(cid:2)1 + OP
(cid:0)1/(cid:112)E dj
(cid:1)(cid:3)[1 − πj/(cid:107)π(cid:107)1]
(cid:105)1/2
(cid:104)
(cid:16)(cid:107)π(cid:107)2
(cid:34)

1 − (cid:107)π(cid:107)2

2/(cid:107)π(cid:107)2

− πj

1

1

1

(cid:107)π(cid:107)g(j),j

(cid:107)π(cid:107)g(j),j

2/(cid:107)π(cid:107)2

(cid:16)(cid:107)π(cid:107)2

2/(cid:107)π(cid:107)2
1 = O(1/n) (Eq. (18), following from Assumption 1), we can
Since (cid:107)π(cid:107)2
apply a convergent Taylor expansion to f (x) = (1− x)−1/2 at 0 (as in Eq. (69)).
(cid:17)2
in this Taylor expansion satis-
Furthermore, the remainder term

(cid:17)2
= O(cid:0)1/n2(cid:1) = O(cid:0)1/(cid:112)E dj
(cid:1) (Assumptions 1 and 3). Hence,
2/(cid:107)π(cid:107)2
(cid:33)(cid:35)(cid:20)
(cid:32)
n(cid:88)
1(cid:112)E dj
(cid:33)(cid:35)
(cid:32)(cid:107)π(cid:107)2

=

minl

1

√E dl

ﬁes
we obtain

(cid:107)π(cid:107)g(j),j

1 + OP

(cid:32)

(cid:34)

(cid:21)

πj

j=1

1

1

1

1 − πj(cid:107)π(cid:107)1
(cid:33)
− πj

1 +

2

=

=

minl

minl

.

− πj

1
2

(cid:107)π(cid:107)2

1

41

(cid:107)π(cid:107)g(j),j

1

πiπj

j=1

n(cid:88)
n(cid:88)
n(cid:88)

j=1

(cid:88)
(cid:88)

i(cid:54)=j

j=1

i(cid:54)=j

=

=

minl

minl

1

√E dl
√E dl

1

=

minl

1

√E dl

(cid:32)
− π2
(cid:32)
j(cid:107)π(cid:107)1
− πj(cid:107)π(cid:107)1

+

+

1
2

1
2

(cid:33)(cid:34)

(cid:32)

1(cid:112)E dj
(cid:32)

(cid:33)(cid:35)
1(cid:112)E dj

(cid:33)(cid:35)

1 + OP

(cid:34)

δg(i)=g(j)

1 + OP

πiπjδg(i)=g(j) · OP

.

(Assumption 1, Eq. (18))

(78)

(79)

Term (2): We now analyze the second error term. Recalling Eq. (60), deﬁne

(2) =

(cid:107)π(cid:107)g(j),j

1

n(cid:88)

j=1

n(cid:88)

j=1

From Chebyshev’s inequality and Assumption 4 it follows that

=

(cid:107)π(cid:107)g(j),j

1

1

E(cid:107)d(cid:107)1

1 + OP

(cid:32)

1(cid:112)E dj

(cid:33)(cid:33)

(80)

This expression is smaller than (3) as deﬁned in Eq. (81).

Term (3): We now analyze the third error term. Recalling Eq. (63), deﬁne

(cid:107)π(cid:107)2
(cid:107)π(cid:107)2

2

(cid:33)

1

πj

(cid:107)π(cid:107)2
(cid:107)π(cid:107)2

2

(cid:18) 1

1

(cid:19)

n

1

E(cid:107)d(cid:107)1

(cid:32)

dj(cid:112)E(cid:107)d(cid:107)1
E dj(cid:112)E(cid:107)d(cid:107)1
(cid:18) (cid:107)d(cid:107)1E(cid:107)d(cid:107)1
(cid:32)
(cid:112)E dj
(cid:33)
(cid:32)(cid:115)
(cid:32)(cid:114)

· OP

OP

OP

− 1

(cid:19) 1(cid:112)E dj
(cid:33)
1(cid:112)E dj
1(cid:112)E(cid:107)d(cid:107)1
(cid:33)

·

1 − πj/(cid:107)π(cid:107)1
(cid:33)
1 − (cid:107)π(cid:107)2
2/(cid:107)π(cid:107)2

1

1
n

1 +

42

Applying Chebyshev’s inequality leads to

(3) =

1
2

(cid:107)π(cid:107)g(j),j

1

n(cid:88)

j=1

j=1

j=1

1
2

n(cid:88)
n(cid:88)
n(cid:88)
n(cid:88)
n(cid:88)

j=1

j=1

j=1

=

=

=

=

=

(cid:107)π(cid:107)g(j),j

1

(cid:107)π(cid:107)g(j),j

OP

1

(cid:107)π(cid:107)g(j),j

OP

1

(cid:107)π(cid:107)g(j),j

1

(cid:107)π(cid:107)g(j),j

1

πj

πj

E dj(cid:112)E(cid:107)d(cid:107)1
E dj(cid:112)E(cid:107)d(cid:107)1
(cid:32)
(cid:32)(cid:112)E dj
(cid:115)
(cid:115)

E(cid:107)d(cid:107)1
πj(cid:107)π(cid:107)1
j(cid:107)π(cid:107)4
π2
πj(cid:107)π(cid:107)1
j(cid:107)π(cid:107)4
π2

1

1

E dj
E(cid:107)d(cid:107)1

(81)

(cid:33)

(Assumption 1, Eqs. (18), (69))

Term (4): We now analyze the fourth error term. Recalling Eq. (72), deﬁne

.

(Assumption 1)

(82)

j=1

n(cid:88)
n(cid:88)
(cid:80)

j=1

=

=

(cid:107)π(cid:107)g(j),j

1

πj

(cid:107)π(cid:107)g(j),j

πj

1

(cid:80)

(cid:115)
(cid:115)

1

OP

(cid:32)(cid:114)
(cid:32)(cid:114)
πj(cid:107)π(cid:107)1(cid:107)π(cid:107)2
1 − πj/(cid:107)π(cid:107)1
(cid:32)(cid:114)
E dj(cid:107)π(cid:107)2
OP

OP

1 +

1

1

1 +

1 +

(cid:33)

1
n

(cid:33)
(cid:33)

1
n

1
n

(cid:33)2(cid:35)(cid:35)

2

1

=

i<j

j=1

j=1

minl

πiπj O

i<j πiπjδg(i)=g(j)

√E dl (cid:107)π(cid:107)1
(cid:34)(cid:32)(cid:107)π(cid:107)2
(cid:34)
(cid:88)
(4) = − n(cid:88)
(cid:34)
(cid:88)
n(cid:88)
(cid:33)2 n(cid:88)
(cid:32)(cid:107)π(cid:107)2
(cid:34)(cid:107)π(cid:107)2
(cid:35) n(cid:88)
(cid:88)
(cid:18) 1
(cid:19) n(cid:88)
(cid:88)

(cid:107)π(cid:107)2
(cid:107)π(cid:107)2
(cid:107)π(cid:107)2

πiπ2
j
(cid:107)π(cid:107)1

(cid:88)

= −O

(cid:107)π(cid:107)2

(cid:107)π(cid:107)2

+ O

πiπj

i(cid:54)=j

i(cid:54)=j

1
2

1
2

j=1

j=1

j=1

i<j

+

2

1

1

2

2

1

= O

n2

j=1

i<j

δg(i)=g(j)

(cid:34)(cid:32)(cid:107)π(cid:107)2

2

(cid:107)π(cid:107)2

1

+

πiπ2
j
(cid:107)π(cid:107)1

O

(cid:33)2(cid:35)(cid:35)

(83)

δg(i)=g(j)

(84)

πiπjδg(i)=g(j)

δg(i)=g(j)

(85)

πj(cid:107)π(cid:107)1

πiπjδg(i)=g(j).

(Assumption 1, Eq. (18))

(86)

Term (5): We now analyze the ﬁfth error term. Recalling Eq. (75), deﬁne

(5) =

δg(i)=g(j)

E Aij(πi + πj)

E Aij(πi + πj)

i<j

i<j

j=1

j=1

E Aij

(cid:34)
E(cid:107)d(cid:107)1
(cid:88)

n(cid:88)
(cid:88)
− n(cid:88)
(cid:88)
n(cid:88)
E(cid:107)d(cid:107)1
n(cid:88)
(cid:88)
E Aij
E(cid:107)d(cid:107)1
n(cid:88)
(cid:88)
≤ 2 maxl πl
E(cid:107)d(cid:107)1
(cid:88)
n(cid:88)

j=1

j=1

j=1

i<j

i<j

i<j

+

=

+

E Aij
E(cid:107)d(cid:107)1

j=1

i<j

πi(cid:107)π(cid:107)1 + πj(cid:107)π(cid:107)1 − (cid:107)π(cid:107)2
(cid:35)

2

δg(i)=g(j)

2

(cid:34)
E(cid:107)d(cid:107)1
1 − (cid:107)π(cid:107)2
(cid:32)(cid:107)π(cid:107)2
(cid:33)2
(cid:107)π(cid:107)1
(cid:18)
(cid:20)
(cid:107)π(cid:107)1
1 + O
(cid:19)2
(cid:18)

2

E Aijδg(i)=g(j)

max

l

δg(i)=g(j)

δg(i)=g(j)

max

l

πl

43

(cid:35)(cid:107)π(cid:107)2

2

(cid:107)π(cid:107)2

1

(87)

δg(i)=g(j)

(cid:19)(cid:21)

πl

(Assumption 1, Eq. (18))

2 maxl πl + O(cid:0)maxl π2

l

(cid:107)π(cid:107)2

1

(cid:1)

(cid:34)
1 − (cid:107)π(cid:107)2
(cid:107)π(cid:107)2

2

1

(cid:35)−1 n(cid:88)

(cid:88)

j=1

i<j

E Aijδg(i)=g(j).

=

Applying a convergent Taylor expansion to f (x) = (1 − x)−1 at 0 with x =
(cid:107)π(cid:107)2

1 (Assumption 1 and Eq. (18)), we obtain

2/(cid:107)π(cid:107)2

(cid:18) 1

(cid:19)(cid:21) n(cid:88)

(cid:88)

n

j=1

i<j

1 + O

E Aijδg(i)=g(j)

2 maxl πl + O(cid:0)maxl π2
(cid:1)
(cid:20)
(cid:19) n(cid:88)
(cid:18) 1
(cid:88)

(cid:107)π(cid:107)2

1

l

n(cid:107)π(cid:107)1

+

1
n2

j=1

i<j

=

=O

πiπjδg(i)=g(j).

(Assumption 1)

(88)

As a consequence of Eqs. (79)–(88), we now know that the error terms

(1), (2), . . . , (5) in our analysis of modularity satisfy

(1) + (2) + (3) + (4) + (5)
= OP

+

1
n minl

√E dl

1
(cid:107)π(cid:107)1 minl

√E dl

(cid:88)

i<j

πiπjδg(i)=g(j).

(cid:19) n(cid:88)
(cid:17)

j=1

1

(cid:107)π(cid:107)1n
(cid:16)√

n2

+

1
n2 +
√E dl = o
(cid:19) n(cid:88)
(cid:88)

πiπjδg(i)=g(j).

(cid:18)

(cid:18)

From Assumption 3, it follows that minl

= o(n). Hence,

= OP

1
n minl

√E dl

+

Recall from Eq. (54) that

i<j

j=1

√E dl
(cid:80)

1
(cid:107)π(cid:107)1 minl
(cid:80)n
min(n,(cid:107)π(cid:107)1) minl

 =

j=1

i<j πiπjδg(i)=g(j)

√E dl

.

It follows that

(1) + (2) + (3) + (4) + (5) = OP ().

As a consequence, we conclude the required result of Lemma E.1; i.e.,

 n(cid:88)

(cid:98)Q = b +

(cid:2)dw

αj

j − E dw

j

n(cid:88)

(cid:3) +

(cid:2)db

βj

j − E db

j

(cid:3) + OP ().

j=1

j=1

We now derive the asymptotic distribution of modularity (cid:98)Q. Recalling the

deﬁnitions of α, β in Eqs. (52), (53), we deﬁne a sequence of random variables
via

n(cid:88)

(cid:2)dw

Xn =

αj

j − E dw

j

n(cid:88)

(cid:3) +

(cid:2)db

βj

j − E db

j

(cid:3).

(89)

j=1

j=1

44

In Lemma E.2 below we show the asymptotic behavior of Xn. The Lemma
parallels Lemma 2 in the main text.

Lemma E.2. Consider Assumptions 1–5, and suppose that the number K of
communities grows strictly more slowly than n, so that K/n → 0. Then, as
n → ∞,

− 1

(Var Xn)

2 Xn

d→ Normal(0, 1).

Proof. First we write Xn as a sum of independent, zero-mean random variables:

j=1

i<j

j=1

n(cid:88)
n(cid:88)
n(cid:88)

j=1

+

j=1

n(cid:88)
n(cid:88)
n(cid:88)
n(cid:88)

j=1

j=1

j

j

j=1

βj

αj

j=1

i(cid:54)=j

i(cid:54)=j

n(cid:88)

(cid:2)db

(cid:3) +

j − E dw

αj[Aij − E Aij]δg(i)=g(j) +

(αi + αj)[Aij − E Aij]δg(i)=g(j)

(cid:3)
j − E db
n(cid:88)
(cid:88)

(cid:2)dw
(cid:88)
(cid:88)
(cid:88)
n(cid:88)
(cid:88)
(cid:2)(αi + αj)δg(i)=g(j) + (βi + βj)δg(i)(cid:54)=g(j)
(cid:88)
(cid:2)(1 + βi + βj)δg(i)=g(j) + (βi + βj)δg(i)(cid:54)=g(j)
(cid:88)
(cid:2)δg(i)=g(j) + βi + βj
(cid:124)
(cid:88)

(βi + βj)[Aij − E Aij]δg(i)(cid:54)=g(j)

[Aij − E Aij]

(cid:123)(cid:122)

cij

(cid:3)
(cid:125)

j=1

i<j

i<j

i<j

i<j

cij[Aij − E Aij].

Xn =

=

=

=

=

=

=

βj[Aij − E Aij]δg(i)(cid:54)=g(j)

(cid:3)[Aij − E Aij]
(cid:3)[Aij − E Aij]

(90)

(91)

j=1

i<j

To apply the Lindeberg–Feller Central Limit Theorem to this sum, we show:

1. Var(cijAij) < ∞;
2. The Lyapunov condition for exponent 1 is satisﬁed; i.e.,

(cid:80)n

E(cid:104)
(cid:80)
(cid:16)(cid:80)n

i<j

(cijAij − E(cijAij))3(cid:105)
(cid:17)(cid:105)3/2
(cid:80)

j=1

i<j cijAij

Var

(cid:104)

j=1

→ 0.

Since both conditions are strongly inﬂuenced by cij, we ﬁrst show that cij =

45

O(1). From Eq. (90) and the deﬁnitions of α, β in Eqs. (52), (53), we see that
cij − δg(i)=g(j)

(cid:34) n(cid:88)
 n(cid:88)

l=1

l=1

=

=

− (cid:107)π(cid:107)g(j),j

− (cid:107)π(cid:107)g(i),i

1

(cid:107)π(cid:107)g(l),l

(cid:16)(cid:107)π(cid:107)g(l),∅

E dl
E(cid:107)d(cid:107)1
− πl

1

(cid:17)

πl

(cid:107)π(cid:107)1

1

 1 − πl(cid:107)π(cid:107)1

1 − (cid:107)π(cid:107)2
(cid:107)π(cid:107)2

2

1

(cid:35)
1(cid:112)E(cid:107)d(cid:107)1
 − (cid:107)π(cid:107)g(j),j

1

1

(cid:16)



2

1 − (cid:107)π(cid:107)2
(cid:107)π(cid:107)2
(cid:107)π(cid:107)1

1

(cid:17)− 1

2

.

− (cid:107)π(cid:107)g(i),i

1

From Assumption 1 and Eq. (17), we know that (cid:107)π(cid:107)2
1 ≤ maxi πi(cid:107)π(cid:107)1/(cid:107)π(cid:107)2
O(1/n). Hence, we can apply a convergent Taylor expansion to f (x) = (1 −
x)−α, α = 1/2, 1 at x = 0. We obtain

1 =

− (cid:107)π(cid:107)g(j),j

1

− (cid:107)π(cid:107)g(j),∅

− (cid:107)π(cid:107)g(i),∅

(cid:17)(cid:105)

2/(cid:107)π(cid:107)2
(cid:104)
1 + O(cid:16) maxi πi
(cid:107)π(cid:107)1
(cid:17)(cid:105)

(cid:107)π(cid:107)1

1

1

(cid:107)π(cid:107)1

− (cid:107)π(cid:107)g(i),i



(cid:104)
1 + O(cid:16) maxi πi
(cid:18) maxi πi
(cid:19)(cid:21)
(cid:20)

(cid:18) 1

1 + O

(cid:107)π(cid:107)1

(cid:107)π(cid:107)1

1 = O(1/n), it follows further that
(cid:19)
− (cid:107)π(cid:107)g(i),∅
1(cid:107)π(cid:107)1

(cid:18) 1

(cid:19)(cid:21)

+ O

n

n

(92)

.

(93)






=

=

=

(cid:17)2 − (cid:107)π(cid:107)2
(cid:107)π(cid:107)1
(cid:17)2

(cid:16)(cid:107)π(cid:107)k,∅
(cid:16)(cid:107)π(cid:107)k,∅

1

2

1

k=1

(cid:80)K
(cid:80)K
(cid:34)

k=1

(cid:107)π(cid:107)1

1

(cid:35)(cid:20)

2

+

+

1 + O

− (cid:107)π(cid:107)2
πj(cid:107)π(cid:107)1
πi(cid:107)π(cid:107)1
(cid:107)π(cid:107)2
Since (cid:107)π(cid:107)2
2/(cid:107)π(cid:107)2
1 ≤ maxi πi(cid:107)π(cid:107)1/(cid:107)π(cid:107)2
(cid:17)2
(cid:16)(cid:107)π(cid:107)k,∅
(cid:80)K

1

k=1

1

− (cid:107)π(cid:107)g(j),∅
1(cid:107)π(cid:107)1

(cid:107)π(cid:107)2

1

The ﬁrst term in Eq. (93) is O(1), and thus we conclude cij = O(1). This in
turn allows us to combine the relative and additive error terms. Furthermore
we see that cij is, up to an additive error term of order at most 1/n, a function
only of g(i) and g(j):

(cid:32)(cid:107)π(cid:107)k,∅

(cid:33)2

1(cid:107)π(cid:107)1

K(cid:88)

k=1

− (cid:107)π(cid:107)g(i),∅
1(cid:107)π(cid:107)1

− (cid:107)π(cid:107)g(j),∅
1(cid:107)π(cid:107)1

+ O

(cid:18) 1

(cid:19)

n

.

(94)

cij = δg(i)=g(j) +

We are now ready to address the two conditions suﬃcient for the Lindeberg-

Feller Central Limit Theorem.

Condition 1:

Var(cijAij) = c2
= c2
< ∞.

ij Var(Aij)
ijΘ(πiπj)

(Assumption 4)

(Eq. (94): cij = O(1); πi, πj ∈ R>0)

46

(Eq. (94): cij = O(1))

j=1

j=1

j=1

j=1

=

j=1

i<j

Var

j=1

i<j c2

= O

i<j c2
ij

i<j c3
ij

(cid:80)

i<j c2

ij Var Aij

ij Var Aij

i<j cijAij

Condition 2:

= O(1) ·

E(cid:104)
(cijAij − E(cijAij))3(cid:105)
(cid:80)
(cid:80)n
(cid:16)(cid:80)n
(cid:104)
(cid:17)(cid:105)3/2
(cid:80)
(Aij − E(Aij))3(cid:105)
E(cid:104)
(cid:80)
(cid:80)n
(cid:105)3/2
(cid:104)(cid:80)n
(cid:80)
(Aij − E(Aij))3(cid:105)
E(cid:104)
(cid:80)
(cid:80)n
(cid:105)3/2
(cid:104)(cid:80)n
 (cid:80)n
 (Assumption 5)
(cid:80)
(cid:104)(cid:80)n
(cid:80)

.(Eq. (94): cij = O(1)).
(cid:104)(cid:80)n
(cid:80)
For Condition 2, it remains to show that(cid:80)n
n(cid:88)
(cid:88)
(cid:1)
 n(cid:88)
iiΘ(cid:0)π2
(cid:34) K(cid:88)
+ O(cid:16)(cid:107)π(cid:107)2
(cid:17)

(cid:105)3/2
(cid:105)1/2

n(cid:88)
K(cid:88)

c2
ij Var Aij =

(Assumption 4)

c2
ijΘ(πiπj)

(cid:17)(cid:35)

ij Var Aij

ij Var Aij

ij Var Aij

(cid:80)

= O

i<j c2

=

=

i<j c2

j=1

i<j c2

c2

i

i=1

j=1

j=1

i<j

c2

j=1

c2
tkΘ

1 (cid:107)π(cid:107)t,∅

1

1

j=1

1
2

1
2

k=1

t=1

ij Var Aij → ∞:

i<j c2

j=1

i<j

i=1

j=1

(cid:88)

n(cid:88)
ijΘ(πiπj) − n(cid:88)
(cid:16)(cid:107)π(cid:107)k,∅
 K(cid:88)

l=1

1
(cid:124)
(cid:107)π(cid:107)1

1

(cid:17)2
(cid:16)(cid:107)π(cid:107)l,∅
(cid:123)(cid:122)
(cid:18) 1
(cid:19)

(cid:107)π(cid:107)1

B

n

47

Recall from Eq. (94) that cij can be written as a function of g(i) and g(j):

ctk = δt=k +

− (cid:107)π(cid:107)t,∅

1 − (cid:107)π(cid:107)k,∅

1

+O

.

2

(Eq. (94): cij = O(1))

(95)

(cid:18) 1

(cid:19)

n


(cid:125)

⇒ c2

tk = δt=k + 2δt=kB + B2 + O

(Eq. (94): cij = O(1))

.

Then, substituting ak for (cid:107)π(cid:107)k,∅

in Eq. (95) (so that (cid:107)a(cid:107)1 = (cid:107)π(cid:107)1), we obtain

K(cid:88)

K(cid:88)

k=1

t=1

c2
tkΘ(akat) =

=

δk=t + 2δk=tB + B2 + O

(cid:18) 1

n

(cid:19)(cid:21)
(cid:18) 1

n

B2 + O

Θ(akat)

(cid:19)(cid:21)

K(cid:88)

(cid:20)
K(cid:88)

k=1

t=1

Θ(akat).

(96)

We now address the two terms on the right-hand side of Eq. (96) separately:

K(cid:88)
K(cid:88)

k=1

k=1

1

t=1

(cid:20)
K(cid:88)
(1 + 2B)Θ(cid:0)a2
(cid:1) +
(cid:32) K(cid:88)

K(cid:88)

k

2
(cid:107)a(cid:107)1
(cid:107)a(cid:107)4
(cid:107)a(cid:107)2

2

1

k=1

− 4

l=1

(cid:107)a(cid:107)3
(cid:107)a(cid:107)1

3

(cid:33)

a2
l(cid:107)a(cid:107)1

− 2ak

a2
k

.

(97)

K(cid:88)

k=1

K(cid:88)

k=1

(1 + 2B)a2

(cid:20)
K(cid:88)
K(cid:88)

t=1

=

k=1

=

1
(cid:107)a(cid:107)2

B2 + O

akat

2 + 2

n

2 +

= (cid:107)a(cid:107)2

(cid:18) 1

k = (cid:107)a(cid:107)2
(cid:19)(cid:21)
(cid:40)
(cid:34) K(cid:88)
K(cid:88)
1
(cid:34)(cid:107)a(cid:107)2
(cid:107)a(cid:107)1
K(cid:88)
K(cid:88)
(cid:107)a(cid:107)1
(cid:32)(cid:107)a(cid:107)2
(cid:33)2
K(cid:88)

(cid:107)a(cid:107)1

k=1

t=1

t=1

l=1

2

1

2

(cid:35)(cid:41)2

(cid:35)2

(al)2
(cid:107)a(cid:107)1

− ak − at

akat + O

− (ak + at)

akat + O

(cid:18) 1

n

1

1

n

n

+ O

(cid:33)
(cid:32)(cid:107)a(cid:107)2
(cid:33)
(cid:32)(cid:107)a(cid:107)2
(cid:19)
akat + O
(cid:32)(cid:107)a(cid:107)2
(cid:32)(cid:107)a(cid:107)2
(cid:33)
(cid:33)

n

n

1

1

+ O

(cid:33)

=

=

=

=

K(cid:88)

t=1

1
(cid:107)a(cid:107)2

1

1

k=1

(cid:107)a(cid:107)4

K(cid:88)
(cid:34)
(cid:104)(cid:107)a(cid:107)4
(cid:104)(cid:107)a(cid:107)4
(cid:88)

1

1

1
(cid:107)a(cid:107)2

1
(cid:107)a(cid:107)2

1
(cid:107)a(cid:107)2

n(cid:88)

2 − 2(cid:107)a(cid:107)4

2 +

k + 2akat + a2
t

(ak + at) + (ak + at)2

(cid:35)
(cid:1)akat
(cid:32)(cid:107)a(cid:107)2

1

n

(cid:105)

+ O

− 2

2

(cid:107)a(cid:107)2
(cid:107)a(cid:107)1
K(cid:88)

k=1

t=1

(cid:0)a2
(cid:32)(cid:107)a(cid:107)2

1

n

2 − 2(cid:107)a(cid:107)4

2 + 2(cid:107)a(cid:107)3

2 + 2(cid:107)a(cid:107)3
(cid:105)

3(cid:107)a(cid:107)1 + 2(cid:107)a(cid:107)4
(cid:33)

2

+ O

.

3(cid:107)a(cid:107)1
(cid:32)

Thus, substituting Eqs. (97) and (98) into Eq. (95), we obtain

(cid:34)

(cid:32)

c2
ij Var Aij = Θ

(cid:107)a(cid:107)2

2 + 3

j=1

i<j

= (cid:107)a(cid:107)2

2

Θ

1 + 3

(cid:107)a(cid:107)2
(cid:107)a(cid:107)2

2

1

− 2

− 2

(cid:33)

2

(cid:107)a(cid:107)4
(cid:107)a(cid:107)2
(cid:107)a(cid:107)2(cid:107)a(cid:107)3
(cid:107)a(cid:107)1(cid:107)a(cid:107)3

3

1

2

3

(cid:107)a(cid:107)3
(cid:107)a(cid:107)1
+ O

(98)

(99)

(cid:33)
(cid:32)(cid:107)a(cid:107)2

+ O

1

(cid:107)a(cid:107)2

2

(cid:32)(cid:107)a(cid:107)2
(cid:40)

n

1

1
n

+

(cid:33)
(cid:41)(cid:33)(cid:35)

2

.

+ (cid:107)π(cid:107)2

(cid:107)π(cid:107)2
(cid:107)a(cid:107)2

2

1

48

(cid:41)(cid:33)(cid:35)

+

(cid:107)π(cid:107)2
(cid:107)π(cid:107)2

2

1

(Assumption 1)

(100)

(101)

(102)

n

(cid:33)

(cid:19)(cid:35)

(cid:18) K

n

+ O

(cid:107)a(cid:107)2
(cid:107)a(cid:107)2

2

1

1/(cid:107)a(cid:107)2

= (cid:107)a(cid:107)2

Since (cid:107)a(cid:107)1 = (cid:107)π(cid:107)1 and (cid:107)a(cid:107)2
(cid:107)a(cid:107)2
(cid:107)a(cid:107)2
(cid:107)a(cid:107)2
(cid:107)a(cid:107)2

= (cid:107)a(cid:107)2

1 + 3

1 + 3

Θ

Θ

1

2

2

2

1

− 2

− 2

≥ (cid:107)a(cid:107)2

= (cid:107)a(cid:107)2

= Θ

1 + 3

− 2

2

1

(cid:107)a(cid:107)2
(cid:107)a(cid:107)2
(cid:107)a(cid:107)2
(cid:107)a(cid:107)1
(K = o(n))

3

− 1√
3

3

2

K

1
n

+ O

(cid:40)
2 ≤ K, it follows that
(cid:107)a(cid:107)2(cid:107)a(cid:107)3
(cid:19)(cid:35)
(cid:107)a(cid:107)1(cid:107)a(cid:107)3
(cid:107)a(cid:107)2(cid:107)a(cid:107)3
(cid:107)a(cid:107)1(cid:107)a(cid:107)3
(cid:33)

(cid:32)
(cid:33)
(cid:33)
(cid:18) K
(cid:19)(cid:35)
(cid:18) K
(cid:18) K

(cid:107)a(cid:107)2
(cid:21)2
(cid:107)a(cid:107)1

(cid:19)(cid:35)

+ O

+ O

(cid:33)

n

n

3

2

+ O

+

2
3

Furthermore, from Eq. (100) we obtain that

ij Var Aij ≤ (cid:107)a(cid:107)2
c2

2

Θ

1 + 3

2

(cid:32)
(cid:34)
(cid:32)
(cid:34)
(cid:32)
(cid:34)
(cid:32)(cid:20)√
(cid:34)
(cid:17)
(cid:16)(cid:107)a(cid:107)2
(cid:88)
n(cid:88)

Θ

Θ

2

2

2

.

j=1

i<j

n(cid:88)

(cid:88)

j=1

i<j

(cid:34)

(cid:32)

(cid:17)

(cid:16)(cid:107)a(cid:107)2

2

(cid:16)

(cid:17)

(cid:107)a(cid:107)2

2 ≥ (cid:107)π(cid:107)2
(cid:16) n

K

1

(cid:17)

= ω

K
= ω(1).

K(cid:107)a(cid:107)2

2 ≥ (cid:107)a(cid:107)2

1

(Assumption 2)

(K = o(n))

and thus, since (cid:107)a(cid:107)2
whenever K = o(n),

2 ≤ (cid:107)a(cid:107)2

1, we conclude from Eqs. (101) and (102) that

c2
ij Var Aij = Θ

.

(103)

Now, since by hypothesis (cid:107)π(cid:107)1 → ∞, and by construction (cid:107)a(cid:107)1 = (cid:107)π(cid:107)1, we

see immediately that

Thus the Lyapunov condition is satisﬁed, and we obtain the claimed result that

(Var Xn)

− 1

2 Xn

d→ Normal(0, 1)

via the Lindeberg–Feller Central Limit Theorem.

Combining Lemma E.1 and Eq. (89), we obtain that modularity (cid:98)Q satisﬁes

⇒ (Var Xn)

− 1

2

2 Xn + (Var Xn)

− 1
2Op().

(104)

(cid:98)Q = b + Xn + Op()
(cid:17)

− 1

= (Var Xn)

(cid:16)(cid:98)Q − b

49

We know from Lemma E.2 that

− 1

(Var Xn)

2 Xn

d→ Normal(0, 1).

Now, we will show that

As in Lemma E.2, deﬁne

(Var Xn)

− 1
2  n→ 0.

n(cid:88)

ak = (cid:107)π(cid:107)k,∅

1 =

πiδg(i)=k,

whence

n(cid:88)

(cid:88)

j=1

i<j

i=1

πiπjδg(i)=g(j) ≤ 1
2

(cid:107)a(cid:107)2
2.

Using this notation, we have from Eqs. (54) and (103) that

(cid:16)(cid:107)a(cid:107)2

2

and Var Xn = Θ

(cid:17)

0 ≤  ≤

(cid:107)a(cid:107)2

2

min(n,(cid:107)π(cid:107)1) minl

√E dl

, respectively. It follows that

(Var Xn)

− 1
2  = O

(cid:107)a(cid:107)−1

2

(cid:32)
(cid:118)(cid:117)(cid:117)(cid:116)
(cid:118)(cid:117)(cid:117)(cid:116)
(cid:118)(cid:117)(cid:117)(cid:116)

= O

= O

= o

(cid:33)

minl E dl

(cid:107)a(cid:107)2

2

1

2

(cid:17)
(cid:17)

(cid:107)a(cid:107)2
n2,(cid:107)π(cid:107)2

min(n,(cid:107)π(cid:107)1) minl
(cid:16)
(cid:16)
(cid:16)

(cid:107)π(cid:107)1
n2,(cid:107)π(cid:107)2

n3/2, n−1/2(cid:107)π(cid:107)2

(cid:107)π(cid:107)1

(cid:17)

minl πl

1

1

min

min

min

√E dl


 (Assumption 1)
 (Assumption 2)

= o(1).

(Assumption 2 and 3)

(105)

We are now ready to complete the proof of Theorem E.1. Observe from (5)

and (90) that s as deﬁned in the statement of Theorem E.1 satisﬁes

s2 = Var Xn.

Combining the results from Eqs. (104), (105) and Lemma E.2 using Slutsky’s
Theorem, we conclude the overall result of this theorem; i.e.,

(cid:98)Q − b

s

d→ Normal(0, 1).

50

F Proof of Theorem 3

To add interpretability to the coeﬃcients α = 0.5 + β and β for the decomposi-
tion of modularity in Theorem 3 in the main text, we change their formulation
from the one in Lemma E.1 in the proof of Theorem E.1 (see Eq. (107) below)
to β∗
j in Eq. 106 below. By doing so, we add an error term that asymptotically
wears oﬀ. More formally, we obtain the following corollary.
√
Corollary F.1. Consider Assumptions 1–4 (πi/(cid:107)π(cid:107)1 = O(1/n), πi = ω(1/
√
πi = o(

n), E Aij = Θ(Var Aij)). Then, the following identity holds:

n),

(cid:98)Q − b =

(cid:2)dw

α∗

j

j − E dw

j

n(cid:88)

(cid:3) +

(cid:2)db

β∗

j

j − E db

j

(cid:3) + OP ()

with α∗

j = 0.5 + β∗

j and

β∗
j =

E dw
E dl

l

l=1

l=1

j=1

− E dw
E dj

j

.

Proof. Recall from Lemma E.1 in the proof of Theorem E.1 that

(cid:2)dw

(cid:3) +

n(cid:88)

j=1

αj

j − E dw

j

βj

j − E db

j

(cid:3) + OP ()

(106)

j=1

 n(cid:88)
(cid:80)n
2(cid:80)n
 n(cid:88)
n(cid:88)

j=1

l=1

(cid:98)Q = b +
(cid:34)

βj =

1
2

where

(cid:2)db
(cid:35)

(cid:107)π(cid:107)g(l),l

1

E dl
E(cid:107)d(cid:107)1

− (cid:107)π(cid:107)g(j),j

1

1(cid:112)E(cid:107)d(cid:107)1

.

(107)

We ﬁrst address how βj and β∗

j relate:

(cid:107)π(cid:107)g(l),l

1

− (cid:107)π(cid:107)g(j),j

E dl
E(cid:107)d(cid:107)1
E Almδg(l)=g(m)

1

(cid:107)π(cid:107)1(1 − πl/(cid:107)π(cid:107)1)

E Almδg(l)=g(m)

(cid:107)π(cid:107)1(1 − πl/(cid:107)π(cid:107)1)

m<l

E Alm

(cid:35)

1(cid:112)E(cid:107)d(cid:107)1
(cid:112)E(cid:107)d(cid:107)1

(cid:107)π(cid:107)1

βj =

=

=

l=1

1
2

(cid:34)
n(cid:88)
(cid:34)(cid:80)n
(cid:80)
(cid:34)(cid:80)n
(cid:80)
2(cid:80)n
(cid:113)

l=1

l=1

·

m<l

(cid:112)E(cid:107)d(cid:107)1
(cid:80)

m<l

l=1
1
1 − (cid:107)π(cid:107)2

2/(cid:107)π(cid:107)2

1

(cid:35)
1(cid:112)E(cid:107)d(cid:107)1
(cid:35)

1

− (cid:107)π(cid:107)g(j),j
− (cid:107)π(cid:107)g(j),j
1(cid:107)π(cid:107)1

1 ≤ maxi πi(cid:107)π(cid:107)1/(cid:107)π(cid:107)2
From Assumption 1 and Eq. (17), we know that (cid:107)π(cid:107)2
O(1/n). Hence, we can apply a convergent Taylor expansion to f (x) = (1 −

2/(cid:107)π(cid:107)2

1 =

51

x)−1/2 at x = 0. We obtain

·

l=1

l=1

l=1

l=1

m<l

m<l

m<l

m<l

E Alm

E Alm

1 + O

(cid:19)(cid:21)

E Almδg(l)=g(m)

E Almδg(l)=g(m)

(cid:34)(cid:80)n
(cid:80)
(cid:80)
2(cid:80)n
(cid:34)(cid:80)n
(cid:80)
2(cid:80)n
(cid:80)
(cid:18) maxi πi
(cid:20)
(cid:34)(cid:80)n
(cid:80)
(cid:107)π(cid:107)1
(cid:80)
2(cid:80)n
E Almδg(l)=g(m)
(cid:20)(cid:80)n
2(cid:80)n
− E dw
(cid:20)(cid:80)n
E dj
2(cid:80)n
− E dw
(cid:19)(cid:21)
(cid:20)
E dj

E dw
E dl
E dw
E dl

(cid:21)(cid:20)
(cid:21)(cid:20)

1 + O

1 + O

E Alm

(cid:18) 1

m<l

m<l

l=1

l=1

l=1

l=1

l=1

l=1

j

j

l

l

=

=

=

=

=

= β∗

j

1 + O

n

(cid:35)(cid:20)

1 + O

− (cid:107)π(cid:107)g(j),j
1(cid:107)π(cid:107)1

−

πj(cid:107)π(cid:107)g(j),j

1

πj(cid:107)π(cid:107)1(1 − πj/(cid:107)π(cid:107)1)

(cid:19)(cid:21)

(cid:18) maxi πi
(cid:35)(cid:18)
1 − πj(cid:107)π(cid:107)1

(cid:107)π(cid:107)1

(cid:19)

(cid:35)(cid:20)

1 + O

(cid:19)(cid:21)

(cid:18) maxi πi

(cid:107)π(cid:107)1

πj(cid:107)π(cid:107)g(j),j

−

1

(cid:19)(cid:21)

(cid:18) maxi πi
πj(cid:107)π(cid:107)1(1 − πj/(cid:107)π(cid:107)1)
(cid:19)(cid:21)
(cid:18) 1
(cid:107)π(cid:107)1

(Assumption 1).

n

.

(108)

We now will substitute Eq. (108) into the result of Lemma E.1. Therefore, ﬁrst
recall from Lemma E.1 that

(cid:3) + OP ()

j

(cid:3) + OP ().
(cid:3) + OP ()

j − E dw

j

j − E db

j − E dw

j

j − E db

j

From Eq. (108), it follows that

(cid:3) +

j

=

=

j=1

αj

j − E dw

(cid:98)Q − b
 n(cid:88)
(cid:2)dw
 n(cid:88)
(0.5 + βj)(cid:2)dw
n(cid:88)
(cid:0)0.5 + β∗
(cid:1)(cid:2)dw
 1
n(cid:88)

+ O

j=1

j=1

=

j

n

j=1

We now address the error term:

j [dj − E dj] =
β∗

n(cid:88)

j=1

1
n

j [dj − E dj]
β∗
n(cid:88)
 1

= OP

1
n

j=1

j

j=1

j=1

βj

βj

(cid:2)db
(cid:2)db

n(cid:88)
(cid:2)db
j − E db
n(cid:88)
(cid:3) +
n(cid:88)
(cid:3) +
.
(cid:112)E dj
(cid:18)(cid:80)n
n(cid:88)
2(cid:80)n

β∗

β∗

j=1

l=1

n

j

j

l=1

j=1

52

(Chenyshev’s inequality)

(cid:19)(cid:112)E dj



E dw
E dl

l

E dw
E dj

j

+

n

 1



= OP

= OP

= OP

n(cid:88)

j=1

l=1

(cid:32)(cid:80)n
2(cid:80)n
√E dl

l=1

1
n minl

1
n minl

√E dl

l

E dw
E dl

E dj(cid:112)E dj
(cid:80)n
2(cid:80)n
n(cid:88)
(cid:88)

E dw

l=1

l

l=1

j=1

i(cid:54)=j

+

E dw

(cid:33)
j(cid:112)E dj
(cid:80)n
n(cid:88)


E dj

E dl

j=1

j=1

+

πiπjδg(i)=g(j)



E dw

j

As a consequence, we conclude the required result of Corollary F.1; i.e.,

= OP ().

(cid:98)Q = b +

 n(cid:88)

(cid:2)dw

α∗

j

j − E dw

j

n(cid:88)

(cid:3) +

(cid:2)db

β∗

j

j − E db

j

(cid:3) + OP ().

j=1

j=1

G Approximation of the bias of modularity

We state in the main text that the shift of modularity b in Theorem E.1 Eq. (4)
is equal to the approximate bias b(cid:48) to leading order; with

n(cid:88)

(cid:18)
(cid:88)

(cid:19)

δg(i)=g(j).

E Aij − E didj
E(cid:107)d(cid:107)1
More formally, we obtain the following Lemma.

b(cid:48) =

j=1

i<j

Lemma G.1. Consider Assumptions 1 and 2. Then it holds for b in Eq. (4)
that

Proof. Recall from Theorem E.1 Eq. (4) that

b =

j=1

n(cid:88)
n(cid:88)
n(cid:88)

j=1

i<j

(cid:88)
(cid:88)
(cid:88)

i<j

j=1

i<j

+

(cid:32)

b =

=

=

(cid:18) 1

(cid:19)(cid:21)(cid:19)

n3/2

1 + O

δg(i)=g(j).

n(cid:88)

j=1

(cid:18)
(cid:20)
(cid:88)
E Aij − E didj
E(cid:107)d(cid:107)1
(cid:16)E di + E dj − (cid:107)π(cid:107)2

i<j

2

(cid:17)

E Aij

E(cid:107)d(cid:107)1

δg(i)=g(j)

i πj((cid:107)π(cid:107)1 − πi) + πiπ2
π2

j ((cid:107)π(cid:107)1 − πj) − πiπj(cid:107)π(cid:107)2
E(cid:107)d(cid:107)1
1 + Var Aij − Var Aij

2

πiπj(cid:107)π(cid:107)2

1 − πiπj(cid:107)π(cid:107)2

E(cid:107)d(cid:107)1

i πj((cid:107)π(cid:107)1 − πi) + πiπ2
π2

j ((cid:107)π(cid:107)1 − πj) − πiπj(cid:107)π(cid:107)2
E(cid:107)d(cid:107)1

2

53

δg(i)=g(j)

(cid:33)

δg(i)=g(j)

i πj − πiπjπj(cid:107)π(cid:107)1 + πiπ3

j

(cid:33)

δg(i)=g(j)

2

i<j

1 − (cid:107)π(cid:107)2

+ Var Aij − Var Aij

1 − πiπjπi(cid:107)π(cid:107)1 + π3
E(cid:107)d(cid:107)1
+ Var Aij − Var Aij

(cid:17)
E(cid:107)d(cid:107)1
(cid:17)
E(cid:107)d(cid:107)1

(cid:16)(cid:107)π(cid:107)2
(cid:32) πiπj
(cid:88)
− πiπj(cid:107)π(cid:107)2
(cid:16)(cid:107)π(cid:107)2
(cid:32) πiπj
(cid:88)
− πiπj((cid:107)π(cid:107)1 − πi)((cid:107)π(cid:107)1 − πj) + π3
(cid:32)
(cid:88)
E Aij − E di E dj + Var Aij + π3
E(cid:107)d(cid:107)1

1 − (cid:107)π(cid:107)2

E(cid:107)d(cid:107)1

i πj + πiπ3
j

i<j

2

i πj + πiπ3

i<j

n(cid:88)

j=1

n(cid:88)

j=1

n(cid:88)

j=1

=

=

=

(cid:33)

δg(i)=g(j)
j − Var Aij

(cid:33)

δg(i)=g(j).

Recall from Eq. (11) that cov(di, dj) = Var Aij for i (cid:54)= j. Furthermore, it holds
that E didj = E di E dj + cov(di, dj). Hence,

n(cid:88)
n(cid:88)

j=1

(cid:88)
(cid:88)

i<j

j=1

i<j

(cid:32)
E Aij − E didj + π3
(cid:32)
(cid:34)
E Aij − E didj
E(cid:107)d(cid:107)1

1 +

=

=

(cid:33)

j − Var Aij

i πj + πiπ3
E(cid:107)d(cid:107)1
π3
i πj + πiπ3

j − Var Aij

E didj

δg(i)=g(j)

(cid:35)(cid:33)

δg(i)=g(j).

We now deﬁne and analyze the error term:
j − Var Aij

π3
i πj + πiπ3

3 =

=

=

E didj

j − Var Aij
π3
i πj + πiπ3
E di E dj + Var Aij
i πj + πiπ3
π3

= Θ

1

π3
i πj + πiπ3

j − Var Aij
(cid:33)

πiπj(cid:107)π(cid:107)2
j − 1

j − πiπj
(cid:33)

(cid:32)
πiπj((cid:107)π(cid:107)1 − πi)((cid:107)π(cid:107)1 − πj) + Var Aij
(cid:32)

(cid:18) 1

 (Assumption 1)

1
n2,(cid:107)π(cid:107)2

π2
i + π2
(cid:107)π(cid:107)2

(cid:110)
(cid:19)

(cid:111)

min

1

1

.

(Assumption 2)

= Θ

= O

= O

n3/2

(Assumption 1)

54

The required result follows; i.e.,

n(cid:88)

(cid:88)

(cid:18)

b =

j=1

i<j

(cid:20)

(cid:18) 1

n3/2

(cid:19)(cid:21)(cid:19)

δg(i)=g(j).

(Assumption 2)

E Aij − E didj
E(cid:107)d(cid:107)1

1 + O

References

[1] Newman MEJ, Girvan M (2004) Finding and evaluating community struc-

ture in networks. Phys Rev E, 69:1–15.

[2] Holland PW, Laskey KB, Leinhardt S (1983) Stochastic blockmodels: First

steps. Soc Netw, 5:109–137.

[3] Hoﬀ PD, Raftery AE, Handcock MS (2002) Latent space approaches to

social network analysis. J Amer Statist Assoc, 97:1090–1098.

[4] Zhang Y, Levina E, Zhu, J (2015) Community detection in networks with

node features. Unpublished manuscript, arXiv:1509.01173.

[5] Fosdick BK, Hoﬀ PD (2015) Testing and modeling dependencies between

a network and nodal attributes. J Amer Statist Assoc, 110:1047–1056.

[6] Arias-Castro E, Verzelen N (2014) Community detection in dense random

networks. Ann Statist, 42:940–969.

[7] Bickel PJ, Sarkar P (2016) Hypothesis testing for automated community

detection in networks. J R Statist Soc B, 78:253–273.

[8] Chung F, Lu L (2002) The average distances in random graphs with given

expected degrees. Proc Natl Acad Sci USA, 99:15879–15882.

[9] Resnick MD, Bearman PS, Blum RW, Bauman KE, Harris KM, Jones J,
Tabor J, Beuhring T, Sieving RE, Shew M, Ireland M, Bearinger LH, Udry
JR (1997) Protecting adolescents from harm: Findings from the National
Longitudinal Study on Adolescent Health. J Amer Med Assoc, 278:823–832.

[10] Perry PO, Wolfe PJ (2012) Null models for network data. Unpublished

manuscript, arXiv:1201.5871.

[11] Olhede SC, Wolfe PJ (2012) Degree-based network models. Unpublished

manuscript, arXiv:1211.6537.

[12] Newman MEJ (2006) Modularity and community structure in networks.

Proc Natl Acad Sci, 103:8577–8582.

[13] Gleiser PM, Danon L (2003) Community structure in jazz. Adv Complex

Syst, 6:565–573.

55

[14] Adamic L, Glance N (2005) The political blogosphere and the 2004 US
election: Divided they blog. Proceedings of the 3rd International Workshop
on Link Discovery (ACM Press, New York), 36–43.

[15] Newman MEJ (2001) The structure of scientiﬁc collaboration networks.

Proc Natl Acad Sci, 98:404–409.

[16] Duch J, Arenas A (2005) Community detection in complex networks using

extremal optimization. Phys Rev E, 72:027104.

[17] Zhou Y, Goldberg M, Magdon-Ismail M, Wallace WA (2007) Strategies for
cleaning organizational emails with an application to Enron email dataset.
5th Annual Conference of the North American Association for Computa-
tional Social and Organizational Science (NAACSOS, Pittsburgh, PA).

[18] Perry PO, Wolfe PJ (2013) Point process modelling for directed interaction

networks. J R Statist Soc B, 75:821–849.

[19] Cameron AC, Trivedi PK (1986) Econometric models based on count data.
Comparisons and applications of some estimators and tests. J Appl Econo-
metrics, 1:29–53.

[20] Brockwell PJ, Davis RA (1991) Time Series: Theory and Methods. New

York: Springer.

[21] Lehmann EL (1999) Elements of Large-Sample Theory. New York:

Springer.

[22] Billingsley P (1995) Probability and Measure. New York: John Wiley &

Sons.

56

