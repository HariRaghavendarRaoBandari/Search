6
1
0
2

 
r
a

 

M
7
1

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
4
3
3
5
0

.

3
0
6
1
:
v
i
X
r
a

A general convex framework for multiple testing with prior

information
Edgar Dobriban∗

Stanford University

Abstract

Using prior information may improve power in frequentist multiple testing. P-value weight-
ing is a promising methodology where each test is conducted at a diﬀerent level, using critical
values based on independent prior data. However, existing methods are limited, and do not
allow the user to specify properties of the weights that are desired in practice, such as bound-
edness, or monotonicity in the strength of prior evidence.

Here we develop a general framework for p-value weighting based on convex optimization.
This allows ﬂexible constraints and leads to a variety of new methods, such as bounded and
monotone weights, stratiﬁed weights, and smooth weights.
It also recovers several existing
heuristics. Finally we focus on the promising special case of bounded monotone weights. These
are appealing as they increase with the strength of prior evidence, and stable because they are
within pre-speciﬁed bounds. We show they have good empirical power in the analysis of
genome-wide association studies, better than any weighting method we have seen.

1 Introduction

Multiple hypothesis testing problems are common in many scientiﬁc areas, including genomics,
neuroscience, and climate studies. Often we have p-values Pi for null hypotheses Hi, i = 1, . . . , J,
and we want to ﬁnd out which ones are signiﬁcant. We commonly control a form of frequentist
type I error, like the family-wise error rate (FWER) or the false discovery rate (FDR). The FWER
is controlled at level α by the Bonferroni procedure, which declares signiﬁcant the i-th hypothesis
if Pi ≤ α/J. However, if J is large, say at least a few millions, this method may have small power.
To improve power, it is becoming increasingly common to use prior information to “focus in”
on some subset of the hypotheses. The classical “candidate study” is an example where we simply
look at the most promising few hypotheses based on our prior knowledge. A more general approach
is p-value weighting (reviewed in Roeder and Wasserman, 2009; Gui et al., 2012). We form weights
wi ≥ 0 based on independent prior information, and use the weighted p-values Qi = Pi/wi for
signiﬁcant if Qi ≤ α/J, controls the FWER at level α.

multiple testing. If(cid:80) wi = J, the weighted Bonferroni procedure that declares the i-th hypothesis

This mode of inference blends frequentist statistics—by controlling type I error—and Bayesian
statistics—by using prior information. These methods follow George Box’s advice to be Bayesian
∗E-mail: dobriban@stanford.edu. Supported in part by NSF grants DMS-1418362 and DMS-1407813, and by an

HHMI International Student Research Fellowship.

1

when predicting but frequentist when testing (Box, 1980). In the biomedical sciences, there is a
growing number of successful applications (see Table 1 for a few recent examples). Given the large
number of studies and datasets that can potentially serve as prior information for any new study,
the number of applications may grow sharply in the future.

Table 1: Some recent uses of prior information in biomedical studies

Source
Saccone et al. (2007)
Andreassen et al. (2013)
Li et al. (2013)
Rietveld et al. (2014)
Fortney et al. (2015)
Sveinbjornsson et al. (2016)

Current GWAS
nicotine dependence
schizophrenia
childhood asthma
cognitive performance
extreme longevity
aggregate over broad set

Prior
nicotine receptor status
cardiovascular disease GWAS
eQTL
educational attainment GWAS
age-related traits
annotation in established hits

To meet this demand, many p-value weighting and related methods for improving power in
multiple testing have been developed. Representative examples include Spjøtvoll (1972), Carlin
and Louis (1985), Benjamini and Hochberg (1997), Westfall et al. (1998), Westfall and Krishen
(2001), Genovese et al. (2006), Rubin et al. (2006), Sun et al. (2006), Storey (2007), Eskin (2008),
Roeder and Wasserman (2009), Roquain and Van De Wiel (2009), Pe˜na et al. (2011), Rietveld et al.
(2014), Basu et al. (2015), as well as our own earlier work (Dobriban et al., 2015). These and others
will be discussed in Section 2.2 after presenting our methods.

Despite this work, the current p-value weighting methods are limited, because they do not allow
the investigator to specify certain reasonable requirements for the weights. To explain this, consider
the Gaussian sequence model—the focus of a lot of work—where the observed data are test statistics
Ti ∼ N (µi, 1), and the null hypotheses are Hi : µi ≥ 0 for the eﬀect sizes µi. In this model it is
possible to ﬁnd the optimal Spjøtvoll weights maximizing expected power, as a function the prior
eﬀect size µi (e.g., Spjøtvoll, 1972; Westfall et al., 1998; Rubin et al., 2006; Eskin, 2008; Roeder
and Wasserman, 2009). In practice, empirical estimates of the µi are used.

However, these weights have certain undesired and unintuitive properties. For instance, they are
not monotonic in the magnitude of the prior eﬀect size µi. In our experience, this is unappealing to
practitioners, who would prefer monotonic weights, as larger eﬀects are worth more. In addition,
the weights can take extreme values such as 108 or 10−100 (e.g., Roeder and Wasserman, 2009;
Dobriban et al., 2015). These extreme values may lead to instability, because the weighted and
unweighted p-values will diﬀer greatly. Small weights are risky, because they can make a very strong
p-value insigniﬁcant.

This could be ﬁxed by adding constraints to the optimization problem deﬁning the weights. In
general, however, this does not seem feasible. The general framework where µi have proper prior
distributions leads to diﬃcult nonconvex problems, for which the existing optimization algorithms
are slow and have no guarantees for ﬁnding the global optimum (Westfall et al., 1998; Westfall and
Krishen, 2001). In certain special cases, such a Gaussian prior on the eﬀect sizes, it is possible
to ﬁnd nearly optimal weights using eﬃcient algorithms (Dobriban et al., 2015). However, that
framework does not extend to allow constraints in an obvious way.

In this paper, we show how to allow general convex constraints in the Gaussian means model
where µi are considered ﬁxed. It was apparently only recently noted that this leads to a convex
optimization problem (Dobriban et al., 2015). Previous approaches worked with an equivalent
formulation that did not exploit convexity (Rubin et al., 2006; Roeder and Wasserman, 2009). Our

2

key contribution is to take full advantage of the convexity (Sec. 2), and to show that many useful
constraints can be imposed in a simple and direct way (Sec. 2.1).
In particular, we show how
to obtain bounded and monotone weights, stratiﬁed and smooth weights. This recovers several
existing heuristics. We explain how each of these properties may be desirable in certain settings.

We then focus on the particular class of monotone and bounded weights (Sec. 3). We illustrate
that they do not lose too much power compared to the optimal Spjøtvoll weights (Sec. 3.1). We
analyze several publicly available genome-wide association studies (GWAS), and show that mono-
tone bounded weights lead to the largest number of hits among several methods (Sec. 3.2). We also
provide an eﬃcient optimization method for bounded weights, and report numerical experiments
conﬁrming its eﬃciency (Sec. 4). A MATLAB implementation of our methods, as well as the code
to reproduce our computational results is available at github.com/dobriban/pvalue_weighting_
matlab, and will be provided in the software supplement.
A general conclusion of our work is that imposing a lower bound such as wi ≥ 1/2 can drastically
improve the empirical performance of p-value weighting methods. The reason is that the current
p-values Pi are multiplied by a small constant—at most two in the example above—and hence
signiﬁcant eﬀects stay signiﬁcant. We think that this simple observation may go a long way in
making p-value methods routinely applicable in large-scale projects like those common in genomics.

2 Convex P-value weights
To explain our general framework, suppose we observe Ti ∼ N (µi, 1), i = 1, . . . , J and want to test
Hi : µi ≥ 0 against µi < 0 simultaneously. We construct p-values Pi = Φ(Ti), and reject Hi if

Pi ≤ qwi, with wi ≥ 0 and(cid:80) wi = J. This is the weighted Bonferroni procedure, which controls

the family-wise error rate—the probability of making any false rejections—at level α = Jq. The
weights are based on independent prior data. Without loss of generality we assume wi ≤ 1/q.

To optimize the choice of weights, suppose for a moment that we actually know µi, while of
course in practice we will use empirical estimates based on independent studies. If µi were known,
we would assign zero weight to any non-negative µi; so in the remainder we will assume that all
µi < 0. We can choose the remaining weights to maximize the expected number of discoveries. The

power to reject Hi is Φ(cid:0)Φ−1 (qwi) − µi
J(cid:88)

max

w∈[0,1/q]J

(cid:1), leading to the Gaussian Spjøtvoll weights problem:
Φ(cid:0)Φ−1 (qwi) − µi

J(cid:88)

wi = J.

(cid:1)

s.t.

i=1

i=1

(1)

Following up on the general framework of Spjøtvoll (1972), independently Rubin et al. (2006)
and Roeder and Wasserman (2009) showed that the weights have the near-explicit form wi =
Φ(µi/2+c/µi)/q, for the unique c > 0 such that the weights sum to J. They studied the Lagrangian
of the problem under the change of variable ci = Φ−1(qwi), without using convexity. However, the

objective is in fact concave, because the function w → Φ(cid:0)Φ−1 (qw) − µ(cid:1) is strictly concave if µ < 0.

In this special context, convexity was apparently only recently noted (Dobriban et al., 2015), but
of course the general property is well-known and related to the monotone likelihood ratio property
of the Gaussian location problem (e.g., Lehmann and Romano, 2005, p. 101, Problem 3.41).
To exploit this, let S = [0, 1/q]J and fk : S → R, k = 1, . . . , K be convex and twice continuously
diﬀerentiable closed functions, encoding convex inequality constraints fk(w) ≤ 0. Let A ∈ RL·J be
a matrix encoding equality constraints Aw = b. We let the vector (1, 1, . . . , 1)(cid:62) ∈ R1·J be the ﬁrst
row of A, and J be the ﬁrst entry of b. We assume rank A = L < J. We also assume that the

3

problem is strictly feasible, and is solvable, i.e., an optimal w∗ exists. In the following examples,
one can check these conditions easily.
fk(w) ≤ 0 andAw = b to the classical Spjøtvoll weights problem:

We now formulate the general constrained Spjøtvoll weights problem, which adds the constraints

J(cid:88)

Φ(cid:0)Φ−1 (qwi) − µi

(cid:1)

max

w∈[0,1/q]J

i=1

s.t. fk(w) ≤ 0,

k = 1, . . . , K

Aw = b.

by l = 1, 2, . . ., replacing the inequality constraints by a penalty −(1/tl) (cid:80)

This problem can be solved, for instance, using the log-barrier interior point method (e.g., Boyd
and Vandenberghe, 2004, Sec. 11). This solves a sequence of equality-constrained problems indexed
k log(−fk(w)) for an
increasing sequence of tl. The equality-constrained problems are solved using Newton’s method.
While in principle this solves the original problem, it must of course be implemented eﬃciently,
taking into account that the problems of interest have millions of variables. However, we can at
least state a general convergence result.

Proposition 2.1 (Convergence of barrier method for constrained Spjøtvoll weights). Under the
above assumptions, the barrier method converges for solving the convex constrained Spjøtvoll weights
optimization problem.

Proof. This is a direct consequence of the general convergence analysis for the barrier method, see
p. 577, Sec. 11.3.3, in Boyd and Vandenberghe (2004). Our assumptions ensure that all required
conditions are met. The strong convexity is ensured by the compactness of the set S.

2.1 Examples

We now give several examples of convex constraints.

1. Monotone weights. Suppose the means are sorted such that 0 > µ1 ≥ µ2 ≥ . . . ≥ µJ .
Monotone weights require that larger absolute eﬀects |µi| have a larger weight, so that w1 ≤
w2 ≤ . . . ≤ wJ .
Unconstrained Spjøtvoll weights are not monotone in general. However, monotonicity is intu-
itively desirable, as larger eﬀects are worth more. Many popular weighting schemes are mono-
tone, including exponential weights wi ∝ exp(β|µi|) and cumulative weights wi ∝ Φ(|µi|− B),
for B > 0, both proposed in Roeder et al. (2006). The weights of Li et al. (2013) based
on normalized versions of (−2 log ˜Pj)1/2, where ˜Pj are p-values from independent expression
Quantitative Trait Locus (eQTL) studies, are also monotone in the strength of the prior
information.

2. Bounded weights. Boundedness requires that l ≤ wi ≤ u for two constants 0 ≤ l < 1 < u.
This ensures that the current p-values Pi get multiplied by at most 1/l. Hence, if l = 1/2 and
the original p-value cutoﬀ is—say—the conventional threshold q = 5 · 10−8 for genome-wide
signiﬁcance, then each hypothesis with p-value Pi ≤ ql = 2.5· 10−8 is signiﬁcant, regardless of
the strength of prior information. This is desirable as the prior information is often unreliable.

4

Small weights are risky, as they can weaken strong p-values. For instance if Pi = 10−10 but
wi = 10−9, then the weighted p-value is a meager Qi = 0.1.
Choosing the lower bound presents a bias-variance tradeoﬀ. A large lower bound, say 0.5,
has “low variance” as the weighted p-values do not change much. However it potentially has
“high bias” as the truly optimal Spjøtvoll weights may be small. Empirically, we found that
a lower bound in the range 0.01-0.5 works well (see the data analysis section), but more work
is required to explore this issue in speciﬁc applications.
Using an upper bound u, no hypothesis with p-value Pi ≤ qu is rejected. For instance, if
q = 5 · 10−8 and u = 10, then only the p-values Pi ≤ 5 · 10−7 have a chance of being rejected.
The upper bound ensures that we do not place too much weight on any hypothesis. However,
in our simulations it seems a nonzero lower bound l > 0 automatically leads to upper bounded
weights. Therefore the upper bound seems less critical.

3. Stratiﬁed weights. Let S1, . . . , Sk be disjoint subsets of {1, . . . , J}. With stratiﬁed weights,
we want to assign an equal weight to the hypotheses in the same subset Si, for each i. This
is the linear equality constraint that wj = wk, for all j, k ∈ Si, and for all i.
Stratiﬁcation is natural when eﬀects are grouped, say into diﬀerent functional classes of genetic
variants in genome-wide association studies. The stratiﬁed FDR method (Sun et al., 2006), is
essentially the equivalent formulation targeting FDR instead of FWER. It controls the FDR
separately in diﬀerent strata, such that the overall FDR is controlled.

Related ideas were used informally in previous work.
In a study of nicotine dependence
with approximately 3700 genetic variants, Saccone et al. (2007) improve power by giving
nicotine receptor genes ten times the weight of other candidate genes. More generally, Roeder
and Wasserman (2009) study binary weights that take only two possible values. Similarly,
Sveinbjornsson et al. (2016) assign weights to classes of variants in GWAS based on functional
annotation. As another example, Eskin (2008) groups polymorphisms by location in the
genome, assigning them to a tag and requiring the same weight for each tag. Stratiﬁed
weights are an example of our general convex framework.

4. Smooth weights. If there is some spatial structure on the hypotheses, then it may be rea-
sonable to require that the weights be “smooth” with respect to this structure. For instance,
genetic variants are aligned on chromosomes, and the weights should perhaps be smoothly
varying as a function the position. This can be achieved in many ways, borrowing from the
vast literature on regularization in statistics. A simple example is to add a total variation

constraint(cid:80)

i |wi − wi+1| ≤ εJ, for ε = 0.01 say.

A related idea appears in Rubin et al. (2006), who show in simulations that power improves
substantially by applying a smoothing spline to the weights, when the true eﬀects i → µi are
smooth as a function of i. Roeder and Wasserman (2009) also ﬁnd that smoothing the weights
improves power when weights are estimated by sample splitting. They suggest a within-group
linear shrinkage procedure to reduce variance. Ignatiadis et al. (2015) also observe that adding
i |wi − 1| ≤ c can reduce variance. Our

a total-variation penalty or a penalty of the form(cid:80)

convex formulation allows these constraints as a special case.

There could be many other methods ﬁtting in our framework. A promising example is regression
weights, deﬁned as wi = g(x(cid:62)
i β). Here xi are vectors of covariates and g is a suitable link function.
This framework generalizes stratiﬁed weights, and shows the breadth of the optimization framework.

5

2.2 Related work

There are many methods for multiple testing with prior information, partially reviewed by Roeder
and Wasserman (2009) and Gui et al. (2012). Candidate studies—testing the top candidates based
on prior information—are as old as statistics itself. They can be viewed as p-value weighting
methods with binary weights.

More general methods for multiple testing with prior information have been developed since
at least the 1970’s. Most work focuses on single-step Bonferroni procedures maximizing the aver-
age power and controlling the family-wise error rate. In seminal work, Spjøtvoll (1972) found a
theoretical description of such procedures under very general conditions.

Later work extended Spjøtvoll’s results in several ways. Benjamini and Hochberg (1997) allowed
weights in the importance of the hypotheses. Roeder and Wasserman (2009) and Rubin et al. (2006)
found explicit optimal weights in the Gaussian model N (µi, 1). Eskin (2008) and Darnell et al.
(2012) extended their framework to genome-wide association studies, accounting for correlations.
Westfall et al. (1998) considered the model N (µi, 1) with proper priors on µi. They gave small-
scale algorithms to ﬁnd optimal weights for the Bonferroni method via nonconvex optimization.
Dobriban et al. (2015) showed how to ﬁnd the weights eﬃciently under a Gaussian prior.

Less is known about using weights beyond the Bonferroni method. Holm (1979)’s step-down
method can use weights, and Westfall and Krishen (2001) and Westfall et al. (2004) considered
ﬁnding optimal weights for small J. Genovese et al. (2006) showed that the weighted Benjamini–
Hochberg procedure controls the FDR and Roquain and Van De Wiel (2009) showed how to choose
weights optimally in a special asymptotic regime. Pe˜na et al. (2011) developed a general decision-
theoretic framework for weighted family-wise error rate and FDR control.

It is interesting to see if weights can be constructed using the same dataset where the tests are
performed. Rubin et al. (2006) proposed a sample-splitting approach combined with smoothing. In
a slightly diﬀerent setup, Storey (2007) developed the Optimal Discovery Procedure, maximizing
the expected number of true discoveries, subject to a constraint on the expected number of false
discoveries. This depended on a pilot estimate on the truth of each hypothesis, based on a bootstrap
estimate. Sun and Cai (2007) developed a method maximizing the marginal False Nondiscovery
Rate (mFNR) subject to controlling the marginal False Discovery Rate (mFDR), and showed how
to consistently estimate the oracle procedure in a hierarchical model.

Bourgon et al. (2010) proposed an independent ﬁltering approach where test statistics are inde-
pendent of the prior information only under the null, not under the alternative hypothesis. Recently,
Ignatiadis et al. (2015) proposed the more general Independent Hypothesis Weighting framework.
This promising approach focuses on the false discovery rate, relies on convex relaxations for eﬃcient
computations, and splits of the tests to ensure type I error control.

3 Monotone P-value weights

We now study the promising special case of bounded monotone weights in more detail. We develop
algorithms to compute them, and explore their performance in simulations and in the analysis of
GWAS data. Assuming the prior means are sorted such that 0 ≥ µ1 ≥ µ2 ≥ . . . ≥ µJ , the bounded

6

(a) Lower bounded weights.

(b) Upper bounded weights.

(c) Bounded weights.

Figure 1: Upper and lower bounded weights.

monotone weights problem with lower bound l and upper bound u is:

J(cid:88)

Φ(cid:0)Φ−1 (qwi) − µi

(cid:1)

J(cid:88)

i=1

max

w∈[0,1/q]J

s.t.

i=1

s.t.
l ≤ w1 ≤ w2 ≤ . . . ≤ wJ ≤ u.

wi = J.

(2)

(3)

and G(c) =(cid:80)J

One justiﬁcation for monotonicity is that larger eﬀects are worth more. Another justiﬁcation is
that for suﬃciently small signiﬁcance level q, the optimal Spjøtvoll weights are in fact monotone.
Proposition 3.1 (Monotonicity of Spjøtvoll weights for small q). Let us denote M = maxi{µ2
i /2}
i=1 Φ(µi/2 + c/µi). Suppose that the signiﬁcance level q is small enough that q ≤
G(M )/J. Then the unconstrained Spjøtvoll weights wi = w(µi) deﬁned by Eq. (1) are monotone
increasing in |µi|.
Proof. As shown by Rubin et al. (2006) and Roeder and Wasserman (2009), the Spjøtvoll weights
are w(µi) = Φ(µi/2 + c/µi)/q, for the unique c such that G(c) = Jq. Since all means µi < 0 are
negative, G(c) is decreasing. By the assumption on q, Jq = G(c) ≤ G(M ), hence c ≥ M .
Now, the map µ → l(µ) = µ/2 + c/µ has derivative l(cid:48)(µ) = 1/2 − c/µ2, which is negative if
c ≥ µ2/2. By assumption on M , and by the above conclusion, c ≥ M ≥ µ2
i /2 for all i. Hence
the map l is decreasing in a range including all µi. Therefore the weights wi = Φ(l(µi))/q are
monotonically decreasing as a function of µi, as desired.

While the above proposition holds under a mild constraint, the optimal weights are not bounded

away from zero or inﬁnity in general. This must be imposed as a separate constraint.
We illustrate monotone weights, showing the dependence on the bounds. We take J = 103
hypotheses, set the p-value cutoﬀ q = 10−7, and draw random eﬀect sizes µ = −(|Z1|, . . . ,|ZJ|),
where Zi are iid standard normal. First we vary l from 0 to 0.75 in steps of size 0.25, keeping
u = ∞. Next we vary u from 1.25 to 2 in steps of size 0.25, keeping the l = 0 (Fig. 1).

Lower bounded weights (Fig. 1(a)) are ﬂat near the two endpoints, and increase sharply in
between. This may be surprising because ﬂatness is not required in the optimization problem. In
addition, increasing the lower bound also leads to a decreased upper bound, a property that is not
immediately obvious theoretically. Upper bounded weights (Fig. 1(b)) have a similar but steeper
shape. The weights are still ﬂat, however, they are not automatically lower bounded (strictly above

7

Figure 2: The loss in power of bounded weights compared to Spjøtvoll weights.

0) anymore. Finally, we vary both u and l (Fig. 1 (c)), with l in (0.25, 0.5), and u in (1.5, 1.75).
The weights have the same general shape.

3.1 Power loss compared to Spjøtvoll weights

Since bounded weights are suboptimal to Spjøtvoll weights when the model is correct, it is in-
teresting to understand the loss in power. This may help to form heuristics for the choice of
2 we report the results of a simulation where J = 104,
lower and upper bounds. On Fig.
q = 0.05/J, the means are generated as iid negative absolute Gaussian, while the lower bounds are
(10−3, 5 · 10−3, 10−2, 5 · 10−2, 10−1, 5 · 10−1, 0.9) and the upper bounds are (2, 10,∞). We deﬁne
the power as the value of the maximized objective function in Eq. (2). We also display the power
of the unweighted Bonferroni method (lowest horizontal line), and the optimal Spjøtvoll weighted
Bonferroni method (highest horizontal line).

In this setting, an upper bound of two leads to a severe power loss of at least 50%, regardless
of the lower bound. However, an upper bound of 10 leads to only a small loss in power, while an
upper bound of ∞ leads to nearly full power if the lower bound is at most 0.1. We conclude that
lower bounds below 0.1 and upper bounds above 10 seem to lead to a small loss in power.

3.2 Data analysis example

To illustrate the empirical performance of bounded monotone weights, we analyze several pairs of
publicly available genome-wide association studies. We follow the protocol and methodology laid
out in Dobriban et al. (2015). There, we analyzed ﬁve studies on four complex human traits and
diseases: CARDIoGRAM and C4D for coronary artery disease (Schunkert et al., 2011; Coronary
Artery Disease Genetics Consortium, 2011), blood lipids (Teslovich et al., 2010), schizophrenia
(Schizophrenia Psychiatric Genome-Wide Association Study Consortium, 2011), and estimated
glomerular ﬁltration rate (eGFR) creatinine (K¨ottgen et al., 2010).

In addition to these, here we also include the 90Plus dataset (Deelen et al., 2014), which compares
the lifespan of a sample of Caucasian individuals living at least 90 years with matched controls (see
the Supplement). These studies have the p-values for the marginal association of 500,000–2.5 million
single nucleotide polymorphisms (SNPs) to the outcome. More detail is provided in Sec. 6.

8

We analyze several pairs of these datasets, starting with those that were already included in
Dobriban et al. (2015). As a positive control for our method, we use CARDIoGRAM as prior
information for C4D. Motivated by the Bayesian analysis of Andreassen et al. (2013), we use the
blood lipids study as prior information for the schizophrenia study. Motivated by the comorbidity
between heart disease and renal disease (Silverberg et al., 2004), we use the creatinine study as
prior information for the C4D study.

In addition to these pairs, we add four new examples:

1. We switch the roles of the two heart disease studies, and use C4D as prior information for

CARDIoGRAM.

2. We use the 90Plus dataset as a target study, and check if studies on heart disease and
schizophrenia can increase the number of hits. This is a challenging example, as the 90Plus
data set has weak signal (Deelen et al., 2014). We use the above three datasets as prior
information as they seem to be the most promising from our prior work (Fortney et al., 2015).

For each pair of studies, we restrict to the SNPs that appear in both. We start with the prior
data T0i = Φ−1(P0i/2), where P0i is the i-th prior p-value, and form µi = (Ni/N0i)1/2T0i, as in
(Dobriban et al., 2015, Sec. 6.2). Here Ni and N0i are the current and prior sample size for the
i-th variant, respectively. We control the family-wise error rate at α = 0.05. In Dobriban et al.
(2015), we reported the results of ﬁve weighting schemes: unweighted Bonferroni testing, as well as
weighted Bonferroni testing using Spjøtvoll weights, Bayes weights (Dobriban et al., 2015), ﬁltering,
and exponential weights (Roeder et al., 2006). Filtering selects all tests below a prior p-value cutoﬀ,
and weights them equally. Here we report new results using bounded monotone weights with lower
bounds l = 0 · 01, 0 · 1, 0 · 5, and upper bound u = ∞. The results for upper bound u = 10 are very
similar. We exclude exponential weights, as other methods performed better in our earlier work.

As in Dobriban et al. (2015), we prune the signiﬁcant single nucleotide polymorphisms for linkage
disequilibrium using the DistiLD database (Palleja et al., 2012). We compute a score smd for each
weighting method m on each data set d. This is deﬁned as +1 if the weighting method increases the
number of hits compared to unweighted testing, 0 if it leaves it unchanged, and −1 otherwise. The
score sm of a weighting method m is the sum of scores over datasets. We compute scores separately
for the pruned and unpruned analysis, and add them to ﬁnd the ﬁnal score. Table 2 shows the
number of signiﬁcant loci.
Monotone weights show a good performance for all settings. In the pruned analysis, they have
a score of two for the lower bound l = 0 · 5. They increase the number of rejections in two out
of the seven settings, and keep it the same in the others. In the unpruned analysis, they have a
score of four with l = 0 · 5, increasing the number of hits in four settings, and never decreasing it.
Monotone weights with other lower bounds l = 0 · 1, 0 · 01 also lead to more hits in some cases, but
they can also decrease the number of discoveries.
Monotone weights perform well compared to the other methods. In particular, ﬁltering decreases
the number of hits in many cases, especially when the p-value cutoﬀ is 10−4 or 10−6. Bayes weights,
especially for σ = 1, have a relatively stable performance. With σ = 1, they decrease the number
of hits in only two settings, while increasing it in ﬁve pairs. However, they decrease the number
of hits more frequently for other σ values. Moreover, monotone weights have a larger number of
discoveries than Bayes weights.

No weighting scheme increases the number of pruned SNPs in the 90Plus study. Even in this
challenging example, however, monotone weights seem more stable than the others, as they do not
decrease the number of hits.

9

Table 2: Number of signiﬁcant loci for ﬁve methods on three examples. Top: results pruned for
linkage disequilibrium. Bottom: results without pruning. The score of each method is also reported.
The weighting schemes compared are unweighted (Un); Spjøtvoll (Spjot); ﬁltering (Filter), Bayes,
and Monotone (Mon) with l = 0 · 01, 0 · 1, 0 · 5 and u = ∞. CG stands for CARDIoGRAM, SCZ
for the schizophrenia study, and eGFRcrea for the creatinine study

Un

Spjot

Parameter

Filter(− log P )
2

6

4

0·1

Bayes(σ)
10

1

0·01

Mon(l)
0·1

0·5

Pruned

CG → C4D
C4D → CG
Lipids → SCZ
eGFRcrea → C4D
CG → 90Plus
C4D → 90Plus
SCZ → 90Plus

Score (Pruned)

4
9
4
4
1
1
1

0

Unpruned

CG→ C4D
29
C4D → CG
38
Lipids → SCZ 116
eGFRcrea → C4D
29
CG → 90Plus
4
C4D → 90Plus
1
SCZ → 90Plus
2

Score (Unpruned)

Total Score

0

0

11
14
1
2
1
1
0

-1

45
39
214
18
3
2
0

1

0

10
22
2
1
1
1
0

10
11
2
0
0
1
0

6
3
2
1
0
0
0

-1

-2

-5

40
68
217
1
1
2
0

1

0

48
48
96
0
0
1
0

-2

-4

34
30
39
1
0
0
0

-6

-11

10
14
1
2
1
1
0

-1

44
39
214
18
4
2
0

2

1

8
12
1
4
1
1
1

1

39
39
223
23
4
1
2

2

3

4
9
5
4
1
1
1

1

29
38
123
29
4
1
2

1

2

11
16
2
4
1
1
1

1

45
53
217
23
4
2
2

3

4

10
15
2
4
1
1
1

1

43
52
220
25
4
2
2

3

4

9
15
4
4
1
1
1

2

40
48
225
29
4
2
2

4

6

We think that these results are encouraging. Monotone weights have a better score than any of
the other methods. When using a lower bound of 0·5, they do not decrease the number of signiﬁcant
hits, even in the most challenging scenarios. In contrast, when there is good prior information, they
seem to detect it reliably.

4 Optimization for monotone weights

cuss a few analytic properties of the optimization problem. The function f (w) = Φ(cid:0)Φ−1 (qw) − µ(cid:1)

In this section we explain our method for computing monotone bounded weights. To start, we dis-

(plotted on Fig. 3) is deﬁned without ambiguity on (0, 1/q), and can be deﬁned by continuity at the
two endpoints. Indeed, as w → 0, Φ−1 (qw) → −∞, so that f (w) → 0. Similarly f (w) → 1 at the
other endpoint. Further, f is concave on [0, 1/q]. Indeed, denoting the z-score z = Φ−1 (qw), we

10

Figure 3: Plot of the function f appearing in the objective for various parameters µ, q.

have f(cid:48)(w) = q exp(cid:0)µz − µ2/2(cid:1) and f(cid:48)(cid:48)(w) = q2µ

√

2π exp(cid:0)z2/2 + µz − µ2/2(cid:1). Thus f is concave

in w for all µ ≤ 0, and it is strictly concave for µ < 0. The second derivative is unbounded as
w → 0, because exp(z2/2) is much larger than the other terms.

To compute monotone weights, we use the standard log-barrier method (see e.g., Boyd and
Vandenberghe, 2004, p. 569, Sec. 11.3.1). Since the objective f (w) is not implemented by default
in the standard optimization environment cvx (Grant et al., 2008), we choose to implement our
own version of the barrier method (Algorithm 1). While it is possible to pass a structured Hessian
to the MATLAB fmincon function, with an interior-point algorithm, we are not aware of a way
to exploit the tridiagonal structure of the Hessian using this approach. The tridiagonal structure
is important, because it leads to an O(J)-time solution to the Newton equations. Brieﬂy put,
implementing our own version of Newton’s method seems to be the most direct way to solve the
problem, and it allows us to have full control over all parameter choices.

The barrier method involves a number of optimization parameters. We use the default settings
from Boyd and Vandenberghe (2004) Sec. 11, although these choices should not aﬀect performance
too much. In Algorithm 1, we set the centering step parameters t = 103, µ = 10, the stopping
criterion ε = 10−3, and the line search parameters α = 0.01, β = 0.5. In addition, we need to choose
0. For this, we let u = (1, 2, . . . , J)(cid:62), and v be the mean-centered
a strictly feasible starting point w∗
version of u. Then we let

(4)
where e = (1, 1, . . . , 1)(cid:62) is the all ones vector and δ > 0 is a small constant such that the vector w∗
is strictly feasible. This is clearly possible if 0 ≤ l < 1 < u ≤ 1/q.

0

w∗
0 = e + δv

4.1 Centering problem

From now on we ﬂip the sign of the objective, so that we are minimizing a convex function. For
a penalty parameter t > 0, the logarithmic barrier problem—or centering step— is the convex

11

Algorithm 1 Barrier Method for Monotone Weights
1: procedure Monotone Weights
2:
3:
4:
5:

Given 0 > µ1 ≥ µ2 ≥ . . . ≥ µJ , q, l, u
0 ← strictly feasible starting point from (4)
w∗
Set the index k ← 0
Set optimization parameters t > 0, µ > 1, ε > 0
loop

6:
7:
8:
9:
10:
11:

12:
13:
14:

15:

16:
17:
18:

19:
20:

Centering step: Solve (5) with parameter t, starting from w∗
w ← w∗
loop

k

k

Compute Newton step ∆wnt by solving system (7)
Compute Newton decrement λ(w) from (8)
quit loop if λ2/2 < ε
Line search: Choose step size s by backtracking line search (parameters α, β)
Update w ← w + s∆wnt
k+1 ← w, k ← k + 1

end loop
Update w∗
quit loop if J/t < ε
Increase t ← µt
ε ← w∗

k+1

end loop
return w∗

program:

J(cid:88)

i=1

− t

Φ(cid:0)Φ−1 (qwi) − µi

(cid:1) − J(cid:88)

i=0

log(wi+1 − wi)

min

w∈[0,1/q]J

J(cid:88)

s.t.

wi = J.

(5)

(6)

i=1

Here we deﬁne the constants w0 := l, wJ+1 := u for brevity. Proposition 2.1 implies that the barrier
method for the unconstrained problem converges, and in particular that Newton’s method for this
problem also converges.

We now show that the Karush-Kuhn-Tucker (KKT) matrix (Boyd and Vandenberghe, 2004, p.
577) is a sum of a tridiagonal and a rank one matrix, so that the KKT system can be solved in
O(J) time. Let g be the objective function, ∇g its gradient, and ∇2g its Hessian. Let us also
denote by ∆wnt the Newton step for w, and by ν the scalar dual variable corresponding to the sum
constraint (Boyd and Vandenberghe, 2004, p. 577). Then the KKT system for ﬁnding (∆wnt, ν)
at a particular w (suppressed for ease of notation) is

(cid:20)∇2g

e(cid:62)

e
0

(cid:21)(cid:20) ∆wnt

(cid:21)

(cid:20) −∇g

(cid:21)

=

0

.

(7)

ν

12

Assuming ∇2g is invertible, standard linear algebra shows that the solution has the form:

(cid:17)−1(cid:16)

e

e(cid:62)(cid:0)∇2g(cid:1)−1 ∇g

(cid:17)−1

ν = −(cid:16)
e(cid:62)(cid:0)∇2g(cid:1)−1
∆wnt = −(cid:0)∇2g(cid:1)−1
(cid:16)

(ν e + ∇g) .

The Newton decrement is deﬁned as

(cid:0)∇2g(cid:1)−1
To solve the Newton system, we must calculate(cid:0)∇2g(cid:1)−1
(cid:19)

of the gradient are:

∆w(cid:62)

λ(w) =

nt

(cid:17)1/2
e and(cid:0)∇2g(cid:1)−1 ∇g. First, the components

(8)

.

∆wnt

(cid:18)
(cid:18) z2

i
2

= −qt exp

∂g
∂wi

µizi − µ2
i
2

−

1

wi − wi−1

−

1

wi − wi+1

where we denoted the z-score zi = Φ−1 (qwi). Next, the diagonal components of the Hessian are

√

= −q2tµi

2π exp

∂2g
∂w2
i

+ µizi − µ2
i
2

+

1

(wi − wi−1)2 +

1

(wi − wi+1)2

(cid:19)

and the oﬀ-diagonal terms vanish except those on the band just one-oﬀ the diagonal, which are
∂2g/(∂wi∂wi+1) = −1/(wi − wi+1)2. Hence, the Hessian matrix ∇2g is tridiagonal. With the
notation hi = 1/(wi − wi+1)2, the Hessian has the form ∇2g = H + D where H is the tridiagonal
matrix



h0 + h1
−h1
...
0

H =



0
0
...

hJ + hJ+1

−h1
h1 + h2

...
0

···
···
. . .
···

√

2π · Diag(cid:0)µi exp(cid:0)z2
1 +(cid:80)J−1

i /2(cid:1)(cid:1) . Finally ∇2g =

i /2 + µizi − µ2

and D is the diagonal matrix D = −q2t
H +D is invertible because it is the sum of two positive deﬁnite matrices. Indeed the diagonal terms
in D are strictly positive if µi < 0. Further H is positive deﬁnite, as for a vector x = (x1, . . . , xJ ),
the quadratic form x(cid:62)Hx equals x(cid:62)Hx = h1x2
J . Now hi > 0 for
all i, so x(cid:62)Hx = 0 implies that all xi are equal to 0. This shows that H is positive deﬁnite.
Hence, we have shown that ∇2g is positive deﬁnite, and thus invertible. Therefore, the KKT
system involves the solution of a tridiagonal-plus-rank 1 linear system, taking O(J) time. This is
implemented using a standard sparse tridiagonal linear solver in MATLAB, which is stable since H
is positive deﬁnite.

i=1 hi+1(xi − xi+1)2 + hJ+1x2

4.1.1 Reﬁnements

Based on the earlier simulation results, and on intuition from isotonic regression, we expect that
the solution may have many equal terms, lying on the boundary of the feasible set. It is well-known
that this can lead to an ill-conditioned KKT system (e.g., Nocedal and Wright, 2006, Ch. 17). As
a heuristic solution that also reduces the size of the problem, if J is larger than a constant L, we
subsample L means evenly spaced among the indices 1, . . . , J. Here we choose L = 104. To avoid

13

(a) Absolute error.

(b) Relative error.

Figure 4: Accuracy of subsampling.

ties, we then subsample the remaining means µj starting from µ1 and discard the remaining terms
µ2, µ3, . . ., until the ﬁrst index j where µj < µ1 − c0. We then repeat this starting from j, and so
on. We choose the small constant c0 = 10−6. We use the barrier method to compute weights on
the subsample, and then interpolate linearly to the remaining means.

We show in Section 4.2.1 that this does not aﬀect too much the accuracy of the weights on smaller
problems. On larger problems, it usually avoids the ill-conditioned KKT systems encountered by
the naive barrier method.

4.2 Experiments

In this section we report the results of several experiments with monotone and bounded weights.

4.2.1 Accuracy of subsampling

In Section 4.1.1 we introduced a subsampling method to avoid the ill-conditioning of the KKT
system. Here we show that the method does not lose too much accuracy compared to the full
barrier method. We set J = 2 · 104, q = 5 · 10−3, draw random µ = −(|Z1|, . . . ,|ZJ|) where Zi are
iid standard normal, and set the bounds l = 0, u = ∞. We compute weights using the standard
barrier method (wb), and using the barrier method with subsampling (ws). We then compute the
absolute error |wb − ws| and relative error (deﬁned pointwise as |wb(i) − ws(i)|/|wb(i)|). The two
weighting schemes (Fig. 4) agree within at least two signiﬁcant digits in terms of absolute error for
all means. The relative error can be as large as 10−1, but only when the absolute error is smaller
than 10−6, so that this still translates to a good accuracy.

4.2.2 Comparison with Spjøtvoll

We showed in Proposition 3.1 that Spjøtvoll weights are monotone if q is small. Thus we can
compare the two methods on a problem where they should give the same results. We set J = 103,
q = 10−7, draw random µ = −(|Z1|, . . . ,|ZJ|) where Zi are iid standard normal, and set l = 0,
u = ∞. Spjøtvoll weights (1) and monotone weights (2) are visually indistinguishable (Fig. 5(a).).

14

(a) Spjøtvoll and Monotone weights.

(b) Numerical agreement.

Figure 5: (a) Spjøtvoll and Monotone weights. (b) Numerical agreement of Monotone and Spjøtvoll
weights. Left: Base 10 logarithm of average (cid:96)1 diﬀerence per coordinate. Right: Base 10 logarithm
of 1 − ρ, where ρ is the correlation.

To examine the agreement in more depth, we run 103 simulations with these settings, where µ
is always drawn at random, and q = 5U · 10−5 is random, where U is a uniform random variable
on the unit interval. With a small probability, Spjøtvoll weights are not monotone. In this case
we set q ← q/2 until the weights become monotone. For each draw we compute Spjøtvoll and
monotone weights, and record the Mean Absolute Deviation (MAD) and 1 − ρ, where ρ is the
correlation between the two weight vectors (Fig. 5(b)). The MAD between the weights is of the
order of 10−3–10−4, while 1− ρ is of the order of 10−7–10−10. This shows the agreement of the two
methods, and conﬁrms the accuracy of the log-barrier method.
Finally we show some examples where the two weighting schemes diﬀer (Fig.6(a)). The simu-
lation was as above except q was much larger, q = 5 · 10−3. Spjøtvoll weights have the well-known
unimodal shape. Monotone weights look like capped Spjøtvoll weights. On the log-scale Spjøtvoll
weights take extremely small values (on the order of 10−500), while monotone weights go down only
to about 10−10–10−20. This further shows that they are better conditioned than Spjøtvoll weights.

4.2.3 Running time

To test the running time of our method, we vary J in the range of 102–105, choosing the µ as above.
The parameters q and l are chosen randomly as q = U/10, l = V /10, where U , V are independent
uniform random variables on the unit interval. We also take u = ∞. The average running times
over 50 simulations are shown in Fig. 6(b), and they are below a second even for the largest J.

5 Discussion

We presented a general convex framework for multiple testing with prior information. Future work
may involve the development of eﬃcient optimization methods for constraints beyond monotonicity,
the study of theoretical properties of monotone weights, and extensions beyond Gaussianity.

15

(a) Spjøtvoll and Monotone weights.

(b) Timing.

Figure 6: (a) Spjøtvoll and Monotone weights. Linear (left subplot) and log10 scale (right rub-
plot). (b) Base 10 log running time of barrier method. Averages and two standard errors over 50
independent MC trials.

Acknowledgements

We thank Art Owen for pointing out that the objective is convex and numerous other helpful
suggestions, and Stuart Kim for insisting that weights should be monotone. We thank David
Donoho for ﬁnancial support through his grant.

6 Data sources

6.1 CARDIoGRAM and C4D

CARDIoGRAM is a meta-analysis of 14 coronary artery disease genome-wide association studies,
comprising 22,233 cases and 64,762 controls of European descent (Schunkert et al., 2011). The study
includes 2·3 million single nucleotide polymorphisms. In each of the 14 studies and for each single
nucleotide polymorphism, a logistic regression of coronary artery disease status was performed on
the number of copies of one allele, along with suitable controlling covariates. The resulting eﬀect
sizes were combined across studies using ﬁxed eﬀects or random eﬀects meta-analysis with inverse
variance weighting.

C4D is a meta-analysis of 5 heart disease genome-wide association studies, totalling 15,420 coro-
nary artery disease cases and and 15,062 controls (Coronary Artery Disease Genetics Consortium,
2011). The samples did not overlap those from CARDIoGRAM. The analysis steps were similar to
CARDIoGRAM.

The consortia require that the following acknowledgment be included: Data on coronary artery
disease / myocardial infarction have been contributed by CARDIoGRAMplusC4D investigators and
have been downloaded from www.CARDIOGRAMPLUSC4D.ORG.

16

6.2 Chronic Kidney Disease Consortium

This is a genome-wide association study of kidney traits in 67,093 participants of European ancestry
from 20 population-based cohorts (K¨ottgen et al., 2010). Estimated glomerular ﬁltration rate
creatinine was the trait with the largest sample size. There is no reported overlap with the samples
from C4D. The analysis steps were similar to the previous two studies.

6.3 Blood Lipids

This is a genome-wide association study of blood lipids in a sample from European populations
(Teslovich et al., 2010). Triglyceride levels were one of the traits, with sample size 96,598, chosen
here out of all lipids because of its previous appearance in Andreassen et al. (2013). Standard
protocols for genome-wide association studies were used: linear regression analysis controlling for
study-speciﬁc covariates, combined using ﬁxed-eﬀects meta-analysis.

6.4 Psychiatric Genomics Consortium - Schizophrenia

This is a mega-analysis, which uses the raw data not just summaries of other studies, combining
genome-wide association study data from 17 separate studies of schizophrenia, with a total of 9,394
cases and 12,462 controls (Schizophrenia Psychiatric Genome-Wide Association Study Consortium,
2011). They tested for association using logistic regression of schizophrenia status on the allelic
dosages. The overlap with the blood lipids study consists of 1,459 controls, which amounts to 12%
of controls in the schizophrenia study. The overlapping controls are from the British 1958 Birth
Cohort of the Wellcome Trust Case Control Consortium.

6.5

90Plus Study - Aging

Deelen et al. (2014) performed a genome-wide association meta-analysis of 5406 long-lived indi-
viduals of European descent (aged at least 90 years). They combined the results of 14 studies
originating from seven European countries. The analysis steps were similar to the ones above. This
dataset was used in Fortney et al. (2015), and it is the most conveniently available aging data set
among those analyzed in that paper.

References

O. A. Andreassen, S. Djurovic, W. K. Thompson, A. J. Schork, K. S. Kendler, M. C. ODonovan, D. Rujescu,
T. Werge, M. van de Bunt, A. P. Morris, et al. Improved detection of common variants associated with
schizophrenia by leveraging pleiotropy with cardiovascular-disease risk factors. The American Journal of
Human Genetics, 92(2):197–209, 2013.

P. Basu, T. T. Cai, K. Das, and W. Sun. Weighted false discovery rate control in large-scale multiple

testing. arXiv preprint arXiv:1508.01605, 2015.

Y. Benjamini and Y. Hochberg. Multiple hypotheses testing with weights. Scandinavian Journal of Statis-

tics, 24(3):407–418, 1997.

R. Bourgon, R. Gentleman, and W. Huber.

Independent ﬁltering increases detection power for high-

throughput experiments. Proceedings of the National Academy of Sciences, 107(21):9546–9551, 2010.

G. E. P. Box. Sampling and Bayes’ inference in scientiﬁc modelling and robustness (with Discussion).

Journal of the Royal Statistical Society. Series A (General), 143(4):383–430, 1980.

S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.

17

B. J. Carlin and T. A. Louis. Controlling error rates by using conditional expected power to select tumor
sites. In Proceedings of the Biopharmaceutical Section, pages 11–18. American Statistical Association,
1985.

Coronary Artery Disease Genetics Consortium. A genome-wide association study in Europeans and South

Asians identiﬁes ﬁve new loci for coronary artery disease. Nature Genetics, 43(4):339–344, 2011.

G. Darnell, D. Duong, B. Han, and E. Eskin.

Incorporating prior information into association studies.

Bioinformatics, 28(12):i147–i153, 2012.

J. Deelen, M. Beekman, H.-W. Uh, L. Broer, K. L. Ayers, Q. Tan, Y. Kamatani, A. M. Bennet, R. Tamm,
S. Trompet, et al. Genome-wide association meta-analysis of human longevity identiﬁes a novel locus
conferring survival beyond 90 years of age. Human Molecular Genetics, 23(16):4420–4432, 2014.

E. Dobriban, K. Fortney, S. K. Kim, and A. B. Owen. Optimal multiple testing under a Gaussian prior on

the eﬀect sizes. Biometrika, 102(4):753–766, 2015.

E. Eskin. Increasing power in association studies by using linkage disequilibrium structure and molecular

function as prior information. Genome Research, 18(4):653–660, 2008.

K. Fortney, E. Dobriban, P. Garagnani, C. Pirazzini, D. Monti, D. Mari, G. Atzmon, N. Barzilai,
C. Franceschi, A. B. Owen, and S. K. Kim. Genome-wide scan informed by age-related disease identiﬁes
loci for exceptional human longevity. PLoS Genet, 11(12):e1005728, 2015.

C. R. Genovese, K. Roeder, and L. Wasserman. False discovery control with p-value weighting. Biometrika,

93(3):509–524, 2006.

M. Grant, S. Boyd, and Y. Ye. Cvx: Matlab software for disciplined convex programming, 2008.
J. Gui, T. D. Tosteson, and M. E. Borsuk. Weighted multiple testing procedures for genomic studies.

BioData Mining, 5(4), 2012.

S. Holm. A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics, 6(2):

65–70, 1979.

N. Ignatiadis, B. Klaus, J. Zaugg, and W. Huber. Data-driven hypothesis weighting increases detection

power in big data analytics. bioRxiv, 2015. doi: 10.1101/034330.

A. K¨ottgen, C. Pattaro, C. A. B¨oger, C. Fuchsberger, M. Olden, N. L. Glazer, A. Parsa, X. Gao, Q. Yang,
A. V. Smith, et al. New loci associated with kidney function and chronic kidney disease. Nature Genetics,
42(5):376–384, 2010.

E. L. Lehmann and J. P. Romano. Testing Statistical Hypotheses. Springer Science & Business Media, 2005.
L. Li, M. Kabesch, E. Bouzigon, F. Demenais, M. Farrall, M. F. Moﬀatt, X. Lin, and L. Liang. Using
eqtl weights to improve power for genome-wide association studies: a genetic study of childhood asthma.
Frontiers in Genetics, 4(103), 2013.

J. Nocedal and S. Wright. Numerical Optimization. Springer Science & Business Media, 2006.
A. Palleja, H. Horn, S. Eliasson, and L. J. Jensen. DistiLD Database: diseases and traits in linkage

disequilibrium blocks. Nucleic Acids Research, 40(D1):D1036–D1040, 2012.

E. A. Pe˜na, J. D. Habiger, and W. Wu. Power-enhanced multiple decision functions controlling family-wise

error and false discovery rates. The Annals of Statistics, 39(1):556–583, 2011.

C. A. Rietveld, T. Esko, G. Davies, T. H. Pers, P. Turley, B. Benyamin, C. F. Chabris, V. Emilsson, A. D.
Johnson, J. J. Lee, et al. Common genetic variants associated with cognitive performance identiﬁed using
the proxy-phenotype method. Proceedings of the National Academy of Sciences, 111(38):13790–13794,
2014.

K. Roeder and L. Wasserman. Genome-wide signiﬁcance levels and weighted hypothesis testing. Statistical

Science, 24(4):398–413, 2009.

K. Roeder, S.-A. Bacanu, L. Wasserman, and B. Devlin. Using linkage genome scans to improve power of

association in genome scans. The American Journal of Human Genetics, 78(2):243–252, 2006.

E. Roquain and M. A. Van De Wiel. Optimal weighting for false discovery rate control. Electronic Journal

of Statistics, 3:678–711, 2009.

D. Rubin, S. Dudoit, and M. Van der Laan. A method to increase the power of multiple testing procedures

through sample splitting. Statistical Applications in Genetics and Molecular Biology, 5(1):1–19, 2006.

18

S. F. Saccone, A. L. Hinrichs, N. L. Saccone, G. A. Chase, K. Konvicka, P. A. Madden, N. Breslau, E. O.
Johnson, D. Hatsukami, O. Pomerleau, et al. Cholinergic nicotinic receptor genes implicated in a nicotine
dependence association study targeting 348 candidate genes with 3713 snps. Human Molecular Genetics,
16(1):36–49, 2007.

Schizophrenia Psychiatric Genome-Wide Association Study Consortium. Genome-wide association study

identiﬁes ﬁve new schizophrenia loci. Nature Genetics, 43(10):969–976, 2011.

H. Schunkert, I. R. K¨onig, S. Kathiresan, M. P. Reilly, T. L. Assimes, H. Holm, M. Preuss, A. F. Stewart,
M. Barbalic, C. Gieger, et al. Large-scale association analysis identiﬁes 13 new susceptibility loci for
coronary artery disease. Nature Genetics, 43(4):333–338, 2011.

D. Silverberg, D. Wexler, M. Blum, D. Schwartz, and A. Iaina. The association between congestive heart
failure and chronic renal disease. Current Opinion in Nephrology and Hypertension, 13(2):163–170, 2004.
E. Spjøtvoll. On the optimality of some multiple comparison procedures. The Annals of Mathematical

Statistics, 43(2):398–411, 1972.

J. D. Storey. The optimal discovery procedure: a new approach to simultaneous signiﬁcance testing. Journal

of the Royal Statistical Society: Series B (Statistical Methodology), 69(3):347–368, 2007.

L. Sun, R. V. Craiu, A. D. Paterson, and S. B. Bull. Stratiﬁed false discovery control for large-scale
hypothesis testing with application to genome-wide association studies. Genetic Epidemiology, 30(6):
519–530, 2006.

W. Sun and T. T. Cai. Oracle and adaptive compound decision rules for false discovery rate control. Journal

of the American Statistical Association, 102(479):901–912, 2007.

G. Sveinbjornsson, A. Albrechtsen, F. Zink, S. A. Gudjonsson, A. Oddson, G. M´asson, H. Holm, A. Kong,
U. Thorsteinsdottir, P. Sulem, et al. Weighting sequence variants based on their annotation increases
power of whole-genome association studies. Nature Genetics, 48:314–317, 2016.

T. M. Teslovich, K. Musunuru, A. V. Smith, A. C. Edmondson, I. M. Stylianou, M. Koseki, J. P. Pirruccello,
S. Ripatti, D. I. Chasman, C. J. Willer, et al. Biological, clinical and population relevance of 95 loci for
blood lipids. Nature, 466(7307):707–713, 2010.

P. H. Westfall and A. Krishen. Optimally weighted, ﬁxed sequence and gatekeeper multiple testing proce-

dures. Journal of Statistical Planning and Inference, 99(1):25–40, 2001.

P. H. Westfall, A. Krishen, and S. S. Young. Using prior information to allocate signiﬁcance levels for

multiple endpoints. Statistics in Medicine, 17(18):2107–2119, 1998.

P. H. Westfall, S. Kropf, and L. Finos. Weighted FWE-controlling methods in high-dimensional situations.
In Recent Developments in Multiple Comparison Procedures, pages 143–154. Institute of Mathematical
Statistics, 2004.

19

