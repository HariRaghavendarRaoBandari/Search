Text Understanding with the Attention Sum Reader Network

Rudolf Kadlec, Martin Schmid, Ondrej Bajgar & Jan Kleindienst ∗

IBM Watson

{rudolf kadlec,martin.schmid,obajgar,jankle}@cz.ibm.com

V Parku 4, Prague, Czech Republic

6
1
0
2

 
r
a

M
4

 

 
 
]
L
C
.
s
c
[
 
 

1
v
7
4
5
1
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

Several
large cloze-style context-question-
answer datasets have been introduced recently:
the CNN and Daily Mail news data and the
Children’s Book Test. Thanks to the size of
these datasets, the associated text comprehen-
sion task is well suited for deep-learning tech-
niques that currently seem to outperform all al-
ternative approaches. We present a new, sim-
ple model that uses attention to directly pick
the answer from the context as opposed to
computing the answer using a blended repre-
sentation of words in the document as is usual
in similar models. This makes the model par-
ticularly suitable for question-answering prob-
lems where the answer is a single word from
the document. Our model outperforms models
previously proposed for these tasks by a large
margin.
Introduction

1
Most of the information humanity has gathered up to
this point is stored in the form of plain text. Hence the
task of teaching machines how to understand this data
is of utmost importance in the ﬁeld of Artiﬁcial Intelli-
gence. One way of testing the level of text understand-
ing is simply to ask the system questions for which the
answer can be inferred from the text. A well-known
example of a system that could make use of a huge col-
lection of unstructured documents to answer questions
is for instance IBM’s Watson system used for the Jeop-
ardy challenge (Ferrucci et al., 2010).

Cloze style questions (Taylor, 1953), i.e. questions
formed by removing a phrase from a sentence, are an
appealing form of such questions. While the task is
easy to evaluate, one can vary the context, the question
sentence or the speciﬁc phrase missing in the question
to dramatically change the task structure and difﬁculty.
One way of altering the task difﬁculty is to vary the
word type being replaced, as in (Hill et al., 2015). The
complexity of such variation comes from the fact that
∗ We would also like to thank Tim Klinger for providing
us with masked softmax code that we used in our implemen-
tation.

the level of context understanding needed in order to
correctly predict different types of words varies greatly.
While predicting prepositions can easily be done using
relatively simple models with very little context knowl-
edge, predicting named entities requires a deeper un-
derstanding of the context.

Also, as opposed to selecting a random sentence
from a text (as done in (Hill et al., 2015)), the ques-
tions can be formed from a speciﬁc part of a document,
such as a short summary or a list of tags. Since such
sentences often paraphrase in a condensed form what
was said in the text, they are particularly suitable for
testing text comprehension (Hermann et al., 2015).

An important property of cloze style questions is that
a large amount of such questions can be automatically
generated from real world documents. This opens the
task to data-hungry techniques such as deep learning.
In the ﬁrst part of this article we introduce the task
at hand and the main aspects of the relevant datasets.
Then we present our own model to tackle the problem.
Subsequently we compare the model to previously pro-
posed architectures and ﬁnally describe the experimen-
tal results on the performance of our model.

2 Task and datasets

In this section we brieﬂy introduce the task that we are
seeking to solve and relevant large-scale datasets that
have recently been introduced for this task.

2.1 Formal Task Description
The task consists of answering a cloze style question,
the answer to which is dependent on the understanding
of a context document provided with the question. The
model is also provided with a set of possible answers
from which the correct one is to be selected. This can
be formalized as follows:

The training data consist of tuples (q, d, a, A),
where q is a question, d is a document that contains
the answer to question q, A is a set of possible answers
and a ∈ A is the ground truth answer. Both q and d
are sequences of words from vocabulary V . We also
assume that all possible answers are words from the
vocabulary, that is A ⊆ V , and that the ground truth
answer a appears in the document, that is a ∈ d.

CNN
train valid

527
26.4
762

187
26.5
763
118,497

# queries
Max # options
Avg # options
Avg # tokens
Vocab. size

test

test
380,298 3,924 3,198 879,450 64,835 53,182 120,769 2,000 2,500 108,719 2,000 2,500
10
10
424

396
24.5
716

371
26.5
813

245
26.0
780

10
10
461

10
10
433

test

10
10
470

10
10
448
53,185

232
25.5
774
208,045

Daily Mail
valid

train

test

CBT CN
train valid

CBT NE
train valid

10
10
412
53,063

Table 1: Statistics on the 4 data sets used to evaluate the model. CBT CN stands for CBT Common Nouns and
CBT NE stands for CBT Named Entites. CBT had a ﬁxed number of 10 options for answering each question.
Statistics were taken from (Hermann et al., 2015) and the statistics provided with the CBT data set.

2.2 Datasets
We will now brieﬂy summarize important features of
the datasets.

2.2.1 News Articles — CNN and Daily Mail
The ﬁrst two datasets1 (Hermann et al., 2015) were
constructed from a large number of news articles from
the CNN and Daily Mail websites. The main body of
each article forms a context, while the cloze style ques-
tion is formed from one of short highlight sentences,
appearing at the top of each article page. Speciﬁcally,
the question is created by replacing a named entity
from the summary sentence (e.g. “Producer X will
not press charges against Jeremy Clarkson, his lawyer
says.”).

Furthermore the named entities in the whole dataset
were replaced by anonymous tokens which were fur-
ther shufﬂed for each example so that the model cannot
build up any world knowledge about the entities and
hence has to genuinely rely on the context document to
search for an answer to the question.

third dataset2,

2.2.2 Children’s Book Test
The
the Children’s Book Test
(CBT) (Hill et al., 2015), is built from books that
are freely available thanks to Project Gutenberg3.
Each context document is formed by 20 consecutive
sentences taken from a children’s book story. Due to
the lack of summary, the cloze style question is then
constructed from the subsequent (21st) sentence.

One can also see how the task complexity varies with
the type of the omitted word (named entity, common
noun, verb, preposition). (Hill et al., 2015) have shown
that while standard LSTM language models have hu-
man level performance on predicting verbs and prepo-
sitions, they lack behind on named entities and com-
mon nouns. In this article we therefore focus only on
predicting the ﬁrst two word types.

1The CNN and Daily Mail datasets are available at

https://github.com/deepmind/rc-data

2The CBT dataset

is available at http://www.

thespermwhale.com/jaseweston/babi/
CBTest.tgz

3https://www.gutenberg.org/

Basic statistics about the CNN, Daily Mail and CBT

datasets are summarized in Table 1.

3 Our Model — Attention Sum Reader
3.1 Motivation
Our model called the Attention Sum Reader (AS
Reader) is tailor-made to leverage the fact that the an-
swer is a word from the context document. This is a
double-edged sword. While it achieves state-of-the-
art results on all of the mentioned datasets (where this
assumption holds true), it cannot produce an answer
which is not contained in the document.
Intuitively,
our model is structured as follows:

1. We compute a vector embedding of the query.

2. We compute a vector embedding of each individ-
ual word in the context of the whole document
(contextual embedding).

3. Using a dot product between the question embed-
ding and the contextual embedding of each occur-
rence of a candidate answer in the document, we
select the most likely answer.

3.2 Formal Description
Our model uses one word embedding function and two
encoder functions. The word embedding function e
translates words into vector representations. The ﬁrst
encoder function is a document encoder f that encodes
every word from the document d in the context of the
whole document. We call this the contextual embed-
ding. For convenience we will denote the contextual
embedding of the i-th word in d as fi(d). The sec-
ond encoder g is used to translate the query q into a
ﬁxed length representation of the same dimensionality
as fi(d). Both encoders use word embeddings com-
puted by e as their input. Then we compute a weight
for every word in the document as the dot product of its
contextual embedding and the query embedding. This
weight might be viewed as an attention over the docu-
ment d.

Figure 1: Structure of the model.

To form a proper probability distribution over the
words in the document, we normalize the weights using
the softmax function. This way we model probability si
that the answer to query q appears at position i in the
document d. In a functional form this is:
si ∝ exp (fi(d) · g(q))

(1)

Finally we compute the probability that word w is a

correct answer as:

P (w|q, d) =

si

(2)

(cid:88)

i∈I(w,d)

where I(w, d) is a set of positions where w appears
in the document d. We call this mechanism pointer
sum attention since we use attention as a pointer over
discrete tokens in the context document and then we
directly sum the word’s attention across all the oc-
currences. This differs from the usual use of atten-
tion in sequence-to-sequence models (Bahdanau et al.,
2015) where attention is used to blend representa-
tions of words into a new embedding vector. Our use
of attention was inspired by Pointer Networks (Ptr-
Nets) (Vinyals et al., 2015).

A high level structure of our model is shown in Fig-

ure 1.

3.3 Model instance details
In our model the document encoder f is implemented
as a bidirectional Gated Recurrent Unit (GRU) net-
work (Cho et al., 2014) whose hidden states form
the contextual word embeddings,
that is fi(d) =
fi (d) || ←−
−→
fi (d), where || denotes vector concatenation
−→
←−
and
fi and
fi denote forward and backward contextual
embeddings from the respective recurrent networks.
The query encoder g is implemented by another bidi-
rectional GRU network. This time the last hidden
state of the forward network is concatenated with the
last hidden state of the backward network to form the

query embedding, that is g(q) = −→g|q|(q) || ←−g1(q). The
word embedding function e is implemented in a usual
way as a look-up table V. V is a matrix whose rows
can be indexed by words from the vocabulary, that is
e(w) = Vw, w ∈ V . Therefore, each row of V contains
embedding of one word from the vocabulary. During
training we jointly optimize parameters of f, g and e.

4 Related Work
Two recent deep neural network architectures (Her-
mann et al., 2015; Hill et al., 2015) were applied to the
task of text comprehension. Both these architectures
use an attention mechanism that allows them to high-
light places in the document that might be relevant to
answering the question. We will now brieﬂy describe
these architectures and compare them to our approach.

4.1 Attentive and Impatient Readers
Attentive and Impatient Readers were proposed
in (Hermann et al., 2015). The simpler Attentive
Reader is very similar to our architecture. It also uses
bidirectional document and query encoders to compute
an attention in a similar way we do. The more com-
plex Impatient Reader computes attention over the doc-
ument after reading every word of the query. However,
empirical evaluation has shown that both models per-
form almost identically on the CNN and Daily Mail
datasets.

embeddings of words in d, that is r = (cid:80)

The key difference between the Attentive Reader and
our model is that the Attentive Reader uses attention to
compute a ﬁxed length representation r of the docu-
ment d that is equal to a weighted sum of contextual
i sifi(d).
A joint query and document embedding is then a non-
linear function of r and the query embedding g(q).
This joint embedding is in the end compared against
all candidate answers a(cid:48) ∈ A using the dot prod-
uct e(a(cid:48)) · r, in the end the scores are normalized by
softmax. That is: P (a(cid:48)|q, d) ∝ exp (e(a(cid:48)) · r).

DocumentQuestionSoftmax(cid:1)(cid:2)over words in the sentencePObamaq,d=(cid:12)(cid:1)(cid:2)(cid:2)∈(cid:14)((cid:16)(cid:17)(cid:18)(cid:19)(cid:18),d)=(cid:1)(cid:21)+(cid:1)(cid:21)(cid:23)(cid:24)…..   Obama    and     Putin     ……       said   Obama in      PragueXXXXX  visited  Prague Probability of the answer…………(cid:25)(cid:26)(cid:27)(cid:28)(cid:27)|(cid:30)|(cid:27)|(cid:30)|(cid:27)(cid:28)…..….........Recurrentneural networksDot products(cid:31) ←(cid:31) →(cid:31)|#|(cid:31)|#|…………probtEmbeddingsInput text….. $(Obama)		$and			$(Putin)…..      $(said)	$(Obama)$(in)$(Prague)				$(XXXXX)$visited		$(Prague)Table 2: Attention in an example where our system se-
lected the correct answer. Note that the attention is fo-
cused only on named entities.

...

according to a entity21 lawmaker (education policy
entity25 , who an op-ed
is super gay, obviously);
writer for entity28, entity29, claims is being used by
entity30 to ”attract young girls” to her show (uh-huh);
the entity36 princess movie ”entity40” according to
radio hosts in entity38 (that dress!); and now, accord-
ing to a potential 2016 entity34 presidential contender,
entity32 , there’s prison. yep, prison. stay away from
crime, kids.
entity32 , who ,let me
reiterate , is a potential presidential candidate from a
major entity54 party

turns ya gay.

...

possible 2016 entity34 candidate X stirs controversy
with comments on gays , prison

In contrast to the Attentive Reader, we select the an-
swer from the context directly using the computed at-
tention rather than using such attention for a weighted
sum of the individual representations (see Eq. 2). The
motivation for such simpliﬁcation is the following.

Consider a context “Bayern Munchen plays FC
Barcelona tonight. Last time it played Liverpool” 4 and
question “X is one of the candidates for tonight’s vic-
tory.”

Since both Bayern Munchen and FC Barcelona
are equally good candidates, the attention mechanism
might put the same attention on both these candidates
in the context. Because the resulting representation is
computed as a weighted sum when using the above
blending mechanism, the vector will represent some-
thing in between these two good candidates. It could
very well be that the candidate with closest representa-
tion to the resulting blended embedding is Liverpool.

By using a sum rather than an average, our architec-
ture would correctly propose Bayern and Barcelona as
equally good candidates rather than potentially propos-
ing a third one.

4.2 Memory Networks
MemNNs (Weston et al., 2014) were applied to the task
of text comprehension in (Hill et al., 2015).

The best performing memory networks model setup
- window memory - uses windows of ﬁxed length (8)
centered around the candidate words as memory cells.
Due to this limited context window, the model is unable
to capture dependencies out of scope of this window.

4In the text comprehension task as deﬁned in (Hermann et
al., 2015) these named entities will be anonymized. However,
this problem can still emerge with anonymous word embed-
dings.

Table 3: Attention over an example where our system
failed to select the correct answer (entity43). The sys-
tem was probably mislead by the co-occurring word
’ﬁlm’. Namely, entity11 occurs 7 times in the whole
document and 6 times it is together with the word
’ﬁlm’. On the other hand, the correct answer occurs
only 3 times in total and only once together with ’ﬁlm’.

...

entity11 ﬁlm critic entity29 writes in his review that
”anyone nostalgic for childhood dreams of transfor-
mation will ﬁnd something to enjoy in an uplifting
movie that invests warm sentiment in universal themes
of loss and resilience , experience and maturity . ”
more : the best and worst adaptations of ”entity” en-
entity44 and entity46 star in director en-
tity43,
tity48’s crime ﬁlm about a hit man trying to save his
estranged son from a revenge plot.
entity11 chief
ﬁlm critic entity52 writes in his review that the ﬁlm

...

X stars in crime ﬁlm about hit man trying to save his
estranged son

Furthermore, the representation within such window is
computed simply as the sum of embeddings of words
in that window. By contrast, in our model the repre-
sentation of each individual word is computed using a
recurrent network, which not only allows it to capture
context from the entire document but also the embed-
ding computation is much more ﬂexible than a simple
sum.

To improve the initial accuracy, a heuristic approach
called self supervision is used in (Hill et al., 2015) to
help the network to select the right supporting “mem-
ories”. Plain MemNNs without this heuristic are not
competitive on these machine reading tasks. Our model
does not need any similar heuristics.

architecture was

4.3 Pointer Networks
Our model
inspired by Ptr-
Nets (Vinyals et al., 2015) in using an attention
mechanism to select the answer in the context rather
than to blend words from the context into an answer
representation. While a Ptr-Net consists of an encoder
as well as a decoder, which uses the attention to select
the output at each step, our model outputs the answer
in a single step. Furthermore, the pointer networks
assume that no input in the sequence appears more
than once, which is not the case in our settings.

4.4 Summary
Our model combines the best features of the architec-
tures mentioned above. We use recurrent networks to
“read” the document and the query as does Attentive
and Impatient readers and we use attention in a way
similar to Ptr-Nets. We also use summation of atten-

Table 6: Average duration of one epoch of training on
the four datasets.

Dataset
CNN
Daily Mail
CBT Named Entity
CBT Common Noun

Time per epoch
10h 22min
25h 42min
1h 5min
0h 56min

kept increasing the upper bound by 128 until we started
observing a consistent decrease in validation accuracy.
The region of the parameter space that we explored to-
gether with the parameters of the model with best vali-
dation accuracy are summarized in Table 7.

Table 7: Dimension of the recurrent hidden layer and of
the source embedding for the best model and the range
of values that we tested.

tion weights in a way similar to MemNNs (Hill et al.,
2015).

5 Evaluation
In this section we evaluate our model on the CNN,
Daily Mail and CBT datasets. We show that despite
its simplicity the model achieves state-of-the-art per-
formance on each of these datasets.

5.1 Training Details
To train the model we used stochastic gradient descent
with the ADAM update rule (Kingma and Ba, 2015)
and learning rate of 0.001 or 0.0005. We used negative
log-likelihood as the training objective.

The initial weights in the word embedding ma-
trix were drawn randomly uniformly from the interval
[−0.1, 0.1]. Weights in the GRU networks were initial-
ized by random orthogonal matrices (Saxe et al., 2014)
and biases were initialized to zero. The gradient clip-
ping (Pascanu et al., 2012) threshold was set to 10 and
the batch size to 32.

During training we randomly shufﬂed all examples
in each epoch. To speedup training, we always pre-
fetched 10 batches worth of examples and sorted them
according to the length of the document. This way each
batch contained documents of roughly the same length.
For the CNN and Daily Mail datasets, for each
batch we randomly reshufﬂed the assignment of named
entities to the corresponding word embedding vec-
tors to match the procedure proposed in (Hermann et
al., 2015). This guaranteed that word embeddings of
named entities were used only as semantically mean-
ingless labels not encoding any intrinsic features of the
represented entities. This forced the model to truly de-
duce the answer from the single context document as-
sociated with the question.

During training we evaluated the model performance
after each epoch and stopped the training when the er-
ror on the validation set started increasing.

The models usually converged after two epochs of
training. Time needed to complete a single epoch of
training on each dataset on an Nvidia K40 GPU is
shown in Table 6.

The hyperparameters, namely the recurrent hidden
layer dimension and the source embedding dimension,
were chosen by grid search. We started with a range
of 128 to 384 for both parameters and subsequently

Dataset
CNN
Daily Mail
CBT NE
CBT CN

Embedding

Rec. Hid. Layer
min max
512
128
128
1024
512
128
128
1536

best min max
512
384
512
512
512
384
256
512

128
128
128
128

best
128
384
384
384

Our model was implemented using Theano (Bastien
et al., 2012) and Blocks (van Merrienboer et al., 2015).

5.2 Evaluation Method
We evaluated the proposed model both as a single
model and using ensemble averaging.

For single models we are reporting results for the
best model as well as the average of accuracies for
the best 20% of models with best performance on val-
idation data since single models display considerable
variation of results due to random weight initialization,
even for identical hyperparameter values. Single model
performance may consequently prove difﬁcult to repro-
duce.

What concerns ensembles, we used simple averag-
ing of the answer probabilities predicted by ensemble
members5.

The ensemble models were chosen either as the top
70% of all trained models or using the following al-
gorithm: We started with the best performing model
according to validation performance. Then in each step
we tried adding the best performing model that had not
been previously tried. We kept it in the ensemble if it
did improve its validation performance and discarded
it otherwise. This way we gradually tried each model
once. We call the resulting model a greedy ensemble.

5.3 Results
Performance of our models on the CBT dataset is sum-
marized in Table 5, Table 4 shows results on the CNN
and Daily Mail datasets. The tables also list perfor-
mance of other published models that were evaluated
on these datasets.
Ensembles of our models set the
new state-of-the-art results on all evaluated datasets.
However, even our best single models outperform the
best previously reported results.

5Attempts to use the Constrained Optimization By Linear
Approximation (COBYLA) method (Powell, 1994) to opti-
mize the weights lead to overﬁtting on the validation data
with respect to which the optimization was done.

Table 4: Results of our AS Reader on the CNN and Daily Mail datasets. Results for models marked with † are
taken from (Hermann et al., 2015), results of models marked with ‡ are taken from (Hill et al., 2015). Performance
of ‡models was evaluated only on CNN dataset.

valid
Deep LSTM Reader †
55.0
Attentive Reader †
61.6
Impatient Reader †
61.8
MemNNs (single model) ‡
63.4
MemNNs (ensemble) ‡
66.2
68.6
AS Reader (single model)
68.4
AS Reader (avg for top 20%)
AS Reader (avg ensemble)
73.9
AS Reader (greedy ensemble) 74.5

CNN

Daily Mail
test
62.2
69.0
68.0
NA
NA
73.9
73.5
77.1
77.7

test valid
63.3
57.0
70.5
63.0
63.8
69.0
NA
66.8
NA
69.4
75.0
69.5
69.9
74.5
75.4
78.1
74.8
78.7

Table 5: Results of our AS Reader on the CBT datasets. Results marked with ‡ are taken from (Hill et al., 2015).
(∗)Human results were collected on 10% of the test set.

Named entity
valid
Humans (query) (∗)
NA
Humans (context+query) (∗)
NA
LSTMs (context+query) ‡
51.2
MemNNs (window memory + self-sup.) ‡ 70.4
73.8
AS Reader (single model)
AS Reader (avg for top 20%)
73.3
AS Reader (avg ensemble)
74.5
AS Reader (greedy ensemble)
76.2

Common noun
test
64.4
81.6
56.0
63.0
63.4
63.2
68.9
67.5

test valid
NA
52.0
81.6
NA
41.8
62.6
64.2
66.6
68.8
68.6
68.4
67.7
71.1
70.6
71.0
72.4

Table 8 then measures accuracy as the proportion of
test cases where the ground truth was among the top
k answers proposed by the greedy ensemble model for
k = 1, 2, 5.

CNN. On the CNN dataset our single model with
best validation accuracy achieves a test accuracy of
69.5%, this is 0.1% better than ensemble of models re-
ported in (Hill et al., 2015). The average performance
of the top 20% models according to validation accuracy
is 69.9% which is even 0.5% better than the single best-
validation model. This shows that there were many
models that performed better on test set than the best-
validation model. Fusing multiple models then gives
a signiﬁcant further increase in accuracy. Our simple-
average ensemble outperforms the accuracy of the best
previously reported ensemble (Hill et al., 2015) by 6%.
Daily Mail. On the Daily Mail dataset our single
model with accuracy of 73.9% outperforms the best
previous result achieved by the Attentive Reader (Her-
mann et al., 2015) by 4.9% absolute and our averaging
ensemble is by 8.7% absolute better.

CBT. In named entity prediction our best single
model with accuracy of 68.6% performs 2% absolute

better than the MemNN with self supervision, the av-
eraging ensemble performs 4% absolute better than the
best previous result. In common noun prediction our
single models is 0.4% absolute better than MemNN
however the ensemble improves the performance to
69% which is 6% absolute better than MemNN.

Table 8: Proportion of test examples for which the top
k answers proposed by the greedy ensemble included
the correct answer.

Dataset
CNN
Daily Mail
CBT NE
CBT CN

k = 1
74.8
77.7
71.0
67.5

k = 2
85.5
87.6
86.9
82.5

k = 5
94.8
94.8
96.8
95.4

6 Analysis
To further analyze the properties of our model, we ex-
amined the dependence of accuracy on the length of the
context document (Figure 2), the number of candidate

(a)

(c)

(b)

(d)

Figure 2: Sub-ﬁgures (a) and (b) plot the test accuracy against the length of the context document (for CNN
the count was multiplied by 10). The examples were split into ten buckets of equal size by their context length.
Averages for each bucket are plotted on each axis. Sub-ﬁgures (c) and (d) show distributions of context lengths in
the four datasets. The number of examples was multiplied by 10 for the CNN dataset.

(a)

(b)

(a)

(b)

Figure 3: Subﬁgure (a) illustrates how the model accu-
racy decreases with an increasing number of candidate
named entities. Subﬁgure (b) shows the overall distri-
bution of the number of candidate answers in the news
datasets. The number of examples was multiplied by
10 for the CNN dataset.

Figure 4: Subﬁgure (a) shows the model accuracy when
the correct answer is among n most frequent named
entities for n ∈ [1, 10]. Subﬁgure (b) shows the num-
ber of test examples for which the correct answer was
among n most frequent entities. The number of exam-
ples was multiplied by 10 for the CNN dataset.

llllllllll0.690.720.750.780.8140080012001600Number of tokens in context documentTest accuracyDatasetlDaily MailCNNCNN & Daily Mailllllllllll0.660.690.720.75300400500600700Number of tokens in context documentTest accuracyWord typelCommon NounsNamed EntitiesChildren's Book Test02000400060000500100015002000Number of tokens in context document# Examples in test dataDatasetDaily MailCNN (x10)CNN & Daily Mail0100200300300600900Number of tokens in context document# Examples in test dataWord typeNamed EntitiesCommon NounsChildren's book testllllllllll0.50.60.70.80.9204060Number of candidate answersTest accuracyDatasetlDaily MailCNNCNN & Daily Mail02000400060008000020406080Number of candidate answer entities# Examples in test dataDatasetDaily MailCNN (x10)CNN & Daily Maillllllllll0.50.60.70.80.9123456789nTest accuracyDatasetlDaily MailCNNCNN & Daily Mail050001000015000123456789n# Examples in test dataDatasetDaily MailCNN (x10)CNN & Daily Mailanswers (Figure 3) and the frequency of the correct an-
swer in the context (Figure 4).

On the CNN and Daily Mail datasets, the accuracy
decreases with increasing document length (Figure 2a).
We hypothesize this may be due to multiple factors.
Firstly long documents may make the task more com-
plex. Secondly such cases are quite rare in the training
data (Figure 2b) which motivates the model to special-
ize on shorter contexts. Finally the context length is
correlated with the number of named entities, i.e. the
number of possible answers which is itself negatively
correlated with accuracy (see Figure 3).

On the CBT dataset this negative trend seems to dis-
appear (Fig. 2c). This supports the later two expla-
nations since the distribution of document lengths is
somewhat more uniform (Figure 2d) and the number
of candidate answers is constant (10) for all examples
in this dataset.

The effect of increasing number of candidate an-
swers on the model’s accuracy can be seen in Figure 3a.
We can clearly see that as the number of candidate an-
swers increases, the accuracy drops. On the other hand,
the amount of examples with large number of candidate
answers is quite small (Figure 3b).

Finally, since the summation of attention in our
model inherently favours frequently occurring tokens,
we also visualize how the accuracy depends on the fre-
quency of the correct answer in the document. Fig-
ure 4a shows that the accuracy signiﬁcantly drops as
the correct answer gets less and less frequent in the
document compared to other candidate answers. On
the other hand, the correct answer is likely to occur fre-
quently (Fig. 4a).

6.1 Comparison to Weighted Average Blending
We hypothesized in Section 4.1 that the fact that the
Attentive Reader uses attention to create a blended rep-
resentation potentially harms its performance. In order
to verify this intuition, we implemented blending into
our model, bringing it closer to the Attentive Reader
(Hermann et al., 2015).

In this modiﬁed model we compute attention weights
si in the same way as in our original model (see Eq. 1).
However, we replace Eq. 2 with the following equa-
tions:

(cid:88)

r =

sie(wi)

(3)

i

P (a(cid:48)|q, d) ∝ exp (r · e(a(cid:48)))

(4)
where r is the blended response embedding and a(cid:48) ∈ A
is a possible candidate response. This change in the
architecture indeed lead to a signiﬁcant decrease in ac-
curacy across all four datasets.

Namely, on each CBT dataset the difference was al-
most 15% while on CNN and Daily Mail the decrease
was over 6% and 2% respectively. Besides the training
time for the models with blending was several times

longer than our attention sum architecture both mea-
sured by the time per epoch and by the number of
epochs required for the model to converge.

7 Conclusion
In this article we presented a new neural network archi-
tecture for natural language text comprehension. While
our model is simpler than previously published models,
it gives a new state-of-the-art accuracy on all the eval-
uated datasets.

References
[Bahdanau et al.2015] Dzmitry Bahdanau, Kyunghyun
Cho, and Yoshua Bengio. 2015. Neural Machine
Translation by Jointly Learning to Align and Trans-
late. ICLR.

[Bastien et al.2012] Fr´ed´eric Bastien, Pascal Lamblin,
Razvan Pascanu, James Bergstra, Ian J. Goodfellow,
Arnaud Bergeron, Nicolas Bouchard, and Yoshua
Bengio. 2012. Theano: new features and speed im-
provements. Deep Learning and Unsupervised Fea-
ture Learning NIPS 2012 Workshop.

[Cho et al.2014] Kyunghyun Cho, Bart van Merrien-
boer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi
Bougares, Holger Schwenk, and Yoshua Bengio.
2014. Learning Phrase Representations using RNN
Encoder-Decoder for Statistical Machine Transla-
tion. EMNLP, jun.

[Ferrucci et al.2010] David Ferrucci, Eric Brown, Jen-
nifer Chu-Carroll,
James Fan, David Gondek,
Aditya a. Kalyanpur, Adam Lally, J. William Mur-
dock, Eric Nyberg, John Prager, Nico Schlaefer, and
Chris Welty. 2010. Building Watson: An Overview
of the DeepQA Project. AI Magazine, 31(3):59–79.

[Hermann et al.2015] Karl Moritz Hermann, Tomas
Kocisky, Edward Grefenstette, Lasse Espeholt, Will
Kay, Mustafa Suleyman, and Phil Blunsom. 2015.
Teaching machines to read and comprehend. In Ad-
vances in Neural Information Processing Systems,
pages 1684–1692.

[Hill et al.2015] Felix Hill, Antoine Bordes, Sumit
Chopra, and Jason Weston. 2015. The goldilocks
principle: Reading children’s books with ex-
arXiv preprint
plicit memory representations.
arXiv:1511.02301.

[Kingma and Ba2015] Diederik

Jimmy Lei Ba.
Stochastic Optimization.
on Learning Representations, pages 1–13.

2015. Adam:

P. Kingma

and
a Method for
International Conference

[Pascanu et al.2012] Razvan Pascanu, Tomas Mikolov,
and Yoshua Bengio. 2012. On the difﬁculty of train-
ing recurrent neural networks. Proceedings of The
30th International Conference on Machine Learn-
ing, pages 1310–1318.

[Powell1994] Michael J. D. Powell, 1994. Advances in
Optimization and Numerical Analysis, chapter A Di-
rect Search Optimization Method That Models the
Objective and Constraint Functions by Linear Inter-
polation, pages 51–67. Springer Netherlands, Dor-
drecht.

[Saxe et al.2014] Andrew M Saxe, James L Mcclelland,
and Surya Ganguli. 2014. Exact solutions to the
nonlinear dynamics of learning in deep linear neural
networks. ICLR.

[Taylor1953] Wilson L Taylor. 1953. Cloze procedure:
Journalism

a new tool for measuring readability.
and Mass Communication Quarterly, 30(4):415.

[van Merrienboer et al.2015] Bart

van Merrienboer,
Dzmitry Bahdanau, Vincent Dumoulin, Dmitriy
Serdyuk, David Warde-farley,
Jan Chorowski,
and Yoshua Bengio.
2015. Blocks and Fuel :
Frameworks for deep learning. pages 1–5.

[Vinyals et al.2015] Oriol Vinyals, Meire Fortunato,
and Navdeep Jaitly. 2015. Pointer networks. In Ad-
vances in Neural Information Processing Systems,
pages 2674–2682.

[Weston et al.2014] Jason Weston, Sumit Chopra, and
Antoine Bordes. 2014. Memory networks. arXiv
preprint arXiv:1410.3916.

