6
1
0
2

 
r
a

M
2

 

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
0
0
8
0
0

.

3
0
6
1
:
v
i
X
r
a

Speciﬁcation Test based on Convolution-type

Distribution Function Estimates for Non-linear

Auto-regressive Processes

Kun Ho Kim, and Jiwoong Kim

March 3, 2016

Abstract

The paper proposes a speciﬁcation test based on two estimates of distribution func-

tion. One is the traditional kernel distribution function estimate and the other is a

newly proposed convolution-type distribution function estimate. Asymptotic proper-

ties of the new estimate are studied when the innovation density is known and when

it is unknown. The MISE-type statistic based on these estimates is suggested to test

parametric speciﬁcations of the mean and volatility functions. The relating asymptotic

results are obtained and the ﬁnite-sample properties are studied based on the bootstrap

methodology. A simulation study shows that the proposed test competes favorably to

benchmark tests in terms of the empirical level and power.

Key words: Speciﬁcation test, Distribution function, Convolution, Kernel density,

Auto-regressive models, Non-linearity, Conditional heteroskedasticity

1

1

Introduction

Consider the following model framework in time series:

Xi = µ(Xi−1) + σ(Xi−1)ǫi,

i ∈ Z

(1)

where Xi is a stationary process, µ : R → R and σ : R → R are unknown conditional
mean and variance functions respectively, and ǫi are independent and identically distributed

(iid) innovations. The purpose of this paper is to construct a speciﬁcation test based on

cumulative distribution function (c.d.f.) estimates for Xi in (1).

Various tests for (1) have been proposed in the time series literature: A¨ıt-Sahalia (1996)

proposed a parametric speciﬁcation test by comparing the nonparametric kernel density

estimate of the marginal density of Xi with its closed-form density estimate under the para-

metric form. Given that the kernel density estimate always converges to the true density,

the diﬀerence between these estimates would converge to zero only if the parametric forms

are correctly speciﬁed. In stead of using the density estimates, Corradi and Swanson (2005)

propose a test that utilizes the empirical c.d.f. of Xi and the closed-form c.d.f. estimate

under the parametric forms of mean and variance functions. Given that the limiting distri-

bution of their test statistic is a functional of a Gaussian process, they employ bootstrap

procedures to carry out inference.

Despite the innovative nature of the idea, the tests in A¨ıt-Sahalia (1996) and Corradi and

Swanson (2005) are not applicable if the closed-form density and closed-form c.d.f. of Xi are

not available. This signiﬁcantly reduces the applicability of these tests because the closed-

form density and c.d.f. are typically unavailable for many prominent non-linear time series

models, such as the autoregressive conditional heteroskedastic (ARCH) process. To address

this issue, Kim, Zhang and Wu (2015) introduce a convolution-type density estimate that

1

is used to test for the framework (1). They propose a test statistic based on the maximal

deviation of this convolution density estimate from the traditional kernel density estimate.

Since the convolution only requires independence between the mean and variance, one needs

not to know the closed-form density/c.d.f. of Xi, which greatly enhances the applicability of

the proposed test.

The potential problem of the test in Kim et al (2005) is that their test depends on the

kernel density estimate of which the convergence is very slow. The slow convergence could

potentially lead to size distortion and a low power of the test. One way to address this

issue is to construct a test that employs c.d.f. estimates based on kernel smoothing and

convolution. Given the additive mean and multiplicative variance of (1), the convolution

can be applied to obtain a c.d.f. estimate of Xi. A simple modiﬁcation of (1) shows:

Yi = m(Xi−1) + ǫi

(2)

where Yi := Xi/σ(Xi−1) and m(x) := µ(x)/σ(x). Note that there is independence between
m(Xi−1) and ǫi, such that the convolution applies. While Kim et al (2015) use it for density
estimation, we use convolution to estimate the c.d.f. of Xi and compare it to the kernel c.d.f

estimate because both the convolution-type c.d.f. estimate and the kernel c.d.f. estimate

achieve the root-n-consistency. As shown in Remark 1 of this paper, our test based on the

kernel c.d.f. estimate and the convolution-type c.d.f. estimate enjoys a faster convergence

than that in Kim et al (2015), which explains why our test tends to perform better than

its competitors including that in Kim et al (2015), as shown by the simulation study in this

work.

The organization of the paper is the following: Section 2 introduces the technical as-

sumptions required for our results and discusses the methodology on the convolution-based

c.d.f. estimate. We ﬁrst consider the case of known innovation density to derive the relating

2

asymptotic properties. The result is later generalized to the case of unknown innovation

density, which has more of practical relevance than the former. Section 3 describes how

to construct a speciﬁcation test based on the kernel c.d.f. estimate and the convolution

c.d.f. estimate from the previous section. The asymptotic distribution of the test statistics

is derived, and its ﬁnite-sample properties based on the bootstrap methodology are examined

by a simulation study. Section 4 concludes the paper and discusses related future research.

Tables and ﬁgures are relegated to the appendix of the paper.

2 Methodology

For simplicity, we ﬁrst consider the autoregressive process with a homoskedastic innovation:

Xi = mθ(Xi−1) + ǫi

(3)

where θ is an unknown parameter. The homoskedasticity assumption here will be relaxed

to the case of conditional heteroskedasticity later. The parametric speciﬁcation that needs

to be tested is the following:

H0 : m(·) = mθ(·)

(4)

where m(·) is the true mean function. Testing (4) has been conducted in various contexts.
Corradi and Swanson (2005) propose a MISE-type statistic that utilizes an empirical c.d.f.

of Xi and its parametric closed-form c.d.f.

function. Kim et al (2015) consider a kernel

density estimate and a convolution-type density estimate for Xi to test (4) using the maximal

distance between the two estimates.

In this paper, we combine the ideas of Corradi and Swanson (2005) and Kim et al (2015)

to propose the test statistic based on the kernel c.d.f. estimate and the convolution-type

3

c.d.f. estimate for Xi. First, deﬁne the kernel c.d.f. estimate for Xi:

ˆFk(x) =

1
n

n

Xi=1

b (cid:19)
G(cid:18)x − Xi

(5)

−∞

where G(u) =R u
K(x)dx and K(·) is a kernel function. Here b is a bandwidth. Given the
independence between the mean and innovation of (3), the c.d.f. of Xi, FX (·), also can be
estimated by the following convolution c.d.f. estimate:

ˆFc(x) =ZR

ˆFǫ(x − t) ˆfg(t)dt

(6)

where Fǫ(·), the c.d.f. of ǫi, and fg(·), the density function of mθ(Xi−1), are estimated,
respectively, by:

n

b (cid:19)
G(cid:18)x − ˆǫi

Xi=1
K(cid:18)x − mˆθ (Xi−1)

b

(cid:19)

ˆfg(x) =

1
nb

ˆFǫ(x) =

1
n

n

Xi=1

Here ˆǫi = Xi − mˆθ(Xi−1) and ˆθ is an √n-consistent estimator of parameter θ in (3), respec-
tively. Obviously, ˆFk(x) in (5) converges to the true c.d.f. of Xi regardless of the parametric
form of mθ(·) in (3), while ˆFc(x) in (6) converges only under its correct form. Hence the
properly centered and scaled diﬀerence between the two c.d.f. estimates in (5) and (6) can

be used as a statistic for testing the parametric speciﬁcation of (3).

2.1 Assumptions

Some notations are needed to introduce the assumptions in this study. For a random vari-

able W , write W ∈ Lp, p > 0, if kWkp := [E(|W|p)]1/p < ∞, and write kWk = kWk2.
We deﬁne the projection operator P as Pi[·] ≡ E[·|Ii] − E[·|Ii−1], where Ii = (ǫi, ǫi−1, . . .).
The following assumptions are needed to derive the asymptotic properties of the convolution

4

c.d.f. estimator:

Assumption 1. Let the kernel function K be bounded, symmetric, with bounded support

[−A, A], K ∈ C1[−A, A], K(±A) = 0 and supu |K′(u)| < ∞.

Assumption 2. supx6=x′ |mθ(x) − mθ(x′)|/|x − x′| < 1, and ǫi ∈ Lp, p > 0

Assumption 3.

O(|x|−β) for some β > 0.

supx(cid:2)Fǫ(x) +(cid:12)(cid:12)F ′

ǫ (x)(cid:12)(cid:12) +(cid:12)(cid:12)F ′′

ǫ (x)(cid:12)(cid:12)(cid:3) < ∞, and as |x| → ∞, Fǫ(x) =

Assumption 4. ˆθ is an estimate of θ such that

√n(cid:16)ˆθ − θ(cid:17) =

1
√n

n

Xi=1

Ri + oP (1)

where Ri = R(ǫi, ǫi−1, . . .) satisﬁes the short-range dependence condition

∞

Xi=0

kP0Rik < ∞.

(7)

(8)

Assumption 5. Let

˙mθ(x) = ∂mθ(x)/∂θ exist, and |mθ(x) − mθ0(x)| ≤ ˙mθ(x)|θ − θ0|

with E [ ˙m2

θ(Xi)] < ∞.

Assumption 1 allows popular kernels such as Parzen, Epanechnikov and uniform kernels

among others. Assumption 2 represents a contraction condition and it ensures that process

Xi is a stationary and ergodic solution of the form Xi = G (ǫi, ǫi−1,· · ·). The process is also
causal. For many non-linear times series models, the innovation c.d.f. satisﬁes Assumption

3. Assumption 4 is an important intermediate step in obtaining a central limit theorem for

an estimate ˆθ. In certain situations, (7) is called the Bahadur representation. Assumption 5

is not the weakest possible. Based on these assumptions, we introduce the convolution c.d.f.

estimate and investigate its asymptotic properties.

5

2.2 Convolution c.d.f. estimation

i=1 Fǫ (x − mˆθ(Xi−1)), where ˆθ is an √n-consistent estimate of θ and Fǫ(·) is

Let ˆSn(x) =Pn
the c.d.f. of ǫi in (3). Given Fǫ(·), the convolution c.d.f. estimator is:
(cid:19) Fǫ(x − t)dt
Fǫ (x − ub − mˆθ(Xi−1)) du

˘Fc(x) = ZR

=

1

n

n

b

1
nb

K(cid:18) t − mˆθ(Xi−1)
Xi=1

Xi=1
nZ K(u)
nZ K(u) ˆSn(x − ub)du

1

=

(9)

(10)

Given Assumptions 1–5, we introduce the following lemmas:

Lemma 1. Given the c.d.f. of ǫi, Fǫ(·),

1

√n(cid:16) ˆSn(x) − nFX (x)(cid:17) ⇒ N(cid:0)0, σ2

1(x)(cid:1)

where FX (·) is the c.d.f. of Xi, and σ1(x) = kP∞i=1 P0 [Fǫ (x − mθ(Xi−1)) + c0R (Xi−1, θ, ǫi)]k.

proof) Note that Assumption 4 ensures:

√n(cid:16)ˆθ − θ(cid:17) = −

1
√n

n

Xi=1

R (Xi−1, θ, ǫi) + oP(1)

(11)

where ER (Xi−1, θ, ǫi) = 0. Moreover, by the ergodicity of Xi under Assumption 2,

cn =

1
n

n

Xi=1

fǫ (x − mθ(Xi−1)) ˙mθ

P

→ c0 = E [fǫ (x − mθ(Xi−1)) ˙mθ]

(12)

where ˙mθ(x) = ∂mθ(x)/∂θ and fǫ(·) is the density function for ǫi. By a Taylor’s expansion
of ˆSn(x),

ˆSn(x) =

n

Xi=1

Fǫ (x − mθ(Xi−1)) −(cid:16)ˆθ − θ(cid:17)

n

Xi=1

6

fǫ (x − mθ(Xi−1)) ˙mθ + OP(cid:16)(ˆθ − θ)2(cid:17) (13)

Then, by (11)–(13),

1

√n(cid:16) ˆSn(x) − nFX (x)(cid:17) =

1
√n

n

Xi=1
[Fǫ (x − mθ(Xi−1)) − FX (x) + c0 R(Xi−1, θ, ǫi)] − cn oP(1)
1
OP(cid:16)(ˆθ − θ)2(cid:17) + (cn − c0)
√n

R (Xi−1, θ, ǫi)

1
√n

(14)

n

Xi=1

+

By Theorem 2 in Wu and Shao (2004),

n

n

Xi=1
Xi=1

≤

kP0 [Fǫ (x − mǫ(Xi−1)) − FX (x) + c0R (Xi−1, θ, ǫi)]k

kP0Fǫ (x − mǫ(Xi−1))k + c0

n

Xi=1

kP0R (Xi−1, θ, ǫi)k < ∞

(15)

Then, by (15), ER (Xi−1, θ, ǫi) = 0, E [Fǫ (x − mθ(Xi−1)) − FX(x)] = 0 and Theorem 3 in
Wu (2005),

1
√n

n

Xi=1

[Fǫ (x − mθ(Xi−1)) − FX (x) + c0 R(Xi−1, θ, ǫi)] ⇒ N(cid:0)0, σ2

1(x)(cid:1)

(16)

By applying (11), (12), (16) and 1√nPn

to (14), the lemma follows.

i=1 R (Xi−1, θ, ǫi) → N (0,kP∞i=1 P0R (Xi−1, θ, ǫi) k2)

For Lemmas 2–4, we deﬁne the following processes:

(cid:3)

ˆFǫ(cid:16)x, ˆθ(cid:17) =

ˆFǫ (x, θ0) =

ˆfg(cid:16)x, ˆθ(cid:17) =

ˆfg (x, θ0) =

1
n

1
n

1
nb

1
nb

n

n

b

b

G(cid:18)x − Xi + mˆθ(Xi−1)
(cid:19)
Xi=1
(cid:19)
G(cid:18)x − Xi + mθ0(Xi−1)
Xi=1
(cid:19)
K(cid:18) x − mˆθ (Xi−1)
Xi=1
(cid:19)
K(cid:18) x − mθ0 (Xi−1)
Xi=1

b

b

n

n

where G(u) =R u

−∞

K(x)dx.

7

Lemma 2.

ZR(cid:16) ˆFǫ(cid:16)x, ˆθ(cid:17) − ˆFǫ (x, θ0)(cid:17)2

nb(cid:19)
dx = OP(cid:18) 1

proof) Let C > 0 such that RR |G(u + δ) − G(u)|2du ≤ Cδ2. By the Cauchy-Schwarz

inequality,

n

1

1

n

dx

ZR(cid:16) ˆFǫ(cid:16)x, ˆθ(cid:17) − ˆFǫ (x, θ0)(cid:17)2
n2 ZR" n
Xi=1 (cid:18)G(cid:18)x − ˆǫi
Xi=1 (cid:20)G(cid:18)x − ˆǫi
n2 ZR
Xi=1 ZR(cid:20)G(cid:18)x − ˆǫi
Xi=1 ZR(cid:20)G(cid:18)u +

b (cid:19)(cid:19)#2
b (cid:19) − G(cid:18)x − ǫi
b (cid:19)(cid:21)2
b (cid:19) − G(cid:18)x − ǫi
b (cid:19)(cid:21)2
b (cid:19) − G(cid:18)x − ǫi
b (cid:19) − G(u)(cid:21)2
ǫi − ˆǫi

bdu ≤

1
n

1
n

dx

n

n

=

≤

=

=

dx

dx

Cb
n

n

b (cid:19)2
Xi=1 (cid:18)ǫi − ˆǫi

By Assumptions 4 and 5,

ZR(cid:16) ˆFǫ(cid:16)x, ˆθ(cid:17) − ˆFǫ (x, θ0)(cid:17)2

dx ≤

≤

Cb
n

Cb
n

Lemma 2-1.

n

n

b

Xi=1 (cid:18)mˆθ (Xi−1) − mθ0 (Xi−1)


Xi=1



ˆθ − θ0(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)

˙mθ0(Xi−1)
b

2

(cid:19)2
= OP(cid:18) 1
nb(cid:19)

(cid:3)

ZR(cid:16) ˆFǫ (x, θ0) − E ˆFǫ (x, θ0)(cid:17)2

dx = OP(b/n)

(17)

proof) Let li(x) := G ((x − ǫi)/b). Note that

E(cid:16) ˆFǫ (x, θ0) − E ˆFǫ (x, θ0)(cid:17)2

= E( 1

n

n

Xi=1

(li(x) − Eli(x)))2

8

n

E(li(x) − Eli(x))2

Xi=1
E(l0(x) − El0(x))2
El2

0(x).

1
n2

1
n
1
n

=

=

≤

(18)

The second inequality follows from the fact that expectation of the cross product terms is

zero due to the independence of ǫi. Observe that

El2

0(x) = ZR

G2(cid:18)x − y

b (cid:19) dFǫ(y)

= bZRZR

G2(u) du dFǫ(y)

= O(b).

(19)

By Fubini’s theorem, (18), and (19), (17) follows, thereby completing the proof of the lemma.

Lemma 3.

ZR(cid:16) ˆFǫ (x, θ0) − Fǫ(x)(cid:17)2

dx = OP(cid:18)b4 +

1

nb(cid:19)

proof) Note the following:

E ˆFǫ(x, θ0) − Fǫ(x) = EG(cid:18)x − ǫi

b (cid:19) − Fǫ(x)

= bZR
= b(cid:18)−
= ZR

G(u)fǫ(x − ub)du − Fǫ(x)
1
b

u=∞
u=−∞
K(u)Fǫ(x − ub)du − Fǫ(x)

G(u)Fǫ(x − ub)(cid:12)(cid:12)(cid:12)

+

By a Taylor’s expansion on (20),

E ˆFǫ(x, θ0) − Fǫ(x) =

b2φK

2

f′ǫ(x) +

b4

4!ZR

u4K(u)f (3)

ǫ (x0)du

where x0 ∈ (x, x − ub). Moreover,

ZR(cid:16) ˆFǫ (x, θ0) − E ˆFǫ (x, θ0)(cid:17)2

9

dx = OP(1/(nb))

1

bZR

K(u)Fǫ(x − ub)du(cid:19) − Fǫ(x)

(20)

(21)

(22)

By (21) and (22),

dx

ZR(cid:16) ˆFǫ (x, θ0) − Fǫ(x)(cid:17)2
= ZR(cid:16) ˆFǫ (x, θ0) − E ˆFǫ (x, θ0) + E ˆFǫ (x, θ0) − Fǫ(x)(cid:17)2
≤ 2ZR(cid:16) ˆFǫ (x, θ0) − E ˆFǫ (x, θ0)(cid:17)2

dx + 2ZR(cid:16)E ˆFǫ (x, θ0) − Fǫ(x)(cid:17)2

dx

dx = OP(cid:18) 1

nb

+ b4(cid:19)

(cid:3)

Lemma 4.

sup

ZRh ˆFǫ(cid:16)x − y, ˆθ(cid:17) − Fǫ(x − y)i h ˆfg(cid:16)y, ˆθ(cid:17) − fg(y)i dy(cid:12)(cid:12)(cid:12)(cid:12)

x (cid:12)(cid:12)(cid:12)(cid:12)
proof) Choose C0 > 0 such that RR |K(u + δ) − K(u)|2du ≤ δ2C0. Then, by the Cauchy-

Schwarz inequality and by Assumptions 4 and 5,

= OP r 1

n2b4 +

n!

b

ZRh ˆfg(cid:16)y, ˆθ(cid:17) − ˆfg (y, θ0)i2

n

dy

(cid:19) − K(cid:18) y − mθ0(Xi−1)

b

n

n

b

≤

≤

1
nb2

1
nb2

Xi=1 ZR(cid:20)K(cid:18)y − mˆθ(Xi−1)
C0b(cid:18)mˆθ(Xi−1) − mθ0(Xi−1)
Xi=1
!2
C0b |ˆθ − θ0| ˙mθ0(Xi−1)
Xi=1
≤
nb3(cid:19)
= OP(cid:18) 1

1
nb2

b

b

(cid:19)2

(cid:19)(cid:21)2

dy

(23)

Then,

(cid:18)ZRh ˆFǫ(cid:16)x − y, ˆθ(cid:17) − Fǫ(x − y)i h ˆfg(cid:16)y, ˆθ(cid:17) − fg(y)i dy(cid:19)2
dy ZRh ˆfg(cid:16)y, ˆθ(cid:17) − fg(y)i2
≤ ZRh ˆFǫ(cid:16)x − y, ˆθ(cid:17) − Fǫ(x − y)i2
= ZRh ˆFǫ(cid:16)x − y, ˆθ(cid:17) − ˆFǫ (x − y, θ0) + ˆFǫ (x − y, θ0) − Fǫ(x − y)i2
×ZRh ˆfg(cid:16)y, ˆθ(cid:17) − ˆfg (y, θ0) + ˆfg (y, θ0) − fg(y)i2

dy

dy

dy

10

≤ (cid:18)2ZRh ˆFǫ(cid:16)x − y, ˆθ(cid:17) − ˆFǫ (x − y, θ0)i2
×(cid:18)2ZRh ˆfg(cid:16)y, ˆθ(cid:17) − ˆfg (y, θ0)i2

dy + 2ZRh ˆFǫ (x − y, θ0) − Fǫ (x − y)i2
dy + 2ZRh ˆfg (y, θ0) − fg (y)i2

dy(cid:19)

dy(cid:19)

Then, by Lemmas 2 and 3, (23), and Lemma 1 in Kim et al (2015),

(cid:18)ZRh ˆFǫ(cid:16)x − y, ˆθ(cid:17) − Fǫ(x − y)i h ˆfg(cid:16)y, ˆθ(cid:17) − fg(y)i dy(cid:19)2

= OP(cid:18) 1

n2b4 +

b

n(cid:19)

(24)

Hence the lemma follows.

(cid:3)

n

Lemma 5. Recall c0 = E [ ˙mθ fǫ (x − mθ (Xi−1))]. Then,
1
√n

[Fǫ (x − mθ (Xi−1)) + Fg (x − ǫi) − 2FX (x) + c0R (Xi−1, θ, ǫi)] ⇒ N(cid:0)0, σ2

Xi=1

2(x)(cid:1)

where σ2(x) = kP∞i=1 P0 [Fǫ (x − mθ(Xi−1)) + Fg (x − ǫi) + c0R (Xi−1, θ, ǫi)]k.
proof) Recall from (11) that ER (Xi−1, θ, ǫi) = 0. Then,

E [Fǫ (x − mθ (Xi−1)) + Fg (x − ǫi) − 2FX (x) + c0R (Xi−1, θ, ǫi)] = 0

By a similar argument to (15),

∞

Xi=1

kP0 [Fǫ (x − mǫ(Xi−1)) + Fg (x − ǫi) − 2FX (x) + c0R (Xi−1, θ, ǫi)]k < ∞

Hence, by Theorem 3 in Wu (2005), the lemma follows.

Lemma 6. Deﬁne ˜Sn(x) =Pn

1

i=1(cid:2)Fǫ(cid:0)x − mˆθ (Xi−1)(cid:1) + Fg (x − ˆǫi)(cid:3). Then,

√n(cid:16) ˜Sn(x) − 2nFX (x)(cid:17) ⇒ N(cid:0)0, σ2

2(x)(cid:1)

proof) By a Taylor’s expansion,

(cid:3)

n

1
√n

Xi=1(cid:2)Fǫ(cid:0)x − mˆθ (Xi−1)(cid:1) + Fg (x − ˆǫi)(cid:3) −

n

n

1
√n

Xi=1
[Fǫ (x − mθ (Xi−1)) + Fg (x − ǫi)] +
O(cid:18)(cid:16)ˆθ − θ(cid:17)2(cid:19)
1
√n

∂
∂θ

= (cid:16)ˆθ − θ(cid:17) 1
Xi=1
√n
= −√n(cid:16)ˆθ − θ(cid:17) cn +

[Fǫ (x − mθ (Xi−1)) + Fg (x − ǫi)]

1
√n

O(cid:18)(cid:16)ˆθ − θ(cid:17)2(cid:19)

(25)

11

where cn = 1

nPn
i=1 [ ˙mθ fǫ (x − mθ (Xi−1))]. By (11) and (25),
1
√n(cid:16) ˜Sn(x) − 2nFX (x)(cid:17)
1
√n

n

=

Xi=1

[Fǫ (x − mθ (Xi−1)) + Fg (x − ǫi) − 2FX (x) + c0R (Xi−1, θ, ǫi)]

+ (cn − c0)

1
√n

n

Xi=1

R (Xi−1, θ, ǫi) − cn oP(1) +

1
√n

O(cid:18)(cid:16)ˆθ − θ(cid:17)2(cid:19)

Then, by 1√nPn
→ c0, the lemma follows.

cn

P

i=1 R (Xi−1, θ, ǫi) → N(cid:0)0,kP∞i=1 P0R (Xi−1, θ, ǫi) k2(cid:1) and by (11), Lemma 5 and

(cid:3)

We are now ready to state the main theoretic result:

Theorem 1A. For each x ∈ X ,

√n(cid:16) ˆFc(x) − FX (x)(cid:17) ⇒ N(cid:0)0, σ2

2(x)(cid:1)

proof) Recall ˆFc(x) deﬁned by (6), the convolution c.d.f. estimate when the innovation c.d.f. is

unknown. By (24), we have the following:

ZRh ˆFǫ(cid:16)x − y, ˆθ(cid:17) − Fǫ(x − y)i h ˆfg(cid:16)y, ˆθ(cid:17) − fg(y)i dy

= ˆFc(x) −ZR
= OP r 1

Fǫ(x − y) ˆfg(cid:16)y, ˆθ(cid:17) dy −ZR
n2b4 +

n!

b

ˆFǫ(cid:16)x − y, ˆθ(cid:17) fg(y)dy + FX (x)

Note also

ZR

Fǫ(x − y) ˆfg(cid:16)y, ˆθ(cid:17) dy = ZR

1
nb

n

Xi=1

K(cid:18) y − mˆθ (Xi−1)

b

(cid:19) Fǫ(x − y)dy

ZR

ˆFǫ(cid:16)x − y, ˆθ(cid:17) fg(y)dy = ZR

1
n

n

Xi=1

G(cid:18) y − ˆǫi

b (cid:19) fg(x − y)dy

12

(26)

(27)

(28)

Then, by (26)–(28),

ˆFc(x) + FX (x) = OP r 1

n2b4 +

b

n! +

1
n

By the integration-by-parts,

+

n

Xi=1ZR
Xi=1ZR

n

b
n

K(u)Fǫ(cid:0)x − ub − mˆθ (Xi−1)(cid:1) du
G(u)fg (x − ub − ˆǫi) du

(29)

ZR

G(u)Fg (x − ub − ˆǫi)(cid:12)(cid:12)(cid:12)
G(u)fg (x − ub − ˆǫi) du = −
1
b ZR

K(u)Fg (x − ub − ˆǫi) du

=

u=∞
u=−∞

1
b

+

1

b ZR

G′(u)Fg (x − ub − ˆǫi) du
(30)

By (29) and (30),

ˆFc(x) + FX (x) = OP r 1

n2b4 +

b

n! +

1

nZR

K(u) ˜Sn(x − ub)du

(31)

By a Taylor’s expansion,

ZR

K(u) ˜Sn(x − ub)du = ˜Sn(x) +

b2φK

2

˜S′′n(x) +

b4

4! ZR

u4K(u) ˜S(4)

n (x0)du

(32)

where φK =RR u2K(u)du and x0 ∈ (x, x − ub). Then, by (31) and (32),

=

1

√n(cid:16) ˆFc(x) − FX (x)(cid:17)
√n(cid:16) ˜Sn(x) − 2nFX (x)(cid:17) + OP r 1
nb4 + b!
b4√n
n ! +
  ˜S′′n(x)
4! (cid:18) 1
nZR

φK b2√n

+

2

u4K(u) ˜S(4)

n (x0)du(cid:19)

(33)

where ˜S′′n(x) = Pn

the theorem follows.

i=1(cid:2)f′ǫ(cid:0)x − mˆθ (Xi−1)(cid:1) + f′g (x − ˆǫi)(cid:3). Hence, by Assumption 3 and Lemma 6,

(cid:3)

Theorem 1B. Let D2(x) =P∞i=1 P0 [Fǫ (x − mθ(Xi−1)) + Fg(x − ǫi) + c0R (Xi−1, θ, ǫi)]. Then,
for any compact interval X ,

n√nh ˆFc(x) − FX (x)i , x ∈ Xo ⇒ {W2(x), x ∈ X}

13

where W2(x) is a mean-zero Gaussian process with covariance function cov [W2(x), W2(x′)] =

E [D2(x)D2(x′)].

proof) It is straightforward to verify the ﬁnite-dimensional convergence based on the Cram´er-

Wold device. Hence we need to verify the tightness condition. By the Lipschitz continuity of FX (·)
and Fǫ(·) (i.e. Assumption 3),

n

n

n

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Xi=1(cid:2)Fǫ(x − mˆθ(Xi−1)) − FX (x)(cid:3) −
Xi=1(cid:13)(cid:13)(cid:2)Fǫ(x − mˆθ(Xi−1)) − Fǫ(x′ − mˆθ(Xi−1))(cid:3)(cid:13)(cid:13)
≤
≤ |x − x′|2 O(n)

Xi=1(cid:2)Fǫ(x′ − mˆθ(Xi−1)) − FX (x′)(cid:3)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Xi=1(cid:13)(cid:13)(cid:0)FX (x′) − FX (x)(cid:1)(cid:13)(cid:13)

2 +

n

2

2

Similarly, by the Lipschitz continuity of FX (·) and Fg(·) (i.e. Assumption 3),

n

n

n

[Fg(x − ˆǫi) − FX (x)] −

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Xi=1
Xi=1(cid:13)(cid:13)(cid:2)Fg(x − ˆǫi) − Fg(x′ − ˆǫi)(cid:3)(cid:13)(cid:13)
≤
≤ |x − x′|2 O(n)

Xi=1(cid:2)Fg(x′ − ˆǫi) − FX (x′)(cid:3)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Xi=1(cid:13)(cid:13)(cid:0)FX (x′) − FX (x)(cid:1)(cid:13)(cid:13)

2 +

n

2

2

By (33),

=

√n(cid:16) ˆFc(x) − FX (x)(cid:17)
1
√n

n

Xi=1(cid:2)Fǫ(x − mˆθ(Xi−1)) − FX(x) + Fg(x − ˆǫi) − FX (x)(cid:3) + oP(1)

(34)

(35)

(36)

Hence, by applying (34) and (35) to (36),

√n(cid:16) ˆFc(x) − FX (x)(cid:17) − √n(cid:16) ˆFc(x′) − FX (x′)(cid:17)(cid:12)(cid:12)(cid:12)
E(cid:12)(cid:12)(cid:12)

|x − x′|2

2

Therefore, the tightness easily follows.

14

= O(1)

(37)

(cid:3)

3 Speciﬁcation Test

Given the kernel and convolution c.d.f. estimates in (5) and (6), we deﬁne:

VT,b(u) := √n(cid:16) ˆFk(u) − ˆFc(u)(cid:17)

(38)

where ˆFk(u) and ˆFc(u) are the kernel estimate in (5) and the convolution estimate in (6), respec-

tively. We propose the following statistic:

Theorem 2.

ZU

V 2

T,b(u)π(u)du ⇒ ZU

Z 2(u)π(u)du

where RU π(u)du = 1 and Z(·) is a Gaussian process with covariance kernel σ2

v(u, u′).

proof) (i) Note that

VT,b(u) = √n(cid:16) ˆFk(u) − FX (u)(cid:17) − √n(cid:16) ˆFc(u) − FX (u)(cid:17)

By the Liapunov C.L.T., for any ﬁxed u ∈ U ,

√n(cid:16) ˆFk(u) − FX (u)(cid:17) ⇒ N(cid:0)0, σ2

k(u)(cid:1)

where σ2

k(u) = FX (u) (1 − FX (u)). Moreover, by Theorem 1A,

√n(cid:16) ˆFc(u) − FX (u)(cid:17) ⇒ N(cid:0)0, σ2

2(u)(cid:1)

(39)

(40)

(41)

where σ2(u) = kP∞i=1 P0 [Fǫ (u − mθ(Xi−1)) + Fg (u − ǫi) + c0R (Xi−1, θ, ǫi)]k. Hence, for any ﬁxed
u ∈ U ,

VT,b(u) ⇒ N(cid:0)0, σ2

v(u, u)(cid:1)

(42)

where σ2

v(u, u) := σ2

k(u) + σ2

2(u) − 2 C(u, u). Here the covariance kernel C(u, u′) is given by the

limit of:

1
n2b

−

n

n

n

Xi=1

Xj=1

Xk=1

Cov(cid:18)G(cid:18) u − Xi

b (cid:19) , Z G(cid:18) u′ − t − Xj + mˆθ(Xj−1)

b

(cid:19) K(cid:18) u′ − mˆθ(Xk−1)

b

(cid:19) dt(cid:19)

15

as n → ∞. The Cramer-Wold device ensures:



 ⇒ N









VT,b(u′)

VT,b(u)

0

0

, 


σ2
v(u, u)

σ2
v(u, u′)

v(u, u′) σ2
σ2

v(u′, u′)







(43)

(ii) It leaves us to prove the tightness condition. Note that:

VT,b(u) − VT,b(u′)

= √n(cid:16) ˆFk(u) − FX(u)(cid:17) − √n(cid:16) ˆFk(u′) − FX(u′)(cid:17) + √n(cid:16) ˆFc(u′) − FX (u′)(cid:17) − √n(cid:16) ˆFc(u) − FX (u)(cid:17)

Hence

E|VT,b(u) − VT,b(u′)|2

|u − u′|2

≤

Note that

2

√n(cid:16) ˆFk(u) − FX (u)(cid:17) − √n(cid:16) ˆFk(u′) − FX (u′)(cid:17)(cid:12)(cid:12)(cid:12)
E(cid:12)(cid:12)(cid:12)
√n(cid:16) ˆFc(u′) − FX (u′)(cid:17) − √n(cid:16) ˆFc(u) − FX (u)(cid:17)(cid:12)(cid:12)(cid:12)
E(cid:12)(cid:12)(cid:12)

|u − u′|2

|u − u′|2

+

2

(44)

√n(cid:16) ˆFk(u) − FX (u)(cid:17) − √n(cid:16) ˆFk(u′) − FX (u′)(cid:17)(cid:12)(cid:12)(cid:12)
E(cid:12)(cid:12)(cid:12)

|u − u′|2

2

= O(1)

(45)

Moreover, by (37),

√n(cid:16) ˆFc(u) − FX (u)(cid:17) − √n(cid:16) ˆFc(u′) − FX (u′)(cid:17)(cid:12)(cid:12)(cid:12)
E(cid:12)(cid:12)(cid:12)

|u − u′|2

2

= O(1)

(46)

By applying (45) and (46) to (44),

E|VT,b(u) − VT,b(u′)|2

|u − u′|2

= O(1)

Hence the tightness easily follows.

16

(47)

(cid:3)

Remark 1. Similarly, we can formulate a test statistic based on density estimates:

vT,b(u) := √nb(cid:16) ˆfk(u) − ˆfc(u)(cid:17)

(48)

where ˆfk(u) is a kernel density estimate and ˆfc(u) is the convolution density estimate from Kim et

al (2015). From the root-n convergence of the convolution density estimate,

vT,b(u) ⇒ N(cid:18)0, f (x)ZR

K 2(u)du(cid:19)

where f (x) is the true density. Hence one can propose the following similar test statistics:

ZU

v2

T,b(u)π(u)du ⇒ ZU

Z 2(u)π(u)du

where Z(u) is a Gaussian process. Note, however, that the convergence rate in (38) is faster than

that from (48) given the order of the bandwidth b. Thus, our test based on (38) is expected to

perform better than the test based on (48).

4 Simulation study

4.1 Setup

In this section we consider testing for model speciﬁcation described in Section 2. Consider hypoth-

esis testing

H0 : Xi = θ0|Xi−1| + ǫi,

Ha : Xi = θ0|Xi−1| + ǫiqθ2

0 + θ2

1X 2

i−1

(49)

where ǫ ∼ N (0, 1). We generate random sample of {Xi : i = 1, 2, ..., T}. When we generate the
sample, we set θ0 = 0.3, θ1 = 0.9 and T = 200, 400, and 600. We demonstrate that our proposed test

outperforms the benchmark test-see Kim et al (2015). To that end, we report empirical levels and

powers and compare the ﬁndings with those of the benchmark test. The benchmark test, however,

did not report powers, and hence, we compute them by monte carlo simulation as described therein.

For our proposed test we employ block-wise bootstrap method proposed by K¨unsch (1989) and

17

Liu and Singh (1992). We ﬁrst determine lB, a size of the block so that the number of blocks,

nB, is T /lB. Naik-Nimbalakar and Rajarshi (1994) showed that weak convergence of block-wise

bootstrapped empirical process depends on the order of the lB. They obtained desired results when

lB = O(n1/2− ǫ), with 0 < ǫ < 1
2 . Motivated by their work, lB = {10, 15, 20, 25} are tried; we found
that the proposed test displays the optimal result when lB = 10 for all T . Once we determine the

value of lB, we construct a block: we draw any uniform random number between 1 and T − lB + 1,
say k, and choose lB consecutive observations, Xk+1, ..., Xk+lB . We repeat constructing a block

nB times, combine these nB blocks all together, and obtain resampled observations, X∗1 , ..., X∗T .

Recall VT,b(u) in (38). Let V 2

T,b denote the integral of V 2

T,b(u) as in (39). For the calculation of
the statistics, we use uniform kernel function: K(u) := 2−1I(|u| ≤ 1) where I(·) is an indicator
function. Therefore,

G(u) =Z u

−∞

K(x)dx =

0,

u < −1;

u+1

2 , −1 ≤ u < 1;
1,

u ≥ 1.




For ˆθ, we use least squares estimator. Deﬁne h(t) := (−t2 + 2cit)/8b where ci := x + b − ˆǫi. Then
ˆFc in (6) can be rewritten as

n

b

Xj=1Z G(cid:18) x − t − ˆǫi
Xj=1

IFij(x),

n

ˆFc(x) =

=

1
n2b

1
n2b

n

n

Xi=1
Xi=1

0,

h(x − ˆǫi + b) − h(mˆθ (Xj−1) − b),
bG(cid:16) x−m ˆθ(Xj−1)−ˆǫi−b

(cid:17) + h(mˆθ (Xj−1) + b)

b

−h(x − ˆǫi − b),

b,

18

(cid:19) K(cid:18) x − mˆθ (Xj−1)

b

(cid:19) dt

x < mˆθ (Xj−1) + ˆǫi − 2b;
mˆθ (Xj−1) + ˆǫi − 2b ≤ x < mˆθ (Xj−1) + ˆǫi;

mˆθ (Xj−1) + ˆǫi ≤ x < mˆθ (Xj−1) + ˆǫi + 2b;
otherwise.

where

IFij(x) =




Consequently, the great deal of simpliﬁcation of VT,b(u) in (38) follows directly.

Deﬁne the bootstrap test statistic

V 2∗T,b =ZU

(V ∗T,b(u) − VT,b(u))2π(u) du

(50)

where V ∗T,b(u) denotes the counterpart of VT,b(u) which is obtained from resampled observations.
We repeat block-wise bootstrap BIter times, obtain V 2∗T,b’s, and calculate 100(1 − α) percentiles,
q∗1−α. As various lB’s are tried, so are BIter’s. Our ﬁndings show that empirical levels approaches
more closely to suggested signiﬁcance level α as BIter increases. See, e.g., Table 1. After q∗1−α is
obtained, we reject H0 if V 2
T,b > q∗1−α. As a ﬁnal step, we repeat this procedure 1000 times, count

the number of rejections, and obtain empirical levels and powers by dividing it by 1000.

4.2 Selection of BIter, b, and, lB

In the simulation study, b = {0.05, 0.1, 0.15, 0.2} are tried for bandwidth. Since the choice of b does
not aﬀect the powers and levels much, we only report the result corresponding to b = 0.1. Table 1

reports empirical levels corresponding to various sizes of block and numbers of bootstrap iteration.

As shown in the table, we obtain the optimal result at (lB, BIter) = (10, 200). Table 2 compares

the proposed test with the benchmark test. It is hard to tell which test is superior in terms of the

level. However, there is no room for argument in terms of the power: the proposed test dominates

the benchmark test. When T = 200, the diﬀerences of the powers between two tests are more

than 0.3 for all α′s. When T increase, the diﬀerences decrease: approximately 0.15 (0.1) for all α’s

when T is 400 (600). However, benchmark test does not obtain the power of 0.9 for most of all

α’s even though T reaches 600; when α = 0.01, power is still smaller than 0.8. On the contrary,

our proposed test accomplishes the power more than 0.9 except a few cases: α = 0.05, 0.025, 0.01

with T = 200 and α = 0.01 with T = 400. Therefore, we conclude that the proposed test is much

superior to the benchmark test.

19

lB = 8

lB = 10

α

0.1

BIter = 40

80

120

160

200

BIter = 40

80

120

160

200

0.149

0.124

0.119

0.114

0.109

0.125

0.108

0.104

0.107

0.101

0.075

0.107

0.098

0.086

0.084

0.081

0.102

0.088

0.084

0.078

0.078

0.05

0.079

0.072

0.063

0.059

0.055

0.074

0.064

0.053

0.047

0.045

0.025

0.056

0.046

0.030

0.033

0.027

0.044

0.035

0.028

0.028

0.026

0.01

0.050

0.025

0.016

0.014

0.016

0.029

0.013

0.012

0.014

0.011

lB = 16

lB = 20

α

0.1

BIter = 40

80

120

160

200

BIter = 40

80

120

160

200

0.128

0.119

0.127

0.124

0.120

0.158

0.142

0.134

0.130

0.127

0.075

0.091

0.087

0.085

0.086

0.089

0.131

0.107

0.104

0.103

0.099

0.05

0.064

0.057

0.055

0.054

0.052

0.097

0.076

0.075

0.067

0.065

0.025

0.041

0.032

0.029

0.025

0.026

0.065

0.051

0.039

0.035

0.031

0.01

0.025

0.015

0.015

0.012

0.013

0.058

0.029

0.020

0.019

0.016

Table 1: Levels when BIter and lB vary with T being ﬁxed at 400.

20

α

T=200

T=400

T=600

V 2
T,b

V sup
T,b

V 2
T,b

V sup
T,b

V 2
T,b

V sup
T,b

0.1

0.102

0.097

0.099

0.104

0.115

0.098

0.075

0.073

0.074

0.078

0.077

0.078

0.073

Level

0.05

0.041

0.052

0.045

0.050

0.058

0.054

0.025

0.024

0.022

0.026

0.028

0.024

0.029

0.01

0.013

0.007

0.013

0.010

0.012

0.007

0.1

0.940

0.604

0.974

0.822

0.984

0.900

0.075

0.909

0.567

0.966

0.804

0.982

0.884

Power

0.05

0.860

0.519

0.951

0.760

0.976

0.870

0.025

0.783

0.434

0.911

0.712

0.952

0.836

0.01

0.687

0.332

0.860

0.636

0.930

0.785

Table 2: Proposed test vs the benchmark test when T = 200, 400, and 600

21

References

[1] A¨ıt-Sahalia, Y. (1996). Testing continuous-time models of the spot interest rate. Review of

Financial Studies 9, 385–426.

[2] Corradi, V. and Swanson, N.R. (2005). Bootstrap speciﬁcation tests for diﬀusion processes.

Journal of Econometrics 124, 117–148.

[3] Kim, K.H. and Wu, W.B. (2007). Density estimation for nonlinear time series. manuscript.

[4] Kim, K.H., Zhang, T. and Wu, W.B. (2015). Parametric speciﬁcation test for nonlinear au-

toregressive models. Econometric Theory 31, 1078–1101.

[5] K¨unsch, H. R. (1989). The jackknife and the bootsrap for general stationary observations.

Ann. Statist. 17, 1217-1241.

[6] Liu, R. Y. and Singh, K. (1992). Moving blocks jackknife and bootstrap capture weak depen-

dence. In Exploring the Limits of Bootstrap (R. Lepage and L. Billard, eds.) 225-248. Wiley,

New York.

[7] Pritsker, M. (1998). Nonparametric density estimation and tests of continuous time interest

rate models. Review of Financial Studies 11, 449–487.

[8] Naik-Nimbalkar, U. V. and Rajarshi, M. B. (1994). Validity of blockwise bootstrap for empir-

ical processes with stationary observations. Ann. Statist. 22, 980-994.

[9] Wu, W.B. (2005). Nonlinear System Theory: Another Look at Dependence. Proceedings of the

National Academy of Sciences USA. 102, 14150–14154.

[10] Wu, W.B. and Shao, X.F. (2004). Limit Theorems for Iterated Random Functions. Journal of

Applied Probability 41, 425–436.

22

