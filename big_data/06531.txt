Deep video gesture recognition using illumination invariants

Otkrist Gupta

Dan Raviv

Massachusetts Institute of Technology

Ramesh Raskar

6
1
0
2

 
r
a

 

M
1
2

 
 
]

V
C
.
s
c
[
 
 

1
v
1
3
5
6
0

.

3
0
6
1
:
v
i
X
r
a

Figure 1: Automated facial gesture recognition is a fundamental problem in human computer interaction. While tackling real world tasks of
expression recognition sudden changes in illumination from multiple sources can be expected. We show how to build a robust system to detect
human emotions while showing invariance to illumination.

Abstract

In this paper we present architectures based on deep neural nets
for gesture recognition in videos, which are invariant to local scal-
ing. We amalgamate autoencoder and predictor architectures using
an adaptive weighting scheme coping with a reduced size labeled
dataset, while enriching our models from enormous unlabeled sets.
We further improve robustness to lighting conditions by introduc-
ing a new adaptive ﬁler based on temporal local scale normaliza-
tion. We provide superior results over known methods, including
recent reported approaches based on neural nets.

Keywords:
tion, neural nets, machine learning.

deep learning, gesture recognition, video classiﬁca-

1 Introduction

Human beings, as social animals, rely on a vast array of methods to
communicate with each other in the society. Non-verbal commu-
nication, that includes body language and gestures, is an essential
aspect of interpersonal communication. In fact, studies have shown
that non-verbal communication accounts for more than half of all
societal interactions [Frith 2009]. Studying facial gestures is there-
fore of vital importance in ﬁelds like sociology, psychology and
automated recognition of gestures can be applied towards creating
more user affable software and user agents in these ﬁelds.

Automatic gesture recognition has wide implications in the ﬁeld of
human computer interaction. As technology progresses, we spend
large amounts of our time looking at screens, interacting with com-
puters and mobile phones. In spite of their wide usage, majority of
software interfaces are still non-verbal, impersonal, primitive and
terse. Adding emotion recognition and tailoring responses towards
users emotional state can help improve human computer interaction
drastically [Cowie et al. 2001; Zhang et al. 2015] and help keep
users engaged. Such technologies can then be applied towards im-
provement of workplace productivity, education and telemedicine
[Kołakowska et al. 2014]. Last two decades have seen some in-
novation in this area [Klein and Picard 1999; Cerezo et al. 2007;

Andr´e et al. 2000] such as humanoid robots for example Pepper
which can both understand and mimic human emotions.

Modeling and parameterizing human faces is one of the most fun-
damental problems in computer graphics [Liu et al. 2014a]. Under-
standing and classiﬁcation of gestures from videos can have appli-
cations towards better modeling of human faces in computer graph-
ics and human computer interaction. Accurate characterization of
face geometry and muscle motion can be used for both expres-
sion identiﬁcation and synthesis [Pighin et al. 2006; Wang et al.
2013] with applications towards computer animation [Cassell et al.
1994]. Such approaches combine very high dimensional facial fea-
tures from facial topology and compress them to lower dimensions
using a series of parameters or transformations [Waters 1987; Pyun
et al. 2003]. This paper demonstrates how to use deep neural net-
works to reduce dimensionality of high information facial videos
and recover the embedded temporal and spatial information by uti-
lizing a series of stacked autoencoders.

Over the past decade algorithms for training neural nets have dra-
matically evolved, allowing us to efﬁciently train deep neural nets
[Hinton et al. 2006; Jung et al. 2015]. Such models have become
a strong driving force in modern computer vision and excel at ob-
ject classiﬁcation [Krizhevsky et al. 2012], segmentation and facial
recognition [Taigman et al. 2014].
In this paper we apply deep
neural nets for recognizing and classifying facial gestures, while
pushing forward several architectures. We obtain high level infor-
mation in both space and time by implementing 4D convolutional
layers and training an autoencoder on videos. Most of neural net
applications use still images as input and rely on convolutional ar-
chitectures for automatically learning semantic information in spa-
tial domain. Second, we reface an old challenge in learning the-
ory, where not all datasets are labeled. Known as semi-supervised
learning, this problem, once again, attracts attention as deep nets
require massive datasets to outperform other architectures. Finally,
we provide details of a new normalization layer, which robustly
handles temporal lighting changes within the network itself. This
new architecture is adaptively ﬁne tuned as part of the learning pro-
cess, and outperforms all other reported techniques for the tested
datasets. We summarize our contributions as follows:

Figure 2: Schematic representation of deep neural networks for supervised and unsupervised learning. We use pink boxes to denote con-
volutional layers, yellow boxes denote rectiﬁed linear unit layers and green boxes indicate normalization layers. Our technique combines
unsupervised learning approaches (a) with labeled prediction (b) to predict gestures using massive amounts of unlabeled data and few labeled
samples. Autoencoder (a) is used to initialize weights and then predictor (b) is ﬁne tuned to predict labels.

1.1 Contributions

1. We develop a scale invariant architecture for generating illu-

mination invariant deep motion features.

2. We report state of the art results for video gesture recognition

using spatio-temporal convolutional neural networks.

3. We introduce an improved topology and protocol for semi-
supervised learning, where the number of labeled data points
is only a fraction of the entire dataset.

2 Related Work

Machine learning strategies such as random forests or SVMs
combined with local binary features (or sometimes facial ﬁducial
points) have been used for facial expression recognition in the past
[Kotsia and Pitas 2007; Michel and El Kaliouby 2003; Shan et al.
2005; Dhall et al. 2011; Walecki et al. 2015; Presti and Cascia
2015; Vieriu et al. 2015]. Other intriguing methodologies include
performing emotion recognition through speech [Nwe et al. 2003;
Schuller et al. 2004], using temporal features and manifold learning
[Liu et al. 2014b; Wang et al. 2013; Kahou et al. 2015; Chen et al.
2015] and combining multiple kernel based approaches [Liu et al.
2014c; Senechal et al. 2015]. Majority of such systems involve
a pipeline with multiple stages - face discovery, face alignment,
feature extraction and landmark localization followed by classiﬁca-
tion of labels as the ﬁnal step. Our approach combines all of these
phases (after face detection) into the neural net which takes entire
video clip as input.

Recently, deep neural nets have triumphed over traditional vision
algorithms, thereby dominating the world of computer vision. Deep
neural networks have proven to be an effective tool to classify and
segment high dimensional data such as images [Krizhevsky et al.
2012; Szegedy et al. 2015], audio and videos [Karpathy et al. 2014;
Tran et al. 2014]. With advances in convolutional neural nets, we
have seen neural nets applied for face detection [Taigman et al.
2014; Zhao et al. 2015] and expression recognition [Abidin and
Harjoko 2012; Gargesha and Kuchi 2002; He et al. 2015] but these

networks were not deep enough or used other feature extraction
techniques like PCA or Fisherface. By contrast this paper proposes
an end to end system which takes a sequence of frames as input and
gives classiﬁcation labels as output while using deep autoencoders
to generate high dimensional spatio-temporal features.
While deep neural nets are notorious for stellar results, training a
neural net can be challenging because of huge data requirements.
A way around this is to use autoencoders for feature extraction or
weights initialization [Vincent et al. 2008], followed by ﬁne tuning
over a smaller labeled dataset. This issue can also be solved using
embeddings in lower dimensional manifold [Weston et al. 2012;
Kingma et al. 2014] or pre-train using pseudo labels [Lee 2013]
thereby requiring fewer number of labeled samples. Approaches
based on semi supervised learning have shown to work for smaller
labeled datasets [Papandreou et al. 2015] and techniques using deep
neural nets to combine labels and unlabeled data in the same archi-
tecture [Liu et al. 2014d; Kahou et al. 2013] have emerged victori-
ous. In this paper we propose similar hybrid approaches incorporat-
ing deep autoencoders for unlabeled data and additive loss function
for the classiﬁcation tasks.
Introducing invariants in neural networks is an area of active re-
search, some examples include illumination invariant face recogni-
tion techniques [Mathur et al. 2008; Li et al. 2004] and deep lamber-
tian networks [Tang et al. 2012; Jung et al. 2015]. Our method tries
to introduce similar invariants for video neural networks by intro-
ducing temporal invariants to illumination. While we test our tech-
niques on facial gesture datasets, in principal they can be extended
to any neural network taking videos as input. In [Anonymous Sub-
mission 2016], the author considered velocity changes in videos as
well as a semi-supervised learning approach. Here we focus on
a different neural network topology and parameter calibration, and
report better results on similar databases using new invariant layers.

3 Method

Our facial expression recognition pipeline comprises of Viola-Jones
algorithm [Viola and Jones 2004] for face detection followed by
a deep convolutional neural network for predicting expressions.

Figure 3: We learn 7 different facial emotions from short (about 1 sec, 25 frames) video clips. The Illumination invariant aspect is achieved
by adding an illumination invariant neural network to induce illumination invariance on original input. Our prediction system is based on
slow temporal fusion neural network, trained by hybridization of autoencoding a huge collected dataset and a loss prediction on a small set
of labeled gestures. This ﬁgure shows the complete architecture combining both illumination invariant and predictor components.

(a)

(b)

(c)

(d)

Figure 4: Results from autoencoder reconstruction while using
scale invariant autoencoder. (a), (b) Input video sequence. (c), (d)
Output video sequence from illumination invariant neural network.

The deep convolutional network includes an autoencoder com-
bined with a predictor which relies on the semi-supervised learning
paradigm. The autoencoder neural network takes videos containing
9 frames of size 145 × 145 as input and produces 145 × 145 × 9
tensor as output. Predictor neural net sources innermost hidden
layer of autoencoder and uses a cascade of fully connected layers
accompanied by the softmax layer to classify expressions. Since
videos can have different sizes and durations they need to be re-
sized in temporal and spatial domain using standard interpolation
techniques. The network topologies and implementation are de-
scribe henceforth.

3.1 Autoencoder

Stacked autoencoders can be used to convert high dimensional data
into lower dimensional space which can be useful for classiﬁca-
tion, visualization or retrieval [Hinton and Salakhutdinov 2006].
Since video data is extremely high dimensional we rely on a deep
convolutional autoencoder to extract meaningful features from this
data by embedding it into R4096. The autoencoder topology is in-
spired by ImageNet [Krizhevsky et al. 2012] and comprises of con-
volutional layers gradually reducing data dimensionality until we
reach a fully connected layer. Central fully connected layers are
followed by a cascade of deconvolutional layers which essentially
invert the convolutional layers thereby reconstructing the input ten-
sor (R145×145×9). The complete autoencoder architecture can be
described in following shorthand C(96, 11, 3)−N−C(256, 5, 2)−
N−C(384, 3, 2)−N−F C(4096)−F C(4096)−DC(96, 11, 3)−
N − DC(256, 5, 2) − N − DC(384, 3, 2). Here C(96, 11, 3) is a

convolutional layer containing 96 ﬁlters of size 11 × 11 in spatial
domain and spanning 3 frames in temporal domain. N stands for
local response normalization layers, DC stands for deconvolutional
layers and F C(4096) stands for fully connected layers containing
4096 neurons.

In the same way that spatial convolutions consolidate nearby spa-
tial characteristics of an image, we use the slow fusion model de-
scribed in [Karpathy et al. 2014] to gradually combine temporal
features across multiple frames. We implement slow fusion by ex-
tending spatial convolution to the temporal domain and adding rep-
resentation of ﬁlter stride for both space and time domains. This
allows us to control ﬁlter size and stride in both temporal and spa-
tial domains leading to a generalized 3D convolution over spatio-
temporal input tensor followed by 4D convolutions on intermediate
layers. The ﬁrst convolutional layer sets temporal size and stride as
3 and 2 respectively whereas the subsequent layer has both size and
stride of 2 in temporal domain. Finally the third convolutional layer
merges temporal information from all frames together, culminating
in a lower dimensional vector of size 4096 at the innermost layer.

Since weight initialization is critical for convergence in a deep au-
toencoder, we use pre-training for each convolutional layer as we
add the layers on. Instead of initializing all weights at once and
training from there, we train the ﬁrst and last layer ﬁrst, followed
by the next convolutional layer and so on. We discuss this in detail
in section 5.1.

3.2 Semi-Supervised Learner

Our predictor neural net consists of a combination of several con-
volutional layers followed by multiple fully connected layers end-
ing in a softmax logistic regression layer for prediction. Archi-
tecture can be described as C(96, 11, 3) − N − C(256, 5, 2) −
N − C(384, 3, 2)− N − F C(4096)− F C(8192)− F C(4096)−
F C(1000) − F C(500) − F C(8) using shorthand notation de-
scribed in section 3.1. Notice that our autoencoder architecture is
overlaid on top of the predictor architecture by adding deconvolu-
tional layers after the ﬁrst fully connected layer to create a semi-
supervised topology which is capable of training both autoencoder
and predictor together (see Figure 3). We use autoencoder to ini-
tialize weights for all convolutional layers, all deconvolutional lay-
ers and central fully connected layers and we initialize any remain-
ing layers randomly. We use stochastic gradient descent to train
weights by combining losses from both predictor and autoencoder
while training, this combined loss function for the semi-supervised

Figure 5: Results from reconstruction using temporal convolutional autoencoder on a face video. (a) Input video sequence. (b) Reconstruc-
tion after using 4 convolutional layers. (c) Reconstruction after using 8 layers. (d) Reconstruction after using 12 layers.

learner is described in the equation 1.

L = −β

yjlog

(cid:88)

j

(cid:18) eoj(cid:80)

k eok

(cid:19)

+ α||¯x − ¯xo||2

(1)

puts (x1, x2) and multiplies the output of exponential layer (x2)
with the original input tensor (x1). If ¯F (¯x) denotes function emu-
lated by ﬁrst convolution layer, we can write the transfer function
of this sub-net as follows (equation 2).

(cid:17)

(cid:16) eoj(cid:80)

||¯x− ¯xo||2 whereas −(cid:80)

Equation 1 deﬁnes semi-supervised learner loss by combining the
loss terms from predictor and autoencoder neural networks. Here
yj refers to the input labels to represent each facial expression
uniquely while ok are the outputs from the ﬁnal layer of predic-
tor neural net. Also ¯x is the input tensor (∈ R145×145×9) and ¯xo
is the corresponding output from autoencoder. Autoencoder loss
is the Euclidean loss between input and output tensors given by
is the softmax loss from
the predictor [Bengio et al. 2005]. Each step of stochastic gradient
descent is performed over a batch of 22 inputs and loss is obtained
by adding loss terms for the entire batch. At the commencement of
training of the predictor layers, we select values of β which make
softmax loss term an order of magnitude higher than the Euclidean
loss term (see equation 1). We continue training predictor layers by
gradually decreasing loss coefﬁcient α alongside of softmax loss to
prevent overﬁtting of autoencoder. Amalgamation of predictor and
autoencoder architectures is depicted in Figure 2.

j yjlog

k eok

3.3 Illumination Invariant Learner

We introduce scale invariance to pixel intensities by adding addi-
tional layers as an illumination invariant neural network in the be-
ginning of semi-supervised learner. The illumination invariant lay-
ers include a convolutional layer, an absolute value layer, a recipro-
cal layer followed by a Hadamard product layer. Scale invariance is
achieved by applying element wise multiplication between the out-
put layers of proposed architecture and the original input layer. This
normalization can be written as C(9, 1, 9) − Abs − Log(α, β) −
Exp(−γ, δ)− P rod(x1, x2) (please refer to shorthand notation in
section 3.1). Here C(9, 1, 9) refers to the ﬁrst convolutional layer
containing 9 ﬁlters with size 1 × 1 in spatial domain and a size of
9 in time domain. Abs is a ﬁxed layer to compute absolute value,
Log(α, β) layer computes the function ln(α∗x+β) and Exp(γ, δ)
layer gives us eγ∗x+δ. In the end P rod(x1, x2) layer takes two in-

H(¯x) = ¯xe

−γlog(α| ¯F (¯x)|+β)+δ =

eδ ¯x

(α| ¯F (¯x)| + β)γ

(2)

Log and Exp layers are used to generate a reciprocal layer by set-
ting meta-parameters γ to 1 and δ to zero. We can also ”switch
off” this sub-net by setting both of these parameters to zero. Trans-
fer function meta parameters α (scale) and β (shift) can be tuned
as well for optimal performance. We perform a grid search to ﬁnd
optimal values for these after re-characterizing the transfer function
parameters as a global multiplicative factor τ and a proportion fac-
tor η (see equation 3). Table 1 shows results for various choices of
α and β. We can reformulate equation 2 as given below:

H(¯x) =

e0 ¯x

(α| ¯F (¯x)| + β)1

=

1
β ¯x
β | ¯F (¯x)| =

1 + α

τ ¯x

1 + η| ¯F (¯x)|

(3)

The output from scale invariant neural net is a 145 × 145 × 9 ten-
sor which is used as input in the autoencoder and predictor neu-
ral networks. The convolution layer can be parametrized using a
9 × 1 × 1 × 9 tensor and changes during ﬁne tuning while α and
β are ﬁxed constants greater than zero. In our experiments we ini-
tialized convolutional ﬁlter of scale invariant sub-net using several
approaches, such as partial derivatives, mellin transform, moving
average and laplacian kernel and found that it performed best when
using neighborhood averaging. Algorithm 1 demonstrates initial-
ization of convolutional layer at the beginning of illumination in-
variant neural net.

4 Datasets and Implementation

Surprisingly, high quality facial expression datasets are hard to
come across. In the same way that majority of facial expression
algorithms focus on still images, majority of facial gesture datasets

η →
τ ↓
0.2

0.5

1

5

0.1

0.5
5
0.2
2
0.1
1

0.02
0.2

scale (η/τ)
shift (1/τ)
scale (η/τ)
shift (1/τ)
scale (η/τ)
shift (1/τ)
scale (η/τ)
shift (1/τ)

0.486

0.50

0.499

-*

scale (η/τ)
shift (1/τ)
scale (η/τ)
shift (1/τ)
scale (η/τ)
shift (1/τ)
scale (η/τ)
shift (1/τ)

1

5
5
2
2
1
1
0.2
0.2

0.472

0.5135

0.51

0.44

10

50
5
20
2
10
1
2
0.2

scale (η/τ)
shift (1/τ)
scale (η/τ)
shift (1/τ)
scale (η/τ)
shift (1/τ)
scale (η/τ)
shift (1/τ)

0.243*

0.45*

0.47

0.50

Table 1: Accuracies on Florentine (ﬁrst presented in [Anonymous Submission 2016]) dataset for various values of scale(α) and shift(β)
for illumination invariant neural net. We do a grid search for τ varying from 0.5 to 5 and η varying from 0.1 to 10. Yello columns show
corresponding values scale and shift and blue columns show test accuracies. Cells marked with asterisk(*) indicate conﬁgurations that did
not converge during training.

r ← (wSize − 1)/2
A ← zeros(nF rames, nF rames)
for (i ← 0; i < nF rames; i + +) do

Algorithm 1 Generate convolution layer for scale invariant sub-net
Input: Total number of frames nF rames, window size wSize
Output: Caffe Weight Matrix W
1: function AUTOENCODERWEIGHTS(nFrames, wSize)
2:
3:
4:
5:
6:
7:
8: W ← A
9: return W

n ← min(i, r)
n ← min(n, nF rames − i)
Ai,i−n:i+n ← 1/(2n + 1)

rely on images alone and don’t emphasize on complete video clips.
For accurate analysis we compare our method against external tech-
niques using 3 different datasets. Each of these datasets have fa-
cial video clips varying from neutral face to its peak facial expres-
sion. Facial expressions can be naturally occurring (non-posed) or
artiﬁcially enacted (posed), we attempt to classify both using our
method and compare our results against published techniques. Here
we present the two known datasets from literature along with two
additional datasets collected by us. The ﬁrst dataset was used for
unsupervised learning and contains 160 million face images com-
bined into 6.5 million short (25 frames) clips. The second dataset
contains 2777 video clips which are labeled for seven basic emo-
tions.

4.1 Autoencoder dataset

Training the unsupervised component of our neural net required a
large amount of data to ensure that the deep features were general
enough to represent any face expression. We trained the deep con-
volutional autoencoder using a massive collection of unlabeled data
points comprising of 6.5 million video clips with 25 image frames
per clip. The clips were generated by running Viola-Jones face al-
gorithm to detect and isolate face bounding boxes on public domain
videos. We further enhanced the data quality by removing any clips
which showed high variation in-between consecutive frames. This
eliminated video clips containing accidental appearance of occlu-
sions, rapid facial motions or sudden appearance of another face.

Emotion
Anger
Sadness
Contempt
Fear
Surprise
Joy
Disgust
Total

Posed
132
118
153
137
188
172
132
1032

Non-Posed

Cumulative

318
148
301
96
232
503
147
1745

450
266
454
233
420
675
279
2777

Table 2: Data distribution for Florentine dataset for various emo-
tions. Posed clips refer to the artiﬁcially generated clips, while
non-posed refer to those captured using the stimulus activation pro-
cedure.

[Asthana et al. 2014]. We ﬁtted the facial landmarks to a 3D de-
formable model and restricted our dataset to clips containing less
than 30 degrees of yaw, pitch or roll, thereby eliminating faces
looking sideways. For data generation, we relied on daily feeds
from news sources such as CNN, MSNBC, FOX and CSPAN. Col-
lection of this dataset required development of an automated system
to mine video clips, segment faces and ﬁlter meaningful data and it
took us more than 6 months to collect the entire dataset. To our
knowledge this is the largest dataset containing facial video clips
and we plan to share it with scientiﬁc community by making it pub-
lic.

4.2 Cohn Kanade Dataset

The Cohn Kanade dataset [Lucey et al. 2010] is one of the oldest
and well known dataset containing facial expression video clips. It
contains a total of 593 video clip sequences from which 327 clips
are labeled for seven basic emotions (most of these are posed).
Clips contain the frontal view of face performing facial gesture
varying from neutral expression to maximum intensity of emotion.
While the dataset contains a lot of natural smile expressions it lacks
diversity of induced samples for other facial expressions.

4.3 MMI Dataset

As an additional step we obtained the facial pose information by
using active appearance models and generating facial landmarks

MMI facial expression dataset [Pantic et al. 2005] involves an ongo-
ing effort for representing both enacted and induced facial expres-

Confusion matrix using our methods on Florentine Dataset

Confusion matrix using external methods on Florentine dataset

Anger
Contempt
Happy
Disgust
Fear
Sadness
Surprise

Anger
Contempt
Happy
Disgust
Fear
Sadness
Surprise

Anger Contempt Happy Disgust
0.54
0.07
0.01
0.13
0.04
0.18
0.04

0.01
0.14
0.78
0.15
0.08
0.07
0.07

0.12
0.06
0.04
0.31
0.08
0.10
0.05

0.13
0.47
0.15
0.21
0.14
0.22
0.07

Fear
0.03
0.04
0.00
0.07
0.28
0.02
0.21

Sadness
0.12
0.14
0.00
0.05
0.04
0.27
0.05

Confusion matrix using our methods on Cohn-Kanade
Anger Contempt Happy Disgust
Sadness
0.92

0
0

0.06

0

0.38

0

0.08
0.80

0
0

0
0
0
0
0

1.00

0

0.14

0
0

0
0
0

0.94

0
0
0

Fear
0
0
0
0

0.71

0
0

0
0
0
0
0

0.50

0

Surprise

0.04
0.07
0.00
0.08
0.32
0.13
0.51

Surprise

0

0.20

0
0

0.14
0.12
1.00

Anger
Contempt
Happy
Disgust
Fear
Sadness
Surprise

Anger
Contempt
Happy
Disgust
Fear
Sadness
Surprise

Anger Contempt Happy Disgust
0.61
0.09
0.01
0.19
0.21
0.26
0.15

0.07
0.06
0.01
0.22
0.07
0.02
0.01

0.08
0.44
0.07
0.14
0.09
0.20
0.03

0.06
0.27
0.85
0.25
0.06
0.19
0.09

Fear
0.04
0.02

0

0.01
0.12
0.05
0.06

Confusion matrix using external methods on Cohn-Kanade
Sadness

Anger Contempt Happy Disgust
0.73

0.07

0

Sadness

Surprise

0.06
0.06
0.01
0.02
0.06
0.13
0.05

0.20
0.14

0

0.12

0

0.44

0

0.08
0.05
0.06
0.16
0.39
0.16
0.61

Surprise

0
0
0

0.12

0

0.11
0.95

0
0

0.25

0

0.33

0

0.86

0

0.12

0
0
0

0

0.95

0
0
0
0

Fear
0
0

0.05

0

1.00

0
0

0
0
0

0.38

0

0.11
0.05

Table 3: Confusion matrices over test results for Cohn Kanade and Florentine datasets using our methods and best performing external
method which uses Expressionlets for CKPlus [Liu et al. 2014b] and Covariance Riemann kernel for Florentine [Liu et al. 2014c]. On the
left we show results for the proposed illumination invariant semi-supervised approach across various facial expressions, while on the right we
present confusion matrix from external methods. Highest accuracy in each category is marked using blue color. For CKPlus we outperform
competing method in 5 verticals by getting 100% accuracy on happiness, 100% on surprise, 94% on disgust, 92% in anger and 50% in
sadness. For both methods misclassiﬁcation occur when emotions like sadness get recognized as anger and vice-versa.

sions. The dataset comprises of 2894 video samples out of which
around 200 video clips are labeled for six basic emotions. The clips
contain faces going from blank expression to the peak emotion and
then back to neutral facial gesture. MMI which originally contained
only posed facial expressions, was recently extended to include nat-
ural versions of happiness, disgust and surprise [Valstar and Pantic
2010].

4.4

Florentine dataset

We developed specialized video recording and annotation tools to
collect and label facial gestures (ﬁrst presented in [Anonymous
Submission 2016]). The application was developed in Python pro-
gramming language and we used well known libraries such as
OpenCV for video capture and annotation. The database contains
facial clips from 160 subjects (both male and female), where ges-
tures were artiﬁcially generated according to a speciﬁc request, or
genuinely given due to a shown stimulus. We captured 1032 clips
for posed expressions and 1745 clips for induced facial expressions
amounting to a total of 2777 video clips. Genuine facial expres-
sions were induced in subjects using visual stimuli, i.e. videos
selected randomly from a bank of Youtube videos to generate a
speciﬁc emotion. Please refer to Table 2 to see the distribution of
database, where posed clips refers to the artiﬁcially generated ex-
pressions and non-posed refers to the stimulus activation procedure.

5 Experiments and results

5.1 Video autoencoder

Since deep autoencoders can show slow convergence when trained
from randomly initialized weights [Hinton and Salakhutdinov
2006], we used contrastive divergence minimization to train stacked
autoencoder layers iteratively [Carreira-Perpinan and Hinton 2005].
Initially, we pre-trained the beginning and end convolutional layers
by creating an intermediate neural network (C(96, 11, 3) − N −
C(256, 5, 2) − N − DC(256, 5, 2) − N − DC(384, 3, 2)) and
training it on facial video clips. Inner layers were trained succes-
sively by adding them to the intermediate neural network and keep-

ing pre-trained layers ﬁxed until the convergence of weights. To
yield best results, we also ﬁne tuned the entire network at the end
of each iteration. This process was repeated until the required num-
ber of layers had been added and ﬁnal architecture was achieved.
Training of the entire autoencoder typically required 3 days and a
million data inputs.

Our neural network was implemented using the Caffe framework
[Jia et al. 2014] and trained using NVIDIA Tesla K40 GPUs. The
trained weights used to initialize next phase were stored as Caffe
model ﬁles and each intermediate neural network was implemented
as a separate prototxt ﬁle. Weights were shared using shared pa-
rameter feature and transferred across neural networks using the
resume functionality provided in Caffe. Our deep autoencoder took
145× 145× 9 clips as input, the spatial resolution was achieved by
down-sampling all clips to a ﬁxed size using bi-cubic interpolation.
9 frames were obtained by extracting every third frame from video
clips. All videos were converted into 1305 × 145 image clips con-
taining consecutive input frames placed horizontally and we used
the Caffe ”imagedata”, ”split” and ”concat” layers to isolate indi-
vidual frames for autoencoder input and output.

Please see Figure 5 to visualize results obtained from intermediate
autoencoders using different number of layers.

5.2 Semi-Supervised predictor

We created a semi-supervised predictor by adding a deep neural
network after the innermost fully connected layer of our autoen-
coder. The architecture of predictor neural net can be written as
F C(8192) − F C(4096) − F C(1000) − F C(500) − F C(8).
The complete semi-supervised neural network contains an autoen-
coder and a predictor that share neural links and can be trained on
the same input simultaneously. Weights from autoencoder training
were used to initialize weights of semi-supervised predictor which
were later ﬁne tuned using labeled inputs from datasets described
in section 3.2. The weights from this step are used for initialization
of our scale-invariant predictor which we describe next.

Main Gaussian Riemann
Main Grassman
Main Covariance Reimann
Expressionlets
Semi-Supervised Learner
Scale Invariant Learner

MMI
40.9
9.09
40.9
52.91
59.01
65.57

CKPlus

67
17.9
79
82.7
87.36
90.52

Florentine

46.92
44.99
51.05
48.6
51.11
51.35

Main Gaussian Riemann
Main Grassman
Main Covariance Reimann
Expressionlets
Semi-Supervised Learner
Scale Invariant Learner

MMI
39.39
9.09
36.36
55.38
55.7
59.03

CKPlus

65
11
65
70

68.42
73.68

Florentine

43.9
43.91
46.44
46.02
38.66

48

(a) Comparison of results using Illumination Invariant Techniques for
datasets under standard conditions.

(b) Comparison of results using Illumination Invariant Techniques for
datasets under changing lighting.

Table 4: Comparison of results from various techniques on CKPlus, MMI and Florentine datasets. The dataset was divided into 3 parts
test, train and val randomly. Training set was 50%, test and validation were 30% and 20% respectively. The table on the left shows results
on original data while the one on right shows results after we added illumination changes. Our method consistently won for both small and
large datasets (winning method is shown in blue and the leading method is showed using yellow).

5.3 Illumination-Invariant Semi-Supervised predictor

Our scale-invariant neural network preﬁxes semi-supervised learner
with an axillary neural net to induce scale invariance (see 3.3). We
test our method on three datasets (MMI, CK and Florentine ) by
randomly dividing each of them into non-intersecting train, test and
validation subsets. Our training dataset contains 50% inputs while
testing and validation datasets contain 30% and 20% of inputs. Af-
ter the split we increase the size of training dataset by adding rota-
tion, translation or ﬂipping the image.

For quantitative analysis we compare our
results against
expression-lets base approaches [Liu et al. 2014b] and multiple ker-
nel methods [Liu et al. 2014c]. We utilize sources downloaded from
Visual data transforming and taking in Resources [Sources ] as a
reference to contrast with our strategies. For reasonable compari-
son we use same partitioning techniques while comparing our tech-
niques with external methods. While we cannot compare against
methods such as [Liu et al. 2014a] because of absence of publicly
available code our method still wins on MMI dataset.

We test our method with and without varying illumination on exter-
nal datasets, results of our ﬁndings can be summarized in Table 4.
Please see tables 3 for confusion matrices demonstrating results for
each expression. We outperform all external methods on datasets
in almost all cases. Our method also shows large margin of im-
provement over plain semi-supervised approaches. Both autoen-
coder and predictor network topologies are implemented as Caffe
prototxt ﬁles [Jia et al. 2014] and they will be made available for
public usage.

6 Discussions and future work

In this paper we introduce a framework for facial gesture recog-
nition which combines semi-supervised learning approaches with
carefully designed neural network topologies. We demonstrate how
to induce illumination invariance by including specialized layers
and use spatio-temporal convolutions to extract features from mul-
tiple image frames. Currently, our system relies on utilization of
Viola-Jones to distinguish and segment out the faces and is limited
to analyzing only the front facing views. Emotion recognition in the
wild still remains an elusive problem with low reported accuracies
which we hope will be addresses in future work.

In this work we only considered video frames but other, richer,
modalities could be taken into account. Sound, for example, has a
direct inﬂuence on the emotional status and can improve our current
system. Higher refresh rates, multi-resolution in space and time, or
interactions between our subjects are just few of many possibilities

Semi-Supervised Learner
LRN @(0.5)
LRN @(0.75)
Scale Invariant Learner

MMI
55.7
54.09
55.73
59.03

CKPlus
68.42
69.47
69.47
73.68

Florentine

38.66

35.84

35

48

Table 5: Comparison of illumination invariant learner to plain
semi supervised learner with Local Response Normalization layers
in the beginning. We try changing coeffecients of Local Response
Normalization and got good results when setting β at 0.75. Our
method continued to win for both small and large datasets (win-
ning method is shown in blue and the leading method is showed
using yellow).

which can to enrich our data and can lead to better classiﬁcation or
inference.

Deep neural networks have proven to be extremely effective in solv-
ing computer vision problems even though training them at large
scale continues to be both CPU and memory intensive. Our system
tries to make best use of resources available and further improve-
ments in hardware and software can help us build even larger and
deeper neural networks while enabling us to train and test them on
portable devices. Over here, we introduce a new layer which cre-
ates illumination invariance adaptively and can be ﬁne tuned to get
best results. In this work, we emphasize on scale invariance for illu-
mination, in future we hope to explore induction of other invariants,
which continues to be an area of rapid research in neural networks.

Another approach to induce scale invariance can involve using stan-
dardized Local Response Normalization (LRN) based layers in the
neural network right after the ﬁrst input layer. This approach is
similar to pre-normalizing the data before testing. We compare our
method to this approach as well and found that adaptive normaliza-
tion performed better than plain LRN based learner. Our results are
summarized in Table 5.

6.1 Limitations

In this section we explore limitations of our system and discuss
where our system may fail or be of less value. One of our great-
est limitations is that the system was built and tested using only
frontal perspectives thereby imposing a constraint on the input fa-
cial orientations. Further the pipeline takes a ﬁxed number of video
frames as input which imposes a restriction on minimum number of
frames required for recognition. We restrict individual frames to a
ﬁxed size of 140 × 140 and higher resolution frames need to be re-
sized which may lead to information loss. Both spatio and temporal

size constraints can be improved by increasing neural network size
at the cost of computing resources.

Learning for deep neural networks can be extremely computa-
tionally intensive and can impose massive constraints on systemic
space-time complexity. Our system is no different and requires spe-
cialized hardware (NVIDIA TeslaTMor K40TMGrid GPUs) with a
minimum of 9 GB of VRAM on the graphics card for lowest of
batch sizes. Deep autoencoders can be data intensive and require
millions of unlabeled samples to train. Further the stacked autoen-
coder we train takes over 3 days to train requiring an additional
day to ﬁne tune predictor weights for larger labeled datasets. Even
though the system supports 7 emotions and 1 neutral face state, it
was not trained to detect neutral emotions - a constraint which can
be ﬁxed by adding more labeled data for neutral facial gestures.
The pipeline only recognizes 7 facial emotions but recent research
shows that there is a much wider range of emotions. Even though
neural networks win in a lot of scenarios, a lot more research needs
to be done to understand exactly how and why they work.

7 Conclusions

This paper uses semi-supervised paradigms in convolutional neural
nets for classiﬁcation of facial gestures in video sequences. Our
topologies are trained on millions of facial video clips and use
spatio-temporal convolutions to extract transient features in videos.
We developed a new scale-invariant sub-net which showed supe-
rior results for gesture recognition under variable lighting condi-
tions. We demonstrate effectiveness of our approach on both pub-
licly available datasets and samples collected by us.

References

ABIDIN, Z., AND HARJOKO, A. 2012. A neural network based fa-
cial expression recognition using ﬁsherface. International Jour-
nal of Computer Applications 59, 3, 30–34.

ANDR ´E, E., KLESEN, M., GEBHARD, P., ALLEN, S., AND RIST,
T. 2000. Exploiting models of personality and emotions to con-
trol the behavior of animated interactive agents. In Workshop on
Achieving Human-Like Behavior in Interactive Animated Agents,
3–7.

ASTHANA, A., ZAFEIRIOU, S., CHENG, S., AND PANTIC, M.
2014. Incremental face alignment in the wild. In Conference on
Computer Vision and Pattern Recognition (CVPR), IEEE, 1859–
1866.

BENGIO, Y., ROUX, N. L., VINCENT, P., DELALLEAU, O., AND
MARCOTTE, P. 2005. Convex neural networks. In Advances in
Neural Information Processing Systems, 123–130.

CARREIRA-PERPINAN, M. A., AND HINTON, G. E. 2005. On
contrastive divergence learning. In Proceedings of International
Workshop on Artiﬁcial Intelligence and Statistics, 33–40.

CASSELL, J., PELACHAUD, C., BADLER, N., STEEDMAN, M.,
ACHORN, B., BECKET, T., DOUVILLE, B., PREVOST, S., AND
STONE, M. 1994. Animated conversation: rule-based genera-
tion of facial expression, gesture & spoken intonation for multi-
ple conversational agents. In Proceedings of the 21st annual con-
ference on Computer graphics and interactive techniques, ACM,
413–420.

CHEN, H., LI, J., ZHANG, F., LI, Y., AND WANG, H. 2015. 3d
model-based continuous emotion recognition. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recogni-
tion, 1836–1845.

COWIE, R., DOUGLAS-COWIE, E., TSAPATSOULIS, N., VOT-
SIS, G., KOLLIAS, S., FELLENZ, W., AND TAYLOR, J. G.
2001. Emotion recognition in human-computer interaction. Sig-
nal Processing Magazine, IEEE 18, 1, 32–80.

DHALL, A., ASTHANA, A., GOECKE, R., AND GEDEON, T.
2011. Emotion recognition using PHOG and LPQ features. In
International Conference on Automatic Face & Gesture Recog-
nition, IEEE, 878–883.

FRITH, C. 2009. Role of facial expressions in social interactions.
Philosophical Transactions of the Royal Society B: Biological
Sciences 364, 1535, 3453–3458.

GARGESHA, M., AND KUCHI, P. 2002. Facial expression recogni-
tion using artiﬁcial neural networks. Artiﬁcial Neural Computer
Systems, 1–6.

HE, L., JIANG, D., YANG, L., PEI, E., WU, P., AND SAHLI,
H.
2015. Multimodal affective dimension prediction using
deep bidirectional long short-term memory recurrent neural net-
works. In Proceedings of the 5th International Workshop on Au-
dio/Visual Emotion Challenge, ACM, 73–80.

HINTON, G. E., AND SALAKHUTDINOV, R. R. 2006. Reducing
the dimensionality of data with neural networks. Science 313,
5786, 504–507.

HINTON, G. E., OSINDERO, S., AND TEH, Y.-W. 2006. A fast
learning algorithm for deep belief nets. Neural Computation 18,
7, 1527–1554.

JIA, Y., SHELHAMER, E., DONAHUE, J., KARAYEV, S., LONG,
J., GIRSHICK, R., GUADARRAMA, S., AND DARRELL, T.
2014. Caffe: Convolutional architecture for fast feature embed-
ding. arXiv preprint arXiv:1408.5093.

JUNG, H., LEE, S., YIM, J., PARK, S., AND KIM, J. 2015. Joint
ﬁne-tuning in deep neural networks for facial expression recog-
nition. In Proceedings of the IEEE International Conference on
Computer Vision, 2983–2991.

KAHOU, S. E., PAL, C., BOUTHILLIER, X., FROUMENTY, P.,
G ¨ULC¸ EHRE, C¸ ., MEMISEVIC, R., VINCENT, P., COURVILLE,
A., BENGIO, Y., AND FERRARI, R. C. 2013. Combining
modality speciﬁc deep neural networks for emotion recognition
in video. In Proceedings of the 15th ACM on International con-
ference on multimodal interaction, ACM, 543–550.

KAHOU, S. E., BOUTHILLIER, X., LAMBLIN, P., GULCEHRE,
C., MICHALSKI, V., KONDA, K., JEAN, S., FROUMENTY,
P., DAUPHIN, Y., BOULANGER-LEWANDOWSKI, N., ET AL.
2015. Emonets: Multimodal deep learning approaches for emo-
tion recognition in video. Journal on Multimodal User Inter-
faces, 1–13.

KARPATHY, A., TODERICI, G., SHETTY, S., LEUNG, T., SUK-
THANKAR, R., AND FEI-FEI, L. 2014. Large-scale video clas-
siﬁcation with convolutional neural networks. In Conference on
Computer Vision and Pattern Recognition (CVPR), IEEE, 1725–
1732.

CEREZO, E., BALDASSARRI, S., AND SERON, F. 2007.

Inter-
active agents for multimodal emotional user interaction. Multi
Conferences on Computer Science and Information Systems, 35–
42.

KINGMA, D. P., MOHAMED, S., REZENDE, D. J., AND
WELLING, M. 2014. Semi-supervised learning with deep gen-
erative models. In Advances in Neural Information Processing
Systems, 3581–3589.

KLEIN, T., AND PICARD, W. 1999. Computer response to user
frustration. MIT Media Laboratory Vision and Modelling Group
Technical Reports, TR 480.

PAPANDREOU, G., CHEN, L.-C., MURPHY, K., AND YUILLE,
A. L. 2015. Weakly-and semi-supervised learning of a dcnn for
semantic image segmentation. arXiv:1502.02734.

KOŁAKOWSKA, A., LANDOWSKA, A., SZWOCH, M., SZWOCH,
W., AND WR ´OBEL, M. 2014. Emotion recognition and its appli-
cations. In Human-Computer Systems Interaction: Backgrounds
and Applications 3. Springer, 51–62.

KOTSIA, I., AND PITAS, I. 2007. Facial expression recognition
in image sequences using geometric deformation features and
support vector machines. Transactions on Image Processing 16,
1, 172–187.

KRIZHEVSKY, A., SUTSKEVER, I., AND HINTON, G. E. 2012.
Imagenet classiﬁcation with deep convolutional neural networks.
In Advances in Neural Information Processing Systems, 1097–
1105.

LEE, D.-H. 2013. Pseudo-label: The simple and efﬁcient semi-
supervised learning method for deep neural networks. In Work-
shop on Challenges in Representation Learning, ICML, vol. 3.

LI, W.-J., WANG, C.-J., XU, D.-X., AND CHEN, S.-F. 2004.
Illumination invariant face recognition based on neural network
ensemble. In International Conference on Tools with Artiﬁcial
Intelligence, IEEE, 486–490.

LIU, M., LI, S., SHAN, S., WANG, R., AND CHEN, X. 2014.
Deeply learning deformable facial action parts model for dy-
In Computer Vision–ACCV 2014.
namic expression analysis.
Springer, 143–157.

LIU, M., SHAN, S., WANG, R., AND CHEN, X. 2014. Learning
expressionlets on spatio-temporal manifold for dynamic facial
expression recognition. In Conference on Computer Vision and
Pattern Recognition (CVPR), IEEE, 1749–1756.

LIU, M., WANG, R., LI, S., SHAN, S., HUANG, Z., AND CHEN,
X. 2014. Combining multiple kernel methods on riemannian
In Proceedings
manifold for emotion recognition in the wild.
of the 16th International Conference on Multimodal Interaction,
ACM, 494–501.

LIU, P., HAN, S., MENG, Z., AND TONG, Y. 2014. Facial ex-
pression recognition via a boosted deep belief network. In Con-
ference on Computer Vision and Pattern Recognition (CVPR),
IEEE, 1805–1812.

LUCEY, P., COHN, J. F., KANADE, T., SARAGIH, J., AMBADAR,
Z., AND MATTHEWS, I. 2010. The extended cohn-kanade
dataset (ck+): A complete dataset for action unit and emotion-
speciﬁed expression. In Computer Vision and Pattern Recogni-
tion Workshops (CVPRW), IEEE, 94–101.

MATHUR, S. N., AHLAWAT, A. K., AND VISHWAKARMA, V. P.
2008. Illumination invariant face recognition using supervised
and unsupervised learning algorithms. In Proceedings of World
Academy of Science, Engineering and Technology, vol. 33.

MICHEL, P., AND EL KALIOUBY, R. 2003. Real time facial ex-
pression recognition in video using support vector machines. In
Proceedings of the 5th international conference on Multimodal
interfaces, ACM, 258–264.

NWE, T. L., FOO, S. W., AND DE SILVA, L. C. 2003. Speech
emotion recognition using Hidden Markov Models. Speech com-
munication 41, 4, 603–623.

PANTIC, M., VALSTAR, M., RADEMAKER, R., AND MAAT, L.
2005. Web-based database for facial expression analysis.
In
International Conference on Multimedia and Expo, IEEE, 5–pp.

PIGHIN, F., HECKER, J., LISCHINSKI, D., SZELISKI, R., AND
SALESIN, D. H. 2006. Synthesizing realistic facial expressions
In ACM SIGGRAPH 2006 Courses, ACM,
from photographs.
19.

PRESTI, L., AND CASCIA, M. 2015. Using hankel matrices for
dynamics-based facial emotion recognition and pain detection.
In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops, 26–33.

ANONYMOUS SUBMISSION. 2016.

PYUN, H., KIM, Y., CHAE, W., KANG, H. W., AND SHIN, S. Y.
2003. An example-based approach for facial expression cloning.
In Proceedings of the 2003 ACM SIGGRAPH/Eurographics
symposium on Computer animation, Eurographics Association,
167–176.

SCHULLER, B., RIGOLL, G., AND LANG, M. 2004. Speech emo-
tion recognition combining acoustic features and linguistic infor-
mation in a hybrid support vector machine-belief network archi-
tecture. In International Conference on Acoustics, Speech, and
Signal Processing, vol. 1, IEEE, I–577.

SENECHAL, T., MCDUFF, D., AND KALIOUBY, R. 2015. Fa-
cial action unit detection using active learning and an efﬁcient
non-linear kernel approximation. In Proceedings of the IEEE In-
ternational Conference on Computer Vision Workshops, 10–18.

SHAN, C., GONG, S., AND MCOWAN, P. W. 2005. Robust facial
expression recognition using local binary patterns. In Interna-
tional Conference on Image Processing, vol. 2, IEEE, II–370.

SOURCES. Visual information processing and learning. [Online;

accessed 10-July-2015].

SZEGEDY, C., LIU, W., JIA, Y., SERMANET, P., REED, S.,
ANGUELOV, D., ERHAN, D., VANHOUCKE, V., AND RABI-
In Pro-
NOVICH, A. 2015. Going deeper with convolutions.
ceedings of the IEEE Conference on Computer Vision and Pat-
tern Recognition, 1–9.

TAIGMAN, Y., YANG, M., RANZATO, M., AND WOLF, L. 2014.
Deepface: Closing the gap to human-level performance in face
In Conference on Computer Vision and Pattern
veriﬁcation.
Recognition (CVPR), IEEE, 1701–1708.

TANG, Y., SALAKHUTDINOV, R., AND HINTON, G. 2012. Deep

lambertian networks. arXiv:1206.6445.

TRAN, D., BOURDEV, L., FERGUS, R., TORRESANI, L., AND
PALURI, M. 2014. Learning spatiotemporal features with 3d
convolutional networks. arXiv preprint arXiv:1412.0767.

VALSTAR, M., AND PANTIC, M. 2010. Induced disgust, happiness
and surprise: an addition to the mmi facial expression database.
In Workshop on EMOTION: Corpora for Research on Emotion
and Affect, 65.

VIERIU, R.-L., TULYAKOV, S., SEMENIUTA, S., SANGINETO,
E., AND SEBE, N. 2015. Facial expression recognition un-
In Automatic Face and Ges-
der a wide range of head poses.
ture Recognition (FG), 2015 11th IEEE International Confer-
ence and Workshops on, vol. 1, IEEE, 1–7.

VINCENT, P., LAROCHELLE, H., BENGIO, Y., AND MANZAGOL,
P.-A. 2008. Extracting and composing robust features with de-

noising autoencoders. In Proceedings of the 25th international
conference on Machine learning, ACM, 1096–1103.

VIOLA, P., AND JONES, M. J. 2004. Robust real-time face detec-
tion. International Journal of Computer Vision 57, 2, 137–154.
WALECKI, R., RUDOVIC, O., PAVLOVIC, V., AND PANTIC, M.
2015. Variable-state latent conditional random ﬁelds for facial
expression recognition and action unit detection. In Automatic
Face and Gesture Recognition (FG), 2015 11th IEEE Interna-
tional Conference and Workshops on, vol. 1, IEEE, 1–8.

WANG, Z., WANG, S., AND JI, Q. 2013. Capturing complex
spatio-temporal relations among facial muscles for facial expres-
sion recognition. In Computer Vision and Pattern Recognition
(CVPR), IEEE, 3422–3429.

WATERS, K.

1987. A muscle model for animation three-
In Proceedings of the annual
dimensional facial expression.
conference on Computer graphics and interactive techniques,
vol. 21, ACM, 17–24.

WESTON, J., RATLE, F., MOBAHI, H., AND COLLOBERT, R.
2012. Deep learning via semi-supervised embedding. In Neu-
ral Networks: Tricks of the Trade. Springer, 639–655.

ZHANG, Z., LUO, P., LOY, C.-C., AND TANG, X. 2015. Learning
In Proceedings of the
social relation traits from face images.
IEEE International Conference on Computer Vision, 3631–3639.
ZHAO, K., CHU, W.-S., DE LA TORRE, F., COHN, J. F., AND
ZHANG, H. 2015. Joint patch and multi-label learning for facial
action unit detection. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2207–2216.

