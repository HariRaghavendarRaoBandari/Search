6
1
0
2

 
r
a

M
1

 

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
4
4
4
0
0

.

3
0
6
1
:
v
i
X
r
a

On divergences tests for composite hypotheses under

composite likelihood

N. Mart´ın1, L. Pardo2 K. Zografos3

1Department of Statistics and O.R. II, Complutense University of Madrid, 28003 Madrid, Spain
2Department of Statistics and O.R. I, Complutense University of Madrid, 28040 Madrid, Spain

3Department of Mathematics, University of Ioannina, 45110 Ioannina, Greece

Abstract

It is well-known that in some situations it is not easy to compute the likelihood function as
the datasets might be large or the model is too complex. In that contexts composite likelihood,
derived by multiplying the likelihoods of subjects of the variables, may be useful. The extension
of the classical likelihood ratio test statistics to the framework of composite likelihoods is used as
a procedure to solve the problem of testing in the context of composite likelihood. In this paper
we introduce and study a new family of test statistics for composite likelihood: Composite
φ-divergence test statistics for solving the problem of testing a simple null hypothesis or a
composite null hypothesis. To do that we introduce and study the asymptotic distribution of
the restricted maximum composite likelihood estimate.

AMS 2001 Subject Classiﬁcation:

Keywords and phrases: Composite likelihoods, maximum composite likelihood estimator, re-
stricted maximum composite likelihood estimator, composite likelihood φ-divergence test-statistics.

1

Introduction

Hypothesis testing is a cornerstone of mathematical statistics and, subsequently, the theory of log-
likelihood ratio tests is a cornerstone in the theory of testing statistical hypotheses, too. On the
other hand, maximum likelihood estimators play a key role in the development of log-likelihood ratio
tests. Albeit maximum likelihood estimators can be easily obtained and they obey nice large sample
properties, there are cases, like the case of complicated probabilistic models where the maximum
likelihood estimators do not exist or they can not be obtained. In such a case the problem is usually
overcomed by the use of pseudo-likelihood functions and the respective estimators which result by
maximization of such a function. Composite likelihood and the respective composite likelihood
estimators are an appealing case of pseudo-likelihood estimators. There is an extensive literature
composite likelihood methods in Statistics. The history of the composite likelihood may be traced
back to the pseudo-likelihood approach of Besag (1974) for modeling spatial data. The name
of composite likelihood was given by Lindsay (1988) to refer a likelihood type object formed by
multiplying together individual component likelihoods, each of which corresponds to a marginal or
conditional event. Composite likelihood ﬁnds applications in a variety of ﬁelds, including genetics,
spatial statistics, longitudinal analysis, multivariate modeling, to mention a few. The special issue,
with guest editors Reid, Lindsay and Liang (2011), of the journal Statistica Sinica is devoted to

1

composite likelihood methods and applications in several ﬁelds and it, moreover, provides with an
exhaustive and updated source of knowledge in the subject. The recent papers by Reid (2013)
and Cattelan and Sartori (2016) concentrate on new developments on the composite likelihood
inference.

Distance or divergence based on methods of estimation and testing are fundamental tools and
constitute a methodological part in the ﬁeld of statistical inference. The monograph by Kullback
(1959) was probably the starting point of usage of the so called divergence measure for testing
statistical hypotheses. The next important steps were the monographs by Read and Cressie (1988),
Vajda (1989), Pardo (2006) and Basu et al. (2011) where distance, divergence or disparity methods
were developed for estimation and testing. Thousands of papers have been also published in this
frame and many of them have been exploited and mentioned in the above monographs. For testing
a statistical hypothesis in a parametric framework, a test-statistic can be constructed by means of
a distance or divergence measure between the empirical model and the model which is speciﬁed
by the null hypothesis. The empirical model is the parametric model which governs the data with
the unknown parameters to be replaced by their maximum likelihood estimators. The asymptotic
normality of the maximum likelihood estimators is exploited along with the well known delta
method in order to reach the asymptotic distribution of the respective divergence test-statistics.

The divergence test-statistics are based on considering the distance between density functions,
chosen in an appropriate way. In the statistical situations in which we only have composite densities
it seems completely natural to deﬁne statistical procedures of testing based on divergence measures
but between the composite densities instead of the densities. This paper is motivated by the ne-
cessity to develop divergence based on methods, described above, for testing statistical hypotheses
when the maximum composite likelihood estimators are used instead of the classic maximum likeli-
hood estimators and we consider divergence measures between composite density functions in order
to get an appropriate test-statistic. In this framework, the next section introduces the notation
which will be used and reviews composite likelihood estimators. Section 3 is devoted to present a
family of φ-divergence test-statistics for testing simple null hypothesis. The formulation of testing
composite null hypotheses by means of φ-divergence type test-statistics is the subject of Section 5.
But in order to get the results in relation to the composite null hypothesis it is necessary in Sec-
tion 4 to introduce and study the restricted maximum composite estimator as well its asymptotic
distribution and the relationship between the restricted and the un-restricted maximum composite
likelihood estimators. Section 6 is devoted to present a numerical example and ﬁnally a simulation
study is carried out in Section 7. The proofs of the main theoretic results are provided in the
Appendix.

2 Composite likelihood and divergence tests

We adopt here the notation by Joe et al. (2012) regarding composite likelihood function and the
respective maximum composite likelihood estimators. In this regard, let {f (·; θ), θ ∈ Θ ⊆ Rp, p ≥
1} be a parametric identiﬁable family of distributions for an observation y, a realization of a random
m-vector Y . In this setting, the composite density based on K diﬀerent margins or conditional
distributions has the form

CL(θ,y) =

KQk=1

f wk
Ak

(yj, j ∈ Ak; θ)

2

and the composite log-density based on K diﬀerent margins or conditional distributions has the
form

cℓ(θ,y) =

with

wkℓAk (θ,y),

KXk=1

ℓAk (θ,y) = log fAk(yj, j ∈ Ak; θ),

where {Ak}K
k=1 is a family of random variables associated either with marginal or conditional
distributions involving some yj, j ∈ {1, ..., m} and wk, k = 1, ..., K are non-negative and known
weights. If the weights are all equal, then they can be ignored, actually all the statistical procedures
produce equivalent results.

Let also y1, ..., yn be independent and identically distributed replications of y. We denote by

cℓ(θ,y1, ..., yn) =

cℓ(θ,yi)

nXi=1

the composite log-likelihood function for the whole sample. In complete accordance with the classic

maximum likelihood estimator, the maximum composite likelihood estimatorbθc is deﬁned by

cℓ(θ,yi) = arg max

wkℓAk (θ,yi).

bθc = arg max

θ∈Θ

nXi=1

nXi=1

KXk=1

θ∈Θ

It can be also obtained by the solution of the equation

u(θ,y1, ...,yn) = 0p,

where

u(θ,y1, ...,yn) =

∂cℓ(θ,y1, ...,yn)

∂θ

=

nXi=1

KXk=1

wk

∂ℓAk (θ,y)

∂θ

,

is the composite likelihood score function, that is the partial derivative of the composite log-
likelihood with respect to the parameter vector.

The maximum composite likelihood estimatorbθc obeys asymptotic normality and in particular

√n(bθc − θ) L−→n→∞ N(cid:0)0, G−1

∗ (θ)(cid:1) ,

where G∗(θ) denotes Godambe information matrix, deﬁned by
G∗(θ) = H(θ)J−1(θ)H(θ),

with H(θ) being the sensitivity or Hessian matrix and J (θ) being the variability matrix, deﬁned,
respectively, by

H(θ) = Eθ[− ∂
J(θ) = V arθ[u(θ,Y )] = Eθ[u(θ,Y )uT (θ,Y )],

∂θ uT (θ,Y )],

where the superscript T denotes the transpose of a vector or a matrix.

The matrices H(θ) and J(θ) are, by deﬁnition, nonegative deﬁnite matrices but throughout
this paper both, H(θ) and J (θ), are assumed to be positive deﬁnite matrices. Since the component

3

score functions can be correlated, we have H(θ) 6= J (θ). If cℓ(θ,y) is a true log-likelihood function
then H(θ) = J(θ) = IF (θ), being I F (θ) the Fisher information matrix of the model. Using
multivariate version of the Cauchy-Schwarz inequality we have that the matrix G∗(θ) − I F (θ) is
non-negative deﬁnite, i.e., the full likelihood function is more eﬃcient than any other composite
likelihood function (cf. Lindsay, 1988, Lemma 4A).

For two densities p and q associated with two m-dimensional random variables respectively,

Csisz´ar’s φ-divergence between p and q is deﬁned by

Dφ(p, q) =ZRm

q(y)φ(cid:18) p(y)

q(y)(cid:19) dy,

where φ is a real valued convex function, satisfying appropriate conditions which ensure the exis-
tence of the above integral (cf., Csisz´ar, 1963, 1967, Ali and Silvey 1963, and Pardo, 2006. Csisz´ar’s
φ-divergence has been axiomatically characterized and studied extensively by Liese and Vajda (1987,
2006), Vajda (1989), and Stummer and Vajda (2010), among many others. Particular choices of
the convex functions φ, lead to important measures of divergence including Kullback and Leibler
(1951) divergence, R´enyi (1960) divergence and Cressie and Read (1984) λ-power divergence, to
mention a few. Csisz´ar’s φ-divergence can be extended and used in testing hypotheses on more
than two distributions (cf. Zografos (1998) and references appeared therein).

In this paper we are going to consider φ-divergence measures between the composite densities
CL(θ1,y) and CL(θ2,y) in order to solve diﬀerent problems of testing hypotheses. The φ-divergence
measure between composite densities CL(θ1,y) and CL(θ2,y) will be deﬁned by

Dφ(θ1,θ2) =ZRm

CL(θ2,y)φ(cid:18)CL(θ1,y)

CL(θ2,y)(cid:19) dy,

(1)

φ(v)

v }.

v→∞

φ ∈ Ψ, with

Ψ = {φ : φ is strictly convex, φ(1) = φ′(1) = 0, 0φ(cid:0) 0

0(cid:1) = 0, 0φ(cid:0) u

0(cid:1) = u lim

An important particular case is the Kullback-Leibler divergence measure obtained from (1) with

φ(x) = x log x − x + 1, i.e.

DKullback(θ1,θ2) =ZRm

CL(θ1,y) log CL(θ1,y)
CL(θ2,y)

dy.

Based on (1) we shall present in this paper some new test-statistics for testing simple null
hypothesis as well as composite null hypothesis. To the best of our knowledge, it is the ﬁrst time
that φ-divergences are used for solving testing problems in the context of composite likelihood.
However, the Kullback-Leibler divergence has been used, in the context of composite likelihood, by
many authors in model selection, see for instance Varin (2008).

3 Hypothesis testing: Simple null hypothesis

In this section we are interested in testing

H0 : θ = θ0 versus H1 : θ 6= θ0.

(2)

4

If we consider the φ-divergence between the composite densities CL(bθc,y) and CL(θ0,y),

Dφ(bθc,θ0) =ZRm

CL(θ0,y)! dy
CL(θ0,y)φ CL(bθc,y)

veriﬁes Dφ(bθc,θ0) ≥ 0, and the equality holds if and only if CL(bθc,y) = CL(θ0,y). Small values
of Dφ(bθc,θ0) are in favour of H0 : θ = θ0, while large values of Dφ(bθc,θ0), suggest rejection of
H0. This is due to the fact that large values of Dφ(bθc,θ0) suggest that the model CL(bθc,y) is not
very close to CL(θ0,y). Therefore, H0 is rejected if Dφ(bθc,θ0) > c, where c is speciﬁed so that

the signiﬁcance level of the test is α. In order to obtain c, in the next theorem we shall obtain the
asymptotic distribution of

Tφ,n(bθc,θ0) =

2n
φ′′(1)

Dφ(bθc,θ0),

(3)

which we shall refer to as composite φ-divergence test-statistics for testing simple null hypothesis.

Theorem 1 Under the null hypothesis H0 : θ = θ0,

Tφ,n(bθc,θ0) L−→n→∞

λiZ 2
i ,

kXi=1

where λi, i = 1, ..., k, are the eigenvalues of the matrix J (θ0)G−1

∗ (θ0),

and Z1, ...Zr are independent standard normal random variables.

k = rank (J(θ0)) ,

Proof. Under the standard regularity assumptions of asymptoitc statistics (cf. Serﬂing, 1980, p.
144 and Pardo, 2006, p. 58), we have

∂Dφ(θ,θ0)

∂CL(θ,y)

∂θ

=ZRm
(cid:12)(cid:12)(cid:12)(cid:12)θ=θ0
= φ′ (1)ZRm

φ′(cid:18) CL(θ,y)
CL(θ0,y)(cid:19) dy,
(cid:12)(cid:12)(cid:12)(cid:12)θ=θ0

∂θ

∂CL(θ,y)

dy = 0p.

∂θ

∂Dφ(θ,θ0)

∂θ

therefore

On the other hand,

∂2Dφ(θ,θ0)

∂θ∂θT

=ZRm

∂θ∂θT φ′(cid:18) CL(θ,y)
CL(θ0,y)(cid:19) dy+ZRm
∂2CL(θ,y)
= φ′′ (1)ZRm

∂θ∂θT

∂θ

∂2Dφ(θ,θ0)

∂cℓ(θ,y)

(cid:12)(cid:12)(cid:12)(cid:12)θ=θ0

and

1

∂CL(θ,y)

∂θT

CL(θ0,y)

φ′′(cid:18) CL(θ,y)
CL(θ0,y)(cid:19) dy
(cid:12)(cid:12)(cid:12)(cid:12)θ=θ0 CL(θ0,y)dy = φ′′ (1) J (θ0).

∂CL(θ,y)

∂θ

∂cℓ(θ,y)

∂θT

5

Then, from

the desired result is obtained. The value of k comes from

φ′′ (1)

2

Dφ(bθc,θ0) =
k = rank(cid:0)G−1

(bθc − θ0)T J (θ0)(bθc − θ0) + o(cid:16)n−1/2(cid:17)
∗ (θ0)(cid:1) = rank(J (θ0)).

∗ (θ0)J T (θ0)G−1

Remark 1 Based on the previous Theorem we shall reject the null hypothesis H0 : θ = θ0 if

Tφ,n(bθc,θ0) > cα, where cα is the quantile of order 1−α of the asymptotic distribution of Tφ,n(bθc,θ0)

given in (3). The value of k is usually p, since the components of θ are assumed to be non-redundant.

In most cases, the power function of this testing procedure can not be calculated explicitly. In
the following theorem we present a useful asymptotic result for approximating the power function.

Theorem 2 Let θ∗ be the true parameter, with θ∗ 6= θ0 . Then it holds
φ (θ∗)(cid:1) ,

√n(cid:16)Dφ(bθc,θ0) − Dφ(θ∗,θ0)(cid:17) L−→n→∞ N(cid:0)0, σ2

where

and q = (q1, ..., qp)T with qj = ∂Dφ(θ,θ0)

∂θj

, j = 1, ..., p.

Proof. A ﬁrst order Taylor expansion gives

φ (θ∗) = qT G−1
σ2

∗ (θ0)q

(cid:12)(cid:12)(cid:12)θ=θ∗

But

Dφ(bθc,θ0) = Dφ(θ∗,θ0) + qT (bθc − θ∗) + o((cid:13)(cid:13)(cid:13)bθc − θ∗(cid:13)(cid:13)(cid:13)).

∗ (θ)(cid:1)
√n(bθc − θ) L−→n→∞ N(cid:0)0, G−1

and √no((cid:13)(cid:13)(cid:13)bθc − θ∗(cid:13)(cid:13)(cid:13)) = op(1). Now the result follows.

Remark 2 From Theorem 2, a ﬁrst approximation to the power function, at θ∗ 6= θ0, is given by

βn,φ (θ∗) = 1 − Φ(cid:18) √n

σφ (θ∗)(cid:18) φ′′(1)cα

2n − Dφ(θ∗,θ0)(cid:19)(cid:19)

where Φ is the standard normal distribution function. If some θ∗ 6= θ0 is the true parameter, the
probability of rejecting θ0 with the rejection rule Tφ,n(bθc,θ0) > cα, for ﬁxed signiﬁcance level α,
tends to one as n → ∞. Hence, the test is consistent in Fraser’s sense.

6

4 Restricted maximum composite likelihood estimator

In some common situations such as the problem of testing composite null hypotheses, it is necessary
to get the maximum composite likelihood estimator which is restricted by some restrictions of the
type

(4)
where g is a function such that g : Θ ⊆ Rp −→ Rr, r is an integer, with r < p and 0r denotes the
null vector of dimension r. The function g is a vector valued function such that the p × r matrix
(5)

g(θ) = 0r,

∂gT (θ)

G(θ) =

∂θ

exists and is continuous in θ with rank(G(θ)) = r. The restricted maximum composite likelihood
estimator of θ is deﬁned by

and is obtained by the solution of the restricted likelihood equations

cℓ(θ,yi) = arg max
θ∈Θ,g(θ)=0r

wkℓAk (θ,yi).

nXi=1

KXk=1

eθrc = arg max

θ∈Θ,g(θ)=0r

nXi=1
nXi=1

∂
∂θ

cℓ(θ,yi) + G(θ)λ = 0p,

g(θ) = 0r,

where λ ∈Rr is a vector of Lagrange multipliers.

In this section we shall get the asymptotic distribution of the restricted maximum composite
likelihood estimator. Consider a random sample y1, ..., yn from the parametric model f (·; θ), θ ∈

Θ ⊆ Rp, p ≥ 1, and let bθc and eθrc be the unrestricted and the restricted maximum composite
likelihood estimators of θ. The following result derives the asymptotic distribution ofeθrc.

Theorem 3 Under the constraints g(θ) = 0r the restricted maximum composite likelihood estima-
tor obeys asymptotic normality in the sense

with

√n(eθrc − θ) L−→n→∞ N (0p,eΣrc),

P (θ) = H−1(θ) + Q(θ)GT (θ)H−1(θ),

eΣrc = P (θ)J(θ)P T (θ),
Q(θ) = −H−1(θ)G(θ)(cid:2)GT (θ)H−1(θ)G(θ)(cid:3)−1

The proof of the Theorem is outlined in Section 9.1 of Appendix.

.

The lemma that follows formulates the relationship between the maximum composite and the

restricted maximum composite likelihood estimatorsbθc andeθrc respectively.
Lemma 4 The estimators of θ, bθc and eθrc, satisfy
√n(eθrc − θ) =(cid:0)I p + Q(θ)GT (θ)(cid:1)√n(bθc − θ) + oP (1).

The proof of the lemma is given in Section 9.2 of Appendix.

7

5 Composite null hypothesis

Following Basu et al. (2015), consider the null hypothesis

H0 : θ ∈ Θ0 against H0 : θ /∈ Θ0,

which restricts the parameter θ to a subset Θ0 of Θ ⊆ Rp, p ≥ 1. Based on Sen and Singer (1993,
p. 239), we shall assume that the composite null hypothesis H0 : θ ∈ Θ0 can be equivalently
formulated in the form
(6)

H0 : g(θ) = 0r.

For testing the composite null hypothesis (6) on the basis of a random sample y1, ..., yn from the
parametric model f (·; θ), θ ∈ Θ ⊆ Rp, p ≥ 1, there are well-known procedures to be applied. The
likelihood ratio test-statistic, the Wald and Rao statistics are used in this direction. Test-statistics
based on divergences or disparities, as they have been described and mentioned above, constitute an
appealing procedure for testing this hypothesis. Moreover, there are composite likelihood methods
analog to the likelihood ratio test or the Wald test. However, there are not composite likelihood
versions of the tests based on divergence measures, to the best of our knowledge. So, our aim in
this Section is to develop test-statistics for testing (6), on the basis of divergence measures and in

the composite likelihood framework. The φ-divergence between the composite densities CL(bθc,y)
and CL(eθrc,y), is given by

CL(eθrc,y)! dy.
Dφ(bθc,eθrc) = RRm CL(eθrc,y)φ  CL(bθc,y)

Based on the property Dφ(bθc,eθrc) ≥ 0, with equality, if and only if CL(eθrc,y) = CL(bθc,y),
small values of Dφ(bθc,eθrc) are in favour of (6), while large values of Dφ(bθc,eθrc) suggest that the
composite densities CL(eθrc,y) and CL(bθc,y) are not the same and the same is expected for the
respective theoretic models fθ with θ ∈ Θ and fθ with θ ∈ Θ0. So, small values of Dφ(bθc,eθrc) are
in favor of (6) while large values of Dφ(bθc,eθrc) suggest the rejection of H0. Given the asymptotic
normality of the maximum composite likelihood estimator bθc, the asymptotic normality of the
respective restricted estimator eθrc should be veriﬁed. The asymptotic normality of eθrc and the
investigation of the asymptotic distribution of the test-statistic Dφ(bθc,eθrc) is the subject of the

Based on Theorem 3 and Lemma 4, the composite likelihood φ-divergence test-statistic is in-
troduced in the next theorem and its asymptotic distribution is derived under the composite null
hypothesis (6). The standard regularity assumptions of asymptotic statistic are assumed to be
valid (cf. Serﬂing, 1980, p. 144 and Pardo, 2006, p. 58).

next section.

Theorem 5 Under the composite null hypothesis (6),

where βi, i = 1, ..., k, are the eigenvalues of the matrix

Tφ,n(bθc,eθrc) =

2n
φ′′(1)

J (θ0)G(θ)QT (θ)G−1

βiZ 2
i ,

kXi=1
Dφ(bθc,eθrc) L−→n→∞
∗ (θ)Q(θ)GT (θ),

k = rank(cid:0)G(θ)QT (θ)G−1

∗ (θ)Q(θ)GT (θ)J(θ0)G(θ)QT (θ)G−1

∗ (θ)Q(θ)GT (θ)(cid:1) ,

and Z1, ...Zr are independent standard normal random variables.

8

by composite φ-divergence test-statistics for testing composite null hypothesis.

The proof of this Theorem is presented in Appendix C. In the following we refer Tφ,n(bθc,eθrc)

Remark 3 For the testing problem considered in this Section it is perhaps well-known the composite
likelihood ratio test but it was not possible for us to ﬁnd it in the statistical literature. This test will
be used in Section 4 and this is the reason to develop the said test in the present remark.
We shall denote

The composite likelihood ratio test for testing the composite null hypothesis (6), considered in this
paper, is deﬁned by

cℓ (θ) =

cℓ (θ, yi)

nPi=1

A second order Taylor expansion gives

=

∂θ

∂cℓ (θ, yi)

cℓ(eθrc, yi) − cℓ(bθc, yi)
(cid:12)(cid:12)(cid:12)(cid:12)θ=bθc
(bθc −eθrc) +
(cid:12)(cid:12)(cid:12)(cid:12)θ=bθc

λn(bθc,eθrc) = 2(cid:16)cℓ(bθc) − cℓ(eθrc)(cid:17) .
(cid:12)(cid:12)(cid:12)(cid:12)θ=bθc
(bθc −eθrc)T ∂2cℓ (θ, yi)
(cid:12)(cid:12)(cid:12)(cid:12)θ=bθc

(bθc −eθrc) + oP (1).
a.s.−→n→∞ −H(θ0),
2(cℓ(bθc) − cℓ(eθrc)) = √n(bθc −eθrc)T H(θ0)√n(bθc −eθrc) + oP (1)

∂2cℓ (θ, yi)

∂cℓ (θ, yi)

1
n

nPi=1

= 0p

and

∂θ

∂θ∂θT

1
2

∂θ∂θT

But,

and therefore,

which yields

where γi, i = 1, ..., ℓ are the non null eigenvalues of the matrix

λn(bθc,eθrc) = 2(cℓ(bθc) − cℓ(eθrc)) L−→n→∞

ℓXi=1

γiZ 2
i ,

(7)

H(θ0)G(θ)QT (θ)G−1

∗ (θ)Q(θ)GT (θ),

with

∗ (θ)Q(θ)G(θ)T H(θ)G(θ)QT (θ)G−1
and Zi, i = 1, ..., ℓ are independent standard normal random variables.

ℓ = rank(cid:0)G(θ)QT (θ)G−1

∗ (θ)Q(θ)GT (θ)(cid:1) ,

Remark 4 In order to avoid the problem of getting percentiles or probabilities from the distribution
of linear combinations of chi-squares we are going to present some adjusted composite likelihood φ-
divergence test-statistics.

Following Corollary 1 of Rao and Scott (1981) one can use the statistic

1Tφ,n(bθc,eθrc) =

Tφ,n(bθc,eθrc)

λmax

≤

rXi=1

Z 2
i ,

9

Z 2
i ∼ χ2

rPi=1

where λmax = max (β1, ..., βr). As

r, a strategy that rejects the null hypothesis H0 :

level , where χ2

r,1−α is the quantile of order 1 − α for χ2
r.

g(θ) = 0r for 1Tφ,n(bθc,eθrc) > χ2
Another approximation to the asymptotic tail probabilities of Tφ,n(bθc,eθrc) can be obtained through

r,1−α produces an asymptotically conservative test at α nominal

the modiﬁcation

where ¯λ = 1
tion with r degrees of freedom. In this case we can observe that

i=1 βi (see Satterthwaite, 1946), considered approximated by a chi-squared distribu-

Tφ,n(bθc,eθrc)

λ

,

2Tφ,n(bθc,eθrc) =
rPr
Eh2Tφ,n(bθc,eθrc)i = r = E(cid:2)χ2
r(cid:3) ,
2Pr
V arh2Tφ,n(bθc,eθrc)i =
i# =

i=1 β2
i
2
λ

βiZ 2

E" kXi=1

kXi=1

If we denote by Λ = diag (β1, ..., βr), we get

= 2r + 2

rXi=1(cid:0)βi − λ(cid:1)2

λ

2

> 2r = V ar(cid:2)χ2
r(cid:3) .

βi = trace (Λ) = trace (A (θ) G∗(θ)) .

The test given by the statistic 2Tφ,n(fbθc

, fbθR

) is more conservative than the one based on

and we can ﬁnd ν by imposing the condition V arh3Tφ,n(bθc,eθrc)i = 2Eh3Tφ,n(bθc,eθrc)i , as in the

chi-squared distribution. Since

3Tφ,n(bθc,eθrc) =
Eh3Tφ,n(bθc,eθrc)i =

ν = 1 +

2Tφ,n(bθc,eθrc)

ν

=

Tφ,n(bθc,eθrc)

νλ

,

r
ν

and V arh3Tφ,n(bθc,eθrc)i =

= 1 + CV 2({βi}r

i=1),

2r
ν

,

rXi=1(cid:0)βi − λ(cid:1)2

rλ

2

where CV represents the coeﬃcient of variation. Then a chi-square distribution with r

ν degrees of

freedom approximates the asymptotic distribution of the statistic 3Tφ,n(bθc,eθrc) for large n.
The degrees of freedom of 3Tφ,n(bθc,eθrc) is k

ν , which may not be an integer. To avoid this
diﬃculty one can modify the statistic such that the ﬁrst two moments match speciﬁcally with the
r distribution (rather than with just any other χ2 distribution). Speciﬁcally let
χ2

We have

X = 2Tφ,n(bθc,eθrc).
E [X] = r = E(cid:2)χ2
r(cid:3) ,

2

2

rPi=1

λ

βi
2 = 2r + 2

V ar[X] =

rXi=1(cid:0)βi − λ(cid:1)2

λ

2

10

= 2r + c,

where c stands for the last indicated term in the previous expression. We deﬁne Y = (X − a)/b,
where the constants a and b are such that

Thus,

Solving these equations, we get

E(Y ) = r,

V ar(Y ) = 2r.

r − a

b

= r,

2r + c

b2 = 2r.

b =r1 +

c
2r

,

a = r(1 − b).

Thus it makes sense to consider another modiﬁcation of the statistic given by

the large sample distribution of which may be approximated by the χ2

r distribution.

4Tφ,n(bθc,eθrc) =

2Tφ,n(bθc,eθrc) − a

b

,

The approximation presented in this remark for the asymptotic distribution of the φ-divergence

test-statistics, Tφ,n(bθc,eθrc), can be used in the approximation of the φ-divergence test-statistics
Tφ,n(bθc,θ0) as well.
By the previous theorem, the null hypothesis should be rejected if Tφ,n(bθc,eθrc) ≥ cα, where cα
is the quantile of order 1− α of the asymptotic distribution of Tφ,n(bθc,eθrc). The following theorem
can be used to approximate the power function. Assume that θ /∈ Θ0 is the true value of the
parameter so that bθc
θ and that there exists θ∗ ∈ Θ0 such that the restricted maximum
composite likelihood estimator satisﬁeseθrc

θ∗, as well as

a.s.−→n→∞

a.s.−→n→∞

n1/2(cid:16)(bθc,eθrc) − (θ, θ∗)(cid:17) L−→n→∞ N(cid:18)(cid:18) 0p

0p (cid:19) ,(cid:18) G−1

∗ (θ)

12(θ, θ∗) Σ (θ, θ∗) (cid:19)(cid:19) ,

A12(θ, θ∗)

AT

where A12(θ, θ∗) and Σ (θ, θ∗) are appropriate p × p matrices . We have then the following result.
Theorem 6 Under H1 we have

where

being

σ2 (θ, θ∗) = tT G−1

n1/2(cid:16)Dφ(bθc,eθrc) − Dφ(θ, θ∗)(cid:17) L−→n→∞ N(cid:0)0, σ2 (θ, θ∗)(cid:1)
(cid:12)(cid:12)(cid:12)(cid:12)θ2=θ∗

∗ (θ) t+ 2tT A12(θ, θ∗)s + sT Σ (θ, θ∗) s

(cid:12)(cid:12)(cid:12)(cid:12)θ1=θ

∂Dφ(θ1, θ∗)

∂Dφ(θ2, θ∗)

and s =

∂θ1

∂θ2

t =

.

(8)

Proof. The result follows in a straightforward manner by considering a ﬁrst order Taylor expansion

of Dφ(bθc,eθrc), which yields

Dφ(bθc,eθrc) = Dφ(θ, θ∗) + tT (bθc − θ) + sT (eθrc − θ∗) + o((cid:13)(cid:13)(cid:13)bθc − θ∗(cid:13)(cid:13)(cid:13) +(cid:13)(cid:13)(cid:13)eθrc − θ0(cid:13)(cid:13)(cid:13)).

11

Remark 5 On the basis of the previous theorem we can get an approximation of the power function

πφ

n (θ) = Prθ(cid:16)Tφ,n(bθc,eθrc) ≥ c(cid:17)

πβ,γ

n,α (θ) = 1 − Φ  n1/2

σ (θ, θ∗)(cid:16) c

2n − Dφ(θ, θ∗)(cid:17)! ,

(9)

at θ as

where

with

and

where Φ(x) is the standard normal distribution function and σ2 (θ, θ∗) was deﬁned in (8).

If some θ 6= θ∗ is the true parameter, then the probability of rejecting H0 with the rule that it
is rejected when Tφ,n(bθc,eθrc) ≥ c for a ﬁxed test size α tends to one as n → ∞. The test-statistic

Obtaining the approximate sample size n to guarantee a power of π at a given alternative θ∗ is

is consistent in the Fraser’s sense.

an interesting application of formula (9). Let n∗ be the positive root of the equation (9), i.e.

n∗ =

A + B +pA(A + 2B)

φ(θ, θ∗)

2D2

,

Then the required sample size is n = [n∗] + 1, where [·] is used to denote “integer part of”.

A = σ2 (θ, θ∗)(cid:0)Φ−1 (1 − π)(cid:1)2

and B = cDφ(fθ, fθ∗).

Remark 6 The class of φ-divergence measures is a wide family of divergence measures but un-
fortunately there are some classical divergence measures that are not included in this family of
φ-divergence measures such as the R´enyi’s divergence or the Sharma and Mittal’s divergence. The
expression of R´enyi’s divergence is given by

Da

R´enyi(bθc,eθrc) =

1

a (a − 1)

logZRm

CLa(bθc,y)CL1−a(eθrc,y)dy , if a 6= 0, 1,
CL(bθc,y) log CL(bθc,y)
CL(eθrc,y)

R´enyi(bθc,eθrc) = DKull(eθrc,bθc) = ZRm

dy

D0

R´enyi(bθc,eθrc) = lim

a→0

Da

D1

R´enyi(bθc,eθrc) = lim

a→1

DR´enyi(bθc,eθrc) = DKull(bθc,eθrc).

This measure of divergence was introduced in R´enyi (1961) for a > 0 and a 6= 1 and Liese and Vajda
(1987) extended it for all a 6= 1, 0. An interesting divergence measure related to R´enyi divergence
measure is the Bhattacharya divergence deﬁned as the R´enyi divergence for a = 1/2 divided by
4. Other interesting example of divergence measure, not included in the family of φ-divergence
measures, is the divergence measures introduced by Sharma and Mittal (1997).

In order to unify the previous divergence measures as well as another divergence measures
Men´endez et al. (1995, 1997) introduced the family of divergences called “(h, φ)-divergence mea-
sures” in the following way

Dh

φ(bθc,eθrc) = h(cid:16)Dφ(bθc,eθrc)(cid:17) ,

12

t i onto [0,∞),
where h is a diﬀerentiable increasing function mapping from h0, φ (0) + limt→∞
with h(0) = 0, h′(0) > 0, and φ ∈ Ψ. In the next table these divergence measures are presented,
along with the corresponding expressions of h and φ.

φ(t)

Divergence

R´enyi

Sharma-Mittal

h (x)

1

1

a(a−1) log (a (a − 1) x + 1) ,

b−1n[1 + a (a − 1) x]

b−1

a−1 − 1o ,

a 6= 0, 1

b, a 6= 1

φ (x)

xa−a(x−1)−1
xa−a(x−1)−1

a(a−1)
a(a−1)

,

,

a 6= 0, 1
a 6= 0, 1

Based on the (h, φ)-divergence measures we can deﬁne a new family of (h, φ)-divergence test-

statistics for testing the null hypothesis H0 given in (6)

Since

Th,φ,n(bθc,eθrc) =

2n

φ′′(1)h′(0)

h(cid:16)Dφ(bθc,eθrc)(cid:17) .

h(x) = h(0) + h′(0)x + o(x)

(10)

the asymptotic distribution of Th,φ,n(bθc,eθrc) coincides with the asymptotic distribution of Tφ,n(bθc,eθrc).

In a similar way we can deﬁne the family of (h, φ)-divergence test-statistics for testing the null hy-
pothesis H0 given in (2) by

Th,φ,n(bθc,θ0) =

2n

φ′′(1)h′(0)

h(cid:16)Dφ(bθc,θ0)(cid:17) .

6 Numerical Example

In this section we shall consider an example, studied previously by Xu and Reid (2011) on the
robustness of maximum composite estimator. The aim of this section is to clarify the diﬀerent
issues which are discussed in the previous sections.

Consider the random vector Y = (Y1, Y2, Y3, Y4)T which follows a four dimensional normal

distribution with mean vector µ = (µ1, µ2, µ3, µ4)T and variance-covariance matrix

i.e., we suppose that the correlation between Y1 and Y2 is the same as the correlation between
Y3 and Y4. Taking into account that Σ must be semi-deﬁnite positive, the following condition
is imposed, − 1
In order to avoid several problems regarding the consistency of the
maximum likelihood estimator of the parameter ρ (cf. Xu and Reid, 2011), we shall consider the
composite likelihood function

5 ≤ ρ ≤ 1
3 .

where

CL(θ, y) = fA1(θ, y)fA2(θ, y),

fA1(θ, y) = f12(µ1, µ2, ρ, y1, y2),
fA2(θ, y) = f34(µ3, µ4, ρ, y3, y4),

13

Σ =

ρ
1
ρ
1
2ρ 2ρ
2ρ 2ρ

2ρ 2ρ
2ρ 2ρ
1
ρ
1
ρ

 ,

(11)

where f12 and f34 are the densities of the marginals of Y , i.e. bivariate normal distributions with
mean vectors (µ1, µ2)T and (µ3, µ4)T , respectively, and common variance-covariance matrix

(cid:18) 1 ρ
ρ 1 (cid:19) ,
expn− 1

1

with expressions given by

being

fh,h+1(µh, µh+1, ρ, yh, yh+1) =

2(1−ρ2) Q(yh, yh+1)o , h ∈ {1, 3},
Q(yh, yh+1) = (yh − µh)2 − 2ρ(yh − µh)(yh+1 − µh+1) + (yh+1 − µh+1)2, h ∈ {1, 3}.

2πp1 − ρ2

In this context, the interest is focused in testing the composite null hypothesis hypotheses

by using the composite φ-divergence test-statistics, presented above. In this case, the parameter
space is given by

H0 : ρ = ρ0

against H1 : ρ 6= ρ0,

(12)

If we consider g : Θ ⊆ R5 −→ R, with

Θ =(cid:8)θ = (µ1, µ2, µ3, µ4, ρ)T : µi ∈ R, i = 1, ..., 4 and − 1
g(θ) = g((µ1, µ2, µ3, µ4, ρ)T ) = ρ − ρ0,

3(cid:9) .
5 ≤ ρ ≤ 1

(13)

the parameter space under the null hypothesis is given by

Θ0 =(cid:8)θ = (µ1, µ2, µ3, µ4, ρ)T ∈ Θ : g(µ1, µ2, µ3, µ4, ρ) = 0(cid:9) .

It is now clear that the dimensions of both parameter spaces are dim(Θ) = 5 and dim(Θ0) = 4.
Consider now a random sample of size n, yi = (yi1, ..., yi4)T , i = 1, ..., n. The maximum composite
likelihood estimators of the parameters µi, i = 1, 2, 3, 4, and ρ in Θ are obtained by standard
maximization of the composite log-density function associated to the random sample of size n,

cℓ(θ, y1, ..., yn) =

=

=

= −

where

nXi=1
nXi=1
nXi=1

n
2

cℓ(θ, yi) =

nXi=1

log CL(θ, yi)

[log fA1(θ, yi) + log fA2(θ, yi)]

ℓA1(θ, yi) +

nXi=1
log(cid:0)1 − ρ2(cid:1) −

ℓA2(θ, yi)

1

2 (1 − ρ2)

(ς 2

1 + ς 2

2 + ς 2

3 + ς 2

4 − 2ρ(ς 12 + ς 34)) + k,

ς 2
j =

ς h,h+1 =

nXi=1
(yij − µj)2, j ∈ {1, 2, 3, 4},
nXi=1

(yih − µh)(yi,h+1 − µh+1), h ∈ {1, 3},

14

and k is a constant no dependent of the unknown parameters, i.e., by solving the system of the
following system of ﬁve equations

(y1 − µ1) − ρ(y2 − µ2) = 0,
(y2 − µ2) − ρ(y1 − µ1) = 0,
(y3 − µ3) − ρ(y4 − µ4) = 0,
(y4 − µ4) − ρ(y3 − µ3) = 0,
= 0,

ς 12 + ς 34

− n(cid:19) ρ −

2

yij, j ∈ {1, 2, 3, 4}.

(14)

nρ3 −

ς 12 + ς 34

2

with

From the ﬁrst two equations we get

1 + ς 2

2 + ς 2

3 + ς 2
4

2

ρ2 +(cid:18) ς 2
nXi=1

yj =

1
n

ρ(y1 − µ1) − ρ2(y2 − µ2) = 0,
−ρ(y1 − µ1) + y2 − µ2 = 0.

5 , 1

(cid:0)1 − ρ2(cid:1) (y2 − µ2) = 0,
and bµ2 = y2.
bµ1 = y1
bµ3 = y3
and bµ4 = y4.
ρ2 +(cid:18)v2

2 + v2

1 + v2

3 + v2
4

2

Therefore

and since we assume that ρ ∈ (− 1

3 ) ⊂ (−1, 1), it is obtained that

In a similar manner, from the third and fourth equations we can get that

cubic equation

The maximum composite likelihood estimator of ρ under Θ,bρ, is the real solution of the following

− 1(cid:19) ρ −

v12 + v34

= 0,

2

ρ3 −

where

v12 + v34

2

v2
j =

vh,h+1 =

1

1
n

j =

nXi=1
nbς 2
(yij − yj)2, j ∈ {1, 2, 3, 4},
nXi=1
nbς h,h+1 =

1
n

1

(yih − yh)(yi,h+1 − yh+1), h ∈ {1, 3},

and v2

j , j ∈ {1, 2, 3, 4}, are sampling variances and vh,h+1, h ∈ {1, 3}, sampling covariances.

Under Θ0, the restricted maximum composite likelihood estimators of the parameters µj, j ∈

{1, 2, 3, 4} are given by,

with yi given by (14). Therefore, in our model, the maximum composite likelihood estimators are

eµj = yj, j ∈ {1, 2, 3, 4},

bθc = (y1, y2, y3, y4,bρ)T andeθrc = (y1, y2, y3, y4, ρ0)T ,

15

under Θ and Θ0 respectively.

After some heavy algebraic manipulations the sensitivity or Hessian matrix H(θ) is given by

H(θ) =

1

1 − ρ2



1 −ρ
−ρ
1
0
0
0 −ρ
0
0
0
0

0
0
0
0
1 −ρ
1
0

0
0
0
0

2 1+ρ2
1−ρ2



.

(15)

In a similar manner, the expression of the variability matrix J(θ) coincides with that of sensitivity
matrix H(θ), i.e. J (θ) = H(θ).

In order to get the unique non zero eigenvalue β1 from Theorem 5, it is necessary to obtain (5),

which, in the present setup, is given by

G(θ) =

∂g(θ)

∂θ

= (0, 0, 0, 0, 1)T ,

(16)

where g(θ) is given by (13) in the context of the present example.
In addition, taking into ac-
count that Q(θ) = −G(θ) and after some algebra, it is concluded that β1 = 1 and therefore the
asymptotic distribution of the composite φ-divergence test-statistics is

under the null hypothesis H0 : ρ = ρ0. In a completely similar manner, the composite likelihood
ratio test, presented in Remark 1, for testing H0 : ρ = ρ0, is

because the only non zero eigenvalue of the asymptotic distribution (7) is equal to one.

In a similar way, if we consider the composite (h, φ)-divergence test-statistics, we have

2n
φ′′(1)

χ2
1,

Dφ(bθc,eθrc) L−→n→∞
Tφ,n(bθc,eθrc) =
λn(bθc,eθrc) = 2(cid:16)cℓ(bθc) − cℓ(eθrc)(cid:17) L−→n→∞
h(cid:16)Dφ(bθc,eθrc)(cid:17) L−→n→∞
Th,φ,n(bθc,eθrc) =

φ′′(1)h′(0)

χ2
1,

2n

χ2
1.

f r
f r−1

I order to apply the above theoretic issues in practice it is necessary to consider a particular convex
function φ in order to get a concrete φ-divergence or to consider φ and h in order to get an (h, φ)-
divergence. Using the R´enyi’s family of divergences, i.e, a family of (h, φ)-divergences with φ and
h given in Table 1, the family of test-statistics is given by

dy1dy2,

2n

4n

For r = 1 we have

T r

T 1

=

r(r − 1)

dy3dy4(cid:19)

f r
f r−1
f r
f r−1

dy1dy2 + logZR2

34(bµ3,bµ4,bρ, y3, y4)
34 (bµ3,bµ4, ρ0, y3, y4)

n(bθc,eθrc) =
for r 6= 0, 1. The last equality follows becuase the integrals does not depend on bµ1,bµ2,bµ3 andbµ4.
n (bθc,eθrc) = 2n(cid:18)Z R2
f12(bµ1,bµ2,bρ, y1, y2)
f12(bµ1,bµ2, ρ0, y1, y2)
dy3dy4(cid:19)
+Z R2
= 4nZ R2

r(r − 1)(cid:18)logZR2
12(bµ1,bµ2,bρ, y1, y2)
12 (bµ1,bµ2, ρ0, y1, y2)
logZR2
12(bµ1,bµ2,bρ, y1, y2)
12 (bµ1,bµ2, ρ0, y1, y2)
f34(bµ3,bµ4,bρ, y3, y4)dy3dy4Z R2
f12(bµ1,bµ2,bρ, y1, y2)dy1dy2Z R2
f12(bµ1,bµ2,bρ, y1, y2) log

f34(bµ3,bµ4,bρ, y3, y4) log
f12(bµ1,bµ2,bρ, y1, y2)
f12(bµ1,bµ2, ρ0, y1, y2)

f34(bµ3,bµ4,bρ, y3, y4)
f34(bµ3,bµ4, ρ0, y3, y4)

f12(bµ1,bµ2,bρ, y1, y2) log

dy1dy2

16

dy1dy2

Based on the expression for R´enyi divergence in normal populations (for more details see Pardo,
2006, p. 33) we have

T r

n(bθc,eθrc) =

Similarly, using the Cressie-Read’s family of divergences, the family of test-statistics is given by

0

2n

T 0

(1 − ρ2

r−1 ρ0 − 1
|r−1|
r−1 ρ0 − 1
|r−1|

r(r−1) log
+∞,

n (bθrc,eθc).
r /∈ {0, 1}, bρ ∈(cid:16) r
r /∈ {0, 1}, bρ /∈(cid:16) r

n (bθc,eθrc) = T 1
0)r(1 −bρ2)−(r−1)
1 − [rρ0 + (1 − r)bρ]2 ,
0 (cid:17) ,
1−bρ2 + 2 ρ0(ρ0−bρ)
1−ρ2
1−bρ2 (cid:17) ,
bρ(bρ−ρ0)
1−ρ2
1 − [(λ + 1)ρ0 − λbρ]2 − 1! , λ /∈ {0,−1}, bρ ∈(cid:16) λ+1
0)λ+1(1 −bρ2)−λ
λ /∈ {0,−1}, bρ /∈(cid:16) λ+1
0 (cid:17) = T 1
1−bρ2 (cid:17) = T 0

2n(cid:16)log 1−ρ2
2n(cid:16)log 1−bρ2
λ(λ+1) s (1 − ρ2
2n(cid:16)log 1−ρ2
2n(cid:16)log 1−bρ2

λ = −1.

+∞,

r = 1,

r = 0.

+ 2

4n

0

0




T λ

n (bθc,eθrc) =

0

+ 2

1−bρ2 + 2 ρ0(ρ0−bρ)
1−ρ2
bρ(bρ−ρ0)
1−ρ2

n (bθc,eθrc), λ = 0,
n (bθc,eθrc),
λn(bθc,eθrc) = 2(cid:16)cℓ(bθc, y1, ..., yn) − cℓ(eθrc, y1, ..., yn)(cid:17)
4)(cid:18) 1

= 2n(cid:20)log

1 − ρ2

1 + v2

3 + v2

2 + v2

0 −

1 − ρ2

0

1 −bρ2 + (v2

(18)
After some algebra we can also obtain the composite likelihood ratio test. This has the following
expression

,

,

r

|r−1|(cid:17) ,
r−1 ρ0 + 1
|r−1|(cid:17) ,
r−1 ρ0 + 1

r

(17)

λ ρ0 − 1
|λ|
λ ρ0 − 1
|λ|

, λ+1

λ ρ0 + 1

, λ+1

λ ρ0 + 1

|λ|(cid:17) ,
|λ|(cid:17) ,

1

1 −bρ2(cid:19) − 2(v12 + v34)(cid:18) ρ0

1 − ρ2

1 −bρ2(cid:19)(cid:21) .
0 − bρ

(19)

7 Simulation study

In this section a simulation study is presented in order to study the behavior of the composite
φ-divergence test-statistics. The theoretical model studied in the previous section is followed by
using the composite Cressie-Read test-statistics (18). The composite likelihood ratio test-statistic
(CLRT), given in (19), is also considered. A special attention has been paid to the hypothesis
testing (12) with ρ0 ∈ {−0.1, 0.2}. The case ρ0 = 0 has been considered, but this case is less
important since taking into account the way of the theoretical model under consideration and
having the case of independent observations, the composite likelihood theory is useless. For ﬁnite
sample sizes and nominal size α = 0.05, the estimated signiﬁcance level for diﬀerent composite
Cressie-Read test-statistics as well as for the CLRT, are given by

α(λ)
n (ρ0) = Pr(T λ

1,0.05|H0), λ ∈ R and α(CLRT )

n

n (bθc,eθrc) > χ2

More thorougly, the composite Cressie-Read test-statistics with λ ∈ {−1,−0.5, 0.2/3, 1, 1.5} have
beed selected for the study. Following Dale (1986), we consider the inequality

(ρ0) = Pr(cid:16)λn(bθc,eθrc) > χ2

1,0.05|H0(cid:17) .

(cid:12)(cid:12)(cid:12)logit(1 − α(•)

n ) − logit(1 − α)(cid:12)(cid:12)(cid:12) ≤ ε

17

(20)

where logit(p) = ln(p/ (1 − p)). By chosing ε = 0.45 the composite test-statistics valid for the
study are limited to those verifying α(λ)
n ∈ (0.0325, 0.07625). This criterion has been used in many
previous studies, see for instance Cressie et al (2003), Mart´ın et al. (2014), Mart´ın and Pardo
(2012) and references therein.

Through R = 10, 000 replications of the simulation experiment, with the model under the null
hypothesis, the estimated signiﬁcance level for diﬀerent composite Cressie-Read test-statistics are

n (ρ0) =cPr(T λ
bα(λ)

n (bθc,eθrc) > χ2

1,0.05|H0) =

with I(S) being and indicator function (with value 1 if S is true and 0 otherwise) and the estimated
signiﬁcance level for CLRT

RXi=1

I(T λ

n,i(bθc,eθrc) > χ2

R

1,0.05|ρ0)

,

n

bα(CLRT )

(ρ0) =cPr(λn(bθc,eθrc) > χ2

1,0.05|H0) =

RXi=1

I(λn(bθc,eθrc) > χ2

R

1,0.05|ρ0)

.

In Table 1 we present the simulated level for diﬀerent values of λ ∈ {−1,−0.5, 0.2/3, 1, 1.5}
as well as for the CLRT, when n = 100, n = 200 and n = 300 for ρ0 = −0.1 and ρ0 = 0.2.
In order to investigate the behavior for ρ0 = 0 we present in Table 2 the simulated level for λ
∈ {−1,−0.5, 0.2/3, 1, 1.5} as well as the simulated level of CLRT for n = 50, n = 100, n = 200
and n = 300. Clearly, as expected the performance of the traditional divergence and likelihood
methods is stronger in comparison with the composite divergence and likelihood methods.

n = 100

n = 200

n = 300

CLRT
λ = −1
λ = −0.5
λ = 0
λ = 2/3

λ = 1
λ = 1.5

ρ0 = −0.1 ρ0 = 0.2 ρ0 = −0.1 ρ0 = 0.2 ρ0 = −0.1 ρ0 = 0.2
0.0662
0.0688
0.0756
0.0685
0.0670
0.0738
0.0672
0.0725
0.0677
0.0726
0.0739
0.0680
0.0677
0.0762

0.0673
0.0706
0.0697
0.0691
0.0694
0.0700
0.0711

0.0645
0.0666
0.0662
0.0659
0.0662
0.0662
0.0674

0.0687
0.0740
0.0727
0.0720
0.0719
0.0720
0.0729

0.0694
0.0762
0.0746
0.0739
0.0739
0.0747
0.0769

Table 1: Simulated signiﬁcance level for ρ0 = −0.1 and ρ0 = 0.2.

n = 50 n = 100 n = 200 n = 300
LRT
0.0526
0.0543 0.0529
λ = −1
0.0542
0.0707 0.0605
λ = −0.5 0.0677 0.0594
0.0540
0.0540
λ = 0
0.0659 0.0577
0.0540
λ = 2/3 0.0670 0.0591
0.0541
0.0686 0.0597
λ = 1
λ = 1.5
0.0726 0.0610
0.0544

0.0527
0.0559
0.0553
0.0552
0.0552
0.0553
0.0564

Table 2: Simulated signiﬁcance level for ρ0 = 0.

For ﬁnite sample sizes and nominal size α = 0.05, the simulated powers are obtained under H1
in (12), when ρ ∈ {−0.2,−0.15, 0, 0.1} and ρ0 = −0.1 (Table 3) and when ρ ∈ {0, 0.15, 0.25, 0.3}

18

and ρ0 = 0.2 (Table 4). The (simulated) power for diﬀerent composite Cressie-Read test-statistics
is obtained by

β(λ)
n (ρ0, ρ) = Pr(T λ

and for the CLRT by

n (bθc,eθrc) > χ2

1,0.05|H1) andbβ

RXi=1

(λ)
n (ρ0, ρ) =

I(T λ

n (bθc,eθrc) > χ2

R

1,0.05|ρ0, ρ)

,

(CLRT )
n

(ρ0, ρ) =

RXi=1

I(λn(bθc,eθrc) > χ2

R

1,0.05|ρ0, ρ)

.

β(CLRT )

n

(ρ0, ρ) = Pr(λn(bθc,eθrc) > χ2

1,0.05|H1) andbβ

Among the composite test-statistics with simulated signiﬁcance levels verifying (20), at ﬁrst sight
the composite test-statistics with higher powers should be selected however since in general high
powers correspond to high signiﬁcance levels, this choice is not straighforward. For this reason,
based on βLRT
as baseline, the eﬃciencies relative to the composite likelihood ratio test,
given by

n − αLRT

n

e(λ)
n =

(β(λ)
n − α(λ)
n ) − (βLRT
βLRT
n − αLRT

n

n − αLRT

n

)

, λ ∈ {−1,−0.5, 0.2/3, 1, 1.5} ,

were considered for n = 100, n = 200 and n = 300. Only the values of the power for λ = −1/2 are
included in Tables 3 and 4, in order to show that the corresponding composite test-statistic is a
good alternative to the composite likelihood ratio test-statistic. The values of the powers for which
the values of e(−1/2)
are positive, i.e., the case in which the composite test-statistic associated to
λ = −1/2 is better that the composite likelihood ratio test, are shown in bold in Tables 3 and 4.
This choice of λ = −1/2 divergence based test-statistic has been also recommended in Morales et
al. (1997) and Mart´ın et al. (2016).

n

n = 100 CLRT

ρ = −0.2 ρ = −0.15 ρ = 0 ρ = 0.1
0.1604
0.3584
0.2993 0.7958
λ = −1/2 0.3751
0.1750 0.3057 0.8076
0.2227
0.5455
0.5087 0.9705
λ = −1/2 0.5512
0.2322 0.5114 0.9737
0.2705
0.8087 0.9962
0.7770
λ = −1/2 0.7797
0.2795 0.8112 0.9970

n = 200 CLRT

n = 300 CLRT

Table 3: Simulated powers for ρ0 = −0.1.

n = 100 CLRT

0.1227

0.1534

ρ = 0 ρ = 0.15 ρ = 0.25 ρ = 0.3
0.3689
0.8054
λ = −1/2 0.8118 0.1305 0.1602 0.3806
0.2146
0.5818
0.9813
λ = −1/2 0.9825
0.2194 0.5957
0.2870
0.9978
0.7482
λ = −1/2 0.9979
0.2935 0.7612

0.1904
0.1920
0.2591
0.2577

n = 200 CLRT

n = 300 CLRT

Table 4: Simulated powers for ρ0 = 0.2.

19

8 Conclusions

This paper presents the theoretical background for the development of statistical tests for testing
composite hypotheses when the composite likelihood is used instead of the classic likelihood of
the data. The test statistic is based on the notion of phi-divergence and its by products, that is
measures of the statistical distance between the theoretical model and the respective empirical one.
The notion of divergence or disparity provides with abstract methods of estimation and testing
and four monographs, mentioned in the introductory section, developed the state of the art on this
subject.

This work is the ﬁrst, to the best of our knowledge, which try to link the notion of composite
likelihood with the notion of divergence between theoretical and empirical models for testing hy-
potheses. There are several extensions to this framework which can be considered. The theoretical
framework, presented here, would be extended to develop statistical tests for testing homogeneity
of two or more populations on the basis of composite likelihood. On the other hand, minimum
phi-divergence or disparity procedures have been observed to provide strong robustness properties
in estimation and testing problems. It would be maybe of interest to proceed in this direction in a
composite likelihood setting.

9 Appendix

9.1 Proof of Theorem 3

Following Sen and Singer (1993, p. 242-3), let θn = θ + n−1/2v, where kvk < K∗, 0 < K∗ < ∞.
Consider now the following Taylor expansion of the partial derivative of the composite log-density,

=

1
√n

∂
∂θ

cℓ(θ,yi)+

1
n

nXi=1

nXi=1

∂2

∂θ∂θT cℓ(θ,yi)(cid:12)(cid:12)(cid:12)(cid:12)θ=θ∗

n

√n (θn − θ) ,

(21)

where θ∗n belongs to the line segment joining θ and θn. Then, observing that (cf. Theorem 2.3.6
of Sen and Singer, 1993, p. 61)

1
√n

nXi=1

∂
∂θ

cℓ(θ,yi)(cid:12)(cid:12)(cid:12)(cid:12)θ=θn
nXi=1

∂2

∂θ∂θT cℓ(θ,yi)

1
n

equation (21) leads

Eθ(cid:20)

P−→n→∞

∂2

∂θ∂θT cℓ(θ,Y )(cid:21) = Eθ(cid:20) ∂

∂θ

uT (θ,Y )(cid:21) = −H(θ),

∂
∂θ

nXi=1

cℓ(θ,yi)+G(θ)λ = 0p,

g(θ) = 0r,

20

1
√n

∂
∂θ

nXi=1

cℓ(θ,yi)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)θ=θn

=

1
√n

nXi=1

∂
∂θ

cℓ(θ,yi)−H(θ)√n (θn − θ) + oP (1).

Since G(θ) = ∂gT (θ)

∂θ

is continuous in θ, it is true that,

g(θn) = GT (θ)√n (θn − θ) + oP (1).

Since, the restricted maximum composite likelihood estimator eθrc should satisfy the likelihood

equations

(22)

(23)

and in view of (22) and (23) it holds that

In matrix notation it may be re-expressed as

1√n

nXi=1

0r

(cid:18) H(θ) −G(θ)
−GT (θ)

1
√n

∂
∂θ

nXi=1

∂
∂θ cℓ(θ,yi)

1
√n

λn + oP (1) = 0p,

cℓ(θ,yi)−H(θ)√n(cid:16)eθrc − θ(cid:17) + G(θ)
n−1/2λn (cid:19) =
0r×r (cid:19)(cid:18) √n(eθrc − θ)
QT (θ) R(θ) (cid:19)
n−1/2λn (cid:19) =(cid:18) P (θ) Q(θ)
(cid:18) √n(eθrc − θ)

GT (θ)√n(eθrc − θ) + oP (1) = 0p.
 + oP (1).
 + oP (1),

nXi=1
0r×r (cid:19)−1
QT (θ) R(θ) (cid:19) =(cid:18) H(θ) −G(θ)
(cid:18) P (θ) Q(θ)
P (θ) = H−1(θ)(cid:16)Ip − G(θ)(cid:0)GT (θ)H−1(θ)G(θ)(cid:1)−1
R(θ) = −(cid:0)GT (θ)H−1(θ)G(θ)(cid:1)−1

Q(θ) = −H−1(θ)G(θ)(cid:0)GT (θ)H−1(θ)G(θ)(cid:1)−1

GT (θ)H−1(θ)(cid:17) ,

−GT (θ)

∂
∂θ cℓ(θ,yi)

1√n

0r

.

,

.

This last equation implies (cf. Sen and Singer, 1993, p. 243, eq. (5.6.24)),

Based on the central limit theorem (Theorem 3.3.1 of Sen and Singer, 1993, p. 107) and the
Cram´er-Wold theorem (Theorem 3.2.4 of Sen and Singer, 1993, p. 106) it is obtained

(24)

Then

where

with

or

Therefore,

with V arθ[u(θ,Y )] = J (θ). Then, it follows from (24) that

1
√n

∂
∂θ

cℓ(θ,yi) L−→n→∞ N (0p, V arθ[u(θ,Y )]) .

nXi=1
(cid:18) √n(eθrc − θ)
n−1/2λn (cid:19) L−→n→∞ N (0, Σ) ,

Σ =(cid:18) P (θ) Q(θ)

0r×p 0r×r (cid:19)(cid:18) P T (θ) QT (θ)
QT (θ) R(θ) (cid:19)(cid:18) J(θ) 0p×r
Q(θ) RT (θ) (cid:19) ,
Σ =(cid:18) P (θ)J (θ)P T (θ)
QT (θ)J (θ)P T (θ) QT (θ)J (θ)QT (θ) (cid:19) .
√n(eθrc − θ) L−→n→∞ N(cid:0)0p, P (θ)J(θ)P T (θ)(cid:1) ,

P (θ)J (θ)QT (θ)

21

with

P (θ) = H−1(θ)(cid:16)Ip − G(θ)(cid:0)GT (θ)H−1(θ)G(θ)(cid:1)−1
= H−1(θ) − H−1(θ)G(θ)(cid:0)GT (θ)H−1(θ)G(θ)(cid:1)−1

= H−1(θ) + Q(θ)GT (θ)H−1(θ),

GT (θ)H−1(θ)(cid:17)

GT (θ)H−1(θ)

and the proof of the lemma is now completed.

9.2 Proof of Lemma 4

Based on equation (24), above,

The Taylor series expansion (21) gives that

0 =

1
√n

∂
∂θ

nXi=1

or

√n(eθrc − θ) = P (θ)
cℓ(θ,yi)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)θ=bθc
nXi=1
nXi=1

cℓ(θ,yi) = −

1
√n

∂
∂θ

1
n

∂
∂θ

=

nXi=1

1
√n

1
√n

∂
∂θ

nXi=1

cℓ(θ,yi) + oP (1).

(25)

cℓ(θ,yi)+

1
n

nXi=1

∂2

∂θ∂θT cℓ(θ,yi)(cid:12)(cid:12)(cid:12)(cid:12)θ=θ∗

n

∂2

∂θ∂θT cℓ(θ,yi)(cid:12)(cid:12)(cid:12)(cid:12)θ=θ∗
√n(bθc − θ),

n

√n(bθc − θ),

where θ∗n belongs to the line segment joining θ andbθc. Taking into account Theorem 2.3.6 of Sen

and Singer (1993, p. 61),

and the above two equations lead

1
n

nXi=1

1
√n

nXi=1

∂
∂θ

n

∂2

P−→n→∞ −H(θ),

∂θ∂θT cℓ(θ,yi)(cid:12)(cid:12)(cid:12)(cid:12)θ=θ∗
cℓ(θ,yi) = H(θ)√n(bθc − θ) + oP (1).
nXi=1

cℓ(θ,yi) + oP (1)

∂
∂θ

Equations (25), (26) and the fact that P (θ) = H−1(θ) + Q(θ)GT (θ)H−1(θ) give that

√n(eθrc − θ) = P (θ)

1
√n

= P (θ)H(θ)√n(bθc − θ) + oP (1)
=(cid:0)H−1(θ) + Q(θ)GT (θ)H−1(θ)(cid:1) H(θ)√n(bθc − θ) + oP (1),

which completes the proof of the lemma.

(26)

22

9.3 Proof of Theorem 5

+

1
2

∂
∂θ
∂2

A second order Taylor expansion of Dφ(bθc,eθrc), considered as a function ofbθc, aroundeθrc, gives

Dφ(θ,eθrc)(cid:12)(cid:12)(cid:12)(cid:12)θ=eθrc
∂θ∂θT Dφ(θ,eθrc)(cid:12)(cid:12)(cid:12)(cid:12)θ=eθrc

Dφ(bθc,eθrc) = Dφ(bθc,eθrc) +
(bθc −eθrc)
(bθc −eθrc)T
(bθc −eθrc) + o(kbθc −eθrck2).
∂θ Dφ(θ,eθrc)(cid:12)(cid:12)(cid:12)θ=eθrc
Based on Pardo (2006, p. 411-412), we obtain Dφ(eθrc,eθrc) = 0,
∂θ∂θT Dφ(θ,eθrc)(cid:12)(cid:12)(cid:12)θ=eθrc
= φ′′(1)J (eθrc). Then, the above equation leads
Dφ(bθc,eθrc) = √n(bθc −eθrc)T J (eθrc)√n(bθc −eθrc) + no(kbθc −eθrck2),
Tφ,n(bθc,eθrc) = √n(bθc −eθrc)T J (eθrc)√n(bθc −eθrc) + no(kbθc −eθrck2).

On the other hand (cf., Pardo, 2006, p. 63),

2n
φ′′(1)

or

∂

= 0p and

(27)

∂ 2

no(||bθc −eθrc||2) ≤ no(kbθc − θk2) + no(||eθrc − θ||2),

and no(kbθc − θk2) = oP (1), no(||eθrc − θ||2) = oP (1). Therefore, o(||bθc −eθrc||2) = oP (1). To apply

the Slutsky’s theorem, it remains to obtain the asymptotic distribution of the quantity

√n(bθc −eθrc)T J (eθrc)√n(bθc −eθrc).

From Lemma 4 it is immediately obtained that

On the other hand, we know that

√n(bθc −eθrc) = Q(θ)GT (θ)√n(bθc − θ) + oP (1).

Therefore,

√n(bθc − θ) L−→n→∞ N(cid:0)0p, G−1
∗ (θ)(cid:1) .
√n(bθc −eθrc) L−→n→∞ N(cid:0)0p, G(θ)QT (θ)G−1
∗ (θ)Q(θ)GT (θ)(cid:1) ,

and taking into account (27) and Corollary 2.1 of Dik and de Gunst (1985), Tφ,n(fbθc

) converge
i , where βi, i = 1, ..., k, are the eigenvalues of the matrix

, feθrc

J (θ)G(θ)QT (θ)G−1

i=1 βiZ 2
∗ (θ)Q(θ)G(θ)T and

in law to the random variablePk
k = rank(cid:0)G(θ)QT (θ)G−1

∗ (θ)Q(θ)GT (θ)J (θ)G(θ)QT (θ)G−1

∗ (θ)Q(θ)GT (θ)(cid:1) .

Acknowledgments. The third author wants to cordialy thank Prof. Alex de Leon, from the
University of Calgary, for fruitful discussions on composite likelihood methods, some years ago.
This research is partially supported by Grants MTM2012-33740 from Ministerio de Economia y
Competitividad (Spain).

23

References

[1] Basu, A., Shioya, H. and Park, C. (2011). Statistical inference. The minimum distance ap-

proach. Chapman & Hall/CRC. Boca Raton.

[2] Basu, A., Mandal, A., Martin, N. and Pardo, L. Density Power Divergence Tests for Composite

Null Hypotheses. arXiv: 1403.0330v3.

[3] Cattelan, M. and Sartiri, N. (2016). Empirical and simulated adjustments of composite likeli-

hood ratio statistics. Journal of Statistical Computation and Simulation, 86, 1056–1067.

[4] Cressie, N., Pardo, L. and Pardo, M.C. (2003) Size and power considerations for testing log-

linear models using φ-divergence test statistics, Statistica. Sinica, 13, 550–557.

[5] Cressie, N. and Read, T. R. C. (1984). Multinomial goodness-of-ﬁt tests. J. Roy. Statist. Soc.

Ser. B 46, 440-464.

[6] Csisz´ar, I. (1963). Eine informationstheoretische Ungleichung und ihre Anwendung auf den
Beweis der Ergodizitat von Markoﬀschen Ketten. Magyar Tud. Akad. Mat. Kutato Int. Kozl.,
8, 85–108.

[7] Csisz´ar, I. (1967). Information-type measures of diﬀerence of probability distributions and

indirect observations. Studia Scientiarum Mathematicarum Hungarica, 2, 299–318.

[8] Dale, J. R. (1986). Asymptotic normality of goodness-of-ﬁt statistics for sparse product multi-

nomials. Journal of the Royal Statistical Society, Series B, 48, 48–59.

[9] Dic, J. J. and Gunst, M. C. M. (1985). The distribution of general quadratic forms in normal

variables. Statistica Neerlandica, 39, 14-26.

[10] Joe, H., Reid, N., Somg, P. X., Firth, D. and Varin, C. (2012). Composite Like-
lihood Methods. Report on the Workshop on Composite Likelihood. Available at
http://www.birs.ca/events/2012/5-day-workshops/12w5046.

[11] Klein, A. M´elard, G. and Zahaf, T. (1998). Computation of the exact information matrix of

Gaussian dynamic regression time series models. The Annals of Statistics, 26, 1636–1650.

[12] Kullback, S. and Leibler, R. A. (1951). On information and suﬃciency. Ann. Math. Statistics,

22, 79-86.

[13] Kullback, S. (1959). Information Theory and Statistics. Wiley, New York.

[14] Liese, F. and Vajda, I. (1987). Convex statistical distances. Teubner Texts in Mathematics,

Leipzig.

[15] Liese, F. and Vajda, I. (2006). On divergences and informations in statistics and information

theory. .IEEE Transactions on Information Theory, 52, 4394-4412.

[16] Lindsay, G. (1988). Composite likelihood methods. Contemporary Mathematics, 80, 221-239.

[17] Martin, N. and Pardo, L. (2012). Poisson-loglinear modeling with linear constraints on the

expected cell frequencies. Sankhya B, 74, 238–267.

24

[18] Martin, N., Mata, R. and Pardo, L. (2014). Phi-divergence statistics for the likelihood ratio
order: an approach based on log-linear models. Journal of Multivariate Analysis,. 130, 387–
408.

[19] Martin, N., Mata, R. and Pardo, L. (2016). Wald type and phi-divergence based test-statistics

for isotonic binomial proportions. Mathematics and Computers in Simulation, 120, 31–49.

[20] Men´endez, M. L., Morales, D., Pardo, L. and Salicr´u, M. (1995). Asymptotic behavior and
statistical applications of divergence measures in multinomial populations: A uniﬁed study.
Statistical Papers, 36, 1–29.

[21] Men´endez, M. L., Pardo, J. A., Pardo, L. and Pardo, M. C. (1997). Asymptotic approximations
for the distributions of the (h, φ)-divergence goodness-of-ﬁt statistics: Applications to R´enyi’s
statistic. Kybernetes, 26, 442–452.

[22] Morales, D., Pardo, L. and Vajda, I. (1997). Some New Statistics for Testing Hypotheses in

Parametric Models. Journal of Multivariate Analysis, 62, 137 - 168.

[23] Pardo, L. (2006). Statistical inference based on divergence measures. Chapman & Hall/CRC.

Boca Raton.

[24] Read, T. R. C. and Cressie, N. (1988). Goodness-of-ﬁt Statistics for Discrete Multivariate data.

Springer-Verlag. New York.

[25] Reid, N. (2013). Aspects of likelihood inference. Bernoulli 19, 1404-1418.

[26] Reid, N., Lindsay, B. and Liang, K.-Y. (2011). Introduction to Special Issue. Statistica Sinica,

21, 1-3.

[27] R´enyi, A. (1960). On measures of entropy and information. Proceedings of the 4th Berkeley

Symposium on Mathematical Statistics and Probability, I., Berkeley, 547-561.

[28] Sen, P. K. and Singer, J. M. (1993). Large Sample Methods in Statistics. Chapman &

Hall/CRC. New York.

[29] Serﬂing, R. J. (1980). Aproximation Theorems of Mathematical Statistics. Wiley. New York.

[30] Sharma, B. D. and Mittal, D. P. (1997). New non-additive measures of relative information.

Journal of Combinatorics, Information & Systems Science, 2, 122–133.

[31] Stummer, W. and Vajda, I. (2010). On divergences of ﬁnite measures and their applicability

in statistics and information theory. Statistics, 44, 169-187.

[32] Vajda, I. (1989). Theory of statistical inference and information. Kluwer Academic Publishers.

Dordrecht.

[33] Xu, X. and Reid, N. (2011). On the robustness of maximum composite estimate. Journal of

Statistical Planning and Inference, 141, 3047-3054.

[34] Zografos, K. (1998). f -dissimilarity of several distributions in testing statistical hypotheses,

Ann. Inst. Statist. Math. 50, 295–310.

25

