Noname manuscript No.
(will be inserted by the editor)

Convolutional Patch Representations for Image Retrieval:
an Unsupervised Approach

Mattis Paulin · Julien Mairal · Matthijs Douze · Zaid Harchaoui ·
Florent Perronnin · Cordelia Schmid

6
1
0
2

 
r
a

M
1

 

 
 
]

V
C
.
s
c
[
 
 

1
v
8
3
4
0
0

.

3
0
6
1
:
v
i
X
r
a

the date of receipt and acceptance should be inserted later

Abstract Convolutional neural networks (CNNs) have
recently received a lot of attention due to their ability to
model local stationary structures in natural images in
a multi-scale fashion, when learning all model parame-
ters with supervision. While excellent performance was
achieved for image classiﬁcation when large amounts of
labeled visual data are available, their success for un-
supervised tasks such as image retrieval has been mod-
erate so far.

Our paper focuses on this latter setting and ex-
plores several methods for learning patch descriptors
without supervision with application to matching and
instance-level retrieval. To that eﬀect, we propose a new
family of convolutional descriptors for patch represen-
tation, based on the recently introduced convolutional
kernel networks. We show that our descriptor, named
Patch-CKN, performs better than SIFT as well as other
convolutional networks learned by artiﬁcially introduc-
ing supervision and is signiﬁcantly faster to train. To
demonstrate its eﬀectiveness, we perform an extensive
evaluation on standard benchmarks for patch and im-
age retrieval where we obtain state-of-the-art results.
We also introduce a new dataset called RomePatches,
which allows to simultaneously study descriptor perfor-
mance for patch and image retrieval.

Keywords Low-level image description, Instance-level
retrieval, Convolutional Neural Networks.

1 Introduction

This paper explores convolutional architectures as

robust visual descriptors for image patches and

Thoth team, Inria Grenoble Rhone-Alpes, Laboratoire Jean
Kuntzmann, CNRS, Univ. Grenoble Alpes, France.

evaluates them in the context of patch and image re-
trieval. We explore several
levels of supervision for
training such networks, ranging from fully supervised
to unsupervised. In this context, requiring supervision
may seem unusual since data for retrieval tasks typ-
ically does not come with labels. Convolutional Neu-
ral Networks (CNNs) have achieved state-of-the-art in
many other computer vision tasks, but require abun-
dant labels to learn their parameters. For this reason,
previous work with CNN architectures on image re-
trieval have focused on using global (Babenko et al
2014) or aggregated local (Razavian et al 2014; Gong
et al 2014; Ng et al 2015; Babenko and Lempitsky 2015;
Tolias et al 2015) CNN descriptors that were learned
on an unrelated classiﬁcation task. To improve the per-
formance of these transferred features, Babenko et al
(2014) showed that ﬁne-tuning global descriptors on a
dataset of landmarks results in improvements on re-
trieval datasets that contain buildings. It is unclear,
however, if the lower levels of a convolutional architec-
ture, appropriate for a local description, will be im-
pacted by such a global ﬁne-tuning (Yosinski et al
2014). Recent approaches have, thus, attempted to dis-
criminatively learn low-level convolutional descriptors,
either by enforcing a certain level of invariance through
explicit transformations (Fischer et al 2014) or by train-
ing with a patch-matching dataset (Zagoruyko and Ko-
modakis 2015; Simo-Serra et al 2015). In all cases, the
link between the supervised classiﬁcation objective and
image retrieval is artiﬁcial, which motivates us to in-
vestigate the performance of new unsupervised learning
techniques.

To do so, we propose an unsupervised patch
descriptor based on Convolutional Kernel Networks
(CKNs)(Mairal et al 2014b). This required to turn the
CKN proof of concept of Mairal et al (2014b) into a

2

Mattis Paulin, Julien Mairal, Matthijs Douze, Zaid Harchaoui, Florent Perronnin, Cordelia Schmid

Fig. 1: Proposed image retrieval pipeline. Interest points are extracted with the Hessian-aﬃne detector (left),
encoded in descriptor space (middle), and aggregated into a compact representation using VLAD-pooling (right).
The main focus of the paper lies in the choice of the patch-level visual descriptor.

descriptor with state-of-the-art performance on large-
scale benchmarks. This paper introduces signiﬁcant im-
provements of the original model, algorithm, and imple-
mentation, as well as adapting the approach to image
retrieval. Our conclusion is that supervision might not
be necessary to train convolutional networks for im-
age and patch retrieval, since our unsupervised descrip-
tor achieves the best performance on several standard
benchmarks.

One originality of our work is also to jointly eval-
uate our models on the problems of patch and im-
age retrieval. Most works that study patch representa-
tions (Brown et al 2011; Winder et al 2009; Zagoruyko
and Komodakis 2015; Fischer et al 2014), do so in the
context of patch retrieval only, and do not test whether
conclusions also generalize to image retrieval (typically
after an aggregation step). In fact, the correlation be-
tween the two evaluation methods (patch- and image-
level) is not clear beforehand, which motivated us to
design a new dataset to answer this question. We call
this dataset RomePatches; it consists of views of sev-
eral locations in Rome (Li et al 2010), for which a sparse
groundtruth of patch matches is obtained through 3D
reconstruction. This results in a dataset for patch and
image retrieval, which enables us to quantify the per-
formance of patch descriptors for both tasks.

To evaluate the descriptor performance, we adopt
the following pipeline (see Fig. 1), described in detail in
section 6.1.1. We use the popular Hessian-Aﬃne detec-
tor of Mikolajczyk and Schmid (2004), which has been
shown to give state-of-the-art results (Tuytelaars and
Mikolajczyk 2008). The regions around these points are
encoded with the convolutional descriptors proposed
in this work. We aggregate local features with VLAD-
pooling (J´egou et al 2010) on the patch descriptors to
build an approximate matching technique. VLAD pool-

ing has been shown to be better than Bag-of-Word,
and to give similar performance to Fisher Vectors (Per-
ronnin and Dance 2007), another popular technique for
image retrieval (J´egou et al 2012).

A preliminary version of this article has appeared
in (Paulin et al 2015). Here we extend the related
work (Section 2) and describe in detail the convolu-
tional kernel network (CKN), in particular its reformu-
lation which leads to a fast learning algorithm (Sec-
tion 4). We also add a number of experiments (see Sec-
tion 6). We compare to two additional supervised local
CNN descriptors. The ﬁrst one is based on the network
of Fischer et al (2014), but trained on our RomePatches
dataset. The second one is trained with the Siamese
architecture of Zagoruyko and Komodakis (2015). We
also compare our approach to an AlexNet network ﬁne-
tuned on the Landmarks dataset. Furthermore, we pro-
vide an in-depth study of PCA compression and whiten-
ing, standard post-processing steps in image retrieval,
which further improve the quality of our results. We also
show that our method can be applied to dense patches
instead of Hessian-Aﬃne, which improves performance
for some of the benchmarks.

The remainder of the paper is organized as fol-
lows. We discuss previous work that is most relevant
to our approach in Section 2. We describe the frame-
work for convolutional descriptors and convolutional
kernel networks in Sections 3 and 4. We introduce the
RomePatches dataset as well as standard benchmarks
for patch and image retrieval in Section 5. Section 6
describes experimental results.

2 Related Work

In this section we ﬁrst review the state of the art for
patch description and then present deep learning ap-

Keypoint detectionPatch descriptionAggregationHessian-affineDeep NetworkVLADConvolutional Patch Representations for Image Retrieval: an Unsupervised Approach

3

proaches for image and patch retrieval. For deep patch
descriptors, we ﬁrst present supervised and, then, un-
supervised approaches.

2.1 Patch descriptors

A patch is a image region extracted from an im-
age. Patches can either be extracted densely or at
interest points. The most popular patch descriptor
is SIFT (Lowe 2004), which showed state-of-the-art
performanceMikolajczyk and Schmid (2005) for patch
matching. It can be viewed as a three-layer CNN, the
ﬁrst layer computing gradient histograms using convo-
lutions, the second, fully-connected, weighting the gra-
dients with a Gaussian, and the third pooling across
a 4x4 grid. Local descriptors that improve SIFT in-
clude SURF (Bay et al 2006), BRIEF (Calonder et al
2010) and LIOP (Wang et al 2011). Recently, Dong
and Soatto (2015) build on SIFT using local pooling on
scale and location to get state-of-the-art performance
in patch retrieval.

All these descriptors are hand-crafted and their rel-
atively small numbers of parameters have been opti-
mized by grid-search. When the number of parameters
to be set is large, such an approach is unfeasible and
the optimal parametrization needs to be learned from
data.

A number of approaches learn patch descriptors
without relying on deep learning. Most of them use a
strong supervision. Brown et al (2011) (see also Winder
et al (2009)) design a matching dataset based on 3D
models of landmarks and use it to train a descrip-
tor consisting of several existing parts, including SIFT,
GLOH (Mikolajczyk and Schmid 2005) and Daisy (Tola
et al 2010). Philbin et al (2010) learn a Mahalanobis
metric for SIFT descriptors to compensate for the bi-
narization error, with excellent results in instance-level
retrieval. Simonyan et al (2014) propose the “Pooling
Regions” descriptor and learn its parameters, as well as
a linear projection using stochastic optimization. Their
learning objective can be cast as a convex optimization
problem, which is not the case for classical convolu-
tional networks.

An exception that departs from this strongly super-
vised setting is (Bo et al 2010) which presents a match-
kernel interpretation of SIFT, and a family of kernel de-
scriptors whose parameters are learned in an unsuper-
vised fashion. The Patch-CKN we introduce generalizes
kernel descriptors; the proposed procedure for comput-
ing an explicit feature embedding is faster and simpler.

2.2 Deep learning for image retrieval

If a CNN is trained on a suﬃciently large labeled set
such as ImageNet (Deng et al 2009), its intermediate
layers can be used as image descriptors for a wide va-
riety of tasks including image retrieval (Babenko et al
2014; Razavian et al 2014). The output of one of the
fully-connected layers is often chosen because it is com-
pact, usually 4,096-dim. However, global CNN descrip-
tors lack geometric invariance (Gong et al 2014), and
produce results below the state of the art for instance-
level image retrieval.

In (Razavian et al 2014; Gong et al 2014), CNN re-
sponses at diﬀerent scales and positions are extracted.
We proceed similarly, yet we replace the (coarse) dense
grid with a patch detector. There are important diﬀer-
ences between (Razavian et al 2014; Gong et al 2014)
and our work. While they use the penultimate layer as
patch descriptor, we show in our experiments that we
can get improved results with preceding layers, that are
cheaper to compute and require smaller input patches.
Closely related is the work of Ng et al (2015) which
uses VLAD pooling on top of very deep CNN feature
maps, at multiple scales with good performance on Hol-
idays and Oxford. Their approach is similar to the one
of Gong et al (2014), but faster as it factorizes com-
putation using whole-image convolutions. Building on
this, Tolias et al (2015) uses an improved aggregation
method compared to VLAD, that leverages the struc-
ture of convolutional feature maps.

Babenko et al (2014) use a single global CNN de-
scriptor for instance-level image retrieval and ﬁne-tune
the descriptor on an external landmark dataset. We ex-
periment with their ﬁne-tuned network and show im-
provement also with lower levels on the Oxford dataset.
CKN descriptors still outperform this approach. Fi-
nally, (Jiang et al 2014) proposes a Siamese architecture
to train image retrieval descriptors but do not report
results on standard retrieval benchmarks.

2.3 Deep patch descriptors

Recently (Long et al 2014; Fischer et al 2014; Simo-
Serra et al 2015; Zagoruyko and Komodakis 2015) out-
perform SIFT for patch matching or patch classiﬁca-
tion. These approaches use diﬀerent levels of supervi-
sion to train a CNN. (Long et al 2014) learn their patch
CNNs using category labels of ImageNet. (Fischer et al
2014) creates surrogate classes where each class corre-
sponds to a patch and distorted versions of this patch.
Matching and non-matching pairs are used in (Simo-
Serra et al 2015; Zagoruyko and Komodakis 2015).
There are two key diﬀerences between those works and

4

Mattis Paulin, Julien Mairal, Matthijs Douze, Zaid Harchaoui, Florent Perronnin, Cordelia Schmid

ours. First, they focus on patch-level metrics, instead of
actual image retrieval. Second, and more importantly,
while all these approaches require some kind of super-
vision, we show that our Patch-CKN yields competi-
tive performance in both patch matching and image
retrieval without any supervision.

2.4 Unsupervised learning for deep representations

To avoid costly annotation, many works leverage un-
supervised information to learn deep representations.
Unsupervised learning can be used to initialize network
weights, as in Erhan et al (2009, 2010). Methods that
directly use unsupervised weights include domain trans-
fer (Donahue et al 2014) and k-means (Coates and Ng
2012). Most recently, some works have looked into us-
ing temporal coherence as supervision (Goroshin et al
2015, 2014). Closely related to our work, Agrawal et al
(2015) propose to train a network by learning the aﬃne
transformation between synchronized image pairs for
which camera parameters are available. Similarly, Ja-
yaraman and Grauman (2015) uses a training objective
that enforces for sequences of images that derive from
the same ego-motion to behave similarly in the feature
space. While these two works focus on a weakly super-
vised setting, we focus on a fully unsupervised one.

3 Local Convolutional Descriptors

In this section, we brieﬂy review notations related to
CNNs and the possible learning approaches.

3.1 Convolutional Neural Networks

In this work, we use convolutional features to encode
patches extracted from an image. We call convolutional
descriptor any feature representation f that decom-
poses in a multi-layer fashion as:

f (x) = γK(σK(WK . . . γ2(σ2(W2γ1(σ1(W1x)) . . . )),

(1)

where x is an input patch represented as a vector, the
Wk’s are matrices corresponding to linear operations,
the σk’s are pointwise non-linear functions, e.g., sig-
moids or rectiﬁed linear units, and the functions γk
perform a downsampling operation called “feature pool-
ing”. Each composition γk(σk(Wk•)) is called a “layer ”
and the intermediate representations of x, between each
layer, are called “maps”. A map can be represented as

pixels organized on a spatial grid, with a multidimen-
sional representation for each pixel. Borrowing a clas-
sical terminology from neuroscience, it is also common
to call “receptive ﬁeld ” the set of pixels from the input
patch x that may inﬂuence a particular pixel value from
a higher-layer map. In traditional convolutional neu-
ral networks, the Wk matrices have a particular struc-
ture corresponding to spatial convolutions performed
by small square ﬁlters, which will need to be learned.
In the case where there is no such structure, the layer
is called “fully-connected”.

The hyper-parameters of a convolutional architec-
ture lie in the choice of non-linearities σk, type of pool-
ing γk, in the structure of the matrices Wk (notably the
size and number of ﬁlters) as well as in the number of
layers.

The only parameters that are learned in an auto-
mated fashion are usually the ﬁlters, corresponding to
the entries of the matrices Wk. In this paper we in-
vestigate the following ways of learning: (i) encoding
local descriptors with a CNN that has been trained
for an unrelated classiﬁcation task (Sec. 3.2.1), (ii) us-
ing a CNN that has been trained for a classiﬁcation
problem that can be directly linked to the target task
(e.g. buildings, see Sec. 3.2.1), (iii) devising a surro-
gate classiﬁcation problem to enforce invariance (Sec.
3.2.2), (iv) directly learning the weights using patch-
level groundtruth (Sec. 3.3) or (v) using unsupervised
learning, such as convolutional kernel networks, which
we present in Section 4.

3.2 Learning Supervised Convolutional Descriptors

of

learning

traditional way

The
the weights
W = (W1, W2, . . . , WK) in (1) consists in using a
training set X = (x1, x2, . . . , xn) of examples, equipped
with labels Y = (y1, y2, . . . , yn), choose a loss function
(cid:96)(X ,Y,W) and minimize it over W using stochastic
gradient optimization and back-propagation (LeCun
et al 1989; Bottou 2012). The choice of examples,
labelings and loss functional leads to diﬀerent weights.

3.2.1 Learning with category labels

now

classical

architecture
(Krizhevsky et al 2012). AlexNet

is
A
con-
AlexNet
sists of 8 layers: the ﬁrst ﬁve are convolutional layers
and the last ones are fully connected.

CNN

Convolutional Patch Representations for Image Retrieval: an Unsupervised Approach

5

semantic information into the network that is more re-
lated to the target task. The resulting network is called
AlexNet-Landmarks.

3.2.2 Learning from surrogate labels

Most CNNs such as AlexNet augment the dataset with
jittered versions of training data to learn the ﬁlters Wk
in (1). Dosovitskiy et al (2014); Fischer et al (2014)
use virtual patches, obtained as transformations of ran-
domly extracted ones to design a classiﬁcation prob-
lem related to patch retrieval. For a set of patches P,
and a set a transformations T , the dataset consists of
all τ (p), (τ, p) ∈ T × P. Transformed versions of the
same patch share the same label, thus deﬁning surro-
gate classes. Similarily to the previous setup, the net-
work use softmax loss (2).

In this paper, we evaluate this strategy by using
the same network, called PhilippNet, as in Fischer et al
(2014). The network has three convolutional and one
fully connected layers, takes as input 64x64 patches,
and produces a 512-dimensional output.

3.3 Learning with patch-level groundtruth

When patch-level labels are available, obtained by man-
ual annotation or 3D-reconstruction (Winder et al
2009), it is possible to directly learn a similarity mea-
sure as well as a feature representation. The simplest
way to do so, is to replace the virtual patches in the
architecture of Dosovitskiy et al (2014); Fischer et al
(2014) described in the previous section with labeled
patches of RomeTrain. We call this version “FisherNet-
Rome”.

It can also be achieved using a Siamese network
(Chopra et al 2005), i.e. a CNN which takes as input
the two patches to compare, and where the objective
function enforces that the output descriptors’ similarity
should reproduce the ground-truth similarity between
patches.

Optimization can be conducted with either a metric-

learning cost (Simo-Serra et al 2015):

n(cid:88)

n(cid:88)

i=1

j=1

(cid:96)(X ,Y, WK) =

C(i, j,(cid:107)M (i)

K − M (j)
K (cid:107))

(cid:26) d

with

C(i, j, d) =

if yi = yj
max(0, 1 − d) otherwise

(3)

(4)

or as a binary classiﬁcation problem (“match”/“not-
match”) with a softmax loss as in eq. (2) (Zbontar

Fig. 2: Typical organization of two successive layers
of a CNN. The spatial map M(cid:48)k is obtained from Mk
by convolution and pointwise non-linearity, and the
top layer Mk+1 is obtained from M(cid:48)k by a downsam-
pling operation called feature pooling. Usual CNNs
use ReLU non-linearities and max-pooling (Krizhevsky
et al 2012), while CKNs rely on exponentials and Gaus-
sian pooling (Mairal et al 2014b).

In this case, the training examples are images that
have been hand-labeled into C classes such as “bird” or
“cow” and the loss function is the softmax loss:

n(cid:88)

C(cid:88)

(cid:16)

(cid:17)

(cid:96)(X ,Y,W) =

log

exp

i=1

j=1

M (i)
K [j] − M (i)

K [yi]

.

(2)

In Eq. (2) and throughout the paper, M (i)
is the out-
k
put of the k-th layer (k ∈ {1, . . . , K}) of the network
applied to example xi. The [j] notation corresponds to
the j-th element of the map.

Even though the network is designed to process im-
ages of size 224 × 224, each neuron of a map has a
“receptive ﬁeld”, see the “coverage” column in Table 2
from Section 6. Using an image of the size of the re-
ceptive ﬁeld produces a 1x1 map that we can use as a
low dimensional patch descriptor. To ensure a fair com-
parison between all approaches, we rescale the ﬁxed-size
input patches so that they ﬁt the required input of each
network.

We explore two diﬀerent sets of

labellings for
AlexNet:
the ﬁrst one, which we call AlexNet-
ImageNet, is learned on the training set of ILSVRC
2012 (C = 1000), as in the original paper (Krizhevsky
et al 2012). This set of weights is popular in oﬀ-the-
shelf convolutional features, even though the initial task
is unrelated to the target image retrieval application.
Following Babenko et al (2014), we also ﬁne-tune the
same network on the Landmarks dataset, to introduce

Mk(z0)patchpzconvolutionWk+non-linearityσkM0k(z00)featurepoolingγkMk+1(z1)6

Mattis Paulin, Julien Mairal, Matthijs Douze, Zaid Harchaoui, Florent Perronnin, Cordelia Schmid

and LeCun 2015; Zagoruyko and Komodakis 2015). For
those experiments, we use the parameters of the siamese
networks of Zagoruyko and Komodakis (2015), avail-
able online1. Following their convention, we refer to
these architectures as “DeepCompare”.

4 Convolutional Kernel Networks

In this paper, the unsupervised learning strategy for
learning convolutional networks is based on the con-
volutional kernel networks (CKNs) of Mairal et al
(2014b). Similar to CNNs, these networks have a multi-
layer structure with convolutional pooling and nonlin-
ear operations at every layer. Instead of learning ﬁlters
by optimizing a loss function, say for classiﬁcation, they
are trained layerwise to approximate a particular non-
linear kernel, and therefore require no labeled data.

The presentation of CKNs is divided into three
stages: (i) introduction of the abstract model based on
kernels (Sections 4.1, 4.2, and 4.3); (ii) approximation
scheme and concrete implementation (Sections 4.4, 4.5,
and 4.7); (iii) optimization (Section 4.6).

4.1 A Single-Layer Convolutional Kernel for Images

The basic component of CKNs is a match kernel that
encodes a similarity between a pair of images (M, M(cid:48))
of size m × m × d pixels, which are assumed to be
square. The integer d represents the number of chan-
nels, say 3 for RGB images. Note that when applied
to image retrieval, these images M, M(cid:48) correspond to
regions – patches – extracted from an image. We omit
this fact for simplicity since this presentation of CKNs
is independent of the image retrieval task.

We denote by Ω the set of pixel locations, which is
of size |Ω| = m × m, and choose a patch size e × e.
Then, we denote by Pz (resp. P (cid:48)z(cid:48)) the e× e× d patch of
M (resp. M(cid:48)) at location z ∈ Ω (resp. z(cid:48) ∈ Ω). Then,
the single-layer match kernel is deﬁned as follows:

Deﬁnition 1 Single-Layer Convolutional Kernel.

(cid:88)

z,z(cid:48)∈Ω

K(M, M(cid:48)) =

with

e− 1

2β2 (cid:107)z−z

(cid:48)

(cid:107)2

κ(Pz, P (cid:48)z(cid:48)),

(5)

(cid:48)

(cid:107)2

,

2α2 (cid:107) ˜P− ˜P

κ(P, P (cid:48)) = (cid:107)P(cid:107)(cid:107)P (cid:48)(cid:107)e− 1
where α and β are two kernel hyperparameters, (cid:107) · (cid:107)
denotes the usual (cid:96)2 norm, and ˜P and ˜P (cid:48) are (cid:96)2-
normalized versions of the patches P and P (cid:48).

(6)

1 https://github.com/szagoruyko/cvpr15deepcompare

K is called a convolutional kernel ; it can be inter-
preted as a match-kernel that compares all pairs of
patches from M and M(cid:48) with a nonlinear kernel κ,
weighted by a Gaussian term that decreases with their
relative distance. The kernel compares indeed all loca-
tions in M with all locations in M(cid:48). It depends no-
tably on the parameter α, which controls the nonlin-
earity of the Gaussian kernel comparing two normal-
ized patches ˜P and ˜P (cid:48), and on β, which controls the
size of the neighborhood in which a patch is matched
with another one. In practice, the comparison of two
patches that have very diﬀerent locations z and z(cid:48) will
be negligible in the sum (5) when β is small enough.
Hence, the parameter β allows us to control the local
shift-invariance of the kernel.

4.2 From Kernels to Inﬁnite-Dimensional Feature
Maps

Designing a positive deﬁnite kernel on data is equiv-
alent to deﬁning a mapping of the data to a Hilbert
space, called reproducing kernel Hilbert space (RKHS),
where the kernel is an inner product (Cucker and Zhou
2007); exploiting this mapping is sometimes referred to
as the “kernel trick” (Sch¨olkopf and Smola 2002). In
this section, we will show how the kernel (5) may be
used to generalize the concept of “feature maps” from
the traditional neural network literature to kernels and
Hilbert spaces.2 The kernel K is indeed positive deﬁ-
nite (see the appendix of Mairal et al 2014b) and thus
it will suits our needs.

Basically, feature maps from convolutional neural
networks are spatial maps where every location carries a
ﬁnite-dimensional vector representing information from
a local neighborhood in the input image. Generalizing
this concept in an inﬁnite-dimensional context is rela-
tively straightforward with the following deﬁnition:
Deﬁnition 2 Let H be a Hilbert space. The set of fea-
ture maps is the set of applications ϕ : Ω → H.
Given an image M , it is now easy to build such a
feature map. For instance, consider the nonlinear ker-
nel for patches κ deﬁned in Eq. (6). According to the
Aronzsjan theorem, there exists a Hilbert space Hκ and
a mapping φκ such that for two image patches P, P (cid:48)
– which may come from diﬀerent images or not –,
κ(P, P (cid:48)) = (cid:104)φκ(P ), φκ(P (cid:48))(cid:105)Hκ. As a result, we may use

2 Note that in the kernel literature, “feature map” denotes
the mapping between data points and their representation
in a reproducing kernel Hilbert space (RKHS). Here, feature
maps refer to spatial maps representing local image charac-
teristics at every location, as usual in the neural network lit-
erature LeCun et al (1998).

Convolutional Patch Representations for Image Retrieval: an Unsupervised Approach

7

this mapping to deﬁne a feature map ϕM : Ω → Hκ for
image M such that ϕM (z) = φκ(Pz), where Pz is the
patch from M centered at location z. The ﬁrst property
of feature maps from classical CNNs would be satisﬁed:
at every location, the map carries information from a
local neighborhood from the input image M .

We will see in the next subsection how to build se-
quences of feature maps in a multilayer fashion, with
invariant properties that are missing from the simple
example we have just described.

4.3 From Single-Layer to Multi-Layer Kernels

M , . . . , ϕk

feature
M for an input image M initially rep-
M : Ω0 → Rp0,

We now show how to build a sequence of
maps ϕ1
resented as a ﬁnite-dimensional map ϕ0
where Ω0 is the set of pixel locations in M and p0 is
the number of channels. The choice of initial map ϕ0
M
is important since it will be the input of our algorithms;
it is thus discussed in Section 4.7. Here, we assume
that we have already made this choice, and we explain
how to build a map ϕk
M : Ωk → Hk from a previous
map ϕk–1
M : Ωk–1 → Hk–1. Speciﬁcally, our goal is to
design ϕk
M such that

(i) ϕk

M (z) for z in Ωk carries information from a local

neighborhood from ϕk–1

M centered at location z;

(ii) the map ϕk

M is “more invariant” than ϕk–1
M .

These two properties can be obtained by deﬁning a pos-
itive deﬁnite kernel Kk on patches from ϕk–1
M . Denoting
by Hk its RKHS, we may call ϕk
M (z) the mapping to Hk
of a patch from ϕk–1
M centered at z. The construction is
illustrated in Figure 3.

Concretely, we choose a patch shape Pk, which is a
set of coordinates centered at zero along with a set of
pixel locations Ωk such that for all z in Ωk and u in Pk,
the location z + u is in Ωk–1. Then, the kernel Kk for
comparing two patches from ϕk–1
M(cid:48) at respective
locations z, z(cid:48) in Ωk is deﬁned as

M and ϕk–1

e− 1

2β2

k (cid:107)u−u

(cid:48)

(cid:107)2

κk(ϕk–1

M (u + z), ϕk–1

M (u(cid:48) + z(cid:48))), (7)

(cid:88)

u,u(cid:48)∈Pk
where

(cid:48)
k (cid:107) ˜ϕ− ˜ϕ

2α2

(cid:107)2Hk–1 ,

κk(ϕ, ϕ(cid:48)) = (cid:107)ϕ(cid:107)Hk–1(cid:107)ϕ(cid:48)(cid:107)Hk–1e− 1
for all ϕ, ϕ(cid:48) in Hk–1, where ˜ϕ (resp. ˜ϕ(cid:48)) are normalized—
that is, ˜ϕ = (1/(cid:107)ϕ(cid:107)Hk–1)ϕ if ϕ = 0 and 0 otherwise.
This kernel is similar to the convolutional kernel for
images already introduced in (5), except that it oper-
ates on inﬁnite-dimensional feature maps. It involves
two parameters αk, βk to control the amount of invari-
ance of the kernel. Then, by deﬁnition, ϕk
M : Ωk → Hk

M , . . . , ϕk

the sequence of

Fig. 3: Construction of
feature
maps ϕ1
M for an input image M . The point
ϕk
M (z) for z in Ωk represents the mapping in Hk
of a patch from ϕk–1
M centered at z. Figure adapted
from Mairal et al (2014b).

This

M (z), ϕk

M(cid:48)(z(cid:48))(cid:105)Hk .

framework yields a sequence of

is the mapping such that the value (7) is equal to the
inner product (cid:104)ϕk
inﬁnite-
dimensional image representations but requires ﬁnite-
dimensional approximations to be used in practice.
Among diﬀerent approximate kernel embeddings tech-
niques (Williams and Seeger 2001; Bach and Jordan
2002; Rahimi and Recht 2008; Perronnin et al 2010;
Vedaldi and Zisserman 2012), we will introduce a data-
driven approach that exploits a simple expansion of the
Gaussian kernel, and which provides a new way of learn-
ing convolutional neural networks without supervision.

4.4 Approximation of the Gaussian Kernel

Speciﬁcally, the previous approach relies on an ap-
proximation scheme for the Gaussian kernel, which is
plugged in the convolutional kernels (7) at every layer;
this scheme requires learning some weights that will be
interpreted as the parameters of a CNN in the ﬁnal
pipeline (see Section 4.5).

More precisely, for all x and x(cid:48) in Rq, and α > 0,
(cid:107)2
2 can be shown to be

(cid:48)
2α2 (cid:107)x−x

the Gaussian kernel e− 1
equal to

(cid:18) 2

(cid:19) q
2(cid:90)

πα2

v∈Rq

e− 1

α2 (cid:107)x−v(cid:107)2

2e− 1

α2 (cid:107)x

(cid:48)

−v(cid:107)2

2 dv.

(8)

Furthermore, when the vectors x and x(cid:48) are on the
sphere—that is, have unit (cid:96)2-norm, we also have

e− 1

2α2 (cid:107)x−x

(cid:48)

2 = E
(cid:107)2

v∼p(v)[s(v(cid:62)x)s(v(cid:62)x(cid:48))],

(9)

Ω0ϕ0M(z0)∈H0{z1}+P1ϕ1M(z1)∈H1Ω1{z2}+P2Ω2ϕ2M(z2)∈H28

Mattis Paulin, Julien Mairal, Matthijs Douze, Zaid Harchaoui, Florent Perronnin, Cordelia Schmid

α2 + 2u

where s is a nonlinear function such that s(u) ∝
e− 1
α2 and p(v) is the density of the multivariate nor-
mal distribution N (0, (α2/4)I). Then, diﬀerent strate-
gies may be used to approximate the expectation by a
ﬁnite weighted sum:

e− 1

2α2 (cid:107)x−x

(cid:48)

(cid:107)2
2 ≈

1
p

ηjs(v(cid:62)j x)s(v(cid:62)j x(cid:48)),

(10)

which can be further simpliﬁed, after appropriate
changes of variables,

e− 1

2α2 (cid:107)x−x

(cid:48)

(cid:107)2
2 ≈

(cid:62)
j x+bj ew

(cid:62)
(cid:48)
j x

ew

+bj ,

(11)

for some sets of parameters wj in Rp and bj in R,
j = 1, . . . , p, which need to be learned. The approxima-
tion leads to the kernel approximations (cid:104)ψ(x), ψ(x(cid:48))(cid:105)
where ψ(x) = [ew
j=1, which may be interpreted
as the output of a one-layer neural network with p neu-
rons and exponential nonlinear functions.

(cid:62)
j x+bj ]p

The change of variable that we have introduced
yields a simpler formulation than the original formu-
lation of Mairal et al (2014b). Given a set of training
pairs of normalized signals (x1, x(cid:48)1), . . . , (xn, x(cid:48)n) in Rq,
the weights wj and scalars bj may now be obtained by
minimizing

e

n(cid:88)

i=1

p(cid:88)

j=1

(cid:107)xi−x(cid:48)

i(cid:107)2

2α2 −

min
W,b

p(cid:88)

j=1

p(cid:88)

j=1

2

(cid:62)
j xi+bj ew

(cid:62)
(cid:48)
j x
i+bj

ew

,

(12)

which is a non-convex optimization problem. How we
address it will be detailed in Section 4.6.

4.5 Back to Finite-Dimensional Feature Maps

Convolutional kernel networks use the previous ap-
proximation scheme of the Gaussian kernel to build
ﬁnite-dimensional image representations M1 : Ω1 →
Rp1 , M2 : Ω2 → Rp2, . . . , Mk : Ωk → Rpk of an input
image M with the following properties:

(i) There exists a patch size ek × ek such that a patch
Pl,z of Ml at location z—which is formally a vec-
tor of size e2
kpk–1—provides a ﬁnite-dimensional ap-
proximation of the kernel map ϕl
M (z). In other
words, given another patch P (cid:48)l,z(cid:48) from a map M(cid:48)l ,
we have (cid:104)ϕl
3 Note that to be more rigorous, the maps Ml need to
be slightly larger in spatial size than ϕl
M since otherwise a
patch Pl,z at location z from Ωl may take pixel values out-
side of Ωl. We omit this fact for simplicity.

M (z)(cid:105)Hl ≈ (cid:104)Pl,z, P (cid:48)l,z(cid:48)(cid:105).3

M (z), ϕl

(ii) Computing a map Ml from Ml–1 involves convolu-
tion with learned ﬁlters and linear feature pooling
with Gaussian weights.

Speciﬁcally, the learning and encoding algorithms are
presented in Algorithms 1 and 2, respectively. The re-
sulting procedure is relatively simple and admits two
interpretations.

– The ﬁrst one is to consider CKNs as an ap-
the inﬁnite-dimensional
feature
proximation of
maps ϕ1
M presented in the previous sec-
tions (see Mairal et al 2014b, for more details about
the approximation principles).

M , . . . , ϕk

– The second one is to see CKNs as particular types
of convolutional neural networks with contrast-
normalization. Unlike traditional CNNs, ﬁlters and
nonlinearities are learned to approximate the Gaus-
sian kernel on patches from layer k–1.

With both interpretations, this representation induces
a change of paradigm in unsupervised learning with
neural networks, where the network is not trained to
reconstruct input signals, but where its non-linearities
are derived from a kernel point of view.

Algorithm 1 Training layer k of a CKN.

Hyper-parameters: Kernel parameter αk, patch size ek ×
ek, number of ﬁlters pk.
Input model: A CKN trained up to layer k–1.
Input data: A set of training images.
Algorithm:

– Encode the input images using the CKN up to layer k–1
– Extract randomly n pairs of patches (Pi, P (cid:48)

by using Algorithm 2;

i ) from the

maps obtained at layer k–1;

– Normalize the patches to make then unit-norm;
– Learn the model parameters by minimizing (12), with

(xi, x(cid:48)

i) = (Pi, P (cid:48)

i ) for all i = 1, . . . , n;

Output: Weight matrix Wk in Rpk–1e2

k×pk and bk in Rpk .

4.6 Large-Scale Optimization

One of the challenge we faced to apply CKNs to im-
age retrieval was the lack of scalability of the original
model introduced by Mairal et al (2014b), which was
a proof of concept with no eﬀort towards scalability. A
ﬁrst improvement we made was to simplify the original
objective function with changes of variables, resulting
in the formulation (12), leading to simpler and less ex-
pensive gradient computations.

The second improvement is to use stochastic opti-
mization instead of the L-BFGS method used by Mairal
et al (2014b). This allows us to train every layer by us-
ing a set of one million patches and conduct learning

Convolutional Patch Representations for Image Retrieval: an Unsupervised Approach

9

Algorithm 2 Encoding layer k of a CKN.

Hyper-parameters: Kernel parameter βk;
Input model: CKN parameters learned from layer 1 to k
by using Algorithm 1;
Input data: A map Mk–1 : Ωk−1 → Rpk–1 ;
Algorithm:
– Extract patches {Pk,z}z∈Ωk–1 of size ek × ek from the

input map Mk–1;

– Compute contrast-normalized patches

˜Pk,z =

1

(cid:107)Pk,z(cid:107) Pk,z if Pk,z (cid:54)= 0 and 0 otherwise.

– Produce an intermediate map ˜Mk : Ωk–1 → Rpk with

linear operations followed by non-linearity:
˜Mk(z) = (cid:107)Pk,z(cid:107)eW

˜Pk,z +bk ,

(cid:62)
k

(13)

where the exponential function is meant “pointwise”.
– Produce the output map Mk by linear pooling with

Gaussian weights:

Mk(z) =

(cid:107)u−z(cid:107)2

− 1
β2
e
k

˜Mk(u).

(cid:88)

u∈Ωk–1

Output: A map Mk : Ωk → Rpk .

on all of their possible pairs, which is a regime where
stochastic optimization is unavoidable. Unfortunately,
applying stochastic gradient descent directly on (12)
turned out to be very ineﬀective due to the poor con-
ditioning of the optimization problem. One solution is
to make another change of variable and optimize in a
space where the input data is less correlated.

(cid:80)n

More precisely, we proceed by (i) adding an “inter-
cept” (the constant value 1) to the vectors xi in Rq,
yielding vectors ˜xi in Rq+1; (ii) computing the result-
ing (uncentered) covariance matrix G = 1
i=1 ˜xi ˜x(cid:62)i ;
n
(iii) computing the eigenvalue decomposition of G =
U ∆U(cid:62), where U is orthogonal and ∆ is diagonal with
non-negative eigenvalues; (iv) computing the precondi-
tioning matrix R = U (∆ + τ I)1/2U(cid:62), where τ is an
oﬀset that we choose to be the mean value of the eigen-
values. Then, the matrix R may be used as a precon-
ditioner since the covariance of the vectors R˜xi is close
to identity. In fact, it is equal to the identity matrix
when τ = 0 and G is invertible. Then, problem (12)
with preconditioning becomes

(cid:62)
j R˜xiez

ez

(cid:62)
(cid:48)
j R˜x
i

,

(14)

e

n(cid:88)

i=1

p(cid:88)

j=1

(cid:107)xi−x(cid:48)

i(cid:107)2

2α2 −

min

Z

2

obtained with the change of variable [W (cid:62), b(cid:62)] = Z(cid:62)R.
Optimizing with respect to Z to obtain a solution (W, b)
turned out to be the key for fast convergence of the
stochastic gradient optimization algorithm.

Note that our eﬀort also consisted on implement-
ing heuristics for automatically selecting the learning
rate during optimization without requiring any manual
tuning, following in part standard guidelines from Bot-
tou (2012). More precisely, we select the initial learning
rate in the following range: {1, 2−1/2, 2−1, . . . , 2−20}, by
performing 1K iterations with mini-batches of size 1000
and choosing the one that gives the lowest objective,
evaluated on a validation dataset. After choosing the
learning rate, we keep monitoring the objective on a
validation set every 1K iteration, and perform back-
tracking in case of divergence. The learning rate is also
divided by √2 every 50K iterations. The total num-
ber of iterations is set to 300K. Regarding initializa-
tion, weights are randomly initialized according to a
standard normal distribution. These heuristics are ﬁxed
over all experiments and resulted in a stable parameter-
free learning procedure, which we will release in an
open-source software package.

4.7 Diﬀerent Types of CKNs with Diﬀerent Inputs

We have not discussed yet the initial choice of the
map M0 for representing an image M . In this paper,
we follow Mairal et al (2014b) and investigate three
possible inputs:

1. CKN-raw: We use the raw RGB values. This cap-
tures the hue information, which is discriminant in-
formation for many application cases.

2. CKN-white: It is similar to CKN-raw with the fol-
lowing modiﬁcation: each time a patch P0,z is ex-
tracted from M0, it is ﬁrst centered (we remove its
mean color), and whitened by computing a PCA on
the set of patches from M0. The resulting patches
are invariant to the mean color of the original patch
and mostly respond to local color variations.

3. CKN-grad: The input M0 simply carries the two-
dimensional image gradient computed on graysale
values. The map has two channels, corresponding
to the gradient computed along the x-direction and
along the y-direction, respectively. These gradients
are typically computed by ﬁnite diﬀerences.

Note that the ﬁrst layer of CKN-grad typically uses

patches of size 1× 1, which are in R2, and which are en-

coded by the ﬁrst layer into p1 channels, typically with
p1 in [8; 16]. This setting corresponds exactly to the ker-
nel descriptors introduced by Bo et al (2010), who have
proposed a simple approximation scheme that does not
require any learning. Interestingly, the resulting repre-
sentation is akin to SIFT descriptors.

Denoting by (Gx,z, Gy,z) the gradient components
of image M at location z, the patch Pz is simply

10

Mattis Paulin, Julien Mairal, Matthijs Douze, Zaid Harchaoui, Florent Perronnin, Cordelia Schmid

G2

x,z + G2

(cid:113)
the vector [Gx,z, Gy,z] in R2. Then, the norm (cid:107)Pz(cid:107)2
can be interpreted as the gradient magnitude ρz =
y,z, and the normalized patch ˜Pz repre-
sents a local orientation. In fact, there exists θz
such that ˜Pz = [cos(θz), sin(θz)]. Then, we may use
the relation (8) to approximate the Gaussian kernel
exp(−(cid:107) ˜Pz − ˜P (cid:48)z(cid:48)(cid:107)2/(2α2
1)). We may now approximate
the integral by sampling p1 evenly distributed orienta-
tions θj = 2jπ/p1, : j ∈ {1, . . . , p1}, and we obtain, up
to a constant scaling factor,

e− 1

α2

1 (cid:107) ˜Pz−Pθj (cid:107)2

e− 1

α2

1 (cid:107) ˜P

(cid:48)
z(cid:48)−Pθj (cid:107)2

,

j=1

(15)

p1(cid:88)

e− 1

2α2

1 (cid:107) ˜Pz− ˜P

(cid:48)
z(cid:48)(cid:107)2

≈

where Pθ = [cos(θ), sin(θ)]. With such an approxima-
tion, the j-th entry of the map ˜M1(z) from (13) should
be replaced by

ρze− 1

2α2
1

((cos(θj )−cos(θz))2+(sin(θj )−sin(θz))2)

.

This formulation can be interpreted as a soft-binning
of gradient orientations in a “histogram” of size p1
at every location z. To ensure an adequate distribu-

tion in each bin, we choose α1 =(cid:0)(1 − cos (2π/d1))2 +
sin (2π/d1)2(cid:1)1/2

. After the pooling stage, the represen-

tation becomes very close to SIFT descriptors.

A visualization of all input methods can be seen in
ﬁgure 4. See (Mairal et al 2014a) for more analysis of
image preprocessing.

5 Datasets

In this section, we give details on the standard datasets
we use to evaluate our method, as well as the protocol
we used to create our new dataset.

5.1 Standard datasets

We give details on the commonly used benchmarks for
which we report results.

5.1.1 Patch retrieval

custom detectors. We extract regions with the Hessian-
Aﬃne detector and match the corresponding descrip-
tors with Euclidean nearest-neighbor. A pair of ellipses
is deemed to match if the projection of the ﬁrst re-
gion using the ground-truth homography on the second
ellipse overlaps by at least 50%. The performance is
measured in terms of mean average precision (mAP).

5.1.2 Image retrieval

We selected three standard image retrieval bench-
marks: Holidays, Oxford and the University of Ken-
tucky Benchmark (UKB).

Holidays The Holidays dataset (J´egou et al 2008) con-
tains 500 diﬀerent scenes or objects, for which 1,491
views are available. 500 images serve as queries. Fol-
lowing common practice, in contrast to (Babenko et al
2014) though, we use the unrotated version, which al-
lows certain views to display a 90◦ rotation with re-
spect to their query. While this has a non-negligible
impact on performance for dense representations (the
authors of (Babenko et al 2014) report a 3% global
drop in mAP), this is of little consequence for our
pipeline which uses rotation-invariant keypoints. Ex-
ternal learned parameters, such as k-means clusters for
VLAD and PCA-projection matrix are learned on a
subset of random Flickr images. The standard metric is
mAP.

Oxford The Oxford dataset (Philbin et al 2007) con-
sists of 5,000 images of Oxford landmarks. 11 locations
of the city are selected as queries and 5 views per loca-
tion is available. The standard benchmarking protocol,
which we use, involves cropping the bounding-box of
the region of interest in the query view, followed by re-
trieval. Some works, such as (Babenko et al 2014) forgo
the last step. Such a non-standard protocol yields a
boost in performance. For instance Babenko and Lem-
pitsky (2015) report a close to 6% improvement with
non-cropped queries. mAP is the standard measure.

The dataset introduced in (Mikolajczyk et al 2005) in
now standard to benchmark patch retrieval methods.
This dataset consists of a set of 8 scenes viewed under
6 diﬀerent conditions, with increasing transformation
strength. In contrast to (Winder et al 2009; Zagoruyko
and Komodakis 2015) where only DoG patches are
available, the Mikolajczyk et al (2005) dataset allows

UKB Containing 10,200 photos, the University of Ken-
tucky Benchmark (UKB) (Nister and Stewenius 2006)
consists of 4 diﬀerent views of the same object, un-
der radical viewpoint changes. All images are used as
queries in turn, and the standard measure is the mean
number of true positives returned in the four ﬁrst re-
trieved images (4×recall@4).

Convolutional Patch Representations for Image Retrieval: an Unsupervised Approach

11

Fig. 4: Vizualization of the possible CKN inputs: CKN-raw (left); input when subtracting the mean color of
each patch (middle-left); CKN-white, similar to the previous but with whitening (middle-right) and CKN-grad
(right). Patches were extracted with a size of 3x3; images are reconstructed by averaging the pixel values of the
pre-processed patches (up to normalization to ﬁt the 0-256 range). Best viewed in color on a computer screen.

5.2 Rome

One of the goals of this work is to establish a link be-
tween performance in patch retrieval with performance
in image retrieval. Because of the way datasets are con-
structed diﬀers (e.g. Internet-crawled images vs succes-
sive shots with the same camera, range of viewpoint
changes, semantic content of the dataset, type of key-
point detector), patch descriptors may display diﬀerent
performances on diﬀerent datasets. We therefore want a
dataset that contains a groundtruth both at patch and
image level, to jointly benchmark the two performances.
Inspired by the seminal work of (Winder et al 2009), we
introduce the Rome retrieval dataset, based on 3D re-
construction of landmarks. The Rome16K dataset (Li
et al 2010) is a Community-Photo-Collection dataset
that consists of 16,179 images downloaded from photo
sharing sites, under the search term “Rome”. Images
are partitioned into 66 “bundles”, each one containing
a set of viewpoints of a given location in Rome (e.g.
“Trevi Fountain”). Within a bundle, consistent param-
eters were automatically computed and are available4.
The set of 3D points that were reconstructed is also
available, but we choose not to use them in favor of
our Hessian-Aﬃne keypoints. To determine matching
points among images of a same bundle, we use the fol-
lowing procedure. i) we extract Hessian-Aﬃne points
in all images. For each pair of images of a bundle, we
match the corresponding SIFTs, using Lowe’s reverse
neighbor rule, as well as product quantization (J´egou
et al 2011) for speed-up. We ﬁlter matches, keeping
those that satisfy the epipolar constraint up to a toler-
ance of 3 pixels. Pairwise point matches are then greed-
ily aggregated to form larger groups of 2D points viewed
from several cameras. Groups are merged only if the

4 www.cs.cornell.edu/projects/p2f

Fig. 5: Examples of patches matched under our pro-
cedure. We observe signiﬁcant changes in lighting, but
smaller changes in rotation and skew.

reproduction error from the estimated 3D position is
below the 3 pixel threshold.

To allow safe parameter tuning, we split the set of
bundles into a train and a test set, respectively contain-
ing 44 and 22 bundles.

5.2.1 Patch retrieval

We design our patch retrieval datasets by randomly
sampling in each train and test split a set of 1, 000 3D
points for which at least 10 views are available. The
sampling is uniform in the bundles, which means that
we take roughly the same amount of 3D points from
each bundle. We then sample 10 views for each point,
use one as a query and the remaining as targets. Both
our datasets therefore contain 1, 000 queries and 9, 000
retrieved elements. We report mean average precision
(mAP). An example of patch retrieval classes can be
seen in Fig. 5.

12

Mattis Paulin, Julien Mairal, Matthijs Douze, Zaid Harchaoui, Florent Perronnin, Cordelia Schmid

Keypoint detection. A popular design choice in image
representations, inspired by text categorization meth-
ods, is to consider images as sets of local patches, taken
at various locations. The choice of these locations is
left to the interest point detector, for which multiple
alternatives are possible.

In this work, we use the popular “Hessian-Aﬃne”
detector of Mikolajczyk and Schmid (2004). It aims at
ﬁnding reproducible points, meaning that selected 3D
points of a scene should always belong to the set of de-
tected image points when the camera undergoes small
changes in settings (e.g. viewpoint, blur, lighting). Be-
cause of the “aperture” problem the set of such points is
limited to textured patches, to the exclusion of straight
lines, for which precise localization is impossible. This
leaves “blobs”, as used for instance in Lowe’s Diﬀerence
of Gaussians (DoG) detector (Lowe 2004) or corners, as
is the case for our Hessian-Aﬃne detector. Speciﬁcally,
a “cornerness” measure is computed on the whole im-
age, based on the Hessian of the pixel intensities. The
set of points whose cornerness is above a threshold is
kept. The detector takes the points at their characteris-
tic scale and estimates an aﬃne-invariant local region.
Rotation invariance is achieved by ensuring the domi-
nant gradient orientation always lies in a given direc-
tion. This results in a set of keypoints with locally-aﬃne
invariant regions. Fig. 7 shows the various steps for de-
tecting keypoints. We sample the point with a resolu-
tion of 51 × 51 pixels, value that was found optimal for
SIFT on Oxford. Pixels that fall out of the image are
set to their nearest neighbor in the image. This strategy
greatly increases patch retrieval performance compared
to setting them to black, as it does not introduce strong
gradients.

Note that the choice of a particular interest point
detector is arbitrary and our method is not speciﬁc to
Hessian-Aﬃne locations. To show this, we also experi-
ment with dense patches in section 6.3.5

Patch description. Because of the aﬃne-invariant de-
tectors, and as seen in Fig. 7, for a given 3D point seen
in two diﬀerent images, the resulting patches have small
diﬀerences (e.g. lighting, blur, small rotation, skew).
The goal of patch description, and the focus of this
work, is to design a patch representation, i.e. a mapping
Φ of the space of ﬁxed-size patches into some Hilbert
space, that is robust to these changes.

Fig. 7: Example of matching points in two diﬀerent im-
ages. Salient points are extracted (left), aﬃne rectiﬁed
(middle), and normalized in rotation (right). Note that
the level of invariance which remains to be covered by
the patch descriptor is relatively low, as most of the
work has been accomplished by the detector.

5.2.2 Image retrieval

Using the same aforementioned train-test bundle split,
we select 1,000 query images and 1,000 target images
evenly distributed over all bundles. Two images are
deemed to match if they come from the same bundle,
as illustrated in Fig. 6

6 Experiments

In this section, we describe the implementation of our
pipelines, and report results on patch and image re-
trieval benchmarks.

6.1 Implementation details

We provide details on the patch and image retrieval
pipelines. Our goal is to evaluate the performance of
patch descriptors, and all methods are therefore given
the same input patches (computed at Hessian-Aﬃne
keypoints), possibly resized to ﬁt the required input
size of the method. We also evaluate all methods with
the same aggregation procedure (VLAD with 256 cen-
troids). We believe that improvements in feature detec-
tion and aggregation are orthogonal to our contribution
and would equally beneﬁt all architectures.

6.1.1 Pipeline

We brieﬂy review our image retrieval pipeline.

Matching/Aggregation. Stereo-vision uses this keypoint
representation to establish correspondences between
images of the same instance, with 3D reconstruction
as an objective. The cost of this operation is quadratic

Convolutional Patch Representations for Image Retrieval: an Unsupervised Approach

13

Fig. 6: Example of classes of the image retrieval dataset of Rome. Each class consists of a particular location.
Some bundle display signiﬁcant viewpoint changes (extreme left and right), while others have little variation in
appearance (middle). Best viewed in color.

in the number of keypoints, which is prohibitive in im-
age retrieval systems that need to scan through large
databases. Instead, we choose to aggregate the local
patch descriptors into a ﬁxed-length global image de-
scriptor. For this purpose, we use the popular VLAD
representation (J´egou et al 2012). Given a clustering in
the form of a Voronoi diagram with points {c1, . . . , ck}
of the feature space (typically obtained using k-means
on an external set of points), VLAD encodes the set of
visual words {x1, . . . , xn} as the total shift with respect
to their assigned centroid:

k(cid:88)

n(cid:88)

i=1

j=1

VLAD =

εi,j(xj − ci),

(16)

where εi,j is the assignment operator, which is 1 if
xj is closer to centroid ci than to the others, and 0
otherwise.

The ﬁnal VLAD descriptors is power-normalized
with exponent 0.5 (signed square-root), as well as (cid:96)2-
normalized.

6.1.2 Convolutional Networks training

AlexNet-ImageNet. We use the Caﬀe package (Jia et al
2014) and its provided weights for AlexNet, that have
been learned according to Krizhevsky et al (2012).
Speciﬁcally, the network was trained on ILSVRC’12
data for 90 epochs, using a learning rate initialized to
10−2 and decreased three times prior to termination.
Weight decay was ﬁxed to 0.0005, momentum to 0.9
and dropout rate to 50%. It uses three types of image
jittering: random 227 × 227 out of 256 × 256 cropping,
ﬂip and color variation.

dataset5. Following their protocol, we strip the net-
work of its last layer and replace it with a 671-dim
fully-connected one, initialized with random Gaussian
noise (standard deviation 0.01). Other layers are initial-
ized with the old AlexNet-ImageNet weights. We use
a learning rate of 10−3 for ﬁne-tuning. Weight decay,
momentum and dropout rate are kept to their default
values (0.0005, 0.9 and 0.5). We use data augmentation
at training time, with the same transformations as in
the original paper (crop, ﬂip and color). We decrease
the learning rate by 10 when it saturates (around 20
epochs each time). We report a validation accuracy of
59% on the Landmarks dataset, which was conﬁrmed
through discussion with the authors. On Holidays, we
report a mAP of 77.5 for the sixth layer (against 79.3
in (Babenko et al 2014)), and 53.6 (against 54.5) on
Oxford. Even though slightly below the results in the
original paper, ﬁne-tuning still signiﬁcantly improves
on ImageNet weights for retrieval.

PhilippNet. For PhilippNet, we used the model pro-
vided by the authors. The model is learned on 16K
surrogate classes (randomly extracted patches) with
150 representatives (composite transformations, includ-
ing crops, color and contrast variation, blur, ﬂip, etc.).
We were able to replicate their patch retrieval results
on their dataset, as well as on Mikolajczyk et al’s
dataset when using MSER keypoints.

PhilippNet-Rome. The patch retrieval dataset of
RomePatches does not contain enough patches to learn
a deep network. We augment it using patches extracted
in a similar fashion, grouped in classes that correspond
to 3D locations and contain at least 10 examples. We
build two such training sets, one with 10K classes, and

AlexNet-Landmarks. Following Babenko et al (2014),
we ﬁne-tune AlexNet using images of the Landmarks

5 http://sites.skoltech.ru/compvision/projects/

neuralcodes/

14

Mattis Paulin, Julien Mairal, Matthijs Douze, Zaid Harchaoui, Florent Perronnin, Cordelia Schmid

one with 100K classes. Training is conducted with the
default parameters.

Deepcompare. As previously described, we use the
online code provided by Zagoruyko and Komodakis
(2015). It consists of networks trained on the three
distinct datasets of Winder et al (2009): Liberty,
NotreDame and Yosemity. For our image retrieval ex-
periments, we can only use the siamese networks, as
the others do not provide a patch representation. These
were observed in the original paper to give suboptimal
results.

Convolutional Kernel Networks To train the convolu-
tional kernel networks, we randomly subsample a set of
100K patches in the train split of the Rome dataset.
For each layer, we further extract 1M sub-patches with
the required size, and feed all possible pairs as input
to the CKN. The stochastic gradient optimization is
run for 300K iterations with a batch size of 1000, fol-
lowing the procedure described in section 4.6. Train-
ing a convolutional kernel network, for a particular set
of hyperparameters, roughly takes 10 min on a GPU.
This stands in contrast to the 2-3 days for training us-
ing the L-BFGS implementation used in Mairal et al
(2014b). We show in Fig. 8 a visualization of the ﬁrst
convolutional ﬁlters of CKN-raw. The sub-patches for
CKN-grad and CKN-white are too small (3 × 3) and
not adapted to viewing.

6.2 Patch retrieval

6.2.1 CKN parametric exploration

The relatively low training time of CKNs (10 min. on
a recent GPU), as well as the fact that the training is
layer-wise – and therefore lower layer parameters can
be reused if only the top layer change – allows us to
test a relatively high number of parameters, and select
the ones that best suit our task. We tested parameters
for convolution and pooling patch sizes in range of 2 to
5, number of neuron features in powers of 2 from 128
to 1024. For the σ parameter, the optimal value was
found to be 10−3 for all architectures. For the other
parameters, we keep one set of optimal values for each
input type, described in Table 1.

In Figure 9, we explore the impact of the various
hyper-parameters of CKN-grad, by tuning one while
keeping the others to their optimal values.

6.2.2 Dimensionality reduction

Since the ﬁnal dimension of the CKN descriptor is pro-
hibitive for most applications of practical value (see

Input
CKN-raw
CKN-white
CKN-grad

Layer 1

5x5, 5, 512
3x3, 3, 128
1x1, 3, 16

Layer 2

—-

2x2, 2, 512
4x4,2,1024

dim.
41,472
32,768
50,176

Table 1: For each layer we indicate the sub-patch size,
the subsampling factor and the number of ﬁlters. For
the gradient network, the value 16 corresponds to the
number of orientations.

Fig. 9: mAP results on the train set of RomePatches
with CKN-grad, whose hyper-parameters have been
changed one by one around the optimal point. Note
that 10 and 12 gradient bins give slightly better results
but 16 was kept to align with the logarithm scale of the
grid.

Table 1), we investigate dimensionality reduction tech-
niques. Note that this step is unnecessary for CNNs,
whose feature dimension do not exceed 512 (Philipp-
Net). We only perform unsupervised dimensionality re-
duction through PCA, and investigate several forms of
whitening. Denoting X the n × d matrix of n CKN
features (we used n = 10K), the singular value decom-
position of X writes as

X = U SV (cid:62),

(17)

where U and V are orthogonal matrices and S is
diagonal with non-increasing values from upper left to
bottom right. For a new n(cid:48) × d matrix of observations
X(cid:48), the dimensionality reduction step writes as

X(cid:48)proj = XL(cid:62),

(18)

The three types of whitening we use are:

where L is a d(cid:48) × d projection matrix.
i) no
whitening; ii) full whitening; iii) semi-whitening. Full
whitening corresponds to

L = V (1 : d(cid:48), :)/D(1 : d(cid:48), 1 : d(cid:48)) ,

(19)

88899091921345128256512123568101210−110−210−420Pooling sizeOutputsSub-patch sizeGradient binsSigma (quantile)Convolutional Patch Representations for Image Retrieval: an Unsupervised Approach

15

Fig. 8: All 512 kernels of the ﬁrst layer of our CKN-raw architecture.

CKN-grad

CKN-raw

CKN-white

Architecture

coverage Dim RomePatches Miko.

SIFT
AlexNet-conv1
AlexNet-conv2
AlexNet-conv3
AlexNet-conv4
AlexNet-conv5
PhilippNet
PhilippNet
CKN-grad
CKN-raw
CKN-white

51x51
11x11
51x51
99x99

131x131
163x163

64x64
91x91
51x51
51x51
51x51

128
96
256
384
384
256
512
2048
1024
1024
1024

train
91.6
66.4
73.8
81.6
78.4
53.9
86.1
88.0
92.5
79.3
91.9

test
87.9
65.0
69.9
79.2
75.7
49.6
81.4
83.7
88.1
76.3
87.7

57.8
40.9
46.4
53.7
43.4
24.4
59.7
61.3
59.5
50.9
62.5

Table 2: Results of convolutional architectures for patch
retrieval.

Results are given in Table 2. For AlexNet CNNs, we
report results for all outputs of the 5 convolutional
layers (after ReLU). We note that SIFT is an excellent
baseline for these methods, and that CNN architec-
tures that were designed for local invariances perform
better than the ones used in AlexNet, as observed by
Fischer et al (2014). The results of the PhilippNet on
Mikolajczyk et al’s dataset are diﬀerent from the ones
reported by Fischer et al (2014), as we evaluate on
Hessian-Aﬃne descriptors while they use MSER. To
have a comparable setting, we use their network with
an input of 64x64 that corresponds to the coverage
of one top neuron, as well as their protocol that slide
it on 91x91 patches. We notice that this last step
only provides a small increase of performance (2% for
patch retrieval and 1% for image retrieval). We observe
that PhilippNet outperforms both SIFT and AlexNet,
which was the conclusion of Fischer et al (2014); CKN
trained on whitened patches do however yield better
results.

Fig. 10: Inﬂuence of dimensionality reduction on patch
retrieval performance. Results reported in mAP (%) on
the train split of RomePatches as a function of the PCA
dimension. As a comparison, SIFT reports 91.6%.

while semi-whitening corresponds to

(cid:112)

L = V (1 : d(cid:48), :)/

D(1 : d(cid:48), 1 : d(cid:48)) .

(20)

The matrix division / denotes the matrix multipli-
cation with the inverse of the right-hand matrix. The
square-root is performed element-wise.

of

the

on

Results

diﬀerent methods

the
RomePatches dataset are displayed in Fig. 10. We
observe that semi-whitening works best for CKN-grad
and CKN-white, while CKN-raw is slightly improved
by full whitening. We keep these methods in the
remainder of this article, as well as a ﬁnal dimension
of 1024.

6.2.3 Results

compare
three

convolutional architectures on
We
our
datasets: RomePatches-train,
RomePatches-test and Mikolajczyk et al’s dataset.

the
patch

 90 91 92 93 64 256 1024 50 60 70 80 90 64 256 1024 90 91 92 93 64 256 1024no PCAPCAPCA+whiteningPCA+semi-whitening16

Mattis Paulin, Julien Mairal, Matthijs Douze, Zaid Harchaoui, Florent Perronnin, Cordelia Schmid

6.2.4 DeepCompare

As the architectures of DeepCompare (Zagoruyko and
Komodakis 2015) do not rely on an underlying patch
descriptor representation but rather on similarities be-
tween pairs of patches, some architectures can only be
tested for patch-retrieval. We give in Table 3 the per-
formances of all their architectures on RomePatches.

Network

RomePatches-Train RomePatches-Test

2ch2stream Y
2ch2stream ND
2ch2stream L

2ch Y
2ch ND
2ch L
siam Y
siam ND
siam L

siam2stream Y
siam2stream ND
siam2stream L

89.3
88.3
90.2
86.6
81.2
83.4
82.9
84.8
83.2
78.8
84.2
82.0

85.7
84.9
86.6
83.5
78.4
80.1
79.2
81.0
79.8
75.4
80.6
78.6

Table 3: Evaluation of the deepcompare architectures
on RomePatches in mAP (%). Networks were trained
on the three subsets of the Multi-view Stereo Corre-
spondence Dataset: Yosemity (Y), Notre-Dame (ND)
and Statue of Liberty (L). The network notations are as
in the original paper (Zagoruyko and Komodakis 2015).

We note that the only networks that produce de-
scriptors that can be used for image retrieval are
the architectures denoted “siam” here. They also per-
form quite poorly compared to the others. Even the
best architectures are still below the SIFT baseline
(91.6/87.9).

The method of Zagoruyko and Komodakis (2015)
optimizes and tests on diﬀerent patches (DoG points,
64x64 patches, greyscale), which explains the poor per-
formances when transferring them to our RomePatches
dataset. The common evaluation protocol on this
dataset is to sample the same number of matching and
non-matching pairs, rank them according to the mea-
sure and report the false positive rate at 95% recall
(“FPR@95%”, the lower the better). The patches used
for the benchmark are available online, but not the ac-
tual split used for testing. With our own split of the Lib-
erty dataset, we get the results in Table 4. We note that
all our results, although diﬀering slightly from the ones
of Zagoruyko and Komodakis (2015), do not change
their conclusions.

We note that our CKNs whose hyperparameters
were optimized on RomePatches work poorly. However,
optimizing again the parameters for CKN-grad on a
grid leads to a result of 14.4% (“best CKN”). To ob-

Descriptor

SIFT

Best AlexNet (3)

siam-l2 (trained on Notre-Dame)

2ch-2stream (trained on Notre-Dame)

CKN-grad
CKN-white
CKN-raw
best CKN

FPR@95% (%)

19.9
13.5
14.7
1.24
27.7
30.4
41.7
14.4

Table 4: Experiments on a set of pairs of the Lib-
erty dataset. Measure is false positive rate @95% recall,
and therefore the lower the better. The third layer of
AlexNet is used, as it provides the best results.

tain this result, the number of gradient histogram bins
was reduced to 6, and the pooling patch size was in-
creased to 8 on the ﬁrst layer and 3 on the second.
These changes indicate that the DoG keypoints are less
invariant to small deformations, and therefore require
less rigid descriptors (more pooling, less histograms).
The results are on par with all comparable architec-
tures (siam-l2: 14.7%, AlexNet: 13.5%), as the other
architectures either use central-surround networks or
methods that do not produce patch representations.
The best method of Zagoruyko and Komodakis (2015),
2ch-2stream, reports an impressive 1.24% on our split.
However, as already mentioned, this architecture does
not produce a patch representation and it is therefore
diﬃcult to scale to large-scale patch and image retrieval
applications.

6.2.5 Impact of supervision

We study the impact of the diﬀerent supervised train-
ings, between surrogate classes and real ones. Results
are given in Table 5.

Training parameters

RomePatches-Train RomePatches-Test

PhilippNet

PhilippNet-Rome 10K
PhilippNet-Rome 100K

(Best) CKN-grad
(Baseline) SIFT

86.1
84.1
89.9
92.5
91.6

81.4
80.1
85.6
88.1
87.9

is

is

trained on a larger

Table 5: Impact of supervision on patch retrieval.
trained on surrogate classes, while
PhilippNet
PhilippNet-Rome
set of
RomePatches-Train, containing either 10K or 100K
classes. We see that retraining improves performance
provided enough data is given. Supervised CNNs are
still below the SIFT baseline, as well as the unsuper-
vised CKNs.

Convolutional Patch Representations for Image Retrieval: an Unsupervised Approach

17

Holidays UKB Oxford

Rome

SIFT
AlexNet-conv1
AlexNet-conv2
AlexNet-conv3
AlexNet-conv4
AlexNet-conv5
PhilippNet 64x64
PhilippNet 91x91
CKN-grad
CKN-raw
CKN-white
CKN-mix

64.0
59.0
62.7
79.3
77.1
75.3
74.1
74.7
66.5
69.9
78.7
79.3

3.44
3.33
3.19
3.74
3.73
3.69
3.66
3.67
3.42
3.54
3.74
3.76

43.7
18.8
12.5
33.3
34.3
33.4
38.3
43.6
49.8
23.0
41.8
43.4

train
52.9
28.9
36.1
47.1
47.9
45.7
50.2
51.4
57.0
33.0
51.9
54.5

test
62.7
36.8
21.0
54.7
55.4
53.1
60.4
61.3
66.2
43.8
62.4
65.3

Table 6: Image retrieval results. Results are in mAP
except for UKB where we measure the average number
of true positives in the ﬁrst 4 results. CKN-mix is the
result of the concatenation of the VLAD descriptors for
the three channels.

On datasets for which color is dominant (e.g. Holi-
days or UKB), the best individual CKN results are at-
tained by CKN-white, improved by combining the three
channels. On images of buildings, gradients still per-
form best and the addition of color channels is harmful,
which explains on the one hand the poor performance
of AlexNet and on the other hand the relatively good
performance of PhilippNet which was explicitly trained
to be invariant to colorimetric transformations.

6.3.3 Inﬂuence of context

Through experiments with AlexNet-landmarks, we
study the impact of context in the local features. Specif-
ically, we test whether ﬁne-tuning on a dataset that
shares semantic information with the target retrieval
dataset improves performance. We compare the same
network architecture – AlexNet – in two settings: when
parameters are learned on ImageNet (which involves
a varied set of classes) and when they are learned on
the Landmarks dataset which solely consists of build-
ings and places. Results, shown in Table 7, show clear
improvement for Oxford, but not for Holidays. We ex-
plain this behavior by the fact that the network learns
building-speciﬁc invariances and that fewer building
structures are present in Holidays as opposed to Ox-
ford.

6.3.4 Dimensionality reduction

As observed in previous work (J´egou and Chum 2012),
projecting the ﬁnal VLAD descriptor to a lower dimen-
sion using PCA+whitening can lead to lower memory
costs, and sometimes to slight increases in performance
(e.g. on Holidays (Gong et al 2014)). We project to

Fig. 11: First convolutional ﬁlters of the PhilippNet
learned with surrogate classes (left), 10K Rome classes
(middle) and 100K Rome classes. Best viewed in color.

Figure 11 gives the ﬁrst convolutional ﬁlters of the
PhilippNet learned on surrogate classes, and on Rome.
As can be seen, the plain surrogate version reacts to
more diverse color, as it has been learned with diverse
(ImageNet) input images. Both Rome 10K and Rome
100K datasets have colors that correspond to skies and
buildings and focus more on gradients. It is interest-
ing to note that the network trained with 100K classes
seems to have captured ﬁner levels of detail than its
10K counterpart.

6.2.6 Robustness

We investigate in Fig. 12 the robustness of CKN
descriptors to transformations of the input patches,
speciﬁcally rotations, zooms and translations. We select
100 diﬀerent images, and extract the same patch by jit-
tering the keypoint along the aforementioned transfor-
mation. We then plot the average L2 distance between
the descriptor of the original patch and the transformed
one. Note the steps for the scale transformation; this is
due to the fact that the keypoint scale is quantized on
a scale of powers of 1.2, for performance reasons.

6.3 Image retrieval

6.3.1 Settings.

We learn a vocabulary of 256 centroids on a related
database: for Holidays and UKB we use 5,000 Flickr
images and for Oxford, we train on Paris (Philbin
et al 2008). The vocabulary for RomePatches-Train is
learned on RomePatches-Test and vice-versa. The ﬁnal
VLAD descriptor size is 256 times the local descriptor
dimension.

6.3.2 Results

We compare all convolutional approaches as well as the
SIFT baseline in the image retrieval settings. Results
are summarized in Table 6.

18

Mattis Paulin, Julien Mairal, Matthijs Douze, Zaid Harchaoui, Florent Perronnin, Cordelia Schmid

Fig. 12:
Impact of various transformations on CKN descriptors. Represented is the average distance between a
patch descriptor and its jittered version, as a function of the transformation magnitude. Best viewed in numerical
format.

Holidays

Oxford

ImageNet

Landmarks

ImageNet

Landmarks

conv1
conv2
conv3
conv4
conv5

59.0
62.7
79.3
77.1
75.3

57.7
72.4
76.1
73.9
66.1

18.8
12.5
33.3
34.3
33.4

20.7
23.8
38.6
38.3
32.5

Table 7: Inﬂuence of the pretraining dataset on image
retrieval. With the same architecture (AlexNet), train-
ing is conducted either on ILSVRC’12 data or on the
Landmarks dataset. Using semantic information related
to buildings and places yields improvements for Oxford,
but not for Holidays.

Dataset

Holidays
(CKN-mix)

UKB

Oxford

(CKN-mix)

(CKN-grad)

Full (262K-dim)
PCA (4096-dim)

79.3
82.9

3.76
3.77

49.8
47.2

Table 8:
Impact of dimensionality reduction by
PCA+whitening on the best channel for each dataset.
PCA to 4096 dimensions.

4096-dim descriptors, the same output dimension as
Babenko et al (2014) and obtain the results in Table 8.
We indeed observe a small improvement on Holidays
but a small decrease on Oxford.

6.3.5 Dense keypoints

Our choice of Hessian-Aﬃne keypoints is arbitrary and
can be suboptimal for some image retrieval datasets.
Indeed we observe that by sampling points on a regular
grid of 8 × 8 pixels at multiple scales, we can improve
results. We learn the CKN parameters and the PCA
projection matrix on a dense set of points in Rome,

Descriptor

Hessian-Aﬃne

Dense (same parameters)
Dense (changed pooling)

SIFT
64.0
70.3
70.3

grad
66.5
68.5
71.3

raw
69.9
72.3
72.3

white mix
78.7
79.3
76.8
80.8

82.6

.

Table 9: Dense results on Holidays. Right hand side
of the table are CKN descriptors. “Same parameters”
correspond to CKNs that have the same parameters as
Hessian-Aﬃne ones, yet learned on densely extracted
patches. “Changed pooling” have pooling size increased
by one at each layer (CKN-raw is unchanged as it only
has one layer, and already gives good performances).

and apply it to image retrieval as before. We use SIFT
descriptors as a baseline, extracted at the exact same
locations. We observe that for CKN-grad and CKN-
white, the previous models lead to suboptimal results.
However, increasing the pooling from 3 (resp. 2 for the
second layer) to 4 (resp. 3), leads to superior perfor-
mances. We attribute this to the fact that descriptors
on dense points require more invariance than the ones
computed at Hessian-Aﬃne locations. Results on the
Holidays dataset are given in Table 9. As observed on
Hessian-Aﬃne points, the gradient channel performs
much better on Oxford. We therefore only evaluate this
channel. As explained in section 5, there are two ways to
evaluate the Oxford dataset. The ﬁrst one crops queries,
the second does not. While we only consider the ﬁrst
protocol to be valid, it is interesting to investigate the
second, as results in Table 10 tend to indicate that it fa-
vors dense keypoints (and therefore the global descrip-
tors of Babenko et al (2014)).

2015105051015200.00.10.20.30.40.50.60.70.80.9Rotation (deg)0.40.60.81.01.21.41.60.00.10.20.30.40.50.60.7Scale (factor)151050510150.00.10.20.30.40.50.60.70.8X-translation (pixels)151050510150.00.10.20.30.40.50.60.70.8Y-translation (pixels)2015105051015200.000.050.100.150.200.250.300.35Rotation (deg)0.40.60.81.01.21.41.60.000.050.100.150.200.250.300.35Scale (factor)151050510150.000.050.100.150.200.250.300.350.40X-translation (pixels)151050510150.000.050.100.150.200.250.300.350.40Y-translation (pixels)2015105051015200.00.10.20.30.40.50.60.7Rotation (deg)0.40.60.81.01.21.41.60.00.10.20.30.40.50.6Scale (factor)151050510150.00.10.20.30.40.50.60.7X-translation (pixels)151050510150.00.10.20.30.40.50.60.7Y-translation (pixels)Convolutional Patch Representations for Image Retrieval: an Unsupervised Approach

19

Method \ descriptor
Hessian-Aﬃne, no crop

Hessian-Aﬃne, crop

Dense, no crop

Dense, crop

SIFT CKN-grad
45.7
43.7
51.1
47.6

49.0
49.8
55.4
50.9

Table 10: Dense keypoints on the Oxford dataset.
“Crop” indicates
the protocol where queries are
cropped to a small bounding-box containing the rele-
vant object, while “no crop” takes the full image as
a query. For CKN, parameters have increased pooling
sizes for dense keypoints, as on Holidays.

6.3.6 Comparison with state of the art

Table 11 compares our approach to recently published
results. Approaches based on VLAD with SIFT (Arand-
jelovic and Zisserman 2013; J´egou et al 2012) can be im-
proved signiﬁcantly by CKN local descriptors (+15% on
Holidays). To compare to the state of the art with SIFT
on Oxford (Arandjelovic and Zisserman 2013), we use
the same Hessian-Aﬃne patches extracted with gravity
assumption (Perd’och et al 2009). Note that this alone
results in a 7% gain.

We also compare with global CNN (Babenko et al
2014). Our approach outperforms it on Oxford, UKB,
and Holidays. For CNN features with sum-pooling en-
coding (Babenko and Lempitsky 2015), we report bet-
ter results on Holidays and UKB, and on Oxford with
the same evaluation protocol. Note that their method
works better than ours when used without cropping the
queries (58.9%).

On Holidays, our approach is slightly below the
one of Gong et al (2014), that uses AlexNet descrip-
tors and VLAD pooling on large, densely extracted
patches. It is however possible to improve on this re-
sult by using the same dimensionality reduction tech-
nique (PCA+whitening) which gives 82.9% or dense
keypoints (82.6%).

7 Conclusion

showed that Convolutional Kernel Networks
We
(CKNs) oﬀer similar and sometimes even better per-
formances than classical Convolution Neural Networks
(CNNs) in the context of patch description, and that
the good performances observed in patch retrieval
translate into good performances for image retrieval,
reaching state-of-the-art results on several standard
benchmarks. The main advantage of CKNs compared
to CNNs is their very fast training time, and the fact
that unsupervised training suppresses the need for man-
ually labeled examples. It is still unclear if their success

is due to the particular level of invariance they induce,
or their low training time which allows to eﬃciently
search through the space of hyperparamaters. We leave
this question open and hope to answer it in future work.
Acknowledgments. This work was partially sup-
ported by projects “Allegro” (ERC), “Titan” (CNRS-
Mastodon), “Macaron” (ANR-14-CE23-0003-01), the
Moore-Sloan Data Science Environment at NYU and a
Xerox Research Center Europe collaboration contract.
We wish to thank Fischer et al (2014); Babenko et al
(2014); Gong et al (2014) for their helpful discussions
and comments. We gratefully acknowledge the support
of NVIDIA Corporation with the donation of the GPUs
used for this research.

References

Agrawal P, Carreira J, Malik J (2015) Learning to see by
moving. In: IEEE Conference on Computer Vision and
Pattern Recognition

Arandjelovic R, Zisserman A (2013) All about VLAD.
In: IEEE Conference on Computer Vision and Pattern
Recognition

Babenko A, Lempitsky V (2015) Aggregating deep convolu-
tional features for image retrieval. In: International Con-
ference on Computer Vision

Babenko A, Slesarev A, Chigorin A, Lempitsky V (2014) Neu-
ral codes for image retrieval. In: European Conference on
Computer Vision

Bach FR, Jordan MI (2002) Kernel independent component

analysis. Journal of Machine Learning Research

Bay H, Tuytelaars T, Van Gool L (2006) SURF: Speeded up
robust features. In: European Conference on Computer
Vision

Bo L, Ren X, Fox D (2010) Kernel descriptors for visual
recognition. In: Advances in Neural Information Process-
ing Systems

Bottou L (2012) Stochastic gradient descent tricks. In: Neural

Networks: Tricks of the Trade, Springer

Brown M, Hua G, Winder S (2011) Discriminative learning
of local image descriptors. IEEE Transactions on Pattern
Analysis and Machine Intelligence

Calonder M, Lepetit V, Strecha C, Fua (2010) BRIEF: Bi-
nary robust independent elementary features. In: Euro-
pean Conference on Computer Vision

Chopra S, Hadsell R, LeCun Y (2005) Learning a similar-
ity metric discriminatively, with application to face ver-
iﬁcation. In: IEEE Conference on Computer Vision and
Pattern Recognition

Coates A, Ng AY (2012) Learning feature representations
with k-means. In: Neural Networks: Tricks of the Trade,
Springer

Cucker F, Zhou DX (2007) Learning theory : an approxima-
tion theory viewpoint. Cambridge Monographs on Ap-
plied and Computational Mathematics, Cambridge Uni-
versity Press, Cambridge, New York

Deng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei L (2009)
ImageNet: A large-scale hierarchical
image database.
In: IEEE Conference on Computer Vision and Pattern
Recognition

Donahue J, Jia Y, Vinyals O, Hoﬀman J, Zhang N, Tzeng
E, Darrell T (2014) DeCAF: A deep convolutional acti-

20

Mattis Paulin, Julien Mairal, Matthijs Douze, Zaid Harchaoui, Florent Perronnin, Cordelia Schmid

Method \ Dataset
VLAD (J´egou et al 2012)
VLAD++ (Arandjelovic and Zisserman 2013)
Global-CNN (Babenko et al 2014)
MOP-CNN (Gong et al 2014)
Sum-pooling OxfordNet (Babenko and Lempitsky 2015)
Ours

Holidays

63.4
64.6
79.3
80.2
80.2

79.3 (82.9)

UKB
3.47

-

3.56

-

3.65
3.76

Oxford

-

55.5*
54.5

-

53.1

49.8 (56.5*)

Table 11: Comparison with state-of-the-art image retrieval results. Results with * use a Hessian-Aﬃne detector
with gravity assumption (Perd’och et al 2009). Our best result on Holidays uses whitening as in MOP-CNN.

vation feature for generic visual recognition. In: Interna-
tional Conference on Machine Learning

larity with deep ranking. In: IEEE Conference on Com-
puter Vision and Pattern Recognition

Dong J, Soatto S (2015) Domain-size pooling in local descrip-
tors: Dsp-sift. In: IEEE Conference on Computer Vision
and Pattern Recognition

Dosovitskiy A, Springenberg JT, Riedmiller M, Brox T (2014)
Discriminative unsupervised feature learning with convo-
lutional neural networks. Advances in Neural Information
Processing Systems

Erhan D, Manzagol PA, Bengio Y, Bengio S, Vincent P (2009)
The diﬃculty of training deep architectures and the eﬀect
of unsupervised pre-training. In: Twelfth International
Conference on Artiﬁcial Intelligence and Statistics

Erhan D, Bengio Y, Courville A, Manzagol PA, Vincent P,
Bengio S (2010) Why does unsupervised pre-training help
deep learning? The Journal of Machine Learning Research
Fischer P, Dosovitskiy A, Brox T (2014) Descriptor match-
ing with Convolutional Neural Networks: a comparison to
SIFT. arXiv Preprint

Gong Y, Wang L, Guo R, Lazebnik S (2014) Multi-scale or-
derless pooling of deep convolutional activation features.
In: European Conference on Computer Vision

Goroshin R, Bruna J, Tompson J, Eigen D, LeCun Y (2014)
Unsupervised feature learning from temporal data. In:
Advances in Neural Information Processing Systems

Goroshin R, Mathieu M, LeCun Y (2015) learning to linearize
under uncertainty. In: Advances in Neural Information
Processing Systems

Jayaraman D, Grauman K (2015) Learning image represen-
tations equivariant to ego-motion. In: IEEE Conference
on Computer Vision and Pattern Recognition

J´egou H, Chum O (2012) Negative evidences and co-
occurrences in image retrieval: the beneﬁt of PCA and
whitening. In: European Conference on Computer Vision
J´egou H, Douze M, Schmid C (2008) Hamming embedding
and weak geometric consistency for large scale image
search. In: European Conference on Computer Vision

J´egou H, Douze M, Schmid C, P´erez P (2010) Aggregating
local descriptors into a compact image representation.
In: IEEE Conference on Computer Vision and Pattern
Recognition

J´egou H, Douze M, Schmid C (2011) Product quantization for
nearest neighbor search. IEEE Transactions on Pattern
Analysis and Machine Intelligence

J´egou H, Perronnin F, Douze M, S´anchez J, P´erez P, Schmid
C (2012) Aggregating local image descriptors into com-
pact codes. IEEE Transactions on Pattern Analysis and
Machine Intelligence

Jia Y, Shelhamer E, Donahue J, Karayev S, Long J, Girshick
R, Guadarrama S, Darrell T (2014) Caﬀe: Convolutional
architecture for fast feature embedding. In: ACM Multi-
media Conference

Jiang W, Song Y, Leung T, Rosenberg C, Wang J, Philbin J,
Chen B, Wu Y (2014) Learning ﬁne-grained image simi-

Krizhevsky A, Sutskever I, Hinton G (2012) ImageNet classi-
ﬁcation with deep convolutional neural networks. In: Ad-
vances in Neural Information Processing Systems

LeCun Y, Boser B, Denker J, Henderson D, Howard R, Hub-
bard W, Jackel L (1989) Handwritten digit recognition
with a back-propagation network

LeCun Y, Bottou L, Bengio Y, Haﬀner P (1998) Gradient-

based learning applied to document recognition

Li Y, Snavely N, Huttenlocher DP (2010) Location recog-
nition using prioritized feature matching. In: European
Conference on Computer Vision

Long J, Zhang N, Darrell T (2014) Do Convnets learn cor-
respondances? In: Advances in Neural Information Pro-
cessing Systems

Lowe DG (2004) Distinctive image features from scale-
invariant keypoints. International Journal on Computer
Vision

Mairal J, Bach F, Ponce J (2014a) Sparse Modeling for Im-
age and Vision Processing. Foundations and Trends in
Computer Graphics and Vision

Mairal J, Koniusz P, Harchaoui Z, Schmid C (2014b) Con-
volutional kernel networks. In: Advances in Neural Infor-
mation Processing Systems

Mikolajczyk K, Schmid C (2004) Scale & aﬃne invariant in-
terest point detectors. International Journal on Computer
Vision

Mikolajczyk K, Schmid C (2005) A performance evaluation of
local descriptors. IEEE Transactions on Pattern Analysis
and Machine Intelligence

Mikolajczyk K, Tuytelaars T, Schmid C, Zisserman A, Matas
J, Schaﬀalitzky F, Kadir T, Van Gool L (2005) A com-
parison of aﬃne region detectors. International Journal
on Computer Vision

Ng JYH, Yang F, Davis LS (2015) Exploiting Local Features
from Deep Networks for Image Retrieval. In: DeepVision
Workshop

Nister D, Stewenius H (2006) Scalable recognition with a vo-
cabulary tree. In: IEEE Conference on Computer Vision
and Pattern Recognition

Paulin M, Douze M, Harchaoui Z, Mairal J, Perronnin F,
Schmid C (2015) Local convolutional features with un-
supervised training for image retrieval. In: International
Conference on Computer Vision

Perd’och M, Chum O, Matas J (2009) Eﬃcient representation
of local geometry for large scale object retrieval. In: IEEE
Conference on Computer Vision and Pattern Recognition
Perronnin F, Dance C (2007) Fisher kernels on visual vocab-
ularies for image categorization. In: IEEE Conference on
Computer Vision and Pattern Recognition

Perronnin F, S´anchez J, Liu Y (2010) Large-scale image cat-
egorization with explicit data embedding. In: IEEE Con-
ference on Computer Vision and Pattern Recognition

Convolutional Patch Representations for Image Retrieval: an Unsupervised Approach

21

Philbin J, Chum O, Isard M, Sivic J, Zisserman A (2007)
Object retrieval with large vocabularies and fast spatial
matching. In: IEEE Conference on Computer Vision and
Pattern Recognition

Philbin J, Chum O, Isard M, Sivic J, Zisserman A (2008) Lost
in quantization: Improving particular object retrieval in
large scale image databases. In: IEEE Conference on
Computer Vision and Pattern Recognition

Philbin J, Isard M, Sivic J, Zisserman A (2010) Descriptor
learning for eﬃcient retrieval. In: European Conference
on Computer Vision

Rahimi A, Recht B (2008) Random features for large-scale
kernel machines. In: Advances in Neural Information Pro-
cessing Systems 20

Razavian AS, Azizpour H, Sullivan J, Carlsson S (2014) CNN
features oﬀ-the-shelf: an astounding baseline for recogni-
tion. preprint arXiv:14036382

Sch¨olkopf B, Smola AJ (2002) Learning with kernels: Sup-
port vector machines, regularization, optimization, and
beyond. MIT press

Simo-Serra E, Trulls E, Ferraz L, Kokkinos I, Moreno-Noguer
F (2015) Discriminative learning of deep convolutional
feature point descriptors. In: International Conference on
Computer Vision

Simonyan K, Vedaldi A, Zisserman A (2014) Learning lo-
cal feature descriptors using convex optimisation. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence

Tola E, Lepetit V, Fua P (2010) Daisy: An eﬃcient dense
descriptor applied to wide-baseline stereo. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence

Tolias G, Sicre R, J´egou H (2015) Particular Object Re-
trieval with Integral Max-Pooling of CNN Activations.
In: preprint arXiv:1511.05879

Tuytelaars T, Mikolajczyk K (2008) Local invariant feature
detectors: A survey. Foundations and Trends in Computer
Graphics and Vision

Vedaldi A, Zisserman A (2012) Eﬃcient additive kernels
via explicit feature maps. IEEE Transactions on Pattern
Analysis and Machine Intelligence

Wang Z, Fan B, Wu F (2011) Local intensity order pattern
for feature description. In: International Conference on
Computer Vision

Williams C, Seeger M (2001) Using the Nystr¨om method to
speed up kernel machines. In: Advances in Neural Infor-
mation Processing Systems

Winder S, Hua G, Brown M (2009) Picking the best Daisy.
In: IEEE Conference on Computer Vision and Pattern
Recognition

Yosinski J, Clune J, Bengio Y, Lipson H (2014) How transfer-
able are features in deep neural networks? In: Advances
in Neural Information Processing Systems

Zagoruyko S, Komodakis N (2015) Learning to compare im-
age patches via convolutional neural networks. In: IEEE
Conference on Computer Vision and Pattern Recognition
Zbontar J, LeCun Y (2015) Computing the stereo matching
cost with a convolutional neural network. In: IEEE Con-
ference on Computer Vision and Pattern Recognition

