6
1
0
2

 
r
a

 

M
6
1

 
 
]
T
I
.
s
c
[
 
 

1
v
4
7
2
5
0

.

3
0
6
1
:
v
i
X
r
a

New Sufﬁcient Conditions for Multiple-Access

Channel with Correlated Sources

Mohsen Heidari
EECS Department

University of Michigan

Ann Arbor,USA

Email: mohsenhd@umich.edu

Farhad Shirani
EECS Department

University of Michigan

Ann Arbor,USA

Email: fshirani@umich.edu

S. Sandeep Pradhan

EECS Department

University of Michigan

Ann Arbor,USA

Email: pradhanv@umich.edu

The problem of three-user Multiple-Access Channel (MAC) with correlated sources is investigated. An extension to the Cover-
El Gamal-Salehi (CES) scheme is introduced. We use a combination of this scheme with linear codes and propose a new coding
strategy. We derive new sufﬁcient conditions to transmit correlated sources reliably. We consider an example of three-user MAC
with binary inputs. Using this example, we show strict improvements over the CES scheme.

Abstract

I. INTRODUCTION

T HE separation principle of Shannon plays a fundamental role to reinforce the notion of modularity. This in turn allows

separate development of source and channel code design. However, as shown by Shannon [1], the separation does not
generalize to multi-terminal communications. For instance, this phenomenon was observed in many-to-one communications
involving transmission of correlated sources over MAC [2].

In the problem of MAC with correlated sources, there are multiple transmitters, each observing a source correlated to others.
The transmitters do not communicate with each other and wish to send their observations via a MAC to a central receiver.
The receiver reconstructs the sources losslessly. The separate coding approach involves a source coding part and a channel
coding part. In the channel coding part, Ahlswede [3] and Liao [4] studied the case where the transmitters have independent
information and derived the capacity region for channel coding over MAC. In the source coding part, the distributed source
coding problem was studied in which transmitters can communicate to the receiver error-free. Slepian and Wolf showed that
lossless reproduction of the sources is possible with rates close to the joint entropy [5].

Due to suboptimality of the separation based strategies, the joint source-channel coding approach has been of great interest.
The CES scheme introduced in [2], is a generalization of the results in [3] and [6]. Using this scheme a single-letter
characterization of the set of sources that can be reliably transmitted was derived. It was shown that this scheme strictly
improves upon the previously known strategies. However, Dueck [7] proved that this approach only gives a sufﬁcient condition
and not a necessary one. The joint source-channel coding problem is well studied in other settings such as: source coding with
side information via a MAC [8], broadcast channels with correlated sources [9] and interference channels [10].

Recently, structured codes were used to design coding strategies for joint source-channel coding problems, [11], [12], [13],
[14], [15]. A graph-theoretic framework was introduced in [11], [12] to improve the joint source-channel coding schemes both
in the MAC and the broadcast channel.

In this work, we study the three-user MAC with correlated sources. We ﬁrst extend the CES scheme to this problem
and derive an achievable rate region. As shown in [2], this coding strategy improves upon separate source-channel coding
techniques. This is done by choosing the codewords such that they are statistically dependent on the distribution of the sources.
We observe that further improvements are possible when the sources impose an algebraic structure. One example is when one
of the sources is the modulo sum of the other two. In this scenario, a structured coding strategy is needed for the codebooks to
match with the structure of the sources. With this intuition, we use linear codes in the extension of the CES scheme to derive
a new achievable region. Through an example, we show strict improvements over the extension of the CES scheme.

The rest of this paper is as follows: In section II, we provide the notations, deﬁnitions and the problem statement. In Section
III, an extension of the CES scheme is discussed. A new coding strategy based on linear codes is provided in Section IV. The
improvements over the CES scheme are discussed in Section V. Section VI concludes the paper.

A. Notations

II. PRILIMINARIES AND PROBLEM STATEMENT

In this paper, random variables are denoted using capital letters such as X, Y , and their realizations are shown using lower
case letters such as x, y, respectively. Vectors are shown using lowercase bold letters such as x, y. Sequences of number are
also represented by bold letters. Calligraphic letters are used to denote sets such as X ,Y. For any set A, let SA = {Sa}a∈A.
If A = ∅, then SA = ∅. As a shorthand, we sometimes denote a triple (s1, s2, s3) by s. We also denote a triple of sequences
(s1, s2, s3) by s. By Fq, we denote the ﬁeld of integers modulo-q, where q is a prime number.

B. Three-User MAC with Correlated Sources

Consider a MAC with conditional Probability Mass Function (PMF) p(y|x1, x2, x3), input alphabets Xj, j = 1, 2, 3 and
output alphabet Y. Suppose (S1, S2, S3) represent three sources with joint distribution p(s1, s2, s3). After observing Sn
j , the
jth transmitter encodes it and sends the encoder’s output to the channel. Upon receiving Yn from the channel, the decoder
wishes to reconstruct the sources losslessly. A code for this setup consists of three encoding functions fj : S n
j → Xj, j = 1, 2, 3,
and a decoding function g : Y n → S n
Deﬁnition 1. The source (S1, S2, S3) ∼ p(s1, s2, s3) can be reliably transmitted over the MAC p(y|x1, x2, x3), if for any
 > 0, there exist encoding functions f1, f2, f3 and a decoding function g such that

2 × S n
3 .

1 × S n

P{g(Y n) (cid:54)= (Sn

1 , Sn

2 , Sn

3 |X n

i = fi(Sn

i ), i = 1, 2, 3} ≤ .

C. Common part

To deﬁne the common parts between the sources, we use the notion given in [16].

Deﬁnition 2. Consider random variables Sj, j = 1, 2, . . . , m. W is deﬁned as the common part among these random variables
by ﬁnding maximum positive integer k for which there exist functions

fj : Sj → {1, 2, . . . , k},

j = 1, 2, . . . , m

with P{fj(Sj) = i} > 0 for all i ∈ {1, 2, . . . , k} and j ∈ {1, 2, . . . , m} such that W = fj(Sj), j = 1, 2, .., m, with probability
one.

III. A THREE-USER EXTENSION OF THE CES SCHEME

In this section, we ﬁrst review the CES scheme for the problem of two-user MAC with correlated sources. Then we introduce
an extension to the scheme in the three-user case. Consider a MAC with conditional PMF p(y|x1, x2). Let S1 and S2 be two
correlated sources and W be the common part between them as deﬁned in Deﬁnition 2. In the CES scheme, ﬁrst the common
part W is calculated at each encoder. Since both encoders have access to W , they can fully cooperate to encode it (as if it is
done by a centralized encoder). Next at each transmitter, each source is encoded using a codebook that is “super-imposed” on
the common codebook.

It is shown in [2] that using this scheme, reliable transmission of S1 and S2 is possible if the following holds

H(S1|S2) ≤ I(X1; Y |X2, S2, U ),
H(S2|S1) ≤ I(X2; Y |X1, S1, U ),
H(S1, S2|W ) ≤ I(X1X2; Y |W, U ),

H(S1, S2) ≤ I(X1X2; Y ),

where p(s1, s2, u, x1, x2, y) = p(s1, s2)p(u)p(x1|s1, u)p(x2|s2, u)p(y|x1, x2).

We use the above argument to extend the CES scheme for sending correlated sources over a three-user MAC. Consider the
sources S1, S2, S3. We use Deﬁnition 2 to construct four different common parts among the sources. Let Wij be the common
part of Si, Sj. For more convenience, we denote the common part of Si and Sj, either by Wij or Wji (we simply drop the
condition j > i, as it is understood that Wij = Wji). Lastly, W123 is the common part of S1, S2 and S3.
By observing Si at the ith transmitter, three common parts can be calculated, W123 and Wij, j (cid:54)= i. The three-user extension
of CES involves three layers of coding. In the ﬁrst layer W123 is encoded at each encoders. Next, based on the output of the
ﬁrst layer, the Wij’s are encoded. Finally, based on the output of the ﬁrst and the second layers, S1, S2 and S3 are encoded.
The following preposition determines sufﬁcient conditions for which correlated sources can be transmitted using this scheme.
Proposition 1. The source (S1, S2, S3) ∼ p(s1, s2, s3) can be reliably transmitted over a MAC with conditional probability
p(y|x1x2x3), if for distinct i, j, k ∈ {1, 2, 3} and any B ⊆ {12, 13, 23} the following holds:
H(Si|SjSk) ≤ I(Xi; Y |SjSkXjXkU123U12U13U23)
H(SiSj|SkWB) ≤ I(XiXj; Y |SkWBU123UikUjkUBXk)

H(S1S2S3|W123WB) ≤ I(X1X2X3; Y |W123WBU123UB)

H(S1S2S3) ≤ I(X1X2X3; Y ),

(cid:89)

b∈{12,13,23}

(cid:89)

i,j,k∈{1,2,3}

j<k

where Uij = Uji and

p(s,x, u123, u12, u13, u23) = p(s)p(u123)[

p(ub|wbu123)] · [

p(xi|siu123uijuik)].

(1)

Outline of the proof: Let the random variables X1, X2, X3, U123, U12, U13 and U23 be distributed according to the above
theorem. Let the n-length sequence si be a realization of the source Si, where i = 1, 2, 3.
Codebook Generation: For each w123 ∈ W123 randomly generate a sequence u123 according to the PMF of U123. Index
them by u123(w123). For each u123 and wb, b ∈ {12, 13, 23} randomly generate a sequence according to p(ub|wbu123). Index
them by ub(wb, u123).
For each si, ﬁrst ﬁnd the corresponding sequences of the common parts w123, wij and wik, where i, j, k ∈ {1, 2, 3} are
distinct. Next ﬁnd the corresponding sequences u123(w123), uij(wij, u123) and uik(wik, u123) as generated above. Lastly
generate a sequence xi randomly and independently according to p(xi|siu123uijuik). For shorthand we denote such sequence
by xi(si, u123, uij, uik).

Encoding: Upon observing the output si of the source, the ith transmitter ﬁrst calculates the common part sequences
w123, wij and wik. Then at the ﬁrst stage it ﬁnds u123(w123). At the second stage, it ﬁnds uij(wij, u123) and uik(wik, u123).
Lastly, at the third stage, it sends xi(si, u123, uij, uik).

Decoding: Upon receiving y from the channel, the decoder ﬁnds ˜s = (˜s1, ˜s2, ˜s3) such that

(˜s, ˜u123, ˜u12, ˜u13, ˜u23, ˜x1, ˜x2, ˜x3, y) ∈ A(n)



(S, U123, U12, U13, U23, X1, X2, X3, Y )

where ˜u123 = u123( ˜w123), ˜uij = uij( ˜wij, ˜u123). Note ˜w123, ˜wij are the corresponding common part sequences of ˜s1, ˜s2, ˜s3.
A decoding error will be occurred, if no unique (˜s1, ˜s2, ˜s3) is found. Using a standard argument as in [2], it can be shown

that the probability of error approaches zero, if the conditions in Preposition 1 are satisﬁed.

IV. NEW SUFFICIENT CONDITIONS

In this section, new sufﬁcient conditions for transmission of correlated sources are derived. We enhance the previous scheme

using linear codes.

The common parts used in CES are deﬁned using univariate functions as in Deﬁnition 2. In the case of more than two
sources, the notion of common parts can be extended using bivariate functions as in [18]. The following example provides a
triple of sources with a bivariate common part.
Example 1. Let S1, S2, S3 be three binary sources, where S1 is independent of S2 and S3 = S1 ⊕2 S2, with probability one.
Here, S3 is a bivariate common part of S1 and S2. However, there is no univariate common parts among the sources.

Next, we present a linear coding scheme for the above example. Select a linear code with a generator matrix G chosen
i using this linear code. Since S3 = S1⊕2 S2, using this approach
randomly and uniformly on F2. The ith transmitter encodes Sn
X3 = X1 ⊕2 X2, with probability one. In this case, X1 and X2 are independent and uniform. In contrast, using randomly
generated unstructured codes as in the extension of CES, the equality can not hold (unless the encoders are trivial). More
precisely, in the CES scheme, given S1, S2, S3 the random variables X1, X2, X3 are mutually independent. Hence, one can
conclude that for all valid joint distributions for CES, with high probability, X3 (cid:54)= X1 ⊕2 X2 . This assertion is discussed in
more detail in the proof of Lemma 1.

We use the intuition behind the argument above and propose a new coding strategy in which a combination of linear codes
and the CES scheme is used. We deﬁne a new class of common parts. This class of common parts are linked with our
understanding of bivariate common information [18]. The new common part is called a q-additive common part. The common
part consists of a vector of random variables (T1, T2, T3). Here, Ti is available at the ith encoder. In contrast to the univariate
common parts in the CES scheme, these three random variables are not equal. Rather, each of them is a linear combination
of the other two. This linear structure can be exploited using structured codes. The next deﬁnition formalizes this notion.
Deﬁnition 3. For a prime number q, we say that (T1, T2, T3) is a q-additive common part of (S1, S2, S3), if there exist
functions f1, f2, f3 such that with probability one 1) Ti = fi(Si), 2) T3 = T1 ⊕q T2, 3) Ti are nontrivial random variables.
In the following Theorem, we derive sufﬁcient conditions for transmission of correlated sources. We will show in Section

V that this leads to enlarging the class of correlated sources that can be reliably transmitted.
Theorem 1. The source (S1, S2, S3) can be reliably transmitted over a MAC with conditional PMF p(y|x1, x2, x3) if for any
distinct i, j, k ∈ {1, 2, 3} and for any A ⊆ {1, 2, 3},B ⊆ {12, 13, 23} the followings hold:

H(Si|SjSk) ≤ I(Xi; Y |SjSkU123U12U13U23V1V2V3XjXk)
H(SiSj|SkWBTA) ≤ I(XiXj; Y |SkWBU123UikUjkUBTAVkVAXk)

H(SiSjSk|W123WBTA) ≤ I(XiXjXk; Y |W123WBU123UBTAVA)

H(SiSjSk|TA) ≤ I(XiXjXk; Y |TAVA)

(2)
(3)
(4)
(5)

where 1) (T1, T2, T3) is a q-additive common part of the sources for a prime q, 2) p(u123u12u13u23|s) is the same as in (1),
3) the Markov chain U123U{12,13,23} ↔ S1S2S3 ↔ V1V2V3 holds, 4) V3 = V1 ⊕q V2 with probability one and p(v1, v2) = 1
q2 ,

5) p(x|u123u12u13u23, v, s) =

p(xi|siu123uijuikvi).

(cid:89)

i,j,k∈{1,2,3}

j<k

Remark 1. Suppose the source (S1, S2, S3) has no univariate common part. Consider the set of such sources that can be
transmitted either using linear codes or using the CES scheme. This set is included in the set of sources that satisfy (2)-(5).
Outline of the proof: Consider only the special case where S3 = S1 ⊕q S2 and there is no univariate common part. In
q and an n × n
Codebook Generation: For each sequence si, deﬁne vi(si) = siG ⊕q bi, where all the additions and multiplications
j=1 p(xi,j|si,jvi,j). Index them by

this situation, set Ti = Si and ﬁx probability mass function p(xi|sivi), where i = 1, 2, 3. Generate b1, b2 ∈ Fn
matrix G with elements selected randomly, uniformly and independently from Fq. Set b3 = b1 ⊕q b2.
are modulo-q. For each sequence si, vi ∈ Fn
xi(si, vi).

q independently generate xi according to (cid:81)n

Encoding: Given the sequence si, encoder i ﬁrst ﬁnds vi(si), then sends xi(si, vi(si)).
Decoding: Upon

(˜s, v1(˜s1), v2(˜s2), v3(˜s3), ˜x1, ˜x2, ˜x3, y) ∈ A(n)
were found.

that
, where ˜xi = xi(˜si, vi(˜si)). An error is declared, if no unique (˜s1, ˜s2, ˜s3)
We show in Appendix C that the probability of error approaches zero as n → ∞, if (2)-(5) are satisﬁed. For a general

from the

receiving

channel,

˜s1, ˜s2

the

decoder

ﬁnds

y

and

˜s3

such



(S1, S2, S3) the proof follows by the above argument and the proof of Preposition 1.

V. IMPROVEMENTS OVER THE CES SCHEME

In this section, through an example, we show that Theorem 1 strictly enlarges the class of correlated sources that can be

transmitted reliably using linear codes or the CES scheme. We introduce a setup consisting of a source triple and a MAC.
Example 2. Consider binary sources S1, S2, S3, where S1 and S3 are independent and S3 = S1 ⊕2 S2. Let S1 ∼ Be(σ) and
S3 ∼ Be(γ), where σ, γ ∈ [0, 1]. Consider the MAC in Figure 1, where the input alphabets are binary. N is independent of
other random variables and is distributed according to Table I, where 0 ≤ δ ≤ 1

2 , δ (cid:54)= 1
4.

TABLE I

DISTRIBUTION OF N

N
PN

0
− δ

1
2

1
1
2

2
δ

3
0

Fig. 1. The diagram the setup introduced in Example 2. Note the input alphabets of this MAC are restricted to {0, 1}.

For this setup, we show that there exist a σ and γ whose corresponding sources in Example 2 cannot be transmitted reliably

using the CES scheme. However, based on Theorem 1, this sources can be reliably transmitted.
Remark 2. Let σ = 0. In this case, S1 = 0 and S2 = S3, with probability one. From Proposition 1, (S1, S2, S3) can be
transmitted using the CES strategy, as long as h(γ) ≤ 2 − H(N ).

We ﬁnd a γ, in Remark 2, such that h(γ) = 2− H(N ). Since 2− H(N ) ≤ 1, we can calculate h−1(2− H(N )). This gives

two candidates for γ. We select the one that is less than 1/2 and denote it by γ∗.

NS1∼Be(σ)S2=S1⊕2S3S3∼Be(γ)X1X2X3Y24Enc.1Enc.2Enc.3By Remark 2, the source (S1, S2, S3) with σ = 0 and γ = γ∗ can be transmitted using the CES scheme. However, we argue
that for small enough  the source (S1, S2, S3) with σ =  and γ = γ∗ −  cannot be transmitted using this scheme (Lemma
1). Whereas, from Theorem 1, this source can be transmitted reliably (Lemma 2).
Lemma 1. Consider the setup in Example 2. ∃  > 0 such that for any σ > 0 and γ ≥ γ∗ − , the source (S1, S2, S3)
corresponding to σ and γ cannot be transmitted using the three-user CES strategy.
Lemma 2. ∃ (cid:48) > 0 such that for any σ ≤ (cid:48) and |γ − γ∗| ≤ (cid:48), the source (S1, S2, S3) corresponding to σ and γ, as in
Example 2, can be transmitted.

The proof of Lemma 1 and 2 is given in Appendix A and B, respectively.

Remark 3. Consider  and (cid:48) as in Lemma 1 and 2, respectively. Take (cid:48)(cid:48) = min{, (cid:48)}. As a result of these lemmas, the source
(S1, S2, S3) corresponding to σ = (cid:48)(cid:48) and γ = γ∗ − (cid:48)(cid:48) can be transmitted reliably while it cannot be transmitted using the
CES scheme.

Transmission of correlated sources over thee-user MAC was investigated in this paper and an extension of the CES strategy
was presented for this problem. We characterized sufﬁcient conditions for which reliable transmission of correlated sources is
possible using this scheme. Then by proposing a new coding technique, we enlarged the set of sources that can be transmitted
reliably.

VI. CONCLUSION

APPENDIX A

PROOF OF LEMMA 1

Proof: We ﬁrst derive an outer bound for the CES scheme. Consider the fourth inequality in Preposition 1. Since σ > 0
there is no common part. Let U(cid:48) = U123U12U13U23. Suppose the source (S1, S2, S3) in Example 2 can be transmitted using
the CES, then the following holds

h(γ) + h(σ) ≤ max

I(X1X2X3; Y |U(cid:48)

),

p(u(cid:48))p(x|u(cid:48)s)

where

p(s, x, u(cid:48)

) = p(s)p(u(cid:48)

)p(x1|s1, u(cid:48)

)p(x2|s2, u(cid:48)

)p(x3|s3, u(cid:48)

).

It can be shown that the right-hand side in (6) is equivalent to
h(γ) + h(σ) ≤ max
p(x|s)

I(X1X2X3; Y ),

(6)

(7)

where p(s, x) = p(s)p(x1|s1)p(x2|s2)p(x3|s3).
Next, we argue that the right-hand side in (7) is strictly less than h(γ∗) = 2− H(N ). For the moment assume this argument
is true. Then by the bound above, h(γ) + h(σ) < h(γ∗). This implies that ∃0 > 0 such that for any σ, h(γ∗) − h(γ) > 0.
Hence, as the entropy function is continuous, ∃ > 0 such that any source with σ > 0 and γ ≥ γ∗ −  cannot be transmitted
using the CES scheme.
It remains to show that the right-hand side in (7) is strictly less than 2 − H(N ). Note I(X1, X2, X3; Y ) = H(Y ) − H(N ).
Hence, we need to show H(Y ) < 2. We proceed by ﬁnding all the necessary and sufﬁcient conditions on p(x1, x2, x3) for
which Y is uniform over Z4. Then we show that since the distributions taken for maximization in (7) do not satisfy these
conditions.
2 ⊕4 X3 = i) = q(i) where i = 1, 2, 3, 4.
Since X(cid:48)

2 and X3 are binary, q(3) = 0. Given the distribution of N is Table I, the distribution of Y is as follows:

From Figure 1, Y = (X1 ⊕2 X2) ⊕4 X3 ⊕4 N. Denote X(cid:48)

2 = X1 ⊕2 X2. Let P (X(cid:48)

P (Y = 0) = q(0)(

1
2

P (Y = 2) = q(0)δ + q(2)(

− δ) + q(2)δ,
− δ),

1
2

P (Y = 1) = q(0)

P (Y = 3) = q(2)

− δ)

1
2

1
2
1
2

+ q(1)(

+ q(1)δ

Assume δ (cid:54)= 1
q(0) = q(2) = 1

4. By comparing the ﬁrst and third bounds, we can show that Y is uniform, if and only if q(1) = 0 and
2. Note

q(1) = P (X(cid:48)

2 = 0, X3 = 1) + P (X(cid:48)

2 = 1, X3 = 0)

Therefore, q(1) = 0 implies that X3 = X(cid:48)
q(2) = P (X3 = 1). Since q(0) = q(2) = 1
only if 1) X3 = X1 ⊕2 X2. 2) X3 is uniform over {0, 1}.

2 with probability one. If this condition is satisﬁed, then q(0) = P (X3 = 0) and
2 then X3 is uniform over {0, 1}. To sum up, we proved that Y is uniform, if and

Note the distributions given in the CES scheme for this case satisfy the Markov chain X3 − S3 − X1, X2. Hence, we can
show for these distributions, the condition X3 = X1 ⊕2 X2 hold if and only if X3 is a function of S3. However, as γ < 1/2,
X3 cannot be uniform over {0, 1}. This contradicts with the second condition.

APPENDIX B

PROOF OF LEMMA 2

Proof: For the setup in Example 2, the bounds given in Theorem 1 are simpliﬁed to

(8)
(9)
(10)
(11)
Set Xi = Vi, i = 1, 2, 3, where the distribution of these random variables are given in Theorem 1. One can verify that the

h(γ) ≤ I(X2X3; Y |X1S1V1)
h(σ) ≤ I(X1X2; Y |X3S3V3)
h(γ) + h(σ) − h(σ ∗ γ) ≤ I(X1X3; Y |X2S2V2)

h(γ) + h(σ) ≤ I(X1X2X3; Y ).

source corresponding to σ = 0 and γ = γ∗ satisﬁes the above inequalities and therefore can be transmitted.
to conditional density p(x|s, v). Hence, one can show that ∀0 > 0, there exist a conditional density p(x|s, v) such that

No that all the terms in (8)-(11) are entropy functions and mutual information. Therefore, they are continuous with respect

where η() is function of  such that η() → 0 as  → 0.

Note also that the left-hand sides in (8)-(11) are continuous in σ and γ. Hence ∃(cid:48) > 0 such that when σ ≤ (cid:48),|γ − γ∗| ≤ (cid:48),

we have

I(X2X3; Y |X1S1V1) ≥ 2 − H(N ) − η(0)
I(X1X2; Y |X3S3V3) ≥ 0
I(X1X3; Y |X2S2V2) ≥ 2 − H(N ) − η(0)
I(X1X2X3; Y ) ≥ 2 − H(N ) − η(0),

h(γ) ≤ 2 − H(N ) − η(0)
h(σ) ≤ 0

h(γ) + h(σ) − h(σ ∗ γ) ≤ 2 − H(N ) − η(0)
h(γ) + h(σ) ≤ 2 − H(N ) − η(0)

This implies that the source corresponding to σ ≤ (cid:48), γ ≤ γ∗ − (cid:48) can be transmitted reliably and the proof is complete.

APPENDIX C

PROOF OF THEOREM 1

Proof: There are two error events, E0 and E1. E0 occurs if no ˜s was found. E1 is declared if ˜s (cid:54)= s. To show that E0
is small, we need the next lemma. Suppose vi() and vi() are a realization of random functions generated as in the outline of
the proof of Theorem 1.
Lemma 3. Suppose si, i = 1, 2, 3 are jointly typical with respect to PS. Then

(cid:0)v1(s1), v2(s2), v3(s3), x1(s1, v1(s1)), x2(s2, v2(s2)), x3(s3, v3(s3))(cid:1) ∈ A(n)

(V1V2V3X1X2X3|s1s2s3).



Proof: The proof is straightforward.

As a result, the sequences si, vi, xi, i = 1, 2, 3 are jointly typical with y with respect to PS,V,X,Y . This implies that P (E0)
0). For a given s ∈ A(S), using the deﬁnition of E1 and the union

approaches 0 as n → ∞. Next, we calculate P (E1 ∩ Ec
bound we obtain,

1{vi = vi(si), xi = xi(si, vi), i = 1, 2, 3} (cid:88)

P (E1 ∩ Ec

p(y|x)

0|s) ≤ (cid:88)
(cid:88)

(v,x)∈A(V,X|s)

1{˜vj = vj(˜sj), ˜xj = xj(˜sj, ˜vj), j = 1, 2, 3}

y∈A(Y |x)

Taking expectation over random functions Xi(, ) and Vi() gives,

pe(s) = E{P (E1|s)} ≤

p(y|x)

(˜s,˜v,˜x)∈A(S,V ,X|y)

˜s(cid:54)=s

(cid:88)

(cid:88)

˜s(cid:54)=s

(v,x,y)∈A(V ,X,Y |s)
P{vl = Vl(sl), xl = Xl(sl, vl), ˜vl = Vl(˜sl), ˜xl = Xl(˜sl, ˜vl) for l = 1, 2, 3}

(˜s,˜v,˜x)∈A(S,V ,X|y)

(12)

Note that Vi() and Xi( , ) are generated independently. So the most inner term in (12) is simpliﬁed to
P{vj = Vj(sj), ˜vj = Vj(˜sj) j = 1, 2}P{xl = Xl(sl, vl), ˜xl = Xl(˜sl, ˜vl) l = 1, 2, 3}.

(13)
Note j = 3 is redundant because, v3 = v1 ⊕q v2 and ˜v3 = ˜v1 ⊕q ˜v2. By deﬁnition, Vj(sj) = sjG + Bj, j = 1, 2, where
B1, B2 are uniform and independent of G. Then

P{vj = Φj(sj), ˜vj = Φj(˜sj) j = 1, 2} =

1

q2n P{(˜sj − sj)G = ˜vj − vj, j = 1, 2}

(14)

The following lemma determines the above term.
Lemma 4. Suppose elements of G are generated randomly and uniformly from Fq. If s1 or s2 is nonzero, the following holds:

 q−n1{vj = 0},

q−n1{v1 = v2},
q−2n,

if sj = 0
if s1 (cid:54)= 0, s2 (cid:54)= 0, s1 = s2.
if otherwise.

P{sjG = vj, j = 1, 2} =

Outline of the proof: We can write sjG =(cid:80)n

G. Not that Gi are independent random variables with uniform distribution over Fn
over Fn
X is independent of Y and is uniform over Fq, then X ⊕q Y is also uniform over Fq and is independent of Y .

i=1 sjiGi, where sji is the ith component of sj and Gi is the ith row of
q . Hence, if sj (cid:54)= 0, then sjG is uniform
q . If s1 (cid:54)= s2, one can show that s1G is independent of s2G. The proof follows by arguing that if a random variables
Finally, we are ready to characterize the conditions in which pe → 0. We divide the last summation in (12) into the following
Case 1, ˜s1 (cid:54)= s1, ˜s2 = s2: In this case, using Lemma 4, (14) equals to q−3n1{˜v2 = v2}. Therefore, (12) is simpliﬁed to

cases:

p(y|x)

q−3nP{xl = Xl(sl, vl), ˜xl = Xl(˜sl, ˜vl) l = 1, 2, 3}.

(cid:88)

(cid:88)

pe1 (s) :=

(v,x,y)∈A(V ,X,Y |s)

(˜s,˜v,˜x)∈A(S,V ,X|y)

˜s(cid:54)=s,˜s2=s2,˜v2=v2

Note that Xl(sl, vl) is independent of Xk(˜sk, ˜vk), if l (cid:54)= k or sl

(cid:54)= ˜sl or vl
2nH(Xl|SlVl)). As s2 = ˜s2 and v2 = ˜v2, then X2(˜s2, ˜v2) = X2(s2, v2). Therefore,

(cid:54)= ˜vl. Moreover, P{xl = Xl(sl, vl)} ≈

P{xl = Xl(sl, vl), ˜xl = Xl(˜sl, ˜vl) l = 1, 2, 3} = 2

−n[2H(X1|S1V1)+H(X2|S2V2)+2H(X3|S3V3)]1{˜x2 = x2}.

Hence, we have:

pe1(s) ≈ 2nH(V ,X|S)2nH(S1,V1,X1,S3,V3,X3|Y S2V2X2) 1

−n[2H(X1|S1V1)+H(X2|S2V2)+2H(X3|S3V3)].

q3n 2

Note that H(V , X|S) = 2 log2 q +(cid:80)3

i=1 H(Xi|Si, Vi). Therefore, pe1 → 0, if

H(S1, V1, X1, S3, V3, X3|Y S2V2X2) ≤ log2 q + H(X1|S1V1) + H(X3|S3V3)

(15)
The right-hand side in the above inequality equals to H(X1X3V1V3|S1S2S3X2V2). We simplify the left-hand side. Observe
that

H(S1, V1, X1, S3, V3, X3|Y S2V2X2) = H(V1, X1, V3, X3|Y S2V2X2) + H(S1|S2V X),

where Y is removed from the second term, because conditioned on X, Y is independent of S1. Note that

H(S1|S2V X) = H(S1|S2X2V2) − I(S1; X1V1X3V3|S2V2X2) = H(S1|S2) − I(S1; X1V1X3V3|S2V2X2).

Therefore, using the above argument the inequality in (15) is simpliﬁed to

H(S1|S2) ≤ I(S1; X1V1X3V3|S2V2X2) − H(V1, X1, V3, X3|Y S2V2X2) + H(X1X3V1V3|S1S2S3X2V2)

= I(X1V1X3V3; Y |S2V2X2) = I(X1X3; Y |S2V2X2.)

Case 2, ˜s1 = s1, ˜s2 (cid:54)= s2: A similar argument as in the ﬁrst case gives H(S2|S1) ≤ I(X2X3; Y |S1V1X1).

Case 3, ˜s1 (cid:54)= s1, ˜s2 (cid:54)= s2, ˜s1 ⊕q ˜s2 = s1 ⊕q s2: Using Lemma 4,

P{vj = Φj(sj), ˜vj = Φj(˜sj) j = 1, 2} = q−3n1{˜v1 ⊕q ˜v2 = v1 ⊕q v2}

Therefore, the above probability is nonzero only when ˜v3 = v3. Hence, as s3 = ˜s3, we get X3(˜s3, ˜v3) = X3(s3, v3). This
implies that,

P{xl = Xl(sl, vl), ˜xl = Xl(˜sl, ˜vl) l = 1, 2, 3} = 2

−n[2H(X1|S1V1)+2H(X2|S2V2)+H(X3|S3V3)]1{˜x3 = x3}.

As a result, (12), in this case, is simpliﬁed to :

pe3 (s) ≈ 2nH(S1,V1,X1,S2,V2,X2|Y S3V3X3)q−n2

−n[H(X1|S1V1)+H(X2|S2V2)].

Therefore, pe3 → 0, if H(S1, V1, X1, S2, V2, X2|Y S3V3X3) ≤ H(X1, X2, V1, V2|S1S2S3V3X3). Using a similar argument

as in the ﬁrst case, this inequality is equivalent to H(S1S2|S3) ≤ I(X1, X2; Y |S3V3X3).

Case 4, ˜si (cid:54)= si, i = 1, 2, 3: Observe that,

P{vj = Φj(sj), ˜vj = Φj(˜sj) j = 1, 2} = q−4n
−2n(cid:80)3
P{xl = Xl(sl, vl), ˜xl = Xl(˜sl, ˜vl) l = 1, 2, 3} = 2
Therefore, (12), in this case, is simpliﬁed to pe4(s) ≈ q−2n2nH(S,V ,X|Y )2−n(cid:80)3
Finally, note that Pe(s) ≤(cid:80)4
l=1 H(Xl|SlVl). As a result, one can show that
Pe4 → 0, if H(S1S2S3) ≤ I(X1X2X3; Y ).
Pe approaches zero as n → ∞, if the following bounds are satisﬁed:

i=1 Pei(s). Moreover, Pei(s) depends on s only through its PMF. Therefore, for any typical s,

l=1 H(Xl|SlVl).

H(S1|S2) ≤ I(X1X3; Y |S2V2X2)
H(S2|S1) ≤ I(X2X3; Y |S1V1X1)

H(S1S2|S1 ⊕q S2) ≤ I(X1X2; Y |S1 ⊕q S2, V3X3)

H(S1, S2) ≤ I(X1X2X3; Y ).

REFERENCES

[1] C. E. Shannon, “Two-way communication channels,” in Proc. 4th Berkeley Symp. Math. Statist. Prob., Univ. California, pp. 611-644,1961.
[2] T. Cover, A.E. Gamal, M. Salehi, “Multiple access channels with arbitrarily correlated sources,” IEEE Transactions on Inf. Theory , vol.26, no.6, pp.648-

657, Nov 1980.

[3] R. Ahlswede, “Multi-way communication channels,” in Proc. 2nd Int. Symp. Information Theory, Tsahkadsor, S.S.R. Armenia, 1971, pp. 23–52.
[4] H. Liao, “A coding theorem for multiple access communications,” in Proc. Int. Symp. Information Theory, Asilomar, CA, 1972,
[5] D. Slepian and J. Wolf, “Noiseless coding of correlated information sources,” IEEE Trans. Inf. Theory, vol. IT-19, no. 4, pp. 471–480, Jul. 1973.
[6] D. Slepian and J. K. Wolf, “A coding theorem for multiple-access channels with correlated sources,” Bell Syst. Tech. J., vol. 52, pp. 1037–1076, Sep.

[7] G. Dueck, “A note on the multiple-access channel with correlated sources,” IEEE Trans. Inf. Theory, vol. IT-27, no. 2, pp. 232–235, Mar. 1981.
[8] R. Ahlswede and T. S. Han, “On source coding with side information via a multiple access channel and related problems in multiuser information theory,”

IEEE Trans. Inf. Theory, vol. IT-29, no. 3, pp. 396–412, May 1983.

[9] T. S. Han and M. H. M. Costa, “Broadcast channels with arbitrarily correlated sources,” IEEE Trans. Inf. Theory, vol. IT-33, no. 5, pp. 641–650, Sep.

[10] M. Salehi and E. Kurtas, “Interference channels with correlated sources,” in Proc. IEEE Int. Symp. Information Theory, San Antonio, TX, Jan. 1993, p.

1973.

1987.

208.

[11] S.S. Pradhan, Suhan Choi, K. Ramchandran, “A Graph-Based Framework for Transmission of Correlated Sources Over Multiple-Access Channels,” IEEE

[12] Suhan Choi, S.S. Pradhan, “A Graph-Based Framework for Transmission of Correlated Sources Over Broadcast Channels,” IEEE Trans. on Inf. Theory,

Trans. on Inf. Theory, vol.53, no.12, pp.4583-4604, Dec. 2007.

vol.54, no.7, pp.2841-2856, July 2008.

[13] B. A. Nazer and M. Gastpar, “Computation over multiple-access channels”, IEEE Trans. on Inf. Theory, Oct. 2007.
[14] A. Padakandla and S.S. Pradhan, “Computing sum of sources over an arbitrary multiple access channel,” IEEE International Symposium on Information

Theory Proceedings (ISIT), 2013 , pp.2144-2148, July 2013.

[15] M. Heidari, F. Shirani and S. Pradhan, “ Beyond group capacity in multi-terminal communications”, 2015
[16] H. S.Witsenhausen, “On sequences of pairs of dependent random variables,” SIAM J. Appl. Math., vol. 28, pp. 100–113, Jan. 1975.
[17] M. Heidari, F. Shirani, S.S. Pradhan, “New Sufﬁcient Conditions for Multiple-Access Channel with Correlated Sources,” http://arxiv.org, 2016.
[18] A. Padakandla, A. Ghasemian Sahebi, S. Pradhan, “Achievable rate region for 3-user interference channel based on coset codes,” IEEE Trans. on Inf.

Theory, vol.PP, no.99, pp.1-1.

