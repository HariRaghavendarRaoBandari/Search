6
1
0
2

 
r
a

 

M
4
1

 
 
]

O
C

.
t
a
t
s
[
 
 

1
v
6
6
1
4
0

.

3
0
6
1
:
v
i
X
r
a

The Normal Law Under Linear Restrictions:

Simulation and Estimation via Minimax Tilting

The University of New South Wales, botev@unsw.edu.au

Z. I. Botev

Abstract

Simulation from the truncated multivariate normal distribution in high di-
mensions is a recurrent problem in statistical computing, and is typically only
feasible using approximate MCMC sampling. In this article we propose a mini-
max tilting method for exact iid simulation from the truncated multivariate nor-
mal distribution. The new methodology provides both a method for simulation
and an eﬃcient estimator to hitherto intractable Gaussian integrals. We prove
that the estimator possesses a rare vanishing relative error asymptotic property.
Numerical experiments suggest that the proposed scheme is accurate in a wide
range of setups for which competing estimation schemes fail. We give an appli-
cation to exact iid simulation from the Bayesian posterior of the probit regression
model.

1 Introduction

More than a century ago Francis Galton (1889) observed that he scarcely knows “any-
thing so apt to impress the imagination as the wonderful form of cosmic order ex-
pressed by the law of frequency of error. The law would have been personiﬁed by the
Greeks if they had known of it.”

In this article we address some hitherto intractable computational problems related

to the d-dimensional multivariate normal law under linear restrictions:

f (z) = 1

ℓ exp(cid:16)− 1

2z⊤z(cid:17) I{l 6 Az 6 u},

z = (z1, . . . , zd)⊤, A ∈ Rm×d,

Aesthetic considerations aside, the problem of estimating ℓ or simulating from f (z)

u, l ∈ Rm ,
(1)
where I{·} is the indicator function, rank(A) = m 6 d, and ℓ = P(l 6 AZ 6 u) is the
probability that a random vector Z with standard normal distribution in d-dimensions
(that is, Z ∼ N(0, Id)) falls in the H-polytope deﬁned by the linear inequalities.
arises frequently in various contexts such as: Markov random ﬁelds (Bolin and Lindgren,
2015); inference for spacial processes (Wadsworth and Tawn, 2014); likelihood esti-
mation for max-stable processes (Huser and Davison, 2013; Genton et al., 2011); com-
putation of simultaneous conﬁdence bands (Aza¨ıs et al., 2010); uncertainty regions
for latent Gaussian models (Bolin and Lindgren, 2015); ﬁtting mixed eﬀects models
with censored data (Gr¨un and Hornik, 2012); and probit regression (Albert and Chib,
1993), to name a few.

For the reasons outlined above, the problem of estimating ℓ accurately has received
considerable attention. For example, Craig (2008); Miwa et al. (2003); Gassmann

1

(2003); Genz (2004); Hayter and Lin (2012, 2013) and Nomura (2014b) consider ap-
proximation methods for special cases (orthant, bivariate, or trivariate probabilities)
and Geweke (1991); Genz (1992); Joe (1995); Vijverberg (1997); S´andor and Andr´as
(2004); Nomura (2014a) consider estimation schemes applicable for general ℓ. Exten-
sive comparisons amongst the numerous proposals in the literature (Genz and Bretz,
2009; Gassmann et al., 2002; Genz and Bretz, 2002) indicate the method of Genz (1992)
is the most accurate across a wide range of test problems of medium and large dimen-
sions. Even in low dimensions (d 6 7), the method compares favorably with highly
specialized routines for orthant probabilities (Miwa et al., 2003; Craig, 2008). For this
reason, Genz’ method is the default choice across diﬀerent software platforms like
Fortran, Matlab

r and R.

One of the goals of this article is to propose a new methodology, which not only
yields an unbiased estimator orders of magnitude less variable than the Genz estimator,
but also works reliably in cases where the Genz estimator and other alternatives fail to
deliver meaningful estimates (e.g., relative error close to 100%) 1.

The obverse to the problem of estimating ℓ is simulation from the truncated mul-
tivariate normal f (z). Despite the close relation between the two problems, they have
rarely been studied concurrently (Botts, 2013; Chopin, 2011; Fern´andez et al., 2007;
Philippe and Robert, 2003). Thus, another goal of this article is to provide an exact
accept-reject sampling scheme for simulation from f (z) in high dimensions, which
traditionally calls for approximate MCMC simulation. Such a scheme can either ob-
viate the need for Gibbs sampling (Fern´andez et al., 2007), or can be used to accel-
erate Gibbs sampling through the blocking of hundreds of highly dependent variables
(Chopin, 2011). Unlike existing algorithms, the accept-reject sampler proposed in this
article enjoys high acceptance rates in over one hundred dimensions, and takes about
the same time as one cycle of Gibbs sampling.

The gist of the method is to ﬁnd an exponential tilting of a suitable importance
sampling measure by solving a minimax (saddle-point) optimization problem. The op-
timization can be solved eﬃciently, because it exploits log-concavity properties of the
normal distribution. The method permits us to construct an estimator with a tight deter-
ministic bound on its relative error and a concomitant exact stochastic conﬁdence in-
terval. Our importance sampling proposal builds on the celebrated Genz construction,
but the addition of the minimax tilting ensures that the new estimator enjoys theoreti-
cally better variance properties than the Genz estimator. In an appropriate asymptotic
tail regime, the minimax tilting yields an estimator with vanishing relative error (VRE)
property (Kroese et al., 2011). Within the light-tailed exponential family, Monte Carlo
estimators rarely possess the valuable VRE property (L’Ecuyer et al., 2010) and as yet
no estimator of ℓ with such properties has been proposed. The VRE property implies,
for example, that the new accept-reject instrumental density converges in total varia-
tion to the target density f (z), rendering sampling in the tails of the truncated normal
distribution asymptotically feasible. In this article we focus on the multivariate normal
law due to its central position in statistics, but the proposed methodology can be easily
generalized to other multivariate elliptic distributions.

1 Matlab

r

and

R

implementations

are

available

http://www.mathworks.com/matlabcentral/fileexchange/53796,
TruncatedNormal),
itory
as
http://web.maths.unsw.edu.au/˜zdravkobotev/

as well

(under

name

the

2

from Matlab

and
the
from the

r

Central,
repos-
author’s website:

CRAN

2 Background on Separation of Variables Estimator

We ﬁrst brieﬂy describe the separation of variables (SOV) estimator of Genz (1992)
(see also Geweke (1991)). Let A = LQ⊤ be the LQ decomposition of the matrix A,
where L is m×d lower triangular with nonnegative entries down the main diagonal and
Q⊤ = Q−1 is d × d orthonormal. A simple change of variable x ← Q⊤z then yields:

ℓ = P(l 6 LZ 6 u) =Zl6Lx6u

φ(x; 0, I) dx,

where φ(x; µ, Σ) denotes the pdf of the N(µ, Σ) distribution. For simplicity of notation,
we henceforth assume that m = d so that L is full rank. The case of m < d is considered
later in the experimental section. Genz (1992) decomposes the region C = {x : l 6
Lx 6 u} sequentially as follows:
l1
˜l1
L11
l2 − L21x1

u1
def
= ˜u1
L11
u2 − L21x1

def
= ˜u2(x1)

˜l2(x1) def

6 x1 6

6 x2 6

=

def
=

L22

L22

˜ld(x1, . . . , xd−1) def

=

ld −Pd−1

j=1 Ld jx j
Ldd

6 xd 6

ud −Pd−1

j=1 Ld jx j
Ldd

def

= ˜ud(x1, . . . , xd−1)

...

This decomposition motivates the separation of variables estimator of ℓ

φ(X; 0, I)

g(X)

,

X ∼ g(x)

bℓ =

where g is an importance sampling density over the set C and in the SOV form

g(x) = g1(x1)g2(x2 | x1)· · · gd(xd | x1, . . . , xd−1),

x ∈ C .

(2)

(3)

k = 1, . . . , d

We denote the measure corresponding to g by P0. The Genz SOV estimator, which we

denote by ˚ℓ to distinguish it from the more generalbℓ, is obtained by selecting for all
(4)
Denoting by Φ(·) the cdf of the standard normal distribution, this gives the following.
Algorithm 2.1 (SOV estimator)
Require: The lower triangular L such that A = LQ⊤, bounds l, u, and uniform se-

gk(xk | x1, . . . , xk−1) ∝ φ(xk; 0, 1) × I{˜lk 6 xk 6 ˜uk}

quence U1, . . . , Ud−1
for k = 1, 2, . . . , d − 1 do

iid

∼ U(0, 1).

Simulate Xk ∼ N(0, 1) conditional on ˜lk(X1, . . . , Xk−1) 6 Xk 6 ˜uk(X1, . . . , Xk−1)
using the inverse transform method. That is, set

Xk = Φ−1(cid:16)Φ(˜lk) + Uk(cid:16)Φ(˜uk) − Φ(˜lk)(cid:17)(cid:17) .

3

return ˚ℓ =

dYk=1hΦ(˜uk(X1, . . . , Xk−1)) − Φ(˜lk(X1, . . . , Xk−1))i.

The algorithm can be repeated n times to obtain the iid sample ˚ℓ1, . . . , ˚ℓn used for the
construction of the unbiased point estimator ¯ℓ = (˚ℓ1 + · · · + ˚ℓn)/n and its approximate
95% conﬁdence interval (¯ℓ ± 1.96 × S / √n), where S is the sample standard deviation
of ˚ℓ1, . . . , ˚ℓn.

2.1 Variance Reduction via Variable Reordering

Genz and Bretz (2009) suggest the following improvement of the SOV algorithm. Let
π = (π1, . . . , πd) be a permutation of the integers 1, . . . , d and denote the corresponding
permutation matrix P so that P(1, . . . , d)⊤ = π.
It is clear that for any π we have
ℓ = P(Pl 6 PAZ 6 Pu). Hence, to estimate ℓ, one can input in the SOV Algorithm 2.1
the permuted bounds and matrix: l ← Pl, u ← Pu, and A ← PA. This results in
an unbiased estimator ˚ℓ(π) whose variance will depend on π — the order in which
this high-dimensional integration is carried out. Thus, we would like to choose the π∗
amongst all possible permutations so that

π∗ = argmin

Var(˚ℓ(π))

π

This is an intractable combinatorial optimization problem whose objective function is
not even available. Nevertheless, Genz and Bretz (2009) propose a heuristic for ﬁnding
an acceptable approximation to π∗. We henceforth assume that this variable reordering
heuristic is always applied as a preprocessing step to the SOV Algorithm 2.1 so that
the matrix A and the bounds l and u are already in permuted form. We will revisit
variable reordering in the numerical experiments in Section 5.

The main limitation of the estimator ˚ℓ (with or without variable reordering) is that
Var(˚ℓ) is unknown and its estimate S 2 can be notoriously unreliable in the sense that
the observed S 2 may be very small, while the true Var(˚ℓ) is huge (Kroese et al., 2011;
Botev et al., 2013). Such examples for which ˚ℓ fails to deliver meaningful estimates of
ℓ will be given in the numerical Section 5.

2.2 Accept-Reject Simulation

The SOV approach described above suggests that we could simulate from f (z) ex-
actly by using g(x) as an instrumental density in the following accept-reject scheme
(Kroese et al., 2011, Chapter 3).

Algorithm 2.2 (Accept-Reject Simulation from f)
Require: Supremum of likelihood ratio c = supx∈C φ(x; 0, I)/g(x).

Simulate U ∼ U(0, 1) and X ∼ g(x), independently.
while cU > φ(X; 0, I)/g(X) do

Simulate U ∼ U(0, 1) and X ∼ g(x), independently.

return X, an outcome from the truncated multivariate normal density f in (1).

Of course, the accept-reject scheme will only be usable if the probability of accep-
tance P0(cU 6 φ(X; 0, I)/g(X)) = ℓ/c is high and simulation from g is fast. Thus,

4

this scheme presents two signiﬁcant challenges which need resolution. The ﬁrst one
is the computation of the constant c (or a very tight upper bound of it) in ﬁnite time.
Locating the global maximum of the likelihood ratio φ(x; 0, I)/g(x) may be an in-
tractable problem — a local maximum will yield an incorrect sampling scheme. The
second challenge is to select an instrumental g so that the acceptance probability is
not prohibitively small (a “rare-event” probability). Unfortunately, the obvious choice
(4) resolves neither of these challenges (Hajivassiliou and McFadden, 1998). Other
accept-reject schemes (Chopin, 2011), while excellent in one and two dimensions, ul-
timately have acceptance rates of the order O(21−d) rendering them unusable for this
type of problem with, say, d = 100. We now address these issues concurrently in the
next section.

3 Minimax Tilting

Exponential tilting is a prominent technique in simulation (L’Ecuyer et al., 2010; Kroese et al.,
2011). For a given light-tailed probability density h(y) on R, we can associate with h its
exponentially tilted version hµ(y) = exp (µy − K(µ)) h(y), where K(µ) = ln E exp(µX) <
∞, for some µ in an open set, is the cumulant generating function. For example,
the exponentially tilted version of φ(x; 0, I) is exp(cid:0)µ⊤x − K(µ)(cid:1) φ(x; 0, I) = φ(x; µ, I).
Similarly, the tilted version of (4) yields

gk(xk; µk | x1, . . . , xk−1) =

φ(xk; µk, 1) × I{˜lk 6 xk 6 ˜uk}
Φ(˜uk − µk) − Φ(˜lk − µk)

(5)

To simplify the notation in the subsequent analysis, let

ψ(x; µ) def

= −x⊤µ + kµk2

2

+Xk

ln(cid:16)Φ(˜uk(x1, . . . , xk−1) − µk) − Φ(˜lk(x1, . . . , xk−1) − µk)(cid:17)

(6)

= Qd

Pµ, where Pµ is the measure with pdf g(x; µ) def

Then, the tilted version of estimator (2) can be written asbℓ = exp (ψ(X; µ)) with X ∼
k=1 gk(xk; µk | x1, . . . , xk−1). It is now
clear that the statistical properties ofbℓ depend on the tilting parameter µ. There is a
large literature on the best way to select the tilting parameter µ; see L’Ecuyer et al.
(2010) and the references therein. A recurrent theme in all works is the eﬃciency of
the estimatorbℓ in a tail asymptotic regime where ℓ ↓ 0 is a rare-event probability —

precisely the setting that makes current accept-reject schemes ineﬃcient. Thus, before
we continue, we brieﬂy recall the three widely used criteria for assessing eﬃciency in
estimating tail probabilities.

The weakest type of eﬃciency and the most commonly encountered in the design
of importance sampling schemes (Kroese et al., 2011) is logarithmic eﬃciency. The

estimatorbℓ is said to be logarithmically or weakly eﬃcient if

ln Var(bℓ)

ln ℓ2

> 1

lim inf

ℓ↓0

5

The second and stronger type of eﬃciency is bounded relative error,

lim sup

ℓ↓0

Var(bℓ)
bℓ2

6 const. < ∞.

Finally, the best one can hope for in an asymptotic regime is the highly desirable
vanishing relative error (VRE) property:

= 0 .

lim sup

ℓ↓0

Var(bℓ)
bℓ2

An estimator is strongly eﬃcient if it exhibits either bounded relative error or VRE. In
order to achieve one of these eﬃciency criteria, most methods (L’Ecuyer et al., 2010)
rely on the derivation of an analytical asymptotic approximation to the relative error

Var(bℓ)/ℓ2, whose behavior is then controlled using the tilting parameter. The strongest

type of eﬃciency VRE is uncommon for light-tailed probabilities, and is typically only
achieved within a state-dependent importance sampling framework (L’Ecuyer et al.,
2010).

Here we take a diﬀerent tack, one that exploits features unique to the problem at
hand and that will yield eﬃciency gains in both an asymptotic and non-asymptotic
regime. A key result in this direction is the following Lemma 3.1, whose proof is
given in the appendix.

Lemma 3.1 (Minimax Tilting) The optimization program

ψ(x; µ)

inf
µ

sup
x∈C

is a saddle-point problem with a unique solution given by the concave optimization
program:

(x∗, µ∗) = argmax

x,µ

ψ(x; µ)

subject to:

∂ψ

∂µ

= 0,

x ∈ C

(7)

Note that (7) minimizes with respect to µ the worst-case behavior of the likelihood
ratio, namely supx∈C exp (ψ(x; µ)). The lemma states we can both easily locate the
global worst-case behavior of the likelihood ratio, and simultaneously locate (in ﬁnite
computing time) the global minimum with respect to µ. Prior to analyzing the theo-
retical properties of minimax tilting, we ﬁrst explain how to implement the minimax
method in practice.

Practical Implementation. How do we ﬁnd the solution of (7) numerically? With-
out the constraint x ∈ C , the solution to (7) would be obtained by solving the non-
linear system of equations ∇ψ(x; µ) = 0, where the gradient is with respect to the
vector (x, µ). To show why this is the case, we introduce the following notation. Let

6

D = diag(L), ˘L = D−1L, and

def
=

Ψ j

φ(˜l j; µ j, 1) − φ(˜u j; µ j, 1)
P(˜l j − µ j 6 Z 6 ˜u j − µ j)

,

(˜l j − µ j)φ(˜l j; µ j, 1) − (˜u j − µ j)φ(˜u j; µ j, 1)

=

def
=

Ψ′j

∂Ψ j
∂µ j

P(˜l j − µ j 6 Z 6 ˜u j − µ j)
Then, the gradient equation ∇ψ(x; µ) = 0 can be written as

− Ψ2
j .

∂ψ
∂x

= −µ + ( ˘L⊤ − I)Ψ = 0,

∂ψ

∂µ

= µ − x + Ψ = 0 ,

(8)

and the Jacobian matrix has elements:

∂2ψ
∂µ2

= I +diag(cid:0)Ψ′(cid:1) ,

∂2ψ
∂µ∂x

= ( ˘L− I)diag(Ψ′)− I,

∂2ψ
∂x2

= ( ˘L− I)⊤diag(cid:0)Ψ′(cid:1) ( ˘L− I) .

(9)
The Karush-Kuhn-Tucker equations give the necessary and suﬃcient condition for the
global solution (x∗, µ∗) of (7):

∂ψ/∂µ = 0,
η1 > 0,
Lx − u 6 0,
η2 > 0, −Lx + l 6 0,

∂ψ/∂x − ˘L⊤η1 + ˘L⊤η2 = 0
η⊤1 (Lx − u) = 0
η⊤2 (Lx − l) = 0,

(10)

where η1, η2 are Lagrange multipliers.

Suppose we ﬁnd the unique solution of the nonlinear system (8) using, for example,
a trust-region Dogleg method (Powell, 1970). If we denote the solution to (8) by (˘x, ˘µ),
then the Karush-Kuhn-Tucker equations imply that (˘x, ˘µ) = (x∗, µ∗) if and only if
(˘x, ˘µ) ∈ C or equivalently η1 = η2 = 0. If, however, the solution (˘x, ˘µ) to (8) does not
lie in C , then (˘x; ˘µ) will be suboptimal and, in order to compute (x∗; µ∗), one has to use
a constrained convex optimization solver. This observation then leads to the following
procedure.

Algorithm 3.1 (Computation of optimal pair (x∗, µ∗))

Use Powell’s (1970) Dogleg method on (8) with Jacobian (9) to ﬁnd (˘x, ˘µ).
if (˘x, ˘µ) ∈ C then
else

(x∗, µ∗) ← (˘x, ˘µ)
Use a convex solver to ﬁnd (x∗, µ∗), where (˘x, ˘µ) is the initial guess.

return (x∗, µ∗)

Numerical experience suggests almost always (˘x, ˘µ) happens to lie in C and there is
no need to do any additional computation over and above Powell’s (1970) trust-region
method.

7

4 Theoretical Properties of Minimax Tilting

X ∼ Pµ∗ ,

There are a number of reasons why the minimax program (7) is an excellent way of
selecting the tilting parameter. The ﬁrst one shows that, unlike its competitors, the
proposed estimator,

achieves the best possible eﬃciency in a tail asymptotic regime.

bℓ = exp(cid:0)ψ(X; µ∗)(cid:1) ,
Let Σ = AA⊤ be a full rank covariance matrix. Consider the tail probability ℓ(γ) =
P(X > γl), where X ∼ N(0, Σ) and γ > 0,
l > 0. We show that the estimator (11)
exhibits strong eﬃciency in estimating ℓ(γ) as γ ↑ ∞. To this end, we ﬁrst introduce
the following simplifying notation.
Similar to the variable reordering in Section 2.1, suppose that P is a permutation
matrix which maps the vector (1, . . . , d)⊤ into the permutation π = (π1, . . . , πd)⊤, that
is, P(1, . . . , d)⊤ = π. Let L be the lower triangular factor of PΣP⊤ = LL⊤ and p = Pl.
It is clear that

(11)

ℓ(γ) = P(PX > γPl) = P(LZ > γp)

for any permutation π. For the time being, we leave π unspeciﬁed, because unlike in
Section 2.1, here we do not use π to minimize the variance of the estimator, but to
simplify the notation in our eﬃciency analysis.

Deﬁne the convex quadratic programming problem:

min

1
2kxk2
subject to: Lx > γp

x

(12)

The Karush-Kuhn-Tucker equations, which are a necessary and suﬃcient condition to
ﬁnd the solution of (12), are given by:

x − L⊤λ = 0
λ > 0, γp − Lx 6 0
λ⊤(γp − Lx) = 0 ,

(13)

where λ ∈ Rd is a Lagrange multiplier vector. Suppose the number of active constraints
in (12) is d1 and the number of inactive constraints is d2, where d1 + d2 = d. Note that
since Lx > γp > 0, the number of active constraints d1 > 1, because otherwise x = 0
and Lx = 0, reaching a contradiction.

Given the partition λ = (λ⊤1 , λ⊤2 )⊤ with dim(λ1) = d1 and dim(λ2) = d2, we now
choose π such that all the active constraints in (13) correspond to λ1 > 0 and all the
inactive ones to λ2 = 0. Similarly, we deﬁne a partitioning for x, p, and the lower
triangular

L =  L11 O

L21 L22 ! .

Note that the only reason for introducing the above variable reordering via the
permutation matrix P and insisting that all active constraints of (12) are collected in
the upper part of vector λ is notational convenience and simplicity. At the cost of
some generality, this preliminary variable reordering allows us to state and prove the

8

eﬃciency result in the following Theorem 4.1 in its simplest and neatest form.

Theorem 4.1 (Strong Eﬃciency of Minimax Estimator) Consider the estimation of
the probability

ℓ(γ) = P(X > γl) = P(LZ > γp)

where X ∼ N(0, Σ), Z ∼ N(0, I); and LL⊤ = PΣP⊤, p = Pl > 0 are the permuted
versions of Σ, l ensuring that the Lagrange multiplier vector λ in (13) satisﬁes λ1 > 0
and λ2 = 0. Deﬁne

q def

= L21L−1

11 p1 − p2

and let J be the set of indices for which the components of the vector q are zero, that
is,

def

J

= { j : q j = 0,

j = 1, . . . , d2}

(14)

If J = ∅, then the minimax estimator (11) is a vanishing relative error estimator:

lim supγ↑∞

Varµ∗ (bℓ(γ))

ℓ2(γ)

= 0 .

Alternatively, if J , ∅, thenbℓ is a bounded relative error estimator:

lim supγ↑∞

Varµ∗ (bℓ(γ))

ℓ2(γ)

< const. < ∞.

The theorem suggests that, unless the covariance matrix Σ has a very special struc-
ture, the estimator enjoys VRE. This raises the question: Is there a simple setting that
guarantees VRE for any full-rank covariance matrix under any preliminary variable
reordering?

The next result shows that when l can be represented as a weighted linear combi-
nation of the columns of the covariance matrix Σ = AA⊤, then we always have VRE.

Theorem 4.2 (Minimax Vanishing Relative Error) Consider the estimation of the
tail probability ℓ(γ) = P(γl 6 AZ 6 ∞), where l = Σl∗ for some positive weight
l∗ > 0. Then, the minimax estimator (11) is a vanishing relative error estimator.

In contrast, under the additional assumption L⊤l∗ > 0 (strong positive covari-
ance), where L is the lower triangular factor of Σ = LL⊤, the SOV estimator ˚ℓ is a
bounded relative error estimator; otherwise, it is a divergent one2:

Var0(exp(ψ(X;0)))

ℓ2(γ)

O(1),
exp(O(γ2) + O(ln γ) + O(1)),

if L⊤l∗ > 0
otherwise

.

≃

Note that the permutation matrix P plays no role in the statement of Theorem 4.2 (we
can assume P = I), and that we do not assume l > 0, but only that l = Σl∗ for some
l∗ > 0.

In light of Theorems 4.1 and 4.2, for the obverse problem of simulation from the

truncated multivariate normal, we obtain the following result.

2The symbols f (x) ≃ g(x), f (x) = O(g(x)), and f (x) = o(g(x)), as x ↑ ∞ and g(x) , 0, stand for

f (x)/g(x) = 0, respectively.

limx↑∞

f (x)/g(x) = 1, lim supx↑∞ | f (x)/g(x)| < ∞, and limx↑∞

9

Corollary 4.1 (Asymptotically Eﬃcient Simulation) Suppose that the instrumental
density in the Accept-Reject Algorithm 2.2 for simulation from

f (z) ∝ φ(z; 0, I) × I{Az > γl},

is given by g(x; µ∗). Suppose further that, either l > 0 and the corresponding esti-
mator (11) enjoys VRE, or l = Σl∗ for some l∗ > 0. Then, the measure Pµ∗ becomes
indistinguishable from the target P:

A |P(Z ∈ A ) − Pµ∗(Z ∈ A )| → 0,
sup

γ ↑ ∞.

A second reason that recommends our choice of tilting parameter is that exp (ψ(x∗; µ∗))

is a nontrivial deterministic upper bound to ℓ, that is, ℓ 6 exp(ψ(x∗; µ∗)).

As a result, unlike many existing estimators (Vijverberg, 1997; Genz, 1992), we
can construct an exact (albeit conservative) conﬁdence interval for ℓ as follows. Let
ε > 0 be the desired width of the 1− α conﬁdence interval and ℓL 6 ℓ be a lower bound
to ℓ. Then, by Hoeﬀding’s inequality for ¯ℓ = (bℓ1 + · · · +bℓn)/n with
n(ε) =(cid:6) − ln(α/2) × (exp(ψ(x∗; µ∗)) − ℓL)2/(2ε2)(cid:7),
we obtain: Pµ∗(¯ℓ − ε 6 ℓ 6 ¯ℓ + ε) > 1 − α.
As is widely-known (Kroese et al., 2011), the main weakness of any importance
sampling estimator ¯ℓ of ℓ is the risk of severe underestimation of ℓ. Thus, plugging ¯ℓ
(or even more conservatively, plugging zero) in place of ℓL in the formula for n above
will yield a robust conﬁdence interval (¯ℓ ± ε). For practitioners who are not satisﬁed
with such a heuristic approach, we provide the following deterministic lower bound to
ℓ.

(15)

Lemma 4.1 (Cross Entropy Lower Bound) Deﬁne the product measure P with pdf

φ(x) ∝ φ(x; ν, diag2(σ)) × I{l 6 x 6 u} ,

where ν and σ = (σ1, . . . , σd)⊤ are location and scale parameters, respectively. Deﬁne

ℓL = sup
ν,σ

exp(cid:16)− 1

2tr(Σ−1Var(X)) − 1

2

E[X]⊤Σ−1E[X] − E[ln φ(X)](cid:17)

,

(2π)d/2| det(A)|

where Σ = AA⊤. Then, ℓL 6 ℓ is a variational lower bound to ℓ. In addition, under the
conditions of Theorem 4.2, namely, (l, u) = (γΣl∗, ∞) , we have that ℓL ↑ ℓ(γ) and

A |P(Z ∈ A ) − P(A−1Z ∈ A )| ↓ 0,
sup

γ ↑ ∞ .

(16)

Since simulation from P is straightforward, one may be tempted to consider using P as
an alternative importance measure to Pµ∗. Unfortunately, despite the similarity of the
results in Theorem 4.2 and Lemma 4.1, the pdf φ is not amenable to an accept-reject
scheme for exact sampling from f and as an importance sampling measure it does not
yield VRE. Thus, the sole use of Lemma 4.1 is for constructing an exact conﬁdence
interval and lower bound to ℓ in the tails of the normal distribution.

10

Note that under the conditions of Theorem 4.2, the minimax estimator enjoys the
bounded normal approximation property (Tuﬃn, 1999). That is, if ¯ℓ and S 2 are the
mean and sample variance of the iidbℓ1, . . . ,bℓn, and Fn(x) is the empirical cdf of Tn =
√n(¯ℓ − ℓ)/S , then we have the Berry–Ess´een bound, uniformly in γ:

sup

x∈R,γ>0 |Fn(x) − Φ(x)| 6 const./ √n

This Berry–Ess´een bound implies that the coverage error of the approximate (1 − α)
level conﬁdence interval ¯ℓ±z1−α/2×S / √n remains of the order O(n−1/2), even as ℓ ↓ 0.
Thus, if a lower bound ℓL is not easily available, one can still rely on the conﬁdence
interval derived from the central limit theorem.

Finally, in addition to the strong eﬃciency properties of the estimator, another
reason that recommends the minimax estimator is that it permits us to tackle intractable
simulation and estimation problems as illustrated in the next section.

5 Numerical Examples and Applications

We begin by considering a number of test cases used throughout the literature (Fern´andez et al.,
2007; Craig, 2008; Miwa et al., 2003). We are interested in both the eﬃcient sim-
ulation of the Gaussian vector X = AZ ∼ N(0, Σ) conditional on X ∈ A , and the
estimation of ℓ in (1).
In all examples we compare the separation-of-variables (SOV) estimator of Genz
with the proposed minimax-exponentially-tilted (MET) estimator. We note that ini-
tially we considered a comparison with other estimation schemes such as the radially
symmetric approach of Nomura (2014a) and the specialized orthant probability algo-
rithm of Miwa et al. (2003); Craig (2008); Nomura (2014b). Unfortunately, unless a
special autoregressive covariance structure is present, these methods are hardly com-
petitive in anything but very few dimensions. For example, the orthant algorithm of
Miwa et al. (2003) has complexity O(d! × n), which becomes too costly for d > 10.
For this reason, we give a comparison only with the broadly applicable SOV scheme,
which is widely recognized as the current state-of-the-art method.

Since both the SOV and MET estimators are smooth, one can seek further gains in
eﬃciency using randomized quasi Monte Carlo. The idea behind quasi Monte Carlo
is to reduce the error of the estimator by using quasirandom or low-discrepancy se-
quences of numbers, instead of the traditional (pseudo-) random sequences. Typically
the error of a sample average estimator decays at the rate of O(n−1/2) when using ran-
dom numbers, and at the rate of O((ln n)d/n) when using pseudorandom numbers; see
Gerber and Chopin (2015) for an up-to-date discussion.

For both the SOV and MET estimator we use the n-point Richtmyer quasirandom
sequence with randomization, as recommended by Genz and Bretz (2009). The ran-
domization allows us to estimate the variability of the estimator in the standard Monte
Carlo manner. The details are summarized as follows.

Algorithm 5.1 (Randomized Quasi Monte Carlo (Genz and Bretz, 2009))
Require: Dimension d and sample size n.

d′ ← ⌈5d ln(d + 1)/4⌉,

n′ ← ⌈ n
12⌉

11

Let p1, . . . , pd′ be the ﬁrst d′ prime numbers.
qi ← √pi × (1, . . . , n′)⊤ for i = 1, . . . , d′
for k = 1, . . . , 12 do

for i = 1, . . . , d − 1 do

Let U ∼ U(0, 1), independently.
si ← |2 × [(qi + U) mod 1] − 1|

qms ← (s1, . . . , sd−1)
Use the sequence qms to compute an n′-point sample average estimatorbℓk.

12Pkbℓk with estimated relative error 1

return ¯ℓ ← 1
Note that, since there is no need to integrate the xd-th component, the loop over i goes
up to d − 1.
5.1 Structured Covariance Matrices

12qPk(bℓk − ¯ℓ)2. ¯ℓ.

At this junction we assume that the matrix A (or equivalently Σ) and the bounds l and
u have already been permuted according to the variable reordering heuristic discussed
in Section 2.1. Thus, the ordering of the variables during the integration will be the
same for both estimators and will not matter in the comparison.

Example I (Fern´andez et al., 2007). Consider A = [1/2, 1]d with a covariance ma-
trix

Σ−1 =

I +

11⊤

1
2

1
2

Columns three and four in Table 1 show the estimates of ℓ for various values of d. The
brackets give the estimated relative error in percentage.

Figure 1: Estimates of ℓ for various values of d using n = 104 replications.

d
2
3
5
10
15
20
25
30
40
50

ℓL

0.0148955
0.0010771
2.4505 × 10−6
8.5483 × 10−15
1.3717 × 10−25
1.7736 × 10−38
2.674 × 10−53
6.09 × 10−70
2.17 × 10−108
2.1310 × 10−153

SOV

0.0148963 (4×10−4%)
0.0010772 (3×10−3%)
2.4508 × 10−6 (0.08%)
8.4591 × 10−15 (0.8%)
1.366 × 10−25 (11%)
1.65 × 10−38 (37%)
2.371 × 10−48 (33%)

-
-
-

MET

0.01489 (4×10−5%)
0.001077 (3×10−4%)
2.451 × 10−6 (0.002%)
8.556 × 10−15 (0.01%)
1.375 × 10−25 (0.01%)
1.7796 × 10−38 (0.03%)
2.6847 × 10−53 (0.02%)
6.11 × 10−70 (0.03%)
2.18 × 10−108 (0.05%)
2.1364 × 10−153 (0.06%)

exp (ψ(x∗; µ∗))

0.0149
0.00108
2.48 × 10−6
2.1046 × 10−14
1.43 × 10−25
1.869 × 10−38
2.83 × 10−53
6.46 × 10−70
2.30 × 10−108
2.24 × 10−153

worst err.
2 × 10−4%
6 × 10−3%
0.012%
0.03%
0.04%
0.05%
0.05%
0.05%
0.06%
0.05%

accept pr.

0.99
0.99
0.98
0.97
0.95
0.95
0.94
0.94
0.94
0.95

The second column shows the lower bound discussed in Lemma 4.1 and column
ﬁve shows the deterministic upper bound. These two bounds can then be used to
compute the exact conﬁdence interval (mentioned in the previous section) whenever
we allow n to vary freely. Here, since n is ﬁxed and the error is allowed to vary, we
instead display the upper bound to the relative error (given in column six under the
“worst err.” heading)

qVar(¯ℓ)/ℓ 6 (exp(cid:0)ψ(x∗; µ∗)(cid:1) /ℓL − 1)/ √n.

12

Finally, column seven (accept pr.) gives the acceptance rate of Algorithm 2.2 when
using the instrumental density g(· ; µ∗) with enveloping constant c = exp (ψ(x∗; µ∗)).
What makes the MET approach better than other methods? First, the acceptance
rate in column seven remains high even for d = 50. In contrast, the acceptance rate
from naive acceptance-rejection with instrumental pdf φ(0, Σ) is a rare-event probabil-
ity of approximately 2.13 × 10−153. Note again that the existing accept-reject scheme
of Chopin (2011) is an excellent algorithm designed for extremely fast simulation in
one or two dimensions (in quite general settings) and is not suitable here.

Second, the performance of both the SOV and MET estimators gradually deterio-
rates with increasing d. However, the SOV estimator has larger relative error, does not
give meaningful results for d > 25, and possesses no theoretical quantiﬁcation of its
performance. In contrast, the MET estimator is guaranteed to have better relative error
than the one given in column six (worst err.).

Finally, in further numerical experiments (not displayed here) we observed that the
width, ε, of the exact conﬁdence interval, ¯ℓ ± ε with α = 0.05, based on the Hoeﬀding
bound (15), was of the same order of magnitude as the width of the approximate
conﬁdence interval ¯ℓ ± z1−α/2 × S / √n(ε).
Example II (Fern´andez et al., 2007). Consider the hypercube A = [0, 1]d and the
isotopic covariance with elements

(Σ−1)i, j =

1
2|i− j| × I{|i − j| 6 d/2} .

Figure 2: Estimates of ℓ for various values of d using n = 104 replications.

d
2
3
10
20
25
50
80
100
120
150
200
250

ℓL

0.09114
0.02303
1.338 × 10−6
1.080 × 10−12
9.770 × 10−16
5.925 × 10−31
3.252 × 10−49
2.18 × 10−61
1.462 × 10−73
8.026 × 10−92
2.954 × 10−122
1.087 × 10−152

SOV

0.09121 (6×10−4%)
0.02307 (0.001%)

1.3493 × 10−6 (0.03%)
1.0982 × 10−12 (0.23%)
1.00 × 10−15 (0.28%)
6.137 × 10−31 (0.7%)
3.477 × 10−49 (1.8%)
2.351 × 10−61 (3%)
1.641 × 10−73 (5.6%)
9.751 × 10−92 (6.3%)
3.581 × 10−122 (11%)
1.359 × 10−152 (15%)

MET

0.09121 (2×10−4%)
0.02307 (4×10−4%)
1.3490 × 10−6 (0.003%)
1.0989 × 10−12 (0.004%)
9.9808 × 10−16 (0.02%)
6.188 × 10−31 (0.05%)
3.479 × 10−49 (0.1%)
2.384 × 10−61 (0.2%)
1.622 × 10−73 (0.3%)
9.142 × 10−92 (0.18%)
3.525 × 10−122 (0.5%)
1.357 × 10−152 (0.6%)

exp (ψ(x∗; µ∗))

0.09205
0.0234

1.454 × 10−6
1.289 × 10−12
1.222 × 10−15
9.368 × 10−31
6.812 × 10−49
5.50 × 10−61
4.45 × 10−73
3.23 × 10−91
1.905 × 10−121
1.120 × 10−151

worst err.
0.009%
0.01%
0.07%
0.17%
0.2%
0.5%
1.0%
1.3%
1.7%
2.5%
4.4%
7.2%

accept pr.

0.99
0.98
0.92
0.85
0.81
0.66
0.50
0.43
0.36
0.28
0.18
0.12

Observe how rapidly the probabilities become very small. Why should we be in-
terested in estimating small “rare-event” probabilities? The simple answer is that all
probabilities become eventually rare-event probabilities as the dimensions get larger
and larger, making naive accept-reject simulation infeasible. These small probabilities
sometimes present not only theoretical challenges (rare-event estimation), but prac-
tical ones like representation in ﬁnite precision arithmetic and numerical underﬂow.
For instance, in using the SOV estimator Gr¨un and Hornik (2012) note that: “Numer-
ical problems arise for very small probabilities, e.g. for observations from diﬀerent
components. To avoid these problems observations with a small posterior probability
(smaller than or equal to 10−6) are omitted in the M-step of this component.” The MET
estimator is not immune to numerical underﬂow and loss of precision during compu-

13

tation, but consistent with Theorems 4.1 and 4.2, it is typically much more robust than
the SOV estimator in estimating small probabilities.

5.2 Random Correlation Matrices

One can argue that the covariance matrices we have considered so far are too structured
and hence not representative of a “typical” covariance matrix. Thus, for simulation
and testing Miwa et al. (2003) and Craig (2008) ﬁnd it desirable to use random corre-
lation matrices. In the subsequent examples we use the method of Davies and Higham
(2000) to simulate random test correlation matrices whose eigenvalues are uniformly
distributed over the simplex {x : x1 + · · · + xd = d}.
Example III. A natural question is whether the MET estimator would still be prefer-
able when integrating over a “non-tail” region such as A = [−1/2, ∞]100. The table
below summarizes the output of running the algorithms on 100 independently simu-
lated random correlation matrices. Both the SOV and MET estimators used n = 105
quasi Monte Carlo points. The ‘accept rate’ row displays the ﬁve number summary of
the estimated acceptance probability of Algorithm 2.2.

Figure 3: Table: ﬁve number summary for relative error based on 100 independent
replications; Graph: boxplots of these 100 outcomes on logarithmic scale.

MET
SOV

accept rate

min
0.07%
0.27%
1.2%

1-st quartile median
0.17%
1.00%
5.5%

0.12%
0.63%
3.9%

3-rd quartile

0.20%
1.68%
7.3%

max
0.44%
9.14%
12%

-1

-1.5

-2

-2.5

)
.
r
r
e

.
l
e
r
(
0
1
g
o
l

-3

1

MET

2SOV

So far we have said little about the cost of computing the optimal pair (x∗; µ∗), and
the measures of eﬃciency we have considered do not account for the computational
cost of the estimators. The reason for this is that in the examples we investigated, the
computing time required to ﬁnd the pair (x∗; µ∗) is insigniﬁcant compared to the time

it takes to evaluate n > 105 replications ofbℓ or ˚ℓ.

14

In the current example, the numerical experiments suggest that the MET estimator
If one adjusts the results in
is roughly 20% more costly than the SOV estimator.
Figure 3 in order to account for this time diﬀerence, then the relative error in the
SOV row would be reduced by a factor of at most 1.2. This adjustment will thus give a
reduction in the typical (median) relative error from 1.0 to 1/1.2 ≈ 0.83 percent, which
is hardly signiﬁcant.

Example IV. Finally, we wish to know if the strong eﬃciency described in Theo-
rem 4.1 may beneﬁt the MET estimator as we move further into the tails of the distri-
bution. Choose the “tail-like” A = [1, ∞]100 and use n = 105. The following table
and graph summarize the results of 100 replications.

Figure 4: Relative errors of SOV and MET estimators over 100 random correlation
cases.

MET
SOV

accept rate

min

0.020%
4.3%
1.5%

1-st quartile median
0.077%

0.044%

15%
10%

26%
18%

3-rd quartile

0.12%
48%
26%

max
0.44%
99%
43%

0

-0.5

-1

-1.5

-2

-2.5

-3

-3.5

)
.
r
r
e

.
l
e
r
(
0
1
g
o
l

1

MET

2SOV

As seen from the results, in this particular example the variance of the MET esti-
mator is typically more than 105 times smaller than the variance of the SOV estimator.

5.3 Computational Limitations In High Dimensions

It is important to emphasize the limitations of the minimax tilting approach. Like all
other methods, including MCMC, it is not a panacea against the curse of dimension-
ality. The acceptance probability of Algorithm 2.2 ultimately becomes a rare-event
probability as the dimensions keep increasing, because the bounded or vanishing rela-

tive error properties ofbℓ do not hold in the asymptotic regime d ↑ ∞.

Numerical experiments suggest that the method generally works reliably for d 6
100. The approach may sometimes be eﬀective in higher dimensions provided ℓ does

15

not decay too fast in d.
orthant probability ℓ = P(X ∈ [0, ∞]d) with the positive correlation structure

In this regard, Miwa et al. (2003); Craig (2008) study the

Σ =

1
2

I +

1
2

11⊤ .

This is a rare case for which the exact value of the probability is known, namely ℓ =
1/(d + 1), and decays very slowly to zero as d ↑ ∞. For this reason, we use it to
illustrate the behavior of the SOV and MET estimators for very large d.
Figure 5 shows the output of a numerical experiment with n = 105 for various
values of d. The graph on the left gives the computational cost in seconds. Both the
SOV and the MET estimators have cost of O(d3) — hence the excellent agreement
with the least squares cubic polynomials ﬁtted to the empirical CPU data. The table
on the right displays the relative error for both methods. In this example, we apply the
variable reordering heuristic to the SOV estimator only, illustrating that the heuristic
is not always necessary to achieve satisfactory performance with the MET estimator.

Figure 5: Graph: computational cost in seconds; Table: relative error in percentage;

x 104

3.5

)
.
c
e
s
(

e
m

i
t

U
P
C

3

2.5

2

1.5

1

0.5

0
0

SOV

MET

1000

2000

3000

4000

5000

6000

7000

8000

9000 10000

d

SOV

d MET
10
30
50
100
300
500
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000

0.0063% 0.0076%
0.080%
0.053%
0.090%
0.038%
0.15%
0.29%
1.4%
0.11%
2.0%
0.21%
3.0%
0.26%
0.18%
3.9%
4.8%
0.35%
8.6%
0.26%
12%
0.33%
0.28%
7.5%
11%
0.21%
8.3%
0.29%
15%
0.28%
0.24%
12%

This example conﬁrms the result in Theorem 4.2 that the SOV estimator works
better in settings with strongly positive correlation structure (but poorly with nega-
tive correlation). Further, the results suggest the MET estimator is also aided by the
presence of positive correlation.

5.4 Exact Simulation of Probit Posterior

A popular GLM (Koop et al., 2007) for binary responses y = (y1, . . . , ym)⊤ with ex-
planatory variables xi = (1, xi2, . . . , xik)⊤, i = 1, . . . , m is the probit Bayesian model:

16

β0 = 0;

2(β − β0)⊤V−1(β − β0)(cid:17) with β ∈ Rk and for simplicity

• Prior: p(β) ∝ exp(cid:16)− 1
• Likelihood: p(y| β) ∝ exp(cid:16)Pm
The challenge is to simulate from the posterior p(β | y). One can use latent variables
(Albert and Chib, 1993) to represent the posterior as the marginal of a truncated mul-
tivariate normal. Let λ ∼ N(0, Im) be latent variables and deﬁne the design matrix
˜X = diag(2y − 1)X. Then, the marginal f (β) of the joint pdf

i=1 ln Φ(cid:16)(2yi − 1)x⊤i β(cid:17)(cid:17).

f (β, λ) ∝ exp(cid:16)− 1

2kV−1/2βk2 − 1

2kλk2(cid:17) I{ ˜Xβ − λ > 0}

equals the desired posterior p(β | y). We can thus apply our accept-reject scheme,
because the joint f (β, λ) is of the desired truncated multivariate form (1) with d = k +m
and

z =" V−1/2β

λ

# , A =(cid:16) ˜XV 1/2, −I(cid:17) ,

l = 0,

u = +∞.

Figure 6: Marginal distribution of β computed from 8000 exact iid realizations.

1

0.5

0

-0.5

-1

-1.5

-2

Const.

Male

Year

Kids

Relig.

Ed.

Happy

As an numerical example, we apply the probit model to the widely studied extra-
marital aﬀairs dataset from Koop et al. (2007). The dataset contains m = 601 inde-
pendent observations: the binary response yi indicates if the i-th respondent has had
an extramarital aﬀair; the six explanatory variables (k = 7) are male indicator (Male),
number of years married (Year), ‘has’ or ‘has not’ children (Kids), religious or not
(Relig.), years of formal education (Ed.), and a binary variable denoting whether the
marriage is happy or not (Happy). Figure 6 shows the boxplots of the marginal dis-
tributions of β1, . . . , β7 based on 8000 iid simulations from the posterior p(β | y) with
prior covariance V = 5I.

17

The conclusion that only years of marriage, religiosity, and conjugal happiness
are statistically signiﬁcant is, of course, well known (Koop et al., 2007) and used to
validate our new simulation scheme. The question is what have we gained in using
minimax tilting?

On the one hand, for the ﬁrst time we have conducted the Bayesian inference using
exact iid samples from the posterior and we did not have to fret about unquantiﬁable
issues such as ‘burn-in’ and ‘mixing-speed’ as is typical with approximate MCMC
simulation (Philippe and Robert, 2003).

On the other hand, the acceptance rate in the simulation was 1/217, that is, we
had to simulate (on average) 217 random vectors to accept one as an exact indepen-
dent realization from the posterior. Admittedly, this acceptance rate could have been
better and as shown in the previous experiments it is going to deteriorate with increas-
ing dimensionality. However, there are hardly any alternatives for exact sampling —
naive acceptance rejection for the extramarital data would enjoy an acceptance rate of
O(10−146) and without minimax tilting (say, with proposal g(x; 0)) the Accept-Reject
Algorithm 2.2 enjoys an acceptance rate of O(10−16).

Thus, our main point stands: the proposed accept-reject scheme can be used for
exact simulation whenever, say d 6 100, and when d is in the thousands it can be used
to accelerate Gibbs sampling by grouping or blocking dozens of highly correlated
variables together (Chopin, 2011; Philippe and Robert, 2003).

Concluding Remarks

The minimax tilting method can be eﬀective for exact simulation from the truncated
multivariate normal distribution. The proposed method permits us to dispense with
Gibbs sampling in dimensions less than 100, and for larger dimensions to accelerate
existing Gibbs samplers by sampling jointly hundreds of highly correlated variables.

The minimax approach can also be used to estimate normal probability integrals.
Theoretically, the method improves on the already excellent SOV estimator and in a
tail asymptotic regime it can achieve the best possible eﬃciency — vanishing relative
error. The numerical experiments suggest that the proposed method can be signiﬁ-
cantly more accurate than the widely used SOV estimator, especially in the tails of the
distribution. The experiments also point out to its limitations — as the dimensions get
larger and larger it eventually fails.

The minimax tilting approach in this article can be extended to other multivariate
densities related to the normal. Upcoming work by the author will argue that signif-
icant eﬃciency gains are also possible in the case of the multivariate student-t and
general elliptic distributions for which a strong log-concavity property holds. Just as
in the multivariate normal case, the approach permits us to estimate accurately hith-
erto intractable student-t probabilities, for which existing estimation schemes exhibit
relative error close to 100%.

Acknowledgments

This work was supported by the Australian Research Council under grant DE140100993.

18

A Appendix

A.1 Proof of Lemma 3.1

First, we show that ψ is a concave function of x for any µ. To see this, note that
if Z ∼ N(0, 1) under P, then by the well-known properties of log-concave measures
(Pr´ekopa, 1973), the function q1 : R → R deﬁned as

q1(w) = ln P(l 6 Z + w 6 u) = ln 1

√2πRR exp(cid:16)− 1

2z2(cid:17) I{(Z+w)∈Z }dz ,

where Z = [l, u] is a convex set, is a concave function of w ∈ R. Hence, for an
arbitrary linear map C ∈ Rd×1, the function q2 : Rd → R deﬁned as q2(x) = q1(Cx) is
concave as well. It follows that each function

ln P(˜lk 6 Z + µk 6 ˜uk) = ln P((Z + Ckx) ∈ Zk)

(using the obvious choices of Ck and Zk) is concave in x. Hence, ψ is concave in x,
because it is a non-negative weighted sum of concave functions.

Second, we show that ψ is convex in µ for each value of x. After some simpliﬁca-

tion, we can write

ψ(x; µ) = −x⊤µ +Xk

ln E exp (µkZ) I

{˜lk 6Z6˜uk}

.

{˜lk6Z6˜uk}

Now, each of ln E exp(µkZ) I
is convex in µk, because up to a normalizing
constant, this is the cumulant generating function of a standard normal random variable
Z, truncated to [˜lk, ˜uk]. Since a non-negatively weighted sum of convex functions is
convex, we conclude that ψ(x; µ) is convex in µ. Finally, since convexity is preserved
under pointwise supremum, supx∈C ψ(x; µ) is still convex in µ. Moreover, here we
infµ supx∈C ψ(x; µ) = supx∈C infµ ψ(x; µ), from
have the strong min-max property:
which the lemma follows.

(cid:3)

A.2 Proof of Theorem 4.1

Before proceeding with the proof we note the following.

of (12) explicitly as x1 = γL−1
from (13) we can also deduce that λ1 = γL−⊤11 L−1

First, using the necessary and suﬃcient condition (13), we can write the solution
11 p1k2. In addition,
11 p1 − p2 > 0.
Second, the asymptotic behavior of ℓ(γ) = P(X > γl) has been established by
Hashorva and H¨usler (2003). For convenience, we restate their result using our sim-
pliﬁed notation.

11 p1, x2 = 0 with minimum γ2

11 p1 > 0 and q = L21L−1

2 kL−1

Proposition A.1 (Hashorva and H¨usler (2003)) Consider the tail probability ℓ(γ) =
P(X > γl), where X ∼ N(0, Σ) and γ > 0, l > 0. Deﬁne the set J as in (14). Then,
the tail behavior of ℓ(γ) as γ ↑ ∞ is
γ2
2 kL−1

11 p1k2 −

ln(cid:16)γnL−⊤11 L−1

ℓ(γ) ≃ c × exp−

11 p1ok(cid:17) ,

d1Xk=1

19

where the constant c is given by:

c =

P(Y j > 0, ∀ j ∈ J )

(2π)d1/2|L11|

,

(Y1, . . . , Yd2)⊤ ∼ N(0, L22L⊤22)

if J , ∅, and c = (2π)−d1/2|L11|−1 if J = ∅.

The last two observations pave the way to proving that, depending on the set J ,
either exp(ψ(x∗, µ∗)) = O(ℓ(γ)), or exp(ψ(x∗, µ∗)) ≃ ℓ(γ). The details of the argument
are as follows.
In the setting of Theorem 4.1, the Karusch-Kuhn-Tucker conditions (10) simplify

to:

µ − x + Ψ = 0
−µ + ( ˘L⊤ − I)Ψ + ˘L⊤η = 0
η > 0, γp − Lx 6 0
η⊤(γp − Lx) = 0

(17)

where η is a Lagrange multiplier (corresponding to η2 in (10)) and we replaced l with
γp.

Case J = ∅. We now verify by substitution that, if J = ∅, the unique solution of
(17) is of the asymptotic form

x1 ≃ ˜x1 = γL−1
11 p1
x2 ≃ ˜x2 = o(1)
µ1 ≃ ˜µ1 = −γ(D1L−⊤11 − I)L−1
11 p1
µ2 ≃ ˜µ2 = o(1)
η ≃ ˜η = o(1)
11 p1 − p2(cid:17) − L22 ˜x2 = −γq + o(1) ↓ −∞, as γ ↑ ∞.

(18)

Equation four in (17) is obviously satisﬁed, because ˜η tends to zero by assumption in

Hence, line three in (17) is also satisﬁed for suﬃciently large γ:

(18). Next, note that −γ(cid:16)L21L−1
γp1 − L11 ˜x1

γp − L˜x = 

Next, note that

γp2 − L21 ˜x1 − L22 ˜x2 ! =

−γ(cid:16)L21L−1

0

11 p1 − p2(cid:17) − L22 ˜x2  .

˜l1 = D−1
˜l2 = D−1

1 (γp1 − (L11 − D1)˜x1) = γL−1
2 (γp2 − L21 ˜x1 − (L22 − D2)˜x2) = −γD−1

11 p1 = ˜x1

2 q + o(1) ↓ −∞

Hence, from ˜l1 − ˜µ1 = γL−1
˜l2− ˜µ2 = −γD−1
0) we obtain the asymptotic behavior of Ψ:

11 p1 = D1λ1 > 0 and
2 q + o(1), and Mill’s ratio (φ(γ; 0, 1)/Φ(γ) ≃ γ and φ(−γ; 0, 1)/Φ(−γ) ↓

11 p1 + γ(D1L−⊤11 − I)L−1

11 p1 = γD1L−⊤11 L−1

Ψ1 ≃ γD1L−⊤11 L−1

11 p1,

Ψ2 = o(1) ,

20

where we recall that λ1 = γL−⊤11 L−1
that

11 p1 > 0. Equation one in (17) thus simply veriﬁes

˜x1 = Ψ1 + ˜µ1 ≃ γD1L−⊤11 L−1
˜x2 = Ψ2 + ˜µ2 = o(1)

11 p1 − γ(D1L−⊤11 − I)L−1

11 p1 = γL−1

11 p1

Equation one and two yield x = ˘L⊤Ψ = L⊤D−1Ψ, which again is easily veriﬁed:

x1 = L⊤11D−1
x2 = L⊤22D−1

1 Ψ1 + L⊤21D−1
2 Ψ2 = o(1) = ˜x2

2 Ψ2 ≃ γL−1

11 p1 = ˜x1

The asymptotic behavior of ψ∗ = ψ(x∗; µ∗) is obtained by evaluating ψ at the asymp-
totic solution (18), that is, ˜ψ

def
= ψ(˜x; ˜µ) =

= k ˜µk2

2 − ˜x⊤ ˜µ +

dXk=1

= k ˜µ1k2

2 − ˜x⊤1 ˜µ1 + O(k˜x2k2) +

ln Φ(˜lk − ˜µk) +

ln Φ(˜lk − ˜µk),
d1Xk=1

where by deﬁnition Φ(x) def

= P(Z > x)

(19)

ln Φ(−γ{D−1

2 q}k + o(1)))

d2Xk=1
2 γ2 − ln γ − 1
2 ln(2π), and ln Φ(−γ) ↑ 0 that
d1Xk=1
ln(γ{L−1
11 p1}k) + o(1)

ln(2π) −
d1Xk=1

ln(γ{D1L−1

11 L−1

11 L−1

d1
2

11 p1}k) + o(1)

It follows from Mill’s ratio, ln Φ(γ) ≃ − 1
˜ψ = k ˜µ1k2

11 L−1

11 p1k2 −

2 − ˜x⊤1 ˜µ1 −
γ2
2 kL−1

11 p1k2 −

γ2
2 kD1L−1
d1
2

ln(2π) − ln |L11| −

= −

In other words, from Proposition A.1 we have that exp( ˜ψ) ≃ ℓ(γ) as γ ↑ ∞. Therefore,
Varµ∗(bℓ)

exp(ψ(x∗; µ∗))Eµ∗ exp(ψ(X; µ∗))

Eµ∗ exp(2ψ(X; µ∗))

− 1 6

− 1

ℓ2

ℓ2

=

exp(ψ(x∗; µ∗))

6

ℓ(γ)

ℓ2
− 1 ≃

exp( ˜ψ)
ℓ(γ) − 1 = o(1) .

It follows that for J = ∅ the minimax estimator (11) exhibits vanishing relative

error — the best possible asymptotic tail behavior.

Case J , ∅. Recall that (˘x, ˘µ) is the solution of the nonlinear system (8), as well
as the optimization program (7) without its constraint x ∈ C (note that a reordering of
the variables via the permutation matrix P does not change the statement of (7) or (8)).
We have ψ(x∗; µ∗) 6 ψ(˘x; ˘µ), because dropping a constraint in the maximization of (7)
cannot reduce the maximum. As in the case of J = ∅, one can then verify via direct
substitution that

˜x1 = γL−1

11 p1,

˜x2 = O(1),

˜µ1 = −γ(D1L−⊤11 − I)L−1

11 p1,

˜µ2 = O(1)

21

is the asymptotic form of the solution to (8). In other words, ˜ψ = ψ(˜x; ˜µ) ≃ ψ(˘x; ˘µ) >
ψ(x∗; µ∗). Similar manipulations as the ones in (19) lead to ˜ψ = O(1) − γ2
11 p1k2 −
2 kL−1
d1 ln γ. An examination of Proposition A.1 when J , ∅ thus shows that exp( ˜ψ) =
O(ℓ(γ)) as γ ↑ ∞. In other words,bℓ is a bounded relative error estimator for ℓ(γ):

Varµ∗ (bℓ)

ℓ2

6

exp(ψ(x∗;µ∗))

ℓ(γ)

− 1 6

exp(ψ(˘x; ˘µ))

ℓ(γ)

− 1 ≃

exp(ψ(˜x; ˜µ))

ℓ(γ)

− 1 = O(1) .

A.3 Proof of Theorem 4.2

In the following proof we use the following multidimensional Mill’s ratio (Savage,
1962):

P(AZ>γΣl∗)

φ(γΣl∗;0,Σ) ≃ exp(cid:16)−Pk ln(γl∗k)(cid:17) ,

γ ↑ ∞ .

(20)

This is a generalization of the well-known one-dimensional result: Φ(γ)
γ , γ ↑
∞ . As in the proof of Theorem 4.1, we proceed to ﬁnd the asymptotic solution of
the nonlinear optimization program (7) by considering the necessary and suﬃcient
Karusch-Kuhn-Tucker conditions (10). In the setup of Theorem 4.2 these conditions
simplify to (replacing l with γΣl∗):

φ(γ;0,1) ≃ 1

µ − x + Ψ = 0
−µ + ( ˘L⊤ − I)Ψ + ˘L⊤η = 0
η > 0, γLL⊤l∗ − Lx 6 0
η⊤(γLL⊤l∗ − Lx) = 0

We can thus verify via direct substitution that the following

˜x = γL⊤l∗,

˜µ = γ(L⊤ − D)l∗,

˜η = o(1)

(21)

(22)

satisfy the equations (21) asymptotically. Equations three and four in (21) are satisﬁed,
because γLL⊤l∗ − L˜x = γLL⊤l∗ − LγL⊤l∗ = 0. Let us now examine equations one and
two in (21). First, note that from (22)

˜l − ˜µ = γ ˘LL⊤l∗ − ( ˘L − I)˜x − ˜µ = γDl∗ > 0

and hence from the one-dimensional Mill’s ratio we have

Ψk =

φ(˜lk − ˜µk; 0, 1)
Φ(˜lk − ˜µk)

=

φ(γDkkl∗k; 0, 1)
Φ(γDkkl∗k) ≃ γDkkl∗k,

γ ↑ ∞ .

In other words, Ψ ≃ γDl∗ as γ ↑ ∞. It follows that for equation one in (21) we obtain

˜µ − ˜x + Ψ = −γDl∗ + Ψ = o(1)

and for equation two (recall that ˘L = D−1L, so that ˘L⊤ = L⊤D−1)

− ˜µ + ( ˘L⊤ − I)Ψ + ˘L⊤ ˜η = −γ( ˘L⊤ − I)Dl∗ + ( ˘L⊤ − I)Ψ + ˘L⊤ ˜η
= ( ˘L⊤ − I)(Ψ − γDl∗) + o(1) = o(1) .

22

Thus, all of the equations in (21) are satisﬁed asymptotically and since (21) has a
unique solution, we can conclude that (x∗, µ∗) ≃ (˜x, ˜µ). We now proceed to substitute
2 − x⊤µ +Pk ln Φ(˜lk − µk). Using the one-dimensional
this pair (˜x, ˜µ) into ψ(x; µ) = kµk2
Mill’s ratio, ln Φ(γ) ≃ − 1
2 kDl∗k2 −Xk

ln Φ(γDkkl∗k) ≃ −

2 γ2 − ln γ − 1

2 ln(2π), we obtain

ln(γDkkl∗k) −

γ ↑ ∞ .

Xk

d
2

ln(2π),

γ2

As a consequence, using the fact that ln | det(L)| =Pk ln Dkk (recall that L is triangular

with positive diagonal elements), we have

˜ψ = ψ(˜x; ˜µ) = ψ(γL⊤l∗; γ(L⊤ − D)l∗)
2 kDl∗k2 +Xk

= −

γ2

ln Φ(γDkkl∗k)

1
2k˜xk2 +
γ2
2

≃ −

In other words,

(l∗)⊤LL⊤l∗ −

d
2

ln(2π) − ln | det(L)| −Xk

ln(γl∗k)

However, by Mill’s ratio (20), we also have

exp(ψ(˜x; ˜µ)) ≃ φ(γΣl∗; 0, Σ) exp(cid:16)−Pk ln(γl∗k)(cid:17) ,
P(AZ > γΣl∗) ≃ φ(γΣl∗; 0, Σ) exp(cid:16)−Pk ln(γl∗k)(cid:17) ,

γ ↑ ∞

γ ↑ ∞

It follows that exp(ψ(˜x; ˜µ)) ≃ ℓ(γ) and the minimax estimator (11) exhibits vanishing
relative error:

Varµ∗(bℓ)

ℓ2

=

Eµ∗ exp(2ψ(X; µ∗))

ℓ2

− 1 6
≃

exp (ψ(x∗; µ∗))

ℓ

exp (˜x; ˜µ))

ℓ(γ)

− 1

− 1 = o(1),

γ ↑ ∞ .

In contrast, for the SOV estimator ˚ℓ we have at most bounded relative error under

quite stringent conditions. First, the second moment on the SOV estimator satisﬁes

lim inf
γ↑∞

E0 exp (2ψ(X; 0)) > E0 lim inf
γ↑∞

exp (2ψ(X; 0))

and in considering the asymptotics of ψ(x; 0) we are free to select x to obtain the best
error behavior subject to the constraint ˘Lx > γ ˘LL⊤l∗. This gives

exp (2ψ(x; 0)) ≃ exp(cid:0)2ψ(γL⊤l∗; 0)(cid:1) ≃ 1

γ2tr(Λ) exp(cid:16)−γ2(l∗)⊤LΛL⊤l∗ − 2c1(cid:17) ,

where Λ = diag([e1, . . . , ed]) is a diagonal matrix such that ei = I{P j L jil∗j > 0} and
ln(2π) +Pk:ek =1 ln(P j L jkl∗j). It follows that the relative error of the SOV

c1 = tr(Λ)
2

23

estimator behaves asymptotically as

(2π)d/2 det(L)γd−tr(Λ) exp(cid:16) 1

2 γ2(l∗)⊤L(I − Λ)L⊤l∗ − c1 +Pk ln l∗k(cid:17) .

(cid:3)

A.4 Proof of Corollary 4.1

The corollary follows from a Pinsker-type inequality (Devroye and Gy¨orﬁ, 1985, Page
222, Theorem 2) by observing that (the expectation operator E corresponds to the
measure P):

A |P(Z ∈ A ) − Pµ∗(Z ∈ A )| =
sup

1

2Z | f (z) − g(z; µ∗)|dz
6 r1 − exp(cid:18) − E ln f (Z)
g(Z;µ∗)(cid:19)
6 p1 − ℓ(γ) exp (−ψ(x∗; µ∗))
≃ q1 − ℓ(γ) exp(cid:16)− ˜ψ(cid:17) = o(1) ,

where the last equality follows from exp( ˜ψ) ≃ ℓ(γ), which is the case when (11) is a
VRE estimator.

(cid:3)

A.5 Proof of Lemma 4.1

That ℓL is a variational lower bound follows immediately from Jensen’s inequality:

1

exp(cid:16)− 1

2tr(Σ−1Var(X)) − 1

(2π)d/2 √|Σ|
Note that if αi
then all the quantities on the left-hand side are available analytically:

φ(X) (cid:19) .
E[X]⊤Σ−1E[X] − E[ln φ(X)](cid:17) = exp(cid:18)E ln φ(X;0,Σ)
(23)
= (ui−νi)/σi, pi = Φ(αi)−Φ(βi) and φ(·) ≡ φ(· ; 0, 1),

= (ℓi−νi)/σi, βi

def

def

2

E[Xi] = νi + σi

tr(Σ−1Var(X)) =Pd
−E[ln φ(X)] =Pd

pi

φ(αi)−φ(βi)
i=1{Σ−1}i,i σ2
αiφ(αi)−βiφ(βi)

i=1

pi

i(cid:16)1 + αiφ(αi)−βiφ(βi)

−(cid:16) φ(αi)−φ(βi)
+ ln(cid:0)p2π exp(1) σi pi(cid:1)

pi

2pi

(cid:17)2(cid:17)

(24)

Next, we establish the asymptotic behavior of ℓL(γ) under the conditions of Theo-
rem 4.2. Suppose the pair (˜ν, ˜σ) satisﬁes diag2( ˜σ) ≃ Σ and ˜ν ≃ l − γdiag2( ˜σ)l∗ =
γ(Σ − diag2( ˜σ))l∗ as γ ↑ ∞. Then, α ≃ γdiag( ˜σ)l∗, which in combination with
ln Φ(γ) ≃ − 1
2 ln(2π), implies E[X] ≃ γΣl∗. Hence, substituting (˜ν, ˜σ) into

2 γ2 − ln(γ)− 1

24

(24) and then into the left-hand-side of (23), and simplifying, we obtain

ℓ(γ) > ℓL >

≃
≃

1

1

(2π)d/2 √|Σ|
(2π)d/2 √|Σ|
(2π)d/2 √|Σ|

1

2

exp − 1
exp(cid:16)− 1
exp(cid:18)− γ2

E[X]⊤Σ−1E[X] + 1

2Pi(cid:18) φ(αi)
Φ(αi)(cid:19)2
2(γΣl∗)⊤Σ−1(γΣl∗) −Pi ln(αi/ ˜σi)(cid:17)
2 (l∗)⊤Σl∗ −Pi ln(γl∗i )(cid:19) ≃ ℓ,

γ ↑ ∞

+Pi ln(√2π ˜σiΦ(αi))!

where the last asymptotic equivalence follows from (20). Finally, the convergence of
(16) follows by applying the Pinsker-type inequality (Devroye and Gy¨orﬁ, 1985) in

conjunction with r1 − exp(cid:18)−E ln φ(X)

f (X)(cid:19) = r1 − 1

ℓ exp(cid:18)E ln φ(X;0,Σ)

φ(X) (cid:19) 6 √1 − ℓL/ℓ =

(cid:3)

o(1).

References

Albert, J. H. and S. Chib (1993). Bayesian analysis of binary and polychotomous

response data. Journal of the American Statistical Association 88(422), 669–679.

Aza¨ıs, J.-M., S. Bercu, J.-C. Fort, A. Lagnoux, and P. L´e (2010). Simultaneous conﬁ-
dence bands in curve prediction applied to load curves. Journal of the Royal Statis-
tical Society: Series C (Applied Statistics) 59(5), 889–904.

Bolin, D. and F. Lindgren (2015). Excursion and contour uncertainty regions for la-
tent gaussian models. Journal of the Royal Statistical Society: Series B (Statistical
Methodology) 77(1), 85–106.

Botev, Z. I., P. L’Ecuyer, and B. Tuﬃn (2013). Markov chain importance sampling with
applications to rare event probability estimation. Statistics and Computing 23(2),
271–285.

Botts, C. (2013). An accept-reject algorithm for the positive multivariate normal dis-

tribution. Computational Statistics 28(4), 1749–1773.

Chopin, N. (2011). Fast simulation of truncated Gaussian distributions. Statistics and

Computing 21(2), 275–288.

Craig, P. (2008). A new reconstruction of multivariate normal orthant probabilities.
Journal of the Royal Statistical Society: Series B (Statistical Methodology) 70(1),
227–243.

Davies, P. I. and N. J. Higham (2000). Numerically stable generation of correlation

matrices and their factors. BIT Numerical Mathematics 40(4), 640–651.

Devroye, L. and L. Gy¨orﬁ (1985). Nonparametric density estimation: the L1 view,

Volume 119. John Wiley & Sons Inc.

Fern´andez, P. J., P. A. Ferrari, and S. P. Grynberg (2007). Perfectly random sampling
of truncated multinormal distributions. Advances in Applied Probability 39(4), 973–
990.

25

Galton, F. (1889). Natural inheritance, Volume 42. Macmillan.

Gassmann, H., I. De´ak, and T. Sz´antai (2002). Computing multivariate normal prob-
abilities: A new look. Journal of Computational and Graphical Statistics 11(4),
920–949.

Gassmann, H. I. (2003). Multivariate normal probabilities: implementing an old idea

of Plackett’s. Journal of Computational and Graphical Statistics 12(3), 731–752.

Genton, M. G., Y. Ma, and H. Sang (2011). On the likelihood function of Gaussian

max-stable processes. Biometrika 98(2), 481–488.

Genz, A. (1992). Numerical computation of multivariate normal probabilities. Journal

of computational and graphical statistics 1(2), 141–149.

Genz, A. (2004). Numerical computation of rectangular bivariate and trivariate normal

and t probabilities. Statistics and Computing 14(3), 251–260.

Genz, A. and F. Bretz (2002). Comparison of methods for the computation of mul-
tivariate t probabilities. Journal of Computational and Graphical Statistics 11(4),
950–971.

Genz, A. and F. Bretz (2009). Computation of multivariate normal and t probabilities,

Volume 195. Springer.

Gerber, M. and N. Chopin (2015). Sequential Quasi-Monte-Carlo sampling. J. R.

Statist. Soc. B 77(3), 1–44.

Geweke, J. (1991). Eﬃcient simulation from the multivariate normal and student-t
distributions subject to linear constraints and the evaluation of constraint probabil-
ities. In Computing science and statistics: Proceedings of the 23rd symposium on
the interface, pp. 571–578. Citeseer.

Gr¨un, B. and K. Hornik (2012). Modelling human immunodeﬁciency virus ribonucleic
acid levels with ﬁnite mixtures for censored longitudinal data. Journal of the Royal
Statistical Society: Series C (Applied Statistics) 61(2), 201–218.

Hajivassiliou, V. A. and D. L. McFadden (1998). The method of simulated scores for

the estimation of LDV models. Econometrica 66(4), 863–896.

Hashorva, E. and J. H¨usler (2003). On multivariate gaussian tails. Annals of the

Institute of Statistical Mathematics 55(3), 507–522.

Hayter, A. J. and Y. Lin (2012). The evaluation of two-sided orthant probabilities for

a quadrivariate normal distribution. Computational Statistics 27(3), 459–471.

Hayter, A. J. and Y. Lin (2013). The evaluation of trivariate normal probabilities
Journal of Statistical Computation and Simula-

deﬁned by linear inequalities.
tion 83(4), 668–676.

Huser, R. and A. C. Davison (2013). Composite likelihood estimation for the Brown–

Resnick process. Biometrika 100(2), 511–518.

26

Joe, H. (1995). Approximations to multivariate normal rectangle probabilities based on
conditional expectations. Journal of the American Statistical Association 90(431),
957–964.

Koop, G., D. J. Poirier, and J. L. Tobias (2007). Bayesian econometric methods, Vol-

ume 7. Cambridge University Press.

Kroese, D. P., T. Taimre, and Z. I. Botev (2011). Handbook of Monte Carlo Methods,

Volume 706. John Wiley & Sons.

L’Ecuyer, P., J. H. Blanchet, B. Tuﬃn, and P. W. Glynn (2010). Asymptotic robust-
ness of estimators in rare-event simulation. ACM Transactions on Modeling and
Computer Simulation (TOMACS) 20(1), 6.

Miwa, T., A. J. Hayter, and S. Kuriki (2003). The evaluation of general non-centred
orthant probabilities. Journal of the Royal Statistical Society: Series B (Statistical
Methodology) 65(1), 223–234.

Nomura, N. (2014a). Computation of multivariate normal probabilities with polar
coordinate systems. Journal of Statistical Computation and Simulation 84(3), 491–
512.

Nomura, N. (2014b). Evaluation of Gaussian orthant probabilities based on orthogonal

projections to subspaces. Statistics and Computing, in press.

Philippe, A. and C. P. Robert (2003). Perfect simulation of positive Gaussian distribu-

tions. Statistics and Computing 13(2), 179–186.

Powell, M. J. D. (1970). A hybrid method for nonlinear equations. Numerical methods

for nonlinear algebraic equations 7, 87–114.

Pr´ekopa, A. (1973). On logarithmic concave measures and functions. Acta Scientiarum

Mathematicarum 34, 335–343.

S´andor, Z. and P. Andr´as (2004). Alternative sampling methods for estimating multi-

variate normal probabilities. Journal of Econometrics 120(2), 207–234.

Savage, I. R. (1962). Mills’ ratio for multivariate normal distributions. J. Res. Nat.

Bur. Standards Sect. B 66, 93–96.

Tuﬃn, B. (1999). Bounded normal approximation in simulations of highly reliable

markovian systems. Journal of Applied Probability 36(4), 974–986.

Vijverberg, W. (1997). Monte Carlo evaluation of multivariate normal probabilities.

Journal of Econometrics 76(1), 281–307.

Wadsworth, J. L. and J. A. Tawn (2014). Eﬃcient inference for spatial extreme value

processes associated to log-Gaussian random functions. Biometrika 101(1), 1–15.

27

