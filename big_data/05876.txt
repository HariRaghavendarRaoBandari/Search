6
1
0
2

 
r
a

 

M
8
1
 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
6
7
8
5
0

.

3
0
6
1
:
v
i
X
r
a

Generalized support vector regression: duality

and tensor-kernel representation.

Saverio Salzo1 and Johan A.K. Suykens2

1LCSL, Istituto Italiano di Tecnologia and Massachusetts Institute of Technology

Bldg. 46-5155, 77 Massachusetts Avenue, Cambridge, MA 02139, USA

Email: saverio.salzo@iit.it

2KU Leuven, ESAT-STADIUS

Kasteelpark Arenberg 10, B-3001 Leuven (Heverlee), Belgium

Email: johan.suykens@esat.kuleuven.be

Abstract

In this paper we study the variational problem associated to support vector regression in Banach
function spaces. Using the Fenchel-Rockafellar duality theory, we give explicit formulation of the dual
problem as well as of the related optimality conditions. Moreover, we provide a new computational
framework for solving the problem which relies on a tensor-kernel representation. This analysis
overcomes the typical diﬃculties connected to learning in Banach spaces. We ﬁnally present a large
class of tensor-kernels to which our theory fully applies: power series tensor kernels. This type of
kernels describe Banach spaces of analytic functions and include generalizations of the exponential
and polynomial kernels as well as, in the complex case, generalizations of the Szegö and Bergman
kernels.

Keywords: support vector regression, regularized empirical risk, reproducing kernel Banach

spaces, tensors, Fenchel-Rockafellar duality.

1

Introduction

Support vector regression is a kernel-based estimation technique which allows to estimate
a function belonging to an inﬁnite dimensional function space based on a ﬁnite number of
pointwise observations [7, 21, 23, 24]. The (primal) problem is classically formulated as an
empirical risk minimization on a reproducing kernel Hilbert space of functions, the regulariza-
tion term being the square of the Hilbert norm. This inﬁnite dimensional optimization problem
is approached through its dual problem which turns out to be ﬁnite dimensional, quadratic
(possibly constrained), and involving the kernel function only, evaluated at the available data
points [7, 20, 24]. Therefore, the knowledge of the kernel suﬃces to completely describe and

1

solve the dual problem as well as to compute the solution of the primal (inﬁnite dimensional)
problem. This is what it is known as the kernel trick and makes support vector regression
eﬀective and so popular in applications.

Learning in Banach spaces of functions is an emerging area of research which in principle
permits to consider learning problems with more general types of norms than Hilbert norms
[5, 10, 27]. The main motivation for this generalization comes from the need of ﬁnding more
eﬀective sparse representations of data or for feature selection. To that purpose, several types
of alternative regularization schemes have been proposed in the literature, and we mention,
among others, ℓ1 regularization (lasso), elastic net, and bridge regression [8, 11]. Moreover,
the statistical consistency of such more general regularization schemes have been addressed in
[5, 6, 8, 15]. However, moving to Banach spaces of functions and Banach norms pose serious
diﬃculties from the computational point of view [22].
Indeed, even though, in this more
general setting, it is still possible to introduce appropriate reproducing kernels [27], they fail
to properly represent the solution of the dual and primal problem, so that the dual approach
becomes cumbersome. For this reason, the above mentioned estimation techniques are often
implemented by directly tackling the primal problem and therefore, as a matter of fact, reduces
to a ﬁnite dimensional estimation methods (that is to parametric models).

In this work we address support vector regression in Banach function spaces and we provide
a new computational framework for solving the associated optimization problem, overcoming
the diﬃculties we discussed above. Our model is described in the primal by means of an
appropriate feature map in Banach spaces of features and a general regularizer. We ﬁrst
study, in great generality, the interplay between the primal and the dual problem through the
Fenchel-Rockafellar duality. We obtain an explicit formulation of the dual problem, as well as
of the related optimality conditions, in terms of the feature map and the subdiﬀerentials of
the loss function and of the regularizer. As a byproduct we also provide a general representer
theorem.

Next, we consider the setting of a linear model described through a countable dictionary
of functions with the regularization term being the ℓr-norm of the related coeﬃcients, with
r = m/(m − 1) and m an even integer. This choice allows r > 1 to be close to 1 and
hence to approximate ℓ1 regularization, possibly keeping the stability properties of the ℓ2
regularization based estimation. Then we introduce a new type of kernel function which turns
to be a symmetric positive-deﬁnte tensor of order m, and we prove that it allows to formulate
the dual problem without any reference to the underlying feature map as well as to evaluate
the optimal solution function at any point in the input space. In this way, the dual problem
becomes a ﬁnite dimensional convex homogeneous m-degree-polynomial minimization problem
which can be solved by standard smooth optimization algorithms, e.g., the conjugate gradient
method. In the end, we show that the kernel trick can be fully extended to tensor-kernels and
makes the dual approach in the Banach setting still viable for computing the solution of the
primal (inﬁnite dimensional) problem. Finally, we illustrate the theoretical framework above
by presenting an entire class of tensor-kernel functions, that is power series tensor-kernels,
which are extensions of the analogue matrix-type power series kernels considered in [29]. We
show that this class includes kernels of exponential and polynomial type as well as, in the
complex case, generalizations of the Szegö and Bergman kernels.

2

The rest of the paper is organized as follows. Section 2 gives basic deﬁnitions and facts.
Section 3 presents the dual framework for SVR in general Banach spaces of features.
In
Section 4 we introduce tensor kernels and explain their role in making Banach space problems
more practical numerically. Section 5 treats tensor kernels of power series type, which give
rise to a general class of function Banach spaces to which the theory applies. Finally Section 6
contains conclusions.

2 Basic deﬁnitions and facts

Let F be a real Banach space. We denote by F ∗ its dual space and by h·, ·i the canonical
pairing between F and F ∗, meaning that, for every (w, w∗) ∈ F × F ∗, hw, w∗i = w∗(w). We
denote by k·k the norm of F as well as the norm of F∗. Let F : F → ]−∞, +∞]. The domain
of F is dom F = {w ∈ F | F (w) < +∞} and F is proper if dom F 6= ∅. Suppose that F
is proper and convex. The subdiﬀerential of F is the set-valued operator ∂F : F → 2F ∗ such
that,

(∀ w ∈ F ) ∂F (w) =(cid:8)w∗ ∈ F ∗ (cid:12)(cid:12) (∀v ∈ F ) F (w) + hv − w, w∗i ≤ F (v)(cid:9),

and its domain is dom ∂F = {w ∈ F | ∂F (w) 6= ∅}. The Fenchel conjugate of F is the
function F ∗ : F ∗ → ]−∞, +∞] : w∗ ∈ F ∗ 7→ supw∈F hw, w∗i − F (w). We denote by Γ0(F )
the set of proper, convex, and lower semicontinuous functions on F . If C ⊂ F , we denote
by ιC the indicator function of C, that is ιC : F → ]−∞, +∞], such that, for every w ∈ F ,
ιC(w) = 0 if w ∈ C, and ιC(w) = +∞ if w /∈ C. Let F ∈ Γ0(F ). Then the following duality
relation between the subdiﬀerentials of F and its conjugate F ∗ holds [26, Theorem 2.4.4(iv)]

(∀ (w, w∗) ∈ F × F ∗)

w∗ ∈ ∂F (w) ⇔ w ∈ ∂F ∗(w∗).

(2.1)

Let p ∈ [1, +∞[. The conjugate exponent of p is p∗ ∈ ]1, +∞] such that 1/p + 1/p∗ = 1.
If (Z, A, µ) is a ﬁnite measure space, we denote by h·, ·ip,p∗ the canonical pairing between the

Lebesgue spaces Lp(µ) and Lp∗(µ), i.e., hf, gip,p∗ =RZ f g dµ. If K is a countable set, we deﬁne

the sequence space

. The pairing between ℓp(K) and ℓp∗

(K) is

ℓp(K) =(cid:26)(wk)k∈K ∈ RK (cid:12)(cid:12)(cid:12)(cid:12) Xk∈K
endowed with the norm kwkp = (cid:0)Pk∈K |wk|p(cid:1)1/p
hw, w∗ip,p∗ =Pk∈K wkw∗

k.

|wk|p < +∞(cid:27)

The Banach space F is called smooth [4] if, for every w ∈ F there exists a unique w∗ ∈ F ∗
such that kw∗k = 1 and hw, w∗i = 1. The smoothness property is equivalent to the Gâteaux
diﬀerentiability of the norm on F \ {0}. We say that F is strictly convex if, for every w
and every v in F such that kwk = kvk = 1 and w 6= v, one has k(w + v)/2k < 1. Let F
be a reﬂexive, strictly convex and smooth real Banach space and let p ∈ ]1, +∞[. Then the
p-duality map of F is the mapping [4]

Jp : F → F ∗ such that (∀ w ∈ F )

hw, Jp(w)i = kwkp

and kJp(w)k = kwkp−1.

(2.2)

3

This map is a bijection from F onto F ∗ and its inverse is the p∗-duality map of F ∗. Moreover,
for every w ∈ F and every λ ∈ R+, Jp(λw) = λp−1Jp(w) and Jp(−w) = −Jp(w). The mapping
J2 is called the normalized duality map of F . The Banach space ℓp(K) is reﬂexive, strictly
convex, and smooth, and, it is immediate to verify from (2.2) that, its p-duality map is

Jp : ℓp(K) → ℓp∗

(K) : w = (wk)k∈K 7→ (|wk|p−1 sign(wk))k∈K.

(2.3)

Moreover, J −1
(2.3) with p replaced by p∗.

: ℓp∗

p

(K) → ℓp(K) is the p∗-duality map of ℓp∗

(K), hence it has the same form as

Fact 2.1 ([1, Example 13.7]). Let F be a reﬂexive, strictly convex, smooth, and real Banach
space, let p ∈ ]1, +∞[, and let ϕ ∈ Γ0(R) be even. Then (ϕ ◦ k·k)∗ = ϕ∗ ◦ k·k and

(∀ w ∈ F )

∂(ϕ ◦ k·k)(w) =


∂ϕ(kwk)
kwkp−1 Jp(x)
{w∗ ∈ F ∗ | kw∗k ∈ ∂ϕ(0)}

if w 6= 0

if w = 0.

Fact 2.2 (Fenchel-Rockafellar duality [26, Corollary 2.8.5 and Theorem 2.8.3(vi)]). Let F and
B be two real Banach spaces. Let f ∈ Γ0(F ), let g ∈ Γ0(B), and let B : F → B be a bounded

linear operator. Suppose that 0 ∈ int(cid:0)B(dom f ) − dom g(cid:1). Then the dual problem

f ∗(−B∗y∗) + g∗(y∗)

(2.4)

min
y∗∈B

admits solutions and strong duality holds, that is

inf
x∈F

f (x) + g(Bx) = − min
y∗∈B

f ∗(−B∗y∗) + g∗(y∗).

Moreover, if in addition f + g ◦ B admits a minimizer, then, for every (¯x, ¯y∗) ∈ F × B∗, ¯x is
a minimizer for f + g ◦ B and ¯y∗ is a solution of (2.4) iﬀ −B∗ ¯y∗ ∈ ∂f (¯x) and ¯y∗ ∈ ∂g(B ¯x).

3 General SVR in Banach spaces of features.

We start by describing the problem setting. We consider the following optimization problem

min

(w,b)∈F×R

γZX ×Y

L(cid:0)y − hw, Φ(x)i − b(cid:1) dP (x, y) + G(w),

where the following assumptions are made:

(3.1)

A1 X and Y are two nonempty sets such that Y ⊂ R. P is a probability distribution on
X × Y, deﬁned on some underlying σ-algebra A on X × Y. F is a real separable reﬂexive
Banach space and Φ : X → F∗ is a measurable function. The function L : R → R+ is
positive and convex, p ∈ [1, +∞[, γ ∈ R++, and G : F → ]−∞, +∞] is proper, lower
semicontinuous, and convex.

A2 (∃ (a, b) ∈ R2

+)(∀ t ∈ R) L(t) ≤ a + b|t|p.

4

A3 ZX ×Y

ZX ×Y

|y|p dP (x, y) < +∞ and

kΦ(x)kp dP (x, y) < +∞.

In this context F and Φ are respectively the feature space and the feature map, and L is
the loss function [5, 27].1 Problem (3.1) can be considered as a continuous version of support
vector regression, for general loss L and regularizer G. Indeed, if P is chosen as a discrete
i=1 δ(xi,yi), for some sample (xi, yi)1≤i≤n ∈ (X × Y)n, then one

distribution, say P = (1/n)Pn

obtains

min

(w,b)∈F×R

γ
n

n

Xi=1

L(cid:0)yi − hw, Φ(xi)i − b(cid:1) + G(w),

which is the way support vector regression is formulated in [12]. Assumption A2 corresponds
to an upper growth condition for the loss L, whereas assumption A3 includes a moment
condition for the distribution P and an integrable condition for the feature map Φ, with
respect to P — they are both standard assumptions in support vector machines [21]. In the
following we consider the Lebesgue space

Lp(P ) =(cid:26)u : X × Y → R(cid:12)(cid:12)(cid:12) u is A-measurable and ZX ×Y

|u(x, y)|pdP (x, y) < +∞(cid:27).

Problem (3.1) is a convex optimization problem of a composite form on an inﬁnite dimensional
space. The following result ﬁrst recasts the problem in a constrained form, as done in [7, 23],
then presents its dual problem, with respect to the Fenchel-Rockafellar duality, and the related
optimality conditions.

Theorem 3.1. Let assumptions A1, A2, and A3 hold. Then problem (3.1) is equivalent to

min

(w,b,e)∈F×R×Lp(P )

γZX ×Y

L(e(x, y)) dP (x, y) + G(w),

subject to y − hw, Φ(x)i − b = e(x, y)

for P -a.a. (x, y) ∈ X × Y

(P)




and its dual is

min

u∈Lp∗(P )

G∗(cid:18)ZX ×Y

u(x, y)Φ(x) dP (x, y)(cid:19)
L∗(cid:18)u(x, y)
+ γZX ×Y

γ (cid:19) dP (x, y) −ZX ×Y

subject to ZX ×Y

u dP = 0.

y u(x, y) dP (x, y)

(D)





1Usually one requires that L is also even. In that case it is easy to see that necessarily 0 is a minimizer of L
and that L is increasing on R+. Indeed for every t ∈ R+, we have −t ≤ 0 ≤ t, and hence 0 = (1−α)(−t)+αt, for
some α ∈ [0, 1]. Then, by convexity L(0) ≤ (1 − α)L(−t)+ αL(t) = L(t), for L(−t) = L(t). Moreover, for every
s, t ∈ R, with 0 ≤ s ≤ t, we have s = (1 − α)0 + αt, for some α ∈ [0, 1], and hence L(s) ≤ (1 − α)L(0) + αL(t)
which yields L(s) − L(0) ≤ α(L(t) − L(0)) ≤ L(t) − L(0).

5

Moreover, the dual problem (D) admits solutions, strong duality holds, and for every (w, b, e) ∈
F × R × Lp(P ) and every u ∈ Lp∗(P ), (w, b, e) is a solution of (P) and u is a solution of (D)
if and only if the following optimality conditions hold

u(x, y)Φ(x) dP (x, y)(cid:19)

u dP = 0

w ∈ ∂G∗(cid:18)ZX ×Y

ZX ×Y


u(x, y)

∈ ∂L(e(x, y))

γ

y − hw, Φ(x)i − b = e(x, y)

for P -a.a. (x, y) ∈ X × Y.

for P -a.a. (x, y) ∈ X × Y

(3.2)

Proof. The Banach spaces Lp(P ) and Lp∗(P ) are put in duality by means of the pairing

h·, ·ip,p∗ : Lp(P ) × Lp∗

(P ) → R : (e, u) 7→ZX ×Y

In virtue of A3, the following linear operator

e(x, y)u(x, y) dP (x, y).

(3.3)

A : F × R → Lp(P ) s.t. (∀ (w, b) ∈ F × R) A(w, b) : X × Y → R : (x, y) 7→ hw, Φ(x)i + b (3.4)

is well-deﬁned and the function

is in Lp(P ). Then problem (3.1) can be written in the following constrained form

pr2 : X × Y → R : (x, y) 7→ y,

min

(w,b,e)∈F×R×Lp(P )

γZX ×Y

L(e(x, y)) dP (x, y) + G(w),

(3.5)

subject to pr2 − A(w, b) = e




— where, in the constraint, the equality is meant to be in Lp(P ) — and hence (P) follows.
Now, deﬁne the following integral functional

RP : Lp(P ) → R : e 7→ZX ×Y

L(e(x, y)) dP (x, y),

B : F × R × Lp(P ) → Lp(P ) : (w, b, e) 7→ A(w, b) + e,

the linear operator

and the functional

f : F × R × Lp(P ) → ]−∞, +∞] : (w, b, e) 7→ γRP (e) + G(w).

We note that the functional RP is well deﬁned, convex, and continuous. This follows from
from the convexity and continuity of L and from the fact that, because of A1, for every

6

(x, y) ∈ X × Y, L(e(x, y)) ≤ a + b|e(x, y)|p. Then, problem (3.5) can be equivalently written
as

min

(w,b,e)∈F×R×Lp(P )

f (w, b, e) + ι{−pr2}(−B(w, b, e)), with f (w, b, e) = γR(e) + G(w).

(3.6)

This form of problem (3.1) is amenable by the Fenchel-Rockafellar duality theory. In view of

Fact 2.2 we need only to check that 0 ∈ int(cid:0) − B(dom f ) + pr2(cid:1). This is almost immediate.

Indeed, since dom f = dom G × R × Lp(P ), we have

B(dom f ) =(cid:8)A(w, b) + e(cid:12)(cid:12) (w, b) ∈ dom G × R and e ∈ Lp(P )(cid:9) = Lp(P ).

Now we compute the dual of (3.6). We have

(∀ u ∈ Lp∗

(P ))

(ι{−pr2})∗(u) = h−pr2, uip,p∗

(3.7)

and, for every (w∗, b∗, u) ∈ F∗× R × Lp∗(P ),

f ∗(w∗, b∗, u) =

sup

h(w, b, e), (w∗, b∗, u)i − f (w, b, e)

(w,b,e)∈F×R×Lp(P )

= sup
w∈F

sup
b∈R

sup

e∈Lp(P )

hw, w∗i − G(w) + hu, eip,p∗ − γRP (e) + bb∗

(3.8)

=(G∗(w∗) + γR∗

+∞

P (u/γ)

if b∗ = 0
if b∗ 6= 0.

Moreover, we need also to compute A∗ : Lp∗
To that purpose, we note that for every (w, b, e) ∈ F × R × Lp(P ) and every u ∈ Lp∗

(P ) → F∗× R × Lp∗
(P ),

(P ) → F∗× R and B∗ : Lp∗

(P ).

hB(w, b, e), uip,p∗ = hA(w, b) + e, uip,p∗ = h(w, b), A∗ui + he, uip,p∗

= h(w, b, e), (A∗u, u)i

and

h(w, b), A∗ui = hA(w, b), uip,p∗

(hw, Φ(x)i + b)u(x, y) dP (x, y)

which yields

and

=ZX ×Y
=Dw,ZX ×Y
=D(w, b),(cid:18)ZX ×Y
A∗u =(cid:18)ZX ×Y

B∗u = (A∗u, u) =(cid:18)ZX ×Y

7

u(x, y)Φ(x) dP (x, y)E + bZX ×Y
u(x, y)Φ(x) dP (x, y),ZX ×Y
uΦ dP,ZX ×Y

u dP(cid:19)
uΦ dP,ZX ×Y

u dP, u(cid:19),

u dP

u dP(cid:19)E,

(3.9)

(3.10)

where, for brevity, we putRX ×Y uΦ dP =RX ×Y u(x, y)Φ(x) dP (x, y). Thus, taking into account

(3.8),(3.9), and (3.10), we have that, for every u ∈ Lp∗(P ),

f ∗(B∗u) = f ∗(A∗u, u) =
G∗(cid:18)ZX ×Y


+∞

uΦ dP(cid:19) + γR∗

P (u/γ)

u dP = 0

if ZX ×Y

otherwise.

Moreover, it follows from [18, Theorem 21(a)] that the Fenchel conjugate of RP is still an
integral operator, more precisely

(∀ u ∈ Lp(P ))

R∗

P (u/γ) =ZX ×Y

L∗(u(x, y)/γ) dP (x, y).

Therefore, recalling (3.7), the ﬁnal form (D) is obtained. The corresponding optimality con-
ditions for problem (3.6) and its dual (D) are (see Fact 2.1)

B∗u ∈ ∂f (w, b, e) = ∂G(w) × {0} × γ∂R(e)

and B(w, b, e) = pr2.

(3.11)

Now, recalling (3.10), conditions (3.11) can be gathered together as follows

uΦ dP ∈ ∂G(w)

u dP = 0

(3.12)

ZX ×Y
ZX ×Y

u
γ




∈ ∂R(e)

y − hw, Φ(x)i − b = e(x, y)

for P -a.a. (x, y) ∈ X × Y.

Thus, subdiﬀerentiating under the integral sign [18, Theorem 21(c)] and recalling (2.1), (3.2)
follows.

Remark 3.2.

(i) The form (P) resembles the way the problem of support vector machines for regression is
often formulated [23, eq. (3.51)] and the optimality conditions (3.2) are the continuous
versions of the one stated in [23, eq. (3.52)] for RKHS, diﬀerentiable loss functions, and
square norm regularizers.

(ii) If b = 0, condition RX ×Y u dP = 0 in (3.2) should be omitted.

(iii) If G is strictly convex on every convex subset of dom ∂G and int(dom G∗) = dom ∂G∗,
then G∗ is Gâteaux diﬀerentiable (hence ∂G∗ is single valued) on dom ∂G∗ [1, Proposi-
tion 18.9] and, if a solution w of the primal problem (P) exists, then the ﬁrst of (3.2)
yields

w = ∇G∗(cid:18)ZX ×Y

uΦ dP(cid:19),

8

(3.13)

where u is any solution of the dual problem (D). This constitutes a general nonlinear
representer theorem, since the solution of problem (P) is expressed in terms of the values
of the feature map Φ. In the special case that F is a Hilbert space and G = (1/2)k·k2,
∇G∗ = Id and the ﬁrst and third condition in (3.2) reduce to the ones obtained in [9,
i=1 δ(xi,yi), for some sample

Corollary 3]. When P is the discrete distribution P = (1/n)Pn

(xi, yi)1≤i≤n ∈ (X × Y)n, then (3.13) becomes

w = ∇G∗(cid:18) n
Xi=1

uiΦ(xi)(cid:19).

(3.14)

We note that in (3.13)-(3.14) the nonlinearity relies on the mapping ∇G∗ only.

The optimality conditions (3.2) in Theorem 3.1 directly yield a continuous representer

theorem in Banach space setting.

Corollary 3.3 (Continuous representer theorem). Let assumptions A1, A2, and A3 hold.
Suppose that F is strictly convex and smooth and let r ∈ ]1, +∞[. In problem (P), suppose
that G = ϕ ◦ k·k, for some convex and even function ϕ : R → R+ such that argmin ϕ = {0}.
Then the solution w of problem (P) admits the following representation

Jr(w) =ZX ×Y

c(x, y)Φ(x) dP (x, y),

(3.15)

for some function c ∈ Lp∗

(P ), where Jr : F → F∗ is the r-duality map of F.

Proof. Let t > 0. We ﬁrst note that, since 0 is the unique minimizer of ϕ and t > 0, then
0 /∈ ∂ϕ(t); moreover, for every ξ ∈ ∂ϕ(t), we have ξt ≥ ϕ(t) − ϕ(0) > 0, hence, ξ > 0. Now,
if w = 0, then (3.15) holds trivially. Suppose that w 6= 0. Then, it follows from Fact 2.1 that
when w 6= 0,

Therefore, it follows from the ﬁrst of (3.12) that

∂G(w) =

∂ϕ(kwk)
kwkr−1 Jr(w).

ZX ×Y

uΦ dP =

ξ

kwkr−1 Jr(w),

ξ ∈ ∂ϕ(kwk).

Hence, since ξ > 0,

and the statement follows.

Jr(w) =

kwkr−1

ξ

ZX ×Y

uΦ dP

Remark 3.4. If in Corollary 3.3, r = 2 and P is a discrete measure, say P = (1/n)Pn

for some sample (xi, yi)1≤i≤n ∈ (X × Y)n, then (3.15) becomes

i=1 δ(xi,yi),

J2(w) =

n

Xi=1

ciΦ(xi),

(ci)1≤i≤n ∈ Rn,

(3.16)

9

where J2 is the normalized duality map. Formula (3.16) is the way the representer theorem
is usually presented in reproducing kernel Banach spaces [10, 27, 28]. Here it is a simple
consequence of the more general Theorem 3.1 and Corollary 3.3. Moreover, we stress that
our derivation of (3.16) relies on convex analysis arguments only, while in the above cited
literature it is proved as a consequence of a representer theorem for function interpolation,
ultimately using diﬀerent techniques and stronger hypotheses. We ﬁnally note that, if F is a
Hilbert space and r = 2, then J2 is the identity map of F and (3.16) becomes

w =

n

Xi=1

ciΦ(xi).

This is the classical representer theorem in Hilbert spaces [19].

Example 3.5. We consider the case of the Vapnik’s ε-insensitive loss [20, 24]. Let ε > 0 and
deﬁne

Lε : R → R+ : t 7→ max{0, |t| − ε}.

(3.17)

This loss clearly satisﬁes A2 for every p > 1. We note that (3.17) is the distance function
from the set [−ε, ε], that is, using the notation in [13], we have Lε = d[−ε,ε]. Then, the Fenchel
conjugate of Lε is (see [13, Example 13.24(i)])

Therefore, for the loss (3.17), the dual problem (D) becomes

L∗

ε = σ[−ε,ε] + ι[−1,1] = ε|·| + ι[−1,1].

min

u∈Lp∗(P )

G∗(cid:18)ZX ×Y

u(x, y)Φ(x) dP (x, y)(cid:19)
+ εZX ×Y

|u(x, y)| dP (x, y) −ZX ×Y

y u(x, y) dP (x, y)

subject to ZX ×Y

u dP = 0 and |u(x, y)| ≤ γ for P -a.a. (x, y) ∈ X × Y.





This is a generalization of the dual problem that arises in classical support vector regression
when the linear ε-insensitive loss is considered [7, Proposition 6.21] and [20] — here we have
a general regularizer and a Banach feature space.

Remark 3.6. Let us consider the case that F is a Hilbert space. Then F is isomorphic to its
dual and the pairing reduces to the inner product in F. Moreover, suppose that G = (1/2)k·k2,

that L = (1/2)|·|2, and that b = 0, so that in (3.2) the conditionRX ×Y u dP = 0 is not present.

Then it follows from the ﬁrst and the third in (3.2) that

w =ZX ×Y

uΦ dP,

u
γ

= e

and hence

hw, Φ(x)i =ZX ×Y

u(x′, y′)hΦ(x′), Φ(x)i dP (x′, y′).

10

Thus, the last of (3.2) yields the following integral equation

(∀ (x, y) ∈ X × Y)

u(x, y)

γ

+ZX ×Y

u(x′, y′)hΦ(x′), Φ(x)i dP (x′, y′) = y.

4 Tensor-kernel representation

We present our framework. For clarity we consider separately the real and complex case. We
describe the real case with full details, whereas in the complex case we provide results with
sketched proofs only.

4.1 The real case

Let F = ℓr(K), with K a countable set and r = m/(m − 1) for some even integer m ≥ 2. Thus,
we have r∗ = m. Let (φk)k∈K be a family of measurable functions from X to R such that, for
every x ∈ X , (φk(x))k∈K ∈ ℓr∗

(K) and deﬁne the feature map as

Thus, we consider the following linear model

Φ : X → ℓr∗

(K) : x 7→ (φk(x))k∈K.

wkφk + b (pointwise),

where h·, ·ir,r∗ is the canonical pairing between ℓr(K) and ℓr∗(K). The space

fw,b = hw, Φ(·)ir,r∗ + b =Xk∈K

(cid:0)∀ (w, b) ∈ ℓr(K) × R(cid:1)
B =(cid:26)f : X → R(cid:12)(cid:12)(cid:12) (∃(w, b) ∈ ℓr(K) × R)(∀ x ∈ X )(cid:18)f (x) =Xk∈K
(∀ f ∈ B) kf kB = inf(cid:26)kwkr + |b| (cid:12)(cid:12)(cid:12) (w, b) ∈ ℓr(K) × R and f =Xk∈K

is a reproducing kernel Banach space with norm

wkφk(x) + b(cid:19)(cid:27)

wkφk + b (pointwise)(cid:27),

meaning that, with respect to that norm, the point-evaluation operators are continuous [5, 27].
We also consider the following regularization function

G(w) = ϕ(kwkr),

(4.4)

for some convex and even function ϕ : R → R+, such that argmin ϕ = {0}, and we set

i=1 δ(xi,yi), for some given sample (xi, yi)1≤i≤n ∈ (X × Y)n.

In such setting the primal and dual problems of support vector regression considered in

(4.1)

(4.2)

(4.3)

Theorem 3.1 turn into

P = (1/n)Pn



min

(w,b,e)∈ℓr(K)×R×Rn

γ
n

n

Xi=1

L(ei) + ϕ(kwkr),

subject to yi − hw, Φ(xi)ir,r∗ − b = ei,

for every i ∈ {1, . . . , n}

(Pn)

11

and, since G∗ = ϕ∗ ◦ k·kr∗ (Fact 2.1),

uiΦ(xi)(cid:13)(cid:13)(cid:13)(cid:13)r∗(cid:19) +

γ
n

n

Xi=1

L∗(cid:18) ui

γ(cid:19) −

1
n

n

Xi=1

yiui

(Dn)




min
u∈Rn

ϕ∗(cid:18)(cid:13)(cid:13)(cid:13)(cid:13)

n

1
n

Xi=1
Xi=1

n

subject to

ui = 0.

Moreover, assuming that w 6= 0, Fact 2.1 and (3.2) yield the following optimality conditions2

i=1 uiΦ(xi)(cid:13)(cid:13)r∗(cid:1)
n(cid:13)(cid:13)Pn
∂ϕ∗(cid:0) 1
i=1 uiΦ(xi)(cid:13)(cid:13)
(cid:13)(cid:13)Pn

r∗−1
r∗

ui = 0

Jr∗(cid:18) n
Xi=1

uiΦ(xi)(cid:19)

(4.5)

ui/γ ∈ ∂L(ei)

for every i ∈ {1, . . . , n}

yi − hw, Φ(xi)ir,r∗ − b = ei

for every i ∈ {1, . . . , n}.

w ∈

n

Xi=1




The dual problem (Dn) is a convex optimization problem and it is ﬁnite dimensional, since
it is deﬁned on Rn. Once (Dn) is solved, expressions in (4.5) in principle allow to recover the
primal solution (w, b) and eventually to compute the estimated regression function hw, Φ(x)i+b
at a generic point x of the input space X . However, if K is an inﬁnite set, that procedure is not
feasible in practice, since it relies on the explicit knowledge of the feature map Φ, which is an
inﬁnite dimensional object. In the following we show that, in the dual problem (Dn), we can
actually get rid of the feature map Φ and use instead a new type of kernel function evaluated
at the sample points (xi)1≤i≤n. This will ultimately provide a new and eﬀective computational
framework for treating support vector regression in Banach spaces of type (4.3).

Remark 4.1. Consider the reproducing kernel Banach space

B =(cid:26)f : X → R (cid:12)(cid:12)(cid:12) (∃w ∈ ℓr(K))(∀ x ∈ X )(cid:18)f (x) =Xk∈K

wkφk(x)(cid:19)(cid:27)

endowed with norm kf kB = inf(cid:8)kwkr (cid:12)(cid:12) w ∈ ℓr(K) and f = Pk∈K wkφk (pointwise)(cid:9). Let
f ∈ B and let (wk)k∈K ∈ ℓr(K) be such that f =Pk∈K wkφk pointwise. Then, for every ﬁnite
subset J ⊂ K we have f −Pk∈J wkφk =Pk∈K\J wkφk pointwise; hence, by deﬁnition

(cid:13)(cid:13)(cid:13)(cid:13)
f −Xk∈J

wkφk(cid:13)(cid:13)(cid:13)(cid:13)B
2Note that G∗ = ϕ∗ ◦ k·kr∗ and {0} = argmin ϕ = ∂ϕ∗(0). Thus, since, by (3.2), w ∈ ∂G∗(Pn
if w 6= 0, then Fact 2.1 yields Pn

≤(cid:13)(cid:13)(wk)k∈K\J(cid:13)(cid:13)r =(cid:18) Xk∈K\J

|wk|r(cid:19)1/r

as |J| → +∞.

i=1 uiΦ(xi) 6= 0.

→ 0

i=1 uiΦ(xi)),

12

Thus, the family (wkφk)k∈K is summable in (B, k·kB) and it holds f =Pk∈K wkφk in (B, k·kB).

Therefore, if the family of functions (φk)k∈K is pointwise ℓr-independent, in the sense that

(∀ (wk)k∈K ∈ ℓr(K)) Xk∈K

wkφk = 0 (pointwise) ⇒ (wk)k∈K ≡ 0,

(4.6)

then (φk)k∈K is an unconditional Schauder basis of B. Indeed if Pk∈K wkφk = 0 in (B, k·kB),
since the evaluation operators on B are continuous, we have Pk∈K wkφk = 0 pointwise, and

hence, by (4.6), (wk)k∈K ≡ 0. We ﬁnally note that when (φk)k∈K is a (unconditional) Schauder
basis of B, then B is isometrically isomorphic to ℓr(K).

We start by ﬁrst providing a generalized Cauchy-Schwartz inequality for sequences which is
a consequence of a standard generalization of Hölder’s inequality [2, Corollary 2.11.5] and that
we prove for completeness. We use the following compact notation for the component-wise
product of two sequences:

(∀ a ∈ ℓr(K))(∀ b ∈ ℓr∗

(K)) Xk∈K

ab :=Xk∈K

a[k]b[k].

Proposition 4.2 (Generalized Cauchy-Schwartz inequality). Let K be a nonempty set. Let
m ∈ N and let a1, a2, . . . , am ∈ lm

+ (K). Then a1a2 · · · am ∈ ℓ1

+(K) and

Xk∈K

a1a2 · · · am ≤(cid:18)Xk∈K

am

1(cid:19)1/m(cid:18)Xk∈K

am

2(cid:19)1/m

· · ·(cid:18)Xk∈K

am

m(cid:19)1/m

.

Proof. We prove it by induction. The statement is true for m = 2. Suppose that the statement
, . . . , a(m+1)/m
holds for m ≥ 2 and let a1, a2, . . . , am, am+1 ∈ ℓm+1
∈
ℓm
+ (K) and by induction hypothesis (a1a2 · · · am)(m+1)/m ∈ ℓ1

+ (K). Then a(m+1)/m
+(K) and

, a(m+1)/m

m

1

2

Xk∈K

(a1a2 · · · am)(m+1)/m ≤(cid:18)Xk∈K

am+1

1 (cid:19)1/m(cid:18)Xk∈K

am+1

2 (cid:19)1/m

· · ·(cid:18)Xk∈K

am+1

m (cid:19)1/m

.

Now, since a1a2 · · · am ∈ ℓ(m+1)/m
exponents, it follows from Hölder inequality that a1a2 · · · amam+1 ∈ ℓ1

(K), am+1 ∈ ℓm+1

+

+ (K), and (m+1)/m and m+1 are conjugate

+(K) and

Xk∈K

a1a2 · · · amam+1 ≤(cid:18)Xk∈K
≤(cid:18)Xk∈K

(a1a2 · · · am)(m+1)/m(cid:19)m/(m+1)(cid:18)Xk∈K
2 (cid:19)1/m+1
1 (cid:19)1/m+1(cid:18)Xk∈K

am+1

am+1

· · ·(cid:18)Xk∈K

am+1

m+1(cid:19)1/(m+1)
m+1(cid:19)1/m+1

am+1

.

Now we are ready to deﬁne a tensor-kernel associated to the feature map (4.1) and give its

main properties.

13

Proposition 4.3. In the setting (4.1) described above, the following function is well-deﬁned

K : X m = X × · · · × X

|

}

m times

{z

and the following hold.

→ R : (x′

1, . . . , x′

m) 7→Xk∈K

φk(x′

1) · · · φk(x′

m),

(4.7)

(i) For every (x′

1, . . . , x′

m) ∈ X m, and for every permutation σ of the indexes {1, . . . , m},

K(x′

σ(1) . . . x′

σ(m)) = K(x′

1, . . . x′

m).

(ii) For every (xi)1≤i≤n ∈ X n

(∀ u ∈ Rn)

(iii) For every (xi)1≤i≤n ∈ X n

n

Xi1,...,im=1
uiΦ(xi)(cid:13)(cid:13)(cid:13)(cid:13)

r∗

r∗

u ∈ Rn 7→(cid:13)(cid:13)(cid:13)(cid:13)

n

Xi=1

K(xi1, . . . , xim)ui1 . . . uim ≥ 0 .

=

n

Xi1,...,im=1

K(xi1, . . . , xim)ui1 . . . uim

(4.8)

is a homogeneous polynomial form of degree m on Rn.

(iv) For every x ∈ X , K(x, . . . , x) ≥ 0.
(v) For every (x′

1, . . . , x′

m) ∈ X m
1, . . . , x′

|K(x′

m)| ≤ K(x′

1, . . . , x′

1)1/m · · · K(x′

m, . . . , x′

m)1/m.

Proof. Since (φk(x′
that (φk(x′

1)φk(x′

1))k∈K, (φk(x′

2))k∈K, . . . (φk(x′
m))k∈K ∈ l1(K) and

2) · · · φk(x′

m))k∈K ∈ lm(K), it follows from Proposition 4.2

Xk∈K

|φk(x′

1) · · · φk(x′

m)| ≤(cid:18)Xk∈K

|φk(x′

1)|m(cid:19)1/m

· · ·(cid:18)Xk∈K

|φk(x′

m)|m(cid:19)1/m

.

(4.9)

This shows that deﬁnition (4.7) is well-posed and moreover, since m is even we can remove
the absolute values in the right hand side of (4.9) and get (v). Properties (i) and (iv) are
immediate from the deﬁnition of K. Finally, since r∗ = m is even, for every u ∈ Rn, we have

n

Xi=1

(cid:13)(cid:13)(cid:13)(cid:13)

r∗

r∗

uiΦ(xi)(cid:13)(cid:13)(cid:13)(cid:13)

uiφk(xi)(cid:19)m

n

=Xk∈K(cid:18) n
Xi=1
=Xk∈K
Xi1,...,im=1
Xi1,...,im=1(cid:18)Xk∈K

=

n

φk(xi1) · · · φk(xim)ui1 . . . uim

φk(xi1) · · · φk(xim)(cid:19)ui1 . . . uim .

(4.10)

Therefore, recalling the deﬁnition of K, (ii) and (iii) follow.

14

Remark 4.4. Let (xi)1≤i≤n ∈ X n. Then (K(xi1, . . . , xim))i∈{1,...n}m deﬁnes a tensor of degree
m on Rn. Then, properties (i) and (ii) establish that the tensor is symmetric and positive
deﬁnite: they are natural generalization of the deﬁning properties of standard positive (matrix)
kernels.

Because of Proposition 4.3(v), tensor kernels, as deﬁned in (4.7), can be normalized as for

the matrix kernels.

Proposition 4.5 (normalized tensor kernel). Let K be deﬁned as in (4.7) and suppose that,
for every x ∈ X , K(x, . . . , x) > 0. Deﬁne

˜K : X m → R,

(x′

1, . . . , x′

m) 7→

K(x′

1, . . . , x′

1, . . . , x′
K(x′
m)
1)1/m · · · K(x′
m, . . . , x′

m)1/m .

(4.11)

Then ˜K is still of type (4.7), for some family of functions ( ˜φk)k∈K, ˜φk : X → R, and the
following hold.
(i) For every x ∈ X , ˜K(x, . . . , x) = 1.
m) ∈ X m, | ˜K(x′
(ii) For every (x′

m)| ≤ 1.

1, . . . x′

1, . . . x′

Proof. Just note that, for every x ∈ X , kΦ(x)km
φk(x)/kΦ(x)km
m.

m = K(x, · · · , x) > 0. Then deﬁne ˜φk(x) =

We present the ﬁrst main result of the section, which is a direct consequence of Proposi-

tion 4.3.

Theorem 4.6. In the setting (4.1)-(4.4) described above, the dual problem (Dn) reduces to the
following ﬁnite dimensional problem

K(xi1, . . . , xim)ui1 . . . uim(cid:19)1/r∗(cid:19) +

γ
n

n

Xi=1

L∗(cid:18)ui

γ(cid:19) −

1
n

n

Xi=1

yiui

min
u∈Rn




n

ϕ∗(cid:18) 1
n(cid:18)
Xi1,...,im=1
Xi=1

ui = 0.

n

subject to

(4.12)

Remark 4.7.

(i) Problem (4.12) is a convex optimization problem with linear constraints.

(ii) If the tensor kernel K is explicitly computable by means of (4.7), the dual problem (4.12)
is a very ﬁnite dimensional problem, in the sense that it does not involve the feature map
anymore. This is exactly how the kernel trick works within the kernel matrix.

15

Remark 4.8. The homogeneous polynomial form (4.8) can be written as follows

where, for every multi-index α = (α1, . . . , αn) ∈ Nn and for every vector u ∈ Rn, we used the
standard notation uα = uα1

i=1 αi, and the multinomial coeﬃcient

1 · · · uαn

, . . . , . . . , xn, . . . , xn

)uα

(4.13)

|

αn

{z

}

Indeed it follows from (4.10) and the multinomial theorem [3, Theorem 4.12] that

m

α1, . . . , αn(cid:19) =

m!

α1! . . . αn!

.

(4.14)

α1

|α|=m

|

Xα∈Nn

α(cid:19)K(x1, . . . , x1
(cid:18)m
{z
}
n , |α| =Pn
α(cid:19) =(cid:18)
(cid:18)m
uiΦ(xi)(cid:13)(cid:13)(cid:13)(cid:13)

r∗

r∗

n

Xi=1

(cid:13)(cid:13)(cid:13)(cid:13)

=Xk∈K(cid:18) n
uiφk(xi)(cid:19)m
Xi=1
α(cid:19)φk(x1)α1 . . . φk(xn)αnuα
(cid:18)m
=Xk∈K Xα∈Nn
α(cid:19)(cid:18)Xk∈K
(cid:18)m
= Xα∈Nn

φk(x1)α1 . . . φk(xn)αn(cid:19)uα.

|α|=m

|α|=m

Thus (4.13) follows from (4.7).

Corollary 4.9. In Theorem 4.6, let ϕ = (1/r)|·|r (which gives G = (1/r)k·kr
problem (4.12) becomes

r). Then the dual

K(xi1, . . . , xim)ui1 . . . uim +

γ
n

n

Xi=1

L∗(cid:18)ui

γ(cid:19) −

1
n

n

Xi=1

yiui

(4.15)

min
u∈Rn

1

r∗nr∗

subject to




n

Xi1,...,im=1
Xi=1

n

ui = 0.

Proof. Just note that ϕ∗ = (1/r∗)|·|r∗

and apply Theorem 4.6.

Remark 4.10. The ﬁrst term in the objective function in (4.15) is a positive deﬁnite homoge-
neous polynomial of order m. So, if the function L∗ is smooth, which occurs when L is strictly
convex, then the dual problem (4.15) is a smooth convex optimization problem with a linear
constraint and can be approached by standard optimization techniques such as Newton-type
or gradient-type methods — in the case of square loss, the dual problem (4.15) is a polynomial
convex optimization problems and possibly more appropriate optimization methods may be

16

employed. We ﬁnally specialize (4.15) to the case of ε-insensitive loss (see Example 3.5)




min
u∈Rn

1

mnm

subject to

n

Xi1,...,im=1
Xi=1

n

K(xi1, . . . , xim)ui1 . . . uim +

ε
n

n

Xi=1

|ui| −

1
n

n

Xi=1

yiui

(4.16)

ui = 0 and |ui| ≤ γ for every i ∈ {1, . . . , n}.

This problem clearly shows similarities with the dual formulation of standard support vector
regression [20, 24].

Once a solution u ∈ Rn of the dual problem (4.12) is computed, then one can compute the
solution of the primal problem (Pn) by means of the equations in (4.5). In particular, if ϕ∗
and L∗ are diﬀerentiable, then the solution of the primal problem (Pn) is given by

w =

(ϕ∗)′( 1

n K[u]1/r∗)
K[u]1/r

and

where

n

uiΦ(xi)(cid:19), K[u] :=

Jr∗(cid:18) n
Xi1,...,im=1
Xi=1
γ (cid:19),
b = y1 − hw, Φ(x1)ir,r∗ − (L∗)′(cid:18)u1
(K) → ℓr(K) : u 7→(cid:0)|uk|r∗−1 sign(uk)(cid:1)k∈N.

Jr∗ : ℓr∗

Now note that r∗ = m and m − 1 is odd, therefore

K(xi1, . . . , xim)ui1 . . . uim > 0

(4.17)

(4.18)

Jm : ℓm(K) → ℓr(K) : u 7→ (um−1

k

)k∈N

and hence (4.17) yields

(∀ k ∈ N)

wk = ξ(u)(cid:18) n
Xi=1

uiφk(xi)(cid:19)m−1

,

ξ(u) =

(ϕ∗)′( 1

n K[u]1/r∗)
K[u]1/r

.

(4.19)

Remark 4.11. It follows from the last two of (4.5) that in (4.18) any index i ∈ {1, . . . , n}
can be actually chosen to determine b. We chose i = 1.

The next issue is to evaluate the regression function corresponding to (w, b) at a general
input point, without the explicit knowledge of the feature map but relying on the tensor-kernel
K only. In the analogue case of matrix-kernels, this is what is usually called kernel trick. The
following proposition shows that the kernel trick is still viable in our more general situation
and that a tensor-kernel representation holds.

17

Proposition 4.12. Under the assumptions (4.1)-(4.4), let K be deﬁned as in (4.7). Suppose
that ϕ∗ is diﬀerentiable on R++ and that L∗ is diﬀerentiable on R. Let u ∈ Rn be a solution
of the dual problem (4.12) and set (w, b) as in (4.19)-(4.18). Then, for every x ∈ X ,

hw, Φ(x)ir,r∗ =

(ϕ∗)′( 1

n K[u]1/r∗)
K[u]1/r

n

Xi1,...,im−1=1

K(xi1, . . . , xim−1, x)ui1 · · · uim−1

b = y1 − (L∗)′(cid:18)u1
γ (cid:19)

−

(ϕ∗)′( 1

n K[u]1/r∗)
K[u]1/r

(4.20)

n

Xi1,...,im−1=1

K(xi1, . . . , xim−1, x1)ui1 · · · uim−1.

Proof. Let x ∈ X . Then, we derive from (4.19) that

hw, Φ(x)ir,r∗ =Xk∈K

wkφk(x)

uiφk(xi)(cid:19)m−1

φk(x)

n

= ξ(u)Xk∈K(cid:18) n
Xi=1
= ξ(u)Xk∈K
Xi1,...,im−1=1
Xi1,...,im−1=1

= ξ(u)

n

φk(xi1) · · · φk(xim−1)φk(x)ui1 · · · uim−1

K(xi1, . . . , xim−1, x)ui1 · · · uim−1,

where we used the deﬁnition (4.7) of K.

Remark 4.13. In the case treated in Corollary 4.9, (4.20) yields the following representation
formula

hw, Φ(x)ir,r∗ + b =

1

nm−1

n

Xi1,...,im−1=1(cid:0)K(xi1, . . . , xim−1, x) − K(xi1, . . . , xim−1, x1)(cid:1)ui1 · · · uim−1
γ (cid:19).
+ y1 − (L∗)′(cid:18) u1

Moreover, if in model (4.2) we assume no oﬀset (b = 0), then we can avoid the requirement of
the diﬀerentiability of L∗ and the representation formula becomes

hw, Φ(x)ir,r∗ =

1

nm−1

n

Xi1,...,im−1=1

K(xi1, . . . , xim−1, x)ui1 · · · uim−1.

Concluding we have shown that, the estimated regression function can be evaluated at
every point of the input space by means of a ﬁnite summation formula, provided that the
tensor-kernel K is explicitly available: we will show in Section 5 several signiﬁcant examples
in which this occurs.

18

4.2 The complex case

In this section we give the complex version of the theory developed in Section 4.1. Therefore,
we let F = ℓr(K; C), with K a countable set and r = m/(m − 1) for some even integer m ≥ 2.
Let (φk)k∈K be a family of measurable functions from X to C such that, for every x ∈ X ,
(φk(x))k∈K ∈ ℓr∗

(K; C). The feature map is now deﬁned as

Φ : X → ℓr∗

(K; C) : x 7→ (φk(x))k∈K,

(4.21)

which generates the model

(∀ w ∈ ℓr(K; C))(∀ b ∈ C)

x 7→ hw, Φ(x)ir,r∗ + b =Xk∈K

wkφk(x) + b,

(4.22)

where hw, w∗ir,r∗ = Pk∈N wkw∗

k is the canonical sesquilinear form between ℓr(K; C) and
ℓr∗(K; C). This case can be treated as a vector-valued real case by identifying complex func-
tions with R2-valued functions and the space ℓr(K; C) with ℓr(K; R2). Moreover, it is not
diﬃcult to generalize the dual framework presented in Section 3 to the case of vector-valued
(and speciﬁcally to R2-valued) functions. Then, the (complex) feature map (4.21) deﬁnes an
underlying real vector-valued feature map on ℓr(K; R2) [5], that is

ΦR : X → L(R2, ℓr∗

(K; R2)) ≅ ℓr∗

(K; R2×2) : x 7→ (φR,k(x))k∈K,

(4.23)

where L(R2, ℓr∗(K; R2)) is the spaces of linear continuous operators from R2 to ℓr∗(K; R2)
(which is isomorphic to ℓr∗(K; R2×2)) and

(∀ x ∈ X )(∀ k ∈ K) φR,k(x) =(cid:20) Reφk(x)

−Imφk(x) Reφk(x)(cid:21) ∈ R2×2.

Imφk(x)

(4.24)

This way, denoting, for every x ∈ X , by φR,k(x)∗ the transpose of the matrix φR,k(x), we have

(∀ x ∈ X )(∀ k ∈ K)(∀ wk ∈ R2 ≅ C)

φR,k(x)∗wk = wkφk(x),

(4.25)

hence ΦR(x)∗w = hw, Φ(x)ir,r∗. Moreover

(∀ x ∈ X )(∀ u ∈ R2 ≅ C)

ΦR(x)u = (φR,k(x)u)k∈K = (uφk(x))k∈K = uΦ(x).

(4.26)

Then, problems (Pn) and (Dn) become

and

min
u∈Cn






min

(w,b,e)∈ℓr(K;C)×C×Cn

γ
n

n

Xi=1

L(ei) + ϕ(kwkr),

subject to yi − hw, Φ(xi)ir,r∗ − b = ei,

for every i ∈ {1, . . . , n}

uiΦ(xi)(cid:13)(cid:13)(cid:13)(cid:13)r∗(cid:19) +

ϕ∗(cid:18)(cid:13)(cid:13)(cid:13)(cid:13)

n

1
n

Xi=1
Xi=1

n

subject to

ui = 0,

γ
n

n

Xi=1

L∗(cid:18) ui

γ(cid:19) −

1
n

n

Xi=1

Re(uiyi)

19

(Pn(C))

(Dn(C))

where, L∗ : C → R : z∗ 7→ supz∈C
ity conditions (4.5) still hold, where now Jr∗ : ℓr∗(K; C) → ℓr(K; C) : w∗ 7→ (|w∗
and

Re(zz∗)−L(z). Moreover, assuming that w 6= 0, the optimal-
k|)k∈K,

k|r−1w∗

k/|w∗

(∀ e ∈ C) ∂L(e) =(cid:8)z∗ ∈ C(cid:12)(cid:12) (∀ z ∈ C) L(z) ≥ L(e) + Re(cid:0)z∗(z − e)(cid:1)(cid:9).

In the following we give the result corresponding to Proposition 4.3.

Proposition 4.14. In the setting described above, suppose that m is even and set q = m/2.
Then, the following function is well-deﬁned

K : X q × X q → C : (x′

1, . . . , x′

q; x′′

1, . . . , x′′

q ) 7→Xk∈K

φk(x′

1) · · · φk(x′

q)φk(x′′

1) · · · φk(x′′

q ),

(4.27)

and the following hold.

(i) For every (x′

1, . . . , x′

q; x′′

1, . . . , x′′

q ) ∈ X q × X q, and for every permutation σ′ and σ′′ of the

indexes {1, . . . , q},

K(x′

σ′(1) . . . x′

σ′(q); x′′

σ′′(1) . . . x′′

σ′′(q)) = K(x′

1, . . . x′

q; x′′

1 . . . x′′

q ).

(ii) For every (x′; x′′) ∈ X q × X q K(x′; x′′) = K(x′′; x′);
(iii) For every (xi)1≤i≤n ∈ X n

(∀ u ∈ Cn)

n

Xi1,...,iq=1

j1,...,jq=1

(iv) For every (xi) ≤i≤n ∈ X n

K(xj1, . . . , xjq; xi1, . . . , xiq)ui1 . . . uiquj1 . . . ujq ≥ 0 .

u ∈ Cn 7→(cid:13)(cid:13)(cid:13)(cid:13)

n

Xi=1

uiΦ(xi)(cid:13)(cid:13)(cid:13)(cid:13)

r∗

r∗

=

n

Xi1,...,iq=1

j1,...,jq=1

K(xj1, . . . , xjq; xi1, . . . , xiq )ui1 . . . uiquj1 . . . ujq

is a positive homogeneous polynomial form of degree m on Cn.

(v) For every (x′
(vi) For every (x′

1, . . . , x′
1, . . . , x′

q) ∈ X q, K(x′
q; x′′

1, . . . , x′′

1, . . . , x′

q; x′
q ) ∈ X q × X q,

1, . . . , x′

q) ≥ 0;

|K(x′

1, . . . , x′

q; x′′

1, . . . , x′′

q )| ≤ K(x′

1, . . . , x′

1; x′

1, . . . , x′

1)1/m · · · K(x′′

q , . . . , x′′

q ; x′′

q , . . . , x′′

q )1/m.

Remark 4.15. Item (iii) states that (cid:0)K(xi1, . . . , xim)(cid:1)i∈{1,...,n}m is a positive-deﬁnite tensor

of degree m.

20

As in the real case, the dual problem (Dn) reduces to





min
u∈Cn

n(cid:18)
ϕ∗(cid:18) 1

n

Xi1,...,iq=1

j1,...,jq=1

K(xj1, . . . , xjq, xi1, . . . , xiq)ui1 . . . uiquj1 . . . ujq(cid:19)1/r∗(cid:19)
γ(cid:19) −

L∗(cid:18) ui

γ
n

1
n

+

n

Xi=1

Re

yiui

n

Xi=1

subject to

ui = 0

n

Xi=1

and the homogeneous polynomial form in Proposition 4.14(iv) can be written as follows

Xα∈Nn,β∈Nn

|α|=q,|β|=q

α(cid:19)(cid:18)q
(cid:18)q

β(cid:19)K(x1, . . . x1
}
{z

|

α1

Finally, in the setting of Proposition 4.12, deﬁning

, . . . . . . , xn, . . . , xn

, x1, . . . x1

, . . . . . . , xn, . . . , xn

)uα uβ.

(4.28)

|

αn

{z

}

|

β1

{z

}

|

βn

{z

}

K[u] =

n

Xi1,...,iq=1

j1,...,jq=1

K(xj1, . . . , xjq, xi1, . . . , xiq )ui1 . . . uiquj1 . . . ujq,

(4.29)

for every x ∈ X , the following representation formulas hold

hw, Φ(x)ir,r∗ =

(ϕ∗)′( 1

n K[u]1/r∗)
K[u]1/r

n

Xi1,...,iq=1

j1,...,jq−1=1

K(xj1, . . . xjq−1, x; xi1, . . . xiq )ui1 · · · uiquj1 · · · ujq−1

b = y1 − hw, Φ(x1)ir,r∗ − ∇L∗(cid:18)u1
γ (cid:19).

(4.30)

where ∇L∗ is the (real) gradient of L∗, considered as a function from R2 to R.

Remark 4.16. In view of Proposition 4.14(iv), deﬁnitions (4.21) and (4.27) correspond to
those given in [21, Lemma 4.2] and the concept of positive deﬁniteness stated in (iii) is a
natural generalization of the analogue notion given in [21, Deﬁnition 4.15].

5 Power series tensor-kernels

In this section we consider reproducing kernel Banach spaces of complex analytic functions
which are generated through power series. We show that, for such spaces, the corresponding
tensor kernel, deﬁned according to (4.7), admits an explicit expression. We provide also

21

representation formulas. In this section we assume, for simplicity, that ϕ = (1/r)|·|r, therefore
we address the support vector regression problem

min

(w,b)∈ℓr(K;C)×C

γ
n

n

Xi=1

L(cid:0)yi − hw, Φ(xi)ir,r∗ − b(cid:1) + kwkr

r,

for a speciﬁc choice of the feature map (4.21).

We ﬁrst need to set special notation for multi-index powers of complex vectors. Let d ∈ N
with d ≥ 1. We will denote the component of a vector x ∈ Cd, by xt, with t ∈ {1, . . . , d}. For
every x ∈ Cd and every ν ∈ Nd we set

xν =

xνt
t ,

|x| = (|x1|, . . . , |xd|),

and ν! =

d

Yt=1

νt!

d

Yt=1

so that ∀ ν ∈ Nd we have |xν| =Qd
that is (m, . . . , m), so that xm means Qd

t=1 |xt|νt = |x|ν. Moreover, when the exponent of the vector
x ∈ Cd is an index (not a multi-index), say m ∈ N, we consider m as a constant multi-index,
t . Finally, we deﬁne the binary inner operation
of pointwise multiplication in Cd. For every x, x′ ∈ Cd, we set x ⋆ x′ ∈ Cd, such that, for every
t. Let m ∈ N and x ∈ Cd. We set x⋆m = x ⋆ · · · ⋆ x (m-times), so
t ∈ {1, . . . , d}, (x ⋆ x′)t = xtx′
that x⋆m ∈ Cd and, for every t ∈ {1, . . . , d}, (x⋆m)t = xm
t .

t=1 xm

Let ρ = (ρν)ν∈Nd be a multi-sequence in R+, let r = m/(m − 1) for some even integer

m ≥ 2. Let Dρ be the domain of (absolute) convergence of the power series Pν∈Nd ρνzν, that
is the interior of the set(cid:8)z ∈ Cd (cid:12)(cid:12) Pν∈Nd ρν|zν| < +∞(cid:9). The set Dρ is a complete Reinhardt
domain3 and we assume that Dρ 6= {0}. Let κ : Dρ → C be the sum of the series Pν∈Nd ρνzν,

that is

Clearly κ is an analytic function on Dρ. Set

D⋆1/m

ρ

let X ⊂ D⋆1/m

ρ

, and deﬁne the dictionary

ρνzν.

(∀ z ∈ Dρ) κ(z) = Xν∈Nd
=(cid:8)x ∈ Cd (cid:12)(cid:12) x⋆m = (xm

1 , . . . , xm

d ) ∈ Dρ(cid:9),

(∀ ν ∈ Nd)

φν : X → C : x 7→ ρ1/m

ν xν.

(5.1)

Then, for every x ∈ X , since x⋆m ∈ Dρ, we have

Xν∈Nd

|φν(x)|m = Xν∈Nd

ρν|x⋆m|ν < +∞,

hence (φν(x))ν∈Nd ∈ ℓm(Nd; C). Thus, we are in the framework described at the beginning of
Section 4.2. We deﬁne

Br

ρ,b(X ) =(cid:26)f ∈ CX(cid:12)(cid:12)(cid:12)(cid:12)

(∃ (cν)ν∈Nd ∈ ℓr(Nd; C))(∃ b ∈ C)(∀ x ∈ X )(cid:16)f (x) = Xν∈Nd

cνφν(x) + b(cid:17)(cid:27),

3 It means that if z ∈ Dρ, then Dρ contains the polydisk {t ∈ Cd | (∀ j ∈ {1, . . . , d}) |tj| ≤ |zj|}.

22

which is a reproducing kernel Banach spaces with norm

kf kBr

ρ,b(X ) = infnkckr + |b| (cid:12)(cid:12)(cid:12) (cν)ν∈Nd ∈ ℓr(Nd; C) and f = Xν∈Nd

cνρ1/m

ν xν + b (pointwise)o.

Suppose now that b = 0 and that, for every ν ∈ Nd, ρν > 0. Then, deﬁning the weights
(ην)ν∈Nd = (ρ−r/m

)ν∈Nd and the corresponding weighted ℓr space

ν

ℓr

η(Nd; C) =(cid:26)(aν)ν∈Nd ∈ CNd (cid:12)(cid:12)(cid:12) Xν∈Nd

1

ρr/m
ν

|aν|r < +∞(cid:27),

we can express the space Br

ρ,0(X ) in the form of a weighted Hardy-like space [17, 25]

(∃ (aν)ν∈Nd ∈ ℓr

η(Nd; C))(∀ x ∈ X )(cid:16)f (x) = Xν∈Nd

aνxν(cid:17)(cid:27).

Br

ρ,0(X ) =(cid:26)f ∈ CX(cid:12)(cid:12)(cid:12)(cid:12)
q ) = Xν∈Nd

1, . . . , x′′

1, . . . , x′

q; x′′

Moreover, for every (x′

q, x′′

1, . . . , x′′

q ) ∈ X q × X q,

K(x′

1, . . . , x′

ρνx′ν

1 · · · x′ν

q x′′ν

1 · · · x′′ν

q = κ(x′

1 ⋆ · · · ⋆ x′

q ⋆ x′′

1 ⋆ · · · ⋆ x′′

q ). (5.2)

Remark 5.1. Suppose that ρν > 0, for every ν ∈ Nd. Then Pν∈Nd cνρ1/m

ν xν = 0 (pointwise)
ν = 0, for every ν ∈ Nd and hence cν = 0, for every ν ∈ Nd. Thus, in virtue of
ρ,0(X ) and that

implies cνρ1/m
Remark 4.1 this yields that (φν)ν∈Nd is an unconditional Schauder basis of Br
Br

ρ,0(X ) is isometric to ℓr(Nd; C).

Proposition 5.2. Under the notation and assumption above, suppose that X is a compact
subset of D⋆1/m
ρ,b(X ) is dense in C (X ; C), the
space of continuous functions on X endowed with the uniform norm.

and that, for every ν ∈ Nd, ρν > 0. Then Br

ρ

Proof. It is enough to note that Br

ρ,b(X ) contains the set

A = span(cid:8)φν (cid:12)(cid:12) ν ∈ N(cid:9) =(cid:26)Xν∈I

cνxν (cid:12)(cid:12)(cid:12) I ⊂ Nd and I ﬁnite (cν)ν∈I ∈ CI(cid:27)

which is the algebra of polynomials on X in d variables with complex coeﬃcients. Thus the
statement is a consequence of the Stone-Weierstrass theorem.

In the sequence we also assume that the oﬀset b is zero. Because of (5.2), the representation

given in (4.28) yields the following homogenous polynomial form

u ∈ Cn 7→(cid:13)(cid:13)(cid:13)(cid:13)

n

Xi=1

uiΦ(xi)(cid:13)(cid:13)(cid:13)(cid:13)

r∗

r∗

= Xα∈Nn,β∈Nn

|α|=q,|β|=q

α(cid:19)(cid:18)q
(cid:18)q

β(cid:19)κ(x⋆β1

1 ⋆· · ·⋆x⋆βn

n ⋆x⋆α1

1 ⋆· · ·⋆x⋆αn

n )uαuβ, (5.3)

23

where (xi)1≤i≤n ∈ X n is the training set and, according to the convention established at the
beginning of the section, x⋆αi
i,d). Moreover, in this case, recalling (4.30) and
(5.2), for every x ∈ X , we have

i = (xαi

i,1, . . . , xαi

hw, Φ(x)ir,r∗ =

1

nm−1

n

Xi1,...,iq=1

j1,...,jq−1=1

κ(xj1 ⋆ · · · ⋆ xjq−1 ⋆ x ⋆ xi1 ⋆ · · · ⋆xiq )ui1 · · · uiquj1 · · · ujq−1. (5.4)

We now treat two special cases of power series tensor-kernels. Let (γk)k∈N ∈ RN

1/ lim supk γ1/k

suppose that the power series Pk∈N γkζ k (ζ ∈ C) has radius of convergence Rγ > 0 (Rγ =
respectively the disk of convergence and the sum of the power series Pk∈N γkζ k.

k > 0). We denote by D(Rγ) = {ζ ∈ C | |ζ| < Rγ} and by ψ : D(Rγ) → R

Case 1. We set

+ and

|ν|!

ν1! · · · νd!

.

(5.5)

Then, the domain of absolute convergence of the series Pν∈Nd ρνzν is the strip

d

(∀ ν ∈ Nd)

ρν = γ|ν|(cid:18)|ν|
Dρ =(cid:26)z ∈ Cd (cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:12)
γk Xν∈Nd

ν (cid:19) = γ|ν|
zt(cid:12)(cid:12)(cid:12)(cid:12)
zν =Xk∈N

Xt=1

ν1! · · · νd!

k!

|ν|=k

< Rγ(cid:27)

and, it follows from the multinomial theorem [3, Theorem 4.12] that, for every z ∈ Dρ,

κ(z) = Xν∈Nd

ρνzν =Xk∈N

γk(cid:18) d
Xt=1

zt(cid:19)k

= ψ(cid:18) d
Xt=1

zt(cid:19).

(5.6)

Note also that D⋆1/m
K(x′

ρ

= {z ∈ Cd | kzkm
1, . . . , x′′

q; x′′

1, . . . , x′

m < Rγ}. Thus, it follows from (5.2) that
q ) = κ(x′

1 ⋆ · · · x′′
q )

1 ⋆ · · · x′

q ⋆ x′′

= ψ(cid:18) d
Xt=1

x′
1,t · · · x′

q,tx′′

1,t · · · x′′

q,t(cid:19),

(5.7)

for every (x′
reduces to

1, . . . , x′

q, x′′

1, . . . , x′′

q ) ∈ X q × X q. For q = 1, the right hand side of (5.7)

K(x′, x′′) = ψ(hx′ | x′′i) =Xk∈N

γkhx′ | x′′ik,

where h· | ·i is the Euclidean scalar product in Rd. These kind of kernels have been also
called Taylor kernels in [21]. Thus, in virtue of (5.7), (5.3) takes the form

u ∈ Cn 7→(cid:13)(cid:13)(cid:13)(cid:13)

n

Xi=1

r∗

r∗

uiΦ(xi)(cid:13)(cid:13)(cid:13)(cid:13)

|α|=q,|β|=q

= Xα∈Nn,β∈Nn
= Xα∈Nn,β∈Nn

|α|=q,|β|=q

α(cid:19)(cid:18)q
(cid:18)q
α(cid:19)(cid:18)q
(cid:18)q

β(cid:19)ψ(cid:18) d
Xt=1
β(cid:19)ψ(cid:18) d
Xt=1

24

xα1
1,t · · · xαn

n,txβ1

1,t · · · xβn

n,t(cid:19)uαuβ

(x·,t)α(x·,t)β(cid:19)uαuβ,

where we put, for every t ∈ {1, . . . , d}, x·,t = (x1,t, . . . xn,t) ∈ Cn.4 The representation
formula (5.4) turns to

hw, Φ(x)ir,r∗ =

1

nm−1

Case 2. We set

n

Xi1,...,iq=1

j1,...,jq−1=1

ψ(cid:18) d
Xt=1

xi1,t · · · xiq,txj1,t · · · xjq−1,txt(cid:19)ui1 · · · uiquj1 · · · ujq−1.

(∀ ν ∈ Nd)

ρν =

γνt.

(5.8)

d

Yt=1

Then the domain of absolute convergence of the series Pν∈Nd ρνzν is
Dρ =(cid:26)z ∈ Cd (cid:12)(cid:12)(cid:12) (∀ t ∈ {1, . . . , d}) |zt| < Rγ(cid:27)
Yt=1Xk∈N

(∀ z ∈ Dρ) κ(z) = Xν∈Nd

ρνzν = Xν∈Nd

Yt=1

γνj zνt

γkzk

t =

and

d

d

t =

ψ(zt).

d

Yt=1

In this case D⋆1/m

ρ

= {z ∈ Cd | (∀ t ∈ {1, . . . , d})|zt| < R1/m

γ

} and (5.2) becomes,

K(x′

1, . . . , x′

q; x′′

1, . . . , x′′

q ) = κ(x′

1 ⋆ · · · x′

q ⋆ x′′

1 ⋆ · · · x′′
q )

=

d

Yt=1

ψ(cid:18)x′

1,t · · · x′

q,tx′′

1,t · · · x′′

(5.9)

q,t(cid:19),

for every (x′
can obtain the corresponding expression for the homogeneous polynomial form (5.3)

q ) ∈ X q × X q. Thus, as done before, relying on (5.9) we

1, . . . , x′′

1, . . . , x′

q, x′′

u ∈ Cn 7→(cid:13)(cid:13)(cid:13)(cid:13)

n

Xi=1

uiΦ(xi)(cid:13)(cid:13)(cid:13)(cid:13)

r∗

r∗

= Xα∈Nn,β∈Nn

|α|=q,|β|=q

α(cid:19)(cid:18)q
(cid:18)q

β(cid:19) d
Yt=1

ψ(cid:0)xα1

1,t · · · xαn

n,txβ1

1,t · · · xβn

n,t(cid:1)uαuβ

(5.10)

and the representation formula (5.4),

hw, Φ(x)ir,r∗ =

1

nm−1

n

Xi1,...,iq=1

j1,...,jq−1=1

ψ(xj1,t · · · xjq−1,txtxi1,t · · · xiq,t)ui1 · · · uiquj1 · · · ujq−1.

d

Yt=1

(5.11)

4 If we consider the matrix of the data X = (xi,t)1≤i≤n
1≤t≤d

the vectors x·,t are the columns of X.

∈ Cn×d, having the training set (xi)1≤i≤n as rows,

25

Example 5.3. We list signiﬁcant examples of power series tensor kernels and for each one we
provide the corresponding representation formulas.

(i) In (5.8) set (γk)k∈N ≡ 1, hence (ρν)ν∈Nd ≡ 1 too. Then Rγ = 1 and ψ(ζ) = 1/(1 − ζ).

Therefore, relying on (5.9), we obtain the tensor-Szegö kernel

K(x′

1, . . . , x′

q; x′′

1, . . . , x′′

q ) =

1

t=1(1 − x′

1,t · · · x′

q,tx′′

1,t · · · x′′

q,t)

.

Qd

This kernel generates a reproducing kernel Banach space of multi-variable analytic func-
tions [17, 25]

Br

ρ,0(X ) =(cid:26)f ∈ CX(cid:12)(cid:12)(cid:12)(cid:12)

(∃ (cν)ν∈Nd ∈ ℓr(Nd; C))(∀ x ∈ X )(cid:16)f (x) = Xν∈Nd

cνxν(cid:17)(cid:27)

with norm kf kBr
(pointwise). This space reduces to the Hardy space when r = 2. Moreover, (5.10) yields
the following homogenous polynomial form

ρ,b(X ) = kckr, where (cν)ν∈Nd ∈ ℓr(Nd; C) is such that f = Pν∈Nd cνxν

Finally, in view of (5.11), we have the following tensor-kernel representation

u ∈ Cn 7→(cid:13)(cid:13)(cid:13)(cid:13)

n

Xi=1

hw, Φ(x)ir,r∗ =

r∗

r∗

uiΦ(xi)(cid:13)(cid:13)(cid:13)(cid:13)
Xi1,...,iq=1

nm−1

1

n

j1,...,jq−1=1

= Xα∈Nn,β∈Nn

|α|=q,|β|=q

β(cid:19)
α(cid:19)(cid:18)q
(cid:18)q

uαuβ

t=1(1 − (x·,t)α(x·,t)β)

Qd

.

ui1 · · · uiquj1 · · · ujq−1

t=1(1 − xj1,t · · · xjq−1,txt, xi1,t · · · xiq,t)

Qd

.

(ii) Set (γk)k∈N ≡ ((k + 1)/π)k∈N in (5.8). Then Rγ = 1 and ψ(ζ) = 1/(π(1 − ζ)2). We then

obtain the following Taylor type tensor kernel

K(x′

1, . . . , x′

q; x′′

1, . . . , x′′

q ) =

1

t=1 (1 − x′

1,t · · · x′

q,tx′′

1,t · · · x′′

q,t)

2 .

πdQd

This kernel gives rise to a reproducing kernel Banach space of analytic functions which
reduces to the Bergman space when m = 2. Proceeding as in the previous point, the
expression of the corresponding homogeneous polynomial form and the representation
formula can be obtained.

(iii) Let (γk)k∈N =(cid:0)1/k!(cid:1)k∈N in (5.8). Then Rγ = +∞ and ψ(ζ) = eζ. Hence, by (5.9),

d

Yt=1

K(x′

1, . . . , x′

q; x′′

1, . . . , x′′

q ) =

26

ex′

1,t···x′

q,tx′′

1,t···x′′

q,t,

which is the tensor-exponential kernel and the form (5.10) becomes

u ∈ Cn 7→(cid:13)(cid:13)(cid:13)(cid:13)

n

Xi=1

uiΦ(xi)(cid:13)(cid:13)(cid:13)(cid:13)

r∗

r∗

= Xα∈Nn,β∈Nn

|α|=q,|β|=q

α(cid:19)(cid:18)q
(cid:18)q

β(cid:19)ePd

j=1(x·,j )α(x·,j )β

uαuβ.

The corresponding tensor representation is

hw, Φ(x)ir,r∗ =

1

nm−1

(iv) Let α > 0, set

n

Xi1,...,iq=1

j1,...,jq−1=1

exi1,t···xiq−1,txt,xj1 ,t···xjq ,t.

d

Yt=1

(∀ k ∈ N)

γk =(cid:18)−α

k (cid:19)(−1)k =

k

Yi=1

α + i − 1

i

> 0,

and deﬁne (ρν)ν∈Nd according to (5.5). Then Rγ = 1 and ψ(z) = (1 − ζ)−α and formula
(5.7) yields the following tensorial version of the binomial kernel [21]

K(x′

1, . . . , x′

q; x′′

1, . . . , x′′

q ) =

(v) Let s ∈ N, set

(∀ k ∈ N)

q,tx′′

1,t · · · x′′

q,t(cid:17)α .

1

t=1 x′

1,t · · · x′

(cid:16)1 −Pd
γk =
k(cid:19) if k ≤ s
(cid:18)s


if k > s,

0

and deﬁne (ρν)ν∈Nd according to (5.5). Then Rγ = +∞ and ψ(ζ) = (1 + ζ)s. This way,
by (5.7), we have

K(x′

1, . . . , x′

q; x′′

1, . . . , x′′

q ) =(cid:16)1 +

d

Xt=1

x′
1,t · · · x′

q,tx′′

1,t · · · x′′

q,t(cid:17)s

,

which is the polynomial tensor-kernel of order s. By (5.5) we have that ρν > 0 if |ν| ≤ s
and ρν = 0 if |ν| > s. Therefore, recalling (5.1), we have that

Br

ρ,0(X ) =(cid:26)f ∈ CX(cid:12)(cid:12)(cid:12)(cid:12)

(∃ (cν)ν∈Nd ∈ ℓr(Nd; C))(∀ x ∈ X )(cid:16)f (x) = Xν∈Nd

cνφν(x)(cid:17)(cid:27),

is the space of polynomials in d variables with coeﬃcients in C of degree up to s.

27

6 Conclusion

In this work we ﬁrst provided a complete duality theory for support vector regression in Banach
function spaces with general regularizers. Then, we specialized the analysis to reproducing
kernel Banach spaces that admit a representation in terms of a (countable) dictionary of
functions with ℓr-summable coeﬃcients and regularization terms of type ϕ(k·kr), being r =
m/(m−1) and m an even integer. In this context we showed that the problem of support vector
regression can be explicitly solved through the introduction of a new type of kernel of tensorial
type (with degree m) which completely encodes the ﬁnite dimensional dual problem as well
as the representation of the corresponding inﬁnite dimensional primal solution (the regression
function). This can provide a new and eﬀective computational framework for solving support
vector regression in Banach space setting. We ﬁnally study a whole class of reproducing kernel
Banach spaces of analytic functions to which the theory applies and show signiﬁcant examples
which can become useful in applications.

Acknowledgments. The research leading to these results has received funding from the Euro-
pean Research Council (FP7/2007–2013) / ERC AdG A-DATADRIVE-B (290923) under the Euro-
pean Union’s Seventh Framework Programme. This paper reﬂects only the authors’ views, the Union
is not liable for any use that may be made of the contained information; Research Council KUL:
GOA/10/09 MaNet, CoE PFV/10/002 (OPTEC), BIL12/11T; PhD/Postdoc grants; Flemish Gov-
ernment: FWO: PhD/Postdoc grants, projects: G.0377.12 (Structured systems), G.088114N (Tensor
based data similarity); IWT: PhD/Postdoc grants, projects: SBO POM (100031); iMinds Medical
Information Technologies SBO 2014; Belgian Federal Science Policy Oﬃce: IUAP P7/19 (DYSCO,
Dynamical systems, control and optimization, 2012–2017).

References

[1] H. H. Bauschke and P. L. Combettes, Convex Analysis and Monotone Operator Theory

in Hilbert Spaces. Springer, New York 2011.

[2] V. I. Bogachev, Measure Theory. Springer, Berlin 2007.

[3] M. Bóna, A Walk Through Combinatorics. 3rd Ed. World Scientiﬁc, Singapore 2011.

[4] I. Cioranescu, Geometry of Banach Spaces, Duality Mappings and Nonlinear Problems.

Kluwer, Dordrecht 1990.

[5] P. L. Combettes, S. Salzo, and S. Villa, Consistency of Regularized Learning Schemes in

Banach Spaces. arXiv:1410.6847v3, 2015.

[6] P. L. Combettes, S. Salzo, and S. Villa, Consistent Learning by Composite Proximal

Thresholding. arXiv:1504.04636v2, 2015.

[7] N. Cristianini and J. Shawe-Taylor, An Introduction to Support Vector Machines. Cam-

bridge University Press, Cambridge 2000.

[8] C. De Mol, E. De Vito, and L. Rosasco, Elastic-net regularization in learning theory, J.

Complexity, vol. 25, pp. 201–230, 2009.

28

[9] E. De Vito, L. Rosasco, A. Caponnetto, M. Piana, and A. Verri, Some properties of

regularized kernel methods, J. Mach. Learn. Res., vol. 5, pp. 1363–1390, 2004.

[10] G. E. Fasshauer, F. J. Hickernell, and Q. Ye, Solving support vector machines in repro-
ducing kernel Banach spaces with positive deﬁnite functions, Appl. Comput. Harmon.
Anal., vol. 38, pp. 115–139, 2015.

[11] W. Fu, Penalized regressions: the bridge versus the lasso, J. Comput. Graph. Stat., vol.

7, pp. 397–416, 1998.

[12] F. Girosi, An Equivalence Between Sparse Approximation and Support Vector Machines,

Neural Comput., vol. 10(6), pp. 1455–1480, 1998.

[13] J.-B. Hiriart-Urruty and C. Lemaréchal, Convex Analysis and Minimization Algorithms

II. Springer, Berlin 1996.

[14] T. Hofmann, B. Schölkopf, and A. J. Smola, Kernel methods in machine learning, Ann.

Statist., vol. 36, pp. 1171–1220, 2008.

[15] V. Koltchinskii, Sparsity in penalized empirical risk minimization, Ann. Inst. Henri

Poincaré Probab. Stat., vol. 45, pp. 7–57, 2009.

[16] B. S. Mendelson and J. Neeman, Regularization in Kernel Learning, Ann. Statist., 38(1),

pp. 526–565, 2010.

[17] V. I. Paulsen, An Introduction to the Theory of Reproducing Kernel Hilbert Spaces. [On

line]. Available: http://www.math.uh.edu/~vern/rkhs.pdf

[18] R. T. Rockafellar, Conjugate Duality and Optimization. SIAM, Philadelphia, PA 1974.

[19] B. Schölkopf, R. Herbrich, and A. J. Smola, A Generalized Representer Theorem. In
Computational Learning Theory: 14th Annual Conference on Computational Learning
Theory, COLT 2001. Springer Berlin Heidelberg, 2001.

[20] I. Steinwart and A. Christmann, Sparsity of SVMs that use the ε-insensitive loss. In

Advances in Neural Information Processing Systems 21. Curran Associates, Inc., 2009.

[21] I. Steinwart and A. Christmann, Support Vector Machines. Springer, New York 2008.

[22] B. K. Sriperumbudur, K. Fukumizu, and G. R. G. Lanckriet, Learning in Hilbert vs. Ba-
nach spaces: a measure embedding viewpoint, in: Advances in Neural Information Pro-
cessing Systems 24. Curran Associates, Inc., 2011.

[23] J. A. K. Suykens, T. Van Gestel, J. De Brabanter, B. De Moor, J. Vandewalle, Least

Squares Support Vector Machines. World Scientiﬁc, Singapore 2002.

[24] V. N. Vapnik, Statistical Learning Theory. Wiley, New York 1998.

[25] R. M. Young, An Introduction to Nonharmonic Fourier Series. Academic Press, San

Diego 2001.

[26] C. Zălinescu, Convex Analysis in General Vector Spaces. World Scientiﬁc, River Edge,

NJ 2002.

[27] H. Zhang, Y. Xu, and J. Zhang, Reproducing kernel Banach spaces for machine learning,

J. Mach. Learn. Res., vol. 10, pp. 2741–2775, 2009.

29

[28] H. Zhang and J. Zhang, Regularized learning in Banach spaces as an optimization prob-

lem: representer theorems, J. Global Optim., vol. 54, pp. 235–250, 2012.

[29] B. Zwicknagl, Power series kernels, Constr. Approx., vol. 29, pp. 61–84, 2009.

30

