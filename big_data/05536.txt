LOCAL LIMIT THEOREMS AND RENEWAL THEORY WITH NO

MOMENTS

KENNETH S. ALEXANDER AND QUENTIN BERGER

Abstract. We study i.i.d. sums τk of nonnegative variables with index 0: this means
P(τ1 = n) = ϕ(n)n−1, with ϕ(·) slowly varying, so that E(τ ǫ
1 ) = ∞ for all ǫ > 0. We
prove a local limit and local (upward) large deviation theorem, giving the asymptotics of
P(τk = n) when n is at least the typical length of τk. A recent renewal theorem in [21] is
an immediate consequence: P(n ∈ τ ) ∼ P(τ1 = n)/P(τ1 > n)2 as n → ∞. If instead we
P(k ∈ τ ), we
obtain a similar equivalence but with P(τ1 = n) replaced by its average over a short interval.
We give an application to the local asymptotics of the distribution of the ﬁrst intersection
of two independent renewals. We further derive downward moderate and large deviations
estimates, that is, the asymptotics of P(τk ≤ n) when n is much smaller than the typical
length of τk.

only assume regular variation of P(n ∈ τ ) and slow variation of Un :=Pn

k=0

1. Introduction

It is classical to study renewal processes τ = {0 = τ0, τ1, τ2, . . . }, and in particular the
relation between the renewal mass function P(n ∈ τ ) and the inter-arrival distribution
P(τ1 = n). We assume the inter-arrival distribution P(τ1 = n) is regularly varying: there
exists a positive slowly varying function ϕ(·) and α ≥ 0 such that

(1.1)

P(τ1 = n) = ϕ(n) n−(1+α) .

In particular the process is aperiodic. The case receiving the least attention (under the
general assumption (1.1)) is α = 0, in which τ1 has no moments and is not in the domain
of attraction of a stable law, and that is our focus here. Tauberian theorems are of less use
here than in other cases, so our methods are primarily probabilistic. An example with α = 0
is the return times of symmetric simple random walk (SSRW) on Z2, τ = {n , S2n = 0}, for
which P(τ1 = n) n→∞∼ π/n(log n)2, from [18, Thm. 4].

The limiting distributions of τn and related quantities in the α = 0 case have been studied
in [8, 16, 19, 21, 22, 23, 24]. Deﬁning r(n) := P(τ1 > n), Theorem 4.1 in [8] states that if
r(n) is slowly varying, then for any y > 0

6
1
0
2

 
r
a

 

M
1
3

 
 
]

.

R
P
h
t
a
m

[
 
 

2
v
6
3
5
5
0

.

3
0
6
1
:
v
i
X
r
a

(1.2)

as n → +∞.

Recently in [21], Nagaev proved a strong renewal theorem:

P(cid:0)n r(τn) < y(cid:1) → 1 − e−y

(1.3)

P(n ∈ τ ) n→∞∼

P(τ1 = n)
P(τ1 > n)2 ,

and for P(τk > n), some “upward” large deviation results (meaning for n much larger than
the typical size of τk) were proved in [22].

1

2

K. ALEXANDER AND Q. BERGER

1.1. Renewal theorems. The assumption (1.1) is very natural: beyond the dimension-2
case, it includes the case τ = {n , S2n = 0}, where (Sn)n≥0 is SSRW on Zd for any d. One has
2 − 1, ϕ(n) n→∞→ cd
α = 1/2 and ϕ(n) n→∞→ (4π)−1/2 for d = 1 (see e.g. [13, Ch. III]); and α = d
for d ≥ 3 (see [11, Thm. 4]). Equation (1.1) also includes the case τ = {n , Sn = 0} where
(Sn)n≥0 is an aperiodic random walk in the domain of attraction of a symmetric stable law,
see [20, Thm. 8].

The asymptotics of the renewal function P(n ∈ τ ) under (1.1) have been widely studied

in the literature, including [9], [12], [14], [21], [25]. We recall brieﬂy the results.

First, when τ is transient and (1.1) holds, we have

(1.4)

P(n ∈ τ ) n→∞∼

P(τ1 = n)
P(τ1 = +∞)2 .

This is a consequence of Theorem 1 in [4], and is also proved in [15, App. A.5] with elementary
methods.

If τ is recurrent, then

• if E[τ1] < +∞, then the classical Renewal Theorem (see e.g. [2]) gives that

(1.5)

(1.6)

(1.7)

(1.8)

lim
n→∞

P(n ∈ τ ) =

1

E[τ1]

;

• if α = 1 in (1.1), and E[τ1] = +∞, Erickson [12, Eq. (2.4)] proved that

P(n ∈ τ ) n→∞∼

1

E [τ1 ∧ n]

;

• if α ∈ (0, 1) in (1.1), Doney [9, Thm. B] proved that

P(n ∈ τ ) n→∞∼

α sin(πα)

π

n−(1−α) ϕ(n)−1 ;

• if α = 0 in (1.1), then Nagaev [21] showed

P(n ∈ τ ) n→∞∼

P(τ1 = n)
P(τ1 > n)2 .

The condition (1.1) is not best possible for the validity of these strong renewal theorems
with inﬁnite mean. Assume simply that P(τ1 > n) n→∞∼ α−1ϕ(n)n−α with α ∈ (0, 1] (and
E[τ1] = +∞ if α = 1), so that τ1 is in the domain of attraction of a stable law with index α.
Garsia and Lamperti [14] showed that (1.7) holds whenever α ∈ ( 1
2, 1), and Erickson proved
(1.6) in the case α = 1. When α ∈ (0, 1
2], some additional conditions on the distribution of τ1
are necessary for (1.7) to be valid, and suﬃcient ones were given in [6], [7], [9], [25]. It is only
recently that a complete necessary and suﬃcient condition for the strong renewal theorem
(1.7) was proved in simultaneous papers by Caravenna [3] and Doney [10]. A necessary and
suﬃcient condition remains to be found in the case α = 0.

Throughout the paper, c1, c2, . . . are constants depending only on the distribution of τ1.
Also, we treat certain large quantities at times as if they were integers, to avoid the clutter
of integer-part notation; in all cases these can be treated as if the integer-part notation were
in use.

RENEWAL THEORY WITH NO MOMENTS

3

Our ﬁrst result is a local limit and local (upward) large deviation theorem, proved in
Section 2, in the case of a recurrent τ . Deﬁne rn := r(n) := P(τ1 > n), which in the α = 0
case is slowly varying and satisﬁes (see [5, Proposition 1.5.9a])

(1.9)

ϕ(n) = o(rn) as n → ∞.

In particular we have ϕ(n) → 0.

In [22], it is proved that P(τk > n) ∼ krn as n, k → ∞ with krn → 0. We improve
here this result by establishing a local limit theorem, and extending the range of validity
to kϕ(n) → 0. This extension is signiﬁcant because krn → 0 allows only values of n much
larger than the typical value of τk; see the remarks following the theorem.

Theorem 1.1. If τ is recurrent and (1.1) holds with α = 0, then uniformly for k such that
kϕ(n) → 0, we have

(1.10)

P(τk = n) n→∞∼ kP(τ1 = n)(1 − rn)k.

Further, there exists a constant c1 > 0 such that for n suﬃciently large and all 1 ≤ k ≤ n,

(1.11)

P(τk = n) ≤ c1kP(τ1 = n)(1 − rn)k.

Note that, as soon as k ≫ r−1

n , we have P(τk ≤ n) ≤ (1 − rn)k → 0, and n is therefore
much smaller than the typical size of τk. By (1.9), k ≫ r−1
n is consistent with the hypothesis
k ≪ 1/ϕ(n). Equation (1.10) therefore includes n down to a size much smaller than the
typical size of τk. Heuristically, (1.10) says that even for much smaller-than-usual n, when
τk = n it is because there was a single gap of length very close to n, among the ﬁrst k gaps
τj − τj−1; this is unique to α = 0.

In comparison, in the case where (1.1) holds with α ∈ (0, 1), Doney [9, Thm. A] proved that
P(τk = n) ∼ kP(τ1 = n) provided that krn → 0. If we consider the case krn → x ∈ (0, +∞),
we have that n/ak → x−1/α, where ak is such that P(τ1 > ak) ∼ k−1 (so that τk/ak converges
to an α-stable distribution with non-degenerate density g). Then, Gnedenko’s local limit
theorem (see [17, § 50] gives that P(τk = n) ∼ α−1x−(1+1/α)g(x−1/α)kP(τ1 = n), in contrast
with (1.10) when α = 0.

follows. Assume (1.1) with α = 0 and let θn satisfy r−1

The strong renewal theorem (1.3) from [21] is an easy consequence of Theorem 1.1, as
n ≪ θn ≪ ϕ(n)−1. We write
k=1 P(τk = n), and decompose it according to whether k is smaller or larger

than θn. Thanks to (1.10), by our choice of θn we have

kP(τ1 = n)(1 − rn)k n→∞∼ r−2

n P(τ1 = n) .

P(n ∈ τ ) =Pn
Xk≤θn
P(τk = n) ≤ c1Xk>θn

Xk>θn

P(τk = n) n→∞∼ Xk≤θn

For the rest of the sum, we use (1.11) together with θn ≫ r−1

n , to get that, for n ≥ n0

kP(τ1 = n)(1 − rn)k = o(1)r−2

n P(τ1 = n)

as n → +∞.

These two estimates give (1.3). Combining with (1.4), we obtain the following statement: if
(1.1) holds with α ≥ 0, and P(τ1 > n) is slowly varying (that is, either τ is transient, or τ
is recurrent with α = 0), then (1.3) holds.

The heuristic behind (1.3) may be seen by restating it as P(τ1 = n | n ∈ τ ) ∼ P(τ1 > n)2.
This says that given n ∈ τ , in order to have τ1 = n (i.e. no renewals between 0 and n), the

4

K. ALEXANDER AND Q. BERGER

trajectory mainly needs to “escape” without renewals at each end, and these two escapes
are approximately independent, each with probability near P(τ1 > n). This independence
in the recurrent case is unique to α = 0, since in that case the only renewals that typically
occur given n ∈ τ are very close to 0 and n.

1.2. Large and moderate downward deviations. Theorem 1.1 may be viewed as both
a local limit theorem and a local large deviation theorem for the case α = 0, covering
upward deviations (in the sense that n is much larger than the typical size of τk) and
downward deviations that are not too great. As a complement we now consider estimates
for downward deviations of the form P(τk ≤ n) for n much smaller than the typical size of
τk, that is krn → ∞.

Let ϕ∗ denote a slowly varying function conjugate to ϕ, that is, such that x 7→ xϕ∗(x)
is an asymptotic inverse of y 7→ yϕ(y), see [5, §1.5.7] for more. For most common slowly
varying functions ϕ one has ϕ∗ ∼ 1/ϕ, but this is not true if ϕ is “barely slowly varying,”
for example ϕ(n) = n1/ log log n. We will prove the following in Section 1.2.

Theorem 1.2. Suppose τ is recurrent and (1.1) holds with α = 0. Let n ≥ k.

(i) Given M > 0 there exists aM , with aM → 1 as M → 0, such that if n is large and

kϕ(n) ≤ M, then

(1.12)

aM (1 − rn)k ≤ P(τk ≤ n) ≤ (1 − rn)k.

(ii) If k, n → +∞ with kϕ(n) → +∞ and n/k → +∞, then we have

(1.13)

P(τk ≤ n) = expn−(1 + o(1)) k r(cid:16)n

k

ϕ∗(cid:16)n

k(cid:17)(cid:17)o .

(iii) For n = bk with b ≥ 1, the limit −I(b) = lim
n→∞
if b ≥ min{j : P(τ1 = j) > 0}. Moreover, it satisﬁes

1
n log P(τk ≤ bk) exists, and it is ﬁnite

I(b) ∼ r(bϕ∗(b))

as b → +∞.

This theorem extends the result (1.2) of Darling [8] to the case y → +∞ as n → +∞. In
particular, (i) allows to recover (1.2) by taking k = y/rn (since {τk ≤ n} = {r(τk) ≥ y/k}),
and moreover extends it to P(k r(τk) ≥ y) ∼ e−y as k → ∞, uniformly for y ≪ rn/ϕ(n) (we
recall (1.9)).

1.3. Reverse renewal theorems. Though (1.1) is very natural, verifying that it holds is
often diﬃcult, for example if τ = {n, Sn = 0}, with (Sn)n≥0 an aperiodic random walk in the
domain of attraction of a symmetric stable distribution, see [18]. But in that case, a local
limit theorem (see [17, § 50]) easily gives the asymptotic behavior of P(Sn = 0) = P(n ∈ τ ).
Therefore, one would like to get a general result to infer from P(n ∈ τ ) something about
the behavior of P(τ1 = n). We call such a result a reverse renewal theorem. An additional
application of such theorems is given in Section 1.4.

In general, it is not true that regular variation of P(n ∈ τ ) implies regular variation of
P(τ1 = n), an example being given in Section 4.3. But the average of the values P(τ1 = n)
over a relatively short interval may be better behaved.
In fact we can obtain a reverse
renewal theorem corresponding to (1.3) and (1.4) in the α = 0 case, as follows.

RENEWAL THEORY WITH NO MOMENTS

5

Deﬁne

Un :=

nXk=0

and note that

P(k ∈ τ ),

U∞ := E[|τ |] =

P(k ∈ τ ) (cid:18)=

∞Xk=0

1

P(τ1 = ∞)

if U∞ < ∞(cid:19) ,

(1.14)

if Un is slowly varying, then Un

n→∞∼ P(τ1 > n)−1.

This is trivial if τ is transient: |τ | is then a geometric random variable, and Un converges to
E[|τ |] = P(τ1 = +∞)−1. In the recurrent case, we refer to Theorem 8.7.3 in [5]; the proof
uses standard properties of convolution of Laplace transforms. Note that in the following we
do not assume (1.1).

Theorem 1.3. Assume that P(n ∈ τ ) is regularly varying and Un is slowly varying. Then
there exist ǫn → 0 such that

(1.15)

P(τ1 = k) n→∞∼ P(τ1 > n)2P(n ∈ τ ) .

1

ǫnn X(1−ǫn)n<k≤n

If also P(τ1 = n) is regularly varying, then

(1.16)

P(τ1 = n) n→∞∼ P(τ1 > n)2P(n ∈ τ ).

This theorem applies in the recurrent case when P(n ∈ τ ) is regularly varying with index
−1, and in the case of a transient renewal τ . When τ is transient, we are able to prove the
following stronger statement.

Theorem 1.4. If P(n ∈ τ ) is regularly varying and τ is transient, then

P(τ1 = n) n→∞∼ P(τ1 = ∞)2P(n ∈ τ ) .

This theorem was proved in [11] in the case where τ1, τ2, · · · are the return times to the

origin of a transient aperiodic random walk, but we prove it here in general.

Section 4.1 is devoted to the proof of Theorem 1.4, and Section 4.2 to the proof of The-
orem 1.3 (where only the recurrent case has to be considered, with Theorem 1.4 proved.)
Finally, in Section 4.3, we give an example where P(τ1 = n) is not regularly varying but
P(n ∈ τ ) is, and Un is slowly varying. This shows that (1.16) cannot hold in the general
case of a recurrent renewal, and our Theorem 1.3 is in that sense optimal.

In general, Theorem 1.3 reduces the problem of proving (1.16) to showing that P(τ1 = k)

is approximately constant over the interval ((1 − ǫn)n, n].

1.4. Application of reverse renewal theorems: the intersection of two independent
renewals. Let τ and σ be independent renewal processes with inter-arrival distributions
satisfying

(1.17)

P(τ1 = n) = ϕ(n)n−(1+α) ,

P(σ1 = n) = eϕ(n)n−(1+ eα)
for some α,eα ≥ 0 and slowly varying functions ϕ(·),eϕ(·). We assume α ≤eα.

function and renewal function

We denote the intersection ρ := τ ∩ σ, which is a renewal process with renewal mass

P(n ∈ ρ) = P(n ∈ τ )P(n ∈ σ), U ∗

n =

P(k ∈ ρ).

nXk=0

6

K. ALEXANDER AND Q. BERGER

These are regularly varying, and their asymptotic behavior is thus known from the results
for σ, τ in Section 1.1. In [1] our reverse renewal theorems, 1.3 and 1.4, are applied to help
establish the following. If ρ is transient (i.e. U ∗

∞ < ∞) then

P(ρ1 = n) n→∞∼ (U ∗

∞)−2P(n ∈ τ )P(n ∈ σ).

(1.18)

slowly varying, and

P(ρ1 = n) n→∞∼ (U ∗

If ρ is recurrent and either (i) α,eα ∈ (0, 1) with α +eα = 1, or (ii) α = 0,eα ≥ 1, then U ∗
for some (asymptotically known) slowly varying ψ∗. In [1], general 0 ≤ α ≤ eα are covered,

and Theorems 1.3 and 1.4 here are essential for the cases (i) and (ii). The key step to get
from (1.15) for ρ to (1.18) is to show that, due to the regularity (1.17) in σ and τ , P(ρ1 = k)
is approximately constant over short intervals, so that the left side of (1.15) (for ρ) is as-
ymptotic to P(ρ1 = n).

n)−2P(n ∈ τ )P(n ∈ σ) n→∞∼

ψ∗(n)

n

n is

2. Proof of Theorem 1.1

We ﬁrst prove (1.10), and turn to (1.11) as a second step. We introduce some notations:

let

Gi := τi − τi−1

and Mk := max
1≤i≤k

Gi .

be i.i.d. with distribution P(τ1 ∈ · | τ1 ≤ m).

We also let bG(m)

1

k

, . . . ,bG(m)

2.1. Proof of the local limit and local large deviation result (1.10). The proof is
divided into three steps, in which we control several contributions to P(τk = n).

• Step 1. Contribution of the case of only one jump larger than (1 − ǫ)n, all the other
ones being (necessarily) smaller than n/2. This gives the right order in Theorem 1.1
when k ≪ ϕ(n)−1;

• Step 2. Contribution of the case when all jumps are smaller than n/2: it is negligible,
so there must be one jump larger than n/2 (and there can be only one such jump);
• Step 3. Contribution of the case when there is one jump larger than n/2, but smaller

than (1 − ǫ)n. This is also negligible.

Step 1: We show that, for any ﬁxed ǫ > 0, and provided that kϕ(n) n→∞→ 0,

P(cid:16)τk = n, Mk > (1 − ǫ)n(cid:17) = (1 + O(ǫ)) kP(τ1 = n)(1 − rn)k,

as n → ∞.

(2.1)

We have

(2.2) P(cid:16)τk = n, Mk > (1 − ǫ)n(cid:17) = k(1 − rn)k−1

This gives the upper bound

ǫnXm=1

i = m! P(τ1 = n − m).

P  k−1Xi=1 bG(n)

P (τk = n, Mk > (1 − ǫ)n) ≤ k(1 − rn)k−1 max

(1−ǫ)n≤j≤n

P(τ1 = j)

(2.3)

provided that n is large enough.

≤ (1 + 2ǫ)k(1 − rn)kP(τ1 = n),

RENEWAL THEORY WITH NO MOMENTS

7

P(τ1 = j) .

(1−ǫ)n≤j≤n

x=1 ϕ(x) n→∞∼ nϕ(n), we have that for n large enough

In the other direction, (2.2) gives

i ≤ ǫn! min

P(cid:16)τk = n, Mk > (1 − ǫ)n(cid:17) ≥ k(1 − rn)k−1P  k−1Xi=1 bG(n)
Then, using that E[bG(n)

1 ]

1 ] = (1−rn)−1Pn
P k−1Xi=1 bG(n)
i ≤ ǫn! ≥ 1 −
P(cid:16)τk = n, Mk > (1 − ǫ)n(cid:17) ≥ (1 − 2ǫ)k(1 − rn)kP(τ1 = n).

E[bG(n)

2(k − 1)ϕ(n)

≥ 1 −

Therefore, since kϕ(n) → 0, we end up with

provided that n is large enough.

(2.4)

ǫn

.

ǫ

Step 2: We want to show that the main contribution to P(τk = n) comes when Mk ≥ n/2.

We prove that there exists a constant c2 > 0 such that, if kϕ(n) is small enough,

(2.5)

P (τk = n, Mk ≤ n/2) ≤ c2k2ϕ(n) P(τ1 = n)(1 − rn)k ,

which is negligible compared to (2.1) when kϕ(n) → 0. It is suﬃcient to show that, if kϕ(n)
is small enough,

P  kXi=1 bG(n)

i = n ; bG(n)

i ≤ n/2 for all i ≤ k! ≤ c2k2ϕ(n) P(τ1 = n)

To prove this, we rely on the following lemma.

Lemma 2.1. Suppose (1.1) holds with α = 0. There exist constants c3, c4 > 0 such that for
n large, for all 1 ≤ m ≤ n and k ≥ 0,

P  kXi=1 bG(m)

i ≥ n/2! ≤(cid:18)c3 kmϕ(m)

n

2m

(cid:19) n

≤(cid:16)c4kϕ(n)(cid:17) n

2m

.

Proof The second inequality is a consequence of the fact that mϕ(m) is asymptotically
increasing, so we prove the ﬁrst inequality.

For any λ > 0 we have

(2.6)

(2.7)

(2.8)

(2.9)

There exists a constant c5 such that for any j ≥ 1

1

.

P(cid:16) kXi=1 bG(m)
Eh(bG(m)

i ≥ n/2(cid:17) ≤ e−λn/2Eheλ bG(m)
1 ik
)ji ≤ mj−1 E[τ1 | τ1 ≤ m] ≤ c5mjϕ(m) .
1 i ≤ 1 + c5ϕ(m)(cid:0)emλ − 1(cid:1) .
Eheλ bG(m)
c5ϕ(m)(cid:0)emλ − 1(cid:1) =

n
km

,

Hence, for any λ > 0, we have

Now, let us deﬁne λ by

K. ALEXANDER AND Q. BERGER

≤ en/m

n

Eheλ bG(m)
1 ik
c5mkϕ(m)(cid:19)−n/2m
i ≥ n/2(cid:17) ≤(cid:18)c5kmϕ(m)

e−λn/2 =(cid:18)1 +
P(cid:16) kXi=1 bG(m)

n

(cid:19)n/2m

.

n

≤(cid:18)c5kmϕ(m)
(cid:19)n/2m

en/m ≤(cid:16)c5e2kmϕ(m)

n

2m .

(cid:17) n

(cid:3)

To control the probability on the left in (2.6), we decompose it according to the value of

. Let us denote ms := 2−sn and Js = (ms+1, ms]. We have

Therefore, (2.8) yields

i

the largest bG(n)
P(cid:16) kXi=1bG(n)

8

so that

and

(2.10)

(2.11)

i = n − m!

i ≤ m for all 2 ≤ i ≤ k ,

i ≤ n/2 for all i ≤ k(cid:17)
i = n ; bG(n)
k P bG(n)
kXi=2 bG(n)
= Xn/k≤m≤n/2
1 = m, bG(n)
1 = m(cid:17) P  kXi=2 bG(m)
i = n − m!
1 − rn(cid:19)k−1
k(cid:18)1 − rm
P(cid:16)bG(n)
≤ X1≤s≤log2 k Xm∈Js
P k−1Xi=1 bG(m)
i = n − m!
≤ 2k X1≤s≤log2 k Xm∈Js
2! ,
P k−1Xi=1 bG(ms)
≤ 2c6k X1≤s≤log2 k

ϕ(ms+1)

ϕ(m)

ms+1

m

≥

n

i

where in the last inequality we used that there exists c6 such that for suﬃciently large ms
and all m ∈ Js, ϕ(m) ≤ c6 ϕ(ms+1). Since n/k ≫ nϕ(n) → ∞, all values ms in (2.11) are
suﬃciently large in this sense, when n is large.

Since ϕ is slowly varying, given a ≤ 1 we have ϕ(an)/ϕ(n) ≤ 1/a for n large. With (2.11)

and Lemma 2.1 this shows that

P  kXi=1 bG(n)

i = n ; bG(n)

i ≤ n/2 for all i ≤ k! ≤ 2c6kXs≥1

≤ 8c6 k

2ms

2s+1ϕ(n)

2−(s+1)n(cid:16)c4 kϕ(n)(cid:17) n
4s(cid:16)c4 kϕ(n)(cid:17)2s−1
n Xs≥1

ϕ(n)

(2.12)

≤ c2k2ϕ(n)P(τ1 = n) ,

where we used in the last inequality that kϕ(n) is small. Hence, (2.6) is proved, and so is
(2.5).

RENEWAL THEORY WITH NO MOMENTS

9

Step 3: We show that the main contribution to P(τk = n) comes when not only Mk ≥ n/2,

but when Mk ≥ (1 − ǫ)n: we prove that for n large enough,

(2.13)

Indeed, we have that

P(cid:16)τk = n, n/2 < Mk ≤ (1 − ǫ)n(cid:17) ≤

6
ǫ

k2ϕ(n)P(τ1 = n) (1 − rn)k.

P(cid:16)τk = n, n/2 < Mk ≤ (1 − ǫ)n(cid:17) ≤ k (1 − rn)k−1 max
together with Markov’s inequality and the fact that E[bG(n)

enough. This yields (2.13).

n/2≤j≤n

Then, we use that maxn/2≤j≤n P(τ1 = j) ≤ 3P(τ1 = n) provided that n is large enough,
1 ] ≤ 2nϕ(n) when n is large

Combining (2.3)-(2.4) with (2.5) and (2.13), since ǫ is arbitrary we get that, uniformly for
(cid:3)

k such that kϕ(n) → 0, (1.10) holds.

P(τ1 = j)P k−1Xi=1 bG(n)

i ≥ ǫn! .

2.2. Proof of the uniform bound (1.11). To prove the uniform bound, we rely on Lemma
2.1, and we decompose the probability according to the value of Mk.

Let n ≥ n0 and deﬁne

ℓn = min{ℓ : 2ℓ ≥ n},

ℓn,k := max{ℓ : c3k2ℓϕ(2ℓ) ≤ 1

2n},

where c3 is the constant from Lemma 2.1.

Then for some (large) ℓ0, there exists a constant c7 > 0 such that for all ℓ0 < ℓ ≤ ℓn − 2,

P(cid:0)τk = n, Mk ∈ (2ℓ−1, 2ℓ](cid:1) ≤ kP(cid:18)G1 ∈ (2ℓ−1, 2ℓ], max
P (τ1 = m) P k−1Xi=1 bG(2ℓ)
2! .
2ℓ P  k−1Xi=1 bG(2ℓ)

≤ c7k(1 − r2ℓ)k−1 ϕ(2ℓ)

≤ k(1 − r2ℓ)k−1 max

m∈(2ℓ−1,2ℓ]

i >

2≤i≤k

n

Gi ≤ 2ℓ, τk = n(cid:19)
i ∈ (n − 2ℓ, n]!

We now have 4 cases according to the value of ℓ.

(2.14)

Case 1. For ℓ0 ∨ ℓn,k < ℓ ≤ ℓn − 2 we bound the last probability in (2.14) by 1, and

observe that provided ℓ0 is large enough, r2ℓ − rn ≥ 1

2ϕ(2ℓ), which leads to

P(cid:0)τk = n, 2ℓ0∨ℓn,k < Mk ≤ 2ℓn−2(cid:1) ≤ 2c7 k(1 − rn)k

≤ 2c7 k(1 − rn)k

(2.15)

≤ 2c7 k(1 − rn)k ϕ(n)
n

n
2ℓ

ϕ(2ℓ)
ϕ(n)

e−n/8c32ℓ

,

r2ℓ − rn

1 − rn (cid:19)k

ϕ(2ℓ)

2ℓ (cid:18)1 −
ϕ(2ℓ)
2ℓ e−kϕ(2ℓ)/4

ℓn−2Xℓ=ℓ0∨ℓn,k+1
ℓn−2Xℓ=ℓ0∨ℓn,k+1
ℓn−2Xℓ=1

10

K. ALEXANDER AND Q. BERGER

where we used that 2ℓϕ(2ℓ) is asymptotically increasing in ℓ. We obtain easily that the last
sum remains bounded as n → ∞. In the end, we have a constant c8 > 0 such that for n ≥ n0

Case 2. To handle ℓ = ℓn − 1, ℓn we have analogously to (2.14), for n ≥ n0

P(cid:0)τk = n, 2ℓ0∨ℓn,k < Mk ≤ 2ℓn−2(cid:1) ≤ c8k(1 − rn)kP(τ1 = n).
P(cid:0)τk = n, Mk > 2ℓn−2(cid:1) ≤ k(1 − rn)k−1

m∈(2ℓn−2,2ℓn ]
≤ c9k(1 − rn)kP(τ1 = n).

max

P (τ1 = m)

(2.16)

(2.17)

Case 3. We now deal with ℓ0 < ℓ ≤ ℓn,k. We bound the last probability in (2.14) using

Lemma 2.1. We obtain, analogously to (2.15)

P(cid:0)τk = n, 2ℓ0 < Mk ≤ 2ℓn,k(cid:1) ≤ 2c7k

ℓn,k∧ℓnXℓ=ℓ0+1

(cid:17)n/2ℓ+1

n

ϕ(2ℓ)

2ℓ (cid:16) c3k2ℓϕ(2ℓ)
(1 − r2ℓ)k ϕ(2ℓ)
2(cid:17)n/2ℓ+1
2ℓ (cid:16) 1
ℓnXℓ=ℓ0+1
2(cid:17)n/2ℓn+1
2ℓn+1 (cid:16) 1

≤ 2c7k(1 − rn)k

≤ c10k(1 − rn)k ϕ(2ℓn+1)
≤ c11k(1 − rn)k ϕ(n)
n
= c11k(1 − rn)kP(τ1 = n).

(2.18)

Here the third inequality uses the fact that n/2ℓn+1 ≥ 1/4, and consequently the sum in the
second line of (2.18) is of the same order as the ℓ = ℓn term.

Case 4. Finally to handle ℓ ≤ ℓ0 we have, using Lemma 2.1 and writing m0 := 2ℓ0

= n!

P(τk = n, Mk ≤ 2ℓ0) ≤ (1 − rm0)kP  kXi=1 bG(m0)
≤ (1 − rn)k(cid:18)1 − rm0
≤ (1 − rn)k e−c12k(cid:18)minn c13k

n

i

, 1o(cid:19)n/m0

.

1 − rn (cid:19)k(cid:18)minnc3m0ϕ(m0)k

n

, 1o(cid:19)n/m0

(2.19)

Considering separately the cases k ≤ n/2c13 and n/2c13 < k ≤ n, we conclude that there is
some c14 > 0 such that for n large,

(2.20)

P(τk = n, Mk ≤ 2ℓ0) ≤ (1 − rn)ke−c14n ≤ c15k(1 − rn)kP(τ1 = n).

Collecting (2.16),(2.17),(2.18) and (2.20) concludes the proof of (1.11).

(cid:3)

RENEWAL THEORY WITH NO MOMENTS

11

3. Large deviations: proof of Theorem 1.2

, . . . are i.i.d. with distribution P(τ1 ∈ · | τ1 ≤ m).

Recall that Gi = τi−τi−1, and bG(m)

1

2

,bG(m)

Given 0 < ǫ < 1,

Proof of (i). The second inequality is trivial, so we prove the ﬁrst. Suppose kϕ(n) ≤ M.

so for large n,

rǫn − rn ∼ ϕ(n) log

1
ǫ

as n → +∞,

(3.1)

(3.3)

On the other hand, since E[bG(m)

1

(3.2)

] m→∞∼ mϕ(m), given ǫ > 0 we have for n large enough

ǫ(cid:19) ≥ ǫ2M .

1

rǫn − rn

≥ exp(cid:18)−2kϕ(n) log

i ≤ ǫn(cid:19) =(cid:18)1 −
i ≤ n! ≥ 1 −

1 − rn (cid:19)k
P(cid:18)max
i≤k bG(n)
P  kXi=1 bG(ǫn)
kE(cid:16)bG(ǫn)
1 (cid:17) ≥ 1 − 2ǫkϕ(n) ≥ 1 − 2ǫM.
P(τk ≤ n) ≥ (1 − rn)kP  nXi=1 bG(n)
i ≤ n! ≥ (1 − rn)k(1 − 2M) .

1
n

If M ≤ 1/3, we apply (3.2) with ǫ = 1:

If M > 1/3, we take ǫ = 1/4M, and combining (3.1) with (3.2), we obtain for n large enough

P(τk ≤ n) ≥ (1 − rn)kP(cid:18)max

Proof of (ii). Deﬁne, for any λ > 0,

(1 − rn)k.

1

2(cid:18) 1
4M(cid:19)2M

i ≤ n! ≥

i ≤ ǫn(cid:19) P  kXi=1 bG(ǫn)
i≤k bG(n)
ν(λ) := 1 − E(cid:0)e−λτ1(cid:1) ,
λ(cid:19) → +∞ as λ ց 0.
ϕ(cid:18) 1

ν′(λ) ∼

and

1
λ

so − log(1 − ν(·)) is non-decreasing and strictly concave. Moreover, it is standard to obtain
that

(3.4)

ν(λ) ∼ r(cid:18) 1

λ(cid:19) → 0,

We may view (1.13) as a combination of an upper and a lower bound, which we now prove.

Upper bound in (1.13). Deﬁne

fn(λ) := −nλ − k log(1 − ν(λ));

note the notation suppresses the dependence on k. We will use the standard exponential
bound

(3.5)

P(τk ≤ n) = P(cid:0)e−λτk ≥ e−λn(cid:1) ≤ eλn(1 − ν(λ))k = e−fn(λ)

n(λn) = 0, or equivalently,

Now, we deﬁne λn > 0 by f ′

for all λ > 0 .

(3.6)

ν′(λn)

1 − ν(λn)

=

n
k

,

12

K. ALEXANDER AND Q. BERGER

so that fn achieves its (positive) supremum at λn. Then λn → 0, since n/k → +∞.
Therefore, thanks to (3.4), we get that

n
k

(3.7)

which is equivalent to

(3.8)

n→∞∼ ν′(λn) n→∞∼

1
λn

n→∞∼

n
k

1
λn

ϕ(cid:18) 1
λn(cid:19) ,
k(cid:17) .
ϕ∗(cid:16) n

k

(3.9)

(3.10)

Then, (3.4) gives that ν(λn) n→∞∼ r(1/λn) ≫ ϕ(1/λn), which with (3.7) shows that nλn ≪
kν(λn). In the end, we get

With (3.5) this lets us conclude

Lower bound in (1.13) As is standard, we will obtain a corresponding lower bound using

P(τk ≤ n) ≤ exph−(1 + o(1))k r(cid:16) n

fn(λn) = (1 + o(1))kν(λn) n→∞∼ k r(cid:18) 1

λn(cid:19) n→∞∼ k r(cid:16)n
k(cid:17)(cid:17) .
ϕ∗(cid:16) n
k(cid:17)(cid:17)i .
ϕ∗(cid:16)n
a tilted distribution. Let ǫ > 0, and leteλn satisfy (analogously to (3.6))
Then, let eP,eE,eVar denote the probability, expectation and variance with respect to the

tilted distribution of the i.i.d. sequence (G1, G2, . . . ) given by

ν′(eλn)
1 − ν(eλn)

= (1 − ǫ)

(3.11)

k

n
k

.

E(cid:16)e−eλnτ11{τ1∈·}(cid:17)
eP(G1 ∈ ·) =
E(cid:16)e−eλnτk1{τk∈((1−2ǫ)n,n)}(cid:17)

E(e−eλnτ1)

E(e−eλnτk)

.

We estimate

P(τk ≤ n) ≥

E(e−eλnτk)
e−(1−2ǫ)neλn

(3.12)

≥ exp(cid:18)(1 − 2ǫ)neλn + k log(1 − ν(eλn))(cid:19)eP(cid:0)τk ∈ ((1 − 2ǫ)n, n)(cid:1).

Note that (3.8) translates here as

1

n→∞∼ (1 − ǫ)

k(cid:17) ,
ϕ∗(cid:16)n
so that ν(eλn) n→∞∼ r(1/eλn) n→∞∼ ν(λn). As in (3.9), we get that
k(cid:17)(cid:17)i ×eP(cid:0)τk ∈ ((1 − 2ǫ)n, n)(cid:1) ,
ϕ∗(cid:16) n

P(τk ≤ n) ≥ exph−(1 + o(1))k r(cid:16)n

and it only remains to show that the last probability converges to 1 as n → +∞.

eλn

(3.13)

n
k

k

RENEWAL THEORY WITH NO MOMENTS

13

It is standard that

(3.14)

(3.15)

so we only need to show that eVar (G1) = o(n2/k). In fact, we have
ϕ(cid:18) 1
eλn(cid:19) n→∞∼

eE(cid:2)(G1)2(cid:3) =

jϕ(j)e−eλnj n→∞∼

(eλn)2

where the last equivalence is a slight variant of (3.7). Since kϕ(n) → ∞, by a similar variant
of (3.8) we have

(1 − ǫ)

n
k

,

eλn

1

1

1

ν′(eλn)
1 − ν(eλn)

eE (G1) =
∞Xj=1
1 − ν(eλn)
eλn(cid:19) ∼ (1 − ǫ)
ϕ(cid:18) 1

eλn

1

= (1 − ǫ)

n
k

,

n
k

≪ nϕ(n),

and thereforeeλ−1

Proof of (iii). The existence of I(b) is standard, and its asymptotics as b → ∞ simply

n = o(n). With (3.15) this shows that indeed eVar (G1) = o(n2/k).

follow from (ii).

4. Reverse renewal theorems

4.1. Transient case, proof of Theorem 1.4. Denote p∞ := P(τ1 = +∞) > 0. We ﬁx
ǫ > 0, and A large enough so P(τ1 > A) ∈ [p∞, p∞ + ǫ], and hence P(A < τ1 < +∞) ≤ ǫ.
We then deﬁne the events

A1 = {τ ∩ (0, A] = ∅}

and A2 = {τ ∩ [n − A, n) = ∅}.

We claim that if n is large enough,

(4.1)

(1 − ǫ)P(τ1 ≤ A) ≤ P(Ac
(1 − ǫ)P(τ1 ≤ A) ≤ P(Ac

1|n ∈ τ ) ≤ (1 + ǫ)P(τ1 ≤ A) ,
2|n ∈ τ ) ≤ (1 + ǫ)P(τ1 ≤ A) ,

(1 − ǫ)P(τ1 ≤ A)2 ≤ P(Ac

1 ∩ Ac

2|n ∈ τ ) ≤ (1 + ǫ)P(τ1 ≤ A)2 .

Indeed, we can write

(4.2)

P(Ac

1 ∩ Ac

2|n ∈ τ ) =

AXi=1

AXj=1

P(τ1 = i)P(τ1 = j)

P(n − i − j ∈ τ )

P(n ∈ τ )

.

Since P(n ∈ τ ) is regularly varying, for large n, the last ratio is close to 1 uniformly in
i, j ≤ A, and the third line in (4.1) follows. The ﬁrst two lines are proved similarly.

It follows from (4.1) that

P(A1 ∩ A2|n ∈ τ ) = 1 − P(Ac

1|n ∈ τ ) − P(Ac

2|n ∈ τ ) + P(Ac

1 ∩ Ac

1|n ∈ τ )

(4.3)

≤ 1 − 2P(τ1 ≤ A) + P(τ1 ≤ A)2 + 3ǫ
≤ P(τ1 > A)2 + 3ǫ
≤ (p∞ + ǫ)2 + 3ǫ .

Therefore for large n,

(4.4)

P(τ1 = n) ≤ P(A1, A2, n ∈ τ ) ≤(cid:0)(p∞ + ǫ)2 + 3ǫ(cid:1) P(n ∈ τ ) .

14

K. ALEXANDER AND Q. BERGER

Similarly to (4.3), P(A1 ∩ A2|n ∈ τ ) ≥ p2

∞ − 3ǫ and hence

(4.5)

P(A1, A2, n ∈ τ ) ≥ (p2

∞ − 3ǫ)P(n ∈ τ ) .

To turn this into a lower bound on P(τ1 = n), we show that conditionally on {A1, A2, n ∈
τ }, it is very likely that τ1 = n. More precisely, we claim that there exists c16 such that, for
n large,

(4.6)

P(τ1 6= n, A1, A2, n ∈ τ ) ≤ c16ǫP(n ∈ τ ).

With (4.5), this shows that

(4.7)

P(τ1 = n) = P(τ1 = n, A1, A2, n ∈ τ ) ≥(cid:0)p2

∞ − 3ǫ − c16ǫ(cid:1) P(n ∈ τ ) .

Since ǫ is arbitrary, (4.4) and (4.7) complete the proof of Theorem 1.4.

To prove (4.6), we write

P(τ1 6= n, A1, A2, n ∈ τ ) ≤

(4.8)

n/2Xi=A+1
n−A−1Xi=n/2+1

+

P(τ1 = i)P(n − i ∈ τ )

n−iXj=A+1

P(τ1 = i)P(τ1 = j)P(n − i − j ∈ τ ) .

For the ﬁrst sum in (4.8), since P(n ∈ τ ) is regularly varying, there is a constant c17 such

that, provided that n is large, P(n − i ∈ τ ) ≤ c17P(n ∈ τ ) for every i ≤ n/2. Hence

(4.9)

P(τ1 = i)P(n − i ∈ τ ) ≤ c17P(n ∈ τ )P(A < τ1 < +∞) ≤ c17 ǫ P(n ∈ τ ) .

For the second sum in (4.8), we use that P(τ1 = i) ≤ P(i ∈ τ ) ≤ c17P(n ∈ τ ) for n large

enough, since i ∈ (n/2, n). Therefore,

n/2Xi=A+1

n−A−1Xi=n/2+1

n−iXj=A+1

P(τ1 = i)P(τ1 = j)P(n − i − j ∈ τ )

(4.10)

≤ c17P(n ∈ τ )P(A < τ1 < +∞) ×

and the proof of (4.6) is complete.

P(k ∈ τ ) ≤

c17
p∞

ǫ P(n ∈ τ ) ,

+∞Xk=0

4.2. Recurrent case, proof of Theorem 1.3. We assume now that τ is recurrent, so that
Un grows to inﬁnity as a slowly varying function. We can rewrite (1.15) as

(4.11)

P(τ1 ∈ ((1 − ǫn)n, n]) = r(1−ǫn)n − rn

n→∞∼ r2

n ǫnn P(n ∈ τ ) .

Note that rn is slowly varying by (1.14), so if ǫn → 0 slowly enough then rǫnn ∼ rn. We
prove separately an upper and lower bound for the probability on the left in (4.11). The
upper bound is provided by the following lemma, which will also be useful in the proof of
the lower bound.

RENEWAL THEORY WITH NO MOMENTS

15

Lemma 4.1. Assume that P(n ∈ τ ) is regularly varying and Un is slowly varying. Suppose
ǫn ∈ (0, 1) satisﬁes rǫnn
(4.12)

P(τ1 ∈ ((1 − ǫn)n, n]) = r(1−ǫn)n − rn ≤ (1 + o(1)) r2

n→∞∼ rn. Then

n ǫnn P(n ∈ τ ) .

Proof The idea is to obtain a lower bound on ǫnnP(n ∈ τ ) ∼ Un − U(1−ǫn)n, the mean
number of renewals in ((1 − ǫn)n, n], by considering trajectories in which the ﬁrst “big” gap
has size in ((1 − ǫn)n, n], and lands in the interval ((1 − ǫn)n, (1 + δn)n], see (4.15).

Since {rk} is slowly varying, we can choose such δn = o(ǫn) with rδnn

n→∞∼ rn. Deﬁne

(4.13)

In := min{i ≥ 1 : τi − τi−1 > δnn},

and Ln := τIn − τIn−1 ,

the index and length of the ﬁrst “big” gap. Let Nn be the number of renewals in ((1 −
ǫn)n, (1 + 2δn)n], and let Tn be the location of the ﬁrst renewal in the smaller interval
((1 − ǫn)n, (1 + δn)n], when one exists, otherwise Tn = ∞. Then

E[Nn | Tn = t] ≥ Uδnn

for all t ∈ ((1 − ǫn)n, (1 + δn)n],

EhNn | Tn ∈ ((1 − ǫn)n, (1 + δn)n]i ≥ Uδnn.

By independence, we therefore get that

P(cid:16)Tn ∈ ((1 − ǫn)n, (1 + δn)n](cid:17) ≥ P (τIn−1 ≤ δnn) P(cid:0)Ln ∈ ((1 − ǫn)n, n](cid:1) .

We claim that

P (τIn−1 ≤ δnn) → 1 as n → ∞,

P(cid:16)Tn ∈ ((1 − ǫn)n, (1 + δn)n](cid:17) ≥ (1 − o(1))P(cid:0)τ1 ∈ ((1 − ǫn)n, n] | τ1 > δnn(cid:1) .

Together with (4.14), this shows that

(4.18) U(1+2δn)n −U(1−ǫn)n = E(Nn) ≥ (1−o(1))

r2
n
where we used (1.14). Since P(k ∈ τ ) is regularly varying and δn ≪ ǫn, we also have

rδnn

Uδnn = (1−o(1))

r(1−ǫn)n − rn

r(1−ǫn)n − rn

,

(4.19)

U(1+2δn)n − U(1−ǫn)n

n→∞∼ (ǫn + 2δn)nP(n ∈ τ ) n→∞∼ ǫnn P(n ∈ τ ) .

Together with (4.18), this completes the proof of the lemma.

We now prove (4.16). Notice that

so

(4.14)

(4.15)

(4.16)

so that

(4.17)

(4.20)

(4.21)

(4.22)

Therefore we can choose Dn → ∞ satisfying DnE[τ11{τ1≤δnn}] = o(δnn rn). Then,

E(cid:2)τ11{τ1≤δnn}(cid:3) = X0≤j<δnn

(rj − rδnn) ≪ X0≤j<δnn

rj

n→∞∼ δnnrn.

P(cid:18)In ≤
E(cid:20)τIn−1(cid:12)(cid:12)(cid:12)(cid:12) In ≤

Dn

Dn

rn(cid:19) = 1 − (1 − rδnn)Dn/rn n→∞→ 1.
rn(cid:21) ≤

(1 − rδnn)−1E(cid:2)τ11{τ1≤δnn}(cid:3) = o(δnn).

Dn
rn

Also, for all j ≥ 1, E[τIn−1 | In = j] = (j − 1)E[τ1 | τ1 ≤ δnn], so that

16

K. ALEXANDER AND Q. BERGER

With (4.21), it shows that

(4.23) P (τIn−1 ≤ δnn) ≥ P(cid:18)τIn−1 ≤ δnn(cid:12)(cid:12)(cid:12)(cid:12) In ≤

Dn

rn(cid:19) P(cid:18)In ≤

Dn

rn(cid:19) → 1 as n → +∞.

(cid:3)

For the rest of the section, we let δn, In, Ln be as in the proof of Lemma 4.1.

We now complete the proof of Theorem 1.3 by proving that the upper bound in Lemma 4.1
is sharp, essentially by showing that the lower bound (4.17) is sharp. More precisely, denoting
Jn the numbers of renewals in ((1 − ǫn + 2δn)n, n], we claim that

(4.24)

P(Jn ≥ 1) ≤ (1 + o(1))

r(1−ǫn)n − rn

rn

.

Since E[Jn | Jn ≥ 1] ≤ Uǫnn ≤ Un, together with (1.14) this yields

Un − U(1−ǫn+2δn)n = E[Jn] ≤ P(Jn ≥ 1)Un ≤ (1 + o(1))

r(1−ǫn)n − rn

r2
n

.

Analogously to (4.19) we have Un − U(1−ǫn+2δn)n
gives (4.12) with the inequality reversed. We are therefore left with proving (4.24).

n→∞∼ ǫnnP(n ∈ τ ) since δn = o(ǫn), and this

First, we deal with the main contribution to the event {Jn ≥ 1}:

P(cid:0)Jn ≥ 1, τIn−1 ≤ δnn, τIn ∈ ((1 − ǫn + δn)n, n](cid:1) ≤ P(cid:0)τIn−1 ≤ δnn , Ln ∈ ((1 − ǫn)n, n](cid:1)

r(1−ǫn)n − rn

≤

rδnn

(4.25)

≤ (1 + o(1))rn ǫnnP(n ∈ τ ) ,

where we used Lemma 4.1 and rδnn ∼ rn for the last inequality.

Then we need to show that other ways of achieving Jn ≥ 1 have a negligible probability.

We claim that

(4.26)

(4.27)

and

(4.28)

P(cid:0)Jn ≥ 1, τIn−1 ≤ n
P(cid:0)Jn ≥ 1, δnn < τIn−1 ≤ n
P(cid:0)τIn−1 > n

2 , τIn ≤ (1 − ǫn + δn)n(cid:1) = o(1) rn ǫnnP(n ∈ τ ) ,

2 , τIn ∈ ((1 − ǫn + δn)n, n](cid:1) = o(1) rn ǫnnP(n ∈ τ ) ,
2(cid:1) = o(1) rn ǫnnP(n ∈ τ ) .

In (4.26) the big gap ends early (relative to (4.25)), in (4.27) it starts late, and in (4.28) it
starts very late. All together, (4.25)–(4.28) prove (4.24).

Proof of (4.26). Let us ﬁx n ≥ 1 and let

Q1 = (n − 2ǫnn, (1 − ǫn + δn)n], Qq = (n − 2qǫnn, n − 2q−1ǫnn] for q ≥ 2,

RENEWAL THEORY WITH NO MOMENTS

17

and let qn = min{q : 2qǫn ≥ 1}. For any ﬁxed j ≤ n/2 the position of τIn−1, we decompose
according to the interval Qq containing τIn:

P(cid:16)Jn ≥ 1,τIn ≤ (1 − ǫn + δn)n | τIn−1 = j(cid:17) =

qnXq=1

(4.29)

≤

P(τ1 ∈ Qq − j | τ1 ≥ δnn) max
m∈n−Qq

qnXq=1

P (Jn ≥ 1, τIn ∈ Qq | τIn−1 = j)

P(cid:16)|τ ∩ (m − (ǫn − 2δn)n, m]| ≥ 1(cid:17).

Here the big gap corresponds to [j, n − m].

First, we control the last probability in (4.29). Note that for any given interval (a, b] ⊂
(0, ∞], conditioning on the location of the ﬁrst renewal (if any) in (a, b], we get for any h ≥ 1

(4.30)

P(cid:0)|τ ∩ (a, b]| ≥ 1(cid:1)Uh ≤ Ub+h − Ua .

For any q ≥ 2 and m ∈ n − Qq we have m ∈ (2q−1ǫnn, 2qǫnn], and in particular m ≥ 2ǫnn.

Therefore, applying (4.30) with h = δnn, there exists a constant c18 such that

(4.31) P(cid:0)|τ ∩ (m − (ǫn − 2δn)n, m]| ≥ 1(cid:1) ≤

Um+δnn − Um−ǫnn

Uδnn

≤ c18rnǫnnP(2qǫnn ∈ τ ).

For the last inequality we used regular variation of P(k ∈ τ ), and (1.14).

For q = 1 and m ∈ n − Qq, we have m ∈ ((ǫn − δn)n, 2ǫn). Hence, applying again (4.30)

with h = δnn, we get
(4.32)

P(cid:0)|τ ∩ (m − (ǫn − 2δn)n, m]| ≥ 1(cid:1) ≤ P(cid:0)|τ ∩ (δnn, 2ǫnn]| ≥ 1(cid:1) ≤

U(ǫn+δn)n − Uδnn

Uδnn

n→∞→ 0,

n→∞∼ Un

n→∞∼ Uδnn for the last convergence, which is uniform in

where we used that U(ǫn+δn)n
m ∈ n − Qq.

We now control P(τ1 ∈ Qq − j | τ1 ≥ δnn). For 1 ≤ q ≤ qn − 3, we have 2qǫnn < n/4 so
n − 2qǫnn − j > n/4. Hence, using Lemma 4.1 (with an interval of length 2q−1ǫnn, which
satisﬁes the hypotheses since rn ≤ r2q−1ǫn ≤ rǫnn

n→∞∼ rn),

P(τ1 ∈ Qq − j) ≤ (1 + o(1))r2

n 2q−1ǫnn max
n/4<x≤n

P(x ∈ τ ) ≤ c19r2

n2q−1ǫnn P(n ∈ τ ),

and therefore, there is some constant c20 such that

(4.33)

P(τ1 ∈ Qq − j | τ1 ≥ δnn) ≤ c20 rn 2q−1ǫnnP(n ∈ τ ).

For q = qn − 2, qn − 1, qn we have

(4.34)

P(τ1 ∈ Qq − j | τ1 ≥ δnn) ≤

rδnn − rn

rδnn

n→∞→ 0,

and, similarly to (4.31), there is a constant c21 such that, for m ∈ n − Qq,

(4.35)

P(|τ ∩ (m − ǫnn, m]| ≥ 1) ≤ c21 rn ǫnn P(n ∈ τ ).

18

K. ALEXANDER AND Q. BERGER

Combining (4.31)–(4.35) with (4.29), we ﬁnally obtain

P(cid:0)Jn ≥ 1,τIn ≤ (1 − ǫn − δn)n | τIn−1 = j(cid:1)
≤ c22 rn ǫnn P(n ∈ τ ) o(1) + rn

(4.36)

Now there is a constant c23 such that

2q−1ǫnnP(2qǫnn ∈ τ )! .

qn−3Xq=2

Therefore, since Un − Uǫnn = o(Un) = o(r−1

2q−1ǫnnP(2qǫnn ∈ τ ) ≤ c23

qn−3Xq=1 Xx∈n−Qq

qn−3Xq=2
P(cid:0)Jn ≥ 1, τIn ≤ (1 − ǫn − δn)n | τIn−1 = j(cid:1) = o(1) rn ǫnn P(n ∈ τ ) ,

n ), we get that

P(x ∈ τ ) ≤ c23(Un − Uǫnn).

uniformly for j ≤ n/2, and (4.26) follows.

Proof of (4.27). For all j ∈ (δnn, n/2], by Lemma 4.1 we have

P(cid:0)τIn ∈ ((1 − ǫn + δn)n, n] | τIn−1 = j(cid:1) = P(cid:0)τ1 ∈ ((1 − ǫn + δn)n − j, n − j] | τ1 > δnn(cid:1)

≤ (1 + o(1))rn(ǫn − δn)nP(n − j ∈ τ ) ≤ c24 rn ǫnn P(n ∈ τ ) .

(4.37)

Therefore, we get that

P(cid:0)τIn ∈ ((1 − ǫn − δn)n, n] | δnn < τIn−1 ≤ n

and since P (τIn−1 > δnn) n→∞→ 0 (see (4.23)), (4.27) follows.

2(cid:1) ≤ c24rn ǫnn P(n ∈ τ ) ,

Proof of (4.28). Since P(n ∈ τ ) is regularly varying and Un is slowly varying with
Un → ∞, P(n ∈ τ ) must have index of regular variation −1. Hence by [5, Proposition
1.5.9a] we have nP(n ∈ τ ) ≪ Un. Therefore

is a slowly varying function which by (1.14) satisﬁes

(4.39)

and (4.28) is equivalent to proving

(4.38)

(4.40)

n nP(n ∈ τ )

bϕ(n) := r2

bϕ(n) ≪ rn,

P(τIn−1 > n/2) = o(1) ǫnbϕ(n)

rn

.

Note that if we knew (1.1) held, necessarily with α = 0, we could conclude from (1.3) that

(4.41)

bϕ(n) ∼ ϕ(n), so bϕ may be viewed as a surrogate for ϕ in the absence of (1.1). Decomposing
over the value of In − 1, we get, recalling bG(m)
> n/2! .

P(In−1 = k , τk > n/2) =

P(τIn−1 > n/2) =

rδnn(1−rδnn)k−1P  kXi=1 bG(δnn)

We now need an analogue of Lemma 2.1.

from Section 2,

+∞Xk=1

+∞Xk=1

i

i

RENEWAL THEORY WITH NO MOMENTS

19

Lemma 4.2. Assume that P(n ∈ τ ) is regularly varying with index −1 and Un is slowly
varying. There exist m0, c25 > 0 such that for all m0 ≤ m ≤ n and k ≥ 1,

(4.42)

(4.43)

P  kXi=1 bG(m)

i ≥

n

2! ≤(cid:18) c25kmbϕ(m)

n

2m

(cid:19) n

.

Proof The proof follows the same lines as that of Lemma 2.1. The only modiﬁcation needed
is in (2.9): the computation of E[τ11{τ1≤m}] required knowledge on P(τ1 = n). Therefore it
is suﬃcient to show that there exists some constant c26 such that

Indeed, thanks to Lemma 4.1, we get that there exist ℓ0 > 0 and a constant c27 such that,
for any ℓ ≥ ℓ0

E[τ11{τ1≤m}] ≤ c26mbϕ(m) .

Therefore for any m ≥ ℓ0,

E[τ11{τ1≤m}] ≤ 2ℓ0 +

P(cid:0)τ1 ∈ (2ℓ−1, 2ℓ](cid:1) ≤ c27(r2ℓ)22ℓ−1P(2ℓ ∈ τ ) ≤ c27bϕ(2ℓ) .
⌈log2(m)⌉Xℓ=ℓ0+1
2ℓP(cid:0)τ1 ∈ (2ℓ−1, 2ℓ](cid:1)
⌈log2(m)⌉Xℓ=1
2ℓbϕ(2ℓ)
mXx=1bϕ(x) ,

≤ 2ℓ0 + c27

≤ 2ℓ0 + c28

and (4.43) is proved.

(cid:3)

Going back to (4.41), we can apply Lemma 4.2 with m = δnn, and note that 2rn ≥ rδnn ≥

rn for large n, to obtain

(4.44)

e−krnk1/2δn .

The sum here is readily approximated by a gamma function, and an application of Stirling’s
formula then yields

c29
δ1/2

e−krnk1/2δn ≤

+∞Xk=1
P(τIn−1 > n/2) ≤ 3rn(cid:0)c25δnbϕ(δnn)(cid:1)1/2δn
n rn(cid:18)c30bϕ(δnn)
δnrn (cid:19)1/2δn
n(cid:18)c32bϕ(δnn)
rn (cid:19)2
P(τIn−1 > n/2) ≤ ǫn(cid:18)bϕ(n)

+∞Xk=1
n (cid:18)c31bϕ(δnn)

rn (cid:19)1/2δn

c29
δ1/2

rn (cid:19)1/2δn

≤ δ3

,

,

so that for large n,
(4.45)

P(τIn−1 > n/2) ≤

δn = o(ǫn), we therefore obtain

Since bϕ is slowly varying, for n large enough we have δnbϕ(δnn) ≤ bϕ(n). Since we chose

(4.46)

≤ δn(cid:18)δnbϕ(δnn)

rn

(cid:19)2

.

20

K. ALEXANDER AND Q. BERGER

which with (4.39) completes the proof of (4.40), hence also of (4.28), (4.24) and ﬁnally (4.11),
or equivalently (1.15).

Equation (1.16) is an immediate consequence of (1.15).

(cid:3)

4.3. Why not expect a stronger reverse renewal theorem? In general, regular varia-
tion of P(n ∈ τ ) (here with index of regular variation −1) does not imply regular variation
of P(τ1 = n). This shows that (1.16) cannot be true in general under the assumptions used
to obtain (1.15). We give here only a description of an example, without proof details.

Let σ be a recurrent renewal with inter-arrival distribution of form

(4.47)

P(σ1 = n) = ϕ(n) n−1.

Now, let τ1 be 2σ1 or 1, with probability 1/2 each:

P(τ1 = 1) = 1

2 , P(τ1 = 2m) = 1

2 P(σ1 = m) , P(τ1 = 2m − 1) = 0

for m ≥ 1.

Note that rn := P(τ1 > n) ∼ 1
2P(σ1 > n). Then P(τ1 = n) is not regularly varying, but it
can be shown that the gaps of length 1 have a smoothing eﬀect, and make P(n ∈ τ ) regularly
varying. More precisely, it can be proved that
ϕ(n)
2r2
nn

P(τ1 = 2⌊ n
2 ⌋)
2P(τ1 > n)2 ,

P(n ∈ τ ) n→∞∼

(4.48)

n→∞∼

where ⌊·⌋ denotes the integer part.

Acknowledgments: The authors are grateful to V. Wachtel for bringing the work of

Nagaev [21] to their attention.

References

[1] K. S. Alexander and Q. Berger, Local asymptotics for the ﬁrst intersection of two independent renewals,

preprint, arXiv:1603.05531 [math.PR]

[2] S. Asmussen, Applied Probability and Queues, Second Edition, Applications of Mathematics 51,

Springer-Verlag, New York, 2003.

[3] F. Caravenna, The strong renewal theorem, preprint, arXiv:1507.07502 [math.PR]
[4] J. Chover, P. Ney and S. Wainger, Functions of probability measures, J. Anal. Math., 25 pp. 255–302,

1973.

[5] N. H. Bingham, C. M. Goldie and J. L. Teugels, Regular variations, Cambridge University Press,

Cambridge, 1987.

[6] Z. Chi, Strong renewal theorem with inﬁnite mean beyond local large deviations, Ann. Appl. Probab. 25

(2015), pp. 1513–1539.

[7] Z. Chi, Integral criteria for Strong Renewal Theorems with inﬁnite mean, preprint, arXiv:1312.6089v3

[math.PR]

[8] D. A. Darling, The inﬂuence of the maximum term in the addition of independent random variables,

Trans. Amer. Math. Soc. 73, pp. 95–107, 1952.

[9] R. A. Doney, One-sided local large deviation and renewal theorems in the case of inﬁnite mean, Probab.

Theory Relat. Fields, 107, pp. 451-465, 1997.

[10] R. A. Doney, The strong renewal theorem with inﬁnite mean via local

large deviations, preprint,

arXiv:1507.06790 [math.PR]

[11] R. A. Doney and D. A. Korshunov, Local asymptotics for the time of ﬁrst return to the origin of transient

random walk, Stat. Probab. Letters, 81 5, pp. 363–365, 2011.

[12] K. B. Erickson, Strong renewal theorems with inﬁnite mean, Transaction of the American Mathematical

Society, 151, 1970.

[13] W. Feller, An introduction to probability theory and its applications, Vol. 1, 2nd edition, Wiley series in

probability and mathematical statistics, John Wiley & Sons. Inc., New York-London-Sydney, 1966.

RENEWAL THEORY WITH NO MOMENTS

21

[14] A. Garsia and J. Lamperti, A discrete renewal theorem with inﬁnite mean, Comm. Math. Helv. 37,

pp. 221-234, 1963.

[15] G. Giacomin, Random polymer models, Imperial College Press, 2007.
[16] B. V. Gnedenko, Sur la distribution limite du terme maximum d’une s´erie al´eatoire, Ann. Math. (2)

44, pp. 423–453, 1943.

[17] B. V. Gnedenko and A. N. Kolmogorov, Limit Theorems for Sums of Independent Random Variables,

Addison-Wesley, Cambridge, 1954.

[18] N. C. Jain and W. E. Pruitt, The range of random walk, Proc. Sixth Berkeley Symp. Math. Statist.

Probab. 3 pp. 31–50, Univ. California Press, Berkeley, 1972.

[19] Y. Kasahara, A limit theorem for sums of i.i.d. random variables with slowly varying tail probability, J.

Math. Kyoto Univ. 26, pp. 437–443, 1986.

[20] H. Kesten, Ratio Theorems for Random Walks II, J. Analyse Math. 11, pp. 323–379, 1963.
[21] S. V. Nagaev, The Renewal Theorem in the Absence of Power Moments, Theory Probab. Appl., 56

No. 1, pp. 166–175, 2012.

[22] S. V. Nagaev and V. I. Wachtel, On sums of independent random variables without power moments,

Sib. Math. J., 49, No. 6, pp. 1091–1010, 2008.

[23] H. Teicher, Rapidly growing random walks and an associated stopping time, Ann. Probab. 7, pp. 1078–

1081, 1979.

[24] S. Watanabe, A limit theorem for sums of i.i.d. random variables with slowly varying tail probability,
Multivariate Analysis, V, Proc. Fifth Internat. Sympos., Pittsburgh, PA, 1978, pp. 249–261, North-
Holland, Amsterdam, 1980.

[25] J. A. Williamson, Random walks and Riesz kernels, Paciﬁc J. Math. 25, pp. 393-415, 1968.

Department of Mathematics, KAP 108, University of Southern California, Los Angeles,

CA 90089-2532 USA

LPMA, Universit´e Pierre et Marie Curie, Campus Jussieu, case 188, 4 place Jussieu, 75252

Paris Cedex 5, France

E-mail address: quentin.berger@upmc.fr

