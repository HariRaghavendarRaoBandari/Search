6
1
0
2

 
r
a

 

M
6
1

 
 
]

G
L
.
s
c
[
 
 

1
v
1
9
1
5
0

.

3
0
6
1
:
v
i
X
r
a

Distributed Inexact Damped Newton Method: Data Partitioning and

Load-Balancing

Chenxin Ma
Industrial and Systems Engineering, Lehigh University, USA
Martin Tak´aˇc
Industrial and Systems Engineering, Lehigh University, USA

CHM514@LEHIGH.EDU

TAKAC.MT@GMAIL.COM

Abstract

In this paper we study inexact dumped Newton
method implemented in a distributed environ-
ment. We start with an original DiSCO algorithm
[Communication-Efﬁcient Distributed Optimiza-
tion of Self-Concordant Empirical Loss, Yuchen
Zhang and Lin Xiao, 2015]. We will show that
this algorithm may not scale well and propose
an algorithmic modiﬁcations which will lead to
less communications, better load-balancing and
more efﬁcient computation. We perform numer-
ical experiments with an regularized empirical
loss minimization instance described by a 273GB
dataset.

1. Introduction
As the size of the datasets becomes larger and larger, dis-
tributed optimization methods for machine learning have
become increasingly important (Bertsekas & Tsitsiklis,
1989; Dekel et al., 2012; Shamir & Srebro, 2014). Exist-
ing mehods often require a large amount of communication
between computing nodes (Yang, 2013; Jaggi et al., 2014;
Ma et al., 2015b; Yang et al., 2013), which is typically sev-
eral magnitudes slower than reading data from their own
memory (Marecek et al., 2014). Thus, distributed machine
learning suffers from the communication bottleneck on real
world applications.
In this paper we focus on the regularized empirical risk
minimization problem. Suppose we have n data samples
{xi, yi}n
i=1, where each xi ∈ Rd (i.e. we have d fea-
tures), yi ∈ R. We will denote by X the data matrix, i.e.
X := [x1, ..., xn] ∈ Rd×n. The optimization problem is to
minimize the regularized empirical loss

f (w) :=

1
n

φi(w, xi) +

(cid:107)w(cid:107)2
2,

λ
2

(P)

n(cid:88)

i=1

where the ﬁrst part is the data ﬁtting term, φ : Rd×Rd → R
is L-smooth loss function which typically depends on yi. 1
The second part of objective function (P) is (cid:96)2 regularizer
(λ > 0) which helps to prevent over-ﬁtting of the data.
There has been an enormous interest in large-scale ma-
chine learning problems and many parallel (Bradley et al.,
2011; Recht et al., 2011) or distributed algorithms have
been proposed (Agarwal & Duchi, 2011; Tak´aˇc et al., 2015;
Richt´arik & Tak´aˇc, 2013; Shamir et al., 2013; Lee & Roth,
2015).
From algorithmic point of view some researches try to
minimize (P) directly including SGD (Shalev-Shwartz
et al., 2011), SVRG and S2GD (Johnson & Zhang, 2013;
Nitanda, 2014; Koneˇcn´y et al., 2014) and SAG/SAGA
(Schmidt et al., 2013; Defazio et al., 2014; Roux et al.,
2012). On the other side, one very popular approach is
to solve its dual formulation (Hsieh et al., 2008) which has
been successfully done in multicore or distributed settings
(Tak´aˇc et al., 2013; Jaggi et al., 2014; Ma et al., 2015b;
Tak´aˇc et al., 2015; Qu et al., 2015; Csiba et al., 2015; Zhang
& Xiao, 2015). The dual problem of (P) has following
form:

max
α∈Rn

D(α) := − 1
n

i (−αi) − λ
φ∗
2

(cid:107) 1
λn

Xα(cid:107)2,

(D)

n(cid:88)

i=1

φ∗
i is a convex conjugate function of φi.

The Challenge In Distributed Computing. We can
identify few challenges when we deal with high-
performance distributed environment.

1. Load-Balancing. Assume that we have m computa-
tional nodes available use. In order to have an algo-
rithm, which is scalable, the algorithm should make
each node equally busy. Amdahl’s law (Rodgers,
1985) implies that if the parallel/distributed algorithm
runs e.g. 75% of the time only on one of the nodes
(usually the master node), then the possible speed-
0.75+0.25/m and
up of the algorithm is bounded by
1Function φ is L-smooth, if ∇φ(·) is L-Lipschitz continuous.

1

Distributed Inexact Damped Newton Method: Data Partitioning and Work-Balancing

is shown on Figure 1 and is asymptotically bounded
by 4/3 ≈ 1.333.
Hence, any algorithm which is targeted for a very
large scale problems has to be design in such a way,
that the sequential portion of the algorithm is negligi-
ble.

puting nodes, researchers use various standard technique
from optimization. Few of them based their algorithms
on ADMM type methods (Boyd et al., 2011; Deng & Yin,
2012), another used block-coordinate type algorithms (Lee
& Roth, 2015; Yang, 2013; Jaggi et al., 2014; Ma et al.,
2015b), where they solved on each node some local sub-
problems which together formed an upper-bound on the
optimization problem. The balancing of computation and
communication was achieved by varying the accuracy of
the solutions of the local sub-problems, which turns out to
be more efﬁcient than some earlier approaches (Tak´aˇc et al.,
2013; 2015)).
Let us now give an overview of existing approaches to solve
problem (P) in distributed setting:

Figure 1. Maximal possible speed-up of an Algorithm which runs
75% in sequential mode.

2. Communication efﬁciency. As it was stressed in the
introduction, in a distributed setting the communica-
tion between nodes should be avoided or minimized
(if possible). Another challenge is to balance the
time the nodes are going some computation and the
time they spent in the communication (usually in MPI
calls).

In this paper we modify the design of promising DiSCO al-
gorithm (Zhang & Xiao, 2015). We completely redesign
the algorithm (partitioning of the data, preconditioning,
communication patterns) to get a new algorithm which

1. has very good scaling (the serial computation is al-

most negligible),

2. balances work-load equally between nodes (all the
nodes are working all the time, no real job for mas-
ter node),

3. has much smaller amount of data set over the network

than in the original DiSCO algorithm.

1.1. Related Work

As stressed in the previous section, one of the main bot-
tlenecks in distributed computing is communication. This
challenge was handled by many researchers differently. In
the ideal case, one would like to never communicate or
maybe communicate only once at the end to form a result.
However, such a procedure which could communicate only
max once and would be able to give arbitrary good solu-
tions (when no node can have access to all the data) is more
fantasy than reality.
Hence, to somehow ”synchronize” work on different com-

1. SGD-based Algorithms.

For the empirical

loss
minimization problems of interest here, stochastic
subgradient descent (SGD) based methods are well-
established. Several distributed variants of SGD have
been proposed, many of which build on the idea of a
parameter server (Niu et al., 2011; Liu et al., 2014;
Duchi et al., 2013). The downside of this approach,
even when carefully implemented, is that the amount
of required communication is equal to the amount of
data read locally (e.g., mini-batch SGD with a batch
size of 1 per worker). These variants are in practice
not competitive with the Newton-type methods con-
sidered here, which allow more local updates per com-
munication round.

2. ADMM. An alternative approach to distributed op-
timization is to use the alternating direction method
of multipliers (ADMM), as used for distributed SVM
training in, e.g., (Forero et al., 2010). This uses
a penalty parameter balancing between the equality
constraint and the optimization objective (Boyd et al.,
2011). However, the known convergence rates for
ADMM are weaker than the more problem-tailored
methods mentioned previously, and the choice of the
penalty parameter is often unclear.

3. DANE Algorithm.

The DANE (Distributed Approximate Newton) algo-
rithm (Shamir et al., 2013) is applicable to solve prob-
lems (P) with any smooth loss functions, which gen-
really requires two rounds of communication in each
iteration. In k-th iteration, the ﬁrst round of commu-
nication is used to compute the gradient f(cid:48)(wk) by a
ReduceAll operation on a Rd vector. Then each ma-
chine solves the local problem
i (wk) − η∇f (wk))T w
{fi(w)−(∇f(cid:48)
(cid:107)w − wk(cid:107)2
2},
µ
(1)
2

wi = arg min

+

w

and takes the second round of communication to com-
pute wk+1 by another ReduceAll operation. The pa-

10010110210311.051.11.151.21.251.31.351.4Maximal Possible Speed−upNumber of Nodes −− mDistributed Inexact Damped Newton Method: Data Partitioning and Work-Balancing

rameter µ ≥ 0 plays the role of damping. The iter-
ation complexity of DANE is O( 1
 ), if the
loss function in (P) is quadratic loss. However, current
analysis does not guarantee that DANE has the same
convergence rate on non-quadratic functions.

λ )2 log 1

n ( L

4. CoCoA+ Algorithm. The CoCoA+ (Communication
Efﬁcient Primal-Dual Coordinate Ascent Framework)
(Jaggi et al., 2014; Ma et al., 2015a;a) allows each ma-
chine to solve the subproblem which can be regarded
as a variant of dual problem of (P).
It allows addi-
tive combination of local updates to the global param-
eters at each iteration, and requires only one round of
communication per iteration. Also, the trade-off be-
tween communication and local computation can be
controled freely based on the problem and the system
hardware. CoCoA+ uses only ﬁrst order information,
and the primal-dual convergence rate has been proven
for both smooth and general convex (L-Lipschitz) loss
functions (Ma et al., 2015b). The iteration complexity
for smooth loss functions is O( L

5. DiSCO Algorithm. The DiSCO algorithm is a New-
ton type method, where in each iteration the step is
solved inexactly using Preconditioned Conjugate Gra-
dient (PCG) method. It, however, requires the step to
be computed to some good accuracy. Number of iter-
ations of PCG depends on many factors, including the
data partitioning, the quality of pre-conditioner and
of course, the requested desired accuracy. Number of
PCG steps also changes from iteration to iteration as
the local geometry of the problem changes. Compared
with other methods discussed above, few parameters
need to be tuned for optimizing the performance in
DiSCO.

λn log 1

 ).

1.2. Contributions

In this section we summarize the main contributions of this
paper (not in order of signiﬁcance).

1. Preconditioning is solved in closed form and efﬁ-
ciently. The PCG methods need to solve the the pre-
conditioned system of linear equations.
In general,
solving the linear system exactly is very expensive due
to the dimension of the problem, especially when d
is large even infeasible. Therefore, in DiSCO algo-
rithm (Zhang & Xiao, 2015) authors suggested to use
an iterative method to solve it. They suggested to use
SAG/SAGA algorithm which has a linear rate of con-
vergence. However, the SAG/SAGA algorithm is run
only on master node, while all other workers are being
idle, and unfortunately, the time to solve the precon-
ditioned system is not at all negligible. In our experi-
ments, we observed that for same dataset the percent-
age of time spent in solving PCG was more then 50%

which implies poor scaling of the DiSCO algorithm.

To overcome that issue, we propose a new precondi-
tioning matrix P , which can be viewed as an approx-
imated or stochastic Hessian. By exploring the struc-
ture of the new preconditioning matrix P , the linear
system can be solved much more efﬁcient (actually,
exactly) by Woodbury Formula. Because, the matrix
P is constructed only based on τ (cid:28) n samples, the
time needed to solve the preconditioning system is
negligible. By applying this approach, we proposed
a variant of DiSCO algorithm called DiSCO-S. Our
practical experiments in Section 5 not only conﬁrms
that this preconditioning is superior to the precondi-
tioning suggested in original DiSCO algorithm, but
also demonstrate that a very small τ would give a good
performance.

2. Data Partitioned by Features. In our setting we as-
sume that the dataset is large enough that it cannot
be stored entirely on any single node and hence the
dataset has to be partitioned.

Both DiSCO and DiSCO-S algorithms are based on
partitioning dataset by samples. By considering an-
other way of making partitions, i.e., partitioning by
features, we proposed a new DiSCO-F algorithm. In
this new setting, the number of communications is
reduced by half compared with the original DiSCO.
Moreover, the computation in each machine is more
balanced, such that the computing resources can be
better utilized. Compared with making partitions by
samples, we do not need to pick a machine as the mas-
ter node, which will do more computation than others.
In the DiSCO-F algorithm, all machines will do ex-
actly the same work and the computation will be dis-
tributed more properly, hence it can possible obtain
almost linear speed-up.

2. Assumptions
We assume that the loss function φi is convex and self-
concordant (Zhang & Xiao, 2015):
Assumption 1. For all i ∈ [n] := {1, 2, . . . , n} the convex
function φ is self-concordant with parameter M i.e.
the
following inequality holds:

|uT (f(cid:48)(cid:48)(cid:48)(w)[u])u| ≤ M (uT f(cid:48)(cid:48)(w)u)

(2)
for any u ∈ Rd and w ∈ dom(f ), where f(cid:48)(cid:48)(cid:48)(w)[u] :=
limt→0

t (f(cid:48)(cid:48)(w + tu) − f(cid:48)(cid:48)(w)).

3
2

1

Table 1 lists some examples of loss functions which satisfy
the Assumption 1 with corresponding constant M.

Distributed Inexact Damped Newton Method: Data Partitioning and Work-Balancing

Table 1. Loss functions satisfying Assumption 1 and the parame-
ter M.

quadratic loss

squared hinge loss

logistic loss

φi(w, xi)
(yi − wT xi)2

(max{0, yi − wT xi})2
log(1 + exp(−yiwT xi))

M
0
0
1

Also, we assume that the function f is both L-smooth and
λ-strongly convex.
Assumption 2. The function f : Rd → R is trice contin-
uously differentiable, and there exist constants L ≥ λ > 0
such that

λI (cid:22) f(cid:48)(cid:48)(w) (cid:22) LI,

∀w ∈ Rd,

(3)

where f(cid:48)(cid:48)(w) denotes the Hessian of f at w, and I is the
d × d identity matrix.

3. Algorithm
We assume that we have m machines (computing nodes)
available which can communicate between each other over
the network. We assume that the space needed to store the
data matrix X exceeds the memory of every single node.
Thus we have to split the data (matrix X) over the m nodes.
The natural question is: How to split the data into m parts?
There are many possible ways, but two obvious ones:

1. split the data matrix X by rows (i.e. create m blocks
by rows); Because rows of X corresponds to features,
we will denote the algorithm which is using this type
of partitioning as DiSCO-F;

2. split the data matrix X by columns; Let us note that
columns of X corresponds to samples we will denote
the algorithm which is using this type of partitioning
as DiSCO-S;

Algorithm 1 High-level DiSCO algorithm
1: Input: parameters ρ, µ ≥ 0, number of iterations K
2: Initializing w0.
3: for k = 0,1,2,...,K do
4:

Option 1: Given wk, run DiSCO-S PCG Algorithm
2, get vk and δk
Option 2: Given wk, run DiSCO-F PCG Algorithm
3, get vk and δk
Update wk+1 = wk − 1

vk

5:

1+δk

6:
7: end for
8: Output: wK+1

Newton step vk such that (cid:107)f(cid:48)(cid:48)(wk)vk − ∇f(cid:48)(wk)(cid:107)2 ≤ k,
which is an approximate solution to the Newton system
f(cid:48)(cid:48)(wk)vk = ∇f (wk). The discussion about how to
choose k and K and a convergence guarantees for Algo-
rithm 1 can be found in (Zhang & Xiao, 2015). And the
main convergence result still applies here: If Algorithm 2
or 3 is run starting with w0 then after

(f (w0) − f (w∗) + log(1/))(cid:112)1 + 2µ/λ

T ∼ O(cid:16)

(cid:17)

communication rounds (iterations) the algorithm will pro-
duce a solution ˆw satisfying f ( ˆw) − f (w∗) < .
The main goal of this work is to analyze the algorith-
mic modiﬁcations to DiSCO-S when the partitioning type
is changed.
It will turn out that partitioning on features
(DiSCO-F) can lead to an algorithm which uses less com-
munications (depending on the relations between d and n)
(see Section 5).

nj(cid:88)

DiSCO-S Algorithm.
If the dataset is partitioned by
samples, such that j–th node will only store Xj =
[xj,1, ..., xj,nj ] ∈ Rd×nj , which is a part of X, then each
machine can evaluate a local empirical loss function

1
nj

(cid:107)w(cid:107)2
2.

λ
2

(4)

i=1

fj(w) :=

φ(w, xj,i) +

Because {Xj} is a partition of X we have(cid:80)m
(cid:80)m
j=1 nj = n,
our goal now becomes to minimize the function f (w) =
h=1 fj(w). Let H denote the Hessian f(cid:48)(cid:48)(wk). For
1
m
simplicity in this section we present it only for square loss
(and hence in this case f(cid:48)(cid:48)(wk) is constant – independent
on wk), however, it naturally extends to any smooth loss.
In Algorithm 2, each machine will use its local data to com-
pute the local gradient and local Hessian and then aggregate
them together. We also have to choose one machine as the
master, which computes all the vector operations of PCG
loops (Preconditioned Conjugate Gradient), i.e., step 5-9
in Algorithm 2.
The preconditioning matrix for PCG is deﬁned only on
master node and consists of the local Hessian approximated
by a subset of data available on master node with size τ, i.e.

τ(cid:88)

j=1

P =

1
τ

(cid:48)(cid:48)

φ

(w, x1,j) + µI,

(5)

where µ is a small regularization parameter. Algorithm 2
presents the distributed PCG mathod for solving the linear
system

Hvk = ∇f (wk).

(6)

Notice that the DiSCO-S is exactly the same as DiSCO pro-
posed and analyzed in (Zhang & Xiao, 2015). In each it-
eration of Algorithm 1, wee need to compute an inexact

Notice that in Algorithm 2, there is another linear system

s = P −1r

(7)

Distributed Inexact Damped Newton Method: Data Partitioning and Work-Balancing

√
Table 2. Communication efﬁciency of several distributed algorithms when the regularization parameter λ ∼ 1/
n (common seen in
machine learning), we can simplify and compare the complexity for the three algorithms. CoCoA+ use more round of communication to
reach  accuracy since it is a ﬁrst order method. DANE and DiSCO are Newton-type methods, which tend to use less communications.

Algorithm

Number of Communication

Quadratic Loss

Logistic Loss

DANE
CoCoA+

m log(1/)
n log(1/)

(mn)1/2 log(1/)

n log(1/)

DiSCO m1/4 log(1/) m3/4d1/4 + m1/4d1/4 log(1/)

communication (Broadcast wk ∈ Rd and reduceAll ∇fi(wk) ∈ Rd )

communication (Broadcast ut ∈ Rd and reduceAll f(cid:48)(cid:48)

i(wk)ut ∈ Rd )

Algorithm 2 Distributed DiSCO-S: PCG algorithm – data partitioned by samples
1: Input: wk ∈ Rd, and µ ≥ 0.
2: Initialization: Let P be computed as (5). v0 = 0, s0 = P −1r0, r0 = ∇f (wk), u0 = s0.
3: for t = 0, 1, 2, ... do
4:
5:
6:
7:
8:
9:
10:
11: end for
12: Return: vk = vt+1, δk =

Compute Hut
Compute αt =
Update v(t+1) = vt + αtut, Hv(t+1) = Hvt + αtHut, rt+1 = rt − αtHut.
Update P s(t+1) = r(t+1).
Compute βt =
(cid:104)rt,st(cid:105)
Update u(t+1) = s(t+1) + βtut.
until: (cid:107)r(r+1)(cid:107)2 ≤ k

(cid:104)rt,st(cid:105)
(cid:104)ut,Hut(cid:105)

(cid:104)r(t+1),s(t+1)(cid:105)

(cid:113)

vT
(t+1)Hvt + αtvT

(t+1)Hut

5:

0 , r[j]

0 = 0, s[j]

0 = (P −1)[j]r[j]

k ∈ Rdj for j = 1, 2, ..., m, and µ ≥ 0.

Algorithm 3 Distributed DiSCO-F: PCG algorithm – data partitioned by features
1: Input: w[j]
2: Initialization: Let P be computed as (5). v[j]
3: while (cid:107)rr+1(cid:107)2 ≤ k do
(cid:80)m
Compute (Hut)[j].
4:
(cid:80)m
j=1(cid:104)r[j]
Compute αt =
j=1(cid:104)u[j]
t+1 = v[j]
t + αtu[j]
Update v[j]
(cid:80)m
t+1 = r[j]
Update P [j]s[j]
t+1.
(cid:80)m
t+1(cid:105)
j=1(cid:104)r[j]
t+1,s[j]
Compute βt =
t (cid:105) .
j=1(cid:104)r[j]
t ,s[j]
(cid:113)
t+1 = s[j]
t+1 + βtu[j]
Update u[j]
t .

t , (Hvt+1)[j] = (Hvt)[j] + αt(Hut)[j], r[j]

t (cid:105)
t ,s[j]
t ,(Hut)[j](cid:105).

6:
7:

8:

9:
10: end while
v[j]
11: Compute δ[j]
t+1
12: Integration: vk = [v[1]
13: Return: vk, δk

k =

T

(Hvt)[j] + αtv[j]
t+1

t+1, ..., v[m]

t+1], δk = [δ[1]

T

(Hut)[j].
t+1, ..., δ[m]
t+1]

k ), u[j]

0 = s[j]
0 .

0 = f(cid:48)(w[j]
communication (ReduceAll an Rn vector)
communication (ReduceAll a number)
t − αt(Hut)[j].

t+1 = r[j]

communication (ReduceAll a number)

communication (Reduce an Rdj vector)

to be solved, which has the same dimension as (6). How-
ever, becasue we only apply a subset of data to compute the
preconditioning matrix P , (7) can be solved by Woodbury
formula (Press et al., 2007), which will be described detail
in Section 4.

Rdj×n, which contains all the samples, but only with a
subset of features. Also, each machine will only store
k ∈ Rdj and thus only be responsible for the compu-
w[j]
tation and updates of Rdj vectors. By doing so, we only
need one ReduceAll on a vector of length n, in addition to
two ReduceAll on scalars number.

DiSCO-F Algorithm.
tures, then j-th machine will store Xj = [a[j]

If the dataset is partitioned by fea-
n ] ∈

1 , ..., a[j]

Distributed Inexact Damped Newton Method: Data Partitioning and Work-Balancing

Table 4. Comparison of communication between different algo-
rithms.

DiSCO-S
2 × Rd

DiSCO-F

1 × Rn, 2 × R 2 × Rd

DANE CoCoA+
1 × Rd

4. Woodbury Formula for solving P s = r
In each iteration of Algorithms 2 and 3, we need to solve a
linear system in the form of P s = r, where P ∈ Rd×d in
Algorithm 2 and P ∈ Rdi×di for i = 1, 2, ..., m in Algo-
rithm 3, which is usually very expensive. To solve it more
efﬁciently, we can apply Woodbury Formula (Press et al.,
2007).
Notice that if we use P deﬁned in (5), P can be considered
as τ rank-1 updates on a diagnal matrix. For example, if
φ(·) is Quadtratic Loss, then

τ(cid:88)

i=1

P = D +

1
τ

xixT
i .

(8)

τ(cid:88)

i=1

If φ(·) is Logistic Loss, then

P = D +

1
τ

exp(−wT

k xi)

(exp(−wT

k xi) + 1)2

xixT
i .

(9)

In both cases, D is the diagnal matrix with Dii = λ + µ for
i = 1, ..., m. Then we can follow the procedure to get the
solution s.

Algorithm 4 Woodbury Formula to solve P s = r
1: Compute zi = 1
λ+µ xi for i = 1, ..., τ
2: Let Z = [z1, ..., zτ ], X = [x1, ..., xτ ]
3: Compute y = 1
λ+µ r
4: Solve the linear system (I + X T Z)v = X T y
5: Return: s = y − Xv

Notes that v ∈ Rτ and τ (cid:28) d (in our experiments, τ = 100
usually works very well), step 4 can be done efﬁciently by
any solver. In Section 5.3, we compare the effect of setting
different values for τ.

5. Numerical Experiments
We present experiments on several
large real-world
datasets distributed across multiple machines, running on
an Amazon EC2 cluster with 4 instances. We show that
DiSCO-F with a small τ converges to the optimal solution
faster in terms of total rounds of communications compared
to original DiSCO, DANE and CoCoA+ in most cases.
Also, for the dataset with d > n, DiSCO-F will also domi-
nate others in elapsed time. In Section 5.3, we compare the
affects of preconditioning matrices with different values for

Figure 2. Flow diagrams of few iterations of DiSCO-S (top) and
DiSCO-F (bottom). DiSCO-F uses less time for one iteration,
due to the more efﬁcient and balanced computation. Green boxes
represent the processes are busy, while red boxes represent idle
nodes. Yellow boxes show the status of communicating between
all processes. Double arrows stand for ReduceAll operations. The
thin red arrows represent a communication of few scalars only.
Comparison of Communication and Computational
Cost.
In Table 3 we compare the communication cost for
the two approaches DiSCO-S/DiSCO-F. As it is obvious
from the table, DiSCO-F requires only one reduceAll of
a vector of length n, whereas the DiSCO-S needs one re-
duceAll of a vector of length d and one broadcast of vector
of size d. So roughly speaking, when n < d then DiSCO-
F will need less communication. However, very interest-
ingly, the advantage of DiSCO-F is the fact that it uses
CPU on every node more effectively. It also requires less
total amount of work to be performed on each node, lead-
ing to more balanced and efﬁcient utilization of nodes (see
Figure 2 for illustration how . DiSCO-F utilizes resources
more efﬁciently and Table 4 for the size of communication
required in each PCG step).

Table 3. Comparison of computation between different algo-
rithms. For DiSCO-S, there exists large differences of compu-
tation between the master node and other node. But for DiSCO-F,
each node will do exactly the same computation as each other.
1(Rd1×d1 · Rd1 )

DiSCO-S
1(Rd×d · Rd)

DiSCO-F

master

nodes

Operation
y = M x
M x = y

x + y
xT y

y = M x
M x = y

x + y
xT y

1 (Rd)
4 (Rd)
4 (Rd)

0
0
0

1 (Rd1)
4 (Rd1)
4 (Rd1)

1 (Rdi)
4 (Rdi)
4 (Rdi)

1 (Rd×d · Rd)

1(Rd1×di · Rdi )

Process 0Process 3Process 2Process 1Iteration 1Iteration 2Process 0Process 3Process 2Process 1Iteration 1Iteration 2Iteration 3Distributed Inexact Damped Newton Method: Data Partitioning and Work-Balancing

Table 5. Datasets used for numerical experiments.

n

677,399
19,996
4,627,840

d
47,236
1,355,191
11,725,480

size(GB)
1.21
0.13
273.4

Dataset
rcv1.test
news20

splice-site.test

τ, and investigate that a small τ (around 100) would re-
sult in a impressive performance. In Section 5.4, we show
that extra speed-up can be gained in some cases, by trying
to shrink the number of samples that are used to compute
Hessian.

5.1. Implementation Details

We implement DiSCO and all other algorithms for com-
parison in C++, and run them in Amazon cluster using four
m3.large EC2 instances. We apply all methods on applying
Quadratic loss and Logistic loss in (P). A summary of the
datasets used is shown in Table 5.

5.2. Comparison of different algorithms

We compare the DiSCO-S, DiSCO-F, DiSCO, DANE and
CoCOA+ directly using two datasets (new20 and rcv1.test)
across two loss functions, where λ is ﬁxed to be 1e − 4.
In DiSCO-S and DiSCO-F, we set τ = 100.
In DiSCO
and DANE, we apply Stochastic Average Gradient(SAG)
(Schmidt et al., 2013) to solve linear system P s = r and
subproblem (1), respectively. Also, µ was set as 1e − 2 for
both of them. In CoCoA+, SDCA was used as the solver
for subproblems.
In Figure 3, we plot how the norm of the gradient of objec-
tive decreases with respective to the total number of com-
munication and the elapsed time. In all the cases, DiSCO-F
uses only half of the rounds of communications compared
with DiSCO-S. Also, DiSCO-S often uses similar rounds of
communications with the original DiSCO, which demon-
strates the advantage of using preconditioning matrix based
on only a small subset of the samples. Finally, DANE and
CoCoA+ will decrease the norm of gradient very fast at
the ﬁrst few iterations, but the decreasing become much
weaker as the iterations continue.
For the news20 (d (cid:29) n) and splice-site.test (d ∼ n)
dataset, the DiSCO-F converges to the optimal solution
with fewer iterations than all the other methods. The
elapsed time for DiSCO-F is only 10% of DiSCO-S in the
news20 case, due to the smaller size of the vector that needs
to be communicated.
However, for the rcv.test dataset (d < n), even though
DiSCO-F uses less number of communications, it tends
to take longer time to reach an expected tolerance than
DiSCO-S and CoCOA+. This is because the longer vec-
tors (Rn) that DiSCO-F needs to communicate in each CG

Figure 4. Comparison of different number of samples used in Pre-
conditioning by running DiSCO-F.

iteration, compared with them in DiSCO-S and CoCOA+
(Rd).

5.3. Impact of the Parameter τ

In this section, we compare the performance of DiSCO-
F algorithm of setting different τ.
If we apply methods
described in Section 4, the parameter τ would determine
how well the preconditioning matrix P can approximate
the true Hessian H. In a extreme case, if we only use one
machine and τ = n, then (cid:107)P − H(cid:107)2 = 0 and each iteration
of Algorithm 1 will only use 1 iteration of CG algorithm.
However, too large τ will cause computation in Algorithm
4 quite expensive, thus resulting in long elapsed time. In
our experiment, τ = 500 is even not acceptable, in terms
of elapsed time.
As shown in Figure 4, the larger τ we use, the less total
number of communications the algorithm takes to reach op-
timality. However, τ = 100 always leads to shorter time in
both of these two datasets.

5.4. How Many Samples to Compute Hessian?

Notice that in step 4 of Algorithm 2 and 3, the product of
the Hessian matrix and a vector need to be computed. In
this section, We would like to try reducing the size of sam-
ples to compute Hessian. By doing so, we have to give
up the current guaranteed complexity, since now the Hes-
sian will be approximated. However, less elapsed time is
expected if we choose proper size of samples. Due to the
lack of theoretical analysis on this attempt, we only list the
observation from the experiments.

02040608010−1010−810−610−410−2100news, Quadratic lossNumber of CommunicationsNorm of Gradient  05105010050000.511.522.510−1010−810−610−410−2100news, Quadratic lossWall Clock Time (s)Norm of Gradient  05105010050005010015020025030035010−1010−810−610−410−2100rcvtest, Quadratic lossNumber of CommunicationsNorm of Gradient  051050100500051015202510−1010−810−610−410−2100rcvtest, Quadratic lossWall Clock Time (s)Norm of Gradient  051050100500Distributed Inexact Damped Newton Method: Data Partitioning and Work-Balancing

Figure 3. Norm of gradient vs. the round of communication, as well as norm of gradient vs. elapsed time in seconds for three datasets:
news20 (top, λ = 1e − 3), rcv1.test (middle, λ = 1e − 4), splice-site.test (bottom, λ = 1e − 6) , each for two loss functions, Quadratic
loss (left) and Logistic loss (right).

For each iteration of Algorithm 1, we choose a subset of
samples uniformly randomly to get the approximated Hes-
sian. We try to choose subsets of samples whose sizes
range from 100% to 6.25% of the entire dataset, as shown
in Figure 5. For news20 dataset, such attempt will bring
no beneﬁts. The more samples we use, the less round of
communications and elapsed time the algorithm spends to
reach optimality. But for rcv1.test dataset, the elapsed time
decreases as we reduce the number of samples to compute
Hessian, which illustrates that using only a small portion of
samples will be helpful to get enough information of Hes-
sian in each iteration.
The reason of this result might be for the dataset with a
rather large number of features (news20), ignoring some
samples will result in lots of relationship between features
missed. For the dataset with rather small number of fea-
tures (rcv1), the Hessian can be approximated well by only
a small subset of data.

Figure 5. Comparison of different number of samples used in ap-
proximating Hessian by running DiSCO-F.

05010015010−1010−810−610−410−2100news, Quadratic lossNumber of CommunicationsNorm of Gradient  DiSCO−SDiSCO−FDiSCODANECoCoA+05101510−1010−810−610−410−2100news, Quadratic lossWall Clock Time (s)Norm of Gradient  DiSCO−SDiSCO−FDiSCODANECoCoA+05010015010−1010−810−610−410−2100news, Logistic lossNumber of CommunicationsNorm of Gradient  DiSCO−SDiSCO−FDiSCODANECoCoA+0246810121410−1010−810−610−410−2100news, Logistic lossWall Clock Time (s)Norm of Gradient  DiSCO−SDiSCO−FDiSCODANECoCoA+05010015010−1010−810−610−410−2100rcvtest, Quadratic lossNumber of CommunicationsNorm of Gradient  DiSCO−SDiSCO−FDiSCODANECoCoA+05101510−1010−810−610−410−2100rcvtest, Quadratic lossWall Clock Time (s)Norm of Gradient  DiSCO−SDiSCO−FDiSCODANECoCoA+05010015010−1010−810−610−410−2100rcvtest, Logistic lossNumber of CommunicationsNorm of Gradient  DiSCO−SDiSCO−FDiSCODANECoCoA+05101510−1010−810−610−410−2100rcvtest, Logistic lossWall Clock Time (s)Norm of Gradient  DiSCO−SDiSCO−FDiSCODANECoCoA+05010015020025010−1010−810−610−410−2100splice, Quadratic lossNumber of CommunicationsNorm of Gradient  DiSCO−SDiSCO−FDiSCODANECoCoA+0123456x 10410−1010−810−610−410−2100splice, Quadratic lossWall Clock Time (s)Norm of Gradient  DiSCO−SDiSCO−FDiSCODANECoCoA+02040608010010−1010−810−610−410−2100splice, Logistic lossNumber of CommunicationsNorm of Gradient  DiSCO−SDiSCO−FDiSCODANECoCoA+0200040006000800010−1010−810−610−410−2100splice, Logistic lossWall Clock Time (s)Norm of Gradient  DiSCO−SDiSCO−FDiSCODANECoCoA+010020030040050010−1010−810−610−410−2100news, Quadratic lossNumber of CommunicationsNorm of Gradient  100%50%25%12.5%6.25%0123456710−1010−810−610−410−2100news, Quadratic lossWall Clock Time (s)Norm of Gradient  100%50%25%12.5%6.25%05010015020025030035010−1010−810−610−410−2100rcvtest, Quadratic lossNumber of CommunicationsNorm of Gradient  100%50%25%12.5%6.25%012345610−1010−810−610−410−2100rcvtest, Quadratic lossWall Clock Time (s)Norm of Gradient  100%50%25%12.5%6.25%Distributed Inexact Damped Newton Method: Data Partitioning and Work-Balancing

6. Conclusion
In conclusion, based on the DiSCO algorithm (Zhang &
Xiao, 2015), we study inexact dumped Newton method im-
plemented in a distributed way. We found that by partition-
ing the dataset by features, the number of communications
can be reduced and the computation in each machine be-
comes more balanced. Also, we shrink the size of samples
to generate the preconditioning matrix, which greatly im-
proves the efﬁciency of solving the linear system in CG.
Our experimental results show signiﬁcant speedups over
previous methods, including the original DiSCO algorithm
as well as other state-of-the-art methods.

References
Agarwal, Alekh and Duchi, John C. Distributed delayed
stochastic optimization. In Advances in Neural Informa-
tion Processing Systems, pp. 873–881, 2011.

Bertsekas, Dimitri P and Tsitsiklis, John N. Parallel and
distributed computation: numerical methods. Prentice-
Hall, Inc., 1989.

Boyd, Stephen, Parikh, Neal, Chu, Eric, Peleato, Borja, and
Eckstein, Jonathan. Distributed optimization and statisti-
cal learning via the alternating direction method of mul-
tipliers. Foundations and Trends R(cid:13) in Machine Learn-
ing, 3(1):1–122, 2011.

Bradley,

Joseph K, Kyrola, Aapo, Bickson, Danny,
Parallel coordinate descent
arXiv preprint

and Guestrin, Carlos.
for l1-regularized loss minimization.
arXiv:1105.5379, 2011.

Csiba, Dominik, Qu, Zheng, and Richt´arik, Peter. Stochas-
tic dual coordinate ascent with adaptive probabilities.
arXiv preprint arXiv:1502.08053, 2015.

Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon.
Saga: A fast incremental gradient method with support
In Ad-
for non-strongly convex composite objectives.
vances in Neural Information Processing Systems, pp.
1646–1654, 2014.

Dekel, Ofer, Gilad-Bachrach, Ran, Shamir, Ohad, and
Xiao, Lin. Optimal distributed online prediction using
mini-batches. The Journal of Machine Learning Re-
search, 13(1):165–202, 2012.

Deng, Wei and Yin, Wotao. On the global and linear con-
vergence of the generalized alternating direction method
of multipliers. Journal of Scientiﬁc Computing, pp. 1–
28, 2012.

Forero, Pedro A, Cano, Alfonso, and Giannakis, Geor-
gios B. Consensus-based distributed support vector ma-
chines. The Journal of Machine Learning Research, 11:
1663–1707, 2010.

Hsieh, Cho-Jui, Chang, Kai-Wei, Lin, Chih-Jen, Keerthi,
S Sathiya, and Sundararajan, Sellamanickam. A dual
coordinate descent method for large-scale linear svm. In
Proceedings of the 25th international conference on Ma-
chine learning, pp. 408–415. ACM, 2008.

Jaggi, Martin, Smith, Virginia, Tak´ac, Martin, Terhorst,
Jonathan, Krishnan, Sanjay, Hofmann, Thomas, and Jor-
dan, Michael I. Communication-efﬁcient distributed
dual coordinate ascent. In Advances in Neural Informa-
tion Processing Systems, pp. 3068–3076, 2014.

Johnson, Rie and Zhang, Tong. Accelerating stochas-
tic gradient descent using predictive variance reduction.
NIPS, pp. 315–323, 2013.

Koneˇcn´y, Jakub, Liu, Jie, Richt´arik, Peter, and Tak´aˇc,
Martin. mS2GD: Mini-batch semi-stochastic gradi-
arXiv preprint
ent descent in the proximal setting.
arXiv:1410.4744, 2014.

Lee, Ching-Pei and Roth, Dan. Distributed box-constrained
ICML,

quadratic optimization for dual linear SVM.
2015.

Liu, Ji, Wright, Stephen J, R´e, Christopher, Bittorf, Vic-
tor, and Sridhar, Srikrishna. An Asynchronous Paral-
lel Stochastic Coordinate Descent Algorithm. In ICML,
2014.

Ma, Chenxin, Koneˇcn`y, Jakub, Jaggi, Martin, Smith, Vir-
ginia, Jordan, Michael I, Richt´arik, Peter, and Tak´aˇc,
Martin. Distributed optimization with arbitrary local
solvers. arXiv preprint arXiv:1512.04039, 2015a.

Ma, Chenxin, Smith, Virginia, Jaggi, Martin, Jordan,
Michael I, Richt´arik, Peter, and Tak´aˇc, Martin. Adding
vs. averaging in distributed primal-dual optimization. In
ICML 2015 - Proceedings of the 32th International Con-
ference on Machine Learning, volume 37, pp. 1973–
1982. JMLR, 2015b.

Marecek, Jakub, Richt´arik, Peter, and Tak´ac, Martin. Dis-
tributed block coordinate descent for minimizing par-
tially separable functions. Numerical Analysis and Op-
timization 2014, Springer Proceedings in Mathematics
and Statistics, 2014.

Duchi, John C, Jordan, Michael I, and McMahan, H Bren-
dan. Estimation, Optimization, and Parallelism when
Data is Sparse. In NIPS, 2013.

Nitanda, Atsushi. Stochastic proximal gradient descent
with acceleration techniques. In Advances in Neural In-
formation Processing Systems, pp. 1574–1582, 2014.

Distributed Inexact Damped Newton Method: Data Partitioning and Work-Balancing

Yang, Tianbao. Trading computation for communication:
In Ad-
Distributed stochastic dual coordinate ascent.
vances in Neural Information Processing Systems, pp.
629–637, 2013.

Yang, Tianbao, Zhu, Shenghuo, Jin, Rong, and Lin, Yuan-
qing. Analysis of distributed stochastic dual coordinate
ascent. arXiv preprint arXiv:1312.1031, 2013.

Zhang, Yuchen and Xiao, Lin. Communication-efﬁcient
distributed optimization of self-concordant empirical
loss. arXiv preprint arXiv:1501.00263, 2015.

Niu, Feng, Recht, Benjamin, R´e, Christopher, and Wright,
Stephen J. Hogwild!: A Lock-Free Approach to Paral-
lelizing Stochastic Gradient Descent. In NIPS, 2011.

Press, William H, Teukolsky, Saul A, Vetterling, William T,
and Flannery, Brian P. Numerical recipes: the art of sci-
entiﬁc computing. 2007.

Qu, Zheng, Richt´arik, Peter, and Zhang, Tong. Quartz:
Randomized dual coordinate ascent with arbitrary sam-
In Advances in Neural Information Processing
pling.
Systems, pp. 865–873, 2015.

Recht, Benjamin, Re, Christopher, Wright, Stephen, and
Niu, Feng. Hogwild: A lock-free approach to paralleliz-
ing stochastic gradient descent. In Advances in Neural
Information Processing Systems, pp. 693–701, 2011.

Richt´arik, Peter and Tak´aˇc, Martin. Distributed coordinate
descent method for learning with big data. arXiv preprint
arXiv:1310.2059, 2013.

Rodgers, David P. Improvements in multiprocessor system
design. In ACM SIGARCH Computer Architecture News,
volume 13, pp. 225–231. IEEE Computer Society Press,
1985.

Roux, Nicolas L, Schmidt, Mark, and Bach, Francis R. A
stochastic gradient method with an exponential conver-
gence rate for ﬁnite training sets. In Advances in Neural
Information Processing Systems, pp. 2663–2671, 2012.

Schmidt, Mark, Roux, Nicolas Le, and Bach, Francis. Min-
imizing ﬁnite sums with the stochastic average gradient.
arXiv:1309.2388, 2013.

Shalev-Shwartz, Shai, Singer, Yoram, Srebro, Nathan, and
Cotter, Andrew. Pegasos: Primal estimated sub-gradient
solver for svm. Mathematical programming, 127(1):3–
30, 2011.

Shamir, Ohad and Srebro, Nathan. Distributed stochastic
optimization and learning. In Communication, Control,
and Computing (Allerton), 2014 52nd Annual Allerton
Conference on, pp. 850–857. IEEE, 2014.

Shamir, Ohad, Srebro, Nathan, and Zhang, Tong. Com-
munication efﬁcient distributed optimization using an
arXiv preprint
approximate newton-type method.
arXiv:1312.7853, 2013.

Tak´aˇc, Martin, Bijral, Avleen, Richt´arik, Peter, and Srebro,
Nathan. Mini-batch primal and dual methods for SVMs.
ICML, 2013.

Tak´aˇc, Martin, Richt´arik, Peter, and Srebro, Nathan.
arXiv preprint

Distributed mini-batch SDCA.
arXiv:1507.08322, 2015.

