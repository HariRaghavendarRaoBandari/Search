6
1
0
2

 
r
a

 

M
9
1

 
 
]

V
C
.
s
c
[
 
 

1
v
3
9
0
6
0

.

3
0
6
1
:
v
i
X
r
a

Large scale near-duplicate image retrieval using

Triples of Adjacent Ranked Features (TARF) with

embedded geometric information

Sergei Fedorov∗1 and Olga Kacher†1

1ABBYY, Moscow, Russia

March 22, 2016

Abstract

Most approaches to large-scale image retrieval are based on the
construction of the inverted index of local image descriptors or visual
words. A search in such an index usually results in a large number
of candidates. This list of candidates is then re-ranked with the help
of a geometric veriﬁcation, using a RANSAC algorithm, for example.
In this paper we propose a feature representation, which is built as a
combination of three local descriptors. It allows one to signiﬁcantly de-
crease the number of false matches and to shorten the list of candidates
after the initial search in the inverted index. This combination of local
descriptors is both reproducible and highly discriminative, and thus
can be eﬃciently used for large-scale near-duplicate image retrieval.

1

Introduction

Most systems for large-scale image retrieval are designed for a scenario in
which a query image is searched for within a large image set (hundreds of
thousands or millions of images), and the goal is to ﬁnd its partial duplicates.
To solve this task most state-of-the-art algorithms make use of the Bag
of Words (BoW) representation of an image, where each local descriptor
extracted from an image is represented with a visual word, and implement
an eﬀective search in the inverted index (see for example [1]). One of the
problems of all such algorithms is the large list of initial candidates, which
needs to be further pruned with geometrical checks, usually with a RANSAC
algorithm. For large image sets the candidate list can be very long, which

∗
†

sfedorov@abbyy.com
okacher@abbyy.com

1

makes it infeasible to perform geometric veriﬁcation for each candidate. In
order to increase the discriminative power of the features extracted from an
image, and to decrease the size of the initial candidates list, diﬀerent ways
of embedding spatial information into the features were suggested. Among
those are bundling features [2], bundle min-hashing [3], nested SIFT [4],
geometry-preserving visual phrases [5], and weak geometrical consistency [6].
These approaches allow one to signiﬁcantly increase the eﬀectiveness of the
duplicates search in a large image set.

For the closely related task of logo recognition there have also been
proposed methods for adding geometric information to the inverted index.
In [7] triples of SIFT features were used for logo detection, and in [8] authors
proposed multi-scale Delaunay triangulation for grouping local features into
triples. In both papers [7, 8] a set of training images with the same logo is
required for the construction of a logo model, a key point by which these
approaches diﬀer from ours, in which a single image is used for extracting
triples of local features and indexing.

In this paper we consider the task of ﬁnding duplicates between two
large image sets, both with hundreds of thousands or millions of images.
One possible approach is to build a search index using the ﬁrst image set,
and then perform the search within it for each image from the second image
set using one of the methods discussed above. This approach requires a
large number of search queries, and is absolutely infeasible when the search
index does not ﬁt in the main memory. The problem here is the large list of
initial candidates. For example, nested SIFT would ﬁrst generate the list of
all images with matched bounding features, and only then this list will be
pruned by the requirement that there should be similar member features.
Finding duplicates between two large image sets is in fact equivalent to
considering all possible pairs of images, thus the initial list of candidates
would be vast.

To address this problem we propose a composite local feature, which
we have named TARF. A TARF descriptor is a combination of three local
descriptors, and because of this it is highly discriminative, thus it allows us
to signiﬁcantly decrease the number of false matches and to shorten the list
of candidates after the initial search in an inverted index. In order to in-
crease the reproducibility of TARFs we propose a method of their extraction
which takes into account the scores of constituent feature points, described
in detail in the next section. Adding geometric information to this feature
is straightforward and further increases its discriminative power, allowing
us to achieve a very low false positives rate. This makes it possible to use
TARF descriptors for the task of ﬁnding duplicates between two image sets
even for very large image sets that don’t ﬁt in the main memory.

2

2 TARF features

In this paper we propose a new composite local feature, Triple of Adjacent
Ranked Features (TARF). These triples of feature points are used to con-
struct complex descriptors, which include three local descriptors, as well as
the geometric layout of the three points. It is essential that these complex
descriptors should be a) highly distinctive, and b) reproducible. To achieve
these aims, we propose the following scheme for extracting TARFs from an
image.

2.1 Extracting TARFs

Triples contain heterogeneous feature points. One feature point is detected
with a blob detector, such as SIFT [9] or SURF [10]. Two other points are
found with a corner detector, such as the BRISK detector [11]. We will
denote these points as BLOB and CORNER. A group of three points gives
a richer description of the local image area, as compared to a single feature
point, and includes very distinctive geometric features. We ﬁrst extract
BLOB feature points from an image, and then each BLOB feature point is
used to select nearby CORNER feature points. Enumerating all such triples
would lead to a large number of combinations, most of which would not be
very reproducible, because even if one of the three points is missing on a
duplicate image, the whole triple will be lost. So, it is essential to select only
those triples that have a good chance of being reproduced on a duplicate.
To achieve this, we make use of the score of CORNER points (given by the
CORNER points detector), modiﬁed to take into account position of the
CORNER points relative to a BLOB feature point.

More precisely, the modiﬁed score is calculated as

(cid:20)

 S∗ = exp

−0.5

d0 = 0.5Rb,
R0 = 0.33Rb,

σd = 0.15Rb

σR = 0.15Rb

(cid:17)2(cid:21)

(cid:16) d−d0

σd

exp

(cid:20)
−0.5

(cid:16) Rc−R0

σR

(cid:17)2(cid:21)

S

,

(1)

where Rc and Rb are CORNER and BLOB feature points radii correspond-
ingly, d – distance between centers of CORNER and BLOB, S and S∗ are
original and modiﬁed CORNER scores. Parameters d0, σd, R0 and σR were
determined experimentally; the values in (1) correspond to TARFs based
on SIFT/BRISK. These parameters play an important role for the quality
of the TARF extractor. All CORNER points which lie in the vicinity of a
BLOB feature point are sorted by modiﬁed score S∗, and Ntop CORNER
feature points are taken. The value Ntop = 7 was found to give best results.
Next a global score threshold S∗
0 is determined in such a way that taking all
CORNER points with modiﬁed score S∗ < S∗

0 gives a total of N0 triples:

3

C1(cid:48)

O

C2

C1

C(cid:48)(cid:48)

C2(cid:48)

B

Figure 1: TARF descriptor. The descriptor is composed of BLOB point B
with the center at O and radius Rb and two CORNER points, C1 and C2.
Segments C1C(cid:48)
2 are related to the directions of CORNER feature
points. C(cid:48)(cid:48) is the middle point of the line segment C1C2.

1 and C2C(cid:48)

(cid:88)

i∈all BLOB points

N0 =

ni(ni − 1)/2,

(2)

where ni is the number of CORNER points in the vicinity of i-th BLOB,
0 . N0 is a parameter of the

with modiﬁed score below threshold, S∗ < S∗
detector, usually N0 = 2500..3000

The exact algorithm for extracting TARF features:

Algorithm 1 Extracting TARFs

1: Extract BLOB feature points with BLOB detector
2: Extract CORNER feature points with low threshold (for BRISK detec-

tor threshold 5 was used)
3: for each BLOB point do
4:

Choose all CORNER points from the neighbourhood of BLOB fea-

ture point

5:

Re-rank CORNER points according to (1)
Choose top n∗ CORNER points (n∗ = 7)

6:
7: end for
8: Find global threshold for the scores of CORNER points. For each BLOB
select ni top CORNER feature points in its neighbourhood with the
score above global threshold, ni ≤ n∗

9: Add all combinations (BLOB point + 2 diﬀerent CORNER points from

the list, determined in step 8) to the list of extracted TARFs

4

Figure 2: Matched TARF features on a partial duplicate images with 30
degrees rotation

2.2 Matching TARFs

Matching TARF features is straightforward: descriptors of all three con-
stituent points should be matched, and the geometrical layout of three fea-
ture points should be similar. Geometrical features describing the TARF
layout may be chosen in diﬀerent ways. Our choice of geometrical features
is given below (Rb is the radius of the BLOB feature point):
1. Angle between vectors OC1 and OC2, α = (cid:54) (C1OC2)
1, β1 = (cid:54) (OC1C(cid:48)
2. Angle between vectors OC1 and C1C(cid:48)
1)
2, β2 = (cid:54) (OC2C(cid:48)
3. Angle between vectors OC2 and C2C(cid:48)
2)
4. Ratio 1 = OC1/OC2

5. Ratio 2 = C1C2/Rb
6. Ratio 3 = OC(cid:48)(cid:48)/Rb

Fig. 1 shows the geometric layout of feature points in TARF. An example
of partial duplicates with matched TARF descriptors is given in Fig. 2.

2.3 Matching TARFs in the inverted index

Two TARFs are fully matched if all the three constituent descriptors are
matched and their geometric layout is similar. Geometric layout is checked
by verifying that all the geometric features listed above diﬀer by less than
a threshold:



|α(1) − α(2)| < t1,
k | < t1,
k − β(2)
|β(1)
|(1)
k − (2)
k | < t,

k = 1, 2
k = 1, 2, 3

Thresholds were determined experimentally, t1 = 20◦, t = 0.1.

5

(3)

2.3.1 Matching descriptors with visual words

For large scale image retrieval it’s essential to be able to match three con-
stituent descriptors of TARFs simultaneously. The descriptors are repre-
sented in the inverted index with visual words. Three visual words, as-
signed to three descriptors, are packed into one 32-bit integer. Constituent
descriptors of TARFs are matched if these 32-bit integers are exactly equal.
Moreover, each visual word has an inverse document frequency (IDF) score
associated with it. Several visual words with the lowest score (i.e. belong-
ing to the largest clusters of descriptors) are placed into the stop list, and
TARFs with such constituent descriptors are ignored.

One of the problems is that such a representation with visual words
makes the probability of matching two similar descriptors signiﬁcantly lower,
as compared to matching with e.g. LSH (locality sensitive hashing), even for
small vocabulary sizes. To illustrate this, Table 1 shows experimental results
for probabilities of matching descriptors by various methods. For a set of
500 images we generated transformed duplicates; transformations included
rotations and downscale. Feature points were extracted from original images
and from duplicates. Those feature points that are at the same position
and scale (up to image transformation) are considered to be reproduced.
The reproducibility rate R is the percentage of feature points that were
extracted from the original image and were reproduced on the transformed
image. Depending on the image transformations this rate is R = 30..40% for
both SIFT and BRISK detectors. Next, pairs of descriptors were collected
for the reproduced feature points, taking one descriptor from the original
image, and another from the transformed image. These pairs of descriptors
were matched using various methods, and the percentage of matched pairs
is given in the TPR column. Thus, the overall probability of ﬁnding a match
for a particular feature point from the original image on a transformed image
is given by p = R · TPR. The probability of false matches is given in the
FPR column, i.e FPR is the match rate for pairs of descriptors calculated
at random uncorrelated points of images.

As seen from Table 1, matching with visual words results in a TPR
that is lower compared to matching by threshold, even for small vocabulary
sizes. Though usually this TPR is suﬃcient, sometimes it may be desirable
to increase the recall. This can be achieved by training two diﬀerent vo-
cabularies for each type of descriptor, and representing each descriptor with
two visual words (diﬀerent vocabularies are trained using clusterization of
descriptors extracted from diﬀerent sets of images). In this case descriptors
are matched if any of the two visual words is matched. This makes TPR
signiﬁcantly higher and comparable with matching by a threshold, though
FPR is of course much higher (see Table 1).
If this approach with several vocabularies of visual words is taken, then
each TARF descriptor will be represented by 2 × 2 × 2 = 8 32-bit integers.

6

matching method

threshold, L2 distance 0.4
visual words, 128 words
visual words, 256 words
visual words, 512 words

visual words, 2 groups of 128 words
visual words, 2 groups of 256 words
visual words, 2 groups of 512 words

SIFT

matching method

threshold, hamming distance 90

visual words, 128 words
visual words, 256 words
visual words, 512 words

visual words, 2 groups of 128 words
visual words, 2 groups of 256 words
visual words, 2 groups of 512 words

BRISK

FPR
3 · 10−4
8.5 · 10−3
4.3 · 10−3
2.2 · 10−3
1.3 · 10−2
6.6 · 10−3
3.4 · 10−3

FPR
7 · 10−4
10−2
5.2 · 10−3
2.8 · 10−3
1.9 · 10−2
9.6 · 10−3
5.1 · 10−3

TPR
0.75
0.65
0.61
0.57
0.76
0.73
0.7

TPR
0.57
0.43
0.38
0.35
0.6
0.54
0.5

Table 1: Descriptor match rates.

This requires 8 tables in the inverted index. Descriptors from two TARFs
are considered to be matched if any of the eight 32-bit integers matches
exactly. This can be viewed as a modiﬁcation of LSH, with diﬀerent hashes
being generated by diﬀerent vocabularies of visual words. Other TARF
representations may also be considered: a) a BLOB descriptor is represented
with 1 visual word, and a CORNER descriptor is represented with 2 visual
words (1 × 2× 2) or b) 2 visual words for BLOB, and 1 visual word for
CORNER, 2 × 1 × 1.

Equation (4) illustrates how a TARF descriptor can be represented with
visual words when using two vocabularies for each type of constituent de-
scriptor.

 BLOB point descriptor → (wb

1st CORNER point descriptor → (wc1
2nd CORNER point descriptor → (wc2
TARF → (wb
({wb
1 , wc2

2 ) × (wc2
2 }, ...,{wb

1, wc1

1, wb
1 },{wb

2) × (wc1
1, wc1

1 , wc1
1 , wc2

1, wb
2)

(4)

1 , wc1
2 )
1 , wc2
2 )

1 , wc2
2, wc1

2 ) =
2 , wc2

2 })

7

matching method

threshold, geometric features discarded

threshold

visual words, 1x1x1, no stop list

visual words, 1x1x1, stop lists of size 10
visual words, 2x1x1, stop lists of size 10
visual words, 1x2x2, stop lists of size 10
visual words, 2x2x2, stop lists of size 10

FPR
2.5 · 10−5
4 · 10−8
4 · 10−9
1.5 · 10−9
2.5 · 10−9
4 · 10−9
5 · 10−9

TPR
0.7
0.68
0.28
0.25
0.27
0.41
0.44

Table 2: TARF match rates. First row corresponds to matching without
checks of geometric layout. Other rows correspond to fully matched TARFs.

2.4 TARFs reproducibility and match rates

We have tested TARF features on the same set of 500 images with trans-
formed duplicates as described in the previous section. TARFs that were
extracted in the same places, i.e. in which all three constituent feature points
were found at the same positions and scale (up to transformations), were
considered to be reproduced. The reproducibility of TARFs is RT = 8%.
Though this is a low number, it is still much higher than the reproducibility
of randomly chosen triples of feature points, and this is due to the proposed
scheme of TARFs extraction.

Table 2 shows the FPR and TPR for TARFs. The probability of false
matches is given in the FPR column. TPR is deﬁned as the probability that
a pair of reproduced TARF descriptors is matched.

Normally TARFs are matched fully, including checks of geometric lay-
out (3). In order to evaluate the importance of geometric features, we have
tested TARFs matching without performing these checks, i.e. discarding
geometric features and matching only constituent decriptors. Results are
presented in the ﬁrst row of the table. It can be seen that discarding geo-
metric features leads to an FPR that is several orders of magnitude higher
than the FPR for full TARF matching, yet it is still quite low at 2.5 · 10−5.
Including geometric features makes TARFs highly discriminative, with the
FPR as low as 10−9..10−8. Using stop lists for the visual words further re-
duces FPR, making it several times lower, and almost does not aﬀect TPR.
The TPR depends on the matching method; it has the largest value of 0.68 if
the constituent descriptors are matched by threshold. In the case of match-
ing constituent descriptors with visual words it is lower and varies from 0.25
to 0.44; the TPR is higher when several dictionaries of visual words are used.

3 Experimental results

The search of near-duplicate images is performed as follows. First, a search
index is built. It is an inverted index of three visual words packed into 32-bit

8

number of matched TARFs

probability

1x1x1 scheme
no stop lists

1x1x1 scheme

stop lists of size 10

0.985
0.013
10−3
3 · 10−4

0.995
0.004
10−4
10−5

0
1
2
3

Table 3: Probability of a given number of matched TARFs on a pair of
non-duplicate images

integers. Geometric data is stored in the index as auxiliary data. Three IDF
scores of constituent visual words are summed up to give the IDF score of
the TARF.

After ﬁnding all the images that have at least one TARF matching a
query image, the score of the candidate images is calculated as the sum of
IDF scores of all matched TARFs. A precision-recall curve can be obtained
by varying the threshold for this score. Matched images can be ﬁltered
by applying a geometric model. We use a standard RANSAC algorithm.
RANSAC converges very fast, because of the extremely low number of out-
liers, so we make only 10 iterations of RANSAC.

An important property of TARFs is a very low false positive rate. Table 3
shows the probability of ﬁnding a given number of matched TARF features
It can be seen that the probability
for a pair of non-duplicate images.
decreases fast as the the number of false positive matches increases.
It
should be noted, that the use of stop lists for visual words is essential and
reduces the number of false candidate images by the order of magnitude. If
one builds a search index with 100K images using the 1x1x1 scheme with
stop lists of size 10, and then searches for a query image in this index, the
result of such a search will have on average as little as 400 false candidate
images with one matched TARF descriptor, 10 false candidates with two
matched TARFs and 1 false candidate with three matched TARFs. So,
checking all the candidates with more sophisticated algorithms, e.g. with
geometric veriﬁcation by a RANSAC algorithm, is feasible even for very
large search indices.

In order to evaluate image retrieval based on TARFs we have constructed
an image set of 1853 images, and four sets of near-duplicates of these im-
ages, with the following modiﬁcations: a) downscale to small size 30k pixels
(150x200), b) rotation 30 degrees, c) crop 70% (30 % of an image retained)
and d) strong gaussian blur with σ = 4. Original images were added to the
search index and diluted with the 100K distracting images, taken from the
Flickr 100k Dataset [12]. For each of these modiﬁcations we have evaluated
the precision-recall curve, and have measured AP (average precision), de-

9

l
l
a
c
e
r

1

0.8

0.6

0.4

0.2

0

with RANSAC

without RANSAC

0

0.2

0.6
0.4
precision

0.8

1

Figure 3:
The eﬀect of geometric veriﬁcation with RANSAC on the
precision-recall curve. Query images are duplicates downscaled to 30k pixels;
original images are diluted with 100K distracting images.

ﬁned as the area under the precision-recall curve. Figure 3 illustrates how
geometric veriﬁcation with RANSAC aﬀects the precision-recall curve.

We have varied diﬀerent parameters of the image retrieval system, and
measured the resulting performance. The default parameters were: BLOB
– SIFT descriptors, 1 visual word (dictionary with 256 words, stop list of
size 10); CORNER – BRISK descriptors, 1 visual word (dictionary with 128
words, stop list of size 10); 3000 TARF descriptors per image; RANSAC
veriﬁcation, 10 iterations; 100K distracting images in the search index.

1. Number of distracting images

Fig. 4 shows the dependence of AP on the number of distracting images
in the search index.
It is a key feature of TARF that the number
of false positive candidates is very low, so even without geometric
veriﬁcation by RANSAC the precision remains high for a large number
of distracting images.

2. Number of dictionaries of visual words

Fig. 5 shows the dependence of AP on the number of dictionaries of
visual words. 2x2x2 – two dictionaries for BLOB descriptors, and two
dictionaries for CORNER descriptors, 8 tables in the inverted index;
1x2x2 – one dictionary for BLOB descriptors, and two dictionaries for
CORNER descriptors, 4 tables in the inverted index; 2x1x1 – 2 tables
in the inverted index; 1x1x1 – 1 table in the inverted index.

10

P
A

1

0.98

0.96

0.94

0.92

0.9

2K

10K

index size

100K

downscale 30k
rotation 30 deg

crop 70%

blur with σ = 4.0

downscale 30k RANSAC
rotation 30 deg RANSAC

crop 70% RANSAC

blur with σ = 4.0 RANSAC

Figure 4: Average precision as a function of the size of the index (i.e.
number of distracting images).

Best results are achieved for the 2x2x2 conﬁguration, and the 1x2x2
conﬁguration is nearly as good, thus one visual word is suﬃcient for
BLOB point representation. However, using two visual words to rep-
resent each CORNER point is useful and increases AP.

3. Types of descriptors used

Fig. 6 illustrates the dependence of AP on the type of detectors/descriptors
used to build TARF. For BLOB points we have tested SIFT, SURF,
SURF detector + BRISK descriptor, and AKAZE [13]. For COR-
NER points we have always used BRISK. With the only exception of
AKAZE feature points, which appear to give low recall for crop 70%,
all other conﬁgurations give similar results.

4. Number of TARF descriptors per image

Fig. 7 shows the dependence of AP on the number of TARF descriptors
extracted from each image; the number of descriptors varies from 1000
to 4000. This graph justiﬁes the choice of 3000 as a default number of
TARF descriptors extracted from an image.

Table 4 shows experimental results for AP on popular Holidays and
Copydays datasets [6], with 100K distracting images. The AP for Hol-
idays and Copydays strong is low due of the low recall. Low recall
on these datasets can be explained by the fact that TARF descriptors are
matched only if the part of the image is reproduced almost exactly, up to
scale/rotation. So, TARF descriptors are very sensitive to the viewpoint
change, geometrical distortions of an image, etc. Holidays and Copydays

11

1

0.98

P
A

0.96

0.94

downscale 30k
rotation 30 deg

crop 70%

blur with σ = 4.0

2x2x2

1x2x2

2x1x1

1x1x1

Number of visual words

Figure 5: Average precision as a function of the number of visual words.

1

0.9

P
A

0.8

0.7

0.6

downscale 30k
rotation 30 deg

crop 70%

blur with σ = 4.0

SIFT

SURF SURF/BRISKAKAZE

BLOB points detector

Figure 6: Average precision vs types of detectors and descriptors of BLOB
feature points.

12

1

0.95

0.9

P
A

0.85

0.8

downscale 30k
rotation 30 deg

crop 70%

blur with σ = 4.0

1,000

2,000

3,000

4,000

Number of TARFs per image

Figure 7: Average precision as a function of the number of TARFs extracted
from each image.

image set
Holidays

Copydays, strong
Copydays, crop 30
Copydays, crop 70
Copydays, crop 80
Copydays, jpeg 50
Copydays, jpeg 30
Copydays, jpeg 10

average precision (AP)

0.278
0.466
1.0
0.916
0.697
1.0
0.999
0.98

Table 4: Average precision for commonly used datasets

strong image sets include many duplicates which don’t have suﬃciently
large undistorted areas.

4 Conclusion

In this paper we have proposed a novel composite image feature – TARF,
which is a combination of three local features. This feature is highly dis-
criminative, because 1) TARF incorporates three local descriptors, thus the
probability of a false positive match of a TARF is of the order of a third
power of the probability of a false positive match for a single descriptor,
which is a very low probability. In other words, it is very improbable that
a random combination of three descriptors would match a given TARF. 2)
TARF embeds the geometric relationships between three constituent local

13

features.

A method to eﬀectively build an inverted index of TARF features is
proposed. Each combination of three descriptors is represented by one 32-bit
integer, and geometric information is added to the search index as auxiliary
data.

It is shown that this approach makes it possible to build a system for
large scale near-duplicates search, with high recall and a low false positives
rate.

References

[1] J. Sivic and A. Zisserman. Video google: a text retrieval approach
to object matching in videos. In Computer Vision, 2003. Proceedings.
Ninth IEEE International Conference on, pages 1470–1477 vol.2, Oct
2003.

[2] Zhong Wu, Qifa Ke, M. Isard, and Jian Sun. Bundling features for
large scale partial-duplicate web image search. In Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages
25–32, June 2009.

[3] Stefan Romberg and Rainer Lienhart. Bundle min-hashing for logo
recognition. In Proceedings of the 3rd ACM Conference on International
Conference on Multimedia Retrieval, ICMR ’13, pages 113–120, New
York, NY, USA, 2013. ACM.

[4] Pengfei Xu, Lei Zhang, Kuiyuan Yang, and Hongxun Yao. Nested-sift
for eﬃcient image matching and retrieval. MultiMedia, IEEE, 20(3):34–
46, July 2013.

[5] Yimeng Zhang, Zhaoyin Jia, and Tsuhan Chen. Image retrieval with
geometry-preserving visual phrases. In Computer Vision and Pattern
Recognition (CVPR), 2011 IEEE Conference on, pages 809–816, June
2011.

[6] Herv´e J´egou, Matthijs Douze, and Cordelia Schmid. Hamming em-
bedding and weak geometric consistency for large scale image search.
In Andrew Zisserman David Forsyth, Philip Torr, editor, European
Conference on Computer Vision, volume I of LNCS, pages 304–317.
Springer, oct 2008.

[7] Stefan Romberg. Aggregating Local Features into Bundles for High-

Precision Object Retrieval. PhD thesis, 2014.

14

[8] Yannis Kalantidis, Lluis Garcia Pueyo, Michele Trevisiol, Roelof van
Zwol, and Yannis Avrithis. Scalable triangulation-based logo recogni-
tion. In Proceedings of the 1st ACM International Conference on Mul-
timedia Retrieval, ICMR ’11, pages 20:1–20:7, New York, NY, USA,
2011. ACM.

[9] David G. Lowe. Distinctive image features from scale-invariant key-

points. International Journal of Computer Vision, 60:91–110, 2004.

[10] Herbert Bay, Andreas Ess, Tinne Tuytelaars, and Luc Van Gool.
Speeded-up robust features (surf). Computer Vision and Image Un-
derstanding, 110(3):346 – 359, 2008. Similarity Matching in Computer
Vision and Multimedia.

[11] S. Leutenegger, M. Chli, and R.Y. Siegwart. Brisk: Binary robust
invariant scalable keypoints. In Computer Vision (ICCV), 2011 IEEE
International Conference on, pages 2548–2555, Nov 2011.

[12] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman. Object re-
trieval with large vocabularies and fast spatial matching. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition,
2007.

[13] P. F. Alcantarilla, J. Nuevo, and A. Bartoli. Fast explicit diﬀusion
for accelerated features in nonlinear scale spaces. In British Machine
Vision Conf. (BMVC), 2013.

15

