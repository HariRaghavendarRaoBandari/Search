6
1
0
2

 
r
a

 

M
4
1

 
 
]

Y
S
.
s
c
[
 
 

1
v
4
1
3
4
0

.

3
0
6
1
:
v
i
X
r
a

Gradient approximation and extremum seeking

via needle variations ∗

Simon Michalowsky and Christian Ebenbauer ∗∗

March 15, 2016

Nevertheless, this class of dithers could also be of interest in
certain applications, e.g. when it is only possible to apply inputs
in a short period of time.
Our contributions are as follows: For the case of needle-shaped
dither signals we show that the extremum seeking system ap-
proximates a weighted averaged gradient descent. Secondly, we
use that a large class of dither signals is well approximated by
a summation of multiple needle-shaped signals and we show
how extremum seeking systems with such dither signals can be
analyzed with the presented theory as well. We establish new
results for ﬁnite period length that are in the limit in accordance
with existing results. We further propose a new gradient-based
optimization algorithm that is indeed motivated by the results
on gradient approximation via needle variations but can also be
regarded separately. The algorithm can be seen as a combina-
tion of a continuous-time version of Nesterov’s method and the
heavy ball method and is of interest on its own.
Our analysis relies on well-known results established in the con-
text of the Pontryagin Maximum Principle where the effects
of needle-shaped variations of the optimal inputs are studied.
Since the Maximum Principle is well-established in a very broad
setup we hope that our ideas can be used for much more general
problems. However, our results should be seen as a ﬁrst step and
in the present paper we only consider basic cases.
The structure of the present paper is as following: In Section 2
we give a brief introduction to needle variations and the vari-
ational equations. In Section 3 we present our main results.
We ﬁrst consider the case of two needles with opposite sign,
then we show how this can be generalized to a superposition
of many needles and third we present the new continuous-time
algorithm. In Section 4 we illustrate our approximation formulas
for the particular case of a quadratic objective function and give
simulation results for the proposed new algorithm. We conclude
our work in Section 5.

Abstract. We consider a gradient approximation scheme
that is based on applying needle shaped inputs. By using ideas
known from the classic proof of the Pontryagin Maximum Prin-
ciple we derive an approximation that reveals that the considered
system moves along a weighted averaged gradient. Moreover,
based on the same ideas, we give similar results for arbitrary
periodic inputs. We also present a new gradient-based optimiza-
tion algorithm that is motivated by our calculations and that can
be interpreted as a combination of the heavy ball method and
Nesterov’s method.

1 Introduction

Extremum seeking is a well-known technique that has success-
fully been used in several applications (see e.g. [5], [17], [18])
in order to operate a system at an a priori unknown setpoint that
is optimal with respect to some objective function. In a typical
extremum seeking problem this objective function is unknown
such that gradient-based methods do not apply. The majority
of the extremum seeking schemes relies on approximating the
gradient by the excitation of the system with a so called dither
signal. Although a much larger class of periodic signals is ap-
propriate in principle, in most cases sinusoidal dither signals are
used. The effect of the choice of the dither has been studied in
[15] where the authors conclude that dither signals other than
sinusoidal ones could be beneﬁcial.
In the present paper we introduce periodic needle-shaped dither
signals for gradient approximation. In particular we consider
the case where the period length is large compared to the pulse
length. This is in contrast to standard averaging results ([2]) or
Lie bracket averaging results ([7]) where the period length is
also assumed to be small. Our main objective is not to present a
new extremum seeking scheme here but to get insight into the
extremum seeking process using needle-shaped dither signals.
∗ This article is an extended version of [12].
∗∗ S. Michalowsky and C. Ebenbauer are with the Institute for Systems
Theory and Automatic Control, University of Stuttgart, 70550 Stuttgart,
simon.michalowsky@ist.uni-stuttgart.de,
Germany.
ce@ist.uni-stuttgart.de. This work was supported by the German
Research Foundation (DFG, EB 42S/4-1).

1

2 Preliminaries

2.1 Notation
We denote by N the set of natural numbers, by Z the set of
integer numbers and by R the set of real numbers. We denote
by C n the set of n times continuously differentiable functions.
The gradient of a function f : Rp → R, f ∈ C 1 is denoted by
(x)](cid:62). Moreover, we make
∇ f (x) = [ ∂ f
∂ x1
use of the Landau notation: For f ,g : Rn → R we write f (x) =
such that | f (x)| ≤ M|g(x)| for all |x| ≤ δ .

O(cid:0)g(x)(cid:1) meaning that there exists some M > 0 and some δ > 0

∂ f
∂ xp

∂ f
∂ x2

(x)

(x)

...

2.2 Needle variations and the variational

equation

In the following we brieﬂy repeat well-known results on the
effect of so called needle variations in the control inputs to
trajectories of dynamic systems. Our overview follows closely
the lines of [11, Chapter 4] but is adapted to the special needs
for the argumentation of our main results. Consider

(cid:0)x(t)(cid:1)u1(t) + g2

(cid:0)x(t)(cid:1)u2(t)

˙x(t) = g1

(1)
where g1,g2 : R → R and u1,u2 : R → R. Further, suppose
that g1,g2 ∈ C 1 and u1,u2 are piecewise continuous such that
at least local existence and uniqueness of a solution to (1) is
ensured. Let x∗(t) denote the solution of (1) for u1(t) = u∗
1(t)
2(t) ≡ 0. We are interested in how the solution
and u2(t) = u∗
x∗(t) will change when we perturb the input u∗
2 by a so called
needle variation, also known as Pontryagin-McShane variation,
where the perturbed input is deﬁned as

u2(t) =

u∗
2(t)
α

if t /∈ [¯t − ε, ¯t]
if t ∈ [¯t − ε, ¯t]

(2)

(cid:40)

with some constant α ∈ R and ε > 0. Thus, the input is per-
turbed on an interval of length ε by some constant value α,
see Figure 1. In the following we will investigate the effect of
such a perturbation for small ε. Let x(t) denote the solution of
(1) when applying the perturbed input and suppose that u∗
1(t) is
continuous at t = ¯t. By some Taylor expansions one can show
that (see e.g. [11])

x(¯t) = x∗(¯t) + ε(cid:2)g1

= x∗(¯t) + εg2

(cid:0)x∗(¯t)(cid:1)u∗
(cid:0)x∗(¯t)(cid:1)u∗
(cid:0)x∗(¯t)(cid:1)α + O(ε2).

(cid:0)x∗(¯t)(cid:1)α
1(¯t)(cid:3) + O(ε2)

1(¯t) + g2

− g1

(3)

We will now investigate how the perturbed solution x(t) is prop-
agated after time ¯t in comparison to the unperturbed solution
x∗(t). This can be studied using perturbation theory ([9]) where
one is interested in how the solution of a differential equa-
tion evolves when starting from a perturbed initial condition
compared to the solution when starting from a nominal initial

2

condition. Here, x∗(¯t) plays the role of the nominal initial con-
dition whereas the perturbed one is given by (3). The basic idea
is to do a Taylor expansion of the perturbed solution in ε about
ε = 0 which leads to

x(t) = x∗(t) + εv1(t) + O(ε2)

(4)

where v1(t) is some unknown function that can be determined
as following. Since x(t) must fulﬁll the unperturbed differential
equation one can put x(t) as given by (4) into (1) with u1 = u∗
1,
u2 = u∗
2 and compare the terms linear in ε which then gives
([11],[9])

˙v1(t) = ∂ g1
∂ x

(cid:0)x∗(¯t)(cid:1)α. This is known as the variational equa-

with v1(¯t) = g2
tion ([11]) and it is the same as the linearization of (1) with
u1(t) = u∗

2(t) about the trajectory x∗(t).

1(t), u2(t) = u∗

1(t)v1(t)

(5)

(cid:0)x∗(t)(cid:1)u∗

3 Main results

We consider the following nonlinear input-afﬁne system

˙x(t) = F(cid:0)x(t)(cid:1)u1(t) + u2(t)

(6)

with x(0) = x0 ∈ R and where F : R → R, F ∈ C 1.
In ex-
tremum seeking problems the usual approach is to choose the
inputs u1,u2 such that the trajectories of (6) approximately move

along those of the gradient ﬂow ˙¯x(t) = −∇F(cid:0) ¯x(t)(cid:1), ¯x(0) = x0,

such that x converges to a solution of the optimization problem
min F(x) (see e.g. [2] or [7]). Here, we want to investigate
the averaged behavior of (6) for a new class of inputs: We ﬁrst
consider needle-shaped inputs and then give a generalization
thereof. We consider the scalar case here such that, strictly
speaking, we are only treating derivative approximation. How-
ever it should be noted that one can expect similar results for the
multidimensional case.

3.1 Dither signals composed of two needles

(cid:40)

Deﬁne the following T -periodic input sequence

(7)

u1(t) =



for t ∈ [0, T
2 )
for t ∈ [ T
2 ,T )
for t ∈ [0,ε)
for t ∈ [ε, T
2 )
2 , T
2 + ε)
2 + ε,T )

1
−1
α
0
−α for t ∈ [ T
for t ∈ [ T
0
2 and −∞ < α < ∞. The input sequence
where T > 0, 0 < ε < T
is illustrated in Figure 2. The following theorem reveals how
this input sequence affects (6) for ε being small.

u2(t) =

(8)

α

u2(t)

u∗
2(t)

x0

¯t − ε ¯t

x(¯t)
x∗(¯t − ε)

x(t)x(t)

x∗(t)

Figure 1. Illustration of the needle variation as deﬁned by (2) (left) as
well as the optimal and the varied trajectory (right).

)
t
(
1
u

1

0
−1

0

α

0

−α

0ε

)
t
(
2
u

T
2

T
2

T

T

Figure 2. Illustration of the input sequence as deﬁned by (7) and (8).
The upper plot shows u1(t), the lower one shows the needle variation
u2(t).

Theorem 1. Consider (6) together with the input sequence as
deﬁned by (7) and (8). Let x∗(t) denote the solution of (6) when
u1(t) as deﬁned by (7) and u2(t) ≡ 0 and suppose that x∗(t)
exists on [0,T ]. Let Φ(t,t0) denote the state-transition matrix at
time t corresponding to the variational equation

(cid:0)x∗(t)(cid:1)v1(t)

˙v1(t) = ∂ F
∂ x

with initial time t0. Then

x(T ) =

x0 + εαΦ(0,ε)

(cid:90) T

2 −ε

ε

∂ F
∂ x

(cid:0)x∗(τ)(cid:1)Φ(τ, T

2 − ε)dτ + O(ε2).

(9)

(10)

Proof. We use the results presented in Section 2.2 where u2
plays the role of the needle variation. With (3) we have after the
ﬁrst needle, i.e. at t = ε, x(ε) = x∗(ε) + εα + O(ε2). Using (4)
we have at the point where the second needle is applied, i.e. at
t = T
2 ,

2 ) = x∗( T
x( T

2 ) + εv1(t) + O(ε2)

(11)

3

where v1(t) is the solution of (9) with initial condition
v1(ε) = α. Since Φ(t,t0) denotes the state-transition matrix cor-
responding to (9) we have that v1(t) = Φ(t,ε)v1(ε) = Φ(t,ε)α
for t ∈ [ε, T

2 ] such that

2 ) = x∗( T
x( T

2 ,ε)α + O(ε2).

2 ) + εΦ( T
We will now repeat
the same procedure for the second
¯x(t) denote the solution when only apply-
needle.
ing the ﬁrst but not
i.e. we have
¯x(t) = x∗(t) + ε ¯Φ(t,ε)α + O(ε2) where ¯Φ(t,t0) denotes the
state-transition matrix corresponding to

the second needle,

(12)

Let

(cid:0)x∗(t)(cid:1)u1(t)v(t)

˙v(t) = ∂ F
∂ x

(13)

with initial time t0. In comparison to the previous calculations
¯x(t) now plays the role of x∗(t) in (3) such that

x( T

2 + ε) = ¯x( T

2 + ε)− εα + O(ε2).

(14)

Hence, propagating the second needle, we obtain by (4)

x(t) = ¯x(t) + εv2(t) + O(ε2)

= x∗(t) + ε(cid:0) ¯Φ(t,ε)α + v2(t)(cid:1) + O(ε2)

(15)
2 + ε ≤ t ≤ T . Let v(t) = ¯Φ(t,ε)α + v2(t). Notice that v(t)

for T
fulﬁlls (13) such that
x(T ) = x∗(T ) + ε ¯Φ(T, T
= x∗(T ) + ε ¯Φ(T, T

2 + ε)v( T

2 + ε)(cid:0) ¯Φ( T

2 + ε) + O(ε2)

2 + ε,ε)α − α(cid:1) + O(ε2).

(16)

We will now investigate how ¯Φ(t,t0) and Φ(t,t0) are related. Let
Φ2(t,t0) denote the state-transition matrix corresponding to (13)
when u1(t) = −1 and notice that Φ(t,t0) is the state-transition
matrix for the case of u1(t) = 1. Then we have



¯Φ(t,t0) =

Φ(t,t0)
Φ(t, T
Φ2(t, T
Φ2(t,t0)

2 )Φ2( T
2 )Φ( T

if t ∈ [0, T
2 ,t0) if t ∈ [0, T
2 ,t0) if t ∈ [ T
if t ∈ [ T

2 ),t0 ∈ [0, T
2 ]
2 ],t0 ∈ [ T
2 ,T ]
2 ,T ],t0 ∈ [0, T
2 )
2 ,T ],t0 ∈ [ T
2 ,T ]

Now, since (13) is a scalar linear time varying differential equa-

tion, we have Φ(t,t0) = exp((cid:82) t

∂ F
∂ x

t0

(cid:0)x∗(τ)(cid:1)dτ. Similarly
(cid:90) t
(cid:0)x∗(T − τ)(cid:1)dτ

(cid:90) t
Φ2(t,t0)
= exp(−
(cid:90) T−t

t0

= exp(

T−t0

∂ F
∂ x

∂ F
∂ x

(cid:0)x∗(τ)(cid:1)dτ = exp(−
(cid:0)x∗(s)(cid:1)ds = Φ(T −t,T −t0)

∂ F
∂ x

t0

(17)

(18)

where we used that x∗(t) = x∗(T − t) since x∗(t) is just going
back and forth along F(x). Using (17) and (18) in (16) we obtain

2 + ε)(cid:1) + O(ε2)

with x∗(T ) = x0

2 )Φ( T

x(t) = x0 + εα(cid:0)Φ2(T, T
= x0 + εα(cid:0)Φ(0,ε)− Φ(0, T
= x0 + εαΦ(0,ε)(cid:0)1− Φ(ε, T
(cid:90) T
(cid:90) T

= x0 + εαΦ(0,ε)

2 −ε

ε

2 −ε

= x0 + εαΦ(0,ε)

∂ F
∂ x

2 ,ε)− Φ2(T, T

2 − ε)(cid:1) + O(ε2)
2 − ε)(cid:1) + O(ε2)
(cid:0)x∗(τ)(cid:1)Φ(τ, T

2 − ε)dτ + O(ε2)
2 − ε)dτ

∂
∂τ Φ(τ, T

ε

+ O(ε2)

obtain

=

(cid:90) 3ε
(cid:90) 3ε

ε

ε

∂ F
∂ x

(cid:0)x∗(τ)(cid:1)Φ(τ,3ε)dτ
(cid:0)x∗(4ε)(cid:1)Φ(4ε,3ε) + O(τ − 4ε)dτ
(cid:0)x∗(4ε)(cid:1)Φ(4ε,3ε) + O(ε2).

∂ F
∂ x

=2ε ∂ F
∂ x

(23)

Moreover, by a Taylor expansion of Φ(4ε,t0) about t0 = 4ε, it
is

(19)

Φ(4ε,3ε) = Φ(4ε,4ε)− d
dt0

Φ(4ε,t0)|t0=4εε + O(ε2).

(24)

where in the last step we used the differentiation property of the
state-transition matrix, see e.g [8].

Equivalently, we have

Φ(0,ε) = Φ(0,0) + d
dt0

Φ(0,t0)|t0=0ε + O(ε2).

(25)

Remark 1. Equation (10) gives an approximation at t = T .
However, due to the periodicity, it can as well be extended to
arbitrary integer multiples of T which leads to

x(cid:0)(k + 1)T(cid:1) =

x(kT ) + εαΦ(kT,tk1)

(cid:90) tk2

tk1

∂ F
∂ x

(cid:0)x∗(τ)(cid:1)Φ(τ,tk2)dτ + O(ε2)

(20)

where k ∈ N, tk1 := kT + ε, tk2 := kT + T

2 − ε.

Eq. (10) gives insight into the gradient approximation process
and shows that system (6) with inputs (7) and (8) approximately
moves along a weighted average of the gradient of F. Thus,
depending on the sign of α, this system does a modiﬁed gradient
descent or ascent. This is also comparable to what is done
in stochastic approximation ([13]) where one uses the sum of
several gradients.
Notice that the approximation (10) is valid for small ε but T
does not have to be small which is in contrast to existing results.
In the following we consider the case where T is of the same
order of magnitude as ε. In particular, we consider T = 8ε.

Lemma 1. Consider (6) together with the input sequence as
deﬁned by (7) and (8). Let x∗(t) denote the solution of (6) when
u1(t) as deﬁned by (7) and u2(t) ≡ 0. Suppose T = 8ε. Then

(cid:18)

2 )(cid:1) + ∂ F
(cid:0)x∗( T

∂ x

(cid:0)x0

∂ F
∂ x

(cid:1)(cid:19)

x(T ) = x0 + ε2α

+ O(ε3).

(21)

Proof. In case of T = 8ε we have from (10)

x(T ) = x0 + εαΦ(0,ε)

3ε(cid:90)

∂ F
∂ x

(cid:0)x∗(τ)(cid:1)Φ(τ,3ε)dτ + O(ε2).

ε

(22)

Expanding the integrand into a Taylor series about τ = 4ε we

4

Hence we obtain using (23) - (25) in (22)

x(T ) = x0 + 2ε2α ∂ F
∂ x

(26)
2 )+O(ε) but also x∗(4ε) = x0 +O(ε).

By (11) it is x∗(4ε) = x( T
Thus

(cid:0)x∗(4ε)(cid:1) + O(ε3).
(cid:0)x( T
2 )(cid:1) + O(ε3)
(cid:0)x0
(cid:1) + O(ε3)

x(T ) = x0 + 2ε2α ∂ F
∂ x
= x0 + 2ε2α ∂ F
∂ x

(27)
(28)

such that – by adding both expressions – we ﬁnally arrive at
(21).

Remark 2. Lemma 1 is consistent with the well-known fact
that an input-afﬁne system ˙x = g1(x)u1(t) + g2(x)u2(t) with u1,
u2 as deﬁned by (7) and (8) where T = 8ε approximately moves
into the direction of the Lie bracket [g1,g2](x), see e.g. [4] or
[7] in terms of extremum seeking. Here, it is g1(x) = F(x) and
g2(x) = 1 such that we obtain for the Lie bracket
∂ x (x)g2(x) = − ∂ F

∂ x (x)g1(x)− ∂ g1

[g1,g2](x) = ∂ g2

∂ x (x).

(29)

Remark 3. In case of T = 4ε the integral in (10) vanishes such
that there are no ﬁrst order terms. Thus, the ratio between ε and
T is important but nevertheless T does not necessarily have to
be 8ε.

In comparison to common approximation formulas our result
(21) not only includes the gradient of F evaluated at the start
point but also at the point where u1 changes its sign, i.e. at the
two points where x∗ reverses its direction. Usually, this term
disappears by including it in the higher order terms.

3.2 Dither signals composed of inVnitely

many needles

Up to now we have considered the very special input sequence
given by (7) and (8). In the following we consider the same
nonlinear input-afﬁne system (6) with more general inputs u1

)
t
(
u

1

0
−1

0

u1
u2

T

Figure 3. Illustration of the approximation of the input function u1 by
many needles as used in Theorem 2.

and u2. More precisely, we impose the following assumptions
on the input functions u1,u2 : R → R:
A1 The functions u1,u2 are piecewise continuous and bounded.

functions,

A2 The functions u1,u2 are T -periodic and have zero mean.
√
For example, this includes the well-known case of trigono-
√
ω cos(ωt) and
metric input
ω sin(ωt) that was considered e.g. in [10] or more
u2(t) =
recently in [7] in the context of extremum seeking. Loosely
spoken, the idea is to approximate the input u2 by inﬁnitely
many needles to go along the same lines as in the previous case,
see Figure 3.

u1(t) =

i.e.

Theorem 2. Suppose that Assumptions A1 and A2 hold. Con-
sider (6) together with u2(t) deﬁned by

u2(t) = u2(ti+1)

for t ∈ [ti,ti+1),ti+1 ≤ T

N , N ∈ N, i = 0,1, . . . ,N − 1. Let x∗(t)
where ti = εi, ε = T
denote the solution of (6) for u2(t) ≡ 0 and u1(t) as deﬁned
by Assumptions A1 and A2 and suppose that x∗(t) exists on
[0,T ]. Let further Φ(t,t0) denote the state-transition matrix
corresponding to

(cid:0)x∗(t)(cid:1)u1(t)v1(t)

˙v1(t) = ∂ F
∂ x

(30)

with initial time t0. Then, if N is ﬁnite,

(cid:0)x∗(τ)(cid:1)u1(τ)Φ(cid:0)τ,ti+1

(cid:1)dτ + 1(cid:1)u2

(cid:0)ti+1

(31)

(cid:1).

x(T ) = x∗(T ) + O(ε2)

(cid:0) T(cid:90)

ti+1

∂ F
∂ x

+ ε

N−1
∑

i=0

Moreover,

x(T ) = x∗(T ) +

lim
N→∞

O(ε2). Let ¯x(t) denote the solution in case we would not apply
the second needle at t = ε. Then

¯x(2ε) = x∗(2ε) + εv1(2ε) + O(ε2)

= x∗(2ε) + εΦ(2ε,ε)u2(ε) + O(ε2)

(33)

and hence when the second needle is applied we obtain using
again (3)

x(2ε) = ¯x(2ε) + εu2(2ε) + O(ε2)

= x∗(2ε) + εΦ(2ε,ε)u2(ε) + εu2(2ε) + O(ε2).

(34)

Here, the ﬁrst term involving the state-transition matrix is the
propagation of the ﬁrst needle whereas the other term linear in ε
is the second needle. Going on along these lines we obtain

x(3ε) = x∗(3ε) + Φ(3ε,2ε)(cid:0)Φ(2ε,ε)εu2(ε) + εu2(2ε)(cid:1)

+ εu1(3ε) + O(ε2)
= x∗(3ε) + εΦ(3ε,ε)u2(ε) + εΦ(3ε,2ε)u2(2ε)
+ εu2(3ε) + O(ε2).

(35)

Generalizing this we have
k−1
∑

x(kε) = x∗(kε) + ε

i=0

Φ(kε,ti+1)u2(ti+1) + O(ε2)

(36)

where k ∈ N and ti = εi. Thus, we have after one period, i.e. at
T = Nε,

x(T ) = x∗(T ) + ε

N−1
∑

i=0

Φ(T,ti+1)u2(ti+1) + O(ε2).

(37)

We will now take a closer look at the sum. It is

Φ(T,ti+1)u2(ti+1)

(38)

(cid:19)

∂Φ
∂τ (τ,ti+1)dτ + 1

u2(ti+1)

(cid:19)
(cid:0)x∗(τ)(cid:1)u1(τ)Φ(cid:0)τ,ti+1)dτ + 1

∂ F
∂ x

u2(ti+1)

ε

=ε

=ε

N−1
∑
i=0
N−1
∑

i=0

N−1
∑

i=0

(cid:18) T(cid:90)
(cid:18) T(cid:90)

ti+1

ti+1

T(cid:90)

T(cid:90)

0

t

∂ F
∂ x

(cid:0)x∗(τ)(cid:1)u1(τ)Φ(τ,t)u2(t)dτ dt.

(32)

where we used in the last step that the state-transition matrix Φ
fulﬁlls the variational equation (30). This, together with (37),
proves (31). Next we look at the case when N tends to inﬁnity.
We consider ﬁrst the part without he integral, i.e.

N−1
∑

i=0

lim
N→∞

εu2(ti+1) = lim
N→∞

N−1
∑

i=0

(ti+1 −ti)u2(ti+1).

(39)

Proof. We use again the results presented in Section 2.2. With
(3) we have at the end of the ﬁrst needle x(ε) = x∗(ε)+εu2(ε)+

This is the limit of a Riemann sum (see e.g [16, Chapter 3]) with

5

partition {ti} such that
N−1
∑

lim
N→∞

i=0

T(cid:90)

0

εu2(ti+1) =

u2(t)dt.

(40)

Since u2 has zero mean by Assumption A2, this integral van-
ishes. For the second part of the sum we calculate similarly

ti+1

∂ F
∂ x

T(cid:90)
(cid:0)x∗(τ)(cid:1)u1(τ)Φ(cid:0)τ,ti+1
T(cid:90)
(cid:0)x∗(τ)(cid:1)u1(τ)Φ(cid:0)τ,ti+1
(cid:0)x∗(τ)(cid:1)u1(τ)Φ(τ,t)u2(t)dτ dt

∂ F
∂ x

ti+1

N−1
∑

i=0

N−1
∑

i=0

∂ F
∂ x

lim
N→∞

= lim
N→∞

T(cid:90)

T(cid:90)

=

(cid:1)εu2(ti+1)dτ
(cid:1)(ti+1 −ti)u2

(cid:0)ti+1

(cid:1)dτ

(41)

0

t

where we once more used that we have a Riemann sum here that
converges to the Riemannian integral.

Remark 4. Theorem 2 also applies with a slight modiﬁcation if
Assumption A2 is not fulﬁlled. However, periodicity is required
to extend the approximation as explained in Remark 1. Further,
the mean of u2(t) appears in (32) if it does not vanish.

√
ω cos(ωt) and u2(t) =

Remark 5. For the standard case of trigonometric functions
u1(t) =
T , one can
verify using (32) the result as presented in [7] where the case
that T tends to zero is considered. However, due to space limita-
tions, we skip the calculations here. Notice also that the related

√
ω sin(ωt), ω = 2π

T t)(cid:1) 1
2N+1 and u2(t) =(cid:0)cos( 2π

functions u1(t) =(cid:0)sin( 2π

T t)(cid:1)2N+1

are a smooth approximation of u1,u2 as deﬁned by (7) and (8)
for N ∈ N sufﬁciently large.
Remark 6. Theorem 1 is a special case of Theorem 2. This
can be seen by evaluating (31) for u1,u2 as given by (7) and
(8) and using the differentiation property of the state-transition
matrix Φ to get rid of the integral. Notice that Φ plays the role
of ¯Φ as deﬁned in the proof of Theorem 2. By that, one ends up
with (16) and following the rest of the proof of Theorem 1 the
relation becomes clear.

3.3 An accelerated gradient algorithm

In the following we present a new continuous-time optimization
algorithm. The algorithm is presented in the following Theorem
and is motivated by Lemma 1 as we will explain in Remark 7.

Theorem 3. Consider

˙z1 = z2
˙z2 = −kz2 − c1∇F(z1)− c2∇F(z1 + γz2)

(42)

equilibrium(cid:2)z∗(cid:62) 0(cid:3)(cid:62)

where k,c1,c2,γ > 0 and z1,z2 ∈ Rn. Suppose F : Rn → R,
F ∈ C 1 is locally convex on an open convex set D ⊆ Rn and
has a unique isolated minimum on D at z∗ ∈ D. Then the
of (42) is locally asymptotically stable.
Moreover, if D = Rn and F is radially unbounded, then the
equilibrium is globally asymptotically stable for (42).

Proof. It is straight forward to verify that ¯z1 = z∗, ¯z2 = 0 is an
equilibrium of (42) since ∇F(¯z1) = ∇F(¯z1 +γ ¯z2) = 0. Consider
now the Lyapunov function candidate

V = 1

2 z(cid:62)
2 z2 + (c1 + c2)F(z1)− (c1 + c2)F(z∗)

(43)
which is positive deﬁnite for z1 ∈ D, z2 ∈ Rn and attains its
minimum at the considered equilibrium. The derivative of V
along the trajectories of (42) is given by

1 ∇F(z1)

2 ˙z2 + (c1 + c2)˙z(cid:62)

˙V = z(cid:62)
= z(cid:62)
2
+ (c1 + c2)z(cid:62)
= −k(cid:107)z2(cid:107)2

(cid:0)− kz2 − c1∇F(z1)− c2∇F(z1 + γz2)(cid:1)
(cid:0)∇F(z1 + γz2)− ∇F(z1)(cid:1).

2 ∇F(z1)

2 − c2z(cid:62)

2

(44)

for all α,β ∈ D.

Since F is assumed to be locally convex, we have that

(α − β )(cid:62)(cid:0)∇F(α)− ∇F(β )(cid:1) ≥ 0

(cid:0)∇F(z1 +z2)−∇F(z1)(cid:1) ≥ 0. Thus, ˙V ≤

Here, α = z1 +γz2 and β = z1 such that α −β = γz2 and hence,
since γ > 0, we have z(cid:62)
2
0 for z1 ∈ D, z2 ∈ Rn. In particular, ˙V = 0 for z2 = 0. However,
putting z2 ≡ 0 into (42), it follows that z1 ≡ z∗ such that by
the invariance principle of Krasovskii-LaSalle we conclude that
(z∗,0) is locally asymptotically stable for (42). Equally, in case
D = Rn, V is radially unbounded and ˙V ≤ 0 for all z1,z2 ∈ Rn
such that – again by the invariance principle – the equilibrium is
globally asymptotically stable.

Remark 7. The algorithm is motivated by the result of
Lemma 1 as we will show in the following. Consider again
2 ), k ∈ N. Then we have as a generaliza-
(21) and let x(k) := x(k T
tion of (21)

x(k+2) = x(k) + ε2α(cid:0) ∂ F

(cid:0)x(k+1)(cid:1) + ∂ F

(cid:0)x(k)(cid:1)(cid:1) + O(ε3)

(45)

∂ x

∂ x

where k ∈ N. With ξ (k)
1
as

:= x(k), ξ (k)
2

:= x(k+1) we can write this

1

− ξ (k)
− ξ (k)

1 = ξ (k)
2 = ξ (k)

2 − ξ (k)
1 − ξ (k)
∂ x (ξ (k)

+ ε2α(cid:0) ∂ F

2

2 )(cid:1) + O(ε3).

(46)

1 ) + ∂ F

∂ x (ξ (k)

ξ (k+1)
1
ξ (k+1)
2

6

and

(47)

Φ(t,t j) =(cid:12)(cid:12)cos(cid:0)p(t j + Kj(x j))(cid:1)
cos(cid:0)p(t + Kj(x j))(cid:1)(cid:12)(cid:12)2

for t ∈ [ jT + ε, jT − ε + T

(cid:112)

2 ] with
4c− b2
Kj(x j) := − jT − ε + 1

p := 1
2

p arctan(

b+2x j√
4c−b2

).

This is the Euler discretization (with step size 1) of

˙ξ1 = ξ2 − ξ1

˙ξ2 = ξ1 − ξ2 + ε2α(cid:0) ∂ F

∂ x (ξ2)(cid:1) + O(ε3).

∂ x (ξ1) + ∂ F

With z1 := ξ1, z2 := ξ2−ξ1, c1 = c2 = ε2α, γ = 1 and neglecting
the O(ε3)-terms we arrive at (42). Thus, loosely speaking,
(42) can be interpreted as the averaged system corresponding
to the extremum seeking system discussed in Lemma 1. This
relationship suggests that in certain cases extremum seeking
algorithms do not mimic a simple gradient ﬂow but the more
advanced method presented in Theorem 3.
Remark 8. The continuous-time algorithm (42) can be in-
terpreted as a combination of the continuous-time heavy ball
method (see e.g. [3]) and a continuous-time version of Nes-
terov’s method as presented in [6]. In the heavy ball method
only the term ∇F(z1) is present whereas in Nesterov’s method
only the term ∇F(z1 + γz2) occurs. To be more precise, these
are given by

˙z2 = u

˙z1 = z2

(48)
where u = −kz2 − c1∇F(z1) for the heavy ball and u = −kz2 −
c2∇F(z1 + γz2) for Nesterov’s method. Notice that if F is
quadratic, i.e.
the gradient is linear, there is no difference
between Nesterov’s method and (42) up to the choice of pa-
rameters.
Remark 9. The algorithm (42) uses the gradient at two differ-
ent points which can be interpreted as a simple way of averag-
ing. Moreover, the term ∇F(z1 +γ ˙z1) uses knowledge about the
derivative of z1 which implements some kind of preview. For
small ˙z1, it is also ∇F(z1 + γ ˙z1) ≈ ∇F(z1) + γ∇2F(z1)˙z1 such
that, close to the critical point, this term may also be interpreted
as a curvature dependent damping.

4 Example

In the following we illustrate the results from Theorem 1 and
Theorem 3 by means of an example and simulations.

4.1 The case of quadratic F(x)
Suppose that F in (6) is a scalar, quadratic function, i.e.

F(x) = x2 + bx + c,

(49)
where b,c ∈ R and F : R → R. The quadratic case is important
since many convex functions are locally quadratic in a region
around their minimum. For the input sequence deﬁned by (7)
and (8) one can compute x∗(t) and Φ(t,t0) in (10) analytically.
In particular, in case of 4c− b2 (cid:54)= 0 one obtains

(cid:0)tan(cid:0)p(t + Kj(x j))(cid:1)(cid:112)

4c− b2 − b(cid:1)

x∗(t) = 1
2

(50)

7

(51)

(52)

(53)

A derivation of these equations is given in the Section 6. Notice
that this also includes the case of 4c−b2 < 0, i.e. p is a complex
number, by the deﬁnition of the trigonometric functions for
complex arguments but we do not discuss this in detail here. The
case of 4c− b2 = 0 can be handled similarly and is not treated
here as well. Notice further that x∗(t) has ﬁnite escape time at
2p π − Kj(x j) with m ∈ Z. However, in most cases
tesc, j = 2m+1
this is no problem in the implementation of the approximation
from Theorem 1. We implemented the iteration

+ εαΦ( jT, jT + ε)(cid:0)1− Φ( jT + ε, jT + T

(54)

2 − ε)(cid:1)

x(cid:0)( j + 1)T(cid:1) = x( jT )

which is another representation of (20) and where we neglected
the higher order terms. Thus, if tesc, j (cid:54)= jT and tesc, j (cid:54)= jT + ε,
the evaluation makes no problem. As shown in the Section 6,
the iteration (54) has ﬁx points at

(cid:0)(cid:112)
4c− b2 cos( pT
sin( pT

2 − 2ε)∓ 1
2 − 2ε)

− b(cid:1).

¯x1/2 = 1
2

(55)

2. Thus, since cos( pT

2 −2ε)∓1(cid:54)= 0 for sin( pT

Notice that the minimum of the function F = x2 + bx + c is at
xmin =− b
2 −2ε)(cid:54)=
0, it is ¯x1/2 (cid:54)= xmin such that the iteration never converges to
the minimum of F. However, we can get arbitrarily close.
Secondly, it is worth to mention that (54) possesses two ﬁx
points. Simulations suggest that one is asymptotically stable
and the other one is unstable where the stability depends on the
chosen period length T .
In our simulations we compared the approximative iteration (54)
with a ﬁxed step simulation of the original system, i.e. (6) with
inputs (7) and (8), and a simple gradient descent. The simulation
results are depicted in Figure 4. At the respective time points
jT our approximation is much closer to the original system as
the gradient descent which seems to correspond to an average of
the original system. This is to be expected since here the period
length is large compared to the length of the needle which is also
the reason for the large jumps to be seen on the right hand side
of Figure 4. In the considered example our approximation seems
to be a good measure for the lower “boundary” of the trajectory
of the original system. Notice that also an approximation of the
upper boundary can be computed similarly.

0

−1

−2

)
t
(
x

−3

−4

0

−1

−2

−3

1000

2000

3000
time t

4000

5000

6000

−4

0

5

10

15

20

25

time t

Figure 4. A comparison between the original system (green), i.e. system (6) with inputs (7) and (8), the approximation (10) (orange) and a gradient
descent algorithm (blue) in the case of quadratic F(x) (b = 2,c = 3,ε = 0.00001,T = 1.3,xmin = −1,α = −10). On the left hand side the thick green
line depicts the original system evaluated at multiples of the period length; the shaded area shows the original system jumping up and down very
fast. A zoomed plot that illustrates this jumping can be seen on the right hand side.
4.2 Simulative analysis of the algorithm (42)
We compared the proposed algorithm (42) to the heavy ball
method and a continuous-time version of Nesterov’s method (see
also Remark 8). Simulation results for the case of F(x) =|x|3 are
depicted in Figure 5. In the considered case the proposed algo-
rithm shows a fast convergence without overshoot in comparison
to both other algorithms. Further simulations with different ob-
jective functions or varied parameters show a similar behavior.
However, the simulation results should be interpreted with care
when it comes to performance or convergence speed since all
three algorithms include parameters and it is not clear how to
choose them in a way such that direct comparisons are possible.

0
−2
−4

time t

)
t
(
1
z

0

1

2

3

4

2

5 Conclusions and outlook

In this work we introduced needle-shaped dither signals for
gradient approximation and extremum seeking. We derived for-
mulas that give insight into the averaging process of extremum
seeking schemes with needle-shaped dither signals. We fur-
ther showed how this can be generalized to arbitrary periodic
dither signals by superposition. Thus, needle-shaped dithers
can be seen as basis functions for a wide range of more general
signals. Motivated by these results we also proposed a new
gradient-based optimization algorithm. The algorithm is related
to well-known accelerated gradient methods and is of interest
on its own. By taking two gradients into account the behavior of
the proposed algorithm is similar to the averaged behavior of the
extremum seeking scheme with needle-shaped dither signals.
This might be one hint why extremum seeking schemes often
perform relatively well in practice despite their simplicity.
Since our approach relies on well-established ideas from the

4

5

6

Figure 5. Comparison of the heavy ball method (blue), Nesterov’s
method (green) and the proposed algorithm (42) (orange) for the min-
imization of F(x) = |x|3. The parameters of the new algorithm are
chosen as c1 = c2 = γ = 1, K = 2. For Nesterov’s method they are cho-
sen such that for the case of quadratic F(x) it is equal to the proposed
algorithm. The numerical integration is done using a standard ﬁxed
step Euler method.

Maximum Principle we hope that we can extend our setup using
existing generalizations of the Maximum Principle ([14]). As
already mentioned before, we also expect that our results can be
extended to the multidimensional case. Moreover, we aim to use
our new knowledge about the gradient approximation process to
design dither signals that are in some terms optimal.

References

[1] Milton Abramowitz, Irene A. Stegun, et al. Handbook
of mathematical functions, volume 1. Dover New York,
1972.

8

[2] Kartik B. Ariyur and Miroslav Krstic. Real-time optimiza-
tion by extremum-seeking control. John Wiley & Sons,
2003.

[3] Hedy Attouch, Xavier Goudou, and Patrick Redont. The
heavy ball with friction method, I. The continuous dynam-
ical system. Communications in Contemporary Mathemat-
ics, 2(01):1–34, 2000.

[4] Roger W. Brockett. Nonlinear systems and differential
geometry. Proceedings of the IEEE, 64(1):61–72, Jan
1976.

[5] Pascal Cougnon, Denis Dochain, Martin Guay, and Michel
Perrier. On-line optimization of fedbatch bioreactors by
adaptive extremum seeking control. Journal of Process
Control, 21(10):1526–1532, 2011.

[6] Hans-Bernd Dürr, Erkin Saka, and Christian Ebenbauer.
A smooth vector ﬁeld for quadratic programming. In De-
cision and Control (CDC), 2012 IEEE 51st Annual Con-
ference on, pages 2515–2520, Dec 2012.

[7] Hans-Bernd Dürr, Milos S. Stankovic, Christian Eben-
bauer, and Karl Henrik Johansson. Lie bracket approxima-
tion of extremum seeking systems. Automatica, 49(6):1538
– 1552, 2013.

[8] Thomas Kailath. Linear Systems. Prentice-Hall, 1980.
[9] Hassan K. Khalil. Nonlinear Systems. Prentice-Hall, 2002.
[10] Jaroslav Kurzweil and Jiˇrí Jarník. Limit processes in or-
dinary differential equations. Zeitschrift für angewandte
Mathematik und Physik ZAMP, 38(2):241–256, 1987.

[11] Daniel Liberzon. Calculus of variations and optimal con-
trol theory: A concise introduction. Princeton University
Press, 2011.

[12] S. Michalowsky and C. Ebenbauer. Gradient approxi-
mation and extremum seeking via needle variations. In
American Control Conference (ACC), 2016 (to appear),
2016.

[13] James C. Spall. Multivariate stochastic approximation
using a simultaneous perturbation gradient approximation.
Automatic Control, IEEE Transactions on, 37(3):332–341,
1992.

[14] Héctor J. Sussmann. Needle variations and almost lower
semicontinuous differential inclusions. Set-valued analy-
sis, 10(2-3):233–285, 2002.

[15] Ying Tan, Dragan Nesic, and Iven Mareels. On the choice
of dither in extremum seeking systems: A case study.
Automatica, 44(5):1446 – 1450, 2008.

[16] William F. Trench. Introduction to real analysis. Prentice

Hall/Pearson Education Upper Saddle River, NJ, 2003.

[17] Hsin-Hsiung Wang, Simon Yeung, and Miroslav Krstic.
Experimental application of extremum seeking on an axial-
ﬂow compressor. Control Systems Technology, IEEE
Transactions on, 8(2):300–309, 2000.

[18] Chunlei Zhang and Raúl Ordóñez. Extremum-seeking
control and applications: A numerical optimization-based
approach. Springer, 2011.

9

6 Appendix

6.1 Derivation of (50), (51)
For F as deﬁned by (49) and t ∈ [ jT + ε, jT − ε + T
x∗(t) is the solution of

2 ], j ∈ N,

˙x∗(t) = x∗2(t) + bx∗(t) + c,

x( jT + ε) =: x j.

(56)

We will now solve this differential equation via separation of
variables. Resorting and integrating gives

(cid:90) x∗

x j

(cid:20)

(cid:90) t

(cid:21)x∗

)

x j

1

ξ 2+bξ +c dξ =

jT +ε

1dτ.

(57)

These integrals can be solved using standard formulas for ele-
mentary functions, see e.g. [1]. If 4c− b2 (cid:54)= 0 we have that

2√
4c−b2

arctan( b+2ξ√
4c−b2

= t − jT − ε.

(58)

∂ F

2 ln( i+z

(50) into this equation we obtain

Notice that this also includes the case of 4c − b2 < 0 by the
deﬁnition of the arctan as arctan(z) = i
i−z ) (see [1]) where
√−1 is the imaginary unit. Thus, solving this equation for x∗
i =
we obtain the solution as given by (50). To obtain the transition
∂ x (x∗(τ))dτ(cid:1). Putting the solution
e.g. [8]) Φ(t,t j) = exp(cid:0)(cid:82) t
matrix Φ(t,t j) we use that the variational equation (9) here is
a scalar linear time varying differential equation such that (see
(cid:19)
(cid:18)(cid:90) t
(cid:112)
4c− b2 tan(cid:0)p(τ + Kj(x j))(cid:1)dτ
− ln(cid:0)(cid:12)(cid:12)cos(p(τ + Kj(x j)))(cid:12)(cid:12)(cid:1)
(cid:19)
(cid:21)t
(cid:20)
(cid:18)(cid:112)
=(cid:12)(cid:12) cos(cid:0)p(t + Kj(x j))(cid:1)
cos(cid:0)p(t j + Kj(x j))(cid:1)(cid:12)(cid:12)−2

t j
4c− b2

Φ(t,t j) = exp

= exp

(59)

p

t j

t j

.

6.2 Fix points of the iteration (54)

We brieﬂy analyze the iteration (54). Let t j1 := jT + ε and
2 − ε. Since ε > 0, α > 0, (54) has a ﬁx point at ¯x
t j2 := jT + T
implicitly given by the equation

1− Φ(t j1,t j2) = 1−(cid:12)(cid:12) cos(cid:0)p(t j2+Kj( ¯x))(cid:1)
cos(cid:0)p(t j1+Kj( ¯x))(cid:1)(cid:12)(cid:12)2
Φ(t j1,t j2) =(cid:12)(cid:12) cos(cid:0) pT
)(cid:1)
cos(cid:0)arctan(

2 −2pε+arctan(
b+2x j√
4c−b2

b+2x j√
4c−b2

= 0.

(60)

)(cid:1)

(cid:12)(cid:12)2

(61)

By (51) we have with (52) and (53)

and using the trigonometric identity

we obtain

cos(α+β )
cos(β ) = cos(α) + sin(α) sin(β )
cos(β )

Φ(t j1,t j2) =(cid:12)(cid:12)cos(α)− sin(α)

(cid:12)(cid:12)2

b+2x j√
4c−b2

(62)

(63)

with α := pT

2 − 2pε. We compute further

1− Φ(t j1,t j2)

= 1− cos2(α) + 2sin(α)cos(α)

= sin2(α)(cid:0)1− (b+2x j)2

α√
4c−b2

(cid:1) + 2sin(α)cos(α)

4c−b2

+ sin2(α) (b+2x j)2
4c−b2

b+2x j√
4c−b2

.

(64)

In the following we assume that sin(α) (cid:54)= 0. Then, using the
previous results, the equation for the ﬁx point is given by

sin(α)(cid:0)1− (b+2 ¯x)2

4c−b2

(cid:1) + 2cos(α) b+2 ¯x√

4c−b2

= 0.

(65)

Let w := (b+2 ¯x)√
4c−b2
solutions are given by w1/2 = cos(α)∓1
sin(α)
2pε, the ﬁx points are given as in (55).

. Then this is a quadratic equation in w and its
2 −

. Thus, with α = pT

10

