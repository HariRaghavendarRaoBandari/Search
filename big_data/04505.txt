6
1
0
2

 
r
a

 

M
4
1

 
 
]
h
c
e
m

-
t
a
t
s
.
t
a
m
-
d
n
o
c
[
 
 

1
v
5
0
5
4
0

.

3
0
6
1
:
v
i
X
r
a

Eigenvector Method for Umbrella Sampling

Eigenvector method for umbrella sampling enables error analysis

Erik Thiede,1, a) Brian Van Koten,2, a) Jonathan Weare,2, b) and Aaron R. Dinner1, c)
1)Department of Chemistry and James Franck Institute, the University of Chicago, Chicago,
IL 60637
2)Department of Statistics and James Franck Institute, the University of Chicago, Chicago,
IL 60637

Umbrella sampling eﬃciently yields equilibrium averages that depend on exploring rare states of a model
by biasing simulations to windows of coordinate values and then combining the resulting data with physical
weighting. Here, we introduce a mathematical framework that casts the step of combining the data as an
eigenproblem. The advantage to this approach is that it facilitates error analysis. We discuss how the error
scales with the number of windows. Then, we derive a central limit theorem for averages that are obtained
from umbrella sampling. The central limit theorem suggests an estimator of the error contributions from
individual windows, and we develop a simple and computationally inexpensive procedure for implementing it.
We demonstrate this estimator for simulations of the alanine dipeptide and show that it emphasizes low free
energy pathways between stable states in comparison to existing approaches for assessing error contributions.
We discuss the possibility of using the estimator and, more generally, the eigenvector method for umbrella
sampling to guide adaptation of the simulation parameters to accelerate convergence.

I.

INTRODUCTION

One of the main uses of molecular simulations is the
calculation of equilibrium averages. For understanding
reaction processes, the free energy projected onto selected
coordinates (collective variables) is of special interest. It
relates directly to the probabilities of the coordinates
taking particular values, and it can provide valuable in-
formation about the stable states, the barriers between
them, and the origin of their stabilization. Furthermore,
it is the starting point for most rate theories. Although
in principle the free energy can be estimated from a long
unbiased simulation, in practice doing so is challenging
because bottlenecks slow the exploration of the conﬁgu-
ration space. In other words, transitions between regions
of the space are very infrequent in comparison to local
ﬂuctuations.

Various methods have been introduced to overcome
this problem. Here, we consider one of the oldest and
still most widely used such methods, umbrella sampling
(US).1 In this approach, the collective-variable interval
of interest is covered by a series of simulations, in each
of which the system is biased such that sampling is re-
stricted to a relatively narrow window of values of the
collective variables. This can be accomplished by addi-
tion of a biasing potential that is small in the window
and large outside it. The information from the diﬀerent
simulations must be combined, and the eﬀect of the bias
removed, to obtain the overall free energy proﬁle. This
requires consistently normalizing the probabilities in dif-
ferent windows, a task that is complicated by the fact
that the simulations are run independently.

Considerable eﬀort has been devoted to determining
how best to combine the results from diﬀerent simula-

a)Equal contributions
b)Electronic mail: weare@uchicago.edu
c)Electronic mail: dinner@uchicago.edu

Initially, researchers manually adjusted the zero
tions.
of free energy in each window to make the full free en-
ergy proﬁle continuous and, often, smooth; conﬂicting
results arising from limited sampling at the window pe-
ripheries were removed. The desire to use all the simula-
tion data motivated the introduction of estimators that
allow for systematically combining the data from diﬀer-
ent simulations. By far, the most widely used of these in
chemical physics applications is the weighted histogram
analysis method (WHAM). The multistate Bennett ac-
ceptance ratio (MBAR) method, as it is referred to in
the the molecular-simulation literature and will be re-
ferred to here, is closely related but does not rely on
binning the data.2–5 Both WHAM and MBAR can be
derived from maximum-likelihood or minimum asymp-
totic variance principles assuming independent, identi-
cally distributed sampling in each window, and have cor-
responding statistical optimality properties under those
conditions. Recent extensions seek to improve perfor-
mance when the sampling is limited and to extend the
algorithm to more general ensembles.6,7

In the present paper, we introduce an alternative
scheme for estimating the free energy from US simula-
tion data. In this approach, the normalization constants
needed to combine information from separate simula-
tion windows are the components of the eigenvector of
a stochastic matrix that can be constructed from run-
ning averages in the windows. We thus term our method
Eigenvector Method for Umbrella Sampling (EMUS).
The advantage of our method is that it lends itself to
error analysis. Following previous work,8–10 we measure
error with the asymptotic variance.

Our paper is organized as follows. After giving some
background on US in Section II, we formulate EMUS
in Section III. In Section V, we show that EMUS per-
forms comparably to WHAM and MBAR, and discuss
its connection with the latter.
In Section VI, we use
scaling arguments with simplifying assumptions to show
that accounting for the error associated with combining

Eigenvector Method for Umbrella Sampling

2

the data is important and limits the speedups that can
be achieved by increasing the number of simulation win-
dows. In Section VII, we provide the full numerical analy-
sis, which applies generally, without simplifying assump-
tions. Speciﬁcally, we derive a central limit theorem for
averages from EMUS and use it to develop a means for
estimating the error contributions from individual win-
dows. We demonstrate the method for the free energy
projected onto the φ and ψ dihedral angles of the alanine
dipeptide and compare the error contributions with those
from an estimator introduced by Zhu and Hummer11. We
conclude in Section VIII.

II. BACKGROUND ON UMBRELLA SAMPLING

Here, we review umbrella sampling and establish ba-
sic terms and notation. The goal is the calculation of
an average of an observable g over a time-independent
probability distribution π:

hgi =Z g(x)π(x)dx.

(1)

At thermal equilibrium, π is the Boltzmann distribution:

π(x) =

,

(2)

exp (−H0(x)/kBT )
R exp (−H0(x)/kBT ) dx

where H0 is the system Hamiltonian, kB is Boltzmann’s
constant, and T is the temperature. In particular, we can
express the the free energy diﬀerence between two states
S1 and S2 as

h1S2i(cid:19) ,
∆G = −kBT ln(cid:18)h1S1i

where 1 is the indicator function

1S(x) =(1

0

if x ∈ S, and
otherwise.

(3)

(4)

Similarly, the reversible work to constrain a collective
variable q(x) to a particular value q′, also known as the
potential of mean force (PMF), may be written as

W (q′) = −kBT lnhδ (q − q′)i .

(5)

For complex systems, averages of the form in (1) must
be evaluated numerically. Typically, this is done by gen-
erating a chain of related conﬁgurations, Xt, using Monte
Carlo methods or molecular dynamics, and by assuming
ergodicity. Namely, as the number of conﬁgurations N
goes to inﬁnity, hgi is the limit of the sample mean:

¯g =

1
N

N −1

Xt=0

g(Xt).

(6)

In all practical sampling methods, successive conﬁgura-
tions are strongly correlated. While ergodicity guaran-
tees that sample means converge to averages over π, con-
vergence can be extremely slow if the correlation between

subsequent points is strong. This is the case when sam-
pling π relies on visiting low-probability states, such as
transition states of chemical reactions.

US methods address this issue by enforcing sampling of
diﬀerent regions of conﬁguration space (windows), intro-
ducing L nonnegative bias functions ψi and then using L
independent simulations to sample from the biased prob-
ability distributions

πi(x) ∝ ψi(x)π(x).

(7)

The essential idea is that sampling each πi is fast because
ψi is chosen so that relatively likely states under ψi are
not separated by relatively unlikely states. This is ac-
complished by restricting the set of states on which ψi
is non-negligible so that π is closer to constant on that
set. In Section VI C we make this point more carefully
by examining a regime in which umbrella sampling can
be shown to be exponentially more eﬃcient than direct
simulation. A popular choice is to use bias functions that
take a Gaussian form:

ψi(q) = exp(cid:18)−

1
2

i(cid:1)2
ki(cid:0)q − q0

/kBT(cid:19) ,

such that

πi(x) ∝ exp(cid:20)−(cid:18)H0(x) +

1
2

ki(cid:0)q − q0

i(cid:1)2(cid:19) /kBT(cid:21) .

This corresponds to adding a harmonic potential centered
at q0
i with spring constant ki to the system Hamiltonian.
We call the relative normalization constant (or partition
function) of the i-th biased distribution zi:

(8)

(9)

(11)

We also deﬁne the free energy in window i as

zi =

.

(10)

R ψi(x)π(x)dx
PL
k=1R ψk(x)π(x)dx
Gi = −kBT ln zi.

We denote averages over the biased distributions by

hgii =Z g(x)πi(x) dx.

Overall averages of interest, hgi, can be estimated as zi-
weighted sum of averages computed in each of the win-
dows. We detail our prescription in the next section.

III. THE EIGENVECTOR METHOD FOR UMBRELLA
SAMPLING

In this section, we present the Eigenvector Method for

Umbrella Sampling (EMUS). We begin by deﬁning

g∗ ≡

.

g

k=1 ψk

PL

Eigenvector Method for Umbrella Sampling

3

for any function g. Then, we observe that

we denote the sample means by

π(x) dx

k=1 ψk(x)

hgi =Z g(x)π(x) dx
R ψi(x)π(x)dxi
i=1 ψi(x)h R ψi(x)π(x)dx

=Z g(x)
PL


Xi=1Z ψi(x)π(x)dxR g∗(x)ψi(x)π(x)dx
R ψi(x)π(x)dx
zi  L
Xk=1Z ψk(x)π(x)dx! hg∗ii .
Xi=1

PL

=

=

L

L

The factor in parentheses can be taken out of the sum
over i. To express this factor in terms of computable
averages, we repeat the same steps with g = 1:

L

Xk=1Z ψk(x)π(x)dx =

Substituting (13) into (12),

.

(13)

1

PL
i=1 zi h1∗ii

hgi = PL
k=1 zi hg∗ii
PL
k=1 zi h1∗ii

.

(14)

Consequently, if we can evaluate the zi and the hg∗ii then
we can assemble the original average hgi of interest. The
averages hg∗ii can be computed by sequences X i
t (typi-
cally independent for each i) that sample πi. Umbrella
sampling methods diﬀer primarily in how the zi are com-
puted.

To express the constants zi in terms of averages over
the biased distributions, we take g(x) = ψj(x) in (12).
Then, zi solves

zj =

L

Xi=1

ziFij , where Fij =(cid:10)ψ∗
j(cid:11)i

.

(15)

That is, the vector of normalization constants z is a left
eigenvector of the matrix F with eigenvalue one. Under
conditions to be elaborated upon in Section III B, the
solution to (15) is uniquely speciﬁed when we notice that

zi = 1.

(16)

L

Xi=1

A. Computational Procedure

In the EMUS algorithm, we estimate the entries of F
and the averages hg∗ii and h1∗ii by sample means, then
assemble the estimate of hgi using (14). To be precise,

¯g∗
i =

¯1∗
i =

¯Fij =

1
Ni

1
Ni

1
Ni

Ni−1

Ni−1

Xt=0
Xt=0
Xt=0

Ni−1

,

, and

.

g(X i
t )

1

Pk ψk(X i
Pk ψk(X i
Pk ψk(X i

ψj(X i
t )

t )

t )

t )

EMUS proceeds as follows:

(12)

1. Choose the biasing functions ψi.

2. Compute trajectories that sample states X i

t from

the biased distributions πi.

3. Calculate the matrix ¯F and the averages ¯g∗

i and 1∗
i .

4. Calculate the vector of estimated normalization

constants z EMUS as the solution to

z EMUS
j

=

L

Xi=1

z EMUS
i

¯Fij with

z EMUS
i

= 1.

L

Xi=1

We use QR factorization as given by Golub and
Meyer.12 See also Section VII B 1.

5. Compute the estimate of hgi:

hgiEMUS = PL
PL

i

i=1 z EMUS
i=1 z EMUS

i

¯g∗
i
¯1∗
i

(17)

by substituting z EMUS and the sample means
in (14).

We remark that when one wishes to compute a free en-
ergy diﬀerence or a ratio of two observables, as in equa-
tion (3), it is not necessary to compute 1∗
i . Instead, one
may use the formula

h1S1i
h1S2i

= PL
i=1 zi h1S1ii
PL
i=1 zi h1S2ii

in place of (17).

B. The Eigenvector Problem

In this section, we give conditions under which the
eigenvector problem has a unique solution. First, we
show that F is a stochastic matrix; that is, each element
Fij is nonnegative and every row of F sums to one. For
the latter,

Fij =

L

Xj=1

L

Xj=1* ψj
k=1 ψk+i
PL

j=1 ψj

=*PL
k=1 ψk+i
PL

= 1.

Eigenvector Method for Umbrella Sampling

4

The entries of F are nonnegative since we require that
the bias functions be nonnegative. One can show that
the matrix ¯F is also stochastic by similar arguments.

A stochastic matrix J has a unique eigenvector with
eigenvalue one if and only if it is irreducible: for every
possible grouping of the indices into two distinct sets, A
and B, Jij 6= 0 for some i ∈ A and j ∈ B.13 In fact,
this statement remains true when J is non-negative with
largest eigenvalue equal to one. For any such matrix
we let z(J) denote the continuous function returning the
unique left eigenvector of J corresponding to eigenvalue
one.

In the case of the particular stochastic matrix F de-
ﬁned in (15) these statements imply that if, for any di-
vision of the indices into sets A and B, there is suﬃ-
cient overlap between the sets ∪i∈A{x : ψi(x) > 0} and
∪j∈B{x : ψj(x) > 0} then there will be a unique solu-
tion z(F ) to (15) which will necessarily equal the relative
normalization constants z deﬁned in (10). Because z(J)
is a continuous function of its arguments, z EMUS = z( ¯F )
converges to z as ¯F converges to F. Consequently, EMUS
produces a consistent estimator in the sense that if the
sample averages used to estimate the entries Fij and hg∗ii
converge (in probability or with probability one) to the
true values, then the estimate of hgi also converges (in
the same sense).

IV. THE CONNECTION BETWEEN EMUS AND MBAR

in

the

upon

Shirts

Building

earlier work

and Chodera5

statistics
literature2,3,14,
suggested a
class of algorithms for estimating free energy diﬀerences
between states, which they termed MBAR. This method
is similar to WHAM but does not require binning the
simulation data to form histograms (see Tan et al.8).
In this section, we explain the relation between EMUS
and MBAR5. We also derive a new iterative method
for solving the MBAR equations, and we show that
our iteration leads naturally to a new family of related
consistent estimators.

Shirts and Chodera’s5 starting point is the identity (see

their (5))

L

L

Xi=1

zj

hαij (x)ψi(x)π(x)ij =

zi hαij(x)ψj (x)π(x)ii ,
(18)
where αij (x) is an arbitrary function. They proposed the
choice

Xi=1

αMBAR

ij

(x) =

,

(19)

ni/zi

Pk ψk(x)π(x)nk/zk

where ni is the number of uncorrelated samples in win-
dow i. Substituting (19) into (18) gives

zj =

L

Xi=1

zi(cid:28) ψj(x)ni/zi
Pk ψk(x)nk/zk(cid:29)i

.

(20)

We can cast (20) in a form reminiscent of EMUS by

(21)

(22)

writing

where

zj =

L

ziFij (z),

Xi=1
Pk ψk(x)nk/wk(cid:29)i

Fij(w) =(cid:28) ψj(x)ni/wi

for any vector w with positive entries. EMUS corre-
sponds to setting w = n so that

αEMUS

ij

(x) =

1

,

Pk π(x)ψk(x)

and (18) reduces to the eigenproblem (15).

In practice, one must replace the matrix Fij (w) in (22)

by the sample mean approximation

¯Fij (w) =

1
Ni

Ni−1

Xt=0 " ψj(cid:0)X i
Pk ψk(cid:0)X i

t(cid:1) nk/wk# .
t(cid:1) ni/wi

Substituting ¯Fij (z) for Fij (z) in (21) yields the equation

(23)

zMBAR
j

=

L

Xi=1

zMBAR
i

¯Fij (zMBAR)

(24)

for zMBAR, which we refer to here as the MBAR estima-
t are independent, MBAR is the
tor.
nonparametric maximum-likelihood estimator of z.2

If the samples X i

In practice, the samples X i

t are not independent for a
given i, and the ni must be estimated from data. Several
algorithms for estimating the ni have been proposed.15,16
Shirts and Chodera5 base their estimates on the inte-
grated autocorrelation times of physically-motivated co-
ordinates, and we follow this common practice here.
In fact, once the ni have been estimated, Shirts and
Chodera5 suggest replacing sample averages over all Ni
points by sample averages over the ni points obtained by
including only every Ni/ni-th sample along the trajec-
tory. We note that both the subsampling approach and
the one in (23) correspond to approximations of expres-
sion (18) with (19), and we regard both as variations on
the MBAR estimator. When the samples are indepen-
dent, the two approaches are the same. In tests of the
iterative EMUS algorithm introduced below, we ﬁnd es-
timates to be insensitive to the choice of ni and they can
be set equal to 1, though in that case the estimator no
longer corresponds directly to MBAR.

As written above, the MBAR estimator (24) resem-
bles an eigenvector problem. However, the dependence
of ¯F (z) on z implies that the solution must be obtained
self-consistently. The approach advocated by Shirts
and Chodera for computing the MBAR estimator corre-
sponds in the framework described here to solving (24) by
a Newton-type iteration. However, the eigenvector form
of (24) suggests an alternative approach. Rather than
Newton’s method, we employ the following algorithm:

Eigenvector Method for Umbrella Sampling

5

1. As an initial guess for zMBAR, choose a vector z0
with positive entries. Estimate the ni. Set m = 0.

2.

(a) Calculate ¯Fij (zm) according to (23).
(b) Calculate a new estimate zm+1 of zMBAR by

solving the eigenproblem

L

zm+1
i

¯Fij (zm).

(25)

=

zm+1
j

Xi=1
i |/zm
− zm
(a) Increment m;

3. If maxi |zm+1

i

i > Tolerance,

(b) Go to Step 2.

To show that this iteration makes sense, we must prove
that the eigenproblem (25) always has a unique solution
and that zm converges to zMBAR as m goes to inﬁnity.
To see that the eigenproblem has a solution, ﬁrst observe
that if ¯Fij (w) is irreducible for one vector with positive
entries, w, then it is irreducible for all vectors with pos-
itive entries. When applying the EMUS method, we as-
sume that both F = F (n) and ¯F = ¯F (n) are irreducible.
Thus, we may assume that ¯F (w) is irreducible for any
w. Moreover, observe that for any positive vector w, the
vector with entries ni/wi is a right eigenvector of ¯F (w)
with eigenvalue one and positive entries. It follows from
the Perron-Frobenius theorem that the matrix ¯F (w) has
a unique left eigenvector z( ¯F (w)) with eigenvalue one
and that z( ¯F (w)) has positive entries. Thus, the eigen-
problem always has a unique solution. We do not have a
proof that the iterates converge. However, since zMBAR is
a ﬁxed point of the iteration, if the iterates do converge
their limit must be zMBAR. In practice, we ﬁnd that the
iteration converges quickly, usually to a relative error of
10−6 within 10 iterates.

In addition to its apparently rapid convergence, an-
other argument in favor of the algorithm that we intro-
duce above for solving (24) is that each iteration of the
scheme results in a new consistent estimator. We will use
the term iterative EMUS to refer to this family of esti-
mators. With the initial guess z0 = n, the result, z1, of
the ﬁrst iteration is the EMUS estimator deﬁned in Sec-
tion III. In Appendix A, we show that for any ﬁxed ﬁnite
number of iterations m, zm is also a consistent estimator
of the vector z of normalization constants. By contrast,
other schemes, such as Newton’s method, for solving (24)
may require that the number of iterations goes to inﬁnity
to obtain a consistent estimate. We also remark that the
consistency result in Appendix A holds as long as the ni
converge to non-random, positive values with increasing
numbers of samples Ni. They can be chosen as described
above, or simply set to a ﬁxed value.

Diﬀerences between the iterative EMUS scheme above
and the application of Newton’s method proposed by
Shirts and Chodera5 are mostly matters of implemen-
tation. As we will see in the next section, the results are
not very sensitive to these computational details; most of

the accuracy in the iterative EMUS approach is achieved
in the ﬁrst step. In any case, we remind the reader that
the primary goal of this paper is to characterize those
properties of the broader umbrella sampling approach
that are essential to its success, not to analyze details
of implementation.

While we focus here on potentials of mean force, the
MBAR estimator has been applied to a broader category
of free energy problems, including the analysis of single-
molecule pulling experiments and alchemical free energy
calculations.5,17,18 The close relation between EMUS and
MBAR indicates that error analysis of EMUS may pro-
vide insight into the sources of error in MBAR for these
problems, but we do not pursue this idea further in the
present work.

V. NUMERICAL COMPARISON

To test the algorithm numerically, we performed 100
independent umbrella sampling calculations for the PMF
of the φ coordinate of the alanine dipeptide (i.e., N -
acetyl-alanyl-N ′-methylamide) in vacuum. Simulations
were run using GROMACS version 5.1.1 with harmonic
bias potentials applied using the PLUMED 2.2.0 soft-
ware package.19,20 The molecule was represented by the
CHARMM 27 force ﬁeld without CMAP corrections,21
with covalent bonds to hydrogen atoms constrained by
the SHAKE algorithm.22 Twenty windows were evenly
spaced along the φ dihedral angle. The force constant
ki = 0.00760535 × 10−2 kcal mol−1 degree−2 such that
the standard deviation of the Gaussian bias functions
was 9◦. In each window, we integrated the equations of
motion with the GROMACS leap-frog Langevin integra-
tor with a 1 fs time step. The system was equilibrated
for 40 ps and then sampled for 100 ps, saving structures
every 10 fs.

The data was then analyzed with EMUS, Grossﬁeld’s
implementation of WHAM,23 and the algorithm pro-
posed by Zhu and Hummer (ZH) (see equation A1 and
following discussion in Appendix A).11 The data was also
analyzed with pyMBAR;5 as pyMBAR gave results vir-
tually identical to WHAM, the results are not shown.
In Figure 1, we show the resulting average potentials of
mean force, as well as the standard deviation of the es-
timates over the 100 runs. WHAM and EMUS converge
to the same result. This is to be expected, as both al-
gorithms are consistent (i.e., they converge to the exact
result as the amount of samples in each window tends
to inﬁnity; see Section VII), although WHAM exhibits a
small bias from the binning of data for the histograms.5
Although the standard deviations of the free energies are
generally higher for EMUS than for WHAM, they are of
comparable magnitude. Compared to the ZH algorithm,
EMUS also has a higher standard deviation. However,
ZH is based on thermodynamic integration and uses the
trapezoid rule to calculate free energy diﬀerences between
windows.11 As a consequence it suﬀers from noticeable

Eigenvector Method for Umbrella Sampling

6

quadrature error,24 which causes the barrier height to
converge to an artiﬁcially low value.

In Figure 1 C, we apply the self-consistent iteration.
For this calculation, we estimate the number of indepen-
dent samples in each window (ni) from the integrated
autocorrelation time of the φ dihedral angle time series.
We plot the standard deviation of the values of z calcu-
lated after the ﬁrst iteration (EMUS), the second itera-
tion, and after convergence to a relative residual smaller
than 10−6. In general, convergence is achieved after an
average of 9 iterations; none of the 100 data sets required
more than 15 iterations. However, we note that after two
iterations, the estimates of z already have a standard de-
viation equivalent to that of the WHAM algorithm.

JUSTIFICATION FOR UMBRELLA SAMPLING BY

VI.
SCALING ARGUMENTS

The quality of a statistical estimate from umbrella
sampling depends strongly on the choices made for the
simulation windows. In this section, we discuss how the
error scales as properties of the simulation change. We
begin in Subsection VI A with a description of a preva-
lent justiﬁcation for the use of US. We show in Subsection
VI B that this argument is incomplete and, in turn, mis-
leading. In Subsection VI C, we provide an alternative
justiﬁcation; namely, we show that in the low temper-
ature limit, the cost to achieve a ﬁxed accuracy by US
grows slowly compared to direct simulation. In this Sec-
tion we make several simplifying assumptions that allow
us to draw precise conclusions about the scaling proper-
ties of EMUS. In Section VII we provide error bounds for
EMUS under much more general assumptions.

A. Scaling in the Limit of Many Windows

To justify umbrella sampling, it is often suggested that
the total computational time required to accurately sam-
ple statistics is inversely proportional to the number of
windows, L.15,25–28 The argument for this scaling pro-
ceeds as follows.

• Divide a one-dimensional collective variable space
into L windows of equal length, inversely propor-
tional to L (i.e., L−1).

• Assume the windows are small enough that no free
energy barriers exist in each window. The time to
explore a window should be diﬀusion limited and
proportional to the length of the window squared.
Therefore, the simulation time required to accu-
rately sample statistics in one window is also pro-
portional to L−2.

• Because there are L windows, the total simulation
time required to compute averages to ﬁxed accu-
racy should scale as L × L−2 = L−1.

FIG. 1. Comparison of umbrella sampling methods applied to
simulation data for the alanine dipeptide. (A) Average win-
dow free energies, Gi, for the indicated methods. Error bars
are estimated standard deviations of the means. (B) Stan-
dard deviation of each method relative to that of the WHAM
algorithm. Colors are the same as in (A). (C) EMUS as the
ﬁrst step in a self-consistent iteration to solve the MBAR
equations (see text). The number of uncorrelated samples in
each window (ni) was estimated by calculating the integrated
autocorrelation of the φ dihedral angle from each trajectory.
Results shown are for identical molecular dynamics data (see
text for simulation details); the methods diﬀer only with re-
spect to combination of the data to estimate the free energies.

Eigenvector Method for Umbrella Sampling

7

While this argument is now standard,15,25–28 Virnau and
M¨uller29 observed that the error for computing the free
energy diﬀerence between phases of Lennard-Jones par-
ticles with an approximately ﬁxed amount of sampling
was insensitive to the number of windows in practice,
and they noted that the argument above neglects the
error associated with combining the data from diﬀerent
simulation windows. This intuition is supported by our
analysis in the next subsection, which shows that the to-
tal computational cost to achieve a ﬁxed accuracy should
be insensitive to the choice of L, so long as it is suﬃciently
large.

B. A Simple Model Problem

To perform a more precise analysis, we make a number
of simplifying assumptions. We emphasize that these as-
sumptions are in force only for the purposes of the scaling
arguments in this section. We provide more general error
bounds for EMUS in Section VII.

Assumption VI.1. The total computation time, N , is
divided equally among the windows such that Ni = N/L.

Assumption VI.2. The ψi are functions on the one-
dimensional interval [0, 1], and the set of points where ψi
is non-zero, {q : ψi > 0}, is an interval of length |{q :
ψi > 0}| ≤ γ/L. We also assume that ψiψj = 0 unless
|j−i| < 0. Consequently, both the exact matrix F and the
sample mean ¯F are tri-diagonal. This assumption clearly
does not hold when the bias functions ψi are Gaussian.
Nonetheless, the rapid decay of Gaussian bias functions
away from their peaks guarantees that entries of F and ¯F
far from the diagonal are very small, such that we expect
our conclusions to still hold (though their justiﬁcation
would be more complicated).

Assumption VI.3. The overlap of ψi and ψi±1 (i.e.,
the integral of their product) is large enough that

min{Fi,i+1, Fi,i−1} > δ > 0

(26)

for all L and for all i ≤ L. If our last assumption holds,
but this one does not, then we can ﬁnd more than one vec-
tor z satisfying equations (15) and (16). This assumption
is a slightly stronger version of the notion of irreducibility
that we deﬁned earlier (see Section III B). Note that we
require the irreducibility to hold uniformly in the large L
limit, and we thus introduce the δ, which is independent
of L.

Assumption VI.4. Sample averages computed in diﬀer-
ent windows are independent, i.e., ¯Fi,i±1 and ¯Fj,j±1 for
j 6= i are independent. We do not assume (here or any-
where else in this paper) that samples generated within
a single window are independent.
Indeed, even if the
samples from πi are independent, ¯Fi,i+1 and ¯Fi,i−1 are
dependent random variables.

As an example average, let us consider the error in the
free energy diﬀerence between the ﬁrst and last windows:

z1(cid:19) .
∆ ¯GL,1 = −kBT ln(cid:18) zL

(27)

Assumption VI.2 is suﬃcient for z( ¯F ) to be in detailed
balance with ¯F (Kelly30, Lemma 1.5 and Section 1.3):

zi+1( ¯F ) = zi( ¯F )

¯Fi,i+1
¯Fi+1,i

.

Using (28) recursively,

i=1

i=1

¯Fi,i+1

¯Fi+1,i!

∆ ¯GL,1 = −kBT ln QL−1
QL−1
ln(cid:18) ¯Fi,i+1
¯Fi,i−1(cid:19) .
Xi=2

= −kBT ln ¯F1,2 + kBT ln ¯FL,L−1

+ kBT

L−1

(28)

(29)

To understand the error (variance) of the terms in (29),
we must further specify Fi,i+1 and Fi,i−1.

Assumption VI.5. For Nmin and Lmin suﬃciently
large, when Ni ≥ Nmin and L ≥ Lmin,
¯Fi,i−1(cid:19)(cid:19) ≤

NiL2 ≤ var(cid:18)ln(cid:18) ¯Fi,i+1

Kmax
NiL2

Kmin

(30)

for i = 2, 3, . . . , L − 1, and the same upper and lower
bounds hold for var (ln F1,2) and var (ln FL,L−1). This is
just a precise interpretation of the diﬀusion limited sam-
pling assumption made in the standard justiﬁcation of
US reproduced in the last subsection. Under such an as-
sumption we expect both ¯Fi,i+1 and ¯Fi,i−1 to have vari-
ance on the order of 1/(NiL2) and, in light of (26), the
function ln(x/y) is smooth near (x, y) = (Fi,i+1, Fi,i−1).
These considerations are closely related to Lemma VII.2
in the next section.

With all the assumptions in hand, we now complete the
argument by taking the variance of both sides of (29).
Since samples from diﬀerent windows are independent,
the variance of ¯GL,1 is a sum of contributions from each
window:

var(cid:0)∆ ¯GL,1(cid:1) = var(cid:0)kBT ln ¯F1,2(cid:1) + var(cid:0)kBT ln ¯FL,L−1(cid:1)

L−1

var(cid:18)kBT ln(cid:18) ¯Fi,i+1

¯Fi,i−1(cid:19)(cid:19) .

+

Xi=2

(31)

Using (30) and substituting Ni = N/L, we ﬁnd that, as
long as N/L ≥ Nmin and L ≥ Lmin,

(kBT )2 Kmin

N ≤ var(cid:0)∆ ¯GL,1(cid:1) ≤ (kBT )2 Kmax

N

We thus see that the variance is independent of L.

.

(32)

Eigenvector Method for Umbrella Sampling

8

C. The Low Temperature Limit

To understand the beneﬁts of umbrella sampling, we
must study its performance when free energy barriers are
large in comparison with the temperature, i.e., the low
temperature limit (kBT → 0). In this limit, the cost of
direct sampling increases exponentially with 1/T , while,
as we show, the cost of umbrella sampling increases only
algebraically. A formal discussion is given in a separate
publication;31 here, we present a simple plausibility ar-
gument.

Owing to the free energy barriers, the assumption of
diﬀusive dynamics in each window no longer holds. In-
stead, we expect a form typical of reaction rate theories
in each window. We deﬁne ∆Wi as the maximum diﬀer-
ence in the PMF in window i:

FIG. 2. The scaling of umbrella sampling error with number
of windows on a ﬂat potential. A Brownian particle on a
ﬂat, one-dimensional potential was simulated for 480 identical
runs, and the free energy diﬀerence between the ﬁrst and last
windows was calculated, as described in the text. Here, the
mean square error from the exact result is plotted against
the number of windows. The lines show the scaling in error
predicted by the L−1 and L0 scalings. Fitting the data on a
log-log scale gives a scaling exponent of −0.026 ± 0.028.

To verify that this conclusion carries over to harmonic
bias potentials, we performed multiple umbrella sampling
calculations for a Brownian particle on a ﬂat potential
on the interval [0, 1] with a stepsize of 1.0 × 10−6 and
kBT = 1.0 using Gaussian bias functions with a standard
deviation of 1/L. The number of windows was varied
from L = 10 to 46 in steps of 2. For each value, a total of
107 steps were distributed equally in the windows and the
US calculation was repeated 480 times. We then calcu-
lated the mean square error of the free energy diﬀerence
between the ﬁrst and last window over the 480 replicates
and determined how the mean square error scaled with
L. An L−1 scaling would predict that mean square er-
ror would decrease inversely with the number of windows
used. By contrast, the data plotted in Figure 2 support
a scaling of L0, consistent with (32).

It is worth noting that the inverse scaling with total
cost N in (32) is exactly the scaling one would expect
for the variance of an estimate of the free energy diﬀer-
ence ∆ ¯GL,1 constructed from a molecular dynamics tra-
jectory of length N . Because US and direct simulations
of comparable total numbers of steps require comparable
computational eﬀort (ignoring the overhead associated
with combining the simulation data, which is typically
small in comparison with the computational cost of the
sampling), the beneﬁts of US must be encoded in the
constants Kmin and Kmax. A dramatic demonstration of
this observation is the purpose of the next subsection.

∆Wi = max

{q: ψi(q)>0} {W (q)} − min

{q: ψi(q)>0} {W (q)} . (33)

Assumption VI.5′. We now replace the upper and
lower bounds in (30) by the upper bound

var(cid:18)ln(cid:18) ¯Fi,i+1

¯Fi,i−1(cid:19)(cid:19) ≤

K

kBT (cid:19)
kBT NiL2 exp(cid:18) ∆Wi

(34)

for i = 2, 3, . . . , L − 1 with analogous replacements for
i = 1 and i = L, as long as Ni ≥ Nmin and L ≥ Lmin.
The constant K here is assumed to be independent of
temperature. This bound captures the diﬀusion limited
sampling assumption when L is very large, but is more
detailed than (30) in that it captures (crudely) the in-
creasing diﬃculty of the sampling problem as the tem-
perature decreases with all other parameters held ﬁxed.
Under reasonable additional assumptions on the under-
lying potential, the bias functions ψi, and the sampling
scheme, one can rigorously establish an asymptotic (large
Ni) bound of the form in (34).31

Substituting this new bound into (31), we ﬁnd that, if

L ≥ Lmin and N/L ≥ Nmin, then
Xi=1

var(cid:0)∆ ¯GL,1(cid:1) ≤

KkBT
N L

L

exp(cid:18) ∆Wi
kBT (cid:19) .

(35)

As the temperature decreases, we choose to increase L
such that ∆Wi/kBT is bounded above. This can be
achieved by scaling L linearly with 1/T :
if the deriva-
tive of the PMF is bounded (in absolute value) by W ′
max,
choosing L so that

W ′
max
ΩkBT

L ≥

(36)

ensures that ∆Wi/kBT is bounded by Ω (since we have
assumed that the argument of W is in [0, 1]). On the
other hand, our assumption that the length of {q : ψi >
0} (Assumption VI.2) does not exceed γ/L implies that

∆Wi
kBT ≤

W ′
maxγ
kBT L

.

(37)

Eigenvector Method for Umbrella Sampling

Consequently, as long as (36) holds,

We furthermore deﬁne the function

∆Wi
kBT ≤ Ωγ.

Finally, substituting this result into (35) we ﬁnd that if
L ≥ Lmin and N/L ≥ Nmin, then
KkBT exp(Ωγ)
var(cid:0)∆ ¯GL,1(cid:1) ≤

KLkBT exp(Ωγ)

With the best possible (smallest) choice of L allowed by
(36), this bound becomes

Nmin

≤

N

.

vi(x) =(cid:0)ψ∗

so that

1(x), . . . , ψ∗

L(x), g∗

1,i(x), g∗

2,i(x)(cid:1)

¯vi =

1
Ni

Ni−1

Xt=0

vi(X i

t ) =(cid:0) ¯Fi1, . . . , ¯FiL, ¯g∗

1,i, ¯g∗

2,i(cid:1) ,

where we remind the reader that, for each i, the process
X i

t samples the biased distribution πi. Deﬁne

¯v = (¯v1, . . . , ¯vL),

9

(39)

var(cid:0)∆ ¯GL,1(cid:1) ≤

KW ′

max exp(Ωγ)
ΩNmin

.

(38)

and let

The remarkable feature of the bound in (38) is that it is
independent of T . This does not mean that the cost to
achieve a ﬁxed accuracy is independent of T. However, it
does imply that as the temperature is decreased we do
not have to increase Nmin to maintain a ﬁxed accuracy.
Expression (36) and the fact that Ni ≥ Nmin together
imply that the computational cost of obtaining an ac-
curate estimate of ∆ ¯GL,1 by US increases algebraically
with (kBT )−1. That scaling is to be compared to ex-
ponential in (kBT )−1 to achieve the same accuracy by
direct simulation.

VII. ANALYSIS OF THE ERROR OF EMUS

In this section, we study the error of EMUS in full gen-
erality, without imposing the simplifying assumptions of
the previous section. Our main results are a central limit
theorem for EMUS (Theorem VII.4 below) and an eas-
ily computed, practical error estimator which reveals the
contributions of the diﬀerent windows to the total error.
These results may be used to compare the eﬃciency of
EMUS and other methods and to study how the eﬃciency
of EMUS depends on parameters such as the number of
samples allocated to each window.

A. A Central Limit Theorem for EMUS

hvi = (hv1i1 , . . . ,hvLiL)

denote the corresponding vector of exact averages. The
EMUS estimator takes the form B(¯v), where for a free-
energy diﬀerence,

B(¯v) = −kT log PL
PL

i=1 zi( ¯F )¯g∗
i=1 zi( ¯F )¯g∗

2,i! ,

1,i

and for an ensemble average,

(40)

i=1 zi( ¯F )¯g∗
i=1 zi( ¯F )¯g∗

2,i

1,i

B(¯v) = PL
PL

.

(41)

We now proceed with the error analysis. First, we
characterize the error of the sample means over the bi-
ased distributions. As discussed by Frenkel and Smit15
(Appendix D), the variance of a sample mean may be ex-
panded in terms of the integrated autocovariance of the
process. We deﬁne the autocovariance function of vi(X i
t )
to be

0) − hviii)(vi(X i

Ci(t) =(cid:10)(vi(X i

t ) − hviii)T(cid:11)i ,
where T denotes a vector transpose, and here the outer
h. . .ii denotes the exact average not only over X i
0 sampled
from πi but also subsequent points of the sequence X i
t .
Note that Ci(t) is a (L + 2) × (L + 2) matrix. We deﬁne
the integrated autocovariance to be

Before developing the error analysis, we deﬁne a sin-
gle notation for EMUS which incorporates both the case
of a free-energy diﬀerence and the case of an ensemble
average. In either case, one must compute ¯F and also
¯g∗
1,i and ¯g∗
2,i for two real valued functions g1 and g2. To
compute a free energy diﬀerence, we choose based on (3)

g1 = 1S1 and g2 = 1S2.

To compute an ensemble average hgi, we choose based on
(14)

g1 = g and g2 = 1.

Σi =

∞

Xt=−∞

Ci(t).

(42)

The integrated autocovariance is the leading order coef-
ﬁcient in an expansion of the covariance ¯vi (see Frenkel
and Smit15 (D.1.3)):

cov(¯vi) =

Σi
Ni

+ o(cid:18) 1

Ni(cid:19) ,

(43)

where o(1/Ni) denotes terms that go to zero faster than
Ni (i.e., Nio(1/Ni) → 0).

Eigenvector Method for Umbrella Sampling

10

Under certain conditions on the process X i

t , one can
strengthen the expansion of the covariance (43) to a cen-
tral limit theorem (CLT) for ¯vi. We expect such a CLT
to hold for most problems and most sampling methods
in computational statistical physics. However, to avoid a
lengthy and technical digression, we simply take the CLT
as an assumption; we justify this assumption in more de-
tail in another work,31 and we refer to Leli`evre et al.32
(Section 2.3.1.2) for a general discussion of the CLT in
the context of computational statistical physics.

Assumption VII.1 (Central Limit Theorem for ¯vi). We
assume that

pNi (¯vi − hvii) d→ N (0, Σi)

(44)
where Σi ∈ R(L+2)×(L+2) is the integrated autocovariance
matrix deﬁned in (42). The symbol d→ denotes conver-
gence in distribution as Ni → ∞. Notice that when the
elements of the sequence X i
t are independent and drawn
from πi then Σi = h(vi − hviii)(vi − hviii)Tii /Ni. More
generally, samples are correlated, so Σi includes a factor
that accounts for the time to decorrelate.

Having characterized the errors in the sample means,
we now study how these errors propagate through the
EMUS algorithm. Our goal is to prove a CLT for EMUS.
We accomplish this using the delta method.

Lemma VII.2 (The Delta Method; Proposition 6.2 of
Bilodeau and Brenner33). Let θN be a sequence of ran-
dom variables taking values in Rd. Assume that a central
limit theorem holds for θN with mean µ ∈ Rd and asymp-
totic covariance matrix Σ ∈ Rd×d; that is, assume

√N (θN − µ)

d

→ N(0, Σ).

Let Φ : Rd → R be a function diﬀerentiable at µ. Then
we have the central limit theorem

√N (Φ (θN ) − Φ (µ)) d→ N (0,∇ΦT (µ) Σ∇Φ (µ))

for the sequence of random variables Φ(θN ).

To motivate the delta method, we observe that if X
has distribution N(µ, Σ), then ∇Φ(µ)TX has distribu-
tion N (Φ(µ),∇ΦT (µ) Σ∇Φ (µ)). That is, according to
the delta method, the asymptotic distribution of Φ(X) is
the linearization of Φ at µ applied to the asymptotic dis-
tribution of X. Thus, one may regard the delta method
as a rigorous version of the standard error propagation
formula based on linearization.

We prove the CLT for EMUS by applying the delta
method with ¯v taking the place of θN and with the func-
tion B taking the place of Φ. We require the following
assumptions in addition to Assumption VII.1.

Assumption VII.3. We assume:

1. The proportion of the total number of samples
drawn from each window is constant in the limit
as N → ∞; that is,
lim
N→∞

Ni/N = κi.

(45)

2. Sampling in diﬀerent windows is independent; that

is, ¯vi is independent of ¯vj when j 6= i.

3. The biasing functions ψi are chosen so that F is

irreducible; see Section III B.

We now give the CLT for EMUS.

Theorem VII.4 (Central Limit Theorem for EMUS).
Let Assumptions VII.1 and VII.3 hold. Let

∂B
∂¯vi

=  ∂B

∂ ¯Fi1

, . . . ,

∂B
∂ ¯FiL

,

∂B
∂¯g∗
1,i

,

∂B
∂¯g∗

2,i! ∈ RL+2

denote the partial derivative of B with respect to ¯vi, eval-
uated at v. Under the assumptions stated above,

√N (B (¯v) − B (hvi)) d→ N(0, σ2),

where

σ2 =

L

Xi=1

1

κi (cid:18) ∂B

∂vi

T

Σi

∂B

∂vi(cid:19) .

(46)

We refer to σ2 as the asymptotic variance of EMUS.

Proof. First, we write down a central limit theorem for
(¯v1, . . . , ¯vL). We have that

√N (¯vi − hviii) d→ N(0, κ−1

i Σi)

(47)

by Assumption VII.1 and (45). Since the sampling in
diﬀerent windows is assumed to be independent, (47) im-
plies

√N (¯v − hvi) d→ N(0, Σ),

where Σ ∈ RL(L+2)×L(L+2) is the block diagonal matrix

Σ1/κ1

0

0
0
...

Σ2/κ2

0
...

0
0

. . .
. . .
Σ3/κ3 . . .
. . .

...

Σ =


.




Second, we verify that B is diﬀerentiable at ¯v. Since F
is assumed to be an irreducible stochastic matrix, z( ¯F ) is
diﬀerentiable at ¯F . We refer to Thiede et al.34 Lemma 3.1
for a complete explanation. It follows from the chain rule
that B is diﬀerentiable at ¯v.

Finally, applying Lemma VII.2 with B playing the role

of Φ and ¯v the role of θN concludes the proof.

The asymptotic variance σ2 appearing in Theo-
rem VII.4 measures the rate at which the error of EMUS
decreases with the number of samples. To make this pre-
cise, we observe that Theorem VII.4 is equivalent to the
following asymptotic result concerning conﬁdence inter-
vals. For every α > 0,

lim
N→∞

P(cid:20)|B(¯v) − B(hvi)| ≤

ασ

√N(cid:21) = erf(cid:18) α

√2(cid:19) ,

(48)

Eigenvector Method for Umbrella Sampling

where P denotes a probability and erf denotes the error
function.

The asymptotic variance is commonly used to measure
the eﬃciency of an estimator. We refer to van der Vaart35
for an explanation and for a discussion of other possibil-
In Section VII B, we explain how the proportion
ities.
κi of samples allocated to each window may be adjusted
to minimize the asymptotic variance of EMUS, thereby
maximizing eﬃciency.

We note that a central limit theorem similar to Theo-
rem VII.4 has been proved for the MBAR estimator by
Gill et al.3 (Proposition 2.2). However, the authors of
this work do not study the dependence of the asymptotic
variance on the parameters, as we do. In fact, the MBAR
estimator is signiﬁcantly more complicated than EMUS,
and its dependence on the number of windows and the
allocation of samples is harder to understand.

Van Koten and Weare31 use a result similar to The-
orem VII.4 to generalize the conclusions of Section VI
to periodic and multi-dimensional reaction coordinates
and to a wider class of observables than free energy dif-
ferences. We show both that the asymptotic variance is
constant in the limit of large L and that the work re-
quired to compute an average to ﬁxed precision increases
only algebraically in the low temperature limit. In ad-
dition, we use recently developed perturbation estimates
for Markov chains34 to quantify the dependence of the
asymptotic variance of EMUS on the degree to which
the bias functions overlap.

∂B
∂ ¯Fij
∂B
∂¯g∗
1,i
∂B
∂¯g∗
2,i

∂B
∂ ¯Fij

∂B
∂¯g∗
1,i
∂B
∂¯g∗
2,i

11

,

age hfi, B is deﬁned by (41), and we have
k − B(¯v)¯1∗
k)

(¯v) = Pk zi( ¯F )(I − ¯F )#

jk(¯g∗

k

Pk zk( ¯F )¯1∗

, and

(¯v) =

(¯v) = −

zi( ¯F )

k

B(¯v)zi( ¯F )

Pk zk( ¯F )¯1∗
Pk zk( ¯F )¯1∗

k

.

2. When EMUS is used to compute a free-energy dif-

ference, B is deﬁned by (40), and we have

S2,k

jk

1∗

(¯v) = kT zi( ¯F ) Pk(I − F )#
Pk zk( ¯F )1∗
−Pk(I − F )#
Pk zk( ¯F )1∗

zi( ¯F )

S2,k

jk

(¯v) = kT

, and

1∗

S1,k

S1,k ! ,

(¯v) = −kT

S1,k

zi( ¯F )

Pk zk( ¯F )¯1∗
Pk zk( ¯F )¯1∗

.

S2,k

3. When EMUS is used to compute the free energy of

a window, B is deﬁned by (11), and we have

∂B
∂ ¯Fij
∂B
∂¯g∗
i,1

(¯v) =

(¯v) =

zi(F )
zk(F )
∂B
∂¯g∗
i,2

(I − F )#

jk, and

(¯v) = 0.

B. Estimating the Asymptotic Variance of EMUS

Our goal in this section is to derive a computable
estimate ¯σ2 of the asymptotic variance σ2, which can
be decomposed to assess the contributions from individ-
ual windows to errors in averages. We recall that for-
mula (46) for σ2 involves partial derivatives of B. Our
estimate ¯σ2 of σ2 requires explicit formulas for these par-
tial derivatives. We provide the appropriate expressions,
both for ensemble averages and for free-energy diﬀer-
ences, in Lemma VII.5. Following the partial derivatives,
we present an algorithm for evaluating ¯σ2 and demon-
strate it for the alanine dipeptide. Finally, we compare
with the output of a procedure from Zhu and Hummer
(ZH)11 in Section VII B 3.

Note that for a free energy diﬀerence between win-
dows, we can simply subtract derivatives for the
corresponding windows.

Proof. We begin by reminding the reader that the out-
put of EMUS is the vector of window normalization con-
stants, z, which depends on the sample mean ¯F . Be-
cause all other averages and, in turn, their derivatives
rely on z, we need to determine the sensitivity of each
element of z to each element of ¯F (i.e., ∂zk/∂ ¯Fij). Since
¯F is a stochastic matrix, some care must be taken in
deﬁning this derivative. We resolve the technical diﬃ-
culties in detail elsewhere; see Van Koten and Weare31
and Thiede et al.34 (Lemma 3.1). Here, to obtain the
derivative ∂zk/∂ ¯Fij , evaluated at ¯F , we perturb around
¯F :

Lemma VII.5. We have the following formulas for
∂B/∂¯vi:

1. When EMUS is used to compute an ensemble aver-

zk( ¯F + εE) =

L

Xi,j=1

∂zk
∂ ¯Fij

( ¯F )Eij ,

(49)

d

dε(cid:12)(cid:12)(cid:12)(cid:12)ε=0

where E is an arbitrary matrix, ε is a scalar, and we as-
sume that the sum ¯F + εE is also a stochastic matrix.
The righthand side follows from the chain rule, eﬀectively

Eigenvector Method for Umbrella Sampling

treating each element of the matrix as a separate argu-
ment to each element z. Then, we employ a relation from
Golub and Meyer12 (Theorem 3.1):

Deﬁning the sequence

zk( ¯F + εE) = z( ¯F )TE(I − ¯F )#ek,

(50)

we ﬁnd that

12

(53)

d

dε(cid:12)(cid:12)(cid:12)(cid:12)ε=0

where # denotes the group inverse, a generalized matrix
inverse similar to the Moore-Penrose inverse. It is deﬁned
as satisfying AA#A = A, A#AA# = A#, AA# = A#A.
We refer to Golub and Meyer12 for further discussion of
the group inverse and an algorithm for computing it. Fi-
nally, we equate (49) and (50) and solve for the derivative
of interest:

∂zk
∂ ¯Fij

( ¯F ) = zi( ¯F )(I − ¯F )#
jk.

(51)

Thus the sensitivity of each element of z to each element
of ¯F can be computed from linear algebra operations.

With (51), we can now compute the derivatives of B.
We derive the formulas for the free-energy diﬀerence ex-
plicitly; the other cases are similar. In this case,

zk( ¯F )¯g∗

2,k!−kT log  L
Xk=1

zk( ¯F )¯g∗

1,k! .

B(¯v) = kT log  L
Xk=1

By the chain rule,

∂
∂¯g∗
1,i

log  L
Xk=1

and

1,k! =

PL
1,k! = PL
PL

∂
∂ ¯Fij

log  L
Xk=1

zk( ¯F )¯g∗

k=1

∂zk
∂ ¯Fij

( ¯F )¯g∗
1,k
k=1 zk( ¯F )¯g∗

1,k

.

The stated result follows by substituting g1 = 1S1, g2 =
1S2, and the expression in (51) for ∂zk/∂ ¯Fij.

1. Computational Procedure

We now provide a practical procedure that uses the
derivatives above to estimate σ2 from trajectories that
sample the distributions πi. For clarity, we assume that
the system is equilibrated (i.e., X i
0 has distribution πi,
so that the process X i
t is stationary) throughout this sec-
tion.

We begin by rewriting (46) as

ζ i
t =

t ) − hviii(cid:1)

∂B

∂vi(cid:12)(cid:12)(cid:12)(cid:12)hvi ·(cid:0)vi(X i
Xt=−∞(cid:10)ζ i

i =

χ2

∞

t ζ i

0(cid:11)

which is the integrated autocovariance of ζ i
t .

We thus propose the following algorithm, given simu-

lation data:

1. Compute ¯v.
2. Compute z( ¯F ) and (I − ¯F )# using the algorithm

of Golub and Meyer.12

3. Evaluate ∂B/∂vi at ¯v using the formulas in

Lemma VII.5.

4. Compute

¯ζ i
t =

· (vi(X i

t ) − ¯vi).

∂B

∂vi(cid:12)(cid:12)(cid:12)(cid:12)¯vi

5. Compute an estimate ¯χ2

i of the integrated autoco-
t using an algorithm such as ACOR.36

variance of ¯ζ i

¯σ2 =

¯χ2
i
κi

.

L

Xi=1

(54)

Since ¯F , ¯v, and z are all computed in the process of ob-
taining the EMUS averages, estimating ¯σ2 only requires
one additional pass over the simulation data. This ad-
ditional cost is insigniﬁcant compared with that of com-
puting the trajectories.

Both (46) and its approximation (54) decompose the
asymptotic variance of EMUS into a sum of contributions
from each window. By comparing the sizes of terms in
the sum, we can determine the degrees to which diﬀerent
windows contribute to the error. In principle, this infor-
mation can be used to guide modiﬁcation of the parame-
ters of the simulation to improve eﬃciency. For instance,
one might adjust the proportion of samples allocated to
each window, κi, to minimize the asymptotic variance.
From (52), the asymptotic variance σ2 is minimized when
κi ∝ χi (see Zhu and Hummer11 (42)). Consequently,
we can deﬁne the relative importance of window i as

zk( ¯F )¯g∗

zi( ¯F )

k=1 zk( ¯F )¯g∗

1,k

,

6. Compute the estimate of σ2:

σ2 =

χ2
i
κi

L

Xi=1

where

χ2

i =

=

∂B
∂vi

∂B
∂vi

T

Σi

∂B
∂vi

T  ∞
Xt=−∞

Ci(t)! ∂B

∂vi

.

(52)

µi = L

,

(55)

χi
k=1 χi

PL

where the normalization is chosen so that µi = 1, regard-
less of L, if all windows have the same importance. The
relative importance represents how many samples would
be allocated to a window to optimally estimate a speciﬁc
observable, compared to a uniform distribution over all
umbrellas.

Eigenvector Method for Umbrella Sampling

13

average. To illustrate that this is the case numerically,
we show the log importances for the free energy diﬀerence
between a window in the C7 axial basin and one located
on TS1 in Figure 4B. Compared to Figure 4A, the im-
portances are higher in the C7 axial basin and lower in
the C7 equatorial basin, which highlights that the im-
portances depend on the average computed and do not
simply mirror the free energy.
In Figures 4C and 4D,
we plot the importances for estimating the window free
energy (not the free energy diﬀerence) of the window on
TS1 and the window in the C7 axial basin, respectively.
We note that the importances in the C7 equatorial basin
are higher in Figures 4C and 4D than in 4B. This sug-
gests that when the free energy diﬀerence between the
two windows is considered, there is some cancellation of
the errors arising in the C7 equatorial basin.

3. Comparison with Other Algorithms for Determining
Error Contributions

Zhu and Hummer11 proposed an algorithm for deter-
mining window free energies by calculating the mean re-
straining forces for each window and using thermody-
namic integration to estimate free energy diﬀerences be-
tween adjacent windows. These are combined using least
squares to calculate window free energies. Like EMUS,
this algorithm allows one to construct error estimates
that can be decomposed into contributions from individ-
ual windows. The authors give an expression for the error
in the free energy of one window. This expression can be
easily extended to the free energy diﬀerence between two
windows, giving

var (∆Gji) =

L

Xk

var  D
Xα

α! ,
(cjkα − cikα) ¯f k

(56)

where ¯f k
α is the average force exerted by the bias function
for window k in the α-th dimension. The constants cikα
and cjkα are deﬁned in Appendix A of Zhu and Hum-
mer11. The authors propose that these error estimates
are applicable to WHAM and other umbrella sampling
algorithms.

Both (46) and (56) are sums of contributions from in-
dividual windows. Using the formalism introduced in
Section VII B, we deﬁne the process

ζ k,ZH
t

=

D

Xα

(cjkα − cikα)(cid:0)f k

α(cid:0)X k

t(cid:1) −(cid:10)f k

α,n(cid:11)(cid:1) ,

(57)

i

as the integrated autocovariance of ζ k,ZH

and χZH
. This
allows us to deﬁne importances for the Zhu and Hummer
algorithm analogously to those for EMUS (see (55)).

t

We applied the ZH error analysis

to the two-
dimensional umbrella sampling data used in Section
VII B and calculated the importances for the same free
energy diﬀerence as in Figure 3 (Figure 5A). Rather than

FIG. 3. Potential of mean force obtained from US with biases
on the φ and ψ dihedral angles. Major basins and barriers
on pathways connecting them are indicated. The scale bar
indicates PMF values in kcal/mol, and the contour spacing
is 2 kBT . The surface is constructed from simulation data
accumulated in histograms with 100 bins in each collective
variable. See text for simulation details.

2. Numerical Results

To study the behavior of these estimates, we performed
a two-dimensional umbrella sampling calculation with re-
straints on the φ and ψ dihedral angles of the alanine
dipeptide. Parameters were the same as in the one-
dimensional calculation above, with the addition of 20
bias functions in the ψ dihedral with the same force con-
stant, creating a grid of 400 windows. Each window was
equilibrated for 40 ps and sampled for a further 150 ps,
with the collective variable values output every 10 fs.

In Figures 3 and 4A, we plot the two-dimensional PMF
from EMUS and the importances for the free energy dif-
ference between two windows located at the C7 equa-
torial and C7 axial conﬁgurations. Comparison shows
that the importances are high for windows on low free
energy pathways between the two windows of interest.
Two such pathways exist. In the representation in Fig-
ure 3, one proceeds up and to the left of the C7 equatorial
basin and then (via the periodic boundaries) enters the
C7 axial basin through transition state 1 (TS1 in Fig-
ure 3). The other pathway proceeds down then right
through transition state 2 (TS2 in Figure 3). Of these
two pathways, the ﬁrst has a lower free energy barrier.
We observe that the EMUS importances are larger for
windows located on this pathway. In contrast, windows
oﬀ these pathways in regions with high free energies are
given very low importances.

We expect the importances to depend on the computed

Eigenvector Method for Umbrella Sampling

14

FIG. 4. EMUS relative Importances. (A) Relative importances for the free energy diﬀerence between windows in the C7 axial
and C7 equatorial basins. The window in the C7 equatorial basin is centered at (–81,81), and the window in the C7 axial
basin at (63,–63). (B) Window importances for the free energy diﬀerence between windows in the C7 axial basin and at TS1.
Windows are centered at (63,–63) and (135,−117), respectively. (C) Importances for the free energy of the window at TS1.
(D) Importances for the window in the C7 axial basin.

falling along the low free energy pathways, as for EMUS,
the ZH importances mirror the autocorrelation times
(Figure 5B). This indicates that windows have large ZH
importances if they have large ﬂuctuations in free energy.
We thus see that diﬀerent algorithms emphasize diﬀerent
windows in US. We can understand the behaviors of these
two algorithms by considering (53) and (57). The factor
∂B/∂vi in (53) depends explicitly on the normalization
constant for each window (see Lemma VII.5). By con-
trast, the factor (cjkα − cikα) in (57) depends only on
the relative positions of the windows and not on their
free energies.

VIII. CONCLUSIONS

The success of an umbrella sampling simulation de-
pends on the choice of windows (i.e., how the system is
biased) and the estimator used to determine the normal-
ization constants of the windows from trajectory data.
Here, we show that the normalization constants can be
obtained from an eigenvector of a stochastic matrix. This
eigenvector method for umbrella sampling (EMUS) can
be viewed as the ﬁrst step in an implementation of the
MBAR estimator.
In our experience, this ﬁrst step is

Eigenvector Method for Umbrella Sampling

15

FIG. 5. Comparison to Zhu and Hummer. (A) ZH estimates for the relative importances for the free energy diﬀerence between
windows in the C7 axial and C7 equatorial basins. Compare with Figure 4A. (B) Autocorrelation times of the trajectory ζ k,ZH
in each window. The largest value observed is 3 ps, but the scale is limited to 1 ps for visual clarity.

t

nearly converged, and machine precision is reached in
only a few iterations. Moreover, each iteration yields a
consistent estimate. Most importantly, error analysis is
considerably easier for EMUS than MBAR because the
elements of the stochastic matrix do not depend on the
normalization constants.

Within this framework, we revisited a common scaling
argument for justifying umbrella sampling and showed
that once the number of windows becomes suﬃciently
large, the scheme does not beneﬁt from the addition of
more windows (i.e., the variance is not further reduced
for a ﬁxed computational eﬀort). We show that an alter-
native scaling regime in which temperature decreases (or,
equivalently, free-energy barrier heights increase) as the
number of windows increases best demonstrates the po-
tential beneﬁts of the umbrella sampling strategy; in that
regime the eﬃciency improvement over direct simulation
is exponential in the (inverse) temperature.

Our main theoretical result is a central limit theo-
rem for the statistical averages obtained from EMUS.
This result relies on the delta method, which we use
to characterize the propagation of the asymptotic error
through the solution of a stochastic matrix eigenproblem.
The central limit theorem provides an expression for the
asymptotic variance of the averages of interest. It is a
sum of contributions from individual windows, and we
use it to develop a prescription for estimating the relative
importances of windows for averages from the trajectory
data. For free energy diﬀerences of states of the ala-
nine dipeptide, we ﬁnd numerically that the importances
are largest for low-free energy pathways that connect the
speciﬁc states of interest. These results suggest that the

importances could serve as the basis for adaptive schemes
that focus computational eﬀort on the windows of most
importance. Even more interesting would be to adjust
the bias functions as the simulation progresses. How best
to do this remains an open area of investigation.

Appendix A: Consistency of Iterative EMUS

Here, we prove that for ﬁxed ﬁnite m, zm is a consis-
tent estimator of the vector of normalization constants z.
With the initial guess z0 = n, the result, z1, of the ﬁrst
iteration is the EMUS estimator. We now show that z2
is also consistent in the sense that if the trajectory aver-
ages deﬁning ¯F (w) converge then z2 converges to z. Be-
cause the various sequences in question are sequences of
random variables one must specify what is meant by con-
vergence. The argument below applies when convergence
refers either to convergence in probability or convergence
with probability one (almost sure convergence) as long as
the notion of convergence is consistent throughout. Con-
sistency of zm follows by induction on m using a similar
argument.

For any positive vector w, we deﬁne

wj
nj

,

uk = Yj6=k

and we write

¯Fij (w) =

1
Ni

Ni−1

Xt=0

hij (u, x),

Eigenvector Method for Umbrella Sampling

16

where

hij(u, x) =

We then observe that

.

uiψj(x)

Pk ukψk(x)

ui

∂uk hij(u, x) =( 1
− 1
Because hij(u, x) ≤ ui/uj,

ui

hij(u, x)[1 − hii(u, x)],
hij (u, x)hik(u, x),

if k = i
if k 6= i.

.

|∂uk hij(u, x)| ≤

1
uj

max(cid:26)1,

ui

uk(cid:27) .

Therefore,

|hij(˜u, x) − hij(u, x)| ≤ γ(u, ˜u),

(A1)

for a continuous function γ deﬁned for positive vectors u
and ˜u and such that γ(u, u) = 0 for any u. The function
γ(u, ˜u) must explode when the entries of u or w approach
0. Now deﬁne

z1
j
nj

uk = Yj6=k

and ˜uk = Yj6=k

zj
nj

,

where z is the exact vector of normalization constants.
By (A1), we have

| ¯Fij (z1) − ¯Fij(z)| ≤ γ(u, ˜u).

(A2)

As the number of samples N increases, ¯F (z) converges
to F (z). Moreover, since z1 is the EMUS estimate of z,
z1 converges to z. Therefore, u converges to ˜u, and (A2)
implies that ¯Fij (z1) converges to Fij (z). Finally, since
the function mapping an irreducible, stochastic matrix
to its invariant vector is continuous, it follows that z2
converges to the invariant vector of F (z), which is z.
This veriﬁes the consistency of z2.

ACKNOWLEDGMENTS

This research was supported by National Institutes of
Health (NIH) Grant Number 5 R01 GM109455-02. We
wish to thank Jonathan Mattingly, Jeremy Tempkin, and
Charlie Matthews for helpful discussions.

1G. M. Torrie and J. P. Valleau, Journal of Computational Physics
23, 187 (1977).
2Y. Vardi, The Annals of Statistics, 178(1985).
3R. D. Gill, Y. Vardi, and J. A. Wellner, Ann. Statist. 16, 1069
(09 1988), http://dx.doi.org/10.1214/aos/1176350948 .
4S. Kumar, D. Bouzida, R. H. Swendsen, P. A. Kollman, and
J. M. Rosenberg, Journal of Computational Chemistry 13, 1011
(1992).
5M. R. Shirts and J. D. Chodera, The Journal of Chemical Physics
129, 124105 (2008).

6E. Rosta and G. Hummer, Journal of Chemical Theory and Com-
putation 11, 276 (2014).
7A. S. Mey, H. Wu, and F. No´e, Physical Review X 4, 041018
(2014).
8Z. Tan, E. Gallicchio, M. Lapelosa, and R. M. Levy, The Journal
of Chemical Physics 136, 144102 (2012).
9T. Leli`evre, G. Stoltz, and M. Rousset, Free Energy Computa-
tions: A Mathematical Perspective (World Scientiﬁc, 2010).

10D. D. Minh and J. D. Chodera, The Journal of Chemical Physics

131, 134110 (2009).

11F. Zhu and G. Hummer, Journal of Computational Chemistry

33, 453 (2012).

12G. H. Golub and C. D. Meyer, Jr, SIAM Journal on Algebraic

Discrete Methods 7, 273 (1986).

13H. Schneider, Linear Algebra and its applications 18, 139 (1977).
14Z. Tan, Journal of the American Statistical Association 99, 1027

(2004).

15D. Frenkel and B. Smit, Understanding Molecular Simulation:
From Algorithms to Applications, Vol. 1 (Academic press, 2001).

16C. J. Geyer, Statistical Science, 473(1992).
17H. Paliwal and M. R. Shirts, Journal of Chemical Theory and

Computation 9, 4700 (2013).

18M. R. Shirts, D. L. Mobley, and J. D. Chodera, Ann Rep Comput

Chem 3, 41 (2007).

19M. J. Abraham, T. Murtola, R. Schulz, S. P´all, J. C. Smith,

B. Hess, and E. Lindahl, SoftwareX 1, 19 (2015).

20G. A. Tribello, M. Bonomi, D. Branduardi, C. Camilloni, and
G. Bussi, Computer Physics Communications 185, 604 (2014).
21A. D. MacKerell, N. Banavali, and N. Foloppe, Biopolymers 56,

257 (2000).

22J.-P. Ryckaert, G. Ciccotti, and H. J. Berendsen, Journal of Com-

putational Physics 23, 327 (1977).

23A.

Grossﬁeld,
analysis

“WHAM:
method

the
(version

weighted
2.0.9),”

his-
(2013),

togram
http://membrane.urmc.rochester.edu/content/wham.

24E. S¨uli and D. F. Mayers, An introduction to numerical analysis

(Cambridge university press, 2003).

25D. Chandler, Introduction to Modern Statistical Mechanics, by
David Chandler, pp. 288. Foreword by David Chandler. Ox-
ford University Press, Sep 1987. ISBN-10: 0195042778. ISBN-13:
9780195042771 1 (1987).

26C. Chipot and A. Pohorille, Free Energy Calculations (Springer,

2007).

27J. van Duijneveldt and D. Frenkel, The Journal of Chemical

Physics 96, 4655 (1992).

28J. Comer, J. C. Gumbart, J. Hnin, T. Lelivre, A. Pohorille,
and C. Chipot, The Journal of Physical Chemistry B 119, 1129
(2014).

29P. Virnau and M. M¨uller, The Journal of Chemical Physics 120,

10925 (2004).

30F. P. Kelly, Reversibility and stochastic networks (Cambridge

University Press, 2011).

31B. Van Koten and J. Weare, in preparation.
32T. Leli`evre, G. Stoltz, and M. Rousset, Free energy computations
a mathematical perspective (Imperial College Press, Hackensack,
N.J., 2010) p. 458.

33M. Bilodeau and D. Brenner, Theory of Multivariate Statistics

(Springer Science & Business Media, 2008).

34E. Thiede, B. Van Koten, and J. Weare, SIAM Journal on Matrix

Analysis and Applications 36, 917 (2015).

35A. W. van der Vaart, Asymptotic Statistics, Cambridge series on
statistical and probabilistic mathematics. (Cambridge University
Press, Cambridge ; New York, 1998) p. 443.
and J. Goodman,
https://pypi.python.org/pypi/acor/1.1.1.

36D. Foreman-Mackey

“ACOR 1.1.1,”

