6
1
0
2

 
r
a

 

M
9
1

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
5
4
0
6
0

.

3
0
6
1
:
v
i
X
r
a

Non-standard conditionally speciﬁed models for

non-ignorable missing data ∗

Alexander M. Franks, Edoardo M. Airoldi, Donald B. Rubin

Department of Statistics

Harvard University, Cambridge, MA 02138, USA

∗Alexander M. Franks is a Moore-Sloan Data Science Fellow at the University of Washington and a gradu-

ate of the Department of Statistics at Harvard University (afranks@post.harvard.edu) Edoardo M. Airoldi is

an Associate Professor of Statistics at Harvard University (airoldi@fas.harvard.edu). Donald B. Rubin is the

John L. Loeb Professor of Statistics at Harvard University (dbrubin@fas.harvard.edu) This work was par-

tially supported by the National Science Foundation under grants CAREER IIS-1149662 and IIS-1409177,

and by the Oﬃce of Naval Research under grant YIP N00014-14-1-0485. Edoardo M. Airoldi is an Alfred

Sloan Research Fellow, and a Shutzer Fellow at the Radcliﬀe Institute for Advanced Studies. The authors

are grateful to Dr. Shahab Jolani (Department of Methodology and Statistics, Faculty of Social Sciences,

Utrecht University) and Dr. Stef Van Buuren (Netherlands Organisation for Applied Scientiﬁc Research

TNO) for sharing preliminary work and analyses that contributed to the framing of this paper.

Abstract

Data analyses typically rely upon assumptions about missingness mechanisms that

lead to observed versus missing data. When the data are missing not at random,

direct assumptions about the missingness mechanism, and indirect assumptions about

the distributions of observed and missing data, are typically untestable. We explore an

approach, where the joint distribution of observed data and missing data is speciﬁed

through non-standard conditional distributions. In this formulation, which traces back

to a factorization of the joint distribution, apparently proposed by J.W. Tukey, the

modeling assumptions about the conditional factors are either testable or are designed

to allow the incorporation of substantive knowledge about the problem at hand, thereby

oﬀering a possibly realistic portrayal of the data, both missing and observed. We apply

Tukey’s conditional representation to exponential family models, and we propose a

computationally tractable inferential strategy for this class of models. We illustrate

the utility of this approach using high-throughput biological data with missing data

that are not missing at random.

Keywords: Missing data; missing not at random; non-ignorable missing data mecha-

nisms; Tukey’s representation; conditionally speciﬁed models; Bayesian analysis.

Contents

1 Introduction

1.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Basic models for missing data

2.1 The selection factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2 The pattern-mixture factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3 Tukey’s representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Modeling and inference using Tukey’s representation

3.1 Exponential family models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2 Estimation and inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 Numerical examples and an application in biology

4.1

Illustration with semicontinuous data . . . . . . . . . . . . . . . . . . . . . . . . . . .

1

2

2

3

3

4

5

6

7

8

9

4.1.1 Analysis using Tukey’s representation . . . . . . . . . . . . . . . . . . . . . . 11

4.1.2 Remarks on the use of the selection factorization . . . . . . . . . . . . . . . . 14

4.1.3 Remarks on the use of the pattern-mixture factorization . . . . . . . . . . . . 15

4.2 Robustness to Misspeciﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

4.3 Analysis of transcriptomic and proteomic data . . . . . . . . . . . . . . . . . . . . . 18

5 Discussion

24

5.1 Theoretical insights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

5.2 A note on the integrability condition . . . . . . . . . . . . . . . . . . . . . . . . . . . 26

5.3

Inference strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

5.4 Concluding remarks

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29

A Detailed derivations

35

A.1 Derivations From Section 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

A.2 Derivations From Section 4.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

1

Introduction

Missing data are ubiquitous in applications of statistics, including survey sampling, health-

care, public policy and bioinformatics. The eﬀort needed for modeling and analyzing ob-

servations in these situations crucially depends on the mechanism that induces the missing

data as well as on the mode of inference (Little and Rubin, 2014).

Here, we work within the inferential framework outlined in Rubin (2004). The inferential

target of interest is generally a function of observed and missing data. One key concept is

that of a “missing at random” missing data mechanism, for which the conditional probability

distribution of the missingness indicators is a function only of the observed data, and the

related concept of ignorable missing data (Rubin, 1976, 1978; Mealli and Rubin, 2015).

For Bayesian inference, ignorability implies that the posterior distribution of the target is

conditionally independent of the observation indicators, given the observed data, and so

there is no need to specify a model for observation indicators to achieve valid Bayesian or

likelihood-based inference. When the posterior distribution of the inferential target depends

on these indicators, valid Bayesian inference requires specifying a joint model for the data

and the indicators.

With missing data, there are two basic approaches to specify the the joint distribution of

the complete data and missing data indicators given parameters, and the implied likelihood,

as a product of marginal and conditional distributions. The ﬁrst basic approach (Rubin,

1974) is to posit a standard model for the complete data, and then specify an explicit model

that selects observed data from the complete data, called the missingness mechanism (Rubin,

1976). The second basic approach is to specify separate distributions for the observed data

and the missing data, thus eschewing explicit assumptions about the missingness mechanism

(Rubin, 1977; Little, 1993). The fundamental challenge with these two basic approaches is

that some assumptions about the missingness mechanism, whether explicit or implicit, are

1

typically not testable from the observed data. As a result, some literature on inference in

the presence of missing data centers on assessing sensitivity to diﬀerent model speciﬁcations

(Rubin, 1977; Scharfstein et al., 1999; Andrea et al., 2001; Little and Rubin, 2014).

1.1 Contributions

Here, we develop an alternative approach to modeling data possibly missing not at random,

evidentially originally proposed by Tukey and discussed by Hartigan and Rubin, reported by

Holland (1986). The key insight is to represent the joint distribution of the complete data

and missing-data indicators as proportional to factors that involve only observed values and

the missingness mechanism. Assumptions about these factors are either testable, or typically

allow the incorporation of substantive knowledge about the problem at hand, thereby oﬀering

a clear path to eliciting a realistic portrayal of the sources of variation in the data.

In Section 2, we review the two basic model speciﬁcations for missing data, before for-

mally introducing Tukey’s representation. In Section 3, we discuss technical issues involved

when using Tukey’s representation, and provide a full characterization for exponential-family

models. In Section 4, we then illustrate the use of these models on simulated data when the

model is correctly speciﬁed, when the model is incorrectly speciﬁed, and on an application

in biology. In Section 5, we oﬀer theoretical insights and use them to state formal results.

2 Basic models for missing data

Discussion of models using basic factorizations for missing data can be found in a variety of

places, including Glynn et al. (1986) and Molenberghs et al. (2015).

(cid:124)
Throughout, let Y = (Y1, Y2, ..., YN )

(cid:124)
represent the complete data and R = (R1, R2, ..., RN )

represent the response indicators for Y , which are “missing” when Ri=0 and “observed” when

2

Ri = 1. The joint distribution, assuming (Yi, Ri) are i.i.d., is then

(cid:89)

P (Y, R | θ) =

f (Yi, Ri | θ),

where θ is a parameter vector. For simplicity, we focus on the the case without covariates.

i

2.1 The selection factorization

N(cid:89)

The selection approach (Rubin, 1974) factors the joint distribution of (Y, R) as

P (Y, R | θ) =

f (Yi|θY )f (Ri|Yi, θR|Y ),

(1)

i

using the distribution of the complete data, P (Y |θY ), and the missingness mechanism, or
selection function, P (R|Y, θR|Y ), which controls which data are actually observed, where the
parameters (θY , θR|Y ), are functions of θ. Models for f (Yi|θY ) include the normal, with θY
the mean and variance of the normal, or the Bernoulli with θY the probability of success of
the Bernoulli. Typical models for f (Ri|Yi, θR|Y ) include the logistic and probit models (e.g.,
see Gelman et al., 2004).

2.2 The pattern-mixture factorization

The pattern-mixture approach (Rubin, 1977; Little, 1993; Little and Rubin, 2014) is the

alternative basic factorization; the complete data distribution is speciﬁed as a mixture of

observed and missing data components,

N(cid:89)
N(cid:89)

i=1

P (Y, R|θ) =

=

f (Yi|Ri, θY |R)f (Ri|θR)

1(cid:89)

(cid:2)f (Yi|Ri = r, θY |R=r)f (Ri = r|θR)(cid:3)I(Ri=r) ,

(2)

i=1

r=0

3

where (θY |R, θR) are functions of θ, leading to a mixture likelihood, which for a single obser-

vation is

P (Yi | θ) =

1(cid:88)

f (Yi|Ri = r, θY |R=r)f (Ri = r|θR).

r=0

The model for f (Ri|θR) is a Bernoulli distribution with parameter θR. The model for
f (Yi|Ri = 1) is typically chosen to ﬁt the observed data well, whereas the model for
f (Yi|Ri = 0) is commonly chosen to be a location shift or scale change of f (Yi|Ri = 1)

(e.g., see Gelman et al., 2004; Little and Rubin, 2014).

2.3 Tukey’s representation

John W. Tukey, in a discussion of Glynn et al. (1986), suggested an alternative factorization

of the joint distribution for (Y, R) in terms of conditional distributions (recorded in Holland,

1986), which he refers to as the simpliﬁed selection model, with parameters θY |R=1 and θR|Y ,

P (Y, R | θ) ∝ N(cid:89)
with normalizing constant(cid:81)N

i=1

f (Yi | Ri = 1, θY |R=1) ·

f (Ri | Yi, θR|Y )
f (Ri = 1 | Yi, θR|Y )

,

(3)

i=1 f (Ri = 1 | θR|Y , θY |R=1) ensuring integrability. As Holland
notes, a main advantage of this factorization is that it only involves the observed data density,
f (Yi | Ri = 1, θY |R=1), which can be estimated directly, and the missingness mechanism,
f (Ri | Yi, θR|Y ), which can be easy to elicit in the context of a speciﬁc application.

Tukey’s representation can be obtained through a simple application of Brook’s lemma,

which equates the ratio of joint distributions to the ratio of their full conditionals (Brook,

1964; Besag, 1974). Although this lemma is most commonly referenced in the theory of

spatial autoregressive models (Cressie, 2011), its connection to Tukey’s representation is rel-

evant because it immediately reveals some theoretical insights. Importantly, Brook’s Lemma

is only applicable when the so-called positivity condition is satisﬁed (Hammersley and Clif-

4

ford, 1971), which for Tukey’s representation means that

If: P (Ri = r|θ) > 0 and P (Yi = y|θ) > 0
Then: P (Ri = r, Yi = y|θ) > 0

for all pairs of values (r, y). This condition is not trivially satisﬁed in missing data problems.
For instance, Tukey’s representation cannot be applied to models where P (Ri = 1|Yi <
c, θR|Y ) = 0, deterministically, for some cutoﬀ c, as when the complete data model is normal

and the observed data model is truncated normal. Consequently, here we focus on problems
where P (Ri = 1|Yi, θR|Y ) > 0, that is, where the support of the missing data is contained in
the support of the observed data.

In addition, the conditional distributions speciﬁed in Equation 3 must imply an inte-

grable joint density (Besag, 1974). With Tukey’s representation, the integrability condition

constrains the rate at which the tails of the distribution for observed data decrease relative

to the rate at which the odds of a missing value increase. This condition is illustrated in

Section 4.1, and discussed in Section 5.2.

3 Modeling and inference using Tukey’s representation

Let P (Yi = yi | Ri = 1, θYi|Ri=1) be denoted f obs(yi | θY |R), for simplicity. Using Equation 3,
we can write the density for (yi, ri) as

f obs(yi | θY |R)

f (ri=0|yi,θR|Y )
f (ri=1|yi,θR|Y )f obs(yi | θY |R)

f (yi, ri | θR|Y , θY |R) ∝

if ri = 1

if ri = 0

= f (ri = 1 | yi, θR|Y )ri−1f (ri = 0 | yi, θR|Y )1−rif obs(yi | θY |R)

(4)

5

with normalizing constant

Q(θY |R, θR|Y ) =

(cid:18)

1 +

(cid:90) f (ri = 0 | yi, θR|Y )

f (ri = 1 | yi, θR|Y )

(cid:19)−1

,

f obs(yi | θY |R) dyi

(5)

which ensures the integral of Equation 4 over random variables R and Y times Q is unity.

The normalizing constant Q is generally diﬃcult to compute. As a consequence, the

missing data density cannot easily be expressed. Below, we introduce a class of models for

which computation of the normalizing constant is tractable and which also implies simple

distributional forms for the missing data and complete data densities.

3.1 Exponential family models

Assume that the observed data distribution belongs to an exponential family and that the

logit of the missingness mechanism is linear in the suﬃcient statistics of that family, which

is related to the exponential tilt pattern-mixture models introduced by Birmingham et al.
(2003). Formally, let f obs(yi | θY |R=1) be an exponential family distribution with natural
parameter θY |R=1. Then,

f obs(yi | θY |R=1) = h(yi)g(θY |R=1)eT (yi)(cid:48)θY |R=1,

(6)

where g(θY |R=1) is the normalizing constant and T (yi) is the natural suﬃcient statistic,

possibly multivariate. The missingness mechanism

f (ri = 1 | yi, θR|Y ) = logit (T (yi)(cid:48)θR|Y ) =

1

1 + e−θR|Y

(7)

implies that

f (ri = 0 | yi, θR|Y )
f (ri = 1 | yi, θR|Y )

= e−T (yi)θR|Y .

6

Then the normalizing constant Q in Equation 5 can be written as a simple function of the
normalizing constant g(·) in the exponential family formulation of f obs,

Q(θY |R=1, θR|Y ) =

g(θY |R=1 + θR|Y )

g(θY |R=1 + θR|Y ) + g(θY |R=1)

.

(8)

For the class of exponential-logistic models deﬁned by Equations 6–7, the missing data

distribution, as speciﬁed in Equation 9, is from the same exponential family as the observed

data with natural parameter1 θY |R=0 = θY |R=1 + θR|Y ,

f mis(y | θY |R=1, θR|Y ) = h(y)g(θY |R=0)eT (y)(cid:48)θY |R=0.

(9)

Thus, missing data imputation with this class of models is straightforward.

In Tukey’s representation, the main source of uncertainty about any inferential target is

due to the missingness mechanism, and the prior distribution on θR|Y .

3.2 Estimation and inference

The primary estimands of interest are typically functions of the parameters specifying the

complete-data distribution, (θY |R=1, θR|Y ). Because the observed and missing data densities

for exponential-logistic models are exponential families, see Equations 6 and 9, the complete-

data distribution is a mixture of exponential families. Further, analytic expressions for the

normalizing constant Q and the likelihood are available, and thus standard Markov Chain

Monte Carlo methods are applicable (Robert and Casella, 2004).

We take a simple approach to inference via Markov Chain Monte Carlo, which is com-

putationally less demanding than with samplers that explicitly characterize the geometry of

the solution space, as discussed in Section 5. Consider a simple Normal-logistic model as an

1In the exponential-logistic models we consider in this section, the two parameter vectors (θY |R=1, θR|Y )

always have the same dimensionality.

7

illustration, with θR|Y = (β0, β1) = β ∈ R2 with β0 corresponding to the intercept and β1
the rate at which the odds of selection change in T (y) = y:

f (ri = 1 | yi, β) = logit (β0 + β1yi) =(cid:0) 1 + exp{−β0 − β1yi}(cid:1)−1

f obs(yi) = Normal (0, 1).

Rather than specifying a prior distribution on (β0, β1), we specify a prior distribution on Q

and β1, but not on β0. We then invert Equation 8 and solve for β0 to get,

(cid:18) g(η + β1)(1 − Q)

(cid:19)

g(η)

β0(Q, η, β1) = log

.

(10)

In this Normal-logistic example, the standard Normal has natural parameters (η1, η2) =
(0,−1/2), and g(η) = − η2

2 ln(−2η2). Thus, Equation 8 becomes,

− 1

1
4η2

and solving Equation 10 we get

β0 = log

(cid:18)β2

Q(β0, β1) =

,

β2
1
1 − 2eβ0
β2
(cid:19)

1(1 − Q)

.

2

Next, we use this inferential strategy to demonstrate the utility of Tukey’s representation.

4 Numerical examples and an application in biology

In Section 4.1, we explore selection, pattern-mixture, and Tukey representations for missing
data using simulation studies, where f obs(yi | ri = 1, θY |R=1) is a mixture of exponential fam-
ily distributions. In this example, we discuss how both the selection and the pattern mixture

factorizations require diﬃcult modeling choices in practice, and illustrate how Tukey’s rep-

8

(cid:32) K(cid:88)

(cid:33)

(cid:32) M(cid:88)

(cid:33)
pm · δ{γm}(yi)

resentation can be employed with relative ease. In Section 4.2 we explore the robustness

of logistic-exponential family missing-data models when the true generating process cannot

be expressed as a ﬁnite mixture of exponential families. In Section 4.3 we demonstrate the

utility of Tukey’s factorization in an application to biological data (Franks et al., 2014).

4.1 Illustration with semicontinuous data

Let us posit that the complete data observations are drawn from a mixture of discrete

and continuous distributions, which we term a semicontinuous distribution. Speciﬁcally, we

assume that the continuous component is a mixture of normals and that the missing data

mechanism is logistic in the data. Here, we specify the full data generating process using

Tukey’s representation. First, we assume the observed data density has form

f obs(yi | ri = 1, θY |R=1) = α

wk · Normal (yi; µk, σ2
k)

+ (1 − α)

k=1

m=1

where θY |R=1 = (α, w1:k,, µ1:k, σ2

1:k, p1:m, γ1:m), and δ{γ}(y) is the Dirac delta function that is

one when y = γ, and zero elsewhere. The parameter α captures the fraction of observations

attributable to the continuous component, and the parameters p1:M specify the probabilities

for the point masses at M locations γ1:M . Recalling the notation introduced in Section

3.1, for the k-th Normal mixture component, the natural parameter is θY |R=1 = (µk, σ2
k)

and the corresponding suﬃcient statistics is T (yi) = (yi, y2

i ). For the discrete distribution,

θY |R=1 = p1:M and T (yi) = 1 if yi equals one of the M discrete locations, zero otherwise.

The missingness mechanism speciﬁes that the logit of the selection probabilities are

quadratic in yi, but importantly, linear as a function of the suﬃcient statistics of the normal

9

(cid:32) K(cid:88)

µk − 2β1β2 σ2
1 − 2β2 σ2

k

k

,

σ2
k

1 − 2β2 σ2

k

(cid:33)

)

+

f mis(yi | −) = α∗

(cid:32) M(cid:88)

k · Normal (yi;
w∗

(cid:33)
k · δ{γj}(yi)
p∗

k=1

+ (1 − α∗)

components:

f (ri = 1 | yi, θR|Y ) = logit−1(−(β0 + (yi − β1)2β2))

= (cid:0) 1 + exp{β0 + y2

i β2 − 2β1β2yi + β2

1β2}(cid:1)−1

(11)

where θR|Y = (β0, β1, β2). Here, β1 corresponds to the value at which an observation is most

likely to occur, whereas β2 controls how quickly the observation probability decays with the

distance of yi to β1. Non-monotonicity in the probabilities of selection occurs in practice

when extreme values, both small and large, are less likely to be observed.

Using Equation 9, we can see that the missing data distribution for the continuous com-

ponent is also a mixture of Normals. The missing data has density

.

(12)

m=1

This density, with speciﬁcations for the mixture weights α∗ and w∗

1:K, is derived in Appendix

A.1. The observed data and missing data densities specify the complete data as semi-

continuous mixture, from which all relevant estimands can be computed.

In order to simulate data from this complex mixture, we ﬁx K = 3 components for the

continuous portion of the observed data mixture, with parameters set as follows,

µ1:3 = (−2, 0, 3)

w1:3 = (0.3, 0.4, 0.3)

σ1:3 = (1, 1, 1) .

(13)

(14)

(15)

We set α = 0.8, that is, 80% of the observed data comes from the continuous component. We

10

and ﬁx the discrete component of the observed data to be uniform on γ1:9 = {−4,−3, ..., 3, 4},
where M = 9. Finally, for the missingness mechanism, we assume that β2 = .06, β1 = −2

and Q = 0.5, that is, 50% of the complete data is missing. We then use Equation 10 to
derive that β0 ≈ −0.85. The full derivation is provided in Appendix A.1.

The top panel of Figure 1 shows the true missingness mechanism used to simulate data, as

a bold black line. The bottom panel shows a histogram of draws from the observed data as

well as the true observed, missing and complete data densities for the continuous component.

For this simulation study, we will assume that the relevant estimands are the complete data

mean and complete data standard deviation.

4.1.1 Analysis using Tukey’s representation

Here we use Tukey’s representation to estimate the posterior distribution of the complete

data mean and standard deviation, from observations and observation indicators generated

from the model above. In order to do so, we specify prior distributions for the parameters of

the missingness mechanism, β, the fraction of missing data, Q, and a model for the observed

data, f obs.

We posit the following prior distributions on the parameters of the missingness mechanism,

β1 ∼ Normal (−2; 2)
β2 ∼ 0.08 · Beta (3; 1)
Q ∼ Uniform (0, 1)

(16)

These prior distribution are centered around the parameters of the true missingness mecha-

nism, (β1, β2), with some variability to reﬂect a degree of uncertainty. Importantly, neither

β1 nor β2 can be estimated from the observed data, and thus the uncertainty in this prior

speciﬁcation will propagate to the posterior, regardless of the amount of observed data. The

11

Figure 1: Top: the true missingness mechanism (black) and 10 samples from the prior dis-
tribution speciﬁed in Section 4.1.1 (gray). Bottom: the observed data histogram (gray,
N = 10, 000) with observed (red) and missing data (blue) densities for the continuous
component. Ten percent of the observed data comes from a discrete distribution on {-
4,-3,...,0,...,3,4}.

top panel of Figure 1 shows a number of draws from the above prior distributions, in gray.

Next, we posit the following prior distributions for the observed data parameters, θY |R=1:

pi ∼ Dirichlet(1, 1, . . . , 1) for i = 1, . . . , M = 9
µk ∼ Normal(0, 10) for k = 1, . . . , K = 3
wk ∼ Dirichlet(1, 1, 1)
σk ∼ Uniform(0, 2)

(17)

12

−10−50510xPobs0.00.51.0−10−50510Unlike the prior speciﬁcation for the parameters of the missingness mechanism, all of the

observed data parameters are estimable. Thus, the results are only sensitive to the prior

speciﬁcations of θY |R=1 for smaller observed data sample sizes.

Note that by the integrability condition stated in Section 3.1, in order for the continuous

mixture components of the complete data to all have positive variance, it must hold that

β2 < min
k=1...K

1
2σ2
k

. This inequality is due to Equation 33 in Appendix A.1. To ensure the

integrability condition is satisﬁed, we also bound the prior distributions on σ1:3 above by 2,

because 0.08 < 1

8, where 0.08 is the maximum value of β2 under our prior distribution.

We present the simulation results for diﬀerent sample sizes in Figure 2. With a reasonably

informative prior distribution for the missingness mechanism, these results show that the

analysis with the Tukey’s representation accurately recovers the posterior distributions for

the relevant estimands. As the sample size increases, our posterior uncertainty shrinks, but

never disappears. This is because even with perfect knowledge of the observed data density2,

there is no information in the data about the parameters of the missingness mechanism, as

mentioned above. Therefore uncertainty about the missingness mechanism propagates to

the uncertainty about the complete-data quantities, and is the main source of uncertainty

in the posterior when enough data is available. This fact facilitates the sensitivity analysis

about the missingness mechanism when using Tukey’s representation.

Above, we illustrated how data analysis using Tukey’s representation can be applied in a

straightforward manner to a complex data generating process. It does not require any ex-

plicit assumptions about the complete-data distribution; rather it only requires a reasonable

model for the observed data, and plausible assumptions about the selection mechanism. In

contrast, there are often signiﬁcant challenges for data analysis when applying the selection

factorization, or pattern mixture factorization, as we now discuss.

2We simulate this situation by setting the parameters underlying the observed data distribution to their

true values, and we label the corresponding results in Figure 2 with N = ∞.

13

Figure 2: Posterior estimates of the complete-data mean (left), complete-data standard
deviation (middle) and maximum absolute diﬀerence between the true and inferred discrete
probability masses (right). Dashed red line represents the true complete-data value. As
we get more data, estimates of the parameters underlying the observed-data distribution
improve, and the posterior concentrates around the true values. However, information about
the parameters underlying the selection mechanism does not accumulate, thus posterior
uncertainty about the complete-data parameters remains.

4.1.2 Remarks on the use of the selection factorization

Inference under selection factorization models typically require numerical or Monte Carlo

integration of the complete data distribution, and hence is computationally and analytically

more demanding than both pattern mixture models and Tukey’s representation. For in-

stance, often the missing data density does not take on a simple form, and thus missing data

imputation is non-trivial. Finally, it is not obvious that the speciﬁed selection factorization

model will imply a reasonable ﬁt to the this observed data. This is a fundamental challenge

associated with any approach that does not directly model the observed data (Rubin, 2004;

Little and Rubin, 2014; Lunagomez and Airoldi, 2014; Mealli and Rubin, 2015). The focus

on modeling the observed data explicitly, rather than implicitly via assumptions on the data

generating process, is one of the strengths of analyses based on the Tukey’s representation.

Even if we knew the complete data distribution corresponded to a six component mixture

of Normal with discrete masses at certain values, it would still be diﬃcult to specify appropri-

14

ately informed prior distribution for the relevant parameters. Note that in this example, the

parameters of the missing data density are a function of the selection parameters (Equation

12). For this reason, specifying independent prior information for the mixture components

of the complete data density implicitly adds additional prior information about the parame-

ters of the missingness mechanism. By parameterizing the model in terms of complete data

parameters θY , we are combining what we don’t have information about (θY |R=0) with what

we do have information about (θY |R=1) (Holland, 1986).

4.1.3 Remarks on the use of the pattern-mixture factorization

The true data generating process described at the beginning of Section 4.1 is speciﬁed as a

mixture. Thus, at least in principle, the pattern mixture factorization, described in Section

2.2, provides an appropriate way to model the complete data. However, the implied missing

data distribution in this example does not correspond to a simple location or scale change of

the observed data distribution. Thus, as is common with pattern mixture models, the missing

data distribution for this data cannot be represented with a simple between-components

location and scale change (as discussed, e.g., in Daniels and Hogan, 2000).

More generally, when the missing data distribution is complicated, it can be diﬃcult

to propose plausible, scientiﬁcally justiﬁable prior distributions for the parameters of the

missing data density, θY |R=0. Perhaps even more problematic is that these parameters often

imply smoothness and monotonicity of the selection mechanism, which is not explicitly

speciﬁed in analyses that use the pattern-mixture factorization.

4.2 Robustness to Misspeciﬁcation

In Section 4.1.1, we performed the analysis using Tukey’s representation using a family

of models that included the true data generating process.

In practice, ﬁnite mixtures of

15

exponential families can only be expected to approximate the observed data density. Here,

we evaluate the robustness of Tukey’s approach in such cases.

We consider data generated under a simple selection factorization model. We assume

the complete data is a standard normal, i.e. θY = (µ, σ) = (0, 1), and that the selection

mechanism logistic, linear in the data yi. Under this data generating process, the observed

data density is

f obs(yi | ri = 1, θY , θR|Y ) ∝

exp

2σ2

(cid:16) (yi−µ)2

(cid:17)

σ(1 + exp(−β0 − β1yi))

,

which cannot be represented as a ﬁnite mixture of exponential families.

For the analysis, we again assume the goal is to estimate the complete data mean and

standard deviation. We use a mixture of normals to approximate the observed data density

but take the parameters of the logistic missingness mechanism, θR|Y = (β0, β1) to be known

exactly. We estimate the posterior distributions for µ and σ for increasing sample sizes and

for diﬀerent values of the selection parameters, β1. More in detail, in each simulation β0 is

ﬁxed to 1/2, the parameter β1 takes values 1, 2 and 5, and we replicate the analysis for 100

and 1,000 data points. To increase the realism of the data analysis, we ﬁt a mixture model

with 3 components to 100 data points, but we increase the ﬂexibility of the model for the

larger sample size by ﬁtting a mixture model with 5 components to 1,000 data points.

Figure 3 shows box plots of the posterior distributions for the complete data mean (top

panels) and complete data standard deviation (bottom panels) for the diﬀerent values of

β1 and increasing sample sizes. To better appreciate the results, note that, as β1 increases,

the selection probability approaches zero for all yi < β0 and the observed data density

approaches a truncated normal model. Because of the positivity condition, such a model

cannot be estimated using Tukey’s representation. The results in Figure 3 show that even

when the positivity condition holds, in theory, when the selection probabilities are very close

to zero, the complete data estimates (blue dots) become unstable, increasing the bias of the

16

complete data estimates. Thus, in this example, increasing sample size only partially oﬀsets

the failure of inference procedures based only on the observed data.

More generally, instability occurs when the odds of selection are small in regions where

the inferred observed data density is non-zero. This insight can be formalized by examining

the interaction of the observed data density and the selection probabilities in the Equation

24. As a consequence, the ﬁt of the observed data distribution in the tails of the distribution

can be very important for accurately inferring complete data quantities. In practice, caution

is needed when using Tukey’s representation in cases where the observed data sample size is

(a) Complete Data Mean

(b) Complete Data Standard Deviation

Figure 3: Posterior estimates of the complete data mean and complete data standard devi-
ation as a function of sample size and the slope of the missingness mechanism. Blue dots
represent the empirical observed data estimates and the dashed red line corresponds to the
true complete data quantity. As the slope, β1 increases, the observed data density approaches
a truncated normal, for which Tukey’s representation is not applicable.

17

small or where the selection probabilities diverge rapidly.

4.3 Analysis of transcriptomic and proteomic data

In experiments involving measurements of transcriptomic and proteomic data, mRNA tran-

scripts and proteins which occur at low levels are less likely to be observed (Walther and

Mann, 2010; Soon et al., 2013). This makes it challenging to infer normalizing constants

for absolute protein levels (Karpievitch et al., 2012), cluster genes into functionally related

sets (Troyanskaya et al., 2001), infer the degree of coordination between transcription and

translation (Franks et al., 2014), and determine the ratio of dynamic range inﬂation from

transcript to protein levels (Cs´ardi et al., 2014). Here, we demonstrate how data analysis

with Tukey’s representation can be used to investigate some these issues by assessing the

sensitivity of estimands to diﬀerent assumptions about the missingness mechanism.

In this analysis, we use transcriptomic data from Pelechano and P´erez-Ort´ın (2010), with

14% missing data, and protein abundance data from Ghaemmaghami et al. (2003), with 34%

missing data. For illustrative purposes we treat the complete data mean and variance as the

estimands of interest in the analysis.

It is standard to assume that both mRNA and protein levels are log-normally distributed

(Bengtsson et al., 2005; Lu et al., 2007), although this assumption may not be justiﬁed

(Lu and King, 2009; Marko and Weil, 2012). Here, we instead model the observed data

as a mixture of normals and specify a prior distribution for the parameters of the logistic

missingness mechanism. Together these assumptions imply a more ﬂexible prior distribution

over complete data densities.

Further, as noted in Karpievitch et al. (2009), missing values can occur for multiple

reasons, at diﬀerent stages of the data collection process. They ﬁnd that a small fraction of

missing proteomic data collected using mass spectrometry is missing completely at random.

18

Consistent with this, we again generalize the results of Section 3.1 to allow the selection

mechanism to have a logistic form that asymptotes at some value less than one. The observed

data distribution and missingness mechanism together deﬁne the joint distribution:

f obs(yi|ri = 1, β, κ, µk, σk) ∼ K(cid:88)

wkN (yi; µk, σ2
k)

f (ri = 1|yi, θR|Y ) =

k=1

κeβ1yi+β0

1 + eβ1yi+β0

(18)

(19)

with θY |R=1 = (µ, σ, w) and θR|Y = (β0, β1, κ). Here, 0 < (1− κ) < 1 corresponds the fraction
of data that is missing completely at random, and β0 and β1 describe the odds of a missing

value, with β1 parametrizing the rate at which the odds of a missing value change in yi.

Under this model, the implied missing data distribution is

f miss(yi|ri = 0, β, κ, µk, σk) = (1 − κ∗)f obs(yi|ri = 1, µk, σk)

(cid:32) K(cid:88)

+ κ∗

(cid:33)

w∗
kN (yi; µk + β1σ2

k, σ2
k)

(20)

The full derivation of the mixture weights w∗

k and κ∗ is given in Appendix A.2.

k=1

In the analysis of the observations, to poist a good model for f obs in Tukey’s representa-

tion, we found that K = 3 components were enough to approximate the observed data well.

Furthermore, we chose the following prior distributions for Q and θR|Y ,

β1 ∼ Beta(1, 3)
Q ∼ Uniform(0, 1)
κ ∼ 1 − (1 − Q)Beta(2, 1),

κ ≥ Q

(21)

Note that κ must be greater than Q because the selection probabilities cannot be less than

the population fraction of observed data. Draws from this prior distribution are shown in

Figure 4, top-left panel, in grey, around the prior mean, in black. We implemented the

19

sampler for the making inference with this model using STAN (Stan Development Team,

2014).

Figure 4, bottom-left panel, shows the ﬁt to the protein data (Ghaemmaghami et al.,

2003), when β1 is set to its median posterior value. For comparison, the bottom-right panel,

shows the ﬁt of the selection factorization model published in Franks et al. (2014), which

assumes the complete data is distributed according to a lognormal and the missingness

mechanism is logistic, with the mean linear in in yi. The black, red and blue lines, in both

bottom panels, correspond to the estimated densities of complete data, missing data and

observed data, respectively. Lack-of-ﬁt to the observed data is the analysis with the selection

model. Figure 5 shows the corresponding results for the transcript data set (Pelechano and

P´erez-Ort´ın, 2010), where the lack-of-ﬁt using the selection factorization is less pronounced.

Figure 4: Model ﬁt to proteomic data from Ghaemmaghami et al. (2003) data using two
diﬀerent approaches: Tukey’s representation (left) and the selection factorization (right).
The grey lines in the top-left panel represent draws of the selection mechanism from the
prior distribution provided in Equation 21. The black, red and blue lines in the bottom
panels correspond to the estimated densities of complete data, missing data and observed
data, respectively.

In Figures 6–7, we compare the estimated complete data posterior means and standard

deviations for the protein and transcript data sets, respectively, using three diﬀerent miss-

20

Pobs0.00.51.0Protein Abundance051015Pobs0.00.51.0Protein Abundance051015Figure 5: Model ﬁt to mRNA data from Pelechano and P´erez-Ort´ın (2010) data using two
diﬀerent approaches: Tukey’s representation (left) and the selection factorization (right).
The grey lines in the top-left panel represent draws of the selection mechanism from the
prior distribution provided in Equation 21. The black, red and blue lines in the bottom
panels correspond to the estimated densities of complete data, missing data and observed
data, respectively.

ing data models: Tukey’s representation model implied by Equations 18 and 21; a selection

factorization model that assumes log-normality of the complete data and a logistic selection

mechanism (Franks et al., 2014; Cs´ardi et al., 2014); and a missing complete at random

model. For both data sets, the estimates obtained with the MCAR model and with the

selection factorization models bookend the estimates obtained with the model speciﬁed with

Tukey’s representation. Under the selection factorization model, the complete data standard

deviation is large and the mean is small relative to the estimates from Tukey’s factorization.

This is likely because the strong parametric assumptions associated with the selection factor-

ization models overly constrain the ﬁt to the observed data. Table 1 reports exact numerical

estimates of the two estimands of interest, using the three competing models.

Recent published analyses of data using the selection factorization found that transla-

tional regulation widens the dynamic range of protein expression (Franks et al., 2014; Cs´ardi

et al., 2014). One way to quantify the relative dynamic ranges of mRNA and protein is by

21

Pobs0.00.51.0mRNA levels−4−202468Pobs0.00.51.0mRNA levels−505Figure 6: Posterior distributions of the complete data mean (right) and complete data stan-
dard deviation (left) for protein data (Ghaemmaghami et al., 2003). The MCAR estimates
(red) and an estimate assuming normality of the complete data (blue) are shown as vertical
lines for comparison. Under the prior distribution in Equation 21, estimates using the MCAR
and the selection factorization models are at opposite ends of these posterior distributions.

Figure 7: Posterior distributions of the complete data mean (right) and complete data
standard deviation (left) for mRNA data (Pelechano and P´erez-Ort´ın, 2010). The MCAR
estimates (red) and an estimate assuming normality of the complete data (blue) are shown as
vertical lines for comparison. Under the prior distribution in Equation 21, estimates using
the MCAR and the selection factorization models are at opposite ends of these posterior
distributions.

22

MeanFrequency6.87.07.27.47.67.88.00100200300400Standard Deviation1.61.82.02.22.42.62.8MCARNormalMeanFrequency0.300.400.500.600100200300400500Standard Deviation1.01.11.21.31.41.5MCARNormalEstimand Tukey’s representation Selection factorization MCAR Data set

Mean
Std. Dev.

Mean
Std. Dev.

7.42 (7.18, 7.73)
1.66 (1.52, 1.94)

0.51 (0.44, 0.59)
1.13 (1.07, 1.23)

Ratios

1.48 (1.28, 1.73)

6.84
2.01

0.35
1.23

1.62

7.82
1.55

0.60
1.08

1.43

Prot.
Prot.

mRNA
mRNA

Both

Table 1: Estimates for the quantities of interest obtained with diﬀerent models, from protein
and mRNA data. The dynamic range ratios are computed using both data sets. We report
maximum likelihood point estimates for both the MCAR and selection models. We report
posterior medians and 95% posterior intervals (in parentheses) for Tukey’s representation.

computing the ratio of the standard deviations between log-mRNA and log-protein levels,

a protein-speciﬁc quantity often referred to as the “dynamic range ratio”. A value of this

ratio less then one suggests that the dynamic range of protein levels is smaller than that of

mRNA, and is taken as evidence of a suppressive role of translational regulation. A value

greater than one is taken as evidence of ampliﬁcation, as claimed by the recent analyses.

We used posterior estimates of the complete data standard deviations, obtained from the

three competing models ﬁt to both protein and mRNA data sets (Pelechano and P´erez-Ort´ın,

2010; Ghaemmaghami et al., 2003) to estimate the distribution of the dynamic range ratios,

in Figure 8. The results obtained with Tukey’s representation are consistent with those

reported by Cs´ardi et al. (2014), suggesting that translational regulation reﬂects ampliﬁcation

of protein levels. Table 1 also reports numerical estimates of the the dynamic range ratios.

These results demonstrate the relative ease of applied data analysis with Tukey’s repre-

sentation models, and the increased ﬂexibility of models speciﬁed using this full conditional

speciﬁcation. By directly modeling the observed data, we avoid the need for Monte Carlo

integration of the missing data, and do not require parametric speciﬁcations for the complete

data density as is typical for selection models. By modeling the selection function directly,

we are also able to express uncertainty about the missing data density beyond the simple

location and scale changes typical in pattern-mixture model sensitivity analyses.

23

Figure 8: Dynamic range ratios obtained using Tukey’s representation (histogram), the
selection factorization model (blue) of Cs´ardi et al. (2014), and an MCAR model (red).

5 Discussion

Tukey’s representation provides a powerful alternative for specifying missing data models.

It allows analysts to eschew some diﬃcult questions about identiﬁability in models for non-

ignorable missing data (Miao et al., 2015) by factoring the joint distribution of the complete-

data, Y , and missing-data indicators, R, in such a way that the missingness mechanism is

the only component that must rely on assumptions unassailable using observed data.

5.1 Theoretical insights

Thus far we largely worked with exponential-family models. Here, we make formal statements

about exponential family, and give results that hold in greater generality.

Theorem 1. The normalizing constant Q is the population fraction of observed data.

24

Ratio of Standard DeviationsFrequency1.01.21.41.61.82.0050100150200250300MCARNormalThe proof is straightforward because

E[ri | θY |R, θR|Y ] = f (ri = 1 | θY |R, θR|Y )

(22)

(cid:90)

=

f (yi, ri = 1 | θY |R, θR|Y ) dyi

(cid:90)

f obs(yi | θY |R) dyi

= Q(θY |R, θR|Y )

= Q

The density of the observed values, yobs and r, can be written in terms of the population

fraction of the observed data, Q, in full generality,

f (yobs, r | θR|Y , θY |R) =

{i:ri=1}

f obs(yi | θY |R) Q ×

(cid:90) f (ri = 0 | yi, θR|Y )
i ri (cid:89)

(cid:89)
× (cid:89)
i ri(1 − Q)N−(cid:80)
(cid:80)
= 1 − (1 +(cid:82) . . . dyi)−1 = 1 − Q. And the missing-data

f obs(yi | θY |R) Q dyi

f obs(yi | θY |R),

f (ri = 1 | yi, θR|Y )

{i:ri=0}

{i:ri=1}

(23)

= Q

(cid:82) ... dyi
1+(cid:82) ... dyi

using the observation that

density can be expressed as a function of the observed-data density in full generality.

Theorem 2. The missing-data density can be expressed a function of the observed-data

density and the odds of missingness,

f mis(yi | θR|Y , θY |R) =

Q(θY |R, θR|Y )
1 − Q(θY |R, θR|Y )

f (ri = 0 | yi, θR|Y )
f (ri = 1 | yi, θR|Y )

f obs(yi | θY |R).

(24)

This result can be derived from from the complete-data likelihood and the formula for Q

in Equations 4 and 5.

When the missingness mechanism, f (ri | yi, θR|Y ) does not depend on yi, the distribu-
tion of the missing data is directly proportional to the distribution of the observed data.

25

Equation 24 can help assess the plausibility of diﬀerent missingness mechanisms—not at

random, completely at random, and at random (Mealli and Rubin, 2015)—by viewing them
as functions of the odds of a missing value, f (ri=0|yi,θR|Y )

f (ri=1|yi,θR|Y ). For instance, when the odds have
low variance, it may be reasonable to assume the missing data mechanism is completely at

random, or at random.

Equation 24 also leads to a general understanding of the main result of Section 3.1, which

can be crystallized in the following statement.

Theorem 3. If the observed-data distribution, f obs, belongs to an exponential family, and

the log-odds of a missing value are linear in the natural suﬃcient statistics of that observed-

data distribution, then the missing-data distribution, f mis, must have the same exponential

family as the observed data distribution.

This result can be immediately extended to mixtures.

Corollary 4. If the observed-data distribution, f obs, can be expressed as a K-component

mixture of an exponential family distribution, and the log-odds of a missing value are linear

in the natural suﬃcient statistics of that observed-data distribution, then the implied missing-

data distribution, f mis, must be a K-component mixture of the same exponential family.

We conjecture that a similar relation might hold outside exponential family models.

5.2 A note on the integrability condition

Not all integrable speciﬁcations for f obs(yi | θY |R=1) and f (ri | yi, θR|Y ) imply a proper dis-
tribution for f mis(yi | θY |R=1, θR|Y ). In the setting we consider in Section 3, the integrability
condition requires the sum θY |R=1 + θR|Y to lie in the natural parameter space of the expo-

nential family. In practice, analysts may want to consider missing data mechanisms that

involve a richer set of parameters, ˜θR|Y , such as including an intercept, as we illustrate in the

26

context of a biology application, in Section 4.3. In such cases, θR|Y is taken to denote the
subset of parameters in ˜θR|Y that multiply the suﬃcient statistics of f obs. The derivations

in Section 3 are simple to repurpose for this situation, accordingly.

For example, assume that the natural parameter θY |R=1 = η, and that the missing data
mechanism is logistic with extended parameter vector ˜θR|Y = (β0, β1) = (β0, θR|Y ) and
f (ri = 1 | yi, β) = (1 + e−(β0+T (yi)β1))−1. Then, Equations 8 and 9 become

Q(η, β) =

g(η + β1)

g(η + β1) + g(η)eβ0

f mis(y | η, β) = h(y)g(η + β1)eT (y)(cid:48)(η+β1).

(25)

(26)

The class of exponential-logistic models deﬁned in Equations 6–7 can be further general-

ized in two useful ways while maintaining its desirable properties. For instance, generalizing

f obs to be a mixture of exponential families as in Section 4.1 is straightforward, and does

not increase computation substantially. Relaxing assumptions about the missingness mech-
anism can be more diﬃcult. Still, it is possible to model f (ri | yi, θR|Y ) with a mixture of
logistic functions, including a missingness mechanism where a fraction of the data is missing

completely at random as in Section 4.3.

5.3 Inference strategies

Recall the simple Normal-logistic model of Section 3.2,

f (ri = 1 | yi, β) = logit (β0 + β1yi) =(cid:0) 1 + exp{−β0 − β1yi}(cid:1)−1

f obs(yi) = Normal (0, 1).

The inferential strategy proposed was to posit prior distributions on β1 and Q, and solve

for β0 at each iteration of the Markov Chain Monte Carlo sampler. A conceptually simpler

27

approach to inference would be to place a prior distribution on all the parameters of the

missingness mechanism, and solve for the implied Q at each iteration of the sampler.

In situations where the number of missing values is itself missing, as with truncated data,

specifying a prior distribution for all the parameters of the missingness mechanism would lead

to an implied prior distribution for the unknown number of missing values, or equivalently,

the population fraction of observed data Q.

In situations where the number of missing values is known, however, as with censored

data, and therefore Q can be estimated from observed data, the support of the likelihood

is a constrained parameter space, and a number of choices for the prior distribution on

β = θR|Y would lead to a posterior distribution that is challenging to explore using Monte

Carlo methods. Speciﬁcally, Equation 22 induces a moment constraint that restricts the

region where the parameters of the missingness mechanism have positive support to a lower

dimensional ridge. Figure 9 illustrates this phenomenon for the simple Normal-logistic model,

Figure 9: The region of positive support for the likelihood, restricted to the parameters of
the missingness mechanism, is increasingly constrained as the population fraction of missing
data Q is estimated with increasingly high precision. This intuition is illustrated by the width
of the ridge, which is a function of the amount of information about the fraction of missing
data Q. We simulated data from a standard Normal distribution and a logistic missingness
mechanism. The parameters (β0, β1) were set to get 90 percent missing data. The sample
size determines the amount of information: N = 100 (left) and N = 1000 (right).

28

0.00.20.40.60.81.0−3−2−10123−3−2−10123N = 100b1b00.00.20.40.60.81.0−3−2−10123−3−2−10123N = 1000b1b0and increasing sample size.

Sequential Monte Carlo and other specialized Monte Carlo methods that exploit the

geometry of the support of posterior distribution may provide a solution in this situation

(Doucet et al., 2001; Liu, 2008; Girolami and Calderhead, 2011; Airoldi and Haas, 2011).

5.4 Concluding remarks

In this paper, we used logistic-exponential family models to illustrate how Tukey’s represen-

tation can be used to encode non-monotonicity in the missingness mechanism, and to model

data with complex distributional forms. These models could also be used to facilitate tipping

point analyses (Liublinska and Rubin, 2014), or to incorporate subjective model uncertainty

via prior distributions on the missingness mechanism (Rubin, 2004).

Tukey’s representation is most useful when positing reasonable prior distributions on

the selection mechanism is feasible. Translating expert knowledge or expectations into a

functional form can be challenging, in general, and a logistic missingness mechanism is

not always a good choice.

In practice, Tukey’s representation should be used in concert

with strategies for expert prior elicitation (O’Hagan et al., 2006; Kynn, 2005; Paddock and

Ebener, 2009). Nevertheless, prior elicitation for Tukey’s representation is simpler than

for other factorizations, since it involves only the set of parameters θR|Y . In contrast, the

selection factorization requires additional assumptions about the complete data density.

In many settings, like the example presented in Section 4.3, we may be able to collect

data which partially informs the speciﬁcation for the selection mechanism. As such, when
possible, we can design experiments to learn about the functional form of f (ri|yi, θR|Y ) as
well as to further reﬁne prior distributions for θR|Y . Along these lines, Tukey’s representation

may be useful in the context of multiphase inference, which is intimately related to problems

in missing data (Blocker and Meng, 2013). In these problems, when preprocessing data, it

29

is often the case that we have strong knowledge (or control) of the missingness mechanism

yet a weaker understanding of the underlying scientiﬁc model.

All in all, we argue that Tukey’s representation, which represents a hybrid of the selection

and pattern mixture models is an under-researched yet promising alternative for modeling

non-ignorable missing data.

References

Airoldi, E. M. and Haas, B. (2011), “Polytope samplers for inference in ill-posed inverse

problems,” in Journal of Machine Learning Research, vol. 15 of W&CS (AiStat).

Andrea, R., Scharfstein, D., Su, T.-L., and Robins, J. (2001), “Methods for conducting

sensitivity analysis of trials with potentially nonignorable competing causes of censoring,”

Biometrics, 57, 103–113.

Bengtsson, M., St˚ahlberg, A., Rorsman, P., and Kubista, M. (2005), “Gene expression pro-

ﬁling in single cells from the pancreatic islets of Langerhans reveals lognormal distribution

of mRNA levels,” Genome research, 15, 1388–1392.

Besag, J. (1974), “Spatial interaction and the statistical analysis of lattice systems,” Journal

of the Royal Statistical Society. Series B (Methodological), 192–236.

Birmingham, J., Rotnitzky, A., and Fitzmaurice, G. M. (2003), “Pattern-mixture and selec-

tion models for analysing longitudinal data with monotone missing patterns,” Journal of

the Royal Statistical Society. Series B: Statistical Methodology, 65, 275–297.

Blocker, A. W. and Meng, X.-L. (2013), “The potential and perils of preprocessing: Building

new foundations,” Bernoulli, 19, 1176–1211.

Brook, D. (1964), “On the distinction between the conditional probability and the joint

30

probability approaches in the speciﬁcation of nearest-neighbour systems,” Biometrika,

481–483.

Cressie, N. (2011), Statistics for spatio-temporal data, Hoboken, N.J: Wiley.

Cs´ardi, G., Franks, A., Choi, D. S., Airoldi, E. M., and Drummond, D. A. (2014), “Account-

ing for experimental noise reveals that mRNA levels, ampliﬁed by post-transcriptional

processes, largely determine steady-state protein levels in yeast,” bioRxiv.

Daniels, M. J. and Hogan, J. W. (2000), “Reparameterizing the pattern mixture model for

sensitivity analyses under informative dropout.” Biometrics, 56, 1241–1248.

Doucet, A., de Freitas, N., and Gordon, N. (eds.) (2001), Sequential Monte Carlo Methods

in Practice, Springer.

Franks, A. M., Cs´ardi, G., Drummond, D. A., and Airoldi, E. M. (2014), “Estimating a

structured covariance matrix from multi-lab measurements in high-throughput biology,”

Journal of the American Statistical Association, 00–00.

Gelman, A., Carlin, J., Stern, H., and Rubin, D. B. (2004), Bayesian Data Analysis, Boca

Raton: CRC Press.

Ghaemmaghami, S., Huh, W. K., Bower, K., Howson, R. W., Belle, A., Dephoure, N.,

O’Shea, E. K., and Weissman, J. S. (2003), “Global analysis of protein expression in

yeast,” Nature, 425, 737–741.

Girolami, M. and Calderhead, B. (2011), “Riemann manifold Langevin and Hamiltonian

Monte Carlo methods,” Journal of the Royal Statistical Society. Series B: Statistical

Methodology, 73, 123–214.

Glynn, R. J., Laird, N. M., and Rubin, D. B. (1986), “Selection modeling versus mixture

modeling with nonignorable nonresponse,” in Drawing inferences from self-selected sam-

ples, Springer, pp. 115–152.

31

Hammersley, J. M. and Cliﬀord, P. (1971), “Markov ﬁelds on ﬁnite graphs and lattices,” .

Holland, P. (1986), “Discussion 4: Mixture Modeling Versus Selection Modeling With Non-

ignorable Nonresponse,” in Drawing inferences from self-selected samples, ed. Wainer, H.,

Routledge, pp. 143–149.

Karpievitch, Y., Stanley, J., Taverner, T., Huang, J., Adkins, J. N., Ansong, C., Heﬀron, F.,

Metz, T. O., Qian, W.-J., Yoon, H., Smith, R. D., and Dabney, A. R. (2009), “A statistical

framework for protein quantitation in bottom-up MS-based proteomics.” Bioinformatics

(Oxford, England), 25, 2028–34.

Karpievitch, Y. V., Dabney, A. R., and Smith, R. D. (2012), “Normalization and missing

value imputation for label-free LC-MS analysis,” BMC bioinformatics, 13, S5.

Kynn, M. (2005), “Eliciting expert knowledge for Bayesian logistic regression in species

habitat modelling,” .

Little, R. J. (1993), “Pattern-mixture models for multivariate incomplete data,” Journal of

the American Statistical Association, 88, 125–134.

Little, R. J. and Rubin, D. B. (2014), Statistical analysis with missing data, John Wiley &

Sons.

Liu, J. S. (2008), Monte Carlo Strategies in Scientiﬁc Computing, Springer, 2nd ed.

Liublinska, V. and Rubin, D. B. (2014), “Sensitivity analysis for a partially missing binary

outcome in a two-arm randomized clinical trial.” Statistics in medicine, 33, 4170–85.

Lu, C. and King, R. D. (2009), “An investigation into the population abundance distribution

of mRNAs, proteins, and metabolites in biological systems,” Bioinformatics, 25, 2020–

2027.

32

Lu, P., Vogel, C., Wang, R., Yao, X., and Marcotte, E. M. (2007), “Absolute protein ex-

pression proﬁling estimates the relative contributions of transcriptional and translational

regulation,” Nature biotechnology, 25, 117–124.

Lunagomez, S. and Airoldi, E. (2014), “Valid inference from non-ignorable network sampling

designs,” arXiv no. 1401.4718.

Marko, N. F. and Weil, R. J. (2012), “Non-Gaussian Distributions Aﬀect Identiﬁcation of

Expression Patterns, Functional Annotation, and Prospective Classiﬁcation in Human

Cancer Genomes,” PloS one, 7, e46935.

Mealli, F. and Rubin, D. B. (2015), “Clarifying missing at random and related deﬁnitions,

and implications when coupled with exchangeability,” Biometrika, in press.

Miao, W., Ding, P., and Geng, Z. (2015), “Identiﬁability of Normal and Normal Mixture

Models With Nonignorable Missing Data,” .

Molenberghs, G., Fitzmaurice, G. M., Kenward, M. G., Tsiatis, A. A., and Verbeke, G.

(eds.) (2015), Handbook of Missing Data Methodology, Chapman & Hall/CRC Press.

O’Hagan, A., Buck, C. E., Daneshkhah, A., Eiser, J. R., Garthwaite, P. H., Jenkinson, D. J.,

Oakley, J. E., and Rakow, T. (2006), Uncertain judgements: eliciting experts’ probabilities,

John Wiley & Sons.

Paddock, S. M. and Ebener, P. (2009), “Subjective prior distributions for modeling lon-

gitudinal continuous outcomes with non-ignorable dropout,” Statistics in medicine, 28,

659–678.

Pelechano, V. and P´erez-Ort´ın, J. E. (2010), “There is a steady-state transcriptome in

exponentially growing yeast cells,” Yeast, 27, 413–422.

Robert, C. P. and Casella, G. (2004), Monte Carlo Statistical Methods, Second Edition,

Springer-Verlag.

33

Rubin, D. (1977), “Formalizing Subjective Notions About The Eﬀect on Nonrespondents in

Sample Surveys,” Journal of the American Statistical Association.

Rubin, D. B. (1974), “Characterizing the estimation of parameters in incomplete data prob-

lems,” Journal of the American Statistical Association, 69, 467–474.

— (1976), “Inference and Missing Data,” Biometrika, 63, 581.

— (1978), “Bayesian inference for causal eﬀects: The role of randomization,” Annals of

Statistics, 6, 34–58.

— (2004), Multiple Imputation for Nonresponse in Surveys, Wiley Classic Library.

Scharfstein, D. O., Rotnitzky, A., and Robins, J. M. (1999), “Adjusting for Nonignorable

Drop-Out Using Semiparametric Nonresponse Models,” Journal of the American Statisti-

cal Association, 94, 1096–1120.

Soon, W. W., Hariharan, M., and Snyder, M. P. (2013), “High-throughput sequencing for

biology and medicine,” Molecular Systems Biology, 9.

Stan Development Team (2014), “Stan: A C++ Library for Probability and Sampling,

Version 2.2,” .

Troyanskaya, O., Cantor, M., Sherlock, G., Brown, P., Hastie, T., Tibshirani, R., Botstein,

D., and Altman, R. B. (2001), “Missing value estimation methods for DNA microarrays,”

Bioinformatics, 17, 520–525.

Walther, T. C. and Mann, M. (2010), “Mass spectrometry–based proteomics in cell biology,”

The Journal of Cell Biology, 190, 491–500.

34

A Detailed derivations

A.1 Derivations From Section 4.1

First, assume the observed data are normally distributed. For the k-th normal component

expressed in exponential family form, with natural parameters θY |R=1, we have:

θY |R=1 = (η1, η2) = (

µk
σ2
k

,− 1
2σ2
k

)

T (yi) = (yi, y2
i )

g(θY |R=1) =

1√−2η2

η2
1
4η2
2

e

(27)

(28)

(29)

where η are the natural parameters, T(y) the suﬃcient statistics and g(η) is the partition

function. By Equation 11 the odds of a missing value are

f (ri = 0|yi, θR|Y )
f (ri = 1|y, θR|Y )

= β2y2

i − 2β2β1yi + β2β2

1 + β0

(30)

with θR|Y = (β0, β1, β2). Finally, by Equation 9 the implied missing data distribution for the

k-th component is normal with natural parameters

(η∗
1, η∗

2) = (η1 − 2β2β1, η2 + β2)

(cid:19)

−1
2η2

,

The inverse parameter mapping for the normal speciﬁes that

(µk, σ2

k) =

(cid:18)−η1

2η2

35

(31)

(32)

and thus the moments of the missing data distribution are

(cid:32) −µk
(cid:18) µk − 2β1β2σ2

+ 2β2β1

2( −1

+ β2)

2σ2
k

σ2
k

,

k

1 − 2β2σ2

k

−1
2σ2 + β2)

2( −1

,

σ2
k

1 − 2β2σ2

k

)

(cid:33)
(cid:19)

(33)

(µ∗

k, σ∗2

k ) =

=

We now extend the results of Section 3.1 to mixture models. First, by Equation 9, the

observed data distribution is a mixture of exponential families, the missing data distribution

is also a mixture of those same exponential families. By applying Equation 8 to a mixture

of exponential families (speciﬁcally a mixture of normal and discrete distributions), we have

1 +(cid:82)(cid:104)

Q(w, η, β) =

=

1 + eβ0+β2β2

1

j h(pj, γj, β)

(cid:105)
(cid:105)

1

1

k wk

k wkh(y)g(ηk)eηkT (y) + (1 − α)(cid:80)m
α(cid:80)K
(cid:104)
α(cid:80)K
k) + (1 − α)(cid:80)m


k) + (1 − α)(cid:80)m

α(cid:80)K

1 − Q

g(ηk)
g(η∗

g(ηk)
g(η∗

k wk

(cid:104)

Q

j elogpj

(e(β2y2−2β2β1y+β2β2

1 +β0))

(34)

(35)

 − β2β2

1

(cid:105)

We invert this equation to express β0 as a function of Q, η and (β1, β2):

β0(Q, w, η, β1, β2) = log

j h(pj, γj, β)

with h(pj, γj, β) = elogpj +β2(γj−β1)2. Using Equation 24, we can show that the mixture weights

are simply

w∗
k =

α∗ =

g(ηk)
g(η∗
k)
g(ηk)
k wk
g(η∗
k)

wk

(cid:80)K
α(cid:80)K

α(cid:80)K
k) + (1 − α)(cid:80)m

g(ηk)
g(η∗
k)

k wk

k wk

g(ηk)
g(η∗

j h(pj, γj, β)

(36)

Together, Equations 36 , and 33 yield the missing data distribution speciﬁed in 12.

36

A.2 Derivations From Section 4.3

Assume that the observed data can be represented as a mixture of normals, and also that

the odds of a missing value can also be represented by a mixture. Speciﬁcally, we allow for

some missing values to occur completely at random. We posit that,

f (ri = 1|y, θR|Y ) =

κeβ1y+β0

1 + eβ1y+β0

(37)

The mechanism is logistic, but asymptotes at some value κ < 1, where (1 − κ) represents

the fraction of the complete data that is missing completely at random. Under this model,

(cid:18)

the odds of a missing value are

f (ri = 0|y, θR|Y )
f (ri = 1|y, θR|Y )

=

=

=

(cid:19) 1 + eβ1y+β0

κeβ1y+β0

.

1 − κeβ1y+β0
1 + eβ1y+β0
1 + (1 − κ)eβ1y+β0
1 − κ
κ

κeβ1y+β0
e−β1y−β0 +

1
κ

From Equation 8 applied to a mixture of normals,

(38)

(39)

(40)

(41)

(42)

We can invert Equation 41 to express β0 as a function of Q,η, β and κ:

k wkh(y)g(η)eηT (y)( 1

κ e−β1y−β0 + 1−κ
κ )

1

1

wk
κ

g(η)
g(η∗)

(cid:17)
1 − Q

Q

+ 1−κ

κ

 .

(cid:16)(cid:80)K

(1 + 1−κ
κ )
g(η)
g(η∗)

wk
κ

k

(cid:17)

Q(η, β) =

=

1 +(cid:82)(cid:80)K
(cid:16)(cid:80)K

1 + e−β0

k

β0(η, β1, κ) = − log

37

Finally using Equation 24 we ﬁnd that the missing data are a mixture normals with weights

(cid:80)K
k wk
g(β+ηk) + 1 − κ
g(ηk)
g(ηk)
g(η∗
k)
g(ηk)
k wk
g(η∗
k)

k wk
wk

(cid:80)K
(cid:80)K

κ∗ =

w∗
k =

g(ηk)

g(β+ηk)

.

(43)

(44)

38

