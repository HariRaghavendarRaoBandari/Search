Reliable Prediction Intervals for Local Linear Regression

Mohammad Ghasemi Hameda,∗, Masoud Ebadi Kivajb

aIFSTTAR-COSYS-LIVIC, 25 All´ee des Marronniers 78000 Versailles, France

bIndependent Researcher, 3 Alborz st. Koohsar, 35731 Tehran, Iran

Abstract

This paper introduces two methods for estimating reliable prediction intervals for local linear least-squares regres-
sions, named Bounded Oscillation Prediction Intervals (BOPI). It also proposes a new measure for comparing interval
prediction models named Equivalent Gaussian Standard Deviation (EGSD). The experimental results compare BOPI
to other methods using coverage probability, Mean Interval Size and the introduced EGSD measure. The results
were generally in favor of the BOPI on considered benchmark regression datasets. It also, reports simulation studies
validating the BOPI method’s reliability.

Keywords: Prediction Intervals, Local Linear Regression, Tolerance Intervals, Equivalent Gaussian Standard
Deviation

1 Introduction

Almost all methods aiming at learning a continuous response variable predict a conditional distribution for the
response variable. Having an estimated regression model built upon a ﬁnite sample, one may be interested in providing
inferences more than what point-wise model’s predictions would provide. In particular, dealing with high dimensional
datasets or considering a complex model naturally demands a more comprehensive study of the predicted value’s
dispersion. This demand becomes fundamental in applications requiring a high level of conﬁdence, like aircraft
trajectory prediction, health informatics, security and safety systems, etc. For this purpose, one may use a high
conﬁdence prediction interval: a prediction interval with a high probability β of containing the next observation of the
regression output.

MS E 1

2

2

1.1 Motivation
This work considers prediction intervals for least-squares Local Linear Regression (LLR). A common practice
in the interval prediction is to take ˆf (x) ± Z 1−β
and MS E are respectively
the 1−β
2 -quantile of the standard normal distribution and the mean squared error of the regression method given by a
leave-one-out or a cross validation scheme. This method, described as the “conventional method” in Section 2.3, have
some drawbacks which cause their β-content prediction intervals to be less reliable when the desired content is high
β ≥ 0.9. However high conﬁdence prediction intervals are very commonly used in machine learning and statistical
hypothesis-testing.

2 as prediction intervals, where Z 1−β

Figure 1 displays empirical distribution of coverage probability (see Equation(14) of a simulation study with loess
models (see Section 3.2) estimated on a training set of 1000 instances. All β-content prediction intervals, β = 0.9, 0.95,
are obtained using the conventional method on separated test sets of 500 instances. The whole process is iterated 1000
times. The datasets were generated with data generating processes Friedman#1 and Friedman#2 (Breiman, 1996;
Friedman, 1991). One could observe that the conventional intervals are not reliable, only 20 of the 4000 tested models
obtained a coverage greater than or equal to their nominal content which is far below the desired β rate. This problem
is the motivation of the current work.

∗Corresponding author
Email address: mohammad.ghasemi-hamed@ifsttar.fr (Mohammad Ghasemi Hamed)

1

6
1
0
2

 
r
p
A
1

 

 
 
]
E
M

.
t
a
t
s
[
 
 

4
v
7
8
5
5
0

.

3
0
6
1
:
v
i
X
r
a

(a) β = 0.9

(b) β = 0.95

Figure 1: The empirical distribution of coverage probability (see Equation(14)) of a simulation study with 1000 loess
models, each built on a training set of 1000 instances. All β-content intervals, β = 0.9, 0.95, are obtained using the
conventional method on separated tests set of 500 instances. The dotted (red) lines display the desired coverage. The
datasets were generated with regression functions Friedman#1 and Friedman#2 (Breiman, 1996; Friedman, 1991).

1.2 Related works

Non-parametric regression has been widely studied since 1975. Several monographs like (Eubank, 1999), (Hastie
and Tibshirani, 1990), (H¨ardle, 1990), Wahba (1990) and (Fan and Gijbels, 1996) have discussed this topic. The
idea of Local Polynomial Regression (LPR) appeared in (Stone, 1977) and (Cleveland, 1979). (Cleveland, 1979) in-
troduced Locally Weighted Regression (LWR) and a robust version of locally weighted regression known as Robust
Locally Weighted regression Scatter plot Smoothing (LOWESS). (Cleveland and Devlin, 1988) shown that locally
weighted linear regression could be very useful in real data modeling applications. They introduced ”loess” which is
a multivariate version of locally weighted regression. Their work includes the application of loess with multivariate
predictor dataset and an introduction of some statistical procedures analogous to those usually used in parametric
regression. They also proposed an ANOVA test for loess. (Fan, 1992, 1993) studied some theoretical aspects of local
polynomial regression. He showed that Locally Weighted Linear Regression (LWLR) (or weighted local linear re-
gression) is design-adaptive, it adapts to random and ﬁxed design. LWLR can be used as well in highly clustered than
nearly uniform design. He also showed that the best local linear smoother has 100% eﬃciency among all possible
linear smoothers, including kernel regression, orthogonal series and splines in minimax sense. Another important
property of LWLR is their adaptation to boundary points. As shown in (Fan and Gijbels, 1992), the LWLR estimator
does not have boundary eﬀects and therefore it does not require any modiﬁcations at the boundary points. This is
an attractive property of these estimators, because in practice, a large proportion of the data can be included in the
boundary regions. Then (Ruppert and Wand, 1994) extended Fan’s results on asymptotic bias and variance to the case
of multivariate predictor variables.

Prediction intervals along with other statistical intervals have been rigorously studied for the linear model in
(Rao and Toutenburg, 1999; Krishnamoorthy and Mathew, 2009; Paulson, 1943; Hahn and Meeker, 1991). There are
currently some prediction intervals for the regression problems with a non-linear model, however their applications in
literature remain limited for non-parametric regression models. (Ghasemi Hamed et al., 2012) proposed a K-Nearest
Neighbors (KNN) based interval prediction method, called simultaneous interval regression for KNN. Unlikely to
that work, here the authors are not looking after models that guarantee the simultaneous condition or the reliability
conditions of tolerance intervals. Furthermore, the prediction intervals introduced in this work are based on LLR
instead of KNN.

2

Friedman#1Friedman#265707580859095CoverageFriedman#1Friedman#2707580859095Coverage1.3 The contribution

We introduce two methods for obtaining Bounded Oscillation Prediction Intervals (BOPI) for local linear regres-
sion. It is assumed that the mean regression function is locally linear and the prediction error is locally normal and
homoscedastic. The BOPI methods consider regression bias and ﬁnd variable size intervals that work properly with
biased regression models. The proposed prediction intervals are constructed using prediction errors of the estimated
local linear regression model. These errors are obtained by a cross validation schema, for instance a leave-one-out or
a 10-fold cross validation.

In order to estimate prediction intervals, the current work introduces a bandwidth called LHNPE bandwidth (Local
Homoscedastic Normal Prediction Error bandwidth) which is diﬀerent from the regression bandwidth, as explained in
Section 5.1. One of the introduced prediction intervals method, “Farness BOPI”, has a bandwidth with a ﬁxed number
of neighbors and the other one, “Adaptable BOPI”, uses a LHNPE bandwidth with varying number of neighbors. Both
methods obtain variable size intervals which will be discussed in Section 5. The idea behind the variable LHNPE
bandwidth selection method is to ﬁnd the “best” LHNPE bandwidth for each input vector x∗. This iterative procedure,
described in Section 6.2, leads one to choose the prediction interval that has the best trade-oﬀ between the precision
(in term of interval size) and the uncertainty to contain the response value. It is achieved by ﬁnding a balance between
the faithfulness of the local assumptions (LHNPE conditions) and the required sample size to contain the desired β
proportion of the response value. In the same context, the Equivalent Gaussian Standard Deviation (EGSD) measure
is used for ranking interval prediction models. This measure rates the eﬃciency of an interval prediction method.

In order to validate the introduced methods, several artiﬁcial and real datasets are used to compare the introduced
prediction interval methods for local linear regression (Section 6) with commonly used interval predictions method.
These methods are the linear prediction intervals, Support Vector Machines (SVM) quantile regression and a common
interval prediction technique, that we call the conventional prediction intervals. The conventional prediction intervals,
described in Section 2.3, are similar to the Wald method for obtaining conﬁdence intervals. They use Gaussian conﬁ-
dence intervals with mean and variance, respectively equal to the prediction value and the mean squared error of the
regression given by a Leave-One-Out (LOO) or (the same measure in) a 10-fold cross validation scheme. Selected
methods are tested upon their capacity to provide two-sided β-content prediction intervals. The models are compared
for the reliability and eﬃciency of their obtained envelope as described in Section 4. This comparison is performed
with simulation studies on two artiﬁcial data generating process (DGP) and a 10-fold cross validation schema on 11
benchmark regression datasets with sample sizes and number of independent variables varying respectively from N =
103 to N = 8192 and from p = 1 to p = 21. Some of the real datasets contain numerical variables and some datasets
have numerical and categorical variables.

This work is organized as follows: Section 2 is a background on regression and prediction intervals. Section
3 describes the local linear regression and particularly the loess method which is used in the experimental section.
Section is a discussion on the selection criterion over diﬀerent prediction interval methods. Section 5 explains the
idea and hypothesis behind BOPI for LLR. Section 6 introduces the BOPI algorithms while Section 7 provides a
detailed explanation for their application using linear loess. Section 8 uses experiments to compare our methods with
other least squares and quantile regression methods on artiﬁcial and real benchmark dataset. The ﬁnal section is a
discussion with concluding remarks.

2 Background

2.1 Context and Notation
This work considers prediction intervals for local linear regression in ﬁxed design. Fixed design assumes that the
regression dataset S is a random sample composed of N pairs (xi, Y(xi)), where xi is a deterministic (non-random)
vector composed of p − 1 variables (non-random observation) and the Y(xi) observations are Independent Identically
Distributed (iid) random variables. The function f (·) denotes the mean of Y(x)’s distribution with a zero mean error
and an unknown variance σ2. Hereafter the following notations are used:
• S = (x1, Y(x1)), . . . , (xn, Y(xN)): the random sample of regression;

3

pred
x

: the prediction error at x, ε

= Y(x) − ˆf (x);

pred
x

• N: the number of observations in the regression dataset S;
• p: the number of independent variables plus one;
• f (x): the conditional mean of the response variable for a speciﬁed combination of the predictors;
• ˆf (x): the estimated regression function given x;
• ˆf −i(x) the estimated regression function given x without using the ith observation (xi, yi);
• ε: the random error term;
• ε
• ˆσ2: the estimated variance of the error term;
• x∗: a new observation in the predictor space that may not exist in the training set;
• Y(x): the conditional response variable for a given combination of the predictors, Y(x) = f (x) + ε;
• Yi: the ith random response variable, Yi = Y(xi);
• yi: an observation of the random variable Yi;
• I(ε
• I(x)Pred
• I(ε
• Zβ: the β-quantile of a standard normal distribution;
• χ2

)Tol
γ,β : β-content γ-coverage tolerance interval for the distribution of the prediction error at x;

β,n: the β-quantile of a chi-square distribution with n degrees of freedom.

: β-content prediction interval for the response variable at point x;

: β-content prediction interval of the prediction error at point x;

pred
x

)Pred
β

pred
x

β

Note that, in this work, we suppose that the prediction error at any xi is obtained with ε
the mean estimate ˆf −i(xi) is obtained without using the observation (xi, yi).

pred
xi

= Y(xi) − ˆf −i(xi), where

2.2 Least-squares Regression

Regression analysis is a statistical technique for estimating the value of one variable as a function of independent
variables. As mentioned in ﬁxed-design regression, the random variable Yi or Y(xi) follows a mean function f (xi)
with a random error term εi deﬁned as:

(1)
The model supposes that the εi are Independent and Identically Distributed (iid) random variables. The objective
is to estimate the mean function f (·) by ˆf (·). The usual assumption is to suppose that the variance of the error is the
same everywhere (homoscedasticity). Least-squares regression takes an estimator ˆf (·) that minimizes the Mean of
Squared Errors (MSE):

Yi = f (xi) + εi, where E(εi) = 0.

N(cid:88)
(yi − ˆf (xi))2

MS E( ˆf ) =

1
N

(2)

i=1

4

(cid:18)
Y(x) ∈(cid:20)

P

2.3 Conventional Interval prediction for least-squares Regression

One of the common interval prediction techniques used in practice is to take ˆf (x) ± Z 1−β

RMS E as the interval
which contains a β proportion of Y(x)’s population, where RMS E is the root mean squared error of the regression
method given by a LOO or a 10-fold cross validation scheme.

2

(cid:21)(cid:19)

ˆf (x) − Z 1−β

2

RMS E, ˆf (x) + Z1− 1−β

2

RMS E

= β.

(3)

While the conventional interval prediction method is simple, it has some drawbacks:
• The estimation does not take into account the regression sample size, unlike prediction interval or tolerance

intervals;

• It estimates global inter-quantile for the conditional response variable;
• It supposes that the estimated regression function is non-biased, but we know that the regression bias term in
non-parametric regression methods does not disappear when the sample size N goes to inﬁnity (Fan and Gijbels,
1996; Atkeson et al., 1997).

2.4 Prediction interval for normal distribution

The prediction interval for the future observation from a normal distribution is given by (Hahn, 1969):

Xn+1 − X
√
1 + 1/n
ˆσ

∼ tn−1,

where Xn+1, X and ˆσ respectively represent the (n + 1)th observation, the sample mean and sample standard error on
the n past observations. A two-sided β-content prediction interval for the future observation Xn+1 is obtained as:

P(Xn+1 ∈ IPred

β

) = β,

IPred
β

=

X ± t( 1−β

(cid:114)

 .

1
n

2 ,n−1) ˆσ

1 +

2 ,n−1) is the ( 1−β

where X is the estimated mean from the n past observations, t( 1−β
with n − 1 degrees of freedom.

2 )-quantile of Student’s t-distribution

2.5 Tolerance interval for normal distribution

Let X = (X1, . . . , Xn) denote a random sample from a continuous probability distribution. A tolerance interval is
an interval that is guaranteed, with a speciﬁed conﬁdence level γ, to contain a speciﬁed proportion β of the population.
A β-content γ-coverage tolerance interval, denoted by ITol
P(X ∈ ITol

(cid:19)
γ,β , is deﬁned as: (Krishnamoorthy and Mathew, 2009):
γ,β |X) ≥ β

= γ.

(5)

(cid:18)

PX

When the sample set (of size n) follows a univariate normal distribution, the lower and upper tolerance bounds (Xl
and Xu, respectively) are obtained as follows:

(cid:118)(cid:117)(cid:117)(cid:116)(n − 1)(1 + 1

Xl = ˆθ − c ˆσ, Xu = ˆθ + c ˆσ
1− 1−β

n)Z2

2

c =

χ2
1−γ,n−1

(4)

(6)

(7)

where ˆθ is the sample mean of the distribution, ˆσ is the sample standard deviation, χ2
of the chi-square distribution with n−1 degrees of freedom and Z2
normal distribution (Howe, 1969).

1− 1−β

is the square of (1− 1−β

2

1−γ,n−1 represents the 1−γ quantile
2 ) quantile of the standard

5

2.6 Prediction intervals in Regression
Deﬁnition 1. A β-content prediction interval for x , denoted here by I(x)Pred
observation of Y(x). It is deﬁned by the equation below (Rao and Toutenburg, 1999):

β

Equation (8)

(cid:17)

, has a probability of β to contain the next

(cid:16)
(8)
(cid:16)
] is an estimation of the true interval (population interval) I(x)Pop
Y (x) ∈ I (x)Pop

Y (x) ∈ I (x)Pred
(cid:17)

= β,

= β.

β

β

β

PS,Y(x)

PY(x)

where the interval I(x)Pred
such that:

β

= [L(x)Pred

β

, U(x)Pred

β

β

A prediction interval I(x)Pred

, which in contrary to prediction intervals, is not random. The population interval I(x)Pop

is obtained with a random sample S. It is an estimator for the unknown population
interval I(x)Pop
has ﬁxed
interval limits that are obtained by the true distribution of Y(x). Since for a given value of x, the distribution of
depends on the random sample S, prediction intervals have a joint probability distribution
the bounds of I(x)Pred
P (s, y (x)) for the random variables S and Y(x). For a detailed discussion about the diﬀerences between prediction
and other statistical intervals, see (Paulson, 1943; Hahn and Meeker, 1991; Krishnamoorthy and Mathew, 2009).

β

β

β

3 Local regression methods

Local Polynomial Regression (LPR) assumes that the unknown function f (·) can be locally approximated by a
low degree polynomial. Local Polynomial Regression (LPR) ﬁts a low degree polynomial model in the neighborhood
(xi) of x. The estimated vector of parameters used in the ﬁtted LPR is the vector that minimizes a locally weighted
sum of squares. Thus for each x a new polynomial is ﬁtted to its neighborhood and the response value is estimated by
evaluating the ﬁtted local polynomial with the vector x as covariate. In general the polynomial degree d is 1 or 2; for
d = 0, LPR becomes a kernel regression and when d = 1 it changes to LLR.

This LPR estimator is computed as follows (Fan and Gijbels, 1996):

ai(x)Yi,
where a(x) = 1T Lx, 1T = (1, 0,··· , 0) and Lx is computed as follow:
T Xx)−1Xx

Lx = (Xx

T Wx

i=1

ˆf (x) =

N(cid:88)

(10)
where Y = (Y1,··· , YN)T is the vector of response variables and for each x, Xx and Wx are respectively its predictor
matrix and weight matrix as described below:

T Wx,

1 (x1 − x)

...
1 (xn − x)

...

Xx =

 , Wx = diag(K(

···
...
···

(x1−x)d

d!

...

d!

(xn−x)d

xi − x
b

))N×N.

(11)

where, the Kernel function K(·) is used to weight the observations. Kernel functions are chosen so that observa-
tions closer to the ﬁtting point x have larger weights and those far from x have smaller weights. If K(·) is a kernel,
then Kb(·) is also a kernel function:

The term b, known as the bandwidth, is a constant scalar value used to select an appropriate scale for the data. In

this work, we use the following kernel:

(9)

(12)

Kb(u) =

K(

1
b

u
b

), where b > 0.

Kb(u) =

K(cid:18) D(u)

(cid:19)

,

b

1
b
6

where D(·) is a distance function like the L2-norm1.

3.1 Bandwidth Selection

A commonly used bandwidth selection method is LOO technique suggested in (Stone, 1977) which chooses the

following bandwidth b:

N(cid:88)

b = Argmin

(yi − ˆf −i(xi))2,

(13)

i=1

where ˆf −i(xi) is the estimation without using the ith observation. Estimating the bandwidth by LOO is a time-
consuming task, so it is common to minimize the k-fold2 cross-validation score with k = 5 or k = 10 instead of
LOO. This leads to an approximation of LOO. In this work, we use 10-fold cross validation to estimate the bandwidth
of our dataset. For more details about the use of a local version of PRESS statistics (which is also called leave-one-out
MSE in the literature) to speed up the cross-validation procedure see (Atkeson et al., 1997). For more details on other
bandwidth selection strategies, see (Atkeson et al., 1997; Fan and Gijbels, 1996; H¨ardle, 1990; Bowman and Azzalini,
2003).

3.2 Loess

Loess was introduced by (Cleveland and Devlin, 1988), and is a multivariate version of Locally Weighted Scatter-
plot Smoothing (LOWESS) (Cleveland, 1979). It is another version of LPR. Loess is described by injecting Equations
(10 and 11) in (9) and taking the degree of the polynomial term d = 1 or d = 2 in Equation (11). For the bandwidth
selection and weight calculation, loess applies similar bandwidths to KNN. Its weights are calculated with (12) where
u = (xi − x), D(·) is u’s L2-norm in the predictor space and b is the Euclidean distance between the input vector x
and its Kth nearest neighbor. The weight function chosen by (Cleveland and Devlin, 1988) was the Tricube kernel,
however it is not mandatory.
In this work, we used loess of degree one as the non-parametric smoother function. For each input vector x, we use
Equation (11), with d = 1, to estimate the vector of parameter ˆθx by using the training set.

4 Comparing Interval Prediction Methods

In this section we discuss the selection criterion over diﬀerent prediction interval methods. For a given dataset, we
may use several prediction intervals methods but we need some quality measure to compare them. For this purpose,
we deﬁne the dataset measures listed below.
] must be obtained for observations not contained
in the training set S. Therefore, for small to large datasets, these measures are obtained by a cross-validation or a
LOO schema.

The β-content prediction intervals I(xi)Pred

= [L(xi)Pred

, U(xi)Pred

β

β

β

4.1 Coverage Probability

coverage probability is the fraction of the response values that are contained in the β-content prediction interval

I(xi)Pred

.

β

coverageβ = N−1

V(xi)

(14)

where is V(xi) is deﬁned as:

V(xi) =

N(cid:88)

1

0

i=1

if Y(xi) ∈ I(xi)Pred
otherwise.

β

,

1Discussing more about local regression methods and their computational and practical aspects are not among the scope of this work. For a
review on local regression methods see (Atkeson et al., 1997) and for a discussion about the computational and practical aspects of nonparametric
smoothing see (Bowman and Azzalini, 2003; Hart, 1997).

2Note that the k used in k-fold cross-validation is diﬀerent from the k denoting the number of neighbors in the forthcoming sections.

7

4.2 Mean of Interval Size (MIS)

Mean of Interval Size (MIS) is the average size of prediction intervals estimated on the training set:

N(cid:88)

MIS β = N−1

size(I(xi)Pred

β

).

Another criterion is to report the sample standard deviation of interval sizes σis.

i=1

4.3 Equivalent Gaussian Standard Deviation (EGSD)

If we have diﬀerent interval prediction models estimated on the same dataset giving diﬀerent coverage values but
approximately equal MIS values, one generally select the estimated model with the higher coverage. However, this
model selection criteria would not be make sense when confronted to models (estimated on the same dataset) giving
diﬀerent values for both MIS and coverage. Let m be a β-content interval prediction model estimated on the dataset S,
yielding MIS m and coveragem
β . The Equivalent Gaussian Distribution (EGD) for m is the normal distribution of the
length of intervals obtained by method m that contains their response variable. Therefore, the EGD with the smallest
standard deviation (EGSD) corresponds to the “best” model. So, for a model m giving coveragem
β , its EGSD is the
standard deviation of the normal distribution, θ-content inter-quantile size of which is be equal to MIS m and it is
calculated as:

EGS DmS =

MIS mS
2Z1− 1−θ

2

, where θ = Coverm
β

(15)

EGSD measures the trade-oﬀ between average interval size and the fraction of successful predictions. Smaller
EGSD values denote more eﬃcient interval prediction models. Finally, for the sake of readability, all computed
EGSD are normalized on each dataset. This normalized value is the ratio of the method’s EGS Dm to the maximum
EGS D value on the underlying dataset:

normalizedEGS Dm =

EGS Dm

(EGS Di) .

max
i∈(1,...,c)

Note that if the method m1 has a smaller EGSD than the model m2, it does not mean that the m2’s envelope is wider
than the m1’s envelope. As seen above, smaller normalized MIS values means tighter envelopes and smaller EGSD
values means more eﬃcient methods.

5 Bounded Oscillation Prediction Intervals (BOPI) for Local Linear Regression

In this section two methods for obtaining Bounded Oscillation Prediction Intervals (BOPI) for local linear re-
gression are introduced. It is assumed that the mean regression function is locally linear and the prediction error is
locally normal and homoscedastic. The introduced methods consider regression bias and ﬁnd variable size intervals
that work properly with biased regression models. The BOPI are constructed using prediction errors of the local linear
regression which are obtained by a cross validation schema, for instance a LOO or a 10-fold cross validation. In order
to estimate local linear regression, one should consider a regression bandwidth; the authors consider the KNN band-
width. However, the choice of the regression bandwidth is independent of the BOPI methods (for more on bandwidth
selection in regression see Section 3.2). In order to estimate bounded oscillation prediction intervals, the current work
introduces a bandwidth called LHNPE bandwidth. This work suggests two diﬀerent LHNPE bandwidths: a band-
width having a ﬁxed number of neighbors and a bandwidth having a variable number of neighbors. Both of them
result variable size intervals which will be discussed in more details on sections bellow.
The idea behind BOPI methods is to exploit the local density of the prediction errors (Yi− ˆf (xi)) inside the LHNPE
neighborhood (explained further in the next section) of a new observation x∗ and then, to ﬁnd the most appropriate
interval which should contain a desired proportion β of the Y(x∗)’s distribution. The introduced prediction intervals are
estimated by adding the mean regression estimates ˆf (x∗) to the tolerance intervals for the prediction error I(ε
)Tol
γ,β .
This technique should be eﬃcient, since as we will see later, these tolerance intervals are centered on the negative of

pred
x

8

estimated bias and when added to the regression estimates, the bias term (which is always present) is treated properly.
The presence of bias is due to the fact that, the optimal smoothing in non-parametric regression consists of a balance
between the vraiance and the squared bias of the regression estimator. Therefore, the regression bias in non-parametric
regression is a non-vanishing term, even asymptotically(H¨ardle (1990)).

5.1 Deﬁnition

This part describes the context and idea behind of bounded oscillation prediction intervals for local linear regres-
sion. We ﬁrst deﬁne the concept of a Local Homoscedastic Normal Prediction Error (LHNPE) regression estimator.
Then we deﬁne the LHNPE neighborhood at x∗ to obtain the bounded oscillation prediction interval at x∗. In fact,
if a regression method satisﬁes the LHNPE conditions, then for every x∗ we can use its LHNPE neighborhood to
estimate the bounded oscillation prediction interval of x∗. Finally we obtain the equation of the estimator of bounded
oscillation prediction intervals for local linear regression. Let us begin with deﬁnitions of the assumptions that will be
used in this work:
Deﬁnition 2. The oscillation of the function f : X → R on an open set U is deﬁned as:

ω f (U) = sup
x∈U

f (x) − inf
x∈U

f (x).

Deﬁnition 3. A regression estimator ˆf (x) is a Local Homoscedastic Normal Prediction Error (LHNPE) regression
estimator if its prediction errors satisfy the following conditions:

• Normal prediction error: the prediction error ε
• Almost constant distribution of the prediction error: the mean µ(ε

pred
x

pred
x

= Y(x) − ˆf (x) follows a normal distribution.

) and the standard deviation σ(ε

pred
x

) of

the distribution for the prediction error have small local oscillations. This is deﬁned formally as:
For all x, there exists an open set U (cid:51) x, such that:

where υ1 and υ2 are small ﬁxed positive values.

)(U) ≤ υ1 and ωσ(ε

)(U) ≤ υ2,

pred
x

ωµ(ε

pred
x

Deﬁnition 4. Let ˆf (x∗) be a LHNPE regression estimator for x∗ deﬁned in Deﬁnition 3. The LHNPE neighborhood
for x∗ is deﬁned as instances for which the prediction error satisﬁes the LHNPE conditions. This neighborhood is
described as below:

Ksetx∗ = {(xi, Yi)| d(x∗

, xi) ≤ b},

(16)

(17)

where d(x∗, xi) is a distance function in the feature space and b denote the LHNPE bandwidth.

Note that the LHNPE neighborhood Ksetx∗ is diﬀerent from the regression neighborhood Regx∗ in local linear

regression. The regression neighborhood is described as below:
Regx∗ = {(xi, Yi)| d(x∗

, xi) ≤ breg}.

It should be noted that while the regression bandwidth (breg) ﬁnds a trade-oﬀ between regression’s variance and
squared bias, the LHNPE bandwidth is used to ﬁnd the neighborhood where the oscillation of the mean and variance
of the distribution of the prediction error is bounded by a small positive value.

The LLR assumptions constraint the regression neighbors to be the set of observations for which the mean regres-
sion function is almost linear. This is less restrictive than the LHNPE conditions. So, the LHNPE neighborhood of an
input vector x∗, is more likely to be included in its regression neighborhood:

Ksetx∗ ⊆ Regx∗ .

(18)

However, it is possible to have a dataset with two or more instances having diﬀerent regression neighborhoods and

approximately the same LHNPE neighborhood.

9

Proposition 1. Let Y(x) = f (x) + εx and let
estimator satisﬁes the conditions below:

ˆf (x) denote its local linear regression estimator.

If this regression

• Normal error distribution: εx ∼ N(0, σ2
x);
• ˆf (x) has an almost constant distribution as deﬁned as in Deﬁnition 3.

Then we have these following statements:

ˆf (x) is an LHNPE regression estimator;

(a)
(b) The interval I(x∗)Pred

β

for the input x∗ obtained by Equation (19) is a β-content prediction interval for Y(x∗);

I(x∗)Pred

β

= ˆf (x∗) + I(ε

pred
x∗

)Pred
β

,

(19)

and I(ε

where I(x∗)Pred
for the normal distribution (computed using Equation (4)) on the prediction errors ε
borhood.

respectively denote the response prediction interval and the prediction interval
pred
x∗ within the LHNPE neigh-

)Pred
β

pred
x∗

β

(c) The sample bias of the prediction error in the LHNPE neighborhood is a consistent estimator of the regression

where K/N → 0 as N → ∞, K = card(Ksetx∗) and (cid:100)bias ˆf (x∗) are respectively the cardinal of Ksetx∗ and the sample

plim
K→∞

pred

x∗ = Y(x∗) − ˆf (x∗),

ε

(cid:16) ˆf (x∗) − (cid:100)bias ˆf (x∗)

(cid:17)

= f (x∗),

bias:

bias of ˆf (x∗).
Proof: See Appendix B.

pred
x∗

)Pred
β

LHNPE conditions assume that the prediction error has an unknown normal distribution with unknown an mean
and an unknown variance being respectively the negative bias and the variance of the prediction error. By adding
I(ε
to the biased regression estimator, the bias is reduced by the estimated bias and results to prediction inter-
vals that work better with local linear regression estimators. Proposition (1) shows that the knowledge of x∗’s LHNPE
neighborhood enables us to calculate its prediction interval by Equation (19) ?. However the LHNPE neighborhood
of x∗ is not known, so it should be estimated on the training set. Having a ﬁnite sample, the estimation of the LHNPE
neighborhood may lead to prediction intervals smaller than the true model prediction intervals (less reliable because
its actual content is less than the desired β). Therefore, we approximate the prediction interval for the prediction errors
obtained in x∗’s LHNPE neighborhood (I(ε

in Equation (19)) by an upper bounds described below.

Proposition 2 shows that for any β-prediction intervals of the standard normal distribution, we always have a
tolerance interval that is wider than or equal to it. Having in mind Propositions 1 and 2 and in order to avoid smaller
prediction intervals, one can approximate the unknown prediction intervals on prediction error I(ε
in Equation
(19) by the tolerance interval on the prediction errors within the estimated LHNPE neighborhood. Proposition 2 is
veriﬁed numerically, so it impose the LHNPE neighborhood to be such as 20 ≤ K ≤ 10000. The lower limit of 20
is chosen because methods are not intended to be used for very small values of LHNPE neighborhood and the upper
limit is justiﬁed by the fact that a LHNPE neighborhood of size greater than 10000 may occur in very limited cases.
The LHNPE neighborhood can be estimated on the training set. In other words, the β-content prediction interval
on the response variable for x∗ is obtained as:

)Pred
β

pred
x∗

)Pred
β

pred
x∗

I(x∗)Pred

β

(cid:39) ˆf (x∗) + ˆI(ε

γ,β , γ ≥ 0.7
)Tol

pred
x∗

(20)

pred
x∗

where ˆI(ε
)Tol
γ,β denote a γ-coverage β-content tolerance interval for the normal distribution (obtained by Equa-
tions (6) and (7)) on the prediction errors within the estimated LHNPE neighborhood. The optimal value of γ will
vary depending on the underlying dataset, the desired content β and the required reliability of the ﬁnal prediction
intervals obtained using Equation (20).

10

5.2 Tolerance intervals as upper limits of prediction interval
By properties of tolerance intervals for a normal distribution (see Section 2.5), if one ﬁxes n and β such that
20 ≤ n ≤ 10000 and 0 < β < 1, and let the conﬁdence γ ≥ 0.7, then γ-coverage β-content tolerance intervals of the
normal distribution are greater than or equal to its β-content prediction intervals.
Proposition 2. For any random sample larger than 20 ≤ n ≤ 10000, if we set γ and β, then the γ-coverage β-content
tolerance interval of the standard normal distribution is greater than or equal to its β-prediction intervals. This is
stated formally below:

∀20 ≤ n ≤ 10000, γ ≥ 0.7, β ∈ [0.01, 0.99], size(ITol

).

(21)

γ,β ) ≥ size(IPrev

β

where size(I) = U − L, I = [L, U] and the terms ITol
β-prediction interval of the standard normal distribution.

γ,β and IPrev

β

refer to γ-coverage β-content tolerance interval and

Proof: See Appendix B.

Figure (2) compares the size of tolerance intervals and prediction intervals of the standard normal distribution
for n ≥ 20, γ = 0.7 and 0.01 ≤ β ≤ 0.99. The mentioned tolerance interval and prediction intervals are obtained
respectively by Equations (6) and (4). We can see that in this case, tolerance intervals are always greater than or equal
to prediction intervals.

Figure 2: This Figure compares the size of tolerance intervals (solid blue line) and prediction intervals (dashed orange
line) of the standard normal distribution for n ≥ 20, γ = 0.7 and 0.01 ≤ β ≤ 0.99. The mentioned tolerance interval
(solid blue) and prediction intervals (dashed orange) are obtained respectively by Equations (6 and 4).

Table 1 represents the smallest sample size such that a two-sided γ-coverage β-content tolerance interval contains
its corresponding two-sided β-content prediction interval. The tolerance intervals and prediction intervals are com-
puted for the standard normal distribution and they are respectively obtained by Equations (6) and (4). As one see in
the table, the required sample size is decreasing with the desired proportion β. For example, consider the comparison
between β = 0.8 and β = 0.95. By looking at Table 1, one can see that the two-sided 0.55-coverage 0.8-content toler-
ance interval for the standard normal distribution contains its 0.8-content prediction interval. However, for β = 0.95,
we need to have a sample of n ≥ 100 to guarantee that the two-sided 0.55-coverage 0.95-content tolerance interval will
contain its 0.95-content prediction interval. Since these methods are not intended to be used for very small datasets,
this table does not show n < 20.

11

0.00.20.40.60.81.00.00.51.01.52.02.53.0 ProportionInterval SizeTolerance IntervalPrediction Intervalγ

Desired Proportion

0.9
0.55
50
0.6
20
0.65 ≤ 20 ≤ 20
0.7

0.8
0.95
20
100
≤ 20
50
20
≤ 20 ≤ 20 ≤ 20

0.99
350
80
40
20

Table 1: Smallest sample size n for which a two-sided γ-coverage β-content tolerance interval contains its corre-
sponding two-sided β-content prediction interval. The mentioned tolerance and prediction intervals are computed for
the standard normal distribution and they are respectively obtained by Equations (6) and (4). Note that by properties
of tolerance intervals, when γ increases and β is ﬁxed, the required sample size n decreases.

6 The BOPI Algorithms

As described before, having a local linear model satisfying the LHNPE conditions, one can take advantage of the
LHNPE conditions for the local linear estimator and, as described by (20), use the tolerance interval of the normal
distribution on the prediction errors within the estimated LHNPE neighborhood to approximate the BOPI on the
response value at x∗. Let Esetx∗ denote the prediction error inside the estimated LHNPE neighborhood of x∗ and it is
deﬁned as:

(22)
where ˆf −i(xi) is the local linear estimation without using the ith observation, obtained by (9). Note that when (xi, Yi)
belongs to the training set, Yi− ˆf (xi) becomes a residual and it depends on the random variable Yi; however, Yi− ˆf −i(xi)
Given an input vector x∗, (cid:98)Klhnpe the number of neighbors in Esetx∗, β the desired content and γ the conﬁdence
and Yi are independent.

| (xi, Yi) ∈ Ksetx∗}, where ε

= Yi − ˆf −i(xi).

Esetx∗ = {ε

pred
xi

pred
xi

is computed by replacing ˆθ, ˆσ and n in Equations (6)

level, the tolerance interval for the prediction error variable ε
and (7):

pred
x∗

(cid:118)(cid:117)(cid:117)(cid:117)(cid:116)((cid:98)Klhnpe − 1)(1 + 1(cid:98)Klhnpe
1−γ,(cid:98)Klhnpe−1
(cid:80)
((cid:98)Klhnpe − 1)
∈Esetx∗ (ε

pred
ε
i

χ2

pred

)Z2

1− 1−β

2

,

xi − εpred

xi

)2

.

ˆI(ε

pred
x∗

)Tol
γ,β

ˆθ = εpred

xi

= ˆθ ± c ˆσ, where c =
=(cid:98)K−1

(cid:88)

ε

lhnpe

pred
xi
xi ∈Esetx∗

pred

and ˆσ2 =

(23)

(24)

ε

In this work the authors suggest two methods for computing the (cid:98)Klhnpe-nearest neighbors of x∗. One of them
Equation (20) uses (cid:98)Klhnpe of prediction errors (obtained by a cross validation schema) in the training set inside the

deals with estimated LHNPE neighborhood as ﬁxed and the other as variable number of neighbors and both of them
are tuned on the training set, so this results in two methods for obtaining BOPI. The tolerance interval ˆI(ε
)Tol
γ,β in

estimated LHNPE neighborhood of x∗. Prediction error of the whole training set is denoted by error set:

pred
x∗

For the relationship between the minimum coverage level γ in tolerance intervals and(cid:98)Klhnpe, see Table 1.

| (xi, Yi), i ∈ (1,··· , N)}, where ε

= Yi − ˆf −i(xi).

error set = {ε

pred
xi

pred
xi

(25)

6.1 LHNPE bandwidth with Farness BOPI (F-BOPI)

This method considers a ﬁxed number of the nearest neighbors of x∗ as its LHNPE neighborhood. We denote this
interval prediction method for LLR by Farness BOPI (F-BOPI) and the ﬁxed number of returned neighbors is denoted
by K f
lhnpe is a hyper-parameter to be tuned on the training set such that the LHNPE conditions are respected

lhnpe. K f

12

for the majority of instances in training set. This neighborhood is generally selected in such a way to keep the most of
training instances’ LHNPE neighborhood inside their corresponding regression neighborhood. Once the local linear
model is built and the error set is computed, the computational complexity of F-BOPI for a new instance is the same
as under the conventional prediction intervals.

6.2 LHNPE bandwidth with Adaptable BOPI (A-BOPI)

pred
x∗

lhnpe ≤ Ka

lhnpe ≤ Kmax

lhnpe. Finally, the ˆI(ε

lhnpe may also reduce prediction variance; this issue is controlled by Kmax

The idea behind this LHNPE bandwidth selection, denoted by Adaptable BOPI (A-BOPI), method is to ﬁnd the
“best” LHNPE bandwidth for each input vector x∗. Here, the best number of LHNPE neighbors is denoted by Ka
lhnpe.
For a ﬁxed value of β, and for each input vector x∗, the β-content γ-coverage normal tolerance interval of errors in
Esetx∗ deﬁned in (22) is calculated and this process is repeated for the same input vector x∗ but with diﬀerent values of
γ,β having the smallest size is chosen and is added to ˆf (x∗). This
Ka
)Tol
lhnpe, Kmin
iterative procedure leads us to choose the interval that has the best trade-oﬀ between the precision (size of intervals)
and the uncertainty (number of observations used to obtain the interval) to contain the response value. The more
Ka
lhnpe increases, the less the local homoscedasticity assumption (bounded oscillation of the prediction error) match
the reality and this yields a prediction error variance diﬀerent from the true one. If by increasing Ka
lhnpe, the local
estimation of the prediction error variance exceeds its true value, the fact that the tolerance interval size decreases
when Ka
lhnpe increases could partially compensates the interval size growth caused by this over estimation. However,
an increase in Ka
lhnpe. On the contrary, when
Ka
lhnpe is too small, the LHNPE conditions are more likely to be respected but the tolerance intervals size get larger (due
to the small Ka
lhnpe that minimizes a ﬁxed β-content γ-coverage tolerance interval
ensures that we will have the best3 trade-oﬀ between the faithfulness of the local assumptions (LHNPE conditions)
and the required neighborhood size to contain the desired β proportion of the response value. The optimal value of
Ka
lhnpe may vary much more on heterogeneous datasets.
lhnpe-nearest neighbors of x∗ in its LHNPE neighborhood, we put two global limits
for the search process: the variables Kmin
lhnpe is the smallest number of neighbors which is assumed
here to be greater than or equal to 20. The upper bound Kmax
lhnpe, is used to stop the search process if by growing the
number of neighbors we constantly decrease the interval size. This break may be necessary when an increase in the
number of neighbors result in adding new neighbors all having smaller prediction errors than the current neighbors. In
practice, these smaller prediction errors usually belong to a diﬀerent neighborhood in the feature space with diﬀerent
error variances and/or prediction error distributions. Therefore these two bounds serve to restrict the search process in
a region where it is most likely to contain the LHNPE neighborhood of x∗. Kmax
lhnpe should almost always be included in
the regression neighborhood. However one can take it greater than the regression bandwidth and let the search process
ﬁnd the neighborhood which gives the smallest tolerance interval.

lhnpe). Thus choosing the value of Ka

In order to ﬁnd to keep the Ka

lhnpe and Kmax

lhnpe. Kmin

Once the local linear model is built and error set is found on the training set, the computational complexity of
interval prediction for a new instance is (Kmax
+ 1) times higher than the complexity of an evaluation under
the local linear regression. This is because an interval prediction for x∗ with A-BOPI has a Ksetx∗-ﬁnding step in
which (Kmax
+ 1) diﬀerent intervals are evaluated. In this step, A-BOPI ﬁnds the tightest interval among the
computed ones and shifts its center to the LLR’ estimation. More explanation on the LLR complexity can be found
in (Atkeson et al., 1997; Fan and Marron, 1994; Gasser and Kneip, 1989)

lhnpe − Kmin

lhnpe − Kmin

lhnpe

lhnpe

Figure 3 illustrates an example of the comparison of the BOPI methods with the conventional prediction intervals
when β = 0.9. The results are explained in detail in Table 5. In this example, our introduced methods are more reliable
than the conventional method; they provide variable size intervals with a good trade-oﬀ between the interval size and
the model coverage.

Figure 4 displays the coverage, of a simulation study with 1000 loess models, each built on a training set of 1000
instances. The β-content intervals, β = 0.95, 0.95, are obtained using the conventional method (Loess Conv.) and
F-BOPI and A-BOPI methods on separated test sets of 500 instances. The datasets are the same (generated) datasets

3Assuming γ as ﬁxed and Kmin

lhnpe ≤ Ka

lhnpe ≤ Kmax
lhnpe.

13

Figure 3: The comparison of A-BOPI (in green) and F-BOPI (in blue) prediction intervals to the conventional (F-
BOPI in orange) prediction intervals on the Motorcycle dataset (described in Section 8.3) when β = 0.9. The three
envelopes around the mean are obtained on the same loess regression model with a 10-fold cross validation schema.
The red crosses in the plot show the points where the conventional method fail to cover wheras both BOPI methods
cover successfuly.

used in Figure 1, they were generated using data generating processes Friedman#1 and Friedman#2 (described in
8.2) (Friedman, 1991; Breiman, 1996). Kloess = 100 as the regression bandwidth is constant for the three methods,
K f
= 50) for A-BOPI. We can observe that F-BOPI and A-BOPI methods
lhnpe
are much more reliable than the conventional method. These simulations are reported in Section 8.2 (Tables A.12 and
A.14).

= 40 for F-BOPI and (Kmin
lhnpe

= 30, Kmax
lhnpe

6.3 Hyper-parameter Tuning

Assuming that the regression model is already built (the regression bandwidth breg is already estimated for the
dataset), one needs to ﬁnd the optimal vector of hyper-parameters for the prediction interval methods proposed above.
The hyper-parameter tuning problem is ﬁrst converted into an optimization problem and then an optimization algo-
rithm is proposed. The tuning process uses prediction errors obtained by LLR on the training set to ﬁnd optimal
solutions.

Let λ denote the vector of hyper-parameters for A-BOPI or F-BOPI with β as their desired proportion of content.

The optimization problem is the following:

N(cid:88)

i=1

λ0 = Argmin(MIS λ

β), where MIS λ

β

=

1
N

size(I(xi)Tol
λ,β )

Subject to:

coverage Tuning Constraint: coverageλ0
β
λ-Speciﬁc Constraints: depends on the prediction intervals.

= β

(26)

(27)

Note that the coverage Tuning Constraint is a hard constraint and there is no trade-oﬀ between satisfying this
constraint and minimizing the MIS. Once λ0 which satisﬁes the constraint deﬁned above is found it would, results in
intervals having the smallest MIS measure where coverage and MIS are computed based on a leave-one-out or 10-fold
cross validation scheme on the training set.

14

(a) Friedman#1, β = 0.9

(b) Friedman#1, β = 0.95

(c) Friedman#2 β = 0.9

(d) Friedman#2 β = 0.95

Figure 4: coverage of a simulation study with 1000 loess models, each built on a training set of 1000 instances. The
β-content intervals, β = 0.9, 0.95, are obtained using the conventional method (Loess Conv.), F-BOPI and A-BOPI
methods on separated test sets of 500 instances. The dotted (red) lines display the desired coverage. The datasets
(same as Figure 1) were generated using data generatipn processes Friedman#1 and Friedman#2 (described in 8.2)
(Friedman, 1991; Breiman, 1996). Kloess = 100 as the regression bandwidth; it is constant for the three methods,
K f
= 50) for A-BOPI. For more details see Table A.12 and Table A.14.
lhnpe

= 40 for F-BOPI and (Kmin
lhnpe

= 30, Kmax
lhnpe

15

Loess Conv.F−BOPIA−BOPI707580859095100CoverageLoess Conv.F−BOPIA−BOPI7580859095CoverageLoess Conv.F−BOPIA−BOPI65707580859095100CoverageLoess Conv.F−BOPIA−BOPI65707580859095100Coverage7 Application to loess

This subsection brieﬂy reviews an application with the loess of degree one regression method. As described in
Section (3.2), loess is a version of linear polynomial regression that, for each observation, takes its K nearest in-
stances in the feature space as its neighborhood. Let us denote loess’s regression bandwidth with Kloess. Loess could
use among others a ﬁrst or second degree polynomial.

Prediction intervals with loess of degree one have three or four hyper-parameters: Kloess and the prediction hyper-
lhnpe) and
lhnpe) are respectively the LHNPE bandwidth for prediction intervals obtained with ﬁxed and variable number

parameters which are the conﬁdence level γ and the estimated LHNPE bandwidth. As seen above, (K f
(Kmin
of instances. Based on (18), for A-BOPI we usually have:

lhnpe, Kmax

and for F-BOPI, we have:

7.1 Optimization problem for loess of degree one

lhnpe ≤ Kloess
Kmax

lhnpe ≤ Kloess.
K f

As described in (6.3), it is assumed that at this stage the loess bandwidth Kloess has been found. The diﬀerence be-
tween A-BOPI and F-BOPI is in their LHNPE bandwidth hyper-parameters, so we have λ = (Kloess, (γ, Kmin
lhnpe))
for A-BOPI and λ = (Kloess, (γ, K f
lhnpe)) for F-BOPI Once the loess regression model is estimated, the prediction in-
terval hyper-parameter tuning reduces to the constraint optimization problem listed below where all the constraints
are hard constraints.

lhnpe, Kmax

Optimization problem for ﬁxed(cid:98)Klhnpe:

(γ, K f

lhnpe) = Argmin(MIS λ

i=1

β

=

β

1
N

0 < K f

ˆI(εxi)Tol

γ,β

β), where MIS λ

λ-speciﬁc Constraints:

= β
lhnpe ≤ N

(cid:40) t ≤ γ < 1, see Table 1

N(cid:88)
 coverage Tuning Constraint: coverageλ0
N(cid:88)
 coverage Tuning Constraint: coverageλ0
(cid:40) t ≤ γ < 1, see Table 1
= β
lhnpe ≤ Kmax
lhnpe ≤ N

β), where MIS λ

λ-speciﬁc Constraints:

0 < Kmin

=

β

1
N

i=1

β

(γ, Kmin

lhnpe, Kmax

lhnpe) = Argmin(MIS λ

I(εxi)Tol

γ,β

With Tuning Constraints:

Optimization problem for variable(cid:98)Klhnpe:

With Tuning Constraints:

Note that for F-BOPI, the smallest value of γ (which is denoted by t in the optimization problem above), depends
lhnpe and this relationship is shown in Table 1. For A-BOPI the same dependency exists between the smallest

on K f
value of γ and Kmin

lhnpe.

16

lhnpe, Kmax

7.2 Hyper-parameter tuning for loess of degree one

Algorithm 1 describes how to tune the prediction interval hyper-parameters for variable (cid:98)Klhnpe. The algorithm
used for the ﬁxed (cid:98)Klhnpe is almost the same, except that it computes the hyper-parameter K f

lhnpe instead of the pair
(Kmin
lhnpe), so we omit its description. In a ﬁrst attempt, γ is considered as a ﬁxed high value like γ = 0.9 or
γ = 0.99 and the focus is on ﬁnding the LHNPE neighborhood hyper-parameter: the hyper-parameter K f
lhnpe or the
pair (Kmin
β deﬁned by Equation (27) must be greater than or equal
to β. Thus the LHNPE neighborhood hyper-parameter(s) which ﬁnd(s) intervals that, based on a LOO or 10-fold cross
validation scheme on the training set, satisﬁes the coverage tuning constraint deﬁned in (27) and also have the smallest
Mean Interval Size (MIS) is selected. Once K f
lhnpe) is found, one searches for the smallest value of γ
that satisﬁes the coverage tuning constraint.

lhnpe). as described before the variable coverageλ

lhnpe or (Kmin

lhnpe, Kmax

lhnpe, Kmax

= 50) and K f

By taking K f

lhnpe from 20 to 70 by steps of 5. For each value of K f

Figure 5 displays the variation of the distribution of coverage and MIS of F-BOPI, with constant values for γ = 0.9
and β = 0.95, by changing K f
lhnpe, 100 loess models are estimated,
each built on a training set of 1000 instances. Plot of distribution of coverage and MIS values obtained on separated
test sets of 500 instances where the datasets were generated with the Friedman#1 and Friedman#2 data generating
processes (described in 8.2) (Friedman, 1991; Breiman, 1996). As the results in this example show, coverage and
lhnpe ≥ 45 and the coverage
MIS decrease by increasing K f
lhnpe ≥ 55. The results suggest that the
lhnpe ≥ 50 or K f
distribution begins to have smaller minimum values for K f
function LHNPE neighborhood may be between 30 and 50, so setting (Kmin
= 30, Kmax
= 40 may
lhnpe
lhnpe
be an acceptable solution.

lhnpe, the MIS average does not decrease much more for K f

As seen before, when the neighborhood size K f

LHNPE neighborhoods lead in larger tolerance interval sizes and thus a higher coverage.

lhnpe decreases the tolerance interval size increases, as a result small
lhnpe ≥ 20 the estimated neighborhood is enlarged with instances that generally satisfy the LHNPE
conditions, so the MIS decreases rapidly (tolerance intervals decrease faster than prediction intervals) while in the
same time, the coverage decreases and converges to the desired β. In fact, as long as γ and K f
lhnpe are chosen as
lhnpe nearest instances to x∗ are in its LHNPE neighborhood, tolerance intervals
recommended by Table 1, and the K f
of the estimated normal distribution are wider than its prediction intervals. On the other hand due to the limited
sample size, the number of instances in the training set that belong to the LHNPE neighborhood of x∗ usually changes
based on the location of x∗ in the feature space. Indeed, using a too large K f
lhnpe leads to considering far neighbors of
x∗, those not belonging to its LHNPE neighborhood, as if they were in so. If this happens for a signiﬁcant number
of tested instances during the hyper parameter tuning, we end up with many intervals being wider than necessary,
causing a large MIS. This diﬃculty in estimating the K f
lhnpe are caused by the heterogeneity and heteroscedasticity of
the underlying dataset, and directly inﬂuence the mentioned MIS and coverage variations. Therefore, we can state
lhnpe ≥ 20 generally decreases MIS without having a signiﬁcant impact on coverage, this
that: although increasing K f
situation usually changes after a threshold and the variation of coverage and MIS after the K f
lhnpe’s threshold depends
on the testing model.

lhnpe

In practice, evaluating the eﬃciency of both methods on datasets, and incorporating obtained a priori knowledge
lhnpe for F-BOPI method and when it comes to
lhnpe value

in the hyper-parameter tuning phase is suggested. One could ﬁnd K f
the ﬁnding (Kmin
found before.

lhnpe] interval such that it contains the ﬁxed K f

lhnpe), she can try to choose the [Kmin

lhnpe, Kmax

lhnpe, Kmax

lhnpe ≤ K f
Kmin

lhnpe ≤ Kmax
lhnpe.

Once K f

lhnpe, Kmax

lhnpe or the pair (Kmin

lhnpe) is found, next step is decreasing value of γ, which decreases the mean
interval size. The goal is to have the smallest mean tolerance interval size that satisﬁes coverage tuning constraint.
The idea is to set the value of the neighborhood parameters with those found in the previous process and decrease
γ. This procedure is repeated as long as the inclusion constraint is satisﬁed and γ is larger than its minimum value
shown in Table 1. High values of γ will guarantee the satisfaction of the coverage Tuning constraint but the computed
intervals can be very large. Note that, with this approach, the value of γ can be less than β and this may happen
when the local density of the response variable is quite high. Based on the new value of γ, one can go to the ﬁrst

17

(a) Friedman#1

(b) Friedman#1

(c) Friedman#2

(d) Friedman#2

Figure 5: The distribution of coverage and MIS of F-BOPI for γ = 0.9, β = 0.95 and K f
lhnpe varying from 20 to 70
by steps of 5. For each value of K f
lhnpe, 100 loess models are estimated, each trained and tested on separated datasets
generated by the Friedman#1 Friedman#2 data generating processes (described in 8.2) (Friedman, 1991; Breiman,
1996).

18

202530354045505560657084868890929496LHNPE Neighborhood SizeCoverage20253035404550556065701.41.51.61.71.8LHNPE Neighborhood SizeMIS2025303540455055606570859095LHNPE Neighborhood SizeCoverage202530354045505560657013141516LHNPE Neighborhood SizeMISstep and recalculate new values for the neighborhood hyper-parameter (K f
lhnpe)) and this can
be repeated for one or two iterations until the coverage Tuning constraint is satisﬁed and the obtained MIS change is
negligible.

lhnpe or the pair (Kmin

lhnpe, Kmax

Algorithm 1 Hyper-parameter tuning for prediction intervals with variable(cid:98)Klhnpe.

(cid:46) or γ ← 0.9 depending on the dataset

(cid:46) t value can be found in Table 1.
(cid:46) Goes outside of the loop

γ ← 0.99
lhnpe) ← (MINK0 , MAXK0) initial values
lhnpe, Kmax
(Kmin
λ ← (γ, Kmin
lhnpe, Kmax
lhnpe)
for iteration = 1..3 do
(Coverage, MIS ) ← ComputeOnTrainigSet(β, λ)
MIS min ← MIS .
while Coverage ≥ β and MIS ≤ MIS min do

lhnpe, Kmax

(Kmin
lhnpe, Kmax
λ ← (γ, Kmin
MIS min ← MIS
(Coverage, MIS ) ←ComputeOnTrainigSet(β, λ)

lhnpe) + somestep

end while
while Coverage ≥ β and MIS ≤ MIS min do

lhnpe) ← (Kmin
lhnpe, Kmax
lhnpe)

1: function TuneHyper-Params(error set, β)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25: end function
26:
27: function ComputeOnTrainigSet(β, λ)
28:

γ ← γ − step
if γ < t then

end for
return (Kmin

lhnpe, Kmax

end while

lhnpe, γ)

Break;

end if
λ ← (γ, Kmin
MIS min ← MIS
(Coverage, MIS ) ←ComputeOnTrainigSet(β, λ)

lhnpe, Kmax
lhnpe)

schema.

Use Equations (23) and (24) to obtain BOPI intervals on the training set with a LOO or 10-fold cross-validation
Coverage ← use Equation (14) on the intervals calculated in the previous step.
MIS ← compute the mean size of these intervals found above.
return (Coverage, MIS )

29:
30:
31:
32: end function

8 Experiments

In this section, several artiﬁcial and real datasets are used to compare the introduced prediction intervals methods
for local linear regression described in Section 6) with the conventional prediction intervals described by Equation (3),
the linear prediction intervals and SVM quantile regression. The selected methods will be tested upon their capacity to
provide two-sided β-content prediction intervals. The estimated prediction intervals are compared for their reliability
and eﬃciency of their obtained envelope as described in Section 4. Note that we are interested in comparing the
aforementioned methods, regardless of any variable selection or outliers detection pre-processing.

19

8.1 Prediction Intervals Methods

This part involves the description of the tested prediction intervals. The numerical study in 8.2 uses three of
these methods (F-BOPI, A-BOPI and Loess Conv.), whereas Section 8.4 reports the application of all of them on real
datasets.

8.1.1 Method’s Implementation

The tested methods are the followings:
• F-BOPI: two-sided prediction interval for linear loess as explained in Section 5 with the ﬁxed K LHNPE neigh-
lhnpe). The prediction intervals are obtained on the same estimated linear loess model as A-BOPI

borhood (Ka
and Loess Conv. F-BOPI hyper-parameters values for the real datasets can be found in Tables 3 and 4.

• A-BOPI: two-sided prediction interval for linear loess as explained in Section 5 with the variable K LHNPE
neighborhood (K f
lhnpe). The prediction intervals are obtained on the same estimated linear loess model as A-
BOPI and Loess Conv. The A-BOPI hyper-parameters values for the real datasets can be found in Tables 3 and
4.

• Loess Conv.

the conventional interval prediction method explained by Equation 3 obtained with the same
estimated linear loess model as F-BOPI and values of the estimated Kloess for the real datasets can be found in
Table 2.

• OLS prediction intervals for classical linear regression (Ordinary Least-Squares) obtained by:

(cid:115)

ˆf (x) ± ct(1− 1−β

2 ,N−p), c =

N ˆσ2
N − p

(1 + x∗T (XT X)−1x∗)

(28)

where p and ˆσ2 are respectively the number of independent variables plus one, and the estimated variance of
the error term (Rao and Toutenburg, 1999).

• LS-SVM Conv.: the conventional interval prediction method explained in Equation 3 obtained with a least-
square SVM regression. We used the ksvm function in R’s kernlab package. This function is used with the
following arguments: kernel=“rbfdot” (for a radial basis kernel function), kpar= “automatic” (default value for
radial basis functions), tau = 0.01, cross=10, reduced = TRUE, tol = 0.0001.

• SVM Quantile: two-sided interval prediction by two SVM quantile regression models (Takeuchi et al., 2006).
For this purpose, one must build two distinct quantile regression models: a lower ( 1−β
2 )-quantile regression
model and an upper (1 − ( 1−β
2 ))-quantile regression model. This method’s hyper-parameter minimizes the Pin-
ball loss function with a 10-fold CV on the training set. This method is implemented by the kqr function in R’s
kernlab package. kqr is used with the following arguments: kernel=“rbfdot” (radial basis kernel function),
kpar= “automatic” (default value for radial basis functions), the cost regularization parameter is set between 3.8
and 5, depending on the dataset; its values for the real datasets can be found in Table 2.

• SVM Quantile CV : two-sided interval prediction by two SVM quantile regression models (Takeuchi et al.,
2 )-quantile
2006). This method is similar to SVM Quantile mentioned above.
regression model and an upper (1 − ( 1−β
2 ))-quantile regression model. The “NPQR CV” hyper-parameters are
tuned in a way to ﬁnd intervals that, in a 10-fold CV on the training set, have the smallest MIS and satisfy the
tuning coverage constraint. We use the kqr function in R’s kernlab package with the following arguments:
kernel=“rbfdot” (radial basis kernel function), kpar= “automatic” (default value for radial basis functions), the
cost regularization parameter is chosen to lie 0.05 and 0.2, depending on the dataset; its values for the real
datasets can be found in Table 2. Satisfying the tuning coverage constraint on the training set requires us to
select small values of cost regularization parameters.

It also requires a lower ( 1−β

Tricube kernel, as in (Cleveland and Devlin, 1988), is the kernel function used in all local linear models above.

20

8.1.2 Hyper-parameter tuning for real datasets

In a ﬁrst attempt, datasets are divided into two sub-samples of size 2

3 N, where N represents the dataset
3 of observations is used to tune the estimated model’s hyper-parameters. Then, all of the

size. The part containing 2
instances serve to validate the results using a 10-cross validation scheme.

3 N and 1

Once the optimal value of Kloess has been found for each dataset, the aforementioned tuning strategy is used
to ﬁnd the prediction intervals hyper-parameters. Linear loess regression uses the Kloess-nearest neighbors as the
bandwidth. This Kloess is found by minimizing the 10-fold cross validation error on the training set; its values for the
real datasets can be found in Table 2. For more details about linear loess see Section 3.2. Tables 2, 3 and 4 show the
hyper-parameters values for the methods described in Section 8.1.1.

8.2 Simulations

This part, compares the BOPI methods for local linear regression in Section 6 with the conventional prediction
intervals described by Equation (3) on two artiﬁcial benchmark data generating process (DGP) Friedman#1 DGP and
Friedman#2 DGP (Friedman, 1991; Breiman, 1996), also available in mlbench package of R.

The results are based on a 3 fold cross-validation schema where 2
3 of the generated sample is taken as training set
and 1
3 as validation set. The method is applied to these simulated samples and computed results, that is coverage and
MIS are reported. For simplicity and based on some experience (see Section 7.2) the methods’ hyper-parameters are
selected as follows: Kloess = 100 as the regression bandwidth; it is constant for the three methods, K f
= 40 for
F-BOPI and (Kmin
lhnpe

= 50) for A-BOPI.

= 30, Kmax
lhnpe

lhnpe

Friedman#1 DGP is consisted of 10 independent predictors, x = (x1, . . . , x10), uniformly distributed over [0, 1]

and the response variable is given by:

Y(x) = 10sin(πx1x2) + 20(x3 − 0.5)2 + 10x4 + 5x5 + ε, ε ∼ N(0, 1).

Friedman#2 DGP response is is given by:

Y(x) =

+

x2x3 − 1
x2x4

(cid:32)

x2

1

(cid:33)2 1

2

+ ε, ε ∼ N(0, 125).

It is consisted of 4 independent predictors, x = (x1, . . . , x4), uniformly distributed over:

0 ≤ x1 ≤ 100
40π ≤ x2 ≤ 560π
0 ≤ x3 ≤ 1
1 ≤ x4 ≤ 11

Table A.8 reports coverage and MIS of the tested methods on 500 samples of 1500 observations each of them
generated by Freidman#1 with β = 0.95. The whole process is iterated for γ = 0.8, 0.9, 0.95 and 0.99 and the
average results and their standard errors are reported. As one could observe, while the conventional method’s coverage
is always signiﬁcantly less than the desired content (β = 0.95), the coverage rates of the BOPI methods are not
signiﬁcantly diﬀerent from the desired content, except when γ = 0.8. These results show that by increasing γ, coverage
increases which by its turn increases MIS substantially. This latter is in the same line of our method’s description in
Section 6. Note that F-BOPI is always more reliable than A-BOPI Table A.10 reports the same simulation experiment
for Friedman#2 DGP. The results are generally the same as above except that coverage does not exceed the desired
content β and coverages’ standard error are higher.

Table A.12 and Table A.14 report coverage of the tested methods on samples of diﬀerent size (1500 and 3000)
generated respectively by Friedman#1 and Friedman#2 with β = 0.8, 0.9, 0.95 and 0.99 and γ = 0.99. While consider-
ing Table A.12, one could note that: the coverage of the conventional method is always signiﬁcantly below the desired
content β, the coverage of F-BOPI is always a bit higher than β and the coverage of A-BOPI is always a bit lower than

21

β. Doubling the sample size from 1500 observations to 3000 observations does not alter the results signiﬁcantly, and
as expected, doubling the iteration steps lessens standard errors of coverage rates. Reported coverages in Table A.14
are generally lower than the desired content β. Results shows that all methods yield coverages lower than the desired
β, although the BOPI’s coverages are much more closer to β than the conventional method’s coverage. Doubling the
sample size does not alter the results, but doubling the iterations from 500 to 1000 steps worsens the results and yield
higher standard errors. Since it happens for all the three methods, it could be due to the fact that the local linear
regression with chosen hyper-parameters is not a suitable method for capturing Friedman#2 DGP characteristics.

8.3 Real Datasets

Eleven benchmark datasets are considered to compare the methods. The datasets are chosen from the UCI repos-
itory (Frank and Asuncion, 2010), Delve dataset repository (Delve Development Group, 2003) and a well-known
article on non-parametric regression (Silverman, 1985). The UCI repository datasets are also documented and avail-
able in R’s mlbench package. Datasets were chosen to cover small and moderate size datasets. The dataset sizes
vary from N = 103 (Slump) to N = 8192 (Computer), and the number of regressors vary from p = 1 (Motorcycle) to
p = 21 (Parkinson1). Some of the datasets contain only numerical variables and some datasets have numerical and
categorical variables. Instances with missing values are omitted.

These datasets are listed below (where we can ﬁnd each dataset name in double quotes and its abbreviation in
parentheses, their numbers of predictor and number of instances, respectively denoted by p and N). Note that some of
these datasets have fewer variables than their source because we systematically removed any instances having null val-
ues. The “Parkinsons Telemonitoring” dataset (Frank and Asuncion, 2010) contains two regression variables named
“motor UPDRS” and “total UPDRS”. We considered it as two distinct datasets named “Parkinson1” and “Parkin-
son2”. Each dataset has one of the “motor UPDRS” or “total UPDRS” variables.

• “Computer Activity” (Computer) (Delve Development Group, 2003). We used the small variant of this dataset

which contains only 12 of the 32 attributes. N = 8192, p = 12.

• “Bank” (Bank) (Delve Development Group, 2003). We used the 8nm variant of this dataset, which just contains

8 of the 32 attributes, and is highly non-linear with moderate noise. N = 8192, p = 8.

• “Parkinsons Telemonitoring” (Parkinson1) (Frank and Asuncion, 2010). We removed “motor UPDRS” variable

and left “total UPDRS” as the response variable. N = 5875, p = 21.

• “Parkinsons Telemonitoring” (Parkinson2) (Frank and Asuncion, 2010). We removed the “total UPDRS” vari-

able and left “motor UPDRS” as the response variable. N = 5875, p = 21.

• “Abalone” (Abalone) (Yeh, 2007). N = 4177, p = 10.
• “Concrete Compressive Strength” (Concrete) (Yeh, 1998). N = 1030, p = 9.
• “Boston Housing” (Housing) (Frank and Asuncion, 2010). N = 506, p = 14.
• “Auto MPG” (Auto) (Frank and Asuncion, 2010). N = 392, p = 8.
• “CPU”(CPU) (Frank and Asuncion, 2010). N = 209, p = 7.
• “Concrete Slump Test” (Slump) (Yeh, 2007). N = 103, p = 10.
• “Motorcycle” (Motorcycle) (Silverman, 1985). N = 133, p = 1.

22

8.4 Results on Real Datasets

The goal of this section is to compare the above-mentioned interval prediction methods based on their strength
while providing β-content prediction intervals. The models are compared based on reliability and eﬃciency of their
envelope. A-BOPI and F-BOPI methods are used to obtain prediction intervals for Local Linear Regression (LLR).
Consequently, we compare those methods with the conventional prediction intervals on the local linear regression
(Loess Conv.) and other prediction intervals stated above. For this purpose, we will use Tables 5 and 6 which compare
Loess Conv., A-BOPI, F-BOPI, LS-SVM Conv. and OLS. For each dataset, we build a unique linear loess models,
then we apply on this estimated model the BOPI methods and the conventional method. So the only diﬀerence between
the results obtained with prediction intervals for linear loess models (A-BOPI, F-BOPI and linear loess) is due to their
prediction interval method and not the regression model. Tables 5 and 6 provide detailed experimental results. For
the sake of clarity and ease of interpretation, diﬀerent charts are drawn to compare all of the prediction intervals. This
comparison measures a method’s strength, while providing β-prediction interval with β = 0.8, 0.9, 0.95 and 0.99.

8.4.1 Comparing Methods by Tables

Outliers, limited number of observations and contrast between assumptions and the true regression function are
among potentials cause of errors in the prediction process. These errors occur in a similar manner when estimating
the response variable distribution and they increase with β. For β = 0.9, 0.95, and particularly for β = 0.99, it becomes
a critical task to ﬁnd an eﬃcient interval prediction procedure that is able to ﬁnd an upper bound of Y(x). However
these inter-quantiles are the most used ones in machine learning and statistical hypothesis-testing. Hence, we will
compare the methods based on their strength, while providing β-prediction intervals with β = 0.8, 0.9, 0.95 and 0.99.
Tables 5 and 6 display the direct dataset measures explained in Section 4, for each dataset. These tables compare
ﬁve diﬀerent models: Loess Conv., A-BOPI, F-BOPI, LS-SVM Conv. and OLS. For each dataset of the 11 benchmark
datasets described in Section 8.3, we have to estimate 20 models, (5 methods × 4 β’s value).

8.4.2 Table description

In Tables 5 and 6, each cell represents a combination of dataset and β which displays F0.05

β,N for the underlying ex-
periment. The F0.05
β,N column represents the Wilson Score critical value for a binomial proportion test at a signiﬁcance
level of 0.05 and the alternate hypothesis as βpop < β, where βpop denotes the average proportion of response values
that are contained in the tested prediction intervals. So, the null hypothesis claims that the constructed prediction
intervals cover on average a proportion βpop of response values and βpop is greater than or equal to the desired pro-
portion β. In order to test each model reliability, its coverage value is compared with its corresponding critical value
F0.05
β,N , and if coverage < F0.05
β,N , it means that the null hypothesis is rejected, with a signiﬁcance level of at most 0.05.
In such cases, we consider the model as unreliable.

Tables 5 and 6 illustrate the coverage probability of the ﬁve diﬀerent models stated before.

If the computed
β,N , the model is considered as non-reliable and this is indicated by legend (cid:74) or (cid:67) next to the
coverage is less than F0.05
coverage. When there is only one non-reliable model, the legend (cid:74) is used and when there are more than one non-reliable model,
the legend (cid:67) is used. For each experiment the reliable model having the smallest MIS is written in bold. Two-sided paired t-tests
at levels 0.05, 0.01 and 0.001 are used to compare the interval size of the two estimated models which ﬁnd the smallest MIS and
are not rejected for the Wilson Score binomial proportion test at level 0.05 (reliability test). *, ** and *** signs indicate that the
two-sided paired t-tests are respectively statistically signiﬁcant at levels 0.05, 0.01 and 0.001. Although less conservative results
are obtained by Wilcoxon signed rank test, the results are similar. A-BOPI value of Kmin
and γ are given Tables 3 and 4.

lhnpe

lhnpe, Kmax

lhnpe and γ and F-BOPI value for K f

23

Dataset

Computer

Bank

Parkinson1

Parkinson2

Abalone

Concrete

Housing

Auto

CPU

Slump

Motorcycle

“SVM Quantile ” C

“SVM Quantile CV” C

“Loss Conv.” Kloess

5

4.2

5

5

4

4

4.5

3.8

4

4.5

4

0.15

0.2

0.2

0.1

0.2

0.1

1

0.2

0.2

0.05

0.1

500

500

80

70

700

80

60

30

40

30

15

Table 2: Hyper-parameter values for prediction intervals with SVM Quantile, SVM Quantile CV and Loss Conv.

Dataset

Computer

Bank

Parkinson1

Parkinson2

Abalone

Concrete

Housing

Auto

CPU

Slump

Motorcycle

F-BOPI

K f
lhnpe
K f
lhnpe
K f
lhnpe
K f
lhnpe
K f
lhnpe
K f
lhnpe
K f
lhnpe
K f
lhnpe
K f
lhnpe
K f
lhnpe
K f
lhnpe

= 40, γ = 0.8

= 40, γ = 0.8

= 40, γ = 0.9

= 50, γ = 0.9

= 40, γ = 0.7

= 35, γ = 0.5

= 40, γ = 0.9

= 50, γ = 0.9

= 40, γ = 0.9

= 20, γ = 0.5

= 35, γ = 0.55

A-BOPI

lhnpe , Kmax
(Kmin
(Kmin
lhnpe , Kmax
(Kmin
lhnpe , Kmax
(Kmin
lhnpe , Kmax
lhnpe , Kmax
(Kmin
(Kmin
lhnpe , Kmax
(Kmin
lhnpe , Kmax
(Kmin
lhnpe , Kmax
lhnpe , Kmax
(Kmin
(Kmin
lhnpe , Kmax
(Kmin
lhnpe , Kmax

lhnpe , γ) = (30, 50, 0.8)
lhnpe , γ) = (30, 50, 0.8)
lhnpe , γ) = (20, 60, 0.9)
lhnpe , γ) = (30, 60, 0.9)
lhnpe , γ) = (30, 50, 0.7)
lhnpe , γ) = (20, 60, 0.9)
lhnpe , γ) = (30, 55, 0.9)
lhnpe , γ) = (30, 60, 0.9)
lhnpe , γ) = (20, 50, 0.9)
lhnpe , γ) = (15, 30, 0.5)
lhnpe , γ) = (20, 35, 0.55)

Table 3: Hyper-parameter values for BOPI methods with β = 0.8, 0.9 .

Dataset

Computer

Bank

Parkinson1

Parkinson2

Abalone

Concrete

Housing

Auto

CPU

Slump

Motorcycle

F-BOPI

K f
lhnpe
K f
lhnpe
K f
lhnpe
K f
lhnpe
K f
lhnpe
K f
lhnpe
K f
lhnpe
K f
lhnpe
K f
lhnpe
K f
lhnpe
K f
lhnpe

= 40, γ = 0.9

= 40, γ = 0.9

= 40, γ = 0.99

= 50, γ = 0.99

= 40, γ = 0.9

= 35, γ = 0.55

= 40, γ = 0.9

= 50, γ = 0.99

= 40, γ = 0.99

= 20, γ = 0.9

= 35, γ = 0.7

A-BOPI

(Kmin
lhnpe , Kmax
(Kmin
lhnpe , Kmax
(Kmin
lhnpe , Kmax
(Kmin
lhnpe , Kmax
(Kmin
lhnpe , Kmax
(Kmin
lhnpe , Kmax
(Kmin
lhnpe , Kmax
(Kmin
lhnpe , Kmax
(Kmin
lhnpe , Kmax
(Kmin
lhnpe , Kmax
(Kmin
lhnpe , Kmax

lhnpe , γ) = (30, 50, 0.9)
lhnpe , γ) = (30, 50, 0.9)
lhnpe , γ) = (20, 60, 0.99)
lhnpe , γ) = (30, 60, 0.99)
lhnpe , γ) = (30, 50, 0.9)
lhnpe , γ) = (20, 60, 0.9)
lhnpe , γ) = (30, 50, 0.99)
lhnpe , γ) = (30, 60, 0.99)
lhnpe , γ) = (20, 50, 0.99)
lhnpe , γ) = (15, 30, 0.9)
lhnpe , γ) = (20, 35, 0.7)

Table 4: Hyper-parameter values for BOPI methods with β = 0.95, 0.99.

24

Dataset

Method

Computer

Loess Conv.

LS-SVM Conv.

Bank

Parkinson1

Parkinson2

Abalone

OLS

F-BOPI

A-BOPI

Loess Conv.

LS-SVM Conv.

OLS

F-BOPI

A-BOPI

Loess Conv.

LS-SVM Conv.

OLS

F-BOPI

A-BOPI

Loess Conv.

LS-SVM Conv.

OLS

F-BOPI

A-BOPI

Loess Conv.

LS-SVM Conv.

OLS

F-BOPI

A-BOPI

Concrete

Loess Conv.

LS-SVM Conv.

Housing

Auto

CPU

OLS

F-BOPI

A-BOPI

Loess Conv.

LS-SVM Conv.

OLS

F-BOPI

A-BOPI

Loess Conv.

LS-SVM Conv.

OLS

F-BOPI

A-BOPI

Loess Conv.

LS-SVM Conv.

OLS

F-BOPI

A-BOPI

β = 0.8

β = 0.9

Cover

MIS (σis)

F0.05
0.8,N

Cover

MIS (σis)

F0.05
0.9,N

82.99

93.31

94

85.22

82.74

83.05

87.12

86.85

84.88

82.38

90.93

83.33

80.42

91.55

88.55

91.48

83.14

77.64(cid:74)

91.46

89.08

83.69

86.54

85.46

84.4

82.1

81.06

82.22

80.28

82.61

83.68

86.18

92.08

86.16

87.97

84.59

84.96

85.72

83.16

87.77

83.17

86.09

96.16

89.47

85.16

80.37

6.64 (0.03)

25.19 (0.16)

12.77 (0.16)

6.8 (2.9)

6.34*** (2.72)

0.05 (0.001)

0.04 (0.001)

0.08 (0.0001)

0.04 (0.01)

0.04 (0.01)

6.81 (0.16)

13.89 (0.012)

23.79 (0.14)

5.48 (4.4)

4.39*** (3.78)

5.21 (0.14)

9.96 (0.1)

18.49 (0.1)

4.2 (3.22)

3.52*** (2.95)

5.14 (0.02)

5.53 (0.02)

5.63 (0.04)

5.19 (1.76)

4.81*** (1.65)

17.1 (0.23)

17.08 (0.26)

26.8 (0.16)

16.76 (5.73)

17.03 (5.91)

8.13 (0.32)

10.17 (0.41)

12.36 (0.25)

8.67 (3.31)

7.8** (2.8)

7.33 (0.3)

7.04 (0.16)

8.64 ( 0.1)

7.92 (3.21)

7.03 (2.83)

123.68 (15.13)

302.38 (17.58)

156.15 (9)

88.07 (64.23)

78.49** (59.2)

79.27

79.27

79.14

79.14

78.98

77.94

76.67

77.07

75.44

89.45

89.45

89.35

89.35

89.23

88.46

87.5

87.8

86.58

8.52 (0.04)

32.34 (0.28)

16.39 (0.21)

8.73 (3.72)

8.13*** (3.49)

0.06 (0.001)

0.06 (0.001)

0.1 (0.0001)

0.06 (0.02)

0.05***(0.02)

6.81 (0.16)

17.83 (0.15)

30.54 (0.0001)

7.04 (5.64)

5.64*** (4.85)

6.69 (0.19)

12.79 (0.13)

23.74 (0.13)

5.4 (4.14)

4.52*** (3.79)

6.6 (0.02)

7.1 (0.03)

7.22 (0.05)

6.67 (2.26)

6.18*** (2.12)

21.95(0.3)

21.93 (0.33)

34.41 (0.21)

21.52* (7.36)

22.2 (7.59)

10.43 (0.41)

13.05 (0.53)

15.87 (0.32)

11.14 (4.25)

10.01** (3.6)

9.41 (0.38)

9.03 (0.21)

11.1 (0.13)

10.17 (4.12)

9.02* (3.64)

158.75 (19.42)

388.11 (22.56)

200.71 (11.57)

113.04 (82.44)

100.75** (20.89)

89.36(cid:74)

96.56

96.56

92.13

90.31

89.94

91.66

93.24

92.29

90.48

90.93

89.18(cid:74)

90.93

94.88

92.81

93.86

89.49

91.15

94.64

93.03

90.03

91.59

91.18

91.75

89.91

88.73

90.19

89.7

91.45

93

91.31

94.46

92.67

92.7

91.72

90.57

93.37

91.82

94.15

90.83

91.37

96.63

93.78

91.4

88.97

25

Dataset

Method

β = 0.8

β = 0.9

Cover

MIS (σis)

F0.05
0.8,N

Cover

MIS (σis)

F0.05
0.9,N

Slump

Loess Conv.

LS-SVM Conv.

OLS

F-BOPI

A-BOPI

Motorcycle

Loess Conv.

LS-SVM Conv.

OLS

F-BOPI

A-BOPI

85.72

87.36

84.63

85.72

83.81

78.84

84

78.95

88.67

85.6

4.7 (0.4)

7.55 (0.68)

6.72 ( 0.23)

4.85 (1.41)

4.32** (1.24)

57.82 (1.22)

64.61 (4.13)

120.44 (2.86)

65.7 (17.35)

57.16 (22.31)

73.51

74.29

89.54

92.18

89.45

88.54

87.63

89.5

90.16

88.67

94.77

94

6.03 (0.52)

9.69 (0.88)

8.66 (0.3)

6.23 (1.81)

5.55** (1.6)

74.21 (1.57)

82.92 (5.3)

154.93 (2.39)

73.36 (28.63)

72.82 (32.44)

85.13

85.72

Table 5: Prediction intervals for the linear loess regression model with prediction intervals built on benchmark datasets
with desired contents β = 0.8 and β = 0.9. Loess Conv. described by Equation (3), F-BOPI and A-BOPI described in
Section 7 are used to obtain prediction intervals on the same linear loess model. If the computed coverage probability
β,N the model is considered as non-reliable, and it is indicated by legend (cid:74) or (cid:67) next to the coverage.
is less than F0.05
When there is only one non-reliable model, the legend (cid:74) is used and when there are more than one non-reliable model, the legend
(cid:67) is used. For each experiment the reliable model having the smallest MIS is written in bold. The *, ** and *** signs indicate
that the diﬀerence between the bold MIS (smallest reliable) and the second smallest reliable MIS were statistically signiﬁcant at
respectively 0.05, 0.01 and 0.001 level with a two-sided paired t-test.

8.4.3 Table commentaries

By looking at Tables 5 and 6, one could see that almost all method’s reliability test (except one case for OLS) are not rejected
for β = 0.8 on all benchmark datasets. When the desired proportion is 0.8, A-BOPI is the most eﬃcient method (in EGSD sense)
and then F-BOPI and Loess Conv. are approximately equally eﬃcient. LS-SVM Conv. and OLS result respectively the largest and
the second largest Mean Interval Size (MIS).

When β is equal to 0.9, all methods, except two cases (Loess Conv. for Computer dataset and LS-SVM Conv. for Parkinson1
dataset), result in reliable prediction intervals. In these two cases, the obtained coverage is smaller than the F0.05
0.9,N column which
represents the Wilson Score critical value for a binomial proportion test at a signiﬁcance level of 0.05 described in Section 8.4.2.
This means that the null hypothesis of the binomial proportion test of βpop ≥ 0.9, where βpop denotes the average proportion of
response value that are contained in the tested prediction intervals, are rejected for two cases (Loess Conv. for Computer dataset
and LS-SVM Conv. for Parkinson1 dataset), at a signiﬁcance level of 0.05.

When the desired proportion increases to 0.95, results are the same for all of the estimated models by A-BOPI and F-BOPI;
their reliability tests are not rejected (they are reliable). However Loess Conv. and LS-SVM Conv. have a lower reliability when
β = 0.8, 0.9, and they respectively have coverage < F0.05
0.95,N in three and four cases. The reliability test on the conventional methods,
Loess Conv. and LS-SVM Conv., are rejected (lead to unreliable estimated models) on larger datasets. So the failure of rejection
on the smaller datasets may be caused by a lack of suﬃcient observations rather than reliable prediction intervals (the power of the
reliability test increases with sample size).

When it comes to the comparison of MIS, A-BOPI remains the most eﬃcient4 method, Loess Conv. becomes the second
eﬃcient method and F-BOPI and LS-SVM Conv. produce similar results. The desired proportion of 0.99 is the most diﬃcult one

4Note that we compare the Mean Interval Size (MIS) of reliable models, because it makes not a lot of sense to compare the MIS of a reliable
method with a non-reliable one. It generally obtains a reliable estimated model with the smallest MIS and its diﬀerence with the second smallest
MIS (which must be a reliable estimated model) is usually statistically signiﬁcant at 0.05 level with a two-sided paired t-test. Moreover, the EGSD
measure (introduced in Section 4.3) are used in Section 8.4.4 to compare the eﬃciency of interval prediction methods indecent of their reliability
results.

26

Dataset

Method

Computer

Bank

Loess Conv.

LS-SVM Conv.

OLS

F-BOPI

A-BOPI

Loess Conv.

LS-SVM Conv.

OLS

F-BOPI

A-BOPI

Cover

93.1(cid:74)

97.39

96.72

96.41

95.46

93.65(cid:67)
94.13(cid:67)

95.84

96.27

95.3

Parkinson1

Loess Conv.

LS-SVM Conv.

95.26

92.62(cid:74)

Parkinson2

Abalone

Concrete

Housing

Auto

CPU

OLS

F-BOPI

A-BOPI

Loess Conv.

LS-SVM Conv.

OLS

F-BOPI

A-BOPI

Loess Conv.

LS-SVM Conv.

OLS

F-BOPI

A-BOPI

Loess Conv.

LS-SVM Conv.

OLS

F-BOPI

A-BOPI

Loess Conv.

LS-SVM Conv.

OLS

F-BOPI

A-BOPI

Loess Conv.

LS-SVM Conv.

OLS

F-BOPI

A-BOPI

Loess Conv.

LS-SVM Conv.

OLS

F-BOPI

A-BOPI

95

97.61

96.31

95.46

93.38(cid:74)

97.08

97.4

96.35

93.17(cid:67)
93.96(cid:67)
93.84(cid:67)

96.09

94.89

94.36

94.36

94.65

95.62

95.72

94.26

95.64

95.24

95.45

96.24

93.88

95.92

94.89

97.45

96.95

92.82

97.11

94.25

96.16

94.25

β = 0.95

β = 0.99

MIS (σis)

F0.05
0.95,N

Cover

MIS (σis)

F0.05
0.99,N

94.60

94.60

94.53

94.53

94.45

93.88

93.18

10.15 (0.05)

19.53 (0.25)

38.54 (0.34)

10.99 (4.68)

10.22 (4.38)

0.08 (0.001)

0.07 (0.001)

0.12 (0.0001)

0.07 (0.02)

0.07 (0.02)

10.41 (0.24)

21.25 (0.18)

36.39 (0.21)

9.63 (7.7)

7.72*** (6.51)

7.97 (0.22)

15.24 (0.15)

28.29 (0.16)

7.26 (5.57)

6.1*** (5.07)

7.86 (0.03)

8.47 (0.04)

8.61 (0.06)

8.72 (2.95)

8.06*** (2.74)

26.15 (0.35)

26.13 (0.4)

41.01 (0.25)

25.64* (8.77)

26.46 (9.04)

12.43 *** (0.48)

15.55 (0.63)

18.93 (0.38)

13.27 (5.07)

13.8 (5.01)

11.21 (0.45)

10.76*** (0.25)

93.4

13.23 (0.16)

13.68 (5.54)

12.2 (4.87)

189.16 (23.15)

462.46 (26.89)

239.54 (13.81)

154.67 (112.8)

137.68 (101.75)

92.52

97.15(cid:67)
98.5(cid:67)
97.27(cid:67)
98.64(cid:67)
98.09(cid:67)
97.39(cid:67)
96.99(cid:67)
97.99(cid:67)
98.44(cid:67)
97.93(cid:67)
96.96(cid:67)
97.4(cid:67)

99.78

98.74(cid:67)
98.08(cid:67)
97.04(cid:67)
97.42(cid:67)

99.91

98.64(cid:67)
98.13(cid:67)
96.95(cid:67)
96.76(cid:67)
97.14(cid:67)
98.15(cid:67)
97.64(cid:67)

98.82

97.46(cid:74)

99.21

99.02

99.02

97.42(cid:67)
96.83(cid:67)
97.03(cid:67)

13.34 (0.07)

25.67 (0.33)

50.65 (0.45)

14.44 (6.15)

13.43 (5.76)

0.1 (0.001)

0.09 (0.001)

0.16 (0.0001)

0.1 (0.03)

0.09 (0.03)

13.68 (0.32)

27.93 (0.24)

47.84 (0.28)

12.66 (10.2)

10.15 (8.56)

10.48 (0.3)

20.03 (0.2)

37.18 (0.21)

9.54 (7.32)

8.02 (6.76)

10.33 (0.04)

11.13 (0.05)

11.32 (0.08)

11.47 (3.88)

10.59 (3.61)

34.37 (0.47)

34.34 (0.52)

53.94 (0.33)

33.7* (11.53)

34.77 (11.88)

16.34 (0.64)

20.44 (0.83)

24.92 (0.5)

98.61

17.44*(6.66)

98.61

97.2(cid:67)

98.73

97.44(cid:67)

98.71

98.71

18.14 (6.58)

14.74 (0.6)

14.15*** (0.33)

17.43 (0.21)

17.98 (7.28)

16.03 (6.4)

248.6 (30.42)

96.66(cid:67)
97.59(cid:67) 607.77 (35.34)
96.16(cid:67) 316.03 (18.22)

98.81

98.81

98.78

98.78

98.74

98.49

98.17

98.27

97.86

98.07

203.27 (148.24)

96.64(cid:67) 180.95 (133.72)

27

Dataset

Method

β = 0.95

β = 0.99

Cover

MIS (σis)

F0.05
0.95,N

Cover

MIS (σis)

F0.05
0.99,N

Slump

Motorcycle

Loess Conv.

91.45

7.19*** (0.62)

LS-SVM Conv.

OLS

F-BOPI

A-BOPI

Loess Conv.

LS-SVM Conv.

OLS

F-BOPI

A-BOPI

91.46

91.89

96

93.36

97.18

96.27

93.23

92.46

93.18

96.31

96.31

11.55 (1.05)

10.36 (0.36)

9.35 (2.72)

8.16 (2.25)

88.43 (1.87)

98.81 (6.32)

185.07 (2.86)

105.7 (27.92)

92.59 (35.9)

94.45(cid:74)

98

99

98.09

98.09

98.51

97.74

99.23

100

100

9.45 (0.82)

15.18 (1.38)

13.73 (0.47)

12.29 (3.57)

10.73*** (2.96)

116.22 (2.46)

129.86 (8.3)

244.68 (3.79)

138.91 (36.7)

121.69 (47.18)

97.38

97.58

Table 6: Prediction intervals for the linear loess regression model with prediction intervals built on benchmark datasets
with desired content β = 0.95 and β = 0.99. Loess Conv. described by Equation (3), F-BOPI and A-BOPI described in
Section 7 are used to obtain prediction intervals on the same linear loess model. If the computed coverage probability
β,N the model is considered as non-reliable, and it is indicated by legend (cid:74) or (cid:67) next to the coverage.
is less than F0.05
When there is only one non-reliable model, the legend (cid:74) is used and when there are more than one non-reliable model, the legend
(cid:67) is used. For each experiment the reliable model having the smallest MIS is written in bold. The *, ** and *** signs indicate
that the diﬀerence between the bold MIS (smallest reliable) and the second smallest reliable MIS were statistically signiﬁcant at
respectively 0.05, 0.01 and 0.001 level with a two-sided paired t-test.

to satisfy. In this case, while F-BOPI is the most reliable method and A-BOPI and OLS are the second most reliable methods,
F-BOPI and A-BOPI intervals are much tighter in mean than the OLS ones.

We conclude this comment by stressing that all the models given by A-BOPI and F-BOPI are reliable for β = {0.8, 0.9, 0.95}.
These methods also provide tighter reliable estimated models than others on the inquired datasets. By looking at Tables 5 and 6, one
could see that A-BOPI usually provides a reliable model with the smallest MIS and F-BOPI usually gives a larger coverage than
A-BOPI. One could also see that when A-BOPI and F-BOPI are reliable (reliability test not rejected), the conventional estimated
models could be not reliable and not the other way around.

8.4.4 Comparing Methods by Charts

Figures A.6a, A.6b, A.6c and A.6d are coverage charts for our experiments on the benchmark datasets and they compare the
coverage of the seven prediction intervals (Loess Conv., A-BOPI, F-BOPI, LS-SVM Conv., OLS, Loess Conv., SVM Quantile and
SVM Quantile CV) described in Section 8.1.1.

Figures A.7, A.8, A.9 and A.10 display EGSD charts for our experiments on the benchmark datasets and they compare the eﬃ-
ciency of the seven aforementioned prediction intervals while ignoring their reliability. These EGSD charts display the normalized
EGSD measure described by Equation (15) for all benchmark datasets. For a given dataset, the model having the lowest EGSD
value has an Equivalent Gaussian distribution with the smallest variance, which means that it is the most eﬃcient model compared
to the others.

8.4.5 Chart commentaries
coverage charts (Figure A.7, A.8, A.9 and A.10), show that SVM Quantile and SVM Quantile CV always obtain coverage
smaller than the desired one. For β = {0.8, 0.9} the two methods that usually obtain the larger coverage are respectively LS-SVM
Conv. and F-BOPI and other methods are on average similar. This order changes for β = {0.95, 0.99} with F-BOPI having usually
the larger coverage and no real ordering for other methods.
Figures A.7, A.8, A.9 and A.10 show respectively the EGSD chart for β = {0.8, 0.9, 0.95, 0.99}. One can observe that A-BOPI
and F-BOPI models are almost always more eﬃcient than the others on the inquired datasets. If we look in more detail, we can
see that A-BOPI usually ﬁnds the smallest EGSD value and the conventional method Loess Conv. is the next eﬃcient one. It is
interesting to note that while A-BOPI provides reliable models, it has the lowest EGSD (more eﬃcient) compared to other methods.

28

8.5 Discussion of Results

The introduced methods are compared with the conventional prediction intervals. This comparison is performed with sim-
ulation studies on two artiﬁcial data generating process (DGP) and a 10-fold cross validation schema on eleven real benchmark
regression datasets with sizes and number of independent variables varying respectively from N = 103 to N = 8192 and from p = 1
to p = 21. Some of the real datasets contain only numerical variables and some datasets have numerical and categorical variables.
For β ≥ 0.8, it becomes a critical task to ﬁnd an eﬃcient and reliable prediction interval. However these proportions are the
most used ones in machine learning and statistical hypothesis testing. The experimental part compares F-BOPI and A-BOPI pre-
diction intervals for local linear regression while providing β prediction intervals for β = {0.8, 0.9, 0.95, 0.99}. This comparison is
made with ﬁve well-known models of prediction intervals: the conventional prediction interval for local linear regression denoted
by Loess Conv., the conventional prediction interval for least-squares SVM (LS-SVM Conv.), prediction intervals for classical lin-
ear regression (OLS) and two SVM quantile regression model (SVM Quantile and SVM Quantile CV). These models are described
in Section 8.1.1.

When comparing the BOPI methods with other prediction intervals, they appear to be the most reliable methods in both
simulated and real cases. The simulation studies with the artiﬁcial DGP, rate the conventional method very poorly, so that it
produce always non-reliable interval prediction models and the average proportion of response values inside the obtained intervals
(coverage) where always less than the desired content. The BOPI methods obtain in all cases more reliable intervals with F-BOPI
being more reliable than A-BOPI On the other hand A-BOPI yields intervals that are on average tighter than F-BOPI and both
obtain intervals being on average wider than those obtained by Loess Conv..

Experiments on real datasets have shown that BOPI methods provide usually the most eﬃcient solution. A Wilson Score test
for binomial proportion at a signiﬁcance level of 0.05 and the alternate hypothesis as βpop < β 5 is used to test the reliability of
the prediction intervals on real datasets. The conventional methods Loess Conv. and LS-SVM Conv. turn out to be unreliable for
higher value of β. Furthermore, they are usually less eﬃcient than A-BOPI and F-BOPI. Comparison of interval size of the two
reliable estimated models which ﬁnd the smallest MIS using two-sided paired t-tests at levels 0.05, 0.01 and 0.001 shows that A-
BOPI generally obtains the better model (minimum MIS) and its diﬀerence with the second smallest MIS (which is also reliable) is
usually statistically signiﬁcant at most at 0.05 level. On the other hand, ignoring the reliability, the conventional prediction interval
methods Loess Conv. and LS-SVM Conv. rank as the most eﬃcient methods after A-BOPI and F-BOPI. According to the results
reported in Figures A.7, A.8, A.9 and A.10, the SVM quantile regression model (SVM Quantile and SVM Quantile CV) are not
suited for reliable interval prediction.

In a regression context, the conditional mean, the conditional variance and/or the conditional quantile may have diﬀerent
functions. The conditional mean is the general trend of the regression function whereas the conditional quantile is more related
to the local distribution of the response variable. Least-squares based interval prediction methods (OLS, LS-SVM Conv. Loess
Conv., F-BOPI and A-BOPI) try to indirectly estimate the conditional quantile function. They ﬁrst estimate the conditional mean
and then, they estimate the conditional quantile. On the other hand, quantile regression based methods (SVM Quantile and SVM
Quantile CV) directly estimate the conditional quantile. The general trend is easier to predict and its estimator, compared to the
conditional quantile, has a higher speed of convergence (Koenker, 2005). This is why all the tested least-squares based interval
prediction methods are more eﬃcient than the quantile regression based methods. Another reason for this superiority may be the
absence of a global conditional quantile function. It can occur when the conditional variance of the error distribution is not a global
function of the predictors. The proposed methods belong to the class of least-squares based interval prediction methods, so they
take advantage of this fast convergence. However they are more reliable and eﬃcient than the conventional methods. The LHNPE
assumptions permit to take into account the prediction error oscillation so the introduced prediction intervals consider the local
conditional distribution for the response variable. Besides the use of tolerance intervals incorporate the eﬀect of the local sample
size used to estimate the prediction intervals.

9 Discussion and Conclusion

Having the question of reliable prediction intervals for local linear regression in mind, the authors investigated two new methods
(BOPI) for estimating prediction intervals. The main assumptions are that the mean regression function is locally linear and the
prediction error is locally normal and homoscedastic. The prediction intervals for the input vector is obtained based on a tolerance
interval computed on a restricted set of prediction errors (obtained by a cross validation schema) of the local linear regression. This
restricted set is composed of the instances inside the LHNPE bandwidth of the input vector. Two diﬀerent LHNPE bandwidths are

5βpop denotes the average proportion of response value that are contained in the tested prediction intervals (coverage).

29

considered, a bandwidth having a ﬁxed number of neighbors and a bandwidth having a variable number of neighbors. In order to test
the BOPI methods individually and to compare them with ﬁve other interval prediction methods, the authors used the following
measures for ranking interval predictions methods: coverage probability, Mean Interval Size and Equivalent Gaussian Standard
Deviation. The ﬁve aforementioned interval prediction methods were: the conventional interval prediction method (described in
Section 2.3) with local linear regression and least-squares SVM, prediction intervals for classical linear regression and two SVM
quantile regression methods. The rankings were performed with a cross validation schema on eleven benchmark regression datasets,
and the estimated results were generally in favor of the introduced methods. They also reported a simulation study comparing the
BOPI methods with the conventional interval prediction method.

General remarks

The advantages, drawbacks and limitations of BOPI are listed below:

Advantages

• It is a reliable prediction interval for local linear least squares models;
• It does not ignore the non-parametric regression bias;
• It can be used with models having heteroscedastic errors;
• It does not suﬀer from the crossing quantiles eﬀect;
• It is based on local linear regression, which is a well-known regression method.

Drawbacks

• It is limited to local linear regression;
• It has a greater computational complexity than conventional and quantile regression methods.

Limit of Applications

In the following cases, our methods may obtain similar results to its alternatives:
• For prediction interval with a very high desired proportion (0.99 or more) of the distribution of Y(x);
• The dataset is almost identically distributed in the feature space.
BOPI are not suited when:
• There exists regression models having signiﬁcantly better prediction results than non-parametric regression models;
• The distribution of prediction errors diﬀers signiﬁcantly from the normal distribution.
For future work, the most promising idea is the extension of these prediction intervals to other regression function, e.g. support
vector machines. Another horizon may be its generalization to the one-sided interval prediction problem. One can also apply these
methods to interval prediction in time series models.

References

Atkeson, C. G., Moore, A. W., S., S., 1997. Locally weighted learning. Artiﬁcial Intelligence Review, 11–73.
Bowman, A., Azzalini, A., 2003. Computational aspects of nonparametric smoothing with illustrations from the sm library. Computational Statistics

& Data Analysis 42 (4), 545 – 560.

Breiman, L., Aug. 1996. Bagging predictors. Mach. Learn. 24 (2), 123–140.

URL http://dx.doi.org/10.1023/A:1018054314350

Cleveland, W. S., 1979. Robust locally weighted regression and smoothing scatterplots. Journal of the American Statistical Association 74 (368),

829–836.

Cleveland, W. S., Devlin, S. J., 1988. Locally weighted regression: An approach to regression analysis by local ﬁtting. Journal of the American

Statistical Association 83 (403), 596–610.

Delve Development Group, 2003. University of toronto, department of computer science, Delve datasets.

URL http://www.cs.toronto.edu/~delve/data/datasets.html

Eubank, R., 1999. Nonparametric Regression and Spline Smoothing, Second Edition. Statistics: A Series of Textbooks and Monogrphs. Marcel

Dekker.
URL http://books.google.com.au/books?id=T1Jrv88TuH8C

Fan, J., 1992. Design-adaptive nonparametric regression. Journal of the American Statistical Association 87 (420), 998–1004.

URL http://www.jstor.org/stable/2290637

30

Fan, J., 1993. Local linear regression smoothers and their minimax eﬃciencies. Annals of Statistics 21 (1), 196–216.

Fan, J., Gijbels, I., 1992. Variable bandwidth and local linear regression smoothers. Annals of Statistics 20 (4), 2008–2036.

Fan, J., Gijbels, I., 1996. Local Polynomial Modelling and Its Applications: Monographs on Statistics and Applied Probability 66. Monographs on

Fan, J., Marron, J. S., 1994. Fast implementations of nonparametric curve estimators. Journal of Computational and Graphical Statistics 3 (1),

URL http://www.jstor.org/stable/3035587

URL http://www.jstor.org/stable/2242378

Statistics and Applied Probability, 66. Chapman & Hall.

35–56.
URL http://www.jstor.org/stable/1390794

Frank, A., Asuncion, A., 2010. UCI machine learning repository.

URL http://archive.ics.uci.edu/ml

Friedman, J. H., 1991. Multivariate Adaptive Regression Splines. Annals of Statistics 19 (1), 1–67.

URL http://dx.doi.org/10.2307/2241837

Gasser, T., Kneip, A., 1989. Discussion: Linear smoothers and additive models. Annals of Statistics 17 (2), 532–535.

URL http://www.jstor.org/stable/2241566

Ghasemi Hamed, M., Serrurier, M., Durand, D., 2012. Simultaneous interval regression for k-nearest neighbor. In: Australasian Conference on

Hahn, G., 1969. Factors for calculating two-sided prediction intervals for samples from a normal distribution. Journal of the American Statistical

Artiﬁcial Intelligence. pp. 602–613.

Association 64 (327), 878–888.

Hahn, G. J., Meeker, W. Q., 1991. Statistical Intervals: A Guide for Practitioners. John Wiley and Sons.
H¨ardle, W., 1990. Applied nonparametric regression. Econometric Society Monographs (No. 19). Cambridge University Press.
Hart, J. D., 1997. Nonparametric smoothing and lack-of-ﬁt tests. Springer Series in Statistics. Springer, New York.
Hastie, T., Tibshirani, R., 1990. Generalized Additive Models. Monographs on Statistics and Applied Probability. Chapman & Hall.

URL http://books.google.fr/books?id=qa29r1Ze1coC

Howe, W. G., 1969. Two-sided tolerance limits for normal populations, some improvements. Journal of the American Statistical Association

Koenker, R., 2005. Quantile Regression. Econometric Society Monographs. Cambridge University Press.
Krishnamoorthy, K., Mathew, T., 2009. Statistical Tolerance Regions: Theory, Applications, and Computation. Wiley Series in Probability and

64 (326), 610–620.

Statistics. Wiley.

Paulson, E., 1943. A note on tolerance limits. The Annals of Mathematical Statistics 14 (1), 90–93.
Rao, C. R., Toutenburg, H., Jul. 1999. Linear Models: Least Squares and Alternatives (Springer Series in Statistics). Springer.
Ruppert, D., Wand, M. P., 1994. Multivariate locally weighted least squares regression. Annals of Statistics 22 (3), 1346–1370.

Silverman, B. W., 1985. Some aspects of the spline smoothing approach to non-parametric regression curve ﬁtting. Journal of the Royal Statistical

URL http://www.jstor.org/stable/2242229

Society. Series B (Methodological) 47 (1), 1–52.

Stone, C. J., 1977. Consistent nonparametric regression. Annals of Statistics 5 (4), 595–620.
Takeuchi, I., Le, Q. V., Sears, T. D., Smola, A. J., Dec. 2006. Nonparametric quantile estimation. Journal of Machine Learning Research 7,

Wahba, G., 1990. Spline models for observational data. Vol. 59 of CBMS-NSF Regional Conference Series in Applied Mathematics. Society for

Industrial and Applied Mathematics (SIAM), Philadelphia, PA.
URL http://www.ams.org/mathscinet-getitem?mr=1045442

Yeh, I.-C., 1998. Modeling of strength of high-performance concrete using artiﬁcial neural networks. Cement and Concrete Research 28 (12),

Yeh, I.-C., 2007. Modeling slump ﬂow of concrete using second-order regressions and artiﬁcial neural networks. Cement and Concrete Composites

1231–1264.

1797–1808.

29 (6), 474–480.

31

Appendix A

Table A.7: Coverage and MIS for diﬀerent γ on Friedman#1 DGP

γ

0.80

0.90

0.95

0.99

Conv.
88.06
(2.43)
88.06
(2.43)
88.06
(2.43)
88.06
(2.43)

Coverage
F − BOPI
91.09
(2.20)
92.60
(2.01)
93.94
(1.85)
95.99
(1.39)

A − BOPI
89.48
(2.32)
91.26
(2.20)
92.50
(2.03)
94.83
(1.65)

Conv.
1.335
(0.033)
1.335
(0.033)
1.335
(0.033)
1.335
(0.033)

MIS
F − BOPI
1.468
(0.060)
1.550
(0.062)
1.625
(0.064)
1.780
(0.068)

A − BOPI
1.400
(0.060)
1.476
(0.061)
1.543
(0.062)
1.680
(0.065)

Table A.8: Computed coverage for γ = {0.8, 0.9, 0.95, 0.99} and β = 0.95. The coverage values are computed using
Friedman#2 data generating process by a 3-fold cross validation schema where 2
3 of the generated sample is used for
training and the remaining for test. The generated sample sizes are N = 1500 and the simulation process is iterated for
Nsim = 500 times. The method hyper-parameters are as follows: K f
= 50)
for A-BOPI and the regression bandwidth Kloess = 100 is constant for the three interval prediction methods. Loess
Conv. is shortened to “Conv.” and the standard deviation over the Nsim coverage values is shown in parentheses.

= 40 for F-BOPI, (Kmin
lhnpe

= 30, Kmax
lhnpe

lhnpe

Table A.9: Coverage and MIS for diﬀerent γ on Friedman#2 DGP

γ

0.80

0.90

0.95

0.99

Conv.
85.199
(4.013)
85.199
(4.013)
85.199
(4.013)
85.199
(4.013)

Coverage
F − BOPI
89.748
(3.287)
91.387
(3.094)
92.721
(2.829)
94.789
(2.402)

A − BOPI
88.425
(3.596)
90.124
(3.324)
91.382
(3.066)
93.685
(2.635)

Conv.
11.678
(0.181)
11.678
(0.181)
11.678
(0.181)
11.678
(0.181)

MIS
F − BOPI
13.397
(0.516)
14.154
(0.543)
14.832
(0.567)
16.253
(0.617)

A − BOPI
12.870
(0.522)
13.560
(0.542)
14.170
(0.560)
15.414
(0.599)

Table A.10: Computed coverage for γ = {0.8, 0.9, 0.95, 0.99} and β = 0.95. The coverage values are computed using
Friedman#2 data generating process by a 3-fold cross validation schema where 2
3 of the generated sample is used for
training and the remaining for test. The generated sample sizes are N = 1500 and the simulation process is iterated for
Nsim = 500 times. The method hyper-parameters are as follows: K f
= 50)
for A-BOPI and the regression bandwidth Kloess = 100 is constant for the three interval prediction methods. Loess
Conv. is shortened to “Conv.” and the standard deviation over the Nsim coverage values is shown in parentheses.

= 40 for F-BOPI, (Kmin
lhnpe

= 30, Kmax
lhnpe

lhnpe

Appendix A.1 Proof of Proposition 1

Let Y(x) = f (x) + εx and let ˆf (x) denote its local linear regression estimator. If this regression estimator satisﬁes the conditions

below:

32

Table A.11: Coverage for diﬀerent β on Friedman#1 DGP

Coverage

Conv.
67.814
(3.083)
80.370
( 2.987)
88.056
(2.427)
96.003
(1.488)

N = 1500
F − BOPI
81.871
(2.797)
91.600
(2.123)
95.992
(1.395)
99.197
(0.558)

A − BOPI
79.368
(2.953)
89.928
(2.247)
94.827
(1.654)
98.812
(0.659)

Conv.
67.816
(3.081)
80.374
(2.987)
88.060
(2.428)
96.007
(1.487)

N = 3000
F − BOPI
81.873
(2.795)
91.604
(2.122)
95.996
(1.393)
99.198
(0.556)

A − BOPI
79.369
(2.952)
89.932
(2.246)
94.831
(1.654)
98.814
(0.657)

Nsim = 500

Nsim = 1000

γ

α

0.990

0.800

0.990

0.900

0.990

0.950

0.990

0.990

0.990

0.800

0.990

0.900

0.990

0.950

0.990

0.990

66.923
(1.768)
79.143
(1.752)
86.936
(1.535)
95.199
(0.845)

81.264
(2.084)
90.906
(1.485)
95.543
(1.014)
98.986
(0.453)

78.606
(2.197)
88.955
(1.719)
94.282
(1.185)
98.614
(0.492)

66.925
(1.768)
79.143
(1.751)
86.936
(1.534)
95.199
(0.844)

81.266
(2.084)
90.908
(1.485)
95.543
(1.013)
98.986
(0.453)

78.607
(2.196)
88.957
(1.719)
94.284
(1.184)
98.614
(0.491)

Table A.12: Computed coverage for β = {0.8, 0.9, 0.95, 0.99} and γ = 0.99. The coverage values are computed using
Friedman#2 data generating process by a 3-fold cross validation schema where 2
3 of the generated sample is used
for training and the remaining for test. The generated sample sizes are N = 1500 (left) and N = 3000 (right). The
simulation process is iterated (Nsim times) for 500 times (up) and 1000 times (down). The method hyper-parameters are
as follows: K f
= 50) for A-BOPI and the regression bandwidth Kloess = 100
is constant for the three interval prediction methods. Loess Conv. is shortened to “Conv.” and the standard deviation
over the Nsim coverage values is shown in parentheses.

= 40 for F-BOPI, (Kmin
lhnpe

lhnpe

= 30, Kmax
lhnpe

33

Table A.13: Coverage for diﬀerent β on Friedman#2 DGP

Coverage

Conv.
64.033
(3.564)
77.041
(4.212)
85.199
(4.013)
93.689
(2.588)

N = 1500
F − BOPI
80.659
(4.436)
90.335
(3.189)
94.789
(2.402)
98.528
(1.048)

A − BOPI
78.316
(4.268)
88.680
(3.545)
93.685
(2.635)
98.069
(1.256)

Conv.
64.026
(3.562)
77.034
(4.212)
85.191
(4.014)
93.682
(2.589)

N = 3000
F − BOPI
80.652
(4.436)
90.329
(3.191)
94.784
(2.402)
98.526
(1.048)

A − BOPI
78.311
(4.268)
88.674
(3.548)
93.680
(2.635)
98.066
(1.257)

Nsim = 500

Nsim = 1000

γ

α

0.990

0.800

0.990

0.900

0.990

0.950

0.990

0.990

0.990

0.800

0.990

0.900

0.990

0.950

0.990

0.990

Table A.14: Computed coverage for β = {0.8, 0.9, 0.95, 0.99} and γ = 0.95. The coverage values are computed using
Friedman#2 data generating process by a 3-fold cross validation schema where 2
3 of the generated sample is used
for training and the remaining for test. The generated sample sizes are N = 1500 (left) and N = 3000 (right). The
simulation process is iterated (Nsim times) for 500 times (up) and 1000 times (down). The method hyper-parameters are
as follows: K f
= 50) for A-BOPI and the regression bandwidth Kloess = 100
is constant for the three interval prediction methods. Loess Conv. is shortened to “Conv.” and the standard deviation
over the Nsim coverage values is shown in parentheses.

= 40 for F-BOPI, (Kmin
lhnpe

lhnpe

= 30, Kmax
lhnpe

59.460
(3.507)
71.974
(3.749)
80.061
(3.792)
89.721
(3.057)

76.089
(4.220)
86.154
(3.752)
91.324
(3.066)
96.457
(1.757)

73.690
(4.259)
84.284
(3.888)
89.986
(3.309)
95.656
(2.011)

59.465
(3.508)
71.978
(3.749)
80.065
(3.792)
89.725
(3.058)

76.092
(4.218)
86.158
(3.752)
91.327
(3.066)
96.457
(1.756)

73.694
(4.257)
84.287
(3.887)
89.989
(3.309)
95.657
(2.011)

34

(a) β = 0.8

(b) β = 0.9

(c) β = 0.95

(d) β = 0.99

Figure A.6: Coverage charts for benchmark datasets with β = {0.8, 0.9, 0.95, 0.99}.

35

35404550556065707580859095100ComputerBankAbaloneParkinson1Parkinson2ConcreteSlumpHousingAutoCPUMotocycleF-BOPIA-BOPILoess Conv.OLSLS-SVM Conv.SVM QuantileSVM Quantile CV556065707580859095100ComputerBankAbaloneParkinson1Parkinson2ConcreteSlumpHousingAutoCPUMotocycleF-BOPIA-BOPILoess Conv.OLSLS-SVM Conv.SVM QuantileSVM Quantile CV65707580859095100ComputerBankAbaloneParkinson1Parkinson2ConcreteSlumpHousingAutoCPUMotocycleF-BOPIA-BOPILoess Conv.OLSLS-SVM Conv.SVM QuantileSVM Quantile CV8486889092949698100ComputerBankAbaloneParkinson1Parkinson2ConcreteSlumpHousingAutoCPUMotocycleF-BOPIA-BOPILoess Conv.OLSLS-SVM Conv.SVM QuantileSVM Quantile CVFigure A.7: EGSD chart for benchmark datasets with β = 0.80. The smallest value denotes the most eﬃcient band.
This measure ignores the reliability.

Figure A.8: EGSD chart for benchmark datasets with β = 0.9. The smallest value denotes the most eﬃcient band.
This measure ignores the reliability.

Figure A.9: MIS Ratio chart for benchmark datasets with β = 0.95. The smallest value denotes the tightest reliable
band.

36

0,000,100,200,300,400,500,600,700,800,901,00ComputerBankAbaloneParkinson1Parkinson2ConcreteSlumpHousingAutoCPUMotocycleF-BOPIA-BOPILoess Conv.OLSLS-SVM Conv.SVM QuantileSVM Quantile CV0,000,100,200,300,400,500,600,700,800,901,00ComputerBankAbaloneParkinson1Parkinson2ConcreteSlumpHousingAutoCPUMotocycleF-BOPIA-BOPILoess Conv.OLSLS-SVM Conv.SVM QuantileSVM Quantile CV0,000,100,200,300,400,500,600,700,800,901,00ComputerBankAbaloneParkinson1Parkinson2ConcreteSlumpHousingAutoCPUMotocycleF-BOPIA-BOPILoess Conv.OLSLS-SVM Conv.SVM QuantileSVM Quantile CVFigure A.10: EGSD chart for benchmark datasets with β = 0.99. The smallest value denotes the most eﬃcient band.
This measure ignores the reliability.

• Normal error distribution: εx ∼ N(0, σ2
x).
• ˆf (x) has an almost constant distribution as deﬁned in Deﬁnition 3.
Then:
ˆf (x) is an LHNPE regression estimator;

(a)
(b) The interval I(x∗)Pred
(c) The sample bias of the prediction error in the LHNPE neighborhood is a consistent estimator of the regression bias:

for the input x∗ obtained by Equation (19) is a β-content prediction interval for Y(x∗).

β

(cid:16) ˆf (x∗) − (cid:100)bias ˆf (x∗)

(cid:17)

= f (x∗),

plim
K→∞

where K/N → 0 as N → ∞, K = card(Ksetx∗) and (cid:100)bias ˆf (x∗) are respectively the cardinal of Ksetx∗ and the sample bias of

ˆf (x∗).

Proof:
Part (a): (Fan and Gijbels, 1996, pp. 302) have shown that, under certain regularity conditions, the local linear estimator
, where bias ˆf (x∗) = E[ ˆf (x∗) − f (x∗)] is the
ˆf (x∗) is the regression estimator variance. The latter result along with the

has asymptotically the following normal distribution:
estimator’s bias, σ2
proposition’s condition proves (a).

x is the variance of the error and σ2

f (x) + bias ˆf (x), σ2

ˆf (x)

ˆf (x) ∼ N(cid:18)

(cid:19)

Part (b): Let x∗ denote the input vector and let εpred
x∗

denote its prediction error, then by accepting (a), we assume that the
prediction errors follow a normal distribution whose variance is approximately the same in the neighborhood of x∗. So, we have:

where by deﬁnition εx∗ and ˆf (x∗) are independent and f (x∗) is non-random. Thus we have:

x∗ = Y(x∗) − ˆf (x∗) = εx∗ + f (x∗) − ˆf (x∗),
εpred

x∗ ∼ N(−bias ˆf (x∗), σ2
εpred

ε

),

pred
x∗

(A.1)

pred
x∗

where σ2
ε

= σ2
as an iid sample of εpred

the prediction interval of the prediction error is calculated by replacing X, ˆσ and n in Equation (4) with −(cid:100)bias ˆf (x∗) and ˆσε

ˆf (x∗). Based on the above assumptions, one can use the prediction error of the LHNPE neighbors of x∗
by using Equation (4). In this case,
and

and calculate the prediction interval of the prediction error I(εpred

x∗ + σ2
x∗

)Pred
β

x∗

pred
x∗

37

0,000,100,200,300,400,500,600,700,800,901,00ComputerBankAbaloneParkinson1Parkinson2ConcreteSlumpHousingAutoCPUMotocycleF-BOPIA-BOPILoess Conv.OLSLS-SVM Conv.SVM QuantileSVM Quantile CVK. These value are estimated as:

(cid:100)bias ˆf (x∗) = (−K)−1 (cid:88)
+ (cid:100)bias ˆf (x∗))2

xi∈Ksetx∗

(εpred

xi

(K − 1)−1 (cid:88)

εpred
xi

 1

2

,

ˆσε

pred
x∗

=

where K/N → 0 as N → ∞ and we have K = card(Ksetx∗) and (cid:100)bias ˆf (x∗) are respectively the cardinal of Ksetx∗ and the

xi∈Ksetx∗

)Pred
β

takes into account two kinds of uncertainties: the regression’s method uncertainty and the

sample bias of ˆf (x∗). Thus I(εpred
x∗
observation error. It results in :

(cid:114)

I(εpred

)Pred
β

)Pred
β

)Pred
β

, U(εpred

= [L(εpred

] =

x∗

x∗

x∗

−(cid:100)bias ˆf (x∗) ± ˆσε
x∗ has a normal distribution with the unknown mean −bias ˆf (x∗). The prediction
is constructed based on the Ksetx∗, which has a ﬁnite sample size, and it is centered on

2 ,K−1)

1
K .

t( 1−β

1 +

pred
x∗

Equation (A.1) shows that the prediction error εpred

interval for the prediction errors I(εpred

the sample bias −(cid:100)bias ˆf (x∗). However, because of its deﬁnition, I(εpred
x∗
γ,β ≤ εpred
x∗ ≤ U(εpred
)T
x∗

L(εpred

)Pred
β

PT ,ε

(cid:18)

x∗

x∗

)Pred
β we have:

(cid:19) ≥ β,

)T
γ,β

where T = ( ˆf (x∗), ˆσx∗) is the estimated vector at x∗. This equation can be rewritten as:

(cid:18)

(cid:18)

x∗

L(εpred

)Pred
PT ,ε
β
ˆf (x∗) + L(εpred
x∗

= PT ,ε

≤ ε + f (x∗) − ˆf (x∗) ≤ U(εpred
x∗
(cid:18)
Y(x∗) ∈(cid:16) ˆf (x∗) + I(εpred
≤ Y(x∗) ≤ ˆf (x∗) + U(εpred
x∗

)Pred
β

= PT ,ε

x∗

)Pred
β

(cid:19) ≥ β
(cid:19) ≥ β
(cid:17)(cid:19) ≥ β.

)Pred
β

)Pred
β

(A.2)

Having in mind the assumptions, Equation (A.2) could be interpreted as follows: the prediction interval for the response vari-

able is computed by adding the local linear regression estimate to the prediction interval on the prediction error:

I(x∗)Pred

γ,β

= ˆf (x∗) + I(εpred
x∗

)Pred
β

.

Note that, since εpred

has a normal distribution, then I(εpred

x∗

x∗

)Pred
β

is obtained by a prediction interval for normal distribution

calculated using Equation (4).

Part (c): even though the prediction is biased, the prediction interval contains on average a desired proportion β of the condi-
tional distribution of the response variable: the prediction intervals are computed on the prediction error and the prediction error is
centered on the sample estimate of negative bias.

In order to show that the sample bias converges in mean squared to the local linear regression bias, we will show that the

expectation of the sample bias is the local linear regression bias and its asymptotic variance is zero.

K−1 (cid:88)

xi∈Ksetx∗

= E

(εx∗)

= E

E

= E

εpred
xi

xi∈Ksetx∗

K−1 (cid:88)
(cid:16)

(cid:104)(cid:100)bias ˆf (x∗)
(cid:105)
K−1 (cid:88)
(cid:16)
εx∗ + f (x∗) − ˆf (x∗)
 + E
K−1 (cid:88)
= K−1 (cid:88)

f (x∗) − ˆf (x∗)
(cid:104)

xi∈Ksetx∗
E

f (x∗) − ˆf (x∗)


(cid:17)
(cid:17)
(cid:105)

xi∈Ksetx∗

xi∈Ksetx∗

= bias ˆf (x∗).

38

(cid:104)(cid:100)bias ˆf (x∗)

(cid:105)

=

Var

+ Var

(cid:35)

(εx∗)

(cid:34)
K−1 (cid:80)
(cid:16)
K−1 (cid:80)

Var

xi∈Ksetx∗

(cid:34)

xi∈Ksetx∗

f (x∗) − ˆf (x∗)

(cid:17)(cid:35)

= K−1σ2

ε

,

pred
x∗

(cid:20)(cid:16)(cid:100)bias ˆf (x∗) − bias ˆf (x∗)

(cid:17)2(cid:21)

K→∞E
lim

K→∞K−1σ2
= lim

ε

pred
x∗

= 0.

where K/N → 0 as N → ∞ and K = card(Ksetx∗). By deﬁnition εx∗ and ˆf (x∗) are independent and f (x∗) is non-random, thus:

which implies a convergence in probability.

Under the mentioned conditions the sample −(cid:100)bias ˆf (x∗) is a consistent estimator of bias ˆf (x∗), and it is evident that:

(cid:16) ˆf (x∗) − (cid:100)bias ˆf (x∗)

(cid:17)

plim
K→∞

(cid:16) ˆf (x∗)

(cid:17)

(cid:17) − plim
(cid:16)(cid:100)bias ˆf (x∗)

K→∞

(cid:16)(cid:100)bias ˆf (x∗)
(cid:17)

= f (x∗)

= plim
K→∞

= f (x∗) + bias ˆf (x∗) − plim
K→∞

Appendix A.2 Proof of Proposition 2

For any random sample 20 ≤ n ≤ 10000, if we set γ and β, then the γ-coverage β-content tolerance interval of the standard

normal distribution is greater than or equal to its β-prediction intervals. This is stated formally below:

(cid:4)

where size(I) = U − L, I = [L, U] and the terms ITol

γ,β and IPrev

β

interval of the standard normal distribution.

refer to γ-coverage β-content tolerance interval and β-prediction

∀n ≥ 20, γ ≥ 0.7, β ∈ [0.01, 0.99], size(ITol

γ,β ) ≥ size(IPrev

β

).

Proof:
In order to verify this property, one must show that the proportion of tolerance factor on prediction factor for a normal dis-
tribution, when 20 ≤ n ≤ 10000, is always greater than or equal to 1. The tolerance and prediction factor are the coeﬃcient of
ˆσ in Equation (6) and (4). So, the proportion of tolerance factor on prediction factor is the proportion of the coeﬃcient of ˆσ in
Equation (6) on the coeﬃcient of ˆσ in Equation (4). We call this proportion the tolerance prediction proportion and it can be
simpliﬁed as:

√

(cid:113)
n − 1

Z 1−β
2

t( 1−β

2 ,n−1)

χ2
1−γ,n−1

(A.3)

This property is veriﬁed numerically by Figure A.11. In order to verify this property for sample size 20 ≤ n ≤ 10000, one has to
ensure that for ﬁxed n and γ = 0.7, the minimum tolerance prediction proportion obtained over β ∈ [0.01, 0.99] must be greater
than or equal to 1 and this inequality must hold for all 20 ≤ n ≤ 10000. This is described formally as:

∀n ∈ [20, . . . , 10000], β ∈ [0.01, 0.99], min

β

Z 1−β
2

t( 1−β

2 ,n−1)

√
(cid:113)
n − 1

χ2
1−γ,n−1

≥ 1,

(A.4)

where γ = 0.7. The inequality described by Equation (A.4) is proven for γ = 0.7 by Figure (A.11). We have seen above that
(cid:4)

the left part of Equation (A.4) is an increasing function of γ, so this property holds also for small samples with γ ≥ 0.7

39

Figure A.11: This ﬁgure plots the inequality described by Equation (A.4). The vertical axis is the minimum tolerance
prediction proportion obtained within β ∈ [0.01, 0.99] and the horizontal axis is the sample size n.

40

