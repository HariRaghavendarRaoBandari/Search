6
1
0
2

 
r
a

M
3

 

 
 
]

.

R
S
h
p
-
o
r
t
s
a
[
 
 

1
v
5
9
9
0
0

.

3
0
6
1
:
v
i
X
r
a

Mon. Not. R. Astron. Soc. 000, 1–5 (2015)

Printed 4 March 2016

(MN LATEX style ﬁle v2.2)

Parameterizing Stellar Spectra Using Deep Neural
Networks

Xiangru Li⋆, Ruyang Pan

School of Mathematical Sciences, South China Normal University, No. 55, West of Yat-sen Avenue, Guangzhou, 510631, China

4 March 2016

ABSTRACT
This work investigates the spectrum parameterization problem using deep neural net-
works (DNNs). The proposed scheme consists of the following procedures: ﬁrst, the
conﬁguration of a DNN is initialized using a series of autoencoder neural networks;
second, the DNN is ﬁne-tuned using a gradient descent scheme; third, stellar parame-
ters (Teff, log g, and [Fe/H]) are estimated using the obtained DNN. This scheme was
evaluated on both real spectra from SDSS/SEGUE and synthetic spectra calculated
from Kurucz’s new opacity distribution function models. Test consistencies between
our estimates and those provided by the spectroscopic parameter pipeline of SDSS
show that the mean absolute errors (MAEs) are 0.0048, 0.1477, and 0.1129 dex for
log Teff, log g, and [Fe/H] (64.85 K for Teff), respectively. For the synthetic spectra,
the MAE test accuracies are 0.0011, 0.0182, and 0.0112 dex for log Teff, log g, and
[Fe/H] (14.90 K for Teff), respectively.

Key words: methods: statistical–techniques: spectroscopic–stars: atmospheres–stars:
fundamental parameters

1

INTRODUCTION

Large-scale sky survey programs, such as the Sloan Digi-
tal Sky Survey (SDSS; York et al. 2000; Ahn et al. 2012),
Large Sky Area Multi-Object Fiber Spectroscopic Tele-
scope/Guoshoujing Telescope (LAMOST; Zhao et al. 2006;
Cui et al. 2012), and Gaia-ESO Survey (Gilmore et al.
2012; Randich et al. 2013), are collecting and will obtain
very large numbers of stellar spectra. This large amount of
data necessitates a fully automated process to characterize
the spectra, which will consequently enable the statistical
exploration of atmospheric parameter-related properties in
the spectra.

The present work studies the spectrum parameteriza-
tion problem. A typical class of schemes are based on (feed-
forward) neural networks ((F)NNs: Willemsen et al. 2005;
Giridhar et al. 2006; Re Fiorentin et al. 2007; Gray et al.
2009; Tan et al. 2013a). In these NNs, the information
moves in only one direction, that is from the input nodes
(neurons), through the hidden nodes, and to the output
nodes (neurons). In atmospheric parameter estimation, the
input nodes represent a stellar spectrum, and the out-
put node(s) represent(s) the atmospheric parameter(s) to
be estimated, e.g., Teff, log g and [Fe/H]. A NN is com-
monly obtained by a back-propagation (BP) algorithm
(Rumelhart et al. 1986).

⋆ E-mail: xiangru.li@gmail.com (X. Li)

c(cid:13) 2015 RAS

For example, Bailer-Jones (2000) investigated the es-
timation precision of stellar parameters Teff,
log g, and
[M/H] using a FNN network with two hidden layers on
synthetic spectra with diﬀerent resolutions and signal-to-
noise ratios. Snider et al. (2001) explored the application of
FNN with one and two hidden layers in the estimation of
atmospheric parameters from medium-resolution spectra of
F- and G-type stars. Manteiga et al. (2010) parameterized
stellar spectra by extracting features based on Fourier anal-
ysis and wavelet decomposition, and constructing a map-
ping from a feature space to the parameter space by a FNN
with one hidden layer. Li et al. (2014) investigated the at-
mospheric parameter estimation problem by detecting spec-
tral features by LASSO ﬁrst and subsequently estimating
the atmospheric parameters using a FNN with one hidden
layer.

This article investigates the spectrum parameterization
problem using a deep NN (DNN). In application, a tradi-
tional NN usually has one or two hidden layers. By contrast,
DNNs have two typical characteristics: 1) A DNN usually
has more hidden layers, 2) Two procedures are needed in
estimating a DNN: prelearning and ﬁne-tuning. This scheme
has been studied extensively in artiﬁcial intelligence and
data mining, and shows excellent performance in many ap-
plications. This work investigated the application of this
scheme in spectrum parameterization.

This paper is organized as follows. Section 2 introduces
the NN, DNN, their learning algorithms, and the proposed

2

Xiangru Li and Ruyang Pan

+1

Layer

1

+1

Layer

2

Layer L

Figure 1. A diagram of a neural network.

stellar parameter estimation scheme. Section 3 reports some
experimental evaluations on real and synthetic spectra. Fi-
nally, we summarize our work in Section 4.

2 PARAMETERIZING STELLAR SPECTRA

USING A DNN

2.1 A neural network (NN)

This work investigated a scheme to parameterize a stellar
spectrum using a DNN. A NN consists of a series of neurons
on multiple layers. Figure 1 is a diagram of a NN with L
layers. In this diagram, every circle with solid line represents
a neuron, and a circle with a dashed line is a bias unit used
in describing the relationships between neurons.

k and a(l)

In a NN, every neuron is a simple computational units
and has an input and output, z and a, respectively. For
example, the z(l)
k denote the input and output of
the k-th neuron on the l-th layer, respectively, where l =
1, 2, · · · , L; k = 1, · · · , nl; and nl is the number of neurons
on the l-th layer. The relationship between an input and
output is usually described by an activation function g(˙):

a = g(z).

(1)

Two common choices for the activation function are a sig-
moid function

g(z) =

1

1 + e−z

and hyperbolic tangent function

g(z) =

ez − e−z
ez + e−z

.

(2)

(3)

and output layers, respectively; the other layers are referred
to as hidden layers. On the input layer, the output of a
neuron is the same with its input

a(1)
k = z(1)

k , k = 1, · · · , n1.

The output of the last layer can be denoted as a(L):

a(L) = (a(L)

1

, · · · , a(L)

nL ).

(5)

(6)

Suppose x = (x1, · · · , xn1 )T is a representation of a
signal (e.g., a stellar spectrum). If the x is an input into a
NN in Figure 1:

z(1) = x,

(7)

an output a(L) can be computed by this network (equations
4 and 1), where z(1) = (z(1)
n1 )T . Therefore, a NN
implements a non-linear mapping hW ,b(·) from an input x =
(x1, · · · , xn1 )T to an output a(L):

1 , · · · , z(1)

a(L) = hW ,b(x),

b = {b(l)}

where

is the set of biases,

(8)

(9)

W = {W (l), l = 1, · · · , L}

(10)

the set of the weights of a NN in equation (4), bl = {b(l)
j 6 nl} and W (l) = {W (l)

ji }.

j , 1 6

To deﬁne a NN, besides L, W and b, one more set of

parameters exists:

(n1, n2, · · · , nL)

(11)

2.2 A BP algorithm for obtaining a NN

Suppose that

S = {(x, y)}

(12)

is a training set for a NN, where x = (x1, · · · , xn1 )T can be a
representation of a spectrum, and y is the expected output
corresponding to x. Section 3.1 discusses more about the
training set.

In a NN, some parameters W and b can be given.
These parameters can be obtained by minimizing an ob-
jective function, namely, J, in equation (13):

The present work used the sigmoid function in equation (2).
A neuron receives signals from every neuron on the pre-

vious layer as the following:

J(W , b) =

+ λ

1

N Px∈S( 1
2 PL−1
l=1 Pnl

2 khW ,b(x) − yk2)
ji )2,

j=1 (w(l)

i=1Pnl+1

z(l+1)

k

=

nl

Xi=1

w(l)

ki a(l)

i + b(l)
k ,

(4)

where l = 1, · · · , L − 1, and w(l)
ki describe the relationship
between the kth and the ith neurons on the (l + 1)th and
lth layers (this relationship is represented with a line be-
tween the two neurons in Fig. 1), respectively; b(l)
is the
bias associated with the kth neuron on the (l + 1)th layer
(represented with a line between the kth neuron and bias
unit on the (l + 1)th and lth layers, respectively), and nl is
the number of neurons on the lth layer.

k

Generally, the ﬁrst and last layers are called as input

(13)

where N is the number of samples in a training set S, and
λ > 0 is a preset parameter. In literature, λ is commonly
referred to as a weight decay parameter.

In equation (13), the ﬁrst term represents an empirical
error evaluation between the actual and expected outputs of
an autoencoder; this term also ensures a good reconstruction
performance of the network. The second term, a regulariza-
tion term of w(l)
ji , is used to overcome possible overﬁtting to
the training set by reducing the scheme’s complexity.

To obtain our NN from a training set, we initialize
each parameter w(l)
to a small random value near
zero; subsequently, two parameters W and b are itera-
tively optimized using a gradient descent method based

ij and b(l)

i

c(cid:13) 2015 RAS, MNRAS 000, 1–5

Parameterizing Stellar Spectra Using Deep Neural Networks

3

on the objective function J in equation (13). This learn-
ing scheme is referred to as BP algorithm (Rumelhart et al.
1986; Andrew et al. 2010).

2.3 Self-Taught Learning to DNNs

In a BP algorithm, the parameters W and b are initialized
with a small random value. However, the obtained results of
BP algorithm is unsatisfactory when the number of layers
of a NN is higher than 4. In this case, b = {b(l)} and W =
{W (l), l = 1, · · · , L} can be initialized using autoencoder
networks.

An autoencoder is a speciﬁc kind of NN with three char-

acteristics:

(i) Only one hidden layer exists. The number of neurons in this

layer is referred to as nae
2 .

(ii) The number of neurons in the output layer is equal to that
in the input layer. The number of neurons in input layer is
represented by nae
1 .

(iii) The expected output of the NN is NN’s input.

Therefore, the parameters of an autoencoder are bae, W ae,
and nae, where bae = {b(1,ae), b(2,ae)} is a set of biases,
W ae = {W (1,ae), W (2,ae)} a set of weights between neurons
on diﬀerent layers, and nae = (nae
2 ) numbers of neurons
on input layer and hidden layer.1

1 , nae

Therefore, to obtain a DNN (Figure 1), the proposed

learning scheme consists of the following processes:

(i)

1 , nae

Initialization using autoencoders. To initialize the
parameters W (1) and b(1)
in equations (2.1) and (2.1),
an autoencoder with (nae
2 ) = (n1, n2) is established;
W ae = {W (1,ae), W (2,ae)} and bae = {b(1,ae), b(2,ae)} are
obtained from a training set S(1) = {(x, x), x ∈ S} using
the BP algorithm (section 2.2) and let W (1) = W (1,ae) and
b(1) = b(1,ae), where n1 and n2 are deﬁned in equation (11).
To initialize W (l) and b(l), the training set S is input into
the DNN in Fig. 1 to produce the outputs S(l) from the
lth layer of the DNN in Fig. 1; Subsequently, an autoen-
coder with (nae
2 ) = (nl, nl+1) is established, W ae =
{W (1,ae), W (2,ae)} and bae = {b(1,ae), b(2,ae)} are obtained
from the training set S(l) using the BP algorithm (section
2.2), the computed W (1,ae) and b(1,ae) are the initializations
of W (1) and b(1), respectively, where l = 2, · · · , L.

1 , nae

(ii) Fine-tuning. From the initialized W and b from the au-
toencoders, these two parameters are optimized using a gra-
dient descent method based on the objective function J in
equation (13) (this optimization procedure is the same with
that in the BP algorithm: section 2.2, Andrew et al. 2010).

2.4 Spectrum parameterization and performance

evaluation

This work parameterizes stellar spectra using a NN with
six layers; its conﬁgurations of the DNN are L = 6 and
(n1, · · · , n6) = (3821, 1000, 500, 100, 30, 1), where nl is the
number of neurons on the lth layer of the NN.

In the training set S in equation (12), let y represent the
eﬀective temperature corresponding to a spectrum x. From

this training set S, a DNN estimator, namely, hW,h, can be
obtained for estimating Teff. Suppose that S ′ = {(x, y)} is
a set of stellar spectra and their eﬀective temperatures. In
the present work, whether S ′ can be S or not is deﬁned to
introduce performance evaluation schemes.

On S ′, the performance of the estimator hW,h is evalu-
ated using three methods: mean error (ME), mean absolute
error (MAE), and standard deviation (SD). They are deﬁned
as follows:

M E =

M AE =

1

M X(x,y)∈S ′
M X(x,y)∈S ′

1

e(x, y),

(14)

|e(x, y)|,

(15)

SD =s 1

M X(x,y)∈S ′

(e(x, y) − M E)2,

(16)

where M is the number of stellar spectra in S ′, and em is
the error/diﬀerence between the reference value of the stellar
parameter and its estimate

e(x, y) = y − hW,h(x).

(17)

These evaluation schemes are widely used in related re-
search (Re Fiorentin et al. 2007; Jofre et al. 2010; Tan et al.
2013b), and more about them are discussed in Li et al.
(2015).

Similarly, the estimators for log g and [Fe/H] are ob-

tained and evaluated.

3 EXPERIMENTS

The scheme proposed above is evaluated on both real spectra
from SDSS/SEGUE and synthetic spectra calculated from
Kurucz’s new opacity distribution function (NEWODF)
models.

3.1 Performance on SDSS spectra

This work uses 50,000 real spectra from the SDSS/SEGUE
database (Abazajian et al. 2009; Yanny et al. 2009). The se-
lected spectra span the ranges [4088,9740] K in eﬀective tem-
perature Teff, [1.015, 4.998] dex in surface gravity log g, and
[-3.497, 0.268] dex in metallicity [Fe/H], as given by the
SDSS/SEGUE Spectroscopic Parameter Pipeline (SSPP;
Beers et al. 2006; Lee et al. 2008a,b; Allende Prieto et al.
2008; Smolinski et al. 2011; Lee et al. 2011). All stellar spec-
tra are initially shifted to their rest frames (zero radial ve-
locity) using the radial velocity provided by SSPP. They are
also rebinned to a maximal common log(wavelength) range
[3.581862, 3.963961] with a sampling step of 0.0001.2 We
consider the real spectra atmospheric parameters previously
estimated by SSPP as reference values Lee et al. (2008a,b);
Smolinski et al. (2011).

The real spectra are divided into two subsets: a training
set and a test set. The training set is the carrier of knowledge

1 The superscript ’ae’ is an abbreviation of ’autoencoder’.

2 The common wavelength range is approximately [3818.23,
9203.67]˚A.

c(cid:13) 2015 RAS, MNRAS 000, 1–5

4

Xiangru Li and Ruyang Pan

and used to estimate the model parameters W and b in
equation (2.1) based on the algorithm in section 2.3. The
test set acts as a referee to evaluate the performance of the
established model objectively. The sizes of the training and
test sets are 5,000 and 45,000, respectively.

On the test set of 30,000 SDSS spectra, the MAE con-
sistencies of the proposed scheme are 0.0048, 0.1477, and
0.1129 dex for log Teff (64.85 K for Teff), log g, and [Fe/H],
respectively, where the MAE evaluation method is deﬁned
in Equation (15). Therefore, the detected features provide
excellent linear support for estimating atmospheric parame-
ters Teff, log g, and [Fe/H].

Related works in literature use various performance eva-
luation methods. To obtain a better comparison with those
schemes, we also make a performance evaluation of the pro-
posed scheme based on ME (equation 14) and SD (equation
16) measures; the results are presented in Table 1 (a). Some
related results in the literature are summarized in Table 1
(b).

based on speciﬁc needs to estimate the atmospheric parame-
ters. Experiments both on real and synthetic spectra show
the favorable robustness and accurateness of the proposed
scheme.

ACKNOWLEDGMENTS

The authors would like to thank Professor Ali Luo and Fang
Zuo for their supports and discussions. This work is sup-
ported by the National Natural Science Foundation of China
(grant No: 61273248, 61075033, 61174190), the Natural Sci-
ence Foundation of Guangdong Province (2014A030313425,
S2011010003348), the Natural Science Foundation of Shan-
dong Province (ZR2014FM002) and the Joint Research
Fund in Astronomy (U1531242) under cooperative agree-
ment between the National Natural Science Foundation of
China (NSFC) and Chinese Academy of Sciences (CAS).

3.2 Performance on Synthetic spectra

REFERENCES

To further evaluate the proposed scheme further, a set
of 18,969 synthetic spectra is calculated from the SPEC-
TRUM (v2.76) package (Gray & Corbally 1994) with Ku-
rucz’s NEWODF models (Castelli & Kurucz 2003).

Our grids of synthetic stellar spectra span the param-
eter ranges [4000,9750] K in Teff (45 values, step sizes of
100K between 4000 and 7500 and 250 K between 7750 and
9750K), [1, 5] dex in log g (17 values, step size of 0.25 dex),
and [-3.6, 0.3] dex in [Fe/H] (27 values, step size of 0.2 dex
between -3.6 and -1 dex 0.1 dex between -1 and 0.3 dex).
The synthetic stellar spectra are also divided into two sub-
sets: a training set and a test set consisting of 5,000 and
13969 spectra, respectively.

Using the model obtained from 5000 synthetic spectra
and MAE measure, the performance of the proposed scheme
on synthetic spectra are 0.0011, 0.0182, and 0.0112 dex for
log Teff (14.90 K for Teff), log g, and [Fe/H], respectively.
More evaluation results are presented in Table 1 (c). Some
results in the related literature are presented in Table 1 (d).

4 CONCLUSION

In this work, we studied the estimation of eﬀective tempera-
ture (Teff), surface gravity (log g), and metallicity ([Fe/H])
from stellar spectra. This is commonly called the spectrum-
parameterization problem or stellar spectrum classiﬁcation
in related literature. The proposed scheme is evaluated us-
ing both real spectra from SDSS and synthetic spectra com-
puted from Kurucz’s model. Favorable results are achieved
in both cases.

The spectrum-parameterization problem aims to deter-
mine a mapping from a stellar spectrum to its parameters.
This work investigated this problem using a DNN. The pro-
posed scheme uses two procedures to determine the map-
ping: prelearning and ﬁne-tuning. The prelearning procedure
initializes the structure of the deep network by analyzing the
intrinsic properties of a set of empirical data (stellar spectra
in this work). Fine-tuning procedure readjusts the network

Abazajian K.N., Adelman-McCarthy J.K., Ag¨ueros M.A.,
Allam S.S., Allende Prieto C., An D., Anderson K.S.J.,
Anderson S.F. et al., 2009, ApJS,182, 543

Ahn C.P., Alexandroﬀ R., Allende Prieto C., Anderson
S.F., Anderton T., Andrews B.H., Aubourg E., Bailey S.
et al., 2012, ApJS, 203,21

Allende Prieto C., Sivarani T., Beers T.C., Lee Y.S.,
Koesterke L., Shetrone M., Sneden C., Lambert D.L., et
al., 2008, AJ, 136, 2070

Allende Prieto C., Beers T.C., Wilhelm R., et al. 2006,

ApJ, 636(2), 804

Andrew

Ng,

Ngiam
C.,

J.,
2010,

Foo
UFLDL

Suen

Y.,
http://uﬂdl.stanford.edu/wiki/index.php/UFLDL Tutorial

C.Y.,

Mai
Tutorial,

Bailer-Jones C.A.L., 2000, A&A, 357, 197
Beers T.C., Lee Y.S., Sivarani T., Allende Prieto C., Wil-
helm R., Re Fiorentin P., Bailer-Jones C., Norris J.E., et
al., 2006, Mem. Soc. Astron. Ital., 77, 1171

Castelli F., Gratton R. G., Kurucz R.L., 1997, A&A, 318,

841

Castelli F., Kurucz R.L., 2003,

in Piskunov N., Weiss
W.W., Gray D.F.,, eds, IAU Symp. 210, Modelling of Stel-
lar Atmospheres. Kluwer, Dordrecht, p.A20

Cui X., Zhao Y., Chu Y., Li G., Li Q., Zhang L., Su H.,
Yao Z. et al., 2012, Res. Astron. Astrophys., 12(9), 1197

Geary, R.C. 1935, Biometrika, 27(3/4), 310
Gilmore G., Randich S., Asplund M., Binney J., Bonifacio
P., Drew J., Feltzing S., Ferguson A. et al. 2012, The
Messenger, 147, 25

Giridhar S., Muneer S., Goswami A. 2006, Memorie della

Societa Astronomica Italiana, 77, 1130

Gray R.O., Corbally C.J., 1994, AJ, 107, 742
Gray R., Corbally C.J., 2009, Stellar Spectral Classiﬁ-
cation, Princeton University Press, 41 William Street,
Princeton, New Jersey, USA.

Grevesse N., Sauval A.J., 1998, Sov. Sci. Rev., 85, 161
Jofre P., Panter B., Hansen C.J., Weiss A., 2010, A&A,

517, 57

Lee Y.S., Beers T.C., Sivarani T., Allende Prieto C.,

c(cid:13) 2015 RAS, MNRAS 000, 1–5

Table 1. Performance of the proposed scheme

Parameterizing Stellar Spectra Using Deep Neural Networks

5

Estimation Method

Evaluation Method

log Teff (dex)

Teff (K)

log g (dex)

[Fe/H](dex)

(a) Performance of the proposed scheme on SDSS spectra

Proposed

MAE
ME
SD

0.0048
0.00005
0.0075

64.85
0.6219
104.97

0.1477
0.0149
0.2180

(b) Performance of some schemes in literature on SDSS spectra

ANN [1]
MAχ [2]
SVRG[3]
OLS [4]
SVRl [5]

MAE
ME
MAE

SD

MAE

0.0126

-

0.0075

-

0.0060

-

130
101.6
196.5
80.67

0.3644

0.5

0.1896
0.596
0.2225

(c) Performance of the proposed scheme on synthetic spectra

0.1129
0.0043
0.1582

0.1949

0.24

0.1821
0.466
0.1545

Estimation Method

Evaluation Method

log Teff (dex)

Teff (K)

log g (dex)

[Fe/H](dex)

Proposed

MAE
ME
SD

0.0011
0.0002
0.0016

14.90
2.861
22.55

0.0182
0.0029
0.0646

(d) Performance of some schemes in literature on SDSS spectra

ANN [1]
SVRG [3]

OLS [5]

MAE
MAE
MAE

0.0030
0.0008
0.0022

-
-

31.69

0.0245
0.0179
0.0337

0.0112
0.0008
0.0153

0.0269
0.0131
0.0268

Note. OLS (Ordinary Least Squares): linear least squares regression; SVRl: Support Vector Regression with a
linear kernel; SVRG: Support Vector Regression with a Gaussian kernel; ANN: Artiﬁcial Neural Network; MAχ:
MAssive compression of χ2. [1]:Re Fiorentin et al. (2007), [2]:Jofre et al. (2010), [3]:Li et al. (2014), [4]:Tan et al.
(2013b),[5]:Li et al. (2015) .

troscopy and Spectral Analysis, 33(6), 1701

Tan X., Pan J., Wang J., Luo A., Tu L., 2013, Spectrosc.

Spectral Anal., 33, 1397

Willemsen, P.G., Hilker, M., Kayser, A., et al. 2005, A&A,

436, 379

Yanny B., Rockosi C., Newberg H.J., Knapp G.R.,
Adelman-McCarthy J.K., Alcorn B., Allam S., Allende
Prieto C., et al., 2009, AJ, 137, 4377

York D.G., Adelman J., Anderson J.E.Jr., Anderson S.F.,
Annis J., Bahcall N.A., Bakken J.A., Barkhouser R.et al.,
2000, AJ, 120, 1579

Zhao G., Chen Y., Shi J., Liang Y., Hou J., Chen L., Zhang

H., Li A., 2006, Chin. J. Astron. Astrophys., 6, 265

This paper has been typeset from a TEX/ LATEX ﬁle prepared
by the author.

Koesterke L., Wilhelm R., Re Fiorentin P., Bailer-Jones
C.A.L., et al, 2008a, AJ, 136, 2022

Lee Y.S., Beers T.C., Sivarani T., Johnson J.A., An D.,
Wilhelm R., Allende Prieto C., Koesterke L., et al., 2008b,
AJ, 136, 2050

Lee Y.S., Beers T.C., Allende Prieto C., Lai D.K., Rockosi
C.M., Morrison H.L., Johnson J.A., An D., Sivarani T.,
Yanny B., 2011, AJ, 141, 90

Li X., Wu Q. M. J., Luo A., Zhao Y., Lu Y., Zuo F., Yang

T., Wang Y., 2014, ApJ, 790, 105

Li X., Lu Y., Comte G., Luo A., Zhao Y., Wang Y., 2015,

ApJS, 218(1),3
Manteiga M., ORD ´O ˜NEZ D., Dafonte C., ARCAY B.,
2010, PASP122, 608

Mishenina T.V., Bienaym´e O., Gorbaneva T.I., et al. 2006,

A&A, 456(3), 1109

Muirhead P.S., Hamren K., Schlawin E., Rojas-Ayala B.,

Covey K.R., Lloyd J.P., 2012, ApJL, 750, L37

Randich, S., Gilmore, G., Gaia-ESO Consortium. 2013,

The Messenger, 154, 47

Re Fiorentin P., Bailer-Jones C.A.L., Lee Y.S., Beers T.C.,
Sivarani T., Wilhelm R., Allende Prieto C., Norris J.E.,
2007, A&A, 467, 1373

Rumelhart D.E., Hinton G.E., Williams R.J., 1986, Nature,

323, 533

Shkedy Z., Decin L., Molenberghs G., Aerts C. 2007, MN-

RAS, 377(1), 120

Smolinski J.P., Lee Y.S., Beers T.C., An D., Bickerton S.J.,
Johnson J.A., Loomis C.P., Rockosi C.M., Sivarani T.,
Yanny B., 2011, AJ, 141(3), 89

Snider S., Allende Prieto C., von Hippel T., et al. 2001,

ApJ, 562(1), 528

Tan X., Pan J., Wang J., Luo A., Tu L., 2013a, Spec-

c(cid:13) 2015 RAS, MNRAS 000, 1–5

