6
1
0
2

 
r
a

M
7

 

 
 
]

.

A
N
h
t
a
m

[
 
 

1
v
0
2
1
2
0

.

3
0
6
1
:
v
i
X
r
a

A new relaxed HSS preconditioner for Saddle point problems

Davod khojasteh Salkuyeha, Mohsen Masoudia

aFaculty of Mathematical Sciences, University of Guilan, Rasht, Iran

Abstract

We present a preconditioner for saddle point problems. The proposed preconditioner
is extracted from a stationary iterative method which is convergent under a mild condi-
tion. Some properties of the preconditioner as well as the eigenvalue distribution of the
preconditioned matrix are presented. The preconditioned system is solved by a Krylov
subspace method like restarted GMRES. Finally, some numerical experiments on test
problems arisen from ﬁnite element discretization of the Stokes problem are given to
show the eﬀectiveness of the preconditioners.

Keywords: Saddle point problems, HSS preconditioner, Preconditioning, Krylov
subspace method, GMRES.
2010 MSC: 65F08, 65F10.

1. Introduction

We study the solution of system linear equation with the following block 2×2 structure

Au = (cid:20) A BT

−B 0 (cid:21)(cid:20) x

y (cid:21) = (cid:20) f

g (cid:21) ≡ b,

(1.1)

where A ∈ Rn×n is a symmetric positive deﬁnite matrix, B ∈ Rm×n with rank(B) =
m < n. In addition, x, f ∈ Rn, and y, g ∈ Rm. We also assume that the matrices A and
B are large and sparse. According to Lemma 1.1 in [13] the matrix A is nonsingular.
Such systems are called saddle point problems and appear in a variety of scientiﬁc and
engineering problems; e.g., computational ﬂuid dynamics, constrained optimization, etc.
The readers are referred to [3, 14] for more discussion on this subject.

Several eﬃcient iterative methods have been proposed during the recent decades to
solve the saddle point problems (1.1), such as SOR-like method [25], modiﬁed block SSOR
iteration [1, 2], generalized SOR method [10], Uzawa method [29], parametrized inexact
Uzawa methods [11], Hermitian and skew-Hermitian splitting (HSS) iteration methods
[4, 8, 7] and so on. However, in some situations, these iterative methods may be less eﬃ-
cient than the Krylov subspace methods [29]. When Krylov subspace methods applied to
the saddle point problem (1.1), tend to converge slowly. But these methods can produce a

Email addresses: khojasteh@guilan.ac.ir (Davod khojasteh Salkuyeh),

masoudi_mohsen@phd.guilan.ac.ir (Mohsen Masoudi)

Preprint submitted to Elsevier

March 8, 2016

preconditioner for accelerating the rate of convergence of the Krylov subspace method. In
general, favourable rates of convergence of Krylov subspace methods are often associated
with a clustering of most of the eigenvalues of preconditioned matrices around 1 and away
from zero [12]. In view of this, many preconditioners have been presented in literature,
e.g., block diagonal preconditioners [27, 31], constraint preconditioners [9, 26], block tri-
angular preconditioners [6, 19, 30, 32], parametrized block triangular preconditioners [24],
Hermitian and skew-Hermitian splitting (HSS) preconditioners [8, 13, 28? ].

In [7], Bai et al. proposed the HSS iteration method to solve non-Hermitian positive
deﬁnite linear systems Ax = b which unconditionally converges to the unique solution of
system. For a given initial guess x0, the HSS iteration can be written as

(cid:26) (αI + H) xk+ 1

2 = (αI − S) xk + b,

(αI + S) xk+1 = (αI − H) xk+ 1

2 + b,

k = 0, 1, 2, . . . ,

(1.2)

where α > 0 and A = H + S, in which H = (A + A∗)/2 and S = (A − A∗)/2, where A∗
denotes the conjugate transpose of A.

Benzi and Golub in [13] have applied the HSS iteration method to the generalized sad-
dle point problem (saddle point problems with nonzero (2, 2)-block). As they mentioned
the convergence of the method to solve the saddle point problem is typically too slow
for the method to be competitive. For this reason they proposed using a nonsymmetric
Krylov subspace method like the GMRES algorithm or its restarted version to acceler-
ate the convergence of the iteration. Since the method has promising performance and
elegant mathematical properties, it has attracted many researchers attention and many
algorithmic variants and theoretical analysis of the HSS iteration for saddle point prob-
lems have been presented. In [5], Bai et al. investigated the convergence properties of the
HSS iteration for the saddle point problem (1.1) with A being non-Hermitian and positive
semideﬁnite. In [16], Benzi and Guo proposed a dimensional split (DS) preconditioner for
Stokes and linearized Navier-Stokes equations. The DS preconditioner is extracted from
an HSS iteration method based on the dimensional splitting of A. A modiﬁcation of the
DS preconditioner has been presented by Cao et al. in [18]. Benzi et al. have presented
a relaxed version of DS in [17]. Some variants of the HSS preconditioner including its
relaxed versions have also been presented in the literature (see, e.g., [33, 23, 20]).

In this paper, we present a new preconditioner which can be considered as relaxed
version of the HSS preconditioner for the saddle point problem. Throughout the paper,
for a matrix X, ρ(X) and X ∗ stand for the spectral radius and conjugate transpose of X,
respectively. For a vector x ∈ Cn, kxk2 denotes the Euclidian norm of x.

This paper is organized as follows. In Section 2 we present our preconditioner. Some
properties of the preconditioner are presented in Section 3. Implementation of the pro-
posed preconditioner is presented in Section 4. Numerical experiments are given in Section
5. The paper is ended by some concluding remarks in Section 6.

2. A review of the HSS preconditioner and its relaxed version

In this section, we ﬁrst brieﬂy review the HSS iteration method and the induced HSS
preconditioner for the saddle point problem. Then, a relaxed version of the HSS (RHSS)

2

preconditioner, proposed by Cao et al. in [20], is presented. Next, we give a new relaxed
HSS (REHSS) preconditioner and investigate some of its properties.

2.1. The HSS preconditioner for the saddle point problem

According to the HSS iteration, the matrix A is split as

A = H + S,

where

H =

1

2 (cid:0)A + AT(cid:1) = (cid:20) A 0

0 0 (cid:21) and S =

1

−B 0 (cid:21).
2 (cid:0)A − AT(cid:1) = (cid:20) 0 BT

Obviously, both of the matrices αI + H and αI + S are nonsingular. In this case the HSS
iteration for the saddle point problem (1.1) is written as

(cid:26) (αI + H) xk+ 1

(αI + S) xk+1 = (αI − H) xk+ 1

2 + b.

2 = (αI − S) xk + b,

(2.1)

Computing xk+ 1
the iteration

2 from the ﬁrst equation and substituting in the second equations yields

xk+1 = ΓHSSxk + c,

where

and

ΓHSS = (αI + S)−1 (αI − H) (αI + H)−1 (αI − S) ,

c = 2α (αI + H)−1 (αI + S)−1 b.

It is known that there is a unique splitting A = Mα − Nα, with Mα nonsingular, which
induces the iteration matrix ΓHSS, i.e.,

ΓHSS = M−1

α Nα = I − M−1

α A,

where

Mα =

1
2α

(αI + H) (αI + S) , Nα =

1
2α

(αI − H) (αI − S) .

(2.2)

Benzi et al.

in [13] have shown that for all α > 0, the HSS iteration is convergent
unconditionally to the unique solution of the saddle point problem (1.1). As we know the
HSS iteration serves the preconditioner Mα for the system (1.1) which is called the HSS
preconditioner. Since the pre-factor 1
2α in the HSS preconditioner Mα has no eﬀect on
the preconditioned system, the HSS preconditioner can be taken as

PHSS =

1
α

(αI + H) (αI + S) =

= (cid:20) A + αI BT + 1

αABT
αI

−B

0

1

α(cid:20) A + αI
(cid:21).

0

αI (cid:21)(cid:20) αI BT
−B αI (cid:21)

(2.3)

The diﬀerence between the HSS preconditioner PHSS and the coeﬃcient matrix A is

RHSS = PHSS − A = (cid:20) αI

0

1
α ABT

αI

(cid:21).

(2.4)

3

2.2. The RHSS preconditioner

From (2.4), we see that as α tends to zero, the diagonal blocks tend to zero while
the nonzero oﬀ-diagonal block becomes unbounded. Hence, it is sought an appropriate α
to balance the weight of both parts. To do so, Cao et al.
in [20] consider the following
relaxed HSS (RHSS) preconditioner for the saddle point problem (1.1)

PRHSS =

1

α(cid:20) A 0

0 αI (cid:21)(cid:20) αI BT

−B 0 (cid:21) = (cid:20) A 1

−B

αABT

0

(cid:21).

(2.5)

In this case, the diﬀerence between the RHSS preconditioner and the matrix A is given
by

RRHSS = PRHSS − A = (cid:20) 0 (cid:0) 1

αA − I(cid:1) BT

0

0

(cid:21).

(2.6)

Here, we see that as the parameter α tends to zero, the (1, 2)-block of RRHSS becomes
unbounded.

From (2.6), we have A = PRHSS − RRHSS which produces the RHSS iteration

PRHSSxk+1 = RRHSSxk + b,

k = 0, 1, . . . ,

where x0 is an initial guess. Hence, the iteration matrix of the RHSS iteration is given
by ΓRHSS = P −1
RHSSRRHSS. In [20], it was shown that ρ (ΓRHSS) < 1 for all 0 < α < 2
µ1
and the optimal value of α is αopt = 2/(µ1 + µm), where µ1 and µm are, respectively, the

largest and smallest eigenvalues of the matrix (cid:0)BBT(cid:1)−1(cid:0)BA−1BT(cid:1).

3. The REHSS preconditioner

As we mentioned when α tends to zero, the (1, 2)-block in both of the matrices RHSS
and RRHSS become unbounded. To overcome this problem we consider the following
splitting for the matrix A as

A = PREHSS − RREHSS = (cid:20) A ABT

−B αI (cid:21) −(cid:20) 0 (A − I)BT

αI

0

(cid:21),

(3.1)

where α > 0. As α tends to zero the (2, 2)-block of RREHSS tends to zero and in contrast
with the HSS and the RHSS preconditioners the (1, 2)-block remains bounded. This shows
that, for small values of α the REHSS preconditioner is more closer than the HSS and
the RHSS preconditioners to A.

From the REHSS splitting (3.1) we state the REHSS iteration as

−B αI (cid:21)uk+1 = (cid:20) 0 (A − I)BT
(cid:20) A ABT

αI

0

(cid:21)uk +(cid:20) f

g (cid:21),

for the saddle point problem (1.1).
iteration is given by

In this case, the iteration matrix of the REHSS

ΓREHSS = P −1

REHSSRREHSS = (cid:20) A ABT

−B αI (cid:21)−1(cid:20) 0 (A − I)BT

αI

0

(cid:21).

(3.2)

4

The next theorem provides a suﬃcient condition for the convergence of the REHSS iter-
ation.

Theorem 1. Let Q = B(cid:0) 1

it holds that ρ(ΓREHSS) < 1.

2A−1 − I(cid:1) BT . If δ = λmax(Q), then for every α > max{δ, 0},

Proof. We have

PREHSS = M1M2 = (cid:20) A 0

0 I (cid:21)(cid:20) I BT

−B αI (cid:21),

where

Therefore

M2 = (cid:20) I BT

−B αI (cid:21) = (cid:20) I

−B I (cid:21)(cid:20) I

0 αI + BBT (cid:21)(cid:20) I BT

I (cid:21).

0

0

0

P −1

REHSS = M−1

2 M−1
1
= (cid:20) I −BT
= (cid:20) A−1 − BT S−1BA−1 −BT S−1

(cid:21)(cid:20) I
0 (cid:0)αI + BBT(cid:1)−1 (cid:21)(cid:20) I
(cid:21),

S−1BA−1

S−1

0

0

I

0

B I (cid:21)(cid:20) A−1 0
I (cid:21)

0

where S = αI + BBT . Hence, we get

P −1

REHSSA = (cid:20) I

0

˜A
ˆA (cid:21),

(3.3)

(3.4)

where ˜A = A−1BT − BT S−1BA−1BT and ˆA = S−1BA−1BT . As a result, we obtain

ΓREHSS = P −1

REHSSRREHSS = P −1

REHSS (PREHSS − A)

= I − P −1

REHSSA = (cid:20) 0 − ˜A

0 I − ˆA (cid:21).

Hence, if λ is an eigenvalue of the matrix ΓREHSS, then λ = 0 or λ = 1 − µ, where µ is
an eigenvalue of the matrix ˆA. Therefore, there exists a vector x 6= 0 such that

Without loss of generality, we assume that kxk2 = 1. Since BT x 6= 0, we have

ˆAx = (cid:0)αI + BBT(cid:1)−1

BA−1BT x = µx.

Hence, |λ| < 1 if and only if

µ =

x∗BA−1BT x
α + x∗BBT x

> 0.

x∗BA−1BT x
α + x∗BBT x

< 2.

5

which is equivalent to

α > x∗B(cid:18) 1

2

A−1 − I(cid:19) BT x = x∗Qx.

(3.5)

Therefore, a suﬃcient condition to have |λ| < 1 is

α > max
kxk2=1

x∗Qx = λmax(Q) = δ.

It is necessary to mention that the matrix Q is symmetric and hence all of its eigenvales
are real.

Corollary 1. Let

λmin(A) >

1
2

κ(B)2,

(3.6)

where κ(B) and λmin(A) stand for the spectral condition number and smallest eigenvalue
A. Then, for every α > 0, it holds that ρ(ΓREHSS) < 1.

Proof. From [29, Theorem 1.22] we have

x∗BA−1BT x ≤ λmax(A−1)x∗BBT x ≤

1

λmin(A)

λmax(BBT )x∗x =

σmax(B)2
λmin(A)

,

x∗BBT x ≥ λmin(BBT )x∗x = σmin(B)2,

where σmin(B) and σmax(B) stand for the smallest and largest singular values of B. From
these inequalities and Eq. (3.5) we deduce

x∗Qx =

1
2

x∗BA−1BT x − x∗BBT x ≤

1
2

σmax(B)2
λmin(A)

− σmin(B)2 = θ.

It follows from this equatin that δ ≤ θ, where δ was deﬁned in Theorem 1. Hence, if
α > max{0, θ}, then the convergence of the REHSS iteration is achieved. Now, if θ < 0
then for every α > 0 we get ρ(ΓREHSS) < 1. Obviously, θ < 0 is equivalent to the
condition (3.6).

The next theorem analyses the behavior of P −1

REHSSA.

Theorem 2. (a) For α > 0, the preconditioned matrix P −1
REHSSA has eigenvalue 1 of
algebraic multiplicity at least n. The remaining eigenvalues are µi, where µi are the

eigenvalues of the m × m matrix ˆA = (cid:0)αI + BBT(cid:1)−1

(b) Let (µ, [x; y]) be an eigenpair of P −1
be written as µ = (αˆb + ˆc)/ˆa, where

BA−1BT .

REHSSA. Then, x 6= 0 and µ either is µ = 1 or can

ˆa = x∗(cid:0)αI + BT B(cid:1) A(cid:0)αI + BT B(cid:1) x,

Moreover, when α → 0, then either is µ = 1 or

ˆb = x∗(cid:0)BT B(cid:1) x,

ˆc = x∗(cid:0)BT B(cid:1)2

x.

1

µmax(A)

6 µ 6

1

µmin(A)

,

where µmin(A) and µmin(A) are the smallest and largest eigenvalues of A, respectively.

6

Proof. Part (a) follows immediately from Eq. (3.4). To prove (b), let (µ, [x; y]) be an
eigenpair of P −1

REHSSA. Therefore,

A(cid:20) x

y (cid:21) = µPREHSS(cid:20) x

y (cid:21),

which is equivalent to

Hence

Ax + BT y = µAx + µABT y,

−Bx = −µBx + µαy.

(µ − 1) Ax + (µA − I) BT y = 0,
µαy = (µ − 1) Bx.

Premultiplying both sides of (3.9) by BT yields

µαBT y = (µ − 1) BT Bx.

Multiplying both side of (3.8) by µα and substituting (3.10) in it, gives

µα (µ − 1) Ax + (µA − I) (µ − 1) BT Bx = 0.

(3.7)

(3.8)
(3.9)

(3.10)

We show that x 6= 0. Otherwise, from (3.7) we get µ = 0 or y = 0. In fact, neither of
them can be zero. So x 6= 0. Without loss of generality, let kxk2 = 1. Hence

µ2A(cid:0)αI + BT B(cid:1) x − µ(cid:0)A(cid:0)αI + BT B(cid:1) + BT B(cid:1) x + BT Bx = 0.

(3.11)

Multiplying x∗(cid:0)αI + BT B(cid:1) to both sides of (3.11), yields

ˆaµ2 −(cid:16)ˆa + αˆb + ˆc(cid:17) µ +(cid:16)αˆb + ˆc(cid:17) = 0.

The roots of this quadratic equation are µ = 1 and

µ =

αˆb + ˆc

ˆa

=

α2x∗Ax + α (x∗BT BAx + x∗ABT Bx) + x∗BT BABT Bx

αˆb + ˆc

.

To prove the last part of theorem, we show that if Bx = 0, then µ = 1. If Bx = 0, then
it follows from Eq. (3.9) that y = 0. Substituting, this in Eq. (3.8), yields (µ − 1)Ax = 0.
Now, since Ax 6= 0, we conclude that µ = 1. Therefore, if α → 0, then µ = 1 or

µ =

x∗(cid:0)BT B(cid:1)2

x∗BT BABT Bx

x

=

(BT Bx)∗(BT Bx)
(BT Bx)∗A(BT Bx)

=

z∗z
z∗Az

,

where z = BT Bx. Since, A is symmetric positive deﬁnite we have

which completes the proof.

1

µmax(A)

6

z∗z
z∗Az

6

1

µmin(A)

,

7

Theorem 3. The degree of the minimal polynomial of the preconditioned matrix P −1
is at most m + 1. Thus, the dimension of the Krylov subspace K(P −1
m + 1.

REHSSA
REHSSA, b) is at most

Proof. Let χ be the characteristic polynomial of the preconditioned matrix P −1
using (3.4), we have

REHSSA. By

χ(x) = (x − 1)n

m

Yi=1

(x − µi) ,

where µi, for i = 1, . . . , m, are the eigenvalues of the matrix ˆA. Let

Therefore

p(x) = (x − 1)

m

Yi=1

(x − µi) .

m

p(P −1

˜A

REHSSA) = (cid:0)P −1
= (cid:20) 0



Yi=1(cid:0)P −1
REHSSA − I(cid:1)
ˆA − Im (cid:21) m
(cid:20) (1 − µi)In
Yi=1

( ˆA − µiIm)
Yi=1

0 ( ˆA − Im)

( ˆA − µiIm)

m

Yi=1

0

0

m

˜A

=

REHSSA − µiI(cid:1)
˜A

ˆA − µiIm (cid:21)

0

.

m

Since for i = 1, . . . , m, µi is an eigenvalue of the matrix ˆA, we have

Yi=1
REHSSA) = 0. Hence minimal polynomial of the matrix P −1

and so p(P −1

REHSSA is as

( ˆA − µiIm) = 0

m(x) = (x − 1)

m

Yi=1

(x − µi) ,

where k 6 m. Therefore the degree of the minimal polynomial preconditioned matrix
P −1
REHSSA is at most m + 1. Hence, by [29, Proposition 6.1], the dimension of the Krylov
subspace K(P −1

REHSSA, b) is also at most m + 1.

4. Implementation of PREH SS

We use the restarted version of the GMRES (denoted by GMRES(m)) in conjunction
with preconditioner PREHSS to solve the saddle point problem (1.1). At each step of
applying the preconditioner PREHSS within the GMRES(m) algorithm we need to compute
a vector of the form z = P −1
REHSSr for a given vector r = [r1; r2] where r1 ∈ Rn and r2 ∈ Rm.

8

Let z = [z1; z2], where z1 ∈ Rn and z2 ∈ Rm. Now, form Eq. (3.3) we can compute the
vector z via

(cid:20) z1
z2 (cid:21) = (cid:20) I −BT

0

I

0

(cid:21)(cid:20) I
0 (cid:0)αI + BBT(cid:1)−1 (cid:21)(cid:20) I

0

B I (cid:21)(cid:20) A−1 0

I (cid:21)(cid:20) r1

r2 (cid:21).

0

We can use Algorithm 1 to compute the z

Algorithm 1. Computation of z = P −1

REHSSr.

1. Solve Aw1 = r1 for w1.
2. Solve (αI + BBT )w2 = Bw1 + r2 for w2.
3. z2 := w2.
4. z1 := w1 − BT w2.

Both of the matrices A and αI + BBT are symmetric positive deﬁnite. Hence, we can
solve systems appeared in steps 1 and 2 of Algorithm 1 exactly by the Cholesky factor-
ization or approximately by the conjugate gadient (CG) or the preconditioned conjugate
gradient (PCG) iterative method.

5. Numerical experiments

In this section, we present some numerical experiments to illustrate the eﬀectiveness
of the preconditioner PREHSS for the saddle point problem (1.1). The restarted GMRES
method [29] with restarting frequency 30, i.e., GMRES (30), is appled to the left pre-
conditioned saddle point problem (1.1) in conjunction with the preconditioner PREHSS
and the corresponding numerical results are compared with those of the preconditioners
PHSS and PRHSS in terms of iteration counts and CPU timings. All runs are performed
in Matlab 2014 on an Intel core i7 (12G RAM) Windows 8 system.

In all the tests, the initial vector is set to be a zero vector and the right-hand side
vector b = [f ; g] ∈ Rn+m is chosen such that the exact solution of the saddle point problem
(1.1) is a vector of all ones. We use the stopping criterion

kPrkk2 6 10−12 kPbk2 ,

where rk = b − Auk is the residual at the kth iteration and P is one of the preconditioners
PHSS, PRHSS or PREHSS. The maximum number of the iterations and the maximum
elapsed CPU time are set to be kmax = 500 and tmax = 3600s, respectively. Throughout
this section, “IT” and “CPU” stand for the numbers of the restarts in GMRES(m) and
the CPU time, respectively. In all the tables, a dagger (†) shows that the method has
not converged in at most kmax iterations. Similarly, a “‡” shows that the method has not
converged after elapsing tmax seconds. At each step of applying the preconditioners PHSS,
PRHSS, and PREHSS, we need to solve two sub-systems with symmetric positive deﬁnite
coeﬃcients matrix (see Algorithm 1 and [20, Algorithm 3.3 and Algorithm 3.4]) and all
of these systems are solved by the Cholesky factorization.

9

Table 1: The size of the matrices A and B for diﬀerent of grids.

channel domain problem

lid driven cavity and cooliding ﬂow problem

Gride
16 × 16
32 × 32
64 × 64

128 × 128
256 × 256

n
578
2178
8450
33282
132098

m
192
768
3072
12288
49152

nnz(A)

6698
29546
124550
511152
2070764

nnz(B)

2084
10142
45062
192174
791738

n
578
2178
8450
33282
132098

m
190
766
3070
12286
49150

nnz(A)

6178
28418
122206
506376
2061140

nnz(B)

1967
9868
44516
191084
789560

Consider the Stokes problem (see [21] or [22, page 221])

(cid:26) −△u + ∇p = f,

∇.u = 0,

(5.1)

in Ω = [−1, 1] × [−1, 1], where ∆, ∇, u, and p stand for Laplace operator, gradient
operator, velocity and pressure of the ﬂuid, respectively, with suitable boundary condition
on ∂Ω. It is known that many discretization schemes for (5.1) will lead to saddle point
problems of the form (1.1). We consider Q2-P1 ﬁnite element discretizations on uniform
grids on the unit square of the tree standard model problems (see [21, 22] )

1. The leaky lid-driven cavity problem;
2. The channel domain problem;
3. The colliding ﬂow problem.

We use the IFISS software package developed by Elman et al.

[21] to generate the
linear systems corresponding to 16 × 16, 32 × 32, 64 × 64, 128 × 128, and 256 × 256
meshes. The IFISS software provides the matrices Ast and Bst for the matrices A and B,
respectively. For the channel domain problem the matrix Bst is of full rank, but for the
colliding ﬂow and lid driven cavity problem is rank deﬁcient. Therefore, in these cases
we drop two ﬁrst rows of Bst to get a full rank matrix. Generic information of the test
problems, including n, m, nnz(A) and nnz(B), are given in Table 1 where nnz stand
for the number of the nonzero entries of a matrix. We present the numerical results for
diﬀerent values of α (α = 10−6, 10−4, 10−2, 1, 102).

Numerical results for the leaky lid-driven cavity, the channel domain , and the colliding
ﬂow problems are, respectively, presented in Tables 2, 3 and 4. In all the tables “IT” stands
for the number of restarts in the GMRES(30) algorithm and “CPU” denotes the elapsed
CPU time for the convergence. As the numerical results show for all the three problems
the preconditioner PREHSS is more eﬀective than the preconditioners PHSS and PRHSS
in terms of the iteration counts and CPU times. As we see, the GMRES(30) method
for the preconditioned system with preconditioner PREHSS always converges, whereas it
does not converge for other two preconditioners. Another observation which can be posed
here is that, despite preconditioners PHSS and PRHSS, the behavior of the preconditioned
iteration corresponding to the preconditioner PREHSS is not very sensitive to the choice
of α.

In Figure 5, the eigenvalues distribution of the matrices A and the preconditioned
REHSSA for the cavity problem on 32 × 32 grid, with diﬀerent values of α (

matrix P −1

10

Table 2: Numerical results lid driven cavity problem on 2r × 2r grid.

α = 10−4

α = 10−2

α = 1

α = 102

Preconditioner

PH SS
PRH SS
PREH SS

PH SS
PRH SS
PREH SS

PH SS
PRH SS
PREH SS

PH SS
PRH SS
PREH SS

PH SS
PRH SS
PREH SS

IT
4
3
3
8
5
5
14
8
11
38
15
9

CPU
0.04
0.02
0.02
0.37
0.19
0.21
3.87
1.97
3.08
76.32
30.85
17.3

115
37
5

2064.44

641.5
67.63

IT
5
3
3
9
5
4
47
8
3
†
14
3
-
28
3

CPU
0.06
0.02
0.02
0.45
0.2
0.12
14.21
2.03
0.47

-

25.95
3.86

‡

471.97
33.11

IT
13
3
3

144

5
3
†
9
3
†
17
3
-
38
3

CPU
0.16
0.02
0.02
8.45
0.21
0.07

-

2.29
0.42

-

33.46
3.69

‡

648.74
31.91

IT
106

4
3
†
9
3
†
27
3
†
79
3
-
-
3

CPU
1.46
0.04
0.02

-

0.43
0.07

-
8.1
0.39

-

160.91

3.86

‡
‡

27.43

Table 3: Numerical results for channel domain problem on 2r × 2r grid.

α = 10−4

α = 10−2

α = 1

α = 102

Preconditioner

PH SS
PRH SS
PREH SS

PH SS
PRH SS
PREH SS

PH SS
PRH SS
PREH SS

PH SS
PRH SS
PREH SS

PH SS
PRH SS
PREH SS

IT
5
3
3
9
5
5
21
8
6
48
17
5

CPU
0.05
0.02
0.02
0.48
0.22
0.21
5.94
2.08
1.45
96.57
32.79
7.45

169
42
4

3093.67
758.74
40.52

IT
6
3
3
10
5
3
13
8
3
15
16
3
20
37
3

CPU
0.06
0.03
0.02
0.5
0.22
0.09
3.46
2.06
0.4

28.06
30.53
2.84

336.76
654.38
24.29

IT
7
3
3
13
5
3
28
8
3
83
15
3
-
34
3

CPU
0.07
0.02
0.02
0.72
0.2
0.07
8.39
1.96
0.4

169.45
28.83
2.65

‡

603.85
22.11

IT
17
4
3
47
9
3

498
21
3

320
86
3
-
-
2

CPU
0.21
0.03
0.02
2.84
0.47
0.06

154.59

6.15
0.38

1089.81

178.1
2.52

‡
‡

16.03

r

4

5

6

7

8

r

4

5

6

7

8

α = 0.1, α = 1, and α = 10) are displayed. We see that the eigenvalues of preconditioned
matrices are well-clustered.

In Figure 5, the number of iterations and the CPU time of GMRES (30) for solving
the preconditioned system with the preconditioners PREHSS, PHSS and PRHSS for the
channel domain problem with 128 × 128 grid for diﬀerent values of α are presented. As
we see the PREHSS is superior to the preconditioners PHSS and PRHSS, in terms of the
iterations count and the CPU times.

11

Table 4: Numerical results for the colliding ﬂow problem on 2r × 2r grid.

α = 10−4

α = 10−2

α = 1

α = 102

r

4

5

6

7

8

Preconditioner

PH SS
PRH SS
PREH SS

PH SS
PRH SS
PREH SS

PH SS
PRH SS
PREH SS

PH SS
PRH SS
PREH SS

PH SS
PRH SS
PREH SS

IT
4
3
3
8
5
5
14
8
11
39
15
9

CPU
0.04
0.02
0.02
0.37
0.2
0.21
3.91
2.01
3.15
87.2
29.54
16.61

123
39
5

2222.41
700.77
70.94

IT
5
3
3
9
5
4
53
8
3
†
14
3
-
31
3

CPU
0.05
0.02
0.02
0.43
0.22
0.13
16.24
2.05
0.48

-

27.68
4.11

‡

548.88
35.25

IT
13
3
3

115

5
3
†
8
3
†
17
3
-
38
3

CPU
0.16
0.02
0.02
7.1
0.19
0.09

-

2.21
0.43

-

35.07
3.81

‡

690.25
33.79

IT
90
4
3
†
9
3
†
28
3
†

CPU
1.17
0.03]
0.02

-

0.44
0.08

-

8.55
0.4
-

111

250.86

3
-
-
3

4.4
‡
‡

27.73

No preconditioning

 α=0.01

0.02

0.01

0

−0.01

−0.02

0

1

0.5

0

−0.5

−1

0

2

4

6

8

 α=0.1

1

0.5

0

−0.5

−1

0

1

0.5

0

−0.5

0.5

1

1.5

 α=1

0.5

1

−1

0

0.5

1

Figure 1: Eigenvalues distribution of the matrices A and P −1
32 × 32 grid with diﬀerent values of α (α = 0.01, 0.1, 1).

REHSSA for the for the cavity problem on

12

80

60

40

20

 
0
−3

40

30

20

10

 
0
−3

  α=10t

−2
t

  α=10t

−1.5

 

−1

 

timeHSS
timeRHSS
timeREHSS

−2.5

iterREHSS
iterRHSS
iterHSS

−2.5

−2
t

−1.5

−1

)
0
3
(
 

S
E
R
M
G

 
f
o
 
e
m

i
t

)
0
3
(
 

S
E
R
M
G

 
f

o

 
s
p
e

t
s
 

n
o

i
t

a
r
e

t
i

Figure 2: Number of iterations and CPU time with respect to t = log10 α for the channel domain problem
with 128 × 128 grid.

6. Concluding remarks

We have presented a new relaxed version of the Hermitian and skew-Hermitian split-
ting preconditioner say REHSS for the saddle point problem (1.1). Some properties of the
preconditioner have been presented. From numerical point of view the proposed precon-
ditioner has been compared with two recently proposed preconditioners HSS and RHSS.
Numerical results showed that the REHSS preconditioner is superior to the HSS and
RHSS preconditioners. Moreover, the REHSS preconditioner is not very sensitive to the
involving preconditioner.

Acknowledgements

This work is supported by the Iran National Science Foundation (INSF) under Grant

No. 93050251.

References

References

[1] Z.-Z. Bai, A class of modiﬁed block SSOR preconditioners for symmetric positive

deﬁnite systems of linear equations, Adv. Comput. Math. 10 (1999) 169-186.

13

[2] Z.-Z. Bai, Modiﬁed block SSOR preconditioners for symmetric positive deﬁnite linear

systems, Ann. Oper. Res. 103 (2001) 263-282.

[3] Z.-Z. Bai, Structured preconditioners for nonsingular matrices of block two-by-two

structures, Math. Comput. 75 (2006) 791-815.

[4] Z.-Z. Bai, G.H. Golub, Accelerated Hermitian and skew-Hermitian splitting iteration

methods for saddle-point problems, IMA J. Numer. Anal. 27 (2007) 1-23.

[5] Z.-Z. Bai, G.H. Golub, C.-K. Li, Convergence properties of preconditioned Hermi-
tian and skew-Hermitian splitting methods for non-Hermitian positive semideﬁnite
matrices, Math. Comput., 76 (2007), 287-298.

[6] Z.-Z. Bai, G.H. Golub, L.-Z. Lu, J.-F. Yin, Block triangular and skew-Hermitian
splitting methods for positive-deﬁnite linear systems, SIAM J. Sci. Comput. 26 (2004)
844-863.

[7] Z.-Z. Bai, G.H. Golub, M.K. Ng, Hermitian and skew-Hermitian splitting methods
for non-Hermitian positive deﬁnite linear systems, SIAM J. Matrix Anal. Appl. 24
(2003) 603-626.

[8] Z.-Z. Bai, G.H. Golub, J.-Y. Pan, Preconditioned Hermitian and skew-Hermitian
splitting methods for non-Hermitian positive semideﬁnite linear systems, Numer.
Math. 98 (2004) 1-32.

[9] Z.-Z. Bai, M.K. Ng, Z.-Q. Wang, Constraint preconditioners for symmetric indeﬁnite

matrices, SIAM J. Matrix Anal. Appl. 31 (2009) 410-433.

[10] Z.-Z. Bai, B.N. Parlett, Z.-Q. Wang, On generalized successive overrelaxation meth-

ods for augmented linear systems, Numer. Math. 102 (2005) 1-38.

[11] Z.-Z. Bai, Z.-Q. Wang, On parametrized inexact Uzawa methods for generalized

saddle point problems, Linear Algebra Appl. 428 (2008) 2900-2932.

[12] M. Benzi, Preconditioning Techniques for Large Linear Systems: A Survey, J. Com-

put. Phys. 182 (2002) 418-477.

[13] M. Benzi, G.H. Golub, A preconditioner for generalized saddle point problems, SIAM

J. Matrix Anal. Appl., 26 (2004) 20-41.

[14] M. Benzi, G.H. Golub, J. Liesen, Numerical solution of saddle point problems, Acta

Numer. 14 (2005) 1-137.

[15] M. Benzi, M.J. Gander and G.H. Golub, Optimization of the Hermitian and skew-

Hermitian splitting iteration for saddle-point problems, BIT, 43 (2003) 881-900.

[16] M. Benzi, X.-P. Guo, A dimensional split preconditioner for Stokes and linearized

Navier-Stokes equations, Appl. Numer. Math. 61 (2011) 6676.

14

[17] M. Benzi, M.K. Ng, Q. Niu, Z. Wang, A relaxed dimensional factorization precondi-
tioner for the incompressible Navier-Stokes equations, J. Comput. Phys. 230 (2011)
6185-6202.

[18] Y. Cao, L.-Q. Yao, M.-Q. Jiang, A modiﬁed dimensional split preconditioner for

generalized saddle point problems, J. Comput. Appl. Math. 250 (2013) 70-82.

[19] Z.-H. Cao, Positive stable block triangular preconditioners for symmetric saddle point

problems, Appl. Numer. Math. 57 (2007) 899-910.

[20] Y. Cao, L.-Q. Yao, M. -Q. Jiang, Q. Niu , A Relaxed HSS Preconditioner for Saddle
Point Problems from Mesh free discretization, J. Comput Math. 31 (2013) 398-421.

[21] H.C. Elman, A. Ramage, D.J. Silvester, IFISS: A Matlab toolbox for modelling

incompressible ﬂow, ACM Trans. Math. Software. 33 (2007) Article 14.

[22] H. C. Elman, D. J. Silvester and A.J. Wathen, Finite Elements and Fast Iterative

Solvers, Oxford University Press, Oxford, 2003.

[23] H.-T. Fan, B. Zheng, X. -Y. Zhu, A relaxed positive semi-deﬁnite and skew-Hermitian
splitting preconditioner for non-Hermitian generalied saddle point problems, Numer.
Algor., DOI 10.1007/s11075-015-0068-5, In press.

[24] M.-Q. Jiang, Y. Cao, L.-Q. Yao, On parametrized block triangular preconditioners
for generalized saddle point problems, Appl. Math. Comput. 216 (2010) 1777-1789.

[25] G.H. Golub, X. Wu, J.Y. Yuan, SOR-like methods for augmented systems, BIT 55

(2001) 71-85.

[26] C. Keller, N.I.M. Gould, A.J. Wathen, Constraint preconditioning for indenite linear

systems, SIAM J. Matrix Anal. Appl. 21 (2000) 1300-1317.

[27] M.F. Murphy, G.H. Golub, A.J. Wathen, A note on preconditioning for indenite

linear systems, SIAM J. Sci. Comput. 21 (2000) 1969-1972.

[28] J.-Y. Pan, M.K. Ng, Z.-Z. Bai, New preconditioners for saddle point problems, Appl.

Math. Comput. 172 (2006) 762-771.

[29] Y. Saad, Iterative Methods for Sparse Linear Systems (2nd edn), SIAM: Philadelphia,

2003.

[30] V. Simoncini, Block triangular preconditioners for symmetric saddle-point problems,

Appl. Numer. Math. 49 (2004) 63-80.

[31] E.D. Sturler, J. Liesen, Block-diagonal and constraint preconditioners for nonsym-

metric indenite linear systems SIAM, J. Sci. Comput. 26 (2005) 1598-619.

[32] X.N. Wu, G.H. Golub, J.A. Cuminato, J.Y. Yuan, Symmetric-triangular decomposi-
tion and its applications Part II: preconditioners for indenite systems, BIT 48 (2008)
139-162.

15

[33] Y.-J. Xie, C.-F. Ma, A modiﬁed positive-deﬁnite and skew-Hermitian splitting pre-
conditioner for generalized saddle point problems from the Navier-Stokes equation,
Numer. Algor., DOI 10.1007/s11075-015-0043-1.

16

