6
1
0
2

 
r
a

 

M
4
1

 
 
]

V
C
.
s
c
[
 
 

1
v
6
8
1
4
0

.

3
0
6
1
:
v
i
X
r
a

Visual Concept Recognition and Localization via

Iterative Introspection

Amir Rosenfeld and Shimon Ullman

{amir.rosenfeld,shimon.ullman}@weizmann.ac.il

Department of Computer Science & Applied Mathematics

Weizmann Institute of Science

Rehovot, Israel

Abstract. Convolutional neural networks have been shown to develop
internal representations, which correspond closely to semantically mean-
ingful objects and parts, although trained solely on class labels. Class
Activation Mapping (CAM) is a recent method that makes it possible to
easily highlight the image regions contributing to a network’s classiﬁca-
tion decision. We build upon these two developments to enable a network
to re-examine informative image regions, which we term introspection.
We propose a weakly-supervised iterative scheme, which shifts its cen-
ter of attention to increasingly discriminative regions as it progresses,
by alternating stages of classiﬁcation and introspection. We evaluate our
method and show its eﬀectiveness over a range of several datasets, obtain-
ing a top-1 accuracy 84.48% CUB-200-2011, which is the highest to-date
without using external data or stronger supervision. On Stanford-40 Ac-
tions, we set a new state-of the art of 87.89%, and on FGVC-Aircraft
and the Stanford Dogs dataset, we show consistent improvements over
baselines, some of which include signiﬁcantly more supervision.

Keywords: Classiﬁcation, Introspection, Attention, Semi-Supervised Learn-
ing

1 Introduction

With the advent of deep convolutional neural networks as the leading method
in computer vision, several attempts have been made to understand their inner
workings. Examples of pioneering work in this direction include [1,2]; providing
glimpses into the representations learned by intermediate levels in the network.
Speciﬁcally, the recent work of Zhou et al. [3] provides an elegant mechanism to
highlight the discriminative image regions that served the CNN for a given task.
This can be seen as a form of introspection, highlighting the source of the net-
work’s conclusions. A useful trait we have observed in experiments is that even
if the ﬁnal classiﬁcation is incorrect, the highlighted image regions are still be
informative with respect to the correct target class. This is probably due to the
similar appearance of confused classes. see Figures 1 and 4 for some examples.
Motivated by this observation, we propose an iterative mechanism of internal

2

Amir Rosenfeld, Shimon Ullman

supervision, termed introspection, which revisits discriminative regions to reﬁne
the classiﬁcation. As the process is repeated, each stage further highlights dis-
criminative sub-regions. Each stage uses its own classiﬁer, as we found this to
be beneﬁcial when compared to using the same classiﬁer for all sub-windows.

(a) brushing teeth (b) looking through a

(c) ﬁshing

telescope

(d) phoning

(e) taking photo

(f) walking a dog

Fig. 1: (Top) Class Activation Maps [3] show the source of a network’s classi-
ﬁcation. The network tends to focus on relevant image regions even if its ﬁnal
prediction is wrong. An SVM trained on features extracted from VGG-GAP [3]
misclassiﬁed all of these images, while highlighting the discriminative regions.
(a,b,c) the predicted classes appears in red. (Bottom, zoomed in version of top)
The proposed method eﬀectively removes many such errors by focusing atten-
tion on the highlighted regions. (d,e,f ) the corrected prediction following the
introspection stage appears in green

We describe strategies for how to leverage the introspection scheme, and

demonstrate how these consistently improve results on several benchmark datasets,
while progressively reﬁning the localization of discriminative regions. As shown,
our method is particularly beneﬁcial for ﬁne-grained tasks such as species [4,5]
or model [6] identiﬁcation and to challenging cases in e.g., action recognition [7],
which requires attention to small and localized details.

In the following we will ﬁrst review some related work. In Section 3 we
describe our method in detail. Section 4 contains experiments and analysis to
evaluate the proposed method, followed by concluding remarks in 5.

Visual Concept Recognition and Localization via Iterative Introspection

3

2 Related Work

Supervised methods consistently outperform unsupervised or semi-supervised
methods, as they allow for the incorporation of prior knowledge into the learn-
ing process. There is a trade-oﬀ between more accurate classiﬁcation results
and structured output on the one and, the cost of labor-intensive manual an-
notations, on the other. Some examples are [8,9], where bounding boxes and
part annotations are given at train time. Aside from the resources required for
large-scale annotations, such methods elude the question of learning from weakly
supervised data (and mostly unsupervised data), as is known to happen in hu-
man infants, who can learn from limited examples [10]. Following are a few lines
of work related to the proposed method.

2.1 Neural Net Visualization and Inversion

Several methods have been proposed to visualize the output of a neural net
or explore its internal activations. Zeiler et al. [1] found patterns that activate
hidden units via deconvolutional neural networks. They also explore the local-
ization ability of a CNN by observing the change in classiﬁcation as diﬀerent
image regions are masked out. [2] Solves an optimization problem, aiming to
generate an image whose features are similar to a target image, regularized by a
natural image prior. Zhou et al. [11] aims to explicitly ﬁnd what image patches
activate hidden network units, ﬁnding that indeed many of them correspond to
semantic concepts and object parts. These visualizations suggest that, despite
training solely with image labels, there is much to exploit within the internal
representations learned by the network and that the emergent representations
can be used for weakly supervised localization and other tasks of ﬁne-grained
nature.

2.2 Semi-Supervised class Localization

Some recent works attempt to obtain object localization through weak labels,
i.e., the net is trained on image-level class labels, but it also learns localization.
[12] Localizes image regions pertaining to the target class by masking out sub-
images and inspecting change in activations of the network. Oquab et al. [13] use
global max-pooling to obtain points on the target objects. Recently, Zhou et al.
[3] used global average pooling (GAP) to generate a Class-Activation Mapping
(CAM), visualizing discriminative image regions and enabling the localization
of detected concepts. Our introspection mechanism utilizes their CAMs to it-
eratively identify discriminative regions and uses them to improve classiﬁcation
without additional supervision.

2.3 Attention Based Mechanisms

Recently, some attention based mechanisms have been proposed, which allow
focusing on relevant image regions, either for the task of better classiﬁcation

4

Amir Rosenfeld, Shimon Ullman

[14] or eﬃcient object localization [15]. Such methods beneﬁt from the recent fu-
sion between the ﬁelds of deep learning and reinforcement learning [16]. Another
method of interest is the spatial-transformer networks in [17]: they designed a
network that learns and applies spatial warping to the feature maps, eﬀectively
aligning inputs, which results in increased robustness to geometric transforma-
tions. This enables ﬁne-grained categorization on the CUB-200-2011 birds [4]
dataset by transforming the image so that only discriminative parts are consid-
ered (bird’s head, body). Additional works appear in [18], who discovers discrim-
inative patches and groups them to generate part detectors, whose detections
are combined with the discovered patches for a ﬁnal classiﬁcation. In [19], the
outputs of two networks are combined via an outer-product, creating a strong
feature representation. [20] discovers and uses parts by using co-segmentation
on ground-truth bounding boxes followed by alignment.

Fig. 2: Overview of proposed method. At each iteration, an image window (top)
is classiﬁed using features from a GAP-network. Top scoring classes are used
to generate Class Activation Maps (bottom), which are then used to select sub-
windows (green rectangles). The process is repeated for a few iterations and
features from all visited image windows are aggregated and used in a ﬁnal clas-
siﬁer. The correct class is “writing on a book”. Attention shifts gradually and
closes in on the discriminative image region, i.e., the boy’s hand holding a pencil

3 Approach

Our approach is composed of alternating between two main steps: classiﬁcation
and introspection. In the classiﬁcation step, we apply a trained network to an

Visual Concept Recognition and Localization via Iterative Introspection

5

image region (possibly the entire image). In the introspection step, we use the
output of a hidden layer in the network, whose values were set during the classi-
ﬁcation step. This highlights image regions which are fed to the next iteration’s
classiﬁcation step. This process is iterated a few times (typically 4, see Section 4,
Fig. 5), and ﬁnally the results of all stages are combined. Training proceeds by
learning a specialized classiﬁer for each iteration, as diﬀerent iterations capture
diﬀerent contexts and levels of detail (but without additional supervision).

Both classiﬁcation/introspection steps utilize the recent Class-Activation Map-

ping method of [3]. We brieﬂy review the CAM method and then describe how we
build upon it. In [3], a deep neural network is modiﬁed so that post-classiﬁcation,
it is possible to visualize the varying contribution of image regions, via a so-called
Class Activation Mapping (CAM). A global average pooling was used as a penul-
timate feature representation. This results in a feature vector which is a spatial
averaging of each of the feature maps of the last convolutional layer. Using the
notation in [3]: let fk(x, y) be the k’th output of the last convolutional layer at
grid location (x, y). The results of the global-average pooling results in a vector
F = (F1, F2, . . . , Fk), deﬁned as:

Fk =

fk(x, y)

(1)

This is followed by a fully connected layer with C outputs (assuming C target

x,y

classes). Hence the score for class c before the soft-max will be:

(cid:88)
(cid:88)
(cid:88)

k

k

Sc =

=

=

wc

kFk

(cid:88)
(cid:88)

wc
k

x,y
wc

fk(x, y)

kfk(x, y)

(cid:88)

(cid:88)

(2)

(3)

(4)

(5)

Now, deﬁne

x,y

k

(cid:88)

k

ωc

kfk(x, y)

Mc(x, y) =

where ωc

k are class-speciﬁc weights. Hence we can express Sc as a summation

of terms over (x, y):

Sc =

Mc(x, y)

(6)

x,y

And the class probability scores are computed via soft-max, e.g, Pc = eSc(cid:80)
t eSt .
Eq. 5 allows us to measure the contribution of each grid cell Mc(x, y) for each
speciﬁc class c. Indeed, [3] has shown this method to highlight informative image

6

Amir Rosenfeld, Shimon Ullman

regions (with respect to the task at hand), while being on par with the classiﬁca-
tion performance obtained by the unmodiﬁed network (GoogLeNet [21] in their
case). See Fig. 1 for some CAMs. Interestingly, we can use the CAM method
to highlight informative image regions for classes other than the correct class,
providing intuition on the features it has learned to recognize. This is discussed
in more detail in Section 4.2 and demonstrated in Fig. 4. We name a network
whose ﬁnal convolutional layer is followed by a GAP layer as a GAP-network,
and the output of the GAP layer as the GAP features. We next describe how
this is used in our proposed method.

3.1 Iterative Classiﬁcation-Introspection

The proposed method alternates classiﬁcation and introspection. Here we provide
the outline of the method, with speciﬁc details such as values of parameters
discussed in Section 4.6.

For a given image I and window w (initially the entire image), a learned
classiﬁer is applied to the GAP features extracted from the window Iw, resulting
in C classiﬁcation scores Sc, c ∈ [1 . . . C] and corresponding CAMs M w
c (x, y).
The introspection phase employs a strategy to select a sub-window for the next
step by applying a beam-search to a set of putative sub-windows. The sequence
of windows visited by the method is a route on an exploration-tree, from the root
to one of the leaves. Each node represents an image window and the root is the
entire image. We next explain how the sub-windows are created, and how the
search is applied.

We order the current classiﬁcation scores Sc by descending order and retain
the top k scoring classes. Let ˆc be one of these classes and M w
ˆc (x, y) the corre-
sponding CAM. We extract a square sub-window w(cid:48) centered on the maximal
ˆc (x, y). Each such w(cid:48) (k in total) is added as a child of the current
value of M w
node, which is represented by w. In this way, each iteration of the method ex-
pands a selected node in the exploration-tree, corresponding to an image window,
until a maximum depth is reached. The tree depth is the number of iterations.
We deﬁne iteration 0 as the iteration acting on the root. The size of a sub-window
w(cid:48) is of a constant fraction of the size of its parent w. We next describe how the
exploration-tree is used for classiﬁcation.

3.2 Feature Aggregation

Let k be the number of windows generated at iteration t > 0. We denote the set
of windows by:

Wt = (wt

i)k

i=1

(7)

And the entire set of windows as:

(8)
w ∈
RK, e.g., K = 1024, the dimension of the GAP features, as well as classiﬁcation

where W0 is the entire image. For each window wt

i we extract features f t

R = (Wt)T

t=0

Visual Concept Recognition and Localization via Iterative Introspection

7

i

∈ RC. The set of windows R for an image I is arranged as nodes in
scores Swt
the exploration-tree. The ﬁnal prediction is a result of aggregating evidence from
selected sub-windows along some path from the root to a tree-leaf. We evaluate
variants of both early fusion (combining features from diﬀerent iterations) or
later fusion (combining predictions from diﬀerent iterations).

3.3 Training

Training proceeds in two main stages. The ﬁrst is to train a sequence of classiﬁers
that will produce an exploration-tree for each training/testing sample. The sec-
ond is training on feature aggregations along diﬀerent routes in the exploration-
trees, to produce a ﬁnal model.

During training, we train a classiﬁer for each iteration (for a predeﬁned num-
ber of iterations, 5 total) of the introspection/classiﬁcation sequence. The au-
tomatic training of multiple classiﬁers at diﬀerent scales contributes directly to
the success of the method, as using the same classiﬁer for all iterations yielded
no improvement over the baseline results (Section 4.1). For the ﬁrst iteration, we
simply train on entire images with the ground-truth class-labels. For each itera-
tion t > 1, we set the training samples to sub-windows of the original images and
the targets to the ground-truth labels. The sub-windows selected for training are
always those corresponding to the strongest local maximum in Mc(x, y), where
Mˆc(x, y) is the CAM corresponding to the highest scoring class. Each classiﬁer
is an SVM trained on the features the output of the GAP layer of the network
(as was done in [3]). We also checked the eﬀect of ﬁne-tuning the network and
using additional features. The Results are discussed in the experiments, Section
4.

Routes on Exploration Trees The image is explored by traversing routes
on a tree of nested sub-windows. The result of training is a set of classiﬁers,
E = (E)i=1...T . We produce an exploration tree by applying at each iteration
j the classiﬁer Ej on the features of the windows produced by the previous it-
eration. The window of iteration 0 is the entire image. A route along the tree
will consist of a sequence of windows w1, w2, ...wT where T is the number of
iterations. We found in experiments that more than 5 iterations (including the
ﬁrst) brings negligible boosts in performance. The image score for a given class
is given by either (1) summing the scores of classiﬁers along a route (late fu-
sion), or (2) learning a classiﬁer for the combined features of all visited windows
along the route (early fusion). Features are combined via averaging rather than
concatenation. This reduces the training time at no signiﬁcant change to the
ﬁnal performance; such an eﬀect has also been noted by [22]. See Fig. 2 for an
overview of the proposed method. Fig. 2 shows some examples of how progres-
sively zooming in on image regions helps correct early classiﬁcation mistakes.

8

Amir Rosenfeld, Shimon Ullman

Fig. 3: Exploration routes on images: Each row shows the original image and
3 iterations of the algorithm, including the resulting Class-Activation Maps [3]
used to guide the next iteration. The selected sub-window is shown with a green
bounding box. Despite being trained and tested without any bounding box an-
notations, the proposed method closes in on the features relevant to the target
class. The ﬁrst 2 predictions (columns 2,3 ) in each row are mistaken and the
last one (rightmost column) is correct

4 Experiments

4.1 Setup:

In all our experiments, we start with a variant of the VGG-16 network [22] which
was ﬁned tuned on ILSVRC by [3]. We chose it over GoogLeNet-GAP as it ob-
tained slightly higher classiﬁcation results on the ILSVRC validation set. In this
network, all layers after conv5-3 have been removed, including the subsequent
pooling layer; hence the spatial-resolution of the resultant feature maps/CAM
is 14 × 14 for an input of size 224 × 224 (leaving the pooling layer would reduce
resolution to be 7 × 7). A convolutional layer of 1024 ﬁlters has been added,
followed by a fully-connected layer to predict classes. This is our basic GAP-
network, called VGG-GAP. Each image window, including the entire original
image, is resized so that its smaller dimension is 224 pixels, resulting in a fea-
ture map 14 × n × 1024, for which we compute the average along the ﬁrst two
dimensions, to get a feature representation. We resize the images using bilinear
interpolation. We train a separate classiﬁer for each iteration of the classiﬁca-
tion/introspection process; treating all visited image windows with the same
classiﬁer yielded a negligible improvement (0.3% in precision) over the baseline.

Visual Concept Recognition and Localization via Iterative Introspection

9

Table 1: Classiﬁcation accuracy of our method vs. a baseline for several datasets.
VGG-GAP* is our improved baseline using the VGG-GAP network [3]. ours-
late-fusion: we aggregate the scores of image windows along the visited path.
ours-early-fusion: aggregate scores of classiﬁers trained on feature combination
of windows along the visited path. ours-ft: same as ours-early fusion but ﬁne-
tuned. +D: concatenated with fc6 features from VGG-16 at each stage. *Stanford
Dogs is a subset of ILSVRC dataset. For this dataset, we compare to work which
also used a network pre-trained on ILSVRC. †means ﬁne tuning the network on
all iterations
Method

Aircraft

Dogs*

Birds

GoogLeNet-GAP[3]
VGG-16-fc6
VGG-GAP*
ours-late-fusion
ours-early-fusion
ours-ft
ours-early-fusion+D
ours-ft+D
Previous Work

40-
Actions
72.03
68.49
75.23
76.88
77.08
80.13
86.8
87.89
72.03[3],81
[23]

-
74.52
81.83
82.63
83.58
82.62
91.38
90.06
79.92[24]

63.00
52.88
65.72
73.52
71.64
78.77
79.22
82.26 / 84.48†
77.9 [18], 81.01
[25], 82 [20],
84.1 [17]

-
55.15
66.1
68.05
68.26
77.92
68.26
80.89
84.1[19]

All classiﬁers are trained with a linear SVM [26] on (cid:96)2 normalized feature vectors.
If the features are a concatenation of two feature vectors, those are (cid:96)2 normalized
before concatenation. Our experiments were carried out using the MatConvNet
framework [27], as well as [28,29]. We evaluated our approach on several datasets,
including Stanford-40 Actions [7], the Caltech-USCD Birds-200-2011 [4] (a.k.a
CUB-200-2011), the Stanford-Dogs dataset, [5] and the FGVC-Aircraft dataset
[6]. See Table 1 for a summary of our results compared to recent work. In the
following, we shall ﬁrst show some analysis on the validity of using the CAMs to
guide the next step. We shall then describe interesting properties of our method,
as well as the eﬀects of diﬀerent parameterizations of the method.

4.2 Correlation of Class and Localization

In this section, we show some examples to verify our observation that CAMs
tend to highlight informative image locations w.r.t to the target class despite
the fact that the image may have been misclassiﬁed at the ﬁrst iteration (i.e.,
before zooming in on sub-windows).

To do so, we have applied to the test-set of the Stanford-40 actions dataset
a classiﬁer learned on the GAP features of VGG-GAP. For each category in
turn, we ranked all images in the test set according to the classiﬁer’s score for
that category. We then picked the top 5 true positive images and top 5 false

10

Amir Rosenfeld, Shimon Ullman

positive images. See Fig. 4 for some representative images. We can see that the
CAMs for a target class tend to be consistent in both positive images and high-
ranking non-class images. Intuitively, this is because the classiﬁer gives more
weight to patterns which are similar in appearance. For the “writing on a book”
category (top-left of Fig. 4 ) we can see how in positive images the books and
especially the hands are highlighted, as they are for non-class images, such as
“reading”, or “cutting vegetables”. For “texting message” (top-right) the hand
region is highlighted in all images, regardless of class. For “shooting an arrow”
class (bottom-right), elongated structures such as ﬁshing rods are highlighted. A
nice confusion appears between an archer’s bow and a violinist’s bow (bottom-
right block, last row, ﬁrst image), which are also referred to by the same word
in some human languages.

To check our claim quantitatively, we computed the extent of two square
sub-windows for each image in the test-set: one using the CAM of the true class
and one using the CAM of the highest scoring non-true class. For each pair we
computed the overlap (intersection over union) score. The mean score all images
was 0.638; This is complementary evidence to [3], who shows that the CAMs
have good localization capabilities for the correct class.

4.3 Early and Late Fusion

In all our experiments, we found that using the features extracted from a window
at some iteration can bring worse results on its own compared to those extracted
from earlier iterations (which include this window). However, the performance
tends to improve as we combine results from several iterations, in a late-fusion
manner. Training on the combined (averaged) features of windows from multiple
iterations further improves the results (early-fusion). Summing the scores of
early-fused features for diﬀerent route lengths further improves accuracy: if Si
is the score of the classiﬁer trained on a route of length i. Then creating a ﬁnal
score from S1 + St + . . . ST tends to improve as T grows, typically stabilizing at
T = 5. See Fig. 5 for an illustration of this eﬀect. Importantly, we tried using the
classiﬁer from the ﬁrst iteration (i.e., trained on entire images) for all iterations.
This performed worse than learning a classiﬁer per-iteration, especially in later
iterations.

4.4 Fine-Grained vs General Categories

The Standford-40 Action dataset [7] is a benchmark dataset made of 9532 im-
ages of 40 diﬀerent action classes, with 4000 images for training and the rest for
testing. It contains a diverse set of action classes including transitive ones with
small objects (smoking, drinking) and large objects (horses), as well as intran-
sitive actions (running, jumping). As a baseline, we used the GAP-network of
[3] as a feature extractor and trained a multi-class SVM [26] using the resulting
features. Using 5 iterations, we obtain a ﬁnal precision of 77.08% and 80.06%
with ﬁne-tuning. It is particularly interesting to examine the classes for which
our method is most beneﬁcial. We have calculated the F-measure for each class

Visual Concept Recognition and Localization via Iterative Introspection

11

(writing on a book)

(taking photos)

(texting message)

(shooting an arrow)

Fig. 4: Top ranking images (both true positives and false positives) for various ac-
tion categories along with Class-Activation Maps. Misclassiﬁed images still carry
information on where additional attention will disambiguate the classiﬁcation.
Each block of images shows, from ﬁrst to fourth row: high-ranking true-positives
and their respective CAMs , high ranking false-positive and their respective
CAMs. The target class appears below each block. We recommend viewing this
ﬁgure online to zoom in on the details

using the classiﬁcation scores from the fourth and ﬁrst iteration and compared
them. Fig. 6 shows this; the largest absolute improvements are on relatively
challenging classes such as texting a message (7.89%), drinking (9.24%), smok-
ing (9.38%), etc. For all of these, the discriminative objects are small objects
and are relatively hard to detect compared to most other classes. In some cases,
performance is harmed by zooming in on too-local parts of an image: for “riding
a bike” (-3.12%), a small part of the bicycle will not allow disambiguating the
image from e.g., “ﬁxing a bike”. Another pair of categories exhibiting similar
behavior is “riding a horse” vs. “feeding a horse”.

12

Amir Rosenfeld, Shimon Ullman

Fig. 5: Eﬀect of learning at diﬀerent iterations: using the same classiﬁer trained
on entire images for all iterations tends to cause overall precision to drop (same-
classiﬁer ). Accumulating the scores of per-iteration learned classiﬁers along the
explored path improves this (late-fusion). Using all features along the observed
exploration path improves classiﬁcation as the path length increases (early-
fusion) and summing all scores along the early-fusion path brings the best
performance (early-fusion-accum). Performance is shown on the Stanford-40 [7]
dataset

Fig. 6: Our approach improves mainly ﬁne-grained tasks and categories where
classiﬁcation depends on small and speciﬁc image windows. The ﬁgure shows
absolute diﬀerence in terms of F-measure over the baseline approach on all cat-
egories of the Stanford-40 Actions [7] dataset

iteration no.11.522.533.54classification accuracy72737475767778Early vs. Late Fusionsame-classifierlate-fusionearly-fusionearly-fusion-accumchange in F measure-4-3-2-1012345678910classriding a bikethrowing frisbyriding a horsefixing a carcleaning the floorlooking through a telescopefixing a bikecutting treesfeeding a horseclimbingpushing a cartfishingrunningapplaudingusing a computerholding an umbrellagardeningreadingrowing a boatwalking the dogwatching TVjumpingplaying guitarlooking through a microscopecookingshooting an arrowpouring liquidwaving handsbrushing teethwashing dishesblowing bubbleswriting on a boardphoningplaying violintaking photoswriting on a bookcutting vegetablestexting messagedrinkingsmokingVisual Concept Recognition and Localization via Iterative Introspection

13

4.5 Top-Down vs. Bottom-Up attention

To further verify that our introspection mechanism highlights regions whose
exploration is worthwhile, we evaluated an alternative to the introspection stage
by using a generic saliency measure [30]. On the Stanford-40 dataset, instead of
using the CAM after the ﬁrst classiﬁcation step, we picked the most salient image
point as the center of the next sub-window. Then we proceeded with training
and testing as usual. This produced a sharp drop in results: on the ﬁrst iteration
performance dropped from 74.47% when using the CAM to 62.31% when using
the saliency map. Corresponding drops in performance were measured in the
late-fusion and early fusion steps, which improve results in the proposed scheme
but made them worse when using saliency as a guide.

Usage of Complementary Feature Representations The network used for
drawing attention to discriminative image regions need not necessarily be the one
used for feature representation. We used the VGG-16 [22] network to extract fc6
features along the GAP features for all considered windows. On the Stanford-40
Actions dataset, when used to classify categories using features extracted from
entire images, these features we slightly weaker than the GAP features (68%
vs 72%). However, training on a concatenated feature representation boosted
results signiﬁcantly, reaching a precision of 85%. We observed a similar eﬀect on
all datasets, showing that the two representations are complementary in nature.
Combined with our iterative method, we were able to achieve 87.89%, compared
to the previous best 81% of [23].

Eﬀect of Aspect-Ratio Distortion Interestingly, our baseline implementa-
tion (using only the VGG-GAP network as a feature extractor for the entire
image) got a precision score of 75.23% compared to 72.03% of [3]. We suspect
that it may be because in their implementation , they modiﬁed the aspect ratio
of the images to be square regardless of the original aspect ratio, whereas we
did not. Doing so indeed got a score more similar to theirs, which is interesting
from a practical viewpoint.

4.6 Various Parameters & Fine-Tuning

Our method includes several parameters, including the number of iterations, the
width of the beam-search used to explore routes of windows on the image and
the ratio between the size of the current window and the next. For the number
iterations, we have consistently observed that performance saturates, and even
deteriorates a bit, around iteration 4. An example of this can be seen in Fig.
5 showing the performance vs iteration number on the Standford-40 dataset. A
similar behavior was observed on all the datasets on which we’ve evaluated the
method. This is probably due to the increasingly small image regions considered
at each iteration. As for the number of windows to consider at each stage, we
tried choosing between 1 and 3 of the windows relating to the highest ranking

14

Amir Rosenfeld, Shimon Ullman

√

classes on a validation set. At best, such strategies performed as well as the
greedy strategy, which chose only the highest scoring window at each iteration.
The size of the sub-window with respect to the current image window was set
2m where m is the geometric mean of the current window’s height and
as
width (in eﬀect, all windows are square, except the entire image). We have
experimented with smaller and larger values on a validation set and found this
parameter to give a good trade-oﬀ between not zooming in too much (risking
“missing” relevant features) and too little (gaining too little information with
respect to the previous iteration).

We have also evaluated our results when ﬁne-tuning the VGG-GAP network
before the ﬁrst iteration. This improves the results for some of the datasets, i.e.,
Stanford-40 [7], CUB-200-2011 [4], but did not improve results signiﬁcantly for
others (Dogs [5], Aircraft [6]).

Finally, we evaluated the eﬀect of ﬁne-tuning the network for all iterations
on the CUB-200-2011. This resulted in a top-1 precision of 84.48%, which to
our knowledge the best result to-date, second only to works which added a
massive amount of external data mined from the web [31] (91.9%) and/or strong
supervision [32] (84.6%).

5 Conclusions

We have presented a method, which by repeatedly examining the source of the
current prediction, decides on informative image regions to consider for further
examination. The method is based on the observation that a trained CNN can
be used to highlight relevant image areas even when its ﬁnal classiﬁcation is
incorrect. This is a result of training on multiple visual categories using a shared
feature representation. We have built upon Class Activation Maps [3] due to
their simplicity and elegance, though other methods for identifying the source
of the classiﬁcation decision (e.g., [1]) could probably be employed as well. The
proposed method integrates multiple features extracted at diﬀerent locations
and scales. It makes consistent improvements on ﬁne-grained classiﬁcation tasks
and on tasks where classiﬁcation depends on ﬁne localized details. It obtains
state-of-the art results on CUB-200-2011 [4], among methods which avoid strong
supervision such as bounding boxes or keypoint annotations. The improvements
are shown despite the method being trained using only class labels, avoiding the
need for supervision in the form of part annotations or even bounding boxes. In
future work, it would be interesting to examine the use of recurrent nets (RNN,
LSTM [33]) to automatically learn sequential processes, which incrementally
improve classiﬁcation results, extending the approach described in the current
work.

Visual Concept Recognition and Localization via Iterative Introspection

15

References

1. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks.

In: Computer vision–ECCV 2014. Springer (2014) 818–833

2. Mahendran, A., Vedaldi, A.: Understanding deep image representations by in-
verting them. In: Computer Vision and Pattern Recognition (CVPR), 2015 IEEE
Conference on, IEEE (2015) 5188–5196

3. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Learning Deep Fea-

tures for Discriminative Localization. arXiv preprint arXiv:1512.04150 (2015)

4. Wah, C., Branson, S., Welinder, P., Perona, P., Belongie, S.: The caltech-ucsd

birds-200-2011 dataset. (2011)

5. Khosla, A., Jayadevaprakash, N., Yao, B., Fei-Fei, L.: Novel Dataset for Fine-
Grained Image Categorization. In: First Workshop on Fine-Grained Visual Catego-
rization, IEEE Conference on Computer Vision and Pattern Recognition, Colorado
Springs, CO (June 2011)

6. Maji, S., Rahtu, E., Kannala, J., Blaschko, M., Vedaldi, A.: Fine-grained visual

classiﬁcation of aircraft. arXiv preprint arXiv:1306.5151 (2013)

7. Yao, B., Jiang, X., Khosla, A., Lin, A.L., Guibas, L., Fei-Fei, L.: Human action
recognition by learning bases of action attributes and parts. In: Computer Vision
(ICCV), 2011 IEEE International Conference on, IEEE (2011) 1331–1338

8. Zhang, N., Donahue, J., Girshick, R., Darrell, T.: Part-based R-CNNs for ﬁne-
In: Computer Vision–ECCV 2014. Springer (2014)

grained category detection.
834–849

9. Zhang, N., Shelhamer, E., Gao, Y., Darrell, T.: Fine-grained pose prediction,

normalization, and recognition. arXiv preprint arXiv:1511.07063 (2015)

10. Lake, B.M., Salakhutdinov, R., Tenenbaum, J.B.: Human-level concept learning

through probabilistic program induction. Science 350(6266) (2015) 1332–1338

11. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Object detectors

emerge in deep scene cnns. arXiv preprint arXiv:1412.6856 (2014)

12. Bergamo, A., Bazzani, L., Anguelov, D., Torresani, L.: Self-taught object localiza-

tion with deep networks. arXiv preprint arXiv:1409.3964 (2014)

13. Oquab, M., Bottou, L., Laptev, I., Sivic, J.: Is object localization for free?-weakly-
In: Proceedings of the

supervised learning with convolutional neural networks.
IEEE Conference on Computer Vision and Pattern Recognition. (2015) 685–694

14. Mnih, V., Heess, N., Graves, A., et al.: Recurrent models of visual attention. In:

Advances in Neural Information Processing Systems. (2014) 2204–2212

15. Caicedo, J.C., Lazebnik, S.: Active object localization with deep reinforcement
learning. In: Proceedings of the IEEE International Conference on Computer Vi-
sion. (2015) 2488–2496

16. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D.,
Riedmiller, M.: Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602 (2013)

17. Jaderberg, M., Simonyan, K., Zisserman, A., et al.: Spatial transformer networks.

In: Advances in Neural Information Processing Systems. (2015) 2008–2016

18. Xiao, T., Xu, Y., Yang, K., Zhang, J., Peng, Y., Zhang, Z.: The application of
two-level attention models in deep convolutional neural network for ﬁne-grained
image classiﬁcation. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition. (2015) 842–850

19. Lin, T.Y., RoyChowdhury, A., Maji, S.: Bilinear CNN models for ﬁne-grained
visual recognition. In: Proceedings of the IEEE International Conference on Com-
puter Vision. (2015) 1449–1457

16

Amir Rosenfeld, Shimon Ullman

20. Krause, J., Jin, H., Yang, J., Fei-Fei, L.: Fine-grained recognition without part
In: Proceedings of the IEEE Conference on Computer Vision and

annotations.
Pattern Recognition. (2015) 5546–5555

21. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition. (2015) 1–9
22. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale

image recognition. arXiv preprint arXiv:1409.1556 (2014)

23. Gao, B.B., Wei, X.S., Wu, J., Lin, W.: Deep spatial pyramid: The devil is once

again in the details. arXiv preprint arXiv:1504.05277 (2015)

24. Zhang, Y., Wei, X.s., Wu, J., Cai, J., Lu, J., Nguyen, V.A., Do, M.N.: Weakly
Supervised Fine-Grained Image Categorization. arXiv preprint arXiv:1504.04943
(2015)

25. Simon, M., Rodner, E.: Neural activation constellations: Unsupervised part model
discovery with convolutional networks. In: Proceedings of the IEEE International
Conference on Computer Vision. (2015) 1143–1151

26. Fan, R.E., Chang, K.W., Hsieh, C.J., Wang, X.R., Lin, C.J.: LIBLINEAR: A
Library for Large Linear Classiﬁcation. Journal of Machine Learning Research 9
(2008)

27. Vedaldi, A., Lenc, K.: MatConvNet: Convolutional neural networks for matlab.
In: Proceedings of the 23rd Annual ACM Conference on Multimedia Conference,
ACM (2015) 689–692

28. Doll´ar, P.: Piotr’s Computer Vision Matlab Toolbox (PMT). http://vision.

ucsd.edu/~pdollar/toolbox/doc/index.html

29. Vedaldi, A., Fulkerson, B.: Vlfeat: an open and portable library of computer
vision algorithms. In Bimbo, A.D., Chang, S.F., Smeulders, A.W.M., eds.: ACM
Multimedia, ACM (2010) 1469–1472

30. Zhu, W., Liang, S., Wei, Y., Sun, J.: Saliency optimization from robust background
detection. In: Proceedings of the IEEE conference on computer vision and pattern
recognition. (2014) 2814–2821

31. Krause, J., Sapp, B., Howard, A., Zhou, H., Toshev, A., Duerig, T., Philbin, J.,
Fei-Fei, L.: The Unreasonable Eﬀectiveness of Noisy Data for Fine-Grained Recog-
nition. arXiv preprint arXiv:1511.06789 (2015)

32. Xu, Z., Huang, S., Zhang, Y., Tao, D.: Augmenting strong supervision using web
In: Proceedings of the IEEE International

data for ﬁne-grained categorization.
Conference on Computer Vision. (2015) 2524–2532

33. Hochreiter, S., Schmidhuber, J.: Long Short Term Memory. Neural Computation

9 (1997) 1735–1780

