6
1
0
2

 
r
a

M
4

 

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
1
3
6
1
0

.

3
0
6
1
:
v
i
X
r
a

CLASSIFICATION AND REGRESSION TREE METHODS

FOR INCOMPLETE DATA FROM SAMPLE SURVEYS

By Wei-Yin Loh†,∗ ,

John Eltinge‡ ,
MoonJung Cho‡

and

Yuanzhi Li†

University of Wisconsin-Madison† and Bureau of Labor Statistics‡

Analysis of sample survey data often requires adjustments to ac-
count for missing data in the outcome variables of principal inter-
est. Standard adjustment methods based on item imputation or on
propensity weighting factors rely heavily on the availability of auxil-
iary variables for both responding and non-responding units. Appli-
cation of these adjustment methods can be especially challenging in
cases for which the auxiliary variables are numerous and are them-
selves subject to substantial incomplete-data problems. This paper
shows how classiﬁcation and regression trees and forests can over-
come some of the computational diﬃculties. An in-depth simulation
study based on incomplete-data patterns encountered in the U.S.
Consumer Expenditure Survey is used to compare the methods with
two standard methods for estimating a population mean in terms of
bias, mean squared error, computational speed and number of vari-
ables that can be analyzed.

1. Introduction. We consider estimation of a population mean µ of
a variable Y in simple random sampling without replacement from a ﬁnite
population when the sample S is incompletely observed. We ﬁrst review
several existing solutions and then propose some new solutions based on
classiﬁcation and regression trees and forests. We aim to show, by means
of a realistic simulation study, that certain tree methods are as good as
or better than two standard methods in four important respects: (i) bias,
(ii) mean squared error, (iii) computational speed, and (iv) applicability to
large numbers of prospective predictor variables. Within the large and com-

∗Supported in part by NSF grant DMS-1305725 and an ASA/NSF/BLS research fel-

lowship.

MSC 2010 subject classiﬁcations: Primary 62D05, 62G08; secondary 62H30
Keywords and phrases: AMELIA, GUIDE (generalized unbiased interaction detection
and estimation), imputation, incomplete predictor variable, item nonresponse, MICE (mul-
tiple imputation by chained equations), predicted-mean model, response propensity, U.S.
Consumer Expenditure Survey

1

2

W.-Y. LOH ET AL.

plex space of incomplete-data methods for surveys, one does not expect any
given computational procedure to dominate all other procedures uniformly
with respect to each of criteria (i)–(iv). Instead, performance will depend on
several factors, including (a) the number of prospective predictor variables,
and the degree of association among those variables; (b) incomplete-data
patterns encountered among the predictor variables; and (c) the extent to
which the observed data and missing-data patterns are consistent with model
conditions that are used implicitly or explicitly by a given computational
procedure.

Let n denote the number of subjects in S and let S1 ⊂ S be the sub-
set containing the non-missing Y values. Let yi denote the value of Y for
subject i in S1. If the probability πi that Y is non-missing for subject i is
i yi is an unbiased estimate of µ (throughout this
paper, expectations are evaluated with respect to both the sample design
and the nonresponse mechanism). Let ˆπi be an estimate of πi if the latter is
unknown. The inverse probability weighted (IPW) estimate of µ is (see, e.g.,
Little, 1986; Seaman and White, 2013)

known, then n−1Pi∈S1 π−1

(1.1)

(cid:16) Xi∈S1

ˆπ−1

i (cid:17)−1

Xi∈S1

ˆπ−1
i yi.

The bias of the IPW estimate depends on accurate speciﬁcation and esti-
mation of a model for πi. In applications where covariates are completely ob-
served, logistic regression is typically used. The sample mean, ¯y = n−1
where n1 is the number of observations in S1, is an IPW estimate with
ˆπi = n1/n for i ∈ S1.

1 Pi∈S1 yi,

An alternative approach is imputation of the missing values. Let S2 =
S − S1 be the subset of the sample with missing Y . Let ˆyj denote the
imputed value of Y for subject j in S2. Then µ is estimated by the mean of
the completed sample

(1.2)

n−1(cid:16) Xi∈S1

yi + Xj∈S2

ˆyj(cid:17).

The mean ¯y of the non-missing Y values is a special case of mean imputation
with ˆyj = ¯y in (1.2). If predictor (X) variables are also observed, the ˆyj
may be obtained by regression imputation (Buck, 1960), where a regression
model of Y on X is ﬁtted to the observations in S1 and the ˆyj are predicted
from the X values in S2. If the X variables have missing values as well, the
complete-case method ﬁts the regression model to the subset of values with
complete observations in the X and Y variables.

TREES FOR INCOMPLETE DATA

3

Hot deck (Little and Rubin, 2002) methods impute missing values by ran-
dom sampling of non-missing values within ‘adjustment cells’, which are
prespeciﬁed partitions of the data. One way to construct the cells is to split
the range of each X variable into a small number of sets and use Cartesian
products of the sets to deﬁne the cells. It can produce, however, cells with
few or no observations. For example, in one analysis of the Current Pop-
ulation Survey, the U.S. Census Bureau had 11,232 cells constructed from
seven X variables (Bollinger and Hirsch, 2006). Alternatively, the cells can
be deﬁned by partitioning the sample according to the estimated probability
that Y is missing, where these probabilities are estimated through the use
of logistic regression (see, e.g., Little, 1986). A diﬃculty is the choice of X
variables for logistic regression if there are many X or they have missing
values too.

Another method is maximum likelihood, which draws random observa-
tions from a parametric model ﬁtted to the (X, Y ) observations. Assum-
ing that (i) the parametric model is correct, (ii) the X variables are com-
pletely observed, and (iii) the Y values are missing at random (MAR),
that is, the probability that a value is missing does not depend on the
value itself, conditional on the non-missing values of the X variables, Rubin
(1987) showed that inferences from multiply imputed data are statistically
valid for large samples. If there are missing X values, the EM algorithm
(Dempster, Laird and Rubin, 1977) is often used to estimate the parame-
ters in the model. In that case, there is no guarantee that the results are
statistically valid. AMELIA (Honaker, King and Blackwell, 2011) uses mul-
tivariate normal likelihoods. Each categorical variable is ﬁrst converted to a
dummy 0-1 vector and then a multivariate normal model is ﬁtted to all the
variables. As a result, the minimum sample size for AMELIA depends on
the number of ordinal variables and the number of dummy variables.

Yet another method is sequential regression (see, e.g., Raghunathan, 2016).
It is a regression switching technique in which the missing values in the X
and Y variables are initially imputed by their means, medians, or modes.
Then each variable is regressed in turn on the other variables and miss-
ing values are updated with the predicted values. The procedure is con-
tinued for several cycles to reduce the eﬀects of the initial imputed values.
MICE (van Buuren and Groothuis-Oudshoorn, 2011), which stands for mul-
tiple imputation by chained equations, is one implementation. It uses linear
regression for imputation of ordinal variables and polytomous logistic regres-
sion for categorical variables. Theoretical arguments for the eﬀectiveness of
sequential regression have been given, but they are based on the restrictive
assumption of a correct linear regression model relationship between the

4

W.-Y. LOH ET AL.

variable being imputed and the covariates (White and Carlin, 2010).

In practice, MICE can experience computational problems when there
are many predictor variables with missing values. For example, linear re-
gression can fail if there is multicollinearity and logistic regression fails
if there is quasi-complete separation in the data (Albert and Anderson,
1984; Hosmer, Lemeshow and Sturdivant, 2013). To overcome these prob-
lems, Burgette and Reiter (2010) proposed replacing linear regression and
logistic regression with CART (Breiman et al., 1984) classiﬁcation and re-
gression trees from the R tree package. Calling the algorithm CART-MICE,
they compared it to MICE in a simulation experiment by generating obser-
vations from a quadratic regression model with 10 correlated and normally
distributed X variables, with 2 of the latter completely observed. Missing
values in Y and the other 8 X variables were simulated using an MAR mech-
anism that depended only on the 2 completely observed X variables. Their
results showed that the mean squared error and bias of the estimated re-
gression coeﬃcients from CART-MICE were better than those from MICE.
Rubin (1987) proposed imputing the missing values multiple times to ob-
tain variance estimates. Although there are many simulation studies on mul-
tiple imputation (MI), almost all used normally distributed data and miss-
ingness mechanisms deﬁned by linear logistic regression models (see, e.g.,
Allison, 2000; Schafer and Graham, 2002; Carpenter, Kenward and White,
2007; Burgette and Reiter, 2010; White and Carlin, 2010). Little is known
about the performance of the methods in real-world settings where variables
are not normally distributed (e.g., categorical variables) and probabilities of
missingness are not determined by linear logistic regression.

To our knowledge, there are only three simulation studies that involve real
data. Yu, Burton and Rivero-Arias (2007) used as their simulation popula-
tion a data set with completely observed values on eight X and ﬁve Y
variables from 1060 participants in a clinical trial. Simulated samples of size
500 were drawn from the population by bootstrap sampling (i.e., sampling
with replacement) and missing values in the dependent variables were arti-
ﬁcially generated under a MAR assumption. There were no missing values
in the X variables. They compared several MI implementations in standard
statistical packages and found that the relative performance of the methods
depended on the skewness and proportion of zeros of the Y distribution;
performance was similar across methods when Y deviated only slightly from
normal.

Ambler, Omar and Royston (2007) used data from 20,378 patients in a
clinical database. The Y variable was binary and there were 16 X variables
with varying amounts of missing values. Missing values in the database were

TREES FOR INCOMPLETE DATA

5

imputed by MICE to produce a population without missing values. Then
samples were drawn without replacement from the population. Missing Y
values in the samples were generated randomly according to a logistic re-
gression model ﬁtted to the population. Finally missing patterns in X were
randomly generated with probabilities proportional to the frequencies of the
top 16 missing patterns in the original data. The authors found that MICE
was one of the best performers.

Andridge and Little (2010) took data from a health and nutrition survey
of 16,739 respondents with completely observed values on a Y variable and
seven X variables as their simulation population. Simple random samples of
size 500 were drawn from the population. A logistic regression model with
three X variables was used to simulate missing values in the Y variable.
They compared several variations of hot deck imputation with MICE.

These three studies are limited by having fewer than 20 X variables and,
except for Ambler, Omar and Royston (2007), having no missing X values.
In this article, we use a data set from the Consumer Expenditure Survey with
hundreds of variables and substantial amounts of missing values. We study
several new methods based on the CART and GUIDE (Loh, 2002, 2009;
Loh and Zheng, 2013) classiﬁcation and regression algorithms and compare
them with AMELIA and MICE in terms of bias, mean squared error and
computational speed via simulation. The X values are intentionally designed
to be naturally missing; i.e., they are not artiﬁcially made missing as in
previous studies and hence are not necessarily MAR. Only the Y variable is
MAR. No previous simulation study comes close in terms of the number of X
variables and the real-world authenticity of their missingness mechanisms.

2. Consumer expenditure data. The Consumer Expenditure (CE)
Quarterly Interview Survey is a longitudinal survey sponsored by the Bu-
reau of Labor Statistics. It collects information on consumers’ expenditures
and incomes as well as characteristics of the consumers. We use a subset
of public-use microdata of the 2013 CE Survey. Answers from 25,822 con-
sumer units (CUs) were obtained on more than 600 questions. For general
background and further details on the survey, see Bureau of Labor Statistics
(2016, chap. 6).

The data contain ﬂag variables that explain whether the values of their
associated variables are observed, missing, top-coded, etc. Variables with
underscores at the end of their names are typically ﬂag variables. For exam-
ple, INTRDVX is the ﬂag variable associated with INTDRVX, the amount
received by a CU in interest or dividends during the past 12 months. Table 1
gives the possible values taken by the ﬂag variables. In this article, we use IN-

6

W.-Y. LOH ET AL.

Table 1

Codes and deﬁnitions of missing value ﬂag variables

A valid nonresponse: a response is not anticipated
C “don’t know”, refusal or other type of nonresponse
D valid data value
T topcoding applied to value

TRDVX as the Y variable and omit CUs with INTRDVX = A and T. This
leaves a sample size of 4609 of which 1771 CUs have INTRDVX = C, i.e., IN-
TRDVX is missing. We ignore the sampling weight here because the R soft-
ware used in our simulations for AMELIA (Honaker, King and Blackwell,
2011) and MICE (van Buuren and Groothuis-Oudshoorn, 2011) do not al-
low sampling weights. Besides, the weights do not vary greatly in the CE
data; for example, their coeﬃcient of variation is 0.375.

There are 630 X variables that can be used to predict Y . Tables 4–7 give
deﬁnitions of those used in the analyses below; see Bureau of Labor Statistics
(2013) for deﬁnitions of the other variables. Almost 20% (124) of the X vari-
ables have missing values. Table 8 lists their names and numbers of missing
values; 67 variables have more than 95% of their values missing (2 are miss-
ing all values).

Note especially that these predictor variables include both variables de-
ﬁned at the Consumer Unit level (e.g., housing tenure) and variables deﬁned
for geographical areas. The latter variable type includes State, Region and
PSU. The State identiﬁer is subject to conﬁdentiality restrictions in some
cases. PSUs are small clusters of counties. Only “A” size PSUs are identi-
ﬁed and other PSU labels are coded as missing in the CE public dataset due
to conﬁdentiality concerns. In 2013, each “A” size PSU is a cluster with a
population of over 2.7 million people, and is self-representing under the CE
design. Consequently, this paper treats “PSU” membership as a ﬁxed eﬀect,
instead of random eﬀect, for missing value imputation.

3. Methods. We study the following ten methods on their ability to

estimate the population mean µ of INTRDVX accurately and quickly.

AME. This is AMELIA (Honaker, King and Blackwell, 2011) with default
parameters except that the empirical prior level is set at 5. According
to the manual, the prior shrinks the covariances of the data but keeps
the means and variances the same. It helps when there are many miss-
ing values, small sample sizes, large correlations among the variables,
or categorical variables with many levels.

MICE. This is the R software (van Buuren and Groothuis-Oudshoorn, 2011)

TREES FOR INCOMPLETE DATA

7

with default options, including ﬁve multiple imputations. Problems
due to multi-collinearity in linear regression and quasi-complete sep-
aration in logistic regression severely limits the number of variables
it can employ. With the help of previous analyses with similar types
of data, we identiﬁed 19 variables that do not cause the software to
fail. Their names and numbers of missing values are given in Table 4.
None are income variables. Only 5 of the 19 have missing values (and
only 1 with a signiﬁcant amount). This set of 19 is quite easy for all
imputation methods.

SIM. This is the simple method that estimates µ with the mean ¯y of the

non-missing Y values, ignoring the values of the X variables.

GCT. This uses a GUIDE classiﬁcation tree (Loh, 2009) to form adjust-
ment cells for conditional mean imputation. Figure 1 shows the GUIDE
tree for predicting INTRDVX = C or D. The ﬁrst split of the tree
sends 368 CUs with ERANKH either missing or ≤ 0.006 to the left
node where 304 (83%) of the CUs are missing INTRDVX (i.e., IN-
TRDVX = C). The mean INTRDVX ($952) among the 64 CUs in
the node with non-missing INTRDVX is used to impute the value of
INTRDVX for the other 304 CUs in the node. Repeating this at all
the terminal nodes and substituting the imputed values in (1.2) gives
the estimate of µ. The same answer can be obtained with the IPW
method too. Let t denote a terminal node of the tree and p(t) the
proportion of non-missing Y in t. Then setting ˆπ = p(t) for all CUs in
each t in (1.1) gives the same estimate.

RCT. This is the RPART version of GCT where instead of GUIDE, RPART
is used to construct the classiﬁcation tree shown in Figure 2. GUIDE
diﬀers from CART and RPART in several major respects, one being se-
lection bias. Because CART employs greedy search to split each node,
it is biased toward selecting variables that permit more splits of the
data (Loh and Shih, 1997). Examples are ordinal variables with many
distinct values and categorical variables with many levels. GUIDE does
not have the bias. There is a hint of the bias if we compare the GCT
and RCT trees. RCT splits on four categorical variables: STATE twice
and PSU, FAM TYPE and BUILDING once each. GCT splits once
on STATE and PSU and on no other categorical variables. Because
STATE, PSU, FAM TYPE and BUILDING have 39, 21, 9, and 10 lev-
els, respectively, they allow approximately 238, 220, 28, and 29 splits of
the data. The codes for PSU, FAM TYPE and BUILDING are given
in the Appendix. Another diﬀerence between GUIDE and CART is
their treatment of missing values. GUIDE sends all missing values on

8

W.-Y. LOH ET AL.

ERANKH
≤∗ 0.006

0.83
$952

368

STATE
in S1

LIQUIDX_
= C

RETS_RVX = C

0.70
$2465

PSU
in S2

0.93
$6080

FEDR_NDX
= C

162

0.64
$3444

277

73

RETS_RVX
= D or T

ROOMSQ
≤7

0.21
$1903

2037

0.76
$4365

88

51

0.36
$3052

AGE2
≤∗60

404

FEDR_NDX
= C

0.66
$1210

0.41
$1448

0.66
$3499

161

0.32
$1556

77

911

Fig 1. GUIDE classiﬁcation tree for predicting missingness in INTRDVX (INTRDVX =
C) from 4609 observations and 630 X variables, with minimum node size 50. At each split,
an observation goes to the left branch if and only if the condition is satisﬁed. The symbol
‘≤∗’ stands for ‘≤ or missing’. Set S1 = {CO, DE, FL, HI, IL, LA, MA, MO, NJ, NY,
OH, PA, SC, TN, WA, WI}; set S2 = {1102, 1110, 1423}. PSU codes are given in Table 9.
Numbers beside each terminal node are the proportion missing INTRDVX (top) and the
mean INTRDVX of the non-missing values; the sample size is beneath the node. Yellow
and green nodes have proportions of INTRDVX missing greater and less, respectively, than
0.50.

TREES FOR INCOMPLETE DATA

9

INC__ANK = D

RETS_RVB = A

STATE
in S1

0.83
$952

367

0.92
$3873

SLRF_NDX
6= D

140

SMSASTAT = 2

0.22
$1981

0.62
$1972

0.17
$1412

PSU
in S2

1814

97

271

FAM_TYPE
in S3

STATE
in S4

0.29
$1790

INC_HRS1
≥∗51

FEDR_NDX
6= D

0.69
$3963

450

0.20
$1755

60

FEDR_NDX
= C

BUILDING
6= 2,5

0.40
$3359

0.64
$2725

0.35
$1931

212

239

646

61

198

0.83
$70

54

0.69
$1963

Fig 2. RPART classiﬁcation tree for predicting missingness in INTRDVX (INTRDVX
= C) from 4609 observations and 630 X variables, with minimum node size 50. Set S1 =
{AL, AK, AZ, CA, CT, DC, GA, ID, IN, KS, KY, ME, MD, MN, MO, NE, NV, NH,
OR, TX, UT, VA, WV}; S2 = {1103, 1111, 1207, 1208, 1210, 1320}; S3 = {2, 7, 8, 9};
S4 = {AZ, DE, LA, NY, PA, SC, TN}. Codes for PSU, FAM TYPE and BUILDING
are given in Tables 9, 10 and 11. Numbers beside each terminal node are the proportion
missing INTRDVX (top) and the mean INTRDVX of the non-missing values; the sample
size is beneath the node. Yellow and green nodes have proportions of INTRDVX missing
greater and less, respectively, than 0.50.

10

W.-Y. LOH ET AL.

a split variable to one node or the other while CART and RPART use
surrogate splits. See Loh (2009) for more information.

GCF. This is an alternative IPW method to GCT where, instead of a single
tree, a GUIDE classiﬁcation forest (Loh, 2014) is used to estimate πi.
GUIDE forest is an ensemble of 500 unpruned classiﬁcation trees, each
constructed from a bootstrap sample of the data to predict whether
INTRDVX is missing. The Random forest (Breiman, 2001) R imple-
mentation (Liaw and Wiener, 2002) cannot be used directly here be-
cause: (1) it requires missing X values to be imputed beforehand and
(2) it does not allow categorical variables with more than 32 levels.

GRT. This is a conditional mean imputation method that uses a GUIDE
piecewise-constant regression tree (Loh, 2002) to impute missing IN-
TRDVX values. Unlike GCT and GCF, it uses only the subset of 2838
CUs with non-missing INTRDVX. The tree is shown in Figure 3. It
splits ﬁrst on AGE REF = missing or ≤ 57. About half of the 2838
CUs satisfy this condition; their mean ($907) is used to impute the
missing INTRDVX values in the node. Repeating this procedure at
each terminal node yields a completed set of INTRDVX values for
application in (1.2).

RRT. This is the RPART version of GRT where RPART is used instead
of GUIDE. The tree, shown in Figure 4, has the same top split as the
GRT tree. It subsequently splits on STATE three times, likely because
STATE permits more splits.

GRF. This is an alternative to GRT. It uses a GUIDE regression forest
(Loh, 2012, 2014) of 500 unpruned regression trees to carry out con-
ditional mean imputation.

GMICE. CART-MICE cannot be used for the CE data due to limitations
in the CART algorithm and software. The R TREE package (Ripley,
2015) used in CART-MICE cannot handle categorical variables with
more than 32 levels. RPART (Therneau, Atkinson and Ripley, 2015)
does not have this limitation, but both are too computationally ex-
pensive for imputation of categorical variables with many categories.
GMICE is CART-MICE with GUIDE in place of CART, with ten
iterations for each imputation.

To understand the diﬀerences between CART-MICE and GMICE, recall
that at each node, CART searches for the best split on every X variable
and then chooses the split that most reduces node impurity. Suppose Y is
categorical with more than two categories and X is also categorical with
p categories. CART has to search through (2p−1 − 1) splits of the form
“X ∈ A” to ﬁnd the subset A that yields the best split on X. Because

TREES FOR INCOMPLETE DATA

11

AGE_REF
≤∗57

ETOTA
≤∗24030

$907

1433

STOCKX
≤∗27500

FFTAXOWE
≤8104

BATHRMQ
≤2

$8000

$8863

$4154

54

64

162

PSU
in S1

VEHQ ≤∗2

RENTEQVX
≤∗794

$714

111

$6495

$2262

96

75

STATE
in S2

$749

252

REGION
= West

$6694

86

200

$1458

305

$3237

Fig 3. GUIDE regression tree for predicting INTRDVX from 2838 observations and 630
X variables, with minimum node size 50. At each split, an observation goes to the left
branch if and only if the condition is satisﬁed. The symbol ‘≤∗’ stands for ‘≤ or missing’.
Set S1 = {1102, 1103, 1109, 1313, 1419, 1420, 1429}; S2 = {AL, AZ, CA, CT, FL, GA,
ID, IN, KY, ME, MD, MI, MN, MO, NV, OH, OR, SC, TX, WA, WI}. Codes for PSU
are given in Table 9. Sample sizes and means of INTRDVX are printed below and beside
nodes. Blue and yellow nodes have mean INTRDVX below and above, respectively, the SIM
estimate of $2009.

12

W.-Y. LOH ET AL.

AGE_REF
<58

STATE
not in S1

$907

1433

RENTEQVX
<1796

STATE
not in S2

$9933

80

STATE
not in S3

$1417

TOTXEST
<42

HEALTHPQ
<1586

$8719

85

726

$1073

EARNCOMP
6= 8

$2489

$5849

70

243

85

$2003

$9006

55

61

Fig 4. RPART regression tree for predicting INTRDVX from from 2838 observations and
630 X variables, with minimum node size 50. At each split, an observation goes to the left
branch if and only if the condition is satisﬁed. Set S1 = {CO, ME, NV, SC, WA, WV};
S2 = {FL, GA, MO, NH, OH, OR, VA, WI}; S3 = {AL, CT, ID, IL, KY, MO, NY,
TX}. Codes for EARNCOMP are in Table 12. Sample sizes and means of INTRDVX are
printed below and beside nodes. Blue and yellow nodes have mean INTRDVX below and
above, respectively, the SIM estimate of $2009.

TREES FOR INCOMPLETE DATA

13

STATE has 39 categories in our data, there are 238−1 ≈ 2×1011 splits. Other
variables with large numbers of categories are HHID (household identiﬁer),
PSU, OCCUCOD1, and OCCUCOD2 (spouse occupation), with 46, 21, 15,
and 15 levels, respectively.

GUIDE gets around the computation problem with a two-step approxi-
mate solution. In the ﬁrst step, GUIDE selects an X variable to split the
node by means of chi-squared tests of association with Y . In the second step,
GUIDE ﬁnds a split on the selected X. Therefore GUIDE does not search
for the best split on Xi for every Xi. There are two important beneﬁts to
this approach. One is the obvious signiﬁcant reduction in computation time.
The other, which is more subtle, is the ﬁrst step eliminates the selection bias
inherent in the CART approach (Loh and Shih, 1997; Kim and Loh, 2001).
Suppose in the second step that the selected X variable has p categories
{c1, c2, . . . , cp} and Y has q categories {j1, j2, . . . , jq}. GUIDE uses the fol-
lowing sequential procedure that reduces the search space if p is large and
q > 2.

1. If q = 2, let c′

1, c′
#(Y = j1, X = c′
and Ai = {c′
the (p − 1) splits {X ∈ Ai} (Breiman et al., 1984, p. 101).

p be the ordered categories of X such that
2) ≤ . . . ≤ #(Y = j1, X = c′
p)
i}, i = 1, 2, . . . , p. Search for the best split among

2, . . . , c′
1) ≤ #(Y = j1, X = c′

1, c′

2, . . . , c′

2. Otherwise, if p ≤ 11, search through all (2p−1 − 1) ≤ 1023 splits on X.
3. Otherwise, if 2 < q ≤ 11 and p > 20, deﬁne a new categorical variable
X ′ by merging the categories of X into q categories. Speciﬁcally, for
k = 1, 2, . . . , p, let j′
k be the predicted value of Y that minimizes
the node impurity among the observations with X = ck in the node.
kI(X = ck) and split the node on X ′, which has

Deﬁne X ′ = Pk j′

2q−1 − 1 ≤ 1023 splits.

4. If none of the above conditions holds, use linear discriminant analy-
sis on Y to transform X into an ordered variable X ′′. Deﬁne indica-

tor variables Dk = I(X = ck), k = 1, 2, . . . , p, and X ′′ = Pk akDk,

where (a1, a2, . . . , ap) is the normalized vector corresponding to the
largest discriminant coordinate (equivalent to maximizing the ANOVA
F-statistic of Y on X ′′). Find the split on X ′′ that most reduces node
impurity. Because X ′′ takes at most p ordered values, the search is
on only (p − 1) splits. Further, because X ′′ induces an order on the
unordered X values, a split of the form X ′′ ≤ b can be expressed in the
form X ∈ A. This technique is borrowed from Loh and Vanichsetakul
(1988).

14

W.-Y. LOH ET AL.

Table 2

Estimates of mean INTRDVX (in dollars) and computation times for 3 nested sets of

predictor variables. Estimated mean for SIM is $2009. Columns are ordered by

computation time. Results for AMELIA and MICE are based on 5 multiple imputations.

For 630 variables, AMELIA did not produce any imputations after 33 days.

RRT RCT GRT GCT GCF GRF GMICE AME MICE

Mean
Time

Mean
Time

Mean
Time

2020
0.2s

2027
0.5s

2013
5s

2009
0.1s

2069
2s

2059
20s

2087
3s

1941
6s

2058
118s

19 predictor variables

2151
3s

2002
9s

2109
207s

52 predictor variables

2112
8s

2091
307s
630 predictor variables
2038
29m

2100
134s

1897
18s

1886
259s

2089
54s

2132
429s

2122
147s

2009
12h

2032
27h

?
33d+

2201
443s

Fail
Fail

Fail
Fail

Another important diﬀerence between GUIDE and CART is how they
deal with missing values. If X has missing values, GUIDE creates a “missing”
level to use in the chi-squared tests for variable selection as well as for split
set selection. As a result, all observations are used. CART uses surrogates
splits, which has been shown to induce a selection bias on variables that
have more missing values (Kim and Loh, 2001). Besides, there is empirical
evidence that the GUIDE approach to missing values yields higher average
classiﬁcation accuracy than the RPART implementation of CART (Loh,
2009).

To accommodate MICE which works only for 19 variables in Table 4,
we applied the methods to three nested sets of X variables. The smallest
is this set of 19. It is far from an ideal test-bed because there are only
5 variables with missing values (and 4 of them have trivially small num-
bers of missing values). The second set consists of 52 X variables, obtained
by combining three groups of variables: the 19 in Table 4 and the top 20
X variables determined by the GUIDE importance ranking method (Loh,
2012; Loh, He and Man, 2015) for predicting INTRDVX and INTRDVX,
respectively; see Tables 5 and 6. The third set is the full set of 630 variables.
SIM estimates the mean INTRDVX as $2009 for all three sets because it
does not use X variables. Results for the other methods are shown in Table 2
and graphed in Figure 5. Every method works on the set of 19 variables but
MICE fails for the other two sets. The mean estimates range from a low of
$1886 for GCF to a high of $2201 for MICE. Computation times (measured
on a Linux computer with a 2.4 GHz AMD Opteron 16-core processor and 64
GB memory) vary greatly. MICE is the slowest on the only set of variables
for which it does not fail. It takes 443 sec. for ﬁve imputations—more than

TREES FOR INCOMPLETE DATA

15

#Variables

19

52

630

d
o
h
t
e
M

SIM

RRT

RCT

MICE

GRT

GRF

GMICE

GCT

GCF

AME

1900

2000
Estimated means ($)

2100

2200

Fig 5. Estimated mean INTRDVX of ten methods for CE data using 19, 52 and 630 X
variables

4000 times slower than RCT, which is the fastest at 0.1 sec. AMELIA is the
next slowest at 147 sec. for this set. For the other two larger sets, AMELIA
is the slowest, taking 12 hours for 52 variables and ﬁve imputations. For
630 variables, AMELIA ran for 33 days without producing a single imputed
data set. In the next section, we report on a series of simulation experiments
to compare the bias and accuracy of the estimates.

4. Simulation study.

4.1. Experimental design. Simulation studies whose goal is to evaluate
methods under realistic conditions usually start with a real data set D and
then generate artiﬁcial populations and samples from it in two steps:

Step I. Impute the missing Y values in D and treat the resulting data set

as a ﬁnite population P1 from which the mean µ of Y is computed.

Step II. Generate a population P2 from P1 by making some X and Y

values in P1 missing.

If the aim is for P2 to be as similar to D as possible, the choice of methods
in these two steps is critical. Typically, MICE is used in Step I and a logistic
regression propensity model is ﬁtted to the missing value ﬂag variable in
D to estimate the probability that the variable is missing in Step II. These

16

W.-Y. LOH ET AL.

choices have the three undesirable consequences.

1. Using MICE in Step I limits the number of variables with missing
values to no more than a few, due to problems with multi-collinearity
and quasi-complete separation. This makes it impossible to apply to
data sets such as ours, without preselection of variables. Besides, to
impute a Y variable, MICE necessarily imputes all X variables with
linear and logistic regression models. This forces relationships among
the X variables in P1 that do not exist in D, thus producing simulated
data that look more unlike the real data.

2. In Step II, logistic regression propensity models cannot be constructed
from X variables with missing values. The latter must be imputed
ﬁrst or the propensity models must be built from subsets of vari-
ables or subsets of data. Neither solution is desirable. Imputing the
X variables (e.g., with MICE) distorts the data and building propen-
sity models from subsets of data requires the artiﬁcial assumption
that the X variables are MAR. Some studies solve this problem by
using only completely observed X variables for propensity modeling
(Burgette and Reiter, 2010), but this is artiﬁcial too because the prob-
ability of a missing value in Y often depends on X variables with
missing values. For example, whether or not interest and dividends is
missing depends on the values and missingness of variables such as
salary, income taxes paid, and value of stocks.

3. Standard linear and logistic regression models with prespeciﬁed ﬁxed
sets of predictor variables are inapplicable if the number of variables
exceeds the sample size.

The diﬃculties vanish if GUIDE forests are used instead of MICE and

logistic regression in Steps I and II respectively.

Step I. Fit a GUIDE regression forest to the 2838 CUs in D with non-
missing INTRDVX values and let σ2 be the mean squared residual of
the ﬁtted model. Let ˆyi (i = 1, 2, . . . , 1771) denote the predicted value
of the ith CU in D with missing INTRDVX and ǫi be an independent
random number drawn from a normal distribution with mean 0 and
variance σ2. Impute the missing INTRDVX value with max(ˆyi + ǫi, 0).
Adding ǫi to ˆyi prevents the imputed values from being more smooth
than the non-missing values. It also produces a diﬀerent P1 for each
simulation trial. Truncation at 0 ensures that all INTRDVX values
are nonnegative. Non-missing INTRDVX values in D are carried over
unchanged to P1.

Step II. Fit a GUIDE classiﬁcation forest to all 4609 members of D, us-

TREES FOR INCOMPLETE DATA

17

ing the missing value ﬂag INTRDVX as dependent variable. Use the
predicted values from the model to estimate the probability that IN-
TRDVX is missing for each member of D. Finally, make the value
of INTRDVX (original or imputed) MAR in P2 according to these
probabilities.

A principal advantage of GUIDE forests over other methods, including Ran-
dom forest, is that no imputation is required of missing X values. As a
result, their missing mechanisms in the simulated data are the same as in
the real data, i.e., not MAR. Other advantages are fast computation speed,
unlimited by number of variables and sample size, and absence of the multi-
collinearity and quasi-complete separation diﬃculties that aﬄict linear and
logistic regression.

In our simulation experiments, D is the CE data set of 4609 CUs with 19,
52, or 630 X variables. A GUIDE regression forest is ﬁtted to each choice
of D and a population P1 obtained. Each simulation trial then consists of
generating P2 and drawing a simple random sample without replacement
from it. We use sampling fractions of 5%, 10%, and 25% (corresponding to
sample sizes 230, 461, and 1152). Finally, each method being evaluated is
applied to the sample to impute the missing INTRDVX values (for condi-
tional mean imputation) or estimate πi (for IPW) and an estimate ˆµ of µ
computed.

4.2. Results. Let M denote the number of simulation trials, where M =
500 except for the case with 19 X variables and 5% sampling when M =
1000. For trial m, let µm denote the mean of P1 (which varies with m) and
let its estimate be ˆµm for a given method. The bias and root mean squared
error (RMSE) of the method are estimated by

Bias = M −1Xm
RMSE = (M −1Xm

(ˆµm − µm)

(ˆµm − µm)2)1/2

.

Figure 6 plots the estimated biases with 95% conﬁdence intervals. We ob-
serve that:

1. AMELIA has the smallest bias in the situations where it works, viz.,

where sample size is large relative to number of variables.

2. MICE is the only method having positive bias. In the only situation
where it works (19 X variables), MICE has the largest bias at 5%
and 10% sampling. This is surprising given that there is only one X
variable with a signiﬁcant amount of missing values here.

18

W.-Y. LOH ET AL.

3. GMICE is always better than MICE in terms of bias. The former is

consistently among the top three methods.

4. RRT has the next largest bias after MICE. Its bias can be larger than
that of SIM, which does not use X variables. On the other hand, RCT
has relatively low bias.

5. Comparing RPART versus GUIDE regression trees, RRT has larger
bias than GRT for 19 and 52 variables; the two are about the same
for 630 variables. The reverse is true for classiﬁcation trees, with RCT
doing consistently better than GCT.

6. Among the GUIDE methods, GCF tends to have the smaller bias than

GCT, GRT, and GRF.

The RMSE results shown in Figure 7 reveal another aspect of the meth-

ods:

1. MICE is again worst in two of the three situations where it works (5%

and 10% sampling with 19 variables).

2. AMELIA is not as good in RMSE as it is in bias—its RMSE is largest
for 52 variables and 10% and 25% sampling (it fails due to inadequate
sample size for 5% sampling).

3. RCT and RRT tend to have larger RMSEs.
4. The best method in terms of RMSE is GRF. It is followed by GCF

and GMICE.

Overall, GCF and GRF have the best combination of bias and RMSE, and
GMICE is third. When D has 19 variables, MICE has the largest bias and
RMSE for 5% and 10% sampling, though they are much improved for 25%.
Its failure to work for larger numbers of variables, however, is its biggest
weakness. AMELIA has the lowest bias but the highest RMSE when there
are 52 variables.

Table 3 shows the average computational time to perform one simulation
trial for each method. If D has 19 variables, MICE is the slowest, taking 3
min. per simulation trial for 1152 observations (25% sampling). This is 4.4
times longer than AMELIA, the next slowest. GRF is just behind AMELIA
in third place; GMICE is fourth, being 9.5 times faster than MICE. If D has
52 variables (where MICE is out of contention), the diﬀerence in average
time per simulation trial for sample size 1152 is 2.6 hours for AMELIA and
2.6 min. for GMICE. Finally, if D has 630 variables, MICE and AMELIA are
both out of contention. Then GMICE is slowest at 62 hours per simulation
trial and the corresponding times for GCF and GRF, the next two slowest,
are 33.7 and 32.3 sec., respectively. The fastest methods are RCT and RRT,
which take from fractions of a second to at most 4 sec.

TREES FOR INCOMPLETE DATA

19

19vars, 5% samp.

19vars, 10% samp.

19vars, 25% samp.

MICE
AME
RRT
RCT
GMICE
GRF
GRT
GCF
GCT
SIM

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|
|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

MICE
AME
RRT
RCT
GMICE
GRF
GRT
GCF
GCT
SIM

|

|

|

|

|

|

|

|

|

|

|
|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

MICE
AME
RRT
RCT
GMICE
GRF
GRT
GCF
GCT
SIM

|

|

|

|

|

|

|

|

|

−100

100

300

−100 0

100

−100

0

50

Bias

Bias

Bias

52vars, 5% samp.

52vars, 10% samp.

52vars, 25% samp.

MICE
AME
RRT
RCT
GMICE
GRF
GRT
GCF
GCT
SIM

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

MICE
AME
RRT
RCT
GMICE
GRF
GRT
GCF
GCT
SIM

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

MICE
AME
RRT
RCT
GMICE
GRF
GRT
GCF
GCT
SIM

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

−200

−100

0

−200

−100

0

−200

−100

0

Bias

Bias

Bias

630vars, 5% samp.

630vars, 10% samp.

630vars, 25% samp.

MICE
AME
RRT
RCT
GMICE
GRF
GRT
GCF
GCT
SIM

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

MICE
AME
RRT
RCT
GMICE
GRF
GRT
GCF
GCT
SIM

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

MICE
AME
RRT
RCT
GMICE
GRF
GRT
GCF
GCT
SIM

|

|

|

|

|

|

|

|

|

|

|

|
|

|

|

|

|

|

|

|

|

|

|

|

−150

−50

0

−150

−50

0

−150

−50

0

Bias

Bias

Bias

Fig 6. Bias (with 95% conﬁdence intervals) for populations generated from 19, 52 and 630
X variables. MICE fails for 52 and 630 variables; AMELIA fails for 630 variables and for
52 variables at 5% sampling.

20

W.-Y. LOH ET AL.

19vars, 5% samp.

19vars, 10% samp.

19vars, 25% samp.

|

|

|

MICE
AME
RRT
RCT
GMICE
GRF
GRT
GCF
GCT
SIM

|

|

|

|

|

|

|

|

|

|

|

|
|

|

|

|

|

|
|

|

|

|

|

|

|

|

|

|

|

|

MICE
AME
RRT
RCT
GMICE
GRF
GRT
GCF
GCT
SIM

|

|

|

|

|
|

|

|

|

|

|

|

|

|

|

|

|

|
|

|

|

|

|

|

|

|

|

MICE
AME
RRT
RCT
GMICE
GRF
GRT
GCF
GCT
SIM

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

350

450

550

650

250

350

140 160 180 200

RMSE

RMSE

RMSE

52vars, 5% samp.

52vars, 10% samp.

52vars, 25% samp.

MICE
AME
RRT
RCT
GMICE
GRF
GRT
GCF
GCT
SIM

|

|

|

|

|

|
|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

MICE
AME
RRT
RCT
GMICE
GRF
GRT
GCF
GCT
SIM

|

|

|

|

|

|

|

|

|

|

|

|
|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

MICE
AME
RRT
RCT
GMICE
GRF
GRT
GCF
GCT
SIM

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

350

450

550

250

350

450

150

200

250

300

RMSE

RMSE

RMSE

630vars, 5% samp.

630vars, 10% samp.

630vars, 25% samp.

|

|

|

MICE
AME
RRT
RCT
GMICE
GRF
GRT
GCF
GCT
SIM

|

|

|

|

|
|
|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

MICE
AME
RRT
RCT
GMICE
GRF
GRT
GCF
GCT
SIM

|

|

|

|

|

|

|

|

|

|
|

|

|

|

|

|

|

|

|

|

|

MICE
AME
RRT
RCT
GMICE
GRF
GRT
GCF
GCT
SIM

|
|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

350

450

550

300

350

400

180 200 220 240

RMSE

RMSE

RMSE

Fig 7. RMSE of methods (with 95% conﬁdence intervals) using populations generated from
19, 52 and 630 X variables. MICE fails for 52 and 630 variables; AMELIA fails for 630
variables and for 52 variables at 5% sampling.

TREES FOR INCOMPLETE DATA

21

Average computation times (sec.) per simulation trial

Table 3

Frac

n RCT RRT GCT GCF GRT GRF GMICE AME MICE

230
5%
461
10%
25% 1152

230
5%
10%
461
25% 1152

230
5%
10%
461
25% 1152

0.02
0.05
0.10

0.05
0.11
0.32

0.57
1.27
3.97

0.02
0.02
0.05

0.03
0.05
0.11

0.33
0.56
1.45

19 variables

0.2
0.4
1.9

2.1
6.3
31.4

52 variables

1.0
1.5
4.1

4.2
9.5
37.1

1.6
3.9
19.0

3.3
6.2
22.3

630 variables

33.3
94.2
319.7

5.8
12.0
32.3

27.8
77.7
278.1

0.5
1.1
3.4

3.2
5.0
10.2

6.0
12.6
33.7

3.2
7.3
19.4

31.1
65.6
156.9

2810
6957
22204

7.3
16.3
41.1

FAIL
4272
9323

FAIL
FAIL
FAIL

70
113
181

FAIL
FAIL
FAIL

FAIL
FAIL
FAIL

5. Conclusion. We introduced several techniques to use classiﬁcation
and regression methods for mean estimation of incomplete data. Some em-
ploy regression trees to estimate conditional means in adjustment cells de-
ﬁned by the nodes of the trees. Others employ classiﬁcation trees to estimate
missing propensities for inverse probability weighting. Using a data set from
the U.S. Consumer Expenditure Survey as a test bed, we performed a large
simulation experiment to compare the methods with AMELIA and MICE.
A major goal in the experimental design is the novelty of ensuring that the
predictor variables are naturally missing, i.e., not constrained to be MAR, in
the simulation populations. This is achieved by using GUIDE forests, which
leave the predictor-value missingness patterns intact.

The results demonstrate that classiﬁcation and regression tree methods
have the following desirable properties that make them deserving of serious
consideration for analysis of incomplete data.

1. They are often superior to traditional methods (as represented by
AMELIA and MICE) in terms of bias and mean squared error for mean
estimation. This is due to the nonparametric nature of tree models.
AMELIA and MICE are based on normality and multivariate linear
model assumptions that are seldom satisﬁed in real complex data.

2. Tree-modeling methods can adapt to available sample sizes. Conversely,
parametric methods with a prespeciﬁed set of parameters to be esti-
mated are problematic unless the sample size is substantially larger
than the number of parameters.

3. Tree methods are not hindered or crippled by multi-collinearity or
quasi-complete separation. In fact, collinearity is often used to advan-
tage in tree algorithms; e.g., CART surrogate splits (Breiman et al.,

22

W.-Y. LOH ET AL.

1984) and GUIDE linear splits (Loh, 2009).

4. For cases that involve large numbers of candidate predictors, tree
methods can be orders of magnitude faster compared to traditional
methods, which are impracticable on large data sets. The speed ad-
vantage increases exponentially with number of variables. Therefore it
is invaluable to imputation of multiple data sets, bootstrapping, and
other variance estimation techniques in large surveys.

In addition, note that the nodes in the tree models as displayed in Fig-
ures 1–4 can be interpreted as nonresponse adjustment cells. For example,
Little and Vartivarian (2003) extended the ideas of Little (1986) to outline
two strategies for reducing the number of adjustment cells: (a) choosing
cells that are homogeneous with respect to the probability of response, and
(b) choosing cells that are homogeneous with respect to outcome variable
Y . They noted that weighting based on either of these methods of group-
ing removes non-response bias in estimating population means: The nodes
produced by the classiﬁcation tree method constitute cells that are homo-
geneous with respect to the estimated probability of response as shown in
Figures 1 and 2, while the regression tree method produces cells that are
homogeneous with respect to the predicted outcome variable, INTRDVX,
as shown in Figures 3 and 4

This work here can be extended in several directions. First, it would be
useful to consider design-adjusted versions of the procedures proposed here.
This potentially would include the use of weights in the growth of individual
trees or forests; in estimation of tree parameters; and in the corresponding
estimation of (sub)population means. Of special interest would be evalua-
tion of the extent to which a given procedure may be sensitive to speciﬁed
patterns of heterogeneity in the weights. For example, as noted in Section 3,
all tree and forest algorithms use approximations, and the properties of the
resulting procedures can be sensitive to the extent to which a given dataset
is consistent with the approximations used for that procedure; and some
weight-heterogeneity patterns may exacerbate that sensitivity. In addition,
several analyses here used some predictor variables that are equal to mem-
bership indicators for certain large primary sample units. It would be of
interest to extend previous literature on the use of stratum and PSU labels
in regression to the current case.

Second, development of appropriate variance estimators would provide
important tools for use in pruning of trees, and for inference related to tree-
or forest-based mean estimators. This would require tree- and forest-related
extensions of standard theorems on the properties of replication-based vari-
ance estimators under complex sample designs. Of special importance would

TREES FOR INCOMPLETE DATA

23

be conditioning arguments arising from the fact that the structure of a given
tree is data-driven and not determined a priori.

Third, the current paper focused on estimation of the means of the “in-
terest and dividend” variable, and a related missing-data ﬂag, for the U.S.
Consumer Expenditure Survey. Some users of CE data, however, are inter-
ested in carrying out econometric analyses based on, e.g., regression, related
generalized linear models and more complex hierarchical models. For those
situations, one may need to impute simultaneously a substantial number of
missing income and expenditure variables for a given consumer unit, and
evaluation criteria for the properties of the resulting imputation procedure
may be more complex, as discussed in, e.g., Rubin (1996).

APPENDIX A: DETAILS OF VARIABLES

Table 4 lists the 19 X variables in the simulation experiment where MICE
did not fail. Tables 5 and 6 list the top predictors of INTRDVX and IN-
TRDVX according to GUIDE. Table 7 gives deﬁnitions of ﬁve additional X
variables that appear in the tree diagrams. Table 8 gives the names of the
124 X variables with missing values (the number missing is beside name).
Tables 9–12 give the value codes of the categorical variables appearing in
the trees in Figures 1–4.

ACKNOWLEDGMENTS

The authors thank Steve Henderson and Geoﬀ Paulin for very produc-
tive discussions of the U.S. Consumer Expenditure Survey and to Matthew
Blackwell for help with the AMELIA software. The views expressed in this
paper are those of the authors and do not necessarily reﬂect the policies of
the U.S. Bureau of Labor Statistics.

REFERENCES

Albert, A. and Anderson, J. A. (1984). On the existence of maximum likelihood esti-

mates in logistic regression models. Biometrika 71 1-10.

Allison, P. D. (2000). Multiple imputation for missing data. A cautionary tale. Socio-

logical Methods and Research 28 301-309.

Ambler, G., Omar, R. Z. and Royston, P. (2007). A comparison of imputation tech-
niques for handling missing predictor values in a risk model with a binary outcome.
Statistical Methods in Medical Research 16 277-298.

Andridge, R. R. and Little, R. J. A. (2010). A review of hot deck imputation for

survey non-response. International Statistical Review 78 40–64.

Bollinger, C. R. and Hirsch, B. T. (2006). Match bias from earnings imputation
in the Current Population Survey: The case of imperfect matching. Journal of Labor
Economics 24 483-519.

Breiman, L. (2001). Random Forests. Machine Learning 45 5–32.

24

W.-Y. LOH ET AL.

Nineteen X variables for which MICE did not fail repeatedly; last column gives number

of missing values

Table 4

Rank Name

Deﬁnition
Age of reference person
Number complete bathrooms in unit

1 AGE REF
2 BATHRMQ
3 BEDROOMQ Number of bedrooms in unit
4 BLS URBN
5 BUILDING
6 CUTENURE

7 EARNCOMP Composition of earners (8 unordered values)
8 EDUC REF
9 FAM TYPE

Urban or rural
Which best describes this building?
Housing tenure (owned with or without mortgage, rented, etc.; 6 un-
ordered values

Education of reference person (9 ordered values)
CU type based on relationship of members to reference person; “own”
children include blood-related sons and daughters, step children and
adopted children
Marital status of reference person (5 categories)
Number of earners
Number of owned automobiles
Occupation (18 categories)
Race of reference person (6 categories)
Region of country (NE, MW, S, W)
Number of rooms in unit
Sex of reference person (2 categories)
Are these living quarters presently used as student housing by a college
or university? (yes/no)
Total outlays last and current quarters

10 MARITAL1
11 NO EARNR
12 NUM AUTO
13 OCCUCOD1
14 REF RACE
15 REGION
16 ROOMSQ
SEX REF
17
18
ST HOUS

19 ETOTA

#
0
21
25
0
0
0

0
0
0

0
0
0
1697
0
33
30
0
0

0

TREES FOR INCOMPLETE DATA

25

Top 20 predictors of INTRDVX (ﬂag variable for INTRDVX); last column gives

number of missing values out of 4609 records

Table 5

Rank Name

1 RETS RVX

IRAX

STATE
LIQUIDX
SLRF NDX

2 FEDR NDX
3
4
5
6 PSU
7
8 ERANKH
9
10
11
12 POV PY
13 POV CY
14 RETS RVB
15 RETS VBX

INC ANK
16
17 ERANKH
18 RESPSTAT
19 ROYESTX
20
FEDTAXX

INC RANK
SLOC AXX
FEDRFNDX Amount of federal income tax refund

Deﬁnition
Flag variable for RETSURVX (amount received in retirement, survivor
or disability pensions in past 12 months)
Flag variable for Federal income tax refund
State (39 categorical values)
Flag variable for checking, savings, money market accounts, and CDs
Flag variable for refund state and local income tax refund
Primary sampling unit (21 categorical values)
Flag variable for value of all retirement accounts
Percent expenditure outlay rank
Percent income rank
Flag variable for state and local income taxes paid

Is CU income below previous year’s poverty threshold?
Is CU income below current year’s poverty threshold?
Flag variable for range of retirement, survivor or disability pensions
Flag variable for median value of bracket range for RETSURVB (re-
tirement, survivor, or disability pensions)
Flag variable for percent income rank
Flag variable for percent expenditure outlay rank
Completeness of income response (yes/no)
Flag variable for income from royalty or estates and trusts
Flag variable for Federal income tax paid by all CU members

#
0

0
486
0
0
2579
0
367
367
0
2530
0
0
0
0

0
0
0
0
0

26

W.-Y. LOH ET AL.

Top 20 predictors of INTRDVX; last column gives number of missing values out of 2838

records with non-missing INTRDVX

Table 6

Rank Name

1 AGE REF
2 CUTENURE

Deﬁnition
Age of reference person
Housing tenure (owned with or without mortgage, rented, etc.; 6 un-
ordered values
State (39 unordered values)

STATE

3
4 RENTEQVX Expected monthly rent of home, if rented out
5 AGE2
6

INCNONW1

Age of spouse
Reason reference person did not work in past 12 months (6 unordered
values
Value of all directly-held stocks, bonds, and mutual funds
Employer from which reference person received most earnings (6 un-
ordered values)

7
8

STOCKX
INCOMEY1

Estimated amount contributed to Social Security

STOCKYRX Median value of bracket range for STOCKX
FJSSDEDX

9
10
11 EARNCOMP Composition of earners (8 unordered values)
12
INC HRS1
13 PERSOT64
14 NO EARNR
15
FRRETIRX
16 RENT QVX
17 PROPTXPQ Property taxes paid last quarter
18
19
20

Number hours usually worked per week by reference person
Number of persons over 64 in CU
Number of earners
Social Security and Railroad Retirement income
Flag variable for RENTEQVX

Percent income rank
Flag variable for employer type
Flag variable for reason did not work in past 12 months

INC RANK
INCO EY1
INCN NW1

Five variables appearing in Figures 1–4 but not listed in Tables 4, 5 and 6; none has

missing values

Table 7

Deﬁnition
Does CU reside inside a Metropolitan Statistical Area? (yes/no)
Estimated total taxes paid

Name
SMSASTAT
TOTXEST
FFTAXOWE Weighted estimate for federal tax liabilities at the tax unit level
HEALTHPQ Amount health care last quarter
VEHQ

Number of owned vehicles

#NA
0
0

316
444
1201
1869

2612
969

2630
0
0
969
0
0
0
0
0
64
0
0

TREES FOR INCOMPLETE DATA

27

Variables and their numbers of missing values

Table 8

OTHASTBX 4589
INCWEEK2 1879
INC HRS1 1697
OTHASTX 4564
INC HRS2 2832
OTHFINX 4571
INC RANK 367
OTHLNYRB 4605 ROYESTX 4364
OTHLNYRX 4559
INTRDVX 1771
OTHLOAN 3424
IRAB 4432
OTHLONB 4606
IRABX 4432
OTHLONBX 4606
IRAX 3853
OTHLONX 4555
IRAYRB 4407
OTHLYRBX 4605
IRAYRBX 4407
OTHREGB 4594
IRAYRX 3899
OTHREGBX 4594
LIQDYRBX 4448
OTHREGX 4338
LIQUDYRB 4448
OTHRINCB 4603
LIQUDYRX 3876
OTHRINCX 4483
LIQUIDB 4481
OTHSTYRB 4585
LIQUIDBX 4481
LIQUIDX 3827
OTHSTYRX 4572
LMPSUMBX 4600 OTHSYRBX 4585
LUMPSUMB 4600 OTRINCBX 4609
LUMPSUMX 4378

AGE2 1879
APTMENT 4535
BATHRMQ 21
BEDROOMQ 25
BUILT 585
CNTRALAC 1459
CREDFINX 4282
CREDITB 4584
CREDITBX 4584
CREDITX 4233
CREDTYRX 4248
CREDYRB 4573
CREDYRBX 4573
DEFBENRP 3490
DIRACC 154
EDUCA2 1879
EITC 1032
ERANKH 367
FEDRFNDX 2530
POPSIZE 33
FEDTAXX 3752
PORCH 997
FMLPYYRX 4514 MEALSPAY 9
FS MTHI 4560
MISCTAXX 4520
POV CY 378
MLPAYWKX 4514 POV PY 378
HHID 4531
MLPYQWKS 4508 PSU 2579
HISP2 1879
NETRENTB 4582
HLFBATHQ 23
NETRENTX 4258
HORREF1 4448
HORREF2 4495
NETRNTBX 4582
INCNONW1 2912 OCCUCOD1 1697
INCNONW2 3656 OCCUCOD2 2832
OFSTPARK 1160
INCOMEY1 1697
INCOMEY2 2832
OTHASTB 4589

RACE2 1879
REGION 33
RENTEQVX 660 WHLFYRX 4444
RETSRVBX 4542 WHOLIFB 4571
RETSURVB 4542 WHOLIFBX 4571
RETSURVM 3289 WHOLIFX 4428
RETSURVX 3520 WINDOWAC 3977

ROOMSQ 30
ROYESTB 4570
ROYESTBX 4570

SEX2 1879
SLOCTAXX 3990
SLRFUNDX 3167
STATE 486
STCKYRBX 4531
STDNTYRB 4591
STDNTYRX 4483
STDTYRBX 4591
STOCKB 4550
STOCKBX 4550
STOCKX 4319
STOCKYRB 4531
STOCKYRX 4347
STUDFINX 4511
STUDNTB 4598
STUDNTBX 4598
STUDNTX 4473
SWIMPOOL 4045
WELFAREX 4596
WELFREBX 4609
WHLFYRB 4564
WHLFYRBX 4564

28

W.-Y. LOH ET AL.

PSU codes in the CE data; only “A” size PSUs are identiﬁed, other PSUs are coded as

Table 9

missing

1109 New York, NY
1110 New York, Connecticut suburbs
1111 New Jersey suburbs
1102 Philadelphia Wilmington Atlantic City, PA NJ DE - MD
1103 Boston Brockton Nashua, MA NH ME CT
1207 Chicago Gary Kenosha, IL IN - WI
1208 Detroit Ann Arbor Flint, MI
1210 Cleveland Akron, OH
1211 Minneapolis St. Paul, MN WI
1312 Washington, DC MD VA WV
1313 Baltimore, MD
1316 Dallas Ft. Worth, TX
1318 Houston Galveston Brazoria, TX
1319 Atlanta, GA
1320 Miami Ft. Lauderdale, FL
1419
1420
1422
1423
1424
1429 Phoenix Mesa, AZ

Los Angeles Orange, CA
Los Angeles suburbs, CA
San Francisco Oakland San Jose, CA
Seattle Tacoma Bremerton, WA
San Diego, CA

Codes for variable FAM TYPE. CU type is based on relationship of members to reference

person. “Own” children include blood-related sons and daughters, step children and

adopted children.

Table 10

1 Husband and wife (H/W) only
2 H/W, own children only, oldest child under 6 years old
3 H/W, own children only, oldest child 6 to 17 years old
4 H/W, own children only, oldest child over 17 years old
5 All other H/W CUs
6 One parent, male, own children only, at least one child age under 18 years old
7 One parent, female, own children only, at least one child age under 18 years old
8
9 Other CUs

Single persons

TREES FOR INCOMPLETE DATA

29

Table 11

Codes for variable BUILDING

1

Single family detached (detached structure with only one primary residence; however, the
structure could include a rental unit(s) in the basement, attic, etc.)

2 Row or townhouse inner unit (2, 3 or 4 story structure with 2 walls in common with other
units and a private ground level entrance; it may have a rental unit as part of structure)

3-plex or 4-plex (3 or 4 unit structure with all units occupying the same level or levels)

3 End row or end townhouse (one common wall)
4 Duplex (detached two unit structure with one common wall between the units)
5
6 Garden (a multi-unit structure, usually wider than it is high, having 2, 3, or possibly 4
ﬂoors; characteristically the units not only have common walls but are also stacked on top
of one another)

7 High-rise (a multi-unit structure which has 4 or more ﬂoors)
8 Apartment or ﬂat (a unit not described above; could be located in the basement, attic,

second ﬂoor or over the garage of one

Codes for EARNCOMP (composition of earners)

Table 12

1 Reference person only
2 Reference person and spouse
3 Reference person, spouse and others
4 Reference person and others
5
6
7 Others only
8 No earners

Spouse only
Spouse and others

30

W.-Y. LOH ET AL.

Breiman, L., Friedman, J. H., Olshen, R. A. and Stone, C. J. (1984). Classiﬁcation

and Regression Trees. Wadsworth, Belmont.

Buck, S. F. (1960). A method of estimation of missing values in multivariate data suitable
for use with an electronic computer. Journal of the Royal Statistical Society, Series B
22 302–306.

Bureau of Labor Statistics (2013). Public-use Interview Survey Data Dictionary. U.S.

Department of Labor http://www.bls.gov/cex/2013/csxintvwdata.pdf.

Bureau of Labor Statistics (2016). Handbook of Methods. U.S. Department of Labor

http://www.bls.gov/opub/hom/pdf/homch16.pdf.

Burgette, L. F. and Reiter, J. P. (2010). Multiple imputation for missing data via

sequential regression trees. American Journal of Epidemiology 172 1070-1076.

Carpenter, J. R., Kenward, M. G. and White, I. R. (2007). Sensitivity analysis
after multiple imputation under missing at random: a weighting approach. Statistical
Methods in Medical Research 16 259-275.

Dempster, A. P., Laird, N. M. and Rubin, D. B. (1977). Maximum likelihood from
incomplete data via the EM algorithm (with discussion). Journal of the Royal Statistical
Society, Ser. B 39 1-38.

Honaker, J., King, G. and Blackwell, M. (2011). Amelia II: A Program for Missing

Data. Journal of Statistical Software 45 1-47.

Hosmer, D. W., Lemeshow, S. and Sturdivant, R. X. (2013). Applied Logistic Regres-

sion, Third ed. Wiley.

Kim, H. and Loh, W. Y. (2001). Classiﬁcation trees with unbiased multiway splits. Jour-

nal of the American Statistical Association 96 589–604.

Liaw, A. and Wiener, M. (2002). Classiﬁcation and Regression by randomForest. R

News 2 18-22.

Little, R. J. A. (1986). Survey nonresponse adjustments for estimates of means. Inter-

national Statistical Review 54 139-157.

Little, R. J. A. and Rubin, D. B. (2002). Statistical Analysis With Missing Data, Second

ed. Wiley, New York.

Little, R. J. A. and Vartivarian, S. (2003). On weighting the rates in non-response

weights. Statistics in Medicine 22 1589-1599.

Loh, W. Y. (2002). Regression Trees With Unbiased Variable Selection and Interaction

Detection. Statistica Sinica 12 361–386.

Loh, W. Y. (2009). Improving the precision of classiﬁcation trees. Annals of Applied

Statistics 3 1710–1737.

Loh, W. Y. (2012). Variable selection for classiﬁcation and regression in large p, small n
problems. In Probability Approximations and Beyond (A. Barbour, H. P. Chan and
D. Siegmund, eds.). Lecture Notes in Statistics—Proceedings 205 133–157. Springer,
New York.

Loh, W. Y. (2014). Fifty years of classiﬁcation and regression trees (with discussion).

International Statistical Review 34 329-370.

Loh, W. Y., He, X. and Man, M. (2015). A regression tree approach to identifying

subgroups with diﬀerential treatment eﬀects. Statistics in Medicine 34 1818-1833.

Loh, W. Y. and Shih, Y. S. (1997). Split selection methods for classiﬁcation trees.

Statistica Sinica 7 815–840.

Loh, W. Y. and Vanichsetakul, N. (1988). Tree-structured classiﬁcation via generalized
discriminant analysis (with discussion). Journal of the American Statistical Association
83 715–728.

Loh, W. Y. and Zheng, W. (2013). Regression trees for longitudinal and multiresponse

data. Annals of Applied Statistics 7 495-522.

TREES FOR INCOMPLETE DATA

31

Raghunathan, T. E. (2004). What do we do with missing data? Some options for analysis

of incomplete data. Annual Review of Public Health 25 99-117.

Raghunathan, T. E. (2016). Missing Data Analysis in Practice. CRC Press, Boca Raton,

FL.

Ripley, B. (2015). tree: Classiﬁcation and Regression Trees R package version 1.0-36.
Rubin, D. B. (1987). Multiple Imputation for Nonresponse in Surveys. Wiley.
Rubin, D. B. (1996). Multiple imputation after 18+ years. Journal of the American

Statistical Association 91 473-489.

Schafer, J. L. and Graham, J. W. (2002). Missing data: our view of the state of the

art. Psychological Methods 7 147-177.

Seaman, S. R. and White, I. R. (2013). Review of inverse probability weighting for

dealing with missing data. Statistical Methods in Medical Research 22 278-295.

Therneau, T., Atkinson, B. and Ripley, B. (2015). rpart: Recursive Partitioning and

Regression Trees R package version 4.1-10.

van Buuren, S. and Groothuis-Oudshoorn, K. (2011). mice: Multivariate Imputation

by Chained Equations in R. Journal of Statistical Software 45 1–67.

White, I. R. and Carlin, J. B. (2010). Bias and eﬃciency of multiple imputation com-
pared with complete-case analysis for missing covariate values. Statistics in Medicine
29 2920-2931.

Yu, L. M., Burton, A. and Rivero-Arias, O. (2007). Evaluation of software for multiple
imputation of semi-continuous data. Statistical Methods in Medical Research 16 243-
258.

Department of Statistics
University of Wisconsin
1300 University Avenue
Madison, WI 53706
E-mail: loh@stat.wisc.edu
E-mail: yuanzhi.li@wisc.edu

Office of Survey Methods Research
Bureau of Labor Statistics
2 Massachusetts Avenue, NE
Washington, DC 20212
E-mail: Eltinge.John@bls.gov
E-mail: Cho.Moon@bls.gov

