R´enyi entropy, abundance distribution and the equivalence of ensembles

1 Laboratoire de physique statistique, CNRS, UPMC and ´Ecole normale sup´erieure, 24, rue Lhomond, Paris, France and

2 Laboratoire de physique th´eorique, CNRS, UPMC and ´Ecole normale sup´erieure, 24, rue Lhomond, Paris, France

Thierry Mora1, Aleksandra M. Walczak2

6
1
0
2

 
r
a

 

M
7
1

.

 
 
]
E
P
o
i
b
-
q
[
 
 

1
v
8
5
4
5
0

.

3
0
6
1
:
v
i
X
r
a

Distributions of abundances or frequencies play an important role in many ﬁelds of science, from
biology to sociology, as does the R´enyi entropy, which measures the diversity of a statistical ensemble.
We derive a mathematical relation between the abundance distribution and the R´enyi entropy,
by analogy with the equivalence of ensembles in thermodynamics. The abundance distribution is
mapped onto the density of states, and the R´enyi entropy to the free energy. The two quantities are
related in the thermodynamic limit by a Legendre transform, by virtue of the equivalence between
the micro-canonical and canonical ensembles. In this limit, we show how the R´enyi entropy can
be constructed geometrically from rank-frequency plots. This mapping predicts that non-concave
regions of the rank-frequency curve should result in kinks in the R´enyi entropy as a function of its
order. We illustrate our results on simple examples, and emphasize the limitations of the equivalence
of ensembles when a thermodynamic limit is not well deﬁned. Our results help choose reliable
diversity measures based on the experimental accuracy of the abundance distributions in particular
frequency ranges.

As an increasing number of large datasets are becoming
available in a variety of ﬁelds, one often turns to reduced
statistics that can capture important properties of the
system, or help detect deviations from our expectations.
Distributions of abundances have proven useful as such
statistics, and have been used in many diﬀerent contexts,
from biology to linguistics, astrophysics and sociology.
This notion is best explained when counting biological
species from a sample. Say that species 1 was observed
n1 times, species 2 n2 times, etc. The abundance dis-
tribution discards information about the identity of the
sampled species, and focuses on the distribution of the
counts themselves n1, n2, etc. This notion is very gen-
eral and extends well beyond ecology. Counts can refer
to the number of times a word is used in a text, to the
number of people living in a given city, to the occurence
of speciﬁc spiking patterns in a population of neurons,
or to the abundance of speciﬁc lymphocyte clones in the
immune system, to give but a few examples.

An equivalent way of representing the abundance dis-
tribution is to order the counts from largest to smallest,
and plot them as a function of their rank in this order-
ing. For example, in the English language, one can order
words by their frequency of occurence, and study how
this frequency decreases with the rank. In 1949 Zipf ob-
served that this dependency roughly followed a power law
[1], and similar observations have later been made in a
variety of contexts [2]. Because of the ubiquity of these
power laws [3], frequency-versus-rank plots are commonly
represented on a double logarithmic scale.

Abundance distributions can contain useful, albeit in-
direct, information about the underlying process at work
in the system.
In ecology, they are used as a diagnos-
tic tool for detecting deviations from the prediction of a
neutral model of population dynamics [4, 5]. The Yule
speciation process [6, 7], called the preferential attach-
ment process in the context of networks [8], also predicts
a speciﬁc form for the abundance distribution, which is
consistent with Zipf’s law in some limit [7]. The abun-

dance distribution of spike patterns in the retina has been
used to study the critical properties of the underlying
neural network [9], and a similar analysis was performed
on small patches of natural images [10]. The distribution
of sizes of lymphocyte clones in the immune system also
seems to generically follow power-laws, which puts con-
straints on the rules of their population dynamics [11].

Abundance distributions are closely related to the no-
tion of diversity. Diversity can be deﬁned in a number
of ways: total number of types in the distribution, Shan-
non’s entropy [12], Simpson’s diversity index, etc. It has
long been realized [13] that these diﬀerent kinds of di-
versity can all be brought under the common deﬁnition
of the R´enyi entropy [14]. This quantity, which depends
on a single parameter called order, generalizes Shannon’s
and Gibbs’ entropy. It is commonly used in ecology to
quantify diversity, but has also received increasing at-
tention in condensed matter in the context of quantum
entanglement [15].

Here we show how the R´enyi entropy can be geometri-
cally constructed from the abundance distribution in the
thermodynamic limit. This construction allows one to
visualize graphically how diﬀerent measures of diversity
arise from a given abundance distribution. It indicates
which are the abundances that are determinant in each
diversity measure, and gives a visual assessment of when
to trust the estimate of these measures.

Our result relies on the framework of statistical me-
chanics, piecing together previous observations. The
equivalence between rank-frequency curves and the
micro-canonical entropy has been previously reported in
[16]. The link between micro-canonical entropy and free
energy is a classical result of statistical mechanics, known
as the equivalence of ensembles [17]. The mapping be-
tween free energy and the R´enyi entropy has been pointed
out recently [18]. By bringing these results in a common
framework, we hope to clarify the correspondance be-
tween abundance distributions, the density of states and
R´enyi entropies, and propose a straightforward geomet-

ric method for assessing diversity directly from the abun-
dance distribution represented in an appropriate manner.

where Z(β), the normalization factor of the Boltzmann
distribution pβ(s) = exp(−βE)/Z(β) at inverse temper-
ature β,

R´enyi entropy and free energy

Z(β) ≡

e−βE(s)

,

(7)

(cid:34)(cid:88)

(cid:35)

2

(cid:80)

Let us deﬁne a probability distribution p(s), where s is
a state or a type that can take a discrete number of val-
ues. p(s) is a relative abundance, or a frequency, so that
s p(s) = 1. The variable s can be a spin conﬁguation
of a large system, a species, a biological sequence, or a
spiking pattern from a population of neurons.
The R´enyi entropy of order β is deﬁned as:

(cid:35)

(cid:34)(cid:88)

s

H(β) =

1
1 − β

ln

p(s)β

.

This quantity generalizes Shannon’s entropy,

(1)

(2)

H1 = −(cid:88)

s

p(s) ln p(s),

(cid:32)(cid:88)

(cid:33) 1

1−β

to which it reduces in the limit β → 1. The R´enyi entropy
is associated with a family of diversity indices, deﬁned as:

D(β) = exp[H(β)] =

p(s)β

.

(3)

s

This quantity can be interpreted as an eﬀective number
of states. When β = 0, D(0) is just the raw, total num-
ber of possible types in the system. When β = 2, it is
equal to the inverse of Simpson’s index, also interpreted
as an eﬀective number of types, and related to the Gini-
Simpson index (deﬁned as 1 − 1/D(2)), commonly used
to measure inequalities. When β = 1, D(1) is the expo-
nential of Shannon’s entropy, and is sometimes called the
true diversity. In Shannon’s original work [12], D(1) is
the eﬀective number of codewords needed to compress s.
We ﬁrst derive an equivalence between the R´enyi en-
tropy and the free energy of statistical mechanics, as al-
ready reported in [18]. We formally rewrite the probabil-
ity distribution p(s) as a Boltzmann distribution:

p(s) =

e−E(s),

1
Z1

(4)

where the temperature is set to kBT = 1 by deﬁnition.
For example, this mapping can be realized by deﬁning
E(s) ≡ − ln P (s) and Z1 = 1, but to keep things general
we will assume an arbitrary Z1. We deﬁne the free energy
at unit temperature as F1 = − ln Z1. The R´enyi entropy
can be rewritten as:

H(β) =

1
1 − β

ln

e−βE(s)+βF1

β(F1 − F (β))

1 − β

,

=

(5)
In this formula, F (β) is the usual free energy at inverse
temperature β:

(cid:34)(cid:88)

s

(cid:35)

s

is called the partition function.

Thus the R´enyi entropy is closely related to the free en-
ergy, after mapping to the Boltzmann distribution. Note
that this mapping is a deﬁnition, and does not follow
from physical considerations.

Abundance distribution and micro-canonical entropy

There also exists a rigorous analogy between the den-
sity of states and the abundance distribution [16]. The
abundance distribution is deﬁned as the distribution over
p itself:

ρ(p) =

δ[p(s) − p],

(8)

where δ(·) is Dirac’s delta function.
It is more conve-
nient to work with the cumulative density of p, as it is
not plagued with Dirac deltas, and is invariant under
reparametrization:

Cp(p) =

Θ[p(s) − p],

(9)

s

where Θ(x) is the Heaviside function, equal to 1 for x ≥ 0
and 0 otherwise.

The cumulative distribution of abundances is related
to another representation of diversity, the rank-frequency
curve or Zipf’s plot [1]. In this representation, the sys-
tem’s states are ranked from most abundant to least
abundant, and their abundance shown as a descreasing
function of the rank. The rank of a given abundance p
is given exactly by Cp(p). Hence, rank-frequency graphs
are simply plots of p versus Cp(p). In other words, they
represent the inverse function of the cumulative abun-
dance distribution.

Since p and E are related by the Boltzmann distribu-
tion (4), we can equivalently deﬁne the cumulative den-
sity of states, which counts all states under a given tem-
perature E.

CE(E) =

Θ[E − E(s)].

(10)

This cumulative distribution is related to the cumulative
distribution of p through CE(E) = Cp(e−E/Z1). The
usual density of states,

(cid:88)

s

(cid:88)

(cid:88)

s

(cid:88)

s

F (β) ≡ − 1
β

ln Z(β),

(6)

ρ(E) =

δ(E − E(s)),

(11)

is obtained as dCE(E)/dE. In order to avoid issues of
deﬁnition with Dirac delta functions, we deﬁne a cumula-
tive micro-canonical entropy as S(E) = ln CE(E), rather
than the usual micro-canonical entropy.
In this deﬁni-
tion, for ease of notation we implictly take the Boltzmann
constant to be kB = 1.

Equivalence of ensembles

Following textbook statistical mechanics, the partition
function (7) can be rewritten entirely as a function of the
density of states:

Z(β) =

dEρ(E)e−βE = β

= β

dEeS(E)−βE,

dECE(E)e−βE

(12)

(cid:90)

(cid:90)

(cid:90)

where we have used integration by parts in the second
equality. In other words, Z(β) is the Laplace transform
of the density of states.
In the standard thermodynamic limit, where both the
entropy and energy are assumed to be extensive, S(E) ∼
E ∼ N , where N is the system’s size, this integral can
be approximated by its saddle point, an approximation
also known as Laplace’s method:

(cid:18)

(cid:19)1/2

Z(β) ≈ β

π

|S(cid:48)(cid:48)(E∗)|

eS(E∗)−βE∗

,

(13)

where E∗, which maximizes the term in the exponen-
tial in Eq. 12, is given by the standard thermodynamic
relation:

(cid:12)(cid:12)(cid:12)(cid:12)E∗

dS
dE

= β,

(14)

or more classically dS/dE = 1/T . The free energy (Eq. 6)
then reads:
F (β) = E∗ − 1
β

S(E∗) − ln(β)
β

ln(|S(cid:48)(cid:48)(E∗)|

2β

− ln(π)
2β

+

.

(15)
In the thermodynamic limit the last three terms are
subextensive (scaling sublinearly with the system’s size)
and therefore dropped, reducing to the usual deﬁnition of
the free energy, F = E−T S. Then the Massieu potential
(also called the Helmholtz free entropy) Φ(β) = −βF (β)
and the micro-canonical entropy S(E) are related by a
Legendre transform:

Φ(β) = extrE[S(E) − βE],
S(E) = extrβ[Φ(β) − βE],

(16)
(17)

in which E and β are conjugate variables. These rela-
tions deﬁne the equivalence between the micro-canonical
and canonical ensembles, which is valid as long as S(E)
is a concave function [17].
In this equivalence, diﬀer-
ent inverse temperatures β are used to sample states of

3

diﬀerent typical energies, acting as a large-deviation pa-
rameter. These relations formally follow from the Boltz-
mann distribution in the thermodynamic limit, and are
the same as in standard thermodynamics.

The saddle-point approximation is more than a compu-
tational trick. It also impies that, in the thermodynamic
limit, the measure is dominated by just a few states that
all have pratically the same energy E∗. There are of the
order of exp[S(E∗)] such states, which each have roughly
the same probability exp(−βE∗)/Z(β) = exp[−S(E∗)].
Their entropy is then given by Boltzmann’s formula:
pβ(s) ln pβ(s) ≈ ln[eS(E∗)] = S(E∗),

H1[pβ] = −(cid:88)

s

(18)
where H1[pβ] is the canonical entropy at inverse tempera-
ture β, not to be confused with the R´enyi entropy H(β).
The result of Eq. 18 can be shown more rigorously by
using the exact identity:

with (cid:104)x(cid:105)β = (cid:80)

H1[pβ] = β[(cid:104)E(cid:105)β − F (β)],
(19)
s pβ(s)x(s), and by showing (cid:104)E(cid:105)β ≈ E∗

using Laplace’s method.

Legendre construction

The Legendre transform (16) can be constructed geo-
metrically, as illustrated by Fig. 1. In this construction,
F (β) is obtained as the intercept of the tangent to S(E)
of slope β (dashed line in Fig. 1) with the abscissa. To see
this, we write the condition for E∗ at the point where the
tangent of slope β touches the S(E) curve, dS/dE = β,
which is exactly Eq. 14. The equation deﬁning the tan-
gent is then given in (E, S) space by:

S = S(E∗) + β(E − E∗).

(20)

Solving in E for the intercept with the abscissa, S = 0,
gives E∗− S(E∗)/β = F (β), which is the result of Eq. 15
up to the sub-extensive terms.

We can generalize this construction to the R´enyi en-
tropy, which is obtained as the intersection of two tan-
gents to S(E), of slopes 1 and β respectively. To ver-
ify this assertion, one writes the system of two linear
equations parametrizing these two tangents in the (S, E)
space:

S = E − F1,
S = β[E − F (β)].

(21)
(22)
The solution to these two equations in S is β(F1 −
F (β)/(1 − β), which is exactly the R´enyi entropy H(β)
according to (5).

As already mentioned, the R´enyi entropy reduces to

the classical Shannon or Gibbs entropy, H1, for β = 1:

lim
β→1

H(β) =

dF
dβ

= H1.

(23)

(cid:12)(cid:12)(cid:12)(cid:12)β=1

4

then CE(E) =(cid:82) E

tors, which are involved in recognizing pathogens, are
generated according to the probability distribution p(s),
which was inferred from the data. For each generated
sequence, its probability of generation p, and thus its en-
ergy E = − ln p, is also output by the model. This allows
us to compute empirically the probability distribution of
E under the model, which is proportional to the number
of states with a certain energy multiplied by their prob-
ability p = e−E, P (E) ∝ ρ(E)e−E, from which ρ(E) and
0 dE(cid:48)ρ(E(cid:48)) are obtained. Note that this
distribution is synthetically created from the model of
generation by drawing random, independent sequences.
It is distinct from clone-size distributions usually found
in the literature [11, 20, 21], which have a clonal struc-
ture and are not made of independent samples. Also note
that this ensemble has no natural thermodynamic limit,
because sequences have a ﬁnite length.
It makes for a
good test case for our method in a real-world example.

The rank-frequency plot is represented in Fig. 2 in a
double logarithmic scale. Following the previous argu-
ments, in this representation the diversity index D(β) =
eH(β) can be approximated by:

1. drawing the tangent of slope -1 (black solid line) to

the rank-frequency curve;

2. drawing the tangent of slope −β−1 (dashed line) to

the same curve;

3. projecting the intersection point between these two

lines onto the rank axis.

the true diversity index D(1) = (cid:81)
Simpson’s inverse index D(2) = 1/(cid:80)

The tangency point of the tangent of slope 1 gives
s p(s)−p(s), i.e.
the
exponential of Shannon’s entropy.
In Fig. 2 we illus-
trate the example of β = 2, which allows us to read of
s p(s)2. The exact
values for these two quantities, D(1) = 4.9 · 1013 and
D(2) = 3.4 · 109, are roughly approximated, although
underestimated, by the construction.

In the true thermodynamic limit, which is not strictly
realized but approached in this example, the diversity
measure D(β) is eﬀectively dominated by just a fraction
of sequences whose rank is close to D(β) (on a logarith-
mic scale), according to Laplace’s approximation. A con-
sequence of this concentration is that diﬀerent diversity
measures, such as Shannon’s entropy or Simpson’s index,
may in fact be determined by entirely distinct sequences.
The construction allows for a quick assessment of
whether the sampling depth can support the estimation
of the R´enyi entropy H(β), and its associated diversity
D(β), for a given index β. When the tangent of slope
−β−1 touches the curve towards its end, where states are
becoming rare and may have been observed only once, it
is probably safe to assume that the R´enyi entropy can-
not be reliably computed from the data, because it is
determined by states which have not been sampled well.
This limitation applies to the Legendre construction as
well as to any other estimate of Renyi’s entropy. Such

FIG. 1: Geometric construction of the R´enyi entropy from
the density of states. In the classical Legendre construction,
the free energy F (β) is obtained as the intersection of the
tangent to the micro-canonical entropy curve S(E) (in red)
of slope β, where β is the inverse temperature, and the ab-
scissa. The R´enyi entropy of order β, H(β), is obtained as the
intersection between the tangents of slope 1 and β, projected
onto the ordinate. Inset: the micro-canonical entropy curve is
equivalent, up to a 90o rotation, to the rank-frequency curve
represented on a logarithmic scale.

This limit can also be undertood geometrically. When
β → 1, the intersection between the two tangents tends
to the point of tangency of slope 1, dS/dE|E∗ = 1, where
S(E∗) is equal to the Shannon entropy H1 (Eq. 18).

From the abundance distribution to R´enyi entropy:

a geometric approach

Now that we have derived analogous relations to stan-
dard thermodynamics, we can use the geometric repre-
sentation of the Legendre transform to read oﬀ diver-
sity measures from data. The Legendre construction
can be transposed into the language of the abundance
distribution, provided that this distribution is appropri-
ately represented as a rank-frequency curve. Recall that
S(E) = ln CE(E), where CE is the rank of states of en-
ergy E, ordered from the lowest to the highest energy,
i.e.
from the most frequent to the least frequent state.
On the other hand, E = − ln p + F1, where p is the fre-
quency. Thus, the micro-canonical entropy function, S
vs. E, and the rank-frequency relation in logarithmic
scale, ln(p) vs. ln(rank), are exactly equivalent up to a
90o rotation, as illustrated in the inset of Fig. 1.

Thanks to this equivalence, the Legendre construc-
tion described above can be applied directly to the rank-
frequency curve plotted on a log-log scale. We illustrate
such a consctruction with the distribution of generation
probabilities of T-cell receptor beta chains [19]. DNA
sequences s coding for the beta chain of T-cell recep-

E/N-0.4-0.3-0.2-0.100.10.20.30.4S/N-0.100.10.20.30.40.50.60.7S(E)F1H111H(β)F(β)β190oln(rank)/N00.20.40.6ln(p)/N-0.8-0.6-0.4-0.2hence:

(cid:12)(cid:12)(cid:12)(cid:12)β=1

dH
dβ

(cid:12)(cid:12)(cid:12)(cid:12)β=1

.

=

1
2

d2F
dβ2

5

(26)

Therefore, the discontinuity in dF (β)/dβ at β = 1 trans-
lates into a discontinuity in H(β) itself. Again, this dis-
continuity can be seen geometrically. Let us assume that
S(E) = S0 +(E−S0) over a range (E1, E2). The tangent
of slope 1 coincides with S(E) throughout this range. As
a result, the intersection between the tangent of slope β
jumps from E1 to E2 as β crosses 1, causing H(β) to
jump from S(E1) to S(E2).

This kind of singularity not only implies discontinuities
in the R´enyi entropy or its derivatives, but also suggest
that the entropy may ill-deﬁned or hard to estimate when
α = 1. In that case, a whole range of (S, E) pairs, in-
stead of a single point, are candidates for the tangency
point between the line of slope 1 and the micro-canonical
entropy S(E). The entropy is ultimately determined by
corrections that are ignored in the thermodynamic limit.
The micro-canonical entropy need not be strictly linear
over a portion of energies for a discontuinuity to occur.
In fact, any convexity in S(E) is predicted to produce
the same eﬀect [17].

For this reason, caution should be used when dealing
with distributions that look like a power law, or are not
concave in logarithmic scale. Not only may the Legen-
dre construction be unreliable, but so may other more
direct estimates of the R´enyi entropy, since the system
lacks a characteristic energy scale. Interestingly, several
abundance distributions in biology have been reported
to follow power laws with exponent α = 1 [9, 10, 16], for
which the R´enyi entropy is expected to have a disconti-
nuity.

Discussion

In this paper we have made an explicit link between
classical representations of diversity in ecology and other
ﬁelds, and the framework of classical statistical mechan-
ics. This mapping allows one to bring many quantities
coming under many diﬀerent names – species abundance
distribution, clone-size distribution, frequency spectrum,
Shannon entropy, R´enyi entropy, Simpson’s index, etc.
– within a common framework. It provides an quick an
easy way to simply read oﬀ diversity directly from rank-
frequency plots.

Our geometric construction assumes the thermody-
namic limit, which may not be satisﬁed or even well
deﬁned. For instance, the distribution of abundances
prediced by a neutral model, or Fisher’s log-series P (n) ∝
αn/n [22], does not admit a natural deﬁnition of system’s
size, and thus has no well deﬁned thermodynamic limit.
The same goes for Pareto distributions P (n) ∝ n−a. In
these cases where no thermodynamic limit exists, the
Legendre construction is no substitute for a direct es-
timate, but may still give a reasonable guess. It can also

FIG. 2: Illustration of the Legendre construction of the R´enyi
entropy on the rank-frequency curve of randomly generated
T-cell receptor beta chains [19]. The construction is identi-
cal to that of Fig. 1, with slope β replaced by slope −β−1
because of the rotation. The projection onto the rank axis
gives an approximation to the diversity index of order β,
D(β) = exp[H(β)].

An extreme case is β = 0:

a diagnosis indicates which diversity measure might be
appropriate to use in a given context, depending on the
shape of the rank-frequency curve.
the tangent of slope
−β−1 = −∞ intersects with the rank-frequency curve at
the maximal possible rank, which is also the total num-
ber of sampled types. In most cases (as in this one) this
maximal rank does not represent well the true total di-
versity, D(0), which should also include unseen types. A
similar underestimation is expected to happen for ﬁnite
values of β for which the tangent is ill-deﬁned.

Singular cases

It is interesting to consider what happens to the R´enyi
entropy when the rank-frequency relation is locally a
power law. In the micro-canonical framework, a power
law in the cumulative density of abundances [16],

C(p) ∝ 1
pα ,

(24)

translates into a linear density of states, S(E) = S0 +
α(E − E0). This behaviour, as long as it spans an ex-
tensive range of energies, leads to a discontinuity in the
derivative of F (β) at β = α. For α (cid:54)= 1, Eq. 5 implies
that the derivative of H(β) exhibits the same kind of
discontinuity at β = α.

For α = 1 the discontinuity is of a diﬀerent nature.

Eq. 5 can be expanded around β = 1 as:

H(β) ≈ H1 +

β − 1

2

d2F
dβ2

,

(25)

(cid:12)(cid:12)(cid:12)(cid:12)β=1

rank100101010201030frequency10-3510-3010-2510-2010-1510-1010-5100slopeβ−1=∞≈D(0)slope1≈D(1)slopeβ−1=1/2≈D(2)hint whether such a direct estimate is possible at all, by
identifying the range of frequencies or abundances that
are expected to dominate the diversity measure.

Depending on how well sampled the distribution is,
diﬀerent orders β of the R´enyi entropy may be appro-
priate. The proposed framework can aid in choosing the
right measure depending on the data. In general, the less
well sampled the data is, the higher the order should be
chosen. For instance, Simpson’s index is less sensitive to

poor sampling than the Shannon entropy, which itself is
easier to estimate from the data than the total number of
states. Ultimately, the particular form of the abundance
distribution should be examined to decide which measure
can or should be used.

This work was supported in part by grant ERCStG n.
306312, and by the National Science Foundation under
Grant No. NSF PHY11-25915 through the KITP where
part of this research was done.

6

[1] Zipf GK (1949) Human behavior and the principle of

[12] Shannon CE (1948) A mathematical Theory of Commu-

least eﬀort (Addison-Wesley Press).

nication. Bell Sys Tech J 27:379–423 & 623–656.

[2] Newman MEJ (2005) Power laws, Pareto distributions

[13] Hill AMO (1973) Diversity and Evenness : A Unifying

and Zipf’s law. Contemp. Phys. 46:323–351.

[3] Schwab DJ, Nemenman I, Mehta P (2014) Zipf’s law
and criticality in multivariate data without ﬁne-tuning.
Phys. Rev. Lett. 113:068102.

[4] Volkov I, Banavar JR, Hubbell SP, Maritan A (2003)
Neutral theory and relative species abundance in ecology.
Nature 424:1035–1037.

[5] Chisholm RA, Pacala SW (2010) Niche and neutral
models predict asymptotically equivalent species abun-
dance distributions in high-diversity ecological commu-
nities. Proc. Natl. Acad. Sci. 107:15821–15825.

[6] Yule G (1925) A mathematical theory of evolution, based
on the conclusions of Dr. J. C. Willis, F.R.S. Philosoph-
ical Transactions of the Royal Society of London. Series
BContaining Papers of a Biological Character 213:21–87.
[7] Simon H (1955) On a class of skew distribution functions.

Biometrika 42:425–440.

[8] Barab´asi AL, Albert R (1999) Emergence of scaling in

random networks. Science (80-. ). 286:11.

[9] Tkaˇcik G, et al. (2015) Thermodynamics and signatures
of criticality in a network of neurons. Proc. Natl. Acad.
Sci. 112:11508–13.

[10] Stephens GJ, Mora T, Bialek W, Tkaˇcik G, Bialek W
(2013) Thermodynamics of natural images. Phys. Rev.
Lett. 110:018701.

[11] Desponds J, Mora T, Aleksandra W (2016) Fluctuat-
ing ﬁtness shapes the clone size distribution of immune
repertoires. Proc Natl Acad Sci USA 113:274.

Notation and Its Consequences. Ecology 54:427–432.

[14] R´enyi A (1961) On measures of entropy and information.

Entropy 547:547–561.

[15] Hastings MB, Gonz´alez I, Kallin AB, Melko RG (2010)
Measuring Renyi entanglement entropy in quantum
Monte Carlo simulations. Phys. Rev. Lett. 104:2–5.

[16] Mora T, Bialek W (2011) Are Biological Systems Poised

at Criticality? J. Stat. Phys. 144:268–302.

[17] Touchette H, Ellis RS, Turkington B (2004) An intro-
duction to the thermodynamic and macrostate levels of
nonequivalent ensembles. Physica A 340:138–146.

[18] Baez JC (2011) Renyi Entropy and Free Energy. arXiv

p 1102.2098.

[19] Murugan A, Mora T, Walczak AM, Callan CG (2012)
Statistical inference of the generation probability of T-
cell receptors from sequence repertoires. Proc. Natl.
Acad. Sci. 109:16161–16166.

[20] Mora T, Walczak AM, Bialek W, Callan CG (2010) Max-
imum entropy models for antibody diversity. Proc. Natl.
Acad. Sci. U. S. A. 107:5405–5410.

[21] Zarnitsyna VI, Evavold BD, Schoettle LN, Blattman JN,
Antia R (2013) Estimating the diversity, completeness,
and cross-reactivity of the T cell repertoire. Front. Im-
munol. 4:485.

[22] Fisher RA, Corbet AS, Williams CB (1943) The relation
between the number of species and the number of indi-
viduals in a random sample of an animal population. J.
Anim. Ecol. 12:42–58.

