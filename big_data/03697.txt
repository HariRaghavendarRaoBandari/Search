SUBSAMPLING FOR GRAPH POWER SPECTRUM ESTIMATION

Sundeep Prabhakar Chepuri and Geert Leus

Delft University of Technology (TU Delft), The Netherlands

Email: {s.p.chepuri; g.j.t.leus}@tudelft.nl.

6
1
0
2

 
r
a

M
 
1
1

 
 
]
T
I
.
s
c
[
 
 

1
v
7
9
6
3
0

.

3
0
6
1
:
v
i
X
r
a

ABSTRACT

In this paper we focus on subsampling stationary random processes
that reside on the vertices of undirected graphs. Second-order sta-
tionary graph signals are obtained by ﬁltering white noise and they
admit a well-deﬁned power spectrum. Estimating the graph power
spectrum forms a central component of stationary graph signal pro-
cessing and related inference tasks. We show that by sampling a sig-
niﬁcantly smaller subset of vertices and using simple least squares,
we can reconstruct the power spectrum of the graph signal from the
subsampled observations, without any spectral priors. In addition,
a near-optimal greedy algorithm is developed to design the subsam-
pling scheme.

Index Terms— Graph signal processing, stationary graph pro-
cesses, covariance sampling, subsampling, power spectrum estima-
tion.

1. INTRODUCTION

Processing signals residing on the vertices of graphs is recently re-
ceiving a signiﬁcant amount of interest for network science appli-
cations. In particular, generalizing as well as drawing parallels of
classical time-frequency analysis tools to graph data analysis while
incorporating the irregular structure on which the graph signals are
deﬁned is an emerging area of research [1, 2].

We are interested in sampling and processing stationary graph
signals, which are stochastic processes deﬁned on graphs with
second-order statistics that are invariant similar to time series, but
in the graph setting. Second order stationary graph signals have a
well-deﬁned graph power spectrum. Stationary graph signals can
be generated by ﬁltering white noise (or any other stationary graph
process) and the graph power spectrum of the ﬁltered signal will be
characterized by the squared magnitude of the frequency response
of the ﬁlter. Using the idea of graph second-order stationarity,
inference problems on graphs such as smoothing, prediction, and
deconvolution can be solved by designing optimum (minimum mean
squared error) Wiener-like ﬁlters. Although Wiener ﬁlters for graph
signals can be derived similar to time-domain signals [3], graph
power spectrum estimation forms a crucial component of such ﬁlter
designs.

In this paper, we focus on reconstructing graph second-order
statistics, more speciﬁcally the graph power spectrum by observing
a reduced subset of graph nodes. The fact that we are reconstruct-
ing the graph power spectrum, instead of the graph signal enables
us to subsample or sparsely sample the graph signal and yet recon-
struct the power spectrum of the original graph signal, even without
any spectral priors (e.g., sparsity, bandlimited with known support).

This work was supported by the KAUST-MIT-TUD consortium

grant OSR-2015-Sensors-2700.

Software to produce results of this paper can be downloaded from

http://cas.et.tudelft.nl/∼sundeep/sw/gpsd.zip

This is a new and different perspective as compared to subsampling
for graph signal reconstruction [4–6] and it extends the ﬁeld of com-
pressive covariance sensing [7, 8] to graph settings.

We present two approaches, namely, the graph spectral domain
and the graph vertex domain approach, where the processing is
done in the graph frequency and data domain, respectively (hence
the name). One of the results shows that with a reduced subset of
O(√N ) observations and using least squares, we can reconstruct the
graph power spectrum of a length-N graph signal, even in the ab-
sence of spectral priors. Any available spectral priors will naturally
lead to a higher compression. We provide a low-complexity and
near-optimal greedy algorithm for designing the sampling matrix
that essentially performs node subset selection, which is a discrete
combinatorial optimization problem.

2. BACKGROUND AND MODELING

Throughout the paper we denote matrices (column vectors) with up-
per (lower) bold face letters. The ℓ0-(quasi) norm refers to the num-
ber of non-zero entries in w, i.e., kwk0 := |{m : wm 6= 0}|.
2.1. Graph signals

Consider a dataset with N elements, which live on an irregular struc-
ture represented by a known undirected graph G = (V, E ), where the
vertex set V = {v1,··· , vN} denotes the set of nodes, and the edge
set E reveals any connection between the nodes. We refer to such
datasets as graph signals.
Let us construct the adjacency matrix A ∈ SN with a nonzero
(i, j)th entry [A]i,j denoting the strength of the edge connecting the
ith node and the jth node, while the entry is set to zero if no edge
exists between the ith node and the jth node. The degree of the ith
node is deﬁned as di =PM
j=1[A]i,j. An associated metric, the so-
called graph Laplacian is deﬁned as L = D−A ∈ SN , where D =
diag(d1, d2, ··· , dN ) ∈ RN×N . We introduce a symmetric matrix
S ∈ SN , where [S]i,j is nonzero only if i = j or (i, j) ∈ E. The
sparsity pattern of S captures the local structure of the graph, hence
S is referred to as the graph-shift operator [1,9]. Possible candidates
for S are the graph Laplacian L or the adjacency matrix A. Since
S is symmetric, it admits the following eigenvalue decomposition

S = U ΛU H

= [u1,·· · , uN ] diag(λ1,·· · , λN ) [u1, ··· , uN ]H ,
n=1 and the eigenvalues {λn}N

where the eigenvectors {un}N
n=1 of
S provide the notion of frequency in the graph setting [1,2]. Specif-
ically, {un}N
n=1 provide a Fourier-like basis for graph signals with
the entire spectrum denoted by {λn}N

(1)

The graph shift operator S can be used to deﬁne graph ﬁlters of

n=1.

the form [1, 9]

3. GRAPH POWER SPECTRUM ESTIMATOR

H =

hlSl = U L−1Xl=0

L−1Xl=0

hlΛl! U H ,

(2)

where the ﬁlter H is of degree L − 1 with ﬁlter coefﬁcients
h = [h0, h1, . . . , hL−1]T and the diagonal matrixPL−1
l=0 hlΛl =
diag(V Lh) can be viewed as the frequency response of the graph
ﬁlter. Here, V L is an N × L Vandermonde matrix with entries
[V ]i,j = λj−1

.

i

2.2. Stationary graph signals

Let x = [x1, x2,··· , xN ]T ∈ RN be a stochastic process deﬁned
on the vertices of the graph G with expected value m = E{x} and
covariance matrix Rx = E{(x − m)(x − m)T}. The notion of
second-order (or wide-sense) stationarity of signals deﬁned over reg-
ular structures can be generalized to graph signals as follows.

Deﬁnition 1 (Second-order graph stationarity [10,11]). A stochastic
graph process x is graph second-order stationary, if and only if the
following properties hold:

1. The mean of the graph signal is constant, E{xi} = m.
2. Matrices S and Rx are jointly diagonalizable.

An example of a second-order stationary graph process is white
noise with zero mean (thus satisﬁes the ﬁrst property) and covariance
matrix Rx = I, which can be expressed as Rx = U IU H (thus it
can be simultaneously diagonalized with S).

One way to generate second-order stationary graph signals is
by (graph) ﬁltering zero-mean unit-variance white noise, which we
denote by n ∈ RN . In other words, a stochastic graph process x can
be modeled as x = Hn, where we recall the graph ﬁlter deﬁned
in (2). It is easy to verify that the ﬁltered signal will have zero mean
and covariance matrix Rx = E{(Hn)(Hn)H} given by

Rx = HH H = U [diag(V Lh)]2U H

= U diag(p)U H .

(3)

This conforms with the second property listed in Deﬁnition 1. The
diagonal matrix diag(p) is the graph power spectral density or
graph power spectrum matrix. We formally introduce it through the
following deﬁnition.

Deﬁnition 2 (Graph power spectrum). The graph power spectral
density of a stationary graph process is a real-valued nonnegative
length-N vector p deﬁned as

diag(p) = U H RxU .

(4)

Alternatively, [p]n = [V Lh]2
n.

It is worth observing that white noise deﬁned on graphs has a
constant graph power spectrum. In sum, graph stationarity is pre-
served by linear ﬁltering, thus graph stationary signals with a pre-
scribed graph power spectrum can be generated by ﬁltering white
noise. In fact, the graph power spectrum of the ﬁltered signal is re-
shaped according to the ﬁlter.

The size of the datasets inhibits a direct computation (using a graph
Fourier transform matrix) of the graph power spectrum using (4) as
it requires diagonalization of the graph shift operator that computa-
tionally costs O(N 3), and in addition, it requires observing all the
nodes for computing Rx. In what follows, we introduce the concept
of subsampling graph signals for power spectrum estimation, where
we leverage the second-order graph stationarity. More speciﬁcally,
we are interested in determining a reduced set of K graph nodes to
sample and in estimating the power spectrum of the entire graph sig-
nal from these subsampled observations. This problem is even more
challenging in the graph setting as compared to compressive covari-
ance sensing of signals deﬁned over regular structures [7, 8]. This is
because for signals with regular support, the covariance matrix has
some structure (e.g., Toeplitz) that enables elegant subsampling, but
for graph signals, the covariance matrix does not admit any known
structure, in general.

3.1. Graph spectral domain

y = Φ(w)x = Φ(w)Hn.

Consider the problem of estimating the graph power spectrum of
the second-order stationary graph process x ∈ RN from a set of
K ≪ N linear observations stacked in the vector y ∈ RK, given by
(5)
Here, Φ(w) = diagr(w) ∈ {0, 1}K×N is a sparse sampling or
subsampling matrix guided by a component selection vector w =
[w1,··· , wN ]T ∈ {0, 1}N , where wi = 1 indicates that the ith
graph node is selected, otherwise it is not selected (diagr(·) repre-
sents a diagonal matrix with the argument on its diagonal but with
the all-zero rows removed).

Using the subsampling scheme in (5), the covariance matrix of

the subsampled graph process y can be computed as

Ry = ΦRx

ΦH = ΦU diag(p)U H ΦH ∈ RK×K ,

(6)

where we simply write Φ(w) as Φ for conciseness. Vectorizing1 (6),
we obtain a set of K 2 equations in N unknowns:

ry = vec(Ry) = (ΦU ◦ ΦU )p

(a)

= (Φ ⊗ Φ)(U ◦ U )p=(Φ ⊗ Φ)Ψsp,

(7)

where (a) is due to the matrix property (A⊗ B)(C ◦ D) = (AC ◦
BD), and the subscript “s” in Ψs (constructed using the graph
Fourier matrix) stands for spectral domain approach. If the matrix
(ΦU ◦ ΦU ) has full column rank, which requires K 2 ≥ N, then
the graph power spectrum can be estimated in closed form via least
squares, given by

bp = (ΦU ◦ ΦU )†ry,

where for a full column rank matrix A, we have A† = (AT A)−1AT .

Remark 1 (Spectral priors). A higher compression can be achieved
if we have some prior knowledge about the graph spectra. More
speciﬁcally, it is possible to have K 2 < N, if we know a priori that
(a) the spectrum is bandlimited (e.g., lowpass) with known support,
or (b) the spectrum is sparse, but with unknown support. Further,
this information can be included while estimating the graph power
spectrum from (7), e.g., using a reduced-order least squares or an
ℓ1-norm regularized least squares.

1We use the matrix property vec(Adiag(d)B) = (BH ◦ A)d, where
◦ denotes the Khatri-Rao or columnwise Kronecker product, ⊗ denotes the
Kronecker product, and vec(·) is the matrix vectorization operator.

Remark 3. The fact that Sq = U ΛqU H from (1) allows us to
express Ψv in (10) as

Using (11) in (10), we get

Ψv = (U ◦ U )V Q.

ry = (Φ ⊗ Φ)(U ◦ U )V Qα = (ΦU ◦ ΦU )p.

(8)

4. SUBSAMPLER DESIGN

(11)

(12)

3.2. Graph vertex domain

The covariance matrix of a stochastic graph process and the graph
shift operator can be simultaneously diagonalized. This allows us to
express the covariance matrix Rx as a polynomial of the graph shift
operator of the form:

Rx =

αqSq,

Q−1Xq=0

where the Q = min{2L − 1, N} unknown expansion coefﬁcients
{αq}Q−1
q=0 collected in the vector α = [α0, α1,·· · , αQ−1]T ∈ RQ
completely characterize the covariance matrix. In other words, we
assume a linear parametrization of the covariance matrix Rx using
the set of Q symmetric matrices {S0, S,··· , SQ−1} ⊂ SN as a
basis.

Vectorizing Rx in (8) yields

rx = vec(Rx) =

Q−1Xq=0

αqvec(Sq) = Ψvα,

(9)

where we have stacked vec(Sq) to form columns of the matrix
Ψv ∈ RN 2×Q, and the subscript “v” in Ψv stands for ver-
tex domain approach. Using the matrix property vec(ABC) =
(CH ⊗ A)vec(B), the covariance matrix of the subsampled graph
ΦH, can be vectorized to obtain a set
process [cf. (6)] Ry = ΦRx
of K 2 equations in Q unknowns, given by

ry = vec(Ry) = (Φ ⊗ Φ)vec(Rx)

= (Φ ⊗ Φ)Ψvα.

(10)

If the matrix (Φ⊗Φ)Ψv has full column rank, which requires K 2 ≥
Q, then the overdetermined system can be solved using least squares
as

bα = [(Φ ⊗ Φ)Ψv]†ry.

The problem of estimating {αq}Q−1
q=0 is known as covariance match-
ing [12], and in the graph setting we refer to it as graph covariance
matching. The computationally expensive eigenvalue decomposi-
tion of the graph Laplacian that costs O(N 3) is not needed to recon-
struct α.

The graph power spectrum can be subsequently recovered ac-

cording to the following remark.

Remark 2. We can relate the vector p and the vector α, by using
(4) and (8). That is, we can write diag(p) = PQ−1
q=0 αq Λq, or in
matrix-vector form we have p = V Qα, where V Q is an N × Q
Vandermonde matrix with entries [V Q]i,j = λj−1
. To recover p
from α, however, requires all the N eigenvalues to construct V Q.

i

Finally, we show the equivalence between linear models (7) and

(10) as follows.

Algorithm 1 Greedy algorithm
1. Require X = ∅, K.
2. for k = 1 to K
s∗ = arg max
3.
X ← X ∪ {s∗}

s /∈X

4.
5. end
6. Return X

f (X ∪ {s})

If we can design a full-column rank model matrix (Φ ⊗ Φ)Ψ with
Ψ := Ψs or Ψ := Ψv, then we can perfectly recover the graph
power spectrum by observing a reduced set of only K graph nodes.
We will develop a low-complexity algorithm to design such full-
column rank matrices in this section.

trum of

The least squares solution developed in §3 depends on the spec-
T (w) = [(Φ(w) ⊗ Φ(w))Ψ(w)]T [(Φ(w) ⊗ Φ(w))Ψ]

i.e., the performance of least squares is better if the spectrum of
(Φ ⊗ Φ)Ψ is more uniform. Thus a good sparse sampler w can
be obtained by solving:

arg max
w∈{0,1}N

f (w)

s.t. kwk0 = K

(13)

with either f (w) = λmin{T (w)} or f (w) = log det{T (w)},
both of which try of balance the spectrum of T (w). Although the
above Boolean nonconvex problem with f (w) = λmin{T (w)} and
f (w) = log det{T (w)} can be relaxed and solved using convex
optimization (e.g., see [13, 14]), we will focus on the optimization
problem (13) with f (w) = log det{T (w)} as it can be solved near-
optimally using a low-complexity greedy algorithm.
Let us deﬁne an index set X that is related to the component
selection vector w as X = {m | wm = 1, m = 1, . . . , N}, where
X ⊆ N with N = {1, . . . , N}. We can now express the cost
function f (w) = log det{T (w)} equivalently as the set function
given by

f (X ) = log det(cid:26)X(i,j)∈X ×X

ψi,j ψT

i,j(cid:27) ,

(14)

where the N 2 column vectors {ψ1,1, ψ1,2,··· , ψN,N} are used to
form the rows of Ψ as Ψ = [ψ1,1, ψ1,2,·· · , ψN,N ]T . We use
such an indexing because the sampling matrix Φ ⊗ Φ results in a
structured (row) subset selection.
Submodularity —a notion based on the property of diminish-
ing returns, is useful for solving discrete combinatorial optimization
problems of the form (13) (see e.g., [15]). Submodularity can be
formally deﬁned as follows.
Deﬁnition 3 (Submodular function). Given two sets X and Y such
that for every X ⊆ Y ⊆ N and s ∈ M\Y, the set function f :
2N → R deﬁned on the subsets of N is said to be submodular, if it
satisﬁes

f (X ∪ {s}) − f (X ) ≥ f (Y ∪ {s}) − f (Y).

Further, if the submodular function is monotone nondecreasing,
that is, f (X ) ≤ f (Y) for all X ⊆ Y ⊆ N and normalized (i.e.,
f (∅) = 0), then a greedy maximization of such a function via Algo-
rithm 1 is near optimal with an approximation factor of (1 − 1/e),
where e is Euler’s number [16]. That is, f (X ) ≥ (1−1/e)f (OPT),
where f (OPT) = maxX ⊂N ,|X |=K f (X ). The cost function (14)
after a slight modiﬁcation satisﬁes the above property as stated in the
following lemma.

 

1

1.4

1.2

1

0.5

0.8

0

−0.5

−1

0.6

0.4

0.2

0

 
0

1

 

True PSD
Estimated PSD (0% compression)
Estimated PSD (50% compression)

 

2

Laplacian eigenvalues

3

4

5

Fig. 1: Graph spectral domain subsampling. Left: Random graph with N = 100 nodes. Sampled K = 50 nodes are depicted with a black circle. Right: True
and estimated graph power spectrum.

 

1

1.4

1.2

1

0.5

0.8

0

−0.5

−1

0.6

0.4

0.2

0

 
0

1

 

True PSD
Estimated PSD (0% compression)
Estimated PSD (90% compression)

 

2

Laplacian eigenvalues

3

4

5

Fig. 2: Graph vertex domain subsampling. Left: Random graph with N = 100 nodes. Sampled K = 10 nodes are depicted with a black circle. Right: Graph
power spectrum, where the true power spectrum is modeled with Q = 12.

Lemma 1. The set function f : 2N → R given by

f (X ) = log det(cid:26)X(i,j)∈X ×X

ψi,j ψT

i,j + ǫI(cid:27) − N log ǫ

is a normalized, nonnegative monotone, submodular function on the
set X ⊂ N . Here, ǫ > 0 is a small constant. Hence (15) is a
reasonable approximation of (14).

(15)

In (15), N log ǫ ensures that f (∅) is zero. Using the result

from [17] that the set function g : 2N → R, given by

g(X ) = log detnXi∈X

aiaT

i + ǫIo − N log ǫ

(16)

with column vectors {ai}N
i=1 is a normalized, nonnegative mono-
tone, submodular function on the set X ⊂ N , we can prove
Lemma 1. Therefore, the solution based on the greedy algorithm
summarized as Algorithm 1 results in a (1 − 1/e) optimal solution
for (13). Note that the number of summands in (16) and (15), is
respectively, |X| and |X|2. It is worth mentioning that the greedy
algorithm is linear in K, while computing (15) remains the domi-
nating cost. Nevertheless, (15) can be computed efﬁciently using
rank-1 updates, similar to [17].

Other submodular functions that promote full-column rank
model matrices, e.g., frame potential [18] deﬁned as f (w) =
tr{T H (w)T (w)}, are also reasonable costs to optimize. Finally,
random subsampling is not suitable as it might not always result in
a full-column rank model matrix.

5. NUMERICAL EXPERIMENTS

In this section we test the practical performance of the proposed esti-
mator as well the designed sparse sampler. For the experiments, we
use a random sensor graph with N = 100 nodes generated using the
GSPBOX [19]. The graph topology can be seen on the left side of
Figure 1 and Figure 2 along with a random signal realization. Graph

stationary signals are generated by ﬁltering zero-mean unit-variance
white noise with a lowpass ﬁlter, which has a squared magnitude fre-
quency response as shown in Figure 1 (labeled as “True PSD”) and
it has L = 7 ﬁlter coefﬁcients. We use Ns = 1000 snapshots to
form a sample covariance matrix, which we use in the experiments.
In the graph spectral domain approach, using Algorithm 1, we
ﬁrst design the subsampler by selecting rows of the matrix Ψs in
a structured manner determined by Φ ⊗ Φ, one by one. In other
words, we perform a row subset selection of the (modiﬁed) graph
Fourier matrix U ◦ U . For this particular scenario, a full-column
rank matrix (Φ ⊗ Φ)Ψs was obtained for K > 11. We show on
the right side of Figure 1, the reconstructed graph power spectrum
for K = 50 (i.e., 50% compression) as well as for K = N (it is the
diagonal of the sample covariance matrix with no compression). On
the left side of Figure 1, we show the selected graph nodes with a
black circle.

In the graph vertex domain approach, we use Q = 12 to con-
struct the model matrix Ψv. As before, we perform a row subset
selection of the matrix Ψs in a structured way using Algorithm 1.
We show on the right side of Figure 2, the least squares estimate
of the graph power spectrum using K = 10 (i.e., a compression of
90%). Such a high compression is possible because we a priori know
the low value of Q.

6. CONCLUSIONS

In this paper we have investigated sampling of random processes de-
ﬁned on graphs. In particular, we have focused on subsampling sta-
tionary graph signals for estimating the power spectral density. We
have shown that it is possible to subsample as low as O(√N ) ver-
tices and yet reconstruct the power spectrum of a signal deﬁned on a
graph with N vertices, without any spectral priors. The subsamplers
are designed using a greedy algorithm, which near optimally solves
the combinatorial Boolean optimization problem. A least squares es-
timator has been proposed to reconstruct the graph power spectrum
from the subsampled observations.

[17] M. Shamaiah, S. Banerjee, and H. Vikalo, “Greedy sensor
selection: Leveraging submodularity,” in Proc. of 49th IEEE
Conference on Decision and Control, Dec. 2010, Atlanta,
Georgia, USA, 2010.

[18] J. Ranieri, A. Chebira, and M. Vetterli, “Near-optimal sensor
placement for linear inverse problems,” IEEE Trans. Signal
Process., vol. 62, no. 5, pp. 1135–1146, Mar. 2014.

[19] N. Perraudin, J. Paratte, D. Shuman, V. Kalofolias, P. Van-
dergheynst, and D. K. Hammond, “GSPBOX: A toolbox for
signal processing on graphs,” ArXiv e-prints, Aug. 2014.

7. REFERENCES

[1] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and
P. Vandergheynst, “The emerging ﬁeld of signal processing
on graphs: Extending high-dimensional data analysis to net-
works and other irregular domains,” IEEE Signal Process.
Mag., vol. 30, no. 3, pp. 83–98, 2013.

[2] A. Sandryhaila and J. M. Moura, “Big data analysis with signal
processing on graphs: Representation and processing of mas-
sive data sets with irregular structure,” IEEE Signal Process.
Mag., vol. 31, no. 5, pp. 80–90, 2014.

[3] B. Girault, P. Goncalves, E. Fleury, and A. S. Mor, “Semi-
supervised learning for graph to signal mapping: a graph sig-
nal wiener ﬁlter interpretation,” in Proc. of IEEE International
Conference on Acoustics, Speech and Signal Processing, May
2014, Florence, Italy, 2014.

[4] A. Anis, A. Gadde, and A. Ortega, “Towards a sampling theo-
rem for signals on arbitrary graphs,” in Proc. of IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing,
May 2014, Florence, Italy, 2014.

[5] A. G. Marques, S. Segarra, G. Leus, and A. Ribeiro, “Sampling
of graph signals with successive local aggregations,” IEEE
Trans. Signal Process., vol. 64, no. 7, pp. 1832–1843, 2016.

[6] M. Tsitsvero, S. Barbarossa, and P. D. Lorenzo, “Uncertainty
principle and sampling of signals deﬁned on graphs,” in Proc.
of 49th Asilomar Conference on Signals, Systems and Comput-
ers, Nov. 2014, California, USA, 2015.

[7] D. D. Ariananda and G. Leus, “Compressive wideband power
spectrum estimation,” IEEE Trans. Signal Process., vol. 60,
no. 9, pp. 4775–4789, 2012.

[8] D. Romero, D. D. Ariananda, Z. Tian, and G. Leus, “Compres-
sive covariance sensing: Structure-based compressive sensing
beyond sparsity,” IEEE Signal Process. Mag., vol. 33, no. 1,
pp. 78–93, Jan 2016.

[9] A. Sandryhaila and J. M. Moura, “Discrete signal processing
on graphs,” IEEE Trans. Signal Process., vol. 61, no. 7, pp.
1644–1656, 2013.

[10] B. Girault, “Stationary graph signals using an isometric graph
translation,” in Proc. of 23rd European Signal Processing Con-
ference, Aug 2015, Nice, France, 2015.

[11] N. Perraudin and P. Vandergheynst, “Stationary signal process-

ing on graphs,” arXiv preprint arXiv:1601.02522, 2016.

[12] B. Ottersten, P. Stoica, and R. Roy, “Covariance matching es-
timation techniques for array signal processing applications,”
Digital Signal Processing, vol. 8, no. 3, pp. 185–210, 1998.

[13] S. P. Chepuri and G. Leus, “Sparsity-promoting sensor selec-
tion for non-linear measurement models,” IEEE Trans. Signal
Process., vol. 63, no. 3, pp. 684–698, Feb. 2015.

[14] ——, “Continuous sensor placement,” IEEE Signal Process.

Lett., vol. 22, no. 5, pp. 544–548, May 2015.

[15] A. Krause, Optimizing sensing: Theory and applications, ser.
Ph.D. dissertation, School of Comput. Sci. Carnegie Mellon
Univ., Pittsburgh, PA, United States, 2008.

[16] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher, “An analysis
of approximations for maximizing submodular set functions—
I,” Mathematical Programming, vol. 14, no. 1, pp. 265–294,
1978.

