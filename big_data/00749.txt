Non-additive Security Game

Sinong Wang1, Fang Liu1 and Ness Shroff2

6
1
0
2

 
r
a

M
3

 

 
 
]
T
G
.
s
c
[
 
 

2
v
9
4
7
0
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

Strategically allocating resources to protect targets against potential threats in an efﬁcient way is a key challenge
facing our society. From the classical interdiction game to the recently proposed Stackelberg Security Game, applying
computational game models to the security domain has made a real-world impact on numerous ﬁelds including
military attack and defense, ﬁnancial system security, political campaign and civil safeguarding. However, existing
methods assume additive utility functions, which are unable to capture the inherent dependencies that exist among
different targets in current complex networks.

In this paper, we introduce a new security game model, called Non-additive Security Game (NASG). It adopts
a non-additive set function to assign the utility to every subset of targets, which completely describes the internal
linkage structure of the game. However, both the number of utility functions and the strategies exponentially increase
in the number of targets, which poses a signiﬁcant challenge in developing algorithms to determine the equilibrium
strategy. To tackle this problem, we ﬁrst reduce the NASG to an equivalent zero-sum game and construct a low-rank
perturbed game via matrix decomposition and random low-dimensional embedding. Then, we incorporate the above
low-rank perturbed game into the Augmented Lagrangian and Coordinate Descent method. Using a series of careful
constructions, we show that our method has a total complexity that is nearly linear in the number of utility functions
and achieves asymptotic zero error. To the best of our knowledge, the NASG is the ﬁrst computational game model
to investigate dependencies among different targets.

I. INTRODUCTION

Providing security for critical infrastructure, cyber-physical networks and other ﬁnancial systems is a large and
growing area of concern. The key problem in many of these security domains is how to efﬁciently allocate limited
resources to protect targets against potential threats. With the development of computational game theory over the
past two decades, such resource allocation problems can be cast in the game-theoretic contexts, which provides
a more sound mathematical approach to determine the optimal defense strategy. It allows the analyst to factor
differential risks and values into the model, incorporate game-theoretic predictions of how the attacker would
respond to the security policy, and ﬁnally determine an equilibrium strategy that cannot be exploited by adversaries
to obtain a higher payoff.

One line of research initiated in the seminal paper of Wollmer (1964) is the interdiction game (IG). It is a
two-player normal form game, in which both players move simultaneously. The evader attempts to traverse a path

1Sinong Wang and Fang Liu is with graduate student of Department of Electrical and Computer Engineering, the Ohio State University, 2015

Neil Avenue, Columbus, OH 43210, USA, wang.7691@osu.edu

2Ness Shroff is with the Faculty of Department of Electrical and Computer Engineering, the Ohio State University, 2015 Neil Avenue,

Columbus, OH 43210, USA, shroff.11@osu.edu

through a network from the source node to the destination node, without being detected by an interdictor, while
the defender deploys the interdictor in different nodes or links to halt the possible intrusion. The IG has been
successfully applied to United States’ drug interdiction [18], communication network vulnerability [3] and urban
network security recently [4].

In some security domains, the defender can build the fortiﬁcations before the attack and it is thus in the leader’s
position from the point view of the game and able to move ﬁrst. The branch of research inspired by this phenomenon
is called Stackelberg Security Games (SSG), which includes a defender and an attacker, and several targets. The
defender moves ﬁrst by committing to a strategy and then it is observed by the attacker, who plays a best response
to the defender’s strategy. The SSG is currently used by many security agencies including Federal Air Marshals
Service, US Coast Guard, Transportation System Administration and even in the wildlife protection; see book by
Tambe [16] for an overview.

A. Motivation

A common limitation of existing security game models is that they do not consider the dependency among the
different targets. In the SSG, the payoff functions for both players are additive, i.e, the payoff of a group of targets
is the sum of the payoffs of each target separately. This assumption means that the security agency measures the
importance of several targets without considering the synergy effect among them. In practice, there exists some
linkage structure among the targets such that attacking one target will inﬂuence the other targets. For instance, an
attacker attempts to destroy the connectivity of a network and the defender aims to protect it. The strategy for
both players is to choose the nodes of the network. If there are two nodes that constitute a bridge of this network,
successfully attacking both of them will split the network into two parts and incur a huge damage, while attacking
any one of them will have no signiﬁcant effect. Hence, traditional models that ignore the inherent synergy effect
between the targets are limiting and could lead to catastrophic consequences.

Fig. 1. Example of security game in a 20−nodes network.

Example 1: As shown in Fig. 1, we have a 20−node network. It is clearly that nodes 1, 2, 3 and 4 are the
critical battleﬁelds in this network. Suppose attacker’s and defender’s strategies are {1},{2},{3},{1, 2} or {3, 4},
where {v} denotes the index of the nodes. We adopt the network value proposed by [3] as the security measure
for different nodes, which calculates the importance of group of nodes via subtracting the value of the network by
removing these nodes from the value of the original network1. For example, if we adopt the network value as a

1Compared with traditional measures such as degree and betweenness centrality, the network value provides a more accurate description of

the importance of different nodes.

Additive	  Utility	  FunctionStrategy12341,23,4Benefit3939757578150Non-­‐additive	  Utility	  FunctionStrategy12341,23,4Benefit393975752381421234function f ({ni}) =(cid:80)

i n2

i , where ni is number of nodes in the ith component, the value of the original network
is 202 = 400. After removing node 3, the network will be divided into two components: one 18−node network
and one isolated node, the network value is reduced to 182 + 12 = 325. Thus the beneﬁt of node 3 is equal to
the decrement 400 − 325 = 75. Similarly, we can get the beneﬁts of other nodes as illustrated in the bottom table
of Fig 1. In traditional security game models, they assume that the beneﬁt of strategy {1, 2} and {3, 4} is equal
to 39 + 39 = 78 and 75 + 75 = 150. The mixed strategy equilibrium2 under this case is that defender choose
nodes 1, 2 with probability 0.34 and nodes 3, 4 with probability 0.66. Instead, if we adopt the true value of nodes
{1, 2} and {3, 4} (as illustrated in red of bottom table), the equilibria is that the defender chooses nodes 1, 2 with
probability 0.63 and nodes 3, 4 with probability 0.37. From the point view of the network, the second one provides
a more reliable strategy.

[10] investigated the security investment in the multi-player scenario and introduced the Interdependent Security
Game (IDS). The insight in this work is that a large number of players must make individual investment decisions
related to security, but in which the total utility of each participant may depend in a complex way on the actions
of the entire population. Further,
[6] construct a computational game model of this problem based on the linear
inﬂuential network and provides a polynomial algorithm to determine the equilibrium investment strategy for each
player.

B. Conceptual Contributions and Results

In this paper, we introduce a new security game, named as Non-additivity Security Game (NASG), which is a
normal form non-zero-sum game including two players - the defender and attacker, and n targets. We deal with
the internal linkage structure among targets via deﬁning the strategy set of each player as a family of sets and
adopt the set function as the utilities. More speciﬁcally, we use [n] = {1, 2, . . . , n} to denote all the targets and the
strategy of attacker and defender is the subset of [n]. The attacker will obtain the beneﬁts for successfully attacked
targets and pay the cost for its strategy. Correspondingly, the defender will lose the beneﬁts for those targets and
also pays the cost. A critical point in the NASG is that the beneﬁt and cost for different strategy is described by the
non-additive set function deﬁned on the power set of [n]. Namely, the utility for several targets is not the summation
of each target’s utility, instead, dependent on the speciﬁc combination of targets. Compared with traditional security
game models, the NASG provides a reﬁned description of the dependency among different targets. However, the
following are some of key challenges in being able to determine the equilibrium strategy of the NASG.

• The NASG is a large-scale security game with both exponentially increasing number of strategies and utility
functions. This is unavoidable because we need to completely describe the internal combinatorial structure
of the problem. In previous security game models such as IG and SSG, the number of utility functions is
linear in the number of targets and the difﬁculty mainly derived from the exponentially large strategy set. In

2In this example, we adopt the zero-sum game model and assume the defender can protect the nodes with probability 1.

NASG, exponentially growing parameters further complicate the problem, which is unavoidable when we want
to describe the internal combinatorial structure of the problem.

• The NASG is the non-zero-sum game under a general utility function. In the NASG setting, we do not consider
any speciﬁc form of the utility functions and deﬁne them as a system of set functions. This general assumption
of utility functions not only yields an extremely large input size when combined with their huge quantity, but
also makes designing an efﬁcient algorithm more difﬁcult.

In NASG, each player can choose any subsets of n targets, which yields N = 2n strategies and 3N utility
functions. Thus, the game size is denoted by N. For example, in the Italian communication network, the number of
targets, i.e., cities, is 25, and the number of possible actions and utility functions for each player is 225 ≈ 108, which
is substantially larger than the current record for solving the large-scale game.3 Since NASG is a non-zero-sum
game, if we use the Lemke’s algorithm to exactly solve it, in the worst case, the time complexity is up to O(2N )
and can only be applied to the cases for which n < 5.

Further, by utilizing the internal speciﬁc structure of the NASG, we show that it is equivalent to a speciﬁcally
constructed zero-sum game. There are two mainstreams in the current state-of-art to tackle the large-scale zero-sum
game. The ﬁrst technique introduced by [8] focuses on compressing the strategies based on their relationship and
solving an equivalent game under the compact representation. We show that this approach can only be applied
to the case when all the utility functions are additive in NASG. Another technique is ﬁrst introduced by [14] to
solve convex games. Then, [4] applied the oracle algorithm into the large scale urban security game and showed
a signiﬁcant speed up. The key in this algorithm is to design the efﬁcient solver for attacker and defender oracle
problem. Unfortunately, due to the large input size in NASG, solving such oracle problem in each iteration requires
at least time of O(N ). Moreover, the oracle algorithm is a heuristic algorithm without theoretical guarantee. In the
worst case, it will enumerate all the strategies, and thus is less efﬁcient than applying any linear programing into
the original problem.

The key limitation of previous algorithms is that they are only designed for an large strategy set, i.e., the oracle
algorithm incrementally adds the strategy to the current strategy set and possibly terminates before enumerating all
the strategies, the compact representation compresses the exponential number of strategies into the linear quantity
based on their relationship. However, in the NASG, due to intrinsic linkage structure among different targets, both
number of strategies and utility functions are exponentially increasing. Nonetheless, our goal for this problem is

design a fast algorithm to determine the equilibrium strategy of the NASG with complexity linear in the

game size N.

Such an algorithm—whose error is asymptotically close to 0 when the game size N goes to inﬁnity- is known
as being asymptotically optimal. Since the input size of the NASG is the number of beneﬁt and cost functions,
i.e, N, even loading the original data and outputting the optimal solution will incur O(N ) time. We will design a

3Although some previous work claims that they have solved the game under 109 strategy set, the main difference is that they use a simple

utility function that only has no more than 102 values. From the perspective of input size, the NASG is much larger.

linear time algorithm by building new concepts and leveraging recent progress in several ﬁelds, including matrix
decomposition, low-dimensional embedding, fast algebra transform and primal-dual iterative algorithm.

In the ﬁrst part of our work, we reduce the original problem to a low-rank perturbed zero-sum game based on the
decomposable property of the payoff matrix and the random low-dimensional embedding, whose payoff matrices
only exhibit logarithmic rank and asymptotic zero error. This process exhibits two advantages: ﬁrst, solving the zero-
sum game is much easier than the non-zero-sum game; second, previous work by [13] shows that the logarithmic
rank of payoff matrix means that the game has a logarithmic support and we can solve this small support game
much more efﬁciently. Further, by utilizing the structure of our decomposition, we design a nearly linear time
algorithm to implement the above low-rank approximation procedure via the fast M¨obius and Zeta transform.

In the second part, we incorporate our approximation framework into the Augmented Lagrangian and Coordinate
Descent (AL-CD) Method to determine the equilibrium strategy, which is a kind of iterative algorithm designed
for the large-scale linear programming problem. Recent progress on the analysis of this algorithm [19] shows that
it only requires O((log(1/))2) iterations to converge. The key bottleneck of the AL-CD method is the matrix-
vector multiplication, which requires the cost of O(N 2) per iteration. In our low-rank approximation framework,
we decompose the original N × N payoff matrix into the summation of one symmetric matrix and two constant
rank-1 matrices, where the symmetric matrix is equal to product of two N × log(N ) matrices. Thus, such matrix-
vector multiplication can be transformed into four much smaller multiplications and only requires the O(N log(N ))
time. Putting all these approaches together, we can get a nearly linear time algorithm to solve the NASG. The
theoretical framework built in this paper provides an additional alternative for the traditional techniques of compact
representation and the oracle algorithms in order to solve large-scale games.

A. Game-theoretical Modeling of NASG

II. PRELIMINARIES

we begin by deﬁning the NASG as a two-player normal-form non-zero-sum game.
Players: A non-additive security game contains a defender and an attacker, and n dependent targets, indexed by
set [n]. The players need not be individuals, but could also be the organizations and groups who adopt a strategy.
The target can be quite general and dependent on the application in mind. For example, they could represent links
in the communication networks, roads in urban networks and individuals in security investment.

Strategies: The pure strategy for each player is the subset of [n]. Here we consider the complete pure strategy

set, deﬁned as

Deﬁnition 1: The strategy set for attacker and defender is the power set of [n], denoted by A = {V |V ⊂ [n]}

and D = {V |V ⊂ [n]}, respectively.
Correspondingly, the mixed strategy is the probability distribution p and q over the pure strategy set A and D.
The assumption that attacker and defender can choose any group of targets to attack and defend could be true for
certain military scenarios, such as missile attacks and defense. While considering other domains, there are some
constraints in the strategy set for both player due to the limited resources. In the Section VI, we show that our

theoretical framework can be extended to this kind of subgame and still get a nearly linear time algorithm of the
number of utility functions.

Utilities: Let set function C a : A → R and C d : D → R be the attacker’s and defender’s cost function, and the
set function B : A → R be the beneﬁt function for each player. All of them are strictly monotonically increasing
and the beneﬁt function is strictly larger than the cost function under the same strategy. We adopt the following
tie-breaking rule: when the attacker and defender choose strategy A ∈ A and D ∈ D , the attacker’s and defender’s
utility is given by B(A\D) − C a(A) and −B(A\D) − C d(D), respectively4. We make the standard assumption
that all utility functions lie in the range [0, 1].

Remark 1: Instead of estimating the total beneﬁts of targets as their summation, we use the set function to
describe the beneﬁt and cost of targets. Our model is equivalent to the previous security model when we assume
that the beneﬁt function is additive and do not consider the cost for each strategy.

When the attacker and defender play the mixed strategy p ∈ P and q ∈ Q, the expected payoffs for attacker

(cid:88)
(cid:88)

A∈A

(cid:88)
(cid:88)

D∈D

p(A)q(D) [B(A\D) − C a(A)] = pM aqT

p(A)q(D)(cid:2)−B(A\D) − C d(D)(cid:3) = pM dq

T

.

(1)

(2)

and defender are

U a(p, q) =

U d(p, q) =

A∈A

D∈D

where p(A) and q(D) is the probability that attacker and defender choose pure strategy A and D under the mixed
strategy p and q, respectively. M a and M d is the payoff matrices of attacker and defender. Thus, the NASG can
be described by the bimatrix game(M a, M d), where the attacker is the row player, and the defender is the column
player.

Solution concept: If both players move simultaneously, the standard solution concept is the Nash equilibrium

(NE), in which no single player can obtain a higher payoff by deviating unilaterally from this strategy.

Deﬁnition 2: A pair of strategies (p∗, q∗) forms a NE if they satisfy the following
1) The defender plays the best response: U d(p∗, q∗) ≥ U d(p∗, q),∀q ∈ Q.
2) The attacker plays the best response: U a(p∗, q∗) ≥ U d(p, q∗),∀p ∈ P.
Correspondingly, the approximate equilibria means that both player cannot gain more than  by a unilateral

deviation, which is a relaxation of above relation via an additive error .

Based on our general assumption, NASG can be regarded as the abstract mathematical model of several security
games. For example, the computational IDS model proposed by [6] is a special case of NASG, where the beneﬁt
function is deﬁned as the linear combination of beneﬁt of each target (called individual in IDS). The communication
network security game proposed by [3] and network interdiction game [4] can be regarded as a subgame of NASG.
As shown in the sequel, NASG exhibits the property that NE set and Stackelberg Equilibrium (SSE) set are

4A\D is the standard set difference, deﬁned by A\D = {x|x ∈ A, x /∈ D} and is equal to A ∩ Dc, where Dc is the complementary set

of subset D.

equivalent. Without considering the cost functions, NASG is also an extension from previous SSG model to the
non-additive case5.

B. Properties of the NASG

As deﬁned in (1) and (2), the payoff matrices M a and M d can be written as
M a = M − Ca(A), M d = −M − Cd(D),

(3)
where M is the beneﬁt matrix that consists of all the beneﬁt functions, Ca(A) and Cd(D) is the corresponding
cost matrix. Note that the cost matrix Ca(A) is the column matrix and Cd(D) is the row matrix6.

Proposition 1: NASG is the bilinear game with rank 2.

The bilinear game has the ﬁxed rank of matrix M a + M d, which is one subclass of the general non-zero-sum
game. The zero-sum game can be categorized to the bilinear game of rank 0. It was ﬁrst introduced by Kannan
and Theobald [5], and they also developed a fully polynomial time approximation scheme (FPTAS) for ﬁxed rank
games. Based on their technique, we can get a Θ(N 4/2) algorithm for computing the -NE of NASG.

Proposition 2: NASG is equivalent to the zero-sum game with payoff matrices M − Ca(A) + Cd(D).

Proof: To prove the equivalence between non-zero-sum game (M a, M d) and zero-sum game M − Ca(A) +
Cd(D), we need to prove the NE set is equivalent between these two games. The proof is based on the property
of cost matrix and details can be seen in the Appendix VIII-A.

This proposition allows us to solve the NASG via the equivalent zero-sum game, which can be tracked by several

efﬁcient methods.

Remark 2: In fact, the NASG belongs to the strategically zero-sum game, proposed by Moulin [15]. In this kind
of game, not only is it equivalent to the zero-sum game, but the SSE7 set and NE set are equivalent, thus the
defender has no dilemma to consider how smart the attacker is and only needs to determine the NE set [9].

We deﬁne the utility function for attacker and defender in the zero-sum game as

U a(p, q) = p(cid:2)M − Ca(A) + Cd(D)(cid:3) qT , U d(p, q) = −p(cid:2)M − Ca(A) + Cd(D)(cid:3) qT .

(4)

Proposition 3: The beneﬁt matrix M is isomorphically symmetric.

Proof: Suppose that the order of strategy set for row player is given by index function σ(·) : A → N, we
deﬁne the following index fucntion of strategy set for column player: µ(U ) = σ(U c). Then considering the element
M σ(A),µ(D) of matrix M and corresponding element in the symmetric position M µ(D),σ(A), we have
M µ(D),σ(A) = M σ(Dc),µ(Ac) = B(Dc\Ac) = B(Dc ∩ A) = B(A\D) = M σ(A),µ(D),∀A, D.

5There exist four kinds of beneﬁt function in the classic SSG model. Rigorously, NASG can only be regarded as an extension when we only

consider one kind of beneﬁt functions.

6The column matrix and row matrix refers to the matrix that has the equal columns or equal rows
7In some applications, the defender is in the leader’s position and always moves ﬁrst, the corresponding standard solution concept is Strong

Stackelberg Equilibrium (SSE)

The ﬁrst equality comes from the relation between index function σ(·) and µ(·). The rest of equalities are based
on the deﬁnition of the beneﬁt matrix.

Above deﬁnition of index function for the strategy is intuitive in that each strategy for both players exhibits the
complementary relation when they are in the same location of both strategy set. For example, if n = 2, the order
of strategy set for row player is σ({1, 2}) = 1, σ({2}) = 2, σ({1}) = 3 and σ({∅}) = 4, then the order for column
player is µ({∅}) = 1, µ({1}) = 2, µ({2}) = 3 and µ({1, 2}) = 4. Based on Proposition 2 and Proposition 3, the
payoff matrix we need to solve in the NASG can be written as the summation of one symmetric matrix M, one
column matrix Ca(A) and one row matrix Cd(D).

Notation: We adopt the above default order of strategy set in the rest of the paper. We use the 2[n] to represent
the power set of [n] and do not distinguish it with strategy set A and D. M is the matrix only containing the beneﬁt
function. Ca(A) and Cd(D) is the cost matrix for attacker and defender. The M◦ = M − Ca(A) + Cd(D) is

the payoff matrix for equivalent zero-sum game. (cid:102)M is the low rank approximation matrix of beneﬁt matrix M.

1{·} is the indictor function. B(2[n]) represents all the beneﬁt function. The EN is the identity matrix, 1N (0N )
is N−dimensional vector with all elements equal to 1 (0).

Before solving the NASG generally, we ﬁrst tackle several special cases of the NASG to reveal some useful

III. SPECIAL CASE

insights.

A. Saddle Point

A pair of pure strategies (A∗, D∗) forming the NE is always referred as the saddle point and satisﬁes the following

condition,

B(A∗\D∗) − C a(A∗) ≥ B(A\D∗) − C a(A),∀A ⊂ A,
−B(A∗\D∗) − C d(D∗) ≥ −B(A∗\D) − C d(D),∀D ⊂ D.

(5)

(6)

The pure strategy A for attacker can be represented as the mixed strategies eσ(A), the unit vector with 1 in the
position σ(A) and 0 elsewhere. Similarly, the pure strategy D for defender is represented by eµ(D). Thus, if
the game contains the saddle point, we can solve NASG directly by outputting it as the mixed NE strategy. The
following theorem gives the negative result that NASG does not exhibit the saddle point property.

Theorem 1: The NASG contains no saddle point.

Proof: We prove by contradiction. Assuming the pair of pure strategies (A∗, D∗) forms the NE, we show that
the NE should satisfy A∗ ⊆ (D∗)c and D∗ ⊆ A∗, which contradicts with a standard set system. The details are
listed in Appendix VIII-B.

From the proof of Theorem 1, we can observe that the NE strategy for attacker should be disjoint with the
defender’s strategy, while for the defender, it should be a subset of attacker’s strategy. This result is intuitive under
our tie breaking rule: the defender can defend the targets within his strategy with probability 1, thus the attacker

will attempt to evade the defense while the defender aims to cover the attacked targets. In the fair setting, i.e.,
monotonicity of cost, both player cannot stay in the dominant position and there exists no saddle point.

B. Additive Solution

A simple case is to consider the additive beneﬁt and cost function. In this special case, the NASG can be
solved in Θ(polylog(N )) time based on the technique of compact representation proposed by [8]. This technique
compresses the original 2n variables (mixed strategy) into only n variables (marginal distribution) and solve the
original large-scale game via a much smaller one.

Let M◦ = M − Ca(A) + Cd(D). Then, based on the equivalent zero-sum game and max min theorem, the

NASG can be formulated as the following optimization problem,

U a(p, q) = pM◦qT ,

max
p∈Γa

N

(7)

min
q∈Γd

N

N is the N−dimensional polyhedra described by pσ(A) ≥ 0,∀A ∈ 2[n] and 1N · pT = 1. Similarly, Γd

N is

where Γa
described by qµ(D) ≥ 0,∀D ∈ 2[n] and 1N · qT = 1.

It can be further written as,

U a(p, q) =

(cid:34) (cid:88)

n(cid:88)

v=1

A:v∈A

U a(p, q) =

pσ(A)

pσ(A)qµ(D)

(cid:88)
(cid:35)

A∈A

(cid:88)
(cid:40)(cid:34)
1 − (cid:88)

D∈D

·

(cid:2)B(A\D) − C a(A) + C d(D)(cid:3) .
(cid:34) (cid:88)
(cid:35)

B(v) − C a(v)

(cid:41)

+

qµ(D)

D:v∈D

(cid:35)

qµ(D)

C d(v).

D:v∈D

Utilizing the additive condition to expand each beneﬁt and cost function, we have

The last equality derives from simple exchanging order of summation. If we deﬁne the following variables,

(8)

(9)

(cid:88)

A:v∈A

(cid:88)

D:v∈D

˜pv =

pσ(A)

and

˜qv =

qµ(D),∀v ∈ [n].

It is clearly the above summation is over all the pure strategies that contains the target v, and ˜pv is the marginal
probability that attacker chooses target v. Similar deﬁnition for ˜qv. Then we can formulate the following simpliﬁed
optimization problem,

min
˜q∈∆n

max
˜p∈∆n

U a(˜p, ˜q) =

˜pv [(1 − ˜qv)B(v) − C a(v)] + ˜qvC d(v).

(10)

n(cid:88)

v=1

Although the above model reduces the original 2n+1 variables p, q to only 2n variables ˜p and ˜q, the bottleneck in
this method is how to describe the optimizing region ∆n via a small number of constraints. In this case, the region
∆n is a n−dimensional cube and only requires n constraints s.t. ∆n = {˜p|0 ≤ ˜pv ≤ 1,∀v ∈ [n]}. This result is
not new that the previous work by [8] uses the compact representation to solve the massive SSG and the above
result of ∆n can be regarded as the combination of Theorem 1 and Theorem 2 in his work. Therefore, applying
the standard linear programming approach, the NASG under additive assumption can be solved in polynomial time
of n, and log-polynomial of game size N.

Using the compact representation to signiﬁcantly reduce the size of strategy set is widely used in the large-scale
security game. A natural question is that whether we can extend this idea to the non-additive settings. To generalize

this idea, we need a higher-level idea based on isomorphism and projection of polyhedra, and we present the
following result,

Theorem 2: If the rank(M◦) = r, the Nash Equilibrium of NASG is equivalent to the solution of the following

optimization problem,

min
q∈Πr(˜Γd
N )

max
p∈Πr(˜Γa
N )

pM◦

rqT ,

(11)

N and ˜Γd

where the ˜Γa
The operator Πr(·) is to project the N−dimension polyhedra into the ﬁrst r−coordinates. M◦
block in the row canonical form of matrix M◦.

N is the N dimensional polyhedra isomorphic to attacker and defender’s mixed strategy space.
r is the non-zero

We ﬁrst conduct the gauss elimination of the matrix M◦ to transform it into the row canonical form, which is

equivalent to left and right multiplicand by the elementary matrix E and F . Then we have

min
q∈Γd
N

max
p∈Γa
N

pM◦qT = min
q∈Γd

N

max
p∈Γa
N

pE−1EM◦F F −1qT ,

M◦

 F −1qT ,

where r is the rank of payoff matrix M◦ and M◦
If we deﬁne the invertible afﬁne projection f (p) = pE−1,∀p ∈ Γa
projected polyhedra be denoted by ˜Γa

N and ˜Γd

pE−1

r 0

max
p∈Γa
N

= min
q∈Γd
N
r is the non-zero block in the row canonical form of matrix M◦.
N , and let the

N and g(q) = F −1qT ,∀q ∈ Γd

0

0

(12)

N , we can get the following optimization model.

M◦

0

r 0

0

 qT .

min
q∈˜Γd
N

max
p∈˜Γa
N

p

(13)

Based on above invertible afﬁne projection, the polyhedra ˜Γa
N is isophormic and thus has the equivalent
optimal solution set. Similar argument for the polyhedra ˜Γd
N . Thus the optimization model (12) and (13)
is equivalent. Further, considering the fact that only the ﬁrst r elements in vector p and q have the non-zero
coefﬁcients in the objective function, the (13) can be further reduced to

N and Γa

N and Γd

(14)
where the operator Πr(·) is to project the N−dimension polyhedra into the ﬁrst r−coordinates and can be
implemented by the Fourier-Motzkin elimination.

max
p∈Πr(˜Γa
N )

min
q∈Πr(˜Γd
N )

rqT ,

pM◦

Based on the above polyhedra isophormism and projection framework, the previous compact representation is
the special case where r = n (As we will show in the next section, the rank of payoff matrices M◦ is closely
related to the non-additivity of the beneﬁt function and the rank will be n under the additive assumption) and
N into the ﬁrst n coordinates will yield n−dimensional hypercube. However, when there exists some
projecting ˜Γa
non-additive beneﬁt functions, the rank of payoff matrices will be larger than n. In this situation, several critical
problems are revealed: (1) projection of ˜Γa
N into other coordinates will yield an extremely irregular polyhedra, which
requires even more than N constraints to describe. One possible solution is to construct the −net to approximate
such polyhedra, but the computational complexity is 2O(r); (2) there exists some implemented problem of marginal

probability in the compact represented model when we consider the resource constraint [11]. Thus, the new concepts
and algorithms are required to solve the NASG.

IV. LOW-RANK PERTURBED GAME

In [13], it has been proven that (i) when the maximum difference between two payoff matrices is bounded by ,
the NE under one payoff matrix is the 2-NE under another payoff matrix; (ii) the size of the support is less than
the rank of the payoff matrix. These interesting results provide a possibility to solve the original problem via the
technique of low-rank approximation. Eckart-Young-Mirsky theorem shows that the low-rank approximation under
Frobenious norm has an analytic solution in terms of the singular value decomposition of the original matrix. While
the −NE requires a stronger notion of matrix approximation that all the difference should be less than , which
lays a challenge in the current approximation techniques.

In this section, we show that the payoff matrix M◦ can be approximated by a logarithmic-rank matrix and
the central procedure in our theoretical framework of approximation is: ﬁrst, the payoff matrix M◦ = M −
Ca(A) + Cd(D), where two cost matrices have constant rank−1. Thus, we only need to focus on the low-rank
approximation of the beneﬁt matrix M. Then, we analytically separate the beneﬁt matrix into the difference of
two positive semideﬁnite (PSD) matrices and construct the /2−approximation matrix of each PSD matrix via the
random low-dimensional embedding technique. Finally, utilizing the special structure of such approximation, we
conduct the M¨obius and zeta transform to implement it in nearly linear time.

A. Decomposition of the Beneﬁt Matrix

To this end, we ﬁrst show how to decompose the beneﬁt matrix M into the product of one diagonal matrix, one

binary matrix and its transpose. The following deﬁnition is critical in our decomposition.

Deﬁnition 3: The common utility function is deﬁned as

Bc(U ) =

(−1)|U\V |B(V ),∀U ∈ 2[n].

(15)

(cid:88)

V ⊆U

(cid:88)

V ⊆U

If we regard the beneﬁt function as a measure deﬁned on a given σ−algebra of set [n], then using the inclusion-
exclusion principle to expand each B(V ) in the summand, we can ﬁnd that Bc(A) is equivalent to measuring the
beneﬁt of intersection of targets in A. It seems like measuring the synergy effect of targets in A and we refer Bc(·)
as the common utility function. We have the following relation between the common utility function and beneﬁt
function.

Lemma 1:

B(U ) =

Bc(V ),∀U ∈ 2[n].

(16)

Proof: The proof is based on counting in another way. We ﬁrst expand each Bc(V ) based on the deﬁnition of
common utilities. Then we count the coefﬁcient in front of a speciﬁc beneﬁt function B(U ), U ∈ 2[n] and prove
this coefﬁcient is equal to 1 if and only if |U| = n. The details can be seen in the Appendix VIII-C.

From Deﬁnition 3 and Lemma 1, the beneﬁt functions and common utility functions can be constructed by each
other. In the last part of this section, we can see that the common utility function is the Mobius transform of beneﬁt
function, while the beneﬁt function is the zeta transform of common utility function. Based on Lemma 1, we have
the following decomposition in term of common utilities,

Theorem 3: The beneﬁt matrix M can be decomposed into the following product form,

M = QDQT ,

(17)

where the D is the diagonal matrix and the elements in the main diagonal is the common utility for each strategy with
each element Dσ(A),σ(A) = Bc(A). The matrix Q is the 0− 1 matrix and each element Qσ(A),µ(D) = 1{Dc ⊆ A}.
Proof: Arbitrarily choose one element (A, D) in matrix QDQT , we prove that such element is equal to

M σ(A),µ(D) based on our deﬁnition of matrix Q and D. The details are listed in Appendix VIII-D.

The diagonal matrix D is constituted by common utilities and matrix Q is binary, which seems like transforming
the original beneﬁt matrix M into the simpliﬁed form in terms of the common utilities. The key insight in this
decomposition is that, in terms of beneﬁt function, original beneﬁt matrix M is dense and contains approximately
O(N 2) non-zero terms, while in terms of common utilities, the matrix Q is sparse and only contains approximately
O(N 1.585) non-zero terms. Here we provide a simple example to visualize such decomposition procedure.

Example 2: If n = 2, the pure strategies for defender and attacker is D = {{∅},{1},{2},{1, 2}} and A =

{{1, 2},{2},{1},{∅}}. Then we can decompose the beneﬁt matrix into the following form,



0

0

0

0




M =

=

B({1, 2}) B({2}) B({1})
B({2}) B({2})
B({1})

B({1})

0

0

 ·

0



1

0

1

0

1

1

1

1

0

1 1

0 1

0 0

0 0

0
Bc({1, 2})

0

Bc({2})

0

0

0

0

Bc({1})

0

0

0

0

(18)

(19)

 = QDQT .

0

0

0

1

0

1

0

1

0

0

1

1



1

1

1

1

 ·

0

0

0

0

B. Low-rank Approximation

When all the common utility functions are larger than 0, all the diagonal elements of D contains the real square
root and the beneﬁt matrix can be further decomposed into the cholesky form W W T . In this case, we present the
following result.

Theorem 4: If the function Bc(A) ≥ 0,∀A ∈ A, there exists a perturbed matrix (cid:102)M ∈ RN×N such that
rank((cid:102)M ) = log(N )/2 and

(cid:13)(cid:13)(cid:13)(cid:102)M − M

(cid:13)(cid:13)(cid:13)∞ < ,

(20)

Since the beneﬁt matrix M is decomposed into the product of one matrix W and its transpose, each element
of M can be regarded as the inner product of two row vectors of W . Further, the matrix W and M constitutes

an inner product space, if we adopt an approximation matrix exhibiting the same structure, the problem of ﬁnding
a low-rank approximation matrix that has bounded maximum difference is equal to the problem of ﬁnding a low-
dimensional inner product space that can preserve the distance between different points. The following lemma,
known as Johnson Lindenstrauss (JL) lemma, will be our main technical tool.

Lemma 2: Let u, v be vectors in RN s.t. (cid:107)u(cid:107),(cid:107)u(cid:107) ≤ 1. Let R be the random matrix with entries i.i.d from

N (0, 1

k ). Deﬁne u(cid:48) = RT u and v(cid:48) = RT v. Then for any  > 0,

Pr[uT v −  ≤ u(cid:48)T v(cid:48) ≤ uT v + ] ≥ 1 − 4e−(2−3) k
4 .

(21)

Here we provide the sketch of the proof of Theorem 4, the details can be seen in Appendix VIII-E.

of Theorem 4: Since all the common utility functions are large than 0, the square root of matrix D, denoted
2 can be regarded as the element-wise square root of each diagonal element of D, then we have

1

by D

M = QDQT = QD

2 . Let the low-rank perturbed matrix (cid:102)M be denoted by W RRT W T , where R is a N × k

where W = QD
random matrix with each element i.i.d from N (0, 1/k). The only thing is to use the JL lemma to show the matrix

M and (cid:102)M has the bounded maximum difference when k = log(N )/2.

2 QT = W W T ,

(22)

1

1

1
2 D

As shown in the proof of Theorem 4, the rank of perturbed matrix W RRT W T has a dependency on the error
of 1/2, which incurs a large k when we require a smaller . However, due to the exponential decay of the above
tail inequality, we do not need such large k when the game size N is sufﬁciently large, i.e, N > 220. In certain
cases, we can also combine the JL lemma with some information about the distribution of beneﬁt function to derive

a more tight bounds [1]. Another insight is that the low-rank perturbed matrix (cid:102)M is the product of two N × k

matrix with k (cid:28) N, which is critical in the design of iterative algorithm.

In practice, the beneﬁt matrix is not always PSD and we cannot decompose it into the above cholesky-like form
to obtain the approximation. A simple idea is to exploit the product form QDQT of matrix M to write the diagonal
matrix D as the difference of two positive diagonal matrix. Then, the original beneﬁt matrix can be written as the
difference of two PSD matrices and we can /2−approximate each matrix respectively. There exists one technical
difﬁculty in this approach: separating the diagonal matrix will break the dual-like relationship between the common
utility and beneﬁt function and the row vector for each PSD matrix might have a norm larger than 1. In this case,
we cannot apply the Lemma 2 to get an useful tail inequality. From the proof of Lemma 2, the above problem
derives from the fact that the classic random projection technique is only for the Euclidean distance and the inner
product is roughly estimated by scaling. However, we can directly estimate the tail inequality for the inner product
and the following version comes from [12].

Lemma 3: Let u, v be vectors in RN s.t. (cid:107)u(cid:107)2 = m1,(cid:107)v(cid:107)2 = m2 and uvT = a Let R be the random matrix

with entries i.i.d from N (0, 1

k ). Deﬁne u(cid:48) = RT u and v(cid:48) = RT v. Then for any  > 0,
m1m2 + a2
(m1m2 − a2)2

Pr[uT v −  ≤ u(cid:48)T v(cid:48) ≤ uT v + ] ≥ 1 − 2 exp

− k2
2

(cid:20)

(cid:21)

.

(23)

(cid:13)(cid:13)(cid:13)(cid:102)M − M

(cid:13)(cid:13)(cid:13)∞ < ,

The Lemma 3 presents a tail inequality containing the norm of the vector, which enables us to get the following

result.

Theorem 5: There exists a perturbed matrix (cid:102)M ∈ RN×N such that rank((cid:102)M ) = Θ(log(N )) and

(24)
Proof: We write the diagonal matrix D = D+ − D−, where D+ contains the positive elements of D and
D− contains the additive inverse of negative elements of D. Then the elements of both matrices D+ and D− are
larger than zero and the matrix M can be further written as

1
2

1
2

+D

M = QD

+QT − QD
2−. Then we can use a similar procedure as in Theorem 4 to conduct
where W+ = QD
the approximation. Note that in this case we adopt the Lemma 3 as the main tool. The details are listed in
Appendix VIII-F.

+ and W− = QD

T − W−W−T .

2−D

2−QT = W+W+

(25)

1

1

1
2

1

From the proof of Theorem 5, we can see the rank of approximation matrix is dependent on the norm of vector
in matrix W+ and W−. In practice, to obtain an useful threshold, we can conduct a simple scaling of the above
formula and adopt the k ≥ ∆2 · log(N )/2 to get a good approximation, where ∆ is the maximum norm of row
vectors in matrix W+. This result means that parameter k is still equal to the logarithmic of game size N. Besides
that, it also has a dependency on the maximum norm of row vectors in matrix W+. In the Section 6, we show that
it can be diminished in the large game size.

C. Fast Implementation

The key problem in the above low-rank approximation is how to efﬁciently construct the matrix (cid:102)M. It can be
seen that matrix (cid:102)M is equal to the product of matrix W R and its transpose. Using the fastest matrix multiplication
algorithm still requires ˜O(N 2) time. Fortunately, we do not need constructing the matrix (cid:102)M. As shown in the next
i.e., calculating (cid:102)M x. Let the dimension of matrix W R denoted by N × k, where k = Θ(log(N )) based on our
As shown in Equation (70), the element in the matrix W has the form(cid:112)Bc(U ),∀U ∈ 2[n]. Therefore, we need

approximaton. Thus we only need to ﬁnd the N × k matrix W R and such a matrix-vector multiplication can be
implemented by two much smaller matrix-vector multiplications8. The main procedure is listed in the Algorithm 1.

section, the basic operation in the augmented lagarangian-coordinate descend is the matrix-vector multiplication,

to ﬁrst calculate all the common utility function. The direct calculation will requires ˜O(N 1.585) time, while it can
be achieved in time ˜O(N ) via the following fast M¨obius transform [7]: Initially, for all U ∈ 2[n],

For 1 ≤ i ≤ n, the algorithm computes Bc

(i)(U ) for all U ∈ 2[n] based on the following recurrence

Bc

(0)(U ) = B(U ).

 Bc

Bc

(i)(U ) =

(i−1)(U ), if i /∈ U
−Bc
(i−1)(U\{i}) + Bc

(i−1)(U ), if i ∈ U

(26)

(27)

8Here we just show how to quickly ﬁnd the matrix W R under the PSD case, the process can be easily extended to the non-PSD case.

Algorithm 1: Linear Time Algorithm to Construct the Matrix W R
Input: N beneﬁt functions: B(U ),∀U ∈ 2[n].
Output: The matrix C.
Bc(2[n]) = M¨obiusT(B(2[n]));
repeat

r = N dimensional vector with element i.i.d drawn from N (0, 1/k);
f (2[n]) = W σ(2[n]) · r;
f ζ(2[n]) = ZetaT(f (2[n])) then CT
l + +;

l = f ζ(2[n]);

until l > k;

Then, Bc(U ) is equal to the Bc
given by the following Lemma.

(n)(U ),∀U ∈ 2[n]. The time complexity and correctness of the above algorithm is

Lemma 4: The time complexity of M¨obius transform is ˜O(N ), and

(cid:88)

Bc

(n)(U ) =

(−1)|U\V |B(V ).

V ⊆U
Proof: In each step i, we need to calculate all the Bc

i (U ). Therefore, there are in total n· 2n additions and the
time complexity is ˜O(N ). The correctness can be proved by induction. The detailed can be found in Appendix VIII-G

(28)

Let C = W R and the rest of the part is to construct the matrix C in the nearly linear time. The following
Lemma presents some speciﬁc structure of matrix C, which enables us to jump over the matrix W to construct
C straightforwardly.

Lemma 5: Let r be the random vectors in RN with each entry i.i.d from N (0, 1

k ), then consider the lth column

of matrix C, we have

Cσ(A),l =

rσ(U c) ·(cid:112)Bc(U ),∀A ∈ A.

(cid:88)

U⊆A

The proof of Lemma 5 is based on the deﬁnition of matrix C and Equation (70). This lemma allows us to calculate

each element of matrix C via the analytical formula (29). Further, if we deﬁne the rσ(U c)
set function f (·) : 2[n] → R and the elements in lth column vector of matrix C as the set function f ζ(·) : 2[n] → R,
we ﬁnd f ζ(·) is the inverse M¨obius transform of f (·). Using a similar algorithm, called fast zeta transform [7], it
can be achieved in time ˜O(N ).

Initially, construct random vector r from N (0, 1/k). For all U ∈ 2[n],

σ(U ),l = rσ(U c) ·(cid:112)Bc(U ).

C(0)

(30)

(29)

(cid:112)Bc(U ),∀U ∈ 2[n] as the

 C(i−1)

C(i)

σ(U ),l =

σ(U ),l, if i /∈ U
C(i−1)
σ(U ),l(U\{i}, l) + C(i−1)

σ(U ),l, if i ∈ U

For 1 ≤ i ≤ n, the algorithm computes Ci(U, l) for all U ∈ 2[n] based on the following recurrence

(31)

σ(U ),l,∀U ∈ 2[n]. Repeat this process d times, we can construct the matrix C.
Then, the Cσ(U ),l is equal to the C(n)
The time complexity and correctness of the above algorithm can be proved by the similar argument of Lemma 4.
Remark 3: One critical property of Algorithm 1 is that it can be modiﬁed to the parallel algorithm. In l iteration
of Algorithm 1, we generate the random vector and use the zeta transform to obtain one column of matrix C. Since
the above random vector is i.i.d, we can implement Algorithm 1 in Θ(log(N )) processors with each cost of only
O(N ).

V. ITERATIVE ALGORITHM

In this section, we focus on how to design the fast iterative algorithm via our low-rank perturbed payoff matrix.
We utilize the augmented lagrange - coordinate descend method (AL-CD) to derive a theoretically-efﬁcient algorithm
that only requires (log(1/))2 iterations with ˜O(N ) costs per iteration.

A. Augmented Lagrange-Coordinate Descend Method

Based on the low-rank approximated zero-sum game and min-max Theorem, the NASG under the low-rank

perturbed payoff matrix can be formulated as the linear programming problem.

p(cid:103)M◦qT ⇐⇒

min

q

max

p

min u

(cid:104) (cid:103)M◦ −1N

s.t.

1N · qT = 1,

(cid:105) ·

u
q ≥ 0N .

 qT

 ≤ 0N ,

Naive implementation of above model requires solving an O(N 2) size linear programming. Since the low-rank

approximated matrix (cid:103)M◦ can be written as CCT − Ca(A) + Cd(D), we can further utilize the auxiliary variables

to represent the constraint matrix in term of N × k dimensional matrix instead of the original N × N payoff matrix.

We ﬁrst formulate the following primal linear programming model,

min

x1,x2,q,v

v

s.t.

(cid:105) ·



qT

T

T

x1

x2

v

N

0N×N EN C −1T

(cid:104)
Ca(A) − Cd(D) EN

CT

1N

0k×k
0k×N −Ek
0k
0N

N ,

 ≤ 0T

 ·

0

0

0

qT

T

T

x1

x2

v

 =

 ,

0T

N
0T
k

1

q ≥ 0N .

Correspondingly, the dual linear programming model is

(cid:105) ·

0N×N
EN

 ≤ 0N ,

CT
−1N
Ca(A) − Cd(D)




EN
0T
k×N

0N

min

y1,y2,p,u

u

(cid:104)

s.t.

p y1 y2 u

(cid:104)

(cid:105) ·

p y1 y2 u

p ≥ 0N .

 =

(cid:104)

(cid:105)

,

0N 0k

1

C

1T
N
0N×k 0T
N
−Ek
0T
k
0k

0

(32)

(33)

Let the inequality constraint matrix of the primal problem be denoted by H, and the equality constraint matrix be
denoted by L. Due to our formulation, the matrix H consists of the identity matrix, zero matrix and low dimension
matrix C, thus it can be efﬁciently tackled in any linear programming algorithm. The main obstacle is the equality
constraint matrix L, which has the dimension of (N + k + 1) × (2N + k + 1). Utilizing traditional methods such
as simplex and interior point method, we need to solve a linear system involving matrix L, which requires at least
time of ˜O(N 2). However, in the AL-CD method, we only need to compute the matrix-vector product Lx. It can
be implemented in nearly linear time because the block Ca(A) − Cd(D) is the difference betweenrow matrix and
column matrix such that the matrix-vector multiplication only requires 2N ﬂops.

Let auxiliary variables [x1, x2] denoted by vector x, [y1, y2] denoted by vector y, and objective function of
primal model denoted by cT · [q, x, v], where c = [02N +k, 1]. The primal AL method can be regarded as the dual

proximal point algorithm that for each iteration t, solves

The dual of (34) takes the form:

min
q,x,v,λ

v +

ηt
2

[pt+1, yt+1, ut+1] = arg min
p,y,u

u +

(cid:107)[p, y, u] − [pt, yt, ut](cid:107)2.

1
2ηt

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
 H · [q, x, v]T + λT

L · [q, x, v]T − [0N , 0k, 1]T

 +

(cid:104)

1
ηt

(cid:105)T

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

pt yt ut

s.t.

q ≥ 0N , λ ≥ 0N .

(34)

(35)

It is a bound-constraint quadratic optimization problem. Given (q, x, v, λ) as the Lagrangian Multipliers of (34),
the corresponding p, y, v minimizing Lagrangian is

(cid:104)

(cid:105)T

p y u

= ηt

 H · [q, x, v]T + λT

L · [q, x, v]T − [0N , 0k, 1]T

 +

(cid:104)

ptT ytT

ut

,

(36)

and one can obtain (q∗, x∗, v∗, λ
listed in Algorithm 2.

∗

) from (35), and ﬁnd (pt+1, yt+1, ut+1) through (36). The resulting algorithm is

Given the dual solution [pt, yt, ut], we can adopt the Randomized Coordinate Descent (RCD) method to solve
subproblem (35). Note that the Lagrangian Multipliers λ under given [q, x, v] can be minimized via the following
close-form expression.

where [u]+ truncates each element of vector u to be non-negative. Then problem (35) can be formulated as

(cid:105)T

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

λ(q, x, v)T = [−H · [q, x, v]T − pt/ηk]+,

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)


[H · [q, x, v]T + pt/ηt]+

L · [q, x, v]T − [0N , 0k, 1]T + [y, u]T /ηt

min
q≥0,x,v

F (q, x, v) = v +

ηt
2

The gradient of (38) can be expressed as

where

and the Hessian is

∇F (q, x, v) = [0N , 0k, 1]T + ηtH T [w1]+ + ηtLT w2,

w1 = H · [q, x, v]T + ptT
w2 = L · [q, x, v]T − [0N , 0k, 1]T + [yk, u]T /ηt

/ηt,

∇2F (q, x, v) = ηtH T T H + ηtLT L,

where T is a diagonal matrix with T ii = 1 if xi > 0,∀i.

(37)

(38)

(39)

(40)

(41)

(42)

In each iteration l of RCD, it picks a coordinate jl uniformly at random and minimizes with respect to that
coordinate. The minimization is operated via a single step Newton step, which ﬁrst determines the Newton direction
d∗
jl through minimizing the quadratic second-order approximation. In this case, it has the following close-from
expression.

d∗

jl

=

qt,l
jl

− ∇jl F (qt,l, xt,l, vt,l)/∇2

jl

F (qt,l, xt,l, vt,l)

− qt,l

jl

,∀jl.

+

(43)

(cid:104)

(cid:105)

Algorithm 2: Augmented Lagrangian Method

Initialize [p0, y0, u0] and η0 > 0.;
repeat

(cid:104)

(cid:105)

Solve (35) to obtain (qt+1, xt+1, vt+1, λt+1) from [pt, yt, ut];
Update

based on formula (36);

p y u

k=k+1 and increase parameter ηt by a constant factor if necessary;

until (cid:107) max(cid:8)0, H · [q, x, v]T(cid:9)(cid:107)∞ ≤  and (cid:107)[q · C − x, 1N · qT − 1](cid:107)∞ ≤ ;

and then conduct a line search to ﬁnd the smallest s ∈ {1, 2, . . .} such that

ejl ) − F ([qt,l, xt,l, vt,l]) ≤ σαs(cid:2)∇jl F ([qt,l, xt,l, vt,l])d∗

(cid:3) .

(44)

jl

F ([qt,l, xt,l, vt,l] + αsd∗

jl

The overall complexity consists of two parts: the time complexity per iteration and the iteration complexity. The
iteration complexity is based on the global linear rate of convergence, which typically relies on the strong-convexity
of the objective function. However, the objective function (35) is clearly not strong convex. Fortunately, existing
recent work by [17], [19] conduct the extensive analysis and shows that we can still get a global convergence rate.
We list this results for completeness.

Theorem 6: To guarantee the solution produced by AL-CD method has the bounded error such that (cid:107)[p, v] −

[p∗, v∗](cid:107) ≤  , the iteration of Algorithm 2 is at least O(N (log(1/))2).
Therefore, to guarantee a total complexity is O(N k(log(1/))2), the iteration complexity per coordinate update
should be O(k). We show this objective can implemented via utilizing the dynamic programming approach. The ma-
jor cost in each iteration is the calculation of the jlth gradient ∇jl F (qt,l, xt,l, vt,l), jlth Hessian ∇2
j F (qt,l, xt,l, vt,l)
and the line search (44).

The naive calculation of the jth gradient and Hessian based on formula (39) and (42) will require the time of
O(N k). Another clever implementation is to maintain the relation(39) and (42) as follows whenever a coordinate
is updated by

wt,l+1

wt,l+1

1

2

 =

wt,l

1
wt,l
2

 + αsd∗

jl

H jl

 ,

Ljl

where H jl and Ljl is the jlth column of matrix H and L. The complexity of updating vector w1 is at most O(k)
since the inequality constraint matrix H only consists of identity matrix, zero matrix and low dimension matrix C.
While the obstacle is that the complexity of updating vector w2 is O(N ) when the coordinate jl falls into the block
[Ca(A)T − Cd(D)T , C, 1T
N ]T . We show in the sequel this problem can be tackled by the dynamic programming.

The gradient and hessian can be calculated as

∇jl F (qt,l, xt,l, vt,l) = cjl + ηt(cid:104)H jl , [wt,l

∇2

jl

F (qt,l, xt,l, vt,l) = ηt

 (cid:88)

1 ]+(cid:105) + ηt(cid:104)Ljl , wt,l
2 (cid:105),
(cid:88)

 .

H 2

i,jl

+

L2

i,jl

i:w1 i>0

i

(45)

(46)

(47)

L2

i,jl

=

Ca

i,jl

(A) − Cd

i,jl

(D)

(cid:88)

i

(cid:104)

(cid:88)
(cid:88)

i

i

(cid:105)2
(D) ·(cid:88)

i

Ca

i,jl

(A)2 − 2Cd

i,jl

Ca

i,jl

(A) + Cd

i,jl

(D)2

(48)

The term (cid:80)
(A)2 and (cid:80)
O(N ). Then, the value of each (cid:80)

Ca

=

i

i,jl

Ca

(A) is constant for all 1 ≤ jl ≤ N, which can be pre-processed with time
i,jl
i L2
i,jl can be determined in O(1). Secondly, when N < j < N + k + 1, the
calculation is O(1) since the column Ljl is all zero vector or the vector only has one entry 1. Thus, we can calculate
the hessian (47) in O(1) time.

i

Although above simple formula provides an efﬁcient approach to calculate the gradient and Hessian, it still
2 (cid:105) when the coordinate 1 ≤ jl ≤ N (When 1 ≤ jl ≤ N, we need
requires O(N ) time to calculate term (cid:104)Ljl , wt,l
to pick the column vector from the block [Ca(A)T − Cd(D)T , C, 1T
N ]T , which consists of N + k + 1 non-zero
elements and such dot product requires O(N + k + 1) time). As we discussed, the rest task is to design an efﬁcient
algorithm to update w2 and calculate the term (cid:104)Ljl , wt,l
2 (cid:105). We show the following simple dynamic programming
algorithm, which only requires O(1) time and additional O(1) space to implement.

Assuming the coordinate 1 ≤ jl ≤ N and 1 ≤ jl−1 ≤ N, then we have

(cid:104)Ljl , wt,l

2

2 (cid:105) = (cid:104)Ljl , wt,l−1
= (cid:104)Ljl , wt,l−1
= αsd∗

2

jl−1

+ αsd∗
jl−1
(cid:105) + αsd∗

Ljl−1(cid:105)
(cid:104)Ljl , Ljl−1(cid:105)

jl−1

(cid:104)Ljl , Ljl−1(cid:105) + (cid:104)Ljl − Ljl−1, wt,l−1

2

(cid:105)

(49)

Ca

i,jl

(A) − Cd

i,jl

(D)

2

(cid:105) + (cid:104)Ljl−1, wt,l−1
(cid:105)

(D)

i,jl−1

(A) − Cd

(cid:88)

i

(cid:105) ·(cid:104)
(cid:105)(cid:88)

Ca

(D)

i

i,jl−1

(A)2 − αsd∗

jl−1

Cd

i,jl

(D) + Cd

i,jl−1

Ca

i,jl−1

(A) + αsd∗

jl−1

(A)2

Ca

i,jl

(50)

(A)2 are constant for any jl. Thus above formula can be calculated in O(1)

(cid:104)

(cid:88)

i

(cid:104)

The ﬁrst term can be written as
(cid:104)Ljl , Ljl−1(cid:105) = αsd∗
(cid:88)

αsd∗

jl−1

jl−1

= αsd∗

The terms(cid:80)

jl−1

i
Ca

i,jl

i

i,jl

Ca

(A)2 and(cid:80)

Ca

i,jl

i

time. The second term can be written as

1 ]+(cid:105) and (cid:80)
on the structure of matrix H. The calculation of term(cid:80)

For each coordinate jl, the term (cid:104)H jl , [wt,l

L2

i:w1 i>0

H 2

i,jl can be calculated in time at most O(k) based
i,jl is based on the following rule: for the ﬁrst summand,

we can exploit the block structure of matrix L and tackle it in the following two cases: ﬁrst, 1 ≤ jl ≤ N,

i

(cid:104)Ljl − Ljl−1, wt,l−1

2

(cid:105) =

=

=

(cid:88)
(Li,jl − Li,jl−1)w2
(cid:104)
(cid:88)
(cid:104)

(D) − Cd

Cd

i,jl

(D) − Cd

Cd

i

i

i,jl−1

i,jl

(D)

i,jl−1

t,l−1
i

(cid:105)
(cid:105)(cid:88)

(D)

i

t,l−1
i

w2

t,l−1
i

w2

(51)

The last equality is based on the column property of cost matrix Cd(D). Then if we use additional O(1) space
(cid:105), the second term and third term can be calculated in the O(1) time.

and (cid:104)Ljl−1, wt,l−1

w2

t,l−1
i

2

to store (cid:80)

i

2 (cid:105) can be implemented by above dynamic programming approach with O(1) time. Further, it can
Thus, (cid:104)Ljl , wt,l
(cid:80)
be observed that actually we do not need to update the vector w2 and only require maintaining the summation

i , which can be implemented with O(1) time by the following rule

w2

t,l

i

(cid:88)

i

(cid:88)

(cid:88)

i

t,l−1
i

+

(A) − Cd

(D).

i

w2

t,l
i =

w2

Ca

i,jl−1

(52)
The case when 1 ≤ jl ≤ N and N + 1 ≤ jl−1 ≤ 2N + k + 1 can be tackled by the similar way. We can expand
recursive formula (49) until the iteration l(cid:48) such that 1 ≤ jl(cid:48) ≤ N. Each additional expansion term can be calculated
in O(1) time (vector Lj has at most 1 non-zero term). Besides that, the complexity of line search mainly derives
from the calculation of objective function, which can be tackled in O(1) time via the similar dynamic programming
approach.

i,jl−1

Based on above dynamic programming approach, the overall complexity of AL-CD method is O(N k(log(1/))2).
Since the complexity of previous low-rank approximation is O(N k2), putting these mechanisms together, the
complexity of the overall procedure is O(N k2) + O(N k(log(1/))2) = ˜O(N ).

By incorporating our low-rank approximation framework, this algorithm exhibits several advantageous: 1. there
exists no asymptotic ill-conditionedness phenomenon, which is a common problem in the interior point method;
2. the optimal solution under our low rank approximation matrix has logarithmic support, thus most primal (dual)
variables become binding at zero as the iterates approach the optimal solution, which potentially speed up the
algorithms.

VI. SUBGAMES

In Section 2, we have mentioned that not all the targets can be attacked and defended because of the resource
constraints. For example, in the classic stackelberg security game model, the defender has a limited number of
resource schedules, each of which can only cover some targets [2]. In the urban network security game, the
defender can deploy at most 5 check points in 190 links and attacker only choose one source-destination path. In
the colonel blotto game, both player have the limited budget to distribute and at most choose a portion of n targets.
In other applications, the beneﬁt function exhibits the bounded non-additivity. This concept is a little subtle and
the rigorous deﬁnition is given in the sequel.

In this part, we model above discussed phenomenon as two kinds of restricted NASG: ﬁrst, the strategy space is
bounded while the beneﬁt and cost function is arbitrary; second, the strategy space is complete while the beneﬁt
and cost function has the bounded non-additivity.

A. Bounded Strategy set

Here we only consider the case that the attacker and defender’s strategy is bounded by a cardinality constraint,
which means that the attacker and defender cannot choose an extremely large strategy. This assumption is realistic
in most cases and corresponds to the SSAS property as discussed in [9]. As show in the sequel, this assumption

can be straightforwardly extended to the arbitrary cardinality constraint, i.e., the size of strategy is ﬁxed, larger than
a threshold or bounded in an interval.

Let the attacker and defender’s strategy set is denoted by Ap = {U||U| ≤ ka,∀U ∈ 2[n]} and Dp = {U||U| ≤
kb,∀U ∈ 2[n]}, the number of strategies for attacker and defender is Na and Nb. The beneﬁt matrix M b under
bounded strategy set is an Na × Nb rectangular matrix and equivalent to a sub-matrix of M. Based on previous
results, the original beneﬁt matrix M can be written as9 BBT . Therefore, the sub-matrix M b is equal to the
product of one Na × N sub-matrix and one N × Nb sub-matrix of B. In fact, due to the property of matrix B and
the cardinality constraint, the dimension of above two sub-matrix can be reduced to Na × Na sub-matrix Ba and
Na × Nb sub-matrix Bd.

Example 3: The setup is same as Example 2, the cardinality constraint is ka = kb = 1, which means both player

at most attack and defend one target. Then we can decompose the beneﬁt matrix into the following form,



B({1})

0

0

(53)

(54)

0

0

0

B({1})

B({2}) B({2})
0 Bc({2}) 1
Bc({2}) 1

0

0

0

0

0

2

2

0

M b =

2

2

0

2

0

0

2

2

0

0

0

0

0

=

Bc({1}) 1

Bc({2}) 1

Bc({1, 2}) 1
Bc({2}) 1
Bc({1}) 1



 ·
Bc({2}) 1
 ·
 = W aW d.
Theorem 7: There exists a perturbed matrix (cid:102)M b ∈ RNa×Nb such that rank((cid:102)M b) = Θ(log(Na)) and
(cid:13)(cid:13)(cid:13)∞ < ,
(cid:13)(cid:13)(cid:13)(cid:102)M b − M b
Further, low-rank perturbed matrix (cid:102)M b = W aRbRT

Therefore, based on Lemma 3 and above decomposition, we can get the following results,

0
2 Bc({2}) 1

b W d and matrix W aRb and RT

Bc({1}) 1

Bc({1}) 1

Bc({1}) 1

Bc({1}) 1

=

0

0

0

0

0

0

0

0

0

0

2

2

0

0

2

0

2

2

b W d can be found in nearly

linear time ˜O(Na) and ˜O(Nb), respectively.

The proof is similar to that of Theorem 5. Note that in this case, we randomly projected the Na-dimension vectors
into the log(Na) dimension. The fast implementation of above approximation is based on the partial M¨obius
transform, which only calculates the common utilities of available strategy. Then we can conduct the AL-CD
algorithm to solve such low-rank perturbed game. The overall complexity of resulting algorithm is still linear of
the game size Na and Nb.

9Here we only consider the PSD case, the result for non-PSD case is similar.

B. Bounded Non-additivity

In some applications, the beneﬁt function only exhibits the non-additivity when the size of the strategy is less
than a threshold. Considering the attack and defense of the nodes in a network, the attacker attempts to destroy
the connectives of the network while the defender aims to prevent it. One intuitive metric for the beneﬁt is the
number of adjacent links of targets. It can be easily checked that the common utilities for the strategies with size
larger than 2 is equal to 0. While in other cases, it exhibits the non-additivity when the size of strategy is larger
than a threshold. For example, in the colonel blotto game, Roberson has proposed a kind of piece-wise successful
function, in which the beneﬁt function is linear and only when the number of attacked targets exceed a critical
value, the attacker will get a bonus.

Under the second cases, the deﬁnition of bounded-additivity is trivial, i.e., B(X) = (cid:80)
B(X) (cid:54)=(cid:80)

v∈X B(v) if |X| < k;
v∈X B(v), otherwise. But this deﬁnition cannot be adapted to the ﬁrst case, because when the small
strategy is non-additive, the large strategy containing this small strategy will not be additive. The essential point is
how to make a systematic deﬁnition for both cases. For this reason, the additivity cannot be easily characterized by,
for instance, the linear summation form. Instead, one is led to the conclusion that common utilities is an appropriate
notion with which to describe the additivity. Thus, we have the following deﬁnition for the ﬁrst case,

Deﬁnition 4: The beneﬁt has the bounded non-additivity iff there exists constant kc s.t.

The deﬁnition for the second case is similar. Let Nb =(cid:80)kc

Bc(X) = 0,∀X ∈ 2[n],|X| > kc.

(cid:0)n

(cid:1). It is clearly that the rank of beneﬁt matrix is

(55)

upper bounded by Nb and there exists (N − Nb) columns of matrix B is all zero vector. In this case, the complexity
of constructing approximate payoff matrix can be reduced from ˜(N ) to ˜(Nb) since the matrix B is sparse and the
maximum number of non-zero elements in each row is bounded by Nb. While the theoretical complexity of AL-CD
algorithm is still ˜(N ). We have shown that, under bounded strategy set, the complexity can be reduced to the linear
size of Na and Nd, one open problem under the bounded-additivity is although the strategy set is complete, whether
there exists a algorithm with linear complexity of Nb.

i=1

i

We ﬁrst provide some simulation results of the low-rank approximation as a sanity check. The following four

VII. NUMERICAL RESULTS

• Attack defense game in the Italian communication network. The beneﬁt function is generated based on the

• Attack defense game in the Erd¨os-Renyi network G(n, p) with p = 0.3. The beneﬁt function is generated via

types of data were used in our evaluation.

network value f ({ni}) =(cid:80)
• Sub-additive beneﬁt function with formula: B(X) =(cid:2)(cid:80)
• Super-additive beneﬁt function with formula: B(X) =(cid:2)(cid:80)

the same network value function.

i ni log(ni) [3].

v∈X B(v)(cid:3) · |X|κ.
v∈X B(v)(cid:3) /|X|κ.

We ran the simulation under two cases: small game size N = 212 = 4096 and large game size N = 216 = 65536.
The parameter κ is set to 0.1 to guarantee the monotonicity of the beneﬁt function. The rank of original matrix is
equal to N. As illustrated in Fig. 2, the results agree with our theoretical analysis well: (1) The error exhibits an
exponential decay with the dimension k; (2) The approximation in the large game size performs much better than the
small game size. (3) When the beneﬁt function is all sub-additive or super-additive, a very small k (approximately
1% of the original rank) is sufﬁcient. (4) The approximation error in the real-world trace is signiﬁcantly smaller
than in the random network. The reason is that the maximum norm ∆ in the random network is larger than other
cases, and only can be diminished when N is large.

Fig. 2. Random projection applied to four types of beneﬁt function.

Moreover, we compared our modiﬁed AL-CD algorithm (called as L-AL-CD) with the state-of-the-art implemen-
tation of interior point and dual simplex methods and original AL-CD algorithm. For all experiments, the stopping
criterion is set to require the both primal and dual infeasibility smaller than 10−3 and set the initial sub-problem
tolerance t = 10−2 and ηt = 1. The following table has been generated using the data of Italian communication
network.

TIMING RESULTS (IN SEC UNLESS SPECIFIED OTHERWISE)

TABLE I

game size

Interior-point
D-Simplex

AL-CD
L-AL-CD

N = 210, k = 100 N = 212, k = 200 N = 214, k = 400 N = 216, k = 750

53.42

22.28

5.36

3.96

7942.7

6259.2

184.54

61.89

> 16hour
> 16hour

10258

700.99

out of memory
out of memory
out of memory

9229.8

We observed across all experiments conducted that the proposed L-AL-CD algorithms can be orders of magnitude
faster than other methods when the the game size is larger than 212. In addition, due to our low-rank approximation,
the L-AL-CD method requires a much smaller storage space. For example, when N = 216, k = 750, the input size
of L-AL-CD is only 100MB, while running traditional methods such as dual simplex, interior point and AL-CD

1000200030000.00.51.01.5 Sub additive Super additive Italian network ER networkerrork, N=2161002003004005000.00.51.01.5  errork, N=212 Subadditive Superadditive Italian Network ER Networkmethod, is out of memory in our test platform10.

VIII. DISCUSSION

This paper introduced a new security game model, called Non-additive Security Game, and designed an algorithm
to solve it in nearly linear time in terms of the number of utility functions. The low-rank approximation framework
built in this paper provides an alternate way to design the large-scale game solver. Here we discuss about several
extensions based on the insights we obtain from the NASG.

Extension to Stackelberg Security Game: In the Stackelberg Security Game, there are four kinds of utility
functions, i.e., the beneﬁt and loss function for attacker and defender. If we again consider non-additive condition
for above utility functions, the structure of the payoff matrix will be changed. In this case, there exists two beneﬁt
matrices and two loss matrices, in which all the matrices are isomorphically symmetric but under different deﬁnitions
of index function. Therefore, the low-rank perturbation in this case will become more complicated and one open
problem is whether one can solve Non-additive Stackelberg Security Game efﬁciently.

Extension to Coalition Games (CG): The coalition game consists of a ﬁnite set of n players and a characteristic
function deﬁned on the power set of n players, which describes how much collective payoff a set of players can
gain by forming a coalition. One problem in CG is to determine if the core is non-empty, which is equivalent to
determining the feasibility of a linear programming with 2n + 1 constraints and n variables. It can be observed that
NASG and CG shares the same assumption of the utility functions (2n set functions). We hope that the techniques
introduced in this paper, i.e., matrix decomposition and fast algebra transform, shed light on tackling large input
size incurred by such set functions.

A. Proof of Proposition 2

APPENDIX

Proof: First, assume the mixed strategies ((p∗, q∗)) is the NE point of game (M a, M d). Based on the

deﬁnition of NE point, we can get

p∗ [M − Ca(A)] q∗T ≥ p [M − Ca(A)] q∗T ,∀p ∈ P,

p∗(cid:2)−M − Cd(D)(cid:3) q∗T ≥ p∗(cid:2)−M − Cd(D)(cid:3) qT ,∀q ∈ Q.

(57)
Since the matrix Cd(D) is the row matrix and Ca(A) is the column matrix, we have p∗Cd(D) = pCd(D),∀p ∈
P and Ca(A)q∗T = Ca(A)qT ,∀q ∈ Q. Then, we can get p∗Cd(D)q∗T = pCd(D)q∗T ,∀p ∈ P and
p∗Ca(A)q∗T = p∗Ca(A)qT ,∀q ∈ Q. Hence, the conditions (56) and (57) are equivalent to

p∗(cid:2)M − Ca(A) + Cd(D)(cid:3) q∗T ≥ p(cid:2)M − Ca(A) + Cd(D)(cid:3) q∗T ,∀p ∈ P,
p∗(cid:2)−M + Ca(A) − Cd(D)(cid:3) q∗T ≥ p∗(cid:2)−M + Ca(A) − Cd(D)(cid:3) qT ,∀q ∈ Q.

(56)

(58)

(59)

10we run it in a node with 12 Xeon X5650 CPUs and 128GB memory.

From conditions (58) and (59), we can ﬁnd that mixed strategies ((p∗, q∗)) is the NE point of zero sum game
M − Ca(A) + Cd(D), which implies the NE set of NASG belongs to the the NE set of this zero-sum game.
Similarly, we can obtain the other direction and Proposition follows.

B. Proof of Theorem 1

Proof: Assume the pair of pure strategies (A∗, D∗) forms the NE. Let A = A∗ ∩ (D∗)c. If A∗ ∩ D∗ (cid:54)= ∅,
then there exists v ∈ [n] s.t. v ∈ A∗ and v ∈ D∗, which implies v ∈ A∗ and v /∈ (D∗)c. Then, we can get
A = A∗ ∩ (D∗)c ⊂ A∗ and

B(A\D∗) − C a(A) = B([A∗ ∩ (D∗)c]\D∗) − C a(A∗ ∩ (D∗)c)

= B([A∗ ∩ (D∗)c] ∩ (D∗)c) − C a(A∗ ∩ (D∗)c)
= B(A∗ ∩ (D∗)c) − C a(A∗ ∩ (D∗)c)
> B(A∗\D∗) − C a(A∗).

(60)

The last inequality is due to the strict monotonicity of cost function C a. The above result shows that attacker can
get higher payoff via strategy A, which violates the NE condition of (A∗, D∗), thus A∗ ∩ D∗ = ∅ and we can get
the NE should satisfy A∗ ⊆ (D∗)c.

Let D = A∗ ∩ D∗ and use the similar argument, we can prove −B(A∗\D)− C d(D) > −B(A∗\D∗)− C d(D∗)
if D∗ ∩ (A∗)c (cid:54)= ∅. Thus, we have D∗ ∩ (A∗)c = ∅, the NE should satisfy D∗ ⊆ A∗. Combining this two results,
we can get

which is impossible for a standard set system. Thus, theorem follows.

D∗ ⊆ A∗ ⊆ (D∗)c,

(61)

(62)

(63)

(64)

C. Proof of Lemma 1

Proof:

(cid:88)

V ⊆U

Bc(V ) =

=

=

=

i=0

|V |=i

j=i

= B(U ).

V ⊆U

(cid:88)
(cid:88)
|U|(cid:88)
|U|(cid:88)

i=0

V ⊆U

W⊆V

(−1)|V \W|B(W )

(cid:88)
|V |(cid:88)
(−1)|V |−j · (cid:88)
i(cid:88)
(−1)i−j · (cid:88)
(cid:88)
 |U|(cid:88)
(cid:18)|U| − j
(cid:88)

(−1)i−j

W⊂V,|W|=j

(cid:19)

j=0

V ⊆U,|V |=i

j=0

i − j

B(W )

W⊂V,|W|=j

B(W )

 · B(W ).

As seen in the Equation (62), we ﬁrst traverse all the subsets of U and then for each subset V , traverse its subset
W . Thus, there are multiple copies for a speciﬁc B(W ) of Equation (62). The Equation (63) derives from counting
the coefﬁcient of B(W ) in a backward way. We ﬁrst ﬁx set W and suppose |W| = i, and the number of set V

with |V | = j containing W as the subset is (cid:0)|U|−i
|U|(cid:88)

(cid:1). The corresponding coefﬁcient of B(W ) is (−1)j−i. Then
(cid:18)|U| − i

traversing all the possible set V , i.e., traverse at the order of cardinality, we have the coefﬁcient of B(W ) is

(cid:19)

j−i

j − i

j=i

(−1)j−i.

The above coefﬁcient is equal to 1 iff i = |U| and 0 otherwise, thus the lemma follows.

D. Proof of Theorem 3

Proof: Let’s ﬁx A ∈ A and D ∈ D, then the element (σ(A), µ(D)) of matrix QDQT is equal to

Qσ(A)DQT

µ(D).

Let vector T = Qσ(A)D, Based on the deﬁnition of matrix Q and D, the µ(U )th coordinate of vector T is

(cid:88)
(cid:88)

V ∈2[n]

T µ(U ) =

=

(cid:88)

V ∈2[n]

Qσ(A),µ(V ) · Dµ(V ),µ(U ) =

Qσ(A),µ(V ) · Dσ(V c),µ(U )

1{V c ⊆ A} · 1{V = U} · Bc(U c) = 1{U c ⊆ A} · Bc(U c).

(65)

(66)

(67)

(68)

Then, we have

V ∈2[n]

Qσ(A)DQT

µ(D) =

=

=

=

U∈2[n]

(cid:88)
(cid:88)
(cid:88)
(cid:88)

U∈2[n]

U∈2[n]

Bc(U ).

U∈A∩Dc

T µ(U ) · QT

µ(U ),µ(D) =

T µ(U ) · QT

σ(U c),µ(D)

T µ(U ) · Qµ(D),σ(U c) =

T µ(U ) · Qσ(Dc),µ(U )

(cid:88)
(cid:88)

U∈2[n]

U∈2[n]

1{U c ⊆ A} · 1{U c ⊆ Dc} · Bc(U c)

The above substitution of σ(·) and µ(·) is based on the deﬁnition that σ(U ) = µ(V ) iff U = V c. Applying
Lemma 1, we conclude that Equation (68) is equal to B(A ∩ Dc). Since A and D is arbitrary chosen, based on
the deﬁnition of the beneﬁt matrix, the theorem follows.

E. Proof of Theorem 4

Proof: Since all the common utility functions are large than 0, the square root of matrix D, denoted by D

1
2

can be regarded as the element-wise square root of each diagonal element of D, then we have

M = QDQT = QD

1

2 D

1

2 QT = W W T ,

(69)

1

where W = QD

2 . Let the low-rank perturbed matrix (cid:102)M be denoted by W RRT W T , then we have
(cid:12)(cid:12)(cid:12) < 
(cid:104)(cid:13)(cid:13)(cid:13)W RRT W T − W W T(cid:13)(cid:13)(cid:13)∞ < 
(cid:105)
(cid:105)

µ(D) − W σ(A)W T

where the element in the position (σ(A), µ(D)) achieves the maximum difference between M and (cid:102)M. Based on

= Pr

µ(D)

Pr

the deﬁnition of matrix W , we have

W σ(A),µ(U ) =

Qσ(A),µ(V ) · D

1
2
σ(V c),µ(U )

1
2

=

V ∈2[n]

µ(V ),µ(U ) =

Qσ(A),µ(V ) · D

(cid:104)(cid:12)(cid:12)(cid:12)W σ(A)RRT W T
(cid:88)
1{V c ⊆ A} · 1{V = U} ·(cid:112)Bc(U c)

(cid:88)
(cid:88)
= 1{U c ⊆ A} ·(cid:112)Bc(U c),∀U ∈ 2[n],
(cid:88)
(cid:88)
U c∈A
U∈2[n]
(cid:105) ≥ 1 − 4e−(2−3) k
µ(D)(cid:107)2 ≤ 1. Thus, we can apply the Lemma 2 and obtain

(cid:104)(cid:13)(cid:13)(cid:13)BRRT BT − BBT(cid:13)(cid:13)(cid:13)∞

1{U c ⊆ A} · Bc(U c) =

σ(A),µ(U ) =

(cid:88)

V ∈2[n]

V ∈2[n]

U∈2[n]

B2

Pr

4 .

and the norm of vector W σ(A) satisﬁes

(cid:107)W σ(A)(cid:107)2 =

Similarly, we have (cid:107)W T

(70)

Bc(U c) = B(A) ≤ 1.

(71)

(72)

where matrix R is the random matrix with entries i.i.d from N (0, 1

k ). Let k = log(N )/2, the theorem follows.

F. Proof of Theorem 5

Proof: We write the diagonal matrix D = D+ − D−, where D+ contains the positive elements of D and
D− contains the additive inverse of negative elements of D. Then the elements of both matrices D+ and D− are
larger than zero and the matrix M can be further written as

M = QD

1
2

+D

1

1

1
2

+QT − QD
2−.. Let R1 be the random matrix with entries i.i.d from N (0, 1

T − W−W−T .

2−QT = W+W+

2−D

1

(73)

k ) and apply

where W+ = QD
the Lemma 3, we have

1
2

+ and W− = QD

event ε1 :

T − W+W+

1 W+

(cid:13)(cid:13)(cid:13)W+R1RT
(cid:40)

T(cid:13)(cid:13)(cid:13)∞ <


2

,

(cid:41)

(74)

with Pr[ε1] ≥ 1 − 2 exp

(75)
where σ(A)th row and µ(D)th column of matrix W+ that achieves the inﬁnity norm, mσ(A) = (cid:107)W+σ(A)(cid:107)2,
mµ(D) = (cid:107)W+µ(D)(cid:107)2 and aσ(A),µ(D) = (cid:107)Bσ(A)BT
Using the similar argument, we have

µ(D)(cid:107).

σ(A),µ(D)
σ(A),µ(D)]2

.

− k2
8

mσ(A)mµ(D) + a2
[mσ(A)mµ(D) − a2

event ε2 :

with Pr[ε2] ≥ 1 − 2 exp

(cid:13)(cid:13)(cid:13)W−R2RT
(cid:40)

2 W−T − W−W−T(cid:13)(cid:13)(cid:13)∞ <


2

,

− k2
8

mσ(A(cid:48))mµ(D(cid:48)) + a2
[mσ(A(cid:48))mµ(D(cid:48)) − a2

σ(A(cid:48)),µ(D(cid:48))
σ(A(cid:48)),µ(D(cid:48))]2

(cid:41)

.

(76)

(77)

k =

2

· max

[mσ(A)mµ(D) − a2

σ(A),µ(D)

σ(A),µ(D)]2 ,

mσ(A(cid:48))mµ(D(cid:48)) + a2
[mσ(A(cid:48))mµ(D(cid:48)) − a2

σ(A(cid:48)),µ(D(cid:48))
σ(A(cid:48)),µ(D(cid:48))]2

theorem follows.

G. Proof of Lemma 4

(cid:41)

,

(78)

(79)

(80)

where σ(A(cid:48))th row and µ(D(cid:48))th column of matrix W− that achieves the inﬁnity norm, mσ(A(cid:48)), mµ(D(cid:48)) and
aσ(A(cid:48)),µ(D(cid:48)) has the same deﬁnition. Note that event ε1 and ε2 is independent since the elements of random matrix
2 W−T , according to the triangular inequality,

T − W−R2RT

1 W+

we have

R1 and R2 is independent. Let (cid:102)M = W+R1RT
(cid:104)(cid:13)(cid:13)(cid:13)(cid:102)M − M
(cid:13)(cid:13)(cid:13)∞ < 
(cid:40) mσ(A)mµ(D) + a2

Let

Pr

log(N )

(cid:105) ≥ Pr [ε1 ∩ ε2] = Pr [ε1] · Pr [ε2]

Proof: Induction on the following proposition: In step i,

(cid:88)

Bc

(i)(U ) =

U\{1,2,...,i}⊆V ⊆U

(−1)|U\V |B(V ).

When i = 0, it is trivial. Assuming it holds for step i, consider step i + 1, it has two cases. First, if i + 1 /∈ U,
U\{1, 2, . . . , i} = U\{1, 2, . . . , i, i+1}, then we have Bc
(−1)|U\V |B(V ).
Second, if i + 1 ∈ U, we have

U\{1,2,...,i,i+1}⊆V ⊆U

(i+1)(U ) = Bc

(i)(U ) =

(cid:80)

Bc

(i+1)(U ) = Bc

(i)(U\{i + 1}) + Bc

(i)(U )

(cid:88)
(cid:88)

=

=

U\{1,2,...,i,i+1}⊆V ⊆U\{i+1}

U\{1,2,...,i,i+1}⊆V ⊆U

(−1)|U\V |B(V ) +

(cid:88)

U\{1,2,...,i}⊆V ⊆U

(−1)|U\V |B(V )

(−1)|U\V |B(V ).

(81)

Based on the induction process, the lemma follows.

REFERENCES

[1] N. Alon, T. Lee, A. Shraibman, and S. Vempala. 2013. The approximate rank of a matrix and its algorithmic applications: approximate

rank. In Proceedings of the forty-ﬁfth annual ACM symposium on Theory of computing (STOC). ACM, 675684.

[2] V. Conitzer and T. Sandholm. 2006. Computing the optimal strategy to commit to. In Proceedings of the 7th ACM conference on Electronic

commerce (EC). ACM, 8290.

[3] A. Gueye, V. Marbukh, and J. C. Walrand. 2012. Towards a Metric for Communication Network Vulnerability to Attacks: A Game Theoretic

Approach. In Game Theory for Networks. Springer, 259274.

[4] M.Jain, D.Korzhyk, O.Vanek, V.Conitzer, M.Pechoucek, and M Tambe.2011. A double oracle algorithm for zero-sum security games on
graphs. In The 10th Inter- national Conference on Autonomous Agents and Multiagent Systems-Volume 1 (AA- MAS). International
Foundation for Autonomous Agents and Multiagent Systems, 327334.

[5] R. Kannan and T. Theobald. 2007. Games of ﬁxed rank: A hierarchy of bimatrix games. In Proceedings of the eighteenth annual ACM-SIAM

symposium on Discrete algorithms (SODA). Society for Industrial and Applied Mathematics, 11241132.

[6] M. Kearns and L. E. Ortiz. 2003. Algorithms for interdependent security games. In Advances in Neural Information Processing Systems

(NIPS).

[7] R. Kennes and P. Smets. 2013. Computational aspects of the Mobius transform. arXiv preprint arXiv:1304.1122 (2013).

[8] C. Kiekintveld, M. Jain, J. Tsai, J. Pita, F. Ordonez, and M. Tambe. 2009. Computing optimal randomized resource allocations for
massive security games. In Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems (AAMAS).
International Foundation for Autonomous Agents and Multiagent Sys- tems, 689696.

[9] D. Korzhyk, Z. Yin, C. Kiekintveld, V. Conitzer, and M. Tambe. 2011. Stackelberg vs. Nash in Security Games: An Extended Investigation

of Interchangeability, Equivalence, and Uniqueness. J. Artif. Intell. Res.(JAIR) 41 (2011), 297327.

[10] H. Kunreuther and G. Heal. 2003. Interdependent security. Journal of risk and uncer- tainty 26, 2-3 (2003), 231249.
[11] J. Letchford and V. Conitzer. 2013. Solving Security Games on Graphs via Marginal Probabilities.. In AAAI. Citeseer.
[12] P. Li, T. J Hastie, and K. W Church. 2006. Improving random projections using marginal information. In Learning Theory. Springer,

635649.

[13] R. J Lipton, E. Markakis, and A. Mehta. 2003. Playing large games using simple strategies. In Proceedings of the 4th ACM conference

on Electronic commerce (EC).

[14] H. B. McMahan, G. J Gordon, and A. Blum. 2003. Planning in the presence of cost functions controlled by an adversary. In ICML.

536543.

[15] H. Moulin and J-P Vial. 1978. Strategically zero-sum games: the class of games whose completely mixed equilibria cannot be improved

upon. International Journal of Game Theory 7, 3-4 (1978), 201221.

[16] M. Tambe. 2011. Security and game theory: Algorithms, deployed systems, lessons learned. Cambridge University Press.
[17] P. Wang and C. Lin. 2014. Iteration complexity of feasible descent methods for convex optimization. The Journal of Machine Learning

Research 15, 1 (2014), 15231548.

[18] A. Washburn and K. Wood. 1995. Two-person zero-sum games for network interdiction. Operations Research 43, 2 (1995), 243251.
[19] I. E. Yen, K. Zhong, C. Hsieh, P. K. Ravikumar, and I. S. Dhillon. 2015. Sparse linear programming via primal and dual augmented

coordinate descent. In Advances in Neural Information Processing Systems (NIPS). 23592367.

