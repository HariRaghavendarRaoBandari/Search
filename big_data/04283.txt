6
1
0
2

 
r
a

 

M
4
1

 
 
]

G
L
.
s
c
[
 
 

1
v
3
8
2
4
0

.

3
0
6
1
:
v
i
X
r
a

Universal probability-free conformal prediction

Vladimir Vovk and Dusko Pavlovic

March 15, 2016

Abstract

We construct a universal prediction system in the spirit of Popper’s
falsiﬁability and Kolmogorov complexity. This prediction system does not
depend on any statistical assumptions, but under the IID assumption it
dominates, although in a rather weak sense, conformal prediction.

Not for nothing do we call the laws of nature “laws”:
the more they prohibit, the more they say.

The Logic of Scientiﬁc Discovery
Karl Popper

1 Introduction

In this paper we consider the problem of predicting labels, assumed to be binary,
of a sequence of objects. This is an online version of the standard problem
of binary classiﬁcation. Namely, we will be interested in inﬁnite sequences of
observations

ω = (z1, z2, . . .) = ((x1, y1), (x2, y2), . . .) ∈ (X × 2)∞

(also called inﬁnite data sequences), where X is an object space and 2 := {0, 1}.
For simplicity, we will assume that X is a given ﬁnite set of, say, binary strings
(the intuition being that ﬁnite objects can always be encoded as binary strings).
Finite sequences σ ∈ (X × 2)∗ of observations will be called ﬁnite data
sequences. If σ1, σ2 are two ﬁnite data sequences, their concatenation will be
denoted (σ1, σ2); σ2 is also allowed to be an element of X × 2. A standard
partial order on (X × 2)∗ is deﬁned as follows: σ1 ⊑ σ2 means that σ1 is a preﬁx
of σ2; σ1 ❁ σ2 means that σ1 ⊑ σ2 and σ1 6= σ2.

We use the notation N := {1, 2, . . .} for the set of positive integers and
N0 := {0, 1, 2, . . .} for the set of nonnegative integers. If ω ∈ (X × 2)∞ and
n ∈ N0, ωn ∈ (X × 2)n is the preﬁx of ω of length n.

A situation is a concatenation (σ, x) ∈ (X × 2)∗ × X of a ﬁnite data sequence
σ and an object x; our task in the situation (σ, x) is to be able to predict the

1

label of the new object x given the sequence σ of labelled objects. Given a
situation s = (σ, x) and a label y ∈ 2, we let (s, y) stand for the ﬁnite data
sequence (σ, (x, y)), which is the concatenation of s and y.

2 Laws of nature as prediction systems

According to Popper’s [1] view of the philosophy of science, scientiﬁc laws of
nature should be falsiﬁable: if a ﬁnite sequence of observations contradicts such
a law, we should be able to detect it. (Popper often preferred to talk about
scientiﬁc theories or statements instead of laws of nature.) The empirical content
of a law of nature is the set of its potential falsiﬁers ([1], Sections 31 and 35). We
start from formalizing this notion in our toy setting, interpreting the requirement
that we should be able to detect falsiﬁcation as that we should be able to detect
it eventually.

Formally, we deﬁne a law of nature L to be a recursively enumerable preﬁx-
free subset of (X × 2)∗ (where preﬁx-free means that σ2 /∈ L whenever σ1 ∈
L and σ1 ❁ σ2).
Intuitively, these are the potential falsiﬁers, i.e., sequences
of observations prohibited by the law of nature. The requirement of being
recursively enumerable is implicit in the notion of a falsiﬁer, and the requirement
of being preﬁx-free reﬂects the fact that extensions of prohibited sequences of
observations are automatically prohibited and there is no need to mention them
in the deﬁnition.

A law of nature L gives rise to a prediction system: in a situation s = (σ, x)

it predicts that the label y ∈ 2 of the new object x will be an element of

ΠL(s) := {y ∈ 2 | (s, y) /∈ L} .

(1)

There are three possibilities in each situation s:

• The law of nature makes a prediction, either 0 or 1, in situation s when

the prediction set (1) is of size 1, |ΠL(s)| = 1.

• The prediction set is empty, |ΠL(s)| = 0, which means that the law of

nature has been falsiﬁed.

• The law of nature refrains from making a prediction when |ΠL(s)| = 2.

This can happen in two cases:

– the law of nature was falsiﬁed in past: σ′ ∈ L for some σ′ ⊑ σ;
– the law of nature has not been falsiﬁed as yet.

3 Strong prediction systems

The notion of a law of nature is static; experience tells us that laws of nature
eventually fail and are replaced by other laws. Popper represented his picture
of this process by formulas (“evolutionary schemas”) similar to

PS1 → TT1 → EE1 → PS2 → · · ·

(2)

2

(introduced in his 1965 talk on which [2], Chapter 6, is based and also discussed
in several other places in [2] and [3]; in our notation we follow Wikipedia).
In response to a problem situation PS, a tentative theory TT is subjected to
attempts at error elimination EE, whose success leads to a new problem situation
PS and scientists come up with a new tentative theory TT, etc.
In our toy
version of this process, tentative theories are laws of nature, problem situations
are situations in which our current law of nature becomes falsiﬁed, and there
are no active attempts at error elimination (so that error elimination simply
consists in waiting until the current law of nature becomes falsiﬁed).

If L and L′ are laws of nature, we deﬁne L ❁ L′ to mean that for any σ′ ∈ L′
there exists σ ∈ L such that σ ❁ σ′. To formalize the philosophical picture (2),
we deﬁne a strong prediction system L to be a nested sequence L1 ❁ L2 ❁ · · ·
of laws of nature L1, L2, . . . that are jointly recursively enumerable, in the sense
of the set {(σ, n) ∈ (X × 2)∗ × N | σ ∈ Ln} being recursively enumerable.

The interpretation of a strong prediction system L = (L1, L2, . . .) is that L1
is the initial law of nature used for predicting the labels of new objects until
it is falsiﬁed; as soon as it is falsiﬁed we start looking for and then using for
prediction the following law of nature L2 until it is falsiﬁed in its turn, etc.
Therefore, the prediction set in a situation s = (σ, x) is natural to deﬁne as the
set

ΠL(s) := {y ∈ 2 | (s, y) /∈ ∪∞

n=1Ln} .

(3)

As before, it is possible that ΠL(s) = ∅.

Fix a situation s = (σ, x) ∈ (X × 2)∗ × X. Let n = n(s) be the largest
integer such that s has a preﬁx in Ln. It is possible that n = 0 (when s does not
have such preﬁxes), but if n ≥ 1, s will also have preﬁxes in Ln−1, . . . , L1, by
the deﬁnition of a strong prediction system. Then Ln+1 will be the current law
of nature; all earlier laws, Ln, Ln−1, . . . , L1, have been falsiﬁed. The prediction
(3) in situation s is then interpreted as the set of all observations y that are not
prohibited by the current law Ln+1.

In the spirit of the theory of Kolmogorov complexity, we would like to have
a universal prediction system. However, we are not aware of any useful no-
tion of a universal strong prediction system. Therefore, in the next section we
will introduce a wider notion of a prediction system that does not have this
disadvantage.

4 Weak prediction systems and universal pre-

diction

A weak prediction system L is deﬁned to be a sequence (not required to be
nested in any sense) L1, L2, . . . of laws of nature Ln ⊆ (X × 2)∗ that are jointly
recursively enumerable.

Remark. Popper’s evolutionary schema (2) was the simplest one that he con-

3

sidered; his more complicated ones, such as

PS1

TTa → EEa → PS2a → · · ·

ր
→ TTb → EEb → PS2b → · · ·
ց

TTc → EEc → PS2c → · · ·

(cf. [2], pp. 243 and 287), give rise to weak rather than strong prediction systems.

In the rest of this paper we will omit “weak” in “weak prediction system”.
The most basic way of using a prediction system L for making a prediction in
situation s = (σ, x) is as follows. Decide on the maximum number N of errors
you are willing to make. Ignore all Ln apart from L1, . . . , LN in L, so that the
prediction set in situation s is

ΠN

L (s) := {y ∈ 2 | ∀n ∈ {1, . . . , N } : (s, y) /∈ Ln} .

Notice that this way we are guaranteed to make at most N mistakes: making a
mistake eliminates at least one law in the list {L1, . . . , LN }.

Similarly to the usual theory of conformal prediction, another way of pack-
aging L’s prediction in situation s is, instead of choosing the threshold (or level )
N in advance, to allow the user to apply her own threshold: in a situation s,
for each y ∈ 2 report the attained level

πs
L(y) := min {n ∈ N | (s, y) ∈ Ln}

(4)

(with min ∅ := ∞). The user whose threshold is N will then consider y ∈ 2
with πs
L(y) ≤ N as prohibited in s. Notice that the function (4) is upper
semicomputable (for a ﬁxed L).

The strength of a prediction system L = (L1, L2, . . .) at level N is determined

by its N -part

L≤N :=

N

[

n=1

Ln.

At level N , the prediction system L prohibits y ∈ 2 as continuation of a situation
s if and only if (s, y) ∈ L≤N .

The following lemma says that there exists a universal prediction system,
in the sense that it is stronger than any other prediction system if we ignore a
multiplicative increase in the number of errors made.

Lemma 1. There is a universal prediction system U , in the sense that for any
prediction system L there exists a constant C > 0 such that, for any N ,

L≤N ⊆ U≤CN .

(5)

Proof. Let L1, L2, . . . be a recursive enumeration of all prediction systems; their
component laws of nature will be denoted (Lk
2, . . .) := Lk. For each n ∈ N,

1, Lk

4

deﬁne the nth component Un of U = (U1, U2, . . .) as follows. Let the binary
representation of n be

(a, 0, 1, . . . , 1),

(6)

where a is a binary string (starting from 1) and the number of 1s in the 1, . . . , 1
is k − 1 ∈ N0 (this sentence is the deﬁnition of a = a(n) and k = k(n) in terms
of n). If the binary representation of n does not contain any 0s, a and k are
undeﬁned, and we set Un := ∅. Otherwise, set

Un := Lk
A,

where A ∈ N is the number whose binary representation is a. In other words, U
consists of the components of Lk, k ∈ N; namely, Lk
1 is placed in U as U3×2k−1−1
and then Lk

3, . . . are placed at intervals of 2k:

2, Lk

U3×2k−1−1+2k(i−1) = Lk
i ,

i = 1, 2, . . . .

It is easy to see that

Lk

≤N ⊆ U≤3×2k−1−1+2k(N −1),

(7)

which is stronger than (5).

Let us ﬁx a universal prediction system U. By K(L) we will denote the
smallest preﬁx complexity of the programs for computing a prediction system
L. The following lemma makes (5) uniform in L showing how C depends on L.

Lemma 2. There is a constant C > 0 such that, for any prediction system L
and any N , the universal prediction system U satisﬁes

L≤N ⊆ U≤C2K(L)N .

(8)

Proof. Follow the proof of Lemma 1 replacing the “code” (0, 1, . . . , 1) for Lk
in (6) by any preﬁx-free description of Lk (with its bits written in the reverse
order). Then the modiﬁcation

Lk

≤N ⊆ U≤2k′ +1−1+2k′ (N −1)

of (7) with k′ := K(Lk) implies that (8) holds for some universal prediction
system, which, when combined with the statement of Lemma 1, implies that (8)
holds for our chosen universal prediction system U.

This is a corollary for laws of nature:

Corollary 1. There is a constant C such that, for any law of nature L, the
universal prediction system U satisﬁes

L ⊆ U≤C2K(L) .

(9)

Proof. We can regard laws of nature L to be a special case of prediction systems
identifying L with L := (L, L, . . .). It remains to apply Lemma 2 to L setting
N := 1.

5

We can equivalently rewrite (5), (8), and (9) as

ΠCN
ΠC2K(L)N

U (s) ⊆ ΠN
(s) ⊆ ΠN

U

L (s),

L (s),

and

ΠC2K(L)

U

(s) ⊆ ΠL(s),

(10)

(11)

(12)

respectively, for all situations s. Intuitively, (10) says that the prediction sets
output by the universal prediction system are at least as precise as the prediction
sets output by any other prediction system L if we ignore a constant factor in
specifying the level N ; and (11) and (12) indicate the dependence of the constant
factor on L.

5 Universal conformal prediction under the IID

assumption

Comparison of prediction systems and conformal predictors is hampered by
the fact that the latter are designed for the case where we have a constant
amount of noise for each observation, and so we expect the number of errors
to grow linearly rather than staying bounded.
In this situation a reasonable
prediction set is ΠǫN
L (s), where N is the number of observations in the situation
s. For a small ǫ using ΠǫN
L (s) means that we trust the prediction system whose
percentage of errors so far is at most ǫ.

Up to this point our exposition has been completely probability-free, but
in the rest of this section we will consider the special case where the data are
generated in the IID manner. For simplicity, we will only consider computable
conformity measures that take values in the set Q of rational numbers.

Corollary 2. Let Γ be a conformal predictor based on a computable conformity
measure taking values in Q. Then there exists C > 0 such that, for almost all
inﬁnite sequences of observations ω = ((x1, y1), (x2, y2), . . .) ∈ (X × 2)∞ and all
signiﬁcance levels ǫ ∈ (0, 1), from some N on we will have

ΠCN ǫ ln2(1+1/ǫ)

U

((ωN , xN +1)) ⊆ Γǫ((ωN , xN +1)).

(13)

This corollary asserts that the prediction set output by the universal pre-
diction system is at least as precise as the prediction set output by Γ if we
increase slightly the signiﬁcance level: from ǫ to Cǫ ln2(1 + 1/ǫ). It involves not
just multiplying by a constant (as is the case for (5) and (8)–(12)) but also the
logarithmic term ln2(1 + 1/ǫ).

It is easy to see that we can replace the C in (13) by C2K(Γ), where C
now does not depend on Γ (and K(Γ) is the smallest preﬁx complexity of the
programs for computing the conformity measure on which Γ is based).

6

Proof of Corollary 2. Let

ǫ′ := 2⌈log ǫ⌉+1,

where log stands for the base 2 logarithm. (Intuitively, we simplify ǫ, in the
sense of Kolmogorov complexity, by replacing it by a number of the form 2−m
for an integer m, and make it at least twice as large as the original ǫ.) Deﬁne
a prediction system (both weak and strong) L as, essentially, Γǫ′
; formally,
L := (L1, L2, . . .) and Ln is deﬁned to be the set of all ωN , where ω ranges over
the inﬁnite data sequences and N over N, such that the set
((ωi−1, xi))o

ni ∈ {1, . . . , N } | yi /∈ Γǫ′

is of size n and contains N . The prediction system L is determined by ǫ′,
so that K(L) does not exceed (apart from the usual additive constant) K(ǫ′).
By the standard validity property of conformal predictors ([6], Corollary 1.1),
Hoeﬀding’s inequality, and the Borel–Cantelli lemma,

Πǫ′N

L ((ωN , xN +1)) ⊆ Γǫ((ωN , xN +1))

from some N on almost surely. By Lemma 2 (in the form of (11)),

′

ΠC12K(ǫ

U

)ǫ′N

((ωN , xN +1)) ⊆ Πǫ′N

L ((ωN , xN +1))

(14)

(15)

for all N . The statement (13) of the corollary is obtained by combining (14),
(15), and

2K(ǫ′) ≤ C2 ln2(1 + 1/ǫ).

To check the last inequality, remember that ǫ′ = 2−m for an integer m, which
we assume to be positive, without loss of generality; therefore, our task reduces
to checking that

2K(m) ≤ C3 ln2(1 + 2m),

i.e.,

2K(m) ≤ C4m2.

Since 2−K(m) is the universal semimeasure on the positive integers (see, e.g.,
[5], Theorem 7.29), we even have

2K(m) ≤ C5m(log m)(log log m) · · · (log · · · log m),

where the product contains all factors that are greater than 1 (see [4], Ap-
pendix A).

6 Conclusion

In this note we have ignored the computational resources, ﬁrst of all, the required
computation time and space (memory). Developing versions of our deﬁnitions
and results taking into account the time of computations is a natural next
step. In analogy with the theory of Kolmogorov complexity, we expect that the
simplest and most elegant results will be obtained for computational models
that are more ﬂexible than Turing machines, such as Kolmogorov–Uspensky
algorithms and Sch¨onhage machines.

7

Acknowledgments.

We thank the anonymous referees for helpful comments. This work has been
supported by the Air Force Oﬃce of Scientiﬁc Research (grant “Semantic Com-
pletions”), EPSRC (grant EP/K033344/1), and the EU Horizon 2020 Research
and Innovation programme (grant 671555).

References

[1] Karl R. Popper. Logik der Forschung. Springer, Vienna, 1934. English

translation: The Logic of Scientiﬁc Discovery. Hutchinson, London, 1959.

[2] Karl R. Popper. Objective Knowledge: An Evolutionary Approach. Claren-

don Press, Oxford, revised edition, 1979. First edition: 1972.

[3] Karl R. Popper. All Life is Problem Solving. Abingdon, Routledge, 1999.

[4] Jorma Rissanen. A universal prior for integers and estimation by minimum

description length. Annals of Statistics, 11:416–431, 1983.

[5] Alexander Shen. Around Kolmogorov complexity: Basic notions and re-
sults. In Vladimir Vovk, Harris Papadopoulos, and Alexander Gammerman,
editors, Measures of Complexity: Festschrift for Alexey Chervonenkis, chap-
ter 7, pages 75–115. Springer, Cham, 2015.

[6] Vladimir Vovk. The basic conformal prediction framework. In Vineeth N.
Balasubramanian, Shen-Shyang Ho, and Vladimir Vovk, editors, Conformal
Prediction for Reliable Machine Learning: Theory, Adaptations, and Appli-
cations, chapter 1, pages 3–19. Elsevier, Amsterdam, 2014.

8

