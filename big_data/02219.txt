6
1
0
2

 
r
a

M
7

 

 
 
]
h
p
-
h
t
a
m

[
 
 

1
v
9
1
2
2
0

.

3
0
6
1
:
v
i
X
r
a

Coulomb potentials and Taylor expansions in Time-Dependent Density Functional

Theory

Søren Fournais,1 Jonas Lampart,2 Mathieu Lewin,3 and Thomas Østergaard Sørensen4

1Department of Mathematical Sciences, University of Aarhus,

Ny Munkegade 118, DK-8000 Aarhus C, Denmark

2PSL Research University & CEREMADE (UMR CNRS 7534),

University of Paris-Dauphine, Place de Lattre de Tassigny, F-75775 Paris Cedex 16, France

3CNRS & CEREMADE (UMR CNRS 7534), University of Paris-Dauphine,

4Mathematisches Institut, Universit¨at M¨unchen, Theresienstraße 39, D-80333 Munich, Germany

Place de Lattre de Tassigny, F-75775 Paris Cedex 16, France

(Dated: March 8, 2016)

We investigate when Taylor expansions can be used to prove the Runge-Gross Theorem, which is
at the foundation of Time-Dependent Density Functional Theory (TDDFT). We start with a general
analysis of the conditions for the Runge-Gross argument, especially the time-diﬀerentiability of the
density. The latter should be questioned in the presence of singular (e.g. Coulomb) potentials.
Then, we show that a singular potential in a one-body operator considerably decreases the class
of time-dependent external potentials to which the original argument can be applied. A two-body
singularity has an even stronger impact and an external potential is essentially incompatible with
it. For the Coulomb interaction and all reasonable initial many-body states, the Taylor expansion
only exists to a ﬁnite order, except for constant external potentials. Therefore, high-order Taylor
expansions are not the right tool to study atoms and molecules in TDDFT.

Density Functional Theory (DFT) is one of the pil-
lars of modern quantum chemistry and condensed mat-
ter physics. Its time-independent form has been studied
in depth and its virtues and limitations are rather well
understood [1–3]. On the other hand, Time-Dependent
Density Functional Theory (TDDFT) is more recent and
certainly less well understood.

One of the most important steps in the construc-
tion of TDDFT is a result by Runge and Gross [4, 5]
which says that the external potential V (t, x) in a many-
body system is completely determined (up to a con-
stant) by the one-particle density ρ(t, x). More pre-
cisely, if two potentials V1(t, x) and V2(t, x) give rise to
the same one-particle density ρ(t, x) for all times t, then
V1(t, x) = V2(t, x) + C(t). The applicability of this re-
sult for Coulomb systems is still under debate and our
purpose in this article is to discuss in more detail the
possible problems that can arise with Coulombic or more
general singular potentials.

The original argument of Runge and Gross relies on
the assumption that the external potentials as well as the
many-body body wavefunction Ψ(t) are all time-analytic,
which means that they can be expanded in a convergent
Taylor series in powers of t. If two potentials V1(t, x) and
V2(t, x) give rise to the same ρ(t, x), it was argued that
each coeﬃcient of tk in the two Taylor series of ∇V1(t, x)
and ∇V2(t, x) must be the same. The convergence of
these power series to ∇V1(t, x) and ∇V2(t, x) then implies
that V1(t, x) = V2(t, x) + C(t).
It has recently been noticed [6–8] that the time-
analyticity of the wavefunction fails in many simple ex-
amples, even for time-analytic potentials. This is not sur-
prising, since the time regularity of the solution Ψ(t, x) to
Schr¨odinger’s equation is known to be intimately linked
to the space regularity of the initial state Ψ0(x), as we

If this initial state Ψ0(x) is not smooth
will recall.
enough with respect to x, then the resulting solution
will not be smooth in t. Conversely, even for a very
smooth initial state Ψ0(x), non-smooth (e.g. Coulomb)
potentials can create singularities that propagate in time
and give a Schr¨odinger solution that is not smooth in
t. These examples clearly violate the assumptions used
in the Runge-Gross approach and it is an open question
whether potentials are characterized by the density in
those cases. This is of course a fundamental problem for
Coulomb potentials as in atoms and molecules.

Our purpose in this work is to discuss in detail how
this eﬀect arises and what role it plays for Coulomb po-
tentials.

An important question, rarely discussed in the litera-
ture, is to ﬁnd reasonable assumptions on the potentials
and initial state Ψ0 under which the Runge-Gross ap-
proach, based on Taylor expansions, is applicable. As
explained above, this requires a better understanding of
the link between the properties of Ψ0(x) and those of
V (t, x) that will generate a solution Ψ(t, x) that is an-
alytic, or at least smooth, with respect to time. This
link is naturally expressed in terms of the spectral the-
ory of the underlying many-body Hamiltonian and it is
rather subtle in the presence of singular potentials. We
will introduce a precise framework taking care of these
questions in Sections I and II.

We then analyze in detail three diﬀerent situations.
In the case of smooth external potentials and interac-
tions, we explain for which Ψ0(x) and V (t, x) the origi-
nal Runge-Gross argument works out and we give a com-
plete, mathematically rigorous, proof in this setting in
Section III.

We then look at singular potentials. In Section IV A
we treat two examples of one-particle systems that il-

lustrate how a singularity in the potential considerably
reduces the set of allowed time-dependent potentials. A
two-body singularity turns out to be even more delicate
and an external potential is essentially incompatible with
it. Indeed, except for constant external potentials, the
many-body wavefunction will in general not be smooth
in time for a singular interaction.

The Coulomb interaction suﬀers from these diﬃcul-
ties.
In Section IV B we determine exactly how many
time-derivatives make sense in this case, and derive a
corresponding ﬁnite-order Runge-Gross Theorem. This
is not the result that one might have hoped for, but this
is certainly the best that can be obtained with an argu-
ment based on Taylor expansions.

Another important piece of TDDFT is van Leeuwen’s
construction [9] of an external time-dependent potential
V (t, x) that produces a given density ρ(t, x). His argu-
ment is also based on power series expansions in time
and thus suﬀers from the same regularity issues. Sev-
eral recent works [10–14] have aimed at justifying the
Runge-Gross and van Leeuwen results avoiding the use
of Taylor expansions. This is an important and inter-
esting program which has not yet reached a completely
satisfactory level of mathematical rigor. We hope that
our work will clarify the situation and stimulate further
research.

I. THE SETTING

In this section, we would like to discuss a general set-
ting, based on physical considerations, for the Runge-
Gross Theorem. We consider a system of N parti-
cles (fermions or bosons) interacting through a poten-
tial w(x − y) and submitted to a ﬁxed external poten-
tial V0(x). For atoms and molecules one should consider
Coulomb potentials but the situation is kept general for
the moment. For simplicity of the exposition, we will
discard the spin variable but everything applies mutatis
mutandis if the particles have an internal degree of free-
dom. In a time-dependent external potential V (t, x), the
N -body Schr¨odinger Hamiltonian is

HV =

NXj=1

−∆xj +V0(xj )+V (t, xj )+ X16j<k6N

w(xj − xk)

and the corresponding time-dependent Schr¨odinger evo-
lution equation is

(i ∂

∂t Ψ(t, X) = HV Ψ(t, X)
Ψ(0, X) = Ψ0(X) given,

(1)

where xj ∈ R3 and X = (x1, . . . , xN ). We are using units
such that 2m =  = 1, and also 4πε0 = 1. For atoms
and molecules, w(x − y) = 1/|x − y| and the Coulomb
m=1 Zm/|x−rm| of the nuclei can be either
included in V0, if the nuclei are ﬁxed, or in V (t, x) if they

potential −PM

2

move. Without the potential V (t, x), the Hamiltonian
becomes

H0 =

NXj=1

−∆xj + V0(xj ) + X16j<k6N

w(xj − xk).

We recall that the density of the N -particle solution is
deﬁned by

ρ(t, x) = NZR3(N −1) |Ψ(t, x, x2, ..., xN )|2 dx2 ··· dxN .

For the Runge-Gross Theorem, we have to specify a
class of initial conditions I = {Ψ0’s} as well as a class of
considered external potentials V = {V (t, x)’s} (deﬁned
on a given time interval [0, tmax)). The goal is to ﬁnd the
sets I and V for which the following theorem is valid.
Runge-Gross’ uniqueness. Let Vj(t, x), j = 1, 2, be
two potentials in the class V and let Ψj(t, x1, ..., xN ) be
the corresponding solutions to (1), with the same ini-
tial state Ψ0 ∈ I.
If the associated densities satisfy
ρ1(t, x) = ρ2(t, x) for all t ∈ [0, tmax) and all x ∈ R3,
then V1(t, x) = V2(t, x) + C(t).

Depending on the physical context, the set V could
be chosen very small, for instance it could consist of
the Coulomb potential generated by one moving nucleus,
with the position r(t) of this nucleus being the only pa-
rameter:

V =(cid:26)−

Z

|x − r(t)|

, r(t) ∈ R3(cid:27) .

It could also be very large and contain a whole class of
smooth functions of the time and space variables.
In
general it is desirable to have both sets I and V as large as
possible. However they cannot be chosen independently.
Uniqueness might hold for a very large set V provided
that I is very small, and conversely. Furthermore, the
choice of I and V could (and will) highly depend on the
properties of the potentials w and V0. Since in practice
the exact wavefunction Ψ0 is unknown, we believe that
it is appropriate to choose I as large as possible.
should be imposed:

We think that the following reasonable conditions

(H1) If the initial datum Ψ0(X) is in I and V (t, x) is
a potential in V, then the wavefunction Ψ(t, X),
solution to the Schr¨odinger equation (1), must be
in I for all later times t.

(H2) The constant potentials V (t, x) = C(t) all belong

to V.

(H3) If V (t, x) is in V, then the time-independent poten-
tial V (t0, x) is in V as well, for every t0 ∈ [0, tmax).
(H4) If V (x) ∈ V is time-independent, then all the eigen-

functions of HV are in I.

Condition (H1) is here to be able to apply the Runge-
Gross argument at later times t0 > 0, with new initial
state Ψ(t0, X). There is no reason to give t = 0 a special
role. On the other hand, (H2) covers the case when no
external ﬁeld is applied to the system (V = 0). By a
gauge transformation, all the constants C(t) must then
It is often assumed that the evolu-
be allowed in V.
tion before the considered time t = 0 was governed by a
ﬁxed time-independent potential, which is only changed
at positive times.
It is then natural to assume, as in
(H3), that time-independent potentials are in V as well.
Finally, the assumption (H4) is here to make sure that
the usual time-independent DFT is covered by the the-
ory. Indeed, in applications Ψ0 will often be the ground
state of the potential V (0, x), and V (t, x) would then be
chosen in order to bring the system to another interesting
state.

We remark that I depends on the number of particles
N , whereas we could also demand that V is independent
of N , if needed. This may of course result in a smaller
set, depending on the situation.

To our knowledge the choice of the sets V and I has
never been discussed in the literature. This is not a ques-
tion of purely mathematical interest since the applicabil-
ity of the Runge-Gross Theorem to physical systems re-
lies on these sets not being too small. In the following we
will give several examples that will clarify the respective
In the next section we start by dis-
roles of I and V.
cussing for which choices of I the wavefunction is smooth
in time, for a time-independent potential V (x), before
addressing the more general case of time-dependent po-
tentials V (t, x).

II. TIME-REGULARITY OF SOLUTIONS TO

THE SCHR ¨ODINGER EQUATION

In this section we would like to recall when a solution
Ψ(t) of Schr¨odinger’s equation depends smoothly on the
time t. Based on these results, we propose an abstract
deﬁnition of the sets I and V, which provides a smooth-
in-time solution Ψ, and for which the original Runge-
Gross argument is thus applicable. These sets will be
made more explicit in particular examples in the follow-
ing sections.

We start by discussing the case of time-independent
potentials that must all belong to V by Condition (H2).
We then turn to the general time-dependent case.

A. The time-independent case

Let V (x) be a time-independent potential that makes
HV a self-adjoint operator (more precisely, we as-
sume that the potential energy is inﬁnitesimally H0-
bounded, see the discussion below). Then the solution
to Schr¨odinger’s equation (1) exists for any state Ψ0,

RR3N |Ψ0|2 = 1, and can be expressed in the abstract

form

3

Ψ(t, X) =(cid:0)e−itHV Ψ0(cid:1)(X) .

The operator e−itHV is unitary and it is deﬁned using
the so-called functional calculus for self-adjoint opera-
tors [15]. In general e−itHV Ψ0 is not given by the power

Ψ0’s, (HV )kΨ0 does not make sense. Indeed, if the se-

seriesPk>0(−itHV )kΨ0/k!. The reason is that, for most
ries converges, then etHV Ψ0 =Pk>0(tHV )kΨ0/k! is also

convergent and gives a solution to the “backward heat
equation”

∂
∂t

Ψ − HV Ψ = 0

which is well-known to be ill-posed for most initial states
Ψ0.

The set of functions Ψ for which HV Ψ is square-
integrable is called the domain of HV and is often de-
noted by D(HV ). For instance,
if T = −∆ is the
kinetic energy operator, then ψ ∈ D(T ) if and only

if kT ψk2 = R |∆ψ(x)|2 dx = R |k|4|bψ(k)|2 dk is ﬁnite,
where bψ(k) is the Fourier transform of ψ(x).

When looking at the power series, it is natural to think
that Ψ(t) = exp(−itHV )Ψ0 will be k times diﬀerentiable
in t, if and only if (HV )kΨ0 is square integrable, and that
then

∂k

∂tk Ψ(t, X) = (−i)k(cid:0)e−itHV (HV )kΨ0(cid:1) (X).

This intuitive result is true [16, Theorem VIII.7], but one
has to be very careful with how (HV )k is deﬁned.

Before discussing the deﬁnition of (HV )k, we re-
mark that t 7→ Ψ(t, X) is time-analytic if and only if
is the only case for which Ψ(t, X) can be reconstructed
from its Taylor series

Pk>0 k(HV )kΨ0kRk/k! is ﬁnite for some R > 0, and this

Ψ(t, X) =Xk>0

(−iHV )kΨ0(X)

k!

tk,

for |t| 6 R. Such states Ψ0 are called analytic vectors of
HV , a concept that was introduced by E. Nelson in [17]
and played an important role in quantum ﬁeld theory.

The meaning of the operator (HV )k, from now on de-
noted H k
V , is again determined by the functional calculus.
In more practical terms, H k
V Ψ0 is deﬁned by recursively
calculating HV Ψ0, HV (HV Ψ0), etc., which must all be
square-integrable functions. The domain of H k
V is, there-
fore,

D(H k

V ) =(cid:26)Ψ ∈ D(HV ) : HV Ψ ∈ D(HV ),
HV (HV Ψ) ∈ D(HV ), ··· , H k−1

V Ψ ∈ D(HV )(cid:27).

(2)

The operator H k

The space D(H k
V ) can be shown to be invariant under
the ﬂow exp(−itHV ). It contains all the eigenfunctions
of HV , since HV Ψ = λΨ ∈ D(HV ).
V cannot be fully understood without
identifying precisely its domain D(H k
V ). But computing
the domain D(H k
V ) can sometimes be a rather diﬃcult
task. For instance, for the hydrogen atom which will be
considered in more detail in Section IV A, the domain
D(h) of h = −∆ − 1/|x| is the same as D(−∆). How-
ever, for suﬃciently large k the domain D(hk) becomes
diﬀerent from D(−∆k). It contains additional boundary
conditions on the derivatives of ψ at x = 0. After apply-
ing h too many times, the resulting wavefunction hk−1ψ
will be singular at x = 0 for a smooth ψ.

The operator H k

V can therefore not easily be under-
stood by looking at the formula for HV . For singu-
lar potentials there are usually many consistent choices
of boundary conditions for the expression obtained by
calculating the kth power of HV . Only one of these
characterizes the domain of H k
V , that is, the correct
boundary conditions arising from the constraints that
H j
V Ψ0 ∈ D(HV ) for j = 1, ..., k − 1.
Let us clarify this by an example. In [6] the authors
considered the one-dimensional state ψ0(x) = κe−|x|.
This state is not in the domain D(T ) of the free ki-
netic energy operator T = −d2/dx2. Indeed, functions in
D(T ) can be shown to be diﬀerentiable with a continuous
derivative and ψ0 does not have a continuous derivative.
In particular, ψ(t) = exp(−itT )ψ0 is not diﬀerentiable
in time. On the other hand, (d2/dx2)kψ0 = ψ(2k)
0 = ψ0
has a meaning outside of zero for all k > 1, but the
latter expression cannot be the formula of T kψ0 since
ψ0 /∈ D(T k). This is why ψ(t) = exp(−itT )ψ0 does not
(−it)k/k! = κe−it−|x|,
as was observed by the authors of [6]. The deeper rea-
son of this discrepancy is that D(T ) contains the bound-
ary condition ψ′(0−) = ψ′(0+) whereas the equation
−ψ′′
0 = ψ0 holds with the diﬀerent boundary condition
ψ′(0+) − ψ′(0−) = −2ψ(0).
Let us now return to the question of choosing the sets
I and V. In order to be able to apply the original Runge-
Gross argument which relies on the time-diﬀerentiability
of Ψ(t), the previous discussion tells us that we should
at least choose a set I that satisﬁes
I ⊂ D(H k
V )

coincide with the seriesPk>0 ψ(2k)

0

for all time-independent potentials V (x) ∈ V and all
k > 1. Doing so will imply that Ψ(t, X) is inﬁnitely
diﬀerentiable in t, for any Ψ0 ∈ I. If analyticity is to be
required, then I must be included in the set of analytic
vectors of HV for all V (x). Now, the simplest choice is
to take I as the intersection of all these spaces but I
could then be extremely small. More severely, this set
would not be invariant under the ﬂows corresponding to
all the possible HV ’s, violating our condition (H1), if the
domains D(H k
V ) are truly diﬀerent when V (x) is varied.
For this reason, it is natural to restrict ourselves to the

potentials V (x) that preserve the domain of H k

0 for all k:

4

D(H k

V ) = D(H k

0 ),

∀k > 1.

For such a potential, and any Ψ0 in

I = \k>1

D(H k

0 ),

(3)

(4)

the solution Ψ(t) is diﬀerentiable inﬁnitely often in time,
and Ψ(t) stays in I for all later times t > 0.
In this way, the choice of the Ψ0’s which give a
time-diﬀerentiable solution Ψ(t) has led us to the con-
straint (3) on the set of potentials V. The property (3) is
however not so easy to check for a given potential V (x).
Fortunately, there is an equivalent formulation which we
are going to discuss now.

j=1 V (xj ) is bounded relative to

H0, with inﬁnitesimal bound, if

Let us recall that PN
V (xj )Ψ(X)(cid:12)(cid:12)(cid:12)(cid:12)
ZR3N(cid:12)(cid:12)(cid:12)(cid:12)

NXj=1

2

dX

6 ε2ZR3N |(H0 + Cε)Ψ(X)|2 dX (5)

for every ε > 0 and Ψ ∈ D(H0) (see [15, Sec. 1.4]). This
is a famous condition, satisﬁed for Coulomb potentials as
was shown by Kato in the 50s [18].

0

maps D(H k

).

0

) = D(H k−1

0

V

maps D(H k

D(H k

V ) = D(H k

0 ) to D(H k−1

0 ) to D(H k−1

) for every k > 1.

j=1 V (xj )Ψ(X) is in D(H k−1

for every Ψ ∈ D(H k
0 ),

Lemma 1 (Condition on V (x)). Assume that H0 is
bounded below and that the time-independent potential
V (x) satisﬁes (5). Then V (x) satisﬁes the constraint (3)
the function
if and only if
In
j=1 V (xj )

PN
other words, the multiplication operator by PN
H0 +PN
H0 also has this property, so HV − H0 = PN

Proof. Assume ﬁrst that (3) is satisﬁed. Then HV =
j=1 V (xj ) maps, by deﬁnition of these domains,
). Obviously,
j=1 V (xj )

0 ) to D(H k−1

) to D(H 2k

Theorem 1.4.2]). Now assume that PN

For the converse implication, we recall that the in-
ﬁnitesimal relative bound (5) ensures that HV is self-
adjoint on the domain D(HV ) = D(H0) (see [15,
j=1 V (xj ) maps
D(H 2k+1
0 ). This map is then automati-
cally continuous by the closed graph theorem [16, Theo-
rem III.12], because continuity from D(H0) to the space
L2(R3N ) of square-integrable functions (assumption (5))
implies that the graph of its restriction is closed in
D(H 2k+1
) × D(H 2k
j=1 V (xj)(H0 +
Cε)−1 maps D(H 2k
0 ) continuously to itself. On the other
j=1 V (xj)(H0 + Cε)−1 is norm-
bounded by ε on L2(R3N ). By the theory of interpola-
tion [19, Prop. 9 Chap. IX.4] it is therefore bounded by

hand, (5) means that PN

0 ). Consequently, PN

), for every k > 1.

0

0

0

a quantity proportional to √ε on D(H k
0 ). After choosing
ε suﬃciently small, the same argument as for the Kato-
Rellich theorem (that we used for k = 0) applies [15,
Sec. 1.4], and shows that D(H k

V ) = D(H k

0 ).

If the domain D(H k

0 ) contains speciﬁc boundary con-
ditions at the singularities of the potentials w and V0,
then the lemma says that multiplying by the function
j=1 V (xj ) must preserve these conditions. As we will

show in Section IV, this is a very restrictive condition.

PN

B. The time-dependent case

For a time-dependent potential V (t, x),

the time
derivatives of Ψ(t) will clearly involve time-derivatives
of V . Therefore, in view of the discussion of the previ-
ous section, a natural condition on V (t, x) is to assume
t V (t, xj) exists and satisﬁes the condition of

Lemma 1, that is,

j=1 ∂ℓ

thatPN
ZR3N(cid:12)(cid:12)(cid:12)(cid:12)H k−1

0

dX

2

∂ℓ

NXj=1
6 Ck,ℓ,tZR3N(cid:12)(cid:12)(cid:0)H k

t V (t, xj)Ψ(X)(cid:12)(cid:12)(cid:12)(cid:12)
0 Ψ(cid:1) (X)(cid:12)(cid:12)2

+ |Ψ(X)|2dX (6)

for every k > 1, every ℓ > 0, every t ∈ [0, tmax) and every
Ψ ∈ D(H k
0 ). Additionally, we have the condition (5) that
j=1 V (t, xj ) should be inﬁnitesimally bounded with re-
spect to H0. This now deﬁnes us a set

PN

V =nV (t, x) satisfying (5) and (6)o.

(7)

For these choices of I in (4) and V in (7), it remains
to check if the conditions (H1)–(H4) are satisﬁed. (H2)
and (H3) follow directly from the deﬁnition of V. Using
that eigenfunctions belong to D(H k
0 ) for all
k > 1, (H4) follows immediately. Only (H1) needs a
more careful treatment. The following result shows that
it indeed holds.

V ) = D(H k

Theorem 1 (Regularity of solutions to the time-depen-
dent Schr¨odinger equation). Let Ψ0 ∈ I and V (t, x) ∈ V.
Then the solution Ψ(t, X) to Schr¨odinger’s equation (1)
belongs to I for every t ∈ [0, tmax). It is diﬀerentiable
in t inﬁnitely many times, and all its t-derivatives also
belong to I.
Proof. It was proved by Kato [20, Theorem 4] that
if the domain D(HV (t0)) = D(H0) is independent
j=1 ∂tV (t, xj ) is a
continuous map from D(H0) to the Hilbert space of
square-integrable functions, then the solution Ψ(t, X)
to Schr¨odinger’s equation (1) exists, and it belongs to
D(H0) if the initial condition Ψ0(X) is an element of
D(H0).

of t0 ∈ [0, tmax) and ∂tHV = PN

5

In our case, the conditions on the domain and the ini-
tial condition are implied by the choices of V and I. Con-
tinuity of ∂tHV is exactly the statement of condition (6)
with ℓ = k = 1. We thus have Ψ(t, X) ∈ D(H0) for
t ∈ [0, tmax).
If we consider D(H0) as a Hilbert space, the domain of
HV (t0) on this space is exactly given by D(H 2
0 ) (see Equa-
tion (2)). Using that Ψ0(X) ∈ D(H 2
0 ) and condition (6)
(with ℓ = 1, k = 2) we can apply Kato’s result in this set-
ting, and ﬁnd that Ψ(t, X) is an element of D(H 2
0 ) for all
t ∈ [0, tmax). Hence HV Ψ(t, X) is an element of D(H0).
To prove that HV Ψ(t, X) is diﬀerentiable in time one
uses (6) again. This shows that Ψ(t, X) has two time-
derivatives, and repeating this argument shows that it
has inﬁnitely many.

In this section we have deﬁned the sets I and V such
as to be able to take as many time derivatives as we like,
and the original proof of Runge and Gross will apply. Of
course, the main question is to identify more precisely
the sets I and V in order to understand which physical
systems are covered. It is the potentials w and V0 ap-
pearing in the deﬁnition of H0 that will determine the
properties of these sets.

We remark that the exact same construction can be ap-
plied to obtain a Ψ(t) which has only m time-derivatives.
In this case, I = D(H m
0 ) and the conditions (6) only have
to be veriﬁed for k + ℓ 6 m + 1. We will come back to
this generalization later in Section IV B, when treating
Coulomb interactions.

III. THE SMOOTH CASE

In this section we study the case when w and V0 are
smooth, and identify explicitly the two sets I and V. We
will see that the corresponding wavefunction Ψ(t) is also
smooth, and that the original Runge-Gross method can
be followed without any danger.

We ﬁrst consider the kinetic energy operator T =
j=1 −∆xj . Using the Fourier transform, one sees that
a square-integrable function Ψ belongs to D(T k) if and

PN
only ifRR3N |K|4k|bΨ(K)|2 dK is ﬁnite. Said diﬀerently, a

function will be in all the domains D(T k) with k > 1,
if its Fourier transform decays faster than any polyno-
mial. By the theory of Sobolev [21, 22], this turns out to
imply that Ψ has inﬁnitely many space derivatives. On
the other hand, Ψ will be an analytic vector of T if its
Fourier transform decays exponentially and this implies
that it is a real-analytic function of X.

We now assume that V0 and w are smooth functions,
that is, they are diﬀerentiable inﬁnitely many times and
their derivatives are all uniformly bounded on R3. Under
this condition, one can show that the domain of H k
0 is
equal to that of T k. The argument is exactly the same
as in the proof of Lemma 1. Therefore, the set I deﬁned
in (4) is just the space of square-integrable, fermionic or

bosonic, functions whose Fourier transform decays faster
than any polynomial.

j=1 ∂ℓ

V (t, x) such thatPN

The set V deﬁned in (7) consists of the potentials
t V (t, xj) maps I into itself, for
every ℓ. This shows that ∂ℓ
t V must be smooth. Using
the translation-invariance of T , V (t, x) and its deriva-
tives can also be shown that it is uniformly bounded on
R3. Therefore, Theorem 1 can be rephrased in the more
explicit form:

Theorem 2 (Regularity for smooth potentials). Assume
that V0 and w are diﬀerentiable inﬁnitely often in space
and that their derivatives are all uniformly bounded on
R3. Assume also that V (t, x) is diﬀerentiable inﬁnitely
often in space-time and its derivatives are bounded uni-
formly with respect to x for every t. Then, for every
Ψ0 ∈ I, the solution Ψ(t, X) to (1) is smooth in space-
time and ∂ℓ
t Ψ(t, X) belongs to I for all ℓ > 0 and all
t ∈ [0, tmax).

Based on this result, we can now give a complete proof

of the Runge-Gross Theorem in the smooth case.

Theorem 3 (Runge-Gross for smooth potentials). Let
Ψ0, V0, w and V = Vm, m = 1, 2 be as in Theorem 2.
Let Ψm(t, X) be the solution to (1) with V = Vm and let
ρm(x) be its density. If ρ1(t, x) = ρ2(t, x) =: ρ(t, x) for
all t ∈ [0, tmax) and all x ∈ R3, then

ρ(0, x) ∇(cid:18) ∂ℓ

∂tℓ (V1 − V2)(cid:19)(0, x) = 0

(8)

for all ℓ > 0 and all x ∈ R3. If the nodal set {x ∈ R3 :
ρ(0, x) = 0} has zero volume, then

∂ℓ
∂tℓ (V1 − V2)(0, x) = cℓ

for all ℓ > 0 and all x ∈ R3.
every ﬁxed x ∈ R3, then

If in addition V1(t, x) − V2(t, x) is analytic in t for

V1(t, x) − V2(t, x) = C(t) =Xℓ>0

cℓ
ℓ!

tℓ.

We emphasize that only the time-analyticity of V1 −
V2 is required in order to get the Runge-Gross result.
It is not needed that Ψ(t) is itself time-analytic, as is
sometimes stated in the literature. We now sketch a proof
of the theorem. The reasoning is essentially the same as
in [4], although we do not use the current density j(t, x).

Proof of Theorem 3. Let ϕ(x) be an arbitrary (diﬀeren-
tiable and bounded) function. We clearly have

ZR3

ρm(t, x)ϕ(x)dx = NhΨm(t)|ϕ(x1)Ψm(t)i .

The second time-derivative of this equation is given by

6

d2

ρm(t, x)ϕ(x)dx

Since Ψm(t = 0, X) = Ψ0(X), the ﬁrst term does not
depend on m at time t = 0. A simple computation gives

dt2ZR3
= −NDΨm(t)(cid:12)(cid:12)(cid:12)[H0, [H0, ϕ(x1)]]Ψm(t)E
+ NDΨm(t)(cid:12)(cid:12)(cid:12)[Vm(t, x1), [∆x1 , ϕ(x1)]](cid:1)Ψm(t)E .
(cid:2)Vm(t, x1), [∆x1 , ϕ(x1)](cid:3) = −2∇Vm(t, x1) · ∇ϕ(x1).
(ρ1 − ρ2)(t, x) ϕ(x) dx(cid:12)(cid:12)(cid:12)(cid:12)t=0
dt2ZR3
= −2NZR3
ZR3

Choosing now ϕ(x) = V1(0, x) − V2(0, x), we obtain

ρ(0, x)∇ϕ(x) · ∇ (V1(0, x) − V2(0, x)) dx .

ρ(0, x)|∇(V1 − V2)(0, x)|2 dx = 0 .

Assuming that ρ1(t, x) = ρ2(t, x) then yields

(9)

d2

0 =

Because ρ(0, x) > 0 we then ﬁnd that ∇(V1 − V2)(0, x)
must be zero at every point where ρ(0, x) does not vanish,
which implies (8) for ℓ = 0. This proves that V1(0, x) −
V2(0, x) is constant on each connected component of the
set {ρ(0, x) > 0}. If ρ(0, x) never vanishes, then clearly
In fact, this also holds if the
V1(0, x) − V2(0, x) = c0.
nodal set {ρ(0, x) = 0} has zero volume, since V1(0, x) −
V2(0, x) is continuous by assumption.
To obtain (8) for ℓ = 1 we take the third time-
derivative of hΨm(t)|ϕ(x1)Ψm(t)i at t = 0, which yields
d3
(10)

ρm(t, x)ϕ(x)dx(cid:12)(cid:12)(cid:12)(cid:12)t=0
dt3ZR3
= −N iDΨ0(cid:12)(cid:12)(cid:12)[H0, [H0, [H0, ϕ(x1)]]]Ψ0E
NXj=1DΨ0(cid:12)(cid:12)(cid:12)[Vm(0, xj), [H0, [H0, ϕ(x1)]]]Ψ0E
+ N iDΨ0(cid:12)(cid:12)(cid:12)[H0, [Vm(0, x1), [∆x1 , ϕ(x1)]]]Ψ0E
− 2NZR3

ρ(0, x)∇ϕ(x) · ∇

Vm(0, x) dx .

− N i

∂
∂t

The operator H0 and its (iterated) commutators with
ϕ(x1) and Vm(0, xj) are diﬀerential operators, that is,
they can be written in the form

D = X06|j1|,...,|jN |6M

fj1,...,jN (x1, ..., xN ) ∂j1

x1 ··· ∂jN
xN ,

(11)

with smooth coeﬃcient functions fj1,...,jN (x1, ..., xN ).

We now make the following observation: If D1, D2, D3

are such diﬀerential operators, then

hΨ0|D1[V1(0, xj), D2]D3Ψ0i

= hΨ0|D1[V2(0, xj), D2]D3Ψ0i ,

(12)

for any j = 1, . . . , N . As commuting V1(xj ) with a dif-
ferential operator yields a derivative of V1(xj ) multiplied
(from the left and right) by diﬀerential operators, it suf-
ﬁces to show that

A delta potential in 1D

We now study the case of

(13)

H0 = −

d2
dx2 + λδ(x)

with x ∈ R. The domain of H0 is the set of square-
integrable functions ψ(x) such that (cf. [23])

7

(14)

heD1Ψ0|∇(V1 − V2)(0, xj )eD2Ψ0i = 0

for any such diﬀerential operators eD1, eD2. By (9) and

continuity of V (0, x), the open set of points where
∇(V1 − V2)(0, x) is non-zero is contained in the nodal
set of ρ(0, x). Because we are considering either fermions
or bosons, ρ(0, x) = 0 implies that Ψ0(x1, . . . , xN ) = 0

not vanish, so (13) holds.

if xj = x for some j = 1, .., N . Therefore, eD1Ψ0(X) =
eD2Ψ0(X) = 0 on the set where ∇(V1 − V2)(0, xj) does

Now, using the observation (12) we may replace
V1(0, x) by V2(0, x) in all the commutator expressions
arising in the third time-derivative (10). When taking
the diﬀerence, these terms then cancel and only the last
term remains:

|ψ′′(x)|2 dx and Z 0
Z ∞
0
ψ(0−) = ψ(0+),
ψ′(0+) − ψ′(0−) = λψ(0).



−∞ |ψ′′(x)|2 dx are ﬁnite

Theorem 4 (Potentials for a delta interaction). Assume
that λ 6= 0 and let V (x) a be real-valued, measurable func-
tion on R. Then, V satisﬁes condition (3) if and only if
V has inﬁnitely many continuous and bounded derivatives
on R and at the origin satisﬁes

0 =

d3

dt3ZR3
= −2NZR3

(ρ1 − ρ2)ϕ(t, x) dx(cid:12)(cid:12)(cid:12)(cid:12)t=0

ρ(0, x)∇ϕ(x) · ∇

∂
∂t

(V1 − V2) (0, x) dx .

This gives (8) with ℓ = 1 by choosing ϕ(x) =
∂
∂t (V1 − V2) (0, x). From this we deduce that an equation
like (12) also holds for the time-derivatives ∂
∂t V1(0, x),
∂
∂t V2(0, x), and this allows us to obtain (8) for ℓ = 2, and
recursively for all ℓ.

∂ℓ

If the nodal set of ρ(0, x) has zero volume, then
∂tℓ (V1 − V2)(0, x) equals some constant cℓ. Finally, if
V1(t, x) − V2(t, x) is analytic in t, then the Taylor series

cℓ
ℓ!

Xℓ>0

tℓ = V1(t, x) − V2(t, x)

converges, and is clearly independent of x.

IV. SINGULAR POTENTIALS

A. Singular potentials in one-body operators

dk
dxk V (0) = 0,

for all k > 1.

With our deﬁnition (7), the set V of allowed potentials
then only contains functions V (t, x) that are smooth in
space-time and satisfy (∂k/∂xk)V (t, 0) = 0 for every t ∈
[0, tmax) and k > 1.

0

) for every ψ ∈ D(H k

Proof. Suppose V is smooth, with bounded derivatives
that all equal zero at x = 0. By Lemma 1, the valid-
ity of condition (3) is equivalent to the statement that
V ψ ∈ D(H k−1
0 ). With the given
conditions on V ,
it is clear that V ψ has 2k square-
integrable derivatives (on (0,∞) and (−∞, 0)) if ψ does.
Also, (V ψ)(j)(0) = V (0)ψ(j)(0) for all j < 2k, so V ψ sat-
isﬁes the same boundary conditions as ψ at x = 0. This
shows that V ψ ∈ D(H k
0 ) and thus
that (3) holds.

0 ) for every ψ ∈ D(H k

Now assume condition (3) holds. Since functions in
D(H k
0 ) are diﬀerentiable away from zero, one easily de-
duces that V is too. Boundedness of these derivatives
follows from the fact that V , viewed as an operator, is
continuous (cf. the proof of Lemma 1). From the bound-
ary conditions it is clear that the derivatives of ψ must
have well-deﬁned limits from the left and right at zero,
and this again translates to V .

A function ψ ∈ I, which is in D(H k

fulﬁll inﬁnitely many boundary conditions of the form

0 ) for every k, must

In this section, we consider the case N = 1 with a
singular potential V0. By looking at two examples (ﬁrst
V0(x) = λδ(x) in 1D, and second the hydrogen atom), we
ﬁnd that all the derivatives of the potentials V (t, x) in
V must vanish at the singularities of V0. In other words,
these potentials are so ﬂat that they hardly inﬂuence the
dynamics close to the singularities. We see that the pres-
ence of a singular potential V0 has considerably reduced
the set V.

(

ψ(2j)(0−) = ψ(2j)(0+) ,
ψ(2j+1)(0+) − ψ(2j+1)(0−) = λψ(2j)(0) .

(15)

Then, V ψ must then satisfy the same conditions, for ev-
ery ψ that satisﬁes (15). As we will now see, this implies
that V (k)(0) = 0 for every k > 1. Consider the condi-
tions (15) for V ψ and j = 0. Using continuity of ψ at
zero, the ﬁrst condition yields continuity of V . In view

of the boundary condition for ψ′, the second condition
implies

0 = (V ψ)′(0+) − (V ψ)′(0−) − λ(V ψ)(0)
= ψ(0)(cid:0)V ′(0+) − V ′(0−)(cid:1) ,

so V ′ is also continuous at zero. Now let j > 1 and
assume we have continuity of V (k) at x = 0 for k 6 2j− 1
and V (k)(0) = 0 for 0 < k < 2j − 1. Then

(V ψ)(2j)(0+) = V (0)ψ(2j)(0) + V (2j)(0+)ψ(0)

+ 2jV (2j−1)(0)ψ′(0+) .

Using the boundary condition for ψ′, the continuity of
(V ψ)(2j), which is the ﬁrst condition in (15), gives

2jλV (2j−1)(0) = V (2j)(0−) − V (2j)(0+) .

(16)

Now take ψ satisfying (15) with 0 = ψ(0) = ψ′(0) but
ψ′′(0) 6= 0. Then

(V ψ)(2j+2)(0+) =(cid:18)2j + 2
+(cid:18)2j + 2

3 (cid:19)V (2j−1)(0)ψ(3)(0+) + V (0)ψ(2j+2)(0)

2 (cid:19)V (2j)(0+)ψ′′(0)

and continuity of (V ψ)(2j+2) at x = 0 implies

2j
3

λV (2j−1)(0) = V (2j)(0−) − V (2j)(0+) .

Together with (16) this shows that V (2j−1)(0) = 0, and
that V (2j) is continuous at x = 0. With this information
we can use the second condition in (15), with arbitrary
ψ ∈ I, to obtain

2jλV (2j)(0) = V (2j+1)(0−) − V (2j+1)(0+) .
same

The
(V ψ)(2j+3)(0−), with 0 = ψ(0) = ψ′(0), gives

calculation

(V ψ)(2j+3)(0+) −

for

(cid:18) 2j
3 − 1(cid:19) λV (2j)(0) =

2j
3

V (2j+1)(0−) − V (2j+1)(0+) ,

and this implies that V (2j)(0) = 0 and V (2j+1) is contin-
uous at x = 0. We can thus conclude that V (k)(0) = 0,
for all k > 1, by induction.

The hydrogen atom

In this section we extend the previous 1D consider-
ations to the case of the 3D hydrogen atom in radial
external potentials. The corresponding operator is

H0 = −∆ −

.

1
|x|

The equivalent of Theorem 4 is the following.

8

Theorem 5 (Radial potentials for the hydrogen atom).
Let V = V (r) be a smooth radial potential which satisﬁes
the condition (3) for the hydrogen atom. Then at the
origin V satisﬁes

dk
drk V (0) = 0,

for all k > 1.

The proof goes along the same lines as that of the corre-
sponding implication in Theorem 4, but the calculations
are more tedious and thus given in Appendix A. The
converse implication as in Theorem 4 also holds in this
case, and a similar result is probably true for a non-radial
potential, but we have not pursued in this direction.

One could think that V is so small because we chose I
too large. However, the proofs of Theorems 4 and 5 teach
us that this eﬀect occurs whenever I contains functions
with some non-vanishing derivatives at the singularity.
This property holds for eigenfunctions of H0, and the
proof in Appendix A uses this for the ﬁrst two eigenfunc-
tions only.

The set V characterized by Theorems 4 and 5 is a sub-
set of the one we considered in Section III, because it
contains the additional hypothesis that derivatives must
vanish at x = 0 (respectively r = 0 for hydrogen). For
this set one can prove a Runge-Gross uniqueness theorem
for N = 1, or N > 1 with w = 0, along the same lines
as Theorem 3. We omit the details, as this theorem for
non-interacting systems and a small set V seems to be of
limited interest.

B. The two-body Coulomb interaction

In the previous section we have considered singular
one-body operators and we have discovered that the set
V only contains functions that are very ﬂat at the singu-
larity. In the two-body case, the situation is even worse.
There is essentially no way for an external potential to
avoid the singularity of the two-body electronic repul-
sion. For the singlet state Helium atom, we are actually
able to prove that only constants remain in V, even if we
only assume that D(H k

0 ) for k 6 4.

V ) = D(H k

Theorem 6 (Potentials for two electrons). Let

1

H0 = −∆x1 − ∆x2 +

|x1 − x2|
be the Hamiltonian for two particles,
restricted to
permutation-symmetric, square-integrable functions. Let
V (x) be an external potential that has 6 continuous
bounded derivatives. If D(H 4
0 ), then V is con-
stant.

V ) = D(H 4

The idea is to work in relative and center of mass co-
ordinates and to use arguments as in the one-body case,
in the direction v = x1 − x2. We can then prove that
∆V (u) = 0

∆v(V (u + v/2) + V (u − v/2))|v=0 =

1
2

for all u ∈ R3, and this implies that V is constant. The
details of the proof are given in Appendix B below. A
similar result holds for triplet states but we will not treat
this in detail. Also, we believe that the same result holds
for N electrons in appropriate symmetry classes.

This shows that high-order Taylor expansions cannot
be employed for Coulomb systems, and a diﬀerent route
has to be found. By a closer investigation of the time
derivatives, we can get a weaker Runge-Gross Theorem
that is still somewhat reasonable for practical purposes.
More precisely, for smooth external potentials we can
show that the ﬁrst 4 time derivatives of V1 − V2 are con-
stant if the densities match.
Theorem 6 suggests that it is not possible to diﬀeren-
tiate ρ(t, x) more than 3 times at t = 0. We are going
to use a trick that will allow us to diﬀerentiate it 5 times
(but probably not more). The trick is to take a smooth
test function ϕ and to diﬀerentiate

ZR3

ρ(t, x) ϕ(x) dx,

as was used in the proof of the Runge-Gross Theorem
in Section III. This means that we are viewing the t-
derivatives of ρ(t, x) as distributions, or generalized func-
tions.

Theorem 7 (Finite-order Runge-Gross for Coulomb sys-
tems). Assume that w(x− y) = |x− y|−1 is the Coulomb
repulsion and that V0 is a ﬁxed external potential that has
6 bounded space derivatives. Let

H0 =

NXj=1

−∆xj + V0(xj ) + X16j<k6N

1

|xj − xk|

be the corresponding Hamiltonian for N particles, re-
stricted to square-integrable functions that are symmetric
(resp. anti-symmetric) under permutation of the parti-
cles. Let ﬁnally Ψ0 ∈ D(H 4
0 ).
Assume that V1(t, x) and V2(t, x) are two potentials
with 6 bounded space-time derivatives. Then the corre-
sponding densities ρ1(t, x) and ρ2(t, x) have 5 (resp. 6
in the anti-symmetric case) time-derivatives in the sense
that

fm(t) =ZR3

ρm(t, x) ϕ(x) dx

9

V. DISCUSSION AND OUTLOOK

In this paper, we have analyzed in detail the method
of Taylor expansions for the Runge-Gross Theorem. We
have introduced an abstract setting, based on two sets:
I for the initial conditions Ψ0’s and V for the time-
dependent external potentials V (t, x). The choice of
these sets guarantees that the density ρ(t, x) is diﬀer-
entiable in time inﬁnitely often and then the original
Runge-Gross approach works well. The main question
is to identify these two sets in practical situations.

Assuming that the potentials V0 and w are smooth, we
found in Section III that the sets I and V also consist of
smooth and bounded functions, without further restric-
tions. This is the correct mathematical setting for the
original Runge-Gross Theorem.

We then studied the case of singular potentials, as is
relevant for physical applications. We found that singu-
larities have very diﬀerent consequences for N = 1 and
N > 2.
In the one-particle case, the class of allowed
potentials V (t, x) is reduced to those that avoid the sin-
gularities, in the sense that all of their derivatives vanish
there. This is a very small set, whose physical interest is
debatable.

On the other hand, a singularity in the two-body po-
tential cannot be avoided by an external potential. For
the Coulomb interaction, the sole constraint on I and V
that one can diﬀerentiate many times in t already im-
poses that the external potential is constant, without
knowing anything about the density. Therefore, high-
order Taylor expansions are not the right tool to study
atoms and molecules in TDDFT. As we have shown in
Theorem 7, low-order expansions in t can be used, but of
course they yield only limited information.

A natural strategy to avoid Taylor-expansions is to use
the density ρ(t, x) for all times and not just at t = 0. One
way to make V (t, x) appear in an equation is to diﬀeren-
tiate ρ(t, x) only twice. This is possible for Coulomb in-
teractions as we have shown in Theorem 7. This gives an
implicit equation for V (t, x) [10–14]. Unfortunately, this
equation involves space derivatives of Ψ(t, X) of higher
order which are diﬃcult to control. Hence standard tech-
niques of functional analysis cannot be used in this con-
text and new ideas are needed.

is diﬀerentiable for every smooth ϕ(x).

If ρ1(t, x) = ρ2(t, x) for all t ∈ [0, tmax) and all x ∈ R3,

then

ρ(0, x) ∇(cid:18) ∂ℓ

∂tℓ (V1 − V2)(cid:19)(0, x) = 0

for all ℓ 6 3 (resp. ℓ 6 4 in the anti-symmetric case).

Due to its rather technical nature, we present the proof

of this result in Appendix C.

Acknowledgment. The authors acknowledge ﬁna-
cial support from the Danish Council
for Indepen-
dent Research (S.F., Sapere Aude Grant number DFF–
4181-00221) and the European Research Council (J.L.
and M.L.) under the European Community’s Seventh
Framework Programme (FP7/2007-2013 Grant Agree-
ment MNIQS 258023). M.L. would like to thank Eric
Canc`es for stimulating discussions. Part of this work was
carried out while S.F. was invited professor at Universit´e
de Paris-Dauphine.

with (ea,eb) = Tn(a, b), using the 2 × 2 matrix
2n(2n − 1)(cid:19) .

Tn =(cid:18)(2n − 1)(2n − 2)

0

1

Also, in the case n = 1, we evaluate

−h0f (0) = (1, 2) · (a, b) .

Thus, if f is given by (A2) for some n, then
2 (1, 2)) · (a, b) .

hn
0 f (0) = (−1)n (T ∗

n ··· T ∗

One easily checks that

2 (1, 2) =(2n − 1)!

nXj=1

1

2j − 1

, (2n)! ,

T ∗
n ··· T ∗

by recursion.

10

(A3)

(A4)

Now, by the induction hypothesis, V ψ(r) has a Taylor

expansion of the form

v

V ψ(r) =

+(cid:18)

3!(2k − 2)!
u
4
3!(2k − 1)! −
3

r2k+1

v

4!(2k − 2)!(cid:19) r2k+2 + O(r2k+3),

where (v, u) = (V (2k−2)(0), V (2k−1)(0)). Using this, to-
gether with formula (A4) for n = k + 1, the left hand side
of the ﬁrst equation in (A1) becomes (after multiplication
by (−1)k+1)

Similarly, the second equation in (A1) yields

(A1)

The determinant of this system simpliﬁes to

Appendix A: Proof of Theorem 5

To prove this theorem it is suﬃcient to consider ra-
dial functions ψ that are elements of D(H k
0 ) for every k.
As is well known, multiplication by r is a unitary map
from radial square-integrable functions on R3 to square-
integrable functions on [0,∞). Under this transforma-
tion, the operator H0 becomes

h0 = −

d2
dr2 −

1
r

,

with domain D(h0) given by square-integrable functions
ψ(r) for which

|ψ′′(r)|2dr is ﬁnite [23]

Z ∞

0



ψ(0) = 0 .

0) it must satisfy hk−1

Thus, if ψ is in D(hk
0 ψ(0) = 0.
Now assume that V (r) is a potential that maps D(hk+1
)
to D(hk
0) for every k. For k = 2 this implies, using that
ψ(0) = 0 and h0ψ(0) = 0,

0

0 = h0V ψ(0) =(cid:16)[h0, V ]ψ(cid:17)(0) = −2V ′(0)ψ′(0) ,

and thus V ′(0) = 0. Now let

ψ(r) = 16

3 r(cid:16)e−r/2 − (1 − r/4)e−r/4(cid:17) .

This is just a multiple of the diﬀerence of the ground state
of h0 and the ﬁrst excited state, so it is certainly an ana-
lytic vector of h0. Observe also that ψ′(0) = ψ′′(0) = 0,
but ψ(3)(0) = 1 and (h0ψ)′(0) = −1 are diﬀerent from
zero. Now since V ψ(r) and V h0ψ(r) are elements of
D(hk
0), V (r) must satisfy the following equations, for ev-
ery k > 0,

(cid:16)hk+1
0 V ψ(cid:17)(0) = 0 ,

(cid:16)hk
0V h0ψ(cid:17)(0) = 0.

This is an inﬁnite system of linear equations for the
derivatives of V at r = 0. We will exploit that this
system is triangular in the pairs (A1). That is, we prove
that V (j)(0) = 0 by induction. Assume that we already
know that V (j)(0) = 0 for 1 6 j 6 2k − 3, as we do
for k = 2. Then the equations (A1) depend only on the
values of V (2k−2)(0) and V (2k−1)(0), and (A1) can be
written as A(V (2k−1)(0), V (2k−2)(0)) = 0, for some 2 × 2
matrix A, that we will now determine. We will show that
det(A) 6= 0, and so the derivatives of V need to be zero.
To obtain A from the equations (A1) ﬁrst note that if
f (r) is a smooth function with Taylor expansion at zero
given by

f (r) = ar2n−1 + br2n + O(r2n+1),

(A2)

then, assuming n > 2, −h0f has Taylor expansion,

−h0f (r) =ear2n−3 +ebr2n−2 + O(r2n−1) ,

+ u(cid:18)2k + 2
3 (cid:19) .

+ u2k = 0.

2j − 1 −

k −

kXj=1

4

4 (cid:19)
3(cid:18)2k + 2
2j − 1

1

1

v
(cid:18)2k + 1
3 (cid:19) k+1Xj=1
−v(2k − 1)
3 (cid:19)
(cid:18)2k + 1
>(cid:18)2k + 1

2
3

k(k + 1) +

2k

2k + 1 − 2

3 (cid:19)(cid:26) 2

3

k(k + 1) +

1

2j − 1
kXj=1
2k + 1 − 2k(cid:27) .

2k

This is strictly positive for k > 2, so we ﬁnd that
V (2k−2)(0) = V (2k−1)(0) = 0 is the only solution to (A1).
This completes the proof.

Appendix B: Proof of Theorem 6

We express the operator HV with respect to the rela-
tive coordinate v = x1−x2 and center of mass coordinate
u = 1

2 (x1 + x2) :

HV = − 1

2 ∆u − 2∆v +

1
|v|

+ V (cid:0)u + 1

2 v(cid:1) + V (cid:0)u − 1
2 v(cid:1) .

For simplicity, we will denote

derivatives in this variable satisfy (B1). We deduce that

11

W (u, v) = V (cid:0)u + 1

2 v(cid:1) + V (cid:0)u − 1
2 v(cid:1) .

The domain of HV equals that of the pure kinetic en-
ergy operator D(HV ) = D(T ) = D(H0), as follows from
the criterion discussed in Lemma 1. Although an ele-
ment ψ(u, v) of this space need not be continuous, it
can be restricted to the hyperplane v = 0 using the the-
ory of Sobolev, yielding a square integrable function of
u (see [19, Theorem IX.38]). As a consequence, ψ(u, v)
satisﬁes

lim
v→0|v| ψ(u, v) = 0.

(B1)

0 ) = D(H 4

If V is not constant, this property will lead to a contra-
diction to D(H 4
V ), because the latter implies
that H 3
V W ψ ∈ D(T ), but this diverges at v = 0 leading
to a non-zero limit in (B1).
We now give the details of this argument. First, we
will see that with the given conditions on V , D(H k) =
D(H k
0 ) holds for k = 2, 3. For k = 2 we need to show
that ψ ∈ D(T ) satisﬁes H0ψ ∈ D(H0) = D(T ) if and
only if HV ψ ∈ D(HV ) = D(T ). This follows from the
fact that W has two bounded derivatives and thus maps
D(T ) = D(H0) to itself.

V ) = D(H 2

The domain of the third power is given by those ψ for
0 ). An element ψ of D(H 3
0 )
0 W ψ(u, v) is square-integrable. Be-
V ) and W maps D(H0) to itself, we
0 ψ(u, v) are square-

which HV ψ ∈ D(H 2
V ) if H 2
is thus in D(H 3
cause D(H 2
0 ) = D(H 2
know that H0W H0ψ(u, v) and W H 2
integrable. Thus, we need to show that

(cid:2)H0, [H0, W ](cid:3)ψ =(cid:2)H0, [−2∆v − 1
=(cid:2) − 2∆v − 1
2 ∆u, [−2∆v − 1

2 ∆u, W ](cid:3)ψ
2 ∆u, W ](cid:3)ψ

v

− 4(∇vW ) ·

|v|3 ψ

is square integrable, for ψ ∈ D(H 3
0 ). For the ﬁrst term
this follows from the diﬀerentiability of W and the fact
that ψ ∈ D(T ) has two square-integrable derivatives. For
the second term, note that (∇vW )(u, 0) = 0, so (∇vW )·
v/|v|3 diverges like 1/|v|, i.e. like the Coulomb-potential,
which is well-deﬁned on D(T ). This shows that D(H 3
0 ) ⊂
D(H 3
V ) (the converse inclusion is shown in the same way
and plays no role in our argument).

Now let ψ(u, v) ∈ D(H 4

0 ) be a function of the form
f (u)g(v) with smooth f (u) and g(v) = e|v|/4 for |v| < 1.
This is clearly a possible choice, as g(v) is an eigenfunc-
tion of −2∆v + 1/|v| for |v| < 1, where the singularity
lies. We will see that all the functions of this type are
in D(H 4
V ) only if V is constant. Assume that this func-
tion ψ(u, v) is also an element of D(H 4
V ). We then have
HV ψ ∈ D(H 3
0 ) and can argue as in the previ-
ous step that

V ) = D(H 3

(cid:2)H0, [H0, W ](cid:3)ψ =(cid:2)H0, [−2∆v − 1

2 ∆u, W ](cid:3)ψ ∈ D(T ) .

Since the chosen ψ(u, v) is smooth in the variable u, it
is easy to see that terms in this commutator that involve

lim

v→0|v|(cid:2) − 2∆v +

1
|v|

, [−2∆v, W (u, v)](cid:3)ψ(u, v) = 0

must hold. This commutator evaluates to

4(∆∆W )ψ + 16(∇∆W ) · ∇ψ

+ 16 Tr(Hess(W )Hess(ψ)) − 4(∇W ) ·

v

|v|3 ψ ,

where all the derivatives are taken only in the variable
v.
If we choose an angular variable ω such that v =
|v|ω, the limit v → 0 certainly remains unchanged if we
average over this variable. The ﬁrst two terms disappear
in the limit because ψ, ∇ψ, and the derivatives of W are
bounded. We conclude that

lim
|v|→0

|v|

4πZ (cid:16)4 Tr(Hess(W )Hess(ψ))

− (∇W ) ·

v

|v|3 ψ(cid:17)dω = 0 .

(B2)

To calculate the integral of the second term, we perform
a Taylor expansion of ∇W (u, v) at v = 0 (where it van-
ishes), and ﬁnd
|v|

v

lim
|v|→0

4πZ (∇W ) ·

1

|v|3 ψ(u, v)dω
4πZ ψ(u, v) ∇W (u, v)
|v|
Z ω · Hess(W )(u, 0)ω dω

· ω dω

= lim
|v|→0
ψ(u, 0)

=

4π

= 1

3 f (u)∆vW (u, 0) ,

For the ﬁrst term in (B2) we use the explicit form

where we have used that

3 δij .

1

4πZ ωiωj dω = 1
Hess(ψ)ij = f (u)e|v|/4(cid:18) vivj
16|v|2 −

4|v|3 +
4πZ 4 Tr(Hess(W )Hess(ψ))dω

|v|

lim
|v|→0

vivj

to obtain

δij

4|v|(cid:19) ,

= f (u)∆vW (u, 0)
f (u)

|v|→0

− lim
3 f (u)∆vW (u, 0) .

= 2

4π Z ω · Hess(W )(u, v)ω dω

Since ∆vW (u, 0) = 1

2 ∆V (u) this adds up to the con-

clusion that
|v|

lim
|v|→0

4πZ (H 2

0 W ψ)(u, v) dω = 2

3 f (u)∆V (u) = 0.

Since the function f (u) was arbitrary, this means that
V must be harmonic. Since it was also assumed to be
bounded, V is constant.

Appendix C: Proof of Theorem 7

derivatives. The second commutator equals

12

(cid:2)|xℓ − xk|−1, [−∆xj , V (xj )](cid:3)

Taking the sum over j then gives

= 2 ∇xj|xℓ − xk|−1 · ∇xj V (xj )
= ∇xj V (xj) ·(cid:16)δjℓ
xℓ − xk
|xℓ − xk|3 − δjk
|xℓ − xk|3 ·(cid:16)∇V (xℓ) − ∇V (xk)(cid:17) .

xℓ − xk

xℓ − xk

|xℓ − xk|3(cid:17) .

Since V (x) has bounded second derivatives, the function
above is smaller in modulus than |xℓ − xk|−1 times a
bounded function. This implies that Ψ ∈ D(T ) mul-
tiplied by this function is square-integrable. Thus the
action of (C2) on Ψ ∈ D(H 3
0 ) yields a square-integrable
function, which implies Ψ ∈ D(H 3
V ) as argued in the
proof of Theorem 6.
It is obvious from these calcula-
tions that the same holds for the double commutator of
0 ) =

j=1 V (xj ) with HV and we conclude that D(H 3

PN

D(H 3

V ).

0

Now let Ψ(X) be anti-symmetric, so that it vanishes
if xi = xj. We will show that then |xi − xj|−2Ψ(X) is
square-integrable for Ψ ∈ D(T ). To see why this implies
that any Ψ ∈ D(H 3/2
) is an element of D(T 3/2), ﬁrst
write T = H0 −PN
j=1 V0(xj) −P16j<k6N |xj − xk|−1.
Then note that H0Ψ ∈ D(T 1/2), by deﬁnition, and
PN
j=1 V0(xj )Ψ ∈ D(T 1/2), by regularity of V0(x). We
are left with the interaction term, and it remains to
check that the partial derivatives of P16j<k6N |xj −
xk|−1Ψ(X) are square-integrable. The derivatives of
Ψ(X) deﬁne elements of D(T 1/2), and multiplication by
|xj − xk|−1 yields a square-integrable function (see [19,
p. 169]). Finally, if a derivative acts on |xj − xk|−1, the
resulting term is bounded in modulus by a constant times
|xj − xk|−2Ψ(X).
|xi −
xj|−2Ψ(X). We will prove this by showing an in-
equality for a function f (y) of the relative coordinate
y = xi − xj, which can then be integrated over the
remaining coordinates to obtain square-integrability of
|xi − xj|−2Ψ(X). Using anti-symmetry, we may write
f (y) = y
−1(∇f )(ty)dt, and the Minkowski inequal-
ity [21, Theorem 2.4] gives

Now, back to the square-integrability of

0 ) = D(H k

In this section we prove Theorem 7 by studying pre-
cisely for which k we have D(H k
V ). From The-
orem 6 we know that this does not hold for k > 4. To
obtain sharper results, we will use the concept of the do-
main of a half-integer power of an operator. For the case
we treat here, the following may serve as a deﬁnition.
The domain D(T 1/2) consists of those square-integrable
Ψ(X) for which

ZR3N |∇XΨ(X)|2dX =ZR3N |K|2|bΨ(K)|2dK

is ﬁnite. We have D(T 1/2) = D(H 1/2
deﬁne recursively D(H k+1/2
such that H0Ψ ∈ D(H j+1/2
gously for HV .

V ) and
) to be those Ψ ∈ D(H 1/2
)
) for every j < k, and analo-

) = D(H 1/2

0

0

0

0

Lemma 2. Let H0 be the Hamiltonian with Coulomb
interaction w(x − y) = 1/|x − y| of Theorem 7 and let
V (x) be a function with four bounded derivatives on R3.
If Ψ is symmetric or anti-symmetric, then

(cid:18) NXj=1(cid:2)H0, [H0, V (xj )](cid:3)Ψ(cid:19)(x1, . . . , xN )

(C1)

V ).

0 ) = D(H 3

is square-integrable for every Ψ ∈ D(H0) and we have
D(H 3
If Ψ is anti-symmetric, we additionally have that
|xi − xj|−2Ψ(x1, . . . , xN ) is square-integrable and Ψ ∈
D(T 3/2) for every Ψ ∈ D(H 3/2
) =
D(H 7/2

), as well as D(H 7/2

0

0

V ).

This lemma tells us that for a suﬃciently smooth po-
tential V (x), we have in the symmetric case D(H 3
0 ) =
D(H 3
In Theorem 6 we have shown that D(H 4
V ).
0 ) 6=
D(H 4
V ), except if V is constant. However, the same
proof also shows that D(H 7/2
In the
anti-symmetric case, we have D(H 7/2
V ) and
a reasoning similar to that of Theorem 6 shows that
D(H 4

) 6= D(H 7/2
V ).

) = D(H 7/2

V ) as well.

0

0

0 ) 6= D(H 4

Proof. First, calculate the commutator

NXj=1(cid:2)H0, [H0, V (xj)](cid:3)

=

NXj=1(cid:16) NXk=1(cid:2) − ∆xk + V0(xk), [−∆xj , V (xj )](cid:3)
+ X16ℓ<k6N(cid:2)|xℓ − xk|−1, [−∆xj , V (xj )](cid:3)(cid:17) .

(C2)

ZR3

|f (y)|2

2 ·R 1
|y|4 dy 6ZR3
6 Z 1

2

1

−1

4|y|2(cid:12)(cid:12)(cid:12)(cid:12)Z 1
−1(cid:18)ZR3

(∇f )(ty)dt(cid:12)(cid:12)(cid:12)(cid:12)
dy(cid:19)1/2

|(∇f )(ty)|2

dy

dt!2

.

4|y|2
Hardy’s inequality [19, p. 169] implies

When the ﬁrst term acts on Ψ(X) the result is square-
integrable, because Ψ ∈ D(T ) has two square-integrable

ZR3

|(∇f )(ty)|2

4|y|2

dy 6

3Xk,ℓ=1ZR3

t2|(∂yk ∂yℓf )(ty)|2dy .

Changing variables z = ty and performing the t-integral
then gives

ZR3

|f (y)|2

|y|4 dy 6 4ZR3

3Xk,ℓ=1

|(∂yk ∂yℓf )(z)|2dz .

This shows that |y|−2f (y) is square-integrable if f (y)
has two square-integrable derivatives, and thus |xi −
xj|−2Ψ(X) is square-integrable for Ψ ∈ D(T ).
mains to show that H 2
D(H 7/2
this is equivalent to showing that

V ), it re-
0 HV Ψ ∈ D(T 1/2) for all Ψ ∈
). By the arguments in the proof of Theorem 6

To prove the inclusion of D(H 7/2

) in D(H 7/2

0

0

∇xi

NXj=1(cid:2)H0, [H0, V (xj)](cid:3)Ψ(X)

(C3)

is square-integrable for any i 6 N . From the formulas
for the commutator we see that the only non-trivial terms
are

∇xℓ

xℓ − xk

|xℓ − xk|3 ·(cid:16)∇V (xℓ) − ∇V (xk)(cid:17)Ψ(X) ,

but these can be controlled by combining all the bounds
we have just discussed. Again, the argument for the con-
verse inclusion is the same.

Coming back to the proof of Theorem 7, we ﬁrst need

to show that

f (t) =ZR3

ρ(t, x) ϕ(x) dx

= NhΨ(t)|ϕ(x1)Ψ(t)i

has ﬁve (resp. six if Ψ0 is anti-symmetric) derivatives
for V (t, x) satisfying the conditions on Vm, m = 1, 2 of
Theorem 7. Since Ψ0 ∈ D(H 4
0 ), Lemma 2 implies that
Ψ0 ∈ D(H 3
V ), so we immediately have existence of three
derivatives of f (t). Using the (anti-)symmetry of Ψ0, the
third derivative equals

Let A denote the operator

d3

+ i

dt3 f (t) = NDΨ(t)(cid:12)(cid:12)(cid:12) −(cid:2) ˙V (t, x1), [H0, ϕ(x1)](cid:3)Ψ(t)E
NXj=1(cid:18)DHV Ψ(t)(cid:12)(cid:12)(cid:12) −(cid:2)HV , [HV , ϕ(xj )](cid:3)Ψ(t)E
−DΨ(t)(cid:12)(cid:12)(cid:12) −(cid:2)HV , [HV , ϕ(xj )](cid:3)HV Ψ(t)E(cid:19) .
NXj=1(cid:2)HV , [H0, ϕ(xj )](cid:3).
NXj=1(cid:2)HV , [HV , ϕ(xj )](cid:3) = −
dt3 f (t) = − 2ℑ(cid:16)hHV Ψ(t)|AΨ(t)i(cid:17)

− 2NDΨ(t)(cid:12)(cid:12)(cid:12)(cid:0)∇ ˙V (x1) · ∇ϕ(x1)(cid:1)Ψ(t)E

d3

(C4)

A is a symmetric operator, so we can write

A = −

13

Equation (C1) (with V = ϕ), together with diﬀerentia-
bility of V , implies that A maps functions in D(T ) to
square-integrable functions. The same obviously holds
for the ﬁrst four time-derivatives of A. This shows
that AΨ(t) has two time-derivatives which are square-
integrable functions. Consequently, the expression (C4)
can be diﬀerentiated twice in time, and f (t) ﬁve times.

Those terms in the ﬁfth derivative of f (t) that involve
a time-derivative of V are once again diﬀerentiable, by
the argument above. Using the symmetry of A, the re-
maining terms can be brought into the form

2ℑ(cid:0)hH 3

V Ψ(t)|AΨ(t)i − 3hH 2

(C5)
The second term is clearly diﬀerentiable because Ψ(t) ∈
D(H 3
0 ). To treat the ﬁrst term of (C5), note that, if
Ψ0 is anti-symmetric, AΨ0 is an element of D(T 1/2) =
D(H 1/2

V Ψ(t)|AHV Ψ(t)i(cid:1) .

V ) by (C3) and we can write

hH 3
V Ψ(t)|AΨ0i
= h(T + 1)−1/2H 3

V Ψ(t)|(T + 1)1/2AΨ0i .

The function (T + 1)−1/2H 3
V Ψ(t) is diﬀerentiable in t
with square-integrable derivative, as follows from the ar-
gument of Theorem 1 applied to the domain D(H 7/2
) of
HV (t) in the Hilbert space D(H 5/2
). Using this, one eas-
ily sees that the diﬀerence quotient for (C5) has a limit, in
the same way one proves the product-rule for the deriva-
tive. This shows that (C5) is diﬀerentiable, and hence
the sixth derivative of f (t) can be taken.

0

0

We have thus shown that we can take the ﬁrst ﬁve
(resp.
six) derivatives that were used in the proof of
Theorem 3 and we can follow that proof up to this or-
der. The argument goes through essentially unchanged,
though there are some subtleties regarding the regularity
of the functions involved that we comment on below.

First of all, it is important to remark that ρ(0, x) is a
continuous function, although Ψ0 might not be, because
the evaluation of Ψ0(x1, . . . , xN ) at x1 = x is a square-
integrable function of x2, . . . , xN that depends contin-
uously on x (this follows from [19, Theorem IX.38]).
Hence, the connected components of the set {ρ(0, x) > 0}
are open, and

Z ρ(0, x)|∇(V1 − V2)|2(x)dx = 0

implies that V1(x) − V2(x) is constant on every compo-
nent.
Finally we need to show that for k 6 3 (respectively

k 6 4 in the fermionic case)

t (V1 − V2)(x1)|t=0 · ∇ϕ(x1)(cid:1)Ψ0E ,

where fm(t) denotes the function f (t) with V = Vm,
using that for l < k

dk+2

dtk+2 (f1(t) − f2(t))(cid:12)(cid:12)(cid:12)t=0
= −2NDΨ0(cid:12)(cid:12)(cid:12)(cid:0)∇∂k
ZR3 |∇∂ℓ

t (V1 − V2)|ρ(0, x)dx = 0 .

In the proof of Theorem 3 this was shown using smooth-
ness of Ψ0, which is no longer given. We will now discuss
how to adapt these arguments for the fermionic case and
k = 4, which is the most diﬃcult case.

The term with no ‘explicit’ time-derivatives in f (6)

m , i.e.

the term without time-derivatives on Vm, is (cf. (C5))

gm(t) := 2ℜ(cid:8)hH 4

Vm Ψ|AmΨi − 4hH 3
+ 3hH 2

VmΨ|AmHVm Ψi
Vm Ψ|AmH 2

VmΨio.

Here Am denotes the operator A with V = Vm and the
term hH 4
Vm Ψ|AmΨi has to be understood in the sense of
the pairing between AmΨ(t) ∈ D(T 1/2) and the distri-
bution H 4
Vm Ψ(t), as was done in showing existence of the
sixth derivative above. The other pairings are standard
scalar products.

We will only analyze gm in detail and show that
g1(0) − g2(0) = 0. The other terms in f (6) have explicit
time derivatives and therefore fewer operators. There-
fore all scalar products are immediately understandable
as integrals and also the algebra is slightly simpler—due
to the fewer factors. However, the basic idea of the cal-
culation is the same.

Write W = V2 − V1 and observe that

R ρ(0, x)|∇W|(x)dx = 0, we have

since

Actually, even more is true, namely

|∇W|(xj )Ψ0 = 0.

NXj=1
V1 Ψ0(cid:12)(cid:12)(cid:12) +(cid:12)(cid:12)(cid:12)|∇W|(xj )∇H s
NXj=1(cid:16)(cid:12)(cid:12)(cid:12)|∇W|(xj )H s

V1 Ψ0(cid:12)(cid:12)(cid:12)(cid:17) = 0.

(C6)

with s ∈ {1, 2, 3}. Of course, a similar identity holds
for V2, and one can even have mixed products of the
two operators. As in the the proof of Theorem 3, these
identities follow from the fact that HV1 is a local operator,
so that H s
V1Ψ0 = 0 on the open set where ∇W does not
vanish. Here we used that H s
V1 Ψ0) deﬁnes
a function (in contrast to a distribution).

V1 Ψ0 (and ∇H s

Similarly to (C6) we have

14

and

[W, Am]Φ = 0,

(C8)

for Φ = Ψ0 orQk

s=1 HVjs Ψ0, with k 6 3 and js ∈ {1, 2}.
The equality (C7) allows us to replace A2 by A1—which
we abbreviate by A—everywhere in g2(0). Notice also
that using (C6) we get for k 6 4,

H k

V2 Ψ0 = (HV1 + W )kΨ0 =Xj6k(cid:18)k

j(cid:19)W jH k−j

V1

Therefore,

Ψ0.

V1 Ψ0|AΨ0i

g2(0) − g1(0)

V1 + 3W H 2

V1 Ψ0|AΨ0i + 6hW 2H 2

= 2ℜn4hW H 3
+ 4hW 3HV1 Ψ0|AΨ0i + hW 4Ψ0|AΨ0i
− 4h(H 3
− 4h(3W H 2
+ 3h(2W HV1 + W 2)Ψ0|A(H 2
+ 3hH 2

V1Ψ0|A((2W HV1 + W 2)Ψ0io.

V1 + 3W 2HV1 + W 3)Ψ0|AW Ψ0i

V1 + 3W 2HV1 + W 3)Ψ0|AHV1 Ψ0i

V1 + 2W HV1 + W 2)Ψ0i

At this point notice that all involved vectors belong to the
Hilbert space L2, i.e. there are no distributions anymore.
A tedious calculation shows that this term vanishes. Let
us only argue that the terms with four powers of W can-
cel, the argument for the others being similar (Notice
though, that for the other powers of W , we use the fact
that we take the real part to reach the conclusion). I.e.
we will prove that

0 = ℜnhW 4Ψ0|AΨ0i − 4hW 3Ψ0|AW Ψ0i

+ 3hW 2Ψ0|AW 2Ψ0io.

But this is easy, since using (C8), we get

hW 4Ψ0|AΨ0i − 4hW 3Ψ0|AW Ψ0i + 3hW 2Ψ0|AW 2Ψ0i
= (1 − 4 + 3)hW 2Ψ0|AW 2Ψ0i = 0.

A1Φ = A2Φ,

(C7)

This concludes the proof of Theorem 7.

[1] W. Kohn and L. J. Sham, Phys. Rev. (2) 140, A1133

(1965).

[2] M. Levy, Proc. Natl. Acad. Sci. USA 76, 6062 (1979).
[3] E. H. Lieb, Int. J. Quantum Chem. 24, 243 (1983).
[4] E. Runge and E. K. U. Gross, Phys. Rev. Lett. 52, 997

(1984).

Advanced Course, Theoretical and Mathematical Physics
(Springer, 2011).

[6] N. T. Maitra, T. N. Todorov, C. Woodward,

and

K. Burke, Phys. Rev. A 81, 042525 (2010).

[7] Z.-h. Yang, N. T. Maitra, and K. Burke, Phys. Rev. Lett.

108, 063003 (2012).

[5] E. Engel and R. Dreizler, Density Functional Theory: An

[8] Z.-h. Yang and K. Burke, Phys. Rev. A 88, 042514

15

(2013).

[9] R. van Leeuwen, Phys. Rev. Lett. 82, 3863 (1999).

[10] M. Ruggenthaler, M. Penz, and D. Bauer, J. Phys. A,

Math. Theor. 42, 425207 (2009).

[11] M. Ruggenthaler and R. van Leeuwen, EPL (Europhysics

Letters) 95, 13001 (2011).

[12] M. Penz and M. Ruggenthaler, J. Phys. A, Math. Theor.

44, 335208 (2011).

[13] M. Ruggenthaler, K. J. H. Giesbertz, M. Penz,
R. van Leeuwen, Phys. Rev. A 85, 052504 (2012).

and

[14] M. Ruggenthaler, M. Penz, and R. van Leeuwen, J. Phys.

Condens. Matter 27, 203202 (2015).

[15] E. Davies, Spectral Theory and Diﬀerential Operators,
Cambridge studies in advanced mathematics, Vol. 42
(Cambridge University Press, Cambridge, 1995) pp.
x+182.

[16] M. Reed and B. Simon, Methods of Modern Mathematical

Physics. I. Functional Analysis (Academic Press, 1972).
[17] E. Nelson, Ann. of Math. Second Series, 70, 572 (1959).
[18] T. Kato, Trans. Amer. Math. Soc. 70, 195 (1951).
[19] M. Reed and B. Simon, Methods of Modern Mathemati-
cal Physics. II. Fourier Analysis, Self-Adjointness (Aca-
demic Press, New York, 1975) pp. xv+396.

[20] T. Kato, J. Math. Soc. Japan 5, 208 (1953).
[21] E. H. Lieb and M. Loss, Analysis, 2nd ed., Graduate
Studies in Mathematics, Vol. 14 (American Mathemati-
cal Society, Providence, RI, 2001) pp. xxii+346.

[22] H. Brezis, Functional Analysis, Sobolev Spaces and Par-
tial Diﬀerential Equations, Universitext ed. (Springer,
2010).

[23] Here, ψ′′ denotes the weak derivative (cf. [22]) of ψ on the
respective interval. Evaluation of a function at 0± stands
for its limit at zero from the right (0+) respectively left
(0−). These limits exist for ψ and ψ′, given that ψ′′ is
square-integrable [22, Theorem 8.2].

