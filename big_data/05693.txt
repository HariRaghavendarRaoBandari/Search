6
1
0
2

 
r
a

 

M
7
1

 
 
]

.

R
P
h
t
a
m

[
 
 

1
v
3
9
6
5
0

.

3
0
6
1
:
v
i
X
r
a

Reward Algorithms

for Semi-Markov Processes

Dmitrii Silvestrov1 and Raimondo Manca2

Abstract: New algorithms for computing power moments of hitting times
and accumulated rewards of hitting type for semi-Markov processes. The al-
gorithms are based on special techniques of sequential phase space reduction
and recurrence relations connecting moments of rewards. Applications are
discussed as well as possible generalizations of presented results and exam-
ples.

Keywords: Semi-Markov process, Hitting time, Accumulated reward,

Power moment, Phase space reduction, Recurrent algorithm.

2010 Mathematics Subject Classiﬁcation: Primary: 60J10, 60J22, 60J27,

60K15; Secondary: 65C40.

1. Introduction

In this paper, we study recurrent relations for power moments of hitting
times and accumulated rewards of hitting type for semi-Markov processes and
present eﬀective algorithms for computing these moments. These algorithms
are based on procedures of sequential of phase space reduction for semi-
Markov processes.

Hitting times are often interpreted as transition times for diﬀerent stochas-
tic systems describing by Markov-type processes, for example, occupation
times or waiting times in queuing systems, life times in reliability models,
extinction times in population dynamic models, etc. We refer to works by
Korolyuk, Brodi and Turbin (1974), Kovalenko (1975), Korolyuk and Turbin
(1976, 1978), Courtois (1977), Silvestrov (1980b), Anisimov, Zakusilo and
Donchenko (1987), Ciardo, Raymonf, Sericola and Trivedi (1990), Kovalenko,
Kuznetsov and Pegg (1997), Korolyuk, V.S. and Korolyuk, V.V. (1999),
Limnios and Opri¸san (2001, 2003), Barbu, Boussemart and Limnios (2004),

1Department of Mathematics, Stockholm University, SE-106 81 Stockholm, Sweden.

Email address: silvestrov@math.su.se

2Department of Methods and Models for Economics, Territory and Finance, University

“La Sapienza”, 00161 Rome, Italy, E-mail address: raimondo.manca@uniroma1.it

1

Yin and Zhang (2005, 2013), Janssen and Manca (2006, 2007), Anisimov
(2008), Gyllenberg and Silvestrov (2008), D’Amico, Petroni and Prattico
(2013), and Papadopoulou (2013).

In ﬁnancial and insurance applications, the hitting times for semi-Markov
processes can be also interpreted as rewards accumulated up to some hitting
terminating time for a ﬁnancial or insurance contract. We refer here to
works by D’Amico, Janssen and Manca (2005), Janssen and Manca (2006,
2007), Stenberg, Manca and Silvestrov (2006, 2007), Biﬃ, D’Amigo, Di Biase,
Janssen, Manca and Silvestrov (2008), Silvestrov, Silvestrova and Manca
(2008), D’Amico and Petroni (2012), Papadopoulou, Tsaklidis, McClean and
Garg (2012), D’Amico, Guillen and Manca (2013), and D’Amico, Petroni and
Prattico (2015).

Moments of hitting times also play an important role in limit and ergodic
theorems for Markov type processes. As a rule, the ﬁrst and second order
moments are used in conditions of theorems, higher order moments in rates
of convergence and asymptotical expansions. We refer here to works by
Silvestrov (1974, 1980b, 1994, 1996), Korolyuk and Turbin (1976, 1978),
Korolyuk, V. S. and Korolyuk, V. V. (1999), Koroliuk and Limnios (2005),
Anisimov (2008), Gyllenberg and Silvestrov (2008), Hunter (2005), Yin and
Zhang (2005, 2013), Silvestrov and Drozdenko (2006) and Silvestrov, D. and
Silvestrov, S. (2015).

Recurrent relations, which link power moments of hitting times for Markov
chains have been ﬁrst obtained for Markov chains by Chung (1954, 1960).
Further development have been achieved by Lamperty (1963), Kemeny and
Snell (1961a, 1961b), and Pitman (1974a, 1974b, 1977), Silvestrov (1980a,
1980b). Similar relations as well as description of these moments as minimal
solutions of some algebraic or integral equations were considered for Markov
chains and semi-Markov processes with discrete and arbitrary phase spaces
by Cogburg (1975), Nummelin (1984), and Silvestrov (1980b, 1983a, 1983b,
1996) and Silvestrov, Manca and Silvestrova (2014). Analogous results for
mixed power exponential moments of ﬁrst hitting times for semi-Markov pro-
cesses have been obtained in Gyllenberg and Silvestrov (2008).

The paper includes ﬁve sections.

In Section 2, we introduce Markov
renewal processes, semi-Markov processes and deﬁne hitting times and accu-
mulated rewards of hitting type. We also present basic stochastic relations
and recurrent systems of linear equations for power moments of these random
functionals. In Section 3, we describe a procedure of phase space reduction
for semi-Markov processes and formulas for computing transition character-

2

istics for reduced semi-Markov processes, We also prove invariance of hitting
times and their moments with respect to the above procedure of phase space
reduction. In Section 4, we describe a procedure of sequential phase space
reduction for semi-Markov process and derive recurrent formulas for comput-
ing power moments of hitting times for semi-Markov processes. In Section
5, we present useful generalizations of the above results to real-valued and
vector accumulated rewards of hitting type, general hitting times with hit-
ting state indicators, place-dependent and time-dependents hitting times and
accumulated rewards of hitting type and give a numerical example for the
corresponding recurrent algorithms for computing power moments of hitting
times and accumulated rewards of hitting type for semi-Markov processes.

2. Semi-Markov processes and hitting times

In this section, we introduce Markov renewal processes and semi-Markov
processes. We deﬁne also hitting times and accumulated rewards of hitting
times, and give basic recurrent system of linear equations for their power
moments, which are the main objects of our studies.

2.1. Markov renewal processes and semi-Markov processes. Let
X = {0, . . . , m} and (Jn, Xn), n = 0, 1, . . . be a Markov renewal process, i.e.,
a homogeneous Markov chain with the phase space X × [0, ∞), an initial
distribution ¯p = hpi = P{J0 = i, X0 = 0} = P{J0 = i}, i ∈ Xi and transition
probabilities,

Qij(t) = P{J1 = j, X1 ≤ t/J0 = i, X0 = s}, (i, s), (j, t) ∈ X × [0, ∞).

(1)

In this case, the random sequence ηn is also a homogeneous (embedded)

Markov chain with the phase space X and the transition probabilities,

pij = P{J1 = j/J0 = i} = Qij(∞), i, j ∈ X.

(2)

As far as random variable Xn is concerned, it can be interpreted as sojourn

time in state Jn, for n = 1, 2, . . ..

We assume that the following communication conditions hold:

A: X is a communicative class of states for the embedded Markov chain Jn.

We also assume that the following condition excluding instant transitions

holds:

3

B: Qij(0) = 0, i, j ∈ X.

Let us now introduce a semi-Markov process,

J(t) = JN (t), t ≥ 0,

(3)

where N(t) = max(n ≥ 0 : Tn ≤ t) is a number of jumps in the time interval
[0, t], for t ≥ 0, and Tn = X1 + · · ·+ Xn, n = 0, 1, . . ., are sequential moments
of jumps, for the semi-Markov process J(t).

This process has the phase space X, the initial distribution ¯p = hpi =

P{J(0) = i}, i ∈ Xi and transition probabilities Qij(t), t ≥ 0, i, j ∈ X.

2.2. Hitting times and accumulated rewards of hitting type. Let

us also introduce moments of sojourn times,

e(r)
ij = EiX r

1I(J1 = j) =Z ∞

0

trQ(ε)

ij (dt), r = 0, 1, . . . , i, j ∈ X.

(4)

Here and henceforth, notations Pi and Ei are used for conditional proba-

bilities and expectations under condition J(0) = i.

Note that,

e(0)
ij = pij, i, j ∈ X.

(5)

We assume that the following condition holds, for some integer d ≥ 1:

Cd: e(d)

ij < ∞, i, j ∈ X.

The ﬁrst hitting time to state 0 for the semi-Markov process J(t) can be

deﬁned as,

U0

W0 = inf(t ≥ X1 : J(t) = 0) =

Xn,

(6)

Xn=1

where U0 = min(n ≥ 1 : Jn = 0) is the ﬁrst hitting time to state 0 for the
Markov chain Jn.

The random variable W0 can also be interpreted as a reward accumulated

on trajectories of Markov chain Jn up to its ﬁrst hitting to state 0.

The main object of our studies are power moments for the ﬁrst hitting

times,

Note that,

E(r)

i0 = EiW r

0 , r = 1, . . . , d, i ∈ X.

E(0)

i0 = 1, i ∈ X.

4

(7)

(8)

As well known, conditions A, B and Cd imply that,

E(r)

i0 < ∞, r = 1, . . . , d, i ∈ X.

(9)

In what follows, symbol Y d= Z is used to denote that random variables

or vectors Y and Z have the same distribution.

The Markov property of the Markov renewal process (Jn, Xn) implies that

following system of stochastic equalities takes place for hitting times,

( Wi,0

i ∈ X,

d= Xi,1I(Ji,1 = 0) +Pj6=0(Xi,1 + Wj,0)I(Ji,1 = j),

(10)

where: (a) Wi,0 is a random variable which has distribution P{Wi,0 ≤ t} =
Pi{W0 ≤ t}, t ≥ 0, for every i ∈ X; (b) (Ji,1, Xi,1) is a random vector, which
takes values in space X × [0, ∞) and has the distribution P {Ji,1 = j, Xi,1 ≤
t} = Qij(t), j ∈ X, t ≥ 0, for every i ∈ X; (c) the random variables Wi,0 and
the random vector (Ji,1, Xi,1) are independent, for every i ∈ X.

By taking expectations in stochastic relations (10) we get the following

system of linear equations for expectations of hitting times E(1)

i0 , i ∈ X,

i0 = e(1)

(cid:26) E(1)

i ∈ X.

i0 +Pj∈X,j6=0 e(1)

ij +Pj∈X,j6=0 pijE(1)

j0 ,

(11)

In general, taking moments of the order r in stochastic relations (10) we
i0 , i ∈ X, for

get the following system of linear equations for moments E(r)
r = 1, . . . , d,

where

i0 = f (r)

(cid:26) E(r)

i ∈ X,

i0 = e(r)
f (r)

i0 + Xj∈X,j6=0

j0 ,

i0 +Pj∈X,j6=0 pijE(r)
Xl=0 (cid:18)r

l(cid:19)e(r−l)

ij E(l)

r−1

j0 , i ∈ X.

(12)

(13)

The system of linear equation given in (12) has, for r = 1, . . . , d, the same
matrix of coeﬃcients I − P0, where I = kI(i = j)k is the unit matrix and
matrix P0 = kpijI(j 6= 0)k.

This is readily seen that matrix Pn

0 = kPi{U0 > n, Jn = j}k. Condition
A implies that Pi{U0 > n, Jn = j} → 0 as n → ∞, for i, j ∈ X and, thus,
det(I − P0) 6= 0.

5

Therefore, moments E(r)

i0 , i ∈ X are the unique solution for the system of

linear equation (12), for every r = 1, . . . , d.

free terms f (r)
for moments E(r)

These systems have a recurrent character, since, for every r01, . . . , d, the
j0 , j 6= 0, k = 1, . . . , r − 1), i ∈ X of the system (12)
j0 , j 6= 0, k = 1, . . . , r−1.
Thus, the systems given in (12) should be solved recurrently, for r =

i0 , i ∈ X are functions of moments E(k)

i0 = f (r)

i0 (E(k)

1, . . . , d.

This is useful to note that the above remarks imply that condition A can

be replaced by simpler hitting condition:

A0: Pi{U0 < ∞} = 1, i ∈ X.

Let denote matrix [I − P0]−1 = kgi0jk. The elements of this matrix have

the following probabilistic sense, gi0j = EiPU0

The recurrent formulas for moments E(r)

for r = 1, . . . , d,

n=1 I(Jn−1 = j), i, j ∈ X.

i0 , i ∈ X have the following form,

gi0jf (r)

j0 (E(k)

l0 , l 6= 0, k = 1, . . . , r − 1), i ∈ X.

(14)

E(r)

i0 =Xj∈X

This method of computing moments E(r)

i0 , i ∈ X requires to compute the

inverse matrix [I − P0]−1.

In this paper, we propose an alternative method, which can be consid-
ered as a stochastic analogue of Gauss elimination method for solving of the
recurrent systems of linear equations (12).

3. Semi-Markov processes with reduced phase spaces

In this section, we describe an one-step algorithm for reduction of phase
space for semi-Markov processes. We also give recurrent systems of linear
equations for power moments of hitting times for reduced semi-Markov pro-
cesses.

3.1. Reduced semi-Markov processes. Let us choose some state
k ∈ X and consider the reduced phase space kX = X \ {k}, with the state k
excluded from the phase space X.

Let us deﬁne the sequential moments of hitting the reduced space kX by

the embedded Markov chain Jn,

kVn = min(r > kVn−1, Jr ∈ kX), n = 1, 2, . . . , kV0 = 0.

(15)

6

Now, let us deﬁne the random sequence,

(kJn, kXn) =( (J0, 0)

for n = 0,

for n = 1, 2, . . . .

(16)

r= kVn−1+1 Xr)

(J kVn ,P kVn

This sequence is also a Markov renewal process with phase space X ×
[0, ∞), the initial distribution ¯p = hpi = P{J0 = i, X0 = 0} = P{J0 = i}, i ∈
Xi and transition probabilities,

kQij(t) = P{ kJ1 = j, kX1 ≤ t/ kJ0 = i, kX0 = s}

= Qij(t) +

∞

Xn=0

Qik(t) ∗ Q(∗n)

kk (t) ∗ Qkj(t), t ≥ 0, i, j ∈ X.

(17)

Here, symbol ∗ is used to denote the convolution of distribution functions
kk (t) is the n times convolution of the distribu-

(possibly improper), and Q(∗n)
tion function Qkk(t).

In this case, the Markov chain kJn has the transition probabilities,

kpij = kQij(∞) = P{ kJ1 = j, / kJ0 = i}

= pij +

∞

Xn=0

pikpn

kkpkj = pij + pik

pkj

1 − pkk

, i, j ∈ X.

(18)

Note that condition A implies that probabilities pkk ∈ [0, 1), k ∈ X.
The transition distributions for the Markov chain kJn are concentrated

on the reduced phase space kX, i.e., for every i ∈ X,

Xj∈ k X

kpij = Xj∈ k X
= Xj∈ k X

pkj

1 − pkk

pij + pik Xj∈ k X

pij + pik = 1.

(19)

If the initial distribution ¯p is concentrated on the phase space kX, i.e.,
pk = 0, then the random sequence (kJn, kXn), n = 0, 1, . . . can be considered
as a Markov renewal process with the reduced phase kX × [0, ∞), the initial
distribution k ¯p = h pi = P{kJ0 = i, kX0 = 0} = P{kJ0 = i}, i ∈ kXi and
transition probabilities kQij(t), t ≥ 0, i, j ∈ kX.

If the initial distribution ¯p is not concentrated on the phase space kX, i.e.,
pk > 0, then the random sequence (kJn, kXn), n = 0, 1, . . . can be interpreted
as a Markov renewal process with so-called transition period.

7

Let us now introduce the semi-Markov process,

kJ(t) = kJkN (t), t ≥ 0,

(20)

where kN(t) = max(n ≥ 0 : kTn ≤ t) is a number of jumps at time interval
[0, t], for t ≥ 0, and kTn = kX1 + · · · + kXn, n = 0, 1, . . . are sequential
moments of jumps, for the semi-Markov process kJ(t).

As follows from the above remarks, the semi-Markov process kJ(t), t ≥
0 has transition probabilities kQij(t), t ≥ 0, i, j ∈ X concentrated on the
reduced phase space kX, which can be interpreted as the actual “reduced”
phase space of this semi-Markov process kJ(t).

If the initial distribution ¯p is concentrated on the phase space kX, then
process kJ(t), t ≥ 0 can be considered as the semi-Markov process with the
reduced phase kX, the initial distribution k ¯p = h kpi = P{kJ1(0) = i}, i ∈ kXi
and transition probabilities kQij(t), t ≥ 0, i, j ∈ kX.

According to the above remarks, we can refer to the process kJ(t) as a

reduced semi-Markov process.

If the initial distribution ¯p is not concentrated on the phase space kX,
then the process kJ(t), t ≥ 0 can be interpreted as a reduced semi-Markov
process with transition period.

3.2. Transition characteristics for reduced semi-Markov pro-
cesses. Relation (18) implies the following formulas, for probabilities kpkj
and kpij, i, j ∈ kX,

(cid:26) kpkj = pkj

1−pkk

,

kpij = pij + pik kpkj = pij + pik

pkj

1−pkk

.

(21)

It is useful to note that the second formula in relation (21) reduces to the

ﬁrst one, if to assign i = k in this formula

Taking into account that kV1 is Markov time for the Markov renewal pro-
cess (Jn, Xn), we can write down the following system of stochastic equalities,
for every i, j ∈ kX,




kXi,1I( kJi,1 = j)

d= Xi,1I(Ji,1 = j)

+ (Xi,1 + kXk,1)I(Ji,1 = k)I( kJk,1 = j),

kXk,1I( kJk,1 = j)

d= Xk,1I(Jk,1 = j)

(22)

+ (Xk,1 + kXk,1)I(Jk,1 = k)I( kJk,1 = j),

8

where: (a) (Ji,1, Xi,1) is a random vector, which takes values in space X ×
[0, ∞) and has the distribution P {Ji,1 = j, Xi,1 ≤ t} = Qij(t), j ∈ X, t ≥ 0,
for every i ∈ X; (b) (kJi,1, kXi,1) is a random vector which takes values in the
space kX × [0, ∞) and has distribution P{kJi,1 = j, kXi,1 ≤ t} = Pi{kJ1 =
j, kX1 ≤ t} = kQij(t), j ∈ kX, t ≥ 0, for every i ∈ X; (c) (Ji,1, Xi,1) and
(kJk,1, kXk,1) are independent random vectors, for every i ∈ X.

Let us denote,

ke(r)

ij = Ei kX r

1 I( kJ1 = j) =Z ∞

0

tr

kQij(dt), r = 0, 1, . . . , i, j ∈ kX.

(23)

Note that,

ke(0)

ij = kpij, i ∈ X, j ∈ kX.

(24)

By taking moments of the order r in stochastic relations (22) we get, for
every i, j ∈ kX, the following system of linear equations for the moments
ke(r)

for r = 1, . . . , d,

kj , ke(r)

ij

ke(r)

kj = e(r)
ij = e(r)

ke(r)

kk

kj +Pr−1
l=0 (cid:0)r
ij +Pr−1
l=0 (cid:0)r

l(cid:1)e(r−l)
l(cid:1)e(r−l)

ik

ke(l)

kj + pkk ke(r)
kj ,
kj + pik ke(r)
kj ,

ke(l)

(25)

Relation (25) implies the following recurrent formulas for moments ke(r)
kj
ij , which should be used, for every i, j ∈ kX, recurrently for r =

and ke(r)
1, . . . , d,







ke(r)

kj = 1

ke(r)

ij = e(r)

ke(l)

kk

l(cid:1)e(r−l)
1−pkk(cid:0)e(r)
kj +Pr−1
l=0 (cid:0)r
l(cid:1)e(r−l)
ij +Pr−1
l=0 (cid:0)r
1−pkk(cid:0)e(r)
l(cid:1)e(r−l)
kj +Pr−1
l=0 (cid:0)r

ke(l)
kj

+ pik

kk

ik

kj(cid:1),
kj(cid:1),

ke(l)

(26)

It is useful to note that the second formula in relation (26) reduces to the

ﬁrst one, if to assign i = k in this formula.

3.3. Hitting times for reduced semi-Markov processes. Let us
assume that k 6= 0 and introduce the ﬁrst hitting time to state 0 for the
reduced semi-Markov process kJ(t),

kW0 = inf(t ≥ kX1 : kJ(t) = 0) =

kXn,

(27)

kU0

Xn=1

9

where kU0 = min(n ≥ 1 : kJn = 0) is the ﬁrst hitting time to state 0 by the
reduced Markov chain kJn.

Let also introduce moments,

kE(r)

i0 = Ei kW r

0 , r = 0, 1, . . . , d, i ∈ X.

Note that,

kE(0)

i0 = 1, i ∈ X.

(28)

(29)

The following theorem plays the key role in what follows.
Theorem 1. Conditions A, B and Cd assumed to hold for the semi-
Markov process J(t) also hold for the reduced semi-Markov process kJ(t), for
any state k 6= 0. Moreover, the hitting times W0 and kW0 to the state 0,
respectively, for semi-Markov processes J(t) and kJ(t), coincide, and, thus,
for every r = 1, . . . , d and i ∈ X,

E(r)

i0 = EiW r

0 = kE(r)

i0 = Ei kW r
0 .

(30)

Proof. Holding of conditions A and B for the semi-Markov process kJ(t)
is obvious. Holding of condition Cd for the semi-Markov process kJ(t) follows
from relation (26).

The ﬁrst hitting times to a state 0 are connected for Markov chains Jn

and kJn by the following relation,

U0 = min(n ≥ 1 : Jn = 0) = min(kVn ≥ 1 : kJn = j) = kVkU0,

(31)

where kU0 = min(n ≥ 1 : kJn = 0).

The above relations imply that the following relation holds for the ﬁrst

hitting times to state 0, for the semi-Markov processes J(t) and kJ(t),

U0

kVk U0

kU0

W0 =

Xn =

Xn =

kXn = kW0.

(32)

The equality for moments of the ﬁrst hitting times is an obvious corollary

of relation (32). (cid:3)

We can write down the recurrent systems of linear equations (12) for
i0 , i ∈ kX of the reduced semi-Markov process kJ(t),

moments kE(r)
which should be solved recurrently, for r = 1, . . . , d,

k0 and kE(r)

Xn=1

Xn=1

Xn=1




kE(r)
kE(r)

k0 = kf (r)
i0 = kf (r)

j0 ,

k0 +Pj∈ k X,j6=0 kpkj kE(r)
i0 +Pj∈ k X,j6=0 kpij kE(r)

10

j0 , i ∈ kX,

(33)

where

kf (r)

i0 = ke(r)

i0 + Xj∈ k X, j6=0

r−1

Xl=0 (cid:18)r

l(cid:19) ke(r−l)

ij

kE(l)

j0 , i ∈ X.

(34)

Theorem 1 makes it possible to compute moments E(r)

i0 , i ∈
X, r = 1, . . . , d in the way alternative to solving recurrent systems of linear
equations (12).

i0 = kE(r)

Instead of this, we can, ﬁrst, compute transition probabilities and mo-
ments of transition times for the reduced semi-Markov process kJ(t) using,
respectively, relations (21) and (26), and, then, by solving the systems of
linear equations (33) sequentially for r = 1, . . . , d.

Note that every system given in (12) has m equations for moments E(r)

X, i 6= 0 plus the explicit formula for computing moment E(r)
moments E(r)

i0 , i ∈ X, i 6= 0.

i0 , i ∈
00 as function of

While, every system given in (33) has, in fact, m − 1 equations for mo-
i0 , i ∈ kX, i 6= 0, plus two explicit formulas for computing moment

k0 as functions of moments kE(r)

i0 , i ∈ kX, i 6= 0.

ments kE(r)
kE(r)

00 and kE(r)

4, Algorithms of sequential phase space reduction

In this section, we present a multi-step algorithm for sequential reduction
of phase space for semi-Markov processes. We also present the recurrent
algorithm for computing power moments of hitting times for semi-Markov
processes, which are based on the above algorithm of sequential reduction of
the phase space.

4.1. Sequential reduction of phases space for semi-Markov pro-
cesses. In what follows, let i ∈ {1, . . . , m} and let ¯ki,m = hki,1, . . . , ki,mi =
hki,1, . . ., ki,m−1, ii be a permutation of the sequence h1, . . . , mi such that
ki,m = i, and let ¯ki,n = hki,1, . . . , ki,ni, n = 1, . . . , m be the corresponding
chain of growing sequences of states from space X.

Let us assume that p0 + pi = 1. Denote as ¯ki,0J(t) = J(t), the initial semi-
Markov process. Let us exclude state ki,1 from the phase space ¯ki,0
X = X
of semi-Markov process ¯ki,0J(t) using the time-space screening procedure de-
scribed in Section 3. Let ¯ki,1J(t) be the corresponding reduced semi-Markov
process. The above procedure can be repeated. The state ki,2 can be excluded
from the phase space of the semi-Markov process ¯ki,1J(t). Let ¯ki,2J(t) be the
corresponding reduced semi-Markov process. By continuing the above pro-

11

cedure for states ki,3, . . . , ki,n, we construct the reduced semi-Markov process
¯ki,nJ(t).

The process ¯ki,nJ(t) has, for every n = 1, . . . , m, the actual “reduced”

phase space,

¯ki,n

X = ¯ki,n−1

X \ {ki,n} = X \ {ki,1, ki,2, . . . , ki,n}.

(35)

The transition probabilities ¯ki,npki,n,j′, ¯ki,npi′j′, i′, j′ ∈ ¯kn

X, and the mo-

ki,n,j′, ¯ki,ne(r)

i′j′, i′, j′ ∈ ¯ki,n

ments ¯ki,ne(r)
X, r = 1, . . . , d are determined for the
semi-Markov process ¯ki,nJ(t) by the transition probabilities and the expec-
tations of sojourn times for the semi-Markov process ¯ki,n−1J(t), respectively,
via relations (21) and (26), which take the following recurrent forms, for
i′, j′ ∈ ¯ki,n

X, r = 1, . . . , d and n = 1, . . . , m,

¯ki,npki,n,j′ =

¯ki,n−1
1− ¯ki,n−1

pki,n,j′
pki,n,ki,n

,

¯ki,npi′j′

= ¯ki,n−1pi′j′

+ ¯ki,n−1pi′ki,n

¯ki,n−1
1− ¯ki,n−1

pki,n,j′
pki,n,ki,n ,

,




and

¯ki,ne(r)

ki,nj′ =

(36)

(37)




¯ki,ne(r)

i′j′ = ¯ki,n−1e(r)
¯ki,n−1
1− ¯ki,n−1

+

ki,nj′

ki,nki,n

1
1− ¯ki,n−1

pki,nki,n(cid:0)e(r)
l(cid:1) ¯ki,n−1e(r−l)
+Pr−l
l=0(cid:0)r
i′j′ +Pr−1
l=0 (cid:0)r
pki,nki,n(cid:0) ¯ki,n−1e(r)
l(cid:1) ¯ki,n−1e(r−l)
+Pr−l
l=0(cid:0)r

pi′ki,n

ki,nki,n

¯ki,n−1e(l)

ki,nj′(cid:1),

¯ki,n−1e(l)

ki,nj′

l(cid:1) ¯ki,n−1e(r−l)

i′ki,n

ki,nj′

¯ki,n−1e(l)

ki,nj′(cid:1).

4.2. Recurrent algorithms for computing of moments of hitting
times. Let us ¯ki,nW0 be the ﬁrst hitting time to state 0 for the reduced
semi-Markov process ¯ki,nJ(t) and ¯ki,nE(r)
X, r = 1, . . . , d
be the moments for these random variables.

i′0 = Ei′ ¯ki,nW r

0 , i′ ∈ ¯ki,n

By Theorem 1, the above moments of hitting time coincide for the semi-

Markov processes ¯ki,0J(t), ¯ki,1J(t), . . . , ¯ki,nJ(t), i.e., for n′ = 0, . . . , n,

¯kj,n′ E(r)

ki,n′ 0 = E(r)

ki,n′ 0, ¯kj,n′ E(r)

i′0 = E(r)

i′0 , i′ ∈ ¯ki,n

X, r = 1, . . . , d.

(38)

12

i′0 , i′ ∈ ¯ki,n

i,1, . . ., k′
Indeed, for every permutation ¯k′

Moreover, the moments of hitting times ¯kj,nE(r)

X, r =
1, . . . , d resulted by the recurrent algorithm of sequential phase space reduc-
tion described above, are invariant with respect to any permutation ¯k′
i,n =
hk′

i,ni of sequence ¯ki,n = hki,1, . . . , ki,ni.

ki,n0, ¯ki,nE(r)

i,n of sequence ¯ki,n, the corresponding
J(t) is constructed as the sequence of states
reduced semi-Markov process ¯k′
for the initial semi-Markov process J(t) at sequential moment of its hitting
into the same reduced phase space ¯k′
X =
X \ {ki,1, . . . , ki,n}. The times between sequential jumps of the reduced semi-
J(t) are the times between sequential hitting of the above
Markov process ¯k′
reduced phase space by the initial semi-Markov process J(t).

X = X \ {k′

i,n} = ¯ki,n

i,1, . . . , k′

i,n

i,n

i,n

This implies that the transition probabilities ¯ki,npki,nj′, ¯ki,npi′j′, i′, j′ ∈
X and the moments ¯ki,ne(r)
X, r = 1, . . . , d and, in

i′j′, i′, j′ ∈ ¯ki,n

ki,nj′, ¯ki,ne(r)

¯ki,n
sequel, moments ¯ki,nE(r)
n = 1, . . . , m, invariant with respect to any permutation ¯k′
¯ki,n.

ki,n0, ¯ki,nE(r)

i′0 , i′ ∈ ¯ki,n

X, r = 1, . . . , d are, for every
i,n of the sequence

Let us now choose n = m. In this case, the reduced semi-Markov process

¯ki,mJ(t) has the one-state phase space ¯ki,m

X = {0} and state ki,m = i.

In this case, the reduced semi-Markov process ¯ki,mJ(t) return to state 0
after every jump and hitting time to state 0 coincides with the sojourn time
in state ¯ki,mJ(0).

Thus, the transition probabilities,

¯ki,mpi0 = ¯ki,mp00 = 1.

Also, by Theorem 1, moments,

E(r)

i0 = ¯ki,mE(r)

i0 = ¯ki,me(r)

i0 , r = 1, . . . , d,

and

E(r)

00 = ¯ki,mE(r)

00 = ¯ki,me(r)

00 , r = 1, . . . , d.

(39)

(40)

(41)

The above remarks can be summarized in the following theorem, which
presents the recurrent algorithm for computing of power moments for hitting
times.

Theorem 2. Moments E(r)

00 , r = 1, . . . , d are given, for every i =
1, . . . , m, by formulas (40) – (41), where transition probabilities ¯ki,npki,n,j′,
¯ki,npi′j′, i′, j′ ∈ ¯kn
X, r = 1, . . . , d

X, and moments ¯ki,ne(r)

i′j′, i′, j′ ∈ ¯ki,n

ki,n,j′, ¯ki,ne(r)

i0 , E(r)

13

are determined, for n = 1, . . . , m, by recurrent formulas (36) – (37) and for-
mula (39). The moments E(r)
00 , r = 1, . . . , d are invariant with respect
to any permutation ¯ki,m of sequence h1, . . . , mi used in the above recurrent
algorithm.

i0 , E(r)

5. Generalizations and examples

In this section, we describe several variants for generalization of the re-
sults concerned recurrent algorithms for computing power moments of hitting
times and accumulated rewards of hitting type.

5.1. Real-valued accumulated rewards of hitting type. First, we
would like to mention that Theorems 1 and 2 can be generalized on the
model, where of the Markov renewal process (Jn, Xn), n = 0, 1, . . . has the
phase space X × R1, an initial distribution ¯p = hpi = P{J0 = i, X0 = 0} =
P{J0 = i}, i ∈ Xi and transition probabilities,

Qij(t) = P{J1 = j, X1 ≤ t/J0 = i, X0 = s}, (i, s), (j, t) ∈ X × R1.

(42)

In this case, we the random variable,

W0 =

Xn

U0

Xn=1

(43)

can be be interpreted as a reward accumulated on trajectories of Markov
chain Jn up to its ﬁrst hitting time U0 = min(n ≥ 1, Jn = 0) of this Markov
chain to the state 0.

Condition Cd should be replaced by condition:

˙Cd: Ei|X1|d < ∞, i ∈ X.

As well known, in this case moments ˙E(d)
All recurrent relations for moments E(r)

i = Ei|W0|d, i ∈ X are ﬁnite.
i = EiW r

0 , r = 1, . . . , d, i ∈ X,
given in Sections 3 – 4, as well as Theorems 1 and 2 take the same forms as
in the case of nonnegative rewards.

5.2. Vector accumulated rewards of hitting type. Second, we
would like to show, how the above results can be generalized on the case of
vector accumulated rewards.

For simplicity, let us consider the bivariate case, where the Markov re-
newal process (Jn, ¯Xn) = (Jn, (X1,n, X2,n)) = 0, 1, . . . has the phase space

14

X × R2, an initial distribution ¯p = hpi = P{J0 = i, ~X0 = (0, 0)} = P{J0 =
i}, i ∈ Xi and transition probabilities,

Qij(¯t) = P{J1 = j, ¯X1 ≤ ¯t/J0 = i, ¯X0 = s}, (i, ¯s), (j, ¯t) ∈ X × R2.

(44)

Here and henceforth symbol ¯u ≤ ¯v for vectors ¯u = (u1, u2), ¯v = (v1, v2) ∈

R2 means that u1 ≤ v1, u2 ≤ v2.

The vector accumulated reward ¯W0 = (W1,0, W2,0) is deﬁned as a bivariate

random vector with components,

Wl,0 =

U0

Xn=1

Xl,n, l = 1, 2.

(45)

Condition ˙Cd should be replaced by condition:

˙C′

d: Ei|Xl,1|d < ∞, l = 1, 2, i ∈ X.

In this case, moments ˙E(d)
Let us introduce mixed moments,

l,i = Ei|Wl,0|d < ∞, l = 1, 2, i ∈ X.

E(q,r)

i

= EiW q

1,0W r−q

2,0 , 0 ≤ q ≤ r ≤ d, i ∈ X.

(46)

Let us deﬁne random variables W0(a) = aW1,0+(1−a)W2,0, 0 ≤ a ≤ 1. By
n=1(aX1,n + (1 −a)X2,n) is also an accumulated reward
for the corresponding local rewards Xn(a) = aX1,n + (1 −a)X2,n, n = 1, 2, . . ..

deﬁnition, W0(a) =PU0

i (a) = EiW r(a), 0 ≤ a ≤ 1, r = 1, . . . , d, i ∈ X.

Let us denote E(r)
Let us assume that the moments of non-negative accumulated rewards
E(r)
r , p =
0, . . . , r, for every r = 1, . . . , d, i ∈ X using recurrent algorithms described in
Sections 2 – 4.

i (a) are found for r +1 values ap, p = 0, . . . , r, for example, for ap = p

Then, the following system of linear equations can be written down for

the correlation moments E(q,r)

i

, q = 0, . . . , r, for every r = 1, . . . , d, i ∈ X,

i (ap) =

nE(r)

r

q(cid:19)aq
Xq=0(cid:18)r

p(1 − ap)r−qE(q,r)

i

, q = 0, . . . , r.

(47)

It can be shown that the above linear system has the non-zero determi-
r , p = 0, . . . , r uniquely determine

i (ap), ap = p

nant. Thus, the moments E(r)
the mixed moments E(q,r)

i

, 0 ≤ q ≤ r, for every r = 1, . . . , d, i ∈ X.

15

5.3. General hitting times with hitting state indicators. Third,
the above results can be generalized on the case of more general hitting times,

WD =

Xn,

UD

Xn=1

(48)

where UD = min(n ≥ 1, Jn ∈ D), for some nonempty set D ⊂ X.

In this case main object of studies are power moments for the ﬁrst hitting

times with hitting state indicators,

E(r)

D,ij = EiW r

DI(JUD = j), r = 0, 1, . . . , d, j ∈ D, i ∈ X.

Note that,

E(0)

D,ij = Pi{JUD = j}, i ∈ X, j ∈ D.

(49)

(50)

As well known, conditions A, B and Cd imply that, for any nonempty

set D ⊂ X,

E(r)

D,ij < ∞, r = 1, . . . , d, i ∈ X, j ∈ D.

(51)

Note that the simpler condition A can, in fact, be replaced by a simpler

condition:

AD: Pi{UD < ∞} = 1, i ∈ X.

In this case, theorems, analogous of Theorems 1 and 2, take place, and
recurrent systems of linear equations and recurrent formulas analogous to
those given in Sections 2 – 4 can be written down.

let kE(r)

For example,

D,ij, r = 1, . . . , d, i ∈ X, j ∈ D be the moments
D,ij < ∞, r = 1, . . . , d, i ∈ X, j ∈ D computed for the reduced semi-

E(r)
Markov process kJ(t), for some k /∈ D.

The key recurrent systems of linear equations analogous to (33) take,
for every j ∈ D, nonempty set D ⊂ X and k /∈ D, the following form, for
r = 0, . . . , d,




where

D,j′j,

D,j′j, i ∈ kX,

kE(r)
kE(r)

D,kj = kf (r)
D,ij = kf (r)

D,kj +Pj′∈ k X\D kpkj′ kE(r)
D,ij +Pj′∈ k X\D kpij′ kE(r)
l(cid:19) ke(r−l)

Xl=0 (cid:18)r

r−1

ij′

ij + Xj′∈ k X\D

kf (r)

D,ij = ke(r)

kE(r)

D,j′j, i ∈ X.

16

(52)

(53)

The diﬀerence with the recurrent systems of linear equations (33) is that,
in this case, the corresponding system of linear equations for hitting proba-
bilities E(0)

D,ij, i ∈ X should also be solved.

Also, the corresponding changes caused by replacement of the hitting
state 0 by state j ∈ D and set kX\{0} by set kX\D sould be taken into account
when writing down systems of linear equations (52) instead of systems of
linear equations (33).

5.4. Place-dependent hitting times. Fourth, the above results can

be generalized on so-called place-dependent hitting times,

YG =

Xn,

UG

Xn=1

(54)

where UG = min(n ≥ 1 : (Jn−1, Jn) ∈ G), for some nonempty set G ⊂ X × X.
Note that set G can be represented in the form G = ∪i∈X {i} × Gi, where
Gi = {j ∈ X : (i, j) ∈ G}. Respectively, the ﬁrst hitting time UG can be
represented as UG = min(n ≥ 1 : Jn ∈ GJn−1). This representation explains
using of the term “place-dependent hitting time”.

In fact, the above model can be embedded in the previous one, if to
consider the new Markov renewal process ( ¯Jn, Xn) = ((Jn−1, Jn), Xn), n =
0, 1, . . . constructed from the initial Markov renewal process (Jn, Xn), n =
0, 1, . . . by aggregating sequential states for the initial embedded Markov
chain Jn.

The Markov renewal process ( ¯Jn, Xn) has the phase space (X×X)×[0, ∞).
For simplicity, we can take the initial state ¯J0 = (J−1, J0), where J−1 is a
random variable taking values in space X and independent on the Markov
renewal process (Jn, Xn).

Note that the simpler condition A can, in fact, be replaced by a simpler

condition:

A′

G: Pi{UG < ∞} = 1, i ∈ X.

The above assumption, that domain G is hittable, is implied by condition

A, for any domain G containing a pair of states (i, j) such that pij > 0.

The results concerned moments of usual accumulated rewards WD can
be expanded to the place-depended accumulated rewards YG for hittable
domains, using the above embedding procedure.

5.5. Time-dependent hitting times. Let (Jn, Xn), n = 0, 1, . . . be an
inhomogeneous in time Markov renewal process, i.e., an inhomogeneous in

17

time Markov chain with phase space with the phase space X×[0, ∞), an initial
distribution ¯p = hpi = P{J0 = i, X0 = 0} = P{J0 = i}, i ∈ Xi and transition
probabilities, deﬁned for (i, s), (j, t) ∈ X × [0, ∞) and n = 0, 1, 2, . . .,

Q(n+1)

ij

(t) = P{Jn+1 = j, Xn+1 ≤ t/Jn = i, Xn = s}.

(55)

As in homogeneous in time case, we exclude instant jumps and assume

that the following condition holds;

B′: Q(n)

ij (0) = 0, i, j ∈ X, n ≥ 1.

n = n, n = 0, 1, . . .. Indeed, process ( ¯Jn, Xn) = ((J ′

Process (Jn, Xn) can be transformed in a homogeneous in time Markov
renewal process by adding to this process an additional counting time com-
ponent J ′
n, Jn), Xn), n =
0, 1, . . . is a homogeneous in time Markov renewal process. This process has
the phase space (N × X) × [0, ∞), where N = {0, 1, . . .}. It has the initial
distribution ¯p = hpi = P{J ′
0 = 0, J0 = i, X0 = 0} = P{J0 = i}, i ∈ Xi and
transition probabilities,

Q(n,i),(k,j)(t) =( Q

(n+1)
ij

(t)

for t ≥ 0, k = n + 1, n = 0, 1, . . . , i, j ∈ X,

0

for t ≥ 0, k 6= n + 1, n = 0, 1, . . . , i, j ∈ X.

(56)

( ¯J (h)

The phase space of the process ( ¯Jn, Xn) is countable.
Let now deﬁne a time-truncated version of process ( ¯Jn, Xn) as the process
n , X (h)
The process ( ¯J (h)

n∧h, Jn∧h), Xn∧h), n = 0, 1, . . ., for some integer h ≥ 1.
n , X (h)
n ), n = 0, 1, . . . is also a homogeneous in time Markov
It has the ﬁnite phase space (H × X) × [0, ∞), where

n ) = ((J ′

renewal process.
H = {0, 1, . . . , h}.

Let hD1, . . . , Dhi be some sequence of subsets of space X such that Dh = X
n ∈ {n} × Dn) = min(n ≥ 1 : Jn ∈ Dn) is the
n=1{n} × Dn for the Markov chain

and let U˜Dh
ﬁrst hitting time to the domain ˜Dh = ∪h
¯J (h)
n .

= min(n ≥ 1 : ¯J (h)

≤ h} = 1, i ∈ X, i.e., domain ˜D is hittable for the

Obviously, Pi{U˜Dh

Markov chain ¯J (h)
n .

Thus, all results presented in Sections 2 – 4 can be applied to the time-

dependent accumulated rewards of hitting type,

Z˜Dh

=

U˜Dh

Xn=1

Xn.

18

(57)

Note only hat condition Cd should be, in this case, replaced by condition:

Ch,d: E{X d

nI(Jn = j)/Jn−1 = i} < ∞, n = 1, . . . , h, i, j ∈ X.

In conclusion, we would like also to note that it is possible to combine
all ﬁve listed above generalization aspects in the frame of one semi-Markov
model.

5.6. An example. Let us consider a numerical example illustrating
the recurrent algorithm for computing power moment of hitting times and
accumulated rewards of hitting times for semi-Markov processes, based on
sequential reduction of their phase spaces.

Let J(t) be a semi-Markov process with the phase space X = {0, 1, 2, 3},
and the 4 × 4 matrix of transition probabilities, kQij(t)k, which has the
following form, for t ≥ 0,

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

0
1
4(1 − e−t/4)
0
0

0
1
4(1 − e−t/4)
3(1 − e−t/3)
0

1

0
1
4(1 − e−t/4)
3(1 − e−t/3)
1
2I(t ≥ 2)

1

I(t ≥ 1)
1
4(1 − e−t/4)
3(1 − e−t/3)
1
2I(t ≥ 2)

1

.

(58)

The 4 × 4 matrices of transition probabilities kpijk, for the embedded
ij k of sojourn

Markov chain Jn, expectations ke(1)
times, for the semi-Markov process J(t), have the following forms,

ij k and second moments ke(2)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

.

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

0
1
4
0
0

0
1
4
1
3
0

0
1
4
1
3
1
2

1
1
4
1
3
1
2

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

, (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

0
1
0
0

0
1
1
0

0
1
1
1

1
1
1
1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

and (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

0
8
0
0

0
8
6
0

0
8
6
2

1
8
6
2

(59)

Let us compute ﬁrst two moments of hitting times E(1)
using the recurrent algorithm described in Sections 3 – 5.

00 , E(1)

10 and E(2)

00 , E(2)

10

Let us ﬁrst exclude state 3 from the phase space X = {0, 1, 2, 3} of the
semi-Markov process J(t). The corresponding reduced semi-Markov process
h3iJ(t) has the phase space h3iX = {0, 1, 2}.

The recurrent formulas (36) and (37) for transition probabilities of the
embedded Markov chain h3iJn, expectations and second moments of sojourn
times for the semi-Markov process h3iJ(t) have the following forms, respec-
33 h3ip3j)
tively, h3ipij = pij + pi3

i3 h3ip3j + pi3

ij = e(1)

3j + e(1)

ij + e(1)

h3ie(1)

(e(1)

,

p3j

1−p33

1−p33

19

and h3ie(2)
for i = 0, 1, 2, 3, j = 0, 1, 2.

ij = e(2)

ij + e(2)

i3 h3ip3j + 2e(1)

i3 h3ie(1)

3j + pi3

1−p33

(e(2)

3j + e(2)

33 h3ip3j + 2e(1)

33 h3ie(1)
3j ),

The 4×3 matrices of transition probabilities kh3ipijk, expectations kh3ie(1)
ij k,
ij k, computed according the above recurrent for-

and second moments kh3ie(2)
mulas, take the following forms,

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

0
1
4
0
0

0
1
4
1
3
0

1
1
2
2
3
1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

, (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

0
1
0
0

0
1
1
0

5
3
10
3
4

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

and (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

0
8
0
0

0
8
6
0

33
30
28
24

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

.

(60)

Let us now exclude state 2 from the phase space h3iX = {0, 1, 2} of the
semi-Markov process h3iJ(t). The corresponding reduced semi-Markov pro-
cess h3,2iJ(t) has the phase space h3,2iX = {0, 1}.

The recurrent formulas (36) and (37) for transition probabilities of the
embedded Markov chain h3,2iJn, expectations of sojourn times and second
moments of sojourn times for the semi-Markov process h3,2iJ(t) have the
h3,2ie(1)
following forms, respectively,
ij =
,
i2 h3,2ip2j + h3ipi2
h3ie(1)
ij + h3ie(1)
22 h3,2ip2j) and h3,2ie(2)
ij = h3ie(2)
ij +
1− h3ip22
h3ie(2)
i2 h3,2ip2j + 2 h3ie(1)
i2 h3,2ie(1)
22 h3,2ie(1)
2j + h3ie(2)
2j ),
for i = 0, 1, 2, j = 0, 1.

h3,2ipij = h3ipij + h3ipi2
(h3ie(1)
2j + h3ipi2
1− h3ip22

2j + h3ie(1)
(h3ie(2)

22 h3,2ip2j+2 h3ie(1)

1− h3ip22

h3ip2j

The 3 × 2 matrices of transition probabilities

ij k, and second moments kh3,2ie(2)

kh3,2ie(1)
recurrent formulas, take the following forms,

kh3,2ipijk, expectations
ij k, computed according the above

0
1
4
0

1
3
4
1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

, (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

0
1
0

18
21
2
13

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

and (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

0
8
0

525
297
362

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

.

(61)

Finally, let us exclude state 1 from the phase space h3,2iX = {0, 1} of
the semi-Markov process h3,2iJ(t). The corresponding reduced semi-Markov
process h3,2,1iJ(t) has the phase space h3,2,1iX = {0}.

The recurrent formulas (36) and (37) for transition probabilities of the
embedded Markov chain h3,2,1iJn, expectations of sojourn times and second
moments of sojourn times for the semi-Markov process h3,2,1iJ(t) have the
following forms, respectively, h3,2,1ipi0 = h3,2ipi0 + h3,2ipi1
i0 =

, E(1)

h3,2ip10

1− h3,2ip11

20

h3,2,1ie(1)
and E(2)

i0 = h3,2ie(1)
i0 = h3,2,1ie(2)

i0 + h3,2ie(1)

i1 h3,2,1ip10 + h3,2ipi1
1− h3,2ip11
i1 h3,2,1ip10 + 2 h3,2ie(1)

i0 + h3,2ie(2)

(h3,2ie(1)

10 + h3,2ie(1)

11 h3,2,1ip10)
10 +

i1 h3,2,1ie(1)

i0 = h3,2ie(2)

h3,2ipi1

1− h3,2ip11

(h3,2ie(2)

10 + h3,2ie(2)

11 h3,2,1ip10 + 2 h3,2ie(1)

11 h3,2,1ie(1)

10 ), for i = 0, 1.

Here, equalities, h3,2,1ie(0)

i0 = h3,2,1ipi0 = 1, i = 0, 1, should be taken into

account that simpliﬁes the corresponding calculations.

The 2 × 1 matrices of expectations kE(1)

i0 k, and second moments kE(2)
i0 k
computed according the above recurrent formulas, take the following forms,

and kE(2)

.

(62)

kE(1)

i0 k =(cid:13)(cid:13)(cid:13)(cid:13)

64

46 (cid:13)(cid:13)(cid:13)(cid:13)

i0 k =(cid:13)(cid:13)(cid:13)(cid:13)

7265

5084 (cid:13)(cid:13)(cid:13)(cid:13)

In conclusion, we would like to note that recurrent algorithms presented
in the paper are subjects of eﬀective program realization. These programs
let one compute power moments for hitting times and accumulated rewards
of hitting times for semi-Markov processes with very large numbers of states.
We are going to present such programs and results of large scale experimental
studies in future publications.

References

[1] Anisimov, V. V. (2008). Switching Processes in Queueing Models. Applied Stochastic

Methods Series. ISTE, London and Wiley, Hoboken, NJ, 352 pp.

[2] Anisimov, V. V., Zakusilo, O. K., Donchenko, V. S. (1987). Elements of Queueing

and Asymptotical Analysis of Systems. Lybid’, Kiev, 246 pp.

[3] Barbu, V., Boussemart, M., Limnios, N. (2004). Discrete time semi-Markov model
for reliability and survival analysis. Commun. Stat. Theory, Methods, 33, no. 11,
2833–2868.

[4] Biﬃ, G., D’amigo, G., Di Biase, G., Janssen, J., Manca, R., Silvestrov, D. (2008).
Monte Carlo semi-Markov methods for credit risk migration and Basel II rules. I, II.
J. Numer. Appl. Math., 1(96), I: 28–58, II: 59–86.

[5] Chung, K. L. (1954). Contributions to the theory of Markov chains II. Trans. Amer.

Math. Soc., 76, 397–419.

[6] Chung, K. L. (1960, 1967). Markov Chains with Stationary Transition Probabilities.
Fundamental Principles of Mathematical Sciences, 104, Springer, Berlin, x+278 pp.

[7] Ciardo, G., Raymonf, M. A., Sericola, B., Trivedi, K. S. (1990). Performability
analysis using semi-Markov reward processes. IEEE Trans. Comput., 39, no. 10,
1251–1264.

21

[8] Cogburn, R. (1975). A uniform theory for sums of Markov chain transition proba-

bilities. Ann. Probab., 3, 191–214.

[9] Courtois, P. J. 1977). Decomposability. Queueing and Computer System Applica-

tions. ACM Monograph Series, Academic Press, New York, xiii+201 pp.

[10] D’Amico, G., Guillen, M., Manca, R. (2013). Semi-Markov disability insurance mod-

els. Commun. Stat. Theory Methods, 42, no. 16, 2172–2188.

[11] D’Amico, G., Janssen, J., Manca, R. (2005). Homogeneous semi-Markov reliability

models for credit risk management. Decis. Econ. Finance, 28, no. 2, 79–93.

[12] D’Amico, G., Petroni, F. (2012). A semi-Markov model for price returns. Physica A,

391, 4867–4876.

[13] D’Amico, G., Petroni, F., Prattico, F. (2013). First and second order semi-Markov

chains for wind speed modeling. Physica A, 392, no. 5, 1194–1201.

[14] D’Amico, G., Petroni, F., Prattico, F. (2015). Performance analysis of second order
semi-Markov chains: an application to wind energy production Methodol. Comput.
Appl. Probab., 17, no. 3, 781–794.

[15] Gyllenberg, M., Silvestrov, D. (2008). Quasi-Stationary Phenomena in Nonlinearly
Perturbed Stochastic Systems. De Gruyter Expositions in Mathematics, 44, Walter
de Gruyter, Berlin, ix+579 pp.

[16] Hunter, J. J. (2005). Stationary distributions and mean ﬁrst passage times of per-

turbed Markov chains. Linear Algebra Appl., 410, 217–243.

[17] Janssen, J., Manca, R. (2006). Applied Semi-Markov Processes. Springer, New York,

xii+309 pp.

[18] Janssen, J., Manca, R. (2007). Semi-Markov Risk Models for Finance, Insurance and

Reliability. Springer, New York, xvii+429 pp.

[19] Kemeny, J. G., Snell, J. L. (1961a). Potentials for denumerable Markov chains. J.

Math. Anal. Appl., 6, 196–260.

[20] Kemeny, J. G., Snell, J. L. (1961b). Finite continuous time Markov chains. Theor.

Probab. Appl., 6, 110–115.

[21] Korolyuk, V. S., Brodi, S. M., Turbin, A. F. (1974). Semi-Markov processes and their
application. Probability Theory. Mathematical Statistics. Theoretical Cybernetics,
11, VINTI, Moscow, 1974, 47–97.

[22] Korolyuk, V. S., Korolyuk, V. V. (1999). Stochastic Models of Systems. Mathematics

and its Applications, 469, Kluwer, Dordrecht, xii+185 pp.

[23] Koroliuk, V. S., Limnios, N. (2005). Stochastic Systems in Merging Phase Space.

World Scientiﬁc, Singapore, xv+331 pp.

[24] Korolyuk, V. S., Turbin, A. F. (1976). Semi-Markov Processes and its Applications.

Naukova Dumka, Kiev, 184 pp.

[25] Korolyuk, V. S., Turbin, A. F. (1978). Mathematical Foundations of the State Lump-
ing of Large Systems. Naukova Dumka, Kiev, 218 pp. (English edition: Mathematics
and its Applications, 264, Kluwer, Dordrecht, 1993, x+278 pp.).

22

[26] Kovalenko, I. N. (1975). Studies in the Reliability Analysis of Complex Systems.

Naukova Dumka, Kiev, 210 pp.

[27] Kovalenko, I. N., Kuznetsov, N. Yu., Pegg, P. A. (1997). Mathematical Theory of
Reliability of Time Dependent Systems with Practical Applications. Wiley Series in
Probability and Statistics, Wiley, New York, 316 pp.

[28] Lamperty, J. (1963). Criteria for stochastic processes II: passage-time moments. J.

Math. Anal. Appl., 7, 127–145.

[29] Limnios, N., Opri¸san, G. (2001). Semi-Markov Processes and Reliability. Statistics

for Industry and Technology, Birkh¨auser, Boston, xii+222 pp.

[30] Limnios, N., Opri¸san, G. (2003). An introduction to Semi-Markov processes with ap-
plication to reliability. In: Shanbhag, D.N., Rao, C. R. (Eds.) Handbook of Statistics,
21, 515–556.

[31] Nummelin, E. (1984). General Irreducible Markov Chains and Nonnegative Opera-
tors. Cambridge Tracts in Mathematics, 83, Cambridge University Press, Cambridge,
xi+172 pp.

[32] Papadopoulou, A. A., Tsaklidis, G., McClean, S., Garg, L. (2012). On the moments
and the distribution of the cost of a semi-Markov model for healthcare systems.
Methodol. Comput. Appl. Probab., 14, no. 3, 717–737.

[33] Papadopoulou, A. A. (2013). Some results on modeling biological sequences and web
navigation with a semi- Markov chain. Comm. Stat. Theory, Methods, 42, no. 16,
2153–2171.

[34] Pitman, J. W. (1974a). An identity for stopping times of a Markov process.
In: Williams, E. J. (Ed.) Studies in Probability and Statistics. Academic Press.
Jerusalem, 41–57.

[35] Pitman, J. W. (1974b). Uniform rates of convergence for Markov chain transition

probabilities. Z. Wahrscheinlichkeitsth., 29, 193–227.

[36] Pitman, J. W. (1977). Occupation measures for Markov chains. Adv. Appl. Probab.,

9, 69–86.

[37] Silvestrov, D. S. (1974). Limit Theorems for Composite Random Functions. Vysshaya

Shkola and Izdatel’stvo Kievskogo Universiteta, Kiev, 318 pp.

[38] Silvestrov D. S. (1980a). Mean hitting times for semi-Markov processes, and queueing

networks. Elektron. Infor. Kybern., 16, 399–415.

[39] Silvestrov, D. S. (1980b). Semi-Markov Processes with a Discrete State Space. Li-

brary for an Engineer in Reliability, Sovetskoe Radio, Moscow, 272 pp.

[40] Silvestrov, D. S. (1983a). Method of a single probability space in ergodic theorems
for regenerative processes I. Math. Operationsforsch. Statist., Ser. Optimization, 14,
286–299.

[41] Silvestrov, D. S. (1983b). Invariance principle for the processes with semi-Markov
switch-overs with an arbitrary state space. In: Proceedings of the Fourth USSR-
Japan Symposium on Probability Theory and Mathematical Statistics, Tbilisi 1983.
Lecture Notes in Math., 1021, 617–628.

23

[42] Silvestrov, D. S. (1994). Coupling for Markov renewal processes and the rate of
convergence in ergodic theorems for processes with semi-Markov switchings. Acta
Applic. Math., 34, 109–124.

[43] Silvestrov, D S. (1996). Recurrence relations for generalised hitting times for semi-

Markov processes. Ann. Appl. Probab., 6, 617–649.

[44] Silvestrov, D. S., Drozdenko, M. O. (2006). Necessary and suﬃcient conditions for
weak convergence of ﬁrst-rare-event times for semi-Markov processes. Theory Stoch.
Process., 12(28), no. 3-4, Part I: 151–186, Part II: 187–202.

[45] Silvestrov, D., Manca, R., Silvestrova, E. (2014). Computational algorithms for mo-
ments of accumulated Markov and semi-Markov rewards. Comm. Statist. Theory
Methods, 43, no. 7, 1453–1469.

[46] Silvestrov, D., Silvestrova, E., Manca, R. Stochastically ordered models for credit

rating dynamics. J. Numer. Appl. Math., 1(96), 206–218, (2008).

[47] Silvestrov, D., Silvestrov, S. (2015). Asymptotic expansions for stationary distribu-
tions of perturbed semi-Markov processes. Research Report 2015-9, Department of
Mathematics, Stockholm University, 75 pp.

[48] Stenberg, F., Manca, R., Silvestrov, D. (2006). Semi-Markov reward models for

disability insurance. Theory Stoch. Proces., 12(28), no. 3-4, 239–254.

[49] Stenberg, F., Manca, R., Silvestrov, D. (2007). An algorithmic approach to discrete
time non-homogeneous backward semi-Markov reward process with an application
to disability insurance. Metodol. Comput. Appl. Probab., 9, 497–519.

[50] Yin, G. G., Zhang, Q. (2005). Discrete-Time Markov Chains. Two-Time-Scale Meth-
ods and Applications. Stochastic Modelling and Applied Probability, 55, Springer,
New York, xix+348 pp.

[51] Yin, G. G., Zhang, Q. (2013). Continuous-Time Markov Chains and Applications. A
Two-Time-Scale Approach. Second edition, Stochastic Modelling and Applied Prob-
ability, 37, Springer, New York, xxii+427 pp. (An extended variant of the ﬁrst (1998)
edition).

24

