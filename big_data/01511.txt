Hoaxy: A Platform for Tracking Online Misinformation

∗
Chengcheng Shao
School of Computer

National University of Defense Technology,

China

Giovanni Luca Ciampaglia1,

Alessandro Flammini1,2,

Filippo Menczer1,2

1 Indiana University Network Science Institute

2 School of Informatics and Computing
Indiana University, Bloomington, USA

6
1
0
2

 
r
a

M
4

 

 
 
]
I
S
.
s
c
[
 
 

1
v
1
1
5
1
0

.

3
0
6
1
:
v
i
X
r
a

ABSTRACT
Massive amounts of misinformation have been observed to
spread in uncontrolled fashion across social media. Exam-
ples include rumors, hoaxes, fake news, and conspiracy the-
ories. At the same time, several journalistic organizations
devote signiﬁcant eﬀorts to high-quality fact checking of on-
line claims. The resulting information cascades contain in-
stances of both accurate and inaccurate information, un-
fold over multiple time scales, and often reach audiences of
considerable size. All these factors pose challenges for the
study of the social dynamics of online news sharing. Here
we introduce Hoaxy, a platform for the collection, detection,
and analysis of online misinformation and its related fact-
checking eﬀorts. We discuss the design of the platform and
present a preliminary analysis of a sample of public tweets
containing both fake news and fact checking. We ﬁnd that,
in the aggregate, the sharing of fact-checking content typi-
cally lags that of misinformation by 10–20 hours. Moreover,
fake news are dominated by very active users, while fact
checking is a more grass-roots activity. With the increas-
ing risks connected to massive online misinformation, social
news observatories have the potential to help researchers,
journalists, and the general public understand the dynamics
of real and fake news sharing.

Keywords
Misinformation; hoaxes;
checking; Twitter

fake news; rumor tracking;

fact

1.

INTRODUCTION

The recent rise of social media has radically changed the
way people consume and produce information online [21].
Approximately 65% of the US adult population accesses the
news through social media [2, 30], and more than a billion

∗Work performed as a visiting scholar at Indiana University.

Email: shaoc@indiana.edu

Preprint version
.

people worldwide are active on a daily basis on Facebook
alone [16].

The possibility for normal consumers to produce content
on social media creates new economies of attention [10] and
has changed the way companies relate to their customers [35].
Social media allow users to participate in the propagation
of the news. For example, in Twitter, users can rebroad-
cast, or retweet, any piece of content to their social circles,
creating a competition among posts for our limited atten-
tion [37]. This has the implication that no individual author-
ity can dictate what kind of information is distributed on the
whole network. While such platforms have brought about
a more egalitarian model of information access according to
some [4], the inevitable lack of oversight from expert jour-
nalists makes social media vulnerable to the unintentional
spread of false or inaccurate information, or misinformation.
Large amounts of misinformation have been indeed ob-
served to spread online in viral fashion, oftentimes with wor-
rying consequences in the oﬄine world [8, 22, 29, 23, 27, 7];
examples include rumors [18], false news [8, 23], hoaxes, and
even elaborate conspiracy theories [1, 13, 19].

Due to the magnitude of the phenomenon, media orga-
nizations are devoting increasing eﬀorts to produce accu-
rate veriﬁcations in a timely manner. For example, during
Hurricane Sandy, false reports that the New York Stocks
Exchange had been ﬂooded were corrected in less than an
hour [36]. These fact-checking assessments are consumed
and broadcast by social media users like any other type
of news content, leading to a complex interplay between
‘memes’ that vie for the attention of users [37]. Examples
of such organizations include Snopes.com, PolitiFact, and
FactCheck.org.

Structural features of the information exchange networks
underlying social media create peculiar patterns of infor-
mation access. Online social networks are characterized by
homophily [25], polarization [12], algorithmic ranking [3],
and social bubbles [28] — information environments with
low content diversity and strong social reinforcement.

All of these factors, coupled with the fast news life cy-
cle [14, 10], create considerable challenges for the study of
the dynamics of social news sharing. To address some of
these challenges, here we present Hoaxy, an upcoming Web
platform for the tracking of social news sharing. Its goal is
to let researchers, journalists, and the general public moni-
tor the production of online misinformation and its related
fact checking.

As a simple proof of concept for the capabilities of this
kind of systems, here we present the results of a prelimi-

nary analysis on a dataset of public tweets collected over
the course of several months. We focus on two aspects: the
temporal relation between the spread of misinformation and
fact checking, and the diﬀerences in how users share them.
We ﬁnd that, in absolute terms, misinformation is produced
in much larger quantity than fact-checking content. Fact
checks obviously lag misinformation, and we present evi-
dence that there exists a characteristic lag of approximately
13 hours between the production of misinformation and that
of fact checking. Finally, we show preliminary evidence that
fact-checking information is spread by a broader plurality of
users compared to fake news.

2. RELATED WORK

Tracking abuse of social media has been a topic of in-
tense research in recent years. Beginning with the detection
of simple instances of political abuse like astroturﬁng [31],
researchers noted the need for automated tools for monitor-
ing social media streams. Several such systems have been
proposed in recent years, each with a particular focus or a
diﬀerent approach. The Truthy system [32], which relies on
network analysis techniques, is among the best known of
such platforms. The TweetCred system [9] focuses instead
on content-based features and other kind of metadata, and
distills a measure of overall information credibility.

More recently, speciﬁc systems have been proposed to
detect rumors. These include RumorLens [33], Twitter-
Trails [26], and FactWatcher [20]. The fact-checking capa-
bilities of these systems range from completely automatic
(TweetCred), to semi-automatic (TwitterTrails, RumorLens).
In addition, some of them let the user explore the prop-
agation of a rumor with an interactive dashboard (Twit-
terTrails, RumorLens). However, they do not monitor the
social media stream automatically, but require the user to
input a speciﬁc rumor to investigate. Compared with these,
the objective of the Hoaxy system is to track both accurate
and inaccurate information in automatic fashion.

Automatic attempts to perform fact checking have been
recently proposed for simple statements [11], and for multi-
media content [6]. At this initial stage, because our focus
is on automatic tracking of news sharing, the Hoaxy system
does not perform any kind of fact checking. Instead, we fo-
cus on tracking news shares from sources whose accuracy has
been determined independently. There have been investiga-
tions on the related problems of ﬁnding reliable information
sources [15] and news curators [24].

3. SYSTEM ARCHITECTURE

Our main objective is to build a uniform and extensible
platform to collect and track misinformation and fact check-
ing. Fig. 1 shows the architecture of our system. Currently
our eﬀorts have been focused on the ‘Monitors’ part of the
system. We have implemented a tracker for the Twitter API,
and a set of crawlers for both fake news and fact checking
websites, as well as a database.

We begin by describing the origin of our data. The sys-
tem collects data from two main sources: news websites and
social media. From the ﬁrst group we can obtain data about
the origin and evolution of both fake news stories and their
fact checking. From the second group we collect instances of
these news stories (i.e., URLs) that are being shared online.

Figure 1: Architecture of the Hoaxy system.

To collect data from such disparate sources, we make use
of a number of technologies: Web scraping, Web syndica-
tion, and, where available, APIs of social networking plat-
forms. For example, we use the Twitter streaming API to
do real-time tracking of news sharing. Because tweets are
limited to 140 characters, the most common method to share
a news story on Twitter is to include directly a link to its
Web article. This means that we can focus only on tweets
containing links to speciﬁc domains (websites), a task that
is performed eﬃciently by the ﬁlter endpoint of the Twitter
streaming API.1

To collect data on news stories we rely on RSS, which al-
lows us to use a uniﬁed protocol instead of manually adapt-
ing our scraper to the multitude of Web authoring systems
used on the Web. Moreover, RSS feeds contain information
about updates made to news stories, which let us track the
evolution of news articles. We collect data from news sites
using the following two steps: when a new website is added
to our list of monitored sources, we perform a ‘deep’ crawl
of its link structure using a custom Python spider written
with the Scrapy framework2; at this stage, we also identify
the URL of the RSS feed, if available. Once all existing sto-
ries have been acquired, we perform every two hours a ‘light’
crawl by checking its RSS feed only. To perform the ‘deep’
crawl, we use a depth-ﬁrst strategy. The ‘light’ crawl is in-
stead performed using a breadth-ﬁrst approach. We store all
these structured data into a database; this allows convenient
retrieval for future analysis, which we plan to implement as
an interactive Web dashboard. Unfortunately, URLs do not
make for good, unique identiﬁers, since URLs with diﬀerent
protocol schema, query parameters, or fragments may all
refers to the same page. We rely on canonical URLs where
possible, and adopt a simple URL canonization technique in
other cases (see below).

1dev.twitter.com/streaming/reference/post/
statuses/filter
2scrapy.org

DATABASEStoreMonitorsURL Tracker(stream api)RSS ParserScrapy SpiderFetchNews SitesSocial NetworksAPICrawlerAnalysis DashboardFigure 2: Lagged cross correlation (Pearson’s r) be-
tween news sharing activity of misinformation and
fact-checking, with peak value at lag = −13 hours.

Figure 3: Daily volume of tweets. The gaps corre-
spond to two windows with missing data when our
collection script crashed.

Table 1: Summary statistics of tweet data.
Nusers NURLs

source Nsites

Ntweets

fake news
fact checking

71
6

1,287,769
154,526

171,035
78,624

96,400
11,183

4. PRELIMINARY ANALYSIS

In this section we report results from a preliminary anal-
ysis performed on a large set of public tweets collected over
the course of several months. Since we are interested in
characterizing the relation between the overall social shar-
ing activity of misinformation and fact checking, we begin
our analysis by focusing on the overall aggregate volume of
tweets, without breaking activity down to the level of an in-
dividual story or set of stories. We take aggregate volume as
a proxy for the overall social sharing activity of news stories.
4.1 Data

We collect tweets containing URLs from two lists of Web
domains: the ﬁrst, fake news, covers 71 domains and was
taken from a comprehensive resource on online misinforma-
tion.3 We manually removed known satirical sources like
The Onion. The second list is composed of the six most
popular fact-checking websites: Snopes.com, PolitiFact.
com, FactCheck.org, OpenSecrets.org, TruthOrFiction.
com, and HoaxSlayer.com. The keywords we used to collect
all these tweets correspond to the domain names of these
websites.

To convert the URLs to canonical form we perform the
following steps: ﬁrst, we transform all text into lower case;
then we remove the protocol schema (e.g. ‘http://’); then we
remove, if present, any preﬁx instance of the strings ‘www.’
or ‘m.’; ﬁnally, we remove all URL query parameters.

We collected about 3 months of ﬁltered tweets traﬃc from
Oct 14, 2015 to Jan 24, 2016. The summary statistics for
the numbers of tweets, unique users, and unique canonical

3fakenewswatch.com

URLs (Table 1) illustrate the imbalance between the sets of
fake news and fact-checking sites.
4.2 Tweet Volume

Fig. 3 plots the daily volume of tweets. As described be-
fore, we track more fake sites than fact checking ones, so
the volume of fake news tweets is larger than that of fact
checking ones by approximately one order of magnitude.

While both time series display signiﬁcant ﬂuctuations, the
presence of aligned peaks (Nov 16) and valleys (Nov 2) sug-
gests the presence of cross-correlated activity. To better
understand this, we perform a lagged cross-correlation anal-
ysis, which measures the similarity between two time series
signals as a function of the lag.
Fig. 2 shows the results of the cross correlation analysis,
with lags ranging from −48 hours to +48 hours. A higher
correlation at a negative lag indicates that the sharing of
fake news precedes that of fact checking. To eliminate circa-
dian ﬂuctuations, we use a simple moving average method
with centered window of size equal to 24 hours. The results
suggest that, in the limited number of examples at our dis-
posal, there is a characteristic time lag between fake news
and fact checking of approximately 13 hours. Because the
moving average cleaning could only remove circadian ﬂuc-
tuations, we do not exclude the presence of correlations at
larger lags (e.g., weekly).

While this cross-correlation is suggestive of a temporal
relation between misinformation and fact-checking, it is im-
portant to understand that it is based on aggregate data.
We selected a subset of URLs from both data sets to see if
these correlations also hold at the level of individual events.
We followed two strategies: (1) we selected a single URL
from our pool of fake news stories and a matching URL
from that of fact-checking stories; (2) we considered a small
set of keywords and used them to perform pattern matching
on the lists of URLs.

Table 2 displays the two URLs (A1, A2) used in the ﬁrst
strategy. The reported story focuses on the current Syr-
ian conﬂict, and contains several inaccuracies that were de-
bunked in a piece by Snopes.com. For the second strategy,

−40−2002040Lag (hours)0.000.050.100.150.200.250.300.350.40rNovDecJan2016103104105Number of tweetsFake newsFact checkingTable 2: A1: An example of inaccurate news story. A2: Corresponding fact checking page. B1: News articles
reporting inaccurate information about the death of actor Alan Rickman. B2: Corresponding fact-checking
pages.

A1

A2

B1

www.infowars.com/white-house-gave-isis-45-minute-warning-before-bombing-oil-tankers/

www.snopes.com/2015/11/23/obama-dropped-leaflets-warning-isis-airstrikes/

en.mediamass.net/people/alan-rickman/deathhoax.html
www.disclose.tv/forum/david-bowie-alan-rickman-death-hoax-100-staged-t108254.html
worldtruth.tv/david-bowie-and-alan-rickman-death-hoax-100-staged/
beforeitsnews.com/alternative/2016/01/

alan-rickman-the-curse-of-the-69-takes-another-victim-january-man-predicts-his-death-video-3277444.
html

beforeitsnews.com/celebrities/2016/01/

david-bowie-alan-rickman-death-hoax-100-staged-both-69-died-from-cancer-2474208.html

age-69.beforeitsnews.com/alternative/2016/01/
harry-potter-star-alan-rickman-dead-at-age-69-3277454.html
from-cancer.beforeitsnews.com/celebrities/2016/01/

david-bowie-alan-rickman-death-hoax-100-staged-both-69-died-from-cancer-2474208.html

B2

www.snopes.com/2016/01/14/alan-rickman-dies-at-69/
www.snopes.com/alan-rickman-potter-meme/

Figure 4: Daily volume of tweets for (a) A1 and A2 (cf. Table 2); (b) B1 and B2. Semi-log scale was used
for the plot.

we focused on a recent event: the death of famous actor
Alan Rickman on January 14, 2016. We used the keywords
‘alan’ and ‘rickman’ to match URLs from our database, and
found 15 matches (B1) among fake news sources and two
from fact-checking ones (B2). The fake news, in particu-
lar, were spreading the rumor that the actor had not died.
Fig. 4 plots the volume of tweets containing URLs from both
strategies. Despite low data volumes, the spikes of activity
and the successive decay show fairly strong alignment.
4.3 User Activity and URL Popularity

We measure the activity a of users by counting the num-
ber of tweets they posted, and the popularity of a given
story (either fake news or fact checking) by counting ei-
ther the total number n of times its URL was tweeted, or
the total number p of people who tweeted it. Fig. 5 shows
that these quantities display heavy-tailed, power-law distri-
butions P (x) ∼ x−γ. We estimated the power-law decay
exponents, obtaining the following results: for user activity
γfn
a = 2.3, γfc
a = 2.7; for URL popularity by tweets (tail ﬁt
for n ≥ 200) γfn
n = 2.5; and for URL popularity
by users (tail ﬁt for p ≥ 200) γfn
p = 2.5. These
observations suggest that fake news and fact checking have

p = 2.9, γfc

n = 2.7, γfc

similar popularity proﬁles, with fake news being spread by
accounts that, in some cases, can generate huge numbers of
tweets. While it is expected that active users are responsible
for producing a majority of news shares, the strong diﬀer-
ence between fake news and fact checking deserves further
scrutiny.

In Twitter there are four types of content: original tweets,
retweets, quotes, and replies. In our data, original tweets and
retweets were the most common category (80–90%) while
quotes and replies correspond to only 10–20% of the total,
usually with slightly more replies than quotes. However, we
observe diﬀerences between fact checking and misinforma-
tion tweets. The ﬁrst is that there are more replies and
quotes among fact checking tweets (> 20%) than misinfor-
mation (≈ 10%), suggesting that fact checking is a more
conversational task.

The second diﬀerence has to do with how content genera-
tion is shared among the top active users and the remaining
user base. To investigate this diﬀerence, we select for both
fake news and fact checking the tweets generated by the top
active users, which we deﬁne as the 1% most active user by
number of tweets. In Fig. 6 we plot the ratio ρ between origi-
nal tweets and retweets for all users and top active ones. For

DecJan2016252330071421280411180100101102103104Frequency of URL occurrence(a)A1, fake newsA2, fact checking13Jan2016231415161718192021220100101102103104Frequency of URL occurrence(b)B1, fake newsB2, fact checkingFigure 5: Complementary cumulative distribution function (CCDF) of (a) user activity a (tweets per user)
(b) URL popularity n (tweets per URL) and (c) URL popularity p (users per URL).

active accounts, and grass-roots responses that spread fact
checking information several hours later.

In the future we plan to study the active spreaders of
fake news to see if they are likely social bots [17, 34]. We
will also expand our analysis to a larger set of news stories
and investigate how the lag between misinformation and fact
checks varies for diﬀerent types of news.
Acknowledgments
CS was supported by the China Scholarship Council while
visiting the Center for Complex Networks and Systems Re-
search at the Indiana University School of Informatics and
Computing. GLC acknowledges support from the Indiana
University Network Science Institute (iuni.iu.edu) and from
the Swiss National Science Foundation (PBTIP2–142353).
This work was supported in part by the NSF (award CCF-
1101743) and the J.S. McDonnell Foundation.

6. REFERENCES
[1] A. Anagnostopoulos, A. Bessi, G. Caldarelli,

M. Del Vicario, F. Petroni, A. Scala, F. Zollo, and
W. Quattrociocchi. Viral misinformation: the role of
homophily and polarization. arXiv preprint
arXiv:1411.2893, 2014.

[2] M. Anderson and A. Caumont. How social media is

reshaping news.
http://www.pewresearch.org/fact-tank/2014/09/
24/how-social-media-is-reshaping-news/, 2014.
[Online; accessed December 2015].

[3] E. Bakshy, S. Messing, and L. A. Adamic. Exposure to

ideologically diverse news and opinion on facebook.
Science, 348(6239):1130–1132, 2015.

[4] Y. Benkler. The wealth of networks: How social

production transforms markets and freedom. Yale
University Press, 2006.

[5] T. Berners-Lee, W. Hall, J. Hendler, N. Shadbolt, and
D. J. Weitzner. Creating a science of the web. Science,
313(5788):769–771, 2006.

[6] C. Boididou, S. Papadopoulos, Y. Kompatsiaris,

S. Schiﬀeres, and N. Newman. Challenges of
computational veriﬁcation in social multimedia. In
Proceedings of the 23rd International Conference on
World Wide Web, WWW ’14 Companion, pages
743–748, 2014.

Figure 6: Ratio of original tweets to retweets for all
vs. top active users.

all users, the ratio is similar; there are more retweets than
original tweets. This is also the case for the top spreaders of
fact checking. However, for top spreaders of fake news, this
ratio is much higher: these users do not retweet as much but
post many original messages promoting the misinformation.
Taken together, these observations strongly suggest that
rumor-mongering is dominated by few very active accounts
that bear the brunt of the promotion and spreading of mis-
information, whereas the propagation of fact checking is a
more distributed, grass-roots activity.

5. CONCLUSIONS & FUTURE WORK

Social media provide excellent examples of marketplaces
of attention where diﬀerent memes vie for the limited time
of users [10]. A scientiﬁc understanding of the dynamics of
the Web is increasingly critical [5], and the dynamics of on-
line news consumption exemplify this need, as the risk of
massive uncontrolled misinformation grows. Our upcoming
Hoaxy platform for the automatic tracking of online misin-
formation may provide an important tool for the study of
these phenomena. Our preliminary results suggest an inter-
esting interplay between fake news promoted by few very

100101102103104a10-610-510-410-310-210-1100Pr©A¸aªa100101102103104n10-510-410-310-210-1100Pr©N¸nªbFake newsFact checking100101102103p10-510-410-310-210-1100Pr©P¸pªcFake newsFact checking0.00.51.01.52.0½AllTop[7] A. M. Buttenheim, K. Sethuraman, S. B. Omer, A. L.

Hanlon, M. Z. Levy, and D. Salmon. {MMR}
vaccination status of children exempted from
school-entry immunization mandates. Vaccine,
33(46):6250 – 6256, 2015.

[8] C. Carvalho, N. Klagge, and E. Moench. The

persistent eﬀects of a false news shock. Journal of
Empirical Finance, 18(4):597 – 615, 2011.

[9] C. Castillo, M. Mendoza, and B. Poblete. Information

credibility on Twitter. In Proceedings of the 20th
International Conference on World Wide Web, page
675, 2011.

[10] G. L. Ciampaglia, A. Flammini, and F. Menczer. The

production of information in the attention economy.
Scientiﬁc Reports, 5:9452, 2015.

[11] G. L. Ciampaglia, P. Shiralkar, L. M. Rocha,

J. Bollen, F. Menczer, and A. Flammini.
Computational fact checking from knowledge
networks. PLoS ONE, 10(6):e0128193, 06 2015.

[12] M. Conover, J. Ratkiewicz, M. Francisco,

B. Gon¸calves, A. Flammini, and F. Menczer. Political
polarization on Twitter. In Proc. 5th International
AAAI Conference on Weblogs and Social Media
(ICWSM), 2011.

[13] M. Del Vicario, A. Bessi, F. Zollo, F. Petroni,

A. Scala, G. Caldarelli, H. E. Stanley, and
W. Quattrociocchi. The spreading of misinformation
online. Proceedings of the National Academy of
Sciences, 113(3):554–559, 2016.

[23] T. Lauricella, C. S. Stewart, and S. Ovide. Twitter

hoax sparks swift stock swoon. The Wall Street
Journal, 23, 2013.

[24] J. Lehmann, C. Castillo, M. Lalmas, and

E. Zuckerman. Finding news curators in twitter. In
Proceedings of the 22nd International Conference on
World Wide Web, WWW ’13 Companion, pages
863–870, 2013.

[25] M. McPherson, L. Smith-Lovin, and J. M. Cook.
Birds of a feather: Homophily in social networks.
Annual Review of Sociology, 27(1):415–444, 2001.
[26] P. T. Metaxas, S. Finn, and E. Mustafaraj. Using

twittertrails.com to investigate rumor propagation. In
Proceedings of the 18th ACM Conference Companion
on Computer Supported Cooperative Work & Social
Computing, CSCW’15 Companion, pages 69–72, 2015.

[27] D. Mocanu, L. Rossi, Q. Zhang, M. Karsai, and

W. Quattrociocchi. Collective attention in the age of
(mis)information. Computers in Human Behavior, 51,
Part B:1198–1204, 2015.

[28] D. Nikolov, D. Fregolente, A. Flammini, and

F. Menczer. Measuring online social bubbles. PeerJ
Computer Science, 1:e38, 2015.

[29] B. Nyhan, J. Reiﬂer, and P. A. Ubel. The hazards of

correcting myths about health care reform. Medical
Care, 51(2):127–132, 2013.

[30] A. Perrin. Social media usage: 2005-2015.

http://www.pewinternet.org/2015/10/08/2015/
Social-Networking-Usage-2005-2015/.

[14] Z. Dezs¨o, E. Almaas, A. Luk´acs, B. R´acz, I. Szakad´at,

[31] J. Ratkiewicz, M. Conover, M. Meiss, B. Gon¸calves,

and A.-L. Barab´asi. Dynamics of information access
on the web. Phys. Rev. E, 73:066132, Jun 2006.

[15] N. Diakopoulos, M. De Choudhury, and M. Naaman.

Finding and assessing social media information
sources in the context of journalism. In Proceedings of
the SIGCHI Conference on Human Factors in
Computing Systems, CHI ’12, pages 2451–2460, 2012.

[16] Facebook Newsroom. Company info.

https://web.archive.org/web/20151222043012/
http://newsroom.fb.com/company-info/. Online;
accessed December 2015.

[17] E. Ferrara, O. Varol, C. Davis, F. Menczer, and

A. Flammini. The rise of social bots. Comm. ACM,
Forthcoming. preprint arXiv:1407.5225.

A. Flammini, and F. Menczer. Detecting and tracking
political abuse in social media. In Proc. 5th
International AAAI Conference on Weblogs and Social
Media (ICWSM), 2011.

[32] J. Ratkiewicz, M. Conover, M. Meiss, B. Gon¸calves,

S. Patil, A. Flammini, and F. Menczer. Truthy:
Mapping the spread of astroturf in microblog streams.
In Proceedings of the 20th International Conference
Companion on World Wide Web, WWW ’11, pages
249–252, 2011.

[33] P. Resnick, S. Carton, S. Park, Y. Shen, and N. Zeﬀer.

Rumorlens: A system for analyzing the impact of
rumors and corrections in social media. In Proc.
Computational Journalism Conference, 2014.

[18] A. Friggeri, L. A. Adamic, D. Eckles, and J. Cheng.

[34] V. Subrahmanian, A. Azaria, S. Durst, V. Kagan,

Rumor cascades. In Proc. Eighth Intl. AAAI Conf. on
Weblogs and Social Media (ICWSM), 2014.

[19] S. Galam. Modelling rumors: the no plane pentagon

A. Galstyan, K. Lerman, L. Zhu, E. Ferrara,
A. Flammini, F. Menczer, et al. The DARPA Twitter
Bot Challenge. arXiv preprint arXiv:1601.05140, 2016.

french hoax case. Physica A: Statistical Mechanics and
Its Applications, 320:571–580, 2003.

[35] D. Tapscott and A. D. Williams. Wikinomics: How

mass collaboration changes everything. Penguin, 2008.

[20] N. Hassan, A. Sultana, Y. Wu, G. Zhang, C. Li,

[36] E. Wemple. Hurricane Sandy: NYSE NOT ﬂooded!

J. Yang, and C. Yu. Data in, fact out: Automated
monitoring of facts by factwatcher. Proc. VLDB
Endow., 7(13):1557–1560, Aug. 2014.

[21] A. M. Kaplan and M. Haenlein. Users of the world,

unite! The challenges and opportunities of Social
Media. Business Horizons, 53(1):59 – 68, 2010.

[22] A. Kata. Anti-vaccine activists, web 2.0, and the
postmodern paradigm–an overview of tactics and
tropes used online by the anti-vaccination movement.
Vaccine, 30(25):3778–3789, 2012.

http://wapo.st/1QiG16A, October 2012. Last
accessed 2016-02-04.

[37] L. Weng, A. Flammini, A. Vespignani, and

F. Menczer. Competition among memes in a world
with limited attention. Scientiﬁc Reports, 2, 2012.

