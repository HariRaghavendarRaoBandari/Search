6
1
0
2

 
r
a

 

M
7
1

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
6
1
4
5
0

.

3
0
6
1
:
v
i
X
r
a

Bernoulli 22(3), 2016, 1301–1330
DOI: 10.3150/14-BEJ692

Estimation of inverse autocovariance matrices
for long memory processes

CHING-KANG ING1,*, HAI-TANG CHIOU2,** and MEIHUI GUO2,†

1Institute of Statistical Science, Academia Sinica, Taipei, Taiwan.
E-mail: *cking@stat.sinica.edu.tw
2Department of Applied Mathematics, National Sun Yat-sen University, Kaohsiung, Taiwan.
E-mail: **d002040001@student.nsysu.edu.tw; †guomh@math.nsysu.edu.tw

This work aims at estimating inverse autocovariance matrices of long memory processes admit-
ting a linear representation. A modiﬁed Cholesky decomposition is used in conjunction with an
increasing order autoregressive model to achieve this goal. The spectral norm consistency of the
proposed estimate is established. We then extend this result to linear regression models with
long-memory time series errors. In particular, we show that when the objective is to consistently
estimate the inverse autocovariance matrix of the error process, the same approach still works
well if the estimated (by least squares) errors are used in place of the unobservable ones. Appli-
cations of this result to estimating unknown parameters in the aforementioned regression model
are also given. Finally, a simulation study is performed to illustrate our theoretical ﬁndings.

Keywords: inverse autocovariance matrix; linear regression model; long memory process;
modiﬁed Cholesky decomposition

1. Introduction

Statistical inference for dependent data often involves consistent estimates of the inverse
autocovariance matrix of a stationary time series. For example, by making use of a
consistent estimate of the inverse autocovariance matrix of a short-memory time series (in
the sense that its autocovariance function is absolutely summable), Wu and Pourahmadi
[19] constructed estimates of the ﬁnite-past predictor coeﬃcients of the time series and
derived their error bounds. Moreover, in regression models with short-memory errors,
Cheng, Ing and Yu [6] proposed feasible generalized least squares estimates (FGLSE) of
the regression coeﬃcients using a consistent estimate of the inverse autocovariance matrix
of the error process. They then established an asymptotically eﬃcient model averaging
result based on the FGLSEs.

Having observed a realization u1, . . . , un of a zero-mean stationary time series {ut},
a natural estimate of its autocovariance function γk = cov(u0, uk) is the sample autoco-
variance function ˆγk = n−1Pn−|k|
i=1 uiui+|k|, k = 0,±1, . . .,±(n− 1). Moreover, it is known

This is an electronic reprint of the original article published by the ISI/BS in Bernoulli,
2016, Vol. 22, No. 3, 1301–1330. This reprint diﬀers from the original in pagination and
typographic detail.

1350-7265

c(cid:13) 2016 ISI/BS

2

C.-K. Ing, H.-T. Chiou and M. Guo

, provided kn ≪ n and P∞

that the kn-dimensional sample autocovariance matrix ˘Ωkn = (ˆγi−j )1≤i,j≤kn and its in-
verse ˘Ω−1
are consistent estimates of their population counterparts Ωkn = (γi−j)1≤i,j≤kn
kn
and Ω−1
k=1 |γk| < ∞. See, for example, Berk [1], Shibata [15],
kn
Ing and Wei [9] and Wu and Pourahmadi [19]. However, when the objective is to esti-
mate the n-dimensional autocovariance matrix Ωn, Wu and Pourahmadi [19] showed that
˘Ωn is no longer consistent in the short-memory case. In addition, Palma and Pourah-
madi [14] pointed out that this dilemma carries over to the long-memory case, assum-
ing P∞
k=1 |γk| = ∞. To circumvent this diﬃculty, Wu and Pourahmadi [19] proposed a
banded covariance matrix estimate ˘Ωn,l = (ˆγi−j 1|i−j|≤l)1≤i,j≤n of Ωn, where l ≥ 0 is
an integer and called the banding parameter. When {ut} is a short-memory time series
satisfying some mild conditions and l = ln grows to inﬁnity with n at a suitable rate,
they established consistency of ˘Ωn,l and ˘Ω−1
n,l under spectral norm. The result of Wu and
Pourahmadi [19] was subsequently improved by Xiao and Wu [20] to a better convergence
rate, and extended by McMurry and Politis [13] to tapered covariance matrix estimates.
Alternatively, Bickel and Gel [2] considered a banded covariance matrix estimate ˘Ωpn,l of
Ωpn , with pn = o(n). Assuming that {ut} is a stationary short-memory AR(∞) process,
they obtained ˘Ωpn,l’s consistency under the Frobenius norm, provided l = ln tends to
inﬁnity suﬃciently slowly.

Although the banded and tapered covariance matrix estimates work well for the short-
memory time series, they are not necessarily suitable for the long-memory case because
the autocovariance function of the latter is not absolutely summable. As a result, the
banded and tapered matrix estimates may incur large truncation errors, which prevent
them from achieving consistency. A major repercussion of this inconsistency property
n can no longer be obtained by inverting ˘Ωn,l or its
is that a consistent estimate of Ω−1
tapered version. On the other hand, since the spectral densities of most long-memory
time series encountered in common practice are bounded away from zero, it follows from
Proposition 4.5.3 of Brockwell and Davis [4] that

k≥1kΩ−1
sup

k k2 < ∞,

(1.1)

x=1}(x′A′Ax)1/2 denotes its
where for a k-dimensional matrix A, kAk2 = sup{x∈Rk: x
spectral norm. Motivated by (1.1), this paper aims to propose a direct estimate of Ω−1
n
and establish its consistency in the spectral norm sense, which is particularly relevant
under the long-memory setup.

′

To ﬁx ideas, assume

ut =

∞

Xj=0

ψjwt−j,

(1.2)

where ψ0 = 1 and {wt} is a martingale diﬀerence sequence with E(wt) = 0 and E(w2
σ2 for all t, and

t ) =

ψj = O(j−1+d),

(1.3)

Estimation of inverse autocovariance matrices

3

with d satisfying 0 < d < 1/2. We shall also assume that {ut} admits an AR(∞) repre-
sentation,

where

ut =

∞

Xi=1

aiut−i + wt,

ai = O(i−1−d).

In view of (1.3), the autocovariance function of {ut} obeys

γk =

∞

Xj=0

ψjψj+|k|σ2 = O(|k|−1+2d),

(1.4)

(1.5)

(1.6)

which may not be absolutely summable. A well-known model satisfying (1.2)–(1.5) is the
FARIMA(p, d, q) processes,

φ(B)(1 − B)dut = θ(B)wt,

(1.7)

where B is the backshift operator, φ(z) and θ(z) are polynomials of orders p and q,
respectively, |φ(z)θ(z)| 6= 0 for |z| ≤ 1, and |φ(z)| and |θ(z)| have no common zeros. Note
that when (1.7) is assumed, the spectral density of {ut}, fu(λ), satisﬁes

inf

λ∈[−π,π]

fu(λ) > 0,

from which (1.1) follows.

Let

k = E(ut − ak,1ut−1 − ··· − ak,kut−k)2,
σ2

where k ≥ 1 and

(ak,1, . . . , ak,k) = arg min

(α1,...,αk)∈Rk

E(ut − α1ut−1 − ··· − αkut−k)2.

(1.8)

(1.9)

(1.10)

To directly estimate Ω−1
e.g., Berk [1] and Wu and Pourahmadi [18]) of Ωn:

n , we start by deﬁning the modiﬁed Cholesky decomposition (see,

where

TnΩnT′

n = Dn,

Dn = diag(γ0, σ2

n−1),

1, σ2

2, . . . , σ2

and Tn = (tij)1≤i,j≤n is a lower triangular matrix satisfying

0,
1,
−ai−1,i−j,

tij =


if i < j;
if i = j;
if 2 ≤ i ≤ n, 1 ≤ j ≤ i − 1.

4

Hence,

C.-K. Ing, H.-T. Chiou and M. Guo

Ω−1

n = T′

nD−1

n Tn.

(1.11)

Because there are too many parameters in Tn and Dn, we are led to consider a banded
Cholesky decomposition of Ω−1
n ,

Ω−1

n (k) = T′

n(k)D−1

n (k)Tn(k),

(1.12)

where 1 ≤ k ≪ n is referred to as the banding parameter and allowed to grow to inﬁnity
with n,

Dn(k) = diag(γ0, σ2

1, . . . , σ2

k, . . . , σ2

k),

and Tn(k) = (tij (k))1≤i,j≤n with

0,
1,
−ai−1,i−j,
−ak,i−j ,
We propose estimating Ω−1

tij(k) =


if i < j or {k + 1 < i ≤ n, 1 ≤ j ≤ i − k − 1};
if i = j;
if 2 ≤ i ≤ k, 1 ≤ j ≤ i − 1;
if k + 1 ≤ i ≤ n, i − k ≤ j ≤ i − 1.
n using the sample counterpart of (1.12),

ˆΩ−1

n (k) := ˆT′

n(k) ˆD−1

n (k) ˆTn(k),

(1.13)

where ˆTn(k) and ˆDn(k) are obtained by plugging in the least squares estimates of the
coeﬃcients in Tn(k) and the corresponding residual variances in Dn(k); see Section 3
for more details.

Under (1.2)–(1.5), this paper establishes

k ˆΩ−1

n (k) − Ω−1

n k2 = op(1),

(1.14)

1 , . . . , u(1)

n )′, . . . , U(m) = (u(m)

with k = Kn → ∞ satisfying (3.16). To appreciate the subtlety of (1.14), note that if m
independent realizations U(1) = (u(1)
n )′ of {ut} are
available, Bickel and Levina [3] introduced alternative estimates ˇTn,m(k) and ˇDn,m(k)
of Tn(k) and Dn(k) through a multivariate analysis approach, where k < m < n. More
speciﬁcally, set ˜Uj = (u(1)
)′ and denote the regression coeﬃcients of ˜Uj on
˜Uj−1, . . . , ˜Umax {j−k,1} by ˇaj . Then ˇTn,m(k) and ˇDn,m(k), respectively, are obtained by
replacing the coeﬃcients in the ith row of Tn(k) with −ˇai, and ith diagonal element of
Dn(k) with the corresponding residual variance, where i = 2, . . . , n. Bickel and Levina [3]
also showed that the resultant estimate ˇΩ−1
n has
the property

n,m(k) ˇTn,m(k) of Ω−1

n,m(k) = ˇT′

n,m(k) ˇD−1

, . . . , u(m)

, . . . , u(m)

1

j

j

k ˇΩ−1

n,m(k) − Ω−1

n k2 = op(1),

(1.15)

Estimation of inverse autocovariance matrices

5

under m → ∞, m−1 log n → 0, k = Kn,m ≍ (m/ log n)θ for some 0 < θ < 1/2, (1.1), and
(1.16)

sup
k≥1kΩkk2 < ∞,

where g(x) ≍ h(x) means that there exists a constant 0 < C < ∞ such that C ≤
lim inf x→∞ h(x)/g(x) ≤ lim supx→∞ h(x)/g(x) ≤ C−1. Since (1.16) fails to hold for long-
memory processes like (1.7) and m → ∞ is needed in (1.15), the most distinctive feature
of (1.14) is that it holds for one (m = 1) realization, without imposing (1.16). It is also
noteworthy that Cai, Ren and Zhou [5] have recently established the optimal rate of
convergence for estimating the inverse of a Toeplitz covariance matrix under the spectral
norm. However, the covariance matrix associated with (1.7) is still precluded by the class
of matrices considered in their paper, which needs to obey assumptions like (1.16) and
(1.1).

n (k) and Ω−1

n (k) − Ω−1

The rest of the paper is organized as follows. In Section 2, we analyze the diﬀerence
between Ω−1
n . In particular, by deriving convergence rates of kTn(k)− Tnk2
and kDn(k) − Dnk2, we obtain a convergence rate of kΩ−1
n k2, which plays
an indispensable role in the proof of (1.14). Section 3 is devoted to proving (1.14). By
establishing a number of sharp bounds for the higher moments of the quadratic forms
in ut, we obtain a convergence rate of k ˆΩ−1
n (k)k2, which, in conjunction with
the results in Section 2, leads to a convergence rate of k ˆΩ−1
n k2, and hence
(1.14). In Section 4, the results in Section 3 are extended to regression models with long-
memory errors satisfying (1.2)–(1.5). Speciﬁcally, we show that when the unobservable
long-memory errors are replaced by the corresponding least squares residuals, our esti-
mate of Ω−1
n still has the same convergence rate, without imposing any assumptions on
the design matrices. Moreover, the estimated matrix is applied to construct an estimate
of the ﬁnite-past predictor coeﬃcient vector of the error process, and an FGLSE of the
regression coeﬃcient vector. Rates of convergence of the latter two estimates are also
derived in a somewhat intricate way. In Section 5, we present a Monte Carlo study of
the ﬁnite-sample performance of the proposed inverse matrix estimates.

n (k) − Ω−1

n (k) − Ω−1

2. Bias analysis of banded Cholesky factors

Our analysis of kΩ−1
deﬁned in (1.10).

n − Ω−1

n (k)k2 is reliant on the following two conditions on am,i’s

(i) There exists C1 > 0 such that for any 1 ≤ i ≤ m < ∞,
m − i + 1(cid:19)d

≤ C1(cid:18) m

am,i

.

(2.1)

(ii) There exist C2 > 0 and 0 < δ < 1 such that for any 1 ≤ i ≤ δm and 1 ≤ m < ∞,

≤ C2

i
m

.

(2.2)

(cid:12)(cid:12)(cid:12)(cid:12)

ai (cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)
ai − 1(cid:12)(cid:12)(cid:12)(cid:12)

am,i

6

C.-K. Ing, H.-T. Chiou and M. Guo

Some comments on (2.1) and (2.2) are in order. Note ﬁrst that (2.1) and (2.2), together

with (1.5), immediately imply that there exists C > 0 such that for any k = 1, 2, . . . ,

k

Xi=1

|ak,i| ≤ C,

(2.3)

which will be used frequently in the sequel. Throughout the rest of the paper, C denotes
a generic positive constant independent of any unbounded index sets of positive integers.
These two conditions assert that the ﬁnite-past predictor coeﬃcients am,i, i = 1, . . . , m
approach to the corresponding inﬁnite-past predictor coeﬃcients a1, a2, . . . in a nonuni-
form way. More speciﬁcally, they require that am,i/ai is very close to 1 when i = o(m),
but has order of magnitude m(1−θ)d when m− i ≍ mθ with 0 ≤ θ < 1. This does not seem
to be counterintuitive because for a long-memory process, the ﬁnite order truncation
tends to create severer upward distortions in those ai’s with i near the truncation lag
m + 1. In fact, when {ut} is an I(d) process with 0 < d < 1/2, (2.1) and (2.2) follow
directly from the proof of Theorem 13.2.1 of Brockwell and Davis [4]. In the following,
we shall show that (2.1) and (2.2) are satisﬁed by model (1.7). To this end, we need an
auxiliary lemma.

Lemma 2.1. Assume (1.2), (1.4),

and

ψi ∼ i−1+dL(i)

ai ∼

i−1−dd sin(πd)

πL(i)

,

(2.4)

(2.5)

where g(x) ∼ h(x) if limx→∞ g(x)/h(x) = 1 and L(x) is a positive slowly varying function,
namely, limx→∞ L(λx)/L(x) = 1 for all λ > 0. Then for all large m,

max

1≤i≤m(cid:12)(cid:12)(cid:12)(cid:12)

j=i∧(m+1−i) |aj|(cid:12)(cid:12)(cid:12)(cid:12)
m(am,i − ai)
P∞
where u ∧ v = min{u, v}.
Proof. For h, j ∈ N ∪ {0}, we deﬁne
ξh+j,
∞

≤ C,

if s = 1;

ds(h, j) =


where ξt =P∞

obtain

m|(am,i − ai)|

ξh+j+vds−1(h, v),

if s = 2, 3, . . . ,

Xv=0

v=0 ψvav+t for t = 0, 1, . . . . By Theorem 2.9 of Inoue and Kasahara [11], we

Estimation of inverse autocovariance matrices

7

(2.6)

∞

= m(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
Xs=1

ai+jd2s(m + 1, j) +

∞

Xj=0

∞

∞

Xs=1

Xj=0

am+1−i+j d2s−1(m + 1, j)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

.

Let κ > 1 satisfy 0 < κ sin(πd) < 1. According to Proposition 3.2(i) of Inoue and Kasahara
[11], there exists N ∈ N such that

0 < ds(h, j) ≤

gs(0){κ sin(πd)}s

h

,

j ∈ N ∪ {0}, s ∈ N, h ≥ N,

(2.7)

,

if s = 1;

if s = 2;

1

π(1 + x)
1

π2 Z ∞
πs Z ∞

1

0

0

,

dv1

(v1 + 1)(v1 + 1 + x)

···Z ∞

0

1

vs−1 + 1

where

gs(x) =




×(s−2
Yj=1

1

vj+1 + vj + 1)

1

v1 + 1 + x

dvs−1 ··· dv1,

if s = 3, 4, . . . .

Thus, for m ≥ N and i = 1, 2, . . . , m,

∞

∞

∞

ai+jd2s(m + 1, j)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
m(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
Xj=0
Xs=1
|ai+j|  ∞
Xs=1
Xj=0
≤ m
Xs=1
|aj|

Xj=i

m + 1

m

=

∞

∞

m + 1

g2s(0){κ sin(πd)}2s

!
g2s(0){κ sin(πd)}2s ≤ C

(2.8)

∞

Xj=i

|aj|,

where the ﬁrst inequality is by (2.7) and the last one is by Lemma 3.1(i) of Inoue and
Kasahara [11]. Similarly, (2.7) and Lemma 3.1(ii) of Inoue and Kasahara [11] imply

∞

∞

Xs=1

Xj=0

m(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

am+1−i+jd2s−1(m + 1, j)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

≤ C

∞

Xj=m+1−i

|aj|.

Combining (2.6), (2.8) and (2.9) yields the desired conclusion.

(2.9)

(cid:3)

8

C.-K. Ing, H.-T. Chiou and M. Guo

Remark 2.1. Theorem 3.3 of Inoue and Kasahara [11] shows that for any ﬁxed integer
i,

lim
m→∞

m(am,i − ai) = d2

aj.

∞

Xj=i

Therefore, Lemma 2.1 can be viewed as a uniform extension of (2.10).

Remark 2.2. Note that (1.3) and (1.5) are fulﬁlled by (2.4) and (2.5) if

0 < inf

i L(i) ≤ sup

i L(i) < ∞.

(2.10)

(2.11)

By making use of Lemma 2.1, the next theorem shows that (2.1) and (2.2) are met
by (2.4) and (2.5) with L(i) obeying (2.11). Since the coeﬃcients of the MA and AR
representations of (1.7) take the form of (2.4) and (2.5), respectively, for which L(i) is
a constant function (see Corollary 3.1 of Kokoszka and Taqqu [12] and Example 2.6 of
Inoue and Kasahara [11]), this theorem guarantees that (1.7) satisﬁes (2.1) and (2.2),
conﬁrming the ﬂexibility of these two conditions.

Theorem 2.1. Under the same assumptions as in Lemma 2.1 with L(i) satisfying (2.11),
we have (2.1) and (2.2).

Proof. It suﬃces to show that (2.1) and (2.2) hold for all suﬃciently large m. By
Lemma 2.1 and (2.11), it follows that for all 1 ≤ i ≤ m and all large m,

|m(am,i − ai)| ≤ C max{i−d, (m − i + 1)−d},

yielding

m(am,i − ai)

m − i + 1(cid:19)d
Therefore, (2.1) follows. Similarly, for all 1 ≤ i ≤ δm with 0 < δ < 1 and all large m,

+ 1 ≤ C(cid:18) m

max{i−d, (m − i + 1)−d}

=(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)

am,i

ai (cid:12)(cid:12)(cid:12)(cid:12)

mi−1−d

.

mai

≤ C

am,i − ai

ai

max{i−d, (m − i + 1)−d}

mi−1−d

i
m

,

≤ C

≤ C

+ 1(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)

which leads to (2.2). Thus the proof is complete.

(cid:3)

Throughout the rest of this paper, let Kn denote a sequence of numbers satisfying
Kn → ∞ and Kn/n → 0 as n → ∞. We are now ready to provide upper bounds for
kTn − Tn(Kn)k2 and kD−1
n − Dn(Kn)−1k2 in Propositions 2.1 and 2.2, which in turn
lead to a rate of convergence of kΩ−1
n (Kn)k2 in Theorem 2.2. Before proceeding,
we need two technical lemmas.

n − Ω−1

Estimation of inverse autocovariance matrices

Lemma 2.2. Assume (1.5), (2.1) and (2.2). Then

n

j=Kn+1 |ak,j| ≤ CK −d
j=1 |ak,j − aKn,j| ≤ CK −d
j=max(1,Kn+1−k) |aj+k,j − aKn,j| ≤ CK −d

n

n

for any Kn + 1 ≤ k ≤ n − 1.

|aj+k,j − aKn,j| ≤ CK −d

n

for any n − Kn ≤ k ≤ n − 2.

for any Kn + 1 ≤ k ≤ n − 1.

for any 1 ≤ k ≤ n − Kn − 1.

(i) Pk
(ii) PKn
(iii) PKn
(iv) Pn−k−1

j=1

Proof. The proof is straightforward, and thus omitted.

9

(cid:3)

Lemma 2.3. Assume (1.2)–(1.5). Then for any k ≥ 1, σ2
deﬁned in (1.9).

k − σ2 ≤ Ck−1, where σ2

k is

k− σ2 ≤ E(P∞
Proof. In view of (1.4) and (1.10), it follows that for any k ≥ 1, σ2
j=k+1 aj ×
ut−j)2. In addition, by (1.6) (which is ensured by (1.2) and (1.3)), (1.5), and Theorem 2.1
of Ing and Wei [10], one has for any k ≥ 1 and m ≥ k + 1, E(Pm
j=k+1 ajut−j)2 ≤ Ck−1,

which, together with the previous inequality, gives the desired conclusion.

(cid:3)

Proposition 2.1. Under the same assumptions as in Lemma 2.2,

(i) kTn − Tn(Kn)k2 = O((K −d
(ii) kTn(Kn)k2 = O((log Kn)1/2).

n log n)1/2).

Proof. Let kBkk = maxkzkk=1 kBzkk denote the k-norm of an h × h matrix B,
where kzkk = (Ph
i=1 |zi|k)1/k is the k-norm of the vector z = (z1, . . . , zh)′. Then, by

Lemma 2.2(i) and (ii),

i

kTn − Tn(Kn)k∞ =

Xj=Kn+1
Moreover, kTn − Tn(Kn)k1 is the maximum of

Kn+1≤i≤n−1

max

|ai,j| +

Kn

Xj=1

|aKn,j − ai,j| = O(K −d
n ).

max

0≤k≤n−Kn−1(n−Kn−k−2
Xi=0
and maxn−Kn≤k≤n−2Pn−k−1

j=1

max

0≤k≤n−Kn−1

|aKn+1+i+k,Kn+1+i| +

Kn

Xj=max(1,Kn+1−k)

|aj+k,j − aKn,j|)

|aj+k,j − aKn,j|. By (2.1) and Lemma 2.2(iii) and (iv),

n−Kn−k−2

Xi=0

Kn

|aKn+1+i+k,Kn+1+i| = O(log n),

max

0≤k≤n−Kn−1

Xj=max(1,Kn+1−k)

|aj+k,j − aKn,j| = O(K −d
n )

10

and

C.-K. Ing, H.-T. Chiou and M. Guo

max

n−Kn≤k≤n−2

n−k−1

Xj=1

|aj+k,j − aKn,j| = O(K −d
n ).

Hence, kTn − Tn(Kn)k1 = O(log n). The proof of (i) is completed by

kTn − Tn(Kn)k2 ≤ (kTn − Tn(Kn)k1kTn − Tn(Kn)k∞)1/2.

Similarly, it can be shown that kTn(Kn)k∞ = O(1) and kTn(Kn)k1 = O(log Kn), yielding
(ii).

(cid:3)

Proposition 2.2. Under the same assumptions as in Lemma 2.3,

(i) kD−1
(ii) kD−1

n (Kn)k2 = O(K −1
n ).

n − D−1
n (Kn)k2 = O(1).

Proof. Equation (i) is an immediate consequence of Lemma 2.3. Equation (ii) follows
from kD−1
(cid:3)
Theorem 2.2. Assume (1.2)–(1.5), (2.1) and (2.2). Suppose

n (Kn)k2 = max0≤k≤Kn σ−2

k ≤ σ−2, where σ2

0 = γ0.

log n log Kn

K d
n

= o(1).

Then

n − Ω−1
Moreover, if (1.8) is assumed,

kΩ−1

n (Kn)k2 = O(log nK −d

n log Kn)1/2 = o(1).

Proof. Equation (2.13) follows directly from Propositions 2.1 and 2.2,

kΩ−1

n (Kn)k2 = O(1).

(2.12)

(2.13)

(2.14)

kΩ−1

n − Ω−1

n (Kn)k2 ≤ kTn − Tn(Kn)k2kD−1

n k2(kTn − Tn(Kn)k2 + kTn(Kn)k2)

+ kTn(Kn)k2kD−1
+ kTn(Kn)k2kD−1

n − D−1
n (Kn)k2kTn − Tn(Kn)k2,

n (Kn)k2(kTn − Tn(Kn)k2 + kTn(Kn)k2)

and (2.12). Equations (2.13) and (1.1) (which is ensured by (1.8)) further lead to (2.14). (cid:3)

3. Main results

In the sequel, the following assumptions on the innovation process {wt} of (1.2) are
frequently used:

Estimation of inverse autocovariance matrices

11

of σ-ﬁeld generated by ws, s ≤ t.

(M1) {wt,Ft} is a martingale diﬀerence sequence, where Ft is an increasing sequence
(M2) E(w2
(M3) For some q ≥ 1, there is a constant Cq > 0 such that
a.s.

t |Ft−1) = σ2 a.s.

sup

−∞<t<∞

E(|wt|4q|Ft−1) ≤ Cq

As mentioned in Section 1, ˆTn(Kn) is obtained by replacing a(k) = (ak,1, . . . , ak,k)′ in
Tn(Kn) with the corresponding the least squares estimates ˆa(k) = (ˆak,1, . . . , ˆak,k)′, where
k = 1, . . . , Kn and

(ˆak,1, . . . , ˆak,k) = arg min

(α1,...,αk)∈Rk

n

Xt=k+1

(ut − α1ut−1 − α2ut−2 − ··· − αkut−k)2.

Similarly, ˆDn(Kn) is obtained by replacing σ2
and

k in Dn(Kn) with ˆσ2

k, where k = 0, . . . , Kn

n

n

ˆσ2
0 = (n − 1)−1

ˆσ2
k = (n − k)−1

Xt=1
(ut − ¯u)2,
Xt=k+1 ut −

n

k

Xj=1

ui,

¯u = n−1

Xt=1
ˆak,jut−j!2

.

n (Kn) = ˆT′
n (Kn) − Ω−1

n (Kn) ˆTn(Kn). The objective of this section is to show
n(Kn) ˆD−1
Recall ˆΩ−1
that k ˆΩ−1
n k2 = op(1) in Theorem 3.1. To this end, we develop rates of con-
vergence of k ˆTn(Kn) − Tn(Kn)k2 and k ˆD−1
n (Kn)k2 in Propositions 3.1 and
3.2, respectively, whose proofs are heavily reliant on the following four lemmas, Lemmas
3.1–3.4.

n (Kn) − D−1

Lemma 3.1. Assume (1.2)–(1.5) and (M1)–(M3). Let Ut(k) = (ut, ut−1, . . . , ut−k+1)′
and wk,t+1 = ut+1 − a(k)′Ut(k). Then for any 1 ≤ k ≤ n − 1,
≤ C(cid:18) 1

n − k(cid:19)q(1−2d)

n − k

(3.1)

2q

1

2

and

Moreover, for θ > 1/q,

E(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

E(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

2q

n−1

Ut(k)(wk,t+1 − wt+1)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Xt=k
Ut(k)wk,t+1(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
≤ C(cid:26)(cid:18) 1
Ut(k)wk,t+1(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Xt=k

n − k

n−1

1

2

2

2

n−1

1

n − k

Xt=k
1≤k≤Kn(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

max

n − k(cid:19)q(1−2d)

+(cid:18) k

n − k(cid:19)q(cid:27).

= Op(cid:18) K θ

n1−2d +

n

K 1+θ

n

n (cid:19).

(3.2)

(3.3)

12

C.-K. Ing, H.-T. Chiou and M. Guo

Proof. By (1.6), Lemma 2.3 and an argument similar to that used in Lemma 3 of Ing
and Wei [9], one has for any 1 ≤ k ≤ n − 1,

2q

2

1

n−1

Xt=k

E(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
n − k
≤ C(n − k)−qkq((σ2

Ut(k)(wk,t+1 − wt+1)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Xi=−(n−k)+1

k − σ2)

n−k−1

n − k(cid:19)q(1−2d)
which gives (3.1). Equation (3.2) follows from (3.1) and for any 1 ≤ k ≤ n − 1,

≤ C(cid:18) 1

|γi|)q

,

≤ Ckq(n − k)−q,

(3.4)

1

n − k

n−1

Xt=k

1

n − k

n−1

Xt=k

2q

2

Ut(k)wt+1(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Ut(k)wk,t+1(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

2q

2

E(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
1≤k≤Kn(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Xk=1

≤ C

Kn

E max

whose proof is exactly same as that of Lemma 4 of Ing and Wei [9]. To show (3.3), note
that by (3.2) and Kn = o(n),

{n−q(1−2d) + kqn−q} ≤ C{Knn−q(1−2d) + K q+1

n n−q}.

This, together with θ > 1/q, gives the desired conclusion (3.3).

(cid:3)

Remark 3.1. Lemma A.1 of Godet [8] establishes an inequality closely related to (3.1).
In particular, the inequality yields

1

√n − k

n−1

Xt=k

E(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

Ut(k)(wk,t+1 − wt+1)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

This bound together with Lemma 2.3 also leads to (3.1).

2q

2

≤ C{k(n − k)2d}q(σ2

k − σ2)q.

Lemma 3.2. Let

ˆΓk,n =

1

n − k

n−1

Xt=k

Ut(k)Ut(k)′.

Assume (1.2), (1.3), (1.8) and (M1)–(M3) with q = 1. Suppose

o(n1/2),
o((n/ log n)1/2),
o(n1−2d),

if 0 < d < 1/4;
if d = 1/4;
if 1/4 < d < 1/2.

Kn =


(3.5)

Estimation of inverse autocovariance matrices

Then

kˆΓ−1

Kn,nk2 = Op(1).

13

(3.6)

Proof. By the ﬁrst moment bound theorem of Findley and Wei [7], (1.6) and an argu-
ment similar to that used in Lemma 2 of Ing and Wei [9], it follows that

Ek ˆΓKn,n − ΩKnk2

2 =


O(K 2
O(K 2
O(K 2

n(n − Kn)−1),
n(n − Kn)−1 log(n − Kn)),
n(n − Kn)−2+4d),

if 0 < d < 1/4;
if d = 1/4;
if 1/4 < d < 1/2.

Combining this, (3.5) and (1.8) leads to (3.6).

(3.7)

(cid:3)

Lemma 3.3. Under the same assumptions as in Theorem 2.2, one has for any k ≥
1 and m = 0,±1,±2, . . ., γτk (m) = C(|m| + 1)−1+2d, where with τk,t = ut+1 − wt+1 −
a′(k)Ut(k) = wk,t+1 − wt+1, γτk (m) = E(τk,1τk,m+1).

Proof. This result follows by a tedious but direct calculation. The details are omitted. (cid:3)

Lemma 3.4. Assume that (2.1), (2.2), and the assumptions of Lemma 3.1 hold. Then,
for any 1 ≤ k ≤ n − 1,

n−1

Xt=k

w2
k,t+1 − σ2

2q

k(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Moreover, for θ > 1/(2q),

C(n − k)−q,
C({(n − k)−1 log(n − k)}q),
C(n − k)−2q+4qd,

if 0 < d < 1/4,
if d = 1/4,
if 1/4 < d < 1/2.

(3.8)

1

n − k

n−1

Xt=k

w2
k,t+1 − σ2

Op(K θ
Op(K θ
Op(K θ

nn−1/2),
n(log n)1/2n−1/2),
nn−1+2d),

if 0 < d < 1/4,
if d = 1/4,
if 1/4 < d < 1/2.

(3.9)

Proof. To show (3.8), note ﬁrst that

n−1

1

n − k

Xt=k
where E|(A1)|2q = E| 1
n−kPn−1
E|(A3)|2q = E| 1

k,t+1 − σ2
w2
n−kPn−1
t=k w2
k,t − (σ2

t=k τ 2

≤ C(E|(A1)|2q + E|(A2)|2q + E|(A3)|2q),

(3.10)

t+1 − σ2|2q, E|(A2)|2q = E| 2
k − σ2)|2q. It is clear that for any 1 ≤ k ≤ n − 1,
E|(A1)|2q ≤ C(n − k)−q.

n−kPn−1

t=k wt+1τk,t|2q, and

(3.11)

1

max

n − k

E(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
1≤k≤Kn(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
E(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

≤

=
k(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

k(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

2q

14

C.-K. Ing, H.-T. Chiou and M. Guo

In addition, the ﬁrst moment bound theorem of Findley and Wei [7] implies that for any
1 ≤ k ≤ n − 1,

E|(A2)|2q ≤ C((n − k)−1γτk (0))q,
E|(A3)|2q ≤ C((n − k)−1
Xj=0
γ2

n−k−1

τk (j))q

,

which, together with Lemmas 2.3 and 3.3, (3.10) and (3.11), yield (3.8). Equation (3.9)
follows immediately from (3.8) and an argument similar to that used to prove (3.3). The
details are omitted.
(cid:3)

We are now ready to establish rates of convergence of k ˆTn(Kn) − Tn(Kn)k2 and
n (Kn) − D−1

n (Kn)k2.

k ˆD−1

Proposition 3.1. Assume (1.2)–(1.5), (1.8) and (M1)–(M3). Suppose (3.5). Then for
any θ > 1/q,

k ˆTn(Kn) − Tn(Kn)k2

2 = Op(cid:18) K 1+θ
n1−2d +
Proof. Let Sn = (sij)1≤i,j≤n = ˆTn(Kn) − Tn(Kn). Then

n

K 2+θ

n

n (cid:19).

(3.12)

max
1≤i≤n

n

Xt=1

s2
it ≤ max

1≤k≤Knkˆa(k) − a(k)k2
2,

and for each 1 ≤ j ≤ n, ♯Bj ≤ 2Kn − 1, where Bj = {i: Pn

algebraic manipulations yield

t=1 sitsjt 6= 0}. These and some

kSnS′

nk1 = max

n

≤ max

sitsjt(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
1≤j≤n Xi∈Bj(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
Xt=1
it!1/2  n
1≤j≤n Xi∈Bj  n
Xh=1
Xt=1
1≤k≤Kn(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Kn,nk2
≤ CKnk ˆΓ−1

2 max

s2

1

n − k

Now, the desired conclusion (3.12) follows from (3.3), (3.6) and kSnk2

2 ≤ kSnS′

nk1.

(cid:3)

≤ CKn max

1≤k≤Knkˆa(k) − a(k)k2

2

s2

n−1

jh!1/2
Ut(k)wk,t+1(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

Xt=k

2

2

.

Estimation of inverse autocovariance matrices

15

Proposition 3.2. Assume (2.1), (2.2), and the same assumptions as in Proposition 3.1.
Suppose (3.5). Then for any θ > 1/q,

k ˆD−1

n (Kn) − D−1

n (Kn)k2 =


Proof. Note ﬁrst that

n),

Op(n−1/2K θ
Op((log n/n)1/2K θ
Op(n−1+2dK θ

n),

n),

if 0 < d < 1/4,
if d = 1/4,
if 1/4 < d < 1/2.

k ˆDn(Kn) − Dn(Kn)k2 = max

0≤k≤Kn|ˆσ2

k − σ2
k|,

(3.13)

(3.14)

recalling σ2
that

0 = γ0. By (1.6) and an argument similar to that used to prove (3.7), it holds

E(ˆσ2

0 − σ2

O(n−1),
O(log n/n),
O(n−2+4d),

if 0 < d < 1/4,
if d = 1/4,
if 1/4 < d < 1/2.

(3.15)

Straightforward calculations show

1≤k≤Kn|ˆσ2
max

k − σ2

k| ≤ max

w2
k,t+1 − σ2

n−1

1

n − k

Xt=k
1≤k≤Kn(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Kn,nk2 max

k(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
Xt=k

n−1

1

n − k

2

2

,

Ut(k)wk,t+1(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

0)2 =

1≤k≤Kn(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

+ Ck ˆΓ−1

which, in conjunction with (3.15), (3.14), (3.3), (3.6) and (3.9), results in (3.13).

The main results of this section is given as follows.

Theorem 3.1. Assume the same assumptions as in Proposition 3.2. Suppose

log n log Kn

K d
n

+

K 1+θ

n

log Kn

n1−2d

K 2+θ

n

+

log Kn
n

= o(1),

(cid:3)

(3.16)

for some θ > 1/q. Then

k ˆΩ−1
n (Kn) − Ω−1
n k2
= Op(cid:18)(cid:18) log n log Kn

K d
n

(cid:19)1/2

= op(1)

and

log Kn

n

+(cid:18) K 1+θ

n1−2d

K 2+θ

n

+

log Kn
n

(cid:19)1/2(cid:19)

(3.17)

k ˆΩ−1

n (Kn)k2 = Op(1).

(3.18)

16

C.-K. Ing, H.-T. Chiou and M. Guo

Proof. By Propositions 3.1 and 3.2, (1.8), (ii) of Proposition 2.1, (ii) of Proposition 2.2
and

k ˆΩ−1

n (Kn) − Ω−1
≤ k ˆTn(Kn) − Tn(Kn)k2k ˆD−1

n (Kn)k2

n (Kn)k2(k ˆTn(Kn) − Tn(Kn)k2 + kTn(Kn)k2)

+ kTn(Kn)k2k ˆD−1
+ kTn(Kn)k2kD−1

n (Kn) − D−1
n (Kn)k2k ˆTn(Kn) − Tn(Kn)k2,

n (Kn)k2(k ˆTn(Kn) − Tn(Kn)k2 + kTn(Kn)k2)

one obtains

k ˆΩ−1

n (Kn) − Ω−1

n (Kn)k2 = Op(cid:18)(cid:18) K 1+θ

n

n1−2d

log Kn

K 2+θ

n

+

log Kn
n

(cid:19)1/2(cid:19).

This, together with (3.16) and Theorem 2.2, leads to the desired conclusions (3.17) and
(3.18).
(cid:3)

Remark 3.2. It would be interesting to compare Theorem 3.1 with the moment bounds
for ˆΓ−1
Kn,n given by Godet [8]. If {ut} is a Gaussian process satisfying (1.2)–(1.5) and
(1.8), then Theorem 2.1 of Godet [8] yields that for

EkˆΓ−1

Kn,n − Ω−1

Kn = O(nλ)

Knk2 =


with 0 < λ < min{1/2, 1− 2d},
if 0 < d < 1/4,
if d = 1/4,
if 1/4 < d < 1/2.

O(n−1/2Kn),
O((log n/n)1/2Kn),
O(n−1+2dKn),

(3.19)

(3.20)

n (Kn) and ˆΓ−1

One major diﬀerence between ˆΩ−1
Kn,n is that the former aims at estimat-
ing the inverse autocovariance matrix of all n observations, Ω−1
n , but the latter only
focuses on that of Kn consecutive observations, Ω−1
, with Kn ≪ n. While (3.20) plays
Kn
an important role in analyzing the mean squared prediction error of the least squares
predictor of un+1 based on the AR(Kn) model, ˆΓ−1
Kn,n cannot be used in situations where
consistent estimates of Ω−1
n are indispensable. See Section 4.2 for some examples. More-
over, the convergence rate of ˆΩ−1
n (Kn) is determined by not only the estimation error
k ˆΩ−1
n k2. This
latter type of error, however, is irrelevant to the convergence rate of ˆΓ−1
Kn,n. Finally, we
note that (3.20) gives a stronger mode of convergence than (3.17), but at the expense of
more stringent assumptions on moments and distributions.

n (Kn)k2, but also the approximation error kΩ−1

n (Kn) − Ω−1

n (Kn) − Ω−1

Estimation of inverse autocovariance matrices

17

4. Some extensions

Consider a linear regression model with serially correlated errors,

yt = x′

tβ + ut =

p

Xi=1

xtiβi + ut,

(4.1)

where β is an unknown coeﬃcient vector, xt’s are p-dimensional nonrandom input vectors
and ut’s are unobservable random disturbances satisfying the long-memory conditions
described previously. Having observed yn = (y1, . . . , yn)′ and ˇxnj = (x1j , . . . , xnj)′, 1 ≤
j ≤ p, it is natural to estimate un = (u1, . . . , un)′ via the least squares residuals

˜un = (˜u1, . . . , ˜un)′ = (In − Mnp)yn = (In − Mnp)un,

where In is the n × n identity matrix, and Mnp is the orthogonal projection matrix of
sp{ˇxn1, . . . , ˇxnp}, the closed span of {ˇxn1, . . . , ˇxnp}. Note that ˜un is also known as a
detrended time series, in particular when xt represents the trend or seasonal compo-
nent of yt. Let {ˇqni = (q1i, . . . , qni)′, i = 1, . . . , r}, 1 ≤ r ≤ p, be an orthonormal basis of
sp{ˇxn1, . . . , ˇxnp}. It is well known that Mnp =Pr
niun,
Xi=1

ni, and hence with vi = ˇq′

˜un = un −

i=1 ˇqni ˇq′

(4.2)

r

vi ˇqni.

n (Kn) and ˆΩ−1

In Section 4.1, we shall show that the inverse autocovariance matrix, Ω−1
n , of un can still
be consistently estimated by the modiﬁed Cholesky decomposition method proposed in
Section 3 with un replaced by ˜un, which is denoted by ˜Ω−1
n (Kn). We also show that
˜Ω−1
n (Kn) share the same rate of convergence. Moreover, we propose an
estimate of a(n) = (an,1, . . . , an,n)′, the n-dimensional ﬁnite predictor coeﬃcient vector
of {ut}, based on ˜Ω−1
n (Kn), and derive its convergence rate. These asymptotic results are
obtained without imposing any assumptions on the design matrix Xn = (ˇxn1, . . . , ˇxnp).
On the other hand, we assume that Xn has a full rank in Section 4.2, and propose an
FGLSE of β based on ˜Ω−1
n (Kn). The rate of convergence of the proposed FGLSE is also
established in Section 4.2.

4.1. Consistent estimates of Ω−1

n and a(n) based on ˜un

Deﬁne

˜Ω−1

n (Kn) := ˜Tn(Kn)′ ˜D−1

n (Kn) ˜Tn(Kn)

where ˜Tn(Kn) and ˜Dn(Kn) are ˆTn(Kn) and ˆDn(Kn) with ˆaij and ˆσ2
replaced by ˜aij and ˜σ2

i deﬁned as follows:

i , respectively,

(˜ak,1, . . . , ˜ak,k) = arg min

(α1,...,αk)∈Rk

(˜ut − α1 ˜ut−1 − α2 ˜ut−2 − ··· − αk ˜ut−k)2,

Xt=k+1

n

18

C.-K. Ing, H.-T. Chiou and M. Guo

n

n

˜σ2
0 = (n − 1)−1

˜σ2
k = (n − k)−1

Xt=1
(˜ut − ¯˜u)2,
Xt=k+1 ˜ut −

n

k

Xj=1

˜ui,

¯˜u = n−1

Xt=1
˜ak,j ˜ut−j!2

.

By establishing probability bounds for k ˜Tn(Kn)− Tn(Kn)k2 and k ˜D−1
n (Kn)− D−1
n (Kn)k2
in Proposition 4.1, we obtain the convergence rate of k ˜Ω−1
n (Kn)− Ω−1
n k2 in Theorem 4.1.
According to (4.2), un and ˜un diﬀer by the vectorPr
i=1 vi ˇqni, whose entries are weighted
sums of u1, u2, . . . , un with weights qt1,iqt2,j for some 1 ≤ t1, t2 ≤ n and 1 ≤ i, j ≤ r. To ex-
plore the contributions ofPr
n (k)k2, we
need moment bounds for the linear combinations of ui’s and τk,i’s, which are introduced
in the following lemma.

i=1 vi ˇqni to k ˜Tn(k)− Tn(k)k2 and k ˜D−1

n (k)− D−1

Lemma 4.1. Let c1, . . . , cm be any real numbers. Under the same assumptions as in
Lemma 3.1,

E  m
Xi=1

ciui!4q

≤ C  m
Xi=1

c2

i!2q

m4qd.

Moreover, if (2.1) and (2.2) also hold true, then

(4.3)

(4.4)

ciτk,i!4q

E  m
Xi=1

≤ C  m
Xi=1
i=1 ciui)4q ≤ C{E(Pm
Proof. By Lemma 2 of Wei [16], we have E(Pm
orem 2.1 of Ing and Wei [10] and Jensen’s inequality further yield E(Pm
C(Pm

i=1 ciui)2}2q. The-
i=1 ciui)2 ≤
i ). Hence, (4.3) follows. Equation (4.4) is en-
(cid:3)

i=1 |ci|2/(1+2d))1+2d ≤ Cm2d(Pm

sured by Lemma 3.3 and an argument similar to that used to prove (4.3).

i=1 c2

m4qd.

c2

i!2q

Equipped with Lemma 4.1, we can prove another auxiliary lemma, which plays a key
role in establishing Proposition 4.1. First, some notation: ˜wk,t+1 = ˜ut+1 − a(k)′ ˜Ut(k),
˜Ut(k) = (˜ut, ˜ut−1, . . . , ˜ut−k+1)′, ˜Γk,n = 1
˜Ut(k) ˜Ut(k)′, qt = (qt,1, qt,2, . . . , qt,r)′,
Qt(k) = (qt, qt−1, . . . , qt−k+1)′ and Vn = (v1, . . . , vr)′.

n−kPn−1

t=k

Lemma 4.2.

(i) Assume that the same assumptions as in Lemma 3.4 hold. Then for Kn = o(n)

and θ > 1/q,

1

n − k

n−1

Xt=k

max

1≤k≤Kn(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

˜Ut(k) ˜wk,t+1(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

2

2

= Op(cid:18) K θ

n1−2d +

n

K 1+θ

n

n (cid:19).

Estimation of inverse autocovariance matrices

19

(ii) Assume that the same assumptions as in Lemma 3.2 hold. Then for Kn satisfying

(iii) Assume that the same assumptions as in Lemma 3.4 hold. Then for Kn = o(n)

Kn,nk2 = Op(1).

(3.5), k ˜Γ−1
and θ > 1/(2q),

n−1

Xt=k

1

max

n − k

1≤k≤Kn(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
n−kPn−1

˜w2
k,t+1 − σ2

k(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

=


Op(K θ
Op(K θ
Op(K θ

nn−1/2),
n(log n)1/2n−1/2),
nn−1+2d),

if 0 < d < 1/4;
if d = 1/4;
if 1/4 < d < 1/2.

Proof. We begin by proving (i). Deﬁne (B1) = k 1
(B2) = k 1

˜Ut(k)wt+1k2q

t=k

n−kPn−1

t=k

2 . Straightforward calculations yield

˜Ut(k)( ˜wk,t+1 − wt+1)k2q

2 and

1

n − k

n−1

Xt=k

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

˜Ut(k) ˜wk,t+1(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

2q

2

≤ C{(B1) + (B2)},

(4.5)

E(B1) ≤ Cn−2qkq−1

k

Xj=1

{E|(B3)|2q + E|(B4)|2q + E|(B5)|2q + E|(B6)|2q}

(4.6)

and

where

1

n − k

n−1

Xt=k

2q

2

Ut(k)wt+1(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

+ E(B7)),

(4.7)

E(B2) ≤ C(E(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Xt=k

(B3) =

n−1

n−1

(B6) = V′

(B5) = V′
n

Xt=k
n(n−1
Xt=k
(B7) =(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
( 1
n − k

ut+1−jτk,t,

(B4) = V′
n

n−1

Xt=k

qt+1−j τk,t,

(qt+1 − Q′

t(k)a(k))ut+1−j,

qt+1−j(q′

t+1 − a(k)′Qt(k)))Vn,
Qt(k)wt+1)Vn(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

2q

2

.

n−1

Xt=k

20

C.-K. Ing, H.-T. Chiou and M. Guo

An argument similar to that used to prove (3.1) implies E|(B3)|2q = O(k−q(n− k)q+2qd).
In addition, by (4.3), (4.4), (2.3) and Pn

2
t,i = 1 for i = 1, 2, . . . , r, one obtains

t=1 q

E|(B4)|2q ≤ r2q−1

Xi=1

r

[E(v4q

i )]1/2"E n−1
Xt=k

qt+1−j,iτk,t!4q#1/2

≤ Cn4qd,

E|(B5)|2q ≤ Cn4qd, E|(B6)|2q ≤ Cn4qd, and E(B7) ≤ Ckqn2qd/n2q. With the help of
these moment inequalities, (3.4) and (4.5)–(4.7), the proof of (i) can be completed in
the same way as the proof of (3.3). Moreover, by modifying the proofs of (3.6) and (3.9)
accordingly, we can establish (ii) and (iii). The details, however, are not presented here. (cid:3)

Proposition 4.1. Assume the same assumptions as in Proposition 3.2. Suppose (3.5).
Then for any θ > 1/q,

(i) k ˜Tn(Kn) − Tn(Kn)k2
(ii)

2 = Op((K 1+θ

n

/n1−2d) + (K 2+θ

n

/n)).

k ˜D−1

n (Kn) − D−1

n (Kn)k2 =


n),

Op(n−1/2K θ
Op((log n/n)1/2K θ
Op(n−1+2dK θ

n),

n),

if 0 < d < 1/4;
if d = 1/4;
if 1/4 < d < 1/2.

Proof. In view of the proof of Proposition 3.1, (i) follows directly from (i) and (ii)
0 replaced by ˜σ2
of Lemma 4.1. To show (ii), note ﬁrst that (3.15) still holds with ˆσ2
0 .
This, in conjunction with (i)–(iii) of Lemma 4.1 and the argument used in the proof of
Proposition 3.2, yields (ii).
(cid:3)

We are now in a position to introduce Theorem 4.1.

Theorem 4.1. Consider the regression model (4.1). With the same assumptions as in
Proposition 3.2, suppose that (3.16) holds for some θ > 1/q. Then

k ˜Ω−1
n (Kn) − Ω−1
n k2
= Op(cid:18)(cid:18) log n log Kn

K d
n

(cid:19)1/2

and

n

+(cid:18) K 1+θ

log Kn

n1−2d

K 2+θ

n

+

log Kn
n

(cid:19)1/2(cid:19) = op(1)

k ˜Ω−1

n (Kn)k2 = Op(1).

(4.8)

(4.9)

Proof. In view of the proof of Theorem 3.1, (4.8) and (4.9) are immediate consequences
of Proposition 4.1 and Theorem 2.2.
(cid:3)

Remark 4.1. Since no assumptions are imposed on the design matrix Xn, one of the
most intriguing implications of Theorem 4.1 is that Ω−1
n can be consistently estimated

Estimation of inverse autocovariance matrices

21

by ˜Ω−1
interesting to point out that ˜Ω−1

n (Kn) even when Xn is singular. Moreover, according to (4.8) and (3.17), it is
n (Kn) share the same rate of convergence.

n (Kn) and ˆΩ−1

n γn is given by ˇa(n) = Ω−1

Next, we consider the problem of estimating a(n) under model (4.1). Recall Yule–
n γn, where γ n = (γ1, . . . , γn)′. A truncated version of
Walker equations a(n) = Ω−1
Ω−1
is an n-
dimensional vector. A natural estimate of ˇa(n) is a∗(n) = ˜Ω−1
n (Kn)˜γ n, where ˜γn =
(˜γ1, . . . , ˜γKn, 0, . . . , 0)′ is an n-dimensional vector with ˜γj denoting the (1, j + 1)th en-
try of ˜ΓKn+1,n. We shall show that when Kn is suitably chosen, a∗(n) is a consistent
estimate of a(n).

n (Kn)ˇγn, where ˇγ n = (γ1, . . . , γKn , 0, . . . , 0)′

Corollary 4.1. Assume the same assumptions as in Theorem 4.1. Suppose that (3.16)
holds and K 1+2d/3

n1−2d = o(1). Then for any θ > 1/q,

n

ka∗(n) − a(n)k2 =


= op(1).

K 1−2d

n

Op(cid:18)(cid:18) 1
Op(cid:18)(cid:18) 1

n

K 1−2d

+

+

log n
K d
n
log n
K d
n

+

+

n

K 1+θ
n1−2d +
K 1+θ
n1−2d +

n

n

K 2+θ

n (cid:19)1/2(cid:19),
n3−6d (cid:19)1/2(cid:19),

K 3+2d

n

Proof. Note ﬁrst that

if 0 < d ≤ 1/4,

if 1/4 < d < 1/2

(4.10)

(4.11)

ka∗(n) − a(n)k2 ≤ kˇa(n) − a(n)k2 + ka∗(n) − ˇa(n)k2,
kˇa(n) − a(n)k2 ≤ kΩ−1
ka∗(n) − ˇa(n)k2 ≤ k ˜Ω−1

n (ˇγn − γn)k2 + k(Ω−1
n (Kn)(˜γn − ˇγ n)k2 + k( ˜Ω−1

n (Kn) − Ω−1

n )ˇγnk2,
n (Kn) − Ω−1

n (Kn))ˇγnk2. (4.12)

Moreover,

kΩ−1

n (ˇγn − γ n)k2 ≤ kT′

nD−1

n k2kTn(ˇγ n − γn)k2 ≤ CkTn(ˇγ n − γn)k2

(4.13)

and

Ω−1
n − Ω−1

n (Kn) = (Tn − Tn(Kn))′
n(Kn)(D−1
n(Kn)D−1

+ T′

+ T′

D−1

n ((Tn − Tn(Kn)) + Tn(Kn))

n (Kn))((Tn − Tn(Kn)) + Tn(Kn))(4.14)

n − D−1
n (Kn)(Tn − Tn(Kn)).

By (1.5), (1.6), (2.1), (2.2), it follows that kTn(ˇγ n−γn)k2 = O(K −1/2+d
O(1), and k(Tn − Tn(Kn))ˇγ nk2 = O(K −1/2+d

), kTn(Kn)ˇγnk2 =
). These bounds, together with (4.11),

n

n

(4.13) and (4.14), yield

kˇa(n) − a(n)k2 = O(K −1/2+d

n

+ (K −d

n log n)1/2).

(4.15)

22

C.-K. Ing, H.-T. Chiou and M. Guo

By the ﬁrst moment bound theorem of Findley and Wei [7], Cauchy–Schwarz inequality,
Proposition 4.1, (4.7), Lemma 4.2(ii), (4.9) and (4.12), it can be shown that

ka∗(n) − ˇa(n)k2 =( Op((n−1+2dK 1+θ

Op((n−1+2dK 1+θ

)1/2),
n + n−1K 2+θ
n + n−3+6dK 3+2d

n

n

)1/2),

if 0 < d ≤ 1/4,
if 1/4 < d < 1/2,

(4.16)

for any θ > 1/q. Now, the desired conclusion follows from (4.10), (4.15) and (4.16). (cid:3)

n,Kn

Remark 4.2. When u1, . . . , un are observable, Wu and Pourahmadi [19] constructed an
estimate, ˘Ω−1
˘γn, of a(n), where ˘Ωn,Kn = (ˆγi−j 1|i−j|≤Kn )1≤i,j≤n and ˘γn = (˘γ1, . . . , ˘γn)′
j=1 |γj| < ∞, they obtained a convergence rate of
j=Kn |γk|;
see Corollary 2 of Wu and Pourahmadi [19]. However, their proof, relying heavily on

with ˘γi = ˆγi1{i≤Kn}. By assuming P∞
the proposed estimate in terms of Kn, the moment restriction on wt, and P∞
P∞
j=1 |γj| < ∞, is no longer applicable here.

4.2. The rate of convergence of the FGLSE

In this section, we assume that Xn is nonsingular, and hence β is uniquely deﬁned. We
estimate β using the FGLSE,

ˆβFGLS = (X′

n

˜Ω−1

n (Kn)Xn)−1

X′
n

˜Ω−1

n (Kn)yn.

The main objective of this section is to investigate the convergence rate of ˆβFGLS. To
simplify the exposition, we shall focus on polynomial regression models and impose the
following conditions on ai:

aj ∼ C0j−1−d

and

∞

Xj=0

ajeijλ = 0

if and only if λ = 0,

(4.17)

where a0 = −1 and C0 6= 0. As mentioned in Section 2, (4.17) is fulﬁlled by the FARIMA
model deﬁned in (1.7). When Kn diverges to inﬁnity at a suitable rate, we derive the rate
of convergence of ˆβFGLS in the next corollary. It is important to be aware that our proof
is not a direct application of Theorem 4.1. Instead, it relies on a very careful analysis of
the joint eﬀects between the Cholesky factors and the regressors.

Corollary 4.2. Consider the regression model (4.1) with xti = ti−1 for i = 1, . . . , p. As-
sume the same assumptions as in Theorem 4.1 with (1.5) replaced by (4.17). Suppose
that (3.5) holds and n−1+2dK 1+2d
(i) kLn(βFGLS − β)k2 = Op(1),
(ii) kLn(ˆβFGLS − β)k2 = Op(1),

+ n−1K 2+2d

= o(1). Then

n

n

n (Kn) replaced

where Ln = n−d diag(n1/2, n3/2, . . . , np−1/2) and βFGLS is ˆβFGLS with ˜Ω−1
by Ω−1

n (Kn).

Estimation of inverse autocovariance matrices

23

Proof. We only prove Corollary 4.2 for p = 2. The proof for p 6= 2 is analogous. We begin
by showing (i). Let ˜Ln = K −d
n diag(n1/2, n3/2). Then straightforward calculations yield

kLn(βFGLS − β)k2
nk ˜Ln(X′

≤ n−dK d

nΩ−1

n (Kn)Xn)−1 ˜Lnk2k ˜L−1

n X′

nΩ−1

n (Kn)unk2.

Moreover, by (4.17),

(4.18)

(4.19)

n−dK d

nk˜L−1

n X′

k ˜Ln(X′

nΩ−1

nΩ−1

n (Kn)unk2 = Op(1),
n (Kn)Xn)−1 ˜Lnk2 ≤ n−1
Xt=0

κn

λmin(A⌊κn⌋+t + An−t)!−1

= O(1),(4.20)

where λmin(A) denotes the minimum eigenvalue of matrix A, 0 < κ < 1 and At = η′
−1/2
with ηt denoting the tth row of n1/2D
n
yields (i). To show (ii), note ﬁrst that

tηt,
n . Combining (4.18)–(4.20)

(Kn)Tn(Kn)Xn ˜L−1

kLn(ˆβFGLS − β)k2 ≤ kLn(βFGLS − β)k2 + kLn(ˆβFGLS − βFGLS)k2,

kLn(ˆβFGLS − βFGLS)k2 ≤ k(D1)k2 + k(D2)k2,

(4.21)

(4.22)

where (D1) = Ln((X′
n
Ln(X′
n

˜Ω−1
n (Kn)Xn)−1−(X′
n( ˜Ω−1
n (Kn) − Ω−1
n (Kn)Xn)−1X′

˜Ω−1

nΩ−1

n (Kn)Xn)−1)X′

nΩ−1

n (Kn)un, and (D2) =

n (Kn))un. In addition,

k(D5)k2 ≤ (k(D5)k2 + k(D3)k2)k(D4)k2k(D3)k2
nΩ−1
˜Ω−1

n (Kn))Xn ˜L−1
where (D3) = ˜Ln(X′
n ,
n ))˜Ln. By (4.17) and some alge-
and (D5) = ˜Ln((X′
n
braic manipulations, one obtains k(D3)k2 = O(1) and k(D4)k2 = op(1). Thus, by (4.23),
k(D5)k2 = op(1). The bounds for k(D3)k2 and k(D5)k2, together with (4.17) and (4.19),
imply

n )˜Ln, (D4) = ˜L−1
n ) − (X′

n X′
n (Kn)X−1

n (Kn) − Ω−1

n (Kn)X−1

n (Kn)X−1

n( ˜Ω−1

nΩ−1

(4.23)

k(D1)k2 ≤ n−dK d
k(D2)k2 ≤ n−dK d
= op(1).

n X′

nk(D5)k2k˜L−1
n(k(D5)k2 + k(D3)k2)k ˜L−1

nΩ−1n(Kn)unk2 = op(1),

n X′

n( ˜Ω−1

n (Kn) − Ω−1

n (Kn))unk2

(4.24)

(4.25)

Now, the desired conclusion (ii) follows from (4.24), (4.25), (4.21) and (4.22) and (i). (cid:3)

Remark 4.3. Under assumptions similar to those of Corollary 4.2, Theorems 2.2 and
2.3 of Yajima [21] show that the best linear unbiased estimate (BLUE) ˆβBLUE =
(X′
nyn, of β have the same
rate of convergence, and this rate is, in turn, the same as that of ˆβFGLS.

n yn, and the LSE, ˆβLS = (X′

n Xn)−1X′

nXn)−1X′

nΩ−1

nΩ−1

24

C.-K. Ing, H.-T. Chiou and M. Guo

We close this section with a subtle example showing that the convergence rate of ˆβFGLS
is faster than that of ˆβLS, but slower than that of ˆβBLUE. Consider model (4.1), with
p = 1, xt1 = 1 + cos(θt), and θ 6= 0. Assume the same assumptions as in Corollary 4.2.
Then, by an argument similar to that used in the proof of Corollary 4.2, it can be shown
that the rate of convergence of ˆβFGLS is n−1/2+dK −d
n . On the other hand, Theorems 2.1
and 2.2 and Example 2.1(ii) of Yajima [22] yield that the convergence rates of ˆβBLUE
and ˆβLS are n−1/2 and n−1/2+d, respectively. This example gives a warning that the
convergence rate of ˆβBLUE is not necessarily maintained by its feasible counterpart, even
if the consistency of ˜Ω−1

n (Kn) holds true.

5. Simulation study

In Section 5.1, we introduce a data-driven method for choosing the banding parameter
Kn. With this Kn, we demonstrate the ﬁnite sample performance of the inverse auto-
covariance estimator proposed in Section 3 under FARIMA(p, d, q) processes, and that
proposed in Section 4 under polynomial regression models with I(d) errors. The details
are given in Sections 5.2 and 5.3, respectively.

5.1. Selection of Kn

Our approach for choosing Kn is based on the idea of subsampling and risk-minimization
(SAR) introduced by Bickel and Levina [3] and Wu and Pourahmadi [19]. We ﬁrst split
the time series data {ui}n
j=(v−1)b+1 of equal
length b, where b is a prescribed integer and v = 1, 2, . . . ,⌊n/b⌋ with ⌊a⌋ denoting the
largest integer ≤ a. Let 1 ≤ L < H < b be another prescribed integers. For a given banding
parameter L ≤ k < H, let ˆΩ−1
H,k,v represent our inverse autocovariance matrix estimator
of ΩH

i=1 into ⌊n/b⌋ nonoverlapping subseries {uj}vb

j=(v−1)b+1. Deﬁne the average risk

−1 based on the vth subseries {uj}vb

ˆR(O)(k) =

1

⌊n/b⌋

⌊n/b⌋

Xv=1

k ˆΩ−1

H,k,v − Ω−1

H k2.

Our goal is to ﬁnd a banding parameter such that ˆR(O)(k) is minimized. However, since
Ω−1
H,n, the H-dimensional inverse sample autocovariance matrix,
as its surrogate, and replace ˆR(O)(k) by

H is unknown, we use ˆΓ−1

ˆR(k) =

1

⌊n/b⌋

⌊n/b⌋

Xv=1

k ˆΩ−1

H,k,v − ˆΓ−1

H,nk2,

noting that when H ≪ n, ˆΓ−1
H . Now the banding param-
eter Kn is chosen to minimize ˆR(k) over the interval [L, H). In our simulation study, b is

H,n is a consistent estimator of Ω−1

Estimation of inverse autocovariance matrices

25

Table 1. The values of ˆl2(d) under DGPs 1–4

n \ d

0.01

0.1

0.25

0.4

0.49

0.01

0.1

0.25

0.4

0.49

250
500
1000
2000
4000

250
500
1000
2000
4000

0.501
0.389
0.276
0.217
0.173

0.767
0.642
0.512
0.435
0.373

DGP 1
0.603
0.455
0.335
0.274
0.216

DGP 3
0.775
0.652
0.579
0.495
0.430

0.699
0.527
0.396
0.334
0.257

0.642
0.514
0.420
0.358
0.299

0.546
0.443
0.366
0.344
0.344

1.007
0.952
0.953
0.928
0.931

0.758
0.595
0.444
0.367
0.298

0.660
0.529
0.443
0.376
0.320

0.936
0.759
0.537
0.441
0.345

1.141
0.923
0.724
0.625
0.550

DGP 2
1.250
0.981
0.734
0.597
0.481

DGP 4
1.129
0.942
0.839
0.714
0.614

1.040
0.837
0.595
0.498
0.389

1.495
1.373
1.366
1.339
1.337

1.512
1.192
0.867
0.732
0.573

0.836
0.725
0.604
0.518
0.434

1.676
1.309
0.977
0.814
0.647

0.839
0.688
0.594
0.497
0.416

set to ⌊n/5⌋. In addition, inspired by Theorem 3.1, we choose L = ⌊log n⌋ and H = ⌈n0.4⌉,
where ⌈a⌉ denotes the smallest integer ≥ a. The banding parameter for the detrended
time series is also chosen in the same manner.

5.2. Finite sample performance of ˆΩ−1

n (Kn)

n (Kn), with Kn determined by the SAR

We explore the ﬁnite sample performance of ˆΩ−1
method, under the following four data generating processes (DGPs):
DGP 1: (1 − B)dut = wt;
DGP 2: (1 − 0.7B)(1 − B)dut = wt;
DGP 4: (1 + 0.4B)(1 − B)dut = (1 − 0.3B)wt,
DGP 3: (1 − B)dut = (1 − 0.4B)wt;
where the wt’s are i.i.d. N (0, 1) innovations. To improve the speed and accuracy, we
adopt the method of Wu, Michailidis and Zhang [17] to generate the long memory data
n (Kn) is evaluated by ˆl2(d), the average value of
{u1, . . . , un}. The performance of ˆΩ−1
k ˆΩ−1
n k2 over 1000 replications, with n = 250, 500, 1000, 2000, 4000. The results
are summarized in Table 1. Note ﬁrst that for each combination of d and DGP, ˆl2(d)
shows an obvious downward trend as n increases. Moreover, when n = 4000, all ˆl2(d) are
less than 0.65 except for d = 0.1 and DGP = DGP 3 or DGP 4. In the latter two cases,
ˆl2(d), lying between 0.93 and 1.34, are still reasonably small. These ﬁndings suggest that
ˆΩ−1

n (Kn) is a reliable estimate of Ω−1
On the other hand, the decreasing rate of ˆl2(d) apparently changes over d and DGP. To
provide a better understanding of this phenomenon, we ﬁrst consider the fastest possible
convergence rate that can be derived from Theorem 3.1:

n , particularly when n is large enough.

n (Kn)− Ω−1

n (K ∗

k ˆΩ−1

n) − Ω−1

n k2

26

C.-K. Ing, H.-T. Chiou and M. Guo

Table 2. The values of ˆl2(d)/ OP(d) under the DGPs 1–4

n \ d

0.01

0.1

0.25

0.4

0.49

0.01

0.1

0.25

0.4

0.49

250
500
1000
2000
4000

250
500
1000
2000
4000

1.849
1.278
0.817
0.585
0.428

2.829
2.108
1.514
1.172
0.922

DGP 1
3.414
2.399
1.661
1.291
0.973

DGP 3
4.388
3.434
2.873
2.333
1.939

3.782
2.629
1.842
1.460
1.062

3.476
2.565
1.951
1.564
1.237

2.349
1.728
1.309
1.138
1.062

4.333
3.710
3.405
3.072
2.875

3.704
2.640
1.805
1.380
1.045

3.226
2.345
1.802
1.415
1.122

3.453
2.493
1.589
1.188
0.854

4.207
3.032
2.143
1.685
1.361

DGP 2
7.081
5.172
3.644
2.814
2.171

DGP 4
6.392
4.965
4.162
3.366
2.772

4.476
3.264
2.127
1.647
1.200

6.433
5.352
4.880
4.431
4.130

8.187
5.950
4.030
3.198
2.370

4.528
3.618
2.807
2.263
1.795

=( Op(n−d/(4+2d+2θ)(log n)(4+d+2θ)/(4+2d+2θ)),

Op(n−d(1−2d)/(2+2d+2θ)(log n)(2+d+2θ)/(2+2d+2θ)),

if 0 < d ≤ ˜d,
if ˜d < d < 1/2,

where ˜d = {(3 + 2θ)/2 + θ2/4}1/2 − (1 + θ/2), and

8.187
5.804
3.976
3.064
2.272

4.096
3.052
2.418
1.873
1.461

(5.1)

K ∗

n =(cid:26) (n log n)1/(2+d+θ),

(log n)1/(1+d+θ)n(1−2d)/(1+θ+d),

if 0 < d ≤ ˜d,
if ˜d < d < 1/2.

Because wt’s are normally distributed, in view of Theorem 3.1, θ can be any positive

number, and hence ˜d is arbitrarily close to √1.5 − 1 (which, rounded to the nearest
thousandth, is 0.225). We then measure the relative performance of k ˆΩ−1
n (Kn) − Ω−1
n k2
and k ˆΩ−1

n k2 using the ratio ˆl2(d)/ OP(d), where

n (K ∗

n) − Ω−1

OP(d) =(cid:26) 0.05n−d/(4+2d)(log n)(4+d)/(4+2d),

0.05n−d(1−2d)/(2+2d)(log n)(2+d)/(2+2d),

if 0 < d ≤ 0.225,
if 0.225 < d < 1/2,

(5.2)

which is obtained from the bound in (5.1) with θ set to 0 and constants set to 0.05. The
values of ˆl2(d)/ OP(d) under DGPs 1–4 are summarized in Table 2. Note that while the
exact constants are not reported in (5.1), setting them to 0.05 helps us to better interpret
some numerical results in Table 1 through Table 2.

For n ≥ 1000, all values of ˆl2(d)/ OP(d) fall in a reasonable range of (0.4, 5.0), sug-
gesting that the rate of convergence of k ˆΩ−1
n k2 is comparable to the op-
timal rate obtained from Theorem 3.1. Moreover, the asymptotic behaviors of ˆl2(d)
can be well explained by OP(d) when DGP = DGP 1 and d ≥ 0.1. In particular, when
n = 4000, the rankings of {ˆl2(0.1), ˆl2(0.25), ˆl2(0.4), ˆl2(0.49)} coincide exactly with those
of {OP(0.1), OP(0.25), OP(0.4), OP(0.49)}, and OP(0.25) = mind∈{0.1,0.25,0.4,0.49} OP(d).

n (Kn) − Ω−1

Estimation of inverse autocovariance matrices

27

Table 3. 5-number summaries of SF(c)

n

250
500
1000
2000
4000

Minimum

1st quartile

Median

3rd quartile

Maximum

−0.173
−0.229
−0.250
−0.210
−0.194

−0.109
−0.074
−0.022
−0.025
−0.029

0.050
0.044
0.029
−0.005
−0.014

0.087
0.150
0.160
0.134
0.112

0.289
0.228
0.210
0.156
0.152

This gives reasons for explaining why d = 0.25 often provides better results than
d = 0.1, 0.4 or 0.49. The behavior of ˆl2(0.01), however, is apparently inconsistent with
that of OP(0.01). Speciﬁcally, for n ≥ 250, ˆl2(0.01) < mind∈{0.1,0.25,0.4,0.49}
ˆl2(d), whereas
OP(0.01) > maxd∈{0.1,0.25,0.4,0.49} OP(d). One possible explanation of this discrepancy is
that when d is extremely small, the constant associated with the convergence rate of
k ˆΩ−1
n (Kn) − Ω−1
n k2 can also be very small, and the constant, 0.05, assigned to OP(d)
fails to do a good job in this extremal case.
It is relatively diﬃcult to understand the behaviors of ˆl2(d) through OP(d) when short-
memory AR or MA components are added into the I(d) model. However, using the ˆl2(d)
in DGP 1 as the basis for comparison, it seems fair to comment that the AR component
tends to increase ˆl2(d) with d ≥ 0.25 and d = 0.01, whereas the MA component tends
to increase ˆl2(d) with d ≤ 0.25. When both components are included, the values of ˆl2(d)
are uniformly larger than those in the I(d) case. We leave a further investigation of the
impact of the AR and MA components on the ﬁnite sample performance of ˆΩ−1
n (Kn) as
a future work.

In the following, we shall perform a sensitivity analysis of the SAR method by per-

turbing the parameter c in cKn. We deﬁne the sensitivity function

SF(c) = k ˆΩ−1

n (cKn) − Ω−1
k ˆΩ−1

n k2 − k ˆΩ−1
n (Kn) − Ω−1
n (Kn) − Ω−1
n k2

n k2

.

For each c = 0.8, 1.2, d = 0.1, 0.25, 0.45, DGP = DGP 1–4, and n = 250, 500, 1000, 2000,
4000, we compute the average of SF(c), denoted by SF(c), based on 1000 replications,
and the ﬁve-number summaries of SF(c) for each n are presented in Table 3. Table 3
shows that the maximum values of SF(c) are all positive and decrease as n increases.
In contrast, the minimum values of SF(c) are all negative and start to increase when
n ≥ 1000. When n = 4000, the maximum SF(c) and minimum SF(c) are 0.152 and −0.194,
respectively, yielding that the average of k ˆΩ−1
n k2 falls between 0.806–1.152
times the average of k ˆΩ−1
n k2, for all c’s, d’s and DGPs under consideration.
Our analysis reveals that a small perturbation of Kn will not lead to a drastic change on
estimation errors.

n (cKn) − Ω−1

n (Kn) − Ω−1

28

C.-K. Ing, H.-T. Chiou and M. Guo

5.3. Finite sample performance of ˜Ω−1

n (Kn)

We consider three polynomial regression models:

Model 1: yt = 1 + ut, t = 1, 2, . . . , n,
Model 2: yt = 1 + 2t + ut, t = 1, 2, . . . , n,
Model 3: yt = 5 + t + 2t4 + ut, t = 1, 2, . . . , n,

where ut’s are generated by DGP 1. The performance of ˜Ω−1
n (Kn) (with Kn determined
by the SAR method) is investigated with polynomial degree known or unknown. In the
latter situation, we perform best subset selection in the following ﬁfth-order model,

yt = β0 + β1t + β2t2 + β3t3 + β4t4 + β5t5 + ut,

t = 1, 2, . . . , n,

using the selection criterion,

Ln(M ) = log ˆσ2

n(M ) + #M/ log(n),

(5.3)

suggested by Ing and Wei [10], where M = {M : M ⊆ {1, t, t2, t3, t4, t5}} and ˆσ2
n(M )
is the residual mean square error of model M . Note that according to Theorem 4.1
of Ing and Wei [10], Ln(M ) is a consistent criterion in regression models with long-
n (Kn) is evaluated by ˜l2(d), which is ˆl2(d) with
memory errors. The performance of ˜Ω−1
ut’s replaced by the corresponding detrended series. The values of ˜l2(d) are documented
in Table 4, in which d ∈ {0.1, 0.25, 0.4}, n ∈ {250, 500, 1000, 2000, 4000} and models are
known or selected by Ln(M ). Table 4 also reports the correct selection frequencies (in
1000 simulations), which is denoted by ˆqi(d) for model i and long-memory parameter d.
All ˆq3(d)’s are larger than 0.9. However, ˆq1(0.45) and ˆq2(0.45) only fall in the interval
(0.44, 0.63) and the intercept (constant time trend) is often excluded by Ln(M ) in these
cases. In fact, identifying the intercept is a notoriously challenging problem when d is
large and the intercept parameter is not far enough away from 0. Fortunately, Table 4
shows that the ˜l2(d) values obtained with or without model selection procedure are
similar, even when ˆqi(d) is much smaller than 1. This result may be due to the fact that
under models 1 and 2, the performance of ˜Ω−1
n (Kn) is insensitive to misspeciﬁcation of
the intercept, provided d is large enough. Another interesting ﬁnding is that for each
regression model considered in this section and each (n, d) combination, the behavior
of ˜l2(d) coincides with that of ˆl2(d) with DGP = DGP 1. Putting these characteristics
together suggests that ˜Ω−1
n (Kn). This conclusion is
particularly relevant in situations where the latter matrix becomes infeasible.

n (Kn) is a reliable surrogate for ˆΩ−1

Acknowledgements

The work of Ching-Kang Ing was supported in part by the National Science Council
of Taiwan under Grant NSC 97-2628-M-001-022-MY2 and Academia Sinica Investigator
Award. The research of Hai-Tang Chiou was supported by Academia Sinica Investigator

Estimation of inverse autocovariance matrices

29

Table 4. ˜l2(d) with (in parentheses) or without model selection and ˆqi(d)

Model 1

˜l2(d)

n \ d

0.1

0.25

0.45

250
500
1000
2000
4000

0.566 (0.566)
0.461 (0.461)
0.385 (0.385)
0.358 (0.358)
0.353 (0.353)

0.592 (0.593)
0.456 (0.454)
0.333 (0.332)
0.271 (0.271)
0.216 (0.216)

0.702 (0.693)
0.535 (0.533)
0.404 (0.400)
0.334 (0.331)
0.267 (0.264)

Model 2

˜l2(d)

n \ d

0.1

0.25

0.45

250
500
1000
2000
4000

0.585 (0.574)
0.477 (0.470)
0.399 (0.397)
0.368 (0.368)
0.359 (0.359)

0.602 (0.604)
0.457 (0.455)
0.336 (0.336)
0.273 (0.275)
0.218 (0.218)

0.690 (0.691)
0.533 (0.532)
0.400 (0.398)
0.331 (0.331)
0.263 (0.264)

Model 3

˜l2(d)

n \ d

0.1

0.25

0.45

250
500
1000
2000
4000

0.611 (0.611)
0.493 (0.493)
0.411 (0.411)
0.376 (0.376)
0.318 (0.318)

0.617 (0.618)
0.463 (0.463)
0.339 (0.339)
0.275 (0.275)
0.218 (0.218)

0.687 (0.686)
0.532 (0.530)
0.396 (0.395)
0.329 (0.328)
0.262 (0.261)

ˆq1(d)

0.1

0.992
0.999
1.000
1.000
1.000

ˆq2(d)

0.1

0.688
0.839
0.947
0.995
1.000

ˆq3(d)

0.1

1.000
1.000
1.000
1.000
1.000

0.25

0.45

0.867
0.918
0.962
0.984
0.996

0.558
0.593
0.622
0.620
0.609

0.25

0.45

0.487
0.552
0.657
0.735
0.809

0.455
0.454
0.475
0.444
0.458

0.25

0.45

0.995
0.999
1.000
1.000
1.000

0.904
0.912
0.916
0.942
0.967

Award and NSC 102-2118-M-110-002-MY2 from Taiwan’s National Science Council. The
research of Meihui Guo was partly supported by Grant number NSC 102-2118-M-110-
002-MY2, from Taiwan’s National Science Council.

References

[1] Berk, K.N. (1974). Consistent autoregressive spectral estimates. Ann. Statist. 2 489–502.

MR0421010

[2] Bickel, P.J. and Gel, Y.R. (2011). Banded regularization of autocovariance matrices in
application to parameter estimation and forecasting of time series. J. R. Stat. Soc. Ser.
B. Stat. Methodol. 73 711–728. MR2867455

30

C.-K. Ing, H.-T. Chiou and M. Guo

[3] Bickel, P.J. and Levina, E. (2008). Regularized estimation of large covariance matrices.

Ann. Statist. 36 199–227. MR2387969

[4] Brockwell, P.J. and Davis, R.A. (1991). Time Series: Theory and Methods, 2nd ed.

Springer Series in Statistics. New York: Springer. MR1093459

[5] Cai, T.T., Ren, Z. and Zhou, H.H. (2013). Optimal rates of convergence for estimating
Toeplitz covariance matrices. Probab. Theory Related Fields 156 101–143. MR3055254
[6] Cheng, T.-C.F., Ing, C.-K. and Yu, S.-H. (2015). Toward optimal model averaging in

regression models with time series errors. J. Econometrics. To appear.

[7] Findley, D.F. and Wei, C.Z. (1993). Moment bounds for deriving time series CLTs and

model selection procedures. Statist. Sinica 3 453–480. MR1243396

[8] Godet, F. (2010). Prediction of long memory processes on same-realisation. J. Statist.

Plann. Inference 140 907–926. MR2574654

[9] Ing, C.-K. and Wei, C.-Z. (2003). On same-realization prediction in an inﬁnite-order

autoregressive process. J. Multivariate Anal. 85 130–155. MR1978181

[10] Ing, C.-K. and Wei, C.-Z. (2006). A maximal moment inequality for long range dependent
time series with applications to estimation and model selection. Statist. Sinica 16 721–
740. MR2281299

[11] Inoue, A. and Kasahara, Y. (2006). Explicit representation of ﬁnite predictor coeﬃcients

and its applications. Ann. Statist. 34 973–993. MR2283400

[12] Kokoszka, P.S. and Taqqu, M.S. (1995). Fractional ARIMA with stable innovations.

Stochastic Process. Appl. 60 19–47. MR1362317

[13] McMurry, T.L. and Politis, D.N. (2010). Banded and tapered estimates for autoco-
variance matrices and the linear process bootstrap. J. Time Series Anal. 31 471–482.
MR2732601

[14] Palma, W. and Pourahmadi, M. (2012). Banded regularization and prediction of long-

memory time series. Working paper.

[15] Shibata, R. (1980). Asymptotically eﬃcient selection of the order of the model for esti-

mating parameters of a linear process. Ann. Statist. 8 147–164. MR0557560

[16] Wei, C.Z. (1987). Adaptive prediction by least squares predictors in stochastic regression

models with applications to time series. Ann. Statist. 15 1667–1682. MR0913581

[17] Wu, W.B., Michailidis, G. and Zhang, D. (2004). Simulating sample paths of linear

fractional stable motion. IEEE Trans. Inform. Theory 50 1086–1096. MR2094869

[18] Wu, W.B. and Pourahmadi, M. (2003). Nonparametric estimation of large covariance

matrices of longitudinal data. Biometrika 90 831–844. MR2024760

[19] Wu, W.B. and Pourahmadi, M. (2009). Banding sample autocovariance matrices of sta-

tionary processes. Statist. Sinica 19 1755–1768. MR2589209

[20] Xiao, H. and Wu, W.B. (2012). Covariance matrix estimation for stationary time series.

Ann. Statist. 40 466–493. MR3014314

[21] Yajima, Y. (1988). On estimation of a regression model with long-memory stationary

errors. Ann. Statist. 16 791–807. MR0947579

[22] Yajima, Y. (1991). Asymptotic properties of the LSE in a regression model with long-

memory stationary errors. Ann. Statist. 19 158–177. MR1091844

Received December 2013 and revised November 2014

