6
1
0
2

 
r
a

M
7

 

 
 
]

.

A
R
h
t
a
m

[
 
 

1
v
4
0
2
2
0

.

3
0
6
1
:
v
i
X
r
a

ELT Linear Algebra

Guy Blachar and Erez Sheiner

Abstract

Exploded layered tropical (ELT) algebra is an extension of tropical algebra with a structure
of layers. These layers allow us to use classical algebraic results in order to easily prove ana-
logous tropical results. Speciﬁcally we study the connection between the ELT determinant and
linear dependency, and use a generalized version of Kapranov Theorem proved in [5] (called the
Fundamental Theorem).
In this paper we prove that an ELT matrix is singular if and only if its rows are linearly de-
pendent and that the row rank and submatrix rank of an ELT matrix are equal. We also deﬁne
an ELT rank for a tropical matrix, and prove that it is equal to its Kapranov rank.

Contents

0 Introduction

0.1 ELT Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
0.2 The Element −∞ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
0.3 ELT Algebras and Puiseux Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
0.4 Symmetrized Semirings and ELT Rings
. . . . . . . . . . . . . . . . . . . . . . . . .
0.5 Modules over ELT Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1 ELT Determinant and Rank

1.1 Critical Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 The Exploded-Layered Tropical Determinant
. . . . . . . . . . . . . . . . . . . . . .
1.3 Matrix singularity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4
Invertible Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.5 Rank of a matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.5.1 Proof of the rank theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.5.2 Kapranov and Barvinok rank . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Applications of ELT Linear Algebra

2.1

Inner Products and Orthogonality . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.1
Inner product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.2 Orthogonality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Page

1
2
3
4
5
6

7
7
9
10
15
15
16
18

21
21
21
23

0

Introduction

Tropical linear algebra, also known as Max-Plus linear algebra, has been studied for more than
50 years (ref. [4]). While tropical geometry mainly deals with geometric combinatorial problems,
tropical linear algebra deals with algebraic non-linear combinatorial problems (for instance, the as-
signment problem [12]). Tropical linear algebra may also be used as a mean to study the tropical
algebraic geometry (for instance, the tropical resultant). Notable work in this ﬁeld can be found at
[4],[5],[9],[11], and [17].

1

In tropical linear algebra, there are many deﬁnitions for linear dependency of vectors (as in [2]).
The classical deﬁnition is the following: a set of vectors is linearly dependent if for some non-trivial
linear combination the maximal entry at each column is obtained at least twice. For instance, the
vectors

w1 = (1, 2, 0), w2 = (0, 3, 2), w3 = (0, 0, 0)

are linearly dependent. Indeed,

(1 ⊙ w1) ⊕ (w2) ⊕ (2 ⊙ w3) = (2, 3, 1) ⊕ (0, 3, 2) ⊕ (2, 2, 2) = (2, 3, 2).

We introduce an extension of the max-plus algebra with layers, called exploded layered tropical
algebra (or ELT algebra for short). This structure is a generalization of the work of Izhakian and
Rowen ([10]), and is similar to Parker’s exploded structure ([14]). In ELT linear algebra, a set of
vectors is linearly dependent if for some non-trivial linear combination, all of the layers equal zero.
For instance, the vectors

v1 = ([1]1, [1]2, [1]0), v2 = ([1]0, [1]3, [1]2), v1 = ([−1]0, [1]0, [1]0)

are linearly dependent. Indeed,

[1]1v1 + [−1]0v2 + [1]2v3 = ([0]2, [0]3, [0]2).

We notice that while u1 = (1, 1), u2 = (1, 1) are clearly linearly dependent in tropical algebra,

the two vectors

u′
1 = ([1]1, [−1]1), u′

2 = ([−1]1, [1]1)

are independent. Geometrically the span of these two vectors is equal to a span of one vector.
However, the layers of these two spans diﬀers. Naturally we would like to know what is the maximal
size of an independent set.

One of the challenging results presented in this paper is that the size of a maximal independent
set is exactly n. Furthermore, the maximal number of linearly independent rows of a matrix (called
row rank) is always equal to the maximal number of independent columns.
We further study ELT linear algebra, and present the following main results in this paper:

1. Formulation of the natural properties of linear dependence and determinant: A matrix is

singular if and only if its rows are linearly dependent (Theorem 1.7).

2. The row rank and the column rank of a matrix are equal (Theorem 1.19).

3. The ELT rank and Kapranov rank of a tropical matrix are equal (Theorem 1.25, Lemma 1.29).

Acknowledgements: This article is taken from Guy Blachar’s M.Sc. Thesis and from Erez
Sheiner’s Ph.D. Thesis. Both works were carried under the supervision of Prof. Louis Rowen from
Bar-Ilan University, to whom we thank deeply for his help and guidance.

0.1 ELT Algebras

Deﬁnition 0.1. Let L be a semiring, and F a totally ordered semigroup. An ELT algebra is
the set R = L × F , where each element (ℓ, a) is denoted [ℓ]a, together with the semiring (without
zero) structure:

1. [ℓ1]a1 + [ℓ2]a2 :=


[ℓ1]a1
a1 > a2
[ℓ2]a2
a1 < a2
[ℓ1+L ℓ2]a1 a1 = a2

.

2. [ℓ1]a1 · [ℓ2]a2 := [ℓ1·L ℓ2] (a1 +F a2).

2

We write R = R (L , F ). For [ℓ]a, ℓ is called the layer, whereas a is called the tangible value.

ELT algebras originate from [14], and are also discussed in [18].

Let R be an ELT algebra. We write s : R → L for the projection on the ﬁrst component (the

sorting map):

We also write τ : R → F for the projection on the second component:

s(cid:0)[ℓ]a(cid:1) = ℓ
τ(cid:0)[ℓ]a(cid:1) = a

We denote the zero-layer subset

and

[0]R = {α ∈ R|s (α) = 0}

R× = {α ∈ R|s (α) 6= 0} = R \ [0]R

We note some special cases of ELT algebras:

1. If F is a totally ordered group and L is a ring (with unity), then R (L , F ) is called an ELT

ring.

2. An ELT ring R (L , F ) in which both F and L are commutative is called a commutative

ELT ring.

3. If F is a commutative totally ordered group and L is a ﬁeld, then R (L , F ) is called an ELT

ﬁeld.

Deﬁnition 0.2. Let R be an ELT algebra, and take x, y ∈ R. We deﬁne a partial order relation (cid:15)
on R in the following way:

x (cid:15) y ⇐⇒ ∃z ∈ [0]R : x = y + z

We point out some important elements in any ELT ring R:

1. [1]0, which is the multiplicative identity of R.

2. [0]0, which is idempotent for both operations of R.

3. [−1]0, which has the role of “−1” in our theory.

Remark 0.3. Note that [0]0[ℓ]a = [0]a. Therefore, [0]R = [0]0R. In addition, [0]R is an ideal of R.

0.2 The Element −∞

As in the tropical algebra, ELT algebras lack an additive identity. Therefore, we adjoin a formal
element to the ELT algebra R, denoted by −∞, which satisﬁes ∀α ∈ R:

−∞ + α = α + −∞ = α
−∞ · α = α · −∞ = −∞

We also deﬁne s (−∞) = 0. We denote R = R ∪ {−∞}.

We note that R is now a semiring, with the following property:

α + β = −∞ =⇒ α = β = −∞

Such a semiring is called an antiring. Antirings are dealt with in [19] and [6].

3

0.3 ELT Algebras and Puiseux Series

There is a strong connection between ELT rings and Puiseux series. Let R = R (L , F ) be an
ELT ring. Deﬁne a function ELTrop : L (t) → R in the following way:
if x ∈ L (t) \ {0} has a
leading monomial ℓta, then

ELTrop (x) = [ℓ] (−a) .

In addition, ELTrop (0) = −∞.

Lemma 0.4. The following properties hold:

1. ∀x, y ∈ L (t) : ELTrop (x) + ELTrop (y) (cid:15) ELTrop (x + y).

2. ∀α ∈ L ∀x ∈ L (t) : ELTrop (αx) = [α]0 ELTrop (x).

3. ∀x, y ∈ L (t) : ELTrop (x) ELTrop (y) (cid:15) ELTrop (xy).

Proof. If x or y are 0 in any part of this proposition, then the assertion is clear. Theorefore, we

assume that x, y 6= 0, and denote x = αi0 ti0 + Xi0<i∈I

αiti and y = βj0 tj0 + Xj0<j∈J

βjtj.

1. If i0 6= j0, without loss of generality i0 < j0. Then

ELTrop (x + y) = ELTrop

αi0 ti0 + βj0 tj0 + Xi0<i∈I

αiti + Xj0<j∈J

βjtj
 =

= [αi0 ] (−i0) = [αi0 ] (−i0) + [βj0] (−j0) = ELTrop (x) + ELTrop (y)

If i0 = j0, but αi0 + βj0 6= 0, then

ELTrop (x + y) = ELTrop

αi0 ti0 + βj0 tj0 + Xi0<i∈I

αiti + Xj0<j∈J

βjtj
 =

= [αi0 +βj0] (−i0) = [αi0 ] (−i0) + [βj0 ] (−j0) = ELTrop (x) + ELTrop (y)

We are left in the case where i0 = j0 and αi0 + βj0 = 0. Thus,

ELTrop (x) + ELTrop (y) = [αi0 ] (−i0) + [βj0 ] (−j0) = [0] (−i0)

Also, in this case, the leading monomials of x and y cancel in x + y. Therefore, the leading
monomial of x + y, γk0 tk0 has degree k0 > i0, and thus

ELTrop (x) + ELTrop (y) = [0] (−i0) = [0] (−i0) + [γk0 ] (−k0) (cid:15) [γk0 ] (−k0) = ELTrop (x + y)

2. This is a special case of 3, where y is taken to be a constant Puiseux series.

3. First, we assume that αi0 βj0 6= 0. In that case, the leading monomial of x + y is αi0 βj0 ti0+j0 ,

and thus

ELTrop (xy) = [αi0 βj0] (−i0 − j0) = [αi0 ] (−i0) [βj0] (−j0) = ELTrop (x) ELTrop (y)

Otherwise, αi0 βj0 = 0. In that case, the leading monomial of xy, γk0 tk0 has degree k0 > i0 +j0,
and thus

ELTrop (x) ELTrop (y) = [αi0 ] (−i0) [βj0 ] (−j0) = [0] (−i0 − j0) =

= [0] (−i0 − j0) + [γk0 ] (−k0) (cid:15) [γk0 ] (−k0) = ELTrop (xy)

4

We remark that in the case in which R is an ELT integral domain, i.e. L is an integral domain,

we have

∀x, y ∈ L (t) : ELTrop (x) ELTrop (y) = ELTrop (xy)

since the second case in the proof, αi0 βj0 = 0, cannot happen.

Due to the above connection between Puiseux series and ELT algebras, some may prefer the

notation [ℓ] ta for [ℓ]a, as in [18].

0.4 Symmetrized Semirings and ELT Rings

Semirings need not to have additive inverses to all of the elements. While some of the theory of
rings can be copied “as-is” to semirings, there are many facts about rings which use the additive
inverses of the elements. The idea of symmetries on semirings is to imitate the additive inverse map.
Semirings with symmetry are discussed in [1], [7], [8], [2], [3], [16].

Deﬁnition 0.5. Let R be a semiring. A map (−) : R → R is a symmetry (or a negation map)
if the following properties hold:

1. ∀a, b ∈ R : (−) (a + b) = (−)a + (−)b.

2. (−)0R = 0R.

3. ∀a, b ∈ R : (−) (a · b) = a · ((−)b) = ((−)a) · b.

4. ∀a ∈ R : (−) ((−)a) = a.

If R has a symmetry (−), we say that (R, (−)) is a symmetrized semiring. If (−) is clear from
the context, we will not mention it.

We give several examples of symmetrized semirings:

• A trivial example of a symmetrized semiring is (−)a = a.

• If R is a ring, it has a symmetry (−)a = −a.

• If R is an ELT algebra, we have a symmetry given by (−)a = [−1]0a.

The last example is the central example for our theory, since it shows that any ELT ring is
equipped with a natural symmetry. Thus, the theory of symmetrized semirings can be used when
dealing with ELT rings.

We now present several notations from the symmetrized world:

• a + (−)a is denoted a◦.

• R◦ = {a◦|a ∈ R}.

• We deﬁne two partial orders on R:

– The relation (cid:23)◦ deﬁned by

a (cid:23)◦ b ⇔ ∃c ∈ R◦ : a = b + c

– The relation ∇ deﬁned by

a∇b ⇔ a + (−)b ∈ R◦

If R is an ELT ring, then some of these notations have already been deﬁned. For example,

a◦ = [0]0a, R◦ = [0]R and the relation (cid:23)◦ is the relation (cid:15).

5

0.5 Modules over ELT Algebras

We now present some basic notions of modules over ELT algebras. These deﬁnitions generalize the
concept of zero-layered elements and surpassing relation from ELT algebras to ELT modules.

Deﬁnition 0.6. Let R be an ELT ring. A (left) R-module is a commutative monoid (M, +, 0M ),
equipped with an operation of “scalar multiplication”, R × M → M , (α, x) 7→ αx, such that the
following properties hold:

1. ∀α ∈ R ∀x, y ∈ M : α (x + y) = αx + αy.

2. ∀α, β ∈ R ∀x ∈ M : (α + β) x = αx + βx.

3. ∀α, β ∈ R ∀x ∈ M : α (βx) = (α · β) x.

4. ∀α ∈ R : α0M = 0M .

5. ∀x ∈ M : [1]0x = x.

A right R-module is deﬁned similarly.
We note that any R-module can be extended to an R-module, by adding the rule

∀x ∈ M : −∞x = 0M

We recall the element [0]0 = [0]0, which is idempotent in R to both addition and multiplication

and that [0]R = [0]0R. Analogously, we deﬁne the zero-layer submodule of M as

Lemma 0.7.

[0]M := [0]0M =(cid:8)x ∈ M(cid:12)(cid:12)x = [0]0x(cid:9)

[0]M = {x ∈ M |x + x = x}

Proof. If x ∈ [0]M , then x = [0]0x, implying

If x + x = x, add [−1]0x to both sides of the equation. Therefore,

x + x = [1]0x + [0]0x =(cid:0)[1]0 + [0]0x(cid:1) = [1]0x = x

[0]0x =(cid:0)[1]0 + [−1]0(cid:1) x = x + [−1]0x = [1]0x + [−1]0x =(cid:0)[1]0 + [−1]0(cid:1) x = [0]0x = x

implying x ∈ [0]M .

Example 0.8. The following are examples of R-modules, where R is an ELT ring:

1. R is an R-module, where the scalar multiplication is the multiplication of R. The submodules
of R are its left ideals, meaning sets I ⊆ R that are closed under addition and satisfy RI ⊆ I.

2. Mm×n(cid:0)R(cid:1) is an R-module with the standard scalar multiplication, (αA)ij = α · (A)ij.
3. Let {Mi}i∈I be R-modules. Then Yi∈I

Mi and Mi∈I

Mi are R-modules with the usual compon-

entwise sum and scalar multiplication.

4. Taking the direct sum of Mi = R for i ∈ I, we obtain the free R-module:

I

R

R

=Mi∈I

Deﬁnition 0.9. Let R be an ELT ring, and let M be an R-module. We deﬁne a relation (cid:15) on M
in the following way:

x (cid:15) y ⇐⇒ ∃z ∈ [0]M : x = y + z

If x (cid:15) y, we say that x surpasses y.

6

Example 0.10. We refer to some of the examples of ELT modules given here, and demonstrate the
meaning of the surpassing relation in these modules.

1. In R, the surpassing relation of the ELT module coincides with the surpassing relation of the

ELT algebra.

I
2. In R

, the surpassing relation means surpassing componentwise.

3. Similarly, also in Mm×n(cid:0)R(cid:1) the surpassing relation is equivalent to surpassing componentwise.

4. In R [λ], the surpassing relation means surpassing of each coeﬃcient of the polynomials.

Lemma 0.11. (cid:15) is a partial order relation on any R-module.

Proof. Let M be an R-module.

1. If x ∈ M , then x = x + [0]0x, and [0]0x ∈ [0]M ; so x (cid:15) x.

2. If x, y ∈ M satisfy x (cid:15) y and y (cid:15) x, write x = y + z1, y = x + z2, where z1, z2 ∈ [0]M . Then

x = y + z1 = y + z1 + z1 = x + z1

y = x + z2 = x + z1 + z2 = x + z2 + z1 = y + z1 = x

So

as required.

3. If x, y, z ∈ M satisfy x (cid:15) y (cid:15) z, then x = y + z1, y = z + z2, where z1, z2 ∈ [0]M . So

and z1 + z2 ∈ [0]M , implying x (cid:15) z.

x = y + z1 = z + z1 + z2

1 ELT Determinant and Rank

The determinant of a matrix is classically the signed sum of products of the form a1σ(1) · · · anσ(n) for
σ ∈ Sym(n), which we call tracks. Since in tropical algebra addition is actually the maximum, the
sign of a permutation and matrix singularity are not naturally deﬁned. We will present a natural
deﬁnition for the ELT determinant and show that it is singular (i.e., of layer zero) if and only if its
rows are ELT linearly dependent.

We are interested in tracks having the property that each of their elements dominates all the
other entries in its columns. We will use combinatorial methods to produce such tracks, and prove
the main theorem. In part, we obtain a version of the well-known Hungarian algorithm ([12]).

1.1 Critical Matrices

Deﬁnition 1.1. An entry aij of a matrix A = (aij ) ∈ Rn×n is called column-critical if it is maximal
within its column, i.e., if ∀k : aij ≥ akj .
A matrix A is called critical if there exists a permutation σ such that a1σ(1), ..., anσ(n) are column-
critical.

7

In this section, we present a self-contained version of the Hungarian algorithm which solves the
assignment problem: having n tasks and n workers and each task pays diﬀerently to the various
workers, how do you assign the tasks to the workers so that all tasks are done with minimal cost?
(ref.[12]). We use classical algebraic operations over the real numbers, and use the result in the
tropical section.

Theorem 1.2. For any matrix A, there exists a matrix

B(a1, ..., an) =


a1
a2
...
an

such that A + B(a1, ..., an) is critical.

1 · · ·




(cid:0)1

a1
a1
a2
a2
...
...
an an

1(cid:1) =


· · ·
· · ·
...
· · ·

a1
a2
...
an




Proof. It is enough to achieve criticality of the main diagonal after a permutation of the rows and
columns.

We use induction on the size of the matrix, considering that the case n = 1 is obvious. Therefore,
we assume that given an (n + 1) × (n + 1) matrix A, for any value of an+1 there exists a matrix
B(a1, ..., an, an+1)
obtained by removing the last row and column, is critical. Moreover, by permuting the rows and
columns and choosing an+1 appropriately, we may assume that c11, ..., cnn are column-critical and
there is some column-critical element in the last row. We assume that the last element on the
diagonal is not critical, for otherwise we are done.

A + B(a1, ..., an, an+1)

the minor

of D

such

that

=

Given a column-critical element in the last row at column i, one can switch the i-th row with the
n + 1 row while keeping a column-critical element at the i-th entry of the diagonal. We will search
for such a permutation of the rows that will place a column-critical element at the last entry of the
diagonal. If such a permutation does not exist, we will expand the number of rows we can use by
increasing the constants of B.

It is fairly obvious that each column has a column-critical entry. Deﬁne

C1 = {i : ci,n+1 is column-critical},

and deﬁne Ck to be the set of numbers i such that there is a column-critical entry cim with m in
Ck−1. We claim that Ck−1 ⊆ Ck. Indeed the cii are column-critical for all i < k. Thus Cr−1 = Cr
for some r. We deﬁne the critical set of D to be CS(D) := Cr ⊆ {1, 2, ..., n + 1}.

If n + 1 ∈ CS(D), then by the construction of the critical set we can permute the rows to obtain
only column-critical entries in the diagonal. Indeed, n + 1 was added due to some column-critical
entry cn+1,m such that m ∈ CS(D). Thus we can switch rows n + 1 and m keeping the m-th entry
of the diagonal column-critical. There is an entry cm,p such that p ∈ CS(D). Since we started
with rows which have a column-critical entry in the (n + 1)-th column, this process will end with all
entries of the diagonal being column-critical.

Assume n + 1 /∈ CS(D) for every possible D. Pick D such that CS(D) has maximal size. We will
reorder the rows and columns of D so that the elements of CS(D) are 1, 2, ..., k. This is permitted
since we permute rows and columns of numbers in CS(D) with previous numbers which are not in
CS(D), updating CS(D) accordingly. Now we have a matrix in which none of the entries in the
left-lower n + 1 − k by k block are column-critical. If they were, they would be contained in CS(D)
since there is a column-critical entry above them in the diagonal.

8

Deﬁne d to be the minimal diﬀerence between any entry of the block and the column-critical

entry in its column;

d = min{aii − ami : m > i, 1 ≤ i ≤ k}.

Adding d to the last n − k constants B(a1, ..., ak, ak+1 + d, ..., an+1 + d) will add a column-critical

entry and enlarge CS(D), contradiction.

Indeed, A + B(a1, ..., ak, ak+1 + d, ..., an+1 + d) still has the ﬁrst n entries of the diagonal column-
critical. Since d is minimal and positive, it will not remove column-critical entries in any of the ﬁrst
k columns. Thus, the column-critical entries of the diagonal remain critical.

1.2 The Exploded-Layered Tropical Determinant

Deﬁnition 1.3. Consider a commutative ELT ring R, and a matrix A = (aij ) ∈ Mn(cid:0)R(cid:1).

The ELT determinant of A is

|A| = Xσ∈Sn

[sign(σ)]0R · a1σ(1) · · · anσ(n).

Next we introduce some ELT linear algebra deﬁnitions, for which the the layer structure is a
for the remainder of this section we ﬁx

ﬁeld and the tangible values are the real numbers, i.e.
R = R (F, R).

Deﬁnition 1.4. For any two elements [ℓ1]a1, [ℓ2]a2 ∈ R, we deﬁne the tangible distance between
them to be |a1 −R a2|.
For any a ∈ R we deﬁne the tangible distance between a and −∞ to be ∞, where ∞ > x for all x ∈ R.

Recall that R∗ is the subset of R containing the invertible elements

R× := {[ℓ]λ ∈ R|ℓ 6= 0F}.

and R× = R× ∪ {−∞}.

n
Deﬁnition 1.5. A set of vectors S ⊆ R
and

is called linearly dependent if there exist v1, ..., vm ∈ S

such that

a1, ..., am ∈ R×

m

Xi=1

s(cid:16)

aivi(cid:17) = (0F, ..., 0F).

We are now ready to present a key deﬁnition.

Deﬁnition 1.6. Consider a matrix A = (aij) ∈ Mn(cid:0)R(cid:1). We deﬁne the critical layers matrix,

SA ∈ Fn×n, of A in the following way:

(SA)ij :=(s(aij ) aij is column-critical

otherwise

0F

The critical layers matrix enables us to use classical linear algebra, when possible.

9

1.3 Matrix singularity

is singular if and only if its rows and columns are linearly dependent.

We call a matrix A ∈ Mn(cid:0)R(cid:1) singular if |A| is not invertible. In this section we prove that a matrix
Theorem 1.7. Consider A ∈ Mn(cid:0)R(cid:1). Then the rows of A are linearly dependent, iﬀ the columns
of A are linearly dependent, iﬀ s(cid:16)|A|(cid:17) = 0F.

We prove several lemmas which together prove the theorem.

Lemma 1.8. If A ∈ Mn(cid:0)R(cid:1) is a critical matrix such that |SA| = 0, then the rows of A are

dependent.

Proof. Since |SA| = 0, the rows of SA ∈ Fn×n are linearly dependent, and after some permutation
of the rows we know that there exist nonzero scalars such that

ℓ1 Row1(SA) + ... + ℓk Rowk(SA) = (0F, ..., 0F).

If k = n we are done since

s(cid:16)[ℓ1]0 · Row1(A) + ... + [ℓn]0 · Rown(A)(cid:17) = (0F, ..., 0F).

Indeed, in any of the columns the sum of the rows is equal to the sum of the column-critical elements.

Assume k < n. Let C be the set of all columns with no column-critical element at any of the

rows 1, ..., k. If C = ∅ then

s(cid:16)[ℓ1]0 · Row1(A) + ... + [ℓk]0 · Rowk(A)(cid:17) = (0F, ..., 0F).

Otherwise, we use induction on the size of C. Assume that C is not empty. Take the least
tangible distance between column-critical elements in the columns of C and elements in the same
columns in rows 1, ..., k, and call it d. If d = ∞ then for all 1 ≤ i ≤ k and for all j ∈ C, [A]ij = −∞.
Therefore,

and we are done.

s(cid:16)[ℓ1]0 · Row1(A) + ... + [ℓk]0 · Rowk(A)(cid:17) = (0F, ..., 0F),

Assume d ∈ R, and consider the matrix B obtained from the matrix A by taking rows k + 1, ..., n
and columns in C. If SB is singular then there is some permutation of its rows, nonzero scalars and
some p such that

ℓk+1 Row1(SB) + ... + ℓk+p Rowp(SB) = (0F, ..., 0F).

We tropically divide rows k + 1, ..., n by [1] d

2 (which is classical substraction), and call the new

matrix A1. We notice that the following conditions hold:

1. For all 1 ≤ i ≤ k, j ∈ C, (cid:2)A1(cid:3)ij is not column-critical.
2. For all k + 1 ≤ i ≤ n, j 6∈ C, (cid:2)A1(cid:3)ij is not column-critical.

3. A1 is critical.

Therefore,

s(cid:16)[ℓ1]0 · Row1(A1) + ... + [ℓk+p]0 · Rowk+p(A1)(cid:17) = (0F, ..., 0F).

Deﬁne C1 to be the set of all columns with no column-critical element in rows 1, ..., k + p. Since A1
is critical, by condition (2) there must exist a column-critical element in a column in C and the row

10

k + 1. Thus |C| > |C1|, and the rows of A1 are dependent by induction, and therefore so are the
rows of A.

If SB is nonsingular, we tropically divide rows k + 1, ..., n by [1]d (which is classical substraction),

and call the new matrix A2. We notice that the following conditions hold:

1. There exists some column-critical element in rows 1, ..., k and columns in C.

2. For all k + 1 ≤ i ≤ n, j 6∈ C, (cid:2)A1(cid:3)ij is not column-critical.

3. A2 is critical.

Deﬁne

v = ℓ1 Row1(SA2) + ... + ℓk Rowk(SA2 ).

The set C2 of columns with no column-critical elements in rows 1, ..., k, is strictly smaller than C.
Therefore, if v = (0F, ..., 0F) then we are done by induction.

Assume that v 6= (0F, ..., 0F). Then vk 6= (0F, ..., 0F), where vk is the vector obtained from v by
striking all columns which are not in C. Since SB is nonsingular, there must exist some permutation
of the rows of A2 and nonzero scalars such that

ℓ1 Row1(SA2 ) + ... + ℓk+p Rowk+p(SA2 ) = (0F, ..., 0F).

Deﬁne C′
we are done by induction.

2 as the set of columns with no column-critical elements in rows 1, ..., k +p. Since |C′

2| < |C|,

The following lemma is a version of a similar proposition about rank defect, presented in the

paper of Izhakian and Rowen ([11, section 6]).

Lemma 1.9. Let A ∈ Mn(cid:0)R(cid:1) be a matrix. Either |A| = −∞ or there exist scalars ai ∈ R× such

that the matrix with rows aiRi(A) is critical.
Moreover, if |A| = −∞ then there exist a number r < n and n − r rows with r + 1 columns whose
entries are all −∞.

Proof. Consider a matrix B obtained by permutation and multiplication by some scalars ai ∈ R×
of the rows of A. Also assume that for all 1 ≤ i ≤ k, the entries [B]ii are critical and diﬀerent from
−∞, and that B is with maximal k with respect to a choice of such a matrix.

If k = n then B is critical, |B|, |A| 6= −∞ and we are done.

Therefore we assume that k < n. Next we build the set S, starting with S = φ:

1. Adjoin to S the indices of rows with column-critical elements (diﬀerent than −∞) at columns

k + 1, ..., n.

2. Adjoin to S any index of a row with a column-critical element (diﬀerent than −∞) at a column

with an index already in S.

3. Repeat step 2 until no new index is added.

We notice that S ⊆ {1, ..., k}, for otherwise there exist a permutation that enlarge k.

We assume that B is such that S is maximal possible.

For all i 6∈ S and j ∈ S or j > k, [B]ij = −∞. Otherwise we can multiply row i by a scalar such

that [B]ij will be column critical, and therefore enlarge S with i.

11

Write |S| = r. We obtained n − r rows with r + n − k ≥ r + 1 columns having only −∞ entries.
Therefore any track of the determinant must contain a choice of n − r columns out of the n − r rows,
one of which must be −∞. Therefore |B| = −∞, and consequently |A| = −∞.

Lemma 1.10. Let A ∈ Mn(R) be a matrix.
dependent.

If s(cid:16)|A|(cid:17) = 0F, then the rows of A are linearly

Proof. First, assume that |A| = −∞.

Due to Lemma 1.9, there are n − r rows for which there is a submatrix having r + 1 columns
containing only −∞ entries. Therefore we wish to prove that m = n − r rows with m − 1 = n − r − 1
columns are necessarily linearly dependent. It is trivial that two rows with one column are linearly
dependent. We will use induction on the number of rows.

If we have m − 1 − r′ rows with m − 1 − r′ + 1 columns all of whose entries are −∞, and
we are done by induction. Otherwise, take the ﬁrst m − 1 rows with the m − 1 columns having
some entry 6= −∞. Multiply this submatrix with scalars to obtain a critical (m−1)×(m−1) matrix.

We only need to prove that if A is a critical matrix then for any vector v, the rows of A together
with v are linearly dependent. Add v as the last row to A and add a last column with −∞ entries,
and call the obtained matrix B. B remains critical and |SB| = 0; therefore its rows are linearly
dependent by Lemma 1.8.

Next, we assume that |A| 6= −∞.

By Lemma 1.9, there exist scalars a1, ..., an ∈ R× such that the matrix B with rows aiRowi(A)

is critical. It is clear that s(cid:16)|B|(cid:17) = 0F.
Therefore, |SB| = s(cid:16)|B|(cid:17) = 0F. By Lemma 1.8 the rows of B are linearly dependent, and so are the

Since B is critical, a track dominates in |B| if and only if it contains only column-critical elements.

rows of A.

Lemma 1.11. Let B = {v1, ..., vn} ⊆ R

n

be a set of vectors such that

If every strict subset of B is linearly independent, then the matrix A with rows from B is critical.

s(cid:16)v1 + ... + vn(cid:17) = (0F, ..., 0F).

Proof. Consider the matrix A with rows v1, ..., vn, and assume that A is not critical. By permuting
the rows and columns we may assume that A satisﬁes the following conditions:

• The ﬁrst k elements of the diagonal are column-critical.

• The last n − k elements of the diagonal are not column-critical.

• There is no such permutation of the matrix for larger k.

We assume that k < n, for otherwise A is critical. Next we build a set S, starting with S = ∅:

1. Adjoin to S the indices of rows with column-critical elements at columns k + 1, ..., n.

2. Adjoin to S any index of a row with a column-critical element at a column with an index

already in S.

3. Repeat step 2 until no new index is added.

12

We notice that S ⊆ {1, ..., k}, for otherwise there would exists a permutation that increases k.

For any set I of indices, we denote the set of rows of A with indices in I by RowI (A). We denote

by ColI (A) the set of columns with indices in I.

Any element in a row from k + 1, ..., n and in a column from k + 1, ..., n could not be column-
critical since k is maximal. Also by our choice, any row from k+1, ..., n cannot have a column-critical
element in a column with an index in S.

Now we denote the number of indices in S by r = |S|. If r = k, then the rows k + 1, ..., n do not

have any column-critical elements and so

v1 + ... + vk = v1 + ... + vn.

Therefore we have a strict subset of B which is linearly dependent, in contradiction to our assump-
tion. Thus we may assume that r < k.

We claim that for all i ∈ S ∪ {k + 1, ..., n},


s
Xj∈S

vj


i

= 0F.

Indeed there are no column-critical elements in these columns in any of the rows in RowS(A), where
S = {1, 2, ..., k} − S.

We will show that there exist scalars ai ∈ R such that

s(cid:16)Xj∈S

vj +Xi∈S

aivi(cid:17) = (0F, ..., 0F).

Then Row{1,2,...,k}(A) is a linearly dependent proper subset of {v1, ..., vn}, a contradiction.

Let d be the least tangible distance between column-critical elements of columns in ColS(A) with
the next maximal element in the same column in the rows in RowS(A). If d = ∞ then every entry
of row in RowS(A) and column in ColS(A) must be −∞. Therefore

and we are done.

s(cid:16)Xj∈S

vj(cid:17) = (0F, ..., 0F),

If d ∈ R, we tropically divide the rows in RowS(A) by [1]d. We now have at least one column
in ColS(A) with a column-critical element in a row from RowS(A) and in a row from RowS(A).
Denote the matrix after this change by D.

For all r 6∈ S, j ∈ S,

and

"Xi∈S
Rowi(SD)#r
h Rowj(SD)ir

= 0F,

= 0F.

Therefore, we have |S| + 1 rows with only |S| columns not necessarily zero; thus they must be clas-
sically linearly dependent.

13

There must exist scalars such that

aXi∈S

Rowi(SD) +Xj∈S

aj Rowj(SD) = 0F.

Assume that aj = 0 for all j ∈ S. Then for each column p with a column-critical element in

Pi∈S vi, it is true that

s
"Xi∈S

vi#p


 = 0.

S, and continue the process.

In that case, we can add the indices of the columns with column-critical elements in Pi∈S vi to

Otherwise, for all j ∈ S such that aj 6= 0 add j to S, multiply row j of D by [aj ]0 and continue
the process. Indeed, the element in the j column of the j row is column-critical; therefore for all
j ∈ S ∪ {k + 1, ..., n}

Since S strictly increases, we obtain our result after a ﬁnite number of steps.

s(cid:16)hXi∈S

Rowi(D)ij(cid:17) = 0F.

Lemma 1.12. If A ∈ Mn(cid:0)R(cid:1) is a matrix with linearly dependent rows, then s(cid:16)|A|(cid:17) = 0F.

Proof. We will use induction on n, given that the case n = 1 is trivial.

Assume that the set {Row1(A), ..., Rown−1(A)} is linearly dependent. Then

s(cid:16)|A|(cid:17) = s(cid:16)

n

Xi=1

[(−1)n+i]0 · ani|Ani|(cid:17),

where Ani is the matrix obtained by striking row n and column i from A. By induction

for all i, and therefore s(cid:16)|A|(cid:17) = 0F.

s(cid:16)|Ani|(cid:17) = 0F,

Thus, we may assume that any strict subset of the rows of A is linearly independent. After some

permutation of the rows we may assume there exist scalars ai ∈ R× such that

Deﬁne A1 as the matrix with rows aiRowi(A) for all 1 ≤ i ≤ n. Clearly

s(cid:16)a1Row1(A) + ... + anRown(A)(cid:17) = (0F, ..., 0F).

and

s(cid:16)|A|(cid:17) = 0F ⇐⇒ s(cid:16)|A1|(cid:17) = 0F,
Rowi(A1)(cid:17) = (0F, ..., 0F).
s(cid:16)Xi
Therefore A1 is critical by Lemma 1.11, and s(cid:16)|A1|(cid:17) = |SA1|. Since

Rowi(SA1) = (0F, ..., 0F),

Xi

|SA1 | = 0F and we are done.

Now we will prove Theorem 1.7:

Proof. The result follows from Lemma 1.10 and Lemma 1.12.

14

1.4

Invertible Matrices

Deﬁnition 1.13. Let R be a commutative ELT ring. A matrix A ∈ Mn(cid:0)R(cid:1) is said to be invertible
if there exists B ∈ Mn(cid:0)R(cid:1) such that AB = BA = In.

We will now try to ﬁnd all of the left invertible matrices.

Deﬁnition 1.14. Let R be a commutative ELT ring. A generalized permutation matrix is a
matrix of the form

c1 · eσ(1)

· · ·




cn · eσ(n)


for invertible ci ∈ R and σ ∈ Sn. If each ci = [1]0, we denote that matrix Pσ.

Remark 1.15. Any generalized permutation matrix can be written as a product of a diagonal matrix
with a permutation matrix. Speciﬁcally,


c1 · eσ(1)

· · ·

 =
cn · eσ(n)


cσ(1)

−∞

. . .

−∞

cσ(n)

Pσ




The following theorem is a special case of [6, Theorem 1], combined with [15].

Theorem 1.16. If B ∈ Mn(cid:0)R(cid:1) is left invertible, then it is a generalized permutation matrix.

Proof. By [15], since R is a commutative ELT ring, B is invertible (and not only left invertible).

Now, according to [6, Theorem 1], there exists an invertible diagonal matrix D and aσ ∈ R such

that

B = D Xσ∈Sn

aσPσ

aσ = [1]0 and aσaτ = −∞ if σ 6= τ . But that condition yields that only one aσ 6= −∞,

where Xσ∈Sn

and thus aσ = [1]0, implying B = D · Pσ.

1.5 Rank of a matrix

In this section, we generalize Theorem 1.7 and prove that the ELT row rank of an ELT matrix is
equal to the ELT column rank.

Deﬁnition 1.17. Let A ∈ (R)m×n be an ELT matrix. The maximal number of linearly independent
rows from A, is called the ELT row rank of A.

Similarly, the ELT column rank of A is the maximal number of linearly independent columns of A.

Deﬁnition 1.18. Let A ∈ (R)m×n be an ELT matrix. The ELT submatrix rank of A is the maximal
size of a square nonsingular submatrix of A. If no such matrix exists, then the ELT submatrix rank
of A is deﬁned to be zero.

Theorem 1.19 (Rank Theorem). Let A ∈ (R)m×n be an ELT matrix. Then the ELT row rank of
A is equal to the ELT column rank of A and to the ELT submatrix rank of A.

15

1.5.1 Proof of the rank theorem

n
Deﬁnition 1.20. We deﬁne the unit vectors ei ∈ R

for all 1 ≤ i ≤ n by

[ei]j =(−∞ j 6= i

j = i

[1]0

.

Lemma 1.21. Let A ∈ (R)m×n be an ELT matrix, with n > m. If all m × m submatrices of A are
singular, then the rows of A are linearly dependent.

Proof. Assume that the rows of A are linearly independent. Let B be a matrix obtained by adjoin-
ing a maximal possible number of unit vectors as rows to A, such that the rows of B are linearly
independent.

Assume that B has n rows. Since B was obtained by adding unit vectors to A, the determinant
|B| is equal to the determinant of an m × m submatrix of A, which is singular. However, by our
construction the rows of B are linearly independent and therefore it must be nonsingular.

Therefore B ∈ (R)p×n is a matrix such that n > p, and for all 1 ≤ i ≤ n, the set {v1, ..., vp, ei}
is linearly dependent, where vi = Rowi(B). Since the rows of B are independent, for all 1 ≤ i ≤ n,
there exist scalars αij ∈ R× such that

s(cid:16)αi1v1 + ... + αipvp + ei(cid:17) = (0F, ..., 0F).

ui = αi1v1 + ... + αipvp.

For all 1 ≤ i ≤ n, write

Since for all j 6= i

and

then

= [ui]j

= [ui]i + [1]0,

hui + eiij
hui + eiii
s(cid:16)[ui]j(cid:17) =(0F

j 6= i
j = i

.

−1F

Let U be the matrix with rows u1, ..., un. We claim that U is singular. On the contrary, assume
that U is nonsingular. By Lemma 1.9 we may assume that U is critical after multiplying its rows

with scalars ci ∈ R×, such that s(cid:0)ci(cid:1) = 1F.

Since U is nonsingular, there must be a dominant track all of whose entries have nonzero layers.
The only nonzero layer elements are on the diagonal of U , and therefore the diagonal of U is a
dominant track, and the entries of the diagonal are column-critical.

Assume that an entry of vj dominates in ciui and ckuk at the diagonal of U , i.e.,

and

If τ (ciαij ) > τ (ckαkj) then

τ(cid:16)(cid:2)ciαij vj(cid:3)i(cid:17) = τ(cid:16)[ciui]i(cid:17)
τ(cid:16)(cid:2)ckαkj vj(cid:3)k(cid:17) = τ(cid:16)[ckuk]k(cid:17).

τ(cid:16)(U )kk(cid:17) = τ(cid:16)[ckuk]k(cid:17) = τ(cid:16)(cid:2)ckαkjvj(cid:3)k(cid:17) < τ(cid:16)[ciαij vj]k(cid:17) ≤ τ(cid:16)[ciui]k(cid:17) = τ(cid:16)(U )ik(cid:17).

16

However, τ(cid:16)(U )kk(cid:17) ≥ τ(cid:16)(U )ik(cid:17) since the entries of the diagonal are column-critical, thus

τ (ciαij) ≤ τ (ckαkj ).

By a similar argument, we deduce τ (ciαij ) ≥ τ (ckαkj ) and therefore τ (ciαij) = τ (ckαkj ).

Therefore, any vector vj may contribute to no more than one vector of layers to the layers of the

diagonal of U , by the following argument.

For all 1 ≤ j ≤ p, we write

wj =(s(cid:16)[vj ]i(cid:17), ∃i : τ(cid:16)(cid:2)ciαij vj(cid:3)i(cid:17) = τ(cid:16)[ciui]i(cid:17)

otherwise

0F

with wj ∈ Fn. Recall that the critical layer matrix SU is deﬁned (1.6) by

(SU )ij :=(s(cid:16)[ciui]j(cid:17),

0F

[ciui]j is column-critical

otherwise

,

.

Therefore SU = −In ∈ Fn×n. (Note that s(cid:16)[ciui]j(cid:17) = s(cid:16)[ui]j(cid:17) since s(cid:0)ci(cid:1) = 1F.)

Together, for all i,

Rowi(SU ) = s(αi1)w1 + ... + s(αip)wp,

and therefore

Rowspan(cid:16)SU(cid:17) ⊆ span{w1, ..., wp}.

Since Rowspan(cid:16)SU(cid:17) = Fn and n > p, we have a contradiction.

Therefore U must be singular as claimed, and there exist scalars βi ∈ R×, not all −∞ such that

s(cid:16)β1u1 + ... + βnun(cid:17) = (0F, ..., 0F).

Since s(cid:16)[ui]i(cid:17) 6= 0 and s(cid:16)[ui]j(cid:17) = 0 for i 6= j,

for all 1 ≤ i ≤ n. Therefore for all 1 ≤ i ≤ n, we may choose di from the range

[βiui]i <(cid:2)β1u1 + ... + βnun(cid:3)i

such that

Furthermore, write

0 < di < dist((cid:2)β1u1 + ... + βnun(cid:3)i, [βiui]i),
s(cid:16)[1]d1β1u1 + ... + [1]dnβnun(cid:17) = (0F, ..., 0F).

[1]d1β1u1 + ... + [1]dnβnun = a1v1 + ... + apvp.

Since there are inﬁnitely many choices for each di, one may choose them so that for all j

s(cid:16)aj(cid:17) = s(cid:16)Xi

[1]diβiαij(cid:17) 6= 0F,

and therefore aj ∈ R×. Clearly, since not all βj = −∞, not all aj are equal to −∞.

Therefore v1, ..., vp are linearly dependent.

17

Now we can conclude the proof of Theorem 1.19. If a matrix A has ELT row rank k, then it
has k independent rows and by Lemma 1.21 it must have a k × k nonsingular submatrix. Since any
k + 1 rows are linearly dependent, it is clear that any (k + 1) × (k + 1) submatrix must be singular.
Therefore the ELT submatrix rank of A is k, as is the ELT row rank of A.

Since the ELT submatrix rank of A and At are equal, the ELT column rank must equal the ELT

row rank as well.

1.5.2 Kapranov and Barvinok rank

In their paper [5], Develin, Santos and Sturmfels review three diﬀerent deﬁnitions of matrix rank:
Barvinok, Kapranov and tropical rank. Furthermore, they prove that for any tropical matrix A

tropical-rank(A) ≤ Kapranov-rank(A) ≤ Barvinok-rank(A),

where both of these inequalities can be strict. We will present analogous deﬁnitions for rank admit-
ting the above inequalities.

The analog of tropical rank is the ELT submatrix rank we introduced in deﬁnition 1.18.

In this section we assume that the ﬁeld of layers F is algebraically closed, and recall that K is

the ﬁeld of Puiseux series with coeﬃcients in F and powers in R.

Deﬁnition 1.22. Let A ∈ (R×)m×n be an ELT matrix. The ELT Kapranov rank of A is the
minimal rank of any matrix A(t) ∈ Km×n such that ELTrop(A(t)) = A.

Deﬁnition 1.23. Let A ∈ (R×)m×n be an ELT matrix. The ELT Barvinok rank of A is the min-
imal number r of matrices A1, ..., Ar of submatrix rank 1, such that A1 + A2 + ... + Ar = A.

Since linear dependence of vectors in Kn implies linear dependence of their tropicalization, one

can easily verify that

ELT-submatrix-rank(A) ≤ ELT-Kapranov-rank(A) ≤ ELT-Barvinok-rank(A).

Lemma 1.24. For every ELT matrix A ∈ (R∗)m×n, there exists a matrix A(t) ∈ Km×n such that

ELTrop(cid:16)A(t)(cid:17) = A and

ELT-submatrix-rank(A) = rank(cid:16)A(t)(cid:17).

Proof. Write ELT-submatrix-rank(A) = r. There exists a submatrix of A of size r × r that is nonsin-
gular, and every larger submatrix is singular.

Let Gr be the set of generators of the classical determinantal ideal of size r, over Km×n. For

every polynomial g ∈ Gr+1, the matrix A is an ELT root of ELTrop[g].

Since Gr+1 is a Gr¨obner basis of the determinantal ideal Ir+1 (ref.

of ELTrop[f ] for every f ∈ Ir+1. By the fundamental theorem (ref.
A(t) ∈ V (Ir+1) ⊆ Km×n such that ELTrop(A(t)) = A.

[5]), A is an ELT root
[13]), there exists a matrix

Now rank(cid:16)A(t)(cid:17) ≤ r, since A(t) ∈ V (Ir+1). Also

rank(cid:16)A(t)(cid:17) ≥ ELT-submatrix-rank(ELTrop[A(t)]) = ELT-submatrix-rank(A) = r.

18

Together,

rank(cid:16)A(t)(cid:17) = ELT-submatrix-rank(A).

Theorem 1.25. For any ELT matrix A ∈ (R×)m×n

ELT-submatrix-rank(A) = ELT-Kapranov-rank(A)

Proof. We need to prove the inequality

ELT-submatrix-rank(A) ≥ ELT-Kapranov-rank(A).

By Lemma 1.24 there exist a matrix A(t) such that ELTrop(cid:16)A(t)(cid:17) = A and

By Deﬁnition 1.22,

and thus

ELT-submatrix-rank(A) = rank(cid:16)A(t)(cid:17).
rank(cid:16)A(t)(cid:17) ≥ ELT-Kapranov-rank(A),

ELT-submatrix-rank(A) ≥ ELT-Kapranov-rank(A).

Next, we deﬁne the ELT rank of a tropical matrix in terms of the usual sense (without layers).

Deﬁnition 1.26. Let A ∈ (R)m×n be a tropical matrix (without layers).

The ELT rank of A is the minimal ELT submatrix rank of any matrix EA ∈ (cid:0)R∗(cid:1)m×n

τ (EA) = A (in other words, EA is obtained by assigning layers to the entries of A).

such that

Example 1.27. Consider A ∈ (R)m×n,

A =(cid:18) 0

−∞ 0 (cid:19) .

−∞

Any matrix EA ∈(cid:0)R∗(cid:1)m×n

such that τ (EA) = A is of the form

EA =(cid:18)[x1]0 −∞
−∞ [x2]0(cid:19) ,

where x1, x2 6= 0.

Now for every choice of EA

ELT-submatrix-rank(EA) = 2

since s(cid:16)|EA|(cid:17) = x1x2 6= 0. Therefore

ELT-rank(A) = 2.

19

Example 1.28. Consider A ∈ (R)m×n,

A =(cid:18)0

0

0

0(cid:19) .

Any matrix EA ∈(cid:0)R∗(cid:1)m×n

where x1, x2, x3, x4 6= 0.

such that τ (EA) = A is of the form

EA =(cid:18)[x1]0 [x2]0
[x3]0 [x4]0(cid:19) ,

Now for every choice of EA, any submatrix of size 1 × 1 is nonsingular; therefore

Choosing x1 = x2 = x3 = x4 = 1 we obtain EA which is singular; and therefore

ELT-submatrix-rank(EA) ≥ 1.

Together

ELT-submatrix-rank(EA) = 1.

ELT-rank(A) = 1.

Proposition 1.29. For any tropical matrix A

ELT-rank(A) = Kapranov-rank(A).

Proof. Let EA ∈(cid:0)R∗(cid:1)m×n
A(t) ∈ K m×n such that ELTrop(cid:16)A(t)(cid:17) = EA and

be a matrix such that A = τ (EA). By Lemma 1.24, there exists a matrix

Therefore by Deﬁnition 1.22

rank(cid:16)A(t)(cid:17) = ELT-submatrix-rank(EA).

ELT-submatrix-rank(EA) ≥ Kapranov-rank(A).

Since this is true for all such EA, then

On the other hand, choose a matrix A(t) ∈ K m×n such that

ELT-rank(A) ≥ Kapranov-rank(A).

and

Since

and

then

τ(cid:16)ELTrop(cid:0)A(t)(cid:1)(cid:17) = A,

rank(cid:16)A(t)(cid:17) = Kapranov-rank(A).

rank(cid:16)A(t)(cid:17) ≥ ELT-submatrix-rank(cid:16)ELTrop(cid:0)A(t)(cid:1)(cid:17),
ELT-submatrix-rank(cid:16)ELTrop(cid:0)A(t)(cid:1)(cid:17) ≥ ELT-rank(A),
Kapranov-rank(A) = rank(cid:16)A(t)(cid:17) ≥ ELT-rank(A).

Together we obtain

Kapranov-rank(A) = ELT-rank(A).

20

2 Applications of ELT Linear Algebra

2.1

Inner Products and Orthogonality

In this
section, we introduce the deﬁnitions of ELT inner product and orthogonality.
Although we prove that an orthogonal set of vectors is linearly independent, if we add an orthogonal
vector to a linearly independent set, we may obtain a linearly dependent set.

2.1.1 Inner product

Deﬁnition 2.1. Let R = R (C, R) be an ELT algebra. An ELT inner product is a function

that satisﬁes the following three axioms for all vectors v, u, w ∈ R

n

and all scalars a, b ∈ R:

n
h, i : R

n
× R

→ R,

1. hav + bu, wi = ahv, wi + bhu, wi.

2. τ(cid:16)hv, ui(cid:17) = τ(cid:16)hu, vi(cid:17), and s(cid:16)hv, ui(cid:17) = s(cid:16)hu, vi(cid:17) (complex conjugate).
3. s(cid:16)hv, vi(cid:17) ≥ 0R and if v ∈ R×

, then s(cid:16)hv, vi(cid:17) = 0R ⇐⇒ v = (−∞, ..., −∞).

n

For convenience, we will write u · v instead of hu, vi for the remainder of this section.

Notice that we abuse the over-line notation for both an ELT algebra with the −∞ element (R×)

and the complex conjugate (s(cid:16)hu, vi(cid:17)).

Example 2.2. For any two vectors v1, v2 ∈ R

n

,

v1 = ([z1]α1, ..., [zn]αn),

v2 = ([w1]β1, ..., [wn]βn),

we deﬁne the standard inner product

v1 · v2 := [z1]α1

[w1]β1 + ... + [zn]αn

[wn]βn.

The ﬁrst two axioms are trivial to prove, and we will prove the third.

If v = (−∞, ..., −∞) then v · v = −∞, and thus s(cid:16)v · v(cid:17) = 0C.

Otherwise write

v = ([z1]α1, ..., [zn]αn),

S = {i|αi = max
1≤j≤n

αj }.

Then

and

v · v =Xi∈S
s(cid:16)v · v(cid:17) =Xi∈S

[|zi|2]2αi,

|zi|2 ≥ 0R.

But v ∈ R×

n

and v 6= (−∞, ..., −∞) implying zi 6= 0C for all i ∈ S, and thus s(cid:16)v · v(cid:17) 6= 0C.

21

Lemma 2.3. For every two vectors v1, v2 ∈ R×

n

,

In other words, either

or

Proof. If

then

v1 · v2 ≤ v1 · v1 + v2 · v2.

v1 · v1 ≥ v1 · v2,

v2 · v2 ≥ v1 · v2.

v1 = (−∞, ..., −∞)

and therefore we assume that

(v1 · v2)2 = (v1 · v1)(v2 · v2) = −∞,

v1 6= (−∞, ..., −∞).

For any scalar [z]0 ∈ R,

(v1 + [z]0v2) · (v1 + [z]0v2) = v1 · v1 + [z]0v2 · v1 + v1 · ([z]0v2) + ([z]0v2) · ([z]0v2).

Now

and

τ(cid:16)[z]0v2 · v1 + v1 · ([z]0v2)(cid:17) = τ (v1 · v2)

s(cid:16)[z]0v2 · v1 + v1 · ([z]0v2)(cid:17) = 2Re (z · s(v1 · v2)) .

Since v1, v2 ∈ R×

n

, we may choose z such that both

and

v1 + [z]0v2 ∈ R×

n

2Re (z · s(v1 · v2)) < 0R.

Assume that v1 · v1 + v2 · v2 < v1 · v2. Then

(v1 + [z]0v2) · (v1 + [z]0v2) = [z]0v2 · v1 + v1 · ([z]0v2)

and

which is absurd.

s(cid:16)(v1 + [z]0v2) · (v1 + [z]0v2)(cid:17) = 2Re (z · s(v1 · v2)) < 0R

Now we extend this lemma to several vectors.

Lemma 2.4. If v1, ..., vk ∈ (R)n, then there exists some p for which

vp · vp ≥ X1≤j6=p≤k

vj · vp.

22

Proof. Assume

and choose a speciﬁc i 6= j such that ∀p : vi · vj > vp · vp. It follows that vi · vj > vi · vi. Thus by
Lemma 2.3 vi · vj ≤ vj · vj, which contradicts our assumption.

∀p : X1≤i,j≤k

vi · vj > vp · vp,

Therefore there exists some p for which

and speciﬁcally

vp · vp ≥ X1≤i,j≤k
vp · vp ≥ X1≤j6=p≤k

vi · vj ,

vj · vp.

2.1.2 Orthogonality

n
Deﬁnition 2.5. Consider v1, v2 ∈ R

. We say v1, v2 are orthogonal and write v1 ⊥ v2 if

Theorem 2.6. If v1, ..., vk ∈ (R×)n are vectors such that

s(cid:16)v1 · v2(cid:17) = 0C.

and

∀i : vi 6= (−∞, ..., −∞)

∀i 6= j : vi ⊥ vj,

then v1, ..., vk are linearly independent.

Proof. Assume that v1, ..., vk are linearly dependent. Then there exists α1, ..., αk ∈ R× such that

If ui = αivi, then by Lemma 2.4 there exists p such that

s(cid:16)α1v1 + ... + αkvk(cid:17) = (0C, ..., 0C).

Multiplying by up, we obtain

up · up ≥ X1≤j6=p≤k

uj · up.

s(cid:16)u1 · up + ... + uk · up(cid:17) = s(cid:16)(0C, ..., 0C) · up(cid:17) = 0C.

Therefore ∀i 6= p : s(cid:16)ui·up(cid:17) = 0C and up·up dominates all other term. It follows that s(cid:16)up·up(cid:17) = 0C,

which is absurd.

Lemma 2.7. Let v1, ..., vk ∈ (R×)n such that k < n, vi ⊥ vj for all i 6= j, and vi 6= (−∞, ..., −∞)
for all i. Then there exist vk+1, ..., vn such that the set {v1, ..., vn} is orthogonal.

Proof. Assume V1, ..., Vk ∈ Kn are lifts of v1, ..., vk, i.e., ELTrop(Vi) = vi for all i. Then there exist
vectors Uk+1, ..., Un ∈ Kn such that Uj ⊥ Vi for all 1 ≤ i ≤ k and k + 1 ≤ j, and Uj ⊥ Up for all
j 6= p
It is easy to see that if uj = ELTrop(Uj), then {v1, ..., vk, uk+1, ..., un} is an orthogonal set.

23

Example 2.8. In this example, we present a linearly independent set S = {v1, v2} which is not
orthogonal, and a vector v3 which is orthogonal to S, whereas the set {v1, v2, v3} is linearly depend-
ent.

v1 = ([1]2, [−1]2, [−1]1),

v2 = ([−1]2, [1]2, [−1]1),

v3 = ([1]1, [1]1, [2]1).

These vectors are linearly dependent since

v1 + v2 + v3 = ([0]2, [0]2, [0]1).

However, it is easy to see that v1, v2 are linearly independent, and that v3 is orthogonal to both v1
and v2:

v2 · v3 = v1 · v3 = [0]3.

References

[1] Marianne Akian, Guy Cohen, Stephane Gaubert, R Nikoukhah, and Jean Pierre Quadrat.
Linear systems in (max,+) algebra. In Decision and Control, 1990., Proceedings of the 29th
IEEE Conference on, pages 151–156. IEEE, 1990.

[2] Marianne Akian, St´ephane Gaubert, and Alexander Guterman. Linear independence over trop-

ical semirings and beyond. Contemporary Mathematics, 495:1, 2009.

[3] Marianne Akian, St´ephane Gaubert, and Alexander Guterman. Tropical cramer determinants

revisited. Tropical and Idempotent Mathematics and Applications, 616:45, 2014.

[4] Peter Butkoviˇc. Max-algebra: the linear algebra of combinatorics? Linear Algebra and its

applications, 367:313–335, 2003.

[5] Mike Develin, Francisco Santos, and Bernd Sturmfels. On the rank of a tropical matrix. Com-

binatorial and computational geometry, 52:213–242, 2005.

[6] David Dolˇzan and Polona Oblak. Invertible and nilpotent matrices over antirings. arXiv preprint

arXiv:0806.2996, 2008.

[7] St´ephane Gaubert. Th´eorie des syst`emes lin´eaires dans les dio¨ıdes. PhD thesis, 1992.

[8] Stephane Gaubert. Methods and applications of (max,+) linear algebra. In STACS 97, pages

261–282. Springer, 1997.

[9] Zur Izhakian and Louis Rowen. The tropical rank of a tropical matrix. Communications in

Algebra R(cid:13), 37(11):3912–3927, 2009.

[10] Zur Izhakian and Louis Rowen. Supertropical algebra. Advances in Mathematics, 225(4):2222–

2286, 2010.

[11] Zur Izhakian and Louis Rowen. Supertropical matrix algebra. Israel Journal of Mathematics,

182(1):383–424, 2011.

[12] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics

quarterly, 2(1-2):83–97, 1955.

[13] Diane Maclagan and Bernd Sturmfels. Introduction to tropical geometry, volume 161. American

Mathematical Soc., 2015.

[14] Brett Parker. Exploded manifolds. Advances in Mathematics, 229(6):3256–3319, 2012.

24

[15] Christophe Reutenauer and Howard Straubing. Inversion of matrices over a commutative semir-

ing. Journal of Algebra, 88(2):350–360, 1984.

[16] Louis Halle Rowen. Symmetries in tropical algebra. arXiv preprint arXiv:1602.00353, 2016.

[17] Sergei Sergeev. Max-plus deﬁnite matrix closures and their eigenspaces. Linear algebra and its

applications, 421(2-3):182–201, 2007.

[18] Erez Sheiner. Exploded Layered Tropical Algebra. PhD thesis, Bar-Ilan University, 2015.

[19] Yijia Tan. On invertible matrices over antirings. Linear algebra and its applications, 423(2):428–

444, 2007.

Guy Blachar, Department of Mathematics, Bar-Ilan University, Ramat-Gan, Israel
E-mail address: blachag@biu.ac.il

Erez Sheiner, Department of Mathematics, Bar-Ilan University, Ramat-Gan, Israel
E-mail address: erez@math.biu.ac.il

25

