6
1
0
2

 
r
a

 

M
6
2

 
 
]
T
S
h
t
a
m

.

[
 
 

3
v
1
7
5
4
0

.

3
0
6
1
:
v
i
X
r
a

EDGE EXCHANGEABLE MODELS FOR NETWORK DATA

HARRY CRANE AND WALTER DEMPSEY

Abstract. Exchangeable models for vertex labeled graphs cannot repli-
cate the large sample behaviors of sparsity and power law degree distri-
butions observed in many network datasets. Out of this mathematical
impossibility emerges the question of how network data can be modeled
in a way that reﬂects known empirical behaviors and respects basic sta-
tistical principles. We address this question by observing that edges, not
vertices, act as the statistical units in many network datasets, making a
theory of edge labeled networks more natural for these applications. In
this context we introduce the new invariance principle of edge exchange-
ability, which unlike its vertex exchangeable counterpart admits models
for networks with sparse and/or power law structure. With this, we settle
a longstanding question in statistical network modeling. We characterize
all edge exchangeable network models and establish their basic statistical
properties. We also identify a tractable family of distributions with a clear
interpretation and suitable theoretical properties, whose signiﬁcance in
estimation, prediction, and testing we demonstrate.

1. Introduction

Statistical network analysis is hamstrung by the lack of an inferential
framework that admits both sound models for network formation with
observed empirical properties and facilitates statistical inference in the way
of estimation, prediction, and testing. Network datasets emerging from
a wide range of real world processes, such as communications [18, 26],
collaborations [4, 24], and relationships [22, 33], exhibit common structural
features, namely sparsity and power law degree distribution [1, 4, 15, 17, 20].
Such networks are typically represented by a graphical structure, with labels
assigned arbitrarily for the purpose of distinguishing between units.

Two fundamental statistical principles guide the modeling of such data:
(I) “There should be consistency with known limiting behaviour.” [10,

p. 5]

(II) “The sense of the model and the meaning of the parameter [...] may
not be aﬀected by accidental or capricious choices such as sample
size or experimental design.” [23, p. 1237]

Date: March 29, 2016.
Key words and phrases. network data; sparse network; power law distribution; exchange-

able random graph.

H. Crane is partially supported by NSF grants CNS-1523785 and CAREER DMS-1554092.

1

2

HARRY CRANE AND WALTER DEMPSEY

Within our speciﬁc context, Principle (I) speaks to the need for models
that replicate the asymptotic properties of sparsity and power law, while
Principle (II) recalls the logical necessity that inference is unhindered by
arbitrary choices in the assignment of labels and subsampling mechanism
used for data acquisition.

As is well known throughout statistics and probability, cf. [3, 5], Principles
(I) and (II) are not compatible under the conventional representation of
network data by a graph with labeled vertices, as in Figure 1(b), and the
corresponding notion of exchangeability with respect to vertex relabeling.
Assuming an inﬁnite population of labeled vertices:

An exchangeable random graph is sparse if and only if it is
has no edges with probability 1.

Seemingly, Principles (I) and (II) cannot simultaneously be satisﬁed when
handling sparse network data.

As a heuristic explanation for why ordinary vertex exchangeability is
untenable in most applications, we observe that many network datasets
are induced by a process of interactions within a population. From this
point of view, the sampled vertices are not representative of the larger
population—they are incidental to the interaction sampling scheme and,
thus, tend to have higher than average degree—explaining why vertices
cannot be labeled exchangeably in most applications.

With this observation, we settle a longstanding question of statistical
network analysis by recognizing that the vast majority of network datasets
arise by a process of interactions, with the Enron email corpus [18] and actors
collaboration network [29] as primary examples. In these cases, the edges
correspond to the statistical units, motivating our development of a theory
for edge exchangeable networks. In addition to resolving major statistical issues
at play in network analysis, edge exchangeable models address a closely
related question posed recently in the machine learning literature by Orbanz
and Roy [27, p. 459]:

Is there a notion of probabilistic symmetry whose ergodic mea-
sures [...] describe useful statistical models for sparse graphs with
network properties?

We build our framework from ﬁrst principles, with a formal deﬁnition
In
of network data and basic network properties in Sections 2 and 3.
Section 4, we introduce edge exchangeability as the appropriate invariance
for network data arising in a wide range of applications.
In Section 5,
we identify a family of edge exchangeable models with many properties
suitable for statistical inference. Speciﬁcally, this model furnishes an explicit
generating mechanism for edge exchangeable networks that exhibits sparse
and power law behavior. In Section 6, we discuss statistical inference from
edge exchangeable models, including estimation, prediction, and testing,
which we illustrate with several examples along the way. We discuss how
the edge exchangeable framework diﬀers from prior treatments in Section

EDGE EXCHANGEABLE MODELS FOR NETWORK DATA

3

Figure 1. (a) A network structure derived from some physi-
cal process. Neither vertices nor edges come equipped with
labels. Labels are assigned exogenously during data analysis.
(b) Network data obtained from Panel (a) by labeling vertices.
(c) Network data obtained from Panel (a) by labeling edges.

7. We defer all proofs to Section 8, in which we also prove a de Finetti-type
characterization of the class of edge exchangeable network models.

2. Network data

Figure 1 depicts a network whose edges correspond to the outcome of
a process of interactions among individuals in a population P. Figure 1(a)
reﬂects that the real world process does not usually generate any vertex
or edge labels; rather, labels are assigned after data generation in order to
distinguish among statistical units and facilitate data analysis. The labeled
versions in Figures 1(b) and 1(c) show two possible ways to represent the
interactions in Figure 1(a) as a network with labeled vertices and edges,
respectively.
In the actors collaboration network [4, 29], P consists of all movie actors
and each movie corresponds to an interaction involving the set of individuals
in its cast. In the Enron network [18], edges correspond to emails exchanged
between Enron employees. In general there are more than two actors in
each movie, nothing precludes a set of actors from being cast together in
more than one movie, and actors sometimes play more than one role in the
same ﬁlm; likewise, emails can involve more than two individuals, an email
exchange between the same group of people can occur repeatedly over time,
and a person may be listed multiple times in the recipient list of the same
email.

Both of these examples produce networks with multiple interactions
among the same set of individuals, interactions involving more than two
individuals, and multiple appearances of the same individual within the
same interaction. Analogous observations are possible for many other
network datasets, see Table 1, and so we allow network data to contain
edges with any ﬁnite multiset of vertices and multiple copies of the same
edge. We write ﬁn(P) to denote the set of all ﬁnite multisets of P.

4

HARRY CRANE AND WALTER DEMPSEY

Deﬁnition 2.1 (Network data). An interaction process for a population P is a
collection E = {Ei}i∈I of ﬁnite multisets of P, with I indexing the set of interactions.
In particular, I indexes a set of interactions and each i ∈ I corresponds to the
subset Ei ∈ ﬁn(P) of individuals involved in interaction i. Network data based on
the interactions in E is a graph induced by associating each element of P to a vertex
and each i ∈ I to an edge connecting the vertices corresponding to the elements of
Ei.

Within Deﬁnition 2.1, each interaction i ∈ I has an arity given by #E(i), the
cardinality of the multiset of individuals involved in interaction i counted
with multiplicity. This deﬁnition encompasses the most common instance
of network data for which each interaction involves exactly two vertices.
For simplicity, all of our ﬁgures specialize to the case of binary interactions.
Deﬁnition 2.1 stresses the diﬀerence between the data generating process, as
a process of interactions in a population, and the representation of the data, as
a network. As stated above, the deﬁnition permits network representations
which either disregard or otherwise destroy certain features of the data. As
a common example, the network representation of the Enron email network
in [18] associates every email exchange involving the same set of vertices
to the same edge, thereby discarding potentially meaningful information
about how often certain communications occur. We discuss the implications
and consequences of this approach in Sections 5.4 and 6.1 below.
To represent the interaction process as faithfully as possible, we associate
each element of P to a unique vertex and each interaction Ei to a unique
edge. In the actors collaboration dataset, for example, the interaction process
E = {Ei}i∈I corresponds to a collection of movies, with each Ei ∈ ﬁn(P)
containing the actors who appear in the movie indexed by i ∈ I. The
network induced by E associates actors to vertices and movies to edges by
including an edge connecting the actors in Ei for each i ∈ I.
We stress that the collection of actors seen in the interaction process {Ei}i∈I
corresponds only to those actors who appear in at least one of the movies
indexed by I. As actors not appearing among the observed movies are
unknown to the process, no actor can be thought of in isolation from the
set of roles he or she has played. Just as one might wonder in what sense
someone who acts only in one-man shows before empty auditoriums can
be reasonably regarded as an actor, we note that the vertices in any network
dataset driven by an interaction process have no meaning beyond their
interactions with other vertices. Given the structure of the interactions,
vertex labels are superﬂuous in the graphical representation, and so we
remove vertex labels and represent E by an edge labeled network as in
Figure 1(c). From now on, we do not distinguish between the interaction
process {Ei}i∈I and its associated edge labeled network representation.
Deﬁnition 2.1 captures subtle features of network data that the more
conventional representation by a graph G = (P,E), with vertex set P and
edge set E ⊆ ﬁn(P), does not. In statistical practice, labeling the vertices, as

EDGE EXCHANGEABLE MODELS FOR NETWORK DATA

5

in Figure 1(b), underlies the assumption that the vertices are the statistical
units for the intended application. Aside from Kolaczyk [19, p. 54], who
states it explicitly, this assumption about units is implicit in all other studies
of network models. In many situations, including all examples in Table 1,
the interactions, and thus the edges, act as the statistical units, as Deﬁnition
2.1 and the preceding discussion makes clear. Representing a network by
a vertex labeled graph, on the other hand, suggests that the data arises
by observing all interactions among individuals in a sample S ⊂ P, an
assumption which is plainly false in interaction datasets.
As the interactions I comprise the statistical units, observed data comes
about by sampling interactions J ⊂ I and observing the restricted process
E|J = {Ei}i∈J. The network data corresponding to the sampled interactions
E|J is the edge labeled network obtained by removing edges labeled in
I \ J = {i ∈ I : i (cid:60) J}, as shown in Figure 3(c). We write E|J to denote
both the restricted process of interactions and its representation as an edge
labeled network.

3. Network properties

Associated to any interaction process E = {Ei}i∈I is a sample size parameter
size(E) = #I corresponding to the number of sampled units, that is, interac-
tions. In the network data corresponding to E, size(E) corresponds to the
number of edges, denoted e(E).

Principle (I) factors into network modeling by noting that sparsity and
power law are asymptotic properties of a sequence of network datasets
(En)n≥1 of growing size. For now we proceed without any assumed labeling
and deﬁne v(E) as the number of vertices, e(E) as the number of edges, Mk(E)
as the number of k-ary edges for each k ≥ 1, and d(E) = (dk(E))k≥0 as the
degree distribution of a network dataset E, where dk(E) is the proportion of
vertices with degree k in E with dk(E) = Nk(E)/v(E) for Nk(E) equal to the
number of vertices incident to exactly k edges. For example, the network
data E in Figure 1(a) has v(E) = 7, e(E) = 6, Mk(E) = 6 for k = 2, Mk(E) = 0
for k (cid:44) 2, and d(E) = (1/7, 3/7, 1/7, 1/7, 1/7).
Deﬁnition 3.1 (Sparsity and power law degree distribution). Let (En)n≥1 be
a sequence of network datasets with size(En) = n for each n ≥ 1. The sequence
(En)n≥1 is sparse if

lim sup
n→∞

e(En)

v(En)m•(En)

= 0,

−1(cid:80)

(1)

k≥1 kMk(En) is the average arity of the edges in En.

where m•(En) = e(En)
The sequence (En)n≥1 exhibits power law degree distribution if for some γ > 1
the degree distributions (d(En))n≥1 satisfy dk(En) ∼ L(k)k
−γ as n → ∞ for all large
k for some slowly varying function L(x), that is, limx→∞ L(tx)/L(x) = 1 for all
t > 0, where an ∼ bn indicates that an/bn → 1 as n → ∞.

6

HARRY CRANE AND WALTER DEMPSEY

Figure 2. Statistical operations on network data with labeled
edges. Panel (a) is network data for a sample of six units
labeled 1, . . . , 6. Panel (b) shows the network from (a) rela-
beled according to permutation (1354)(26). Panel (c) shows
the restriction of the network in (a) to the units labeled in
1, . . . , 4.

Condition (1) extends the usual notion of sparsity for binary network data,
in which each interaction involves exactly two vertices, to any network data
representing a process of interactions.
In the familiar case when every
interaction involves exactly 2 vertices, every edge has arity 2, implying that
m•(En) = 2 for all n ≥ 1 and (En)n≥1 is sparse if

lim sup
n→∞

e(Gn)
v(Gn)2

= 0.

Sparsity and power law are deﬁned as properties of a growing sequence
of networks, not the interaction process. The observation of sparsity and/or
power law behavior in network data, therefore, does not immediately cor-
respond to inherent properties of the associated interaction process unless
the network representation respects the structure of the interaction process
and the network data is built from a representative sample of units.

For the network datasets of Table 1, we assume the interactions corre-
spond to a countably inﬁnite collection of units, which we may identify
with N = {1, 2, . . .} without any adverse consequences. Since labels are
arbitrary, we identify any sample of n units from N by [n] = {1, . . . , n}. The
interaction process E = {Ei}i∈N induces a sample E|[n] = {Ei}i∈[n] for each
n ≥ 1, allowing us to extend the deﬁnitions of sparsity and power law to
interaction processes E = {Ei}i∈N in the usual way.
Deﬁnition 3.2 (Sparsity and power law in the population network). Let
E = {Ei}i∈N be an interaction process for population P. Then E is sparse, resp.
exhibits power law degree distribution with exponent γ > 1, if the sequence of
network datasets induced by (E|[n])n≥1 is sparse, resp. exhibits power law degree
distribution with exponent γ > 1, in the sense of Deﬁnition 3.1.

EDGE EXCHANGEABLE MODELS FOR NETWORK DATA

7

Sampling/Growth type
none
edge vertex

network
Actors
Enron
Karate Club
Wikipedia
US Airport
Co-authorship
UC Irvine
Facebook
Political blogs
US Power Grid1

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

Table 1. Catalog of common network datasets categorized
according to how they are sampled. The sampling scheme
is closely tied to how these networks grow by addition of
edges or vertices. See Section 8.3 for a detailed description
of each dataset. Each (cid:88) signiﬁes that the network exhibits
growth or sampling of a certain generic type, that is, vertex,
edge, or no growth/sampling. 1 Network exhibits no growth.

4. Edge exchangeable network models

Deﬁnition 3.2 emphasizes that any discussion of sparsity or power law
behavior in a network dataset makes implicit reference not only to a pop-
ulation network but also to a sampling mechanism by which a sequence
of networks of growing size comes about. The subtle distinction between
the interaction process and its representation as a network elucidates the
role of exchangeability as a logical principle rather than a computational
or practical assumption made for the purpose of tractability. Models for
network data should be invariant with respect to relabeling units precisely
because the labels are assigned exogenously to the real world process, and
for no other reason. Deﬁnition 2.1 and our identiﬁcation of edges as the units
rightly shifts the focus from vertices to edges in network representations of
interaction processes, foreshadowing why our notion of edge exchangeability
in Deﬁnition 4.1 is a natural invariance principle for network modeling in
this context.

With the exception of the Power Grid network, all of the networks in
Table 1 are driven by interactions within a population. The Power Grid
network is a ﬁxed, physical network which exhibits no growth or sampling
behavior; it is not a dataset of statistical interest. The edges are the units in all
other cases we consider, justifying our choice to label edges and assume that
network data E = {Ei}i∈N is obtained by sampling [n] ⊂ N and observing
E|[n] = {Ei}i∈[n] for a given sample size n ≥ 1.

8

HARRY CRANE AND WALTER DEMPSEY

Figure 3. Two edge labelings of the network data from Fig-
ure 1. An edge exchangeable model assigns equal probability
to both networks. Vertices are not labeled and, therefore, ver-
tex labels play no role in the exchangeability condition.

Given network data En = {Ei}i∈[n] and a permutation σ : [n] → [n],
n = {Eσ(i)}i∈[n] to denote the network data obtained from En by
we write Eσ
relabeling units according to σ. In terms of its representation as an edge
labeled graph, Eσ
n corresponds to relabeling edges according to σ. See Figure
2 for a visual illustration of how sampling and relabeling operations act on
the edge labeled network.
From now on, we use Y = {Yi}i∈N to denote a random edge labeled
network.

Deﬁnition 4.1 (Edge exchangeability). A random edge labeled network Y =
{Yi}i∈N is edge exchangeable if Yσ =D Y for all permutations σ : N → N, where
=D denotes equality in distribution.

Edge exchangeable models assign the same probability to all edge labeled
graphs that are equivalent up to relabeling, such as the two networks in
Figure 3. Any random edge labeled network {Yi}i∈N gives rise to a compatible
sequence of ﬁnite networks (Yn)n≥1 by taking each Yn = Y|[n] = {Yi}i∈[n] to be
the restriction by subsampling [n] ⊂ N deﬁned above. Any such sequence
n =D Yn for all permutations σ :
is inﬁnitely edge exchangeable, meaning Yσ
[n] → [n] and Yn|[m] =D Ym for all n ≥ m ≥ 1.
Our characterization of all edge exchangeable network models in Theo-
rem 8.2 explains why vertices cannot be labeled exchangeably in network
representations built from a growing sequence of representative samples
of interactions in a population. While all edges arrive according to an ex-
changeable process, the vertices arrive in size biased order weighted by the
relative frequency of their occurrence in interactions. The sample of vertices,
therefore, does not represent an exchangeable draw from the population. An
equally important practical matter, and in stark contrast to the conventional
setting of exchangeable networks with labeled vertices, edge exchangeable
models admit sparse and power law behavior.

EDGE EXCHANGEABLE MODELS FOR NETWORK DATA

9

5. The Hollywood model

Edge exchangeability abides by Principle (II) as a logical modeling prin-
ciple for network data arising from processes of interactions. As our next
model lays bare, edge exchangeability also satisﬁes Principle (I) with re-
spect to the asymptotic properties of sparsity and power law. With this, the
framework of edge exchangeability directly addresses the main questions
of Section 1 and provides a principled framework within which to develop
sound methods for network analysis. The speciﬁc model we introduce has
other nice practical properties, including a generative description which
makes estimation and prediction tractable and aﬀords a clear interpretation
to model parameters.

5.1. The Hollywood process. For concreteness, we phrase our model in the
context of the actors collaboration network, which exhibits the most ﬂexible
features of the network datasets we consider: the set of actors appearing in a
movie can be of any ﬁnite size; the same set can serve as the cast of multiple
movies; and the same actor can play multiple roles in a given movie, and
therefore appear more than once in a given edge. The Hollywood model
in (3) below accommodates all of these behaviors. It admits an intuitive
generative description, called the Hollywood process, whereby the network
grows by sequential addition of edges as follows.
Let ν = {νk}k≥1 be a probability distribution on the positive integers and
let (α, θ) satisfy either 0 < α < 1 and θ > −α or α < 0 and θ = −kα for some
k = 1, 2, . . .. We generate a sequence (Yn)n≥0 of edge labeled networks, where
each Yn has size(Yn) = e(Yn) = n ≥ 1, as follows.
We start with Y0 having v(Y0) = e(Y0) = 0. For n ≥ 1, every edge in Yn
corresponds to a movie, with the vertices incident to the edge labeled i in
Yn corresponding to the actors who play a role in movie i = 1, . . . , n. Given
Yn−1, for n ≥ 1, we choose the number of available roles in the next movie
independently according to Kn ∼ ν. Given Kn = k, we choose the k actors
in order of their prominence, ﬁrst ﬁlling the lead role, then the second lead
role, and so on until all k roles are ﬁlled. Let Nn(j) be the number of unique
actors seen in all the movies through the (j−1)st actor cast in movie n. (Thus,
Nn(1) is the number of unique actors appearing in movies 1, . . . , n − 1.) For
j = 1, . . . , k, we label the actors arbitrarily 1, . . . , Nn(j) and write Dn(i, j) to
denote the number of roles for which the actor labeled i has been cast up
to and including the (j − 1)st role of movie n. (Note that an actor may play
more than one role in a given movie.) The actor vn(j) cast in the jth lead role
of movie n is chosen randomly among the actors labeled 1, . . . , Nn(j) and a
previously unseen actor, labeled Nn(j) + 1, according to

(cid:40)

(2)

pr(vn(j) = i) ∝

Dn(i, j) − α,
θ + αNn(j),

i = 1, . . . , Nn(j),
i = Nn(j) + 1.

We continue to update according to (2) until all k roles of movie n have been
ﬁlled.

10

HARRY CRANE AND WALTER DEMPSEY

We write Yn as the edge labeled network obtained by orienting each
edge labeled j = 1, . . . , n by vj(1) → ··· → vj(Kj) and then removing vertex
labels. We call the sequence (Yn)n≥1 of networks built this way the Hollywood
process with parameter (α, θ, ν). By its sequential construction, the process
determines a family of distributions, called the Hollywood model, for network
data with inﬁnitely many edges labeled in N. The Hollywood model is
determined by the distributions of Yn, for each n ≥ 1, which we express in
closed form by
(3)

pr(Yn = E; α, θ, ν) =

νMk(E)
k

exp{Nk(E) log((1−α)

↑(k−1))},

(cid:89)

k≥1

 αv(E) (θ/α)

↑v(E)
θ↑mn(E)

∞(cid:89)

k=2

k≥1 kMk(E) is the total degree of E, and x

where E is any edge labeled network with n oriented edges, v(E) is the
mn(E) =(cid:80)
number of nonisolated vertices in E, (Nk(E))k≥0 gives the number of vertices
with degree k for each k ≥ 0, Mk(E) is the number of k-ary edges in E,
↑j = x(x + 1)··· (x + j − 1)
is the ascending factorial function.
The model with general ν is suited to the actors network, as there are
generally more than two actors involved in each movie, and the Enron
email network, in which multiple individuals can be involved in a single
email exchange. Though presented in the context of the actors network, the
Hollywood model is suited to any conceivable network dataset that arises
from a process of interactions. The special case of network data with binary
edges is easily handled by setting ν2 = 1. The closed form expression in (3)
makes parameter estimation straightforward, and the sequential description
in (2) facilitates predictive inference in a supervised learning setting. We
discuss each of these further in Section 6.

Crane [11] previously noted a connection between the Hollywood model
and the Ewens–Pitman two parameter family of distributions on set parti-
tions [14, 28]. The two models coincide in the unary setting of the Hollywood
model, that is, the (α, θ, ν) case when ν1 = 1. In this sense, the Hollywood
model may be viewed as a natural reﬁnement of the Ewens–Pitman dis-
tribution, which enjoys wide relevance throughout statistics, mathematics,
and applied science [12].

5.2. Interpretation of parameters and ﬁnite population model. The split
parameter space of the above model covers the case of bounded and un-
bounded population sizes. The region 0 < α < 1 and θ > −α gives rise to a
sequence (Yn)n≥1 with an unbounded number of vertices, that is, v(Yn) → ∞
almost surely as n → ∞, as is common in many datasets we encounter, for
example, the actors, Enron, and Wikipedia networks of Table 1. The Karate
Club dataset, on the other hand, is known to have a ﬁnite population of
thirty-four club members but no limit on the number of interactions between
each individual. The range α < 0 and θ = −kα accommodates this case

EDGE EXCHANGEABLE MODELS FOR NETWORK DATA

11

by describing an edge exchangeable sequence (Yn)n≥1 for which v(Yn) → k
almost surely.
From the update probabilities in (2), we see that α > 0 increases the prob-
ability of observing previously unseen vertices but decreases the probability
of observing a vertex again after its initial occurrence. The range α < 0
has the opposite eﬀect. Thus, α values near 1 make it more likely that new
edges involve previously unseen vertices, but less likely that previously
seen vertices occur in future edges. On the other hand, α < 0 corresponds
to a ﬁnite population size, so that each newly observed vertex decreases the
number of unseen vertices and increases the probability that future edges
involve previously seen vertices. In the 0 < α < 1 regime, larger values
of θ increase the probability of seeing previously unobserved vertices in
new edges, but the eﬀect of θ diminishes as n → ∞. In Section 5.3, we see
that 0 < α < 1 is directly related to the power law behavior of the sequence
(Yn)n≥1.

The speciﬁc dynamics of the Hollywood model elucidate the general
phenomenon of edge exchangeable networks borne out by the representation
in Theorem 8.2. Edges arrive exchangeably, but vertices arrive in a size-
biased manner according to their relative degree. This observation makes
rigorous our heuristic explanation in Section 1 for why vertices cannot be
labeled exchangeably in network data driven by a process of interactions in
an inﬁnite population.

5.3. Statistical properties of the Hollywood model. The ﬁnite sample dis-
tributions in (3) are edge exchangeable and consistent and, thus, determine
the unique distribution of an edge exchangeable population network Y with
inﬁnitely many labeled edges.
Theorem 5.1. Let (α, θ) satisfy either α < 0 and θ > −α or 0 < α < 1 and
θ = −kα for some k = 1, 2, . . ., and let ν = {νk}k≥1 be any distribution on the
positive integers. The sequence (Yn)n≥1 of edge labeled networks generated as in (2)
from the Hollywood model with parameter (α, θ, ν) is inﬁnitely edge exchangeable.
Theorem 5.2. Let (Yn)n≥1 obey the Hollywood process with parameter (α, θ, ν).
For each n ≥ 1, let pn(k) = Nk(Yn)/v(Yn) be the empirical degree distribution of Yn,
where Nk(Yn) is the number of vertices with degree k ≥ 1 and v(Yn) is the number
of vertices in Yn, respectively. Then, for every k ≥ 1,

pn(k) ∼ αk

−(α+1)/Γ(1 − α)

a.s. as n → ∞,

where Γ(t) =(cid:82) ∞
0 xt−1e

−xdx is the gamma function. That is, Y exhibits power law

degree distribution with exponent γ = 1 + α ∈ (1, 2).
Theorem 5.3. Let (Yn)n≥1 obey the Hollywood process with parameter (α, θ, ν) for
0 < α < 1 and θ > −α. Then the expected number of vertices in Yn satisﬁes
(4)

as n → ∞,

(µn)α

E(v(Yn)) ∼ Γ(θ + 1)
αΓ(θ + α)

12

where µ =(cid:80)

HARRY CRANE AND WALTER DEMPSEY

k≥1 kνk is the mean edge arity. Furthermore, if 1/µ < α < 1, then

(Yn)n≥1 is sparse almost surely.

Theorems 5.1, 5.2, and 5.3 address the major questions posed in Section 1
by establishing that the edge exchangeable framework admits a family of
models for networks that are sparse and exhibit power law degree distri-
bution. The power law behavior in the range 1 < γ < 2 of the Hollywood
model complements the behavior of the preferential attachment model, see
[8] and discussion in Section 7.1 below. Though previous authors [4] suggest
that γ > 2 is more prevalent in datasets that exhibit power law, recent work
demonstrates empirically that the range 1 < γ < 2 of the Hollywood model
is commonplace for networks derived from interaction processes [13].

5.4. Projecting to a network without multiple edges. Despite the natural
occurrence of multiple edges in real world interaction processes, most
network models are designed to handle only simple graphs, and many
network datasets record only a single edge to indicate the occurrence of
some positive number of interactions. Whether the focus on models for
simple networks is a consequence of this data storage convention or vice
versa, the fact that many network datasets are obtained by projecting a
graph with multiple edges is often ignored during data analysis; and the
signiﬁcance of this action on modeling and scientiﬁc conclusions is hardly
appreciated in the broader literature.
Given an interaction process E = {Ei}i∈N for a population P, we deﬁne
∗
the standard projection network as the hypergraph G
) with edges
E
(5)
that is, A ∈ E∗
deﬁne the (t, c)-projection network G
t({i ∈ N : Ei = A}) > c,
(6)
for some thresholding function t and cutoﬀ value c ≥ 0.
(The standard
projection in (5) corresponds to (6) with t equal to the cardinality map and
−1(A) has positive
c = 0 so that A is present in the projected graph as long as E
cardinality.)
Theorem 5.4 (Sparsity of the standard projection). Let (Yn)n≥1 obey the binary
Hollywood model with parameter (α, θ). For each n ≥ 1, let G
∗
n be the simple graph
obtained by applying the projection in (6) to Yn with any c ≥ 0 and t(A) = #A,
∗
n)n≥1 is sparse almost surely for all
the cardinality map. Then the sequence (G
0 < α < 1 and θ > −α.

A ∈ E∗
provided A occurs at least once in E. More generally, we can
A ∈ E(t,c)

= (P,E∗
#{i ∈ N : Ei = A} ≥ 1,
∗ = (P,E(t,c)) by putting

if and only if

if and only if

Theorem 5.4 elucidates the danger in needlessly projecting a network with
multiple edges: by Theorem 5.3, the network data produced from the binary
Hollywood model with parameter (α, θ) is sparse only for 1/2 < α < 1, while
the projected network after applying operation (6) is sparse for all 0 < α < 1.
Thus, if the data obeys the Hollywood model with 0 < α < 1/2, then a

EDGE EXCHANGEABLE MODELS FOR NETWORK DATA

13

Figure 4. Simulation results showing the degree distribution
of networks and their projection to a simple network by
removing multiple edges.
(A1) Network generated from
the binary Hollywood model with (α, θ) = (0.67, 1).
(B1)
Network generated from the binary Hollywood model with
parameters (α, θ) = (0.25, 1). (A2) Simple network obtained
by reducing multiple edges to single edge in (A1) network.
(B2) Simple network obtained by reducing multiple edges
to single edge in (B1) network. Results suggest that the
generated network and its standard projection both exhibit
power law of similar degree. The line with slope −γ in (A1)
and (B1) indicates the true power law based on Theorem 5.2.
The line with slope −γ in (A2) and (B2) is the conjectured
power law based on visual evidence and by connection to
Theorem 5.2.

test for sparsity based on the projected graph could lead to the specious
conclusion that the data generating process is sparse, when in fact it is not.
We discuss this further in Section 6.4.

On the other hand, Figure 4 suggests that the power law behavior of the
Hollywood model, as established in Theorem 5.2, might ﬁlter down to the
standard projection. Whether this phenomenon is real or perceived, the
observation lends some credence to the belief that the power law behavior
observed in many network datasets reﬂects a real phenomenon in nature.

012345−6−5−4−3−2log degreelog proportion*****************************************************************************************************************************************************g=1.67(A2)012345−11−9−8−7log degreelog proportion*****************************************************************************************************************************************************g=1.67(A1)0246−8−6−4−2log degreelog proportion**************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************g=1.25(B2)0246−10−8−6−4log degreelog proportion**************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************g=1.25(B1)14

HARRY CRANE AND WALTER DEMPSEY

Such a conjecture, however, is speculative, and there remains no logical
justiﬁcation for projecting interaction data to a simple graph, especially when
this operation makes the otherwise easy practice of parameter estimation
intractable.

6. Inference from edge exchangeable models

k=1

(7)

l(α, θ, ν; En) =

Mk(En) log νk + v(En) log(α) +

∞(cid:88)
− mn(En)(cid:88)

6.1. Maximum likelihood estimation. Given edge labeled network data
En with n edges, the log-likelihood l(α, θ, ν; En) of the Hollywood model with
parameter (α, θ, ν), where ν = {νk}k≥1 is an arbitrary distribution, satisﬁes
log(θ/α + j)−

v(En)−1(cid:88)
k−2(cid:88)
is the number of vertices in En with degree k, and mn(En) =(cid:80)∞
where Mk(En) is the number of edges in En with exactly k vertices, Nk(En)
k=1 kMk(En) is
the total degree of En.
Maximum likelihood estimation of ν returns the empirical distribution
ˆνMLE = {ˆνk}k≥1, where ˆνk = Mk(En)/n for each k = 1, 2, . . .. We then estimate
α and θ by iterating between the score functions

log(θ + j − 1) +

log(1 − α + j),

∞(cid:88)

Nk(En)

k=2

j=0

j=1

j=0

Nk(En)
1 − α + j

= 0

and

(8)

(9)

∂l(α, θ, ν; En)

∂α

∂l(α, θ, ν; En)

∂θ

=

v(En)−1(cid:88)

j=0

1/α

θ/α + j

+

= v(En)
α

v(En)−1(cid:88)

j=0

− n(cid:88)

k−2(cid:88)

k=2

j=0

1

θ + j

= 0.

−θ/α2
θ/α + j

− 2n−1(cid:88)

j=0

The distribution in (3), and therefore the log-likelihood in (7), applies to
the oriented multigraph generated by the model of Section 5.1. Let ˜Yn be the
undirected edge labeled multigraph obtained by removing the orientations
in Yn from the Hollywood model with parameter (α, θ, ν). Then for any
undirected multigraph ˜En with n edges,
(10)
pr( ˜Yn = ˜En; α, θ, ν) = C( ˜En)

 αv( ˜En) (θ/α)

↑v( ˜En)
θ↑mn( ˜En)

(cid:89)

∞(cid:89)

νMk( ˜En)
k

k≥1

exp{Nk( ˜En) log((1−α)

↑(k−1))},

k=2

where C( ˜En) is a combinatorial factor counting the number distinct ways to
orient the edges of ˜En to obtain a directed multigraph. Since the suﬃcient
statistics in (10) are unchanged by reassigning an arbitrary orientation to
each edge of ˜Yn, the log-likelihood based on ˜Yn = ˜En satisﬁes

˜l(α, θ, ν; ˜En) = l(α, θ, ν; En) + log C( ˜En),

EDGE EXCHANGEABLE MODELS FOR NETWORK DATA

15

Maximum likelihood estimates

ˆαMLE
network
Wikipedia
0.37
Karate Club -1.80

std. error

0.01
0.47

ˆθMLE
183
61.20

std. error

14.76
15.98

Table 2. Maximum likelihood estimates and standard errors
for (α, θ) from binary Hollywood model ﬁt to Wikipedia
voting and Karate Club networks.

where l(α, θ, ν; En) is the log-likelihood from (7) with En taken to be any
oriented edge labeled network whose edges agree with ˜En. Thus, the score
functions based on ˜En are just as in (8) and (9) and maximum likelihood
estimation for (α, θ, ν) can be performed without issue.

Problems arise, however, when projecting multiple edges according to
∗
the operation in (6). For deﬁniteness, suppose G
n has n edges and is the
standard projection of a multigraph from the binary Hollywood model with
parameter (α, θ). The log-likelihood l
exp{l
∗

∗
(α, θ, ν; G
n) in this case satisﬁes

∗

exp{Nk(E) log((1 − α)

↑(k−1))}

exp{Nk(G

n) log(1 − α)
∗

↑(k−1)} ×

=

n)} =
∗
(α, θ, ν; G
νMk(E)
k

=

k≥1

E≥G∗

(cid:89)
(cid:88)
(cid:89)
×(cid:88)

k≥1

n

νMk(G
k

∗
n)

k=2

 αv(E) (θ/α)
∞(cid:89)
↑v(E)
 αv(G
θ↑mn(E)
∞(cid:89)
↑v(G
∗
∗
n)(θ/α)
n)
(cid:81)
θ↑mn(G∗
n)
k≥1 νMk(E)−Mk(G
(cid:89)
(θ + mn(G∗

 αv(G

∗
νMk(G
n)
k

k=2
∗
n)

n

k

(G

E≥G∗

∞(cid:89)

∗
n; α, θ, ν)

n)) exp{(Nk(E) − Nk(G
n)) log(1 − α)
∗
n))↑(mn(E)−mn(G∗
↑v(G
∗
∗
exp{Nk(G
n)(θ/α)
n)
θ↑mn(G∗
n)

n) log(1 − α)
∗
∗
= C
where E ≥ G
∗
∗
n indicates that E is a multigraph which projects to G
n un-
der the standard projection in (5). Without a manageable expression for
∗
∗
n; α, θ, ν), maximum likelihood estimation based on projected network
C
(G
∗
data G
n is intractable. In concert with Theorem 5.4, we see that projecting
not only promotes spurious or misleading conclusions by contorting the
network structure but also obstructs standard inference procedures.

k≥1

k=2

↑(k−1)}

↑(k−1)},

6.2. Application to Wikipedia voting and Karate Club networks. Table 2
shows the maximum likelihood estimates for the binary Hollywood model
ﬁt to the Wikipedia voting and Karate Club networks. We choose these
networks as examples because together they cover both regimes in the
parameter space of the Hollywood model. The Karate Club network consists
of interactions among thirty-four club members, warranting the choice of

16

HARRY CRANE AND WALTER DEMPSEY

k ≥ 1,

k = 34 when ﬁtting the model with parameters α < 0 and θ = −34α. The
Wikipedia network, by contrast, has no upper limit on the number of vertices,
and so we ﬁt the model under regime 0 < α < 1 and θ > −α.

pr(K = k; γ) = (γ − 1)Γ(γ)/Γ(k + γ),

The large standard error for maximum likelihood estimates of θ agrees
with what is known about estimation of the mutation rate in Ewens’s
sampling formula [12, 14]: although ˆθMLE → θ almost surely as the sample
size grows, it converges at a rate on the order of log(n), rendering it practically
inconsistent. The estimate of α is of greater interest in the applications we
envision because of its relationship to the power law behavior, cf. Theorem
5.2. The maximum likelihood estimate ˆαMLE = 0.37 for the Wikipedia dataset
is supported by an estimated power law exponent of ˆγYULE = 1.44 for the
degree distribution of the network ﬁt to the Yule distribution,
(11)
for γ > 1. By Theorem 5.2 the two parameter model exhibits power law
with estimated exponent ˆγMLE = 1 + ˆαMLE = 1.37.
6.3. Prediction using growth dynamics. The growth dynamics of edge
exchangeable models by a sequential process of edge addition facilitates
predictive inferences in networks generated by a process of repeated in-
teractions in a population. In the setting of Section 5, we can predict the
next interaction, based on network data Yn = E of size n, from the update
probabilities in (2) with (α, θ, ν) = ( ˆαMLE, ˆθMLE, ˆνMLE) given by the maximum
likelihood estimates obtained from (7).
For a concrete application, we consider the actors collaboration network
from [4]. Each i ∈ N corresponds to the ﬁnite subset Ei ⊂ P of actors
who play a role in the corresponding movie. For (α, θ, ν) in the parameter
space of the Hollywood model, we compute predictive probabilities for the
next movie analogously to the update probabilities in (2). For the speciﬁc
question of whether the next movie includes at least one never previously
seen actor, a straightforward calculation based on (2) and the law of total
probability gives

pr(new actor in next movie | Yn; α, θ, ν) = 1 −(cid:88)
k≥1 Nk(Yn) is the number of actors in Yn and M =(cid:80)

↑k/(θ + M)

k≥1 kNk(Yn)
is the total number of roles in the dataset Yn. (To see the calculation in (12),
we note that (M − αN)
↑k is the probability that the next movie
does not feature a new actor given that it has k actors in its cast. Summing
over all k ≥ 1 gives the total probability that the next movie does feature a
new actor, yielding the rightmost term in (12). The probability that a new
actor appears follows by taking the complementary probability.)
Fitting the model to the actors collaboration dataset yields ˆαMLE = 0.66
(std. error 6.8× 10
−4) and ˆθMLE = 4.21 (std. error 2.86), with ˆνMLE as recorded
in Table 3. The estimated predictive probability based on these maximum

(12)

where N =(cid:80)

(M − αN)
↑k
(θ + M)↑k

,

νk

k≥1

EDGE EXCHANGEABLE MODELS FOR NETWORK DATA

17

Maximum likelihood estimates

ˆνk

k
7
8
9

ˆνk
k
0.057
1 0.081
0.054
2 0.071
0.046
3 0.065
4 0.072 10 0.275
5 0.062 11 0.161
6 0.059

Table 3. Maximum likelihood estimates of the distribution
of movie sizes for the actors collaboration network. The
mean number of actors in each movie is 7.136.

likelihood estimates is 0.78. By Theorem 5.3, ˆαMLE = 0.66 lies in the range
1/ν < α < 1, where µ = 7.136 is computed from Table 3, suggesting that the
actors collaboration network is sparse.

We check the accuracy of this predictive probability by data splitting cross
validation based on samples of 2, 000 movies of the total sample of about
200, 000. For each iteration we compare the estimated probability in (12) to
the empirical probability obtained as the proportion of unsampled movies
for which there appears an actor not among the 2, 000 sampled movies.
The mean relative error between (12) and the empirical probability for 100
iterations was −0.003 with a standard deviation of 0.002.

6.4. Tests for sparsity and power law. As asymptotic network properties,
sparsity and power law cannot be veriﬁed with certainty based on any ﬁnite
amount of data. Statistical tests for sparsity and/or power law based on
ﬁnite sample data, therefore, require that the ﬁnite sample models faith-
fully represent the properties exhibited by the inﬁnite population network,
typically in the form of a small number of interpretable parameters. Any
such test requires a framework that admits models for both sparse and
non-sparse networks, as the class of edge exchangeable does.

Our discussion in Section 5.4, in particular Theorem 5.4, advises caution
when testing for asymptotic properties based on projected network data. If
the parameter space Π partitions as Π = Π0 ∪ Π1 such that π ∈ Π1 param-
eterizes sparse networks and π ∈ Π0 parameterizes non-sparse networks,
then testing H0 : π ∈ Π0 versus H1 : π ∈ Π1 might yield a valid test for
sparsity. Alternative interpretations of the partition Π = Π0 ∪ Π1, however,
may provide a more parsimonious conclusion. For example, under the
binary Hollywood model of Section 5, the region 1/2 < α < 1 corresponds
to sparse networks while α < 1/2 parameterizes dense networks, with α < 0
corresponding to the case where the number of edges grows to inﬁnity
while the number of vertices stays bounded and ﬁnite. Theorem 5.4 estab-
lishes that if the network is projected to a simple graph using the standard

18

HARRY CRANE AND WALTER DEMPSEY

projection in (5), however, the projected network is sparse a.s. for all α > 0.
This observation points out the importance of distinguishing between the
interaction process as in Deﬁnition 2.1 and its associated projection to a
simple graph.

A particular illustration of this is on display when testing sparsity in the
US airport dataset [25], which is built from the ﬂight map between all US
airports in 2010. For each pair of airports i and j, there are as many edges as
there were seats on all ﬂights scheduled between those airports. The edge
weights are very large and range over several orders of magnitude from a
minimum of 1 to several hundred thousand, making a projection operation
as in (5) particularly deleterious to the structure in the data. For the binary
Hollywood model ﬁt this dataset, we obtain ˆαMLE = 0.13 (std. error 0.003)
and ˆθMLE = 0.08 (std. error 0.134), which is supported by ˆαYULE = 0.11
estimated from the Yule distribution (11) ﬁt to the degree distribution. We
cannot reject the hypothesis H0 : α < 1/2; however, if testing for sparsity
based on the projected network, ˆαMLE falls in the range 0 < α < 1/2 for
which the network with weights is not sparse but the standard projection
from (5) is, per Theorem 5.4. We note that the analysis in [7, Section 7.2]
concludes that the US airport network is sparse based on a 99% credible
interval of [0.099, 0.181] for an analogous parameter to α in our case, but
their analysis appears to be based on the standard projection.

7. Discussion of other approaches

The edge exchangeable framework addresses a longstanding problem of
statistical inference from network data in which edges act as the statistical
units. We conclude with a brief discussion of prior attempts to model sparse
networks by Barab´asi and Albert [4], Bickel and Chen [5], and Caron and
Fox [7].

7.1. Preferential attachment models. Barab´asi and Albert’s [4] preferential
attachment model [4] describes a generating mechanism for vertex labeled
networks with power law degree distribution of exponent γ > 2. Propo-
nents of the preferential attachment model [4, 8] cite not only its power
law behavior but also its vertex growth dynamics as favorable theoretical
properties. From a statistical point of view, preferential attachment models
are far less attractive: they not only lack exchangeability with respect to rela-
beling vertices but also fail to satisfy the more fundamental property of label
equivariance, meaning that the model, as a set of probability distributions, is
not preserved under arbitrary relabeling of the data. Furthermore, as Table
1 makes plain, the vertex growth dynamics, though perhaps reasonable
for modeling the network associated to the router-level Internet and other
physical network structures, do not accurately reﬂect the dynamics of many
network datasets.

EDGE EXCHANGEABLE MODELS FOR NETWORK DATA

19

(cid:82)

[0,1]×[0,1]

7.2. Bickel and Chen’s decoupled Aldous–Hoover models. Bickel and
Chen [5] modify the Aldous–Hoover theory [3, 16] of exchangeable vertex
labeled graphs in order to enforce sparsity in a class of models which lacks
sampling consistency. With φ : [0, 1] × [0, 1] → [0, 1] symmetric in its
arguments and {ρn}n≥1 satisfying ρn → ∞ and ρ−1
φ(u, v)du dv =
n n
n = ([n],E∗
O(1) as n → ∞, they deﬁne the distribution of a graph G
∗
n) with
vertices labeled in [n] by taking U1, . . . , Un i.i.d. Uniform[0, 1] and putting
(13)
conditionally independently for all 1 ≤ i ≤ j ≤ n. Under (13), every Gn is
exchangeable and collectively the sequence (Gn)n=1,2,... is sparse in the sense
of Deﬁnition 3.1, but the formulation does not describe a valid generating
process for a population network since the marginal distributions in (13) are
not mutually consistent with respect to subsampling vertices.

n | U1, . . . , Un} = ρ−1

1 ≤ i < j ≤ n,

P{{i, j} ∈ E∗

n φ(Ui, Uj),

By abandoning sampling consistency, Bickel and Chen sacriﬁce Principle
(II) by using a model for which the meaning of the parameter changes with
the sample size and, relatedly, is not anchored to any parameter describing
the distribution of a population network. As such, the meaning of the
parameter φ varies with the sample size, obscuring the interpretation of es-
timated parameters and calling into question the signiﬁcance of consistency
results, as in [6, 32], for ﬁnite sample estimation.

(cid:48)

(cid:48)

7.3. Caron and Fox’s completely random measure approach. Caron and
Fox [7], and later Veitch and Roy [30], propose to associate network data with
a class of exchangeable point processes X on [0,∞) × [0,∞). In this setting,
) ∈ [0,∞) × [0,∞)
the vertices are labeled in [0,∞) and a point occurs at (x, x
if there is an edge between vertices labeled x, x
in the graph. The point
process X is assumed to be exchangeable in the sense that its distribution is
invariant under measure preserving transformations of [0,∞) × [0,∞).
In this context, a sequence of network data is obtained by deﬁning a graph
Gt for each t ≥ 0, where Gt is derived from the restriction of X to [0, t]× [0, t]
by only including in Gt those vertices that are labeled in [0, t] and which
are not isolated in the restriction. Though presented as a way to obtain
sparsity in the context of an exchangeable data generating mechanism, the
exchangeability here refers to an abstract spatial representation of networks,
and not a logical invariance of the model with respect to relabeling units
in accordance with Principle (II). Therefore, although these models admit
sparse graph sequences (Gt)t≥0 as t ↑ ∞, the framework does not address
the fundamental statistical questions of Section 1 because exchangeability
of the point process does not obviously endow the graph sequence (Gt)t≥0
with any natural invariance properties for statistical inference.

8. Appendix

8.1. Representation theorem. The class of edge exchangeable network
models elicits a de Finetti-type representation theorem, which we state

20

HARRY CRANE AND WALTER DEMPSEY

Figure 5. Illustration of the generic process of generating
edge exchangeable networks in Section 8.1.
(a) Edge ex-
changeable network Y(X1, . . . , X6) corresponding to random
sequence X1 = {2, 4}, X2 = {1, 2}, X3 = {1, 5}, X4 = {6, 9},
X5 = {2, 6}, X6 = {2, 6}. (b) Edge labeled network obtained by
removing vertex labels from the vertex-edge labeled network
in Panel (a). (c) Canonical vertex labeling of the network in
Panel (b).

here in the special case of edge exchangeable networks with binary edges.
We write ﬁn2(P) ⊂ ﬁn(P) to denote the set of all size 2 multisets of P, so
that E = {Ei}i∈N has #Ei = 2 for every i ∈ N. The more general representa-
tion follows by a technically involved, but straightforward, analogy. The
representation theorem characterizes the structure of all edge exchangeable
models and establishes a general generative scheme for all edge exchange-
able networks.
Given a binary edge labeled network E = {Ei}i∈N, we call S : [n] → ﬁn2(N)
a selection function for E if the vertex-edge labeled network GS = (N,ES)
coincides with E upon removing vertex labels, where GS = (N,ES) is the
multigraph with vertices labeled by N and multiple edges between i and j
with labels in S

−1({i, j}) for every i, j ∈ N.

: [n] → ﬁn2(N) are equivalent, written S (cid:27) S
(cid:48)

(cid:48) (cid:27) S} with the edge labeled graph it represents.

(cid:48)
Selection functions S, S
, if
they represent the same edge labeled network. For any selection function
S : [n] → ﬁn2(N), we identify the equivalence class S/ (cid:27) = {S
: [n] →
(cid:48)
ﬁn2(N) : S
We associate every edge labeled graph E with n edges a canonical selection
function SE : [n] → ﬁn2(N) deﬁned as follows. We initialize by putting
SE(1) = {1, 1} if the edge labeled 1 is a self loop and otherwise SE(1) = {1, 2}.
Given SE(1), . . . , SE(i−1), we deﬁne SE(i) = {v1(i), v2(i)} for v1(i) ≤ v2(i) chosen
to be the smallest vertex labels consistent with the structure of E|[i]. See
j ≥ i ≥ 0 and(cid:80)
Figures 5(b) and 5(c) for an illustration.
The ﬁn2(N)-simplex consists of all ( f{i,j})j≥i≥0 such that f{i,j} ≥ 0 for all
j≥i≥0 f{i,j} = 1. For any f = ( f{i,j})j≥i≥0 in the ﬁn2(N)-simplex

EDGE EXCHANGEABLE MODELS FOR NETWORK DATA

21

and i ∈ N, we deﬁne

f (i)• =

∞(cid:88)

j=0

f{i,j}

as the sum of masses involving element i.

j ≥ i ≥ 0.

P{Xk = {i, j} | f} = f{i,j},

Every f = ( f{i,j})j≥i≥0 in the ﬁn2(N)-simplex determines a probability
distribution on edge labeled graphs, denoted  f , as follows. Let X1, X2, . . .
be i.i.d. random pairs {i, j} in N with
(14)
Given X = (X1, X2, . . .), we deﬁne the selection function SX : N → ﬁn2(Z),
where Z = {. . . ,−1, 0, 1, . . .}, as follows. We initialize with m0 = 0. For n ≥ 1,
suppose mn = z ≤ 0. If Xn contains no 0s, then we deﬁne SX(n) = Xn and
update mn = mn−1. Otherwise, if Xn = {0, j} for some j ≥ 1, then we put
SX(n) = {z − 1, j} and update mn = z − 1; and if Xn = {0, 0}, then we put
SX(n) = {z− 1, z− 2} and update mn = z− 2. We deﬁne Y = Y(X1, X2, . . .) ∼  f
to be the edge labeled graph given by Y = SX/ (cid:27). See Figure 5(a) for an
illustration of this procedure.
Proposition 8.1. The edge labeled graph Y = Y(X1, X2, . . .) generated by X1, X2, . . .
i.i.d. from (14) is inﬁnitely edge exchangeable for all f in the ﬁn2(N)-simplex.

For identiﬁability, we deﬁne the ranked reordering of f by f

↓
{i,j})j≥i≥0,
↓
{0,0} = f{0,0} and re-
for all i ≥ 1 and then breaking ties f (i)• =
by declaring that ( f{i,i}, f{i,i+2}, . . .) comes before ( f{i+1,i+1}, f{i+1,i+2}, . . .)
to denote the space of rank

the element of the ﬁn2(N)-simplex obtained by putting f
labeling elements so that f (i)• ≥ f (i+1)
f (i+1)
•
in the lexicographic ordering. We write F ↓
reordered elements of the ﬁn2(N)-simplex.

As the vertex labels are inconsequential, it is clear that  f and  f(cid:48) determine
the same distribution for any f, f
. For any edge labeled
graph E, we write |E|↓ ∈ F ↓
to denote its signature, if it exists, as follows.
Let SE : N → N × N be the canonical selection function for Y. For every
{i, j} ∈ ﬁn2(N), j ≥ i ≥ 1, we deﬁne

for which f

↓ = ( f

↓ = f

(cid:48)↓

•

(cid:48)

n(cid:88)
n(cid:88)

k=1

−1

−1

f{i,j}(E) = lim

n→∞ n

f (i)• (E) = lim

n→∞ n

1{SE(k) = {i, j}},

1{i ∈ SE(k)},

we deﬁne |E| = ( f{i,j}(E))j≥i≥0 by putting f{0,i}(E) = f (i)• (E) −(cid:80)
and f{0,0}(E) = 1 −(cid:80)
j≥i≥0:(i,j)(cid:44)(0,0) f{i,j}(E) and we put |E|↓ = ( f{i,j})
↓
j≥i≥0.

if the limits exist. Provided each of the above limiting frequencies exists,
j≥1 f{i,j}(E), i ≥ 1,

k=1

22

HARRY CRANE AND WALTER DEMPSEY

Theorem 8.2. Let Y = {Yi}i∈N be an edge exchangeable random graph. Then there
exists a unique probability measure φ on F ↓

such that Y ∼ φ, where

(cid:90)

(15)

φ(·) =

 f (·)φ(d f ).

F ↓

That is, every inﬁnitely edge exchangeable graph Y can be generated by ﬁrst
sampling f ∼ φ and, given f , putting Y = Y(X1, X2, . . .) for X1, X2, . . . i.i.d. from
(14).
8.2. Proof of main theorems.
8.2.1. Proof of Theorem 5.1. Let (Yn)n∈N be the sequence of random edge
labeled networks from the Hollywood process with parameter (α, θ, ν) as in
Section 5.1. Then the distribution of Yn is given by (3) for each n ∈ N, and
these distributions are consistent with respect to the restriction operation
by consequence of the sequential description in (2). The distribution in (3)
depends on Yn only through v(Yn), e(Yn), Mk(Yn), and d(Yn), that is, the
number of vertices, number of edges, number of k-ary edges for each k ≥ 1,
and empirical degree distribution, all of which are invariant with respect to
relabeling edges. Inﬁnite edge exchangeability follows.
8.2.2. Proof of Theorem 5.2. Let (Yn)n≥1 follow the Hollywood process with
parameter (α, θ, ν). The power law behavior is apparent by the connection
between (Yn)n≥1 and the two parameter Ewens–Pitman distribution with
parameter (α, θ); see [28] for more details. In particular, the Ewens–Pitman
distribution corresponds to the special case of the unary Hollywood model,
that is, the model with 0 < α < 1, θ > −α, and ν1 = 1. In this case, there is
exactly one vertex incident to every edge and the network data collapses
to a partition of the edges. The number of blocks in the Ewens–Pitman
partition corresponds to the number of vertices in the unary Hollywood
process. Let Nk(Yn) be the number of vertices with degree k in Yn and let
v(Yn) be the total number of vertices in Yn. By Lemma 3.11 in [28], we know
that Nk(Yn)/v(Yn) → α(1− α)
↑(k−1)/k! a.s. for every k ≥ 1 as n → ∞. Thus, the
sequence of degree distributions {dn}n≥1 = {(Nk(Yn)/v(Yn))k≥1}n≥1 converges
a.s. to the distribution given by pα(k) = α(1 − α)
by {Kr}r≥1 for Kr = (cid:80)r
In the general Hollywood process with arbitrary distribution ν, the degree
sequences of (Yn)n≥1 correspond to a random subsequence of{dn}n≥1 indexed
κj, r = 1, 2, . . ., where κ1, κ2, . . . are i.i.d. from
ν. Thus, the degree distributions of (Yn)n≥1 correspond to {dKr
}r≥1, which
is a subsequence of the a.s. converging sequence {dn}n≥1 and, therefore,
must have the same a.s. limit. The proof is complete by noting that α(1 −
↑(k−1)/k! ∼ αk
α)
8.2.3. Proof of Theorem 5.3. We once again exploit the connection to the
Ewens–Pitman distribution from the proof of Theorem 5.2. Let Nn be
the number of vertices in the unary Hollywood process with parameter
(α, θ) satisfying 0 < α < 1 and θ > −α. Theorem 3.8 of [28] implies that

−(α+1)/Γ(1 − α) as k → ∞.

↑(k−1)/k!.

j=1

EDGE EXCHANGEABLE MODELS FOR NETWORK DATA

23

−αNn → Sα a.s., where Sα is a strictly positive and ﬁnite random variable.
n
The sequence (Nn)n≥1 then satisﬁes Nn ∼ nαSα a.s. as n → ∞, implying

E(Nn) ∼ Γ(θ + 1)
αΓ(θ + α)

nα

as n → ∞.

Let (Yn)n≥1 be the Hollywood process with parameter (α, θ, ν) for arbitrary
distribution ν. Then (v(Yn))n≥1 is a random subsequence of (Nn)n≥1 given
κj for κ1, κ2, . . . i.i.d. from ν. It follows from
nSα a.s. as n → ∞. By the strong
n ∼ (µn)α a.s. as n → ∞, where

by (NKr)r≥1, where Kr =(cid:80)r
µ =(cid:80)

the above argument that v(Yn) = NKn
law of large numbers, n

∼ Kα
−1Kn ∼ µ a.s. and Kα

j=1

k≥1 kνk; whence,

E(v(Yn)) ∼ Γ(θ + 1)
αΓ(θ + α)

(µn)α

as n → ∞.

To establish sparsity of (Yn)n≥1, we consider (n

−1v(Yn)m•(Yn))n≥1, where
m•(Yn) is the average total degree in Yn. We must identify values of 0 <
−1v(Yn)m•(Yn) = +∞. By the strong law of
α < 1 for which lim infn→∞ n
large numbers, m•(Yn) → µ a.s. as n → ∞. By the above discussion,
v(Yn) ∼ (µn)αSα a.s. as n → ∞ for a strictly positive and ﬁnite random
−1(µn)µαSα a.s. as n → ∞,
variable Sα.
which goes to inﬁnity as long as µα > 1. It follows that (Yn)n≥1 is sparse
with probability 1 provided 1/µ < α < 1, for all θ > −α.

−1v(Yn)m•(Yn) ∼ n

It follows that n

8.2.4. Proof of Theorem 5.4. Let (Yn)n≥1 be the binary Hollywood process
with parameter 0 < α < 1 and θ > −α. For each n ≥ 1, let G
∗
n be the simple
graph obtained by applying the standard projection (5) to Yn. To establish
sparsity of (G

∗
n)n≥1, we must show

∗
e(G
n)
v(G∗
n)2

lim sup
n→∞

= 0

a.s.

n

∗
n) when G(t,c)
) = v(G
n

) ≤ e(G
∗
n) and v(G(t,c)
n

It suﬃces to establish sparsity under the standard projection in (5), since
e(G(t,c)
is the projection obtained by
applying the cardinality map t(A) = #A and any threshold c ≥ 0 as in (6).
We employ the same notation as we have throughout the paper. Specif-
ically, for each k ≥ 1 and n ≥ 1, we write Nk(Yn) and Nk(G
∗
n) to denote the
∗
number of vertices with degree k in Yn and G
n, respectively. We also write
∗
∗
e(Yn) and e(G
n) to denote the number of edges in Yn and G
n, respectively,
∗
∗
and v(Yn) = v(G
n) to denote the number of vertices in Yn and G
n. Since
the projection operation reduces multiple occurrences of the same edge to a
∗
∗
single edge, the degree of each vertex in G
n can be no larger than v(G
n) and
(k ∧ v(G
∗
n))Nk(Yn).

n) ≤(cid:88)

∗
n) =
e(G

∗
kNk(G

(16)

(cid:88)

k≥1

k≥1

24

HARRY CRANE AND WALTER DEMPSEY

k=1

k=1

∗
n)

∗
n)

∞(cid:88)

lim sup
n→∞

Nk(Yn)
v(G∗
n)

Nk(Yn)
v(G∗
n)

+ lim sup
n→∞

∗
e(G
n)
v(G∗
n)2

K(cid:88)
K(cid:88)

For every K ≥ 1, (16) implies
≤ lim sup
n→∞
≤ lim sup
K(cid:88)
n→∞

k ∧ v(G
v(G∗
n)
k ∧ v(G
v(G∗
n)
k ∧ v(G
∗
n)
v(G∗
n)
n) → Sα a.s., where Sα is a strictly
−αv(G
∗
Corollary 3.9 of [28] implies that n
n) → ∞ a.s. and lim supn→∞(k ∧
∗
positive, ﬁnite random variable; thus, v(G
∞(cid:88)
n) → 0 a.s. for every k = 1, . . . , K, so that (17) implies
∗
∗
n))/v(G
v(G
for all K ≥ 1.
lim sup
n→∞

k=K+1
Nk(Yn)
v(G∗
n)
Nk(Yn)
v(G∗
n)

≤ lim sup
n→∞

∞(cid:88)
∞(cid:88)

+ lim sup
n→∞

+ lim sup
n→∞

Nk(Yn)
v(G∗
n)

lim sup
n→∞

k=K+1

k=K+1

(17)

(18)

∗
e(G
n)
v(G∗
n)2
By [28, Lemma 3.11],

k=K+1

≤

k=1

.

lim
n→∞

Nk(Yn)
v(G∗
n)

= pα(k) = αΓ(k − α)/(k!Γ(1 − α))

for every k ≥ 1 a.s.,

which implies that for every ε, δ > 0 and k ≥ 1 there exists R = R(ε, δ, k) such
that
For any K ≥ 1, we choose R
pr(|v(G
and

pr(|v(G
∗
n)
−1Nk(Yn) − pα(k)| < ε/K for all n > R
∞(cid:88)

−1Nk(Yn) − pα(k)| < ε for all n > R) ≥ 1 − δ.
∗ = max1≤k≤K R(ε/K, δ/K, k) so that

∗, for all k = 1, . . . , K) ≥ 1 − δ

We combine this with (18) and the tail calculation pα(> K) =(cid:80)∞

pα(k)| < ε for all n > R
∗

−1Nk(Yn) −
∗
v(G
n)

) ≥ 1 − δ.

∞(cid:88)

pr(|

k=K+1

k=K+1

∗
n)

k=K+1 pα(k) =

Γ(K + 1 − α)/(Γ(K + 1)Γ(1 − α)) to observe

for every K ≥ 1. Since pα(> K) ↓ 0 as K → ∞, it follows that

pr(lim sup

∗
n)/v(G
n→∞ e(G

n)2 > ε) ≤ δ
∗

for all ε, δ > 0

n)2 > pα(> K) + ε) ≤
∗
∗
∞(cid:88)
n)/v(G

Nk(Yn)/v(G

∗
n) > pα(> K) + ε)

∞(cid:88)

k=K+1

∗
v(G
n)

−1Nk(Yn) −

k=K+1

k=K+1

n→∞ e(G
pr(lim sup
≤ pr(lim sup
∞(cid:88)
n→∞

≤ pr(|
≤ δ

pα(k)| > ε for some n > R
∗

)

EDGE EXCHANGEABLE MODELS FOR NETWORK DATA

25

and (G

∗
n)n≥1 is sparse a.s.

8.2.5. Proof of Theorem 8.2. We prove Theorem 8.2 for binary edge exchange-
able graphs. To avoid technical issues, we write EN to denote the space
of edge labeled networks with edges labeled in N. We equip EN and F ↓
with their respective Borel σ-ﬁelds deﬁned with respect to the usual product
topology on either space.
Let S : N → ﬁn2(N) be a selection function for an edge exchangeable ran-
dom graph Y = (Yn)n≥1. By edge exchangeability, (S(n))n∈N is an exchange-
able ﬁn2(N)-valued sequence and, therefore, de Finetti’s theorem implies
the existence of a unique probability measure π on the space P(ﬁn2(N)) of
probability measures on ﬁn2(N) such that

(19)

∞

(·) denotes the inﬁnite product measure induced by m. By the

where m
strong law of large numbers and (19),

(cid:90)

P{(S(n))n∈N ∈ ·} =
n(cid:88)
n(cid:88)

f{i,j}(S) = lim

n→∞ n

−1

r=1

−1

∞
m

(·)π(dm),

P(ﬁn2(N))

1{S(r) = {i, j}} and

1{i ∈ S(r)}

r=1

f (i)• (S) = lim

n→∞ n

exist almost surely for all j ≥ i ≥ 1. We can deﬁne f{0,i} = f (i)• −(cid:80)∞
f00 = 1−(cid:80)

j≥i≥0:(i,j)(cid:44)(0,0) f{i,j} so that |S|↓ = f
: N → ﬁn2(N) be any other selection function for Y. Then S

with probability 1.
implies that there exists an injection ρ : N → N such that ρS
therefore,

j=1 f{i,j} and
↓
{i,j}(S))j≥i≥0 is well deﬁned
(cid:48) (cid:27) S
(cid:48) = S and,

(cid:48)
Let S

(S) = ( f

↓

) = f{ρ(i)ρ(j)}(S)
) = f{0,ρ(i)}(S)

for all j ≥ i ≥ 1
for all i ≥ 1.

and

(cid:48)
f{i,j}(S
(cid:48)
f{0,i}(S
(cid:48)|↓ = |S|↓

It follows that |S
deﬁned, where SY is the canonical selection function for Y.

with probability 1 and, thus, |Y|↓ = f

↓
By de Finetti’s theorem and the equivalence of f
(S) for all selection
↓
(Y) = ( f{i,j})j≥i≥0 = f
functions of Y, the conditional distribution of Y given f
exists for P-almost every E ∈ EN,
is  f as deﬁned in Section 8.1. Since |E|↓
we write φ(·) = P{|Y|↓ ∈ ·} and see that

↓

(SY) is well

(cid:90)

P{Y ∈ ·} =

|y|↓(·)P{Y ∈ dy} =

 f (·)φ(d f )

F ↓

by the change of variables formula for measures.

(cid:90)

EN

26

HARRY CRANE AND WALTER DEMPSEY

8.3. Description of network datasets. Description and references for net-
work datasets cited in Table 1.

• Actors [4]: Network built from collaborations among actors in a
given sample of movies. Each edge connects the actors who played
a role in the corresponding movie.
• Enron [18]: Network built from a corpus of about 500,000 emails.
Vertices are employees in the Enron Corporation with an undirected
edge between vertices i and j if there was at least one email exchanged
between the two in the corpus.
• Karate Club [33]: Network built from social interactions among 34
members of a karate club. Vertices are the members of the club and
an edge between i and j corresponds to a social interaction between
the two. The network exhibits no vertex sampling or growth since it
is assumed all club members have been observed.
• Wikipedia [21]: The Wikipedia voting network represents voting
behavior for elections to the administrator role in Wikipedia. Vertices
are Wikipedia users and a directed edge points from i to j if user i
voted for user j.
• US Airport [9]: Network built from the ﬂight map between all US
airports in 2010. A directed edge from i to j indicates that a ﬂight
was scheduled from airport i to airport j in 2002. Edges are weighted
by the number of seats on the scheduled ﬂights. The network grows
as a consequence of additional ﬂights between airports. There is no
vertex sampling or growth since the network is complete.
• Co-authorship [24]: Network built from co-authorship of preprints
on the Condensed Matter section of arXiv between 1995 and 1999.
Vertices are of two types, authors and papers, and edges only exist
between vertices of a diﬀerent type. An edge between i (author)
and j (paper) indicates that i is an author on paper j. The data is
more succinctly represented by a network as in Deﬁnition 2.1 by
associating each paper to an edge that connects all its authors.
• UC Irvine [26]: Network built from UC Irvine online community.
Vertices are active members of the community and a directed edge
from i to j indicates that a message was sent from user i to user j.
• Facebook [22]: Network built from ‘friends lists’ on Facebook. Ver-
tices are Facebook accounts with an undirected edge between ver-
tices i and j if the friendship between i and j is recorded in one
of the observed friends lists. The sample is obtained by sampling
the ‘ego-networks’ of a small sample of Facebook accounts, that is,
the observed individuals are obtained by ﬁrst sampling Facebook
accounts and then sampling all accounts that are friends with one
of the selected accounts, so-called “egos”. The observed network
records friendships among all users in the ego-networks. The sam-
pling here is a combination of vertex and edge sampling, with the

EDGE EXCHANGEABLE MODELS FOR NETWORK DATA

27

structure of the network mostly driven by the sampling mechanism
via edges connected to a common vertex.
• Political blogs [2]: Network built from hyperlinks between political
blogs. Vertices are websites (blogs) with a directed edge from i to j
for every hyperlink from website i to website j. Sampling is similar
to the Facebook network.
• US Power Grid [31]: Network built from the power grid in the
Western United States. The vertices are transformers, substations,
and generators with an edge between vertices if there is a high-
voltage transmission line between them. As the physical structure
of the US Power Grid rarely changes, the corresponding network
exhibits no vertex or edge growth of any statistical signiﬁcance.

References

[1] J. Abello, A. Buchsbaum, and J. Westbrook. A functional approach to external graph
algorithms. Proceedings of the 6th European Symposium on Algorithms, pages 332–343,
1998.

[2] L. A. Adamic and N. Glance. The Political Blogosphere and the 2004 US Election:
Divided They Blog. In Proceedings of the 3rd international workshop on Link discovery,
pages 36–43, 2005.

[3] D. J. Aldous. Exchangeability and related topics. In ´Ecole d’´et´e de probabilit´es de Saint-
Flour, XIII—1983, volume 1117 of Lecture Notes in Math., pages 1–198. Springer, Berlin,
1985.

[4] A.-L. Barab´asi and R. Albert. Emergence of scaling in random networks. Science,

286(5439):509–512, 1999.

[5] P. Bickel and A. Chen. A nonparametric view of network models and Newman–Girvan
and other modularities. Proceedings of the National Academy of Sciences of the United States
of America, 106(50):21068–21073, 2009.

[6] C. Borgs, J. Chayes, H. Cohn, and S. Ganguly. Consistent nonparametric estimation for

heavy-tailed sparse graphs. arXiv:1508.06675, 2015.

[7] F. Caron and E. Fox. Sparse graphs using exchangeable random measures. Accessed at

arXiv:1401.1137, 2015.

[8] F. Chung and L. Lu. Complex graphs and networks, volume 107 of CBMS Regional Conference
Series in Mathematics. Published for the Conference Board of the Mathematical Sciences,
Washington, DC, 2006.

[9] V. Colizza, R. Pastor-Satorras, and A. Vespignani. Reaction diﬀusion processes and

metapopulation models in heterogeneous networks. Nature Physics, 3:276–282, 2007.

[10] D. Cox and D. Hinkley. Theoretical Statistics. Chapman and Hall, London, 1974.
[11] H. Crane. Rejoinder: The ubiquitous Ewens sampling formula. Statistical Science,

31(1):37–39, 2016.

[12] H. Crane. The ubiquitous Ewens sampling formula (with discussion and a rejoinder by

the author). Statistical Science, 31(1):1–39, 2016.

[13] H. Crane and W. Dempsey. Atypical scaling behavior persists in real world interaction

networks. Unpublished manuscript, July 2015.

[14] W. J. Ewens. The sampling theory of selectively neutral alleles. Theoret. Population

Biology, 3:87–112, 1972.

[15] M. Faloutsos, P. Faloutsos, and C. Faloutsos. On power-law relationships of the Internet

topology. ACM Comp. Comm. Review, 29, 1999.

[16] D. Hoover. Relations on Probability Spaces and Arrays of Random Variables. Preprint,

Institute for Advanced Studies, 1979.

28

HARRY CRANE AND WALTER DEMPSEY

[17] H. Jeong, S. Mason, A.-L. Barab´asi, and Z. Oltvai. Lethality and centrality in protein

networks. Nature, 411:41, 2001.

[18] B. Klimt and Y. Yang. Introducing the Enron corpus. CEAS, 2004.
[19] E. D. Kolaczyk. Statistical analysis of network data. Springer Series in Statistics. Springer,

New York, 2009. Methods and models.

[20] R. Kumar, P. Raghavan, S. Rajagopalan, and A. Tomkins. Trawling the web for emerging

cyber communities. Proceedings of the 8th World Wide Web Conference, 1999.

[21] J. Leskovec, D. Huttenlocher, and J. Kleinberg. Signed networks in social media. CHI,

2010.

[22] J. J. McAuley and J. Leskovec. Learning to discover social circles in ego networks. In

Neural Information Processing Systems, pages 539–547, 2012.

[23] P. McCullagh. What is a statistical model? Ann. Statist., 30(5):1225–1310, 2002. With

[24] M. Newman. The structure of scientiﬁc collaboration networks. Proceedings of the National

comments and a rejoinder by the author.

Academy of Sciences, 98:404–409, 2001.

Available at http://wp.me/poFcY-Vw, 2011.

163, 2009.

[25] T. Opsahl. Why Anchorage is not (that) important: Binary ties and Sample selection.

[26] T. Opsahl and P. Panzarasa. Clustering in weighted networks. Social Networks, 31:155–

[27] P. Orbanz and D. Roy. Bayesian Models of Graphs, Arrays and Other Exchangeable Ran-
dom Structures. IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(2):437–
461, 2015.

[28] J. Pitman. Combinatorial stochastic processes, volume 1875 of Lecture Notes in Mathematics.
[29] R. A. Rossi and N. K. Ahmed. The network data repository with interactive graph
analytics and visualization. In Proceedings of the Twenty-Ninth AAAI Conference on
Artiﬁcial Intelligence, 2015.

[30] V. Veitch and D. Roy. The class of random graphs arising from exchangeable random

measures. arXiv:1512.03099 accessed at http://arxiv.org/pdf/1512.03099v1.pdf, 2015.

[31] D. Watts and S. Strogatz. Collective dynamics of ‘small-world’ networks. Nature, 393:440–

442, 1998.

2014.

[32] P. Wolfe and S. Olhede. Nonparametric graphon estimation. Available at arXiv:1309.5936,

[33] W. Zachary. An information ﬂow model for conﬂict and ﬁssion in small groups. Journal

of Anthropological Research, 33:452–473, 1977.

Department of Statistics & Biostatistics, Rutgers University, 110 Frelinghuysen Av-

enue, Piscataway, NJ 08854, USA

E-mail address: hcrane@stat.rutgers.edu
URL: http://stat.rutgers.edu/home/hcrane

Department of Statistics, University of Michigan, 1085 S. University Ave, Ann Arbor,

MI 48109, USA

E-mail address: wdem@umich.edu

