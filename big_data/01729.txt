Low-Rank Matrix Completion for Topological
Interference Management by Riemannian Pursuit

Yuanming Shi, Member, IEEE, Jun Zhang, Senior Member, IEEE, and Khaled B. Letaief, Fellow, IEEE

1

6
1
0
2

 
r
a

M
5

 

 
 
]
T
I
.
s
c
[
 
 

1
v
9
2
7
1
0

.

3
0
6
1
:
v
i
X
r
a

Abstract—In this paper, we present a ﬂexible low-rank matrix
completion (LRMC) approach for topological interference man-
agement (TIM) in the partially connected K-user interference
channel. No channel state information (CSI) is required at
the transmitters except the network topology information. The
previous attempt on the TIM problem is mainly based on its
equivalence to the index coding problem, but so far only a few in-
dex coding problems have been solved. In contrast, in this paper,
we present an algorithmic approach to investigate the achievable
degrees-of-freedom (DoFs) by recasting the TIM problem as an
LRMC problem. Unfortunately, the resulting LRMC problem is
known to be NP-hard, and the main contribution of this paper
is to propose a Riemannian pursuit (RP) framework to detect
the rank of the matrix to be recovered by iteratively increasing
the rank. This algorithm solves a sequence of ﬁxed-rank matrix
completion problems. To address the convergence issues in the
existing ﬁxed-rank optimization methods, the quotient manifold
geometry of the search space of ﬁxed-rank matrices is exploited
via Riemannian optimization. By further exploiting the structure
of the low-rank matrix varieties, i.e., the closure of the set of ﬁxed-
rank matrices, we develop an efﬁcient rank increasing strategy
to ﬁnd good initial points in the procedure of rank pursuit.
Simulation results demonstrate that the proposed RP algorithm
achieves a faster convergence rate and higher achievable DoFs
for the TIM problem compared with the state-of-the-art methods.

Index Terms—Interference alignment, topological interference
management, degrees-of-freedom, index coding, low-rank matrix
completion, Riemannian optimization, quotient manifolds.

I. INTRODUCTION

Network densiﬁcation with interference coordination has
been recognized as a promising way to meet the exponen-
tially growing mobile data trafﬁc in next generation wireless
networks [1], [2], [3]. In particular, interference alignment
[4] has been proposed as a powerful tool to understand the
Shannon capacity in various interference-limited scenarios,
e.g., the MIMO interference channel [5] and cellular networks
[6]. Although interference alignment can serve as a linear in-
terference management strategy achieving the optimal DoFs in
many scenarios, the overhead of obtaining the required global
instantaneous channel state information (CSI) has hindered

Manuscript received xxx; revised xxx; accepted xxx. Date of publication
xxx; date of current version xxx. This work is supported by the Hong Kong
Research Grant Council under Grant No. 610113.

Y. Shi is with the School of Information Science and Technology, Shang-

haiTech University, Shanghai, China (e-mail: shiym@shanghaitech.edu.cn).

J. Zhang is with the Department of Electronic and Computer Engineering,
Hong Kong University of Science and Technology, Hong Kong (e-mail:
eejzhang@ust.hk).
K. B. Letaief

is with Hamad bin Khalifa University (e-mail: kle-
taief@hbku.edu.qa) and Hong Kong University of Science and Technology
(e-mail: eekhaled@ust.hk).

its practical
implementation [7]. This motivates numerous
research efforts on CSI overhead reduction for interference
alignment, e.g., with delayed CSI [8] and alternating CSI [9].
However, the practical applicability of these results remain
unclear. More recently, a new proposal has emerged, namely,
topological interference management (TIM) [10], as a promis-
ing solution for the partially connected interference channels.
It is mainly motivated by the fact that most of the channels in
a wireless network are very weak and can be ignored due to
the shadowing and pathloss [10], [11], [12]. It thus provides an
opportunity to manage interference only based on topological
information rather than the instantaneous CSI.

i.e.,

Speciﬁcally, in the TIM problem, we assume that no CSI
at the transmitters is available beyond the network topology
knowledge,
the connectivity of the wireless network.
Due to the practical applicability of such CSI assumption
and information theoretic interest, the TIM problem has re-
ceived tremendous attentions and been investigated in vari-
ous scenarios with partial connectivity, e.g., the fast fading
scenarios [11], [13], transmitter cooperation [14] and MIMO
interference channels [15]. In particular, in a slow fading
scenario, by establishing the connection between the wireless
TIM problem and the wired index coding problem, efﬁcient
capacity and DoF analysis was provided in [10] based on
the existing results from index coding problems. However,
the index coding problem itself is an open problem, and thus
the existing solutions are only valid for some special cases.
For general network topologies in the wireless TIM problem,
the optimal DoF is still unknown. In a fast fading scenario, a
matrix rank-loss approach based on matroid and graph theories
was presented in [13] to characterize the symmetric DoF for
a class of TIM problems.

In this paper, we will present an algorithmic approach to
evaluate the achievable DoFs in the TIM problem for general
partially connected interference channels. It is achieved by
recasting the original TIM problem as a low rank matrix
completion (LRMC) problem [16]. Then the minimum number
of channel uses for interference-free data transmission will be
equal to the minimum rank of the matrix in the associated
LRMC problem. This approach has recently been applied to
solve the linear index coding problem over the ﬁnite ﬁeld [17]
and the wireless TIM problem with symmetric DoFs [18], [19].
We shall extend the previous results on the symmetric DoF
case with single data transmission for each user [18], [19] to
any achievable DoF region. The presented LRMC approach
will serve as a ﬂexible way to maximize the achievable DoFs
for any network topology, thereby providing insights on the
TIM problem for general network topologies that are not yet

2

available in theory.

Unfortunately, the resulting LRMC problem is NP-hard due
to the non-convex rank objective. Although the widely used
nuclear norm based convex relaxation provides an effective
way to solve the LRMC problem with polynomial
time
complexity and optimality guarantees with well structured
afﬁne constraints [16], it is inapplicable to our problem as
it always returns a full rank solution [18]. Another category
of algorithms is based on alternating minimization [20], [21]
by recasting the original LRMC problem as a ﬁxed-rank
optimization problem. Although the optimality can be guaran-
teed with standard assumptions (e.g., the original data matrix
should be incoherent [16]), the existing ﬁxed-rank methods
may converge slowly [22], [23] and require the optimal rank
of the matrix as a prior information [24].

A. Contributions

We present a low-rank matrix completion approach to max-
imize the achievable DoFs for the TIM problem. In particular,
we extend the results in [19], [18] for the symmetric DoF
with single data transmission for each user to any DoF region.
To address the limitations of existing ﬁxed-rank approaches,
we propose a Riemannian pursuit (RP) algorithm to solve the
LRMC problem for the TIM problem. This is achieved by
iteratively increasing the rank of the matrix to be recovered. In
particular, the developed RP algorithm possesses the following
properties:

• We can efﬁciently solve the ﬁxed-rank optimization prob-
lems to address the convergence issues in the existing
ﬁxed-rank methods;

• We design an efﬁcient rank increasing strategy to ﬁnd a

good initial point in the next iteration for rank pursuit.

In the proposed RP framework, by exploiting the Rie-
mannian quotient manifold geometry of the search space of
ﬁxed-rank matrices via low-rank matrix factorization [23],
[25], [26], [27],
the nonlinear conjugate gradient (a ﬁrst-
order method with superlinear convergence rate endowed with
a good Riemannian metric [26], [27]) and trust-region (a
second-order method with quadratic convergence rate [28])
based Riemannian optimization algorithms [29] are developed
to solve the smooth ﬁxed-rank optimization problems. These
algorithms can achieve faster convergence rates and higher
precision solutions compared with the existing ﬁxed-rank
methods, such as the alternating minimization method [20],
[21] and the embedded manifold based Riemannian optimiza-
tion algorithm [22]. Furthermore, by exploiting the structures
of low-rank matrix varieties [24], [30], [19], i.e., the closure
of the set of ﬁxed-rank matrices, an efﬁcient rank increasing
strategy is proposed to ﬁnd a high quality initial point and
to guarantee that the objective decreases monotonically in the
procedure of rank pursuit.

In summary, the major contributions of the paper are as

follows:

1) A Riemannian pursuit framework is proposed to solve
the resulting LRMC problem by solving a sequence of
ﬁxed-rank optimization problems with an efﬁcient rank
increasing strategy.

2) To address the convergence issues in the existing ﬁxed-
rank based methods, we present a versatile Riemannian
optimization framework by exploiting the quotient man-
ifold geometry of the ﬁxed-rank matrices and the least-
squares structure of the cost function [26] as well as the
second-order information of the problem.

3) A novel rank increasing strategy is proposed, which
considers intrinsic manifold structures in the developed
Riemannian optimization algorithms. In particular, by
exploiting the structures of low-rank varieties, we extend
the results in [24], [19] for the embedded manifold to
the framework of the quotient manifold.

Simulation results will demonstrate the superiority of the
proposed RP algorithms with faster convergence rates and
the capability of automatic rank detection compared with the
existing ﬁxed-rank optimization algorithms to maximize the
achievable DoFs for the TIM problem.

B. Organization

The remainder of the paper is organized as follows. Section
II presents the system model and problem formulations. In
Section III, the low-rank matrix completion approach with Rie-
mannian pursuit is developed. The Riemannian optimization
algorithms are developed in Section IV. The rank increasing
strategy is presented in Section V. Numerical results will be
demonstrated in Section VI. Finally, conclusions and discus-
sions are presented in Section VII. The derivations of the
Riemannian optimization related ingredients are diverted to
the appendix.

II. SYSTEM MODEL AND PROBLEM STATEMENT

A. Channel Model

Consider the topological interference management (TIM)
problem in the partially connected K-user interference channel
with K single-antenna transmitters and K single-antenna
receivers [10]. Speciﬁcally, let V be the index set of the
connected transceiver pairs such that (i, j) ∈ V representing
the i-th receiver is connected to the j-th transmitter. That is,
the channel propagation coefﬁcients belonging to the set V are
nonzero and are set to be zeros otherwise. Each transmitter j
wishes to send a message Wj to its corresponding receiver j.
Here, Wj is uniformly chosen in the corresponding message
set Wj.
Each transmitter j encodes its message Wj into a vector
xj ∈ CN of length N and transmits the signal over N time
slots. Therefore, the input-output relationship is given by

H[ij]xj + ni,∀i = 1, . . . , K, (1)

yi = H[ii]xi + X(i,j)∈V,i6=j
where ni ∼ CN (0, IN ) and yi ∈ CN are the additive
isotropic white Gaussian noise and received signal at receiver
i, respectively; H[ij] = diag{Hij} = Hij IN is an N × N
diagonal matrix with Hij ∈ C as the channel coefﬁcient
between transmitter j and receiver i in the considered block.
We consider the block fading channel model, and thus the
channel stays constant during the N time slots, i.e., all the

diagonal entries in H[ij] are the same. The matrix repre-
sentation for the channel coefﬁcients in (1) is mainly for
the comparison of different channel models to establish the
interference alignment conditions, which will be explained in
Section II-C. In this paper, following the TIM setting [10],
we assume that only the network topology information V is
available at transmitters. Furthermore, each transmitter has an
average power constraint, i.e., 1
E[kxik2] ≤ ρ with ρ > 0 as
N
the maximum transmit power.

B. Achievable Rates and DoF

We assume that transmitters 1, 2, . . . , K have independent
messages W1, W2, . . . , WK intended for receivers 1, 2, . . . , K,
respectively. The rate tuple (R1, R2, . . . , RK) with Ri =
log |Wi|
is achievable if there exists an encoding and decoding
scheme such that the probability of decoding error for all the
messages can be made arbitrarily small simultaneously as the
codewords length N approaches inﬁnity [31].

N

The degrees of freedom (DoF) in the partially connected

K-user interference channel is deﬁned as [10], [4]

di = lim sup

ρ→∞

Ri

log(ρ)

,∀i.

(2)

The DoF region D is deﬁned as the closure of the set of
achievable DoF tuples. In particular, the symmetric DoF dsym
is the highest value d0, such that the DoF allocation di =
d0,∀i, is inside the DoF region. This is given by [10]
log(ρ)(cid:21) .

ρ→∞ (cid:20)sup(Rsym,...,Rsym)∈D

dsym = lim sup

Rsym

(3)

In this paper, we choose the DoF as the performance metric
and design the corresponding linear interference management
strategies to maximize the achievable DoFs [10], [5].

C. Topological Interference Management

Linear schemes become particular interesting for interfer-
ence management due to their low-complexity and the DoF
optimality in many scenarios [10], [4], [5]. We thus restrict
the class of interference management strategies to linear
schemes to maximize the achievable DoFs as the signal-to-
noise ratio (SNR) approaches inﬁnity. Speciﬁcally, for message
Wj, let Vj ∈ CN ×Mj and Ui ∈ CN ×Mi be the precoding
matrix at transmitter j and the receiver combining matrix
at receiver i, respectively. Assume that each message Wj
is split
into Mj independent scalar data streams, denoted
as sj = [s1(Wj ), s2(Wj ), . . . , sMj (Wj )]T ∈ CMj . And
sm(Wj )’s are independent Gaussian codebooks, each of which
carries one symbol and is transmitted along the column vectors
of the precoding matrix Vj. Therefore, over the N channel
uses, the input-output relationship (1) is rewritten as
H[ij]Vj sj + ni,∀i.

yi = H[ii]Visi + X(i,j)∈V,i6=j

(4)

In the regime of asymptotically high SNR, to accomplish
decoding, we impose the constraints that, at each receiver i, the
desired signal space H[ii]Vi is complementary to the interfer-

ence space P(i,j)∈V,i6=j H[ij]Vj. That is, after projecting the

3

received signal vector yi onto the space Ui, the interference
terms should be aligned and then cancelled while the desired
signal should be preserved [5], [32], [4], i.e.,

i H[ij]Vj = 0,∀i 6= j, (i, j) ∈ V,
UH
i H[ii]Vi(cid:17) 6= 0,∀i.

det(cid:16)UH

(5)

(6)

If conditions (5) and (6) are satisﬁed, the parallel interference-
free channels can be obtained over N channel uses. Therefore,
the DoF of Mi/N is achieved for message Wi. However, this
requires instantaneous CSI and its acquisition is challenging
in dense networks with a large number of transceiver pairs [7],
[10].

Observe that the channel matrix H[ij] equals Hij IN for the
constant channel over the N channel uses. The conditions (5)
and (6) can be rewritten as the following channel independent
conditions:

UH

i Vj = 0,∀i 6= j, (i, j) ∈ V,
i Vi(cid:1) 6= 0,∀i.

det(cid:0)UH

(7)
(8)

Therefore, we can design the transceivers Ui’s and Vj’s only
based on the knowledge of the network topology without re-
quiring the instantaneous CSI. This is fundamentally different
from the conventional interference alignment approach [5],
[4], [33], in which the global instantaneous CSI is required.
In contrast, the channel independent topological interference
management conditions (7) and (8) make the corresponding
interference management approach much more practical.

Remark 1: In this paper, we consider the block fading
channel model to capture the channel coherence phenomenon
in a slow fading scenario. Speciﬁcally, we assume that channel
gains stay constant over N time slots such that the effective
channel matrix H[ij]
is a diagonal matrix with identical
diagonal entries, which plays a key role to yield the channel
independent interference alignment conditions (7) and (8).
This further motives the low-rank matrix completion approach
in Section III. However, in a fast fading scenario, i.e., the
channel gains change at each time instant, the approaches
presented in this paper may not be applicable, and other
approaches (e.g., the rank-loss approach [13]) are required.

The problem of studying the DoFs in the partially connected
interference channels based on the network topology infor-
mation is known as the topological interference management
(TIM) problem [10], [11], [34]. Most of the existing works
on the TIM problem are trying to establish the topology
conditions under which the desired DoF is achievable based on
graph theory [11], [34], or applying the existing results from
the index coding problem [10]. In contrast, in this paper, by
generalizing the preliminary results in [18], [19] for the case of
single data stream transmission, we present a novel approach
based on the low-rank matrix completion [16] to solve the
TIM problem based on conditions (7) and (8) for arbitrary
network topologies with arbitrary number of data streams.
Furthermore, novel algorithms will be developed based on
Riemannian optimization techniques [29] to solve the resulting
NP-hard LRMC problem.

4

III. LOW-RANK MATRIX COMPLETION FOR TOPOLOGICAL
INTERFERENCE MANAGEMENT VIA RIEMANNIAN PURSUIT
In this section, we present a low-rank matrix completion
approach to solve the TIM problem, i.e., ﬁnding the minimum
channel uses N such that the interference alignment conditions
(7) and (8) are feasible. Speciﬁcally, deﬁne Xij = UH
i Vj ∈
CMi×Mj . Then, conditions (7) and (8) can be rewritten as

Transmitters

1

2

3

4

5

1

2

3

4

5

Transformation

R
e
c
e
v
e
r
s

i

PΩ(X) = IM ,

(9)

k=1 Mk + 1, . . . ,Pi

where X = [Xij ] ∈ CM×M with M = Pi Mi, IM is the
M × M identity matrix, and PΩ : RM×M → RM×M is the
orthogonal projection operator onto the subspace of matrices
which vanish outside Ω such that the (i, j)-th component of
PΩ(X) equals to Xij if (i, j) ∈ Ω and zero otherwise. Here,
the set Ω is deﬁned as Ω = {Gi × Gj, (i, j) ∈ V}, where
Gi = {Pi−1
k=1 Mk}. For example, given the
network topology adjacency matrix V = {(1, 1), (1, 2), (2, 2)}
the set Ω is given as Ω =
and M1 = M2 = 2,
{(1, 1), (1, 2), (2, 1), (2, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 3),
(3, 4), (4, 3), (4, 4)}. To yield a nontrivial solution, we assume
that N ≤ M . As X = [UH
i Vj ] = UHV ∈ CM×M with U =
[U1, . . . , UK ]H ∈ CM×N , V = [V1, . . . , VK] ∈ CN ×M , we
have rank(X) = N .
Remark 2: To assist numerical algorithm design, we specify
i Vi = I,∀i for condition (8) to recover the desired
UH
signal. Speciﬁcally, for the desired message Wi, as UH
i Vi is
invertible, by projecting yi onto the Ui space, we have

˜yi =

=

= si +

= si +

1

1

UH

i yi

Hii (cid:2)UH
Hii (cid:2)UH

i Vi(cid:3)−1
i Vi(cid:3)−1(cid:0)HiiUH
Hii (cid:2)UH

i Vi(cid:3)−1

UH

1

i ni

1
Hii

UH

i ni,

i Visi + UH

i ni(cid:1)

(10)

(11)

(12)

(13)

where the second equation is based on condition (7) to
eliminate the interference contributed by other messages, and
the last equation is obtained by setting UH
i Vi = I. Based on
(13), we have the following parallel interference-free channels
for each desired symbol steam:

˜yi,m = si,m + ˜ni,m, m ∈ {1, 2, . . . , Mi},

(14)

UH

where ˜yi = [˜yi,m], si = [si,m] and 1
i ni = [˜ni,m]. As
Hii
each interference-free channel contributes 1/N DoF, we have
Mi/N DoFs for the desired message Wi. Note that for the
i Vi, we can always obtain the
generic invertible matrix UH
parallel interference-free channels (14) with different noise
terms to achieve Mi/N DoF in the high SNR regime.

Given the number of data streams M1, . . . , MK, to max-
imize the achievable DoFs,
is
equivalent to minimizing N , or the rank of the matrix X,
subject to constraint (9). Thus the linear TIM problem can
be reformulated as the following matrix completion problem
[18], [19]:

i.e., M1/N, . . . , MK/N ,

it

P : minimize
X∈RM ×M

rank(X)

subject to PΩ(X) = IM .

(15)

Fig. 1.
(a) The topological interference management problem in a partially
connected network with no CSI at transmitters (except the network topology
information). The desired channel links are black and interference links are
red. (b) Associated incomplete matrix with “⋆” representing arbitrary values.
For example, as there is no interference from transmitter 2 to receiver 1,
1 v3 must be 0 as it
X12 = uH
represents the equivalent interference channel from user 3 to user 1.

1 v2 can take any value; while X13 = uH

Note that, we only need to consider problem P in the real
ﬁeld without losing any performance in terms of achievable
DoFs, as the problem parameter IM is a real matrix and the
i Vj,∀i 6= j, (i, j) /∈ V can be further restricted to
matrices UH
the real ﬁeld, whose corresponding signals will not contribute
any interference. Let X⋆ be the solution of problem P, and we
can extract the precoding matrices Vj’s and decoding matrices
Ui’s by performing matrix factorization as X⋆ = UHV =
i Vj], which can be obtained by the QR decomposition for
[UH
matrix X⋆ using the Gram-Schmidt process.

be

The

then

given

achievable DoFs will

by
M1/rank(X⋆), . . . , M1/rank(X⋆) with X⋆ as the optima
of problem P. This LRMC approach for the TIM problem
has been presented in [18], [19] for the single data stream
transmission with the performance metric as the symmetric
DoF, i.e., Mi = 1,∀i. While problem P in (15) provides a
clean formulation of the TIM problem, compared to existing
matrix completion problems, unique challenges arise with the
poorly structured afﬁne constraint, as will be illustrated in
the next subsection. An example of the idea of transforming
the TIM problem to the corresponding matrix completion
problem is illustrated in Fig. 1. For this special case, we can
rewrite the conditions (7) and (8) as the incomplete matrix
X = [Xij] with Xij = uH

i vj.

A. Problem Analysis

The problem of rank minimization with afﬁne constraints
has received enormous attention in areas such as collaborative
ﬁltering, statistical machine learning, as well as image and
signal processing [16], [35]. Recently, the rank minimization
approach has been proposed to solve the design problem of
transmit and receive beamaformers for interference alignment
in MIMO interference channels [36]. However, the non-convex
rank objective function in the LRMC problem P makes
it NP-hard. Enormous progress has been made recently to
address the NP-hardness of the LRMC problem with elegant
theoretical results using convex relaxation approaches [16] and
non-convex optimization approaches [21]. However, most of
the results highly rely on the assumptions of well structured
afﬁne constraints, e.g., the set Ω is uniformly sampled [16],
[21] and the original matrix to be recovered is incoherent [16].
Unfortunately, with the poorly structured afﬁne constraint

in problem P, none of the above standard assumptions in
the literature is satisﬁed. This brings unique challenges for
solving and analyzing the LRMC problem P for topological
interference management. In this subsection, we will ﬁrst
review the existing algorithms for the LRMC problem and
then motivate our proposed algorithm based on Riemannian
optimization [29].

1) Nuclear Norm Minimization: Let X = PM

i=1 σiuivH
i
be the singular value decomposition (SVD) of the matrix X
with σi’s as the singular values and ui’s and vi’s as the
left and right singular vectors, respectively. The rank function
rank(X) = kσk0 with σ = (σ1, . . . , σM ) is often relaxed
with the nuclear norm kXk∗ = kσk1 as a convex surrogate
[16], which can be regarded as an analogy with convex ℓ1-
norm relaxation of the non-convex ℓ0-norm in sparse signal
recovery. If we apply this relaxation to problem P, it will
give the following problem,

minimize kXk∗
subject to PΩ(X) = IM .

Unfortunately, based on the following fact [18]:

(16)

(17)

|Tr(X)| =(cid:12)(cid:12)(cid:12)
Tr(cid:16)Xi
=(cid:12)(cid:12)(cid:12)Xi
≤Xi

σivH

σiuivH

i (cid:17)(cid:12)(cid:12)(cid:12)
i ui(cid:12)(cid:12)(cid:12) ≤ Xi

= (cid:12)(cid:12)(cid:12)Xi
σi|vH

i (cid:1)(cid:12)(cid:12)(cid:12)
Tr(cid:0)σiuivH
i ui|

σi = kXk∗,

problem (16) will always return the solution X = IM , which
is full rank. As a consequence, with the poorly structured
afﬁne constraint in problem P, the nuclear norm based convex
relaxation approach is inapplicable to problem P.

2) Alternating Optimization Approaches: Alternating min-
imization [21], [20] is another popular non-convex optimiza-
tion approach to solve the LRMC problem. Speciﬁcally, the
alternating minimization approach involves expressing the un-
known rank-r matrix X as the product of two smaller matrices
UVT , where U ∈ RM×r and V ∈ RM×r, such that the low-
rank property of the matrix X is automatically satisﬁed. Based
on this factorization, the original LRMC problem P with the
optimal rank as a prior information can be reformulated as the
following non-convex optimization problem:

minimize

U∈RM ×r,V∈RM ×r kPΩ(UVT ) − IMk2
F .

(18)

The alternating minimization algorithm for problem (18) con-
sists of alternatively solving for U and V while ﬁxing the
other factor.
However,

the ﬁxed-rank based alternating minimization
approach has a low convergence rate [22], [26]. It also
fails to utilize the second-order information to improve the
convergence rate, e.g., the Hessian of the objective function.
Moreover, it requires the optimal rank as a prior information,
which is, however, not available in problem P.

B. Riemannian Pursuit

In this paper, we propose a Riemannian pursuit algorithm
based on the Riemannian optimization technique [29] to solve

5

the LRMC problem P by alternatively performing the ﬁxed-
rank optimization and rank increase, thereby detecting the
minimum rank of matrix X in problem P. The proposed
algorithm is described as Algorithm 1. It will well address
the limitations of the existing ﬁxed-rank based methods [18],
[20], [21], [37] by

1) Designing efﬁcient algorithms for ﬁxed-rank optimiza-
tion to minimize the squared errors of the afﬁne con-
straint in problem P;

2) Designing an effective rank increasing strategy to ﬁnd
good initial points in the procedure of rank pursuit,
thereby detecting the minimum rank of matrix X such
that the afﬁne constraint in problem P is satisﬁed.

Speciﬁcally, by ﬁxing the rank of matrix X as r (1 ≤
r ≤ M ), we propose to solve the following smooth ﬁxed-
rank constrained optimization problem,

Pr : minimize

X∈Mr

f (X),

(19)

2kPΩ(X) − IMk2

where f (X) := 1
F is the cost function
representing the squared errors of the afﬁne constraint in
problem P, and Mr is a smooth (C∞) manifold given by
(20)

Mr := {X ∈ RM×M : rank(X) = r}.

Observing that the least-squared cost function in problem
Pr is also smooth, we thus adopt the Riemannian optimiza-
tion technique [29] to solve it. Riemannian optimization has
recently gained popularity due to its capability of exploit-
ing the geometry of well structured search spaces based on
matrix factorization [29], [22], [23], [38], [25], [26], [27],
thereby being competitive with alternative approaches, e.g.,
convex relaxation and alternating minimization. In particular,
the Riemannian optimization is the generalization of standard
unconstrained optimization, where the search space is Rn,
to optimization of a smooth objective function on the search
space of a Riemannian manifold. The details of Riemannian
optimization for the ﬁxed-rank optimization problem Pr will
be presented in Section IV.

The rank increasing strategy plays an important role in the
proposed algorithm. In particular, by embedding the critical
point X[r] in the current iteration into the manifold Mr+1 in
the next iteration, we propose an efﬁcient rank increasing strat-
egy to generate good initial points and guarantee monotonic
decrease of the objective function for ﬁxed-rank optimization
in the procedure of rank pursuit. This is achieved by exploiting
the structures of the low-rank matrix varieties and the manifold
geometry of ﬁxed-rank matrices. The rank increasing strategy
will be presented in Section V.

IV. A RIEMANNIAN OPTIMIZATION FRAMEWORK FOR

SMOOTH FIXED-RANK OPTIMIZATION

In this section, we present a versatile framework of Rie-
mannian optimization for the ﬁxed-rank matrix completion
problem Pr. It is performed on the quotient manifolds and
exploits the symmetry structure (i.e., the quotient manifold
geometry) in the search space of the ﬁxed-rank constraint
and the Hessian of the least-squares structure of the cost
function. Speciﬁcally, the problem structures will be presented

6

Algorithm 1 Riemannian Pursuit (RP) for LRMC problem P
1: Input: M , Ω, desired accuracy ǫ.
2: Initialize: X[1]
0 ∈ RM×M , r = 1.
3: while not converged do
4:

Compute a critical point X[r] for the smooth ﬁxed
rank-r problem Pr with initial point X[r]
0 with the
Riemannian optimization algorithm in Section IV.
Update the rank r ← r + 1. Compute the initial point
X[r]
for the next iteration based on the rank increasing
0
algorithm in Section V.

5:

6: end while
7: Output: X[r] and the detected minimum rank r.

in Section IV-A. The framework of Riemannian optimization
on the quotient manifolds will be demonstrated in Section
IV-B. In particular, the matrix representations of all the opti-
mization ingredients and algorithm implementation details will
be provided in Section IV-C and in Section IV-D, respectively.

A. Problem Structures

To develop efﬁcient algorithms for the smooth ﬁxed-rank
optimization problem Pr, we exploit two fundamental struc-
tures: one is the symmetry in the ﬁxed-rank constraint; and
the other is the least-squares structure of the cost function.
All the structures will be incorporated into the Riemannian
optimization framework.

1) Matrix Factorization and Quotient Manifold: The set
Mr is known to be a smooth submanifold of dimension (2M−
r)r embedded in the Euclidean space RM×M [22]. Based on
the SVD-type factorization, we represent X ∈ Mr as [25]

X = UΣVT ,

(21)
where U, V ∈ St(r, M ) and Σ ∈ GL(r). Here, St(r, M ) =
: YT Y = Ir} denotes the Stiefel
{Y ∈ RM×r
manifold of orthonormal M × r matrices and GL(r) =
: rank(Y) = r} is the set of all r × r
{Y ∈ Rr×r
invertible matrices. However,
the factorization in (21) is
not unique as we have the symmetry structures X =
U ΣQV )(VQV )T , QU , QV ∈ Q(r), where Q(r) is
(UQU )(QT
the set of all r×r orthogonal matrices given by O(r) = {Q ∈
Rr×r : QT Q = Ir}. Therefore, the search space for problem
Pr should be the set of equivalence classes as follows:

[X] = {(UQU , QT

U ΣQV , VQV ) : QU , QV ∈ Q(r)}. (22)
In particular, denote the computation space (or the total space)
as Mr := St(r, M )×GL(r)×St(r, M ). The abstract quotient
space Mr/ ∼ makes the optima isolated as Mr/ ∼:=
Mr/(O(r) × O(r)), where O(r) × O(r) is the ﬁber space
and ∼ represents the equivalence relation. More details of
the quotient manifolds can be found in [29]. As the quotient
manifold Mr/ ∼ is an abstract space, to design algorithms,
the matrix representation in the computation space is required.
2) Least-Squares Structures and Riemannian Metric: To
optimize on the abstract search space Mr/ ∼, a Riemannian
metric in the computation space Mr is required such that
Mr/ ∼ is a Riemannian submersion [29, Section 3.6.2]. In

particular, the only constraint imposed on the metric is that it
should be invariant along the set of equivalence classes [X]
(22). The Riemannian metric gX : TXMr × TXMr → R
deﬁnes an inner product between the tangent vectors on the
tangent space TXMr in the computation space Mr.
Furthermore, by encoding the Hessian (the second-order in-
formation) of the cost function into the metric gX, superlinear
convergence rates can be achieved for the ﬁrst-order optimiza-
tion algorithms [39], [27]. However, calculating the Hessian of
the cost function f in problem P is computationally costly.
We thus propose a valid Riemannian metric based on the block
diagonal approximation of the Hessian of the simpliﬁed cost
function as presented in the following proposition.

Proposition 1 (Riemannian Metric): By exploiting the sec-
ond order information of the least-squares cost function, the
Riemannian metric gX : TXMr × TXMr → R is given by

gX(ξX, ζX) = hξU , ζ U ΣΣTi + hξΣ, ζΣi +

hξV , ζ V ΣT Σi,

Proof: Please refer to Appendix A for details.

(23)
where ξX := (ξU , ξΣ, ξV ) ∈ TXMr, ζX := (ζU , ζΣ, ζV ) ∈
TXMr and X := (U, Σ, V).
Note that, different from the conventional metric [38], which
only takes the search space into consideration, the novel metric
(23) can encode the second-order information of the cost
function, thus leads to a faster convergence speed for the ﬁrst-
order algorithms [27], [39]. This will be further justiﬁed in the
simulation section.

B. Riemannian Optimization on Quotient Manifolds

The main idea of Riemannian optimization is to encode the
constraints on the manifold into the search space, and then
perform descent on this manifold search space rather than
in the ambient Euclidean space. In particular, the Euclidean
gradient and Euclidean Hessian need to be converted to the
Riemannian gradient and Riemannian Hessian, respectively,
to implement the conjugate gradient method and trust-region
method in the Riemannian optimization framework. This will
be explicitly presented in Section IV-C. For the quotient man-
ifold Mr/ ∼, the abstract geometric objects call for concrete
matrix representations in the computation space Mr, which
is achieved by the principle of the Riemannian submersion
[29, Section 3.6.2]. Therefore, essentially, the algorithms are
implemented in the computation space. Speciﬁcally, with the
Riemannian metric (23), the quotient manifold Mr/ ∼ is
submersed into Mr. We now have the Riemannian quotient
manifold as follows:

Deﬁnition 1 (Riemannian Quotient Manifold [29, Section 3.6.2]):

Endowed with the Riemannian metric (23), Mr/ ∼ is called
a Riemannian quotient manifold of Mr.
Let T[X](Mr/ ∼) denote the abstract tangent space in the
quotient manifold Mr/ ∼, which has the matrix representa-
tion in TXMr. The abstract tangent vectors in T[X](Mr/ ∼)
are restricted to the directions that do not produce a displace-
ment along the equivalence class [X] (22). This is achieved
by decomposing the tangent space TXMr in the computa-
tion space into complementary spaces as follows: TXMr =

VXMr ⊗HXMr, where VXMr and HXMr are the vertical
space and horizontal space, respectively. In particular, the
horizontal space HXMr, which is the orthogonal complement
of VXMr in the sense of the Riemannian metric gX, provides
a valid matrix representation of the abstract tangent space
T[X](Mr/ ∼) [29, Section 3.5.8]. The vertical space VXMr
is obtained from the tangent space of the equivalence class [X]
(22). We call it the horizontal lift given that any element in
the abstract tangent space ξ[X] ∈ T[X](Mr/ ∼) has a unique
element in the horizontal space ξX ∈ HXMr.
As gX is constrained to be invariant along the equiv-
alent class [X] (22),
it can deﬁne a Riemannian metric
g[X](ξ[X], ζ[X]) : T[X](Mr/ ∼) × T[X](Mr/ ∼) → R in the
quotient space Mr/ ∼ as g[X](ξ[X], ζ [X]) := gX(ξX, ζ X),
where ξ[X], ζ[X] ∈ T[X](Mr/ ∼) and ξX, ζX ∈ HXMr are
the horizontal lifts or matrix representations of ξ[X] and ζ[X].
Note that both ξX and ζX belong to the tangent space TXMr.
In summary, we have Riemannian submersion as follows:
Deﬁnition 2 (Riemannian Submersion [29, Section 3.6.2]):
The choice of the metric (23), which is invariant along the
equivalent class [X], and of the horizontal space HXMr
as the orthogonal complement of VX, in the sense of the
Riemannian metric (23), makes the search space Mr/ ∼ a
Riemannian submersion.
Therefore, with the metric (23), the Riemannian optimiza-
tion algorithms on the quotient manifold Mr/ ∼ call for
lifts) in the computation
matrix representation (horizontal
space Mr. Speciﬁcally, let Ξi ∈ HXiMr be the search
direction at the i-th iteration. Deﬁne RX : HXMr → Mr
as the retraction mapping operator that maps the element
in the horizontal space Ξi ∈ HXMr to the points on the
computation space Mr. The Riemannian optimization frame-
work for the smooth optimization problem Pr is presented in
Algorithm 2 and the corresponding schematic view is shown in
Fig. 2. In particular, the parameter αi in Algorithm 2 denotes
the step size, which we will explain in Section IV-D.

Algorithm 2 A Riemannian Optimization Framework for the
Fixed-Rank Optimization Problem Pr
1: Input: M , r, Ω, desired accuracy ε.
2: Initialize: X0 = Xinitial, Ξ0 = 0, i = 0.
3: while not converged do
4:
5:
6: end while
7: Output: X⋆ = Xi.

Compute the search direction Ξi ∈ HXiMr.
Update Xi+1 = RXi (αiΞi). Update i = i + 1.

7

Vx

TxM = Hx ⊕ Vx

Rx(ξx)

Hx

x

ξx

M

y

x+

ξ[x]

M/ ∼

[x]

[x+]

[Rx(ξx)]

T[x](M/ ∼)

Fig. 2. A schematic view of Riemannian optimization framework: abstract
geometric objects (shown in dotted line) on a quotient manifold Mr/ ∼ call
for matrix representatives (shown in solid lines) in the computation space (or
total space) Mr. The points x and y in Mr belong to the same equivalence
class (shown in solid blue color) and they represent a single point [x] = {y ∈
Mr : y ∼ x} on the quotient manifold Mr/ ∼. Figure courtesy of Mishra
et al. [27].

Proposition 2 (Horizontal Space): The horizontal

objective function f (X) on this space, which is the gener-
alization of the Euclidean gradient ∇f (X) = PΩ(X) − IM
of f (X). To achieve this goal, we ﬁrst provide the following
proposition on the matrix representation of the abstract tangent
space T[X](Mr/ ∼).
space
HXMr, which is any complementary subspace of VXMr
in the sense of
the Riemannian metric gX (23), pro-
vides a valid matrix representation of the abstract tangent
space T[X](Mr/ ∼) as HXMr = {ηX ∈ TXMr
:
S1 and S2 are symmetric}, where S1 = ΣΣT ηT
UU − ΣηT
and S2 = ΣT ΣηT
Proof: Please refer to Appendix B for details.

VV + ηT

ΣΣ.

Σ

To compute the Riemannian gradient, we need to deﬁne two
projection operators: tangent space projection and horizontal
space projection. Speciﬁcally, the tangent space projection is
the operator that projects the ambient space onto the tangent
space.

Proposition 3 (Tangent Space Projection): The

tangent
space projection operator PTXMr : RM×r × Rr×r × RM×r →
TXMr that projects the ambient space RM×r× Rr×r× RM×r
onto the tangent space TXMr is given by:

PTXMr (AU , AΣ, AV ) = (ξU , ξΣ, ξV ),

(24)
where ξU = AU − UBU (ΣΣT )−1, ξΣ = AU , ξV = AV −
VBV (ΣT Σ)−1. Here, BU and BV are symmetric matrices of
size r× r that are obtained by solving the Lyapunov equations
U U)ΣΣT , (25)
ΣΣT BU + BU ΣΣT = ΣΣT (UT AU + AT
ΣT ΣBV + BV ΣT Σ = ΣT Σ(VT AV + AT
V V)ΣT Σ. (26)

C. Quotient Manifold Representation

In this subsection, we derive the concrete matrix repre-
sentations (horizontal lifts) in the computation space Mr for
abstract geometric objects on the quotient manifold Mr/ ∼,
thereby implementing the Riemannian optimization algo-
rithms.

1) Riemannian Gradient: To design an algorithm using the
conjugate gradient method on he quotient space Mr/ ∼,
we need to deﬁne the Riemannian gradient grad[X]f for the

Proof: Please refer to Appendix C for details.

The horizontal space projection is the operator that extracts
the horizontal component of the tangent vector by projecting
the tangent space onto the horizontal space.

Proposition 4 (Horizontal Space Projection): The horizon-
tal space projection operator ΠHXMr : TXMr → HXMr
that projects the tangent space TXMr onto the horizontal
space HXMr is given by ΠHXMr (ξX) = (ζ U , ζΣ, ζV ),
where ζU = ξU − UΘ1, ζΣ = ξΣ + Θ1Σ − ΣΘ2,
ζV = ξV − VΘ2. Here, Θ1 and Θ2 are skew-symmetric

8

matrices of size r× r that are obtained by solving the coupled
system of Lyapunov equations

ΣΣT Θ1 + Θ1ΣΣT − ΣΘ2ΣT = Skew(UT ξU ΣΣT ) +

(27)
ΣT ΣΘ2 + Θ2ΣT Σ − ΣT Θ1Σ = Skew(VT ξV ΣT Σ) +
(28)

Skew(ΣT ξΣ),

Skew(ΣξT

Σ),

where Skew(·) extracts the skew-symmetric part of a square
matrix, i.e., Skew(C) = (C − CT )/2.

Proof: Please refer to Appendix D for details.

Based on Propositions 3 and 4, we have the matrix repre-
sentation (horizontal lift) gradXf of the Riemannian gradient
grad[X]f on the quotient manifold Mr/ ∼ at X = (U, Σ, V)
as follows:

gradXf = (ξU , ξΣ, ξV ),

(29)

where ξU = AVΣT (ΣΣT )−1 − UBU (ΣΣT )−1, ξΣ =
UT SV, ξV = AT UΣ(ΣT Σ)−1 − VBV (ΣT Σ)−1, with
A = ∇f (X) = PΩ(X) − IM . Here, BU and BV are the
solutions to the Lyapunov equations

ΣΣT BU + BU ΣΣT = 2Sym(ΣΣT UT AVΣ),
ΣT ΣBV + BV ΣT Σ = 2Sym(ΣT ΣVT ST UΣ),

(30)
(31)

where Sym(·) extracts the symmetric part of a square matrix,
i.e., Sym(C) = (C + CT )/2. Please refer to Appendix E for
the details on the derivation of the Riemannian gradient (29).
2) Riemannian Hessian: To design second-order algorithms
(e.g., the trust-region scheme) on the quotient space Mr/ ∼,
we need to deﬁne the Riemannian connection on this space,
which is the generalization of directional derivative of a vector
ﬁeld on the manifold. Let ∇ηX ξX be the directional derivative
of the vector ﬁeld ξX ∈ TXMr applied in the direction
ηX ∈ TXMr on the computation space Mr. Then the matrix
representation (horizontal lift) of the Riemannian connection
ξ[X] on the quotient space Mr/ ∼ with η[X], ξ[X] ∈
∇η[X]
T[X](Mr/ ∼) is given by ΠHXMr (∇ηX ξX), which is the
horizontal projection of the Riemannian connection onto the
horizontal space. By the Koszul formula [29, Theorem 5.3.1],
the Riemannian connection is given by

∇ηX ξX = DξX[ηX] + (θU , θΣ, θV ),

(32)

where DξX[ηX] is the classical Euclidean directional deriva-
tive and θU = ηU BU + UBU + 2ξU Sym(ηΣΣT )(ΣΣT )−1,
θΣ = 0, θV = ηV BV + VBV + 2ξV Sym(ηT
ΣΣ)(ΣT Σ)−1.
Here, BU and BV are the solutions to the Lyapunov equations
(30) and (31).

Therefore, the matrix representation (horizontal lift) of the
Riemannian Hessian Hess[X]f [ξX] on the quotient manifold
Mr/ ∼ is given by

HessXf [ξX] = ΠHXMr (∇ξXgradXf ),

(33)

where gradXf (29) is the Riemannian gradient in the compu-
tation space Mr and the Riemannian connection is given in
(32).

(a) Retraction

(b) Vector transport

Fig. 3. Visual representation of the concept of retraction and vector transport
within the framework of Riemannian optimization techniques. Figure courtesy
of Kressner et al. [40].

D. Riemannian Optimization Algorithms

Based on the above matrix representations or horizontal lifts
of the geometric objects on abstract search space Mr/ ∼,
it is ready to implement the algorithms in the computation
space Mr. To trade off the convergence rate and the compu-
tational complexity, we present a ﬁrst-order algorithm (i.e., the
conjugate gradient method) and a second-order method (i.e.,
the trust-region method) in Section IV-D1 and Section IV-D2,
respectively.

1) Conjugate Gradient Method:

In the conjugate gradi-
ent scheme, the search direction at iteration i is given by
Ξi := −gradXi f + βiTXi−1→Xi (Ξi−1), where gradXif ∈
HXMr is the Riemannian gradient at point Xi ∈ Mr
and TXi−1→Xi (ξX) : HXiMr → HXiMr is the matrix
lift) of the vector transport
representation (the horizontal
T[Xi−1]→[Xi](ξ[X]) that maps tangent vectors from one tangent
space T[Xi−1](Mr/ ∼) to another tangent space T[Xi](Mr/ ∼
) given by TXi−1→Xi (Ξi−1) = ΠHXi Mr (PTXi Mr (Ξi−1)).

Therefore, the sequence of the iterates is given by

Xi+1 = RXi (αiΞi),

(34)

where αi denotes the step size satisfying the strong Wolf
conditions [24], [29] and RX : HXMr → Mr is the
retraction mapping operator that maps the element in the
horizontal space Ξi ∈ HXMr to the points on the com-
putation space Mr. The product nature of the computation
space Mr allows to choose a retraction by simply combining
the retractions on the individual manifolds [29, Example
4.1.3], RX(ξX) = (uf(U + ξU ), Σ + ξΣ, uf(V + ξV )),
where ξX := (ξU , ξΣ, ξV ) ∈ HXMr and uf(·) extracts the
orthogonal factor of a full column-rank matrix, i.e., uf(A) =
A(AT A)−1/2.

The concepts of vector transport and retraction in the total
space Mr are illustrated on the right and left sides of Fig. 3,
respectively.
2) Trust Region Method: To provide quadratic convergence
rate, we implement the second-order optimization algorithm
based on the trust-region method [28]. In particular, in the
quotient manifold Mr/ ∼, the trust-region subproblem is
horizontally lifted to HXMr and formulated as

m(ξX)

minimize
ξX∈HXMr
subject to gX(ξX, ξX) ≤ δ2,

(35)

9

OPTIMIZATION-RELATED INGREDIENTS FOR PROBLEM Pr

TABLE I

Matrix representation of an element X ∈ Mr
Computational space Mr
Quotient space
Metric gX(ξX, ζX) for ξX, ζX ∈ TXMr
Riemannian gradient gradXf
Riemannian Hessian HessXf [ξX]
Retraction RX(ξX) : HXMr → Mr

Pr : minimizeX∈Mr f (X)

X = (U, Σ, V)

St(r, M ) × GL(r) × St(r, M )

St(r, M ) × GL(r) × St(r, M )/(O(r) × O(r))

gX(ξX, ζX) = hξU , ζU ΣΣT i + hξΣ, ζΣi + hξV , ζV ΣT Σi

gradXf = (ξU , ξΣ, ξV ) (29)

HessXf [ξX] = ΠHXMr (∇ξX gradXf ) (33)

(uf(U + ξX), Σ + ξΣ, uf(V + ξV ))

where δ is the trust-region radius and the cost function is given
by

m(ξX) = f (X) + gX(ξX, gradXf ) +

1
2

gX(ξX, HessXf [ξX]),

(36)

where gradXf (29) and HessXf (33) are the horizontal
lift (matrix representation) of the Riemannian gradient and
Riemannian Hessian on the quotient manifold Mr/ ∼. Given
the matrix representation of the search direction (35), the
details of the implementation of the trust-region algorithm can
be found in [41].

In summary, the optimization-related ingredients for prob-

lem Pr are provided in Table I.

V. RANK INCREASING ALGORITHM

In this section, we propose a rank-one update algorithm to
generate good initial points and provide monotonic decrease
for the objective functions for ﬁxed-rank optimization in the
procedure of rank pursuit in Algorithm 1. This is achieved by
exploiting the structure of the low-rank matrix varieties [42],
[30].

A. Low-Rank Matrix Varieties

We present a systematic way to develop the rank increasing
strategy in Algorithm 1 based on the following low-rank
matrix varieties M≤r = {X ∈ RM×M : rank(X) ≤ r},
which is the closure of the set of ﬁxed-rank metrics Mr.
Furthermore, we consider the linear-search method on M≤r+1
with the iterates as follows,

Xi+1 = P≤r+1(Xi + αiΞi),

(37)
where Ξi is a search direction in the tangent cone TXiM≤r+1
at Xi [42], αi is a step-size, and P≤r+1 is a metric projection
onto M≤r+1 with a best rank-(r + 1) approximation in the
Frobenius norm.

r

where ∇X[r] f = (PΩ(X[r]) − IM ) is the Euclidean gradient
of the cost function f at point X[r] and Ξ(r)
is the orthogonal
projection on the tangent space TX[r]Mr given by the Rie-
mannian gradient, i.e., Ξ(r)
is the
best rank-one approximation of
Σr = −∇X[r] f − Ξ(r)

r − ∇X[r] f (X[r]) + gradX[r] f

r = −gradX[r] f , and Ξ(1)

= −∇X[r] f (X[r]) + ξU ΣVT + UξΣVT + UΣξT
which is orthogonal to the tangent space TX[r]Mr [43].
Based on (37) and (38), we shall adopt the following rank
update strategy to ﬁnd a good initial point for the next iteration
in Algorithm 1,

V , (38)

r

0

X[r+1]

= P≤r+1(cid:16)X[r] + αr(cid:16)Ξ(1)

r − gradX[r] f(cid:17)(cid:17) , (39)
where αr ≥ 0 is a step size and satisﬁes the following
condition [24],

0

αr
2 hΘr, Θri.

f (X[r+1]

) ≤ f (X[r]) −

Therefore, if Ξr is zero,
terminate.

(40)
then ∇X[r] f = 0 and we can
Remark 3: Note that when the Riemannian gradient
gradX[r] f equals zero, the rank update strategy (39) is equiv-
alent to the following rank increasing strategy [44]

0

X[r+1]

= X[r] − σuvT ,

(41)
where σ ≥ 0 is the dominant singular value and (u, v) is the
pair of top left and right singular vectors with unit-norm of the
Euclidean gradient ∇X[r] f . Although the rank update strategy
(41) ensures that the cost function f decreases monotonically
w.r.t. r, it ignores the intrinsic manifold structure of ﬁxed-
rank matrices in Algorithm 2. Speciﬁcally, the Riemannian
gradient gradX[r] f (29), which belongs to the tangent space
TX[r]Mr, is not necessarily equal to zero, as the corresponding
ﬁxed-rank optimization problem may not be solved exactly in
practice, e.g., Algorithm 2 may terminate when the maximum
number of iterations is exceeded [24].

B. Riemannian Pursuit

Assume that the iterate X[r] has rank r at the r-th iteration
in Algorithm 1. In the next iteration, we will increase the rank
by r+1. To embed X[r] into the search space M≤r+1, suppose
that we choose the projection of the negative Euclidean gra-
dient on the tangent cone TX[r]M≤r+1 as a search direction,
Ξr = arg minΞ∈TX[r] M≤r+1 k−∇X[r] f−ΞkF = Ξ(r)
r +Ξ(1)
r ,

C. Monotonic Decrease of the Objective Function

We shall show that the Riemannian manifold rank update
strategy (39) ensures that the objective function decreases
monotonically with respect to r. Speciﬁcally, as gradX[r] f ∈
TX[r]Mr and Σr (38) is orthogonal to TX[r]Mr, we have the
following fact that

hΣ(1)

r , gradX[r] fi = 0.

(42)

10

Let X[1], X[2], . . . , be the sequence generated by Algorithm
1, based on (40) and (42), we have

f (X[r+1]) ≤(1) f (X

[r+1]
0

≤(3) f (X[r]) −
≤(4) f (X[r]).

) ≤(2) f (X[r]) −
τr
2

(kΣ(1)
r k2

αr
2 hΘr, Θri
F +kgradX[r] fk2
F )

(43)

Here, the ﬁrst inequality is due to the fact that the iterates
of the Riemannian optimization algorithm try to minimize the
cost function f , the second and the third inequalities are based
on the facts (40) and (42), respectively. Therefore, the cost
function f (X[r]) decreases monotonically with respect to r.
Remark 4: Although only the rank-one update strategy is
considered in Algorithm 1,
the proposed rank increasing
algorithm in this section can be easily generalized to the
general rank-r with r > 1 updates to improve the convergence
rate [24], [30] for the RP algorithm. However, this may yield
the detected rank of matrix X overestimated.

VI. SIMULATION RESULTS

In this section, we simulate the proposed Riemannian
pursuit algorithms for topological interference management
problems in partially connected K-user interference channels.
The conjugate gradient Riemannian algorithm and the trust-
region Riemannian pursuit algorithm, are termed “CGRP” and
“TRRP”, respectively. The two algorithms are compared to the
following state-of-the-art algorithms:

• LRGeom with Riemannian Pursuit: In this algorithm [24],
[19], termed “LRGeom”, the embedded manifold based
ﬁxed-rank optimization algorithm developed in [22] with
the Riemannian pursuit rank increasing strategy proposed
in [19], [24] is adopted to solve problem P.

• LMaFit: In this algorithm, the alternating minimization
scheme with rank adaptivity is adopted to solve problem
P [20].

The Matlab implementation of all

the Riemannian al-
gorithms for the ﬁxed-rank optimization problem Pr is
based on the manifold optimization toolbox ManOpt [41].
All the Riemannian optimization algorithms are initialized
randomly as shown in [22] and are terminated when either
the norm of the Riemannian gradient is below 10−6, i.e.,
kgradXfk ≤ 10−6, or the number of iterations exceeds
500. The setting for LMaFit is the same as that in [20]. We
adopt the following normalized residual [20] as the stopping
criteria for Algorithm 1 to estimate the rank for matrix X:

ǫ = kPΩ(X) − IMkF /√M . We set ǫ = 10−6 for all the

algorithms to estimate the minimum rank of matrix X such
that it satisﬁes the afﬁne constraint in problem P.

A. Convergence Rate

Consider a 100-user partially connected interference chan-
nel with 400 interference channel links. The sets of the con-
nected interference links are generated uniformly at random.
We turn off rank adaptivity for all the algorithms to solve the
ﬁxed-rank optimization problem Pr. Fig. 4 and Fig. 5 show
the convergence rates of different algorithms for the ﬁxed-rank

l

i

a
u
d
s
e
R
d
e
z
i
l

 

a
m
r
o
N

100

10−1

10−2

10−3

10−4

10−5

10−6
 
0

TRRP

 

LMaFit

LRGeom

CGRP

100

200

Iteration

300

400

500

Fig. 4. Convergence rate with the rank of matrix X as four.

l

i

a
u
d
s
e
R
d
e
z
i
l

 

a
m
r
o
N

100

10−1

10−2

10−3

10−4

10−5

10−6
 
0

 

LMaFit

LRGeom

CGRP

TRRP

50

100

150

Iteration

200

250

300

Fig. 5. Convergence rate with the rank of matrix X as ﬁve.

optimization problem Pr with r = 4 and r = 5, respectively.
Both ﬁgures show that
the trust-region based Riemannian
optimization algorithm TRRP has the fastest convergence rate
and achieves higher precision solutions in a few iterations
compared with the other three algorithms. Encoded with
the second-order information in the Riemannian metric (23),
the conjugate gradient based Riemannian algorithm CGRP
achieves a faster convergence rate than LRGeom [22], while
LMaFit [20] has the lowest convergence rate among all the
algorithms.

These two ﬁgures also indicate that, with the same stopping
criteria ǫ = 10−6 in Algorithm 1, the detected rank of matrix
X by TRRP is 4. Although the detected rank of matrix
X by both CGRP and LRGeom is 5,
the latter one has
a slower convergence rate. Furthermore, the required rank
of LMaFit should be larger than 5 to achieve the stopping
criteria ǫ = 10−6. This conclusion will be further conﬁrmed
in the following simulations on the empirical results for the
achievable DoFs.

 

TRRP
CGRP
LRGeom
LMaFit

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

F
o
D
c

 

i
r
t
e
m
m
y
S
e
b
a
v
e
h
c
A

i

 

l

0.1

 
0

10

20

30
50
Interfering Links

40

60

70

80

Fig. 6. Achievable symmetric DoF versus different numbers of interference
links.

B. Achievable Symmetric DoF and Optimal DoF Results

Consider a 20-user partially connected interference channel.
The sets of the connected interference links are generated
uniformly at random. We simulate and average 100 network
topology realizations. Fig. 6 demonstrates the achievable
symmetric DoF with different algorithms assuming that the
data streams Mi = 1,∀i. We can see that the second-order
algorithm TRRP can achieve the highest symmetric DoF,
but it has the highest computational complexity due to the
computation expensive calculation of the Hessian. For the ﬁrst-
order optimization algorithm, CGRP can achiever a higher
symmetric DoF than LRGeom [24], [19] and LMaFit [20]. In
particular, we can see that, with few interference links, quite
high DoFs can be achieved.

To further justify the effectiveness of the RP framework, we
numerically check that our RP algorithms can recover all the
optimal DoF results for the speciﬁc TIM problems in [10]. The
same conclusion has also been presented in [19]. Note that our
proposed automatic rank detection capable RP algorithms do
not need the optimal rank as a prior information, while the
alternating projection algorithm [18] requires the optimal rank
as a prior information to perform low-rank matrix projection.
Moreover, it is interesting to theoretically identify the class of
network topologies such that the proposed RP framework can
provide optimal symmetric DoFs.

In summary, all

the simulation results illustrate the ef-
fectiveness of the proposed Riemannian pursuit algorithms
by exploiting the quotient manifold geometry of the ﬁxed-
rank matrices and encoding the second-order information in
the Riemannian metric (23), as well as utilizing the second-
order optimization scheme. In particular, there is a tradeoff
between the achievable symmetric DoF and the computational
complexity using the ﬁrst-order algorithm CGRP (which is
applicable in large-sized networks) and the second-order algo-
rithm TRRP (which is applicable in small-sized and medium-
sized networks).

11

VII. CONCLUSIONS AND FURTHER WORKS

In this paper, we presented a ﬂexible low-rank matrix
completion approach to maximize the achievable DoFs for
the partially connected K-user interference channel with
any network topology. A Riemannian pursuit algorithm was
proposed to solve the resulting low-rank matrix completion
optimization problem by exploiting the quotient manifold
geometry of the search space and the structure of low-rank
matrix varieties for rank pursuit. In particular, we showed
that, by encoding the second-order information, the quotient
manifold based Riemannian optimization algorithms achieve
a faster convergence rate and higher precious solutions than
the existing algorithms. Simulation results showed that the
proposed Riemannian pursuit algorithms achieve higher DoFs
for general network topologies compared with the state-of-the-
art methods.

Several future directions of interest are listed as follows:
• From the algorithmic perspective,

is interesting to
establish the optimality of the Riemannian pursuit al-
gorithms for the low-rank matrix completion problem
P,
thereby establishing the relationship between the
achievable DoF and the network topology.

it

• From the information theoretic perspective, it is critical
to translate the numerical insights (e.g., optimal DoF
achievability for the speciﬁc network topologies in [10])
provided by the LRMC approach into the optimal DoF
for any network topology.

• It is particularly interesting to extend the LRMC approach
to more general scenarios, e.g., with ﬁnite SNR scenarios,
MIMO interference channels,
transmitter cooperations
with data sharing, and wired linear index coding problems
in the ﬁnite ﬁeld. In particular, as optimization on mani-
folds deeply relies on smoothness, the search space will
become discrete in a ﬁnite ﬁeld. Therefore, the presented
Riemannian pursuit algorithms cannot be extended to the
ﬁnite ﬁeld in principle.

• It is also interesting to apply the Riemannian optimization
technique to other wireless communications and network-
ing problems (e.g., the hybrid precoding in millimeter
wave systems [45]). In particular, extending the corre-
sponding algorithms to the complex ﬁeld is critical, as
most of the Riemannian algorithms are only developed
in real ﬁeld and complex ﬁeld extension is not trivial.

APPENDIX A

PROOF OF PROPOSITION 1: RIEMANNIAN METRIC

To induce the metric based on the Hessian of the cost
function f in problem Pr, we consider a simpliﬁed cost
F /2, yielding the following optimization
function kX − IMk2
problem:

minimize

X∈Mr

1
2

Tr(XT X) − Tr(X),

(44)

Based on the factorization X = UΣVT , we have the
matrix representation of Lagrangian for problem (44) as fol-
lows L(X) = 1
2 Tr(VΣT UT UΣVT ) − Tr(UΣVT ), where
X has the matrix representation (U, Σ, V) ∈ St(r, n) ×

12

GL(r) × St(r, n). The second-order derivative of L(X) ap-
plied in the direction ξX is given by D2L(X)[ξX] =
(ξUΣΣT + 2USym(ΣξΣ) − VξΣ − ξVΣT ,−ξUVT +
ξΣ + 2ΣSym(VT ξV)− UT ξV, ξVΣΣT − UξΣ − ξUΣT +
2VSym(ΣT ξΣ)), where ξX has the matrix representation
(ξU, ξΣ, ξV) ∈ Rn×r × Rr×r × Rn×r.
As the cost function in (44) is convex and quadratic in X,
it is also convex and quadratic in the arguments (U, Σ, V)
individually. Therefore, the block diagonal elements of the
second-order derivative LXX(X) of the Lagrangian are strictly
positive deﬁnite. The following Riemannian metric can be
induced from the block diagonal approximation of LXX(X),

gX(ξX, ζX) = hξX, D2L(X)[ζ X]i

≈ hξU, ζUΣΣTi + hξΣ, ζΣi +

hξV, ζVΣT Σi,

(45)
where ξX = (ξU, ξΣ, ξV), ζ X = (ζ U, ζΣ, ζ V) ∈ TXMr
and X ∈ (U, Σ, V).
the metric is invariant along the equiv-
To verify that
(22), based on [29, Proposition 3.6.1],
alent class [X]
tangent
it
is equivalent
to show that
the
vectors ξX, ζ X ∈ TXMr does not change under
U ΣQV , QV V),
transformations (U, Σ, V)
U ξΣQV , ξV V), (ζ U , ζΣ, ζ V ) 7→
(ξU , ξΣ, ξV ) 7→ (ξU QU , QT
U ζΣQV , ζV V). After simple computation, we can
(ζ U QU , QT
verify that (45) is a valid Riemannian metric and does not
depend on the speciﬁc matrix representations along the equiv-
alence class [X] (22).

7→ (UQU , QT

the metric for

APPENDIX B

PROOF OF PROPOSITION 2: HORIZONTAL SPACE

The vertical space VXMr is the linearization of the equiv-
alence classes [X] (22) and formed by the set of directions
that contains tangent vectors to the equivalence classes. Based
on the matrix representation of the tangent space for the
orthogonal matrices [29, Example 3.5.3], we have the matrix
representation for the vertical space as

VXMr = (UΘ1, ΣΘ2 − Θ1Σ, VΘ2),

(46)

i = −Θi, i = 1, 2.

where Θ1 and Θ2 are any skew-symmetric matrices of size
r × r, i.e., ΘT
The horizontal space HXMr, which is any complementary
subspace to VXMr in TXMr with respect to the Riemannian
metric gX (23), provides a valid matrix representation of
the abstract tangent space T[X](Mr/ ∼) [29, Section 3.5.8]
based on the Riemannian submersion principle. Speciﬁcally,
let ηX = (ηU, ηΣ, ηV) ∈ HXMr and ζX = (ζ U, ζΣ, ζV) ∈
VXMr. By deﬁnition, ηX should be orthogonal to ζX with
respect to the Riemannian metric gX, i.e.,

gX(ηX, ζX) = Tr((ΣΣT )ηT

UUΘ1) +

ΣΣΘ2 − ηT

Tr(ηT
Tr((ΣT Σ)ηT

VVΘ2)

ΣΘ1Σ) +

where S1 = ΣΣT ηT
ΣΣ.
Based on the fact that Tr(GT Θ) = 0, if and only if G is

= Tr(S1Θ1) + Tr(S2Θ2) = 0,
UU− ΣηT

Σ and S2 = ΣT ΣηT

VV + ηT

(47)

symmetric, the characterization of the horizontal space is given
by
HXMr = {ηX ∈ TXMr : S1 and S2 are symmetric}.(48)

APPENDIX C

PROOF OF PROPOSITION 3: TANGENT SPACE PROJECTION
Given a matrix in the ambient space RM×r×Rr×r×RM×r,
its projection onto the tangent space TXMr is obtained by
extracting the component normal space NXMr to the tangent
space in the Riemannian metric sense.
We ﬁrst derive the matrix characterization of the normal
space. Speciﬁcally, let ηX = (ηU, ηΣ, ηV) ∈ TXMr and
ζX = (ζU, ζΣ, ζ V) ∈ NXMr. By deﬁnition, ηX should be
orthogonal to ζX with respect to the Riemannian metric gX,
i.e., g(ηX, ζX) = 0. That is, the following conditions
hξU, ζUΣΣTi = 0,hξV, ζ VΣT Σi = 0,hξΣ, ζΣi = 0, (49)
should hold for any ηX ∈ TXMr. It is obvious that ζΣ = 0.
Furthermore, based on [29, Example 3.5.2], we have the matrix
characterization of ηU as

ηU = UΩ + U⊥K,

(50)
where Ω is a skew-symmetric matrix of size r × r, K ∈
R(M−r)×r can be any matrix, and U⊥ is any M × (M − r)
matrix such that span(X⊥) is the orthogonal complement of
span(X). Similarly, we can obtain the characterization of ηV.
We rewrite ζU as ¯ζU = ζUΣΣT with,

¯ζU = UBU + U⊥AU ,

(51)
where AU ∈ Rr×r and BU ∈ R(M−r)×r can be deduced from
conditions (49) and (50). Based on the fact that Tr(GT Θ) =
0, if and only if G is symmetric, we can conclude that BU is
symmetric and AU = 0. Therefore, we have

ζ UΣΣT = UBU ,

(52)

where BU = BT
U . Similarly, we can obtain the matrix
characterization of ζV. Therefore, we arrive at the matrix
representation of the norm space,

NXMr = {(UBU (ΣΣT )−1, 0, VBV (ΣT Σ)−1)}, (53)

where BU and BV are symmetric metrics of size r × r.
As the tangent space projector PTXMr is obtained by
extracting the component normal to the tangent space TXMr
in the ambient space RM×r × Rr×r × RM×r, we have the
expression for the operator PTXMr as
PTXMr(AU , AΣ, AV ) = (AU − UBU (ΣΣT )−1,

AΣ, AV − VBV (ΣT Σ)−1)),(54)
which belongs to the tangent space. The tangent space TXMr
in the computation space Mr at the point X = (U, Σ, V) is
the product of the tangent spaces of the individual manifolds,
which has the following matrix representation [29, Example
3.5.2],

TXMr = {(ξU , ξΣ, ξV ) ∈ RM×r × Rr×r × RM×r :

UT ξU + ξT

U U = 0, VT ξV + ξT

V V = 0}. (55)

Based on (54) and (55), we know that U should satisfy the
condition:

APPENDIX F

RIEMANNIAN QUOTIENT MANIFOLDS

13

UT ξU + ξT

U U = UT (cid:2)AU − UBU (ΣΣT )−1(cid:3) +

(cid:2)AU − UBU (ΣΣT )−1(cid:3)T

U = 0, (56)

which is equivalent to the Lyapunov equation for the symmet-
ric matrix BU ,

ΣΣT BU + BU ΣΣT = ΣΣT (UT AU + AT

U U)ΣΣT .(57)
Similarly, we can obtain the Lyapunov equation for the sym-
metric matrix BV as in (26).

PROOF OF PROPOSITION 4: HORIZONTAL SPACE

APPENDIX D

PROJECTION

The horizontal space projector ΠHXMr can be obtained
by extracting the horizontal component of the tangent vector.
Speciﬁcally, let ξX = (ξU , ξΣ, ξV ) ∈ TXMr and ζX =
(ζ U , ζ Σ, ζV ) ∈ HXMr. We have the expression for the
operator ΠHXMr as

ΠHXMr (ξX) = (ξU − UΘ1, ξΣ + Θ1Σ − ΣΘ2,

ξV − VΘ2)
= (ζ U , ζΣ, ζV ),

(58)
which belongs to the horizontal space HXMr. Based on (48),
we have

ΣΣT ζT

U U − Σζ T

Σ = ΣΣT (ξU − UΘ1)T U −
Σ(ξΣ + Θ1Σ − ΣΘ2)T
= (ΣΣT ξT

U U − ΣξT

Σ) + (ΣΣT Θ1 +

ΣΣT Θ1 − ΣΘ2ΣT ),

(59)
which is symmetric. As ΣΣT ζ T
U U −
ΣζT
Σ)T , we can obtain the equation in (27). Similarly, we can
obtain the equation in (28) by checking the condition that ζV
is symmetric.

U U − ΣζT

Σ = (ΣΣT ζT

APPENDIX E

COMPUTE THE RIEMANNIAN GRADIENT (29)

Let X = (U, Σ, V) and A = ∇f (X) = PΩ(X)− I denote
the Euclidean gradient of f at point X. The partial derivatives
of f (X) with respective to U, Σ and V are given by

∂f (X)

∂U

= AVΣT ,

∂f (X)

∂Σ

= UT AV,

∂f (X)

∂V

= AT UΣ.(60)

With metric (23), the scaled Euclidean gradient is given by

¯A = (AVΣT (ΣΣT )−1, UT AV, AT UΣ(ΣT Σ)−1). (61)
By further projecting ¯A onto the tangent space based on (24),
we have the matrix representation (horizontal lift) gradXf of
grad[X]f as

gradXf = PTXMr ( ¯A),

(62)

which yields the equations in (29). Note that, based on the Rie-
mannian submersion principle [29, Section 3.6], PTXMr ( ¯A) is
already the horizontal lift, which can be veriﬁed that the hori-
zontal space projection ΠHXMr will not change PTXMr ( ¯A).

We now consider the case of a quotient manifold M/ ∼,
where the structure space M is endowed with a Riemannian
metric g. The horizontal space HX and X ∈ M is canonically
chosen as the orthogonal complement in TXM of the vertical
space VX = TXπ−1(X), namely,

HX := (TXVX)⊥

= {ηX ∈ TXM : g(χX, ηX) = 0,∀χX ∈ VX}. (63)
Recall that the horizontal lift at X ∈ π−1([X]) of a tangent
vector ξ[X] ∈ T[X](M/ ∼) is the unique tangent vector ξX ∈
HX that satisﬁes Dπ(X)[ξX]. If, for every [X] ∈ M/ ∼ and
every ξ[X], ζ[X] ∈ T[X](M/ ∼), the expression gX(ξX, ζX)
does not depend on X ∈ π−1([X]), then

g[X](ξ[X], [ζ]X) := gX(ξX, ζX)

(64)
deﬁnes a Riemannian metric on M/ ∼. Endowed with this
Riemannian metric, M/ ∼ is called a Riemannian quotient
manifold of M, and the natural projection π : M → M/ ∼
is a Riemannian submersion. (In other words, a Riemannian
submersion is a submersion of Riemannian manifolds such
that Dπ preserves inner products of vectors normal to ﬁbers.)

ACKNOWLEDGMENT

The authors would like to thank Dr. Bamdev Mishra, Dr.
Nicolas Boumal and Prof. Bart Vandereycken for insightful
discussions about Riemannian optimization for low-rank ma-
trix completion.

REFERENCES

[1] Y. Shi, J. Zhang, K. Letaief, B. Bai, and W. Chen, “Large-scale convex
optimization for ultra-dense Cloud-RAN,” IEEE Wireless Commun.
Mag., vol. 22, pp. 84–91, Jun. 2015.

[2] D. Gesbert, S. Hanly, H. Huang, S. Shamai Shitz, O. Simeone, and
W. Yu, “Multi-cell MIMO cooperative networks: A new look at inter-
ference,” IEEE J. Sel. Areas Commun., vol. 28, pp. 1380–1408, Sep.
2010.

[3] Y. Shi, J. Zhang, B. O’Donoghue, and K. Letaief, “Large-scale convex
optimization for dense wireless cooperative networks,” IEEE Trans.
Signal Process., vol. 63, pp. 4729–4743, Sept. 2015.

[4] V. Cadambe and S. Jafar, “Interference alignment and degrees of
freedom of the K-user interference channel,” IEEE Trans. Inf. Theory,
vol. 54, pp. 3425–3441, Aug. 2008.

[5] G. Bresler, D. Cartwright, and D. Tse, “Feasibility of interference
alignment for the MIMO interference channel,” IEEE Trans. Inf. Theory,
vol. 60, pp. 5573–5586, Sep. 2014.

[6] V. Ntranos, M. Maddah-Ali, and G. Caire, “Cellular interference align-

ment,” IEEE Trans. Inf. Theory, vol. PP, no. 99, pp. 1–1, 2015.

[7] O. El Ayach, A. Lozano, and R. Heath, “On the overhead of interference
alignment: Training, feedback, and cooperation,” IEEE Trans. Wireless
Commun., vol. 11, pp. 4192–4203, Nov. 2012.

[8] M. A. Maddah-Ali and D. Tse, “Completely stale transmitter channel
state information is still very useful,” IEEE Trans. Inf. Theory, vol. 58,
pp. 4418–4431, Jul. 2012.

[9] R. Tandon, S. Jafar, S. Shamai Shitz, and H. Poor, “On the synergistic
beneﬁts of alternating CSIT for the MISO broadcast channel,” IEEE
Trans. Inf. Theory, vol. 59, pp. 4106–4128, Jul. 2013.

[10] S. Jafar, “Topological interference management through index coding,”

IEEE Trans. Inf. Theory, vol. 60, pp. 529–568, Jan. 2014.

[11] N. Naderializadeh and A. Avestimehr, “Interference networks with no
CSIT: Impact of topology,” IEEE Trans. Inf. Theory, vol. 61, pp. 917–
938, Feb. 2015.

[38] G. Meyer, S. Bonnabel, and R. Sepulchre, “Linear regression under
ﬁxed-rank constraints: a Riemannian approach,” in Proc. Int. Conf.
Mach. Learn. (ICML), 28th, 2011.

[39] J. Nocedal and S. Wright, Numerical optimization. Springer Science &

Business Media, 2006.

[40] D. Kressner, M. Steinlechner, and B. Vandereycken, “Low-rank tensor
completion by Riemannian optimization,” BIT Numer. Math., vol. 54,
no. 2, pp. 447–468, 2014.

[41] N. Boumal, B. Mishra, P.-A. Absil, and R. Sepulchre, “Manopt, a Matlab
toolbox for optimization on manifolds,” J. Mach. Learn. Res., vol. 15,
pp. 1455–1459, 2014.

[42] R. Schneider and A. Uschmajew, “Convergence results for projected
line-search methods on varieties of low-rank matrices via Łojasiewicz
inequality,” SIAM J. Optim., vol. 25, no. 1, pp. 622–646, 2015.

[43] P.-A. Absil and I. V. Oseledets, “Low-rank retractions: a survey and new
results,” Computational Optimization and Applications, pp. 1–25, 2014.
[44] B. Mishra, G. Meyer, F. Bach, and R. Sepulchre, “Low-rank optimization
with trace norm penalty,” SIAM J. Optim., vol. 23, no. 4, pp. 2124–2149,
2013.

[45] X. Yu, J. C. Shen, J. Zhang, and K. Letaief, “Alternating minimization
algorithms for hybrid precoding in millimeter wave MIMO systems,”
IEEE J. Sel. Topics Signal Process., to appear, 2016.

14

[12] Y. Shi, J. Zhang, and K. Letaief, “Optimal stochastic coordinated
beamforming for wireless cooperative networks with CSI uncertainty,”
IEEE Trans. Signal Process., vol. 63, pp. 960–973, Feb. 2015.

[13] A. E. Gamal, N. Naderializadeh, and A. S. Avestimehr, “When does
an ensemble of matrices with randomly scaled rows lose rank?,” arXiv
preprint arXiv:1501.07544, 2015.

[14] X. Yi and D. Gesbert, “Topological

interference management with
transmitter cooperation,” IEEE Trans. Inf. Theory, vol. 61, pp. 6107–
6130, Nov. 2015.

[15] H. Sun and S. Jafar, “Topological interference management with multiple
antennas,” in Proc. IEEE Int. Symp. Inform. Theory (ISIT), pp. 1767–
1771, Jun. 2014.

[16] E. J. Cand`es and B. Recht, “Exact matrix completion via convex
optimization,” Found. Comput. Math., vol. 9, pp. 717–772, Apr. 2009.
[17] H. Esfahanizadeh, F. Lahouti, and B. Hassibi, “A matrix completion
approach to linear index coding problem,” in IEEE Information Theory
Workshop (ITW), 2014, pp. 531–535, Nov 2014.

[18] B. Hassibi, “Topological interference alignment in wireless networks,”

Smart Antennas Workshop, Aug. 2014.

[19] Y. Shi, J. Zhang, and K. B. Letaief, “Low-rank matrix completion via
Riemannian pursuit for topological interference management,” in Proc.
IEEE Int. Symp. Inform. Theory (ISIT), (Hong Kong), Jun. 2015.

[20] Z. Wen, W. Yin, and Y. Zhang, “Solving a low-rank factorization
model for matrix completion by a nonlinear successive over-relaxation
algorithm,” Mathematical Programming Computation, vol. 4, no. 4,
pp. 333–361, 2012.

[21] P. Jain, P. Netrapalli, and S. Sanghavi, “Low-rank matrix completion us-
ing alternating minimization,” in ACM Symp. Theory Comput., pp. 665–
674, ACM, 2013.

[22] B. Vandereycken, “Low-rank matrix completion by Riemannian opti-

mization,” SIAM J. Optim., vol. 23, pp. 1214–1236, Jun. 2013.

[23] N. Boumal and P.-a. Absil, “RTRMC: A riemannian trust-region method
for low-rank matrix completion,” in Advances in neural information
processing systems, pp. 406–414, 2011.

[24] M. Tan, I. W. Tsang, L. Wang, B. Vandereycken, and S. J. Pan,
“Riemannian pursuit for big matrix recovery,” in Proc. Int. Conf. Mach.
Learn. (ICML), vol. 32, pp. 1539–1547, Jun. 2014.

[25] B. Mishra, G. Meyer, S. Bonnabel, and R. Sepulchre, “Fixed-rank matrix
factorizations and Riemannian low-rank optimization,” Comput. Statist.,
vol. 29, no. 3-4, pp. 591–621, 2014.

[26] B. Mishra and R. Sepulchre, “R3MC: A Riemannian three-factor algo-
rithm for low-rank matrix completion,” in IEEE Conference on Decision
and Control, 2014.

[27] B. Mishra and R. Sepulchre, “Riemannian preconditioning,” arXiv

preprint arXiv:1405.6055, 2014.

[28] P.-A. Absil, C. G. Baker, and K. A. Gallivan, “Trust-region methods
on riemannian manifolds,” Found. Comput. Math., vol. 7, pp. 303–330,
Feb. 2007.

[29] P.-A. Absil, R. Mahony, and R. Sepulchre, Optimization algorithms on

matrix manifolds. Princeton University Press, 2009.

[30] A. Uschmajew and B. Vandereycken, “Line-search methods and rank
increase on low-rank matrix varieties,” in Proceedings of
the 2014
International Symposium on Nonlinear Theory and its Applications
(NOLTA2014), 2014.

[31] T. M. Cover and J. A. Thomas, Elements of information theory. John

Wiley & Sons, 2012.

[32] K. Gomadam, V. R. Cadambe, and S. A. Jafar, “A distributed numerical
approach to interference alignment and applications to wireless interfer-
ence networks,” IEEE Trans. Inf. Theory, vol. 57, pp. 3309–3322, Jun.
2011.

[33] M. Razaviyayn, G. Lyubeznik, and Z.-Q. Luo, “On the degrees of free-
dom achievable through interference alignment in a MIMO interference
channel,” IEEE Trans. Signal Process., vol. 60, pp. 812–821, Feb. 2012.
interference management with
transmitter cooperation,” in Proc. IEEE Int. Symp. Inform. Theory (ISIT),
pp. 846–850, Jun. 2014.

[34] X. Yi and D. Gesbert, “Topological

[35] E. J. Candes, Y. C. Eldar, T. Strohmer, and V. Voroninski, “Phase
retrieval via matrix completion,” SIAM Journal on Imaging Sciences,
vol. 6, no. 1, pp. 199–225, 2013.

[36] D. Papailiopoulos and A. Dimakis, “Interference alignment as a rank
constrained rank minimization,” IEEE Trans. Signal Process., vol. 60,
pp. 4278–4288, Aug. 2012.

[37] P. Jain, R. Meka, and I. S. Dhillon, “Guaranteed rank minimization
via singular value projection,” in Advances in Neural Information
Processing Systems, pp. 937–945, 2010.

