6
1
0
2

 
r
a

M
6

 

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
7
9
8
1
0

.

3
0
6
1
:
v
i
X
r
a

Bias Correction of Semiparametric Long Memory

Parameter Estimators via the Pre-ﬁltered Sieve

Bootstrap∗

D. S. Poskitt, Gael M. Martin† and Simone D. Grose

Department of Econometrics & Business Statistics, Monash University

March 8, 2016

Abstract

This paper investigates bootstrap-based bias correction of semiparametric es-

timators of the long memory parameter, d, in fractionally integrated processes.

The re-sampling method involves the application of the sieve bootstrap to data

pre-ﬁltered by a preliminary semiparametric estimate of the long memory param-

eter. Theoretical justiﬁcation for using the bootstrap technique to bias adjust

log periodogram and semiparametric local Whittle estimators of the memory

parameter is provided in the case where the true value of d lies in the range
0 ≤ d < 0.5. That the bootstrap method provides conﬁdence intervals with the
correct asymptotic coverage is also proven, with the intervals shown to adjust ex-

plicitly for bias, as estimated via the bootstrap. Simulation evidence comparing

the performance of the bootstrap bias correction with analytical bias-correction

techniques is presented. The bootstrap method is shown to produce notable bias

reductions, in particular when applied to an estimator for which some degree of

bias reduction has already been accomplished by analytical means.

MSC2010 subject classiﬁcations: Primary 62M10, 62M15; Secondary 62G09

Keywords and phrases: Bias correction, bootstrap-based inference, fractional process, log periodogram

regression, local Whittle estimator.

∗This research has been supported by Australian Research Council (ARC) Discovery Grant
DP120102344. The authors would like to thank the Editor, a co-editor and two referees for very
detailed and constructive comments on earlier drafts of the paper.

†Corresponding author: Gael Martin, Department of Econometrics and Business Statistics,
Monash University, Victoria 3800, Australia. Tel.:+61-3-9905-1189; fax:+61-3-9905-5474; E-mail:
gael.martin@monash.edu.

Sieve Bootstrap Bias Correction

1

1

Introduction

The so-called long memory, or strongly dependent, processes have come to play an
important role in time series analysis. Long-range dependence, observed in a very

wide range of empirical applications, is characterized by an autocovariance structure
that decays too slowly to be absolutely summable. Speciﬁcally, rather than the au-

tocovariance function declining at the exponential rate characteristic of a stable and
invertible ARMA process, it declines at a hyperbolic rate dependent on a “long mem-
ory” parameter. A detailed description of the properties of such processes can be

found in Beran (1994). Perhaps the most popular model of a long memory process
is the fractionally integrated (I(d)) process introduced by Granger and Joyeux (1980)

and Hosking (1981). This class of processes can be characterized by the speciﬁcation,

y(t) =

∞Xj=0

k(j)ε(t − j) =

κ(z)

(1 − z)d ε(t),

(1)

where ε(t) is zero-mean white noise, z is here interpreted as the lag operator (zjy(t) =

y(t − j)), and κ(z) = Pj≥0 κ(j)zj, κ(0) = 1. For any d > −1 the operator (1 −
the coeﬃcients of k(z) are square-summable (Pj≥0 |k(j)|2 < ∞).

z)d is deﬁned via a binomial expansion and if the “short memory” component κ(z)
is the transfer function of a stable, invertible ARMA process and |d| < 0.5, then
In this case y(t)

is well-deﬁned as the limit in mean square of a covariance-stationary process and
the model is essentially a generalization of the classic Box-Jenkins ARIMA model
(Box and Jenkins, 1970),

(1 − z)dΦ(z)y(t) = Θ(z)ε(t),

(2)

in which we now allow non-integer values of the integrating parameter d and κ(z) =
Θ(z)/Φ(z).

The long-run behaviour of the process in (2) naturally depends on the fractional

integration parameter d. In particular, for any d > 0 the impulse response coeﬃcients
of the Wold representation in (1) are not absolutely summable and, for 0 < d < 0.5,
the autocovariances decline at the rate γ(τ ) ∼ Cτ 2d−1. Such processes have been
found to exhibit dynamic behaviour very similar to that observed in many empiri-
cal time series. See Robinson (2003) for a collection of the seminal articles in the

area and Doukhan, Oppenheim and Taqqu (2003) for a thorough review of theory and
applications.

Statistical procedures for analyzing long memory processes have ranged from the

likelihood-based methods of Fox and Taqqu (1986), Dahlhaus (1989), Sowell (1992)
and Beran (1995), to the semiparametric techniques advanced by Geweke and Porter-Hudak

Sieve Bootstrap Bias Correction

2

(1983) and Robinson (1995a,b), among others. The asymptotic theory for maximum
likelihood estimation of the parameters of such processes is well established, at least

under the assumption of Gaussian errors. In particular, we have consistency, asymp-
totic eﬃciency, and asymptotic normality for the MLE of the fractional diﬀerencing

parameter, so providing a basis for large sample inference in the usual manner. Such
asymptotic results are, however, conditional on correct model speciﬁcation, with the
MLE of d typically inconsistent if either or both the autoregressive and moving average

operators in (2) (or, equivalently, the operator κ(z) in (1)) are incorrectly speciﬁed.
The semiparametric methods aim to produce consistent estimators of d while placing
only very mild restrictions on the behaviour of κ(eıλ) for frequency values λ near zero.
The semiparametric estimators are therefore robust to diﬀerent forms of short-run dy-
namics and oﬀer broader applicability than a fully parametric method. They are also

asymptotically pivotal and have particularly simple asymptotic normal distributions.
Whilst such features place the semiparametric methods at the forefront for use in

conducting inference on d, the price paid for their application is a reduction in asymp-
totic eﬃciency (relative to exact ML) and a slower rate of convergence to the true
parameter (Giraitis, Robinson and Samarov, 1997). Also, despite asymptotic robust-

ness to the short-run dynamics, semiparametric estimators have been shown to exhibit
large ﬁnite sample bias in the presence (in particular) of a substantial autoregressive

component – see Agiakloglou, Newbold and Wohar (1993) and Lieberman (2001) for
examples. In response to these ﬁndings, analytical approaches to reducing the ﬁrst-
order bias of semiparametric estimators have been proposed. Moulines and Soulier

(1999), for example, reduce bias by ﬁtting a ﬁnite number of Fourier coeﬃcients to
the logarithm of the short memory spectrum and constructing a broad-band log peri-
odogram regression (LPR) estimator of d that uses all of the frequencies in the range

(0, π], not just those in a neighborhood of zero. Andrews and Guggenberger (2003)
consider a bias-adjusted estimator of d obtained by including even powers of frequency

as additional regressors in the pseudo regression that deﬁnes the LPR estimator, and
Andrews and Sun (2004) adapt this approach to the semiparametric local Whittle
(SPLW) estimator examined in Robinson (1995a).

As a point of contrast with existing work, the focus of this paper is on the use of the
bootstrap to bias correct semiparametric estimators of the long memory parameter.

As is consistent with the semiparametric approach to estimation of d itself, a semi-
parametric approach to the bootstrap scheme is also adopted, based on the “sieve”
technique. This works by “pre-whitening” the data using an autoregressive approx-

imation, with the dynamics of the process captured in a ﬁtted autoregression (see
Politis, 2003). Provided the order, h, of the autoregression increases at a suitable rate

with T , the convergence rates for the sieve bootstrap are much closer (in fact arbitrar-
ily close) to those for simple random samples. Choi and Hall (2000) demonstrate the
superior convergence performance of the sieve bootstrap over the block bootstrap for

Sieve Bootstrap Bias Correction

3

linear short memory processes, while Poskitt, Grose and Martin (2015) build on the
results of Poskitt (2008) to show that under regularity conditions that allow for I(d)
processes the sieve bootstrap achieves an error rate of Op(T −(1−max{0,d})+β), β > 0, for
the quantiles of the sampling distribution of a general class of statistics that includes

the sample mean and second-order moments.

The current paper uses a modiﬁed sieve bootstrap, wherein a consistent semipara-
metric estimator of the long memory parameter is used to pre-ﬁlter the raw data, prior

to the use of a long autoregressive approximation as the sieve from which bootstrap
samples are produced. As the focus of the paper is on the use of the bootstrap to bias

correct, theoretical results are presented that pertain directly to the accuracy with
which the pre-ﬁltered sieve method estimates the true bias in the relevant estimators
of d. Speciﬁcally, we derive error rates for bootstrap-based estimation of the bias of

√N –CAN (consistent and asymptotically normal); N ∼ KT ν, K ∈ (0,∞), 0 < ν < 1;

estimators that satisfy a requisite Edgeworth expansion, subject to the pre-ﬁltering

value itself converging almost surely to the true value of d at a suﬃcient rate. The
theoretical validity of bootstrap highest probability density (HPD) conﬁdence inter-
vals constructed from the pre-ﬁltered bootstrap replications is also established. To

demonstrate the bootstrap technique we use it to bias correct the LPR and SPLW
estimators, plus the analytically-bias-adjusted variants of Andrews and Guggenberger

(2003) and Andrews and Sun (2004).

Our exposition centers around the short and long memory stationary case, with
the true value of d assumed to lie in the range 0 ≤ d < 0.5. Whilst this may be
deemed to be a limitation of sorts, our key theoretical results are stated in a form that
suggests that they will have more general applicability, subject only to the proviso
that the assumption of stationarity can be relaxed to accommodate more general

processes. For example, non-stationary long memory structures could be catered for
by considering data generating mechanisms driven by fractional noise of the form

s=0 α(d)

n(t) =( Pt−1
s=0Pt−s−1
Pt−1

s ε(t − s),
τ =0 α(1−d)

τ

d ∈ [0, 0.5) ;
ε(t − s − τ ), d ∈ [0.5, 1.5) ,

where α(d)
s , s = 0, 1, 2, . . ., denote the coeﬃcients of the fractional diﬀerence operator
when expressed in terms of its binomial expansion, as in equation (3) below. The
pre-ﬁltered sieve bootstrap could then be applied as described in Section 2 using an
appropriate estimator, such as the quasi (Gaussian) maximum likelihood estimator of

Tanaka (1999) or the exact local Whittle estimator of Shimotsu and Phillips (2005).
The diﬃculty here lies not in the practical implementation of the pre-ﬁltered sieve

bootstrap for such estimators, but in showing that a large-deviations condition nec-
essary to establish the theoretical validity of the method holds – we will return to a
brief discussion of this issue below.

Sieve Bootstrap Bias Correction

4

In addition to the theoretical results, an extensive simulation exercise is undertaken,
in which the bias and mean squared error (MSE) of the bootstrap-bias-adjusted esti-

mators is documented, in comparison with the corresponding statistics both for the
original unadjusted estimators, and the estimators that are adjusted by analytical

methods alone. As a benchmark for the eﬀectiveness of the bias correction we also
present bias and MSE results for the correctly speciﬁed (and hence asymptotically opti-
mal) MLE. The bootstrap bias adjustment is implemented both as a one-step exercise,

and as an iterative procedure, with a stochastic stopping rule invoked to produce the
ﬁnal estimator. The empirical coverage (and average length) of the HPD conﬁdence

intervals is also recorded for all estimators. In accordance with the theoretical results,
we investigate the (relative) performance of the bootstrap bias adjustment using val-
ues of d in the range 0 ≤ d < 0.5 to generate the data in the simulation experiments.
The LPR and SPLW estimators themselves however – both in unadjusted and bias-
adjusted form - are essentially left unconstrained, as there is nothing in the structure

of the pre-ﬁltered sieve bootstrap algorithm per se that requires that the estimator
that is to be bias corrected, or the pre-ﬁlter, be restricted to lie in the 0 to 0.5 range.2
The paper proceeds as follows. Section 2 brieﬂy summarizes the statistical prop-

erties of long memory processes, and outlines the pre-ﬁltered sieve bootstrap in this
context. The bootstrap-based bias-adjustment algorithm is also described in this sec-

In Section 3 we present the key theoretical results, namely the almost sure
tion.
convergence of the bootstrap estimator of the true bias, and the correctness of the
(asymptotic) coverage probability of the bootstrap conﬁdence intervals. The associ-

ated proofs are assembled in Appendix A. Section 4 outlines the iterated version of
the bootstrap bias-adjustment technique, with details of the stochastic stopping rules
invoked therein given in Appendix B. Details and discussion of the simulation study

follow in Section 5, the results of which are tabulated in Appendix C. Section 6 con-
cludes the paper with a summary of our contribution and a discussion of the extension

of the pre-ﬁltered sieve bootstrap methodology to more general processes.

2 Long memory Processes, Autoregressive Approx-

imation, and the Pre-Filtered Sieve Bootstrap

Let y(t) for t ∈ Z denote a linearly regular, covariance-stationary process, with repre-
sentation as in (1), where:

2 The qualiﬁcation ‘essentially’ contained in this statement refers to a deterministic stopping criterion
that supplements two stochastic stopping rules applied, in turn, to the iterative version of the bias-
correction method. This point is discussed further in Sections 4 and 5.1.

Sieve Bootstrap Bias Correction

5

Assumption 1 The transfer function in the representation (1) is given by k(z) =
κ(z)/(1−z)d where d ∈ [0, 0.5) and κ(z) 6= 0, |z| ≤ 1. The impulse response coeﬃcients

of κ(z) satisfy k(0) = 1 andPj≥0 j|κ(j)| < ∞.

Assumption 2 The innovations process ε(t) is an i.i.d. zero mean Gaussian white
noise process with variance σ2.

Assumption 1 serves to characterize the spectral features of quite a wide class of
short and long memory processes, including long-range dependent members of the

ARFIMA family of models that are the focus of this paper. This assumption im-
plies that the innovations in (1) are fundamental ; meaning that ε(t) lies in the space
spanned by current and past values of y(t), and ε(t) and y(s) are uncorrelated for

all s < t. For a discussion of the role of fundamentalness in the context of the au-
toregressive sieve bootstrap see Kreiss, Paparoditis and Politis (2011). Note that the

regularity conditions employed in Kreiss et al. (2011) exclude fractional time series,
but using the extension of Baxter’s inequality to long-range dependent processes due
to Inoue and Kasahara (2006) it is possible to generalize the results of Kreiss et al.

(2011) to time series generated from a fractional transformation of a linear process.
In particular, since the statistics that we investigate are asymptotically pivotal the

results in Kreiss et al. (2011, Section 3) can be extended to the statistics and class of
processes under consideration here.

Assumptions 1 and 2 imply that y(t) is a Gaussian linear process. A basic property

results is that y(t) is linearly regular and the linear predictor ¯y(t) =P∞
whereP∞

of a linear process that underlies the sieve bootstrap methodology and the associated
j=1 π(j)y(t−j) ,
j=0 π(j)zj = (1 − z)dκ(z)−1, is the minimum mean squared error predictor of
y(t) based upon its entire past. The need to invoke Gaussianity is unfortunate but is
unavoidable here as we wish to employ certain results from the existing literature where

the assumption that y(t) is a Gaussian process is adopted. The use of these results
is made explicit in Section 3. It is likely that our results can be extended to more

general linear processes, although the regularity conditions and manipulations needed
for such extensions are liable to be relatively involved. Fay, Moulines and Soulier
(2004), for example, provide a discussion of Edgeworth expansions in the context of

linear statistics applied to long-range dependent linear processes where the innovations
process is i.i.d. zero mean white noise with variance σ2, but Gaussianity is replaced by a
strengthening of the Cram´er condition on the characteristic function of the innovations.

Extensions of the results in Fay et al. (2004) to the LPR estimator are presented in
Fay (2010).3

3 Edgeworth expansions

for quadratic

forms

ory series are developed in Lieberman, Rousseau and Zucker
Lieberman, Rosemarin and Rousseau (2012).

and the MLE in Gaussian long mem-
See also

(2001, 2003).

Sieve Bootstrap Bias Correction

6

j=0 φh(j)γ(j − k) = δ0(k)σ2
h ,

j=1 πh(j)y(t − j) ≡ −Ph

of past observations (MMSEP (h)) is ¯yh(t) =Ph
an autoregression of order h (AR(h)), namely εh(t) =Ph
equationsPh

The minimum mean squared error predictor of y(t) based only on a ﬁnite number h
j=1 φh(j)y(t −
j), where the minor reparameterization from πh to φh allows us, on also deﬁning
φh(0) = 1, to conveniently write the corresponding prediction error in the form of
j=0 φh(j)y(t − j). The ﬁnite-
order autoregressive coeﬃcients φh(1), . . . , φh(h) can be derived from the Yule-Walker
k = 0, 1, . . . , h; in which γ(τ ) = γ(−τ ) =
E[y(t)y(t− τ )], τ = 0, 1, . . ., is the autocovariance function of the process y(t), δ0(k) is
Kronecker’s delta (i.e., δ0(k) = 0 ∀ k 6= 0; δ0(0) = 1), and the minimum mean squared
error is σ2

h = E(cid:2)εh(t)2(cid:3) , the prediction error variance associated with ¯yh(t).

The use of autoregressive models of ﬁnite order h to approximate an unknown (but
suitably regular) process therefore requires that the optimal predictor ¯yh(t) determined
from the AR(h) model be a good approximation to the “inﬁnite-order” predictor ¯y(t)
for suﬃciently large h. The asymptotic validity and properties of AR(h) models when
h → ∞ with the sample size T , under regularity conditions that admit long-range
dependent processes, were established in Poskitt (2007), and we refer the reader to that
paper for more details. That the sieve bootstrap, which uses an AR(h) approximation

to capture the dynamics of y(t) (with h selected optimally) is, accordingly, a plausible
semiparametric bootstrap for a long memory process, was subsequently established in
Poskitt (2008). We focus in this paper on a modiﬁed version of this form of bootstrap.4

2.1 The pre-ﬁltered sieve bootstrap

Let α(d)

j

; d ∈ [0, 0.5); j = 0, 1, 2, . . .; denote the coeﬃcients of the binomial expansion

of the fractional diﬀerence operator (1 − z)d =P∞

j=0 α(d)

j zj,

α(d)
j =

Γ(j − d)

Γ(−d)Γ(j + 1)

; j = 0, 1, 2, . . .

(3)

The pre-ﬁltered sieve bootstrap (PFSB) realizations of y(t) are generated using the

following algorithm:
1. For a given preliminary value df of d calculate the coeﬃcients of the ﬁlter (1−z)df ,

and from the observed data generate the ﬁltered series

wf (t) =

t−1Xj=0

α(df )

j

y(t − j) ,

t = 1, . . . , T .

(4)

4 Andrews, Lieberman and Marmer (2006) examine properties of the parametric bootstrap for the
current class of processes. Our aim in this exercise, however, is to avoid full parametric speciﬁca-
tions and the associated implications of misspeciﬁcation. Recent (non-bootstrap-based) work in
Nadarajah, Martin and Poskitt (2014) indicates that substantial bias can be incurred by various
parametric estimators, including the Gaussian MLE, as a result of misspeciﬁcation, highlighting
that the nature of any misspeciﬁcation would be critical to the performance of associated para-
metric bootstrap procedures.

Sieve Bootstrap Bias Correction

7

2. Fit an autoregression to wf (t) and generate a sieve bootstrap sample w∗f (t),

t = 1, . . . , T , of the ﬁltered data as follows:

¯φh(1), . . . , ¯φh(h) and ˆσ2

(a) Given the ﬁltered series wf (t), t = 1, . . . , T , calculate the parameter estimates,
h, of the AR(h) approximation, and evaluate the residu-
¯φh(j)wf (t− j), t = 1, . . . , T , using wf (1− j) = wf (T − j + 1),

j = 1, . . . , h, as initial values.

(b) Construct the standardized residuals ˜εh(t) = (¯εh(t) − ¯εh)/¯σh, t = 1, . . . , T ,

j=0

als ¯εh(t) =Ph
where ¯εh = T −1PT

t=1 ¯εh(t) and ¯σ2

t=1(¯εh(t) − ¯εh)2.

h = T −1PT

(c) Set ε∗

h(t) = ¯σhe(t), t = 1, . . . , T , where e(t), t = 1, . . . , T , denotes a simple
random sample of T i.i.d. values drawn from the standard normal distribution.

(d) Construct the sieve bootstrap realization w∗f (1), . . . , w∗f (T ) using the au-
h(t), t = 1, . . . , T , initiated at
w∗f (1 − j) = wf (τ − j + 1), j = 1, . . . , h, where τ has the discrete uniform
distribution on the integers h, . . . , T .

toregressive process Ph

¯φh(j)w∗f (t − j) = ε∗

j=0

3. Using the coeﬃcients of the (inverse) ﬁlter (1−z)−df , construct the corresponding
pre-ﬁltered sieve bootstrap draw of the process y∗f (t) = Pt−1
w∗f (t − j),

j=0 α(−df )

t = 1, . . . , T .

j

j=0

The basic, or ‘raw’, sieve bootstrap is equivalent to setting df = 0 in the PFSB; in
which case Steps 1 and 3 are redundant and Step 2 is applied to the raw data y(t). The
properties of the raw sieve bootstrap for fractional processes are given in Poskitt (2008).
Crucially, if ¯φh(z) =Ph
j=0 φh(j)zj when
the sieve bootstrap is applied to the raw data, and y(t) is a linearly regular, covariance-
stationary process that satisﬁes Assumptions 1 and 2, then for all h ≤ HT = a(log T )c,
a > 0, c < ∞, Ph
(See also
Theorem 5 and Corollary 1 of Poskitt, 2007, and the associated discussion.) Given
that the order of magnitude of | ¯φh(z)−φh(z)| is a function of the fractional integration
parameter, it is apparent that convergence of the algorithm must depend on the values
of df and d.5

¯φh(j)zj denotes the estimator of φh(z) =Ph
j=1 | ¯φh(j) − φh(j)|2 = O(cid:0)h(log T /T )1−2 max{0,d}(cid:1) a.s.

Now, when df 6= 0, (1 − z)df y(t) = (1 − z)df −dκ(z)ε(t) has fractional index d − df ,
where by assumption |df − d| = o(1) a.s.; i.e.
the pre-ﬁltering value – presumed
to be estimated from the data, and denoted hereafter by df = df
T accordingly – is
assumed to be a strongly consistent estimator of d. Hence, for any δ > 0 the event
(df
T − d) ∈ Nδ = {x : |x| < δ} will occur with probability one as T → ∞. The error

5 Poskitt et al. (2015) build on Poskitt (2008) to show that under appropriate conditions, and for
particular statistics, a sieve bootstrap generated sampling distribution achieves a convergence rate
of Op(T −(1−max{0,d})+β) for all β > 0 and |d| < 0.5. Obviously, the closer is d to zero the closer the
convergence rate will be to Op(T −1+β), the rate achieved with short memory (and anti-persistent)
processes, and a rate arbitrarily close to that achieved with simple random samples.

Sieve Bootstrap Bias Correction

8

in the AR approximation to wf (t) will accordingly be of order O(h(log T /T )1−2δ)
In Section 3 it is shown that df
or smaller.
T needs to satisfy the large deviations
property |df
T − d| log T → 0 a.s. as T → ∞ in order for this level of accuracy to be
transferred to the pre-ﬁltered sieve bootstrap realizations y∗f (t) of y(t), via the sieve
bootstrap draws w∗f (t) of wf (t). Theoretical results pertaining to the accuracy of the
PFSB algorithm as a mechanism for bias reduction of semiparametric long memory
parameter estimators are then provided.

2.2 Bias correction via the pre-ﬁltered sieve bootstrap

With the conditions on df
T veriﬁed in any particular case, we employ the pre-ﬁltered
sieve bootstrap for the purpose of bias correcting the LPR and SPLW estimators of

the memory parameter, and their analytically adjusted variants. To bias correct any

realizations y∗f

2. Using an appropriate data-based pre-ﬁltering value df = df

chosen estimator, bdT , of d we proceed as follows:
1. Calculate bdT from the data y(t), t = 1, . . . , T .
construct B bootstrap values of the estimator, bd∗f
the estimator bdT for each of the B independent bootstrap draws.
3. Estimate the bias of bdT by
T,B − df

T , produce B bootstrap
b (t), t = 1, . . . , T ; b = 1, 2, . . . , B; of the process y(t). From these
T,b, b = 1, 2, ..., B, by evaluating

T

∗f

T,B = d

where

(5)

(6)

(7)

and produce the bias-adjusted estimator

bb∗f

d

T,b

∗f
T,B = B−1

BXb=1 bd∗f
edT = bdT −bb∗f

T,B.

While there is no fundamental requirement that the pre-ﬁltering value correspond
to the estimator being bias corrected, this correspondence is a natural one to adopt.
As such, df
T is initially taken to represent either the LPR or SPLW estimator, with
or without analytical bias adjustment, according to whichever of these estimators is
the subject of bias correction. With the introduction of an iterative version of this

bias-correction procedure in Section 4, the set of pre-ﬁlters is expanded to include
bootstrap-based bias-corrected versions of the base estimators. The validity of all
such versions of the pre-ﬁlter is established in the following section.

Sieve Bootstrap Bias Correction

9

3 Key Theoretical Results

3.1 Convergence of the bootstrap estimator of bias

Motivated by a consideration of the properties of the LPR and SPLW estimators

that are the focus herein, suppose that bdT (the estimator to be bias corrected) is
an asymptotically pivotal √N –CAN estimator of d where N ∼ KT ν, K ∈ (0,∞),

and, following Hurvich, Deo and Brodsky (1998) and Giraitis and Robinson (2003),
wherein Gaussianity is assumed as it is in the current paper, we now restrict ν to lie

in the range (2/3, 4/5). Let bT denote the ﬁnite sample bias of bdT , that is,

(8)

bT = E[bdT ] − d.

Here E denotes expectation taken with respect to the original probability space (Ω, F, P ).

Now let bd∗f

cess, y∗f (t), t = 1, . . . , T , constructed using the PFSB algorithm where the pre-ﬁltering
value df
equate df
h(t) is Gaussian, it
follows that y∗f (t) will be a fractionally integrated AR(h) Gaussian process with pa-
rameters df
T . Proceeding

T denote the value of bdT calculated from a bootstrap realization of the pro-
T by construction satisﬁes the conditions stated above for bdT , given that we
T to bdT in any particular instance. Since the process ε∗
T is a √N –CAN estimator of df
T and ¯φh(1), . . . , ¯φh(h), and bd∗f
T and E[bdT ] by E∗[bd∗f
T ], we have
T = E∗[bd∗f
T ] − df

as previously, replacing bdT by bd∗f

where E∗ denotes the expectation associated with (Ω∗, F∗, P ∗), the probability space
∗f
T,B in (6) can be made arbitrarily close
induced by the bootstrap process. Given that d
T ] by taking B suﬃciently large, (9) represents the estimator of the true ﬁnite

T , d by df

b∗

T

(9)

sample bias induced by the pre-ﬁltered sieve bootstrap.

The accuracy with which b∗

T estimates bT obviously underpins the validity of the
bootstrap bias correction method. To evaluate the magnitude of |bT − b∗
T| note that
|κ(eıλ)|2 (for κ(·) as deﬁned in (1)) is a bounded, even function of λ, and we have the

power series (McLaurin) expansion |κ(eıλ)|2 = c0 +Pj≥1 cj|λ|2j = c0 + c1|λ|2 + o(|λ|3)

as |λ| → 0. Then, as is shown in Hurvich et al. (1998) and Giraitis and Robinson
(2003), see also Andrews and Guggenberger (2003) and Andrews and Sun (2004),

to E∗[bd∗f

bT = −β

2c1

9c0(cid:18) N
T(cid:19)2

T 2(cid:19)
+ o(cid:18)N 2

(10)

Sieve Bootstrap Bias Correction

10

for the LPR and SPLW estimators, where β > 0. Similarly, set ¯κh(z) =P∞

where the ¯κh(j) and ¯φh(j) are related by the recursion

j=0 ¯κh(j)zj,

¯φh(0) = ¯κh(0) = 1 ,

jXi=0

¯κh(i) ¯φh(j − i) = 0, j = 1, 2, . . . .

(11)

By construction ¯κh(z) ¯φh(z) = 1 for all |z| ≤ 1 and ¯κh(z) yields the AR(h) approxima-
tion to κ(z) implicit in the bootstrap algorithm. Then |¯κh(eıλ)|2 = |Ph
¯φh(j)eıλj|−2 =

¯c0 + ¯c1|λ|2 + o(|λ|3) as |λ| → 0 and

j=0

b∗
T = −β

2¯c1

9¯c0(cid:18)N
T(cid:19)2

T 2(cid:19) .
+ o(cid:18)N 2

(12)

Theorem 1 Suppose that the process y(t) satisﬁes Assumptions 1 and 2. Assume that
df
T is chosen such that |df
T −d| < δT , where δT log T → 0 as T → ∞, and that an AR(h)
approximation is used within the pre-ﬁltered sieve bootstrap, where h ≤ HT = a(log T )c,
a > 0, c < ∞. Assume also that bT = E[bdT ] − d and b∗
T are given by

expressions (10) and (12) respectively. Then

T = E[bd∗f

T ] − df

|bT − b∗

T| = O max(h(cid:18)log T
T (cid:19) 1

2 −δT

, δT h−|d|, δT log T)! + o(cid:18)N 2
T 2(cid:19)

almost surely.

It is obvious from Theorem 1 that |bT − b∗

T| = o(1) a.s., and not surprisingly, that
the rate of convergence of b∗
T to bT induced by the PFSB depends on the order of the
autoregressive approximation (h) and the proximity of the preliminary ﬁltering value
to the true d, that is the value of δT implicit in the choice of df
T . Which term in
2 −δT ) or the O(δT log T ),
Theorem 1 ultimately dominates |bT − b∗
will depend on whether δT → 0 faster or slower than h/(T log T )1/2. Given that the
values of the three Landau “big-Oh”constants that appear in Theorem 1 have not been
quantiﬁed, this indicates that the choice of h and df
T will have an important impact
on both the ﬁnite sample and asymptotic behaviour of bT − b∗
T . Selection of h by AIC
yields h ∼ K log T a.s. as T → ∞, which is asymptotically eﬃcient in the sense of
Shibata (1980); see Poskitt (2008) and (Politis, 2003, §3).

T|, the O(h(log T /T )

Appropriate selection of the pre-ﬁltering value for d is less clear. As noted earlier,
we initially choose as pre-ﬁlters the actual estimators that we are interested in bias
correcting, namely the LPR and SPLW estimators and their analytically bias-reduced

1

variants. Noting that in both cases the analytic bias reduction involves the inclusion
of one or more even powers of frequency in the respective objective functions (see

Andrews and Guggenberger, 2003 and Andrews and Sun, 2004 for details), we desig-
nate the LPR-based and SPLW-based estimators as LPR(P ) and SPLW(P ) respec-

Sieve Bootstrap Bias Correction

11

tively, where P = 0 indicates the original Geweke and Porter-Hudak (1983)/Robinson
(1995a,b) estimators; and P = 1, 2, . . . indicate the corresponding bias-reduced vari-

ants based on the inclusion of P even powers of the frequencies. The limiting distri-
butions of the latter are related to those of the former via a “variance inﬂation factor”
ψ2

P ; that is,

(13)

N 1/2(bdT − d) D→ N(cid:0)0, ω2ψ2
P(cid:1) ,

where ω2 = π2/24 for bdT produced via LPR, ω2 = 1/4 for bdT produced via SPLW,

ψ2
0 = 1 yields the baseline variance for the uncorrected estimator, and ψ2
with P . In particular, ψ2

P increases

2 = 3.52 and ψ2

1 = 2.25, ψ2

3 = 4.79.

That each of these estimators can serve as a legitimate pre-ﬁltering value rests on

the following proposition:

Proposition 1 Let df
P = 0, 1, 2, ... Then under the conditions of Theorem 1 |df
with probability one.

T denote any one of the estimators LPR(P ) or SPLW(P ), with
T − d| log T → 0 as T → ∞

As is made clear in Appendix A, this proposition follows directly for the SPLW(0)

T,B−b∗

estimator from existing results. However, for the remaining estimators detailed proofs
are required. Furthermore, for a bootstrap-bias-adjusted version of an initial estimator
T,B, and adding and subtracting the bootstrap bias before
T|. Since
the bootstrap estimate of bias will obey the law of the iterated logarithm (in B) we

we have edT − d = bdT − d−bb∗f
applying the triangle inequality gives |edT − d| ≤ |bdT − d| + |bb∗f
T| = O(plog log B/B) a.s.. Consistency and asymptotic normality of the
have |bb∗f
T| = o(N −1/2). We therefore conclude that |edT − d| log T ≤
o(1) + log T{O(plog log B/B) + o(N −1/2)} → 0 as T → ∞ for any B ∼ KT β, β > 0,
T = edT can serve as a valid pre-ﬁltering value in a subsequent application

and hence df
of the algorithm. This observation prompts the extension of Section 4, in which

estimator also imply that |b∗

successive bootstrap-bias-adjusted versions of the LPR(P ) and SPLW(P ) estimators
play the role of the preliminary pre-ﬁltering value within an iterative bias-correction
scheme.

T,B − b∗

T| + |b∗

3.2 Asymptotic coverage of bootstrap conﬁdence intervals

The following theorem links the accuracy of the bias estimation to the accuracy with
which the full sampling distribution of the relevant estimator is approximated via

the bootstrap and, hence, to the coverage accuracy of the HPD conﬁdence intervals
computed using the bootstrap draws.

Theorem 2 Set

Pr

∗nN

∗f

T,B) < xo = B−1

BXb=1

1nN

T − d

1

2 (bd∗f

∗f

T,B) ≤ xo .

T,b − d

1

2 (bd∗f

Sieve Bootstrap Bias Correction

12

Then under the conditions of Theorem 1 it follows that

sup

1

1

N

1

∗f

+ rBN

T − d

∗nN

2 (bd∗f

2|bT − b∗
T|
υ√2π

2 (bdT − E[bdT ]) < xo − Pr

x (cid:12)(cid:12)(cid:12)PrnN
T,B) < xo(cid:12)(cid:12)(cid:12) ≤
where the remainder rBN = N 1/2O(plog log B/B) + o(N 5/2/T 2).
Theorem 2 makes it clear that the distribution of N 1/2(bd∗f
T −E∗[bd∗f
proximate the true ﬁnite sampling distribution of N 1/2(bdT −E[bdT ]) provided N 1/2|bT −

T| is suﬃciently small. Given N = KT v for ν ∈ (2/3, 4/5), it follows from Theorem
b∗
1 that N 1/2|bT − b∗
T| → 0, and the accuracy with which the bootstrap-based estimate
of the bias replicates the true bias as N 1/2|bT − b∗
T| approaches zero can be viewed as
a representation of the accuracy with which the pre-ﬁltered bootstrap reproduces the

T ]) will closely ap-

true sampling distribution of the estimator per se. This implies, in turn, that for B
suﬃciently large (B ∼ KT 4/5+β, β > 0) HPD (1 − αU − αL)100% conﬁdence intervals
constructed from B bootstrap draws will have the correct (asymptotic) coverage. To
T,B(αL)),
T,B(αL) denoting the upper and lower quantiles of the narrow-
est interval containing (1 − αU − αL)100% of the bootstrap distribution of the mean
T,B, we
can see that the intervals so constructed correspond to bootstrap centered percentile

wit, use B bootstrap draws to construct the interval (bdT −bq∗f
withbq∗f
corrected values bd∗f

T,B(1−αU ),bdT −bq∗f
T +bb∗f

T,B(1− αU ) andbq∗f

∗f
T,B, b = 1, . . . , B. Noting from (5) that d

conﬁdence intervals that adjust for bias and accommodate possible asymmetry about
the mean.6

∗f
T,B = df

T,b − d

4 An Iterative Bias-Correction Procedure

Although the bias of edT in (7) will be smaller than that of bdT , the remaining bias
E[edT ]− d may still be large because the bias in any preliminary value df
in ﬁnite samples, andbb∗f

T can be severe
T,B in (5) will, as a consequence, be a biased estimate of its true
counterpart bT in (8). To obtain a more accurate estimate of d we propose a further
reﬁnement to the PFSB-based bias-correction procedure via a recursive algorithm
involving two stochastic stopping criteria as follows:

1. Initialization: Set k = 0 and assign desirable tolerance levels τ1 = τ (0)
1

and τ2 =

τ (0)
2

for the two stopping rules. For the chosen estimator bdT , set ed(0)
set df = bdT ).

2. Recursive Calculation: For the kth iteration set the preliminary value of d,
T and perform the second and third steps of the bias-correction

T = bdT (i.e.

namely df

T , to ed(k)

6 See, inter alia, Hansen (2014, Chapter 10) and van der Vaart (1998, Chapter 23) for discussions

of bootstrap conﬁdence intervals and their associated properties.

Sieve Bootstrap Bias Correction

13

T , producing, in an obvi-

T

procedure of Section 2.2 with bdT therein replaced by ed(k)
ous notation, ed(k+1)
T −eb∗f (k)
T −ed(k)

update the tolerance levels τ1 = τ (k)

and τ2 = τ (k)

T,B .

T

1

2

T and stop.

, and repeat Step 2. Otherwise

T −eb∗f (k)
= ed(k)
3. Stopping Rules: If(cid:12)(cid:12)(cid:12)ed(k+1)
T (cid:12)(cid:12)(cid:12) > τ1 and(cid:12)(cid:12)(cid:12)ed(0)
T,B (cid:12)(cid:12)(cid:12) > τ2 set k = k+1,
−ed(k)
set edT = ed(k)
The rationale behind the recursions is as follows: since the estimator df = bdT is
biased,bb∗f
edT will therefore still contain some bias. Replacing the initial values bdT = ed(0)
bb∗f
T,B = eb∗f (0)
T,B , and (for general k) ed(k−1)
by ed(k)
eb∗f (k)
upon more accurate estimators, the updated estimate ed(k)

T,B will on average be a biased estimate of bT , and the bias-adjusted estimate
T and
and
T,B , and so on, produces more accurate estimates and bias assessments. Being based
T would be expected to be
closer to the true value of d. The procedure is iterated until no meaningful gain in

T and eb∗f (1)

and eb∗f (k−1)

T,B by ed(1)

accuracy is achieved. Details of the two stochastic criteria used to determine when
suﬃcient accuracy has been attained are given in Appendix B. Some further comment
on stopping rules is also included in the section following.

T,B

T

T

5 Simulation Exercise

5.1 Simulation design

In this section we illustrate the performance of the bootstrap-bias-corrected estima-

tors via a Monte Carlo experiment. Following Andrews and Guggenberger (2003) we
simulate data from an ARFIMA(1, d, 0) process,

(1 − L)dΦ(z)y(t) = ε(t) , 0 ≤ d < 0.5 ,

(14)

where Φ(z) = 1 − φz is the operator for a stationary AR(1) component and ε(t) is
zero mean white noise, assumed initially to be Gaussian. The choice of this model

is motivated, in part, by earlier work that highlights the distinct ﬁnite sample bias
of the LPR estimator of d in this setting, when the value of φ is positive and large

(See Agiakloglou et al., 1993). Indeed, Andrews and Guggenberger (2003) document
substantial remaining bias in the bias-corrected version of the LPR estimator in the
presence of a large autoregressive parameter. The impetus for applying bootstrap-

based bias correction to the various estimators is accordingly particularly strong in
this setting.

The process in (14) is simulated R = 1000 times for d = 0.0, 0.2, 0.3, 0.4; φ =
0.3, 0.6, 0.9; and sample sizes T = 100 and 500 via Levinson recursion applied to the
autocovariance function of the desired ARFIMA(p, d, q) process and the generated

Sieve Bootstrap Bias Correction

14

pseudo-random ε(t) (see, for instance, Brockwell and Davis, 1991, §5.2). The ACF
for given T , φ, θ, and d is calculated using Sowell’s (1992) algorithm as modiﬁed by

Doornik and Ooms (2001).

The estimators to which we apply the iterative bias-correction procedure of Section
4 are LPR(P ) and SPLW(P ), P = 0, 1, 2, implemented with bandwidth N = T 0.7.
The value of N accords with common practice, with the exponent falling within the
(2/3, 4/5) range.7 The order (h) of the autoregressive approximation underlying the
sieve component of the bootstrap algorithm is chosen via AIC, and Burg’s algorithm is
used to estimate the autoregressive parameters. The number of bootstrap realizations

is B = 1000.

We compute the empirical bias and MSE for the original estimators prior to
bootstrap-based bias correction (i.e., LPR(P ) and SPLW(P ), P = 0, 1, 2), and for

the bootstrap-bias-corrected versions thereof. The latter are produced through formal
application of the stochastic stopping rules described in Appendix B, augmented by
< −1 or ≥ 1.5
retained as the ﬁnal choice. We also report bias and MSE
results for the bootstrap-bias-corrected estimators based on the ﬁrst two iterations of

a deterministic criterion, whereby the iterative scheme ceases if ed(k+1)
and the estimator ed(k)

T

T

the iterative procedure. This comparison of the sampling properties of estimators with
varying degrees of analytical bias correction with those of estimators that exploit the

bootstrap bias adjustment, allows us to investigate, ﬁrstly, the eﬃcacy of using the
bootstrap method rather than an analytical method to bias adjust; and, secondly, the
possibility of obtaining additional improvement by bias-correcting (via the bootstrap)

an estimator that has already been bias-adjusted via analytical means. Finally, as a
reference for the magnitude of the bias and MSE of the various raw and bias-adjusted
semiparametric estimators, we record the corresponding statistics for the correctly
speciﬁed MLE.8

We also compute the empirical coverage and length of nominal 95% HPD inter-

vals obtained by applying the bootstrap procedure to the estimators LPR(P ) and
SPLW(P ), P = 0, 1, 2. For each of the R Monte Carlo replications the intervals are
constructed as described in Section 3, each in turn based on B bootstrap draws, and

the empirical coverage is calculated as the proportion of times (in R replications) that
each interval includes the true value of d. The empirical length of the intervals is

7 A lower bound of N ∼ KT 2/3 reﬂects the fact that unless N increases suﬃciently quickly with T
terms of order O(log3 N/N ) in the expansions of bT and b∗
T compete with the terms in (10) and (12);
and the upper bound reﬂects that the estimators are known to be rate optimal when N ∼ KT 4/5
in the uncorrected case (Giraitis et al., 1997) and N ∼ KT (4+4P )/(5+4P ) in the corrected case
(Andrews and Guggenberger, 2003; Andrews and Sun, 2004), although asymptotic normality of
the estimators requires that N = o(T 4/5).

8 Numerical evidence presented in Nielsen and Frederiksen (2005) suggests that semiparametric es-
timators can be competitive with correctly speciﬁed parametric methods. Comparison of the
performance of the semiparametric estimators with that of the correctly speciﬁed, and hence
asymptotically optimal, MLE is therefore of interest.

Sieve Bootstrap Bias Correction

15

recorded as their average length across the R replications. These coverage and length
statistics are compared with the empirical coverage and (constant) length of 95% con-

ﬁdence intervals constructed from the appropriate asymptotic distributions in (13).
Results did not vary markedly with d, and hence are presented averaged over the four
values of d considered.9

In line with the assumption of Gaussianity, thus far we have supposed that the boot-
strap innovations generated in Step 2(c) of the PFSB algorithm are i.i.d. N (0, ¯σ2
h).
Such bootstrap realizations are said to be generated via a parametric bootstrap. Non-
parametric bootstrap innovations can be generated using the following modiﬁcation of

Step 2(c):

2(c′) Let ε+

h (t), t = 1, . . . , T , denote a simple random sample of i.i.d. values drawn
t=1 1{˜εh(t) ≤ e}, the probability distribution function
h(t) =

that places a probability mass of 1/T at each of ˜εh(t), t = 1, . . . , T . Set ε∗
¯σhε+

from U˜εh,T (e) = T −1PT

h (t), t = 1, . . . , T .

The innovations generated by the nonparametric bootstrap are i.i.d. (0, ¯σ2
h) by con-
struction, and when y(t) is Gaussian we can expect ε∗
h(t), t = 1, . . . , T , and hence
y∗f (t), to be approximately Gaussian. This suggests that replacing the innovations
generated in PFSB-2(c) by those generated in 2(c′) should not produce outcomes that
are substantially diﬀerent, and we document this by presenting some selected results
in which Gaussianity is retained for the data generating process, but nonparametric
bootstrap innovations are generated as per 2(c′) above. PFSB-2(c′) also caters for
the possibility that y(t) is a linear process with innovations that do not satisfy As-

sumption 2, and so allows us to examine the robustness of our results to violations of
the assumption of Gaussianity. Accordingly, we report some selected results obtained
using the nonparametric pre-ﬁltered bootstrap with a Student t distribution adopted

for ε(t).

In summary, Tables 1-5 record results based on the parametric version of the boot-

strap, with Gaussian errors adopted in (14); Tables 6 and 7 record selected results
based on the replacement of Step 2(c) in the PFSB algorithm with the nonparametric
2(c′) with Gaussian errors retained; while the results recorded in Tables 8 and 9 use
the nonparametric version of the bootstrap and assume Student t innovations. Note
that for brevity speciﬁc results for d = 0.3 are omitted from Tables 1-4, while the
results in the subsequent tables are reported after averaging over all four values of d,

including d = 0.3. To shed some light on the eﬀect of misspeciﬁcation on the relative
performance of the semi-parametric estimators (with the nonparametric version of the

bootstrap used) and the MLE, the results for the Gaussian MLE under Student t

9 Analogous results for nominal 90% HPD intervals were found to be qualitatively similar in all cases

and, hence, are not reported or explicitly discussed.

Sieve Bootstrap Bias Correction

16

innovations are summarized in the ﬁnal column of Tables 8 and 9. To aid interpreta-
tion, the MLE results under Gaussian innovations (as recorded in Tables 1-4) are also

averaged across the four d values and reported in the ﬁnal columns of Tables 6 and
7. All tables are included in Appendix C, with the most favorable result (within the

semiparametric set) for each design setting highlighted. The columns headed ‘SSR’ in
certain of the tables record the results based on the stochastic stopping rules discussed
in Appendix B and modiﬁed as described above.

5.2 Simulation results: LPR

Tables 1 and 2 record (for T = 100 and T = 500 respectively) the bias and MSE
results for all estimators based on the LPR method, using the parametric version of

the bootstrap, and with Gaussian errors adopted in (14). All results pertaining to the
use of the bootstrap to bias adjust LPR(P ) are indicated by appending ‘-BBA(K)’
to the LPR acronym, where K is the number of times the bootstrap-bias-correction
procedure is applied.10 The key message is that the bootstrap technique does reduce
bias, but with the most substantial gains to be had by using the bootstrap algorithm

to bias adjust an estimator that has already been bias reduced analytically. For ex-
ample, for T = 100, and for eight of the nine cases, the smallest bias is produced by
bias adjusting (via the bootstrap) either the LP R(1) or LP R(2) estimator at least

once. For T = 500, the same qualitative result holds, with LPR(2)-BBA(1) being
the least biased estimator overall. Importantly, for T = 500 at least, the reduction

in bias is so substantial that this estimator also has the lowest MSE of all estimators
(including those not bias adjusted) for φ = 0.9 and all values of d. Moreover, even
when the bootstrap bias adjustment does cause the (expected) increase in MSE, it is
not excessive.11

The results recorded in Table 1 indicate that the stopping rules are useful for
the smaller sample size, producing estimators with the smallest bias in seven cases.

For LP R(1), for which results for both K = 1 and 2 are recorded, the MSE for
the SSR method is seen to fall in-between the corresponding ﬁgures based on these

ﬁxed numbers of applications of the bootstrap in virtually all instances. The results
in Table 2, however, demonstrate that for T = 500 a ﬁxed number of bootstrap-
based bias adjustments is preferable overall, with the SSR method yielding less gains.

Hence, and with due consideration taken of the limitations of the experimental design,

10 That is, K = 1 refers to the single application of the bias-correction procedure without iteration,
while K = 2 (3, etc.) corresponds to k = 1 (2, etc.) iterations in the iterative version of the
algorithm.

11 Note that all versions of the LPR estimator, including the bootstrap-bias-corrected versions, are
very biased when φ = 0.9. This conﬁrms (as documented in the literature op. cit.) that semi-
parametric estimators experience problems in this part of the parameter space. The use of our
procedure does, however, reduce the bias, indicating that even in this worst case scenario appre-
ciable gains can be made.

Sieve Bootstrap Bias Correction

17

we can conclude that although a stopping rule tailors the number of iterations to the
realization at hand, its use does not appear to guarantee an improvement in overall

performance compared to using a ﬁxed number of iterations, at least when the sample
is reasonably large. Note that the ﬁnding that the bias results for the bootstrap-

bias-adjusted estimators are superior overall also applies to the results based on the
nonparametric version of the PFSB algorithm, and under both the Gaussian and
Student t errors, as can be seen by the location of the highlighted ﬁgures in Tables 6

and 8 respectively.

Another result of interest pertains to the relationship between the overall accu-

racy of the bootstrap-bias-corrected estimators (as measured by MSE) and that of
the comparable analytically-adjusted estimators. For instance, K bias adjustments of
an LPR(P ) estimator via the bootstrap can – for some designs – yield an estimator

with a smaller MSE than does the equivalent number of analytical bias adjustments.
In certain cases this reduction in MSE goes hand in hand with a smaller bias.12 We
return to this point in Section 5.4. With regard to overall accuracy, our results are also
in accord with the ﬁndings of Nielsen and Frederiksen (2005) in that, in the absence
of persistent short-run dynamics (i.e. φ 6= 0.9), the bootstrap-bias-corrected semi-
parametric estimators often exhibit signiﬁcant bias reduction relative to the correctly
speciﬁed MLE. That this improvement is at the expense of an increase in MSE relative

to the MLE is perhaps not surprising given that the correctly speciﬁed MLE is (asymp-
totically) optimal. From a comparison of the results recorded in the ﬁnal column of
Tables 6 and 8 it is evident that misspeciﬁcation of the innovations as Student t has

little impact on the performance of the Gaussian MLE. Hence, the qualitative nature
of the comment made above regarding the relative performance of the semiparametric
and parametric estimators continues to obtain.

Finally, the most notable characteristic of the HPD results in Panel A of Table
5 (produced under the parametric algorithm) is the improvement in coverage yielded

by the bootstrap technique, relative to that yielded by the relevant asymptotic distri-
bution. In particular, use of the PFSB distributions produces intervals with close to
correct coverage for the low and moderate values of φ, and for the estimators based on
P ≥ 1. Unsurprisingly, while for any particular LPR(P ) estimator (i.e. for any given
value of P ) the improvement in coverage accuracy is accompanied by an increase in

interval width, this decrease in precision is not excessive. In both the bootstrap and
asymptotic cases, an increase in P tends to lead to an improvement in coverage accu-
racy, but at the expense (as would be expected) of an increase in interval width, due

to the larger variance of the underlying estimator.

12 For example, for T = 100, d = 0.4 and φ = 0.3 : the LP R(1)-BBA(1) estimator has an MSE that
is notably less than that of the ‘matching’ LP R(2) estimator, at the cost of only a small increase
in bias. For the same parameter design, but with T = 500, the ﬁrst estimator has both smaller
bias and smaller MSE than the second.

Sieve Bootstrap Bias Correction

18

5.3 Simulation results: SPLW

Tables 3 and 4 record (for T = 100 and 500 respectively) the bias and MSE results for
all estimators based on the SPLW method (with the postﬁx ‘-BBA(K)’ used to indicate

bootstrap bias adjustment as described above). As with the LPR-based estimators,
the bootstrap-based bias adjustment yields the largest bias reductions overall, but (in

all but one case) only when applied to an SPLW estimator that has already been
analytically bias adjusted. Most notably, the SSR method, speciﬁcally as applied to
SPLW(2), yields the best bias reductions overall, and for both sample sizes; although as

was the case for the LPR results, it does not guarantee an improvement in performance
over using a ﬁxed number of iterations.

The biases of all SPLW-based estimators are broadly similar in magnitude to those
of the comparable LPR-based estimators, and as with the LPR-based estimators,
the reduction in bias produced by the bootstrap technique (in certain cases) is not

obtained at the expense of MSE. Once again, the qualitative results regarding bias
adjustment still hold when the nonparametric form of the bootstrap is used, and when
Student t errors feature in the true DGP, rather than Gaussian errors, as seen from

the results recorded in Tables 7 and 9 respectively. The MSE results demonstrate
that the increase in precision sometimes yielded by the bootstrap over and above a

comparable number of analytical adjustments, in the case of LPR estimator, continues
to obtain in the SPLW case, also at times allied with a reduction in bias. As with the
LPR results, the best performing bootstrap-bias-corrected SPLW-based estimators are

often substantially less biased than the MLE (both correctly and incorrectly speciﬁed),
for φ 6= 0.9 at least, but at the expense of MSE as expected. Once again, the bootstrap
yields HPD intervals with notably better coverage than those associated with the
asymptotic distribution, but at some cost in precision.

5.4 Retrospective

Our simulation results raise the question of how the sieve bootstrap as implemented
in the PFSB algorithm is able to bias correct the basic LPR and SPLW estimators,
and the analytically-bias-adjusted versions thereof, without necessarily incurring a

substantial, if any, loss in overall precision.

By way of explanation for this phenomenon, consider the LPR estimator. This

estimator is commonly motivated by observing that

IT (λj)2π|1 − e−ıλj|2d

σ2|κ(eıλj )|2

D→ Vj ,

(15)

wherein IT (λ) denotes the periodogram and Vj is distributed exponentially when d = 0,
and as an unequally weighted sum of independent χ2(1) random variables when d 6= 0

Sieve Bootstrap Bias Correction

19

(Hurvich and Beltrao, 1993, Theorem 6). Taking logarithms in (15) and using the
approximation |1 − e−ıλ|2d = |λ|2d(1 + o(1)) as λ → 0 leads to the linear regression
model

log(IT (λj)) = α0 − 2d log(λj) + ηj,

(16)

where E[ηj] = 0 and the intercept α0 is presumed to capture the eﬀects of the adjust-
ments

aj = log |κ(1)|2 + log(cid:18)|κ(eıλj )|2
= log |κ(1)|2 − Cj + O(N 2/T 2)

|κ(1)|2 (cid:19) − d log(cid:18)|1 − e−ıλj|2

λ2
j

for all

1 ≤ j ≤ N ,

(cid:19) − Cj

(17)

(18)

where the mean correction term Cj ≤ 0.577216 (Euler’s constant).13 The presumption
that α0 absorbs the eﬀects of the adjustment terms assumes the aj approach a constant
as T increases and (see the discussion in Hurvich and Beltrao, 1993) it is the failure
of this assumption that can be a source of bias.

The analytical correction replaces the simple regression in (16) by the multiple

regression

αpλ2p

j − 2d log(λj) + ηj ,

(19)

log(IT (λj)) =

PXp=0
the rationale being that the termPP

j provides a better approximation to the
Maclaurin series expansion of the right hand side of (17) than supposing aj is constant
in a neighbourhood of zero. The introduction of λ2p
j , p = 1, . . . , P , in (19) reduces the
bias in the estimate of d, but it is also the presence of these additional regressors that

p=0 αpλ2p

causes the variance inﬂation seen in (13).

The pre-ﬁltered bootstrap, on the other hand, takes the speciﬁcation of the re-

gression in (16) or (19) as given and adjusts the estimator by mimicking the sam-

pling behaviour of the regressand. Recall that IT (λ) = (2π)−1PT −1
r=1−Tbγ(r)eıλr where
bγ(r) =bγ(−r), r = 0, 1, . . . , T−1, denotes the sample autocovariance function. Hosking
(1996) shows that even in moderate to large samples the bγ(r) have substantial neg-

ative bias relative to the true autocovariances when d is large. The bootstrap works

by reducing the value of the fractional integration parameter in the “data” to which
the sieve bootstrap is applied, via the pre-ﬁltering procedure. This reduces the afore-
mentioned bias. The reduction in d also increases the proximity of the Cj in (18) to
Euler’s constant and renders the ηj in (16) and (19) closer to centered Gumbel random
variables with variance π2/6. Whether it is applied to (16) or (19), the bootstrap is
thereby able to attack the problem of bias in the estimation of d without compromis-
ing (indeed, reinforcing) the fundamental result assumed to underlie log periodogram

13 The expression in (18) follows as a consequence of the fact that log(|κ(eıλ)|2/|κ(1)|2) = log(1 +

(c1/c0)|λ|2 + o(|λ|3)) and log(|1 − e−ıλ|2/λ2) = log(1 − (1/12)|λ|2 + o(|λ|3)) as λ → 0.

Sieve Bootstrap Bias Correction

20

regression and determine the estimators’ variance, namely, the pivotal nature of the
ratio in (15).

Although the underlying reasoning is somewhat heuristic, the previous arguments
provide a straightforward explanation of how the pre-ﬁltered bootstrap is able to

exhibit the type of creditable performance observed in simulation when it is used to
bias correct the LPR estimator. Similar arguments can also be employed to explain the
performance characteristics seen when the algorithm is applied to the SPLW estimator.

6 Discussion

This paper has developed a bootstrap method for bias correcting semiparametric esti-

mators of the long memory parameter in fractionally integrated processes. The method
involves applying the sieve bootstrap to data pre-ﬁltered by a preliminary semipara-

metric estimate of the long memory parameter. In addition to providing theoretical
(asymptotic) justiﬁcation for using the bootstrap techniques to bias correct, we doc-
ument the results of simulation experiments, in which the ﬁnite sample performance

of the bootstrap-bias-corrected estimators is compared with that of both unadjusted
estimators and estimators adjusted via analytical means. The numerical results are

very encouraging, and suggest that the bootstrap bias correction can yield accurate
inference about long memory dynamics in the types of samples that are encountered
in practice – most notably when applied to estimators for which some preliminary

analytical bias reduction has been used. The bootstrap method is also shown to
yield (asymptotically) valid conﬁdence intervals that formally adjust for bias, with the
empirical coverage of the bootstrap intervals being much closer to the nominal level

than is the coverage of intervals based on the asymptotic distributions of the relevant
semiparametric estimators.

Our discourse has focused on stationary long memory fractional processes, but as
noted previously the pre-ﬁltered sieve bootstrap algorithm does not restrict either the
estimator that is to be bias corrected or the pre-ﬁlter to lie in the interval [0, 0.5).

The broader applicability of the pre-ﬁltered sieve bootstrap to more general processes,
and to estimators and pre-ﬁlters capable of handling this generality, is therefore only

contingent on establishing its theoretical validity, and as is apparent from Theorems
1 and 2 this hinges on showing that the pre-ﬁltering value df
T −
d| log T = o(1) a.s.. If N 1/2(df
T − d) were exactly N(0, υ) then it would follow from the
tail area properties of the normal distribution that this condition would be satisﬁed.
Unfortunately, approximate Gaussianity associated with a pre-ﬁltering value df
T being
a √N -CAN estimator of d – as has been established for the more general estimators
of Tanaka (1999) and Shimotsu and Phillips (2005) – is not suﬃcient to derive the
required result because departures of (df
T − d) from zero that are inconsequential for

T is such that |df

Sieve Bootstrap Bias Correction

21

weak convergence need not be so for large-deviation probabilities. Nevertheless, the
necessary large-deviation property can be derived on a case by case basis, as we have

demonstrated for the LPR and SPLW estimators for the stationary case. It therefore
seems reasonable to suppose that arguments similar to those employed in the proof
of Proposition 1 can be used to show that the condition |df
T − d| log T = o(1) a.s.
will also be satisﬁed by the aforementioned more general estimators, and under more
general data generating processes, but demonstration of this is beyond the scope of

the current paper. We hope to extend our results on the pre-ﬁltered sieve bootstrap
to the non-stationary case in future research.

A Proofs

Proof of Theorem 1: Simple algebraic manipulation applied to (10) and (12) gives

us the following bound

|bT − b∗

c1

T| =(cid:12)(cid:12)(cid:12)(cid:12)
≤(cid:18)(cid:12)(cid:12)(cid:12)(cid:12)

¯c1
¯c0 −
c1(¯c0 − c0)

c0(cid:12)(cid:12)(cid:12)(cid:12) O(cid:18)N 2
T 2(cid:19) + o(cid:18)N 2
T 2(cid:19)
(cid:12)(cid:12)(cid:12)(cid:12) +(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:19) O(cid:18) N 2

(¯c1 − c1)

c0¯c0

¯c0

T 2(cid:19) + o(cid:18)N 2
T 2(cid:19) .

(20)

From the ﬁrst term on the right-hand-side of (20) it can be seen that the order of
magnitude of |bT − b∗
T| depends on that of (¯c0 − c0) and (¯c1 − c1), and that the larger
the bandwidth N, with the attendant increase in bias, the closer the approximation
|¯κh(eıλ)|2 invoked by the algorithm needs to be to the true |κ(eıλ)|2, for |¯κh(eıλ)|2 −
|κ(eıλ)|2 = (¯c0 − c0) + (¯c1 − c1)|λ|2 + o(|λ|3) in a neighbourhood of the origin. Since,
trivially, 2βN 2/9T 2 = O(N 2/T 2), in order to establish the theorem it is suﬃcient to
show that |¯c0 − c0| and |¯c1 − c1| are of order O (T 2MT /N 2) or smaller where MT =
max{h( log T
2 −δT , δT h−|d|, δT log T}. The magnitude of both (¯c0 − c0) and (¯c1 − c1) can
T )
be derived from the following lemma.

1

h(1), . . . , φf

Lemma 1 Assume that the conditions of Theorem 1 hold. Let φf
where φf
in (4), with df = df
κf (z) = κ(z)/(1− z)d−df
¯φh(z) by those of φf

h(j)zj
h(h) denote the coeﬃcients in the MMSEP(h) of the process wf (t)
h denote the minimum mean squared error. Set
h(z)}−1 by replacing the coeﬃcients of

h(z) in the recursions in equation (11).

T , and let σf 2

h(z) = {φf

T and deﬁne κf

j=0 φf

h(z) =Ph

Then

lim

T →∞(cid:12)(cid:12)|¯κh(eıλ)|2 − |κ(eıλ)|2(cid:12)(cid:12) ≤ ν1,T + ν2,T + ν3,T

Sieve Bootstrap Bias Correction

22

where for all λ ∈ [2π/T, 2πN/T ]

h(eıλ)|2(cid:12)(cid:12)(cid:12) = O(h(log T /T )
ν1,T =(cid:12)(cid:12)(cid:12)|¯κh(eıλ)|2 − |κf
h(eıλ)|2 − |κf (eıλ)|2(cid:12)(cid:12)(cid:12) = O(δT h−|d|)
ν2,T =(cid:12)(cid:12)(cid:12)|κf
ν3,T =(cid:12)(cid:12)|κf (eıλ)|2 − |κ(eıλ)|2(cid:12)(cid:12) = O(δT log T )

1

2 −δT )

and

with probability one.

Proof of Lemma 1: Addition and subtraction, and straightforward manipulation,
yields

|¯κh(eıλ)|2 − |κ(eıλ)|2 =(cid:16)|¯κh(eıλ)|2 − |κf

h(eıλ)|2(cid:17) +(cid:16)|κf
+(cid:0)|κf (eıλ)|2 − |κ(eıλ)|2(cid:1) .

Consider the ﬁrst term in (21), |¯κh(eıλ)|2 − |κf

h(eıλ)|2. By deﬁnition

h(eıλ)|2 − |κf (eıλ)|2(cid:17)

(21)

¯κh(z) − κf

h(z) =

h(z) − ¯φh(z)
φf
¯φh(z)φf
h(z)

,

and since ¯φh(z) 6= 0 and φf

h(z) 6= 0, |z| ≤ 1, there exists an ǫ > 0 such that

But

for all

|z| ≤ 1 .

≤ ǫ−2

(cid:12)(cid:12)(cid:12)¯κh(z) − κf

h(z) − ¯φh(z)(cid:12)(cid:12)(cid:12)
h(z)(cid:12)(cid:12)(cid:12) ≤ ǫ−2(cid:12)(cid:12)(cid:12)φf
h(j) − ¯φh(j)(cid:12)(cid:12)(cid:12)
hXj=0(cid:12)(cid:12)(cid:12)φf
h(j) − ¯φh(j)(cid:12)(cid:12)(cid:12) ≤ h
hXj=0(cid:12)(cid:12)(cid:12)φf
hXj=0

2

h(j) − ¯φh(j)|2! 1
|φf
= O h(cid:18)log T
T (cid:19) 1
= O h(cid:18)log T
T (cid:19) 1

2 −δT! a.s.

2 (1−2 max{0,d−df

T })!

by an application of Poskitt (2007, Theorem 5) and the fact that |df
assumption. It follows that |¯κh(eıλ) − κf

T − d| < δT by
2 −δT ) a.s. uniformly in
2 −δT ) a.s. uniformly in λ.
We can therefore interchange limit operations (Apostol, 1960, Theorem 13.3) to give

h(eıλ)| = O(h(log T /T )

λ, and hence that (cid:12)(cid:12)(cid:12)|¯κh(eıλ)|2 − |κf
λ→0(cid:12)(cid:12)(cid:12)|¯κh(eıλ)|2 − |κf

h(eıλ)|2(cid:12)(cid:12)(cid:12) = O(h(log T /T )
h(eıλ)|2(cid:12)(cid:12)(cid:12) = lim

T →∞(cid:12)(cid:12)(cid:12)|¯κh(eıλ)|2 − |κf

h(eıλ)|2(cid:12)(cid:12)(cid:12) ,

lim
T →∞

lim

lim

λ→0

1

1

Sieve Bootstrap Bias Correction

23

which implies that ν1,T = O(h(log T /T )

1

2 −δT ) a.s. for all λ ∈ [2π/T, 2πN/T ].

For the second term in (21), |κf

h(eıλ)|2 − |κf (eıλ)|2, we have

κf
h(z) − κf (z) =

1 − κf (z)φf
φf
h(z)

h(z)

,

giving us the bound

for all

|z| ≤ 1 .

h(z). Then from Parseval’s relation

(cid:12)(cid:12)(cid:12)κf
h(z) − κf (z)(cid:12)(cid:12)(cid:12) ≤ ǫ−1(cid:12)(cid:12)(cid:12)1 − κf (z)φf
h(z)(cid:12)(cid:12)(cid:12)
Let ρh(z) =Pj≥1 ρh(j)zj = 1 − κf (z)φf
ρh(j)2 =Z π
−π(cid:12)(cid:12)(cid:12)1 − κf (eıλ)φf
h(eıλ)(cid:12)(cid:12)(cid:12)
dλ = 2πσ−2(cid:16)σf 2
Xj≥1
h − σ2 =P∞

and from the Levinson–Durbin recursions (Durbin, 1960; Levinson, 1947) we have
σf 2
h = (1 − φf
h =
h(h)2σf 2
σf 2
h+1 + φf
r , from which
we obtain the bound

h(h)2)σf 2
h leads to the series expansion σf 2

h−1. Substituting sequentially in the recurrence formula σf 2

h − σ2(cid:17)

r (r)2σf 2

r=h φf

2

Xj≥1

ρh(j)2 ≤ 2πσ−2E(cid:2)wf (t)2(cid:3) ∞Xr=h

r (r)2 .
φf

But φf
therefore we can infer that

h(h) ∼ |df

T − d|/h as h → ∞ (Inoue, 2002; Inoue and Kasahara, 2004) and

ρh(j)2 ≤ const.|df

T − d|2
h2|d|

ζ(2(1 − |d|)),

Xj≥1

where ζ(·) denotes the Riemann zeta function. It follows that limh→∞ ρh(eıλ) = 0 and
that limT →∞ |ρh(eıλ)|2 = O(δ2
T h−2|d|) almost everywhere on [−π, π]. Hence we can con-

clude that limT →∞ limλ→0(cid:12)(cid:12)(cid:12)|κf

and hence that ν2,T = O(δ2

T h−2|d|).

h(eıλ)|2 − |κf (eıλ)|2(cid:12)(cid:12)(cid:12) = limλ→0 limT →∞(cid:12)(cid:12)(cid:12)|κf
T −d) − 1(cid:17) .

|κf (eıλ)|2 − |κ(eıλ)|2 = |κ(eıλ)|2(cid:16)|1 − eıλ|2(df

The third and ﬁnal term in (21) is

h(eıλ)|2 − |κf (eıλ)|2(cid:12)(cid:12)(cid:12)

(22)

2 log|λ| + log(1 + o(|λ|)) as λ → 0, we can deduce that

Substituting |1 − eıλ|2(df
expansion |1 − e−ıλ|2 = 2P∞
(cid:12)(cid:12)(cid:12)|κ(eıλ)|2(|1 − eıλ|2(df

T −d) = exp{(df

T − d) log|1 − eıλ|2} into (22) and using the
j=1(−1)j−1|λ|2j/(2j)!, which implies that log |1 − eıλ|2 =

T −d) − 1)(cid:12)(cid:12)(cid:12) ≤ { sup

[−π,π]|κ(eıλ)|2}(cid:12)(cid:12)(cid:12)expn2(df

T − d) log|λ| + o(|λ|)o − 1(cid:12)(cid:12)(cid:12)

Sieve Bootstrap Bias Correction

24

as λ → 0. Furthermore, by assumption |df
and since | exp(x) − 1| = |x| · |1 + 1
it follows that

T − d| ≤ δT where δT log T → 0 as T → ∞,
2x + o(|x|)| for x in a neighbourhood of the origin,

(cid:12)(cid:12)(cid:12)|κ(eıλ)|2(|1 − eıλ|2(df

T −d) − 1)(cid:12)(cid:12)(cid:12) ≤ 2{ sup

[−π,π]|κ(eıλ)|2}(cid:12)(cid:12)(cid:12)df

T − d(cid:12)(cid:12)(cid:12)|(log 2πN/T ) + o(N/T )|

for all λ ∈ [2π/T, 2πN/T ] as T → ∞. We can therefore infer that (22) is O(δT log T )
or smaller, uniformly in λ for all λ ∈ [2π/T, 2πN/T ]. This completes the proof of
Lemma 1.

Returning to the proof of Theorem 1, evaluating the expression

(¯c0 − c0) + (¯c1 − c1)|λ|2 = |¯κh(eıλ)|2 − |κ(eıλ)|2 + o(|λ|3)

(23)

at λ = 2π/T and 2πN/T , and solving for ¯c0−c0 and ¯c1−c1, it follows as a consequence
of Lemma 1 that |¯c0 − c0| = O (MT ) + o(T −3) and |¯c1 − c1| = O (T 2MT /N 2) + o(N/T ).
Extracting the dominant term completes the proof of Theorem 1.

Proof of Proposition 1: Let bdT denote the LPR(0) estimator. Then bdT is the OLS

coeﬃcient of the regressor −2 log λj in the regression of log IT (λj) on 1 and −2 log λj.
Substituting aj − 2d log(λj) + ηj for log IT (λj) in this regression leads to the expression

bdT − d = −PN
2PN
NXj=1

= −

1
2

j=1(log λj − log λ)(ηj + aj)

j=1(log λj − log λ)2
rj(ηj + aj)

(24)

for the estimation error where ηj and aj are deﬁned in expressions (16) and (17), and
j=1(log λj −log λ)2, j = 1, . . . , N. See the discussion associated

rj = (log λj −log λ)/PN

with (16) and (17) for clariﬁcation.

By Theorem 2 of Moulines and Soulier (1999) there exist sequences ej and fj,
j = 1, . . . , N, such that ηj = ej +fj, where the ej, j = 1, . . . , N, are weakly dependent,
centered Gumbel random variables with variance π2/6 and covariance cov{ek, ej} =
O(log2(j)k−2|d|j2(|d|−1)) for 1 ≤ k < j ≤ N, and |fj| = O(log(1 + j)/j) with probability
one. Since max1≤j≤N | log λj − log λ| = O(log N) and PN
j=1(log λj − log λ)2 = O(N)
it follows thatPN
j=1 rjfj = O(log3 N/N) a.s.. Given thatPN
j=1 rj = 0, it also follows
from (18) thatPN

j=1 rjaj = O(N 2 log N/T 2). We can therefore infer from (24) that

bdT − d = −

1
2

NXj=1

rjej + RN

Sieve Bootstrap Bias Correction

25

where |RN| log T = O(ν3 log4 T /T ν) + O(ν log2 T /T 2(1−ν)) = o(1) a.s., 2/3 < ν < 4/5.
The desired result now follows because on application of a law of large numbers

for triangular arrays of weakly dependent random variables we ﬁnd that for all δ > 0

NXj=1

rjej = o(cid:0)(ν log T )5/2(log(ν log T ))(1+δ)/2T −ν/2(cid:1)

a.s.

.

More speciﬁcally, let Sn =Pn

4E[|S2k|2], and using the bounds on the covariance of ej we have

j=1 rjej. Then by Doob’s inequality E[(maxn≤2k |Sn|)2] ≤

E[|Sn|2] =

nXj=1

r2
j E[e2

j ] + 2 X1≤k<j≤n

rkrjcov{ek, ej} = O(log4 n/n) .

We can therefore conclude that for any δ > 0

2k

k5(log k)1+δ E[(max

n≤2k |Sn|)2] ≤

∞Xk=1

2k

k5(log k)1+δ O(cid:18)k4

2k(cid:19) < ∞ ,

∞Xk=1

a.s. since the function (log n)5/2(log log n)(1+δ)/2 is slowly varying at inﬁnity.

sinceP∞
k=1 1/k(log k)1+δ < ∞, which by the Borel-Cantelli lemma implies maxn≤2k |Sn| =
o(k5/2(log k)(1+δ)/22−k/2) a.s.. Consequently √N|SN| = o((log N)5/2(log log N)(1+δ)/2)
Now let bdT denote the LPR(P ) estimator with P ≥ 1. The analytically-bias-
adjusted LPR estimator is the OLS coeﬃcient of the regressor −2 log λj in the re-
gression of log IT (λj) on 1, −2 log λj, and λ2p
j , p = 1, . . . , P . Applying the Frisch-
Waugh-Lovell theorem and projecting out the regressors λ2p
j , p = 1, . . . , P , as well
as unity we can express the estimation error bdT − d exactly as in (24), save that
the rj are now deﬁned in terms of −2]log λj, say, the component of −2 log λj orthog-
onal to 1 and λ2p
j , p = 1, . . . , P . This projection does not alter the overall mag-
nitudes, so for the orthogonalized regressor we have max1≤j≤N |]log λj| = O(log N)
and PN
j=1(]log λj)2 = O(N) (Andrews and Guggenberger, 2003, Lemma 2, parts (j)
& (k)). The proof that |bdT − d| log T = o(1) a.s. now proceeds as previously with
rj = ]log λj/PN
(2003, Lemma 5.8), which implies that the SPLW estimator satisﬁes P (|bdT −d| log T >

ǫ) = o(N −p), where p > 1/ǫ and N, the bandwidth, satisﬁes T ǫ < N < T 1−ǫ for some
ǫ > 0. For the SPLW(P ≥ 1) estimator the proposition can be established in a
manner similar to that employed above for the LPR(P ) estimators. Using Lemma

For the SPLW(0) estimator the proposition follows directly from Giraitis and Robinson

j=1(]log λj)2, j = 1, . . . , N.

SPLW(P ) estimator with P ≥ 1, as a function of the standardized score and from
Andrews and Sun (2004, Lemma 5) we can conclude that the standardized score is

4 of Andrews and Sun (2004) we can express bdT − d, where bdT now denotes the
of an order that implies that |bdT − d| log T = o(1) a.s.; cf. Andrews and Sun (2004,

Sieve Bootstrap Bias Correction

26

Theorem 4).

(25)

Proof of Theorem 2: The sampling distribution of N 1/2(bdT − d) for the LPR and

SPLW estimators, under the Gaussianity assumption, admits a normal approximation

such that

sup

x (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
Pr(N 1/2(bdT − d)

υ

< x) − G(x)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

= o(cid:18)N 5/2
T 2 (cid:19) ,

where υ = ωψP and G(·) denotes the standard normal distribution function. Substi-
tuting (8) into (25) gives the approximation

sup

(26)

T − df
T )
υ

Pr∗(N 1/2(bd∗f

PrnN 1/2(cid:16)bdT − E[bdT ](cid:17) < xo = PrnN 1/2(cid:16)bdT − d(cid:17) < x + N 1/2bTo
= G(cid:0)(x + N 1/2bT )/υ(cid:1) + o(N 5/2/T 2)
for the distribution of the ﬁnite sample deviation bdT − E[bdT ]. Similarly,
< x) − G(x)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
= o(cid:18)N 5/2
T 2 (cid:19) ,
T ](cid:17) < xo = Pr∗nN 1/2(cid:16)bd∗f
= G(cid:0)(cid:0)x + N 1/2b∗
T − E∗[bd∗f
T − E∗[bd∗f
T ]) < x} and Pr{N 1/2(bdT − E[bdT ]) < x} is

x (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
Pr∗nN 1/2(cid:16)bd∗f
T − E∗[bd∗f
for the bootstrap deviation bd∗f
diﬀerence between Pr∗{N 1/2(bd∗f

T(cid:17) < x + N 1/2b∗
To
T − df
T(cid:1) /υ(cid:1) + o(N 5/2/T 2)

and substituting (9) into (27) we obtain the approximation

T ]. Subtracting (26) from (28) we ﬁnd that the

bounded in absolute value by

(27)

(28)

and

(cid:12)(cid:12)G(cid:0)(x + N 1/2bT )/υ(cid:1) − G((x + N 1/2b∗
x (cid:12)(cid:12)G((x + N 1/2bT )/υ) − G((x + N 1/2b∗

T )/υ)(cid:12)(cid:12) + o(N 5/2/T 2) ,
T )/υ)(cid:12)(cid:12) ≤

N 1/2
υ√2π|bT − b∗
T|

sup

by the ﬁrst mean value theorem for integrals (Apostol, 1960, Theorem 7.30).

Recognizing that d

∗f
T,B = df
we also have the approximation

Pr∗{N

T − d

1

2 (bd∗f

T,B and employing the expansion (27) once more

∗f

T +bb∗f
T,Bo
T,B) < x} = Pr∗nN
2bb∗f
2 (bd∗f
T − df
= G(cid:16)(x + N
T,B)/υ(cid:17) + o(N 5/2/T 2) .
2bb∗f

T ) < x + N

1

1

1

(29)

Sieve Bootstrap Bias Correction

27

Subtracting the approximation in (29) from that in (26) and using the triangle in-
equality, having added and subtracted (28), now yields the result that

N

1

2|bT − b∗
T|
υ√2π

+ rBN

(30)

sup

x (cid:12)(cid:12)(cid:12)PrnN

1

2 (bdT − E[bdT ]) < xo − Pr∗nN

1

1

2 (bd∗f

T − d

∗f

T,B) < xo(cid:12)(cid:12)(cid:12) ≤

since |bb∗f

1

where the remainder rBN = N
right hand side in (30) follows from the inequality supx |G((x + N
N
from the inequality supx |G((x+N

2 O(plog log B/B) + o(N 5/2/T 2). The ﬁrst term on the
2 bT )/υ) − G((x +
T|/υ√2π, and similarly, the ﬁrst term of the remainder derives
T|/υ√2π,
2bb∗f

T,B)/υ)−G((x+N

2 b∗
T )/υ)| ≤ N

2 b∗
T )/υ)| ≤ N

2|bT − b∗

T,B − b∗

T,B−b∗

1

1

1

1

1

1

∗f

Pr

T − d

T,B) < xo = B−1

T| = O(plog log B/B) a.s.. Furthermore, if we set
∗nN
x (cid:12)(cid:12)(cid:12)Pr

T,B) < xo − Pr∗nN

2 (bd∗f
∗nN

1nN

2 (bd∗f

2 (bd∗f

2 (bd∗f

BXb=1

T,b − d

T − d

T − d

∗f

2|bb∗f
T,B) ≤ xo
T,B) < xo(cid:12)(cid:12)(cid:12) > δ

∗f

∗f

1

1

1

then by the Dvoretsky–Kiefer–Wolfowitz inequality the probability of the event

is bounded by 2 exp(−2Bδ2). It therefore follows that for all B ∼ KT 4/5+β, β > 0,
the (1 − αU − αL)100% signiﬁcance level HPD intervals are consistent with respect to
the Kolmogorov-Smirnov metric.

sup

B Stochastic Stopping Rules

Two criteria are used to determine if any meaningful gain in accuracy will be achieved

1

1

T

T

T,B = − 1

−ed(k)
T | > τ (k)

T (cid:17), where ˜d∗f (k)

by adding a further iteration to the iterative procedure of Section 4. The ﬁrst,
, is based on Cauchy’s convergence criterion. Given the stochastic

nature of the bias correction mechanism we can think of this as a statistical decision
rule in which τ (k)
governs the probability of moving from the kth to the (k + 1)th it-
denotes

|ed(k+1)
b=1(cid:16) ˜d∗f (k)
BPB
eration. Now ed(k+1)
T,b −ed(k)
the estimator produced from the bth PFSB draw with ed(k)
value; and since bdT is a √N –CAN estimator, given the data and the current and pre-
T ) D→ N (0, υ2). The conditional (asymptotic)
T,b −ed(k)
T (cid:17) is therefore υ2/NB, and using the rule that the
variance of B−1PB
T,b −ed(k)
overall variance equals the variance of the conditional mean (in this case V ar[ed(k)
estimators is given by V arhed(k+1)

T ]) plus
the expectation of the conditional variance (in this case the constant υ2/NB) we can
infer that the (asymptotic) variance of the diﬀerence between successive bias-adjusted
N B . Furthermore, from the

T = −eb∗f (k)
− ed(k)
b=1(cid:16) ˜d∗f (k)

T i = V arhed(k)
−ed(k)

T used as the pre-ﬁltering

T i + υ2

vious bootstrap iterations, N

2 ( ˜d∗f (k)

T,b

T

1

Sieve Bootstrap Bias Correction

28

T,b

T

T,B

− 1

T

T = ed(k−1)

T

= ed(k−1)

T ] =
T ] + υ2/NB = (2B + 1)υ2/NB. Moreover, at each iteration the bias-adjusted
estimate is constructed as a linear combination of asymptotically normal random vari-
ables and is itself therefore asymptotically normal. This indicates that τ (k)
can be
evaluated from percentile points of the normal approximation.

(cid:17), it
b=1(cid:16) ˜d∗f (k−1)
BPB
−eb∗f (k−1)
recurrence formula ed(k)
−ed(k−1)
i + υ2
T i = 2 · V arhed(k−1)
follows by a similar logic that V arhed(k)
N B , where V ar[ed(1)
2· V ar[ed(0)
Similarly, the second convergence criterion,(cid:12)(cid:12)(cid:12)ed(0)
T −ed(k)
accumulated bias correction, ed(0)
T −ed(k)
T −(cid:18) 1
eb∗f (k)
T −eb∗f (k)
T,B . From the expression ed(0)
T − ed(k)
T,B = ed(0)
the (asymptotic) variance, V arhed(0)
T,B i = υ2
T −eb∗f (k)
T −ed(k)

, is perhaps
best thought of as the decision rule that examines the diﬀerence between the current
T , and the current bootstrap estimate of the bias,

T,B (cid:12)(cid:12)(cid:12) > τ (k)
T −eb∗f (k)
T,b (cid:19) , it follows that
BPb=1
B(cid:3)(cid:1) , and the

N (cid:0)1 + 2k−1(cid:2)1 + 1

can once again be set using percentile points from the asymptotic

˜d∗f (k)

1

2

T

B

tolerance level τ (k)
normal approximation.

2

The interpretation of the convergence criteria as statistical decision rules in which

1

T

and τ (k)

the tolerance levels govern the probability of going from the current to the next itera-
tion suggests that τ (k)
2 be set by reference to conventional critical values used
still

in statistical hypothesis tests. When k is very small we might conjecture that ed(k)
T has already undergone several adjustments to produce ed(k)
initial estimate ed(0)

contains some bias and we may wish to iterate further unless there is strong evidence
that so doing will produce very little change. On the other hand, when k is large the
T and we
may prefer to terminate iteration unless there is strong evidence that further iteration
will produce additional, substantial correction. We can therefore calibrate τ (k)
and
τ (k)
2 using quantile points of the normal distribution z(1−pk/2) (where G(z(1−p)) = 1− p)
and pk, the probability of going from the kth to the (k + 1)th iteration, is assigned to be
large when k is small and vice versa. In the simulation experiments we set p0 = 0.95,
p1 = 0.9, and pk = (0.1)2(1−k) for k = 2, 3, . . . for LPR(0) and SPLW(0); and p0 = 0.9,
pk = (0.1)2−k for k = 1, 2, 3, . . . for LPR(P ) and SPLW(P ), P ≥ 1.

1

C Tables

Table 1

Bias and mean square error (MSE) for all LPR-based estimators; T = 100, using the parametric version of the bootstrap.

(Unadjusted and analytically-bias-adjusted LPR(P ), P = 0, 1, 2; plus bootstrap-bias-adjusted

LPR(P )-BBA(K); K = 1, . . . , 3−P , and their iteratively-adjusted (SSR) variants.)

The lowest bias (in absolute value) and MSE for each design are highlighted.

Analogous ﬁgures for the ML estimator are reported in italics.

LPR(P )

LPR(0)-BBA(K)

LPR(1)-BBA(K)

P = 0

P = 1

P = 2 K = 1 K = 2 K = 3

SSR K = 1 K = 2

SSR

LPR(2)-BBA(K)
K = 1

SSR

(Gaussian)

ML

d

0

0.2

0.4

d

0

0.2

0.4

φ

0.3
0.6
0.9
0.3
0.6
0.9
0.3
0.6
0.9

φ

0.3
0.6
0.9
0.3
0.6
0.9
0.3
0.6
0.9

0.1391
0.3873
0.8141
0.1312
0.3790
0.7955
0.1335
0.3720
0.7144

0.0422
0.1754
0.6866
0.0422
0.1691
0.6592
0.0417
0.1608
0.5387

0.0329
0.2062
0.7393
0.0376
0.2037
0.7237
0.0420
0.2130
0.6768

0.0804
0.1107
0.6179
0.0800
0.1145
0.5977
0.0718
0.1175
0.5401

0.0145
0.0929
0.6352
0.0124
0.1237
0.6414
0.0162
0.1142
0.6165

0.1563
0.1479
0.5495
0.1547
0.1600
0.5565
0.1654
0.1566
0.5312

0.1210
0.3387
0.7935
0.1123
0.3263
0.8166
0.1066
0.3214
0.7939

0.0591
0.1649
0.6781
0.0617
0.1581
0.7405
0.0558
0.1523
0.7325

0.0909
0.2589
0.7592
0.0814
0.2402
0.8164
0.0636
0.2360
0.7504

0.1270
0.2019
0.7006
0.1383
0.1986
0.9021
0.1171
0.1918
1.0400

0.0320
0.1169
0.6628
0.0240
0.0811
0.6524
-0.0181
0.0705
0.4633

0.3720
0.4207
0.7788
0.3968
0.4346
1.0148
0.3411
0.4158
1.1629

Bias

0.1142
0.3215
0.7849
0.1015
0.2990
0.8024
0.0920
0.3031
0.7607

0.0134
0.1615
0.7014
0.0174
0.1514
0.7232
0.0203
0.1623
0.7269

MSE

0.0822
0.2027
0.6870
0.1010
0.2155
0.7863
0.0974
0.1865
0.8361

0.1349
0.1463
0.6249
0.1346
0.1480
0.6953
0.1167
0.1592
0.7686

-0.0241
0.0873
0.6189
-0.0155
0.0669
0.6161
-0.0183
0.0716
0.4985

-0.0006
0.1477
0.6836
0.0082
0.1372
0.7042
0.0126
0.1435
0.6780

-0.0156
0.0445
0.5922
-0.0165
0.0744
0.6358
-0.0158
0.0709
0.6562

-0.0366
0.0338
0.5706
-0.0280
0.0556
0.6168
-0.0360
0.0619
0.6312

0.2869
0.2855
0.6659
0.2767
0.2873
0.7698
0.2516
0.3146
0.8741

0.1858
0.1897
0.6556
0.1704
0.1860
0.7232
0.1551
0.2139
0.8838

0.2743
0.2413
0.6202
0.2488
0.2445
0.7146
0.2746
0.2446
0.8340

0.3556
0.2995
0.6842
0.3110
0.3190
0.7622
0.3672
0.3009
0.9044

-0.0933
-0.0321
0.1222
-0.1055
-0.0652
0.0330
-0.1103
-0.0937
-0.0161

0.0549
0.0367
0.0474
0.0591
0.0340
0.0185
0.0488
0.0298
0.0084

S
i
e
v
e

B
o
o
t
s
t
r
a
p
B
i
a
s
C
o
r
r
e
c
t
i
o
n

2
9

Table 2

Bias and mean square error (MSE) for all LPR-based estimators; T = 500, using the parametric version of the bootstrap.

(Unadjusted and analytically-bias-adjusted LPR(P ), P = 0, 1, 2; plus bootstrap-bias-adjusted

LPR(P )-BBA(K); K = 1, . . . , 3−P , and their iteratively-adjusted (SSR) variants.)

The lowest bias (in absolute value) and MSE for each design are highlighted.

Analogous ﬁgures for the ML estimator are reported in italics.

LPR(P )
P = 1

LPR(0)-BBA(K)
P = 2 K = 1 K = 2 K = 3

P = 0

LPR(1)-BBA(K)

SSR K = 1 K = 2

SSR

LPR(2)-BBA(K)
K = 1

SSR

(Gaussian)

ML

d

0

0.2

0.4

d

0

0.2

0.4

φ

0.3
0.6
0.9
0.3
0.6
0.9
0.3
0.6
0.9

φ

0.3
0.6
0.9
0.3
0.6
0.9
0.3
0.6
0.9

Bias

0.0596
0.2199
0.6722
0.0571
0.2177
0.6670
0.0639
0.2206
0.6472

0.0101
0.0554
0.4581
0.0099
0.0541
0.4514
0.0110
0.0551
0.4259

0.0081
0.0705
0.4894
0.0083
0.0702
0.4895
0.0206
0.0761
0.4872

0.0163
0.0224
0.2564
0.0157
0.0214
0.2571
0.0168
0.0235
0.2538

-0.0072
0.0158
0.3619
0.0047
0.0179
0.3667
0.0100
0.0320
0.3689

0.0294
0.0337
0.1591
0.0286
0.0283
0.1645
0.0299
0.0290
0.1672

0.0317
0.1558
0.5914
0.0310
0.1532
0.5956
0.0353
0.1480
0.6503

0.0131
0.0390
0.3613
0.0134
0.0366
0.3695
0.0139
0.0342
0.4469

-0.0075
0.0556
0.4610
-0.0049
0.0535
0.4752
-0.0036
0.0370
0.6581

-0.0709
-0.1176
0.2321
-0.0628
-0.1195
0.2620
-0.0654
-0.1557
0.6030

0.0280
0.1247
0.5441
0.0287
0.1335
0.5360
0.0323
0.1190
0.6262

0.0013
0.0332
0.4070
0.0003
0.0299
0.3954
0.0091
0.0298
0.4190

-0.0099
-0.0245
0.2782
-0.0133
-0.0312
0.2486
-0.0064
-0.0368
0.3090

-0.0023
0.0239
0.3715
-0.0057
0.0224
0.3481
0.0054
0.0190
0.3904

-0.0159
-0.0103
0.2831
-0.0054
-0.0091
0.2814
-0.0045
0.0009
0.2856

-0.0239
-0.0222
0.2580
-0.0115
-0.0175
0.2480
-0.0101
-0.0048
0.2444

0.0282
0.0444
0.2424
0.0292
0.0391
0.2665
0.0288
0.0349
0.5214

0.0859
0.1470
0.1510
0.0849
0.1308
0.1974
0.0832
0.1329
0.6908

MSE

0.0210
0.0738
0.3678
0.0156
0.0573
0.4140
0.0183
0.0696
0.5318

0.0237
0.0309
0.1961
0.0223
0.0291
0.1904
0.0226
0.0292
0.2129

0.0434
0.0664
0.1517
0.0411
0.0641
0.1477
0.0394
0.0606
0.1944

0.0365
0.0550
0.2391
0.0441
0.0476
0.2609
0.0401
0.0538
0.2463

0.0404
0.0532
0.1326
0.0382
0.0432
0.1329
0.0383
0.0406
0.1441

0.0639
0.0907
0.1832
0.0610
0.0766
0.1957
0.0686
0.0618
0.2341

-0.0240
-0.0276
0.0716
-0.0249
-0.0321
0.0087
-0.0276
-0.0445
0.0023

0.0105
0.0159
0.0277
0.0105
0.0143
0.0055
0.0071
0.0110
0.0034

S
i
e
v
e

B
o
o
t
s
t
r
a
p
B
i
a
s
C
o
r
r
e
c
t
i
o
n

3
0

Table 3

Bias and mean square error (MSE) for all SPLW-based estimators; T = 100, using the parametric version of the bootstrap.

(Unadjusted and analytically-bias-adjusted SPLW(P ), P = 0, 1, 2; plus bootstrap-bias-adjusted

SPLW(P )-BBA(K); K = 1, . . . , 3−P , and their iteratively-adjusted (SSR) variants.)

The lowest bias (in absolute value) and MSE for each design are highlighted.

Analogous ﬁgures for the ML estimator are reported in italics.

SPLW(P )

SPLW(0)-BBA(K)

SPLW(1)-BBA(K)

P = 0

P = 1

P = 2 K = 1 K = 2 K = 3

SSR

K = 1 K = 2

SSR

SPLW(2)-BBA(K)
K = 1

SSR

(Gaussian)

ML

d

0

0.2

0.4

d

0

0.2

0.4

φ

0.3
0.6
0.9
0.3
0.6
0.9
0.3
0.6
0.9

φ

0.3
0.6
0.9
0.3
0.6
0.9
0.3
0.6
0.9

0.1300
0.3985
0.8242
0.1191
0.3973
0.7944
0.1217
0.3828
0.7409

0.0329
0.1773
0.6975
0.0308
0.1770
0.6500
0.0315
0.1643
0.5672

-0.0162
0.1583
0.7197
0.0011
0.1602
0.6898
-0.0090
0.1766
0.6762

-0.0648
0.0604
0.6122
-0.0657
0.0526
0.6012
-0.0200
0.0425
0.5966

0.0527
0.0803
0.5695
0.0492
0.0807
0.5346
0.0576
0.0869
0.5101

0.1133
0.1126
0.4973
0.1190
0.1081
0.4757
0.1134
0.1195
0.4593

0.1136
0.3695
0.8197
0.1063
0.3607
0.8244
0.1044
0.3539
0.836

0.0361
0.1611
0.7032
0.0348
0.1576
0.7247
0.0337
0.1533
0.7580

0.0901
0.3255
0.8120
0.0875
0.3037
0.8615
0.0783
0.3072
0.8916

0.0577
0.1555
0.7287
0.0562
0.1501
0.8792
0.0529
0.1585
1.1781

0.0479
0.2500
0.7893
0.0545
0.2033
0.8259
0.0329
0.2207
0.7652

0.1442
0.2015
0.8036
0.1374
0.2038
1.0641
0.1312
0.2347
1.4480

Bias

0.1124
0.3685
0.8172
0.1062
0.3585
0.8301
0.1045
0.3495
0.8669

-0.0010
0.1431
0.7248
0.0192
0.1424
0.7011
-0.0029
0.1564
0.7456

0.0099
0.1126
0.7190
0.0342
0.1125
0.6645
-0.0019
0.1208
0.6859

-0.0037
0.1394
0.7204
0.0162
0.1386
0.6861
-0.0062
0.1417
0.7278

-0.0577
0.0611
0.6123
-0.0492
0.0417
0.6181
-0.0044
0.0324
0.6619

MSE

0.0387
0.1614
0.7019
0.0354
0.1583
0.7632
0.0337
0.1571
0.8734

0.0774
0.1039
0.6022
0.0745
0.0986
0.6079
0.0819
0.1074
0.6878

0.1508
0.1795
0.6551
0.1430
0.1626
0.6862
0.1491
0.1804
0.8896

0.0888
0.1170
0.6066
0.0835
0.1114
0.6358
0.0905
0.1531
0.7243

0.1758
0.1628
0.5650
0.1660
0.1540
0.5873
0.1562
0.1763
0.6727

-0.0760
0.0496
0.5951
-0.0537
0.0325
0.5999
-0.0092
0.0217
0.6467

0.2432
0.2089
0.5935
0.1994
0.1861
0.6211
0.1834
0.2100
0.7178

-0.0933
-0.0321
0.1222
-0.1055
-0.0652
0.0330
-0.1103
-0.0937
-0.0161

0.0549
0.0367
0.0474
0.0591
0.0340
0.0185
0.0488
0.0298
0.0084

S
i
e
v
e

B
o
o
t
s
t
r
a
p
B
i
a
s
C
o
r
r
e
c
t
i
o
n

3
1

Bias and mean square error (MSE) for all SPLW-based estimators; T = 500, using the parametric version of the bootstrap.

(Unadjusted and analytically-bias-adjusted SPLW(P ), P = 0, 1, 2; plus bootstrap-bias-adjusted

SPLW(P )-BBA(K); K = 1, . . . , 3−P , and their iteratively-adjusted (SSR) variants.)

The lowest bias (in absolute value) and MSE for each design are highlighted.

Analogous ﬁgures for the ML estimator are reported in italics.

Table 4

SPLW(P )

SPLW(0)-BBA(K)

SPLW(1)-BBA(K)

P = 0

P = 1

P = 2 K = 1 K = 2

K = 3

SSR

K = 1 K = 2

SSR

SPLW(2)-BBA(K)
K = 1

SSR

(Gaussian)

ML

d

0

0.2

0.4

d

0

0.2

0.4

φ

0.3
0.6
0.9
0.3
0.6
0.9
0.3
0.6
0.9

φ

0.3
0.6
0.9
0.3
0.6
0.9
0.3
0.6
0.9

Bias

0.0561
0.2305
0.7220
0.0566
0.2291
0.7177
0.0603
0.2265
0.6982

0.0075
0.0577
0.5264
0.0075
0.0572
0.5205
0.0080
0.0560
0.4935

-0.0137
0.0538
0.5220
-0.0077
0.0494
0.5220
0.0056
0.0572
0.5093

-0.0097
0.0040
0.3798
-0.0100
0.0046
0.3838
-0.0086
0.0046
0.3819

0.0118
0.0135
0.2867
0.0117
0.0140
0.2866
0.0122
0.0151
0.2730

0.0196
0.0193
0.1663
0.0217
0.0204
0.1701
0.0214
0.0200
0.1660

0.0289
0.1746
0.6731
0.0323
0.1731
0.6829
0.0338
0.1623
0.7399

0.0078
0.0374
0.4603
0.0076
0.0368
0.4768
0.0077
0.0336
0.5659

-0.0081
0.0892
0.6003
-0.0002
0.0892
0.6278
-0.0013
0.0662
0.8157

-0.0638
-0.0552
0.4808
-0.0481
-0.0539
0.5347
-0.0525
-0.0977
0.8543

0.0288
0.1639
0.6694
0.0323
0.1618
0.6799
0.0336
0.1440
0.7850

-0.0096
0.0269
0.4683
-0.0006
0.0204
0.4648
0.0093
0.0242
0.4689

-0.0064
-0.0130
0.3878
0.0066
-0.0226
0.3797
0.0125
-0.0218
0.4047

-0.0109
0.0241
0.4628
-0.0025
0.0190
0.4599
0.0077
0.0245
0.4583

0.0028
-0.0017
0.3270
0.0000
-0.0011
0.3242
-0.0015
-0.0104
0.3267

0.0141
0.0246
0.3740
0.0129
0.0246
0.4173
0.0130
0.0221
0.7358

0.0382
0.0552
0.2657
0.0325
0.0555
0.3460
0.0345
0.0653
1.0847

MSE

0.0080
0.0432
0.4574
0.0076
0.0429
0.4756
0.0078
0.0508
0.6647

0.0159
0.0175
0.2406
0.0151
0.0183
0.2373
0.0150
0.0178
0.2476

0.0248
0.0335
0.1920
0.0224
0.0347
0.1854
0.0215
0.0325
0.2301

0.0158
0.0242
0.2454
0.0150
0.0217
0.2384
0.0151
0.0179
0.2553

0.0233
0.0283
0.1422
0.0256
0.0282
0.1411
0.0236
0.0277
0.1421

0.0008
-0.0056
0.3158
-0.0042
-0.0041
0.3132
-0.0049
-0.0090
0.3180

0.0233
0.0347
0.1589
0.0286
0.0314
0.1575
0.0236
0.0281
0.1545

-0.0240
-0.0276
0.0716
-0.0249
-0.0321
0.0087
-0.0276
-0.0445
0.0023

0.0105
0.0159
0.0277
0.0105
0.0143
0.0055
0.0071
0.0110
0.0034

S
i
e
v
e

B
o
o
t
s
t
r
a
p
B
i
a
s
C
o
r
r
e
c
t
i
o
n

3
2

Table 5

Empirical coverage and length of nominal 95% bootstrap HPD intervals for unadjusted and analytically-

bias-adjusted LPR(P ) and SPLW(P ), P = 0, 1, 2; T = 100, 500, using the parametric version of the bootstrap.
Analogous results for intervals based on the asymptotic distribution of each of the estimators are included.

Figures are averaged over all values of d used in the experimental design for each value of φ.

The highlighting indicates the empirical coverage closest to the nominal 95%, and the shortest length.

Panel A: LPR(P )

Panel B: SPLW(P )

BHDCI
P = 1

P = 0

Asymp. interval

P = 2

P = 0

P = 1

P = 2

P = 0

BHDCI
P = 1

Asymp. interval

P = 2

P = 0

P = 1

P = 2

φ

0.3

0.6

0.9

φ

0.3

0.6

0.9

T

100
500
100
500
100
500

T

100
500
100
500
100
500

Coverage

0.8828
0.8980
0.3210
0.2405
0.0073
0.0000

0.9520
0.9573
0.8918
0.9220
0.2445
0.0420

0.9480
0.9605
0.9443
0.9560
0.5923
0.4280

0.7558
0.8348
0.1975
0.1738
0.0025
0.0000

0.8303
0.9058
0.7113
0.8440
0.1128
0.0228

0.7845
0.8885
0.7700
0.8735
0.3100
0.2693

0.8720
0.8660
0.1865
0.0790
0.0005
0.0000

0.9603
0.9505
0.8985
0.9310
0.1530
0.0060

Interval length

0.6407
0.3274
0.6405
0.3307
0.6111
0.3306

1.1093
0.5267
1.1039
0.5272
1.0353
0.5222

1.5662
0.6982
1.5609
0.6983
1.4687
0.6950

0.5016
0.2856
0.5016
0.2856
0.5016
0.2856

0.7523
0.4283
0.7523
0.4283
0.7523
0.4283

0.9404
0.5354
0.9404
0.5354
0.9404
0.5354

0.5399
0.2629
0.5452
0.2677
0.5033
0.2648

0.9552
0.4290
0.9570
0.4308
0.8863
0.4238

Coverage

0.9590
0.9568
0.9515
0.9603
0.5298
0.2513

0.7095
0.7878
0.0728
0.0418
0.0000
0.0000

Interval length

1.3848
0.5766
1.3836
0.5778
1.2934
0.5759

0.3911
0.2226
0.3911
0.2226
0.3911
0.2226

0.8070
0.8755
0.6700
0.8323
0.0453
0.0015

0.7395
0.8588
0.7328
0.8655
0.2163
0.1158

0.5866
0.3340
0.5866
0.3340
0.5866
0.3340

0.7332
0.4175
0.7332
0.4175
0.7332
0.4175

S
i
e
v
e

B
o
o
t
s
t
r
a
p
B
i
a
s
C
o
r
r
e
c
t
i
o
n

3
3

Table 6

Bias and mean square error (MSE) for all LPR-based estimators*; T = 100, 500, using the nonparametric version of the

bootstrap, with Gaussian innovations. Figures are averaged over the four values of d used in the experimental design for each

value of φ, and the lowest average bias (in absolute value) and MSE for each design highlighted.

Analogous ﬁgures for the (Gaussian) MLE are reported in italics.

(* Unadjusted and analytically-bias-adjusted LPR(P ), P = 0, 1, 2; plus bootstrap-bias-adjusted

LPR(P )-BBA(K); K = 1, . . . , 3−P , and their iteratively-adjusted (SSR) variants.)

LPR(P )

P = 0

P = 1

P = 2 K = 1 K = 2

LPR(0)-BBA(K)
K = 3

LPR(1)-BBA(K)

SSR K = 1 K = 2

SSR

LPR(2)-BBA(K)
K = 1

SSR

(Gaussian)

ML

T

100

500

T

100

500

φ

0.3
0.6
0.9
0.3
0.6
0.9

φ

0.3
0.6
0.9
0.3
0.6
0.9

Bias

0.1401
0.3865
0.7783
0.0609
0.2209
0.6650

0.0408
0.2022
0.7196
0.0118
0.0693
0.4936

0.0161
0.1051
0.6485
0.0086
0.0269
0.3709

0.1182
0.3396
0.8131
0.0333
0.1545
0.6184

0.0821
0.2625
0.8842
-0.0048
0.0525
0.5409

0.0133
0.1237
0.9822
-0.0656
-0.1232
0.3784

0.1034
0.3200
0.9018
0.0322
0.1365
0.5833

0.0201
0.1543
0.7310
0.0027
0.0270
0.4118

-0.0140
0.0771
0.6610
-0.0108
-0.0366
0.2837

0.0118
0.1418
0.7233
0.0034
0.0175
0.3826

-0.0106
0.0595
0.6577
-0.0004
-0.0027
0.2866

-0.0260
0.0409
0.6492
0.0003
-0.0085
0.2563

MSE

0.0451
0.1747
0.6324
0.0103
0.0554
0.4490

0.0747
0.1127
0.5871
0.0168
0.0214
0.2604

0.1439
0.1511
0.5545
0.0300
0.0307
0.1677

0.0614
0.1653
0.7363
0.0129
0.0365
0.4008

0.1295
0.2027
1.0506
0.0268
0.0373
0.3560

0.3644
0.4286
1.9247
0.0771
0.1258
0.3566

0.1028
0.2045
1.2497
0.0145
0.0528
0.4387

0.1231
0.1476
0.7027
0.023
0.0285
0.2033

0.2542
0.2856
0.8762
0.0389
0.0610
0.1652

0.1584
0.1844
0.7889
0.0282
0.0500
0.2349

0.2327
0.2409
0.7383
0.0377
0.0449
0.1384

0.3085
0.3207
0.8570
0.0451
0.0690
0.2011

-0.1053
-0.0683
0.0364
-0.0257
-0.0349
0.0219

0.0552
0.0332
0.0217
0.0096
0.0134
0.0103

S
i
e
v
e

B
o
o
t
s
t
r
a
p
B
i
a
s
C
o
r
r
e
c
t
i
o
n

3
4

Table 7

Bias and mean square error (MSE) for all SPLW-based estimators*; T = 100, 500, using the nonparametric version of the

bootstrap, with Gaussian innovations. Figures are averaged over the four values of d used in the experimental design for each

value of φ, and the lowest average bias (in absolute value) and MSE for each design highlighted.

Analogous ﬁgures for the (Gaussian) MLE are reported in italics.

(* Unadjusted and analytically-bias-adjusted SPLW(P ), P = 0, 1, 2; plus bootstrap-bias-adjusted

SPLW(P )-BBA(K); K = 1, . . . , 3−P , and their iteratively-adjusted (SSR) variants.)

SPLW(P )

SPLW(0)-BBA(K)

SPLW(1)-BBA(K)

P = 0

P = 1

P = 2 K = 1 K = 2

K = 3

SSR K = 1 K = 2

SSR

SPLW(2)-BBA(K)
K = 1

SSR

(Gaussian)

ML

T

100

500

T

100

500

φ

0.3
0.6
0.9
0.3
0.6
0.9

φ

0.3
0.6
0.9
0.3
0.6
0.9

Bias

0.1274
0.3907
0.7837
0.0572
0.2296
0.7150

-0.0038
0.1644
0.6950
-0.0023
0.0583
0.5256

-0.0375
0.0554
0.6092
-0.0091
0.0105
0.3849

0.1126
0.3589
0.8277
0.0318
0.1706
0.7042

0.0915
0.3081
0.8622
-0.0023
0.0818
0.6922

0.0532
0.2168
0.7982
-0.0531
-0.0692
0.6478

0.1124
0.3562
0.8418
0.0318
0.1627
0.7150

0.0120
0.1494
0.7272
0.0032
0.0292
0.4760

0.0288
0.1216
0.6999
0.0081
-0.0139
0.4006

0.0096
0.1428
0.7122
0.0013
0.0273
0.4682

-0.0194
0.0537
0.6414
0.0014
0.0016
0.3285

-0.0311
0.0445
0.6190
-0.0023
0.0004
0.3209

MSE

0.0340
0.1720
0.6337
0.0075
0.0573
0.5169

0.0523
0.0794
0.5343
0.0107
0.0141
0.2889

0.1102
0.1112
0.4784
0.0197
0.0198
0.1697

0.0379
0.1568
0.7323
0.0075
0.0355
0.5091

0.0606
0.1540
0.9395
0.0131
0.0221
0.5227

0.1484
0.2112
1.1430
0.0347
0.0539
0.5879

0.0381
0.1598
0.7879
0.0075
0.0369
0.5354

0.0770
0.0991
0.6331
0.0134
0.0175
0.2486

0.1422
0.1677
0.7556
0.0191
0.0332
0.2084

0.0830
0.1129
0.6613
0.0137
0.0204
0.2552

0.1567
0.1663
0.626
0.0225
0.0276
0.1432

0.1966
0.1973
0.6798
0.0242
0.0281
0.1489

-0.1053
-0.0683
0.0364
-0.0257
-0.0349
0.0219

0.0552
0.0332
0.0217
0.0096
0.0134
0.0103

S
i
e
v
e

B
o
o
t
s
t
r
a
p
B
i
a
s
C
o
r
r
e
c
t
i
o
n

3
5

Table 8

Bias and mean square error (MSE) for all LPR-based estimators*; T = 100, 500, using the nonparametric version of the

bootstrap, with Student t innovations. Figures are averaged over the four values of d used in the experimental design for each

value of φ, and the lowest average bias (in absolute value) and MSE for each design highlighted.

Analogous ﬁgures for the (Gaussian) MLE are reported in italics.

(* Unadjusted and analytically-bias-adjusted LPR(P ), P = 0, 1, 2; plus bootstrap-bias-adjusted

LPR(P )-BBA(K); K = 1, . . . , 3−P , and their iteratively-adjusted (SSR) variants.)

LPR(P )

P = 0

P = 1

P = 2 K = 1 K = 2

LPR(0)-BBA(K)
K = 3

LPR(1)-BBA(K)

SSR K = 1 K = 2

SSR

LPR(2)-BBA(K)
K = 1

SSR

(Gaussian)

ML

T

100

500

T

100

500

φ

0.3
0.6
0.9
0.3
0.6
0.9

φ

0.3
0.6
0.9
0.3
0.6
0.9

Bias

0.1366
0.381
0.7768
0.0626
0.2234
0.6659

0.0501
0.2023
0.7229
0.0169
0.0730
0.4905

0.0269
0.1082
0.6499
0.0098
0.0254
0.3687

0.1124
0.3304
0.8107
0.0343
0.1560
0.6187

0.0729
0.2474
0.7957
-0.0050
0.0524
0.5403

-0.0026
0.0852
0.5868
-0.0675
-0.1265
0.3729

0.1039
0.3138
0.7858
0.0308
0.1334
0.5747

0.0296
0.1501
0.7331
0.0081
0.0305
0.4080

-0.0084
0.065
0.5965
-0.0055
-0.0332
0.2777

0.0198
0.1376
0.7095
0.0041
0.0203
0.3721

-0.0016
0.0584
0.6546
0.0004
-0.0035
0.2840

-0.0149
0.0400
0.6283
-0.0015
-0.0098
0.2470

MSE

0.0425
0.1694
0.6308
0.0105
0.0563
0.4503

0.0753
0.1169
0.5970
0.0170
0.0214
0.2575

0.1422
0.1519
0.5706
0.0292
0.0297
0.1669

0.0574
0.1566
0.7358
0.0131
0.0370
0.4019

0.1195
0.1874
0.9413
0.0274
0.0376
0.3586

0.3435
0.3791
1.0700
0.0803
0.1281
0.3627

0.0805
0.1893
0.8099
0.0183
0.0634
0.4472

0.1235
0.1532
0.7198
0.0239
0.0282
0.2009

0.2673
0.2958
0.7951
0.0425
0.0604
0.1647

0.1681
0.1916
0.7595
0.0418
0.0529
0.2465

0.2243
0.2417
0.7641
0.0368
0.0438
0.1380

0.2887
0.3237
0.8416
0.0513
0.0692
0.2157

-0.1054
-0.0676
0.0368
-0.0264
-0.0407
0.0198

0.0554
0.0345
0.0210
0.0111
0.0142
0.0101

S
i
e
v
e

B
o
o
t
s
t
r
a
p
B
i
a
s
C
o
r
r
e
c
t
i
o
n

3
6

Table 9

Bias and mean square error (MSE) for all SPLW-based estimators*; T = 100, 500, using the nonparametric version of the

bootstrap, with Student t innovations. Figures are averaged over the four values of d used in the experimental design for each

value of φ, and the lowest average bias (in absolute value) and MSE for each design highlighted.

Analogous ﬁgures for the (Gaussian) MLE are reported in italics.

(* Unadjusted and analytically-bias-adjusted SPLW(P ), P = 0, 1, 2; plus bootstrap-bias-adjusted

SPLW(P )-BBA(K); K = 1, . . . , 3−P , and their iteratively-adjusted (SSR) variants.)

SPLW(P )

SPLW(0)-BBA(K)

SPLW(1)-BBA(K)

P = 0

P = 1

P = 2 K = 1 K = 2

K = 3

SSR K = 1 K = 2

SSR

SPLW(2)-BBA(K)
K = 1

SSR

(Gaussian)

ML

T

100

500

T

100

500

φ

0.3
0.6
0.9
0.3
0.6
0.9

φ

0.3
0.6
0.9
0.3
0.6
0.9

Bias

0.1269
0.3914
0.7846
0.0576
0.2306
0.7151

0.0005
0.1722
0.6982
-0.0025
0.0551
0.5224

-0.0218
0.0659
0.6124
-0.0106
0.0081
0.3799

0.1104
0.3574
0.8288
0.0318
0.1706
0.7041

0.0861
0.3040
0.8632
-0.0026
0.0799
0.6918

0.0424
0.2086
0.8055
-0.0532
-0.0742
0.6450

0.1093
0.3535
0.8399
0.0317
0.1576
0.7142

0.0120
0.1530
0.7300
0.0023
0.0254
0.4712

0.0186
0.1196
0.6926
0.0066
-0.0188
0.3932

0.0072
0.1457
0.7124
0.0006
0.0250
0.4630

-0.0054
0.0603
0.6429
-0.0004
-0.0021
0.3210

-0.0185
0.0487
0.6287
-0.0035
-0.0028
0.3109

MSE

0.0323
0.1710
0.6345
0.0074
0.0578
0.5168

0.0525
0.0821
0.5425
0.0102
0.0139
0.2855

0.1066
0.1134
0.4833
0.0200
0.0196
0.1659

0.0354
0.1540
0.7325
0.0073
0.0360
0.5086

0.0563
0.1503
0.9407
0.0128
0.0230
0.5209

0.1386
0.2089
1.1541
0.0342
0.0579
0.5816

0.0370
0.1590
0.7923
0.0074
0.0422
0.5364

0.0776
0.1018
0.6480
0.0131
0.0174
0.2438

0.1483
0.1723
0.7619
0.0191
0.0334
0.2023

0.0931
0.1233
0.6867
0.0129
0.0189
0.2506

0.1514
0.1656
0.6207
0.0225
0.0274
0.1392

0.1959
0.2026
0.6530
0.0235
0.0276
0.1524

-0.1054
-0.0676
0.0368
-0.0264
-0.0407
0.0198

0.0554
0.0345
0.0210
0.0111
0.0142
0.0101

S
i
e
v
e

B
o
o
t
s
t
r
a
p
B
i
a
s
C
o
r
r
e
c
t
i
o
n

3
7

Sieve Bootstrap Bias Correction

38

References

Agiakloglou, C., Newbold, P. and Wohar, M. (1993). Bias in the estimator of
the fractional diﬀerence parameter. Journal of Time Series Analysis, 14 235–246.

Andrews, D. W., Lieberman, O. and Marmer, V. (2006). Higher-order improve-

ments of the parametric bootstrap for long-memory Gaussian processes. Journal of
Econometrics, 133 673–702.

Andrews, D. W. K. and Guggenberger, P. (2003). A bias-reduced log-
periodogram regression estimator for the long-memory parameter. Econometrica, 71

675–712. URL http://www.jstor.org/view/00129682/sp030005/03x0090l/0.

Andrews, D. W. K. and Sun, Y. (2004). Adaptive local polynomial Whittle esti-

mation of long-range dependence. Econometrica, 72 569–614.

Apostol, T. M. (1960). Mathematical Analysis. Addison-Wesley, Reading.

Beran, J. (1994). Statistics for long-memory processes, vol. 61 of Monographs on

Statistics and Applied Probability. Chapman and Hall, New York.

Beran, J. (1995). Maximum likelihood estimation of the diﬀerencing parameter for

invertible short and long memory autoregressive integrated moving average models.
Journal of the Royal Statistical Society, B 57 654–672.

Box, G. and Jenkins, G. (1970). Time Series Analysis: Forecasting and Control.

Holden Day, San Francisco.

Brockwell, P. J. and Davis, R. A. (1991). Time Series: Theory and Methods.

2nd ed. Springer Series in Statistics, Springer-Verlag, New York.

Choi, E. and Hall, P. G. (2000). Bootstrap conﬁdence regions from autoregressions

of arbitrary order. Journal of the Royal Statistical Society, B 62 461–477.

Dahlhaus, R. (1989). Eﬃcient parameter estimation for self-similar processes. An-

nals of Statistics, 17 1749–1766.

Doornik, J. A. and Ooms, M. (2001). Computational aspects of maximum like-

lihood estimation of autoregressive fractionally integrated moving average models.
Computational Statistics & Data Analysis, 42 333–348.

Doukhan, P., Oppenheim, G. and Taqqu, M. S. (eds.) (2003). Theory and

applications of long-range dependence. Birkh¨auser Boston Inc., Boston, MA.

Durbin, J. (1960). The ﬁtting of time series models. Review of International Statis-

tical Institute, 28 233–244.

Sieve Bootstrap Bias Correction

39

Fay, G.

(2010).

the peri-
odogram. Stochastic Processes and their Applications, 120 983 – 1009. URL

Moment bounds

for non-linear

functionals of

http://www.sciencedirect.com/science/article/pii/S0304414910000463.

Fay, G., Moulines, E. and Soulier, P.

(2004).

Edgeworth ex-

linear

for
pansions
ear processes.
http://www.sciencedirect.com/science/article/pii/S0167715203003407.

Statistics & Probability Letters, 66 275 – 288.

long-range-dependent

statistics

possibly

of

lin-
URL

Fox, R. and Taqqu, M. S. (1986). Large sample properties of parameter estimates
for strongly dependent stationary gaussian time series. Annals of Statistics, 14

517–532.

Geweke, J. and Porter-Hudak, S. (1983). The estimation and application of

long-memory time series models. Journal of Time Series Analysis, 4 221–238.

Giraitis, L. and Robinson, P. M. (2003). Edgeworth expansions for semiparamet-

ric Whittle estimation of long memory. Annals of Statistics, 31 1325–1375.

Giraitis, L., Robinson, P. M. and Samarov, A. (1997). Rate optimal semi-
parametric estimation of the memory parameter of the gaussian time series with
long-range dependence. Journal of Time Series Analysis, 18 49–60.

Granger, C. W. J. and Joyeux, R. (1980). An introduction to long-memory time
series models and fractional diﬀerencing. Journal of Time Series Analysis, 1 15–29.

Hansen, B. E. (2014). Econometrics. Online; accessed 15 August 2014, URL

http://www.ssc.wisc.edu/~bhansen/econometrics/.

Hosking, J. R. M. (1981). Fractional diﬀerencing. Biometrika, 68 165–176.

Hosking, J. R. M. (1996). Asymptotic distributions of the sample mean, autocovari-
ances, and autocorrelations of long memory time series. Journal of Econometrics,

73 261–284.

Hurvich, C. M. and Beltrao, K. I. (1993). Asymptotics for the low-frequency
ordinates of the periodogram of a long-memory time series. Journal of Time Series

Analysis, 14 445–472.

Hurvich, C. M., Deo, R. and Brodsky, J. (1998). The mean squared error of

Geweke and Porter-Hudak’s estimator of the memory parameter of a long memory
time series. Journal of Time Series Analysis, 19 19–46.

Inoue, A. (2002). Asymptotic behavior for partial autocorrelation functions of frac-

tional ARIMA processes. Annals of Applied Probability, 12 1471–1491.

Sieve Bootstrap Bias Correction

40

Inoue, A. and Kasahara, Y. (2004). Partial autocorrelation functions of the frac-
tional ARIMA processes with negative degree of diﬀerencing. Journal of Multivariate

Analysis, 89 135–147.

Inoue, A. and Kasahara, Y. (2006). Explicit representation of ﬁnite predictor

coeﬃcients and its applications. Annals of Statistics, 34 973–993.

Kreiss, J. P., Paparoditis, E. and Politis, D. N. (2011). On the range of validity

of the autoregressive sieve bootstrap. Annals of Statistics, 39 2103–2130.

Levinson, N. (1947). The Wiener RMS (root mean square) error criterion in ﬁlter

design and prediction. Journal of Mathematical Physics, 25 261–278.

Lieberman, O. (2001). The exact bias of the log-periodogram regression estimator.

Econometric Reviews, 20 369–383.

Lieberman, O., Rosemarin, R. and Rousseau, J.

(2012).

Asymp-

totic theory for maximum likelihood estimation of the memory parameter
in stationary gaussian processes.
Econometric Theory, 28 457–470. URL

http://journals.cambridge.org/article_S0266466611000399.

Lieberman, O., Rousseau, J. and Zucker, D. M. (2001). Valid Edgeworth
expansion for the sample autocorrelation function under long range dependence.

Econometric Theory, 17 257–275.

Lieberman, O., Rousseau, J. and Zucker, D. M. (2003). Valid asymptotic

expansions for the maximum likelihood estimator of the parameter of a stationary,
gaussian, strongly dependent process. The Annals of Statistics, 31 586–612.

Moulines, E. and Soulier, P. (1999). Broad band log-periodogram regression of

time series with long range dependence. Annals of Statistics, 27 1415–1439.

Nadarajah, K., Martin, G. M. and Poskitt, D. S. (2014). Issues in the es-

timation of misspeciﬁed models of fractionally integrated processes. Economet-
rics & Business Statistics Working Paper WP18/14, Monash University. URL

http://arxiv.org/abs/1407.1347.

Nielsen, M. . and Frederiksen, P. H. (2005). Finite sample comparison of para-
metric, semiparametric, and wavelet estimators of fractional integration. Economet-

ric Reviews, 24 405–443.

Politis, D. N. (2003). The impact of bootstrap methods on time series analysis.

Statistical Science, 18 219–230.

Sieve Bootstrap Bias Correction

41

Poskitt, D. S. (2007). Autoregressive approximation in nonstandard situations: The
fractionally integrated and non-invertible cases. Annals of Institute of Statistical

Mathematics, 59 697–725.

Poskitt, D. S. (2008). Properties of the sieve bootstrap for fractionally integrated

and non-invertible processes. Journal of Time Series Analysis, 29 224–250.

Poskitt, D. S., Grose, S. D. and Martin, G. M. (2015). Higher order im-
provements of the sieve bootstrap for fractionally integrated processes. Journal of

Econometrics, 188 94–100.

Robinson, P. (ed.) (2003). Time series with long memory. Advanced texts in econo-

metrics, Oxford University Press, Oxford.

Robinson, P. M. (1995a). Gaussian semiparametric estimation of long range depen-

dence. Annals of Statistics, 23 1630–1661.

Robinson, P. M. (1995b). Log periodogram regression of time series with long

memory. Annals of Statistics, 23 1048–1072.

Shibata, R. (1980). Asymptotically eﬃcient selection of the order of the model for

estimating parameters of a linear process. Annals of Statistics, 8 147–164.

Shimotsu, K. and Phillips, P. C. B. (2005). Exact local whittle estima-

tion of fractional
http://www.jstor.org/stable/3448627.

integration. The Annals of Statistics, 33 1890–1933. URL

Sowell, F. (1992). Maximum likelihood estmation of stationary univariate fraction-

ally integrated time series models. Journal of Econometrics, 53 165–188.

Tanaka, K. (1999). The nonstationary fractional unit root. Econometric Theory, 15

549–582.

van der Vaart, A. W. (1998). Asymptotic Statistics. Cambridge University Press.

