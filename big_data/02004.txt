6
1
0
2

 
r
a

M
7

 

 
 
]

.

A
N
h
t
a
m

[
 
 

1
v
4
0
0
2
0

.

3
0
6
1
:
v
i
X
r
a

Posterior Consistency for Gaussian Process Approximations of

Bayesian Posterior Distributions

Andrew M. Stuart1, Aretha L. Teckentrup1

1 Mathematics Institute, Zeeman Building, University of Warwick, Coventry, CV4 7AL, England.
a.m.stuart@warwick.ac.uk, a.teckentrup@warwick.ac.uk

Abstract

We study the use of Gaussian process emulators to approximate the parameter-to-observation
map or the negative log-likelihood in Bayesian inverse problems. We prove error bounds on the
Hellinger distance between the true posterior distribution and various approximations based on
the Gaussian process emulator. Our analysis includes approximations based on the mean of the
predictive process, as well as approximations based on the full Gaussian process emulator. Our
results show that the Hellinger distance between the true posterior and its approximations can
be bounded by moments of the error in the emulator. Numerical results conﬁrm our theoretical
ﬁndings.

Keywords: inverse problem, Bayesian approach, surrogate model, Gaussian process regression,

posterior consistency

AMS 2010 subject classiﬁcations: 60G15, 62G08, 65D05, 65D30, 65J22

1

Introduction

Given a mathematical model of a physical process, we are interested in the inverse problem of
determining the inputs to the model given some noisy observations related to the model outputs.
Adopting a Bayesian approach [18, 37], we incorporate our prior knowledge of the inputs into a
probability distribution, referred to as the prior distribution, and obtain a more accurate represen-
tation of the model inputs in the posterior distribution, which results from conditioning the prior
distribution on the observations. Since the posterior distribution is generally intractable, sampling
methods such Markov chain Monte Carlo (MCMC) [17, 24, 32, 10, 15, 13] are typically used to
explore it. A major challenge in the application of MCMC methods to problems of practical inter-
est is the large computational cost associated with numerically solving the mathematical model for
a given set of the input parameters. Since the generation of each sample by the MCMC method
requires a solve of the governing equations, and often millions of samples are required, this process
can quickly become very costly.

This drawback of fully Bayesian inference for complex models was recognised several decades
ago in the statistics literature, and resulted in key papers which had a profound inﬂuence on
methodology [33, 19, 28]. These papers advocated the use a of Gaussian process surrogate model
to approximate the solution of the governing equations, and in particular the likelihood, at a
much lower computational cost. This approximation then results in an approximate posterior

1

distribution, which can be sampled more cheaply using MCMC. However, despite the widespread
adoption of the methodology, there has been little analysis of the eﬀect of the approximation on
posterior inference.
In this work, we study this issue, focussing on the use of Gaussian process
emulators [30, 36, 33, 19, 28, 6] as surrogate models. Other choices of surrogate models such as
those described in [8, 3], generalised Polynomial Chaos [40, 21], sparse grid collocation [4, 20] and
adaptive subspace methods [12, 11] might also be studied similarly, but are not considered here.
Indeed we note that the paper [20] studied the eﬀect, on the posterior distribution, of stochastic
collocation approximation within the forward model and was one of the ﬁrst papers to address such
questions. That paper used the Kullback-Leibler divergence, or relative entropy, to measure the
eﬀect on the posterior, and considered ﬁnite dimensional input parameter spaces.

The main focus of this work is to analyse the error introduced in the posterior distribution by
using a Gaussian process emulator as a surrogate model. The error is measured in the Hellinger
distance, which is shown in [37, 14] to be a suitable metric for evaluation of perturbations to the
posterior measure in Bayesian inverse problems, including problems with inﬁnite dimensional input
parameter spaces. We consider emulating either the parameter-to-observation map or the negative
log-likelihood.
In both cases, we study three diﬀerent approximations to the posterior distribu-
tion. Firstly, we consider using the mean of the Gaussian process emulator as a surrogate model,
resulting in a deterministic approximation to the posterior distribution. Our second approxima-
tion is obtained by using the full Gaussian process as a surrogate model, leading to a random
approximation in which case we study the expected value of the Hellinger distance between the
true and the approximate posterior distribution. Finally, we construct an alternative deterministic
approximation by using the full Gaussian process as surrogate model, and taking the expected value
(with respect to the distribution of the surrogate) of the likelihood. Our analysis is restricted to
ﬁnite dimensional input spaces. This reﬂects the state-of-the-art with respect to Gaussian process
emulation itself; the analysis of the eﬀect on the posterior is less sensitive to dimension.

For the three approximations discussed above, we show that the Hellinger distance between
the true and approximate posterior distribution can be bounded by the error between the true
parameter-to-observation map (or log-likelihood) and its Gaussian process approximation, mea-
sured in a norm that depends on the approximation considered. Combining this with results from
the theory of scattered data interpolation [39, 34, 26], we give convergence estimates of the error
in terms of the design points used to construct the Gaussian process emulator.

The remainder of this paper is organised as follows. In section 2, we set up the Bayesian inverse
problem of interest. We then recall some results on Gaussian process regression in section 3. The
heart of the paper is section 4, where we introduce the diﬀerent approximations to the posterior
and perform an error analysis. Our theoretical results are conﬁrmed on a simple model problem in
section 5, and some conclusions are ﬁnally given in section 6.

2 Bayesian Inverse Problems

Let X and V be separable Banach spaces, and deﬁne the measurable mappings G : X → V and
O : V → RJ , for some J ∈ N. Denote by G : X → RJ the composition of O and G. We refer to
G as the forward map, to O as the observation operator and to G as the parameter-to-observation
map. We denote by k · k the Euclidean norm on Rn, for n ∈ N. We consider the setting where the
Banach space X is a compact subset of RK, for some ﬁnite K ∈ N, representing the range of a
ﬁnite number K of parameters u. The inverse problem of interest is to determine the parameters

2

u ∈ X from the noisy data y ∈ RJ given by

y = G(u) + η,

where the noise η is a realisation of the RJ -valued Gaussian random variable N (0, σ2
ηI), for some
known variance σ2
η. We adopt a Bayesian perspective in which, in the absence of data, u is dis-
tributed according to a prior measure µ0. We are interested in the posterior distribution µy on the
conditioned random variable u|y, which can be characterised as follows.
Proposition 2.1. ([37]) Suppose G : X → RJ is continuous and µ0(X) = 1. Then the posterior
distribution µy on the conditioned random variable u|y is absolutely continuous with respect to µ0
and given by Bayes’ Theorem:

where

dµy
dµ0

(u) =

1
Z

exp(cid:0) − Φ(u)(cid:1),

Φ(u) =

1
2σ2

η ky − G(u)k2

and

Z = Eµ0(cid:16) exp(cid:0) − Φ(u)(cid:1)(cid:17).

(2.1)

We make the following assumption on the regularity of the parameter-to-observation map G.
Assumption 2.2. We assume that G : X → RJ satisﬁes G ∈ H s(X; RJ ), for some s > K/2, and
that supu∈X kG(u)k =: CG < ∞.

Under Assumption 2.2, it follows that the negative log-likelihood Φ : X → R satisﬁes Φ ∈
H s(X), and supu∈X |Φ(u)| =: CΦ < ∞. Since s > K/2, the Sobolev Embedding Theorem further-
more implies that G and Φ are continuous. Examples of model problems satisfying Assumption 2.2
include linear elliptic and parabolic partial diﬀerential equations [9, 35] and non-linear ordinary
diﬀerential equations [38, 16]. A speciﬁc example is given in section 5.

3 Gaussian Process Regression

We are interested in using Gaussian process regression to build a surrogate model for the forward
map, leading to an approximate Bayesian posterior distribution that is computationally cheaper to
evaluate. Generally speaking, Gaussian process regression (or Gaussian process emulation, or krig-
ing) is a way of building an approximation to a function f , based on a ﬁnite number of evaluations
of f at a chosen set of design points. We will here consider emulation of either the parameter-
to-observation map G : X → RJ or the negative log-likelihood Φ : X → R. Since the eﬃcient
emulation of vector-valued functions is still an open question [5], we will focus on the emulation of
scalar valued functions. An emulator of G in the case J > 1 is constructed by emulating each entry
independently.
Let now f : X → R be an arbitrary function. Gaussian process emulation is in fact a Bayesian
procedure, and the starting point is to put a Gaussian process prior on the function f . In other
words, we model f as

(3.1)
with known mean m : X → R and two point covariance function k : X × X → R. Here, we use the
Gaussian process notation as in, for example, [30]. In the notation of [37], we have f0 ∼ N (m, C),
where m = m(·) and C is the integral operator with covariance function k as kernel.

f0 ∼ GP(m(u), k(u, u′)),

3

Typical choices of the mean function m include the zero function and polynomials [30]. A family
of covariance functions k frequently used in applications are the Mat`ern covariance functions [22],
given by

1

Γ(ν)2ν−1 (cid:18)√2ν ku − u′k

λ

(cid:19)ν

Bν(cid:18)√2ν ku − u′k

λ

(cid:19) ,

kν,λ,σ2

k

(u, u′) = σ2
k

(3.2)

where Γ denotes the Gamma function, Bν denotes the modiﬁed Bessel function of the second kind
and ν, λ and σ2
k are positive parameters. The parameter λ is referred to as the correlation length,
and governs the length scale at which f0(u) and f0(u′) are correlated. The parameter σ2
k is referred
to as the variance, and governs the magnitude of f0(u). Finally, the parameter ν is referred to as
the smoothness parameter, and governs the regularity of f0 as a function of u. As the limit when
ν → ∞, we obtain the Gaussian covariance

k∞,λ,σ2

k

(u, u′) = σ2

k exp(cid:18)−ku − u′k2
2λ2 (cid:19) .

(3.3)

Now suppose we are given data in the form of a set of distinct design points U := {un}N

n=1 ⊆ X,

together with corresponding function values

f (U ) := [f (u1), . . . , f (uN )] ∈ RN .

(3.4)

Conditioning the Gaussian process (3.1) on the known values (U, f (U )), we obtain another Gaussian
process fN , known as the predictive process. We have

fN ∼ GP(mf

N (u), kN (u, u′)),

(3.5)

where the predictive mean mf
N : X → R and predictive covariance kN : X × X → R are known
explicitly [30], and depend on the modelling choices made in (3.1). In the following discussion, we
will focus on the popular choice m ≡ 0; the case of a non-zero mean is discussed in Remark 3.7.
When m ≡ 0, we have

mf

N (u) = k(u, U )T K(U, U )−1f (U ),

kN (u, u′) = k(u, u′) − k(u, U )T K(U, U )−1k(u′, U ),

(3.6)

There are several points to note about the predictive mean mf

where k(u, U ) = [k(u, u1), . . . , k(u, uN )] ∈ RN and K(U, U ) ∈ RN ×N is the matrix with ijth entry
equal to k(ui, uj) [30].
N is a linear
combination of the function evaluations f (U ), and hence a linear predictor. It is in fact the best
linear predictor [36], in the sense that it is the linear predictor with the smallest mean square error.
Secondly, mf
N interpolates the function f at the design points U , since the vector k(un, U ) is the
nth row of the matrix K(U, U ). In other words, we have mf
N (un) = f (un), for all n = 1, . . . , N .
Finally, we remark that mf

N is a linear combination of kernel evaluations,

N in (3.6). Firstly, mf

mf

N (u) =

N

Xn=1

αnk(u, un),

where the vector of coeﬃcients is given by α = K(U, U )−1f (U ). Concerning the predictive covari-
ance kN , we note that kN (u, u) < k(u, u) for all u ∈ X, since K(U, U )−1 is positive deﬁnite. Further-
more, we also note that kN (un, un) = 0, for n = 1, . . . , N , since k(un, U )T K(U, U )−1 k(un, U ) =
k(un, un).

4

For stationary covariance functions k(u, u′) = k(ku − u′k), the predictive mean is a radial basis
functions interpolant of f , and we can make use of results from the radial basis function literature
to investigate the behaviour of mf
N and kN as N → ∞. Before we do this, in subsection 3.2, we
recall some results on native spaces (also know as reproducing kernel Hilbert spaces) in subsection
3.1.

3.1 Native spaces of Mat`ern kernels

We recall the notion of the reproducing kernel Hilbert space corresponding to the kernel k, usually
referred to as the native space of k in the radial basis function literature.

Deﬁnition 3.1. A Hilbert space Hk of functions f : X → R, with inner product h·,·iHk , is called
the reproducing kernel Hilbert space (RKHS) corresponding to a symmetric, positive deﬁnite kernel
k : X × X → R if

i) for all u ∈ X, k(u, u′), as a function of its second argument, belongs to Hk,
ii) for all u ∈ X and f ∈ Hk, hf, k(u,·)iHk = f (u).
By the Moore-Aronszajn Theorem [2], a unique RKHS exists for each symmetric, positive
deﬁnite kernel k. Furthermore, this space can be constructed using Mercer’s Theorem [23], and it
is equal to the Cameron-Martin space [7] of the covariance operator C with kernel k. For covariance
kernels of Mat`ern type, the native space is isomorphic to a Sobolev space [39, 34].

Proposition 3.2. Let kν,λ,σ2
space Hkν,λ,σ2
and the Sobolev norm are equivalent.

k

be a Mat`ern covariance kernel as deﬁned in (3.2). Then the native
is equal to the Sobolev space H ν+K/2(X) as a vector space, and the native space norm

k

Native spaces for more general kernels, including non-stationary kernels, are analysed in [39].
For stationary kernels, the native space can generally be characterised by the rate of decay of the
Fourier transform of the kernel. The native space of the Gaussian kernel (3.3), for example, consists
of functions whose Fourier transform decays exponentially, and is hence strictly contained in the
space of analytic functions. Proposition 3.2 shows that as a vector space, the native space of the
is fully determined by the smoothness parameter ν. The parameters λ and
Mat`ern kernel kν,λ,σ2
σ2
k do, however, inﬂuence the constants in the norm equivalence of the native space norm and the
standard Sobolev norm.

k

3.2 Radial basis function interpolation
For stationary covariance functions k(u, u′) = k(ku − u′k), the predictive mean is a radial basis
functions interpolant of f . In fact, it is the minimum norm interpolant [30],

mf

N =

arg min

g∈Hk : g(U )=f (U ) kgkHk .

(3.7)

Given the set of design points U = {un}N
qU and mesh ratio ρU by

n=1 ⊆ X, we deﬁne the ﬁll distance hU , separation radius

hU := sup
u∈X

un∈U ku − unk,
inf

qU :=

1
2

i6=j kuj − uik,
min

ρU :=

hU
qU ≥ 1.

5

The ﬁll distance is the maximum distance any point in X can be from U , and the separation radius
is half the smallest distance between any two distinct points in U . The mesh ratio provides a
measure of how uniformly the design points U are distributed in X. We have the following theorem
on the convergence of mf
Proposition 3.3. Suppose X ⊆ RK is a bounded, Lipschitz domain that satisﬁes an interior cone
condition, and the symmetric positive deﬁnite kernel k is such that Hk is isomorphic to the Sobolev
space H τ (X), with τ = n + r, n ∈ N, n > K/2 and 0 ≤ r < 1. Suppose mf
N is given by (3.6). If
f ∈ H τ (X), then there exists a constant C, independent of f , U and N , such that

N to f [39, 25, 26].

kf − mf

NkH β (X) ≤ Chτ −β

U kfkH τ (X),

for any β ≤ τ,

for all sets U with hU suﬃciently small.

Proposition 3.3 assumes that the function f is in the RKHS of the kernel k. Convergence
estimates for a wider class of functions can be obtained using interpolation in Sobolev spaces [26].
Proposition 3.4. Suppose X ⊆ RK is a bounded, Lipschitz domain that satisﬁes an interior cone
condition, and the symmetric positive deﬁnite kernel k is such that Hk is isomorphic to the Sobolev
space H τ (X). Suppose mf
N is given by (3.6). If f ∈ H ˜τ (X), for some ˜τ ≤ τ , ˜τ = n + r, n ∈ N,
n > K/2 and 0 ≤ r < 1, then there exists a constant C, independent of f , U and N , such that

kf − mf

NkH β (X) ≤ Ch˜τ −β

U ρ˜τ −β

U kfkH ˜τ (X),

for any β ≤ ˜τ ,

for all sets U with hU and ρU suﬃciently small.

Convergence of the predictive variance kN (u, u) follows under the assumptions of Proposition
3.3 or Proposition 3.4 using the relation in Proposition 3.5 below. This was already noted, without
proof, in [34]; we give a proof here for completeness.

Proposition 3.5. Suppose mf

N and kN are given by (3.6). Then

kN (u, u)

Proof. For any u ∈ X, we have

1
2 = sup

kgkHk =1|g(u) − mg

N (u)|.

sup

kgkHk =1|g(u) − mg

N (u)| = sup

N

N

= sup

Xj=1

g(u) −

(k(u, U )T K(U, U )−1)jg(uj )(cid:12)(cid:12)(cid:12)

kgkHk =1(cid:12)(cid:12)(cid:12)
(k(u, U )T K(U, U )−1)jhg, k(·, uj )iHk(cid:12)(cid:12)(cid:12)
kgkHk =1(cid:12)(cid:12)(cid:12)hg, k(·, u)iHk −
Xj=1
(k(u, U )T K(U, U )−1)jk(·, uj )iHk(cid:12)(cid:12)(cid:12)
kgkHk =1(cid:12)(cid:12)(cid:12)hg, k(·, u) −
Xj=1

= kk(·, u) − k(·, U )T K(U, U )−1k(u, U )kHk .

= sup

N

6

The ﬁnal equality follows from the Cauchy-Schwarz inequality, which becomes an equality when
the two functions considered are linearly dependent. By Deﬁnition 3.1, we then have

kk(·, u) − k(·, U )T K(U, U )−1k(u, U )k2

Hk

= hk(·, u) − k(·, U )T K(U, U )−1k(u, U ), k(·, u) − k(·, U )T K(U, U )−1k(u, U )iHk
= hk(·, u), k(·, u)iHk − 2hk(·, u), k(·, U )T K(U, U )−1k(u, U )iHk

+ hk(·, U )T K(U, U )−1k(u, U ), k(·, U )T K(U, U )−1k(u, U )iHk

= k(u, u) − 2k(u, U )T K(U, U )−1k(u, U ) + k(u, U )T K(U, U )−1k(u, U )
= kN (u, u).

The identity which leads to the third term in the penultimate line uses the fact that hk(·, u′), k(·, u)iHk =
k(u, u′), for any u, u′ ∈ X. If ℓ(u) = K(U, U )−1k(u, U ) then
hk(·, U )T K(U, U )−1k(u, U ), k(·, U )T K(U, U )−1k(u, U )iHk =Xj,k
=Xj,k

ℓj(u)hk(·, uj ), k(·, uk)iHk ℓk(u)
ℓj(u)k(uj , uk)ℓk(u)

= ℓ(u)T K(U, U )ℓ(u)
= k(u, U )T K(U, U )−1k(u, U )

as required. This completes the proof.

The second string of equalities, appearing in the middle part of the proof Proposition 3.5, might
appear counter-intuitive at ﬁrst glance in that the left-most quantity is a norm squared of quantities
which scale like k, whilst the right-most quantity scales like k itself. However, the space Hk itself
depends on the kernel k, and scales inversely proportional to k, explaining that the identity is
indeed dimensionally correct.

Remark 3.6. (Exponential convergence for the Gaussian kernel) The RKHS corresponding to the
Gaussian kernel (3.3) is no longer isomorphic to a Sobolev space; it is contained in H τ (X), for
any τ < ∞. For functions f in this RKHS, Gaussian process regression with the Gaussian kernel
converges exponentially in the ﬁll distance hU . For more details, see [39].

Remark 3.7. (Regression with non-zero mean) If in (3.1) we use a non-zero mean m(·), the formula
for the predictive mean mf

N changes to
mf
N (u) = m(u) + k(u, U )T K(U, U )−1(f (U ) − m(U )),

(3.8)

where m(U ) := [m(u1), . . . , m(uN )] ∈ RN . The predictive covariance kN (u, u′) is as in (3.6). As
in the case m ≡ 0, we have mf
N is an interpolant of f . If
m ∈ Hk, then mf
N given by (3.8) is also in Hk, and the proof techniques in [25, 26] can be applied.
The conclusions of Propositions 3.3 and 3.4 then hold, with the factor kfk in the error bounds
replaced by kfk + kmk.

N (un) = f (un), for n = 1, . . . , N , and mf

7

4 Approximation of the Bayesian posterior distribution

In this section, we analyse the error introduced in the posterior distribution µy when we use a
Gaussian process emulator to approximate the parameter-to-observation map G or the negative
log-likelihood Φ. The aim is to show convergence, in a suitable sense, of the approximate posterior
distributions to the true posterior distribution as the number of observations N tends to inﬁnity.
For a given approximation µy,N of the posterior distribution µy, we will focus on bounding the
Hellinger distance [37] between the two distributions, which is deﬁned as

dHell(µy, µy,N ) =


1

2Z X


dµ0 
dµ0 −s dµy,N
s dµy


dµ0


.

2

1/2

As proven in [14, Lemma 6.12 and 6.14], the Hellinger distance provides a bound for the Total
Variation distance

sup

kf k∞≤1(cid:12)(cid:12)

Eµy (f ) − E

µy,N (f )(cid:12)(cid:12) ≤

√2 dHell(µy, µy,N ),

µy,N (X), the Hellinger distance also provides a bound on the error in

and for f ∈ L2
expected values

dTV(µy, µy,N ) =

1
2

µy (X) ∩ L2
(cid:12)(cid:12)

Eµy (f ) − E

µy,N (f )(cid:12)(cid:12) ≤ 2(Eµy (f 2) + E

µy,N (f 2))1/2 dHell(µy, µy,N ).

Depending on how we make use of the predictive process GN or ΦN to approximate the Radon-
Nikodym derivative dµy
, we obtain diﬀerent approximations to the posterior distribution µy. We
dµ0
will distinguish between approximations based solely on the predictive mean, and approximations
that make use of the full predictive process.

4.1 Approximation based on the predictive mean

Using simply the predictive mean of a Gaussian process emulator of the parameter-to-observation
map G or the negative log-likelihood Φ, we can deﬁne the approximations µy,N,G
mean , given
by

mean and µy,N,Φ

dµy,N,G
mean
dµ0

(u) =

Z mean
N,G

exp(cid:0) −
N,G = Eµ0(cid:16) exp(cid:0) −

Z mean

1

1

dµy,N,Φ
mean
dµ0

1
2σ2
1
2σ2

2(cid:1),
η (cid:13)(cid:13)y − mG
N (u)(cid:13)(cid:13)
2(cid:1)(cid:17),
η (cid:13)(cid:13)y − mG
N (u)(cid:13)(cid:13)
N (u)(cid:1),
N (u)(cid:1)(cid:17),

(u) =

Z mean

Z mean
N,Φ

exp(cid:0) − mΦ
N,Φ = Eµ0(cid:16) exp(cid:0) − mΦ
N (u)] ∈ RJ . We have the following lemma concerning the normal-
N,Φ , which is followed by the main Theorem 4.2 and Corollary 4.3
mean , µy,N,Φ
mean .

N (u) = [mG1
where mG
ising constants Z mean
N,G
concerning the approximations µy,N,G

N (u), . . . , mGJ
and Z mean

8

Lemma 4.1. Suppose supu∈X kG(u) − mG
N (u)| converge to 0 as N
tends to ∞, and assume supu∈X kG(u)k ≤ CG. Then there exist positive constants C1 and C2,
independent of U and N , such that

N (u)k and supu∈X |Φ(u) − mΦ

C1 ≤ Z mean

N,G ≤ 1

and

C2 ≤ Z mean

N,Φ ≤ C −1
2 .

Proof. Let us ﬁrst consider Z mean

N,G . The upper bound follows from a straight forward calculation,

since the potential

is non-negative:

2

1
2σ2

N (u)(cid:13)(cid:13)

η (cid:13)(cid:13)y − mG
N,G = Eµ0(cid:16) exp(cid:0) −

Z mean

1
2σ2

2(cid:1)(cid:17) ≤ Eµ0(1) = 1.

η (cid:13)(cid:13)y − mG
N (u)(cid:13)(cid:13)

N (u)(cid:13)(cid:13)
2(cid:1)(cid:17) = exp(cid:0) −

1
2σ2
η

sup

u∈X(cid:13)(cid:13)y − mG

N (u)(cid:13)(cid:13)

2(cid:1),

For the lower bound, we have

Z mean

N,G ≥ Eµ0(cid:16) exp(cid:0) −

1
2σ2
η

sup

u∈X(cid:13)(cid:13)y − mG

since RX µ0(du) = 1. Using the triangle inequality, the assumption supu∈X kG(u)k ≤ CG and the

fact that every convergent sequence is bounded, we have

2

sup

u∈X(cid:13)(cid:13)y − mG

N (u)(cid:13)(cid:13)

≤ sup

u∈X ky − G(u)k2 + sup

u∈X(cid:13)(cid:13)G(u) − mG

N (u)(cid:13)(cid:13)

where C1 is independent of U and N .

2

=: − ln C1,

(4.1)

The proof for Z mean

inequality to derive

N,Φ is similar. For the upper bound, we use RX µ0(du) = 1 and the triangle
N (u)|(cid:1).
Since supu∈X |Φ(u)| is bounded when supu∈X kG(u)k is bounded, the fact that every convergent
sequence is bounded again gives

N (u)|(cid:1) ≤ exp(cid:0) sup

N (u)(cid:1) ≤ exp(cid:0) sup

exp(cid:0) − mΦ

u∈X |Φ(u) − mΦ

u∈X |Φ(u)| + sup

Z mean
N,Φ ≤ sup

u∈X |mΦ

u∈X

u∈X |Φ(u)| + sup
sup

u∈X |Φ(u) − mΦ

N (u)| =: − ln C2,

for a constant C2 independent of U and N . For the lower bound, we note that sinceRX µ0(du) = 1,

Z mean

N,Φ ≥ Eµ0(cid:16) exp(cid:0) − sup

u∈X |mΦ

N (u)|(cid:1)(cid:17) = exp(cid:0) − sup

u∈X |mΦ

N (u)|(cid:1) ≥ C −1

2 .

We may now prove the desired theorem and corollary concerning µy,N,G

mean and µy,N,Φ
mean .

Theorem 4.2. Under the Assumptions of Lemma 4.1, there exist constants C1 and C2, independent
of U and N , such that

dHell(µy, µy,N,G

and dHell(µy, µy,N,Φ

mean ) ≤ C1(cid:13)(cid:13)G − mG
N(cid:13)(cid:13)L2
mean ) ≤ C2(cid:13)(cid:13)Φ − mΦ
N(cid:13)(cid:13)L2

µ0

µ0

(X;RJ ) ,

(X) .

9

Proof. Let us ﬁrst consider µy,N,G

mean . By deﬁnition of the Hellinger distance, we have

2

2 d2

Hell(µy, µy,N,G

mean

dµ0 
mean ) =ZX
dµ0 −s dµy,N,G
s dµy


η ky − G(u)k2(cid:1) − exp(cid:0) −

1
4σ2

N,G (cid:16)Z −1/2 − (Z mean

N,G )−1/2(cid:17)2

2

Z ZX(cid:18)exp(cid:0) −

+ 2 Z mean

≤

=: I + II.

µ0(du)

1
4σ2

η (cid:13)(cid:13)y − mG

N (u)(cid:13)(cid:13)

µ0(du)

2(cid:1)(cid:19)2

For the ﬁrst term, we use the local Lipschitz continuity of the exponential function, together with
the equality a2 − b2 = (a − b)(a + b) and the reverse triangle inequality to bound

Z
2

1
4σ2

1
4σ2

I =ZX(cid:18)exp(cid:0) −
≤ZX(cid:18) 1
=ZX
≤

2σ2
1
4σ4

1
4σ4
η

µ0(du)

η (cid:13)(cid:13)y − mG

η (cid:16)ky − G(u)k2 −(cid:13)(cid:13)y − mG
η (cid:0)ky − G(u)k + ky − mG
u∈X(cid:0)ky − G(u)k + ky − mG

η ky − G(u)k2(cid:1) − exp(cid:0) −
2(cid:17)(cid:19)2
N (u)(cid:13)(cid:13)
N (u)k(cid:1)2(cid:13)(cid:13)G(u) − mG
N (u)(cid:13)(cid:13)
N (u)k(cid:1)2(cid:13)(cid:13)G(u) − mG
N (u)(cid:13)(cid:13)
N (u)(cid:13)(cid:13)

I ≤ C(cid:13)(cid:13)G(u) − mG

(X;RJ ) ,

2
L2

sup

µ0

2

µ0(du)

2(cid:1)(cid:19)2

N (u)(cid:13)(cid:13)

µ0(du)

2
L2

µ0

(X;RJ )

As in equation (4.1), the ﬁrst supremum can be bounded independently of U and N , from which
it follows that

for a constant C independent of U and N . For the second term, a very similar argument, together
with Lemma 4.1 and Jensen’s inequality, shows

II = 2 Z mean
≤ 2 Z mean
= 2 Z mean

N,G (cid:16)Z −1/2 − (Z mean

N,G max(Z −3, (Z mean

N,G max(Z −3, (Z mean

N,G )−1/2(cid:17)2
N,G )−3)|Z − Z mean
N,G |2
N G )−3)(cid:18)ZX
exp(cid:0) −

2
L2

µ0

(X;RJ ) ,

for a constant C independent of U and N .

≤ C(cid:13)(cid:13)G(u) − mG

N (u)(cid:13)(cid:13)

The proof for µy,N,Φ
distance dHell(µy, µy,N,Φ
we have

1
4σ2

η ky − G(u)k2(cid:1) − exp(cid:0) −

1
4σ2

η (cid:13)(cid:13)y − mG

N (u)(cid:13)(cid:13)

2(cid:1)µ0(du)(cid:19)2

mean is similar. We use an identical corresponding splitting of the Hellinger
mean ) ≤ I + II. Using the local Lipschitz continuity of the exponential function,

Z
2

I =ZX(cid:0)exp(cid:0) − Φ(u)(cid:1) − exp(cid:0) − mΦ

N (u)(cid:1)(cid:1)2

µ0(du) ≤ 2(cid:13)(cid:13)Φ(u) − mΦ

N (u)(cid:13)(cid:13)

2
L2

µ0

(X) .

10

Using Lemma 4.1 and Jensen’s inequality, we furthermore have

N,Φ max(Z −3, (Z mean

II ≤ 2 Z mean
≤ C(cid:13)(cid:13)Φ(u) − mΦ

N (u)(cid:13)(cid:13)

2
L2

µ0

(X) ,

for a constant C independent of U and N .

N,Φ )−3)(cid:18)ZX

exp(cid:0) − Φ(u)(cid:1) − exp(cid:0) − mΦ

N (u)(cid:1)µ0(du)(cid:19)2

N other than the requirement that supu∈X kG(u) − mG

We remark here that Theorem 4.2 does not make any assumptions on the predictive means
mG
N and mΦ
N (u)|
converge to 0 as N tends to ∞. Whether the predictive means are deﬁned as in (3.6), or are
derived by alternative approaches to Gaussian process regression [30], does not aﬀect the conclusions
of Theorem 4.2. Under Assumption 2.2, we can combine Theorem 4.2 with Proposition 3.3 (or
Proposition 3.4) with β = 0 to obtain error bounds in terms of the ﬁll distance of the design points.

N (u)k and supu∈X |Φ(u) − mΦ

Corollary 4.3. Suppose mΦ
k = kν,λ,σ2
satisﬁed. Then there exist constants C1 and C2, independent of U and N , such that

N , j = 1, . . . , J, are deﬁned as in (3.6), with Mat`ern kernel
. Suppose Assumption 2.2 and the assumptions of Proposition 3.3 and Theorem 4.2 are

k

N and mGj

dHell(µy, µy,N,G

mean ) ≤ C1hν+K/2

U

,

and dHell(µy, µy,N,Φ

mean ) ≤ C2hν+K/2

U

.

4.2 Approximations based on the predictive process

Alternative to the mean-based approximations considered in the previous section, we now consider
approximations to the posterior distribution µy obtained using the full predictive processes GN and
ΦN . For the remainder of this section, we denote by νG
N the
distribution of ΦN , for N ∈ N ∪ {0}. We note that since the process GN consists of J independent
Gaussian processes Gj
N . ΦN is a Gaussian
process with mean mΦ
N , for j = 1, . . . , J, is a Gaussian process
with mean mGj
N and covariance kernel kN . Replacing G by GN in (2.1), we obtain the approximation
µy,N,G
sample given by

N , the measure νG
N and covariance kernel kN , and Gj

N the distribution of GN and by νΦ

N is a product measure, νG

N = QJ

j=1 νGj

Similarly, we deﬁne for the predictive process ΦN the approximation µy,N,Φ

sample by

dµy,N,Φ
sample
dµ0

(u) =

1

Z sample

N,Φ

exp(cid:0) − ΦN (u)(cid:1),

Z sample

N,Φ = Eµ0(cid:16) exp(cid:0) − ΦN (u)(cid:1)(cid:17).

The measures µy,N,G
sample are random approximations of the deterministic measure µy.
Deterministic approximations of the posterior distribution µy can now be obtained by taking the

sample and µy,N,Φ

11

where

dµy,N,G
sample
dµ0

(u) =

1

Z sample

N,G

Z sample

N,G = Eµ0(cid:16) exp(cid:0) −

exp(cid:0) −

1
2σ2

η ky − GN (u)k2(cid:1),
η ky − GN (u)k2(cid:1)(cid:17).

1
2σ2

expected value with respect to the predictive processes GN and ΦN . This results in the marginal
approximations

dµy,N,G
marginal
dµ0

dµy,N,Φ
marginal
dµ0

(u) =

1

(Z sample

N,G

)

E

ν G
N

E

(u) =

1
(Z sample
N,Φ )

E

E

νΦ
N

η ky − GN (u)k2(cid:1)(cid:17),

ν G

1
2σ2

N(cid:16) exp(cid:0) −
N(cid:16) exp(cid:0) − ΦN (u)(cid:1)(cid:17).

νΦ

Note that by Tonelli’s Theorem, the measures µy,N,G
sures.

marginal and µy,N,Φ

marginal are indeed probability mea-

Before proving bounds on the error in the marginal approximations µy,N,G
sample and µy,N,Φ
and Z sample

section 4.2.2, and the error in the random approximations µy,N,G
crucially prove boundedness of the normalising constants Z sample

marginal and µy,N,Φ

marginal in
sample in section 4.2.3, we

in section 4.2.1.

N,G

N,Φ

4.2.1 Moment bounds on Z sample

N,G

and Z sample

N,Φ

Firstly, we recall the following classical results from the theory of Gaussian measures on Banach
spaces [29, 1].

Proposition 4.4. (Fernique’s Theorem) Let E be a separable Banach space and ν a centred Gaus-
sian measure on (E,B(E)). If λ, r > 0 are such that
log(cid:18)1 − ν(f ∈ E : kfkE ≤ r)
ZE

ν(f ∈ E : kfkE ≤ r) (cid:19) ≤ −1 − 32λr2,

E)ν(df ) ≤ exp(16λr2) +

exp(λkfk2

e2 − 1.

then

e2

Proposition 4.5. (Borell-TIS Inequality1) Let f be a scalar, almost surely bounded Gaussian ﬁeld
on a compact domain T ⊆ RK, with zero mean E(f (t)) = 0 and bounded variance 0 < σ2
f :=
supt∈T

V(f (t)) < ∞. Then E(supt∈T f (t)) < ∞, and for all r > 0,

P(sup
t∈T

f (t) − E(sup

t∈T

f (t)) > r) ≤ exp(−r2/2σ2
f ).

Proposition 4.6. (Sudakov-Fernique Inequality) Let f and g be scalar, almost surely bounded
Gaussian ﬁelds on a compact domain T ⊆ RK. Suppose E((f (t) − f (s))2) ≤ E((g(t) − g(s))2) and
E(f (t)) = E(g(t)), for all s, t ∈ T . Then
E(sup
t∈T

f (t)) ≤ E(sup

g(t)).

t∈T

1The Borell-TIS inequality is named after the mathematicians Borell and Tsirelson, Ibragimov and Sudakov, who

independently proved the result.

12

Using these results, we are now ready to prove bounds on moments of Z sample

and Z sample
N,Φ ,
similar to those proved in Lemma 4.1. The reader interested purely in approximation results for
the posterior can simply read the statements of the following two lemmas, and then proceed directly
to subsections 4.2.2 and 4.2.3.

N,G

Lemma 4.7. Let X ⊆ RK be compact. Suppose supu∈X(cid:13)(cid:13)G(u) − mG
N (u)(cid:13)(cid:13), supu∈X(cid:12)(cid:12)Φ(u) − mΦ
N (u)(cid:12)(cid:12)
and supu∈X kN (u, u) converge to 0 as N tends to inﬁnity, and assume supu∈X kG(u)k ≤ CG < ∞.
Suppose the assumptions of the Sudakov-Fernique inequality hold, for g = Φ0 and f = ΦN − mΦ
N ,
and for g = Gj
N , for j ∈ {1, . . . , J}. Then, for any 1 ≤ p < ∞, there exist positive
constants C1 and C2, independent of U and N , such that for all N suﬃciently large

0 and f = Gj

C −1
1 ≤ E

and

C −1
2 ≤ E

νΦ

ν G

N,G

N − mGj
N(cid:0)(Z sample
N(cid:0)(Z sample

)p(cid:1) ≤ 1,
N,Φ )p(cid:1) ≤ C2,

and

1 ≤ E

ν G

and

C −1

2

E

N,G

N(cid:0)(Z sample
N(cid:0)(Z sample

)−p(cid:1) ≤ C1.
N,Φ )−p(cid:1) ≤ C2.

νΦ

Proof. We start with Z sample

N,G

. Since the potential

1 =RC0(X;RJ ) νG

N (dGN), we have for any 1 ≤ p < ∞,
1
exp(cid:0) −
2σ2

)p) =ZC0(X;RJ )(cid:18)ZX

N,G

((Z sample

E

ν G
N

From Jensen’s inequality, it then follows that

1
2σ2

η ky − GN (u)k2 is non-negative andRX µ0(du) =
η ky − GN (uk2(cid:1)µ0(du)(cid:19)p

νG
N (dGN) ≤ 1.

((Z sample

N,G

E

ν G
N

((Z sample

N,G

ν G
N

)−p(cid:1) ≥(cid:0)E

)p)(cid:1)−1 ≥ 1.

To determine C1, we use the triangle inequality to bound, for any 1 ≤ p < ∞,

1
2σ2

η ky − GN (u)k2(cid:1)µ0(du)(cid:19)−p

νG
N (dGN)

)−p(cid:1) =ZC0(X;RJ )(cid:18)ZX

exp(cid:0) −

E

1
2σ2
η

ν G

N,G

N(cid:0)(Z sample
≤ZC0(X;RJ )(cid:0) exp(cid:0) −
=ZC0(X;RJ )
exp(cid:0)
≤ exp  supu∈X ky − mG

p
2σ2
η

sup

u∈X ky − GN (u)k2(cid:1)νG
!ZC0(X;RJ )
N (u)k2
with supu∈X kG(u)k ≤ CG and supu∈X(cid:13)(cid:13)G(u) − mG

2p−1σ2
η

sup

N (dGN)

u∈X ky − GN (u)k2(cid:1)(cid:1)−pνG
N (dGN)
exp  supu∈X kGN (u) − mG

2p−1σ2
η

N (u)(cid:13)(cid:13) → 0 as N → ∞. For the second factor, we

The ﬁrst factor can be bounded independently of U and N using the triangle inequality, together

N (u)k2

! νG
N (dGN).

13

use Fernique’s Theorem (Proposition 4.4). First, we note that (using independence)

J

2p−1σ2
η

exp  supu∈X kGN (u) − mG
ZC0(X;RJ )
exp  supu∈XPJ
j=1 |Gj
=ZC0(X;RJ )
exp
supu∈X |Gj
≤ZC0(X;RJ )
Xj=1

exp  supu∈X |Gj
=ZC0(X;RJ )
Yj=1
exp  supu∈X |Gj
Yj=1ZC0(X)

N (u)|2

N (u)k2

! νG
N (dGN)
N (u) − mGj
! νG
N (u)|2
N (dGN)

 νG
! νG
N (dGN)
! νGj
N (dGj
N).

N (dGN)

N (u)|2

N (u)|2

N (u) − mGj
2p−1σ2
η
N (u) − mGj
2p−1σ2
η

2p−1σ2
η
N (u) − mGj
2p−1σ2
η

=

J

J

It remains to show that, for N suﬃciently large, the assumptions of Fernique’s Theorem hold for
η /2 and a value of r independent of U and N , for ν equal to the push-forward of νGj
λ = pσ−2
under the map T (f ) = f − mGj
N,r ⊂ C 0(X) the set of all functions f such that
kf − mGj
N . By the Borell-TIS
Inequality, we have for all r > E(supu∈X (Gj

N kC0(X) ≤ r, for some ﬁxed r > 0 and 1 ≤ j ≤ J. Let Gj

N . Denote by BGj

N − mGj

N = Gj

N (u)),

N

νGj
N (Gj

N : sup

u∈X Gj

N (u) > r) ≤ exp −(cid:0)r − E(supu∈X Gj

2σ2
N

N (u)(cid:1)2
N (u′))2) ≤ E

! ,
((Gj

where σ2
and so E(supu∈X(Gj
choose r > E(supu∈X(Gj
νGj
N (Gj

N := supu∈X kN (u, u). By assumption, E

N (u)) ≤ E(supu∈X (Gj

0(u′))2),
0(u)), by the Sudakov-Fernique Inequality. We can hence

N (u) − Gj

0(u) − Gj

((Gj

νj
N

νj
0

0(u)), independent of U and N , such that the bound

N : sup

u∈X Gj

N (u) > r) ≤ exp −(cid:0)r − E(supu∈X Gj

2σ2
N

0(u)(cid:1)2

! ,

holds for all N ∈ N . By assumption we have σ2
measures, we hence have νGj
suﬃciently large, the inequality

N (BGj

N → 0 as N → ∞, and by the symmetry of Gaussian
0(u)). For N = N (p)

N,r) → 1 as N → ∞, for all r > E(supu∈X(Gj
log  1 − νGj

N (BGj
N,r) ! ≤ −1 − 32λr2,

N (BGj
νGj

N,r)

in the assumptions of Fernique’s Theorem is then satisﬁed, for λ = pσ−2

both independent of U and N . Hence, E(cid:0)(Z sample

dent of U and N . From Jensen’s inequality, it then ﬁnally follows that

)−p(cid:1) ≤ C1(p), for a constant C1(p) < ∞ indepen-

N,G

η /2 and r > E(supu∈X(Gj

0(u)),

((Z sample

N,G

E

ν G
N

)p(cid:1) ≥(cid:0)E

((Z sample

N,G

ν G
N

14

)−p)(cid:1)−1 ≥ C −1

1 (p).

The proof for Z sample

N,Φ

is similar. Using RX µ0(du) = 1 and the triangle inequality, we have

E

νΦ

N(cid:0)(Z sample

N,Φ )p(cid:1) =ZC0(X)(cid:18)ZX

exp(cid:0) − ΦN (u)(cid:1)µ0(du)(cid:19)p
u∈X |ΦN (u)|(cid:1)νΦ
N (u)|(cid:1)ZC0(X)

exp(cid:0)p sup

N (dΦN)

≤ZC0(X)
≤ exp(cid:0)p sup

exp(cid:0)p sup
u∈X |mΦ

νΦ
N (dΦN)

u∈X |ΦN (u) − mΦ

N (dΦN).

N (u)|(cid:1)νΦ

The ﬁrst factor can be bounded independently of U and N , since supu∈X kG(u)k ≤ CG and
supu∈X(cid:12)(cid:12)Φ(u) − mΦ
N (u)(cid:12)(cid:12) converges to 0 as N → ∞. The second factor can be bounded by Fer-
nique’s Theorem. Using the same proof technique as above, we can show that νΦ
N,r) → 1 as
N → ∞ for all r > E(supu∈X(Φ0(u)), where BΦ
N,r ⊂ C 0(X) denotes the set of all functions f such
that kf − mΦ
NkC0(X) ≤ r. Hence, it is possible to choose r > 0, independent of U and N , such that
the assumptions of Fernique’s Theorem hold for ν equal to the push-forward of νΦ
N under the map
T (f ) = f − mΦ

N , for some λ > 0 also independent of U and N . By Young’s inequality, we have

N (BΦ

u∈X |ΦN (u) − mΦ

exp(cid:0)p sup

N (u)|(cid:1) ≤ exp(cid:0)λ sup

u∈X |ΦN (u) − mΦ

N (u)|2 + p2/4λ(cid:1),

and it follows that Eω(cid:0)(Z sample
any 1 ≤ p < ∞. Furthermore, we note
N(cid:0)(Z sample

N,Φ )p(cid:1) ≤ C2(p), for a constant C2(p) < ∞ independent of U and N , for
N,Φ )−p(cid:1) ≤ZC0(X;R)

u∈X |ΦN (u)|(cid:1)νΦ

νΦ

E

By Jensen’s inequality, we ﬁnally have E

νΦ

N(cid:0)(Z sample

N (dΦN) ≤ C2(p).
N(cid:0)(Z sample

exp(cid:0)p sup
N,Φ )−p(cid:1) ≥ C2(p)−1 and E
0 and f = Gj

N − mGj

νΦ

N,Φ )p(cid:1) ≥ C2(p)−1.

In Lemma 4.7, we supposed that the assumptions of the Sudakov-Fernique inequality hold, for
g = Φ0 and f = ΦN − mΦ
N , for j ∈ {1, . . . , J}. This is an
assumption on the predictive variance kN . In the following Lemma, we prove this assumption for
the predictive variance given in (3.6).

N , and for g = Gj

Lemma 4.8. Suppose the predictive variance kN is given by (3.6). Then the assumptions of the
N −mGj
Sudakov-Fernique inequality hold, for g = Φ0 and f = ΦN −mΦ
N ,
for j ∈ {1, . . . , J}.
Proof. We give a proof for g = Φ0 and f = ΦN − mΦ
j ∈ {1, . . . , J}, is identical. For any u, u′ ∈ X, we have E
and
N (cid:0)((ΦN (u) − mΦ

N (u′)))2(cid:1) = kN (u, u) − kN (u, u′) − kN (u′, u) + kN (u′, u′),

N − mGj
(ΦN (u) − mΦ

N , the proof for g = Gj

N (u)) − (ΦN (u′) − mΦ

N , and for g = Gj

0 and f = Gj

0 and f = Gj

0 (cid:0)(Φ0(u) − Φ0(u′))2(cid:1) = k(u, u) − k(u, u′) − k(u′, u) + k(u′, u′).

(Φ0(u)) = 0 = E

N , for
N (u)),

νΦ
N

νΦ
0

νΦ

νΦ

E

E

By (3.6), we have

kN (u, u′) = k(u, u′) − k(u, U )T K(U, U )−1 k(u′, U ),

15

and so

E

νΦ

0 (cid:0)(Φ0(u) − Φ0(u′))2(cid:1) − E
=(cid:0)k(u, U )T − k(u′, U )T(cid:1) K(U, U )−1 (cid:0)k(u, U ) − k(u′, U )(cid:1)
≥ 0,

N (cid:0)((ΦN (u) − mΦ

νΦ

N (u)) − (ΦN (u′) − mΦ

N (u′)))2(cid:1)

since the matrix K(U, U )−1 is positive deﬁnite.

We are now ready to prove bounds on the approximation error in the posterior distributions.

4.2.2 Error in the marginal approximations µy,N,G

marginal and µy,N,Φ

marginal

We start by analysing the error in the marginal approximations µy,N,G

marginal and µy,N,Φ

marginal.

Theorem 4.9. Under the assumptions of Lemma 4.7, there exist constants C1 and C2, independent
of U and N , such that

Proof. We start with µy,N,G

marginal. By the deﬁnition of the Hellinger distance, we have

dHell(µy, µy,N,G

dHell(µy, µy,N,Φ

.

E

µ0

ν G

(X)

νΦ
N

N(cid:16)kG − GNk1+δ(cid:17)(cid:17)1/(1+δ)(cid:13)(cid:13)(cid:13)(cid:13)L2
marginal) ≤ C1(cid:13)(cid:13)(cid:13)(cid:13)(cid:16)E
(|Φ − ΦN|)(cid:13)(cid:13)(cid:13)L2
marginal) ≤ C2(cid:13)(cid:13)(cid:13)
dµ0 −s dµy,N,G
dµ0 
marginal) =ZX
s dµy


η ky − G(u)k2(cid:1) −sE
N(cid:16) exp(cid:0) −
(cid:1)−1/2(cid:17)2
(cid:1)(cid:16)Z −1/2 − E
N(cid:0)Z sample

1
2σ2

1
2σ2

µ0(du)

marginal

N,G

N,G

ν G

ν G

2

2 d2

Hell(µy, µy,N,G

≤

2

Z ZX sexp(cid:0) −
N(cid:0)Z sample

+ 2E

ν G

= I + II.

,

for any δ > 0,

(X)

µ0

η ky − GN (u)k2(cid:1)(cid:17)!2

µ0(du)

for a, b > 0, to derive

ν G

Z
2

1
2σ2

For the ﬁrst term, we use the (in)equalities a − b = (a2 − b2)/(a + b) and (√a + √b)2 ≥ a + b,
I =ZX sexp(cid:0) −
≤ZX (cid:16)exp(cid:0) − 1
exp(cid:0) − 1
u∈X(cid:18)exp(cid:0) −

η ky − G(u)k2(cid:1) −sE
η ky − G(u)k2(cid:1) − E
η ky − G(u)k2(cid:1) + E
η ky − G(u)k2(cid:1) + E

N(cid:16) exp(cid:0) −
N(cid:16) exp(cid:0) − 1
N(cid:16) exp(cid:0) − 1
N(cid:16) exp(cid:0) −

η ky − GN (u)k2(cid:1)(cid:17)!2

η ky − GN (u)k2(cid:1)(cid:17)(cid:17)2
η ky − GN (u)k2(cid:1)(cid:17)
η ky − GN (u)k2(cid:1)(cid:17)(cid:19)−1
η ky − GN (u)k2(cid:1)(cid:17)(cid:19)2

1
2σ2

η ky − G(u)k2(cid:1) − E

ZX(cid:18)exp(cid:0) −

N(cid:16) exp(cid:0) −

≤ sup

1
2σ2

1
2σ2

1
2σ2

1
2σ2

µ0(du).

µ0(du)

µ0(du)

2σ2

2σ2

2σ2

2σ2

ν G

ν G

ν G

ν G

16

For the ﬁrst factor, using the convexity of 1/x on (0,∞), together with Jensen’s inequality, we have
for all u ∈ X the bound
1
2σ2

1
2σ2

(cid:18)exp(cid:0) −

ν G

1
2σ2

η ky − G(u)k2(cid:1) + E

N(cid:16) exp(cid:0) −
η ky − G(u)k2(cid:1)−1 + E
η ky − G(u)k2(cid:1) + E
u∈X ky − G(u)k2(cid:1) + E

N(cid:16) exp(cid:0)

1
2σ2
1
2σ2
η

sup

ν G

ν G

ν G

≤ exp(cid:0) −
≤ exp(cid:0)
≤ exp(cid:0)

η ky − GN (u)k2(cid:1)(cid:17)(cid:19)−1

N(cid:16) exp(cid:0) −

1
2σ2

η ky − GN (u)k2(cid:1)(cid:17)−1

1
2σ2

η ky − GN (u)k2(cid:1)(cid:17)

1
2σ2
η

N(cid:16) exp(cid:0)

sup

u∈X ky − GN (u)k2(cid:1)(cid:17).

µ0(du)

µ0(du),

As in the proof of Lemma 4.7, it then follows by Fernique’s Theorem that the right hand side can
be bounded by a constant independent of U and N .

ν G

ν G

µ0(du)

1
2σ2

1
2σ2

1
2σ2

1
2σ2

For the second factor in the bound on Z

η ky − G(u)k2(cid:1) − E

N(cid:16) exp(cid:0) −
η ky − G(u)k2(cid:1) − exp(cid:0) −

η ky − GN (u)k2(cid:1)(cid:17)(cid:19)2
η ky − GN (u)k2(cid:1)(cid:17)(cid:19)2

2 I, the linearity of expectation, the local Lipschitz
continuity of the exponential function, the equality a2 − b2 = (a − b)(a + b), the reverse triangle
inequality and H¨older’s inequality with conjugate exponents p = (1 + δ)/δ and q = 1 + δ give
ZX(cid:18)exp(cid:0) −
=ZX(cid:18)E
N(cid:16) exp(cid:0) −
≤ 2ZX(cid:18)E
N(cid:16)|
η ky − G(u)k2 −
η ZX(cid:16)E
N(cid:16) (ky − G(u)k + ky − GN (u)k)kG(u) − GN (u)k(cid:17)(cid:17)2
≤
η ZX(cid:16)E
N(cid:16) (ky − G(u)k + ky − GN (u)k)(1+δ)/δ(cid:17)(cid:17)2δ/(1+δ)(cid:16)E
N(cid:16) (ky − G(u)k + ky − GN (u)k)(1+δ)/δ(cid:17)(cid:17)2δ/(1+δ)ZX(cid:16)E
u∈X(cid:16)E

N(cid:16)kG(u) − GN (u)k1+δ(cid:17)(cid:17)2/(1+δ)
for any δ > 0. The supremum in the above expression can be bounded by a constant independent
of U and N by Fernique’s Theorem as in the proof of Lemma 4.7, since supu∈X kG(u)k ≤ CG < ∞.
It follows that there exists a constant C independent of U and N such that

N(cid:16)kG(u) − GN (u)k1+δ(cid:17)(cid:17)2/(1+δ)

η ky − GN (u)k2 |(cid:17)(cid:19)2

2
4σ4
2
4σ4
2
4σ4
η

1
2σ2

1
2σ2

µ0(du)

µ0(du)

µ0(du)

sup

≤

≤

ν G

ν G

ν G

ν G

ν G

ν G

For the second term in the bound on the Hellinger distance, we have

2

ν G

Z
2

N(cid:16)kGN − Gk1+δ(cid:17)(cid:17)1/(1+δ)(cid:13)(cid:13)(cid:13)(cid:13)
I ≤ C(cid:13)(cid:13)(cid:13)(cid:13)(cid:16)E
(cid:1)(cid:1)−1/2(cid:17)2
II =(cid:16)Z −1/2 −(cid:0)E
N(cid:0)Z sample

≤ max(Z −3, (E

N,G

ν G

L2

µ0

.

(X)

1

2E

ν G

N,G

N(cid:0)Z sample

(cid:1)

ν G

N,G

N(cid:0)Z sample

(cid:1))−3)|Z−E

ν G

N,G

N(cid:0)Z sample

(cid:1)|2.

17

Using the linearity of expectation, Tonelli’s Theorem and Jensen’s inequality, we have

2

E

ν G

N,G

(cid:12)(cid:12)(cid:12)
Z − E
=(cid:12)(cid:12)(cid:12)(cid:12)
ZX
≤ZX(cid:18)E

(cid:1)(cid:12)(cid:12)(cid:12)
N(cid:0)Z sample
N(cid:16) exp(cid:0) −
N(cid:16) exp(cid:0) −

ν G

ν G

1
2σ2

η ky − G(u)k2(cid:1) − exp(cid:0) −
η ky − G(u)k2(cid:1) − exp(cid:0) −

1
2σ2

1
2σ2

η ky − GN (u)k2(cid:1)(cid:17)µ0(du)(cid:12)(cid:12)(cid:12)(cid:12)
η ky − GN (u)k2(cid:1)(cid:17)(cid:19)2

1
2σ2

2

µ0(du).

which can now be bounded as before. The ﬁrst claim of the theorem now follows by Lemma 4.7.
marginal is similar. We use an identical corresponding splitting of the Hellinger
marginal) ≤ I + II. For the ﬁrst term, we have

The proof for µy,N,Φ
distance dHell(µy, µy,N,Φ

Z
2

I =ZX qexp(cid:0) − Φ(u)(cid:1) −rE
u∈X(cid:16)exp(cid:0) − Φ(u)(cid:1) + E
≤ sup

N(cid:16) exp(cid:0) − ΦN (u)(cid:1)(cid:17)!2
N(cid:16) exp(cid:0) − ΦN (u)(cid:1)(cid:17)(cid:17)−1

νΦ

νΦ

µ0(du)

νΦ

N(cid:16) exp(cid:0) − ΦN (u)(cid:1)(cid:17)(cid:17)2

µ0(du).

ZX(cid:16)exp(cid:0) − Φ(u)(cid:1) − E
N(cid:16) exp(cid:0) − ΦN (u)(cid:1)(cid:17)(cid:17)−1

The ﬁrst factor can again be bounded using Jensen’s inequality,

sup

u∈X(cid:16)exp(cid:0) − Φ(u)(cid:1) + E

νΦ

≤ exp(cid:0) sup

u∈X

Φ(u)(cid:1) + E

νΦ

N(cid:16) exp(cid:0) sup

u∈X

ΦN (u)(cid:1)(cid:17),

which as in the proof of Lemma 4.7, can be bounded by a constant independent of U and N by
Fernique’s Theorem. For the second factor in the bound on Z
2 I, the linearity of expectation and
the local Lipschitz continuity of the exponential function give

ZX(cid:16)exp(cid:0) − Φ(u)(cid:1) − E

νΦ

N(cid:16) exp(cid:0) − ΦN (u)(cid:1)(cid:17)(cid:17)2

µ0(du)

νΦ

νΦ

=ZX(cid:16)E
≤ 2ZX(cid:16)E
= 2(cid:13)(cid:13)(cid:13)

N(cid:16) exp(cid:0) − Φ(u)(cid:1) − exp(cid:0) − ΦN (u)(cid:1)(cid:17)(cid:17)2
N(cid:16)|Φ(u) − ΦN (u)|(cid:17)(cid:17)2
(|Φ(u) − ΦN (u)|)(cid:13)(cid:13)(cid:13)

µ0(du)

νΦ
N

(X)

L2

µ0

E

2

.

µ0(du)

For the second term in the bound on the Hellinger distance, the linearity of expectation, Tonelli’s

Theorem and Jensen’s inequality give

(cid:12)(cid:12)(cid:12)

Z − E

νΦ

N,Φ (cid:1)(cid:12)(cid:12)(cid:12)
N(cid:0)Z sample

2

≤ZX(cid:16)E

νΦ

N(cid:16) exp(cid:0) − Φ(u)(cid:1) − exp(cid:0) − ΦN (u)(cid:1)(cid:17)(cid:17)2

µ0(du),

which can now be bounded as before. The second claim of the theorem then follows by Lemma
4.7.

18

Similar to Theorem 4.2, Theorem 4.9 provides error bounds for general Gaussian process em-
ulators of G and Φ. An example of a Gaussian process emulator that satisﬁes the assumptions of
Theorem 4.9 is the emulator deﬁned by (3.6), however, other choices are possible.

Corollary 4.10. Suppose GN and ΦN are deﬁned as in (3.6), with Mat`ern kernel k = kν,λ,σ2
.
k
Suppose Assumption 2.2 and the assumptions of Proposition 3.3 and Theorem 4.9 are satisﬁed.
Then there exist constants C1, C2, C3 and C4, independent of U and N , such that

dHell(µy, µy,N,G

marginal) ≤ C1hν+K/2
Proof. We give the proof for µy,N,G
inequality and the triangle inequality, we have

+ C2hν
U ,

U

and dHell(µy, µy,N,Φ

marginal) ≤ C3hν+K/2

U

+ C4hν
U .

marginal, the proof for µy,N,Φ

marginal is similar. Using Theorem 4.9, Jensen’s

dHell(µy, µy,N,G

2

ν G

marginal)2 ≤ C(cid:13)(cid:13)(cid:13)(cid:13)(cid:16)E
= CZX
≤ 2CZX kG(u) − mG

N(cid:16)kG − GNk2(cid:17)(cid:17)1/2(cid:13)(cid:13)(cid:13)(cid:13)
N(cid:16)kG(u) − GN (u)k2(cid:17)µ0(du)

N (u)k2µ0(du) + 2CZX

(X)

ν G

L2

µ0

E

E

ν G

N(cid:16)kmG

N (u) − GN (u)k2(cid:17)µ0(du).

The ﬁrst term can be bounded by using Assumption 2.2, Proposition 3.2 and Proposition 3.3,

ZX kG(u) − mG

N (u)k2µ0(du) =ZX

J

(Gj (u) − mGj
Xj=1

N (u))2µ0(du) ≤ Ch2ν+K

U

J

Xj=1

kGjk2

H ν+K/2(X),

for a constant C independent of U and N . The second term can be bounded by using Assumption
2.2, Proposition 3.2, Proposition 3.3, Proposition 3.5, the linearity of expectation and the Sobolev
Embedding Theorem

E

ZX

ν G

N(cid:16)kmG

E

N (u) − GN (u)k2(cid:17)µ0(du) =ZX
= JZX
≤ J sup
≤ Ch2ν
U ,

u∈X

ν G

N(cid:16)

J

Xj=1

(mGj

N (u) − Gj

N (u))2(cid:17)µ0(du)

kN (u, u)µ0(du)

sup

kgkHk =1|g(u) − mg

N (u)|2

for a constant C independent of U and N . The claim of the corollary then follows.

Note that it in Corollary 4.10, we were not able to recover the convergence rate hν+K/2

, corre-
sponding to the convergence rate of the error measured in the L2(X)-norm in Proposition 3.3. The
reason for this is the supremum over g appearing in the expression for kN (u, u) in Proposition 3.5,
which allows us only to conclude on the lower rate of convergence hν

U

U for kk1/2

N kL2(X).

19

4.2.3 Error in the random approximations µy,N,G

sample and µy,N,Φ

sample

We have the following result for the random approximations µy,N,G

sample and µy,N,Φ
sample.

Theorem 4.11. Under the Assumptions of Lemma 4.7, there exist constants C1 and C2, indepen-
dent of U and N , such that

(cid:16)E
(cid:16)E

ν G

N (cid:16)dHell(µy, µy,N,G
N (cid:16)dHell(µy, µy,N,Φ

sample)2(cid:17)(cid:17)1/2
sample)2(cid:17)(cid:17)1/2

νΦ

≤ C1(cid:13)(cid:13)(cid:13)(cid:13)(cid:16)E
≤ C2(cid:13)(cid:13)(cid:13)(cid:13)(cid:16)E

ν G

νΦ

N(cid:16)kG − GNk2+δ(cid:17)(cid:17)1/(2+δ)(cid:13)(cid:13)(cid:13)(cid:13)L2
N(cid:16)|Φ − ΦN|2(cid:17)(cid:17)1/2(cid:13)(cid:13)(cid:13)(cid:13)L2

(X)

µ0

.

,

(X)

µ0

Proof. We start with µy,N,G
expectation, we have

sample. By the deﬁnition of the Hellinger distance and the linearity of

E

ν G

ν G

= E

sample)(cid:17)2

N 
ZX


N (cid:16)2 dHell(µy, µy,N,G
≤

dµ0 −s dµy,N,G
dµ0 
s dµy

N (cid:18)ZX(cid:0)exp(cid:0) − Φ(u)/2(cid:1) − exp(cid:0) − ΦN (u)/2(cid:1)(cid:1)2 µ0(du)(cid:19)
N (cid:16)Z sample

|Z −1/2 − (Z sample

)−1/2|2(cid:17)

+ 2 E

sample

2
Z

N,G

N,G

ν G

ν G

E

=: I + II.

2

µ0(du)


For the ﬁrst term, Tonelli’s Theorem, the local Lipschitz continuity of the exponential function,
the equality a2 − b2 = (a − b)(a + b), the reverse triangle inequality and H¨older’s inequality with
conjugate exponents p = (1 + δ)/δ and q = 1 + δ give

η ky − GN (u)k2(cid:1)(cid:19)2! µ0(du)

Z
2

E

E

ν G

ν G

1
4σ2

1
4σ2

I =ZX
η ZX
≤
η ZX
η ZX(cid:16)E
u∈X(cid:16)E

1
σ2
1
σ2
1
σ2
1
σ2
η

N  (cid:18)exp(cid:0) −
η ky − G(u)k2(cid:1) − exp(cid:0) −
N (cid:16)(cid:0)ky − G(u)k2 − ky − GN (u)k2(cid:1)2(cid:17) µ0(du)
N(cid:16) (ky − GN (u)k + ky − G(u)k)2 kGN (u) − G(u)k2(cid:17)µ0(du)
N (cid:16)(ky − G(u)k + ky − GN (u)k)2(1+δ)/δ)(cid:17)(cid:17)δ/(δ+1)(cid:16)E
N (cid:16)(ky − G(u)k + ky − GN (u)k)2(1+δ)/δ)(cid:17)(cid:17)δ/(δ+1)ZX(cid:16)E

sup

≤

≤

≤

ν G

ν G

ν G

ν G

E

N (cid:16)kG(u) − GN (u)k2(1+δ)(cid:17)(cid:17)1/(1+δ)

ν G

N (cid:16)kG(u) − GN (u)k2(1+δ)(cid:17)(cid:17)1/(1+δ)

µ0(du)

µ0(du).

for any δ > 0. The supremum in the above bound can be bounded independently of U and N
by Fernique’s Theorem as in the proof of Lemma 4.7. It follows that there exists a constant C
independent of U and N such that

Z
2

I ≤ C (cid:13)(cid:13)(cid:13)(cid:13)(cid:16)E

ν G

N(cid:16)kGN − Gk2(1+δ)(cid:17)(cid:17)1/2(1+δ)(cid:13)(cid:13)(cid:13)(cid:13)

20

2

.

L2

µ0

(X)

For the second term in the bound on the Hellinger distance, we have

1
2

II = E

≤ E

N,G

|Z −1/2 − (Z sample
N,G max(Z −3, (Z sample

N,G

N,G

ν G

ν G

N (cid:16)Z sample
N (cid:16)Z sample

)−1/2|2(cid:17)
)−3)|Z − Z sample

N,G

|2(cid:17) .

By Jensen’s inequality and the same argument as above, we have

|Z − Z sample

N,G

|2 =(cid:12)(cid:12)(cid:12)(cid:12)

≤

ZX(cid:18)exp(cid:0) −
η ZX

1
σ4

1
4σ2

η ky − G(u)k2(cid:1) − exp(cid:0) −

2

1
4σ2

η ky − GN (u)k2(cid:1)(cid:19) µ0(du)(cid:12)(cid:12)(cid:12)(cid:12)

(ky − GN (u)k + ky − G(u)k)2 kGN (u) − G(u)k2µ0(du).

Together with Tonelli’s Theorem and H¨older’s inequality with conjugate exponents p = (1 + δ)/δ
and q = 1 + δ, we then have

II

1
2
≤

=

≤

E

E

ν G

ν G

N,G

1
σ4
η
1
σ4
1
σ4
η

N (cid:18)Z sample
N,G max(Z −3, (Z sample
N (cid:16)Z sample
N,G max(Z −3, (Z sample
N (cid:16)(Z sample
N (cid:16)kGN (u) − G(u)k2(1+δ)(cid:17)(cid:17)1/(1+δ)

η ZX
u∈X(cid:16)E
ZX(cid:16)E

sup

N,G

N,G

ν G

ν G

)(1+δ)/δ max(Z −3, (Z sample

N,G

µ0(du),

(ky − GN (u)k + ky − G(u)k)2 kGN (u) − G(u)k2µ0(du)(cid:19)
)−3)ZX
)−3)(ky − GN (u)k + ky − G(u)k)2kGN (u) − G(u)k2(cid:17) µ0(du)

)−3)(1+δ)/δ (ky − G(u)k + ky − GN (u)k)2(1+δ)/δ)(cid:17)(cid:17)δ/(δ+1)

for any δ > 0. The supremum in the bound above can be bounded independently of U and N by
Lemma 4.7 and Fernique’s Theorem. The ﬁrst claim of the Theorem then follows.

The proof for µy,N,Φ

sample is similar. Using an identical corresponding splitting of the Hellinger

distance E
local Lipschitz continuity of the exponential function:

sample)(cid:17) ≤ I + II, we bound the ﬁrst term by Tonelli’s Theorem and the

νΦ

Hell(µy, µy,N,Φ

N (cid:16)2 d2
N (cid:16)(cid:0)exp(cid:0) − Φ(u)/2(cid:1) − exp(cid:0) − ΦN (u)/2(cid:1)(cid:1)2(cid:17) µ0(du) ≤(cid:13)(cid:13)(cid:13)(cid:13)(cid:16)E

ν G

Z
2

I =ZX

E

2

.

(X)

L2

µ0

ν G

N(cid:16)(ΦN − Φ)2(cid:17)(cid:17)1/2(cid:13)(cid:13)(cid:13)(cid:13)
N,Φ |2(cid:17) .

For the second term, we have as before

1
2

II ≤ E

and

νΦ

N (cid:16)Z sample

N,Φ max(Z −3, (Z sample

N,Φ )−3)|Z − Z sample

|Z − Z sample

N,Φ |2 =(cid:12)(cid:12)(cid:12)(cid:12)

ZX(cid:0)exp(cid:0) − Φ(u)(cid:1) − exp(cid:0) − ΦN (u)(cid:1)(cid:1) µ0(du)(cid:12)(cid:12)(cid:12)(cid:12)

2

≤ 4ZX

(Φ(u) − ΦN (u))2µ0(du).

21

Together with Tonelli’s Theorem and H¨older’s inequality with conjugate exponents p = (1 + δ)/δ
and q = 1 + δ, we then have

(Φ(u) − ΦN (u))2µ0(du)(cid:19)
N,Φ )−3)ZX
N,Φ )−3)(Φ(u) − ΦN (u))2(cid:17) µ0(du)

1
2

II ≤ 4E
= 4ZX
≤ 4(cid:16)E

E

νΦ

νΦ

N,Φ max(Z −3, (Z sample

N (cid:18)Z sample
N,Φ max(Z −3, (Z sample
N (cid:16)Z sample
N (cid:16)(Z sample
N,Φ )(1+δ)/δ max(Z −3, (Z sample
ZX(cid:16)E

N (cid:16)kΦ(u) − ΦN (u)k2(1+δ)(cid:17)(cid:17)1/(1+δ)

νΦ

ν G

N,Φ )−3)(1+δ)/δ(cid:17)(cid:17)δ/(δ+1)

µ0(du),

for any δ > 0. The ﬁrst expected value in the bound above can be bounded independently of U
and N by Lemma 4.7. The second claim of the Theorem then follows.

Similar to Theorem 4.2 and Theorem 4.9, Theorem 4.11 provides error bounds for general
Gaussian process emulators of G and Φ. As a particular example, we can take the emulators
deﬁned by (3.6).

Corollary 4.12. Suppose GN and ΦN are deﬁned as in (3.6), with Mat`ern kernel k = kν,λ,σ2
.
k
Suppose Assumption 2.2 and the assumptions of Proposition 3.3 and Theorem 4.11 are satisﬁed.
Then there exist constants C1, C2, C3 and C4, independent of U and N , such that

dHell(µy, µy,N,G

marginal) ≤ C1hν+K/2

U

+ C2hν
U ,

and dHell(µy, µy,N,Φ

marginal) ≤ C3hν+K/2

U

+ C4hν
U .

Proof. The proof is similar to that of Corollary 4.10, exploiting that for a Gaussian random variable
X, we have E((X − E(X))4) = 3(E((X − E(X))2))2.

We furthermore have the following result on a generalised total variation distance [31], deﬁned

by

dgTV(µy, µy,N,G

sample) =

sup

kf kC0 (X)≤1(cid:16)E

for µy,N,G

sample, and deﬁned analogously for µy,N,Φ
sample.

ν G

N(cid:0)|Eµy (f ) − E

µy,N,G
sample

(f )|2(cid:1)(cid:17)1/2

,

Theorem 4.13. Under the Assumptions of Lemma 4.7, there exist constants C1 and C2, indepen-
dent of U and N , such that

dgTV(µy, µy,N,G

dgTV(µy, µy,N,Φ

sample) ≤ C1 (cid:13)(cid:13)(cid:13)(cid:13)(cid:16)E
sample) ≤ C2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:16)E

,

(X)

µ0

ν G

ν G

N(cid:16)kG − GNk2+δ(cid:17)(cid:17)1/(2+δ)(cid:13)(cid:13)(cid:13)(cid:13)L2
N(cid:16)kΦ − ΦNk2(cid:17)(cid:17)1/2(cid:13)(cid:13)(cid:13)(cid:13)L2

(X)

µ0

.

22

Proof. We give the proof for µy,N,Φ

sample; the proof for µy,N,G

sample is identical. By deﬁnition, we have

νΦ

νΦ

=

sup

sup

µy,N,Φ
sample

dgTV(µy, µy,N,Φ

sample) =

(f )|2(cid:1)(cid:17)1/2

N(cid:0)|Eµy (f ) − E

kf kC0 (X)≤1(cid:16)E
N  (cid:12)(cid:12)(cid:12)(cid:12)
kf kC0(X)≤1 E
ZX
N  (cid:12)(cid:12)(cid:12)(cid:12)
≤ E
ZX(cid:16)exp(−Φ(u))Z −1 − exp(−ΦN (u))(Z sample
N (cid:18)ZX | exp(−Φ(u)) − exp(−ΦN (u))|2µ0(du)(cid:19)(cid:19)1/2
Z (cid:18)E
≤
N (cid:16)(Z sample
(cid:16)E

f (u)(cid:16)exp(−Φ(u))Z −1 − exp(−ΦN (u))(Z sample
N,Φ )−1(cid:17) µ0(du)(cid:12)(cid:12)(cid:12)(cid:12)

N,Φ )−1|2(cid:17)(cid:17)1/2

N,Φ )2|Z −1 − (Z sample

νΦ

νΦ

+

νΦ

2

2!!1/2

N,Φ )−1(cid:17) µ0(du)(cid:12)(cid:12)(cid:12)(cid:12)
2!!1/2

=: I + II.

The terms I and II can be bounded by the same arguments as the terms I and II in the proof of

Theorem 4.11, by noting that |Z −1 − (Z sample

N,Φ )−1|2 ≤ max(Z −4, (Z sample

N,Φ )−4)|Z − Z sample

N,Φ |2.

5 Numerical Examples

We consider the model inverse problem of determining the diﬀusion coeﬃcient of a divergence form
elliptic partial diﬀerential equation (PDE) from observation of a ﬁnite set of noisy continuous func-
tionals of the solution. This type of equation arises, for example, in the modelling of groundwater
ﬂow in a porous medium. We consider the one-dimensional model problem

d

(x; u)(cid:17) = 1 in (0, 1),
where the coeﬃcient κ depends on parameters u = {uj}K

dx(cid:16)κ(x; u)

−

dp
dx

p(1; u) = p(0; u) = 0,

(5.1)

j=1 ∈ [−1, 1]K through the linear expansion

κ(x; u) =

1
100

+

K

Xj=1

uj

200(K + 1)

sin(2πjx).

duj
As prior measure µ0 on [−1, 1]K , we use the uniform product measure µ0(du) = NK
2 . The
observations y are given by noisy point evaluations of the solution, yj = p(xj; u∗) + ηj with η ∼
N (0, I) and {xj}J
j=1 evenly spaced points in (0, 1). The truth u∗ was chosen as a random sample
from the prior, and the solution p was approximated by standard, piecewise linear, continuous
ﬁnite elements on a uniform grid with mesh size h = 1/1024.
In this setting, the forward map
G : [−1, 1]K → H 1
0 (D), deﬁned by G(u) = p, is an analytic function [9]. Since the observation
operator O is linear and bounded, Assumption 2.2 is hence satisﬁed for any s > K/2.
The emulators GN and ΦN are computed as described in section 3.2, with mean and covariance
kernel given by (3.6). In the Gaussian process prior (3.1), we choose m ≡ 0 and k = kν,1,1, a Mat`ern
kernel with variance σ2

k = 1, correlation length λ = 1 and smoothness parameter ν.

j=1

23

For a given approximation µy,N to µy, we will compute twice the Hellinger distance squared,

2dHell(µy, µy,N )2 =Z[−1,1]K


s dµy

dµ0

(u) −s dµy,N

dµ0

2

dµ0(u).

(u)


The integral over [−1, 1]K is approximated by a randomly shifted lattice rule with product weight
parameters γj = 1/j2 [27]. The generating vector for the rule used is available from Frances
Kuo’s website (http://web.maths.unsw.edu.au/∼fkuo/) as “lattice-39102-1024-1048576.3600”.
For the marginal and random approximations, the expected value over the Gaussian process is
approximated by Monte Carlo sampling, using the MATLAB command mvnrnd. The solution p to
(5.1) is approximated by standard, piecewise linear, continuous ﬁnite elements on a uniform grid
with mesh size h = 1/32.

For the design points U , we choose a uniform tensor grid. In [−1, 1]K , the uniform tensor grid
∗ points, for some N∗ ∈ N, has ﬁll distance hU = √K(N∗ − 1)−1. In Table 1,
L2(X) predicted by

consisting of N = N K
we give the convergence rates in N for supu∈X |f (u) − mf
Proposition 3.3.

N (u)|2 and kf − mf

Nk2

supu∈X |f (u) − mf
N (u)|2
3

K

1

2

❍

❍

4

❍

❍❍

❍

ν

kf − mf
K
1

Nk2
2

❍

❍❍

❍

❍

❍

ν

L2(X)

3

4

1

5

2

1

5

0.67

0.5

3.3

1

5

3

2

6

1.7

4.3

1.5

Table 1: Convergence rates in N predicted by Proposition 3.3 for uniform tensor grids.

mean )2 (left) and 2dHell(µy, µy,N,Φ

5.1 Mean-based approximations
In Figure 1, we show 2dHell(µy, µy,N,G
mean )2 (right), for a variety of choices
of K and ν, for J = 1. For each choice of the parameters K and ν, we have as a dotted line added
the least squares ﬁt of the form C1N −C2, for some C1, C2 > 0, and indicated the rate N −C2 in the
legend. By Corollary 4.3, we expect to see the faster convergence rates in the right panel of Table
1. For convenience, we have added these rates in parentheses in the legends in Figure 1. For µy,N,G
mean ,
we observe the rates in Table 1, or slightly faster. For µy,N,Φ
mean , we observe rates slightly faster than
predicted for ν = 1, and slightly slower than predicted for ν = 5. Finally, we remark that though
the convergence rates of the error are slightly slower for µy,N,Φ
mean , the actual errors are smaller for
µy,N,Φ
mean .

In Figure 2, we again show 2dHell(µy, µy,N,G

mean )2 (right), for a variety
of choices of K, with J = 15 and ν = 1. We again observe convergence rates slightly faster than
the rates predicted in the right panel of Table 1. As in Figure 1, we observe that the errors in
mean are smaller, though the rates of convergence are slightly faster for µy,N,G
µy,N,Φ
mean .

mean )2 (left) and 2dHell(µy, µy,N,Φ

24

marginal)2 (left) and 2dHell(µy, µy,N,Φ

5.2 Marginal approximations
In Figure 3, we show 2dHell(µy, µy,N,G
marginal)2 (right), for a variety of
choices of K and ν, for J = 1. For each choice of the parameters K and ν, we have again added the
least squares ﬁt of the form C1N −C2, and indicated the rate C2 in the legend. By Corollary 4.10,
we expect the error to be the sum of two contributions, one of which decays at the rate indicated
in the left panel of Table 1, and another which decays at the rate indicated by the right panel
of Table 1. For convenience, we have added these rates in parentheses in the legends in Figure
3.For µy,N,G
marginal, we observe the faster convergence rates in the right panel of Table 1, although a
closer inspection indicates that the convergence is slowing down as N increases. For µy,N,G
marginal, the
observed rates are somewhere between the two rates predicted by Table 1.

5.3 Random approximations

(dHell(µy, µy,N,G

sample)2) (left) and 2E

νΦ
N

ν G
N

sample)2) (right), for a
In Figure 4, we show 2E
variety of choices of K and ν, for J = 1. For each choice of the parameters K and ν, we have
again added the least squares ﬁt of the form C1N −C2, and indicated the rate C2 in the legend.
By Corollary 4.12, we expect the error to be the sum of two contributions, as for the marginal
approximations considered in the previous section, and the corresponding rates from Table 1 have
been added in parentheses in the legends. For µy,N,G
sample, we again observe the faster convergence rates
in the right panel of Table 1, but the convergence again seems to be slowing down as N increases.
For µy,N,G

marginal, the observed rates are very close to the slower rates in the left panel of Table 1.

(dHell(µy, µy,N,Φ

6 Conclusions and further work

Gaussian process emulators are frequently used as surrogate models. In this work, we analysed the
error that is introduced in the Bayesian posterior distribution when a Gaussian process emulator
is used to approximate the forward model, either in terms of the parameter-to-observation map or
the negative log-likelihood. We showed that the error in the posterior distribution, measured in
the Hellinger distance, can be bounded in terms of the error in the emulator, measured in a norm
dependent on the approximation considered.

An issue that requires further consideration is the eﬃcient emulation of vector-valued functions.
A simple solution, employed in this work, is to emulate each entry independently. In many applica-
tions, however, it is natural to assume that the entries are correlated, and a better emulator could
be constructed by including this correlation in the emulator. Furthermore, there are still a lot of
open questions about how to do this optimally [5]. Another interesting question is the optimal
choice of hyper-parameters λ and σ2
k in the Gaussian process emulator. And ﬁnally the questions
of scaling the Gaussian process methodology to both high dimensional input and output spaces
remain open.

References

[1] R. Adler, The Geometry of Random Fields, John Wiley, 1981.

[2] N. Aronszajn, Theory of Reproducing Kernels, Transactions of the American Mathematical

Society, 68 (1950), pp. 337–404.

25

d
e
r
a
u
q
s
 
e
c
n
a

t
s
d

i

 
r
e
g
n

i
l
l

 

e
H
e
c
w
T

i

10-2

10-4

10-6

10-8

10-10

10-12

10-14

10-16

101

K=2,  ν=1
N-2.6 (2)
K=2,  ν=5
N-6.2 (6)
K=3,  ν=1
N-2.4 (1.7)
K=3,  ν=5
N-4.5 (4.3)

102

N

103

104

d
e
r
a
u
q
s
 
e
c
n
a

t
s
d

i

 
r
e
g
n

i
l
l

 

e
H
e
c
w
T

i

10-2

10-4

10-6

10-8

10-10

10-12

10-14

10-16

101

K=2,  ν=1
N-2.5 (2)
K=2,  ν=5
N-5.4 (6)
K=3,  ν=1
N-2 (1.7)
K=3,  ν=5
N-3.8 (4.3)

102

N

103

104

Figure 1: 2dHell(µy, µy,N,G
ν, for J = 1.

mean )2 (left) and 2dHell(µy, µy,N,Φ

mean )2 (right), for a variety of choices of K and

100

10-2

10-4

10-6

10-8

10-10

10-12

K=1,  ν=1
N-4.1 (3)
K=2,  ν=1
N-2.7 (2)
K=3,  ν=1
N-2.3 (1.7)
K=4,  ν=1
N-2.3 (1.5)

d
e
r
a
u
q
s
 
e
c
n
a
t
s
d
 
r
e
g
n

i

i
l
l

 

e
H
e
c
w
T

i

100

10-2

10-4

10-6

10-8

10-10

10-12

K=1,  ν=1
N-4 (3)
K=2,  ν=1
N-2.7 (2)
K=3,  ν=1
N-2.1 (1.7)
K=4,  ν=1
N-1.9 (1.5)

d
e
r
a
u
q
s
 
e
c
n
a
t
s
d
 
r
e
g
n

i

i
l
l

 

e
H
e
c
w
T

i

10-14

100

101

102
N

103

104

10-14

100

101

102
N

103

104

Figure 2: 2dHell(µy, µy,N,G
ν = 1, for J = 15.

mean )2 (left) and 2dHell(µy, µy,N,Φ

mean )2 (right), for a variety of choices of K and

26

d
e
r
a
u
q
s
 

e
c
n
a
t
s
d
 
r
e
g
n

i

i
l
l

 

e
H
e
c
w
T

i

10-3

10-4

10-5

10-6

10-7

10-8

10-9

10-10

10-11

10-12

10-13

101

K=2,  ν=1
N-2.6 (2+1)
K=2,  ν=5
N-6.2 (6+5)
K=3,  ν=1
N-2.2 (1.7+0.67)
K=3,  ν=5
N-4.6 (4.3+3.3)

102

N

103

104

d
e
r
a
u
q
s
 

e
c
n
a
t
s
d
 
r
e
g
n

i

i
l
l

 

e
H
e
c
w
T

i

10-3

10-4

10-5

10-6

10-7

10-8

10-9

10-10

10-11

10-12

10-13

101

K=2,  ν=1
N-1.8 (2+1)
K=2,  ν=5
N-4.9 (6+5)
K=3,  ν=1
N1.1 (1.7+0.67)
K=3,  ν=5
N-3.2 (4.3+3.3)

102

N

103

104

Figure 3: 2dHell(µy, µy,N,G
and ν, for J = 1.

marginal)2 (left) and 2dHell(µy, µy,N,Φ

marginal)2 (right), for a variety of choices of K

d
e
r
a
u
q
s
 
e
c
n
a
t
s
d
 
r
e
g
n

i

i
l
l

e
H
 
e
c
w

i

t
 
f
o

 

l

e
u
a
v
 

d
e

t
c
e
p
x
E

10-2

10-3

10-4

10-5

10-6

10-7

10-8

10-9

10-10

10-11

101

K=2,  ν=1
N-2.3 (2+1)
K=2,  ν=5
N-6.1 (6+5)
K=3,  ν=1
N-1.7 (1.7+0.67)
K=3,  ν=5
N-4.4 (4.3+3.3)

102

N

103

104

d
e
r
a
u
q
s
 
e
c
n
a
t
s
d
 
r
e
g
n

i

i
l
l

e
H
 
e
c
w

i

t
 
f
o

 

l

e
u
a
v
 

d
e

t
c
e
p
x
E

10-2

10-3

10-4

10-5

10-6

10-7

10-8

10-9

10-10

10-11

101

K=2,  ν=1
N-1.1 (2+1)
K=2,  ν=5
N-4.9 (6+5)
K=3,  ν=1
N-0.76 (1.7+0.67)
K=3,  ν=5
N-3.3 (4.3+3.3)

102

N

103

104

Figure 4: 2E
of K and ν, for J = 1.

ν G
N

(dHell(µy, µy,N,G

sample)2) (left) and 2E

(dHell(µy, µy,N,Φ

sample)2) (right), for a variety of choices

νΦ
N

27

[3] S. Arridge, J. Kaipio, V. Kolehmainen, M. Schweiger, E. Somersalo, T. Tar-
vainen, and M. Vauhkonen, Approximation Errors and Model Reduction with an Applica-
tion in Optical Diﬀusion Tomography, Inverse Problems, 22 (2006), p. 175.

[4] I. Babuˇska, F. Nobile, and R. Tempone, A stochastic collocation method for elliptic

partial diﬀerential equations with random input data, SIAM review, 52 (2010), pp. 317–355.

[5] I. Bilionis, N. Zabaras, B. A. Konomi, and G. Lin, Multi-output separable gaussian
process: Towards an eﬃcient, fully bayesian paradigm for uncertainty quantiﬁcation, Journal
of Computational Physics, 241 (2013), pp. 212–239.

[6] N. Bliznyuk, D. Ruppert, C. Shoemaker, R. Regis, S. Wild, and P. Mugunthan,
Bayesian calibration and uncertainty analysis for computationally expensive models using op-
timization and radial basis function approximation, Journal of Computational and Graphical
Statistics, 17 (2008).

[7] V. I. Bogachev, Gaussian measures, vol. 62 of Mathematical Surveys and Monographs,

American Mathematical Society, 1998.

[8] T. Bui-Thanh, K. Willcox, and O. Ghattas, Model reduction for large-scale systems with
high-dimensional parametric input space, SIAM Journal on Scientiﬁc Computing, 30 (2008),
pp. 3270–3288.

[9] A. Cohen, R. Devore, and C. Schwab, Analytic regularity and polynomial approximation

of parametric and stochastic elliptic pde’s, Analysis and Applications, 9 (2011), pp. 11–47.

[10] P. R. Conrad, Y. M. Marzouk, N. S. Pillai, and A. Smith, Asymptotically exact
mcmc algorithms via local approximations of computationally intensive models, arXiv preprint
arXiv:1402.1694, (2014).

[11] P. G. Constantine, Active subspaces: Emerging ideas for dimension reduction in parameter

studies, vol. 2, SIAM, 2015.

[12] P. G. Constantine, E. Dow, and Q. Wang, Active subspace methods in theory and
practice: Applications to kriging surfaces, SIAM Journal on Scientiﬁc Computing, 36 (2014),
pp. A1500–A1524.

[13] S. L. Cotter, G. O. Roberts, A. M. Stuart, and D. White, MCMC methods for func-
tions: modifying old algorithms to make them faster, Statistical Science, 28 (2013), pp. 424–446.

[14] M. Dashti and A.M.Stuart, The bayesian approach to inverse problems, in Handbook of

Uncertainty Quantiﬁcation, R. Ghanem, D. Higdon, and H. Owhadi, eds., Springer.

[15] M. Girolami and B. Calderhead, Riemann manifold Langevin and Hamiltonian Monte
Carlo methods, Journal of the Royal Statistical Society: Series B (Statistical Methodology),
73 (2011), pp. 123–214.

[16] M. Hansen and C. Schwab, Sparse adaptive approximation of high dimensional parametric

initial value problems, Vietnam Journal of Mathematics, 41 (2013), pp. 181–215.

28

[17] W. Hastings, Monte-Carlo sampling methods using Markov chains and their applications,

Biometrika, 57 (1970), pp. 97–109.

[18] J. P. Kaipio and E. Somersalo, Statistical and Computational Inverse Problems, vol. 160,

Springer, 2005.

[19] M. C. Kennedy and A. O’Hagan, Bayesian calibration of computer models, Journal of the

Royal Statistical Society: Series B (Statistical Methodology), 63 (2001), pp. 425–464.

[20] Y. Marzouk and D. Xiu, A Stochastic Collocation Approach to Bayesian Inference in In-

verse Problems, Communications in Computational Physics, 6 (2009), pp. 826–847.

[21] Y. M. Marzouk, H. N. Najm, and L. A. Rahn, Stochastic Spectral Methods for Eﬃcient
Bayesian Solution of Inverse Problems, Journal of Computational Physics, 224 (2007), pp. 560–
586.

[22] B. Mat´ern, Spatial variation, vol. 36, Springer Science & Business Media, 2013.

[23] J. Mercer, Functions of positive and negative type, and their connection with the theory
of integral equations, Philosophical transactions of the royal society of London, series A, 209
(1909), pp. 415–446.

[24] N. Metropolis, A. Rosenbluth, M. Rosenbluth, A. Teller, and E. Teller, Equa-
tion of state calculations by fast computing machines, The J. of Chemical Physics, 21 (1953),
p. 1087.

[25] F. Narcowich, J. Ward, and H. Wendland, Sobolev bounds on functions with scattered
zeros, with applications to radial basis function surface ﬁtting, Mathematics of Computation,
74 (2005), pp. 743–763.

[26] F. J. Narcowich, J. D. Ward, and H. Wendland, Sobolev error estimates and a bernstein
inequality for scattered data interpolation via radial basis functions, Constructive Approxima-
tion, 24 (2006), pp. 175–186.

[27] H. Niederreiter, Random Number Generation and quasi-Monte Carlo methods, SIAM, 1994.

[28] A. O’Hagan, Bayesian analysis of computer code outputs: a tutorial, Reliability Engineering

& System Safety, 91 (2006), pp. 1290–1300.

[29] G. D. Prato and J. Zabczyk., Stochastic equations in inﬁnite dimensions, vol. 44 of En-

cyclopedia Math. Appl., Cambridge University Press, Cambridge, 1992.

[30] C. E. Rasmussen and C. K. Williams, Gaussian processes for machine learning, MIT

Press, 2006.

[31] P. Rebeschini and R. Van Handel, Can local particle ﬁlters beat the curse of dimension-

ality?, The Annals of Applied Probability, 25 (2015), pp. 2809–2866.

[32] C. Robert and G. Casella, Monte Carlo Statistical Methods, Springer, 1999.

[33] J. Sacks, W. J. Welch, T. J. Mitchell, and H. P. Wynn, Design and analysis of

computer experiments, Statistical science, (1989), pp. 409–423.

29

[34] M. Scheuerer, R. Schaback, and M. Schlather, Interpolation of spatial data–A stochas-
tic or a deterministic problem?, European Journal of Applied Mathematics, 24 (2013), pp. 601–
629.

[35] C. Schillings and C. Schwab, Sparsity in bayesian inversion of parametric operator equa-

tions, Inverse Problems, 30 (2014), p. 065007.

[36] M. L. Stein, Interpolation of Spatial Data: Some Theory for Kriging, Springer, 1999.

[37] A. M. Stuart, Inverse problems, vol. 19 of Acta Numerica, Cambridge University Press,

2010, pp. 451–559.

[38] W. Walter, Ordinary Diﬀerential Equations, vol. 182 of Graduate Texts in Mathematics,

Springer, 1998.

[39] H. Wendland, Scattered Data Approximation, vol. 17, Cambridge University Press, 2004.

[40] D. Xiu and G. E. Karniadakis, Modeling uncertainty in ﬂow simulations via generalized

polynomial chaos, Journal of computational physics, 187 (2003), pp. 137–167.

30

