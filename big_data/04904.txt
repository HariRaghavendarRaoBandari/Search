6
1
0
2

 
r
a

 

M
5
1

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
4
0
9
4
0

.

3
0
6
1
:
v
i
X
r
a

Turing learning: a metric-free approach to

inferring behavior and its application to swarms

Wei Li∗1, Melvin Gauci†2 and Roderich Groß‡1

1Department of Automatic Control and Systems Engineering,

The University of Sheﬃeld, Sheﬃeld, UK

2Wyss Institute for Biologically Inspired Engineering,

Harvard University, Boston, MA

Abstract

We propose Turing Learning, a novel system identiﬁcation method for
inferring behavior. Turing Learning simultaneously optimizes models and
classiﬁers. The classiﬁers are provided with data samples from both an
agent and models under observation, and are rewarded for discriminat-
ing between them. Conversely, the models are rewarded for ‘tricking’ the
classiﬁers into categorizing them as the agent. Unlike other methods for
system identiﬁcation, Turing Learning does not require predeﬁned met-
rics to quantify the diﬀerence between the agent and models. We present
two case studies with swarms of simulated robots that show that Tur-
ing Learning outperforms a metric-based system identiﬁcation method in
terms of model accuracy. The classiﬁers perform well collectively and
could be used to detect abnormal behavior in the swarm. Moreover, we
show that Turing Learning also successfully infers the behavior of physical
robot swarms. The results show that collective behaviors can be directly
inferred from motion trajectories of a single agent in the swarm, which
may have signiﬁcant implications for the study of animal collectives.

Keywords: system identiﬁcation; Turing test; swarm robotics; coevolu-
tion; machine learning

1

Introduction

System identiﬁcation is a process of modeling natural or artiﬁcial systems through
observed data. It has drawn a large interest among researchers for decades [Ljung,

All authors contributed equally to this work.
∗wei.li11@sheﬃeld.ac.uk
†mgauci@seas.harvard.edu
‡r.gross@sheﬃeld.ac.uk (corresponding author)

1

2010, Billings, 2013]. One application of system identiﬁcation is the reverse
engineering of agent behavior (biological organisms or artiﬁcial agents). For
example, evolutionary computation has proven to be a powerful method to
automate the generation of models, especially for behaviors that are hard to
formulate [Bongard and Lipson, 2005, 2007].

A limitation of current system identiﬁcation methods is that they rely on
predeﬁned metrics, such as the square error, to measure the diﬀerence between
the output of the models and that of the system under investigation. Model
optimization then proceeds by minimizing the measured diﬀerences. However,
for complex systems, deﬁning a metric can be non-trivial and case-dependent.
It may require prior information about the systems. Moreover, an unsuitable
metric may not distinguish well between good and bad models, or even bias the
identiﬁcation process. This paper overcomes these problems by introducing a
system identiﬁcation method that does not rely on predeﬁned metrics.

A promising application of such a metric-free method is the identiﬁcation
of collective behaviors, which are emergent behaviors that arise from the inter-
actions of numerous simple agents [Camazine et al., 2001]. Inferring collective
behaviors is particularly challenging, as the agents not only interact with the
environment but also with each other. Typically their motion appears stochas-
tic and is hard to predict [Helbing and Johansson, 2011]. For instance, given
a swarm of simulated ﬁsh, one would have to evaluate how close its behavior
is to that of a real ﬁsh swarm, or how close the individual behavior of a sim-
ulated ﬁsh is to that of a real ﬁsh. Characterizing the behavior at the level of
the swarm (that is, an emergent behavior) is diﬃcult [Harvey et al., 2015]. It
may require domain-speciﬁc knowledge and not discriminate among alternative
individual rules that exhibit similar collective dynamics [Weitz et al., 2012].
Characterizing the behavior at the level of agents is also diﬃcult, as even the
same individual ﬁsh in the swarm is likely to exhibit a fundamentally diﬀerent
trajectory every time it is being looked at.

In this paper, we propose Turing Learning, a novel system identiﬁcation
method that allows a machine to infer the behavior of an agent in an autonomous
manner. The method optimizes two populations simultaneously: one of models,
and the other of classiﬁers. Each classiﬁer is provided with a data sample, which
originates from an observation of either the agent or a model. The classiﬁer
outputs a Boolean value indicating where the sample is believed to come from.
The classiﬁer receives a reward if and only if it makes the correct judgment. The
reward thus depends solely on the classiﬁer’s ability to discriminate between the
agent and models. Conversely, the reward for the models depends solely on their
ability to ‘trick’ the classiﬁers into categorizing them as the agent.

Turing Learning does not rely on predeﬁned metrics for measuring the simi-
larity of behavior between models and agent; rather, the metrics (classiﬁers) are
produced automatically in the learning process. The method is inspired by the
Turing test [Turing, 1950, Saygin et al., 2000, Harnad, 2000], which machines
can pass if behaving indistinguishably from humans. Similarly, the models could
pass the tests by the classiﬁers if behaving indistinguishably from the agent. We
hence call our method Turing Learning.

2

In the following, we examine the ability of Turing Learning to infer the be-
havior of swarms of agents. The agents are the robots of existing swarm robotic
systems. This allows us to compare the obtained models to the ground truth.
We present two case studies on canonical problems in swarm robotics: self-
organized aggregation [Gauci et al., 2014b], and object clustering [Gauci et al.,
2014c]. The corresponding behaviors are reactive; in other words, each agent
maps its inputs (sensor readings) directly onto the outputs (actions). The prob-
lem of inferring this mapping is challenging, as the inputs are unknown. Instead,
all relevant information has to be extracted from the agents’ data samples (mo-
tion trajectories). A replica is mixed into a group of these agents. The replica
executes behavioral rules deﬁned by the model, and also produces data samples
(motion trajectories). We show that by observing the motion trajectories (but
not knowing the sensory values that produced these), Turing Learning auto-
matically infers the behavioral rules underpinning the collective behaviors.

We originally presented the basic idea of Turing Learning, along with pre-
liminary simulations, in [Li et al., 2013, 2014]. This paper extends our prior
work as follows:

• It presents an algorithmic description of Turing Learning;

• It shows that Turing Learning outperforms a metric-based system identi-

ﬁcation method in terms of model accuracy;

• It demonstrates, to the best of our knowledge for the ﬁrst time, that
system identiﬁcation can infer the behavior of swarms of physical robots;

• It examines in detail the usefulness of the classiﬁers;

• It examines through simulation how parameters of the agents’ morphology
(their ﬁeld of view) and brain (controller) can be simultaneously inferred.

This paper is organized as follows. Section 2 discusses related work. Sec-
tion 3 describes Turing Learning and the general methodology of the two case
studies. Section 4 investigates the ability of Turing Learning to infer behav-
iors of swarms of simulated robots. Section 5 presents a real-world validation
of Turing Learning with a swarm of physical robots. Section 6 concludes the
paper.

2 Related Work

Turing Learning is a system identiﬁcation method that simultaneously opti-
mizes a population of models and a population of classiﬁers. The objective
for the models is to be indistinguishable from the system under investigation.
The objective for the classiﬁers is to distinguish between the models and the
system. The idea of Turing Learning was ﬁrst proposed by [Li et al., 2013],
who presented a coevolutionary approach for inferring the behavioral rules of
a single agent. The agent moved in a simulated, one-dimensional environment.

3

Classiﬁers were rewarded for distinguishing between the models and agent. In
addition, they were able to control the stimulus that inﬂuenced the behavior
of the agent. This allowed the classiﬁers to interact with the agent during the
learning process. Turing Learning was subsequently investigated with swarms
of simulated robots [Li et al., 2014].

Goodfellow et al. [2014] proposed generative adversarial nets (GANs). GANs,
while independently invented, are essentially based on the same idea as Turing
Learning. The authors use GANs to train models for generating counterfeit
images that resemble real images, for example, from the Toronto Face Database
(for further impressive examples, see [Radford et al., 2016]). They simultane-
ously optimize a generative model (producing counterfeit images) and a dis-
criminative model that estimates the probability of an image to be real. The
optimization is done using a stochastic gradient descent method.

In a work reported in [Herbert-Read et al., 2015], humans were asked to
discriminate between the collective motion of real and simulated ﬁsh. The
authors report that the humans could do so even though the data from the model
is consistent with the real data according to predeﬁned metrics. Their results
“highlight a limitation of ﬁtting detailed models to real-world data”. They
argue that “observational tests [...] could be used to cross-validate model” (see
also [Harel, 2005]). This is in line with Turing Learning. Our method, however,
also automatically generates the models as well as the classiﬁers, and thus does
not require human observers.

While Turing Learning can in principle be used with any optimization al-
gorithm, our implementation relies on coevolutionary algorithms. Metric-based
coevolutionary algorithms have already proven eﬀective for system identiﬁca-
tion [Bongard and Lipson, 2004a,b, 2005, 2007, Koos et al., 2009, Mirmomeni
and Punch, 2011, Le Ly and Lipson, 2014]. A range of work has been performed
on simulated agents. In [Bongard and Lipson, 2004b], the authors proposed the
estimation-exploration algorithm, a nonlinear system identiﬁcation method to
coevolve inputs and models in a way that minimizes the number of inputs to be
tested on the system. In each generation, the input (test) that leads, in simu-
lation, to the highest disagreement between the models’ predicted output was
carried out on the real system. The quality of models was evaluated through
quantitatively comparing the output of the real system and the models’ predic-
tion. The method was applied to evolve morphological parameters of a simu-
lated quadrupedal robot after it had undergone ‘physical’ damage. In a later
work [Bongard and Lipson, 2004a], the authors reported that “in many cases
the simulated robot would exhibit wildly diﬀerent behaviors even when it very
closely approximated the damaged ‘physical’ robot. This result is not surpris-
ing due to the fact that the robot is a highly coupled, non-linear system: thus
similar initial conditions [...] are expected to rapidly diverge in behavior over
time”. The authors addressed this problem by using a more reﬁned comparison
metric reported in [Bongard and Lipson, 2004a]. In [Koos et al., 2009], an al-
gorithm which also coevolves models and inputs (tests) was presented to model
a simulated quadrotor and improve the control quality. The tests were selected
based on multiobjective performances (e.g., disagreement ability of models as

4

in [Bongard and Lipson, 2004b] and control quality of a given task). Models
were then reﬁned through comparing their predicted trajectories (corresponding
to the selected tests) with those of the real system. In these works, predeﬁned
metrics were critical for evaluating the performance of models. Moreover, the
algorithms are not applicable to the scenarios we consider here, as there are no
exogenous inputs in the swarm systems under observation (the same would be
the case for a swarm of biological agents).

Some studies also investigated the implementation of evolution directly in
physical environments, on either a single robot [Floreano and Mondada, 1996,
Zykov et al., 2004, Bongard et al., 2006, Koos et al., 2013, Cully et al., 2015]
or multiple robots [Watson et al., 2002, O’Dowd et al., 2014, Heinerman et al.,
2015].
In [Bongard et al., 2006], a four-legged robot was built to study how
it can infer its own morphology through a process of continuous self-modeling.
The robot ran a coevolutionary algorithm on-board. One population evolved
models for the robot’s morphology, while the other evolved actions (inputs) to
be conducted on the robot for gauging the quality of these models. Note that
this approach required knowledge of the robot’s sensor data. O’Dowd et al.
[2014] presented a distributed approach to coevolve on-board simulators and
controllers for a swarm of ten robots to perform foraging behavior. Each robot
had its own simulator which modeled the environment. The evolution of each
robot’s simulator was driven by comparing the real-world foraging eﬃciency of
its nearby neighbors each executing the best controller generated by their own
simulators. Each robot had a population of controllers, which evolved according
to the robot’s on-board simulator. The best controller was chosen for performing
real-world foraging. This physical/embodied evolution helped reduce the reality
gap between the simulated and physical environments [Jakobi et al., 1995]. In
all these approaches, the model optimization is based on predeﬁned metrics
(explicit or implicit).

The use of replicas can be found in ethological studies in which researchers
use robots that interact with animals to study their behaviors [Vaughan et al.,
2000, Halloy et al., 2007, Faria et al., 2010, Halloy et al., 2013, Schmickl et al.,
2013]. Robots can be created and systematically controlled in such a way
that they are accepted as conspeciﬁcs or heterospeciﬁcs by the animals in the
group [Krause et al., 2011]. For example, in [Faria et al., 2010], a replica ﬁsh,
which resembled sticklebacks in appearance, was created to investigate two types
of interaction: recruitment and leadership. In [Halloy et al., 2007], autonomous
robots which executed a derived model were mixed into a group of cockroaches
to modulate their decision-making of selecting a shelter. The robots behaved
in a similar way to the cockroaches. Although the robots’ appearance was dif-
ferent to that of the cockroaches, the robots released a speciﬁc odor that the
cockroaches could perceive them as conspeciﬁcs.
In these works, the models
were manually derived and the robots were only used for model validation. We
believe that this robot-animal interaction framework could be enhanced through
Turing Learning, which autonomously infers the collective behavior.

5

3 Methodology

In this section, we present the Turing Learning method and show how it can be
applied to two case studies in swarm robotics.

3.1 Turing Learning

Turing Learning is a method that generates models of behavior. The agent un-
der “investigation” could be natural or artiﬁcial. In either case, Turing Learning
needs data samples originating from the behavior. For example, if the behavior
of interest was to shoal like ﬁsh, data samples could be trajectory data from
ﬁsh.
If the behavior was to paint in a particular style (e.g., Cubism), data
samples could be photos of paintings with this style. In addition, data samples
would need to be produced by the models. The models could take the form of
computer programs, or be physical, such as robots.

Simultaneously to model generation, Turing Learning generates classiﬁers.
Given a data sample, the classiﬁers judge where it comes from. Does it originate
from a real ﬁsh or a model? Does it originate from a Cubist painter or a model?
This setup is akin of a Turing test; hence the name Turing Learning.

The models and classiﬁers are competing. The models are rewarded for mis-
leading the classiﬁers, whereas the classiﬁers are rewarded for making correct
judgements. Turing Learning thus optimizes models for producing indistin-
guishable behavior. This is in contrast to other system identiﬁcation methods,
which optimize models for producing as similar as possible behavior. The Turing
test inspired setup allows for model generation irrespective of whether suitable
similarity metrics are known.

In principle, the models can take any form. The model must however be
expressive enough to produce data samples that—from an observer’s perspec-
tive—are indistinguishable from those of the agent. If the classiﬁers have access
to only a subset of the agent’s outputs (e.g., its motion trajectories), any agent
characteristic not inﬂuencing this output can not be learned.

The classiﬁers can be any algorithm that takes a sequence of data as input
In principle, more complex outputs could be

and produces a binary output.
considered, such as ones providing conﬁdence levels.

Algorithm 1 provides a description of Turing Learning. We assume a popula-
tion of M models and a population of N classiﬁers. After an initialization stage,
Turing Learning proceeds in an iterative manner until a termination criterion
is met.

In each iteration cycle, data samples are obtained from observations of both
the agent and models. The classiﬁers may inﬂuence the sampling process. This
would enable a classiﬁer to choose the conditions under which the behavior is
observed [Li et al., 2013].
In this case, independent data samples should be
generated for each classiﬁer. In the case studies considered here, the classiﬁers
do not inﬂuence the sampling process. Therefore, the same set of data samples
is provided to all classiﬁers of an iteration cycle. For simplicity, we assume that

6

3:
4:
5:
6:
7:
8:

9:
10:
11:
12:
13:

Algorithm 1 Turing Learning
1: procedure Turing learning
2:

initialize population of M models and population of N classiﬁers
while termination criterion not met do
for all classiﬁers i ∈ {1, 2, . . . , N} do

obtain data samples (agent)
for each sample, obtain and store output of classiﬁer i
for all models j ∈ {1, 2, . . . , M} do
obtain data samples (model j)
for each sample, obtain and store output of classiﬁer i

end for

end for
reward models (rm) for misleading classiﬁers (classiﬁer outputs)
reward classiﬁers (rc) for making correct judgements (classiﬁer out-

puts)

improve model and classiﬁer populations based on rm and rc

14:
15:
16: end procedure

end while

each of the N classiﬁers is provided with K data samples for the agent and with
one data sample for every model.

A model’s reward is determined by its ability of misleading classiﬁers to
judge it as the agent. Let mij = 1 if classiﬁer i (wrongly) classiﬁed the data
sample of model j, and mij = 0 otherwise. The reward of model j is then given
by:

N(cid:88)

i=1

rm(j) =

1
N

mij.

(1)

A classiﬁer’s reward is determined by how well it judges data samples from

both the agent and models. By default, the reward of classiﬁer i is given by:

rc(i) =

1
2

(speciﬁcityi + sensitivityi).

(2)

The speciﬁcity of a classiﬁer denotes its true negative rate: the percentage of
agent data samples that were correctly identiﬁed as such (i.e., the tests were
negative). Formally,

speciﬁcityi =

1
K

aik,

(3)

K(cid:88)

k=1

where, aik = 1 if classiﬁer i (correctly) classiﬁed the kth data sample for the
agent, and aik = 0 otherwise.

The sensitivity of a classiﬁer denotes its true positive rate: the percentage
of model data samples that were correctly identiﬁed as such (i.e., the tests were

7

Figure 1: An e-puck robot ﬁtted with a black ‘skirt’ and a top marker for motion
tracking.

positive). Formally,

M(cid:88)

j=1

1
M

(1 − mij).

sensitivityi =

(4)

Using the reward values, rm and rc, the model and classiﬁer populations are
improved. In principle, any population-based optimization method can be used.

3.2 Case Studies

We examine the ability of Turing Learning to infer the behavioral rules re-
ported for two canonical problems in swarm robotics [Blum and Groß, 2015,
Bayındır, 2016]: self-organized aggregation [Gauci et al., 2014b], and object
clustering [Gauci et al., 2014c].

3.2.1 Agents

The agents move in a two-dimensional, continuous space. They are diﬀerential-
wheeled robots. The speed of each wheel can be independently set to [−1, 1],
where −1 and 1 correspond to the wheel rotating backwards and forwards,
respectively, with maximum speed. Fig. 1 shows the agent platform, the e-
puck [Mondada et al., 2009], which is used in the experiments.

Each agent is equipped with a line-of-sight sensor that detects the type of
item in front of it. We assume that there are n types (e.g., background, other
agent, object [Gauci et al., 2014b,c]). The state of the sensor is denoted by
I ∈ {0, 1, . . . , n − 1}.
The swarm behaviors investigated in this paper use a reactive control ar-
chitecture [Brooks, 1991]. Each agent maps the input (I) onto the outputs,
that is, a pair of predeﬁned speeds for the left and right wheels, (v(cid:96)I , vrI ),
v(cid:96)I , vrI ∈ [−1, 1]. Given n sensor states, the mapping can be represented using

8

initial conﬁguration

after 60 s

after 180 s

after 300 s

Figure 2: Snapshots of the aggregation behavior of 50 agents in simulation.

2n system parameters, which we denote as:

p = (v(cid:96)0, vr0, v(cid:96)1, vr1,··· , v(cid:96)(n−1), vr(n−1)).

(5)

Using p, any reactive behavior for above agent can be expressed.

In the
following, we consider two example behaviors in detail. We investigate the
ability of Turing Learning to infer these behaviors as well as 1000 randomly-
generated reactive behaviors.

Aggregation.

In this behavior, the sensor is binary, that is, n = 2.

It
gives a reading of I = 1 if there is an agent in the line of sight, and I = 0
otherwise. The environment is free of obstacles. The objective for the agents is
to aggregate into a single compact cluster as fast as possible. Further details,
including a validation with 40 physical e-puck robots, are reported in [Gauci
et al., 2014b].

The aggregation controller was found by performing a grid search over the
space of possible controllers [Gauci et al., 2014b]. The controller exhibiting the
highest performance was:

p = (−0.7,−1.0, 1.0,−1.0) .

(6)

When I = 0, an agent moves backwards along a clockwise circular trajectory
(v(cid:96)0 = −0.7 and vr0 = −1.0). When I = 1, an agent rotates clockwise on the
spot with maximum angular speed (v(cid:96)1 = 1.0 and vr1 = −1.0). Note that
rather counterintuitively, an agent never moves forward, regardless of I. With
this controller, an agent provably aggregates with another agent or a quasi-static
cluster of agents [Gauci et al., 2014b]. Fig. 2 shows snapshots from a simulation
trial with 50 agents.

Object Clustering. This behavior uses n = 3 sensor states: I = 0 if the
sensor is pointing at the background (e.g., the wall of the environment, if the
latter is bounded), I = 1 if the sensor is pointing at an object, and I = 2 if
it is pointing at another agent. The objective of the agents is to arrange the
objects into a single compact cluster as fast as possible. Details of this behavior,
including a validation using 5 physical e-puck robots and 20 cylindrical objects,
are presented in [Gauci et al., 2014c].

9

initial conﬁguration

after 20 s

after 40 s

after 60 s

Figure 3: Snapshots of the object clustering behavior in simulation. There are
5 agents (blue) and 10 objects (green).

The controller’s parameters, found using an evolutionary algorithm [Gauci

et al., 2014c], are:

p = (0.5, 1.0, 1.0, 0.5, 0.1, 0.5) .

(7)

When I = 0 and I = 2, the agent moves forward along an anti-clockwise
circular trajectory, but with diﬀerent linear and angular speeds. When I = 1,
it moves forward along a clockwise circular trajectory. Fig. 3 shows snapshots
from a simulation trial with 5 agents and 10 objects.

3.2.2 Models

The models are executed on replicas of the agents described in Section 3.2.1. In
other words, the replica has the same diﬀerential drive and line-of-sight sensor
as the agents.1 We assume that the agents are unable to store information
during run time. Thus, Turing Learning has to infer the input-output mapping,
represented by the parameters in Eq (5). In simulation, the search space for the
algorithm is unbounded; hence, the replica can move with arbitrary speed.

The explicit representation makes it possible to objectively measure the qual-
ity of the obtained models in the post-evaluation analysis. Note that the classi-
ﬁers, which determine the rewards of the models, have no knowledge about the
representation or the inputs the models and agents receive.

By default, we consider the situation that some replicas are mixed into a
group of agents. The case of studying groups of agents and groups of replicas
in isolation is, however, also investigated (see Section 4.9).

3.2.3 Classiﬁers

The classiﬁers need to discriminate between the agents and models. We use the
term individual to refer to either the agent or a replica executing a model.

A data sample comes from the motion trajectory of an individual observed
for the duration of a trial. We assume that it is possible to track both the

1In Section 4.7, we show that this assumption can be relaxed by also inferring some aspect

of the agent’s morphology.

10

individual’s position and orientation. The sample comprises the linear speed
(s) and angular speed (ω).2

The classiﬁer is represented as an Elman neural network [Elman, 1990]. The
network has i = 2 inputs (s and ω), h = 5 hidden neurons and one output
neuron. Each neuron of the hidden and output layers has a bias. The network
thus has a total of (i + 1)h + h2 + (h + 1) = 46 parameters, which all assume
values in R. The activation function used in the hidden and the output neurons
is the logistic sigmoid function, which has the range (0, 1) and is deﬁned as:

sig x =

1

1 + e−x

∀x ∈ R.

(8)

The ﬁnal value of the output neuron is used to make the judgment: model, if
its value is less than 0.5, and agent otherwise. The network’s memory (hidden
neurons) is reset after each judgment.

3.2.4 Optimization Algorithm

The optimization of models and classiﬁers is realized using an evolutionary
algorithm. We use a (µ + λ) evolution strategy with self-adaptive mutation
strengths [Eiben and Smith, 2003] to optimize either population. The optimiza-
tion algorithm can be thought of consisting of two sub-algorithms: one for the
model population, and another for the classiﬁer population. The sub-algorithms
do not interact with each other, except for the reward calculation described in
Section 3.1. The implementation of the evolutionary algorithm is detailed in [Li
et al., 2013].

For the remainder of this paper, we adopt terminology used in evolutionary
computing, and refer to the reward of solutions as their ﬁtness and to iteration
cycles as generations.

3.2.5 Termination Criterion

The algorithm stops after running for a ﬁxed number of iterations.

4 Simulation Experiments

In this section, we present the simulation experiments for the two case studies,
including simulation platform, simulation setups and the results obtained.

4.1 Simulation Platform

We use the open-source Enki library [Magnenat et al., 2011], which models the
kinematics and dynamics of rigid objects, and handles collisions. Enki has a
built-in 2-D model of the e-puck. The robot is represented as a disk of diameter

2We deﬁne the linear speed to be positive when the angle between the individual’s orien-

tation and its direction of motion is smaller than π/2 rad, and negative otherwise.

11

7.0 cm and mass 150 g. The inter-wheel distance is 5.1 cm. The speed of each
wheel can be set independently. Enki induces noise on each wheel speed by
multiplying the set value by a number in the range (0.95, 1.05) chosen randomly
with uniform distribution. The maximum speed of the e-puck is 12.8 cm/s,
forwards or backwards. The line-of-sight sensor is simulated by casting a ray
from the e-puck’s front and checking the ﬁrst item with which it intersects (if
any). The range of this sensor is unlimited in simulation.

In the object clustering case study, we model objects as disks of diameter
10 cm with mass 35 g and a coeﬃcient of static friction with the ground of 0.58,
which makes it movable by a single e-puck.

The robot’s control cycle is updated every 0.1 s, and the physics is updated

every 0.01 s.

4.2 Simulation Setups

In all simulations, we used an unbounded environment. For the aggregation case
study, we used groups of 11 individuals—10 agents and 1 replica that executes a
model. The initial positions of individuals were generated randomly in a square
region of sides 331.66 cm, following a uniform distribution (average area per
individual = 10000 cm2). For the object clustering case study, we used groups
of 5 individuals—4 agents and 1 replica that executes a model—and 10 cylin-
drical objects. The initial positions of individuals and objects were generated
randomly in a square region of sides 100 cm, following a uniform distribution
(average area per object = 1000 cm2). In both case studies, individual starting
orientations were chosen randomly in [−π, π) with uniform distribution.
We performed 30 runs of Turing Learning for each case study. Each run
lasted 1000 generations. The model and classiﬁer populations each consisted of
100 solutions (µ = 50, λ = 50). In each trial, classiﬁers observed individuals for
10 s at 0.1 s intervals (100 data points).

4.3 Analysis of Inferred Models

In order to objectively measure the quality of the models obtained through Tur-
ing Learning, we deﬁne two metrics. Given a candidate model (candidate con-

troller) x and the agent (original controller) p, where x ∈ R2n and p ∈ [−1, 1]2n,
we deﬁne the absolute error (AE) in a particular parameter i ∈ {1, 2, . . . , 2n}
as:
(9)

AEi = |xi − pi|.

We deﬁne the mean absolute error (MAE) over all parameters as:

AEi.

(10)

2n(cid:88)

i=1

MAE =

1
2n

12

(a) Aggregation

(b) Object Clustering

Figure 4: The model parameters Turing Learning inferred from swarms of sim-
ulated agents performing (a) aggregation and (b) object clustering. Each box
corresponds to the models with the highest subjective ﬁtness in the 1000th
generation of 30 runs. The dotted black lines correspond to the values of the
parameters that the system is expected to learn (i.e., those of the agent).

Fig. 4 shows a box plot3 of the parameters of the inferred models with the
highest ﬁtness value in the ﬁnal generation. The ﬁtness of the models depends on
the solutions (classiﬁers) from the competing population, and is hence referred
to as the subjective ﬁtness. It can be seen that Turing Learning identiﬁed the
parameters for both behaviors with good accuracy (dotted black lines represent
the ground truth, that is, the parameters of the observed swarming agents).
In the case of aggregation, the means (standard deviations) of the AEs in the
parameters were (from left to right in Fig. 4(a)): 0.01 (0.01), 0.01 (0.01), 0.07
(0.07), and 0.06 (0.04). In the case of object clustering, these values were: 0.03
(0.03), 0.04 (0.03), 0.02 (0.02), 0.03 (0.03), 0.08 (0.13), and 0.08 (0.09).

We also investigated the evolutionary dynamics. Fig. 5 shows how the model
parameters converged over generations.
In the aggregation case study (see
Fig. 5(a)), the parameters corresponding to I = 0 were learned ﬁrst. After
around 50 generations, both v(cid:96)0 and vr0 closely approximated their true values
(−0.7 and −1.0). For I = 1, it took about 200 generations for both v(cid:96)1 and vr1 to
converge. A likely reason for this eﬀect is that an agent spends a larger propor-
tion of its time seeing nothing (I = 0) than other agents (I = 1)—simulations
revealed these percentages to be 91.2% and 8.8%, respectively (mean values
across 100 trials).

In the object clustering case study (see Fig. 5(b)), the parameters corre-
sponding to I = 0 and I = 1 were learned faster than the parameters corre-

3The box plots presented here are all as follows. The line inside the box represents the
median of the data. The edges of the box represent the lower and the upper quartiles of the
data, whereas the whiskers represent the lowest and the highest data points that are within
1.5 times the range from the lower and the upper quartiles, respectively. Circles represent
outliers.

13

-1.5-1.0-0.50.00.51.01.5v‘0vr0v‘1vr1modelparametervalueofmodelparameters-1.5-1.0-0.50.00.51.01.5v‘0vr0v‘1vr1v‘2vr2modelparametervalueofmodelparameters(a) Aggregation

(b) Object Clustering

Figure 5: Evolutionary dynamics of model parameters for the (a) aggregation
and (b) object clustering case studies. Curves represent median parameter
values of the models with the highest subjective ﬁtness across 30 runs of Turing
Learning. Dotted black lines indicate the ground truth.

sponding to I = 2. After about 200 generations, v(cid:96)0, vr0, v(cid:96)1 and vr1 started
to converge; however it took about 400 generations for v(cid:96)2 and vr2 to approx-
imate their true values. Note that an agent spends the highest proportion of
its time seeing nothing (I = 0), followed by objects (I = 1) and other agents
(I = 2)—simulations revealed these proportions to be 53.2%, 34.2% and 12.6%,
respectively (mean values across 100 trials).

Although the inferred models approximate the agents well in terms of pa-
rameters, it is not uncommon in swarm systems that small changes in individual
behavior can lead to vastly diﬀerent emergent behaviors, especially when using
large numbers of agents [Levi and Kernbach, 2010]. For this reason, we evalu-
ated the quality of the emergent behaviors that the models give rise to. In the
case of aggregation, one measure of the emergent behavior is the dispersion of
the swarm after some elapsed time as deﬁned in [Gauci et al., 2014b]4. For each
of the 30 models with the highest subjective ﬁtness in the ﬁnal generation, we
performed 30 trials with 50 replicas executing the model. For comparison, we
also performed 30 trials using the agent (see Eq. (6)). The set of initial conﬁgu-
rations was the same for the replicas and agents. Fig. 6(a) shows the dispersion
of agents and replicas after 400 s. All models led to aggregation. We performed
a statistical test5 on the ﬁnal dispersion of the individuals between the agents
and replicas for each model. There was no statistically signiﬁcant diﬀerence in
26 out of 30 cases (30 out of 30 cases with Bonferroni correction).

In the case of object clustering, we use the dispersion of the objects after

4The measure of dispersion is based on the robots’/objects’ distances from their centroid.
For a formal deﬁnition, see Eq. (5) of [Gauci et al., 2014b], Eq. (2) of [Gauci et al., 2014c]
and [Graham and Sloane, 1990].

5Throughout this paper, the statistical test used is a two-sided Mann-Whitney test with a

5% signiﬁcance level.

14

12004006008001000-1.5-1-0.500.511.5generationvalueofmodelparametersv‘0vr0v‘1vr112004006008001000-1.5-1-0.500.511.5generationvalueofmodelparametersv‘0vr0v‘1vr1v‘2vr2(a) Aggregation

(b) Object Clustering

Figure 6: (a) Dispersion of 50 simulated agents (red box) or replicas (blue
boxes), executing one of the 30 inferred models in the aggregation case study.
(b) Dispersion of 50 objects when using a swarm of 25 simulated agents (red box)
or replicas (blue boxes), executing one of the 30 inferred models in the object
clustering case study. In both (a) and (b), boxes show the distributions obtained
after 400 s over 30 trials. The models are from the 1000th generation. The
dotted black lines indicate the minimum dispersion that 50 individuals/objects
can possible achieve [Graham and Sloane, 1990]. See Section 4.3 for details.

400 s as a measure of the emergent behavior. We performed 30 trials with 25
individuals and 50 objects for the agent and each model. The results are shown
in Fig. 6(b). In a statistical test on the ﬁnal dispersion of objects by the agent
or any of the models (replicas), there was no statistically signiﬁcant diﬀerence
in 24 out of 30 cases (26 out of 30 cases with Bonferroni correction).

4.4 Comparison with a Metric-Based System Identiﬁca-

tion Method

In order to compare Turing Learning against a metric-based approach, we used
an evolutionary algorithm with a single population of models. The algorithm
was identical to the model optimization sub-algorithm in Turing Learning except
for the ﬁtness calculation. In each generation, the reciprocal of the total square
error between the model’s and the agents’ linear and angular speed sequences
in a trial was used as the model’s ﬁtness. The square error of a model is deﬁned
as follows:

em =

m − s(t)
(s(t)

i )2 + (ω(t)

m − ω(t)

,

(11)

na(cid:88)

T(cid:88)

(cid:110)

i=1

t=1

i )2(cid:111)

where s(t)
m and s(t)
are the linear speed of the model and agent i, respectively,
i
at time step t; ω(t)
are the angular speed of the model and agent i,
respectively, at time step t; na is the number of agents in the group; T is the
total number of time steps in a trial.

m and ω(t)

i

15

400500600700800051015202530indexofcontrollerdispersion400500600700800051015202530indexofcontrollerdispersion(a) Aggregation

(b) Object Clustering

Figure 7: The model parameters a metric-based evolutionary method inferred
from swarms of simulated agents performing (a) aggregation and (b) object
clustering. Each box corresponds to the models with the highest ﬁtness in the
1000th generation of 30 runs. The dotted black lines correspond to the values
of the parameters that the system is expected to learn (i.e., those of the agent).

Each evolution run lasted 1000 generations. The simulation setup and num-
ber of ﬁtness evaluations for the models were kept the same as in Turing Learn-
ing. Fig. 7a shows the parameter distribution of the evolved models with highest
ﬁtness in the last generation over 30 runs. The distributions of the evolved pa-
rameters corresponding to I = 0 and I = 1 are similar. This phenomenon can
be explained as follows.
In the identiﬁcation problem that we consider, the
method has no knowledge of the input, that is, whether the agent perceives
another agent (I = 1) or not (I = 0). The metric-based algorithm seems to
construct controllers that do not respond diﬀerently to either input, but work
as good as it gets on average, that is, for the particular distribution of inputs, 0
and 1. For the left wheel speed both parameters are approximately −0.54. This
is almost identical to the weighted mean (−0.7∗ 0.912 + 1.0∗ 0.088 = −0.5504),
which takes into account that parameter v(cid:96)0 = −0.7 is observed around 91.2%
of the time, whereas parameter v(cid:96)1 = 1 is observed around 8.8% of the time
(see also Section 4.3). The parameters related to I = 1 evolved well as the
agent’s parameters are identical regardless of the input (vr0 = vr1 = −1.0).
Similar results have been found when inferring the object clustering behavior
(see Fig. 7b).

By comparing Figs. 4 and 7, one can see that Turing Learning outperforms
the metric-based evolutionary algorithm in terms of model accuracy in both
case studies. As it was argued before, due to the unpredictable interactions
in swarms the traditional metric-based method is not suited for inferring the
Instead, Turing Learning automatically synthesizes its own metric
behavior.
based on the data observed.

16

-1.5-1.0-0.50.00.51.01.5v‘0vr0v‘1vr1modelparametervalueofmodelparameters-1.5-1.0-0.50.00.51.01.5v‘0vr0v‘1vr1v‘2vr2modelparametervalueofmodelparametersFigure 8: The average decision accuracy of the best classiﬁers and classiﬁer
groups over generations (nonlinear scale) in 30 runs of Turing Learning. The
error bars show standard deviations. See text for details.

4.5 Analysis of Generated Classiﬁers

The primary outcome of the Turing Learning method (and of any system identi-
ﬁcation method) is the model, which has been discussed in the previous section.
However, the generated classiﬁers can also be considered as a useful byproduct.
For instance they could be used to detect abnormal agents in a swarm. We will
now analyze the performance of the classiﬁers. For the remainder of this paper,
we consider only the aggregation case study.

To assess the performance of the classiﬁers, we measure the percentage
of correct judgments over agents and a wide range of models. The mod-
els are uniformly distributed across the entire parameter space of the agents:
[−1, 1]4. To keep the analysis of classiﬁers within a reasonable computation
time, we discretize this space using 11 settings per parameter, to obtain: X =
{−1.0,−0.8, ..., 0.8, 1.0}4. This discretized space is a grid consisting of |X| =
114 = 14641 points (i.e., models). The classiﬁer’s performance is computed as
follows. The model is executed by a replica mixed into a group of 10 agents (as
in the Turing Learning runs). 10 trials are performed using a set of initial con-
ﬁgurations common to all classiﬁers. The motion data is fed to each classiﬁer,
which makes 10 judgments per individual. If the classiﬁer consistently judges
the individual as a model (i.e., not an agent) in 10 out of 10 trials, it outputs
a “model” decision. Otherwise, it outputs “agent”. This conservative approach
was used to minimize the risk of false positive detection of abnormal behavior.

4.5.1 Using a Single Classiﬁer

Fig. 8 shows the average decision accuracy of the classiﬁer with the highest
subjective ﬁtness in 30 runs of Turing Learning (best classiﬁer (subjective)).

17

1610601006000.40.50.60.70.80.91generationdecisionaccuracybestclassiﬁer(subjective)classiﬁergroup(subjective)bestclassiﬁer(archive)classiﬁergroup(archive)bestclassiﬁer(objective)The accuracy combines the percentage of correct judgments about models (50%
weight) with the percentage of correct judgments about agents (50% weight),
analogous to the reward deﬁnition in Eq. (2). The accuracy of the classiﬁer
increases in the ﬁrst 5 generations, then drops and ﬂuctuates within range 62%–
80%.

An alternative strategy is to select the classiﬁer that achieves the highest
ﬁtness when evaluated on the whole historical tracking data (not just those of
the current generation). The decision accuracy of this classiﬁer is also shown in
Fig. 8 (best classiﬁer (archive)). The trend is similar to that of best classiﬁer
(subjective). The accuracy increases in the ﬁrst 10 generations, and then starts
decaying, dropping to around 65% by the 1000th generation. However, in the
earlier generations, the accuracy of the best classiﬁer (archive) is higher than
that of the best classiﬁer (subjective). For a comparison, we also plot the highest
decision accuracy that a single classiﬁer achieves for each generation (best clas-
siﬁer (objective)). Interestingly, the accuracy of the best classiﬁer (objective),
which is shown in Fig. 8 (black curve), increases almost monotonically, reaching
a level above 95%. Note that to select the best classiﬁer (objective), one needs
to perform additional trials (146410 in this case).

At ﬁrst sight, it seems counterintuitive that both the classiﬁer (subjective)
and the classiﬁer (archive) have a low decision accuracy. This phenomenon,
however, can be explained when considering the model population. We have
shown in the previous section (see especially Fig. 5(a)) that the models con-
verge rapidly at the beginning of the coevolutions. As a result, when classiﬁers
are evaluated in later generations, the trials are likely to include models very
similar to each other. Classiﬁers that become overspecialized to this small set of
models (the ones dominating the later generations) have a higher chance of be-
ing selected during the evolutionary process and in the post-evaluation. These
classiﬁers may however have a low performance when evaluated across the entire
model space.

4.5.2 Using a Classiﬁer Group

The results of the previous section have shown that using a single classiﬁer is
not a good solution; although there may be a good classiﬁer in each generation,
it may take signiﬁcant eﬀort (trials with agents) to ﬁnd it.

To address this problem, we propose the use of a classiﬁer group, that is, a
number of classiﬁers working in tandem to judge a given candidate. We choose
the best 10 classiﬁers6. This is either the set of classiﬁers with the highest
subjective ﬁtness in the current generation or the set of classiﬁers that achieve
the highest ﬁtness when evaluated on the whole historical data. If one or more
classiﬁers make a decision about the candidate as a model (i.e., not an agent),
the system outputs a “model” decision. Otherwise, it outputs “agent”.

The results of using a classiﬁer group are shown in Fig. 8 (green and magenta,
respectively). Both types of classiﬁer groups exhibit signiﬁcantly improved de-

6Note that the number of classiﬁers chosen to form the group is not necessarily optimal.

18

Figure 9: MAE (deﬁned in Eq. (10)) of the inferred models when using Turing
Learning with varying numbers of agents (excluding the replica). Red and blue
boxes show, respectively, the cases where all agents are observed, and one agent
is observed. Boxes show distributions for the models with the highest subjective
ﬁtness after 1000 generations, over 30 runs.

cision accuracy across all generations. After 1000 generations, each system has
a high accuracy of above 95%, on average, and performs thus similarly well to
best classiﬁer (objective).

4.6 Observing Only a Subset of Agents

So far, we have used motion data about all agents in the swarm when evaluating
classiﬁers. However, this may not always be feasible in practice. For instance,
given a video recording of a large and/or dense swarm, extracting motion data
about all agents may be infeasible or lead to highly inaccurate results. A more
practical solution might be to only track a subset of agents (e.g., by equipping
them with markers).

We now compare the case where all agents are observed with the other
extreme, where only one agent is observed. We study how these two cases
compare as the swarm size increases. We conducted 30 runs of Turing Learning
with each number of agents n ∈ {1, 5, 10, 50, 100}. There was always one replica
in the group. When observing only one agent, this was chosen randomly in each
trial. Note that the total number of trials in each run for the case of observing
all agents and one agent is identical. We measured the total square error of the
model with the highest subjective ﬁtness in the last (1000th) generation of each
run. The results are shown in Fig. 9.

There is no statistically signiﬁcant diﬀerence for any n. On the other hand,
as the swarm size increases, performance improves. For example, there is a
statistically signiﬁcant diﬀerence between n = 10 and n = 100, both when

19

0.00.10.20.30.40.5151050100numberofagentsMAEobservingallagentsobservingoneagent(a)

(b)

Figure 10: A diagram showing the angle of view of the agent’s sensor investigated
in Section 4.7.

observing all agents and one. These results suggest that the key factor in Turing
Learning is not the number of observed agents, so much as the richness of
information that comes from increasing inter-agent interactions, and is reﬂected
in each agent’s motion. This means that in practice, observing a single agent is
suﬃcient to infer accurate models, as long as the swarm size is suﬃciently large.
A similar phenomenon was observed in a diﬀerent scenario, where the goal
was to distinguish between diﬀerent ‘modes’ of a swarm (i.e., global behaviors)
through observing only a few individuals [Eldridge and Maciejewski, 2014].

4.7

Inferring Control and Morphology

In the previous sections, we assumed that we fully knew the agents’ morphology
(i.e., structure), and only their behavior (controller) was to be identiﬁed. We
now present a variation where one aspect of the morphology is also unknown.
The replica, in addition to the four controller parameters, takes a parameter
θ ∈ [0, 2π] rad, which determines the horizontal ﬁeld of view of its sensor, as
shown in Fig. 10 (the sensor is still binary). Note that in the previous sections
the agents’ line-of-sight sensors can be considered as sensors with a ﬁeld of view
of 0 rad.

The models now have ﬁve parameters. As before, we let Turing Learning
run in an unbounded search space (i.e., now, R5). However, as θ is necessarily
bounded, before a model was executed on a replica, the parameter correspond-
ing to θ was mapped to the range (0, 2π) using an appropriately-scaled logistic
sigmoid function. The controller parameters were directly passed to the replica.
In this setup, the classiﬁers observed the individuals for 100 s in each trial (pre-
liminary results indicated that this setup requires a longer observation time).

Fig. 11(a) shows the parameters of the subjectively best models in the last
(1000th) generations of 30 runs. The means (standard deviations) of the AEs in
each model parameter were: 0.02 (0.01), 0.02 (0.02), 0.05 (0.07), 0.06 (0.06), and
0.01 (0.01). All parameters including θ were still learned with high accuracy.

The case where the true value of θ is 0 rad is an edge case, because given
an arbitrarily small  > 0, the logistic sigmoid function maps an unbounded
domain of values onto (0, ). This makes it simpler for Turing Learning to infer
this parameter. For this reason, we also considered another scenario where the
agents’ angle of view is π/3 rad rather than 0 rad. The controller parameters for

20

I = 0(cid:1)/2(cid:1)/2no fragments ofagents within this sectorI = 1(cid:1)/2(cid:1)/2(a)

(b)

Figure 11: Turing Learning simultaneously inferring control and morphological
parameters (ﬁeld of view). The agents’ ﬁeld of view is (a) 0 rad and (b) π/3 rad.
Boxes show distributions for the models with the highest subjective ﬁtness in the
1000th generation over 30 runs. Dotted black lines indicate the ground truth.

achieving aggregation in this case are diﬀerent from those in Eq. (6). They were
found by re-running a grid search with the modiﬁed sensor. Fig. 11(b) shows
the results from 30 runs with this setup. The means (standard deviations) of the
AEs in each parameter were: 0.04 (0.04), 0.03 (0.03), 0.05 (0.06), 0.05 (0.05),
and 0.20 (0.19). The controller parameters were still learned with good accuracy.
The accuracy in the angle of view is noticeably lower, but still reasonable.

4.8

Inferring Other Behaviors

The aggregation controller that agents used in our case study was originally
synthesized by searching over the space [−1, 1]4, using a metric to assess the
swarm’s global performance [Gauci et al., 2014b]. Other points in this space
lead to diﬀerent global behaviors that can be ‘meaningful’ to a human observer
(e.g., circle formation [Gauci et al., 2014a]).

We now investigate whether Turing Learning can infer arbitrary controllers
in this space, irrespective of the global behaviors they lead to. We generated
1000 controllers randomly in [−1, 1]4, with uniform distribution. For each con-
troller we performed one run, and selected the subjectively best model in the
last (1000th) generation.

Fig. 12(a) shows a histogram of the MAE of the inferred models. The distri-
bution has a single mode close to zero, and decays rapidly for increasing values.
Over 89% of the 1000 cases have an error below 0.05. This suggests that the
accuracy of Turing Learning is not highly sensitive to the particular behavior
under investigation (i.e., most behaviors are learned equally well). Fig. 12(b)
shows the AEs of each model parameter. The means (standard deviations) of
the AEs in each parameter were: 0.01 (0.05), 0.02 (0.07), 0.07 (0.6), and 0.05
(0.20). We performed a statistical test on the AEs between the model parame-

21

-2-10123v‘0vr0v‘1vr1θmodelparametervalueofmodelparameters-2-10123v‘0vr0v‘1vr1θmodelparametervalueofmodelparameters(a)

(b)

Figure 12: Turing Learning inferring the models for 1000 randomly-generated
agent behaviors. For each behavior, one run of Turing Learning was performed
and the model with the highest subjective ﬁtness after 1000th is considered. (a)
Histogram of the models’ MAE (deﬁned in Eq. (10); 43 points that have an
MAE larger than 0.1 are not shown); and (b) AEs (deﬁned in Eq. (9)) for each
model parameter.

ters corresponding to I = 0 (v(cid:96)0 and vr0) and I = 1 (v(cid:96)1 and vr1). The AEs of
the inferred v(cid:96)0 and vr0 were signiﬁcantly lower than those of v(cid:96)1 and vr1. This
was likely due to the reason reported in Section 4.3; that is, an agent is likely to
spend more time seeing nothing (I = 0) than other agents (I = 1) in each trial.

4.9 Separating Replicas and Agents

In our two case studies, the replica was mixed into the group of agents. However,
in some collective behaviors, abnormal agent(s) inﬂuence the swarm [Bjerknes
and Winﬁeld, 2013]. For the same reason, the insertion of a replica that exhibits
diﬀerent behavior or is not recognized as conspeciﬁc may disrupt the behavior
of the swarm and hence the models obtained may be biased.
In this case,
an alternative method would be to isolate the inﬂuence of the replica(s). We
performed an additional simulation study where the agents and replicas were
never mixed.
Instead, each trial focused on either a group of agents, or of
replicas. All replicas in a trial executed the same model. The group size was
identical in both cases. The tracking data of the models and agents from each
sample were then fed into the classiﬁers for making judgments.

The distribution of the inferred model parameters is shown in Fig. 13. The
results show that Turing Learning can still identify the model parameters well.
There is no signiﬁcant diﬀerence between either approach in the case studies
considered in this paper. The method of separating replicas and agents is rec-

22

MAEfrequency0.000.020.040.060.080.1001002003004000.00.51.01.52.0v‘0vr0v‘1vr1modelparameterAEFigure 13: The model parameters inferred by a variant of Turing Learning
that observes swarms of aggregating agents and swarms of replicas in isolation,
thereby avoiding potential bias. Each box corresponds to the models with the
highest subjective ﬁtness in the 1000th generation of 30 simulation runs.

ommended if potential biases are suspected.

5 Physical Experiments

In this section, we present a real-world validation of Turing Learning. We ex-
plain how it can be used to identify the behavior of a swarm of real agents.
The agents and replica are represented by physical robots. We use the same
type of robot (e-puck) as in simulation. The agents execute the aggregation
behavior described in Section 3.2.1. The replicas execute the candidate models.
We use two replicas to speed up the identiﬁcation process, as will be explained
in Section 5.3.

5.1 Physical Platform

The physical setup, shown in Fig. 14, consists of an arena with robots (repre-
senting agents or replicas), a personal computer (PC) and an overhead camera.
The PC runs the Turing Learning algorithm. It communicates with the repli-
cas, providing them models to be executed, but does not exert any control
over the agents. The overhead camera supplies the PC with a video stream of
the swarm. The PC performs video processing to obtain motion data about
individual robots. We will now describe the physical platform in more detail.

23

-1.5-1.0-0.50.00.51.01.5v‘0vr0v‘1vr1modelparametervalueofmodelparametersFigure 14: Illustration of the general setup for inferring the behavior of physical
agents—e-puck robots (not to scale). The computer runs the Turing Learning
algorithm, which produces models and classiﬁers. The models are uploaded and
executed on the replica. The classiﬁers run on the computer. They are provided
with the agents’ and replica’s motion data, extracted from the video stream of
the overhead camera.

Figure 15: Schematic top-view of an e-puck, indicating the locations of its
motors, wheels, camera and infrared sensors. Note that the marker is pointing
towards the robot’s back.

5.1.1 Robot Arena

The robot arena was rectangular with sides 200 cm × 225 cm, and bounded by
walls 50 cm high. The ﬂoor had a light gray color, and the walls were painted
white.

24

ReplicaAgentsOverhead CameraComputervideo stream (robot positions & orientations)model updatesRobot Arenastart/stopsignal#0#1#2#3#4#5#6#7Left(cid:14)wheel7.0(cid:14)cmInfrared(cid:14)sensors(cid:14)Camera5.1.2 Robot Platform and Sensor Implementations

A schematic top view of the e-puck is shown in Fig. 15. We implemented the
line-of-sight sensor using the e-puck’s directional camera, located at its front.
For this purpose, we wrapped the robots in black ‘skirts’ (see Fig. 1) to make
them distinguishable against the light-colored arena. While in principle the
sensor could be implemented using one pixel, we used a column of pixels from
a sub-sampled image to compensate for misalignment in the camera’s vertical
orientation. The gray values from these pixels were used to distinguish robots
(I = 1) against the arena (I = 0). For more details about this sensor realization,
see [Gauci et al., 2014b].

We also used the e-puck’s infrared sensors, in two cases. Firstly, before
each trial, the robots dispersed themselves within the arena. In this case, they
used the infrared sensors to avoid both robots and walls, making the dispersion
process more eﬃcient. Secondly, we observed that using only the line-of-sight
sensor can lead to robots becoming stuck against the walls of the arena, hinder-
ing the identiﬁcation process. We therefore used the infrared sensors for wall
avoidance, but in such a way as to not aﬀect inter-robot interactions7. Details of
these two collision avoidance behaviors are provided in the online supplementary
material [Li et al., 2016].

5.1.3 Motion Capture

To facilitate motion data extraction, we ﬁtted robots with markers on their
tops, consisting of a colored isosceles triangle on a circular white background
(see Fig. 1). The triangle’s color allowed for distinction between robots; we
used blue triangles for all agents, and orange and purple triangles for the two
replicas. The triangle’s shape eased extraction of robots’ orientations.

The robots’ motion was captured using a camera mounted around 270 cm
above the arena ﬂoor. The camera’s frame rate was set to 10 fps. The video
stream was fed to the PC, which performed video processing to extract motion
data about individual robots (position and orientation). The video processing
software was written using OpenCV [Bradski and Kaehler, 2008].

5.2 Turing Learning with Physical Robots

Our objective is to infer the agent’s aggregation behavior. We do not wish to
infer the agent’s dispersion behavior, which is periodically executed to distribute
already-aggregated agents. To separate these two behaviors, the robots (agents
and replicas) and the system were implicitly synchronized. This was realized by
making each robot execute a ﬁxed behavioral loop of constant duration. The
PC also executed a ﬁxed behavioral loop, but the timing was determined by the
signals received from the replicas. Therefore, the PC was synchronized with the
swarm. The PC communicated with the replicas via Bluetooth. At the start

7To do so, the e-pucks determine whether a perceived object is a wall or another robot.

25

Figure 16: Flow diagram of the programs run by the PC and a replica in the
physical experiments. Dotted arrows represent communication between the two
units. See Section 5.2 for details. The PC does not exert any control over the
agents.

of a run, or after a human intervention (see Section 5.3), robots were initially
synchronized using an infrared signal from a remote control.

Fig. 16 shows a ﬂow diagram of the programs run by the PC and the replicas,

respectively. Dotted arrows indicate communication between the units.

The program running on the PC has the following states:
• P1: Wait for “Stop” Signal. The program is paused until “Stop” signals
are received from both replicas. These signals indicate that a trial has
ﬁnished.

• P2: Send Model Parameters. The PC sends new model parameters to the

buﬀer of each replica.

• P3: Wait for “Start” Signal. The program is paused until “Start” signals

are received from both replicas, indicating that a trial is starting.

• P4: Track Robots. The PC waits 1 s and then tracks the robots using
the overhead camera for 5 s. The tracking data contains the positions and
orientations of the agents and replicas.

• P5: Update Turing Learning Algorithm. The PC uses the motion data
from the trial observed in P4 to update the rewards (ﬁtness values) of

26

ReplicaPCR1: Send "Stop" Signal (1s)  R3: Receive Model Parameters (1s)R5: Execute Model (7s)R4: Send "Start" Signal (1s) P1: Wait for "Stop" Signal P2: Send Model Parameters P3: Wait for "Start" Signal       P5:  Update Turing       Learning Algorithm    P4: Track RobotsR2: Disperse (8s) the corresponding two models and all classiﬁers. Once all models in the
current iteration cycle (generation) have been evaluated, the PC also gen-
erates new model and classiﬁer populations. The reward calculation and
optimization algorithm are described in Sections 3.1 and 3.2.4, respec-
tively. The PC then goes back to P1.

The program running on the replicas has the following states:

• R1 : Send “Stop” Signal. After a trial stops, the replica informs the PC by
sending a “Stop” signal. The replica waits 1 s before proceeding with R2,
so that all robots remain synchronized. Waiting 1 s in other states serves
the same purpose.

• R2 : Disperse. The replica disperses in the environment, while avoiding

collisions with other robots and the walls. This behavior lasts 8 s.

• R3 : Receive Model Parameters. The replica reads new model parameters
from its buﬀer (sent earlier by the PC). It waits 1 s before proceeding
with R4.

• R4 : Send “Start” Signal. The replica sends a start signal to the PC
to inform it that a trial is about to start. The replica waits 1 s before
proceeding with R5.

• R5 : Execute Model. The replica moves within the swarm according to
its model. This behavior lasts 7 s (the tracking data corresponds to the
middle 5 s, see P4 ). The replica then goes back to R1.

The program running on the agents has the same structure as the replica
program. However, in the states analogous to R1, R3, and R4, they simply wait
1 s rather than communicate with the PC. In the state corresponding to R2,
they also execute the Disperse behavior. In the state corresponding to R5, they
execute the agent’s aggregation controller, rather than a model.

Each iteration (loop) of the program for the PC, replicas and agents lasts

18 s.

5.3 Experimental Setup

As in simulation, we used a population size of 100 for classiﬁers (µ = 50, λ =
50). However, the model population size was reduced from 100 to 20 (µ = 10,
λ = 10), to shorten the experimental time. We used 10 robots: 8 representing
agents executing the original aggregation controller (Eq. (6)), and 2 representing
replicas that executed models. This meant that in each trial, 2 models from the
population could be simultaneously evaluated; consequently, each generation
consisted of 20/2 = 10 trials.

The Turing Learning algorithm was implemented without any modiﬁcation
to the code used in simulation (except for model population size and observation
time in each trial). We still let the model parameters evolve unboundedly (i.e.,

27

in R4). However, as the speed of the physical robots is naturally bounded,
we applied the hyperbolic tangent function (tanh x) on each model parameter,
before sending a model to a replica. This bounded the parameters to (−1, 1)4,
with −1 and 1 representing the maximum backwards and forwards wheel speeds,
respectively.
The Turing Learning runs proceeded autonomously. In the following cases,

however, human intervention was made:

• The robots had been running continuously for 25 generations. All batteries

were replaced.

• Hardware failure occurred on a robot, for example because of a lost battery
connection or because the robot became stuck on the ﬂoor. Appropriate
action was taken for the aﬀected robot to restore its functionality.

• A replica lost its Bluetooth connection with the PC. The connection with

both replicas was restarted.

• A robot indicated a low battery status through its LED after running for

only a short time. That robot’s battery was changed.

After an intervention, the ongoing generation was restarted, to limit the

impact on the identiﬁcation process.

5.4 Results

We conducted 10 runs of Turing Learning using the physical system. Each run
lasted 100 generations, corresponding to 5 hours (excluding human intervention
time). Video recordings of all runs can be found in the online supplementary
materials [Li et al., 2016].

5.4.1 Analysis of Inferred Models

We will ﬁrst investigate the quality of the models obtained. To select the ‘best’
model from each run, we post-evaluated all models of the ﬁnal generation 5
times using all classiﬁers of that generation. The parameters of these models
are shown in Fig. 17. The means (standard deviations) of the AEs in each
parameter were: 0.08 (0.06), 0.01 (0.01), 0.05 (0.08), and 0.02 (0.04).

To investigate the eﬀects of real-world conditions on the identiﬁcation pro-
cess, we performed 10 simulated runs of Turing Learning with the same setup as
in the physical runs. Fig. 18 shows the evolutionary dynamics of the parameters
of the inferred models (with the highest subjective ﬁtness) in the physical and
simulated runs. The dynamics show good correspondence. However, the conver-
gence in the physical runs is somewhat less smooth than that in the simulated
ones (e.g., see spikes in v(cid:96)0 and v(cid:96)1). In each generation of every run (physical
and simulated), we computed the MAE of each model. We compared the error
of the model with the highest subjective ﬁtness with the average and lowest
errors. The results are shown in Fig. 19. For both the physical and simulated

28

Figure 17: The model parameters Turing Learning inferred from swarms of
physical robots performing aggregation. The models are those with the highest
subjective ﬁtness in the 100th generation of 10 runs. Dotted black lines indi-
cate the ground truth, that is, the values of the parameters that the system is
expected to learn.

(a) physical coevolutions

(b) simulated coevolutions

Figure 18: Evolutionary dynamics of model parameters in (a) 10 physical and
(b) 10 simulated runs of Turing Learning (in both cases, equivalent setups were
used). Curves represent median parameter values of the models with the highest
subjective ﬁtness across the 10 runs. Dotted black lines indicate the ground
truth.

runs, the subjectively best model (green) has an error in between the lowest
error (blue) and the average error (red) in the majority of generations.

As we argued before (Section 4.3), in swarm systems, good agreement be-
tween local behaviors (e.g., controller parameters) may not guarantee similar
global behaviors. For this reason, we performed 20 trials using 40 physical e-

29

-1.5-1.0-0.50.00.51.01.5v‘0vr0v‘1vr1modelparametervalueofmodelparameters120406080100-1.5-1-0.500.511.5generationvalueofmodelparametersv‘0vr0v‘1vr1120406080100-1.5-1-0.500.511.5generationvalueofmodelparametersv‘0vr0v‘1vr1(a) physical coevolutions

(b) simulated coevolutions

Figure 19: Evolutionary dynamics of MAE (deﬁned in Eq. (10)) for candidate
models in (a) 10 physical and (b) 10 simulated runs of Turing Learning. Curves
represent median values across 10 runs. The red curve represents the average
error of all models in a generation. The green and blue curves show, respectively,
the errors of the models with the highest subjective and the highest objective
ﬁtness in a generation.

pucks, lasting 10 minutes each: 10 trials with the original controller (Eq. (6)),
and 10 trials with a controller obtained from the physical runs. This latter
controller was constructed by taking the median values of the parameters over
the 10 runs, which are:

p = (−0.65,−0.99, 0.99,−0.99) .

The set of initial conﬁgurations of the robots was common to both controllers.
As it was not necessary to extract the orientation of the robots, a red circular
marker was attached to each robot so that its position can be extracted with
higher accuracy in the oﬄine analysis [Gauci et al., 2014b]. Fig. 20(a) shows
the proportion of robots in the largest cluster8 over time with the agent and
model controllers. Fig. 20(b) shows the dispersion (as deﬁned in Section 4.3) of
the robots over time with the two controllers. The aggregation dynamics of the
agents and and models show good correspondence. Fig. 21 shows a sequence of
snapshots from a trial with 40 e-pucks executing the inferred model controller.
A video accompanying this paper shows the Turing Learning identiﬁcation
process of the models (in a particular run) both in simulation and on the physical
system. Additionally, videos of all 20 post-evaluation trials with 40 e-pucks, are
available in [Li et al., 2016].

30

12040608010000.20.40.60.81generationMAEaveragesubjectiveobjective12040608010000.20.40.60.81generationMAEaveragesubjectiveobjective(a) Largest Cluster Dynamics

(b) Dispersion Dynamics

Figure 20: Average aggregation dynamics in 10 physical trials with 40 physi-
cal e-puck robots executing the original agent controller (red) and the model
controller (blue) inferred through observation of the physical system. In (a),
the vertical axis shows the proportion of robots in the largest cluster; in (b),
it shows the robots’ dispersion (see Section 4.3). Dotted lines in (a) and (b),
respectively, represent the maximum proportion and minimum dispersion that
40 robots can achieve.

initial conﬁguration

after 20 s

after 40 s

after 180 s

after 360 s

after 420 s

after 480 s

after 600 s

Figure 21: Example of collective behavior produced by a model that was in-
ferred by Turing Learning through the observation of swarms of physical e-puck
robots. A group of 40 physical e-puck robots, each executing the inferred model,
aggregates in a single spot.

31

110020030040050060000.20.40.60.81time(s)proportionoriginalcontrollerinferredcontroller11002003004005006000246810time(s)dispersion(×103)originalcontrollerinferredcontrollerFigure 22: The average decision accuracy of the best classiﬁers and classiﬁer
groups over generations (nonlinear scale) in 10 runs of Turing Learning with
swarms of physical robots. The error bars show standard deviations. See text
for details.

5.4.2 Analysis of Generated Classiﬁers

When post-evaluating the classiﬁers generated in the physical runs of Turing
Learning, we limited the number of candidate models to 100, in order to reduce
the physical experimentation time. Each candidate model was randomly cho-
sen with uniform distribution from [−1, 1]4. Fig. 22 shows the average decision
accuracy of the best classiﬁers and classiﬁer groups over the 10 runs. Similar to
the results in simulation, the classiﬁer groups obtained in the physical runs still
have a high decision accuracy. The classiﬁer group (archive) has a higher accu-
racy than that of the classiﬁer group (subjective) and best classiﬁer (objective).
However, in contrast to simulation, the decision accuracy of the best classiﬁer
(subjective) and best classiﬁer (archive) does not drop within 100 generations.
This could be due to the noise present in the physical runs, which may have
prevented the classiﬁers from getting over-specialized in the comparatively short
time provided (100 generations).

6 Conclusion

This paper presented a new system identiﬁcation method—Turing Learning—
that can autonomously infer agent behavior from observations. To our knowl-
edge, Turing Learning is the ﬁrst system identiﬁcation method that does not
rely on any predeﬁned metric to quantitatively gauge the diﬀerence between

8A cluster of robots is deﬁned as a maximal connected subgraph of the graph deﬁned by
the robots’ positions, where two robots are considered to be adjacent if another robot cannot
ﬁt between them [Gauci et al., 2014b].

32

12468104060801000.40.50.60.70.80.91generationdecisionaccuracybestclassiﬁer(subjective)classiﬁergroup(subjective)bestclassiﬁer(archive)classiﬁergroup(archive)bestclassiﬁer(objective)agents and learned models. This eliminates the need to choose a suitable metric
and the bias that such metric may have on the obtained solutions.

Through competitive and successive generation of models and classiﬁers, the
system successfully learned two swarm behaviors: self-organized aggregation
and object clustering. Both the model parameters, which were automatically
inferred, and emergent global behaviors closely matched those of the original
In addition, simulation results showed that Turing Learning
swarm system.
outperforms a metric-based system identiﬁcation method (which diﬀered by
using the least square metric instead of classiﬁers) in terms of the obtained
model accuracy. We also constructed a robust classiﬁer group that, given an
individual’s motion data, can tell whether the individual is an original agent or
not. Such a classiﬁer group could be eﬀective in detecting abnormal behavior,
for example, when faults occur in some members of the swarm. These classiﬁers
are automatically produced without the need to deﬁne a priori what constitutes
abnormal behavior.

The Turing Learning method was further validated using a physical system.
We applied it to automatically infer the aggregation behavior of an observed
swarm of e-puck robots. The behavior was learned successfully, and the results
obtained in the physical experiments showed good correspondence to those ob-
tained in simulation. This shows the robustness of our method with respect to
noise and uncertainties in the real world. To the best of our knowledge, this
is also the ﬁrst time that a system identiﬁcation method was used to infer the
behavior of a swarm of physical robots.

A scalability study showed that the interactions in a swarm can be char-
acterized by the eﬀects on a subset of agents. In other words, when learning
swarm behaviors especially with large number of agents, instead of considering
the motion of all the agents in the group, we could focus on a subset of agents.
This becomes critical when the available data about agents in the swarm is
limited. Our approach was proven to work even if using only the motion data
of a single agent and replica, as the data from this agent implicitly contained
enough information about the interactions in the swarm.

In the two case studies presented, the model was explicitly represented by a
set of parameters. The inferred parameters could thus be compared against the
ground truth, enabling us to objectively gauge the quality of inferred models in
the two case studies as well as for 1000 randomly sampled behaviors. Although
the search space for the models is relatively small, identifying the parameters
is challenging as the input values are unknown (consequently, a metric-based
evolutionary algorithm did not succeed in approximating the parameter values).
Parameter estimation plays an important part in the modeling of biological
systems, as the model structure is often assumed to be known [Gautrais et al.,
2012]. In principle, Turing Learning could also infer the structure of the agent’s
control system. The results of learning the agent’s angle of view showed that
our method may even learn the morphology of the swarming agents.

In the future, we intend to use Turing Learning to infer behaviors exhibited

in natural swarms, such as in shoals of ﬁsh or herds of land mammals.

33

Acknowledgements

The authors are grateful for the support received by Jianing Chen, especially in
relation to the physical implementation of Turing Learning.

References

L. Bayındır. A review of swarm robotics tasks. Neurocomputing, 172:292–321,

2016.

S. A. Billings. Nonlinear system identiﬁcation: NARMAX methods in the time,

frequency, and spatio-temporal domains. Wiley, Hoboken, NJ, USA, 2013.

J. Bjerknes and A. F. Winﬁeld. On fault tolerance and scalability of swarm
In A. Martinoli et al., editors, Distributed Auton. Robot.

robotic systems.
Syst., volume 83, pages 431–444. Springer, Berlin, Germany, 2013.

C. Blum and R. Groß. Swarm intelligence in optimization and robotics.

In
J. Kacprzyk and W. Pedrycz, editors, Springer Handbook of Computational
Intelligence, pages 1291–1309. Springer, Berlin, Germany, 2015.

J. Bongard and H. Lipson. Automated robot function recovery after unantic-
ipated failure or environmental change using a minimum of hardware trials.
In Proc. 2004 NASA/DoD Conf. Evolvable Hardware, pages 169–176, Piscat-
away, NJ, 2004a. IEEE.

J. Bongard and H. Lipson. Automated damage diagnosis and recovery for remote
robotics. In Proc. 2004 IEEE Int. Conf. Robot. Autom., pages 3545–3550,
Piscataway, NJ, 2004b. IEEE.

J. Bongard and H. Lipson. Nonlinear system identiﬁcation using coevolution of

models and tests. IEEE Trans. Evol. Comput., 9(4):361–384, 2005.

J. Bongard and H. Lipson. Automated reverse engineering of nonlinear dynam-

ical systems. PNAS, 104(24):9943–9948, 2007.

J. Bongard, V. Zykov, and H. Lipson. Resilient machines through continuous

self-modeling. Sci., 314(5802):1118–1121, 2006.

G. Bradski and A. Kaehler. Learning OpenCV: Computer vision with the

OpenCV library. O’Reilly Media, Sebastopol, CA, USA, 2008.

R. A. Brooks. Intelligence without representation. Artif. Int., 47(1–3):139–159,

1991.

S. Camazine et al. Self-Organization in Biological Systems. Princeton University

Press, Princeton, NJ, USA, 2001.

A. Cully, J. Clune, D. Tarapore, and J. Mouret. Robots that can adapt like

animals. Nature, 521(7553):503–507, 2015.

34

A. E. Eiben and J. E. Smith. Introduction to Evolutionary Computing. Springer,

Berlin, Germany, 2003.

B. D. Eldridge and A. A. Maciejewski. Limited bandwidth recognition of collec-
tive behaviors in bio-inspired swarms. In Proc. 2014 Int. Conf. Auton. Agents
and Multi-Agent Syst., pages 405–412, Richland, SC, 2014. IFAAMS.

J. L. Elman. Finding structure in time. Cognitive Sci., 14(2):179–211, 1990.

J. J. Faria et al. A novel method for investigating the collective behaviour of ﬁsh:

Introducing ‘Roboﬁsh’. Behav. Ecol. Sociobiology, 64(8):1211–1218, 2010.

D. Floreano and F. Mondada. Evolution of homing navigation in a real mobile

robot. IEEE Trans. Syst. Man, Cybern. B, Cybern., 26(3):396–407, 1996.

M. Gauci, J. Chen, T. Dodd, and R. Groß. Evolving aggregation behaviors in
multi-robot systems with binary sensors. In M. Ani Hsieh and G. Chirikjian,
editors, Distributed Auton. Robot. Syst., volume 104, pages 355–367. Springer,
Berlin, Germany, 2014a.

M. Gauci, J. Chen, W. Li, T. J. Dodd, and R. Groß. Self-organized aggregation

without computation. Int. J. Robot. Res., 33(8):1145–1161, 2014b.

M. Gauci, J. Chen, W. Li, T. J. Dodd, and R. Groß. Clustering objects with
In Proc. 2014 Int. Conf. Auton. Agents and

robots that do not compute.
Multi-Agent Syst., pages 421–428, Richland, SC, 2014c. IFAAMS.

J. Gautrais et al. Deciphering interactions in moving animal groups. PLoS

Comput. Biol., 8(9):e1002678, 2012.

I. Goodfellow et al. Generative adversarial nets. In Z. Ghahramani, M. Welling,
C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural
Information Processing Systems 27, pages 2672–2680. Curran Associates, Inc.,
2014.

R. L. Graham and N. J. A. Sloane. Penny-packing and two-dimensional codes.

Discrete Comput. Geom., 5(1):1–11, 1990.

J. Halloy, F. Mondada, S. Kernbach, and T. Schmickl. Towards bio-hybrid
systems made of social animals and robots. In N. F. Lepora, A. Mura, H. G.
Krapp, P. Verschure, and T. J. Prescott, editors, Biomimetic and Biohybrid
Systems, volume 8064, pages 384–386. Springer, Berlin, Germany, 2013.

J. Halloy et al. Social integration of robots into groups of cockroaches to control

self-organized choices. Sci., 318(5853):1155–1158, 2007.

D. Harel. A Turing-like test for biological modeling. Nat. Biotechnol., 23:

495–496, 2005.

S. Harnad. Minds, machines and Turing: The indistinguishability of indistin-

guishables. J. Logic Lang. Inform., 9(4):425–445, 2000.

35

J. Harvey, K. Merrick, and H. A. Abbass. Application of chaos measures to a

simpliﬁed boids ﬂocking model. Swarm Intell., 9(1):23–41, 2015.

J. Heinerman, M. Rango, and A. E. Eiben. Evolution, individual learning, and
social learning in a swarm of real robots. In Proc. 2015 Genet. Evol. Comput.
Conf., pages 177–183, New York, NY, 2015. ACM.

D. Helbing and A. Johansson. Pedestrian, crowd and evacuation dynamics.
In R. A. Meyers, editor, Extreme Environmental Events, pages 697–716.
Springer, New York, NY, 2011.

J. E. Herbert-Read, M. Romenskyy, and D. J. T. Sumpter. A Turing test for

collective motion. Biol. Lett., 11(12), 2015. 20150674.

N. Jakobi, P. Husbands, and I. Harvey. Noise and the reality gap: The use of
simulation in evolutionary robotics. In F. Mor´an, A. Moreno, J. Merelo, and
P. Chac´on, editors, Advances in Artiﬁcial Life, volume 929, pages 704–720.
Springer, Berlin, Germany, 1995.

S. Koos, J. Mouret, and S. Doncieux. Automatic system identiﬁcation based on
coevolution of models and tests. In Proc. 2009 IEEE Congr. Evol. Comput.,
pages 560–567, Piscataway, NJ, 2009. IEEE.

S. Koos, J. Mouret, and S. Doncieux. The transferability approach: Crossing
the reality gap in evolutionary robotics. IEEE Trans. Evol. Comput., 17(1):
122–145, 2013.

J. Krause, A. F. Winﬁeld, and J.-L. Deneubourg. Interactive robots in experi-

mental biology. Trends Ecol. Evol., 26(7):369–375, 2011.

D. Le Ly and H. Lipson. Optimal experiment design for coevolutionary active

learning. IEEE Trans. Evol. Comput., 18(3):394–404, 2014.

P. Levi and S. Kernbach. Symbiotic multi-robot organisms: Reliability, adapt-

ability, evolution. Springer, Berlin, Germany, 2010.

W. Li, M. Gauci, and R. Groß. A coevolutionary approach to learn animal
behavior through controlled interaction. In Proc. 2013 Genet. Evol. Comput.,
pages 223–230, New York, NY, 2013. ACM.

W. Li, M. Gauci, and R. Groß. Coevolutionary learning of swarm behaviors
without metrics. In Proc. 2014 Genet. Evol. Comput. Conf., pages 201–208,
New York, NY, 2014. ACM.

W. Li, M. Gauci, and R. Groß. Online supplementary material, March 2016.

URL http://naturalrobotics.group.shef.ac.uk/supp/2016-003.

L. Ljung. Perspectives on system identiﬁcation. Annu. Rev. Control, 34(1):

1–12, 2010.

36

S. Magnenat, M. Waibel, and A. Beyeler. Enki: The fast 2D robot simulator,

2011. URL http://home.gna.org/enki.

M. Mirmomeni and W. Punch. Co-evolving data driven models and test data
sets with the application to forecast chaotic time series. In Proc. 2011 IEEE
Congr. Evol. Comput., pages 14–20, Piscataway, NJ, 2011. IEEE.

F. Mondada et al. The e-puck, a robot designed for education in engineering.
In Proc. 9th Conf. Auton. Robot Syst. Competitions, volume 1, pages 59–65,
2009.

P. J. O’Dowd, M. Studley, and A. F. Winﬁeld. The distributed co-evolution
of an on-board simulator and controller for swarm robot behaviours. Evol.
Intell., 7(2):95–106, 2014.

A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning
with deep convolutional generative adversarial networks. In ICLR 2016, 2016.
In press; available online: http://arxiv.org/abs/1511.06434.

A. P. Saygin, I. Cicekli, and V. Akman. Turing test: 50 years later. Minds and

Machines, 10:463–518, 2000.

T. Schmickl et al. Assisi: Mixing animals with robots in a hybrid society. In
N. F. Lepora, A. Mura, H. G. Krapp, P. Verschure, and T. J. Prescott, editors,
Biomimetic and Biohybrid Systems, volume 8064, pages 441–443. Springer,
Berlin, Germany, 2013.

A. M. Turing. Computing machinery and intelligence. Mind, 59(236):433–460,

1950.

R. Vaughan, N. Sumpter, J. Henderson, A. Frost, and C. Stephen. Experiments

in automatic ﬂock control. Robot. Auton. Syst., 31(1):109–117, 2000.

R. A. Watson, S. G. Ficici, and J. B. Pollack. Embodied evolution: Distributing
an evolutionary algorithm in a population of robots. Robot. Auton. Syst., 39
(1):1–18, 2002.

S. Weitz et al. Modeling collective animal behavior with a cognitive perspective:

A methodological framework. PLoS ONE, 7(6):e38588, 2012.

V. Zykov, J. Bongard, and H. Lipson. Evolving dynamic gaits on a physical
In Proc. 2004 Genet. Evol. Comput. Conf., pages 4722–4728, New

robot.
York, NY, 2004. ACM.

37

