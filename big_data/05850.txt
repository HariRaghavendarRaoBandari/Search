SUBMITTED TO TRANSACTION ON INFORMATION THEORY

1

N-ary Error Correcting Coding Scheme

Joey Tianyi Zhou, Ivor W. Tsang, Shen-Shyang Ho and Klaus-Robert M¨uller,

6
1
0
2

 
r
a

 

M
8
1

 
 
]

G
L
.
s
c
[
 
 

1
v
0
5
8
5
0

.

3
0
6
1
:
v
i
X
r
a

Abstract—The coding matrix design plays a fundamental role
in the prediction performance of the error correcting output
codes (ECOC)-based multi-class task. In many-class classiﬁca-
tion problems, e.g., ﬁne-grained categorization, it is difﬁcult to
distinguish subtle between-class differences under existing coding
schemes due to a limited choices of coding values. In this paper,
we investigate whether one can relax existing binary and ternary
code design to N-ary code design to achieve better classiﬁcation
performance. In particular, we present a novel N-ary coding
scheme that decomposes the original multi-class problem into
simpler multi-class subproblems, which is similar to applying a
divide-and-conquer method. The two main advantages of such a
coding scheme are as follows: (i) the ability to construct more
discriminative codes and (ii) the ﬂexibility for the user to select
the best N for ECOC-based classiﬁcation. We show empirically
that the optimal N (based on classiﬁcation performance) lies
in [3, 10] with some trade-off in computational cost. Moreover,
we provide theoretical insights on the dependency of the gener-
alization error bound of an N-ary ECOC on the average base
classiﬁer generalization error and the minimum distance between
any two codes constructed. Extensive experimental results on
benchmark multi-class datasets show that the proposed coding
scheme achieves superior prediction performance over the state-
of-the-art coding methods.

Index Terms—Multi-class Classiﬁcation, Coding Scheme, Error

Correcting Output Codes

I. INTRODUCTION

Many real-world problems are multi-class in nature. To
handle multi-class problems, many approaches have been
proposed. One research direction focuses on solving multi-
class problems directly. These approaches include decision tree
based methods [3, 4, 9, 15, 23, 25, 29]. In particular, decision-
tree based algorithms label each leaf of the decision tree with
one of the NC classes, and internal nodes can be selected
to discriminate between these classes. The performance of
decision-tree based algorithms heavily depends on the internal
tree structure. Thus, these methods are usually vulnerable to
outliers. To achieve better generalization, [15, 29] propose to
learn the decision tree structure based on the large margin
criterion. However, these algorithms usually involve solving
sophisticated optimization problems and their training time
increases dramatically with the increase of the number of
classes. Contrary to these complicated methods, K-Nearest
Neighbour (KNN) [7] is a simple but effective and stable
approach to handle multi-class problems. However, KNN is
sensitive to noise features and can therefore suffer from the
curse-of-dimensionality. Meanwhile, Crammer et al. [8, 17]
propose a direct approach for learning multi-class support

J. T. Zhou is with the Institute of High Performance Computing, Singapore,
I. W. Tsang is with the University of Sydney, Australia,
S.-S. Ho is with Nanyang Technological University, Singapore,
K.-R. M¨uller is with the Department of Machine Learning, Berlin Institute
of Technology, Berlin, Germany, and also with the Department of Brain and
Cognitive Engineering, Korea University, Seoul, Korea.

vector machines (M-SVM) by deriving the generalized notion
of margins as well as separating hyperplanes.

is that

Another research direction focuses on the error correcting
output codes (ECOC) framework that decomposes a multi-
class problem into multiple binary problems so that one
reuses the well-studied binary classiﬁcation algorithms for
their simplicity and efﬁciency. Many ECOC approaches [10,
16, 20, 21, 24, 27, 32] have been proposed in recent years to
design a good coding matrix. They fall into two categories:
problem-independent and problem-dependent. The challenge
with problem-independent codings, such as random ECOC
[12],
they are not designed and optimized for a
particular dataset. In fact, there is little guarantee that the
created base codes are always discriminative for the multi-
class classiﬁcation task. Therefore,
they usually require a
large number of base classiﬁers generated by the pre-designed
coding matrix [1]. To overcome this weakness, problem-
dependent methods such as discriminant ECOC (DECOC) [22]
and node embedding ECOC (ECOCONE) [13] are proposed.
Recently, subspace approaches such as subspace ECOC [2]
and adaptive ECOC [33] are proposed to further improve
the ECOC classiﬁcation framework. Though all the above-
mentioned variations of the ECOC approach endeavor to
enhance the ECOC paradigm for classiﬁcation tasks, their
designs are conﬁned to binary {−1, 1} and ternary codes
{−1, 0, 1}. Such a code design constraint poses limitations
on the error correcting capability of ECOC that relies on
the minimum distance, ∆min(M ), between any distinct pair
of rows in the coding matrix M. A larger ∆min(M ) is
more likely to rectify the errors committed by individual base
classiﬁers [1].

However, in the more challenging real-world applications,
there exists multi-class problems where some of the classes are
very similar and difﬁcult to differentiate with each other. For
example, in the ﬁne-grained classiﬁcation [30] unlike basic-
level recognition, even humans might have difﬁculty with
some of the ﬁne-grained categorization. One major challenge
in ﬁne-grained image classiﬁcation is to distinguish subtle
between-class differences while each class often has large
within-class variation in the image level [28]. The existing
binary ECOC codes cannot handle this challenge due to
limited choices of coding values. It is highly possible that
some classes out of multi-class classiﬁcation problems are
assigned with same or similar codes. To address this issue,
we investigate whether one can extend the existing binary or
ternary coding scheme to an N-ary coding scheme to (i) allow
users the ﬂexibility to choose N to construct the codes in
order to (ii) improve the ECOC classiﬁcation performance for
a given dataset. The main contributions of this paper are as
follows.

• We propose a novel N-ary coding scheme that achieves

SUBMITTED TO TRANSACTION ON INFORMATION THEORY

2

that dense and sparse random ECOC approaches require only
10 log2(NC) and 15 log2(NC) base classiﬁers, respectively,
to achieve optimal results. However, a random ECOC coding
approach cannot guarantee that the created base codes are
always discriminative for the multi-class classiﬁcation task.
Therefore, it is possible that either some base classiﬁers that
are redundant for the prediction exist or badly designed base
classiﬁers are constructed.

a large expected distance between any pair of rows in
M at a reasonable N (> 3) for a multi-class problem
(see Section III). The main idea of our coding scheme
is to decompose the original multi-class problem into
a series of smaller multi-class subproblems instead of
binary classiﬁcation problems. Suppose that a metaclass
is a subset of classes. Also, suppose that all classes
are in a one large metaclass. So, in each level, there
is a classiﬁer to divide a metaclass into two smaller
metaclasses. This coding scheme is like a divide-and-
conquer method. The two main advantages of such a
coding scheme are as follows: (i) the ability to construct
more discriminative codes and (ii) the ﬂexibility for the
user to select the best N for ECOC-based classiﬁcation.
• We provide theoretical insights on the dependency of
the generalization error bound of a N-ary ECOC on
the average base classiﬁer generalization error and the
minimum distance between any two constructed codes
(see Section V). Furthermore, we conduct a series of
empirical analyses to verify the validity of the theorem
on the ECOC error bound (see section VI).

• We show empirically that

the optimal N (based on
classiﬁcation performance) lies in [3, 10] with a slight
trade-off in computational cost (see Section VI).

• We show empiricially the superiority of the proposed
coding scheme over the state-of-the-art coding methods
for multi-class prediction tasks on a set of benchmark
datasets (see Section VI).

To the best of our knowledge, there is no previous work that
attempts to extend and generalize the coding scheme to N-ary
codes with N > 3.

The rest of this paper is organized as follows. Section II
reviews the related work. Section III presents the generaliza-
tion from binary coding to N-ary coding. In Section IV,we
give the complexity analysis of N-ary coding and compare
it with other coding schemes with the SVM classiﬁer as a
showcase. Section V gives the error bound analysis of N-ary
coding. Finally, Section VI discusses our empirical studies and
Section VII concludes this work.

II. RELATED WORK

Many ECOC approaches [1, 2, 12, 22] have been proposed
to design a good coding matrix in recent years. Most of
them fall into the following two categories. The ﬁrst one
is problem-independent coding, such as OVO, OVA, random
dense ECOC, and random sparse ECOC [12]. However, the
coding matrix design is not optimized for the training dataset
or the instance labels. Therefore, these approaches usually
require a large number of base classiﬁers generated by the
pre-designed coding matrix. For example, the random dense
ECOC coding approach aims to construct the ECOC matrix
M ∈ {−1, 1}NC×NL where NC is the number of classes,
NL is the code length, and its elements are randomly cho-
sen as either 1 or -1 [11]. [1] extends this binary coding
scheme to ternary coding by using a coding matrix M ∈
{−1, 0, 1}NC×NL where the classes corresponding to 0 are not
considered in the learning process. Allwein et al. [1] suggest

To overcome this problem, some problem-dependent meth-
ods have been proposed. In particular, the coding matrix is
learned by taking the instances as well as labels into con-
sideration. For instance, discriminant ECOC (DECOC) [22]
embeds a binary decision tree into the ternary codes. Its key
idea is to ﬁnd the most discriminative hierarchical partition of
the classes which maximizes the quadratic mutual information
between the data subsets and the class labels created for
each subset. As a result, DECOC needs exactly NC − 1 base
classiﬁers which signiﬁcantly accelerate the testing process
without sacriﬁcing performance. However, this decision tree
based method has one major drawback: if the parent node
misclassiﬁes an instance, the mistake will be propagated to all
the subsequent child nodes.

To address this weakness, Escalera et al. [13] proposed to
optimize node embedding for ECOC, called ECOCONE. For
this approach, one initializes a problem-independent ECOC
matrix (usually OVA) and iteratively adds the base classiﬁers
that discriminate the most confusing pairs of classes into the
previous ECOC ensemble to improve performance. However,
ECOCONE suffers from three major limitations. Firstly, its
performance relies on the initial coding matrix. If the initial
coding matrix fails to perform well, the ﬁnal results of ECO-
CONE are usually unsatisfactory. Secondly, its improvement is
usually hindered if it fails to discriminate the most confusing
pairs. Lastly, similar to DECOC, the training process is also
time-consuming.

In addition to the above problem-dependent ECOC methods,
Rocha et al. [24] considers the correlation and joint probability
of base binary classiﬁers to reduce the number of base clas-
siﬁers without sacriﬁcing accuracy in the ECOC code design.
More recently, Zhao et al. [32] proposed to impose the sparsity
criterion into output code learning. It is shown to have much
better performance and scalability to large scale classiﬁcation
problems compared to traditional methods like OVA. However,
it involves a series of complex optimizations to solve the
proposed model with integer constraints in learning the ECOC
coding matrix.

Different from the aforementioned methods, some sub-
space approaches have been developed. For example, subspace
ECOC [2] is based on using different feature subsets for
learning each base classiﬁer to improve independence among
classiﬁers. Adaptive ECOC [33] reformulates the ECOC mod-
els into multi-task learning where the subspace for data and
base classiﬁers are learned.

Due to the favorable properties and promising performance
of ECOC approaches for the classiﬁcation task, they have
been applied to real-world classiﬁcation applications such as
face veriﬁcation [18], ECG beats classiﬁcation [27], and even
beyond multi-class problems, such as feature extraction [34]

SUBMITTED TO TRANSACTION ON INFORMATION THEORY

3

(a) Multi-class Data

(b) Binary code (e.g., OVA)

(c) Ternary code (e.g., OVO)

(d) N-ary Coding

Fig. 1. Fail cases of existing coding scheme: The binary coding scheme sometimes creates non-separate binary classiﬁcation problems as shown in Figure
1(b). The ternary coding scheme sometimes creates cases where the data from the same class is assigned to different classes as shown in Figure 1(b). N-ary
coding scheme decomposes the difﬁcult task into some smaller and easier tasks.

and fast similarity search [31].

Though all the above-mentioned variations of the ECOC
approach endeavor to enhance the ECOC paradigm for the
classiﬁcation task, their designs are still based on either binary
or ternary codes which lack some desirable properties available
in their generalized form.

III. FROM A BINARY TO N -ARY CODING MATRIX

In this section, we discuss necessities and advantages of
N-ary coding scheme from aspects of column correlation
of coding matrix and separation between codewords of
different classes.
Existing ECOC algorithms constrain the coding values
either in {−1, 1} or {−1, 0, 1}. A lot of studies show that
when there are sufﬁcient classiﬁers, ECOC can reach stable
and reasonable performance [11, 24]. Nevertheless, binary
and ternary codes can generate at most 2NC and 3NC binary
classiﬁers, where NC denotes the number of classes. On the
other hand, due to limited choices of coding values, existing
codes tend to create correlated and redundant classiﬁers and
make them less effective “voters”. Moreover, some studies
show that binary and ternary codes usually require only
10 log2(NC) and 15 log2(NC) base classiﬁers, respectively, to
achieve optimal results [1, 12]. Furthermore, when the original
multi-class problem is difﬁcult, the existing coding schemes
cannot handle well. For example, as shown in Figure 1(b),
the binary codes like OVA may create difﬁcult base binary
classiﬁcation tasks. Ternary codes may cause cases where the
test data from the same class is assigned to different classes.

AN EXAMPLE OF N -ARY CODING MATRIX M WITH N = 4 AND NL = 6.

TABLE I

represents the code of each class and the code consists of NL
numbers in {1··· N}, where N > 3; while a column Ms of
M represents the N partitions of classes to be considered. To
be speciﬁc, the N-ary ECOC approach consists of four main
steps:

1) Generate an N-ary matrix M by uniformly random

sampling from a range {1..N} (e.g., Table I).

2) For each of the NL matrix columns, partition original
training data into N groups based on the new class
assignments and build an N-class classiﬁer.

3) Given a test example xt, use the NL classiﬁers to output
NL predicted labels for the testing output code (e.g.,
f (xt) = [4, 3, 1, 2, 4, 2]).

4) Final label prediction yt for xt is the nearest class based
on minimum distance between the training and the test-
ing output codes (e.g., yt = arg mini d(f (xt), Ci) = 4
).

One notes that N-ary ECOC randomly breaks a large
multi-class problem into a number of smaller multi-class
subproblems. These subproblems are more complicated than
binary problems and they incur additional computational cost.
Hence, there is a trade-off between error correcting capability
and computational cost.1 Fortunately, our empirical studies
indicate that N does not need to be too large to achieve good
classiﬁcation performance.

A. Column Correlations of Coding Matrix

M1 M2 M3 M4 M5 M6
1
1
2
1
1
3
2
4
3
4
4
4
3
4

4
3
2
1
2
3
4

1
1
2
3
3
3
4

2
1
1
1
2
3
4

1
2
3
4
4
3
2

C1
C2
C3
C4
C5
C6
C7

To address these issues, we extend the binary or ternary
codes to N-ary codes. One example of the N-ary coding
matrix to represent seven classes is shown in Table I. Unlike
the existing ECOC framework, a row of coding matrix M

Fig. 2. Column Correlation Comparison ( PCC v.s. NL )

1More complexity analyses can be found from Section IV.

10203040506070800.020.040.060.080.10.120.140.16ECOCN-ary ECOCECOC(Average)N-ary ECOC(Average)SUBMITTED TO TRANSACTION ON INFORMATION THEORY

4

In traditional ECOC, it suggests longer codes, i.e., NL
is larger, however more binary base classiﬁers are likely to
be more correlated. Thus, more base classiﬁers created by
binary or ternary codes are not effective for ﬁnal multi-class
classiﬁcation. To illustrate the advantage of N-ary coding
scheme in creating uncorrelated codes for base classiﬁcations,
we conduct an experiment to investigate the column corre-
lations of matrix M. The results are shown in the Figure
2. In the experiment, we set NC = 20, N = 5, and NL
varies in [10, 80], and use Pearson’s correlation (PCC) which
is a normalized correlation measure that eliminates the scaling
effect of the codes. From Figure 2, we observe that N-
ary coding scheme achieves lower correlations for columns
of coding matrix compared to conventional ternary ECOC.
Especially, when the number of tasks is small, the correlations
over the created tasks for ECOC is higher than that of the
N-ary ECOC. Therefore, an N-ary coding scheme not only
provides more ﬂexibility in creating a coding matrix, but also
generates codes that are less correlated and less redundant,
compared to traditional ECOC coding schemes.

B. Separation Between Codewords of Different Classes

Apart from the column correlation, the row separation is an-
other important measure to evaluate the error correcting ability
of the coding matrix M [1, 11]. The codes for different classes
are expected to be as dissimilar as possible. If codes (rows)
for different classes are similar, it is easier to commit errors.
Thus, the capability of error correction relies on the minimum
distance, ∆min(M ) or expected ∆(M ) for any distinct pair of
rows in the coding matrix M ∈ {−1, 0, 1}NC×NL where NC
is the number of classes, and NL is the code length. Both the
absolute distance and the Hamming distance can serve as the
measure of row separation. The key difference between these
two distances is that Hamming distance measures a scale-
invariant difference. Speciﬁcally, the Hamming distance only
cares about the number of different elements. It ignores the
scale of the difference.
Hamming Distance: One can use the generalized Hamming
distance to calculate the ∆Ham(M ) for the existing coding
schemes, which is deﬁned as follows,
Deﬁnition 1 (Generalized Hamming Distance). Let M (r1, :
), M (r2, :) denote row r1, r2 coding vectors in coding matrix
M with length NL, respectively. Then the generalized ham-
ming distance can be expressed as

0

NL(cid:88)

s=1

∆Ham(M (r1, :), M (r2, :)) =

if M (r1, s) = M (r2, s) ∧ M (r1, s) (cid:54)= 0 ∧ M (r2, s) (cid:54)= 0
if M (r1, s)(cid:54)= M (r2, s) ∧ M (r1, s) (cid:54)= 0 ∧ M (r2, s) (cid:54)= 0

1
0.5 if M (r1, s) = 0 ∨ M (r2, s) = 0.

min

(cid:19)

(cid:18)(cid:18)NC

For the OVA coding, every two rows have exactly two
entries with opposite signs, ∆Ham(OV A)
(M ) = 2. For the
OVO coding, every two rows have exactly one entry with op-
posite signs, ∆Ham(OV O)
/2+1, where
NC is the number of classes. Moreover, for a random coding
matrix with its entries uniformly chosen, the expected value of
any two different class codes is ∆Ham(RAN D)(M ) is NL/2,
where NL is the code length. A larger ∆Ham(RAN D)(M ) is

(M ) =

(cid:19)

− 1

min

2

more likely to rectify the errors committed by individual base
classiﬁers. Therefore, when NL (cid:29) NC, a random ECOC is ex-
pected to be more robust and rectify more errors than the OVO
and OVA approaches [1]. However, the choice of only either
binary or ternary codes hinders the construction of longer and
more discriminative codes. For example, binary codes can only
construct codes of length NL ≤ 2NC . Moreover, they lead
to many redundant base learners [12]. In contrast, for N-ary
ECOC, the expected value of ∆Ham(N )(M ) is NL(1 − 1
N )
(see Lemma 1 for proof). ∆Ham(N )(M ) is expected to be
larger than ∆Ham(RAN D)(M ) when N ≥ 3.

COMPARISON OF DISTANCE OF DIFFERENT CODES.

TABLE II

Coding
Schemes

OVA

OVO

ECOC

N-ary ECOC

Generalized

Hamming Distance

(cid:18)(cid:18)NC

(cid:19)

(cid:19)

2
− 1

/2 + 1

2
NL(1 − 1/N )

NL/2

Absolute
Distance

4

2NC − 2

NL

NL(N 2 − 1)/3N

Lemma 1. The expected Hamming distance for any two
distinct rows in a random N-ary coding matrix M ∈
{1, 2,··· , N}NC×NL is

).

∆Ham(N )(M ) = NL(1 − 1
N

(1)
Proof. Given a random matrix M with components chosen
uniformly over {1, 2,··· , N}, for any distinct pair of entries
in column s, i.e., M (ri, s) and M (rj, s), the probability of
N . Then the probability of M (ri, s) (cid:54)=
M (ri, s) = M (rj, s) is 1
M (rj, s) is 1 − 1
N .
Therefore, according to Deﬁnition 1, the expected Hamming

distance for M can be computed as follows,

∆Ham(N )(M ) = NL

1 × (1 − 1
N

) + 0 × 1
N

(cid:18)

(cid:19)

= NL(1 − 1
N

).

Absolute Distance: Different from the Hamming distance, the
absolute distance measures the difference scales. Thus, for
a fair comparison, we assume that coding values are in the
same scale for the absolute distance analysis. The deﬁnition
of absolute distance is given as follows,
Deﬁnition 2 (Absolute Distance). Let M (r1, :) and M (r2, :)
denote row r1 and r2 coding vectors in coding matrix M with
length NL, respectively. Then the absolute distance can be
expressed as

∆abs(M (r1, :), M (r2, :)) =

|M (r1, s) − M (r2, s)|.
For the convenience of analysis, we ﬁrst give the expected

s=1

absolute distance for N-ary coding matrix in Lemma 2.

NL(cid:88)

SUBMITTED TO TRANSACTION ON INFORMATION THEORY

5

Lemma 2. The expected absolute distance for any two
distinct rows in a random N-ary coding matrix M ∈
{1, 2,··· , N}NC×NL is

∆abs(N )(M ) = NL

.

(2)

(N 2 − 1)

3N

Proof. Given a random matrix M with components chosen
uniformly over {1, 2,··· , N}, for any distinct pair of entries
in column s, i.e., M (ri, s), M (rj, s), we denote the corre-
sponding expected absolute distance as ∆abs(N )(M (:, s)) =
E dij = E |M (ri, s) − M (rj, s)|.
It can be calculated by averaging all the possible pairwise
distances dij for i, j ∈ {1, 2,··· , N}. Since the two numbers
ri, rj are chosen randomly from {1, ..., N}, ∆N (M ) can be
expressed as follows:

∆abs(N )(M (:, s)) =

=

1
N 2

1
N 2

dij

|M (ri, s) − M (rj, s)|(3)

N(cid:88)
N(cid:88)

i,j=1

i,j=1

TABLE III

ALL POSSIBLE CHOICES OF dij .

dij

ri = 1

rj = 1

0

rj = 2

1

ri = 2

...

ri = N

1
...
N-1

0
...
···

···
···
···
0
1

rj = N

N-1
...
1
0

First, we deﬁne the sequence an as follows:
n(n + 1)

an = (1 + 2 + ··· + n) =

.

2

(4)

Table III gives all
the possible choices of dij. Thus the
calculation of ∆N (M ) is equal to taking the average of all
the entries in Table III, which can be expressed as follows:

∆abs(N )(M (:, s))=

=

=

=

=

=

2

1

N 2 (a1 + a2 + ··· + aN−1)
N 2 (1 × 2 + 2 × 3 + ··· + (N − 1)N )
N(cid:88)
(cid:33)
(cid:32) N(cid:88)
(cid:18) N (N + 1)(2N + 1)

n2 − N(cid:88)

(n2 − n)

1
N 2

1
N 2

n=1

n

n=1

n=1

− N (N + 1)

2

(5)

(cid:19)

1
N 2
N 2 − 1
3N

6

,

where (5) comes from the symmetry of dij. Then

∆abs(N )(M ) =

∆abs(N )(M (:, s)) = NL

(N 2 − 1)

3N

.

NL(cid:88)

s=1

min

For the OVA coding scheme, every two rows have exactly
two entries with opposite signs, the minimum absolute distance
∆abs(OV A)
(M ) = 4; while for the OVO coding scheme,
every two rows have exactly one entry with opposite signs
and only 2NC − 4 entries with a difference of exactly one,
(M ) = 2NC − 2. For binary random codes, the
∆abs(OV O)
expected absolute distance between any two different rows is
∆abs(RAN D)(M ) = NL. Thus, when N is large, ∆abs(N )(M )
is much larger than ∆abs(RAN D)(M ), and N-ary coding is
expected to be better.

min

The Hamming and absolute distance comparisons for dif-
ferent codes are summarized in the Table II. We can see that
N-ary coding scheme has an advantage in creating more dis-
criminative codes with larger distances for different classes in
both two distance measures. This advantage is very important
to analyze the generalization error analysis of N-ary ECOC.

IV. COMPLEXITY COMPARISON

As discussed in Section III, N-ary codes have a better error
correcting capability than the traditional random codes when
N is larger than 3. However, one notes that the base classiﬁer
of each column is no longer solving a binary problem. Instead,
we randomly break a large multi-class problem into a number
of smaller multi-class subproblems. These subproblems are
more complicated than binary problems and they incur addi-
tional computational cost. Hence, there is a trade-off between
the error correcting capability and computational cost.
If the complexity of the algorithm employed to learn
the small-size multi-class base problem is O(g(N, Ntr, d))
with N classes, Ntr training examples, d predictive features
and g(N, Ntr, d) is the complexity function w.r.t N, Ntr,
d,
then the computational complexity of N-ary codes is
O(NLg(N, Ntr, D)) for codes of length NL.

Taking SVM as the base learner for example, one can
learn each binary classiﬁcation task created by binary ECOC
codes with training complexity of O(N 3
tr) for traditional
SVM solvers that build on the quadratic programming (QP)
problems. However, a major stumbling block for these tradi-
tional methods is in scaling up these QPs to large data sets,
such as those commonly encountered in data mining appli-
cations. Thus, some state-of-the-art SVM implementations,
e.g., LIBSVM [6], Core Vector Machines [26], have been
proposed to reduce training time complexity from O(N 3
tr)
to O(N 2
tr) and O(Ntr), respectively. Nevertheless, how to
efﬁciently train SVM is not
the focus of our paper. For
the convenience of complexity analysis, we use the time
complexity of the traditional SVM solvers as the complexity of
the base learners. Then, the complexity of binary ECOC codes
is O(NLN 3
tr). Different from ECOC in the ensemble manner,
one can directly address the multi-class problem in one single
optimization process, e.g., multi-class SVM [8]. This kind
of model combines multiple binary-class optimization prob-
lems into one single objective function and simultaneously
achieves the classiﬁcation of multiple classes. In this way,
the correlations across multiple binary classiﬁcation tasks are
captured in the learning model. The resulting QP optimization
requires a complexity of O((NCNtr)3). However, it causes

SUBMITTED TO TRANSACTION ON INFORMATION THEORY

6

s=1

NL(cid:88)
NL(cid:88)
NL(cid:88)

s=1

s=1

=

1
2

≥ 1
2

≥ 1
2

high computational complexity for a relatively large number
of classes. In contrast, N-ary codes are in the complexity of
O(NL(N Ntr)3), where N < NC. In this case, it achieves
better trade-off between the error correcting capability and
computational cost, especially for large class size NC.

We summarize the time complexity of different codes in
Table IV. In Section VI-A4, our empirical studies indicate
that N does not need to be too large to achieve optimal
classiﬁcation performance.

TABLE IV

COMPLEXITY ANALYSIS

Classiﬁer

Binary ECOC

Direct Multi-Class

N-ary ECOC

SVM
O(NLN 3
tr)
O((NCNtr)3)
O(NL(N Ntr)3)

V. GENERALIZATION ANALYSIS OF N -ARY ECOC.

In Section V-A, we study the error correcting ability of an
N-ary code. In Section V-B, we derive the generalization error
bound for N-ary ECOC independent of the base classiﬁer.

A. Analysis of Error Correcting on N-ary Codes

i.e., ∆N (M (ri), M (rj)) = (cid:80)NL

To study the error correcting ability of N-ary codes,
we ﬁrst deﬁne the distance between the codes in any dis-
tinct pair of rows, M (ri) and M (rj),
in an N-ary cod-
ing matrix M as ∆N (M (ri), M (rj)). It is the sum of the
NL distances between two entries, M (ri, s) and M (rj, s)
two different rows, ri and rj,
in the same column s at
s=1 ∆N (M (ri, s), M (rj, s)).
We further deﬁne ρ = minri(cid:54)=rj ∆N (M (ri), M (rj)) as the
minimum distance between any two rows in M.
Proposition 3. Given an N-ary coding matrix M and a vector
of predicted labels f (x) = [f1(x)),··· , fNL(x))] by NL base
classiﬁers for a test instance x. If x is misclassiﬁed by the N-
ary ECOC decoding, then the distance between the correct
label in M (y) and f (x) is greater than one half of ρ, i.e.,

∆N (M (y), f (x)) ≥ 1
2

ρ.

(6)

Proof. Suppose that the distance-based decoding incorrectly
classiﬁes a test instance x with known label y. In other words,
there exists a label r (cid:54)= y for which

∆N (M (y), f (x)) ≥ ∆N (M (r), f (x)).

Here, ∆N (M (y), f (x)) and ∆N (M (r), f (x)) can be ex-

panded as the element-wise summation. Then, we have

NL(cid:88)

∆N (M (y, s), fs(x)) ≥ NL(cid:88)

s=1

s=1

Based on the above inequality, we obtain:

∆N (M (y), f (x))

(cid:8)∆N (M (y, s), fs(x))+∆N (M (y, s), fs(x))(cid:9)

{∆N (M (y, s), fs(x))+∆N (M (r, s), fs(x))} (8)

{∆N (M (y, s), M (r, s))}

(9)

∆N (M (r), M (y))

1
=
2
≥ 1
2
where Inequality (8) uses Inequality (7) and Inequality (9)
follows from the triangle inequality.

ρ,

Remark 1. From Proposition 3, one notes that a mistake on
a test instance (x, y) implies that ∆N (M (y), f (x)) ≥ 1
2 ρ.
In other words,
the prediction codes are not required to
be exactly the same as the ground-truth codes for all the
base classiﬁcations. As long as the distance is no larger
than 1
2 ρ, N-ary ECOC can rectify the error committed by
some base classiﬁers, and is still able to make an accurate
prediction. This error correcting ability is very important
especially when the labeled data is insufﬁcient. Moreover, a
larger minimum distance, i.e., ρ, leads to a stronger capability
of error correcting. Note that this proposition holds for all the
distance measures and traditional ECOC schemes due to the
fact that only the triangle inequality is required in the proof.

B. Generalization Error of N-ary ECOC.

The next result provides a generalization error bound for
any type of base classiﬁer, such as the SVM classiﬁer and
decision tree, used in the N-nary ECOC classiﬁcation.
Theorem 4 (N-ary ECOC Error Bound). Given NL
base
trained on NL subsets
{(xi, M (yi, s))i=1,··· ,Ntr}s=1,··· ,NL of the dataset with Ntr
instances for coding matrix M ∈ {1, 2,··· , N}NC×NL. The
generalized error rate for the N-ary ECOC approach using
distance-based decoding is upper bounded by

classiﬁers, f1,··· , fNL,

2NL ¯B

ρ

,

(10)

(cid:80)NL

s=1 Bs and Bs is the upper bound of the

where ¯B = 1
NL
distance-based loss for the sth base classiﬁer.
Proof. According to Proposition 3,
for any misclassiﬁed
data instance, the distance between its incorrect label vec-
(cid:80)NL
tor f (x) and the true label vector M (y) should sat-
isfy the minimal distance ρ
i.e., ∆N (M (y), f (x)) =
2 ,
s=1 ∆N (M (y, s), fs(x)) ≥ ρ
2 .
Let a be the number of incorrect label predictions for a set

of test instances of size Nte. One obtains

∆N (M (r, s), fs(x)).

(7)

a

ρ
2

∆N (M (yi, s), f (xi)).

(11)

≤ Nte(cid:88)

NL(cid:88)

i=1

s=1

SUBMITTED TO TRANSACTION ON INFORMATION THEORY

7

(cid:80)NL

s=1 Bs
ρ

=

2NteNL ¯B

ρ

,

(12)

Then,

a ≤ 2Nte

(cid:80)NL

where ¯B = 1
NL

s=1 Bs.

Hence, the testing error rate is bounded by 2NL ¯B

.

ρ

Remark 2. From Theorem 4, one notes that for the ﬁxed NL,
the generalization error bound of the N-ary ECOC depends
on the two following factors:

1) The averaged loss ¯B for all the base classiﬁers. In
practice, some base tasks may be badly designed due
to the randomness. As long as the averaged loss ¯B over
all the tasks is small, the resulting ensemble classiﬁer is
still able to make a precise prediction.

2) The minimum distance ρ for coding matrix M. As we
discussed in Proposition 3, the larger ρ, the stronger
capability of error correcting N-ary code has.

Both two factors are affected by the choice of N. In particular,
¯B increases as N increases since the base classiﬁcation tasks
become more difﬁcult. On the other hand, from experimental
results in Figure 3(b), it is observed that ρ becomes larger
when N increases. Therefore, there is a tradeoff between these
two factors.

VI. EXPERIMENTAL RESULTS

We present experimental results on 11 well-known UCI
multi-class datasets from a wide range of application domains.
The statistics of these datasets are summarized in Table V. The
parameter N is chosen by cross-validation procedure. With
the tuned parameters, all methods are run ten realizations.
Each has different random splittings with ﬁxed training and
testing size as given in Table V. Our experimental results focus
on the comparison of different encoding schemes rather than
decoding schemes. Therefore, we ﬁx generalized hamming
distance as the decoding strategy for all the coding designs
for a fair comparison.

SUMMARY OF THE DATASETS USED IN THE EXPERIMENTS.

TABLE V

Dataset
Pendigits

Vowel
News20
Letters
Auslan
Sector
Aloi
Glass

Satimage

Usps

Segment

#Train
3498
462
3993
5000
1000
3207
50000
100
3435
4298
1310

#Test
7494
528
15935
15000
1565
6412
58000
114
3000
5000
1000

#Features

16
10

62061

16
128
55197
128
9
36
256
19

#Classes

10
10
20
26
95
105
1000
10
7
10
7

To investigate the effectiveness of the proposed N-ary
coding scheme, we compare it with problem-independent
coding schemes including OVO, OVA, and random ECOC as
well as the state-of-art problem-dependent methods such as
ECOCONE and DECOC. For the random ECOC encoding
scheme, or ECOC in short, and the N-ary ECOC strategy, we

select the matrix with the largest minimum absolute distance
from 1000 randomly generated matrices.
For the problem-dependent approach DECOC, the length of
the ECOC codes is exactly NC − 1 [22]. For the ECOCONE,
we initialize the ECOC matrix with OVA matrix [13]. The
length of the ECOC code is also learned during the training
step. We use the ECOC library [14] for the implementation
of all these baseline methods. To ensure a fair comparison
and easy replication of results, the base learners decision tree
CART [5] and linear SVM are implemented with the CART
decision tree MATLAB toolbox and the LIBSVM [6] with the
linear kernel in default settings, respectively.

A. Error Bound Analysis on N-ary ECOC.

In the bound analysis, we choose hamming distance 1 to
measure the row separation as a showcase. According to
Theorem 4, the generalization error bound depends on the
minimum distance ρ between any two distinct rows in the
N-ary coding matrix M as well as the average loss of base
classiﬁers ¯B. In particular, the expected value of ∆N (M )
scales with O(N ).

that

1) Average distance ∆N (M ) v.s. N.: Recall

In this subsection, we investigate the effect of the number
of classes N using the Pendigits dataset with CART as the
base classiﬁer to illustrate the following aspects: (i) ∆N (M )
between any two distinct rows of codes (see Figure 3(a) ),
(ii) ρ (see Figure 3(b)), (iii) ¯B
ρ (see Figure 3(c)), and (iv)
the classiﬁcation performance (see Figure 4). The empirical
results corroborate the proposed error bounds in Theorem 4.
the
hamming distance for different coding matrices discussed in
Section III are: ∆N (M ) = NL(1 − 1
N ), ∆rand(M ) = NL/2,
min(M ) = 2 and ∆ovo
∆ova
From Figure 3(a), we observe that the empirical average
hamming distances of the constructed N-ary coding matri-
ces for random N-ary schemes are close to NL(1 − 1
N ).
Furthermore, when there are 45 base classiﬁers, the average
distance for N-ary ECOC is larger than 30, which is larger
than that of the binary random codes with an average absolute
distance of 22.5. Moreover, a higher N leads to a larger
average distance. Comparing Figure 3(a) and Figure 3(b) , the
large average distance ∆N (M ) also correlates with the large
minimum distance ρ.

(cid:18)(cid:18)NC

min(M ) =

/2 + 1.

(cid:19)

(cid:19)

− 1

2

2) Minimum distance ρ v.s. N.: For the Pendigits dataset
with 10 classes, ρ for OVA and OVO are 4 and 18, respectively.
From Figure 3(b), we observe that with a ﬁxed number of
base classiﬁers, ρ increases with the number of multi-class
subproblems of class-size N, meanwhile ρ also increases with
respect to the code length NL. Furthermore, in comparison to
the other coding schemes, our proposed method usually creates
a coding matrix with a large ρ. For example, in Figure 3(b),
one observes that when there are 25 and 45 base classiﬁers,
the corresponding ρ for binary random codes are 0. On the
other hand, N-ary ECOC, given a sufﬁciently large N, creates
an N-ary coding matrix with ρ to be larger than 10 and
20, respectively. Although N-ary ECOC creates an N-ary
coding matrix with a large ρ when N is larger, in real-world

SUBMITTED TO TRANSACTION ON INFORMATION THEORY

8

(a) Average ∆N (M ) v.s. N.

(b) ρ v.s. N.

(c) ¯B

ρ v.s. N.

Fig. 3. Experimental results to study error bound (Theorem 4) w.r.t. N.

applications, it is preferred that N is not too large to ensure
reasonable computational cost and difﬁculty of subproblems.
In short, N-ary ECOC provides a better alternative to creating
a coding matrix with a large class separation compared to
traditional coding schemes.

3) Ratio ¯B/ρ v.s. N.: Both ¯B and ρ are dependent on N.
Moreover, from the generalization error bound, we observe
that ¯B/ρ directly affects classiﬁcation performance. Hence,
this ratio, which bounds the classiﬁcation error, requires
further investigation. Figure 3(c) shows that when N = 4,
the ratio ¯B/ρ is lowest. This observation suggests that the
more the row and column separation of the coding matrix, the
stronger the capability of error correction [11]. Therefore, N-
ary ECOC is a better way to creating the coding matrix with
large separation among the classes as well as more diversity,
compared to the binary and ternary coding schemes. One notes
that ¯B/ρ starts to increase when N ≥ 5. This means that the
increase of the average base classiﬁer loss ¯B overwhelms the
increase in ρ. The reason for this phenomena is the increase in
difﬁculty of the subproblem classiﬁcation with more classes.
4) Classiﬁcation Accuracy v.s. N.: Next, we study the
impact of N on the multi-class classiﬁcation accuracy. We use
datasets Pendigits, Letters, Sectors, Aloi with 10 classes, 26
classes, 105 classes, 1000 classes respectively as showcase. In
order to a obtain meaningful analysis, we choose a suitable
classiﬁer for different datasets. In particular, we apply the
CART to datasets Pendigits, Letters and Aloi and linear
SVM to Sectors. One observes from Figure 4 that the N-
ary ECOC achieves competitive prediction performance when
3 ≤ N ≤ 10. However, given sufﬁcient base learners, the
classiﬁcation error starts increasing when N is large (e.g.
N > 4 for Pendigits, N > 5 for Letters and N > 8 for Sector).
This is because the base tasks are more challenging to solve
when N is large and it indicates the inﬂuence of ¯B outweighs
that of ρ. Furthermore, one observes that the performance
curves in Figure 3(c) and 4(a) roughly correlate to each other.
Hence, one can estimate the trend in the empirical error using
the ratio ¯B/ρ. This veriﬁes the validity of the generalized
error bound in Theorem 4. To investigate the choice of N
on multi-class classiﬁcation more comprehensively, we further
conduct experiments on the other datasets. The results of
datasets Pendigits, Letters, Sectors and Aloi are summarized
in Figure 4(a), 4(b), Figure 4(c) and Figure 4(d), respectively.

For the rest of the datasets, we have the similar observations.
In general, smaller values of N (N ∈ [3, 10]) usually lead
to reasonably competitive performance. In other words, the
complexity of base learners for N-ary codes does not need to
signiﬁcantly increase above 3 for the performance to be better
than existing binary or ternary coding approaches.

5) Classiﬁcation Accuracy v.s. NL.: From Figure 5, we
observe that high accuracy can be achieved with a small
number of base learners. Another important observation is that
given fewer base learners, it is better to choose a large value
of N rather than a small N. This may be due to the fact that
a larger N leads to stronger discrimination among codes as
well as base learners. However, neither a large nor small N
can reach optimal results given a sufﬁciently large NL.

B. Comparison to State-of-the-art ECOC Strategies.

We compare our proposed N-ary ECOC scheme to other
state-of-the-art ECOC schemes with different base classiﬁers
including decision tree (DT) [5] and support vector machine
(SVM) [6]. 2 The two binary classiﬁers can be easily extended
to a multi-class setting. In particular, we use the multi-class
SVM (M-SVM) [8] implemented with the MSVMpack [19]. In
addition to the multi-class extension of the two classiﬁers, we
also compare N-ary ECOC to OVO, OVA, random ECOC,
ECOCONE and DECOC with the two binary classiﬁers.
For random ECOC and N-ary ECOC, we report the best
results with NL ≤ NC(NC − 1)/2, which is sufﬁcient for
conventional random ECOC to reach optimal performance
[1, 12]. But for Aloi dataset with 1000 classes, we only report
the results for all the ECOC based methods within NL = 1000
due to its large class size.

1) Comparison to state-of-the-art ECOC with SVM Clas-
siﬁers: The classiﬁcation accuracy of different ECOC cod-
ing schemes as well as proposed N-ary ECOC with SVM
classiﬁers are presented in Table VI. We observe that OVO
has the best and most stable performance on most datasets
of all the encoding schemes except for N-ary ECOC. This
is because all the information between any two classes is
used during classiﬁcation and the OVO coding strategy has
no redundancy among different base classiﬁers. However, it

2Note that coding design is independent from base learners. It is fair to ﬁx

the base learners for ECOC coding comparison.

345678100101102103Base Multi−class Size NAverage Distance  NL = 5NL = 25NL = 45345678100101102Base Multi−class Size NMin Distance  NL = 5NL = 25NL = 4534567810−310−210−1Base Multi−class Size NRatio  NL = 5NL = 25NL = 45SUBMITTED TO TRANSACTION ON INFORMATION THEORY

9

(a) Pendigits.

(b) Letters.

(c) Sector.

(d) Aloi.

Fig. 4. Accuracy vs. N.

(a) Pendigits.

(b) Letters.

(c) Sector.

(d) Aloi.

Fig. 5. Accuracy vs. NL.

CLASSIFICATION ACCURACY AND STANDARD DEVIATION OBTAINED BY SVM CLASSIFIERS FOR UCI DATASETS.

TABLE VI

Dataset
Pendigits

Vowel
News20
Letters
Auslan
Sector
Aloi
Glass

Satimage

Usps

Segment

Mean Rank

OVO

93.71 ± 2.03
48.67 ± 2.63
68.36 ± 1.70
81.85 ± 1.26
89.94 ± 2.23
85.06 ± 1.54
91.49 ± 1.68
61.84 ± 2.24
85.69 ± 2.57
94.30 ± 1.14
92.30 ± 1.46

2.8

OVA

81.75 ± 1.07
30.30 ± 1.42
72.65 ± 2.24
66.93 ± 1.37
41.12 ± 1.28
89.06 ± 1.35
84.26 ± 2.53
55.00 ± 2.23
83.11 ± 1.86
92.37 ± 1.65
92.00 ± 1.89

4.9

ECOC

87.64 ± 1.08
34.28 ± 1.55
70.61 ± 1.67
77.78 ± 1.26
83.15 ± 0.84
88.01 ± 1.47
85.48 ± 1.17
56.00 ± 0.89
81.19 ± 1.62
90.76 ± 1.45
87.80 ± 1.61

5.1

DECOC

72.21 ± 0.88
32.42 ± 1.49
66.84 ± 1.90
68.58 ± 2.35
50.86 ± 2.20
88.47 ± 1.05
82.47 ± 1.05
52.21 ± 1.21
82.71 ± 1.44
83.11 ± 1.72
90.78 ± 1.78

5.6

ECOCONE
88.84 ± 1.57
31.67 ± 1.75
70.78 ± 1.22
70.56 ± 1.58
46.24 ± 1.60
89.89 ± 0.88
85.09 ± 0.88
56.84 ± 1.12
82.56 ± 1.61
78.28 ± 1.67
78.28 ± 1.23

5.0

M-SVM

89.85 ± 1.65
41.67 ± 1.42
70.78 ± 0.85
80.16 ± 2.06
90.06 ± 1.58
88.75 ± 1.55
86.69 ± 1.02
58.84 ± 2.42
83.28 ± 0.36
92.16 ± 1.27
89.96 ± 1.49

3.0

Nary-ECOC
93.22 ± 1.71
39.96 ± 2.07
72.79 ± 1.08
82.27 ± 1.09
91.57 ± 0.96
91.05 ± 1.32
92.77 ± 1.86
56.84 ± 1.56
86.50 ± 0.93
96.15 ± 2.47
93.60 ± 1.53

1.6

CLASSIFICATION ACCURACY AND STANDARD DEVIATION OBTAINED BY CART CLASSIFIERS FOR UCI DATASETS.

TABLE VII

Dataset
Pendigits

Vowel
News20
Letters
Auslan
Sector
Aloi
Glass

Satimage

Usps

Segment

Mean Rank

OVO

93.84 ± 2.33
44.45 ± 1.74
50.60 ± 1.17
81.56 ± 1.30
79.84 ± 2.23
39.49 ± 1.33
89.26 ± 1.49
52.84 ± 1.15
85.70 ± 1.27
90.94 ± 2.16
93.68 ± 1.25

3.6

OVA

78.12 ± 1.24
33.57 ± 2.31
45.23 ± 1.15
74.69 ± 1.50
72.86 ± 2.04
41.89 ± 1.26
72.10 ± 2.60
50.12 ± 1.24
84.15 ± 1.08
80.89 ± 1.47
86.45 ± 2.22

6.3

ECOC

83.54 ± 1.30
43.65 ± 2.35
51.29 ± 1.26
89.75 ± 1.55
83.15 ± 2.84
43.60 ± 1.17
79.41 ± 1.08
54.65 ± 1.35
85.86 ± 2.75
91.95 ± 1.57
96.44 ± 2.52

2.8

DECOC

81.45 ± 1.38
35.73 ± 1.16
44.25 ± 2.29
78.56 ± 1.05
75.30 ± 1.15
44.47 ± 2.35
75.33 ± 2.13
52.21 ± 2.38
80.37 ± 1.16
80.25 ± 1.19
80.57 ± 2.16

5.7

ECOCONE
80.53 ± 1.12
36.15 ± 2.25
50.28 ± 1.37
77.10 ± 1.26
75.28 ± 2.36
44.29 ± 1.18
72.78 ± 1.80
53.02 ± 2.12
84.28 ± 2.36
80.45 ± 2.36
88.78 ± 1.36

5.1

M-CART
87.64 ± 1.12
45.45 ± 2.25
50.83 ± 1.37
77.35 ± 1.26
78.89 ± 1.18
45.89 ± 2.15
73.00 ± 2.07
64.00 ± 2.12
83.47 ± 2.36
83.54 ± 1.16
92.70 ± 1.34

3.4

Nary-ECOC
95.84 ± 1.08
48.50 ± 1.20
53.71 ± 1.70
92.15 ± 1.92
85.17 ± 1.26
47.05 ± 1.27
95.13 ± 1.89
56.00 ± 1.14
86.47 ± 2.23
92.77 ± 1.15
97.10 ± 1.28

1.1

Base Multi-class Size N2345678Accuracy0.840.860.880.90.920.940.96NL = 5NL = 25NL = 45Base Multi-class Size N2468101214161820Accuracy0.450.50.550.60.650.70.750.80.850.90.95NL = 25NL = 50NL = 200Base Multi-class Size N05101520253035404550Accuracy0.340.360.380.40.420.440.460.48NL = 25NL = 200NL = 325051015202530354045500.650.70.750.80.850.90.951Base Multi−class Size NAccuracy  NL = 50NL = 250NL = 1000Number of Base Learners NL51015202530354045Accuracy0.70.750.80.850.90.951N = 3N = 4N = 8Number of Base Learners NL020406080100120140160180200Accuracy0.60.650.70.750.80.850.90.95N = 4N = 10N = 20Number of Base Learners NL050100150200250300350Accuracy0.280.30.320.340.360.380.40.420.440.460.48N = 6N = 10N = 30010020030040050060070080090010000.40.50.60.70.80.91Number of Base Learners NLAccuracy  N = 5N = 10N = 30SUBMITTED TO TRANSACTION ON INFORMATION THEORY

10

sacriﬁces efﬁciency for better performance. It
is very ex-
pensive for both training and testing when there are many
classes in the datasets such as the Auslan, Sector and Aloi.
Especially, for Aloi with 1000 classes, it is often not viable
to calculate the entire OVO classiﬁcations in the real-world
application as it would require 499 500 base learners in the
pool of possible combinations for training and testing. The
performance of OVA is unstable. For the datasets News20 and
Sector, OVA even signiﬁcantly outperforms OVO. However,
the performances of OVA on the datasets Vowel, Letters, and
Glass are much worse than other encoding schemes. Note that
ECOCONE is initialized with OVA. Its performance largely
depends on the performance of the initial coding matrix.
When OVA performs poorly, ECOCONE also performs poorly.
Another problem-dependent coding approach DECOC sets the
ﬁxed length of ECOC codes to NC − 1. Although ECOCONE
and DECOC are problem-dependent coding strategies, their
performance is not satisfactory in general. We observe that M-
SVM achieves better results than ECOC because it considers
relationship among classes. However, the training complexity
of M-SVM is very high. In contrast to M-SVM, ECOC can
be parallelized due to independencies of base tasks. N-ary
ECOC combines the advantages of both M-SVM and ECOC
to achieve better performance.

2) Comparison to State-of-the-art ECOC with Decision
Tree Classiﬁers: Next, we compare N-ary ECOC with other
state-of-the-art coding schemes using binary decision tree
classiﬁers CART [5] as well as its multi-class extension M-
CART. We implement it with the CART toolbox with a default
setting and the results are reported in Table VII. We observe
that binary decision tree classiﬁers with traditional ECOC
strategies are worse than the direct multi-class extension of
the decision tree. The decision tree classiﬁers show better
performances than SVM on the Pendigits, Vowel, and Letters
datasets. However, it shows very poor performances on high
dimensional datasets such as News20 and Sector. This is due to
the fact that high-dimensional features often lead to complex
tree structure construction. Nevertheless, N-ary ECOC still
can signiﬁcantly improve the performance on either traditional
coding schemes with binary decision tree learner as well as
the multi-class decision tree.

In summary, our proposed N-ary ECOC is superior to
traditional ECOC encoding schemes and direct multi-class
algorithms on most tasks, and provides a ﬂexible strategy
to decompose many classes into many smaller multi-class
problems, each of which can be independently solved by either
M-SVM or M-CART in parallelization.

3) Discussion on Many class Situation: From the exper-
iments results, we observe that
the N-ary ECOC shows
signiﬁcant improvement on the Aloi dataset with 1000 classes
over other existing coding schemes as well as direct multi-class
classiﬁcation algorithms, especially decision tree classiﬁers.
For the binary or ternary ECOC codes, it is highly possible
to assign the same codes to different classes. From the
experimental results, we observe the minimum distance ρ for
binary and ternary coding are small or even tends to be 0. In
other words, the existing coding cannot help the classiﬁcation
algorithms to differentiate some classes. In contrast, N-ary

(a) Binary Code.

(b) Ternary Code.

(c) N-ary Code.

Fig. 6. Confusion matrix on Pendigits: In confusion matrix, the entry in
the ith row and jth column is the percentage of images from class i that are
misidentiﬁed as class j. Average classiﬁcation rates for individual classes are
listed along the diagonal. The last column and last row are precision (Pre)
and recall (Rec) respectively.

with NL = 1000 and N = 5, the minimum distance ρ is
741. Thus, it creates codes with larger margins for different
classes, whichexplains the superior On the other hand, the
direct multi-class algorithms cannot work well when the class
size is large. Furthermore, the computation cost for direct
C). When the class size NC is
multi-class algorithms is in O(N 3
large, the algorithms are expensive to train. On the contrary,
random ECOC codes can be easily parallelized due to the
independency among the subproblems.

4) Discussion on Performance on Each Individual Class:
To understand the performances of different codes for each in-
dividual class, we show the confusion matrix on the Pendigits
dataset in Figure 6. First, we observe that binary code (i.e.,
OVA) has very poor performances on some classes in terms
of recall or precision. For example, recall on class 2, 6 and

Class12345678910Pre190.200000.1000.70.388.92096.30.10000000.194.8300.589.80.100000.30.389.2400.1092.100000098.550.10.10089.800.3000.194.7600.300095.2000.4086.1700000090.700.1098.780.21.10.10.20.10.9089.80.7075.990.200000.500.193.1089.9100.14.601.40.13.200.10.591.246.9Rec94.435.497.882.198.249.996.498.671.891.881.8Target ClassOutput ClassClass12345678910Pre190.200000.2000.20.392.92090.70.3000.100.200.290.5300.189.9000000098.3400.6090.6000000.390.750.30.10089.700000.194.6600.100091.4000.30.292.9700000090.400099.4800.100.100089.90.3095.490.200000.40091.8092.11000.100.100.200.10.591.789.8Rec9489.697.497.999.489.699.696.985.485.693.7Target ClassOutput ClassClass12345678910Pre189.900.20.10.10.10.1000.392.92090.50.20.10.10.10.10.10.20.190.2300.390.40.10.100.100094.7400.30.1910.10.10000.391.150.100090.300000.197.2600.100.1090.9000.20.292.8700.10000.190.800.1095.98000.10.1000.189.90.5092.190.1000.10.10.10.1091.7094.810000.10.10.10.10.10.10.291.491.1Rec97.291.791.993.793.69595.397.686.589.293.2Target ClassOutput ClassSUBMITTED TO TRANSACTION ON INFORMATION THEORY

11

precision on class 10 are below 50%. It can be explained by
that as illustrated in Figure 1(b), binary codes may lead to
nonseparable cases. Nevertheless, it achieves best classiﬁcation
results on the class 2, 4, 6 and class 9. Compared to the binary
code, ternary code (i.e., OVO) largely reduces the bias and
improve precision and recall scores on most classes. What
is more interesting, when the ternary code and N-ary code
achieves comparable overall performances, N-ary achieves
smaller maximal errors. It may be beneﬁted from simpler
subtasks created by N-ary coding scheme, as shown in Figure
1(d).

VII. CONCLUSIONS

In this paper, we investigate whether one can relax binary
and ternary code design to N-ary code design to achieve better
classiﬁcation performance. In particular, we present an N-
ary coding scheme that decomposes the original multi-class
problem into simpler multi-class subproblems. The advantages
of such a coding scheme are as follows: (i) the ability to
construct more discriminative codes and (ii) the ﬂexibility for
the user to select the best N for ECOC-based classiﬁcation.
We derive a base classiﬁer independent generalization error
bound for the N-ary ECOC classiﬁcation problem. We show
empirically that the optimal N (based on classiﬁcation perfor-
mance) lies in [3, 10] with some tradeoff in computational cost.
Experimental results on benchmark multi-class datasets show
that the proposed coding scheme achieves superior prediction
performance over the state-of-the-art coding methods. In the
future, we will investigate a more efﬁcient realization of N-ary
coding scheme to improve the prediction speed.

VIII. ACKNOWLEDGEMENTS

This work is done when Dr Joey Tianyi Zhou were at
Nanyang Technological University supported by the Research
Scholarship. Dr. Ivor W. Tsang is grateful for the support from
the ARC Future Fellowship FT130100746 and ARC grant
LP150100671. Dr. Shen-Shyang Ho acknowledges the support
from MOE Tier 1 Grants RG41/12. Dr. Klaus-Robert M¨uller
gratefully acknowledges partial ﬁnancial support by DFG,
BMBF and BK21 from NRF (Korea).

REFERENCES

[1] Erin L. Allwein, Robert E. Schapire, and Yoram Singer.
Reducing multiclass to binary: a unifying approach for
J. Mach. Learn. Res., 1:113–141,
margin classiﬁers.
September 2001.

[2] Mohammad Ali Bagheri, Gholam Ali Montazer, and
Ehsanollah Kabir. A subspace approach to error correct-
ing output codes. Pattern Recognition Letters, 34(2):176
– 184, 2013.

[3] S. Bengio, J. Weston, and D. Grangier. Label embedding
trees for large multi-class tasks. In NIPS, pages 163–171,
2010.

[4] A. Beygelzimer, J. Langford, Y. Lifshits, G. Sorkin, and
A. Strehl. Conditional probability tree estimation analysis
and algorithms. In UAI, pages 51–58, 2009.

[5] Leo Breiman, J. H. Friedman, R. A. Olshen, and C. J.
Stone. Classiﬁcation and Regression Trees.
Statis-
tics/Probability Series. Wadsworth Publishing Company,
Belmont, California, U.S.A., 1984.

[6] Chih-Chung Chang and Chih-Jen Lin. Libsvm: A library
for support vector machines. ACM Trans. Intell. Syst.
Technol., 2(3):27:1–27:27, May 2011.

[7] T. Cover and P. Hart. Nearest neighbor pattern clas-
Information Theory, IEEE Transactions on,

siﬁcation.
13(1):21–27, January 1967.

[8] Koby Crammer and Yoram Singer. On the algorithmic
implementation of multiclass kernel-based vector ma-
chines. J. Mach. Learn. Res., 2:265–292, March 2002.
[9] J. Deng, S. Satheesh, A. Berg, and L. Fei-Fei. Fast
and balanced: Efﬁcient label tree learning for large scale
object recognition. In NIPS, 2011.

[10] Thomas G. Dietterich and Ghulum Bakiri.

Error-
correcting output codes: A general method for improving
multiclass inductive learning programs. In AAAI, pages
572–577. AAAI Press, 1991.

[11] Thomas G. Dietterich and Ghulum Bakiri.

Solving
multiclass learning problems via error-correcting output
codes. J. Artif. Intell. Res., 2:263–286, 1995.

[12] S. Escalera, O. Pujol, and P. Radeva. On the decoding
process in ternary error-correcting output codes. Pattern
Analysis and Machine Intelligence, IEEE Transactions
on, 32(1):120–134, 2010.

[13] Sergio Escalera and Oriol Pujol. Ecoc-one: A novel
coding and decoding strategy. In ICPR, pages 578–581,
2006.

[14] Sergio Escalera, Oriol Pujol, and Petia Radeva. Error-
J. Mach. Learn. Res.,

correcting ouput codes library.
11:661–664, March 2010.

[15] T. Gao and D. Koller. Discriminative learning of relaxed
In ICCV,

hierarchy for large-scale visual recognition.
pages 2072–2079, 2011.

[16] Nicol´as Garc´ıa-Pedrajas and Domingo Ortiz-Boyer. An
empirical study of binary classiﬁer fusion methods for
Inf. Fusion, 12(2):111–130,
multiclass classiﬁcation.
April 2011.

[17] Robert Jenssen, Marius Kloft, Alexander Zien, Soeren
Sonnenburg, and Klaus-Robert M¨uller. A scatter-based
prototype framework and multi-class extension of sup-
port vector machines. PloS one, 7(10):e42947, 2012.

[18] Josef Kittler, Reza Ghaderi, Terry Windeatt, and Jiri
Face veriﬁcation via error correcting output
Image Vision Comput., 21(13-14):1163–1169,

Matas.
codes.
2003.

[19] F. Lauer and Y. Guermeur. MSVMpack: a multi-class
support vector machine package. Journal of Machine
Learning Research, 12:2269–2272, 2011.

[20] Xu-Ying Liu, Qian-Qian Li, and Zhi-Hua Zhou. Learn-
ing imbalanced multi-class data with optimal dichotomy
weights. In Data Mining (ICDM), 2013 IEEE 13th In-
ternational Conference on, pages 478–487. IEEE, 2013.
[21] Gholam Ali Montazer, Sergio Escalera, et al. Error
correcting output codes for multiclass classiﬁcation: Ap-
plication to two image vision problems. In AISP, pages

SUBMITTED TO TRANSACTION ON INFORMATION THEORY

12

508–513. IEEE, 2012.

[22] Oriol Pujol, Petia Radeva, and Jordi Vitri`a. Discriminant
ECOC: a heuristic method for application dependent
design of error correcting output codes. In IEEE Transac-
tion on Pattern Analysis and Machine Intelligence, pages
1007–1012, 2006.

[23] J. R. Quinlan. Induction of decision trees. Mach. Learn.,

1(1):81–106, March 1986.

[24] Anderson Rocha and Siome Goldenstein. Multiclass
from binary: Expanding one-vs-all, one-vs-one and ecoc-
IEEE Transactions on Neural Net-
based approaches.
works and Learning Systems, 25(2):289–302, 2014.

[25] Jiang Su and Harry Zhang. A fast decision tree learning
In Proceedings of the 21st National Con-
algorithm.
ference on Artiﬁcial Intelligence - Volume 1, AAAI’06,
pages 500–505. AAAI Press, 2006.

[26] Ivor W. Tsang, James T. Kwok, and Pak-Ming Cheung.
Core vector machines: Fast SVM training on very large
data sets. Journal of Machine Learning Research, 6:363–
392, 2005.

[27] Elif Derya ¨Ubeyli. Ecg beats classiﬁcation using mul-
ticlass support vector machines with error correcting
output codes. Digital Signal Processing, 17(3):675–684,
2007.

[28] Xiaoyu Wang, Tianbao Yang, Guobin Chen, and Yuan-
qing Lin. Object-centric sampling for ﬁne-grained image
classiﬁcation. arXiv preprint arXiv:1412.3161, 2014.

[29] Jian-Bo Yang and Ivor W. Tsang. Hierarchical maximum
In UAI,

margin learning for multi-class classiﬁcation.
2011.

[30] Bangpeng Yao, Aditya Khosla, and Li Fei-Fei. Com-
bining randomization and discrimination for ﬁne-grained
In Computer Vision and Pattern
image categorization.
Recognition (CVPR), 2011 IEEE Conference on, pages
1577–1584. IEEE, 2011.

[31] Zhou Yu, Deng Cai, and Xiaofei He. Error-correcting
output hashing in fast similarity search. In Proceedings
of the Second International Conference on Internet Multi-
media Computing and Service, ICIMCS ’10, pages 7–10,
New York, NY, USA, 2010. ACM.

[32] Bin Zhao and Eric P. Xing. Sparse output coding for
In CVPR, pages 3350–

large-scale visual recognition.
3357. IEEE, 2013.

[33] Guoqiang Zhong and Mohamed Cheriet. Adaptive error-
In IJCAI, pages 1932–1938,

correcting output codes.
2013.

[34] Guoqiang Zhong and Cheng-Lin Liu. Error-correcting
output codes based ensemble feature extraction. Pattern
Recognition, 46(4):1091–1100, 2013.

Joey Tianyi Zhou is a scientist with the Computing
Science Department, Institute of High Performance
Computing (IHPC), Singapore. Prior to join IHPC,
he was a research fellow in Nanyang Technological
University (NTU). He received the Ph.D. degree in
computer science from NTU, Singapore, in 2015.

PLACE
PHOTO
HERE

PLACE
PHOTO
HERE

Ivor W. Tsang is an Australian Future Fellow and
an Associate Professor with the Centre for Quantum
Computation and Intelligent Systems, University of
Technology at Sydney, Ultimo, NSW, Australia. He
received the Ph.D. degree in computer science from
the Hong Kong University of Science and Technol-
ogy, Hong Kong, in 2007. He was the Deputy Di-
rector of the Centre for Computational Intelligence,
Nanyang Technological University, Singapore. He
has authored more than 100 research papers in ref-
ereed international journals and conference proceed-
ings, including JMLR, TPAMI, TNN/TNNLS, NIPS, ICML, UAI, AISTATS,
SIGKDD, IJCAI, AAAI, ACL, ICCV, CVPR, and ICDM. Dr. Tsang was a
recipient of the 2008 Natural Science Award (Class II) from the Ministry of
Education, China, in 2009, which recognized his contributions to kernel meth-
ods. He was also a recipient of the prestigious Australian Research Council
Future Fellowship for his research regarding Machine Learning on Big Data
in 2013. In addition, he received the prestigious IEEE TRANSACTIONS
ON NEURAL NETWORKS Outstanding 2004 Paper Award in 2006, the
2014 IEEE TRANSACTIONS ON MULTIMEDIA Prized Paper Award, and
a number of Best Paper Awards and Honors from reputable international
conferences, including the Best Student Paper Award at CVPR 2010, the Best
Paper Award at ICTAI 2011, and the Best Poster Award Honorable Mention
at ACML 2012. He was also a recipient of the Microsoft Fellowship in 2005
and the ECCV 2012 Outstanding Reviewer Award

PLACE
PHOTO
HERE

Shen-shyang Ho received the BS degree in math-
ematics and computational science from the Na-
tional University of Singapore in 1999, and the MS
and PhD degrees in computer science from George
Mason University in 2003 and 2007, respectively.
From August 2007 to May 2010, he was a NASA
Postdoctoral Program (NPP) fellow and then a post-
doctoral scholar at the California Institute of Tech-
nology afﬁliated to the Jet Propulsion Laboratory
(JPL). From June 2010 to December 2012, he was
a researcher working on projects funded by NASA
at
the University of Maryland Institute for Advanced Computer Studies
(UMIACS). Currently, he is a tenure-track assistant professor in the School
of Computer Engineering at
the Nanyang Technological University. His
research interests include data mining, machine learning, pattern recognition
in spatiotemporal/data streaming settings, and privacy issues in data mining.
He is also currently involved in industrial research projects funded by BMW
and Rolls-Royces. He has given tutorials at AAAI, IJCNN, and ECML.

SUBMITTED TO TRANSACTION ON INFORMATION THEORY

13

PLACE
PHOTO
HERE

Klaus-Robert M¨uller received the Diploma de-
gree in mathematical physics and the Ph.D. degree
in computer science from Technische Universit¨at
Karlsruhe, Karlsruhe, Germany, in 1989 and 1992,
respectively.
He has been a Professor of computer science with
Technische Universit¨at Berlin, Berlin, Germany,
since 2006, and he is directing the Bernstein Fo-
cus on Neurotechnology Berlin, Berlin. Since 2012,
he has been a Distinguished Professor with Korea
University, Seoul, Korea, within the WCU Program.
After a Post-Doctoral at GMD-FIRST, Berlin, he was a Research Fellow
with the University of Tokyo, Tokyo, Japan, from 1994 to 1995. In 1995, he
built up the Intelligent Data Analysis Group, GMD-FIRST (later Fraunhofer
FIRST) and directed it until 2008. From 1999 to 2006, he was a Professor
with the University of Potsdam, Potsdam, Germany. His current research
interests include intelligent data analysis, machine learning, signal processing,
and brain computer interfaces.
Dr. Mller received the Olympus Prize by the German Pattern Recognition
Society, DAGM, in 1999, and the SEL Alcatel Communication Award in 2006.
In 2012, he was elected to be a member of the German National Academy
of Sciences Leopoldina.

