6
1
0
2

 
r
a

 

M
6
1

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
9
2
2
5
0

.

3
0
6
1
:
v
i
X
r
a

PAC-Bayesian bounds for the Gram matrix and
least squares regression with a random design.

Olivier Catoni

March 17, 2016

the estimation of the Gram matrix E(cid:0)XX(cid:62)(cid:1) of a random vector

Abstract: The topics dicussed in this paper take their origin in
X ∈ Rd from a sample made of n independent copies X1, . . . , Xn of
X. They comprise the estimation of the covariance matrix and the
study of least squares regression with a random design. We pro-
pose four types of results, based on non-asymptotic PAC-Bayesian
generalization bounds: a new robust estimator of the Gram ma-
trix and of the covariance matrix, new results on the empirical
Gram matrix 1
i , new robust least squares estimators
n
and new results on the ordinary least squares estimator, including
its exact rate of convergence under polynomial moment assump-
tions.
Keywords: Gram matrix, covariance matrix, least squares re-
gression with a random design, robust estimation, ordinary least
squares estimator, PAC-Bayesian generalization bounds.

(cid:80)n
i=1 XiX(cid:62)

MSC2010: 62J10, 62J05, 62H20, 62F35, 15A52

Contents

1. A robust Gram matrix estimate.

1.1. Definition of a new estimator .

1.2. Generalization bounds

.

.

.

.

1.3. Estimation of the eigenvalues .

.

.

.

.

.

.

.

.

.

.

.

.

2. The empirical Gram matrix estimate .

3. Estimation of the covariance matrix .

4. Least squares regression .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

4.1. From Gram matrix to least squares estimates

4.2. A robust least squares estimator .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

5
5
7
8

9

18

22
22
24

CNRS – CREST, UMR 9194, 3 avenue Pierre Larousse, 92240 Malakoﬀ, France. e-

mail: olivier.catoni@ensae.fr

1

4.3. Generalization bounds for the empirical risk minimizer

26

4.4. Some lower bound for the empirical risk minimizer .

.

.

4.5. Exact convergence rate for the empirical risk minimizer

36

A. Proof of Proposition 1.1 on page 7 .

.

B. Proof of Proposition 2.2 on page 11 .

C. Proof of Lemma 2.5 on page 16 .

.

.

.

D. Proof of Proposition 3.3 on page 19 .

E. Proof of lemma 3.4 on page 20 .

.

.

.

F. Proof of Proposition 4.9 on page 34 .

G.Proof of Proposition 4.10 on page 36 .

H. Proof of Proposition 4.12 on page 38 .

I. Obtaining a quadratic form.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

J. Computation of the robust Gram matrix estimator .

J.1. Computation of the robust estimator in a fixed di-

rection.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

J.2. Computation of a robust estimate of the Gram ma-

trix .

.

.

.

.

K.Some simulation .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

2

34

42

55

56

57

58

59

63

64

67

74

74

76

77

Introduction

Let us consider n independent copies (X1, . . . , Xn) of a random vector X

whose probability distribution P belongs to M1
+
measures on Rd (equiped with the Borel σ-algebra).

(cid:0)Rd(cid:1), the set of probability
Gram matrix G = E(cid:0)XX(cid:62)(cid:1) and comprise the estimation of the covariance

The topics discussed in this paper take their origin in the estimation of the
matrix G − E(X)E(X)(cid:62) and least squares regression with a random design.
We propose four things. A new robust estimator of G, new results on the

empirical estimator

G =

1
n

n(cid:88)

i=1

XiX(cid:62)
i ,

March 17, 2016

Olivier Catoni

3

new robust least squares estimators and new results on the ordinary least
squares estimator

(cid:98)θ ∈ arg min

θ∈Rd

n(cid:88)
(cid:0)Yi − (cid:104)θ, Xi(cid:105)(cid:1)2,

i=1

where (X1, Y1), . . . , (Xn, Yn) are n independent copies of a couple (X, Y ) ∈
Rd × R of random variables. In particular we give the exact rate of conver-

gence of R((cid:98)θ) − inf θ∈Rd R(θ), where R(θ) = E(cid:2)(cid:0)Y − (cid:104)θ, X(cid:105)(cid:1)2(cid:3).

Technically, our approach is based on the estimation of the quadratic form

N (θ) def= E(cid:0)(cid:104)θ, X(cid:105)2(cid:1) = θ(cid:62)Gθ,

that computes the energy in direction θ. It can also be seen as the square of
the norm deﬁned by the Gram matrix G. Recovering G from N can be done
through the polarization identity

(cid:2)N (ξ + θ) − N (ξ) − N (θ)(cid:3),

(1)

ξ(cid:62)Gθ =

1
4

(cid:2)N (ξ + θ) − N (ξ − θ)(cid:3) =
(cid:2)N (ei + ej)− N (ei − ej)(cid:3) =

1
2

1
4

i Gej =

where ei are the vectors of the canonical basis of Rd.

that gives as a special case
Gi,j = e(cid:62)

(cid:2)N (ei + ej)− N (ei)− N (ej)(cid:3),
error |N (θ) − (cid:98)N (θ)| can be bounded with a probability close to one jointly

Our purpose is to deﬁne and study robust estimators, whose estimation
for all values of θ ∈ Rd under weak polynomial moment assumptions. More
speciﬁcally, we make a θ-dependent assumption on the variance of (cid:104)θ, X(cid:105)2,
that takes the form

1
2

sup

(cid:110)
(cid:111) ≤ κ,
E(cid:0)(cid:104)θ, X(cid:105)4(cid:1), θ ∈ Rd, E(cid:0)(cid:104)θ, X(cid:105)2(cid:1) ≤ 1
Var(cid:0)(cid:104)θ, X(cid:105)2(cid:1) ≤(cid:0)κ − 1(cid:1)E(cid:0)(cid:104)θ, X(cid:105)2(cid:1)2.

implying that

This kurtosis coeﬃcient measures the heaviness of the tail of the distribution
of (cid:104)θ, X(cid:105)2. To give a point of comparison, when X is a Gaussian random
vector, whatever its Gram matrix maybe, the above assumption is satisﬁed
for κ = 3.

Based on this assumption, we deﬁne an estimator (cid:98)N (θ) and prove a θ-
dependent uniform bound on the estimation error |N (θ) − (cid:98)N (θ)|. More pre-

cisely, instead of bounding merely

(cid:110)(cid:12)(cid:12)N (θ) − (cid:98)N (θ)(cid:12)(cid:12) : θ ∈ Rd,(cid:107)θ(cid:107) ≤ 1
(cid:111)

,

sup

March 17, 2016

Olivier Catoni

we bound with a probability close to one

(cid:12)(cid:12)(cid:12)(cid:12) N (θ)(cid:98)N (θ)

sup
θ∈Rd

4

(cid:12)(cid:12)(cid:12)(cid:12),

− 1

with the convention that 0/0 = 1 and z/0 = +∞ when z > 0. Remark that
this type of bound implies that it is possible to estimate exactly the null
space Ker(G) with a probability close to one.

This new estimator, built on the same principles as the robust mean es-
timator of [10], is interesting in at least two ways. First, it can be actually
used to estimate G, with increased performances in some heavy tail situa-
tions and with mathematical guaranties taking the form of non-asymptotic
convergence bounds under weak hypotheses.

Second, it can be compared with the empirical estimate G of G and used
as a mathematical tool to prove new generalization bounds for G, under
various hypotheses, including polynomial moment assumptions.

The estimation of the Gram matrix has many interesting possible applica-
tions. The most obvious one is to derive robust alternatives to the classical
principal component analysis based on G.
In this paper however, we will
rather focus on least squares regression with a random design. We propose
and study new stable least squares regression estimators, and also provide
new bounds for the ordinaray least squares estimator. We already studied
robust least squares regression in [5], but this time, as the reader will see,
we come up with simpliﬁed estimators and tighter results. We also come up
with interesting new results about the ordinary least squares estimator. In
particular we give its exact rate of convergence under polynomial moment
assumptions, including the case when the noise, deﬁned as Y −(cid:104)θ∗, X(cid:105), where

θ∗ ∈ arg minθ∈Rd E(cid:2)(cid:0)Y − (cid:104)θ, X(cid:105)(cid:1)2(cid:3), is not independent from X. When inde-
E(cid:2)(cid:0)Y −(cid:104)θ∗, X(cid:105)(cid:1)2(cid:3)d/n and can depart from it by an arbitrarily large or small

pendence is not assumed, this exact rate is quite interestingly not equal to

factor, as we show on examples.

Let us close this introduction with some precisions on our use of the big
O notation. In this paper we will always prove precise and fully explicit non-
asymptotic bounds. Nevertheless, as these bounds are sometimes diﬃcult
to read, we will use the big O notation to give a representation of their
order of magnitude. When we write A = O(B), where A and B are two
expressions depending on parameters of the problem, we mean that there is
a numerical constant c such that A ≤ cB. When we write A = On→∞(B),
where n is the sample size, we mean that there is a numerical constant

c such that lim supn→∞ A/B ≤ c. Remark that A = O(cid:0)B(cid:1) implies that
A = On→∞(cid:0)B(cid:1), but that the reverse implication is false in general. The

March 17, 2016

Olivier Catoni

notation A = On→∞(cid:0)B(cid:1) means that B bounds the order of magnitude of the
ﬁrst order term of A seen as a function of the sample size, whereas A = O(cid:0)B(cid:1)

5

means that B bounds the order of magnitude of A in all circumstances.

1. A robust Gram matrix estimate

1.1. Definition of a new estimator. Following the same route as in
[10] in a more elaborate setting, we deﬁne some M -estimator of N (θ), the
energy in direction θ, and derive for it non-asymptotic deviation bounds that
are uniform with respect to θ. To do this, we need to introduce the inﬂuence
function ψ : R → R deﬁned as

x (cid:55)→ ψ(x), compared with x (cid:55)→ x

x (cid:55)→ log(cid:0)1 + x + x2/2(cid:1), and x (cid:55)→ − log(cid:0)1 − x + x2/2(cid:1)
log(2),
− log(cid:0)1 − x + x2/2(cid:1), 0 ≤ x ≤ 1,
− log(cid:0)1 − x + x2/2(cid:1) ≤ ψ(x) ≤ log(cid:0)1 + x + x2/2(cid:1),

−ψ(−x),

This is a symmetric, non-decreasing, and bounded function, satisfying
x ∈ R,

ψ(x) =

x ≥ 1,

x ≤ 0.

March 17, 2016

Olivier Catoni

−2−1012−2−10121.1 Definition of a new estimator

6

as can be seen from the identity

(cid:0)1 − x + x2/2(cid:1)−1 =

1 + x + x2/2

1 + x4/4

≤ 1 + x + x2/2,

x ∈ R.

Let us consider for any positive parameter λ ∈ R+ the empirical sample

distribution

P =

1
n

δXi

n(cid:88)
(cid:110)
λ(cid:2)(cid:104)θ, x(cid:105)2 − 1(cid:3)(cid:111)

i=1

dP(x).

and the empirical criterion

rλ(θ) = λ−1

(cid:90)

ψ

This empirical criterion depends on a positive scale parameter λ ∈]0,∞[,
whose value will be set later on.
The centering of the inﬂuence function is done by adjusting the norm
(cid:107)θ(cid:107), as we are going to explain now. To achieve centering, we introduce the

multiplicative factor (cid:98)α(θ) deﬁned as
(cid:110)
(cid:98)α(θ) = sup
As rλ(0) = −λ−1ψ(λ) < 0, we see that (cid:98)α(θ) ∈ R+ ∪ {+∞}, for any θ ∈ Rd.
(cid:0)(cid:98)α θ(cid:1) = 0 as soon as (cid:98)α(θ) <

Moreover, since α (cid:55)→ rλ(α θ) is continuous, rλ
+∞. Considering that ψ is close to the identity on a neighbourhood of 0 and
that the empirical measure P is typically close to P, we may hope that, for
suitable values of λ, rλ(θ) (cid:39) N (θ) − 1 with large probability. If this is the

α ∈ R+ : rλ(αθ) ≤ 0

case, and if moreover, (cid:98)α(θ) < ∞, then

(cid:2)(cid:98)α(θ)θ(cid:3) (cid:39) N(cid:2)(cid:98)α(θ)θ(cid:3) − 1 =(cid:98)α(θ)2N (θ) − 1.

0 = rλ

(cid:111)

.

This is an incitation to deﬁne a new estimator of N (θ) as

(cid:98)Nλ(θ) =(cid:98)α(θ)−2.
(cid:16)
n(cid:88)

(cid:104)

ψ

λ

(2)

To make things easier to understand, we can also deﬁne this estimator with-
out introducing intermediate steps as

(cid:26)

(cid:98)Nλ(θ) = inf

ρ ∈ R∗

+ :

(cid:27)
ρ−1(cid:104)θ, Xi(cid:105)2 − 1(cid:1)(cid:105) ≤ 0

.

i=1

March 17, 2016

Olivier Catoni

2κ d

.

n

(cid:16)

κ(cid:2)d + log(−1)(cid:3)(cid:17)

= O

1.2 Generalization bounds

7

1.2. Generalization bounds. We prove in Appendix A a detailed propo-
sition, Proposition A.10 on page 53, whose main conclusions can be summa-
rized as follows.

Proposition 1.1 Let us assume that for some known constant κ

sup

Choose λ =

2

(cid:115)
(cid:114)

(cid:111) ≤ κ < ∞.

(cid:110)
E(cid:0)(cid:104)θ, X(cid:105)4(cid:1) : θ ∈ Rd, E(cid:0)(cid:104)θ, X(cid:105)2(cid:1) ≤ 1
(cid:2)log(−1) + 0.73 d(cid:3),
(cid:114)
(cid:2)log(−1) + 0.73 d(cid:3) + 6.81
2(κ − 1)(cid:2)log(−1) + 0.73d(cid:3)(cid:35)2
(cid:19)(cid:113)

(κ − 1)n
2(κ − 1)

(cid:18)5

+

n

1

2(κ − 1)

2

and consider µ =

(cid:34)

√
κd +
20

n >

For any conﬁdence parameter  > 0, and any sample size n such that

,

(3)

(4)

with probability at least 1 − 2, for any θ ∈ Rd,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) N (θ)
(cid:98)Nλ(θ)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ µ

1 − 2µ

− 1

(cid:32)(cid:115)

κ(cid:2)d + log(−1)(cid:3)

(cid:33)

,

n

= O

where it should be understood that z/0 = 1 if z = 0 and +∞ if z > 0, and
where inequality (4) can be decomposed into two inequalities (by removing the
absolute value) that hold each with probability at least 1 − .

N (θ) by (cid:98)Nλ(θ) is of order N (θ)(cid:112)κd/n with a subgaussian tail up to very

The conclusion of this proposition is that the accuracy of the estimation of

high (exponential with n) conﬁdence levels. Indeed, equation (4) can also be
written as

(cid:12)(cid:12)(cid:12)N (θ) − (cid:98)Nλ(θ)

(cid:12)(cid:12)(cid:12) ≤ µ(cid:98)Nλ(θ)

1 − 2µ

≤ µN (θ)
1 − 3µ

.

This fairly strong result is made possible by the assumption that κ is bounded.
As already mentioned in the introduction, when X is a multidimensional
Gaussian random variable, we can take κ = 3. Assuming that κ < ∞ requires
that the behaviour of the fourth moment of the distribution of (cid:104)θ, X(cid:105) is not

March 17, 2016

Olivier Catoni

1.3 Estimation of the eigenvalues

8

too far from the Gaussian case. This remains nonetheless a much weaker
assumption than the existence of exponential moments.

Let us mention that instead of getting a generalization bound or order

(cid:32)(cid:115)

κ(cid:2)d + log(−1)(cid:3)

(cid:33)

,

O

n

depending on the dimension d of the ambient space (or more accurately on
the rank of the Gram matrix G), it is also possible to obtain dimension
free bounds where the dimension d is replaced with the θ dependent term
Tr(G)(cid:107)θ(cid:107)2/N (θ) (to see that it is indeed some substitute for the dimension,
we can remark that this θ dependent factor is uniformly equal to the dimen-
sion d in the case when G = I, because then Tr(G) = d and N (θ) = (cid:107)θ(cid:107)2).
For such results, we refer to the works of our student Ilaria Giulini [12, 13].

The estimator (cid:98)Nλ(θ) is not a quadratic form in θ, and therefore does not
we show in appendix that it is possible to deduce from (cid:98)Nλ a robust estimate
(cid:98)G of G.
Proposition 1.2 There exists an estimator (cid:98)G of the Gram matrix G, de-
duced from (cid:98)Nλ as explained in Appendix I, such that under the same hypothe-

deﬁne an estimate of the Gram matrix G in an obvious way. Nevertheless,

ses as in Proposition 1.1, for any conﬁdence parameter  > 0 and any sample
size n satisfying equation (3), with probability at least 1− 2, for any θ ∈ Rd,

(cid:12)(cid:12)(cid:12)(cid:12) θ(cid:62)(cid:98)Gθ

θ(cid:62)Gθ

(cid:12)(cid:12)(cid:12)(cid:12) ≤ 2µ

1 − 4µ

− 1

(cid:32)(cid:115)

κ(cid:2)d + log(−1)(cid:3)

(cid:33)

n

= O

,

(5)

where µ is deﬁned as in Proposition 1.1.

lary I.2 on page 70, where the construction is applied to (cid:98)Nλ that satisﬁes

Proof. This proposition is a simpliﬁed formulation of the end of Corol-
equation (32) on page 67 with δ = µ/(1 − 2µ), so that

2δ
1 − 2δ

=

2µ
1 − 4µ

.

(cid:3)

1.3. Estimation of the eigenvalues. Let us mention that the result
stated in Proposition 1.2 induces an estimation of the eigenvalues of G. In-
deed, if λ1 ≥ λ2 ≥ ··· ≥ λd are the eigenvalues of G (counted with their

March 17, 2016

Olivier Catoni

9

,

.

− 1

λi

1 − 4µ

.

sup

i∈{1,...,d}

equation (5) holds,

(cid:98)λi = sup

The above inequality is a direct consequence of the fact that

(cid:12)(cid:12)(cid:12)(cid:12) ≤ 2µ
(cid:12)(cid:12)(cid:12)(cid:12)(cid:98)λi
θ(cid:62)(cid:98)Gθ : θ ∈ V ∩ Sd

multiplicities), and(cid:98)λ1 ≥(cid:98)λ2 ≥ ··· ≥(cid:98)λd are the eigenvalues of (cid:98)G, then when
Proof. Let Gr(cid:0)Rd, i(cid:1) be the set of linear subspaces of Rd of dimension i.

: V ∈ Gr(cid:0)Rd, i(cid:1)(cid:27)
: V ∈ Gr(cid:0)Rd, i(cid:1)(cid:27)
any V ∈ Gr(cid:0)Rd, i(cid:1), and any orthonormal basis (e1, . . . , ed),
(cid:9)(cid:17) ≥ 1,
considering the case when (e1, . . . , ed) is a basis of eigenvectors of (cid:98)G or of G,
corresponding to the eigenvalues ((cid:98)λ1, . . . ,(cid:98)λd) or (λ1, . . . , λd) respectively. (cid:3)

V ∩ span(cid:8)ei, ei+1, . . . , ed

These two identities themselves can be established from the remark that for

θ(cid:62)Gθ : θ ∈ V ∩ Sd

whereas λi = sup

inf

(cid:26)
(cid:26)

(cid:16)

dim

(cid:110)
(cid:110)

inf

(cid:111)
(cid:111)

2. The empirical Gram matrix estimate

In this section, we study the empirical Gram matrix estimate

n(cid:88)

i=1

G =

1
n

XiX(cid:62)
i ,

λ deﬁned in Proposition 1.1 on page 7, we will write in this section for short

and the corresponding quadratic form N (θ) = θ(cid:62)Gθ. We use the previous

robust estimate (cid:98)Nλ(θ) of θ(cid:62)Gθ as a tool. As we will always use the value of
(cid:98)N instead of (cid:98)Nλ. Our approach is to analyze the diﬀerence N (θ) − (cid:98)N (θ),
First of all, we deduce from the deﬁnitions of (cid:98)N and N that (cid:98)N (θ) =
with probability at least 1 − , N (θ)/(cid:98)N (θ) ≤ 1 +(cid:98)δ, so that in the case when
(cid:0)(cid:98)N (θ)−1/2θ(cid:1) = 0. As a consequence,
N (θ) > 0, (cid:98)N (θ) > 0 also, so that rλ
λ(cid:2)(cid:104)θ, x(cid:105)2(cid:98)N (θ)−1 − 1(cid:3) dP(x)

showing that it is small under suitable assumptions.
N (θ) = 0 almost surely for any θ ∈ Ker G (that is any θ such that N (θ) = 0).
On the other hand, under the hypotheses of Proposition 1.1 on page 7,

− 1 = λ−1

(cid:90)

N (θ)(cid:98)N (θ)

March 17, 2016

Olivier Catoni

(cid:90)

= λ−1

g(cid:2)λ(cid:0)(cid:104)θ, x(cid:105)2(cid:98)N (θ)−1 − 1(cid:1)(cid:3) dP(x),

10

where g(z) = z − ψ(z). It is easy to compute

1,



z ≥ 1

g(cid:48)(z) =

z2

1 + (z − 1)2 , 0 ≤ z ≤ 1,
g(cid:48)(−z),

z ≤ 0,

showing that g(cid:48)(z) ≤ z2, and therefore that g(z) ≤ max{z, 0}3/3, for any
z ∈ R. We see also that for any p ∈ [0, 2], and any z ∈ R+, g(cid:48)(z) ≤ zp,
so that more generally g(z) ≤ max{z, 0}p+1/(p + 1) for any z ∈ R and any
p ∈ [0, 2]. As a consequence

Proposition 2.1 Let us make the same assumptions as in Proposition 1.1
on page 7. On an event of probability at least 1−  that includes the event of
probability at least 1 − 2 described in Proposition 1.1 on page 7,

where (z)+ = max{z, 0} and λ is deﬁned as in Proposition 1.1 on page 7, so
that

This proposition uses random upper bounds. Nevertheless, it gives an indica-
tion that in good cases, when the ﬂuctuations of these random upper bounds
− 1
should be of order λ2, that is of order n−1, whereas, as we have already seen,

(cid:12)(cid:12)(cid:12)(cid:12),
(cid:12)(cid:12)(cid:12)(cid:12) N (θ)(cid:98)N (θ)
are not too wild, the diﬀerence between N and (cid:98)N , measured by
(cid:12)(cid:12)(cid:12)(cid:12) is of order n−1/2. More precisely, the second inequality proves
(cid:12)(cid:12)(cid:12)(cid:12) N (θ)(cid:98)N (θ)
that N (θ) cannot be signiﬁcantly smaller than (cid:98)N (θ), while the ﬁrst inequal-

− 1

ity shows that it can be signiﬁcantly larger, but only in the case when the

March 17, 2016

Olivier Catoni

N (θ)(cid:98)N (θ)

− 1 ≤ inf
p∈[0,2]

≤ λ2
3

and

1 − N (θ)(cid:98)N (θ)

≤ λ2
3

dP(x)

+

λp

p + 1

(cid:90) (cid:16)(cid:104)θ, x(cid:105)2(cid:98)N (θ)−1 − 1
(cid:17)p+1
(cid:90) (cid:18)
(cid:19)3
(cid:104)θ, x(cid:105)2(cid:98)N (θ)−1 − 1
(cid:90) (cid:16)
1 − (cid:104)θ, x(cid:105)2(cid:98)N (θ)−1(cid:17)3
2(cid:2)log(−1) + 0.73 d(cid:3)

dP(x),

+

+

λ2 =

(κ − 1)n

.

dP(x) ≤ λ2
3

,

11

ﬂuctuations of the random quantity

(cid:90)

(cid:104)θ, x(cid:105)6N (θ)−3dP(x)

are not bounded with n, since with probability 1 − 2

(cid:90) (cid:16)(cid:104)θ, x(cid:105)2(cid:98)N (θ)−1−1
(cid:17)3

dP(x) ≤

+

(cid:90) (cid:18)

(cid:18) 1 − µ

(cid:19)

(cid:19)3

(cid:104)θ, x(cid:105)2N (θ)−1

(cid:18) 1 − µ

(cid:19)3(cid:90)

1 − 2µ

≤

−1

dP(x)

1 − 2µ
(cid:104)θ, x(cid:105)6N (θ)−3 dP(x),

+

where µ is deﬁned as in Proposition 1.1 on page 7.

We will now replace the bounds in the previous proposition by more

explicit ones.
Write the Gram matrix G in diagonal form as

G = U diag(cid:0)λ1, . . . , λd

(cid:1) U(cid:62),

where U U(cid:62) = I and λ1 ≥ ··· ≥ λd, and deﬁne
−1/2
i

G−1/2 = U diag

As almost surely Xi ∈ Im(G), almost surely G1/2 G−1/2Xi = Xi and therefore

U(cid:62).

, i = 1, . . . , d

(cid:104)
(cid:105)
1(cid:0)λi > 0(cid:1)λ
(cid:110)(cid:10)G−1/2Xi, θ(cid:11) : θ ∈ Im(G),(cid:107)θ(cid:107) ≤ 1
(cid:111)
(cid:110)(cid:10)G−1/2Xi, G1/2θ(cid:11) : θ ∈ Rd,(cid:13)(cid:13)G1/2θ(cid:13)(cid:13) ≤ 1
(cid:111)
(cid:110)(cid:104)Xi, θ(cid:105) : θ ∈ Rd, E(cid:0)(cid:104)θ, Xi(cid:105)2(cid:1) ≤ 1
(cid:111)
(cid:110)(cid:104)θ, Xi(cid:105) : θ ∈ Rd, E(cid:0)(cid:104)θ, X(cid:105)2(cid:1) ≤ 1
(cid:111)

sup

.

. (6)

(cid:13)(cid:13)G−1/2Xi

(cid:13)(cid:13) = sup

= sup

= sup

Consider

R = max
i=1,...,n

(cid:107)G−1/2Xi(cid:107) = max

i=1,...,n

Using these remarks and this deﬁnition, we can state the following conse-
quence of Proposition 2.1:

Proposition 2.2 Deﬁne the quantities

(cid:98)δ =

µ

1 − 2µ

,

where µ is as in Proposition 1.1 on page 7,

March 17, 2016

Olivier Catoni

γ+ =

γ− =

δ+(θ) =

12

,

where R is deﬁned in equation (6),

3(κ − 1)n

2(cid:2)log(−1) + 0.73 d(cid:3)R4 (1 +(cid:98)δ)2
2(cid:2)log(−1) + 0.73 d(cid:3)
2(cid:2)log(−1) + 0.73 d(cid:3)
N (θ)(cid:98)N (θ)
2(cid:2)log(−1) + 0.73 d(cid:3)

3(κ − 1)n

3(κ − 1)n

,

(cid:90) (cid:16)(cid:104)θ, x(cid:105)2(cid:98)N (θ)−1 − 1
(cid:17)3
(cid:90) (cid:16)
1 − (cid:104)θ, x(cid:105)2(cid:98)N (θ)−1(cid:17)3

+

+

dP(x)

dP(x)

≤ γ+

≤ γ−.

δ−(θ) =

3(κ − 1)n

−(cid:98)δ + γ−
1 +(cid:98)δ
(cid:124) (cid:123)(cid:122) (cid:125)
=O((cid:98)δ)

≤ −(cid:98)δ + δ−(θ)
1 +(cid:98)δ

≤ N (θ)
N (θ)

− 1 ≤ (cid:98)δ + δ+(θ)
1 −(cid:98)δ
(1 −(cid:98)δ)(1 − γ+)
(cid:32)(cid:114)

1

Under the hypotheses of Proposition 1.1 on page 7, using the above notation
and deﬁnitions, with probability at least 1 − 2, for any θ ∈ Rd

≤

(cid:98)δ + γ+
(1 −(cid:98)δ)(1 − γ+)
(cid:33)
where it is useful to remember that(cid:98)δ = O
lower bound for N (θ)/N (θ), that is always O(cid:0)(cid:98)δ(cid:1) and holds under the hy-

The proof of this proposition is given in appendix. Let us remark that our

κ[d + log(−1)]

− 1 ≤

n

.

,

potheses stated in Proposition 1.1 on page 7, can be compared with the
lower bound on the smallest singular value of a random matrix with i.i.d.
isotropic columns given in [14, Theorem 1.3]. The ﬁrst point in this theorem
of V. Koltchinskii and S. Mendelson gives a slightly worse lower bound with
less explicit constants (in particular, the dependence in κ is not explicit) un-
√
der a slightly stronger condition, whereas points 2. and 3. of their theorem
prove slower rates than 1/

n under weaker assumptions than ours.

Let us now upper-bound the random quantity R deﬁned by equation
(6) and consequently γ+ under suitable assumptions. A simple choice is to
assume an exponential moment of the type

(cid:26)

E

exp

(cid:16)(cid:107)G−1/2X(cid:107)2 − d − η

(cid:17)(cid:21)(cid:27)

(cid:20) α

2

≤ 1,

(7)

March 17, 2016

Olivier Catoni

13

where α and η are two positive real constants.

Under this assumption, with probability at least 1 − ,

Remark that E(cid:0)(cid:107)G−1/2X(cid:107)2(cid:1) = d, so that assumption (7) can also be written

2
α

(cid:20) α

R2 ≤ d + η +

log(cid:0)n/(cid:1).
(cid:16)(cid:107)G−1/2X(cid:107)2 − E(cid:0)(cid:107)G−1/2X(cid:107)2(cid:1) − η
(cid:16)(cid:107)G−1/2X(cid:107)2 +

log(1 − α)

2

d
α

(cid:17)(cid:21)(cid:27)

≤ 1.

Let us also remark that in the case when X ∈ Rd is a centered Gaussian
vector,

= 1,

0 < α < 1,

(cid:17)(cid:21)(cid:27)
(cid:2)log(1 − α) + α(cid:3). In the case when
(cid:17)(cid:21)(cid:27)

0 < α < 1,

log(1 − α)

d
α

≤ 1,

so that we can take in this case η = − d
α
X is a non centered Gaussian vector, we can also check that

(cid:16)(cid:107)G−1/2X(cid:107)2 +

E

exp

as

E

exp

(cid:26)

(cid:20) α

2

(cid:20) α

2

E

exp

(cid:26)

(cid:26)

so that the same choice of η is still valid.

Hypothesis (7) is quite strong, and can be replaced by the more general

assumption that

(cid:26)

E

exp

(cid:20)α

2

(cid:16)(cid:107)G−1/2X(cid:107)2p − E(cid:0)(cid:107)G−1/2X(cid:107)2p(cid:1) − η

(cid:17)(cid:21)(cid:27)

≤ 1,

(8)

for some exponent p ∈]0, 1] and positive constants α and η. Under this new
assumption, with probability at least 1 − ,

R2 ≤

(cid:19)1/p

log(cid:0)n/(cid:1) + η
(cid:19)1/p

(cid:18)
E(cid:0)(cid:107)G−1/2X(cid:107)2p(cid:1) +
(cid:18)
log(cid:0)n/(cid:1) + η
E(cid:0)(cid:107)G−1/2X(cid:107)2p(cid:1) ≤ E(cid:0)(cid:107)G−1/2X(cid:107)2(cid:1)p = dp.

dp +

2
α

2
α

≤

,

since

March 17, 2016

Olivier Catoni

14

Proposition 2.3 Let us assume that condition (8) as well as the hypotheses
of Proposition 1.1 on page 7 are satisﬁed, and introduce the constant

2(cid:2)log(−1) + 0.73 d(cid:3)(cid:2)E(cid:0)(cid:107)G−1/2X(cid:107)2p(cid:1) + 2α−1 log(n/) + η(cid:3)2/p(1 +(cid:98)δ)2
(cid:98)γ+ =
≤ 2(cid:2)log(−1) + 0.73 d(cid:3)(cid:2)dp + 2α−1 log(n/) + η(cid:3)2/p(1 +(cid:98)δ)2

3(κ − 1)n

.

3(κ − 1)n

With probability at least 1 − ,

γ+ ≤(cid:98)γ+,

so that with probability at least 1 − 3, for any θ ∈ Rd,

−(cid:98)δ + γ−
1 +(cid:98)δ
(cid:124) (cid:123)(cid:122) (cid:125)
=O((cid:98)δ)

≤ N (θ)
N (θ)

− 1 ≤

,

(cid:98)δ +(cid:98)γ+
(1 −(cid:98)δ)(1 −(cid:98)γ+)+
(cid:124)
(cid:123)(cid:122)
(cid:125)
= On→∞((cid:98)δ)
(cid:33)

n

.

(cid:32)(cid:115)

κ(cid:2)d + log(−1)(cid:3)

(cid:98)δ = O

where the notations are the same as in Proposition 2.2 on page 11 and where
it is useful to remember that

We can also replace hypothesis (8) by a polynomial moment assumption.
Remembering Proposition 2.1 on page 10, remark that on the set Ω of prob-
ability at least 1− 2 appearing in Proposition 1.1 on page 7, for any θ ∈ Rd,

− 1 ≤ λp
p + 1

(cid:90) (cid:16)(cid:104)θ, x(cid:105)2(cid:98)N (θ)−1 − 1
(cid:17)p+1
N (θ)(cid:98)N (θ)
(cid:90) (cid:16)
(cid:17)p+1
N (θ)(cid:107)G−1/2x(cid:107)2(cid:98)N (θ)−1 − 1
Since N (θ)(cid:98)N (θ)−1 ≤ 1 +(cid:98)δ on the event Ω,
(cid:90) (cid:16)(cid:104)θ, x(cid:105)2(cid:98)N (θ)−1 − 1
(cid:17)p+1

≤ λp
p + 1

f (x)P(x),

dP(x)

+

+

(cid:90)
(cid:17)p+1
(1 +(cid:98)δ)(cid:107)G−1/2x(cid:107)2 − 1

dP(x) ≤

+

(cid:16)

.

+

where

f (x) =

dP(x).

(9)

March 17, 2016

Olivier Catoni

(cid:90)

From Bienaym´e Chebyshev’s inequality, with probability at least 1 − ,

(cid:18)Var(cid:2)f (X)(cid:3)

(cid:19)1/2 ≤ E(cid:2)f (X)(cid:3)+

(cid:18) E(cid:2)f (X)2(cid:3)

(cid:19)1/2

f (x) dP(x) ≤ E(cid:2)f (X)(cid:3)+

n

n

15

.

This leads to
Proposition 2.4 Consider some exponent p ∈]1, 2] and introduce the bound

(cid:101)γ+ =

1

p + 1

(cid:18)2(cid:2)log(−1) + 0.73 d(cid:3)

(κ − 1)n

(cid:18)2(cid:2)log(−1) + 0.73 d(cid:3)

+

(κ − 1)n

≤ 1
p + 1

E

(cid:104)(cid:16)

(cid:19)p/2(cid:34)
(cid:105)
(cid:17)p+1
(1 +(cid:98)δ)(cid:107)G−1/2X(cid:107)2 − 1
(cid:32) E(cid:2)(cid:0)(1 +(cid:98)δ)(cid:107)G−1/2X(cid:107)2 − 1(cid:1)2p+2
(cid:33)1/2(cid:35)
(cid:3)
(cid:34)
(cid:19)p/2
E(cid:0)(cid:107)G−1/2X(cid:107)2p+2(cid:1)
(cid:33)1/2(cid:35)
(cid:32) E(cid:0)(cid:107)G−1/2X(cid:107)4p+4(cid:1)

(1 +(cid:98)δ)p+1

n

+

+

+

n

.

Under the hypotheses of Proposition 1.1 on page 7, with probability at least
1 − 3, for any θ ∈ Rd,

−(cid:98)δ + γ−
1 +(cid:98)δ
(cid:124) (cid:123)(cid:122) (cid:125)
= O((cid:98)δ)

≤ N (θ)
N (θ)

− 1 ≤

,

(cid:98)δ +(cid:101)γ+
(1 −(cid:98)δ)(1 −(cid:101)γ+)+
(cid:123)(cid:122)
(cid:125)
(cid:124)
= On→∞((cid:98)δ)
(cid:33)

n

.

(cid:32)(cid:115)

κ(cid:2)d + log(−1)(cid:3)

(cid:98)δ = O

where the notations are otherwise the same as in Proposition 2.2 on page 11
and whese we recall that

(cid:18)2(cid:2)log(−1) + 0.73 d(cid:3)

To give an idea of the best possible order of the bound with respect to the
dimension, let us notice that, from Jensen’s inequality,

(cid:19)p/2(cid:2)(1 +(cid:98)δ)d − 1(cid:3)p+1(cid:2)1 + (n)−1/2(cid:3).
(cid:101)γ+ ≥ 1
a(cid:101)γ+ of order d3p/2+1/np/2 = d4/n when p = 2. The power of d in this second

Therefore, if we take for example  = 1/n, the best we can hope for is to get

3(κ − 1)n

p + 1

March 17, 2016

Olivier Catoni

16

order term is clearly not optimal, due to the rather crude inequality (9) used
to get this proposition. It would have been more satisfactory to get an upper

bound(cid:101)γ+ of the same order as γ−, that is of order d/n, at least when p = 2.
Also, one may think that imposing that E(cid:0)(cid:107)G−1/2X(cid:107)4p+4(cid:1) < ∞ for some
p > 1 that is necessary to get (cid:101)γ+ < ∞ is asking for a pretty high moment

Anyhow, the idea here is that, since we are dealing with a second order term
in 1/n, we can privilege the simplicity of the proof over the sharpness of the
result.

condition. One can get a rate depending on a lower moment assumption
using the following variant of Bienaym´e Chebishev’s inequality.
Lemma 2.5 Let q ∈ [1, 2] be some exponent. Consider the constant

Cq =

qq−1

2(q − 1)q−1(1 − q/2)(2−q)/q ≤ 1.4,

where by convention C1 = limq→1+ Cq = 1 and C2 = limq→2− Cq = 1. Let
W1, . . . , Wn be n independent copies of a non-negative real valued random
variable W . With probability at least 1 − 2,

n(cid:88)

i=1

1
n

Wi ≤ E(W ) +

CqE(W q)1/q
1/qn1−1/q

.

q (cid:55)→ Cq

Using this lemma, we obtain the following variant of Proposition 2.4.

March 17, 2016

Olivier Catoni

1.01.21.41.61.82.01.01.11.21.3Proposition 2.6 Consider some exponents p ∈ [1, 2] and q ∈ [1, 2] and the
bound

(cid:101)γ+ =

1

p + 1

(cid:18)2(cid:2)log(−1 + 0.73d(cid:3)

(cid:19)p/2(cid:0)1 +(cid:98)δ)p+1
(cid:34)
E(cid:0)(cid:107)G−1/2X(cid:107)2(p+1) +

(κ − 1)n

×

CqE

Under the hypotheses of Proposition 1.1 on page 7, with probability at least
1 − 4, for any θ ∈ Rd,

−(cid:98)δ + γ−
1 +(cid:98)δ
(cid:124) (cid:123)(cid:122) (cid:125)
= O((cid:98)δ)

≤ N (θ)
N (θ)

− 1 ≤

(1 −(cid:98)δ)(1 −(cid:101)γ+)+

1

− 1 ≤

17

(cid:16)(cid:107)G−1/2X(cid:107)2q(p+1)(cid:17)1/q

1/qn1−1/q

(cid:35)

.

(cid:98)δ +(cid:101)γ+
(1 −(cid:98)δ)(1 −(cid:101)γ+)+
(cid:125)
(cid:123)(cid:122)
(cid:124)
= On→∞((cid:98)δ)

,

when p>1

where it is useful to remember that

(cid:32)(cid:115)

κ(cid:2)d + log(−1)(cid:3)

(cid:33)

.

(cid:98)δ = O

n

We have done two things in this section about the empirical Gram matrix
estimate. The ﬁrst was to make a direct comparison between N and N . The
second was to introduce non-random bounds in terms of the quantities γ−,

(cid:98)γ+ and(cid:101)γ+.
estimate (cid:98)N (θ). This comes from the fact that (cid:104)θ, X(cid:105)2 being a non-negative

Remark that γ−, controlling the accuracy of N (θ) as an upper bound
for N (θ), is always of order 1/n, meaning that the empirical quadratic form
N (θ) always provides an upper bound of the same quality as the robust

random variable cannot have a long tail on the left-hand side, but only on
the right-hand side, so that its empirical mean can be too large, but not too
small.

On the other hand, δ+(θ), controlling the accuracy of N (θ) as a lower
bound for N (θ), is not necessarily a second order term in all circumstances.
When this is not case, the fact that the empirical Gram estimate may become

less accurate than the robust estimate (cid:98)N (θ) is not ruled out, and we will see

on simulations that this does happen in practice.

March 17, 2016

Olivier Catoni

3. Estimation of the covariance matrix

18

To estimate the covariance matrix

Σ = E(cid:2)(cid:0)X − E(X)(cid:1)(cid:0)X − E(X)(cid:1)(cid:62)(cid:3),
E(cid:2)(cid:0)(cid:104)θ, X(cid:105) − γ(cid:1)2(cid:3)
N (θ, γ) = E(cid:2)(cid:2)(cid:104)θ, X(cid:105) − γ(cid:1)2(cid:3)

θ(cid:62)Σθ = inf
γ∈R

one can remark that

and consider the quadratic form

that corresponds to the Gram matrix of the extended variable (X,−1) ∈
Rd+1.

Proposition 3.1 Consider an estimator (cid:98)N (θ, γ) of N (θ, γ). Let δ and  be

two positive constants. Assume that with probability at least 1 − , for any
(θ, γ) ∈ Rd+1

Deﬁne

With probability at least 1 − , for any θ ∈ Rd,

− 1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ δ.
γ∈R (cid:98)N (θ, γ)
(cid:12)(cid:12)(cid:12)(cid:12) ≤ δ.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) N (θ, γ)
(cid:98)N (θ, γ)
(cid:101)N (θ) = inf
(cid:12)(cid:12)(cid:12)(cid:12)θ(cid:62)Σθ(cid:101)N (θ)
(1 − δ)(cid:98)N (θ, γ) ≤ inf

− 1

Proof. With probability at least 1 − ,

(1 − δ)(cid:101)N (θ) = inf

γ∈R

γ∈R

N (θ, γ)

(1 + δ)(cid:98)N (θ, γ) = (1 + δ)(cid:101)N (θ).
tors (cid:98)N (θ, γ) that can be used under suitable conditions. To establish those

We can then use the previous sections to describe more precisely estima-

= θ(cid:62)Σθ ≤ inf

(cid:3)

γ

conditions, the following lemma will be helpful.

March 17, 2016

Olivier Catoni

19

Lemma 3.2 Let us put

(cid:111)
(cid:110)
E(cid:0)(cid:104)θ, X − E(X)(cid:105)4(cid:1) : θ ∈ Rd, E(cid:0)(cid:104)θ, X − E(X)(cid:105)2(cid:1) ≤ 1
(cid:110)
(cid:111)
E(cid:2)(cid:0)(cid:104)θ, X(cid:105) − ξ(cid:1)4(cid:3) : θ ∈ Rd, ξ ∈ R, E(cid:2)(cid:0)(cid:104)θ, X(cid:105) − ξ(cid:1)2(cid:3) ≤ 1

.

κ = sup
κ(cid:48) = sup

and

These two kurtosis coeﬃcients are related by the inequality

√
κ(cid:48) ≤ (

κ + 1)2.

Proof. Using successively the triangular inequality in L4(P), the deﬁnition
of κ and the Cauchy-Schwarz inequality in R2,

E(cid:0)(cid:104)θ, X − E(X)(cid:105)4(cid:1)1/4 + |(cid:104)θ, E(X)(cid:105) − ξ|(cid:17)2
E(cid:2)(cid:0)(cid:104)θ, X(cid:105) − ξ(cid:1)4(cid:3)1/2 ≤(cid:16)
κ1/4E(cid:0)(cid:104)θ, X − E(X)(cid:105)2(cid:1)1/2 + |(cid:104)θ, E(X)(cid:105) − ξ|(cid:17)2
(cid:104)
E(cid:0)(cid:104)θ, X − E(X)(cid:105)2(cid:1) +(cid:0)(cid:104)θ, E(X)(cid:105) − ξ(cid:1)2(cid:105)

≤ (κ1/2 + 1)

≤(cid:16)

= (κ1/2 + 1)E(cid:2)(cid:0)(cid:104)θ, X(cid:105) − ξ(cid:1)2(cid:3).

(cid:3)

Proposition 3.3 Let (X1, . . . , Xn) be n independent copies of a vector val-
ued random variable X ∈ Rd. Assume that for any θ ∈ Rd,

for a known constant κ ∈ R+. Let

E(cid:0)(cid:104)θ, X − E(X)(cid:105)4(cid:1) ≤ κ E(cid:0)(cid:104)θ, X − E(X)(cid:105)2(cid:1)2,
(cid:21)(cid:27)

ψ

(cid:20)(cid:0)(cid:104)θ, Xi(cid:105) − ξ(cid:1)2
(cid:26)
n(cid:88)
(cid:2)log(−1) + 0.73(d + 1)(cid:3).

− 1

i=1

λ

ρ

(cid:40)
(cid:115)

inf

ρ ∈ R∗
+ ,

√
2
(κ + 2

κ)n

(cid:98)N (θ) = inf

ξ∈R

where

λ =

(cid:41)

≤ 0

,

(cid:34)

n >

+

(cid:18)5

2

For any conﬁdence parameter  > 0, and any sample size n such that

20 (κ1/2 + 1)(d + 1)1/2

+

1

2(κ + 2κ1/2)

2 (κ + 2κ1/2)(cid:2)log(−1) + 0.73(d + 1)(cid:3)(cid:35)2
(cid:19)(cid:113)

,

(10)

March 17, 2016

Olivier Catoni

20

,

(cid:32)(cid:115)

with probability at least 1 − 2, for any θ ∈ Rd,

(cid:12)(cid:12)(cid:12)(cid:12) E(cid:0)(cid:104)θ, X − E(X)(cid:105)2(cid:1)
(cid:114)

(cid:12)(cid:12)(cid:12)(cid:12) ≤ µ

κ(cid:2)d + log(−1)(cid:3)
(cid:114)
(cid:2)log(−1) + 0.73(d + 1)(cid:3) + 6.81 (κ1/2 + 1)

where

µ =

(cid:98)N (θ)

2(κ + 2κ1/2)

1 − 2µ

− 1

= O

n

(cid:33)

,

assuming by convention that 0/0 = 1 and z/0 = +∞ for any z > 0.
The proof of this proposition is given in the appendix.

n

2(d + 1)

n

Consider now the empirical covariance estimate

n(cid:88)

n(cid:88)

i=1

j=1

Σ =

1
2n2

(Xi − Xj)(Xi − Xj)(cid:62).

(We choose a biased normalization by n2 instead of n(n − 1), because, as we
will see, we can prove a simpler non-asymptotic result for it.)

Remark that

θ(cid:62)Σ θ = inf
ξ∈R

1
n

(cid:0)(cid:104)θ, Xi(cid:105) − ξ(cid:1)2 =

n(cid:88)

i=1

n(cid:88)

(cid:68)

i=1

1
n

θ, Xi − 1
n

(cid:69)2

Xj

.

n(cid:88)

j=1

In order to use Proposition 2.2 on page 11, we need the following lemma.
Lemma 3.4 Almost surely, for any (θ, ξ) ∈ Rd+1 such that

E(cid:2)(cid:0)(cid:104)θ, X(cid:105) − ξ(cid:1)2(cid:3) ≤ 1,

(cid:104)θ, Xi(cid:105) − ξ ≤(cid:0)(cid:107)Σ−1/2(cid:0)Xi − E(X)(cid:1)(cid:107)2 + 1(cid:1)1/2.
(cid:0)(cid:13)(cid:13)Σ−1/2(cid:0)Xi − E(X)(cid:1)(cid:13)(cid:13)2 + 1(cid:1)1/2,

(cid:2)log(−1) + 0.73 (d + 1)(cid:3) + 6.81 (κ1/2 + 1)

(cid:114)

This lemma is proved in appendix.

We are now ready to apply Proposition 2.2 on page 11. Deﬁne

2(d + 1)

n

,

n

µ =

2(κ + 2κ1/2)

R = max
i=1,...,n

(cid:114)
2(cid:2)log(−1) + 0.73 (d + 1)(cid:3)

(cid:98)δ =
γ+ = γ− R4(cid:0)1 +(cid:98)δ(cid:1)2.

3(κ + 2κ1/2) n

1 − 2µ

γ− =

µ

,

,

March 17, 2016

Olivier Catoni

21

sup

Proposition 3.5 Assume that for some known constant κ ∈ R+,

(cid:110)
E(cid:0)(cid:104)θ, X − E(X)(cid:105)4(cid:1) : θ ∈ Rd, E(cid:0)(cid:104)θ, X − E(X)(cid:105)2(cid:1) ≤ 1
(cid:34)

Consider any conﬁdence parameter  > 0 and any sample size n satisfying

(cid:111) ≤ κ < ∞.

n >

20(κ1/2 + 1)

√
d + 1

(cid:19)(cid:113)
2(κ + 2κ1/2)(cid:2)log(−1) + 0.73 (d + 1)(cid:3)(cid:21)2

.

(cid:18)5

2

+

With probability at least 1 − 2, for any θ ∈ Rd,

(cid:98)δ + γ+

(1 −(cid:98)δ)(1 − γ+)+

,

+

1

2(κ + 2κ1/2)

−(cid:98)δ + γ−
1 +(cid:98)δ
≤ θ(cid:62)Σθ
(cid:124) (cid:123)(cid:122) (cid:125)
θ(cid:62)Σθ
= O((cid:98)δ)
(cid:32)(cid:115)
(cid:33)
κ(cid:2)d + log(−1)(cid:3)

n

− 1 ≤

.

where(cid:98)δ = O

Proof. Let us introduce

N (θ, ξ) =

1
n

n(cid:88)
(cid:0)(cid:104)θ, Xi(cid:105) − ξ(cid:1)2.

i=1

From proposition 2.2 on page 11, we see that on some event Ω(cid:48) of probability
at least 1 − 2, for any (θ, ξ) ∈ Rd+1,

B− def= −(cid:98)δ + γ−
1 +(cid:98)δ

≤ N (θ, ξ)
N (θ, ξ)

− 1 ≤ B+

def=

(cid:98)δ + γ+

(1 −(cid:98)δ)(1 − γ+)+

.

Remark that on Ω(cid:48),
θ(cid:62)Σθ = inf
ξ∈R
θ(cid:62)Σθ ≥ (1 − B−) inf
ξ∈R

N (θ, ξ) ≤ (1 + B+) inf
ξ∈R

N (θ, ξ) = (1 + B+) θ(cid:62)Σθ,

N (θ, ξ) = (1 − B−) θ(cid:62)Σθ.

(cid:3)

It is easy from there to give non-asymptotic bounds for the empirical term
R. The discussion is similar to the one following equation (7) on page 12,
with G replaced by Σ and X replaced by X − E(X), so that we will not
repeat it. One can also obtain results similar to Propositions 2.4 on page 15
and Proposition 2.6 on page 17.

March 17, 2016

Olivier Catoni

22

4. Least squares regression

In this section, we use our results on the estimation of the Gram matrix in
two ways. First, we derive new robust estimators for least squares regression
with a random design. Second, we obtain new results for the ordinary least
squares estimator, including its exact asymptotic rate of convergence in ex-
pectation under quite weak assumptions. In particular, it turns out that this
rate is C/n, with a constant C diﬀerent from dσ2, where σ2 is the variance
of the noise, in the case when the noise is correlated to the design.

4.1. From Gram matrix to least squares estimates. Let us ﬁrst
make a connection between Gram matrix estimates and least squares esti-
mates, that will serve to study a new robust least squares estimate as well
as the empirical risk minimizer. Consider n independent copies (Xi, Yi),
i = 1, . . . , n, of the couple of random variables (X, Y ) ∈ Rd × R and the
question of minimizing the quadratic risk

R(θ) = E(cid:2)(cid:0)Y − (cid:104)θ, X(cid:105)(cid:1)2(cid:3)

in θ ∈ Rd. Introduce the homogeneous quadratic form

θ ∈ Rd, ξ ∈ R,

and assume that ˘N (θ, ξ) is a quadratic estimator of N (θ, ξ), based on the
sample (Xi, Yi), i = 1, . . . , n, such that, for some positive constant δ, with
probability 1 − , for any (θ, ξ) ∈ Rd+1,

N (θ, ξ) = E(cid:2)(cid:0)ξY − (cid:104)θ, X(cid:105)(cid:1)2(cid:3),
(cid:12)(cid:12)(cid:12)(cid:12) ≤ δ.

(cid:12)(cid:12)(cid:12)(cid:12) ˘N (θ, ξ)

N (θ, ξ)

− 1

(11)

Hypotheses under which such an estimator exists are stated in Proposition 1.2
on page 8 for a robust estimator and in Proposition 2.3 on page 14 and 2.4
on page 15 for the empirical Gram matrix estimator. We will come back to
this in more detail afterwards.

Proposition 4.1 Under the above hypotheses, any estimator

(cid:98)θ ∈ arg min

˘N (θ, 1)

θ∈Rd
is such that with probability at least 1 − ,

R((cid:98)θ) − inf

θ∈Rd

R(θ) ≤

δ2

(1 − δ)(1 − δ2)

˘N ((cid:98)θ, 1).

March 17, 2016

Olivier Catoni

4.1 From Gram matrix to least squares estimates

23

Corollary 4.2 With probability at least 1−, for any θ∗ ∈ arg minθ∈Rd R(θ),

˘N ((cid:98)θ − θ∗, 0)

1 + δ
≤

≤ N ((cid:98)θ − θ∗, 0) = R((cid:98)θ) − R(θ∗) ≤

δ2

(1 − δ)(1 − δ2)

δ2

(1 − δ)(1 − δ2)

˘N (θ∗, 1) ≤

δ2

(1 − δ)2 R(θ∗) ≤

δ2

(1 − δ)2 R(0)

˘N ((cid:98)θ, 1)
E(cid:0)Y 2(cid:1).

δ2

=

(1 − δ)2

This gives an observable non-asymptotic conﬁdence region for θ∗, deﬁned by
the equation

˘N ((cid:98)θ − θ∗, 0) ≤

˘N ((cid:98)θ, 1).

δ2

(1 − δ)2

This also provides a simple non-asymptotic bound for the excess risk

R((cid:98)θ) − R(θ∗) ≤

δ2

(1 − δ)2 R(θ∗) ≤

δ2

(1 − δ)2

E(cid:0)Y 2(cid:1).

As a consequence, in the case when condition (11) on page 22 is satisﬁed and
Y = f (X) + W where W is independent of X, centered and E(W 2) ≤ σ2,
with probability at least 1 − ,

R((cid:98)θ) − R(θ∗) ≤

δ2

(1 − δ)2

(cid:110)
σ2 + E(cid:2)f (X)2(cid:3)(cid:111)

.

More precisely, using the Cauchy-Schwarz inequality (as well as its case

Proof. We take advantage of the fact that not only the derivatives of

θ (cid:55)→ R(θ) and θ (cid:55)→ ˘N (θ, 1) vanish at θ∗ and (cid:98)θ, respectively, but also their
symmetric ﬁnite diﬀerences R(θ+θ(cid:48))−R(θ−θ(cid:48)) and ˘N(cid:0)θ+θ(cid:48), 1(cid:1)− ˘N(cid:0)θ−θ(cid:48), 1(cid:1).
of equality), the fact that E(cid:2)(cid:0)(cid:104)θ∗, X(cid:105) − Y(cid:1)(cid:104)θ(cid:48), X(cid:105)(cid:3) = 0 for any θ(cid:48) ∈ Rd, and
R((cid:98)θ) − R(θ∗) = E(cid:0)(cid:104)(cid:98)θ − θ∗, X(cid:105)2(cid:1)

equation (11), we obtain the following chain of inequalities, that holds with
probability at least 1 − , for any (possibly random) positive number a:

(cid:110)
a−1E(cid:0)(cid:104)(cid:98)θ − θ∗, X(cid:105)(cid:104)θ(cid:48), X(cid:105)(cid:1) : θ(cid:48) ∈ Rd, E(cid:0)(cid:104)θ(cid:48), X(cid:105)2(cid:1) ≤ a2(cid:111)2
a−1E(cid:2)(cid:0)(cid:104)(cid:98)θ, X(cid:105) − Y(cid:1)(cid:104)θ(cid:48), X(cid:105)(cid:3)| θ(cid:48) ∈ Rd, E(cid:0)(cid:104)θ(cid:48), X(cid:105)2(cid:1) ≤ a2(cid:111)2
(cid:110)
(cid:16)(cid:0)(cid:104)(cid:98)θ − θ(cid:48), X(cid:105) − Y(cid:1)2(cid:17)(cid:105)
(cid:104)
(cid:110) 1
: θ(cid:48) ∈ Rd, E(cid:0)(cid:104)θ(cid:48), X(cid:105)2(cid:1) ≤ a2(cid:111)2

(cid:16)(cid:0)(cid:104)(cid:98)θ + θ(cid:48), X(cid:105) − Y(cid:1)2(cid:17) − E

4a

E

= sup

= sup

= sup

March 17, 2016

Olivier Catoni

(cid:26)

= sup

δ

2a(1 − δ2)

≤ sup

4a

1 − δ

(cid:20) ˘N(cid:0)(cid:98)θ + θ(cid:48), 1(cid:1)
(cid:26) 1
(cid:104) ˘N(cid:0)(cid:98)θ, 1(cid:1) + ˘N(cid:0)θ(cid:48), 0(cid:1)(cid:105)
˘N(cid:0)(cid:98)θ, 1(cid:1)

(cid:115)

=

(cid:21)

− ˘N(cid:0)(cid:98)θ − θ(cid:48), 1(cid:1)
: θ(cid:48) ∈ Rd, ˘N(cid:0)θ(cid:48), 0(cid:1) ≤ a2(1 + δ)

: θ(cid:48) ∈ Rd, ˘N (θ(cid:48), 0) ≤ a2(1 + δ)

1 + δ

(cid:20) ˘N(cid:0)(cid:98)θ, 1(cid:1)

a

δ2

4(1 − δ2)2

+ a(1 + δ)

24

(cid:27)2
(cid:27)2
(cid:21)2

4.2 A robust least squares estimator

Taking the optimal value a =
probability at least 1 − ,

1 + δ

R((cid:98)θ) − R(θ∗) ≤

δ2

(1 − δ)(1 − δ2)

˘N(cid:0)(cid:98)θ, 1(cid:1).

, we obtain as desired that with

The statements made in the corollary are obvious consequences of this in-
equality and the deﬁnitions. Note that this line of proof would also work
(with the necessary modiﬁcations) in the case when (Xi, Yi) are independent
couples of random variables that are not necessarily identically distributed.
(cid:3)

(cid:110)

(cid:104)(cid:0)ξY − (cid:104)θ, X(cid:105)(cid:1)4(cid:105)

sup

E

4.2. A robust least squares estimator. To draw the consequences of
Proposition 4.1, we have to explain when the required hypothesis, expressed
by equation (11) on page 22, is satisﬁed.

For this, we will apply Proposition 1.2 on page 8, assuming that

(12)

: θ ∈ Rd, ξ ∈ R,

E

(cid:104)(cid:0)ξY − (cid:104)θ, X(cid:105)(cid:1)2(cid:105) ≤ 1

(cid:111) ≤ κ < ∞.
(cid:16)(cid:113)
(cid:17)
κ(cid:2)d + log(−1)(cid:3)/n

When this is satisﬁed, there is a robust estimator satisfying condition (11)
with δ = 2µ/(1 − 4µ), where µ is as in Proposition 1.1 on page 7 with d
replaced by d + 1, and is therefore of order O

.

Condition (12) is not very explicit, since it bears on the joint distri-
It may be more instructive to replace it by sepa-
bution of X and Y .
rate kurtosis assumptions bearing on Y − (cid:104)θ∗, X(cid:105) and on (cid:104)θ, X(cid:105), where

θ∗ ∈ arg minθ∈Rd E(cid:2)(cid:0)Y − (cid:104)θ, X(cid:105)(cid:1)2(cid:3). This is what the following lemma does.

March 17, 2016

Olivier Catoni

4.2 A robust least squares estimator

Lemma 4.3 Let us deﬁne

(cid:110)
(cid:111)
E(cid:2)(cid:0)ξY − (cid:104)θ, X(cid:105)(cid:1)4(cid:3) : θ ∈ Rd, ξ ∈ R, E(cid:2)(cid:0)ξY − (cid:104)θ, X(cid:105)(cid:1)2(cid:3) ≤ 1
(cid:110)
(cid:111)
E(cid:0)(cid:104)θ, X(cid:105)4(cid:1) : θ ∈ Rd, E(cid:0)(cid:104)θ, X(cid:105)2(cid:1) ≤ 1
 E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)4(cid:3)
E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3)2 , E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3) > 0,

,

otherwise.

0,

κ = sup

κ1 = sup

κ2 =

25

,

κ2.

√

κ1 +

Those three kurtosis coeﬃcients are linked together by the relation

Proof. Using the triangular inequality in L4, followed by the deﬁnitions of
κ1 and κ2 and the Cauchy-Schwarz inequality in R2, we see that

√
κ ≤ √
(cid:110)(cid:2)ξ(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1) + (cid:104)ξθ∗ − θ, X(cid:105)(cid:3)4(cid:111)1/2
E(cid:0)(cid:104)ξθ∗ − θ, X(cid:105)2(cid:1)1/2(cid:111)2
E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3)1/2 + κ1/4
(cid:110)
ξ2E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3) + E(cid:0)(cid:104)ξθ∗ − θ, X(cid:105)2(cid:1)(cid:111)

E(cid:2)(cid:0)ξY − (cid:104)θ, X(cid:105)(cid:1)4(cid:3)1/2 = E
≤(cid:110)|ξ|E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)4(cid:3)1/4 + E(cid:0)(cid:104)ξθ∗ − θ, X(cid:105)4(cid:1)1/4(cid:111)2
≤(cid:110)|ξ|κ1/4

≤ (κ1/2

1 + κ1/2
2 )

2

1

2 )E(cid:2)(cid:0)ξY − (cid:104)θ, X(cid:105)(cid:1)2(cid:3).

= (κ1/2

1 + κ1/2

(cid:3)
As a consequence of this lemma and of Proposition 1.2 on page 8

Proposition 4.4 Consider the bound

(cid:115)

2(cid:2)(cid:0)√

√

κ2

κ1 +

µ =

(cid:1)2 − 1(cid:3)(cid:2)log(−1) + 0.73 (d + 1)(cid:3)
+ 6.81(cid:0)√

n

κ1 +

(cid:1)(cid:114)

√

κ2

2(d + 1)

n

.

There exists a robust quadratic estimator ˘N (θ, ξ) such that for any estimator

(cid:98)θ ∈ arg min

θ∈Rd

˘N (θ, 1),

for any  > 0 and n ∈ N satisfying

March 17, 2016

Olivier Catoni

4.3 Generalization bounds for the empirical risk minimizer

n >

d + 1 +

(cid:34)

(cid:1)√

√
κ2

20(cid:0)√
(cid:18)
2(cid:2)(cid:0)√

κ1 +

×

√
κ2

κ1 +

2

+

2(cid:2)(cid:0)√

(cid:18)5
(cid:1)2 − 1(cid:3)(cid:19)
(cid:1)2 − 1(cid:3)(cid:2)log(−1) + 0.73 (d + 1)(cid:3)(cid:19)1/2(cid:35)2
(κ1 + κ2)(cid:2)d + log(−1)(cid:3)(cid:17)

(cid:16)

κ1 +

= O

1
√

κ2

26

,

with probability at least 1 − 2,

R((cid:98)θ) − R(θ∗) ≤

where

(cid:32)

δ2 =

(cid:33)2

2µ
1 − 4µ

= O

δ2

(1 − δ)2 R(θ∗),

(cid:18)(κ1 + κ2)(cid:2)d + log(−1)(cid:3)

(cid:19)

.

n

4.3. Generalization bounds for the empirical risk minimizer. Let
us now examine the conditions under which equation (11) on page 22 is sat-
isﬁed by the empirical Gram matrix estimator

n(cid:88)
(cid:0)ξYi − (cid:104)θ, Xi(cid:105)(cid:1)2.

˘N (θ, ξ) = N (θ, ξ) =

1
n

i=1

To apply Proposition 2.2 on page 11, we have to bound

(cid:110)
(cid:111)
ξYi −(cid:104)θ, Xi(cid:105) : (θ, ξ) ∈ Rd+1, E(cid:2)(cid:0)ξY −(cid:104)θ, X(cid:105)(cid:1)2(cid:3) ≤ 1

R = max
i=1,...,n

sup

. (13)

Lemma 4.5 The above deﬁned quantity satisﬁes almost surely

(cid:26) (cid:0)Yi − (cid:104)θ∗, Xi(cid:105)(cid:1)2
(cid:27)
E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3) +(cid:13)(cid:13)G−1/2Xi(cid:107)2
(cid:0)Yi − (cid:104)θ∗, Xi(cid:105)(cid:1)2
(cid:13)(cid:13)G−1/2Xi(cid:107)2,
E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3) + max

i=1,...,n

,

R2 ≤ max

i=1,...,n

≤ max

i=1,...,n

with the convention that 0/0 = 0.

Proof. Remark that for any positive constants a and b,

March 17, 2016

Olivier Catoni

4.3 Generalization bounds for the empirical risk minimizer

(cid:0)ξYi − (cid:104)θ, Xi(cid:105)(cid:1)2 =(cid:2)ξ(cid:0)Yi − (cid:104)θ∗, Xi(cid:105)(cid:1)a−1a + (cid:104)ξθ∗ − θ, Xi(cid:105)b−1b(cid:3)2
≤(cid:2)ξ2(cid:0)Yi − (cid:104)θ∗, Xi(cid:105)(cid:1)2a−2 + (cid:104)ξθ∗ − θ, Xi(cid:105)2b−2(cid:3)(cid:0)a2 + b2(cid:1).
Now take a2 = ξ2E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3) and b2 = E(cid:0)(cid:104)ξθ∗ − θ, X(cid:105)2(cid:1), and remark
a2 + b2 = E(cid:2)(cid:0)ξY − (cid:104)θ, X(cid:105)(cid:1)2(cid:3).

that

27

Notice also that in the case when b2 > 0,

(cid:104)ξθ∗ − θ, Xi(cid:105)2b−2 ≤ (cid:107)G1/2(ξθ∗ − θ)(cid:107)2(cid:107)G−1/2Xi(cid:107)2b−2 = (cid:107)G−1/2Xi(cid:107)2.

Consequently, when a2 > 0 and b2 > 0,

(cid:0)ξYi − (cid:104)θ, Xi(cid:105)(cid:1)2 ≤

(cid:18) (cid:0)Yi − (cid:104)θ∗, Xi(cid:105)(cid:1)2
E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3)

(cid:19)

E(cid:2)(cid:0)ξY − (cid:104)θ, X(cid:105)(cid:1)2(cid:3).

+ (cid:107)G−1/2Xi(cid:107)2

(14)
In the case when a2 = 0, Yi−(cid:104)θ∗, Xi(cid:105) = 0 almost surely, and in the case when
b = 0, (cid:104)ξθ∗ − θ, Xi(cid:105) = 0 almost surely, so that in those cases, equation (14) is
still satisﬁed, and the desired result is an easy consequence of this inequality.
(cid:3)

are that E(cid:2)(Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3) > 0 and for positive constants p, q ∈]0, 1], αi and

In view of the above lemma, suitable hypotheses to obtain a bound for R

Under those hypotheses, with probability at least 1 − 2,

ηi, i = 1, 2,

and E

(cid:26)

exp

(cid:18)

R2 ≤

(15)

(16)

2

E

exp

2

≤ 1.

(cid:17)(cid:105)(cid:111) ≤ 1
(cid:16)(cid:107)G−1/2X(cid:107)2p − dp − η1
(cid:110)
(cid:104) α1
(cid:18) (Y − (cid:104)θ∗, X(cid:105))2q
(cid:20)α2
(cid:19)(cid:21)(cid:27)
E(cid:2)(Y − (cid:104)θ∗, X(cid:105))2(cid:3)q − 1 − η2
log(cid:0)n/(cid:1)(cid:19)1/q
(cid:18)
log(cid:0)n/(cid:1)(cid:19)1/p
(cid:18)

(cid:19)1/p

1 + η2 +

2
α2

2
α1

dp + η1 +

log(n/)

.

R2 ≤

2
α1

dp + η1 +

In the case when E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3) = 0, it is easy to see that when (15) is

+

.

satisﬁed, with probability at least 1 − 

Applying Proposition 2.2 on page 11 and Proposition 4.1 on page 22 and its
corollary, we obtain

March 17, 2016

Olivier Catoni

4.3 Generalization bounds for the empirical risk minimizer

28

Proposition 4.6 Assume that the hypotheses expressed by equations (15)
and (16) are satisﬁed. Consider

n

µ

κ2

κ2

√

√

µ =

κ1 +

κ1 +

2(d + 1)

(cid:18)2(cid:2)(cid:0)√
(cid:1)2 − 1(cid:3)
(cid:2)log(−1) + 0.73 (d + 1)(cid:3)(cid:19)1/2
(cid:1)(cid:114)
+ 6.81(cid:0)√
2(cid:2)log(−1) + 0.73 (d + 1)(cid:3)
κ2)2 − 1(cid:3) n
3(cid:2)(
2(cid:2)log(−1) + 0.73 (d + 1)(cid:3)(1 +(cid:98)δ)2
κ2)2 − 1(cid:3) n
3(cid:2)(
(cid:20)(cid:16)
(cid:17)2/p

(cid:98)δ =
γ+ = γ1R4(cid:0)1 +(cid:98)δ(cid:1)2,
(cid:98)γ+ =

1 − 2µ
√
κ1 +

γ− =

(cid:16)

κ1 +

√

√

√

n

,

,

,

×

dp + η1 + 2α−1

1

log(n/)

+

1 + η2 + 2α−1

2

(cid:17)2/q(cid:21)2

log(n/)

,

where κ1 and κ2 are deﬁned in Lemma 4.3 on page 25 and R is deﬁned in
(13) on page 26. Consider any conﬁdence parameter  > 0 and any sample
size n such that

n >

κ2

d + 1 +

(cid:1)√

(cid:34)

20(cid:0)√
(cid:18)

×

√

κ1 +

2(cid:2)(cid:0)√

√
κ2

κ1 +

2

+

2(cid:2)(cid:0)√

(cid:1)2 − 1(cid:3)(cid:19)
(cid:18)5
(cid:1)2 − 1(cid:3)(cid:2)log(−1) + 0.73 (d + 1)(cid:3)(cid:19)1/2(cid:35)2
(κ1 + κ2)(cid:2)d + log(−1)(cid:3)(cid:17)

1
√
κ2

(cid:16)

κ1 +

= O

.

With probability at least 1 − 4, for any θ ∈ Rd, any ξ ∈ R,

−(cid:98)δ + γ−
1 +(cid:98)δ
(cid:12)(cid:12)(cid:12)(cid:12) ≤
(cid:12)(cid:12)(cid:12)(cid:12)N (θ, ξ)

N (θ, ξ)

− 1

so that in particular

≤ N (θ, ξ)
N (θ, ξ)

− 1 ≤

(cid:98)δ +(cid:98)γ+

(1 −(cid:98)δ)+(1 −(cid:98)γ+)+

(cid:98)δ + γ+

(1 −(cid:98)δ)+(1 − γ+)+

(cid:98)δ +(cid:98)γ+

≤

(1 −(cid:98)δ)+(1 −(cid:98)γ+)+
(cid:32)(cid:115)(cid:0)κ1 + κ2(cid:1)(cid:2)d + log(−1)(cid:3)

= On→∞

n

(cid:33)

.

March 17, 2016

Olivier Catoni

4.3 Generalization bounds for the empirical risk minimizer

29

As a consequence, the empirical risk minimizer

(cid:98)θ ∈ arg min

n(cid:88)
(cid:0)Yi − (cid:104)θ, Xi(cid:105)(cid:1)2

i=1

is such that with probability at least 1 − 4,

E(cid:2)(cid:0)Y − (cid:104)(cid:98)θ, X(cid:105)(cid:1)2(cid:3) − E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3) ≤

n(cid:88)
(cid:0)Yi − (cid:104)(cid:98)θ, Xi(cid:105)(cid:1)2
E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3),

i=1

δ2

(1 − δ)(1 − δ2)n

≤

δ2

(1 − δ)2

where

(cid:33)2

(cid:32)

(cid:98)δ +(cid:98)γ+
(1 −(cid:98)δ)(1 −(cid:98)γ+)

δ2 =

(cid:18)(κ1 + κ2)(cid:2)d + log(−1)(cid:3)

n

(cid:19)

.

= On→∞

We can also apply Proposition 2.4 on page 15 and work under polynomial
moment assumptions. Consider the Gram matrix

(cid:19)(cid:0)X(cid:62),−Y(cid:1)(cid:21)

.

(cid:20)(cid:18) X−Y
(cid:101)G = E
(cid:26)(cid:0)ξY − (cid:104)θ, X(cid:105)(cid:1)2

= sup

(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:27)
: (θ, ξ) ∈ Rd+1, E(cid:2)(cid:0)ξY − (cid:104)θ, X(cid:105)(cid:1)2(cid:3) ≤ 1
E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3) + (cid:107)G−1/2X(cid:107)2,
≤ (Y − (cid:104)θ∗, X(cid:105))2

Remark that

(cid:13)(cid:13)(cid:13)(cid:13)(cid:101)G−1/2

(cid:18) X−Y

according to equation (14) on page 27. Therefore, according to the Minkowski
inequality in Lp+1,

(cid:32)(cid:13)(cid:13)(cid:13)(cid:13)(cid:101)G−1/2

(cid:18) X−Y

(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)2p+2(cid:33)

E

(cid:32) E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2p+2(cid:3)1/(p+1)
E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3)

≤

+ E(cid:0)(cid:107)G−1/2X(cid:107)2p+2(cid:1)1/(p+1)

(cid:33)p+1

.

March 17, 2016

Olivier Catoni

4.3 Generalization bounds for the empirical risk minimizer

30

In the same way, with a change of notation,

(cid:32)(cid:13)(cid:13)(cid:13)(cid:13)(cid:101)G−1/2

(cid:18) X−Y

(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)4p+4(cid:33)

E

≤

(cid:32) E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)4p+4(cid:3)1/(2p+2)
E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3)
+ E(cid:0)(cid:107)G−1/2X(cid:107)4p+4(cid:1)1/(2p+2)

(cid:33)2p+2

.

Proposition 4.7 Consider some exponent p ∈]1, 2]. Consider the same
hypotheses as in Proposition 4.6 on page 28, except that instead of conditions
(15) and (16) on page 27 we assume now that

(cid:19)p/2
(cid:33)p+1
+ E(cid:0)(cid:107)G−1/2X(cid:107)2p+2(cid:1)1/(p+1)
(cid:33)p+1(cid:35)
+ E(cid:0)(cid:107)G−1/2X(cid:107)4p+4(cid:1)1/(2p+2)

,

√

κ1 +

√

×

1

p + 1

Deﬁne

(cid:101)γ+ =

E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)4p+4(cid:3) < ∞,
and E(cid:0)(cid:107)G−1/2X(cid:107)4p+4(cid:1) < ∞.
(cid:18)2(cid:2)log(−1) + 0.73 (d + 1)(cid:3)
(1 +(cid:98)δ)p+1
3(cid:2)(
κ2)2 − 1(cid:1)n
(cid:34)(cid:32) E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2p+2(cid:3)1/(p+1)
E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3)
(cid:32) E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)4p+4(cid:3)1/(2p+2)
E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) N (θ, ξ)
E(cid:2)(cid:0)Y − (cid:104)(cid:98)θ, X(cid:105)(cid:1)2(cid:3) − E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3) ≤

(1 −(cid:98)δ)+(1 −(cid:101)γ+)+

(cid:98)δ +(cid:101)γ+

+

1√
n

N (θ, ξ)

− 1

,

where κ1 and κ2 are as in Lemma 2.5 on page 16. Under the same condition
on n and  as in Proposition 4.6, with probability at least 1 − 3, for any
(θ, ξ) ∈ Rd+1,

so that in particular the empirical risk minimizer is such that with probability
at least 1 − 3,

n(cid:88)
(cid:0)Yi − (cid:104)(cid:98)θ, Xi(cid:105)(cid:1)2
E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3),

i=1

δ2

(1 − δ)(1 − δ2)n

≤

δ2

(1 − δ)2

March 17, 2016

Olivier Catoni

4.3 Generalization bounds for the empirical risk minimizer

31

where

δ2 =

(cid:18)

(cid:98)δ +(cid:101)γ+
(1 −(cid:98)δ)(1 −(cid:101)γ+)

(cid:19)2

= On→∞(cid:0)(cid:98)δ2(cid:1)

= On→∞

(cid:18)(κ1 + κ2)(cid:2)d + log(−1)(cid:3)

(cid:19)

n

.

We can also apply Proposition 2.6 on page 17 to get weaker moment
assumptions at the price of a worse non-asymptotic bound, but still with the
same leading term when the sample size n goes to inﬁnity.
Proposition 4.8 Consider some exponents p ∈]1, 2] and q ∈]1, 2[. Make
the same hypotheses as in Proposition 4.6 on page 28, except that instead of
conditions (15) and (16) on page 27, we assume that

E(cid:2)(Y − (cid:104)θ∗, X(cid:105)(cid:1)2q(p+1)(cid:3) < ∞ and E(cid:0)(cid:107)G−1/2X(cid:107)2q(p+1)(cid:1) < ∞.

Deﬁne

(cid:101)γ+ =

×

1

(cid:18)2(cid:2)log(−1) + 0.73(d + 1)(cid:3)
(cid:2)(
(cid:34)(cid:32) E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2p+2(cid:3)1/(p+1)
E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3)

√
κ1 +

√

p + 1

κ2)2 − 1)n

(cid:19)p/2(cid:0)1 +(cid:98)δ(cid:1)p+1
+ E(cid:0)(cid:107)G−1/2X(cid:107)2p+2(cid:1)1/(p+1)
(cid:32) E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2q(p+1)(cid:3)q−1(p+1)−1
E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3)
(cid:16)(cid:107)G−1/2X(cid:107)2q(p+1)(cid:17)q−1(p+1)−1(cid:33)p+1(cid:35)

(cid:33)p+1

+ E

,

+

Cq

1/qn1−1/q

where the constant Cq is deﬁned in Lemma 2.5 on page 16 and κ1 and κ2
are deﬁned in Lemma 4.3 on page 25. Under the same conditions on n and
 as in Proposition 4.6 on page 28, with probability at least 1 − 4, for any
(θ, ξ) ∈ Rd+1,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) N (θ, ξ)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤
E(cid:2)(cid:0)Y − (cid:104)(cid:98)θ, X(cid:105)(cid:1)2(cid:3) − E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3)

N (θ, ξ)

− 1

(cid:98)δ +(cid:101)γ+

(1 −(cid:98)δ)+(1 −(cid:101)γ+)+

,

so that the empirical risk minimizer is such that with probability at least
1 − 4,

March 17, 2016

Olivier Catoni

4.3 Generalization bounds for the empirical risk minimizer

32

≤

δ2

(1 − δ)+(1 − δ2)+n

n(cid:88)

i=1

(cid:0)Yi − (cid:104)(cid:98)θ, Xi(cid:105)(cid:1)2

E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3),

≤

δ2

(1 − δ)2

(cid:32)

where

δ2 =

(cid:98)δ +(cid:101)γ+

(1 −(cid:98)δ)+(1 −(cid:101)γ+)+

(cid:33)2

= On→∞(cid:0)(cid:98)δ2(cid:1)

= On→∞

(cid:32)

(κ1 + κ2)(cid:2)d + log(−1)(cid:3)

n

(cid:33)

.

Remark that in these last two propositions, the hypotheses on the design
X and the noise Y − (cid:104)θ∗, X(cid:105) are weaker than in Proposition 4.6, since they
√
involve only polynomial moment assumptions. As a counterpart, the depen-
dence in the conﬁdence parameter  is worse, since we have a factor 1/
n in
Proposition 4.7 and −1/qn−(1−1/q) in Proposition 4.8 that precludes the use
of a conﬁdence level 1 − 3 (resp. 1 − 4) much higher than 1 − 1/n (resp.
√
1 − n−(q−1)). Nevertheless, this factor 1/
n (resp. −1/qn−(1−1/q)) appears
only in second order terms, so that we still get an asymptotic upper bound
(expressed here in big O notation) where the dependence on the conﬁdence
parameter is proportional to log(−1).

The results in this section are stronger than those obtained in [5].

In
particular, for the robust estimator, we are not limited to considering the
minimization in θ in a bounded subset Θ of Rd, as in Theorem 3.1 of [5],
where some of the constants involved in the bound depend on the fact that Θ
is bounded. Also, we provide non-asymptotic bounds for the empirical risk
minimizer, whereas in [5] the corresponding result, Theorem 2.1, is asymp-
totic, since it is satisﬁed only for large enough sample sizes n, without an
explicit condition on n. Moreover the constant B appearing in Theorem 2.2
of [5] is in fact necessarily dependent on the dimension and more precisely
not smaller than d, as mentioned to us by Guillaume L´ecu´e, whom we are
grateful for pointing out this mistake (the comments we made after Theorem
2.2 of [5] about the size of the constant B are in fact false, we apologize for
this error).

It is also interesting to compare our results with those of [15]. In par-
ticular, it is relevant to compare Proposition 4.8 above with Theorem 1.3 of
[15]. Our hypotheses are only slightly stronger than those of [15], since we
require that (cid:107)G−1/2X(cid:107) belongs to a little more than L4, namely to L2q(p+1),
where q > 1 and p > 1 can be taken arbitrarily close to 1, so that 2q(p + 1)

March 17, 2016

Olivier Catoni

4.3 Generalization bounds for the empirical risk minimizer

33

can also be made as close to 4 as desired, if one is willing to accept a larger

second order term(cid:101)γ+. Indeed, the coeﬃcient θ0 appearing in [15] is related

. Moreover, as proved

to the kurtosis coeﬃcient κ1 by the relation θ0 = κ1/4
in Lemma A.7 on page 51

1

E(cid:0)(cid:107)G−1/2X(cid:107)4(cid:1) ≤ dκ1,

so that when κ1 (or θ0) is ﬁnite, (cid:107)G−1/2X(cid:107) belongs to L4. So we ask for a
little higher moment, and we get as a reward a better bound, since our bound
still has a subexponential ﬁrst order term (meaning that the dependence in
the conﬁdence parameter  is in log(−1)), whereas the bound obtained in
[15] is

(cid:34)

E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3)(cid:35)

,

On→∞

dκ3

1κ1/2
2
n

bound, we have to assume that E(cid:0)(cid:107)G−1/2X(cid:107)15/2(cid:1) < ∞, but we can take

when cast into our notation. For instance, if we choose p = q = 3/2 in our

 = n−1/2 and still have a second order term in n−3/2, whereas the ﬁrst order
term is in n−1. As a comparison, if we choose  = n−1/2, the bound from [15]
is no more in n−1, but in n−1/2.

We would like also to remark that in some situations, κ1, that is equal to
θ4
0 in [15], may in fact depend on d. This is for instance the case in uniform
histogram regression, where

X =(cid:0)Xk, k = 1, . . . d(cid:1) =(cid:2)gk(U ), k = 1, . . . , d(cid:3)

(17)

gk(u) = 1(cid:2) (k − 1)/d ≤ u < k/d(cid:3).

the random variable U being distributed according to the uniform distribu-
tion in the unit interval [0, 1] and the functions gk being deﬁned as

In this case E(cid:2)gk(U )p(cid:3) = 1/d for all values of the exponent p, so that neces-
sarily κ1 ≥ E(cid:2)gk(U )4(cid:3)/E(cid:2)gk(U )2(cid:3)2 = d (in fact, it is easy to see that κ1 = d
(cid:80)d

in this case). The same unfavourable scaling appears in all local bases of
the wavelet type.
Indeed, in the context of functional regression of Y by
k=1 θkgk(U ), where U is uniform in the unit interval, rescaling a regression
function g by a scale factor λ will impact its L4/L2 ratio according to the
formula

(18)

E(cid:2)g(λU )4(cid:3)
E(cid:2)g(λU )2(cid:3)2 = λ

E(cid:2)g(U )4(cid:3)
E(cid:2)g(U )2(cid:3)2 .

March 17, 2016

Olivier Catoni

4.4 Some lower bound for the empirical risk minimizer

34

4.4. Some lower bound for the empirical risk minimizer. We will
show in this section that the order of magnitude of the previous upper bound
cannot be improved in the worst case, except for the values of the numerical
constants.

Proposition 4.9 Consider a sample ((cid:101)X1,(cid:101)Y1), . . . , ((cid:101)Xn,(cid:101)Yn) made of n inde-
pendent copies of the couple of random variables ((cid:101)X,(cid:101)Y ) ∈ Rd × R. Assume

that for some δ ∈]0, 1[, some  ∈]0, 1/2[ and some n ∈ N, for any n ≥ n,
with probability at least 1 − 2, for any θ ∈ Rd,

where by convention 0/0 = 1 and z/0 = ∞ for z > 0. Assume that

(19)

η|(cid:101)X = N(0, σ2), with σ > 0, so that the noise η is a Gaussian noise

n

where P

(cid:12)(cid:12)(cid:12)(cid:12) 1

independent from (cid:101)X. Assume also that

(cid:80)n
(cid:12)(cid:12)(cid:12)(cid:12) ≤ δ,
i=1(cid:104)θ, (cid:101)Xi(cid:105)2
E(cid:0)(cid:104)θ, (cid:101)X(cid:105)2(cid:1) − 1
(cid:101)Y = (cid:104)θ∗, (cid:101)X(cid:105) + η,
G = E(cid:0)(cid:101)X(cid:101)X(cid:62)(cid:1)
n(cid:88)
(cid:0)(cid:101)Yi − (cid:104)θ, (cid:101)Xi(cid:105)(cid:1)2.
(cid:80)n
i=1 (cid:101)Xi(cid:101)X(cid:62)
E(cid:2)(cid:0)(cid:101)Y − (cid:104)(cid:101)θ, (cid:101)X(cid:105)(cid:1)2(cid:3) − E(cid:2)(cid:0)(cid:101)Y − (cid:104)θ∗, (cid:101)X(cid:105)(cid:1)2(cid:3) ≥
dent from ((cid:101)X,(cid:101)Y ), so that

For any n ≥ n, with probability at least 1 − 3,

(There may be more than one when 1
n

(cid:101)θ ∈ arg min

θ∈Rd

i=1

is of full rank d. Consider any empirical risk minimizer

i

is not of full rank.)

(cid:2)d log(2) − 2 log(−1)(cid:3)σ2

(1 + δ) n

.

Consider now a Bernoulli random variable ξ of parameter p ∈]0, 1], indepen-

P(cid:0)ξ = 1| ((cid:101)X,(cid:101)Y )(cid:1) = 1 − P(cid:0)ξ = 0| ((cid:101)X,(cid:101)Y )(cid:1) = p.

Deﬁne the censored couple of random variables

(cid:0)X, Y(cid:1) =(cid:0)ξ(cid:101)X, ξ(cid:101)Y(cid:1).

March 17, 2016

Olivier Catoni

4.4 Some lower bound for the empirical risk minimizer

35

Deﬁne κ1 and κ2 as in Lemma 4.3 on page 25. Deﬁne in the same way

Consider a sample (X1, Y1), . . . , (Xn, Yn) made of n independent copies of
(X, Y ). Consider any empirical risk minimizer of the censored data

(cid:101)κ1 = sup
(cid:101)κ2 =

,

i=1

θ∈Rd

n(cid:88)
(cid:0)Yi − (cid:104)θ, Xi(cid:105)(cid:1)2.

(cid:98)θ ∈ arg min
(cid:111)
(cid:110)
E(cid:0)(cid:104)θ, (cid:101)X(cid:105)4(cid:1) : θ ∈ Rd, E(cid:0)(cid:104)θ, (cid:101)X(cid:105)2(cid:1) ≤ 1
E(cid:2)(cid:0)(cid:101)Y − (cid:104)θ∗, (cid:101)X(cid:105)(cid:1)4(cid:3)
E(cid:2)(cid:0)(cid:101)Y − (cid:104)θ∗, (cid:101)X(cid:105)(cid:1)2(cid:3)2 = 3 (since η is Gaussian).
n ≥ 4(κ1 + κ2) max(cid:8)n, 2 log(−1)(cid:9)
≥ 4(cid:2)d log(2) − 2 log(−1)(cid:3)(κ1 + κ2)

with probability at least 1 − 5,

(cid:101)κ1 +(cid:101)κ2
E(cid:2)(cid:0)Y − (cid:104)(cid:98)θ, X(cid:105)(cid:1)2(cid:3) − E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3)
7(1 + δ)((cid:101)κ1 +(cid:101)κ2) n

Remark that κ1 = (cid:101)κ1/p and κ2 = (cid:101)κ2/p, so that in particular p−1 = (κ1 +
κ2)/((cid:101)κ1 +(cid:101)κ2). For any

E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3).

(20)

Therefore, the ﬁrst order term dependence in κ1 + κ2 in Propositions 4.7 on
page 30 and 4.8 cannot be improved without further assumptions, since by
varying the value of p−1, we can make κ1 + κ2 arbitrarily large in this lower
bound.

In the case when (cid:101)X is a Gaussian vector, (cid:101)κ1 +(cid:101)κ2 = 6 and we can take

n = 8.4(cid:2)0.73 d + log(−1)(cid:3)(cid:2)1.4 d + 4 log(−1) + 26.3(cid:3)2.

for example

When n ≥ n, we can deduce from Proposition 2.3 on page 14 that equation
(19) on page 34 holds with δ = 8/9.

So in the case of a censored Gaussian design, for any n ≥ 2
3

(κ1 + κ2)n,

with probability at least 1 − 5,

E(cid:2)(cid:0)Y − (cid:104)(cid:98)θ, X(cid:105)(cid:1)2(cid:3) − E(cid:2)(cid:0)Y − (cid:104)θ∗.X(cid:105)(cid:1)2(cid:3)

(cid:2)d − 3 log(−1)(cid:3)(κ1 + κ2)

n

E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3).

≥ 35
1000

×

March 17, 2016

Olivier Catoni

4.5 Exact convergence rate for the empirical risk minimizer

36

Note that the result given in the end of this proposition is not the best
possible and could be improved by using PAC-Bayes bounds speciﬁc to the
Gaussian case, resulting in a smaller n with a better dependence in the
dimension d. However, since we will give another lower bound further on,
we thought it wiser not to delve into these technicalities. The proof of this
proposition is given in the appendix.

4.5. Exact convergence rate for the empirical risk minimizer.
The results of the previous section show that the convergence speed of the
empirical risk mimimizer cannot be On→n
in the worst case. In
this section, we clarify the situation by showing under mild conditions that

where we recall that R(θ) = E(cid:2)(cid:0)Y − (cid:104)θ, X(cid:105)(cid:1)2(cid:3) and where the exact constant

n→∞ C/n,

E

(cid:16)

R(θ∗)d/n

(cid:17)
(cid:104)
min(cid:8)R((cid:98)θ) − R(θ∗), C exp(cid:0)n2−q(cid:1)(cid:9)(cid:105) ∼
(cid:104)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:13)(cid:13)G−1/2X(cid:13)(cid:13)2(cid:105)

C = E

.

C is given by

The form of this constant shows that we get a R(θ∗) d/n convergence rate
when the noise Y − (cid:104)θ∗, X(cid:105) is independent from X (and G is of full rank),
and that it can be larger or smaller otherwise.

κ = sup

and that

C = E

where

< ∞

inverse. Assume that

Proposition 4.10 Consider a sample (X1, Y1), . . . , (Xn, Yn) made of n in-
dependent copies of the couple of random variables (X, Y ) ∈ Rd × R. Let

G = E(cid:0)XX(cid:62)(cid:1) be the Gram matrix of the design X and G−1 its pseudo-

< ∞,

(cid:110)
(cid:111)
E(cid:0)(cid:104)θ, X(cid:105)4(cid:1) : θ ∈ Rd, E(cid:0)(cid:104)θ, X(cid:105)2(cid:1) ≤ 1
(cid:104)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:13)(cid:13)G−1/2X(cid:13)(cid:13)2(cid:105)
E(cid:2)(cid:0)Y − (cid:104)θ, X(cid:105)(cid:1)2(cid:3)
n(cid:88)
(cid:0)Yi − (cid:104)θ, Xi(cid:105)(cid:1)2.

θ∗ ∈ arg min
θ∈Rd

risk minimizer

is some optimal regression parameter. Deﬁne µ as in Proposition 1.1 on

page 7 and γ− and(cid:98)δ as in Proposition 2.2 on page 11. Consider any empirical
For any n ≥ n = O(cid:0)κ(cid:2)d + log(−1)(cid:3)(cid:1) given by equation (3) on page 7, there

is an event Ω of probability at least 1− , such that the excess risk conditional
to Ω satisﬁes

(cid:98)θ ∈ arg min

θ∈Rd

i=1

March 17, 2016

Olivier Catoni

4.5 Exact convergence rate for the empirical risk minimizer

37

(cid:104)
(cid:105) ≤
R((cid:98)θ ) − R(θ∗)(cid:12)(cid:12) Ω

n
C

E

(1 +(cid:98)δ)2
(cid:32)

(1 − γ−)2 P(Ω)
≤

1 + O

(cid:32)(cid:115)

κ(cid:2)d + log(cid:0)−1(cid:1)(cid:3)

n

(cid:33)(cid:33)

(1 − )−1.

Consequently, for any  and n satisfying equation (3) on page 7, for any
positive constant M ∈ R+,

(cid:104)

min(cid:8)R((cid:98)θ ) − R(θ∗), M(cid:9)(cid:105) ≤ (1 +(cid:98)δ)2 C

E

(1 − γ−)2 n

+ M.

E

O

κ

(cid:16)

(cid:32)

, and that

Taking  =

(cid:32)(cid:115)

κ(cid:2)d + log(cid:0)M n/C(cid:1)(cid:3)

C
M n2 , we see that equation (3) on page 7 is satisﬁed for n >

(cid:104)
d + log(cid:2)κM/C(cid:3)(cid:105)(cid:17)
min(cid:8)R((cid:98)θ) − R(θ∗), M(cid:9)(cid:105) ≤
(cid:104)
κ(cid:2)d + log(cid:0)κM/σ2(cid:1)(cid:3)(cid:17)
(cid:16)
(cid:32)

n
C
Assume now moreover that Y = (cid:104)θ∗, X(cid:105) + η, where θ∗ ∈ Rd and η is a
random variable independent from X such that E(η) = 0 and 0 < E(η2) =
σ2 < ∞. In this case, for n ≥ O

κ(cid:2)d + log(cid:0)M n/σ2(cid:1)(cid:3)

(cid:33)(cid:33)

1 + O

n

.

E

(cid:104)
min(cid:8)R((cid:98)θ ) − R(θ∗), M(cid:9)(cid:105)
(cid:16)
E(cid:2)R((cid:98)θ ) − R(θ∗)| X1, . . . , Xn

Moreover, when n ≥ O
with probability at least 1 − ,

≤

1 + O

(cid:32)(cid:115)
κ(cid:2)d + log(−1)(cid:3)(cid:17)
(cid:3) ≤ (1 +(cid:98)δ) d σ2
(cid:32)(cid:115)

(1 − γ−) n

(cid:32)

=

1 + O

κ(cid:2)d + log(−1)(cid:3)

(cid:33)(cid:33)

n

d σ2
n

.

satisﬁes equation (3) on page 7,

,

n

(cid:33)(cid:33)

σ2d
n

.

Remark that the last statement of this proposition is about the random design
risk

R(θ) = E(cid:2)(cid:0)Y − (cid:104)θ, X(cid:105)(cid:1)2(cid:3)

March 17, 2016

Olivier Catoni

4.5 Exact convergence rate for the empirical risk minimizer

38

and not about the weaker ﬁxed design risk

(cid:32)

E

1
n

(cid:0)Yi − (cid:104)θ, Xi(cid:105)(cid:1)2(cid:12)(cid:12)(cid:12) X1, . . . , Xn
n(cid:88)

i=1

(cid:33)

.

The proof of Proposition 4.10 is presented in the appendix.

The next proposition states an upper bound with large probability when

the noise is Gaussian and independent from the design X.

Proposition 4.11 Make the same assumptions as in the end of the pre-
vious proposition. Assume moreover that the noise η is Gaussian (but not
equation (3) on page 7, with probability at least 1 − 2,

necessarily the design X). For any n > n = O(cid:0)κ(cid:2)d + log(−1)(cid:3)(cid:1) given by
R(cid:0)(cid:98)θ(cid:1) − R(θ∗) ≤ (1 +(cid:98)δ) σ2
(cid:32)(cid:115)

(cid:2)2 d log(2) + 4 log(−1)(cid:3)
κ(cid:2)d + log(cid:0)−1)(cid:3)

σ2(cid:2)2 d log(2) + 4 log(−1)(cid:3)

(1 − γ−) n

(cid:33)(cid:33)

(cid:32)

=

1 + O

n

n

,

where it is interesting to remind that 2 log(2) ≤ 1.4.
Proof. Using the same notation as in the proof of Proposition 4.10 on
page 36,

(cid:18)

(cid:19)

P

−1

G

W|X1, . . . , Xn

= N

0,

G

G

,

−1

σ2
n

−1

where G
G is the orthogonal projection on Im(G). Therefore, we can mod-
ify the end of the proof of Proposition 4.10, using a Chernoﬀ deviation bound
for Gaussian vectors. (cid:3)

Now, let us close this section with the corresponding lower bound, to get
the announced exact convergence rate in expectation of the excess risk of the
empirical risk minimizer.

Proposition 4.12 Consider a sample (X1, Y1), . . . , (Xn, Yn) made of n in-
dependent copies of the couple of random variables (X, Y ) ∈ Rd × R. Choose
two exponents p, q ∈ [1, 2]. Assume that

κ = sup(cid:8)E(cid:0)(cid:104)θ, X(cid:105)4(cid:1) : θ ∈ Rd, E(cid:0)(cid:104)θ, X(cid:105)2(cid:1) ≤ 1(cid:9) < ∞,

(cid:104)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:13)(cid:13)G−1/2X(cid:13)(cid:13)2(cid:105)

< ∞,

C = E

C > 0,

March 17, 2016

Olivier Catoni

4.5 Exact convergence rate for the empirical risk minimizer

E

(cid:104)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)4(cid:13)(cid:13)G−1/2X(cid:13)(cid:13)4(cid:105)
(cid:104)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:13)(cid:13)G−1/2X(cid:13)(cid:13)2(cid:105)2 < ∞,
(cid:16)(cid:13)(cid:13)G−1/2X(cid:13)(cid:13)2q(p+1)(cid:17)

< ∞.

E

κ(cid:48) =

E

39

Under these hypotheses, we will give a technical meaning, in two diﬀerent
ways, to the fact that C/n is the exact convergence rate of the excess risk

R((cid:98)θ) − R(θ∗) of the empiricial risk minimizer

n(cid:88)
(cid:0)Yi − (cid:104)θ, Xi(cid:105)(cid:1)2.

θ∈Rd

(cid:98)θ ∈ arg min
(cid:19)p/2(cid:34)

i=1

E(cid:0)(cid:13)(cid:13)G−1/2X(cid:13)(cid:13)2(p+1) +

Deﬁne µ as in Proposition 1.1 on page 7,(cid:98)δ = µ/(1 − 2µ) and deﬁne
(cid:35)(cid:33)
(cid:32)(cid:18)log(−1) + d
E(cid:0)(cid:13)(cid:13)G−1/2X(cid:13)(cid:13)2q(p+1)(cid:1)1/q
(cid:101)γ+ = O
For any n ≥ n = O(cid:0)κ(cid:2)d + log(−1)(cid:3)(cid:1) given by equation (3) on page 7,
(1 −(cid:101)γ+)2

as in Proposition 2.6 on page 17.
there is an event Ω of probability at least 1 − 2 such that

+(1 −(cid:98)δ)2

1/qn1−1/q

1 −

1/2

√

κn

+

κ(cid:48) − 3
3n

(cid:18)

(cid:20)
(cid:16)(cid:2)R((cid:98)θ) − R(θ∗)(cid:3)1Ω

1 +

6

(cid:19)1/2
(cid:16)
(cid:17) ≤ E

≤ E

≤

× C
n

(cid:21)
(cid:17)
R((cid:98)θ − R(θ∗)(cid:12)(cid:12) Ω
(1 +(cid:98)δ)2
κ(cid:2)d + log(−1)(cid:3)
κ(cid:2)d + log(−1)(cid:3)

(cid:32)(cid:115)
(cid:32)(cid:115)

(1 − γ−)2(1 − 2)

n

n

× C
n

.

(21)

(cid:33)
(cid:33)

,

.

+ 1/2

+ 1/2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ On→∞
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ On→∞

Remark that when p > 1, this gives

E

C

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n
(cid:17) − 1
(cid:16)
R((cid:98)θ) − R(θ∗)(cid:12)(cid:12) Ω
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n
(cid:17) − 1
(cid:16)(cid:2)R((cid:98)θ) − R(θ∗)(cid:3)1Ω
min(cid:8)R((cid:98)θ) − R(θ∗), M(cid:9)(cid:105)
(cid:104)

C

E

and

n
C

E

For any M > 0, any  satisfying equation (3) on page 7,

March 17, 2016

Olivier Catoni

4.5 Exact convergence rate for the empirical risk minimizer

40

(cid:34)

≥ (1 −(cid:98)δ)2

+(1 −(cid:101)γ+)2

+

(cid:18)
(cid:18)

1 +

6

1 +

κ(cid:48) − 3
3n
κ(cid:48) − 3
3n

(cid:19)
(cid:19)1/2

1 − 3C
4M n
√

−

(cid:35)

1/2

.

(22)

If we assume that q > 1 and bind  and n by the relation  = n−(q−1), equation

(3) on page 7 is satisﬁed when n ≥ O(cid:0)κ(cid:2)d + log(κ)(cid:3)(cid:1) and
(cid:16)(cid:13)(cid:13)G−1/2X(cid:107)2q(p+1)(cid:17)1/q

(cid:19)p/2

(cid:33)

E

,

(cid:32)(cid:18)log(n) + d
(cid:101)γ+ = O
min(cid:8)R((cid:98)θ) − R(θ∗), M(cid:9)(cid:105)
(cid:32)(cid:115)
κ(cid:2)d + log(n)(cid:3)

κn

(cid:104)

so that

E

n
C
≥ 1 − O

+

n

(1 + κ(cid:48)/n)C

(cid:19)p/2

(cid:18)log(n) + d
+(cid:0)1 + κ(cid:48)/n(cid:1)1/2n−(q−1)/2

κn

E

(cid:33)

(cid:16)(cid:13)(cid:13)G−1/2X(cid:13)(cid:13)2q(p+1)(cid:17)1/q
n−(q−1)/2(cid:17)

1 − On→∞

.

E

C

=

+

M n

(cid:104)

when q<2

Looking at the non-asymptotic bounds, we see that we can bind M to n by

Combining this result with the reverse bound of Proposition 4.10 on page 36
gives, in the case when q ∈]1, 2[,

(cid:12)(cid:12)(cid:12)(cid:12) n
min(cid:8)R((cid:98)θ) − R(θ∗), M(cid:9)(cid:105) − 1
(cid:104)
min(cid:8)R((cid:98)θ) − R(θ∗), C exp(cid:0)n2−q(cid:1)(cid:9)(cid:105) − 1

(cid:16)
(cid:12)(cid:12)(cid:12)(cid:12) ≤ On→∞(cid:0)n−(q−1)/2(cid:1).
the relation M = C exp(cid:0)n2−q(cid:1) and still get when q ∈]1, 2[,
(cid:12)(cid:12)(cid:12)(cid:12) ≤ On→∞(cid:0)n−(q−1)/2(cid:1).
(cid:12)(cid:12)(cid:12)(cid:12) n
This means that we can threshold R((cid:98)θ)−R(θ∗) at a very high level, in the sens
min(cid:8)R((cid:98)θ) − R(θ∗), C exp(cid:0)n2−q(cid:1)(cid:9)(cid:105)
(cid:104)
R((cid:98)θ) − R(θ∗) ≥ C exp(n2−q)
n exp(cid:0)n2−q(cid:1)(cid:19)

that the threshold is reached with a very small probability when the sample
size is large, since from Markov’s inequality,

C exp(cid:0)n2−q(cid:1)
(cid:18)

(cid:105) ≤ E

≤ On→∞

(cid:104)

(23)

C

E

P

1

.

March 17, 2016

Olivier Catoni

4.5 Exact convergence rate for the empirical risk minimizer

41

The proof of this proposition is given in appendix.

The exact rate C/n can be used to construct another lower bound, this
time for the expected excess risk, to complement the lower bound on the
deviations of the excess risk given in Proposition 4.9 on page 34. For this,
we want to describe a case where C is much larger than d R(θ∗)/n. Consider
η ∈ {−1, +1}, a Rademacher random variable independent from X. This
means more precisely that

P(η = +1| X) = P(η = −1| X) = 1/2.

Deﬁne for some θ∗ ∈ Rd

Y = (cid:104)θ∗, X(cid:105) + η(cid:107)G−1/2X(cid:107).

Let κ1 and κ2 be deﬁned as in Lemma 4.3 on page 25. In this case,

(cid:16)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)4(cid:17)

C = E

(cid:16)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:17)2
(cid:16)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:17)

E

= κ2E

(cid:16)(cid:13)(cid:13)G−1/2X(cid:13)(cid:13)2(cid:17)

= κ2E

= κ2R(θ∗)d.

We can add some hypotheses on the structure of X to make sure that κ2 and
κ1 + κ2 are of the same order of magnitude. Consider a centered Gaussian
vector W ∈ Rd such that W ∼ N(0, G) and an independent non-negative
real valued random variable ρ such that E(ρ2) = 1. This implies that G =
E(XX(cid:62)) is the Gram matrix of X as well as of W . Since

E(cid:0)(cid:104)θ, X(cid:105)4(cid:1) = E(cid:0)ρ4(cid:1)E(cid:0)(cid:104)θ, W(cid:105)4(cid:1)
= 3 E(cid:0)ρ4(cid:1)E(cid:0)(cid:104)θ, W(cid:105)2(cid:1)2 = 3 E(cid:0)ρ4(cid:1)E(cid:0)(cid:104)θ, X(cid:105)2(cid:1)2,
κ1 = 3E(cid:0)ρ4(cid:1). On the other hand
(cid:16)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)4(cid:17)
(cid:16)(cid:13)(cid:13)G−1/2X(cid:13)(cid:13)4(cid:17)
(cid:18)
= E(cid:0)ρ4(cid:1)(cid:2)3d + d(d − 1)(cid:3) =
(cid:18)
(cid:18)
E(cid:0)ρ4(cid:1) and

(cid:16)(cid:13)(cid:13)G−1/2W(cid:13)(cid:13)4(cid:17)
= E(cid:0)ρ4(cid:1)E
(cid:19)
(cid:16)(cid:13)(cid:13)G−1/2X(cid:13)(cid:13)2(cid:17)2
E(cid:0)ρ4(cid:1)E
(cid:19)
(cid:16)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:17)2
E(cid:0)ρ4(cid:1)E

so that κ2 =

(cid:19)

=

1 +

= E

2
d

1 +

2
d

E

,

1 +

2
d

(cid:0)κ1 + κ2)R(θ∗)d.

C =

1 + 2/d
4 + 2/d

March 17, 2016

Olivier Catoni

fulﬁlled for p = 1 and any q ∈ [1, 2[, so that for any q < 2,

If we assume that E(cid:0)ρ8(cid:1) < ∞, then the hypotheses of Proposition 4.12 are
(cid:16)
(cid:1)dR(θ∗)

min(cid:8)R((cid:98)θ) − R(θ∗), C exp(cid:0)n2−q(cid:1)(cid:9)(cid:17)
(cid:17) ≥ E
(cid:16)
1 − O(cid:0)n−(q−1)/2(cid:1)(cid:17)(1 + 2/d)(cid:0)κ1 + κ2
≥(cid:16)

R((cid:98)θ) − R(θ∗)

E

42

(4 + 2/d)n

.

Here, we just described a case where C can be arbitrarily larger than

This provides a lower bound for the expected excess risk that complements
Proposition 4.9 on page 34.

dR(θ∗), since κ1 + κ2 =(cid:0)4 + 2/d(cid:1)E(cid:0)ρ4(cid:1) can be arbitrarily large. It is also
sphere of Rd, so that E(cid:0)U U(cid:62)(cid:1) = d−1I. Let ρ be an independent random

interesting to remark that C can be arbitrarily smaller than d R(θ∗). Let us
describe such a situation. Let U be a uniform random vector on the unit

variable taking the two positive real values a and b each with probability
1/2. Let η ∈ {−1, +1} be a Rademacher random variable independent from
U and ρ.

Let X = ρU and Y = (cid:104)θ∗, X(cid:105) + η(cid:107)X(cid:107)−1 = (cid:104)θ∗, X(cid:105) + η/ρ.

In this case

G = E(cid:0)XX(cid:62)(cid:1) = E(cid:0)ρ2(cid:1)E(cid:0)U U(cid:62)(cid:1) =

a2 + b2

2d

I,

C =

and R(θ∗) =

2d

a2 + b2 ,
a−2 + b−2

2

,

so that

C

dR(θ∗)

=

4

(a2 + b2)(a−2 + b−2)

=

4

2 + a2/b2 + b2/a2

can take any value in the range ]0, 1] depending on the value of the ratio a/b.
Meanwhile, it is easy to check that the hypotheses of Proposition 4.12 are
fulﬁlled for any p and q ∈ [1, 2] so that equation (23) on page 40 is satisﬁed
for any q ∈]1, 2[.

A. Proof of Proposition 1.1 on page 7

Let us assume without loss of generality that G is of full rank. Indeed it
is easy to see that X ∈ Im(G) almost surely. This comes from the fact that

for any θ ∈ Ker(G), E(cid:0)(cid:104)θ, X(cid:105)2(cid:1) = 0, and therefore that P(cid:0)(cid:104)θ, X(cid:105) = 0(cid:1) = 1.

March 17, 2016

Olivier Catoni

43

Taking a basis of Ker(G), we obtain that P(cid:0)X ∈ Ker(G)⊥(cid:1) = 1. Remark
now that Ker(G)⊥ = Im(G), since G is symmetric, so that P(cid:0)x ∈ Im(G)(cid:1) =
Let us consider the new variables(cid:101)θ = G1/2θ, and (cid:101)X = G−1/2X. Working
with ((cid:101)θ, (cid:101)X) instead of (θ, X) is the same as assuming that G = I (the identity

1. Restricting the state space to Im(G) and considering the coordinates
of X in some orthonormal basis of Im(G) sets us back to the case where
Ker(G) = {0}.

matrix of rank d), and consequently that N (θ) = (cid:107)θ(cid:107)2. This is what we will
do in the following of this proof, keeping in mind that with this convention,
although (cid:104)θ, Xi(cid:105) is still observable, Xi and θ themselves are not.
For any θ ∈ Rd, let us consider πθ = N(θ, β−1 I), the Gaussian distribution
with mean θ and covariance matrix β−1 I.

In order to apply some PAC-Bayesian inequality, we introduce into the

computations the perturbation of θ deﬁned by πθ.

We are going to prove a succession of lemmas leading to

Proposition A.1 Let us introduce the numerical constant

c =

√
15
8 log(2)(

2 − 1)

exp

(cid:18)1 + 2

√
2

(cid:19)

2

≤ 44.3.

For any real parameter λ > 0, any x, θ ∈ Rd,

λ(cid:2)(cid:104)θ, x(cid:105)2 − 1(cid:3)(cid:111) ≤
(cid:110)

ψ

(cid:90)

1 + λ

log

(cid:20)
(cid:104)θ(cid:48), x(cid:105)2 − 1 − (cid:107)x(cid:107)2

(cid:21)2

+

+

λ2
2

(cid:40)

β

(cid:21)

(cid:20)
(cid:104)θ(cid:48), x(cid:105)2 − 1 − (cid:107)x(cid:107)2
cλ2(cid:107)x(cid:107)2

(cid:18)

β

(cid:104)θ(cid:48), x(cid:105)2 +

β

(cid:19)(cid:41)

(cid:107)x(cid:107)2
2β

dπθ(θ(cid:48)).

Let us introduce the function

ψ(x),

x ≤ x1,

χ(x) =

y1 + p1(x − x1) − (x − x1)2/8, x1 ≤ x ≤ x1 + 4p1,
y1 + 2p2
1,

x ≥ x1 + 4p1,

where x1 ∈ [0, 1], y1, p1 are deﬁned by the conditions ψ(cid:48)(cid:48)(x1) = −1/4, y1 =
ψ(x1), and p1 = ψ(cid:48)(x1).
Since ψ(cid:48)(cid:48)(x) continues to decrease after x1, whereas χ(cid:48)(cid:48)(x) remains con-
stant, until χ(cid:48)(z) = 0, afterwhich the function χ is constant, we see that
ψ(z) ≤ χ(z) for all z ∈ R. On the other hand,

χ(z) ≤ log(cid:0)1 + z + z2/2(cid:1).

March 17, 2016

Olivier Catoni

log(cid:0)1 + z + z2/2(cid:1) is such that f (x1) ≥ χ(x1) = ψ(x1), f(cid:48)(x1) ≥ χ(cid:48)(x1) =

Indeed, we already saw that this is the case for ψ, and as the function f (z) =
ψ(cid:48)(x1), and inf f(cid:48)(cid:48) = −1/4, f is above χ on the right-hand side of x1 also.

Starting from the expressions

44

ψ(cid:48)(z) =

and

ψ(cid:48)(cid:48)(z) =

−z + z2/2
1 − z + z2/2

,

it is easy to compute x1, y1 and p1. We obtain

1 − z

1 − z + z2/2

x1 = 1 −(cid:112)
y1 = − log(cid:2)2(
(cid:112)

p1 =

2(

√
2 − 5
√
4
2 − 1)

√
2 − 5,
4
√

2 − 1)(cid:3),

χ(z) = y1 + 2p2

1 =

sup
z∈R

√
2

− log(cid:2)2(

2 − 1)(cid:3).

√

,

1 + 2
2

We start with a bound comparing perturbations of the parameter θ inside

where by deﬁnition

and outside of χ.
Lemma A.2 For any ρ ∈ M1

χ

(cid:18)(cid:90)
Var(cid:0)h dρ(cid:1) =

h dρ

Var(cid:0)h dρ(cid:1),

(cid:19)
(cid:90)
+(Θ) and any h ∈ L1(ρ),
≤
(cid:90) (cid:18)

χ(h) dρ +

(cid:19)2

(cid:90)

1
8

h −

(cid:18)

1
8

h dρ

dρ(θ) ∈ R ∪ +∞.

(cid:90)

(cid:19)2

h dρ

,

y ∈ R.

Proof. Let us consider the function
y −

g(y) = χ(y) +

Since inf χ(cid:48)(cid:48) = 1/4, the function g is convex. Jensen’s inequality shows that

h dρ

≤

g(h) dρ,

g

(cid:18)(cid:90)
(cid:18)(cid:90)

(cid:19)
(cid:19)

(cid:90)
(cid:18)(cid:90)

(cid:19)

h dρ

.

and we conclude, remarking that

g

(cid:3)

h dρ

= χ

March 17, 2016

Olivier Catoni

45

(cid:18)(cid:90)

(cid:19)

(cid:90)

≤

(cid:110)

Var(cid:0)h dρ(cid:1)(cid:111)

.

Lemma A.3 For any ρ ∈ M1

+(Θ) and any h ∈ L1(ρ),

ψ

h dρ

χ(h) dρ + min

log(4),

Proof.

(cid:18)(cid:90)

ψ

(cid:19)

h dρ

≤ sup ψ +

(cid:90)

χ(h) dρ − inf χ =

1
8

(cid:90)

χ(h) dρ + log(4).

We obtain this lemma by combining this inequality with the previous lemma
(since ψ(z) ≤ χ(z) for any z ∈ R). (cid:3)

(cid:21)(cid:41)

ψ

(cid:90)

= ψ

λ

− 1

(cid:40)
(cid:40)
(cid:110)
(cid:40)

λ(cid:2)(cid:104)θ, x(cid:105)2 − 1(cid:3)(cid:111)
(cid:110)

Applying this to our problem gives, for any θ ∈ Rd,
(cid:20)(cid:90)
(cid:104)θ(cid:48), x(cid:105)2 dπθ(θ(cid:48)) − (cid:107)x(cid:107)2
(cid:21)(cid:41)
(cid:20)
(cid:104)θ(cid:48), x(cid:105)2 − (cid:107)x(cid:107)2
Var(cid:2)(cid:104)x, θ(cid:48)(cid:105)2 dπθ(θ(cid:48))(cid:3)(cid:111)
(cid:21)(cid:41)
(cid:104)θ(cid:48), x(cid:105)2 − (cid:107)x(cid:107)2
(cid:26)

log(4),

+ min

− 1

− 1

λ2
8

dπθ(θ(cid:48))

dπθ(θ(cid:48))

(cid:90)

=

χ

λ

χ

λ

(cid:20)

≤

β

β

β

+ min

log(4),

λ2(cid:107)x(cid:107)2(cid:104)θ, x(cid:105)2

(cid:27)

,

λ2(cid:107)x(cid:107)4
4β2

+

2β
where we have used the fact that when W ∼ N(0, σ2),

(cid:104)(cid:0)W 2 + 2mW(cid:1)2(cid:105) − E(cid:0)W 2 + 2mW(cid:1)2

Var(cid:2)(m + W )2(cid:3) = Var(cid:0)W 2 + 2mW(cid:1)
= E(cid:0)W 4(cid:1) + 4m2σ2 − σ4 = 2σ4 + 4m2σ2,
to compute Var(cid:2)(cid:104)x, θ(cid:48)(cid:105)2 dπθ(θ(cid:48))(cid:3).
min(cid:8)a, bm2 + c(cid:9) ≤ min(cid:8)a, b(m + W )2 + c(cid:9) + min(cid:8)a, b(m − W )2 + c(cid:9),

Remark now that for any positive real numbers a, b, and c,

= E

so that, the distribution of W and −W being the same,

min(cid:8)a, bm2 + c(cid:9) ≤ 2E

(cid:104)

min(cid:8)a, b(m + W )2 + c(cid:9)(cid:105)

.

March 17, 2016

Olivier Catoni

46

(cid:21)(cid:27)

− 1

dπθ(θ(cid:48))

λ2(cid:107)x(cid:107)2(cid:104)θ(cid:48), x(cid:105)2

(cid:27)

λ2(cid:107)x(cid:107)4
2β2

Applying this to the gaussian distribution πθ, we obtain that

ψ(cid:8)λ(cid:2)(cid:104)θ, x(cid:105)2 − 1(cid:3)(cid:9) ≤

β

λ

χ

+

(cid:26)

(cid:90)
(cid:90)

(cid:20)
(cid:104)θ(cid:48), x(cid:105)2 − (cid:107)x(cid:107)2
(cid:26)
(cid:104)
a exp(cid:0)min{b, y}(cid:1)(cid:105)
a + min{b, y}a(cid:0)exp(b) − 1(cid:1)
(cid:27)

4 log(2),

dπθ(θ(cid:48)).
We are now going to use the fact that for any a, b, y ∈ R+, such that y ≤ b,

min

+

β

(cid:26)

log(a) + min{b, y} = log

a(cid:0)exp(b) − 1(cid:1)
(cid:27)
Applying this inequality to a = exp(cid:0)χ(z)(cid:1) and reminding that exp(cid:2)χ(z)(cid:3) ≤

≤ log

≤ log

(cid:26)

a + y

b

b

.

1 + z + z2, we obtain that

χ(z) + min{b, y} ≤ log

(cid:26)

exp(cid:0)sup χ(cid:1)(cid:0)exp(b) − 1(cid:1)

(cid:27)

1 + z + z2/2 + y

b

Coming back to our problem, where we can take b = 4 log(2), we get

λ(cid:2)(cid:104)θ, x(cid:105)2 − 1(cid:3)(cid:111)
(cid:110)
(cid:40)
(cid:90)

ψ

≤

log

1 + λ

(cid:20)
(cid:104)θ(cid:48), x(cid:105)2 − 1 − (cid:107)x(cid:107)2

.

(cid:21)2

(cid:80)n

(cid:104)θ(cid:48), x(cid:105)2 − 1 − (cid:107)x(cid:107)2
(cid:21)(cid:41)

β

(cid:104)θ(cid:48), x(cid:105)2 +

(cid:107)x(cid:107)2
2β

dπθ(θ(cid:48)),

+

+

(cid:21)

λ2
2
β
cλ2(cid:107)x(cid:107)2

(cid:20)
(cid:20)
(cid:18)1 + 2
(cid:110)
λ(cid:2)(cid:104)θ, x(cid:105)2 − 1(cid:3)(cid:111)

2 − 1)

exp

β

2

(cid:19)

,

√

2

where

c =

√
15
8 log(2)(

as announced in Proposition A.1 on page 43.

Now that we have compared ψ

with an expectation with
respect to a Gaussian perturbation of the parameter θ, we are prepared to
use the following PAC-Bayes inequality.

Lemma A.4 Let us consider two measurable spaces X and Θ and an i.i.d.
sample (Xi)n
i=1 δXi
be its empirical measure. Let us consider a prior probability measure ν ∈

i=1 ∈ Xn with probability distribution P⊗n. Let P = 1

n

March 17, 2016

Olivier Catoni

47

+(Θ). For any measurable function f : X× Θ → [a, +∞[, where −1 < a <
M1
+∞, with probability at least 1 −  according to the sample distribution P⊗n,
+(Θ) such that K(ρ, ν) < +∞,
for any posterior probability measure ρ ∈ M1
(cid:90)

log(cid:2)1 + f (x, θ)(cid:3) dρ(θ) dP(x) ≤

f (x, θ) dP(x) dρ(θ)

(cid:90)

K(ρ, ν) − log()

n

+

.

(24)

Let us remark that in this lemma and in the following of this paper we will
encounter events that are the union or the intersection of an uncountable
family of measurable sets, as well as suprema of uncountable families of
functions. To give a meaning to this, consider that when we write P(A) ≥
1 − , we mean that there exists a measurable set B ⊂ A such that P(B) ≥
1 − . In the same way when we write

we mean that there is a measurale function g such that h(x) ≤ g(x) for all
x ∈ X and

The proof of Lemma A.4 on the previous page requires a succession of

preliminary results.

Lemma A.5 For any measurable space Θ, any upper bounded measurable
function h : Θ → R, any probability measures ρ, π ∈ M1

+(Θ),

(cid:90)

(cid:18)(cid:90)

(cid:19)

h dρ − K(ρ, π) ≤ log

exp(h) dπ

.

(cid:90)
(cid:90)

h dP ≤ η,

g dP ≤ η.

Proof. Let us introduce the probability measure πexp(h) (cid:28) π with density
dπexp(h)

exp(h)

. We can check that

(cid:82) exp(h) dπ

(cid:18)(cid:90)

(cid:19)

h dρ − K(ρ, π) − log

(where it is possible that K(ρ, π) = +∞, K(ρ, πexp(h)) = +∞, or (cid:82) h dρ =

exp(h) dπ

= −K(ρ, πexp(h)) ≤ 0,

dπ

=

(cid:90)

−∞). (cid:3)

March 17, 2016

Olivier Catoni

48

Lemma A.6 Let us consider some measurable state space X, some measur-
able parameter space Θ, a prior probability measure ν. For any measurable
function g : X × Θ → R, such that

(cid:90)

exp(cid:2)g(x, θ)(cid:3) dP(x) ≤ 1,
(cid:90)

θ ∈ Θ,

(cid:19)

g(x, θ) dP(x) dρ(θ) − K(ρ, ν)

dP⊗n ≤ 1

(cid:90)

(cid:18)

exp

n

sup

ρ∈M1
+(Θ),
K(ρ,ν)<+∞

Therefore, with probability at least 1 − , for any ρ ∈ M1

+(Θ) such that

K(ρ, ν) < +∞, (cid:90)

g(x, θ) dP(x) dρ(θ) <

K(ρ, ν) − log()

n

.

Let us give some needed explanations about the meaning of this lemma. The
assumptions do not imply that θ (cid:55)→ g(x, θ) ∈ L1(ρ). As we will see in the
proof, the lemma is true without further assumptions if we use the convention

(cid:90)

min(cid:8)g(x, θ), 0(cid:9) dρ(θ) = −∞,

g(x, θ) dρ(θ) = −∞ when

and is deﬁned otherwise as usual (so that the integral is equal to +∞ when
the integral of the positive part of the integrand is inﬁnite and the integral
of its negative part is ﬁnite).
Proof. Let us put gk(x, θ) = min{g(x, θ), k}, k ∈ N. Applying the
monotone convergence theorem, the previous lemma, and the monotone con-
vergence theorem again, we see that

(cid:90)

(cid:32)

(cid:90)
(cid:32)

exp

n

sup

ρ∈M1
+(Θ),
K(ρ,ν)<+∞

g(x, θ) dP(x) dρ(θ) − K(ρ, ν)

(cid:90)

gk(x, θ) dP(x) dρ(θ) − K(ρ, ν)

= sup
k∈N

exp

sup

ρ∈M1
+(Θ),
K(ρ,ν)<+∞

(cid:34)(cid:90)

n

(cid:18)

(cid:90)

(cid:33)

(cid:35)

≤ sup
k∈N

exp

n

gk(x, θ) dP(x)

(cid:90)

(cid:18)

=

exp

n

dν(θ)

g(x, θ) dP(x)

(cid:19)

dν(θ).

(cid:33)

(cid:19)
(cid:90)

March 17, 2016

Olivier Catoni

49

According to Fubini’s theorem, the right-hand side of the last equality is a
measurable function of the sample. Thus, with our conventions about the
use of integrals of non-measurable functions, we can write

(cid:90)

(cid:32)

(cid:90)

(cid:33)

exp

sup

ρ∈M1
+(Θ),
K(ρ,ν)<+∞
≤

=

n

(cid:90)
(cid:90)

(cid:18)
(cid:18)

(cid:90)
(cid:90)

g(x, θ) dP(x) dρ(θ) − K(ρ, ν)

dP⊗n

dν(θ) dP⊗n

(cid:19)
(cid:19)
(cid:19)n
exp(cid:2)g(x, θ)(cid:3) dP(x)

dP⊗n dν(θ)

exp

n

g(x, θ) dP(x)

exp

n

g(x, θ) dP(x)

(cid:90) (cid:18)(cid:90)

=

dν(θ) ≤ 1,

(cid:40)(cid:90)

where we have used Fubini’s theorem for non-negative functions again and
the independence of the sample.

This proves the ﬁrst inequality of the lemma.
To prove the second one, let us consider the two events

A =

g(x, θ) dP(x) dρ(θ) <

(cid:40)

B =

log

(cid:20)(cid:90)

(cid:18)

exp

n

(cid:90)

g(x, θ) dP(x)

K(ρ, ν) − log()

,

(cid:41)
+(Θ), K(ρ, ν) < +∞

,

ρ ∈ M1

(cid:41)

n

(cid:19)

dν(θ)

< − log()

.

(cid:21)

(cid:19)

(cid:21)

According to the ﬁrst part of the proof, B ⊂ A. Moreover B is measurable.
Let us introduce the random variable

(cid:20)(cid:90)

(cid:18)

(cid:90)

W = log

It is measurable and B = {W < 0}, so that P⊗n(B) = 1 − P⊗n(cid:0)W ≥ 0(cid:1). We

g(x, θ) dP

+ log().

dν(θ)

exp

n

end the proof using the exponential Markov inequality
exp(W ) dP⊗n ≤ ,

P⊗n(cid:0)W ≥ 0(cid:1) ≤

(cid:90)

the last inequality being a consequence of the ﬁrst part of the proof. (cid:3)

Proof of Lemma A.4 on page 46

March 17, 2016

Olivier Catoni

(cid:90)

50

(cid:90)

(cid:90)

1,

(cid:90)

We can check that

Let us put m(θ) =

f (x, θ) dP(x) ∈ [a, +∞]. Let us consider

Since 1 + m ≤ exp(m), from the convexity of the exponential function, we
see that

(cid:105)
g(x, θ) = 1(cid:2)m(θ) < +∞(cid:3)(cid:104)
log(cid:0)1 + f (x, θ)(cid:1) − m(θ)
 1 + m(θ)
exp(cid:2)m(θ)(cid:3), when m(θ) < +∞
exp(cid:2)g(x, θ)(cid:3) dP(x) =
exp(cid:2)g(x, θ)(cid:3) dP(x) ≤ 1.
Therefore, we can apply the previous lemma to this choice of g. It shows that
+(Θ), such that K(ρ, ν) < +∞,
with probability at least 1− , for any ρ ∈ M1
(cid:111)
log(cid:2)1 + f (x, θ(cid:48))(cid:3)− m(θ(cid:48))
dP(x) dρθ(θ(cid:48)) < B(ρ, ), (25)
(cid:90)
1(cid:2)m(θ) = +∞(cid:3) dρ(θ) = 0 and the left-hand side of

1(cid:2)m(θ(cid:48)) < +∞(cid:3)(cid:110)

. We can then remark that in the case when

m(θ) dρ(θ) < +∞,

K(ρ, ν) − log()

where B(ρ, ) =

otherwise.

(cid:90)

(cid:90)

n

equation (25) is equal to

(cid:90)

log(cid:2)1 + f (x, θ)(cid:3) dP(x) dρ(θ) −

(cid:90)

m(θ) dρ(θ),

so that in this case equation (25) is equation (24) on page 47 with a strict
inequality, and therefore implies equation (24). On the other hand, when
m(θ) dρ(θ) = +∞, inequality (24) on page 47 is also true because its

right-hand side is equal to +∞. (cid:3)
Let us now apply Lemma A.4 on page 46 to our problem, choosing X =
Θ = Rd, ρ = πθ and ν = π0. It proves that, with probability at least 1 − ,
for any θ ∈ Rd,

(cid:90)

+

ψ

λ2
2

(cid:110)
λ(cid:2)(cid:104)θ, x(cid:105)2 − 1(cid:3)(cid:111)
(cid:18)
(cid:104)θ(cid:48), x(cid:105)2 − 1 − (cid:107)x(cid:107)2

dP(x) ≤

(cid:19)2

+

β

(cid:90) (cid:26)

(cid:18)

λ

cλ2(cid:107)x(cid:107)2

(cid:19)
(cid:104)θ(cid:48), x(cid:105)2 − 1 − (cid:107)x(cid:107)2
(cid:19)(cid:27)
(cid:18)
β
(cid:107)x(cid:107)2
2β

(cid:104)θ(cid:48), x(cid:105)2 +

β

dπθ(θ(cid:48)) dP(x)

March 17, 2016

Olivier Catoni

51

.

K(πθ, π0) − log()

n

+

(cid:90) β(cid:104)θ, x(cid:105)2

β(cid:107)θ(cid:107)2

We can now compute K(πθ, π0) =
into account the change of variables that turns the Gram matrix into the
identity. Integrating then explicitly with respect to the Gaussian measure
πθ, we get with probability at least 1 − , for all θ ∈ Rd, that

dP(x), taking here

=

2

2

(cid:90)

(cid:110)
λ(cid:2)(cid:104)θ, x(cid:105)2 − 1(cid:3)(cid:111)

ψ

(cid:90) (cid:26)
λ(cid:0)(cid:104)θ, x(cid:105)2 − 1(cid:1)
(cid:20)(cid:0)(cid:104)θ, x(cid:105)2 − 1(cid:1)2 +

dP(x) ≤

4(cid:107)x(cid:107)2

(cid:104)θ, x(cid:105)2 +

λ2
2
cλ2(cid:107)x(cid:107)2

(cid:18)

+

+

β

(cid:104)θ, x(cid:105)2 +

(cid:19)

β
3(cid:107)x(cid:107)2
2β

2(cid:107)x(cid:107)4
β2
β(cid:104)θ, x(cid:105)2

(cid:27)

+

2n

(cid:21)

dP(x) − log()

n

Let us introduce

(cid:90)

s4
4 =

(cid:107)x(cid:107)4 dP(x),

(cid:26)(cid:90)

κ = sup

(cid:104)θ, x(cid:105)4 dP(x) ; θ ∈ Rd,

(cid:27)

(cid:104)θ, x(cid:105)2 dP(x) ≤ 1

.

(cid:90)
(cid:19)2

.

.

Lemma A.7 The coeﬃcients s4 and κ are such that

(cid:18)(cid:90)
4 ≤ κ
s4

(cid:107)x(cid:107)2 dP(x)

= κd2.

Proof. Taking coordinates, we can write

(cid:90) (cid:18) d(cid:88)

i=1

(cid:90)

(cid:88)

1≤i,j≤d

x2
i

dP(x) =

(cid:19)2
≤ (cid:88)

(cid:18)(cid:90)
(cid:88)

(cid:18)(cid:90)

1≤i,j≤d
≤ κ

1≤i,j≤d

j dP(x)

x2
i x2

(cid:19)1/2(cid:18)(cid:90)
(cid:19)(cid:18)(cid:90)

x4
i dP(x)

x4
j dP(x)

x2
i dP(x)

x2
j dP(x)

(cid:19)1/2
(cid:19)
(cid:18)(cid:90)

s4
4 =

(cid:3)

(cid:19)2

= κ

(cid:107)x(cid:107)2 dP(x)

March 17, 2016

Olivier Catoni

52

Using the Cauchy-Schwarz inequality, we obtain with probability at least

1 − , for any θ ∈ R,

(cid:20)

rλ(θ) ≤

1 + (κ − 1)λ +

(2 + c)λs2
4

(cid:2)N (θ) − 1(cid:3)2 +

β

+
(κ − 1)λ

β

2nλ

+

√

κ

2

+

κλ
2

(cid:21)(cid:2)N (θ) − 1(cid:3)

√
(2 + c)
β

κλs2
4

+

(2 + 3c)λs4
4

2β2

+

β

2nλ

− log()
nλ

.

Let us choose some special values for β, the strength of the perturbation of
the parameter, and λ, the scale parameter of the inﬂuence function ψ. Let
us also introduce some more concise notations. Let us put accordingly

(cid:21)

,

√
(2 + 3c)s2
4
4
κ(2 + c)

(cid:21)

,

√
(2 + 3c)s2
4
4
κ(2 + c)

log(cid:0)−1(cid:1) +

β = λs4κ1/4(cid:112)2(2 + c)n,
(cid:115)
(cid:20)
log(cid:0)−1(cid:1) +
(cid:115)
(cid:20)

(κ − 1)n

λ =

2

η = (κ − 1)λ =

2(κ − 1)
√
κ

n

,

γ =

2(2 + c)s2
4

n

µ = η + γ,
κη

ξ =

2(κ − 1)

.

(cid:114)

(cid:115)

(cid:20)

Let us remark that, according to Lemma A.7 on page 51, s2

κd, so that

4 ≤ √

(cid:21)

(cid:114)

µ ≤

2(κ − 1)

n

log(−1) +

(2 + 3c)d
4(2 + c)

+

2(2 + c)κd

n

Numerically,

(cid:114)

µ ≤

2(κ − 1)

n

(cid:114)
(cid:2)log(−1) + 0.73 d(cid:3) + 6.81

2κ d

n

Proposition A.8 With probability at least 1 − , for any θ ∈ Rd,

rλ(θ) ≤ ξ(cid:2)N (θ) − 1(cid:3)2 + (1 + µ)(cid:2)N (θ) − 1(cid:3) + µ.

March 17, 2016

Olivier Catoni

Let us now study the reverse inequality. Starting from

(cid:110)
λ(cid:2)1 − (cid:104)θ, x(cid:105)2(cid:3)(cid:111)

ψ

(cid:40)
(cid:40)

(cid:20)
(cid:20)

(cid:90)

≤

= ψ

λ

1 +

−

(cid:104)θ(cid:48), x(cid:105)2 dπθ(θ(cid:48))

χ

λ

1 +

− (cid:104)θ(cid:48), x(cid:105)2

(cid:107)x(cid:107)2
β
(cid:107)x(cid:107)2
β

(cid:90)

(cid:110)

+ min

log(4),

(cid:21)(cid:41)

λ2
8

53

(cid:21)(cid:41)

dπθ(θ(cid:48))

Var(cid:2)(cid:104)θ(cid:48), x(cid:105)2 dπθ(θ(cid:48))(cid:3)(cid:111)

,

and proceeding in the same way as previously, we obtain
Proposition A.9 With probability at least 1 − , for any θ ∈ Rd,

rλ(θ) ≥ −ξ(cid:2)N (θ) − 1(cid:3)2 + (1 − µ)(cid:2)N (θ) − 1(cid:3) − µ.

Proposition A.10 Assume that

2µ + ξ < 1,

(26)

which can be written as

(cid:112)8(2 + c)κ1/4s4
(cid:18)5

n >

1

+

+

2(κ − 1)

2

2(κ − 1)

(cid:19)(cid:115)

.

log(−1) +

√
(2 + 3c)s2
4
4
κ(2 + c)

(cid:21) 2
(cid:20)
2(κ − 1)(cid:2)log(−1) + 0.73 d(cid:3)(cid:35)2
(cid:19)(cid:98)N (θ),
(cid:18)
(cid:19)
2 − 1(cid:1) exp

(cid:18)1 + 2

1 − 2µ
√

1 +

µ

2

2

.

,

1

+

2(κ − 1)

(cid:19)(cid:113)
(cid:19)(cid:98)N (θ) ≤ N (θ) ≤
8 log(2)(cid:0)√

15

√
κd +
20

n >

(cid:18)5

2

(cid:34)

(cid:18)

1 − µ

1 − 2µ

where

c =

A suﬃcient numerical condition implying the one above is that

With probability at least 1 − 2, for any θ ∈ Rd,

March 17, 2016

Olivier Catoni

and µ =

(cid:115)

≤

2(κ − 1)

(cid:115)

n
2(κ − 1)

n

(cid:20)

log(cid:0)−1(cid:1) +
(cid:20)
log(cid:0)−1(cid:1) +
(cid:114)

≤

54

κs2
4

(cid:21)
(cid:21)

(cid:114)
(cid:114)

√
2(2 + c)
n

√
(2 + 3c)s2
4
4
κ(2 + c)

+

(2 + 3c)d
4(2 + c)

+ 2

2(2 + c)κd

(cid:114)
(cid:2)log(cid:0)−1(cid:1) + 0.73d(cid:3) + 6.81

n

2(κ − 1)

n

2κd
n

.

Proof. Assume that both inequalities of Proposition A.8 and A.9 hold for
any θ ∈ Rd, which happens at least with probability 1 − 2. In this case,
when N (θ) = 0, for any α ∈ R+,

so that (cid:98)α = +∞, and (cid:98)N (θ) = 0 = N (θ). Assume now that N (θ) (cid:54)= 0. In

rλ(αθ) ≤ −1 + ξ < 0,

(cid:115)

2

this case, there is α =

such that N (αθ) = 2 and therefore such that

N (θ)
rλ(αθ) ≥ 1 − 2µ − ξ > 0,

proving that (cid:98)α(θ) < +∞, and consequently that r(cid:2)(cid:98)α(θ)θ(cid:3) = 0, as explained
before. For any α ∈ I = [0,(cid:98)α(θ)], rλ(αθ) ≤ 0, because α (cid:55)→ rλ(αθ) is non-

decreasing. Thus N (αθ) − 1 is solution of the following quadratic inequality
in the unknown variable z :

0 ≥ −ξz2 + (1 − µ)z − µ.

(27)

Since the right-hand side of this inequality is positive when z = 1, from
assumption (26) on page 53, its discriminant

∆ = (1 − µ)2 − 4ξµ

is strictly positive. Let us consider the interval

.

√

J =

1
2ξ

(cid:16)

1 − µ +

∆(cid:3)−1, +1(cid:2)(cid:17)
(cid:2)N (αθ) − 1(cid:3) (cid:54)∈ J,
α ∈ [0,(cid:98)α(θ)].
N(cid:2)(cid:98)α(θ)θ(cid:3) − 1 ≤ inf J ≤ µ

1 − 2µ

.

We obtain that

Since α (cid:55)→ N (αθ) − 1 is continuous and −1 ≤ inf J, necessarily

March 17, 2016

Olivier Catoni

Indeed, when z =

µ

1 − 2µ

, the right-hand side of inequality (27) is equal to

1 − 2µ − ξ
(1 − 2µ)2 > 0

55

according to assumption (26) on page 53, so that this value of z belongs to
J.

On the other hand, 1 − N(cid:2)(cid:98)α(θ)θ(cid:3) ≤ 1 is also solution of
two roots and that 1− N(cid:2)(cid:98)α(θ)θ(cid:3) is lower than its lowest root which is in turn
. Thus, as N(cid:2)(cid:98)α(θ)θ(cid:3) =(cid:98)α(θ)2N (θ) = N (θ)/(cid:98)N (θ), we have

When z = 1, the right-hand side of this inequality is strictly negative, due
to assumption (26) on page 53, showing that the corresponding equality has

0 ≤ ξz2 − (1 + µ)z + µ.

lower than

µ

1 − 2µ

proved that

(cid:12)(cid:12)(cid:12)(cid:12) N (θ)(cid:98)N (θ)

(cid:12)(cid:12)(cid:12)(cid:12) ≤ µ

1 − 2µ

,

− 1

from which Proposition A.10 on page 53, and therefore Proposition 1.1 on
page 7, follow. (cid:3)

B. Proof of Proposition 2.2 on page 11

Let Ω be the event of probability at least 1 − 2 that appears in Proposi-

tion 1.1 on page 7. Remark that on Ω,

(cid:90) (cid:16)(cid:104)θ, x(cid:105)2(cid:98)N (θ)−1 − 1
(cid:17)3

dP(x) ≤

+

(cid:90) (cid:104)θ, x(cid:105)6
(cid:98)N (θ)3
(cid:90) (cid:104)θ, x(cid:105)2
(cid:98)N (θ)

dP(x)

dP(x)

≤ max

i=1,...,n

≤ max

i=1,...,n

(cid:104)θ, Xi(cid:105)4

(cid:107)G1/2θ(cid:107)4(cid:107)G−1/2Xi(cid:107)4

(cid:98)N (θ)2
(cid:98)N (θ)2
× R4 × N (θ)(cid:98)N (θ)
(cid:98)N (θ)2
−δ−(θ) ≤ N (θ)(cid:98)N (θ)

N (θ)2

=

− 1 ≤ δ+(θ).

× N (θ)(cid:98)N (θ)
≤(cid:0)1 +(cid:98)δ(cid:1)2R4 N (θ)(cid:98)N (θ)

.

(28)

Remark also that the conclusions of Proposition 2.1 on page 10 hold on Ω,
so that

March 17, 2016

Olivier Catoni

From equation (28),

so that

and

This gives

δ+(θ) ≤ γ+

N (θ)(cid:98)N (θ)

,

N (θ)(cid:98)N (θ)

≤

1

(1 − γ+)+

δ+(θ) ≤

γ+

(1 − γ+)+

.

− 1 ≤ δ+(θ) ≤

−γ− ≤ −δ−(θ) ≤ N (θ)(cid:98)N (θ)
≤ (1 −(cid:98)δ)(cid:98)N (θ) ≤ N (θ) ≤ (1 +(cid:98)δ)(cid:98)N (θ) ≤ (1 +(cid:98)δ)N (θ)

(1 − γ+)+

γ+

.

1 − δ−(θ)

(1 −(cid:98)δ)N (θ)

1 + δ+(θ)

Using Proposition 1.1 again, we get that on the event Ω,

56

.

As a consequence

≤ −(cid:98)δ + δ−(θ)
−(cid:98)δ + γ−
1 +(cid:98)δ
1 +(cid:98)δ
≤ (cid:98)δ + γ+/(1 − γ+)+
1 −(cid:98)δ

− 1 ≤ (cid:98)δ + δ+(θ)
1 −(cid:98)δ
(1 −(cid:98)δ)(1 − γ+)+

1

≤ N (θ)
N (θ)

=

− 1 ≤

(cid:98)δ + γ+

(1 −(cid:98)δ)(1 − γ+)+

.

C. Proof of Lemma 2.5 on page 16

Consider u ∈ R+ that we will choose appropriately and write the decom-

position

1
n

n(cid:88)

i=1

n(cid:88)

i=1

Wi =

1
n

min{Wi, u} +

1
n

Using Bienaym´e Chebyshev’s inequality on one hand and Markov’s inequality
on the other hand, we obtain that with probability at least 1 − 2,

n(cid:88)

i=1

1
n

Wi ≤ E(W ) +

(cid:115)

E(cid:0)min{W, u}2(cid:1)

n

i=1

+.

n(cid:88)
(cid:0)Wi − u(cid:1)
E(cid:2)(cid:0)W − u(cid:1)

+



(cid:3)

+

.

March 17, 2016

Olivier Catoni

Let us now remark that

min{W, u}2 ≤ u2−qW q and(cid:0)W − u(cid:1)

+ ≤ (q − 1)q−1

qq

u−(q−1)W q.

57

necessarily above its tangent W (cid:55)→ W −u, and therefore also above(cid:0)W −u(cid:1)

The ﬁrst inequality is obvious. The second one can be checked remarking
that the two functions of W meet at W = u/(1 − q−1), where they have the
same derivative and that the function on the right-hand side being convex is
+,
since it is positive. More precisely, if we compute the solution of the contact
equations

Wc − u = aW q

c and 1 = qaW q−1

c

,

(cid:19)q−1

1
q

(cid:18) q − 1
(cid:114)

qu

(q − 1)q−1

qq

=

u−(q−1),

(q − 1)q−1E(cid:0)W q(cid:1)

qquq−1

.

we ﬁnd

Wc =

qu
q − 1

and a =

as claimed. Thus with probability at least 1 − 2,

n(cid:88)

i=1

1
n

Wi ≤ E(W ) +

u2−qE(W q)

+

n

The optimal value of the threshold u can be found computing the derivative
of the bound. It is

(q − 1)2E(cid:0)W q(cid:1)1/qn1/q

q2(1 − q/2)2/q1/q

.

u =

Replacing u by its value gives as stated in the lemma
qq−1E(W q)1/q

Wi ≤ E(W ) +

2(q − 1)q−1(1 − q/2)(2−q)/q1/qn1−1/q .

n(cid:88)

i=1

1
n

D. Proof of Proposition 3.3 on page 19

Let us put

(cid:101)N (θ, ξ) = inf

(cid:40)

ρ ∈ R∗
+,

(cid:20)

ψ

n(cid:88)

i=1

λ

(cid:18)(cid:0)(cid:104)θ, X(cid:105) − ξ(cid:1)2

(cid:19)(cid:21)

− 1

(cid:41)

≤ 0

.

ρ

From Lemma 3.2 on page 19, for any (θ, ξ) ∈ Rd+1,

E(cid:2)(cid:0)(cid:104)θ, X(cid:105) − ξ(cid:1)4(cid:3) ≤ (κ1/2 + 1)2 E(cid:2)(cid:0)(cid:104)θ, X(cid:105) − ξ(cid:1)2(cid:3)2.

March 17, 2016

Olivier Catoni

58

We can therefore apply Proposition 1.1 on page 7 and obtain for any sample
size n satisfying equation (10) on page 19 that with probability at least 1−2,

for any (θ, ξ) ∈ Rd+1,(cid:12)(cid:12)(cid:12)(cid:12) E(cid:2)(cid:0)(cid:104)θ, x(cid:105) − ξ(cid:1)2(cid:3)
(cid:101)N (θ, ξ)

(cid:12)(cid:12)(cid:12)(cid:12) ≤ µ

1 − 2µ

.

− 1

We can then apply Proposition 3.1 on page 18 to conclude.

E. Proof of lemma 3.4 on page 20

Consider a = E(cid:0)(cid:104)θ, X − E(X)(cid:105)2(cid:1)1/2. If a = 0, then almost surely (cid:104)θ, Xi −

E(X)(cid:105) = 0, so that in this case

(cid:104)θ, Xi(cid:105) − ξ = (cid:104)θ, E(X)(cid:105) − ξ,

E(cid:2)(cid:0)(cid:104)θ, X(cid:105) − ξ(cid:1)2(cid:3) =(cid:0)(cid:104)θ, E(X)(cid:105) − ξ(cid:1)2 ≤ 1.

and

Consequently (cid:104)θ, Xi(cid:105) − ξ ≤ 1.

in R2,

Assume now that a > 0. In this case, from the Cauchy-Schwarz inequality

(cid:0)(cid:104)θ, Xi(cid:105) − ξ(cid:1)2 =(cid:0)aa−1(cid:104)θ, Xi − E(X)(cid:105) + (cid:104)θ, E(X)(cid:105) − ξ(cid:1)2
≤(cid:2)a−2(cid:104)θ, Xi − E(X)(cid:105)2 + 1(cid:3)(cid:2)a2 +(cid:0)(cid:104)θ, E(X)(cid:105) − ξ(cid:1)2(cid:3).
Remark that almost surely Xi − E(X) ∈ Im(Σ), so that (cid:0)Xi − E(X)(cid:1) =
Σ1/2Σ−1/2(cid:0)Xi − E(X)(cid:1), and therefore,
(cid:104)θ, Xi − E(X)(cid:105)2 = (cid:104)Σ1/2θ, Σ−1/2(cid:0)Xi − E(X)(cid:105)2
≤ (cid:107)Σ1/2θ(cid:107)2(cid:107)Σ−1/2(Xi − E(X))(cid:107)2 = a2(cid:107)Σ−1/2(cid:0)Xi − E(X)(cid:1)(cid:107)2.
a2 +(cid:0)(cid:104)θ, E(X) − ξ(cid:1)2 = E(cid:0)(cid:104)θ, X − E(X)(cid:105)2(cid:1) +(cid:0)(cid:104)θ, E(X) − ξ(cid:1)2
= E(cid:2)(cid:0)(cid:104)θ, X(cid:105) − ξ(cid:1)2(cid:3) ≤ 1.
(cid:17)
(cid:0)(cid:104)θ, Xi(cid:105) − ξ(cid:1)2 ≤(cid:16)(cid:13)(cid:13)Σ−1/2(cid:0)Xi − E(X)(cid:1)(cid:13)(cid:13)2 + 1

Putting the three last equations together, we obtain that

On the other hand,

,

which achieves the proof.

March 17, 2016

Olivier Catoni

F. Proof of Proposition 4.9 on page 34

59

Let us put in this proof

R(θ) = E(cid:2)(cid:0)(cid:101)Y − (cid:104)θ, (cid:101)X(cid:105)(cid:1)2(cid:3) = p−1E(cid:2)(cid:0)Y − (cid:104)θ, X(cid:105)(cid:1)2(cid:3).

i=1

i=1

i ,

1
n

and

W =

1√
nσ

n(cid:88)

Remark that

(cid:101)Xi(cid:101)X(cid:62)

Let η1, . . . , ηn be n independent copies of η, independent from (cid:101)X1, . . . , (cid:101)Xn
and such that (cid:101)Yi = (cid:104)θ∗, (cid:101)Xi(cid:105) + ηi. Consider
n(cid:88)
(cid:101)d = rank((cid:101)G).
ηi(cid:101)G−1/2(cid:101)Xi,
(cid:101)G =
= N(cid:0)0,(cid:101)G−1(cid:101)G(cid:1),
where (cid:101)G−1 is the pseudo inverse of (cid:101)G, so that (cid:101)G−1(cid:101)G is the orthogonal pro-
jection on Im((cid:101)G). Applying Chernoﬀ’s bound, we get
(cid:21)(cid:12)(cid:12)(cid:12) (cid:101)X1, . . . , (cid:101)Xn
and therefore, integrating with respect to (cid:101)X1, . . . , (cid:101)Xn,
(cid:21)(cid:19)

log(1 + α) − log(−1)

(cid:107)W(cid:107)2 ≥ 2
α

W | (cid:101)X1,...,(cid:101)Xn

≥ 1 − ,

(cid:20)(cid:101)d

(cid:19)

(cid:18)

P

P

2

log(1 + α) − log(−1)

≥ 1 − .

(29)

(cid:33)

(cid:101)Yi(cid:101)Xi

+ ξ,

2

P

(cid:18)

Remark now that

(cid:107)W(cid:107)2 ≥ 2
α

(cid:20)(cid:101)d
(cid:101)θ = (cid:101)G−1
where ξ ∈ Ker((cid:101)G). Therefore
(cid:101)θ = (cid:101)G−1
n(cid:88)

n(cid:88)

We see that

(cid:32)

1
n

i=1

(cid:104)θ∗, Xi(cid:105)Xi + ηiXi

(cid:32)

1
n

n(cid:88)
(cid:33)

i=1

+ ξ = (cid:101)G−1(cid:101)Gθ∗ +

n(cid:88)

i=1

1
n

ηi(cid:101)G−1(cid:101)Xi + ξ,

1
n

i=1

(cid:104)(cid:101)θ − θ∗, (cid:101)Xi(cid:105)2 =(cid:0)(cid:101)θ − θ∗(cid:1)(cid:62)(cid:101)G(cid:0)(cid:101)θ − θ∗(cid:1) =(cid:13)(cid:13)(cid:101)G1/2(cid:0)(cid:101)θ − θ∗(cid:1)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)2
ηi(cid:101)G−1/2(cid:101)Xi

(cid:13)(cid:13)(cid:13)(cid:13) 1

n(cid:88)

=

n

i=1

=

σ2
n

(cid:107)W(cid:107)2.

March 17, 2016

Olivier Catoni

60

When n ≥ n, with probability at least 1 − 2, equation (19) on page 34 is

satisﬁed, so that (cid:101)d = rank((cid:101)G) = rank(G) = d and
R((cid:101)θ)−R(θ∗) = E(cid:0)(cid:104)(cid:101)θ−θ∗, X(cid:105)2(cid:1) ≥

n(cid:88)
(cid:104)(cid:101)θ−θ∗, Xi(cid:105)2 =

1

σ2

(1 + δ) n

(cid:107)W(cid:107)2.

(1 + δ) n

i=1

Combining this inequality with equation (29), we obtain that with probability
at least 1 − 3,

R((cid:101)θ) − R(θ∗) ≥

σ2

(1 + δ) αn

(cid:2)d log(1 + α) − 2 log(−1)(cid:3).

Choosing for simplicity α = 1 gives the ﬁrst statement of the proposition.

Consider σ1, . . . , σn, n independent copies of σ, independent of

((cid:101)X1,(cid:101)Y1), . . . , ((cid:101)Xn,(cid:101)Yn),

and such that (Xi, Yi) = σi((cid:101)Xi,(cid:101)Yi). Deﬁne
n(cid:88)

p =

1
n

σi.

i=1

Conditioning with respect to σ1, . . . , σn, we deduce from the ﬁrst part of the
proposition that

We proved in [11, Proposition 20.1 page 289] that with probability at least
1 − 2,

P

(1 + δ)pn

R((cid:98)θ) − R(θ∗) ≥ σ2(cid:2)d log(2) − 2 log(−1)(cid:3)
(cid:18)

(cid:19)
(cid:12)(cid:12)(cid:12) n ≤ np ≤ 7np/4
(cid:12)(cid:12)p − p(cid:12)(cid:12) ≤(cid:112)2 log(−1)p/n + 2 log(−1)/n.
n ≥ 8 log(−1)
with probability at least 1 − 2,

8(κ1 + κ2) log(−1)

(cid:101)κ1 +(cid:101)κ2

Therefore, when

=

p

,

≥ 1 − 3.

When moreover

p/4 ≤ p ≤ 7p/4.

n ≥ 4n
p

=

4(κ1 + κ2)n

(cid:101)κ1 +(cid:101)κ2

.

March 17, 2016

Olivier Catoni

as required in the proposition, with probability at least 1 − 2,

61

n ≤ np ≤ 7np/4.

We can then write that

R((cid:98)θ) − R(θ∗) ≥ 4σ2(cid:2)d log(2) − 2 log(−1)(cid:3)
(cid:18)

P

(cid:19)

= P

≥ P

7(1 + δ)pn

R((cid:98)θ) − R(θ∗) ≥ 4σ2(cid:2)d log(2) − 2 log(−1)(cid:3)
(cid:18)

7(1 + δ)pn

and n ≤ np ≤ 7np/4

(cid:19)
R((cid:98)θ) − R(θ∗) ≥ 4σ2(cid:2)d log(2) − 2 log(−1)(cid:3)
(cid:18)
(cid:19)
(cid:12)(cid:12)(cid:12) n ≤ np ≤ 7np/4
(cid:17)
R((cid:98)θ) − R(θ∗) ≥ σ2(cid:2)d log(2) − 2 log(−1)(cid:3)
(cid:18)
(cid:19)
(cid:12)(cid:12)(cid:12) n ≤ np ≤ 7np/4
E(cid:2)(cid:0)Y − (cid:104)θ, X(cid:105)(cid:1)2(cid:3) = pR(θ) and E(cid:2)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:3) = pR(θ∗) = pσ2,

(1 − 2)
≥ (1 − 3)(1 − 2) ≥ 1 − 5.

n ≤ np ≤ 7np/4

7(1 + δ)pn

(1 + δ)pn

× P

(cid:16)

We conclude the proof of equation (20) on page 35 by remarking that

≥ P

Applying Proposition 2.3 on page 14 to a Gaussian design, we obtain, for

any parameter α ∈]0, 1[, that

We can then use the fact that the log is concave and that for any constant
c > 0

(cid:18)
log(n) = 3 log(cid:0)n1/3(cid:1) ≤ 3

log(c) +

= 3

log(c/e) +

(cid:19)

.

n1/3

c

March 17, 2016

Olivier Catoni

and that

where

.

κ1 + κ2

p−1 =

(cid:101)κ1 +(cid:101)κ2
(cid:104) b√
(cid:98)γ+ ≤ (1 +(cid:98)δ)2
(cid:19)
(cid:18) 1

a = 0.73 d + log(−1),

+

n

a

3

b =

log

1 − α

d
α

(cid:105)2

,

2 log(n)

√
n

α

+

2
α

log(−1).

(cid:19)

(cid:18)

n1/3 − c

c

Therefore,(cid:98)γ+ ≤ χ when
(cid:32)b +

6
α

log(cid:0)c/e(cid:1)

n1/2

(cid:33)2

+

6

αcn1/6

≤

3χ

(1 +(cid:98)δ)2a

.

62

This condition is satisﬁed when

b +

This can be rewritten as

n1/2 ≥ 2

and n1/2 ≥

(cid:115)
(cid:115)

2

log(cid:0)c/e(cid:1)

6
α

6

and

,

.

3χ

3χ

n1/2

(cid:17)

(3χ)1/2

≤ 1
2

(1 +(cid:98)δ)2a
(1 +(cid:98)δ)2a
αcn1/6 ≤ 1
(cid:16)
(1 +(cid:98)δ)
b + 6 log(cid:0)c/e(cid:1)/α
(cid:19)3 (1 +(cid:98)δ)3
(cid:18) 12
(cid:0)3χ(cid:1)3/2 a3/2.
(cid:19)3(cid:0)1 +(cid:98)δ(cid:1)2
(cid:18)12
(1 +(cid:98)δ)2
a(cid:2)b + 6 log(cid:0)c/e(cid:1)/α(cid:3)2.

α

3χ

c3 =

1
2

αc

,

a1/2,

Due to the fact that b ≥ a, the ﬁrst condition implies the second one when

so that with this choice of constant c the condition becomes

n ≥ 4

3χ

gives the condition on n stated in the proposition. One can then check that

Taking χ = 1/4, α = 1/2,(cid:98)δ = 1/4 and working out the constants numerically
the condition on n necessary to obtain µ ≤ 1/6 and therefore (cid:98)δ ≤ 1/4 in
Proposition 2.3 on page 14 is weaker, so that we are entitled to take(cid:98)δ ≤ 1/4,
which gives(cid:98)γ+ ≤ χ = 1/4 and

(cid:98)δ +(cid:98)γ+
(1 −(cid:98)δ)(1 −(cid:98)γ+)

δ =

≤ 8
9

< 1

as required. The last equation of the proposition is then obtained by substi-
tuting this value in equation (20) on page 35.

March 17, 2016

Olivier Catoni

G. Proof of Proposition 4.10 on page 36

63

Consider the empirical Gram matrix

n(cid:88)

i=1

G =

1
n

XiX(cid:62)
i .

According to Proposition 2.2 on page 11, under the same assumptions as in
Proposition 1.1 on page 7, there is an event Ω of probability at least 1−  on
which for any θ ∈ Rd,

≥ 1 −(cid:98)δ + γ−
1 +(cid:98)δ

θ(cid:62)Gθ
θ(cid:62)Gθ

1 − γ−

1 +(cid:98)δ

.

=

(Given that we use only one side of the inequalities involved in Proposition
2.2 and that each side holds with probability at least 1 − , as is clear from
the proof of this proposition.) Remark that on Ω, Im(G) ⊂ Im(G). As
Im(G) ⊂ Im(G) almost surely, we may remove a set of measure zero from
Ω and assume that on Ω, Im(G) = Im(G). Since it does not change R(θ∗),
we may assume without loss of generality that θ∗ ∈ Im(G), by projecting it
on Im(G) if necessary. Remark then that for some ξ ∈ Ker(G) = Ker(G),

(cid:98)θ = G

−1(cid:18) 1

n(cid:88)

n

i=1

(cid:19)

YiXi

+ ξ = G

(cid:0)Yi − (cid:104)θ∗, Xi(cid:105)(cid:1)Xi

(cid:19)

n

i=1

(cid:19)

−1(cid:18) 1
n(cid:88)
−1(cid:18) 1
n(cid:88)
n(cid:88)
(cid:0)Yi − (cid:104)θ∗, Xi(cid:105)(cid:1)Xi.

(cid:104)θ∗, Xi(cid:105)Xi

i=1

n

−1

+ ξ = G

+ G

W + θ∗ + ξ,

(30)

and let us remark that on the event Ω, for any

Let us put ρ =
θ ∈ Im(G),

G−1/2θ(cid:13)(cid:13)2
(cid:13)(cid:13)θ(cid:13)(cid:13)2 =(cid:13)(cid:13)G1/2G−1/2θ(cid:13)(cid:13)2 ≤ ρ(cid:13)(cid:13)G
R((cid:98)θ) − R(θ∗) =(cid:13)(cid:13)G1/2(cid:0)(cid:98)θ − θ∗(cid:1)(cid:13)(cid:13)2 =(cid:13)(cid:13)G1/2G

= ρ(cid:13)(cid:13)(cid:0)G−1/2GG−1/2(cid:1)1/2θ(cid:13)(cid:13)2 ≤ ρ2(cid:13)(cid:13)G−1/2 G G−1/2 θ(cid:13)(cid:13)2.
W(cid:13)(cid:13)2 ≤ ρ2(cid:13)(cid:13)G−1/2W(cid:13)(cid:13)2.

Assuming that we are on the event Ω, we can then write

−1

1/2

March 17, 2016

Olivier Catoni

where we have introduced the notation

W =

1
n

i=1

1 +(cid:98)δ

1 − γ−

Thus,

(cid:104)(cid:0)R((cid:98)θ) − R(θ∗)(cid:1)1Ω

(cid:105) ≤ ρ2E

E

(cid:104)(cid:13)(cid:13)G−1/2W(cid:13)(cid:13)21Ω
(1 +(cid:98)δ)2

(1 − γ−)2 n

E

=

(cid:104)(cid:13)(cid:13)G−1/2W(cid:13)(cid:13)2(cid:105)

(cid:105) ≤ ρ2E
(cid:104)(cid:0)Y − (cid:104)θ∗, X(cid:105)(cid:1)2(cid:13)(cid:13)G−1/2X(cid:13)(cid:13)2(cid:105)

64

.

E

.

becomes

(cid:16)

≤ M  + E

Then if we take  = C/(M n2), the condition n ≥ O

b = 4aκ. Remarking that log(n) ≤ log(b/e) + n/b, we get that

(cid:104)
M 1Ωc +(cid:2)R((cid:98)θ) − R(θ∗)(cid:3)1Ω

This proves the second statement of the proposition. The ﬁrst statement is
deduced from the second one, writing that

(cid:104)
min(cid:8)R((cid:98)θ − R(θ∗), M(cid:9)(cid:105) ≤ E
(cid:104)(cid:0)R((cid:98)θ) − R(θ∗)(cid:1)1Ω
(cid:105)
κ(cid:2)d+log(−1)(cid:3)(cid:17)
n ≥ aκ(cid:2)d + log(cid:0)M n2/C(cid:1)(cid:3), for some constant a > 0. Consider the constant
aκ(cid:2)d + log(cid:0)M n2/C(cid:1)(cid:3) ≤ aκ(cid:2)d + log(cid:0)M/C) + 2 log(cid:0)b/e) + 2n/b(cid:3)
= aκ(cid:2)d + log(cid:0)M/C(cid:1) + 2 log(cid:0)4aκ/e(cid:1)(cid:3) + n/2,
κ(cid:2)d + log(cid:0)κM/C(cid:1)(cid:3)(cid:17)
n ≥ 2aκ(cid:2)d + log(cid:0)M/C(cid:1) + 2 log(cid:0)4aκ/e(cid:1)(cid:3) = O
and such that E(η) = 0 and E(cid:0)σ2(cid:1) = σ2. The event Ω described above
E(cid:2)R((cid:98)θ) − R(θ∗)(cid:12)(cid:12) X1, . . . , Xn

(31)
Let us now assume that Y = (cid:104)θ∗, X(cid:105) + η, where η is independent from X

is measurable with respect to the sigma-algebra generated by (X1, . . . , Xn).
Reasoning as previously we see that on Ω,

so that the condition above is implied by the condition

W(cid:13)(cid:13)2(cid:12)(cid:12) X1, . . . , Xn

(cid:105)

(cid:3) ≤ ρ E

(cid:104)(cid:13)(cid:13)G

(cid:16)

−1/2

(cid:105)

.

The statement about E

(cid:104)
min(cid:8)R((cid:98)θ) − R(θ∗), M(cid:9)(cid:105)

= ρ

rank(G)σ2

n

≤ ρdσ2
n

.

is then deduced as above.

H. Proof of Proposition 4.12 on page 38

From Proposition 2.2 on page 11, we see that, when n ≥ n given by
equation 3 on page 7 on some event Ω of probability at least 1 − 2, for any

March 17, 2016

Olivier Catoni

65

θ ∈ Rd,

Let us put for short ρ+ =

θ(cid:107)2.

1/2

1/2

(cid:107)G

1 − γ−

θ(cid:107)2 ≤ (cid:107)G1/2θ(cid:107)2 ≤ 1 +(cid:98)δ
(1 −(cid:98)δ)(1 −(cid:101)γ+)(cid:107)G
1 +(cid:98)δ
and ρ− = (1 −(cid:98)δ)(1 −(cid:101)γ+).
R((cid:98)θ) − R(θ∗) =(cid:13)(cid:13)G1/2(cid:0)(cid:98)θ − θ∗(cid:1)(cid:13)(cid:13)2 =(cid:13)(cid:13)G1/2G
W(cid:13)(cid:13)2,

1 − γ−

−1

As already shown in a previous proof, the upper bound implies that Im(G) =
Im(G). We can for this reason write that, on Ω,

where the random variable W is deﬁned by equation (30) on page 63. More-
over, for any θ ∈ Im(G),

We have also a reverse inequality on Ω, that is proved as in Propsition 4.10
on page 36 (where the event Ω was larger), and writes as

(cid:107)θ(cid:107)2 = (cid:107)G1/2G−1/2θ(cid:107)2 ≥ ρ− (cid:107)G

Iterate this inequality to get, for any θ ∈ Im(G),

1/2

W , to get on the event Ω

(cid:107)θ(cid:107)2 ≥ ρ2−
−1
R((cid:98)θ) − R(θ∗) ≥ ρ2−

G−1/2θ(cid:107)2 = ρ−(cid:13)(cid:13)(cid:0)G−1/2GG−1/2(cid:1)1/2θ(cid:13)(cid:13)2.
(cid:13)(cid:13)G−1/2GG1/2θ(cid:107)2.
(cid:13)(cid:13)G−1/2W(cid:13)(cid:13)2.
(cid:13)(cid:13)G−1/2W(cid:13)(cid:13)2.
R((cid:98)θ) − R(θ∗) ≤ ρ2
(cid:16)(cid:107)G−1/2W(cid:13)(cid:13)21Ω
(cid:17) ≤ ρ2
(cid:16)(cid:2)R((cid:98)θ) − R(θ∗)(cid:3)1Ω
(cid:17)
(cid:16)(cid:107)G−1/2W(cid:13)(cid:13)2(cid:17)
(cid:16)(cid:2)R((cid:98)θ) − R(θ∗)(cid:3)1Ω
(cid:16)(cid:13)(cid:13)G−1/2W(cid:13)(cid:13)21Ω
(cid:17) ≥ ρ2−E
(cid:17)
(cid:17)(cid:105)
(cid:16)(cid:13)(cid:13)G−1/2W(cid:13)(cid:13)21Ωc
(cid:104)
(cid:16)(cid:13)(cid:13)G−1/2W(cid:13)(cid:13)2(cid:17) − E
P(cid:0)Ωc(cid:1)1/2(cid:105)
(cid:16)(cid:107)G−1/2W(cid:13)(cid:13)4(cid:17)1/2
(cid:16)(cid:13)(cid:13)G−1/2W(cid:13)(cid:13)2(cid:17) − E
(cid:104)
≥ ρ2−

≤ ρ2

= ρ2−

E

E

E

E

+

+

+

,

Apply this to θ = G1/2G

Consequently E

and E

March 17, 2016

Olivier Catoni

where we have used the Cauchy-Schwarz inequality. Let us put

n(cid:88)

i=1

1
n

G−1/2W =

Zi,

= E

where

random vectors Zi are i.i.d. Compute

Zi =(cid:0)Yi − (cid:104)θ∗, Xi(cid:105)(cid:1)G−1/2Xi.
(cid:16)(cid:10)G−1/2W, G−1/2W(cid:11)(cid:17)
n(cid:88)
(cid:16)(cid:10)G−1/2W, G−1/2W(cid:11)2(cid:17)
(cid:34)(cid:18) n(cid:88)
(cid:88)

Remark that E(Zi) = 0, E(cid:0)(cid:107)Zi(cid:107)2(cid:1) = C, E(cid:0)(cid:107)Zi(cid:107)4(cid:1) = κ(cid:48)C 2 and that the
(cid:16)(cid:13)(cid:13)G−1/2W(cid:13)(cid:13)2(cid:17)
(cid:16)(cid:13)(cid:13)G−1/2W(cid:13)(cid:13)4(cid:17)
(cid:32) n(cid:88)
(cid:32) n(cid:88)

(cid:16)(cid:107)Zi(cid:107)2(cid:17)
(cid:19)2(cid:35)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:107)Zi(cid:107)2(cid:107)Zj(cid:107)2 + 4

(cid:107)Zi(cid:107)4 + 2

(cid:107)Zi(cid:107)2 + 2

(cid:104)Zi, Zj(cid:105)2

(cid:104)Zi, Zj(cid:105)

1≤i<j≤n

1≤i<j≤n

1≤i<j≤n

1
n4

1
n4

C
n

,

E

E

1
n2

= E

i=1

i=1

=

E

=

E

i=1

=

=

E

≤ 1
n4

E

i=1

(cid:107)Zi(cid:107)4 + 2

(cid:107)Zi(cid:107)2(cid:107)Zj(cid:107)2 + 4
nκ(cid:48) + 3n(n − 1)

1≤i<j≤n

=

1≤i<j≤n

C 2 = 3

(cid:18)

(cid:107)Zi(cid:107)2(cid:107)Zj(cid:107)2
κ(cid:48) − 3
3n

1 +

n4

66

(cid:33)
(cid:33)
(cid:19)C 2

n2 .

+ C
n P(Ω)

,

This proves that

ρ2−

(cid:16)

E

(cid:20)

(cid:18)

(cid:21)C

(cid:19)1/2

n

1 +

1/2

1 −

≤ E

√
6

κ(cid:48) − 3
3n

(cid:16)(cid:2)R((cid:98)θ) − R(θ∗)(cid:3)1Ω
(cid:17)
(cid:17) ≤ ρ2
(cid:16)
R((cid:98)θ) − R(θ∗)| Ω
(cid:17)
(cid:16)
min(cid:8)R((cid:98)θ) − R(θ∗), M(cid:9)(cid:17) ≥ E
min(cid:8)R((cid:98)θ) − R(θ∗), M(cid:9)1Ω
(cid:13)(cid:13)G−1/2W(cid:13)(cid:13)2, M(cid:9)1Ω
min(cid:8)ρ2−
(cid:16)
min(cid:8)(cid:13)(cid:13)G−1/2W(cid:13)(cid:13)2, M(cid:9)1Ω

(cid:16)
≥ E
≥ ρ2−E

(cid:17)
(cid:17)

≤ E

as stated in equation (21) on page 39 of the proposition. Now

March 17, 2016

Olivier Catoni

(cid:20)

≥ ρ2−

E

≥ ρ2−

(cid:16)
(cid:34)

min(cid:8)(cid:13)(cid:13)G−1/2W(cid:13)(cid:13)2, M(cid:9)(cid:17) − E
(cid:16)(cid:13)(cid:13)G−1/2W(cid:13)(cid:13)2(cid:17) − E

E

67

(cid:17)(cid:21)

(cid:16)(cid:13)(cid:13)G−1/2W(cid:13)(cid:13)21Ωc
(cid:17)
(cid:105)
(cid:104)(cid:16)(cid:13)(cid:13)G−1/2W(cid:13)(cid:13)2 − M
(cid:16)(cid:13)(cid:13)G−1/2W(cid:13)(cid:13)4(cid:17)1/2
P(cid:0)Ωc(cid:1)1/2
(cid:19) C 2
(cid:18)

− E

(cid:35)

+

.

=

3
4M

κ(cid:48) − 3
3n

1 +

(cid:16)(cid:13)(cid:13)G−1/2W(cid:13)(cid:13)4(cid:17)

(cid:19) C 2
n2 −

√

6C
n

(cid:18)

1 +

κ(cid:48) − 3
3n

κ(cid:48) − 3
3n

(cid:19)1/2

n2

(cid:35)

1/2

,

Remark that for any z ∈ R, (z − M )+ ≤ z2/(4M ). Thus

E

4M

E

and

E

(cid:105) ≤ 1
(cid:17)
(cid:104)(cid:16)(cid:13)(cid:13)G−1/2W(cid:13)(cid:13)2 − M
min(cid:8)R((cid:98)θ) − R(θ∗), M(cid:9)(cid:17)
(cid:16)
(cid:18)

(cid:34)

+

≥ ρ2−

C
n

− 3
4M

1 +

that proves equation (22) of the proposition. The end of the proposition is
straightforward, the evaluation of n in big O notation when  = n−(q−1)
being done using the same principle as in the proof of equation (31) on page
64.

I. Obtaining a quadratic form

In this section, we will see how to deduce from the estimator of Propo-
sition 1.1 on page 7, that is not a quadratic form, a quadratic estimator, or

equivalently an estimator of the Gram matrix G = E(cid:0)XX(cid:62)(cid:1) by a symmetric
non-negative matrix (cid:98)G.
(cid:98)N such that for some  and δ ∈]0, 1/2[, on some event Ω(cid:48) of probability at

Let us assume that we derived as in Proposition 1.1 on page 7 an estimator

least 1 − , for any θ ∈ Rd,

(cid:12)(cid:12)(cid:12)(cid:12) ≤ δ.
(cid:12)(cid:12)(cid:12)(cid:12)N (θ)(cid:98)N (θ)
(cid:9), (cid:98)N (θ) = 0.
such that θ ⊥ span(cid:8)X1, . . . , Xn

− 1

Assume moreover that, as it is the case in Proposition 1.1, for any θ ∈ Rd

(32)

March 17, 2016

Olivier Catoni

68

Let us remark that equation (32) implies that on Ω(cid:48)

Ker(G) =(cid:8)θ ∈ Rd : N (θ) = 0(cid:9) =(cid:8)θ ∈ Rd : (cid:98)N (θ) = 0(cid:9).

Remark then that our second assumption implies that on Ω(cid:48)

span(cid:8)X1, . . . , Xn}⊥ ⊂ Ker(G).
(cid:9)⊥
Ker(G) ⊂ span(cid:8)X1, . . . , Xn

Remark moreover that almost surely

since for any θ in a basis of Ker(G) (that is a ﬁnite set), E(cid:0)(cid:104)θ, X(cid:105)2(cid:1) = 0, so

,

This proves that under our assumptions, on some event Ω(cid:48)(cid:48) of the same

that almost surely (cid:104)Xi, θ(cid:105)2 = 0, 1 ≤ i ≤ n.
probability as Ω(cid:48),

Im(G) = span(cid:8)X1, . . . , Xn

(cid:9),

sup

inf
ξ∈Θρ

Consider some positive parameter ρ, let Sd =(cid:8)θ ∈ Rd : (cid:107)θ(cid:107)= 1(cid:9) be the

so that we have an easily computable estimator of Im(G) that is exact on
Ω(cid:48)(cid:48), an event of probability at least 1 − .
unit sphere of Rd, and Θρ some arbitrary ρ-net of span{X1, . . . , Xn} ∩ Sd.
In other words, assume that Θρ is a ﬁnite subset of span{X1, . . . , Xn} ∩ Sd
such that

(cid:107)θ − ξ(cid:107) : θ ∈ span(cid:8)X1, . . . , Xn

(cid:110)
(cid:111) ≤ ρ.
Assume now that (cid:98)G ∈ Rd×d is a random symmetric matrix solution of
(cid:110)
(cid:98)G = arg min

Im(H) ⊂ span(cid:8)X1, . . . , Xn
(cid:98)N (θ)(1 − δ) ≤ θ(cid:62)H θ ≤ (cid:98)N (θ)(1 + δ),

(cid:111)
itself satisﬁes the constraints. We will see below in more detail that (cid:98)G is the

This minimization problem has a solution on Ω(cid:48)(cid:48), since in this case G

Tr(H 2) : H ∈ Rd×d, H(cid:62) = H,

(cid:9) ∩ Sd

θ ∈ Θρ

(cid:9),

H

.

solution of a convex minimization problem very similar to the one appearing
in the estimation of the parameters of a support vector machine using the
popular box constraint learning algorithm.

Proposition I.1 The symmetric matrix (cid:98)G is such that on the event Ω(cid:48)(cid:48) of
probability at least 1 − , Im((cid:98)G) ⊂ Im(G) and for any θ ∈ Rd,

(cid:12)(cid:12)(cid:12)θ(cid:62)(cid:98)Gθ − N (θ)

(cid:12)(cid:12)(cid:12) ≤ 2δ

1 − δ

N (θ) +

4ρ(cid:112)Tr(G2)

(1 − δ)

(cid:107)θ(cid:107)2.

March 17, 2016

Olivier Catoni

The positive part (cid:98)G+ of (cid:98)G is such that on the event Ω(cid:48)(cid:48) of probability at least

1 − , for any θ ∈ Rd,

(cid:12)(cid:12)(cid:12)θ(cid:62)(cid:98)G+θ − N (θ)

(cid:12)(cid:12)(cid:12) ≤ 2δ

1 − δ

N (θ) +

6ρ(cid:112)Tr(G2)

(1 − δ)

(cid:107)θ(cid:107)2.

69

where

Moreover,

Proof. During all this proof, we will assume that the event Ω(cid:48)(cid:48) deﬁned
above is satisﬁed, so that the results will hold with probability at least 1− .
Let us also assume ﬁrst that θ ∈ Im(G)∩ Sd. Recall that on Ω(cid:48)(cid:48), Im(G) =
Since Θρ is a ρ-net of Im(G) ∩ Sd, there is ξ in Θρ such that (cid:107)θ − ξ(cid:107) ≤ ρ.

span{X1, . . . , Xn}, so that by construction of (cid:98)G, Im((cid:98)G) ⊂ Im(G).
(cid:12)(cid:12)(cid:12) ≤ 2ρ(cid:107)(cid:98)G(cid:107)∞,
Consequently,(cid:12)(cid:12)(cid:12)θ(cid:62)(cid:98)Gθ − ξ(cid:62)(cid:98)Gξ
is the operator norm—and spectral radius, since (cid:98)G is symmetric—of (cid:98)G.
(cid:12)(cid:12)(cid:12)ξ(cid:62)(cid:98)Gξ − N (θ)

(cid:12)(cid:12)(cid:12) =
(cid:12)(cid:12)(cid:12)(θ + ξ)(cid:62)(cid:98)G(θ − ξ)
(cid:110)
θ ∈ Sd : (cid:107)(cid:98)Gθ(cid:107)(cid:111)
(cid:107)(cid:98)G(cid:107)∞ = sup
(cid:12)(cid:12)(cid:12) ≤(cid:12)(cid:12)(cid:12)ξ(cid:62)(cid:98)Gξ − (cid:98)N (ξ)
(cid:12)(cid:12)(cid:12) +
(cid:12)(cid:12)(cid:12)(cid:98)N (ξ) − N (ξ)
(cid:12)(cid:12)(cid:12) +
≤ 2δ(cid:98)N (ξ) + 2ρ(cid:107)G(cid:107)∞ ≤ 2δ
(cid:17)
(cid:16)
Remark now that (cid:107)(cid:98)G(cid:107)∞ ≤ (cid:113)
(cid:12)(cid:12)(cid:12) ≤ 2δ
(cid:12)(cid:12)(cid:12)θ(cid:62)(cid:98)Gθ − N (θ)
Recall that Im((cid:98)G−) ⊂ Ker((cid:98)G+) ∩ Im(G), so that for any θ ∈ Im((cid:98)G−) ∩ Sd,
θ(cid:62)(cid:98)G−θ = −θ(cid:62)(cid:98)Gθ ≤ −ξ(cid:62)(cid:98)Gξ + 2ρ(cid:107)(cid:98)G(cid:107)∞
(cid:113)
Tr((cid:98)G2) ≤ 2ρ(cid:112)Tr(G2).
≤ −(1 − δ)(cid:98)N (ξ) + 2ρ

ξ(cid:62)Gξ + 2ρ(cid:107)G(cid:107)∞
+ 2ρ(cid:107)G(cid:107)∞
2δ
1 − δ

N (θ) +

there is ξ ∈ Θρ such that (cid:107)θ − ξ(cid:107) ≤ ρ. Consequently

=

Tr((cid:98)G2) ≤ (cid:113)

(cid:12)(cid:12)(cid:12)ξ(cid:62)Gξ − θ(cid:62)Gθ

(cid:12)(cid:12)(cid:12)

1 − δ
θ(cid:62)Gθ + 2ρ(cid:107)G(cid:107)∞

(cid:112)Tr(G2),

2(1 + δ)
1 − δ

ρ(cid:107)G(cid:107)∞.

from the two above inequalities that

1 − δ

N (θ) +

4ρ
1 − δ

Tr(G2), so that we can deduce

θ ∈ Im(G) ∩ Sd.

≤ 2δ
1 − δ

March 17, 2016

Olivier Catoni

70

N (θ) +

N (θ) +

1 − δ
1 − δ

θ ∈ Im(G) ∩ Sd.

1 − δ

N (θ) +

6ρ
1 − δ

4ρ
1 − δ
6ρ
1 − δ

(cid:112)Tr(G2),

θ ∈ Im(G),
θ ∈ Im(G).

By homogeneity, we get on the event Ω(cid:48)(cid:48) that

(cid:112)Tr(G2)(cid:107)θ(cid:107)2,
(cid:112)Tr(G2)(cid:107)θ(cid:107)2,

Let us now deal with the general case of an arbitrary θ ∈ Rd. We can
decompose it into θ = θ1 + θ2, where θ1 ∈ Im(G) and θ2 ∈ Ker(G). Since

This proves that (cid:107)(cid:98)G−(cid:107)∞ ≤ 2ρ(cid:112)Tr(G2). As a consequence,
(cid:12)(cid:12)(cid:12) ≤(cid:12)(cid:12)(cid:12)θ(cid:62)(cid:98)G+θ − θ(cid:62)(cid:98)Gθ
(cid:12)(cid:12)(cid:12) +
(cid:12)(cid:12)(cid:12)θ(cid:62)(cid:98)G+θ − N (θ)
(cid:12)(cid:12)(cid:12)θ(cid:62)(cid:98)Gθ − N (θ)
(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12) ≤ 2δ
(cid:12)(cid:12)(cid:12)θ(cid:62)(cid:98)Gθ − N (θ)
= θ(cid:62)(cid:98)G−θ +
(cid:12)(cid:12)(cid:12) ≤ 2δ
(cid:12)(cid:12)(cid:12)θ(cid:62)(cid:98)Gθ − N (θ)
(cid:12)(cid:12)(cid:12) ≤ 2δ
(cid:12)(cid:12)(cid:12)θ(cid:62)(cid:98)G+θ − N (θ)
θ1 and θ2 are orthogonal, and since Im((cid:98)G+) ⊂ Im((cid:98)G) ⊂ Im(G) on Ω(cid:48)(cid:48),
θ2 ∈ Ker((cid:98)G) ⊂ Ker((cid:98)G+), so that
θ(cid:62)(cid:98)Gθ = θ1(cid:98)Gθ1,
1 (cid:98)G+θ1, and θ(cid:62)Gθ = θ(cid:62)
θ(cid:62)(cid:98)G+θ = θ(cid:62)
(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)θ(cid:62)
(cid:12)(cid:12)(cid:12)θ(cid:62)(cid:98)G+θ − N (θ)
(cid:12)(cid:12)(cid:12) =
1 (cid:98)G+θ1 − N (θ1)
(cid:112)Tr(G2)(cid:107)θ1(cid:107)2
(cid:112)Tr(G2)(cid:0)(cid:107)θ(cid:107)2 − (cid:107)θ2(cid:107)2(cid:1)
(cid:112)Tr(G2)(cid:107)θ(cid:107)2,
(cid:112)Tr(G2)(cid:107)θ(cid:107)2,

≤ 2δ
1 − δ
2δ
1 − δ
N (θ) +
≤ 2δ
1 − δ

Therefore, on the event Ω(cid:48)(cid:48) of probability at least 1 − ,

(cid:12)(cid:12)(cid:12)θ(cid:62)(cid:98)Gθ − N (θ)

(cid:12)(cid:12)(cid:12) ≤ 2δ

and, due to a similar chain of inequalities,

1 − δ

N (θ) +

4ρ

(1 − δ)

N (θ) +

6ρ
1 − δ

N (θ1) +

6ρ
1 − δ

θ ∈ Rd.

θ ∈ Rd,

6ρ
1 − δ

1 Gθ1.

=

(cid:3)

Corollary I.2 Introduce λmin = inf
> 0, the
smallest non zero eigenvalue of G. On the event Ω(cid:48)(cid:48) of probability at least
1 − , for any θ ∈ Rd,

N (θ) : θ ∈ Im(G) ∩ Sd

(cid:12)(cid:12)(cid:12)(cid:12)θ(cid:62)(cid:98)Gθ

θ(cid:62)Gθ

(cid:12)(cid:12)(cid:12)(cid:12) ≤ 1

1 − δ

− 1

4ρ(cid:112)Tr(G2)

(cid:19)

.

λmin

2δ +

(cid:110)
(cid:18)

(cid:111)

March 17, 2016

Olivier Catoni

As a consequence, when

1
1 − δ

(cid:18)

that is when

2δ +

λmin

(1 − 3δ)λmin

4ρ(cid:112)Tr(G2)
4(cid:112)Tr(G2)
δ2(cid:98)λmin
2(cid:80)d
i=1 (cid:98)N (ei)

,

,

(cid:98)G+ = (cid:98)G on Ω(cid:48)(cid:48). Moreover, if we choose ρ small enough, and more precisely

ρ <

such that

ρ ≤

where (e1, . . . , ed) is some arbitrary orthonormal basis of Rd, and

then on Ω(cid:48)(cid:48), and therefore with probability at least 1 − ,

(cid:98)λmin = inf

(cid:111)

,

(cid:110)(cid:98)N (θ) : θ ∈ span{X1, . . . , Xn} ∩ Sd
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) θ(cid:62)(cid:98)Gθ

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ 2δ

1 − 2δ

θ(cid:62)Gθ

− 1

.

Proof. To prove the ﬁrst inequality of the corollary, consider any θ ∈ Rd
and write it as θ = θ1 + θ2, where θ1 ∈ Im(G) and θ2 ∈ Ker(G). If θ1 = 0,

since we saw that on Ω(cid:48)(cid:48), Im((cid:98)G) ⊂ Im(G), then θ(cid:62)(cid:98)Gθ = 0, so that with the

convention used throughout this paper

71

(cid:19)

< 1,

θ(cid:62)(cid:98)Gθ

θ(cid:62)Gθ

θ(cid:62)(cid:98)Gθ

θ(cid:62)Gθ

=

0
0

= 1.

1 (cid:98)Gθ1

θ(cid:62)
θ(cid:62)
1 Gθ1

.

=

(cid:107)θ1(cid:107)2 ≤ λ−1

minN (θ1),

Otherwise, θ(cid:62)Gθ = θ(cid:62)
event Ω(cid:48)(cid:48),

1 Gθ1 > 0, and, as seen in the previous proof, on the

Moreover

so that on Ω(cid:48)(cid:48), according to the previous proposition,

(cid:12)(cid:12)(cid:12)θ(cid:62)(cid:98)Gθ

θ(cid:62)Gθ

(cid:12)(cid:12)(cid:12) ≤ 2δ

1 − δ

+

4ρ(cid:112)Tr(G2)

1 − δ

− 1

× (cid:107)θ1(cid:107)2

N (θ1)

≤ 1
1 − δ

(cid:18)

2δ +

4ρ(cid:112)Tr(G2)

λmin

(cid:19)

.

March 17, 2016

Olivier Catoni

To prove the end of the corollary, remark that on Ω(cid:48)(cid:48)

(cid:98)λmin(1 − δ) ≤ λmin ≤(cid:98)λmin(1 + δ)

since span{X1, . . . , Xn} = Im(G) on Ω(cid:48)(cid:48) and since

λmin = inf

N (θ) : θ ∈ Im(G) ∩ Sd

(cid:110)

72

(cid:111)

.

Remark also that on the event Ω(cid:48)(cid:48). for any orthonormal basis (e1, . . . , ed) of

Rd,(cid:112)Tr(G2) ≤(cid:112)(cid:107)G(cid:107)∞ Tr(G) ≤ Tr(G) =

d(cid:88)

i=1

≤ (1 + δ)

N (ei)

d(cid:88)

i=1

Tr(G),

(1 − δ)

(cid:98)N (ei) ≤ (1 + δ)
(cid:19)

.

2δ +

− 1

(cid:18)

θ(cid:62)Gθ

1 − δ

When ρ ≤

4ρ(cid:80)d

where (cid:107)G(cid:107)∞ = supθ∈Sd

(cid:107)Gθ(cid:107) is the operator norm (and spectral radius) of G.

Therefore on Ω(cid:48)(cid:48), for any θ ∈ Rd,

(cid:12)(cid:12)(cid:12)(cid:12) ≤ 1
(cid:18)

(cid:12)(cid:12)(cid:12)(cid:12) θ(cid:62)(cid:98)Gθ
δ2(cid:98)λmin
2(cid:80)d
i=1 (cid:98)N (ei)
(cid:12)(cid:12)(cid:12)(cid:12) θ(cid:62)(cid:98)Gθ
(cid:12)(cid:12)(cid:12)(cid:12) ≤ 1
Proposition I.3 The estimator (cid:98)G studied in the previous proposition can

i=1 (cid:98)N (ei)(1 + δ)
(cid:98)λmin(1 − δ)
(cid:19)

2δ(1 + δ2)
1 + δ2 − 2δ

2δ2(1 + δ)
(1 − δ)

≤ 2δ
1 − 2δ

.

, we obtain

1 − δ

− 1

θ(cid:62)Gθ

(cid:3)

2δ +

=

(cid:98)G =

(cid:88)

θ∈Θρ

(cid:2)(cid:98)ξ+(θ) −(cid:98)ξ−(θ)(cid:3)θθ(cid:62)

be expressed as

where(cid:2)(cid:98)ξ+(θ),(cid:98)ξ−(θ)(cid:3)

θ∈Θρ

∈ arg

(cid:88)

(θ,θ(cid:48))∈Θ2

ρ

− 1
2

max

(ξ+, ξ−) ∈ (R2

(cid:2)ξ+(θ) − ξ−(θ)(cid:3)(cid:2)ξ+(θ(cid:48)) − ξ−(θ(cid:48))(cid:3)(cid:104)θ, θ(cid:48)(cid:105)2

+)Θρ

ξ+(θ)(cid:98)N (θ)(1 − δ) − ξ−(θ)(cid:98)N (θ)(1 + δ).

(cid:88)

θ∈Θρ

+

March 17, 2016

Olivier Catoni

73

Proof of Proposition I.3. Let us put

B−(θ) = (cid:98)N (θ)(1 − δ)

The estimated matrix (cid:98)G is solution of the minmax optimisation problem

and B+(θ) = (cid:98)N (θ)(1 + δ).
(ξ+,ξ−)∈(cid:0)R2
ξ+(θ)(cid:2)B−(θ)−θ(cid:62)Hθ(cid:3)+ξ−(θ)(cid:2)θ(cid:62)Hθ−B+(θ)(cid:3).

V (H, ξ+, ξ−),

(cid:1)Θ

sup

+

V = inf

H,H(cid:62)=H

where

V (H, ξ+, ξ−) =

1
2

Tr(H 2)+

(cid:88)

θ∈Θ

On the event Ω(cid:48)(cid:48) of probability at least 1− the constraints are satisﬁed when
H = G, and therefore, since they are linear constraints, Slater’s conditions
are satisﬁed [7, page 226]. This means that there is no duality gap, or in
other words that

inf
H

sup
ξ+,ξ−

V (H, ξ+, ξ−) = sup
ξ+,ξ−

inf
H

V (H, ξ+, ξ−).

It is then elementary to compute explicitly the solution of

inf

H,H(cid:62)=H

(cid:98)H(ξ+, ξ−) =

V (H, ξ+, ξ−),

(cid:2)ξ+(θ) − ξ−(θ)(cid:3)θθ(cid:62).

(cid:88)

θ∈Θρ

(cid:2)ξ+(θ) − ξ−(θ)(cid:3)(cid:2)ξ+(θ(cid:48)) − ξ−(θ(cid:48))(cid:3)(cid:104)θ, θ(cid:48)(cid:105)2

(cid:88)

θ∈Θρ

+

ξ+(θ)B−(θ) − ξ−(θ)B+(θ).

that is

Remark that

V(cid:0)(cid:98)H(ξ+, ξ−), ξ+, ξ−)
(cid:88)

= −1
2

(θ,θ(cid:48))∈Θ2

ρ

Since

V(cid:0)(cid:98)H(ξ+, ξ−), ξ+, ξ−(cid:1) = inf

inf
H

sup
ξ+,ξ−

V (H, ξ+, ξ−) = sup
ξ+,ξ−

we deduce that (cid:98)G = (cid:98)H((cid:98)ξ+,(cid:98)ξ−), as stated in the proposition. (cid:3)

sup
ξ+,ξ−

H

V (H, ξ+, ξ−),

March 17, 2016

Olivier Catoni

Let us remark that, since B−(θ) < B+(θ), the constraints cannot be
reached at the same time for ξ+(θ) and for ξ−(θ), so that either ξ+(θ) = 0 or
ξ−(θ) = 0, implying that ξ+ and ξ− are the positive and the negative parts

of ξ = ξ+ − ξ−. We could thus also write (cid:98)G as (cid:98)G =
(cid:98)ξ ∈ arg max

ξ(θ)(cid:104)θ, θ(cid:48)(cid:105)2ξ(θ(cid:48))

(cid:88)

(cid:32)

−1
2

ξ∈RΘ

θ,θ(cid:48)∈Θ

(cid:88)

θ∈Θ

(cid:98)ξ(θ)θθ(cid:62), where

74

(cid:33)

(cid:88)

θ∈Θ

+

max{ξ(θ), 0}B−(θ) − min{ξ(θ), 0}B+(θ)

.

J. Computation of the robust Gram matrix estimator

Let us remark ﬁrst that

J.1. Computation of the robust estimator in a fixed direction.
In this section, we give some details on the computation of the estimator

(cid:98)N (θ) used in Proposition 1.1 on page 7. In this discussion, θ is ﬁxed.
(cid:12)(cid:12)(cid:12)(cid:8)i;(cid:104)θ, Xi(cid:105) (cid:54)= 0(cid:9)(cid:12)(cid:12)(cid:12) log(2) −(cid:12)(cid:12)(cid:12)(cid:8)i;(cid:104)θ, Xi(cid:105) = 0(cid:9)(cid:12)(cid:12)(cid:12)ψ(λ)

rλ(0) = −λ−1ψ(λ) < 0, and

(cid:96)λ(θ) def= lim

α→+∞ rλ(αθ) =

,

nλ

global convergence properties.

so that these two quantities can easily be computed.

Otherwise, and this is the most usual case, (cid:96)λ(θ) > 0, and we can compute

In the case when (cid:96)λ(λ) ≤ 0, (cid:98)α = +∞ and (cid:98)N (θ) = 0.
(cid:98)α(θ) and therefore (cid:98)N (θ) quickly using a modiﬁed Newton’s method with
Indeed, two algorithms may come to mind to compute (cid:98)α.
a0θ(cid:1) ≤ 0
(cid:0)√
(cid:0)αθ(cid:1) is non-decreasing. Starting from a0, b0 ∈ R+ such that rλ
(cid:0)√
b0θ(cid:1) ≥ 0, we can put
(cid:27)
(cid:26)
(cid:111)
u ∈(cid:110)
uθ(cid:1) ≤ 0
, r(cid:0)√
(cid:26)
(cid:27)
u ∈(cid:110)
(cid:111)
uθ(cid:1) > 0
, r(cid:0)√

The ﬁrst is a divide and conquer algorithm, based on the fact that α (cid:55)→

rλ
and rλ

ak−1 + bk−1

ak−1 + bk−1

ak = max

, bk−1

ak−1,

,

.

bk = min

, bk−1

ak−1,

2

2

March 17, 2016

Olivier Catoni

J.1 Computation of the robust estimator in a fixed direction

75

The second algorithm is the well known Newton’s method, which is de-

scribed in this case as

k = α2
α2

k−1 −

(cid:90)

(cid:104)θ, x(cid:105)2ψ(cid:48)(cid:2)λ(cid:0)α2

rλ

(cid:0)αk−1θ(cid:1)
k−1(cid:104)θ, x(cid:105)2 − 1(cid:1)(cid:3) dP(x)

.

Once we have reached a small enough neighborhood of the solution, New-
ton’s method is faster, whereas the divide and conquer algorithm achieves
a convergence speed of 2−k, concerning the accuracy of the computation of

(cid:98)N (θ)−1, from any starting point, not necessarily close to the solution.

Fortunately, it is quite easy to combine the two methods into a single

algorithm that keeps the best of both worlds.

Let us introduce the function

f (u) = u −

(cid:104)θ, x(cid:105)2 dP(x)

uθ(cid:1)
(cid:0)√

(cid:90)

rλ

(cid:104)θ, x(cid:105)2ψ(cid:48)(cid:2)λ(cid:0)u(cid:104)θ, x(cid:105)2 − 1(cid:1)(cid:3) dP(x)
(cid:19)−1

(cid:26)1 + λ−1
(cid:104)θ, Xi(cid:105)2 ,(cid:104)θ, Xi(cid:105) (cid:54)= 0, 1 ≤ i ≤ n
a0 = max(cid:8)u ∈ A0; rλ
uθ(cid:1) ≤ 0(cid:9),
(cid:0)√
uθ(cid:1) > 0(cid:9).
b0 = min(cid:8)u ∈ A0; rλ
(cid:0)√

, max

(cid:27)(cid:41)

,

(cid:40)

(cid:18)(cid:90)

A0 =

0,

and

Let us put to deﬁne the starting point of the algorithm

(cid:111)

Let us deﬁne then

Ak =

(cid:110)

, f (ak−1), f (bk−1)

,

ak−1 + bk−1

2

ak−1, bk−1,

ak = max(cid:8)u ∈ Ak; rλ
uθ(cid:1) ≤ 0(cid:9),
(cid:0)√
bk = min(cid:8)u ∈ Ak; rλ
(cid:0)√
uθ(cid:1) > 0(cid:9),
(cid:0)(cid:112)(ak + bk)/2 θ(cid:1) > 0 and αk =

√

k −(cid:98)α2| = min(cid:8)|ak −(cid:98)α2|,|bk −(cid:98)α2|(cid:9) ≤ 2−(k+1)|b0 − a0|,

|α2

bk otherwise.

k −(cid:98)α2| ≤(cid:12)(cid:12)f(cid:0)α2

|α2

k−1

(cid:1) −(cid:98)α2(cid:12)(cid:12).

√

and αk =

ak if rλ

Proposition J.1

and

The second inequality shows that the local convergence speed of the combined
algorithm is at least as good as Newton’s method.

March 17, 2016

Olivier Catoni

J.2 Computation of a robust estimate of the Gram matrix

76

J.2. Computation of a robust estimate of the Gram matrix. We
describe here a simpliﬁed algorithm, that does not share the mathematical
properties of the convex optimization scheme described in Appendix I, but
turns out to be eﬃcient in practice to improve on the empirical Gram matrix
when dealing with the estimation of the Gram matrix, or the empirical risk
minimization when dealing with least squares regression.

Since our approach of robust least squares regression in dimension d is
based on the robust estimation of a Gram matrix in dimension d + 1, we start
with the robust estimation of the Gram matrix from a sample X1, . . . , Xn ∈
Rd made of independent copies of some random variable X.
For any vector of weights p = (pi, i = 1, . . . , n) ∈ Rn, and any positive

parameter λ, let S(p, λ) be the solution of

(cid:17)(cid:105)
i − 1

ψ

λ

S(p, λ)−1p2

= 0,

(cid:104)

(cid:16)

n(cid:88)

i=1

that can be computed as explained in the previous section, using some suit-
able Newton algorithm. Given some conﬁdence parameter , deﬁne S(p) =

S(cid:0)p, λ(p)(cid:1), where

(cid:114) 1
n(cid:88)

v

i=1

(cid:104) 2

n

p2
i

λ(p) = m

log(−1)

with m =

1
n

and

(cid:16)

log(−1)

(cid:17)(cid:105)
(cid:0)p2
i − m(cid:1)2.

,

1 − 2
n
1
n

v =

n(cid:88)

i=1

This value of the scale parameter is based on the optimal value for the es-
timation of a single expectation as described in [10] and maybe expected
in practice to be more eﬃcient than the conservative value allowing for the
mathematical proof of generalization bounds.

The algorithm to compute a robust estimate (cid:98)G of the Gram matrix G =

E(XX(cid:62)) works as follows.

Start with the empirical Gram matrix estimate

n(cid:88)

1
n

Xi X(cid:62)
i .

(cid:98)G0 = G =
(cid:98)G(k) = U (k)(cid:62)D(k)U (k)

Assuming that at iteration k we have computed the estimate (cid:98)G(k) ∈ Rd×d,

i=1

decompose it into

where U (k) U (k)(cid:62) = I is an orthogonal matrix and D(k) is a diagonal matrix.
Deﬁne the d × d matrix

March 17, 2016

Olivier Catoni

(cid:20)

S

1
4

M (k)i,j =

(cid:16)

(cid:17)

77

(cid:17)(cid:21)

,

(U (k)X(cid:96))i + (U (k)X(cid:96))j, (cid:96) = 1, . . . , n

− S

(U (k)X(cid:96))i − (U (k)X(cid:96))j, (cid:96) = 1, . . . , n

(cid:16)

and update the estimate by the formula

(cid:98)G(k + 1) = U (k)(cid:62)M (k) U (k),

until some stopping rule is reached (we can for instance use a ﬁxed number
of iterations, as in the simulation below, or stop when the Frobenius norm

(cid:107)(cid:98)G(k + 1) − (cid:98)G(k)(cid:107)F falls under some threshold). The idea is to update the

estimator of the Gram matrix using the polarization formula (1) on page 3
in a basis of eigenvectors of the current estimate. This uses more directions
than using the polarization formula only in the canonical basis of Rd, while
trying to get accurate eigenvectors, and is faster than using a net of directions
as in the mathematically more justiﬁed algorithm described in Appendix I.

from a sample (X1, Y1), . . . , (Xn, Yn) of independent copies of (X, Y ) ∈ Rd+1,
ﬁrst compute as above a robust estimate

inf
θ∈Rd

To solve the least squares problem

E(cid:2)(cid:0)Y − (cid:104)θ, X(cid:105)(cid:1)2(cid:3)
(cid:33)
(cid:32)(cid:98)G1,1 (cid:98)G1,2
(cid:98)G =
(cid:98)G2,1 (cid:98)G2,2
(cid:34)(cid:18) X−Y
(cid:19)(cid:0)X(cid:62),−Y(cid:1)(cid:35)
then deﬁne the robust estimate(cid:98)θ of θ∗ as
(cid:98)θ = −(cid:98)G−1
1,1(cid:98)G1,2,
where (cid:98)G−1

of the Gram matrix

E

,

1,1 is the pseudo inverse of the symmetric matrix (cid:98)G1,1, obtained by

inverting only its non-zero eigenvalues.

K. Some simulation

We present a small simulation to illustrate the beneﬁt of using our robust
least squares estimator in the case of a long tail noise. The idea of this

March 17, 2016

Olivier Catoni

78

Figure 1: A sample

simulation is to show that even in a very simple situation using a robust
estimator may bring a signiﬁcant improvement.
Consider some noise η ∼ 0.9 × N(0, 1) + 0.1 × N(0, 302) that is the mix-
ture of two Gaussian random variables with diﬀerent variances. Consider a

Gaussian random variable (cid:101)X ∼ N(0, 102) independent of the noise η, and
Putting X = ((cid:101)X, 1)(cid:62), we can write this problem as

deﬁne

Y = (cid:101)X + η + 1.

Y = (cid:104)θ∗, X(cid:105) + η,

where θ∗ = (1, 1)(cid:62).

Figure 1 shows a typical sample, where n = 100, and where the two
components of the mixture have been plotted with diﬀerent colors. Figures 2
on the following page plots the excess risk of the empirical risk minimizer

March 17, 2016

Olivier Catoni

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−40−2002040−40−2002040n1 = 13, n2 = 87References

79

Figure 2: The empirical quantile function of the excess risk

and of the robust estimate for 500 trials of the experiment. One can see that
we get a substantial improvement of the mean excess risk using the robust
estimator, since the estimated expected excess risks are of 1.7 against less
than 1.1 for the robust estimator.

In conclusion, it is not necessary to envision very large sample sizes or
very exotic noise structures to feel the improvement brought by the more
stable robust estimator.

References

[1] P. Alquier. Iterative feature selection in least square regression estima-

tion. Ann. Inst. Henri Poincar´e, Probab. Stat., 2008.

[2] P. Alquier. PAC-bayesian bounds for randomized empirical risk mini-

mizers. Mathematical Methods of Statistics, 17(4):279–304, 2008.

March 17, 2016

Olivier Catoni

0100200300400500051015Distribution of errorsR(thetaRobust) − R(thetaStar) = 1.07415 +/− 0.0452628  R(thetaEmp) − R(thetaStar) = 1.70742 +/− 0.0869817References

80

[3] J.-Y. Audibert. Aggregated estimators and empirical complexity for
least square regression. Ann. Inst. Henri Poincar´e, Probab. Stat.,
40(6):685–736, 2004.

[4] J.-Y. Audibert and O. Catoni. Linear regression through PAC-bayesian
truncation. arXiv http: // arxiv. org/ abs/ 1010. 0072v2 , pages 1–
40, 2010, revised in 2011.

[5] J.-Y. Audibert and O. Catoni. Robust linear least squares regression.

Ann. Stat., 39(5):2766–2794, 2011.

[6] J.-Y. Audibert and O. Catoni. Supplement to “robust linear least squares

regression.”. Ann. Stat., pages 1–19, 2011.

[7] S. Boyd and L. Vandengerghe. Convex Optimization. Cambridge Uni-

versity Press, 2004.

[8] O. Catoni. Statistical Learning Theory and Stochastic Optimization,
Lectures on Probability Theory and Statistics, ´Ecole d’ ´Et´e de Proba-
bilit´es de Saint-Flour XXXI – 2001, volume 1851 of Lecture Notes in
Mathematics. Springer, 2004. Pages 1–269.

[9] O. Catoni. PAC-Bayesian Supervised Classiﬁcation: The Thermody-
namics of Statistical Learning, volume 56 of IMS Lecture Notes Mono-
graph Series.
Institute of Mathematical Statistics, 2007. Pages i-xii,
1-163.

[10] O. Catoni. Challenging the empirical mean and empirical variance: a

deviation study. Ann. Inst. Henri Poincar´e, 48(4):1148–1185, 2012.

[11] O. Catoni. Pac-bayes bounds for supervised classiﬁcation. In A. Gam-
merman V. Vovk, H. Papadopoulos, editor, Measures of Complexity,
Festschrift for Alexey Chervonenkis, pages 287–302. Springer, 2015.

[12] I. Giulini. Generalization bounds for random samples in Hilbert spaces.

PhD thesis, Ecole Normale Sup´erieure, Paris, France, 2015.

[13] I. Giulini. Robust dimension-free gram operator estimates. preprint,

2015. http://arxiv.org/abs/1511.06259.

[14] V. Koltchinskii and S. Mendelson. Bounding the smallest singular value
of a random matrix without concentration. Int Math Res Notices, 2015.

[15] G. Lecu´e and S. Mendelson. Performance of empirical risk minimization

in linear aggregation. Bernoulli to appear, 2015.

March 17, 2016

Olivier Catoni

References

81

[16] D. A. McAllester. PAC-Bayesian model averaging.

In Proceedings of
the 12th annual conference on Computational Learning Theory. Morgan
Kaufmann, 1999.

[17] D. A. McAllester. PAC-Bayesian stochastic model selection. Mach.

Learn., 51(1):5–21, April 2003.

[18] D. A. McAllester. Simpliﬁed PAC-Bayesian margin bounds. In In COLT,

pages 203–215, 2003.

[19] M. Seeger. PAC-Bayesian generalization error bounds for gaussian pro-
cess classiﬁcation. Informatics report series EDI-INF-RR-0094, Division
of Informatics, University of Edinburgh, 2002.

March 17, 2016

Olivier Catoni

