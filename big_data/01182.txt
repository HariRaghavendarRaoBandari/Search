PREPRINT SUBMITTED TO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS

1

Network Unfolding Map by Edge Dynamics

Modeling

Filipe Alves Neto Verri, Paulo Roberto Urio, and Liang Zhao

6
1
0
2

 
r
a

M
3

 

 
 
]
I

A
.
s
c
[
 
 

1
v
2
8
1
1
0

.

3
0
6
1
:
v
i
X
r
a

Abstract—The emergence of collective dynamics in neural
networks is a mechanism of the animal and human brain for
information processing. In this paper, we develop a computational
technique of distributed processing elements, which are called
particles. We observe the collective dynamics of particles in a
complex network for transductive inference on semi-supervised
learning problems. Three actions govern the particles’ dynamics:
walking, absorption, and generation. Labeled vertices generate
new particles that compete against rival particles for edge
domination. Active particles randomly walk in the network
until they are absorbed by either a rival vertex or an edge
currently dominated by rival particles. The result from the model
simulation consists of sets of edges sorted by the label dominance.
Each set tends to form a connected subnetwork to represent
a data class. Although the intrinsic dynamics of the model is
a stochastic one, we prove there exists a deterministic version
with largely reduced computational complexity; speciﬁcally, with
subquadratic growth. Furthermore, the edge domination process
corresponds to an unfolding map. Intuitively, edges “stretch”
and “shrink” according to edge dynamics. Consequently, such
effect summarizes the relevant relationships between vertices and
uncovered data classes. The proposed model captures important
details of connectivity patterns over the edge dynamics evolution,
which contrasts with previous approaches focused on vertex
dynamics. Computer simulations reveal that our model can
identify nonlinear features in both real and artiﬁcial data,
including boundaries between distinct classes and the overlapping
structure of data.

Index Terms—Complex networks, nonlinear dynamical sys-

tems, semi-supervised learning, particle competition.

I. INTRODUCTION

O VER the last decades, the world has been experiencing a

data deluge. An impressive volume of data is produced
in a short period, far more than people would be able to
absorb. With those data, machine learning helps us enhance
our understanding of the world. Broadly, machine learning
tasks can be classiﬁed by the availability of the desired output.
Supervised-learning techniques work with the desired output
for each input data instance—the input data are completely
labeled. Conversely, unsupervised learning techniques try to
discover the intrinsic structure of a completely unlabeled data
set [1], [2].

Semi-supervised learning (SSL) is another category of ma-
chine learning. It is in between the unsupervised and super-

This research was supported by the S˜ao Paulo State Research Foundation
(FAPESP), the Coordination for the Improvement of Higher Education Per-
sonnel (CAPES), and the Brazilian National Research Council (CNPq).

F.A.N. Verri and P.R. Urio are with the Institute of Mathematical and
Computer Sciences, University of S˜ao Paulo, S˜ao Carlos, SP, Brazil. Email:
{ﬁlipeneto,urio}@usp.br.

L. Zhao is with Ribeir˜ao Preto School of Philosophy, Science and Literature,

University of S˜ao Paulo, Ribeir˜ao Preto, SP, Brazil. Email: zhao@usp.br.

vised learning paradigms where both unlabeled and labeled
data are taken into account
in class or cluster formation
and prediction process [3], [4]. In real-world applications,
we usually have partial knowledge on a given dataset. For
example, we certainly do not know every movie actor except
a few famous ones; in a large-scale social network, we just
know some friends; in biological domain, we are far away
from completely obtaining a ﬁgure of the functions of all
genes, but we know the functions of some of them. Sometimes,
although we have a complete or almost complete knowledge
of a dataset, labeling it by hand is lengthy and expensive, so,
it is necessary to restrict the labeling scope. For these reasons,
partially labeled datasets are often encountered. In this sense,
supervised and unsupervised learning can be considered as
extreme and special cases of semi-supervised learning. Up
to now, many semi-supervised learning techniques have been
developed,
including generative models [5], discriminative
models [6], clustering and labeling techniques [7], multi-
training [8], low-density separation models [9], and graph-
based methods [10]–[12]. Among the approaches listed above,
graph-based SSL has been triggered much attention. In this
case, each data instance is represented by a vertex and is linked
to other vertices according to a predeﬁned afﬁnity rule. The
labels are propagated to the whole graph using a particular
optimization heuristic [13].

Interpreting the graph representation of the data as a
complex network grants us further access to information.
Complex networks are large-scale graphs with nontrivial topol-
ogy [14]. Such networks introduce a powerful tool to de-
scribe the interplay of topology, structure, and dynamics of
complex systems [14], [15]. Networks also turn out to be an
important mechanism for data representation and analysis. For
this reason, we consider the network-based approach for SSL
in this work. However, the above-mentioned network-based
approach focuses on the optimization of the label propagation
result and pays little attention to the detailed dynamics of
the learning process. On the other hand, it is well-known
that collective neural dynamics generates rich information,
and such a “redundancy” processing handles the adaptability
and robustness of the learning process. Moreover, traditional
graph-based techniques have high computational complexity,
usually at cubic order [16]. A common strategy to overcome
this disadvantage is using a set of sparse prototypes derived
from the data [12]. However, such a sampling process usually
loses information of the original data.

Taking into account the facts above, we study a new type of
dynamical competitive learning mechanism in a complex net-
work, called particle competition. Consider a network, where

PREPRINT SUBMITTED TO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS

2

In this paper, we propose a transductive semi-supervised
learning model that employs a dynamical system in complex
networks. In this dynamical system, namely Edge Domina-
tion System, particles compete for the dominance of edges
in a network. Subnetworks are generated with the edges
grouped by class dominance. Here, we call each subnetwork
an unfolding. The learning model employs the unfoldings to
classify unlabeled data. The proposed model offers satisfactory
performance on semi-supervised learning problems, in both
artiﬁcial and real data. Moreover, it has shown to be suitable
for detecting overlapping regions of data points by simply
counting the edges dominated by each class of particles.
Moreover, it has low computational complexity order.

In comparison to the original particle competition models
and other graph-based semi-supervised learning techniques,
the proposed one presents the following salient features:

several particles walk and compete to occupy as many vertices
as possible while attempting to reject intruder particles. Each
particle performs a combined random and preferential walk by
choosing a neighbor vertex to visit. Finally, it is expected that
each particle occupies a subset of vertices, called a community
of the network. In this way, community detection is a direct
result of the particle competition. The particle competition
model was originally proposed in [17] and extended for the
data clustering task in [18]. Later, it has been applied to semi-
supervised learning [19], [20] where the particle competition is
formally represented by a nonlinear stochastic dynamical sys-
tem. In all the models mentioned above, the authors concern
vertex dynamics—how each vertex changes its state (the level
of dominance of each particle). Intuitively, vertex dynamics
is a rough modeling of a network because each vertex can
have several edges. A problem with the data analysis in this
approach is the overlapping nature of the vertices, where a data
item (a vertex in the networked form) can belong to more than
one class. Therefore, it is interesting to know how each edge
changes its state in the competition process to acquire detailed
knowledge of the dynamical system behavior.

• Particles compete to dominate edges instead of nodes.
The intuition is that the edge domination modeling gives
more details than vertex domination one. This is because
each vertex may have from one to several edges. Edges
determine the network’s topological structure. Conse-
quently, our model has the beneﬁt of granting essential
information concerning overlapping vertices (data items)
by simply counting the edges dominated by each class
of particles. Computer simulations show the proposed
technique achieves a good classiﬁcation accuracy and it
is suitable for situations with a small number of labeled
samples.

• We allow the direct conﬂict between particles of differ-
ent classes, causing them to be absorbed and removed
from the system. This mechanism contrasts with works
that incorporate a preferential walking mechanism where
particles tend to avoid rival particles. As a consequence,
the number of active particles in the system varies over
time. It is worth noting that the elimination of preferential
walking mechanism largely simpliﬁes the dynamical rules
of particle competition model. Now, the new model is

characterized by competition of only random walking
particles, which,
in turn, permits us to ﬁnd out an
equivalent deterministic version. The original particle
competition model is intrinsically stochastic. Then, each
run may generate a different result. Consequently,
it
has high computation cost. In this work, we ﬁnd out
a deterministic system with running time independent
of the number of particles, and we demonstrate that
it is mathematically equivalent to the stochastic model.
Moreover, the deterministic model has linear time order
and ensures a stable learning model. In other words,
the model generates the same output for each run with
the same input. Furthermore, the system is simpler and
easier to be understood and implemented. Such a feature
makes the proposed model more efﬁcient than the original
particle competition model.

• In classical graph-based semi-supervised learning tech-
niques, usually, an objective function is deﬁned for op-
timization. Such function considers not only the label
information as they can also be built upon the semi-
supervised assumptions of smoothness, cluster, or mani-
fold. In particle competition models, an objective function
is not deﬁned. Instead, dynamical rules which govern the
time evolution of particles and vertices (or edges) are
deﬁned. Those dynamical rules mimic the phenomenon
observed in some natural and social systems, such as re-
source competition among animals, territory exploration
by humans (animal), election campaigns, etc. In other
words, a particle competition technique is typically nature
inspired. In those kinds of techniques, we have focused
on behavior modeling instead of objective modeling.
Certain objectives can be achieved if the corresponding
behavioral rules are properly deﬁned. That way, we may
classify classical graph-based semi-supervised learning
techniques as objective-based design and the particle
competition technique as behavior-based design.

The remainder of this paper is organized as follows. The
proposed particle competition system is studied in Section II.
Our transductive semi-supervised learning model
is repre-
sented in Section III. In Section IV, results of computer sim-
ulations are shown to assess the proposed model performance
on both artiﬁcial and real-world datasets. Finally, Section V
concludes this paper.

II. EDGE DOMINATION SYSTEM

In this section, we give an introduction to the Edge
Domination System—a particle competition system for edge
domination—explaining its basic design. Whenever pertinent,
we go into detail for further clariﬁcation.

A. Overview

We consider a complex network expressed by a simple,
unweighted, undirected graph G = (V,E), where V is the
set of vertices and E ⊆ V × V is the set of edges. If two
vertices are considered similar, an edge connects them. This
network contains |V| = l + u vertices that can be either
labeled or unlabeled data points. The set L = {v1, . . . , vl}

PREPRINT SUBMITTED TO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS

3

contains the labeled vertices, where a vertex vi ∈ L is labeled
with yi ∈ {1, . . . , C}. We also use the terms label and class
synonymously—if a vertex is labeled with c, we say this vertex
belongs to class c. The set U = {vl+1, . . . , vl+u} contains the
unlabeled vertices. We suppose that l (cid:28) u. Thus, we have
that L ∩ U = ∅ and V = L ∪ U. The network is represented
by the adjacency matrix A = (aij) where aij = aji = 1 if vi
is connected to vj. We denote (i, j) it be the edge between
vertices vi and vj. For practical reasons, consider a connected
network and at least one labeled vertex of each class.

In this model, particles are the objects that ﬂow within the
network while carrying a label. Labeled vertices are sources
for particles of the same class and sinks for particles of other
classes. After being released a particle randomly walks the
network. There is equal probability among adjacent vertices
to be chosen as the next vertex. Consider a particle that is in
vi. This particle decides to move to vj with probability

aij

deg vi

with deg vi denoting the degree of vi.

In each step, at the moment of a particle decides to move to
a next vertex, it can be absorbed (removed from the system).
If a particle is not absorbed we say that it has survived and
it remains active; and if it survives, then it continues walking.
Otherwise, the absorbed particle ceases to affect the system.
The absorption depends on the level of subordination and
domination of a class against all other classes.

To determine the level of domination and subordination of
each class in an edge, we take into account the active particles
in the system. The current directed domination ˜nc
ij(t) is the
number of active particles belonging to class c that decided
to vj at time t and survived. Similarly,
to move from vi
the current relative subordination ˜σc
ij is the fraction of active
particles that do not belong to class c and have successfully
passed through edge (i, j), regardless of direction, at time t.
Mathematically, we deﬁne the latter as

1 −

1
C

˜σc
ij :=

(cid:80)C

˜nc
ij + ˜nc
ji
q=1 ˜nq

ij + ˜nq

ji

if(cid:80)C

otherwise.

q=1 ˜nq

ij + ˜nq

ji > 0,

The survival of a particle depends on the current relative
subordination of the edge and the destination vertex. If a
particle decides to move into a sink, it will be absorbed. If
the destination vertex is not a sink, its survival probability is

1 − λ˜σc

ij(t)

where λ ∈ [0, 1] is the competition parameter.

A source generates particles according to its degree and the
current number of active particles in the system. Let ˜nc(t)
be the number of active particles belonging to class c in the
system at time t. A source generates new particles if ˜nc(t) <
˜nc(0).
Let Gc = {vi|vi ∈ L and yi = c} be the set of sources for
particles that belong to class c. The number of new particles

(cid:40)

belonging to class c in vi at time t follows the distribution

B(˜nc(0) − ˜nc(t), ρc
i )
B(1, 0)

if ˜nc(0) − ˜nc(t) > 0,
otherwise,

where

ρc
i :=



(cid:80)

0

deg vi

vj∈Gc deg vj

if vi ∈ Gc,

otherwise,

and B(n, p) is a binomial distribution. In other words, if the
number of active particles is fewer than ˜nc(0), each source
performs ˜nc(0)− ˜nc(t) trials with probability ρc
i of generating
a new particle.

Therefore, the expected number of new particles belonging

(cid:40)

to class c in vi at time t is
i ˜nc(0) − ˜nc(t)
ρc
0

if ˜nc(0) − ˜nc(t) > 0,
otherwise.

We are interested in the total number of visits of particles
of each class to each edge. Thus, we introduce the cumulative
domination ˜δc
ij(t) that is the total number of particles belong-
ing to class c that passed through edge (i, j) up to time t.
Mathematically, this is deﬁned as

t(cid:88)

τ =1

˜δc
ij(t) :=

ij(τ ) .
˜nc

Using cumulative domination, we can group the edges by
class domination. For each class c, the subset of edges E c(t) ⊆
E is

(cid:26)

(cid:12)(cid:12)(cid:12)(cid:12)arg max
(cid:16)˜δq

q

E c(t) :=

ij

(cid:17)

(cid:27)

ij(t) + ˜δq

ji(t)

= c

.

We deﬁne the subnetwork

Gc(t) := (V,E c(t))

(1)

as the unfolding of network G according to class c at time t.
We interpret
the unfolding as a subspace with the most
relevant relationships for a given class. We use the available
information in these subnetworks for the study of overlapping
regions and for semi-supervised learning.

B. An Illustrative Example

An iteration of the system’s evolution is illustrated in
Figure 1. The considered system contains 22 active particles
at time t and 20 at time t + 1. In an iteration every particle
moves to a neighbor vertex, without preference. The movement
of a particle is indicated by an arrow. An interrupted line
indicates the edge to absorb the coming particle. A total of
8 particles are absorbed during this iteration, and the red and
green sources have generated 4 new particles to compensate
the absorbed ones.

At time t for example, one of the red particles passing
through edge (1, 3) is absorbed, which is due to a current edge
dominance of 0.5 in that edge (one red particle and one green
particle). Conversely, all green particles that moved through
edge (5, 7) remained active at time t + 1. Since there is no
rival particle (red particle) passing through this edge, the new

PREPRINT SUBMITTED TO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS

4

belonging to class c in vi at time t. The state of this dynamical
system is

(2)

(cid:104)

˜X(t) :=

(cid:105)T

,

˜nc(t) ˜∆c(t)

i ,
i,j .

˜nc(t) :=(cid:2) ˜nc
i (t)(cid:3)
˜∆c(t) :=(cid:0)˜δc
ij(t)(cid:1)
(cid:88)
(cid:0)˜nc
i (t + 1) − ˜ac

j
+ ˜gc

i (t) +

i (t + 1) ,

Let ˜gc

i (t) and ˜ac

i (t) be, respectively, the number of parti-
cles generated and absorbed by vi at time t. The evolution
function ˜φ of the dynamical system is

ij(t + 1)(cid:1)

ji(t + 1) − ˜nc

˜nc
i (t + 1) = ˜nc

where



˜φ :

time t

v3

time t + 1

v3

v1

v6

v1

v6

v4

v4

v2

v7

v2

v7

v5

v5

Fig. 1.
Illustration of an iteration of the system’s evolution. The considered
system is a network formed by 7 vertices and 10 edges; colors represent
the label of particles and vertices. The ﬁrst and the third networks depict
the cumulative domination before and after the iteration. The cumulative
domination is the number of visits of particles to an edge since the initial
state. In the second network, particles are depicted in small circles. Active
particles at time t are depicted in dashed borders, whereas active particles at
time t + 1 are in full borders. An arrow indicates a particle movement, while
an interrupted line indicates that the particle has been absorbed when trying
to move through an edge. Particles without an adjacent arrow are also the
particles present only in vertices with the same color, since they are generated
by these vertices at time t + 1.

value of the current edge dominance is 1 and 0 for green and
red classes, respectively.

In edge (2, 4) two green particles and one red particle chose
to pass through. One green particle is absorbed and has not
affected the new current level of dominance. Since one particle
of each class successfully passed through edge (2, 4), the new
current level of dominance on this edge is 0.5. The same
occurs for edge (4, 7) where no particles have passed through
and, thus, the current level of dominance is set equally among
classes.

In edges (2, 5) and (3, 6), particles have tried to move into
a source of rival particles (sinks). These particles are absorbed
independently from the current level of dominance.

After this step, our edge-centric system could measure the
overlapping nature of the v4 by counting the edges dominated
by each class. A vertex-centric approach would have lost such
information.

C. Mathematical Modeling

Formally, we deﬁne Edge Domination System as a dynam-
i (t) be the number of active particles

ical system ˜X(t). Let ˜nc

˜δc
ij(t + 1) = ˜δc

ij(t) + ˜nc

ij(t + 1) .

Intuitively, the number ˜nc

i of active particles that are in
a vertex is the total number of particles arriving minus the
number of particles leaving or being absorbed; additionally for
labeled vertices, the number of generated particles. Moreover,
to calculate the total number ˜δc
ij of visits of particles to an
edge, we simply add up the number ˜nc
ij at each time. Values
i are obtained stochastically according to the
ij, ˜gc
˜nc
dynamics of walking, absorption, and generation.

i , and ˜ac

The initial state of the system is given by an arbitrary

number ˜nc

i (0) of initial active particles and

(cid:40)˜nc

ij(0) = 0 ,
˜δc
ij(0) = 0 .

In order to achieve the desirable network unfolding, it is
necessary to average the results of several simulations of the
system with a very large number of initial particles ˜nc
i (0).
In this way, the computational cost of such a simulation is
very high. Conversely, we provide an alternative system X(t)
that achieves similar results in a deterministic manner. The
alternative system considers that there exists an asymptotically
inﬁnite number of initial active particles. More details will
follow.

D. Alternative Mathematical Modeling

Consider the dynamical system

i (t)(cid:3)
 nc(t) =(cid:2)nc
N c(t) =(cid:0)nc
ij(t)(cid:1)
∆c(t) =(cid:0)δc
ij(t)(cid:1)

i

i,j

i,j



X(t) :=

is a nonlinear Markovian dynamical system with the

that
deterministic evolution function

nc(t + 1) = nc(t) × P c(X(t)) + gc(nc(t))

N c(t + 1) = diag nc(t) × P c(X(t))
∆c(t + 1) = ∆c(t) + N c(t + 1) ,

(3)

φ :

where

(cid:16)

(cid:17)

,

i,j

P c(X) :=

pc
ij(X)

PREPRINT SUBMITTED TO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS

5

deg vi

aij

0
1 −

1
C

ij(X)(cid:1)

(cid:0)1 − λσc
(cid:80)C

ij + nc
nc
ji
q=1 nq

ij + nq

ji

if vj ∈ L and yj (cid:54)= c ,
otherwise,

q=1 nq

ij + nq

ji > 0,

if(cid:80)C
(cid:105)

otherwise,

(cid:104)

pc
ij(X) :=

σc
ij(X) :=

and

The initial state of the system X is given by an arbitrary

discrete distribution nc(0) of initial active particles and

i (n) := ρc
gc

,

i

gc
i (n)

gc(n) :=
i max{0, 1 · nc(0) − 1 · n} .
(cid:40)

ij(0) = 0 ,
nc
ij(0) = 0 .
δc

If the initial number of particles in each vertex in system ˜X
follows the distribution of initial particles in system X, we will
provide evidence that the unfolding result tends to be the same
i (0) → ∞ for all c ∈ {1, . . . , C} and
for both systems as ˜nc
i ∈ {1, . . . ,|V|}.

E. Mathematical Analysis

=

(4)

(5)

(6)

particle’s class; that is, Ic
particle p that belongs to class c and is in vi at time t.

ment decision of a particle and whether it was absorbed after
the decision. By formulation, in dynamical system ˜X the
conditional probability, given that ˜σc

The probability Pr(cid:2)Ic
ij(t + 1) = 1(cid:12)(cid:12)˜σc
Pr(cid:2)Ic
0

aij

=

deg vi

ij(t + 1) = Iij(p, t + 1) for any

ij(t + 1) = 1(cid:3) is affected by the move-
ij(t) = ξ(cid:3)

ij(t) = ξ, is

if vj ∈ L and yj (cid:54)= c ,

(1 − λ · ξ) otherwise.

(cid:90) ∞

−∞

Let f˜σc
variable ˜σc

That is, when a particle tries to move into a sink, the survival
probability is zero. Otherwise, a particle only reaches vj if it
chooses to move into the vertex and it is not absorbed.

ij (t) be the probability density function of the random

ij(t + 1) = 1(cid:3) is

ij. Hence, the probability Pr(cid:2)Ic
ij(t + 1) = 1(cid:12)(cid:12)˜σc
ij(t) = ξ(cid:3) f˜σc
Pr(cid:2)Ic
(cid:90) ∞
(cid:18)(cid:90) ∞
(cid:90) ∞
(cid:0)1 − λE(cid:2)˜σc

ij (t)(ξ) dξ − λ
f˜σc
aij

(1 − λ · ξ) f˜σc

ij (t)(ξ) dξ

ij (t)(ξ) dξ

deg vi

ξf˜σc

−∞

−∞

−∞

aij

aij

=

=

(cid:19)
ij(t)(cid:3)(cid:1) ,

ij (t)(ξ) dξ

deg vi

deg vi

if vj ∈ U or vj ∈ L ∧ yj (cid:54)= c . Otherwise, it is zero.
ij is convex with ﬁxed values of ˜nq
ij, ˜nq
all q (cid:54)= c. Thus, with the Jensen’s inequality [21], we have

Furthermore, ˜σc

ji for

E(cid:2)˜σc
ij(t)(cid:3) ≥ 1 −

ji(t)(cid:3)
ij(t)(cid:3) + E(cid:2)˜nc
E(cid:2)˜nc
q=1 E(cid:2)˜nq
ij(t)(cid:3) + E(cid:2)˜nq
ji(t)(cid:3).
(cid:80)C

(7)

2) Particle generation study: In dynamical system ˜X the
expected number of particles belonging to class c generated
at vi at time t is

E[˜gc

i (t + 1)] =

E[˜gc

i (t + 1)|˜nc(t) = η] Pr[˜nc(t) = η] .

∞(cid:88)

η=0

The conditional expectation E[˜gc

i (t + 1)|˜nc(t) = η] is, by

formulation,

i · max{0, ˜nc(0) − η} ,
ρc

and thus, E[˜gc

i (t + 1)] is

∞(cid:88)

ρc
i

max{0, ˜nc(0) − η} Pr[˜nc(t) = η]

η=0

= ρc

i E[max{0, ˜nc(0) − ˜nc(t)}] .

Since max{0, x} is convex for all x ∈ R and according to

Jensen’s inequality, we have

E[˜gc

i (t + 1)] ≥ ρc

i max{0, ˜nc(0) − E[˜nc(t)]} .

(8)

In the previous subsections, we modeled two possibly
equivalent systems, X and ˜X. In this section, we present
mathematical results that explain the equivalence of both
systems under certain assumptions.
Theorem 1. Asymptotic equality between systems X and ˜X.
Assuming that

E(cid:2)˜σc
ij(t)(cid:3) → 1 −

ji(t)(cid:3)
ij(t)(cid:3) + E(cid:2)˜nc
E(cid:2)˜nc
q=1 E(cid:2)˜nq
ij(t)(cid:3) + E(cid:2)˜nq
ji(t)(cid:3) and
(cid:80)C
i max{0, ˜nc(0) − E[˜nc(t)]} as
i (t + 1)] → ρc
i (0) → ∞,
˜nc

E[˜gc

for all i, j ∈ V, t > 0, and c ∈ {1, . . . , C}, we have

ij(t)(cid:3) , and

ij(t) = κE(cid:2)˜nc
(cid:104)˜δc
(cid:105)

ij(t)

,

nc
i (t) = κE[˜nc

i (t)] , nc

δc
ij(t) = κE

for k > 0 constant.

In order to prove Theorem 1, we study the following

mechanisms of the particle competition system:

1) Particle motion and absorption study: In the proposed
system, each particle moves independently from the others.
Particle movement through an edge affects the absorption of
rival particles only on the next iteration. Such conditions are
favorable to naturally regard the system’s evolution in terms of
the distribution of particles over the nerwork. Next, we present
a formal model for particle movement.

Let Iij(p, t + 1) be a discrete random variable that is 1 if
particle p was in vi at time t and moved into vj at time t + 1;
and it is 0 otherwise. Since each particle in a vertex moves
independently, we can write this probability in terms of a

PREPRINT SUBMITTED TO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS

6

3) Expected edge domination study: At the beginning of

system ˜X we have

and, for t ≥ 0,

(cid:104)˜δc

(cid:105)

(cid:104)˜δc

E

ij(t + 1)

= E

ij(t) + ˜nc

ij(t + 1)

˜δc
ij(0) = 0

(cid:104)˜δc

= E

ij(t)

(cid:105)
+ E(cid:2)˜nc

(cid:105)

ij(t + 1)(cid:3) .

(9)

5) Scale invariance study: The unfolding Gc(t) from sys-
tem X is invariant under real positive multiplication of the
input nc(0). In order to prove this property, consider the
following lemma.
Lemma 1. System X has positive multiplicative scaling
behavior of order 1. That is
X(t) = Xt | X(0) = X0

⇐⇒ X(t) = κXt | X(0) = κX0

(12)

Given that ˜nc

i (t) = η is known and since each particle in
a vertex moves independently, the number of particles that
successfully reaches vj at time t + 1 is

for all t > 0 and κ > 0.

Proof of Lemma 1: Consider an arbitrary initial state

nc

i (0) = ηi ,
ij(0) = 0 ,
nc
ij(0) = 0 .
δc

X0 =

for all i, j, c.

We have that

σc
ij(κX0) = σc

ij(X0) =

1
C

,

nc
ij(1; κX0) = nc

0

=

(cid:18)

(cid:19)

deg vi

aij

1 − λ
C
ij(0; κX0) · pc
(cid:88)

gc
i (κn, 1) = gc

(cid:88)

j

= κ

if vj ∈ L and yj (cid:54)= c ,
otherwise,

ij(κX0) = κ · ηipc
= κ · nc

ij(X0)
ij(1; X0) ,

i (n, 1) = 0,

nc

i (1; κX0) =

nc
ji(1; κX0) + gc

i (nc

i (0; κX0) , 1)

ji(1; X0) + 0 = κ · nc
nc

i (1; X0),

and

j

δc
ij(1; κX0) = δc

ij(0; κX0) + nc
0 + κ · nc

ij(1; κX0) =
ij(1; X0) = κ · δc

ij(1; X0)

Thus, Relation (12) holds true for t = 1.

Assuming that Relation (12) holds true for some value of

t, we show that the relation holds true for t + 1.

(cid:1)

ij + κnc
ji

ij + κnq

ji

κnc

(cid:0)κnq
(cid:80)C
(cid:1) = σc

q=1

ij(Xt+1), if nc

ij +nc

ji > 0,∀c

ij(κXt+1) = 1 −
σc
(cid:80)C
nc
ij + nc
ji

(cid:0)nq

= 1−

q=1

ij + nq

ji

or

σc
ij(κXt+1) = σc

ij(Xt+1) =

1
C

, otherwise,

pc
ij(κXt+1) = pc

0

=

aij

deg vi

ij(Xt+1)

(cid:0)1 − λσc

ij(Xt+1)(cid:1)

if vj ∈ L and yj (cid:54)= c ,
otherwise,

E[Iij(pk, t + 1)]

pc
ij(κX0) = pc

ij(X0)

where pk is a particle that belongs to class c and is in vi.

Iij(pk, t + 1) ,

k=1

˜nc

ij(t + 1) =

η(cid:88)
Then, the expected value E(cid:2)˜nc
ij(t + 1)(cid:3) is
∞(cid:88)
i (t) = η(cid:3) Pr[˜nc
i (t) = η] · η(cid:88)
i (t) = η] · η(cid:88)
∞(cid:88)

ij(t + 1)(cid:12)(cid:12)˜nc
E(cid:2)˜nc
∞(cid:88)
∞(cid:88)

Pr[˜nc

Pr[˜nc

k=1

k=1

η=0

η=0

η=0

=

=

=

η Pr[˜nc

i (t) = η]

Finally,

η=0

E(cid:2)˜nc
ij(t + 1)(cid:3) = E[˜nc

ij(t) = 1(cid:3) .

Pr[Iij(pk, t + 1) = 1]

i (t) = η] · Pr(cid:2)Ic
ij(t) = 1(cid:3)
i (t)] Pr(cid:2)Ic

(10)

for all t ≥ 0, c = {1, . . . , C}, and i, j ∈ {1, . . . ,|V|}.

4) Expected number of particles study: We know the num-

ber of particles at the beginning of system ˜X(t), so

E[˜nc

i (0)] = ˜nc

i (0)

and, for all t ≥ 0, the expected value E[˜nc

(cid:0)E(cid:2)˜nc
ji(t + 1)(cid:3) − E(cid:2)˜nc

i (t + 1)] is

ij(t + 1)(cid:3)(cid:1)

E[˜nc

i (t)] +

(cid:88)

j

+ E[˜gc

i (t + 1)] − E[˜ac

i (t + 1)] .

However, the expected number of particles that were ab-
sorbed in vi is the expected number of particles in vi minus
the expected number of particles that survived when moving
away. Thus, E[˜nc

i (t + 1)] can be written as

E[˜nc

i (t)] +

+ E[˜gc

And, ﬁnally

j

(cid:88)

(cid:0)E(cid:2)˜nc
ji(t + 1)(cid:3) − E(cid:2)˜nc
ij(t + 1)(cid:3)(cid:1)
ij(t + 1)(cid:3) .
E[˜nc
i (t)] −(cid:88)
E(cid:2)˜nc
E(cid:2)˜nc
ji(t + 1)(cid:3) + E[˜gc

i (t + 1)] −
(cid:88)

j

E[˜nc

i (t + 1)] =

i (t + 1)] ,
for all t ≥ 0, c = {1, . . . , C}, and i ∈ {1, . . . ,|V|}.

j

(11)

PREPRINT SUBMITTED TO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS

7

ij(t + 1; κX0) = nc
nc
= κ · nc

gc
i (κn, t + 1) = κρc

ij(κXt)

ij(Xt) = κ · nc

ij(t; κX0)pc
ij(t; X0)pc
i max{0, 1 · nc(0) − 1 · n}
= κ · gc

ij(t + 1; X0)

i (n, t + 1),

(cid:88)
(cid:88)

j

nc

=

i (t + 1; κX0)
ji(t + 1; κX0) + gc
nc
i (nc
ji(t + 1; X0) + κ · gc
nc

= κ

j

and

i (t; κX0) , t + 1)

i (nc

i (t; X0) , t + 1)
= κ · nc

i (t + 1; X0),

ij(t; κX0) + nc

ij(t + 1; κX0) = δc
δc
= κδc

ij(t; X0) + κ · nc

ij(t + 1; X0) = κ · δc
So Relation (12) indeed holds true for t + 1.

ij(t + 1; κX0)

ij(t + 1; X0)

Since both the basis and the inductive step have been
performed, by mathematical induction, the lemma is proved
for all t ≥ 0 natural.

E[˜nc

Finally, using these studies, we may prove the theorem.
Proof of Theorem 1: By Equations (9)–(11), we have

i (t + 1)] =

(cid:88)
E(cid:2)˜nc
ji(t + 1)(cid:3) + E[˜gc
ij(t) = 1(cid:3) ,
i (t)] Pr(cid:2)Ic
ij(t + 1)(cid:3) = E[˜nc
E(cid:2)˜nc
(cid:104)˜δc
(cid:105)
(cid:104)˜δc
+ E(cid:2)˜nc
ij(t + 1)(cid:3) ,

ij(t + 1)



i (t + 1)] ,

ij(t)

(cid:105)

= E

E

j

i (0) = nc

which is system X assuming that Inequations (7) and (8) tend
to equality when there is a large number of particles and
i (0), for any k > 0 constant (scale invariance
κ˜nc
property).
Remark 1. Even if convergence of Inequations (7) and (8)
true, another property that possibly makes the
were not
two systems equivalent
is the compensation over time. At
the beginning, both systems are equal; however, in the next
iteration both absorption probability (7) and generated parti-
cles (8) are underestimated. Consequently, particles that have
survived may compensate the ones that were not generated.
Furthermore,
the lower the number of absorbed particles
in an iteration, the higher the absorption probability in the
next iteration. Likewise, the lower the number of generated
particles in an iteration, the higher is the expected number of
new particles in the next iteration.

III. SEMI-SUPERVISED LEARNING BY EDGE DOMINATION
Unfoldings generated by Edge Domination System are
incorpored in a semi-supervised learning model. Consider two
sets Xlabeled = {x1, . . . , xl} and Xunlabeled = {xl+1, . . . , xl+u}
such that xi ∈ RD for all i. Each data point xi ∈ Xlabeled
is associated with a label yi ∈ {1, . . . , C}. In the semi-
supervised learning setting, our goal is to correctly assign
existing labels to the unlabeled data Xunlabeled.

In short, the proposed learning model has three steps: a) a
network is constructed based on a dataset, where vertices
repesent data points and edges represent similarity relation-
ship; b) Edge Domination System is applied to obtain the
unfoldings, that is, a distinct set of edges for each class of the
dataset; and c) infer labels for every data point in Xunlabeled.
Next, each step of the proposed learning model is presented
in detail. Further to the model’s algorithm description, its
computational complexity analysis is also presented.

Since the proposed dynamical system takes place on a
complex network, the problem dataset needs to be represented
in a graph structure. Therefore, the ﬁrst step of our learning
model is to obtain a graph representation that stands as a
complex network for the system. Each data point must be
associated with a single vertex of the network. Moreover,
the network must be sparse, undirected, and unweighted.
Labeled vertices correspond to the set of points in Xlabeled,
and unlabeled vertices to the set of points in Xunlabeled. Two
vertices are connected by an edge if they have a relationship
of similarity, which is determined by some metric or by
the particular problem. Any graph construction method that
satisﬁes such conditions may be used in this step. The k-NN
graph construction method is one of them.

The second step is to run system X deﬁned by Equation (3)
using the constructed complex network as its input. Two
conditions are satisﬁed on the system initialization. First, no
class should be privileged. Second, during the ﬁrst iterations,
all particles should be able to ﬂow within the network with a
small probability of absorption. Thus, the initial conditions of
the system, for all i, j, and c ∈ {1, . . . , C}, are

nc

i (0) =

deg vi
2|E| ,

ij(0) = 0 ,
nc
ij(0) = 0 .
δc

(13)

Since there are always particles in the system, the iteration
of system X should be stopped if the time limit has been
reached. The time limit parameter τ controls the maximum
number of iterations of the system.
At the last step, the networks Gc(τ ) are used in the vertex
classiﬁcation. We assign a label yj ∈ {1, . . . , C} for each
unlabeled vertex vj ∈ U, with the information provided by
the networks Gc. Label yj is assigned based on the density of
edges in its neighborhood. Formally, the label for the vj is

|E(Nc,j)| ,

yj = arg max
c∈{1,...,C}

(14)
where Nc,j is the neighborhood of vj in the unfolding Gc(τ ).
We denote the number of edges in this neighborhood as
|E(Nc,j)|.

A. Algorithm

Algorithm 1 summarizes the steps of our learning model.
The algorithm accepts the labeled dataset Xlabeled, the unla-
beled dataset Xunlabeled, and 2 user-deﬁned parameters—the
competition (λ) parameter of the system X and the time limit
parameter (τ). Moreover, it is necessary to choose a network
formation technique.

PREPRINT SUBMITTED TO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS

8

Algorithm 1 Semi-supervised Learning by Edge Domination.
1: function CLASSIFIER(Xlabeled, Xunlabeled, λ, τ)
G ← BUILDNETWORK(Xlabeled, Xunlabeled)
2:
subnetworks ← UNFOLD(G, λ, τ)
3:
return CLASSIFY(Xunlabeled, subnetworks)
4:
5: end function

nc ← n0(G, c)
N c ← N0(G, c)
∆c ← ∆0(G, c)
end for
for t ∈ {1, . . . , τ} do

Algorithm 2 Edge Domination System.
1: function UNFOLD(G, λ, τ)
for c ∈ {1, . . . , C} do
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17: end function

end for
return SUBNETWORKS(G, ∆c)

end for

for c ∈ {1, . . . , C} do
P c ← P (G, N 1, . . . , N C, λ) (cid:46) Equation (4)
gc ← g(G, nc, t)
(cid:46) Equation (6)
N c ← diag nc × P c
nc ← nc × P c + gc
∆c ← ∆c + N c

(cid:46) Equation (13)

(cid:46) Equation (1)

The ﬁrst step of the learning model is mapping the orig-
inal data to a network using a chosen network formation
technique. Afterwards, we unfold the network as described
in Algorithm 2. This algorithm iterates the Edge Domination
System producing one subnetwork for each class. Steps 2–6
initialize the system state as indicated in Equation (13). Steps
7–15 iterate the system until τ using the evolution function φ
(3). Step 16 calculates and returns the unfoldings for each
class. Back to Algorithm 1, by using the produced unfoldings,
the unlabeled data are classiﬁed as described in Equation (14).

B. Computational Complexity and Running Time

Here, we provide the computational complexity analysis

step by step.

The construction of the complex network from the input
dataset depends on the chosen graph construction method.
Since |V| = |Xlabeled| + |Xunlabeled| is the number of data
samples. The k-NN method, for example, has complexity or-
der of O(D |V| log |V|) using multidimensional binary search
tree [22].

The second step is running system X deﬁned by Equa-
tion (3). Using sparse matrices,
the system initializa-
tion, steps 2–6 of Algorithm 2, has complexity order of
O(C |V| + C |E|). The system iteration calculates τ C times
the evolution function φ (3) represented in steps 8–14. The
time complexity of each part of the system evolution is
presented below.
• Step 9, computation of the matrix P c. This matrix has |E|
non-zero entries. It is necessary to calculate σc
ij for each
non-zero entry. Hence, this step has complexity order of

TIME COMPLEXITY OF COMMON GRAPH-BASED TECHNIQUES

DISREGARDING THE GRAPH CONSTRUCTION STEP

TABLE I

Algorithm

Time Complexity

Transductive SVM [9]
Local and Global Consistency [23]
Large Scale Transductive SVM [24]
Dynamic Label Propagation [25]
Label Propagation [26]
Vertex Domination [18]
Edge Domination
Minimum Tree Cut [27]

C |V|3
|V|3
C |V|2
|V|2
|V|2

C2 |V| + C |E|
C |V| + C |E|

|V|

O(C |E|). However, the denominator of Equation (5) is
the same for all values of c.
• Step 10, computation of the vector gc. This vector has |L|
non-zero entries. It is also necessary to calculate the total
number of particles in the system. So, this calculation has
time complexity order of O(|L| + |V|).

• Step 11, computation of the matrix N c. The multiplica-
tion between a diagonal matrix and a sparse matrix with
|E| non-zero entries has time complexity order of O(|E|).
• Step 12, computation of the vector nc. Suppose that (cid:104)k(cid:105)
is the average node degree of the input network; it follows
that this can be performed in O(|V|(cid:104)k(cid:105)) = O(|E|).
summation has complexity order of O(|E|).

• Step 13, computation of the matrix ∆c. This sparse matrix

After system evolution,
the unfolding process performs
O(C |E|) operations. Thus, the total time complexity order of
the system simulation is O(τ C |E| + τ C |V|). However, the
value of τ is ﬁxed and the value of C is usually very small.
The vertex labeling step is the last step of the learning
model. The time complexity of this step depends on the
calculation of the number of edges in the neighborhood
of each unlabeled vertex in each unfolding. It can be efﬁ-
ciently calculated by using one step of a breadth-ﬁrst search
in Gc. Hence, the order of the average-time complexity is

O(cid:0)C |U|(cid:104)k(cid:105)2(cid:1) ≈ O(C |E|).

In summary, considering the discussion above, our learning
model runs in O(D |V| log |V| + C |E| + C |V|) including the
transformation from vector-based dataset to a network. Fur-
thermore, every graph-based algorithm must either implicitly
or explicitly construct the graph. Table I compares the time
complexity of common graph-based techniques disregarding
the graph construction step. Only the proposed Edge Domina-
tion and Minimum Tree Cut [27] have linear time, though
the latter must either receive or construct a spanning tree.
Consequently,
the Minimum Tree Cut has a performance
similar to the scalable version of traditional algorithms, such
as those using subsampling practices.

Figure 2 depicts the running time of single iterations of the
system varying the number of vertices and edges, respectively.
With 10 independent runs, we measure the time for 30
iterations, totalizing 300 samples for each network size. We set
λ = 1, two classes, and 5% of labeled vertices. Experiments
were run on an Intel R(cid:13) CoreTM i7 CPU 860 @ 2.80GHz with
16 GB RAM memory DDR3 @ 1333 MHz. This experiment
the system iteration runs in linear time
demonstrates that

PREPRINT SUBMITTED TO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS

9

Fig. 3. Proportionality simulation. Lines are the correlation measure between
the cumulative domination matrices of systems X and ˜X, varying the initial
number of active particles. Values close to 1 indicate that the cumulative
domination matrices of both systems tend to be proportional.

(a)

(b)

Fig. 2.
Running time in seconds of iterations of the system in random
networks. (a) The input networks have 400000 edges and many different
numbers of vertices. (b) 2000 vertices and many different numbers of edges.

for the number of vertices and edges, which conforms our
theoretical analysis.

Fig. 4.
(left-hand side) and without noise (right-hand side). Colors are the classes.

Three dimensional knot torus dataset with 500 samples with noise

IV. COMPUTER SIMULATIONS

To study the systems X and ˜X, we present experimental
analyses that concern their equivalence. Additionally, we study
the meaning of the parameters of our learning model. After
that, we evaluate the model performance using both artiﬁcial
and synthetic datasets. Then, we show the unfolding process
and the learning model on synthetic data. Finally, we present
the simulation results for a well-known benchmark dataset and
for a real application on human activity recognition.

We consider the correlation between the cumulative domi-
nation matrices of systems X and ˜X. If the two matrices are
proportional, then they must be correlated. Values of correla-
tion close to 1 indicate the cumulative domination matrices
are proportional. In Figure 3,
the correlation is depicted.
As the number of initial particles increases, the correlation
approaches 1. This result suggests that both systems generate
the same unfolding when the number of initial particles grows
to inﬁnity.

A. Experimental Analysis

B. Parameter Analysis

In this section, we present a experiment that assesses the
equivalence between the unfolding results of both systems with
an increasing initial number of particles in system ˜X.

The networks used for the analysis were generated by the
following model: A complex network is constructed given a
labeled vector y, a number m > 0 of edges by vertex, and
a weight p ∈ [0, 1] that controls the preferential attachment
between vertices of different classes. The resulting network
contains |y| vertices. For each vi, m edges are randomly
the preferential
connected, with replacement. If yi = yj,
attachment weight is 1 − p; otherwise, the weight is p. The
parameter p is proportional to the overlap between classes.

If there exists a positive constant κ such that

˜δc
ij(t) = κδc

ij(t),

both systems generate the same unfoldings. In order to assess
this proportionality, both systems were simulated in 10 differ-
ent networks G(y, 3, 0.05), with |y| = 200 vertices arranged
in two classes. The system’s parameter was discretized in
λ = {0, 0.5, 1}. Varying the total number of initial particles,
i (0) ∼ deg vi for all c ∈ {0, . . . , C} and
we set ˜nc
i ∈ {1, . . . ,|V|}.

i (0) = nc

The Edge Domination model has two parameters apart
from the network construction. In this section, we discuss
their meaning. To do so, the learning model was applied in
synthetic datasets whose data items were sampled from a three
dimensional knot torus v(θ) with parametric curve

x(θ) = r(θ) cos 3θ,

y(θ) = r(θ) sin 3θ,
z(θ) = − sin 4θ,

where θ ∈ [0, 2π] and r(θ) = 2 + cos 4θ.

We sampled 500 data items uniformly along the possible
values of θ. We randomly split the data items from 2 to 10
classes so that the samples with adjacent θ belongs to the same
class. We also added to each sample a random noise in each
dimension with distribution N (0, σ) with σ = 0.25 and 0.35.
Figure 4 depicts an example of the dataset with 4 classes with
and without noise. Since the dataset has a complex form, a
small change of parameter value may generate different results.
Therefore, it is suitable to study the sensitivity of parameters.
We run the Edge Domination model with parameters λ ∈
{0.25, 0.5, 0.75, 1} and τ = 500. Finally, 30 unbiased sets of

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.100.150.2010001100120013001400150016001700180019002000Number of verticesTime (s)lllllll0.20.30.4400520640760880100011201240136014801600Number of edges (x1000)Time (s)l=0l=0.5l=1llllllllllllllllllllllllllllllllllll0.51.0101102103104101102103104101102103104Number of particlesCorrelationlt = 10t = 50t = 100xyzllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllxyzllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllPREPRINT SUBMITTED TO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS

10

Fig. 5. Average error of the proposed model for different numbers of classes
in the problem. Colors and shapes indicate the values of parameter λ.

40 labeled points were employed. The k-NN is used for the
network construction with k = {4, 5, . . . , 10}.

Below, we discuss each parameter of the model.
1) Discussion about the network construction parameter:
In our model, the input network must be simple (between any
pair of vertices there must exist at most one edge), unweighted,
undirected, and connected. Besides these requirements, two
vertices must be connected if their data items are considered
similar enough to the particular problem. In our experiments,
we use k-NN graph with Euclidean distance since it was
proved to approximate the low-dimensional manifold of a set
of points [28]. Usually, the smaller the value of k, the better
are the results.

2) Discussion about the system parameter: The Edge Dom-
ination system has only one parameter: the competition pa-
rameter λ. This parameter deﬁnes the intensity of competition
between particles. When λ = 0, particles randomly walk the
network, without competition. As λ approaches to 1, particles
are more likely to compete and, consequently, to be absorbed.
Figure 5 depicts the average error of our method with different
values of λ. Based on the ﬁgure, our model is not sensitive to
λ. In general, we suggest setting λ = 1 because of better and
more consistent classiﬁcation than with other values.

3) Discussion about the system iteration stopping parame-
ter: The time limit parameter τ controls when the simulation
should stop. The parameter τ must be at least as large as the
diameter of the input network. That way, it is guaranteed every
edge to be visited by a particle. Since the network diameter is
usually a small value, the simulation stops in few iterations.

C. Simulations on Artiﬁcial Datasets

For better understanding the details of the Edge Domination
system, in this subsection we illustrate it using two synthetic
datasets. Each dataset has a different class distribution—
banana shape and Highleyman. (The datasets were gener-
ated using the PRTools framework [29].) The banana shape
dataset
is uniformly distributed along speciﬁc shape, and
then superimposed on a normal distribution with the standard
deviation of 1 along the axes. In Highleyman distribution,
two classes are deﬁned by bivariate normal distributions with
different parameters. Because the datasets are not in a network
representation, we use the k-Nearest Neighbor (k-NN) graph

(a)

(b)

Fig. 6.
Unfoldings generated by the proposed system at time t = 100 on
Highleyman dataset. Edges are colored according to the dominating class at
the time. Light gray edges stand for edges presented in the original network
but not in the unfolding. (a) Vertex position is imposed by the original data
points and blue squares represent vertices connected in both unfoldings. (b)
Vertex position is not imposed by the original data points and color of the
vertices are the result of the classiﬁcation.

construction method to transform them into respective network
form. In the constructed network, a vertex represents a datum,
and it connects to its k nearest neighbors—determined by
the Euclidean distance. We set the parameter λ = 1 for the
simulation.

Firstly, the technique is tested on the Highleyman dataset.
Each class has 300 samples, of which 6 are labeled. (We set
k = 10 for the k-NN algorithm.) We can observe that the
labeled data points of the green class form a barrier to samples
of the red class. The unfoldings Gred(100) and Ggreen(100) are
presented in Figure 6a. In this ﬁgure, blue squares represent
vertices that are connected by edges of both unfoldings.
Besides of the labeled data of green class forming a barrier, the
constructed subnetworks are still connected—there is a single
component connecting all the vertices of the subnetwork. It
is better visualized in Figure 6b. In this ﬁgure, the same
unfoldings are presented, but the positions of the vertices
are not imposed by the original data. Furthermore, colors
of vertices in the ﬁgure indicates the result of classiﬁcation.
The overlapping data can be identiﬁed by the vertices that
belongs to two or more unfoldings. This result reveals that
the competition system in edges provide more information
than the competition in vertices, since it is able to identify
the overlapping vertices as part of the system, that is, without
special treatments or modiﬁcations.

The last synthetic dataset has 600 samples equally split
into two classes. In Figure 7a, the initial state of the system
is illustrated, where the dataset is represented by a network.
(The network representation is obtained by setting k = 4 for
the k-NN–graph construction.) At this stage, the edges are

llllllllllllllllll0.060.080.100.120.100.120.140.160.18s=0.25s=0.352345678910Number of classesAverage errorll0.250.50.751llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllPREPRINT SUBMITTED TO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS

11

TEST ERRORS (%) WITH STANDARD DEVIATION AND THE BEST

TABLE II

PARAMETERS

10 labeled
42.90 ± 4.33
46.94 ± 3.93
4.93 ± 2.63
15.65 ± 3.81
59.96 ± 6.13
47.56 ± 1.80
29.71 ± 3.53

k
10
9
5
3
3
9
9

λ
0.25
0
0.75
1
0.625
1
0.875

100 labeled
30.03 ± 2.18
36.08 ± 6.32
1.51 ± 0.31
8.36 ± 2.92
13.73 ± 2.91
34.68 ± 2.26
22.41 ± 1.74

k
10
9
6
3
3
3
10

λ
0.875
0
0.625
1
0
0.25
0.75

g241c
g241n
Digit1
USPS
COIL
BCI
Text

points incorrectly labeled—over the splits. We compare our
results with the following techniques: 1-Nearest Neighbors (1-
NN), Support Vector Machines (SVM), Maximum variance
unfolding (MVU + 1-NN), Laplacian eigenmaps (LEM + 1-
NN), Quadratic criterion and class mass regularization (QC
+ CMR), Discrete regularization (Discrete reg.), Transductive
support vector machines (TSVM), Cluster kernels (Cluster-
Kernel), Low-density separation (LDS), Laplacian regularized
least squares (Laplacian RLS), Local and global consistency
(LGC), Label propagation (LP), Linear neighborhood propa-
gation (LNP), and Network-Based Stochastic Semisupervised
Learning (Vertex Domination), The simulation results were
collected from [3], except for LGC, LP, LNP, and Vertex
Domination that are found in [18].
For the simulation of the Edge Domination system, we dis-
cretize the interval of the parameter in λ = {0, 0.125, . . . , 1}.
Also, we vary the k-NN parameter k ∈ {1, 2, . . . , 10}. We
tested every combination of k and λ. Moreover, we ﬁxed
τ = 1000. In Table II we present the results with the standard
deviation over the splits, along with the best combination of
parameters that generated the best accuracy result.

The test error comparison for 10 labeled points are shown in
Table III; comparison for 100 labeled points are in Table IV.
Apart from each dataset,
the last column is the average
performance rank of a technique over the datasets. A ranking
arranges the methods under comparison by test error rate in
ascending order. For a single dataset, we assign rank 1 for
the method with the lowest average test error on that dataset,
then rank 2 for the method with the second lowest test error,
and so on. The average ranking is the average value of the
rankings of the method on all the datasets. The smaller the
ranking score, the better the method has performed.

From the average rank column, the Edge Domination tech-
nique is not the best ranked, but it is in the best group of
techniques in both 10 labeled and 100 labeled cases.

We statistically compare the results presented in Tables III
and IV. For all tests we set a signiﬁcance level of 5%. First,
we use a test based on the average rank of each method
to evaluate the null hypothesis that all the techniques are
equivalent. With the Friedman test [30], there is statistically
signiﬁcant difference between the rank of the techniques

Since the Friedman test result reports statistical signiﬁcance,
we use the Wilcoxon signed-rank test [30]. In this pairwise
difference test, we test for the null hypothesis that the ﬁrst
technique has greater or equal error results than the second.
If rejected at a 5% signiﬁcance level, then we say the ﬁrst

(a)

(c)

(b)

(d)

Fig. 7.
System evolution on a banana-shaped distribution dataset. Red
and green colors represent the two classes. Unlabeled points are black one;
labeled vertices are represented by larger and colored points. Edges are colored
according to the dominating class at current iteration, where a light gray point
stands for a vertex, which is not dominated yet. (a) The network representation
of the dataset, at the beginning of the system. (b) and (c) System iteration at
time 4 and 20, respectively. (d) The result of the dataset classiﬁcation.

ij +δred

ji > δgreen

not dominated by any of the classes. Starting from this state,
labeled vertices (sources) generate particles that carry the label
of the sources. Though the particles are not shown, Figures 7b
and 7c are snapshots of the system evolution—at time 4 and
20—where each edge is colored by its dominating class at that
iteration. In these illustrations, a solid red line stands for an
ij +δgreen
edge (i, j) that δred
, while a dashed green
ji
ij + δgreen
line stands for the opposite. When δred
,
an edge is drawn in a solid light gray line. As expected, edges
close to sources are dominated ﬁrstly, and farther edges are
progressively dominated. At time 20, Figure 7c, every edge
has been dominated and the edge domination does not change
anymore. Figure 7d shows dataset classiﬁcation following the
system result. In this example, with 1% of points in the
labeled set, the technique is able to correctly identify the
pattern formed by each class. Both results are satisfactory,
reinforcing the ability of the technique of learning arbitrary
class distributions.

ji = δgreen

ij + δred

ji

D. Simulations on Benchmark Datasets

We compare our model with 14 semi-supervised techniques
tested on Chapelle’s benchmark [3]. The benchmark is formed
by seven datasets that have 1500 data points, except for BCI
that has 400 points. The datasets are described in [3].

For each dataset, 24 distinct, unbiased sets (splits) of labeled
points are provided within the benchmark. Half of the splits
are formed by 10 labeled points, and the other half by 100
labeled points. The author of the benchmark ensured that
each split contains at
least one data point of each class.
The result is the average test error—the proportion of data

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllPREPRINT SUBMITTED TO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS

12

TEST ERRORS (%) WITH 10 LABELED TRAINING POINTS

TABLE III

1-NN
SVM
MVU + 1-NN
LEM + 1-NN
QC + CMR
Discrete Reg.
TSVM
Cluster–Kernel
LDS
Laplacian RLS
LGC
LP
LNP
Vertex Domination
Edge Domination

g241c
47.88
47.32
47.15
44.05
39.96
49.59
24.71
48.28
28.85
43.85
45.82
42.61
47.82
41.17
42.90

g241d
46.72
46.66
45.56
43.22
46.55
49.05
50.08
42.05
50.63
45.68
44.09
41.93
46.24
43.51
46.94

Digit1
13.65
30.60
14.42
23.47
9.80
12.64
17.77
18.73
15.63
5.44
9.89
11.31
8.58
8.10
4.93

USPS
16.66
20.03
23.34
19.82
13.61
16.07
25.20
19.41
17.57
18.99
9.03
14.83
17.87
15.69
15.65

COIL
63.36
68.86
62.62
65.91
59.63
63.38
67.50
67.32
61.90
54.54
63.45
55.82
55.50
54.18
59.96

BCI
49.00
49.85
47.95
48.74
50.36
49.51
49.15
48.31
49.27
48.97
47.09
46.37
47.65
48.00
47.56

TEST ERRORS (%) WITH 100 LABELED TRAINING POINTS

TABLE IV

1-NN
SVM
MVU + 1-NN
LEM + 1-NN
QC + CMR
Discrete Reg.
TSVM
Cluster-Kernel
LDS
Laplacian RLS
LGC
LP
LNP
Vertex Domination
Edge Domination

g241c
43.93
23.11
43.01
40.28
22.05
43.65
18.46
13.49
18.04
24.36
41.64
30.39
44.13
21.41
30.03

g241d
42.45
24.64
38.20
37.49
28.20
41.65
22.42
4.95
23.74
26.46
40.08
29.22
38.30
25.85
36.08

Digit1
3.89
5.53
2.83
6.12
3.15
2.77
6.15
3.79
3.46
2.92
2.72
3.05
3.27
3.11
1.51

USPS
5.81
9.75
6.50
7.64
6.36
4.68
9.77
9.68
4.96
4.68
3.68
6.98
17.22
4.82
8.36

COIL
17.35
22.93
28.71
23.27
10.03
9.61
25.80
21.99
13.72
11.92
45.55
11.14
11.01
10.94
13.73

BCI
48.67
34.31
47.89
44.83
46.22
47.67
33.25
35.17
43.97
31.36
43.50
42.69
46.22
41.57
34.68

Text
38.12
45.37
45.32
39.44
40.79
40.37
31.21
42.72
27.15
33.68
46.83
49.53
41.06
34.84
29.71

Text
30.11
26.45
32.83
30.77
25.71
24.00
24.52
34.28
23.15
23.57
56.83
40.79
38.45
27.92
22.41

Avg. Rank
9.3
13.0
9.3
9.1
6.9
10.4
10.0
10.1
8.0
5.9
6.9
5.1
7.1
4.0
4.9

Avg. Rank
11.4
8.1
10.6
10.9
6.6
7.1
7.7
7.4
5.4
4.4
9.3
8.3
11.4
5.3
6.0

technique is superior to the second. By analyzing results for
10 and 100 labeled points together, we conclude that our
technique is superior to 1-NN, LEM + 1-NN, and MVU + 1-
NN. Examining separately, for 10 labeled points, our method
is also superior to discrete regularization, cluster kernel, and
SVM. For 100 labeled points, it is also superior to LNP and
LGC; whereas Laplacian RLS is superior to ours.

pendent labeled set for each conﬁguration. We also provide
the original results from [31] using SVM with approximately
70% of labeled samples. Our technique performs as well as
SVM using far fewer labeled samples and using the suggested
parameter set. Such a feature is quite attractive because it may
represents a big saving in money or efforts when involving
manually data labeling in semi-supervised learning.

E. Simulations on Human Activity Dataset

The Human Activity Recognition Using Smartphones [31]
dataset comprises of 10299 data samples. Each sample
matches 561 features extracted from motion sensors attached
to a person during a time window. Each person performed
six activities which are target labels in the dataset—walking
(WK), walking upstairs (WU), walking downstairs (WD),
sitting (ST), standing (SD), and laying down (LD).

We use k-NN with k = 7 for the dataset network represen-
tation once it is the smallest value that generates a connected
network. The parameters are ﬁxed in λ = 1 and τ = 1000. We
compare our results with the ones published in [31], splitting
the problem into six binary classiﬁcation tasks.

Table V summarises the results. For our technique, we
provide the precision, recall and F Score using 5%, 10%, and
20% of labeled samples. We average the results of 10 inde-

V. CONCLUSION

We presented a transductive semi-supervised learning tech-
nique based on a dynamical system on complex networks.
First, the input data is mapped into a network. Then, the
Edge Domination System particle competition runs on this
network. At this stage, particles compete for edges in the
network. When a particle passes through an edge, it increases
its class dominance over the edge while decreasing other
classes’ dominance. Three dynamics—of particle walking,
absorption and production—provide a biologically inspired
scenario of competition and cooperation. Finally, labels are
assigned according to the closest labeled vertex in the sub-
networks generated by the system. As a result, the system
unfolds the original network by grouping edges dominated by
the same class. Furthermore, rigorous studies have been done
on the novel Edge Domination System.

PREPRINT SUBMITTED TO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS

13

PERFORMANCE COMPARISON IN THE HUMAN ACTIVITY RECOGNITION USING SMARTPHONES DATASET

TABLE V

Precision
WK .984 ± .013
.981 ± .009
WU
.987 ± .017
WD
.864 ± .034
ST
.840 ± .024
SD
.996 ± .002
LD

5% labeled

Recall

.941 ± .030
.935 ± .026
.901 ± .016
.698 ± .049
.842 ± .053
.999 ± .000

F Score

.962 ± .016
.957 ± .015
.942 ± .011
.770 ± .022
.839 ± .017
.998 ± .001

Precision
.992 ± .004
.988 ± .008
.994 ± .008
.883 ± .015
.870 ± .023
.997 ± .001

Edge Domination

10% labeled

Recall

.985 ± .011
.961 ± .013
.918 ± .011
.743 ± .039
.844 ± .022
.999 ± .000

F Score

.989 ± .006
.974 ± .008
.955 ± .007
.806 ± .020
.856 ± .006
.998 ± .000

Precision
.994 ± .002
.991 ± .003
.998 ± .001
.905 ± .014
.896 ± .013
.997 ± .001

20% labeled

Recall

.997 ± .001
.981 ± .006
.945 ± .008
.814 ± .015
.872 ± .021
.999 ± .000

F Score

.995 ± .001
.986 ± .004
.971 ± .004
.857 ± .006
.884 ± .009
.998 ± .000

SVM

≈70% labeled [31]

Precision

F Score

Recall
.992
.958
.976
.880
.974
1.000

.974
.969
.982
.922
.936
1.000

.957
.980
.988
.969
.901
1.000

[13] M. Belkin, P. Niyogi, and V. Sindhwani, “Manifold regularization: A
geometric framework for learning from labeled and unlabeled examples,”
J. Mach. Learn. Res., vol. 7, pp. 2399–2434, 2006.

[14] A. Barrat, M. Barth´elemy, and A. Vespignani, Dynamical processes on
complex networks. New York, NY: Cambridge University Press, 2008.
[15] M. E. J. Newman, Networks: An Introduction, 1st ed. New York, NY:

Oxford University Press, 2010.

[16] X. Zhu, A. B. Goldberg, and T. Khot, “Some new directions in
graph-based semi-supervised learning,” Proc. of the EEE International
Conference on Multimedia and Expo, pp. 1504–1507, 2009.

[17] M. G. Quiles, L. Zhao, R. L. Alonso, and R. A. F. Romero, “Particle
competition for complex network community detection,” Chaos, vol. 18,
no. 3, 2008.

[18] T. C. Silva and L. Zhao, “Network-based stochastic semisupervised
learning,” IEEE Trans. Neural Netw. Learn. Syst., vol. 23, no. 3, pp.
451–66, 2012.

[19] ——, “Network-based stochastic semisupervised learning,” IEEE Trans.

Neural Netw. Learn. Syst., vol. 23, no. 3, pp. 451–466, 2012.

[20] ——, “Detecting and preventing error propagation via competitive

learning,” Neural Networks, vol. 41, pp. 70–84, 2013.

[21] J. Jensen, “Sur les fonctions convexes et les in´egalit´es entre les valeurs

moyennes,” Acta Mathematica, vol. 30, no. 1, pp. 175–193, 1906.

[22] J. L. Bentley, “Multidimensional binary search trees used for associative

searching,” Commun. ACM, vol. 18, no. 9, pp. 509–517, 1975.

[23] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Sch¨olkopf, “Learning
with local and global consistency,” in Advances in Neural Information
Processing Systems 16, S. Thrun, L. Saul, and B. Sch¨olkopf, Eds. MIT
Press, 2004, pp. 321–328.

[24] R. Collobert, F. Sinz, J. Weston, and L. Bottou, “Large Scale Transduc-
tive SVMs,” Journal of Machine Learning Research, vol. 7, no. 7, pp.
1687–1712, 2006.

[25] B. Wang, Z. Tu, and J. K. Tsotsos, “Dynamic label propagation for
semi-supervised multi-class multi-label classiﬁcation,” Proc. of the IEEE
International Conference on Computer Vision, pp. 425–432, 2013.

[26] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled
data with label propagation,” School Comput Sci Carnegie Mellon Univ
Pittsburgh PA Tech Rep, vol. 54, no. CMU-CALD-02-107, pp. 1–19,
2002.

[27] Y. M. Zhang, K. Huang, G. G. Geng, and C. L. Liu, “MTC: A Fast
and Robust Graph-Based Transductive Learning Method,” IEEE Trans
Neural Netw Learn Syst, vol. 26, no. 9, pp. 1979–1991, 2014.

[28] J. B. Tenenbaum, V. de Silva, and J. C. Langford, “A global geometric
framework for nonlinear dimensionality reduction,” Science, vol. 290,
no. 5500, p. 2319, 2000.

[29] R. Duin, P. Juszczak, P. Pacl´ık, E. Pekalska, D. de Ridder, D. Tax, and
S. Verzakov, “PR-Tools4.1, a matlab toolbox for pattern recognition,”
2007.

[30] M. Hollander and D. A. Wolfe, Nonparametric Statistical Methods,

2nd ed. Wiley-Interscience, 1999.

[31] D. Anguita, A. Ghio, L. Oneto, X. Parra, and J. L. Reyes-Ortiz, “A
Public Domain Dataset for Human Activity Recognition Using Smart-
phones,” in 21th European Symposium on Artiﬁcial Neural Networks,
Computational Intelligence and Machine Learning, ESANN 2013, 2013.

The deterministic system implementation brings advantages
over its stochastic counterpart. The time complexity of the
deterministic one does not depend on the number of particles,
so we are beneﬁted from better results when considering an
asymptotically inﬁnite number of initial particles. Besides, the
Edge Domination system presents a stable transductive semi-
supervised learning technique with a subquadratic order of
complexity. Computer simulations show the proposed tech-
nique achieves good classiﬁcation accuracy and it is suitable
to be applied in the situation where a small number of
labeled samples are available. Another interesting feature of
the proposed model is that it directly provides the overlapping
information of each vertex or a subset of vertices.

As future works, we would like to investigate the mathe-
matical property of the Edge Domination system on directed
or weighted networks. Besides of this, it is interesting to
further improve the runtime via network sampling methods or
estimation methods. In this way, the model will be suitable to
be applied to process large enough datasets or streaming data.
Another interesting research is to treat the labels on edges
instead of the nodes in semi-supervised learning environment.

REFERENCES

[1] C. M. Bishop, Pattern Recognition and Machine Learning. Secaucus,

NJ: Springer, 2006.

[2] S. Russell and P. Norvig, Artiﬁcial Intelligence: A Modern Approach,

3rd ed. Upper Saddle River, NJ: Prentice Hall Press, 2009.

[3] O. Chapelle, B. Sch¨olkopf, and A. Zien, Eds., Semi-Supervised Learning.

Cambridge, MA: MIT Press, 2006.

[4] X. Zhu and A. B. Goldberg, Introduction to Semi-Supervised Learning.

Morgan and Claypool Publishers, 2009, vol. 3.

[5] K. Nigam, A. K. McCallum, S. Thrun, and T. Mitchell, “Text clas-
siﬁcation from labeled and unlabeled documents using EM,” Machine
Learning, vol. 39, no. 2-3, pp. 103–134, 2000.

[6] M. Loog and A. C. Jensen, “Semi-Supervised Nearest Mean Classiﬁca-
tion Through a Constrained Log-Likelihood,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 26, no. 5, pp. 995–1006, 2015.

[7] K. Wagstaff, C. Cardie, S. Rogers, and S. Schroedl, “Constrained k-
means clustering with background knowledge,” in Proc. of the 18th
International Conference on Machine Learning, 2001, pp. 577–584.

[8] Z.-H. Zhou and M. Li, “Tri-training: exploiting unlabeled data using
three classiﬁers,” IEEE Trans. Knowl. Data Eng., vol. 17, no. 11, pp.
1529–1541, 2005.

[9] V. N. Vapnik, Statitical learning theory. New York, NY: Wiley, 1998.
[10] T. C. Silva and L. Zhao, “Semi-supervised learning guided by the
modularity measure in complex networks,” Neurocomputing, vol. 78,
no. 1, pp. 30–37, 2012.

[11] L. Cheng and S. J. Pan, “Semi-supervised Domain Adaptation on
Manifolds,” IEEE Trans. Neural Netw. Learn. Syst., vol. 25, no. 12,
pp. 2240–2249, 2014.

[12] K. Zhang, L. Lan, J. T. Kwok, S. Vucetic, and B. Parvin, “Scaling Up
Graph-Based Semisupervised Learning via Prototype Vector Machines,”
IEEE Trans. Neural Netw. Learn. Syst., vol. 26, no. 3, pp. 444–457,
2015.

PREPRINT SUBMITTED TO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS

14

APPENDIX

SIMULATION ON MAGIC GAMMA TELESCOPE

Due to the low time-complexity of our technique, we can test it in a larger dataset. The MAGIC Gamma Telescope [Bock2004]
consist of 19020 with 11 features each. Classiﬁcation accuracy is not meaningful for this data, since classifying a background
event as signal is worse than classifying a signal event as background. To deal with this issues, alternative performance indices
based on ROC curve are used. For detailed explanation of such indices refer to [Bock2004]. To calculate the ROC curve, we
modify our vertex labeling step to result in a fuzzy classiﬁcation.

Similarly to the previous experiment, we ﬁxed k = 9, λ = 1, and τ = 1000. Table VI summarises the obtained results
from 10 independent sets with 10% of labeled samples. Some of the results from [Bock2004] are provided as well. The Edge
Domination model outperforms all of the methods under comparison using far fewer labeled samples.

PERFORMANCE COMPARISON IN THE MAGIC GAMMA TELESCOPE DATASET

TABLE VI

Labeled

loacc

Q0.5

σ0.5

0.689 ± 0.016 ∞ 15.8 ± 0

σmax(γ )

18.6 ± 0.211 (0.689 ± 0.016)

Method
Edge Domination
NNSU
Random Forest
NeuNet
C5.0
SVM

10%
67%
67%
67%
67%
67%

0.472
0.452
0.445
0.441
0.124

3.5
2.8
3.0
2.7
1.4

9.74
8.44
8.73
8.14
4.81

9.82 (0.483)
8.74 (0.412)
8.75 (0.483)
8.96 (0.408)
5.76 (0.784)

[Bock2004] R. Bock, A. Chilingarian, M. Gaug, F. Hakl, T. Hengstebeck, M. Jiˇrina, J. Klaschka, E. Kotrˇc, P. Savick´y,
S. Towers, A. Vaiciulis, and W. Wittek, “Methods for multidimensional event classiﬁcation: a case study using images
from a Cherenkov gamma-ray telescope,” Nuclear Instruments and Methods in Physics Research Section A: Accelerators,
Spectrometers, Detectors and Associated Equipment, vol. 516, no. 2-3, pp. 511—528, 2004.

