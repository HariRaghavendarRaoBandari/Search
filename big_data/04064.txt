6
1
0
2

 
r
a

 

M
3
1
 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
4
6
0
4
0

.

3
0
6
1
:
v
i
X
r
a

A Grothendieck-type inequality for local maxima

Andrea Montanari∗

March 15, 2016

Abstract

A large number of problems in optimization, machine learning, signal processing can be
eﬀectively addressed by suitable semideﬁnite programming (SDP) relaxations. Unfortunately,
generic SDP solvers hardly scale beyond instances with a few hundreds variables (in the un-
derlying combinatorial problem). On the other hand, it has been observed empirically that
an eﬀective strategy amounts to introducing a (non-convex) rank constraint, and solving the
resulting smooth optimization problem by ascent methods. This non-convex problem has –
generically– a large number of local maxima, and the reason for this success is therefore unclear.
This paper provides rigorous support for this approach. For the problem of maximizing
a linear functional over the elliptope, we prove that all local maxima are within a small gap
from the SDP optimum. In several problems of interest, arbitrarily small relative error can be
achieved by taking the rank constraint k to be of order one, independently of the problem size.

1 Motivation and result

Let PSD(n) ≡ {X ∈ Rn×n : X (cid:23) 0} be the cone of n × n symmetric positive semideﬁnite matrice.
The convex set of positive-semideﬁnite matrices with diagonal entries equal to one will be denoted
by

PSD1(n) ≡ (cid:8)X ∈ Rn×n : X (cid:23) 0, Xii = 1∀i ∈ [n](cid:9) .

The set PSD1(n) is also known as the elliptope. Given a symmetric matrix A, we deﬁne1

(1)

(2)

SDP(A) ≡ max(cid:8)hA, Xi : X ∈ PSD1(n)(cid:9) .

This semideﬁnite program (SDP) arises in a large number of applications in particular as a re-
laxation of max-cut, and min-bisection of graphs. Early references include [GW95, Nes98]. Un-
fortunately, despite the many remarkable properties of this SDP, generic SDP solvers hardly scale
beyond n of the order of a few hundreds.

Recently, several authors [BM03, JMRT16] found that an eﬀective strategy is to constrain the
rank of X to satisfy rank(X) ≤ k, explicitly solve the PSD constraint and use gradient ascent or
coordinate ascent to maximize the resulting non-convex objective.

∗Department of Electrical Engineering and Department of Statistics, Stanford University
1Here and below hA

Bi = Tr(ATB) is the usual scalar product between matrices.

,

1

Explicitly, the rank-constrained problem and be written as a smooth optimization problem over
the manifold S(n, k) ⊂ Rn×k, deﬁned by S(n, k) ≡ {σ = (σ1, σ2, . . . , σn) : σi ∈ Rk,kσik2 = 1}.
This can be identiﬁed with the product of n (k − 1)-dimensional spheres

{z
The objective function FA : S(n, k) → R is deﬁned by:

S(n, k) ≃ Sk−1 × Sk−1 × ··· × Sk−1
}

n times

|

.

FA(σ) ≡

n

Xi,j=1

Aijhσi, σji .

We then deﬁne

(3)

(4)

(5)

OPTk(A) ≡ max(cid:8)FA(σ) : σ ∈ S(n, k)(cid:9) .

Of course we have OPTk(A) ≤ SDP(A), with OPTn(A) = SDP(A). The optimizer of the k = n
problem, σ∗, corresponds to an optimizer of the SDP through X∗ = σ∗σT
∗ . However, already much
smaller values of k give excellent (or exact) approximations of the underlying SDP. For instance,
[JMRT16] used this approach to cluster graphs generated according to the sparse stochastic block
model. Empirically, the resulting solutions are undistinguishable from the SDP optimum already
for k = 20. The resulting algorithm can be used to cluster sparse graphs with up to n = 105 vertices
in a matter of minutes2.

Unfortunately, the optimization problem (5) is non-convex and has –generically– a large number
of local maxima (but see Section 1.1 for a discussion of related work). It is therefore unclear why
coordinate or gradient ascent should ﬁnd a good approximation of OPTk(A), let alone a good
approximation of SDP(A). Our main result shows that in fact all the maxima have value close to
the global optimum, and in fact close to OPTk(A). (Here and below kMk2 denotes the ℓ2 operator
norm of matrix M .)
Theorem 1. Let σ be a local maximum of FA over S(n, k), k ≥ 2. Then

SDP(A) ≥ FA(σ) ≥ SDP(A) −

8
√k

nkAk2 .

(6)

The proof of this theorem is provided in Section 2.

Remark 1.1. In typical applications kAk2 = Θ(1), while SDP(A) = Θ(n). In these cases Theorem
1 guarantees that an arbitrarily small relative error can be achieved by taking k to be a large
constant.

A simple example is the so-called Z2 synchronization problem [JMRT16]. In that case A =
T + W with x0 ∈ {+1,−1}n, and W a GOE radom matrix, i.e. a symmetric matrix
(λ/n)x0x0
with independent entries (Wii)1≤i≤n, Wii ∼ N(0, 2/n), and (Wij)1≤i<j≤n, Wij ∼ N(0, 1/n). Basic
T yields
random matrix theory implies kAk2 ≤ 2 + λ with high probability, while using X = x0x0
SDP(A) ≥ nλ + o(n) (and indeed [MS16] proves SDP(A) ≥ n max(2, λ) + o(n)).
Remark 1.2. A naive application of Theorem 1 to the sparse stochastic block model [JMRT16]
shows that an arbitrarily small error is achieved for k = Cplog n/ log log n, with C a large constant.

In fact this can be improved further to k = O(1), by considering a suitably modiﬁed graph.

2Code for the algorithm of [JMRT16] is available at http://web.stanford.edu/∼montanar/SDPgraph/.

2

1.1 Further related work

Burer and Monteiro [BM03] introduced the the idea of constraining the rank and solving the
PSD constraint thus obtaining a smooth non-convex problem. They also proved that, taking

k ≥ √2n, and under suitable conditions on A, the resulting non-convex problem has no local

maxima, except for the global one. Their result actually extend to more general SDPs than Eq. (2).
While interesting, this result does not clarify the empirical ﬁnding that k = 20 is suﬃcient for some
problems with n as large as 105 [JMRT16].

Journ´ee, Bach, Absil and Sepulchre [JBAS10] proved suﬃcient conditions under which a local
optimum of the rank-constrained problem is in fact a global optimum of the SDP. In particular
they proved that this happens if the local optimum σ ∈ S(n, k) is rank-deﬁcient (i.e. has rank
at most k − 1). It is however unclear a priori when this happens, and even computing the exact
rank of σ is very diﬃcult (since σ is typically produced by an ascent method). In the numerical
experiments of [JMRT16], the local optimum was typically full rank.

Bandeira, Boumal and Voroninski recently considered the extreme case k = 2, in the speciﬁc
example of the Z2 synchronization problem [BBV16]. They proved that, if the signal is strong
enough, then this approach can eﬀectively recover the underlying signal. Namely, all local minima
are correlated with the signal.

The SDP (2) was recently studied in the context of the so-called community detection problem,
namely for recovering vertex labels under the sparse stochastic block model. Among a large number
of interesting contributions, the closes to the present work are the ones concerning the so-called
detection threshold [GV15, MS16]. In particular, [MS16] proves a Grothendieck-type inequality for
OPTk(A). In slightly simpliﬁed form, this implies

OPTk(A) ≥ SDP(A) −

C
k

nkAk2 ,

(7)

with C an absolute constant. This can be immediately compared with Theorem 1, which of course

implies OPTk(A) ≥ SDP(A) − (9/√k) nkAk2, This is similar to the result of [MS16], but has a

weaker dependence on k, thus raising the interesting question of whether the dependence on k in
Theorem 1 can be improved.

Grothendieck inequalities have found a broad range of applications in computer science, see
[KN12] and references therein. However, they are typically used to approximate a combinatorial
problem by solving a semideﬁnite program. Theorem 1 instead points at a diﬀerent direction,
namely solving the SDP with arbitrarily high accuracy by solving a non-convex rank-constrained
problem. Of course this in turn provides an approximation of the original combinatorial problem.
Finally, there has been growing interest in non-convex methods for solving high-dimensional
statistical estimation problems. Examples include matrix completion [KMO10], phase retrieval
[CC15], regression with missing entries [LW11], and many others. These papers provide rigorous
guarantees under the assumption that the noise in the data is ‘small enough.’ Under such conditions,
a very good initialization can be constructed, e.g. by a spectral method, and it is suﬃcient to prove
that the optimization problem is well behaved in a neighborhood of the optimum.

The mechanism studied here is dramatically diﬀerent. Not only we do not require any ‘strong
signal’ condition, but there is actually no signal at all (in other words, no ‘signal plus noise’ structure
is assumed). We do not establish a property of a neighborhood of the correct solution, but instead
a global property of the cost function.

3

1.2 Notations

Throughout the paper, vectors and matrices are denoted by boldface, e.g. x, y, z, . . . . We reserve
the upper case, e.g. A, B, . . . , for n × n matrices. Their coordinates are denoted by non-bold
font, e.g. x = (x1, . . . , xn). We identify an element σ = (σ1, . . . , σn) ∈ S(n, k) with a matrix in
Rn×k. The standard scalar product of x, y ∈ Rn is denoted by hx, yi = Pn
i=1 xiyi. Analogously,
hA, Bi = Tr(ABT) is the scalar product between matrices.
The eigenvalues of a symmetric matrix M are denoted by ξ1(M ) ≥ ξ2(M ) ≥ ··· ≥ ξn(M ) =
ξmin(M ). We let kxk2 = phx, xi be the ℓ2 norm of vector x, kMk2 be the ℓ2 operator norm of
matrix M , and kMkF its Frobenius norm.

2 Proofs

2.1 Preliminary lemmas

Our ﬁrst lemma collects the classical second order conditions for problem (5). While this is standard,
we provide a self-contained proof for the reader’s convenience.

Lemma 2.1. Let σ ∈ S(n, k) be a local maximum of FA on S(n, k). Then there exists a diagonal
matrix Λ of Lagrange multipliers such that the following conditions hold:

1. First order stationarity:

2. Second-order stationarity: for all u = (u1, . . . , un), ui ∈ Rk, such that hσi, uii = 0, we have

(Λ − A)σ = 0 .

(8)

n

Xi,j=1

(Λ − A)ijhui, uji ≥ 0 .

(9)

Proof. Fix u as in point 2 above an let ˆui = ui/kuik2 (with the convention ˆui = 0 if ui = 0).
Deﬁne σ(t) for t ∈ R, by

σi(t) ≡ σi cos(cid:0)kuik2 t(cid:1) + ˆui sin(cid:0)kuik2 t(cid:1) .

(10)

Note that σ(t) ∈ S(n, k) for all t ∈ R, and σ(0) = σ. Also t 7→ σ(t) is smooth. Since σ is a local
maximum of FA, it follows that t = 0 must be a local maximum of t 7→ FA(σ(t)).

By Taylor expansion

σi(t) = σi + ui t −

1
2kuik2

2σi t2 + O(t3) ,

and

FA(σ(t)) = FA(σ(0)) + 2

gi ≡

n

Xj=1

Aij σj .

n

Xi=1

hui, gii t −

n

Xi=1

kuik2

2hσi, gii t2 +

n

Xi,j=1

Aijhui, uji t2 + O(t3)

4

(11)

(12)

(13)

By ﬁrst order stationarity of t 7→ FA(σ(t)), we must have hui, gii = 0 for all ui orthogonal to
σi. Therefore gi = λi σi for some λi ∈ R. This yields Eq. (8) with Λ the diagonal matrix with
Λi,i = λi.

Substituting in Eq. (12), we get

FA(σ(t)) = FA(σ(0)) +

n

Xi,j=1

(A − Λ)ijhui, uji t2 + O(t3) ,

(14)

which immediately implies the claim (9), since t 7→ FA(σ(t)) is a local maximum.

Throughout, for Λ the matrix of Lagrange multipliers, we will denote by λi = Λi,i its diagonal

entries. The next lemma collects a few simple facts about local maxima.

Lemma 2.2. Let σ ∈ S(n, k) be a local maximum of FA on S(n, k). Then we have the following

1. The associated matrix of multipliers Λ is uniquely determined by

n

Xj=1

λi = (cid:13)(cid:13)(cid:13)

.

Aij σj(cid:13)(cid:13)(cid:13)2

(15)

2. FA(σ) = Tr(Λ).

3. For every S ⊆ [n], |S| ≤ k − 1, we have (Λ − A)S,S (cid:23) 0. In particular, λi ≥ 0.
4. kΛk2
5. We have

F = Pn

i ≤ nkAk2
2.

i=1 λ2

n
k

.

ξmin(σTσ) ≤
Proof. For point 1, note that, by Eq. (8), λiσi = Pn
norm of both sides since kσik = 1 (and noting that λi ≥ 0 as proved in point 3).
Point 2 follows again from λiσi = Pn
i ∈ {1, . . . , n}.
For point 3 assume, without loss of generality, that S = {1, 2, . . . , k − 1}. Let u ∈ Rk, kuk2 = 1
be orthogonal to σ1, . . . , σk−1. For any x ∈ Rk−1, deﬁne ui = xi u for i ≤ k − 1, and ui = 0 for
i ≥ k. Then, the second order stationarity condition (9) implies

j=1 Aij σj, multiplying both sides by σi and summing over

j=1 Aij σj. The claim follows by taking the

(16)

0 ≤

n

Xi,j=1

(Λ − A)ijhui, uji = hx, (Λ − A)SSxi ,

which proves the claim.

For point 4, note that, since Λσ = Aσ we also have

n

Xi=1

i = hσ, Λ2σi = hΛσ, Λσi
λ2
= hAσ, Aσi = Tr(A2σσT)
≤ kA2k2kσσTk∗ ,

5

(17)

(18)

(19)

(20)

where kσσTk∗ denotes the nuclear norm (sum of absolute values of singular values) of matrix σσT.
The claim follows since

kσσTk∗ = kσk2

F =

n

Xi=1

kσik2

2 = n .

(21)

(The ﬁrst equality holds because the singular values of σσT are obtained by squaring the singular
values of σ.)

For the last point let

σiσT
i .

n

1
n

1
n

Xi=1

σTσ =

Σ ≡
2/n = 1. Hence Pk

(22)

i=1 ξi(Σ) = 1, and since ξi(Σ) ≥ 0, we

Then Σ (cid:23) 0 and Tr(Σ) = Pn
necessarily have ξmin(Σ) ≤ 1/k.

i=1 kσik2

2.2 Proof of Theorem 1

Let σ ∈ S(n, k) be a local maximum of FA on S(n, k), and Λ be the associated multipliers. By
Lemma 2.2 (point 5), we can ﬁx x ∈ Rk kxk2 = 1 such that

∆ ≡ σx ∈ Rn ,
Let S ⊆ [n], be the set of indices S ≡ {i ∈ [n] :
(u1, . . . , un), ui ∈ Rk, by

|∆i| ≥ 1/√2}. By Markov inequality |S| ≤ 2n/k.
For a vector z ∈ Rn, let ˜z be the vector obtained by zero-ing the entries in S. Deﬁne u =

.

(23)

k∆k2 ≤ r n

k

ui = ˜zi x + ˜ui ,

˜ui ≡ −

˜zi∆i
1 − ∆2

i

P ⊥

x σi ,

(24)

where P ⊥
|∆i| = 1.

x = I − xxT is the projector orthogonal to x. Here, it is understood that ˜ui = 0 when
i . We therefore have

Note that ∆i = hσi, xi ∈ [−1, 1]. Hence kP ⊥

x σii = 1 − ∆2

hσi, uii = ˜zi ∆i −

x σik2
˜zi∆i
1 − ∆2

2 = hσi, P ⊥
i hσi, P ⊥

x σii = 0 .

(25)

n

We can therefore apply the second-order stationarity condition (9), to get (with the notation
vR = (vi : i ∈ R), for R ⊆ [n]):
Xi,j=1
Xi,j=1
≤ h ˜z, (Λ − A) ˜zi +

Xi,j=1
λik ˜uik2

(Λ − A)i,jh ˜ui, ˜uji

(Λ − A)i,jhui, uji

2 + kAk2k ˜u ˜uk∗

0 ≤

(26)

(28)

(27)

=

n

n

n

(Λ − A)i,j ˜zi ˜zj +
Xi=1
Xi=1

n

6

≤ h ˜z, (Λ − A) ˜zi +

λik ˜uik2

2 + kAk2k ˜uk2
F ,

(29)

where in the last inequality kMk∗ denotes the nuclear norm of matrix M (sum of absolute values
of singular values), and we used the fact that kMM Tk∗ = kMk2

F . Now

k ˜uk2

F =

n

2

n

=

Xi=1
k ˜uik2
˜z2
i ∆2
Xi=1
i
1 − ∆2
Xi=1
i ∆2
˜z2
i .
≤ 2

n

i

(30)

(31)

(32)

Therefore, continuing from Eq. (29), ad using a similar bound for the ﬁrst sum, we get the inequality

0 ≤ hzS c , (Λ − A)S c,S czS ci + 2 Xi∈S c

λiz2

i ∆2

i + 2kAk2 Xi∈S c

i ∆2
z2
i ,

(33)

where we made explicit the dependence on S.

Now consider v ∈ S(n, n), i.e. v = (v1, v2, . . . , vn), vi ∈ Sn−1. Applying the last inequality to

the a-th column of v, denoted by va, we get

0 ≤ hva

S c, (Λ − A)S c,S cva

S ci + 2 Xi∈S c

λi(va

i )2∆2

i + 2kAk2 Xi∈S c

(va

i )2∆2
i .

(34)

Summing over a ∈ {1, . . . , n} and using Pn

a=1(va

i )2 = 1, we get

0 ≤ hvS c, (Λ − A)S c,S cvS ci + 2Xi∈S c

λi∆2

i + 2kAk2 Xi∈S c

∆2
i .

Using Lemma 2.2, points 2 and 3 (in particular, λi ≥ 0), we get

(35)

(36)

(37)

(38)

λi∆2

i + 2kAk2 Xi∈S c
hvS c, AS c,S cvS ci ≤ FA(σ) + 2Xi∈S c
≤ FA(σ) + √2 Xi∈S c
λi|∆i| + 2kAk2k∆k2
≤ FA(σ) + √2kΛkFk∆k2 + 2kAk2k∆k2

∆2
i

2

2 ,

where the second inequality follows since |∆i| ≤ 1/√2 for i ∈ S, and the last one by Cauchy-

Schwartz. Finally using Lemma 2.2, point 4, together with Eq. (23), we get

hvS c, AS c,S cvS ci ≤ FA(σ) +r 8

k

nkAk2 .

Next notice that

hvS, AvS ci ≤ kAS,S ck2kvSvT
≤ kAk2kvSkFkvc
≤ r 2

nkAk2 .

k

S ck∗
SkF = kAk2p|S|(n − |S|)

7

(39)

(40)

(41)

(42)

Proceeding analogously, we get

Finally, combining the bounds (39), (42), (43)we get

hvS, AvSi ≤

2
k

nkAk2 .

hv, Avi ≤ FA(σ) +

5√2
√k

nkAk2 .

(43)

(44)

Since this holds for any v ∈ S(n, n), it implies the second inequality in Eq. (6), because 5√2 < 8.

The ﬁrst inequality in Eq. (6) is trivial.

Acknowledgments

This work was partially supported by NSF grants CCF-1319979 and DMS-1106627 and the AFOSR
grant FA9550-13-1-0036.

References

[BBV16] Afonso S. Bandeira, Nicolas Boumal, and Vladislav Voroninski, On the low-rank ap-
proach for semideﬁnite programs arising in synchronization and community detection,
arXiv:1602.04426 (2016).

[BM03]

[CC15]

Samuel Burer and Renato D.C. Monteiro, A nonlinear programming algorithm for solv-
ing semideﬁnite programs via low-rank factorization, Mathematical Programming 95
(2003), no. 2, 329–357.

Yuxin Chen and Emmanuel Candes, Solving random quadratic systems of equations is
nearly as easy as solving linear systems, Advances in Neural Information Processing
Systems, 2015, pp. 739–747.

[GV15]

Olivier Gu´edon and Roman Vershynin, Community detection in sparse networks via
grothendiecks inequality, Probability Theory and Related Fields (2015), 1–25.

[GW95] Michel X. Goemans and David P. Williamson, Improved approximation algorithms for
maximum cut and satisﬁability problems using semideﬁnite programming, Journal of the
ACM (JACM) 42 (1995), no. 6, 1115–1145.

[JBAS10] Michel Journ´ee, Francis Bach, P-A Absil, and Rodolphe Sepulchre, Low-rank optimiza-
tion on the cone of positive semideﬁnite matrices, SIAM Journal on Optimization 20
(2010), no. 5, 2327–2351.

[JMRT16] Adel Javanmard, Andrea Montanari, and Federico Ricci-Tersenghi, Phase transitions
in semideﬁnite relaxations, Proceedings of the National Academy of Sciences (2016), In
press.

[KMO10] Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh, Matrix completion

from noisy entries, Journal of Machine Learning Research 11 (2010), 2057–2078.

8

[KN12]

[LW11]

[MS16]

Subhash Khot and Assaf Naor, Grothendieck-type inequalities in combinatorial optimiza-
tion, Communications on Pure and Applied Mathematics 65 (2012), no. 7, 992–1035.

Po-Ling Loh and Martin J Wainwright, High-dimensional regression with noisy and
missing data: Provable guarantees with non-convexity, Advances in Neural Information
Processing Systems, 2011, pp. 2726–2734.

Andrea Montanari and Subhabrata Sen, Semideﬁnite programs on sparse random graphs
and their application to community detection, Proceedings of the 48th Annual ACM
Symposium on Theory of Computing, ACM, 2016.

[Nes98]

Yuri Nesterov, Semideﬁnite relaxation and nonconvex quadratic optimization, Optimiza-
tion methods and software 9 (1998), no. 1-3, 141–160.

9

