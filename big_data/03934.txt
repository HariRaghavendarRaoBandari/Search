6
1
0
2

 
r
a

 

M
2
1

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
4
3
9
3
0

.

3
0
6
1
:
v
i
X
r
a

Some new ideas in nonparametric estimation

O.V. Lepski ∗

Institut de Math´ematique de Marseille

Aix-Marseille Universit´e
39, rue F. Joliot-Curie
13453 Marseille, France

e-mail: oleg.lepski@univ-amu.fr

Abstract: In the framework of an abstract statistical model we discuss how to use the solu-
tion of one estimation problem (Problem A) in order to construct an estimator in another, com-
pletely diﬀerent, Problem B. As a solution of Problem A we understand a data-driven selection

from a given family of estimators A(H) = (cid:8) bAh, h ∈ H(cid:9) and establishing for the selected esti-
mator so-called oracle inequality. If ˆh ∈ H is the selected parameter and B(H) = (cid:8) bBh, h ∈ H(cid:9)
is an estimator’s collection built in Problem B we suggest to use the estimator bBˆh. We present
very general selection rule led to selector ˆh and ﬁnd conditions under which the estimator bBˆh
is reasonable. Our approach is illustrated by several examples related to adaptive estimation.

AMS 2000 subject classiﬁcations: Primary 60E15; secondary 62G07, 62G08.
Keywords and phrases: adaptive estimation, density model, oracle approach, generalized
deconvolution model, upper fucntion.

1. Introduction

Let (X (n), T(n), P
with semi-metrics ℓ and ρ respectively.

(n)
f

, f ∈ F) be the statistical experiment generated by the observation X (n).

Let A : F → S1 and B : F → S2 be two mappings to be estimated and S1, S2 be sets endowed
For any X (n)-measurable Sj-valued map eQj, j = 1, 2, and any q ≥ 1 introduce
f hρ(cid:0)eQ2, B(f )(cid:1)iq

f hℓ(cid:0)eQ1, A(f )(cid:1)iq

A(cid:2)eQ1, f(cid:3) = E
Rq

B(cid:2)eQ2, f(cid:3) = E

; Rq

f ∈ F.

(n)

(n)

,

(n)
f

Here and later E
supposed to be ﬁxed.

(n)
f

denotes the mathematical expectation w.r.t to the P

and the number q is

The main objective of the present paper can be described as follows. Assume that the problem
of estimation of A(·), called furthermore Problem A, is much easier than the estimation of B(·)
(Problem B). We will not precise here the exact meaning of ”easier”, which may be the theoreti-
cal diﬃculty or computational complexity or something else. One can imagine, for instance, that
Problem A has been already solved while Problem B is still not. It is also important to realize
that Problems A and B may have completely diﬀerent natures. For example, A(f ) is f , where
f : Rd → R is the multivariate probability density and ℓ = k · kp is Lp-norm on Rd, while B(f ) is
functional, i.e. S2 = R. It can be the value of f or its derivatives at a given point, some norm of
this function, the entropy functional or Fisher information and so on. Even if both problems have
the same nature, for instance A(f ) = B(f ) = f , the loss functions (semi-metrics ℓ and ρ) can be
diﬀerent. In particular one can consider ℓ(·) = k · kp and ρ(·) = k · ks, p 6= s.

∗This work has been carried out in the framework of the Labex Archim`ede (ANR-11-LABX-0033) and of the
A*MIDEX project (ANR-11-IDEX-0001-02), funded by the ”Investissements d’Avenir” French Government program
managed by the French National Research Agency (ANR).

imsart-generic ver. 2011/11/15 file: ideas-submitted.tex date: March 15, 2016

1

RA(cid:2)bAˆh, f(cid:3) ≤ inf

h∈HA(n)

ℓ

(f, h) + crn,

∀f ∈ F, ∀n ≥ 1.

(1.1)

Here c > 0 is a numerical constant independent on n and f and rn → 0, n → ∞ is given sequence.
As to the quantity A(n)
(·,·) it is explicitly expressed and for some particular problems one can

ℓ

prove the inequality (1.1) with

The problem which we address consists in ﬁnding hypotheses under which some elements of the
solution of Problem A could be used in the construction of an estimation procedure for solving
Problem B. Let us discuss this approach more in detail.

The variety of statistical procedures developed last quarter of century deal with the data-
driven selection from the particular family of estimators, Barron et al. (1999), Baraud et al. (2014),
Birg´e and Massart (2001), Bunea et al. (2007), Cai (1999), Cavalier and Tsybakov (2001), Cavalier and Golubev
(2006), Dalalyan and Tsybakov (2008), Devroye and Lugosi (1997), Goldenshluger (2009), Goldenshluger and Lepski
(2009), Goldenshluger and Lepski (2011b), Goldenshluger and Lepski (2012), Kerkyacharian et al.
(2001), Lepski and Levit (1998), Nemirovski (2000), Rigollet and Tsybakov (2007), Tsybakov (2003),
Wegkamp (2003) among many others. A very detailed overview on this topic can be found in the
recent paper Lepski (2015).

but we will omit this dependence in the notations). The quality of estimation is measured by the

Suppose that we are given by the collection of estimators A(H) = (cid:8)bAh, h ∈ H(cid:9), used for the
estimation of the map A(·), parameterized by some parameter set H (bAh depend usually on n
family of risks (cid:8)RA(cid:2)bAh, f(cid:3), h ∈ H, f ∈ F(cid:9). Let us say that Problem A is solved if one can ﬁnd
X (n)-measurable element ˆh ∈ H (data-driven selector) such that the selected estimator bAˆh satisﬁes

so-called oracle-type inequality:

A(n)

ℓ

(f, h) = CRA(cid:2)bAh, f(cid:3),

where C is as previously a universal constant.

Let B(H) =(cid:8)bBh, h ∈ H(cid:9) be a given collection of statistical procedures related to the estimation

of B(·). It is extremely important for us that both collections A(H) and B(H) are parameterized by
one and the same set H. As it was mentioned above Problem A and Problem B may have diﬀerent
nature and often only the set H relates both problems.

The questions which we want to answer are: under which conditions the selector ˆh provides a
reasonable choice from the collection B(H)? Is it possible to establish an oracle inequality similar

to (1.1) for the selected estimator bBˆh?

We do not think that the answers on aforementioned questions can be obtained when an arbitrary
selection rule led to ˆh is considered. So, in the next section in the framework of an abstract statistical
model we propose a quite general selection scheme and establish for it the oracle inequality (1.1).
This part of the paper has an independent interest since all results will be obtained under few very
general assumptions which can be veriﬁed for many statistical models and problems. The proposed
approach can be viewed as a generalization of several estimation procedures developed by the author
and his collaborators during last twenty years, see Lepski and Levit (1998), Kerkyacharian et al.
(2001), Iouditski et al. (2009), Goldenshluger and Lepski (2008), Goldenshluger and Lepski (2009),
Goldenshluger and Lepski (2011b), Goldenshluger and Lepski (2012), Lepski (2013), Goldenshluger and Lepski
(2014), and Lepski (2015).

Coming back to the study of the behavior of the ”plug-in” estimator bBˆh we would like to empha-
will assume that X (n) = (cid:0)X (n)

size that it will be done under the following assumption imposed on the statistical experiment. We
are independent random elements. This

2 (cid:1), where X (n)

, X (n)

, X (n)

1
2

1

2

fundamental assumption may correspond to splitting data on two independent samples or to the
situation when a statistician disposes an independent copy of the considered statistical model. Our
selection rule (led in particular to the solution of Problem A) is based on the observation X (n)
1 while
the estimator’s collection B(H) is built from the observation X (n)
2
(n)
1,f and P

.
2,f denote marginal laws of X (n)
(n)
1
(n)
i,f .
We ﬁnish this introduction presenting several examples in which the discussed above strategy

(n)
i,f , i = 1, 2, will be used for the mathematical expectation w.r.t. P

The following notations will be used in the sequel: P

respectively and E

and X (n)

2

can be applied. These considerations will be continued in Sections 4 and 5.

Estimation in the deconvolution density model under Lp-loss. Consider the following
observation scheme:

Zi = Xi + Yi,

i = 1, . . . , n,

(1.2)

where Xi, i = 1, . . . , n, are i.i.d. d-dimensional random vectors with common density f to be
estimated. The noise variables Yi, i = 1, . . . , n, are i.i.d. d-dimensional random vectors with known
common density g. The sequences {Xi, i = 1, . . . , n} and {Yi, i = 1, . . . , n} are supposed to be
mutually independent.
Let X (n) = (Z1, . . . , Zn) and let B(f ) = f and ρ(·) = ℓ(·) = k · kp that means that we are
Let K : Rd → R be the function belonging to L1(cid:0)Rd(cid:1) and RR K = 1. Let Hd be the diadic grid
of (0, 1]d and deﬁne for any ~h = (h1, . . . , hd) ∈ Hd

interested in estimation of the underlying density f under Lp-loss.

~h

K~h(t) = V −1

K(cid:0)t1/h1, . . . , td/hd(cid:1), t ∈ Rd, V~h =
For any ~h ∈ Hd let M(cid:0)·,~h(cid:1) : Rd → R satisfy the operator equation
y ∈ Rd.

K~h(y) =ZRd

g(t − y)M(cid:0)t,~h(cid:1)dt,

Introduce the following estimator’s collection

dYj=1

hj.

B(cid:0)Hd(cid:1) =(cid:26)bB~h(·) = n−1

nXi=1

M(cid:0)Zi − ·,~h(cid:1), ~h ∈ Hd(cid:27).

(1.3)

(1.4)

(1.5)

In Comte and Lacour (2013) and in Rebelles (2016) the data-driven selection rules from B(cid:0)Hd(cid:1),

based on the methodology developed in Goldenshluger and Lepski (2012), were proposed. The
authors established oracle inequalities of type (1.1) and deduced from them several results related
to adaptive estimation.

Our idea is quite diﬀerent and consists in the following. Consider ﬁrst the estimation of A(f ) =
g ⋆ f , where here and later ”⋆” denotes the convolution operator. Note that g ⋆ f is the density of Z1
and, therefore, can be easily estimated from the observation X (n) using standard kernel estimator.
Consider the collection

A(cid:0)Hd(cid:1) =(cid:26)bA~h(·) =

nXi=1

3

K~h(Zi − ·), ~h ∈ Hd(cid:27).

(1.6)

and Goldenshluger and Lepski (2014) where several oracle inequalities were proved. Let ~hn be a

The problem of bandwidth selection from A(cid:0)Hd(cid:1) was studied in Goldenshluger and Lepski (2011b)
selected bandwidth. We propose then to use bB~hn

We remark that similar strategy in the linear regression model (selection from the family of spec-
tral cut-oﬀ estimators, d = 1) was adopted in Chernousova and Golubev (2014). In the sequence
space gaussian white noise model the approach discussed above was applied in Knapik and Solomond
(2015) in order to ﬁnd the posterior contraction rate in inverse problems in the context of the
bayesian nonparametrics.

(·) as the ﬁnal estimator.

Estimation of derivatives in the density model under Lp-loss. Let X (n) = (X1, . . . , Xn)
be i.i.d. random variables with unknown common density f (the multidimensional version of the
problem will be presented in Section 5). Let we are interested in estimating of f (m), m ∈ N∗, where
f (m) denotes m-th derivative of f .
Thus, Bm(f ) = f (m) and let ρ(·) = ℓ(·) = k · kp. Set A(f ) = f and consider the family of kernel

estimators

A(cid:0)H1(cid:1) =(cid:26)bAh(·) =

nXi=1

Kh(Xi − ·), h ∈ H1(cid:27).

h (·) is usually used for estimating

Bm(f ). Hence, one of the possibilities consists in the selection from the collection

It is worth noting that for any m ∈ N∗ the estimator bBh,m(·) = bA(m)
Bm(cid:0)H1(cid:1) =(cid:8)bBh,m(·), h ∈ H1(cid:9)

in order to construct an estimator for f (m). Note, however that in this case the corresponding
selector as well as the selected estimator will depend on m. In particular, the selection schemes are
diﬀerent for diﬀerent values of m.

Our approach to considered problem consists in selecting from the family A(cid:0)H1(cid:1) that provides us
with the selector hn. Then for any m ∈ N∗, we suggest to use bBhn,m(·) as an estimator for Bm(f ).

In other words the problem we address can be formulated as follows. Is it possible to diﬀerentiate
the kernel estimator with data-driven bandwidth in order to get the estimator for any derivative
of the underlying density simultaneously?

In the framework of gaussian white noise model, this problem was studied in Efroimovich (1998).
It was shown that the answer is positive when p = 2 and the adaptation is considered over the
collection of Sobolev classes. More precisely it was shown that the diﬀerentiation of the Efromovich-
Pinsker estimator leads to the eﬃcient adaptive estimator of any derivative.

In Section 5 we prove that the answer on aforementioned question is positive when the adaptive
estimation of partial derivatives of a function belonging to an anisotropic Nikol’skii class in Lp(Rd)
is considered under an arbitrary Lp-loss and in an arbitrary dimension d ≥ 1.
Acknowledgement. The author is grateful to A. Goldenshluger and Yu. Golubev for fruitful
discussions.

2. Selection scheme for solving of Problem A.

Let Hn, n ∈ N∗, be a sequence of countable subsets of H. Let {bAh, h ∈ H} and {bAh,η, h, η ∈ H} be

-measurable S1-valued mappings possessing the properties formulated below.

the families of X (n)

1

4

Both bAh and bAh,η depend usually on n but we will omit this dependence for the sake of simplicity
of notations. Let εn → 0, n → ∞ be a given sequence.
Suppose there exist collections of S1-valued functionals {Λh(f ), h ∈ H}, {Λh,η(f ), h, η ∈ H}
and a collection of positive X (n)
-measurable random variables Ψn = {Ψn(h), h ∈ H} for which the
following conditions hold. (The functionals Λh and Λh,η may depend on n (not necessarily) but we
will omit this dependence in the notations.)

1

Apermute.

bAh,η ≡ bAη,h, for any h, η ∈ H.

Aupper. For any n ≥ 1
sup
f ∈F

(i)

E

(n)

1,f(cid:18) sup
h∈Hnhℓ(cid:0)bAh, Λh(f )(cid:1) − Ψn(h)iq
1,f(cid:18) sup
h,η∈Hnhℓ(cid:0)bAh,η, Λh,η(f )(cid:1) −(cid:8)Ψn(h) ∧ Ψn(η)(cid:9)iq

+(cid:19) ≤ εq

n;

(n)

+(cid:19) ≤ εq

n.

(ii)

sup
f ∈F

E

Remark 1. Often the collection {Ψn(h), h ∈ H} satisfying the hypothesis Aupper is not random.
This is typically the case when a statistical problem is studied in the framework of white gaussian
noise or regression model. See also Lemma 1 below, case p ≤ 2.

2.1. Discussion.

(n)

(n)

For many statistical models and problems Λh(f ) = E

1,f(cid:0)bAh(cid:1) and Λh,η(f ) = E

1,f(cid:0)bAh,η(cid:1). In this
case ℓ(cid:0)bAh, Λh(f )(cid:1) and ℓ(cid:0)bAh,η, Λh,η(f )(cid:1) can be viewed as stochastic errors related to the estimators
bAh and bAh,η respectively. Hence, following the terminology used in Lepski (2016) we can say that
{Ψn(h), h ∈ H} and {Ψn(h) ∧ Ψn(η), h, η ∈ H} are upper functions of level εn for the collection of
corresponding stochastic errors.
The development of the probabilistic tools allowing to control the behavior of stochastic er-
rors related to statistical procedures is the subject of vast literature, see for instance the books
van der Vaart and Wellner (1996), van de Geer (2000), Massart (2007). The inequalities similar to
those appeared in the hypothesis Aupper can be found in Egishyants and Ostrovskii (1996) and
Lepski (2013a,b,c). The upper functions for Lp-norm of ”kernel-type” empirical and gaussian pro-
cesses were obtained in Goldenshluger and Lepski (2011a) and Lepski (2016).

We conclude that from the one hand the veriﬁcation of the hypothesis Aupper is in some sense
necessary task when the oracle approach or adaptive estimation is considered. On the other hand
the very developed probabilistic machinery is in our disposal.

Let us also remark that if the analogues of hypotheses Aupper and Apermute can be checked in

Problem B, then the selection rule from the estimator’s collection {bBh, h ∈ H} presented below will

provide the solution of Problem B. However in some cases the veriﬁcation of these hypotheses is
much more diﬃcult in Problem B than in Problem A and it is one of the reasons why we propose
to proceed diﬀerently.

Let us now discuss some examples of estimator’s collections for which the hypothesis Apermute is

fulﬁlled.

Example 1. Let A(cid:0)Hd(cid:1) and B(cid:0)Hd(cid:1) be the families deﬁned in (1.5) and (1.6). For any ~h,~h′ ∈ Hd
set ~h ∨ ~h′ = (h1 ∨ h′

1, . . . , hd ∨ h′

d) and introduce

bA~h,~h′(·) = bA~h∨~h′(·),

bB~h,~h′(·) = bB~h∨~h′(·).

5

It is obvious that the hypothesis Apermute is veriﬁed for these estimators. The selection rules based
on this construction of families of auxiliary estimators can be found in Kerkyacharian et al. (2001),
Kerkyacharian et al. (2008), Rebelles (2016).

Example 2. Consider either the density model generated by the observation X (n) = (X1, . . . , Xn),
where Xi ∈ Rd, i = 1, . . . n, are i.i.d. random vectors or the observation model (1.2).
Let K~h(·) and M(cid:0)·,~h(cid:1),~h ∈ Hd, be deﬁned in (1.3) and (1.4) respectively. For any ~h,~h′ ∈ Hd set

K~h,~h′(·) =ZRd

K~h(· − t)K~h′(t)dt, M(cid:0)·,~h,~h′(cid:1) =ZRd

M(cid:0) · −t,~h(cid:1)M(cid:0)t,~h′(cid:1)dt

and deﬁne

nXi=1

M(cid:0)Zi − ·,~h,~h′(cid:1).
bA~h,~h′(·) =
Since, K~h,~h′ ≡ K~h′,~h and M(cid:0)~h,~h′(cid:1) ≡ M(cid:0)~h′,~h(cid:1) the hypothesis Apermute holds. The selection rules

K~h,~h′(Xi − ·) or bA~h,~h′(·) =

bB~h,~h′(·) =

K~h,~h′(Zi − ·),

based on this construction of auxiliary estimators were proposed in Lepski and Levit (1998), Goldenshluger and Lepski
(2008), Goldenshluger and Lepski (2009), Goldenshluger and Lepski (2011b), Comte and Lacour
(2013), Reynaud–Bouret et al. (2014), Goldenshluger and Lepski (2014).

nXi=1

nXi=1

Example 3. Let D be a set endowed with the Borel measure µ and let(cid:8)ψk, k ∈ Nd(cid:9) be an orthogonal
basis in L2(cid:0)D, µ(cid:1) possessing the following properties.

ψ0 ≡ c 6= 0,

ψk(t)µ(dt) = 0, ∀k 6= 0 = (0, . . . 0).

ZD

Let T = {τ : τ = (cid:0)τk, k ∈ Nd(cid:1)} be a given subset of l2 such that τ0c2µ(D) = 1 for all τ ∈ T.

Introduce

Kτ (t, x) = Xk∈Nd

τkψk(t)ψk(x), τ ∈ T,

and deﬁne for any τ, τ ′ ∈ T

Kτ,τ ′(t, x) =ZD

Kτ (t, y)Kτ (y, x)µ(dy).

Consider the statistical experiment generated by the observation X (n) = (X1, . . . , Xn), where Xi, i =
1, . . . n, are i.i.d. D-valued random variables having unknown density f with respect to the measure
µ. Introduce the following collection of estimators

nXi=1

bAτ,τ ′(·) =

Kτ,τ ′(Xi,·),

τ, τ ′ ∈ T.

As it was shown in Goldenshluger and Lepski (2012), Kτ,τ ′ ≡ Kτ ′,τ for any τ, τ ′ ∈ T and, therefore,
the hypothesis Apermute is fulﬁlled.

We remark that all estimator’s construction discussed above can be applied in other statistical
models where kernel-type estimators are used, e.g. white gaussian noise model and the regression
model.

6

2.2. (Ψn, ℓ)-selection rule and corresponding oracle inequality.

satisfying hypotheses Apermute and Aupper, and establish for it the oracle inequality (1.1).

Our objective is to propose the selection rule from an arbitrary collection A(H) = {bAh, h ∈ H}
Deﬁne for any h ∈ Hn

Let ˆh(n) ∈ Hn be an arbitrary X (n)

η∈Hnhℓ(cid:0)bAh,η, bAη(cid:1) − 2Ψn(η)i+

bRn(h) = sup
1 − measurable random element satisfying
n,

h∈Hn(cid:8)bRn(h) + 2Ψn(h)(cid:9) + ε′

bRn(cid:0)ˆh(n)(cid:1) + 2Ψn(cid:0)ˆh(n)(cid:1) ≤ inf

.

where ε′

n is an arbitrary sequence and we will assume that ε′

n ≤ εn.
Introduce the following notation: for any f ∈ F, h ∈ Hn and n ≥ 1

A (f, h) = ℓ(cid:0)Λh(f ), A(f )(cid:1) + 2 sup
B(n)

1,f(cid:8)Ψq
Theorem 1. Let Apermute and Aupper be fulﬁlled. Then, for any f ∈ F and n ≥ 1

ℓ(cid:0)Λh,η(f ), Λη(f )(cid:1), ψn(f, h) =hE

η∈Hn

(n)

n(h)(cid:9)i 1

q .

RA(cid:2)bAˆh(n), f(cid:3) ≤ inf

h∈HnnB(n)

A (f, h) + 5ψn(f, h)o + 6εn.

Note that the proposed (Ψn, ℓ) selection rule is built using only observation part X (n)

. This
means that the application of the result presented in Theorem 1 does not require any splitting of
data since one can formally suppose that X (n)

is the original data set.

1

1

2.3. Proof of Theorem 1.

For the simplicity of notations throughout the proof we will write ˆh instead of ˆh(n).

Fix h ∈ Hn. We have obviously in view of the deﬁnition of bRn(·)

ℓ(cid:0)bAˆh, bAh,ˆh(cid:1) ≤ 2Ψn(ˆh) +hℓ(cid:0)bAˆh, bAh,ˆh(cid:1) − 2Ψn(ˆh)i+

≤ 2Ψn(ˆh) + bRn(h).

Here we have also used that ˆh ∈ Hn by its deﬁnition. Taking into account Apermute we get

ℓ(cid:0)bAh, bAh,ˆh(cid:1) = ℓ(cid:0)bAh, bAˆh,h(cid:1) ≤ 2Ψn(h) +hℓ(cid:0)bAh, bAˆh,h(cid:1) − 2Ψn(h)i+

We get from (2.1), (2.2) and the deﬁnition of ˆh

≤ 2Ψn(h) + bRn(ˆh).

We have in view of the triangle inequality for any h ∈ Hn

ℓ(cid:0)bAˆh, bAh,ˆh(cid:1) + ℓ(cid:0)bAh, bAh,ˆh(cid:1) ≤ bRn(ˆh) + 2Ψn(ˆh) + bRn(h) + 2Ψn(h)

n.

≤ 2(cid:8)bRn(h) + 2Ψn(h)(cid:9) + ε′
ℓ(cid:0)Λh,η(f ), Λη(f )(cid:1) + ξ1 + ξ2,

7

bRn(h) ≤ sup

η∈Hn

(2.1)

(2.2)

(2.3)

(2.4)

where we have put

Thus, we obtain from (2.3) and (2.4) for any h ∈ Hn

ξ1 = sup

,

η∈Hnhℓ(cid:0)bAη, Λη(cid:1) − Ψn(η)i+
ℓ(cid:0)bAˆh, bAh,ˆh(cid:1) + ℓ(cid:0)bAh, bAh,ˆh(cid:1) ≤ 2 sup

η∈Hn

ξ2 = sup

h,η∈Hnhℓ(cid:0)bAh,η, Λh,η(cid:1) −(cid:8)Ψn(h) ∧ Ψn(η)(cid:9)i+
ℓ(cid:0)Λh,η(f ), Λη(f )(cid:1) + 4Ψn(h) + 2ξ1 + 2ξ2 + ε′

n.

.

(2.5)

(2.6)

Obviously for any h ∈ Hn

It yields together with (2.6) by the triangle inequality

ℓ(cid:0)bAh, A(f )(cid:1) ≤ ℓ(cid:0)Λh(f ), A(f )(cid:1) + Ψn(h) + ξ1.

ℓ(cid:0)bAˆh, A(f )(cid:1) ≤ B(n)

A (f, h) + 5Ψn(h) + 3ξ1 + 2ξ2 + ε′
n,

∀h ∈ Hn, ∀f ∈ F.

(2.7)

Taking into account the hypothesis Aupper we get for any h ∈ Hn and any f ∈ F

(cid:26)E

(n)

1,fhℓ(cid:0)bAˆh, A(f )(cid:1)iq(cid:27) 1

q

≤ B(n)

A (f, h) + 5ψn(f, h) + 6εn.

Here we have used that ε′
independent of h we come to the assertion of the theorem.

n ≤ εn. Noting that the left hand side of the obtained inequality is

3. ”Plug-in” estimator for solving of Problem B and its properties.

2

(Ψn, ℓ)-selection rule.

-measurable S2-valued mappings. The goal of this

Let B(H) = {bBh, h ∈ H} be the family of X (n)
section is to bound from above the risk of ”plug-in” estimator bBˆh(n), where ˆh(n) is obtained by
We add the following assumption to the hypothesis Aupper.
Aupper. There exist the constant CΨ ≥ 1 such that for any n ≥ 1
1,f(cid:18) sup

h∈HnhΨn(h) − CΨψn(f, h)iq

h∈Hnhψn(f, h) − CΨΨn(h)iq

1,f(cid:18) sup

+(cid:19) ≤ εq

+(cid:19) ≤ εq

sup
f ∈F

sup
f ∈F

n,

n.

(n)

E

(n)

E

We note that the hypothesis Aupper is fulﬁlled with CΨ = 1 is Ψn(h) is non-random and in this case
ψn(·,·) is independent of f . Actually the hypothesis Aupper guarantees that the random function
Ψn(·) is well-concentrated around some non-random mapping. We would like to stress that for
all known to the author problems in order to check Aupper one ﬁrst veriﬁes that the required
inequalities hold for the ”non-random” mapping ψn(f, h). However it may depend on unknown f
and, therefore, cannot be used in the estimation construction. In this case this quantity is replaced
by its empirical counterpart Ψn(h) satisfying the hypothesis Aupper.

Let {Υh, h ∈ H} be a collection of S2-valued functionals. Set for any h ∈ H, f ∈ F and n ≥ 1

En(f, h) =(cid:16)E

(n)

2,f(cid:8)ρq(cid:0)bBh, Υh(f )(cid:1)(cid:9)(cid:17) 1

8

q .

(3.1)

Introduce the following set of parameters.

A (f, η) + 2ψn(f, η)(cid:3)o;
A (f, η) + 2ψn(f, η)(cid:3)o.
η∈Hn(cid:2)B(n)
Set δn = inf f ∈F inf h∈Hn ψ(f, h) and deﬁne for any f ∈ F and n ≥ 1

Vn(f ) = nh ∈ Hn : ψn(f, h) < 2C 2
η∈Hn(cid:2)B(n)
Un(f ) = nh ∈ Hn : ℓ(cid:0)Λh, A(f )(cid:1) < 4CΨ inf
h∈Vn(f ) En(f, h) +(cid:0)5εn/δn) sup

h∈Hn En(f, h);

τn(f ) =

Ψ inf

sup

νn(f ) =

sup

h∈Un(f )

ρ(cid:0)Υh, B(f )(cid:1) +(cid:0)30εn/δn) sup
n ≤ δn/4.

h∈Hn

ρ(cid:0)Υh, B(f )(cid:1).

Let ˆh(n) is obtained by (Ψn, ℓ)-selection rule with ε′

Theorem 2. Let Apermute, Aupper and Aupper hold. Then, for any f ∈ F and n ≥ 1

We will see that in particular examples the sequence εn decreases to zero very rapidly. It allows

RB(cid:2)bBˆh(n), f(cid:3) ≤ νn(f ) + τn(f ).

often to assert that

h∈Hn En(f, h) + 6 sup
very fast and the statement of the theorem is reduced to

h∈Hn

f ∈F(cid:18) sup
κn :=(cid:0)5εn/δn) sup
RB(cid:2)bBˆh(n), f(cid:3) ≤ sup

h∈Un(f )

ρ(cid:0)Υh, B(f )(cid:1)(cid:19) → 0, n → ∞

ρ(cid:0)Υh, B(f )(cid:1) + sup

h∈Vn(f ) En(f, h) + κn.

(3.2)

Remark 2. Note that the sets Vn(f ) and Un(f ) are completely determined by the quantities ap-

peared in the solution of Problem A while En(f, h) and ρ(cid:0)Υh, B(f )(cid:1) represent respectively the stan-
dard deviation of stochastic error and approximation error of the estimator bBh used in Problem B.

The functionals τn(f ) and νn(f ) explain how all these quantities are related.

Although, (3.2) is simpler to analyze than the assertion of the theorem, even this oracle inequal-
ity is less natural than those proved in Theorem 1. Next result allows to understand better the

properties of the estimator bBˆh(n).
Proposition 1. Let Apermute and Aupper hold. Suppose additionally that there exists a constant
Cℓ > 0 such that for any f ∈ F, n ≥ 1 and h ∈ Hn

Assume also that there exists a constant CE > 0 such that for any n ≥ 1 and h ∈ Hn

ρ(cid:0)Υh(f ), B(f )(cid:1) ≤ Cℓ ℓ(cid:0)Λh(f ), A(f )(cid:1).

sup
f ∈F En(f, h) ≤ CE Ψn(h).

(3.3)

(3.4)

Then, for any f ∈ F and n ≥ 1

RB(cid:2)bBˆh(n), f(cid:3) ≤ inf

h∈Hnn(2Cℓ + CE )B(n)

A (f, h) + (7Cℓ + 2CE )ψn(f, h)o + (10Cℓ + 3CE )εn.

We remark that the assertions of Theorem 1 and of Proposition 1 diﬀer only by the numerical
constant. We would like to emphasize that although Proposition 1 holds under quite restrictive
assumption (3.3) it is useful for some problems studied in Section 4. Moreover, it does not require
the hypothesis Aupper.

9

To get the last inequality we have used that ˆh ∈ Hn. Next, we get from the deﬁnition of ˆh and (2.4)

ℓ(cid:0)Λˆh, A(f )(cid:1) ≤ ℓ(cid:0)bAˆh, A(f )(cid:1) + ℓ(cid:0)bAˆh, Λˆh(cid:1)
≤ ℓ(cid:0)bAˆh, A(f )(cid:1) + Ψn(cid:0)ˆh(cid:1) + sup
= ℓ(cid:0)bAˆh, A(f )(cid:1) + Ψn(cid:0)ˆh(cid:1) + ξ1.

η∈Hnhℓ(cid:0)bAη, Λη(cid:1) − Ψn(η)i

Ψn(cid:0)ˆh(cid:1) ≤ bRn(ˆh) + 2Ψn(ˆh) ≤ bRn(h) + 2Ψn(h) + εn

ℓ(cid:0)Λh,η(f ), Λη(f )(cid:1) + 2Ψn(h) + ξ1 + ξ2 + ε′

η∈Hn

n

A (f, h) + 2Ψn(h) + ξ1 + ξ2 + ε′
n.

≤ sup
≤ B(n)

(3.5)

(3.6)

(3.7)

(3.8)

3.1. Proof of Theorem 2.

For the simplicity of notations throughout the proof we will write ˆh instead of ˆh(n) and we break
the proof on several steps.

10. Let us prove that for any f ∈ F and n ≥ 1

P

(n)

1,fnˆh /∈ Un(f )o ≤ (30εn/δn)q.

First note that in view of the triangle inequality

Set ξ3 = suph∈Hn(cid:2)Ψn(h) − CΨψ(f, h)(cid:3)+. We deduce from (2.7), (3.6) and (3.7) that

ℓ(cid:0)Λˆh, A(f )(cid:1) ≤ inf
≤ inf

h∈H(cid:2)2B(n)
h∈H(cid:2)2B(n)

A (f, h) + 7Ψn(h)(cid:3) + 5ξ1 + 3ξ2 + 2ε′
A (f, h) + 7CΨψ(f, h)(cid:3) + 5ξ1 + 3ξ2 + 7ξ3 + 2ε′

n

n.

Hence for any f ∈ F and n ≥ 1

nˆh /∈ Un(f )o ⊆(cid:8)5ξ1 + 3ξ2 + 7ξ3 + 2ε′

To get the last inclusion we have used that ε′
Markov inequality and the hypotheses Aupper and Aupper.

n ≥ δn(cid:9) ⊆(cid:8)10ξ1 + 6ξ2 + 14ξ3 ≥ δn(cid:9).
n ≤ δn/4. The statement (3.5) follows now from the

20. For any f ∈ F and n ≥ 1 the following is true:

P(n)

1,fnˆh /∈ Vn(f )o ≤ (5εn/δn)q.

Indeed, in view of (3.7)

(3.9)

Ψn(cid:0)ˆh(cid:1) ≤ 2 inf

h∈H(cid:2)B(n)

A (f, h) + CΨψ(f, h)(cid:3) + ξ1 + ξ2 + 2ξ3 + ε′

n.

Hence, putting ξ4 = suph∈Hn(cid:2)ψ(f, h) − CΨΨn(h)(cid:3)+ we obviously have
ψ(f, ˆh) ≤ CΨΨn(cid:0)ˆh(cid:1) + ξ4 ≤ 2CΨ inf

A (f, h) + CΨψ(f, h)(cid:3) + CΨ(cid:2)ξ1 + ξ2 + 2ξ3 + ε′

h∈H(cid:2)B(n)

n(cid:3) + ξ4.

10

Taking into account that CΨ ≥ 1 and ε′
nˆh /∈ Vn(f )o = nψ(f, ˆh) ≥ 2C 2

n ≤ δn/4 we have
Ψ inf

η∈Hn(cid:2)B(n)
⊆ nξ1 + ξ2 + 2ξ3 + +ξ4 + ε′

A (f, η) + 2ψn(f, η)(cid:3)o
n ≥ 2δno ⊆nξ1 + ξ2 + 2ξ3 + ξ4 ≥ δno.

The statement (3.9) follows now from the Markov inequality hypotheses Aupper and Aupper.

30. Since ˆh is X (n)

1

we get

2

-measurable, {bBh, h ∈ H} is X (n)
q =(cid:16)E
(cid:16)E

f (cid:8)ρq(cid:0)bBˆh, Υˆh(f )(cid:1)(cid:9)(cid:17) 1

(n)

Next, we obviously have

-measurable and X (n)

1

(n)

1,f(cid:8)E q

n(cid:0)f, ˆh(cid:1)(cid:9)(cid:17) 1

q

, X (n)

2

are independent

(3.10)

En(cid:0)f, ˆh(cid:1) = En(cid:0)f, ˆh(cid:1)1ˆh∈Vn(f ) + En(cid:0)f, ˆh(cid:1)1ˆh /∈Vn(f ) ≤ sup

h∈Vn(f ) En(cid:0)f, h(cid:1) + sup

h∈Hn En(cid:0)f, h(cid:1)1ˆh /∈Vn(f ).

Hence, we get from (3.9)

(cid:16)E

(n)

1,f(cid:8)E q

n(cid:0)f, ˆh(cid:1)(cid:9)(cid:17) 1

q ≤ sup

h∈Vn(f ) En(cid:0)f, h(cid:1) + sup

h∈Hn En(cid:0)f, h(cid:1)(cid:0)5εn/δn) = τn(f ).

40. We have

ρ(cid:0)Υˆh(f ), B(f )(cid:1) = ρ(cid:0)Υˆh(f ), B(f )(cid:1)1ˆh∈U(f ) + ρ(cid:0)Υˆh(f ), B(f )(cid:1)1ˆh /∈U(f )

≤ sup

h∈U(f )

ρ(cid:0)Υh(f ), B(f )(cid:1) + sup

h∈Hn

ρ(cid:0)Υh(f ), B(f )(cid:1)1ˆh /∈U(f ).

Hence, in view of (3.5) we obtain

(cid:16)E

(n)

f (cid:8)ρq(cid:0)Υˆh(f ), B(f )(cid:1)(cid:9)(cid:17) 1

q ≤ νn(f ).

It remains to note that in view of triangle inequality

RB(cid:2)bBˆh(n), f(cid:3) ≤(cid:16)E(n)

f (cid:8)ρq(cid:0)Υˆh(f ), B(f )(cid:1)(cid:9)(cid:17) 1

q +(cid:16)E(n)

f (cid:8)ρq(cid:0)bBˆh, Υˆh(f )(cid:1)(cid:9)(cid:17) 1

q

and the assertion of the theorem follows from (3.11) and (3.12).

3.2. Proof of Proposition 1.

We have in view of the assumption (3.3) for any f ∈ F

ρ(cid:0)bBˆh, B(f )(cid:1) ≤ ρ(cid:0)Υˆh(f ), B(f )(cid:1) + ρ(cid:0)bBˆh, Υˆh(f )(cid:1)
≤ Cℓ ℓ(cid:0)Λˆh(f ), A(f )(cid:1) + ρ(cid:0)bBˆh, Υˆh(f )(cid:1)

11

(3.11)

(3.12)

(3.13)

Since by construction ˆh is X (n)

1

-measurable and X (n)

1

and X (n)

2

are independent

(n)

(n)

q .

(cid:16)E

q =(cid:16)E

f (cid:8)ℓq(cid:0)Λˆh(f ), A(f )(cid:1)(cid:9)(cid:17) 1

We deduce from (3.8) and the hypothesis Aupper for any f ∈ F

1,f(cid:8)ℓq(cid:0)Λˆh(f ), A(f )(cid:1)(cid:9)(cid:17) 1
A (f, h) + 7ψn(f, h)o + 10εn.
For any h ∈ Hn and f ∈ F we have in view of (3.7) and the assumption (3.4),
A (f, h) + 2Ψn(h) + ξ1 + ξ2 + εn(cid:3),

f (cid:8)ℓq(cid:0)Λˆh(f ), A(f )(cid:1)(cid:9)(cid:17) 1

h∈Hnn2B(n)

q ≤ inf

and, therefore, we deduce from the hypothesis Aupper and (3.10)

(cid:16)E

(n)

(n)

En(cid:0)f, ˆh(cid:1) ≤ CE Ψn(cid:0)ˆh(cid:1) ≤ CE(cid:2)B(n)
f (cid:8)ℓq(cid:0)bBˆh, Υˆh(f )(cid:1)(cid:9)(cid:17) 1
(cid:16)E
f (cid:8)ℓq(cid:0)bBˆh, Υˆh(f )(cid:1)(cid:9)(cid:17) 1

q ≤ CE(cid:2)B(n)
h∈HnnB(n)

q ≤ CE inf

(n)

(cid:16)E

A (f, h) + 2ψn(f, h)(cid:3) + 3CE εn.
A (f, h) + 2ψn(f, h)o + 3CE εn.

Since the left hand side of the obtained inequality is independent of h we get

(3.14)

(3.15)

The assertion of the theorem follows now from (3.13), (3.14), (3.15) and the triangle inequality.

3.3. From oracle approach to adaptive estimation.

In this section we discuss how to use the oracle approach in adaptive estimation over the given
scale of functional classes. We present several results concerning adaptation which will be directly
deduced from Theorems 1, 2 and Proposition 1.

Let(cid:8)Fa, a ∈ A(cid:9) be a given collection of subsets of F and suppose that an abstract oracle inequality

(1.1) is established. Deﬁne

Rn(cid:0)Fa(cid:1) = sup

f ∈Fa

inf

h∈HA(n)

ℓ

(f, h) + crn,

a ∈ A.

We immediately deduce from (1.1) that for any a ∈ A
n (cid:0)Fa(cid:1) sup

lim sup

R−1

n→∞

f ∈Fa RA(cid:2)bAˆh, f(cid:3) ≤ 1.

Using modern statistical language we can state that the estimator bAˆh is Rn-adaptive, where Rn =
{Rn(cid:0)Fa(cid:1), a ∈ A} is the family of normalizations. If additionally one can prove that for any a ∈ A

lim inf
n→∞

R−1

n (cid:0)Fa(cid:1) inf

eQ

sup

f ∈Fa RA(cid:2)eQ, f(cid:3) > 0,

where inﬁmum is taken over all X (n)-measurable S1-valued random mappings we can assert that

the estimator bAˆh is optimally adaptive over the scale (cid:8)Fa, a ∈ A(cid:9). The latter means that this

estimator is simultaneously asymptotically minimax on each Fa.

12

Consequences of Theorem 1 and Proposition 1. Set Sn = {sn(Fa), a ∈ A}, where

sn(cid:0)Fa(cid:1) = sup

f ∈Fa

inf

h∈Hn(cid:8)B(n)

A (f, h) + ψn(f, h)(cid:9)

and let ˆh(n) is obtained by (Ψn, ℓ)-selection rule.

The results below follow immediately from Theorem 1 and Proposition 1.

Theorem 3. Let Apermute and Aupper hold and assume that εn = o(cid:0) inf a∈A sn(Fa)(cid:1), n → ∞.
(1) Then, for any a ∈ A

lim sup

n→∞

s−1
n (Fa) sup

f ∈Fa RA(cid:2)bAˆh(n), f(cid:3) ≤ 5.

(2) If additionally (3.3) and (3.4) are fulﬁlled, then for any a ∈ A

lim sup

n→∞

s−1
n (Fa) sup

f ∈Fa RB(cid:2)bBˆh(n), f(cid:3) ≤ 7Cℓ + 2CE .

Thus, the estimators bAˆh(n) and bBˆh(n) are simultaneously Sn-adaptive. Hence, if bAˆh(n) is optimally
adaptive in Problem A one can expect that bBˆh(n) is optimally adaptive in Problem B.
Adaptive analogue of Theorem 2. Deﬁne for any a ∈ A
ψn(f, h)

(3.16)

f ∈Fa

ψn(cid:0)Fa, h(cid:1) = inf
Ψsn(Fa)(cid:9), Un(a, f ) =(cid:8)h ∈ Hn : ℓ(cid:0)Λh, A(f )(cid:1) < 8CΨsn(Fa)(cid:9).

and introduce the following sets of parameters

Vn(a) =(cid:8)h ∈ Hn : ψn(Fa, h) < 4C 2
It is obvious that Vn(f ) ⊆ Vn(a) and Un(f ) ⊆ Un(a, f ) for any f ∈ Fa. Set ﬁnally
ρ(cid:0)Υh, B(f )(cid:1),
ρ(cid:0)Υh, B(f )(cid:1)i

κn(Fa) = (cid:0)5εn/δn) sup

sup
h∈Vn(a) En(f, h) + sup

h∈Hn En(f, h) + 6 sup

f ∈Fah sup

ϕn(Fa) = sup
f ∈Fa

h∈Un(a,f )

sup

h∈Hn

f ∈Fa

and let ˆh(n) is obtained by (Ψn, ℓ)-selection rule.

The following statement is the direct consequence of Theorem 2.

Theorem 4. Let Apermute, Aupper and Aupper hold.

Assume that κn(Fa) = o(cid:0)ϕn(Fa)(cid:1), n → ∞, for any a ∈ A. Then, for any a ∈ A

lim sup

n→∞

sup
f ∈Fa

ϕ−1

n (Fa)RB(cid:2)bBˆh

(n), f(cid:3) ≤ 1.

3.4. Some computations in the density model.

Our objective now is to give an example of statistical model and problem in which we are able
to check the hypotheses Apermute, Aupper and Aupper. We also gives some bounds for the quanti-
ties B(n)
A (f, h) and ψn(f, h) involved in the (Ψn, ℓ)-oracle inequality. It allows us, in particular, to

compute the rate

sn(cid:0)Fa(cid:1) = sup

f ∈Fa

inf

h∈Hn(cid:8)B(n)

A (f, h) + ψn(f, h)(cid:9)

13

appeared in all adaptive results in the case when Fa = Np,d(cid:0)~β, L(cid:1), a = (cid:0)~β, L(cid:1), where Np,d(cid:0)~β, L(cid:1)

anisotropic Nikol’skii class. The results presented in this section form basic tools for the solution
of problem studied in Sections 4 and 5.

p

p

and E(m)

Let T (m) = (T1, . . . , Tm), m ∈ N∗, be i.i.d. random vectors taking values in Rd and having
density p with respect to Lebesgue measure. As before P(m)
denote the probability law
and mathematical expectation of T (m). The goal is to estimate A(p) = p and the quality of an
estimator procedures is measured by Lp-risk, 1 ≤ p < ∞, that is
pi(cid:17)1/p
,
where P is the set of all probability densities and ℓp(g) = kgkp
Remind that Hd = (cid:8)(cid:0)(2−k1, . . . , 2−k1(cid:1)), (k1, . . . , kd) ∈ Nd(cid:9) is the diadic grid in (0, 1]d and
K~h(·),~h ∈ Hd be deﬁned in (1.3). Introduce the collection of standard kernel estimators

RA(cid:2)eAm, p(cid:3) =(cid:16)E

p =RRd |g(x)|pdx.

h(cid:13)(cid:13)eAm − p(cid:13)(cid:13)p

p ∈ P

(m)
p

A(cid:0)Hd

m(cid:1) =(cid:26)bA~h(·) = m−1

mXi=1

m(cid:27),
K~h(Ti − ·), ~h ∈ Hd

m =(cid:8)~h ∈ Hd : ln(m)/m ≤ V~h ≤ e−√ln(m)(cid:9) and V~h =Qd
where Hd
bounded, symmetric function belonging to L1(Rd) and R K = 1.

j=1 hj.

From now on we will assume that the kernel K involved in the deﬁnition of K~h is continuous,

3.4.1. Veriﬁcation of the hypotheses Apermute, Aupper and Aupper.
Recall that ”⋆” denotes the convolution operator and introduce for any ~h, ~η ∈ Hd

(3.17)

(3.18)

bA~h,~η(·) = m−1

mXi=1(cid:2)K~h ⋆ K~η(cid:3)(Ti − ·).

Set also for any ~h ∈ Hd and m ∈ N∗

∆m(cid:0)~h(cid:1) =

ln(p) (cid:26) 1
√m(cid:18)ZRd(cid:20) 1
480pkKk1

128kKk1kKkp(cid:0)mV~h(cid:1)1/p−1,
9kKk1kKk2(cid:0)mV~h(cid:1)−1/2,
(Zi − t)(cid:21)p/2

K 2
~h

m

mXi=1

1 ≤ p < 2;
p = 2;

dt(cid:19)1/p

+ 2kKkp(cid:0)mV~h(cid:1)−1/2(cid:27).

and if p > 2

∆m(cid:0)~h(cid:1) =

We remark that ∆m(·) is not random if p ≤ 2. It is obvious that the hypothesis Apermute is fulﬁlled.
Deﬁne the empirical process

ξm(cid:0)x,~h(cid:1) =

1
m

mXi=1hK~h(Ti − x) − E

(m)

p K~h(Ti − x)i,

x ∈ Rd.

Auxiliary results. The following statements are the consequences of the results established in
Lemmas 1 and 2 in Goldenshluger and Lepski (2011b) and Theorem 3 in Goldenshluger and Lepski
(2011a). Set P(D) = {p ∈ P : kpk∞ ≤ D}, D > 0.
14

Lemma 1. For any 1 ≤ p < ∞ and for all m ∈ N∗

~h∈Hd

E(m)

sup
p∈P

p (cid:18) sup
p (cid:18) sup
where for any p ≥ 1 and D > 0

p∈P(D)

~h∈Hd

sup

(m)

E

mh(cid:13)(cid:13)ξm(cid:0)·,~h(cid:1)(cid:13)(cid:13)p − kKk−1
mh(cid:13)(cid:13)ξm(cid:0)·,~h(cid:1)(cid:13)(cid:13)p − kKk−1

1 ∆m(~h)iq
1 ∆m(~h)iq

+(cid:19) ≤ εq
+(cid:19) ≤ εq

m(p),

m(p),

p ∈ [1, 2);

(3.19)

p ∈ [2,∞),

(3.20)

lim sup
m→∞

maεm(p) = 0,

∀a > 0.

(3.21)

Moreover for any p > 2, there exists D > 0 completely determined by K, p, D and d such that

sup

E

p∈P(D)

(m)

p (cid:18) sup

~h∈Hd

mh∆m(~h) − D(cid:0)mV~h(cid:1)−1/2iq

+(cid:19) ≤ εq

m(p).

Let us remark the following simple consequence of (3.21) and (3.22).

p∈P(D)(cid:16)E

sup

(m)
p ∆q

m(~h)(cid:17)1/q

≤ D(cid:0)mV~h(cid:1)−1/2,

∀~h ∈ Hd

m, ∀m ∈ N∗.

(3.22)

(3.23)

In order to get this inequality it suﬃces to note that (cid:0)mV~h(cid:1)−1/2 ≥ m−1/2 for all ~h ∈ (0, 1]d.
Veriﬁcation of the hypothesis Aupper. Set Λ~h(p,·) = E

(m)

m and note that

p (cid:8)bA~h(·)(cid:9), ~h ∈ Hd

Λ~h(p,·) = E

(m)

p K~h(T1 − ·) =ZRd

K~h(z − ·)p(z)dz.

(3.24)

In view of the assertions (3.19) and(3.20) of Lemma 1 we can conclude that the hypothesis Aupper(i)
is fulﬁlled for any p ≥ 1. In order to verify Aupper(ii) note ﬁrst the following obvious identities.

Hence, putting Λ~h,~η(p,·) = E
Λ~h(p,·) we deduce from the Young inequality, Folland (1999), Theorem 6.18,

~h,~η

(m)

bA~h,~η ≡ K~h ⋆ bA~η ≡ K~η ⋆ bA~h,
p nbA~h,~η(·)o, ζ (m)
~h,~η (cid:13)(cid:13)(cid:13)p ≤ kKk1(cid:13)(cid:13)(cid:13)ζ (m)
(cid:13)(cid:13)(cid:13)ζ (m)

∀~h, ~η ∈ Hd.

(3.25)

~h

(·) = bA~h,~η(·) − Λ~h,~η(p,·) and ζ (m)
(·) = bA~h(·) −
~h (cid:13)(cid:13)(cid:13)p(cid:17).
~h,~η (cid:13)(cid:13)(cid:13)p ≤ kKk1(cid:16)(cid:13)(cid:13)(cid:13)ζ (m)
~η (cid:13)(cid:13)(cid:13)p ∧(cid:13)(cid:13)(cid:13)ζ (m)
~h (cid:13)(cid:13)(cid:13)p ⇒ (cid:13)(cid:13)(cid:13)ζ (m)
~h (cid:13)(cid:13)(cid:13)p − kKk−1
m(cid:16)(cid:13)(cid:13)(cid:13)ζ (m)
1 ∆m(~h)(cid:17)+

~h∈Hd

,

It yields for all ~h, ~η ∈ Hd

~h,~η (cid:13)(cid:13)(cid:13)p ≤ kKk1(cid:13)(cid:13)(cid:13)ζ (m)
~η (cid:13)(cid:13)(cid:13)p
(cid:13)(cid:13)(cid:13)ζ (m)
(cid:16)(cid:13)(cid:13)(cid:13)ζ (m)
~h,~η (cid:13)(cid:13)(cid:13)p − ∆m(~h) ∧ ∆m(~η)(cid:17)+ ≤ kKk1 sup

m in view of the obvious inequality a ∧ c ≤ b ∧ d + (a − b)+ ∨ (c − d)+

In view of the assertions (3.19) and (3.20) of Lemma (1) we can conclude that the hypothesis
Aupper(ii) is fulﬁlled for any p ≥ 1.

15

Veriﬁcation of the hypothesis Aupper. First, we note that ∆m is not random if p ∈ [1, 2] and,
therefore, in this case Aupper is checked with CΨ = 1.

Next, if p > 2 we have in view of the deﬁnition of ∆m and (3.23) for any p ∈ P(D)

960pkKk1kKkp

ln(p)

(cid:0)mV~h(cid:1)−1/2 ≤(cid:16)E

(m)
p ∆q

m(~h)(cid:17)1/q

≤ D(cid:0)mV~h(cid:1)−1/2,

∀~h ∈ Hd
m.

(3.26)

Moreover, ∆m ≥ 960pkKk1kKkp
(3.20) of Lemma (1) with CΨ = D ∨ 960pkKk1kKkp

ln(p)

ln(p)

.

(cid:0)mV~h(cid:1)−1/2 and, therefore, Aupper is fulﬁlled in view of the assertion

3.4.2. Some bounds for the quantities B(n)
Let us ﬁnd upper estimate for B(n)
A (f, h) and lower and upper estimates for ψn(f, h), which both
appear in (Ψn, ℓ)-oracle inequality. In the considered case f = p, n = m, h = ~h, A(f ) = A(p) = p,
ℓ(·) = k · kp and Ψn = ∆m.

A (f, h) and ψn(f, h).

Note ﬁrst that (3.25) implies that for any x ∈ Rd

Λ~h,~η(p, x) − Λ~η(p, x) = K~η ⋆ Λ~h(p, x) − Λ~η(p, x)

K~η(x − y)(cid:18)ZRd
Applying the Young inequality we obtain for all p ≥ 1

= ZRd

∀~h ∈ Hd.
In view of assertion (3.23) of Lemma 1 and the deﬁnition of ∆m we get

A (cid:0)p,~h(cid:1) ≤ (1 + 2kKk1)(cid:13)(cid:13)K~h ⋆ p − p(cid:13)(cid:13)p,
B(m)

K~h(z − y)p(z)dz − p(y)(cid:19)dy.

C(cid:0)nV~h(cid:1) 1

p∧2 −1 ≤ ψn(cid:0)p,~h(cid:1) :=hE

(m)

p (cid:8)∆p

m(h)(cid:9)i 1

p ≤ C(cid:0)nV~h(cid:1) 1

p∧2 −1,

(3.27)

∀p ∈ Pp,

(3.28)

where Pp = P if p < 2, Pp = {p ∈ P : p ∈ P(D)} if p ≥ 2 and C = C = 128kKk1kKkp if
p ∈ [1, 2), C = C = 9kKk1kKk2 if p = 2 and C = 960pkKk1kKkp
, C = D if p > 2. We remark that

the obtained lower and upper estimates are independent of p.

ln(p)

3.4.3. Computation of the rate on anisotropic Nikol’skii class.

In view of (3.27) and (3.28) we should bound from above

sup
p∈Fa

inf
~h∈H d

mn(cid:13)(cid:13)K~h ⋆ p − p(cid:13)(cid:13)p +(cid:0)mV~h(cid:1) 1

p∧2 −1o .

evaluated in Goldenshluger and Lepski (2011b).

This quantity with Fa = Np,d(cid:0)~β, L(cid:1), a = (~β, L(cid:1), where Np,d(cid:0)~β, L(cid:1) is anisotropic Nikol’skii class was
Let (e1, . . . , ed) denote the canonical basis of Rd. For function G : Rd → R1 and real number
u ∈ R deﬁne the ﬁrst order diﬀerence operator with step size u in direction of the variable xj by

∆u,jG(x) = G(x + uej) − G(x),

16

j = 1, . . . , d.

By induction, the k-th order diﬀerence operator with step size u in direction of the variable xj is
deﬁned as

∆k

u,jG(x) = ∆u,j∆k−1

u,j G(x) =

(−1)l+k(cid:18)k
kXl=1

l(cid:19)∆ul,jG(x).

Deﬁnition 1. For given ~β = (β1, . . . , βd) ∈ (0,∞)d, L > 0 and p ≥ 1 we say that function
G : Rd → R1 belongs to the anisotropic Nikolskii class Np,d(cid:0)~β, L(cid:1) if kGkp ≤ L and for every
j = 1, . . . , d there exists natural number kj > βj such that

(cid:13)(cid:13)(cid:13)∆kj
u,jG(cid:13)(cid:13)(cid:13)p ≤ L|u|βj ,

∀u ∈ R, ∀j = 1, . . . , d.

Kerkyacharian et al. (2001) or Goldenshluger and Lepski (2011b)].

We will use the following speciﬁc kernel K in the deﬁnition of the family A(cid:0)Hd
Let s ∈ N∗ be ﬁxed and let w : R1 → R1 satisfy R w(y)dy = 1, and w ∈ C(s)(R1). Put

m(cid:1) [see, e.g.,

Ks(y) =

(cid:18)s
i(cid:19)(−1)i+1 1

i

i(cid:17),
w(cid:16) y

sXi=1

dYj=1

K(x) =

Ks(xj),

x = (x1, . . . , xd).

(3.29)

Let s > 0 be an arbitrary but a priory chosen number and let the kernel K is constructed in
accordance with (3.29). Next result can be found for instance in Goldenshluger and Lepski (2011b).

For any ~β ∈ (0, s]d, L > 0 and p ≥ 1 there exists υ > 0 independent of L such that
p∧2 −1o ≤ υ sm(cid:0)Np,d(cid:0)~β, L(cid:1) ∩ Pp(cid:1)

mn(cid:13)(cid:13)K~h ⋆ p − p(cid:13)(cid:13)p +(cid:0)mV~h(cid:1) 1

p∈Np,d(~β,L)∩Pp

inf
~h∈H d

sup

(3.30)

=: υ L

1−1/(p∧2)

β+1−1/(p∧2) m−

1−1/(p∧2)

1+(1/β)(1−1/(p∧2) .

It is well-known that sm(cid:0)Np,d(cid:0)~β, L(cid:1) ∩ Pp(cid:1) is the minimax rate of convergence on Np,d(cid:0)~β, L(cid:1) ∩ Pp.
Concluding remarks. (Ψn, ℓ)-selection rule with Ψn = ∆m, n = m, and ℓ(·) = k·kp is a particular
case of the selection scheme proposed in Goldenshluger and Lepski (2011b). Selected in accordance
with this rule estimator from the collection (3.17) is, in view of (3.27), (3.28), (3.30) and the ﬁrst
assertion of Theorem 3, optimally-adaptive over the collection of anisotropic Nikol’skii classes. For
the ﬁrst time it was proved in Goldenshluger and Lepski (2011b), Theorem 4.

4. Adaptive estimation in the generalized deconvolution model.

Consider the following observation scheme introduced in Lepski and Willer (2015):

Zi = Xi + ǫiYi,

i = 1, . . . , n,

(4.1)

where Xi, i = 1, . . . , n, are i.i.d. d-dimensional random vectors with common density f to be
estimated. The noise variables Yi, i = 1, . . . , n, are i.i.d. d-dimensional random vectors with known
common density g. At last εi ∈ {0, 1}, i = 1, . . . , n, are i.i.d. Bernoulli random variables with
P(ε1 = 1) = α, where α ∈ [0, 1) is supposed to be known. The sequences {Xi, i = 1, . . . , n},
{Yi, i = 1, . . . , n} and {ǫi, i = 1, . . . , n} are supposed to be mutually independent.
Let us note that the case α = 1 corresponds to the pure deconvolution model Zi = Xi + Yi, i =
1, . . . , n discussed in Introduction, whereas the case α = 0 corresponds to the direct observation

17

scheme Zi = Xi, i = 1, . . . , n. The intermediate case α ∈ (0, 1) can be treated as the mathematical
modeling of the following situation. One part of the data, namely (1−α)n, is observed without noise.
If the indexes corresponding to these observations were known, the density f could be estimated
using only this part of the data, with the accuracy corresponding to the direct case. The main
question we will address in this intermediate case is whether the same accuracy would be achievable
if the latter information is not available?

the quality of an estimator procedures is measured by Lp-risk

p = RRd |g(x)|pdx, F = P is the set of all probability densities, B(f ) = f and

Thus, ρp(g) = kgkp

RB(cid:2)eBn, f(cid:3) =(cid:16)E

(n)

f h(cid:13)(cid:13)eBn − f(cid:13)(cid:13)p

pi(cid:17)1/p

,

f ∈ P.

1 = (X1, . . . , X[n/2]) and X (n)

2 = (X[n/2]+1, . . . , Xn), where [a] denotes the integer

At last, let X (n)
part of a ∈ R.
All results presented in this section will be a established under the following condition imposed
on the distribution of the noise variable Y1. In what follows for any Q ∈ L1(Rd) its Fourier transform
will be denoted by ˇQ.
Assumption 1. 1) if α ∈ (0, 1) then there exists ̟ > 0 such that
∀t ∈ Rd;

2) if α = 1 then there exists ~µ = (µ1, . . . , µd) ∈ (0,∞)d and G1, G2 > 0 such that

(cid:12)(cid:12)1 − α + αˇg(t)(cid:12)(cid:12) ≥ ̟,
dYj=1

2 ≤ |ˇg(t)| ≤ G2

µj

G1

dYj=1

(1 + t2

j )−

(1 + t2

j )−

µj
2 ,

∀t ∈ Rd.

We remark that Assumption 1 is very weak when α ∈ (0, 1). It is veriﬁed for many distributions,
including centered multivariate Laplace and Gaussian ones. Note also that Assumption 1 always
holds with ̟ = 1 − 2α if α < 1/2. Additionally, it holds with ̟ = 1 − α if ˇg is a real positive
function. The latter is true, in particular, for any probability law obtained by the even number of
convolutions of a symmetric distribution with itself. As to the case α = 1 Assumption 1 is s well-
known in the literature condition referred to moderately ill-posed statistical problem. In particular,
it is checked for multivariate Laplace and Gamma laws.

Below we introduce families of estimators whose construction involved kernel K. Throughout
this section without further mentioning we will additionally assume that ˇK is compactly supported
and ˇK(t) = 1 for all t ∈ [−1, 1]d.
α, ̟, G1, G2, ~µ appeared in Assumption 1. In particular they are independent of f and n.

From now on c1, c2, . . . , denote the constants that may depends only on K, p, D, D, d and

4.1. Idea of estimator construction.

Let K~h(·),~h ∈ (0, 1]d, be deﬁned in (1.3) and let M(cid:0)·,~h(cid:1) : Rd → R satisfy the operator equation

K~h(y) = (1 − α)M(cid:0)y,~h(cid:1) + αZRd

g(t − y)M(cid:0)t,~h(cid:1)dt,

y ∈ Rd.

(4.2)

18

j=1 hj, Hd

following estimator’s collection built from X (n)

Recall that V~h =Qd
[n/2](cid:1) =(cid:26)bB~h(·) = (n − [n/2])−1
B(cid:0)Hd

2

m =(cid:8)~h ∈ Hd : ln(m)/m ≤ V~h ≤ e−√ln(m)(cid:9), m ≥ 1, and introduce the

nXi=[n/2]+1

M(cid:0)Zi − ·,~h(cid:1), ~h ∈ Hd

[n/2](cid:27).

(4.3)

Our goal is to select an estimator from this collection and to study its properties. Following our
general receipt we suggest to proceed as follows.

• Consider ﬁrst the estimation of Ag(f ) = (1 − α)f + α[g ⋆ f ], where, remind ”⋆” denotes the
convolution operator. Note that Ag(f ) is the density of Z1 and, therefore, can be estimated
from the observation X (n)

using standard kernel estimator. Introduce the collection

1

• To each estimator from this collection associate its Lp-risk (ℓ(·) = k · kp)

K~h(Zi − ·), ~h ∈ Hd

[n/2](cid:27).

[n/2]Xi=1

(4.4)

A(cid:0)Hd

[n/2](cid:1) =(cid:26)bA~h(·) = [n/2]−1
RA(cid:2)bA~h, f(cid:3) =(cid:16)E

(n)

f h(cid:13)(cid:13)bA~h − Ag(f )(cid:13)(cid:13)p

pi(cid:17)1/p

,

f ∈ P.

• Select an estimator from the collection A(cid:0)Hd
based on the collection of auxiliary estimators (3.18), where T (m) = (X1, . . . , X[n/2]), m =
[n/2] and Ψn = ∆[n/2], ℓ(·) = k · kp. Thus, the selection rule is

[n/2](cid:1) in accordance with (Ψn, ℓ)-selection rule

bRn(cid:0)~h(cid:1)

~hn

:=

:=

sup
~η∈Hd

inf
~h∈Hd

[n/2]h(cid:13)(cid:13)bA~h,~η − bA~η(cid:1)(cid:13)(cid:13)p − 2∆[n/2](~η)i+
[n/2](cid:2)bRn(~h) + 2∆[n/2](~h)(cid:3).

;

(4.5)

(4.6)

• Choose the estimator bB~hn
The theoretical properties of the estimator bB~hn

Theorem 3(2) and Theorem 4.

(·).

(·) will be then deduced with help of Proposition 1,

4.2. Some bounds for the quantity En(·, ·).

Introduce the following notations. Set for any x ∈ Rd

χ(n)
~h

(x) = bB~h − Υ~h(f, x), Υ~h(f, x) = E

(n)

2,fnbB~h(x)o =ZRd

M(cid:0)z − x,~h(cid:1)Ag(f, z)dz

Put for brevity p = Ag(f ). Applying Bahr-Esseen and Rosenthal inequalities, von Bahr and Esseen
(1965), Rosenthal (1970), to the sum of i.i.d. random variables χ(n)
~h

(x), we have for any x ∈ Rd

c−1
4

~h

E

(n)

2,f(cid:12)(cid:12)(cid:12)χ(n)
≤(cid:16)n−1ZRd

p

≤ np−1ZRd(cid:12)(cid:12)M(cid:0)z − x,~h(cid:1)(cid:12)(cid:12)p

(x)(cid:12)(cid:12)(cid:12)
M 2(cid:0)z − x,~h(cid:1)p(z)dz(cid:17)p/2

p(z)dz,

p ≤ 2;

+ np−1ZRd(cid:12)(cid:12)M(cid:0)z − x,~h(cid:1)(cid:12)(cid:12)p

19

c−1
4

E

(n)

2,f(cid:12)(cid:12)(cid:12)χ(n)

~h

p

(x)(cid:12)(cid:12)(cid:12)

p(z)dz, p > 2.

(cid:16)E
(cid:16)E

(n)

p(cid:17)1/p
2,f(cid:13)(cid:13)χ(n)
~h (cid:13)(cid:13)p
p(cid:17)1/p
2,f(cid:13)(cid:13)χ(n)
~h (cid:13)(cid:13)p

(n)

≤ c4n1/p−1(cid:13)(cid:13)M(cid:0)·,~h(cid:1)(cid:13)(cid:13)p,
2(cid:26)ZRd(cid:16)ZRd
≤ c4n− 1
≤ c4(cid:16)kpk1/2−1/p

n− 1

∞

~h

(n)

(n)

p ≤ 2;

p =(cid:13)(cid:13)E
2,f(cid:13)(cid:13)χ(n)
~h (cid:13)(cid:13)p
2,f(cid:12)(cid:12)χ(n)
M 2(cid:0)z − x,~h(cid:1)p(z)dz(cid:17)p/2
2(cid:13)(cid:13)M(cid:0)·,~h(cid:1)(cid:13)(cid:13)2 + n

(·)(cid:12)(cid:12)p(cid:13)(cid:13)1 one has
dx(cid:27)1/p
p (cid:13)(cid:13)M(cid:0)·,~h(cid:1)(cid:13)(cid:13)p(cid:17),

+ c4n

1−p

p > 2

1−p

p (cid:13)(cid:13)M(cid:0)·,~h(cid:1)(cid:13)(cid:13)p

Noting that in view of the Fubini theorem E

(n)

To get the last estimate we have used again the Young inequality. Thus, for any ~h ∈ Hd we have
the following bound for the quantity E(·,·) deﬁned in (3.1)

p(cid:17)1/p
2,f(cid:13)(cid:13)χ(n)
~h (cid:13)(cid:13)p

≤ c5(cid:16)n−1/2(cid:13)(cid:13)M(cid:0)·,~h(cid:1)(cid:13)(cid:13)2 + n1/p−1(cid:13)(cid:13)M(cid:0)·,~h(cid:1)(cid:13)(cid:13)p(cid:17),

E(cid:0)f,~h(cid:1) :=(cid:16)E
We would like to stress that if α = 1 and kgk∞ < ∞ then Pp = P for all p ≥ 1.
Let ˇM(cid:0)t,~h(cid:1), t ∈ Rd, denote the Fourier transform of M(cid:0)·,~h(cid:1). Then, we obtain in view of the
deﬁnition of M(cid:0)·,~h(cid:1)
The conditions imposed on K guarantee that ˇM(cid:0)·,~h(cid:1) ∈ L1(cid:0)Rd(cid:1) ∩ L2(cid:0)Rd(cid:1) for any ~h ∈ Hd and,

ˇM(cid:0)t,~h(cid:1) = ˇK(cid:0)t~h(cid:1)(cid:2)(1 − α) + αˇg(−t)(cid:3)−1,

∀f ∈ Pp.

t ∈ Rd.

hence,

(4.8)

(4.7)

Set ~µ(α) = ~µ, α = 1, ~µ(α) = (0, . . . , 0), α ∈ [0, 1). We have in view of Assumption 1 for any ~h ∈ Hd

(cid:13)(cid:13)M(cid:0)·,~h(cid:1)(cid:13)(cid:13)∞ ≤ (2π)−d(cid:13)(cid:13) ˇM(cid:0)·,~h(cid:1)(cid:13)(cid:13)1,
(cid:13)(cid:13)M(cid:0)·,~h(cid:1)(cid:13)(cid:13)∞ ≤ c6

−1−µj (α)
h
j

dYj=1

,

(cid:13)(cid:13)M(cid:0)·,~h(cid:1)(cid:13)(cid:13)2 = (2π)−d(cid:13)(cid:13) ˇM(cid:0)·,~h(cid:1)(cid:13)(cid:13)2.
(cid:13)(cid:13)M(cid:0)·,~h(cid:1)(cid:13)(cid:13)2 ≤ c6

dYj=1

2 −µj (α)

− 1
h
j

,

(4.9)

Additionally we deduce from (4.9) for any p > 2

(cid:13)(cid:13)M(cid:0)·,~h(cid:1)(cid:13)(cid:13)p ≤(cid:13)(cid:13)M(cid:0)·,~h(cid:1)(cid:13)(cid:13)1− 2

∞ (cid:13)(cid:13)M(cid:0)·,~h(cid:1)(cid:13)(cid:13)

p

2
p

2 ≤ c6

dYj=1

−1+1/p−µj (α)
h
j

,

∀~h ∈ Hd.

(4.10)

If p < 2 we will study only the case α < 1/2. Then we have in view of the deﬁnition of M(cid:0)·,~h(cid:1),

applying the Young and triangle inequalities

It yields, for any p < 2 and α < 1/2

(cid:13)(cid:13)K~h(cid:13)(cid:13)p ≥ (1 − α)(cid:13)(cid:13)M(cid:0)·,~h(cid:1)(cid:13)(cid:13)p − α(cid:13)(cid:13)M(cid:0)·,~h(cid:1)(cid:13)(cid:13)p = (1 − 2α)(cid:13)(cid:13)M(cid:0)·,~h(cid:1)(cid:13)(cid:13)p.
(cid:13)(cid:13)M(cid:0)·,~h(cid:1)(cid:13)(cid:13)p ≤ (1 − 2α)−1kKkp

∀~h ∈ Hd.

h−1+1/p
j

dYj=1

,

Thus, we obtain from (4.7), (4.9), (4.10) and (4.11) for any ~h ∈ Hd

[n/2]

sup

f ∈Pp E(cid:0)f,~h(cid:1) ≤ c7


n− 1

2 Qd
p −1Qd

1

n

2 −µj (α)

− 1
j=1 h
j
−1+1/p−µj(α)
j=1 h
j

,

20

p ≥ 2, α ∈ [0, 1];
p < 2, α ∈ [0, 1/2).

,

(4.11)

(4.12)

To get the bound for p ≥ 2 we have taken into account that nV~h ≥ ln(n) if ~h ∈ Hd
In fact (4.7) is true for all α ∈ [0, 1] but it requires to impose several additional assumption on g
and we do not treat the case p < 2, α > 1/2 in the present paper. The interested reader can look
the paper Rebelles (2016), where the corresponding norm is computed when α = 1.

[n/2].

4.3. Consequences of Proposition 1 and Theorem 3 (2). Case α ∈ (0, 1).

Let us compute the approximation error related to the estimator bB~h. Note that for any ~h ∈ Hd

Υ~h(f, x)

M(cid:0)z − x,~h(cid:1)Ag(z)dz
M(cid:0)z − x,~h(cid:1)f (z)dz + αZRd
f (t)h(1 − α)M(cid:0)t − x,~h(cid:1) + αZRd

:= ZRd
= (1 − α)ZRd
= ZRd
= ZRd

M(cid:0)z − x,~h(cid:1)(cid:2)f ⋆ g(cid:3)(z)dz,
M(cid:0)u,~h(cid:1)g(u − [t − x])duidt

K~h(t − x)f (t)dt.
Thus, we have for any ~h ∈ Hd and any p ≥ 1

(4.13)

(4.14)

kΥ~h(f ) − fkp = kK~h ⋆ f − fkp.

4.3.1. Veriﬁcation of the condition (3.3) and (3.4) of Proposition 1.

We will study two diﬀerent cases: either p 6= 2, α < 1/2 or p = 2, α ∈ (0, 1).

Set Λ~h(f,·) = E

(n)

1,f(cid:8)bA~h(·)(cid:9), ~h ∈ (0, 1]d and note that

We have in view of (4.14) and the Young inequality (recall that g is a density)

Λ~h(f,·) =ZRd

K~h(z − ·)Ag(f, z)dz = (1 − α)(cid:2)K~h ⋆ f ](·) + α(cid:2)K~h ⋆ g ⋆ f ](·).
kΛ~h(f ) − Ag(f )kp = (cid:13)(cid:13)(1 − α)(cid:8)K~h ⋆ f − f(cid:9) + αg ⋆(cid:8)K~h ⋆ f − f(cid:9)(cid:13)(cid:13)p

≥ (1 − 2α)(cid:13)(cid:13)K~h ⋆ f − f(cid:13)(cid:13)p.

Thus, if α < 1/2 one has for any p ≥ 1 in view of (4.13)

kΥ~h(f ) − fkp ≤ (1 − 2α)−1kΛ~h(f ) − Ag(f )kp

and, therefore, the condition (3.3) of Proposition 1 is fulﬁlled with Cℓ = (1 − 2α)−1.
If p = 2 we have in view of (4.14), (4.13), Assumption 1 and the Parseval identity
kΛ~h(f ) − Ag(f )k2 = (cid:13)(cid:13)(1 − α)(cid:2) ˇK(~h·) − 1(cid:3) ˇf (·) + α(cid:2) ˇK(~h·) − 1(cid:3)ˇg(·) ˇf (·)(cid:13)(cid:13)2

= (cid:13)(cid:13)(cid:8)(1 + α) + αˇg(·)(cid:9)(cid:2) ˇK(~h·) − 1(cid:3) ˇf (·)(cid:13)(cid:13)2 ≥ ̟(cid:13)(cid:13)(cid:2) ˇK(~h·) − 1(cid:3) ˇf (·)(cid:13)(cid:13)2

= ̟kΥ~h(f ) − fk2.

We conclude that the condition (3.3) of Proposition 1 is fulﬁlled with Cℓ = ̟−1 if p = 2.

21

Let us now check (3.4) of Proposition 1. Note that the deﬁnition of ∆[n/2] implies

∆[n/2](cid:0)~h(cid:1) ≥ c9


n− 1

2 Qd
p −1Qd

1

n

,

− 1
2
j=1 h
j
j=1 h−1+1/p

j

p ≥ 2;
p < 2.

,

(4.15)

and therefore, (3.4) is fulﬁlled with CE = c9/c7 in view of (4.12), since recall ~µ(α) = 0, α 6= 1.

4.3.2. Main results.

We deduce from Proposition 1 and Lemma 1 the following assertion.
Theorem 5. Let either p 6= 2, α < 1/2 or p = 2, α ∈ (0, 1). Then for all n ≥ 2

RB(cid:2)bB~hn

, f(cid:3) ≤ c10 inf

~h∈Hnn(cid:13)(cid:13)K~h ⋆ f − f(cid:13)(cid:13)p +(cid:0)nV~h(cid:1) 1

p∧2 −1o + ε[n/2],

∀f ∈ Pp.

We also have the following adaptive result.

Theorem 6. Let either p 6= 2, α < 1/2 or p = 2, α ∈ (0, 1). Then the estimator bB~hn
adaptive over the scale of anisotropic classes (cid:8)Np,d(cid:0)~β, L(cid:1) ∩ Pp, ~β ∈ (0, s]d, L > 0o.
The fact that bB~hn

is Sn-adaptive with Sn =ns[n/2](cid:0)Np,d(cid:0)~β, L(cid:1) ∩ Pp(cid:1), ~β ∈ (0, s]d, L > 0(cid:9) follows

from (3.30) with p = f and m = [n/2], the assertion (3.21) of Lemma 1 and the second assertion
of Theorem 3.

is optimally-

The lower bound for minimax risk showing that Sn is the family of minimax rates in this problem

can be found in Lepski and Willer (2015), Theorem 1.

Remark 3. The results presented in Theorems 5 and 6 are new.

4.4. Consequences of Theorem 4. Case α = 1, p = 2, d = 1.

In this section we will study the adaptive estimation over the collection of univariate Sobolev classes.
Deﬁnition 2. Let β1 ∈ (0,∞)d and L > 0 be given. We say that Q ∈ L1(R1) belongs to Sobolev
class W(cid:0)β1, L(cid:1) if

ZR1(cid:0)1 + t2(cid:1)β1(cid:12)(cid:12) ˇQ(t)(cid:12)(cid:12)2dt ≤ L2.

The adaptive estimation over the collection of anisotropic Sobolev classes when α = 1 was studied

in Comte and Lacour (2013).

Auxiliary inequalities. The results below follow from the Parseval identity, the properties of

the kernel K and Assumption 1. First note, that for any f ∈ W(cid:0)β1, L(cid:1) and any h1 ∈ (0, 1)

kKh1 ⋆ Ag(f ) − Ag(f )k2 ≤ G2Lhβ1+µ1

1

,

since obviously Ag(f ) ∈ W(cid:0)β1 + µ1, G2L(cid:1).

It yields in view of (3.27) and (3.28) with p = Ag(f ) and p = 2,

22

sn(cid:0)Fa(cid:1) = sup

f ∈Fa

inf

h∈Hn(cid:8)B(n)

=:

sup

f ∈W(β1,L)

inf

h1∈H 1

A (f, h) + ψn(f, h)(cid:9)
2o
nn(1 + 2kKk1)kKh1 ⋆ Ag(f ) − Ag(f )k2 + c11(cid:0)nh1(cid:1)− 1
2β1+2µ1+1 =: c12sn(cid:0)W(cid:0)β1, L(cid:1)(cid:1).

(4.16)

≤ c12L

1

2β1+2µ1+1 n− β1+µ1

We also have in view of Assumption 1 for any f ∈ W(cid:0)β1, L(cid:1), y > 0 and h1 ∈ (0, 1]

kKh1 ⋆ f − fk2

−y | ˇK(h1t) − 1|2| ˇf (t)|2dt + (1 + kKk1)2ZR

2 ≤ Z y
≤ c13(cid:20)(1 + y2µ1)Z y
≤ c13(cid:20)(1 + y2µ1)kKh1 ⋆ Ag(f ) − Ag(f )k2

−y |g(t)|2| ˇK(h1t) − 1|2| ˇf (t)|2dt + L2y−2β1(cid:21)

2 + L2y−2β1(cid:21).

1R\[−y,y](t)| ˇf (t)|2dt

Minimizing the r.h.s. of the latter inequality in y we get for any f ∈ W(cid:0)β1, L(cid:1) and h1 ∈ (0, 1]
kKh1 ⋆ f − fk2 ≤ c13(cid:20)kKh1 ⋆ Ag(f ) − Ag(f )k2 + L

µ1+β1 kKh1 ⋆ Ag(f ) − Ag(f )k

β1+µ1
2

(cid:21).

µ1

β1

(4.17)

Application of Theorem 4. First, we note that all assumptions of Theorem 4 are fulﬁlled.
Indeed, in Section 3.4.1 we have already checked the hypotheses Apermute, Aupper and Aupper.
Moreover, deﬁned in (3.16) quantity in our case is given by

It yields together with (4.16) that

ψn(cid:0)W(cid:0)β1, L(cid:1), h1(cid:1) = c14(cid:0)nh1(cid:1)− 1

2 .

Vn(a) =nh1 ∈ H1

n : h1 ≥ c15L−

2

2β1+2µ1+1 n−

1

2β1+2µ1+1o,

a = (β1, L),

and we have that in view of (4.12)

sup
f ∈Fa

sup

h∈Vn(a) En(cid:0)f, h1(cid:1) ≤ c16L

1+2µ1

2β1+2µ1+1 n−

β1

2β1+2µ1+1 .

(4.18)

Since Un(a, f ) =(cid:8)h1 ∈ H1

n : kKh1 ⋆ Ag(f ) − Ag(f )k2 < 8sn(cid:0)W(cid:0)β1, L(cid:1)(cid:1)(cid:9) we obtain from (4.17)

µ1

µ1+β1hsn(cid:0)W(cid:0)β1, L(cid:1)(cid:1)i β1

β1+µ1 = c17L

1+2µ1

2β1+2µ1+1 n−

β1

2β1+2µ1+1 .

sup
f ∈Fa

sup

h∈Un(a,f ) kKh1 ⋆ f − fk2 ≤ c17L

It yields together with (4.18)

where c18 independent of L. Putting µ∗ = maxj=1,...,d µj we note that

ϕn(cid:0)W(cid:0)β1, L(cid:1)(cid:1) = c18L

1+2µ1

2β1+2µ1+1 n−

β1

2β1+2µ1+1 .

(4.19)

δn ≥ c19n− 1

2 ,

sup
f ∈P

sup
h1∈H1
n

En(f, h1) ≤ c20nµ∗
23

,

h1∈(0,1]kKh1 ⋆ f − fk2 ≤ 2kKk1L.

sup

f ∈W(cid:0)β1,L(cid:1) sup

Hence, in view of the assertion (3.21) of Lemma 1 we can state that

lim sup

n→∞

naκn(cid:0)W(cid:0)β1, L(cid:1)(cid:1) = 0,

∀a > 0.

(4.20)

Thus, we deduce from (4.17), (4.19), (4.20) and Theorem 4 the following result.

Assertion 1. Let hn comes from (4.5)–(4.6). Then for any µ1 > 0, β1 > 0 and L > 0

lim sup

n→∞

(n)

sup

ϕ−1

f ∈W(cid:0)β1,L(cid:1)nE

n (cid:0)W(cid:0)β1, L(cid:1)(cid:1)
is optimally-adaptive over the scale (cid:8)W(cid:0)β1, L(cid:1), β1 > 0, L > 0(cid:9).

2o 1
f (cid:13)(cid:13)bB~hn − f(cid:13)(cid:13)2

2 < c21,

where c21 is independent of L.

The estimator bB~hn

Assertion 1 is the particular case of the results obtained in Comte and Lacour (2013), which were

established by the use of completely diﬀerent selection scheme.

5. Simultaneous adaptive estimation of partial derivatives in the density model.

Let as previously Xi, i = 1, . . . , n, are i.i.d. d-dimensional random vectors with unknown density
f . This is the particular case of the model considered above corresponding to α = 0.

For any multi-index m = (m1, . . . , md) ∈ Nd let f (m) = ∂m1+···+md f
of order m. The goal is to estimate B(f ) = f (m) under Lp-loss, that is

md
d

···∂x

m1
1

∂x

be the partial derivative of f

RB(cid:2)eBn, f(cid:3) =(cid:16)E

(n)

f h(cid:13)(cid:13)eBn − f (m)(cid:13)(cid:13)p

pi(cid:17)1/p

,

f ∈ P.

[n/2] as it was before or by its subset Hisotrop

Remark 4. We will keep all previous notations with only one change related to the following
simple observation. All oracle results established in the present paper remain valid if one replace in
all formulas the set Hn (abstract model) or Hd
m (density model) by its arbitrary subset. In particular
we will use below the selection rule (4.5)–(4.6) from the collection A(·), which is parameterized by
[n/2] : h1 = ··· = hd(cid:9). The latter
either Hd
set will be used when the adaptation over the scale of isotropic, (β1 = ··· = βd) Nikol’skii classes
is studied.
and let the collection A(cid:0)Hn(cid:1) be given by (4.4)
dYj=1
(−1)mj h

[n/2] is replaced by Hn) and Zi = Xi, i = 1, . . . , [n/2]. Introduce the family of estimators

B(cid:0)Hn(cid:1) =(cid:26)bB~h(·) = (n − [n/2])−1

~h (cid:0)Xi − ·(cid:1), ~h ∈ Hn(cid:27),

Thus, let Hn denote either Hd

:= (cid:8)~h ∈ Hd

[n/2] or Hisotrop

nXi=[n/2]+1

(where Hd

−mj
j K (m)

n

n

where K (m) denotes the partial derivative of K of the order m.

It is important to note that the estimator bB~hn
if we would construct
it using observations Xi, i = 1, . . . , [n/2] instead of Xi, i = [n/2] + 1, . . . , n. The interest to this
collection is dictated by the following minimax result. For any ~β ∈ (0,∞)d and m ∈ Nd put

would coincide with bA(m)

~hn

1
β

=

dXj=1

1
βj

,

1
ω

=

dXj=1

mj
βj

.

24

Proposition 2. For any p ≥ 1, L > 0, any ~β ∈ (0,∞)d and m ∈ Nd provided ω > 1 one can ﬁnd a
is rate-minimax for f (m) on Np,d(cid:0)~β, L(cid:1)∩ Pp.
kernel K and ~hn ∈ (0, 1]d such that the estimator bA(m)

The minimax rate of convergence is given by

~hn

ϕ(m)

n (cid:0)Np,d(cid:0)~β, L(cid:1) ∩ Pp(cid:1) = L

(1/β)(1−1/p∧2)+1/ω

1+(1/β)(1−1/p∧2) n− (1−1/ω)(1−1/p∧2)
1+(1/β)(1−1/p∧2) .

The conditions ω > 1 and p > 1 are necessary for the existence of uniformly consistent on Np,d(cid:0)~β, L(cid:1)

estimators.

The assertions of the proposition seems to be new.
Now, let us formulate the main result of this section. As previously let s ∈ N∗ be an arbitrary
but a priory chosen number. Let the kernel K, involved to the description of estimators from the
collections A(cid:0)Hn(cid:1) and B(cid:0)Hn(cid:1), be constructed in accordance with (3.29).
Set at last Πs =(cid:8)(~β, m) ∈ (0, s]d × Nd : ω > 1(cid:9).
Theorem 7. Let ~hn be the issue of (4.5)–(4.6) with Hn = Hd
Hn = Hisotrop

[n/2] if m1 = ··· = md or with

n

n→∞ hϕ(m)

if β1 = ··· = βd. Then for any p ≥ 1, L > 0, and (~β, m) ∈ Πs
po 1
f (cid:13)(cid:13)bB~hn − f(cid:13)(cid:13)p

n (cid:0)Np,d(cid:0)~β, L(cid:1) ∩ Pp(cid:1)i−1

f ∈Np,d(cid:0)~β,L(cid:1)∩PpnE(n)

sup

lim sup

p < c21,

where c21 is independent of L.

bB~hn

Theorem 7 together with second statement of Proposition 2 allows us to assert that the estimator
is optimally-adaptive over the scale of either anisotropic Nikol’skii classes (if m1 = ··· = md)
or isotropic ones (without any restriction on the order of the considered partial derivative). Note
that the problem is completely solved in the dimension 1. Also we conclude that the diﬀerentiation
of an optimally-adaptive estimator leads to the optimally-adaptive estimator of the corresponding
partial derivative.

5.1. Proof of Proposition 2.

Lemma 2. For any p ≥ 1, any ~β and m, provided ω > 1, and any f ∈ Np,d(cid:0)~β, L(cid:1) one has

kf (m)kp ≤ c23L1/ωkfk1−1/ω

p

,

(5.1)

where c23 is independent of L.

The author persuades that this Kolmogorov-type inequality should be known but he was unable

to ﬁnd the exact reference.

Proof of Lemma 2. The following inequality was proved in Goldenshluger and Lepski (2014).
For any p ≥ 1 and s ∈ Nd there exists c22 is independent of L such that

dXj=1

hβj
j ,

∀~h ∈ (0, 1]d, ∀~β ∈ (0, s]d.

(5.2)

sup

F ∈Np,d(cid:0)~β,L(cid:1)kK~h ⋆ F − Fkp ≤ c22L

25

The following statement can be found in Nikol’skii (1977), Chapter 5, Theorem 5.6.3.

If ω > 1 and f ∈ Np,d(cid:0)~β, L(cid:1) then f (m) exists and

f (m) ∈ Np,d(cid:0)~γ, cL(cid:1),

γj = βj(1 − 1/ω), j = 1, . . . , d,

(5.3)

and c > 0 is independent of L.

Since ~β is ﬁxed we can always choose s ∈ Nd in order to have ~β ∈ (0, s]d and, then, (5.2) will be

fulﬁlled. We obviously have by the triangle inequality

kf (m)kp ≤(cid:13)(cid:13)(cid:13)(cid:0)K~h(cid:1)(m) ⋆ f − f (m)(cid:13)(cid:13)(cid:13)p

j=1(−1)mj h−mj

j K (m)
~hn

+(cid:13)(cid:13)(cid:13)(cid:0)K~h(cid:1)(m) ⋆ f(cid:13)(cid:13)(cid:13)p

.

for any ~h ∈ (0, 1]d and we have ﬁrst by

It is easy to see that (cid:0)K~h(cid:1)(m) = Qd

integrating by parts

Hence, applying (5.2) with F = f (m) and ~β replaced by ~γ we obtain in view of (5.3)

(cid:0)K~h(cid:1)(m) ⋆ f − f (m) = K~h ⋆ f (m) − f (m).

∀~h ∈ (0, 1]d.

hγj
j ,

dXj=1

Moreover, we obtain applying the Young inequality

(cid:13)(cid:13)(cid:13)(cid:0)K~h(cid:1)(m) ⋆ f − f (m)(cid:13)(cid:13)(cid:13)p ≤ c24L
(cid:13)(cid:13)(cid:13)(cid:0)K~h(cid:1)(m) ⋆ f(cid:13)(cid:13)(cid:13)p ≤(cid:13)(cid:13)(cid:13)(cid:0)K~h(cid:1)(m)(cid:13)(cid:13)(cid:13)1kfkp = kfkp(cid:13)(cid:13)K (m)(cid:13)(cid:13)1
kf (m)kp ≤ c25(cid:26)L
j (cid:27),

that yields together with (5.5) for any f ∈ Np,d(cid:0)~β, L(cid:1)
+ kfkp

hβj (1−1/ω)
j

h−mj

dXj=1

dYj=1

dYj=1

h−mj
j

,

∀~h ∈ (0, 1]d.

(5.4)

(5.5)

Choosing hj = (||f||p/L)1/βj (that is possible since ||f||p ≤ L in view of the deﬁnition of the
Nikol’skii class) we come to the assertion of the lemma.

Proof of the proposition. Since

[n/2]Xi=1

dYj=1
(−1)mj h−mj

~hn

bA(m)

(·) = [n/2]−1

j K (m)
we have in view of (5.5) for any p ≥ 1, f ∈ Np,d(cid:0)~β, L(cid:1) and ~h ∈ (0, 1]d
=(cid:13)(cid:13)(cid:13)(cid:0)K~h(cid:1)(m) ⋆ f − f (m)(cid:13)(cid:13)(cid:13)p ≤ c24L

~h (cid:1) − f (m)(cid:13)(cid:13)(cid:13)p

1,f(cid:0)bA(m)

(cid:13)(cid:13)(cid:13)E

(n)

~hn (cid:0)Xi − ·(cid:1)

dXj=1

hβj(1−1/ω)
j

.

(5.6)

Next, repeating the computations led to (4.12) (remind that our considerations here correspond to
the case α = 0) we get for any p ≥ 1, f ∈ P and ~h ∈ (0, 1]d
dYj=1

hj(cid:17) 1

~h − E

dYj=1

(cid:16)n

hE

h−mj
j

p∧2 −1

(5.7)

(n)

.

p

~h (cid:1)(cid:13)(cid:13)(cid:13)
pi 1
1,f(cid:0)bA(m)
p ≤ c26
26

(n)

1,f(cid:13)(cid:13)(cid:13)bA(m)

We deduce from (5.6) and (5.7) that for any p ≥ 1, f ∈ Np,d(cid:0)~β, L(cid:1) ∩ Pp and ~h ∈ (0, 1]d
hj(cid:17) 1
RA(cid:2)bA~h, f (m)(cid:3) :=hE
dYj=1

p ≤ c27(cid:26)L
1,f(cid:13)(cid:13)(cid:13)bA(m) − f (m)(cid:13)(cid:13)(cid:13)
pi 1

hβj(1−1/ω)
j

dXj=1

dYj=1

(cid:16)n

h−mj
j

(n)

+

p

p∧2 −1(cid:27).

Noting that the right hand side of the obtained inequality is independent of f and minimizing it
with respect to ~h we come to the following bound.

sup

f ∈Np,d(cid:0)~β,L(cid:1)RA(cid:2)bA~hn

, f (m)(cid:3) ≤ c28ϕ(m)

n (cid:0)Np,d(cid:0)~β, L(cid:1) ∩ Pp(cid:1),

where ~hn is the minimizer of the right hand side of penultimate inequality.
Thus, we have proved that the maximal risk is upper-bounded by ϕ(m)

proof of the corresponding lower bound estimate (including the third assertion of the proposition)
follows immediately from general lower bound construction established in Goldenshluger and Lepski
(2014), Theorem 3 (tail and dense zones) and it can be omitted.

n (cid:0)Np,d(cid:0)~β, L(cid:1) ∩ Pp(cid:1). The

5.2. Proof of Theorem 7.

The proof consists in the application of Theorem 4. Since in Section 3.4.1 we have already checked
the hypothesis Apermute, Aupper and Aupper, it remains to compute the quantities

f ∈Fa

sup

h∈Un(a,f )

ϕn(Fa) = sup
f ∈Fa

f ∈Fah sup

h∈Hn En(f, h) + 6 sup

h∈Vn(a) En(f, h) + sup
sup

κn(Fa) = (cid:0)5εn/δn) sup

ρ(cid:0)Υh, B(f )(cid:1),
ρ(cid:0)Υh, B(f )(cid:1),i,
where Un(a, f ) =(cid:8)h : ℓ(cid:0)Λh, A(f )(cid:1) < 8CΨsn(Fa)(cid:9) and Vn(a) =(cid:8)h : ψn(Fa, h) < 4C 2
In the considered problem Fa = Np,d(cid:0)~β, L(cid:1) ∩ Pp, a =(cid:0)~β, L(cid:1), h = ~h ∈ Hn and
ρ(cid:0)Υh, B(f )(cid:1) =(cid:13)(cid:13)(cid:13)(cid:0)K~h(cid:1)(m) ⋆ f − f (m)(cid:13)(cid:13)(cid:13)p
hj(cid:17) 1

ℓ(cid:0)Λh, A(f )(cid:1) =(cid:13)(cid:13)(cid:13)K~h ⋆ f − f(cid:13)(cid:13)(cid:13)p
ψn(Fa, h) = C(cid:0)nV~h(cid:1) 1

We also have from (3.28) and (5.7) that for any f ∈ Pp and independently of ~β and L

En(f,~h) ≤ c26

dYj=1

dYj=1

(cid:16)n

p∧2 −1,

h−mj
j

p∧2 −1

h∈Hn

,

.

Ψsn(Fa)(cid:9).

To get the second inequality we have taken into account that the estimators bB~h and bA(m)

same distribution. Moreover

~h

s[n/2](cid:0)Np,d(cid:0)~β, L(cid:1) ∩ Pp(cid:1) = c29L

β+1−1/(p∧2) n−
as it was found in (3.30). It remains to note that δn ≥ c30n− 1
of Hd

[n/2] and moreover in view of (5.8), (5.9) and (5.3) and (5.4)

1−1/(p∧2)

1−1/(p∧2)

1+(1/β)(1−1/(p∧2)

2 in view of (3.28) and the deﬁnition

sup
~h∈Hd

[n/2]

sup

f ∈Pp En(f, h) ≤ nm∗

,

sup

f ∈Np,d(cid:0)~β,L(cid:1)(cid:13)(cid:13)(cid:13)(cid:0)K~h(cid:1)(m) ⋆ f − f (m)(cid:13)(cid:13)(cid:13)p ≤ c31L,

27

(5.8)

(5.9)

have the

where m∗ = maxj=1,...,d mj. To get the last bound we have also used the Young inequality.

Since the hypotheses Aupper and Aupper are checked with ε[n/2](p) verifying the assertion (3.21)

of Lemma 1 we can ﬁrst assert that

lim sup

n→∞

naκn(Fa) = 0,

∀a > 0.

(5.10)

Next, in view of (5.4) we have for any ~h ∈ (0, 1]d

(cid:0)K~h(cid:1)(m) ⋆ f − f (m) = K~h ⋆ f (m) − f (m) =(cid:0)K~h ⋆ f − f(cid:1)(m)

and, applying Lemma 2, we obtain for any f ∈ Np,d(cid:0)~β, L(cid:1)

ρ(cid:0)Υh, B(f )(cid:1)

:= (cid:13)(cid:13)(cid:0)K~h(cid:1)(m) ⋆ f − f (m)(cid:13)(cid:13)p ≤ c23L1/ω(cid:13)(cid:13)K~h ⋆ f − f(cid:13)(cid:13)1−1/ω
=: c23L1/ωℓ1−1/ω(cid:0)Λh, A(f )(cid:1).

p

It yields for any (~β, m) ∈ Πs and any p ≥ 1

sup
f ∈Fa

sup

h∈Un(a,f )

ρ(cid:0)Υh, B(f )(cid:1) ≤ c32L1/ωhsn(cid:0)Np,d(cid:0)~β, L(cid:1) ∩ Pp(cid:1)i1−1/ω

1+(1/β)(1−1/p∧2) n− (1−1/ω)(1−1/(p∧2))
n (cid:0)Np,d(cid:0)~β, L(cid:1) ∩ Pp(cid:1).
Put for brevity sn = sn(cid:0)Np,d(cid:0)~β, L(cid:1) ∩ Pp(cid:1), r = p ∧ 2 and remark that

= c33ϕ(m)

= c33L

(1/β)(1−1/p∧2)+1/ω

1+(1/β)(1−1/(p∧2)

Vn(a) =n~h ∈ Hn : V

1− 1
r
~h

> n

1

r −1(c33sn)−1o =n~h ∈ Hn : V~h > c34(cid:0)Ln1−1/r(cid:1)−

(5.11)

1

β+1−1/ro.

We obtain in view of (5.9)

En(f,~h) ≤ c26


n1/r−1V 1/r−1−m1

~h
d(1/r−1−d−1 Pd
n1/r−1h
1

,

j=1 mj )

,

Note that both bounds coincide since V~h = hd
case. Simple algebra shows that

1 if ~h ∈ Hisotrop

n

m1 = ··· = md;
~h ∈ Hisotrop
and m1 = d−1Pd

n

.

j=1 mj in the ﬁrst

sup

f ∈Np,d(cid:0)~β,L(cid:1)∩Pp

sup

~h∈Vn(a) En(f, h) ≤ c35ϕ(m)

n (cid:0)Np,d(cid:0)~β, L(cid:1) ∩ Pp(cid:1).

(5.12)

The assertion of the theorem follows now from (5.10), (5.11), (5.12) and Theorem 4.

28

References

Baraud, Y., Birg´e, L. and Sart, M. (2014). A new method for estimation and model selection:

ρ-estimation. arXiv:1403.6057v1, http://arxiv.org.

Barron, A., Birg´e, L. and Massart, P. (1999). Risk bounds for model selection via penalization.

Probab. Theory Related Fields, 113, 301–413.

Birg´e, L. and Massart, P. (2001). Gaussian model selection. J. Eur. Math. Soc. (JEMS), 3, 3,

203–268.

von Bahr, B. and Esseen, C.-G. (1965). Inequalities for the rth absolute moment of a sum of

Bunea, F., Tsybakov, A. B. and Wegkamp, M. H. (2007). Aggregation for Gaussian regression.

random variables, 1 ≤ r ≤ 2. Ann. Math. Statist., 36, 299–303.
Ann. Statist., 35, 4, 1674-1697.

Cai, T. T. (1999). Adaptive wavelet estimation: a block thresholding and oracle inequality ap-

proach. Ann. Statist. 27, 3, 898–924.

Cavalier, L. and Tsybakov, A.B. (2001). Penalized blockwise Stein’s method, monotone oracle

and sharp adaptive estimation. Math. Methods Statist., 10, 247–282.

Cavalier, L. and Golubev, G.K. (2006). Risk hull method and regularization by projections of

ill-posed inverse problems. Ann. Statist., 34, 1653–1677.

Chernousova, E. and Golubev, G.K. (2014). Pointwise adaptive estimation of a multivariate

function. Math. Methods of Statist., 23, 2, 1–16.

Comte, F. and Lacour, C. (2013). Anisotropic adaptive kernel deconvolution. Ann. Inst. H.

Poincar´e Probab. Statist., 49, 2, 569–609.

Dalalyan, A. and Tsybakov, A.B. (2008). Aggregation by exponential weighting, sharp PAC-

Bayesian bounds and sparsity. Machine Learning, 72, 39–61.

Devroye, L. and Lugosi, G. (1997). Nonasymptotic universal smoothing factors, kernel com-

plexity and Yatracos classes. Ann. Statist., 25, 2626–2637.

Efroimovich, S. Yu. (1998. Simultaneous sharp estimation of function and their derivatives. Ann.

Statist., 26, 1, 273–278.

Egishyants, S.A. and Ostrovskii, E. I. (1996). Local and global upper function for random

ﬁelds. Theory Probab. App., 41, 4, 657–665.

Folland, G. B. (1999). Real Analysis. Modern Techniques and Their Applications. Second edition.

John Wiley & Sons, Inc., New York.

Goldenshluger, A. and Lepski, O.V. (2008). Pointwise selection rule in multivariate function

estimation. Bernoulli, 14, 4, 1150–1190.

Goldenshluger, A. (2009). A universal procedure for aggregating estimators. Ann. Statist., 37,

1, 542-568.

Goldenshluger, A. and Lepski, O.V. (2009). Structural adaptation via Lp-norm oracle inequal-

ities. Probab. Theory Related Fields, 143, 41-71.

Goldenshluger, A. and Lepski, O. (2011a). Uniform bounds for norms of sums of independent

random functions. Ann. Probab., 39, 6, 2318–2384.

Goldenshluger, A. and Lepski, O.V. (2011b). Bandwidth selection in kernel density estimation:

oracle inequalities and adaptive minimax optimality. Ann. Statist., 39, 1608–1632.

Goldenshluger, A. and Lepski, O.V. (2012). General selection rule from the family of linear

estimators. Theory Probab. Appl., 57, 2, 257–277.

Goldenshluger, A. and Lepski, O.V. (2014). On adaptive minimax density estimation on Rd.

Probab. Theory Related Fields, 159, 479–543.

Iouditski, A.B., Lepski, O.V. and Tsybakov, A.B. (2009). Nonparametric estimation of com-

29

posite functions. Ann. Statist., 37, 3, 1360–1440.

Kerkyacharian, G., Lepski, O. and Picard, D. (2001). Nonlinear estimation in anisotropic

multi–index denoising. Probab. Theory Related Fields, 121, 137–170.

Kerkyacharian, G., Lepski, O. and Picard, D. (2008). Nonlinear estimation in anisotropic

multiindex denoising. Sparse case. Theory Probab. Appl., 52, 58–77.

Knapik, B., and Solomond, J-B. (2015). A general approach to posterior contraction in non-

parametric inverse problems. arXiv:1407.0335v2, http://arxiv.org

Lepski, O.V., and Levit B.Ya. (1998). Adaptive minimax estimation of inﬁnitely diﬀerentiable

functions. Math. Methods Statist., 7, 2, 123–156.

Lepski, O.V. (2013). Multivariate density estimation under sup-norm loss: oracle approach, adap-

tation and independence structure. Ann. Statist., 41, 2, 1005–1034.

Lepski, O. (2013a) Upper Functions for Positive Random Functionals. I. General Setting and

Gaussian Random Functions. Math. Methods of Statist., 22, no. 1 (2013), 1-27.

Lepski, O. (2013b) Upper Functions for Positive Random Functionals. II. Application to the

Empirical Processes Theory, Part 1. Math. Methods of Statist., 22, 2, 83-99.

Lepski, O. (2013c) Upper Functions for Positive Random Functionals. II. Application to the

Empirical Processes Theory, Part 2. Math. Methods of Statist., 22, 3, 193-212.

Lepski, O.V. (2015). Adaptive estimation over anisotropic functional classes via oracle approach.

Annals of Statistics, 43, 3, 1178–1242.

Lepski, O.V. (2016). Upper functions for Lp-norm of gaussian random ﬁelds. Bernoulli, 22, 2,

732–773.

Lepski, O.V., and Willer T. (2015). Lower bounds in the convolution structure density model.

Bernoulli, to appear.

Massart, P. (2007). Concentration inequalities and model selection. Ecole d’´et´e de Probabilit´e de

Saint-Flour 2003. Lecture Notes in Mathematics 1896, Springer Berlin/Heidelberg.

Nemirovski, A. S. (2000). Topics in non-parametric statistics. In Lectures on probability theory

and statistics (Saint-Flour, 1998) Lecture Notes in Math. Springer, Berlin 1738, 85-277.

Nikol’skii, S. M. (1977). Priblizhenie Funktsii Mnogikh Peremennykh i Teoremy Vlozheniya. (in
Russian). [Approximation of functions of several variables and imbedding theorems.] Second
edition, revised and supplemented. Nauka, Moscow.

Reynaud-Bouret, P., Rivoirard, V., Gramont, F. and Tuleau-Malot, C. (2014).
Goodness-of-ﬁt tests and nonparametric adaptive estimation for spike train analisys. Journal
of Mathematical Neuroscience, 4:3, 1–41.

Rebelles, G. (2016). Structural adaptive deconvolution under Lp-loss. Math. Methods of Statist.,

25, 1,

Rigollet, P. and Tsybakov, A. B. (2007). Linear and convex aggregation of density estimators.

Math. Methods Statist., 16, 260–280.

Rosenthal H.P. (1970). An the subspaces of Lp (p > 2) spanned by sequences of independent

random variables Israel J. Math., 8 273303.

Tsybakov, A. (2003). Optimal rate of aggregation. Proc. COLT. Lecture Notes in Artiﬁcial In-

telligence, 2777, 303–313.

Wegkamp, M.H. (2003). Model selection in nonparametric regression. Ann. Statist., 31, 252-273.
van de Geer, S. (2000). Applications of Empirical Process Theory. Cambridge University Press,

Cambridge.

van der Vaart, A. W. and Wellner, J. A. (1996). Weak Convergence and Empirical Processes.

Springer, New York.

30

