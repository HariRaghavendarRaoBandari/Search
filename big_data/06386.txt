6
1
0
2

 
r
a

 

M
1
2

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
6
8
3
6
0

.

3
0
6
1
:
v
i
X
r
a

Stratiﬁed Monte Carlo simulation of Markov chains

Rana Fakhereddinea, Rami El Haddada, Christian L´ecotb,∗

aUniversit´e Saint-Joseph, Facult´e des Sciences, BP 11-514 Riad El Solh, Beyrouth 1107

bUniversit´e de Savoie, LAMA, UMR 5127, Campus universitaire, 73376 Le

Bourget-du-Lac, France

2050, Liban

Abstract

We present several Monte Carlo strategies for simulating discrete-time Markov

chains with continuous multi-dimensional state space; we focus on stratiﬁed

techniques. We ﬁrst analyze the variance of the calculation of the measure of a

domain included in the unit hypercube, when stratiﬁed samples are used. We

then show that each step of the simulation of a Markov chain can be reduced

to the numerical integration of the indicator function of a subdomain of the

unit hypercube. Our approach for Markov chains simulates N copies of the

chain in parallel using stratiﬁed sampling and the copies are sorted after each

step, according to their successive coordinates. We analyze variance reduction

on examples of pricing of European and Asian options: enhanced eﬃciency of

stratiﬁed strategies is shown.

Keywords: Stratiﬁed sampling, Monte Carlo simulation, Markov chains

1. Introduction

Many real-life systems can be modeled using Markov chains. Fields of ap-

plication are queueing theory, telecommunications, option pricing, etc. In most

interesting situations, analytic formulas are not available and the state space of

∗Corresponding author
Email addresses:

ranafakhreddine@hotmail.com (Rana Fakhereddine),

rami.haddad@usj.edu.lb (Rami El Haddad), Christian.Lecot@univ-savoie.fr (Christian
L´ecot)

Preprint submitted to Elsevier

March 22, 2016

5

the chain is so large that classical numerical methods would require a consider-

able computational time and huge memory capacity. So Monte Carlo (MC) sim-

ulation becomes the standard way of estimating performance measures for these

systems. A drawback of MC methods is their slow convergence, with respect to

the number of random points used. Various techniques have been developed, in

10

order to reduce the variance of the approximation, including stratiﬁed sampling

and Latin hypercube sampling [7, 6, 8].

It is shown in a series of papers [13, 12, 4, 5] that each step of a MC simulation

of a Markov chain amounts to approximating the measure of a subdomain of

the s-dimensional unit hypercube I s := [0, 1)s. The techniques presented here

15

use stratiﬁed samples for calculating this approximation.

Among stratiﬁcation strategies, we ﬁrst consider the simple approach (SMC):

the unit hypercube is divided into N subcubes having the same measure, and one

random point is chosen in each subcube. For Latin hypercube sampling (LHS),

the projections of the points on each coordinate axis are evenly distributed: one

20

projection in each of the N subintervals that uniformly divide the unit interval I.

Then we propose an hybrid method between SMC and LHS, that has properties

of both approaches, with one random point in each subcube and one projection

in each subinterval; we call this technique Sudoku Sampling (SS) due to the

properties of the points recalling a Sudoku grid.

25

The improved accuracy of stratiﬁed methods may be lost for problems in

which we have to approximate the measure of subdomains with irregular bound-

aries. It is necessary to take special measures to make optimal use of the greater

uniformity associated with stratiﬁed samples. This is achieved in [16, 17, 15]

through the additional eﬀort of reordering the copies of the chain at each time

30

step. This type of sorting was initiated by [10] in the context of quasi-Monte

Carlo (QMC) methods.

This paper is organized as follows. In Section 2 we present SMC, LHS and

SS methods for numerical integration. We recall variance bounds for SMC and

SS and we establish a new bound for the variance of LHS approach in the re-

35

strictive case of the approximate calculation of the measure of an interval in

2

dimension s. In Section 3, we propose a MC simulation of Markov chains us-

ing stratiﬁed samples in the context of discrete Markov chains with continuous

multi-dimensional state space. The results of numerical experiments are pre-

sented in Section 4. We compute the values of European and Asian options and

40

we compare the variance of the results and the eﬃciency of the approaches. It

is shown that both SMC and SS strategies outperform MC or LHS approaches.

Finally, we give some perspectives for future work.

2. Numerical integration

Let s ≥ 1 be a given dimension; then I s is the s-dimensional half-open
unit hypercube and λs denotes the s-dimensional Lebesgue measure. If g is a

45

square-integrable function deﬁned on I s, we want to approximate

g(x)dλs(x).

(1)

For the usual MC approximation, {U1, . . . , UN} are independent random vari-
ables uniformly distributed over I s. Then

(cid:90)

I s

I :=

(cid:88)

k

X :=

1
N

g(Uk)

(2)

is an unbiased estimator of I. When g = 1A, for some measurable A ⊂ I s, one
has

50

Var(X) =

1
N

λs(A)(1 − λs(A)) ≤ 1
4N

.

(3)

A simple stratiﬁed sampling (SMC) method was proposed in [9] and further

analyzed in [1]. For N = ns, put

(cid:20) (cid:96)i − 1

s(cid:89)

n

i=1

,

(cid:96)i
n

J(cid:96) :=

(cid:19)

1 ≤ (cid:96)1 ≤ n, . . . , 1 ≤ (cid:96)s ≤ n.

,

(4)

Let {V(cid:96) : 1 ≤ (cid:96)1 ≤ n, . . . , 1 ≤ (cid:96)s ≤ n} be independent random variables, with
V(cid:96) uniformly distributed over J(cid:96). Then

g(V(cid:96))

(5)

Y :=

1
N

(cid:88)

(cid:96)

3

55

is another unbiased estimator of I. In [2], we have analysed the following case:
we consider a function f : I

s−1 → I and we deﬁne

Af := {(u(cid:48), us) ∈ I s : us < f (u(cid:48))}.

Then for g = 1Af we obtain

Var(Y ) ≤

(cid:18) s − 1

4

V (f ) +

(cid:19)

1
2

1

N 1+1/s

,

(6)

(7)

if f is of bounded variation V (f ) in the sense of Hardy and Krause (we refer to

[19] for this concept).

Latin hypercube sampling (LHS) was introduced in [18] and examined stu-

diously in [21, 20]. Let

Ik :=

(cid:19)

(cid:20) k − 1

N

,

k
N

1 ≤ k ≤ N

,

(8)

60

65

70

1 , . . . , V i

N} be independent random variables, where V i

and {V i
k is uniformly
distributed over Ik. If {π1, . . . , πs} are independent random permutations of
{1, . . . , N} and Wk := (V 1
πs(k)), then each Wk is uniformly dis-
tributed over I s. Consequently,

π1(k), . . . , V s

Z :=

1
N

is another unbiased estimator of I.

(cid:88)

k

g(Wk)

(9)

We have proposed in [3] a combination of SMC and LHS: we construct
N = ns random points in I s such that in every interval I i−1 × Ik × I s−i (for
1 ≤ i ≤ s and 1 ≤ k ≤ N ) or J(cid:96) (for 1 ≤ (cid:96)1 ≤ n, . . . , 1 ≤ (cid:96)s ≤ n) lies only one
point of the set (property P). This is achieved as follows. If x := (x1, . . . , xs),
we put ˆxi := (x1, . . . , xi−1, xi+1, . . . , xs). Let σ1, . . . , σs be random bijections
(cid:96) : 1 ≤ i ≤ s, 1 ≤ (cid:96)1 ≤ n, . . . , 1 ≤ (cid:96)s ≤ n}
{1, . . . , n}s−1 → {1, . . . , ns−1} and {U i
be random variables uniformly distributed on I; all these variables are assumed

to be mutually independent. We put
σ1(ˆ(cid:96)1) − 1 + U 1

(cid:16) (cid:96)1 − 1

(cid:96)

W ∗

(cid:96) =

+

n

N

, . . . ,

(cid:96)s − 1

n

+

σs(ˆ(cid:96)s) − 1 + U s

(cid:96)

N

(cid:17)

.

(10)

4

75

The point set {W ∗
deﬁned by

(cid:96)

: 1 ≤ (cid:96)1 ≤ n, . . . , 1 ≤ (cid:96)s ≤ n} has property P. If Z∗ is

Z∗ :=

1
N

g(W ∗
(cid:96) ),

(11)

it is an unbiased estimator of I. The following variance bound is established in
[3]. Let A ⊂ I s be such that, for all i, with 1 ≤ i ≤ s,

(cid:88)

(cid:96)

A = {(u1, . . . , us) ∈ I s : ui < fi(ˆui)},

80

have

where fi are Lipschitz continuous functions I

Var(Z∗) ≤(cid:16) κ + 2

+ 2s(κ + 2)2(cid:17)

4

(12)
s−1 → I. Then, for g = 1A, we

1

N 1+1/s

,

(13)

where κ is a Lipschitz constant (for the maximum norm) for all the fi. We

prove a similar result for LHS, in a very restrictive case.

Theorem 1. If A is a subinterval of I s, then the variance of the LHS approx-

imation

satisﬁes (for N ≥ 3)

85

Z =

1
N

(cid:88)

k

1A(Wk)

Var(Z) ≤ 1
N

λs(A)(1 − λs(A)).

Proof. We have

Var(Z) =

(cid:88)

k

1
N 2

Var(1A(Wk)) +

1
N 2

(cid:88)
(cid:88)

k(cid:54)=k(cid:48)

=

1
N

λs(A)(1 − λs(A)) +

1
N 2

For j := (j1, . . . , js) with 1 ≤ j1 ≤ N, . . . , 1 ≤ js ≤ N , we set J j :=(cid:81)s

k(cid:54)=k(cid:48)

cov(1A(Wk), 1A(Wk(cid:48))).

i=1 Iji. If

cov(1A(Wk), 1A(Wk(cid:48)))

k (cid:54)= k(cid:48), then

cov(1A(Wk), 1A(Wk(cid:48))) =

(cid:88)

j1(cid:54)=j(cid:48)

1

N s

(N − 1)s
−(λs(A))2.

··· (cid:88)

js(cid:54)=j(cid:48)

s

λs(A ∩ J j)λs(A ∩ J j

(cid:48))

5

nh(nh + x−1

h + x+1

h − 1)

(cid:89)

i∈H c

xi
i (ni + x

−i
i

)

i − 1) + (ni + x−1

i

)x+1

i + (ni + x+1

i

)x−1

i

).

90

with 1 ≤ mi, ni ≤ N and x−1
for H ⊂ [1, s], denote H c := [1, s] \ H. Then

, x+1

i

(cid:88)

··· (cid:88)

j1(cid:54)=j(cid:48)

1

=

=

Hence

(cid:48))

s

(cid:88)

λs(A ∩ J j)λs(A ∩ J j
(cid:88)
(cid:89)
s(cid:89)

i=±1,i∈H c
(ni(ni + x−1

i + x+1

H⊂[1,s]

h∈H

js(cid:54)=j(cid:48)
1

N 2s

1

N 2s

(N 2(N − 1))scov(1A(Wk), 1A(Wk(cid:48)))

i=1

s(cid:89)
− s(cid:89)

i=1

We may assume that A is a closed interval:

A :=

s(cid:89)

i=1

(cid:104) mi − x−1
i − 1
N
i ∈ I. Let us note [1, s] := {1, 2, . . . , s} and,

mi + ni + x+1

i − 1

(cid:105)

N

,

,

=

N (ni(ni + x−1

i + x+1

i − 1) + (ni + x−1

i

)x+1

i + (ni + x+1

i

)x−1

i

)

(N − 1)(ni + x−1

i + x+1

i

)2.

i=1

Since

(N − 1)(ni + x−1

i

)2

i + x+1
−N (ni(ni + x−1
(N − 2)
x−1
1
i + x+1
2

(cid:16)

i + x+1

(cid:17)2

i − 1) + (ni + x−1
i − 2ni
N − 2

i
N
2

+

=

)x+1
i + (ni + x+1
i − x+1
(x−1

)2 +

i

i

)x−1

i

)

niN (N − 2 − ni)

N − 2

,

we obtain cov(1A(Wk), 1A(Wk(cid:48))) ≤ 0 and the result follows.

95

3. Simulation of Markov chains

In this section, we use the previous stratiﬁcation techniques for Markov

chains simulation.

6

3.1. Markov chain setting and Monte Carlo simulation

Let s ∈ N∗; we consider an homogeneous Markov chain {Xp, p ∈ N} with
state space E ⊂ Rs, evolving according to the stochastic recurrence: for p ≥ 0

100

Xp+1 = ϕp+1(Xp, Up+1).

(14)
Here {Up, p ≥ 1} is a sequence of i.i.d. uniform random variables over I d (for
d ∈ N∗) and each ϕp+1 : E × I d → E is a measurable map. The distribution P0
of X0 is known, and our aim is to approximate the distribution Pp of Xp. The

105

standard iterative Monte Carlo scheme proceeds as follows. A large number
k, 1 ≤ k ≤ N are drawn from the initial distribution P0; then
N of samples x0
we generate N sample paths of the chain as follows. For p ≥ 0 and for each
k ∈ {1, 2, . . . , N}

(15)
where {uk, 1 ≤ k ≤ N} are pseudo-random numbers simulating i.i.d. uniform
random variables over I d, independent from all variables introduced previously.

k, uk),

xp+1
k = ϕp+1(xp

110

QMC variants have been proposed to improve the accuracy of the method [16, 5].

The pseudo-random numbers uk are replaced with quasi-random numbers; in

order to beneﬁt from the great uniformity of quasi-random points, one possibility
is to sort the states xp

k by position in every step. Since QMC methods do not give
conﬁdence intervals, randomized QMC algorithms have also been introduced in

115

[16, 17, 15], with randomized quasi-random points. In the present paper, we

propose a scheme using the sampling strategies presented in section 2.

3.2. Stratiﬁed algorithm

Let M+(E) denote the set of all nonnegative measurable functions on E.

From (14), we obtain

(cid:90)

(cid:90)

(cid:90)

∀f ∈ M+(E)
(16)
Let n ≥ 2 be an integer and put N := ns+d. For each p ≥ 0, we are looking

f ◦ ϕp+1(x, u(cid:48)(cid:48))dPp(x)du(cid:48)(cid:48).

f (x)dPp+1(x) =

E

I d

E

120

for an approximation of Pp of the form

δ(x − xp
k),

(17)

(cid:98)Pp :=

1
N

(cid:88)

k

7

1, . . . , xp

N} is a subset of E to be determined. We ﬁrst sample
where Ξp := {xp
we have calculated a point set Ξp such that (cid:98)Pp approximates Pp, we compute
a point set Ξ0 of N states from the initial probability distribution P0. Once

125

Ξp+1 in two steps: we ﬁrst sort the states of Ξp according to their successive

coordinates, then we perform a numerical integration using a stratiﬁed sample.

Step 1: Relabeling the states. We label the states xp
m using a multi-dimensional
index m = (m1, . . . , ms) with 1 ≤ m1 ≤ n, . . . , 1 ≤ ms−1 ≤ n, 1 ≤ ms ≤ n1+d,
such that:
if m1 < m(cid:48)
if m1 = m(cid:48)
···

1 then xp
1, m2 < m(cid:48)

m,2 ≤ xp

m,1 ≤ xp

2 then xp

m(cid:48),2,

m(cid:48),1,

130

if m1 = m(cid:48)

1, . . . , ms−1 = m(cid:48)

s−1, ms < m(cid:48)

s then xp

m,s ≤ xp

m(cid:48),s.

135

In the case s = 1, this reduces to simply sort the states by increasing order.
If s ≥ 2, the N states are ﬁrst sorted in n batches of size N/n according to
their ﬁrst coordinates; then each batch is sorted in subgroups of n batches of

size N/n2 by order of the second coordinates, and so on. At the last step of

the sorting, subgroups of size nd+1 are ordered according to the last coordinate

of the state. This type of nested sorting was introduced in [11] for the QMC

140

simulation of the Boltzmann equation: since the algorithm is described by a

series of numerical integrations, the sorting tends to reduce the number of the

jumps of the integrand.

Step 2: Using stratiﬁed samples for transition. We deﬁne a probability measure

(cid:101)Pp+1 on E by replacing Pp with (cid:98)Pp in eq. 16:
(cid:90)

f (x)d(cid:101)Pp+1(x) :=

f ◦ ϕp+1(x, u(cid:48)(cid:48))d(cid:98)Pp(x)du(cid:48)(cid:48),

(cid:90)

(cid:90)

f ∈ M+(E).

(18)

E

I d

E

145

To obtain a uniform approximation of Pp+1, similar to (17), we use a quadrature
let {w(cid:96) : 1 ≤ (cid:96)1 ≤ n, . . . , 1 ≤ (cid:96)s+d ≤ n} be pseudo-
random numbers simulating stratiﬁed variables on I s+d as described in section

with stratiﬁed samples:

8

2, independent from all variables introduced previously. For m = (m1, . . . , ms)
with 1 ≤ m1 ≤ n, . . . , 1 ≤ ms−1 ≤ n, 1 ≤ ms ≤ n1+d, let 1m be the indicator
i=1 [(mi − 1)/n, mi/n) × [(ms − 1)/n1+d, ms/n1+d).

function of the interval (cid:81)s−1

For f ∈ M+(E), denote

(cid:88)

m

Then we have:

∀f ∈ M+(E)

We obtain (cid:98)Pp+1 by
(cid:90)

150

155

C pf (u) :=

1m(u(cid:48))f ◦ ϕp+1(xp

m, u(cid:48)(cid:48)),

u = (u(cid:48), u(cid:48)(cid:48)) ∈ I s × I d.

(19)

(cid:90)

E

f (x)d(cid:101)Pp+1(x) =

(cid:90)

I s+d

C pf (u)du.

(20)

f (x)d(cid:98)Pp+1(x) :=

E

(cid:88)

(cid:96)

1
N

C pf (w(cid:96)),

f ∈ M+(E).

(21)

:= (u1, . . . us) and u(cid:48)(cid:48)

The second step of the algorithm may be written as follows. For u ∈ I s+d
:= (us+1, . . . , us+d); for u(cid:48) ∈ I s, let m(u(cid:48)) :=
let u(cid:48)
(1 + (cid:98)nu1(cid:99), . . . , 1 + (cid:98)nus−1(cid:99), 1 + (cid:98)n1+dus(cid:99)). Then
(cid:96)), w(cid:48)(cid:48)
(cid:96) ).

(22)

xp+1
(cid:96) = ϕp+1(xp

m(w(cid:48)

(compare with eq. 15). Here the states are labeled using a multi-dimensional
index (cid:96) = ((cid:96)1, . . . , (cid:96)s+d) with 1 ≤ (cid:96)1 ≤ n, . . . , 1 ≤ (cid:96)s+d ≤ n. The ﬁrst s compo-
nents of w(cid:96) are used to select the state of the chain that perform a transition,

160

while the remaining d components are used to determine the new state.

4. Numerical illustrations

In this section, we compare the stratiﬁed strategies with the standard MC

scheme in numerical experiments

4.1. Pricing a European call option

165

In the Black-Scholes model and under the risk-neutral measure, the asset

price St at time t obeys the stochastic diﬀerential equation: dSt = rStdt +

9

MC LHS

SMC

SS

1.01

1.01

1.51

1.42

Table 1: European option: order α of the variance of the calculation of CE .

σStdBt, where r is the risk-free interest rate, σ the volatility parameter and B

is a standard Brownian motion. The solution of this equation is given by

St = S0 exp(cid:0)(r − σ2/2)t + σBt

(cid:1) .

(23)

170

Let T be the maturity date and K the strike price. We want to estimate the
value of the call option: CE = e−rT E[(ST − K)+]. To formulate the problem
as a Markov chain, we discretize the interval [0, T ] using observation times
0 = t0 < t1 < ··· < tP = T . The discrete version of (23) can be written as: for
p ≥ 0

Stp+1 = Stp exp(cid:0)(r − σ2/2)∆tp+1 + σ(Btp+1 − Btp )(cid:1) ,

(24)

where ∆tp+1 := tp+1 − tp.

175

In this example s = d = 1. We choose the following parameters: S0 = 100,
K = 90, r = 0.06, σ = 0.2, T = 1, P = 100 and ∆tp = T /P , for 1 ≤ p ≤ P .
We want to compare the variances of the MC, LHS, SMC and SS estimators

of CE . We replicate the calculation independently 100 times and we compute

the sample variance. Figure 1 shows the results as functions of N , for N =

180

102, 502, 1002, 1502, . . . , 10002, in log-log scale (base 2).

It is clear that SMC and SS produce smaller variances than MC and LHS

(for the same N ). When comparing the results of SMC and SS, we can see that

185

the later approach outperforms the former. At each step of the SS algorithm,
the mapping (cid:96) ∈ {1, . . . , n}2 → m(w(cid:48)
(cid:96)) ∈ {1, . . . , n2} is one-to-one, so that each
state is considered exactly once for a transition.

Assuming that Var = O(N−α), linear regression is used to evaluate α. The
outputs are listed in Table 1. The convergence rates are close to those estab-

lished for numerical integration in dimension 2.

10

Figure 1: European option. Sample variance of 100 copies of the calculation of CE as a
function of N . MC (+), LHS ((cid:52)), SMC ((cid:3)) and SS (∗) outputs, in log-log scale (base 2).

Since we use techniques that may reduce the variance at the expense of an

190

increase in computation time, we compare the eﬃciency of the approaches. The

eﬃciency as deﬁned in [14] is the inverse of the product of the variance by the

CPU time. It has the property that it is independent of the number N of states

for a naive MC estimator. The results are displayed in Figure 2 and show the

beneﬁts of both SMC and SS techniques.

195

4.2. Pricing an Asian option

We consider the pricing of an Asian option on a single asset. The asset price

St at time t satisﬁes (23) and the value of the call option with strike price K at

maturity date T is given by

CA = e−rT E(cid:104)(cid:16)(cid:16) P(cid:89)

p=1

(cid:17)1/P − K

(cid:17)

(cid:105)

,

+

Stp

(25)

where 0 = t0 < t1 < ··· < tP = T are discrete observation times. We deﬁne
a bi-dimensional Markov chain by: X0 := (S0, 1) and for 1 ≤ p ≤ P : Xp :=

200

11

68101214161820−25−20−15−10−505  MCLHSSMCSSFigure 2: European option: eﬃciency of 100 copies of the calculation of CE as a function of
N . Comparison of MC (+), LHS ((cid:52)), SMC ((cid:3)) and SS (∗) outputs, in log-log scale (base 2).

MC LHS

SMC

SS

0.99

1.12

1.40

1.33

(Stp , ((cid:81)p

Table 2: Asian option: order β of the variance of the calculation of CA.

q=1 Stq )1/p), with Stp given by (24). Here s = 2 and d = 1. We choose:
S0 = 100, K = 90, r = log10(1.09), σ = 0.2, T = 240/365, P = 10 and
∆tp = T /P , for 1 ≤ p ≤ P .

We compute the sample variance of 100 independent calculations of CA

205

by MC, LHS, SMC and SS methods. The variances as functions of N , for
N = (5m)3, 1 ≤ m ≤ 20, are plotted in Figure 3.

The order β of the variance is estimated using linear regression and the

results are given in Table 2. The convergence rates are not far from those

proved for numerical integration in dimension 3.

210

As before, SMC and SS stratiﬁcation techniques give smaller variances and

12

68101214161820−2024681012  MCLHSSMCSSFigure 3: Asian option. Sample variance of 100 copies of the calculation of CA as a function
of N . MC (+), LHS ((cid:52)), SMC ((cid:3)) and SS (∗) outputs, in log-log scale (base 2).

better convergence rates. But the advantage of the SS algorithm compared
to the SMC is lost. At each step of the SS algorithm, the mapping (cid:96) ∈
{1, . . . , n}3 → m(w(cid:48)
(cid:96)) ∈ {1, . . . , n} × {1, . . . , n2} is not necessarily one-to-one.
The eﬃciencies of the four methods are reported in Figure 4. SMC and SS

215

calculations give similar results and outperform MC and LHS outputs.

5. Conclusion

We have proposed upper bounds for the variance, when we approximate the

integral of an indicator function of a subdomain of I s with stratiﬁed Monte

Carlo techniques. We have proposed strategies for simulating Markov chains

220

using stratiﬁed samples and we have shown on examples that this approach

could lead to better eﬃciency than naive Monte Carlo simulation.

The variance bound of the LHS approximation is obtained in a very restric-

tive case and should be extended to less speciﬁc subdomains of I s. The analysis

13

68101214161820−25−20−15−10−50  MCLHSSMCSSFigure 4: Asian option: eﬃciency of 100 copies of the calculation of CA as a function of N .
Comparison of MC (+), LHS ((cid:52)), SMC ((cid:3)) and SS (∗) outputs, in log-log scale (base 2).

of stratiﬁed simulation of Markov chains remains undone and will be the subject

225

of future work.

References

References

[1] R. C. H. Cheng, T. Davenport, The problem of dimensionality in stratiﬁed

sampling, Management Science 35 (1989) 1278–1296.

230

[2] R. El-Haddad, R. Fakhereddine, C. L´ecot, Stratiﬁed Monte Carlo inte-

gration, in: K. K. Sabelfeld, I. Dimov (Eds.), Monte Carlo Methods and

Applications, De Gruyter, Berlin, 2013, pp. 105–113.

[3] R. El-Haddad, R. Fakhereddine, C. L´ecot, G. Venkiteswaran, Extended

Latin hypercube sampling for integration and simulation, in: J. Dick, F.Y.

14

681012141618203456789101112  MCLHSSMCSS235

Kuo, G.W. Peters, I.H. Sloan (Eds.), Monte Carlo and Quasi-Monte Carlo

Methods 2012, Springer, Berlin, 2014, pp. 317–330.

[4] R. El-Haddad, C. L´ecot, P. L’Ecuyer, Quasi-Monte Carlo simulation of

discrete-time Markov chains on multidimensional state spaces,

in: A.

Keller , S. Heinrich, H. Niederreiter (Eds.), Monte Carlo and Quasi-Monte

240

Carlo Methods 2006, Springer, Berlin, 2008, pp. 413–429.

[5] R. El-Haddad, C. L´ecot, P. L’Ecuyer, N. Nassif, Quasi-Monte Carlo meth-

ods for Markov chains with continuous multi-dimensional state space,

Mathematics and Computers in Simulation 81, (2010), 560–567.

[6] M. Evans, T. Swartz, Approximating Integrals via Monte Carlo and De-

245

terministic Methods, Oxford University Press, Oxford, 2000.

[7] G. S. Fishman, Monte Carlo, Springer, New York, 1996.

[8] P. Glasserman, Monte Carlo Methods in Financial Engineering, Springer,

New York, 2004.

[9] S. Haber, A modiﬁed Monte-Carlo quadrature, Mathematics of Compu-

250

tation 20 (1966) 361–368.

[10] C. L´ecot, A Direct Simulation Monte Carlo scheme and uniformly dis-

tributed sequences for solving the Boltzmann equation, Computing 41

(1989) 41–57.

[11] C. L´ecot, I. Coulibaly, A quasi-Monte Carlo scheme using nets for a linear

255

Boltzmann equation, SIAM Journal on Numerical Analysis 35 (1998) 51–

70.

[12] C. L´ecot, B. Tuﬃn, Comparison of quasi-Monte Carlo-based methods for

the simulation of Markov chains, Monte Carlo Methods and Applications

10 (2004) 377–384.

260

[13] C. L´ecot, B. Tuﬃn, Quasi-Monte Carlo methods for estimating transient

measures of discrete time Markov chains, in: H. Niederreiter (Ed.), Monte

15

Carlo and Quasi-Monte Carlo Methods 2002, Springer, Berlin, 2004, pp.

329–343.

[14] P. L’Ecuyer, Eﬃciency improvement and variance reduction, in: J. D.

265

Tew, S. Manivannan, D. A. Sadowski, A. F. Seila (Eds.), Proceedings of

the 1994 Winter Simulation Conference, IEEE Press, 1994, pp. 122–132.

[15] P. L’Ecuyer, C. L´ecot, A. L’Archevˆeque-Gaudet, On array-RQMC for

Markov chains: mapping alternatives and convergence rates,

in: P.

L’Ecuyer, A. B. Owen (Eds.), Monte Carlo and Quasi-Monte Carlo Meth-

270

ods 2008, Springer, Berlin, 2009, pp. 485–500.

[16] P. L’Ecuyer, C. L´ecot, B. Tuﬃn, Randomized quasi-Monte Carlo simu-

lation of Markov chains with an ordered state space, in: H. Niederre-

iter, D. Talay (Eds.) Monte Carlo and Quasi-Monte Carlo Methods 2004,

Springer, Berlin, 2006, pp. 331–342.

275

[17] P. L’Ecuyer, C. L´ecot, B. Tuﬃn, A randomized quasi-Monte Carlo simu-

lation method for Markov chains, Operations Research 56 (2008) 958–975.

[18] M. D. McKay, R. J. Beckman, W. J. Conover, A comparison of three

methods for selecting values of input variables in the analysis of output

from a computer code, Technometrics 21 (1979) 239–245.

280

[19] H. Niederreiter, Random Number Generation and Quasi-Monte Carlo

Methods, SIAM, Philadelphia, Pennsylvania, 1992.

[20] A. B. Owen, Monte Carlo variance of scrambled net quadrature, SIAM

Journal on Numerical Analysis 34 (1997) 1884–1910.

[21] M. Stein, Large sample properties of simulations using Latin hypercube

285

sampling, Technometrics 29 (1987) 143–151.

16

