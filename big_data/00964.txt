Learning Tabletop Object Manipulation by Imitation

1

Electrical Engineering and Computer Science, University of Michigan

Zhen Zeng, Benjamin Kuipers

Ann Arbor, MI 48109

zengzhen@umich.edu, kuipers@umich.edu

6
1
0
2

 
r
a

M
3

 

 
 
]

O
R
.
s
c
[
 
 

1
v
4
6
9
0
0

.

3
0
6
1
:
v
i
X
r
a

Abstract—We aim to enable robot to learn tabletop object
manipulation by imitation. Given external observations of demon-
strations on object manipulations, we believe that two underlying
problems to address in learning by imitation is 1) segment a
given demonstration into skills that can be individually learned
and reused, and 2) formulate the correct RL (Reinforcement
Learning) problem that only considers the relevant aspects of
each skill so that the policy for each skill can be effectively
learned. Previous works made certain progress in this direction,
but none has taken private information into account. The public
information is the information that is available in the external
observations of demonstration, and the private information is
the information that are only available to the agent that executes
the actions, such as tactile sensations. Our contribution is that
we provide a method for the robot to automatically segment the
demonstration into multiple skills, and formulate the correct RL
problem for each skill, and automatically decide whether the
private information is an important aspect of each skill based on
interaction with the world. Our motivating example is for a real
robot to play the shape sorter game by imitating other’s behavior,
and we will show the results in a simulated 2D environment that
captures the important properties of the shape sorter game. The
evaluation is based on whether the demonstration is reasonably
segmented, and whether the correct RL problems are formulated.
In the end, we will show that robot can imitate the demonstrated
behavior based on learned policies.

I. INTRODUCTION

When a robot is presented with an unfamiliar object, the
robot is not aware of what actions can cause what changes to
the state of the object. Learning by imitation is an effective
way for a robot
to gain knowledge about possible useful
actions on objects in its environment.

Given an observed behavior, usually formally deﬁned as
state variables describing the observed environment and action
variables describing the observed actions, the robot should be
able to learn a policy, which is a mapping between states and
actions, such that it can select an action to execute based on
its current state. RL methods are popular for policy learning,
and the robot can improve its performance overtime through
interaction with the world, thus we use RL methods in our
work.

To learn a policy, we should ﬁrst deﬁne a RL problem,
usually formulated as an MDP (Markov Decision Process)
with following components: the reward function, the action
space, and the state space. The importance of formulating the
correct RL problem, i.e., correctly deﬁning each component is
as explained below.

What is the importance of correctly deﬁning the reward
function? For a robot to learn to effectively manipulate an

unfamiliar object by imitating other’s behavior, it is important
for the robot to understand what to imitate, i.e., the goal of the
behavior. For robotic experts, they can directly hand code the
corresponding criteria to deﬁne the goal for the robot, but it is
not realistic for most of the consumers of commercial robots
to do so.

Thus it is important for the robot to automatically capture
the goal of an observed behavior by deﬁning the correct reward
function. For example, if the goal of an observed behavior is to
reach and grasp a cup, then the reward function can be deﬁned
as -1 everywhere except for a big positive reward when then
cup is being grasped.

What is the importance of correctly deﬁning the state and
action space? Think of the way we act in our daily lives, when
we manipulate some objects of interest, we don’t pay much
attention to other objects in the environment. For example, if
there are two blocks and a cup on a table, when we are stacking
the two blocks, we don’t really care about where the cup is;
on the other hand, when we try to put one of the block into the
cup, we don’t care where the other block is. This reﬂects that
even we are accessible to a bunch of information, we always
abstract out the most important information relevant to our
task by hand, and this is formally referred as abstraction in
machine learning. RL problems that involve high-dimensional,
continuous state and action space are difﬁcult to solve, thus
abstraction is a key to reduce the dimension of the problems.
With the correct abstraction, the appropriate state and action
space is deﬁned for the RL problems.

In our work, we aim to ﬁnd the abstraction that is able
to 1) capture the important aspects of the behavior, and 2)
chooses the correct reference frame (i.e., decides the target
object) and relevant objects. As in our experiments, if the
observed behavior is hand reaching for a block with a basket
as a noise object in the environment, then the block is the
target object, the robot should be able to decide to abstract
out the distance and angle of the hand in the block frame as
the state space, and further RL methods can be applied with
that state space.

if

What

there are multiple RL problems involved? The
observed behavior can underlie multiple policies, i.e., there are
several skills involved in the behavior. For example, picking
up a block, and then stacking it onto another one are two
different skills. When the observed behavior contains multiple
skills, the robot should be able to automatically segment the
behavior into multiple pieces, each corresponds to a skill,
and formulate different RL problems for each of them. The
segmentation can be done based on how likely a temporal

piece of the observed behavior is following the same policy,
and this will be explained more in detail in later sections.

What if there are important private information not acces-
sible in the observed behavior? Given an externally observed
behavior, private information such as tactile sensations are not
available to the robot. If human demonstrates to rotate a jar’s
lid to open it, the observed behavior will be the hand rotating
together with the jar’s lid until it is open. With lack of private
information, the robot cannot decide whether it is important or
not by just observing the behavior. Once the robot understands
the goal of the behavior, and selects the correct abstraction,
i.e. the angles of the hand and lid w.r.t the frame of the jar’s
body in this case, it can use appropriate RL methods to learn
a policy by temporarily assuming that the private information
does not matter. As the robot is just grasping the jar’s lid
without controlling the holding force, the robot could fail to
learn a policy to reliably open the jar’s lid. Then the robot will
propose to expand the state space in the original RL problem
to include the private information, i.e., tactile sensation, and
expand the action space to include the controller on holding
force, then restart the learning based on the not-so-good policy
learned previously.

it

is important

In sum, given an observed behavior,

to
segment it into multiple pieces (if there are multiple skills
involved), and formulate the correct RL problem for each
piece such that RL methods can be further applied to learn
policies. One of the important step made in this direction is
by Konidaris et al. [1], they provided an elegant approach for
behavior segmentation, and abstraction selection for formulat-
ing the RL problems. We use a similar approach, and we have
contributed in two new directions:

• We provide a method to online select the appropriate
abstraction such that 1) the observed behavior is captured,
and 2) the correct reference frame and relevant objects are
chosen.
• When private information is not available in the observed
behavior, the robot is able to decide whether the private
information is important, and reformulate the RL prob-
lems if needed.

Next, section II will discuss some related works, section
III, IV introduce notations and terminologies used in this
paper, and then section V explains the methods involved in
our framework, and section VI discuss our experiment setup,
and the evaluation results.

II. RELATED WORK
A. Understanding Manipulation Behavior

A potentially useful manipulation behavior representation
for artiﬁcial agents should satisfy several criteria. As pointed
out in [2], the representation needs to be based on sensory
signals and learnable by observation. From the point of view
of learning by imitation, the representation should also satisfy
that (1) it is not redundant in the sense that it should not encode
information that exists only within certain observed behaviors,
such as speciﬁc motion trajectories of human arm joints and
objects; (2) it should be simple such that it can be easily inter-
preted as a roadmap that guide an agent to act. Previous works

on human manipulation actions recognition have attempted
to represent manipulation behavior in a probabilistic manner.
Human poses, human-object context, and object-object context
have been considered to solve the problem jointly as in [3].
Similarily in works by Kjellstrom et al.[4], pre-deﬁned hand-
object features and manipulation features are extracted, and the
semantic manipulation action-object dependencies are learned
based on CRFs. Their representation of manipulation behavior
is powerful in that they can recognize manipulation actions as
well as object categories. Although previous works are robust
in manipulation action recognition when presented by various
view points and even occlusions, those representations of the
behavior cannot be directly translated into any step by step
guidance, or a roadmap that an agent can follow for effective
imitation. Works by Aksoy’s group [2] revealed an effective
way to represent manipulation behavior. By observing different
human demonstrations of the same manipulation behavior,
they discovered that the manipulator, i.e. hand, and objects
movement trajectory may vary, but there are certain moments
that the spatial relations between the hand and objects are
similar or identical across all the demonstrations, and these
moments are referred as decisive moments. They introduced
Semantic Event Chain (SEC) as a novel, generic representation
of manipulation behavior, where they encoded the spatial
relations between the manipulator and objects only at decisive
moments.

B. Learning by Imitation

Some previous works learn movements by imitating joint
trajectories [5][6][7]. They mounted sensors on human body,
and recorded the joint angles during the demonstrations to
teach humanoid robot drumming and walking patterns. While
in our work, the robot needs to learn a manipulation task,
directly imitating exact joints trajectories for manipulation
tasks won’t generalize well. Two completely different se-
quences of joints trajectories can be performing the same
manipulation task. For example, when we pick up an object,
we can approach it from various angles and along various
joint trajectories, but all these trajectories correspond the same
manipulation task. Thus what really matters in the learning of
a manipulation task is to capture and imitate the important
aspects of the behavior rather than imitating the quantitative
joints trajectories. For example,
the important aspects of
picking up an object are the hand needs to approach and get
in contact with the object ﬁrst, then grasp it to lift it up,
no matter what the joints trajectories are. Another problem
of imitating the joints trajectories is the corresponding issue.
The corresponding issue is understood as the identiﬁcation
of a mapping from the demonstrator to the robot that allows
the transfer of information. For example, in the case of robot
learning to walk by observing human joint angles during
demonstrations, before it can imitate the walking pattern, the
robot needs to know the mapping between the human joint
angles and its own joint angles.

There have been some works that directly learn the mapping
from state to action. In robot domain, the state space is usually
continuous, and the action space can be continuous or discrete.

Continuous action space can be composed by available sensor
values, such as moving speed, joint angular speed, and torque.
Discrete action space can be composed by discrete low level
incremental actions, such as move forward by 0.1 meter for
a mobile robot, or composed by discrete high level primitive
actions, such as reach, grasp an object for a manipulator.

When the action space is discrete, the problem of learning
the mapping is essentially a classiﬁcation problem. For exam-
ple, Chernova et al. [9][10] learns to navigate through corridors
by observing the behavior generated by expert teleoperation.
In their work, the states are continuous variables describing
the distances of the closets walls, and the actions are discrete
variables corresponding to pre-deﬁned controllers that drives
the robot forward, to the left, to the right, and u-turn. The
mapping from state to action is learned based on GMM. Sim-
ilar works have been focused on high-level primitive actions
such as hand gestures, for learning box and ball sorting tasks
[11][12]. When the action space is continuous, the problem of
learning the mapping is then a regression problem. Grollman
et al. [13] has applied locally weighted projection regression
to soccer skill learning task on an AIBO robot.

Our work focus on automatically formulating RL problems
for the skills observed in demonstration, and solving the RL
problems result in good policies that can generate behaviors
that imitate the observed demonstration. The most similar
work is by Konidaris et al. [1]. There are also some works
that focus on learning the correspondence, i.e., the mapping
between the observed states/actions and the robot state/actions.
In our work, we assume that the correspondence is known. For
more details on works that solves the correspondence issue,
please refer to this survey[8].

III. NOTATIONS

First of all, S is the overall state space, and it divides into the
overall public state space Spublic and the overall private state
space Sprivate. In Spublic, each dimension is a public state
variable whose value can be externally observed, such as the
x coordinate of an object center, and a boolean state variable
indicating whether the hand is open or not. In Sprivate, each
dimension is a private state variable whose value cannot be
externally observed, such as tactile sensations on the ﬁngers.
The robot observes a sequence of states sampled at the

frame rate,

O = {s0, s1,··· , sf inal}

where si ∈ Spublic. More speciﬁcally, si is a vector composed
of the pose vectors of objects in workspace, and the pose
vector of hand,

si = [P o1
i

, P o2
i

,··· , P h
i ]T

where P oj
is the pose vector of the jth visible object at ith
i
sampled frame, and P h
is the pose vector of the hand at ith
i
sampled frame. All the pose vectors in the observations are
in the world frame, they can be transformed into any object
frame or the hand frame given the object or hand pose.
Robot action space A is composed by hand movements
(translation and rotation in 3D), open and close gripper (with a
default force), open and close gripper with commanded force.

Given the observation, we deﬁne the action ai ∈ A taken at
state si to be

ai = [d(P h

i , P h

i+1), open(α)]T

and P h

where function d returns the displacement between the hand
poses P h
i+1 and open(α) deﬁnes the opening or
i
closing grippers with force α (positive for opening, negative
for closing, and zero for doing nothing). Note that the force
α in action open(α) is not externally observable,
thus α
should be learned by robot explorations if that is considered
an important aspect of the demonstrated behavior.

Demonstrations on tabletop object manipulations can in-
volve multiple skills, for example, grasp and pick up a cup,
the series of actions can be segmented into three individual
skills: ﬁrst, approach the cup; second, grasp the cup; third,
lift the cup. Each skill is formally deﬁned as an option o, as
introduced in [14]. An option includes three components: 1) an
option policy πo(s, a) which gives the probability of executing
each action in each state in which the option is deﬁned; 2)
an initiation set indicator function Io(s) which gives 1 for
states where the option can be executed and 0 elsewhere;
3) termination condition βo(s) which gives the probability of
option execution terminating in states where it is deﬁned.

When a demonstration is segmented into a sequence of
skills, and each skill is represented as an option, the reward
can be synthesized as negative at each step of action and
positive when the option terminates. In this way, the goal
of each skill is embedded in the termination condition, and
can be captured by the synthesized rewards, as a simple
instance of inverse reinforcement
learning methods. These
synthesized rewards are reasonable since many tabletop object
manipulations involve skills with its own ending goal. If the
real reward function is more complex than that, other inverse
reinforcement learning methods [15] can be applied to infer
the reward function from the demonstration.

The advantage of breaking the demonstration into a se-
quence of skills and learn the corresponding option is that:
a) learned options can be reused in other tasks; b) to learn the
option policy for each option, RL problem can be formulated
with the state and action space composed by the most relevant
state variables (can contain both public and private state
variables) and action variables, instead of the overall state and
action space.
M is a pair of functions (cid:104)σM , τM(cid:105), where

We design a library of abstractions, where each abstraction

σM : S (cid:55)→ SM

is a state abstraction that maps the overall state space S to a
smaller state space SM , and

τM : A (cid:55)→ AM

is an action abstraction that maps the full action space A to
a smaller action space AM . In our work, the abstracted space
SM and AM are subset of S and A, with variables possibly
described in a different reference frame rather than the world
frame.

To formulate the correct RL problem for each segmented
skill, the pair of state and action abstraction M that involves
the more important aspects of the skill should be selected.

IV. LINEAR VALUE FUNCTION APPROXIMATION

The value function V maps a given state vector to expected
return, and it can be approximated as a linear combination of
a given set of basis functions Φ = {φ1,··· , φk},

k(cid:88)

ˆV (s) =

wiφi(s)

i=1

where the basis function are Fourier basis, a generic basis
that generally exhibits good performance [16]. The zth order
Fourier basis for d state variables in the set of basis function
are deﬁned as

φi(s) = cos(πci · s)

where ci = [c1,··· , cd], cj ∈ [0,··· , z] with 1 ≤ j ≤ d. The
set of basis functions is obtained by systematically varying the
coefﬁcients cj.

From the synthesized rewards we can obtain a Monte Carlo

sample of expected return from each appeared state s,

n(cid:88)

R(s) =

γiri

i=0

where γ is the discount factor, and ri is the synthesized reward.
And R(s) is the regression target for ˆV (s) when we are trying
to approximate the value function of the underlying policy in
the demonstration, with a given set of Fourier basis functions.
Each abstraction M in the abstraction library has an asso-
ciated set of Fourier basis functions ΦM deﬁned over SM .
Therefore, abstraction selection amounts to selecting the set
of basis functions that can best represent the value function
inferred from the demonstration.

V. METHOD

We want to break the demonstration into multiple skills
when it consist of underlying policies that use different
abstractions, or it consists of policies that are too complex
to be approximated using a single function approximator.
Changepoint detection algorithm can be applied in this case to
ﬁnd the boundaries of each skill and select the best abstraction
for each skill.

A. Changepoint Detection

First, I’ll introduce the statistical changepoint detection in
a general regression setting. Given observed data and a set of
candidate models Q, we assume that the data are sequentially
generated by an instance of a single model, occasionally
switching to a different model or switching to a different
instance of the same model at certain points in time, called
changepoints. The goal is to infer the number and positions
of the changepoints and select an appropriate model instance
for each segment.

An efﬁcient changepoint detection algorithm was introduced
by Fearnhead and Liu [17] that obtains the MAP changepoints

Figure 1. HMM for changepoint detection [1]. Transitions occur when the
model changes, either to a new model or the same model with different
parameters.

G(l) =(cid:80)

and models via an online Viterbi algorithm: given data tuples
(xt, yt) observed for times t ∈ [1, 2,··· , T ], and a set of
candidate models Q with prior p(q) for each q ∈ Q. The
marginal probability of a segment length l is modeled with
probability mass function g(l) and cumulative mass function
i = 1lg(l). And a segment from time j + 1 to t can
be ﬁt using model q to obtain P (j, t, q), the probability of the
data segment conditioned on q. Functions g(l) and P (j, t, q)
is either given as a prior knowledge or pre-learned based on
some training data.

This results in a HMM where the hidden state at time t is
the model qt and the observed data is yt given xt, as shown
in Figure 1. The transition from model qi to qj occurs with
probability

T (qi, qj) = g(j − i − l)p(qj)

The emission probability of an observed data segment
starting at time t + 1 and continuing through j using model q
is given by

P (yi+1 : y|q) = P (i, j, q)(1 − G(j − i − 1))

An online Viterbi algorithm can be used to compute
Pt(j, q), the probability of the changepoint previous to time t
occurring at time j using model q (i.e., from time j + 1 to t
the data is generated using model q) is

Pt(j, q) = (1 − G(t − j − 1))P (i, j, q)p(q)P M AP

j

where P M AP

is the probability of the MAP changepoint at

j

time j,

P M AP

j

= maxi,q

Pj(i, q)g(j − i)
1 − G(j − i − 1)

Thus at each time t, the algorithm computes Pt(j, q) for
) and
each model q and changepoint time j < t (using P M AP
then computes and stores P M AP
being recursively
calculated, the MAP changepoint positions and models are
stored. When P M AP
is calculated for the complete observed
data sequence, the MAP changepoint positions and models for
generating the observed data are identiﬁed as a result.

. As P M AP

T

j

t

t

Figure 2. Overview of our approach.

B. Demonstration Segmentation and Abstraction Selection

Our framework for simultaneous demonstration segmenta-
tion and abstraction selection is as shown in Figure 2. We
apply the changepoint detection algorithm to segment
the
demonstration at time points where the abstraction changes
and select the appropriate abstraction for each segment such
that, 1) the observed behavior can be effectively captured by
the basis functions associated with the selected abstraction;
2) the observed trajectory in the state space is simple. By
default, only public state and action variables are considered,
if the RL problem formulated with the abstracted public state
and action space cannot reproduce the observed behavior, we
will re-formulate the RL problem by adding in private state
and action variables.

The set of candidate models Q is composed by the sets of
basis functions ΦM associated with each abstraction M, and
R(st) at time t is the target variable yt. For the changepoint
detection algorithm to work well, an appropriate model of
expected segment length and an appropriate model for ﬁtting
the data are required. As introduced by Konidaris et al. [1], it is
suggested to assume a geometric distribution for skill lengths
with parameter p, so that

g(l) = (1 − p)l−1p
G(l) = 1 − (1 − p)l

and this provides a natural way to set p via k = 1/p, the
expected skill length.

A linear regression model with Gaussian noise is assumed
to be the model of observed data, and the Gaussian noise
prior has mean zero and an inverse gamma variance prior with
parameters v/2 and u/2. The prior for each weight is a zero-
mean Gaussian with variance σ2δ. The probability of the data
segment from time j + 1 to t conditioned on abstraction q is

P (j, t, q) = PV (j, t, q)Ptraj(j, t, q)

where PV (j, t, q) measures how well can the basis functions
associated with model q approximate the value function V , and

t(cid:88)

Aq =

t(cid:88)

Φq(si)Φq(si)T

i=k+1

i − bT
R2

q (Aq + D)−1bq

i=j

1
t − j

,

1

(t − j)2 ]T [1,

1
t − j

,

1

(t − j)2 ]

A =

[1,

yq =

t(cid:88)
t(cid:88)

i=k+1

N(cid:89)

Ptraj(j, t, q) measures how simple the observed trajectory is
in the abstracted state space,

PV (j, t, q) =

2

π n
δm/2

|(Aq + D)−1| 1

2

2

u v
(yq + u)

n+v

2

Γ( n+v
2 )
Γ( v
2 )

2

2

2

k=1

u v

π n
δm/2

|(A+D)−1| 1

Γ( n+v
2 )
Ptraj(j, t, q) =
Γ( v
2 )
where n = t− j− 1, q has m basis functions, Γ is the Gamma
function, D is an m× m matrix with δ−1 on the diagonal and
zeros elsewhere, N is the number of relevant object indicated
by the selected state abstraction. And

(yok + u)

n+v

2

yok =

dok (si)2 − bok

T (Atraj + D)−1bok

i=j

with model q evaluated at state si, Ri = (cid:80)t
bq = (cid:80)t

where Φq(si) is a vector of m basis functions associated
j=i γj−irj is
the accumulated reward sample obtained from state si, and
(t−j)2 ]T ,
dok (si) is the distance between the end effector of the agent
and selected relevant object ok.

i=j RiΦq(si), bok = (cid:80)t

i=j dok (si)[1, 1

t−j ,

Using the probability functions deﬁned above, the change-
point detection algorithm can segment
the demonstration
into multiple skills with selected abstraction. Once the RL
problem is formulated for each skill, an initial policy can
be learned based on the demonstration and the robot can
improve upon the initial policy using appropriate RL methods.
In our experiments, we use Sarsa(λ) learning as our RL
method, and if the policy learned after maximum number

1

(a)

(b)

Figure 3. Shape Sorter.
of trials cannot successfully reproduce the observed behavior,
our framework handles that by automatically reformulate RL
problem by adding in private state and action variables, e.g.
tactile sensation state variable and gripping opening/closing
action in our experiment.

VI. EXPERIMENT

A. Shape Sorter Game

Our motivating example is the shape sorter game, as shown
in Figure 3(a), the goal is to pick up a block and align it with
the corresponding shape hole and push it into the box. Now
let’s assume there are only the box and the green block within
the visible ﬁeld, as shown in Figure 3(b), and the demonstrator
demonstrates the inserting the green block into the box.

The overall public state space will contain the pose of the
block P block, the box P box and the hand P hand in a world
frame. The overall private state space will contain the private
tactile sensation on the left and the right ﬁngers λl, λr. The
action space is as introduced earlier. There are three coordinate
frames available in this case, i.e., the world frame, the block
frame, and the box frame.

B. The Abstraction Library

We aim to design an abstraction library that is sufﬁcient
for the tabletop object manipulations such as assembly tasks,
such that the proper abstraction exists for any tabletop object
manipulation skill that the robot may decide to learn.
We deﬁne the mapping functions (cid:104)σM , τM(cid:105) of an abstrac-
tion M by two sets of parameters (cid:104)Pσ, Pτ(cid:105):

Pσ = [IL, IO, IΛ, Io1, Io2,··· , Ij,C]

Pτ = [IT , IR, IF ]

where each parameter is a binary digit. In Pτ , if IT =1, then
hand translation action is included in the abstracted action
space AM ; if IT = 0, then it is not included in AM . Similarly,
IR indicates whether hand rotation action is included in AM ,
and IΛ indicates whether opening/closing with commanded
force is included in AM . In Pσ, IL, IO, IΛ indicate whether
location variables, orientation variables, and tactile sensations
are included in SM , and Io1 , Io2,··· , Ih indicate whether
information of objects o1, o2,··· and hand h are included
in SM , and C indicates the reference frame in which the
information should be represented. For example, Pσ = [IL =

Figure 4. Toy example.
1, IO = 0, IΛ = 0, Iblock = 1, Ibox = 0, Ih = 1,C = Cbox]
indicates that σM maps S to SM , where the state variables are
locations of the block and hand expressed in the box frame.
Note that when only hand translation action is included
in A, i.e., Pτ = [1, 0, 0], the state variables that the action
can affect are the location variables, thus it is reasonable to
constrain SM to include only location variables in this case,
i.e., IL, IO, IΛ = [1, 0, 0]. And the same principle applies to
Pτ = [0, 1, 0], Pτ = [0, 0, 1], Pτ = [1, 1, 0] and so on. As a
result,

[Il, IO,Λ ] = [IT , IR, IF ]

(1)

is a rule of thumb for designing the abstraction library.

By enumerating all possible pairs of parameters (cid:104)Pσ, Pτ(cid:105)
under the constraint 1, the abstraction library is built with
the resulting mapping functions (cid:104)σM , τM(cid:105). The size of the
abstraction library is 2|P|σ−1K, where K is the number of
available coordinate frames.

C. Simpliﬁed 2D Manipulation Domain

Our toy example that captures the important properties in
our motivating example is a 2D manipulation domain, where a
robot, block, and a basket is involved. The observed behavior
is the hand (or robot) reaches the block ﬁrst and then translates
it into the basket, as shown in Figure 4.

D. Evaluation

Given the external observations of a demonstration as
shown in Figure 5,6, and assuming reasonable perception
errors, the evaluation is mostly concerned of 1) whether the
demonstration is reasonably segmented into multiple skills,
and 2) whether the correct RL problem is formulated for each
segmented skill, including whether the private information
is considered in the RL formulation when it
is actually
important. Videos of the robot
learning to reproduce the
observed behavior are available at https://www.dropbox.com/
s/eltjxl0solevnw0/QUAL2_ZHEN_slides.pptx?dl=0.

Given two different sample trajectories, the segmentation
results were successful. And the selected abstraction is also
correct. In both experiments, for the 1st segment, the abstrac-
tion is selected as the robot distance and angle w.r.t the block
frame; for the 2nd segment, the abstraction is selected as robot
and block distance and angle w.r.t the basket frame.

And the RL problem initially formulated for the 2nd seg-
ment only involves public state and action variables, which is
incorrect, since to carry the block to the destination basket,
the robot needs to incorporate holding action. Thus the robot
failed to reproduce the observed behavior in the 2nd segment,

[6] J. Nakanishi, J. Morimoto, G. Endo, G. Cheng, S. Schaal, and
M. Kawato, “Learning from demonstration and adaptation of biped
locomotion,” Robotics and Autonomous Systems, vol. 47, no. 2, pp. 79–
91, 2004.

[7] A. J. Ijspeert, J. Nakanishi, and S. Schaal, “Movement imitation with
nonlinear dynamical systems in humanoid robots,” in Robotics and Au-
tomation, 2002. Proceedings. ICRA’02. IEEE International Conference
on, vol. 2, pp. 1398–1403, IEEE, 2002.

[8] B. D. Argall, S. Chernova, M. Veloso, and B. Browning, “A survey of
robot learning from demonstration,” Robotics and autonomous systems,
vol. 57, no. 5, pp. 469–483, 2009.

[9] S. Chernova and M. Veloso, “Conﬁdence-based policy learning from
demonstration using gaussian mixture models,” in Proceedings of the
6th international joint conference on Autonomous agents and multiagent
systems, p. 233, ACM, 2007.

[10] S. Chernova and M. Veloso, “Interactive policy learning through
conﬁdence-based autonomy,” Journal of Artiﬁcial Intelligence Research,
vol. 34, no. 1, p. 1, 2009.

[11] P. E. Rybski and R. M. Voyles, “Interactive task training of a mobile
robot through human gesture recognition,” in Robotics and Automation,
1999. Proceedings. 1999 IEEE International Conference on, vol. 1,
pp. 664–669, IEEE, 1999.

[12] S. Chernova and M. Veloso, “Teaching multi-robot coordination using
demonstration of communication and state sharing,” in Proceedings
of the 7th international joint conference on Autonomous agents and
multiagent systems-Volume 3, pp. 1183–1186, International Foundation
for Autonomous Agents and Multiagent Systems, 2008.

[13] D. H. Grollman and O. C. Jenkins, “Learning robot soccer skills from
demonstration,” in Development and Learning, 2007. ICDL 2007. IEEE
6th International Conference on, pp. 276–281, IEEE, 2007.

[14] R. S. Sutton, “Between mdps and semi-mdps: Learning, planning, and

representing knowledge at multiple temporal scales,” 1998.

[15] P. Abbeel and A. Y. Ng, “Apprenticeship learning via inverse rein-
forcement learning,” in Proceedings of the twenty-ﬁrst international
conference on Machine learning, p. 1, ACM, 2004.

[16] G. Konidaris, “Value function approximation in reinforcement learning

using the fourier basis,” 2008.

[17] P. Fearnhead and Z. Liu, “On-line inference for multiple changepoint
problems,” Journal of the Royal Statistical Society: Series B (Statistical
Methodology), vol. 69, no. 4, pp. 589–605, 2007.

Figure 5. Example 1. red: robot, blue: block, yellow: basket . left: sample
trajectories composed by moving robot to block, then carry block to basket.
right: segmented result, different color indicates different segments, trajectory
is segmented correctly.

Figure 6. Example 2. red: robot, blue: block, yellow: basket . left: sample
trajectories composed by moving robot to block, then carry block to basket.
right: segmented result, different color indicates different segments, trajectory
is segmented correctly. Compared to example 1 in Figure 5, robot takes a
large detour before getting to either block or basket, the segmentation result
is still correct, thus our algorithm is robust to non-optimal demonstrations.
which result in reformulation of the RL problem by adding
tactile state variable into originally selected SM , and adding
holding action into originally selected AM . Then Sarsa(λ) is
applied again in the reformualted RL problem, when the policy
converged, the robot learned to exert enough holding force
onto the block while carrying it to the basket.

VII. CONCLUSION

We show that based on our framework, given observed
behavior, we can segment the behavior into multiple skills if
needed, and select the appropriate abstraction such that 1) the
observed behavior is captured, and 2) the correct reference
frame and relevant objects are chosen. And when private
information is not available in the observed behavior, the robot
is able to decide whether the private information is important,
and reformulate the RL problems if needed.

REFERENCES

[1] G. Konidaris, S. Kuindersma, R. Grupen, and A. Barto, “Robot learn-
ing from demonstration by constructing skill trees,” The International
Journal of Robotics Research, p. 0278364911428653, 2011.

[2] E. E. Aksoy, A. Abramov, J. Dörr, K. Ning, B. Dellen, and F. Wörgötter,
“Learning the semantics of object–action relations by observation,”
The International Journal of Robotics Research, p. 0278364911410459,
2011.

[3] H. S. Koppula, R. Gupta, and A. Saxena, “Learning human activities
and object affordances from rgb-d videos,” The International Journal of
Robotics Research, vol. 32, no. 8, pp. 951–970, 2013.

[4] H. Kjellström, J. Romero, D. Martínez, and D. Kragi´c, “Simultaneous
visual recognition of manipulation actions and manipulated objects,” in
Computer Vision–ECCV 2008, pp. 336–349, Springer, 2008.

[5] A. J. Ijspeert, J. Nakanishi, and S. Schaal, “Learning rhythmic move-
ments by demonstration using nonlinear oscillators,” in Proceedings of
the ieee/rsj int. conference on intelligent robots and systems (iros2002),
no. BIOROB-CONF-2002-003, pp. 958–963, 2002.

