6
1
0
2

 
r
a

 

M
8
1

 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
3
5
9
5
0

.

3
0
6
1
:
v
i
X
r
a

Katyusha:AcceleratedVarianceReductionforFasterSGDZeyuanAllen-Zhuzeyuan@csail.mit.eduPrincetonUniversityMarch18,2016AbstractWeconsiderminimizingf(x)thatisanaverageofnconvex,smoothfunctionsfi(x),andprovidetheﬁrstdirectstochasticgradientmethodKatyushathathastheacceleratedconver-gencerate.Itconvergestoanε-approximateminimizerusingO((n+√nκ)·logf(x0)−f(x∗)ε)stochasticgradientswhereκistheconditionnumber.Katyushaisaprimal-onlymethod,supportingproximalupdates,non-Euclideannormsmooth-ness,mini-batchsampling,aswellasnon-uniformsampling.Italsoresolvesthefollowingopenquestionsinmachinelearning•Iff(x)isnotstronglyconvex(e.g.,Lasso,logisticregression),Katyushagivestheﬁrststochasticmethodthatachievestheoptimal1/√εrate.•Iff(x)isstronglyconvexandeachfi(x)is“rank-one”(e.g.,SVM),Katyushagivestheﬁrststochasticmethodthatachievestheoptimal1/√εrate.•Iff(x)isnotstronglyconvexandeachfi(x)is“rank-one”(e.g.,L1SVM),Katyushagivestheﬁrststochasticmethodthatachievestheoptimal1/εrate.ThemainingredientinKatyushaisanovel“negativemomentumontopofmomentum”thatcanbeelegantlycoupledwiththeexistingvariancereductiontrickforstochasticgradientdescent.Asaresult,sincevariancereductionhasbeensuccessfullyappliedtofastgrowinglistofpracticalproblems,ourpaperimpliesthatonehadbetterhurryupandgiveKatyushaahugineachofthem,inhopingforafasterrunningtimealsoinpractice.1IntroductionConsiderthefollowingcompositeconvexminimizationproblemminx∈RdnF(x)def=f(x)+ψ(x)def=1nnXi=1fi(x)+ψ(x)o.(1.1)Here,f(x)=1nPni=1fi(x)isaconvexfunctionthatisaﬁniteaverageofnsmooth,convexfunc-tionsfi(x),andψ(x)isconvex,lowersemicontinuous(butpossiblynon-diﬀerentiable)function,sometimesreferredtoastheproximalfunction.Weshallmostlyfocusonthecasewhenψ(x)isσ-stronglyconvexinthispaper.Boththesmoothnessassumptionsonfi(x)andthestrongconvexityassumptiononψ(x)canberelaxed,seeSection1.2.Weareinterestedinﬁndinganapproximateminimizerx∈RdsatisfyingF(x)≤F(x∗)+ε,wherex∗isaminimizerofF(x).Problem(1.1)arisesinmanyplacesinmachinelearning,statistics,andoperationsresearch.Forinstance,allconvexregularizedempiricalriskminimization(ERM)problemsfallintothiscategory,seeSection1.2fordetails.Ithasalsobeenrecentlyobservedthat,eﬃcientstochasticalgorithmsforsolving(1.1)givesrisetofasttrainingalgorithmsforneuralnets[2,15].Perhapsthesimplestﬁrst-ordermethodtosolve(1.1)isbyproximalgradientdescent:xk+1←argminy∈Rdn12ηky−xkk22+h∇f(xk),yi+ψ(y)o.Above,ηisthesteplength,andiftheproximalfunctionψ(y)equalszero,theupdatesimplyreducestoxk+1←xk−η∇f(xk).Sincecomputingthefullgradient∇f(·)isusuallyveryexpensive,stochasticgradientupdateruleshavebeenproposedinstead:xk+1←argminy∈Rdn12ηky−xkk22+he∇k,yi+ψ(y)o,wheree∇kisarandomvectorsatisfyingE[e∇k]=∇f(xk)andisreferredtoasthegradientestimator.Giventhe“ﬁniteaverage”structuref(x)=1nPni=1fi(x),apopularchoiceforthegradientestimatoristosete∇k=∇fi(xk)forsomerandomindexi∈[n]periteration.Methodsbasedonthischoiceareknownasstochasticgradientdescent(SGD)methods[9,34].Asthecomputationof∇fi(x)isusuallyntimesfasterthanthatof∇f(x),SGDissuitableforlarge-scalemachinelearningtasks.VarianceReduction.Recently,theconvergencespeedofSGDhasbeenimprovedwiththevariance-reductiontechnique[8,10,11,15,22,23,28–30,32,33].Inallofthesecitedresults,theauthorshave,inonewayoranother,shownthatSGDconvergesmuchfasterifonemakesabetterchoiceofthegradientestimatore∇ksothatitsvarianceE[ke∇k−∇f(xk)k22]reducesaskincreases.Oneparticularwaytochoosethisestimatorcanbedescribedasfollows.Keepasnapshotex=xkaftereverymstochasticupdatesteps(wheremissomeparameterthatisusuallyontheorderofn),andcomputethefullgradient∇f(ex)onlyforsuchsnapshots.Then,sete∇k=∇fi(xk)−∇fi(ex)+∇f(ex)asthegradientestimator.Onecanverifythat,underthischoiceofe∇k,itsatisﬁesE[e∇k]=∇f(xk)andlimk→∞E[ke∇k−∇f(xk)k22]=0.Unfortunately,allofthesecitedresultsonvariancereductionprovidenon-acceleratedconver-genceratesforsolving(1.1).Forinstance,thewidely-usedSVRGandSAGAalgorithmsobtainε-approximateminimizersfor(1.1)inO(cid:0)(cid:0)n+Lσ(cid:1)logF(x0)−F(x∗)ε(cid:1)iterationsofstochasticgradientupdates.Itisoftendenotedbyκdef=L/σtheconditionnumberoftheproblem,anditisanopenquestionregardinghowtoobtainanacceleratedmethodthatgivestheoptimalsquare-rootdependenceonκ,ratherthanthelineardependenceonκ.TherecentworkofCatalyst[13,19]bytwoindependentgroupsofresearcherspartiallyansweredthisopenquestion.Theydemonstratethatonecansolve(1.1)inonlyO(cid:0)(cid:0)n+√nκ(cid:1)logκlog1ε(cid:1)stochasticiterations,throughablack-boxreduction(thattheyrefertoasCatalyst)tonon-acceleratedmethods.Theirresultisstillimperfectatleastforthefollowingreasons:•Optimality.Itdoesnotmatchtheoptimaldependenceonκ.Itdoesnotgivetheoptimalrate1/√εifF(·)isnotstronglyconvex.Itdoesnotgivetheoptimalrate1/√εiff(·)isnon-smooth.Itdoesnotgivetheoptimalrate1/εifbothF(·)isnotstronglyconvexandf(·)isnon-smooth.Tothebestofourknowledge,itdoesnotsupportnon-Euclideannormsmoothnessonfi(·).•Practicality.Catalystisnotverypracticalsinceeachofitsinneriterationneedstobeveryaccuratelyexecuted.Thismakesthestoppingcriterionhardtobetuned,andmakesCatalystslowerthanitscompetitorsforseveralsubclassofproblem(1.1)(suchasthefamousERMproblems,seeSection1.2).2•Non-convexity.Thenon-acceleratedvariance-reductionalgorithmsapplyverywelleventonon-convexproblemsinpractice(suchastrainingneuralnetsbothempirically[15]andtheo-retically[2]).Therefore,itisverydesirabletodevelopadirectacceleratedvariance-reductionmethodduetoitspotentialapplicabilitytonon-convexproblemsaswell.Unfortunately,theCatalystreductiondoesnotseemtohelpinnon-convexcases.1.1OurResultsInthispaper,weprovideadirect,acceleratedstochasticgradientmethodKatyushaforsolving(1.1)inO(cid:0)(cid:0)n+√nκ(cid:1)logF(x0)−F(x∗)ε(cid:1)iterations,wherex0isthegivenstartingvector.EachiterationofKatyusharequiresonlythecomputationofO(1)stochasticgradients∇fi(x).Thisgivesboththeoptimaldependenceonκandonεwhich,tothebestofourknowledge,wasneverobtainedbeforeamongtheclassofstochasticgradientmethods.1IfF(·)isnotstronglyconvex,ourKatyushacanalsoworkforsuchobjectivesandneedsatotalofO(cid:0)nlogF(x0)−F(x∗)ε+√nL·kx0−x∗k√ε(cid:1)iterations.Thisgivestheoptimalrate1/√εwhich,tothebestofourknowledge,wasneverobtainedbeforeamongtheclassofstochasticgradientmethods.2OurAlgorithm.Ifignoringtheproximaltermψ(·),ourKatyushamethoditerativelyupdates:•xk+1←τ1zk+τ2ex+(1−τ1−τ2)yk;•deﬁnee∇k+1←∇f(ex)+∇fi(xk+1)−∇fi(ex)whereiisarandomindexin[n];•yk+1←xk+1−13Le∇k+1,and•zk+1←zk−αe∇k+1.Above,exisasnapshotpointwhichisupdatedeveryniterations.e∇k+1isthegradientestimatorthatsatisﬁesEi[e∇k+1]=∇f(xk+1)andisdeﬁnedinthevariance-reductionmanner(similartoknownnon-acceleratedvariance-reductionmethods).Thereasonforkeepingasequenceofthreevectors(xk,yk,zk)isacommoningredientthatcanbefoundinallexistingacceleratedmethods.3OurNewTechnique–NegativeMomentum.ThemostsurprisingpartofKatyushaisthenovelchoiceofxk+1whichisaconvexcombinationofthreevectors:yk,zk,andex.Ourtheoreticalanalysissuggeststheparameterchoicesτ2=0.5andτ1=min{√nσL,0.5}.Toproperlyexplainthisnovelcombination,letusrecalla“momentum”viewofacceleratedmethods.Inaclassicalacceleratednon-stochasticgradientmethod,xk+1isonlyaconvexcombinationofykandzk(seeforinstance[4]).Infact,zkplaystheroleof“momentum”whichaddsaweightedsumofthehistoryofthegradientsintoyk+1.Asanillustrativeexample,supposethatτ2=0,τ1=τ,andx0=y0=z0.Then,onecancomputethatyk=x0−13Le∇1,k=1;x0−13Le∇2−(cid:0)1−τ3L+τα(cid:1)e∇1,k=2;x0−13Le∇3−(cid:0)1−τ3L+τα(cid:1)e∇2−(cid:0)(1−τ)23L+(1−(1−τ)2)α(cid:1)e∇1,k=3.Sincetheparameterαisusuallymuchlargerthan1/3L,theaboverecursionsuggeststhatonecangraduallyincreasetheweightofgradientsfromearlieriterations,andthisisknownas“momentum”whichisattheheartofacceleratedﬁrst-ordermethods.Unfortunately,momentumisverydangerousforstochasticgradients.Forinstance,ifoneofthehistoricalgradientestimatore∇tissomewhatinaccurate(i.e.,itisverydiﬀerentfrom∇f(xt)),then1Ofcourse,thenon-stochasticfull-gradientmethodofNesterovcanachievesuchoptimaldependenceonκandε.2Ofcourse,thenon-stochasticfull-gradientmethodofNesterovcanachievesuchoptimalconvergencerate1/√ε.3Onecanofcourserewritethealgorithmandkeeptrackofonlytwovectorsperiteration.However,thatwillmakethestatementofthealgorithmlesscleansowerefrainfromdoingsointhispaper.3furthermovinginthisdirectionmayputusintroubleandnotdecreasetheobjectiveanymore.Thisisoneofthemajorreasonsthatamajorityoftheresearchersworkingonstochasticgradientdescenthavefoundacceleration/momentumnotveryusefulinpractice.InKatyusha,weputa“magnet”aroundex,whichwedeﬁneittobeessentiallytheaverageykofthemostrecentniterations.Wheneverwedeﬁnexk+1,itwillbeattractedbythemagnetexandwedeﬁnetheweightτ2=0.5.Thisisaverystrongmagnet:itensuresthatxk+1isnottoofarawayfromexsothegradientestimatorremainssomewhataccurate;atthesametime,itretractsxk+1backtoex,whichcanbeunderstoodasintroducinganegativemomentumwhichremovesafractionofthepaststochasticgradients.Thissummarizesthehigh-levelideabehindKatyusha,anditsformalconvergenceanalysiscanbefoundinthesubsequentsections.ComparisonwithOtherAcceleratedGradientMethods.Forsmoothconvexminimizationproblems,gradientdescentconvergesatarateLε—orLσlog1εiftheobjectiveisσ-stronglyconvex.Thisisnotoptimalamongtheclassofﬁrst-ordermethods.In1983,Nesterovshowedthattheoptimalrateshouldbe√L√ε—or√L√σlog1εiftheobjectiveisσ-stronglyconvex—andthiswasachievedbyhiscelebratedacceleratedgradientdescentmethod[24].RandomizedCoordinateDescent.Analternativewaytodeﬁnethegradientestimatoristosete∇k=d∇if(xk)where∇if(xk)isthecoordinategradientandiisrandomlychosenin{1,2,...,d}.Thisisknownas(randomized)coordinatedescentasopposedtostochasticgradientdescent.Weemphasizeherethatdesigningacceleratedmethodsforcoordinatedescentissigniﬁcantlyeasierthandesigningthatforstochasticgradientdescent,andthishasindeedbeendoneinmanypreviousresultsincluding[7,12,18,20,21,26].4Thestate-of-the-artacceleratedcoordinatedescentmethodisNUACDM[7]LinearCoupling.InarecentworkbyAllen-ZhuandOrecchia,theauthorshaveproposedanewframeworkcalledlinearcouplingthatfacilitatesthedesignofacceleratedgradientmethods[4].TheirnewframeworknotonlyreconstructsNesterov’saccelerated(full-)gradientmethod[4],pro-videsevenfasteracceleratedcoordinatedescentmethod[7],butalsoleadstomanyrecentbreak-throughsfordesigningacceleratedmethodsonnon-smoothproblems(suchaspositiveLP[5,6]andpositiveSDP[3])orevengeneralnon-convexproblems[2].Thispresentpaperalsofallsintothislinear-couplingframework.1.2OptimalConvergenceRatesforEmpiricalRiskMinimizationProblemsThereareafewinterestingsubcategoriesofproblem(1.1)andeachofthemcorrespondtosomewell-knowntrainingprobleminmachinelearning.Supposewearegivennvectorsa1,...,an∈Rdthatarethefeaturevectorsofnsamples.Then,itisinterestingtostudyspecialcaseof(1.1)whereeachfi(x)is“rank-one”structured:thatis,fi(x)def=fi(hai,xi).(Assuming“rank-one”simpliﬁesthenotations;alloftheresultsstatedinthissubsectiongeneralizetorank-O(1)structuredfi(x)’s.)Insuchacase,werewrite(1.1)asminx∈RdnF(x)def=f(x)+ψ(x)def=1nnXi=1fi(hai,xi)+ψ(x)o.(1.2)4Thereasonbehindthiscanbeunderstoodasfollows.Ifafunctionf(·)isLsmoothwithrespecttoeachcoordinatei,thenaconstant-stepupdatex0←x−1L∇if(x)eiatleastguaranteesthatitdecreasestheobjective,i.e.,f(x+1L∇if(x)ei)<f(x).Decreasingtheobjectivevalueisusuallyviewedasanimportantcomponentinexistingacceleratedmethods(seeforinstancethegradientdescentstepsummarizedin[4]).Unfortunately,thispropertyisfalseforstochasticgradientdescent,becausef(xk−ηe∇k)maybeevenlargerthanf(xk)evenforverysmallsteplengthη>0.4Withoutlossofgenerality,weassumethateachaihasEuclideannorm1.Denotingbylibethetraininglabelofdatasampleai,onecanconsiderthefollowing4interestingclassesof(1.2):Case1:ψ(x)isσstronglyconvexandf(x)isL-smooth.Examples:•ridgeregression:f(x)=12nPni=1(hai,xi−li)2andψ(x)=σ2kxk22.•elasticnet:f(x)=12nPni=1(hai,xi−li)2andψ(x)=σ2kxk22+λkxk1.Case2:ψ(x)isnotstronglyconvexandf(x)isL-smooth.Examples:•Lasso:f(x)=12nPni=1(hai,xi−li)2andψ(x)=λkxk1.•‘1logisticregression:f(x)=1nPni=1log(1+exp(−lihai,xi))andψ(x)=λkxk1.Case3:ψ(x)isσstronglyconvexandf(x)isnon-smoothbutLipschitzcontinuous.Examples:•SVM:f(x)=1nPni=1max{0,1−lihai,xi}andψ(x)=σkxk22.Case4:ψ(x)isnotstronglyconvexandf(x)isnon-smoothbutLipschitzcontinuous.Examples:•‘1-SVM:f(x)=1nPni=1max{0,1−lihai,xi}andψ(x)=λkxk1.Forallofthefourclassesabove,acceleratedstochasticmethodshavealreadybeenintroducedintheliterature,mostnotablyAccSDCA[31],APCG[20],SPDC[35].However,tothebestofourknowledge,allknownacceleratedmethodshavesuboptimalconvergenceratesforCase2,3and4.5Inparticular,thebestknownconvergenceratewaslog(1/ε)√εforCase2and3,andwaslog(1/ε)ε.Thisisafactorlog1εworsethantheoptimalrateforeachofthethreeclasses,andisespeciallyinterestingtotheoptimizationcommunitybecauseobtainingthe“optimalrate”isoneoftheultimategoalsinoptimization.(SeeforinstanceaninterestingattemptbyLanandZhou[17]tryingtoﬁxthislogfactor,aswellassuccessfulattemptsonﬁxingasimilarlogfactorbutinonlinelearningwhichisadiﬀerentproblem[14,16,27]).OurKatyushaalgorithmsimultaneouslyclosesthegapforallofthethreeclassesofproblemswiththehelpfromtherecentoptimalreductionsbyAllen-ZhuandHazan[1].Inshort,weobtainanε-approximateminimizerforCase2inO(cid:0)nlog1ε+√nLε(cid:1)iterations,forCase3inO(cid:0)nlog1ε+√nσε(cid:1)iterations,andforCase4inO(cid:0)nlog1ε+√nL√σε(cid:1)iterations.Incontrast,noneoftheexistingacceleratedmethodscanleadtosuchoptimalrateseveniftheoptimalreductionsof[1]areused,seediscussionsinSection5.Despiteobtainingtheoptimalmethodsforthementionedclasses,ouralgorithmisalsoaprimal-onlymethodbecauseweonlyneedtocompute∇fi(x)atdiﬀerentpointsxandfordiﬀerentindicesi.NoneoftheexistingacceleratedstochasticmethodsforsolvingERMproblemsareprimal-only,andthisisessentiallythereasonthattheirconvergenceratesaresuboptimal.61.3OtherExtensionsMini-batch.Katyushanaturallyextendstotheminibatchscenario.Insteadofusingasin-glestochasticgradient∇fi(·)periteration,onecanusetheaverageofbstochasticgradients1bPj∈S∇fi(·)whereSisarandomsubsetof[n]withcardinalityb.Allofourresultextends5Infact,theyalsohavethesuboptimaldependenceontheconditionnumberL/σforCase1.6ThisissobecauseevenforCase1,convertinganεapproximatemaximizerforthedualobjectivetotheprimal,oneonlyobtainsannκεapproximateminimizerontheprimalobjective.Asaresult,algorithmslikeAPCGwhodirectlyworkonthedual,algorithmslikeSPDCwhomaintainbothprimalanddualvariables,andalgorithmslikeRPDG[17]thatareprimal-likebutstillusedualanalysis,havetosuﬀerfromthisloglossintheconvergencerates.5tothissetting,wheretheonlychangeneededinthealgorithmistore-computethesnapshoteveryn/biterationsratherthaneveryniterations.Non-UniformSampling.Ifeachfi(·)hasadiﬀerentsmoothparameterLi,thenonehastoselecttherandomindexifromanon-uniformdistributioninordertoobtainthefastestrunningtime.Thiscanbedonefollowingthesametechniquesproposedin[7],butwillmakethenotationssigniﬁcantlyheavierespeciallywiththepresenceoftheproximaltermψ(·).Werefrainfromdoingsointhisversionofthepaper.Non-EuclideanNorms.Ifthesmoothnessofthefunctionsfi(x)arewithrespecttoanon-Euclideannorm(suchasthewellknown‘1normcaseoverthesimplex),ourresultsinthispaperstillhold.Inparticular,ourupdateontheyk+1sidebecomesthenon-Euclideannormgradientdescent,andtheupdateonthezk+1sidebecomesthenon-EuclideannormmirrordescentwithrespecttotheBregmandivergencetermofastronglyconvexpotentialfunction.Ouranalysisinthispapercanbetranslatedintothismoregeneralscenariofollowingthetechniquesof[4].Sincethisextensionissimplebutcomplicatesthenotations,wedeferittoafullversionofthispaper.1.4ConclusionRoadmap.WeprovidenecessarynotationsandusefultheoremsinSection2.InSection3,wefocusonanalyzingonesingleiterationofKatyusha,andinSection4weprovidetheconvergenceanalysisonKatyushaforthestronglyconvexcaseofproblem(1.1).InSection5,weapplyKatyushatonon-stronglyconvexornon-smoothobjectivesbyapplyingtheoptimalreductionsin[1].InSection6,weprovideadirectalgorithmforsolvingthenon-stronglycaseofproblem(1.1)withtheoptimal1/√εrate,andcompareitwiththeliterature.Weshallincludeexperimentalresultsinanextversionofthispaper.2PreliminariesThroughoutthispaper,wedenotebyk·ktheEuclideannorm.Wedenoteby∇f(x)thefullgradientvectoroffunctionfifitisdiﬀerentiable,orthesubgradientvectoriffisonlyLipschitzcontinuous.Recallsomeclassicaldeﬁnitionsonstrongconvexityandsmoothness.Deﬁnition2.1(Smoothnessandstrongconvexity).Foraconvexfunctionf:Rn→R,•Wesayfisσ-stronglyconvexif∀x,y∈Rn,itsatisﬁesf(y)≥f(x)+h∇f(x),y−xi+σ2kx−yk2.•WesayfisL-smoothif∀x,y∈Rn,itsatisﬁesk∇f(x)−∇f(y)k≤Lkx−yk.WealsoneedtousethefollowingdeﬁnitionbyAllen-ZhuandHazan:Deﬁnition2.2([1]).Analgorithmsolvingthestronglyconvexcaseofproblem(1.1)satisﬁesthehomogenousobjectivedecrease(HOOD)propertywithtimeTime(L,σ),ifforeverystartingpointx0,itproducesanoutputx0satisfyingE(cid:2)F(x0)(cid:3)−F(x∗)≤F(x0)−F(x∗)4intimeatmostTime(L,σ).Allen-ZhuandHazanprovidedthreeblack-boxreductionsalgorithmsAdaptReg,AdaptSmooth,andJointAdaptRegSmoothintheirpaper[1]toconvertanyalgorithmsatisfyingtheHOODpropertywithtimeTime(L,σ)respectivelyto(1)thenon-stronglyconvexbutsmoothcase,(2)thestronglyconvexbutnon-smoothcase,and(3)thenon-stronglyconvexandnon-smoothcase.Wesimplifyandrestatetheirtheoremsasfollows:6Algorithm1Katyusha(x0,S,σ,L)1:m←n;(cid:5)thetimewindowforre-computingthesnapshotex2:τ2←12,τ1←min(cid:8)√mσ√3L,12(cid:9),α←13τ1L;(cid:5)parameters3:y0=z0=ex0←x0;(cid:5)initialvectors4:fors←1toSdo5:µs←∇f(exs);(cid:5)computethefullgradientonlyonceeverymiterations6:forj←0tom−1do7:k←(sm)+j;8:xk+1←τ1zk+τ2exs+(1−τ1−τ2)yk;9:e∇k+1←µs+∇fi(xk+1)−∇fi(exs)whereiisrandomlychosenfrom{1,2,...,n};10:zk+1=argminz(cid:8)12αkz−zkk2+he∇k+1,zi+ψ(z)(cid:9);11:OptionI:yk+1←argminy(cid:8)3L2ky−xk+1k2+he∇k+1,yi+ψ(y)(cid:9);12:OptionII:yk+1←xk+1+τ1(zk+1−zk)(cid:5)weanalyzeonlyOptionIinthispaper,butOptionIIalsoworks13:endfor14:exs+1←(cid:0)Pm−1j=0(1+ασ)j(cid:1)−1·(cid:0)Pm−1j=0(1+ασ)j·xsm+j+1(cid:1);(cid:5)weightedaverageofthepreviousmiterations15:endfor16:returnexS.Theorem2.3(AdaptReg).Supposethatinproblem(1.1)f(·)isL-smoothandx0isastartingvector.Then,AdaptRegproducesanoutputxsatisfyingE(cid:2)F(x)(cid:3)−F(x∗)≤O(ε)inatotalrunningtimeofPT−1t=0Time(L,σ0·2−t)whereσ0=F(x0)−F(x∗)kx0−x∗k2andT=log2F(x0)−F(x∗)ε.Theorem2.4(AdaptSmooth).Supposethatinproblem(1.2),ψ(·)isσstronglyconvex,eachfi(·)isG-Lipschitzcontinuous,andx0isastartingvector.Then,AdaptSmoothproducesanoutputxsatisfyingE(cid:2)F(x)(cid:3)−F(x∗)≤O(ε)inatotalrunningtimeofPT−1t=0Time(2t/λ0,σ)whereλ0=F(x0)−F(x∗)G2andT=log2F(x0)−F(x∗)ε.Theorem2.5(JointAdaptRegSmooth).Supposethatinproblem(1.2),eachfi(·)isG-Lipschitzcontinuousandx0isastartingvector.Then,JointAdaptRegSmoothproducesanoutputxsat-isfyingE(cid:2)F(x)(cid:3)−F(x∗)≤O(ε)inatotalrunningtimeofPT−1t=0Time(2t/λ0,σ0·2−t)whereλ0=F(x0)−F(x∗)G2,σ0=F(x0)−F(x∗)kx0−x∗k2andT=log2F(x0)−F(x∗)kx0−x∗k2.3One-IterationAnalysisInthissection,wefocusonanalyzingthebehaviorofKatyusha(seeAlgorithm1)inasingleiteration(i.e.,foraﬁxedk).Weviewyk,zkandxk+1asﬁxedinthissectionsotheonlyrandomnesscomesfromthechoiceofiiniterationk.Weabbreviateinthissectionbyex=exswheresistheepochthatiterationkbelongsto,anddenotebyσ2k+1def=k∇f(xk+1)−e∇k+1k2soE[σk+1]isthevarianceofthegradientestimatore∇k+1inthisiteration.OurﬁrstlemmalowerboundstheexpectedobjectivedecreaseF(xk+1)−E[F(yk+1)].OurProg(xk+1)deﬁnedbelowisanon-negative,classicalquantitythatwouldbealowerboundontheamountofobjectivedecreaseife∇k+1wereequalto∇f(xk+1),seeforinstance[4].However,sincethevarianceσ2k+1isnon-zero,thislowerboundmustbecompensatedbyanegativetermthatdependsonE[σ2k+1].7Lemma3.1(proximalgradientdescent).Ifyk+1=argminy(cid:8)3L2ky−xk+1k2+he∇k+1,y−xk+1i+ψ(y)−ψ(xk+1)(cid:9),andProg(xk+1)def=−miny(cid:8)3L2ky−xk+1k2+he∇k+1,y−xk+1i+ψ(y)−ψ(xk+1)(cid:9)≥0,wehaveF(xk+1)−E(cid:2)F(yk+1)(cid:3)≥E(cid:2)Prog(xk+1)(cid:3)−14LE(cid:2)σ2k+1(cid:3).Proof.Prog(xk+1)=−miny{3L2ky−xk+1k2+he∇k+1,y−xk+1i+ψ(y)−ψ(xk+1)(cid:9)x=−(cid:16)3L2kyk+1−xk+1k2+he∇k+1,yk+1−xk+1i+ψ(yk+1)−ψ(xk+1)(cid:17)=−(cid:16)L2kyk+1−xk+1k2+h∇f(xk+1),yk+1−xk+1i+ψ(yk+1)−ψ(xk+1)(cid:17)+(cid:16)h∇f(xk+1)−e∇k+1,yk+1−xk+1i−Lkyk+1−xk+1k2(cid:17)y≤−(cid:16)f(yk+1)−f(xk+1)+ψ(yk+1)−ψ(xk+1)(cid:17)+14Lk∇f(xk+1)−e∇k+1k2.Above,xisbythedeﬁnitionofyk+1,andyusesthesmoothnessoffunctionf(·),aswellastheinequalityha,bi−12kbk2≤12kak2.Takingexpectationonbothsideswearriveatthedesiredresult.(cid:3)Thefollowinglemmaprovidesanovelupperboundontheexpectedvarianceofthegradientestimator.Notethatallknownvariancereductionanalysisforconvexoptimization,inonewayoranother,upperboundsthisvarianceessentiallyby4L·(f(ex)−f(x∗)),theobjectivedistancetotheminimizer(c.f.[10,15]).TherecentbreakthroughofAllen-ZhuandHazan[1]upperboundsitbythepointdistancekxk+1−exk2fornon-convexobjectives,whichistighterifexisclosetoxk+1butunfortunatelynotenoughforthepurposeofthispaper.Inthispaper,weupperbounditbythetightestpossiblequantitywhichisalmost2L·(cid:0)f(ex)−f(xk+1)(cid:1)(cid:28)4L·(cid:0)f(ex)−f(x∗)(cid:1).Unfortunately,thisupperboundneedstobecompensatedbyanadditionaltermh∇f(xk+1),ex−xk+1i,whichcouldbepositivebutweshallcancelitusingtheintroducednegativemomentum.Lemma3.2.E(cid:2)ke∇k+1−∇f(xk+1)k2(cid:3)≤2L·(cid:0)f(ex)−f(xk+1)−h∇f(xk+1),ex−xk+1i(cid:1).Proof.Eachfi(x),beingconvexandL-smooth,impliesthefollowinginequalitywhichisclassicalinconvexoptimizationandcanbefoundforinstanceinTheorem2.1.5ofthetextbookofNesterov[25].k∇fi(xk+1)−∇fi(ex)k2≤2L(cid:0)fi(ex)−fi(xk+1)−h∇fi(xk+1),ex−xk+1i(cid:1)Therefore,takingexpectationovertherandomchoiceofi,wehaveE(cid:2)ke∇k+1−∇f(xk+1)k2(cid:3)=E(cid:2)(cid:13)(cid:13)(cid:0)∇fi(xk+1)−∇fi(ex)(cid:1)−(cid:0)∇f(xk+1)−∇f(ex)(cid:1)(cid:13)(cid:13)2(cid:3)x≤E(cid:2)(cid:13)(cid:13)∇fi(xk+1)−∇fi(ex)(cid:13)(cid:13)2(cid:3)=2L·E(cid:2)fi(ex)−fi(xk+1)−h∇fi(xk+1),ex−xk+1i(cid:3)=2L·(cid:0)f(ex)−f(xk+1)−h∇f(xk+1),ex−xk+1i(cid:1).Above,xisbecauseforanyrandomvectorζ∈Rd,itholdsthatEkζ−Eζk2=Ekζk2−kEζk2.(cid:3)8Thenextlemmaisaclassicaloneforproximalmirrordescent.Lemma3.3(proximalmirrordescent).Supposeψ(·)isσstronglyconvex.Ife∇k+1isﬁxedandzk+1=argminz(cid:8)12kz−zkk2+αhe∇k+1,z−zki+αψ(z)−αψ(zk)(cid:9),thenitsatisﬁesforallu∈Rd,αhe∇k+1,zk+1−ui+αψ(zk+1)−αψ(u)≤−12kzk−zk+1k2+12kzk−uk2−1+ασ2kzk+1−uk2.Proof.Bytheminimalitydeﬁnitionofzk+1,wehavethatzk+1−zk+αe∇k+1+αg=0wheregissomesubgradientofψ(z)atpointz=zk+1.Thisimpliesthatforeveryuitsatisﬁes0=(cid:10)zk+1−zk+αe∇k+1+αg,zk+1−ui.Atthispoint,usingtheequalityhzk+1−zk,zk+1−ui=12kzk−zk+1k2−12kzk−uk2+12kzk+1−uk2,aswellastheinequalityhg,zk+1−ui≥ψ(zk+1)−ψ(u)−σ2kzk+1−uk2whichcomesfromthestrongconvexityofψ(·),wecanwriteαhe∇k+1,zk+1−ui+αψ(zk+1)−αψ(u)=−hzk+1−zk,zk+1−ui−hαg,zk+1−ui+αψ(zk+1)−αψ(u)≤−12kzk−zk+1k2+12kzk−uk2−1+ασ2kzk+1−uk2.(cid:3)ThefollowinglemmacombinesLemma3.1,Lemma3.2andLemma3.3alltogether,usingthespecialchoiceofxk+1whichisaconvexcombinationofyk,zkandex:Lemma3.4(couplingstep1).Ifxk+1=τ1zk+τ2ex+(1−τ1−τ2)yk,whereτ1≤3αLandτ2=12,thenitsatisﬁesαh∇f(xk+1),zk−ui−αψ(u)≤ατ1(cid:16)F(xk+1)−E(cid:2)F(yk+1)(cid:3)+τ2F(ex)−τ2E(cid:2)f(xk+1)(cid:3)−τ2h∇f(xk+1),ex−xk+1i(cid:17)+12kzk−uk2−1+ασ2E(cid:2)kzk+1−uk2(cid:3)+α(1−τ1−τ2)τ2ψ(yk)−ατ1ψ(xk+1)Proof.WeﬁrstapplyLemma3.3andgetαhe∇k+1,zk−ui+αψ(zk+1)−αψ(u)=αhe∇k+1,zk−zk+1i+αhe∇k+1,zk+1−ui+αψ(zk+1)−αψ(u)≤αhe∇k+1,zk−zk+1i−12kzk−zk+1k2+12kzk−uk2−1+ασ2kzk+1−uk2.(3.1)9Bydeﬁningvdef=τ1zk+1+τ2ex+(1−τ1−τ2)yk,wehavexk+1−v=τ1(zk−zk+1)andthereforeEhαhe∇k+1,zk−zk+1i−12kzk−zk+1k2i=Ehατ1he∇k+1,xk+1−vi−12τ21kxk+1−vk2i=Ehατ1(cid:16)he∇k+1,xk+1−vi−12ατ1kxk+1−vk2−ψ(v)+ψ(xk+1)(cid:17)+ατ1(cid:16)ψ(v)−ψ(xk+1)(cid:17)ix≤Ehατ1(cid:16)he∇k+1,xk+1−vi−3L2kxk+1−vk2−ψ(v)+ψ(xk+1)(cid:17)+ατ1(cid:16)ψ(v)−ψ(xk+1)(cid:17)iy≤Ehατ1(cid:16)F(xk+1)−F(yk+1)+14Lσ2k+1(cid:17)+ατ1(cid:16)ψ(v)−ψ(xk+1)(cid:17)iz≤Ehατ1(cid:16)F(xk+1)−F(yk+1)+12(cid:0)f(ex)−f(xk+1)−h∇f(xk+1),ex−xk+1i(cid:1)(cid:17)+ατ1(cid:16)τ1ψ(zk+1)+τ2ψ(ex)+(1−τ1−τ2)ψ(yk)−ψ(xk+1)(cid:17)i.(3.2)Above,xusesourchoiceτ1≤3αL,yusesLemma3.1,zusesLemma3.2.Finally,noticingthatE[he∇k+1,zk−ui]=h∇f(xk+1),zk−uiandτ2=12,weobtainthedesiredinequalitybycombining(3.1)and(3.2).(cid:3)ThenextlemmasimpliﬁesthelefthandsideofLemma3.4usingtheconvexityoff(·),andgivesaninequalitythatrelatestheobjective-distance-to-minimizerquantitiesF(yk)−F(x∗),F(yk+1)−F(x∗),andF(ex)−F(x∗)tothepoint-distance-to-minimizerquantitieskzk−x∗k2andkzk+1−x∗k2.Lemma3.5(couplingstep2).Underthesamechoicesofτ1,τ2asinLemma3.4,wehave0≤α(1−τ1−τ2)τ1(F(yk)−F(x∗))−ατ1(cid:0)E(cid:2)F(yk+1)(cid:3)−F(x∗)(cid:1)+ατ2τ1(cid:0)F(ex)−τ2F(x∗)(cid:1)+12kzk−x∗k2−1+ασ2E(cid:2)kzk+1−x∗k2(cid:3).Proof.Weﬁrstcomputethatα(cid:0)f(xk+1)−f(u)(cid:1)x≤αh∇f(xk+1),xk+1−ui=αh∇f(xk+1),xk+1−zki+αh∇f(xk+1),zk−uiy=ατ2τ1h∇f(xk+1),ex−xk+1i+α(1−τ1−τ2)τ1h∇f(xk+1),yk−xk+1i+αh∇f(xk+1),zk−uiz=ατ2τ1h∇f(xk+1),ex−xk+1i+α(1−τ1−τ2)τ1(f(yk)−f(xk+1))+αh∇f(xk+1),zk−ui.Above,xusestheconvexityoff(·),yusesthechoicethatxk+1=τ1zk+τ2ex+(1−τ1−τ2)yk,andzusestheconvexityoff(·)again.ByapplyingLemma3.4totheaboveinequality,wehaveα(cid:0)f(xk+1)−F(u)(cid:1)≤α(1−τ1−τ2)τ1(F(yk)−f(xk+1))+ατ1(cid:16)F(xk+1)−E(cid:2)F(yk+1)(cid:3)+τ2F(ex)−τ2f(xk+1)(cid:17)+12kzk−uk2−1+ασ2E(cid:2)kzk+1−uk2(cid:3)−ατ1ψ(xk+1)whichimpliesα(cid:0)F(xk+1)−F(u)(cid:1)≤α(1−τ1−τ2)τ1(F(yk)−F(xk+1))+ατ1(cid:16)F(xk+1)−E(cid:2)F(yk+1)(cid:3)+τ2F(ex)−τ2F(xk+1)(cid:17)+12kzk−uk2−1+ασ2E(cid:2)kzk+1−uk2(cid:3).10Afterrearrangingandsettingu=x∗,theaboveinequalityyields0≤α(1−τ1−τ2)τ1(F(yk)−F(x∗))−ατ1(cid:0)E(cid:2)F(yk+1)−F(x∗)(cid:3)(cid:1)+ατ2τ1(cid:0)F(ex)−τ2F(x∗)(cid:1)+12kzk−x∗k2−1+ασ2E(cid:2)kzk+1−x∗k2(cid:3).(cid:3)4StronglyConvexCaseInthissectionwetelescopeLemma3.5fromtheprevioussectionacrossalliterationsk,andprovethefollowingtheorem:Theorem4.1.Ifeachfi(x)isconvex,L-smooth,andψ(x)isσ-stronglyconvexin(1.1),thenKatyusha(x0,S,σ,L)satisﬁesE(cid:2)F(exS)(cid:3)−F(x∗)≤(cid:26)O((1+ασ)−Sm)·(cid:0)F(x0)−F(x∗)(cid:1),ifmσL≤34;O(cid:0)1.5−S(cid:1)·(cid:0)F(x0)−F(x∗)(cid:1),ifmσL>34.Inotherwords,Katyushaachievesanε-additiveerror(i.e.,E(cid:2)F(exS)(cid:3)−F(x∗)≤ε)usingatmostO(cid:0)(cid:0)n+pnL/σ(cid:1)·logF(x0)−F(x∗)ε(cid:1)iterations.7Remark4.2.Becausem=n,eachiterationofKatyushacomputesonlyO(1)stochasticgradients∇fi(·)intheamortizedsense.Therefore,theper-iterationtimecomplexityofKatyushaisdomi-natedbythecomputationtimeof∇fi(·),theproximalupdateinLine10ofAlgorithm1,plusanoverheadO(d).If∇fi(·)hasatmostd0≤dnon-zeroentries,thisoverheadO(d)isimprovabletoO(d0)usingasparseimplementationofKatyusha.8Asaresult,foralltheERMproblemsdeﬁnedin(1.2),theamortizedper-iterationcomplexityofKatyushaisonlyO(d0)whered0isthesparsityoffeaturevectors,asymptoticallythesameastheper-iterationcomplexityofSGD.ProofofTheorem4.1.WedeﬁneDkdef=F(yk)−F(x∗),eDsdef=F(exs)−F(x∗),andrewriteLemma3.5asfollows:0≤(1−τ1−τ2)τ1Dk−1τ1Dk+1+τ2τ1E(cid:2)eDs(cid:3)+12αkzk−x∗k2−1+ασ2αE(cid:2)kzk+1−x∗k2(cid:3).Atthispoint,letusdeﬁneθ=1+ασandmultiplytheaboveinequalitybyθjforeachk=sm+j.Then,wesumuptheresultingminequalitiesforallj=0,1,...,m−1:0≤Eh(1−τ1−τ2)τ1m−1Xj=0Dsm+j·θj−1τ1m−1Xj=0Dsm+j+1·θji+τ2τ1eDs·m−1Xj=0θj+12αkzsm−x∗k2−θm2α(cid:2)kz(s+1)m−x∗k2(cid:3).Notethatintheaboveinequalitywehaveassumedalltherandomnessintheﬁrsts−1epochsareﬁxedandtheonlysourceofrandomnesscomesfromepochs.Wecanrearrangethetermsinthe7Likeinallstochasticﬁrst-ordermethods,onecanapplyaMarkovinequalitytoconcludethatwithprobabilityatleast2/3,KatyushasatisﬁesF(exS)−F(x∗)≤εinthesamestatedasymptoticrunningtime.8Thisrequiresonetodefertheupdatessuchasxk+1←τ1zk+τ2exs+(1−τ1−τ2)ykwithoutnaivelyimplementingit.Anexperiencedprogrammercanconsultforinstance[5]foradetailedtreatmentbutonadiﬀerentproblem.11aboveinequalityandgetEhτ1+τ2−(1−1/θ)τ1mXj=1Dsm+j·θji≤(1−τ1−τ2)τ1(cid:16)Dsm−θmE(cid:2)D(s+1)m(cid:3)(cid:17)+τ2τ1eDs·m−1Xj=0θj+12αkzsm−x∗k2−θm2αE(cid:2)kz(s+1)m−x∗k2(cid:3).Usingthespecialchoicethatexs+1=(cid:0)Pm−1j=0θj(cid:1)−1·Pm−1j=0xsm+j+1·θjandtheconvexityofF(·),wederivethateDs+1≤(cid:0)Pm−1j=0θj(cid:1)−1·Pm−1j=0Dsm+j+1·θj.Substitutingthisintotheaboveinequality,wegetτ1+τ2−(1−1/θ)τ1θE(cid:2)eDs+1(cid:3)·m−1Xj=0θj≤(1−τ1−τ2)τ1(cid:16)Dsm−θmE(cid:2)D(s+1)m(cid:3)(cid:17)+τ2τ1eDs·m−1Xj=0θj+12αkzsm−x∗k2−θm2αE(cid:2)kz(s+1)m−x∗k2(cid:3).(4.1)Weconsidertwocasesnext.Case1.SupposethatmσL≤34.Inthiscase,wechooseα=1√3mσLandτ1=13αL=mασ=√mσ√3L∈[0,12]asinKatyusha.Ourparameterchoicesimplyασ≤1/2mandthereforethefollowinginequalityholds:τ2(θm−1−1)+(1−1/θ)=12((1+ασ)m−1−1)+(1−11+ασ)≤(m−1)ασ+ασ=mασ=τ1.Inotherwords,wehaveτ1+τ2−(1−1/θ)≥τ2θm−1andthus(4.1)impliesthatEhτ2τ1eDs+1·m−1Xj=0θj+1−τ1−τ2τ1D(s+1)m+12αkz(s+1)m−x∗k2i≤θ−m·(cid:16)τ2τ1eDs·m−1Xj=0θj+1−τ1−τ2τ1Dsm+12αkzsm−x∗k2(cid:17)Ifwetelescopetheaboveinequalityoverallepochss=0,1,...,S−1,weobtainE(cid:2)F(exS)−F(x∗)(cid:3)=E(cid:2)eDS(cid:3)x≤θ−Sm·O(cid:16)eD0+D0+τ1αmkx0−x∗k2(cid:17)y≤θ−Sm·O(cid:16)1+τ1αmσ(cid:17)·(F(x0)−F(x∗))z=O((1+ασ)−Sm)·(cid:0)F(x0)−F(x∗)(cid:1).(4.2)Above,xusesthefactthatPm−1j=0θj≥mandτ2=12;yusesthestrongconvexityofF(·)whichimpliesF(x0)−F(x∗)≥σ2kx0−x∗k2;andzusesourchoiceofτ1.Case2.SupposethatmσL>34.Inthiscase,wechooseτ1=12andα=13τ1L=23LasinKatyusha.Ourparameterchoiceshelpussimplify(4.1)as2E(cid:2)eDs+1(cid:3)·m−1Xj=0θj≤eDs·m−1Xj=0θj+12αkzsm−x∗k2−θm2αE(cid:2)kz(s+1)m−x∗k2(cid:3).12Sinceθm=(1+ασ)m≥1+ασm=1+2σm3L≥32,theaboveinequalityimplies32E(cid:2)eDs+1(cid:3)+9L8E(cid:2)kz(s+1)m−x∗k2(cid:3)·m−1Xj=0θj≤eDs·m−1Xj=0θj+3L4kzsm−x∗k2.Ifwetelescopethisinequalityoveralltheepochss=0,1,...,S−1,weimmediatelyhaveEheDS·m−1Xj=0θj+3L4kzSm−x∗k2i≤(cid:0)23(cid:1)S·(cid:16)eD0·m−1Xj=0θj+3L4kz0−x∗k2(cid:17).Finally,sincePm−1j=0θj≥mandσ2kz0−x∗k2≤F(x0)−F(x∗)owingtothestrongconvexityofF(·),weconcludethatE(cid:2)F(exS)−F(x∗)(cid:3)≤O(cid:0)1.5−S(cid:1)·(cid:0)F(x0)−F(x∗)(cid:1).(4.3)Combining(3.1)and(3.2)weﬁnishtheproofofTheorem4.1.(cid:3)5CorollariesViaReductionItisimmediatelyclearfromTheorem4.1thatKatyushasatisﬁestheHOODproperty:Corollary5.1.KatyushasatisﬁestheHOODpropertywithT(L,σ)=O(cid:0)n+√nL√σ(cid:1)iterations.Remark5.2.Noticethatexistingacceleratedstochasticmethods(evenonlyforsolvingthesimplerproblem(1.2)eitherdonotsatisfyHOODpropertyorsatisfyHOODwithanadditionalfactorlog(L/σ)inthenumberofiterations.Thisiswhytheycannotbecombinedwiththereductionsin[1]togettheoptimalconvergencerates.BasedontheHOODproperty,wecanapplyTheorem2.3,2.4and2.5inSection2todeducethatCorollary5.3.Ifeachfi(x)isconvex,L-smoothandψ(·)isnotnecessarilystronglyconvexin(1.1),thenbyapplyingAdaptRegonKatyushawithastartingvectorx0,weobtainanoutputxsatisfyingE[F(x)]−F(x∗)≤εinatmostO(cid:16)nlogF(x0)−F(x∗)ε+√nL·kx0−x∗k√ε(cid:17)iterations.Incontrast,thebestknownconvergenceratewasonlylog2(1/ε)√εduetoCatalyst,orlog(1/ε)√εbyapplyingthenewAdaptRegreductiononCatalyst.Corollary5.4.Ifeachfi(x)isG-Lipschitzcontinuousandψ(x)isσ-stronglyconvexin(1.2),thenbyapplyingAdaptSmoothonKatyushawithastartingvectorx0,weobtainanoutputxsatisfyingE[F(x)]−F(x∗)≤εinatmostO(cid:16)nlogF(x0)−F(x∗)ε+√nG√σε(cid:17)iterations.Incontrast,thebestknownconvergenceratewasonlylog(1/ε)√εduetoAPCGandSPDC.13Algorithm2Katyushans(x0,S,σ,L)1:m←n;(cid:5)thetimewindowforre-computingsnapshotex2:τ2←12;3:y0=z0=ex0←x0;(cid:5)initialvectors4:fors←1toSdo5:τ1,s←2s+4,αs←13τ1,sL(cid:5)diﬀerentparameterchoicescomparingtoKatyusha6:µs←∇f(exs);(cid:5)computethefullgradientonlyonceeverymiterations7:forj←0tom−1do8:k←(sm)+j;9:xk+1←τ1,szk+τ2exs+(1−τ1,s−τ2)yk;10:e∇k+1←µs+∇fi(xk+1)−∇fi(exs)whereiisrandomlychosenfrom{1,2,...,n};11:zk+1=argminz(cid:8)12αskz−zkk2+he∇k+1,zi+ψ(z)(cid:9);12:OptionI:yk+1←argminy(cid:8)3L2ky−xk+1k2+he∇k+1,yi+ψ(y)(cid:9);13:OptionII:yk+1←xk+1+τ1,s(zk+1−zk)(cid:5)weanalyzeonlyOptionIinthispaper,butOptionIIalsoworks14:endfor15:exs+1←1mPmj=1xsm+j;16:endfor17:returnexS.Corollary5.5.Ifeachfi(x)isG-Lipschitzcontinuousandψ(x)isnotnecessarilystronglyconvexin(1.2),thenbyapplyingJointAdaptRegSmoothonKatyushawithastartingvectorx0,weobtainanoutputxsatisfyingE[F(x)]−F(x∗)≤εinatmostO(cid:16)nlogF(x0)−F(x∗)ε+√nGkx0−x∗kε(cid:17)iterations.Incontrast,thebestknownconvergenceratewasonlylog(1/ε)εduetoAPCGandSPDC.6Non-StronglyConvexCaseInthissectionweconsideravariantofKatyushathatdirectlyworksonnon-stronglyconvexobjectivesF(·)forproblem(1.1).WecallthisalgorithmKatyushans,seeAlgorithm2.Asinthestronglyconvexcase,wesetτ2=12throughoutthealgorithm,butchooseτ1=τ1,stobeaparameterthatdependsontheepochindexs,andaccordinglyαs=13Lτ1,s.TheseparameterchoiceswillsatisfythepresumptionsinLemma3.4.Weprovethefollowingtheoreminthissection:Theorem6.1.Ifeachfi(x)isconvex,L-smoothin(1.1)andψ(·)isnotnecessarilystronglyconvex,thenKatyushans(x0,S,L)satisﬁesE(cid:2)F(exS)(cid:3)−F(x∗)≤O(cid:16)F(x0)−F(x∗)S2+Lkx0−x∗k2nS2(cid:17)Inotherwords,Katyushansachievesanε-additiveerror(i.e.,E(cid:2)F(exS)(cid:3)−F(x∗)≤ε)usingatmostO(cid:16)n√F(x0)−F(x∗)√ε+√nLkx0−x∗k√ε(cid:17)stochasticgradientsandthesamenumberofiterations.14Remark6.2.Katyushansisadirect,acceleratedsolverforthenon-stronglyconvexcaseofproblem(1.1).Oneshouldcompareitwiththeconvergencelemmaofadirect,non-acceleratedsolverforthesamesetting,whichcanbeusuallywrittenasfollowswhentranslatedtoournotations(seeforinstanceSAGA[10]):E(cid:2)F(x)(cid:3)−F(x∗)≤O(cid:16)F(x0)−F(x∗)S+Lkx0−x∗k2nS(cid:17).ItisclearatthismomentthatKatyushansisatleastafactorSfasterthannon-acceleratedmethodssuchasSAGA.ThisconvergencecanalsobewrittenintermsofthenumberofiterationswhichisO(cid:0)n(F(x0)−F(x∗))ε+Lkx0−x∗k2ε(cid:1).Remark6.3.OurstatedconvergenceresultinTheorem4.1isaslightlyworsethanthede-siredcomplexityO(cid:0)nlogF(x0)−F(x∗)ε+√nLkx0−x∗k√ε(cid:1)obtainedfromusingtheoptimalreduction,seeCorollary5.5.Thiscanbeﬁxedbymakingsomenon-trivialchangestotheepochlengths,andisomittedinthecurrentversionofthispaper.9ProofofTheorem6.1.AgainbydeﬁningDkdef=F(yk)−F(x∗)andeDsdef=F(exs)−F(x∗),wecanrewriteLemma3.5asfollows:0≤αs(1−τ1,s−τ2)τ1,sDk−αsτ1,sE(cid:2)Dk+1(cid:3)+αsτ2τ1,sneDs+12kzk−x∗k2−12E(cid:2)kzk+1−x∗k2(cid:3).Summinguptheaboveinequalityforalltheiterationsk=sm,sm+1,...,sm+m−1,wehaveEhαs1−τ1,s−τ2τ1,sD(s+1)m+αsτ1,s+τ2τ1,smXj=1Dsm+ji≤αs1−τ1,s−τ2τ1,sDsm+αsτ2τ1,sneDs+12kzsm−x∗k2−12E(cid:2)kz(s+1)m−x∗k2(cid:3).(6.1)Notethatintheaboveinequalitywehaveassumedalltherandomnessintheﬁrsts−1epochsareﬁxedandtheonlysourceofrandomnesscomesfromepochs.Ifwedeﬁneexs=1mPmj=1x(s−1)m+j,thenbytheconvexityoffunctionF(·)wehavemeDs≤Pnj=1D(s−1)m+j.Therefore,foreverys≥1wecanderivefrom(6.1)thatEh1τ21,sD(s+1)m+τ1,s+τ2τ21,sm−1Xj=1Dsm+ji≤1−τ1,sτ21,sDsm+τ2τ21,sm−1Xj=1D(s−1)m+j+3L2kzsm−x∗k2−3L2E(cid:2)kz(s+1)m−x∗k2(cid:3).(6.2)Forthebasecases=0,wecanalsorewrite(6.1)asEh1τ21,0Dm+τ1,0+τ2τ21,0m−1Xj=1Dji≤1−τ1,0−τ2τ21,0D0+τ2nτ21,0eD0+3L2kz0−x∗k2−3L2E(cid:2)kzm−x∗k2(cid:3).(6.3)9Moreprecisely,recallthatasimilarissuehasalsohappenedinthenon-acceleratedworld:theiterationcomplexityO(n+Lε)inSAGAcanbeimprovedtoO(nlog1ε+Lε)usingvaryingepochlength[8].Thetechniquein[8]alsoappliestothispaper.15Atthispoint,ifwechooseτ1,s=2s+4≤12,itsatisﬁes1τ21,s≥1−τ1,s+1τ21,s+1andτ1,s+τ2τ21,s≥τ2τ2s+1.Usingthesetwoinequalities,wecantelescope(6.3)and(6.2)foralls=0,1,...,S−1.WeobtainintheendthatEh1τ21,S−1D(s+1)m+τ1,S−1+τ2τ21,S−1m−1Xj=1D(S−1)m+j+3L2kzSm−z∗k2i≤1−τ1,0−τ2τ21,0D0+τ2nτ21,0eD0+3L2kz0−x∗k2(6.4)SincewehaveeDS≤1mPmj=1D(S−1)m+jwhichisnogreaterthan2τ21,S−1mtimesthelefthandsideof(6.4),weconcludethatE(cid:2)F(exS)−F(x∗)(cid:3)=E(cid:2)eDS(cid:3)≤O(cid:0)τ21,Sm(cid:1)·(cid:16)1−τ1,0−τ2τ21,0D0+τ2nτ21,0eD0+3L2kz0−x∗k2(cid:17)=O(cid:0)1mS2(cid:1)·(cid:16)m(cid:0)F(x0)−F(x∗)(cid:1)+Lkx0−x∗k2(cid:17).(cid:3)References[1]ZeyuanAllen-ZhuandEladHazan.OptimalBlack-BoxReductionsBetweenOptimizationObjectives.ArXive-prints,abs/1603.05642,March2016.[2]ZeyuanAllen-ZhuandEladHazan.VarianceReductionforFasterNon-ConvexOptimization.ArXive-prints,abs/1603.05643,March2016.[3]ZeyuanAllen-Zhu,YinTatLee,andLorenzoOrecchia.Usingoptimizationtoobtainawidth-independent,parallel,simpler,andfasterpositiveSDPsolver.InProceedingsofthe27thACM-SIAMSymposiumonDiscreteAlgorithms,SODA’16,2016.[4]ZeyuanAllen-ZhuandLorenzoOrecchia.Linearcoupling:Anultimateuniﬁcationofgradientandmirrordescent.ArXive-prints,abs/1407.1537,July2014.[5]ZeyuanAllen-ZhuandLorenzoOrecchia.Nearly-LinearTimePositiveLPSolverwithFasterConvergenceRate.InProceedingsofthe47thAnnualACMSymposiumonTheoryofCom-puting,STOC’15,2015.[6]ZeyuanAllen-ZhuandLorenzoOrecchia.Usingoptimizationtobreaktheepsilonbarrier:Afasterandsimplerwidth-independentalgorithmforsolvingpositivelinearprogramsinparallel.InProceedingsofthe26thACM-SIAMSymposiumonDiscreteAlgorithms,SODA’15,2015.[7]ZeyuanAllen-Zhu,PeterRicht´arik,ZhengQu,andYangYuan.Evenfasteracceleratedcoor-dinatedescentusingnon-uniformsampling.ArXive-prints,abs/1512.09103,December2015.[8]ZeyuanAllen-ZhuandYangYuan.ImprovedSVRGforNon-Strongly-ConvexorSum-of-Non-ConvexObjectives.ArXive-prints,abs/1506.01972,June2015.[9]L´eonBottou.Stochasticgradientdescent.http://leon.bottou.org/projects/sgd.16[10]AaronDefazio,FrancisBach,andSimonLacoste-Julien.SAGA:AFastIncrementalGradientMethodWithSupportforNon-StronglyConvexCompositeObjectives.InAdvancesinNeuralInformationProcessingSystems,NIPS2014,2014.[11]AaronJ.Defazio,Tib´erioS.Caetano,andJustinDomke.Finito:AFaster,PermutableIncrementalGradientMethodforBigDataProblems.InProceedingsofthe31stInternationalConferenceonMachineLearning,ICML2014,2014.[12]OlivierFercoqandPeterRicht´arik.Accelerated,parallel,andproximalcoordinatedescent.SIAMJournalonOptimization,25(4):1997–2023,2015.FirstappearedonArXiv1312.5799in2013.[13]RoyFrostig,RongGe,ShamM.Kakade,andAaronSidford.Un-regularizing:approximateproximalpointandfasterstochasticalgorithmsforempiricalriskminimization.InICML,volume37,pages1–28,2015.[14]EladHazanandSatyenKale.Beyondtheregretminimizationbarrier:Optimalalgorithmsforstochasticstrongly-convexoptimization.TheJournalofMachineLearningResearch,15(1):2489–2512,January2014.[15]RieJohnsonandTongZhang.Acceleratingstochasticgradientdescentusingpredictivevari-ancereduction.InAdvancesinNeuralInformationProcessingSystems,NIPS2013,pages315–323,2013.[16]SimonLacoste-Julien,MarkW.Schmidt,andFrancisR.Bach.Asimplerapproachtoob-tainingano(1/t)convergenceratefortheprojectedstochasticsubgradientmethod.ArXive-prints,abs/1212.2002,2012.[17]GuanghuiLanandYiZhou.Anoptimalrandomizedincrementalgradientmethod.ArXive-prints,abs/1507.02000,October2015.[18]YinTatLeeandAaronSidford.Eﬃcientacceleratedcoordinatedescentmethodsandfasteralgorithmsforsolvinglinearsystems.InFoundationsofComputerScience(FOCS),2013IEEE54thAnnualSymposiumon,pages147–156.IEEE,2013.[19]HongzhouLin,JulienMairal,andZaidHarchaoui.AUniversalCatalystforFirst-OrderOptimization.InNIPS,2015.[20]QihangLin,ZhaosongLu,andLinXiao.AnAcceleratedProximalCoordinateGradientMethodanditsApplicationtoRegularizedEmpiricalRiskMinimization.InAdvancesinNeuralInformationProcessingSystems,NIPS2014,pages3059–3067,2014.[21]ZhaosongLuandLinXiao.Onthecomplexityanalysisofrandomizedblock-coordinatedescentmethods.MathematicalProgramming,pages1–28,2013.[22]MehrdadMahdavi,LijunZhang,andRongJin.Mixedoptimizationforsmoothfunctions.InAdvancesinNeuralInformationProcessingSystems,pages674–682,2013.[23]JulienMairal.IncrementalMajorization-MinimizationOptimizationwithApplicationtoLarge-ScaleMachineLearning.SIAMJournalonOptimization,25(2):829–855,April2015.PreliminaryversionappearedinICML2013.17[24]YuriiNesterov.AmethodofsolvingaconvexprogrammingproblemwithconvergencerateO(1/k2).InDokladyANSSSR(translatedasSovietMathematicsDoklady),volume269,pages543–547,1983.[25]YuriiNesterov.IntroductoryLecturesonConvexProgrammingVolume:ABasiccourse,vol-umeI.KluwerAcademicPublishers,2004.[26]YuriiNesterov.EﬃciencyofCoordinateDescentMethodsonHuge-ScaleOptimizationProb-lems.SIAMJournalonOptimization,22(2):341–362,jan2012.[27]AlexanderRakhlin,OhadShamir,andKarthikSridharan.Makinggradientdescentoptimalforstronglyconvexstochasticoptimization.InProceedingsofthe29thInternationalConferenceonMachineLearning,ICML’12,2012.[28]MarkSchmidt,NicolasLeRoux,andFrancisBach.Minimizingﬁnitesumswiththestochasticaveragegradient.arXivpreprintarXiv:1309.2388,pages1–45,2013.PreliminaryversionappearedinNIPS2012.[29]ShaiShalev-ShwartzandTongZhang.ProximalStochasticDualCoordinateAscent.arXivpreprintarXiv:1211.2717,pages1–18,2012.[30]ShaiShalev-ShwartzandTongZhang.Stochasticdualcoordinateascentmethodsforregular-izedlossminimization.JournalofMachineLearningResearch,14:567–599,2013.[31]ShaiShalev-ShwartzandTongZhang.AcceleratedProximalStochasticDualCoordinateAs-centforRegularizedLossMinimization.InProceedingsofthe31stInternationalConferenceonMachineLearning,ICML2014,pages64–72,2014.[32]LinXiaoandTongZhang.AProximalStochasticGradientMethodwithProgressiveVarianceReduction.SIAMJournalonOptimization,24(4):2057—-2075,2014.[33]LijunZhang,MehrdadMahdavi,andRongJin.Linearconvergencewithconditionnumberindependentaccessoffullgradients.InAdvancesinNeuralInformationProcessingSystems,pages980–988,2013.[34]TongZhang.Solvinglargescalelinearpredictionproblemsusingstochasticgradientdescentalgorithms.InProceedingsofthe21stInternationalConferenceonMachineLearning,ICML2004,2004.[35]YuchenZhangandLinXiao.StochasticPrimal-DualCoordinateMethodforRegularizedEmpiricalRiskMinimization.InProceedingsofthe32ndInternationalConferenceonMachineLearning,ICML2015,2015.18