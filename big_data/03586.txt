6
1
0
2

 
r
a

M
 
1
1

 
 
]

.

A
N
h
t
a
m

[
 
 

1
v
6
8
5
3
0

.

3
0
6
1
:
v
i
X
r
a

NUMERICAL LINEAR ALGEBRA WITH APPLICATIONS
Numer. Linear Algebra Appl. 2016; 00:1–26
Published online in Wiley InterScience (www.interscience.wiley.com). DOI: 10.1002/nla

A multigrid perspective on the parallel full approximation scheme

in space and time

Matthias Bolten1, Dieter Moser2∗, Robert Speck2

1Department of Mathematics, Universit¨at Kassel, Germany.

2 J¨ulich Supercomputing Centre, Forschungszentrum J¨ulich GmbH, Germany.

SUMMARY

For the numerical solution of time-dependent partial differential equations, time-parallel methods have
recently shown to provide a promising way to extend prevailing strong-scaling limits of numerical codes.
One of the most complex methods in this ﬁeld is the “Parallel Full Approximation Scheme in Space and
Time” (PFASST). PFASST already shows promising results for many use cases and many more is work
in progress. However, a solid and reliable mathematical foundation is still missing. We show that under
certain assumptions the PFASST algorithm can be conveniently and rigorously described as a multigrid-
in-time method. Following this equivalence, ﬁrst steps towards a comprehensive analysis of PFASST using
block-wise local Fourier analysis are taken. The theoretical results are applied to examples of diffusive and
advective type. Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.

Received . . .

KEY WORDS: parallel-in-time; PFASST; multigrid; local Fourier analysis; high-performance computing

1. INTRODUCTION

Due to the rapid increase of the number of cores of todays and future HPC systems the demand
for new parallelization strategies has grown rapidly in the last decades. When the speedup of a
parallelization of the spatial dimensions is saturated, one general idea is to utilize parallelization of
the temporal dimension. In [1] we ﬁnd a classiﬁcation of such methods, divided into parallelization
across the step, across the method or across the problem.

Direct time-parallel methods mostly belong to the class of parallelization across the method,
examples are certain parallel Runge-Kutta methods [2,3]. Only modest parallel speedup is expected
for these methods, because the number of processing units used for the parallelization are bound by
e.g. the number of Runge-Kutta stage values. Other direct methods for parallel-in-time integration
include RIDC [4], ParaExp [5], tensor-product space-time solvers [6] or methods using Laplace
transformation [7].

If a method decomposes the problem into subproblems which are solvable in a parallel manner
and couples these subproblems using an iterative method,
typically belongs to the class
of parallelizations across the problem. The most prominent example are waveform relaxation
methods [8, 9], which are part of the broad area of domain decomposition methods.

First ideas of parallel-in-time integration date back to Nievergelt in 1964 [10], which belongs
to the class of multiple shooting methods and hence to the class of parallelizations across the
step. More parallel-in-time integration methods were found in the area of multiple-shooting

it

∗Correspondence to: E-mail: d.moser@fz-juelich.de

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls [Version: 2010/05/13 v2.00]

2

methods [11, 12]. Among them, in 2001 by Lions et al., Parareal [13] renewed the interest in
parallel-in-time methods and sparked many new papers in its ﬁeld. The success of Parareal is
accounted to its simplicity and applicability: Only a ﬁne but expensive and a coarse but cheap
propagator in time have to be provided by the user. Then, parallelization across the temporal
dimension can be achieved in an iterative prediction-correction manner. In principle, the number
of processing units is not bounded, but depends on the actual decomposition of the time domain.

Parareal inﬂuenced other methods (see [14]) or even inspired the design of new methods. In [15],
the Parareal approach is coupled to iterative solvers of a collocation problem, the so called spectral
deferred correction (SDC) methods. This approach is extended to the “parallel full approximation
scheme in space and time” (PFASST) in [16]. PFASST adopts and evolves the characteristics of
Parareal by interweaving its iterations with those of the local SDC scheme. In addition, PFASST
uses ideas from the theory of nonlinear FAS multigrid methods.

Multigrid methods in general have a long-standing successful history and a solid mathematical
basis, see e.g. [17]. Regarding parallel-in-time integration, the ﬁrst attempt using multigrid ideas
dates back to Hackbusch in 1984 [18]. Since then, multigrid methods were further developed and
resulted, e.g., in the multigrid waveform relaxation [19, 20], in multigrid reduction-in-time [21], or
in classical space-time-multigrid [22,23]. All these classes are not strictly separated from each other.
Oftentimes methods may be reformulated to ﬁt into a new class. A prominent example is Parareal
itself: it was reformulated as a multiple shooting method as well as a multigrid method in [24],
which in turn paved the way for a comprehensive analysis of Parareal.

This already shows the growing number and diversity of parallel-in-time methods. A classiﬁcation
of PFASST into the diversity of methods contributes to the understanding of PFASST by opening
up the opportunity to use different mathematical tools from different ﬁelds. In particular, multigrid
theory offers a variety of tools such as local Fourier analysis to estimate the convergence and to
obtain a priori error bounds. A mathematical analysis becomes more and more important for the
comparison of these algorithms and the design of algorithms for different applications.

The goal of this work is to associate PFASST with multigrid methods and apply the tools we ﬁnd
in multigrid theory along the lines of two standard problems, namely diffusion and advection. This
sheds light on a general strategy how to estimate the convergence rate of PFASST and hereby the
number of iterations needed to achieve a certain precision.

To achieve this goal, we proceed as follows: In Section 2.1, we we introduce the notation and
preliminaries necessary to state a matrix formulation of PFASST and its constituents. In particular,
we introduce the collocation problem and the notation to deal with the nested multilevel structure
of our setting. On this basis, we introduce the spectral deferred correction and its multi-level
enhancement in matrix form in Section 2.2 and 2.3. Then, we introduce PFASST in algorithmic form
in Section 2.4 which is then converted into matrix form in 3.1 to 3.3. This matrix form facilitates the
use of ideas from multigrid analysis in Section 4 to 4.2, which leads to a block-decomposition of the
iteration matrix of PFASST. In Section 5, we introduce four strategies to estimate the convergence
rate of PFASST. The work is closed with an outlook and a conclusion in Section 6.

2. THE PARALLEL FULL APPROXIMATION SCHEME IN SPACE AND TIME

We start with a brief introduction of the building blocks of PFASST from the perspective of
linear iterative solvers. To this end, we restrict ourselves to linear autonomous ordinary differential
equations and—for the multi-level parts—to two levels only. We will comment on these restrictions
in Section 6.

2.1. Preliminaries and Notation
The starting point is the linear autonomous ordinary differential equation in the Picard formulation

(cid:90) t

U (t) = U0 +

AU (τ )dτ,

t ∈ [t0, T ] ,

(1)

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

t0

where A is a discretized spatial operator, e.g., stemming from a method of line discretization of a
partial differential equation. For the discretization in the temporal dimension the time domain [t0, T ]
is divided into L subintervals. Each subinterval [tl−1, tl], with l ∈ {1, . . . , L}, contains a set of M
nodes {τ1, . . . , τM}. We choose

3

0 = t0 < . . . < tL = T,

tl < τ1 < . . . < τM = tl+1,

∆t = tl+1 − tl, ∆τm = τm+1 − τm.

(2)

Each set of nodes {τ1, . . . , τM} are used as quadrature nodes for the numerical integration with
rules like, e.g., Gauß-Radau or Gauß-Lobatto. Note that the last quadrature node coincides with
the right border of the particular subinterval, which simpliﬁes the formal notation of the algorithm.
The results translate to other quadrature rules with minor modiﬁcations, though. Furthermore, if
a mathematical entity like a set of numerical values or a certain matrix belongs to a subinterval
[tl, tl+1] we denote it e.g. by U [tl,tl+1] (if it is not clear from the context).

Due to the nested structure and the distinct treatment of spatial and temporal dimensions, an
appropriate notation is needed. Continuous functions are always represented by lower case letters,
discretized and semi-discretized functions are the upper case version. Let u(t, x) be a function in
space and time, deﬁned on the domain [t0, T ] × R, with T ∈ R+. For N degrees-of-freedom in
space x1, ..., xN we use the notation

U (t) = (u(t, x1), u(t, x2), . . . , u(t, xN ))T ∈ RN ,

t ∈ [t0, T ]

for semi-discretization in space. A full space-time discretization is denoted as

U [tl−1,tl] = (U (τ1), U (τ2), . . . , U (τM ))T ∈ RM·N ,

U =(cid:0)U [t0,t1], . . . , U [tL−1,T ],(cid:1)T ∈ RM·N·L.

τi ∈ [tl−1, tl], l ∈ {1, . . . , L} ,

On each subinterval a collocation problem is posed. It arises, when quadrature is used as a
numerical counterpart to the integration in (1). The basis for most quadrature formulations is the
interpolation, easily expressed using the Lagrange polynomial basis {(cid:96)i}M

i=1, with

M(cid:89)

(cid:96)i(s) :=

k=1,j(cid:54)=i

s − τk
τi − τk

.

(3)

If we weight each Lagrange polynomial with the evaluation of the function f (t) at the point τi
and sum them up, we get the interpolation polynomial of the function f (t), which is exact on the
nodes {τ1, . . . τM}. Now, quadrature is nothing more than using the exact integration values of the
interpolation polynomial as approximations for the integration of f (t). The following deﬁnition
employs this strategy.
Deﬁnition 1. Let a < τ1 < τ2 < . . . < τM = b be the set of quadrature nodes and Q the quadrature
matrix with entries

(cid:90) τj

qi,j =

(cid:96)i(τ ) dτ,

i, j = 1, ..., M.

a

We discretize (1) at the quadrature nodes, using the matrix Q as approximation of the integral and
obtain this set of linear equations:

U (τi) = U (t0) +

qi,jAU (τi) ,

i = 1, ..., M.

Using the Kronecker product and the vector of ones 1M ∈ RM we write this system of linear
equations as

j=1

U = U 0 + ∆tQ ⊗ AU , with U 0 = 1M ⊗ U (t0),

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

M(cid:88)

4

or, more compactly,

MU =(cid:0)I − ∆tQ ⊗ A(cid:1) U = U 0.

(4)

This problem is called collocation problem on [a, b].

The set of quadrature nodes determines the kind of quadrature. Well-known quadrature rules
are Chebyshev, Gauß-Legendre, Gauß-Radau, and Gauß-Lobatto. These quadrature rules have a
spectral order, which is reﬂected in the high order of the numerical solution of the collocation
problem. Gauß-Radau and Gauß-Lobatto quadrature rules use quadrature nodes which are in
accordance with (2). Due of the higher order we focus on the Gauß-Radau quadrature rule in this
paper.

Finally, the PFASST algorithm is working on a hierarchy of discretizations. As mentioned before,
we focus on the two-level version with spatial coarsening only, i.e. PFASST is solving on a coarse
and a ﬁne level in space. For both levels a separate set of operators and value vectors is needed. The
coarse level versions are simply denoted with a tilde, e.g. ˜A is the coarse level version of A.

2.2. Spectral Deferred Corrections
Instead of directly solving the collocation problem on a subinterval, the spectral deferred corrections
method (SDC) utilizes a low-order method to generate an iterative solution that converges to the
collocation solution U. SDC was ﬁrst introduced by Dutt et al. [25] as improvement of deferred
correction methods [26]. In the last decade, SDC was accelerated with GMRES or other Krylov
subspace methods [27], enhanced to a high-order splitting method [28–30], and found its way
into the domain of parallel respectively time-parallel computing [31, 32], in particular within
PFASST [15, 16].

Regarding the setting of this paper, we cast SDC as a preconditioned Richardson iteration method
for the collocation problem as deﬁned in Deﬁnition 1. This was pointed out earlier by various
authors. For example in the work of Weiser et al. [33] this interpretation was used to optimize
the convergence speed of SDC.

A general preconditioned Richardson iteration, noted as

U k+1 = U k + P−1(c − MU k),

(5)

is fully described by the preconditioner P, the system matrix M, and the right-hand side c of the
linear equation under consideration. P has to be easy to invert, while being an accurate alternative
for the system matrix M. The SDC method follows this approach by replacing the dense quadrature
matrix Q by a lower triangular matrix Q∆. One simple way to generate a lower triangular matrix is
to use the rectangle rule for quadrature instead of the Gauß-Radau rule. In [33] an LU decomposition
of Q provides a Q∆ which results in better convergence properties than the use of the simple
rectangle rule while requiring the same computational effort.

The particular choice

PSDC = I − ∆tQ∆ ⊗ A,

and

c = (U (t0), U (t0), . . . , U (t0))T ∈ RNM ,

(6)

then allows us to write SDC as preconditioned Richardson iteration with system matrix M as
deﬁned in Def. 1 and where the right-hand side is given by the initial values U (t0) of the ODE
spread on each node. If SDC is used on another subinterval than the ﬁrst, the right-hand side
consists of a numerical approximation of U (tl) spread on each node. In order to start the iteration
an initial iteration vector U 0 is needed. For SDC, the right-hand side is an apparent choice for
a initial iteration vector. With these choices, one Richardson iteration is equivalent to one SDC
sweep [33, 34]. The iteration matrix of SDC is simply given by

TSDC = I − P−1

= I −(cid:0)I − ∆tQ∆ ⊗ A(cid:1)−1(cid:0)I − ∆tQ ⊗ A(cid:1) ,

SDCM

(7)

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

5

Note that if we just use the lower triangular part of the Q matrix as Q∆, the preconditioned
Richardson iteration is a Gauß-Seidel iteration. With Q∆ being a simpler integration rule or
stemming from the LU decomposition of Q instead of the lower triangular part of Q, we characterize
SDC as approximative Gauß-Seidel iteration.

2.3. Multi-level Spectral Deferred Corrections
The next step towards PFASST is the introduction of multiple levels in space (and time, which
we will not consider here). This leads to a multi-level spectral deferred corrections method called
(MLSDC), ﬁrst introduced and studied in [35]. Here, SDC iterations (called “sweeps” in this
context) are performed alternately on a ﬁne and on a coarse level in order to shift work load to
coarser, i.e. cheaper, levels. These cheaper levels are obtained, e.g., by reducing the degrees-of-
freedom in space or the order of the quadrature rule in time. Therefore, MLSDC requires suitable
interpolation and restriction operators TF
F , and a coarse-grid correction in order to transfer
information between the different levels. As a consequence, MLSDC can be written as a FAS-
multigrid-like iteration. Like SDC it solves the collocation problem in an iterative manner, using the
same initial iteration vector. For our purpose we derive a two-level version from [35] as:

C and TC

1. Perform nF ﬁne SDC sweep using the values U k according to (5). This yields provisional

updated values U∗.

2. Sweep from ﬁne to coarse:

(a) Restrict the ﬁne values U∗ to the coarse values ˜U
k.
k − TF
CMU∗
(b) Compute the FAS correction τ k+1 = ˜M ˜U
(c) Perform nC coarse SDC sweeps beginning with ˜U

yields new values ˜U

k+1

k and the FAS correction τ k. This

3. Sweep from coarse to ﬁne : Compute the interpolated coarse correction δk and add it to U∗

to obtain U k+1

Note that we use the FAS correction strategy here to match the description of [35]. This is just
a question of notation, because in the linear case using this correction strategy is equivalent to the
standard coarse-grid correction [17]. Note further, that we will only perform one ﬁne and one coarse
SDC sweep in each MLSDC iteration, i.e. nF = nC = 1. The next lemma shows that we can cast
this algorithm as a preconditioned Richardson iteration, too.

C ∈ RNM× ˜N ˜M and TC

F ∈ R ˜N ˜M×NM be the prolongation and restriction
Lemma 1. Let TF
operators which transfer information between the coarse and ﬁne level. We describe the same
problem on a ﬁne space-time grid with the system matrix M and on a coarse space-time grid with
˜M. For both levels we use a preconditioned Richardson iteration method, which is characterized by
P and ˜P to solve MU = c and ˜M ˜U = TC
F c = ˜c, respectively. Then a combination of both methods
using coarse-grid correction can be written as

2 = U k + TF
C

U k+ 1
U k+1 = U k+ 1

˜P−1
2 + P−1

SDC

SDCTC

F

(cid:0)U 0 − MU k(cid:1)
(cid:17)

U 0 − MU k+ 1

2

(cid:16)

It is possible to write (8) in form of (5), using a new preconditioner P MLSDC, where

P −1

MLSDC = TF

C

˜P−1

SDCTC

F + P−1

SDC − P−1

SDCMTF

C

˜P−1

SDCTC
F .

Following (7) and using P SDC and ˜P SDC yields the MLSDC iteration matrix

TF
C

˜P−1

SDCTC

F + P−1

SDC − P−1

SDCMTF

C

˜P−1

SDCTC

F

TSDC = I −(cid:16)

(8)

(9)

(cid:17)

M.

(10)

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

6

Proof
Let U k be the result of the last iteration on the ﬁne level. For the proof we start in the middle of the
algorithm. First we compute the FAS correction

τ k = ˜MTC

F MU k

(11)

(cid:16)

F U k − T C
k(cid:17)

= TC

and use it to modify ˜c for the next iteration on the coarse level. We start the iteration on the coarse
level with

k+1

˜U

k

= ˜U

+ ˜P−1

SDC

˜c + τ k − ˜M ˜U

F U k + ˜P−1

SDCTC

F

(cid:0)c − MU k(cid:1) ,

with the restricted value ˜U

k

and obtain the half-step

= TC

F U k. Then, we compute the coarse correction

(cid:16) ˜U

δk = TF
C

k+1 − TC

F U k(cid:17)
(cid:0)U 0 − MU k(cid:1)

U k+ 1

2 = U k + δk = U k + TF
C

˜P−1TC

F

after some algebraic manipulations. Using this half-step for the next iteration on the ﬁne level gives
(8). Simple algebraic manipulations, after inserting the half-step into the second step, yield the
preconditioner (9), which immediately leads to the iteration matrix (10).

For the matrix formulation it is irrelevant whether the MLSDC step starts with the computation
on the ﬁne or the coarse level. To comply with the literature, we leave the algorithm of MLSDC in
the original order, while changing the order for the matrix formulation.

As a part of PFASST, MLSDC corresponds to the computation performed on each subinterval.
Adding a communication framework between the MLSDC iterations performed on each subinterval
leads to PFASST. However, adding the communication framework yields a structure similar to the
one we have seen in Lemma 1.

2.4. The PFASST algorithm

Figure 1. Schematic representation of the PFASST algorithm with two levels and four processes P0, ..., P3

handling four parallel time steps. Created using pfasst-tikz [36].

The time-parallel PFASST algorithm in its ﬁnal form was introduced in [16] as a combination of
SDC methods [25] with Parareal [13] using an FAS correction strategy to allow for efﬁcient spatial
coarsening along the level hierarchy.

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

coarsesweepﬁnesweepcoarsecomm.ﬁnecomm.P0t0P1t1P2t2P3t3t4computationtimepredictorWe explain PFASST on the basis of the schematic representation in Figure 1. First of all,
we see the time domain, decomposed into subintervals, on the x-axis. On the y-axis we see the
elapsed computational time. Each processor is assigned to a subinterval, where it performs MLSDC
iterations and sends intermediate results on each level to the next processor. The blue and red blocks
represent the SDC sweeps on the coarse and ﬁne level. These blocks are connected through FAS
corrections to the subjacent blocks (red to blue). The arrows represent the communication between
the processors. Before starting with the actual PFASST iterations, a prediction phase, represented
by the ﬁrst blue blocks near the x-axis, computes suitable initial values for the iterations to come.

Based on the schematic representation and the full algorithm description in [16], we state a two-
[tl−1,tl],m be the value on the l-th subinterval at the

level version without the prediction phase. Let U k
k-th iteration and the m-th node. We have

F k

[tl−1,tl] = [AU k

[tl−1,tl],1, . . . , AU k

[tl−1,tl],Ml] and U k

[tl−1,tl] =

U k

[tl−1,tl],1, . . . , U k

[tl−1,tl],Ml

,

(cid:104)

7

(cid:105)

where Ml is the number of nodes on the l-th interval. An upper bar, e.g. ¯U k+1
l−1 , indicates that this
value was sent by the preceding processor. These values are used as a new right-hand side to the
collocation problem on the following subinterval. Denote the initial values for each subinterval as
[tl−1,tl],m. Prepared with this notations, we are ready to formulate the PFASST algorithm:
U 0

1. Go down to the coarse level:

[tl−1,tl] to the coarse values ˜U

(a) Restrict the ﬁne values U k
(b) Compute FAS correction τ k, using ˜F
(c) If l > 0, then receive the new initial value ˜¯U
[tl−1,tl],0, else use the initial value of the ODE.

˜F

k

k

[tl−1,tl] and F k

k

l

k

[tl−1,tl] and compute ˜F

k

[tl−1,tl].

[tl−1,tl].
from processor Pl−1 and compute

(d) Perform nC SDC sweeps with values ˜U

k

[tl−1,tl], ˜F

k

[tl−1,tl] and the FAS correction τ k.

This yields new values ˜U

k+ 1
[tl−1,tl] and ˜F
2

k+ 1
[tl−1,tl].
2

(e) Send ˜U k+ 1

2

[tl−1,tl],Ml

to processor Pl+1 if l < N − 1. This will be received as the new initial

condition ˜¯U k

l for the solver on the coarse level.

2. Return to the ﬁne level:

(a) Interpolate the coarse correction δk = ˜U

k+ 1
2

[tl−1,tl] − ˜U

k

[tl−1,tl] and add to U k

[tl−1,tl],

yielding U k+ 1

[tl−1,tl]. Recompute F k+ 1

[tl−1,tl].

2

2

(b) If l > 0, then receive the new initial value ¯U k

value of the ODE.

(c) Interpolate coarse correction vector δk = ˜¯U k+ 1

2

¯U k+ 1

2

l

. Recompute F k+ 1

2

[tl−1,tl],1.

l−1 from processor Pl−1, else take the initial
l−1 − ˜¯U k

l−1 and add it to ¯U k

l , yielding

3. Perform nF ﬁne SDC sweeps using the values U k+ 1

[tl−1,tl] and F k+ 1

2

2

[tl−1,tl]. This yields values

U k+1

[tl−1,tl] and F k+1

4. Send U k+1

[tl−1,tl],Ml

[tl−1,tl].
to processor Pl+1 if l < N − 1. This will be used as initial value ¯U k+1

l+1 in

the next iteration on the ﬁne level.

This form of the PFASST algorithm is suitable for implementation, but rather not for the
mathematical analysis. It is especially difﬁcult to capture how the parts inﬂuence each other. To
overcome this limitation, we now change the perspective: Instead of building the algorithm in a
“vertical” way (MLSDC on each subinterval), we look at all intervals at once in a “horizontal” way,
i.e., we analyze how the different components of PFASST act on the full time-domain [t0, T ].

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

 ∈ RNM L, with

M[t0,t1]−N M[t1,t2]
0 0

···
···

0 0

...

1
1

...

...

0 0

···

M[t0,T ] =

N =

...
−N M[tL−1,T ]

1

...

 ⊗ IN ∈ RNM .
(cid:40)
 U [t0,t1]

U 0

 =

for l = 0
for l > 0

U 0,
0,

U [t1,t2]

...

...

U [tL−1,T ]

0

0

∈ RNM ,

 = c[t0,T ].

c[tl,tl+1] =

The operator N handles how the new starting value for the upcoming interval is produced.
Furthermore, stacking together

8

3. A MULTIGRID PERSPECTIVE

In this section, the perspective is shifted from solvers on one speciﬁc subinterval to the interaction
of the solvers on the whole time domain [t0, T ]. We begin with stating the composite collocation
problem.
Deﬁnition 2. Let the interval [t0, T ] be decomposed as in (2) into L subintervals [tl, tl+1]. On each
subinterval a collocation problem in the form of (4), denoted by M[tl,tl+1], is posed. The collocation
matrix on the whole time domain is then deﬁned as

form the righ-hand side c[t0,T ] for the composite collocation problem

M[t0,T ]

(12)

Along with the deﬁnition, the block structure of our problem becomes evident. On the diagonal
of the new collocation matrix, we ﬁnd blocks of the size N M, each of them being associated with
the subintervals [tl, tl+1]. The operators on the subdiagonal deal with the communication between
two adjacent subintervals. When designing iterative solvers for the composite collocation problem,
we also want to exploit this block structure. Therefore, the next two sections are dedicated to the
block versions of an approximate Jacobi and a approximate Gauß-Seidel iteration and both will
emerge from the interpretation of SDC as an approximate Gauß-Seidel iterative solver. Later on,
both methods, if correctly interlaced, will yield PFASST.

3.1. Approximative Block Gauß-Seidel solver
The classical Gauß-Seidel solver is a splitting method, which incorporates the lower triangular
part of the system matrix as preconditioner. Obviously this strategy is possible in principle for
the composite collocation problem, as deﬁned in Deﬁnition 2, but this would neglect the particular
block structure of the problem. Therefore, we now construct a block version of the SDC iteration,
following its description as an approximate Gauß-Seidel solver.
Assume we perform one SDC sweep on each subinterval via

U k+1

[tl,tl+1] = U k

[tl,tl+1] + P−1

[tl,tl+1]

[tl,tl+1] − M[tl,tl+1]U k
ck+1

[tl,tl+1]

,

(13)

(cid:16)

(cid:17)

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

9

where P[tl,tl+1] denotes the SDC preconditioner (6), and ck
[tl,tl+1] is the right-hand side on the l-th
subinterval in the k-th iteration. In order to pass the last value forward in time to the next subinterval,
we can use the matrix N. Therefore, the right-hand side of the collocation problem can be written
as

[tl,tl+1] =(cid:2) ¯U k

ck
[t0,t1] = [U0, . . . , U0] ,
l , . . . , ¯U k
l

for

(cid:3) = NU k

ck

l = 0 and k > 0

[tl−1,tl]

for

l > 0 and k > 1

(14)

[tl,tl+1], stemming, e.g., from copying the initial value on each
For some initial iteration vector U 0
node of each subinterval (“spreading”), we can write this process compactly as single approximate
Gauß-Seidel step over the whole time domain.
Lemma 2. Let M[t0,T ] be the matrix of a composite collocation problem. Using (13) on each
subinterval and passing the results via (14), corresponds to

(15)

with

and

U k+1

[t0,T ] = U k

[t0,T ] + P−1

...

[t0,T ]

[t0,t1]

[t1,t2]

U k
U k

 ∈ RNM L,

P[t0,t1]−N P[t1,t2]

[tL−1,T ]

...

U k

[t0,T ]

(cid:0)c[t0,T ] − M[t0,T ]U k
(cid:1) ,
U 0
 ∈ RNM L
 ∈ RNM L×NM L.

c[t0,T ] =

...

0

0

...
−N P[tL−1,T ]

U k

[t0,T ] =

P[t0,T ] =

Proof
We multiply equation (13) with P[tl,tl+1] from the left and equation (15) with P[t0,T ] from the left.
Comparing the resulting terms line by line reveals the equivalence.

This Gauss-Seidel-like iteration can be found in Fig. 1: Here, after each blue block which
l are passed forward in time, providing
represent SDC sweeps on the coarse level, the values ¯U k
new initial values for the sweep on the next interval. Thus, the iteration on the coarse level
can be identiﬁed with an approximate block Gauß-Seidel iteration for the composite collocation
problem (12).

3.2. Block Jacobi-Solver
The communication, emerging from the use of the approximate Block Gauß-Seidel solver, is
blocking. Each processor has to wait for its predecessor. Hence, this is a purely serial approach.
A simple way to avoid the blocking communication is to use a approximate Block Jacobi solver,
omitting the sub diagonal blocks responsible for the communication.

Assume we perform a step similar to (13), but we use the right-hand side

ck
[t0,t1] = [U0, . . . , U0] ,

[tl,tl+1] =(cid:2) ¯U k−1

for
, . . . , ¯U k−1

(cid:3) = NU k−1

l

l

ck

[tl−1,tl]

l = 0 and k > 0

for

l > 0 and k > 1

(16)

instead. This means that not the result of the current but of the previous iteration of the preceding
interval is used. In the ﬁrst iteration, the result of the prediction phase is used. Using the simple
spreading prediction phase, this is easily achieved by choosing ¯U 0

l = U0.

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

10

Lemma 3. Let M[t0,T ] be the matrix of a composite collocation problem. Then, using (13) on each
subinterval and passing the results via (16), corresponds to

U k+1

[t0,T ] = U k

[t0,T ] + ˆP−1

[t0,T ]

(17)

with

ˆP[t0,T ] =

(cid:1)

[t0,T ]

(cid:0)c[t0,T ] − M[t0,T ]U k
 ,

...

P[tL−1,T ]

P[t1,t2]

P[t0,t1]

[t0,T ] and c[t0,T ] deﬁned as in Lemma 2.

as well as U k
Proof
Similar to the proof in Lemma 2 a block line-wise comparison yields the equivalence. Especially,
the inﬂuence of the sub diagonal of M[t0,T ] on the communication is revealed by a block line-wise
view on (17):

(cid:0)U 0 − M[t0,t1]U k
(cid:16)

[t0,t1]

(cid:1) ,

NU k

[tl−1,tl] − M[tl,tl+1]U k

[tl,tl+1]

(cid:17)

for l = 0

,

for l > 1.

U k+1

[t0,t1] = U k
[tl,tl+1] = U k

U k+1

[t0,t1] + P−1
[tl,tl+1] + P−1

[t0,t1]

[tl,tl+1]

The values NU k

[tl−1,tl] are equivalent to 1M ⊗ ¯U k−1

l

.

It is evident that due to the block diagonal structure of ˆP[t0,T ] one block Jacobi iteration may be
performed concurrently on L computing units. This approach corresponds to the sweeps on the ﬁne
(red) blocks in Fig. 1: these sweeps can be performed in parallel, since they do not depend on the
previous subinterval at the same iteration. Therefore, the iteration on the ﬁne level can be identiﬁed
with an approximate block Jacobi iteration for the composite collocation problem (12).

3.3. Assembling PFASST
Already in Section 2.3 multigrid elements where introduced to SDC to form MLSDC. The same
ideas apply when we now interlace both iterative block solvers from above. In order to achieve
more parallelism, we compute the approximate Gauß-Seidel iteration step on the coarse level and
the approximate block Jacobi iteration step on the ﬁne level, so that the more cost intensive work is
done in parallel. As the following Theorem shows, it is now possible to write PFASST in the form
of (8) and we are able to state a iteration matrix.
Theorem 1. Let TC

independently from each other, let(cid:8)P[t0,t1], . . . , P[tL−1,T ]

(cid:9) and(cid:8) ˜P[t0,t1], . . . , ˜P[tL−1,T ]

C be block-wise deﬁned transfer operators, which treat the subintervals

(cid:9) be sets of

preconditioner for the ﬁne and coarse level, respectively, describing SDC sweeps on [tl, tl+1] for
l ∈ {0, . . . , L − 1} and tL = T . Let M[t0,T ] be the composite collocation matrix of Deﬁnition 2 and
N, ˜N be the operations to compute the initial value for the following subinterval. Then the linear
two-level version of PFASST can be written in matrix form as

F and TF

U k+ 1

2

[t0,T ] = U k
[t0,T ] = U k

[t0,T ] + TF
[t0,T ] + ˆP−1

C

U k+1

[t0,T ]

˜P−1

[t0,T ]TC

F

(cid:0)c[t0,T ] − M[t0,T ]U k
(cid:17)

c[t0,T ] − M[t0,T ]U k+ 1

2

[t0,T ]

,

(cid:16)

(cid:1)

[t0,T ]

(18)

F = TC

F N and c[t0,T ] = [U 0, 0, . . . , 0] as well as ˜c[t0,T ] = TC

with ˜P[t0,T ], as in Lemma 2, and ˆP[t0,T ], as in Lemma 3. In addition, let N, ˜N, such that
˜NTC
F c[t0,T ]. Finally, following (7),
the PFASST iteration matrix is given by
I − ˆP−1

˜P−1TC

I − TF

(cid:17)(cid:16)

(cid:17)

(cid:16)

(19)

TPFASST =

F M[t0,T ]

[t0,T ]M[t0,T ]

C

.

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

Proof
We compare systematically each step of PFASST with the sub-computations found in equation (18),
which expands into

11

[t0,T ] = ˜M[t0,T ]TC
τ k
[t0,T ] = ˜U

k+1

k

˜U

U k+ 1

2

[t0,T ] = U k
[t0,T ] = U k+ 1

U k+1

F U k

[t0,T ] + ˜P−1(cid:16)
(cid:16) ˜U
[t0,T ] + ˆP−1(cid:16)

[t0,T ] + TF

C

2

[t0,T ] − TC

F M[t0,T ]U k

[t0,T ]

(cid:17)

˜c[t0,T ] + τ k
[t0,T ] − TC
k+1
c[t0,T ] − M[t0,T ]U k+ 1

[t0,T ] − ˜M[t0,T ]
(cid:17)
F U k

[t0,T ]

2

[t0,T ]

.

(cid:17)

˜U

k
[t0,T ]

(20)

(21)

(22)

(23)

From top to bottom, we have the computation of the FAS correction τ k, the SDC sweep on the
coarse level, coarse-grid correction, and the SDC sweep on the ﬁne level. PFASST’s communication
between the subintervals has been already derived in Lemma 2 and Lemma 3. The evaluations of
right-hand side in the form of F and ˜F are included in the matrix vector multiplication with M[t0,T ]
and ˜M[t0,T ], respectively.

The computation of the FAS correction τ k

[t0,T ] as in (21) differs from the formula (11), which we
derived for MLSDC, i.e. which is formed for each subinterval. The FAS correction vector of (21),
has additional terms:

L = TC

F N − ˜NTC

F

with

τ[t0,T ] =(cid:0)τ[t0,t1], τ[t1,t2] + LU k

[t0,t1], ..., τ[tN−1,T ] + LU k

[tN−1,T ]

(cid:1)T

(24)

(25)

However, by requirement we have L = 0 and in Remark 1 we will investigate how this requirement
is met. The iteration matrix is the result of simple algebraic manipulations.

In contrast to Lemma 1 for MLSDC, we now have an additional requirement.

Remark 1. Let ti,j be the j-th entry of the i-th row of TC
translate to

F . Due to the assumptions above, L = 0

...

0 (cid:80)M
0 (cid:80)M
M(cid:88)

j=1 t1,j
...

 .
∀ i ∈(cid:8)1, . . . , ˜M(cid:9) .

j=1 t ˜M ,j

ti,j

j=1

t ˜M ,1

...
t ˜M ,1

···

···

t ˜M ,M−1

...

t ˜M ,M−1

t ˜M ,M
...
t ˜M ,M

 =

0

...

0

···

···

Hence, we require that

t ˜M ,j = 0 ∀ j ∈ {1, . . . , M − 1}

and

t ˜M ,M =

If the restriction TC
dimension, we infer that,

F of a constant vector yields a constant vector with the same values but a smaller

M(cid:88)

ti,j = 1 ∀ i ∈(cid:8)1, ˜M(cid:9)

j=1

and hence t ˜M ,M = 1. This requirement is met, when the restriction just projects the last node of the
ﬁne level onto the last node of the coarse level. It holds e.g. for the simple linear restriction or just
injection, as long as the quadrature nodes ˜τ ˜M and τM overlap for each subinterval.
Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

12

The hierarchy of discretization on which PFASST is working and the exchange of information
between those levels using FAS and coarse-grid correction obviously indicates a strong similarity
to classical multigrid methods. This relation is in particular emphasized by the iteration matrix.
Standard multigrid methods are typically described and analyzed by their iteration matrix TMG,
which reads

(cid:16)

(cid:17)ν(cid:16)

(cid:17)(cid:16)

(cid:17)µ

TMG(ν, µ) =

I − P−1

postM

I − TF

C

˜M−1TC

F M

I − P−1

preM

,

(26)

for ν post- and µ pre-smoothing steps. The expression in the middle is the coarse grid correction.
In a standard two-grid algorithm, the exact solution ˜M−1 is used at the coarse level. In practice it is
also legitimate to use the approximate solution in form of ˜P−1. PFASST does exactly this. Under
the conditions of Theorem 1, the comparison of (26) and (19) yields that PFASST can be readily
interpreted as multigrid algorithm with one post-smoothing iteration and no pre-smoothing steps.
We point out that this does not prove that PFASST actually behaves like a multigrid method in terms
of convergence and robustness. In particular, properties like smoothing and approximation property
are not necessarily satisﬁed and the analysis of the algorithm in this respect is left for future work.
However, this does not prohibit an analysis based on the tools which are usually used for multigrid
schemes.

4. LOCAL FOURIER ANALYSIS FOR PFASST

The most common tool for analysis and design of multigrid algorithms is the Local Fourier Analysis
(LFA, see e.g. [17]). It simpliﬁes the problem by making assumptions like periodic domains and
constant coefﬁcients. The goal of LFA is, in the rigorous case, the computation and usually the
estimation of the spectral radius of the iteration matrix and its building blocks.

In this work we focus on two prototype problems, namely the diffusion and advection problem in
one dimension, to show how PFASST can be analyzed in principle. We will use periodicity in space
to stay rigorous in that dimension.

The usual approach to LFA is to deﬁne and work with Fourier symbols for each operator. These

Fourier symbols represent the behavior of the operators on the grid functions

ϕθ(x) = exp (iθx/h) ,

x ∈ [0, 1] , θ ∈ [−π, π) ,

(27)

for distinct frequencies θ. The observation, how the different grid functions are damped or changed
on different grids and under different operations is a central point of LFA.

However, in our analysis we will make use of the matrix notation and henceforth avoid the use
of explicit Fourier symbols, but rather perform a block diagonalization of the matrices of PFASST.
The goal is the block-wise diagonalization of the iteration matrix of PFASST. Later on, each block
will be associated with a discrete frequency. Therefore, we will be able to state which frequency is
damped or changed to which extend.

Due to the periodicity in space, parts of the iteration matrix consists of circulant matrices. A
circulant matrix is a special kind of Toeplitz matrix where each row vector is rotated one element to
the right relative to the preceding row vector and denoted as



C =

c0
2 −1
c N

c1

c1
c0
...
···

···

...
2 −1
c N

2 −1
c N

c0

 .

(28)

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

It has the eigenvalues λk and eigenvectors ψk for k = 0, . . . , N − 1

N−1(cid:88)

j=0

1√
N

λk =

ψk =

(cid:18)
(cid:18)

cj exp

(cid:20)

(cid:19)
(cid:19)

k · j

i

2π
N

k · 0

2π
N

exp

i

, exp

and

(cid:18)

(cid:19)

k · 1

i

2π
N

(cid:18)

i

2π
N

(cid:19)(cid:21)T

k · (N − 1)

(29)

.

, . . . , exp

13

(30)

(31)

This also means that with the transformation matrix Ψ, which is orthogonal and consists of the
eigenvectors, it holds

(cid:16)

(cid:17)

ΨT CΨ

= λj.

j,j

For two diagonalizable matrices A, B with the same eigenvector space it holds:

ΨT(cid:0)A + B(cid:1) Ψ = ΨT AΨ + ΨT BΨ = D(A) + D(B),

ΨT ABΨ = ΨT AΨΨT BΨ = D(A)D(B),
ΨT A−1Ψ =

(cid:16)

D(A)(cid:17)−1

Furthermore, for the Kronecker product we have P−1A ⊗ BP = B ⊗ A, where P is a suitable
permutation matrix. Those rules will be used extensively by the following algebraic manipulations.

4.1. Transforming the PFASST iteration matrix
The PFASST algorithm has 3 layers it works on. The ﬁrst layer is the spatial space, the second
consists of the quadrature nodes, and the third is the temporal structure given by the subintervals.
All layers are interweaved: we illustrate this by rewriting the system matrix M[t0,T ] under the
assumption that we have the same problem (i.e. the same discretization of the same operator) on
each subinterval

M[t0,T ] = IL ⊗ IM ⊗ IN − IL ⊗ Q ⊗ A − E ⊗ N ⊗ IN ,

(32)

where N is again the number of degrees of freedom in the spatial dimension, M the number of nodes
per subinterval, and L the number of subintervals. Also, a new operator E ∈ RL×L is introduced,
which has ones on the ﬁrst subdiagonal and zeros elsewhere. In each term the layers are separated
by the Kronecker product, and through the summation of those parts we interweave them again. Our
transformation aims at the layer, where each matrix is diagonalizable by Ψ.

We deﬁne a transformation matrix F, which effects all layers, as

IL ⊗ IN ⊗ ΨT(cid:17) · P−1,
F = P ·(cid:0)IL ⊗ IN ⊗ Ψ(cid:1) , F−1 =
F−1M[t0,T ]F = IN ⊗(cid:0)IL ⊗ IM − E ⊗ N(cid:1) − D(A) ⊗ IL ⊗ Q.

(cid:16)

and therefore

This yields diagonal matrices on the layer for the spatial dimension, so that we can write:

(cid:16)

(cid:17)

F−1M[t0,T ]F = diag
with B(M[t0,T ])

j

B(M[t0,T ])

1

, ..., B(M[t0,T ])

N

= IL ⊗ IM − E ⊗ N − λjIL ⊗ Q.

We call the resulting blocks “time collocation blocks”, highlighting the dimension and components
of the blocks. This idea was recently introduced in [37] in a different notation and is named “semi-
algebraic mode analysis” (SAMA). The motivation behind SAMA is the large gap between the

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

14

theoretical analysis and the actual performance of multigrid methods for parabolic equations and
tine-parallel methods. In [37] Friedhoff et al. demonstrated that SAMA enables accurate predictions
of the short-term behavior and asymptotic convergence factors.

The transformation strategy above leads to a block structure for all matrices which emerge in the
formulation of PFASST, in particular for the iteration matrix. Here, the interpolation and restriction
matrices need special attention, though.

4.1.1. Transforming Interpolation and Restriction In this section we focus on interpolation and
restriction operators, which are designed for two special isometric periodic grids with an even
number of ﬁne grid points. Between these two grids we deﬁne a special class of interpolation and
restriction pairs.
Deﬁnition 3. Let C ∈ RN×N be a circulant matrix, with the associated eigenvalues {λk}k=1... N
and let the ﬁne grid X and coarse grid ˜X be deﬁned as

,

2

X = [x1, . . . , xN ] and ˜X = [˜x1, . . . , ˜xN/2] ,with x2j−1 = ˜xj for all j ∈

Let W(., .) : RN×N × RN×N (cid:55)→ R2N×N be an “interweaving” operator, which stacks together the
rows of two matrices subsequently, beginning with the ﬁrst row of the ﬁrst matrix, followed by the
ﬁrst row of the second matrix and ﬁnally ending with the last row of the second matrix. Then we
deﬁne the class of circulant interweaved interpolation (“CI-interpolation”) operators as

Π =

(cid:110)

(cid:111)
C = W(IN , C)
C : ∃C ∈ RN×N circulant and C · 1 = 1, TF
TF
(cid:27)

(cid:26)

(cid:17)T ∈ Π, c ∈ R

ΠT =

(cid:16)

TC

.

F : c

TC
F

and the class of circulant interweaved restriction (“CI-restriction”) operators as

(33)

(34)

Due to the circulant nature of the interweaved matrices, we are able to state a transformation

analytically.
Lemma 4. Let TF
F the associated CI-restriction operator, Ψ the
transformation matrix for N grid points and ΨC the transformation matrix for N/2 grid points.
Then it holds

C be a CI-interpolation and TC

(cid:26)

(cid:27)

1, . . . ,

.

N
2

and

ΨT TF

CΨC =

d0

1
2

...

ΨT

CTC

F Ψ =





d0

ˆd0

...

...

dN/2−1

ˆdN/2−1

ˆd0

...

dN/2−1

ˆdN/2−1

(35)

(36)

 .

The values on the diagonal depend solely on the circulant matrix C and its eigenvalues λ(C)
k ∈ {0, N/2 − 1}. More precisely, we have
exp(−i 2π
√
2

exp(−i 2π
√
2

1 − λ(C)

1 + λ(C)

ˆdk =

N k)

N k)

dk =

and

k

k

k

.

for

(37)

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

Proof
Using the properties of the interweaving operator we have

C · ΨC = W(I N
TF

2

, C N

2

)ΨC = W(ΨC, C N

2

ΨC).

15

Using the eigenvector eigenvalue relation (29) of the two circulant matrices C and I, see Section 4,
for the computation of

(cid:104)

C · ΨC
TF

(cid:105)

−,k

(cid:114)

2
N

=



exp(i4π/N k · 0)
λk exp(i4π/N k · 0)

...

exp(i4π/N k · (N/2 − 1))
λk exp(i4π/N k · (N/2 − 1))

 .

A comparison to the immediate meaning of (35) demands

(cid:104)

C · ΨC
TF

(cid:105)

!=

dk√
N

−,k

 exp(i2π/N k · 0)

...

exp(i2π/N k · (N − 1))

 +

 exp(i2π/N (N/2 + k) · 0)

...

exp(i2π/N (N/2 + k) · (N − 1))

 .

ˆdk√
N

Solving this system yields (37).

Depending on the structure of C, we are able to state further simpliﬁcations for dk and ˆdk, as we

see in the following remark.

Remark 2. For the special cases where C has a symmetric stencil with

c N

2 −l,
2 −(l+1),
c N
0,

cl =

stencil length odd and l ∈ 1, . . . , m
stencil length even and l ∈ 0, . . . , m − 1
l > m

,

where m ≤ N/4 for the even and m ≤ N/4 − 1/2 for the odd case. Then, it holds for the odd case
dk = dN/2−k and ˆdk = ˆdN/2−k. In addition, for a CI-interpolation and -restriction operator with
C · 1 = 1 we have that λ(C)

0 = 1 and hence d0 = 0 and ˆd0 =

√

2.

(cid:110)

(cid:111)

(cid:110)

We now use Lemma 4 to transform the coarse-grid correction. For the interpolation operator, we
and for the restriction operator the diagonal

d0, ˆd0, . . . , dN/2−1, ˆdN/2−1

obtain diagonal entries

f0, ˆf0, . . . , fN/2−1, ˆfN/2−1

entries
. These entries may coincide if the same circulant matrix
C is used for the construction of both operators. Furthermore, we transform the inverse of the
system matrix ˜A−1 in the spatial dimension into a diagonal matrix consisting of the eigenvalues

(cid:111)

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

˜A−1ΨCΨT

CTC

F Ψ



f0

...

˜λ N

2 −1

ˆf0

...

...

f N

2 −1



ˆf N

2 −1

 .

16(cid:8)˜λ0, . . . , ˜λN/2−1

ΨT TF
C

˜A−1TC

(cid:9) of ˜A−1. Then we obtain
˜λ0

F Ψ = ΨT TF
d0

CΨCΨT

2 −1

...

d N

C

ˆd0



...

ˆd N

2 −1




=

1
2

=

1
2

d1˜λ0f0

d0˜λ0 ˆf0

...

...

d N

2 −1

˜λ N

2 −1f N

2 −1

ˆd0˜λ0f0

ˆd0˜λ0 ˆf0

...

...

˜d N

2 −1

˜λ N

2 −1f N

2 −1

d N

2 −1

˜λ N

2 −1

ˆf N

2 −1

ˆd N

2 −1

˜λ N

2 −1

ˆf N

2 −1

The values are now scattered over 3 diagonals. By using the appropriate permutation matrix we can
gather them to new blocks:

P−1ΨT TF

C

˜A−1TC

(cid:16)
(cid:18)dl˜λlfl
F ΨP = diag

where Bl =

B0, . . . , B N

ˆdl˜λlfl
ˆdl˜λl ˆfl

dl˜λl ˆfl

(cid:17)

(cid:19)

,

2 −1
∈ R2×2.

(38)

(39)

In this structure we ﬁnd the classical mode-mixing property of interpolation and restriction
operators. This well-known property of standard multigrid iterations interweaves pairs of one low
and one high frequency, the“harmonics”.

4.1.2. Transforming the full
iteration matrix The iteration matrix of PFASST can now be
transformed into a block matrix with N/2 blocks of the size M · L. Each block is associated with
a harmonic of the spatial problem and therefore with one high and one low frequency. In contrast,
the smoother alone is decomposed into N blocks, which may be associated with only one single
frequency. This is summarised in the following theorem.
Theorem 2. Let us have a iteration matrix in the form of (19) with
˜P−1TF

I − P−1M

I − TF

(cid:17)(cid:16)

(cid:17)

(cid:16)

T =

,

C

CM

F−1TF = diag

, . . . ,B(S)

N

C are two circulant interweaved transfer operators and
where M is the collocation matrix, TC
P, ˜P are two preconditioner with a matrix in the spatial layer, which is diagonalisable and has the
same eigenvector space as the spatial system matrix A. Then there exists a transformation F so that

F , TF

0

(cid:16)B(S)
I −(cid:16)
(cid:17)−1
0 B(CGC)
B(P )
I − fkdk
(cid:16)
(cid:16)

B( ˜P )
B( ˜P )

− ˆdkfk

k

k

k

B(M )

k

(cid:17)−1
(cid:17)−1

N

(cid:17) ∈ RLM N×LM N , with
 ∈ R2LM×2LM
2 −1B(CGC)
2 −1
(cid:17)−1
I −(cid:16)
(cid:16)
(cid:17)−1
(cid:17)−1
(cid:16)

B(P )
N
2 +k
− ˆfkdk
I − ˆfk ˆdk

B( ˜P )
B( ˜P )

B(M )
N
2 +k

B(M )
N
2 +k
B(M )
N
2 +k

k

k

B(M )

k
B(M )

k

 ∈ R2LM×2LM ,

(40)

(41)

(42)

B(S)
k =

B(CGC)

k

=

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

17

(cid:16)
(cid:16)
(cid:16)

(cid:17)
(cid:17)
(cid:17)

with matrices B(P )
0 . . . N

k

2 − 1, solely depending on the eigenvalues of A and ˜A. Where

, B(M )

k

∈ RLM×LM for k = 0 . . . N − 1 and B( ˜P )

k ∈ RLM×LM for k =

ΨT PΨ = diag

ΨT MΨ = diag

ΨT ˜PΨ = diag

B(P )

0

B(M )

0

B( ˜P )

0

, . . . , B(P )
N−1
, . . . , B(M )
N−1
, . . . , B( ˜P )
2 −1

N

, with B(P )

j = IL ⊗ IM − E ⊗ N − λ(A)
j = IL ⊗ IM − E ⊗ N − λ(A)
j = IL ⊗ IM − E ⊗ N − λ( ˜A)

j IL ⊗ Q∆,
j IL ⊗ Q,
j IL ⊗ Q∆.

, with B(M )

, with B( ˜P )

(43)

We call B(M )
approximating Q, see Section 2.2.

and B( ˜P )

, B(P )

j

j

j

basic blocks. The matrix Q∆ ∈ RM×M is a lower triangular matrix

Proof
The proof consists of straightforward computations. The matrices P, M and ˜P have 3 layers,
separated by Kronecker products like in (32). Applying the transformation in the spatial dimension
leads to the basic blocks (43). Similar to (39), we choose the adequate permutation matrices on the
layers of subintervals and quadrature nodes, to get the blocks of harmonics. Also, each block of
the post smoother is associated with a mode, hence we stack harmonic pairs together to B(S)
in
order to match them with the blocks of the coarse grid correction B(CGC)
, performed by the same
permutation matrix.

k

k

k B(CGC)

This theorem makes it possible, at least semi algebraically, to analyze the convergence properties
of PFASST by computing the spectral radius of each block B(S)
. Until this point the choice
of the particular problem and the operators yields a rigorous transformation. Hence, the blocks and
the full iteration matrix of PFASST have exactly the same eigenvalues. This translates to computing
N/2 eigenvalues of matrices of the size 2M L × 2M L. As we can see in (43), the basic blocks
consists of IL and E on the ﬁrst layer. However, it is not directly possible to apply the transformation
strategy presented above to this layer. For an empirical study like LFA, though, only estimates of
the spectral radii are needed. This is mainly due to the fact, that even the exact spectral radius does
not reﬂect the direct numerical behavior of the method exactly, but rather asymptotically. In the
following section we therefore give up the rigorousness of the transformation in order to ﬁnd a
decomposition of the basic blocks into L blocks of the size 2M × 2M.

k

4.2. Assuming periodicity in time
To enable the further decomposition of the basic blocks, we exchange in the matrix formulation

E =

 ,

1

0

0
0

···

...
1

···

...
1

...

1 0

0 0
(cid:104)

0

0

...

1

0

0

0

 with ˆE =
(cid:18)
(cid:105)

= exp

j,j

(cid:19)

,

−i2π

j
L

Ψ−1 ˆEΨ

which introduces time periodicity to the problem and makes the matrix circulant. Hence, it becomes
easy to transform

and B( ˜P ) further decomposable into N L or N L/2 blocks
which makes the basic blocks B(M )
of the size M × M or 2M × 2M, respectively. This leads directly to the following Theorem that
can be proved using straightforward computations similar to the ones used before.

, B(P )

j

j

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

18

ˆF−1T ˆF = diag

with B(S)

k,j =

B(CGC)

k,j

=

Theorem 3. Let us have the identical requirements as in Theorem 2, except the use of ˆE instead of
E. Then there exists a transformation F such that

0,0

(cid:16)B(S)
I −(cid:16)
(cid:17)−1
0,0 · B(CGC)
B(M )
B(P )
k,j
k,j
I − fkdk
(cid:16)
(cid:16)

(cid:17)−1
(cid:17)−1

B( ˜P )
B( ˜P )

− ˆdkfk

k,j

k,j

,B(S)

0,1 · B(CGC)

0,1

I −(cid:16)

N
2 +k,j

B(P )
− ˆfkdk
I − ˆfk ˆdk

B(M )
k,j
B(M )
k,j

N

,

2 −1,L−1

· B(CGC)

(cid:17)
 ∈ R2M×2M and
 ∈ R2M×2M ,

B(M )

N/2+k,j
B(M )

N/2+k,j

2 −1,L−1

N

, . . . ,B(S)
(cid:17)−1
(cid:16)
(cid:16)

B( ˜P )
B( ˜P )

k,j

k,j

B(M )

N
2 +k,j

(cid:17)−1
(cid:17)−1

(44)

(45)

(46)

with matrices B(P )
k = 0 . . . N

k,j , B(M )

k,j ∈ RM×M for k = 0 . . . N − 1, j = 0 . . . L − 1 and B( ˜P )
k,j ∈ RM×M for
(cid:18)
2 − 1, j = 0 . . . L − 1, solely depending on the eigenvalues of A and ˜A, with
(cid:18)
−i2π
(cid:18)

(cid:19)
(cid:19)
(cid:19)

k ∆tQ∆ + exp

0,0 , . . . , B(M )

0,0 , . . . , B(P )

k ∆tQ + exp

, with B(M )

, with B(P )

k,j = I − λ(A)
k,j = I − λ(A)
k,j = I − λ( ˜A)

0,0 , . . . , B( ˜P )

−i2π

j
L
−i2π

k ∆tQ∆ + exp

(cid:17)
(cid:17)
(cid:17)

, with B( ˜P )

(cid:16)
(cid:16)
(cid:16)

N−1,L−1

N−1,L−1

B(M )

B( ˜P )

B(P )

N,

N,

N.

j
L

N

2 −1,L−1

j
L

ΨT PΨ = diag

ΨT MΨ = diag

ΨT ˜PΨ = diag

We denote those blocks as “collocation blocks” in contrast to the time-collocation blocks of
Theorem 2.

This leaves us with N L/2 blocks of the size 2M × 2M. We identify the matrices Q and Q∆ as
the atomic part of the whole matrix formulation. Further decompositions may only be performed if
a decomposition of Q is found. In the case of a Q ∈ R1×1, the time stepping part reduces to e.g.
an implicit Euler. In this case no eigenvalue computations are necessary any more and the Fourier
symbols are easily derived from the basic collocation blocks.

(47)

Remark 3. With the assumption of periodicity in time we loose the initial value, which means
that if u(t, x) is a solution of the problem then u(t, x) + c is also a solution for any c ∈ R. Hence,
the inverses of B(P )
k,0 do not exist and neither do the inverses of iteration matrix blocks
B(T )
k,0 = B(S)
k,0 · B(CGC)
k,0 to 0. This blocks belong to
constant modes and we assume that there are no constant error modes which have to be damped.

exist. Our remedy for this problem is to set B(T )

k,0 and B( ˜P )

k,0

Based on this transformation of the iteration matrix, we are now able to investigate the behavior

of PFASST for two standard model problems in the following section.

5. NUMERICAL EXPERIMENTS

In this section we show how the convergence properties of PFASST may be examined along the
lines of two examples, namely the diffusion and the advection problem. Within this paper though, a
full analysis of the inﬂuence all the parameters like N, L, M, ∆t, the choice of the quadrature rule
or the PDE parameters is not possible. Therefore, the experiments presented here do not aim for a
complete analysis, they should rather be viewed as a recipe to analyze PFASST for a certain class
of problems, deﬁned by the requirements we posed for the theoretical results above.

All computations are performed with N = 128 degrees of freedom in space. For matrices and
vectors, the inﬁnity norm is used. For the advection problem, we will use the SDC algorithm with

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

19

the LU-based preconditioner Q∆ as in [33], while for the diffusion problem Q∆ is the standard
implicit Euler method. For all experiments we use M = 5 Gauß-Radau nodes on each of the
L = 4 subintervals of the length dt = 0.1. Hence, we have T = 0.4. The interpolation is constructed
such that polynomials up to order 6 are interpolated exactly and the restriction is gained from an
interpolation operator which interpolates polynomials up to the order of 2.

Our main goal will be the estimation of the error by using the block form of the iteration matrix

of PFASST. With blocks Bk, the computation of the norm of a matrix H reduces to

(cid:13)(cid:13)H(cid:13)(cid:13)2

= sup
xk(cid:54)=0

(cid:80)m
(cid:80)m
k=1 (cid:107)Bkxk(cid:107)2
k=1 (cid:107)xk(cid:107)2 = max

k

(cid:107)Bkxk(cid:107)2
(cid:107)xk(cid:107)2 = max

k

sup
xk(cid:54)=0

(cid:13)(cid:13)Bk

(cid:13)(cid:13)2

,

(48)

see [17] for a proof. The same holds for the computation of the spectral radii. In addition, the effort
of computing the eigenvalues of N/2 time-collocation blocks of the size 2LM × 2LM is obviously
less than for a M LN × M LN matrix. With the assumption in Section 4.2 it even reduces to the
computation of N L/2 collocation blocks of the size 2M × 2M.

For both cases (time collocation and collocation blocks) we consider the following strategies for

the estimation of the error vector eκ of the κ iteration

1. use the spectral radius ρ(T) of the iteration matrix
2. use the norm of the iteration matrix (cid:107)T(cid:107)
3. use the norm of the κ-th potency of the iteration matrix (cid:107)Tκ(cid:107)
4. apply κ-th times the iteration matrix to the known error vector
The ﬁrst strategy is based on the inequality for consistent matrix norms (cid:107) · (cid:107) and each κ ∈ N

ρ(A) ≤ (cid:107)Aκ(cid:107) 1
κ ,

see [38]. Strategies 2 and 3 rely on the inequality

(cid:13)(cid:13)Tκe0(cid:13)(cid:13) ≤(cid:13)(cid:13)Tκ(cid:13)(cid:13)(cid:13)(cid:13)e0(cid:13)(cid:13) ≤(cid:13)(cid:13)T(cid:13)(cid:13)κ(cid:13)(cid:13)e0(cid:13)(cid:13) .

(cid:107)eκ(cid:107) =

(49)

(50)

Note that the iteration matrix is separated from the initial error vector e0, and therefore an a priori
estimation of the relative error reduction is possible for this strategies. In contrast, the error vector
e0, i.e. the analytical solution has to be known for strategy 4, making it an a posteriori strategy. If
time collocation blocks are used, the computation following strategy 4 yields the analytically correct
error for each iteration. Using collocation blocks, this approach just provides another estimate.

5.1. Diffusion problem
The elliptic Poisson problem is often used in the multigrid literature to demonstrate the basic ideas
of multigrid, see e.g. [17]. Hence, the time-dependent, parabolic version of it, i.e. the classical heat
equation, is a canonical candidate for the analysis of a multigrid-like time integration method like
PFASST.

The problem in one spatial dimension is given by

ut = ν∆u,
u(x, 0) = u0(x),

x ∈ [0, 1] and t ∈ [0, T ]
u(0, t) = u(1, t),

t ∈ [0, T ]

(51)

for a time T > 0 and the diffusion coefﬁcient ν > 0. Using second-order ﬁnite differences on a
isometric grid we get a simple discretization in the spatial dimension with

X = [x1, . . . , xN ], with xj =

j − 1
N

and ∆x =

1
N

(52)

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

20

which leads to a system of linear ODEs

Ut(t) = AU (t),



t ∈ [0, T ] and U (0) = [u(x1, 0), . . . , u(xN , 0)],
2 −1
−1
0

··· −1

0
2 −1
...
...
−1
··· −1

...
0
2 −1
2

0

 ∈ RN×N .

...
−1

with A =

ν

(∆x)2

Because the matrix A is circulant, the spectral decomposition in eigenvalues and eigenvectors is
easily computed. For the eigenvalues λk and normal eigenvectors ψk, k ∈ {0, . . . , N − 1}, we have

(cid:20)

(cid:18)

(cid:19)

(cid:18)

(cid:19)(cid:21)T

λk =

4ν
∆x2 sin2

and ψk =

1√
N

exp

i

, . . . , exp

k · 0

2π
N

k · (N − 1)

i

2π
N

(cid:18) kπ

(cid:19)

N

(53)

.

(54)

(55)

5.1.1. The error vector For initial values given by the function
k ∈ {1, . . . , N − 1} ,

x ∈ [0, 1] ,
we know that solution to our PDE with periodic boundary conditions is given by

u0(x) = sin (2πxk) ,

u(t, x) = exp

(cid:16)−ν (2πk)2 t
(cid:17)
1 − exp(−ν(2πk)2t0 + τ1)
 ⊗

1 − exp(−ν(2πk)2T )

...

sin (2πkx) .

 sin(2πkx1)

...

 .

sin(2πkxN )

e0 =

Usually the PFASST algorithms starts with an vector where the initial value is spread on each node,
i.e. we have the initial error vector

With the iteration matrix we compute the succeeding error vector for PFASST as

Teκ = eκ+1.

Like the iteration matrix, the error vector ek of the k-th iteration itself can be transformed and
decomposed into parts belonging to a certain mode and associated with the TC-block of the iteration
matrix. We can write

F−1TFF−1eκ = F−1eκ+1.

The transformed error is thus ˆeκ = F−1eκ, following precisely the transformation procedure
described in Section 4.1. The initial value function u0(x) decomposes into two modes, which are
represented by spatial Fourier space functions

(cid:18)

(cid:19)

(cid:18)

(cid:19)

− 1
2i

exp

−i

2π
N

kxj

sin(2πkxj) =

=

1
√
2i

exp

i

2π
N
[ψk]j −

N
2i

kxj
√

N
2i

[ψN−k]j

and belong to two different harmonics. This reduces our analysis to the blocks belonging to
2 −k of sizes 2LM for k = 0, ..., N/2 − 1 in the case
these certain harmonics, which are B(T )
time-collocation blocks are considered and B(T )
k,j ,B(IT )
2 −k,j of sizes 2M for k = 0, ..., N/2 − 1 and
j = 0, ..., L − 1 if collocation blocks are considered.

,B(T )

k

N

N

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

21

(a) time-collocation blocks

(b) collocation blocks

Figure 2. The errors estimates from the strategies 1 to 3, compared to the actual error plotted against the

number of iterations of PFASST, for the diffusion problem with an initial value function of sin(2π8x).

5.1.2. Error prediction We choose ν so that µ = 10. In Figure 2, observing the solid line of
the actual error measured during the iterations, we ﬁrst see a short-term convergence behavior
until roughly 10−4 which is then followed by a much slower, long-term convergence phase. We
see that the use of the norm of the k-th potency of the iteration matrix T (strategy 3) is well-
suited to capture the long-term convergence behavior. This is of course also true for strategy 1,
using the spectral radius of the iteration matrix. Similar plots for various initial value functions
sin (2πkx) for k ∈ {1, . . . , N/2 − 1} , were inspected and showed the same behavior for the long-
term convergence. In particular, there is no signiﬁcant difference between time-collocation and
collocation blocks. However, the norm of the iteration matrix is greater than 1 for most cases, as
a survey over different µ ∈ (0.01, 100) and L ∈ {2, . . . , 50} showed. This renders strategy 2 useless
for most of the cases we considered so far.

The short-term convergence on the other hand is not captured by the ﬁrst 3 strategies. In contrast,
strategy 4 does this very well, as we see in Figure 3. We also see that the short-term convergence
is faster for initial values with a small wave number k and that the long-term convergence speed
is almost independent from the initial value. Our interpretation is that PFASST is more efﬁcient
in reducing the low frequency error modes in space. After the ﬁrst convergence phase, the error
consists of a mixture of modes, which is reduced by PFASST likewise, independently from the
initial value frequency.

Here we actually see a difference between the different types of blocks: For the error prediction
of the ﬁrst iterations, using strategy 4 with collocation blocks is not as accurate as using time
collocation blocks. In contrast, no differences in the quality of the error prediction are notable in the
long-term convergence phase, again.

5.2. Advection problem
The second prototype problem is the 1D advection equation, given by

ut = cux,

u(x, 0) = u0(x),

x ∈ [0, 1] and t ∈ [0, T ]
u(0, t) = u(1, t)

t ∈ [0, T ],

(56)

with advection coefﬁcient c > 0. The discretization is done in the same manner as (53), but we
use an upwind difference stencil of the order 3, instead of a central difference stencil. This yields
again a circulant matrix FD, with eigenvalues and eigenvectors according to (29). For the numerical
experiments we use advection speed c = 4.88 · 10−3, resulting in a CFL number of 62.5 · 10−3. The
discretization in space and time is similar to the discretization in the previous section.

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

01020304050iterations10-1110-910-710-510-310-1101103105errorerrorspectral radius||T||||T||01020304050iterations10-1110-910-710-510-310-1101103105errorerrorspectral radius||T||||T||22

(a)

(c)

(b)

(d)

Figure 3. Strategy 4 for various k, using time collocation blocks on the left and collocation blocks on the
right for the diffusion problem. In the second row the difference between the error and the error prediction

is plotted.

5.2.1. The error vector For a initial value function u0 the solution reads

u(t, x) = u0(x − ct).

We use again the initial values given by (55). The initial values are spread on each node, this yields
the initial error vector

e0 =(cid:0)e0

1, ..., e0
L

(cid:1)T

with

n =(cid:0)e0

(cid:1)T

n,1, ..., e0

e0
j,m = (u0(x1) − u0(x1 − c(tj−1 + τm)), ..., u0(xN ) − u0(xN − c(tj−1 + τm)))
e0

n,M

When the class of initial value (55) is used, the initial values can be decomposed again into two
modes, belonging to different harmonics. The analysis is thus again reduced to certain harmonic
blocks B(T )
2 −k,j, respectively. Note, that this computation of the error vector
works even if only a numerical solution to the problem is given.

2 −k and B(T )

k,j ,B(T )

,B(T )

N

k

N

5.2.2. Error prediction For the advection problem the four strategies yield signiﬁcantly different
results than for the diffusion problem. In Fig. 4 we now observe three phases of convergence: two
rapid phases at the beginning and at the end and one almost stagnating phase in the middle. We
observed these phases for all initial wave numbers κ, with the peculiarity of a decreasing, almost
vanishing ﬁrst phase for increasing κ. Regarding the different strategies, we see that only the spectral

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

01020304050iterations10-1310-1210-1110-1010-910-810-710-610-510-410-310-210-1100101errork = 2k = 8k = 32PFASST01020304050iterations10-1310-1210-1110-1010-910-810-710-610-510-410-310-210-1100101errork = 2k = 8k = 32PFASST01020304050iterations10-1910-1710-1510-1310-1110-910-710-510-310-1101error differencek = 2k = 8k = 32k = 6301020304050iterations10-1310-1210-1110-1010-910-810-710-610-510-410-310-210-1100101error differencek = 2k = 8k = 32k = 63radii (strategy 1) is able to capture the ﬁrst phase, while the norm of the powers of the iteration matrix
(strategy 3) captures the last phase.

23

(a) time-collocation blocks

(b) collocation blocks

Figure 4. The errors estimates from the strategies 1 to 3, compared to the actual error plotted against the
number of iterations of PFASST. Advection problem with an initial value function of u0(x) = sin(2π8x).

Again strategy 4 is successful in exactly predicting the error, when time collocation blocks are
used. On the other hand, for the advection equation the use of collocation blocks only serve as an
assessment for initial values with high k, and then only for the ﬁrst phase. Obviously, the assumption
of periodicity in time is not valid for advection-dominated problems. Thus, time collocation blocks
should be considered in this case.

6. CONCLUSION AND OUTLOOK

In this paper we decomposed the PFASST algorithm into its atomic parts. Using analogies to
classical iterative methods like Gauß-Seidel and Jacobi, we described PFASST for two levels and
linear problems as a combination of a highly parallel, approximative block Jacobi solver on the
ﬁne level and a serial, approximative block Gauß-Seidel solver on the coarse level. With this we
could show that for linear problems PFASST is a multigrid algorithm for the composite collocation
problem in space and time. We stated the underlying composite collocation problem in matrix
formulation, spanning the full domain in space and time, and decomposed it into three layers: spatial
decomposition, time-stepping and quadrature nodes. With suitable transformations, we could show
the similarity of PFASST’s iteration matrix to a block-diagonal matrix, containing either N/2 time-
collocation blocks of size 2M L or N L/2 collocation blocks of size 2M. While in the ﬁrst case the
analysis is rigorous, in the second case periodicity in time is assumed.

We identiﬁed 4 different strategies to test the convergence properties of PFASST using the block
diagonalization of the iteration matrix. Along the lines of two prototype problems, we investigated
the quality of the predictions given by these strategies compared to the numerical results form
PFASST. We explored the effect of PFASST on different modes of the solution, depending on the
initial values.

With a suitable measure for the convergence speed of PFASST at hand, the central next step
would be to estimate the parallel performance of this algorithm in comparison to serial SDC runs.
To this end, block diagonalizations of PFASST and SDC can be compared following the strategies
presented in this work. This would augment the current speedup considerations of PFASST as stated
in [16] by providing estimates for the actual iterations counts. In addition, we have identiﬁed the
following topics as relevant for further studies.

Detailed parameter and component studies. So far, we have only investigated simple 1D
problems, demonstrating how the LFA of the iteration matrix can be used to predict the convergence

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

0246810121416iterations10-1110-910-710-510-310-1101103105errorerrorspectral radius||T||||T||0246810121416iterations10-1110-910-710-510-310-1101103105errorerrorspectral radius||T||||T||24

(a)

(c)

(b)

(d)

Figure 5. Strategy 4 for various k, using time collocation blocks on the left and collocation blocks on the
right for the advection problem. In the second row the difference between the error and the error prediction

is plotted.

behavior of PFASST for different situations. These examples can serve as a blueprint for a much
deeper and more detailed analysis of PFASST’s convergence properties for various problems. Also,
the matrix formulation of PFASST allows us to exchange parts more easily. We can test other
smoothers than SDC, change the quadrature rules used on the subintervals, vary interpolation and
restriction on space (and even time) and apply iterative solvers like standard multigrid in space for
inverting the spatial operators.

Non-linear functions. We restricted our self to linear problems in order to apply the Local Fourier
Analysis. However, the notation used is derived from the Full Approximation Scheme and therefore
is also applicable to non-linear right-hand sides. Meaning that we use a non-linear function f (U (τ ))
of AU (τ ), also meaning that most matrices are exchanged by operators. These changes make a
convergence analysis more difﬁcult.

Extension to multiple levels.
In contrast to Parareal, the PFASST algorithm is designed to use
more than two levels. Due to the simpliﬁcation of the notation and the rigor of the argumentation
chain, this fact was not exploited. For the same reasons, the interpolation and restriction matrices
effected only the spatial dimension, although it is possible to construct coarse levels with less
quadrature nodes than on the ﬁne level. The effects on the formalism in Section 4 would be minor.
It is another story, if a coarse level is constructed where two or more subintervals from the ﬁne level
are merged to one. This would be a step in the direction of full Space-Time MultiGrid, but some
work is needed to adjust the formalism in Section 4 for a similar convergence analysis. Another
step towards full ST-MG would be the use the exact solution on the coarsest level instead of one or

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

02468101214iterations10-1110-1010-910-810-710-610-510-410-310-210-1100101errork = 4k = 8k = 16PFASST02468101214iterations10-1110-1010-910-810-710-610-510-410-310-210-1100101errork = 4k = 8k = 16PFASST02468101214iterations10-1510-1410-1310-1210-1110-1010-910-810-710-610-510-410-310-2error differencek = 4k = 8k = 1602468101214iterations10-610-510-410-310-210-1100error differencek = 4k = 8k = 1625

more SDC sweeps, but ﬁrst brief experiments showed no signiﬁcant difference between the use of
the exact solution or the use of SDC Sweeps.

Rigorous convergence analysis. The usual attempt in Multigrid theory for a rigorous convergence
analysis contains the proof of the smoothing and approximation property. Both endeavors are
difﬁcult on their own, but, in our case, are further impeded by the matrices Q, Q∆. These matrices
are dense and yield no obvious structural properties, which could be exploited. First steps towards
a more rigorous analysis would be to resolve this problem.

REFERENCES

1. Burrage K. Parallel methods for ODEs. Advances in Computational Mathematics 1997; 7:1–3. URL http:

//dx.doi.org/10.1023/A:1018997130884.

2. Iserles A, Nørsett S. On the theory of parallel Runge-Kutta methods. IMA Journal of numerical Analysis 1990;

3. Butcher J. Order and stability of parallel methods for stiff problems. Advances in Computational Mathematics

10(4):463–488.

1997; 7(1):79–96.

4. Christlieb AJ, Macdonald CB, Ong BW. Parallel high-order integrators. SIAM Journal on Scientiﬁc Computing

2010; 32(2):818–835. URL http://dx.doi.org/10.1137/09075740X.

5. G¨uttel S. A parallel overlapping time-domain decomposition method for ODE’s. Domain Decomposition Methods

in Science and Engineering XX. Springer, 2013; 459–466.

6. Maday Y, Rønquist EM. Parallelization in time through tensor-product space-time solvers. Comptes Rendus

Mathematique 2008; 346(1–2):113 – 118.

7. Sheen D, Sloan IH, Thom´ee V. A parallel method for time discretization of parabolic equations based on Laplace

transformation and quadrature. IMA Journal of Numerical Analysis 2003; 23(2):269–299.

8. Gander MJ. A waveform relaxation algorithm with overlapping splitting for reaction diffusion equations. Numerical

linear algebra with applications 1999; 6(2):125–145.

9. Vandewalle S, Roose D. The parallel waveform relaxation multigrid method. Parallel Processing for Scientiﬁc

Computing 1989; :152–156.

10. Nievergelt J. Parallel methods for integrating ordinary differential equations. Commun. ACM 1964; 7(12):731–733.

URL http://dx.doi.org/10.1145/355588.365137.

11. Chartier P, Philippe B. A parallel shooting technique for solving dissipative ODE’s. Computing 1993; 51(3-4):209–

236. URL http://dx.doi.org/10.1007/BF02238534.

12. Bellen A, Zennaro M. Parallel algorithms for initial-value problems for difference and differential equations.
Journal of Computational and Applied Mathematics 1989; 25(3):341 – 350. URL http://dx.doi.org/10.
1016/0377-0427(89)90037-X.

13. Lions JL, Maday Y, Turinici G. A ”parareal” in time discretization of PDE’s. Comptes Rendus de l’Acad´emie
- Mathematics 2001; 332:661–668. URL http://dx.doi.org/10.1016/

des Sciences - Series I
S0764-4442(00)01793-6.

14. Gander M, Jiang YL, Li RJ. Parareal Schwarz waveform relaxation methods. Domain Decomposition Methods in
Science and Engineering XX, Lecture Notes in Computational Science and Engineering, vol. 91, Bank R, Holst M,
Widlund O, Xu J (eds.). Springer Berlin Heidelberg, 2013; 451–458. URL http://dx.doi.org/10.1007/
978-3-642-35275-1_53.

15. Minion ML. A hybrid parareal spectral deferred corrections method. Communications in Applied Mathematics
and Computational Science 2010; 5(2):265–301. URL http://dx.doi.org/10.2140/camcos.2010.5.
265.

16. Emmett M, Minion ML. Toward an efﬁcient parallel

in time method for partial differential equations.
Communications in Applied Mathematics and Computational Science 2012; 7:105–132. URL http://dx.doi.
org/10.2140/camcos.2012.7.105.

17. Trottenberg U, Oosterlee CW, Schuller A. Multigrid. Academic press, 2000.
18. Hackbusch W. Parabolic multigrid methods. Computing Methods in Applied Sciences and Engineering, VI 1984;

:189–197URL http://dl.acm.org/citation.cfm?id=4673.4714.

19. Lubich C, Ostermann A. Multi-grid dynamic iteration for parabolic equations. BIT Numerical Mathematics 1987;

27(2):216–234. URL http://dx.doi.org/10.1007/BF01934186.

20. Vandewalle S, Van de Velde E. Space-time concurrent multigrid waveform relaxation. Annals of Numer. Math 1994;

21. Falgout R, Friedhoff S, Kolev TV, MacLachlan S, Schroder JB. Parallel time integration with multigrid. SIAM

Journal on Scientiﬁc Computing 2014; 36(6):C635–C661.

22. Horton G, Vandewalle S. A space-time multigrid method for parabolic partial differential equations. SIAM Journal

on Scientiﬁc Computing 1995; 16(4):848–864. URL http://dx.doi.org/10.1137/0916050.

23. Gander MJ, Neum¨uller M. Analysis of a new space-time parallel multigrid algorithm for parabolic problems. arXiv

preprint arXiv:1411.0519 2014; .

24. Gander MJ, Vandewalle S. Analysis of the parareal time-parallel time-integration method. SIAM Journal on

Scientiﬁc Computing 2007; 29(2):556–578. URL http://dx.doi.org/10.1137/05064607X.

25. Dutt A, Greengard L, Rokhlin V. Spectral deferred correction methods for ordinary differential equations. BIT
Numerical Mathematics 2000; 40(2):241–266. URL http://dx.doi.org/10.1023/A:1022338906936.

1:347–363.

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

26

26. Frank R, Ueberhuber CW. Iterated defect correction for the efﬁcient solution of stiff systems of ordinary
differential equations. BIT Numerical Mathematics 1977; 17(2):146–159. URL http://dx.doi.org/10.
1007/BF01932286.

27. Huang J, Jia J, Minion M. Accelerating the convergence of spectral deferred correction methods. Journal of
Computational Physics 2006; 214(2):633 – 656. URL http://dx.doi.org/10.1016/j.jcp.2005.10.
004.

28. Layton AT, Minion ML. Conservative multi-implicit spectral deferred correction methods for reacting gas
dynamics. Journal of Computational Physics 2004; 194(2):697 – 715. URL http://dx.doi.org/10.1016/
j.jcp.2003.09.010.

29. Minion ML. Semi-implicit projection methods for incompressible ﬂow based on spectral deferred corrections.
Applied Numerical Mathematics 2004; 48(3–4):369 – 387. URL http://dx.doi.org/10.1016/j.
apnum.2003.11.005, Workshop on Innovative Time Integrators for PDEs.

30. Bourlioux A, Layton AT, Minion ML. High-order multi-implicit spectral deferred correction methods for problems
of reactive ﬂow. Journal of Computational Physics 2003; 189(2):651 – 675. URL http://dx.doi.org/10.
1016/S0021-9991(03)00251-1.

31. Guibert D, Tromeur-Dervout D. Parallel deferred correction method for CFD problems. Parallel Computational
Fluid Dynamics 2006, Kwon J, Ecer A, Satofuka N, Periaux J, Fox P (eds.). Elsevier Science B.V.: Amsterdam,
2007; 131 – 138. URL http://dx.doi.org/10.1016/B978-044453035-6/50019-5.

32. Minion ML, Williams SA. Parareal and spectral deferred corrections. AIP Conference Proceedings, vol. 1048,

2008; 388. URL http://link.aip.org/link/doi/10.1063/1.2990941.

33. Weiser M. Faster SDC convergence on non-equidistant grids with DIRK sweeps 2013. URL http://opus4.

kobv.de/opus4-zib/files/1866/ZR-13-30.pdf, ZIB Report 13–30.

34. Winkel M, Speck R, Ruprecht D. A high-order Boris integrator. Journal of computational physics 2015; 295:456–

35. Speck R, Ruprecht D, Emmett M, Minion M, Bolten M, Krause R. A multi-level spectral deferred correction

method. BIT Numerical Mathematics 2015; 55(3):843–867.

36. Koehler F. Pfasst tikz. https://github.com/Parallel-in-Time/pfasst-tikz 2015.
37. Friedhoff S, MacLachlan S. A generalized predictive analysis tool for multigrid methods. Numerical Linear Algebra
with Applications 2015; 22(4):618–647, doi:10.1002/nla.1977. URL http://dx.doi.org/10.1002/nla.
1977, nla.1977.

38. Kelley C T. Iterative methods for linear and nonlinear equations. Raleigh N. C.: North Carolina State University

474.

1995; .

Copyright c(cid:13) 2016 John Wiley & Sons, Ltd.
Prepared using nlaauth.cls

Numer. Linear Algebra Appl. (2016)
DOI: 10.1002/nla

