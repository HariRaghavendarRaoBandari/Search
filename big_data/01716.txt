6
1
0
2

 
r
a

M
5

 

 
 
]

G
L
.
s
c
[
 
 

1
v
6
1
7
1
0

.

3
0
6
1
:
v
i
X
r
a

Classiﬁer ensemble creation via false labelling

B´alint Antal

Faculty of Informatics, University of Debrecen, 4010 Debrecen, POB. 12, Hungary

E-mail: antal.balint@inf.unideb.hu.

Abstract

In this paper, a novel approach to classiﬁer ensemble creation is presented.

While other ensemble creation techniques are based on careful selection of

existing classiﬁers or preprocessing of the data, the presented approach au-

tomatically creates an optimal labelling for a number of classiﬁers, which

are then assigned to the original data instances and fed to classiﬁers. The

approach has been evaluated on high-dimensional biomedical datasets. The

results show that the approach outperformed individual approaches in all

cases.

Keywords: Ensemble learning, Diversity, Hidden Markov Random Fields,

Simulated annealing, Bioinformatics

1. Introduction

Classiﬁcation is a fundamental task in machine learning.

In numerous

application ﬁelds very complex data needs to be classiﬁed which is often a

diﬃcult task for a single machine learning classiﬁer [1] [2]. There are tremen-

dous amount of research on improving the classiﬁcation performance in such

cases. One highly investigated ﬁeld for this problem is ensemble learning [3],

where multiple prediction are fused the produce a more eﬃcient classiﬁca-

Preprint submitted to Elsevier

March 8, 2016

tion approach. One fundamental requirement for the creation of classiﬁer

ensembles is diversity among them [4], that is, the classiﬁers included in the

ensemble need to complement each other to provide more generalization ca-

pabilities than a single learner. Bagging [5] uses randomly selected training

subsets with possible overlap (bootstrapping [6]) to ensure diversity among

the member of the ensemble. Other diversity creation techniques may in-

volve disjoint random sampling (random subspace methods [7], for example,

some variants of Random Forest algorithms [8]), while Adaboost [9] based

techniques aims to increase the accuracy of a weak learner iteratively (boost-

ing [10]) using targeted sampling: each iteration considers the misclassiﬁed

instances of the training data to be more important, and drives the iteration

process to include them in the current training set. Another approach to cre-

ate diverse ensembles is ensemble selection [11], where diversity of classiﬁers

trained on the same dataset is measured and an optimal subset is selected.

A more comprehensive review on the above described techniques can be

found in [12]. The relationship of classiﬁer diversity and ensemble accuracy

is highly investigated in the ensemble learning community. Although the

deﬁnite connection between diversity measures and ensemble accuracy is an

open question [13], a decomposition of majority voting error into good and

bad diversity is proposed in [14].

In this paper, a novel approach for ensemble creation based on this the-

oretical result is presented. The proposed approach takes the predictions

of a single classiﬁer on a training set. Then, an optimal labelling compli-

menting the predictions of the classiﬁers are created. Thus, an optimal but

false labelling set is created for a number of classiﬁers. The data with each

2

Figure 1: Flowchart of ensemble creation via false labelling

false labelling is trained to a classiﬁer thus forming an ensemble. We de-

ﬁne a Markov Random Field problem to create an optimal ensemble with

this method. The approach has been tested on high-dimensional biomedical

datasets where a large improvement over a single learner is achieved. Other

aspects of the algorithm including its performance comparison with diﬀerent

number of ensemble members is also discussed. The outline of the proposed

algorithm can be seen in Figure 1.

The rest of the paper is organized as follows: section 2 contains the math-

ematical background behind the proposed method, while section 3 deﬁnes an

3

optimization problem to solve it and proposes an implementation for it. Sec-

tion 4 contains our experimental details, while the results are presented and

discussed in section 5. Finally, conclusions are drawn in section 6.

2. Ensemble creation via false labelling

The presented false labelling based ensemble creation are presented is re-

stricted to binary classiﬁcation problems. In this section, the mathematical

background behind the algorithm is presented. Moreover, an optimization

problem is deﬁned to provide an eﬃcient solution for the false labelling prob-

lem. For the basic machine learning and ensemble deﬁnitions, we relied on

the classic literature [3] and [14].

Let Ω = {−1, +1} be a set of class labels. Then, a function

D : Rn → Ω

(1)

is called a classiﬁer, while a vector (cid:126)χ = (χ1, χ2, . . . , χn) ∈ Rn is called a
feature vector. A dataset T ∈ {Rn × Ω}l can be deﬁned as follows:

T = {(cid:104)(cid:126)χ0, ω0(cid:105),(cid:104)(cid:126)χ1, ω1(cid:105), . . . ,(cid:104)(cid:126)χk, ωk(cid:105)},

(2)

where (cid:126)χi ∈ Rn, ωk ∈ Ω, i = 1, . . . , k are feature vectors and labels, respec-
tively.

Let D1, D2, . . . , DL be classiﬁers and dt ((cid:126)χ) ∈ Ω, t = 1, . . . , L their output
on the feature vector (cid:126)χ. Then, the output of the majority voting ensemble
classiﬁer Dmaj : Rn → Ω can be deﬁned as follows:

(cid:32)

(cid:33)

L(cid:88)

t=1

1
L

dmaj ((cid:126)χ) = sign

4

dt ((cid:126)χ)

.

(3)

The creation of an ensemble Dmaj of L classiﬁers (equation 1) starts by
training a base classiﬁer on the half of the training dataset (equation 2) T

(T0). We take the output Corig of the classiﬁer Dorig on the other half of the
training set (T1) and create L− 1 optimal labellings for a the remaining base
classiﬁers Di, i = 2, . . . , L. Then, we train these classiﬁers on T1 with their
respective false labellings Ci

f alse.

The outline of the ensemble creation method is summarized in algorithm

1, while the mathematical formulation is presented in the rest of the section.

Algorithm 1 Outline of ensemble creation via false labelling
Require: a dataset T (cid:54)= ∅, a label set C (cid:54)= ∅, a classiﬁer Dorig, the number

of ensemble members L > 2 (L is odd).

Ensure: an ensemble of trained classiﬁers Dmaj.
1: Split T into T0 and T1 randomly.

2: Train Dorig on T0.
3: Corig ← Dorig (T1)
4: Ccl ← F (Corig) = {C2
5: for i ← 2, . . . , L do

6:

f alse,C3

f alse, . . . ,CL

f alse}

Train a classiﬁer Di on LC(cid:0)T1,Ci

(cid:1) ,Ci

f alse ∈ Ccl.

f alse

7: end for
8: return {Dorig, D2, . . . , DL}

2.1. Ensemble creation

The proposed ensemble creation depends on the output of one classiﬁer

Dorig for a given training dataset T .

5

First, we split T into two equal parts T (0) and T (1) randomly. We train

Dorig on T (1) and classify all (cid:126)χ1
C1
orig = {ωj|ωj = Dorig

(cid:0)(cid:126)χ1

j

(cid:1) , (cid:126)χ1

j ∈ T 1, j = 1, . . . , k/2}.

j ∈ T (0), j = 1, . . . , k/2 element of T (1):

Then, we create a majority voting classiﬁer ensemble of L members:

Dmaj = {D1 = Dorig, D2, . . . , DL}.

(4)

(5)

(6)

To train D2, . . . , DL, we will deﬁne a false labelling function F : Ωk/2 →
Ωk/2·(L−1). That is

F(cid:0)C1

orig

(cid:1) = {C2

f alse,C3

f alse, . . . ,CL

f alse},

i,j|ωf

f alse = {ωf

where Ci
i,j ∈ Ω, i = 2, . . . , L, j = 1, . . . , k/2.}. To apply the
new labels to the existing dataset, we deﬁne the label changing operation
LC : {Rn × Ω × Ω}l → {Rn × Ω}l in the following way:

LC (T,C) = {(cid:104)(cid:126)χj, ωf

j (cid:105)|(cid:104)(cid:126)χj, ωj(cid:105) ∈ T, ωf

j ∈ C},

(7)

LC(cid:0)T,Ci

where T is a dataset and C is a label set. Finally, we train Di, i = 1, . . . , L on

(cid:1), where Ci

f alse ∈ F(cid:0)C1

orig

(cid:1). Then, the false labelling ensemble is

f alse

created.

2.2. Selection of the false labelling function

To deﬁne an optimal false labelling function F (see equation 6), we recite

the decomposition of the majority voting error described in [14]. The ma-

jority voting error can be split into three terms: the individual error of the

classiﬁers , the disagreement of the classiﬁers when they classiﬁed the input

correctly (”good diversity”) and the disagreement of the classiﬁers when they

6

classiﬁed the input incorrectly ( ”bad diversity”). The majority voting error

decomposition is the basis for deﬁning the energy function for our method.

Let y ((cid:126)χ) be the true class label for the feature vector (cid:126)χ. Then, the

zero-one loss for dt ((cid:126)χ) is deﬁned as follows [14]:

(8)

(9)

(10)

(11)

et ((cid:126)χ) =

(1 − y ((cid:126)χ) dt ((cid:126)χ))

1
2

Then, the average individual zero-one loss is [14]

eind ((cid:126)χ) =

1
L

and the ensemble zero-one loss is:

L(cid:88)

t=1

et ((cid:126)χ)

emaj ((cid:126)χ) =

(1 − y ((cid:126)χ) dmaj ((cid:126)χ))

1
2

The disagreement between dt and the ensemble is the following [14]:

δt ((cid:126)χ) =

1
2

(1 − dt ((cid:126)χ) dmaj ((cid:126)χ)) .

The classiﬁcation error of an ensemble is deﬁned [14] as follows:

(cid:90)

(cid:126)χ

Emaj =

eind −

1
L

(cid:126)χ+

(cid:90)

L(cid:88)

t=1

(cid:90)

1
L

(cid:126)χ−

L(cid:88)

t=1

δt ((cid:126)χ) +

δt ((cid:126)χ)

(12)

Based on equations 10-12, an optimization problem can be deﬁned to ﬁnd

such an optimal labelling.

3. Optimization via Hidden Markov Random Fields

To solve the optimization problem, an approach based on Hidden Markov

Random Fields (HMRF) in presented. HMRF is a powerful framework for

7

solving large-scale optimization problems, since there are multiple methods

for solving HMRF problems near optimally in normal time, which would be

a challenging task to ﬁnd exact false labellings for real-life applications.

In this section, we brieﬂy summarize the basis for Hidden Markov Random

Field (HMRF) optimization based on [15]. Let



ωf
1,2
ωf
2,2
...

ωf
1,1
ωf
2,1
...
L−1,1 ωf
ωf

L−1,2

ωf

ωf

···
···
. . .
··· ωf

1,k/2

2,k/2

...

L−1,k/2



Ak/2,L−1 = ai,j =

be a matrix containing a false labelling setup and Corig = bi,j a vector con-
taining the labellings of Dorig and Ctraining = ci,j the labels assigned originally
the training instances. All ai,j is a variable which can contain a possible label

and at the end of the optimization process, each row contain a false labelling

for a classiﬁer Di.

Let Λ = {0, 1} be a set of labels. Then, we assign each ai,j, i = 1, . . . , k/2j =
1, . . . , L − 1 a label ωij . Let X be a labelling ﬁeld. X is a Markov Random

Field if P (X = ω), for all ω ∈ Λ and P(cid:0)ωai,j|ωak,l, ai,j (cid:54)= ik

(cid:1) = P(cid:0)ωai,j|ωak,l, ak,l ∈ Nai,j

(cid:1),

where Nai,j is a neighbourhood of ai,j.

The optimal labelling for the A variables with the HMRF optimization,

one can use the the Hammersley-Cliﬀord Theorem [16] to calculate the global

energy for a labelling by summarizing the local energies for each variable.

That is, during the optimization process, the global energy would be a func-

tion of the changes in the states of the ai,j variables.

We deﬁne the following three neighbourhoods for the optimization pro-

8

cess:

= {am,j|m ∈ {1, k/2}, m (cid:54)= i} ∪ {bi},

N 1

ai,j

(13)

and a neighbourhood of a single variable containing the labelling for all of

the feature vectors for the same classiﬁer

= {ai,l|l ∈ {1, L − 1}, l (cid:54)= j} ∪ {bi},

N 2

ai,j

(14)

which is a neighbourhood of a single variable containing the labelling of the

other classiﬁers for the same feature vector, and

= {ak,l|k ∈ {i − q, i + q}, l ∈ {j − q, j + q}},

N 3

ai,j

(15)

which is a neighbourhood of a variable containing labelling of its close clas-
siﬁers for inputs in a q · q part of A. First, we consider the individual classi-
ﬁcation error the individual classiﬁers:

(cid:80){ak,l|ak,l ∈ N 1

ai,j
k/2

Uind (ai,j) =

∧ ak,l = ωi}

,

(16)

where ωi is the actual label assigned to the feature vector in the training set.

Out next criteria for the optimization process is to give a labelling, where

the number of correct votes is exactly 50%+1 in all cases. Let

o = L/2 + 1.

Then, we deﬁne the function Evotes in the following way:

(cid:80){ak,l|ak,l ∈ N 2

Ugood (ai,j) =

∧ ak,l = bi} − o

.

(17)

ai,j
o

That is, we sum the correct labellings for a given input and subtracting the

optimal number of votes from it. In this way, the Evotes will be minimal if

9

the number of correct votes is less than or equal to the number of optimal

votes. Thus, we maximize the disagreement for bad diversity and minimize to

good diversity [14]. To ensure classiﬁcation accuracy (and avoid having lower

numbers of votes resulting from negative values of Evotes), we also deﬁne

(cid:80){ak,l|ak,l ∈ N 3

Ubad (ai,j) = −

} ∧ {ak,l (cid:54)= bi} − o
ai,j
o

,

(18)

which is the disagreement term for bad diversity.

Finally, we must ensure that the votes are unevenly distributed among

the classiﬁers to have less correlation between variables:

Usmoothness (ai,j) =

,

(19)

for all ak,l ∈ N 2
assigned to the classiﬁers. In summary, the global energy U is the following:

. In this way we ensure low correlation between the label sets

ai,j

β

if ai,j = ak,l
−β otherwise.

k/2(cid:88)

L−1(cid:88)

i=0

j=0

U =

Eind (ai,j) + Egood (ai,j) + Ebad (ai,j) +

(20)

Esmoothness (ai,j) .

The optimization of the HMRF conﬁguration can be done by optimizing U .

Since simulated annealing [17], an eﬃcient algorithm for ﬁnding approximate

global solutions for large state-spaces.

In summary, simulated annealing measures energy values from diﬀerent

states of the variables. Each state is accepted as a better solution if provided

a more optimal energy value or accepted by a function, which uses a random

number to decide it. This step is important in avoiding stuck in local op-

tima, as do other stochastic approaches like stochastic gradient search. The

algorithm for simulated annealing can be found in algorithm 2.

10

Algorithm 2 Solving the optimization problem with simulated annealing.
Require: An initial temperature T , a minimal temperature Tmin and a tem-

perature change quotient q.

Require: A function changeState changing variable values from their cur-

rent state.

Require: An acceptance function accept. E.g.

true,

(cid:18) e − ei

(cid:19)

T

exp

> r,

(21)

accept (u, ubest, T, ) =

where r is a random number.

f alse, otherwise,

Ensure: An approximation of the optimal false labelling.
1: A = am,n ← {0}.
2: u ← U (A)
3: lbest ← A
4: ubest ← u
5: s ← 0
6: while T ≥ Tmin do

A ← changeState (A)
u ← U (A)
if u ≥ ubest or accept(u, ubest, T ) then

7:

8:

9:

10:

11:

lbest ← A
ubest ← u

12:

end if
T ← T · q
14: end while

13:

15: return lbest

11

After the optimization process, the lbest state of A is the optimal false

labelling, which can be used to train the classiﬁers.

4. Methodology

The proposed approach has been evaluated on high-dimensional biomedi-

cal datasets containing gene expressions or proteomics data downloaded from

the the Keng Ridge repository [18]. The description of the datasets including

the number of instances, the number of features per instance and the status

of the patient by disease is summarized in Table 1. As it can be seen, the

datasets contain a large number of features for a small number of instances

thus making it challenging classiﬁcation problems. Thus, the datasets are

bootstrapped for training to ensure the number of instances per class are

similar for better comparison of the methods.

The datasets were splitted into two equal partitions randomly 10 times

to have a fair comparison. The false-labelling ensembles are tested with

3, 5, 7, 9, 11, 13, 15 members with Naive Bayes [19] base classiﬁers for

each problem. The implementation of the classiﬁers was done using Weka

[20]. To measure the accuracy of the ensembles, the classiﬁcation accuracy

of each cross-validation round is measured and their mean and standard

deviation is calculated. For a comparison, we also show the results for a

Naive Bayes classiﬁer, which serves base classiﬁers in the ensembles, and

three state-of-the-art ensemble approaches, namely Adaboost, Bagging and

Random Forest.

12

.
r
o
m
u
t

l
a
n
o
y
r
b
m
e
m
e
t
s
y
s

s
u
o
v
r
e
n

l
a
r
t
n
e
C

.
r
o
m
u
t

n
o
l
o
C

.
r
e
c
n
a
c

t
s
a
e
r
B

a
m
o
h
p
m
y
l

l
l
e
c
-
B
e
g
r
a
l

e
s
u
ﬀ
D

i

r
e
c
n
a
c

n
a
i
r
a
v
O

r
e
c
n
a
c

e
t
a
t
s
o
r
P

1
8
4
4
2

1
8
4
4
2

9
2
1
7

0
0
0
2

6
2
0
4

7
1
8
6

7
1
8
6

9
9
3
7

9
9
3
7

0
4
3
7
3

0
0
6
2
1

0
0
6
2
1

0
0
6
2
1

3
7

9
1

0
6

2
6

7
4

8
5

7
7

0
6
1

0
8

6
1
2

2
0
1

3
3

1
2

n
i
a
r
t
-
l
a
m
r
o
N
S
V
r
o
m
u
t
-
e
t
a
t
s
o
r
p

t
s
e
t
-
l
a
m
r
o
N
S
V
r
o
m
u
t
-
e
t
a
t
s
o
r
p

e
m
o
c
t
u
o
-
e
t
a
t
s
o
r
p

n
i
a
r
t
-
r
e
c
n
a
C
t
s
a
e
r
b

t
s
e
t
-
r
e
c
n
a
C
t
s
a
e
r
b

m
e
t
s
y
S
s
u
o
v
r
e
N
l
a
r
t
n
e
c

d
r
o
f
n
a
t
S
-
L
C
B
L
D

r
o
m
u
T
n
o
l
o
c

I

n
i
a
r
t
-
H
N
-
L
C
B
L
D

I

t
s
e
t
-
H
N
-
L
C
B
L
D

r
o
m
u
T
L
C
B
L
D

9
-
0
C
O

e
m
o
c
t
u
O
L
C
B
L
D

13

s
t
e
s
a
t
a
d

e
h
t

f
o

n
o
i
t
p

i
r
c
s
e
D

:
1

e
l
b
a
T

e
s
a
e
s
i
D

s
e
r
u
t
a
e
F

f
o

r
e
b
m
u
N

s
e
c
n
a
t
s
n
I

f
o

r
e
b
m
u
N

t
e
s
a
t
a
D

5. Results and discussion

The validity of the optimization technique can be seen in Figures 2(a) and

2(b). As it can be seen in this example, the accuracy of the ensemble has

increased steadily through iteration converging to an accuracy of 1, while the

correlation of the labels of the ensemble members has been decreased at the

same time. Figure 2(c) shows the optimization time through iterations. As

it can be seen, in earlier iterations, the optimization procedure increases the

energy function with less changes in the labelling spending less time, while

in later iterations most of the combinations needs to be tested to increase

energy, which require more time.

The mean accuracy and their standard deviations on the datasets for the

ensembles can be found in table 2. Each column contains the classiﬁcation
accuracy of the respective ensembles Di, i ∈ {3, 5, 7, 9, 11, 13, 15}. The re-
sults for the Naive Bayes, Adaboost, Bagging and Random Forest classiﬁers

can be found in table 3. The values in bold for each dataset contain the

best performing method. As it can be seen, for each dataset, the proposed

approach provides the best values. However, the number of ensembles mem-

bers varies among the best results. To have a deeper insight on the choice

of optimal ensemble size, each investigated ensemble is compared to the best

performing among the Naive Bayes, Adaboost, Bagging and Random Forest

classiﬁers. Figures 4-10 show the diﬀerence between the respective ensem-

ble and the best performing other method, where each positive value means

that the respective ensemble performed better than the best among the other

classiﬁers, while a negative value shows otherwise. As it can be seen, only

the ensembles with 5 and 9 members remain above the other methods all

14

(a) Accuracy

(b) Correlation

(c) Time

Figure 2: Accuracy, correlation of the ensemble member labels and execution time through

iterations of the optimization procedure

15

0.860.880.90.920.940.960.981024681012AccuracyAccuracy during optimzation00.050.10.150.20.250.30.35024681012CorrelationCorrelation of labels during optimization0100020003000400050006000700080009000024681012Time (s)Optimization time through iterationsthe time. From table 4 we can see that the sum of the all diﬀerences are the

highest for the D5 ensemble. That is, based on these experiments, a false

labelling ensemble with 5 member can be recommended to generate.

Statistical analysis of the classiﬁers is also performed. First, Friedman-

test [21] was performed to check whether the results of the proposed en-

semble based classiﬁers, the Naive Bayes, Adaboost, Bagging and Random

Forest are from the same distribution. This hypothesis was rejected with p

= 3.8499e-026. Then, we applied post-hoc analysis to reveal the diﬀerences

among the investigated classiﬁers. To recognize these diﬀerences, Tukey’s

multiple comparison test [22] is also performed. The test revealed that the

proposed ensembles consisting of 5-15 member (D5, . . . , D15) were all signif-

icantly diﬀerent from the four classiﬁers they were compared to, while D3

were signiﬁcantly diﬀerent from all but Adaboost. The Friedman ranking

also revealed D5 to be the best performing classiﬁer among the investigated

ones. For a visual representation of the Tukey test, see Figure 3, where a

conﬁdence interval for the sample mean diﬀerences are shown.

6. Conclusion

In this paper, a novel classiﬁer ensemble creation approach is presented.

The presented approach automatically creates an2 optimal labelling for a

number of classiﬁers based on the output of a classiﬁer, which are then as-

signed to the original data instances and fed to classiﬁers. The approach

has been evaluated on high-dimensional biomedical datasets and compared

to state-of-the-art classiﬁers. The results shown improvement in classiﬁca-

tion accuracy. The possible ensemble size is also investigated, with having

16

6
1
.
0
±
1
9
.
0

4
2
.
0
±
2
9
.
0

0
2
.
0
±
4
9
.
0

7
1
.
0
±
4
9
.
0

4
2
.
0
±
4
9
.
0

6
2
.
0
±
3
9
.
0

0
0
.
0
±
0
0
.
1

1
1
.
0
±
4
9
.
0

4
2
.
0
±
4
9
.
0

3
0
.
0
±
8
9
.
0

3
1
.
0
±
7
8
.
0

4
0
.
0
±
6
9
.
0

8
0
.
0
±
4
9
.
0

5
1
.
0
±
9
8
.
0

6
0
.
0
±
0
9
.
0

8
1
.
0
±
5
9
.
0

7
1
.
0
±
2
9
.
0

2
2
.
0
±
3
9
.
0

2
1
.
0
±
5
9
.
0

2
0
.
0
±
8
9
.
0

2
1
.
0
±
1
9
.
0

7
0
.
0
±
4
9
.
0

4
0
.
0
±
5
9
.
0

2
1
.
0
±
6
9
.
0

7
0
.
0
±
7
9
.
0

3
0
.
0
±
8
9
.
0

5
0
.
0
±
6
9
.
0

5
0
.
0
±
7
9
.
0

5
0
.
0
±
3
9
.
0

8
0
.
0
±
1
9
.
0

6
0
.
0
±
8
8
.
0

1
1
.
0
±
2
9
.
0

5
0
.
0
±
5
9
.
0

1
1
.
0
±
2
9
.
0

6
0
.
0
±
4
9
.
0

5
0
.
0
±
5
9
.
0

5
0
.
0
±
8
8
.
0

5
1

D

3
1

D

1
1

D

9

D

7

D

5

D

3

D

t
e
s
a
t
a
D

.
s
t
e
s
a
t
a
d

e
v
i
t
c
e
p
s
e
r

e
h
t

n
o

s
e
l

b
m
e
s
n
e

e
h
t

f
o

s
e
i
c
a
r
u
c
c
a

e
h
t

f
o

n
o
i
t
a
i
v
e
d

d
r
a
d
n
a
t
s

d
n
a

n
a
e
M

:
2

e
l
b
a
T

8
0
.
0
±
5
9
.
0

4
1
.
0
±
8
8
.
0

4
1
.
0
±
9
8
.
0

0
1
.
0
±
5
9
.
0

3
0
.
0
±
8
9
.
0

0
2
.
0
±
8
8
.
0

6
2
.
0
±
1
9
.
0

4
2
.
0
±
2
9
.
0

5
2
.
0
±
1
9
.
0

2
1
.
0
±
8
9
.
0

4
0
.
0
±
0
0
.
1

7
0
.
0
±
8
9
.
0

n
i
a
r
t
-
r
e
c
n
a
C
t
s
a
e
r
b

t
s
e
t
-
r
e
c
n
a
C
t
s
a
e
r
b

3
2
.
0
±
0
9
.
0

3
2
.
0
±
3
9
.
0

4
1
.
0
±
6
9
.
0

1
1
.
0
±
8
9
.
0

5
1
.
0
±
8
9
.
0

6
0
.
0
±
8
9
.
0

m
e
t
s
y
S
s
u
o
v
r
e
N
l
a
r
t
n
e
c

6
1
.
0
±
5
9
.
0

6
1
.
0
±
5
9
.
0

8
1
.
0
±
4
9
.
0

6
0
.
0
±
9
9
.
0

4
0
.
0
±
0
0
.
1

9
0
.
0
±
6
9
.
0

4
2
.
0
±
4
9
.
0

0
2
.
0
±
6
9
.
0

7
1
.
0
±
7
9
.
0

4
1
.
0
±
8
9
.
0

0
1
.
0
±
9
9
.
0

8
0
.
0
±
7
9
.
0

7
2
.
0
±
2
9
.
0

2
2
.
0
±
5
9
.
0

4
1
.
0
±
8
9
.
0

0
1
.
0
±
9
9
.
0

4
1
.
0
±
8
9
.
0

4
0
.
0
±
9
9
.
0

2
2
.
0
±
5
9
.
0

4
1
.
0
±
8
9
.
0

4
1
.
0
±
8
9
.
0

0
0
.
0
±
0
0
.
1

0
1
.
0
±
9
9
.
0

3
1
.
0
±
2
9
.
0

7
0
.
0
±
8
9
.
0

3
0
.
0
±
8
9
.
0

7
0
.
0
±
2
9
.
0

8
0
.
0
±
7
8
.
0

4
0
.
0
±
8
9
.
0

5
0
.
0
±
4
8
.
0

2
2
.
0
±
5
9
.
0

0
2
.
0
±
6
9
.
0

7
1
.
0
±
7
9
.
0

0
1
.
0
±
9
9
.
0

0
0
.
0
±
0
0
.
1

7
0
.
0
±
6
9
.
0

d
r
o
f
n
a
t
S
-
L
C
B
L
D

e
m
o
c
t
u
O
L
C
B
L
D

r
o
m
u
T
L
C
B
L
D

r
o
m
u
T
n
o
l
o
c

I

n
i
a
r
t
-
H
N
-
L
C
B
L
D

I

t
s
e
t
-
H
N
-
L
C
B
L
D

0
C
O

1
C
O

2
C
O

3
C
O

4
C
O

5
C
O

6
C
O

7
C
O

8
C
O

9
C
O

17

4
0
.
0
±
8
9
.
0

0
1
.
0
±
2
9
.
0

8
0
.
0
±
5
9
.
0

0
1
.
0
±
3
9
.
0

0
1
.
0
±
3
9
.
0

9
0
.
0
±
3
9
.
0

1
1
.
0
±
3
9
.
0

9
0
.
0
±
4
9
.
0

9
0
.
0
±
6
9
.
0

2
0
.
0
±
8
9
.
0

8
0
.
0
±
5
9
.
0

1
1
.
0
±
8
8
.
0

9
0
.
0
±
1
9
.
0

6
0
.
0
±
2
9
.
0

8
0
.
0
±
5
9
.
0

6
0
.
0
±
6
9
.
0

5
0
.
0
±
0
9
.
0

7
0
.
0
±
2
9
.
0

5
0
.
0
±
5
9
.
0

4
0
.
0
±
8
9
.
0

3
0
.
0
±
9
9
.
0

7
0
.
0
±
6
9
.
0

3
0
.
0
±
7
9
.
0

9
0
.
0
±
2
9
.
0

7
0
.
0
±
7
9
.
0

7
0
.
0
±
3
9
.
0

5
0
.
0
±
6
9
.
0

4
0
.
0
±
8
9
.
0

0
1
.
0
±
1
9
.
0

8
0
.
0
±
1
9
.
0

0
1
.
0
±
3
9
.
0

7
0
.
0
±
4
9
.
0

5
0
.
0
±
2
9
.
0

7
0
.
0
±
5
9
.
0

3
0
.
0
±
3
9
.
0

5
0
.
0
±
3
9
.
0

5
0
.
0
±
3
9
.
0

9
0
.
0
±
6
8
.
0

5
0
.
0
±
9
8
.
0

8
0
.
0
±
2
9
.
0

0
1
.
0
±
2
9
.
0

8
0
.
0
±
2
9
.
0

6
0
.
0
±
5
9
.
0

3
0
.
0
±
5
9
.
0

6
0
.
0
±
2
9
.
0

7
0
.
0
±
9
9
.
0

0
2
.
0
±
5
9
.
0

0
0
.
0
±
0
0
.
1

0
0
.
0
±
0
0
.
1

0
0
.
0
±
0
0
.
1

4
1
.
0
±
1
9
.
0

n
i
a
r
t
-
l
a
m
r
o
N
S
V
r
o
m
u
t
-
e
t
a
t
s
o
r
p

1
2
.
0
±
0
9
.
0

7
1
.
0
±
3
9
.
0

6
1
.
0
±
6
9
.
0

4
2
.
0
±
1
9
.
0

2
2
.
0
±
2
9
.
0

6
2
.
0
±
0
9
.
0

0
1
.
0
±
8
9
.
0

7
2
.
0
±
1
9
.
0

9
0
.
0
±
8
9
.
0

2
0
.
0
±
0
0
.
1

9
0
.
0
±
8
9
.
0

5
0
.
0
±
0
0
.
1

t
s
e
t
-
l
a
m
r
o
N
S
V
r
o
m
u
t
-
e
t
a
t
s
o
r
p

e
m
o
c
t
u
o
-
e
t
a
t
s
o
r
p

.
s
t
e
s
a
t
a
d

e
v
i
t
c
e
p
s
e
r

e
h
t

n
o

s
r
e
ﬁ

i
s
s
a
l
c

t
r
a
-
e
h
t
-
f
o
-
e
t
a
t
s

r
e
h
t
o

f
o

s
e
i
c
a
r
u
c
c
a

e
h
t

f
o

n
o
i
t
a
i
v
e
d

d
r
a
d
n
a
t
s

d
n
a

n
a
e
M

:
3

e
l
b
a
T

t
s
e
r
o
F
m
o
d
n
a
R

g
n

i
g
g
a
B

t
s
o
o
b
a
d
A

s
e
y
a
B
e
v
i
a
N

t
e
s
a
t
a
D

8
0
.
0
±
7
8
.
0

2
1
.
0
±
0
8
.
0

1
1
.
0
±
1
8
.
0

9
0
.
0
±
6
8
.
0

8
0
.
0
±
8
8
.
0

3
1
.
0
±
8
8
.
0

8
0
.
0
±
3
9
.
0

0
1
.
0
±
4
8
.
0

7
1
.
0
±
0
8
.
0

1
0
.
0
±
9
8
.
0

8
0
.
0
±
6
8
.
0

2
1
.
0
±
4
8
.
0

9
0
.
0
±
5
8
.
0

8
0
.
0
±
1
9
.
0

6
0
.
0
±
7
8
.
0

7
0
.
0
±
6
8
.
0

5
0
.
0
±
2
9
.
0

5
0
.
0
±
3
9
.
0

6
0
.
0
±
7
8
.
0

3
0
.
0
±
7
8
.
0

4
0
.
0
±
4
9
.
0

0
1
.
0
±
8
7
.
0

7
1
.
0
±
4
7
.
0

0
1
.
0
±
7
8
.
0

0
1
.
0
±
3
8
.
0

5
1
.
0
±
4
7
.
0

8
0
.
0
±
8
7
.
0

5
1
.
0
±
4
8
.
0

6
1
.
0
±
8
7
.
0

0
1
.
0
±
4
8
.
0

7
0
.
0
±
3
8
.
0

4
1
.
0
±
8
8
.
0

9
0
.
0
±
6
8
.
0

9
0
.
0
±
4
8
.
0

2
1
.
0
±
8
7
.
0

9
0
.
0
±
2
8
.
0

7
0
.
0
±
2
9
.
0

7
1
.
0
±
5
8
.
0

0
1
.
0
±
6
8
.
0

9
1
.
0
±
7
7
.
0

3
1
.
0
±
9
8
.
0

5
0
.
0
±
3
9
.
0

8
0
.
0
±
2
9
.
0

1
1
.
0
±
8
7
.
0

0
1
.
0
±
3
8
.
0

8
0
.
0
±
9
7
.
0

0
1
.
0
±
6
7
.
0

0
1
.
0
±
7
7
.
0

3
1
.
0
±
2
8
.
0

4
0
.
0
±
1
9
.
0

7
0
.
0
±
2
9
.
0

8
0
.
0
±
2
8
.
0

4
0
.
0
±
9
8
.
0

5
0
.
0
±
7
8
.
0

9
0
.
0
±
1
8
.
0

5
0
.
0
±
7
8
.
0

8
0
.
0
±
6
8
.
0

2
0
.
0
±
1
8
.
0

7
0
.
0
±
4
8
.
0

9
0
.
0
±
8
8
.
0

8
0
.
0
±
5
8
.
0

5
0
.
0
±
1
9
.
0

6
0
.
0
±
2
9
.
0

7
0
.
0
±
3
8
.
0

7
0
.
0
±
6
8
.
0

7
0
.
0
±
6
8
.
0

6
0
.
0
±
5
8
.
0

2
0
.
0
±
0
9
.
0

3
0
.
0
±
2
9
.
0

3
0
.
0
±
8
8
.
0

4
0
.
0
±
2
9
.
0

6
0
.
0
±
4
9
.
0

8
0
.
0
±
1
8
.
0

6
0
.
0
±
0
9
.
0

8
0
.
0
±
9
8
.
0

6
0
.
0
±
5
8
.
0

8
0
.
0
±
8
8
.
0

6
0
.
0
±
8
8
.
0

3
0
.
0
±
4
8
.
0

5
0
.
0
±
8
8
.
0

3
0
.
0
±
4
9
.
0

5
0
.
0
±
2
6
.
0

1
1
.
0
±
7
8
.
0

6
0
.
0
±
4
9
.
0

7
0
.
0
±
2
9
.
0

3
2
.
0
±
8
7
.
0

2
1
.
0
±
9
8
.
0

0
1
.
0
±
7
8
.
0

n

i
a
r
t
-
r
e
c
n
a
C
t
s
a
e
r
b

t
s
e
t
-
r
e
c
n
a
C
t
s
a
e
r
b

m
e
t
s
y
S
s
u
o
v
r
e
N
l
a
r
t
n
e
c

d
r
o
f
n
a
t
S
-
L
C
B
L
D

e
m
o
c
t
u
O
L
C
B
L
D

r
o
m
u
T
L
C
B
L
D

r
o
m
u
T
n
o
l
o
c

n

I

i
a
r
t
-
H
N
-
L
C
B
L
D

I

t
s
e
t
-
H
N
-
L
C
B
L
D

0
C
O

1
C
O

2
C
O

3
C
O

4
C
O

5
C
O

6
C
O

7
C
O

8
C
O

9
C
O

n

i
a
r
t
-
l
a
m
r
o
N
S
V
r
o
m
u
t
-
e
t
a
t
s
o
r
p

t
s
e
t
-
l
a
m
r
o
N
S
V
r
o
m
u
t
-
e
t
a
t
s
o
r
p

e
m
o
c
t
u
o
-
e
t
a
t
s
o
r
p

18

Figure 3: Multiple comparison test

19

0.002.004.006.008.0010.0012.00D3D5D7D9D11D13D15NaïveBayesAdaboostBaggingRandomForestMean ensemble accuracy differencesComparison of the classifiers based on the Tukey testFigure 4: Comparison of the D3 ensemble and the best performing classiﬁers from Naive

Bayes, Adaboost, Bagging and Random Forest.

20

-0,1-0,0500,050,10,150,2D3 Figure 5: Comparison of the D5 ensemble and the best performing classiﬁers from Naive

Bayes, Adaboost, Bagging and Random Forest.

21

-0,1-0,0500,050,10,150,2D5 Figure 6: Comparison of the D7 ensemble and the best performing classiﬁers from Naive

Bayes, Adaboost, Bagging and Random Forest.

22

-0,1-0,0500,050,10,150,2D7 Figure 7: Comparison of the D9 ensemble and the best performing classiﬁers from Naive

Bayes, Adaboost, Bagging and Random Forest.

23

-0,1-0,0500,050,10,150,2D9 Figure 8: Comparison of the D11 ensemble and the best performing classiﬁers from Naive

Bayes, Adaboost, Bagging and Random Forest.

24

-0,1-0,0500,050,10,150,2D11 Figure 9: Comparison of the D13 ensemble and the best performing classiﬁers from Naive

Bayes, Adaboost, Bagging and Random Forest.

25

-0,1-0,0500,050,10,150,2D13 Figure 10: Comparison of the D15 ensemble and the best performing classiﬁers from Naive

Bayes, Adaboost, Bagging and Random Forest.

26

-0,1-0,0500,050,10,150,2D15 Table 4: Diﬀerence of the respective ensembles and the best performing methods from

table 3.

Dataset

D3

D5

D7

D9

D11

D13

D15

breastCancer-train

0.01

0.11

0.08

0.02

0.01

0.08

0.04

breastCancer-test

0.14

0.16

0.14

0.07

0.08

0.07

0.08

centralNervousSystem

0.14

0.14

0.14

0.12

0.09

0.06

0.1

colonTumor

0.08

0.12

0.11

0.06

0.07

0.07

0.06

DLBCL-Stanford

0.05

0.07

0.06

0.05

0.04

0.02

0.02

DLBCLOutcome

0.11

0.1

0.11

0.1

0.07

0.04

0.05

DLBCLTumor

0.05

0.06

0.07

0.05

0.05

0.02

0.07

DLBCL-NIH-train

0

0.03

0.08

0.14

0.14

0.08

0.1

DLBCL-NIH-test

0.14

0.18

0.17

0.15

0.14

0.13

0.12

OC0

OC1

OC2

OC3

OC4

OC5

OC6

OC7

OC8

OC9

-0.01

0.05

0.04

0.06

0.05

0.04

0.06

-0.01

0.04

0.06

0.05

0.02

0.09

0.06

0.01

0.08

0.07

0.05

0.08

0.05

0

0.02

0.04

0

0.05

0.05

0.05

0.08

0

0.03

-0.01

0.01

0.03

0

0.06

0.08

0.09

0.08

0.11

0.09

0.07

0.07

0

0.03

0.03

0

0

0

-0.03

-0.01

0.01

0.04

0.03

0.02

0.05

0.04

-0.07

0

-0.02

0.03

0

0.04

-0.01

0.01

0.05

0.04

0.06

0.05

0.03

0.02

prostate-tumorVSNormal-train

-0.03

0.06

0.06

0.06

0.01

0.05

0.01

prostate-tumorVSNormal-test

0.06

0.04

0.04

0.02

-0.01

-0.04

-0.02

prostate-outcome

sum

0.11

0.88

0.09

27
1.58

0.02

0.01

0.03

0.02

0.04

1.41

1.3

1.11

1.02

1.02

5 ensemble members as an accurate choice. The presented approach is the

ﬁrst ensemble creation algorithm which creates diversity among classiﬁers

using an artiﬁcially created labelling, a technique which can hopefully reused

to create more robust algorithms in problems where individual classiﬁer ac-

curacy can be very varying.

In the future, the ensemble creation method

could be extended to handle unbalanced or multiclass classiﬁcation problems

eﬃciently.

Acknowledgments

The publication was supported by the T ´AMOP-4.2.2.C-11/1/KONV-

2012-0001 project. The project has been supported by the European Union,

co-ﬁnanced by the European Social Fund.

References

[1] B. Antal, A. Hajdu, An ensemble-based system for microaneurysm

detection and diabetic retinopathy grading, IEEE Transactions on

Biomedical Engineering 59 (2012) 1720 – 1726.

[2] B. Antal, A. Hajdu, An ensemble-based system for automatic screening

of diabetic retinopathy, Knowledge-Based Systems 60 (2014) 20–27.

[3] L. I. Kuncheva, Combining Pattern Classiﬁers. Methods and Algorithms,

Wiley, 2004.

[4] G. Brown, J. Wyatt, R. Harris, X. Yao, Diversity creation methods:

a survey and categorisation, Information Fusion 6 (1) (2005) 5–20.

doi:10.1016/j.inffus.2004.04.004.

28

URL

http://linkinghub.elsevier.com/retrieve/pii/

S1566253504000375

[5] L. Breiman, Bagging predictors, Machine Learning 24 (1995) 123–140.

[6] B. Efron, Nonparametric estimates of standard error: The jackknife, the

bootstrap and other methods, Biometrika 68 68 (1981) 589599.

[7] R. Bryll, Attribute bagging: improving accuracy of classiﬁer ensembles

by using random feature subsets, Pattern Recognition 20 (2003) 1291–

1302.

[8] T. Ho, The random subspace method for constructing decision forests,

IEEE Transactions on Pattern Analysis and Machine Intelligence 20

(1998) 832–844.

[9] Y. Freund, R. E. Schapire, A decision-theoretic generalization of on-line

learning and an application to boosting (1995).

[10] R. E. Schapire, The boosting approach to machine learning: An

overview, in: MSRI (Mathematical Sciences Research Institute) Work-

shop on Nonlinear Estimation and Classiﬁcation, 2003.

[11] D. Ruta, B. Gabrys, Classiﬁer selection for majority voting, Information

Fusion 6 (1) (2005) 63 – 81.

[12] R. Polikar, Ensemble based systems in decision making, IEEE Circuits

and Systems magazine Third Quarter (2006) 21–45.

[13] L. Kuncheva, C. Whitaker, Measures of diversity in classiﬁer ensembles

and their relationship with the ensemble accuracy, Machine Learning

29

51 (2) (2003) 181–207. doi:10.1023/A:1022859003006.

URL http://dx.doi.org/10.1023/A%3A1022859003006

[14] G. Brown, L. I. Kuncheva, GOOD and BAD diversity in majority vote

ensembles, in: Proc. 9th International Workshop on Multiple Classiﬁer

Systems (MCS’10), Vol. LNCS 5997 of LNCS, Springer-Verlag, Cairo,

Egypt, 2010, pp. 124–133.

[15] M. Berthod, Z. Kato, S. Yu, J. Zerubia, Bayesian image classiﬁcation

using markov random ﬁelds, Image and Vision Computing 14 (4) (1996)

285 – 295. doi:10.1016/0262-8856(95)01072-6.

URL

http://www.sciencedirect.com/science/article/pii/

0262885695010726

[16] J. M. Hammersley, P. Cliﬀord, Markov ﬁeld on ﬁnite graphs and lattices

(1971).

[17] S. Kirkpatrick, C. D. Gelatt, M. P. Vecchi, Optimization by simulated

annealing, Science 220 (1983) 671–680.

[18] J. Li, H. Liu, L. Wong, Mean-entropy discretized features are eﬀective

for classifying high-dimensional biomedical data, in: Proceedings of the

3rd ACM SIGKDD Workshop on Data Mining in Bioinformatics, 2003,

pp. 17–24.

[19] P. L. George H. John, Estimating continuous distributions in bayesian

classiﬁers, in: Eleventh Conference on Uncertainty in Artiﬁcial Intelli-

gence, 1995, pp. 338–345.

30

[20] I. H. Witten, E. Frank, Data Mining: Practical machine learning tools

and techniques, 2nd Edition, Morgan Kaufmann, San Francisco, 2005.

[21] M. Friedman, The use of ranks to avoid the assumption of normality

implicit in the analysis of variance, Journal of the American Statistical

Association 32 (1937) 675–701.

[22] J. W. Tukey, Comparing individual means in the analysis of variance.,

Biometrics 5 (1949) 99–114.

31

