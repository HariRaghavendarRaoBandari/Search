A Universal Coding Scheme for Remote Generation

of Continuous Random Variables

Cheuk Ting Li and Abbas El Gamal

1

6
1
0
2

 
r
a

M
 
6
1

 
 
]
T
I
.
s
c
[
 
 

1
v
8
3
2
5
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

We consider a setup in which Alice selects a pdf f from a set of prescribed pdfs P and sends a preﬁx-free codeword W to
Bob in order to allow him to generate a single instance of the random variable X ∼ f. We describe a universal coding scheme
for this setup and establish an upper bound on the expected codeword length when the pdf f is bounded, orthogonally concave
(which includes quasiconcave pdf), and has a ﬁnite ﬁrst absolute moment. A dyadic decomposition scheme is used to express
the pdf as a mixture of uniform pdfs over hypercubes. Alice randomly selects a hypercube according to its weight, encodes its
position and size into W , and sends it to Bob who generates X uniformly over the hypercube. Compared to previous results on
channel simulation, our coding scheme applies to any continuous distribution and does not require two-way communication or
shared randomness. We apply our coding scheme to classical simulation of quantum entanglement and obtain a better bound on
the average codeword length than previously known.

Universal code, channel simulation, communication complexity, simulation of quantum entanglement.

Index Terms

I. INTRODUCTION

Consider the one-shot remote random variable generation setting depicted in Figure 1. Alice and Bob both agree on a set
of distributions P (over a discrete or continuous set). Alice selects a distribution p ∈ P and wishes to have Bob generate a
random variable X according to this distribution. To accomplish this goal, Alice and Bob use an agreed upon universal coding
scheme in which Alice uses a stochastic encoder to assign to each p ∈ P a codeword W ∈ {0, 1}∗ from an agreed upon
preﬁx-free code and Bob uses a stochastic decoder to generate a single instance of X ∼ p from the received codeword W . Let
L(W ) be the length of W in bits. Is there a coding scheme such that for every distribution p ∈ P, Bob can generate X ∼ p
with ﬁnite expected codeword length Ep(L(W ))?

p ∈ P

Encoder

W ∈ {0, 1}∗

Decoder

X ∼ p

Figure 1. Universal remote generation of random variables.

The answer to this question clearly depends on the set of distributions P. Consider the following two simple special cases:
1. Let P be the set of probability mass functions over the integers, then we can use the following “generate–compress”
strategy. Alice generates X ∼ p and then uses a universal code over the integers, e.g., [1], [2], to encode X into W .
Upon receiving W , Bob recovers X. Using these codes, the expected codeword length Ep(L(W )) is ﬁnite as long as
Ep(log X) is ﬁnite. Note that this scheme uses a stochastic encoder but a deterministic decoder.

2. Let X be continuous and the class of pdfs P has a ﬁnite (or countable) cardinality, then we can use the following
“compress–generate” strategy. Alice encodes the index of p into W . Upon receiving W , Bob ﬁrst recovers p then use it
to generates X. Note that in this scheme, the encoder is deterministic but the decoder is stochastic.

If we index the set P by θ ∈ Θ, then our setting can be viewed as a one-shot synthesis (or simulation) of a channel
from θ to X with only one-way communication and without common randomness. Several channel simulation scenarios have
been previously studied in classical and quantum information theory. In [3], Bennett et al. considered the asymptotic setting
and established the reverse Shannon theorem, which states that k uses of a channel with capacity C can be simulated using
kC + o(k) bits of communication with unlimited amount of common randomness. In [4], Winter studied the asymptotic case
with limited common randomness and θi distributed according to a given distribution. He showed that kI(θ; X) + o(k) bits
of communication and kH(X|θ) + o(k) bits of common randomness sufﬁce. Subsequently, Cuff [5] characterized the entire
tradeoff region between communication and common randomness for the same setting.
For the one-shot channel simulation setting, schemes based on rejection sampling were developed by Steiner [6], who assumed
that Alice and Bob share unlimited common randomness, and by Massar et al. [7], who assumed two-way communication
between Alice and Bob. Harsha et al. [8] established a one-shot version of the reverse Shannon theorem using rejection sampling.
These rejection sampling schemes, however, are sensitive to the size of P — a large size P leads to a high rejection rate,
which in turn leads to a high computation time.

2

Note that the aforementioned asymptotic and one-shot channel simulation schemes are not universal since a scheme designed
for a channel from θ to X is guaranteed to work only when the simulated distribution lies in the convex hull of the set of
output distributions {p(x|θ)}.
In this paper, we present a universal coding scheme for remote generation of continuous random variables (over scalars or
vectors), which we will refer to as universal dyadic coding scheme. When P is restricted to the set of orthogonally concave
pdfs {fX(x)}, (which includes quasiconcave), we are able to establish an upper bound on the expected codeword length of
W in terms of supx fX (x) and E(kXk∞). Our scheme uses a dyadic decomposition to express the selected pdf as a mixture
of uniform distributions over hypercubes. Alice ﬁrst selects a hypercube from this mixture at random according to its weight,
then encodes its position and size into a codeword W using an agreed upon universal code over the integers. Upon receiving
W , Bob ﬁnds the hypercube and generates X uniformly over it.

In [9], a similar dyadic decomposition scheme was introduced for distributed simulation of continuous random variables
according to an agreed upon pdf in a non-universal manner. In Section II we provide a more detailed comparison between
these two dyadic coding schemes.

To further motivate our setup and universal dyadic coding scheme, consider the following two applications.

Application 1 (Classical simulation of quantum entanglement). The simulation of correlations induced by quantum entan-
glement using classical communication has been widely studied, e.g., see [10], [11], [6]. Consider the Bell state |Φ+i =
(|0iA|0iB + |1iA|1iB)/√2 of a pair of qubits [12], one held by Alice and the other held by Bob. If Alice measures her qubit
in the direction θA (unknown to Bob) to obtain YA ∈ {±1} and Bob measures his qubit in the direction θB (unknown to
Alice) to obtain YB ∈ {±1}, then P{YA = 1} = P{YB = 1} = 1/2 and E[YAYB] = − cos(θA − θB). By Bell’s theorem, it is
impossible to simulate the joint distribution of (YA, YB) for all θA and θB using a classical common randomness source (local
hidden variables) between Alice and Bob in place of the qubits. However, such simulation is possible if we instead allow Alice
to send a codeword W to Bob. By a modiﬁcation of the expression in [13] and letting X ∈ [0, 2π] be a random variable with
conditional pdf
(1)

f (x|yA; θA) =

max{cos(yA(x − θA)), 0},

1
2

and YB = − sgn(cos(X − θB)), then (YA, YB) follows the desired distribution. Hence Alice can generate YA and use our
universal remote generation coding scheme to encode f (x|YA; θA) into W to allow Bob to generate X and YB. Using Theorem 3
in Section IV, we show that the expected number of bits is bounded as E(L(W )) ≤ 12.31, and using numerical computation
we show that E(L(W )) ≤ 8.96 is achievable. In comparison, the scheme in [7], which requires two-way communication (we
only allow one-way) provides a looser upper bound of 20 bits on the average number of bits needed.

Application 2 (Minimax mixed strategy with a helper). In decision theory, it is sometimes desirable to adopt a mixed strategy
in which the decision is chosen at random. Suppose the payoff g(X, θ) depends on the agent’s decision X and an unknown
parameter θ selected from a set Θ (which may be chosen by an adversary). The optimal minimax mixed strategy to choose X
is

F ∗

X (x) = arg max

FX (x)

E [g(X, θ)] .

inf
θ∈Θ

Now suppose the parameter θ = (θ1, θ2), where θ1 is known to a helper (Alice) but θ2 is not known to Alice or the decision
agent (Bob). Alice wishes to help Bob generate the decision X with the optimal pdf given θ1,

F ∗

X (x; θ1) = arg max

FX (x)

E [g(X, θ1, θ2)] .

inf
θ2

To help Bob generate X using this optimal pdf, Alice can use our universal remote generation coding scheme. For example,
consider the payoff function

g(x, θ) =(e2θ−x

0

if x ≥ θ
if x < θ,

where θ ≥ 0. If nothing else is known about θ, then the optimal minimax mixed strategy would be to choose X according to
the pdf f ∗
X(x) = e−x for x ≥ 0, which guarantees a payoff of 1/2. Now assume that Alice knows that θ ≥ a ≥ 0, then the
optimal mixed strategy is f ∗
X (x; a) = e−(x−a) for x ≥ a, which results in a payoff of (1/2)ea. As shown in Theorem 2 in
Section III, Alice can use our universal remote generation coding scheme to enable Bob to generate X according to this pdf
using no more than log(a + 1) + 2 log(log(a + 1) + 12) + 23 bits on average. Our universal coding scheme can also be used
to perform mixed strategies in other scenarios, e.g., Nash equilibrium [14] in non-cooperative games.

The rest of the paper is organized as follows. For clarity of presentation, we ﬁrst present the construction of our universal
dyadic coding scheme for uniform distributions over subsets of Rn and upper bound its expected codeword length. In Section
III, we extend our scheme to non-uniform distributions, establishing an upper bound on the expected codeword length for
orthogonally concave pdfs. In Section IV, we present a variant of our scheme for distributions with a uniform bounded support

3

and apply it to the simulation of the Bell state. Finally, in Section V, we present a lower bound on the expected codeword
length in terms of the relative entropy between the actual and the implied distribution of our scheme.

A. Notation

Throughout this paper, we assume that log is base 2 and entropy H is in bits. Log in base e is written as ln. We use the

notation: [a : b] = [a, b] ∩ Z,
A set A ⊆ Rn is orthogonally convex if for any line L parallel to one of the n axes, L∩ A is a connected set (empty, a point
or an interval). A function f is orthogonally concave if the hypograph {(x, α) : x ∈ Rn, α ≤ f (x)} is orthogonally convex.
1A(x)dx. For A, B ⊆ Rn, A + B denotes the
For a Lebesgue measurable set A ⊆ Rn, we deﬁne the volume Vn(A) = ´Rn
Minkowski sum {a + b : a ∈ A, b ∈ B}, and for x ∈ Rn, A + x = {a + x : a ∈ A}. For γ ∈ R, γA = {γa : a ∈ A}. For
M ∈ Rn×n, M A = {M a : a ∈ A}. The erosion A ⊖ B is deﬁned as {x ∈ Rn : B + x ⊆ A}.

II. UNIFORM DISTRIBUTIONS

In this section, we develop our universal dyadic coding scheme for the set of uniform pdfs over ﬁnite volume sets A ⊆ Rn.

We ﬁrst introduce the dyadic decomposition of a set [9], which is the building block of our coding scheme.
Deﬁnition 1 (Dyadic decomposition). For v ∈ Zn and k ∈ Z, deﬁne the hypercube Ck,v = 2−k([0, 1]n + v) ⊂ Rn. For a set
A ⊆ Rn with a boundary of measure zero and k ∈ Z, deﬁne the set

Dk(A) =(cid:8)v ∈ Zn : Ck,v ⊆ A and Ck−1,⌊v/2⌋ * A(cid:9) ,

where ⌊v/2⌋ is the vector formed by the entries ⌊vi/2⌋. The dyadic decomposition of A is the partitioning of A into hypercubes
{Ck,v} such that v ∈ Dk(A) and k ∈ Z. Since every point x in the interior of A is contained in some hypercube in A, the
interior of A is contained in ∪k∈Z, v∈Dk(A)Ck,v, and the set of points in A not covered by the hypercubes has measure zero.
Our scheme uses a universal code over the integers to encode the position and size of the hypercubes. In particular, we will

use the signed Elias delta code deﬁned as follows [2]. Let

Then the signed Elias code is

gγ+(k) = 0N k 1 k aN −1aN −2 . . . a0,
gδ+(k) = gγ+(N + 1)k aN −1aN −2 . . . a0.

gδ(k) =(gδ+(1 − 2k)

gδ+(2k)

if k ≤ 0,
if k > 0,

where aN aN −1 . . . a0 is the binary representation of k. The length of the codeword gδ(k) is
L(gδ(k)) = ⌊log(2|k| + 1)⌋ + 2 ⌊log (⌊log(2|k| + 1)⌋ + 1)⌋ + 1.
We are now ready to deﬁne the universal dyadic coding scheme for the set of uniform pdfs.

(2)

Universal dyadic coding scheme for uniform pdfs. The universal dyadic coding scheme for the set of uniform pdfs over
positive, ﬁnite volume subsets A ⊂ Rn with a boundary of measure zero consists of:

1) A stochastic encoder that generates ˜x according to the observed uniform pdf over A. It then ﬁnds w = (k, v) such that
v ∈ Dk(A) and ˜x ∈ Ck,v. The encoder then maps (k, v) into a codeword w which consists of the concatenation of
signed Elias delta codewords for k, v1, . . . , vn, i.e., w = gC(k, v) = gδ(k)k gδ(v1)k ··· k gδ(vn).

2) A stochastic decoder that upon receiving w recovers (v, k) and generates x according to a uniform pdf over Ck,v.

The dyadic decomposition for R2 and the assignments of codeword to the squares are illustrated in Figure 2.

The following illustrates how our scheme is used for a given pdf.

Example 1. Consider a uniform pdf over the ellipse A =(cid:8)x ∈ R2 : xT Kx < 1(cid:9), K =


the universal dyadic coding scheme for this pdf. The encoder ﬁrst generates a point in the ellipse uniformly at random, and
then sends the codeword representing the square containing the point. The expected codeword length (computed by listing all
squares in the dyadic decomposition with side length at least 2−16) is 15.6. Note that the entropy of W , H(W ) = 6.35 is
signiﬁcantly smaller since the code is universal.

4/3 
4/3 −2/3

−2/3

. Figure 3 depicts

The length of the codeword of the universal dyadic coding scheme depends on the magnitude of k and v1, . . . , vn, (which
depends on k and kxk), hence the length can be bounded using k and kxk. in [9], it is shown that the expected value of k
can be bounded using the following quantity.

4

0, (−1, 0)

0, (0, 0)

1, (0, 1)

1, (1, 1)

2, (2, 3)

2, (3, 3)

101011

111

010010100

010001000100

011000110001110

011000111001110

0, (−1, −1)

0, (0, −1)

1, (0, 0)

1, (1, 0)

2, (2, 2)

2, (3, 2)

101010101

110101

010011

010001001

011000110001100

011000111001100

Figure 2. Dyadic squares Ck,v used in the dyadic decomposition for n = 2, showing their associated k, v and codeword assignments.

01100101100 01100010001100

011000110001100

010001011

010011

01100011000100

01100011001

01100011110101

011000111101101

010001010101

010010101

011000111101111

011000110101111 01100010101111

Figure 3. Universal dyadic coding scheme on the uniform distribution over an ellipse.

Deﬁnition 2 (Erosion entropy). The erosion entropy of the set A by the set B, where A ⊆ Rn with Vn(A) < ∞, and B ⊆ Rn
is a convex set, is deﬁned as

−∞(cid:18)1 {t ≥ 0} −
where A ⊖ B = {x ∈ Rn : B + x ⊆ A} is the erosion of A by B.

h⊖B(A) = ˆ ∞

Vn (A ⊖ 2−tB)

Vn(A)

(cid:19) dt,

If A is orthogonally convex, the erosion entropy of A by the hypercube [0, 1]n can be bounded by the expected norm of

the uniform distribution on A, as shown in the following.
Lemma 1. Let the set A ⊆ Rn be orthogonally convex with Vn(A) < ∞, and let X ∼ Unif(A), then

h⊖[0,1]n(A) ≤ (n − 1) log E [kXk∞] − log Vn(A) + 4n.

The proof of this lemma is given in Appendix A. We now use the erosion entropy to bound the expected codeword length

of the universal dyadic coding scheme.

Theorem 1. The expected codeword length of the universal dyadic coding scheme for uniform pdfs for X ∼ Unif(A) is upper
bounded as

5

E [L(W )] ≤ nℓδ(cid:16)h + Ehlog(cid:16)kXk∞ + V1/n

n (A)(cid:17)i + 4(cid:17) + ℓδ(cid:18)log(cid:18)h + 2 maxnlog V1/n
≤ nℓδ (h + log E[kXk∞] + 8) + ℓδ (log (h + 2 max{log E[kXk∞], 0} + 9) + 2) ,

n (A), 0o +

where ℓδ(t) = t + 2 log t and h = h⊖[0,1]n(A).

5

2(cid:19) + 2(cid:19)

Theorem 1 shows that the expected codeword length depends on the erosion entropy, the expected magnitude of X, and the
volume of the set. Intuitively, the erosion entropy measures the complexity of the set (or loosely speaking its surface area to
volume ratio). However, the erosion entropy is invariant under shifting. Since our universal scheme is sensitive to the position
of A as well its shape, the bound in Theorem 1 depends also on the expected magnitude of X. The function ℓδ(t) in Theorem 1
comes from the length of the Elias delta code in (2). Other universal codes for integers may be used in place of Elias delta
code, and result in a different bound.

We now prove Theorem 1.

Proof of Theorem 1: Let x ∈ A. Consider the length of the codeword for (v, k) with v ∈ Dk(A) and x ∈ Ck,v. We have

x ∈ 2−k ([0, 1]n + v), hence kxk∞ ≥ 2−k maxi (|vi + 1/2| − 1/2). Since 2−nk ≤ Vn(A), k ≥ −(1/n) log Vn(A). Let

then |k| ≤ k + τ. From (2), the length of the codeword for (v, k) is

L(gC(v, k)) ≤ ⌊log(2|k| + 1)⌋ + 2 ⌊log (⌊log(2|k| + 1)⌋ + 1)⌋

n

τ = 2 max{(1/n) log Vn(A), 0} ,

(a)

+

+

+

n

n

vi +

1

+

vi +

+

1

2(cid:12)(cid:12)(cid:12)(cid:12)

1

2(cid:12)(cid:12)(cid:12)(cid:12)

1

2(cid:19) + 2(cid:19)(cid:19) + 2n + 2

≤ log(|k| + 1/2) + 2 log (log(|k| + 1/2) + 2)

≤ log(k + τ + 1/2) + 2 log (log(k + τ + 1/2) + 2)

2(cid:19) + 2 log(cid:18)log(cid:18)(cid:12)(cid:12)(cid:12)(cid:12)

(⌊log(2|vi| + 1)⌋ + 2 ⌊log (⌊log(2|vi| + 1)⌋ + 1)⌋) + n + 1

Xi=1
Xi=1(cid:18)log(cid:18)(cid:12)(cid:12)(cid:12)(cid:12)
Xi=1(cid:0)log(cid:0)2k(cid:0)kxk∞ + 2−k(cid:1)(cid:1) + 2 log(cid:0)log(cid:0)2k(cid:0)kxk∞ + 2−k(cid:1)(cid:1) + 2(cid:1)(cid:1) + 2n + 2
≤ log(k + τ + 1/2) + 2 log (log(k + τ + 1/2) + 2)
+ n(cid:16)log(cid:16)2k(cid:16)kxk∞ + V1/n
= n(cid:16)log(cid:16)kxk∞ + V1/n
= nℓδ(cid:16)log(cid:16)kxk∞ + V1/n

n (A)(cid:17) + k + 2 + 2 log(cid:16)log(cid:16)kxk∞ + V1/n
n (A)(cid:17) + k + 2(cid:17) + ℓδ (log(k + τ + 1/2) + 2)

n (A)(cid:17)(cid:17) + 2 log(cid:16)log(cid:16)2k(cid:16)kxk∞ + V1/n

+ log(k + τ + 1/2) + 2 + 2 log (log(k + τ + 1/2) + 2)

n (A)(cid:17) + k + 2(cid:17)(cid:17)

n (A)(cid:17)(cid:17) + 2(cid:17)(cid:17) + 2n + 2

where (a) follows by the fact that ⌊log(2|i| + 1)⌋ ≤ log(|2i + 1| + 1).

Let X ∼ Unif(A), and V, K be such that V ∈ DK(A) and X ∈ CK,V . Then we have

E [L(Enc(Unif(A))] = E [L(gC(V, K)]

≤ n Ehℓδ(cid:16)log(cid:16)kXk∞ + V1/n
≤ nℓδ(cid:16)Ehlog(cid:16)kXk∞ + V1/n

n (A)(cid:17) + K + 2(cid:17)i + E [ℓδ (log(K + τ + 1/2) + 2)]
n (A)(cid:17)i + E[K] + 2(cid:17) + ℓδ (log(E[K] + τ + 1/2) + 2)

by Jensen’s inequality and the concavity of ℓδ. We now proceed to bound

Consider

E[K] =

1

Vn(A)

∞

Xk=−∞

k · 2−nl |Dl(A)| .

k

Xl=−∞

2−nl |Dl(A)| = 2−nk |{v ∈ Zn : Ck,v ⊆ A}|

≥ 2−nk(cid:12)(cid:12)(cid:8)v ∈ Zn : Ck−1, (v−w)/2 ⊆ A(cid:9)(cid:12)(cid:12) .

for any w ∈ [0, 1]n, since Ck,v ⊆ Ck−1, (v−w)/2. Note that the (v − w)/2 in the subscript may not have integer entries. The
same deﬁnition of Ck,v = 2−k([0, 1]n + v) can still be applied, however. Also

6

ˆ[0,1]n(cid:12)(cid:12)(cid:8)v ∈ Zn : Ck−1, (v−w)/2 ⊆ A(cid:9)(cid:12)(cid:12) dw = Xv∈Zn
= 2nˆRn
= 2n2n(k−1)Vn(cid:16)A ⊖ [0, 2−(k−1)]n(cid:17)
= 2nkVn(cid:16)A ⊖ [0, 2−(k−1)]n(cid:17) .

ˆ[0,1]n
1 {Ck−1,w ⊆ A} dw

1(cid:8)Ck−1, (v−w)/2 ⊆ A(cid:9) dw

k

2−nl |Dl(A)| ≥ Vn(cid:16)A ⊖ [0, 2−(k−1)]n(cid:17) ,

Xl=−∞
2−nl |Dl(A)| ≤ Vn(A) − Vn(cid:16)A ⊖ [0, 2−(k−1)]n(cid:17) .

∞

Xl=k+1

∞

1

E[K] =

=

≤

k

1

∞

Vn(A)

Vn(A)

k · 2−nl |Dl(A)|

Xk=−∞
Xk=−∞ 1 {k ≥ 0} Vn(A) −
Xk=−∞(cid:16)1 {k ≥ 0} Vn(A) − Vn(cid:16)A ⊖ [0, 2−(k−1)]n(cid:17)(cid:17)
Vn(A) ˆ (cid:0)1 {t ≥ 0} Vn(A) − Vn(cid:0)A ⊖ [0, 2−t]n(cid:1)(cid:1) dt + 2

2−nl |Dl(A)|!

Xl=−∞

Vn(A)

≤
= h⊖[0,1]n(A) + 2.

∞

1

1

Hence

and

As a result,

We have

E [L(W )] ≤ nℓδ(cid:16)h + Ehlog(cid:16)kXk∞ + V1/n

n (A)(cid:17)i + 4(cid:17) + ℓδ(cid:18)log(cid:18)h + 2 maxnlog V1/n

n (A), 0o +

5

2(cid:19) + 2(cid:19) .

It remains to bound V1/n

n (A) by E[kXk∞]. By the Markov inequality,
n (A) · P(cid:26)kXk∞ ≥
1
4
V1/n
n (A) ·

E[kXk∞] ≥

V1/n

1
4

1
4

=

V1/n

n (A)(cid:27)

Vnnx ∈ A : kxk∞ ≥ (1/4)V1/n
Vn(A) −(cid:16)(1/2)V1/n

n (A)(cid:17)n

Vn(A)

Vn(A)

n (A)o

1
4
1
8

≥
≥

V1/n
n (A) ·
V1/n
n (A).

Hence,

E [L(W )] ≤ nℓδ (h + log E[kXk∞] + 4 + log 9) + ℓδ(cid:18)log(cid:18)h + 2 max{log E[kXk∞] + 3, 0} +

This completes the proof of the theorem.

5

2(cid:19) + 2(cid:19) .

Combining Lemma 1 and Theorem 1, we can bound the expected length of the universal dyadic coding scheme for

orthogonally convex sets.

(3)

(4)

7

Corollary 1. The expected codeword length of the universal dyadic coding scheme for uniform pdfs applied to an orthogonally
convex A ⊆ Rn is upper bounded as

E [L(W )] ≤ nℓδ ((n − 1) log r + log(kˆxk∞ + r) − log Vn(A) + 4n + 8)

+ ℓδ (log ((n − 1) log r + 2 max{r, 0} − log Vn(A) + 4n + 9) + 2) .

for any ˆx ∈ Rn, where ℓδ(t) = t + 2 log t and r = E [kX − ˆxk∞].

Proof of Corollary 1: By Lemma 1,

By Theorem 1,

h⊖[0,1]n(A) ≤ (n − 1) log E [kX − ˆxk∞] − log Vn(A) + 4n.

E [L(W )] ≤ nℓδ(cid:16)h + log(cid:16)E[kXk∞] + V1/n

n (A)(cid:17) + 4(cid:17) + ℓδ(cid:18)log(cid:18)h + 2 maxnlog V1/n

n (A), 0o +

5

2(cid:19) + 2(cid:19)

≤ nℓδ (h + log E[kXk∞] + 8) + ℓδ (log (h + 2 max{log E[kX − ˆxk∞], 0} + 9) + 2)
≤ nℓδ ((n − 1) log E [kX − ˆxk∞] + log E[kXk∞] − log Vn(A) + 4n + 8)
+ ℓδ (log ((n − 1) log E [kX − ˆxk∞] + 2 max{log E[kX − ˆxk∞], 0} − log Vn(A) + 4n + 9) + 2) .

An added beneﬁt of our universal dyadic coding scheme is that if X can be generated in a distributed manner. Suppose X
is an n-dimensional vector X1, . . . , Xn and instead of having one decoder wishing to generate X, we have n decoders that all
receive W and decoder i wishes to generate only Xi, i ∈ [1 : n]. Such distributed generation is possible using our universal
dyadic coding scheme since decoder i can generate Xi uniformly over the interval [2−kvi, 2−k(vi + 1)] without any need
to cooperate with other decoders. In [9]. we described a dyadic decomposition coding scheme for distributed generation of a
given pdf. The scheme in this paper differs from that in [9] in several aspects.

and the decoders.

• The scheme in this paper is universal, while the scheme in [9] is constructed for a given pdf known to both the encoder
• In [9] we used an optimal preﬁx free code, such as Huffman code, to encode the hypercubes, while in this paper we use
• In [9], we can perform scaling (and bijective transformations) on each variable Xi before applying the dyadic decompo-
sition scheme. It is not possible to perform such preprocessing here since the decoder would not know the scaling factor
or the bijective transformation used.

a universal code over the integers since the distribution on the hypercubes is known a priori.

• In the analysis of the expected codeword length in [9], it sufﬁces to consider only the distribution of the sizes of the
hypercubes. In our universal scheme, both the size and the position of the hypercube affect the length of the codeword
assigned to it.

III. NON-UNIFORM DISTRIBUTIONS

In this section, we extend the results of the previous section to the case where the pdf of X = (X1, . . . , Xn) is selected
from a set of arbitrary (not necessarily uniform) pdfs. The key idea in extending our scheme is the following. Note that in
z (f )) for z ≥ 0 and
general, any pdf can be written as a mixture of uniform pdfs. Let Z ∼ fZ, where fZ(z) = Vn(L+
z (f )), then we have X ∼ f (x). Hence
z (f ) = {x ∈ Rn : f (x) ≥ z} is the superlevel set of f . Let X|{Z = z} ∼ Unif(L+
L+
f (x) can be expressed as a mixture of uniform distributions over L+
z (f ) for different values of z. Alice can ﬁrst generate
Z ∼ fZ, then apply the universal dyadic coding scheme for uniform distributions on L+
Z (f ). The scheme is formally deﬁned
as follows.
Universal dyadic coding scheme for general pdfs. The universal dyadic coding scheme for the set of almost everywhere
continuous pdfs P consists of:

1) A stochastic encoder that generates ˜x according to the observed f and Z ∼ Unif[0, f (˜x)], and ﬁnds (k, v) such that
z (f )) and ˜x ∈ Ck,v. The encoder maps (k, v) into a codeword w that consists of the concatenation of the

v ∈ Dk(L+
signed Elias delta codewords for k, v1, . . . , vn, i.e., w = gC(k, v) = gδ(k)k gδ(v1)k ··· k gδ(vn).
2) A stochastic decoder that upon receiving w recovers (v, k) and generates x uniformly over Ck,v.
We illustrate this scheme in the following.

Example 2. Assume that the selected pdf is Gaussian with zero mean and unit variance. Figure 4 depicts the universal dyadic
coding scheme for this pdf. The horizontal and vertical axes represent x and z, respectively. The encoder sends the codeword
for the rectangle containing (x, z). The expected codeword length (computed by listing all intervals in the dyadic decomposition
with length at least 2−20) is 7.06.

As a consequence of Theorem 1, we have the following bound on the expected codeword length.

8

01000101

01001

10101

11

010001111

010001100

010000100011 01010101

01011

010000100000

Figure 4. Universal dyadic coding scheme on N (0, 1).

Theorem 2. The expected codeword length of the universal dyadic coding scheme for X ∼ f (x) is upper bounded as

E [L(W )] ≤ nℓδ (EZ [h] + log E[kXk∞] + 8) + ℓδ (log (EZ [h] + 2 max{log E[kXk∞], 0} + 10) + 2) ,

where ℓδ(t) = t + 2 log t and h = h⊖[0,1]n(L+

Z (f )) is a random variable, where Z ∼ fZ, fZ(z) = Vn(L+

z (f )) for z ≥ 0.

Proof: Using (4),
E [L(W )] ≤ n EZ [ℓδ (h + log E[kXk∞ | Z] + 8)] + EZ [ℓδ (log (h + 2 max{log E[kXk∞], 0} + 8.5) + 2)]

≤ nℓδ (EZ [h] + log E[kXk∞] + 8) + ℓδ (log (EZ [h] + 2 EZ [max{log E[kXk∞ | Z], 0}] + 8.5) + 2) .

To bound EZ [max{log E[kXk∞ | Z], 0}], deﬁne a concave function q : [0,∞) → [0,∞),

Note that

Hence,

q(t) =(te−1 log e

log t

if t ≤ e
if t > e.

max{log t, 0} ≤ q(t) ≤ max{log t, 0} + e−1 log e.

EZ [max{log E[kXk∞ | Z], 0}] ≤ EZ [q(E[kXk∞ | Z])]

≤ q(E[kXk∞])
≤ max{log E[kXk∞], 0} + e−1 log e.

Note that we also need L+
z (f ) to have a boundary of measure zero for almost all z, in order for the coding scheme to succeed
almost surely. This is implied by the almost everywhere continuity of f (x). The proof of this claim is given in Appendix B.

We can also generalize Corollary 1 to orthogonally concave pdfs (which includes quasiconcave pdfs) as follows.

Corollary 2. The expected codeword length of the universal dyadic coding scheme for X ∼ f (x), where f is orthogonally
concave, is upper bounded as

E [L(W )] ≤ nℓδ ((n − 1) log r + log(kˆxk∞ + r) + h(Z) + 4n + 8)

+ ℓδ (log ((n − 1) log r + 2 max{log r, 0} + h(Z) + 4n + 10) + 2) .

for any ˆx ∈ Rn, where ℓδ(t) = t + 2 log t, r = E [kX − ˆxk∞], Z ∼ fZ, fZ(z) = Vn(L+
supx f (x) < ∞,
f (x) + 4n + 8(cid:19)

E [L(W )] ≤ nℓδ(cid:18)(n − 1) log r + log(kˆxk∞ + r) + log sup

x

+ ℓδ(cid:18)log(cid:18)(n − 1) log r + 2 max{log r, 0} + log sup

x

f (x) + 4n + 10(cid:19) + 2(cid:19) .

9

z (f )) for z ≥ 0. As a result, if

IV. BOUNDED SUPPORT DISTRIBUTIONS

In this section, we present a variant of the universal dyadic coding scheme for a set of distributions with a uniform bound
on their support. Without loss of generality, assume P consists of the set of pdfs over [0, 1]n. Since the v in the deﬁnition of
our universal dyadic coding scheme (corresponding to the position of the hypercube) is bounded, we can use a ﬁxed length
code to encode (v1, . . . , vn). This allows us to reduce the expected codeword length.
Universal dyadic coding scheme for pdfs over the unit hypercube. The universal dyadic coding scheme for pdfs over [0, 1]n
consists of:

1) A stochastic encoder that generates ˜x according to the observed f and z ∼ Unif[0, f (˜x)] and ﬁnds (k, v) such that
z (f )) and ˜x ∈ Ck,v. The encoder then maps (k, v) into a codeword which consists of the concatenation of
v ∈ Dk(L+
the unsigned Elias gamma codeword for k + 1, and the k-bit binary representations of v1, . . . , vn, i.e., w = gC (k, v) =
gγ+(k + 1)k gb,k(v1)k ··· k gb,k(v1), where gb,k(i) is the binary representation of i with k bits, possibly with leading
zeros.

2) A stochastic decoder that upon observing w recovers (v, k) and generates x uniformly over Ck,v.
Since the length of the unsigned Elias gamma codeword gγ(k + 1) is 2 ⌊log(k + 1)⌋ + 1, the length of w is

The expected codeword length is upper bounded as follows.

L(w) = nk + 2 ⌊log(k + 1)⌋ + 1.

Theorem 3. The expected codeword length of the universal dyadic coding scheme for pdfs over the unit hypercube for
X ∼ f (x), where f is orthogonally concave, is upper bounded as

E [L(W )] ≤ n (h(Z) + log n + log e + 2) + 2 log (h(Z) + log n + log e + 3) + 1,

f (x) + log n + log e + 3(cid:19) + 1.

Proof: In [9] (Theorem 1), it was shown that the erosion entropy for orthogonally convex A is bounded as

where Z ∼ fZ, fZ(z) = Vn(L+
E [L(W )] ≤ n(cid:18)log sup

x

z (f )) for z ≥ 0. As a result, if supx f (x) < ∞,
f (x) + log n + log e + 2(cid:19) + 2 log(cid:18)log sup
h⊖[0,1]n(A) ≤ log(cid:18)Pn

i=1 Vn−1P\i(A)

Vn(A)

x

(cid:19) + log e,
where VP\i(A) = {(x1, . . . , xi−1, xi+1, . . . , xn) : x ∈ A}. Let Z ∼ fZ, fZ(z) = Vn(L+
z (f )) + log n + log e.

h⊖[0,1]n (L+

z (f )) ≤ − log Vn(L+

z (f )), then

Hence

E [L(W )] ≤ n E[K] + 2 log(E[K] + 1) + 1

≤ n(cid:0)E[h⊖[0,1]n(L+
≤ n(cid:0)E[− log Vn(L+

Z (f ))] + 2(cid:1) + 2 log(cid:0)E[h⊖[0,1]n(L+
Z (f ))] + 3(cid:1) + 1
Z (f ))] + log n + log e + 2(cid:1) + 2 log(cid:0)E[− log Vn(L+

= n (h(Z) + log n + log e + 2) + 2 log (h(Z) + log n + log e + 3) + 1.

Z (f ))] + log n + log e + 3(cid:1) + 1

As an example, we apply this result to simulating the Bell state in Application 1

ﬁtted to the interval [0, 1]. Although this pdf is not orthogonally concave, it can be decomposed into at most two orthogonally
concave parts with disjoint support, hence the expected codeword length is the weighted average of the expected codeword
lengths for those two pieces, which incurs a penalty of at most 1 bit. By Theorem (3),

f (x | θ) = π max{cos(2π(x − θ)), 0},

E [L(W )] ≤ log π + log e + 2 + 2 log (log π + log e + 3) + 2 ≈ 12.31.

Figure 5 plots the numerical values of E [L(W )] versus θ computed by listing all intervals in the dyadic decomposition with
length at least 2−17. As can be seen, E [L(W )] ≤ 8.96 for all θ.

10

9.0

8.8

8.6

8.4

8.2

8.0

7.8

7.6

7.4

]
)

W
(
L
E

[

7.2

0.0

0.2

0.4

0.6

0.8

1.0

θ

Figure 5. Universal dyadic coding scheme for pdfs over the unit hypercube applied to the simulation of the Bell state.

V. LOWER BOUND ON EXPECTED CODEWORD LENGTH

In the previous sections we focused on schemes for universal remote generation of continuous random variables and upper
bounds on their expected codeword length. In this section, we establish a lower bound on the expected codeword length that
every remote generation scheme must satisfy.

Consider a universal remote generation coding scheme with a preﬁx-free codeword set C ⊆ {0, 1}∗. Upon receiving w ∈ C,

Bob generates X|{W = w} ∼ ˜pw according to a distribution ˜pw(dx). Deﬁne the implied distribution of this scheme as

pIm(dx) =(cid:16) Xw′∈C

2−L(w′)(cid:17)−1

Xw∈C

2−L(w) ˜pw(dx).

We now show that the expected codeword length Ep(L(W )) is lower bounded by the relative entropy between p and pIm.
Theorem 4. For a universal remote generation scheme with an implied distribution pIm, the average codeword length for
p ∈ P is lower bounded as

Ep(L(W )) ≥ D(pkpIm).

Proof: Consider the input distribution p. Assume the encoder outputs w with probability a(w). Then p =Pw∈C a(w)˜pw,

Ep(L(W )) =Pw∈C a(w)L(w). By convexity of relative entropy,
D(pkpIm) = D Xw∈C
≤ Xw∈C
≤ Xw∈C

a(w)˜pw k pIm!
a(w)D (˜pwkpIm)

a(w)L(w)

= Ep(L(W )).

Consider the (unbounded support) dyadic universal code. The implied distribution in this case is described by a pdf fIm(x) ∝
∞ (log kxk∞)−2n. Theorem 4 gives the lower bound on the expected codeword length for generating X ∼ f ,

kxk−n

D (fX k fIm) ≈ h(X) + n E [ℓδ(log kXk∞)] .

Comparing this to Theorem 1, we see that the upper bound is close to the lower bound when kXk∞ is the dominant term.
Note that Theorem 4 continues to hold even when Alice and Bob are allowed to share unlimited common randomness
(denoted by the random variable Q). Suppose the preﬁx-free codeword set when Q = q is Cq ⊆ {0, 1}∗. Upon receiving

11

w ∈ C, Bob generates X|{Q = q, W = w} ∼ ˜pq,w. The implied distribution is

pIm(dx) = ˆQ(cid:16) Xw′∈Cq

2−L(w′)(cid:17)−1

Xw∈Cq

2−L(w) ˜pq,w(dx)pQ(dq).

Theorem 4 still holds due to the convexity of relative entropy. Comparing this lower bound to the average length of the rejection
sampling scheme in [8] (which requires common randomness), which achieves

for some p∗. Hence, the lower bound is quite tight when unlimited common randomness is allowed.

Ep(L(W )) ≤ D(pkp∗) + 2 log(D(pkp∗) + 1) + O(1)

A. Proof of Lemma 1

The lemma is trivial when n = 1 since A can only be an interval. Hence we assume n ≥ 2.
From the deﬁnition of erosion entropy,

APPENDIX

where (i) is by substituting s = 2−t, and P\i(A) = {(x1, . . . , xi−1, xi+1, . . . , xn) : x ∈ A} ⊆ Rn−1. Let

Then we have ´ ∞

0 q(s)ds = 1, ´ ∞
E [kXk∞] =

−∞(cid:18)1 {t ≥ 0} −

Vn (A ⊖ 2−t[0, 1]n)

Vn(A)

(cid:19) dt

t · d(cid:18) Vn (A ⊖ 2−t[0, 1]n)

Vn(A)

(cid:19)

−∞
1

h⊖[0,1]n(A) = ˆ ∞
= ˆ ∞
Vn(A) ˆ ∞
Vn(A) ˆ ∞
Vn(A) ˆ ∞

(i)
=

=

=

1

1

0

0

0

(log s) · dVn (A ⊖ [0, s]n)
(log s) · d  n
Xi=1(cid:0)Vn(cid:0)A ⊖ {0}n−i × [0, s]i(cid:1) − Vn(cid:0)A ⊖ {0}n−i+1 × [0, s]i−1(cid:1)(cid:1)!
(− log s)  n
Vn−1P\i (A ⊖ [0, s]n)! ds
Xi=1
q(s) = Pn

i=1 Vn−1P\i (A ⊖ [0, s]n)

.

n

n

0

0

0

d

∂

Vn(A)
0 (− log s)q(s)ds = h⊖[0,1]n(A). Also
1
Vn(A) ˆA kxk∞ dx
ds ˆA⊖[0,s]n kxk∞ dx! ds
Vn(A) ˆ ∞
= −1
∂si  ˆA⊖[0,s1]×···[0,sn] kxk∞ dx!(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(s1,...,sn)=(s,...,s)
Vn(A) ˆ ∞
= −1
Xi=1
∂si  ˆA⊖[0,s1]×···[0,sn](cid:13)(cid:13)x[1:n]\i(cid:13)(cid:13)∞ dx!(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(s1,...,sn)=(s,...,s)
Vn(A) ˆ ∞
≥ −1
Xi=1
0   n
ˆP\i(A⊖[0,s]n)(cid:13)(cid:13)x[1:n]\i(cid:13)(cid:13)∞ dx[1:n]\i! ds
Vn(A) ˆ ∞
Xi=1
0   n
(A ⊖ [0, s]n)! ds
Vn(A) ˆ ∞
Xi=1
Vn−1P\i (A ⊖ [0, s]n)!n/(n−1)
n−1/(n−1)  n
8Vn(A) ˆ ∞
Xi=1
(A)ˆ ∞

Vn−1P1+1/(n−1)

n−1/(n−1)V1/(n−1)

qn/(n−1)(s)ds,

≥

≥

1
8

ds

ds

=

=

∂

1

1

(i)

1

\i

n

0

1
8

0

ds

where (i) is by (3) in the proof of Theorem (1). Let

0

˜q(s) =((cid:0)β(log e)n−1Γ(n)(cid:1)−1
0 ˜q(s)ds = 1, ´ ∞

(− log (s/β))n−1

if s ≤ β
if s > β,
0 (− log s)˜q(s)ds = h. Hence,

where β = en2−h, h = h⊖[0,1]n(A). Then ´ ∞
ˆ ∞

qn/(n−1)(s)ds

0

12

n/(n−1)

0

0

0

(i)

˜q(s)n/(n−1) (q(s)/˜q(s))n/(n−1) ds

˜q(s)n/(n−1)ds!−1

˜q(s)n/(n−1)ds!
 ˆ β
˜q(s)n/(n−1)ds!−1/(n−1) ˆ β

qn/(n−1)(s)ds ≥ ˆ β
= ˆ β
≥ ˆ β
˜q(s)1/(n−1)q(s)ds!n/(n−1)
= ˆ β
=(cid:16)n (βΓ(n))−1/(n−1)(cid:17)−1/(n−1) (log e)−1 (βΓ(n))−1/(n−1)ˆ β
≥(cid:16)n (βΓ(n))−1/(n−1)(cid:17)−1/(n−1)(cid:18)(log e)−1 (βΓ(n))−1/(n−1)ˆ ∞
=(cid:16)n (βΓ(n))−1/(n−1)(cid:17)−1/(n−1)(cid:16)(log e)−1 (βΓ(n))−1/(n−1) (h + log β)(cid:17)n/(n−1)

˜q(s)n/(n−1) (q(s)/˜q(s)) ds


ˆ β

0

0

0

0

= n (βΓ(n))−1/(n−1) ,

0

0

(− log s + log β) q(s)ds!n/(n−1)
(− log s + log β) q(s)ds(cid:19)n/(n−1)

where (i) is by weighted power mean inequality. As a result,

0

n

1
8
1
8

qn/(n−1)(s)ds

n−1/(n−1)V1/(n−1)

n(n−2)/(n−1)V1/(n−1)

E [kXk∞] ≥
≥

(A)ˆ ∞
(A)(cid:0)en2−hΓ(n)(cid:1)−1/(n−1)
(A)(cid:19)(cid:19) + log Γ(n) + n log e
h ≤ (n − 1)(cid:18)log E [kXk∞] − log(cid:18) 1
= (n − 1) log E [kXk∞] − log Vn(A) + 3(n − 1) − (n − 2) log n + log Γ(n) + n log e
≤ (n − 1) log E [kXk∞] − log Vn(A) + 3(n − 1) − (n − 2) log n + (n log n − (n − 1) log e) + n log e
= (n − 1) log E [kXk∞] − log Vn(A) + 3(n − 1) + 2 log n + log e
≤ (n − 1) log E [kXk∞] − log Vn(A) + 4n.

n(n−2)/(n−1)V1/(n−1)

8

n

n

,

B. Proof of the claim on measure zero boundary in Theorem 2

We will prove that if f is a pdf which is continuous almost everywhere, then L+

z (f ) is a Borel set and thus measurable). Then we show that there exists z1 6= z2 ∈ G such that Vn(∂L+

z (f ) has a boundary of measure zero for
z (f )) > 0 for all z ∈ G (note that
almost all z. Assume the contrary that there exist an uncountable G ⊆ [0,∞) such that Vn(∂L+
∂L+
z2(f )) > 0
(which follows from σ-ﬁniteness, though we include a proof here for completeness). To show the claim, note that for any
z (f )∩ ([0, 1]n + v) has nonzero measure. Hence there exists a
z ∈ G, there exists a hypercube [0, 1]n + v, v ∈ Zn such that ∂L+
hypercube [0, 1]n + v such that ∂L+
z (f )∩ ([0, 1]n + v) has nonzero measure for an uncountable set of z’s. Since an uncountable
collection of positive numbers must contain a ﬁnite subcollection with sum greater than 1, we can select z1, . . . , zm such that

z1(f )∩∂L+

Pi Vn(∂L+
Now we have z1 < z2 ∈ G such that Vn(∂L+
f is continuous, since x ∈ ∂L+
x ∈ ∂L+
contradiction. Therefore f is discontinuous in ∂L+
everywhere. Therefore L+

zi(f ) ∩ ([0, 1]n + v)) > 1, and hence there exists two of these sets with an intersection of nonzero measure.

z2(f )) > 0. Assume there exists x in the intersection at which
z2(f ), there exist sequence yi → x with f (yi) ≥ z2, and hence f (x) ≥ z2. Also since
z1(f ) ⊆ cl{y : f (y) < z1}, there exist sequence ˜yi → x with f (˜yi) < z1, and hence f (x) ≤ z1, leading to a
z2(f ), contradicting the assumption that f is continuous almost

z (f ) has a boundary of measure zero for almost all z.

z1(f ) ∩ ∂L+

z1(f ) ∩ ∂L+

REFERENCES

[1] V. I. Levenshtein, “On the redundancy and delay of decodable coding of natural numbers,” Probl. Cybern., vol. 20, pp. 173–179, 1968.
[2] P. Elias, “Universal codeword sets and representations of the integers,” IEEE Trans. Info. Theory, vol. 21, no. 2, pp. 194–203, Mar 1975.
[3] C. H. Bennett, P. W. Shor, J. Smolin, and A. V. Thapliyal, “Entanglement-assisted capacity of a quantum channel and the reverse Shannon theorem,”

IEEE Trans. Info. Theory, vol. 48, no. 10, pp. 2637–2655, 2002.

[4] A. Winter, “Compression of sources of probability distributions and density operators,” arXiv preprint quant-ph/0208131, 2002.
[5] P. Cuff, “Distributed channel synthesis,” IEEE Trans. Info. Theory, vol. 59, no. 11, pp. 7071–7096, 2013.
[6] M. Steiner, “Towards quantifying non-local information transfer: ﬁnite-bit non-locality,” Physics Letters A, vol. 270, no. 5, pp. 239–244, 2000.
[7] S. Massar, D. Bacon, N. J. Cerf, and R. Cleve, “Classical simulation of quantum entanglement without local hidden variables,” Physical Review A,

vol. 63, no. 5, p. 052305, 2001.

[8] P. Harsha, R. Jain, D. McAllester, and J. Radhakrishnan, “The communication complexity of correlation,” IEEE Trans. Info. Theory, vol. 56, no. 1, pp.

438–449, Jan 2010.

13

[9] C. T. Li

and A. El Gamal,

http://arxiv.org/abs/1601.05875

“Distributed simulation of

continuous

random variables,”

arXiv preprint, 2016.

[Online]. Available:

[10] T. Maudlin, “Bell’s inequality, information transmission, and prism models,” in PSA: Proceedings of the Biennial Meeting of the Philosophy of Science

Association.

JSTOR, 1992, pp. 404–417.

[11] G. Brassard, R. Cleve, and A. Tapp, “Cost of exactly simulating quantum entanglement with classical communication,” Physical Review Letters, vol. 83,

no. 9, p. 1874, 1999.

[12] J. S. Bell et al., “On the Einstein-Podolsky-Rosen paradox,” Physics, vol. 1, no. 3, pp. 195–200, 1964.
[13] M. Feldmann, “New loophole for the Einstein-Podolsky-Rosen paradox,” Foundations of Physics Letters, vol. 8, no. 1, pp. 41–53, 1995.
[14] J. Nash, “Non-cooperative games,” Annals of mathematics, pp. 286–295, 1951.

