Clustering Financial Time Series: How Long is Enough?

Gautier Marti

Hellebore Capital Ltd

S´ebastien Andler

ENS de Lyon

Frank Nielsen

Ecole Polytechnique

Philippe Donnat

Hellebore Capital Ltd

6
1
0
2

 
r
a

 

M
3
1

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
7
1
0
4
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

Researchers have used from 30 days to several
years of daily returns as source data for cluster-
ing ﬁnancial time series based on their correlations.
This paper sets up a statistical framework to study
the validity of such practices. We ﬁrst show that
clustering correlated random variables from their
observed values is statistically consistent. Then,
we also give a ﬁrst empirical answer to the much
debated question: How long should the time series
be? If too short, the clusters found can be spurious;
if too long, dynamics can be smoothed out.

1 Introduction
Clustering can be informally described as the task of group-
ing objects in subsets (also named clusters) in such a way
that objects in the same cluster are more similar to each other
than those in different clusters. Since the clustering task is no-
tably hard to formalize [Kleinberg, 2003], designing a clus-
tering algorithm that solves it perfectly in any cases seems
farfetched. However, under strong mathematical assumptions
made on the data, desirable properties such as statistical con-
sistency, i.e. more data means more accuracy and in the limit
a perfect solution, have been shown: Starting from Harti-
gan’s proof of Single Linkage [Hartigan, 1981] and Pollard’s
proof of k-means consistency [Pollard and others, 1981] to
recent work such as the consistency of spectral clustering
[Von Luxburg et al., 2008], or modiﬁed k-means [Terada,
2013; Terada, 2014]. These research papers assume that N
data points are independently sampled from an underlying
probability distribution in dimension T ﬁxed. Clusters can be
seen as regions of high density. They show that in the large
sample limit, N → ∞, the clustering sequence constructed
by the given algorithm converges to a clustering of the whole
underlying space. When we consider the clustering of time
series, another asymptotics matter: N ﬁxed and T → ∞.
Clusters gather objects that behave similarly through time.
To the best of our knowledge, much fewer researchers have
dealt with this asymptotics: [Borysov et al., 2014] show the
consistency of three hierarchical clustering algorithms when
dimension T is growing to correctly gather N = n+m obser-
vations from a mixture of two T dimensional Gaussian dis-
tributions N (µ1, σ2
[Ryabko, 2010;

1IT ) and N (µ2, σ2

2IT ).

Khaleghi et al., 2012] prove the consistency of k-means for
clustering processes according to their distribution.
In this
work, motivated by the clustering of ﬁnancial time series,
we will instead consider the consistency of clustering N ran-
dom variables from their T observations according to their
observed correlations.

For ﬁnancial applications, clustering is usually used as a
building block before further processing such as portfolio se-
lection [Tola et al., 2008]. Before becoming a mainstream
methodology among practitioners, one has to provide theo-
retical guarantees that the approach is sound. In this work,
we ﬁrst show that the clustering methodology is theoretically
valid, but when working with ﬁnite length time series extra
care should be taken: Convergence rates depend on many
factors (underlying correlation structure, separation between
clusters, underlying distribution of returns) and implemen-
tation choice (correlation coefﬁcient, clustering algorithm).
Since ﬁnancial time series are thought to be approximately
stationary for short periods only, a clustering methodology
that requires a large sample to recover the underlying clusters
is unlikely to be useful in practice and can be misleading. In
section 5, we illustrate on simulated time series the empirical
convergence rates achieved by several clustering approaches.

Notations

i

• X1, . . . , XN univariate random variables
• X t
i is the tth observation of variable Xi
• X (t)
is the tth sorted observation of Xi
• FX is the cumulative distribution function of X
• ρij = ρ(Xi, Xj) correlation between Xi, Xj
• dij = d(Xi, Xj) distance between Xi, Xj
• Dij = D(Ci, Cj) distance between clusters Ci, Cj
} is a partition of X1, . . . , XN
• Pk = {C (k)
• C (k)(Xi) denotes the cluster of Xi in partition Pk
• (cid:107)Σ(cid:107)∞ = maxij Σij
• X = Op(k) means X/k is stochastically bounded, i.e.

1 , . . . , C (k)
lk

∀ε > 0,∃M > 0, P (|X/k| > M ) < ε.

2 The Hierarchical Correlation Block Model
2.1 Stylized facts about ﬁnancial time series
Since the seminal work in [Mantegna, 1999], it has been ver-
iﬁed several times for different markets (e.g. stocks, forex,

credit default swaps [Marti et al., 2015]) that price time se-
ries of traded assets have a hierarchical correlation structure.
Another well-known stylized fact is the non-Gaussianity of
daily asset returns [Cont, 2001]. These empirical properties
motivate both the use of alternative correlation coefﬁcients
described in section 2.2 and the deﬁnition of the Hierarchical
Correlation Block Model (HCBM) presented in section 2.3.
2.2 Dependence and correlation coefﬁcients
The most common correlation coefﬁcient is the Pearson cor-
relation coefﬁcient deﬁned by

ρ(X, Y ) =

which can be estimated by

ˆρ(X, Y ) =

E[XY ] − E[X]E[Y ]

(cid:112)E[X 2] − E[X]2(cid:112)E[Y 2] − E[Y ]2
(cid:113)(cid:80)T
(cid:0)Y t − Y(cid:1)2
(cid:80)T

(cid:80)T
(cid:0)X t − X(cid:1)2(cid:113)(cid:80)T
t=1(X t − X)(Y t − Y )

t=1

t=1

(1)

(2)

(cid:90) 1

(cid:90) 1

where X = 1
t=1 X t is the empirical mean of X. This
T
coefﬁcient suffers from several drawbacks: it only measures
linear relationship between two variables; it is not robust to
noise and may be undeﬁned if the distribution of one of these
variables have inﬁnite second moment. More robust correla-
tion coefﬁcients are copula-based dependence measures such
as Spearman’s rho

ρS(X, Y ) = 12

C(u, v)dudv − 3
= 12 E [FX (X), FY (Y )] − 3
= ρ (FX (X), FY (Y ))

0

0

and its statistical estimate

ˆρS(X, Y ) = 1 −

6

T (T 2 − 1)

(cid:16)

X (t) − Y (t)(cid:17)2

T(cid:88)

t=1

(3)

(4)
(5)

.

(6)

These correlation coefﬁcients are robust to noise (since rank
statistics normalize outliers) and invariant to monotonous
transformations of the random variables (since copula-based
measures beneﬁt from the probability integral
transform
FX (X) ∼ U[0, 1]).
2.3 The HCBM model
We assume that
the N univariate random variables
X1, . . . , XN follow a Hierarchical Correlation Block Model
(HCBM). This model consists in correlation matrices having
a hierarchical block structure [Balakrishnan et al., 2011], [Kr-
ishnamurthy et al., 2012]. Each block corresponds to a cor-
relation cluster that we want to recover with a clustering al-
gorithm. In Fig. 1, we display a correlation matrix from the
HCBM. Notice that in practice one does not observe the hier-
archical block diagonal structure displayed in the left picture,
but a correlation matrix similar to the one displayed in the
right picture which is identical to the left one up to a permuta-
tion of the data. The HCBM deﬁnes a set of nested partitions
P = {P0 ⊇ P1 ⊇ . . . ⊇ Ph} for some h ∈ [1, N ], where P0
},
is the trivial partition, the partitions Pk = {C (k)
1 , . . . , C (k)
lk

and (cid:70)lk

k

i=1 C (k)

i = {X1, . . . , XN}. For all 1 ≤ k ≤ h,
and ρk such that for all 1 ≤ i, j ≤ N, we
we deﬁne ρ
≤ ρij ≤ ρk when C (k)(Xi) = C (k)(Xj) and
have ρ
k
C (k+1)(Xi) (cid:54)= C (k+1)(Xj), i.e. ρ
and ρk are the minimum
and maximum correlation respectively within all the clusters
C (k)
in the partition Pk at depth k. In order to have a proper
nested correlation hierarchy, we must have ρk < ρ
for all
k. Depending on the context, it can be a Spearman or Pearson
correlation matrix.

k+1

k

i

Figure 1: (left) hierarchical correlation block model; (right)
observed correlation matrix (following the HCBM) identical
to the left one up to a permutation of the data

n1, . . . , nK such that(cid:80)K

Without loss of generality and for ease of demonstration
we will consider the one-level HCBM with K blocks of size
i=1 ni = N. We explain later how to
extend the results to the general HCBM. We also consider the
associated distance matrix d, where dij = 1−ρij
. In practice,
clustering methods are applied on statistical estimates of the
distance matrix d, i.e. on ˆdij = dij + ij, where ij are noises
resulting from the statistical estimation of correlations.

2

3 Clustering methods
3.1 Algorithms of interest
Many paradigms exist in the literature for clustering data. We
consider in this work only hard (in opposition to soft) cluster-
ing methods, i.e. algorithms producing partitions of the data
(in opposition to methods assigning several clusters to a given
data point). Within the hard clustering family, we can classify
for instance these algorithms in hierarchical clustering meth-
ods (yielding nested partitions of the data) and ﬂat clustering
methods (yielding a single partition) such as k-means.

We will consider the inﬁnite Lance-Williams family which
further subdivides the hierarchical clustering since many of
the popular algorithms such as Single Linkage, Complete
Linkage, Average Linkage (UPGMA), McQuitty’s Linkage
(WPGMA), Median Linkage (WPGMC), Centroid Linkage
(UPGMC), and Ward’s method are members of this family
(cf. Table 1 [Murtagh and Contreras, 2012]). It will allow us a
more concise and uniﬁed treatment of the consistency proofs
for these algorithms. Interesting and recently designed hierar-
chical agglomerative clustering algorithms such as Hausdorff
Linkage [Basalto et al., 2007] and Minimax Linkage [Ao et
al., 2005] do not belong to this family [Bien and Tibshirani,
2011], but their linkage functions share a convenient property
for cluster separability.

Table 1: Many well-known hierarchical agglomerative clus-
tering algorithms are members of the Lance-Williams family,
i.e. the distance between clusters can be written:
D(Ci ∪ Cj, Ck) = αiDik + αjDjk + βDij + γ|Dik − Djk|

αi
1/2
1/2
|Ci|

1/2
1/2
|Ci|

|Ci|+|Cj|

Single
Complete
Average
McQuitty
Median
Centroid
Ward

β
0
0
0
0
-1/4

γ
-1/2
1/2
0
0
0
0
0

|Ci|+|Cj|
|Ci|+|Ck|
|Ci|+|Cj|+|Ck| −

− |Ci||Cj|
(|Ci|+|Cj|)2
|Ck|
|Ci|+|Cj|+|Ck|

3.2 Separability conditions for clustering
In our context the distances between the points we want to
cluster are random and deﬁned by the estimated correlations.
However by deﬁnition of the HCBM, each point Xi belongs
to exactly one cluster C (k)(Xi) at a given depth k, and we
want to know under which condition on the distance matrix
we will ﬁnd the correct clusters deﬁned by Pk. We call these
conditions the separability conditions. A separability condi-
tion for the points X1, . . . , XN is a condition on the distance
matrix of these points such that if we apply a clustering pro-
cedure whose input is the distance matrix, then the algorithm
}, . For
yields the correct clustering Pk = {C (k)
example, for {X1, X2, X3} if we have C(X1) = C(X2) (cid:54)=
C(X3) in the one-level two-block HCBM, then a separability
condition is d1,2 < d1,3 and d1,2 < d2,3.

1 , . . . , C (k)
lk

Separability conditions are deterministic and depend on the
algorithm used for clustering. They are generic in the sense
that for any sets of points that satisfy the condition the algo-
rithm will separate them in the correct clusters. In the Lance-
Williams algorithm framework [Chen and Van Ness, 1996],
they are closely related to “space conserving” properties of
the algorithm and in particular on the way the distances be-
tween clusters change during the clustering process.
Space-conserving algorithms
In [Chen and Van Ness, 1996], the authors deﬁne what they
call a semi-space-conserving algorithm.
Deﬁnition 1 (Semi-space-conserving algorithms). An algo-
rithm is semi-space-conserving if for all clusters Ci, Cj, and
Ck,

D(Ci ∪ Cj, Ck) ∈ [min(Dik, Djk), max(Dik, Djk)]
Among the Lance-Williams algorithms we study here, Sin-
gle, Complete, Average and McQuitty algorithms are semi-
space-conserving. Although Chen and Van Ness only con-
sidered Lance-Williams algorithms the deﬁnition of a space
conserving algorithm is useful for any agglomerative hier-
archical algorithm. An alternative formulation of the semi-
space-conserving property is:
Deﬁnition 2 (Space-conserving algorithms). A linkage ag-
glomerative hierarchical algorithm is space-conserving if
Dij ∈

d(x, y), max

d(x, y)

(cid:20)

(cid:21)

.

min

x∈Ci,y∈Cj

x∈Ci,y∈Cj

Such an algorithm does not “distort” the space when points
are clustered which makes the sufﬁcient separability condi-
tion easier to get. For these algorithms the separability con-
dition does not depend on the size of the clusters.

The following two propositions are easy to verify.

Proposition 1. The semi-space-conserving Lance-Williams
algorithms are space-conserving.
Proposition 2. Minimax linkage and Hausdorff linkage are
space-conserving.

For space-conserving algorithms we can now state a sufﬁ-

cient separability condition on the distance matrix.
Proposition 3. The following condition is a separability con-
dition for space-conserving algorithms:

max

1≤i,j≤N
C(i)=C(j)

d(Xi, Xj) < min

1≤i,j≤N
C(i)(cid:54)=C(j)

d(Xi, Xj)

(S1)

The maximum distance is taken over any two points in a same
cluster (intra) and the minimum over any two points in differ-
ent clusters (inter).
ij} of distances between clusters
Proof. Consider the set {ds
ij} is the
after s steps of the clustering algorithm (therefore {d0
inter}
initial set of distances between the points). Denote {ds
intra}) the sets of distances between subclusters be-
(resp. {ds
the same cluster) at step
longing to different clusters (resp.
s. If the separability condition is satisﬁed then we have the
following inequalities:

min d0

intra ≤ max d0

intra < min d0

inter ≤ max d0

inter

(S2)
Then the separability condition implies that the separability
condition S2 is veriﬁed for all step s because after each step
the updated intra distances are in the convex hull of the intra
distances of the previous step and the same is true for the inter
distances. Moreover since S2 is veriﬁed after each step, the
algorithm never links points from different clusters and the
(cid:3)
proposition entails.
Ward algorithm
The Ward algorithm is a space-dilating Lance-Williams algo-
rithm: D(Ci ∪ Cj, Ck) > max(Dik, Djk). This is a more
complicated situation because the structure

min dinter < max dinter < min dintra < max dintra

is not necessarily preserved under the condition max d0
inter <
intra. Points which are not clustered move away from
min d0
the clustered points. Outliers, which will only be clustered at
the very end, will end up close to each other and far from the
clustered points. This can lead to wrong clusters. Therefore a
generic separability condition for Ward needs to be stronger
and account for the distortion of the space. Since the distor-
tion depends on the number of steps the algorithm needs, the
separability condition depends on the size of the clusters.
Proposition 4 (Separability condition for Ward). The sepa-
rability condition for Ward reads:

n[max d0

intra − min d0

intra] < [min d0

inter − min d0

intra]

where n = maxi ni is the size of the largest cluster.

Proof. Let A and B be two subsets of the N points of size a
and b respectively. Then

 2

ab

(cid:88)

i∈A
j∈B

(cid:88)

i∈A
i(cid:48)∈A

(cid:88)

j∈B
j(cid:48)∈B

D(A, B) =

ab

a + b

dij − 1
a2

dii(cid:48) − 1
b2

djj(cid:48)

is a linkage function for the Ward algorithm. To ensure that
the Ward algorithm will never merge the wrong subsets it is
sufﬁcient that for any sets A and B in a same cluster, and A(cid:48),
B(cid:48) in different clusters, we have:

D(A, B) < D(A(cid:48), B(cid:48)).

Since(cid:26)D(A, B) ≤ n(max d0

D(A(cid:48), B(cid:48)) ≥ (min d0
we obtain the condition:
intra − min d0

n(max d0

intra − min d0
inter − max d0

intra) + min d0
intra) + max d0

intra − 1
intra − 1

intra) < min d0

inter − min d0

intra.

(cid:3)

k-means
The k-means algorithm is not a linkage algorithm. For the
k-means algorithm we need a separability condition that en-
sures that the initialization will be good enough for the al-
gorithm to ﬁnd the partition. In [Ryabko, 2010] (Theorem
1), the author proves the consistency of the one-step farthest-
point initialization k-means [Katsavounidis et al., 1994] with
a distributional distance for clustering processes. The separa-
bility condition S1 of Proposition 3 is sufﬁcient for k-means.

4 Consistency of well-known clustering

algorithms

In the previous section we have determined conﬁgurations of
points such that the clustering algorithm will ﬁnd the right
partition. The proof of the consistency now relies on showing
that these conﬁgurations are likely. In fact the probability that
our points fall in these conﬁgurations goes to 1 as T → ∞.
The precise deﬁnition of what we mean by consistency of

1, . . . , X t

an algorithm is the following:
Deﬁnition 3 (Consistency of a clustering algorithm). Let
N ), t = 1, . . . , T , be N univariate random vari-
(X t
ables observed T times. A clustering algorithm A is consis-
tent with respect to the Hierarchical Correlation Block Model
(HCBM) deﬁning a set of nested partitions P if the proba-
bility that the algorithm A recovers all the partitions in P
converges to 1 when T → ∞.

As we have seen in the previous section the correct cluster-
ing can be ensured if the estimated correlation matrix veriﬁes
some separability condition. This condition can be guaran-
teed by requiring the error on each entry of the matrix ˆRT
to be smaller than the contrast, i.e. ρ
, on the theoreti-
cal matrix R. There are classical results on the concentration
properties of estimated correlation matrices such as:

−ρ0
2

1



Theorem 1 (Concentration properties of the estimated corre-
lation matrices [Liu et al., 2012a]). If Σ and ˆΣ are the pop-
ulation and empirical Spearman correlation matrix respec-
tively, then with probability at least 1− 1
log T +2,
we have

T 2 , for N ≥ 24

(cid:114)

(cid:107) ˆΣ − Σ(cid:107)∞ ≤ 24

log N

T

The concentration bounds entails that if T (cid:29) log(N ) then
the clustering will ﬁnd the correct partition because the clus-
ters will be sufﬁciently separated with high probability. In
ﬁnancial applications of clustering, we need the error on the
estimated correlation matrix to be small enough for relatively
short time-windows. However there is a dimensional depen-
dency of these bounds [Tropp, 2015] that make them unin-
formative for realistic values of N and T in ﬁnancial appli-
cations, but there is hope to improve the bounds using the
special structure of HCBM correlation matrices.
4.1 From the one-level to the general HCBM
To go from the one-level HCBM to the general case we need
to get a separability condition on the nested partition model.
For both space-conserving algorithms and the Ward algo-
rithm, this is done by requiring the corresponding separability
condition for each level of the hierarchy.
For all 1 ≤ k ≤ h, we deﬁne dk and dk such that for all
1 ≤ i, j ≤ N, we have dk ≤ dij ≤ dk when C (k)(Xi) =
C (k)(Xj) and C (k+1)(Xi) (cid:54)= C (k+1)(Xj). Notice that dk =
(1 − ρk)/2 and dk = (1 − ρ
Proposition 5. [Separability condition for space-conserving
algorithms in the case of nested partitions] The separability
condition reads:

)/2.

k

dh < dh−1 < . . . < dk+1 < dk < . . . < d1.

This condition can be guaranteed by requiring the error on
each entry of the matrix ˆΣ to be smaller than the lowest con-
trast. Therefore the maximum error we can have for space-
conserving algorithms on the correlation matrix is

(cid:107)Σ − ˆΣ(cid:107)∞ < min

k

ρ

− ρk
k+1
2

.

Proposition 6. [Separability condition for the Ward algo-
rithm in the case of nested partitions] Let nk be the size of
the largest cluster at the level k of the hierarchy.

The separability condition reads:

∀k ∈ {1, . . . , h},

nk(dk − dh) < dk−1 − dh

Therefore the maximum error we can have for space-

conserving algorithms on the correlation matrix is
ρh − ρk−1 − nk(ρh − ρ

(cid:107)Σ − ˆΣ(cid:107)∞ < min

)

k

,

k

1 + 2nk

where nk is the size of the largest cluster at the level k of the
hierarchy.

We ﬁnally obtain consistency for the presented algorithms
with respect to the HCBM from the previous concentration
results.

5 Empirical rates of convergence
We have shown in the previous sections that clustering cor-
related random variables is consistent under the hierarchical
correlation block model. This model is supported by many
empirical studies [Mantegna, 1999] where the authors scruti-
nize time series of returns for several asset classes. However,
it was also noticed that the correlation structure is not ﬁxed
and tends to evolve through time. This is why, besides being
consistent, the convergence of the methodology needs to be
fast enough for the underlying clustering to be accurate. For
now, theoretical bounds such as the ones obtained in Theo-
rem 1 are uninformative for realistic values of N and T . For
example, for N = 265 and T = 2500 (roughly 10 years
of historical daily returns) with a separation between clusters
of d = 0.2, we are conﬁdent with probability greater than
1− 2N 2e−T d2/24 ≈ −2176 that the clustering algorithm has
recovered the correct clusters. These bounds will eventually
converge to 0 with rate OP (
T ). In addition, the
convergence rates also depend on many factors, e.g. the num-
ber of clusters, their relative sizes, their separations, whose
inﬂuence is very speciﬁc to a given clustering algorithm, and
thus difﬁcult to consider in a theoretical analysis.

√
log N /

√

To get an idea of the minimal amount of data one should
use in applications to be conﬁdent with the clustering results,
we suggest to design realistic simulations of ﬁnancial time
series and determine the sample critical size from which the
clustering approach “always” recovers the underlying model.
We illustrate such an empirical study in the following section.

5.1 Financial time series models
For the simulations, we will consider two models:

• The standard but debated model of quantitative ﬁnance,
the Gaussian random walk model whose increments are
realizations from a N-variate Gaussian: X ∼ N (0, Σ).
The Gaussian model does not generate heavy-tailed behavior
(strong unexpected variations in the price of an asset) which
can be found in many asset returns [Cont, 2001] nor does it
generate tail-dependence (strong variations tend to occur at
the same time for several assets). This oversimpliﬁed model
provides an empirical convergence rate for clustering that is
unlikely to be exceeded on real data.

• The increments are realizations from a N-variate Stu-
dent’s t-distribution, with degree of freedom ν = 3:
X ∼ tν(0, ν−2

ν Σ).

The N-variate Student’s t-distribution (ν = 3) captures both
the heavy-tailed behavior (since marginals are univariate Stu-
dent’s t-distribution with the same parameter ν = 3) and
the tail-dependence. It has been shown that this distribution
yields a much better ﬁt to real returns than the Gaussian dis-
tribution [Hu and Kercheval, 2010].

The Gaussian and t-distribution are parameterized by a co-
variance matrix Σ. We deﬁne Σ such that the underlying cor-
relation matrix has the structure depicted in Figure 2. This
correlation structure is inspired by the real correlations be-
tween credit default swap assets in the European “investment
grade”, European “high-yield” and Japanese markets. More

Figure 2: Illustration of the correlation structure used for sim-
ulations: European assets (numbered 0, . . . , 214) are subdi-
vided into 2 clusters which are themselves subdivided into
7 clusters each; Japanese assets (numbered 215 . . . , 264) are
weakly correlated to the European markets: ρ = 0.15 with
“investment grade” assets, ρ = 0.00 with “high-yield” assets

precisely, this correlation matrix allows us to simulate the re-
turns time series for N = 265 assets divided into

• a “European investment grade” cluster composed of 115

assets, subdivided into

– 7 industry-speciﬁc clusters of sizes 10, 20, 20, 5,
30, 15, 15; the pairwise correlation inside these 7
clusters is 0.7;

• a “European high-yield” cluster composed of 100 assets,

subdivided into

– 7 industry-speciﬁc clusters of sizes 10, 20, 25, 15,
5, 10, 15; the pairwise correlation inside these 7
clusters is 0.7;

• a “Japanese” cluster composed of 50 assets whose pair-

wise correlation is 0.6.

We can then sample time series from these two models.

5.2 Experiment: Recovering the initial clusters
For each model, for every T ranging from 10 to 500, we sam-
ple L = 100 datasets of N = 265 time series with length
T from the model. We count how many times the clustering
methodology (here, the choice of an algorithm and a corre-
lation coefﬁcient) is able to recover the underlying clusters
deﬁned by the correlation matrix. In Figure 3, we display the
results obtained using Single Linkage (motivated in Mantegna
et al.’s research [Mantegna and Stanley, 1999] by the ultra-
metric space hypothesis and the related subdominant ultra-
metric given by the minimum spanning tree), Average Link-
age (which is used to palliate against the unbalanced effect of
Single Linkage, yet unlike Single Linkage, it is sensitive to
monotone transformations of the distances dij) and the Ward
method leveraging either the Pearson correlation coefﬁcient
or the Spearman one.

Figure 3: Single Linkage (left), Average Linkage (mid), Ward method (right) are used for clustering the simulated time series;
Red lines represent the ratio of correct clustering over the number of trials when using Pearson coefﬁcient, green lines for the
Spearman one; Solid lines are used when the underlying model is Gaussian, dashed lines for the t-distribution

5.3 Conclusions from the empirical study
As expected, the Pearson coefﬁcient yields the best results
when the underlying distribution is Gaussian and the worst
when the underlying distribution is heavy-tailed. For such
elliptical distributions, rank-based correlation estimators are
more relevant [Liu et al., 2012b; Han and Liu, 2013]. Con-
cerning clustering algorithm convergence rates, we ﬁnd that
Average Linkage outperforms Single Linkage for T (cid:28) N
and T (cid:39) N. One can also notice that both Single Linkage
and Average Linkage have not yet converged after 500 real-
izations (roughly 2 years of daily returns) whereas the Ward
method, which is not mainstream in the econophysics liter-
ature, has converged after 250 realizations (about a year of
daily returns). Its variance is also much smaller. Based on
this empirical study, a practitioner working with N = 265
assets whose underlying correlation matrix may be similar to
the one depicted in Figure 2 should use the Ward + Spearman
methodology on a sliding window of length T = 250.

6 Discussion
In this contribution, we only show consistency with respect
to a model motivated by empirical evidence. All models are
wrong and this one is no exception to the rule: random walk
hypothesis, real correlation matrices are not that “blocky”.
We identiﬁed several theoretical directions for the future:
• The theoretical concentration bounds are not sharp
enough for usual values of N, T . Since the intrinsic
dimension of the correlation matrices in the HCBM is
low, there might be some possible improvements [Tropp,
2015].
• “Space-conserving”, “space-dilating” is a coarse classi-
ﬁcation that does not allow to distinguish between sev-
eral algorithms with different behaviors. Though Single
Linkage (which is nearly “space-contracting”) and Aver-
age Linkage have different convergence rates as shown
by the empirical study, they share the same theoretical
bounds.

And also directions for experimental studies:
• It would be interesting to study spectral clustering tech-
niques which are less greedy than the hierarchical clus-

Figure 4: Heatmap encoding the ratio of correct clustering
over the number of trials for the Ward + Spearman method-
ology as a function of ρ and T ; underlying model is a Gaus-
sian distribution parameterized by a 2-block-uniform-ρ cor-
relation matrix; red color represents a perfect and systematic
recovering of the underlying two clusters, deep blue encodes
0 correct clustering; notice the clear-cut isoquants

tering algorithms. In [Tumminello et al., 2007], authors
show that they are less stable with respect to statistical
uncertainty than hierarchical clustering. Less stability
may imply a slower convergence rate.
• We notice that there are isoquants of clustering accuracy
for many sets of parameters, e.g. (N, T ), (ρ, T ). Such
isoquants are displayed in Figure 4. Further work may
aim at characterizing these curves. We can also observe
in Figure 4 that for ρ ≤ 0.08, the critical value for T ex-
plodes. It would be interesting to determine this asymp-
totics as ρ tends to 0.

Finally, we have provided a guideline to help the prac-
titioner set the critical window-size T for a given cluster-
ing methodology. One can also investigate which consistent
methodology provides the correct clustering the fastest. How-
ever, much work remains to understand the convergence be-
haviors of clustering algorithms on ﬁnancial time series.

1002003004005000.00.20.40.60.81.0Empirical rates of convergence for Single LinkageP - GP - StS - GS - St1002003004005000.00.20.40.60.81.0Empirical rates of convergence for Average LinkageP - GP - StS - GS - St1002003004005000.00.20.40.60.81.0Empirical rates of convergence for WardP - GP - StS - GS - StReferences
[Ao et al., 2005] Sio Iong Ao, Kevin Yip, Michael Ng,
David Cheung, Pui-Yee Fong, Ian Melhado, and Pak C
Sham. Clustag: hierarchical clustering and graph methods
for selecting tag snps. Bioinformatics, 21(8):1735–1736,
2005.

[Balakrishnan et al., 2011] Sivaraman Balakrishnan, Min
Xu, Akshay Krishnamurthy, and Aarti Singh. Noise
thresholds for spectral clustering. In Advances in Neural
Information Processing Systems, pages 954–962, 2011.

[Basalto et al., 2007] Nicolas Basalto, Roberto Bellotti,
Francesco De Carlo, Paolo Facchi, Ester Pantaleo, and
Saverio Pascazio. Hausdorff clustering of ﬁnancial time
series. Physica A: Statistical Mechanics and its Applica-
tions, 379(2):635–644, 2007.

[Bien and Tibshirani, 2011] Jacob Bien and Robert Tibshi-
rani. Hierarchical clustering with prototypes via minimax
linkage. Journal of the American Statistical Association,
106(495):1075–1084, 2011.

[Borysov et al., 2014] Petro Borysov,

and
JS Marron. Asymptotics of hierarchical clustering for
Journal of Multivariate Analysis,
growing dimension.
124:465–479, 2014.

Jan Hannig,

[Chen and Van Ness, 1996] Zhenmin Chen and John W
Van Ness. Space-conserving agglomerative algorithms.
Journal of classiﬁcation, 13(1):157–168, 1996.

[Cont, 2001] Rama Cont. Empirical properties of asset re-

turns: stylized facts and statistical issues. 2001.

[Han and Liu, 2013] Fang Han and Han Liu. Optimal rates
of convergence for latent generalized correlation matrix
estimation in transelliptical distribution. arXiv preprint
arXiv:1305.6916, 2013.

[Hartigan, 1981] John A Hartigan. Consistency of single
linkage for high-density clusters. Journal of the American
Statistical Association, 76(374):388–394, 1981.

[Hu and Kercheval, 2010] Wenbo Hu and Alec N Kercheval.
Portfolio optimization for student t and skewed t returns.
Quantitative Finance, 10(1):91–105, 2010.

[Katsavounidis et al., 1994] Ioannis Katsavounidis, C-C
Jay Kuo, and Zhen Zhang. A new initialization technique
for generalized Lloyd iteration. Signal Processing Letters,
IEEE, 1(10):144–146, 1994.

[Khaleghi et al., 2012] Azadeh Khaleghi, Daniil Ryabko,
J´er´emie Mary, and Philippe Preux. Online clustering of
processes. In International Conference on Artiﬁcial Intel-
ligence and Statistics, pages 601–609, 2012.

[Kleinberg, 2003] Jon Kleinberg. An impossibility theorem
for clustering. Advances in neural information processing
systems, pages 463–470, 2003.

[Krishnamurthy et al., 2012] Akshay

Krishnamurthy,
Sivaraman Balakrishnan, Min Xu, and Aarti Singh.
Efﬁcient active algorithms for hierarchical clustering.
International Conference on Machine Learning, 2012.

[Liu et al., 2012a] Han Liu, Fang Han, Ming Yuan, John
Lafferty, Larry Wasserman, et al. High-dimensional semi-
parametric gaussian copula graphical models. The Annals
of Statistics, 40(4):2293–2326, 2012.

[Liu et al., 2012b] Han Liu, Fang Han, and Cun-hui Zhang.
In Advances in Neural

Transelliptical graphical models.
Information Processing Systems, pages 809–817, 2012.

[Mantegna and Stanley, 1999] Rosario N Mantegna and
H Eugene Stanley. Introduction to econophysics: corre-
lations and complexity in ﬁnance. Cambridge university
press, 1999.

[Mantegna, 1999] Rosario N Mantegna. Hierarchical struc-
ture in ﬁnancial markets. The European Physical Journal
B-Condensed Matter and Complex Systems, 11(1):193–
197, 1999.

[Marti et al., 2015] Gautier Marti, Philippe Very, Philippe
Donnat, and Frank Nielsen. A proposal of a methodologi-
cal framework with experimental guidelines to investigate
clustering stability on ﬁnancial time series. In 14th IEEE
International Conference on Machine Learning and Appli-
cations, ICMLA 2015, Miami, FL, USA, December 9-11,
2015, pages 32–37, 2015.

[Murtagh and Contreras, 2012] Fionn Murtagh and Pedro
Contreras. Algorithms for hierarchical clustering: an
overview. Wiley Interdisciplinary Reviews: Data Mining
and Knowledge Discovery, 2(1):86–97, 2012.

[Pollard and others, 1981] David Pollard et al. Strong con-
sistency of k-means clustering. The Annals of Statistics,
9(1):135–140, 1981.

[Ryabko, 2010] D. Ryabko. Clustering processes. In Proc.
the 27th International Conference on Machine Learning
(ICML 2010), pages 919–926, Haifa, Israel, 2010.

[Terada, 2013] Yoshikazu Terada. Strong consistency of fac-
torial k-means clustering. Annals of the Institute of Statis-
tical Mathematics, 67(2):335–357, 2013.

[Terada, 2014] Yoshikazu Terada. Strong consistency of re-
duced k-means clustering. Scandinavian Journal of Statis-
tics, 41(4):913–931, 2014.

[Tola et al., 2008] Vincenzo Tola, Fabrizio Lillo, Mauro Gal-
legati, and Rosario N Mantegna. Cluster analysis for port-
folio optimization. Journal of Economic Dynamics and
Control, 32(1):235–258, 2008.

[Tropp, 2015] Joel A Tropp. An introduction to matrix con-
centration inequalities. arXiv preprint arXiv:1501.01571,
2015.

[Tumminello et al., 2007] Michele Tumminello, Fabrizio
Lillo, and Rosario N Mantegna. Kullback-leibler distance
as a measure of the information ﬁltered from multivariate
data. Physical Review E, 76(3):031123, 2007.

[Von Luxburg et al., 2008] Ulrike Von Luxburg, Mikhail
Belkin, and Olivier Bousquet. Consistency of spectral
clustering. The Annals of Statistics, pages 555–586, 2008.

