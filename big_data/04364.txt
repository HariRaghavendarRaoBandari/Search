6
1
0
2

 
r
a

 

M
4
1

 
 
]
h
c
e
m

-
t
a
t
s
.
t
a
m
-
d
n
o
c
[
 
 

1
v
4
6
3
4
0

.

3
0
6
1
:
v
i
X
r
a

On the overlaps between eigenvectors of correlated random matrices

Jo¨el Bun1,2,3, Jean-Philippe Bouchaud1, Marc Potters1

1 Capital Fund Management, 23–25, rue de l’Universit´e, 75 007 Paris

2 LPTMS, CNRS, Univ. Paris-Sud, Universit´e Paris-Saclay, 91405 Orsay, France and
3 Leonard de Vinci Pˆole Universitaire, Finance Lab, 92916 Paris La D´efense, France.

We obtain general, exact formulas for the overlaps between the eigenvectors of large correlated
random matrices, with additive or multiplicative noise. These results have potential applications in
many diﬀerent contexts, from quantum thermalisation to high dimensional statistics. We apply our
results to the case of empirical correlation matrices, that allow us to estimate reliably the width of
the spectrum of the “true” underlying correlation matrix, even when the latter is very close to the
identity matrix. We illustrate our results on the example of stock returns correlations, that clearly
reveal a non trivial structure for the bulk eigenvalues.

The structure of the eigenvalues and eigenvectors of
large random matrices is of primary importance in many
diﬀerent contexts, from quantum mechanics to high di-
mensional data analysis. Correspondingly, Random Ma-
trix Theory (RMT) has established itself as a major dis-
cipline, at the frontier between theoretical physics, math-
ematics, probability theory, and applied statistics, with
a somewhat intimidating corpus of knowledge [1]. One of
the most striking applications of RMT concerns quantum
chaos and quantum transport [2], with renewed interest
coming from problems of quantum ergodicity (“eigen-
state thermalisation”) [3, 4], entanglement and dissipa-
tion (for recent reviews see [5, 6]). In the context of signal
processing, RMT is of primary importance in the analysis
of high dimensional statistics [7–9], wireless communica-
tion channels [10, 11], etc. Other examples cover the
dynamics of complex systems – from random ecologies
[12] to glasses and spin-glasses [13].

√

CW√

Whereas the spectral properties of random matrices
have been investigated at length, the interest has recently
shifted to the statistical properties of their eigenvectors
– see e.g. [3, 14] and [15–21] for more recent papers. In
particular, one can ask how the eigenvectors of a sample
matrix S resemble those of the population (or pure) ma-
trix C itself. We recently obtained in [22] explicit formu-
las for the overlaps between these pure and noisy eigen-
vectors for a wide class of random matrices, generalizing
results obtained for sample covariance matrices of [15] –
C where W is a Wishart ma-
obtained as S =
trix – and for matrices of the form S = C +W, where W
is a Gaussian random matrix [18, 23, 24]. In the present
paper, we want to generalize these results to the overlaps
between the eigenvectors of two diﬀerent realisations of
such random matrices, that remain correlated through
their common part C. For example, imagine one mea-
sures the sample covariance matrix of the same process,
but on two non-overlapping time intervals, characterized
by two independent realizations of the Wishart noises
W and W(cid:48). How close are the corresponding eigenvec-
tors expected to be? We provide exact, explicit formulas
for these overlaps in the high dimensional regime. Per-
haps surprisingly, these overlaps can be evaluated from

the empirical spectrum of S only, i.e. without any prior
knowledge of the pure matrix C itself. This leads us to
propose a statistical test based on these overlaps, that
allows one to determine whether two realisations of the
random matrix S and S(cid:48) indeed correspond to the very
same underlying “true” matrix C either in the multi-
plicative and additive cases deﬁned above. We give a
transparent interpretation to our formulas and general-
ize them to various cases, in particular when the noises
are correlated.
Throughout the following, we consider N × N sym-
metric random matrices and denote by λ1 (cid:62) λ2 (cid:62)
. . . (cid:62) λN the eigenvalues of S and by u1, u2, . . . , uN
Similarly, we denote
the corresponding eigenvectors.
N the eigenvalues of S(cid:48) and by
(cid:62) . . . (cid:62) λ(cid:48)
by λ(cid:48)
u(cid:48)
1, u(cid:48)
N the associated eigenvectors. Note that we
will index the eigenvectors by their corresponding eigen-
values for convenience. The central object that we focus
on in this study are the asymptotic (N → ∞) scaled,
mean squared overlaps

(cid:62) λ(cid:48)
2, . . . , u(cid:48)

1

2

(1)

Φ(λ, λ(cid:48)) ..= NE(uλ · u(cid:48)

λ(cid:48))2,
that remain O(1) in the limit N → ∞.
In the above
equation, the expectation E can be interpreted either as
an average over diﬀerent realisations of the randomness
or, for ﬁxed randomness, as an average over small “slices”
of eigenvalues of width η = dλ (cid:29) N−1, such that the re-
sult becomes self-averaging in the large N limit. We will
study the asymptotic behavior of (1) using the complex
function

Tr(cid:2)(z − S)−1(z(cid:48) − S(cid:48))−1(cid:3)(cid:43)

(cid:42)

1
N

ψ(z, z(cid:48)) ..=

,

(2)

P

where z, z(cid:48) ∈ C and (cid:104)·(cid:105)P denotes the average with re-
spect to probability measure associated to S and S(cid:48). In-
deed, using the spectral decomposition, we obtain the
following inversion formula, valid when N → ∞ for any
λ, λ(cid:48) ∈ supp , where  is the spectral density of S (that
we assume here to be the same as that of S(cid:48), but see

Appendix for more general cases):

Φ(λ, λ(cid:48)) =

Re[ψ0(z, z(cid:48)) − ψ0(z, z(cid:48))]

2π2(λ)(λ(cid:48))

,

(3)

with z = λ − iη, z its complex conjugate and ψ0 ≡
limη↓0 ψ. The derivation of the identity (3) can be found
in the Appendix.
The study of the asymptotic behavior of the function ψ
requires to control the resolvent of S and S(cid:48) entry-wise.
It was shown recently that one can approximate these
(random) resolvents by deterministic equivalent matrices
[22, 25, 26].
In the case of correlated Wishart (covari-
C, this allows us to obtain,
ance) matrices S =
for N → ∞:

CW√

√

ψ(z, z(cid:48)) =

ζ(z(cid:48))g(z) − ζ(z)g(z(cid:48))

z(cid:48)ζ(z(cid:48)) − zζ(z)

,

(4)

where g(z) is the Stieltjes transform of S:
g(z) =
N−1 Tr(zIN −S)−1, and ζ(z) ..= (1−q+qzg(z))−1, where
q = N/T is e.g. the ratio between the number of observ-
ables and the length of the empirical time series. The
derivation of (4) is given in the Appendix. In the case of
additive Gaussian noise, S = C+W, one obtains instead:

ψa(z, z(cid:48)) =

g(z) − g(z(cid:48))
ξ(z(cid:48)) − ξ(z)

,

(5)

where ξ(z) = z − σ2g(z) and σ2 is the variance of the
elements of the Gaussian random matrix W. [Here and
henceforth, the subscript a denotes the additive noise.]
Note that from Ref.
[26] we expect the results (4) and
(5) to hold with ﬂuctuations of order N−1/2.

One notices that both Eq. (4) and Eq. (5) only depend
on a priori observable quantities, i.e.
they do not in-
volve explicitly the unknown matrix C. Hence, we will
obtain an observable expression for (1) using the inver-
sion formula (3). For sample covariance matrices, it is
convenient to work with m(z) := (zζ(z))−1. Deﬁning
m0(λ) = limη↓0 m(λ− iη) ≡ mR(λ) + imI (λ), the asymp-
totic behaviour of Eq. (1) for any λ, λ(cid:48) ∈ supp  can be
explicitly computed as (see Appendix for details)

Φ(λ, λ(cid:48)) ∼

N→∞ 2q

α(λ, λ(cid:48))

λ λ(cid:48) γ(λ, λ(cid:48))

,

(6)

where α and γ are deterministic and symmetric functions

α(λ, λ(cid:48)) ..= (λ − λ(cid:48))(cid:0)mR(λ)|m0(λ(cid:48))|2 − mR(λ(cid:48))|m0(λ)|2(cid:1)
γ(λ, λ(cid:48)) ..= (cid:2)(mR(λ) − mR(λ(cid:48)))2 + (mI (λ) + mI (λ(cid:48)))2(cid:3) ×
(cid:2)(mR(λ) − mR(λ(cid:48)))2 + (mI (λ) − mI (λ(cid:48)))2(cid:3)(7)

The result (6) is indeed remarkable in that the mean
squared overlap between the eigenvectors of independent
sample covariance matrices can be expressed to leading
order in N without the explicit knowledge of the true

2

underlying covariance C. Similarly, in the additive case,
we ﬁnd:

Φa(λ, λ(cid:48)) ∼

N→∞ 2σ2(λ − λ(cid:48))

ξR(λ) − ξR(λ(cid:48))

γa(λ, λ(cid:48))

,

(8)

where γa(λ, λ(cid:48)) is given by the same expression as γ(λ, λ(cid:48))
in Eq. (7) with the substitutions mR → ξR and mI → ξI .
Again, (8) does not involve the pure matrix C.

Eqs. (6) and (8) are exact and are the main new results
of this work. They can be simpliﬁed in the limit where
λ(cid:48) → λ, i.e. when one studies the “self” overlaps between
eigenvectors corresponding to the same eigenvalue λ. In
this case, we ﬁnd:

(cid:2)mR(λ)/|m0(λ)|2(cid:3)

|m0(λ)|4∂λ
I (λ)|∂λm0(λ)|2
m2

Φ(λ, λ) =

q
2λ2

,

(9)

(10)

and for the additive case:

Φa(λ, λ) =

σ2
2

I (λ)(cid:12)(cid:12)∂λξ0(λ)(cid:12)(cid:12)2 ,

∂λξR(λ)

ξ2

where we used the notation ξ0(λ) ≡ limη↓0 ξ(λ − iη).

Before investigating some concrete applications of
these formulas, let us give an interesting interpretation
of the above formalism. We ﬁrst introduce the set of
eigenvectors vµ of the pure matrix C, labelled by the
eigenvalue µ. We deﬁne the overlaps between the u’s

and the v’s as(cid:112)O(µ, λ)/N × ε(µ, λ), where O(µ, λ) were

(cid:90)

uλ =

1√
N

explicitly computed in [22] for a wide class of problems,
and ε(µ, λ) are random variables of unit variance. Now,
one can always decompose the u’s as:

dµC(µ)(cid:112)O(µ, λ)ε(µ, λ)vµ,
dµC(µ)(cid:112)O(µ, λ)O(µ, λ(cid:48)) ε(µ, λ)ε(µ, λ(cid:48)).

where C is the spectral density of C. Using the or-
thonormality of the v’s, one then ﬁnds:
uλ·u(cid:48)

(cid:90)

(11)

λ(cid:48) =

1
N

(12)
If we square this last expression and average over the
noise, and make an “ergodic hypothesis” [3] according to
which all signs ε(µ, λ) are in fact independent from one
another, one ﬁnds the following, rather intuitive convo-
lution result for square overlaps:

Φ(λ, λ(cid:48)) =

dµC(µ)O(µ, λ) O(µ, λ(cid:48)).

(13)

(cid:90)

It turns out that this expression is completely general
and exactly equivalent to Eqs. (6) and (8) in the cor-
responding cases. However, whereas this expression still
contains some explicit dependence on the structure of the
pure matrix C, it has disappeared in Eqs. (6) and (8).

Now, let us consider the case of sample correlation ma-
trices, ﬁrst in the theoretical case where the true correla-
tion matrix C is an inverse Wishart matrix of parameter

(cid:35)

(cid:34)

κ ∈ (0,∞), that corresponds to 1/q for Wishart matrices
(see [22] for details). In this case, the function m(z) can
be explicitly computed. This ﬁnally leads to:

2qκ(cid:0)2λ(ν + κ) − λ2κ + κ(2qν − 1)(cid:1)

ν(λ + 2qκ)2

(14)

Φ(λ, λ) =

κ−1(cid:104)

with ν
val

ν + κ ±(cid:112)(2κ + 1)(2qκ + 1)

:= 1 + qκ and λ is within the inter-
[λ−, λ+], where the edges are given by λ± =
. An interesting limit
corresponds to κ → ∞, where C tends to the identity
matrix, and the overlaps are expected to become all equal
to 1/N . Indeed one ﬁnds, for a ﬁxed q:

(cid:105)

Φ(λ, λ(cid:48)) ∼
κ→∞

(λ − 1)(λ(cid:48) − 1)

2q2

1 +

κ−1 + O(κ−2)

,

(15)
which is in fact universal in this limit, provided the eigen-
value spectrum of C has a variance given by (2κ)−1 → 0.
[29] This formula is interesting insofar as it allows one to
estimate the width of the eigenvalue distribution of C,
even when it is close to the identity matrix, i.e. κ (cid:29) 1.
One could think of directly using information on the em-
pirical spectrum, for example the Marˇcenko-Pastur pre-
diction Tr C−1 = (1 − q) Tr S−1, that in principle al-
lows one extract the parameter κ through 1 + (2κ)−1 =
(1 − q) Tr S−1/N . However, this method is numerically
unstable and very imprecise when κ (cid:29) 1 and ﬁnite N s
(for one thing, the RHS can be negative, leading to a
negative variance). Our formula based on overlaps avoid
these diﬃculties. As an illustration, we check the validity
of Eq. (14) in the inset of Figure 1 in the case κ = 10,
N = 500 and q = 0.5. We determine the empirical av-
erage overlap as follows: we consider 100 independent
realisation of the Wishart noise W. For each pair of
samples we compute a smoothed overlap as:

N(cid:88)

j=1

[(ui · u(cid:48)

i)2] =

1
Zi

(ui · u(cid:48)
(λi − λ(cid:48)

j)2

j)2 + η2 ,

(16)

with Zi = (cid:80)N

k=1((λi − λ(cid:48)

k)2 + η2)−1 the normalization
constant and η the width of the Cauchy kernel, that we
choose to be N−1/2 in such a way that N−1 (cid:28) η (cid:28) 1.
We then average this quantity over all pairs for a given
value of i to obtain [(ui · u(cid:48)
i)2]e, and plot the resulting
quantity as a function of the average eigenvalue position
[λi]e, see Figure 1, inset. The agreement with Eq. (14)
is excellent, even when the true underlying matrix C is
close to the identity matrix

We now investigate an application to real data, in the
case of stock markets and using a bootstrap technique
to generate diﬀerent samples. Speciﬁcally, we consider
the standardized daily returns of the 450 most liquid US
stocks from 2005 to 2012. We randomly split the total
period of 1800 days into two non-overlapping subsets of

3

size T = 900. We then measure the overlap between the
eigenvectors of the empirical correlation matrices S and
S(cid:48) corresponding to the two subsets exactly as in Eq. (16)
above and average over 100 diﬀerent bootstrap samples.
The results are reported in Figure 1 where we only focus
on “bulk” eigenvalues (the top eigenvectors are governed
by non trivial dynamics, see e.g. [16]). We conclude from
Figure 1 that bulk eigenvalues have a non trivial structure
since the mean squared overlaps strongly depart from
the null hypothesis and in fact cannot be estimated us-
ing the universal formula Eq. (15) as a parabolic ﬁt is
clearly unwarranted. A ﬁt with the full inverse Wishart
formula (14) leads to κ ≈ 0.7.
[30]. A much better ﬁt
could be achieved by choosing another prior for the spec-
trum of C, for example a power-law as in [8]; we however
leave this issue for later investigations. The conclusion
of this study is that the cross-sample eigenvector over-
laps provide precious insights about the structure of the
true eigenvalue spectrum, much beyond the information
contained in the empirical spectrum itself.

FIG. 1: Main Figure: Evaluation of the self-overlap NE(ui ·
u(cid:48)
i)2 using N = 450 US stocks’ daily returns from 2005 to
2012 and 100 bootstrap replicas of two non-overlapping pe-
riods of size T = 900 and with N = 450. We compare the
empirical results with (i) the null hypothesis 1/N (green dot-
ted line) and (ii) an Inverse-Wishart prior with q = 0.5 and
an estimated parameter κ ≈ 0.7 (red line). Inset: Same plot
using synthetic data coming from an Inverse-Wishart prior
with parameter κ = 10 and N = 500. The empirical average
is taken over 100 realizations of the noise. The agreement
with the theoretical formula is excellent.

All the above results can actually be applied in an
much broader context (see Appendix for technical de-
tails). For instance, the noise parameters can be diﬀer-
ent for the two realisations: for example the observation
ratio q = N/T can be diﬀerent for S and S(cid:48), or the am-
plitude of the noise σ can be diﬀerent in the additive
COBO∗√
case. Another direction is to consider generalized empir-
C,
ical correlation matrices of the form S =
where B is a given matrix independent from C, and O is

√

a random matrix chosen in the Orthogonal group O(N )
according to the Haar measure (see e.g.
[20, 22, 27]).
The case above corresponds to the case where OBO∗ is
a Wishart matrix. We ﬁnd for this general model that
(4) still holds with ζ(z) replaced by SB(zg(z)− 1) where
SB is the so-called Voiculescu’s S-transform of the B ma-
trix [28]. If B = W, then SB(z) = 1/(1 + qz). Similarly,
the additive model can be generalized to S = C + OBO∗
with the same deﬁnitions for B and O. In that case, the
above result (5) again holds with ξ(z) = z − RB(g(z)),
where RB(z) is now the R-transform of the B matrix [28]
– which is simply equal to RB(z) = σ2z when B = W
is a Gaussian random matrix, as considered above. Note
that in all these generalized cases, the overlap convolu-
tion formula (13) is always valid provided that the noises
are independent.

√

√

√

ρ W0 +

√

ρ W0 +

1 − ρ W1 and W(cid:48) =

Finally, an important case is when the noises W, W(cid:48)
or W, W(cid:48) are correlated – while the above calculations
referred to independent noises. In the additive case, the
√
trick is to realize that one can always write (in law) W =
1 − ρ W2,
where W1, W2 are now independent, as above. Since our
formulas do not rely on the common matrix C that can
therefore be replaced by C +
ρW0, (10) trivially holds
with σ2 simply multiplied by 1 − ρ. The corresponding
shape of Φa(λ, λ) for diﬀerent values of ρ is shown in Fig.
2. We also provide in the inset a comparison with syn-
thetic data for a ﬁxed ρ = 0.54, σ2 = 1. The empirical av-
erage is taken over 200 realizations of the noise and again,
the agreement is excellent. The multiplicative model
is also interesting since it describes the case of correla-
tion matrices measured on overlapping periods, such that
S =
C.
This case turns out to be more subtle and is the subject
of ongoing investigations.

C(cid:2)W 0 + W(cid:48)(cid:3)√

C [W 0 + W]

C and S(cid:48) =

√

√

√

In summary, we have provided general, exact formu-
las for the overlaps between the eigenvectors of large
correlated random matrices, with additive or multiplica-
tive noise. Remarkably, these results do not require the
knowledge of the underlying “pure” matrix and have a
broad range of applications in diﬀerent contexts. We
showed that the cross-sample eigenvector overlaps pro-
vide unprecedented information about the structure of
the true eigenvalue spectrum, much beyond that con-
tained in the empirical spectrum itself. For example, the
width of the bulk of the spectrum of the true underlying
correlation matrix can be reliably estimated, even when
the latter is very close to the identity matrix. We have
illustrated our results on the example of stock returns
correlations, that clearly reveal a non trivial structure
for the bulk eigenvalues.

Acknowledgments: We want to thank R. Allez, R.
B´enichou, R. Chicheportiche, B. Collins, A. Rej, E. S´eri´e
and D. Ullmo for very useful discussions.

4

FIG. 2: Main Figure: Evaluation of the self overlap Φa(λ, λ(cid:48))
for a ﬁxed λ(cid:48) ≈ 0.95 as a function of λ for N = 500, σ2 = 1,
and for diﬀerent values of ρ. The population matrix C is given
by a (white) Wishart matrix with parameter T = 2N . Inset:
We compare the theoretical prediction Φa(λ, λ(cid:48) ≈ 0.95) for a
ﬁxed ρ = 0.54 with synthetic data. The empirical averages
(blue points) are obtained from 200 independent realizations
of S.

[1] G. Akemann, J. Baik, and P. Di Francesco, The Oxford
handbook of random matrix theory (Oxford University
Press, 2011).

[2] C. W. Beenakker, Reviews of modern physics 69, 731

(1997).

[3] J. M. Deutsch, Physical Review A 43, 2046 (1991).
[4] G.

Ithier and F. Benaych-Georges, arXiv preprint

arXiv:1510.04352 (2015).

[5] R. Nandkishore and D. A. Huse, Annual Review of Con-

densed Matter Physics 6, 15 (2015).

[6] J. Eisert, M. Friesdorf, and C. Gogolin, Nature Physics

11, 124 (2015).

[7] I. M. Johnstone,

in Proceedings on the International

Congress of Mathematicians I (2007), pp. 307–333.

[8] J.-P. Bouchaud and M. Potters, in The Oxford hand-
book of Random Matrix Theory (Oxford University Press,
2011).

[9] D. Paul and A. Aue, Journal of Statistical Planning and

Inference 150, 1 (2014).

[10] A. M. Tulino and S. Verd´u, Random matrix theory and
wireless communications, vol. 1 (Now Publishers Inc,
2004).

[11] R. Couillet, M. Debbah, et al., Random matrix meth-
ods for wireless communications (Cambridge University
Press Cambridge, MA, 2011).

[12] R. M. May, Nature 238, 413 (1972).
[13] Y. V. Fyodorov, Physical review letters 92, 240601

(2004).

[14] M. Wilkinson and P. N. Walker, Journal of Physics A:

Mathematical and General 28, 6143 (1995).

[15] O. Ledoit and S. P´ech´e, Probability Theory and Related

Fields 151, 233 (2011).

[16] R. Allez and J.-P. Bouchaud, Physical Review E 86,

5

046202 (2012).

[17] P. Bourgade, L. Erd˝os, H.-T. Yau, and J. Yin, Commu-

nications on Pure and Applied Mathematics (2015).

[25] Z. Burda, A. G¨orlich, A. Jarosz, and J. Jurkiewicz, Phys-
ica A: Statistical Mechanics and its Applications 343, 295
(2004).

[18] R. Allez and J.-P. Bouchaud, Random Matrices: Theory

[26] A. Knowles and J. Yin, arXiv preprint arXiv:1410.3516

and Applications 3, 1450010 (2014).

(2014).

[19] A. Bloemendal, A. Knowles, H.-T. Yau, and J. Yin, Prob-

[27] Z. Burda, J. Jurkiewicz, and B. Wac(cid:32)law, Physical Review

ability Theory and Related Fields pp. 1–94 (2015).

E 71, 026111 (2005).

[20] R. Couillet and F. Benaych-Georges, arXiv preprint

arXiv:1510.03547 (2015).

[21] R. Monasson and D. Villamaina, EPL (Europhysics Let-

ters) 112, 50001 (2015).

[22] J. Bun, R. Allez, J.-P. Bouchaud, and M. Potters, arXiv

preprint arXiv:1502.06736 (2015).

[23] Philippe Biane, Quantum Probability Communications

11, 55 (2003).

[24] R. Allez, J. Bun, and J.-P. Bouchaud, arXiv preprint

arXiv:1412.7108 (2014).

[28] D. Voiculescu, Inventiones mathematicae 104, 201 (1991).
[29] The analysis for the additive case leads to a very similar
result. More precisely, taking C = IN + W0 with W0 a
0 →
GOE (independent from W and W(cid:48)) of variance σ2
0, one ﬁnds that Φa(λ, λ(cid:48)) is given by exactly the same
formula (15) with the substitution 2q2κ → σ4/σ2
0.

[30] The calibration of κ is performed by least squares using
Eq. (14) and where we removed the 10 largest eigenval-
ues.

Appendix: Supplementary Material

We collect the derivations of the diﬀerent results in the following sections. For the sake of clarity, we rename S(cid:48), its

eigenvalues and eigenvectors respectively by (cid:101)S, [(cid:101)λi]i∈[[1,N ]] and [(cid:101)ui]i∈[[1,N ]], not to confuse “primes” with derivatives.

Note that our calculations rely on some physical arguments that can certainly be made rigorous in a mathematical
sense. Precise estimates about the error terms for sample covariance matrices or deformed GOE may be obtained
using the results of [26] for instance. Throughout the following, the symbol “∼” denotes the limit N → ∞.

Inversion formula (derivation of (3))

The derivation of the inversion formula (3) is pretty straightforward. We start from Eq. (2) that we rewrite using

the spectral decomposition of S and (cid:101)S as
ψ(z,(cid:101)z) =

(cid:42)

1
N

N(cid:88)

(cid:43)

1

z − λi

1(cid:101)z −(cid:101)λj

(ui ·(cid:101)uj)2

where P denotes the probability density function of the noise part of S and(cid:101)S. For large random matrices, we expect
the eigenvalues of [λi]i∈[[1,N ]]] and [(cid:101)λi]i∈[[1,N ]]] stick to their classical locations, i.e. smoothly allocated with respect to

i,j=1

P

the quantile of the spectral density. Roughly speaking, the sample eigenvalues become deterministic in the large N
limit. Hence, we obtain after taking the continuous limit

,

(17)

where  and (cid:101) are respectively the spectral density of S and (cid:101)S, and Φ denotes the mean squared overlap deﬁned in

(1) above. Then, it suﬃces to compute

(cid:90) (cid:90) (λ)

ψ(z,(cid:101)z) ∼

Φ(λ,(cid:101)λ)dλd(cid:101)λ,

z − λ

(cid:101)((cid:101)λ)
(cid:101)z −(cid:101)λ
(y −(cid:101)λ ∓ iη)
(y −(cid:101)λ)2 + η2

ψ(x − iη, y ± iη) ∼

=

(cid:90) (cid:90) (x − λ + iη)
(cid:90) (cid:90) (x − λ)(y −(cid:101)λ) ± η2 + iη(y −(cid:101)λ ∓ (x − λ))

((x − λ)2 + η2)((y −(cid:101)λ)2 + η2)

(x − λ)2 + η2

(λ)(cid:101)((cid:101)λ)Φ(λ,(cid:101)λ)dλd(cid:101)λ

(18)

(19)

(20)

(λ)(cid:101)((cid:101)λ)Φ(λ,(cid:101)λ)dλd(cid:101)λ,
η(cid:101)((cid:101)λ)
Φ(λ,(cid:101)λ)dλd(cid:101)λ.
(y −(cid:101)λ)2 + η2

from which we deduce that

Re(cid:2)ψ(x − iη, y + iη) − ψ(x − iη, y − iη)(cid:3) ∼ 2

(cid:90) (cid:90)

η(λ)

(x − λ)2 + η2

Finally, the inversion formula follows from Sokhotski-Plemelj identity

Re(cid:2)ψ(x − iη, y + iη) − ψ(x − iη, y − iη)(cid:3) ∼ 2π2(x)(cid:101)(y)Φ(x, y).

lim
η→0

Note that the derivation holds for any models of S and (cid:101)S as long as its spectral density converges to a well-deﬁned

deterministic limit.

Mean squared overlap for independent sample covariance matrices

Derivation of (4). We now compute the asymptotic expression of the function ψ for sample covariance matrices. Note
C
that we shall omit the identity matrix IN in our notations throughout the following. We recall that S =

and(cid:101)S =
parametrized by a possibly diﬀerent dimensional ratio q and (cid:101)q. By independence, we have

CW√
C where W and (cid:102)W are two independent Wishart matrices. Here, we allow these two matrices to be

C(cid:102)W√

√

√

1
N

N(cid:88)
(cid:10)(z − S)−1
ψ(z,(cid:101)z) =
(cid:16)
(cid:11)
(cid:10)(z − S)−1
N ∼ ζ(z)

k,l

kl

P ,

(cid:11)

P

(cid:11)

kl

(cid:10)((cid:101)z −(cid:101)S)−1
(cid:17)−1

zζ(z) − C

+ O(N−1/2),

then, we use the deterministic estimate like that of [22, 25, 26] to ﬁnd for any z = x − iη at global scale that

where we recall that ζ(z) = 1/(1 − q + qzg(z)). Such an estimate holds as well for(cid:101)S by replacing q and g with(cid:101)q and
(cid:101)g. Hence, by deﬁning (cid:101)ζ((cid:101)z) = 1/(1 −(cid:101)q +(cid:101)q(cid:101)z(cid:101)g((cid:101)z)), we can rewrite Eq. (22) as

kl

kl

ψ(z,(cid:101)z) ∼ 1

N

Tr[ζ(z)(zζ(z) − C)−1(cid:101)ζ((cid:101)z)((cid:101)z(cid:101)ζ((cid:101)z) − C)−1],

6

(21)

(22)

(23)

(24)

(25)

(26)

(27)

(28)

(29)

and then, we use the identity

to obtain

(zζ(z) − C)−1((cid:101)z(cid:101)ζ((cid:101)z) − C)−1 =
ψ(z,(cid:101)z) ∼ ζ(z)(cid:101)ζ((cid:101)z)
(cid:101)z(cid:101)ζ((cid:101)z) − zζ(z)
(cid:32)(cid:101)ζ((cid:101)z)
(cid:104)

(cid:101)z(cid:101)ζ((cid:101)z) − zζ(z)

1
N

Tr

1

ψ(z,(cid:101)z) ∼

From this last equation, we deduce

By taking the normalized trace in (23), we have at leading order that

g(z) ∼ ζ(z)
N

Tr(zζ(z) − C)−1

We therefore conclude by plugging this last equation into (27) that

which reduces to Eq. (4) when q (cid:54)=(cid:101)q.

[(zζ(z) − C)−1 − ((cid:101)z(cid:101)ζ((cid:101)z) − C)−1],

1

1
N

(cid:101)z(cid:101)ζ((cid:101)z) − zζ(z)
Tr[(zζ(z) − C)−1 − ((cid:101)z(cid:101)ζ((cid:101)z) − C)−1].
ζ(z)(zζ(z) − C)−1(cid:105) − ζ(z)
and (cid:101)g((cid:101)z) ∼ (cid:101)ζ((cid:101)z)
ψ(z,(cid:101)z) ∼ (cid:101)ζ((cid:101)z)g(z) − ζ(z)(cid:101)g((cid:101)z)
(cid:101)z(cid:101)ζ((cid:101)z) − zζ(z)

Tr((cid:101)z(cid:101)ζ((cid:101)z) − C)−1.

1
N

Tr

N

,

(cid:104)(cid:101)ζ((cid:101)z(cid:101)ζ((cid:101)z) − C)−1(cid:105)(cid:33)

7

(cid:21)

which is equivalent to

confusion. First, we rewrite (4) as

Derivation of (6). Using the deﬁnition of the complex function m and (cid:101)m above, we ﬁnd that ζ(z) = 1/(zm(z)) and
(cid:101)ζ((cid:101)z) = 1/((cid:101)z(cid:101)m((cid:101)z)). Note that we shall omit the arguments z and (cid:101)z in m and (cid:101)m in the following when there are no

(cid:20) g(cid:101)z(cid:101)m
[zgm −(cid:101)z(cid:101)g(cid:101)m] .
Then, we express the function ψ as a function of m and (cid:101)m so that we have
(q −(cid:101)q)(cid:101)m
m − (cid:101)m
To use the inversion formula Eq. (3), we need to evaluate ψ(λ − iη,(cid:101)λ ± iη). Using the short handed notation

ψ(z,(cid:101)z) ∼
1/(cid:101)m − 1/m
ψ(z,(cid:101)z) ∼ 1
z(cid:101)z
m − (cid:101)m
(cid:20) ((cid:101)qz − q(cid:101)z)(cid:101)m2
m − (cid:101)m

ψ(z,(cid:101)z) ∼ 1
q(cid:101)qz(cid:101)z

m + (cid:101)m
q(cid:101)z

qz(cid:101)z
− 1 − q

− (cid:101)g

(cid:21)

(31)

(30)

(32)

zm

+

+

1

1

,

.

m0(λ) = limη→0 m(λ − iη), we ﬁnd

(cid:105)

I )

(33)

(cid:16)

m0

m0

0

m0

0

lim
η→0

(cid:17)(cid:16)

R + (cid:101)m2

(cid:17) + Im,

+ (cid:101)m0(cid:101)m0

Straightforward computations yields

(cid:104)(cid:101)m2
0 − (cid:101)m2
(cid:16)

∼ (cid:101)qλ − q(cid:101)λ
q(cid:101)qλ(cid:101)λ
(cid:105)

s where we omit the explicit expressions of the imaginary part since this is useless (see Eq. (3)). Then, using the

(cid:105)
(cid:104)(cid:101)m0 − (cid:101)m0
(cid:17)(cid:16)
m0 − (cid:101)m0
m0 − (cid:101)m0
R − 2mR(cid:101)mR)(cid:3),

[ψ(λ − iη,(cid:101)λ + iη) − ψ(λ − iη,(cid:101)λ − iη)]
(cid:104)(cid:101)m2
(cid:105)
(cid:104)(cid:101)m0 − (cid:101)m0
0 − (cid:101)m2
+ (cid:101)m0(cid:101)m0
+ (cid:101)q − q
(cid:16)
(cid:17)(cid:16)
(cid:17)
q(cid:101)qλ(cid:101)λ
m0 − (cid:101)m0
m0 − (cid:101)m0
representation m0 = mR + imI and (cid:101)m0 = (cid:101)mR + i(cid:101)mI , one ﬁnds
(cid:105)
(cid:104)(cid:101)m0 − (cid:101)m0
(cid:2)2mI(cid:101)mR + i((cid:101)m2
= 2(cid:101)mI
(cid:105)
(cid:104)(cid:101)m0 − (cid:101)m0
(cid:2)mI − imR
(cid:3),
= 2(cid:101)mI
(cid:17)
m0 − (cid:101)m0
I ) + 2imI (mR − (cid:101)mR).
I − (cid:101)m2
= (mR − (cid:101)mR)2 − (m2
(cid:17)(cid:12)(cid:12)(cid:12)2
(cid:104)
(cid:105)2
I (mR − (cid:101)mR)2,
I − (cid:101)m2
(mR − (cid:101)mR)2 − (m2
(cid:104)
I )2(cid:105)(cid:104)
(mR − (cid:101)mR)2 + (m2
(mR − (cid:101)mR)2 + (m2
I + (cid:101)m2
(cid:104)(cid:101)m0 − (cid:101)m0
(cid:17)
(cid:105)(cid:17) ×(cid:16)
(cid:17)(cid:16)
m0 − (cid:101)m0
= 4mI(cid:101)mI
(cid:104)|(cid:101)m0|2 − |m0|2(cid:105)
(cid:105) ×(cid:16)
(cid:17)
= 2mI(cid:101)mI
m0 − (cid:101)m0
By regrouping these last three equations with the prefactors in (33), and recalling that mI (λ) = πq(λ) and (cid:101)mI ((cid:101)λ) =
π(cid:101)q(cid:101)((cid:101)λ), so we obtain by using the inversion formula (3) the general result:
2((cid:101)qλ − q(cid:101)λ)(cid:2)mR|(cid:101)m0|2 − (cid:101)mR|m0|2(cid:3) + ((cid:101)q − q)(cid:2)|(cid:101)m0|2 − |m0|2(cid:3)
(cid:104)
λ(cid:101)λ
(mR − (cid:101)mR)2 + (mI + (cid:101)mI )2
(mR − (cid:101)mR)2 + (mI − (cid:101)mI )2

I )2(cid:105)
I − (cid:101)m2
mR|(cid:101)m0|2 − (cid:101)mR|m0|2(cid:105)

m0 − (cid:101)m0
(cid:17)(cid:16)
m0 − (cid:101)m0

m0 − (cid:101)m0
(cid:17)(cid:16)
m0 − (cid:101)m0

which is exactly the denominator in (6). For the numerator, elementary complex analysis in Eq. (33) yields

+ (cid:101)m0(cid:101)m0
(cid:104)(cid:101)m0 − (cid:101)m0

Φq,(cid:101)q(λ,(cid:101)λ) =

(cid:104)(cid:101)m2
0 − (cid:101)m2

m0 − (cid:101)m0

(cid:12)(cid:12)(cid:12)(cid:16)

(cid:105)(cid:104)

+ 4m2

(cid:105) .

(cid:105)

0

,

(37)

,

(36)

(cid:16)

m0

(34)

(35)

.

(38)

m0

=

=

and

and

m0

(cid:104)

(39)

One easily retrieve Eq. (6) by taking (cid:101)q = q. Another interesting case if when (cid:101)q = 0 as we expect (cid:101)λ → µ, which is
precisely the framework of [15]. In that case, we have (cid:101)mR = 1/µ and (cid:101)mI = 0. Hence, we deduce from (39) that

8

Φq,(cid:101)q=0(λ, µ) =

λµ(cid:2)(mR − 1/µ)2 + m2

q

(cid:3)

I

=

qµ

λ|1 − µm0(λ)|2 ,

(40)

(41)

(42)

(43)

+ O(ε3)

(cid:105)

which is exactly the result of [15].

Derivation of (9). We set (cid:101)q = q and(cid:101)λ = λ + ε with ε > 0. This yields

mR((cid:101)λ) = mR(λ) + ε∂λmR(λ) + O(ε2),
|m0((cid:101)λ)|2 = |m0(λ)|2 + 2ε(cid:2)mR(λ)∂λmR(λ) + π2q2(λ)∂λ(λ)(cid:3) + O(ε2).

Thus, we get from (7) for q =(cid:101)q that

α(λ, λ + ε) = −ε2(cid:104)

and

(cid:104)

γ(λ, λ + ε) =

2mR(λ)[mR(λ)∂λm(cid:48)

R(λ) + mI (λ)∂λm(cid:48)

I (λ)] − ∂λmR(λ)(m2

R(λ) + m2

I (λ))

= |m0|2∂λmR − mR∂λ|m0|2 + O(ε3),

(cid:0)(∂λa(λ))2 + (∂λmI )2(cid:1)(cid:105)

+ O(ε3)

ε4∂λmR(λ)2 + 4 ε2 m2
I

= 4ε2m2

I (λ)|∂λm0(λ)|2 + O(ε3).

The result (9) follows by plugging Eqs. (42) and (43) into Eq. (6) and then set ε = 0.

Mean squared overlap for deformed GOE matrices

The derivation of the overlaps (8) for two independent deformed GOEs is very similar to sample covariance matrices.
Hence, we shall omit most details that can be obtained by following the arguments of the above Appendix. Again,

we shall skip the arguments λ and(cid:101)λ where there is no confusion.
For completeness, we recall some notations. We deﬁned g and(cid:101)g are respectively the Stieltjes transform of S = C+W
and(cid:101)S = C +(cid:102)W where the noises are independent. Also, we introduced
(cid:101)ξ((cid:101)z) =(cid:101)z −(cid:101)σ2g((cid:101)z),
where σ2 can be diﬀerent from (cid:101)σ2. We still use the convention ξ0(λ) = limη↓0 ξ(λ − iη) ≡ ξR + iξI and (cid:101)ξ0(λ) =
limη↓0 ξ((cid:101)λ − iη) ≡(cid:101)ξR + i(cid:101)ξI . It is easy to see that ξR = λ − σ2gR and ξI = −σ2gI .

For deformed GOE, the resolvent can also be approximated by a deterministic matrix for N → ∞. Indeed, one has

ξ(z) = z − σ2g(z),

(44)

[22, 26]:

and(cid:10)((cid:101)z −(cid:101)S)−1

kl

(cid:11)

(cid:17)−1

ξ(z) − C

(cid:11)

(cid:10)(z − S)−1

P ∼(cid:16)
(cid:2)ψ(λ − iη,(cid:101)λ + iη) − ψ(λ − iη,(cid:101)λ − iη)(cid:3) =

kl

lim
η→0

P is obtained from Eq. (45) by replacing ξ by (cid:101)ξ. Then, plugging this into (2), we obtain,

kl

− g0 −(cid:101)g0
g0 −(cid:101)g0
(cid:101)ξ − ξ
(cid:101)ξ − ξ
(cid:16)(cid:101)ξ −(cid:101)ξ
(cid:17)
(cid:16)(cid:101)g0 −(cid:101)g0
(cid:17)
(cid:16)(cid:101)ξ − ξ
(cid:17)(cid:16)(cid:101)ξ − ξ

+ ξ

g0

=

+ g0(cid:101)ξ −(cid:101)g0(cid:101)ξ
(cid:17)

(45)

(46)

Hence, by putting these last two equations into (46) and then using (3), we get after some straightforward computations

We proceed as above (see Eq. (33) and thereafter) to ﬁnd

(cid:16)(cid:101)ξ −(cid:101)ξ
(cid:17)

g0

and

+ ξ

I − ξ2

= (ξR −(cid:101)ξR)2 + ((cid:101)ξ2

(cid:16)(cid:101)g0 −(cid:101)g0
(cid:17)
+ g0(cid:101)ξ −(cid:101)g0(cid:101)ξ = 2(cid:2)(ξI(cid:101)gI − gI(cid:101)ξI ) + i(cid:0)(cid:101)ξI (gR −(cid:101)gR) −(cid:101)gI (ξR −(cid:101)ξR)(cid:1)(cid:3)
(cid:16)(cid:101)ξ − ξ
(cid:17)
(cid:17)(cid:16)(cid:101)ξ − ξ
I ) + 2iξI ((cid:101)ξR − ξR).
(σ2 +(cid:101)σ2)(ξR −(cid:101)ξR)2 + 2σ2(cid:101)σ2(gR −(cid:101)gR)(ξR −(cid:101)ξR) − (σ2 −(cid:101)σ2)(ξ2
(cid:2)(ξR −(cid:101)ξR)2 + (ξI +(cid:101)ξI )2(cid:3)(cid:2)(ξR −(cid:101)ξR)2 + (ξI −(cid:101)ξI )2(cid:3)
(ξR −(cid:101)ξR)2 + σ2(gR −(cid:101)gR)(ξR −(cid:101)ξR)
2σ2(λ −(cid:101)λ)(ξR(λ) − ξ((cid:101)λ))

(cid:2)(ξR(λ) − ξR((cid:101)λ))2 + (ξI (λ) + ξI ((cid:101)λ))2(cid:3)(cid:2)(ξR(λ) − ξR((cid:101)λ))2 + (ξI (λ) − ξI ((cid:101)λ))2(cid:3) ,
(cid:2)(ξR(λ) − ξR((cid:101)λ))2 + (ξI (λ) + ξI ((cid:101)λ))2(cid:3)(cid:2)(ξR(λ) − ξR((cid:101)λ))2 + (ξI (λ) − ξI ((cid:101)λ))2(cid:3) ,

I −(cid:101)ξ2

I )

,

Φa(λ,(cid:101)λ) =

Φa(λ,(cid:101)λ) = 2σ2

which is the generalization of Eq. (8) for two diﬀerent noise parameters σ (cid:54)=(cid:101)σ. If we now consider σ =(cid:101)σ, we have

=

that is exactly Eq. (8). As for sample covariance matrices, one can again set(cid:101)λ = λ + ε to ﬁnd

9

(47)

(48)

(49)

(50)

(51)

and this is exactly (10). Now, if we consider (cid:101)σ2 = 0 (no noise), it implies that (cid:101)mI = 0 and (cid:101)mR = µ. Then, one can

2ξ2
I

Φa(λ, λ) =

easily check that this yields in Eq. (49):

(cid:0)[∂λξR(λ)]2 + [∂λξI (λ)]2(cid:1) ,

σ2∂λξR(λ)

which is exactly the result derived in [18, 23, 24].

Φa(λ, µ) =

σ2

(ξR − µ)2 + ξ2

,

I

The case of correlated Gaussian additive noises

We deﬁne

(cid:101)S = C + W2,

S = C + W1,

where W1, W2 are two correlated GOE matrices (independent from C) satisfying

(52)

(53)

(cid:19)

(cid:18) σ2

1 ρ12
ρ12 σ2
2

(cid:104)W1(cid:105)N = 0,

(cid:104)W2(cid:105)N = 0, Cov(W1, W2) =

(54)
where we denoted by N the Gaussian measure and used the abbreviation ρ12 = ρσ1σ2. Using the stability of GOE
under addition, let us rewrite the noise terms as

,

where A that satisﬁes

W1 = A + B1,
W2 = A + B2,

and B1 and B2 are two GOEs matrices independent from A with

(cid:104)A(cid:105)N = 0,

(cid:104)A2(cid:105)N = ρ12,

(cid:104)B1(cid:105)N = 0,

(cid:104)B2
(cid:104)B1B2(cid:105)N = 0.

1(cid:105)N = σ2

(cid:104)B2(cid:105)N = 0,
(cid:104)B2

1 − ρ12,

2(cid:105)N = σ2

2 − ρ12,

(55)

(56)

(57)

One can check that this parametrization yields exactly the correlation structure of Eq. (54). Therefore, using (55)
into (53), we have the equivalence (in law)

10

(cid:101)S = D + B2,
1 − ρ12, and (cid:101)σ2 = σ2

S1 = D + B1,

D ..= C + A.

(58)

Since the noises are now independent and that the mean squared overlap Φa, given in Eq. (49), is “independent”
from the exact structure of C, we can therefore replace C by D. Hence, we deduce that the overlaps for this model
will again be given by Eq. (49) with σ2 = σ2
variance σ2(1 − ρ), as announced.

2 − ρ12. If σ =(cid:101)σ, then, Φa is given by Eq. (8) with

