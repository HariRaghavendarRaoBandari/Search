Sparse model selection in the highly under-sampled regime

Nicola Bulso∗

The Kavli Institute and Centre for Neural Computation, Trondheim, Norway and

The Abdus Salam International Centre for Theoretical Physics (ICTP), Trieste, Italy

The Abdus Salam International Centre for Theoretical Physics (ICTP), Trieste, Italy

Matteo Marsili†

Yasser Roudi‡

The Kavli Institute and Centre for Neural Computation, Trondheim, Norway

Institute for Advanced Study, Princeton, USA and

The Abdus Salam International Centre for Theoretical Physics (ICTP), Trieste, Italy

Abstract

We propose a method for recovering the structure of a sparse undirected graphical model when

very few samples are available. The method decides about the presence or absence of bonds between

pairs of variable by considering one pair at a time and using a closed form formula, analytically

derived by calculating the posterior probability for every possible model explaining a two body

system using Jeﬀrey’s prior. The approach does not rely on the optimization of any cost functions

and consequently is much faster than existing algorithms. Despite this time and computational

advantage, numerical results show that for several sparse topologies the algorithm is comparable

to the best existing algorithms, and is more accurate in the presence of hidden variables. We apply

this approach to the analysis of US stock market data and to neural data, in order to show its

eﬃciency in recovering robust statistical dependencies in real data with non-stationary correlations

in time and/or space.

6
1
0
2

 
r
a

M
3

 

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
2
5
9
0
0

.

3
0
6
1
:
v
i
X
r
a

∗ nicola.bulso@ntnu.no
† marsili@ictp.it
‡ yasser.roudi@ntnu.no

1

I.

INTRODUCTION

Natural images and sounds, along with many other signals in the world admit sparse

representations: models with only a small number of non-zero parameters suﬃce to uniquely

identify the signal. Similarly, many real networks, such as neuronal and gene regulatory

networks are sparse that is each node connects to a relatively small fraction of all possible

nodes. In the typical situation, in these cases, the dimensionality of the data (e.g. number

of neurons or genes) exceeds by far the number of available observations, and even when

many samples are available one is hardly ever in the situation where one can regard them as

being drawn independently from the same distribution. In addition, typically only a fraction

of the relevant variables are sampled and the observed behaviour is potentially inﬂuenced

by an unknown number of hidden variables. In these cases, typically, any inferred model is

far from being an accurate description of the data generating process. Models that are very

complex aﬀord a predictive power that exceeds what can be validated statistically, whereas

simpler (sparse) models are more likely to perform well out of sample.

Understanding the theoretical foundations of sparse signals and network recovery from a

small set of high-dimensional observations has thus become an important area of research

in the past few years. This theory is not only of important practical use, but it also is

likely to shine light on fundamental aspects of biological information processing. Biological

systems in their environment usually face the same problem that scientists do when trying

to understand a complex system: identifying signals and relationships between them from

limited noisy measurements and as fast as possible.

In recent years, approaches based on optimizing cost functions that favor sparse repre-

sentations have been shown to be very successful for sparse recovery. From earlier work on

Lasso [1] to more recent ones on compressed sensing [2], a large body of research has shown

the asymptotic power of the sparsity prior in the exact or almost exact recovery of signals

from a small sample of observations.

In network reconstruction, the standard platform for studying network recovery is that

of an Ising model, or a pairwise Markov Random Field as is known in the machine Learning

literature. This takes the form of a distribution

P ((cid:126)S|J, h) =

exp

(1)

(cid:111)

i hiSi

(cid:110)(cid:80)

i<j JijSiSj +(cid:80)

Z

2

over n binary spin variables Si = ±1, where the partition function Z ensures normalisation.
Given a sample ˆS of N observations of the vector (cid:126)S = (S1, . . . , Sn) of spin variables, the
general problem is that of ﬁnding the interactions Jij and ﬁelds hi that best describe the
data. In the case where the data is generated as independent random draws from P (S|J, h)
in Eq. 1, with unknown Jij and hi, the problem is that of recovering these parameters.

In the sparse regime, only a small number of the couplings are non-zero, so the problem

amounts to recovering the network of interactions and then estimating the values of the non-
zero parameters. When ˆS comes from a real experiment or from observation of a complex

system, Eq. 1 becomes a tool that can be used to infer a putative network of statistical

dependencies among the variables Si.

In the general setting where sparsity is not necessarily imposed, starting from the work on

“Boltzmann machine learning” [3], many approximations for inferring the model parameters

have been proposed and studied [4–7]. In the sparse case an important contribution was

made by Ravikumar et al [8], who showed that in the high-dimensional regime, deﬁned as

the asymptotic regime where the size of the system is larger than the number of samples

(while both go to inﬁnity), exact recovery of the set of non-zero bonds is possible via the

optimization of the (cid:96)1 regularized pseudo-likelihood function. In the following we denote

this method by PLM+(cid:96)1.

Although asymptotic high-dimensional analysis ensures the reconstruction of the connec-

tivity pattern by an (cid:96)1 regularized logistic regression, a more recent approach [9] was shown

to outperform the (cid:96)1 regularized approach in certain regimes with ﬁnite datasets. This al-

gorithm by Decelle and Ricci-Tersenghi [9] does not require the selection of a regularization

parameter. Typically, the regularization parameter can be chosen by cross-validation, but

this is not a straightforward task when very few data points are available.

Both the algorithms by Ravikumar et al [8] and Decelle and Ricci-Tersenghi [9] rely on

optimization of a prescribed cost function.

In this work we study the problem of sparse network recovery from a Bayesian model

selection perspective. In brief, network recovery amounts to ﬁnding which model best de-

scribes the data. Yet the number of possible pairwise graphical models grows exponentially

with the (square of the) dimensionality n, which makes Bayesian model selection unfeasible

in practice. However, Bayesian model selection penalises models for their complexity, which

grows with the number of parameters they depend on. This penalty is particularly severe

3

when the number N of samples is not very large – which we will call the under-sampling

regime in what follows. In this regime, sparse networks are the most likely representation

because they are simpler. Indeed, in the deep under-sampling regime, the most likely models

are those where the network of interactions is so sparse that spins hardly interact. In this

regime, it makes sense to consider each pair of spins as if they were independent of all other

spins, and to decide about the presence or absence of a bond between them independently
from all others. This reduces the problem to n(n− 1)/2 simpler problems of Bayesian model
selection for systems of two spins, that we treat in full detail. In the under-sampling regime,

the choice of the prior over the parameters becomes relevant, which is why we use Jeﬀrey’s

priors, that are known to induce the same (uniform) distribution in the space of samples,

regardless of the model [10, 11]. If the recovered network contains no loops, we expect this to

be an accurate reconstruction whereas if it contains loops or cliques, it is expected to over-

estimate the number of bonds. In the weakly interacting regime and for sparse topologies

admitting few loops, we ﬁnd that the method is able to classify relevant features predicting

the probability of having a direct connection between a pair of nodes in a network. In the

under-sampling regime this task is performed quite well and with the same accuracy of more

complex optimisation based methods. This makes it possible to save computational time,

thus facilitating large scale network applications. Moreover, our method has been found

particularly successful in revealing the absence of a direct connection between two spins,

becoming more and more accurate as the number of samples grows. Therefore, our method

can also be used as a pre-processing step to prune the set of possible interactions, before

applying more complex inference methods. These results are supported by simulations and

discussed in detail in Sec. III.

On the computational side, our approach is remarkably fast, as it entails solving a two

spin problem and building a look-up table, using which the decision of which bond is present

or not can be taken. We compare the results of this approach to the PLM+(cid:96)1 on synthetic

data from systems with known topology. Our numerical results demonstrate that, when all

the variables on the nodes of the graph are observed PLM+(cid:96)1 oﬀers a superior performance,

though the results of our method still aﬀord a good performance and are computationally

advantageous. For data on partially observed graphs, where the data only accounts for a

fraction of the variables while the remaining ones act as hidden nodes (unknown unknowns),

the procedure proposed here is both computationally advantageous and it has superior

4

performance, specially in the limit of few samples.

This approach is particularly suited to study real data where the number of available

samples is necessarily limited. For example, in the study of non-stationary data or in the

presence of an external relevant variable. In these cases, taking a too large time window

or neglecting the eﬀect of the external variable induces correlations that result in eﬀective

interactions that do not reﬂect genuine statistical dependence. This eﬀect can be controlled

by performing inference on small time windows or by partitioning the data conditioning on

the external variable. This leads to inference problems where the number of samples is very

small, for which our method is ideally suited. In this situation, a model selection approach

is necessary in order to correctly evince how much structure can be inferred from the data.

Hence the MS approach is a valuable alternative to other approaches [12, 13] to inference in

the presence of hidden variables. We illustrate these points for the speciﬁc cases of a dataset

of daily returns of n = 41 stocks from US stocks market and a dataset on the neural activity

of cells in the entorhinal cortex of a moving rat (the same data studied in Refs. [14] and [15]

respectively). At odds with other methods that need longer samples, our approach is able to

spot the non-stationary nature of the ﬁnancial data by focusing on small time windows, and

in eliminating spurious correlations arising from co-variation in the neural data by focusing

on small cells.

The paper is organized as follows. In the ﬁrst section, we describe the model selection

approach for topology reconstruction in a network of interacting variables. Then we focus

on two-spins clusters, deriving the key equations of Bayesian model selection which lie at

the heart of our method. The third section is aimed at exhibiting results with synthetic

data drawn from an equilibrium Ising distribution and comparing them with those obtained

with a pseudo-likelihood method. Finally we discuss the application of our method to real

data.

II. A MODEL SELECTION APPROACH TO TOPOLOGY RECONSTRUCTION

Suppose we have N observations ˆS = ((cid:126)S1, (cid:126)S2, ..., (cid:126)SN ) of a vector (cid:126)S = (S1, S2, ..., Sn) of
n variables Si. We think of (cid:126)S as a conﬁguration of a graphical model, i.e. a model of n
variables whose interaction is deﬁned by a network between the n variable nodes [16]. Let
us consider a collection of diﬀerent mathematical models, Mi each with a possibly diﬀerent

5

set of connections. Each Mi is a possible hypothesis to explain the data and the probability
P (Mi| ˆS) of Mi given the data, is related to the likelihood P ( ˆS|Mi) that the model Mi
has generated the data ˆS, through Bayes theorem:

P (Mi| ˆS) =

(cid:80)
P ( ˆS|Mi)P0(Mi)
j P ( ˆS|Mj)P0(Mj)

.

(2)

Here P0(Mi) is the prior probability of model Mi. Each model employs a diﬀerent vector
of parameters (cid:126)θ whose length and properties depend on the particular model Mi. The a
priori knowledge on the value of (cid:126)θ is encoded in the prior distribution P ((cid:126)θ|Mi). Hence, the
likelihood P ( ˆS|Mi) can be written as

P ( ˆS|Mi) =

d(cid:126)θP ( ˆS|(cid:126)θ,Mi)P ((cid:126)θ|Mi)

(3)

(cid:90)

where P ( ˆS|(cid:126)θ,Mi) is the conditional probability of observing the data ˆS given a particular
choice of the parameters (cid:126)θ of model Mi.

We focus on systems of binary variables, or Ising spins, where each variable takes values

Si = ±1, and on models in the exponential family, i.e.

P ((cid:126)S|(cid:126)θ,Mi) =

e

,

(4)

(cid:80)Θ
j=1 fj ((cid:126)S)θj
Z((cid:126)θ)

where the number of parameters Θ depends on the particular model Mi, fj((cid:126)S) are co-
eﬃcients multiplying the parameters and depending on spins (cid:126)S and, ﬁnally, Z((cid:126)θ) is the
partition function. Given this and a set of independent observations ˆS = ((cid:126)S1, (cid:126)S2, ..., (cid:126)SN ),

the probability of observing the collected data given a particular choice of the parameters

within model Mi is P ( ˆS|(cid:126)θ,Mi) =(cid:81)

µ P ((cid:126)Sµ|(cid:126)θ,Mi) with µ = 1, 2, ..., N or

P ( ˆS|(cid:126)θ,Mi) =

(cid:88)

µ

φj =

1
N

eN (cid:126)φ·(cid:126)θ
Z N ((cid:126)θ)

fj((cid:126)Sµ).

(5)

(6)

j φjθj The probability of observing the data under model Mi can then be

where

and (cid:126)φ · (cid:126)θ = (cid:80)

written as

P ( ˆS|Mi) =

(cid:90)

d(cid:126)θ eN (cid:126)φ·(cid:126)θ−N log Z((cid:126)θ)P ((cid:126)θ|Mi).

(7)

6

Our problem is to choose from these models by calculating P (Mi| ˆS) through Eq. 2.
The prior on models P0(Mi) can account for any prior belief regarding the structure of the
network, for instance the degree of sparsity.

For large N , the integral in Eq. 7 is dominated by the maximum of (cid:126)φ · (cid:126)θ − log Z((cid:126)θ) and
it can be evaluated by the saddle point method (see e.g.
[11]). This produces a leading
term in log P ( ˆS|Mi) that is proportional to N , which is given by the likelihood evaluated
at the saddle point (cid:126)θ∗. Besides this, a term proportional to Θ
2 log N also arises from the
Gaussian integration over the Θ parameters. This term, which penalises models with many
(Θ (cid:29) 1) parameters, is the basis of the Bayesian Information Criterium (BIC) [17]. Finally,
further constant terms, that depend on the choice of the prior P ((cid:126)θ|Mi) also appear. In the
under-sampling regime, when N is not very large, all these terms become important, and as

N increases one expects the most likely model to become more and more complex.

A direct and exact application of the above scheme for deriving the network would require
ranking the evidence P (Mi| ˆS) for all possible network models Mi, that number to at least
2n(n+1)/2 possible models. Apart from the complications related to calculating the partition

function for a single evaluation of Eq. 9, this exponentially large number of graphs makes it

impossible to work out the above approach based on the model posterior.

Yet, for many real life applications, and in the under-sampling regime, we expect the most

likely models to correspond to sparse graphs composed of many disconnected components.
In this case, P ((cid:126)S|(cid:126)θ,Mi) becomes a product of factors corresponding to each component,
which are easier to handle. In the extreme limit where components are formed of isolated

spins or of dimers of interacting spins, calculations can be done by considering minimal

clusters of only two spins which are taken to interact with the rest of the network through

some eﬀective ﬁelds. As we will show, this approach proves to be eﬀective when there are

few data points, performing as well as (cid:96)1 regularized pseudo-likelihood methods. Given

the simplicity of the minimal cluster of two spins, we are able to perform all calculations

analytically and rank the models. Last but not least an important advantage of such an
approach is that it can be parallelized up to n(n − 1)/2 times since all pairs are assumed to
be independent of each other and can be analyzed in parallel.

7

FIG. 1. The ten competiting models for a two spins system. The simplest one (top left of the

ﬁgure) has no parameters: the two spins are independent of each other (dotted line) and from the

rest of the network; the most complex model (bottom right) is the one with 3 parameters: the two

spins are linked by a direct connection J (solid line) and inﬂuenced by eﬀective ﬁelds h1 and h2

(spots on the edges) representing external biases and other spins’ inﬂuence. Models on the left do

not employ a direct connection between spins whereas the other ﬁve models on the right do. The

parameter h is used to depict situations in which both spins are conditioned by the same bias.

A. The minimal cluster

A two spin system has at most three parameters: two ﬁelds and one coupling. The

number of possible models for two spins is thus 10, as shown in Fig. 1. The models exhibit

a growing degree of complexity with parameters representing ﬁelds acting on each spin (h,

h1 and h2) and a direct connection between them (J), increasing the number of parameters

from Θ = 0 (top left model in the ﬁgure) where the spins are thought as two independent

spins without any ﬁeld, to Θ = 3 (bottom right) for spins connected by a bond and being
aﬀected by two diﬀerent eﬀective ﬁelds. The vector (cid:126)φ in Eq. 5 is made up of combinations

of the empirical mean values of the activity of each spin (m1 and m2) and the empirical

correlation c12.

As for the prior P ((cid:126)θ|Mi), many choices can be made, but a natural choice is derived
from counting the number of distinguishable probability distributions embedded in an in-

ﬁnitesimal volume of a parametric manifold in the space of all distributions. Assuming that

8

Mi Θ

M1 0
M2 1
M3 1
M4 1
M5 2
M6 1
M7 2
M8 2
M9 2

(cid:126)θ

-

h1

h2

h

(cid:126)φ

-

m1

m2

m1+m2

Z((cid:126)θ)

4

4 cosh(h1)

4 cosh(h2)

4 cosh2(h)

(h1,h2)

(m1,m2)

4 cosh(h1) cosh(h2)

J

c12

4 cosh(J)

(h1,J)

(m1,c12)

4 cosh(h1) cosh(J)

(h2,J)

(m2,c12)

4 cosh(h2) cosh(J)

(h,J)

(m1+m2,c12)

4 cosh2(h) cosh(J) +

+ 4 sinh2(h) sinh(J)

det J((cid:126)θ) N δ

((cid:126)θ)

1

( 4
Z )2

( 4
Z )2

8
Z

( 4
Z )2

( 4
Z )2

( 4
Z )2

( 4
Z )2

27eJ
Z3

-

π

π
√

1

1

1

2π 1
2

π2

π

π2

π2

1

1

1

1

2π 3
2

2 log 2

2 log 2

2 log 2

3
2 log 2

2 log 2

2 log 2

2 log 2

2 log 2

7

2 log 2+ J

2

M10 3 (h1,h2,J) (m1,m2,c12) 4 cosh(h1) cosh(h2) cosh(J) + ( 4

Z )4

π2

2

4 log 2

+ 4 sinh(h1) sinh(h2) sinh(J)

TABLE I. The ten competing models for a two spin system

the metric in this space is represented by the Fisher Information, P ((cid:126)θ|Mi) turns out to be
equal to the so called Jeﬀrey’s prior [11, 18]:

(cid:113)
(cid:113)
expression Jij((cid:126)θ) = ∂θi∂θj log Z((cid:126)θ) and N = (cid:82) d(cid:126)θ

P ((cid:126)θ|Mi) = N −1

where J is the Fisher Information matrix whose components are given by the following
det J((cid:126)θ). This choice of the prior is
appealing not only because it assumes a ﬂat distribution on the space of samples, but also

det J((cid:126)θ)

because it corresponds to the same information theoretic assumption for all models. This

avoids introducing unintended biases among the models. Using the Jeﬀrey’s prior, the
probability of the observed data under model Mi can be written as

(8)

(9)

(cid:90)

(cid:113)

P ( ˆS|Mi) = N −1

d(cid:126)θ

eN (cid:126)φ·(cid:126)θ
Z N ((cid:126)θ)

det J((cid:126)θ).

The heart of the problem is then solving Eq. 9 for each model, and ranking the result-

ing probabilities in order to identify which model is the most likely in each region of the

(m1, m2, c12) space which is what we will do in the following.

We ﬁrst note that the determinant of the Fisher Information matrix is related to the

partition function in the following way (see Tab. I)

9

log det J((cid:126)θ) = −δ log Z((cid:126)θ) + ((cid:126)θ).

1
2

(10)

The values of the coeﬃcients δ and ((cid:126)θ) depend on the particular model as shown in Tab.
I: δ is a constant while epsilon is at most linear in (cid:126)θ. Consequently, performing the saddle

point approximation and the Gaussian integral around the saddle, the integral in Eq. (9)

becomes

(cid:90)

(cid:115)

(cid:18)

(cid:19)

d(cid:126)θ eN Ψ((cid:126)θ) ∼

(2π)Θ

N Θ| det H((cid:126)θ(cid:63))|eN Ψ((cid:126)θ(cid:63))

Ψ((cid:126)θ) = (cid:126)φ · (cid:126)θ −

1 +

δ
N

log Z((cid:126)θ) +

((cid:126)θ)
N

where the argument of the exponential is

(11)

(12)

and Hij((cid:126)θ(cid:63)) = ∂θi∂θj Ψ((cid:126)θ)|(cid:126)θ=(cid:126)θ(cid:63) is the Hessian matrix calculated at (cid:126)θ(cid:63) which is the solution of
the saddle point equations

(cid:18)

(cid:19)

φi −

1 +

δ
N

∂θi log Z((cid:126)θ(cid:63)) +

∂θi((cid:126)θ(cid:63))

N

= 0.

(13)

Using Eq. 12, it is not hard to see that the determinant of the Hessian matrix is related

to the determinant of the Fisher Information at the saddle point by the simple relation

det H((cid:126)θ(cid:63)) = −(cid:0)1 + δ

(cid:1)Θ det J((cid:126)θ(cid:63)).

N

P ( ˆS|Mi) as

P ( ˆS|Mi) =

(cid:115)

Putting all the previous results together, we can estimate the conditional probability

N 2N Θ(cid:0)1 + δ

(2π)Θ

N

(cid:1)Θ

eN (cid:126)φ·(cid:126)θ(cid:63)
Z N ((cid:126)θ(cid:63))

.

(14)

As anticipated, the resulting expression in Eq. 14 is made up of two terms: the maximum

likelihood one which counts for the goodness of ﬁt, and a complexity cost which depends on

the dimensionality of the models Θ and on the shape of the prior

Complexity Cost = e−C
C (cid:39) Θ
2

N
2π

+ log

log

(cid:90)

(cid:113)

d(cid:126)θ

det J((cid:126)θ) + O

(cid:18) 1

(cid:19)

N

(15)

Similar to the Bayesian Information Criterion [17],this complexity cost consists of a term

proportional to log N that arises from the dimensionality of the model, a second term ac-

counting for the geometric complexity pertaining to the number of distinguishable probabil-

ity distribution encoded in a parametric family distribution and a third term O(1/N ) called

10

relative complexity [11]. As one can notice from the results obtained so far, the correc-

tions related to the complexity term are O(log N ) and consequently they are evident when
log N/N is not negligible. For N → ∞ one recovers MLE of a two spins system [4].

B. The selected model

As described in the previous section, the probability P ( ˆS|Mi) is fully determined, through
the equations Eq. 14 from the measured statistics {m1, m2, c12}. In this section we discuss
how the space of observations {m1, m2, c12} is partitioned among the models: for each point
in this space, we rank the posterior probability of each model to see which is the most

likely model. It is worth noticing that not all the points in this space are achievable in the
limit N → ∞, therefore we will discard un-physical points lying outside the tetrahedron
idenitiﬁed by the following double inequality: −1 + |m1 + m2| ≤ c12 ≤ 1 − |m1 − m2|.

In Fig. 2 and Fig. 3 the division of the parameters space is sketched for N = 50. Fig.
2 illustrates the parts of the {m1, m2, c12} space where the ﬁrst ﬁve models in Tab. I are
the most likely according to P (Mi| ˆS) for a data length of N = 50. These ﬁve models are
the ones in which the two spins are not directly connected. The region of the space covered

by the union of these ﬁve models, shown in the last picture with diﬀerent colors for each
subregion, lies around the surface c12 = m1m2. In the limit N → ∞, as one expects, the
union of these ﬁve models will become identical to the surface c12 = m1m2.

The last ﬁve models in Tab. I describe models in which the spins do interact with each
other. Fig. 3 shows the regions in the {m1, m2, c12} space where these models are preferred
over the ﬁrst (disconnected spin) models.

In Fig. 4 we show the partitioning of the parameter space among the models for the case

N = 500: left panel of Fig. 4 summarizes the case of models with no interactions between the

spins and the right one represents models with interactions. One can see that for large N ,

models with less parameters extend over smaller regions. In this case, the regions belonging

to the ﬁrst ﬁve models shrink around the surface cij = m1m2 and more complex models
(M5 among the ﬁrst ﬁve models and M10 among the rest of them) occupy more volume. In
fact, as remarked in the previous section, the penalty arising from the complexity of models

is greater for smaller samples, and for very large values of N , the maximum likelihood

estimates become predominant and models with more non-zero parameters are preferred.

11

FIG. 2. The ﬁgures show the regions of the space {m1, m2, c12} where the ﬁrst ﬁve models in Tab.
I are the most likely according to P (Mi| ˆS) given N = 50 measurements. The region of the space
covered by all ﬁve models is shown in the last ﬁgures, with diﬀerent colors for diﬀerent models, and
it encloses the surface c12 = m1m2 which represents the limit of independent spins for N → ∞

.

12

FIG. 3. The ﬁgures show the regions of the space {m1, m2, c12} where the last ﬁve models in Tab.
I are the most likely according to P (Mi| ˆS) given N = 50 measurements. All these models employ
the parameter J meaning that the spins are thought to be directly interacting here in contrast
with the ﬁrst ﬁve models. The last ﬁgure summarizes how the {m1, m2, c12} is partitioned among
these models (Model M10 has not been included in the last ﬁgure for reasons of clarity).

13

FIG. 4. Similarly to the previous ﬁgures for N = 50, these ﬁgures show the partitioning of
the space of observations {m1, m2, c12} among models without a parameter for direct interaction
between spins (left) and models with it (right) for N = 500.

From what we have said so far, one can calculate the conditional probability P (b| ˆS)
that the two spins are directly interacting. If we assume a priori all models have the same
probability, it follows from Eq. 2 that P (b| ˆS) is proportional to the sum of P ( ˆS|Mi) over
all models Mi in which there is a bond between the pair of spins, that is the last 5 models
in the Tab. I

P (b| ˆS) = Γ−1

P ( ˆS|Mi).

(16)

10(cid:88)

i=6

10(cid:88)

i=1

Γ is given by

P ( ˆS|Mi)

Γ =

and ensures normalization P (b| ˆS)+P (nb| ˆS) = 1, with P (nb| ˆS) = Γ−1(cid:80)5

(17)
i=1 P ( ˆS|Mi) denot-
ing the probability of no bonds (nb). It follows that if the quantity η = P (b| ˆS) − P (nb| ˆS),
which we will refer to as the conﬁdence in the following, is greater than or equal to zero, we
put a bond between the spins, otherwise we don’t 1. Accordingly the space {m1, m2, c12}
becomes divided into two regions: one in which the probability of not having a bond is

bigger and one in which the opposite is true. This subdivision is shown in Fig. 5 for two

data set of diﬀerent length: N = 50 and N = 500. The no-bond region contains the surface

c12 = m1m2 with a thickness that decreases as N increases.

1 Notice that conﬁdence is deﬁned as the diﬀerence between the probability of having a bond and the

probability of not having it when a uniform prior on models is assumed.

14

FIG. 5. Portion of the space of observations in which the probability of not having a bond is

greater than the one of having a bond for N = 50 (left) and N = 500 (right).

This procedure for the identiﬁcation of the bond will be performed for each pair of spins

in a network independently in order to recover its topology.

C. A self consistent procedure for selecting the sparsity priors

The assumption that the models are all a priori equally likely that we made in the last

section can be easily relaxed in order to exploit additional knowledge or beliefs on the
degree of sparsity of the network. For instance, we can assume that P0(Mi) = P0(b) for all
models that employ a bond (i = 6, . . . , 10) and P0(Mi) = P0(nb) = 1 − P0(b) for the rest
(i = 1, . . . , 5). The ratio  = P0(b)/P0(nb) is the only parameter to choose. The value of

 reﬂects the a priori belief about the sparsity of the network and it is often compared in

the text with the actual ratio between the number of ’bonds’ and the number of ’no-bonds’

in the graph, i.e. r = nb/nnb. The sparsity of the network is usually deﬁned as the ratio

betwen the number of bonds and the total number of possible connections which is equal
to r/(1 + r). However, for sparse matrices r (cid:28) 1, the ‘bond’-‘no-bond’ ratio r approximates
very well the sparsity of the network ∼ r + O(r2). With the introduction of the above priors,
the quantity P (b| ˆS) − P (nb| ˆS), now called ˜η, takes the following expression

(cid:19)

(cid:18)

η − 1 − 

1 + 

P (b| ˆS) − P (nb| ˆS) =

Γ
2¯Γ

15

(18)

where η is the conﬁdence, Γ has been deﬁned in the previous section and

10(cid:88)



¯Γ =

1 + 

i=6

P ( ˆS|Mi) +

1

P ( ˆS|Mi).

(19)

5(cid:88)

1 + 

i=1

Therefore P (b| ˆS) − P (nb| ˆS) ≥ 0 if and only if η ≥ (1 − )/(1 + ). This means that adding
a prior of that kind basically implies a non-zero threshold for the quantity η (for  = 1, the

case of a ﬂat prior is indeed retrieved).

An interesting procedure for deciding about the choice of  is a self consistent one: one

choses  such that after the graph recovery is performed, the ratio of the present to absent

bonds in the recovered graph also equals , that is (nb/nnb = P0(b)/P0(nb)). A non-trivial

self consistent choice for  may not exist2, but as we have seen in our numerical simulations,

it often does. When it exists, it can be reached as the ﬁxed point of an iterative procedure

where at step t = 0, 1, . . ., given t, one computes rt and sets t+1 = rt for the next iteration.

In all cases we studied, either convergence to a ﬁxed point  = r was chosen, regardless of

the ﬁrst guess 0, or a completely disconnected graph retrieved ( = 0). In the latter case,

when the point r =  = 0 becomes stable, a ﬁxed prior could be used to check the result.

III. NUMERICAL RESULTS FOR NETWORK RECOVERY

To assess the performance of the Model Selection (MS) approach described in the pre-

ceding sections in recovering a network topology, we applied it to synthetic data from an

equilibrium Ising model with diﬀerent connectivity patterns and analyzed the goodness of

reconstruction. In doing so, we also studied the eﬀect of diﬀerent parameters on the per-

formance of the MS approach, such as the strength of couplings, the number of samples,

the prior belief  on the sparsity and the size of the network. In addition, we compared

the results with the ones obtained with PLM+(cid:96)1, which is among the best algorithms for

recovering the connectivity. Unfortunately we found that the approach proposed in [9] was

not stable and convergent in the very low data limit that we were interested in, and therefore

we do not discuss it below.

The graphs we investigated are:

1. Gas of dimers in which each node is connected to one and only one other node,

2  = 0 or  = +∞ are always trivially self-consistent.

16

2. Star graph

3. Erd¨os R´enyi graphs with the average degree c = 2

4. Erd¨os R´enyi graphs with the average degree c = 3

5. two dimensional (2D) regular grid connectivity.

6. diluted two dimensional regular grid

Across these graphs the bond-to-no-bond ratio r ranges from 0.01 to 0.06 (for n = 64),

we also studied network sizes ranging from n = 16 to 100 nodes and sample sizes from
N = 50 to 2000. The couplings are drawn from a bimodal distribution J = ±β, but we also
investigated the ferromagnetic case (J = β), for diﬀerent values of β.

For the sake of brevity, we discuss the results highlighting few representative cases, while

referring to the Appendix. We shall contrast the case of fully observed graphs, where all the

variables on the nodes of the graph are observed, to that of partially observed ones, where

the data only accounts for a fraction of the variables while the remaining ones act as hidden

nodes or unknown unknowns.

A. Fully observed graphs

Fig. 6 summarizes the results for the representative case of the Erd¨os R´enyi graph with

c = 3, for diﬀerent samples sizes N , strength of the couplings β and diﬀerent choices of the

prior sparsity . The coordinates of each point in the ﬁgure represent the average values

over one hundred independent realisations of true negative rate (T N R) and the true positive

rate (T P R) for a given value of N ,  and β, where TPR (TNR) stands for the fraction of

bonds (no-bonds) correctly recovered.

A perfect inference of the graph corresponds to a point on the top right corner of the

graph. For graphs with very few bonds and no loops (e.g. a gas of dimers) MS achieves

almost perfect recovery (see Appendix). Fig. 6 shows instead a typical case where MS is

expected to provide only an approximate reconstruction. MS network recovery works best

for weak interactions (β = 0.5), because when interaction gets strong (e.g. β = 1.5) the

eﬀects of indirect interactions, that are neglected by MS, become important. Interestingly,

in the case of a gas of dimers we see the opposite, i.e. reconstruction improves when the

17

FIG. 6. Testing MS method on synthetic data for an Erd¨os R´enyi graphs with average degree c = 3.

The coordinates of each point in the ﬁgure represents the average over one hundred realisations

of the true negative rate (T N R) and the true positive rate (T P R) for a given value of N and .

Contours have been drawn to cluster the points corresponding to the same value of β. The average

sparsity, i.e. the ratio between the number of connected pairs of spins and the number of not
connected ones, is (cid:104)r(cid:105) = 0.0507. Similar plots for other topologies can be found in the Appendix.

interaction gets stronger (see Appendix). Secondly, the dependence of TPR and TNR with

N conforms to what is expected from the discussion in the previous sections:

for small

samples, MS favours simpler models, i.e. those with few bonds. Hence one expects the TPR

(TNR) to increase (decrease) with N , as in Fig. 6.

Finally, we observe that the choice of the prior is very important, and it gauges the

tradeoﬀ between true positives and true negatives: with larger value of  the reconstructed

network is denser, hence the TPR is larger. Besides this, we observe that the reconstruction

becomes more accurate for values of  that are closer to the true sparsity r. This shows that

18

True negative rate00.10.20.30.40.50.60.70.80.91True positive rate00.10.20.30.40.50.60.70.80.91N = 50N = 100N = 150N = 200N = 250N = 300ErdosRenyic=3hri=0.0507ǫ=0.02ǫ=0.1ǫ=0.3ǫ=0.5ǫ=0.7ǫ=1β=1.5β=1β=0.7β=0.5FIG. 7. (Left) Comparison between MS (blue lines) and PLM+(cid:96)1 (red lines) for Erd¨os R´enyi graphs

with average degree c = 3, of n = 64 nodes. Each point of the curves represents the average of

TNR and TPR over one hundred diﬀerent realisations. The comparison is drawn for two diﬀerent

values of N and in the weak couplings regime (β = 0.5). The curves for PLM+(cid:96)1 are obtained

by varying the regularizer; the ones for MS by varying . Two diﬀerent points are highlighted

for each curve showing the eﬀects of employing the self-consistent (coloured markers) procedure

for selecting the models’ prior coeﬃcient, and an ad hoc N -dependent (white markers) procedure
(N ) = rg + (1 − rg) exp (−N/50), where rg represents our belief of the sparsity of the network
(rg = 0.01 in the ﬁgures).
(Right) F P R (= 1− T N R) with dotted lines and F N R (= 1− T P R), solid lines, versus the size of

the sample N . The reconstruction using MS (blue lines plus error bars) is compared to that using

PLM+(cid:96)1 (red lines plus shaded error bars). The thresholding procedure employed for MS is the

N-dependent one; the regulariser used for PLM+(cid:96)1 is half of the maximal regulariser for which the

inferred network becomes empty. We found that the latter ﬁxed choice of the regulariser optimise

PLM+(cid:96)1 results across the investigated topologies and sample sizes.

the self-consistent procedure for the choice of the prior is an important ingredient of the MS

method. Fig. 7 (left) shows that indeed the self-consistent procedure selects points close to

the top-right corner of the plot. For small sample sizes (N = 50) MS selects too few bonds,

which results in a low TPR. This can be improved by an ad hoc correction for small sample

sizes (see Figure caption).

Fig. 7 (left) also reports the comparison of the MS reconstruction with the one obtained

19

TNR00.20.40.60.81TPR00.10.20.30.40.50.60.70.80.91N=50N=100ErdosRenyic=3hri=0.0507with the PLM+(cid:96)1 method for the case of Erd¨os R´enyi graphs with average degree c =

3 (see the Appendix for similar results on other topologies and diﬀerent network sizes).

Surprisingly, even for a graph with loops as the one in the Fig. 7, the MS algorithm exhibits

performance comparable to those of PLM+(cid:96)1, when N is small. In fact, the curves of the

two methods in the ROC plot are almost always superimposed and this fact implies an

equivalent discriminative power. We found the same results even in the presence of small

external ﬁelds. As expected, as N becomes large (see Fig. 7 right), MS produces many

false positives even when using the self-consistent procedure for selecting the prior. Yet

the FNR stays below that of PLM+(cid:96)1 even for large N , and it tends to zero with very

small error bars. This fact makes MS interesting as a pruning algorithm for large sparse

network and as a pre-treatment procedure for pseudo-likelihood based techniques in order

to save computational time and enlarge their domain of application to larger networks. In

summary, in the investigated cases of sparse graphs with few loops, MS demonstrated its

ability in classiﬁng relevant feature in the deep undersampling regime as well as one of the

best existing algorithm and, moreover, its quality in spotting irrelevant couplings, especially

for large N .

Before moving to partially observed graphs, we discuss a simple way to improve the quality

of the MS reconstruction, that originates from the excessive number of bonds recovered.

B. Removing indirect interactions by conditioning

The excess of false positives in the interaction graph recovered by MS can be cured, at

least partially, by considering model selection on larger graphs. An even simpler recipe, that

requires a minimal additional complexity, is that of re-running the algorithm conditioning

on the value of a spin or of a subset of them. Let us take the simple example of three spins,

where all the bonds between spins 1, 2 and 3 have been recovered by MS. In order to ascertain

whether the interaction between 1 and 2 is genuine, one can condition on the value of S3 and
perform the inference, separately, on the sample where S3 = +1 and S3 = −1. This produces
two predictions ˜η+
1,2, for the diﬀerence between the probabilities of having and not
1,2 ν(S3 = −1),
having a bond between spins 1 and 2. If the quantity ˜η1,2|3 = ˜η+
where ν(S3 = ±1) represents the fraction of times S3 = ±1 in the dataset, is greater than zero
then the interaction between S1 and S2 is genuine and is not induced by S3. Furthermore, one

1,2 ν(S3 = 1)+ ˜η−

1,2 and ˜η−

20

can extend this argument to all the other spins Sk that interact with both S1 and S2 in the

recovered graph, and check whether the interaction between S1 and S2 is ﬁctitiously induced

by some other spin. There are diﬀerent ways in which the eﬀect of diﬀerent spins can be

combined to arrive at a prediction on whether the bond between S1 and S2 exists. We refer

to the Appendix for a detailed discussion. Here we just report the result for the conservative

case where we take the minimal value of ˜η1,2|k over all the common neighbours Sk of S1 and

S2 on the recovered graph. Fig. 8 shows the performance of the corrected reconstruction for

the case of an Erd¨os R´enyi graph and the hard case of the star graph, i.e. a graph built

on one node to which all the others are connected, which is a particularly hard situation

for MS: each pair of not directly connected nodes are conditionally independent given the

node in the centre of the star. Therefore typically MS returns a fully connected matrix with

many false positives. It is interesting to notice that also PLM+(cid:96)1 is quite inaccurate when

dealing with stars and only for large sample sizes N one recovers satisfactory results. In this

regime and in general for all the investigated topologies, we observed that the corrections

explained above allow for lowering the false positive rate to at least one order of magnitude

while keeping the false negative rate almost always below the PLM+(cid:96)1 curve. Even for hard

cases this recipe is able to substantially improve MS performance in the large N regime,

as is shown in Fig. 8 (right panel) for the case of a star graph (see Appendix for similar

results on other topologies). In summary this simple correction allow for an extension of

the domain of applicability of our method to hard classes of sparse topologies and to larger

samples’ length, where usually ordinary MS returns many false positive, reaching results

comparable to those of PLM+(cid:96)1.

Finally, it is worth to stress that this correction requires almost the same computational

eﬀort as the original algorithm: in the ﬁrst case given the length of the sample N one needs

to evaluate only one classiﬁer (i.e. the borders of the partitions of Fig. 5); when adding this

correction, the number of classiﬁers required grows with the number of loops of 3 nodes found

in the recovered graph and with the polarisation of the nodes involved in those structure. In
fact if these nodes are all 1 or −1 roughly half of the times then again approximately only
one classiﬁer is needed.

21

FIG. 8. FNR and FPR for network recovery from data generated from simulations of an Ising

model with n = 64 spins and β = 0.5 on Erd¨os R´enyi c = 3 (left) and star (right) topologies.

The performances of MS (blue) and PLM+(cid:96)1 (red) are compared with the corrected MS algorithm

described in section III B (violet, see text and Appendix).

C. Partially observed graphs

Let us now discuss the case where the data contain only a partial observation of the nodes

of the graph. For example, Fig. 9 shows the results of the inference on a Erd¨os R´enyi random

graph of 250 nodes where only n = 64 of them are observed (see caption for details). From

the ﬁgure it is clear that PLM+(cid:96)1 looses accurancy with respect to MS in the case of a partial

observed network. In fact, PLM+(cid:96)1 returns more false positives than MS for small N , at

odds with the case of fully observed graphs. Even for large N , the number of false positives

remains high with respect to the fully observed case. On the other hand, the number of

false negatives is, as usual, always bigger than the one resulting from MS algorithm, except

for very small values of N where, as previously discussed, the self consistent approach turns

out to be extremely selective. However, this value can be reduced by employing an ad hoc

correction for small sample sizes (see caption in Fig. 7). As a consequence, for small N , the

graph recovered by PLM+(cid:96)1 is always denser than the correspondent one with MS whereas

in the fully observed case the opposite is true. This tendency of overestimating the number

of bonds of the graph is due to the fact that PLM+(cid:96)1 tries to explain accurately all observed

statistical dependences in the data which are noisy and often mediated by hidden variables.

As a consequence, it tends to assign a bond, and therefore give the same distance on the

graph, to each of the identiﬁed statistical dependencies, failing in discerning between direct

22

FPR10-310-210-1100Erdos Renyi c = 3N1002003004005006007008009001000FNR10-210-1100FPR10-310-210-1100star graphN1002003004005006007008009001000FNR10-1100FIG. 9. FNR and FPR for network recovery from data generated from simulations of an Ising

model with β = 0.5 on a Erd¨os R´enyi graph with c = 3 and 250 nodes. The data contains

observations of the spins on n = 64 randomly chosen nodes. Each point on the curves in the

ﬁgures represents the average over one hundred diﬀerent realisations of the same topology and

the standard deviations are depicted with (shaded) error bars. The performances of MS (blue)

are compared to the PLM+(cid:96)1 (red) algorithm: (left) false positive rate (FPR), false negative rate

(FNR) and (right) sparsity of the reconstructed graphs versus the length of the data sample. The

value of the regulariser used is the same as the case of a fully observed graph.

and indirect interactions. This is particularly interesting for real data applications, where

often the observations of the system under investigation are few and concern only a small

portion of it. In fact, as we will see in the following sections when dealing with real data in

this regime, networks recovered with PLM+(cid:96)1 will always contain more bonds.

IV. APPLICATION TO REAL DATA

In this section we apply our method (MS) to real data and we compare its predictions

with those from PLM+(cid:96)1. We discuss a dataset of ﬁnancial returns of the stocks in the Dow

Jones index and a dataset on the neural activity of cells in the entorhinal cortex of a moving

rat. In both cases, the statistics of the data is likely non-stationary, so that inference on

the whole dataset may lead to confounding eﬀects. The virtue of the MS method is that

it is ideally suited to very small samples, and hence it allowed us to study the dynamics in

these datasets on smaller windows of time for ﬁnancial data, and space for the neural data,

thereby revealing genuine statistical dependencies.

23

FPR10-310-210-1100N02004006008001000FNR10-410-310-210-1N02004006008001000Nb/Nnb00.050.10.150.20.25A. US stock market data

In this section we apply our method (MS) to real data and as usual compare predictions

with those from PLM+(cid:96)1. The dataset is the same as the one studied in [14], which was

taken from Yahoo Finance and consists of the returns (i.e. the logarithm of the ratio between

the closing price and the opening one) for 41 stocks in the Dow Jones index from 1st Jan

1980 to 25 June 2007 for each trading day. In order to subtract the overall market trend

and focus on the correlations between the stocks, the average values of the returns for each

day has been subtracted from the values of the returns. Finally we took the sign of the

resulting returns in order to get conﬁgurations of 41 spins, i.e. Si(t) with i = 1, .., 41 for

each day t. This is expected to remove, at least partly, long-term auto-correlation eﬀects in

the (absolute) size of returns – the so-called volatility [14, 19]. The list of stocks taken into

account along with the industrial sector are listed in Tab. II in Sec. B of the Appendix.

As a preliminary step, we check that auto-correlation

t+N−1(cid:88)

t(cid:48)=t

cN
ij (t, τ ) =

1
N

Si(t(cid:48))Sj(t(cid:48) + τ )

with mN

Si(t(cid:48)), between a selected set of pairs of stocks (see later). This
shows periods of high correlations interspersed with stretches where stocks dependencies are

i (t) = 1
N

t(cid:48)=t

weaker. In situations like this one, any eﬀort of representing the evolution of the statistical

dependence by an inferred interaction network, faces the unavoidable limit that the sample

size N cannot be arbitrarily large.

Inference on the whole dataset would mix diﬀerent

market phases. On the contrary, it is reasonable to assume the market to be “locally” in

equilibrium at a given time t, if N is taken to be small enough. From our analysis (see

Appendix) N = 200 is large enough to aﬀord statistically stable results, that are consistent

with results obtained with smaller values of N , but that capture the non-stationary evolution

of market correlations.

24

are statistically insigniﬁcant for τ (cid:54)= 0 (see Appendix). This is consistent with the Eﬃcient
Market Hypothesis, that states that current prices fully reﬂect all available information, and

hence future returns cannot be predicted [19]. Yet, the time series of returns is far from

stationary, as evidenced by the plot in Fig. 11 of the equal time connected correlation

t+N−1(cid:88)

t(cid:48)=t

1
N

C N

ij (t) =

(cid:80)t+N−1

Si(t(cid:48))Sj(t(cid:48)) − mN

i (t)mN

j (t)

We performed inference of the underlying interaction network using MS with a ﬁxed and

self-consistent threshold and with PLM+(cid:96)1, on rolling windows of N days. Here we present

the main results for N = 200, that correspond roughly to 10 months, and refer to the

Appendix for a detailed derivation.

For each pair i < j, we compute the conﬁdences ηij(t) of the MS inference on windows of

N = 200 days and their average ηij over t. Fig. 10 (left) shows that the links separate nicely

into a group with ηij > 0 that correspond to those pairs that are often connected by a bond

in the reconstructed network, and those with ηij < 0 that are hardly or never connected.

This implies a persistent underlying network structure, which is shown in Fig. 10 (right,

thick bonds). Inspection of Table II reveals that all the components highlighted correspond

to stocks in the same economic sector: a large clique groups stocks in the utilities sector

(PEG, AEP, CNP, ED, EIN and PCG), whereas other isolated bonds connect stocks in the

Airline industry (AMR, LUV), Oil (CVX, XOM) and Healthcare (MRK, JNJ).

Likewise, we infer the couplings Jij(t) on the same data, with PLM+(cid:96)1 and compute the

average Jij over t. The resulting histogram exhibits a long tail for positive values of Jij

(see Appendix) that can be distinguished from a noisy bulk by setting a threshold. The

links corresponding to values Jij > 0.02 are shown in Fig. 10 (solid and dashed lines). In

addition to the bonds retrieved with MS, this PLM+(cid:96)1 procedure also retrieves other bonds

that connect the Healthcare cluster to stocks in the Consumer Goods sector (KO, MO, PG)

and it identiﬁes three additional components, in the Chemicals (DD, DOW) and Financial

(AXP, C) sectors, and one that mixes General Electrics (GE) with Technology ﬁrms (IBM,

HPQ).

The full power of our approach, however, lies in its ability to fully account for the non-

stationary nature of the correlations. This is evidenced by Fig. 11, that reports, besides

the value of the connected correlations (top panel), the results of network inference with

MS (middle panel) and PLM+(cid:96)1 (bottom panel). Each row corresponds to one of the 31

pairs of links identiﬁed in Fig. 10 and presence or absence of a bond at time t is indicated

by a bright to dark spot. Visual inspection reveals that MS accounts much better for the

non-stationary nature of correlations than PLM+(cid:96)1.

More precisely, Fig. 12 (left) shows the time evolution of the sparsity of the network

inferred with MS and with PLM+(cid:96)1. While with the former, the rises and falls of the

network are rather sharp, for PLM+(cid:96)1 the density of the network appears to be higher and

25

FIG. 10. Histogram of the averaged conﬁdence ηij (left) and resulting equilibrium graph (right,

thick links). Dashed links refer to additional links inferred by the PLM+(cid:96)1 method, whereas the

dotted one between EIX and EXC indicates the only connection not recovered by PLM+(cid:96)1. The

names of the stocks present in this graph are listed in Table II and highlighted with diﬀerent

colours: one for each connected component recovered by MS.

.

much smoother. We also run MS on a reshuﬄed dataset, where each time N conﬁgurations
(cid:126)S(t) are chosen at random. Fig. 12 (left) also reports the average and typical ﬂuctuations

that one would expect from MS on these system sizes. Clearly, density ﬂuctuations in the

true dataset are much more pronounced than what would be expected in the shuﬄed one.

This also shows that the average density of bonds in MS is smaller than what PLM+(cid:96)1

would predict.

The right panel of Fig. 12 relates the behaviour of the network to the dynamics of the

correlations. As a proxy of the latter we take the root mean square value Coﬀ(t) of the

connected correlations C N

ij (t) (i < j) and compare it to the sparsity of the inferred networks
at the same time. While the network inferred by MS follows closely the dynamics of correla-

tions, that inferred by PLM+(cid:96)1 does not. Fig. 12 also shows the eﬀect of the self-consistent

prior in making the MS algorithm more sensible to non-stationary eﬀects, with respect to

MS with a ﬁxed prior.

It is important to stress that, in spite of these diﬀerences, MS produces results that are

highly consistent with those of PLM+(cid:96)1.

Indeed 94% of the bonds identiﬁed by MS are

also found by PLM+(cid:96)1. This conﬁrms, as could be anticipated by its vary nature, that MS

26

ηij-1-0.500.51050100150200PEGAEPPCGEXCEIXCNPEDMOPGJNJMRKCVXXOMLUVAMRGEIBMHPQKOCAXPDDDOWFIG. 11. Connections between the stocks belonging to the investigated clique from the 1st of

January 1980 to the 25th of June 2007 obtained with MS (middle) and with PLM+(cid:96)1 (bottom
ij (t)| between the same stocks,
panel). For comparison, the absolute values of the correlations |CN
during the same period, is also shown (top panel). Colour maps: (top) very small correlations are

in blue and very high ones in red; (middle and bottom) green corresponds to no-bond and yellow

means a bond. MS results have been obtained with a self-consistent threshold whereas PLM+(cid:96)1

assumes a regulariser that is half of the maximal regulariser for which the inferred network becomes

empty. For such a small samples and across the considered graph, the latter recipe for the regulariser

was found to work better on synthetic data than the one given in [8].

.

delivers rather conservative predictions on network inference.

Still, a sparser and more dynamic network of interaction does not per se makes of MS

a method which is superior to PLM+(cid:96)1. A more stringent test is possible by analysing

the ability of the reconstructed network to describe data out-of-sample.

In practice, we

estimate the network of interactions and the values of parameters in a window of N data

27

1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930311 2 3 4 5 6 7 8 9 10111213141516171819202122232425262728293031t123456789101112131415161718192021222324252627282930311980199520002005199019851)  PEG - AEP2)  AXP - C3)  PEG - CNP4)  AEP - CNP5)  DD - DOW6)  PEG - ED7)  AEP - ED8)  CNP - ED9)  PEG - EIX10) AEP - EIX11) CNP - EIX12) ED - EIX13) PEG - EXC14) AEP - EXC15) CNP - EXC16) ED - EXC17) GE - IBM18) HPQ - IBM19) AMR - LUV20) JNJ - MRK21) PEG - PCG22) AEP - PCG23) CNP - PCG24) ED - PCG25) EIX - PCG26) EXC - PCG27) JNJ - PG28) KO - PG29) MO - PG30) CVX - XOM31) EIX - EXCFIG. 12. Left: Comparison between the recovered ‘bond’-‘no-bond’ ratio with MS assuming a self

consistent approach (blue), a ﬁxed threshold (green) and with PLM+(cid:96)1 (red). The yellow shaded

band corresponds to results obtained by MS with a self consistent approach on a reshuﬄed dataset
(mean ± standard deviation). Right: Scatter plot of the sparsity for MS with self-consistent

(blue) and ﬁxed (green) priors, and for PLM+(cid:96)1 (red) versus the oﬀ diagonal r.m.s. values of the

connected correlations (Coﬀ (t))

.

points (training sample) and evaluate the likelihood of the data in the subsequent window of

N data points (test sample). For completeness, we also compare the results with a sample

of N data points chosen at random (random sample). In order to avoid problems of non

stationarity, we focus on windows of N = 50 and N = 100 points. In this way, the train

and the test samples togheter ﬁlls a time window smaller than the assumed stationary time

scale.

For each t which is a multiple of N , we recover the MS (blue lines) and the PLM+(cid:96)1 (red

lines) networks on the training sample, estimate the non zero parameters using an expectation

consistent inference method following [20] and evaluate the likelihood of the training (dash-

dot lines), test (solid lines) and random (dotted lines) samples, for both networks. The

resulting values of the likelihood are shown in Fig. 13 over the whole timespan of the dataset.

The maximum likelihood values on the training samples (dash-dot lines) are always above

the others, as expected, since they corresponds to the maximum likelihood values on the

training samples. In addition, since PLM+(cid:96)1 always recovers a more complex model (denser

graph), it ﬁts better the in-sample data and consequently the correspondent likelihood of the

28

CToff(t)0.10.110.120.130.140.150.160.170.18Nb(t)/Nnb(t)00.020.040.060.080.10.120.140.160.18FIG. 13. Normalised log-likelihood, 1

N log L((cid:126)θ| ˆS), on the training (dash-dot lines), test (solid lines)

and random (dotted lines) samples as a function of time t, for N = 50 (left) and N = 100 (right).

Blue lines refer to networks inferred on the training sample with the MS method with the self

consistent prior, whereas the red lines refer to PLM+(cid:96)1 inference. As a reference, the normalised

log-likelihood of a random spin model with no parameters is also reported as a black dashed line.

training set is larger than the MS one. The situation is reversed, however, when analyzing

the values of the likelihood in the test samples (solid lines). There the MS method achieves

a larger likelihood with respect to the PLM+(cid:96)1 method, both for N = 50 and for N = 100.

The diﬀerence in likelihood is smaller in periods of strong correlations (e.g. after 2000)

where the MS network is denser. In addition, the diﬀerence between the likelihoods of the

training and the test samples are much smaller for MS than for PLM+(cid:96)1.

The likelihoods of the random sample (dotted lines) are smaller than those of the test

set, but while the diﬀerence is negligible for MS it is considerable for PLM+(cid:96)1. This is

related to the fact that the sample is also reasonably well described by a model of random
spins, where P ((cid:126)S) = 2−n does not depend on any parameter. The corresponding normalised
log-likelihood (−n log 2) is shown as a black dashed line in Fig. 13 and it lies above the
likelihood of the test samples of MS and PLM+(cid:96)1 for most of the time.

In summary, the very sparse topologies recovered by MS represent better the US stock

market interactions with respect to the ones of PLM+(cid:96)1. The MS topologies are very similar

to the ones obtained with an independent spin or zero-parameters model and they have very

similar out-of-sample likelihoods. This suggests that the eﬀects of interactions need to be

invoked only sporadically and especially in the last years (around 2000-2007) which are

29

t198519901995200020051NlogL(~θ|ˆS)-40-35-30-25-20-15-10-50N = 50t198519901995200020051NlogL(~θ|ˆS)-40-35-30-25-20-15-10-50N = 100characterised by a correlated dynamics. This shows that MS is a quite sensitive inference

method in situations where sample sizes are small (e.g. for non-stationary eﬀects) and the

data is quite noisy.

B. Neural data

The dataset is the same as the one studied in [15]. It consists of the recording of the
activity of 65 neurons from the entorhinal cortex of a rat moving in a 1.5 × 1.5 meters
box. The position of the head of the rat is also recorded (see [21] for experimental details).

A detailed analysis [21] reveals that 27 of the recorded cells are grid cells, 5 of them are

interneurons and the remaining ones are of unknown type.

Here we use the Model Selection (MS) method to address the issue of inferring the inter-

actions between these cells. It has long been observed that inference of interactions between

neurons is a particularly diﬃcult problem (see e.g. [22] for a discussion). In fact, the recorded

neurons are sampled from a large pool of diﬀerent kinds of neurons and other cells inter-

acting with characteristic time scales through directed connections. Given the experimental

limitations and the scarce amount of data, resolving directed synaptic connections remains

an hard task. Therefore, our approach, as well as all Inverse Ising inference methods, aims at

detecting eﬀective connections, i.e. strong statistical dependencies between neurons which

can also arise as the result of indirect connections mediated by unobserved components of

the network.

One relevant issue, for this dataset, is that neural activity depends on the position of the

rat. If inference is carried out using data recorded when the rat is in diﬀerent positions,

the spatial dependence of the neural activity induces correlations that result in eﬀective

couplings between neurons that do not reﬂect genuine statistical dependence. If inference

can be performed controlling for the position of the rat, these eﬀects can be avoided and

genuine (hopefully) direct statistical dependencies can be revealed. For this reason, we

divide the dataset in many parts, each corresponding to a diﬀerent spatial position. As in
Ref. [15], we divided the box in a grid of 20× 20 cells, divided the database correspondingly
and performed the analysis on the recording in each cell separately. This again brings us to

an inference problem where the number of samples is very limited and for which we expect

a sparse interaction network. We expect the network to be sparse also because the dataset

30

FIG. 14. (Left) Bond-to-no-bond ratio for inferred graphs with MS (blue circles) and PLM+(cid:96)1

(red circles) versus the length of the samples (left panel). The time bin employed for the ﬁgure is

10 ms but the same results are found also for diﬀerent choices. The inset in the ﬁgure shows the

distribution of the length of samples and the relative cumulative function (right y-axis).

(Right) We report the logarithm of the ﬁring rate, log φ, for a given neuron normalised with its mean

over cells, log φ, and averaged over cells and neurons belonging to the same class (i.e. interneurons

and grid cells) conditioned upon the number of connections (solid lines in the ﬁgure on the right

panel). The standard deviations are depicted with error bars for interneurons and shaded error

bars for grid cells.

documents the activity of only a fraction of the cells that are actually involved in spatial

cognition and navigation tasks. In other words, this is a problem where inference has to be

performed in the presence of many missing degrees of freedom. The eﬀect of these hidden

variables is to make our dataset very noisy. In this situation, a model selection approach

is necessary in order to correctly evince how much structure can be inferred from the data.

Hence the MS approach is a valuable alternative to other approaches [12, 13] to inference in

the presence of hidden variables.

We limit our discussion here to the typical properties of our algorithm, some illustrative

example and the comparison with PLM+(cid:96)1, leaving a full analysis of the neural activity for

a future publication. We considered the neural activity in time bins of 5, 10 and 20 ms and

partition the time series into subsets where the position of the rat is in one of the 400 cells.

31

length of the samples100101102103104Nb/Nnb00.020.040.060.080.10.120.140.160.18MSPLM+ℓ1length of the samples0100200300400500600700800900100001020304000.510.750.25Number of connections05101520253035⟨logφ−logφlogφ⟩-1-0.500.511.52grid cellsinterneuronsFor each cell, we deﬁned spin variables to attain a value Si(t) = −1 if neuron i is inactive in
time bin t and Si(t) = +1 if it is active (here i = 1, . . . , 65 runs over neurons whereas t runs

over all the time bins where the rat is in the given cell). On each of the sub-samples obtained

in this way for the diﬀerent cells, we run both PLM+(cid:96)1 and the MS method in each of the

400 cells. A further clue that MS detects genuine interactions arises from the expectation

that interaction should be more likely between neighbouring cells. A proxy of the spatial

location of cells is the tetrode index, i.e. cells recorded by the same tetrode are expected to

be closer than those recorded by diﬀerent tetrodes. At 10ms we found that out of the 9820

interactions detected, 1606 correspond to cells recorded by the same tetrode. Since there are

10 tetrodes, the probability that two cells are recorded by the same tetrode is 10%. We then

tested the null hypothesis that the recovered bonds connect two randomly selected neurons

regardless of their location (belonging to the same tetrode). In this case the z-score, which

measures the distance between the sample mean and the expected value of the number of
intra-tetrode connections in units of standard deviations, is 20, 99 (P-Value = 2.93 · 10−82)
and since the z-score is positive we can conclude that they mostly connect neurons belonging

to the same tetrode. We found the same results also for 5ms and 20ms. By contrast, the

same analysis for PLM+L1 (77883 contacts of which 8204 inter-tetrode) yields a much lower
z-score of 4, 97 (P-Value = 9.18 · 10−5).

One key aspect, is that diﬀerent positions are visited with diﬀerent frequency by the rat.

Hence diﬀerent cells correspond to samples of widely varying size and about 75% of them

have length less than 300 data-points. This is documented in Figure 14 (see inset). The

left panel of the ﬁgure shows that PLM+(cid:96)1 estimates way more links than MS. The number

of interactions detected with MS increases with the size of the sample, which means that

the inferred network is denser in cells that are visited more often. This is expected in the

MS approach. The results of MS are highly consistent with those of PLM+(cid:96)1 in that 99%

of the interactions detected by MS are also detected by PLM+(cid:96)1. Conversely, PLM+(cid:96)1

detects many more interactions, many of which are attached a very low conﬁdence by MS.

Indeed, we found PLM+(cid:96)1 to suﬀer from instability problems in cases of very small samples.

The top interactions inferred by MS are found to be very consistent across the grid in the

sense that their conﬁdence is positive in several diﬀerent cells. Interestingly, the network

of the most frequent interactions connect the ﬁve interneurons. Moreover the number of

connections found for grid cells and interneurons do not correlate with their activity making

32

FIG. 15. The ﬁgure shows the spatial spiking pattern (a) and the spatial connectivity pattern

(b) of the grid cell T7C1, namely the number of spikes and the number of connections inferred

by MS with a time bin of 10 ms for the above mentioned grid cell versus the spatial position of

the rat in the box. Interestingly for this particular grid cell, the connectivity pattern follows the

spiking one, a feature not common to all the grid cell in the considered ensamble. This neuron

makes connection mostly with the ﬁve interneurons (70%) but also with grid cells (20%) and other

neurons (10%). This is illustrated in detail in panel (c) with the following color code: cyan for

interneurons, red for grid cells and yellow for all other neurons. In particular, panel (c) reports

the number of connections inferred between the grid cell T7C1 and all the other neurons which are

listed on the x axis.

the detected dependency stable against high ﬂuctuactions in the neural activity (right panel).

The ﬁring rates of grid cells exhibits a remarkable variation across space that reveals

the characteristic hexagonal patterns [21], whereas interneurons have a more uniform ﬁring

activity in space. Figure 15 shows a representative example for a particular grid cell. The

pattern of interactions between grid cells and between grid cells and interneurons follows the

spatial dependence of their activity, but it exhibits a more heterogeneous behaviour. This

is expected, because in regions where the ﬁring activity of the grid cell is very low, simpler

33

2010x02010y5002010x02010y10000T10C1T10C2T10C3T10C4T10C5T10C6T12C1T12C2T12C3T12C4T12C5T12C6T12C7T1C1T1C2T1C3T1C4T1C5T1C6T2C1T2C2T2C3T2C4T2C5T2C6T3C1T3C2T3C3T3C4T3C5T3C6T3C7T3C8T4C1T4C10T4C2T4C3T4C4T4C5T4C6T4C7T4C8T4C9T6C1T6C2T6C3T6C4T6C5T6C6T7C1T7C2T7C3T7C4T7C5T7C6T8C1T8C2T8C3T8C4T8C5T8C6T8C7T9C1T9C2T9C301020IXG00.51d)b)a)c)2010x02010y5002010x02010y10000T10C1T10C2T10C3T10C4T10C5T10C6T12C1T12C2T12C3T12C4T12C5T12C6T12C7T1C1T1C2T1C3T1C4T1C5T1C6T2C1T2C2T2C3T2C4T2C5T2C6T3C1T3C2T3C3T3C4T3C5T3C6T3C7T3C8T4C1T4C10T4C2T4C3T4C4T4C5T4C6T4C7T4C8T4C9T6C1T6C2T6C3T6C4T6C5T6C6T7C1T7C2T7C3T7C4T7C5T7C6T8C1T8C2T8C3T8C4T8C5T8C6T8C7T9C1T9C2T9C301020IXG00.51d)b)a)c)2010x02010y5002010x02010y10000T10C1T10C2T10C3T10C4T10C5T10C6T12C1T12C2T12C3T12C4T12C5T12C6T12C7T1C1T1C2T1C3T1C4T1C5T1C6T2C1T2C2T2C3T2C4T2C5T2C6T3C1T3C2T3C3T3C4T3C5T3C6T3C7T3C8T4C1T4C10T4C2T4C3T4C4T4C5T4C6T4C7T4C8T4C9T6C1T6C2T6C3T6C4T6C5T6C6T7C1T7C2T7C3T7C4T7C5T7C6T8C1T8C2T8C3T8C4T8C5T8C6T8C7T9C1T9C2T9C301020IXG00.51d)b)a)c)2010x02010y5002010x02010y10000T10C1T10C2T10C3T10C4T10C5T10C6T12C1T12C2T12C3T12C4T12C5T12C6T12C7T1C1T1C2T1C3T1C4T1C5T1C6T2C1T2C2T2C3T2C4T2C5T2C6T3C1T3C2T3C3T3C4T3C5T3C6T3C7T3C8T4C1T4C10T4C2T4C3T4C4T4C5T4C6T4C7T4C8T4C9T6C1T6C2T6C3T6C4T6C5T6C6T7C1T7C2T7C3T7C4T7C5T7C6T8C1T8C2T8C3T8C4T8C5T8C6T8C7T9C1T9C2T9C301020IXG00.51d)b)a)c)models with fewer parameters (i.e. without a bond) are preferred because the corresponding

spin is almost frozen. Yet we found no statistically signiﬁcant dependence on the spatial

position of the interactions that were detected, i.e. statistically the interactions found for a

given neuron in one cell do not depend on the position of the cell in the space, suggesting that

these correspond to genuine interactions. The grid cell shown in Figure 15 has signiﬁcant

interaction with the interneurons that is statistically independent from the spatial position.

This was found to be true of several other grid cells, but we also found examples of grid

cells preferentially interacting with grid cells. This shows that neighbourhoods of interaction

networks are very heterogeneous, which is to be expected if one thinks of this network as

being a very sparse sub-sampling of a much larger network.

In summary, the application of the MS approach to neural data shows its potential for

statistical dependencies between the neural activity of diﬀerent cells that is not aﬀected by

the confounding eﬀect of the spatial position variable. Furthermore, at odds with other

methods, our approach naturally takes into account the fact that only a tiny region of a

larger network of interactions is observed and recognizes statistical dependencies on the

basis of the quality of the observations, being more selective when only few observations are

available.

V. CONCLUSIONS

In this paper we presented a model selection approach for topology reconstruction of

a network of interacting binary nodes. Our approach explores the idea of calculating, in

a fully Bayesian fashion, the posterior probabilities of all possible graphical models and it

relies on the observation that in the small sample limit, complex models are heavily penalised

with respect to simpler one. It makes sense, in this limit, to consider only minimal clusters

composed of just two spins. Therefore, we derive a full Bayesian approach to model selection

for a system of two spins, potentially interacting with each other and with the rest of the

network through eﬀective ﬁelds. The use of such minimal clusters reduces the problem to

a simple two body system with a relative small number of possible graphs allowing a fully

analytic treatment and a direct geometric visualization. This choice is justiﬁed in the noisy

data regime when usually simple schemes may capture better the underlying structure than

more complex techniques, better suited for high quality data. In fact, we have proven on very

34

noisy synthetic data sets that such a simple method is at least equivalent, if not superior,

to the well known L1 regularized pseudo-likelihood method for sparse networks and weakly

coupled spins. Moreover a list of advantages accompanies the use of our method: it is a fully

automatic procedure and does not need selecting additional ad hoc parameters as the choice

of the regularizer and the thresholding value in PLM+(cid:96)1; it is computationally convenient

since it can be implemented as a direct online tool: the boundaries of the partitions in the

parameters space are computed once for a given value of N , and they can then be used for

all pairs. This sounds particularly appealing for large networks applications where pseudo-

likelihood methods typically becomes slow. Finally in Sec. IV we have shown that MS is a

method that is particularly suited to study non-stationary data, as shown by the analysis

of the US stock market data, or to remove confounding eﬀect of external variables, as in

the case of neural data, because it is ideally suited to work on small samples. Our methods

looses accuracy when less sparse networks are taken into account and when more loops arise

from the connectivity pattern of spins. In these cases it would be straightforward and very

interesting to extend our approach to 3-body or 4-body clusters of spins in order to count

for small loops in the network. There are several other directions where the MS approach

could be generalised in a straightforward manner as, e.g. to non-equilibrium spin models

[23], to Potts spin models such as those used in contact prediction for proteins [7] etc.

Even for high quality data, for sparse topologies and weak couplings, the above presented

Model Selection approach exhibits the interesting property of recognizing all direct connec-

tions in the network at the cost of a resonable small number of false positive. Therefore

our method candidates as an eﬃcient pruning algorithm for sparse networks of any size

and as a pre-treatment tool for reducing the number of features and saving computational

time in subsequent applications of more involved methods, as the pseudo-likelihood ones.

Furthermore the excess of false positive can be cured at least partially in a very simple way

improving signiﬁcatly the accurancy of our method especially in the large N regime.

ACKNOWLEDGMENTS

The authors would like to thank Aur´elien Decelle for interesting discussions. This research

was supported in part with computational resources provided by NOTUR.

35

[1] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society (Series B), 58:267–288, 1996.

[2] D.L. Donoho. Compressed sensing. Information Theory, IEEE Transactions on, 52(4):1289–

1306, April 2006.

[3] David H. Ackley, Geoﬀrey E. Hinton, and Terrence J. Sejnowski. A learning algorithm for

boltzmann machines*. Cognitive Science, 9(1):147–169, 1985.

[4] Yasser Roudi, Joanna Tyrcha, and John Hertz. Ising model for neural data: Model quality

and approximate methods for extracting functional connectivity. Phys. Rev. E, 79:051915,

May 2009.

[5] S. Cocco and R. Monasson. Adaptive cluster expansion for inferring boltzmann machines with

noisy data. Phys. Rev. Lett., 106:090601, Mar 2011.

[6] Federico Ricci-Tersenghi. The bethe approximation for solving the inverse ising problem:

a comparison with other inference methods. Journal of Statistical Mechanics: Theory and

Experiment, 2012(08):P08015, 2012.

[7] Erik Aurell and Magnus Ekeberg. Inverse ising inference using all the data. Phys. Rev. Lett.,

108:090201, Mar 2012.

[8] Pradeep Ravikumar, Martin J. Wainwright, and John D. Laﬀerty. High-dimensional ising

model selection using l1-regularized logistic regression. Ann. Statist., 38(3):1287–1319, 06

2010.

[9] Aur´elien Decelle and Federico Ricci-Tersenghi. Pseudolikelihood decimation algorithm im-

proving the inference of the interaction network in a general class of ising models. Phys. Rev.

Lett., 112:070603, Feb 2014.

[10] V. Balasubramanian. A Geometric Formulation of Occam’s Razor for Inference of Parametric

Distributions. In eprint arXiv:adap-org/9601001, page 1001, January 1996.

[11] In Jae Myung, Vijay Balasubramanian, and Mark A. Pitt. Counting probability distributions:

Diﬀerential geometry and model selection. Proceedings of the National Academy of Sciences,

97(21):11170–11175, 2000.

[12] Benjamin Dunn and Yasser Roudi. Learning and inference in a nonequilibrium ising model

with hidden nodes. Phys. Rev. E, 87:022127, Feb 2013.

36

[13] C Battistin, J Hertz, J Tyrcha, and Y Roudi. Belief propagation and replicas for inference and

learning in a kinetic ising model with hidden spins. Journal of Statistical Mechanics: Theory

and Experiment, 2015(5):P05021, 2015.

[14] Matteo Marsili, Giacomo Raﬀaelli, and Benedicte Ponsot. Dynamic instability in generic

model of multi-assets markets. Journal of Economic Dynamics and Control, 33(5):1170–1181,

May 2009.

[15] Benjamin Dunn, Maria Mørreaunet, and Yasser Roudi. Correlations and functional connec-

tions in a population of grid cells. PLoS Comput Biol, 11:e1004052, 02 2015.

[16] Marc Mezard and Andrea Montanari. Information, Physics, and Computation. Oxford Uni-

versity Press, Inc., New York, NY, USA, 2009.

[17] Gideon Schwarz. Estimating the dimension of a model. Ann. Statist., 6(2):461–464, 03 1978.

[18] Harold Jeﬀreys. An invariant form for the prior probability in estimation problems. Pro-

ceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences,

186(1007):453–461, 1946.

[19] Jean-Philippe Bouchaud and Marc Potters. Theory of Financial Risk and Derivative Pricing.

Cambridge University Press, 2003.

[20] M. Opper and O. Winther. Expectation consistent approximate inference. Journal of Machine

Learning Research, 6:2177–2204, 2005.

[21] Hanne Stensola, Tor Stensola, Trygve Solstad, Kristian Froland, May-Britt Moser, and Ed-

vard I. Moser. The entorhinal grid map is discretized. Nature, 492(7427):72–78, 12 2012.

[22] Cristiano Capone, Carla Filosa, Guido Gigante, Federico Ricci-Tersenghi, and Paolo Del Giu-

dice. Inferring synaptic structure in presence of neural interaction time scales. PLoS One,

10(3):e0118412, Mar 2015. PONE-D-14-47511[PII].

[23] Yasser Roudi and John Hertz. Mean ﬁeld theory for nonequilibrium network reconstruction.

Phys. Rev. Lett., 106:048702, Jan 2011.

37

Appendix A: Synthetic data: methods and supplementary analysis

In the appendices we present two sections with details about the main results of the paper.

In the ﬁrst section we report additional simulations for the diﬀerent topologies discussed in

the paper and give further insights on the methods; in the second section we will focus

on US stock market interactions showing that, under our assumptions, conﬁgurations of
stocks at time t are uncorrelated with those at time t + τ for τ (cid:54)= 0 and for diﬀerent values
of N ; moreover we also further contrast MS and PLM+(cid:96)1 and compare their outcomes

with respect to the absolute values of connected correlations. Finally we provide a Matlab

implementation of our method.

In Fig. 16 we summarize the performances of our method at varying the length of the

sample N , the strength of the couplings β and the value of  for some of the sparse topologies

investigated: gas of dimers, Erdos Renyi graphs with c = 2 and c = 3, a full and diluted

(30% of dilution) bidimensional grid. In particular for each topology and each choice of β,

we generated one hundred diﬀerent istances of the same topology and for each of them we

generated samples with diﬀerent N through Monte Carlo sampling. For all cases of Fig.

16, the size of the network has been ﬁxed to n = 64 and the value of the couplings β have

been drawn from a unimodal distribution for the bidimensional grids and from a bimodal

distribution in the other cases. After having evaluated the matrix of the conﬁdence for each

single realisation and each N, we calculated the TPR and TNR for diﬀerent choices of 

and averaged their values over all realisations with the same topology, β, N and . The

ideal case is the one of the gas of dimers for which the method provides an almost perfect

recovey, especially for high values of β. In all other cases the best performances are achieved

in the weak coupling regime (β = 0.5) and for topologies with few bonds and loops. In this

respect, the case of the full bidimensional grid is representative since it shows the failure of

our method in recovering a graph with many loops even in the weakly interacting regime.

The comparison with PLM+(cid:96)1 in the weak coupling regime (β = 0.5) is in Fig. 17 for

all the previous topologies except for the grid. The curves are drawn at varying  for MS
and the (cid:96)1 regulariser λ for PLM+(cid:96)1 (λ ranging from 0 to 1.5 · λmax where λmax represents
the threshold above which the inferred graph is completely disconnected). The comparison

between MS and PLM+(cid:96)1 for these topologies exhibits the same features discussed for the

representative case of an Erdos Renyi graph c = 3. It is interesting to note how this inference

38

FIG. 16. Testing MS method on synthetic data generated assuming ﬁve sparse topologies: gas of

dimers, Erdos Renyi graphs with c = 2 and c = 3, a full and diluted bidimensional grid.

tecnique behaves almost perfectly (the curve pass through the right upper corner), even with

only N = 100 data points, in the ideal case of a gas of dimers. Important points outlined

in the paper regarding the performance of MS, expressed now in terms of FPR and FNR,

at varying N and n: the algorithm improves at enlarging the size of the network n and

39

True negative rate00.10.20.30.40.50.60.70.80.91True positive rate00.10.20.30.40.50.60.70.80.91N = 50N = 100N = 150N = 200N = 250N = 300β>0.5β=0.5gasofdimersr=0.0161ǫ=0.02ǫ=0.1ǫ=0.3ǫ=0.5ǫ=0.7ǫ=1True negative rate00.10.20.30.40.50.60.70.80.91True positive rate00.10.20.30.40.50.60.70.80.91N = 50N = 100N = 150N = 200N = 250N = 300β=1ǫ=0.02ǫ=0.1ǫ=0.3ǫ=0.5ǫ=1ǫ=0.7ErdosRenyic=2hri=0.0324β=1.5β=0.7β=0.5True negative rate00.10.20.30.40.50.60.70.80.91True positive rate00.10.20.30.40.50.60.70.80.91N = 50N = 100N = 150N = 200N = 250N = 300ErdosRenyic=3hri=0.0507ǫ=0.02ǫ=0.1ǫ=0.3ǫ=0.5ǫ=0.7ǫ=1β=1.5β=1β=0.7β=0.5True negative rate00.10.20.30.40.50.60.70.80.91True positive rate00.10.20.30.40.50.60.70.80.91N = 50N = 100N = 150N = 200N = 250N = 300diluted2Dgridr=0.0402ǫ=0.02ǫ=0.1ǫ=0.3ǫ=0.5ǫ=0.7ǫ=1β=0.75β=1β=1.5β=0.5True negative rate00.10.20.30.40.50.60.70.80.91True positive rate00.10.20.30.40.50.60.70.80.91N = 50N = 100N = 150N = 200N = 250N = 3002Dgridr=0.0588ǫ=0.02ǫ=0.1ǫ=0.3ǫ=0.5ǫ=0.7ǫ=1β>0.5β=0.5FIG. 17. Comparison between MS (blue lines) and PLM+(cid:96)1 (red lines) for network of n = 64 nodes

arranged in four diﬀerent topologies. Each point of the curves represents the average of TNR and

TPR over one hundread diﬀerent realisations of the same topologies with a mean ‘bond’-‘no-bond’
ratio (cid:104)r(cid:105) (r when its value is costant across realisations). The comparison is drawn for two diﬀerent

values of N and in the weak couplings regime (β = 0.5). Two diﬀerent points are highlighted for

each curve showing the eﬀects of employing a self consistent (coloured markers) and a N-varying

procedure (white markers) for selecting the models’ prior coeﬃcient.

the FNRs resulting from the application of MS go to zero faster and with smaller error

bars than those of PLM+(cid:96)1. This is shown more in detail in Fig. 18 for all the topologies

investigated along with the comparison with PLM+(cid:96)1 outcomes for a ﬁxed value of the

regulariser λ = 0.5λmax which optimizes PLM+(cid:96)1 results.

As already pointed out in the paper and shown in detail in Fig. 18, when N becomes large

enough the number of false positive becomes important and more evident in less sparse

networks. Even though our approach is thought to be used in the highly undersampled

regime where only poor predictions are possible and one expects to ﬁnd a very sparse matrix,

one simple recipe is actually available to overcome this problem under the framework we

40

TNR00.20.40.60.81TPR00.10.20.30.40.50.60.70.80.91gasofdimersr=0.0161N=50N=100TNR00.20.40.60.81TPR00.10.20.30.40.50.60.70.80.91ErdosRenyic=2hri=0.0324N=50N=100TNR00.20.40.60.81TPR00.10.20.30.40.50.60.70.80.91N=50N=100ErdosRenyic=3hri=0.0507TNR00.20.40.60.81TPR00.10.20.30.40.50.60.70.80.91diluted2Dgridr=0.0588N=50N=100FIG. 18. Comparison between MS (blue lines plus error bars) and PLM+(cid:96)1 (red lines plus shaded

error bars) for network of diﬀerent sizes n = 16, 64, 100 with nodes arranged in four diﬀerent

topologies. FPR (dotted lines) and FNR (solid lines) are shown versus the size of the sample N .

have developed in the paper with a small additional computational cost. The main idea

is already described in the paper in section III.B, therefore here we just summarise it

including a few further details and simulations. The recipe can be schematised as follows:

1. use MS with a self consistent thresholding procedure for recovering the graph. Typ-

41

FIG. 19. Simulations comparing the performances of MS (blue) and PLM+(cid:96)1 (red) with the ones

of MS employing corrections for high quality data for all topologies investigated. The corrections

are depicted in the graph with the following color code: MS avg (yellow), MS min (violet) and MS

prod (green).

ically for topologies containing many loops (e.g. a regular grid) or many conditional

independencies (e.g. a star graph), the self consistent approach fails and retrieves a

42

FPR10-410-310-210-1100gas of dimersN1002003004005006007008009001000FNR10-310-210-1FPR10-210-1100diluted 2D gridN1002003004005006007008009001000FNR10-410-310-210-1FPR10-210-1100Erdos Renyi c = 2N1002003004005006007008009001000FNR10-310-210-1100FPR10-310-210-1100Erdos Renyi c = 3N1002003004005006007008009001000FNR10-210-1100FPR10-310-210-1100star graphN1002003004005006007008009001000FNR10-1100FPR10-210-11002D gridN1002003004005006007008009001000FNR10-310-210-1densely or fully connected graph, namely a self consistent point r =  > 1. In the

latter case, if no additional knowledge about the expected sparsity of the network is

available, the choice of a uniform prior,  = 1, represents the only possibility;

2. check for each couple of connected nodes if their neighbourhood sets share some vertices

(i.e. ﬁnd the number of loops of length three based on the considered connected

couple);

3. in the latter case, given the two values of the conﬁdence for the couple (i, j) in the two

sub-sets made up by conditioning on the value of a common neighbour k, i.e. ηij|Sk=S,
evaluate the diﬀerence between the probability of having a bond and the one of not

having a bond in both the sub-sets as follows

˜ηij|Sk=S = P (b| ˆS, Sk = S) − P (nb| ˆS, Sk = S) =

Γk
2¯Γk

(cid:18)
ηij|Sk=S − 1 − 

(cid:19)

1 + 

.

(A1)

An unique estimate for the latter probability is given by the weighted average ˜ηij|k =
S=±1 ˜ηij|Sk=Sν(Sk = S) where ν(Sk = S) represents the cardinality of the sub-sets.
4. After having calculated ˜ηi,j|k for each node k belonging to the intersection set, I, of the
neighbourhoods of node i and j, there are diﬀerent ways to obtain a new estimate, ˜η1
ij,
for the diﬀerence between the probability of having a bond and the one of not having a

bond. Given the fact that couples are considered as being independent each other, the
posterior probability of ﬁnding a node in the intersection set I of the neighbourhoods
of i and j, denoted as P (∆k

ij), is given by

(cid:80)

P (∆k

ij) =

with P (b| ˆS) =

Γ
2¯Γ



1 + 

(1 + η),

(A2)

where P (b| ˆS) corresponds to the posterior probability of having a bond. We tested
the following three approaches:

(cid:80)

P (bik| ˆS)P (bjk| ˆS)
k∈I P (bik| ˆS)P (bjk| ˆS)
(cid:88)

ij)

˜ηi,j|k P (∆k

(cid:0)˜ηi,j|k
(cid:1)
(cid:18)1 + ˜ηij|k
(cid:89)

k∈I

2

(cid:19)

− 1

43

MS avg: ˜η1

ij =

k∈I

MS min: ˜η1

ij = min
k∈I

MS prod: ˜η1

ij = 2

According to the above prescriptions, ˜ηij|k = 0 represents the threshold above which the

bond between spins i and j is considered genuine and not induced by the fact that the nodes

share common neighbours. The results are shown in Fig. 19 for all investigated topologies.

It is clear that this simple recipe allows for lowering considerably the false positive rate

while keeping the false negative rate almost always below the PLM+ (cid:96)1 curve in the high N

regime. The three approaches exhibit diﬀerent degree of selectivity being MS prod the most

selective and MS avg the least one. Even in hard cases, e.g. star graph and 2D grid in Fig.

19, these corrections proves their eﬀectivness in improving ordinary MS performance in the

high N regime and in doing sometimes even better than PLM+ (cid:96)1.

Appendix B: Additional results on US stock market interactions

As a ﬁrst step in this section we are going to investigate the behaviour of the correlation

functions cN

ij (t, τ ) at varying τ . In particular we focused on the r.m.s of the diagonal and

oﬀ-diagonal elements of the correlation matrices

(cid:115) 2

N (N − 1)

(cid:88)

i<j

2(t, τ )

cN
ij

(B1)

(cid:118)(cid:117)(cid:117)(cid:116) 1

N

N(cid:88)

i=1

cN
diag(t, τ ) =

2(t, τ )

cN
ii

cN
of f (t, τ ) =

as a measure of the mean magnitude of the diagonal and oﬀ-diagonal terms of the correlation

matrices. The time delay τ and the size of the window N are reported in trading days,

whereas the reference point t for the time window in years. In Figs. 20 and 21, we show

cN
diag(t, τ ) and cN
of f (t, τ ) and compare them with the corresponding values for a shuﬄed
version of the data. As one can see from the left panels in these ﬁgures, both the diagonal

and the oﬀ diagonal terms decay very quickly with τ for any given value of t and at N = 200:

they reach the limiting plateau already for τ = 1 or 2 and then oscillate around it. The

right panels in Figs. 20 and 21 display the ﬂuctuating behavior of cN

of f (t, τ )
as a function of t for three increasing values of τ , i.e. τ = 1, 15 and 30, showing that the

diag(t, τ ) and cN

ﬂuctuations for τ = 1 have in general almost the same magnitude as those for τ = 15 and 30

although they seem to be slightly bigger in some time regions especially in the case of

cN
of f (t, τ ). This behavior suggests that on average there is no memory in the data, i.e.
the conﬁgurations at time t are uncorrelated with those at time t + τ already for τ = 1,

although in certain regions a short memory term seems to be present. This conclusion is

further supported by comparing the correlations between the shuﬄed and un-shuﬄed data: a

44

FIG. 20. cN

diag(t, τ ) versus τ and t for N = 200 and comparison with the same quantity calculated

from a shuﬄed version of the data (left); cN

diag(t, τ ) versus t at N = 200 and τ = 1, 15, 30 (right

on the top) and comparison with the same quantity calculated from a shuﬄed version of the data

(right on the bottom)

.

FIG. 21. cN

of f (t, τ ) versus τ and t for N = 200 and comparison with the same quantity calculated

from a shuﬄed version of the data (left); cN

of f (t, τ ) versus t at N = 200 and τ = 1, 15, 30 (right

on the top) and comparison with the same quantity calculated from a shuﬄed version of the data

(right on the bottom)

.

shuﬄing of the dataset provides a new dataset in which the conﬁgurations at diﬀerent times

t are uncorrelated. This comparison shows that the average magnitude of the ﬂuctuations

45

2005200019951990t19853020τ100100.20.40.60.8cNdiag(t,τ)un-shuffled datashuffled dataN = 200tcNdiag(t,τ)0.060.080.1t198019851990199520002005cNdiag(t,τ)0.060.080.1un-shuffled datashuffled dataτ=1τ=15τ=302005200019951990t19853020τ100100.20.40.60.8cNoff(t,τ)N = 200un-shuffled datashuffled datatcNoff(t,τ)0.10.11t198019851990199520002005cNoff(t,τ)0.10.11un-shuffled datashuffled dataτ=30τ=15τ=1FIG. 22. Diagonal and oﬀ diagonal correlations versus the time delay τ in the highest correlated

scenario

.

for the shuﬄed and un-shuﬄed datasets are almost the same and that, as already said above,

exceptions, i.e. bigger ﬂuctuations in the un-shuﬄed dataset for τ = 1, are limited to some

time windows. In the latter cases the conﬁgurations of returns at time t are weakly correlated
with those at time t + 1 but uncorrelated with the conﬁgurations at time t + τ for τ ≥ 2.
As an example of the highest time correlated scenarios, we plot in Fig. 22 cN
diag, τ ) and
of f (t∗
cN
of f that represent the times at
which cN
of f (t, τ = 1) assume their maximum values, namely the values
corresponding to the biggest ﬂuctuations of the blue line in the right top panels of Figs. 20

of f , τ ) as a function of τ for two values of t, t∗

diag(t, τ = 1) and cN

diag(t∗

diag and t∗

and 21. As one can see in this extreme case, both diagonal and oﬀ diagonal terms seem to

be correlated for τ = 1 but then the correlations decay and ﬂuctuate around a plateau for

τ > 1. In Fig.23, we show the correlations for three diﬀerent values of the time window

N = 100, 200, 300 noting the same same general behavior for the ﬂuctuations: the magnitude

of ﬂuctuations for τ = 1 are almost the same or just a little bit bigger in certain regions than

the ones for τ = 15 and τ = 30 meaning that the conﬁgurations decorrelate very quickly in

time. Therefore, this trend seems to be preserved when varying the size of the window.

Given the ﬁnding that the returns are largely uncorrelated for τ (cid:54)= 0, we then focused
ij (t) for each
pair of spins i, j we noticed that, for the majority of the pairs, the connected correlation

ij (t). Looking at the evolution of C N

on equal time connected correlations C N

46

τ0123456789101112131415161718192021222324252627282930CNdiag(t∗diag,τ)00.20.40.60.81τ0123456789101112131415161718192021222324252627282930CNoff(t∗off,τ)0.090.10.110.120.13t∗off=September1995t∗diag=August1998FIG. 23. Same ﬁgures as the right insets of Fig. 20 and 21 for diﬀerent values of the time window

N = 100, 200, 300

.

ﬂuctuates in time around some small value close to zero while for a few of them the connected

correlations take on average values diﬀerent from zero. Indeed looking at the histogram of

the time averaged connected correlation functions C N

ij for each pair shown in Fig. 24, one
can distinguish a big peak around zero and a small but separate one centered at a positive

value. Performing a shuﬄing in time and building again the previous histograms does not

change their shape 3. These couples, exhibiting a bigger average values of the connected

correlations, correspond exactly to those which are recognised by MS as the ones which

are most oftenly linked by a direct connection (see paper). The same connections are also

recognised by PLM+(cid:96)1 and make up the long tail in the histogram of the average values of

the inferred couplings (see left inset of Fig. 25). Finally changing the size of the window

does not inﬂuence the ‘equilibrium’ graph we got:

in other words, this division between

more and less correlated pairs holds for the diﬀerent investigated time scales. Above this

‘equilibrium’ graph, time varying ﬂuctuations occur and determine the temporal evolution.

An eﬀect of non stationarity in the data is highlighted in Fig. 25 (right) for N = 200: the

long tail in the histogram of the ‘bond’-‘no-bond’ ratio is suppressed after a shuﬄing of the

3 This is not obvious since a shuﬄing in time changes data points inside each time window. Therefore the

fact that we found the same histograms enforces our ﬁnding of pairs more correlated than others in time

47

0.080.10.120.14diagonal termst0.140.15off diagonal termscNdiag(t,τ)0.050.060.070.080.090.1tcNoff(t,τ)0.10.11t1980198519901995200020050.040.060.08t1980198519901995200020050.080.09N = 100N = 200N = 300N = 200N = 300N = 100FIG. 24. Histograms of the time averaged connected correlation functions CN

ij for each couple for

three values of N = 100, 200, 300 showing a big peak around zero and a small one a bit separated

representing couples which tend to be more correlated than others

.

dataset which spreads uniformly the time variability along the entire dataset. The clear

modiﬁcation of the histogram after the shuﬄing is a strong evidence of the presence of time

dependent ﬂuctuations in the data.

FIG. 25. Histogram of time averaged inferred couplings Jij showing a long tail for positive values

of Jij (left). Since a clear gap is not evident here and in order to obtain an ‘equilibrium graph’ as in

the other cases, we thresholded Jij at 0.02 (red dotted line) since qualitatively below that value the

distribution seems to be symmetric and centered around zero; histograms of the ‘bond’-‘no-bond’

ratio with a self consistent approach (right top) and its counterpart from a time shuﬄed version

of the data (right bottom)

.

A closer comparison between the absolute values of the connected correlations and the

48

CNij-0.200.20.40.60.81050100150200N = 100CNij-0.200.20.40.60.81050100150200N = 200CNij-0.200.20.40.60.81050100150200N = 300Jij-0.0500.050.10.150100200300400050010001500Nb/Nnb00.020.040.060.080.10.12050010001500un-shuffled datashuffled dataFIG. 26. Relation between ηij(t) and the absolute values of connected correlations CN
ij (t) while
varying i, j and t (left); same plot showing the relation between |Cij(t)| and |Jij(t)| (centre); A
comparison between MS reconstructions and PLM+(cid:96)1 reconstructions for several values of t along

the dataset. The red circles are pairs found to be directly connected by the self consistent approach.
The 94% of them are located beyond the gap along the |Jij(t)| axis meaning that they are also
recognised as direct connections by the PLM+(cid:96)1 approach (right).

.

outcomes of the two methods under investigation is shown in Fig. 26 for N = 200.

In

particular, as one can appreciate by looking at left panel of Fig. 26, MS approach can be

understood as a quantitative and unambiguous way of thresholding connected correlations.

The relation between Jij and the connected correlations is also depicted in Fig. 26 (central
panel). PLM+(cid:96)1 technique creates a gap along the |Jij| axis and the pairs with a value of
|Jij| beyond that gap are assigned a bond, while the others are considered to be disconnected.
Therefore looking at the ﬁgure, not surprisingly, it turns out that higher values of correlations
|Cij(t)| mean direct connections, very low ones are assigned to disconnected pairs and values
in between belong to a transition region (0.1 (cid:46) |Cij(t)| (cid:46) 0.3 ) whose width is more or less the
same as the one observed in the MS case. However, in the PLM+(cid:96)1 case the relation between

the absolute values of the inferred couplings and the ones of the connected correlations is not

as simple as in the MS case, and its complexity reﬂects the more involved recipe underlying

PLM+(cid:96)1 tecnique. Finally, as already discussed in the paper, PLM+(cid:96)1 typically recovers

more bonds than MS, but 94% of the direct connections retrieved with MS using the self

consistent approach are also recognized by PLM+(cid:96)1 as direct connections. This is shown in

the right panel of Fig. 26 and this further emphasizes the rather conservative and relevant

predictions obtained with MS.

Finally in Tab. II, we report here the list of the 41 stocks in the Dow Jones index along

49

Ticker

AA

PEG

AEP

AXP

CNW

AMR

BA

BNI

CAT

C

CNP

CVX

DD

DIS

Name

Alcoa Inc

Public Serv. Enterprise

American Electric Power

American Expess co

Con-Way Inc

American Airline Group Inc

Boeing Company

Burlington Northern Santa Fe

Caterpillar Inc

Citigroup Inc

Center Point Energy

Chevron Corp

Sector

Materials

Utilities

Utilities

Financials

Services

Industrials

Industrials

Industrials

Industry

Aluminium

Electric Utilities

Electric Utilities

Consumer Finance

Trucking

Airlines

Aereospace and Defence

Rail Freight

Industrials

Farm and Construction Machinery

Financials

Utilities

Energy

Banks

Multiutilities

Integrated Oil and Gas

E. I. du Pont de Nemours and Company

Basic Materials

Agricultural Chemicals

The Walt Disney Company

Services

Entertainment - Diversiﬁed

DOW

The Dow Chemical Company

Basic Materials

Chemicals - Major Diversiﬁed

ED

EIX

EK

EXC

FDX

FO

GE

GM

GT

HON

HPQ

Consolidated Edison, Inc

Edison International

Utilities

Utilities

Electric Utilities

Electric Utilities

Eastman Kodak Co.

Consumer Goods

Electronic Equipment

Exelon Corporation

FedEx Corporation

Utilities

Services

Diversiﬁed Utilities

Air Delivery and Freight Services

Fortune Brands, Inc.

Consumer Goods Home and Security, Spirits, and Golf

General Electric Company

Industrial Goods

Diversiﬁed Machinery

General Motors Company

Consumer Goods

Auto Manufacturers - Major

The Goodyear Tire and Rubber Company Consumer Goods

Rubber and Plastics

Honeywell International Inc.

Industrial Goods

Diversiﬁed Machinery

Hewlett-Packard Company

Technology

Diversiﬁed Computer Systems

IBM International Business Machines Corporation

Technology

Information Technology Services

IP

JNJ

KO

LUV

MCD

MMM

MO

MRK

NAVZ

PCG

PG

UTX

WMT

XOM

International Paper Company

Consumer Goods

Packaging and Containers

Johnson & Johnson

Healthcare

Drug Manufacturers - Major

The Coca-Cola Company

Consumer Goods

Beverages - Soft Drinks

Southwest Airlines Co.

McDonald’s Corp.

Services

Services

Regional Airlines

Restaurants

3M Company

Industrial Goods

Diversiﬁed Machinery

Altria Group Inc.

Merck & Co. Inc.

Consumer Goods

Cigarettes

Healthcare

Drug Manufacturers - Major

Navistar International Corporation

Consumer Goods

Trucks and Other Vehicles

PG&E Corporation

Utilities

Electric Utilities

The Procter & Gamble Company

Consumer Goods

Personal Products

United Technologies Corporation

Industrial Goods Aerospace/Defense Products & Services

Wal-Mart Stores Inc.

Services

Discount, Variety Stores

Exxon Mobil Corporation

Basic Materials

Major Integrated Oil and Gas

TABLE II. List of the 41 stocks in the Dow Jones index along with their economic sector. Stocks

belonging to diﬀerent component in the equilibrium graph of Fig. 10 inferred with MS (thick lines)

are highlighted with diﬀerent colours.

with their economic sector. Stocks belonging to diﬀerent connected components of the

equilibrium graph inferred with MS are highlighted with diﬀerent colours.

50

Appendix C: A Matlab implementation of the MS method

Here we report a simple Matlab code for a direct implementation of our approach. The

function receives as an input the mean activities of two nodes and their correlation and gives

back the value of the conﬁdence.

1 % FUNCTION: MODEL SELECTION DISCRIMINATOR

2 % INPUT: MEAN ACTIVITY, CORRELATION AND NUMBER OF POINTS IN THE SAMPLE

3 % OUTPUT: CONFIDENCE

4

5 function eta = MS discriminator(m1,m2,C,N)

6

7 A = [pi sqrt(2)*pi 2*pi piˆ2];

8 dlt = [1 1/2 3/2 2];

9

10 % SOLVING SADDLE POINT EQUATIONS FOR ALL MODELS

11 [fh1] = Mind saddlepoint(m1,N,dlt(1));

12 [fh2] = Mind saddlepoint(m2,N,dlt(1));

13 [fJ] = Mind saddlepoint(C,N,dlt(1));

14 [fh] = Mind saddlepoint((m1+m2)/2,N,dlt(2));

15 [phi] = M2 saddlepoint(m1,m2,C,N,dlt(3));

16 [psi] = M3 saddlepoint(m1,m2,C,N,dlt(4));

17

18 % MODEL WITH 0 PARAMETERS (M 1)

19 PM0 = -log(4);

20

21 % MODELS WITH 1 PARAMETERS (M 2, M 3, M 4, M 6)

22 PM1a = fh1 -log(2) + log(2*pi/(N*(1+dlt(1)/N)*A(1)ˆ2))/(2*N);

23 PM1b = fh2 -log(2) + log(2*pi/(N*(1+dlt(1)/N)*A(1)ˆ2))/(2*N);

24 PM1c = fJ -log(2) + log(2*pi/(N*(1+dlt(1)/N)*A(1)ˆ2))/(2*N);

25 PM1d = 2*fh + log(2*pi/(N*(1+dlt(2)/N)*A(2)ˆ2))/(2*N);

51

26

27 % MODELS WITH 2 INDEPENDENT PARAMETERS (M 5, M 7, M 8)

28 PM2a = fh1 + fh2 + log(2*pi/(N*(1+dlt(1)/N)*A(4)))/N;

29 PM2b = fh1 + fJ + log(2*pi/(N*(1+dlt(1)/N)*A(4)))/N;

30 PM2c = fh2 + fJ + log(2*pi/(N*(1+dlt(1)/N)*A(4)))/N;

31

32 % MODEL WITH 2 DEPENDENT PARAMETERS (M 9)

33 PM2d = phi + log(2*pi/(N*(1+dlt(3)/N)*A(3)))/N;

34

35 % MODEL WITH 3 PARAMETERS (M 10)

36 PM3 = psi + 3*log(2*pi/(N*(1+dlt(4)/N)*A(4)ˆ(2/3)))/(2*N);

37

38 % VECTOR OF PROBABILITIES

39 logPM = [PM0 PM1a PM1b PM1d PM2a PM1c PM2b PM2c PM2d PM3];

40 PM = exp(N*logPM);

41 % comment: consider improving precision for large N

42

43 % EVALUATING THE CONFIDENCE

44 NB = PM(1) + PM(2) + PM(3) + PM(4) + PM(5);

45 B = PM(6) + PM(7) + PM(8) + PM(9) + PM(10);

46 eta = double((B-NB)/(B+NB));

47 end

48

49 % FIND THE SADDLE POINT FOR MODELS WITH 1 AND 2 INDEPENDENT PARAMETERS

50 function [y] = Mind saddlepoint(a,N,delta)

51 xguess = 0;

52 opts = optimset('Diagnostics','off','Display','off');

53 B = (1+delta/N);

54 x = fsolve(@(x) a - B*tanh(x),xguess,opts);

55 y = a*x - log(2*cosh(x));

52

56 end

57

58 % FIND THE SADDLE POINT FOR MODEL M 9

59 function [phi] = M2 saddlepoint(m1,m2,C,N,delta)

60 xguess = zeros(1,2);

61 opts = optimset('Diagnostics','off','Display','off');

62 B = (1+delta/N);

63 x = fsolve(@(x)M2system(x,m1,m2,C,N,B),xguess,opts);

64 h = x(1);

65 J = x(2);

66 fh = h*(m1+m2) - 2*log(2*cosh(h));

67 fJ = J*C - log(2*cosh(J));

68 sigma = (1+tanh(J)*tanh(h)ˆ2)/2;

69 phi = fh+fJ-log(sigma);

70 end

71

72 %FIND THE SADDLE POINT FOR MODEL M 10

73 function [psi] = M3 saddlepoint(m1,m2,C,N,delta)

74 xguess = zeros(1,3);

75 opts = optimset('Diagnostics','off','Display','off');

76 B = (1+delta/N);

77 x = fsolve(@(x)M3system(x,m1,m2,C,B),xguess,opts);

78 h = [x(1) x(2)];

79 J = x(3);

80 fh1 = h(1)*m1 - log(2*cosh(h(1)));

81 fh2 = h(2)*m2 - log(2*cosh(h(2)));

82 fJ = J*C - log(2*cosh(J));

83 sigma = (1+tanh(h(1))*tanh(h(2))*tanh(J))/2;

84 psi = fh1+fh2+fJ-log(sigma);

85 end

53

86

87 function y = M2system(v,m1,m2,C,N,B)

88 y = [m1 + m2 - 2*B*tanh(v(1)) - 2*B*(1-tanh(v(1))ˆ2)*tanh(v(1))*...

89 tanh(v(2))/(1+tanh(v(1))ˆ2*tanh(v(2)));

90

C + 1/(2*N) - B*tanh(v(2)) - B*((1-tanh(v(2))ˆ2)*tanh(v(1))ˆ2)/...

91 (1+tanh(v(1))ˆ2*tanh(v(2)))];

92 end

93

94 function y = M3system(v,m1,m2,C,B)

95 y = [m1 - B*tanh(v(1)) - B*((1-(tanh(v(1)))ˆ2)*tanh(v(2))*tanh(v(3)))/...

96 (1+tanh(v(1))*tanh(v(2))*tanh(v(3)));

97

m2 - B*tanh(v(2)) - B*((1-(tanh(v(2)))ˆ2)*tanh(v(1))*tanh(v(3)))/...

98 (1+tanh(v(1))*tanh(v(2))*tanh(v(3)));

99

C - B*tanh(v(3)) - B*((1-(tanh(v(3)))ˆ2)*tanh(v(1))*tanh(v(2)))/...

100 (1+tanh(v(1))*tanh(v(2))*tanh(v(3)))];

101 end

54

