Particle Gaussian Mixture (PGM) Filters

D. Raihan and S. Chakravorty

Department of Aerospace Engineering

Texas A&M University
College Station, TX

6
1
0
2

 
r
a

 

M
5
1

 
 
]

Y
S
.
s
c
[
 
 

1
v
0
1
5
4
0

.

3
0
6
1
:
v
i
X
r
a

Abstract—Recursive estimation of nonlinear dynamical sys-
tems is an important problem that arises in several engineering
applications. Consistent and accurate propagation of uncertain-
ties is important to ensuring good estimation performance. It
is well known that the posterior state estimates in nonlinear
problems may assume non-Gaussian multimodal densities. In
the past, Gaussian mixture ﬁlters and particle ﬁlters were
introduced to handle non-Gaussianity and nonlinearity. However,
these methods have seen only limited success as most mixture
ﬁlters attempt to ﬁx the number of mixture modes during
estimation process, and the particle ﬁlters suffer from the curse
of dimensionality. In this paper, we propose a particle based
Gaussian mixture ﬁltering approach for the general nonlinear
estimation problem that is free of the particle depletion problem
inherent to most particle ﬁlters. We employ an ensemble of
randomly sampled states for the propagation of state probability
density. A Gaussian mixture model of the propagated uncertainty
is then recovered by clustering the ensemble. The posterior
density is obtained subsequently through a Kalman measurement
update of the mixture modes. We prove the weak convergence of
the PGM density to the true ﬁlter density assuming exponential
forgetting of initial conditions by the true ﬁlter. The estimation
performance of the proposed ﬁltering approach is demonstrated
through several test cases.

I. INTRODUCTION

Rapid advances in the ﬁelds of control and automation
has made it necessary to be able to estimate the state of a
numerous variety of dynamical systems. As a result, there is
growing interest in the recursive and computationally efﬁcient
algorithms for estimating the state and associated uncertainty
in higher dimensional nonlinear systems. A great deal of
prior research is available on static and dynamic estimation of
parameters and systems. The Kalman ﬁlter was proposed as
the unbiased minimum variance estimator for linear dynamical
systems perturbed by additive Gaussian noise [1], [2]. The
extended Kalman ﬁlter (EKF) was introduced to incorpo-
rate nonlinear systems into the Kalman ﬁltering framework
[3]. However,
the limitations of the Jacobian linearization
assumptions and the accumulation of linearization errors often
resulted in the divergence of EKF estimates. The Unscented
Kalman Filter (UKF) and the broader class of sigma point
Kalman ﬁlters provided a derivative free alternative to the
EKF [4]–[6]. It has been shown that
the UKF algorithm
is in fact a linearization of the process and measurement
functions using statistical regression using the sigma points
[7]. In addition to linearizing the system model, both EKF and
UKF approximate the posterior pdf with a single Gaussian pdf.
However, the state pdf in a general nonlinear ﬁltering problem
can be non-Gaussian and multimodal. A Gaussian mixture

approximation of the state pdf was proposed to incorporate
the multimodality of the problem in nonlinear settings [8],
[9]. These approaches however had a major shortcoming as the
number of Gaussian components were kept constant through
out the estimation process. Also the component weights were
updated only during the measurement update. Approaches
to adapting the weights of individual Gaussian modes by
minimizing the propagation error committed in the GMM
approximation have been proposed recently [10]. A different
approach to improving the accuracy of GMM ﬁlters is by split-
ting the Gaussian components during the propagation based on
nonlinearity induced distortion [11]. Both of these approaches
require frequent optimizations, or entropy calculations, to be
performed during the propagation, which signiﬁcantly add to
the overall computational requirement.

The particle ﬁlters (PF) are a class of sequential Monte
Carlo methods that employ an ensemble of states known as
particles to represent the state pdf [12], [13]. These states
are sampled from the initial pdf and propagated forward in
time based on the nonlinear system model. The measurement
updates are performed by assigning weights to individual
particles which may then be resampled. The PF does not
enforce restrictive assumptions on the nature of dynamics or
pdf. Quite often, the measurement updates in particle ﬁlters
result in weight degeneration wherein a signiﬁcant fraction
of particles lose their importance weights. This problem,
termed the “particle depletion” is a major shortcoming of
the particle ﬁlters as it requires the number of particles to
be increased exponentially with the dimension of state space
[14], [15]. Particle based approaches such as the Ensemble
Kalman ﬁlter (EnKF) and the Feedback particle ﬁlter (FPF)
that forego the resampling based measurement update have
been demonstrated to be more effective in higher dimensional
ﬁltering problems involving unimodal pdfs [16], [17].

In this paper, we propose a particle Gaussian mixture ﬁlter
(PGMF), addressing the general multimodal nonlinear ﬁltering
problem. The PGMF design is inspired by a previous work on
a UKF-PF hybrid ﬁlter that was proposed for space object
tracking [18]. The PGMF employs an ensemble of states for
performing the uncertainty propagation. A functional form of
the propagated pdf is then recovered as a Gaussian mixture
model by clustering the states. The posterior pdf is obtained by
performing a Kalman measurement update on the GMM. The
PGMF is conceived to keep track of the nonlinear uncertainty
propagation without performing any additional optimization

and splitting operation during the propagation step. As the
posterior pdf is obtained without employing the particle mea-
surement update,
the PGMF is not prone to the particle
depletion problem and the associated curse of dimensionality.
As the additional clustering step is performed only during the
measurement update step, the PGMF is especially suitable for
ﬁltering in the sparse measurement scenario.

The remainder of this article is organized as follows. An
introductory discussion on mixture model and clustering is
given in section II. The PGM ﬁlter algorithm, and an associ-
ated convergence result, are presented in section III. Details
pertaining to the actual implementation of the proposed ﬁlter
is given in sectionIV. The PGM ﬁlter is applied to three test
cases and compared extensively with the PF, the UKF and the
EnKF in section V.

II. PRELIMINARIES: MIXTURE MODEL FILTERING

Let the state of the dynamical system of interest be denoted
by x ∈ (cid:60)d. We assume that the state of the system evolves
according to a Markov chain whose transition density is
speciﬁed by p(x(cid:48)/x), and assumed to be known. We also
obtain measurements of the state at discrete times n and the
observation model is speciﬁed by the following:

zn = h(xn) + vn,

(1)
where {vn} is a discrete time Gaussian white noise process
with zero mean and covariance Rn. It is very well known that
the ﬁltered density of the state of the Markov chain follows
the following two steps. Let πn−1(x) denote the pdf of the
state after the measurement zn−1. Then, the prediction of the
pdf before the measurement zn at time n (the predicted prior
pdf) is given by:
π−
n (x) =

p(x/x(cid:48))πn−1(x(cid:48))dx(cid:48),

(cid:90)

(2)

which is the law of total probability. Further, after measure-
ment zn is received, the pdf of the state is updated according
to Bayes rule as (the posterior pdf):

(cid:82) p(zn/x(cid:48))π−

p(zn/x)π−

n (x)
n (x(cid:48))dx(cid:48) ,

πn(x) =

(3)

p(z/x) is the measurement likelihood function and can be
inferred from the measurement model above. The prediction
and the update steps above are the key steps to any recursive
ﬁltering algorithm and different ﬁltering approaches are dis-
tinguished by how they perform the above two steps. Let us
assume that a mixture representation has been chosen for the
predicted and posterior pdfs. In particular, let:

M−(n)(cid:88)
M (n)(cid:88)

i=1

i=1

π−
n (x) =

ω−
i (n)p−

i,n(x),

πn(x) =

ωi(n)pi,n(x),

(4)

i (.), pi(.) are standard pdfs, and {ω−

i (n)},{ωi(n)}
where p−
are positive sets of weights that both add up to unity. The
prediction equation for the mixture model then boils down to
the following:

π−
n (x) =

ωi(n − 1)

p(x/x(cid:48))πi,n−1(x(cid:48))dx(cid:48)

.

(5)

M (n−1)(cid:88)

i=1

(cid:124)

(cid:123)(cid:122)

−
i (n)

ω

(cid:90)
(cid:124)

(cid:125)

(cid:123)(cid:122)

pi,n(x)

(cid:125)

Explicitly, the mixture prediction step can be split into the
following discrete and continuous steps:

(cid:90)

i (n) = ωi(n − 1), ,
ω−
p(x/x(cid:48))pi,n−1(x(cid:48))dx(cid:48).
Given an observation zn, the prior mixture π−
formed into the posterior mixture πn(x) as follows:

p−
i,n(x) =

n (x) is trans-

πn(x) =

=

i=1

ω−
i (n)π−
ω−
i (n)π−
ω−
i (n)p(zn/x)π−

i (n)(cid:82) p(zn/x(cid:48))π−

i,n(x)
i,n(x(cid:48))dx(cid:48)
i,n(x)
i,n(x(cid:48))dx(cid:48) .

i=1

p(zn/x)(cid:80)M−(n)
(cid:82) p(zn/x(cid:48))(cid:80)M−(n)
(cid:80)M−(n)
(cid:80)M−(n)
(cid:90)

ω−

i=1

i=1

li(n) ≡

p(zn/x(cid:48))π−

i,n(x(cid:48))dx(cid:48).

Deﬁne the likelihood that zn comes from the ith mixture
component as:

Rearranging the above mixture expression using the deﬁnition
of the component/ mode likelihood gives us:

M−(n)(cid:88)

i=1

πn(x) =

(cid:80)
w−
(cid:123)(cid:122)
(cid:124)
i (n)li(n)
j w−

j (n)lj(n)

(cid:125)

(cid:124)

wi(n)

p(zn/x)π−
li(n)

(cid:123)(cid:122)

πi,n(x)

(cid:125)

i,n(x)

.

(10)

The above expression clearly shows that the measurement
update has a hybrid nature, a standard update of the indi-
vidual modes of the mixture with the measurement zn, and
a discrete Bayesian update of the mode weights using the
mode likelihoods li(n). Note further that the mode likelihoods
are the Bayes’ normalization factors for the individual modes.
Moreover, note that there are no approximations whatsoever in
the above mixture update equations. Explicitly, we delineate
the discrete and continuous updates of the mixture model
below:

(6)

(7)

(8)

(9)

(cid:80)
w−
i (n)li(n)
j w−
p(zn/x)p−
li(n)

j (n)lj(n)
i,n(x)

, ,

.

ωi(n) =

pi,n(x) =

(11)

(12)

It behooves us to take a closer look at the hybrid prediction
equations 6 and 7 as well as the hybrid update equations 11 and
12. It has been shown that a Gaussian mixture model(GMM)
can be used to approximate a general continuous non-Gaussian
pdf to any degree of accuracy [8]. The GMMs also inherit

many of the desirable properties of Gaussian densities which
makes their analysis easier. In ﬁgure 1, a weighted Gaussian
sum is used to represent a multimodal pdf. Let us assume that
we have ﬁxed the form of the mixture model to a Gaussian
Mixture Model (GMM), i.e., the posterior pdf at time n − 1
can be represented by the GMM:

πi,n−1(x) = G(x; µi(n − 1), Pi(n − 1)),

(13)
where G(x; µ, P ) represents the Gaussian pdf with mean µ
and covariance P . Consider ﬁrst the prediction equations. Note
that from the way it has been written, the number of mixture
components at time n−1, M (n−1), is the same as the number
of mixture components of the prediction at time n, M−(n).
However, this assumes that the prediction of the ithGaussian
component πi,n−1 of the posterior pdf at time n − 1 remains
a single Gaussian at time n, π−
i,n. However, this is, in general,
not true. The number of mixture components necessary to
approximate the state pdf may vary from one time step to the
other. For example, consider the nonlinear dynamical system
given by

represents an expectation of

where Pi,zx(n) = Ei[h(X) − Ei(h(X))(X − Ei(X))(cid:48)]
,Pi,zz(n) = Ei[(h(X) − Ei(h(X))(h(X) − Ei(h(X))(cid:48)],
and Ei[f (X)]
the function
f (X) with respect to the random variable X where X ∼
G(x; µ−
i (n), P −
i (n)). However, this update is not necessarily
correct. Similar to the prediction case, in general, a single
predicted Gaussian component can split into multiple modes
after the Bayesian update 12, i.e., the updated component
itself is a GMM. An illustration of this is given in ﬁgure.
2b. In this case we have a prior ensemble generated from
π(x) = G(X,
). Then, a noisy measurement
0
z = 2 is recorded where

(cid:20)0
(cid:21)

(cid:20)1

(cid:21)

0
2

0

,

z = x2

1 + τ,
τ ∼ G(x, 0, 2).

(18)

An ensemle for the posterior pdf π(x|z) is obtained through
resampling and is seen to split
into two separate modes.
Hence, just as in the prediction step, there is a need to deal
with the time varying number of GMM components after an
update.

(cid:20) ˙x1

(cid:21)

˙x2

=

(cid:21)
(cid:20) − x1
(cid:20)−12
(cid:21)

2
sin( x2
2 )

π0(X) =G(X,

+ Γ(t),

(cid:20)0.2

0

,

0

(cid:21)

0
1

).

(14)

(15)

In ﬁgure 2a, the locations of 200 particles sampled from
the initial pdf π0(x) as they evolve through the dynamics of
the system are plotted. The particles are seen to separate into
two distinct modes as time progresses. Hence, in order to use
mixture models for prediction, we have to ﬁnd a way to tackle
the problem of time varying number of GMM components.
Next, let us consider the measurement update equations 11

(a)

(b)

Fig. 1: Gaussian Mixture representation of a multimodal PDF

Fig. 2: Formation of multimodality a) through dynamics b)
through measurement update

and 12. The discrete weight update Eq. 11 is quite clear,
however, it behooves us to take a closer look at the continuous
mode update Eq. 12. Since the prior component is Gaussian,
and the update Eq. 12 could be done simply using the Kalman/
least squares update, i.e.,:
i (n) + P −1(cid:48)

µi(n) = µ−

i,zx(n)P −1
i (n) − P −1(cid:48)

i,zz(n)(zn − Ei[h(X)]),
i,zx(n)P −1
i,zx(n),

i,zz(n)P −1

Pi(n) = P −

(16)
(17)

For ease of treatment and clarity of exposition, we shall
not consider the measurement update aspect of the GMM
ﬁltering problem in this paper, which will be treated in a
companion paper. Hence, we make the following assumption
for the remainder of the paper.
Assumption 1. We shall assume a Gaussian mixture rep-
resentation for the predicted and posterior ﬁltered densities.

Further, we assume that given a predicted mixture component
at time n, G(x; µ−
i (n)), the update Eq. 12 after an
observation zn is approximated arbitrarily well by the Least
Squares/ Kalman update Eq. 16-17.

i (n), P −

III. THE PARTICLE GAUSSIAN MIXTURE (PGM) FILTER
In this section, we ﬁrst present
the PGM ﬁlter. In the
following subsection, we show the weak convergence of the
PGM ﬁlter density to the true ﬁlter density under the condition
of exponential forgetting of initial conditions by the true ﬁlter.
Finally, we also compare and contrast the PGM algorithm with
other mixture based nonlinear ﬁlters, in particular, the PF, the
Gaussian Mixture Filter (GMF) and the EnKF.

A. The PGM Algorithm

The basic assumption underlying the PGM algorithm is
that the predicted prior and posterior ﬁlter densities can be
represented using a GMM. In particular, let:

π−
n (x) =

i (x; µ−

i (n), P −

i (n)),

M−(n)(cid:88)
i (n)G−
ω−
M (n)(cid:88)

i=1

(19)

(20)

πn(x) =

ωi(n)Gi(x; µi(n), Pi(n)).

i=1

In general, M−(n) and M (n) need not be the same, however,
owing to Assumption 1, they are assumed to be equal for the
purposes of this paper. For instance, given a linear measure-
ment function, this is true. The PGM ﬁltering algorithm is
composed of three basic steps that are described below.

1) Sampling: The PGM ﬁlter assumes the availability of the
Markov transition kernel p(x/x(cid:48)) using which it can draw
samples of the next state x given that the current state is
x(cid:48). The ﬁrst step in the PGM algorithm is the use of the

Algorithm 1 PGM Algorithm

Given π0(x) = (cid:80)M (0)

i=1 ωi(0)Gi(x; µi(0), Pi(0)),

transition

density kernel p(x(cid:48)/x), n = 1.
a) Sample Np particles X (i) from from πn−1 and the transition

kernel p(x(cid:48)/x) as follows:
i) Sample X (i)(cid:48)
from πn−1(.).
ii) Sample X (i) from p(./X (i)(cid:48)
b) Use a Clustering Algorithm C to cluster the set of particles
{X (i)} into M−(n) Gaussian clusters with weights, mean
and covariance given by {w−
c) Update the mixture weights and the mixture means and
covariances to {ωi(n), µi(n), Pi(n)}, given the observation
zn, utilizing the Kalman update Eqs. 16, 17.

i (n), P −

i (n), µ−

i (n)}.

).

d) n = n+1, go to Step 1.

transition kernel to generate a set of samples at the next
time step (which is the same as in a Particle ﬁlter).
2) Clustering: Then, we use a clustering algorithm C to
partition the set of points into M−(n) different clusters
whose means and covariances can be evaluated using

sample averaging. Clustering is a ﬁeld of Machine learn-
ing termed as Unsupervised Learning [19], [20]. There
are many different algorithms that can be used for cluster-
ing, for instance K-means clustering [21], EM clustering
[22] among others. In the basic clustering algorithms, the
number of clusters have to be speciﬁed [23], [24] while
the more advanced techniques also estimate the number of
clusters [25]. In the experimental results presented in this
paper, we use the simple K-means clustering algorithm,
which is computationally very inexpensive while still
being able to give good results for well separated clusters.
The k-means clustering is a popular approach to partition-
ing wherein the data set is grouped into different clusters
so that the sum of squares of within-group distances is
minimized,i.e, the data set S is partitioned into M clusters
M = {S1,··· , SM} such that
G∗
M(cid:88)
(cid:88)

(cid:107)xi − µi(cid:107)2.

(21)

G∗
L = argmin

GL

i=1

xj∈Si

Here GM denotes any partition of the set S into M
clusters and µi represents the mean of the elements of
the ith cluster in that partition. Once the vectors xi
are assigned into different clusters, an M mode GMM
describing the set S may be derived as follows.

(xj − µi)(xj − µi)T .

Here 1(.) represents the indicator function. In ﬁgure 3, a
set of points are chosen from the two dimensional space
and clustered using the k-means algorithm to obtain a
GMM.

3) Measurement update: Incorporate the measurement in-
formation by updating the means and covariances of all
M modes individually using a least squares/ Kalman
measurement update. Also update the mixture weights
using the mode likelihoods li(n) as in Eq. 11.

B. Analysis of the PGM Filter

In the following, we analyze the PGM ﬁlter. We show that
under the assumption of a perfect clustering scheme C, the
PGM ﬁlter density converges weakly to the true ﬁlter density.
Let Fzn (πn−1) = πn denote the true ﬁlter density at time n
given that the ﬁlter density at time n − 1 is πn−1 and the
observation at time n is zn. Further, let ˆFzn (πn−1) denote the
ﬁlter density approximated by the PGM ﬁlter. We make the
following exponential forgetting assumption on the true ﬁlter.
Assumption 2. We assume that there exists C < ∞ and ρ < 1

1(xj ∈ Si),

N(cid:88)

j=1
1
N
1
ni

ni,

(cid:88)

xj∈Si
1
ni − 1

xj,

(cid:88)

xj∈Si

ni =

wi =

µi =

Ci =

(22)

(23)

(24)

(25)

Note that the different terms on the RHS above are:
∆n = ˆFn(ˆπn−1) − Fn(ˆπn−1),
∆n−1 = Fn( ˆFn−1(ˆπn−2)) − Fn(Fn−1(ˆπn−2)), ....
∆1 = Fn..F2( ˆF1(π0)) − Fn..F2(F1(π0)).

Using Assumption 2 and the fact
that
Fzn (ˆπn−1)|| ≤ , for all n, it follows that:

||∆i|| ≤ Cρn−i,

(28)
|| ˆFzn (ˆπn−1) −

and thus,

||ˆπn − πn|| ≤ n(cid:88)

i=1

Cρn−i ≤ C
1 − ρ

.

(29)

The above result also holds for initial conditions in the inﬁnite
past, i.e., at n = −∞. In the following, we assume that the
initial condition was in the inﬁnite past.
Lemma 2. Let Assumptions 2 and 3 hold. Given any δ, ν > 0,
there exists an N < ∞, such that:

(30)

1 − ρ

(1 + ν)C

M (1−ρ) .

) ≤ N δ,

and f = νC

P rob(||ˆπn − πn|| >

i=−∞ ρn−i = f,

where N = n − n(cid:48), and n(cid:48) is such that (cid:80)n(cid:48)
Fzk ( ˆπk−1)||. It follows that en =(cid:80)n
Proof: Let en = ||ˆπn − πn||, and let k = || ˆFzk (ˆπk−1) −
such that(cid:80)n(cid:48)
k=−∞ ρn−kk. Choose n(cid:48)
n(cid:48)(cid:88)

i=−∞ ρn−i ≥ f, where f = νC
(cid:125)

n(cid:88)
(cid:124)

M (1−ρ). Then:

ρn−kk

ρn−kk

(cid:123)(cid:122)

k=−∞

en =

(cid:125)

+

(cid:124)

(31)

k=n(cid:48)

.

(cid:123)(cid:122)

¯en

∆n

From Assumption 3, it follows that P rob(||∆n|| > f M ) = 0,
and thus,

P rob(||∆n|| >

νC
1 − ρ

) = 0.

Similarly, from Lemma 1, it follows that:

P rob(¯en >

C
1 − ρ

) ≤ (n − n(cid:48))δ ≡ N δ.

(32)

(33)

Using the equations 32 and 33, it follows that P rob(en >
(1+ν)C

) ≤ N δ.

1−ρ

(cid:80)N

Lemma 3. Given a random variable X, and a function f (X)
such that E(f (X)) = ¯f, and V ar(f (X)) = σ2
f , let ˆf =
i=1 f (Xi), where Xi are samples of the r.v. X. For large
1
N
enough N, it follows that:

P rob(| ˆf − ¯f| > ) <

√
N√
2πσf

− 2N
2σ2
f .

e

Proof: It can be shown easily that E( ˆf ) = ¯f, and
σ2
V ar( ˆf ) =
N . Application of the Central Limit Theorem
f
implies that ( ˆf − ¯f ) is Normal distributed with mean E( ˆf ) −

Fig. 3: (a) Set of points in the 2-d Euclidean space (b) Contour
plots of the GMM components obtained by clustering the
points into three groups using k-means algorithm

0||,

0)|| ≤ Cρn||π0 − π(cid:48)
(26)
0, and

such that:
||Fzn Fzn−1..Fz1(π0) − Fzn Fzn−1..Fz1(π(cid:48)

for any measurement sequence {z1, z2,··· zn}, any π0, π(cid:48)
where ||.|| denotes the L1 norm.

Similarly let ˆFzn

ˆFzn−1 ··· ˆFz0(π0) denote the ﬁltered den-
sity approximated by the PGM ﬁlter given the measurement
sequence {z1, z2,··· zn} and the initial density π0.
Assumption 3. Let P rob(|| ˆFzn (ˆπn−1)− Fzn (ˆπn−1)|| > ) <
δ, for all n. Further, we asume that P rob(|| ˆFzn (ˆπn−1) −
Fzn (ˆπn−1)|| > M ) = 0, for all n, for some M < ∞ (the
error in a one step approximation of the ﬁlter density is almost
surely uniformly bounded over all time).
Lemma 1. Let || ˆFzn (ˆπn−1) − Fzn (ˆπn−1)|| ≤ , for all n.
Under Assumption 2, it follows that ||ˆπn − πn|| ≤ C
1−ρ .

Proof: We have:

(cid:123)(cid:122)

ˆπn − πn = ˆFzn
(cid:124)

ˆFzn−1.. ˆFz1(π0) − FznFzn−1 ...Fz1(π0),
(cid:125)
(cid:124)
= [ ˆFn ˆFn−1.. ˆF1(π0) − Fn ˆFn−1.. ˆF1(π0)]
(cid:125)
(cid:125)

+··· + [FnFn−1.... ˆF1(π0) − Fn..F1(π0)]

+ [Fn ˆFn−1.. ˆF1(π0) − FnFn−1 ˆFn−2.. ˆF1(π0)]

(cid:123)(cid:122)

(cid:123)(cid:122)

∆n−1

(cid:124)

∆n

∆1

. (27)

predicted state of the system. Let fm(X−), m = 1, 2··· M
the functions corresponding to the various
denote the all
elements in the mean and covariance of X−. Choose:
m,j = V ar(fm(X−)).

m,j, where σ2
σ2

(41)

¯σ2
j = max
m

If we choose N > N j

(cid:48),δ(cid:48) such that
√
− 2N
N√
2¯σ2
e
2π¯σj

j < δ(cid:48),

(42)

then it follows from Lemma 3 that P rob(| ˆfm − ¯fi,m| >
(cid:48)) < δ(cid:48), for all m, where ˆfi,m is the sample average of the
parameters ¯fm corresponding to the different elements of the
mean and covariance. Such N j
(cid:48),δ(cid:48) can be found for all clusters
Cj and given that we choose N(cid:48),δ(cid:48) as:

¯f = 0, and variance V ar( ˆf ). The result then immediately
follows.

The two results above establish that if the sampling error
at each step in the ﬁlter is small enough, and under the
condition of exponential forgetting of initial conditions, then
the true ﬁlter density can be approximated arbitrarily closely
with arbitrary high conﬁdence. In the following, we establish
that the sampling error at each step in the PGM ﬁltering
process can be arbitrarily small and thus, it follows from the
two results above that the PGM ﬁlter can approximate the true
ﬁlter density with arbitrarily high accuracy and arbitrarily high
conﬁdence. First,we deﬁne the following:

i (n), ˆP −

i (n)),

(34)

P (ˆπn−1) ≡ ˆπ−

n =

ˆP (ˆπn−1) ≡ ˆˆπ−

n =

i=1

i (n)Gi(x; ˆµ−
ˆω−

M−(n)(cid:88)
M−(n)(cid:88)
i (n)G(x; ˆˆµ−
ˆˆω−
M (n)(cid:88)
M (n)(cid:88)

i=1

i=1

i (n),

ˆˆP −
i (n)),

(35)

N(cid:48),δ(cid:48) = max

j

N j
(cid:48),δ(cid:48)
ˆω−

j

, ,

(43)

Fzn(ˆπn−1) =

ˆωi(n)G(x; ˆµi(n), ˆPi(n)),

(36)

ˆFzn (ˆπn−1) =

ˆˆωi(n)G(x; ˆˆµi(n),

ˆˆPi(n)).

(37)

i=1

The above represent
the true and the approximate PGM
predicted and ﬁltered densities at time n given the approximate
density ˆπn−1 at time n − 1. We have the following result:
Lemma 4. Given the GMM representation of the prior pdf
above, and a perfect Clustering algorithm C, given any (cid:48) > 0,
andy δ(cid:48) > 0, there exists an N(cid:48),δ(cid:48)(n) < ∞ such that: if the
number of samples used to approximate the predicted pdf at
time n is greater than N(cid:48),δ(cid:48)(n) then:
i (n) − ˆω−
i (n) − ˆµj−
(n) − ˆP jk−

i (n) > (cid:48)) < δ(cid:48),
i (n)| > (cid:48)) < δ(cid:48),
(n)| > (cid:48)) < δ(cid:48),

P rob(|ˆˆω−
P rob(|ˆˆµj−
P rob(| ˆˆP jk−

(38)
(39)

(40)

i

i

for all i, j, k, where ˆµj−
i and ˆP jk−
mean vector ˆµ−
the covariance matrix ˆP −
i .

i

i

represents the jth element of the
represents the (j, k)th element of

Proof: Given a sample X−
i ,

the clustering algorithm
assigns the sample to a particular cluster Cj. The cluster Cj is
the correct one under the assumption of a perfect Clustering
algorithm. Let Nj denote the number of samples assigned to
Cj among all the N samples. Further, let us assume that N
is large enough such that Nj
(we drop the explicit
reference to the time n for notational convenience in the fol-
lowing). This assumption can be relaxed in a straightforward
fashion but at the cost of a more tedious development which
we forego here for the sake of clarity.
Consider the cluster Cj: its mean and covariance ˆµ−
j are
calculated from the Nj samples. Estimates of all the elements
of the mean and covariance functions are ensemble averages
of particular functions of the random variable X−(n), i.e, the

N ≈ ˆω−

j , ˆP −

j

is guaranteed that all

it
the elements of the mean vector
ˆµ−(n) and the covariance matrix ˆP −(n) can be estimated
to an accuracy of (cid:48) with conﬁdence of at least 1 − δ(cid:48), which
completes the proof of the result.

It may be shown that under Assumption 1 that the Kalman
update is an arbitrarily accurate approximation of the true
update, the error incurred in estimating the posterior mean
and covariance ˆµi(n), ˆPi(n) is at most K(n)(cid:48), for some time
varying K(n) < ∞ which depends on the posterior mean and
covariance, given that the predicted prior means and covari-
ances of the clusters of the GMM have been approximated to
an accuracy of (cid:48). This can be summarized in the following
result:
Lemma 5. Given any (cid:48), δ(cid:48) > 0, choose N(cid:48),δ(cid:48)(n) according
Eqs. 41-43. If the number of samples used in the PGM ﬁlter to
approximate the predicted prior pdf at time n is greater than
N(cid:48),δ(cid:48)(n) then, there exists k(n) < ∞ s.t:

P rob(|ˆˆωi(n) − ˆωi(n) > K(n)(cid:48)) < δ(cid:48),
i (n)| > K(n)(cid:48)) < δ(cid:48),
P rob(|ˆˆµj
P rob(| ˆˆP jk
i (n)| > K(n)(cid:48)) < δ(cid:48),

i (n) − ˆµj
i (n) − ˆP jk

(44)
(45)

(46)

for all i, j, k.

Next, we ﬁnd a bound on the L1 error between the estimated
and true ﬁltered densities given the error between the param-
eters of the GMM representing the true and the approximate
ﬁltered densities.
Lemma 6. Let |ˆˆωi(n) − ˆωi(n)| < (cid:48), |ˆˆµj
i (n)| < (cid:48),
and | ˆˆP jk
i (n)| <  for all i, j, k. Then , given that
the state of the system x ∈ (cid:60)d, there exists C(n) < ∞ such
that ||ˆˆπn − ˆπn|| < C(n)d(cid:48).

i (n) − ˆP jk

i (n) − ˆµj

Proof: We show the result for the case of a simple one
component Gaussian with an error in the covariance, it can
be generalized to the GMM in a relatively straightforward
fashion but at
the expense of a very tedious derivation

which we forego here for clarity. We also suppress the
explicit dependence on time n in the following for notational
convenience.

chosen such that:

C(n)K(n)(cid:48)(n)d = ,
δ
N

δ(cid:48)(n) =

,

(51)

(52)

and the corresponding number of samples N(cid:48)(n),δ(cid:48)(n)(n)
be chosen according to Eqs. 41-43,
follows that
||P rob|| ˆFzn (ˆπn−1) − Fzn(ˆπn−1)|| > ) ≤ δ
N .

then it

Proof: Recall

that ˆπn = Fzn (ˆπn−1), and ˆˆπn =
ˆFzn (ˆπn−1). Then, from Lemma 6 we have that ||ˆπn − ˆˆπn|| ≤
C(n)K(n)d(cid:48)(n) if |ˆθi(n) − ˆˆθi(n)| < (cid:48)(n) for all i, where
ˆθi(n) represents the true parameters underlying the GMM
representation of ˆπn and ˆˆθi(n) represents their PGM approx-
imation. Hence:

P rob(||ˆπn − ˆˆπn|| > C(n)K(n)d(cid:48)(n)) > 1 − δ(cid:48)(n),

(53)
which owing to the deﬁnition of (cid:48)(n) and δ(cid:48)(n) leads us to
the desired result.

Hence, using Corollary 1 and Lemma 2, it follows that if the
number of samples used to approximate the parameters of the
predicted GMM pdf at time n is greater than the N(cid:48)(n),δ(cid:48)(n),
) ≤ δ, for
then it follows that P rob(||ˆˆπn − ˆπn|| > (1+ν)C
1−ρ
all n for any arbitrarily small , δ, ν > 0. However, in order
for Assumption 3 to be valid, the estimates ˆˆθn have to be
almost surely bounded. To show this, due to the Strong Law
n → ˆθn as N → ∞,
of Large Numbers, it is also true that ˆˆθN
where ˆˆθN
n represent the estimate of the parameters after N
samples. Given the sample size is large enough, the estimate
ˆˆθN
n is arbitrarily close to the true parameters ˆθn almost surely,
and thus, since the true parameters are bounded, so are the
estimates. This may be summarized in the following result.
Proposition 1. Let Assumptions 1 and 2 hold. Given a perfect
clustering algorithem C, and any , δ, ν > 0, at every time step
n, choose the required accuracy of the approximation (cid:48)(n)
from Eq. 51, the required conﬁdence δ(cid:48)(n) from Eq. 52, and
the corresponding minimum number of samples N(cid:48)(n),δ(cid:48)(n)
from Eqs. 41-43, then:

P rob(||ˆˆπn − ˆπn|| >

(1 + ν)C

1 − ρ

) ≤ δ.

(54)

Several remarks are in order here.

Remark 1. The above result establishes the weak convergence
of the approximate PGM ﬁlter density to the true ﬁltered
density uniformly over all time under the assumptions of expo-
nential forgetting of the initial conditions and the adequacy of
the Kalman update to approximate the true Bayesian update
in the ﬁltering equations. In the absence of the exponential
forgetting condition, the convergence result can be obtained
only for a ﬁnite number of time steps, the development being
almost identical. In the absence of Assumption 1, the adequacy
of the Kalman update, the result still holds except that there

(cid:90)

×

ˆˆπ(x) − ˆπ(x) =

1

(2π)d/2| ˆˆP|1/2

e− 1

2 (x−µ)(cid:48) ˆˆP −1(x−µ)−

1

(2π)d/2| ˆP|1/2

e− 1

2 (x−µ)(cid:48) ˆP −1(x−µ),

≈

1

(2π)d/2| ˆP|1/2
× 1
2

e− 1

2 (x−µ)(cid:48) ˆP −1(x−µ)

(x − µ)(cid:48)( ˆP −1∆ ˆP −1)(x − µ),

where ˆˆP = ˆP + ∆ and since

e− 1
(x−µ)
2 (x−µ)(cid:48)( ˆP +∆)
2 (x−µ)(cid:48) ˆP −1(x−µ)e− 1
2 (x−µ)(cid:48) ˆP −1∆ ˆP −1(x−µ),

−1

≈ e− 1

(47)

(48)

which in turn implies Eq. 47. This in turn implies that:
×

||ˆˆπ − ˆπ|| ≈

1

(2π)d/2| ˆP|1/2

(cid:90)

e− 1

2 (x−µ)(cid:48) ˆP −1(x−µ) 1
2

(x − µ)(cid:48) ˆP −1∆ ˆP −1(x − µ)dx,

≤

C( ˆP )(cid:48)

(2π)d/2| ˆP|1/2

1

there

e− 1

2 (x−µ)(cid:48) ˆP −1(x−µ) 1
2

exists C( ˆP ) < ∞ such that

(x − µ)(cid:48) ˆP −1(x − µ)dx, (49)
2 (x −
since
µ)(cid:48) ˆP −1∆ ˆP −1(x− µ) ≤ C( ˆP )(cid:48) 1
2 (x− µ)(cid:48) ˆP −1(x− µ), owing
to the fact that ||∆|| < L(cid:48) for any suitable norm on the
perturbation ∆.
Now, let Y = ˆP −1/2(X − µ). Then, it follows that:
||ˆˆπ − ˆπ|| ≤ C( ˆP )(cid:48)

e−1/2y(cid:48)yy(cid:48)ydy = C( ˆP )(cid:48)d.
(50)
The last step in the above equation follows from noting that
Y (cid:48)Y is a chi-squared random variable of degree of freedom d
and thus, its expected value is d. This establishes our result.
In general for a GMM, the constant C(n) would depend on
the means and covariances of all the GMM components and
their weights.

(2π)d/2

(cid:90)

1

Lemma 5 and 6 immediately lead us to the following

corollary.
Corollary 1. Let (cid:48)(n) be the desired accuracy in estimating
the parameters of
i.e.,
the true ﬁltered density given observation zn and the PGM
posterior pdf at the previous time ˆπn−1. Let δ(cid:48)(n) be the
desired conﬁdence of the estimate. IF (cid:48)(n) and δ(cid:48)(n) are

the GMM representing Fzn (ˆπn−1),

is a new error incurred in sampling the posterior pdf, which
will be covered in the companion paper.
Remark 2. The analysis above shows that the number of
samples required at any time step to ensure the accuracy of
the ﬁlter depends on the current predicted and posterior pdfs,
and thus, in general, have to be time varying. This is a fact
that is typically ignored in other mixture ﬁlters such as the PF
and the GMF.
Remark 3. The Curse of Dimensionality: It can be seen from
the analysis above, in particular Eqs. 41-43, that the number of
samples required to estimate the parameters of the predicted
and posterior pdfs accurately is completely independent of
the dimension of the state space, and thus, is free from the
”Curse of Dimensionality”. However, we have to be more
careful regarding the functional L1 error in the PGM density:
Eq. 51 shows that the accuracy parameter required at every
time step is inversely proportional to the dimension of the
since (cid:48)(n) =
C(n)K(n)d , and thus, in order to attain the
same accuracy in terms of the functional error of the ﬁltered
density, Eq. 42 shows that the number of samples have to
increase as O(d2) where d is the dimension of the problem.
Further, it should also be noted that the computation of the
sample averages required by the PGM ﬁlter grows as O(d2).
Remark 4. Compressive Assumption: The assumption of a
ﬁnite Gaussian Mixture, is, in our opinion, a compressive
argument. We restrict the set of parameters describing the
predicted random variable to a ﬁnite number that have to
be estimated using sample averages of the predicted random
variable. The variance of these random samples is always
bounded because of the ﬁniteness assumption, and thus, the
number of samples required to estimate the parameters is
independent of the dimension of the problem. In general, if
we were to ﬁnd the moments of a random variable from its
samples, we need to ﬁnd all the moments via their sample
averages. The variance of the samples of these higher order
moments are, in general, not bounded, thereby requiring an
inﬁnite number of samples to estimate the pdf.



C. Relationship to other Nonlinear Filters

In this section, we compare and contrast the PGM ﬁlter with
other nonlinear ﬁlters, in particular the PF, the EnKF and the
GMF in detail.
The prediction stage of the PF is the same as the PGM except
that the PF does not get a GMM from the set of predicted
particles, and directly uses the Bayesian update on the indi-
vidual particles, i.e, weights every particle with its likelihood
p(zn/Xi). In the update step lies the computational trouble
inherent
to the PF, also known as the “particle depletion
problem”: as the number of dimensions increase, it gets in-
creasingly hard to sample particles with high likelihood, in fact
the number of particles goes up exponentially with the number
of dimensions thereby subjecting the PF to the curse of dimen-
sionality. Consider the simple one dimensional example shown
in ﬁgure 4. In this case, a set of 400 particles are sampled

Fig. 4: Particle Depletion

from the prior pdf π(x) = G(x, 11, 0.3). The measurement
likelihood function is assumed to be lz(x) = G(x, 15, 0.1).
Since the two pdfs are widely separated, nearly all the weight
is allocated to a single particle as observed in the histogram of
normalize weights in ﬁg 4. Please see the references [14], [26],
[27] for more rigorous insight. In contrast, the PGM uses the
Kalman update for the GMM components and thereby does
not suffer from the particle depletion problem. Moreover, as
has been shown in the previous section, the number of samples
required by the PGM is not dependent on the dimension of
the problem. In essence, the Kalman update can be thought
of as an automatic method to control/ move the predicted
particles to the correct regions of the state space given the
observation. In fact, this is the philosophy used in the EnKF
[16], [28] that perturbs each of the predicted particles using
the measurement to obtain a perturbed ensemble of points that
actually samples the posterior density. However, the EnKF
always assumes a unimodal Gaussian for its predicted and
posterior ﬁlter densities. At a more minor level, in PGM, we
actually do the mean and covariance update of the components
using the Kalman update equation rather than perturbing the
ensemble of predicted particles [29]. Using the particle based
Kalman Filtering method such as the EnKF, we see that at
least the calculation of the mean and covariance is independent
of the dimension of the state space, and this is precisely the
reason why the EnKF is used regularly for the ﬁltering of
PDEs such as those arising in Meteorology and Geophysics
[30] where even the EKF or UKF are intractable.

The earliest GMFs were a bank of parallel EKFs but the
number of modes in the GM was always ﬁxed through both
the prediction and the update steps [9]. This, as has been
noted in Section II, can be quite restrictive as it only considers
multi-modality arising from initial conditions and never from
the prediction and update steps. Other GMFs have more
sophisticated methods for updating the weights of the GMM
using the Fokker-Planck equation [10] but keeps the number
of modes ﬁxed nonetheless. Recent GMF algorithms have also
focused on time varying number of modes and used various
heuristics to decide when to split a particular Gaussian into
multiple components [11]. Most of these methods use typical

Kalman ﬁltering propagation methods such as in the EKF/
UKF to propagate as well as split the Gaussian components.
In contrast, we use a particle ensemble of the predicted random
variable along with a clustering algorithm to conveniently
ﬁnd the number as well as the mean and covariances of
the component clusters. In particular, we feel that the PGM
harnesses the strength of the PF, the particle prediction step,
along with the strength of the Kalman update in GMFs, using
clustering algorithms, to develop a technique that is free from
the weaknesses of either technique.

IV. PGM FILTER IMPLEMENTATION

In this section, we discuss the computations involved in the
actual implementation of the PGM ﬁlter in detail. We separate
these computations into the following four broad categories.
1) Sampling: Let the posterior pdf at time n be given by

M (n)(cid:88)

πn(x) =

ωi(n)Gi(x; µi(n), Pi(n))

(55)

i=1

of

Sn

Np

n+1 = {x1−

an
ensemble
n }
n,··· , xNp
n,··· , xi

n,··· , wNp
n,··· , wi
xi−
n+1 = f (xi

states
Draw
{x1
from the GMM πn(X).
The set Sn represents the uncertainty involved in the
state estimate at n. Draw Np independent samples of the
process noise term w(n) from its density PW (w) to get
Swn = {w1

n }. Let
(56)
n) + wi
n.
n+1,··· , xi−
n+1,··· , xN−
n+1} ob-
Then the set S−1
tained is distributed according to π−
n+1(x) and represents
the uncertainty in the propagated random variable in
discrete form.
2) Clustering: Before proceeding to the measurement up-
date step, a new GMM is ﬁt to the set S−
n+1 using a
clustering algorithm of choice. However, if a measure-
ment update is not to be performed, this ensemble S−
n+1
is set to be Sn+1 which can be propagated further by
drawing a new set of noise terms and substituting in
the dynamic model as described above. The objective
of clustering is to be able to compute a parametric
mixture model describing the distribution of data in S−
including the number of mixture components M−(n+1).
In the present work, we have used the simple k-means
clustering to compute GMM parameters. However, the k-
means approach requires the total number of clusters to
be speciﬁed externally. To work around this limitation, we
have implemented a naive strategy which only requires
the upper bound M−
max(n + 1) as the external input
instead of M−(n+1). We deﬁne the likelihood agreement
measure (Lmes) [11] as the measure of ﬁtness of the
parametric model θa in describing the dataset S. Let
θa,M be the M-component mixture model arrived at from
k − means clustering. Then the likelihood agreement

n+1

measure may be computed as

Np(cid:88)

Lmes(θa,M ) =

πθa (xi−

n+1)

(57)

i=1

where πθa (x) is the mixture pdf derived from the
parametric model θa,M . Let θa∗,M∗ be the optimal
parametric model with M−
n+1 = M∗ components that
maximized the likelihood agreement measure given the
upper bound M−
the naive strategy
employed in implementing K-means
clustering is
presented in the following algorithm. The most common

max(n + 1). Then,

n+1,··· , xNp−
max(n + 1)

n+1 }, M−

max(n + 1)

−
max(n+1)
)

n+1 = {x1−

max(n + 1)
mes ← Lmes(θa,M

Algorithm 2 Clustering:Naive Strategy
n+1,··· , xi−
Input: S−1
Output: θa∗,M∗, M∗ ≤ M−
1: M ← M−
2: θa∗,M∗ ← θa,M
3: L∗
4: while M > 1 do
5: M ← M − 1
6:
7:
8:
9:
10:
11: end while

θa∗,M∗ ← θa,M
mes ← Lmes(θa,M )
L∗
end if

Compute θa,M using k-means
if Lmes(θa,M ) ≥ L∗
mes then

−
max

implementation of the k- means clustering approach is
the Lloyd’s algorithm [31]. The time complexity of the
Lloyd’s algorithm is known to be O(NpM di) where
Np is the number of particles to be clustered ,d the
dimensionality of the state space, M the number of
clusters and i is the number of iterations [32]. In practice,
the algorithm stops after several iterations ﬁnding a local
optimum. Implementing the naive clustering strategy as
described here will result in a quadratic time complexity
in M−

max(n + 1).

3) Measurement Update: Let π−

n+1 be the mixture pdf

derived from the parametric model θa∗,M∗.

n+1(cid:88)

−

M

π−
n+1 =

wiGi(X, µ−

i (n + 1), P −

i (n)).

(58)

i=1

Let zn+1 be the measurement vector recorded at time
n + 1. The PGMF algorithm approximates the poste-
rior pdf as a GMM with N components. The mixture
parameters for the posterior pdf are computed in two
stages. In the ﬁrst stage,
the component means and
covariances are computed through Kalman measurement
update of the individual mixture modes as described
in Eqs.16,17. However, in the present work we have
considered two different approaches to computing the
covariance terms(Pi,ZX (n + 1), Pi,ZZ(n + 1)) and the
expectations (Ei(h(X))) required for performing the

Kalman update.

A. Update 1(PGM ﬁlter 1)
In this approach, we compute the statistics of the posterior
random variable with the unscented transform using a set
of of 2d + 1 sigma points that are distributed symmetri-
cally .
The covariance terms and the expectations required for
computing the Kalman gain and posterior statistics are
then computed as the weighted sample averages from
the sigma points. Hence in PGM ﬁlter 1, the posterior
means and covariances are computed by performing a
UKF measurement update individually on each GMM
component.

B. Update 2 (PGM ﬁlter2)
In this approach,
the covariances and cross covari-
ances required for computing the gain matrix are
evaluated directly from the particles. Let S−1
j,n+1 =
{x1−
j,n+1} denote the set of par-
ticles that form the j − th cluster. Then the mean and
covariance terms required from the cluster j are computed
as

j,n+1,··· , xN j−

j,n+1,··· , xi−

zl = g(xl

j,n+1)

l = 1, . . . , N j

(59)

N j(cid:88)

ˆzj =

1
N j

Pj,ZZ =

Pj,ZX =

zl

l=1

1

N − 1

1

N − 1

N j(cid:88)
N j(cid:88)

l=1

l=1

(zl − ˆzj) (zl − ˆzj)

(cid:48)

+ R

(zl − ˆzj)(cid:0)xl

j (n + 1)(cid:1)(cid:48)

j,n+1 − µ−

ωi =

µi =

Pi =

closely distributed mixture modes in the posterior pdf.
The components that are located sufﬁciently close may
be merged to obtain a mixture model with well separated
modes. A similar situation may arise when the clustering
scheme assigns a complex model to describe the data due
to overﬁtting. To identify the rights modes to be merged,
we deﬁne the following normalized error metric [33] as
a measure of similarity between modes i and j.

(cid:82) (Gi(x, µi, Pi) − Gj(x, µj, Pj))2dx
(cid:82) Gi(x, µi, Σi)2dx +(cid:82) Gj(x, µj, Σj)2dx

(62)
Clearly, D(i, j) = 0 when the components i, j are
identical and D(i, j) = 1 when they are completely
dissimilar. By evaluating the Gaussian integrals involved,
the expression for D(i, j) can be reduced to

D(i, j) =

D(i, j) =

|4πPi|−1/2 + |4πPj|−1/2 − 2N (µi, µj, Pi + Pj)

|4πPi|−1/2 + |4πPj|−1/2

,

(63)
where |.| represents the determinant of the enclosed
square matrix. Mixture modes that are closely spaced, can
be merged whenever the value of normalized error metric
falls below a predetermined tolerance (tol). In the present
study, we have chosen this tolerance to be tol = 0.01. Let
i1,··· , ik be the indices of the mixture modes that are
to be merged. Then the mixture parameters of the new
Gaussian component obtained after merging is given by

ωil

k(cid:88)
(cid:80)k
(cid:80)k
l=1 ωil µil
ωi
l=1 ωil (Pil + (µil − µi)(µil − µi)T )

l=1

ωi

(64)

(65)

(66)

The posterior mean and covariance µi(n + 1), Pi(n + 1)
for the remaining components are evaluated in a similar
manner. Once all the component means and covariances
are updated, the mixture weights wi can be computed as
described below.
a) Construct the ith mode measurement pdf PZ,i(z) given

by

PZ,i(z) = Gi(z, ˆzi, Pi,ZZ)

b) Evaluate the probability li(n + 1) given by

li(n + 1) = PZ,i(zn+1),

where zk+1 is the measurement vector recorded at t =
n + 1.

c) The weight of the ith posterior Gaussian mixture

component is then given by equation 11

The weights, means and covariances as computed here
characterizes the GMM describing the posterior pdf.

(60)

(61)

Recursive implementation of the prediction, clustering,
update and merging algorithms as described here consti-
tutes the PGM ﬁlter.

V. NUMERICAL EXAMPLES

In this section, the particle Gaussian mixture ﬁlter is applied
to three test case problems to evaluate the ﬁltering perfor-
mance. Other nonlinear ﬁlters such as the UKF and PF are also
simulated for comparison. For the PF, a sequential importance
resampling (SIR) design is considered. The estimation results
are assessed for accuracy, consistency and informativeness.
An exact description of the metrics used to compare the
ﬁlter performance in each of the aforementioned categories
is provided below.
(a) Accuracy: A Monte Carlo averaged root mean squared
error (Erms(t)) is considered for evaluating the accuracy
of the estimates. The value of Erms(t) is computed as

4) Merging: Depending on the clustering scheme, dynam-
ics and measurement models, one may observe several

Erms(t) =

(cid:107) ˆX j(t) − ˆµj(t)(cid:107)2
2.

(67)

(cid:118)(cid:117)(cid:117)(cid:116) 1

NM o

NM o(cid:88)

j=1

mixture modes are well separated, it can be deduced that

E(V1) = [ω1 ··· ωi ··· ωMn ]T .

(73)

It should be observed that the merging procedure de-
scribed in the previous section helps to keep the modes
well separated as it coalesces the components that are
closely spaced. Deﬁne

v =V1 − [ω1 ··· ωi ··· ωMn ]T ,
2
v =T

v v.

(74)
(75)

Here the value of V1 is evaluated over a single instant,

c(x) = 1. Then, it can be shown that
1i

i.e,(cid:80)Mn

i=1

E(2

v) =

ωi(1 − ωi),

Mn(cid:88)

i=1

Mn(cid:88)

i=1

E(2

v − E(2
(cid:88)
(cid:88)

k

j
j(cid:54)=k

v))2 =

ωi(1 − ωi)((1 − ωi)3 + ω3
i )

βj,t.

(70)

+

ωjωk(ωj + ωk − 3ωjωk) − (E(2

v))2.

Here, ˆX j(t) and ˆµj(t) represent the actual and estimated
states at the time instant t during the jth Monte Carlo run.
The time averaged error(Erms) can be computed from
Erms(t) as

T(cid:88)

t=1

Erms =

1
T

Erms(t).

(68)

(b) Consistency: The consistency of the ﬁltered pdf is ex-
amined using the normalized estimation error squared
(NEES) test [34]. For a unimodal state pdf, the NEES
test is evaluated using the χ2 test statistic (βj,t) given by
βj,t = ( ˆX j(t)− ˆµj(t))T ( ˆP j(t))−1( ˆX j(t)− ˆµj(t)). (69)
The term ˆP j(t) in the above expression represents the
covariance of the unimodal ﬁltered pdf at time t during
jth Monte Carlo run. The Monte Carlo averaged NEES
test (βt) is computed from this expression as

NM o(cid:88)

j=1

βt =

1

NM o

Mn(cid:88)

It can be shown that when the state vector x ∈ (cid:60)d
is normally distributed, the product NM oβt has a χ2
distribution with dNM o degrees of freedom. As a result,
the consistency of the ﬁltered pdf can be tested by
determining whether βt falls within probable bounds
determined from the corresponding χ2 random variable.
When a particle ﬁlter is used for estimation, the NEES
test statistic is evaluated using the sample mean and
sample covariance of the ensemble of states.
The NEES test as presented here is not suitable for
evaluating the consistency of a multimodal pdf. Let the
ﬁltered pdf at time t = n be given by

πn(x) =

ωiGi(x; µi, Pi).

(71)

i=1

When the mixture modes are well separated, the total
probability that the r.v represented by the GMM belongs
to any one of the mixture modes is given by its mixture
weight. In deﬁning a GMM describing the state of the
dynamical system,
the ﬁlter hypothesizes the mixture
the component means and their covariances.
weights,
Hence, it is indispensable that, along with the means
and covariances, the consistency test also checks for the
agreement of the mixture weights with the observed data.
A novel two step procedure for evaluating the consistency
of a GMM pdf is developed as part of the present work.
Let the random vector V1 be deﬁned as
c(x)··· 1Mn

(72)
where 1i
c(x) represents the indicator function which
equals 1 when the state belongs to the ith mixture
component and zero otherwise. Then assuming that the

c(x)··· 1i

V1 = [11

(x)]T ,

c

(76)

(77)

(78)

(79)

Let V j
1 (t) be the above deﬁned indicator vector computed
at time t during jth Monte Carlo run. At each instant, the
state vector is assumed to belong to the component which
has the highest likelihood given the truth. That is

c( ˆX j(t)) =
1i

1,
0, otherwise

i = arg maxGi( ˆX j(t), µi, Pi)

(cid:40)

Let the sum Swt be deﬁned as

NM o(cid:88)

i=1

(cid:113)

Swt =

v(t)2 − E(j
j

v(t)2)
v(t)2 − E(j

NM oE(j

v(t)2))2

The expectations involved in this sum are computed
using the mixture weights ωj
i (t) at time t during the
run j. As NM o becomes large, the sum Swt converges
in distribution to a standard Gaussian random variable,
d−→ G(x, 0, 1). Hence probability based bounds on
Swt
the value of Swt can be computed from a standard
normal distribution. Indeed, the ﬁrst step in the two step
procedure for testing consistency of GMM pdfs involves
evaluating Swt to determine whether it falls within the
desired bounds. In the second step, a NEES test statistic
is computed from the GMM except that the mean and
covariance of the most likely mode is used to evaluate
the χ2 random variable,i.e.,

βj,t =( ˆX j(t) − ˆµj

i (t))T ( ˆP j

i (t))−1( ˆX j(t) − ˆµj

i (t)),

(80)

i = arg max N ( ˆX j(t), µj

i (t), P j

i (t))

The χ2 test statistic obtained from the above expression
may then be averaged over several Monte Carlo runs

to perform the NEES test. This completes the two step
consistency check for GMM pdfs.

(c) Informativeness: The informativeness of estimates fur-
nished by an estimator is an important marker of its
performance. A more informative estimate almost always
has a higher utility in comparison to a more uncertain
estimate. In practice, several nonlinear ﬁlters inﬂate the
covariance of the estimates to ensure consistency sacri-
ﬁcing informativeness in the process [35]. Two separate
metrics are considered for evaluating the consistency of
estimates in the present work. When the estimation errors
are similar, a more informative state pdf can be expected
to produce higher likelihoods given the true state. The
averaged likelihood of the truth over NM o Monte Carlo
runs may be computed as

Fig. 5: Example 1:Propagation of states

Mt = 1 in the above equation. We compute the Monte
Carlo averaged 2 − σ volume as

L(t) =

1

NM o

t ( ˆX j(t)).
πj

(81)

V σ2(t) =

V jσ2(t).

(84)

NM o(cid:88)

j=1

T(cid:88)

t=1

NM o(cid:88)

j=1

T(cid:88)

i=1

represent

Here πj
the state pdf conditioned upon all
t
measurements recorded until the time instant t in the
jth Monte Carlo run. The time averaged likelihood is
computed from the above expression as

ˆL =

1
T

L(t).

(82)

When the state pdf is represented as an ensemble of
states,
the likelihood is computed using a unimodal
Gaussian pdf characterized by the sample mean and
covariance of the collection of states. For a unimodal
Gaussian pdf, the total volume occupied by the r-sigma
ellipse contains a fraction fp of the total probability. It
can be shown that this fraction depends only upon the
value of r and the dimensionality d of the state space,
i.e., fp = fp(r, d). Let f(cid:48)
p be the fraction of probability
contained in the r(cid:48)-sigma ellipse for a ﬁxed r(cid:48) > 0.
Then the volume occupied by these r(cid:48) sigma ellipses
may be used as a measure of the uncertainty in the state
estimate with a larger volume indicating less informative
estimates. For a GMM with well separated modes, the
volume of the state space that contains the fraction f(cid:48)
p of
total probability may be computed as the sum of volumes
of n(cid:48) sigma ellipses of individual mixture components.
In the present work, the volume of state space V jσ2 that
contains the fraction fp = fp(2, d) of total probability
is considered for comparing informativeness. For a well
separated GMM pdf, the value of V σ2 can be computed
as

V jσ2(t) =

|2Σi|,

(83)

Mt(cid:88)

i=1

where Mt is the number of modes. Here |.| represents
the determinant of the enclosed square matrix. The ex-
pression for the unimodal case can be derived by setting

We also compute the corresponding time averaged value
ˆV σ2 given by

ˆV σ2 =

1
T

V σ2(t).

(85)

A. Example 1

by

Consider the discrete time nonlinear dynamic system given

xk+1 =

xk
2

+

25xk
(1 + x2
k)

+ 8 cos(1.2k) + νk

(86)

A measurement model aiding the estimation of the system is
speciﬁed as follows.

zk =

x2
k
20

+ nk

(87)

The process noise term νk and measurement noise term nk
are assumed to be independent zero mean Gaussian random
variables with covariances Q and R respectively. Removing
the cosine forcing term and setting xk+1 = xk in equation,
we get the equilibrium points

xk = ±7, 0.

(88)
However, only the equilibrium points at ±7 are stable. Loca-
tion of a set of 200 particles that are sampled and propagated
from the pdf N (0, 5) is provided in ﬁgure 5 for illustrating the
propagation of uncertainty in this system. It is worth noting
that the measurement model does not disambiguate between
±xk given a sufﬁciently uncertain prior pdf.

Two variants of the PGM ﬁlter, an SIR ﬁlter and a UKF
are simulated to estimate the test case 1 system for a duration
of 52 time steps. The uncertain initial state of the system is
assumed to distributed as P0(x) = N (0, 2). The process and
measurement noise covariances are set to be Q = 10, R = 1.

TABLE I

UKF parameters
α
1.3

λ
0.2

β
1.5

Measurements are recorded at every other instant. The estima-
tion process is repeated over 50 Monte Carlo runs and the time
and Monte Carlo averaged performance metrics are computed.
The SIR and the PGM ﬁlters are implemented with a set 50
particles in this one dimensional test case problem. The upper
bound on the number of mixture components Mmax is set to
be 2. The parameter values used in the implementation of the
UKF may be found in Table I.

to this observation. As xj,t

Figure 6 shows the sequence of actual and estimated states
obtained from a single Monte Carlo run. The PGM ﬁlters
and the PF are seen to offer superior tracking performance
in comparison to UKF. The Monte Carlo averaged root mean
squared error(Erms(t)) plotted in ﬁgure 7a provides further
support
is a one dimensional
random variable, the sum NM oβt is a χ2 random variable
in NM o dimensions. For 50 monte carlo runs, 99 per cent
probability upper bound of the random variable NM oβt
is
NM o
found to be U b0.99 = 1.5231. For the multimodal ﬁlters, value
of βt is computed from the covariance of the most likely mode
as mentioned before. The Monte Carlo averaged NEES results
along with the U b0.99 are plotted in ﬁgure 7b. It is observed
that the UKF and PF frequently oversteps the upper bound
which marks inconsistent estimates. Furthermore, βt computed
using the PF estimates are found to frequently exhibit peaks
several orders of magnitude larger than the 99% upper bound,
indicating covariance collapse. To study the informativeness of
the estimates, the averaged likelihood L(t) is computed and
plotted in ﬁgure 7c. The estimates provided by the PGM ﬁlters
are seen to hold the highest likelihoods during most of the
simulation time. A similar trend is observed when the volume
V2σ is plotted against time in ﬁgure 7d where the PGM ﬁlters
are seen to have the lowest V2σ during a large fraction of the
simulated time.

TABLE II

Example 1: Results
Erms
6.3169
6.4223
6.4580
8.1980

βt,c(%)
80.77
78.85
42.31
28.85

ˆL

0.1153
0.1167
0.1072
0.0506

ˆV σ2

63.4740
61.8611
77.1697
101.034

PGM 1(UT)

PGM 2

PF
UKF

To gain further insight, the time averaged values of RMSE
Erms, likelihood ˆL, and the 2σ volume for each ﬁlter are
listed in table II. Also included is the fraction (βc%) of the
time instants during which the computed averaged NEES result

Fig. 6: Time history of actual and estimated states (case 1)

stayed within the 99 percent consistency limits,i.e.,

(cid:80)T

βc% =

t=1

1U b0.99(βt)

T

(89)

where 1U b0.99(βt) is the indicator function which equals 1
when βt < U b0.99 and zero otherwise. The consistency
fractions for the PGM ﬁlters are seen to be almost double
that of the PF. For the PGM ﬁlters, the component weights
are also tested for consistency. For the PGM1 ﬁlter, the value
of SwtZ is found to stay within the 99 percent bounds during
80.38% of the simulated time. For PGM ﬁlter 2, this number
was found to be 73%. The time averaged RMSE for the PGM
ﬁlters are seen to be the lowest and is closely followed by the
PF. The PGM ﬁlters registered the highest averaged likelihoods
and the lowest 2 − σ volumes. The results presented in table
II clearly show that the PGM ﬁlter implementations offer the
most accurate, consistent and informative estimates among the
all the ﬁlters that were tested.

B. Example 2

In this example, the PGM ﬁlters are employed in the esti-
mation of a 3 dimensional Lorenz 63 model for atmospheric
convection. The noise perturbed dynamics of the Lorenz 63
system is described the the following set of equations,

˙x1 =α(−x1 + x2),
˙x2 =βx1 − x2 − x1x3,
˙x3 = − γx3 + x1x2 + Γ(t),
α =10, β = 28, γ = 8/3.

(90)

A scalar nonlinear measurement model(zk) is considered
which is given by

zk =(cid:112)x1(t)2 + x2(t)2 + x3(t)2 + νk.

(91)

The process and measurement noise covariances are both set
be equal to 1.The initial state of the system is assumed to be

(a) Erms(t)

(b) βt

(c) L(t)

(d) V2σ

Fig. 7: Example 1 Results

uncertain and is characterized by the bimodal pdf

√

0.35I3×3)

p0(x) =0.9G(x, [−0.2,−0.2, 8]T ,
√

0.1G(x, [0.2, 0.2, 8]T ,

0.35I3×3)+

(92)

The state of the system is updated at a ﬁxed time step
∆t = 0.01s. The measurements are recorded at the interval
of ten time steps. The Lorenz 63 system is known to exhibit
chaotic solutions for the parameter values considered in the
present simulation. Figure 8 shows the trajectories followed

Fig. 8: Example 2:Propagation of states

by a set of 500 particles that are sampled from the initial
pdf and propagated through the dynamics of the system. The
sampled trajectories are seen to split into two sets that occupy
different regions of the state space.

The two variants of the PGM ﬁlter, the PF and a conven-
tional Gaussian mixture UKF are employed in the estimation
of the Lorenz63 system. The PGM ﬁlters and the SIR ﬁlter
are implemented with 300 particles. The maximum number
of mixture components (Mmax) is set be 2. The UKF is
implemented using the parameters listed in table I. The
dynamics and the estimation process are repeated over 50
Monte Carlo runs. The Monte Carlo averaged mean squared
error of estimation Erms(t) for each ﬁlter is plotted in ﬁgure
9a. The computed values of Erms(t) for PGM1 and PGM2
are found to be the smallest among the four ﬁlters that are
tested. For the Lorenz63 system, when NM o = 50, the 99%
probability upper bound on the random variable βt is found
to be U b0.99 = 3.8642. For the PGM ﬁlters and the mixture
UKF, the value of βt is computed using the covariance of the
mixture component with the highest likelihood. The Monte
Carlo averaged NEES results are plotted in ﬁgure 9b. The line
y = U b0.99 has also been included for reference. It is observed
that the NEES test statistic βt for the PF and the mixture UKF
overstep the y = U b0.99 line early in the simulation. It is also
seen that, once this upper bound is crossed, the value of βt
for these two ﬁlters remain outside the 99% probability limits
for the entire remaining duration of the test. In contrast, he
averaged NEES test statistic for the PGM ﬁlters are seen to
stay within the 99% during most of the simulation time. In
order to compare the informativeness of the estimates, the
averaged likelihoods(L(t)) and V σ2 volumes are computed
and plotted in ﬁgures 9c, 9d respectively. Curiously, with the
highest average likelihoods and the smallest V σ2 volume, the
PF is seen to outperform the PGM ﬁlters in furnishing the most
informative estimates. However, as the NEES results of the PF
are seen to stay above 102 during 40% the simulation time,
the higher likelihoods and the small V σ2 of the PF should be
understood as a consequence of its covariance collapse. The
consistency fractions(βc%) and the time averaged values of
RMSE Erms, likelihood ˆL, and 2σ volume for each ﬁlter are

TABLE III

Example 2: Results

Erms
13.9812
12.7406
15.5565
15.3695

βt,c(%)

100
99.02
19.61
13.73

ˆL

0.0035
0.0044
0.0114
0.0021

ˆV σ2(×105)

1.1508
0.1051
0.0667
0.8564

PGM 1(UT)

PGM 2

PF

GMUKF

(a) Erms(t)

C. Example 3

In this test case, The PGM ﬁlters are employed in the esti-
mation of a Lorenz96 system. The noise perturbed dynamics
of the Lorenz96 system is given by

(b) NEES(βt)

(c) L(t)

(d) V2σ

Fig. 9: Example 2 Results

listed in table IV

For the three multimodal ﬁlters PGM1, PGM2 and mixture
UKF, the computed value of Swtz is found to stay within the
99% bounds during 81.82, 90.91 and 10 percent of the times
considered. The results clearly indicate that the PGM ﬁlters are
more accurate and consistent than the PF and mixture UKF.
The PGM ﬁlter estimates are also seen to be more informative
than the mixture UKF estimates.

˙xi = xi−1(xi+1 − xi−2) − xi + F + Γ(t),

(93)
where i = 1, 2,··· , 40. The term F represents a constant
external forcing. In the present work, we set F = 8. The
covariance of the zero mean Gaussian white noise is assumed
to be Q = 10−2. A linear measurement model is employed in
the estimation of the Lorenz96 system and it is deﬁned as,

zk =HXk + νk, H ∈ R20×40

(94)

(cid:40)

Hi,j =

j = 2i − 1
1,
0, otherwise.

Therefore, the measurement model records the components of
the state vector that have odd indices. The measurement noise
is assumed to be a zero mean Gaussian random vector with
a covariance R = 10−2I20×20 where Ii,j = δi,j. The initial
state of the system is characterized by the pdf

p0(x) =G(x, µ0, P0),

where µ0 = F(cid:2)1··· 1··· 1(cid:3)T

(95)
, µ0 ∈ R40×1 and P0 =
10−3I40×40. The time step for updating the state of the system
is ﬁxed at ∆t = 0.05s. Measurements are recorded at the
interval of 1s. When a UKF is employed to estimate the state
of the Lorenz96 system, the covariance of the estimate is
frequently observed to become non positive deﬁnite. Hence,
the UKF is not included in this comparison study. Instead,
an EnKF is used to estimate the Lorenz96 system along with
the SIR and PGM ﬁlters. All four ﬁlters are equipped with
a set of 2000 particles. The value of Mmax is kept at 2.
The ﬁlters are used to estimate the state of the system for a
duration of 10s. The process is repeated over 50 Monte Carlo
runs and the averaged performance metrics are computed.
The accuracy of the estimates is compared using the Monte
Carlo averaged RMSE (Erms(t))which is plotted in ﬁgure
10a. From the Erms(t) plots, it can be observed that the
tracking performance of the PGM ﬁlters and the EnKF are
comparable. In comparison to the PGM ﬁlters, the PF is found
to offer inferior tracking performance. In order to compare
the consistency of the ﬁlters, the value of the Monte Carlo
averaged NEES test statistic βt is computed and plotted in
ﬁgure10b. The 99% probability upper bound on βt is found
to be U b0.99 = 43.0013. The y = U b0.99 line is included in the

the PGM ﬁlters are seen to perform better in terms of the
v2σ volume, whereas the EnkF estimates are seen to have the
highest averaged likelihoods. The time averaged values of the
RMS error Erms, likelihood ( ˆL) and the ˆV σ2 are listed in table
along with the consistency fractions. The results show that the
PGM2 is the most accurate estimator of the four ﬁlters studied.
The EnKF is seen to offer the best performance in terms of
consistency fractions , closely followed by the PGM ﬁlters.
The value of Swtz was found to stay within the 99% bounds
during 60% of the time for both PGM1 and PGM2 ﬁlter.

TABLE IV

Example 3: Results

PGM 1(UT)

PGM 2

PF

EnKF

Erms
18.0069
18.0452
31.7261
18.1055

βt,c(%)
80.69
70.30
9.90
81.19

ˆlogL
89.6553
89.6227
89.6244
89.8193

ˆ

logV σ2
152.8588
152.7732
152.7548
152.8034

VI. CONCLUSIONS

A novel Gaussian mixture-particle PGM algorithm for non-
linear ﬁltering has been presented. During the prediction step,
the PGM ﬁlter uses an ensemble of particles to propagate
the prior uncertainty. The propagated ensemble is clustered
to recover a GMM representation of the propagated pdf.
Measurements are incorporated through a Kalman update of
the mixture modes to arrive at the posterior pdf. The PGM
approach allows the number and weight of mixture compo-
nents to be adapted during propagation unlike the conventional
mixture ﬁlters. Additionally, the PGM is not prone to the
curse of dimensionality associated with particle measurement
updates. The PGM ﬁlter density is shown to converge weakly
to the true ﬁlter density under the condition of exponential
forgetting of initial conditions by the true ﬁlter. The PGM
ﬁlter is employed in three test cases to evaluate the etimation
performance. It is demonstrated that the PGM ﬁlter offers
superior estimation performance in comparison to UKF, PF
and a mixture UKF. The PGM ﬁlter is demonstrated to be
capable of tracking the 40 dimensional
lorenz 96 system
wherein the PF suffers particle depletion. The design of a
PGM ﬁltering scheme that incorporates splitting of mixture
modes during the measurement update is a goal of future work.
Further, the effect of the clustering scheme on the PGM ﬁlter
needs to be rigorously evaluated.

REFERENCES

[1] R. E. Kalman, “A new approach to linear ﬁltering and prediction
problems,” Transactions of the ASME–Journal of Basic Engineering,
no. 82, pp. 35–45, 1960.

[2] R. E. Kalman and R. S. Bucy, “New results in linear ﬁltering and
prediction theory,” Transactions of ASME-Journal of Basic Engineering,
vol. 83, pp. 96–108, 1961.

[3] G. Smith, S. Schmidt, and L. McGee, “Application of statistical ﬁlter
theory to the optimal estimation of position and velocity on board a
circumlunar vehicle,” NASA, Tech. Rep. NASA TR-135, Jan. 1962.

[4] S. J. Julier, J. K. Uhlmann, and H. Durrant-Whyte, “A new approach
for ﬁltering nonlinear systems,” in Proceedings of the American Control
Conference, 1995, pp. 1628–1632.

(a) Erms(t)

(b) NEES(βt)

(c) log(L(t))

(d) log(V2σ)

Fig. 10: Example 3 Results

NEES plot for reference. The NEES plots show that the EnKF
and the PGM1 ﬁlter offers comparable performance. The
NEES test results for the PF is seen to blow up after the ﬁrst
measurement update. This clearly indicates that the number of
particles employed in the present study is inadequate for a PF
based estimation without risking particle depletion. Finally the
logarithm of Monte Carlo averaged likelihoods and the V σ2
volumes plotted in ﬁgures 10c,10d are used to compare the
informativeness of the estimates. In comparison to the EnKF,

tions,” IEEE control systems magazine, vol. 29, pp. 66–82, 2009.

[31] S. P. Lloyd, “Least squares quantization in pcm,” IEEE Transactions on

Information Theory, vol. 28, no. 2, pp. 129–137, 1982.

[32] D. Pelleg and A. Moore, “Accelerating exact k-means algorithms
with geometric reasoning,” in KDD ’99 Proceedings of the ﬁfth ACM
SIGKDD international conference on Knowledge discovery and data
mining, 1999, pp. 277–281.

[33] U. D. Hanebeck, K. Briechle, and A. Rauh, “Progressive bayes: a new
framework for nonlinear state estimation,” in SPIE vol.5099 Multisensor,
Multisource Information Fusion: Architectures, Algorithms, and Appli-
cations, 2003, pp. 256–267.

[34] T. Bailey, J. Nieto, J. Guivant, M.Stevens, and E.Nebot, “Consistency
the 2006 IEEE/RSJ
of the ekf-slam algorithm,” in Proceedings of
International Conference on Intelligent Robots and Systems, 2006, pp.
3562–3568.

[35] T. Lefebvre, H. Bruyninckx, and J. D. Schutter, “Kalman ﬁlters for non-
linear systems: a comparison of performance,” International Journal of
Control, vol. 77, no. 7, pp. 639–653, 2004.

estimation,” in Proceedings of the IEEE, 2004, pp. 401–402.

[6] E. Wan and R. V. D. Merwe, “The unscented kalman ﬁlter,” in Kalman
Filter and Neural Networkst, S. Haykin, Ed. New York: J.Wiley and
Sons, 2001.

[7] T. Lefevbre, H. Bruyninckx, and J. D. Schutter, “Comment on ”a new
method for the nonlinear transformation of means and covariances in
ﬁlters and estimatiors”,” IEEE Transcations on Automatic Controll,
vol. 47, no. 8, pp. 1406–1409, August 2002.

[8] H. Sorenson and D. Alspach, “Recursive bayesian estimation using

gaussian sums,” Automatica, vol. 7, no. 4, pp. 465–479, 1971.

[9] D. Alspach and H. Sorenson, “Nonlinear bayesian estimation using gaus-
sian sum approximations,” IEEE Transactions on Automatic Control,
vol. 17, no. 4, pp. 439–448, 1972.

[10] G. Terejanu, P. Singla, T. Singh, and P. Scott, “Adaptive gaussian
sum ﬁlter for nonlinear bayesian estimation,” IEEE Transactions on
Automatic Control, vol. 56, no. 9, pp. 2151–2156, 2011.

[11] K. DeMars, R. Bishop, and M. Jah, “Entropy-based approach for
uncertainty propagation of nonlinear dynamical systems,” Journal of
Guidance, Control and Dynamics, vol. 36, no. 4, pp. 1047–1056, 2013.
[12] N. Gordon, D. Salmond, and A. Smith, “A novel approach to
nonlinear/non-gaussian bayesian state estimation,” IEEE Proceedings F,
Radar and Signal Processing, vol. 140, no. 2, pp. 107–113, 1993.

[13] S. Arulampalam, S. Maskell, N. Gordon, and T. Clapp, “A tutorial
on particle ﬁlters for online nonlinear/non-gaussian bayesian tracking,”
IEEE Transactions on signal processing, vol. 50, no. 2, pp. 174–188,
2001.

[14] T. Bengtsson, P. Bickel, and B.Li, “Curse-of-dimensionality revisited:
Collapse of particle ﬁlter in very large scale systems.” in Probability
and Statistics: Essays in Honor of David A. Freedman, 2008, vol. 2, pp.
316–334.

[15] F. Daum and J. Huang, “Curse of dimensionality and particle ﬁlters.” in

Proceedings of IEEE Aerospace Conference, 2003, pp. 1979–1993.

[16] G. Evenson, Data Assimilation:The Ensemble Kalman Filter. Berlin:

Springer, 2002.

[5] S. J. Julier and J. K. Uhlmann, “Unscented ﬁltering and nonlinear

[30] J. L. Anderson, “Ensemble kalman ﬁlters for large geophysical applica-

[17] T. Yang, P. G. Mehta, and S. P.Meyn, “Feedback particle ﬁlter,” IEEE
Transactions on Automatic Control, vol. 58, no. 10, pp. 2465 –2480,
2013.

[18] A. V. D. Raihan and S. Chakravorty, “A ukf-pf based hybrid estimation
scheme for space object tracking,” in Proceedings of the AAS/AIAA
astrodynamics specialist conference, 2015, to appear.

[19] Richard.O.Duda, P. E.Hart, and D. G.Stork, Pattern Classiﬁcation,

2nd ed. New York: Wiley-Interscience, November 2000.

[20] A.K.Jain, M.N.Murthy, and P.J.Flynn, “Data clustering: a review,” ACM

Computing Surveys, vol. 13, no. 3, pp. 264–323, 1999.

[21] J.Macqueen, “Some methods for classiﬁcation and analysis of multivari-

ate observations,” 1967, pp. 281–297.

[22] A. P. Dempster, N. M. Laird, and D. Rubin, “Maximum likelihood from
incomplete data via the em algorithm,” Journal of the Royal Statistical
Society, Series B, vol. 39, no. 1, pp. 1–38, 1977.

[23] C. A.Sugar, “Techniques for clustering and classiﬁcation with applica-
tions to medical problems,” Ph.D. dissertation, Department of Staistics,
Stanford University, Stanford,CA, 1998.

[24] R. Tibshirani, G. Walther, and T. Hastie, “Estimating the number of
clusters ina dataset via the gap statistic,” Journal of the Royal Statistical
Society, Series B, vol. 63, no. 2, pp. 411–423, 2001.

[25] M. Figueiredo and A. Jain, “Unsupervised learning of ﬁnite mixture
models,” IEEE transactions on Pattern Analysis and Machine Intelli-
gence, vol. 24, no. 3, pp. 381–396, 2002.

[26] C. Snyder, T. Bengtsson, P. Bickel, and J. Anderson, “Obstacles to
high- dimensional particle ﬁltering,” Monthly Weather Review, vol. 136,
no. 12, pp. 4629–4640, 2008.

[27] P. Bickel, B. Li, and T. Bengtsson, “Sharp failure rates for the bootstrap
ﬁlter in high dimnesions,” in IMS Collections:Pushing the Limits of
Contemporary Statistics: Contributions in Honor of Jayanta K Ghosh,
2008, vol. 3, pp. 318–329.

[28] G.Evensen, “Sequential data assimilation with a nonlinear quasi-
geostrophic model using monte carlo methods to forecast error statis-
tics,” Journal of Geophysical Research: Oceans, vol. 99, no. C5, pp. 10
143–10 162, 1994.

[29] G. Burgers, P. V. Leeuwen, and G.Evensen, “Analysis scheme in the
ensemble kalman ﬁlter,” Monthyly Weather Review, vol. 126, no. 6, pp.
1719–1724, 1998.

