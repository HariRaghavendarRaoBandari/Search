6
1
0
2

 
r
a

M
 
2

 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
7
1
7
0
0

.

3
0
6
1
:
v
i
X
r
a

1–34

Learning Supervised PageRank with Gradient-Based and

Gradient-Free Optimization Methods

BOGOLUBSKY@YANDEX-TEAM.RU

PAVEL.DVURECHENSKY@WIAS-BERLIN.DE

Lev Bogolubsky
Yandex, Leo Tolstoy st. 16, Moscow, Russian Federation
Pavel Dvurechensky
Weierstrass Institute for Applied Analysis and Stochastics, Mohrenstr. 39, Berlin, Germany
Institute for Information Transmission Problems RAS, Bolshoy Karetny per. 19, build.1, Moscow, Russian
Federation
Alexander Gasnikov
GASNIKOV@YANDEX.RU
Institute for Information Transmission Problems RAS, Bolshoy Karetny per. 19, build.1, Moscow, Russian
Federation
Gleb Gusev
Yandex, Leo Tolstoy st. 16, Moscow, Russian Federation
Yurii Nesterov
Center for Operations Research and Econometrics (CORE)
34 voie du Roman Pays, 1348, Louvain-la-Neuve, Belgium
Andrey Raigorodskii
Yandex, Leo Tolstoy st. 16, Moscow, Russian Federation
Aleksey Tikhonov
Yandex, Leo Tolstoy st. 16, Moscow, Russian Federation
Maksim Zhukovskii
Yandex, Leo Tolstoy st. 16, Moscow, Russian Federation

RAIGORODSKY@YANDEX-TEAM.RU

ALTSOPH@YANDEX-TEAM.RU

ZHUKMAX@YANDEX-TEAM.RU

GLEB57@YANDEX-TEAM.RU

YURII.NESTEROV@UCLOUVAIN.BE

Abstract

In this paper, we consider a non-convex loss-minimization problem of learning Supervised PageR-
ank models, which can account for some properties not considered by classical approaches such as
the classical PageRank model. We propose gradient-based and random gradient-free methods to
solve this problem. Our algorithms are based on the concept of an inexact oracle and unlike the
state state-of-the-art gradient-based method we manage to provide theoretically the convergence
rate guarantees for both of them.
In particular, under the assumption of local convexity of the
loss function, our random gradient-free algorithm guarantees decrease of the loss function value
expectation. At the same time, we theoretically justify that without convexity assumption for the
loss function our gradient-based algorithm allows to ﬁnd a point where the stationary condition is
fulﬁlled with a given accuracy. For both proposed optimization algorithms, we ﬁnd the settings
of hyperparameters which give the lowest complexity (i.e., the number of arithmetic operations
needed to achieve the given accuracy of the solution of the loss-minimization problem). The re-
sulting estimates of the complexity are also provided. Finally, we apply proposed optimization
algorithms to the web page ranking problem and compare proposed and state-of-the-art algorithms
in terms of the considered loss function.

c(cid:13) L. Bogolubsky, P. Dvurechensky, A. Gasnikov, G. Gusev, Y. Nesterov, A. Raigorodskii, A. Tikhonov & M. Zhukovskii.

BOGOLUBSKY ET AL.

1. INTRODUCTION
The most acknowledged methods of measuring importance of nodes in graphs are based on random
walk models. Particularly, PageRank Page et al. (1999), HITS Kleinberg (1998), and their vari-
ants Haveliwala (1999, 2002); Richardson and Domingos (2002) are originally based on a discrete-
time Markov random walk on a link graph. According to the PageRank algorithm, the score of a
node equals to its probability in the stationary distribution of a Markov process, which models a
random walk on the graph. Despite undeniable advantages of PageRank and its mentioned modiﬁ-
cations, these algorithms miss important aspects of the graph that are not described by its structure.
In contrast, a number of approaches allows to account for different properties of nodes and
edges between them by encoding them in restart and transition probabilities (see Dai and Davison
(2010); Eiron et al. (2004); Gao et al. (2011); Jeh and Widom (2003); Liu et al. (2008); Zhukovskii
et al. (2013a, 2014)). These properties may include, e.g., the statistics about users’ interactions with
the nodes (in web graphs Liu et al. (2008) or graphs of social networks Backstrom and Leskovec
(2011)), types of edges (such as URL redirecting in web graphs Zhukovskii et al. (2013a)) or histo-
ries of nodes’ and edges’ changes Zhukovskii et al. (2013b). Particularly, the transition probabilities
in BrowseRank algorithm Liu et al. (2008) are proportional to weights of edges which are equal to
numbers of users’ transitions.

In the general ranking framework called Supervised PageRank Zhukovskii et al. (2014), weights
of nodes and edges in a graph are linear combinations of their features with coefﬁcients as the model
parameters. The existing optimization method Zhukovskii et al. (2014) of learning these parame-
ters and the optimizations methods proposed in the presented paper have two levels. On the lower
level, the following problem is solved: to estimate the value of the loss function (in the case of
zero-order oracle) and its derivatives (in the case of ﬁrst-order oracle) for a given parameter vec-
tor. On the upper level, the estimations obtained on the lower level of the optimization methods
(which we also call inexact oracle information) are used for tuning the parameters by an iterative
algorithm. Following Gao et al. (2011), the authors of Supervised PageRank consider a non-convex
loss-minimization problem for learning the parameters and solve it by a two-level gradient-based
method. On the lower level of this algorithm, an estimation of the stationary distribution of the
considered Markov random walk is obtained by classical power method and estimations of deriva-
tives w.r.t. the parameters of the random walk are obtained by power method introduced in Andrew
(1978, 1979). On the upper level, the obtained gradient of the stationary distribution is exploited
by the gradient descent algorithm. As both power methods give imprecise values of the station-
ary distribution and its derivatives, there was no proof of the convergence of the state-of-the-art
gradient-based method to a local optimum (for locally convex loss functions) or to the stationary
point (for not locally convex loss functions).

The considered constrained non-convex loss-minimization problem from Zhukovskii et al. (2014)
can not be solved by existing optimization methods which require exact values of the objective func-
tion such as Nesterov and Spokoiny (2015) and Ghadimi and Lan (2014) due to presence of con-
straints for parameter vector and the impossibility to calculate exact value of the loss function and
its gradient. Moreover, standard global optimization methods can not be applied to solve it, because
they need access to some stochastic approximation for the loss-function value which in expectation
coincides with the true value of the loss-function.

In our paper, we propose two two-level methods to solve the loss-minimization problem from Zhukovskii

et al. (2014). On the lower level of these methods, we use the linearly convergent method from Nes-

2

LEARNING SUPERVISED PAGERANK

terov and Nemirovski (2015) to calculate an approximation to the stationary distribution of Markov
random walk. We analyze other methods from Gasnikov and Dmitriev (2015) and show that the
chosen method is the most suitable since it allows to approximate the value of the loss function with
any given accuracy and has lowest complexity estimation among others.

Upper level of the ﬁrst method is gradient-based. The main obstacle which we have overcome is
that the state-of-the-art methods for constrained non-convex optimization assume that the gradient
is known exactly, which is not the case in our problem. We develop a gradient method for general
constrained non-convex optimization problems with inexact oracle, estimate its convergence rate
to the stationary point of the problem. One of the advantages of our method is that it does not
require to know the Lipschitz-constant of the gradient of the goal function, which is usually used
to deﬁne the stepsize of a gradient algorithm. In order to calculate approximation of the gradient
which is used in the upper-level method, we generalize linearly convergent method from Nesterov
and Nemirovski (2015) (and use it as part of the lower-level method). We prove that it has a linear
rate of convergence as well.

Upper level of our second method is random gradient-free. Like for the gradient-based method,
we encounter the problem that the existing gradient-free optimization methods Ghadimi and Lan
(2014); Nesterov and Spokoiny (2015) require exact values of the objective function. Our contri-
bution to the gradient-free methods framework consists in adapting the approach of Nesterov and
Spokoiny (2015) to the case when the value of the function is calculated with some known accuracy.
We prove a convergence theorem for this method and exploit it on the upper level of the two-level
algorithm for solving the problem of learning Supervised PageRank.

Another contribution consists in investigating both for the gradient and gradient-free methods
the trade-off between the accuracy of the lower-level algorithm, which is controlled by the number
of iterations of method in Nesterov and Nemirovski (2015) and its generalization (for derivatives
estimation), and the computational complexity of the two-level algorithm as a whole. Finally, we es-
timate the complexity of the whole two-level algorithms for solving the loss-minimization problem
with a given accuracy.

In the experiments, we apply our algorithms to learning Supervised PageRank on real data (we
consider the problem of web pages’ ranking). We show that both two-level methods outperform
the state-of-the-art gradient-based method from Zhukovskii et al. (2014) in terms of the considered
loss function. Summing up, apart from the state-of-the-art method our algorithms have theoretically
proven estimates of convergence rate and outperform it in the ranking quality (as we prove exper-
imentally). The main advantages of the ﬁrst gradient-based algorithm are the following. There is
no need to assume that the function is locally convex in order to guarantee that it converges to the
stationary point. This algorithm has smaller number of input parameters than gradient-free, because
it does not need the Lipschitz constant of the gradient of the loss function. The main advantage of
the second gradient-free algorithm is that it avoids calculating the derivative for each element of a
large matrix.

The remainder of the paper is organized as follows.

In Section 2, we describe the random
walk model. In Section 3, we deﬁne the loss-minimization problem and discuss its properties. In
Section 4, we state two technical lemmas about the numbers of iterations of Nesterov–Nemirovski
method (and its generalization) needed to achieve any given accuracy of the loss function (and
its gradient). In Section 5 and Section 6 we describe the framework of random gradient-free and
gradient-based optimization methods respectively, generalize them to the case when the objective
function values and gradients are inaccurate and propose two-level algorithms for the stated loss-

3

BOGOLUBSKY ET AL.

minimization problem. Proofs of all our results can be found in Appendix. The experimental results
are reported in Section 7. In Section 8, we summarize the outcomes of our study, discuss its beneﬁts
and directions of future work.

2. MODEL DESCRIPTION
Let Γ = (V, E) be a directed graph. Let

F1 = {F (ϕ1,·) : V → R}, F2 = {G(ϕ2,·) : E → R}

be two classes of functions parametrized by ϕ1 ∈ Rm1, ϕ2 ∈ Rm2 respectively, where m1 is the
number of nodes’ features, m2 is the number of edges’ features. As in Zhukovskii et al. (2014), we
suppose that for any i ∈ V and any ˜i → i ∈ E, a vector of node’s features Vi ∈ Rm1
+ and a vector
of edge’s features E˜ii ∈ Rm2

+ are given. We set
F (ϕ1, i) = (cid:104)ϕ1, Vi(cid:105), G(ϕ1,˜i → i) = (cid:104)ϕ2, E˜ii(cid:105).

(2.1)
We denote m = m1 + m2, p = |V |. Let us describe the random walk on the graph Γ, which was
considered in Zhukovskii et al. (2014). A surfer starts a random walk from a random page i ∈ U (U
is some subset in V called seed set, |U| = n). We assume that ϕ1 and node features are chosen in
˜i∈U F (ϕ1,˜i) is non-zero. The initial probability of being at vertex i ∈ V is called

such way that(cid:80)

the restart probability and equals

[π0(ϕ)]i =

(cid:80)

F (ϕ1, i)
˜i∈U F (ϕ1,˜i)

i ∈ U

,

(2.2)

and [π0(ϕ)]i = 0 for i ∈ V \ U. At each step, the surfer (with a current position ˜i ∈ V ) either
chooses with probability α ∈ (0, 1) (originally Page et al. (1999), α = 0.15), which is called the
damping factor, to go to any vertex from V in accordance with the distribution π0(ϕ) (makes a
restart) or chooses to traverse an outgoing edge (makes a transition) with probability 1 − α. We
j:˜i→j G(ϕ2,˜i → j) is non-zero

assume that ϕ2 and edges features are chosen in such way that(cid:80)

for all ˜i with non-zero outdegree. For ˜i with non-zero outdegree, the probability

[P (ϕ)]˜i,i =

(cid:80)
G(ϕ2,˜i → i)
j:˜i→j G(ϕ2,˜i → j)

(2.3)

of traversing an edge ˜i → i ∈ E is called the transition probability. If an outdegree of ˜i equals 0,
then we set [P (ϕ)]˜i,i = [π0(ϕ)]i for all i ∈ V (the surfer with current position ˜i makes a restart
with probability 1). Finally, by Equations 2.2 and 2.3 the total probability of choosing vertex i ∈ V
conditioned by the surfer being at vertex ˜i equals α[π0(ϕ)]i + (1 − α)[P (ϕ)]˜i,i. Denote by π ∈ Rp
the stationary distribution of the described Markov process. It can be found as a solution of the
system of equations

[π]i = α[π0(ϕ)]i + (1 − α)

[P (ϕ)]˜i,i[π]˜i.

(2.4)

(cid:88)

˜i:˜i→i∈E

In this paper, we learn the ranking algorithm, which orders the vertices i by their probabilities [π]i
in the stationary distribution π.

4

LEARNING SUPERVISED PAGERANK

3. LOSS-MINIMIZATION PROBLEM STATEMENT
Let Q be a set of queries and, for any q ∈ Q, a set of nodes Vq which are relevant to q be given.
We are also provided with a ranking algorithm which assigns nodes ranking scores [πq]i, i ∈ Vq,
πq = πq(ϕ), as its output. For example, in web search, the score [πq]i may repesent relevance of the
page i w.r.t. the query q. Our goal is to ﬁnd the parameter vector ϕ which minimizes the discrepancy
of the ranking scores from the ground truth scoring deﬁned by assessors. For each q ∈ Q, there is a
set of nodes in Vq manually judged and grouped by relevance labels 1, . . . , k. We denote V j
q the set
of documents annotated with label k +1−j (i.e., V 1
q is the set of all nodes with the highest relevance
q , let h(j1, j2, [πq]i2 − [πq]i1) be the value of the loss
score). For any two nodes i1 ∈ V j1
function. If it is non-zero, then the position of the node i1 according to our ranking algorithm is
higher than the position of the node i2 but j1 > j2. We consider square loss with margins bj1j2 ≥ 0,
where 1 ≤ j2 < j1 ≤ k: h(j1, j2, x) = (min{x + bj1j2, 0})2 as it was done in previous studies Liu
et al. (2008); Zhukovskii et al. (2014, 2013b). Finally, we minimize

q , i2 ∈ V j2

|Q|(cid:88)

(cid:88)

1
|Q|

(cid:88)

q=1

1≤j2<j1≤k

i1∈V j1

q ,i2∈V j2

q

h(j1, j2, [πq]i2 − [πq]i1)

(3.1)

as a function of ϕ over over some set of feasible values, which may depend on the ranking model,
in order to learn our model using the data given by assessors.

˜ii

We consider the ranking algorithm from the previous section. Namely, πq is the stationary
distribution (2.4) in Markov random walk on a graph Γq = (Vq, Eq) (the set of edges Eq is given,
its elemets represent some relations between nodes which depend on a ranking problem which is
solved by the random walk algorithm). Features vectors (and, consequently, weights of nodes and
, ˜i → i ∈ Eq, depend on q as well. For example,
edges Fq := F and Gq := G) Vq
vertices in Vq may represent web pages which were visited by users after submitting a query q and
features may reﬂect different properties of query–page pair. For ﬁxed q ∈ Q, the graph Γq and
, ˜i → i ∈ E, we consider the notations from the previous section and add
features Vq
the index q: Uq := U, π0
q := π0, Pq := P , pq := p, nq := n, πq := π. The parameters α and
ϕ = (ϕ1, ϕ2)T of the model do not depend on q. We also denote p = maxq∈Q pq, n = maxq∈Q nq.
Also, let s = maxq∈Q sq, where sq = maxi∈Vq |{j : i → j ∈ Eq}| – the sparsity parameter,
maximum number of outgoing links from a node in Vq.

i , i ∈ Vq, Eq

i , i ∈ Vq, Eq

In order to guarantee that the probabilities in (2.2) and (2.3) are non-negative and that they do
not blow up due to zero value of the denominator, we need appropriately choose the set Φ of possible
values of parameters ϕ. Recalling (2.1), it is natural to choose some ˆϕ and R > 0 such that the set
Φ (which we call the feasible set of parameters) deﬁned as Φ = {ϕ ∈ Rm : (cid:107)ϕ − ˆϕ(cid:107)2 ≤ R} lies in
the set of vectors with positive components Rm

1.

˜ii

++

We denote by πq(ϕ) the solution of the equation

q (ϕ) + (1 − α)P T

πq = απ0

(3.2)
which is Equation (2.4) rewritten for ﬁxed q ∈ Q in the vector form. From (3.2) we obtain the
following equation for pq × m matrix dπq(ϕ)
dϕT which is the derivative of stationary distribution πq(ϕ)
q (ϕ), Pq(λϕ) = Pq(ϕ)),
in our experiments, we consider the set Φ = {ϕ ∈ Rm : (cid:107)ϕ − em(cid:107)2 ≤ 0.99} , where em ∈ Rm is the vector of all
ones, that has large intersection with the simplex {ϕ ∈ Rm

q (ϕ)]i, i ∈ Vq, [Pq(ϕ)]˜i,i, ˜i → i ∈ Eq, are scale-invariant (π0

1. As probablities [π0

++ : (cid:107)ϕ(cid:107)1 = 1}

q (λϕ) = π0

q (ϕ)πq

5

where vector x+ has components [x+]i = max{xi, 0}, the matrices Aq ∈ Rrq×pq , q ∈ Q represent
assessor’s view of the relevance of pages to the query q, vectors bq, q ∈ Q are vectors composed
from thresholds bj1,j2 in (3.1) with ﬁxed q, rq is the number of summands in (3.1) with ﬁxed q. We
denote r = maxq∈Q rq. Then the gradient of the function f (ϕ) is easy to derive:

|Q|(cid:88)

(cid:18) dπq(ϕ)

(cid:19)T

dϕT

q=1

∇f (ϕ) =

2
|Q|

AT

q (Aqπq(ϕ) + bq)+.

(3.3)

(3.4)

(3.5)

(3.6)

(3.7)

BOGOLUBSKY ET AL.

with respect to ϕ

where

dπq(ϕ)
dϕT = Π0

q(ϕ) + (1 − α)P T
pq(cid:88)

q (ϕ)

dϕT + (1 − α)

dπ0

i=1

Π0

q(ϕ) = α

q (ϕ)

dπq(ϕ)
dϕT ,

dpi(ϕ)
dϕT [πq(ϕ)]i

and pi(ϕ) is the i-th column of the matrix P T

q (ϕ).
Let us rewrite the function deﬁned in (3.1) as

|Q|(cid:88)

q=1

f (ϕ) =

1
|Q|

(cid:107)(Aqπq(ϕ) + bq)+(cid:107)2
2,

Finally, the loss-minimization problem which we solve in this paper is as follows

f (ϕ), Φ = {ϕ ∈ Rm : (cid:107)ϕ − ˆϕ(cid:107)2 ≤ R}.

min
ϕ∈Φ

To solve this problem, we use gradient-free methods which are based only on f (ϕ) calculations
(zero-order oracle) and gradient methods which are based on f (ϕ) and ∇f (ϕ) calculations (ﬁrst-
order oracle). We do not use methods with oracle of higher order since the loss function is not
convex and we assume that m is large.

4. NUMERICAL CALCULATION OF THE VALUE AND THE GRADIENT OF

f (ϕ)

One of the main difﬁculties in solving Problem 3.7 is that calculation of the value of the function
f (ϕ) requires to calculate |Q| vectors πq(ϕ) which solve (3.2).
In our setting, this vector has
huge dimension pq and hence it is computationally very expensive to ﬁnd it exactly. Moreover, in
order to calculate ∇f (ϕ) one needs to calculate the derivative for each of these huge-dimensional
vectors which is also computationally very expensive to be done exactly. At the same time our
ultimate goal is to provide methods for solving Problem 3.7 with estimated rate of convergence and
complexity. Due to the expensiveness of calculating exact values of f (ϕ) and ∇f (ϕ) we have to use
the framework of optimization methods with inexact oracle which requires to control the accuracy
of the oracle, otherwise the convergence is not guaranteed. This means that we need to be able
to calculate an approximation to the function f (ϕ) value (inexact zero-order oracle) with a given
accuracy for gradient-free methods and approximation to the pair (f (ϕ),∇f (ϕ)) (inexact ﬁrst-order

6

LEARNING SUPERVISED PAGERANK

oracle) with a given accuracy for gradient methods. Hence we need some numerical scheme which
allows to calculate approximation for πq(ϕ) and dπq(ϕ)
dϕT

for every q ∈ Q with a given accuracy.

Motivated by the last requirement we have analysed state-of-the-art methods for ﬁnding the so-
lution of Equation 3.2 in huge dimension summarized in the review Gasnikov and Dmitriev (2015)
and power method, used in Page et al. (1999); Backstrom and Leskovec (2011); Zhukovskii et al.
(2014). Only four methods allow to make the difference (cid:107)πq(ϕ) − ˜πq(cid:107), where ˜πq is the approx-
imation, small for some norm (cid:107) · (cid:107) which is crucial to estimate the error in the approximation of
the function f (ϕ) value. These method are: Markov Chain Monte Carlo (MCMC), Spillman’s,
Nesterov-Nemirovski’s (NN) and power method. Spillman’s algoritm and power method converge
in inﬁnity norm which is usually pq times larger than 1-norm. MCMC converges in 2-norm which
is usually √
pq times larger than 1-norm. Also MCMC is randomized and converges only in average
which makes it hard to control the accuracy of the approximation ˜πq. Apart from the other three,
NN is deterministic and converges in 1-norm which gives minimum √
pq times better approxima-
tion. At the same time, to the best of our knowledge, NN method is the only method that admits
a generalization which, as we prove in this paper, calculates the derivative dπq(ϕ)
dϕT with any given
accuracy.
The method by Nesterov and Nemirovski (2015) for approximation of πq(ϕ) for any ﬁxed q ∈ Q

constructs a sequence πk by the following rule

π0 = π0

q (ϕ),

πk+1 = P T

q (ϕ)πk.

The output of the algorithm (for some ﬁxed non-negative integer N) is

˜πN
q (ϕ) =

α

1 − (1 − α)N +1

(1 − α)kπk.

N(cid:88)

k=0

(4.1)

(4.2)

(cid:109)−1 is used to calculate

(cid:108) 1

Lemma 1 Assume that for some δ1 > 0 Method 4.1, 4.2 with N =
the vector ˜πN

q (ϕ) for every q ∈ Q. Then

α ln 8r
δ1

˜f (ϕ, δ1) =

1
|Q|

(cid:107)(Aq ˜πN

q (ϕ) + bq)+(cid:107)2

2

|Q|(cid:88)

q=1

(4.3)

(4.4)

satisﬁes

| ˜f (ϕ, δ1) − f (ϕ)| ≤ δ1.

Moreover, the calculation of ˜f (ϕ, δ1) requires not more than |Q|(3mps + 3psN + 6r) a.o.

The proof of Lemma 1 can be found in Appendix A.1.
Our generalization of the method Nesterov and Nemirovski (2015) for calculation of dπq(ϕ)
dϕT

any q ∈ Q is the following. Choose some non-negative integer N1 and calculate ˜πN1
(4.1), (4.2). Start with initial point

for
q (ϕ) using

pq(cid:88)

i=1

Π0 = α

dπ0

q (ϕ)

dϕT + (1 − α)

7

dpi(ϕ)
dϕT [˜πN1

q (ϕ)]i.

(4.5)

Iterate

BOGOLUBSKY ET AL.

Πk+1 = P T

q (ϕ)Πk.

The output is (for some ﬁxed non-negative integer N2)

˜ΠN2

q (ϕ) =

1

1 − (1 − α)N2+1

N2(cid:88)

k=0

(1 − α)kΠk.

(4.6)

(4.7)

(cid:80)n1
i=1 |aij|.

maxj=1,...,n2

In what follows, we use the following norm on the space of matrices A ∈ Rn1×n2: (cid:107)A(cid:107)1 =

Lemma 2 Let β1 be a number (explicitly computable, see Appendix A.2 Equation A.11) such that
for all ϕ ∈ Φ

(cid:107)Π0

q(ϕ)(cid:107)1 ≤ α

+ (1 − α)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) dπ0

q (ϕ)
dϕT

(cid:108) 1

Assume that Method 4.1, 4.2 with N1 =
vector ˜πN1
calculate the matrix ˜ΠN2

q (ϕ) (4.2) and Method 4.5, 4.6, 4.7 with N2 =

q (ϕ) (4.7). Then the vector

α ln 24β1r
αδ2

(cid:16) ˜ΠN2

|Q|(cid:88)

q=1

q (ϕ)

˜g(ϕ, δ2) =

2
|Q|

(cid:13)(cid:13)(cid:13)(cid:13) dpi(ϕ)
(cid:13)(cid:13)(cid:13)(cid:13)1
pq(cid:88)
(cid:109) − 1 is used for every q ∈ Q to calculate the
(cid:109)− 1 is used for every q ∈ Q to
(cid:108) 1

≤ β1.

(4.8)

dϕT

i=1

α ln 8β1r
αδ2

(cid:17)T

AT

q (Aq ˜πN1

q (ϕ) + bq)+

(4.9)

satisﬁes

(4.10)
Moreover the calculation of ˜g(ϕ, δ2) requires not more than |Q|(10mps + 3psN1 + 3mpsN2 + 7r)
a.o.

(cid:107)˜g(ϕ, δ2) − ∇f (ϕ)(cid:107)∞ ≤ δ2.

The proof of Lemma 2 can be found in Appendix A.2.

5. RANDOM GRADIENT-FREE OPTIMIZATION METHODS
In this section, we ﬁrst describe general framework of random gradient-free methods with inexact
oracle and then apply it for Problem 3.7. Lemma 1 allows to control the accuracy of the inexact
zero-order oracle and hence apply random gradient-free methods with inexact oracle.

5.1. GENERAL FRAMEWORK
Below we extend the framework of random gradient-free methods Agarwal et al. (2010); Nesterov
and Spokoiny (2015); Ghadimi and Lan (2014) for the situation of presence of uniformly bounded
error of unknown nature in the value of an objective function in general optimization problem. Apart
from Nesterov and Spokoiny (2015), we consider a randomization on a Euclidean sphere which
seems to give better large deviations bounds and doesn’t need the assumption that the objective
function can be calculated at any point of Rm.

8

LEARNING SUPERVISED PAGERANK

Let E be a m-dimensional vector space.

In this subsection, we consider a general function
f (·) : E → R and denote its argument by x or y to avoid confusion with other sections. We choose
some norm (cid:107) · (cid:107) in E and say that f ∈ C1,1

L ((cid:107) · (cid:107)) iff

|f (x) − f (y) − (cid:104)∇f (y), x − y(cid:105)| ≤ L
2

(5.1)
The problem of our interest is to ﬁnd minx∈X f (x), where f ∈ C1,1
L ((cid:107) · (cid:107)), X is a closed convex
set and there exists a number D ∈ (0, +∞) such that diamX := maxx,y∈X (cid:107)x − y(cid:107) ≤ D. Also
we assume that the inexact zero-order oracle for f (x) returns a value ˜f (x, δ) = f (x) + ˜δ(x), where
˜δ(x) is the error satisfying for some δ > 0 (which is known) |˜δ(x)| ≤ δ for all x ∈ X. Let
x∗ ∈ arg minx∈X f (x). Denote f∗ = minx∈X f (x).

∀x, y ∈ E.

(cid:107)x − y(cid:107)2,

Apart from Nesterov and Spokoiny (2015), we deﬁne the biased gradient-free oracle

gµ(x, δ) =

m
µ

( ˜f (x + µξ, δ) − ˜f (x, δ))ξ,

where ξ is a random vector uniformly distributed over the unit sphere S = {t ∈ Rm : (cid:107)t(cid:107)2 = 1}, µ
is a smoothing parameter.

Algorithm 1 below is the variation of the gradient descent method. Here ΠX (x) denotes the

Euclidean projection of a point x onto the set X.

Algorithm 1 Gradient-type method

Input: Point x0 ∈ X, stepsize h > 0, number of steps M.
Set k = 0.
repeat

Generate ξk and calculate corresponding gµ(xk, δ).
Calculate xk+1 = ΠX (xk − hgµ(xk, δ)).
Set k = k + 1.

until k > M
Output: The point ˆxM = arg minx{f (x) : x ∈ {x0, . . . , xM}}.
Next theorem gives the convergence rate of Algorithm 1. Denote by Uk = (ξ0, . . . , ξk) the

history of realizations of the vector ξ generated on each iteration of the algorithm.
Theorem 1 Let f ∈ C1,1
generated by Algorithm 1 with h = 1

L ((cid:107) · (cid:107)2) and convex. Assume that x∗ ∈ intX, and the sequence xk is

8mL. Then for any M ≥ 0, we have

EUM−1f (ˆxM ) − f∗ ≤
≤ 8mLD2
M + 1

+

8

µ2L(m + 8)

+

δmD

4µ

+

δ2m
Lµ2 .

(5.2)

The full proof of the theorem is in Appendix B.
It is easy to see that to make the right hand side of (5.2) less than a desired accuracy ε it is

sufﬁcient to choose

M =

(cid:24) 32mLD2

(cid:25)

ε

, µ =

(cid:115)

√

8mD(cid:112)L(m + 8)

2

ε

3
2

.

(5.3)

2ε

L(m + 8)

δ ≤

,

9

BOGOLUBSKY ET AL.

5.2. SOLVING THE LEARNING PROBLEM
In this subsection, we apply the results of the previous subsection to solve Problem 3.7 in the follow-
ing way. We assume that the set Φ is a small vicinity of some local minimum ϕ∗ and the function
f (ϕ) is convex in this vicinity (generally speaking, the function deﬁned in (3.5) is nonconvex).
We choose the desired accuracy ε for approximation of the optimal value f∗ in this problem. This
accuracy in accordance with (5.3) gives us the number of steps of Algorithm 1, the value of the
parameter µ, the value of the required accuracy δ of the inexact zero-order oracle. Knowing the
value δ, using Lemma 1 we choose the number of steps N of Algorithm 4.1, 4.2 and calculate an
approximation ˜f (ϕ, δ) for the function f (ϕ) value with accuracy δ. Then we use the inexact zero-
order oracle ˜f (ϕ, δ) to make a step of Algorithm 1. Theorem 1 and the fact that the feasible set Φ
is a Euclidean ball makes it natural to choose (cid:107) · (cid:107)2-norm in the space Rm of parameter ϕ. It is easy
to see that in this norm diamΦ ≤ 2R. Algorithm 2 is a formal record of these ideas. To the best of
our knowledge, this is the ﬁrst time when the idea of random gradient-free optimization methods is
combined with some efﬁcient method for huge-scale optimization using the concept of an inexact
zero-order oracle.
Algorithm 2 Gradient-free method for Problem 3.7

(cid:113) 2ε

(cid:108)

(cid:109)

L(m+8)

, δ =

√
√

3
2

2

, µ =

Input: Point ϕ0 ∈ Φ, L – Lipschitz constant for the function f (ϕ) on Φ, accuracy ε > 0.
Deﬁne M =
Set k = 0.
repeat

128m LR2
ε

L(m+8).

ε
16mR

Generate random vector ξk uniformly distributed over a unit Euclidean sphere S in Rm.
Calculate ˜f (ϕk + µξk, δ), ˜f (ϕk, δ) using Lemma 1 with δ1 = δ.
Calculate gµ(ϕk, δ) = m
Calculate ϕk+1 = ΠΦ
Set k = k + 1.

8mL gµ(ϕk, δ)(cid:1).

µ ( ˜f (ϕk + µξk, δ) − ˜f (ϕk, δ))ξk.

(cid:0)ϕk − 1

until k > M
Output: The point ˆϕM = arg minϕ{f (ϕ) : ϕ ∈ {ϕ0, . . . , ϕM}}.

The most computationally hard on each iteration of the main cycle of this method are calcula-
tions of ˜f (ϕk + µξk, δ), ˜f (ϕk, δ). Using Lemma 1, we obtain that each iteration of Algorithm 2
needs not more than

128mrR(cid:112)L(m + 8)

√

ε3/2

2

(cid:33)

+ 6r

(cid:32)

2|Q|

3mps +

3ps
α

ln

a.o. So, we obtain the following result, which gives the complexity of Algorithm 2.
Theorem 2 Assume that the set Φ in (3.7) is chosen in a way such that f (ϕ) is convex on Φ and
some ϕ∗ ∈ arg minϕ∈Φ f (ϕ) belongs also to intΦ. Then the mean total number of arithmetic
operations of the Algorithm 2 for the accuracy ε (i.e. for the inequality EUM−1f ( ˆϕM ) − f (ϕ∗) ≤ ε
to hold) is not more than

(cid:32)

128mrR(cid:112)L(m + 8)

√

ε3/2

2

(cid:33)

+ 6r

.

768mps|Q| LR2
ε

m +

1
α

ln

10

LEARNING SUPERVISED PAGERANK

6. GRADIENT-BASED OPTIMIZATION METHODS
In this section, we ﬁrst develop a general framework of gradient methods with inexact oracle for
non-convex problems from rather general class and then apply it for the particular Problem 3.7.
Lemma 1 and Lemma 2 allow to control the accuracy of the inexact ﬁrst-order oracle and hence
apply proposed framework.

6.1. GENERAL FRAMEWORK
In this subsection, we generalize the approach in Ghadimi and Lan (2014) for constrained non-
convex optimization problems. Our main contribution consists in developing this framework for an
inexact ﬁrst-order oracle and unknown ”Lipschitz constant” of this oracle.
Let E be a ﬁnite-dimensional real vector space and E∗ be its dual. We denote the value of linear
function g ∈ E∗ at x ∈ E by (cid:104)g, x(cid:105). Let (cid:107) · (cid:107) be some norm on E, (cid:107) · (cid:107)∗ be its dual. Our problem of
interest in this subsection is a composite optimization problem of the form

{ψ(x) := f (x) + h(x)},

min
x∈X

(6.1)
where X ⊂ E is a closed convex set, h(x) is a simple convex function, e.g. (cid:107)x(cid:107)1. We assume that
f (x) is a general function endowed with an inexact ﬁrst-order oracle in the following sense. There
exists a number L ∈ (0, +∞) such that for any δ ≥ 0 and any x ∈ X one can calculate ˜f (x, δ) ∈ R
and ˜g(x, δ) ∈ E∗ satisfying

|f (y) − ( ˜f (x, δ) − (cid:104)˜g(x, δ), y − x(cid:105))| ≤ L
2

(6.2)
for all y ∈ X. The constant L can be considered as ”Lipschitz constant” because for the exact ﬁrst-
L ((cid:107) · (cid:107)) Inequality 6.2 holds with δ = 0. This is a generalization
order oracle for a function f ∈ C1,1
of the concept of (δ, L)-oracle considered in Devolder et al. (2013) for convex problems.
X with respect to (cid:107) · (cid:107). This means that for any x, y ∈ X

We choose a prox-function d(x) which is continuously differentiable and 1-strongly convex on

(cid:107)x − y(cid:107)2 + δ.

d(y) − d(x) − (cid:104)∇d(x), y − x(cid:105) ≥ 1
2

(cid:107)y − x(cid:107)2.

We deﬁne also the corresponding Bregman distance:

V (x, z) = d(x) − d(z) − (cid:104)∇d(z), x − z(cid:105).

Let us deﬁne for any ¯x ∈ E, g ∈ E∗, γ > 0

(cid:26)

(cid:104)g, x(cid:105) +

(cid:27)

V (x, ¯x) + h(x)

,

1
γ

xX (¯x, g, γ) = arg min
x∈X

gX (¯x, g, γ) =

(¯x − xX (¯x, g, γ)).

1
γ

(6.3)

(6.4)

(6.5)

(6.6)

We assume that the set X is simple in a sense that the vector xX (¯x, g, γ) can be calculated

explicitly or very efﬁciently for any ¯x ∈ X, g ∈ E∗, γ.

11

BOGOLUBSKY ET AL.

Algorithm 3 Adaptive projected gradient algorithm

Input: Point x0 ∈ X, number L0 > 0.
Set k = 0, z = +∞.
repeat

Set Mk = Lk, ﬂag = 0.
repeat

.

16Mk

Set δ = ε
Calculate ˜f (xk, δ) and ˜g(xk, δ).
Find

(cid:18)

wk = xX

xk, ˜g(xk, δ),

Calculate ˜f (wk, δ).
If the inequality

(cid:19)

1
Mk

˜f (wk, δ) ≤ ˜f (xk, δ) + (cid:104)˜g(xk, δ), wk − xk(cid:105)+

+

Mk
2

(cid:107)wk − xk(cid:107)2 +

ε

8Mk

(6.7)

(6.8)

holds, set ﬂag = 1. Otherwise set Mk = 2Mk.

(cid:16)

(cid:13)(cid:13)(cid:13)gX

until ﬂag = 1
Set xk+1 = wk, Lk+1 = Mk
If
Set k = k + 1.

(cid:17)(cid:13)(cid:13)(cid:13) < z, set z =

xk, ˜gk, 1
Mk

(cid:16)

(cid:13)(cid:13)(cid:13)gX

2 , ˜gk = ˜g(xk, δ).

(cid:17)(cid:13)(cid:13)(cid:13), ˆk = k.

xk, ˜gk, 1
Mk

until z ≤ ε
Output: The point xˆk+1.

Theorem 3 Assume that f (x) is endowed with the inexact ﬁrst-order oracle in a sense (6.2) and
that there exists a number ψ∗ > −∞ such that ψ(x) ≥ ψ∗ for all x ∈ X. Then after M iterations

of Algorithm 3 it holds that(cid:13)(cid:13)(cid:13)(cid:13)gX

(cid:18)

xˆk, ˜gˆk,

1
Mˆk

It is easy to show that when

The full proof of the theorem is in Appendix C.
xˆk, ˜gˆk, 1
√
Mˆk

(cid:68)∇f (xˆk+1) + p, x − xˆk+1

(cid:13)(cid:13)(cid:13)gX
(cid:16)
(cid:69) ≥ −c

+

M + 1

(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)2 ≤ 4L(ψ(x0) − ψ∗)
(cid:17)(cid:13)(cid:13)(cid:13)2 ≤ ε for small ε, then for all x ∈ X it holds

(6.9)

2L
L0

ε
2

.

Moreover, the total number of checks of Inequality 6.8 is not more than M + log2

.

that
ε, where c > 0 is a constant, p is some subgradient of
h(x) at xˆk+1. This means that at the point xˆk+1 the necessary condition of a local minimum is
fulﬁlled with a good accuracy, i.e. xˆk+1 is a good approximation of a stationary point.

12

LEARNING SUPERVISED PAGERANK

√

6.2. SOLVING THE LEARNING PROBLEM
In this subsection, we return to Problem 3.7 and apply the results of the previous subsection. For
this problem, h(·) ≡ 0. It is easy to show that in 1-norm diamΦ ≤ 2R
m. For any δ > 0,
2 allows us to obtain ˜f (ϕ, δ1) such that Inequality 4.4 holds and Lemma 2 with
Lemma 1 with δ1 = δ
√
m allows us to obtain ˜g(ϕ, δ2) such that Inequality 4.10 holds. Similar to Devolder et al.
δ2 = δ
4R
(2013), since f ∈ C1,1
L ((cid:107) · (cid:107)2), these two inequalities lead to Inequality 6.2 for ˜f (ϕ, δ1) in the role
of ˜f (x, δ), ˜g(ϕ, δ2) in the role of ˜g(x, δ) and (cid:107) · (cid:107)2 in the role of (cid:107) · (cid:107).

We choose the desired accuracy ε for approximating the stationary point of Problem 3.7. This
accuracy gives the required accuracy δ of the inexact ﬁrst-order oracle for f (ϕ) on each step of the
inner cycle of the Algorithm 3. Knowing the value δ1 = δ
2 and using Lemma 1, we choose the
number of steps N of Algorithm 4.1, 4.2 and thus approximate f (ϕ) with the required accuracy
√
δ1 by ˜f (ϕ, δ1). Knowing the value δ2 = δ
m and using Lemma 2, we choose the number of
4R
steps N1 of Algorithm 4.1, 4.2 and the number of steps N2 of Algorithm 4.5, 4.6, 4.7 and obtain the
approximation ˜g(ϕ, δ2) of ∇f (ϕ) with the required accuracy δ2. Then we use the inexact ﬁrst-order
oracle ( ˜f (ϕ, δ1), ˜g(ϕ, δ2)) to perform a step of Algorithm 3.
Since Φ is the Euclidean ball, it is natural to set E = Rm and (cid:107) · (cid:107) = (cid:107) · (cid:107)2, choose the

prox-function d(ϕ) = 1

2. Then the Bregman distance is V (ϕ, ω) = 1

Algorithm 4 is a formal record of the above ideas. To the best of our knowledge, this is the ﬁrst
time when the idea of gradient optimization methods is combined with some efﬁcient method for
huge-scale optimization using the concept of an inexact ﬁrst-order oracle.

The most computationally consuming operations of the inner cycle of Algorithm 4 are calcu-
lations of ˜f (ϕk, δ1), ˜f (ωk, δ1) and ˜g(ϕk, δ2). Using Lemma 1 and Lemma 2, we obtain that each
inner iteration of Algorithm 4 needs not more than
6mps|Q|

1024β1rRL

√

m

2(cid:107)ϕ − ω(cid:107)2
2.

2(cid:107)ϕ(cid:107)2

7r|Q| +

ln

α

αε

a.o. Using Theorem 3, we obtain the following result, which gives the complexity of Algorithm 4.

Theorem 4 The total number of arithmetic operations in Algorithm 4 for the accuracy ε (i.e. for
the inequality

≤ ε to hold) is not more than

(cid:16)

(cid:13)(cid:13)(cid:13)gΦ
(cid:18) 8L(f (ϕ0) − f∗)

ϕˆk, ˜g(ϕˆkδ2), 1
Mˆk

(cid:17)(cid:13)(cid:13)(cid:13)2

2

(cid:19)(cid:18)

+ log2

2L
L0

7r|Q| +

ε

6mps|Q|

α

ln

1024β1rRL

αε

√

m

(cid:19)

.

7. EXPERIMENTAL RESULTS
We apply different learning techniques, our gradient-free and gradient-based methods and state-of-
the-art gradient-based method, to the web page ranking problem and compare their performances.
In the next section, we describe the graph, which we exploit in our experiments (the user browsing
graph). In Section 7.2 and Section 7.3, we describe the dataset and the results of the experiments
respectively.

13

BOGOLUBSKY ET AL.

Algorithm 4 Adaptive gradient method for Problem 3.7
Input: Point ϕ0 ∈ Φ, number L0 > 0, accuracy ε > 0.
Set k = 0, z = +∞.
repeat

Set Mk = Lk, ﬂag = 0.
repeat

32Mk

, δ2 =

Set δ1 = ε
Calculate ˜f (ϕk, δ1) using Lemma 1 and ˜g(ϕk, δ2) using Lemma 2.
Find

64MkR

m.

ε

√

(cid:26)

(cid:27)

(cid:104)˜g(ϕk, δ2), ϕ(cid:105) +

(cid:107)ϕ − ϕk(cid:107)2
2.

Mk
2

ωk = arg min
ϕ∈Φ

Calculate ˜f (ωk, δ1) using Lemma 1.
If the inequality

˜f (ωk, δ1) ≤ ˜f (ϕk, δ1) + (cid:104)˜g(ϕk, δ2), ωk − ϕk(cid:105) +

(cid:107)ωk − ϕk(cid:107)2

2 +

Mk
2

ε

8Mk

holds, set ﬂag = 1. Otherwise set Mk = 2Mk.

(cid:13)(cid:13)(cid:13)gΦ

(cid:16)

(cid:17)(cid:13)(cid:13)(cid:13)2

, ˆk = k.

ϕk, ˜g(ϕk, δ2), 1
Mk

< z, set z =

(cid:16)

(cid:13)(cid:13)(cid:13)gΦ

until ﬂag = 1
Set ϕk+1 = ωk, Lk+1 = Mk
If
Set k = k + 1.

ϕk, ˜g(ϕk, δ2), 1
Mk

(cid:17)(cid:13)(cid:13)(cid:13)2

2 , .

until z ≤ ε
Output: The point ϕˆk+1.

7.1. USER BROWSING GRAPH
In this section, we deﬁne the user web browsing graph (which was ﬁrst considered in Liu et al.
(2008)). We choose the user browsing graph instead of a link graph with the purpose to make the
model query-dependent.
Let q be any query from the set Q. A user session Sq (see Liu et al. (2008)), which is started
from q, is a sequence of pages (i1, i2, ..., ik) such that, for each j ∈ {1, 2, ..., k − 1}, the element ij
is a web page and there is a record ij → ij+1 which is made by toolbar. The session ﬁnishes if the
user types a new query or if more than 30 minutes left from the time of the last user’s activity. We
call pages ij, ij+1, j ∈ {1, . . . , k − 1}, the neighboring elements of the session Sq.
We deﬁne user browsing graphs Γq = (Vq, Eq), q ∈ Q, as follows. The set of vertices Vq
consists of all the distinct elements from all the sessions which are started from a query q ∈ Q. The
set of directed edges Eq represents all the ordered pairs of neighboring elements (˜i, i) from such
sessions. We add a page i in the seed set Uq if and only if there is a session which is started from
q and contains i as its ﬁrst element. Moreover, we set ˜i → i ∈ Eq if and only if there is a session
which is started from q and contains the pair of neighboring elements ˜i, i.

14

LEARNING SUPERVISED PAGERANK

7.2. DATA
All experiments are performed with pages and links crawled by a popular commercial search engine.
We randomly choose the set of queries Q the user sessions start from, which contains 600 queries.
There are ≈ 11.7K vertices and ≈ 7.5K edges in graphs Γq, q ∈ Q, in total. For each query, a set of
pages was judged by professional assessors hired by the search engine. Our data contains ≈ 1.7K
judged query–document pairs. The relevance score is selected from among 5 labels. We divide our
data into two parts. On the ﬁrst part Q1 (50% of the set of queries Q) we train the parameters and
on the second part Q2 we test the algorithms. To deﬁne weights of nodes and edges we consider a
set of m1 = 26 query–document features. For any q ∈ Q and i ∈ Vq, the vector Vq
i contains values
of all these features for query–document pair (q, i). The vector of m2 = 52 features Eq
for an edge
˜ii
˜i → i ∈ Eq is obtained simply by concatenation of the feature vectors of pages ˜i and i.

j, Q3

To study a dependency between the efﬁciency of the algorithms and the sizes of the graphs, we
j contain

sort the sets Q1, Q2 in ascending order of sizes of the respective graphs. Sets Q1
ﬁrst (in terms of these order) 100, 200, 300 elements respectively for j ∈ {1, 2}.

j, Q2

7.3. PERFORMANCES OF THE OPTIMIZATION ALGORITHMS
We ﬁnd the optimal values of the parameters ϕ by all the considered methods (our gradient-free
method GFN (Algorithm 2), the gradient-based method GBN (Algorithm 4), the state-of-the-art
gradient-method GBP), which solve Problem 3.5.
The sets of hyperparameters which are exploited by the optimization methods (and not tuned by
them) are the following: the Lipschitz constant L = 10−4 in GFN (and L0 = 10−4 in GBN), the ac-
curacy ε = 10−6 (in both GBN and GFN), the radius R = 0.99 (in both GBN and GFN). On all sets
of queries, we compare ﬁnal values of the loss function for GBN when L0 ∈ {10−4, 10−3, 10−2, 10−1, 1}.
The differences are less than 10−7. We choose L in GFN to be equal to L0 chosen for GFN. On
Figure 2, we show how the choice of L inﬂuences the output of the gradient-free algorithm. More-
over, we evaluate both our gradient-based and gradient-free algorithms for different values of the
2, i ∈ {1, 2, 3}, when
accuracies. The outputs of the algorithms differ insufﬁciently on all test sets Qi
ε ≤ 10−6. On the lower level of the state-of-the-art gradient-based algorithm, the stochastic matrix
and its derivative are raised to the powers N1 and N2 respectively. We choose N1 = N2 = 100,
since the outputs of the algorithm differ insufﬁciently on all test sets, when N1 ≥ 100, N2 ≥ 100.
We evaluate GBP for different values of the step size (50, 100, 200, 500). We stop the GBP algo-
rithms when the differences between the values of the loss function on the next step and the current
step are less than −10−5 on the test sets. On Figure 1, we give the outputs of the optimization
algorithms on each iteration of the upper levels of the learning processes on the test sets.

In Table 1, we present the performances of the optimization algorithms in terms of the loss
function f (3.1). We also compare the algorithms with the untuned Supervised PageRank (ϕ =
ϕ0 = em).

GFN signiﬁcantly outperforms the state-of-the-art algorithms on all test sets. GBN signiﬁcantly
outperforms the state-of-the-art algorithm on Q1
2 (we obtain the p-values of the paired t-tests for
all the above differences on the test sets of queries, all these values are less than 0.005). However,
GBN requires less iterations of the upper level (until it stops) than GBP for step sizes 50 and 100
on Q2

2.
2, Q3

15

BOGOLUBSKY ET AL.

Figure 1: Vales of the loss function on each iteration of the optimization algorithms on the test sets.

Figure 2: Comparison of convergence rates of the power method and the method of Nesterov and Ne-
mirovski (on the left) & loss function values on each iteration of GFN with different values of
the parameter L on the train set Q1
1

Finally, we show that Nesterov–Nemirovski method converges to the stationary distribution
faster than the power method. On Figure 2, we demonstrate the dependencies of the value of the loss
function on Q1
1 for both methods of computing the untuned Supervised PageRank (ϕ = ϕ0 = em).

16

LEARNING SUPERVISED PAGERANK

Q1
2

loss

Meth.
PR
.00357
GBN .00279
GFN .00274
GBP
.00282
50s.
GBP
100s.
GBP
200s.
GBP
500s.

.00282

.00283

.00283

Q2
2

steps

loss

0
12
106
16

.00354
.00305
.00297
.00307

steps

0
12
106
31

Q3
2

loss
.0033
.00295
.00292
.00295

steps

0
12
106
40

8

4

2

.00307

16

.00295

20

.00308

.00308

7

2

.00295

.00295

9

3

Table 1: Comparison of the algorithms on the test sets.

(cid:16)

(cid:16)

(cid:17)(cid:17)

(cid:16) LD2

(cid:16) LD2

8. DISCUSSIONS AND CONCLUSIONS
Let us note that Theorem 1 allows to estimate the probability of large deviations using the ob-
tained mean rate of convergence for Algorithm 1 (and hence Algorithm 2) in the following way.
If f (x) is τ-strongly convex, then we prove (see Appendix) a geometric mean rate of conver-
(cid:17)(cid:17)
gence: EUM−1f (xM ) − f∗ ≤ O
. Using Markov’s inequality, we obtain that
iterations the inequality f (xM ) − f∗ ≤ ε holds with a probability greater
after O
than 1 − σ, where σ ∈ (0, 1) is a desired conﬁdence level. If the function f (x) is convex, but not
(cid:16)
strongly convex, then we can introduce the regularization with the parameter τ = ε/D2 minimizing
2(cid:107)x − ˆx(cid:107)2
2 (ˆx is some point in the set X), which is strongly convex. This
the function f (x) + τ
iterations the inequlity f (xM ) − f∗ ≤ ε holds with a
m LD2
will give us that after O
ln
ε
probability greater than 1 − σ.

(cid:16) LD2

(cid:17)(cid:17)

m L

τ ln

m L

τ ln

εσ

ε

εσ

We consider a problem of learning parameters of Supervised PageRank models, which are based
on calculating the stationary distributions of the Markov random walks with transition probabilities
depending on the parameters. Due to the impossibility of exact calculating derivatives of the sta-
tionary distributions w.r.t.
its parameters, we propose two two-level loss-minimization methods
with inexact oracle to solve it instead of the previous gradient-based approach. For both proposed
optimization algorithms, we ﬁnd the settings of hyperparameters which give the lowest complexity
(i.e., the number of arithmetic operations needed to achieve the given accuracy of the solution of
the loss-minimization problem).

We apply our algorithm to the web page ranking problem by considering a dicrete-time Markov
random walk on the user browsing graph. Our experiments show that our gradient-free method
outperforms the state-of-the-art gradient-based method. For one of the considered test sets, our
gradient-base method outperforms the state-of-the-art as well. For other test sets, the differences in
the values of the loss function are insigniﬁcant. Moreover, we prove that under the assumption of
local convexity of the loss function, our random gradient-free algorithm guarantees decrease of the
loss function value expectation. At the same time, we theoretically justify that without convexity

17

BOGOLUBSKY ET AL.

assumption for the loss function our gradient-based algorithm allows to ﬁnd a point where the
stationary condition is fulﬁlled with a given accuracy.

In future, it would be interesting to apply our algorithms to other ranking problems.

References
A. Agarwal, O. Dekel, and L. Xiao. Optimal algorithms for online convex optimization with multi-

point bandit feedback. In 23rd Annual Conference on Learning Theory (COLT), 2010.

A. Andrew. Iterative computation of derivatives of eigenvalues and eigenvectors. IMA Journal of

Applied Mathematics, 24(2):209–218, 1979.

A. L. Andrew. Convergence of an iterative method for derivatives of eigensystems. Journal of

Computational Physics, 26:107–112, 1978.

L. Backstrom and J. Leskovec. Supervised random walks: predicting and recommending links in

social networks. In WSDM, 2011.

Na Dai and Brian D. Davison. Freshness matters: In ﬂowers, food, and web authority. In SIGIR,

2010.

Olivier Devolder, Franc¸ois Glineur, and Yurii Nesterov. First-order methods of smooth convex

optimization with inexact oracle. Mathematical Programming, 146(1):37–75, 2013.

N. Eiron, K. S. McCurley, and J. A. Tomlin. Ranking the web frontier. In WWW, 2004.

B. Gao, T.-Y. Liu, W. W. Huazhong, T. Wang, and H. Li. Semi-supervised ranking on very large

graphs with rich metadata. In KDD, 2011.

A. Gasnikov and D. Dmitriev. Efﬁcient randomized algorithms for pagerank problem. Comp. Math.

& Math. Phys, 55(3):1–18, 2015.

S. Ghadimi and G. Lan. Stochastic ﬁrst- and zeroth-order methods for nonconvex stochastic pro-

gramming. SIAM Journal on Optimization, 23(4):2341–2368, 2014.

T. H. Haveliwala. Efﬁcient computation of pagerank. Technical report, Stanford University, 1999.

T. H. Haveliwala. Topic-sensitive pagerank. In WWW, 2002.

G. Jeh and J. Widom. Scaling personalized web search. In WWW, 2003.

J. M. Kleinberg. Authoritative sources in a hyperlinked environment. In SODA, 1998.

Y. Liu, B. Gao, T.-Y. Liu, Y. Zhang, Z. Ma, S. He, and H. Li. Browserank: Letting web users vote

for page importance. In SIGIR, 2008.

Yu. Nesterov and A. Nemirovski. Finding the stationary states of markov chains by iterative meth-

ods. Applied Mathematics and Computation, 255:58–65, 2015.

Yu. Nesterov and V. Spokoiny. Random gradient-free minimization of convex functions. Founda-

tions of Computational Mathematics, pages 1–40, 2015. doi: 10.1007/s10208-015-9296-2.

18

LEARNING SUPERVISED PAGERANK

Yurii Nesterov and B.T. Polyak. Cubic regularization of newton method and its global performance.

Mathematical Programming, 108(1):177–205, 2006.

L. Page, S. Brin, R. Motwani, and T. Winograd. The pagerank citation ranking: Bringing order to

the web. Technical report, Stanford InfoLab, 1999.

M. Richardson and P. Domingos. The intelligent surfer: Probabilistic combination of link and

content information in pagerank. In NIPS, 2002.

M. Zhukovskii, G. Gusev, and P. Serdyukov. Url redirection accounting for improving link-based

ranking methods. In ECIR, 2013a.

M. Zhukovskii, A. Khropov, G. Gusev, and P. Serdyukov. Fresh browserank. In SIGIR, 2013b.

M. Zhukovskii, G. Gusev, and P. Serdyukov. Supervised nested pagerank. In CIKM, 2014.

Appendix A. Missed proofs for Section 4
A.1. Proof of Lemma 1
Lemma A.1 Let us ﬁx some q ∈ Q. Let functions Fq, Gq be deﬁned in (2.1), π0
(2.2), matrices Pq(ϕ) be deﬁned in (2.3). Assume that Method 4.1, 4.2 with

q (ϕ) be deﬁned in

(cid:24) 1

α

N =

(cid:25)

ln

2
∆1

− 1

is used to calculate the approximation ˜πN
Equation 3.2. Then the vector ˜πN

q (ϕ) satisﬁes

q (ϕ) to the ranking vector πq(ϕ) which is the solution of

(cid:107)˜πN

q (ϕ) − πq(ϕ)(cid:107)1 ≤ ∆1

(A.1)

and its calculation requires not more than

a.o. and not more than

3mpqsq + 3pqsqN

2pqsq

memory amount additionally to the memory which is needed to store all the data about features and
matrices Aq, bq, q ∈ Q.

Proof. As it is shown in Nesterov and Nemirovski (2015) the vector ˜πN

q (ϕ) (4.2) satisﬁes

(cid:107)˜πN

q (ϕ) − πq(ϕ)(cid:107)1 ≤ 2(1 − α)N +1.

(A.2)

Since for any α ∈ (0, 1] it holds that α ≤ ln 1

1−α we have from the lemma assumption that

N + 1 ≥ 1
α

ln

2
∆1

≥ ln 2
∆1
ln 1
1−α

.

19

BOGOLUBSKY ET AL.

This gives us that 2(1 − α)N +1 ≤ ∆1 which in combination with (A.2) gives (A.1).

Let us estimate the number of a.o and the memory amount used for calculations. We will go
through Method 4.1, 4.2 step by step and estimate from above the number of a.o. for each step.
Since we need to estimate from above the total number of a.o. used for the whole algorithm we will
update this upper bound (and denote it by TAO) by adding on each step the obtained upper bound
of a.o. number for this step. On each step we also estimate from above (and denote this estimate
by MM) maximum memory amount which was used by Method 4.1, 4.2 before the end of this step.
Finally, at the end of each step we estimate from above by UM the memory amount which is still
occupied besides the step is ﬁnished.

1. First iteration of this method requires to calculate π = π0

q . The variable π will store current
(in terms of steps in k) iterate πk which potentially has pq non-zero elements. In accordance
to its deﬁnition (2.2) and Equalities 2.1 one has for all i ∈ Uq

(cid:80)

(cid:104)ϕ1, Vq
i(cid:105)
(cid:104)ϕ1, Vq
j(cid:105)
j∈Uq

[π0

q ]i =

(a) We calculate (cid:104)ϕ1, Vq
1(cid:80)
(cid:80)

(c) We calculate

(b) We calculate

not more than pq memory items since |Uq| = nq ≤ pq and Vq

j ∈ Rm1 for all i ∈ Uq.

i(cid:105) for all i ∈ Uq and store the result. This requires 2m1nq a.o. and

(cid:104)ϕ1,Vq

j∈Uq
(cid:104)ϕ1,Vq
i (cid:105)
j∈Uq

(cid:104)ϕ1,Vq

j(cid:105) which requires nq a.o. and 2 memory items.
j(cid:105) for all i ∈ Uq. This needs nq a.o. and no additional memory.

So after this stage MM = pq + 2, UM = pq, TAO = 2m1nq + 2nq.

2. We need to calculate elements of matrix Pq(ϕ). In accordance to (2.3) and (2.1) one has

[Pq(ϕ)]ij =

(cid:80)
(cid:104)ϕ2, Eq
ij(cid:105)
l:i→l(cid:104)ϕ2, Eq
il(cid:105) .

q on the previous step but each with not
This means that one needs to calculate pq vectors like π0
more than sq non-zero elements and dimension of ϕ2 equal to m2. Thus we need pq(2m2sq +
2sq) a.o. and not more than pqsq + 2 memory items additionally to pq memory items already
used. At the end of this stage we have TAO = 2m1nq + 2nq + pq(2m2sq + 2sq), MM =
pq + 2 + pqsq and UM = pq + pqsq since we store π and Pq(ϕ) in memory.

3. We set ˜πN

q = π0

q (this variable will store current approximation of ˜πN

q which potentially has
pq non-zero elements). This requires nq a.o. and pq memory items. Also we set a = (1 − α).
At the end of this step we have TAO = 2m1nq + 2nq + pq(2m2sq + 2sq) + nq + 1, MM =
pq + 2 + pqsq + pq and UM = pq + pqsq + pq + 1.

4. For every step from 1 to N

(a) We set π1 = P T

q (ϕ)π. This requires not more than 2pqsq a.o. since the number of
non-zero elements in the matrix P T
q (ϕ) is not more than pqsq and we need to multiply
each element by some element of π and add it to the sum. Also we need pq memory
items to store π1.

20

LEARNING SUPERVISED PAGERANK

q = ˜πN

q + aπ1 which requires 2pq a.o.

(b) We set ˜πN
(c) We set a = (1 − α)a.
At the end of this step we have. TAO = 2m1nq + 2nq + pq(2m2sq + 2sq) + nq + 1 +
N (2pqsq + 2pq + 1), MM = pq + 2 + pqsq + pq + pq and UM = pq + pqsq + pq + 1 + pq

5. Set ˜πN

q =

α

1−(1−α)a ˜πN

q . This takes 3 + pq a.o.

ij(cid:105) for all i, j = 1, . . . , pq s.t. i → j ∈ Eq,(cid:80)

So at the end we get TAO = 2m1nq+2nq+pq(2m2sq+2sq)+nq+1+N (2pqsq+2pq+1)+pq+3 ≤
3mpqsq + 3pqsqN, MM = pq + 2 + pqsq + pq + pq ≤ 2pqsq and UM = pq.
Remark 1 Note that we also can store in the memory all the calculated quantities (cid:104)ϕ1, Vq
i ∈ Uq, (cid:104)ϕ2, Eq
l:i→l(cid:104)ϕ2, Eq
case if we need them later. This requires not more than nq + pqsq + 1 + pq memory.
Lemma A.2 Assume that π1, π2 ∈ Spq (1) = {π ∈ Rpq
(cid:107)π1 − π2(cid:107)k ≤ ∆1 holds for some k ∈ {1, 2,∞}. Then

pq π = 1}. Assume also that inequality

i(cid:105) for all
il(cid:105) for the

j(cid:105),(cid:80)

(cid:104)ϕ1, Vq

+ : eT

j∈Uq

√
|(cid:107)(Aqπ1 + bq)+(cid:107)2 − (cid:107)(Aqπ2 + bq)+(cid:107)2| ≤ 2∆1
(cid:107)(Aqπ1 + bq)+ − (Aqπ2 + bq)+(cid:107)∞ ≤ 2∆1

rq

(A.3)

(A.4)
(A.5)
(A.6)
Proof. Note that in any case k ∈ {1, 2,∞} it holds that |[π1]i − [π2]i| ≤ ∆1 for all i ∈ 1, . . . , pq.
Using Lipschitz continuity with constant 1 of the 2-norm we get

(cid:107)(Aqπ1 + bq)+(cid:107)2 ≤ √
rq
(cid:107)(Aqπ1 + bq)+(cid:107)∞ ≤ 1

|(cid:107)(Aqπ1 + bq)+(cid:107)2 − (cid:107)(Aqπ2 + bq)+(cid:107)2| ≤ (cid:107)(Aqπ1 + bq)+ − (Aqπ2 + bq)+(cid:107)2

(A.7)

Note that every row of the matrix Aq contains one 1 and one -1 and all other elements in the row
are equal to zero. Using Lipschitz continuity with constant 1 of the function (·)+ we obtain for all
i ∈ 1, . . . , rq.

|[(Aqπ1 + bq)+]i − [(Aqπ2 + bq)+]i| ≤ |[π1]k − [π1]j − [π2]k + [π2]j| ≤ 2∆1,

where k : [Aq]ik = 1, j : [Aq]ij = −1. This with (A.7) leads to (A.3). Similarly one obtains (A.4).
Now let us ﬁx some i ∈ 1, . . . , rq. Then |[(Aqπ1 + bq)+]i| = |([π1]k − [π1]j + bi)+|. Since
π1 ∈ Spq (1) it holds that [π1]k − [π1]j ∈ [−1, 1]. This together with inequalities 0 < bi < 1 leads
to estimate |([π1]k − [π1]j + bi)+| ≤ 1. Now (A.5) and (A.6) become obvious.
Lemma A.3 Assume that vectors ˜πq, q ∈ Q satisfy the following inequalities

(cid:107)˜πq − πq(ϕ)(cid:107)k ≤ ∆1,

∀q = 1, ...,|Q|,

for some k ∈ {1, 2,∞}. Then

|Q|(cid:88)

q=1

˜f (ϕ) =

1
|Q|

(cid:107)(Aq ˜πq + bq)+(cid:107)2

2

(A.8)

satisﬁes | ˜f (ϕ) − f (ϕ)| ≤ 4r∆1, where f (ϕ) is deﬁned in (3.5).

21

BOGOLUBSKY ET AL.

Proof. For ﬁxed q ∈ Q we have

2 − (cid:107)(Aqπq(ϕ) + bq)+(cid:107)2

|(cid:107)(Aq ˜πq + bq)+(cid:107)2
= |(cid:107)(Aq ˜πq + bq)+(cid:107)2 − (cid:107)(Aqπq(ϕ) + bq)+(cid:107)2| · ((cid:107)(Aq ˜πq + bq)+(cid:107)2 + (cid:107)(Aqπq(ϕ) + bq)+(cid:107)2)
≤ 4∆1rq.

2| =

(A.3),(A.5)≤

Using (3.5) and (A.8) we obtain the statement of the lemma.
The proof ot Lemma 1. Inequality 4.4 follows from Lemma A.1 and Lemma A.3.

We use the same notations TAO, MM, UM as in the proof of Lemma A.1.

1. We reserve variable a to store current (in terms of steps in q) sum of summands in (4.3),
variable b to store next summand in this sum and vector π to store the approximation for
q (ϕ) for current q ∈ Q. So TAO = 0, MM = UM = 2 + pq.
˜πN

2. For every q ∈ Q repeat.

2.1. Set π = ˜πN

q (ϕ). According to Lemma A.1 we obtain TAO = 3mpqsq + 3pqsqN,

MM = 2pqsq + pq + 2, UM = pq + 2.

2.2. Calculate u = (Aq ˜πN

q (ϕ) + bq)+. This requires additionally 3rq a.o. and rq memory

items.

2.3. Set b = (cid:107)u(cid:107)2
2. This requires additionally 2rq a.o.
2.4. Set a = a + b. This requires additionally 1 a.o.

3. Set a = 1|Q| a. This requires additionally 1 a.o.

4. At the end we have TAO =(cid:80)

6r), MM = maxq∈Q(2pqsq + pq) + 2 ≤ 3ps, UM = 1.

q∈Q(3mpqsq + 3pqsqN + 5rq + 1) + 1 ≤ |Q|(3mps + 3psN +

A.2. The proof of Lemma 2
We use the following norms on the space of matrices A ∈ Rn1×n2

(cid:107)A(cid:107)1 = max{(cid:107)Ax(cid:107)1 : x ∈ Rn2,(cid:107)x(cid:107)1 = 1} = max

where the 1-norm of the vector x ∈ Rn2 is (cid:107)x(cid:107)1 =(cid:80)n2

i=1 |xi|.

j=1,...,n2

(cid:107)A(cid:107)∞ = max{(cid:107)Ax(cid:107)∞ : x ∈ Rn2,(cid:107)x(cid:107)∞ = 1} = max

i=1,...,n1

n1(cid:88)

i=1

|aij|,

n2(cid:88)

j=1

|aij|,

where the ∞-norm of the vector x ∈ Rn2 is (cid:107)x(cid:107)∞ = maxi=1,...,n2 |xi|. Note that both matrix norms
possess submultiplicative property

(cid:107)AB(cid:107)1 ≤ (cid:107)A(cid:107)1(cid:107)B(cid:107)1,

(cid:107)AB(cid:107)1 ≤ (cid:107)A(cid:107)∞(cid:107)B(cid:107)∞

(A.9)

for any pair of compatible matrices A, B.

22

LEARNING SUPERVISED PAGERANK

Lemma A.4 Let us ﬁx some q ∈ Q. Let Π0
q (ϕ) be deﬁned in (2.2),
pi(ϕ)T , i ∈ 1, . . . , pq be the i-th row of the matrix Pq(ϕ) deﬁned in (2.3). Then for the chosen
functions Fq, Gq (2.1) and set Φ in (3.7) the following inequality holds.

q(ϕ) be deﬁned in (3.4), π0

(cid:107)Π0

q(ϕ)(cid:107)1 ≤ α

q (ϕ)
dϕT

≤ β1 ∀ϕ ∈ Φ,

(A.10)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) dπ0
(cid:69)
(cid:69) − R

+ R

+ (1 − α)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)(cid:80)
(cid:13)(cid:13)(cid:13)(cid:80)
(cid:69)
(cid:69) − R

˜i∈Uq

˜i∈Uq

+ R

i=1

dϕT

pq(cid:88)

(cid:13)(cid:13)(cid:13)(cid:13) dpi(ϕ)
(cid:13)(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)2
(cid:88)

(cid:13)(cid:13)(cid:13)2
(cid:17)2 max
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:80)
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:80)
(cid:17)2 max

˜i∈Nq(i) Eq
i˜i
˜i∈Nq(i) Eq
i˜i

j∈1,...,m1

˜i∈Uq

Vq
˜i

j∈1,...,m2

j

Vq
˜i
Vq
˜i

˜i∈Nq(i) Eq
i˜i
˜i∈Nq(i) Eq
i˜i

where

β1 = 2α

+ 2(1 − α)

˜i∈Uq

(cid:68)
ˆϕ1,(cid:80)
(cid:16)(cid:68)
ˆϕ1,(cid:80)
(cid:68)
ˆϕ2,(cid:80)
pq(cid:88)
(cid:16)(cid:68)
ˆϕ2,(cid:80)

Vq
˜i
Vq
˜i

˜i∈Uq

i=1

+

 (cid:88)

˜i∈Nq(i)



Eq
i˜i

j

(A.11)

and Nq(i) = {j ∈ Vq : i → j ∈ Eq}, ˆϕ1 ∈ Rm1 – ﬁrst m1 components of the vector ˆϕ, ˆϕ2 ∈ Rm2
– second m2 components of the vector ˆϕ.

(cid:13)(cid:13)(cid:13) dπ0

(cid:13)(cid:13)(cid:13)1

Proof. First inequality follows from the deﬁnition of Π0

norm and inequalities |[πq(ϕ)]i| ≤ 1, i = 1, . . . , pq.

q (ϕ)
dϕT

dπ0

Let us now estimate
q (ϕ)
dϕT
2
of the matrix dπ0
q (ϕ)
dϕT
1
non-negative and have at least one positive component.

= 0. First we estimate the absolute value of the element in the i-th row and j-th column

. We use that ϕ > 0 for all ϕ ∈ Φ and that for all i ∈ Uq vectors Vq

i are

. Note that ϕ = (ϕ1, ϕ2). From (2.1), (2.2) we know that

q(ϕ) (3.4), triangle inequality for matrix

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

1
(cid:104)ϕ1, Vq

i

d[ϕ1]j

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) d(cid:2)π0
q (ϕ)(cid:3)
(cid:16)(cid:80)
(cid:16)(cid:68)
ˆϕ1,(cid:80)

˜i∈Uq

≤

=

i ]j −

(cid:105) [Vq

(cid:104)ϕ1, Vq

(cid:105) [Vq

˜i

˜i

˜i

(cid:104)ϕ1, Vq

˜i∈Uq

˜i∈Uq

1(cid:80)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:88)
(cid:105)(cid:17)2
(cid:13)(cid:13)(cid:13)(cid:80)
(cid:69) − R
(cid:88)

Vq
˜i

1

min
ϕ∈Φ

˜i∈Uq

(cid:104)ϕ1, Vq

(cid:105) =

˜i



(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) =

j

Vq
˜i

˜i

Vq
˜i

˜i∈Uq

˜i∈Uq

(cid:88)
(cid:105)(cid:17)2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤

(cid:88)
 [Vq
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:88)

− R

˜i∈Uq

(cid:43)

(cid:105)

˜i

j

Vq
˜i

(cid:104)ϕ1, Vq
i(cid:105)
(cid:104)ϕ1, Vq
˜i∈Uq

(cid:16)(cid:80)

(cid:88)
(cid:88)

i ]j − (cid:104)ϕ1, Vq
i(cid:105)
(cid:17)2
(cid:42)

˜i∈Uq

ˆϕ1,

Vq
˜i

˜i∈Uq

(cid:13)(cid:13)(cid:13)1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

.

(cid:104)ϕ1, Vq

i ]j + (cid:104)ϕ1, Vq
i(cid:105)

˜i∈Uq

˜i∈Uq

Vq
˜i

Here we used the fact that

(cid:88)

˜i∈Uq

 .



j

Vq
˜i

23

BOGOLUBSKY ET AL.

Then the 1-norm of the j-th column of the matrix dπ0
q (ϕ)
dϕT
1

satisﬁes

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤

i

(cid:88)

i∈Uq

≤ 2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) d(cid:2)π0
q (ϕ)(cid:3)
(cid:68)
ˆϕ1,(cid:80)
(cid:16)(cid:68)
ˆϕ1,(cid:80)

d[ϕ1]j

˜i∈Uq

˜i∈Uq

Vq
˜i
Vq
˜i

Here we used the fact that

˜i∈Uq

Vq
˜i
Vq
˜i
Vq
˜i

˜i∈Uq

˜i∈Uq

(cid:104)ϕ1, Vq

(cid:105) =

˜i

˜i∈Uq

˜i∈Uq

≤ 2

+ R

˜i∈Uq

max
ϕ∈Φ

(cid:16)(cid:68)
ˆϕ1,(cid:80)
(cid:13)(cid:13)(cid:13)(cid:80)
(cid:69)
(cid:13)(cid:13)(cid:13)(cid:80)
(cid:69) − R
(cid:88)
(cid:68)
ˆϕ1,(cid:80)
(cid:16)(cid:68)
ˆϕ1,(cid:80)
(cid:68)
ˆϕ2,(cid:80)
(cid:16)(cid:68)
ˆϕ2,(cid:80)
(cid:68)
ˆϕ1,(cid:80)
(cid:16)(cid:68)
ˆϕ1,(cid:80)
(cid:68)
ˆϕ2,(cid:80)
(cid:16)(cid:68)
ˆϕ2,(cid:80)

˜i∈Uq

Vq
˜i
Vq
˜i

˜i∈Uq
˜i∈Nq(i) Eq
i˜i
˜i∈Nq(i) Eq
i˜i

(cid:88)

˜i∈Uq

(cid:88)

˜i∈Uq



j

≤

Vq
˜i

(cid:104)ϕ1, Vq

˜i

(cid:105)

(cid:17)2

˜i∈Uq

Vq
˜i

Vq
˜i

(cid:13)(cid:13)(cid:13)2

(cid:43)

.

j

Vq
˜i

˜i∈Uq

˜i∈Uq

2

ˆϕ1,

+ R

˜i∈Uq

˜i∈Uq

Vq
˜i
Vq
˜i

(cid:13)(cid:13)(cid:13)(cid:80)
(cid:69) − R
(cid:13)(cid:13)(cid:13)2
(cid:88)
(cid:13)(cid:13)(cid:13)2
(cid:17)2
(cid:42)
(cid:88)
(cid:13)(cid:13)(cid:13)(cid:80)
(cid:69)
(cid:13)(cid:13)(cid:13)(cid:80)
(cid:69) − R
(cid:13)(cid:13)(cid:13)(cid:80)
(cid:69)
(cid:13)(cid:13)(cid:13)(cid:80)
(cid:69) − R
(cid:13)(cid:13)(cid:13)(cid:80)
(cid:69)
(cid:13)(cid:13)(cid:13)(cid:80)
(cid:69) − R
(cid:13)(cid:13)(cid:13)(cid:80)
(cid:69)
(cid:13)(cid:13)(cid:13)(cid:80)
(cid:69) − R

˜i∈Uq

+ R

+ R

+ R

˜i∈Uq

˜i∈Nq(i) Eq
i˜i
˜i∈Nq(i) Eq
i˜i

˜i∈Nq(i) Eq
i˜i
˜i∈Nq(i) Eq
i˜i

Vq
˜i

.

+ R

Vq
˜i

˜i∈Uq

˜i∈Uq

j∈1,...,m1

(cid:88)

Vq
˜i
Vq
˜i

(cid:88)
 (cid:88)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)2
(cid:17)2 max
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)2
(cid:17)2 max
(cid:13)(cid:13)(cid:13)2
(cid:88)

(cid:13)(cid:13)(cid:13)2
(cid:17)2 max
(cid:13)(cid:13)(cid:13)2
 (cid:88)
(cid:13)(cid:13)(cid:13)2
(cid:17)2 max

j∈1,...,m2

j∈1,...,m2

j∈1,...,m1

˜i∈Uq

Vq
˜i

j

˜i∈Nq(i)

+



j

.



Eq
i˜i

j

,

.



j

Eq
i˜i

˜i∈Nq(i)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)1

Now we have

q (ϕ)
dϕT

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) dπ0
(cid:13)(cid:13)(cid:13)(cid:13) dpi(ϕ)
(cid:13)(cid:13)(cid:13)(cid:13)1

dϕT

(cid:107)Π0

q(ϕ)(cid:107)1 ≤ 2α
pq(cid:88)

+ 2(1 − α)

i=1

In the same manner we obtain the following estimate

≤ 2

˜i∈Nq(i) Eq
i˜i
˜i∈Nq(i) Eq
i˜i
where Nq(i) = {k ∈ Vq : i → k ∈ Eq}.

Finally we have that

Vq
˜i
Vq
˜i

pq(cid:88)

i=1

This ﬁnishes the proof.

Let us assume that we have some approximation ˜π ∈ Spq (1) to the vector πq(ϕ). We deﬁne

˜Π0 = α

dπ0

q (ϕ)

dϕT + (1 − α)

dpi(ϕ)
dϕT [˜π]i

(A.12)

and consider the Method 4.6, 4.7 with the starting point Π0 = ˜Π0. Then Method 4.5, 4.6, 4.7 is a
particular case of this general method with ˜πN

q (ϕ) used as approximation ˜π.

24

LEARNING SUPERVISED PAGERANK

Lemma A.5 Let us ﬁx some q ∈ Q. Let Π0
q(ϕ) be deﬁned in (3.4) and ˜Π0 be deﬁned in (A.12),
q (ϕ) is deﬁned in (2.2), pi(ϕ)T , i ∈ 1, . . . , pq is the i-th row of the matrix Pq(ϕ) deﬁned in
where π0
(2.3). Assume that the vector ˜π satisﬁes (cid:107)˜π − πq(ϕ)(cid:107)1 ≤ ∆1. Then for the chosen functions Fq, Gq
(2.1) and set Φ it holds that.

(cid:107) ˜Π0 − Π0

q(ϕ)(cid:107)1 ≤ β1∆1 ∀ϕ ∈ Φ,

(A.13)

where β1 is deﬁned in (A.11).

Proof.

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) pq(cid:88)

i=1

dpi(ϕ)
dϕT

(˜πi − [πq(ϕ)]i)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)1

≤

(1 − α)

|˜πi − [πq(ϕ)]i| (A.10)≤ β1∆1.

(cid:107) ˜Π0 − Π0

≤ (1 − α)

q(ϕ)(cid:107)1
pq(cid:88)

i=1

(3.3),(A.12)

=

(cid:13)(cid:13)(cid:13)(cid:13) dpi(ϕ)

dϕT

(cid:13)(cid:13)(cid:13)(cid:13)1

Lemma A.6 Let us ﬁx some q ∈ Q. Let ˜Π0 be deﬁned in (A.12), where π0
q (ϕ) is deﬁned in (2.2),
pi(ϕ)T , i ∈ 1, . . . , pq is the i-th row of the matrix Pq(ϕ) deﬁned in (2.3), ˜π ∈ Spq (1). Let the
sequence Πk, k ≥ 0 be deﬁned in (4.6), (4.7) with starting point Π0 = ˜Π0. Then for the chosen
functions Fq, Gq (2.1) and set Φ for all k ≥ 0 it holds that

(cid:107)Πk(cid:107)1 ≤ β1,

(cid:13)(cid:13)(cid:13)(cid:2)P T
q (ϕ)(cid:3)k

Π0

q(ϕ)

(cid:13)(cid:13)(cid:13)1

∀ϕ ∈ Φ,
≤ β1,

∀ϕ ∈ Φ.

(A.14)

(A.15)

q(ϕ) is deﬁned in (3.4), β1 is deﬁned in (A.11).

Here Π0
Proof. Similarly as it was done in Lemma A.4 one can prove that (cid:107) ˜Π0(cid:107)1 ≤ β1. Note that all
q (ϕ) are nonnegative for all ϕ ∈ Φ. Also the matrix Pq(ϕ) is row-
elements of the matrix P T
stochastic: Pq(ϕ)epq = epq. Hence maximum 1-norm of the column of P T
q (ϕ) is equal to 1 and
(cid:107)P T
q (ϕ)(cid:107)1 = 1. Using the submultiplicative property (A.9) of the matrix 1-norm we obtain by
induction that

(cid:107)Πk+1(cid:107)1 = (cid:107)P T

q (ϕ)Πk(cid:107)1 ≤ (cid:107)P T

q (ϕ)(cid:107)1(cid:107)Πk(cid:107)1 ≤ β1.

Inequality A.15 is proved in the same way using the Lemma A.4 as induction basis.

Lemma A.7 Let the assumptions of Lemma A.6 hold. Then for any N > 1

(cid:107) ˜ΠN

q (ϕ)(cid:107)1 ≤ β1
α

,

∀ϕ ∈ Φ,

(A.16)

where ˜ΠN
(A.11).

q (ϕ) is calculated by Method 4.6, 4.7 with the starting point Π0 = ˜Π0, β1 is deﬁned in

Proof. Using the triangle inequality for the matrix 1-norm we obtain

(cid:107) ˜ΠN

q (ϕ)(cid:107)1 =

1

1 − (1 − α)N +1

(1 − α)kΠk

≤

1

1 − (1 − α)N +1

N(cid:88)

k=0

(1−α)k(cid:107)Πk(cid:107)1

(A.14)≤ β1
α

.

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

N(cid:88)

k=0

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)1

25

BOGOLUBSKY ET AL.

Lemma A.8 Let us ﬁx some q ∈ Q. Let ˜ΠN
q (ϕ) be calculated by Method 4.6, 4.7 with starting
q (ϕ) is deﬁned in (2.2), pi(ϕ)T , i ∈ 1, . . . , pq
point Π0 = ˜Π0 and dπq(ϕ)
is the i-th row of the matrix Pq(ϕ) deﬁned in (2.3). Assume that the vector ˜π ∈ Spq (1) in (A.12)
satisﬁes (cid:107)˜π − πq(ϕ)(cid:107)1 ≤ ∆1. Then for the chosen functions Fq, Gq (2.1) and set Φ, for all N > 1
it holds that

dϕT be given in (3.3), where π0

q (ϕ) − dπq(ϕ)
dϕT

≤ β1∆1
α

+

2β1
α

(1 − α)N +1,

∀ϕ ∈ Φ,

(A.17)

(cid:13)(cid:13)(cid:13)(cid:13) ˜ΠN

(cid:13)(cid:13)(cid:13)(cid:13)1

where β1 is deﬁned in (A.11).

Proof. Using (A.13) as the induction basis and making the same arguments as in the proof of

the Lemma A.6 we obtain for every k ≥ 0

(cid:16)

(cid:13)(cid:13)(cid:13)P T
(cid:13)(cid:13)(cid:13)1

q (ϕ)]kΠ0

q(ϕ)

≤ β1∆1.

=

q (ϕ)

Πk − [P T

q (ϕ)]kΠ0

q(ϕ)

≤

(cid:17)(cid:13)(cid:13)(cid:13)1

Equation 3.3 can be rewritten in the following way

Π0

q(ϕ) =

Π0

q(ϕ).

(A.18)

Using this equality and the previous inequality we obtain

∞(cid:88)

k=0

(1 − α)k(cid:2)P T
∞(cid:88)

q (ϕ)(cid:3)k
(1 − α)k(cid:2)P T

q (ϕ)(cid:3)k

(1 − α)kΠk −

(cid:13)(cid:13)(cid:13)1

Π0

q(ϕ)

k=0

≤ β1∆1
α

.

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)1

Π0

q(ϕ)

≤

(A.19)

q(ϕ)

dπq(ϕ)

(cid:13)(cid:13)(cid:13)1

q (ϕ)]k+1Π0

(cid:13)(cid:13)(cid:13)Πk+1 − [P T
(cid:13)(cid:13)(cid:13)Πk − [P T
q (ϕ)(cid:13)(cid:13)1
≤(cid:13)(cid:13)P T
dϕT =(cid:2)I − (1 − α)P T
q (ϕ)(cid:3)−1
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ∞(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ∞(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)1
(1 − α)k(cid:13)(cid:13)(cid:13)Πk −(cid:2)P T
∞(cid:88)
q (ϕ)(cid:3)k
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ˜ΠN
∞(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) (1 − α)N +1

(1 − α)kΠk − dπq(ϕ)
dϕT

1 − (1 − α)N +1

(1 − α)kΠk

q (ϕ) −

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)1

≤

k=0

k=0

k=0

k=0

=

=

1

=

1 − (1 − α)N +1
≤ β1(1 − α)N +1
1 − (1 − α)N +1

k=0

N(cid:88)
N(cid:88)
N(cid:88)

k=0

k=0

On the other hand

(4.7)
=

This inequality together with (A.19) gives (A.17).

26

(1 − α)kΠk −

(1 − α)kΠk

(1 − α)kΠk −

(1 − α)kΠk

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)1

=

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)1

(A.14)≤

k=0

∞(cid:88)
∞(cid:88)
∞(cid:88)

k=N +1

k=N +1

(1 − α)k + β1

(1 − α)k =

(1 − α)N +1.

2β1
α

LEARNING SUPERVISED PAGERANK

Lemma A.9 Assume that for every q ∈ Q the approximation ˜πq(ϕ) to the ranking vector, satisfying
(cid:107)˜πq(ϕ) − πq(ϕ)(cid:107)1 ≤ ∆1, is available. Assume that for every q ∈ Q the approximation ˜Πq(ϕ) to
the full derivative of ranking vector dπq(ϕ)

dϕT as solution of (3.3), satisfying

(cid:13)(cid:13)(cid:13)(cid:13)1

≤ ∆2

AT

q (Aq ˜πq(ϕ) + bq)+.

≤ 2r∆2 + 4r∆1 max
q∈Q

is available. Let us deﬁne

Then

q=1

dϕT

2
|Q|

˜∇f (ϕ) =

(cid:13)(cid:13)(cid:13)(cid:13) ˜Πq(ϕ) − dπq(ϕ)
(cid:17)T
(cid:16) ˜Πq(ϕ)
|Q|(cid:88)
(cid:13)(cid:13)(cid:13)∞
(cid:13)(cid:13)(cid:13) ˜∇f (ϕ) − ∇f (ϕ)
(cid:18) dπq(ϕ)
(cid:19)T
(cid:17)T
q (Aq ˜πq(ϕ) + bq)+ −(cid:16) ˜Πq(ϕ)
(cid:19)T
(cid:18) dπq(ϕ)

dϕT

where ∇f (ϕ) is the gradient (3.6) of the function f (ϕ) (3.5).

≤

AT

AT

Proof. Let us ﬁx any q ∈ Q. Then we have

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:16) ˜Πq(ϕ)
(cid:17)T
q (Aq ˜πq(ϕ) + bq)+ −
(cid:13)(cid:13)(cid:13)(cid:13)(cid:16) ˜Πq(ϕ)
(cid:17)T
AT
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:16) ˜Πq(ϕ)
(cid:17)T
(cid:13)(cid:13)(cid:13)1
≤(cid:13)(cid:13)(cid:13) ˜Πq(ϕ)
(cid:13)(cid:13)(cid:13)(cid:13) ˜Πq(ϕ) − dπq(ϕ)
(cid:107)Aq(cid:107)1 (cid:107)(Aqπq(ϕ) + bq)+ − (Aq ˜πq(ϕ) + bq)+(cid:107)∞ +

(cid:107)Aq(cid:107)1 (cid:107)(Aqπq(ϕ) + bq)+(cid:107)∞

q (Aqπq(ϕ) + bq)+ −
AT

(A.4),(A.6)≤

dϕT

dϕT

AT

AT

+

+

(cid:13)(cid:13)(cid:13)(cid:13)1

q (Aqπq(ϕ) + bq)+

q (Aqπq(ϕ) + bq)+

+

q (Aqπq(ϕ) + bq)+

,

(cid:13)(cid:13)(cid:13) ˜Πq(ϕ)
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)∞
(cid:13)(cid:13)(cid:13)(cid:13)∞
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)∞

≤

(cid:13)(cid:13)(cid:13) ˜Πq(ϕ)

(cid:13)(cid:13)(cid:13)1

(A.20)

(A.21)

≤

· r · 2∆1 + ∆2 · r · 1.

Here we used that Aq ∈ Rrq×pq and its elements are either 0 or 1 and the fact that rq ≤ r for all
q ∈ Q, and that for any matrix M ∈ Rn1×n2 (cid:107)M T(cid:107)∞ = (cid:107)M(cid:107)1.

Using this inequality and deﬁnitions (3.6), (A.20) we obtain (A.21).

Proof of Lemma 2

Let us ﬁrst prove Inequality 4.10. According to Lemma A.1 calculated vector ˜πN1

q (ϕ) satisﬁes

(cid:107)˜πN1

q (ϕ) − πq(ϕ)(cid:107)1 ≤ αδ2
12β1r

,

∀q ∈ Q.

(A.22)

This together with Lemma A.8 with ˜πN1

q (ϕ) in the role of ˜π for all q ∈ Q gives

(cid:13)(cid:13)(cid:13)(cid:13) ˜ΠN2

q (ϕ) − dπq(ϕ)
dϕT

(cid:13)(cid:13)(cid:13)(cid:13)1

≤ β1

αδ2
12β1r
α

+

2β1
α

(1 − α)N2+1 ≤ δ2
12r

+

β1
α

αδ2
4β1r

=

δ2
3r

27

BOGOLUBSKY ET AL.

This inequality together with (A.22), Lemma A.7 with ˜πN1
Lemma A.9 with ˜πN1

q (ϕ) in the role of ˜πq(ϕ) and ˜ΠN2

(cid:107)˜g(ϕ, δ2) − ∇f (ϕ)(cid:107)∞ ≤ 2r

q (ϕ) in the role of ˜π for all q ∈ Q and
q (ϕ) in the role of ˜Πq(ϕ) for all q ∈ Q gives
δ2
3r

αδ2
12β1r

= δ2.

β1
α

+ 4r

Let us now estimate number of a.o. and memory which is needed to calculate ˜g(ϕ, δ2). We use

the same notations TAO, MM, UM as in the proof of Lemma A.1.

1. We reserve vector g1 ∈ Rm to store current (in terms of steps in q) approximation of ˜g(ϕ, δ2)

and g2 ∈ Rm to store next summand in the sum (4.9). So TAO = 0, MM = UM = 2m.

2. For every q ∈ Q repeat.

2.1. Set π = ˜πN1

l : i → l;(cid:80)

j(cid:105) and(cid:80)

j∈Uq

(cid:104)ϕ1, Vq

q (ϕ). Also save in memory (cid:104)ϕ1, Vq
l:i→l(cid:104)ϕ2, Eq

j(cid:105) for all j ∈ Uq ; (cid:104)ϕ2, Eq
il(cid:105) for all i ∈ Vq,
il(cid:105) for all i ∈ Vq and the matrix Pq(ϕ).
All this data was calculated during the calculation of ˜πN1
q (ϕ), see the proof of Lemma
A.1. According to Lemma A.1 and memory used to save the listed objects we obtain
TAO = 3mpqsq + 3pqsqN1, MM = 2m + 2pqsq + nq + pqsq + 1 + pq ≤ 2m + 4pqsq,
UM = 2m + pq + nq + pqsq + 1 + pq + pqsq ≤ 2m + 3pqsq.
q (ϕ). We reserve variables Gt, G1, G2 ∈ Rpq×m to
store respectively sum in (4.7) , Πk, Πk+1 for current k ∈ 1, . . . , N2. Hence TAO =
3mpqsq + 3pqsqN1, MM = 2m + 4pqsq + 3mpq, UM = 2m + 3pqsq + 3mpq.

2.2. Now we need to calculate ˜ΠN2

2.2.1. First iteration of this method requires to calculate

˜Π0 = α

dπ0

q (ϕ)

dϕT + (1 − α)

dpi(ϕ)
dϕT [˜πN1

q

]i.

pq(cid:88)

i=1

2.2.1.1. We ﬁrst calculate G1 = α

Equalities 2.1 one has for all i ∈ Uq, l = 1, . . . , m1

dπ0
q (ϕ)
dϕT .

In accordance to its deﬁnition (2.2) and

(cid:35)

l

α[π0
q ]i
dϕ

(cid:34)
(cid:105)
(cid:104) α[π0
a(cid:80)

q ]i

dϕ

=


(cid:80)
j(cid:105), v =(cid:80)

αVq
i
(cid:104)ϕ1, Vq

j(cid:105) −

j∈Uq

(cid:16)(cid:80)

α(cid:104)ϕ1, Vq
i(cid:105)
(cid:104)ϕ1, Vq
j∈Uq



l

(cid:88)

j(cid:105)(cid:17)2
α(cid:80)

j∈Uq

Vq
j

b =

Vq

j∈Uq

and

j∈Uq

l
(cid:104)ϕ1,Vq

= 0 for l = m1 + 1, . . . , m. We set a =

j(cid:105) and
j. This requires 2 + m1nq a.o. and 2 + m1
memory items. Now the calculation of all non-zero elements of α
takes
4m1nq a.o. since for ﬁxed i, l we need 4 a.o. We obtain TAO = 3mpqsq +
3pqsqN1 + 5m1nq + 2, MM = 2m + 4pqsq + 3mpq + m1 + 2, UM =
2m + 3pqsq + 3mpq.

q (ϕ)
dϕT

(cid:104)ϕ1,Vq

j∈Uq

dπ0

2.2.1.2. Now we calculate ˜Π0. For every i = 1, . . . , pq the matrix (1−α) dpi(ϕ)

Rpq×m is calculated in the same way as the matrix α
ﬁcations due to dpi(ϕ)
dϕT
1

]i ∈
dπ0
q (ϕ)
dϕT with obvious modi-
= 0 and number of non-zero elements in vector pi(ϕ) is

dϕT [˜πN1

q

28

LEARNING SUPERVISED PAGERANK

q

not more than sq. We also use additional a.o. number and memory amount to
calculate and save (1 − α)[˜πN1
]i. We save the result for current i in G2. So for
ﬁxed i we need additionally 3 + 5m2sq a.o and 3 + m2 memory items. Also on
every step we set G1 = G1 + G2 which requires not more than m2sq a.o. since
at every step G2 has not more than m2sq non-zero elements. We set Gt = G1.
Note that Gt always has a block of (pq−nq)×m1 zero elements and hence has
not more than m2pq + m1nq non-zero elements. At the end we obtain TAO =
3mpqsq + 3pqsqN1 + 5m1nq + 2 + pq(3 + 5m2sq + m2sq) + m2pq + m1nq,
MM = 2m + 4pqsq + 3mpq + m1 + 2 + m2 + 3 ≤ 3m + 4pqsq + 3mpq + 5,
UM = 2m + pqsq + 3mpq + pq (since we need to store in memory only
g1, g2, Gt, G1, G2, P T

q (ϕ), π).

2.2.2. Set a = (1 − α).
2.2.3. For every step k from 1 to N2

2.2.3.1. We set G2 = P T

q (ϕ)G1. In this pperation potentially each of pqsq elements
q (ϕ) needs to be multiplied my m elements of matrix G1 and this

of matrix P T
multiplication is coupled with one addition. So in total we need 2mpqsq a.o.

2.2.3.2. We set Gt = Gt + aG1. This requires 2m1nq + 2m2pq a.o.
2.2.3.3. We set a = (1 − α)a.
2.2.3.4. In total every step requires not more than 2mpqsq + 2m1nq + 2m2pq + 1 a.o.
2.2.4. At the end o this stage we have. TAO = 3mpqsq + 3pqsqN1 + 5m1nq + 2 +
pq(3 + 5m2sq + m2sq) + m2pq + m1nq + N2(2mpqsq + 2m1nq + 2m2pq + 1),
MM = 3m + 4pqsq + 3mpq + 5, UM = 2m + mpq + pq (since we need to store
in memory only g1, g2, Gt, π).

α

1−(1−α)a Gt. This takes 3 + m2pq + m1nq a.o.

2.2.5. Set Gt =
2.2.6. At the end o this stage we have. TAO = 3mpqsq + 3pqsqN1 + 5m1nq + 2 + pq(3 +
5m2sq + m2sq) + m2pq + m1nq + N2(2mpqsq + 2m1nq + 2m2pq + 1) + 3 +
m2pq + m1nq, MM = 3m + 4pqsq + 3mpq + 5, UM = 2m + mpq + pq (since we
need to store in memory only g1, g2, Gt, π).

q (ϕ) + bq)+. This requires additionally 3rq a.o. and rq memory.

q u. This requires additionally 4rq a.o.
t π. This requires additionally 2m1nq + 2m2pq a.o.

2.3. Calculate u = (Aq ˜πN1
2.4. Calculate π = AT
2.5. Calculate g2 = GT
2.6. Set g1 = g1 + g2. This requires additionally m a.o.
2.7. At the end we have TAO = 3mpqsq +3pqsqN1 +5m1nq +2+pq(3+5m2sq +m2sq)+
m2pq + m1nq + N2(2mpqsq + 2m1nq + 2m2pq + 1) + 3 + m2pq + m1nq + 7rq +
2m1nq + 2m2pq + m, MM = 3m + 4pqsq + 3mpq + 5 + rq, UM = 2m (since we need
to store in memory only g1, g2).

3. Set g1 = 2|Q| g1. This requires additionally m + 1 a.o.

4. At the end we have TAO = (cid:80)

q∈Q(3mpqsq + 3pqsqN1 + 5m1nq + 2 + pq(3 + 5m2sq +
m2sq) + m2pq + m1nq + N2(2mpqsq + 2m1nq + 2m2pq + 1) + 3 + m2pq + m1nq +
7rq + 2m1nq + 2m2pq + m) + m + 1 ≤ |Q|(10mps + 3psN1 + 3mpsN2 + 7r), MM =
3m + 5 + maxq∈Q(4pqsq + 3mpq + rq) ≤ 4ps + 4mp + r, UM = m (since we need to store
in memory only g1).

29

BOGOLUBSKY ET AL.

Appendix B. Missed proofs for Section 5
Consider smoothed counterpart of the function f (x):

fµ(x) = Ef (x + µζ) =

1
VB

(cid:90)

B

f (x + µζ)dζ,

where ζ is uniformly distributed over unit ball B = {t ∈ Rm : (cid:107)t(cid:107)2 ≤ 1} random vector, VB is the
volume of the unit ball B, µ ≥ 0 is a smoothing parameter. This type of smoothing is well known.

It is easy to show that
• If f is convex, then fµ is also convex
L ((cid:107) · (cid:107)2), then fµ ∈ C1,1
• If f ∈ C1,1
• If f ∈ C1,1
L ((cid:107) · (cid:107)2), then f (x) ≤ fµ(x) ≤ f (x) + Lµ2
The random gradient-free oracle is usually deﬁned as follows
(f (x + µξ) − f (x))ξ,

L ((cid:107) · (cid:107)2).

gµ(x) =

2

m
µ

for all x ∈ Rm.

where ξ is uniformly distributed vector over the unit sphere S = {t ∈ Rm : (cid:107)t(cid:107)2 = 1}. It can be
shown that Egµ(x) = ∇fµ(x). Since we can use only inexact zeroth-order oracle we also deﬁne
the counterpart of the above random gradient-free oracle which can be really computed:

gµ(x, δ) =

m
µ

( ˜f (x + µξ, δ) − ˜f (x, δ))ξ.

The idea is to use gradient-type method with oracle gµ(x, δ) instead of the real gradient in order
to minimize fµ(x). Since fµ(x) is uniformly close to f (x) we can obtain a good approximation to
the minimum value of f (x).

We will need the following lemma.

Lemma B.10 Let ξ be random vector uniformly distributed over the unit sphere S ∈ Rm. Then

Eξ((cid:104)∇f (x), ξ(cid:105))2 =

Proof. We have Eξ((cid:104)∇f (x), ξ(cid:105))2 = 1

(cid:82)
(B.1)
S((cid:104)∇f (x), ξ(cid:105))2dσ(ξ), where Sm(r) is the volume
of the unit sphere which is the border of the ball in Rm with radius r, σ(ξ) is unnormalized spherical
(cid:90) π
measure. Note that Sm(r) = Sm(1)rm−1. Let ϕ be the angle between ∇f (x) and ξ. Then

(cid:90)

Sm(1)

(cid:107)∇f (x)(cid:107)2
2.

1
m

((cid:104)∇f (x), ξ(cid:105))2dσ(ξ) =

1

(cid:107)∇f (x)(cid:107)2

2 cos2 ϕSm−1(sin ϕ)dϕ =

0

30

1

Sm(1)

S

=

Sm−1(1)
Sm(1)

(cid:107)∇f (x)(cid:107)2

2

(cid:90) π

Sm(1)

0

cos2 ϕ sinm−2 ϕdϕ

First changing the variable using equation x = cos ϕ, and then t = x2, we obtain

x2(1 − x2)(m−3)/2dx =

t1/2(1 − t)(m−3)/2dt =

(cid:90) 1

0

(cid:90) π

0

cos2 ϕ sinm−2 ϕdϕ =
√

(cid:19)

(cid:18) 3

m − 1

= B

,

2

2

=

where Γ(·) is the Gamma-function and B is the Beta-function. Also we have

LEARNING SUPERVISED PAGERANK

(cid:90) 1
πΓ(cid:0) m−1
(cid:1)
(cid:1) ,
2Γ(cid:0) m+2

−1

2

2

Sm−1(1)
Sm(1)

(cid:18)

=

m − 1
√
π
m

(cid:1)
Γ(cid:0) m+2
(cid:1) .
Γ(cid:0) m+1
(cid:19) Γ(cid:0) m−1
(cid:1)
2Γ(cid:0) m+1
(cid:1) = (cid:107)∇f (x)(cid:107)2

2

2

2

2

(cid:1) =

(B.2)

(B.3)

2
Finally using the relation Γ(m + 1) = mΓ(m), we obtain

E((cid:104)∇f (x), ξ(cid:105))2 = (cid:107)∇f (x)(cid:107)2

2

1 − 1
m

(cid:19)

(cid:18)

1 − 1
m

Γ(cid:0) m−1
(cid:1)
2 Γ(cid:0) m−1

2

2

2 m−1

=

1
m

(cid:107)∇f (x)(cid:107)2

2

Lemma B.11 Let f ∈ C1,1

L ((cid:107) · (cid:107)2). Then, for any x, y ∈ Rm,
2 ≤ m2µ2L2 + 4m(cid:107)∇f (x)(cid:107)2

E(cid:107)gµ(x, δ)(cid:107)2
2 +
− E(cid:104)gµ(x, δ), x − y(cid:105) ≤ −(cid:104)∇fµ(x), x − y(cid:105) +

8δ2m2

µ2
δm
µ

(cid:107)x − y(cid:107)2.

Proof. Using (5.1) we obtain

( ˜f (x + µξ, δ) − ˜f (x, δ))2 =
(f (x + µξ) − f (x) − µ(cid:104)∇f (x), ξ(cid:105) + µ(cid:104)∇f (x), ξ(cid:105) + ˜δ(x + µξ) − ˜δ(x))2 ≤
2(f (x + µξ) − f (x) − µ(cid:104)∇f (x), ξ(cid:105) + µ(cid:104)∇f (x), ξ(cid:105))2 + 2(˜δ(x + µξ) − ˜δ(x))2 ≤

L(cid:107)ξ(cid:107)2

+ 4µ2((cid:104)∇f (x), ξ(cid:105))2 + 8δ2 = µ4L2(cid:107)ξ(cid:107)4 + 4µ2((cid:104)∇f (x), ξ(cid:105))2 + 8δ2

(cid:18) µ2

2

4

(cid:19)2

Using (B.1), we get

Eξ(cid:107)gµ(x, δ)(cid:107)2

2 ≤ m2
µ2Vs

(cid:90)

S

(cid:0)µ4L2(cid:107)ξ(cid:107)4 + 4µ2((cid:104)∇f (x), ξ(cid:105))2 + 8δ2(cid:1)(cid:107)ξ(cid:107)2

2dσ(ξ) =

= m2µ2L2 + 4m(cid:107)∇f (x)(cid:107)2

2 +

8δ2m2

.

µ2

Using the equality Eξgµ(x) = ∇fµ(x), we have

(fδ(x + µξ) − fδ(x))(cid:104)ξ, x − y(cid:105)dσ(ξ) =

(cid:90)

− Eξ(cid:104)gµ(x, δ), x − y(cid:105) = − m
µVs
= − m
µVs

(cid:90)

S

− m
µVs

(f (x + µξ) − f (x))(cid:104)ξ, x − y(cid:105)dσ(ξ)−
S
(˜δ(x + µξ) − ˜δ(x))(cid:104)ξ, x − y(cid:105)dσ(ξ) ≤ −(cid:104)∇fµ(x), x − y(cid:105) +

(cid:107)x − y(cid:107).

δm
µ

(cid:90)

S

31

BOGOLUBSKY ET AL.

Let us denote ψ0 = f (x0), and ψk = EUk−1f (xk), k ≥ 1.
We say that the smooth function is strongly convex with parameter τ ≥ 0 if and only if for any

x, y ∈ Rm it holds that

f (x) ≥ f (y) + (cid:104)∇f (y), x − y(cid:105) +

(cid:107)x − y(cid:107)2.

(B.4)
L ((cid:107) · (cid:107)2) and convex. Assume that x∗ ∈ intX and the

τ
2

Theorem 1 (extended) Let f ∈ C1,1

sequence xk be generated by Algorithm 1 with h = 1

where f∗ is the solution of the problem minx∈X f (x).
constant τ, then

EUM−1f (ˆxM ) − f∗ ≤ 8mLD2
(cid:16)

M + 1

(cid:18)

+

(cid:17)M

+

(cid:19)

8mL. Then for any M ≥ 0, we have
µ2L(m + 8)

δmD

+

δ2m
Lµ2 ,

4µ

8
If, moreover, f is strongly convex with

where δµ = µ2L(m+8)

4τ

L

ψM − f∗ ≤ 1
2
τ µ + 2mδ2
τ µ2L .

+ 4mδD

δµ +

1 − τ

16mL

(D2 − δµ)

,

(B.5)

Proof. We extend the proof in Nesterov and Spokoiny (2015) for the case of randomization on
a sphere (instead of randomization based on normal distribution) and for the case when one can
calculate the function value only with some error of unknown nature.
Consider the point xk, k ≥ 0 generated by the method on the k-th iteration. Denote rk =
(cid:107)xk − x∗(cid:107)2. Note that rk ≤ D. We have:
k+1 = (cid:107)xk+1 − x∗(cid:107)2
r2
= (cid:107)xk − x∗(cid:107)2

2 − 2h(cid:104)gµ(xk, δ), xk − x∗(cid:105) + h2(cid:107)gµ(xk, δ)(cid:107)2
2.

2 ≤ (cid:107)xk − x∗ − hgµ(xk, δ)(cid:107)2

2 =

Taking the expectation with respect to ξk we get

2δmh

µ

rk+

Eξk r2

+ h2

≤ r2

k+1

(cid:19)
k − 2h(cid:104)∇fµ(xk), xk − x∗(cid:105) +
≤

8δ2m2

(B.2),(B.3)≤ r2

m2µ2L2 + 4m(cid:107)∇f (xk)(cid:107)2

(cid:18)
(cid:18)
k − 2h(f (xk) − fµ(x∗)) +

δmhD

4µ

+

2 +

µ2

+ h2

≤ r2

m2µ2L2 + 8mL(f (xk) − f∗) +
k − 2h(1 − 4hmL)(f (xk) − f∗) +
≤

8δ2m2h2

+ m2h2µ2L2 + hLµ2 +

8δ2m2

µ2

δmhD

4µ

(cid:19)

+

≤

− f (xk) − f∗

µ2

+

≤ r2

k +

Dδ
4µL

µ2(m + 8)

+

δ2

8µ2L2 .

(B.6)

64m
Taking expectation with respect to Uk−1 and deﬁning ρk+1
µ2(m + 8)

ρk+1 ≤ ρk − ψk − f∗

8mL

+

8mL

def= EUk r2

k+1 we obtain

+

Dδ
4µL

+

δ2

8µ2L2 .

64m

32

LEARNING SUPERVISED PAGERANK

Summing up these inequalities from k = 0 to k = M and dividing by M + 1 we obtain (5.2)

Estimate 5.2 also holds for ˆψM
Now assume that the function f (x) is strongly convex. From (B.6) we get

def= EUM−1f (ˆxM ), where ˆxM = arg minx{f (x) : x ∈ {x0, . . . , xM}}.

Eξk r2

r2
k +
Taking expectation with respect to Uk−1 we obtain

16mL

1 − τ

Dδ
4µL

+

µ2(m + 8)

64m

+

δ2

8µ2L2

k+1

(B.4)≤ (cid:16)
ρk+1 ≤(cid:16)

(cid:17)

16mL

1 − τ

(cid:17)
ρk+1 − δµ ≤(cid:16)
≤(cid:16)

1 − τ

16mL

ρk +

Rδ
µL

+

µ2(m + 8)

64m

+

δ2

8µ2L2

(cid:17)

(ρk − δµ) ≤

1 − τ

(cid:17)k+1

16mL
(ρ0 − δµ).

and

Using the fact that ρ0 ≤ D2 and ψk − f∗ ≤ 1

2 Lρk we obtain (B.5).

Appendix C. Missed proofs for Section 6
We will need the following two results obtained in Ghadimi and Lan (2014).

Lemma C.12 Let xX (¯x, g, γ) be deﬁned in (6.5) and gX (¯x, g, γ) be deﬁned in (6.6). Then, for any
¯x ∈ X, g ∈ E∗ and γ > 0, it holds

(cid:104)g, gX (¯x, g, γ)(cid:105) ≥ (cid:107)gX (¯x, g, γ)(cid:107)2 +

(h(xX (¯x, g, γ)) − h(x)).

1
γ

Lemma C.13 Let gX (¯x, g, γ) be deﬁned in (6.6). Then, for any g1, g2 ∈ E∗, it holds

(cid:107)gX (¯x, g1, γ) − gX (¯x, g2, γ)(cid:107) ≤ (cid:107)g1 − g2(cid:107)∗

(C.1)

(C.2)

Proof of Theorem 3. First of all let us show that the procedure of search of point wk satisfying
(6.7), (6.8) is ﬁnite. It follows from the fact that for Mk ≥ L the following inequality follows from
(6.2):

˜f (wk, δ) − ε
16Mk

which is (6.8).

(6.2)≤ f (wk)

(6.2)≤ ˜f (xk, δ) + (cid:104)˜g(xk, δ), wk − xk(cid:105) +

(cid:107)wk − xk(cid:107)2 +

L
2

ε

16Mk

33

BOGOLUBSKY ET AL.

Let us now obtain the rate of convergence. Using deﬁnition of xk+1 and (6.8) we obtain for any

k = 0, . . . , M

(6.2)≤ ˜f (wk, δ)
= f (wk) − ε
16Mk
(cid:107)xk+1 − xk(cid:107)2 +

Mk
2

f (xk+1) − ε
16Mk
+ (cid:104)˜g(xk, δ), xk+1 − xk(cid:105) +
= ˜f (xk, δ) − 1
Mk

(cid:28)

˜g(xk, δ), gX

xk, ˜g(xk, δ),

+

1

2Mk

≤ f (xk) +

(cid:18)

(cid:13)(cid:13)(cid:13)(cid:13)gX
(cid:13)(cid:13)(cid:13)(cid:13)gX

(cid:34)

xk, ˜g(xk, δ),

−

1
Mk

16Mk

ε

(cid:18)

xk, ˜g(xk, δ),

1
Mk

(cid:13)(cid:13)(cid:13)(cid:13)gX

1
Mk

+

1

2Mk

This leads to

ψ(xk+1) ≤ ψ(xk) − 1
2Mk

xk, ˜g(xk, δ),

for all k = 0, . . . , M.

Summing up these inequalities for k = 0, . . . , N we get

8Mk

ε

+

(cid:18)
(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:18)
(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)gX
≤ N(cid:88)

(cid:18)

+

1

ε

.

8Mk

2Mk

N(cid:88)

k=0

1
Mk

8Mk
1
Mk
(6.2),(C.1)≤

=

+

(6.6),(6.7)

(6.8)≤ ˜f (xk, δ)+
(cid:19)(cid:29)
ε
(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)2

xk, ˜g(xk, δ),

1
Mk

+ h(xk+1) − h(xk)

+

(cid:35)

(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)2

+

ε

4Mk

.

(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)2 ≤

1
Mk

1
Mk

(cid:18)

(cid:13)(cid:13)(cid:13)(cid:13)gX

1

2Mk

k=0

xk, ˜gk,

(cid:18)

(cid:13)(cid:13)(cid:13)(cid:13)gX

xˆk, ˜gˆk,

1
Mˆk

(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)2 N(cid:88)

k=0

≤ ψ(x0) − ψ(xN +1) +

ε
4

Hence using the fact that Mk ≤ 2L for all k ≥ 0 (which easily follows from the ﬁrst argument

of the proof) and that for all x ∈ X ψ(x) ≥ ψ∗ > −∞, we obtain

(cid:18)

(cid:13)(cid:13)(cid:13)(cid:13)gX

xˆk, ˜gˆk,

1
Mˆk
4L(ψ(x0) − ψ∗)

(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)2 ≤

+

ε
2

,

(cid:32)

1(cid:80)N

k=0

1

2Mk

(cid:33)

N(cid:88)

k=0

ε
4

1
Mk

Mk≤2L≤

ψ(x0) − ψ∗ +

N + 1

which is (6.9).

The estimate for the number of checks of Inequality 6.8 is proved in the same way as in Nesterov

and Polyak (2006).

34

