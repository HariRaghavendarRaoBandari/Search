MacroBase: Analytic Monitoring for the Internet of Things

Peter Bailis†‡, Deepak Narayanan†, Samuel Madden‡∗

†Stanford University

‡MIT CSAIL ∗Cambridge Mobile Telematics

6
1
0
2

 
r
a

 

M
7
1

 
 
]

B
D
.
s
c
[
 
 

2
v
7
6
5
0
0

.

3
0
6
1
:
v
i
X
r
a

ABSTRACT
An increasing proportion of data today is generated by automated
processes, sensors, and systems—collectively, the Internet of Things
(IoT). A key challenge in IoT data management and a core aspect
of many IoT applications (e.g., industrial diagnostics, predictive
maintenance, and equipment auditing) is in identifying and high-
lighting unusual and surprising data (e.g., poor driving behavior,
equipment failures, physical intrusion). We call this task, which is
often statistical in nature and time-sensitive, analytic monitoring.
To facilitate rapid development and scalable deployment of analytic
monitoring queries, we have developed MacroBase, a data analyt-
ics engine that performs analytic monitoring of IoT data streams.
MacroBase implements a customizable pipeline of outlier detection,
summarization, and ranking operators. For efﬁcient and accurate
execution, MacroBase implements several cross-layer optimizations
across robust estimation, pattern mining, and sketching procedures,
allowing order-of-magnitude speedups. As a result, MacroBase
can analyze up to 1M events per second on a single core. Mac-
roBase has already delivered meaningful analytic monitoring results
in production at a medium-scale IoT startup.

1.

INTRODUCTION

The much-heralded Internet of Things (IoT) signals a shift in
the collection and use of large-scale data. A decreasing proportion
of data collected today is generated directly by humans; instead,
automated processes (e.g., monitoring infrastructure, web servers)
and sensors (e.g., automotive diagnostics, heathcare wearables) gen-
erate increasing data volumes. While many have predicted similar
shifts in the past, including visions of ubiquitous computing [48],
smart dust [47] and large-scale sensor networks [6], IoT is widely
anticipated to ﬁnally become mainstream, with tens of billions of
dollars in recent commercial investment [34]. Today’s combination
of inexpensive commodity hardware (e.g, embedded and system-
on-chip devices), widespread access to communication networks
(e.g., cellular and Wi-Fi), and decreased storage cost (both price
per bit and readily-available inexpensive storage software) present
unprecedented opportunity for IoT adoption at scale.

In many IoT applications, this ﬂood of machine-generated data
primarily constitutes largely uninteresting reports of normal-case

behavior. That is, a key challenge in IoT data management is in dis-
covering deviations from this normal case, in highlighting unusual
and surprising behavior in massive IoT datasets that are too large for
manual inspection. This process, which we call analytic monitoring,
is at the core of many IoT applications, including equipment moni-
toring (e.g., detecting failing manufacturing componentry), intrusion
detection (e.g., building break-ins), and environmental observation
(e.g., occupancy and HVAC control). In these applications, ana-
lytic monitoring is critical to reliability, efﬁciency, and observability.
Moreover, their i.) extreme scale (hundreds of thousands to millions
of complex events per second) and ii.) demands for timely (i.e.,
minutes to seconds or shorter) yet complex statistical analysis pose
serious challenges for existing data analysis infrastructure.
1.1 Case Study: Analytic Monitoring at CMT
As a motivating case study, in this paper, we deploy analytic
monitoring at Cambridge Mobile Telematics (CMT), a ﬁve-year-old
commercial IoT company. CMT’s mission is to make roads safer
by making drivers more aware of their driving habits. CMT has
commercialized technology originally developed in the CarTel re-
search project [29], providing drivers with a smartphone application
and optional mobile sensor for their vehicles. This sensor data is
analyzed to provide users with feedback about their driving. CMT
collects data from many hundreds of thousands of vehicles using
both smartphones and custom sensors at rates of many tens of Hz.
Commodity cellular and Wi-Fi connections perform data backhaul
to a cloud-based processing environment, where it is analyzed.

Analytic monitoring has proven especially challenging at CMT.
At high data rates, manual inspection is essentially impossible. Is
the CMT application behaving as expected? Are all users able to
upload and view their trips? Are sensors operating at a sufﬁciently
granular rate and in a power-efﬁcient manner? These questions
are difﬁcult to answer, and, as a result of high data volumes, many
potential problems and inefﬁciences go unnoticed. The most severe
problems in the CMT application are caught by quality assurance
and customer service, but many behaviors are more pernicious. For
example, Apple iOS 9.0 beta 1 introduced a buggy Bluetooth stack
that prevented iOS devices from connecting to CMT’s sensors. Few
devices ran these versions, so the overall failure rate was low; as a re-
sult, CMT’s data volume obscured a potentially serious widespread
issue in later releases of the application. While it is infeasible to
manually inspect every trip, given low storage costs, CMT records
all of the data required to perform analytic monitoring and detection
of these behaviors. What CMT has lacked is a practical solution for
doing so in a timely, efﬁcient, and automated manner.
1.2 MacroBase: Analytic Monitoring for IoT
To address the challenges of timely, large-scale analytic mon-
itoring, we introduce MacroBase, the ﬁrst end-to-end engine de-

1

signed for low-latency analytic monitoring of IoT and other machine-
generated data.1 MacroBase executes analytic monitoring queries
on incoming data by exposing commonalities among abnormal data;
this requires combining techniques from both outlier detection and
data summarization.2. For example, we have used MacroBase to
help CMT improve the quality of its applications, by identifying
combinations of hardware platforms and software versions that
behave unexpectedly. MacroBase is specialized for analytic moni-
toring and provides a pipeline of high-performance operators that
require limited tuning and no supervision to produce reports. In ad-
dition, MacroBase is extensible, and users can implement their own
outlier detection and data summarization techniques to complement
its existing operators. Thus, MacroBase bridges the gap between
specialized, task-speciﬁc monitoring systems (e.g., credit card fraud
monitoring), which are useful but are not designed for extensibility,
and traditional stream processing engines, which implement rela-
tional queries or tuple-at-a-time dataﬂow but do not implement the
complex analytical queries as required by analytic monitoring.

In this paper, we describe how MacroBase provides high scale
and low latency analytic monitoring out of the box by leveraging
several properties of IoT data, which we quantify in Section 6:
1. Data produced by IoT applications often exhibits regular struc-
ture (i.e., comes from some “ordinary” distribution). If we can
automatically recognize this structure, we can exploit it to im-
prove both reporting quality and efﬁciency.

2. Interesting behaviors arising in IoT processes are often systemic.
When interesting behaviors occur, or when something goes awry
in a system, many data points will exhibit the trend (e.g., all
phones with the iOS beta version generated problematic trips).

3. Despite being systemic, interesting behaviors are often relatively
rare. Many interesting IoT events are uncommon. We can use
the rarity of interesting events to eliminate unnecessary work and
pass over the majority of data that is uninteresting.

These observations in turn inform several design choices. First,
to ﬁnd anomalous data, MacroBase employs robust statistical es-
timation [27, 36], a theoretically sound but infrequently employed
branch of statistics, to determine what “normal” behavior looks
like (leveraging insight 1). To provide low latency, MacroBase
implements novel incremental maintenance techniques for these
estimators. Second, MacroBase subsequently uses these estimators
to ﬁnd data that does not ﬁt the normal behavior. To succinctly
summarize its ﬁndings, MacroBase correlates categorical metadata
regarding data points to ﬁnd commonalities between outliers that
are uncommon among inliers (leveraging insight 2). Third, at sev-
eral steps, MacroBase aggressively prunes data to make computa-
tion more efﬁcient—for example, summarizing outliers ﬁrst due to
their restricted cardinality, then ﬁltering inliers before summariz-
ing them (leveraging insight 3). To do so, MacroBase implements
several novel streaming data structures, including a new, amortized-
overhead heavy-hitters sketch and a novel, stream-aware preﬁx tree
to summarize interesting combinations of commonalities. These
cross-layer optimizations lead to two to three order of magnitude
speedups over existing standalone algorithms for each operator.

Our contributions in this paper are as follows:
• MacroBase, a turn-key, end-to-end system and architecture

for integrated analytic monitoring of IoT data streams.

• IoT-speciﬁc optimizations for improving the efﬁciency of
detection and summarization pipelines, including exploiting

1Open source at http://macrobase.io/
2Demo available at https://www.youtube.com/watch?v=077ZyyuDXYY

2

regular structure in IoT data, cardinality imbalance between
outliers and inliers, and redundancy in data summarization.
• Techniques for streaming maintenance of outlier detection
and summarization pipelines, based on extensive use of novel
sampling-, sketch-, and tree-based streaming data structures.
We evaluate MacroBase using production data from CMT and other
real-world data. As a result of the optimizations and data structures
we develop in this paper, our MacroBase prototype allows for inter-
active exploration and high-performance streaming deployment—up
to 1M points per second on a single core—while producing mean-
ingful results that we have already leveraged to ﬁnd previously
unknown bugs in production systems at CMT.

The remainder of this paper proceeds as follows. Section 2
presents the MacroBase interaction model, more formally deﬁnes
the analytic monitoring problem for IoT data, and provides addi-
tional motivating examples from both CMT and other practitioner
reports. Section 3 presents the high-level MacroBase architecture.
Section 4 describes MacroBase’s batch and streaming outlier detec-
tion operators well as optimizations for improving their efﬁciency,
while Section 5 provides the same for MacroBase’s summarization
stage. Section 6 discusses our experiences ﬁnding a set of anoma-
lous behaviors in production at CMT and evaluates the statistical
accuracy and empirical performance of MacroBase as well as the
impact of each of its optimizations. Section 7 discusses ongoing ex-
tensions to MacroBase in research and industry. Section 8 provides
a comparison with related work, and Section 9 concludes.

2. USER INTERFACE AND INTERACTION
The goal of MacroBase is to provide users with an end-to-end
system capable of highlighting “interesting” IoT data and explaining
(or summarizing) the properties of the data that make it interesting.
In this section, we provide three additional motivating use cases and
provide a description of the MacroBase interaction model.
2.1 Analytic Monitoring Scenarios

To further illustrate opportunities for analytic monitoring, we
provide three additional motivating use cases that we will use to
illustrate MacroBase’s interaction model.
CMT. In addition to the iOS application described in Section 1,
CMT has experienced other issues related to deploying applications
in the real world. For example, a popular ISP’s DNS entries for the
CMT cloud provider were misconﬁgured, leading to lost trips when
the application was used over cellular connections. When operating
over Wi-ﬁ, the application functioned correctly.
Amazon. Amazon DynamoDB recently suffered a failure that
resulted in outages at several major web properties including Netﬂix
and Reddit. The Amazon operators reported that “after we addressed
the key issue...we were left with a low overall error rate, hovering
between 0.15-0.25%. We knew there would be some cleanup to
do after the event,” and therefore the system operators deferred
maintenance. However, without more sophisticated analysis, the
engineers “did not realize soon enough that this low overall error rate
was giving some customers disproportionately high error rates” [1].
Horsehead. A explosion and ﬁre in July 2010 killed two workers at
Horsehead Holding Corp.’s Monaca, PA, zinc manufacturing plant.
A postmortem review by the US Chemical Safety board revealed
that “the high rate-of-change alarm warned that the [plant] was in
imminent danger 10 minutes before it exploded, but there appears
to have been no speciﬁc alarm to draw attention of the operator
to the subtle but dangerous temperature changes that were taking

place much (i.e. hours) earlier.” The auditor noted that “it should be
possible to design a more modern control system that could draw
attention to trends that are potentially hazardous” [30].

In each of these cases, the data required for analysis was present;

however, the infrastructure to automate the analysis was not.

2.2

Interaction Model

MacroBase ﬁlls the void in functionality illustrated by the above
use cases. In this section, we present MacroBase’s end-user inter-
action model, including its query semantics. The remainder of this
paper is devoted to implementing this functionality.
Queries. Users interact with MacroBase by issuing queries over
IoT data sources such as Postgres, Kafka, or Amazon Kinesis. From
these sources, MacroBase ingests a sequence of multi-dimensional
data points (e.g., columns in Postgres). At CMT, each point might
represent a trip, with dimensions including user ID, device model,
ﬁrmware version, application version, trip time, and battery drain.
Each query contains two sets of dimensions: metrics, or mea-
surements (e.g., trip time), and attributes, or metadata about the
measurements (e.g., user ID and device ID). In this paper, we con-
sider continuous, real-valued metrics and categorical attributes (al-
though we discuss real-valued attributes in Section 7). Thus, each
point p can be decomposed into a pair p = (m,a), with m ∈ RN
and a ∈ A1 ×···× Am for a ﬁnite set of domains A1 through Am. In
our CMT query, we could choose trip time and battery drain as our
metrics and user ID as our attribute.
Semantics. Informally, given a query with metrics M and attributes
A over a data source D, MacroBase returns a set of attribute values
in A that are representative of the most interesting subsets of D
according to their metric values in M. In our example query from
CMT, MacroBase might report that all of user 5052’s trips have
short duration, indicating a potential problem.

To determine whether a point is “interesting,” MacroBase sup-
ports a range of scoring functions, which we discuss in Section 3.
Its default functions are unsupervised; this allows MacroBase to
produce an initial set of results without user intervention, which can
be followed by more advanced interactions if desired.

Rather than return all interesting data points, MacroBase queries
return summaries of interesting data, in the form of predicates over
the input attributes (e.g., device ID 5052). As we describe in detail
in Section 5, MacroBase ﬁnds combinations of attributes that are
common among outlier points but uncommon among inlier points.
More precisely, this corresponds to combinations of attributes with
(conﬁgurable) minimum occurrence among the outliers and (con-
ﬁgurable) maximum relative occurrence among the inliers across
all dimensions (e.g., 3x more common in outliers than inliers). At
CMT, MacroBase could highlight devices that were found in at least
0.1% of outlier trips and in no more than 0.3% of inlier trips.
Example queries. To detect the ISP problem at CMT, MacroBase
could have used failed or short trips as metrics and ISP and country
as attributes. To detect the subgroup at DynamoDB, MacroBase
could have used failed requests as a metric and customer ID, rack
number, or IP address as attributes. To detect the Horsehead pressure
losses, MacroBase could have used the pressure gauge readings
as metrics and their locations as attributes. Today, conﬁguring
these attributes and metrics is a manual process; ongoing extensions
(Section 7) seek to automate this.
Operating modes. MacroBase supports three operating modes.
First, MacroBase’s graphical front-end allows users to interactively
explore their data by selecting different inputs and selecting different
metrics and attributes. This is typically the ﬁrst step in interacting

3

with the system. Second, MacroBase can execute a batch query
that can be run programmatically in a one-shot execution. Third,
MacroBase can also produce a streaming query that can be run
programmatically in a continuous manner. In streaming mode, Mac-
roBase continuously ingests and processes a potentially inﬁnite
stream of data points, re-rendering the query result and, if desired,
triggering automated alerting for downstream consumers. Mac-
roBase supports exponentially decayed damped-window streaming,
giving precedence to recent items (e.g., decreasing the importance
of points at a rate of 50% every hour). Users can generate batch
and streaming queries from the GUI, allowing easy transition from
exploration to deployment.
Out-of-box experience. Under default operation (the subject of
the remainder of this paper), MacroBase users do not specify any-
thing about the algorithms or “interesting”-ness metrics used in
computation. Instead, users simply describe their data sources—
speciﬁcally, the set of attributes and metrics that are of interest. As
in the DeepDive system [43], this allows users to think about their
data, not about the algorithms behind MacroBase. When starting
from a baseline of no automated support, even basic querying func-
tionality can provide valuable results. However, given its extensible
architecture (Section 3), MacroBase allows an iterative, “pay as you
go” development lifecycle.
Target applications and usage. MacroBase is designed to identify
behaviors that occupy a sweet spot between obvious catastrophes
(e.g., most requests are failing) that will be caught by standard pro-
cesses (e.g., customer assurance) and one-off events (e.g., a single
request is ill-formed). In our experience, the largest opportunities
in analytic monitoring for IoT occupy this middle ground, which
contains too much data for manual inspection and also contains
subtle behaviors that can be difﬁcult to detect manually. MacroBase
acts as an aid to human operators by providing them with a detailed
overview of a system’s operation that they can reﬁne over time.

To underscore this point, while MacroBase supports program-
matic input and output (e.g., alerting as above), MacroBase’s output
is designed to be human-readable. MacroBase’s response to IoT
data volumes is to intelligently prioritize human attention, produc-
ing a small set of summaries that can be skimmed in a matter of
minutes or even tens of seconds. MacroBase’s query results are
designed to be easily consumed and understood by a user who is
an expert in his/her application but not in statistical estimation or
summarization. While MacroBase supports programmatic output,
human oversight acts as a check against the statistical measures un-
derlying MacroBase’s operation. A human can inspect MacroBase’s
summaries as well as key data points and decide whether to take
action. This encourages iterative development and deployment.

3. SYSTEM ARCHITECTURE AND APIS

In this section, we describe MacroBase’s high-level architecture,

query processing pipeline, and interfaces for extensibility.

MacroBase operates as a standalone analytic monitoring engine.
It delegates data storage and updates to a set of storage systems,
including databases such as PostgreSQL, ﬁle systems such as HDFS,
and publish-subscribe queues such as Kafka. MacroBase performs
analytic monitoring in ﬁve stages, which we depict in Figure 1 and
describe in detail below.
1.) Ingestion.
The ﬁrst stage of MacroBase query processing
ingests data from external sources. MacroBase supports custom
connectors to a number of data sources. For example, MacroBase’s
JDBC interface allows users to specify columns of interest from
a base view deﬁned according to a SQL query. MacroBase subse-

Figure 1: MacroBase’s analytic monitoring pipeline: MacroBase ingests IoT data as a series of points, which are scored and classiﬁed
by an outlier detector operator, summarized by a data summarization operator, then ranked and presented to end-users.

Detection

Summarization

double score(p: Point)
void train(data: Iterator[Point])
void observeOutlier(p: Point, score: double)
void observeInlier(p: Point, score: double)
Iterable[(Attribute, Support)] report()

Table 1: MacroBase’s detection and summarization operator
interfaces used in both batch and streaming modes.

quently reads the result-set from the JDBC connector in order to
construct data points to process, with one point per row in the view.
In streaming mode, MacroBase continuously ingests data.
2.) Outlier Detection. Following ingestion, an outlier detection
operator processes each point. We present our operators in Section 4,
but, conceptually, this operator is responsible for both training and
evaluating an outlier detection model on the data.

The interface for both streaming and batch outlier detectors is
equivalent (Table 1). The ﬁrst method call, score, takes a data
point and returns a real-valued outlier score for the point. The
second method call, train is optional, and trains the model on an
unlabeled input set of data points. In batch mode, MacroBase calls
train once all data is loaded, to train the detector on all of the
points. In streaming mode, MacroBase calls train regularly, at a
pre-conﬁgured interval, on a sample of the incoming input stream.
From its input stream of data points, the detection operator pro-
duces two batches (or streams) of points—one for outlier points and
another for inlier points, along with anomaly scores for each.
3.) Summarization. Given inlier and outlier points, MacroBase
ﬁnds attributes that are common among outliers and uncommon
among inliers (Table 1). Thus, the output of the summarization
operator (defaults described in Section 5) is a set of predicates
that best “explain” outlier points, with basic statistics including the
number of outliers and inliers that match each predicate.

In batch mode, all inlier and outlier points arrive at the summa-
rization operator at once, and the summarization operator outputs
a single set of summaries. In streaming mode, inlier and outlier
points arrive tuple-at-a-time, and the summarization operator returns
a set of summaries on demand (either in response to a user request,
or in response to a periodic timer or other pre-conﬁgured refresh
event). Thus, in streaming mode, the summarization operator acts
as a streaming view maintainer whose output can be queried (and
fully materialized) at any time.
4.) Ranking. The number of predicates output by the summarizer
may be large. As a result, the output of the summarizer is ranked
before presentation. In most cases, the ranking is computed by ex-
amining the basic statistics output by the summarizer. For example,
by default, MacroBase sorts the summaries by their corresponding
degree of occurrence in the outliers.
5.) Presentation. Given the ranked list of summaries, MacroBase
delivers them to downstream consumers. The default presentation
mode is a report rendered via the MacroBase REST API or the Mac-
roBase GUI. In the former, programmatic consumers (e.g., reporting
tools such as PagerDuty) can automatically forward its output to

Figure 2: MacroBase’s default outlier detection (Section 4) and
summarization (Section 5) operators.

downstream monitoring and operations pipelines. The former can
also be used to generate static, human-generated reports. In the
GUI, users can interactively inspect the outlier summaries and itera-
tively deﬁne their MacroBase queries. In practice, we have found
that GUI-based exploration is an important ﬁrst step in formulating
standing and periodic MacroBase queries.
Overview and Discussion. MacroBase executes a dataﬂow DAG,
with conﬁgurable, extensible operators for detection, summarization,
and ranking. In the remainder of this paper, we study the efﬁcient
implementation of these operators, with an emphasis on detection
and summarization. However, there is operational value in an end-
to-end pipeline. Compared to working with an R or Python console,
the MacroBase pipeline obviates many ETL and data integration
challenges, and the ability to translate from data exploration to
standing queries (i.e., from development to data product deployment)
has been attractive to users. Finally, the relatively narrow MacroBase
interface has proven useful for extensibility: a master’s student at
MIT and a master’s student at Stanford each implemented and tested
a new outlier detector method in less than a week of part-time work.

4. DEFAULT OUTLIER DETECTION

MacroBase’s outlier detection operators monitor the input for
interesting points that deviate from “normal” behavior. Our goals in
the design of MacroBase’s default outlier detection operators were
three-fold. First, we sought a methodology that would minimize
end-user burden by working “out of the box,” without relying on
annotations. Second, we sought a methodology that would provide
users with meaningful results. We evaluated a set of candidate
methods on real-world datasets (including CMT) and quickly de-
termined a set of methods that identiﬁed qualitatively interesting
points. Third, we sought a methodology that would be efﬁcient to
execute, especially in a streaming model.

In this section, we describe the design and optimization of Mac-
roBase’s default outlier detection operator, which relies on robust
statistical estimators and distribution-distance based scoring.
4.1 Preliminary: Robust Statistics

The core of MacroBase’s default outlier detection operator is
powered by robust statistical estimation [27], a branch of statistics
that pertains to ﬁnding statistical distributions for data that is well-
behaved for the most part but may contain a number of ill-behaved
data points. The basic insight is that if we can ﬁnd a distribution that
reliably ﬁts most of the data, we can measure each point’s distance

4

1. INGESTETL & conversion to points; pairs of (metrics, attrs)2. DETECTIdentiﬁcation of inliers, outliers using metrics4. RANKPolicy-based prioritization of explanations 5. PRESENTExport and consumption: GUI, alerts, dashboards3. SUMMARIZEDiscovery of descriptive outlier attributesFDRFDRretraininginput samplescore sampleretrainingscoresMAD/MCDAMCModel:inlier summary data structuresoutlier summary data structuresoutlier summaryThresholdOUTLIER DETECTIONEXPLANATIONinlier + outlier streamsM-CPS-TreeM-CPS-Treeﬁlter using OI-ratioAMCfrom this distribution in order to ﬁnd outliers. While robust statistics
have a long history, they “remain largely unused and even unknown
by most communities of applied statisticians, data analysts, and
scientists that might beneﬁt from their use” [36].

To understand the importance of robust statistics, consider the Z-
Score of a point drawn from a univariate sample, which measures the
number of standard deviations that a point lies away from the sample
mean. This provides a normalized way to measure the “outlying”-
ness of a point (e.g., a Z-Score of three indicates the point lies three
standard deviations from the mean). However, as we experimentally
demonstrate in Section 6, the Z-Score is not robust to outliers: a
single outlying value can skew the mean and standard deviation by
an unbounded amount, limiting its usefulness.

A robust replacement for the Z-Score is to use the median and a
measure called the Median Absolute Deviation (MAD) as a measure
of distance in place of mean and standard deviation. The MAD
measures the median of the absolute distance from each point in the
sample to the sample median. Thus, each outlying data point has
limited impact on the MAD score of all other points in the sample
since the median itself is not signiﬁcantly skewed by outliers. In
addition to the MAD, MacroBase leverages a multivariate variant—
the MCD—which we describe in greater detail below.

From the perspective of our goal of usability, robust statistical
estimators are particularly attractive. By treating them as the basis of
a distance-based outlier detection methodology, we effectively adopt
an unsupervised model. Users do not manually label data points as
outliers; instead, the system performs scoring automatically.
4.2 Batch Detection

Given the set of input data points, each batch outlier detection
implementation computes a robust estimator for the sample distri-
bution. Given a query with a single, univariate metric, MacroBase
computes the median and MAD. If the MAD is zero (e.g., due to
a sample with many similar values), MacroBase falls back to com-
puting trimmed means (default: 5%), and, failing that, selects a
small ε as the MAD. Given a query with multiple (i.e., multivariate)
metrics, MacroBase computes sample covariance and mean using
the FastMCD [41] algorithm. In the remainder of this section, we
describe MacroBase’s implementation of the Minimum Covariance
Determinant (MCD) estimator.
MCD: Basics. The MCD summarizes a set of points according to
its location (absolute distance in metric space) and scatter (relative
distribution in metric space) and [28]. The MCD model representa-
tion captures scale via a sample mean µ and scatter via a covariance
matrix C. Given µ and C, we can compute the distance between the
MCD estimate of the distribution and a sample data point x via the

Mahalanobis distance:(cid:113)

(x− µ)TC−1(x− µ)

Intuitively, the Mahalanobis distance normalizes (or warps) the met-
ric space using the covariance matrix and then measures the distance
from the center of the transformed space using the mean.

Exactly computing the MCD requires examining all subsets of
points to ﬁnd the subset whose covariance matrix exhibits the min-
imum determinant. This is computationally intractable for even
modestly-sized datasets.
Instead, for the actual computation of
the MCD covariance and mean, MacroBase adopts an iterative ap-
proximation, called FastMCD [41]. In FastMCD, an initial subset
of points S0 is chosen from the input set of points P. FastMCD
computes the covariance C0 and mean µ0 of S0, then performs a
“C-step” by ﬁnding the set S1 of points in P that have the |S1| clos-
est Mahalanobis distances (to C0 and µ0. FastMCD subsequently
repeats C-steps (i.e., computes the covariance C1 and mean µ1 of

5

S1, selects a new subset S2 of points in P, and repeats) until the
change in the determinant of the sample covariance converges (i.e.,
det(Si−1)− det(Si) < ε, for small ε).
MCD: Experiences and Optimizations. The FastMCD literature
suggests |S0| = p + 1 but such a small choice of p frequently results
in singular covariance matrices without substantial impact on run-
time efﬁciency. Instead, MacroBase selects |S0| = |Si| = h|P|,∀i,
where h is a conﬁgurable parameter (by default, 0.5).

MacroBase caches the inverse covariance matrix reducing the
complexity of scoring from O(p3) to O(p2). Implementing a closed-
form (i.e., non-Matrix-based) calculation was faster than generic
matrix multiplication libraries for small dimensions. If many data
points have similar metrics, the covariance matrix may become
non-invertible (e.g., multiple rows in the covariance matrix might
be identical), meaning that we cannot directly compute the Ma-
halanobis distance. “Whitening” or dimensionality reduction can
combat singularities by removing correlations. However, depend-
ing on the FastMCD sample and, in a streaming setting, the set of
correlated columns may change. This problem is exacerbated in
the case of discretized metrics (e.g., the number of fatalities in an
accident). As a result, given a singular covariance matrix, Mac-
roBase instead computes the Moore-Penrose pseudoinverse of the
covariance matrix [7], which preserves many of the properties of
the inverse covariance matrix yet is guaranteed to exist.

Computing Outliers. The partitioning policy between inlier and
outlier data is conﬁgurable. MacroBase currently supports three op-
tions. First, MacroBase can use a percentile: any point with a score
in the top, conﬁgurable percentage of scores (by default, 1%), is
marked as an outlier. Second, MacroBase can use a constant-valued
threshold: any point with an anomaly score above a conﬁgurable
threshold will be marked as an outlier. Third, MacroBase can use
a model-speciﬁc Z-Score-equivalent threshold. For example, if we
were to set a Z-Score threshold of 3, under normally distributed
data, 0.27% of points would be ﬂagged as outliers. To use this
third option, users must implement a Z-Score conversion function
that converts the detector scores into a consistent estimator for the
estimation of the standard deviation; when this is impossible (e.g.,
nearest-neighbor algorithms), this third method is not supported.
4.3 A Note on Distributions

Many robust estimators—including MAD and MCD—presume
that data is drawn from a given distribution. For example, MAD
assumes that data is drawn from a single distribution. Therefore,
if we run MAD on data drawn from two separate distributions
(e.g., one centered at 0 and one centered at 1000), the output of
our modiﬁed Z-Score may not be meaningful. Similarly, the MCD
assumes data is drawn from a single, ellipsoidal distribution. Recent
proposals for robust non-parametric estimators, such as robust kernel
density estimation [32], as well as classic methods for reducing
dimensionality, can help alleviate these problems.

However, even with these limitations, we have found that look-
ing for extreme anomaly scores using MAD and MCD estimators
yields useful results. As Aggarwal describes, “even for arbitrary
distributions, [extreme value analysis such as Z-Scores] provide a
good heuristic idea of the outlier scores of data points, even when
they cannot be interpreted statistically” [4]. That is, on our real-
world datasets—which do not behave according to perfectly normal
distributions—these techniques still yield useful results. This is sim-
ilar in spirit to applying convex optimization routines to non-convex
problems: a convex optimization routine is not guaranteed to ﬁnd an
optimal solution on a non-convex problem (e.g., deep learning [17]),
but, as we report, they perform well on real data.

Algorithm 1 FDR: Flexible Damped Reservoir Sampler

given: max size ∈ N; r: decay rate ∈ (0,1)
initialization: reservoir R ← {}; current weight cw ← 0
function OBSERVE(x: point, w: weight)

else with probability 1
cw

remove random element from R and add x to R

cw ← cw + w
if |R| < max size then

R ← R∪{x}

function DECAY( )

cw ← r· cw

4.4 Streaming

Record-at-a-time evaluation in not discussed in the robust statis-
tics literature; as a result, it is unclear how to immediately adapt the
above models to a streaming context.

Conceptually, MacroBase treats streaming outlier detection as
an online model evaluation and retraining problem. MacroBase
continuously ﬁts distributions to the data it has seen and scores each
point as it arrives. However, there are two attendant problems. First,
how should the system update its models without consulting all
points previously seen? MAD requires a median of distances to
existing points, and MCD is an inherently combinatorial iterative
process. Second, how should the system determine whether a score
is anomalous? Given a percentile-based threshold, the system needs
to update its threshold continuously.
FDR: Flexible Damped Reservoir. The workhorse of our solution
to the above problems is a reservoir sampling mechanism, adapted
to an exponentially damped arbitrary window; this is the ﬁrst known
application of this technique in the literature.

The classic reservoir sampling technique can be used to accumu-
late a uniform sample over a set of data using ﬁnite space and one
pass [45]. The probability of insertion into the sample “reservoir”
is inversely proportional to the number of points observed thus far.
In the context of stream sampling, we can treat the stream as an
inﬁnitely long set of points and the contents of the reservoir as a
uniform sample over the data observed so far.

Chao describes an adaptation of reservoir sampling to weighted
sampling over data streams [12]. As Efraimidis describes [19], this
result was recently re-discovered in the database community for
the speciﬁc case of exponentially decaying reservoir samples, with
decay occurring at every record insertion [3].

MacroBase adapts Chao’s algorithm to provide a novel, exponen-
tially damped streaming reservoir sample that works over arbitrary
windows (Algorithm 1). This allows both time-based and tuple-
based window policies (compared to previous, single-tuple-at-a-
time solutions [3]). We call the resulting data structure the Flexible
Damped Reservoir (FDR). FDR separates the tuple insertion pro-
cess from the decay and maintenance processes. FDR maintains a
running count of the items cw inserted into the reservoir so far. Each
time a new item is observed, the running count is incremented by
one (or an arbitrary weight, if desired). With probability 1
, the ob-
cw
served item is inserted into the reservoir and an existing, randomly
chosen item is evicted. Whenever the reservoir is decayed (e.g., via
a periodic timer or tuple count), the running count is multiplied by a
decay factor (i.e., cw := (1− α)cw).

The resulting algorithm is simple but powerful. As Efraimidis
writes [19], Chao’s results “should become more known to the
databases and algorithms communities.” Although our adaptation
is modest, FDR is the ﬁrst use of an exponentially biased weighted
sample over arbitrary windows and is one of the ﬁrst practical uses
of Chao’s general approach we have encountered in the literature.

Given: mininum support s and minimum OI-ratio r

Algorithm 2 MacroBase’s Basic Summarization
1: ﬁnd attributes w/ support ≥ s in O and OI-ratio ≥ r in O,I
2: build and mine FP-tree over O using only attributes from (1)
3: ﬁlter (2) by removing patterns w/ OI-ratio < r in I; return

Maintaining inputs for training. MacroBase uses an FDR to
address the above model retraining problem. It periodically retrains
models using an FDR that samples the input of the outlier detector
operator; upon request (either on a tuple-based periodic basis, or
using a real-time timer) the contents of the input FDR are used to
retrain the model. This streaming MCD model maintenance and
evaluation strategy is the ﬁrst of which we are aware.
Maintaining percentile thresholds. MacroBase also uses an FDR
to address the problem of threshold maintenance. Streaming quantile
estimation is well studied. However, in an exponentially damped
model with arbitrary window sizes, we were not able to ﬁnd many
computationally easy and conceptually simple alternatives. Thus,
instead, MacroBase uses an FDR to sample the output of the outlier
detector operator. The FDR maintains an exponentially damped
sample of the detector scores used to compute quantiles (e.g., the
99th percentile of recent scores). Periodically, the percentile is re-
recorded. A sample of size O( 1
δ )) yields an ε-approximation
of an arbitrary quantile with probability 1− δ [9], so a FDR of size
20K provides an ε = 1%, δ = 1% approximation.

ε2 log( 1

5. DEFAULT OUTLIER SUMMARIZATION
Once data points are classiﬁed as outliers and inliers according
to their metrics, MacroBase summarizes them according to their
attributes. In this section, we discuss how MacroBase summarizes
outlier data. We again begin with a discussion of batch operation
then discuss how MacroBase executes streaming queries.

5.1 Semantics: Support and OI-Ratio

MacroBase’s outlier summarization ﬁnds attribute values that
are common to the outliers but relatively uncommon to the inliers.
For categorical attributes, this corresponds to two complementary
goals. First, in the terminology of frequent itemset mining, we
wish to ﬁnd combinations of attribute values that have high support,
or occurrence (by count) in the outliers. Second, we wish to ﬁnd
combinations of attribute values that are are relatively uncommon
in the inliers; we deﬁne a combination of attributes’ ratio of support
in outliers to support in inliers as its outlier-inlier (OI-)ratio. As
an example, we may ﬁnd that 50 of 89 records ﬂagged as outliers
correspond to iPhone 6 devices (outlier support of 56.2%), but, if
8019 of 9092 records ﬂagged as inliers also correspond to iPhone
6 devices (outlier support of 88.2%), we are likely uninterested in
iPhone 6 as it has a low OI-ratio of 0.634. As noted in Section 2.2,
the minimum outlier support and OI-ratio are user-conﬁgurable.

5.2 Batch Summarization

Support and OI-ratio correspond to two classic problems in data
mining: respectively, frequent itemset mining and emerging pattern
mining. A na¨ıve solution is to run frequent itemset mining twice,
once on all of the inlier points and another time on all of the outlier
points, and emerging pattern mining once, looking for differences
between the inlier and outlier sets. While this is sufﬁcient, it is inef-
ﬁcient; as we discuss below, MacroBase exploits optimizations that
exploit both the cardinality imbalance between inliers and outliers
as well as the joint summarization of each set.

6

Optimization: Compute Item Ratios First. While computing
OI-ratios for all attribute combinations is expensive, computing OI-
ratios for single attributes is inexpensive: we can compute support
counts over both inliers and outliers using a single pass over the
data. Accordingly, MacroBase ﬁrst computes OI-ratios for single
attribute values, then computes attribute combinations from the
attribute values with sufﬁcient OI-ratios.
Optimization: Summarize Outliers First. The cardinality of the
outlier input stream is by deﬁnition much smaller than that of the
input stream. Therefore, instead of mining the outlier supports and
the inlier supports separately, MacroBase ﬁrst ﬁnds outlier attribute
value sets with minimum support and subsequently mines the inlier
attributes, while only searching for attributes that were supported in
the outliers. This reduces the number of inlier attributes to explore.
End result.
The result is a three-stage process (Algorithm 2):
calculate OI-ratio between inlier attribute values and outlier attribute
values (support counting, followed by a ﬁltering pass based on
OI-ratio), compute supported outlier attribute combinations, then
compute the OI-ratio for each attribute combination based on their
support in the inliers (support counting, followed by a ﬁltering pass
to exclude any attribute sets with insufﬁcient OI-ratio). A number
of itemset mining algorithms can perform this process; we describe
MacroBase’s choice of algorithms and data structures below.
Algorithms and Data Structures.
In the batch setting, single
attribute value counting is straightforward, requiring a single pass
over the data; the streaming setting below is more interesting. We
experimented with several alternative itemset mining techniques
and ultimately decided on preﬁx-tree-based approaches inspired by
FPGrowth. In brief, the FPGrowth algorithm maintains a frequency-
descending preﬁx tree of attributes that can subsequently be mined
by recursively generating a set of “conditional” trees. The FP-
Growth algorithm was fast (corroborating recent benchmarks [21])
and proved extensible in our streaming implementation below.

5.3 Streaming

As in our detection methodology, streaming summarization min-

ing proved more challenging.
Overall Strategy. We sought a tree-based technique that would
admit exponentially damped arbitrary windows but obviate the re-
quirement that each attribute be stored in the tree, as in recent
proposals such as the CPS-tree [44]. As a result, MacroBase adapts
a combination of two data structures: a probabilistic heavy-hitter
sketch for the frequent items, and a novel adaptation of the CPS-
Tree data structure to store frequent attributes. The result is an
approximation of the frequent attribute sets.

MacroBase’s default streaming summarization operator consists
of two main parts: data structure maintenance and querying. When
a new data point arrives at the MacroBase summarization opera-
tor, MacroBase inserts each of the point’s corresponding attributes
into an approximate counting data structure. Subsequently, Mac-
roBase inserts a subset of the point’s attributes into a preﬁx tree
that maintains an approximate, frequency descending order. When
a window has elapsed, MacroBase decays the counts of the items
as well as the counts in each node of the preﬁx tree. MacroBase
remove any items that are no longer supported and rearranges the
preﬁx tree in frequency-descending order. To produce summaries,
MacroBase runs the FPGrowth algorithm on the streaming preﬁx
tree. We provide additional details below:
Implementation: Frequent Item Counts. The problem of main-
taining streaming heavy hitters (attributes with top k occurrence) is
well-studied [14]. Initially, we implemented our item counter using

7

Algorithm 3 AMC: Amortized Maintenance Counter Summary

given: ε ∈ (0,1); r: decay rate ∈ (0,1)
initialization: (item → count) C ← {}; weight wi ← 0
function OBSERVE(i: item, c: count)
C[i] ← wi + c if i /∈ C else C[i] + c

function MAINTAIN( )
remove all but the 1
wi ← the largest value just removed, or, if none removed, 0

ε largest entries from C

function DECAY( )

decay the value of all entries of C by r
call MAINTAIN( )

the SpaceSaving algorithm [37], which has been demonstrated to
provide excellent performance for frequent item counting [15], with
extensions to the exponentially decayed setting [16]. However, for
fractional counts and large k, we found SpaceSaving to have higher
runtime overhead (O(n) for list-based counters, O(nlog(k)) for a
heap-based implementation; see Section 6).

Instead, we opted for a less memory-efﬁcient but often faster struc-
ture, which we call the Amortized Maintenance Counter (AMC). As
input, AMC takes a maximum stable size parameter, which dictates
the number of items that will be stored across periodic maintenance
boundaries (i.e., within windows); as in SpaceSaving, a maximum
size of 1

ε yields an nε approximation of the count of n points.

AMC keeps a set of approximate counts for all items that have
been heavy-hitters in the previous window while also storing ap-
proximate counts for all items that have been observed in the current
window. During maintenance, AMC prunes all but the 1
ε items with
highest counts and records the maximum count that is discarded (wi).
Upon insertion, AMC checks to see if the item is already stored. If
so, the item’s count is incremented. If not, AMC stores the item
count plus wi. If an item is not stored in the current window, the
item must have had count less than or equal to the wi.

ε )) time.

AMC has three major differences compared to SpaceSaving. First,
AMC updates are constant time (hash table insertion) compared
to O(log( 1
ε )) for SpaceSaving. Second, AMC has an additional
maintenance step, which is amortized across all items seen in a
window. Using a min-heap, with I items in the sketch, maintenance
requires O(I · log( 1
If we observe even one item more
than once, this is faster than performing maintenance on every
observation. Third, AMC has higher space overhead; in the limit, it
must maintain all items it has seen between maintenance windows.
Implementation: Streaming Summarization. Given the set of
recently frequent items, MacroBase monitors the attribute stream for
frequent attribute combinations by maintaining an approximately
frequency-descending preﬁx tree of attribute values. The tree is
maintained such that it can be queried at any time to produce sum-
maries, but the bulk of the implementation difﬁculty is in maintain-
ing it. MacroBase adopts the basic CPS-tree data structure [44],
with several modiﬁcations, which we call the M-CPS-tree.

Like the CPS-tree, the M-CPS-tree maintains both the basic FP-
tree data structures as well as a set of leaf nodes in the tree. However,
in an exponentially damped model, the CPS-tree stores at least one
node for every item ever observed in the stream. This is infeasible
at scale. As a compromise, the M-CPS-tree only stores items that
were frequent in the previous window: at each window boundary,
MacroBase updates the frequent item counts in the M-CPS-tree
based on an AMC sketch. Any items that were frequent in the
previous window but were not frequent in this window are removed
from the tree. MacroBase subsequently decays all frequency counts
in the M-CPS-tree nodes and re-sorts the M-CPS-tree in frequency
descending order (as in the CPS-tree, by traversing each path from

leaf to root and re-inserting as needed). Subsequently, attribute
insertion can continue as in the FP-tree, with the caveat that the item
order is not maintained until mining.

6. EXPERIENCES AND EVALUATION

In this section, we evaluate the statistical and empirical perfor-
mance of the MacroBase architecture. We begin with a report on
our experiences at CMT, where MacroBase has successfully discov-
ered several previously unknown behaviors in the production CMT
application. We also qualitatively describe results of applying Mac-
roBase to a range of real-world datasets. Subsequently, we analyze
its end-to-end engine performance and demonstrate the effect of
optimizations in detection and summarization modules, yielding per-
formance improvements of up to three orders of magnitude as well
as promising progress in parallelization. In summary, in addition to
processing a range of analytic monitoring queries at speeds of up
to 1M events per second per core, MacroBase provides meaningful
data summaries we have already leveraged in production.
6.1 Experimental Setup

Our current MacroBase implementation comprises approximately

7,000 lines of code and is available online as open source.
Datasets. To compare the efﬁciency of MacroBase and related
techniques, we compiled a set of production datasets (Table 2)
for evaluation: CMT, a collection of user drives at CMT, including
anonymized metadata such as phone model, drive length, and bat-
tery drain; Telecom, a collection of aggregate internet, SMS, and
telephone activity for a Milanese telecom; Accidents, a collection
of United Kingdom road accidents between 2012 and 2014, in-
cluding road conditions, accident severity, and number of fatalities;
Campaign contains a collection of 1.05M records corresponding to
US Presidential campaign expenditures in election years between
2008 and 2016, including contributor name, occupation, and amount;
and Disburse, a collection of US House and Senate candidate dis-
bursements in election years from 2010 through 2016, including
candidate name, amount, and recipient name.

For each dataset X, we execute two MacroBase queries: a simple
query, with a single attribute and single metric (denoted XS), and a
complex query, with a larger set of attributes and, when available,
multiple metrics (denoted XC). We provide a high-level description
of each query in Table 2.
Deployment. We deploy our MacroBase prototype on a server with
four Intel Xeon E7-4830 2.13 GHz CPUs containing 8 cores per
CPU (32 total) and 264 GB of RAM. To isolate the effects of data
processing, we exclude loading time from our results. We report our
default conﬁguration in Table 2.
6.2 End-to-End Experience

We begin by reporting on our experiences running MacroBase

end-to-end on our example queries.
Query results. To begin, MacroBase found behaviors at CMT
(and in the CMT dataset) that were previously unknown. In one
case, MacroBase’s simple query on CMT (MS) highlighted a small
number of users who experienced issues with their trip detection.
In another case, MacroBase’s complex query (MC) discovered a
rare issue with the CMT application and a device-speciﬁc battery
problem. Consultation and investigation with the CMT team con-
ﬁrmed these issues. This experience proved a useful demonstration
of MacroBase’s analytical monitoring functionality in a production
environment and inspired several ongoing extensions (Section 7).

In the remaining datasets, MacroBase highlighted several inter-
esting outliers. For example: for Accidents, AS reported that 30%

of UK accidents with high casualty counts occurred in conditions of
rain with no wind, while AC reported severe accidents were more
than 4.8 times as likely to occur given a speed limit of 70 kilometers
per hour; for Campaign, ES reported disproportionately high con-
tributions to Rudy Giuliani and Mitt Romney, while EC reported
high contributions from entrepreneurs and homemakers, employees
of Morgan Stanley, and zip codes 10128 (New York City’s Upper
East Side) and 60614 (Chicago’s Lincoln Park neighborhood); for
Disburse, FS reported a number of candidates with high disburse-
ments, including Michelle Bachmann, whose disbursements were
over 4.9 times more likely to be considered high, while FC reported
high disbursements to zip codes 20003 (Capitol Hill in Washing-
ton, DC) and 20007 (Georgetown in Washington, DC) as well as a
number of media and strategic communications ﬁrms that had re-
ceived large payments; ﬁnally, for Telecom, TS and TC highlighted
a number of regions with extreme call volumes.

While our primary production experience has been at CMT, these
additional query results qualitatively conﬁrm the value of Mac-
roBase’s analytic monitoring capabilities.
Performance. Batch and exponentially weighted streaming have
different semantics, as is reﬂected in the summaries they produce.
While batch execution examines the entire dataset at once, expo-
nentially weighted streaming prioritizes later points. Therefore, for
datasets with few distinct attribute values (e.g., Accidents, which
contains only nine types of weather conditions), the summaries
have high similarity. However, for datasets with many distinct at-
tribute values (typically the complex queries, which have hundreds
of thousands of possible combinations—e.g., Disburse has 138,338
different disbursement recipients), the summaries tend to differ. In
CMT, AC returns more summaries in streaming than in batch execu-
tion (on account of a larger number of supported attributes at the end
of the pass through all 10M tuples), while, FC delivers the opposite
result (on account of the many disbursement recipients above). In
practice, users can tune their streaming decay on a per-application
basis (e.g., at CMT, streaming queries may prioritize trips from the
last hour to catch errors arising from the most recent deployment).
The throughput of MacroBase varied from 83K points per second
(on MC) to 1.1M points per second (on ES); the average throughput
for batch was 510K points per second, and the average throughput
for exponentially weighted sampling was 524K points per second.
The better-performing mode depended heavily on the particular
data set and characteristics. In general, queries with multiple met-
rics were slower in batch than queries with single metrics (due to
increased training time, whereas streaming trains over a sample).
Mining each summary incurred an approximately 20% overhead. In
all cases, these queries far exceed the current arrival rate of data for
each dataset; however, we have already identiﬁed several customers
with throughput demands in this regime (Section 7).
Runtime analysis. To understand the overhead of each pipeline
operator, we proﬁled MacroBase’s batch execution. (Streaming
is much more challenging to accurately instrument on account of
its tuple-at-a-time execution.) On MC, MacroBase spent approxi-
mately 52% of its execution training MCD, 21% scoring points, and
26% summarizing them. On MS, MacroBase spent approximately
54% of its execution training MAD, 16% scoring points, and 29%
summarizing them. In contrast, on FC, which returned over 1000
summaries, MacroBase spent 31% of its execution training MAD,
4% scoring points, and 65% summarizing them. Thus, the overhead
of each component is dependent on the query. We investigate the
behavior of these operators in the remainder of this section.

6.3 Microbenchmarks and Comparison

8

Name
MS
MC
AS
AC
ES
EC
FS
FC
TS
TC

Dataset
CMT

Accidents

Campaign

Disburse

Telecom

Queries

Metrics

1
7
1
3
1
1
1
1
1
5

Attrs

1
6
1
3
1
5
1
6
1
2

Points
10M

430K

10M

3.48M

10M

Thru (-Mine)
Batch
431.7K
117.8K
1176.1K
323.3K
684.0K
769.4K
380.2K
393.9K
670.4K
151.5K

EWS
527.8K
121.3K
858.2K
336.2K
988.4K
134.5K
940.7K
358.9K
727.6K
242.7K

Thru (+Mine)
Batch
314.4K
83.2K
798.9K
194.3K
527.9K
141.8K
318.3K
102.1K
509.7K
108.8K

EWS
519.6K
120.6K
854.9K
333.0K
988.2K
126.7K
935.1K
319.2K
720.7K
242.3K

# Summaries
EWS
Batch
46
49
1294
267
2
2
23
25
2
4
25
48
45
40
230
1692
539
545
589
108

Jaccard
Similarity

0.96
0.07
1.00
0.96
0.50
0.18
0.84
0.08
0.99
0.14

Default Parameters

Minimum support
Minimum OI-Ratio
Outlier Percentile
FDR size
AMC size
Decay rate
Retrain rate (tuples)
Window size (tuples)
MCD stopping ε

0.1%
3
1%
10K
10K
0.01
100K
100K
0.001

Table 2: Datasets and query names, baseline performance and summaries produced under batch and exponentially weighted stream-
ing (EWS) execution, and parameters used in experimental evaluation.

Figure 3: Discriminative power of estimators under contamina-
tion by outliers. Robust methods (MCD, MAD) outperform.

In this section, we explore several aspects of MacroBase’s design,
including its use of robust estimators, focus on outlying data, the
effect of support and OI-ratio, the impact of its summarization
optimizations compared to related techniques in both batch and
streaming, the effect of sampling, and the use of AMC sketches.
Use of robust methods. To examine MacroBase’s ability to dis-
cern outliers, we conﬁgured a synthetic dataset of 10M points with
a combination of two distributions: a uniform inlier distribution Di,
with radius 50 centered at the origin, and a uniform outlier distri-
bution Do, with radius 50 centered at (1000, 1000). We varied the
proportion of points in Di and Do to evaluate the effect of outlier
contamination on the Z-Score, MAD, and MCD (using univariate
points for Z-Score and MAD). Figure 3 demonstrates that the MAD
and MCD are able to reliably identify points in Do despite increasing
contamination. The MAD’s outlier scores were linearly affected by
contamination up to 50% (as the median deviation from the median
decreased). The MCD was similarly fairly stable until 40%, at which
point it effectively collapsed. In contrast, the Z-Score lost its ability
to distinguish inliers and outliers under even modest contamination
as the method is not robust to outlying data points. This illustrates
the effectiveness of robust methods despite contamination.
Distribution of scores. While the above demonstrates the utility
of robust statistics on synthetic data, we also wanted to investigate
their efﬁcacy on our real datasets. We empirically evaluate the
distribution of outlier points and the fact that robust statistics can
highlight a small number of points as outliers by plotting the CDF of
scores in each of our real-world dataset queries in Figure 4. While
many points have high anomaly scores, the tail of the distribution
(at the 99th percentile) is extreme: a very small proportion of points
have outlier scores over over 150. Thus, by focusing on this small
upper percentile, MacroBase highlights the most extreme behaviors.
Support and OI-Ratio. To understand the effect of support and
OI-ratio on summarization efﬁciency, we varied each and measured

Figure 4: CDF of scores for all datasets.

Figure 5: Number of summaries produced and summarization
time under varying support and OI-Ratio.

the number of summaries produced as well as the runtime required
for each on the EC and MC datasets, which we plot in Figure 5.
Each dataset has few attributes with outlier support greater than
10%, but each had over 1700 with support greater than 0.001%.
Past 0.01%, support had limited impact on runtime; most time in
summarization is spent in simply iterating over the inliers rather
than maintaining tree structures. This effect is further visible when
varying the OI-Ratio, which has less than 40% impact on runtime
yet leads to an order of magnitude change in number of summaries.
We ﬁnd that our default setting of support and OI-Ratio yields a
reasonable trade-off between number of summaries and runtime.
Batch summarization. MacroBase leverages a unique pruning
strategy that exploits the low cardinality of outliers. We evaluated
the efﬁciency of MacroBase’s OI-pruning compared to traditional
FP-Growth. In addition, on account of a large number of recent
proposals for data summarization techniques (Section 8), we imple-
mented several additional methods. Unfortunately, the results of

9

0.00.10.20.30.40.5Proportion Outliers0102030405060Mean Score (Outliers)MCDMADZ-Score10-210-1100101102103Anomaly Score0.00.20.40.60.81.0CDF102103104105Anomaly Score0.9900.9920.9940.9960.9981.000CDF10-410-310-210-1100Minimum Support100101102103104# SummariesECMC10-210-1100101Minimum OI-Ratio100101102103104# Summaries10-410-310-210-1100Minimum Support050100150200250300350Time (s)10-210-1100101Minimum OI-Ratio01020304050Time (s)Query MB
0.76
MS
1.94
MC
AS
0.37
0.33
AC
0.62
ES
2.14
EC
0.86
FS
2.89
FC
0.83
TS
1.23
TC

FP
1.96
5.11
0.54
0.91
1.11
3.10
3.18
6.73
8.01
13.21

Cube
1.01
DNF
0.39
1.32
0.61
29.25
0.88
132.18
0.75
22.80

DT10
7.70
27.33
0.91
2.32
5.19
30.80
9.32
31.22
9.69
13.65

DT100
74.97
274.73
0.96
2.36
6.21
DNF
88.09
268.56
95.06
96.17

AP
0.79
DNF
0.30
15.10
0.59
79.36
0.82
133.11
0.68
161.54

XR
DNF
DNF
0.73
11.99
3.37
DNF
DNF
DNF
DNF
DNF

Table 3: Running time of batch summarization algorithms (sec-
onds) for maximum 1M tuples of each query. MB: MacroBase,
FP: FP-Growth, Cube: Data cubing; DTX: decision tree, max-
imum depth X; AP: A-Aprioi; XR: Data X-Ray. DNF: did not
complete in 10 minutes.

these methods are not comparable, and prior work has not evaluated
these techniques with respect to one another in terms of semantics or
performance. As a result, while we do not attempt a full comparison
based on semantics, we do perform a comparison based on running
time, which we depict in Table 3.

MacroBase’s produces a summary of each datasets’s inliers and
outliers in 0.37–2.89 seconds.
In contrast, running FP-Growth
separately on inliers and outliers is, on average, 3.7 times slower;
compared to MacroBase’s joint summarization according to support
and OI-ratio, much of the time spent mining inliers (with insufﬁcient
OI-ratio) in FP-Growth is wasted. However, both MacroBase and
FP-Growth must perform a linear pass over all of the inliers, which
places a lower bound on the running time.

In contrast, training a decision tree based on inliers and outliers
(maximum depth: 10) as recently suggested by Scorpion [49] yields
an average summarization time approximately 11.5x slower than
MacroBase. This is due to the large number of passes that must
be made over all of the data, and, generally, scales poorly with the
tree depth: training decision trees of maximum depth 100 takes on
average (among the runs that completed) 84 times longer. We also
compared to a data cubing strategy suggested by Roy and Suciu [42],
which generates counts for all possible combinations (21x slower),
Apriori itemset mining [22] (over 43x slower), and Data X-Ray [46].
Cubing works better for data with fewer attributes, while Data X-
Ray is optimized for hierarchical data; we have veriﬁed with the
authors of Data-XRay that, for MacroBase’s effectively relational
points, Data X-Ray will consider all combinations of categorical
attributes unless stopping criteria are met.

In summary, this comparative performance analysis validates our
use of a cardinality-aware summarization strategy. Moreover, while
our comparison above is based on batch execution, MacroBase’s
ability to execute in a streaming manner is also a beneﬁt.
Streaming Data Structures. We also investigated the behavior of
the M-CPS-tree compared to the generic CPS-tree. The two data
structures have different behaviors and semantics: the M-CPS-tree
captures only itemsets that are frequent for at least two windows
by leveraging an AMC sketch. In contrast, CPS-tree captures all
frequent combinations of attributes but must insert each point’s
attributes into the tree (whether supported or not) and, in the limit,
stores (and re-sorts) all items ever observed in the stream. As a
result, across all queries except ES and EC, the CPS-tree was on
average 130x slower than the M-CPS-tree (std.: 213x); on ES and
EC, the CPS-tree was over 1000x slower. The exact speedup was
inﬂuenced by the number of distinct attribute values in the dataset:
Accidents had few values, incurring 1.3x and 1.7x slowdowns,
while Campaign had many.
Training on samples. We evaluated the accuracy and efﬁciency

Figure 6: Behavior of MAD (MS) and MCD (MC) on samples.

(a) Decay: 10K tuples

(b) Decay: 100K tuples

Figure 7: Streaming heavy hitters sketch comparison. AMC:
Amortized Maintenance Counter; SSL: Space Saving List;
SSH: Space Saving Hash. All share the same accuracy bound.

of training models on samples. In Figure 6, we plot the outlier
classiﬁcation F-Score versus sample size for the CMT queries. MAD
precision and recall are unaffected by sampling, allowing a four
order-of-magnitude speedup without loss in accuracy. In contrast,
MCD accuracy is more sensitive due to variance in the sample
selection: for example, training on a sample of 10K points out of
10M yields a speedup of over ﬁve orders of magnitude but results
in an average F-Score of 0.48 (std: 0.32). The loss in precision
comes from variance in runs: a small number of MCD estimates
have poor F-Scores. This high variance in MCD is offset by the
fact that models are retrained regularly under streaming execution,
compared to batch execution, which only trains models once.
Item summarization. Finally, we compare the performance of
AMC with existing heavy-hitters sketches. AMC outperforms both
implementations of SpaceSaving by a margin of up to 50x in all
conﬁgurations except for SpaceSaving implemented using a list
with 100 items. This is because the overhead of heap maintenance
on every operation is expensive with even modestly-sized sketches,
while the list traversal is costly for decayed, non-integer counts. In
contrast, with an update period of 100K tuples, AMC sustains in
excess of 4M updates per second.
Preliminary scale-out. As a preliminary assessment of Mac-
roBase’s potential for scale-out, we examined its behavior under a
na¨ıve, shared-nothing parallel execution strategy. Each core was
assigned an equally-sized data partition, which it processed indepen-
dently; upon completion, we return the combined set of summaries.
As Figure 8 shows, this strategy delivers excellent linear scalability.
However, as each core effectively trains and summarizes a sample
of the overall dataset, accuracy suffers due to both model drift (as in
Figure 6) and lack of cross-partition cooperation in summarization.
For example, with 32 partitions spanning 32 cores, FS achieves
throughput nearing 29M points per second, with perfect recall but
only 12% accuracy. Improving accuracy while maintaining scalabil-
ity is the subject of ongoing work (Section 7).

7. DISCUSSION AND ONGOING WORK

10

101102103104105106Sample Size10-410-310-210-1100101102103Training Time (s)101102103104105106Sample Size0.00.20.40.60.81.0Outlier F-ScoreMSMCAMCSSLSSH102103104105106Sketch Size02M4M6M8M10MUpdates per Second102103104105106Sketch Size02M4M6M8M10MUpdates per Secondtechniques have seen major success in several vertically-oriented
domains including network intrusion detection (frequently based
on “ﬁngerprint” and rules-based methods) [20, 38], various kinds of
fraud detection (leveraging a variety of classiﬁers and techniques) [8,
39], and industrial automation and predictive maintenance [5, 35].
The array of techniques in the literature motivates MacroBase’s
modular architecture, while the successes these particular domains
illustrate the power of a well-tuned anomaly detection system.
Robust Statistics. In this paper, we described MacroBase’s adap-
tation of robust statistical estimators in unsupervised anomaly de-
tection. These estimators are well-studied [25, 27, 36, 41], if seldom
employed at scale. MacroBase’s application of these techniques is
one of the largest-scale we have encountered in the literature, and its
streaming execution and training, is, to the best of our knowledge,
novel. While we are rapidly expanding MacroBase’s set of detec-
tors, we expect these robust methods to remain a core staple of the
arsenal, due both to their ease of use and computational efﬁciency.
IoT and Sensor Networking.
IoT is the latest manifestation
of long-prophesied visions of widespread sensing and actuation,
including sensor networking [6], smart dust [47], and ubiquitious
computing [48]. Today, IoT has seen massive industrial interest
and growth in real-world deployments including CMT [34]. Our
design of MacroBase is informed by the unconventional demands
placed on data management systems in IoT deployments as well as
the inherently statistical nature of many of these applications. In
contrast with deterministic analyses provided by systems such as
TAG [33], MacroBase executes statistical models. Related work
has examined modeling challenges in related domains (e.g., in data
acquisition in sensor networks [18] and in-network sensor network
anomaly detection [40, 50]). In MacroBase, we seek to draw upon
these prior successes and are motivated by the practical observation
that there is no existing systems architecture for executing analytic
monitoring queries over streaming IoT data at scale.
Data Summarization. Like anomaly detection, data summariza-
tion is a popular topic spanning several communities. In MacroBase,
we leverage techniques from the sketching and streaming litera-
ture [14], speciﬁcally in our AMC sketch [16, 37] and FDR sam-
pler [12,19]. For heavy hitters, we were surprised to ﬁnd many more
algorithms concerned with space overheads instead of update time.
In MacroBase, we desired low (or constant) update time with poten-
tially large but bounded space requirements and a possibly expensive
periodic, amortized maintenance step. For summarizing categorical
data, we adapted a pattern-based approach [22], combining both sup-
port and novel OI-ratio critera. While there are many algorithms for
various models for streaming pattern mining [13], we found that the
CPS-Tree [44] (which generalizes the FP-Growth [23] preﬁx tree)
was a close ﬁt for our needs. Our combination of heavy hitters and
a modiﬁed, exponentially-decayed CPS-tree provides approximate
summaries without materializing the entire stream contents.

The database literature contains several recent proposals for non-
streaming data summarization. Scorpion [49] ﬁnds sets of predicates
that best “explain” why a set of points is anomalous, using a com-
bination of decision trees and Apriori-like [22] pruning. Roy and
Suciu [42] propose the use of data cubes for summarization, with
special extensions for data with functional dependencies. Data
X-Ray [46] uses a Bayesian-inspired metric to ﬁnd interesting sub-
groups within hierarchical datasets. These techniques—and related
data cleaning techniques [24,31]—are powerful and can handle non-
categorical data. However, they are often slower (Section 6) and are
not designed for streaming IoT data. Closing this performance gap
remains an interesting area for future work.

Figure 8: Behavior of na¨ıve, shared-nothing scale-out.

MacroBase is an ongoing project, with several extensions under-
way. We are currently evaluating MacroBase in several domains
including industrial IoT, consumer wearables, datacenter monitoring,
and smart vehicles using additional data from commercial deploy-
ments. Early conversations and demonstrations with over ten IoT
companies have conﬁrmed our hypothesis that data management
systems are currently ill-equipped to handle the scale, expressivity,
and timeliness requirements of analytic monitoring queries. Instead,
the status quo largely consists of either vertical-speciﬁc solutions
that are difﬁcult to adapt or relatively brittle solutions such as alerts
that are based on pre-conﬁgured, static thresholds. The core chal-
lenge for MacroBase going forwards is to evolve its models and
modules to match and subsequently surpass these custom solutions.
In addition, our experiences conﬁrm that data for analytic monitor-
ing is available in many deployments. However, without an engine
for automating the analytic monitoring process, the data is unused.
In this paper, we investigated techniques that could operate with
minimal intervention from the end user. This has proven powerful
in our early engagements with end users. However, MacroBase’s
extensible architecture allows “pay as you go” model evolution. We
are currently investigating both extensions to time-series modeling
and non-parametric kernel density estimation techniques, which
will increase efﬁcacy for periodic and non-symmetric distributions.
Given recent access to additional production datasets and use cases,
we expect to further evolve our set of models, ideally ﬁnding suites
of models that are best-suited to particular domains. Over time,
we expect to develop a core set of models that will allow both
ensembling and, if desired, optional supervision from the user.

MacroBase currently escapes the curse of dimensionality by re-
quiring users to identify attributes and metrics, potentially running
queries in parallel over disjoint combinations. We are interested in
automating this query formulation process and are also currently
exploring how to ﬁnd outliers within particular contexts: for exam-
ple, a given device may not have high temperature readings when
compared to all other devices, but it may have a high temperature
readings when compared to other devices in the same room.

Finally, while MacroBase’s performance has been adequate for
almost all deployments we have encountered thus far, we are also
pursuing more sophisticated parallelization and distribution efforts
that better balance accuracy and scalability. The statistical nature of
analytic monitoring queries allows a range of strategies not available
to deterministic queries.

8. RELATED WORK

The goals of MacroBase span many subﬁelds of Computer Sci-
ence and statistics; as we describe below, we draw upon many of
them in this work:
Anomaly Detection. Anomaly and outlier detection methods date
to at least the 19th century; the literature contains thousands of
techniques from communities including statistics, machine learning,
data mining, and information theory [4, 10, 26]. Anomaly detection

11

12481632# Partitions (Cores)12481632Normalized ThroughputMCMSFSFC12481632# Partitions (Cores)0.00.20.40.60.81.0Summary F-ScoreStream Processing. A range of systems from both the database
community [2, 11] and industry (e.g., Storm, StreamBase, Spark)
provide generic infrastructure for executing streaming queries. Mac-
roBase effectively adopts dataﬂow as the substrate for execution.
However, MacroBase queries provide a higher level of abstraction
than either per-tuple dataﬂow processing or of windowed SQL ag-
gregates. Instead, as we have discussed, our default interface allows
users to simply highlight features and explore “interesting” patterns.
If desired, the user can directly program the dataﬂow operators.
However, dataﬂow is a means to an end rather than an end in itself.

9. CONCLUSIONS

Identifying interesting behaviors within massive volumes of data
is a core challenge and key value proposition in many emerging
IoT applications. Despite a host of methodologies for outlier detec-
tion and data summarization, there is limited systems infrastructure
supporting analytic monitoring queries, or automated detection and
explanation of interesting data—a problem we have encountered in
the ﬁeld for several years at CMT. As a result, we have developed
MacroBase, the ﬁrst engine for scalable, online analytic monitor-
ing. In this paper, we presented the MacroBase architecture and
its default execution strategy, including its use of robust statistical
estimators, novel sketching and streaming data structures, and our
initial experiences using MacroBase on a variety of real-world and
production datasets. By leveraging the observation that most IoT
data points are uninteresting, MacroBase’s design delivers perfor-
mance of up to 1M points per second per core while delivering
accurate results: MacroBase has already identiﬁed and diagnosed
several previously unknown behaviors at CMT. MacroBase is under
active development and is available as open source for deployment.

Acknowledgments
The authors would like to thank Christopher Aberger, Firas Abuzaid,
Ali Ghodsi, Mark Phillips, Christopher R´e, Leif Walsh, and Matei
Zaharia for helpful feedback on this work; the CMT development
team for both feedback and inspiration; Xu Chu, Ihab Ilyas, Vivek
Jain, and Arsen Mamikonyan for ongoing contributions to the Mac-
roBase engine; and Pavlos S. Efraimidis for his lucid explanation
and discussion of weighted reservoir sampling. This research is
supported in part by Toyota and Phillips Lighting.

10. REFERENCES
[1] Summary of the Amazon DynamoDB service disruption and related impacts in

the US-East region, 2015. https://aws.amazon.com/message/5467D2/.
[2] D. J. Abadi, Y. Ahmad, M. Balazinska, U. Cetintemel, M. Cherniack, J.-H.

Hwang, W. Lindner, A. Maskey, A. Rasin, E. Ryvkina, et al. The design of the
borealis stream processing engine. In CIDR, volume 5, pages 277–289, 2005.

[3] C. C. Aggarwal. On biased reservoir sampling in the presence of stream

evolution. In VLDB, 2006.

[4] C. C. Aggarwal. Outlier Analysis. Springer, 2013.
[5] R. Ahmad and S. Kamaruddin. An overview of time-based and condition-based

maintenance in industrial application. Computers & Industrial Engineering,
63(1):135–149, 2012.

[6] I. F. Akyildiz, W. Su, Y. Sankarasubramaniam, and E. Cayirci. Wireless sensor

networks: a survey. Computer networks, 38(4):393–422, 2002.

[7] A. Ben-Israel and T. N. Greville. Generalized inverses: theory and applications,

volume 15. Springer Science & Business Media, 2003.

[8] R. J. Bolton and D. J. Hand. Statistical fraud detection: A review. Statistical

science, pages 235–249, 2002.

[9] C. Buragohain and S. Suri. Quantiles on streams. In Encyclopedia of Database

Systems, pages 2235–2240. Springer, 2009.

[10] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey. ACM

computing surveys (CSUR), 41(3):15, 2009.

[11] S. o. Chandrasekaran. TelegraphCQ: Continuous dataﬂow processing for an

uncertain world. In CIDR, 2003.

[12] M. Chao. A general purpose unequal probability sampling plan. Biometrika,

69(3):653–656, 1982.

12

[13] J. Cheng et al. A survey on algorithms for mining frequent itemsets over data

streams. Knowledge and Information Systems, 16(1):1–27, 2008.

[14] G. Cormode, M. Garofalakis, P. J. Haas, and C. Jermaine. Synopses for massive

data: Samples, histograms, wavelets, sketches. Foundations and Trends in
Databases, 4(1–3):1–294, 2012.

[15] G. Cormode and M. Hadjieleftheriou. Methods for ﬁnding frequent items in

data streams. The VLDB Journal, 19(1):3–20, 2010.

[16] G. Cormode, F. Korn, and S. Tirthapura. Exponentially decayed aggregates on

data streams. In ICDE. IEEE, 2008.

[17] J. Dean et al. Large scale distributed deep networks. In NIPS, 2012.
[18] A. Deshpande et al. Model-driven data acquisition in sensor networks. In VLDB,

2004.

[19] P. S. Efraimidis. Weighted random sampling over data streams. In Algorithms,

Probability, Networks, and Games, pages 183–195. Springer, 2015.

[20] T. Escamilla. Intrusion detection: network security beyond the ﬁrewall. John

Wiley & Sons, Inc., 1998.

[21] P. Fournier-Viger. SPMF: An Open-Source Data Mining Library – Performance,

2015. http://www.philippe-fournier-viger.com/spmf/.

[22] J. Han et al. Frequent pattern mining: current status and future directions. Data

Mining and Knowledge Discovery, 15(1):55–86, 2007.

[23] J. Han, J. Pei, and Y. Yin. Mining frequent patterns without candidate

generation. In SIGMOD, 2000.

[24] J. M. Hellerstein. Quantitative data cleaning for large databases. United Nations

Economic Commission for Europe (UNECE), 2008.

[25] D. C. Hoaglin, F. Mosteller, and J. W. Tukey. Understanding robust and

exploratory data analysis, volume 3. Wiley New York, 1983.

[26] V. J. Hodge and J. Austin. A survey of outlier detection methodologies.

Artiﬁcial Intelligence Review, 22(2):85–126, 2004.

[27] P. J. Huber. Robust statistics. Springer, 2011.
[28] M. Hubert and M. Debruyne. Minimum covariance determinant. Wiley
interdisciplinary reviews: Computational statistics, 2(1):36–43, 2010.

[29] B. Hull et al. Cartel: a distributed mobile sensor computing system. In SenSys,

2006.

[30] W. H. Hunter. US Chemical Safety Board: analysis of Horsehead Corporation

Monaca Reﬁnery fatal explosion and ﬁre, 2015.
http://www.csb.gov/horsehead-holding-company-fatal-explosion-
and-fire/.

[31] I. F. Ilyas and X. Chu. Trends in cleaning relational data: Consistency and

deduplication. Foundatations and Trends in Databases, 2015.

[32] J. Kim and C. D. Scott. Robust kernel density estimation. The Journal of

Machine Learning Research, 13(1):2529–2565, 2012.

[33] S. Madden et al. TAG: A tiny aggregation service for ad-hoc sensor networks.

In OSDI, 2002.

[34] J. Manyika. McKinsey Global Institute: The internet of things: mapping the

value beyond the hype. 2015.

[35] R. Manzini, A. Regattieri, H. Pham, and E. Ferrari. Maintenance for industrial

systems. Springer Science & Business Media, 2009.

[36] R. Maronna, D. Martin, and V. Yohai. Robust statistics. John Wiley & Sons,

Chichester. ISBN, 2006.

[37] A. Metwally et al. Efﬁcient computation of frequent and top-k elements in data

streams. In ICDT. Springer, 2005.

[38] B. Mukherjee, L. T. Heberlein, and K. N. Levitt. Network intrusion detection.

Network, IEEE, 8(3):26–41, 1994.

[39] C. Phua, V. Lee, K. Smith, and R. Gayler. A comprehensive survey of data

mining-based fraud detection research. arXiv preprint arXiv:1009.6119, 2010.
[40] S. Rajasegarar, C. Leckie, and M. Palaniswami. Anomaly detection in wireless

sensor networks. Wireless Communications, IEEE, 15(4):34–40, 2008.
[41] P. J. Rousseeuw and K. V. Driessen. A fast algorithm for the minimum
covariance determinant estimator. Technometrics, 41(3):212–223, 1999.

[42] S. Roy and D. Suciu. A formal approach to ﬁnding explanations for database

queries. In SIGMOD, 2014.

[43] J. Shin et al. Incremental knowledge base construction using DeepDive. In

VLDB, 2015.

[44] S. K. Tanbeer et al. Sliding window-based frequent pattern mining over data

streams. Information sciences, 179(22):3843–3865, 2009.

[45] J. S. Vitter. Random sampling with a reservoir. ACM Transactions on

Mathematical Software (TOMS), 11(1):37–57, 1985.

[46] X. Wang, X. L. Dong, and A. Meliou. Data x-ray: A diagnostic tool for data

errors. In SIGMOD, 2015.

[47] B. Warneke et al. Smart dust: Communicating with a cubic-millimeter

computer. Computer, 34(1):44–51, 2001.

[48] M. Weiser. Some computer science issues in ubiquitous computing.

Communications of the ACM, 36(7):75–84, 1993.

[49] E. Wu and S. Madden. Scorpion: Explaining away outliers in aggregate queries.

In VLDB, 2013.

[50] M. Xie, S. Han, B. Tian, and S. Parvin. Anomaly detection in wireless sensor

networks: A survey. Journal of Network and Computer Applications,
34(4):1302–1325, 2011.

