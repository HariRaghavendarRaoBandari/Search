6
1
0
2

 
r
p
A
6

 

 
 
]
L
C
.
s
c
[
 
 

2
v
4
5
3
5
0

.

3
0
6
1
:
v
i
X
r
a

Modeling phonological similarity effects on the self-organization of vocabularies

Javier Vera (jxvera@gmail.com)

Facultad de Ingenier´ıa y Ciencias, Universidad Adolfo Ib´a˜nez

Santiago, Chile

Abstract

This work develops a computational model (by Automata Net-
works) of phonological similarity effects involved in the for-
mation of word-meaning associations on artiﬁcial populations
of speakers. Classical studies show that in recalling experi-
ments memory performance was impaired for phonologically
similar words versus dissimilar ones. Here, the individuals
confound phonologically similar words according to a prede-
ﬁned parameter. The main hypothesis is that there is a criti-
cal range of the parameter, and with this, of working-memory
mechanisms, which implies drastic changes in the ﬁnal con-
sensus of the entire population. Theoretical results present
proofs of convergence for a particular case of the model within
a worst-case complexity framework. Computer simulations
describe the evolution of an energy function that measures the
amount of local agreement between individuals. The main
ﬁnding is the appearance of sudden changes in the energy func-
tion at critical parameters.
Keywords: Phonological similarity; Working memory; Au-
tomata networks; Linguistic conventions.

Introduction

In natural language a typical linguistic interaction is not a
simple sequence of individual actions, but also a form of joint
activity that involves cooperation and coordination between
participants (Tomasello, 2008). What is more, inside the dia-
logue language users tend to converge in their choice of con-
structions. This mutual convergence process, or alignment,
has been extensively studied within computational studies of
the formation of language through the Naming Game (Steels,
1995, 2011; Baronchelli, Felici, Caglioti, Loreto, & Steels,
2006; Loreto, Baronchelli, Mukherjee, Puglisi, & Tria, 2011).
It attempts to ask how on a population of agents only from lo-
cal interactions it arises a shared word-meaning association
(the simplest version of a vocabulary). This model consid-
ers a ﬁnite population of agents, where each one is endowed
with a memory in which it stores an in principle unlimited
number of words. At each discrete time step a pair of agents
is selected: one plays the role of hearer, one plays the role
of hearer. First, the speaker refers to an object by using a
word. Next, the hearer tries to identify the referent. For this
purpose, the hearer inspects its own memory: (1) if the word
belongs to the memory of the hearer, both speaker and hearer
cancel all the words in their memories, except such word; or
(2) the hearer adds the word to its memory, if the word does
no belong to its memory. A pairwise interaction such as in
the Naming Game only is possible if the interaction is a kind
of joint activity. In a more realistic scenario both agents in-
teract in a context where the object is located and share focus
on the object by means of pointing or eye-gazing. The in-
teraction continues until both agents reach a common word
associated to the object, a linguistic convention.

Here, it is assumed that the development of linguistic con-
ventions is founded on self-organization mechanisms arising
only from local interactions between agents (Steels, 1995,
1996; Baronchelli et al., 2006). Given this self-organized na-
ture Automata Networks (AN) (Neumann, 1966; Wolfram,
2002) provide the adequate framework to explore alignment
from a computational (and mathematical) point of view. AN
are extremely simple models where each vertex of a network
evolves following a local rule based on the states of “nearby”
vertices. Despite of the simplicity of the deﬁning rules, AN
exhibit astonishing rich patterns of behavior. A linguistic con-
vention can be considered as a complex pattern.

What is the relationship between cognitive mechanisms of
the individuals and the emergence of language on artiﬁcial
populations? The question entails the study of computational
machines which exhibit limitations and constraints of human
language users. This work stresses a novel question related
to models of the self-organization of language: To what ex-
tent do working memory constraints inﬂuence the alignment
of shared conventions on artiﬁcial populations of agents?
Language users (and therefore agents playing computational
games of the formation of language) suffer limited working
memory constraints (A. Baddeley & Hitch, 1974; A. Badde-
ley, 2007). Particularly, phonological similarity effects sup-
pose that individuals confound word items sharing large por-
tions of phonological content. A classical work (A. D. Bad-
deley, 1966) reports experiments where subjects heard se-
quences of unrelated words and tried to recall in the correct
order. The results suggest that memory performance was im-
paired for phonologically similar words (man, cad, mat, cap,
can) versus dissimilar ones (pen, sup, cow, day, hot). This ef-
fect is a strong evidence for the existence of the phonological
loop, understood as part of the short-term working memory
system (A. Baddeley & Hitch, 1974; A. Baddeley, 2007).

This paper does not attempt to develop a “realistic” model,
but rather an abstract symbolic approach that extracts the es-
sential elements of the problem. The simulations described
here are based on a parameter that measures the amount of
phonological confusion between words (considered as a way
to describe the inﬂuence of working memory limits). The
work explores the hypothesis of there being a critical range
of the parameter that implies drastic changes in the shared
conventions of the entire population. Therefore, the features
of language (in particular, the consensus on linguistic conven-
tions) emerge abruptly at some critical range of phonological
confusion (Hauser, 1996). This hypothesis is strongly related
to previous work on on the absence of stages in the formation
and evolution of human languages (Bickerton, 1998; Calvin
& Bickerton, 2001; Ferrer-i-Cancho & Sol´e, 2003).

The work is organized as follows. Section “Model” ex-
plains basic notions on AN, the instrumentalization of phono-
logical similarity and the rules of interaction. Section “The-
oretical results” reports simple mathematical results related
to the convergence of particular cases of the model. The
next section (“Simulations”) describes simulation tasks based
on an energy operator, over a parameter that measures the
amount of similarity confusion between words. A brief dis-
cussion of the results is presented in the ﬁnal section.

Model

Elements of the AN model
Roughly speaking the AN model involves the following ele-
ments:

1. A regular grid graph: vertices represent individuals; edges
represent possible communicative interactions. More gen-
erally, as in the section “Theoretical results”, the graph can
be a connected, undirected and simple network.

2. Each individual is associated to a state that eventually
changes along time. This state is a way to represent the
language of the individual at some time frame.

3. A set of local rules that deﬁne how the system changes.
The local rule associated to one individual considers as in-
puts the states of the nearby individuals (the neighbors).

4. A function, the updating scheme, that indicates the order in
which the individuals are updated. Two updating schemes
are considered in this work:
(1) the fully-asynchronous
scheme, where at each time step one individual is choosen
uniformly at random; and (2) the sequential scheme, de-
ﬁned as a permutation of the set of vertices.

In what follows some of the previous elements will be

treated in greater depth.

Basic notions
The set P = {1, ...,n} represents the population of individuals,
located on the vertices of the regular grid graph G = (P,E),
where E is the set of edges. The vertex u ∈ P only inter-
acts (or “talks”) with the set of adjacent vertices Vu = {v ∈
P : (u,v) ∈ E} (the neighborhood). Each individual considers
four neighbors: up, down, left and right ones (Von Neumann
neighborhood). The individual u ∈ P is endowed with a mem-
ory set Mu in which it stores words belonging to a ﬁnite set
W , with p elements.
Let S = {a1, ...,as} be a set of sounds. Each word of W
is constructed by a combination of sounds taken from S
. For
instance, a3a1a8 ∈ W . The length of the word x is its num-
ber of sounds. For the sake of simplicity, all the words have
the same length L. x(k), with k 6 L, denotes the k−th sound
(or position) of the word x. To explicitly measure the amount
of phonological similarity between words the Hamming dis-
tance H(x,y) between the words x and y is deﬁned as the
number of positions in which they differ. Consider two words
a4a6a5 and a7a6a3, then H(a4a6a5,a7a6a3) = 2.

Confusion parameter
To explicitly measure the ability to distinguish between words
the confusion parameter e ∈ [0,1] is deﬁned. Suppose that the
vertex u faces two words x,y. Then,

if H(x,y) > e L, u distinguishes the words x and y

else u confounds the words x and y (or simply “x = y”)

For instance, the individual u ∈ P is confronted with the
words x = a1a8a6a4 and y = a1a8a6a3. Two values of e are
considered, 0 and 0.5:
• (e = 0) H(x,y) = 1 > e L = 0 × 4 = 0, then u distinguishes

the words x and y

• (e = 0.5) H(x,y) = 1 < e L = 0.5 × 4 = 2, then u confounds

the words x and y

Local rules
Inspired in the Naming Game (Baronchelli et al., 2006), the
local rule associated to the individual u ∈ P is based on two
possible actions on the memory Mu:

• u updates its memory Mu by the addition of words; or

• u collapses its memory if Mu is updated by cancelling all

its words, except one of them.

Both actions attempt to take into account lateral inhibition
strategies (Steels, 1995, 2011) in the alignment process: the
individuals add words in order to increase the chance of fu-
ture agreements (local consensus), and defect the words that
do not cooperate with mutual understanding. In a collapse
the individuals prefer the minimal word, according to the lex-
icographic order over the set of words. The lexicographic
order, denoted ≺, is a generalization of the typical alphabet-
ical order of words on the alphabetical order of their com-
ponent letters (or sounds). For example in the dictionary the
word “Me” appears before “My” because the letter e comes
before the letter y in the alphabet. In some sense the word
“Me” is lower than the word “My”. Formally, the order <
is deﬁned on the set S
. Two words x and y of length L
are considered. Then, x ≺ y (x is lexicographically lower
than y) if the ﬁrst position in which they differ, say k 6 L,
satisﬁes x(k) < y(k). For instance given the set of sounds
S = {a,b,c,d}, with a < b < c < d, the words abc, bcd and
cda satisfy abc < bcd < cda. Therefore, abc is the minimal
word or, in other terms, min({abc,bcd,cda}) = abc. Asso-
ciated to the previous words “Me” and “My” it is possible to
write min({“Me”,“My”}) = “Me”.

The preference for the minimal words can be viewed in
accordance with the following hypothetical scenario (Nowak
& Krakauer, 1999).
It is possible to think in a population
of early hominids for which leopards represents a higher risk
than cows. So, the word “leopard” may be more valuable
than “cow”. In the terms of this paper “leopard” can be the
minimal word.

At time step t the vertex u ∈ P is selected according to the
updating scheme (fully asynchronous or sequential). Con-
sider a simple population P = {1,2,3,4,5} (each number rep-
resents one individual). For a fully asynchronous scheme at
each time step any individual of P can be selected (for in-
stance, the individual 3). In the next step the individual 3 can
be selected again. For a sequential scheme a permutation of
the set P is deﬁned, for instance, the order 5-4-3-2-1. The in-
dividual 5 updates ﬁrst, then the individual 4 updates, taking
into account the effects of the changes in the ﬁrst individual,
and so on. At time step 6 the dynamics starts in the same pre-
vious way. In other terms a sequential scheme supposes that
each individual is updated after that all the other individuals
of the population have been updated.

The individual u ∈ P is completely characterized by its
state (Mu,xu), where Mu is the memory to stores words (Mu
is a subset of W ) and xu ∈ Mu is a word that u conveys to the
vertices of Vu. The model induces speciﬁc communication
roles: the vertex u plays the role of “hearer” (it receives the
words conveyed by its neighbors); the neighbors of u play the
role of “speaker” (they convey words to the vertex u). The
set of all words conveyed by the speakers can be re-written
as two subsets: Nu and Bu. Roughly speaking Nu includes the
unknown words, and Bu includes the known ones.

The state pair (Mu,xu) changes according to the following
steps, which deﬁne the local rule of the automata (see Fig.
1):

step 1 the vertex u deﬁnes two sets:

Nu = {xv : (v ∈ Vu) ∧ (∀y ∈ Mu,H(xv,y) > e L)}
Bu = {xv : (v ∈ Vu) ∧ (∀y ∈ Mu,H(xv,y) 6 e L)}

step 2

if Nu 6= /0, Mu adds the words of Nu
else Mu collapses in the word ¯x, selected at random from
the set {x ∈ Bu : H(x,min(Bu)) 6 e L} (that is, the new
state is ({ ¯x}, ¯x))

Step 1 comprises (1) the speaker’s behavior (the neighbors
convey words to the vertex u); and (2) the deﬁnition by the
hearer of the sets Nu and Bu. Given a conveyed word xv, v ∈
Vu, the hearer u decides between: either xv is added to Nu if
for all y ∈ Mu,H(xv,y) > e L; or xv is added to Bu, otherwise.
Step 2 summarizes the behavior of the hearer in order to
align itself with the speakers. In the case that Nu 6= /0, the
hearer simply adds to its memory the words of Nu. Otherwise
(Nu = /0), the hearer collapses its memory in the word ¯x ∈ Bu.
The word ¯x is selected uniformly at random from {x ∈ Bu :
H(x,min(Bu)) 6 e L}. Thus, the preferred word min(Bu) can
be confused by “similar” ones (with H(·,min(Bu)) 6 e L).
Dynamics of the automata
As initial conﬁguration each individual receives a word con-
structed by a random combination of L sounds from the set
of symbols S
. Thus, at t = 0 each individual is associated

to a state of the form ({x},x), with x ∈ W . Each discrete
time step t > 0 supposes that one individual, say u, is selected
uniformly at random (the fully-asynchronous scheme) or ac-
cording to a permutation of the set of vertices (the sequential
scheme). The individual receives the words conveyed by its
neighbors: it plays the role of hearer, the neighbors play the
role of speaker. Regarding to the conveyed words the individ-
ual follows the two deﬁned steps in order to decide possible
changes in its own language state:

step 1 the individual deﬁnes the sets Nu (unknown words)
and Bu (known words).

step 2 the individual either adds words if Nu is non empty
or collapses its memory in the minimal word of Bu, if Nu
is empty.

Both steps involve the possibility of confusion between
similar words. In the next time step t + 1 another individual
(possibly the same one) is selected, and so on.

A ﬁxed point is a conﬁguration invariant under the appli-
cation of local rules, which can be interpreted as a ﬁnal con-
sensus conﬁguration, where all individuals agree about some
linguistic convention.

Theoretical results

An interesting problem related to the formation of consen-
sus on linguistic conventions is to propose a proof of con-
vergence. Given the mathematical framework of this paper
the problem becomes to count (in the worst case) the number
of simulation steps whom the dynamics needs to stop, that
is, to prove that after a ﬁnite number of time steps the pop-
ulation reaches a shared linguistic convention. Despite that
other works have solved related tasks (DeVylder & Tuyls,
2006) the novelty of the rest of this section is to develop a
convergence proof based on the worst-case complexity, which
measures the amount of resources (running time) needed by
the system if it is considered as an algorithm. Running time,
deﬁned as the number of steps until the entire population
reaches a global consensus language, indicates the largest dy-
namics performed by the automata given the size n of the
population (denoted O( f (n)), where f (n) is a function of n).
For instance O(n2) means that in the worst-case the running
time has a growth rate scaling as n2.

In this section individuals are located on a general undi-
rected and connected network (not necessarily a regular grid),
and they do not confound words, that is, e = 0.

It

is straightforward to notice that at

e = 0 (∀y ∈
Mu,H(xv,y) > e L) is equivalent to (xv /∈ Mu). Roughly speak-
ing, the expression (∀y ∈ Mu,H(xv,y) > e L) means that xv is
“different” to every word in Mu and, therefore, xv /∈ Mu. As a
consequence the two steps of the rule take a simpler form:

step 1 the vertex u deﬁnes two sets:

Nu = {xv : (v ∈ Vu) ∧ (xv /∈ Mu)}
Bu = {xv : (v ∈ Vu) ∧ (xv ∈ Mu)}

bacd

cabd

({abcd, bacd}, bacd)

(addition)

({abcd, bacd, cabd, dabc}, bacd)

dabc

bacd

cabd

({abcd, bacd, cabd}, bacd)

(collapse)

({abcd}, abcd)

abcd

Figure 1: Example of the two actions at e = 0. The order a < b < c < d is deﬁned on the set S = {a,b,c,d}. W =
{abcd,bacd,cabd,dabc}. The lexicographic order establishes abcd ≺ bacd ≺ cabd ≺ dabc. The confusion parameter is
set to e = 0 (that is, the individuals do not confound words). Suppose that at some time step the central vertex (u ∈ P) has been
choosen. Four individuals participate in the interaction: the central vertex u and its three neighbors of Vu. Associated to each
action of the local rule, two different conﬁgurations are showed. First row (addition): Bu = {bacd} and Nu = {cabd,dabc}.
Second row (collapse): Bu = {abcd,bacd,cabd} and Nu = /0.

step 2

if Nu 6= /0, Mu adds the words of Nu
else Mu collapses in the word min(Bu) (the new state is
({min(Bu)},min(Bu)))

First of all, a sequential updating scheme is considered.
This means that a permutation of the vertices is deﬁned. As
previously noted, each individual is updated after that all the
other individuals of the population have been updated.
In
the worst case, each time step supposes that one individual
adds one word. This process ends when some individual
has all possible words, that is, after p − 1 steps (there are p
words). Since the population has size n, after n(p − 1) steps
the individuals must collapse their memories. Then, at step
t∗ = n(p −1) all individuals have been collapsed at least once.
As the theorem shows in detail the minimum conveyed word
at t∗ propagates in the system until it reaches a ﬁxed point
where all individuals convey this minimum word.

Theorem 1 Consider a population of n individuals playing
the automata model with the set of p words W and confusion
parameter e = 0. Then,

(I) for the sequential scheme, the system converges to ﬁxed

points in at most O(n2 p) steps;

(II) for the fully-asynchronous scheme,

the system con-
verges to a ﬁxed point in expected time O(n2 p log(n)).

Proof 1 (I) Initially there are p words. Then, in at most p − 1
updates a vertex has collapsed for the ﬁrst time (in the worst
case the vertex must add every possible word, one at a time).

This implies that in n(p − 1) steps (p − 1 updates of each
vertex) all vertices have collapsed at least one time. Let m
be the minimum conveyed word at step t∗ = n(p − 1), and
let u be a vertex such that xu = m (in more precise terms,
m = min({xu}u∈P)).

Since u has collapsed at least one time,

the updating
scheme is sequential, and m is the minimum word, then u
must have another neighbor v ∈ Vu conveying m. In conse-
quence, after t∗ both vertices u and v will remain conveying
m, and at each time a neighbor of any of these two vertices
will necessarily collapse in the word m. The graph has a di-
ameter of O(n) and each np steps it occurs a collapse. There-
fore, in at most O(n2 p) steps the system converges to a ﬁxed
point where all vertices convey the same word m.

(II) In the fully-asynchronous scheme, at each time step a
single vertex is picked independently and uniformly at ran-
dom. The expected convergence time grows by a factor of
O(log(n)) with respect to the sequential scheme. Notice that
in the proof of the part (I) it is shown that after updating
[O(np) times] [each vertex], a ﬁxed point is reached. From
the well known coupon collector’s problem (see, for instance,
(Grimmett & Stirzaker, 2001)), the expected number of steps
required to update [at least once][every vertex in the graph
(or pick every coupon)] is O(n log(n)). Since in a fully asyn-
chronous updating scheme each step is independent to the
others, the result follows.

Simulations

Protocol
To describe the amount of agreement between individuals, an
energy operator is deﬁned. This energy-based approach arises
from a “physicist” interpretation, related to (Regnault, Sch-
abanel, & Thierry, 2009): the energy measures the amount
of local unstability of the system. At each neighborhood Vu
the function (cid:229)
v∈Vu H(xu,xv) is deﬁned, which measures the
Hamming distance between the word xu and the words con-
veyed by the neighbors of the individual u. This function is
bounded by 0 (in case of agreement, that is, the individual u
and its neighbors convey the same word) and 4L (disagree-
ment, which means that the individual u and its neighbors
convey radically different words). Summing over all individ-
uals deﬁnes the total energy function at some time step t:

E(t) =

1
4Ln

u∈P

v∈Vu

H(xu,xv)

(1)

The function E(t) is bounded: 0 6 E(t) 6 1. Thus, the dy-
namics of the AN model can be understood as the trajectory
between initial conﬁgurations associated to large amounts of
unstability (with E(0) ∼ 1) and ﬁnal consensus conﬁgurations
where E(t) ∼ 0, or equivalently, all individuals convey simi-
lar -in the sense deﬁned by the Hamming distance- words.

The analysis focuses on two-dimensional periodic lattices
of size n = 1282 with Von Neumann neighborhood. The
simulations describe 500n steps of the energy function E(t).
At each time step one vertex (playing the role of hearer) is
selected uniformly at random (fully-asynchronous scheme).
The plots show average values over 20 initial conditions. An
initial condition is deﬁned as follows: each individual re-
ceives uniformly at random a word constructed by a random
combination of L sounds from the set of symbols S
, |S
| = 10.
Word-length L varies from: {2,4,8,16,32,64}.
is varied
from 0 to 1 with an increment of 10%.

Given the worst-case complexity approach to theoretical
aspects of convergence it is important to notice that on low-
dimensional lattices it seems hard that one individual adds
O(np) words after it collapses, because lattices are low-
connected. This intuition suggests that running times on lat-
tices will be lower than theoretical bounds (O(n2p log(n)) for
the fully-asynchronous scheme).

Results
There are several remarkable aspects, as shown in Fig. 2 and
Fig. 3. First of all, E(t = 500n) versus e exhibits at e = 1 a
maximum which is close to 0.5 for all values of L (Fig. 2).
Secondly, to the extent L grows, E(t = 500n) versus e evolves
more “smoothly”: L 6 8 supposes “ladder” steps which mean
that different values of the confusion parameter e
lead to the
same energy. Finally, focusing the description on L = 32 (Fig.
3), it is interesting to notice that at e = 0.7 the average value
of E(t) stops approximately after t = 200n steps. This fact
exhibits that only a few set of runs does not converge to the
global minimum of the E(t) = 0. This strongly suggests the

appearance of a critical parameter e ∗ ∼ 0.7 which clearly de-
ﬁnes two phases in the evolution of E(t = 500n) versus e :
(1) e < e ∗ implies the convergence to the global minimum
E(t) = 0, where all individuals convey the same word; and
(2) for e > e ∗ the dynamics changes drastically until it reaches
local minima of the energy function (E(t) → 0.5).

)
n
0
0
5
=

t
(
E

)
n
0
0
5
=

t
(
E

0.6

0.5

0.4

0.3

0.2

0.1

0.0

0.0

0.6

0.5

0.4

0.3

0.2

0.1

0.0

0.0

L =2
L =4
L =8

0.2

0.4

0.6

0.8

1.0

ǫ

L =16
L =32
L =64

0.2

0.4

0.6

0.8

1.0

ǫ

Figure 2: E(t = 500n) versus e . On a two dimensional grid
of size n = 1282, the ﬁgure shows the ﬁnal value of the energy
function versus the parameter e , after 500n steps or until they
reach the global minimum E(t) = 0. The plots show averages
over 20 initial conditions: for an initial condition, each vertex
receives a word constructed by a random combination of L
sounds from the set of symbols S
| = 10. Word-length
varies from: {2,4,8} (top); and {16,32,64} (bottom).
is
varied from 0 to 1 with an increment of 10%.

, |S

Conclusion

This work summarizes an AN approach to the formation of
linguistic conventions under phonological similarity (and, in
general, working memory) mechanisms. The paper presents
the evolution of an energy functional, deﬁned as a word “con-
fusion” average, during the alignment game. Two aspects are
remarkable. On the one hand, the appearance of drastic tran-
sitions can be related to previous works that focus on the ab-
sence of stages in the formation and evolution of human lan-
guages (see, for instance, (Bickerton, 1998; Calvin & Bick-
erton, 2001; Ferrer-i-Cancho & Sol´e, 2003)). On the other
hand, the proposed model becomes an alternative (mathemat-
ical) framework for agent-based studies on language forma-
tion.

As a ﬁrst approach to the convergence of the formation of

(cid:229)
(cid:229)
e
e
)
t
(
E

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
0

0.0
0.7
0.8
0.9
1.0

100

200

300

400

500

t

Figure 3: E(t) versus t, L = 32, for different values of e .
On a two dimensional grid of size n = 1282, the ﬁgure shows
evolution of the energy function E(t) versus t, for different
values of the parameter e . Simulations run 500n steps or until
it reaches the global minimum E(t) = 0. The plot shows av-
erages over 20 initial conditions: for an initial condition, each
vertex receives a word constructed by a random combination
of L sounds from the set of symbols S
| = 10. L = 32 and
is varied from {0,0.7,0.8,0.9,1}.

, |S

linguistic conventions this paper presents within an AN ac-
count simple results of the number of steps needed to reach
ﬁxed points. As the main tool the proofs are based on the
worst case of convergence.

Many extensions of the proposed model should be stud-
ied with the purpose to describe the role of cognitive con-
straints (and, in particular, the role of (working) memory lim-
its) on the formation of linguistic conventions. First, within
a mathematical point of view it seems interesting to explore
convergence bounds on regular lattices. Also, a comparison
between theoretical and numerical convergence times should
be carried out. Second, AN allow to study new aspects of
the formation of linguistic conventions. Indeed, the model
can be extended to other updating schemes, for example, the
synchronous one, where at each time step all individuals are
updated. Third, the results should be compared with larger
word lengths (L) and several number of symbols (S ). Finally,
future work should involve more realistic ways to measure
the amount of phonological confusion and its effects on the
formation of conventions.

Acknowledgments

The author likes to thank CONICYT-Chile under the Doctoral
scholarship 21140288.

References

Baddeley, A. (2007). Working memory, thought, and action.

similarity. Quarterly Journal of Experimental Psychology,
18(4), 362-365. (PMID: 5956080)

Baronchelli, A., Felici, M., Caglioti, E., Loreto, V., & Steels,
L. (2006). Sharp transition towards shared vocabularies in
multi-agent systems. J. Stat. Mech.(P06014).

Bickerton, D. (1998). Catastrophic evolution: The case for
a single step from protolanguage to full human language.
In J. R. Hurford, S. M. Kennedy, & C. Knight (Eds.), Ap-
proaches to the evolution of language: Social and cognitive
bases (pp. 341–358). Cambridge: Cambridge University
Press.

Calvin, W., & Bickerton, D. (2001). Lingua ex machina: Rec-
onciling darwin and chomsky with the human brain. MIT
Press.

DeVylder, B., & Tuyls, K. (2006). How to reach linguistic
consensus: A proof of convergence for the naming game.
The Journal of Theoretical Biology, 242(4), 818–831.

Ferrer-i-Cancho, R., & Sol´e, R. V. (2003). Least Effort and
the Origins of Scaling in the Human Language. Proceed-
ings of the National Academy of Science (USA), 100, 788–
791.

Grimmett, G., & Stirzaker, D. (2001). Probability and ran-

dom processes. OUP Oxford.

Hauser, M. (1996). The evolution of communication. MIT

Press.

Loreto, V., Baronchelli, A., Mukherjee, A., Puglisi, A., &
Tria, F. (2011). Statistical physics of language dynamics.
Journal of Statistical Mechanics: Theory and Experiment,
2011(04), P04006.

Neumann, J. von.

(1966). Theory of self-reproducing au-

tomata. Champain, IL: University of Illinois Press.

Nowak, M. A., & Krakauer, D. C.

(1999). The evolution
of language. Proceedings of the National Academy of Sci-
ences, 96(14), 8028-8033.

Regnault, D., Schabanel, N., & Thierry, E. (2009). Progresses
in the analysis of stochastic 2d cellular automata: A study
of asynchronous 2d minority. Theoretical Computer Sci-
ence, 410(4749), 4844 - 4855.

Steels, L. (1995). A self-organizing spatial vocabulary. Arti-

ﬁcial Life, 2(3), 319-332.

Steels, L. (1996). Self-organizing vocabularies. In Proceed-
ings of artiﬁcial life v, nara, japan (pp. 179–184). Nara,
Japan.

Steels, L. (2011, December). Modeling the cultural evolution

of language. Physics of Life Reviews, 8(4), 339-356.

Tomasello, M. (2008). The origins of human communication.

MIT Press.

Wolfram, S. (2002). A new kind of science. Wolfram Media.

Oxford University Press.
Baddeley, A., & Hitch, G.

In
G. Bower (Ed.), Recent advances in learning and motiva-
tion (Vol. 8, p. 47-90). Academic Press.

(1974). Working memory.

Baddeley, A. D.

(1966). Short-term memory for word se-
quences as a function of acoustic, semantic and formal

e
