Scaled stochastic gradient descent for

low-rank matrix completion

Bamdev Mishra† and Rodolphe Sepulchre§

6
1
0
2

 
r
a

 

M
6
1

 
 
]

G
L
.
s
c
[
 
 

1
v
9
8
9
4
0

.

3
0
6
1
:
v
i
X
r
a

Abstract— The paper looks at a scaled variant of

the
stochastic gradient descent algorithm for the matrix completion
problem. Speciﬁcally, we propose a novel matrix-scaling of
the partial derivatives that acts as an efﬁcient preconditioning
for the standard stochastic gradient descent algorithm. This
proposed matrix-scaling provides a trade-off between local and
global second order information. It also resolves the issue of
scale invariance that exists in matrix factorization models. The
overall computational complexity is linear with the number
of known entries, thereby extending to a large-scale setup.
Numerical comparisons show that
the proposed algorithm
competes favorably with state-of-the-art algorithms on various
different benchmarks.

I. INTRODUCTION

The problem of low-rank matrix completion amounts to
completing a matrix from a small number of entries by
assuming a low-rank model for the matrix. This problem
has been addressed both from theoretical [1]–[3] as well as
from algorithmic viewpoints [2], [4]–[18]. A standard way
of approaching the problem is by casting it as a ﬁxed-rank
optimization problem with the assumption that the optimal
rank r is known a priori, i.e.,

min

X∈Rn×m
subject to rank(X) = r,

1

2 kPΩ(X) − PΩ(X⋆)k2

F

(1)

where X⋆ ∈ Rn×m is a matrix whose entries are known
for indices if they belong to the subset (i, j) ∈ Ω and Ω
is a subset of the complete set of indices {(i, j) : i ∈
{1, ..., n} and j ∈ {1, ..., m}}. The operator PΩ(Xij ) =
Xij if (i, j) ∈ Ω and PΩ(Xij ) = 0 otherwise is called
the orthogonal sampling operator and is a mathematically
convenient way to represent the subset of entries. The cost
function is, therefore, a least squares cost function, where
k · kF is the Frobenius norm and |Ω| is the number of
known entries. The low-rank assumption on (1) implies that
r ≪ min(n, m). The rank constraint correlates the known
entries with the unknown ones. The number of given entries
|Ω| is typically much smaller than nm, the total number
of entries in X⋆. Recent contributions provide conditions on
|Ω| under which exact reconstruction is possible from entries
sampled uniformly and at random [1], [2]. Problem (1)
and its (many) variants ﬁnd applications in control systems

India, Bangalore

†Amazon Development Centre

India
(Bamdevm@amazon.com).
Initial work was done while this author
was with the Department of Electrical Engineering and Computer Science,
University of Li`ege, 4000 Li`ege, Belgium and was visiting the Department
of Engineering (Control Group), University of Cambridge, Cambridge CB2
1PZ, UK.

560055,

§University of Cambridge, Department of Engineering, Trumpington

Street, Cambridge CB2 1PZ, UK (R.Sepulchre@eng.cam.ac.uk).

and system identiﬁcation [19], machine learning [12], and
information theory [20], to name a just few.

A popular way to tackle the rank constraint in (1) is by
using a factorization model. The earlier works [21], [22]
discuss factorization models and show how to perform ﬁrst
and second order optimization by exploiting the Rieman-
nian optimization framework on manifolds. These discuss
the role of scale invariance arising due to non-uniqueness
factorization models. [10], [14] circumvent this by exploiting
the Riemannian submanifold structure of rank constraint and
provide a spectrum of algorithms. Preconditioning with rank
constraint in the context of matrix completion is explored
in [6], [11], [22], [23]. Alternating minimization algorithms
that exploit the least-squares structure of the cost function
in matrix completion are proposed in [9], [18]. The least-
squares structure is also exploited in [2], [6] to develop ﬁrst
and second order algorithms on the Grassmann manifold.
The matlab toolbox Manopt contains additional implemen-
tations [24].

While all the above-mentioned algorithms are sequential
algorithms, the works [8], [12], [13], [16], [17] focus on
parallel and stochastic versions. An alternating least-squares
approach is proposed in [16] to learn the rows of factorization
model, where each subproblem has a closed-form solution.
The paper [17] also exploits the least-squares structure, but
at the level of the entries of the rows of factorization models.
[8] focuses on learning r rank-1 factorizations cyclically,
where each subproblem is solved using the algorithm of [17].
The stochastic gradient descent algorithm (SGD) proposed
in [12] updates the factorization model as and when the
known entries are observed. The speciﬁc focus there is on
parallelization of the SGD algorithm. A distributed version
of SGD is proposed in [13]. Another approach that is suitable
in an online setup is proposed in [15], where data are
assumed to be streaming from low-dimensional subspaces.
The works [14], [21] exploit the Riemannian structure of
rank constraint to propose online algorithms (as particular
instances of problems with rank-one gradients) for low-rank
matrix completion.

Our focus in this paper is on a scaled variant of SGD
that accelerates the standard SGD algorithm and respects
the scale invariance property of the factorization model. To
achieve this, we propose a novel matrix-scaling of the partial
derivatives in Section II that combines global and local
second order information. We show that the computational
cost of the algorithm per pass through |Ω| known entries
is O(|Ω|(r3/b + bLr2/b + bRr2/b + r + logb)), where b is
the batch size that we pick while updating the factorization

PROPOSED STOCHASTIC GRADIENT DESCENT ALGORITHM FOR (1).

TABLE I

1) Pick b known entries with their indices.
2) Set up the completion subproblem by ﬁnding the indices cor-
responding to the submatrices Lb and Rb, which need to be
modiﬁed. Consequently, ﬁnd the subset Ωb of indices out of the
total bLbR indices.

3) Compute the residual Sb = PΩb (LbRT
4) Given a step-size t, update Lb and Rb as

b − X⋆

b ).

Lb+ = Lb

−tSbRb(

Rb+ = Rb

−tST
b

Lb(

bµ

max(m,n) (RT R) + (1 − µ)(RT

b

Rb))−1

bµ

max(m,n) (LT L) + (1 − µ)(LT

b

Lb))−1.

5) Update LT L and RT R.
6) Repeat.

model and bL and bR are the rows of L and R that are up-
dated. The computational cost is comparable to those of [8],
[12], [16] for r ≪ min(n, m). Our numerical comparisons
in Section III suggest that the proposed scaled variant of
SGD competes favorably with state-of-the-art algorithms on
a number of different benchmarks, especially outperforming
others on ill-conditioned and scarcely sampled data.

II. SCALED STOCHASTIC GRADIENT DESCENT

Given a matrix X of size n × m and rank r, it admits the

factorization

X = LRT ,

(2)

are full column-rank
where L ∈ Rn×r
matrices [25]. Consequently, the problem (1) boils down to

and R ∈ Rm×r

∗

∗

min

n×r
∗

,R∈R

1

2 kPΩ(LRT ) − PΩ(X⋆)k2
F .

(3)

m×r
∗

L∈R

Consider a stochastic gradient setup for solving (3), where
we pick b known entries at a time and then take a gradient
descent step that updates the matrices L and R. Due to the
cost function structure, we end up updating only a maximum
of b rows of L and R at a time. Let bL rows of L and bR rows
of R be updated when b known entries are picked, where
bL ≤ b and bR ≤ b. Let Lb be the corresponding submatrix
of L with the bL rows, i.e., its size is bL × r. Similarly, let
Rb be the submatrix of R with the bR rows and size bR × r.
An interpretation is that, each time we pick b known entries,
we have a completion subproblem of a matrix X⋆
b of size
bL × bR with b known entries at indices Ωb, which needs
to be approximated by LbRT
b . If Sb is the residual matrix
of this subproblem, then the partial derivatives at (Lb, Rb)
are (SbRb, ST
b ) is of size
b
bL × bR.

Lb), where Sb = PΩb (LbRT

b − X⋆

The proposed stochastic gradient descent updates are

Lb+ = Lb − tSbRb(
Rb+ = Rb − tST
Lb(
b

bµ

max(m,n) (RT R) + (1 − µ)(RT
max(m,n) (LT L) + (1 − µ)(LT

bµ

b

b

Rb))−1
Lb))−1,

(4)

where t is the step size, b/ max(m, n) is a normalization
constant, and µ is a nonnegative scalar in [0, 1] that weighs
LT L and LT
Lb differently. Similarly, the terms RT R and
b
Rb. The step-size t can be modiﬁed, e.g., using the bold
RT
b
driver protocol [13], [26] or the exponential decay protocol
[12, Section 4.1]. The choice of µ depends on the problem.
It should be noted that the terms LT L and RT R capture a
part of second order information of kLRT − X⋆k2
F /2, which
is related to the cost function of (3) and gives a simpler
way to understand the behavior of the cost function. In fact,
kLRT − X⋆k2
F /2 is obtained by assuming that Ω is the
full set of indices in (3). This relies on strict convexity of
kLRT − X⋆k2
F /2 with the factors L and R individually. For
example, the same is motivated in the works of [6], [11],
[22], where the authors use the terms LT L and RT R to
accelerate convergence of algorithms (e.g., steepest descent,
conjugate gradients, and trust-regions) by scaling the partial
derivatives with (LT L)−1 and (RT R)−1.

Lb and RT
b

In our case, an additional motivation for a trade-off with
µ in (4), i.e., between the terms LT L and RT R on one hand
Rb on the other, comes from the following
and LT
b
intuition. LT L and RT R can be interpreted as capturing the
global second order information as they contain knowledge
of all the rows of L and R [6], [11], [22]. On the other
hand in a stochastic setup we modify only Lb rows of L
and Rb rows of R. This leads to the argument that LT
Lb
b
in the term LT L should be given a higher weight. Similarly,
the part RT
Rb in RT R is given a differentiated weight.
b
Overall the terms (
Rb))−1 and
Lb))−1 which are multiplied
max(m,n) (LT L) + (1 − µ)(LT
(
Lb, respectively, act
to the partial derivatives SbRb and ST
b
as an efﬁcient preconditioner for the standard stochastic gra-
dient descent updates proposed in [12], [13]. The complete
algorithm is shown in Table I.

max(m,n) (RT R)+(1−µ)(RT

bµ

bµ

b

b

A. Computation cost

In Table I, Step 2 costs O(blogb) to identify the indices of
L and R that need to be modiﬁed and costs O(b) to ﬁnd the
subset Ωb. It involves sorting the row entries corresponding
to the b known entries to ﬁnd the unique rows of L and R that
are required to be updated. Computation of the residual Sb
costs O(b) in Step 3. The updates in Step 4 costs O(r3 +br).
Step 5 costs O(bLr2 + bRr2). Consequently, each time we
pick any b entries, our proposed gradient descent step costs
O(bLr2 +bRr2 +br+b+blogb+r3), where bL ≤ b and bR ≤
b. Equivalently, our algorithm costs O(|Ω|(r3/b + bLr2/b +
bRr2/b + r + logb)) after we have seen |Ω| entries.

b

bµ

Depending on b,

max(m,n) (RT R)+ (1 − µ)(RT

the computational cost varies from
O(|Ω|r3) to O(|Ω|r2). For b = 1, the inverse computation
Rb)) costs O(r2) as it re-
of (
quires only a rank-1 modiﬁcation per update. Consequently,
our algorithm costs O(|Ω|r2) for b = 1. For b ∈ (1, r),
the computation cost is upper bounded by O(|Ω|r3) for a
straightforward implementation of matrix inversion in (4).
For b > r, the computational cost is O(|Ω|r2). In particular
if b = |Ω|, then bL = n, bR = m, and the computational cost
is O(|Ω|r + nr2 + mr2), which is same as the computational

cost (per iteration) of most algorithms in the batch setup,
e.g., the ones proposed in [2], [6], [9]–[11], [18], [22].

B. Scale invariance

The proposed updates (4) also resolve the issue of scale
invariance arising from non-uniqueness of matrix factoriza-
tion (2) as X remains unchanged under the action [21], [25]

(L, R) 7→ (LM−1, RMT ),

(5)

for all non-singular matrices M ∈ GL(r), where GL(r) is
the set of all non-singular matrices of size r×r. Equivalently,
X = LRT = LM−1(RMT )T . The issue of scale invariance
refers to the behavior of algorithms which should behave
equivalently when initialized, say, either with (L0, R0) or
with (L0M−1, R0MT ) for all non-singular matrices M ∈
GL(r). In other words, if (L+, R+) is an update obtained
from (L0, R0),
then a scale-invariant algorithm should
also lead to the update (L+M−1, R+MT ) starting from
(L0M−1, R0MT ). It is the case for the algorithm proposed
in Table I. For example, it is straightforward to show that
under the mapping (5), the proposed updates (4) lead to the
MT ). On the
transformation (Lb+, Rb+) 7→ (Lb+
other hand, the standard stochastic gradient descent updates
[7], [13],

M−1, Rb+

Lb+ = Lb − tSbRb
Rb+ = Rb − tST
b

Lb,

(6)

are not scale invariant. Here Lb and Rb are the submatrices
of L and R, respectively that are updated when b known
entries are chosen, t is the step-size, and Sb is the residual
matrix corresponding to Lb and Rb.

The beneﬁts of scale invariant algorithms are discussed in
[21], [23], [27]. We show a practical example to this end in
Section III-B.

C. Choice of µ

A key observation is that the updates (4) with µ = 1 are
equivalent to the stochastic version of the updates proposed
in [9], [18], [22]. For µ < 1, we take additional On the other
hand, µ = 0 gives full weighting to the “local” second order
information and should be used when LT
Rb are
b
positive deﬁnite. This holds true for bL ≥ r and bR ≥ r,
i.e., for a large enough batch size. For a smaller batch size,
i.e., bL < r or bR < r, a non-zero µ should be used.

Lb and RT
b

In problem instances where a large number of entries are
already known, i.e., |Ω| is large, the inﬂuence of µ < 1 is
minimal. However, for ill-conditioned data, making use of
local information is more critical, and a smaller value of µ is
more appropriate, e.g., µ = 0.5. These trade-offs are shown
in Section III-C.

D. Choice of batch size b

For b = |Ω|, the algorithm in Table I behaves like a batch
gradient descent algorithm and with same computational cost
as discussed in Section II-A. The choice of b = 1 is more
appropriate for a fully online system. Other choices depend
on the problem size and set up. Section III-D shows the
robust behavior of the algorithm with different choices of b.

E. Convergence

The convergence analysis of the proposed algorithm in
Table I
follows the discussion in [12], [13], [28] ex-
cept for the (positive deﬁnite) matrix-scaling of the partial
Lb) at (Lb, Rb) by multiplying with
derivatives (SbRb, ST
b
max(m,n) (LT L)+
((
(1 − µ)(LT
b

max(m,n) (RT R)+(1−µ)(RT

Rb))−1, (

Lb))−1).

An equivalent interpretation is that we endow the search

bµ

bµ

b

space Rn×r × Rm×r with the adaptive inner product

hξx, ηxiadaptive =

bµ

b

b

bµ

Rb))ξT
Lb))ξT

Trace((
+Trace((

max(m,n) (RT R) + (1 − µ)(RT
max(m,n) (LT L) + (1 − µ)(LT

L ηL)
RηR),
(7)
which depends on (L, R) and (Lb, Rb). Here x has the
matrix representation (L, R) and ξx and ηx vectors in
Rn×r × Rm×r with matrix representations (ξL, ξR) and
(ηL, ηR), respectively. hξx, ηxiadaptive is the inner product
between ξx and ηx and Trace(·) is the matrix trace operator.
Finally, computing the steepest descent directions with the
inner product (7) leads to the updates (4).

The adaptive inner product (7) is interpreted as a Rieman-
nian metric in Rn×r ×Rm×r [27]. Consequently, the analysis
of a Riemannian stochastic gradient descent algorithm devel-
oped in [29] is applicable to our case, e.g., under a decaying
step-size condition.

Apart from the asymptotic convergence guarantees, the
theoretical analysis of the rate of convergence of the stochas-
tic gradient descent algorithm in Table I is a challenging
task and remains an open problem. However, we point to
the recent work [30] that sheds light on a similar problem.

III. NUMERICAL EXPERIMENTS

In this section, we compare the following algorithms.
1) Scaled-SGD:

the proposed scaled stochastic gra-
dient descent algorithm. The computation cost
is
O(|Ω|(r3/b + bLr2/b + bRr2/b + r + logb)) after we
have seen all the |Ω| entries with b entries at a time.
2) SGD: the standard SGD implementation, where the
updates result by imposing the Euclidean metric [7],
[13]. The computational cost is O(|Ω|(r + logb)) when
we sweep through |Ω| entries with b entries at a time.
It should be stated that the cost is independent of b as
the effect of logb is minimal.

3) Grouse: the algorithm proposed in [15]. Instead of
learning L and R simultaneously, it learns, e.g., the
rank-r left subspace spanned by L ﬁrst by traversing
through the columns of the incomplete matrix X⋆.
The R factor is computed by solving a least-squares
problem in closed form once we have learned the
subspace spanned by L. The learning of L boils down
to an optimization problem on the Grassmann man-
ifold. Consequently, Grouse is a stochastic gradient
descent algorithm on the Grassmann manifold. Its
computational cost is O(mnr) after sweeping through
m columns, i.e., one pass through X⋆.

4) Loreta: the algorithm proposed in [14], but modiﬁed
i.e., we sweep through
to handle data as in [15],
the incomplete matrix X⋆ column by column. The
computational cost is O(mnr) after one pass through
X⋆.

5) ALS: the standard alternating least-squares algorithm,
where we update the low-rank factors L and R row-by-
row by solving the least-squares subproblems in closed
form [16]. The computational cost is O(|Ω|r2 + (n +
m)r3) per update all the rows of L and R.

6) CCD++: the algorithm proposed in [8], which learns
r rank-1 factors sequentially. For learning a rank-
1 factor, it uses T inner iterations of the algorithm
proposed in [17]. Its computational cost is O(|Ω|rT)
after one update of L and R. As suggested in [8], T
is set to 5.

Out of numerous algorithms that exist for matrix com-
pletion, the choice of the above algorithms is motivated
by the fact that these algorithms can be readily adapted
to an online setup or in situations where the dimensions,
e.g., the number of rows, vary with time. Furthermore, since
the mentioned algorithms are well suited for different sce-
narios and have implementations in different programming
languages, we use only their Matlab implementations (which
we implement for all except Loreta and Grouse) and compare
them on the behavior of the cost function against iterations.
An iteration for Scaled-SGD, SGD, Grouse, and Loreta
corresponds to one pass through |Ω| entries. For ALS and
CCD++, an iteration corresponds to one update of the low-
rank factors L and R. The Matlab codes are available at
http://bamdevmishra.com/codes/scaledSGD/.

A. Experimentation setup and stopping criteria

All simulations are performed in Matlab and on a 2.7 GHz
Intel Core i5 machine with 8 GB of RAM. For each example,
an n×m random matrix of rank r is generated as in [4]. Two
matrices A ∈ Rn×r and B ∈ Rm×r are generated according
to a Gaussian distribution with zero mean and unit standard
deviation. The matrix product ABT gives a random rank-
r matrix. A fraction of the entries are randomly removed
with uniform probability. The dimensions of n × m matrices
of rank r is (n + m − r)r. The over-sampling (OS) ratio
determines the number of entries that are known. An OS
of 6 implies that 6(n + m − r)r number of randomly and
uniformly selected entries are known a priori out of the total
nm entries. No regularization is used.

During each iteration, the step-size t is ﬁxed for Scaled-
SGD, SGD, and Loreta. The step-size is then updated accord-
ing to the bold driver heuristic as suggested in [13]. In the
bold driver protocol [26], updating of the steps-size depends
on the cost function, i.e., kPΩ(X) − PΩ(X⋆)k2
F /|Ω|. In case
the cost increases after an iteration, the step-size is reduced
by 50%, else the step-size is increased by 10%. The initial
step-size is computed using the approach used in [22]. For
Grouse, we use the step-size update proposed in [15]. ALS
and CCD++ do not require any step-size tuning. Additionally,
during each iteration, the known entries are randomly chosen

for Scaled-SGD and SGD with uniform probability and
without replacement. Equivalently, each known entry is seen
only once per iteration. Similarly, during an iteration, the
columns (rows) are randomly and uniformly chosen for
Grouse and Loreta (ALS).

The algorithms are stopped when either the mean square
F /|Ω| is less than 10−8 or the
error kPΩ(X) − PΩ(X⋆)k2
relative residual kPΩ(X) − PΩ(X⋆)kF /kPΩ(X⋆)kF is less
than 10−4 or the number of iterations exceeds 100.

B. Effect of scale invariance

bµ

b

max(m,n) (RT R) + (1 − µ)(RT

The difference of the standard stochastic updates (6)
with respect to the proposed updates (4) is in the r × r
Rb)), that are
matrices, e.g., (
inversely applied to (6). However, those extra (but minimal)
computations make the proposed updates invariant to the
transformation (5), which is not the case with the Euclidean
updates (6). To illustrate this effect of scale invariance, we
consider a low-rank matrix completion instance with n =
m = 100, r = 5, and OS = 8. Both Scaled-SGD and SGD
are run with batch size b = 10 and L and R are randomly
initialized such that kLinitkF ≈ kRinitkF . Additionally, we
set µ to 0.5. The performance of both the algorithms is
similar. However, the performance of SGD suffers drastically
when kLinitkF ≈ 4kRinitkF as shown in Figure 1(a).

C. Effect of µ

We consider problem instances of size 5000 × 5000 and
rank 10. In order to understand the inﬂuence of µ on
Scaled-SGD, we consider two scenarios. The ﬁrst scenario
consists of an instance with over-sampling ratio 3. The
second scenario considers an over-sampling ratio of 3 and
ill-conditioned data with condition numbers (CN) 50, which
is obtained by imposing an exponential decay of singular
values (discussed in Section III-G). Figures 1(b) and 1(c)
show the behavior of Scaled-SGD with four different values
of µ. Scaled-SGD is run with batch size b = 10. Figures
1(b) shows that there exists values of µ, which show better
performance. Figure 1(c) shows that relying solely of global
information, i.e., µ = 1, need not be better. In particular,
Scaled-SGD with µ = 1 diverges in 1(c). µ = 0.5, on the
other hand, shows a good performance in many instances.

D. Effect of batch size b

We consider problem instances of size 5000 × 5000 of
rank 5. In order to understand the inﬂuence of b on Scaled-
SGD, we consider two scenarios with b = {1, r, 2r, r2}. The
ﬁrst scenario consists of instance with an over-sampling ratio
of 5. The second scenario consists of ill-conditioned data
with condition number CN equal to 500. µ is set to 0.5 in
Scaled-SGD. Figures 1(d) and 1(e) show the robust behavior
of Scaled-SGD with different batch sizes b.

E. Low-sampling instances

We consider problem instances of size 5000 × 5000 and
rank 10. Different over-sampling ratios of 4, 3, 2.5, and 2.1
are considered. Scaled-SGD and SGD are run with batch

102

100

10-2

10-4

10-6

10-8

r
o
r
r
e

 

e
r
a
u
q
s
 

n
a
e
M

10-10

0

102

100

10-2

10-4

10-6

r
o
r
r
e

 

e
r
a
u
q
s
 

n
a
e
M

10-8

0

Scaled-SGD
SGD

5

10

15

Iterations

20

25

30

102

100

10-2

10-4

10-6

r
o
r
r
e

 

e
r
a
u
q
s
 

n
a
e
M

10-8

0

µ = 0.1
µ = 0.5
µ = 0.9
µ = 1

5

10

15

Iterations

20

25

30

102

100

10-2

10-4

10-6

r
o
r
r
e

 

e
r
a
u
q
s
 

n
a
e
M

10-8

0

µ = 0.1
µ = 0.5
µ = 0.9
µ = 1

5

10

15

Iterations

20

25

30

(a) Scale invariance.

(b) Effect of µ on well-conditioned data.

(c) Effect of µ on data with CN = 50.

b = 1
b = r
b = 2r
b = r2

102

100

10-2

10-4

10-6

r
o
r
r
e

 

e
r
a
u
q
s
 

n
a
e
M

b = 1
b = r
b = 2r
b = r2

102

100

10-2

10-4

10-6

r
o
r
r
e

 

e
r
a
u
q
s
 

n
a
e
M

5

10

Iterations

15

20

10-8

0

2

4
6
Iterations

8

10

10-8

0

20

Scaled-SGD
SGD
Grouse
Loreta
ALS
CCD++

80

100

40

60

Iterations

(d) Effect of b on well-conditioned data.

(e) Effect of b on data with CN = 500.

(f) Low-sampling with OS = 2.1.

t

e
s
 
t
s
e

t
 

n
o

 
r
o
r
r
e

 

e
r
a
u
q
s
 
s
n
a
e
M

102

100

10-2

10-4

10-6

0

20

Scaled-SGD
SGD
Grouse
Loreta
ALS
CCD++

102

100

10-2

10-4

10-6

r
o
r
r
e

 

e
r
a
u
q
s
 

n
a
e
M

80

100

10-8

0

20

40

60

Iterations

Scaled-SGD
SGD
Grouse
Loreta
ALS
CCD++

102

100

10-2

10-4

10-6

r
o
r
r
e

 

e
r
a
u
q
s
 

n
a
e
M

80

100

10-8

0

Scaled-SGD
SGD
Grouse
Loreta
ALS
CCD++

20

40

60

Iterations

80

100

120

40

60

Iterations

(g) Noisy instance.

(h) Ill-conditioned data with CN = 100.

(i) Rectangular instance.

Fig. 1. Scaled-SGD resolves scale-invariance in matrix factorization. Furthermore, it is robust to various choices of µ and b. In particular, a good trade-off
between global and local information, i.e., for µ ∈ (0, 1), leads to improved performance in both well-conditioned and ill-conditioned data. Scaled-SGD
outperforms others in lower sampling instances, i.e., with small OS values and ill-conditioned instances.

size b = 10. Additionally, we set µ to 0.5 in Scaled-SGD.
While most algorithms perform well for larger OS values,
Scaled-SGD particularly outperforms others for smaller OS
values as shown in Figure 1(f).

F. Noisy instances

We consider the problem instance in Section III-E with
OS = 3. Additionally, noise is added to the known entries.
As proposed in [15], noise for each entry is sampled from the
Gaussian distribution with mean zero and standard deviations
10−4. Figure 1(g) shows the performance of the algorithms
on the test set that is held out, which is different from the
training set Ω.

G. Ill-conditioned instances

We consider matrices of size 5000 × 5000 of rank 10 and
impose an exponential decay of singular values. The ratio

of the largest to the lowest singular value is known as the
condition number (CN) of the matrix. For example, at rank
10 the singular values with condition number 100 is obtained
using the Matlab function logspace(-2,0,10). The
over-sampling ratio for these instances is 3. µ is set to
0.5. Figure 1(h) shows the performance of various algo-
rithms, where our proposed approach outperforms others for
CN 100. Scaled-SGD shows a robust performance on ill-
conditioned instances.

H. Rectangular instances

We consider rectangular matrices of size 1000 × 8000 of
rank 10 and over sampling ratio 3. µ is set to 0.5. Most
comparisons suggest that ALS and Grouse perform very well
on those instances. However, even for slightly ill-conditioned
data, the performance of ALS and Grouse degrade as shown
in Figure 1(i). Scaled-SGD remains unaffected.

t

e
s
 

i

g
n
n
a
r
t
 

i

n
o

 
r
o
r
r
e

 

e
r
a
u
q
s
 

n
a
e
M

Scaled-SGD
SGD
Grouse
Loreta
ALS
CCD++

35

30

25

20

15

10

0

20

40

60

Iterations

80

100

Fig. 2. Performance of the algorithms on the Jester dataset
for n = 2000 and r = 7.

FINAL MEAN NMAES OBTAINED ON THE TEST SET OF THE JESTER DATASET.

TABLE II

Algorithm

n = 2000

n = 5000

n = 24983

Scaled-
SGD
SGD
Grouse
Loreta
ALS
CCD++

r = 5

0.158

r = 7

0.159

r = 5

0.160

r = 7

0.158

0.158

0.165

0.159

0.158

0.158

0.159

0.165

0.160

0.159

0.159

0.160

0.179

0.160

0.160

0.160

0.158

0.177

0.158

0.158

0.157

r = 5
0.159

0.159
0.168
0.159
0.159
0.159

r = 7
0.157

0.157
0.166
0.158
0.157
0.157

I. Jester dataset

We consider the Jester dataset 1 [31] consisting of ratings
of 100 jokes by 24983 users. Each rating is a real number
between −10 and 10. Following the protocol in [5], we
select n = {2000, 5000, 24983} users (randomly chosen
for the ﬁrst two cases). We randomly extract two ratings
per user as test data. The algorithms are run by ﬁxing
the rank to {5, 7} with random initialization and for 100
iterations. Predictions are computed at the end of 100th
iteration (to mimic a real-life scenario). The entire process
is repeated ten times. No regularization is used for Scaled-
SGD, SGD, Grouse, and Loreta. The performance of ALS
and CCD++, however, critically depends on regularization
for which we set
the regularization parameter to 10 for
n = {2000, 5000} and 100 for n = 24983; obtained after
cross-validation. The batch size b is set to r for Scaled-SGD
and SGD. Additionally, µ is set to 0.5 in Scaled-SGD. Figure
2 shows the performance plots. Table II shows the ﬁnal mean
normalized mean absolute errors (NMAE) obtained by the
different algorithms on the test dataset. NMAE is deﬁned
as the mean absolute error (MAE) divided by spread of
the ratings, i.e., the difference between the minimum and
maximum ratings. Since the ratings vary from −10 to 10,
NMAE is MAE/20. For each NMAE score shown in Table
II, the standard deviation is 2 · 10−3. Except Grouse, all
other algorithms give similar NMAE scores on the test set.
Scaled-SGD consistently performs better than SGD as shown
in Figure 2.

IV. CONCLUSION

We have proposed a scaled stochastic gradient descent
algorithm for the low-rank matrix completion problem. It
is based on a novel matrix-scaling of the partial derivatives
with terms that combine both local and global second order
information. We show that this scaling is computationally
cheap to implement and the proposed algorithm is poten-
tially scalable to larger datasets. Initial results, especially on
scarcely sampled and ill-conditioned datasets, show a robust
performance of the proposed algorithm. At the conceptual
level,
this paper shows the complementary role of local

and global second order information in a stochastic gradient
setting.

REFERENCES

[1] E. J. Cand`es and B. Recht, “Exact matrix completion via convex
optimization,” Foundations of Computational Mathematics, vol. 9,
no. 6, pp. 717–772, 2009.

[2] R. H. Keshavan, A. Montanari, and S. Oh, “Matrix completion from a
few entries,” IEEE Transactions on Information Theory, vol. 56, no. 6,
pp. 2980–2998, 2010.

[3] K. Wei, J.-F. Cai, T. F. Chan, and S. Leung, “Guarantees of Rieman-
nian optimization for low rank matrix recovery,” arXiv:1511.01562,
Tech. Rep., 2015.

[4] J. F. Cai, E. J. Cand`es, and Z. Shen, “A singular value thresholding
algorithm for matrix completion,” SIAM Journal on Optimization,
vol. 20, no. 4, pp. 1956–1982, 2010.

[5] R. H. Keshavan, A. Montanari, and S. Oh, “Low-rank matrix comple-
tion with noisy observations: a quantitative comparison,” in Annual
Allerton Conference on Communication, Control, and Computing
(Allerton), 2009, pp. 1216–1222.

[6] N. Boumal and P.-A. Absil, “Low-rank matrix completion via precon-
ditioned optimization on the Grassmann manifold,” Linear Algebra
and its Applications, vol. 475, pp. 200–239, 2015.

[7] B. Recht, M. Fazel, and P. A. Parrilo, “Guaranteed minimum-rank
solutions of linear matrix equations via nuclear norm minimization,”
SIAM Review, vol. 53, no. 3, pp. 471–501, 2010.

[8] H.-F. Yu, C.-J. Hsieh, S. Si, and I. S. Dhillon, “Parallel matrix
factorization for recommender systems,” Knowledge and Information
Systems, vol. 41, no. 3, pp. 793–819, 2014.

[9] J. Tanner and K. Wei, “Low rank matrix completion by alternating
steepest descent methods,” Applied and Computational Harmonic
Analysis, 2015, doi: http://dx.doi.org/10.1016/j.acha.2015.08.003.

[10] B. Vandereycken, “Low-rank matrix completion by Riemannian opti-
mization,” SIAM Journal on Optimization, vol. 23, no. 2, pp. 1214–
1236, 2013.

[11] T. T. Ngo and Y. Saad, “Scaled gradients on Grassmann manifolds for
matrix completion,” in Advances in Neural Information Processing
Systems 25 (NIPS), 2012, pp. 1421–1429.

[12] B. Recht and C. R´e, “Parallel stochastic gradient algorithms for large-
scale matrix completion,” Mathematical Programming Computation,
vol. 5, no. 2, pp. 201–226, 2013.

[13] C. Teﬂioudi, F. Makari, and R. Gemulla, “Distributed matrix comple-
tion,” in International Conference on Data Mining (ICDM), 2012, pp.
655–664.

[14] U. Shalit, D. Weinshall, and G. Chechik, “Online learning in the
manifold of low-rank matrices,” in Neural Information Processing
Systems conference (NIPS), 2010, pp. 2128–2136.

[15] L. Balzano, R. Nowak, and B. Recht, “Online identiﬁcation and
tracking of subspaces from highly incomplete information,” in The
48th Annual Allerton Conference on Communication, Control, and
Computing (Allerton), June 2010, pp. 704–711.

[16] Y. Zhou, D. Wilkinson, R. Schreiber, and R. Pan, “Large-scale
parallel collaborative ﬁltering for the Netﬂix prize,” in International
Conference on Algorithmic Aspects in Information and Management
(AAIM), 2008, pp. 337–348.

[17] I. Pil´aszy, D. Zibriczky, and D. Tikk, “Fast als-based matrix factoriza-
tion for explicit and implicit feedback datasets,” in ACM conference
on Recommender systems (RecSys), 2010, pp. 71–78.

[18] Z. Wen, W. Yin, and Y. Zhang, “Solving a low-rank factorization
model for matrix completion by a nonlinear successive over-relaxation
algorithm,” Mathematical Programming Computation, vol. 4, no. 4,
pp. 333–361, 2012.

[19] I. Markovsky and K. Usevich, “Structured low-rank approximation
with missing data,” SIAM Journal on Matrix Analysis and Applica-
tions, vol. 34, no. 2, pp. 814–830, 2013.

[20] Y. Shi, Y. Zhang, Letaif, and K. B. and, “Low-rank matrix completion
interference management by Riemannian pursuit,”

for topological
arXiv preprint arXiv:1603.01729, Tech. Rep., 2016.

[21] G. Meyer, S. Bonnabel, and R. Sepulchre, “Linear regression under
ﬁxed-rank constraints: a Riemannian approach,” in Proceedings of the
28th International Conference on Machine Learning (ICML), 2011,
pp. 545–552.

[22] B. Mishra, K. Adithya Apuroop, and R. Sepulchre, “A Riemannian
geometry for low-rank matrix completion,” arXiv:1211.1550, Tech.
Rep., 2012.

[23] B. Mishra and R. Sepulchre, “Riemannian preconditioning,” SIAM

Journal on Optimization, vol. 26, no. 1, pp. 635–660, 2014.

[24] N. Boumal, B. Mishra, P.-A. Absil, and R. Sepulchre, “Manopt: a
Matlab toolbox for optimization on manifolds,” Journal of Machine
Learning Research, vol. 15, no. Apr, pp. 1455–1459, 2014.

[25] R. Piziak and P. L. Odell, “Full rank factorization of matrices,”

Mathematics Magazine, vol. 72, no. 3, pp. 193–201, June 1999.

[26] R. Battiti, “Accelerated backpropagation learning: two optimization

methods,” Complex Systems, vol. 3, no. 4, pp. 331–342, 1989.

[27] P.-A. Absil, R. Mahony, and R. Sepulchre, Optimization Algorithms on
Matrix Manifolds. Princeton, NJ: Princeton University Press, 2008.
[28] L. Bottou, “Online algorithms and stochastic approximations,” in
Online Learning in Neural Networks, D. Saad, Ed. Cambridge, UK:
Cambridge University Press, 1998, pp. 9–42.

[29] S. Bonnabel, “Stochastic gradient descent on Riemannian manifolds,”
IEEE Transactions on Automatic Control, vol. 58, no. 9, pp. 2217–
2229, 2013.

[30] C. D. Sa, K. Olukotun, and C. R´e, “Global convergence of stochastic
gradient descent for some non-convex matrix problems,” in Interna-
tional Conference on Machine learning (ICML), 2015, pp. 2332–2341.
[31] K. Goldberg, T. Roeder, D. Gupta, and C. Perkins, “Eigentaste: a
constant time collaborative ﬁltering algorithm,” Information Retrieval,
vol. 4, no. 2, pp. 133–151, 2001.

