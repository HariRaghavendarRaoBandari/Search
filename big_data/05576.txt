Extracting Wyner’s Common Randomness Using

Polar Codes

Ling Liu, Jinwen Shi, and Cong Ling

Department of Electronic and Electrical Engineering

Imperial College London, UK

{l.liu12,jinwen.shi12,c.ling}@imperial.ac.uk

6
1
0
2

 
r
a

 

M
2
2

 
 
]
T
I
.
s
c
[
 
 

3
v
6
7
5
5
0

.

3
0
6
1
:
v
i
X
r
a

Abstract—Explicit construction of polar codes for the Gray-
Wyner network is studied. We show that Wyner’s common
information plays an essential role in constructing polar codes for
both lossless and lossy Gray-Wyner coding problems. For discrete
joint sources, extracting Wyner’s common randomness can be
viewed as a lossy compression problem, which is accomplished
by extending polar coding for a single source to a pair of
joint sources with doubled alphabet size. We show that the
lossless Gray-Wyner region can be achieved by an upgraded or
degraded version of the common randomness. For joint Gaussian
sources, we prove that extracting Wyner’s common randomness
is equivalent to lossy compression for a single Gaussian source,
meaning that Wyner’s common randomness can be extracted
by utilizing polar lattice for quantization and represented by a
lattice Gaussian random variable.

I. INTRODUCTION

In this paper we shall be concerned with extracting the com-
mon information contained in a pair of joint sources (X, Y ).
There are different ways to characterize the amount of com-
mon information in literature. Apart from Shannon’s mutual
information [1] and Gács and Körner’s common randomness
[2], Wyner proposed an alternative deﬁnition to quantify the
common information for (X, Y ) with ﬁnite alphabet [3] as
C(X, Y ) = inf X−W−Y I(X, Y ; W ), where the inﬁmum is
taken over all W , such that X − W − Y forms a Markov
chain.

1 , Y N

Wyner’s deﬁnition was originated from his earlier work
in the Gray-Wyner network [4] depicted in Fig. 1, which
demonstrates Wyner’s ﬁrst approach [3] to the explanation
of C(X, Y ). This network model contains an encoder
1 ) and outputs
that observes a pair of sequences (X N
three messages W0, W1 and W2 with rate R0, R1, R2
by observing
respectively. Decoder 1 reconstructs X N
1
from (W0, W2).
(W0, W1) and decoder 2 reconstructs Y N
1
Wyner also gave a second approach to explain the common
information. In that model a common message W is sent
to two independent processors. The processors generate
output
separately according to distributions
PX|W (x|w)
sequences
and ˆY N
ˆX N
1 , ˆyN
1 ) =
1
1
showed
that C(X, Y ) is equal to the minimum rate on the shared
message, with constrains that the sum of rates equals the
joint entropy or that the joint distribution P ˆX N
1 , ˆyN
1 )
is arbitrarily close to PX N

and PY |W (y|w). The
have joint probability P ˆX N
1 |W (ˆxN

w∈W 1|W| PX N

(ˆxN
1 |w). Wyner

ˆY N
1

1 |w)PY N

(xN

1 , yN

1 ).

1 |W (ˆyN

(ˆxN

ˆY N
1

1

output

1

sequences

(cid:80)

1 Y N
1

Fig. 1. Gray-Wyner source coding network.

Wyner and Gács-Körner’s works on common information
can be considered as two different viewpoints of the lossless
Gray-Wyner region. Their works were then expanded by [5],
[6] into the lossy case, where the output sequences ( ˆX N
1 , ˆY N
1 )
have certain distortion.

Polar codes ﬁrstly proposed by Arikan in [7] have become
widely studied due to their achievability of the Shannon
bound with low complexity. Polar codes are also used for
both lossy and lossless compression. For discrete sources, [8]
provides constructions using polar codes for lossless and lossy
compression. [9] gives a solution to the lossy compression
for nonuniform sources. For memoryless Gaussian sources,
[10] proposed a polar lattice construction to achieve the rate-
distortion bound.

The use of polar codes for the common randomness (i.e.
G point in Fig. 3) was recently proposed in [11]. This paper
discussed polarization from the perspective of the maximal
correlation of two discrete sources. Furthermore, it proved
that polar codes are optimal
to extract Wyner’s common
randomness of discrete sources. However, we will investigate
the entire best-known lossless Gray-Wyner region in [4], [3]
for discrete sources. In addition, an explicit construction based
on polar codes and polar lattice is given to achieve the lossy
Gray-Wyner region [6] for both discrete and Gaussian sources
when the distortion is small. We also give an explanation on
the results in [6, Theorem 1] that the common information
deﬁned in lossless Gray-Wyner coding remains the same in
lossy case when the distortion is small.

The paper is organized as follows: Section II presents the
background of lossy and lossless compression using polar
codes. The construction of polar codes for lossless Gray-
Wyner network is investigated in Section III. In Section IV, we
design polar codes and polar lattices for discrete sources and
Gaussian sources for the lossy Gray-Wyner network. Finally,
the paper is concluded in Section V.

Encoder(cid:4666)(cid:1850)(cid:3036)(cid:481)(cid:1851)(cid:3036)(cid:4667)Decoder 1Decoder 2(cid:3553)(cid:1850)(cid:3036)(cid:3553)(cid:1851)(cid:3036)(cid:1849)(cid:2869)(cid:1849)(cid:2868)(cid:1849)(cid:2870)Notations: All random variables (RVs) are denoted by
capital letters. Let PX denote the probability distribution of
a RV X taking value x in a set X . X N
1 denotes a series of
vectors (X1, ..., XN ). For a set I, I c denotes its complement,
and |I| represents its cardinality. XI denotes the subvector
{X i}i∈I. For an integer N, [N ] will be used to denote the
set of all integers from 1 to N. The information is measured
in bits and h(·) denotes the binary entropy function.
II. POLAR CODES FOR SOURCE CODING

A. Polar Codes for Lossless Source Coding[8]

Let X N
1

to be N i.i.d. drawings of a RV X which is a
Bernoulli source with crossover probability p (Ber(p)), for
N = 2n where an integer n ≥ 1. Apply the polarizing
2 , (cid:48)⊗(cid:48) denotes
transformation U N
the Kronecker product and G2 = [ 1 0
1 1 ]. Fix R > H(X)
and let F denote the frozen set such that |F| = (cid:100)N R(cid:101) and
) for all i ∈ F and j /∈ F. We
H(Ui|U i−1
denote the information set by I = F c.

1 GN where GN = G⊗n

) ≥ H(Uj|U j−1

1 = X N

1

1

1 = ˆuN

The Successive Cancellation (SC) encoder introduced in
[12] stores uF and computes ˆuI following the encoding rules
(1) explained in Section II-B, without considering the side
1 . If ˆui (cid:54)= ui for i ∈ I, an estimation error
information Y N
occurs and the index i needs to be announced to the decoder.
The set of error indices is denoted by T . The encoder outputs
(uF , T ). The decoder puts ˆui = ui for i ∈ F, then estimates
uI using the same rule to the SC encoder (1). If i ∈ T , the
decision is ﬂipped. Then the decoder outputs ˆxN
1 GN .
It has been shown in [12] that the error rate tends to zero for
any rate R > H(X).

Since the entropy H(Ui|U i−1

as Z(X|Y ) ≡ 2(cid:80)

y PY (y)(cid:112)PX|Y (0|y)PX|Y (1|y). Also

) is complicated to calcu-
the Bhattacharyya pa-
late when N becomes very large,
rameter
is often used. For source coding with side in-
formation, assume (X, Y ) ∈ {0, 1} × Y be a pair
of RVs. The Bhattacharyya parameter
is deﬁned
Z(X|Y ) and H(X|Y ) are related by [13, Proposition 2] which
indicates that H(X|Y ) is near 0 or 1 if and only if Z(X|Y ) is
near 0 or 1, respectively. Thus the parameters {H(Ui|U i−1
)}N
)}N
and {Z(Ui|U i−1
1
1 polarize simultaneously. Furthermore [9]
shows Z(Ui|U i−1
) is equal to the Bhattacharyya parameter of
a symmetric channel deﬁned in [7]. Therefore we can apply
the method of constructing polar codes for symmetric channels
[14] to asymmetric channels.

[13]

1

1

1

1

B. Polar Codes for Lossy Source Coding[9]

In this part, we discuss polar lossy source coding for
an nonuniform source. We model the source as a sequence
of i.i.d. realizations of a RV Y ∈ Y. Let X denote the
reconstruction space. Let the distortion function denote by
d : Y × X → R+. The rate-distortion function is given by
R(D) = minEXY[d(Y,X)]≤D I(X, Y ).
We assume an information set I ⊂ [N ] and a frozen
set I c. Similarly to lossless source coding, U N
is deﬁned
1 GN . The frozen set can be identiﬁed by
as U N
Z(Ui|U i−1
1 ) ≥ 1− 2−N β or Z(Ui|U i−1
) ≤ 2−N β for all

1 ≡ X N
, Y N

1

1

1

i ∈ I c. Then the information set satisﬁes |I| = N R, where R
is the encoding rate. From [9] such an information set exists if
R > I(X, Y ) = R(D), β < 1/2 and N is sufﬁciently large.
After the indexes for information set and frozen set are
from a given source

identiﬁed, the encoder determines uN
1
sequence yN

1 by
0, with probability PUi|U i−1
1, with probability PUi|U i−1

1

1

(0|ui−1
(1|ui−1

1

1

, yN
1 )
, yN
1 )

(1)

,Y N
1

,Y N
1

(cid:40)

ui =

(cid:40)

for i ∈ I and

1

1

1

1

1

, Y N

if Z(Ui|U i−1

1 ) ≥ 1 − 2−N β
), if Z(Ui|U i−1

¯ui,
arg maxu PUi|U i−1

1 , xN

1 = uN

(u|ui−1

) ≤ 2−N β

(cid:80)N

ui =
for i ∈ I c, where ¯ui is determined beforehand uniformly from
{0, 1}. Then the encoder sends uI to the decoder and the
decoder outputs the reconstructed sequence xN
1 GN .
Finally, from [9] the average distortion results in DN =
D + O(2−N β(cid:48)
) where β(cid:48) < β < 1/2. Notice that the above
construction is also applicable for symmetric sources, with
the index set {i : Z(Ui|U i−1
) ≤ 2−N β} = 0. Additionally the
rule (1) reduces to that of lossless coding in the absence of
1 .
yN
III. POLAR CODES FOR LOSSLESS GRAY-WYNER CODING
In this section, we use the doubly symmetric binary source
(DSBS) as an example to show that polar codes are able
to achieve the rate region of the Gray-Wyner network in
the lossless case. Consider the lossless coding model using
1 ) ≡
Wyner’s ﬁrst approach shown in Fig. 1. Deﬁne dN (yN
i=1 d(yi, xi), where d(x, y) denotes Hamming distance for
discrete RVs or Euclidean distance for continious RVs. Fol-
lowing that, we give a formal deﬁnition to this model.
Deﬁnition 1. An (N, M0, M1, M2) code is deﬁned as follows:
An encoder is a mapping fE : X N ×Y N → IM0 ×IM1 ×IM2,
where IMi = {0, 1, 2, . . . , Mi − 1} for i = 0, 1, 2. A decoder
: IM0 × IM1 → X N and
is a pair of mappings f (X)
D : IM0 ×IM2 → Y N . Let fE(X N
f (Y )
1 ) = (W0, W1, W2),
1 = f (Y )
then ˆX N
D (W0, W2).
The average distortion between the inputs and outputs are
1 ) and (cid:52)Y =
((cid:52)X ,(cid:52)Y ), where (cid:52)X = 1
N EdN (Y N
The achievable rate region based on (N, M0, M1, M2) code is
deﬁned as following.
Deﬁnition 2. For lossless coding, a triple (R0, R1, R2) is said
to be achievable if there exists an (N, M0, M1, M2) code with
Mi ≤ 2N (Ri+), i = 0, 1, 2 and (cid:52) = max((cid:52)X ,(cid:52)Y ) ≤ , for
arbitrary  > 0 and sufﬁciently large N. Denote R as the set
of achievable rate.
Theorem 3. ([4, Theorem 2]) If (R0,R1,R2) ∈ R, then R0 +
R1+R2 ≥ H(X, Y ), R0+R1 ≥ H(X) and R0+R2 ≥ H(Y ).
Let us consider a pair of DSBS (X, Y ) where X = Y =
{0, 1} and Y = X⊕Z, Z ∼ Ber(a0). In this case H(X, Y ) =

1 , Y N
D (W0, W1) and ˆY N

1 = f (X)

N EdN (X N

1 , ˆY N

1 ).

1 , ˆX N

D

1

achieves Wyner’s common information C(X, Y ) at point G
and (R0, R1, R2) = (1 + h(a0) − 2h(a1), h(a1), h(a1)). Next
we demonstrate how to achieve the Pangloss bound using polar
codes without time-sharing.

• Point A:

This point can be trivially achieved since H(X, Y ) = H(X)+
H(Y |X) = H(X) + H(Z) where X does not need to be
1 ⊕
compressed. Encoder sends xN
1 as introduced in Section II-A. Decoder 1 and 2 reconstruct
yN
1 and ˆzN
1 with error probability tends to zero when N is
ˆxN
sufﬁciently large. Then ˆyN
1 . Due to symmetry the
role of source X and Y can be exchanged.

1 and compresses zN

1 ⊕ ˆzN

1 = xN

1 = ˆxN

• Point G:

1 , Y N

Firstly we extract
the common information from source
1 ). Since R0 ≥ I(W ; XY ) = 1 + h(a0) − 2h(a1),
(X N
the encoder applies lossy compression to the joint sources
1 ) as introduced in Section II-B with reconstruction
(xN
1 , yN
lossy compression with
1 . Differently from traditional
wN
a single source,
the test channel for the joint sources is
PXY |W (xy|w) = PX|W (x|w)PY |W (y|w).

1 and xN

1 or
In this way the average distortion between wN
1 will tend to a1 simultaneously. Hence the lossy-
1 and yN
wN
compressed sequence uI is the common message and is sent
to both decoders, where |I| ≥ (1 + h(a0) − 2h(a1))N. Since
H(X|W ) = H(Y |W ) = h(a1), we apply lossless compres-
sion to source xN
1 as side information and
send the lossless compressed sequence privately to Decoder 1.
Symmetrically source Y is operated in the same way.

1 together with wN

At the decoder side, both decoders reconstruct wN

1 from uI
by the lossy decoding rule introduced in Section II-B. Then
decoder 1 receives the compressed sequence from its private
channel and derives a reconstructed sequence ˆzN
1 . Then the
source can be reconstructed as ˆxN
1 . Decoder 2 can
operate the same way. Therefore X and Y can be reconstructed
with error rate tends to zeros when N is sufﬁciently large.
Notice that a similar method was also given in [11].

1 ⊕wN

1 = ˆzN

• Points on dashed line AG:

On this line, the common branch carries more information than
point G. To show how much additional amount of information
to send over the common branch, we keep the relation that
X − W − Y is a Markov chain. However, if we move the
intermediate variable W closer to source (X, Y ), there will
be two new RVs (X(cid:48), Y (cid:48)) between the source and common
variable W correspondingly. Hence we assume the test channel
is a BSC(d1) between X (Y ) and X(cid:48) (Y (cid:48)), and a BSC(d2)
between W and (X(cid:48),Y (cid:48)) where 0 ≤ d1, d2 ≤ a1 as shown in
Fig. 2 (b). Therefore X − X(cid:48) − W − Y (cid:48) − Y forms a Markov
chain.
is
R0 ≥ I(X(cid:48)Y (cid:48); XY ) = I(X(cid:48)Y (cid:48)W ; XY ) = I(XY ; W ) +
I(X(cid:48); X|W ) + I(Y (cid:48); Y |W ) = 1 − 2h(d1) + h(a0). Instead of
extracting the common variable W for the common channel,
decoders can reconstruct (X(cid:48), Y (cid:48)) and retrieve more informa-
tion from the common channel because (X(cid:48), Y (cid:48)) is closer to

the common channel

In this case,

the rate for

Fig. 2. Test channel for (a) Wyner’s common information for DSBS source,
where a0 = a1 ∗ a1. (b) Line AG and lossy Gray-Wyner coding, where
a1 = D1 ∗ D2. (C) Line GB. (a ∗ b (cid:44) a(1 − b) + b(1 − a)).

Fig. 3.
Simulation performance of polar codes for Gray-Wyner lossless
coding when a0 = 0.11 and N = 216, 218, 220. The performance loss
between AG is due to the asymmetric source coding.

2 − 1

1 + h(a0), H(X) = H(Y ) = 1. It has been shown in [3]
that the common information of DSBS is C(X, Y ) = 1 +
2 (1−2a0)1/2. This model can
h(a0)−2h(a1) where a1 = 1
be considered a cascade of two Binary Symmetric Channels
(BSCs) with the same crossover probability a1. The cascaded
channel is equivalent to a single BSC(a0). It was shown in [3]
that the common information C(X, Y ) is achieved when W
is an intermediate RV as depicted in Fig. 2 (a).

A. Polar codes for Pangloss bound

satisfy(cid:80)2

Theorem 3 indicates a fact that the Gray-Wyner network
deﬁned in Deﬁnition 2 cannot perform better than the situation
where the receivers can collaborate. This situation refers to the
Pangloss plane [4] which constrains the triples (R0, R1, R2) to
i=0 Ri = H(X, Y ). The dashed line AG in Fig. 3 is
the Pangloss bound for the lossless Gray-Wyner coding prob-
lem. Point A refers to the case where only the common channel
is used. Therefore the problem is the same as joint compres-
sion for source (X, Y ) and (R0, R1, R2) = (H(X, Y ), 0, 0).
Point G refers to the case where R0 is the smallest rate
that achieves lossless compression for source (X, Y ) and
constrains the total rate equals H(X, Y ). In other words, R0

BSC((cid:1853)(cid:2869))(cid:1850)BSC((cid:1853)(cid:2869))(cid:1849)(cid:1851)(a)BSC((cid:1856)(cid:2870))(cid:1850)(cid:1849)(cid:1851)BSC((cid:1856)(cid:2869))(cid:1850)(cid:495)BSC((cid:1856)(cid:2870))(cid:1851)(cid:495)BSC((cid:1856)(cid:2869))(b)BSC((cid:1853)(cid:2869))(cid:1850)BSC((cid:1853)(cid:2869))(cid:1849)(cid:1851)BSC(ρ)(cid:1849)(cid:1314)(c)00.20.40.60.81R1(R2)00.20.40.60.811.21.41.6R0AchievableboundforG-WnetworkPolarcodesN=220PolarcodesN=218PolarcodesN=216AGB1 , yN

(X, Y ) than W . Then both sources can be losslessly recon-
structed by applying lossless compression to source (X, Y )
with side information (X(cid:48), Y (cid:48)). Therefore the rate of the pri-
vate channels is R1 = R2 ≥ H(X|X(cid:48)) = H(Y |Y (cid:48)) = h(d1).
Now we show a construction using polar codes that achieves
the rate bound. Similarly to G point, we can ﬁrstly retrieve
the common message wN
1 by applying lossy compression
to joint source sequences (xN
1 ). The compressed rate
approaches 1 + h(a0) − 2h(a1). Notice that Z1 = X ⊕ W is
a Ber(a1) source. Then the encoder applies lossy compression
to the nonuniform source xN
1 with distortion d1 as
introduced in Section II-B. We operate the same to source
Y . Thus the additional rate approaches 2(h(a1) − h(d1))
when N is sufﬁciently large. For the private channels, the
encoder ﬁrst reconstructs X(cid:48) and Y (cid:48). Then the encoder applies
lossless compression to source X ⊕ X(cid:48) and Y ⊕ Y (cid:48) with rate
approaching h(d1). It is known that polar codes are optimal for
lossy and lossless compression [9], [13]. Therefore the average
1 ) approaches zero when N is
distortion for source (X N
sufﬁciently large.
B. Polar codes for R0 < C(X, Y ) (Curve GB)

1 ⊕ wN

1 , Y N

In this part we explain how to achieve the dashed curve
GB in Fig. 3 using polar codes. From Theorem 3, the lower
boundary of R should lie above the lines R0+2R1 = 1+h(a0)
and R0 + R1 = 1. In the above section, we have explained
polar code constructions to achieve this lower boundary of
R called Pangloss bound when R0 ≥ C(X, Y ) (e.g. dashed
line AG in Fig. 3). However the lower boundary of R remains
unknown when R0 < C(X, Y ). To the best of our knowledge,
the tightest lower boundary was given in [4] for the case as
follows:
Consider a degradation applied on the common variable W ,
where W is considered the input to BSC(ρ) with output W (cid:48)
as shown in Fig. 2 (C). As a result (X, Y )− W − W (cid:48) forms a
Markov chain and a1 ≤ β = a1 ∗ ρ ≤ 1/2. From this Markov
chain, the transition probability reads

PXY |W (cid:48)(x, y|w(cid:48)) =

PW|W (cid:48)(w|w(cid:48))PX|W (x|w)PY |W (y|w).

(cid:88)

w∈W

(cid:40)

Then the triple (R0, R1, R2) ∈ R satisﬁes

R0 = I(W (cid:48); XY ) = (1 − a0)(1 − h( β−0.5a0
1−a0
R1 = R2 = H(X|W (cid:48)) = H(Y |W (cid:48)) = h(β).

(2)

(3)

))

As β ∈ [a1, 1

2 ], the family of rate triples can generate the
dashed curve GB in Fig. 3. Achieving the triple in (3) is
quite similar to the encoding and decoding for point G using
polar codes. Firstly we apply polar lossy compression to joint
1 ) with distortion β and derive reconstruction
sources (X N
W (cid:48). The test channel used in lossy compression is speciﬁed
in (2), which is the major difference from the construction
of point G. Then send the compressed sequence over the
common channel. After that, we apply lossless compression
to source X and Y with W (cid:48) as side information, and send the

1 , Y N

compressed sequences through private channels to decoder 1
and 2 accordingly. Alternatively we can derive the common
randomness W by lossy compression of (X, Y ). After that,
we apply symmetric lossy compression to W with distortion
ρ to obtain W (cid:48). Moreover it is trivial to achieve point B when
R1 = R2 = 1 and R0 = 0.

Together with the result from III-A, all points from point
A to B along the dashed line in Fig. 3 can be achieved by
polar codes. Moreover the above models can be extended to
achieving the lower boundary for the more general sources
mentioned in [15].

IV. POLAR CODES FOR LOSSY GRAY-WYNER CODING

A. Common Information Preserves in Small Distortion Region
In this section we show how to achieve the the common
information C((cid:52)1,(cid:52)2) which is deﬁned as the smallest com-
mon rate R0 such that the total rate meets the rate-distortion
bound. Based on Deﬁnition 1, we deﬁne the lossy Gray-Wyner
coding as follows:

all

The rate-distortion function for source (X, Y ) is
RXY ((cid:52)1,(cid:52)2) = min I(X(cid:48), Y (cid:48); X, Y ),
test

the minimum is

taken over

channels
and

such that Ed(X(cid:48), X) ≤ (cid:52)1

where
PX(cid:48)Y (cid:48)|XY (x(cid:48)y(cid:48)|xy)
Ed(Y (cid:48), Y ) ≤ (cid:52)2.
Deﬁnition 4. For any (cid:52)1,(cid:52)2 ≥ 0, a number R0 is said to
be ((cid:52)1 ,(cid:52)2 ) − achivable if for any ε > 0 we can ﬁnd a
sufﬁciently large N such that there exists a (N, M0, M1, M2)
2 log Mi ≤ RXY ((cid:52)1,(cid:52)2) + ε,
(cid:52)X ≤ (cid:52)1 +ε, and (cid:52)Y ≤ (cid:52)2 +ε. Then C((cid:52)1,(cid:52)2) is deﬁned
as the inﬁmum of all R0 that is ((cid:52)1,(cid:52)2)-achievable.

code with M0 ≤ 2N R0,(cid:80)2

i=0

1

we

that

provide

Following

operational
meaning to the
[6, Theorem 1]
that
C((cid:52)1,(cid:52)2) = C(X, Y ) in some neighborhood of
the
origin {((cid:52)1,(cid:52)2) : 0 ≤ (cid:52)1,(cid:52)2 ≤ γ}. Again we use DSBS
sources (X, Y ) as an example.

conclusion of

an

The relation between lossy and lossless Gray-Wyner coding
is not difﬁcult to ﬁnd if we recall the construction of the
line AG in Section III-A. We apply the same test channel
where X − X(cid:48) − W − Y (cid:48) − Y forms a Markov chain shown
in Fig. 2 (b). The rate of the shared branch in lossless
Gray-Wyner coding equals I(X(cid:48)Y (cid:48); XY ) = I(XY ; W ) +
I(X(cid:48); X|W )+I(Y (cid:48); Y |W ). Thus we send three sub-sequences
on this branch.
Notice that if we consider (X(cid:48), Y (cid:48)) as the output of the
decoders, the above rate is sufﬁcient for lossy Gray-Wyner
coding. In the lossy case, we only require to recover the
source (X, Y ) with distortions ((cid:52)1,(cid:52)2). So we consider the
intermediate RVs (X(cid:48), Y (cid:48)) as the reconstruction variables.
Again, as in Section III-A, we only consider the plane in
the (R0, R1, R2) space where R1 = R2 and (cid:52)1 = (cid:52)2 = (cid:52).
The encoder applies the same lossy compression as that to the
line AG to extract W and sends on the shared branch with
rate R0 ≥ I(XY ; W ). Then the encoder applies asymmetric
lossy compression to source X ⊕ W with distortion (cid:52) where

large. As a result the total rate(cid:80)2

a1 = d2 ∗ (cid:52). To achieve (cid:52)1 = (cid:52)2 = (cid:52), the additional rate
we should send is I(X(cid:48); X|W ) and I(Y (cid:48); Y |W ) over either
private channels or the shared channel. Then the distortion
between X(Y ) and X(cid:48)(Y (cid:48)) tends to (cid:52) when N is sufﬁciently
i=0 Ri → I(X(cid:48)Y (cid:48); XY ) =
1 + h(a0) − 2h((cid:52)) = RXY ((cid:52),(cid:52)), for 0 ≤ (cid:52) ≤ a1. This
indicates that the lossy Pangloss bound [4] can be achieved
when C((cid:52),(cid:52)) = C(X, Y ) as long as 0 ≤ (cid:52) ≤ a1.
B. Common Information for Joint Gaussian Sources

The common information was generalized to the joint
Gaussian sources in [6]. Let X, Y be two bivariate Gaus-

sian variables with zero mean and covariance matrix (cid:2) 1 ρ

(cid:3).

The common variable of (X, Y ) is described by a Gaussian
variable W with mean 0 and variance ρ such that

ρ 1

(cid:40)

X = W +
Y = W +

√
1 − ρN1
√
1 − ρN2

(4)

where N1 and N2 are standard Gaussian variables and
N1, N2, W are independent of each other. Clearly, the common
information is given by I(X, Y ; W ) = 1

However, extracting a continuous common message from
(X, Y ) is difﬁcult. We show that a discrete version of W
is also eligible to convey the common message of two joint
Gaussian variables according to Wyner’s second approach to
the characterization of common information [3].

An n-dimensional lattice is a discrete subgroup of Rn which

2 log 1+ρ
1−ρ.

can be described by

Λ = {λ = Bz : z ∈ Zn},

where B the full rank generator matrix. For σ > 0 and c ∈ Rn,
the Gaussian distribution of variance σ2 centered at c is deﬁned
as

fσ,c(x) =

1
2πσ)n
Let fσ,0(x) = fσ(x) for short.

(

√

The Λ-periodic function is deﬁned as

− (cid:107)x−c(cid:107)2
2σ2

e

, x ∈ Rn.

(cid:88)

λ∈Λ

fσ,Λ(x) =

fσ,λ(x) =

√
(

1
2πσ)n

(cid:88)

λ∈Λ

− (cid:107)x−λ(cid:107)2

2σ2

e

.

We note that fσ,Λ(x) is a probability density function (PDF)
if x is restricted to the fundamental region R(Λ). It is actually
the PDF of the Λ-aliased Gaussian noise [16].

The ﬂatness factor of a lattice Λ is deﬁned as [16]

Λ(σ) (cid:44) max
x∈R(Λ)

|V (Λ)fσ,Λ(x) − 1|,

where V (Λ) = |det(B)| denotes the volume of a fundamental
region of Λ. It can be interpreted as the maximum variation
of fσ,Λ(x) with respect to the uniform distribution over a
fundamental region of Λ.
at c as the discrete distribution taking values in λ ∈ Λ as

We deﬁne the discrete Gaussian distribution over Λ centered

DΛ,σ,c(λ) =

fσ,c(λ)
fσ,c(Λ)

, ∀λ ∈ Λ,

where fσ,c(Λ) = (cid:80)

λ∈Λ fσ,c(λ). For convenience, we write
DΛ,σ = DΛ,σ,0. It has been shown in [17] that lattice Gaus-
sian distribution preserves many properties of the continuous
Gaussian distribution when the ﬂatness factor is negligible. To
keep the notations simple, we always set c = 0 and n = 1
(one-dimensional lattice Λ) in this work.
Lemma 5. Let ¯W be a RV which follows a discrete Gaussian
ρ. Consider two continuous variables ¯X and
distribution DΛ,
¯Y

√

(cid:40) ¯X = ¯W +

¯Y = ¯W +

√
1 − ρN1
√
1 − ρN2

where N1 and N2 are the same as that given in (4). Let
f ¯X, ¯Y (x, y) and fX,Y (x, y) denote the joint PDF of ( ¯X, ¯Y )
and (X, Y ), respectively. If  = Λ
2 , the
statistical distance between f ¯X, ¯Y (x, y) and fX,Y (x, y) is
upper-bounded by

(cid:16)(cid:113) ρ(1−ρ)

(cid:17)

< 1

1+ρ

(cid:12)(cid:12)f ¯X, ¯Y (x, y) − fX,Y (x, y)(cid:12)(cid:12) dxdy ≤ 4,

ˆ

R2

and the mutual information I( ¯X, ¯Y ; ¯W ) satisﬁes

I( ¯X, ¯Y ; ¯W ) ≥ 1
2

log

1 + ρ
1 − ρ

− 5 log(e).

According to Wyner’s second approach, ¯W is an eligible
candidate of the common message of (X, Y ) when  → 0.
Proof: Since ¯X − ¯W − ¯Y is a Markov chain, we have

f ¯X, ¯Y (x, y)
=

(cid:88)
(cid:88)

=

=

=

f√

a∈Λ

(cid:18)
(cid:19)

a∈Λ
1
ρ(Λ)
· exp

f ¯X, ¯Y , ¯W (x, y, a)
f ¯W (a)f ¯X| ¯W (x|a)f ¯Y | ¯W (y|a)
(cid:19)
(cid:88)
(cid:18)
2π(cid:112)1 − ρ2
(cid:80)

1(cid:112)2π(1 − ρ)
(cid:18)
1(cid:112)2π(1 − ρ)
(cid:19)
− x2 + y2 − 2ρxy
(cid:18)

2(1 − ρ2)

− a2
2ρ

a∈Λ

exp

1

exp

1√
2πρ
− (x − a)2
(cid:18)
2(1 − ρ)
1(cid:113)
(cid:18)

2π ρ(1−ρ)

exp

1+ρ

exp
f√

ρ(Λ)

1+ρ

− (a− ρ(x+y)
1+ρ )2
2· ρ(1−ρ)
(cid:19)

,

a∈Λ

·

(cid:19)

− (y − a)2
2(1 − ρ)
(cid:19)

√

(5)
where
= fX,Y (x, y) is the
PDF of two joint Gaussian variables. By the deﬁnition of the
ﬂatness factor, we have

− x2+y2−2ρxy
2(1−ρ2)

1
1−ρ2

exp

2π

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)V (Λ)

(cid:88)

a∈Λ

1(cid:113)

2π ρ(1−ρ)

1+ρ

(cid:18)

exp

− (a − ρ(x+y)
1+ρ )2
(cid:32)(cid:115)
2 ρ(1−ρ)

1+ρ

ρ(1 − ρ)
1 + ρ

≤ Λ

(cid:19)
(cid:33)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

− 1

= .

(6)

Since Λ(σ) is a monotonically decreasing function of σ (see
√
[18, Remark 2]), we have (

ρ) ≤  and hence

(cid:12)(cid:12)V (Λ)f√

ρ(Λ) − 1(cid:12)(cid:12) ≤ .

(7)

Combining (5), (6) and (7) gives us

fX,Y (x, y)(1 − 2) ≤ fX,Y (x, y) · 1 − 

1 + 

≤ f ¯X, ¯Y (x, y),

and

f ¯X, ¯Y (x, y) ≤ fX,Y (x, y) · 1 + 
1 − 

≤ fX,Y (x, y)(1 + 4),

when  < 1

2. Finally,
ˆ

ˆ

R2

≤ 4

(cid:12)(cid:12)f ¯X, ¯Y (x, y) − fX,Y (x, y)(cid:12)(cid:12) dxdy

fX,Y (x, y)dxdy = 4.

R2

Similarly,

the Kullback-Leibler

divergence

between

f ¯X, ¯Y (x, y) and fX,Y (x, y) can be upper-bounded as

D(f ¯X, ¯Y (cid:107)fX,Y ) =
≤

ˆ
ˆ

R2

R2

f ¯X, ¯Y (x, y) log

f ¯X, ¯Y (x, y)
fX,Y (x, y)

dxdy

f ¯X, ¯Y (x, y) log(1 + 4)dxdy

(8)

= log(1 + 4).

(cid:113) ρ(1−ρ)

For any
1+ρ > 0,  can be made arbitrarily small by
scaling Λ. Therefore, when  → 0, ¯W can be viewed as the
common message according to Wyner’s second approach. To
see that I( ¯X, ¯Y ; ¯W ) can be arbitrarily close to the common
information, we rewrite D(f ¯X, ¯Y (cid:107)fX,Y ) as

D(f ¯X, ¯Y (cid:107)fX,Y )

ˆ

=

R2
= −

ˆ
ˆ

= −

f ¯X, ¯Y (x, y)
fX,Y (x, y)

1

R2

dxdy

(cid:32)

f ¯X, ¯Y (x, y) log

f ¯X, ¯Y (x, y) log
f ¯X, ¯Y (x, y) log fX,Y (x, y)dxdy − h( ¯X, ¯Y )
2π(cid:112)1 − ρ2
(cid:19)(cid:33)
(cid:18)
− x2 + y2 − 2ρxy
(cid:112)
1 − ρ2(cid:1) +
(cid:112)
1 − ρ2(cid:1) +

E ¯X, ¯Y [x2 + y2 − 2ρxy]

2(1 − ρ2)

2(1 − ρ2)

1 + E ¯W [w2]

R2
· exp

dxdy − h( ¯X, ¯Y )

log(e) − h( ¯X, ¯Y ).

= log(cid:0)2π
= log(cid:0)2π

log(e) − h( ¯X, ¯Y )

1 + ρ

√
Note that Λ(
3] , it is easy to make E ¯W

D(f ¯X, ¯Y (cid:107)fX,Y ) ≥ log(cid:0)2π

ρ) ≤ . By [17, Lemma 9] and [17, Remark

(cid:2)w2(cid:3) ≥ ρ(1 − 2). Then we have
(cid:112)
1 − ρ2(cid:1) + (1 − ) log(e) − h( ¯X, ¯Y )

= h(X, Y ) − h( ¯X, ¯Y ) −  log(e).

Using (8), we obtain

I(X, Y ; W ) − I( ¯X, ¯Y ; ¯W ) = h(X, Y ) − h( ¯X, ¯Y )
≤ log(1 + 4) +  log(e)
≤ 5 log(e).

√

Similar to [10], using DΛ,

ρ as the reconstruction distribu-
tion, we can design quantization polar lattices to extract the
common randomness according to “Construction D”. The only
difference is that the size of the source alphabet is doubled
in this work. The next theorem shows that the design of
polar lattices for extracting the common randomness of a
pair of joint Gaussian sources is exactly the same as that for
quantizing a Gaussian source, which means that the technique
proposed in [10] can be directly employed to our work.
Theorem 6. The construction of a polar lattice for extracting
the common randomness of a pair of joint Gaussian sources
(X, Y ) is equivalent to the construction of a rate-distortion
bound achieving polar lattice for a Gaussian source X+Y
Proof: Let ¯W be labelled by bits ¯W1,··· , ¯Wr ( ¯W r

2
1 ) ac-
cording to a binary partition chain Λ/Λ1/··· /Λr−1/Λ(cid:48)(Λr).
Then, DΛ,
whose limit corre-
sponds to DΛ,

ρ induces a distribution P ¯W r

ρ as r → ∞.

√

√

.

1

By the chain rule of mutual information,

I( ¯X, ¯Y ; ¯W r

1 ) =

I( ¯X, ¯Y ; ¯W(cid:96)| ¯W (cid:96)−1

1

),

we obtain r binary-input channels V(cid:96) for 1 ≤ (cid:96) ≤ r. Given
the realization w(cid:96)−1
1) the coset of

, denote by A(cid:96)(w(cid:96)

of ¯W (cid:96)−1

1

1

(cid:96)=1

r(cid:88)

REFERENCES

[1] C. Shannon, “A mathematical theory of communication,” Bell Syst. Tech.

J., vol. 27, no. 3, pp. 379–423, July 1948.

[2] P. Gács and J. Körner, “Common information is far less than mutual
information,” Problems of Control and Information Theory, vol. 2, no. 2,
pp. 149–162, 1973.

[3] A. D. Wyner, “The common information of two dependent random
variables,” IEEE Trans. Inf. Theory, vol. 21, no. 2, pp. 163–179, March
1975.

[4] R. Gray and A. Wyner, “Source coding for a simple network,” Bell

System Technical Journal, vol. 53, no. 9, pp. 1681–1721, 1974.

[5] K. B. Viswanatha, E. Akyol, and K. Rose, “The lossy common infor-
mation of correlated sources,” Information Theory, IEEE Transactions
on, vol. 60, no. 6, pp. 3238–3253, 2014.

[6] G. Xu, W. Liu, and B. Chen, “Wyner’s common information for
continuous random variables - A lossy source coding interpretation,”
in Proc. Annual Conference on Information Sciences and Systems,
Baltimore, MD, March 2011.

[7] E. Arıkan, “Channel polarization: A method for constructing capacity-
achieving codes for symmetric binary-input memoryless channels,” IEEE
Trans. Inf. Theory, vol. 55, no. 7, pp. 3051–3073, July 2009.

[8] S. B. Korada, “Polar codes for channel and source coding,” Ph.D.

dissertation, Ecole Polytechnique Fédèrale de Lausanne, 2009.

[9] J. Honda and H. Yamamoto, “Polar coding without alphabet extension
for asymmetric models,” IEEE Trans. Inf. Theory, vol. 59, no. 12, pp.
7829–7838, Dec. 2013.

[10] L. Liu and C. Ling, “Polar lattices for lossy compression,” Jan. 2015.

[Online]. Available: http://arxiv.org/abs/1501.05683

[11] N. Goela, “Polarized random variables: Maximal correlations and com-
mon information,” in Proc. 2014 IEEE Int. Symp. Inform. Theory,
Honolulu, USA, June 2014, pp. 1643–1647.

[12] H. S. Cronie and S. B. Korada, “Lossless source coding with polar
codes,” in Proc. 2010 IEEE Int. Symp. Inform. Theory, Austin, TX,
June 2010, pp. 904–908.

[13] E. Arikan, “Source polarization,” 2010 IEEE International Symposium

on Information Theory Proceedings, 2010.

[14] I. Tal and A. Vardy, “How to construct polar codes,” IEEE Trans. Inf.

Theory, vol. 59, no. 10, pp. 6562–6582, Oct. 2013.

[15] H. S. Witsenhausen, “Values and bounds for the common information of
two discrete random variables,” SIAM Journal on Applied Mathematics,
vol. 31, no. 2, pp. 313–333, 1976.

[16] G. D. Forney Jr., M. Trott, and S.-Y. Chung, “Sphere-bound-achieving
coset codes and multilevel coset codes,” IEEE Trans. Inf. Theory, vol. 46,
no. 3, pp. 820–850, May 2000.

[17] C. Ling and J. Belﬁore, “Achieving AWGN channel capacity with lattice
Gaussian coding,” IEEE Trans. Inf. Theory, vol. 60, no. 10, pp. 5918–
5929, Oct. 2014.

[18] C. Ling, L. Luzzi, J. Belﬁore, and D. Stehle, “Semantically secure
lattice codes for the Gaussian wiretap channel,” IEEE Trans. Inf. Theory,
vol. 60, no. 10, pp. 6399–6416, Oct. 2014.

[19] U. Wachsmann, R. Fischer, and J. Huber, “Multilevel codes: Theoretical
concepts and practical design rules,” IEEE Trans. Inf. Theory, vol. 45,
no. 5, pp. 1361–1391, July 1999.

[20] Y. Yan, L. Liu, C. Ling, and X. Wu, “Construction of capacity-
achieving lattice codes: Polar lattices,” Nov. 2014. [Online]. Available:
http://arxiv.org/abs/1411.0187

Λ(cid:96) indexed by w(cid:96)−1
transition PDF of the (cid:96)-th channel V(cid:96) is given by

and w(cid:96). According to [19], the channel

1

1

1

1

1

=

f√

f√

f ¯X, ¯Y | ¯W(cid:96), ¯W (cid:96)−1
ρ(A(cid:96)(w(cid:96)
1))

a∈A(cid:96)(w(cid:96)
1)

1√
2πρ

(x, y|w(cid:96), w(cid:96)−1
)
f√

(cid:88)
(cid:18)
(cid:88)
ρ(A(cid:96)(w(cid:96)
1))
a∈A(cid:96)(w(cid:96)
1(cid:112)2π(1 − ρ)
1)
− (x − a)2
(cid:18)
(cid:19)
· exp
2(1 − ρ)
− x2 + y2 − 2ρxy
(cid:18)
ρ(A(cid:96)(w(cid:96)
1))
1(cid:113)
− (a − ρ(x+y)
1+ρ )2
2 · ρ(1−ρ)

ρ(a)f ¯X, ¯Y | ¯W (x, y|a)
(cid:19)
1(cid:112)2π(1 − ρ)
(cid:18)
− (y − a)2
2(1 − ρ)
(cid:19)

(cid:18)
2π(cid:112)1 − ρ2
· (cid:88)

2(1 − ρ2)

− a2
2ρ

(cid:19)

=

=

f√

exp

exp

exp

exp

2π ρ(1−ρ)

1

1

,

1+ρ

a∈A(cid:96)(w(cid:96)
1)

1+ρ

(cid:19)

Using the channel symmetrization technique for asymmetric
channels [10], the channel transition PDF of the symmetrized
channel ˜V(cid:96) is

, w(cid:96) ⊕ ˜w(cid:96)| ˜w(cid:96))

(x, y, w(cid:96)
1)

(x, y, w(cid:96)−1

f ˜V(cid:96)
1
= f ¯X, ¯Y , ¯W (cid:96)

1

1

2π(cid:112)1 − ρ2
· (cid:88)

=

exp

a∈A(cid:96)(w(cid:96)
1)

(cid:18)
− x2 + y2 − 2ρxy
(cid:18)
1(cid:113)

(cid:19) 1
ρ(Λ)
− (a − ρ(x+y)
1+ρ )2
2 · ρ(1−ρ)

2(1 − ρ2)

2π ρ(1−ρ)

f√

exp

1+ρ

1+ρ

(cid:19)

.

(9)
Comparing with the Λ(cid:96)−1/Λ(cid:96) channel [20, eqn. (13)],
we see that the symmetrized channel (9) is equivalent to a
Λ(cid:96)−1/Λ(cid:96) channel with noise variance ρ(1−ρ)
in the sense of
the likelihood ratio.

and covariance matrix (cid:2) 1 ρ

(cid:3). It is well known that X+Y

Recall that X, Y are bivariate Gaussian with zero mean
is
2 . Now consider
using
ρ. The MMSE re-scaled
1+ρ and ρ(1−ρ)
1+ρ ,

Gaussian with zero mean and variance 1+ρ
the construction of a polar lattice to quantize X+Y
the reconstruction distribution DΛ,
coefﬁcient and noise variance are given by 2ρ
which is the same as that in (9).

1+ρ

√

ρ 1

2

2

V. CONCLUSION

Explicit construction of polar codes and polar lattices
for both lossy and lossless Gray-Wyner network coding is
proposed. For a joint Gaussian source, we show that a
rate-distortion bound achieving polar lattice designed for a
Gaussian source can be directly used to extract the common
randomness. For future work, we will extend the result of
Theorem 6 into multiple joint Gaussian RVs. The common
information of multiple Gaussian sources has been given in
[6].

