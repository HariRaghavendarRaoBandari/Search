6
1
0
2

 
r
a

 

M
6
1

 
 
]

V
C
.
s
c
[
 
 

1
v
5
1
0
5
0

.

3
0
6
1
:
v
i
X
r
a

Non-linear Dimensionality Regularizer for

Solving Inverse Problems

Ravi Garg, Anders Eriksson, and Ian Reid

1 University of Adelaide

2 Queensland University of Technology

3 University of Adelaide

Abstract. Consider an ill-posed inverse problem of estimating causal
factors from observations, one of which is known to lie near some (un-
known) low-dimensional, non-linear manifold expressed by a predeﬁned
Mercer-kernel. Solving this problem requires simultaneous estimation of
these factors and learning the low-dimensional representation for them.
In this work, we introduce a novel non-linear dimensionality regulariza-
tion technique for solving such problems without pre-training.
We re-formulate Kernel-PCA as an energy minimization problem in
which low dimensionality constraints are introduced as regularization
terms in the energy. To the best of our knowledge, ours is the ﬁrst at-
tempt to create a dimensionality regularizer in the KPCA framework.
Our approach relies on robustly penalizing the rank of the recovered fac-
tors directly in the implicit feature space to create their low-dimensional
approximations in closed form.
Our approach performs robust KPCA in the presence of missing data
and noise. We demonstrate state-of-the-art results on predicting missing
entries in the standard oil ﬂow dataset. Additionally, we evaluate our
method on the challenging problem of Non-Rigid Structure from Motion
and our approach delivers promising results on CMU mocap dataset
despite the presence of signiﬁcant occlusions and noise.

Dimensionality reduction techniques are widely used in data modeling, vi-
sualization and unsupervised learning. Principal component analysis (PCA[18]),
Kernel PCA (KPCA[28]) and Latent Variable Models (LVMs[20]) are some of
the well known techniques used to create low dimensional representations of the
given data while preserving its signiﬁcant information.

One key deployment of low-dimensional modeling occurs in solving ill-posed
inference problems. Assuming the valid solutions to the problem lie near a low-
dimensional manifold (i.e. can be parametrized with a reduced set of variables)
allows for a tractable inference for otherwise under-constrained problems. After
the seminal work of [8,26] on guaranteed rank minimization of the matrix via
trace norm heuristics [12], many ill-posed computer vision problems have been
tackled by using the trace norm — a convex surrogate of the rank function — as a
regularization term in an energy minimization framework[8,31]. The ﬂexible and
easy integration of low-rank priors is one of key factors for versatility and success

2

Garg et. al.

Fig. 1: Non-linear dimensionality regularisation improves NRSfM perfor-
mance compared to its linear counterpart. Figure shows the ground truth 3D
structures in red wire-frame overlaid with the structures estimated using: (a) proposed
non-linear dimensionality regularizer shown in blue dots and (b) corresponding linear
dimensionality regularizer (TNH) shown in black crosses, for sample frames of CMU
mocap sequence. Red circles represent the 3D points for which the projections were
known whereas squares annotated missing 2D observations. See text and Table 3 for
details.

of many algorithms. For example, pre-trained active appearance models [9] or
3D morphable models [5] are converted to robust feature tracking [24], dense
registration [14] and vivid reconstructions of natural videos [13] with no a priori
knowledge of the scene. Various bilinear factorization problems like background
modeling, structure from motion or photometric stereo are also addressed with
a variational formulation of the trace norm regularization [7].

On the other hand, although many non-linear dimensionality reduction tech-
niques — in particular KPCA — have been shown to outperform their linear
counterparts for many data modeling tasks, they are seldom used to solve inverse
problems without using a training phase. A general (discriminative) framework
for using non-linear dimensionality reduction is: (i) learn a low-dimensional rep-
resentation for the data using training examples via the kernel trick (ii) project
the test examples on the learned manifold and ﬁnally (iii) ﬁnd a data point
(pre-image) corresponding to each projection in the input space.

Non-linear Dimensionality Regularizer for Solving Inverse Problems

3

This setup has two major disadvantages. Firstly, many problems of interest
come with corrupted observations — noise, missing data and outliers — which
violate the low-dimensional modeling assumption.Secondly, computing the pre-
image of any point in the low dimensional feature subspace is non-trivial: the
pre-image for many points in the low dimensional space might not even exist
because the non linear feature mapping function used for mapping the data
from input space to the feature space is non-surjective.

mizing the image reprojection error f (W, R, S) =(cid:80)

Fig. 2: Non-linear dimensionality regularizer for NRSfM. The top part of the
ﬁgure explains the ill-posed inverse problem of recovering the causal factors (1); projec-
tion matrices Ri and 3D structures Si, from 2D image observations (2) Wi’s, by mini-
i (cid:107)Wi − RiSi(cid:107)2. Assuming that the
recovered 3D structures (Si’s) lies near an unknown non-linear manifold (represented
by the blue curve) in the input space, we propose to regularize the dimensionality of
this manifold (3) — span of the non-linearly transformed shape vectors φ(Si)’s — by
minimizing (cid:107)φ(S)(cid:107)∗. The non-linear transformation φ is deﬁned implicitly with a Mer-
cer kernel and maps the non-linear manifold to a linear low rank subspace (shown in
blue line) of RKHS.

Previously, extensions to KPCA like Robust KPCA (RKPCA[22]) and prob-
abilistic KPCA (PKPCA[27]) with missing data have been proposed to address
the ﬁrst concern, while various additional regularizers have been used to estimate
the pre-image robustly [3,21,19,1].

Generative models like LVMs [20] are often used for inference by searching
the low-dimensional latent space for a location which maximizes the likelihood
of the observations. Problems like segmentation, tracking and semantic 3D re-

S1R1S2R2...Causal Factors3D shapes (Si) and the projection matrices (Ri)1Wi =RiSi4

Garg et. al.

construction [25,11] greatly beneﬁt from using LVM. However, the latent space
is learned a priori with clean training data in all these approaches.

Almost all non-linear dimensionality reduction techniques are non-trivial to
generalize for solving ill-posed problems (See section 3.3) without a pre-training
stage. Badly under-constrained problems require the low-dimensional constraints
even for ﬁnding an initial solution, eliminating applicability of the standard “pro-
jection + pre-image estimation” paradigm. This hinders the utility of non-linear
dimensionality reduction and a suitable regularization technique to penalize the
non-linear dimensionality is desirable.

Sum and Substance: A closer look at most non-linear dimensionality re-
duction techniques reveals that they rely upon a non-linear mapping function
which maps the data from input space to a (usually) higher dimensional feature
space. In this feature space the data is assumed to lie on a low-dimensional hyper-
plane — thus, linear low-rank prior is apt in the feature space. Armed with this
simple observation, our aim is to focus on incorporating the advances made in
linear dimensionality reduction techniques to their non-linear counterparts, while
addressing the problems described above. Figure 2 explains this central idea and
proposed dimensionality regularizer in a nutshell with Non Rigid Structure from
Motion (NRSfM) as the example application.

Our Contribution: In this work we propose a uniﬁed for simultaneous ro-
bust KPCA and pre-image estimation while solving an ill-posed inference prob-
lem without a pre-training stage.

In particular we propose a novel robust energy minimization algorithm which
handles the implicitness of the feature space to directly penalize its rank by
iteratively:

– creating robust low-dimensional representation for the data given the kernel

matrix in closed form and

– reconstructing the noise-free version of the data (pre-image of the features
space projections) using the estimated low-dimensional representations in a
uniﬁed framework.

The proposed algorithm: (i) provides a novel closed form solution to robust
KPCA; (ii) yields state-of-the-art results on missing data prediction for the well-
known oil ﬂow dataset; (iii) outperforms state-of-the-art linear dimensionality
(rank) regularizers to solve NRSfM; and (iv) can be trivially generalized to
incorporate other cost functions in an energy minimization framework to solve
various ill-posed inference problems.

1 Problem formulation

This paper focuses on solving a generic inverse problem of recovering causal fac-
tor S = [s1, s2, ··· sN ] ∈ X × N from N observations W = [w1, w2, ··· wN ] ∈
Y × N such that f (W, S) = 0. Here function f (observation,variable), is a generic
loss function which aligns the observations W with the variable S (possibly via
other causal factors. e.g. R or Z in Section 3.2 and 3.3).

Non-linear Dimensionality Regularizer for Solving Inverse Problems

5

If, f (W, S) = 0 is ill-conditioned (for example when Y (cid:28) X ), we want to
recover matrix S under the assumption that the columns of it lie near a low-
dimensional non-linear manifold. This can be done by solving a constrained
optimization problem of the following form:

min

S

rank(Φ(S))

s.t. f (W, S) ≤ 

(1)
where Φ(S) = [φ(s1), φ(s2), ··· , φ(sN )] ∈ H × N is the non-linear mapping of
matrix S from the input space X to the feature space H (also commonly referred
as Reproducing Kernel Hilbert Space), via a non-linear mapping function φ :
X → H associated with a Mercer kernel K such that K(S)i,j = φ(si)T φ(sj).

In this paper we present a novel energy minimization framework to solve

problems of the general form (1).

The trace norm (cid:107)M(cid:107)∗ =:(cid:80)

As our ﬁrst contribution, we relax the problem (1) by using the trace norm
of Φ(S) — the convex surrogate of rank function — as a penalization function.
i λi(M ) of a matrix M is the sum of its eigenvalues
λi(M ) and was proposed as a tight convex relaxation4 of the rank(M ) and
is used in many vision problems as a rank regularizer [12]. Although the rank
minimization via trace norm relaxation does not lead to a convex problem in
presence of a non-linear kernel function, we show in 2.2 that it leads to a closed-
form solution to denoising a kernel matrix via penalizing the rank of recovered
data (S) directly in the feature space.

With these changes we can rewrite (1) as:

f (W, S) + τ(cid:107)Φ(S)(cid:107)∗

min

S

(2)

where τ is a regularization strength.5
It is important to notice that although the rank of the kernel matrix K(S) is
equal to the rank of Φ(S), (cid:107)K(S)(cid:107)∗ is merely (cid:107)Φ(S)(cid:107)2F . Thus, directly penalizing
the sum of the singular values of K(S) will not encourage low-rank in the feature
space.6

Although we have relaxed the non-convex rank function, (2) is in general
diﬃcult to minimize due to the implicitness of the feature space. Most widely
used kernel functions like RBF do not have a explicit deﬁnition of the function φ.
Moreover, the feature space for many kernels is high- (possibly inﬁnite-) dimen-
sional, leading to intractability. These issues are identiﬁed as the main barriers
to robust KPCA and pre-image estimation [22]. Thus, we have to reformulate (2)
4 More precisely, (cid:107)M(cid:107)∗ was shown to be the tight convex envelope of rank(M )/(cid:107)M(cid:107)s,
where (cid:107)M(cid:107)s represent spectral norm of M .
5 1/τ can also be viewed as Lagrange multiplier to the constraints in (1).
6 Although it is clear that relaxing the rank of kernel matrix to (cid:107)K(S)(cid:107)∗ is suboptimal,
works like [17,7] with a variational deﬁnition of nuclear norm, allude to the possibility
of kernelization. Further investigation is required to compare this counterpart to our
tighter relaxation.

6

Garg et. al.

by applying kernel trick where the cost function (2) can be expressed in terms
of the kernel function alone.

The key insight here is that under the assumption that kernel matrix K(S)
is positive semideﬁnite, we can factorize it as: K(S) = C T C. Although, this
factorization is non-unique, it is trivial to show the following:

(cid:112)λi(K(S)) = λi(C) = λi(Φ(S))

Thus: (cid:107)C(cid:107)∗ = (cid:107)Φ(S)(cid:107)∗ ∀ C : C T C = K(S)

(3)

where λi(.) is the function mapping the input matrix to its ith largest eigenvalue.
The row space of matrix C in (3) can be seen to span the eigenvectors as-
sociated with the kernel matrix K(S) — hence the principal components of the
non-linear manifold we want to estimate.

Using (3), problem (2) can ﬁnally be written as:
f (W, S) + τ(cid:107)C(cid:107)∗
s.t. K(S) = C T C

min
S,C

(4)

The above minimization can be solved with a soft relaxation of the manifold
constraint by assuming that the columns of S lie near the non-linear manifold.

min
S,C

f (W, S) +

(cid:107)K(S) − C T C(cid:107)2F + τ(cid:107)C(cid:107)∗

ρ
2

(5)

As ρ → ∞, the optimum of (5) approaches the optimum of (4) . A local optimum
of (4) can be achieved using the penalty method of [23] by optimizing (5) while
iteratively increasing ρ as explained in Section 2.

Before moving on, we would like to discuss some alternative interpretations of
(5) and its relationship to previous work – in particular LVMs. Intuitively, we can
also interpret (5) from the probabilistic viewpoint as commonly used in latent
variable model based approaches to deﬁne kernel function [20]. For example a
RBF kernel with additive Gaussian noise and inverse width γ can be deﬁned as:
+ , where  ∼ N (0, σ). In other words, with a ﬁnite ρ, our
K(S)i,j = e−γ(cid:107)si−sj(cid:107)2
model allows the data points to lie near a non-linear low-rank manifold instead of
on it. Its worth noting here that like LVMs, our energy formulation also attempts
to maximize the likelihood of regenerating the training data W , (by choosing
f (W, S) to be a simple least squares cost) while doing dimensionality reduction.
Note that in closely related work [15], continuous rank penalization (with
a logarithmic prior) has also been used for robust probabilistic non-linear di-
mensionality reduction and model selection in LVM framework. However, unlike
[15,20] where the non-linearities are modeled in latent space (of predeﬁned di-
mensionality), our approach directly penalizes the non-linear dimensionality of
data in a KPCA framework and is applicable to solve inverse problems without
pre-training.

Non-linear Dimensionality Regularizer for Solving Inverse Problems

7

2 Optimization

We approach the optimization of (5) by solving the following two sub-problems
in alternation:

min

S

min

C

f (W, S) +
τ(cid:107)C(cid:107)∗ +

(cid:107)K(S) − C T C(cid:107)2F
(cid:107)K(S) − C T C(cid:107)2F

ρ
2
ρ
2

(6)

(7)

Algorithm 1 outlines the approach and we give a detailed description and inter-
pretations of both sub-problems (7) and (6) in next two sections of the paper.

Algorithm 1: Inference with Proposed Regularizer.

Input: Initial estimate S0 of S.
Output: Low-dimensional S and kernel representation C.
Parameters: Initial ρ0 and maximum ρmax penalty, with scale ρs.
- S = S0, ρ = ρ0 ;
while ρ ≤ ρmax do

while not converged do

- Fix S and estimate C via closed-form solution of (7) using Algorithm
2;
- Fix C and minimize (6) to update S using LM algorithm;

- ρ = ρρs ;

2.1 Pre-image estimation to solve inverse problem.

Subproblem (6) can be seen as a generalized pre-image estimation problem: we
seek the factor si, which is the pre-image of the projection of φ(si) onto the
principle subspace of the RKHS stored in C T C, which best explains the obser-
vation wi. Here (6) is generally a non-convex problem, unless the Mercer-kernel
is linear, and must therefore be solved using non-linear optimization techniques.
In this work, we use the Levenberg-Marquardt algorithm for optimizing (6).

Notice that (6) only computes the pre-image for the feature space projections
of the data points with which the non-linear manifold (matrix C) is learned.
An extension to our formulation is desirable if one wants to use the learned
non-linear manifold for denoising test data in a classic pre-image estimation
framework. Although a valuable direction to pursue, it is out of scope of the
present paper.

2.2 Robust dimensionality reduction

One can interpret sub-problem (7) as a robust form of KPCA where the kernel
matrix has been corrupted with Gaussian noise and we want to generate its

8

Garg et. al.

low-rank approximation. Although (7) is non-convex we can solve it in closed-
form via singular value decomposition. Our closed-form solution is outlined in
Algorithm 2 and is based on the following theorem:
Theorem 1. With S n (cid:51) A (cid:23) 0 let A = U ΣU T denote its singular value de-
composition. Then

min

L

ρ
2

||A − LT L||2F + τ||L||∗
n(cid:88)

(cid:16) ρ

(σi − γ∗2

i )2 + τ γ∗

i

=

2

i=1

(cid:17)

.

(8)

(9)

A minimizer L∗ of (8) is given by

i ∈ {α ∈ R+ | pσi,τ /2ρ(α) = 0} (cid:83) {0}, where pa,b denotes the

L∗ = Γ ∗U T

(10)

+ is the set of n-by-n diagonal matrices

+, γ∗

with Γ ∗ ∈ Dn
depressed cubic pa,b(x) = x3 − ax + b. Dn
with non-negative entries.

As the closed-form solution to (7) is a key contribution of this work, an
extended proof to the above theorem is included in the Appendix A. Theorem
1 shows that each eigenvalue of the minimizer C∗ of (7) can be obtained by
solving a depressed cubic whose coeﬃcients are determined by the corresponding
eigenvalue of the kernel matrix and the regularization strength τ . The roots of
each cubic, together with zero, comprise a set of candidates for the corresponding
egienvalue of C∗. The best one from this set is obtained by choosing the value
which minimizes (9) (see Algorithm 2).

3 Experiments

In this section we demonstrate the utility of the proposed algorithm. The aims
of our experiments are twofold: (i) to compare our dimensionality reduction
technique favorably with KPCA and its robust variants; and (ii) to demonstrate
that the proposed non-linear dimensionality regularizer consistently outperforms
its linear counterpart (a.k.a. nuclear norm) in solving inverse problems.

3.1 Validating the closed form solution

Given the relaxations proposed in Section 1, our assertion that the novel trace
regularization based non-linear dimensionality reduction is robust need to be
substantiated. To that end, we evaluate our closed-form solution of Algorithm 2
on the standard oil ﬂow dataset introduced in [4].

This dataset comprises 1000 training and 1000 testing data samples, each of
which is of 12 dimensions and categorized into one of three diﬀerent classes. We

Non-linear Dimensionality Regularizer for Solving Inverse Problems

9

Algorithm 2: Robust Dimensionality Reduction.

Input: Current estimate of S.
Output: Low-dimensional representation C.
Parameters: Current ρ and regularization strength τ .

- [U Λ U T ] = Singular Value Decomposition of K(S);
// Λ is a diagonal matrix, storing N singular values λi of K(S).
for i = 1 to N do

- Find three solutions (lr : r ∈ {1, 2, 3}) of:

l3 − lλi +

τ
2ρ

= 0 ;

- set l4 = 0;
- lr = max(lr, 0) ∀r ∈ {1, 2, 3, 4} ;
r(cid:107)2 + τ lr } ;
- r = arg min
- ¯λi = lr ;

{ ρ
2(cid:107)λi − l2

r

- C = ¯ΛU T ;
// ¯Λ is diagonal matrix storing ¯λi.

add zero mean Gaussian noise with variance σ to the training data7 and recover
the low-dimensional manifold for this noisy training data Sσ with KPCA and
contrast this with the results from Algorithm 2. An inverse width of the Gaussian
kernel γ = 0.075 is used for all the experiments on the oil ﬂow dataset.

It is important to note that in this experiment, we only estimate the principal
components (and their variances) that explain the estimated non-linear manifold,
i.e. matrix C by Algorithm 2, without reconstructing the denoised version of the
corrupted data samples.

Both KPCA and our solution require model selection (choice of rank and τ
respectively) which is beyond the scope of this paper. Here we resort to eval-
uate the performance of both methods under diﬀerent parameters settings. To
quantify the accuracy of the recovered manifold (C) we use following criteria:

– Manifold Error : A good manifold should preserve maximum variance of the
data — i.e. it should be able to generate a denoised version K(Sest) = C T C
of the noisy kernel matrix K(Sσ). We deﬁne the manifold estimation error
as (cid:107)K(Sest) − K(SGT )(cid:107)2F , where K(SGT ) is the kernel matrix derived using
noise free data. Figure 3 shows the manifold estimation error for KPCA and
our method for diﬀerent rank and parameter τ respectively.8

– Classiﬁcation error: The accuracy of a non-linear manifold is often also tested
by the nearest neighbor classiﬁcation accuracy. We select the estimated man-
ifold which gives minimum Manifold Error for both the methods and report

7 Note that our formulation assumes Gaussian noise in K(S) where as for this evalu-

ation we add noise to S directly.

8 Errors from non-noisy kernel matrix can be replaced by cross validating the entries

of the kernel matrix for model selection for more realistic experiment.

10

Garg et. al.

1NN classiﬁcation error (percentage of misclassiﬁed example) of the 1000
test points by projecting them onto estimated manifolds.

Table 1 shows that the proposed method outperforms KPCA to generate less
noisy manifold representations with diﬀerent ranks and gives better classiﬁcation
results than KPCA for test data. The diﬀerences are more signiﬁcant as the
amount of noise increases. This simple experiment evaluates our Robust KPCA
solution in isolation and indicates that our closed form solution itself can be
beneﬁcial for the problems where pre-image estimation is not required. Note
that we have used no loss function f (W, S) in this experiment however further
investigation into more suitable classiﬁcation loss functions (e.g. [17]) should
lead to better results.

Table 1: Robust dimensionality reduction accuracy by KPCA versus our closed-form
solution on the full oil ﬂow dataset. Columns from left to right represent: (1) standard
deviation of the noise in training samples (2-3) Error in the estimated low-dimensional
kernel matrix by (2) KPCA and (3) our closed-form solution, (4-5) Nearest neigh-
bor classiﬁcation error of test data using (4) KPCA and (5) our closed-form solution
respectively.

Manifold Error Classiﬁcation Error

STD KPCA Our CFS KPCA Our CFS

.2
.3
.4

0.1099 0.1068
9.60%
0.2298 0.2184 19.90% 15.70%
0.3522 0.3339 40.10% 22.20%

9.60%

Fig. 3: Performance comparison between KPCA and our Robust closed-form solution
with dimensionality regularization on oil ﬂow dataset with additive Gaussian noise of
standard deviation σ. Plots show the normalized kernel matrix errors with diﬀerent
rank of the model. Kernel PCA results are shown in dotted line with diamond while
ours are with solid line with a star. Bar-plot show the worst and the best errors obtained
by our method for a single rank of recovered kernel matrix.

02468101214160.10.150.20.250.30.350.4Rank of kernel matrixManifold error  KPCA,σ=.2Ours,σ=.2KPCA,σ=.3Ours,σ=.3KPCA,σ=.4Ours,σ=.4Non-linear Dimensionality Regularizer for Solving Inverse Problems

11

Table 2: Performance comparison on missing data completion on Oil Flow Dataset:
Row 1 shows the amount of missing data and subsequent rows show the mean and
standard deviation of the error in recovered data matrix over 50 runs on 100 samples
of oil ﬂow dataset by: (1) The mean method (also the initialization of other methods)
where the missing entries are replaced by the mean of the known values of the corre-
sponding attributes, (2) 1-nearest neighbor method in which missing entries are ﬁlled
by the values of the nearest point, (3) PPCA [29], (4) PKPCA of [27], (5)RKPCA [22]
and our method.

0.05
0.50
0.10
0.25
p(del)
13 ± 4 28 ± 4 70 ± 9 139 ± 7
mean
5 ± 3
14 ± 5 90 ± 20
1-NN
PPCA 3.7 ± .6 9 ± 2 50 ± 10 140 ± 30
12 ± 3 32 ± 6 100 ± 20
PKPCA 5 ± 1
RKPCA 3.2 ± 1.9 8 ± 4 27 ± 8 83 ± 15
2.3±2
6±3
70±11
Ours

22±7

NA

3.2 Matrix completion

The nuclear norm has been introduced as a low rank prior originally for solving
the matrix completion problem. Thus, it is natural to evaluate its non-linear
extensions on the same task. Assuming W ∈ Rm×n to be the input matrix and
Z a binary matrix specifying the availability of the observations in W , Algorithm
1 can be used for recovering a complete matrix S with the following choice of
f (W, Z, S):

f (W, Z, S) = (cid:107)Z ◦ (W − S)(cid:107)2F

(11)

where ◦ represents Hadamard product.

To demonstrate the robustness of our algorithm for matrix completion prob-
lem, we choose 100 training samples from the oil ﬂow dataset described in sec-
tion 2.2 and randomly remove the elements from the data with varying range of
probabilities to test the performance of the proposed algorithm against various
baselines. Following the experimental setup as speciﬁed in [27], we repeat the
experiments with 50 diﬀerent samples of Z. We report the mean and standard
deviation of the root mean square reconstruction error for our method with the
choice of τ = 0.1, alongside ﬁve diﬀerent methods in Table 2. Our method signif-
icantly improves the performance of missing data completion compared to other
robust extensions of KPCA [29,27,22], for every probability of missing data.

Although we restrict our experiments to least-squares cost functions, it is
vital to restate here that our framework could trivially incorporate robust func-
tions like the L1 norm instead of the Frobenius norm — as a robust data term
f (W, Z, S) — to generalize algorithms like Robust PCA [30] to their non-linear
counterparts.

3.3 Kernel non-rigid structure from motion

Non-rigid structure from motion under orthography is an ill-posed problem
where the goal is to estimate the camera locations and 3D structure of a de-

12

Garg et. al.

formable objects from a collection of 2D images which are labeled with landmark
correspondences [6].
Assuming si(xj) ∈ R3 to be the 3D location of point xj on the deformable
object in the ith image, its orthographic projection wi(xj) ∈ R2 can be written
as wi(x) = Risi(xj), where Ri ∈ R2×3 is a orthographic projection matrix [6].
Notice that as the object deforms, even with given camera poses, reconstruct-
ing the sequence by least-squares reprojection error minimization is an ill-posed
problem. In their seminal work, [6] proposed to solve this problem with an addi-
tional assumption that the reconstructed shapes lie on a low-dimensional linear
subspace and can be parameterized as linear combinations of a relatively low
number of basis shapes. NRSfM was then cast as the low-rank factorization
problem of estimating these basis shapes and corresponding coeﬃcients.

Recent work, like [10,13] have shown that the trace norm regularizer can
be used as a convex envelope of the low-rank prior to robustly address ill-posed
nature of the problem. A good solution to NRSfM can be achieved by optimizing:

τ(cid:107)S(cid:107)∗ +

min
S,R

Zi(xj)(cid:107)wi(xj) − Risi(xj)(cid:107)2F

(12)

F(cid:88)

N(cid:88)

i=1

j=1

where S is the shape matrix whose columns are 3N dimensional vectors storing
the 3D coordinates Si(xj) of the shapes and Zi(xj) is a binary variable indicating
if projection of point xj is available in the image i.

Assuming the projection matrices to be ﬁxed, this problem is convex and
can be exactly solved with standard convex optimization methods. Additionally,
if the 2D projections wi(xj) are noise free, optimizing (12) with very small
τ corresponds to selecting the the solution — out of the many solutions —
with (almost) zero projection error, which has minimum trace norm [10]. Thus
henceforth, optimization of (12) is referred as the trace norm heuristics (TNH).
We solve this problem with a ﬁrst order primal-dual variant of the algorithm
given in [13], which can handle missing data. The algorithm is detailed and
compared with other NRSfM methods favorably in the supplementary material.9

A simple kernel extension of the above optimization problem is:

N(cid:88)

j=1

F(cid:88)
(cid:124)

i=1

τ(cid:107)Φ(S)(cid:107)∗ +

min
S,R

Zi(xj)(cid:107)wi(xj) − Risi(xj)(cid:107)2F
(cid:125)

(cid:123)(cid:122)

f (W,Z,R,S)

(13)

where Φ(S) is the non-linear mapping of S to the feature space using an RBF
kernel.

With ﬁxed projection matrices R, (13) is of the general form (2), for which
the local optima can be found using Algorithm 1. To solve NRSfM problem
with unknown projection matrices, we parameterize each Ri with quaternions

9 TNH is used as a strong baseline and has been validated on the full length CMU
mocap sequences. It marginally outperforms [10] which is known to be the state of
the art NRSfM approach without missing data.

Non-linear Dimensionality Regularizer for Solving Inverse Problems

13

Table 3: 3D reconstruction errors for linear and non-linear dimensionality regular-
ization with ground truth camera poses. Column 1 and 4 gives gives error for TNH
while column (2-3) and (5-6) gives the corresponding error for proposed method with
diﬀerent width of RBF kernel. Row 5 reports the mean error over 4 sequences.

50% Missing Data

No Missing Data

Dataset

Drink
Pickup
Yoga
Stretch
Mean

Linear

Non-Linear

Linear

Non-Linear

0.0227
0.0487
0.0344
0.0418
0.0369

dmed
dmax
0.0083
0.0114
0.0312
0.0279
0.0257 0.0276
0.0286
0.0271
0.0227
0.0242

0.0313
0.0936
0.0828
0.0911
0.0747

dmed
dmax
0.0229
0.0248
0.0709
0.0658
0.0611 0.0612
0.0694 0.0705
0.0551
0.0565

and alternate between reﬁning the 3D shapes S and projection matrices R using
LM.

Results on the CMU dataset We use a sub-sampled version of CMU mocap
dataset by selecting every 10th frame of the smoothly deforming human body
consisting 41 mocap points used in [10].10

In our ﬁrst set of experiments we use ground truth camera projection matrices
to compare our algorithm against TNH. The advantage of this setup is that with
ground-truth rotation and no noise, we can avoid the model selection (ﬁnding
optimal regularization strength τ ) by setting it low enough. We run the TNH
with τ = 10−7 and use this reconstruction as initialization for Algorithm 1. For
the proposed method, we set τ = 10−4 and use following RBF kernel width
selection approach:

– Maximum distance criterion (dmax): we set the maximum distance in the
feature space to be 3σ. Thus, the kernel matrix entry corresponding to the
shape pairs obtained by TNH with maximum Euclidean distance becomes
e−9/2.

– Median distance criterion (dmed): the kernel matrix entry corresponding to

the median euclidean distance is set to 0.5.

(cid:80)

(cid:80)

i

Following the standard protocol in [10,2], we quantify the reconstruction
results with normalized mean 3D errors e3D = 1
j eij, where eij is
the euclidean distance of a reconstructed point j in frame i from the ground
truth, σ is the mean of standard deviation for 3 coordinates for the ground
truth 3D structures, and F, N are number of input images and number of points
reconstructed.

σF N

Table 3 shows the results of the TNH and non-linear dimensionality regu-
larization based methods using the experimental setup explained above, both

10 Since our main goal is to validate the usefulness of the proposed non-linear dimen-
sionality regularizer, we opt for a reduced size dataset for more rapid and ﬂexible
evaluation.

14

Garg et. al.

Table 4: 3D reconstruction errors for linear and non-linear dimensionality regular-
ization with noisy camera pose initialization from rigid factorization and reﬁned in
alternation with shape. The format is same as Table 3.

50% Missing Data

No Missing Data

Dataset

Linear
τ = τ∗

Drink
Pickup
Yoga
Stretch
Mean

0.0947
0.1282
0.2912
0.1094
0.1559

Non-Linear

τ = 10−4
dmed
0.0906
0.1059
0.2639
0.1031
0.1409

dmax
0.0926
0.1071
0.2683
0.1043
0.1430

Non-Linear

Linear
τ = τ∗

τ = 10−4
dmed
dmax
0.0937
0.0942
0.1354
0.1339
0.2455 0.2457
0.1484
0.1552 0.1554

0.0957
0.1598
0.2821
0.1398 0.1459
0.1694

without missing data and after randomly removing 50% of the image measure-
ments. Our method consistently beats the TNH baseline and improves the mean
reconstruction error by ∼ 40% with full data and by ∼ 25% when used with
50% missing data. Figure 1 shows qualitative comparison of the obtained 3D re-
construction using TNH and proposed non-lienar dimensionality regularization
technique for some sample frames from various sequences. We refer readers to
supplementary material for more visualizations.

Table 4 shows the reconstruction performance on a more realistic experi-
mental setup, with the modiﬁcation that the camera projection matrices are
initialized with rigid factorization and were reﬁned with the shapes by opti-
mizing (3). The regularization strength τ was selected for the TNH method
by golden section search and parabolic interpolation for every test case inde-
pendently. This ensures the best possible performance for the baseline. For our
proposed approach τ was kept to 10−4 for all sequences for both missing data
and full data NRSfM. This experimental protocol somewhat disadvantages the
non-linear method, since its performance can be further improved by a judicious
choice of the regularization strength.

However our purpose is primarily to show that the non-linear method adds
value even without time-consuming per-sequence tuning. To that end, note that
despite large errors in the camera pose estimations by TNH and 50% missing
measurements, the proposed method shows signiﬁcant (∼ 10%) improvements
in terms of reconstruction errors, proving our broader claims that non-linear
representations are better suited for modeling real data, and that our robust
dimensionality regularizer can improve inference for ill-posed problems.

As suggested by [10], robust camera pose initialization is beneﬁcial for the
structure estimation. We have used rigid factorization for initializing camera
poses here but this can be trivially changed. We hope that further improvements
can be made by choosing better kernel functions, with cross validation based
model selection (value of τ ) and with a more appropriate tuning of kernel width.
Selecting a suitable kernel and its parameters is crucial for success of kernelized
algorithms. It becomes more challenging when no training data is available. We

Non-linear Dimensionality Regularizer for Solving Inverse Problems

15

hope to explore other kernel functions and parameter selection criteria in our
future work.

We would also like to contrast our work with [16], which is the only work we
are aware of where non-linear dimensionality reduction is attempted for NRSfM.
While estimating the shapes lying on a two dimensional non-linear manifold,
[16] additionally assumes smooth 3D trajectories (parametrized with a low fre-
quency DCT basis) and a pre-deﬁned hard linear rank constraint on 3D shapes.
The method relies on sparse approximation of the kernel matrix as a proxy for
dimensionality reduction. The reported results were hard to replicate under our
experimental setup for a fair comparison due to non-smooth deformations. How-
ever, in contrast to [16], our algorithm is applicable in a more general setup, can
be modiﬁed to incorporate smoothness priors and robust data terms but more
importantly, is ﬂexible to integrate with a wide range of energy minimization
formulations leading to a larger applicability beyond NRSfM.

4 Conclusion

In this paper we have introduced a novel non-linear dimensionality regularizer
which can be incorporated into an energy minimization framework, while solv-
ing an inverse problem. The proposed algorithm for penalizing the rank of the
data in the feature space has been shown to be robust to noise and missing
observations. We have picked NRSfM as an application to substantiate our ar-
guments and have shown that despite missing data and model noise (such as
erroneous camera poses) our algorithm signiﬁcantly outperforms state-of-the-
art linear counterparts.

Although our algorithm currently uses slow solvers such as the penalty method
and is not directly scalable to very large problems like dense non-rigid recon-
struction, we are actively considering alternatives to overcome these limitations.
An extension to estimate pre-images with a problem-speciﬁc loss function is pos-
sible, and this will be useful for online inference with pre-learned low-dimensional
manifolds.

Given the success of non-linear dimensionality reduction in modeling real
data and overwhelming use of the linear dimensionality regularizers in solving
real world problems, we expect that proposed non-linear dimensionality regu-
larizer will be applicable to a wide variety of unsupervised inference problems:
recommender systems; 3D reconstruction; denoising; shape prior based object
segmentation; and tracking are all possible applications.

A Proof of Theorem 3.1

Proof. We will prove theorem 1 by ﬁrst establishing a lower bound for (8) and subse-
quently showing that this lower bound is obtained at L∗ given by (10). The rotational
invariance of the entering norms allows us to write (8) as:

||Σ − W Γ 2W T||2F + τ||Γ||∗.

(14)

ρ
2

min
Γ∈Dn,
W T W =I

16

Garg et. al.

Expanding (14) we obtain

ΣW Γ 2W T(cid:17)
(cid:16)
(cid:19)

σ2
i + γ4

i +

2τ
ρ

γi

− 2

i − 2γ2
σ2

i σi + γ4

i +

tr(cid:0)Σ2(cid:1) − 2 tr
n(cid:88)
n(cid:88)

i=1

min
Γ,W

(cid:18)
(cid:18)
(cid:18)

Γ

min

n(cid:88)

i=1

i=1

min
γi≥0

ρ
2

min
Γ,W

=

ρ
2

≥ ρ
2

=

ρ
2

i − 2γ2
σ2

i σi + γ4

i +

n(cid:88)

i=1

γi

2τ
ρ

ijγ2

j σi

+ tr(cid:0)Γ 4(cid:1) +
n(cid:88)
n(cid:88)
(cid:19)
(cid:19)

j=1

i=1

γi

w2

2τ
ρ

2τ
ρ

γi

(15)

(16)

(17)

(18)

The inequality in (17) follows directly by applying H¨older’s inequality to (16) and using
the property that the column vectors wi are unitary.

Next, with L = Γ U T in (8) we have

||A − LT L||2F +τ||L||∗ =

||Σ − Γ 2||2F + τ||Γ||∗

ρ
2

ρ
2
(σi − γ2

n(cid:88)

(cid:16) ρ

2

i=1

=

(cid:17)

i )2 + τ γi

.

(19)

Finally, since the subproblems in (18) are separable in γi, its minimizer must be KKT-
points of the individual subproblems. As the constraints are simple non-negativity
constraints, these KKT points are either (positive) stationary points of the objective
functions or 0. It is simple to verify that the stationary points are given by the roots
of the cubic function pσi,τ /2ρ. Hence it follows that there exists a γ∗
∗
i ,

∗2
i )2 + τ γ

i − 2γ2
σ2

i such that

(σi − γ

(cid:19)

(cid:18)

(20)

i +

γi

2τ
ρ

≥ ρ
2

i σi + γ4
∀γi ≥ 0, which completes the proof.

ρ
2

Non-linear Dimensionality Regularizer for Solving Inverse Problems

17

References

1. Abrahamsen, T.J., Hansen, L.K.: Input space regularization stabilizes pre-images
for kernel pca de-noising. In: EEE International Workshop on Machine Learning
for Signal Processing. pp. 1–6 (2009)

2. Akhter, I., Sheikh, Y., Khan, S., Kanade, T.: Nonrigid structure from motion in
trajectory space. In: Advances in neural information processing systems. pp. 41–48
(2009)

3. Bakir, G.H., Weston, J., Sch¨olkopf, B.: Learning to ﬁnd pre-images. Advances in

neural information processing systems 16(7), 449–456 (2004)

4. Bishop, C.M., James, G.D.: Analysis of multiphase ﬂows using dual-energy gamma
densitometry and neural networks. Nuclear Instruments and Methods in Physics
Research Section A: Accelerators, Spectrometers, Detectors and Associated Equip-
ment 327(2), 580–593 (1993)

5. Blanz, V., Vetter, T.: A morphable model for the synthesis of 3d faces. In: 26th
annual conference on Computer graphics and interactive techniques. pp. 187–194
(1999)

6. Bregler, C., Hertzmann, A., Biermann, H.: Recovering non-rigid 3d shape from
image streams. In: IEEE Conference on Computer Vision and Pattern Recognition.
pp. 690–696 (2000)

7. Cabral, R., De la Torre, F., Costeira, J.P., Bernardino, A.: Unifying nuclear norm
and bilinear factorization approaches for low-rank matrix decomposition. In: In-
ternational Conference on Computer Vision (ICCV) (2013)

8. Cand`es, E.J., Recht, B.: Exact matrix completion via convex optimization. Foun-

dations of Computational mathematics 9(6), 717–772 (2009)

9. Cootes, T.F., Edwards, G.J., Taylor, C.J.: Active appearance models. IEEE Trans-

actions on pattern analysis and machine intelligence 23(6), 681–685 (2001)

10. Dai, Y., Li, H., He, M.: A simple prior-free method for non-rigid structure-from-
motion factorization. International Journal of Computer Vision 107(2), 101–122
(2014)

11. Dame, A., Prisacariu, V.A., Ren, C.Y., Reid, I.: Dense reconstruction using 3d
object shape priors. In: Computer Vision and Pattern Recognition. pp. 1288–1295.
IEEE (2013)

12. Fazel, M.: Matrix rank minimization with applications. Ph.D. thesis, Stanford Uni-

versity (2002)

13. Garg, R., Roussos, A., Agapito, L.: Dense variational reconstruction of non-rigid
surfaces from monocular video. In: Computer Vision and Pattern Recognition. pp.
1272–1279 (2013)

14. Garg, R., Roussos, A., Agapito, L.: A variational approach to video registration
with subspace constraints. International journal of computer vision 104(3), 286–314
(2013)

15. Geiger, A., Urtasun, R., Darrell, T.: Rank priors for continuous non-linear dimen-
sionality reduction. In: Computer Vision and Pattern Recognition. pp. 880–887.
IEEE (2009)

16. Gotardo, P.F., Martinez, A.M.: Kernel non-rigid structure from motion. In: IEEE

International Conference on Computer Vision. pp. 802–809 (2011)

17. Huang, D., Cabral, R.S., De la Torre, F.: Robust regression. In: European Confer-

ence on Computer Vision (ECCV) (2012)

18. Jolliﬀe, I.: Principal component analysis. Wiley Online Library (2002)

18

Garg et. al.

19. Kwok, J.Y., Tsang, I.W.: The pre-image problem in kernel methods. IEEE Trans-

actions on Neural Networks, 15(6), 1517–1525 (2004)

20. Lawrence, N.D.: Probabilistic non-linear principal component analysis with gaus-
sian process latent variable models. The Journal of Machine Learning Research 6,
1783–1816 (2005)

21. Mika, S., Sch¨olkopf, B., Smola, A.J., M¨uller, K.R., Scholz, M., R¨atsch, G.: Kernel

pca and de-noising in feature spaces. In: NIPS. vol. 4, p. 7 (1998)

22. Nguyen, M.H., De la Torre, F.: Robust kernel principal component analysis. In:

Advances in Neural Information Processing Systems (2009)

23. Nocedal, J., Wright, S.: Numerical optimization. Springer, New York (2006)
24. Poling, B., Lerman, G., Szlam, A.: Better feature tracking through subspace con-
straints. In: Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Con-
ference on. pp. 3454–3461. IEEE (2014)

25. Prisacariu, V.A., Reid, I.: Nonlinear shape manifolds as shape priors in level set
segmentation and tracking. In: Computer Vision and Pattern Recognition. pp.
2185–2192. IEEE (2011)

26. Recht, B., Fazel, M., Parrilo, P.A.: Guaranteed minimum-rank solutions of linear
matrix equations via nuclear norm minimization. SIAM review 52(3), 471–501
(2010)

27. Sanguinetti, G., Lawrence, N.D.: Missing data in kernel pca. In: Machine Learning:

ECML 2006, pp. 751–758. Springer (2006)

28. Sch¨olkopf, B., Smola, A., M¨uller, K.R.: Nonlinear component analysis as a kernel

eigenvalue problem. Neural computation 10(5), 1299–1319 (1998)

29. Tipping, M.E., Bishop, C.M.: Probabilistic principal component analysis. Journal
of the Royal Statistical Society: Series B (Statistical Methodology) 61(3), 611–622
(1999)

30. Wright, J., Ganesh, A., Rao, S., Peng, Y., Ma, Y.: Robust principal component
analysis: Exact recovery of corrupted low-rank matrices via convex optimization.
In: Advances in Neural Information Processing Systems, pp. 2080–2088 (2009)

31. Zhou, X., Yang, C., Zhao, H., Yu, W.: Low-rank modeling and its applications in

image analysis. ACM Computing Surveys (CSUR) 47(2), 36 (2014)

