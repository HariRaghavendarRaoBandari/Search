6
1
0
2

 
r
a

 

M
7
1

 
 
]
T
I
.
s
c
[
 
 

1
v
4
8
5
5
0

.

3
0
6
1
:
v
i
X
r
a

On the Multiplexing Gain of MIMO Phase

1

Noise Channels

Sheng Yang, Member, IEEE, Shlomo Shamai (Shitz), Fellow, IEEE,

Abstract

The capacity of a point-to-point multi-input-multiple-output (MIMO) channel with phase uncertainty (MIMO
phase noise channel) is still open. As a matter of fact, even the pre-log (multiplexing gain) of the capacity in the
high signal-to-noise ratio (SNR) regime is unknown in general. We make some progresses in this direction for two
classes of such channels. With phase noise on the individual paths of the channel (model A), we show that the
2 , which implies that the capacity does not scale with the channel dimension at high SNR.
multiplexing gain is 1
With phase noise at both the input and output of the channel (model B), the multiplexing gain is upper-bounded by
2 (cid:99)}, where nt and nr are the number of transmit and
2 min{nt, (nr − 2)++ 1}, and lower-bounded by 1
1
2 min{nt, nr} without receive phase noise, and
receive antennas, respectively. The multiplexing gain is enhanced to 1
2 min{2nt − 1, nr} without transmit phase noise. In all the cases of model B, the multiplexing gain scales linearly
to 1
with min{nt, nr}. Our main results rely on the derivation of non-trivial upper and lower bounds on the capacity of
such channels.

2 min{nt,(cid:98) nr+1

I. INTRODUCTION

The capacity of a point-to-point multiple-input-multiple-output (MIMO) Gaussian channel is well known in the
coherent case, i.e., when the channel state information is available at the receiver [1], [2]. The capacity of the
non-coherent MIMO channels, however, is still open in general. Nevertheless, asymptotic results of such channels,
e.g., at high signal-to-noise ratio (SNR), have been obtained in some important cases.

In the seminal paper [3], Lapidoth and Moser proposed a powerful technique, called the duality approach, that
can be applied to a large class of fading channels and derived the exact high SNR capacity up to an o(1) term. In
particular, when the differential entropy of the channel matrix is ﬁnite, i.e., h(HHH) > −∞, it was shown in [3] that
the pre-log (a.k.a. multiplexing gain), of the capacity is 0 and the high-SNR capacity is log log SNR + χ(HHH) + o(1)
where χ(HHH) is the so-called fading number of the channel. In addition, capacity upper and lower bounds for the
MIMO Rayleigh and Ricean channels were obtained and shown to be tight at both low and high SNR regimes.
In [4], Zheng and Tse showed that for block fading MIMO Rayleigh channels, the pre-log is M∗(1− M∗/T ) where

T is the channel coherence time and M∗ (cid:44) min(cid:8)nt, nr,(cid:98) T

2 (cid:99)(cid:9). In this work, we are interested in the MIMO phase

noise channels in which the phases of the channel coefﬁcients are not perfectly known.

S. Yang is with LSS, CentraleSupélec, 3 rue Joliot-Curie, 91190 Gif-sur-Yvette, France. (e-mail: sheng.yang@centralesupelec.fr)
S. Shamai (Shitz) is with Technion-Israel Institute of Technology, Haifa, Israel. (e-mail: sshlomo@ee.technion.ac.il)

March 18, 2016

DRAFT

2

Applying the duality approach and the “escape-to-inﬁnity” property of the channel input, Lapidoth characterized
the high-SNR capacity of the discrete-time phase noise channel in the single-antenna case [5]. It was shown in
[6] that the capacity-achieving input distribution is in fact discrete. Recently, capacity upper and low bounds of
the single-antenna channels with Wiener phase noise have been extensively studied in the context of optical ﬁber
and microwave communications (see [7], [8], [9] and the references therein). In these works, the upper bounds are
derived via duality and lower bounds are computed numerically using the auxiliary channel technique proposed
in [10]. In particular, in [9], Durisi et al. investigated the MIMO phase noise channel with a common phase noise, a
scenario motivated by the microwave link with centralized oscillators. The SIMO and MISO channels with common
and separate phase noises are considered in [11]. The 2 × 2 MIMO phase noise channel with independent transmit
and receive phase noises at each antenna was studied in [12], where the authors showed that the multiplexing gain
is 1
2 for a speciﬁc class of input distributions. For general MIMO channels with separate phase noises, to the best
of our knowledge, even the multiplexing gain is unknown.

In this work, we make some progresses in this direction. We consider two classes of discrete-time stationary and
ergodic MIMO phase noise channels: model A with individual phase noises on the entries of the channel matrix, and
model B with individual phase noises at the input and the output of the channel instead. The phase noise processes
in both models are assumed to have ﬁnite differential entropy rate. For model A, we obtain the exact multiplexing
2 for any channel dimension, which implies that the capacity does not scale with the channel dimension at high
gain 1
SNR. For model B with both transmit and receive phase noises, we show that the multiplexing gain is upper-bounded
2 (cid:99)}, where nt and nr are the number of transmit
by 1
and receive antennas, respectively. The upper and lower bounds coincide for nr ≤ 3 or nr ≥ 2nt − 1. Further, when
2 min{nt, nr}. If
receive phase noise is absent, the multiplexing gain is improved and we obtain the exact value of 1
the transmit phase noise is absent instead, the multiplexing gain becomes 1

2 min{nt, (nr − 2)++ 1}, and lower-bounded by 1

2 min{nt,(cid:98) nr+1

2 min{2nt − 1, nr}.

The main technical contribution of this paper is two-fold. First, we derive a non-trivial upper bound on the capacity
of the MIMO phase noise channel with separate phase noises. The novelty of the upper bound lies in the ﬁnding of
a suitable auxiliary distributions with which we apply the duality upper bound proposed by Lapidoth and Moser [3].
It is worth mentioning that, the class of single-variate Gamma output distributions, as the essential ingredient that led
to the tight capacity upper bounds on previously studied channels, are not suitable for MIMO phase noise channels
in general. In this paper, we introduce a class of multi-variate Gamma distributions that, combined with the duality
upper bound, allows us to obtain a complete pre-log characterization for model A and partially for model B. The
second contribution is the derivation of the capacity lower bounds for model B, based on the remarkable property of
the differential entropy of the output vector in this channel. Namely, we prove that, at high SNR, the pre-log of
the said entropy can go beyond the rank of the channel matrix, min{nt, nr}, and scales as nr log SNR as long as
nr ≤ 2nt − 1. The upper and lower bounds suggest that, with nr ≥ 2nt − 1 receive antennas, nt transmitted real
symbols can be recovered at high SNR. This result has an interesting interpretation based on dimension counting.
Let us consider the example of independent and memoryless transmit and receive phase noises uniformly distributed
in [0, 2π). In this case, phases of the input and the output do not contain any useful information, only the amplitudes

March 18, 2016

DRAFT

3

matter. Note that the nr output amplitudes are (non-linear) equations of 2nt − 1 unknowns, namely, the nt input
amplitudes and the nt − 1 relative input phases, assuming the additive noises are negligible at high SNR. It is
now not too hard to believe that with nr = 2nt − 1 equations, the receiver can successfully decode the nt input
amplitudes by solving the equations. This is however not possible with nr < 2nt − 1, in which case there are too
many unknowns as compared to the number of equations. Nonetheless, we can reduce the number of active transmit
2 (cid:99). A
antennas to n(cid:48)
formal proof in Section VI validates such an argument.

t − 1 ≤ nr, which means that the achievable multiplexing gain is n(cid:48)

t < nt such that 2n(cid:48)

t

2 ≤ 1

2(cid:98) nr+1

The remainder of the paper is organized as follows. The system model and main results are presented in Section II.
Some preliminaries useful for the proof of the main results are provided in Section III. The upper bounds are derived
in Section IV and Section V. We prove the lower bound for model B in section VI. Concluding remarks are given
in Section VII. Most of the proofs are presented in the main body of the paper, with some details deferred to the
Appendix.

Notation

II. SYSTEM MODEL AND MAIN RESULTS

Throughout the paper, we use the following notational conventions. For random quantities, we use upper case
letters, e.g., X, for scalars, upper case letters with bold and non-italic fonts, e.g., VVV, for vectors, and upper case letter
with bold and sans serif fonts, e.g., MMM, for matrices. Deterministic quantities are denoted in a rather conventional
way with italic letters, e.g., a scalar x, a vector vvv, and a matrix MMM. Logarithms are in base 2. The Euclidean norm of
a vector and a matrix is denoted by (cid:107)vvv(cid:107) and (cid:107)MMM(cid:107), respectively. The transpose and conjugated transpose of MMM are
MMM T and MMM H, respectively. HHHHHHHHH† is the pseudo-inverse of a tall matrix HHHHHHHHH. The argument (phase) of a complex value x
is denoted by ∠x ∈ [0, 2π). We use AAA ◦ BBB to denote the Hadamard (point-wise) product between vectors/matrices.
xn+k
n+1 is a k-tuple or a column vector of (xn+1, . . . , xn+k); for brevity sometimes xk replaces xk
1. For convenience,
wherever confusion is improbable, elementary scalar functions applied to a vector, e.g., |xxx| or cos(θθθ), stand for a
point-wise map on each element of the vector, and return a vector with the same dimension as the argument. We
use (θ)2π to denote (θ mod 2π), and (x)+ = max{x, 0}. We also use c0 to represent a bounded constant whose
value is irrelevant but may change at each occurrence. Similarly, cH is a constant that may depend on HHH but the
value is irrelevant and bounded for almost all HHH.

A. Channel model

In this paper, we are interested in a class of discrete-time MIMO phase noise channels with nt transmit antennas

and nr receive antennas, deﬁned by

YYYt = (HHH ◦ ejΘΘΘt) xxxt + ZZZt,

(1)
(cid:80)N
where the deterministic channel matrix HHH belongs to a bounded and continuous subset H ⊂ Cnr×nt; xxxt ∈ Cnt×1 is
t=1 (cid:107)xxxt(cid:107)2 ≤ P ; the additive noise process {ZZZt}
the input vector at time t, with the average power constraint 1
N

t = 1, 2, . . . , N,

March 18, 2016

DRAFT

4

Fig. 1. Two models considered in this work: model A with path phase noise, and model B with transmit/receive phase noise.

is assumed to be spatially and temporally white with ZZZt ∼ CN (0, IIInr); ΘΘΘt is the matrix of phase noises on the
individual entries of HHH at time t; the phase noise process {ΘΘΘt} is stationary and ergodic, and is independent of
the additive noise process {ZZZt}. Both {ZZZt} and {ΘΘΘt} are unknown to the transmitter and the receiver. Since the
additive noise power is normalized, the transmit power P is identiﬁed with the SNR throughout the paper. It is
worth mentioning that the results in this paper apply to almost all HHH in H, excluding a subset of ill-conditioned

matrices. The end-to-end channel is captured by the random channel matrix HHH (cid:44)(cid:2)hikeΘik(cid:3)

i,k.

In this paper, we consider two types of phase noise according to the spatial structures, as shown in Fig. 1:

• Model A refers to channels with phase uncertainty on the individual paths (path phase noise), such that the

sequence {ΘΘΘt} has ﬁnite entropy rate

h({ΘΘΘt}) > −∞.

(2)

It corresponds to the case where the phase information of the channel cannot be obtained accurately, e.g., in
optical ﬁber communications. This model covers the channel with spatially independent phase noises as a
special case.

(cid:3)nr

vector ΘΘΘT (cid:44)(cid:2)ΘT,i

(cid:3)nt
i=1 contains the nt phase noises at the transmit antennas, and ΘΘΘR (cid:44)(cid:2)ΘR,k

• Model B refers to channels with phase noises at the input and/or output, i.e., Θik = ΘR,i + ΘT,k. The
k=1 is the vector
of the nr phase noises at the receive antennas. This model captures the phase corruption at both the transmit
and receive RF chains, e.g., caused by imperfect oscillators. We consider three cases of model B:
B1) with both transmit and receive phase noises such that h({ΘΘΘT,t, ΘΘΘR,t}) > −∞;
B2) with only transmit phase noise such that h({ΘΘΘT,t}) > −∞;
B3) with only receive phase noise such that h({ΘΘΘR,t}) > −∞.
Note that model B1 covers the case where both the transmitter and receiver use separate (and imperfect)
oscillators for different antennas, whereas models B2 and B3 correspond to the case with centralized oscillators
at one side and separate oscillators at the other side.

March 18, 2016

DRAFT

ejΘR,nrejΘT,ntYnrZnrhnr￿hnrntYnrZnrhnrnth￿nthnr￿ejΘnr￿ejΘnrntXntejΘ￿nt×××++×++×××ejΘT,￿ejΘR,￿Z￿Z￿ejΘ￿￿×h￿￿h￿￿X￿X￿Y￿Y￿Model AModel Bh￿ntXnt5

(3)

(4)

Fig. 2. Multiplexing gain of the MIMO phase noise channels.

The capacity of such a stationary and ergodic channel is [3], [13]

C(P ) (cid:44) lim

N→∞ sup

1
N

I(XXXN ; YYYN ),

where the supremum is taken over all distributions with the average power constraint

E(cid:2)(cid:107)XXXk(cid:107)2(cid:3) ≤ P.

1
N

N(cid:88)

k=1

Our work focuses on the multiplexing gain r of such a channel, deﬁned as the pre-log of the capacity C(P ) as

P → ∞,

B. Main results

r (cid:44) lim
P→∞

C(P )
log P

.

(5)

The main results of this work are summarized as follows, and are illustrated in Fig. 2. First, the case with common

phase noise is rather straightforward from [9].
Proposition 1. With common phase noise, i.e., ΘΘΘt = Θt111nr×nt and h({Θt}) > −∞, the multiplexing gain is
min{nt, nr} − 1

2 , for almost every HHH.

Proof: The proof is provided in Appendix A.

Then our new results are on channels with separate phase noises either on the individual paths (model A) or at

the input/output (model B) of the channel.

Theorem 1. The multiplexing gain of model A is 1

2 , for almost every HHH.

The above result shows that extra transmit and receive antennas do not improve the multiplexing gain of a channel
with phase uncertainty on each path of the channel. The achievability of the single-antenna case was shown in [5].
Our main contribution lies in the converse, as will be shown in Section IV.

Theorem 2. For almost every HHH, the multiplexing gain of model B is

March 18, 2016

DRAFT

￿￿min{￿nt−￿,nr}￿￿min{nt,nr}Model APath - PN￿￿Model B1Tx & Rx - PNModel B2 Tx - PNModel B3Rx - PNCommon PNmin￿nt,nr￿−￿￿≤￿￿min￿nt,(nr−￿)++￿￿≥￿￿min{nt,￿nr+￿￿￿}6

• upper-bounded by 1

2 min{nt,(cid:98) nr+1
receive phase noises, the upper bound is achievable when nr ≤ 3 or nr ≥ 2nt − 1;

2 min{nt, (nr − 2)+ + 1}, and lower-bounded by 1

2 (cid:99)} with both transmit and

• min{ nr
• min{ nr

2 , nt
2 , nt − 1

2 }, with only transmit phase noise;

2}, with only receive phase noise.

Interestingly, the multiplexing gain of model B depends on the number of transmit and receive antennas differently,

which is rarely the case for previously studied point-to-point MIMO channels.

Remark II.1. As shown in Fig. 2, transmit phase noise is more detrimental than receive phase noise, and strictly
so when nr > nt > 1. Intuitively, with transmit phase noise each transmitted symbol is accompanied by a different
phase noise symbol, which means that no more than half of the total spatial degrees of freedom is available for

useful signal. On the other hand, with receive phase noise, although half of the received signal dimension is occupied

by phase noises, it is enough to increase the number of receive antennas to recover almost all transmitted symbols.

Remark II.2. Obviously, the multiplexing gain of model B1 is upper-bounded by that of models B2 and B3. Such a
“trivial” upper bound is given by min{ nt
2 }. When nr ≤ nt, the optimal multiplexing gain
is achievable with phase
is n

2 with phase noises at either side of the channel, whereas no more than (nr−2)++1

2} = min{ nt

2 , nt − 1

2 , nr

2 , nr

2

noises at both sides. These are the cases for which model B1 is strictly “worse” than both models B2 and B3. When
nr ≥ 2nt − 1, with transmit phase noise, the optimal multiplexing gain is nt
2 regardless of the presence of receive
phase noise.

Remark II.3. Theorem 2 shows that, when nt = nr = 2 and 3, the exact multiplexing gain of model B1 is (nr−2)++1
2 , respectively. These are the
which gives 1
two cases of model B1 for which we obtain exact multiplexing gain that is strictly lower than that of models B2

2 and 1, respectively. In contrast, the trivial upper bound provides 1 and 3

2

and B3.

The remainder of the paper is dedicated to the proof of the main results. We start with some mathematical

preliminaries.

Deﬁnition 1 (Multivariate Gamma distribution [14]). The n-variate Gamma distribution has the following density
function

III. PRELIMINARIES

p(sss) = s1

α1−1(s2 − s1)α2−1 ··· (sn − sn−1)αn−1
· gααα µα1+···+αn exp(−µsn),

(6)
for 0 < s1 < s2 < ··· < sn, and 0 elsewhere, with µ > 0 and αi > 0, i = 1, . . . , n; gααα is the normalization factor
which only depends on ααα and n. When n = 1, we have the standard single-variate Gamma distribution,

March 18, 2016

p(s) = gα µαsα−1 exp(−µs),

s > 0, α > 0.

(7)

DRAFT

Lemma 1 (Monotonicity of E
of freedom k and noncentrality parameter λ, denoted as X ∼ χ2
increasing with λ ≥ 0, for any k ∈ N.

k(λ)

χ2

(cid:2)log X(cid:3) over λ). Let X be a non-central Chi-square distribution with degrees

7

k(λ). Then the expected logarithm of X is strictly

Proof: The case with k = 2n is known and has been proved in [3]. In the following, we provide a simple
proof for the general case of k, although we are only interested in the case k = 1 later in the paper. Let us deﬁne
fk(λ) (cid:44) E

k(λ) is [15]

χ2

k(λ)

(cid:2)log X(cid:3). The probability density function of χ2
(cid:1)l
2(cid:0) λ

e− λ
l!

2(cid:0) λ

k(λ)(x) =

∞(cid:88)

(cid:1)l

pχ2

pχ2

l=0

2

∞(cid:88)

l=0

e− λ
l!

2

fk(λ) =

fk+2l(0).

Then it readily follows from the deﬁnition of fk(λ) that

k+2l(0)(x),

x ≥ 0.

(8)

(9)

To prove that fk(λ) is increasing with λ, it is enough to show that the derivative of fk(λ) with respect to λ is
positive. Indeed,

∞(cid:88)

(cid:1)l

2(cid:0) λ

2

e− λ
l!

fk+2(l+1)(0)

fk(λ) +

k(λ) = − 1
f(cid:48)
2
(fk+2(λ) − fk(λ))

1
2

l=0

=

(cid:0)E(cid:2)log(X + Y )(cid:3) − E(cid:2)log X(cid:3)(cid:1)

1
2
1
2

=

where we used the fact that if X ∼ χ2

2(0), then X + Y ∼ χ2

k+2(λ).

> 0,
k(λ) and Y ∼ χ2

Lemma 2 (Change of variables [13]). Let YYY = f (XXX) with a bijective map f : Rm → Rm. Then

h(YYY) = h(XXX) + E(cid:2)log |det(JJJ)|(cid:3),

where JJJ (cid:44)(cid:104) ∂Yk

∂Xl

(cid:105)

is the Jacobian matrix.

k,l=1,...,m

Lemma 3. If each element of the n-vector XXX is circularly symmetric with independent phases, and the probability
density function (pdf) of XXX exists with respect to the Lebesgue measure, then

n(cid:89)

p|X|(|xxx|) = 2π

|xi| pX (xxx)

i=1

p|X|2 (|xxx|2) = π pX (xxx).

h(XXX) = h(|XXX|) +

E(cid:2)log |Xi|(cid:3) + n log 2π

n(cid:88)

i=1

= h(|XXX|2) + n log π.

Further, if h(XXX) > −∞, we have

March 18, 2016

(10)

(11)

(12)

(13)

(14)

(15)

(16)

(17)

(18)

DRAFT

Lemma 4. Let XXX ∈ Cn with h(XXX) > −∞. Then

Let ΘΘΘ ∈ [0, 2π)n be independent of XXX and h(ΘΘΘ) > −∞. Then

h(XXX) = h(|XXX|2) + h(∠XXX ||XXX|) − n.

|h(ejΘΘΘ ◦ XXX) − h(|XXX|2)| ≤ c0.

Proof: Applying Lemma 2 twice, we readily obtain (19)

n(cid:88)

h(XXX) = h(|XXX|, ∠XXX) +

E log |Xk|

k=1

= h(|XXX|) + h(∠XXX ||XXX|) +

E log |Xk|

n(cid:88)

= h(|XXX|2) + h(∠XXX ||XXX|) − n.

k=1

To prove (20), we introduce ΦΦΦ that is uniformly distributed in [0, 2π)n and independent of XXX and ΘΘΘ, then

h(ejΘΘΘ ◦ XXX) = h(ej(ΘΘΘ+ΦΦΦ) ◦ XXX| ΦΦΦ)

≤ h(ejΦΦΦ(cid:48) ◦ XXX)
= h(|XXX|2) + n log π,

where ΦΦΦ(cid:48) (cid:44) (ΘΘΘ + ΦΦΦ)2π is uniformly distributed in [0, 2π)n, and from (19),

h(ejΘΘΘ ◦ XXX) = h(|XXX|2) + h((∠XXX + ΘΘΘ)2π ||XXX|) − n

≥ h(|XXX|2) + h((∠XXX + ΘΘΘ)2π ||XXX|, ∠XXX) − n
= h(|XXX|2) + h(ΘΘΘ) − n.

Hence, (20) holds with the constant c0 corresponding to max{|h(ΘΘΘ) − n|, n log π}.
Lemma 5. For any Θ ∈ [0, 2π) with h(Θ) > −∞,

E(cid:2)log | sin(Θ)|(cid:3) >

h(Θ) − log(cid:0)B( 1−α

2 )(cid:1)

,

where B(x, y) is the Beta function. Thus, E(cid:2)log | sin(Θ)|(cid:3) > −∞. Let ΘΘΘ ∈ [0, 2π)n. If h(ΘΘΘ) > −∞, then

∀ α ∈ (0, 1),

α

2 , 1

h(cos(ΘΘΘ)) > −∞.

8

(19)

(20)

(21)

(22)

(23)

(24)

(25)

(26)

(27)

(28)

(29)

(30)

(31)

Proof: To prove (30), we introduce an auxiliary distribution with density q(θ) = β| sin(θ)|−α, θ ∈ [0, 2π), with

. Then it follows that h(Θ) + E(cid:2)log(q(Θ))(cid:3) = −D(p(cid:107) q) ≤ 0 where D(·(cid:107)·) is the

α ∈ (0, 1) and β (cid:44)

1
2B( 1−α
2 , 1
2 )

March 18, 2016

DRAFT

Kullback-Leibler divergence, which yields (30). We proceed to prove (31),

h(cos(ΘΘΘ)) ≥ h(cos(ΘΘΘ)| Ω)

= h(ΘΘΘ| Ω) +

n(cid:88)

k=1

E(cid:2)log | sin(Θk)|(cid:3)
n(cid:88)

E(cid:2)log | sin(Θk)|(cid:3)

= h(ΘΘΘ) − I(Ω; ΘΘΘ) +

> −∞,

k=1

where we partition [0, 2π)n in such a way that cos(ΘΘΘ) is a bijective function of ΘΘΘ in each partition indexed by Ω;
the ﬁrst equality is from Lemma 2; the last inequality is from the boundedness of h(ΘΘΘ), the fact that Ω only takes a
ﬁnite number of values, and the application of (30).

Lemma 6. Let VVV ∈ Rm×1 with h(VVV) > −∞ and E(cid:2)(cid:107)VVV(cid:107)2(cid:3) < ∞. Then
E(cid:2)log |VVVTxxx|(cid:3) > −∞.

inf

xxx∈Rm: (cid:107)xxx(cid:107)=1

9

(32)

(33)

(34)

(35)

(36)

(37)

(38)

Proof: This is a straightforward adaptation of the result in [3, Lemma 6.7-f] for the complex case. The
real case can be proved by following the same steps. To be self-contained, we provide an alternative proof as
follows. Deﬁne Vx (cid:44) |VVVTxxx|2, and one can verify from the assumptions that h(Vx) > −∞ and E [Vx] ≤ ∞.
We introduce an auxiliary pdf q(Vx) based on the Gamma distribution deﬁned in (7) with some α ∈ (0, 1).

Then we have for any Vx ∈ R+, h(Vx) ≤ E(cid:2)− log q(Vx)(cid:3) = (1 − α)E(cid:2)log Vx
E(cid:2)log Vx

(cid:3) ≥ (1 − α)−1(h(Vx) − µE [Vx] − c0) > −∞.

(cid:3) + µE [Vx] + c0 which yields

Lemma 7. Let Θ ∈ [0, 2π) with h(Θ) > −∞ and be independent of some Z ∼ CN (0, 1), then for any given
β ∈ C,

(cid:12)(cid:12)h(βejΘ + Z) − log+|β|(cid:12)(cid:12) ≤ c0,
and (cid:12)(cid:12)h(|β + Z|)(cid:12)(cid:12) ≤ c(cid:48)

0.

Proof: First we prove (37). When |β| ≤ 1, we have log+|β| = 0. It follows that h(βejΘ + Z) ≥ h(βejΘ +
Z | Θ) = log(πe) and h(βejΘ + Z) ≤ log(πe(Var(βejΘ + Z))) ≤ log(2πe), which proves (37) for |β| ≤ 1. Next,
we assume that |β| > 1. It is without loss of generality to consider β ∈ R+. Let ZR and ZI be the real and imaginary
parts of Z, respectively. Then

h(βejΘ + Z) = h(β cos(Θ) + ZR)

+ h(β sin(Θ) + ZI | β cos(Θ) + ZR)

≥ h(β cos(Θ)) + h(ZI)

= log β + h(cos(Θ)) +

1
2

log(πe).

(39)

(40)

(41)

DRAFT

March 18, 2016

Since βejΘ + Z = ejΘ(β + ˜Z) where ˜Z (cid:44) Ze−jΘ ∼ CN (0, 1) is independent of Θ, we can apply (20) from
Lemma 4,

10

h(βejΘ + Z)
≤ h(β2 + |Z|2 + 2β|Z| cos(Θ − ∠Z)) + c0
= h(|Z|2 + 2β|Z| cos(Θ − ∠Z)) + c0
≤ 1
2
≤ log β +

log(cid:0)2πeVar(|Z|2 + 2β|Z| cos(Θ − ∠Z))(cid:1) + c0
log(cid:0)2πe(cid:0)Var(|Z|2) + Var(2|Z|)(cid:1)(cid:1) + c0,

1
2

(42)

(43)

(44)

(45)

where we use the condition β > 1. The lower bound (41) and upper bound (45) complete the proof of (37) for
|β| > 1. To prove (38), we introduce some Θ uniformly distributed in [0, 2π), then h(|β + Z|) = h(|βejΘ + Z|) =

h(βejΘ + Z) − E(cid:2)log(|β + Z|)(cid:3) − log 2π. It can be shown that (cid:12)(cid:12)E(cid:2)log(|β + Z|)(cid:3) − log+|β|(cid:12)(cid:12) ≤ c0. To see this,
we write E(cid:2)log(|β + Z|)(cid:3) = 1
E(cid:2)log(|Z|2 + |β|2 + 2|βZ| cos(∠β∗Z))(cid:3) where ∠β∗Z is uniformly distributed in
[0, 2π) and independent of the other variables. Taking expectation over ∠β∗Z, we obtain E(cid:2)log(|β + Z|)(cid:3) =
E(cid:2)log(|Z|2 + |β|2)(cid:3) + c0, since 1
(cid:82) 2π
Jensen’s inequality with expectation over Z, we have E(cid:2)log(|β + Z|)(cid:3) ≤ 1
E(cid:2)log(|Z|2 + |β|2)(cid:3) + c0 ≥
Using the monotonicity of the logarithmic function, we also have E(cid:2)log(|β + Z|)(cid:3) = 1
0. Finally, since both h(βejΘ + Z) and E(cid:2)log(|β + Z|)(cid:3) are “close” to
max{log |β|, E(cid:2)log |Z|(cid:3)} + c0 ≥ log+|β| + c(cid:48)

, for all a ≥ b > 0. Then, applying
2 log(1 + |β|2) + c0 ≤ log+|β| + c(cid:48)
0.

log(a + b cos θ)dθ = log a+

a2−b2
2

√

2π

0

1
2

2

2

log+|β|, they are “close” to each other due to the triangle inequality. This completes the proof of (38).

Lemma 8. For any p, X > 0, we have

| log+(pX) − log+X| ≤ |log p|,

E(cid:2)log+X(cid:3) ≤ p−1 log+(cid:0)E [X p](cid:1) + p−1.

and

(46)

(47)

Proof: To show (46), it is enough to verify that log+(pX)−log+X ≤ log p when p ≥ 1 and log+X−log+(pX) ≤
− log p when p < 1, which completes the proof. The inequality (47) is based on Jensen’s inequality. Speciﬁcally, we

have E(cid:2)log+X(cid:3) = p−1E(cid:2)log+X p(cid:3) ≤ p−1E(cid:2)log(1 + X p)(cid:3) ≤ p−1 log(cid:0)1 + E [X p](cid:1) ≤ p−1 log+(cid:0)E [X p](cid:1) + p−1.

The capacity C(P ) in (3) of a stationary and ergodic channel is upper-bounded by the capacity of the corresponding

IV. CAPACITY UPPER BOUND FOR MODEL A

memoryless channel up to a constant term. Following the footsteps of [3], [5], we have

1
N

I(XXXN ; YYYN ) =

1
N

≤ 1
N

I(XXXN ; YYYk | YYYk−1)

I(XXXk; YYYk) + I(ΘΘΘN ; ΘΘΘN−1)

N(cid:88)
N(cid:88)

k=1

k=1

≤ sup I(XXX; YYY) + c0,

March 18, 2016

(48)

(49)

(50)

DRAFT

where sup I(XXX; YYY) is the capacity of a memoryless phase noise channel with the same temporal marginal distribution

as the original channel, and the supremum is over all input distributions such that E(cid:2)(cid:107)XXX(cid:107)2(cid:3) ≤ P . Since we are

mainly interested in the multiplexing gain, the constant c0 does not matter, and it is thus without loss of optimality
to consider the memoryless case in this section.

The main ingredients of the proof are the genie-aided bound and the duality upper bound. In the following, we

11

detail the ﬁve steps that lead to Theorem 1.

A. Step 1: Genie-aided bound

Let us deﬁne the auxiliary random variable U as the index of the strongest input entry, i.e.,1

(51)
Thus, we use XU to denote the element in XXX with the largest module. It is obvious that U ↔ XXX ↔ YYY form a
Markov chain, and that U does not contain more than log nt bits. Assuming that a genie provides U to the receiver,
we obtain the following upper bound

U (cid:44) arg max
1≤i≤nt

|Xi|.

I(XXX; YYY) ≤ I(XXX; YYY, U )

= I(XXX; YYY | U ) + I(U ; XXX)
≤ I(XXX; YYY | U ) + H(U )
≤ I(XXX; YYY | U ) + log nt.

B. Step 2: Canonical form

Deﬁnition 2 (Canonical channel). We deﬁne the canonical form u, u = 1, . . . , nt, of the channel HHH as

GGG(u) (cid:44) diag(cid:0)h−1
(cid:123)(cid:122)
1,u, . . . , h−1

(cid:124)

(cid:1)
(cid:125)

HHH.

nr,u

AAAu

(52)

(53)

(54)

(55)

(56)

Note that the elements in the u th column of GGG(u) has normalized modules. Now, with the information U from

the genie, the receiver can convert the original channel into one of the canonical forms, namely, the form U.

I(XXX; HHHXXX + ZZZ| U )

= I(XXX; AAAUHHHXXX + AAAUZZZ| U )
≤ I(XXX; AAAUHHHXXX + aZZZ| U )
= I(a−1XXX; a−1GGG(U )XXX + ZZZ| U )
= I( ˜XXX; GGG(U )

˜XXX + ZZZ| U ),

1When there are more than one such elements, we pick an arbitrary one.

March 18, 2016

(57)

(58)

(59)

(60)

DRAFT

where a (cid:44) mink,u |h−1
we deﬁne

k,u|; (58) is due to the fact that reducing the additive noise increases the mutual information;

12

and accordingly,

˜XXX (cid:44) a−1XXX,

WWW (cid:44) GGG(u)

˜XXX + ZZZ.

In the following, we focus on upper-bounding the mutual information I( ˜XXX; WWW | U ). Note that

I( ˜XXX; WWW | U ) = h(WWW | U ) − h(WWW | ˜XXX, U )

= h(WWW | U ) − h(WWW | ˜XXX),

(61)

(62)

(63)

(64)

where the last equality comes from the fact that U is a function of XXX and thus a function of ˜XXX, since ˜XXX is simply a
scaled version of XXX. Therefore, it is enough to lower-bound h(WWW | ˜XXX) and upper-bound h(WWW | U ) separately.

C. Step 3: Lower bound on h(WWW | ˜XXX)
Lemma 9. For model A, we have

h(WWW | ˜XXX) ≥ nr E(cid:2)log+| ˜XU|(cid:3) + nr E(cid:2)log+| ˜XV |(cid:3) + cH ,

where ˜XU and ˜XV have the largest and second largest modules in ˜XXX, respectively.

(65)

Proof: See Appendix B.

It is worth mentioning that the above bound depends not only on the strongest but also on the second strongest

input of the channel.

D. Step 4: Upper bound on h(WWW | U )

Upper-bounding h(WWW | U ) by a non-trivial but tractable function of the input distribution is hard in general. A
viable way for that purpose is through an auxiliary distribution, also called the duality approach [3]. Namely, for
any2 pdf q(www), we have

(68)
due to the non-negativity of the Kullback-Leibler divergence D(pWWW|U =u (cid:107) q). Hence, the key is to choose a proper
auxiliary pdf q(www) in order to obtain a tight upper bound on the capacity of our channel. The commonly used

2Formally, we should state that the probability measure Q corresponding to the density q(www) is such that P (· | U = u) is absolutely continuous

with respect to Q. Throughout the paper, for brevity, we implicitly make the assumption to avoid such formalities.

March 18, 2016

DRAFT

h(WWW | U ) = E(cid:2)− log p(WWW | U )(cid:3)
= E(cid:2)− log q(WWW)(cid:3) − EU
≤ E(cid:2)− log q(WWW)(cid:3)

(cid:2)D(pWWW|U =u (cid:107) q)(cid:3)

(66)

(67)

13

auxiliary distributions for MIMO channels are mostly related to the class of isotropic distributions [3], [5], [9].
Unfortunately, the isotropic distributions are not suitable in our case. To see this, let us assume that an isotropic
output WWW was indeed close to optimal. On the one hand, the pdf of an isotropic output WWW would only depend
on the norm (cid:107)WWW(cid:107) which would be dominated by the largest input entry XU at high SNR. Therefore, the value

of E(cid:2)− log q(WWW)(cid:3) would be insensitive to the number of active input entries. On the other hand, the lower bound
(65). Therefore, with an isotropic distribution q(www), the capacity upper bound E(cid:2)− log q(WWW)(cid:3) − h(WWW | ˜XXX) would

on the conditional entropy h(WWW | ˜XXX) is increasing with both of the largest input entries XU and XV , according to

become larger when the second strongest input went to zero, i.e., only one transmit antenna was active. But this is
in contradiction with the isotropic assumption, since if only one transmit antenna was active, then the output entries
would be highly correlated and the output distribution would be far from being isotropic.

In light of the above discussion, we are led to think that a good choice of q(www) should reﬂect not only the
strongest input entry, but also the weaker ones. We adopt the following pdf built from the multivariate Gamma
distribution in Deﬁnition 1,

q(www) =

nr(cid:89)

(cid:0)| ˆwi|2 − | ˆwi−1|2(cid:1)αi−1

| ˆw1|2(α1−1)

gααα
nr!
· exp(−µ| ˆwnr|2)µα1+···+αnr , www ∈ Cnr,

i=2

(69)

where ˆw1, . . . , ˆwnr are the ordered version of wi’s with increasing modules. Essentially, we let each Wi be circularly
symmetric and let the ordered version of (|W1|2, . . . ,|Wnr|2) follow the multivariate Gamma distribution deﬁned in
Deﬁnition 1. Applying (16) in Lemma 3 and the order statistics (whence the term nr!) [15], we can obtain the pdf
of WWW as written in (69). Remarkably, the differences between |Wi|2 and |Wj|2, i (cid:54)= j, are introduced into the upper
bound, which is crucial for bringing in the impact of individual input entries ˜Xi’s other than the strongest entry as
will be shown in the following.

Lemma 10. By choosing 0 < αi < 1, i = 1, . . . , nr, and µ = min{P −1, 1}, we have for model A,

E(cid:2)− log q(WWW)(cid:3)
nr(cid:88)
≤ nr(cid:88)
nr(cid:88)
(1 − αi)E(cid:2)log+| ˜XV |(cid:3) + cH ,

(1 − α1) +

αi log+P +

(cid:32)

i=1

i=1

+

(cid:33)

(1 − αi)

E(cid:2)log+| ˜XU|(cid:3)

(70)

where ˜XU and ˜XV are the strongest and second strongest elements in ˜XXX, respectively.

i=2

Proof: The calculation is straightforward from the pdf (69), details are provided in Appendix C.

March 18, 2016

DRAFT

E. Step 5: Upper bound for model A

Combining (64), (65), (68), and (70) from the previous steps, we have

αi

E(cid:2)log+| ˜XV |(cid:3) + cH

(1 − αi) − nr

αi log+P +

I( ˜XXX; WWW | U )

≤ nr(cid:88)

i=1

(cid:32) nr(cid:88)

i=2

+

≤ nr(cid:88)
(cid:32) nr(cid:88)

i=1

≤

αi log+P +

(cid:33)

αi +

1
2

(cid:33)

(cid:33)

(cid:33)

(cid:32)
1 − 2α1 − nr(cid:88)
(cid:32)
1 − 2α1 − nr(cid:88)

i=2

i=2

log+P + c(cid:48)
H ,

E(cid:2)log+| ˜XU|(cid:3)

E(cid:2)log+| ˜XU|(cid:3) + cH

αi

14

(71)

(72)

(73)

(74)

(75)

(76)

(77)

where the inequality (72) comes from removing the negative term in (71); to obtain the last inequality, we apply

(47) in Lemma 8 with p = 2 and the power constraint E(cid:2)| ˜XU|2(cid:3) ≤ a2E(cid:2)(cid:107)XXX(cid:107)2(cid:3) ≤ a2P .

i=1

Finally, we conclude from (55) and (60) that, for model A,

I(XXX; YYY) ≤ I(XXX; YYY | U ) + c0
≤ I( ˜XXX; WWW | U ) + c0
≤

(cid:33)

αi +

(cid:32) nr(cid:88)
rA ≤ nr(cid:88)

αi +

1
2

i=1

∀ ααα ∈ (0, 1)nr.

1
2

,

log+P + cH

which implies that the multiplexing gain is upper-bounded by

By taking the inﬁmum over ααα, we have rA ≤ 1
2.

i=1

V. CAPACITY UPPER BOUND FOR MODEL B

In this section, we derive upper bounds for the three cases of model B, where the phase noises are on the
transmitter and receiver sides of the channel. As in the previous section, it is enough to consider the memoryless
case for our purpose.

A. Case B1: Transmit and receive phase noises

Note that the multiplexing gain of this case is upper-bounded by that of case B2 and case B3, since we can
enhance the channel by providing the information on the transmit or receive phase noises to both the transmit
and receiver. In other words, the upper bound min{ nr
2 } is still valid for this case. In the
2 , nt
with the duality upper bound using the
following, we show that we can tighten the upper bound nr
multi-variate Gamma distribution. The proof is in the same vein as the proof for model A. Speciﬁcally, the ﬁrst four

2 to (nr−2)++1

2} = min{ nr

2 , nt − 1

2 , nt

2

March 18, 2016

DRAFT

steps are exactly the same as for model A, except for Step 3 in which the conditional entropy has a different lower
bound, as shown below.

15

Lemma 11. For model B1, we have

h(WWW | ˜XXX) ≥ nr E(cid:2)log+| ˜XU|(cid:3) + E(cid:2)log+| ˜XV |(cid:3) + cH ,

where ˜XU and ˜XV have the largest and second largest modules in ˜XXX, respectively.

Proof: See Appendix B.

Applying (64), (68), (70), and (78), we have

I( ˜XXX; WWW | U )

i=2

2

i=1

+

i=1

i=1

(cid:33)

αi

i=2

αi log+P +

(1 − αi) − 1

≤ nr(cid:88)

E(cid:2)log+| ˜XU|(cid:3)

E(cid:2)log+| ˜XV |(cid:3) + cH

(cid:32)
1 − 2α1 − nr(cid:88)
(cid:32) nr(cid:88)
(cid:33)
≤ nr(cid:88)
αi log+P + E(cid:2)log+| ˜XU|(cid:3)
+ (nr − 2)+ E(cid:2)log+| ˜XV |(cid:3) + cH
≤ nr(cid:88)
αi log+P +(cid:0)(nr − 2)+ + 1(cid:1) E(cid:2)log+| ˜XU|(cid:3) + cH
≤(cid:16) (nr − 2)+ + 1
(cid:32) nr(cid:88)
rB ≤ nr(cid:88)

(nr − 2)+ + 1

log+P + c(cid:48)
H .

∀ ααα ∈ (0, 1)nr.

log+P + c(cid:48)

H

nr(cid:88)

i=1

(cid:17)

2

αi +

i=1

+

αi

(cid:33)

αi +

,

2

Therefore, we conclude from (55), (60), and (81) that, for model B1,

I(XXX; YYY) ≤

which implies that the multiplexing gain is upper-bounded by
(nr − 2)+ + 1

Taking the inﬁmum over ααα, we have rB ≤ (nr−2)++1

2

.

i=1

(78)

(79)

(80)

(81)

(82)

(83)

B. Case B2: Transmit phase noise

In this case, the received signal is YYY = HHH(ejΘΘΘT ◦ XXX) + ZZZ. The channel is characterized by the random matrix

HHH = HHHdiag{ejΘΘΘT}. We shall show that the upper bound is min(cid:8) nt

(cid:9). First, with more receive antennas than

2 , nr

2

transmit antennas, i.e., when nr ≥ nt, we can inverse the channel without losing information,

I(XXX; YYY) = I(XXX; ejΘΘΘT ◦ XXX + HHH†ZZZ)

≤ I(XXX; ejΘΘΘT ◦ XXX + ˜ZZZ),

(84)

(85)

DRAFT

March 18, 2016

where ˜ZZZ ∼ CN (0, σ2
min(HHH†)IIInt), with σmin(HHH†) > 0 being the minimum singular value of HHH†. Note that (85)
is maximized when XXX is circularly symmetric with nt independent phases. To see this, we introduce a vector of
independent and identically distributed (i.i.d.) phases ΦΦΦ uniformly distributed in [0, 2π)nt and show that, for any XXX,

I(ejΦΦΦ ◦ XXX; ej(ΘΘΘT+ΦΦΦ) ◦ XXX + ˜ZZZ)

16

= h(ej(ΘΘΘT+ΦΦΦ) ◦ XXX + ˜ZZZ) − h(ej(ΘΘΘT+ΦΦΦ) ◦ XXX + ˜ZZZ| ejΦΦΦ ◦ XXX)
≥ h(ej(ΘΘΘT+ΦΦΦ) ◦ XXX + ˜ZZZ| ΦΦΦ) − h(ej(ΘΘΘT+ΦΦΦ) ◦ XXX + ˜ZZZ| XXX, ΦΦΦ)
= h(ejΘΘΘT ◦ XXX + ˜ZZZ
= I(XXX; ejΘΘΘT ◦ XXX + ˜ZZZ),

) − h(ejΘΘΘT ◦ XXX + ˜ZZZ

(cid:48) | XXX)

(cid:48)

(86)

(87)

(88)

(89)
(cid:48) (cid:44) e−jΦΦΦ ˜ZZZ. Therefore, to
where we use the fact that ˜ZZZ is circularly symmetric, and has the same distribution as ˜ZZZ
derive an upper bound, it is without loss of optimality to assuming that XXX is circularly symmetric with m independent
phases. With this assumption, we have

I(XXX; ejΘΘΘT ◦ XXX + ˜ZZZ) = I(|XXX|; ejΘΘΘT ◦ XXX + ˜ZZZ) + I(∠XXX; ejΘΘΘT ◦ XXX + ˜ZZZ||XXX|)

≤ I(|XXX|; ej(ΘΘΘT+∠XXX) ◦ |XXX| + ˜ZZZ) + I(∠XXX; ejΘΘΘT ◦ XXX||XXX|)
≤ I(|XXX|; ej(ΘΘΘT+∠XXX) ◦ |XXX| + ˜ZZZ| ΘΘΘT + ∠XXX) + I(∠XXX; (ΘΘΘT + ∠XXX)2π)

= I(|XXX|;|XXX| + Re(cid:8)˜ZZZ

(cid:48)(cid:48)(cid:9)) + h((ΘΘΘT + ∠XXX)2π) − h(ΘΘΘT)

(90)

(91)

(92)

(93)

≤ nt
2

log+P + cH + log 2π − h(ΘΘΘT)

(94)
where the second inequality is obtain by providing ΘΘΘT + ∠XXX to the output and the independence between ΘΘΘT + ∠XXX
and |XXX|; the last inequality is from the capacity upper bound for a real-value Gaussian channel, and the fact that
(cid:48)(cid:48) (cid:44) e−j(ΘΘΘT+∠XXX) ◦ ZZZ. From (94), we get the upper
(ΘΘΘT + ∠XXX)2π is uniformly distributed in [0, 2π); we deﬁne ˜ZZZ
bound nt

2 of the pre-log.

In the following, we assume nr ≤ nt, and follow closely to the proof for model A in Section IV-A. We ﬁrst apply
a genie-aided bound, by providing the set of indices of the nr strongest inputs to the receiver. This information,

(cid:1) bits. Then we also associate with each U a canonical form

also denoted by U, does not take more than log(cid:0)nt

U HHH where HHH U is the submatrix of HHH with the columns corresponding to the nr strongest entries, while

GGG(U ) = HHH−1
HHH ¯U corresponds to the rest of the columns. It follows that HHH−1

nr

U YYY = GGG(U )XXX + HHH−1

U ZZZ, and

I(XXX; HHHXXX + ZZZ| U )

U ZZZ| U )

= I(XXX; GGG(U )XXX + HHH−1
≤ I(XXX; GGG(U )XXX + aZZZ| U )
= I(a−1XXX; a−1GGG(U )XXX + ZZZ| U )
= I( ˜XXX; GGG(U )

˜XXX + ZZZ| U ),

(95)

(96)

(97)

(98)

DRAFT

March 18, 2016

where a (cid:44) (σmax(HHH))−1; we deﬁne ˜XXX (cid:44) a−1XXX and accordingly,
˜XXX + ZZZ = ejΘΘΘT,U ◦ ˜XXXU + HHH−1

WWW (cid:44) GGG(U )

U HHH ¯U (ejΘΘΘT, ¯U ◦ ˜XXX ¯U ) + ZZZ.

The next step is to derive a lower bound on h(WWW | ˜XXX),
h(WWW | ˜XXX) ≥ h(WWW | ˜XXX, ΘΘΘT, ¯U )

= h(ejΘΘΘT,U ◦ ˜XXXU + ZZZ| ˜XXX, ΘΘΘT, ¯U )

h(ejΘT,k ˜Xk + Zk | ˜XXX, ΘΘΘT, ¯U , Θk−1

T

E(cid:2)log+| ˜Xk|(cid:3) + c0,

, Z k−1)

≥ nr(cid:88)
≥ nr(cid:88)

k=1

k=1

17

(99)

(100)

(101)

(102)

(103)

where we assume that U = {1, . . . , nr} for notational convenience, and the last inequality is from Lemma 7.

Finally, we derive an upper bound on h(WWW | U ) via duality using the following auxiliary distribution on the

output WWW,

q(www) = gααα µα1+···+αnr exp(−µ(cid:107)www(cid:107)2)

nr(cid:89)

i=1

|wi|2(αi−1), www ∈ Cn,

(104)

where gααα is the normalization factor which only depends on ααα and nr. Essentially, we let each Wi be independent and
circularly symmetric with the squared module following a single-variate Gamma distribution with parameter (µ, αi),
as deﬁned in (7) from Deﬁnition 1.

Lemma 12. By choosing 0 < αi < 1 and µ = min{P −1, 1}, we have

2(1 − αi)E(cid:2)log+| ˜Xi|(cid:3) + cH ,

nr(cid:88)

i=1

(105)

αi log+P +

where we assume that | ˜X1| ≥ . . . ≥ | ˜Xnt| for notational convenience.

Proof: The following is straightforward from (104),

αi log µ−1 + µE(cid:2)(cid:107)WWW(cid:107)2(cid:3) log e +

nr(cid:88)

2(1 − αi)E(cid:2)log |Wi|(cid:3) + c0

(106)

i=1

i=1

(1 − αi)E(cid:2)log |Wi|2(cid:3) + c(cid:48)

nr(cid:88)
U HHH ¯U )(cid:107) ˜XXX ¯U(cid:107)2 + 2|Zi|2. From the deﬁnition of U, we have (cid:107) ˜XXX ¯U(cid:107)2 ≤
H|)| ˜Xi|2 + 2|Zi|2. Applying Jensen’s inequality on the expectation
H. Plugging it back to (107), we

H|)| ˜Xi|2 + 2)(cid:3) ≤ 2E(cid:2)log+| ˜Xi|(cid:3) + c(cid:48)(cid:48)(cid:48)

(107)

H .

Note that |Wi|2 ≤ 2| ˜Xi|2 + 2σ2
(nt − nr)| ˜Xi|2, ∀ i ∈ U. Thus, |Wi|2 ≤ (2 + |c(cid:48)(cid:48)

over Zi, we get E(cid:2)log |Wi|2(cid:3) ≤ E(cid:2)log((2 + |c(cid:48)(cid:48)

max(HHH−1

readily have (105).

March 18, 2016

DRAFT

E(cid:2)− log q(WWW)(cid:3)
≤ nr(cid:88)

i=1

E(cid:2)− log q(WWW)(cid:3)
≤ nr(cid:88)
≤ nr(cid:88)

i=1

i=1

αi log+P +

Finally, putting together (103) and (105), we obtain

I( ˜XXX; WWW | U )

i=1

≤ nr(cid:88)
≤ nr(cid:88)
nr(cid:88)

i=1

αi log+P +

αi log+P +

nr(cid:88)
nr(cid:88)

i=i

(1 − 2αi)E(cid:2)log+| ˜Xi|(cid:3) + cH
(1 − 2αi)+ E(cid:2)log+| ˜Xi|(cid:3) + cH

(cid:0)2αi + (1 − 2αi)+(cid:1) log+P + c(cid:48)
where, to obtain the last inequality, we apply (47) in Lemma 8 with p = 2, and the power constraint E(cid:2)| ˜Xi|2(cid:3) ≤
a2E(cid:2)(cid:107)XXX(cid:107)2(cid:3) ≤ a2P . Therefore, the multiplexing gain is upper-bounded by

≤ 1
2

(110)

H ,

i=1

i=i

(cid:0)2αi + (1 − 2αi)+(cid:1) ,

nr(cid:88)

i=1

1
2

∀ ααα ∈ (0, 1)nr.

(111)

2 .
Taking the inﬁmum over ααα, we get nr

C. Case B3: Receive phase noise

First it is not hard to show the upper bound nt − 1

2. It is enough to provide the nr − 1 relative angles,
{ΘR,k − ΘR,1}k=2...nr, to the receiver. The channel is now equivalent to the case with common phase noise ΘR,1.
Then we can apply Proposition 1, since

18

(108)

(109)

(112)

(113)

(114)

(115)

(116)

(117)

(118)

h(ΘR,1 |{ΘR,k − ΘR,1}k=2...nr) = h(ΘΘΘR) − h({ΘR,k − ΘR,1}k=2...nr)

≥ h(ΘΘΘR) − (nr − 1) log(2π)
≥ −∞.

Next, we show the upper bound nr

2 . To that end, we write

I(XXX; YYY) = I(HHHXXX;|YYY|) + I(HHHXXX; ∠YYY ||YYY|)

= I(HHHXXX;|YYY|) + I(HHHXXX; (∠HHHXXX+ZZZ(cid:48) + ΘΘΘR)2π ||YYY|)
= I(HHHXXX;|YYY|) + h((∠HHHXXX+ZZZ(cid:48) + ΘΘΘR)2π ||YYY|) − h((∠HHHXXX+ZZZ(cid:48) + ΘΘΘR)2π ||YYY|, HHHXXX)
≤ I(HHHXXX;|YYY|) + nr log(2π) − h((∠HHHXXX+ZZZ(cid:48) + ΘΘΘR)2π ||YYY|, HHHXXX, ∠HHHXXX+ZZZ(cid:48))
= I(HHHXXX;|YYY|) + nr log(2π) − h(ΘΘΘR),

(119)
where we deﬁne ZZZ(cid:48) (cid:44) e−jΘΘΘR ◦ ZZZ which is independent of ΘΘΘR since ZZZ is circularly symmetric; the last equality
follows since YYY = ejΘΘΘR ◦ (HHHXXX + ZZZ(cid:48)) and thus ΘΘΘR is independent of (|YYY|, HHHXXX + ZZZ(cid:48), HHHXXX). It remains to show that
I(HHHXXX;|YYY|) ≤ nr
2 log+P + cH and to use the fact

2 log+P + cH. To prove this, it is enough to apply h(|YYY|) ≤ nr

k=1 h(|Yk|| HHHXXX) is lower-bounded by some constant according to (38) in Lemma 7.

that h(|YYY|| HHHXXX) =(cid:80)nr

March 18, 2016

DRAFT

VI. CAPACITY LOWER BOUND FOR MODEL B

In this section, we derive a lower bound on the capacity of model B. For simplicity, we consider the class of
memoryless Gaussian input distributions. Although the optimal input distribution has been proved to be discrete
in [6], the use of a simple Gaussian input provides tight lower bounds on the pre-log, which is enough for our
purpose here. In the following, we only consider the memoryless phase noise channel which can be shown to have a
lower capacity than the general stationary and ergodic channel when memoryless input is used. To see this, we write

19

I(XXXN ; YYYN ) = h(XXXN ) − h(XXXN | YYYN )

h(XXXt | XXXt−1, YYYN )

h(XXXt | YYYt)

=

t=1

N(cid:88)
≥ N(cid:88)
N(cid:88)

t=1

=

h(XXXt) − N(cid:88)
h(XXXt) − N(cid:88)

t=1

t=1

I(XXXt; YYYt)

t=1

= N I(XXX; YYY).

(120)

(121)

(122)

(123)

(124)

(125)

(126)

Thus, we focus on the single-letter mutual information I(XXX; YYY) in the rest of the section. As in the previous section,
we investigate the three cases separately.

A. Case B1: Transmit and receive phase noises

In this case, we use all the inputs with equal power, i.e., XXX ∼ CN (0, P

IIInt). For convenience, let us rewrite the

nt

received signal as

YYY = ejΘΘΘR ◦(cid:0)HHH(ejΘΘΘT ◦ XXX)(cid:1) + ZZZ
(cid:114) P

ej ˜ΘΘΘR ◦(cid:0)HHH(ej ˜ΘΘΘT ◦ XXX0)(cid:1) + ZZZ

(cid:114) P
(cid:114) P

nt

=

ej ˜ΘΘΘR ◦ ˆYYY + ZZZ =

˜YYY + ZZZ

(127)
where XXX0 ∼ CN (0, IIInt) is the normalized version of XXX; ˜ΘΘΘR (cid:44) ΘΘΘR + ΘT,1 and ˜ΘΘΘT (cid:44) ΘΘΘT − ΘT,1. Note that ˜ΘT,1 = 0
by deﬁnition and h( ˜ΘΘΘR) > −∞. The mutual information of interest can be written as

nt

nt

=

I(XXX; YYY) = I(XXX, ˜ΘΘΘT; YYY) − I( ˜ΘΘΘT; YYY | XXX)

= h(YYY) − h(YYY | XXX, ˜ΘΘΘT) − I( ˜ΘΘΘT; YYY | XXX).

(128)

(129)

First the following lemma, which provides a lower bound on h(YYY) in (129), is crucial for the achievability proof.

Lemma 13. With receive phase noise such that h(ΘΘΘR) > −∞, we have

h(YYY) ≥(cid:16) nr

+

1
2

2

min{nr, 2nt − 1}(cid:17)

Proof: See Appendix D.

March 18, 2016

log+P + cH .

(130)

DRAFT

Next, we derive upper bounds on the two negative terms in (129) as follows. The conditional differential entropy

20

can be upper-bounded as

h(YYY | XXX, ˜ΘΘΘT) ≤ nr(cid:88)
≤ nr(cid:88)

k=1

k=1

h(Yk | XXX, ˜ΘΘΘT)

(cid:34)

E

log+

(cid:12)(cid:12)(cid:12)(cid:12)(cid:114) P

nt

(cid:12)(cid:12)(cid:12)(cid:12)

k(ej ˜ΘΘΘT ◦ XXX0)
hhhT

(cid:35)

+ c0

(131)

(132)

(133)

≤ nr
2

log+P + cH ,

where the second inequality is due to Lemma 7 and the third inequality is from Lemma 8 and the power constraint

E(cid:104)(cid:12)(cid:12)hhhT
k(ej ˜ΘΘΘT ◦ XXX0)(cid:12)(cid:12)2(cid:105)

= n−1

t (cid:107)hhhk(cid:107)2P . And

I( ˜ΘΘΘT; YYY | XXX) ≤ I( ˜ΘΘΘT; YYY, ˜ΘΘΘR | XXX)

= I( ˜ΘΘΘT; YYY | XXX, ˜ΘΘΘR) + I( ˜ΘΘΘT; ˜ΘΘΘR)
= I( ˜ΘΘΘT; ej ˜ΘΘΘT ◦ XXX + HHH†ZZZ| XXX, ˜ΘΘΘR) + c0
≤ I( ˜ΘΘΘT; ej ˜ΘΘΘT ◦ XXX + ˜ZZZ| XXX, ˜ΘΘΘR) + c0
= h(ej ˜ΘΘΘT ◦ XXX + ˜ZZZ| XXX, ˜ΘΘΘR) − h(ej ˜ΘΘΘT ◦ XXX + ˜ZZZ| XXX, ˜ΘΘΘR, ˜ΘΘΘT) + c0
≤ nt − 1

(134)

(135)

(136)

(137)

(138)

(139)
where ˜ZZZ ∼ CN (0, σ2
min(HHH†)IIInt), with σmin(HHH†) being the minimum singular value of HHH†; to obtain the last
inequality, we use the fact that ˜ΘT,1 = 0 and apply Lemma 7 then Lemma 8 for the rest of the nt − 1 entries of ˜ΘΘΘT.

log+P + cH ,

2

Plugging (133), (139), and (130) into (129), we obtain

I(XXX; YYY) ≥ 1
2

min{nr − nt + 1, nt} log+P + cH .

(140)

Note that the above lower bound holds when we substitute nt by any n(cid:48)
antennas. It is clear that when nr − nt + 1 ≥ nt, i.e., nr ≥ 2nt − 1, we should let n(cid:48)
decrease n(cid:48)
bound for model B1.

t to balance between nr − n(cid:48)

t, which gives n(cid:48)

t ≤ nt, i.e., by activating only n(cid:48)

t transmit
t = nt. Otherwise, we should
2 (cid:99). This completes the proof of the lower

t + 1 and n(cid:48)

t = (cid:98) nr+1

B. Case B2: Transmit phase noise

In this case, we use n(cid:48)

denoted by XXX(cid:48), are i.i.d. Gaussian, i.e., XXX(cid:48) ∼ CN (0, P
n(cid:48)
t
where HHH(cid:48) ∈ Cnr×n(cid:48)
follows that I(XXX(cid:48); YYY) = I(XXX(cid:48); (HHH(cid:48))†YYY). Then we have h((HHH(cid:48))†YYY) = h(ejΘΘΘ(cid:48)
h((HHH(cid:48))†YYY | XXX(cid:48)) = h(ejΘΘΘ(cid:48)

t (cid:44) min{nt, nr} input antennas and desactivate the remaining ones. The active inputs,
T ◦ XXX(cid:48)) + ZZZ
T is similarly deﬁned. It
t log+P + cH and

(cid:0)(HHH(cid:48))†(cid:1)ZZZ| XXX(cid:48)). The latter is further upper-bounded

is the submatrix of HHH corresponding to the active inputs, and ΘΘΘ(cid:48)

t ). We rewrite the output vector as YYY = HHH(cid:48)(ejΘΘΘ(cid:48)
IIIn(cid:48)

T◦XXX(cid:48) +(HHH(cid:48))†ZZZ| XXX(cid:48)) ≤ h(ejΘΘΘ(cid:48)

T ◦ XXX(cid:48) + (HHH(cid:48))†ZZZ) = n(cid:48)

T◦XXX(cid:48) +σmax

t

by(cid:80)n(cid:48)

t

k=1

E(cid:2)log+|Xk|(cid:3) + cH ≤ n(cid:48)

2 log+P + c(cid:48)

t

H according to (37) in Lemma 7 and (47) in Lemma 8. This shows

the lower bound 1

2 min{nt, nr} on the multiplexing gain.

March 18, 2016

DRAFT

21

C. Case B3: Receive phase noise

As in Case B1, we let XXX ∼ CN (0, P

nt

from (131) to (133) that

IIInt). First h(YYY) is lower-bounded in Lemma 13. Next, it readily follows

h(YYY | XXX) ≤ nr
2

log+P + cH ,

(141)

since we are in the same situation as in Case B1 when ˜ΘΘΘT is known. Finally, combining (130) and (141), we obtain
a lower bound on the mutual information

I(XXX; YYY) = h(YYY) − h(YYY | XXX)

≥ 1
2

min{nr, 2nt − 1} log+P + cH

(142)

(143)

which provides the desired multiplexing gain.

VII. CONCLUSIONS AND DISCUSSIONS

In this work, we investigated the discrete-time stationary and ergodic nr × nt MIMO phase noise channel. We
characterized the exact multiplexing gain when phase noises are on the individual paths and when phase noises are at
either side of the channel. With both transmit and receive phase noises, upper and lower bounds have been derived.
In particular, the upper bound results in this paper have been obtained via the duality using a newly introduced
multi-variate Gamma distribution.

For model B1, the upper and lower bounds derived in this paper do not match for nr ∈ [4 : 2nt − 2]. We
2 min{nt, nr − 1} is indeed loose. Let us recall that the upper bound is obtained

by lower-bounding h(WWW | ˜XXX) with (78), and by upper-bounding E(cid:2)− log q(WWW)(cid:3) with q(www) being the multi-variate

conjecture that the upper bound 1

Gamma distribution. We believe that both bounds are loose for model B1 in general. First we can show that

h(WWW | ˜XXX) ≤ nr E(cid:2)log+| ˜XU|(cid:3) + E(cid:2)log+| ˜XV |(cid:3) +

(cid:88)

E(cid:2)log+| ˜Xk|(cid:3) + cH .

(144)

k(cid:54)∈{U,V }

nr E(cid:2)log+| ˜XU|(cid:3) + c(cid:48)
E(cid:2)log+| ˜Xk|(cid:3) + c(cid:48)(cid:48)
(cid:80)

k(cid:54)=U

To see this, we can ﬁrst write h(WWW | ˜XXX) = h(WWW | ˜XXX, ˜ΘΘΘT) + I( ˜ΘΘΘT; WWW | ˜XXX), then upper-bound the ﬁrst term with
H by following closely the steps as in (131)-(133), and upper-bound the second term with
H by following closely the steps as in (134)-(139). As compared to the lower bound (78),
the upper bound (144) differs only in the terms involving ˜Xk, k (cid:54)∈ {U, V }. In the following, we argue that even if
the lower bound h(WWW | ˜XXX) was the RHS of (144) – which is the largest that one could get as lower bound since it is
2 min{nr − 1} with
also an upper bound – we still would not be able to tighten the multiplexing gain upper bound 1
the same choice of auxiliary distribution q(www). In other words, for the given q(www), (78) is tight enough with respect

to the upper bound on E(cid:2)− log q(WWW)}(cid:3) − h(WWW | ˜XXX). To prove this, it is enough to observe that E(cid:2)− log q(WWW)(cid:3) does
h(WWW | ˜XXX) is increasing with the strength of each ˜Xk. Therefore, the maximization of E(cid:2)− log q(WWW)(cid:3) − h(WWW | ˜XXX)

not involve any terms of ˜XXX other than ˜XU and ˜XV in such a way to change the high SNR behavior, whereas

over ˜XXX will always put all ˜Xk, k (cid:54)∈ {U, V }, to zero, even if h(WWW | ˜XXX) hits the highest value (144). To sum up, if
2 min{nt, nr − 1} was indeed loose as we conjecture, one would have to ﬁrst ﬁnd a new
the current upper bound 1

March 18, 2016

DRAFT

22

auxiliary distribution q(www) in order to get a tighter upper bound. In particular, the new auxiliary distribution should

be such that E(cid:2)− log q(WWW)(cid:3) depends on ˜Xk, k (cid:54)∈ {U, V } at high SNR in a non-trivial way. With such a distribution,

the second challenge is to ﬁnd a lower abound on h(WWW | ˜XXX) that also depends on ˜Xk, k (cid:54)∈ {U, V }, in a non-trivial
way. In fact, we conjecture that (144) holds with equality.

For model B2 and B3, the results have the following alternative chain rule interpretation. With transmit phase
noise (model B2), the mutual information can be written as I(XXX; YYY) = I(XXX, ΘΘΘT; YYY) − I(ΘΘΘT; YYY | XXX), where the
ﬁrst term scales as min{nt, nr} log P as if the phase noise were part of the transmitted signal whereas the second
2 {nt, nr} log P as if ΘΘΘ were the input with a ﬁxed distribution and XXX were the “fading” known
part scales as 1
at the receiver side. With receive phase noise (model B), the mutual information can be written differently as
I(XXX; YYY) = I(XXX; YYY | ΘΘΘR) − I(ΘΘΘR; XXX| YYY). Here the ﬁrst term corresponds to the rate when the phase noise is known,
while the second term can be considered as the rate of a “reverse” channel with input ΘΘΘR, output XXX, and known
fading YYY. In both cases, the original problem of characterizing I(XXX; YYY) boils down to subproblems involving
channels without phase noise (i.e., I(XXX, ΘΘΘT and I(XXX; YYY | ΘΘΘR)) and communications with ﬁxed phase signaling (i.e.,
I(ΘΘΘT; YYY | XXX) and I(ΘΘΘR; XXX| YYY)).

There are a few interesting future directions. First, it is possible to extend the results to multi-user channels and
study the impact of phase noise to such systems. Second, the lower bound on model B1 suggests the following
dimension counting argument: one can recover nt real information with 2nt− 1 real observations, since the remaining
nt − 1 dimensions are occupied by the nt − 1 relative phase noises. How to design decoding algorithms that “solve”
efﬁciently the 2nt − 1 non-linear equations is a question of both theoretical and practical importance. Finally, a more
reﬁned analysis should lead to tighter upper and lower bounds on the capacity, beyond the pre-log characterization.

A. Proof of Proposition 1

APPENDIX

With common phase noise, we can perform unitary precoding without losing information, and the channel is
equivalent to a parallel channel with common phase noise YYYt = ejΘtΣΣΣ xxxt+ZZZt = ejΘtσσσ◦xxxt+ZZZt, where ΣΣΣ is a diagonal
matrix with the min{nt, nr} non-zero singular values of the matrix HHH and σσσ is a vector of these elements. From [9],
we know that the multiplexing gain of a M × M channel is upper-bounded by M − 1
2. This upper bound applies here
with M = min{nt, nr}. The lower bound is achieved by using the Gaussian memoryless input XXXt ∼ CN (0, P
IIInt),
from which we have I(XXXN ; YYYN ) = h(YYYN ) − h(YYYN | XXXN ) with h(YYYN ) = N h(YYY) = N min{nt, nr} log+P + N cH
and h(YYYN | XXXN ) ≤ N h(YYY | XXX) = N h(ejΘσσσ◦XXX+ZZZ| XXX). Applying a unitary transformation on ejΘσσσ◦XXX+ZZZ, we obtain
N h(ejΘσσσ◦XXX+ZZZ| XXX) = N h(ejΘ(cid:107)σσσ◦XXX(cid:107)+Z(cid:48)
2 log+P +N c(cid:48)
where ZZZ(cid:48) is the rotated version of ZZZ and remains spatially white, the ﬁrst inequality is from Lemma 7 and the
second one is from Lemma 8. Finally, we have 1
H, which completes
the proof.

1 | XXX)+N(cid:80)M
N I(XXXN ; YYYN ) ≥(cid:0)min{nt, nr} − 1

k) ≤ NE(cid:2)log+(cid:107)σσσ◦XXX(cid:107)(cid:3)+N c(cid:48)
(cid:1) log+P + c(cid:48)(cid:48)

k=2 h(Z(cid:48)

2

nt

H ≤ N

H

March 18, 2016

DRAFT

B. Proof of Lemma 9 and 11

In the following we shall derive the lower bounds (65) and (78) on the conditional differential entropy h(WWW | ˜XXX)

for model A and model B1, respectively.

23

First we shall show that, for both models,

h(Wi | ˜XXX) ≥ E(cid:2)log+| ˜XU|(cid:3) + E(cid:2)log+| ˜XV |(cid:3) + cH .

(145)
To that end, we analyze h(Wi | ˜XXX = xxx) with |x1| > |x2| > ··· > |xnt| ≥ 0 without loss of generality, i.e., we
assume that U = 1 and V = 2. A lower bound of h(Wi | ˜XXX = xxx) can be obtained by considering the following
cases separately.

• When |x1| ≤ 1,

• When |x1| ≥ 1 and |x2| ≤ 1,

h(Wi | ˜XXX = xxx) ≥ h(Wi | ˜XXX = xxx, ΘΘΘ)

= h(Zi)

= log(πe).

h(Wi | ˜XXX = xxx) ≥ h(Wi | ˜XXX = xxx, Θi,2, . . . , Θi,nt)

= h(gi1ejΘi,1x1 + Zi | Θi,2, . . . , Θi,nt)
≥ log+|gi1x1| + c0
≥ log+|x1| + cH ,

(146)

(147)

(148)

(149)

(150)

(151)

(152)

where gi1ejΘi,1 is from the matrix GGG(1) deﬁned in (56) since U = 1 by assumption; the second inequality is
from Lemma 7 and the third inequality is from Lemma 8.

• When |x1| ≥ 1 and |x2| ≥ 1,

h(Wi | ˜XXX = xxx)

≥ h(Wi | ˜XXX = xxx, Θi,3, . . . , Θi,nt, Zi)
= h(ejΘi,1 gi1x1 + ejΘi,2gi2x2 | Θi,3, . . . , Θi,nt)
≥ h(ejΘi,1 gi1x1 + ejΘi,2gi2x2 | Θi,3, . . . , Θi,nt, Ω)
= E log(|gi1gi2x1x2 sin(Θi,1 − Θi,2 + φ)|)

+ h(Θi,1, Θi,2 | Θi,3, . . . , Θi,nt, Ω)

≥ log |x1| + log |x2| + E log(| sin(Θi,1 − Θi,2 + φ)|) + cH
≥ log+|x1| + log+|x2| + c(cid:48)
H ,

(153)

(154)

(155)

(156)

(157)

(158)

where the ﬁrst inequality is from conditioning reduces entropy; we partition [0, 2π)2 in such a way that
ejΘi,1gi1x1 + ejΘi,2gi2x2 is a bijective function of (Θi,1, Θi,2) in each partition indexed by Ω which takes

March 18, 2016

DRAFT

a ﬁnite number of values; then we applied the change of variables from Lemma 2 and obtain (156) with
φ (cid:44) ∠gi1x1 − ∠gi2x2; ﬁnally, we use the fact that |gi1gi2| is bounded for almost every HHH and the application
of Lemma 5 to get the last inequality. Note that log |xk| = log+|xk| for k = 1, 2 by assumption.

Combining the three cases above and taking expectation over ˜XXX, we get (145).

24

1) Proof of the lower bound (65) for model A: For model A, we have

h(WWW | ˜XXX) =

i=1

h(Wi | ˜XXX, W i−1)

nr(cid:88)
≥ nr(cid:88)
nr(cid:88)
≥ nrE(cid:2)log+| ˜XU|(cid:3) + nrE(cid:2)log+| ˜XV |(cid:3) + cH ,

h(Wi | ˜XXX, W i−1,{Θl,1, . . . , Θl,nt}l<i)

h(Wi | ˜XXX,{Θl,1, . . . , Θl,nt}l<i)

i=1

=

i=1

where (161) is from the fact that Wi only depends on W i−1 through the input ˜XXX and the phase noises {Θl,1, . . . , Θl,nt}l<i;
where the last inequality is from a modiﬁed version of (145) by introducing {Θl,1, . . . , Θl,nt}l<i in the condition.

(159)

(160)

(161)

(162)

(163)

(164)

nr(cid:88)

2) Proof of the lower bound (78) for model B1: For model B1, we write

h(WWW | ˜XXX) = h(W1 | ˜XXX) +

h(Wi | ˜XXX, W i−1),

i=2
where, according to (145), the ﬁrst term is lower-bounded by

h(W1 | ˜XXX) ≥ E(cid:2)log+| ˜XU|(cid:3) + E(cid:2)log+| ˜XV |(cid:3) + cH .
In the following, we derive a lower bound on the second term. Let Bi (cid:44) (cid:80)nt

k=1 gik ˜XkejΘT,k where gik is the

channel coefﬁcient without phase noise from the canonical form U deﬁned in (56). Then

h(Wi | ˜XXX, W i−1)

= h(ejΘR,iBi + Zi | ˜XXX, W i−1)
≥ h(ejΘR,iBi + Zi | ˜XXX, W i−1, Bi, ΘΘΘT, Θi−1
R )
= h(ejΘR,iBi + Zi | Bi, ΘΘΘT, Θi−1
R )

≥ E(cid:2)log+|Bi|(cid:3) + c0
(cid:104)E(cid:2)log+|Bi|| ˜XXX(cid:3)(cid:105)

= E ˜XXX

+ c0

March 18, 2016

(165)

(166)

(167)

(168)

(169)

DRAFT

where the ﬁrst inequality is from conditioning reduces entropy; (168) is from Lemma 7. The conditional expectation
can be lower-bounded as follows

E(cid:2)log |Bi|| ˜XXX(cid:3) =

log

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) nt(cid:88)

k=1

E

1
2

|gik ˜Xk|ej(ΘT,k+ ˜Φik)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) nt(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) nt(cid:88)

k=1

k=1

log
log

E

E

≥ log (cid:107) ˜XXX(cid:107) +

1
2

inf

xxx∈Rnt : (cid:107)xxx(cid:107)=1

|gik|ej(ΘT,k+ ˜Φik)xk

|gik| cos(ΘT,k + ˜Φik)xk

inf

xxx∈Rnt : (cid:107)xxx(cid:107)=1

≥ log (cid:107) ˜XXX(cid:107) +

1
2
≥ log (cid:107) ˜XXX(cid:107) + cH
≥ log | ˜XU| + cH ,

; (173) is obtained by applying Lemma 6 with VVV (cid:44)(cid:104)|gik| cos(ΘT,k + ˜Φik)

where ˜Φik (cid:44) ∠

(cid:80)
k |gik|2 < ∞ and

gik ˜Xk

25

(170)

(171)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2
(cid:105)
with E(cid:2)(cid:107)VVV(cid:107)2(cid:3) ≤

(174)

(172)

(173)

k

h(VVV) ≥ h(VVV | ˜ΦΦΦ)

= h(cos(ΘΘΘT + ˜ΦΦΦ)| ˜ΦΦΦ) +

(cid:88)

k

log |gik|

> −∞

(175)

(176)

(177)

where the equality is the application of the change of variables from Lemma 2; the last inequality is from Lemma 5.
From (169) and (174), we get

h(Wi | ˜XXX, W i−1) = E ˜XXX
≥ E ˜XXX

+ c0

(cid:104)E(cid:2)log+|Bi|| ˜XXX(cid:3)(cid:105)
(cid:20)(cid:16)E(cid:2)log |Bi|| ˜XXX(cid:3)(cid:17)+(cid:21)
≥ E(cid:104)(cid:0)log | ˜XU| + cH
(cid:1)+(cid:105)
≥ E(cid:2)log+| ˜XU|(cid:3) − |cH| + c0,

+ c0

+ c0

where the last inequality is from the application of (46) in Lemma 8 with p = 2cH . Plugging (164) and (181) into
(163), the lower bound (78) is obtained.

C. Proof of Lemma 10

From Deﬁnition 1, by imposing 1 > αi > 0, i = 1, . . . , nr, and µ = min{P −1, 1}, we have

− log q(WWW) = − log

αi log µ + µ| ˆWnr|2

− nr(cid:88)
(1 − αi) log(cid:0)| ˆWi|2 − | ˆWi−1|2(cid:1).

i=1

gααα
nr!

nr(cid:88)

+ (1 − α1) log | ˆW1|2

+

March 18, 2016

i=2

(178)

(179)

(180)

(181)

(182)

DRAFT

We bound each term as follows.

• The squared module of each output

|Wi|2 ≤ 2|GGGT

i

˜XXX|2 + 2|Zi|2

≤ 2(cid:107)GGGi(cid:107)2(cid:107) ˜XXX(cid:107)2 + 2|Zi|2
≤ 2λHHH(cid:107) ˜XXX(cid:107)2 + 2(cid:107)ZZZ(cid:107)2,

26

(183)

(184)

(185)

i is the i th row of the canonical matrix GGG(U ) deﬁned in (56); (184) is due to Cauchy-Schwarz; and

where GGGT
λHHH is deﬁned as

λHHH (cid:44) max

u=1,...,nt

(cid:107)GGG(u)(cid:107)2.

(186)

• The difference of the squared modules

with

|e−jΘi,U Wi − e−jΘk,U Wk|2

(cid:12)(cid:12)|Wi|2 − |Wk|2(cid:12)(cid:12)
(cid:113)

≤(cid:12)(cid:12)(cid:12)e−jΘi,U (GGGT
(cid:12)(cid:12)(cid:12)e−jΘi,U GGGT
(cid:18)(cid:88)

≤ 2
≤ 2

i

i

l(cid:54)=U
+ 2(cid:107)ZZZ(cid:107)2
≤ 4(nt − 1)λHHH

≤ (|Wi| + |Wk|)|e−jΘi,U Wi − e−jΘk,U Wk|
≤ 2

λHHH(cid:107) ˜XXX(cid:107)2 + (cid:107)ZZZ(cid:107)2 |e−jΘi,U Wi − e−jΘk,U Wk|

3
2

˜XXX + Zi) − e−jΘk,U (GGGT
˜XXX − e−jΘk,U GGGT

˜XXX

k

k

(cid:12)(cid:12)(cid:12)2

+ 2|Zi|2 + 2|Zk|2

(cid:19)

|e−jΘi,U Gil − e−jΘk,U Gkl|2

| ˜Xl|2

(cid:12)(cid:12)(cid:12)2

˜XXX + Zk)

(cid:19)(cid:18)(cid:88)

l(cid:54)=U

V + 2(cid:107)ZZZ(cid:107)2.
˜X 2

(187)

(188)

Note that the above upper bounds does not depend on i and k. Then, with the above bounds, we take expectation of
the terms in (182), and obtain

E(cid:2)| ˆWnr|2(cid:3) ≤ 2(cid:0)λHHHE(cid:2)(cid:107) ˜XXX(cid:107)2(cid:3) + E(cid:2)(cid:107)ZZZ(cid:107)2(cid:3)(cid:1)
E(cid:2)log | ˆW1|2(cid:3) ≤ E(cid:2)log(λHHH(cid:107) ˜XXX(cid:107)2 + E(cid:2)(cid:107)ZZZ(cid:107)2(cid:3))(cid:3) + 1

≤ 2λHHH a2P + c0,

≤ 2 E(cid:2)log+| ˜XU|(cid:3) + cH ,

(189)

(190)

(191)

(192)

DRAFT

March 18, 2016

where the last inequality is from Lemma 8. Similarly, basic calculations lead to

E(cid:104)

log

(cid:12)(cid:12)(cid:12)| ˆWi|2 − | ˆWi−1|2(cid:12)(cid:12)(cid:12)(cid:105)
λHHH(cid:107) ˜XXX(cid:107)2 + E(cid:2)(cid:107)ZZZ(cid:107)2(cid:3)(cid:17)(cid:105)
E(cid:104)
4(nt − 1)λHHH| ˜XV |2 + 2E(cid:2)(cid:107)ZZZ(cid:107)2(cid:3)(cid:17)(cid:105)
(cid:16)
E(cid:104)

(cid:16)

log

log

≤ 1
2

+

1
2

≤ log+| ˜XU| + log+| ˜XV | + cH .

27

(193)

(194)

+ c0

Taking expectation over XXX in (182), and plugging (190), (192), and (194) into it, we readily obtain (70).

D. Proof of Lemma 13

To prove Lemma 13, we deal with the cases nr = 2nt − 1 and nr (cid:54)= 2nt − 1 separately. Let us deﬁne ˆYYY and ˜YYY

such that

YYY =

(cid:114) P

nt

(cid:114) P

nt

ej ˜ΘΘΘR ◦ ˆYYY + ZZZ =

˜YYY + ZZZ.

(195)

For notational convenience, we deﬁne n (cid:44) nr and m (cid:44) nt in the following proof.
1) Case n = 2m−1: First we show that (130) holds for n = 2m−1. We write h(YYY) ≥ h(YYY | ZZZ) = h(YYY−ZZZ| ZZZ) =

= n log P + h( ˜YYY) + c0. Now, it is enough to show that h( ˜YYY) > −∞. From Lemma 4,

(cid:16)(cid:113) P

(cid:17)

h

˜YYY

m

h( ˜YYY) ≥ h(| ˜YYY|2) + c0
= h(| ˆYYY|2) + c0
≥ h(SSS| ˆYn) + h(| ˆYn|2) + c0
= h(SSS| ˆYn) + cH ,

(196)

(197)

(198)

(199)
where SSS ∈ Rn−1 with Si (cid:44) | ˆYi|2 for i = 1, . . . , n − 1; the second inequality is from the chain rule and that adding
the condition on the phase of ˆYn reduces entropy; the last equality is due to ˆYn ∼ CN (0, m−1(cid:107)hhhn(cid:107)2). Next we
need to show that h(SSS| ˆYn) > −∞. Intuitively, given ˆYn, SSS can be expressed as n − 1 = 2(m − 1) real functions of

the 2(m − 1) real random variables(cid:0)Re{ ˆY m−1}, Im{ ˆY m−1}(cid:1). Since h(cid:0)Re{ ˆY m−1}, Im{ ˆY m−1}(cid:1) = h( ˆY m−1) is

ﬁnite for almost every HHH, as long as the mapping is not degenerated, h(SSS| ˆYn) should be ﬁnite too. This argument
is proved formally in the following.

Since for almost every HHH ∈ Cn×m, any m rows of the matrix are linear independent, the remaining n − m rows
can be written as linear combinations of these rows. Let us take the rows {1, 2, . . . , m − 1, n} of HHH. It readily
follows that one can write

(200)
with BBB ∈ C(m−1)×(m−1) and bbb ∈ C(m−1)×1 depending only on HHH. Next let us partition the space Rn−1 into a
ﬁnite number of sets in each one of which the mapping (Re{ˆym−1}, Im{ˆym−1}) (cid:55)→ sss is bijective. Note that this is

ˆY n−1
m = BBB ˆY m−1 + bbb ˆYn

March 18, 2016

DRAFT

possible since the mapping is quadratic in a ﬁnite-dimensional space. Let Ω be the index of the partitions which
only depends on ˆY m−1. Then

28

h(SSS| ˆYn) ≥ h(SSS| ˆYn, Ω)

= E [log |det(JJJ)|] + h( ˆY m−1 | ˆYn, Ω)
= E [log |det(JJJ)|] + cH ,

(203)
where the ﬁrst equality is from Lemma 2; the second equality is due to h( ˆY m−1 | Ω, ˆYn) = h( ˆY m−1 | ˆYn) −
I(Ω; ˆY m−1 | ˆYn) with h( ˆY m−1 | ˆYn) > −∞ for almost all HHH and I(Ω; ˆY m−1 | ˆYn) ≤ H(Ω) < ∞; JJJ is the Jacobian
matrix with

(cid:105)
(cid:105)

(cid:104)

det(JJJ) = det

∂Re{ ˆY m−1}

∂Im{ ˆY m−1}

∂ SSS

∂ SSS

∂ ˆY m−1

(cid:104) ∂ SSS
 diag{( ˆY m−1)∗}
(cid:110)

diag{( ˆY n−1

diag{ ˆY n−1

∂ SSS

∂( ˆY m−1)∗


m }BBB∗diag{( ˆY m−1)∗}(cid:111)

diag{ ˆY m−1}
m }BBB∗

m )∗}BBB diag{ ˆY n−1

= 2m−1det

= 2m−1det

= 4m−1jm−1 Im

where (205) is due to the fact that the complex gradient of a real-valued function is a unitary transformation

of the real gradient (see, e.g., [16, App.A6]); to obtain the last equality, we apply the identity det(cid:2) CCC DDD

(cid:3) =

det(CCC)det(FFF − EEECCC−1DDD). Since ˆY1, . . . , ˆYm−1, ˆYn are jointly circularly symmetric Gaussian with ﬁnite and non-
degenerate covariance for almost every HHH, there exists a ˆY (cid:48)
n circularly symmetric with non-zero bounded variance
and independent of ˆY m−1, such that

EEE FFF

(201)

(202)

(204)

(205)

(206)

(207)

(208)

(209)

(210)

(211)

where fff ∈ C(m−1)×1 depends only on HHH. Then we can continue from (207) and write |det(JJJ)| as

n + fff T ˆY m−1,

ˆYn = ˆY (cid:48)
m = (BBB + bbbfff T) ˆY m−1 + bbb ˆY (cid:48)
ˆY n−1

n

and thus

|det(JJJ)| = 4m−1(cid:12)(cid:12)(cid:12)det
(cid:16)
Im(cid:8) ˆY (cid:48)
= 4m−1(cid:12)(cid:12)(cid:12)det(cid:0) ˆY (cid:48)
(cid:110)
diag{(BBB + bbbfff T) ˆY m−1}BBB∗diag{( ˆY m−1)∗}(cid:111)

(cid:17)(cid:12)(cid:12)(cid:12)
ndiag{bbb}AAA(cid:9) + MMM
n,INNNR + MMM(cid:1)(cid:12)(cid:12)(cid:12)

n,RNNNI + ˆY (cid:48)

where AAA (cid:44) BBB∗diag{( ˆY m−1)∗} and MMM (cid:44) Im
real and imaginary parts of ˆY (cid:48)
Conditioned on ˆY m−1 and ˆY (cid:48)
polynomial of ( ˆY (cid:48)

m,INNNR + MMM)NNN−1

, namely,

I

n,I are the
n, respectively; NNNR and NNNI are the real and imaginary parts of diag{bbb}AAA, respectively.
n,I, the determinant |det(JJJ)| can be written as the absolute value of the characteristic

; ˆY (cid:48)

n,R and ˆY (cid:48)

|det(JJJ)| = 4m−1|det(NNNI)| m−1(cid:89)

t=1

March 18, 2016

| ˆY (cid:48)
n,R − Λt|

(212)

DRAFT

where Λ1, . . . , Λm−1 are the eigenvalues of ( ˆY (cid:48)

in C and are functions of HHH, ˆY m−1, and ˆY (cid:48)

n,I. Then

29

I

n,I

n,INNNR + MMM)NNN−1
(cid:3)

log |det(JJJ)|| ˆY m−1, ˆY (cid:48)

(cid:105)
E(cid:104)
= E(cid:2)log |det(NNNI)|| ˆY m−1, ˆY (cid:48)
m−1(cid:88)
≥ E(cid:2)log |det(NNNI)|| ˆY m−1, ˆY (cid:48)
= E(cid:2)log |det(NNNI)|| ˆY m−1, ˆY (cid:48)

E(cid:2)log | ˆY (cid:48)

n,R − Λt|| ˆY m−1, ˆY (cid:48)
m−1(cid:88)
(cid:3) +
(cid:3) + cH

t=1

t=1

n,I

n,I

+

n,I

(cid:3) + c0
E(cid:2)log | ˆY (cid:48)

n,R|(cid:3) + c0

(213)

(214)

n,I

(215)
m,R − Re{Λt}|, the application of Lemma 1 with k = 1, and

where the inequality is from log | ˆY (cid:48)
the independence between ˆY (cid:48)

n,R − Λt| ≥ log | ˆY (cid:48)

n,R and ( ˆY m−1, ˆY (cid:48)

n,I). Thus, taking expectation over ( ˆY m−1, ˆY (cid:48)

E [log |det(JJJ)|] ≥ E(cid:2)log |det(NNNI)|(cid:3) + cH .

It remains to show that E(cid:2)log |det(NNNI)|(cid:3) > −∞. Let us recall that NNNI (cid:44) Im{diag{bbb}BBB∗diag{( ˆY m−1)∗}} =

(216)

n,I), we have

} where TTT R and TTT I are the real and imaginary parts of diag{bbb}BBB∗, respectively,

TTT Idiag{ ˆY m−1
} − TTT Rdiag{ ˆY m−1
and thus depends only on HHH. Then

R

I

E(cid:2)log |det(NNNI)|(cid:3) − log |det(TTT I)|
(cid:12)(cid:12)(cid:12)det(cid:0)diag{ ˆY m−1
= E(cid:104)

log

R

} − TTT −1

I TTT Rdiag{ ˆY m−1

I

}(cid:1)(cid:12)(cid:12)(cid:12)(cid:105)

(217)

I

R

R

) with ˆY (cid:48)

We shall show that (217) is lower-bounded as follows. Note that ˆYm−1,R can be written as ˆYm−1,R = ˆY (cid:48)
Lm−1( ˆY m−2
( ˆY m−2
, ˆY m−1

, ˆY m−1
), and Lm−1 some linear operator that depends only on HHH. Let PPP m−1 (cid:44) TTT −1

m−1,R +
m−1,R centered normal with non-zero bounded variance and being independent of
I TTT R. It is easy to verify
m−1,R +
, ˆY m−1
)

} − PPP m−1diag{ ˆY m−1
) where PPP m−2 is the upper-left (m−2)×(m−2) sub-matrix of PPP m−1 and L(cid:48)

that det(cid:0)diag{ ˆY m−1

} − PPP m−2diag{ ˆY m−2

m−1( ˆY m−2
m−1,R. Thus, we can again apply Lemma 1 and obtain recursively

m−1( ˆY m−2
L(cid:48)
is some value that is independent of ˆY (cid:48)

}(cid:1) ˆY (cid:48)

, ˆY m−1

R

R

R

I

I

I

I

I

R

}(cid:1) can be written as det(cid:0)diag{ ˆY m−2
}(cid:1)(cid:12)(cid:12)(cid:12)(cid:105)
}(cid:17) ˆY (cid:48)

} − PPP m−1diag{ ˆY m−1

} − PPP m−2diag{ ˆY m−2

diag{ ˆY m−2

R

I

I

m−1,R

(cid:12)(cid:12)(cid:12)(cid:105)

(218)

(219)

log

(cid:12)(cid:12)(cid:12)det(cid:0)diag{ ˆY m−1
E(cid:104)
(cid:12)(cid:12)(cid:12)det
≥ E(cid:104)
(cid:16)
(cid:12)(cid:12)(cid:12) ˆY1,R − P1,1 ˆY1,I
≥ E(cid:104)

log

log

...

R

(cid:12)(cid:12)(cid:12)(cid:105)

+

m−1(cid:88)

k=1

E(cid:2)log | ˆY (cid:48)
k,R|(cid:3)

where the last inequality is from E(cid:2)log | ˆY1,R − P1,1 ˆY1,I|(cid:3) ≥ E(cid:2)log | ˆY1,R|(cid:3) ≥ cH due to the independence between

(220)

≥ cH ,

ˆY1,R and ˆY1,I and the application of Lemma 1.

March 18, 2016

DRAFT

Finally, recalling that TTT I (cid:44) Im{diag{bbb}BBB∗}, we have log |det(TTT I)| > −∞ for almost every HHH, it follows

from (217) that E(cid:2)log |det(NNNI)|(cid:3) is lower-bounded. By now, we have shown that h(YYY) ≥ n log P + cH. Since

h(YYY) ≥ h(YYY | ˜YYY) = h(ZZZ) ≥ n log(πe) ≥ 0, we have h(YYY) ≥ (n log P + cH )+ ≥ n log+P − |cH| from (46) in
Lemma 8 with p = cH

n . This completes the proof for the case n = 2m − 1.

2) Case n (cid:54)= 2m− 1: Note that if (130) holds for n = 2m− 1, then it also holds for n < 2m− 1 and n > 2m− 1.
To see this, in the case with n < 2m − 1, we can add 2m − 1 − n receive antennas to have (YYY, YYY(cid:48)) with YYY(cid:48) being
the extra outputs. Since (130) holds for h(YYY, YYY(cid:48)) by assumption, then we have

30

h(YYY) ≥ h(YYY, YYY(cid:48)) − h(YYY(cid:48))

≥ (2m − 1) log+P − (2m − 1 − n) log+P + cH

(221)

(222)

= n log+P + cH ,

(223)
where the second inequality is from (130) and the fact that h(YYY(cid:48)) ≤ (2m − 1 − n) log+P + cH. When n > 2m − 1,
we partition YYY = (YYY(cid:48), YYY(cid:48)(cid:48)) with YYY(cid:48) ∈ C(2m−1)×1 and obtain
h(Yk | Y k−1)

h(YYY) = h(YYY(cid:48)) +

n(cid:88)

(224)

n(cid:88)
n(cid:88)

k=2m

(cid:34)

(cid:114)

(cid:12)(cid:12)(cid:12)(cid:12)

E

(cid:17)

log+

P
m
log+P + c(cid:48)(cid:48)

H

k=2m

≥ (2m − 1) log+P +

≥ (2m − 1) log+P +

(cid:16)
(cid:16)

=

=

2m − 1 +
n − 1

m +

(cid:17)

2

k=2m

n − 2m + 1

2

log+P + c(cid:48)(cid:48)
H ,

h(Yk | Y k−1, ˜Θk−1

R

, ˜ΘΘΘT, XXX) + cH

(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)

k(ej ˜ΘΘΘT ◦ XXX0)
hhhT

+ c(cid:48)

H

(225)

(226)

(227)

where the second inequality is from Lemma 7; the equality (226) is from the fact that hhhT

(228)
k(ej ˜ΘΘΘT ◦XXX0) ∼ CN (0,(cid:107)hhhk(cid:107)2).

S. Yang would like to thank G. Durisi for helpful discussions and comments during the early stage of this work.

ACKNOWLEDGEMENT

REFERENCES

[1] G. J. Foschini and M. J. Gans, “On limits of wireless communications in a fading environment when using multiple antennas,” Wireless

personal communications, vol. 6, no. 3, pp. 311–335, 1998.

[2] E. Telatar, “Capacity of multi-antenna Gaussian channels,” European transactions on telecommunications, vol. 10, no. 6, pp. 585–595, 1999.
[3] A. Lapidoth and S. Moser, “Capacity bounds via duality with applications to multiple-antenna systems on ﬂat-fading channels,” IEEE

Trans. Inf. Theory, vol. 49, no. 10, pp. 2426–2467, Oct 2003.

[4] L. Zheng and D. Tse, “Communication on the Grassmann manifold: A geometric approach to the noncoherent multiple-antenna channel,”

IEEE Trans. Inf. Theory, vol. 48, no. 2, pp. 359–383, Feb 2002.

[5] A. Lapidoth, “On phase noise channels at high SNR,” in Proc. IEEE Information theory workshop, Oct 2002, pp. 1–4.
[6] M. Katz and S. Shamai, “On the capacity-achieving distribution of the discrete-time noncoherent and partially coherent AWGN channels,”

IEEE Trans. Inf. Theory, vol. 50, no. 10, pp. 2257–2270, Oct 2004.

March 18, 2016

DRAFT

31

[7] L. Barletta, M. Magarini, and A. Spalvieri, “The information rate transferred through the discrete-time Wiener’s phase noise channel,”

Journal of Lightwave Technology, vol. 30, no. 10, pp. 1480–1486, 2012.

[8] H. Ghozlan and G. Kramer, “Models and information rates for Wiener phase noise channels,” arXiv preprint arXiv:1503.03130, 2015.
[9] G. Durisi, A. Tarable, C. Camarda, R. Devassy, and G. Montorsi, “Capacity bounds for MIMO microwave backhaul links affected by phase

noise,” IEEE Trans. Commun., vol. 62, no. 3, pp. 920–929, March 2014.

[10] D. M. Arnold, H.-A. Loeliger, P. O. Vontobel, A. Kavˇci´c, and W. Zeng, “Simulation-based computation of information rates for channels

with memory,” IEEE Trans. Inf. Theory, vol. 52, no. 8, pp. 3498–3508, 2006.

[11] M. R. Khanzadi, G. Durisi, and T. Eriksson, “Capacity of SIMO and MISO phase-noise channels with common/separate oscillators,” IEEE

Trans. Commun., vol. 63, no. 9, pp. 3218–3231, 2015.

[12] G. Durisi, A. Tarable, and T. Koch, “On the multiplexing gain of MIMO microwave backhaul links affected by phase noise,” in Proc. IEEE

International Conference on Commun. (ICC), 2013, pp. 3209–3214.

[13] T. M. Cover and J. A. Thomas, Elements of information theory.
[14] A. M. Mathal and P. G. Moschopoulos, “A form of multivariate Gamma distribution,” Annals of the Institute of Statistical Mathematics,

John Wiley & Sons, 2012.

vol. 44, no. 1, pp. 97–106. [Online]. Available: http://dx.doi.org/10.1007/BF00048672
[15] R. J. Muirhead, Aspects of Multivariate Statistical Theory. New York: Wiley, 1982.
[16] T. Kailath, A. H. Sayed, and B. Hassibi, Linear estimation. Prentice Hall Upper Saddle River, NJ, 2000.

March 18, 2016

DRAFT

