Generalization error bounds for learning to rank:

Does the length of document lists matter?

6
1
0
2

 
r
a

M
6

 

 
 
]

G
L
.
s
c
[
 
 

1
v
0
6
8
1
0

.

3
0
6
1
:
v
i
X
r
a

Ambuj Tewari
University of Michigan, Ann Arbor

Sougata Chaudhuri
University of Michigan, Ann Arbor

Abstract

We consider the generalization ability of algo-
rithms for learning to rank at a query level, a
problem also called subset ranking. Existing
generalization error bounds necessarily degrade
as the size of the document list associated with a
query increases. We show that such a degrada-
tion is not intrinsic to the problem. For several
loss functions, including the cross-entropy loss
used in the well known ListNet method, there is
no degradation in generalization ability as docu-
ment lists become longer. We also provide novel
generalization error bounds under ℓ1 regulariza-
tion and faster convergence rates if the loss func-
tion is smooth.

1. Introduction
Learning to rank at the query level has emerged as an ex-
citing research area at the intersection of information re-
trieval and machine learning. Training data in learning
to rank consists of queries along with associated docu-
ments, where documents are represented as feature vec-
tors. For each query, the documents are labeled with
human relevance judgements. The goal at training time
is to learn a ranking function that can,
for a future
query, rank its associated documents in order of their rel-
evance to the query. The performance of ranking func-
tions on test sets is evaluated using a variety of perfor-
mance measures such as NDCG (J¨arvelin & Kek¨al¨ainen,
2002), ERR (Chapelle et al., 2009) or Average Preci-
sion (Yue et al., 2007).
The performance measures used for testing ranking meth-
ods cannot be directly optimized during training time as
they lead to discontinuous optimization problems. As a re-

Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copy-
right 2015 by the author(s).

TEWARIA@UMICH.EDU

SOUGATA@UMICH.EDU

sult, researchers often minimize surrogate loss functions
that are easier to optimize. For example, one might con-
sider smoothed versions of, or convex upper bounds on,
the target performance measure. However, as soon as one
optimizes a surrogate loss, one has to deal with two ques-
tions (Chapelle et al., 2011). First, does minimizing the
surrogate on ﬁnite training data imply small expected sur-
rogate loss on inﬁnite unseen data? Second, does small
expected surrogate loss on inﬁnite unseen data imply small
target loss on inﬁnite unseen data? The ﬁrst issue is one of
generalization error bounds for empirical risk minimiza-
tion (ERM) algorithms that minimize surrogate loss on
training data. The second issue is one of calibration: does
consistency in the surrogate loss imply consistency in the
target loss?
This paper deals with the former issue, viz. that of gener-
alization error bounds for surrogate loss minimization. In
pioneering works, Lan et al. (2008; 2009) gave generaliza-
tion error bounds for learning to rank algorithms. However,
while the former paper was restricted to analysis of pair-
wise approach to learning to rank, the later paper was lim-
ited to results on just three surrogates: ListMLE, ListNet
and RankCosine. To the best of our knowledge, the most
generally applicable bound on the generalization error of
query-level learning to rank algorithms has been obtained
by Chapelle & Wu (2010).
The bound of Chapelle & Wu (2010), while generally ap-
plicable, does have an explicit dependence on the length of
the document list associated with a query. Our investiga-
tions begin with this simple question: is an explicit depen-
dence on the length of document lists unavoidable in gen-
eralization error bounds for query-level learning to rank al-
gorithms? We focus on the prevalent technique in literature
where learning to rank algorithms learn linear scoring func-
tions and obtain ranking by sorting scores in descending
order. Our ﬁrst contribution (Theorem 3) is to show that di-
mension of linear scoring functions that are permutation in-
variant (a necessary condition for being valid scoring func-
tions for learning to rank) has no dependence on the length

Generalization error bounds for learning to rank

Table1.A comparison of three bounds given in this paper for Lipschitz loss functions. Criteria for comparison: algorithm bound applies
to (OGD = Online Gradient Descent, [R]ERM = [Regularized] Empirical Risk Minimization), whether it applies to general (possibly
non-convex) losses, and whether the constants involved are tight.

Bound

Applies to Handles Nonconvex Loss

Theorem 5
Theorem 6
Theorem 9

OGD
RERM
ERM

No
No
Yes

“Constant” hidden in O(·) notation

Smallest
Small

Hides several logarithmic factors

of document lists. Our second contribution (Theorems 5,
6, 9) is to show that as long as one uses the “right” norm in
deﬁning the Lipschitz constant of the surrogate loss, we can
derive generalization error bounds that have no explicit de-
pendence on the length of document lists. The reason that
the second contribution involves three bounds is that they
all have different strengths and scopes of application (See
Table 1 for a comparison). Our ﬁnal contribution is to pro-
vide novel generalization error bounds for learning to rank
in two previously unexplored settings: almost dimension
independent bounds when using high dimensional features
with ℓ1 regularization (Theorem 12) and “optimistic” rates
(that can be as fast as O(1/n)) when the loss function is
smooth (Theorem 17). We also apply our results on popu-
lar convex and non-convex surrogates. All omitted proofs
can be found in the appendix (see supplementary material).

2. Preliminaries
In learning to rank (also called subset ranking to distinguish
it from other related problems, e.g., bipartite ranking), a
training example is of the form ((q, d1, . . . , dm), y). Here
q is a search query and d1, . . . , dm are m documents with
varying degrees of relevance to the query. Human labelers
provide the relevance vector y ∈ Rm where the entries in
y contain the relevance labels for the m individual docu-
ments. Typically, y has integer-valued entries in the range
{0, . . . , Ymax} where Ymax is often less than 5. For our
theoretical analysis, we get rid of some of these details by
assuming that some feature map Ψ exists to map a query
document pair (q, d) to Rd. As a result, the training exam-
ple ((q, d1, . . . , dm), y) gets converted into (X, y) where
X = [Ψ(q, d1), . . . , Ψ(q, dm)]⊤ is an m × d matrix with
the m query-document feature vector as rows. With this
abstraction, we have an input space X ⊆ Rm×d and a label
space Y ⊆ Rm.
A
examples
training
(X (1), y(1)), . . . , (X (n), y(n)) drawn from some un-
derlying distribution D. To rank the documents in an
instance X ∈ X , often a score vector s ∈ Rm is computed.
A ranking of the documents can then be obtained from
s by sorting its entries in decreasing order. A common
choice for the scoring function is to make it linear in the
input X and consider the following class of vector-valued

consists

set

of

iid

functions:

Flin = {X 7→ Xw : X ∈ Rm×d, w ∈ Rd}.

(1)

Depending upon the regularization, we also consider the
following two subclasses of Flin :
F2 := {X 7→ Xw : X ∈ Rm×d, w ∈ Rd,kwk2 ≤ W2},
F1 := {X 7→ Xw : X ∈ Rm×d, w ∈ Rd,kwk1 ≤ W1}.
In the input space X , it is natural for the rows of X to have
a bound on the appropriate dual norm. Accordingly, when-
ever we use F2, the input space is set to X = {X ∈ Rm×d :
∀j ∈ [m], kXjk2 ≤ RX} where Xj denotes jth row of X
and [m] := {1, . . . , m}. Similarly, when we use F1, we set
X = {X ∈ Rm×d : ∀j ∈ [m], kXjk∞ ≤ ¯RX}. These
are natural counterparts to the following function classes
studied in binary classiﬁcation and regression:

G2 := {x 7→ hx, wi : kxk2 ≤ RX , w ∈ Rd,kwk2 ≤ W2},
G1 := {x 7→ hx, wi : kxk∞ ≤ ¯RX , w ∈ Rd,kwk1 ≤ W1}.
A key ingredient in the basic setup of the learning to rank
problem is a loss function φ : Rm × Y → R+ where R+
denotes the set of non-negative real numbers. Given a class
F of vector-valued functions, a loss φ yields a natural loss
class: namely the class of real-valued functions that one
gets by composing φ with functions in F:
φ ◦ F := {(X, y) 7→ φ(f (X), y) : X ∈ Rm×d, f ∈ F}.
For vector valued scores, the Lipschitz constant of φ de-
pends on the norm ||| · ||| that we decide to use in the score
space (||| · |||⋆ is dual of ||| · |||):
∀y ∈ Y, s, s′ ∈ Rm, |φ(s1, y)−φ(s2, y)| ≤ Gφ|||s1−s2|||.
If φ is differentiable, this is equivalent to: ∀y ∈ Y, s ∈
|||∇sφ(s, y)|||⋆ ≤ Gφ. Similarly, the smoothness
Rm,
constant Hφ of φ deﬁned as: ∀y ∈ Y, s, s′ ∈ Rm,

|||∇sφ(s1, y) − ∇sφ(s2, y)|||⋆ ≤ Hφ|||s1 − s2|||.

also depends on the norm used in the score space. If φ is
twice differentiable, the above inequality is equivalent to

∀y ∈ Y, s ∈ Rm, |||∇2

sφ(s, y)|||op ≤ Hφ

Generalization error bounds for learning to rank

where ||| · |||op is the operator norm induced by the pair
|||·|||,|||·|||⋆ and deﬁned as |||M|||op := supv6=0 |||M v|||⋆
.
|||v|||
Deﬁne the expected loss of w under the distribution D
Lφ(w) := E(X,y)∼D [φ(Xw, y)] and its empirical loss on
the sample as ˆLφ(w) := 1
i=1 φ(X (i)w, y(i)). The min-
imizer of Lφ(w) (resp. ˆLφ(w)) over some function class
(parameterized by w) will be denoted by w⋆ (resp. ˆw). We

nPn

may refer to expectations w.r.t. the sample usingbE [·]. To
reduce notational clutter, we often refer to (X, y) jointly by
Z and X × Y by Z. For vectors, hu, vi denotes the stan-
dard inner productPi uivi and for matrices U, V of the
same shape, hU, V i means Tr(U⊤V ) =Pij UijVij. The

set of m! permutation π of degree m is denoted by Sm. A
vector of ones is denoted by 1.

3. Application to Speciﬁc Losses
To whet the reader’s appetite for the technical presenta-
tion that follows, we will consider two loss functions,
one convex and one non-convex, to illustrate the con-
crete improvements offered by our new generalization
bounds. A generalization bound is of the form: Lφ( ˆw) ≤
Lφ(w⋆)+“complexity term”. It should be noted that w⋆
is not available to the learning algorithm as it needs knowl-
edge of underlying distribution of the data. The complexity
term of Chapelle & Wu (2010) is O(GCW
The constant GCW
is the Lipschitz constant of the sur-
rogate φ (viewed as a function of the score vector s)
w.r.t.
instead be of the

φ W2RXpm/n).
form O(GφW2RXp1/n), where Gφ is the Lipschitz con-
stant of φ w.r.t.
ℓ∞ norm. Note that our bounds are
free of any explicit m dependence. Also, by deﬁnition,
φ √m but the former can be much smaller
Gφ ≤ GCW
as the two examples below illustrate.
In benchmark
datasets (Liu et al., 2007), m can easily be in the 100-1000
range.

ℓ2 norm. Our bounds will

φ

3.1. Application to ListNet
ranking method (Cao et al., 2007) uses
The ListNet
a convex surrogate,
is deﬁned in the following
way1. Deﬁne m maps from Rm to R as: Pj(v) =
i=1 exp(vi) for j ∈ [m]. Then, we have, for

exp(vj)/Pm
s ∈ Rm and y ∈ Rm,

that

φLN(s, y) = −

mXj=1

Pj(y) log Pj(s).

An easy calculation shows that the Lipschitz (as well as
smoothness) constant of φLN is m independent.

1The ListNet paper actually deﬁnes a family of losses based on
probability models for top k documents. We use k = 1 in our def-
inition since that is the version implemented in their experimental
results.

Proposition 1. The Lipschitz (resp. smoothness) constant
of φLN w.r.t. k · k∞ satisﬁes GφLN ≤ 2 (resp. HφLN ≤ 2)
for any m ≥ 1.
Since the bounds above are independent of m, so the gen-
eralization bounds resulting from their use in Theorem 9
and Theorem 17 will also be independent of m (up to log-
arithmic factors). We are not aware of prior generalization
bounds for ListNet that do not scale with m. In particu-
lar, the results of Lan et al. (2009) have an m! dependence
since they consider the top-m version of ListNet. How-
ever, even if the top-1 variant above is considered, their
proof technique will result in at least a linear dependence
on m and does not result in as tight a bound as we get
from our general results. It is also easy to see that the Lip-
schitz constant GCW
of ListNet loss w.r.t. ℓ2 norm is also
φLN
2 and hence the bound of Chapelle & Wu (2010) necessar-
ily has a √m dependence in it. Moreover, generalization
error bounds for ListNet exploiting its smoothness will in-
terpolate between the pessimistic 1/√n and optimistic 1/n
rates. These have never been provided before.

3.2. Application to Smoothed DCG@1
This example is from the work of Chapelle & Wu (2010).
Smoothed DCG@1, a non-convex surrogate, is deﬁned as:

mXi=1

exp(si/σ)

Pj exp(sj/σ)

,

φSD(s, y) = D(1)

G(yi)

where D(i) = 1/ log2(1 + i) is the “discount” func-
tion and G(i) = 2i − 1 is the “gain” function. The
amount of smoothing is controlled by the parameter
σ > 0 and the smoothed version approaches DCG@1
as σ → 0 (DCG stands for Discounted Cumulative Gain
(J¨arvelin & Kek¨al¨ainen, 2002)).
Proposition 2. The Lipschitz constant of φSD w.r.t. k · k∞
satisﬁes GφSD ≤ 2D(1)G(Ymax)/σ for any m ≥ 1. Here
Ymax is maximum possible relevance score of a document
(usually less than 5).

As in the ListNet loss case we previously considered, the
generalization bound resulting from Theorem 9 will be in-
dependent of m. This is intuitively satisfying: DCG@1,
whose smoothing we are considering, only depends on the
document that is put in the top position by the score vec-
tor s (and not on the entire sorted order of s). Our gen-
eralization bound does not deteriorate as the total list size
m grows. In contrast, the bound of Chapelle & Wu (2010)
will necessarily deteriorate as √m since the constant GCW
φSD
is the same as GφSD. Moreover, it should be noted that even
in the original SmoothedDCG paper, σ is present in the de-
nominator of GCW
, so our results are directly comparable.
φSD
Also note that this example can easily be extended to con-
sider DCG@k for case when document list length m ≫ k
(a very common scenario in practice).

Generalization error bounds for learning to rank

3.3. Application to RankSVM
RankSVM (Joachims, 2002) is another well established
ranking method, which minimizes a convex surrogate based
on pairwise comparisons of documents. A number of stud-
ies have shown that ListNet has better empirical perfor-
mance than RankSVM. One possible reason for the better
performance of ListNet over RankSVM is that the Lips-
chitz constant of RankSVM surrogate w.r.t k·k∞ doe scale
with document list size as O(m2). Due to lack of space,
we give the details in the supplement.

4. Does The Length of Document Lists

Matter?

Our work is directly motivated by a very interesting gener-
alization bound for learning to rank due to Chapelle & Wu
(2010, Theorem 1). They considered a Lipschitz continu-
ous loss φ with Lipschitz constant GCW
φ w.r.t. the ℓ2 norm.
They show that, with probability at least 1 − δ,

∀w ∈ F2, Lφ(w) ≤ ˆLφ(w) + 3 GCW

φ W2RXr m
+r 8 log(1/δ)
φ W2RXpm/n).
The dominant term on the right is O(GCW
In the next three sections, we will derive improved bounds
φ √m
but can be much smaller. Before we do that, let us examine
the dimensionality reduction in linear scoring function that
is caused by a natural permutation invariance requirement.

of the form ˜O(GφW2RXp1/n) where Gφ ≤ GCW

n

.

n

4.1. Permutation invariance removes m dependence in

dimensionality of linear scoring functions

As stated in Section 2, a ranking is obtained by sorting a
score vector obtained via a linear scoring function f . Con-
sider the space of linear scoring function that consists of
all linear maps f that map Rm×d to Rm:

Ffull :=nX 7→ [hX, W1i , . . . , hX, Wmi]⊤ : Wi ∈ Rm×do .

These linear maps are fully parameterized by matrices
W1, . . . , Wm. Thus, a full parameterization of the linear
scoring function is of dimension m2d. Note that the popu-
larly used class of linear scoring functions Flin deﬁned in
Eq. 1 is actually a low d-dimensional subspace of the full
m2d dimensional space of all linear maps. It is important
to note that the dimension of Flin is independent of m.
In learning theory, one of the factors inﬂuencing the gen-
eralization error bound is the richness of the class of hy-
pothesis functions. Since the linear function class Flin has
dimension independent of m, we intuitively expect that,
at least under some conditions, algorithms that minimize
ranking losses using linear scoring functions should have

an m independent complexity term in the generalization
bound. The reader might wonder whether the dimension
reduction from m2d to d in going from Ffull to Flin is arbi-
trary. To dispel this doubt, we prove the lower dimensional
class Flin is the only sensible choice of linear scoring func-
tions in the learning to rank setting. This is because scoring
functions should satisfy a permutation invariance property.
That is, if we apply a permutation π ∈ Sm to the rows of X
to get a matrix πX then the scores should also simply get
permuted by π. That is, we should only consider scoring
functions in the following class:
Fperminv = {f : ∀π ∈ Sm,∀X ∈ Rm×d, πf (X) = f (πX)}.
The permutation invariance requirement, in turn, forces a
reduction from dimension m2d to just 2d (which has no
dependence on m).
Theorem 3. The intersection of the function classes Ffull
and Fperminv is the 2d-dimensional class:

F′lin = {X 7→ Xw + (1⊤Xv)1 : w, v ∈ Rd}.

(2)

Note that the extra degree of freedom provided by the v
parameter in Eq. 2 is useless for ranking purposes since
adding a constant vector (i.e., a multiple of 1) to a score
vector has no effect on the sorted order. This is why we
said that Flin is the only sensible choice of linear scoring
functions.

5. Online to Batch Conversion
In this section, we build some intuition as to why it is nat-
ural to use k · k∞ in deﬁning the Lipschitz constant of the
loss φ. To this end, consider the following well known on-
line gradient descent (OGD) regret guarantee. Recall that
OGD refers to the simple online algorithm that makes the
update wi+1 ← wi− η∇wi fi(wi) at time i. If we run OGD
to generate wi’s, we have, for all kwk2 ≤ W2:

nXi=1

fi(wi) −

fi(w) ≤

W 2
2
2η

+ ηG2n

nXi=1

where G is a bound on the maximum ℓ2-norm of
the gradients ∇wi fi(wi) and fi’s have to be convex.
If (X (1), y(1)), . . . , (X (n), y(n)) are iid then by setting
fi(w) = φ(X (i)w, y(i)), 1 ≤ i ≤ n we can do an “on-
line to batch conversion”. That is, we optimize over η, take
expectations and use Jensen’s inequality to get the follow-
ing excess risk bound:

∀kwk2 ≤ W2, E [Lφ( ˆwOGD)] − Lφ(w) ≤ W2Gr 2

n

i=1 wi and G has to satisfy (noting

where ˆwOGD = 1
that s = X (i)wi)

nPn

G ≥ k∇wi fi(wi)k2 = k(X (i))⊤∇sφ(X (i)wi, y(i))k2

Generalization error bounds for learning to rank

where we use the chain rule to express ∇w in terms of ∇s.
Finally, we can upper bound

k(X (i))⊤∇sφ(X (i)wi, y(i))k2
≤ k(X (i))⊤k1→2 · k∇sφ(X (i)wi, y(i))k1
≤ RXk∇sφ(X (i)wi, y(i))k1

j=1 kXjk2 and because of the following

as RX ≥ maxm
lemma.
Lemma 4. For any 1 ≤ p ≤ ∞,
kXvkq
kvkp

kXkp→q = sup
v6=0
kX⊤k1→p = kXkq→∞ =

m

j=1 kXjkp ,
max
q + 1
p = 1).

where q is the dual exponent of p (i.e., 1

Thus, we have shown the following result.
Theorem 5. Let φ be convex and have Lipschitz constant
Gφ w.r.t. k · k∞. Suppose we run online gradient descent
(with appropriate step size η) on fi(w) = φ(X (i)w, y(i))
and return ˆwOGD = 1

i=1 wi. Then we have,

∀kwk2 ≤ W2, E [Lφ( ˆwOGD)]−Lφ(w) ≤ Gφ W2 RXr 2

n

T Pn

.

The above excess risk bound has no explicit m depen-
dence. This is encouraging but there are two deﬁciencies
of this approach based on online regret bounds. First, the
result applies to the output of a speciﬁc algorithm that may
not be the method of choice for practitioners. For exam-
ple, the above argument does not yield uniform conver-
gence bounds that could lead to excess risk bounds for
ERM (or regularized versions of it). Second, there is no
way to generalize the result to Lipschitz, but non-convex
loss functions. It may noted here that the original motiva-
tion for Chapelle & Wu (2010) to prove their generalization
bound was to consider the non-convex loss used in their
SmoothRank method. We will address these issues in the
next two sections.

6. Stochastic Convex Optimization
We ﬁrst deﬁne the regularized empirical risk minimizer:

ˆwλ = argmin
kwk2≤W2

λ
2kwk2

2 + ˆLφ(w).

(3)

We now state the main result of this section.
Theorem 6. Let the loss function φ be convex and have
Lipschitz constant Gφ w.r.t. k·k∞. Then, for an appropriate
choice of λ = O(1/√n), we have

E [Lφ( ˆwλ)] ≤ Lφ(w⋆) + 2 Gφ RX W2   8

n

n! .
+r 2

This result applies to a batch algorithm (regularized ERM)
but unfortunately requires the regularization parameter λ
to be set in a particular way. Also, it does not apply to
non-convex losses and does not yield uniform convergence
bounds. In the next section, we will address these deﬁcien-
cies. However, we will incur some extra logarithmic factors
that are absent in the clean bound above.

7. Bounds for Non-convex Losses
The above discussion suggests that we have a possibil-
ity of deriving tighter, possibly m-independent, general-
ization error bounds by assuming that φ is Lipschitz con-
tinuous w.r.t. k · k∞. The standard approach in binary
classiﬁcation is to appeal to the Ledoux-Talagrand con-
traction principle for establishing Rademacher complexity
(Bartlett & Mendelson, 2003). It gets rid of the loss func-
tion and incurs a factor equal to the Lipschitz constant of
the loss in the Rademacher complexity bound. Since the
loss function takes scalar argument, the Lipschitz constant
is deﬁned for only one norm, i.e., the absolute value norm.
It is not immediately clear how such an approach would
work when the loss takes vector valued arguments and is
Lipschitz w.r.t. k · k∞. We are not aware of an appropriate
extension of the Ledoux-Talagrand contraction principle.
Note that Lipschitz continuity w.r.t.
the Euclidean norm
k · k2 does not pose a signiﬁcant challenge since Slepian’s
lemma can be applied to get rid of the loss. Several au-
thors have already exploited Slepian’s lemma in this con-
text (Bartlett & Mendelson, 2003; Chapelle & Wu, 2010).
We take a route involving covering numbers and deﬁne the
data-dependent (pseudo-)metric:

n

max

i=1 (cid:12)(cid:12)(cid:12)φ(X (i)w, y(i)) − φ(X (i)w′, y(i))(cid:12)(cid:12)(cid:12)

dZ(1:n)
∞ (w, w′) :=
Let N∞(ǫ, φ ◦ F , Z (1:n)) be the covering number at scale
ǫ of the composite class φ◦ F = φ◦ F1 or φ◦ F2 w.r.t. the
above metric. Also deﬁne

N∞(ǫ, φ ◦ F , n) := max

Z(1:n) N∞(ǫ, φ ◦ F , Z (1:n)).

With these deﬁnitions in place, we can state our ﬁrst result
on covering numbers.
Proposition 7. Let the loss φ be Lipschitz w.r.t. k · k∞
with constant Gφ. Then following covering number bound
holds:
log2 N∞(ǫ, φ ◦ F2, n) ≤& G2
' log2(2mn + 1).

φ W 2
2 R2
ǫ2

X

Proof. Note that

n

max

i=1 (cid:12)(cid:12)(cid:12)φ(X (i)w, y(i)) − φ(X (i)w′, y(i))(cid:12)(cid:12)(cid:12)
, w′E(cid:12)(cid:12)(cid:12) .
, wE −DX (i)

j=1 (cid:12)(cid:12)(cid:12)DX (i)

max
i=1

max

m

n

j

j

≤ Gφ ·

Generalization error bounds for learning to rank

n

m

max

max
i=1

This immediately implies that if we have a cover of the
class G2 (Sec.2) at scale ǫ/Gφ w.r.t. the metric

j=1 (cid:12)(cid:12)(cid:12)DX (i)

, wE −DX (i)
then it is also a cover of φ◦F2 w.r.t. dZ(1:n)
, at scale ǫ. Now
comes a simple, but crucial observation: from the point of
view of the scalar valued function class G2, the vectors
(X (i)
j=1:m constitute a data set of size mn. Therefore,

, w′E(cid:12)(cid:12)(cid:12)

j )i=1:n

∞

j

j

N∞(ǫ, φ ◦ F2, n) ≤ N∞(ǫ/Gφ,G2, mn).

(4)

Now we appeal to the following bound due to Zhang (2002,
Corollary 3) (and plug the result into (4)):

log2 N∞(ǫ/Gφ,G2, mn) ≤& G2

φ W 2
2 R2
ǫ2

X

' log2(2mn+1)

Covering number N2(ǫ, φ◦F , Z (1:n)) uses pseudo-metric:

dZ(1:n)

2

(w, w′) :=  nXi=1

1

n(cid:16)φ(X (i)w, y(i)) − φ(X (i)w′, y(i))(cid:17)2!1/2

It is well known that a control on N2(ǫ, φ ◦ F , Z (1:n))
provides control on the empirical Rademacher complexity
and that N2 covering numbers are smaller than N∞ ones.
For us, it will be convenient to use a more reﬁned version2
due to Mendelson (2002). Let H be a class of functions,
with H : Z 7→ R, uniformly bounded by B. Then, we have
following bound on empirical Rademacher complexity
bRn (H)
α>04α + 10Z suph∈H qbE[h2]
α>0 4α + 10Z B
Here bRn (H) is the empirical Rademacher complexity of
the class H deﬁned as

α r log2 N2(ǫ,H, Z(1:n))

r log2 N2(ǫ,H, Z(1:n))

dǫ! .

≤ inf

≤ inf

(6)

n

n

α

dǫ(5)

bRn (H) := Eσ1:n"sup

h∈H

1
n

nXi=1

σih(Zi)# ,

where σ1:n = (σ1, . . . , σn) are iid Rademacher (symmetric
Bernoulli) random variables.

2We

use

due
to
at
http://ttic.uchicago.edu/˜karthik/dudley.pdf

further
Sridharan

reﬁnement

available

a
and

Srebro

Corollary 8. Let φ be Lipschitz w.r.t. k · k∞ and uni-
formly bounded3 by B for w ∈ F2. Then the empirical
Rademacher complexities of the class φ◦ F2 is bounded as

bRn (φ ◦ F2) ≤ 10GφW2RXr log2(3mn)

5GφW2RX√log2(3mn)

6B√n

× log

n

.

Proof. This follows by simply plugging in estimates from
Proposition 7 into (6) and choosing α optimally.

Control on the Rademacher complexity immediately leads
to uniform convergence bounds and generalization error
bounds for ERM. The informal ˜O notation hides factors
logarithmic in m, n, B, Gφ, RX , W1. Note that all hidden
factors are small and computable from the results above.
Theorem 9. Suppose φ is Lipschitz w.r.t. k · k∞ with con-
stant Gφ and is uniformly bounded by B as w varies over
F2. With probability at least 1 − δ,

∀w ∈ F2, Lφ(w) ≤ ˆLφ(w)

+ ˜O GφW2RXr 1

n

+ Br log(1/δ)

n

!

and therefore with probability at least 1 − 2δ,

Lφ( ˆw) ≤ Lφ(w⋆)+ ˜O GφW2RXr 1

n

+ Br log(1/δ)

n

! .

where ˆw is an empirical risk minimizer over F2.
Proof. Follows from standard bounds using Rademacher
complexity.
for example, Bartlett & Mendelson
(2003).

See,

As we said before, ignoring logarithmic factors, the bound
for F2 is an improvement over the bound of Chapelle & Wu
(2010).

8. Extensions
We extend the generalization bounds above to two settings:
a) high dimensional features and b) smooth losses.

8.1. High-dimensional features
In learning to rank situations involving high dimensional
features, it may not be appropriate to use the class F2 of
ℓ2 bounded predictors. Instead, we would like to consider
the class F1 of ℓ1 bounded predictors. In this case, it is
3A uniform bound on the loss easily follows under the
(very reasonable) assumption that ∀y,∃sy s.t. φ(sy, y) = 0.
Then φ(Xw, y) ≤ GφkXw − syk∞ ≤ Gφ(W2RX +
maxy∈Y ksyk∞) ≤ Gφ(2W2RX ).

Generalization error bounds for learning to rank

natural to measure size of the input matrix X in terms of
a bound ¯RX on the maximum ℓ∞ norm of each of its row.
The following analogue of Proposition 7 can be shown.
Proposition 10. Let the loss φ be Lipschitz w.r.t. k·k∞ with
constant Gφ. Then the following covering number bound
holds:

log2 N∞(ǫ, φ ◦ F1, n) ≤& 288 G2

φ W 2
1

¯R2
X (2 + log d)
ǫ2

'
(cid:25) mn + 1(cid:19) .

× log2(cid:18)2(cid:24) 8GφW1 ¯RX

ǫ

Using the above result to control the Rademacher complex-
ity of φ ◦ F1 gives the following bound.
Corollary 11. Let φ be Lipschitz w.r.t. k·k∞ and uniformly
bounded by B for w ∈ F1. Then the empirical Rademacher
complexities of the class φ ◦ F1 is bounded as

bRn (φ ◦ F1) ≤ 120√2GφW1 ¯RXr log(d) log2(24mnGφW1 ¯RX )

¯RX√log(d) log2(24mnGφ W1

40√2GφW1

B+24mnGφ W1

× log2

n
¯RX

¯RX )

.

As in the previous section, control of Rademacher com-
plexity immediately yields uniform convergence and ERM
generalization error bounds.
Theorem 12. Suppose φ is Lipschitz w.r.t. k · k∞ with
constant Gφ and is uniformly bounded by B as w varies
over F1. With probability at least 1 − δ,

∀w ∈ F1, Lφ(w) ≤ ˆLφ(w)

+ ˜O GφW1 ¯RXr log d

n

+ Br log(1/δ)

n

!

and therefore with probability at least 1 − 2δ,

Lφ( ˆw) ≤ Lφ(w⋆)+ ˜O GφW1 ¯RXr log d

n

+ Br log(1/δ)

n

!

where ˆw is an empirical risk minimizer over F1.
As can be easily seen from Theorem. 12, the generalization
bound is almost independent of the dimension of the docu-
ment feature vectors. We are not aware of existence of such
a result in learning to rank literature.

8.2. Smooth losses
We will again use online regret bounds to explain why we
should expect “optimistic” rates for smooth losses before
giving more general results for smooth but possibly non-
convex losses.

8.3. Online regret bounds under smoothness
Let us go back to OGD guarantee, this time presented in a
slightly more reﬁned version. If we run OGD with learning
rate η then, for all kwk2 ≤ W2:

nXi=1

fi(wi) −

nXi=1

fi(w) ≤

W 2
2
2η

+ η

nXi=1

kgik2

2

where gi = ∇wi fi(wi) (if fi is not differentiable at wi
then we can set gi to be an arbitrary subgradient of fi at
wi). Now assume that all fi’s are non-negative functions
and are smooth w.r.t. k·k2 with constant H. Lemma 3.1 of
Srebro et al. (2010) tells us that any non-negative, smooth
function f (w) enjoy an important self-bounding property
for the gradient:

k∇wfi(w)k2 ≤p4Hfi(w)
which bounds the magnitude of the gradient of f at a point
in terms of the value of the function itself at that point. This
2 ≤ 4Hfi(wi) which, when plugged into
means that kgik2
the OGD guarantee, gives:

nXi=1

fi(wi) −

nXi=1

fi(w) ≤

W 2
2
2η

+ 4ηH

fi(wi)

nXi=1

Again, setting fi(w) = φ(X (i)w, y(i)), 1 ≤ t ≤ n, and us-
ing the online to batch conversion technique, we can arrive
at the bound: for all kwk2 ≤ W2:

E [Lφ( ˆw)] ≤

Lφ(w)
(1 − 4ηH)

+

W 2
2

2η(1 − 4ηH)n

At this stage, we can ﬁx w = w⋆, the optimal ℓ2-norm
bounded predictor and get optimal η as:

2 + 2HLφ(w⋆)n

.

(7)

After plugging this value of η in the bound above and some
algebra (see Section H), we get the upper bound

W2

η =

4HW2 + 2p4H 2W 2
E [Lφ( ˆw)] ≤ Lφ(w⋆) + 2r 2HW 2

2 Lφ(w⋆)
n

8HW 2
2

n

+

.
(8)
Such a rate interpolates between a 1/√n rate in the “pes-
simistic” case (Lφ(w⋆) > 0) and the 1/n rate in the “op-
timistic” case (Lφ(w⋆) = 0) (this terminology is due to
Panchenko (2002)).
Now, assuming φ to be twice differentiable, we need H
such that
H ≥ k∇2
sφ(X (i)w, y(i))Xk2→2
where we used the chain rule to express ∇2
w in terms of
s. Note that, for OGD, we need smoothness in w w.r.t.
∇2

wφ(X (i)w, y(i))k2→2 = kX⊤∇2

Generalization error bounds for learning to rank

k · k2 which is why the matrix norm above is the operator
norm corresponding to the pair k·k2,k·k2. In fact, when we
say “operator norm” without mentioning the pair of norms
involved, it is this norm that is usually meant. It is well
known that this norm is equal to the largest singular value
of the matrix. But, just as before, we can bound this in
terms of the smoothness constant of φ w.r.t. k · k∞ (see
Section I in the appendix):
k(X (i))⊤∇2
≤ R2

sφ(X (i)w, y(i))X (i)k2→2

sφ(X (i)w, y(i))k∞→1.

Xk∇2

where we used Lemma 4 once again. This result using on-
line regret bounds is great for building intuition but suffers
from the two defects we mentioned at the end of Section 5.
In the smoothness case, it additionally suffers from a more
serious defect: the correct choice of the learning rate η re-
quires knowledge of Lφ(w⋆) which is seldom available.

8.4. Generalization error bounds under smoothness
Once again, to prove a general result for possibly non-
convex smooth losses, we will adopt an approach based on
covering numbers. To begin, we will need a useful lemma
from Srebro et al. (2010, Lemma A.1 in the Supplementary
Material). Note that, for functions over real valued predic-
tions, we do not need to talk about the norm when dealing
with smoothness since essentially the only norm available
is the absolute value.
Lemma 13. For any h-smooth non-negative function f :
R → R+ and any t, r ∈ R we have

(f (t) − f (r))2 ≤ 6h(f (t) + f (r))(t − r)2.

We ﬁrst provide an extension of this lemma to the vector
case.
Lemma 14. If φ : Rm → R+ is a non-negative function
with smoothness constant Hφ w.r.t. a norm ||| · ||| then for
any s1, s2 ∈ Rm we have
(φ(s1) − φ(s2))2 ≤ 6Hφ · (φ(s1) + φ(s2)) · |||s1 − s2|||2.
Using the basic idea behind local Rademacher complexity
analysis, we deﬁne the following loss class:
Fφ,2(r) := {(X, y) 7→ φ(Xw, y) : kwk2 ≤ W2, ˆLφ(w) ≤ r}.
Note that this is a random subclass of functions since
ˆLφ(w) is a random variable.
Proposition 15. Let φ be smooth w.r.t. k·k∞ with constant
Hφ. The covering numbers of Fφ,2(r) in the dZ(1:n)
metric
deﬁned above are bounded as follows:
log2 N2(ǫ,Fφ,2(r), Z(1:n)) ≤(cid:24) 12Hφ W 2

(cid:25) log2(2mn+1).

2 R2

X r

ǫ2

2

Control of covering numbers easily gives a control on the
Rademacher complexity of the random subclass Fφ,2(r).
Corollary 16. Let φ be smooth w.r.t. k · k∞ with constant
Hφ and uniformly bounded by B for w ∈ F2. Then the
empirical Rademacher complexity of the class Fφ,2(r) is
bounded as

bRn (Fφ,2(r)) ≤ 4√rC log
where C = 5√3W2RXq Hφ log2(3mn)

n

.

3√B
C

With the above corollary in place we can now prove our
second key result.
Theorem 17. Suppose φ is smooth w.r.t. k · k∞ with con-
stant Hφ and is uniformly bounded by B over F2. With
probability at least 1 − δ,

∀w ∈ F2, Lφ(w) ≤ ˆLφ(w) + ˜O r Lφ(w)D0

n

D0

n !

+

where D0 = B log(1/δ) + W 2
probability at least 1 − 2δ,

2 R2

X Hφ. Moreover, with

Lφ( ˆw) ≤ Lφ(w⋆) + ˜O r Lφ(w⋆)D0

n

D0

n !

+

where ˆw, w⋆ are minimizers of ˆLφ(w) and Lφ(w) respec-
tively (over w ∈ F2).
9. Conclusion
We showed that it is not necessary for generalization error
bounds for query-level learning to rank algorithms to dete-
riorate with increasing length of document lists associated
with queries. The key idea behind our improved bounds
was deﬁning Lipschitz constants w.r.t. ℓ∞ norm instead of
the “standard” ℓ2 norm. As a result, we were able to derive
much tighter guarantees for popular loss functions such as
ListNet and Smoothed DCG@1 than previously available.
Our generalization analysis of learning to rank algorithms
paves the way for further interesting work. One possibil-
ity is to use these bounds to design active learning algo-
rithms for learning to rank with formal label complexity
guarantees. Another interesting possibility is to consider
other problems, such as multi-label learning, where func-
tions with vector-valued outputs are learned by optimizing
a joint function of those outputs.

Acknowledgement
We gratefully acknowledge the support of NSF under grant
IIS-1319810. Thanks to Prateek Jain for discussions that
led us to Theorem 3.

Generalization error bounds for learning to rank

Mendelson, Shahar. Rademacher averages and phase tran-
sitions in Glivenko-Cantelli classes. IEEE Transactions
on Information Theory, 48(1):251–263, 2002.

Panchenko, Dmitriy. Some extensions of an inequality of
Vapnik and Chervonenkis. Electronic Communications
in Probability, 7:55–65, 2002.

Shalev-Shwartz, Shai, Shamir, Ohad, Srebro, Nathan, and
Sridharan, Karthik. Stochastic convex optimization. In
Proceedings of the 22nd Annual Conference on Learning
Theory, 2009.

Srebro, Nathan, Sridharan, Karthik, and Tewari, Ambuj.
Smoothness, low noise, and fast rates. In Advances in
Neural Information Processing Systems 23, pp. 2199–
2207, 2010.

Yue, Yisong, Finley, Thomas, Radlinski, Filip, and
Joachims, Thorsten. A support vector method for opti-
mizing average precision. In Proceedings of the 30th An-
nual International ACM SIGIR Conference on Research
and Development in Information Retrieval, pp. 271–278,
2007.

Zhang, Tong. Covering number bounds of certain regu-
larized linear function classes. The Journal of Machine
Learning Research, 2:527–550, 2002.

References
Bartlett, Peter L. and Mendelson, Shahar. Rademacher and
Gaussian complexities: Risk bounds and structural re-
sults. The Journal of Machine Learning Research, 3:
463–482, 2003.

Bousquet, Olivier. Concentration inequalities and empiri-
cal processes theory applied to the analysis of learning
algorithms. PhD thesis, Ecole Polytechnique, 2002.

Cao, Zhe, Qin, Tao, Liu, Tie-Yan, Tsai, Ming-Feng, and Li,
Hang. Learning to rank: from pairwise approach to list-
wise approach. In Proceedings of the 24th International
Conference on Machine Learning, pp. 129–136, 2007.

Chapelle, Olivier and Wu, Mingrui. Gradient descent opti-
mization of smoothed information retrieval metrics. In-
formation retrieval, 13(3):216–235, 2010.

Chapelle, Olivier, Metlzer, Donald, Zhang, Ya, and
Grinspan, Pierre. Expected reciprocal rank for graded
relevance. In Proceedings of the 18th ACM Conference
on Information and Knowledge Management, pp. 621–
630. ACM, 2009.

Chapelle, Olivier, Chang, Yi, and Liu, Tie-Yan. Future di-
rections in learning to rank. In Proceedings of the Ya-
hoo! Learning to Rank Challenge June 25, 2010, Haifa,
Israel, Journal of Machine Learning Research Workshop
and Conference Proceedings, pp. 91–100, 2011.

J¨arvelin, Kalervo and Kek¨al¨ainen, Jaana. Cumulated gain-
based evaluation of IR techniques. ACM Transactions
on Information Systems, 20(4):422–446, 2002.

Joachims, Thorsten. Optimizing search engines using
clickthrough data.
In Proceedings of the 8th ACM
SIGKDD International Conference on Knowledge Dis-
covery and Data Mining, pp. 133–142. ACM, 2002.

Lan, Yanyan, Liu, Tie-Yan, Qin, Tao, Ma, Zhiming, and Li,
Hang. Query-level stability and generalization in learn-
ing to rank.
In Proceedings of the 25th International
Conference on Machine Learning, pp. 512–519. ACM,
2008.

Lan, Yanyan, Liu, Tie-Yan, Ma, Zhiming, and Li, Hang.
Generalization analysis of listwise learning-to-rank al-
gorithms.
In Proceedings of the 26th Annual Interna-
tional Conference on Machine Learning, pp. 577–584,
2009.

Liu, Tie-yan, Xu, Jun, Qin, Tao, Xiong, Wenying, and Li,
Hang. LETOR: Benchmark dataset for research on learn-
ing to rank for information retrieval. In Proceedings of
SIGIR 2007 Workshop on Learning to Rank for Informa-
tion Retrieval, pp. 3–10, 2007.

Generalization error bounds for learning to rank

A. Proof of Proposition 1
Proof. Let ej’s denote standard basis vectors. We have

Therefore,

We also have

Moreover,

exp(sj)
j′=1 exp(sj′ )

ej

exp(sj)
Pm
j′=1 exp(sj′ )kejk1

exp(sj )

Pm
j′ =1 exp(sj′ )

if j = k
if j 6= k .

[∇2

= 2.

exp(2sj )

Pj(y)ej +

Pm

Pj (y)kejk1 +

k∇sφLN(s, y)k1 ≤

∇sφLN(s, y) = −

mXj=1
mXj=1

mXj=1
mXj=1
sφLN(s, y)]j,k =
mXk=1
mXj=1
|[∇2
mXk=1
mXj=1
exp(sj + sk)
(Pm
j′=1 exp(sj′ ))2 +
j′=1 exp(sj′ ))2 + Pm
(Pm
Pm
(Pm

−
(Pm
− exp(sj +sk)
(Pm

sφLN(s, y)]j,k|

j′ =1 exp(sj′ ))2 +

j=1 exp(sj ))2

j′ =1 exp(sj′ ))2

≤

=

= 2

k∇2

sφLN(s, y)k∞→1 ≤

exp(sj)
j′=1 exp(sj′ )

mXj=1

Pm

j=1 exp(sj)
j′=1 exp(sj′ )

B. Proof of Proposition 2
Proof. Let 1(condition) denote an indicator variable. We have

[∇sφSD(s, y)]j = D(1)  mXi=1

Therefore,

σ

G(ri)" 1
mXj=1  mXi=1" 1
σ  Pj exp(sj/σ)
Pj′ exp(sj′ /σ)

σ

1

2
σ

.

exp(si/σ)

Pj′ exp(sj′ /σ)

1(i=j) −

exp(si/σ)

Pj′ exp(sj′ /σ)

+

1(i=j) +

1
σ

(Pj′ exp(sj′ /σ))2!
(Pj exp(sj/σ))2

1
σ

exp((si + sj)/σ)

(Pj′ exp(sj′ /σ))2#!
(Pj′ exp(sj′ /σ))2#!

exp((si + sj)/σ)

k∇sφSD(s, y)k1
D(1)G(Ymax) ≤

=

=

C. RankSVM
The RankSVM surrogate is deﬁned as:

φRS(s, y) =

mXi=1

mXj=1

max(0, 1(yi>yj )(1 + sj − si))

Generalization error bounds for learning to rank

It is easy to see that ∇sφRS(s, y) =Pm

is O(m2) .

i=1Pm

j=1 max(0, 1(yi>yj )(1 + sj − si))(ej − ei). Thus, the ℓ1 norm of gradient

D. Proof of Theorem 3
Proof. It is straightforward to check that F′lin is contained in both Ffull as well as Fperminv. So, we just need to prove that
any f that is in both Ffull and Fperminv has to be in F′lin as well.
Let Pπ denote the m × m permutation matrix corresponding to a permutation π. Consider the full linear class Ffull. In
matrix notation, the permutation invariance property means that, for any π, X, we have Pπ[hX, W1i , . . . ,hX, Wmii]⊤ =
[hPπX, W1i , . . . ,hPπX, Wmi]⊤.
Let ρ1 = {Pπ : π(1) = 1}, where π(i) denotes the index of the element in the ith position according to permutation π.
Fix any P ∈ ρ1. Then, for any X, hX, W1i = hP X, W1i. This implies that, for all X, Tr(W1⊤X) = Tr(W1⊤P X).
Using the fact that Tr(A⊤X) = Tr(B⊤X),∀X implies A = B, we have that W1⊤ = W1⊤P . Because P ⊤ = P −1, this
means P W1 = W1. This shows that all rows of W1, other than 1st row, are the same but perhaps different from 1st row.
By considering ρi = {Pπ : π(i) = i} for i > 1, the same reasoning shows that, for each i, all rows of Wi, other than ith
row, are the same but possibly different from ith row.
Let ρ1↔2 = {Pπ : π(1) = 2, π(2) = 1}. Fix any P ∈ ρ1↔2. Then, for any X, hX, W2i = hP X, W1i and hX, W1i =
hP X, W2i. Thus, we have W ⊤2 = W ⊤1 P as well as W ⊤1 = W ⊤2 P which means P W2 = W1, P W1 = W2. This shows
that row 1 of W1 and row 2 of W2 are the same. Moreover, row 2 of W1 and row 1 of W2 are the same. Thus, for some
u, u′ ∈ Rd, W1 is of the form [u|u′|u′| . . .|u′]⊤ and W2 is of the form [u′|u|u′| . . .|u′]⊤. Repeating this argument by
considering ρ1↔i for i > 2 shows that Wi is of the same form (u in row i and u′ elsewhere).
Therefore, we have proved that any linear map that is permutation invariant has to be of the form:

X 7→u⊤Xi + (u′)⊤Xj6=i

Xj

m

.

i=1

We can reparameterize above using w = u − u′ and v = u′ which proves the result.
E. Proof of Lemma 4
Proof. The ﬁrst equality is true because

kX⊤k1→p = sup
v6=0

kX⊤vkp
kvk1

= sup
v6=0

u6=0(cid:10)X⊤v, u(cid:11)

sup

kvk1kukq
kXuk∞
kukq

= sup
u6=0

hv, Xui
kvk1kukq

The second is true because

= sup
u6=0

sup
v6=0

= kXkq→∞.

kXkq→∞ = sup
u6=0
m

=

max
j=1

kXuk∞
kukq
sup
u6=0

= sup
u6=0
|hXj, ui|

kukq

m

max
j=1

|hXj, ui|

kukq

=

m

j=1 kXjkp.
max

F. Proof of Theorem 6
Our theorem is developed from the “expectation version” of Theorem 6 of Shalev-Shwartz et al. (2009) that was originally
given in probabilistic form. The expected version is as follows.
Let Z be a space endowed with a probability distribution generating iid draws Z1, . . . , Zn. Let W ⊆ Rd and f : W ×Z →

Generalization error bounds for learning to rank

R be λ-strongly convex4 and G-Lipschitz (w.r.t. k · k2) in w for every z. We deﬁne F (w) = E [f (w, Z)] and let

w⋆ = argmin
w∈W

F (w),

ˆw = argmin
w∈W

1
n

nXi=1

f (w, Zi).

Then E [F ( ˆw) − F (w⋆)] ≤ 4G2
carefully going through the proof of Theorem 6 proved by Shalev-Shwartz et al. (2009).
We now derive the “expectation version” of Theorem 7 of Shalev-Shwartz et al. (2009). Deﬁne the regularized empirical
risk minimizer as follows:

λn , where the expectation is taken over the sample. The above inequality can be proved by

ˆwλ = argmin

w∈W

λ
2 kwk2

2 +

1
n

nXi=1

f (w, Zi).

(9)

The following result gives optimality guarantees for the regularized empirical risk minimizer.
Theorem 18. Let W = {w : kwk2 ≤ W2} and let f (w, z) be convex and G-Lipschitz (w.r.t. k · k2) in w for every z. Let
Z1, ..., Zn be iid samples and let λ =r 4G2

. Then for ˆwλ and w⋆ as deﬁned above, we have

W 2
2
2 +

4W 2
2

n

n

E [F ( ˆwλ) − F (w⋆)] ≤ 2 G W2  8

n

n! .
+r 2

(10)

2kwk2

Thus, we get

E(cid:20) λ
2k ˆwλk2

2 + F ( ˆwλ)(cid:21) ≤ min

Proof. Let rλ(w, z) = λ
Applying “expectation version” of Theorem 6 of Shalev-Shwartz et al. (2009) to rλ, we get

2 + f (w, z). Then rλ is λ-strongly convex with Lipschitz constant λW2 + G in k · k2.
w∈W (cid:26) λ
2 + F (w)(cid:27) +
E [F ( ˆwλ) − F (w⋆)] ≤
Minimizing the upper bound w.r.t. λ, we get λ =q 4G2
n r 1
using the fact that √a + b ≤ √a + √b ﬁnishes the proof of Theorem 18.

. Plugging this choice back in the equation above and

λ
2 kw⋆k2

2 + F (w∗) +

4(λW2 + G)2

4(λW2 + G)2

4(λW2 + G)2

2kwk2

λW 2
2

≤

W 2
2
2 +

4W 2
2

n

+

2

λn

λn

.

λn

.

We now have all ingredients to prove Theorem 6.

Proof of Theorem 6. Let Z = X × Y and f (w, z) = φ(Xw, y) and apply Theorem 18. Finally note that if φ is Gφ-
Lipschitz w.r.t. k · k∞ and every row of X ∈ Rm×d has Euclidean norm bounded by RX then f (·, z) is GφRX-Lipschitz
w.r.t. k · k2 in w.
G. Proof of Theorem 12
Proof. Following exactly the same line of reasoning (reducing a sample of size n, where each prediction is Rm-valued, to
an sample of size mn, where each prediction is real valued) as in the beginning of proof of Proposition 7, we have

Plugging in the following bound due to Zhang (2002, Corollary 5):

N∞(ǫ, φ ◦ F1, n) ≤ N∞(ǫ/Gφ,G1, mn).

(11)

log2 N∞(ǫ/Gφ,G1, mn) ≤& 288 G2

φ W 2
1

¯R2
ǫ2

X (2 + ln d)

'
× log2(cid:0)2⌈8GφW1 ¯RX /ǫ⌉mn + 1(cid:1)

into (11) respectively proves the result.

4Recall that a function is called λ-strongly convex (w.r.t. k · k2) iff f − λ

2 k · k2

2 is convex.

Generalization error bounds for learning to rank

H. Calculations involved in deriving Equation (8)
Plugging in the value of η from (7) into the expression

yields (using the shorthand L⋆ for Lφ(w⋆))

Lφ(w⋆)
(1 − 4ηH)

+

W 2
2

2η(1 − 4ηH)n

L⋆ +

2HW2L⋆

2 + 2HL⋆n

p4H 2W 2

W2

n "

+

4H 2W 2
2

p4H 2W 2

2 + 2HL⋆n

+q4H 2W 2

2 + 2HL⋆n + 4HW2#

Denoting HW 2

2 /n by x, this simpliﬁes to

L⋆ +

2√xL⋆ + 4x√x

√4x + 2L⋆

+ √x√4x + 2L⋆ + 4x.

Using the arithmetic mean-geometric mean inequality to upper bound the middle two terms gives

Finally, using √a + b ≤ √a + √b, we get our ﬁnal upper bound

L⋆ + 2p2xL⋆ + 4x2 + 4x.
L⋆ + 2√2xL⋆ + 8x.

I. Calculation of smoothness constant

k(X (i))⊤∇2

sφ(X (i)w, y(i))X (i)k2→2 = sup
v6=0

k(X (i))⊤∇2

sφ(X (i)w, y(i))X (i)vk2

kvk2

k(X (i))⊤k1→2 · k∇2

sφ(X (i)w, y(i))k∞→1 · kX (i)vk∞

kvk2

sφ(X (i)w, y(i))X (i)vk1
kvk2
sφ(X (i)w, y(i))k∞→1 · kX (i)k2→∞ · kvk2

≤ sup
v6=0

k(X (i))⊤k1→2k∇2

k(X (i))⊤k1→2 · k∇2

≤ sup
v6=0
≤ sup
v6=0
≤(cid:18) m
j k(cid:19)2
j=1 kX (i)
sφ(X (i)w, y(i))k∞→1.
≤ R2
Xk∇2

· k∇2

max

kvk2

sφ(X (i)w, y(i))k∞→1

J. Proof of Lemma 14
Proof. Consider the function

It is clearly non-negative. Moreover

f (t) = φ((1 − t)s1 + ts2).

|f′(t1) − f′(t2)| = |h∇sφ(s1 + t1(s2 − s1)) − ∇sφ(s1 + t2(s2 − s1)), s2 − s1i|

≤ |||∇sφ(s1 + t1(s2 − s1)) − ∇sφ(s1 + t2(s2 − s1))|||⋆ · |||s2 − s1|||
≤ Hφ |t1 − t2||||s2 − s1|||2

and therefore it is smooth with constant h = Hφ|||s2 − s1|||2. Appealing to Lemma 13 now gives

(f (1) − f (0))2 ≤ 6Hφ|||s2 − s1|||2(f (1) + f (0))(1 − 0)2

which proves the lemma since f (0) = φ(s1) and f (1) = φ(s2).

Generalization error bounds for learning to rank

K. Proof of Proposition 15
Proof. Let w, w′ ∈ Fφ,2(r). Using Lemma 14

1

1

nXi=1

≤ 6Hφ
· kX (i)w − X (i)w′k2
∞
≤ 6Hφ ·
nXi=1
1

n(cid:16)φ(X (i)w, y(i)) − φ(X (i)w′, y(i))(cid:17)2
n(cid:16)φ(X (i)w, y(i)) + φ(X (i)w′, y(i))(cid:17)
nXi=1
i=1 kX (i)w − X (i)w′k2
max
∞
n(cid:16)φ(X (i)w, y(i)) + φ(X (i)w′, y(i))(cid:17)
i=1 kX (i)w − X (i)w′k2
max
i=1 kX (i)w − X (i)w′k2
max
∞.

= 6Hφ ·
≤ 12Hφr ·

·

n

n

n

∞ ·(cid:16) ˆLφ(w) + ˆLφ(w′)(cid:17)

where the last inequality follows because ˆLφ(w) + ˆLφ(w′) ≤ 2r.
This immediately implies that if we have a cover of the class G2 at scale ǫ/p12Hφr w.r.t. the metric

m

n

then it is also a cover of Fφ,2(r) w.r.t. dZ(1:n)

2

. Therefore, we have

max
i=1

max

j=1 (cid:12)(cid:12)(cid:12)DX (i)

j

, wE −DX (i)

j

, w′E(cid:12)(cid:12)(cid:12)

Appealing once again to a result by Zhang (2002, Corollary 3), we get

N2(ǫ,Fφ,2(r), Z (1:n)) ≤ N∞(ǫ/p12Hφr,G2, mn).
(cid:25)
log2 N∞(ǫ/p12Hφr,G2, mn) ≤(cid:24) 12Hφ W 2

2 R2

X r

ǫ2

× log2(2mn + 1)

which ﬁnishes the proof.

L. Proof of Corollary 16
Proof. We plug in Proposition 15’s estimate into (5):

α

ǫ2

X r

α>0

2 R2

vuutl 12Hφ W 2

bRn (Fφ,2(r)) ≤ inf

4α + 10Z √Br
α>0 4α + 20√3W2RXr rHφ log2(3mn)
Now choosing α = C√r where C = 5√3W2RXq Hφ log2(3mn)
gives us the upper bound
bRn (Fφ,2(r)) ≤ 4√rC 1 + log
C ! ≤ 4√rC log
3√B
√B
C

dǫ
m log2(2mn + 1)
dǫ! .
Z √Br

≤ inf

1
ǫ

n

n

α

n

.

(12)

Generalization error bounds for learning to rank

M. Proof of Theorem 17
Proof. We appeal to Theorem 6.1 of Bousquet (2002) that assumes there exists an upper bound

where ψn : [0,∞) → R+ is a non-negative, non-decreasing, non-zero function such that ψn(r)/√r is non-increasing.
The upper bound in Corollary 16 above satisﬁes these conditions and therefore we set ψn(r) = 4√rC log 3√B
C with C as
deﬁned in Corollary 16. From Bousquet’s result, we know that, with probability at least 1 − δ,

bRn (F2,φ(r)) ≤ ψn(r)

∀w ∈ F2, Lφ(w) ≤ ˆLφ(w) + 45r⋆

nLφ(w)

n +q8r⋆
+q4r0Lφ(w) + 20r0

where r0 = B(log(1/δ) + log log n)/n and r⋆
. This proves the ﬁrst inequality.

(cid:16)4C log 3√B
C (cid:17)2
Now, using the above inequality with w = ˆw, the empirical risk minimizer and noting that ˆLφ( ˆw) ≤ ˆLφ(w⋆), we get

n is the largest solution to the equation r = ψn(r). In our case, r⋆

n =

Lφ( ˆw) ≤ ˆLφ(w⋆) + 45r⋆

n +q8r⋆
+q4r0Lφ( ˆw) + 20r0

nLφ( ˆw)

The second inequality now follows after some elementary calculations detailed below.

M.1. Details of some calculations in the proof of Theorem 17
Using Bernstein’s inequality, we have, with probability at least 1 − δ,

ˆLφ(w⋆) ≤ Lφ(w⋆) +r 4Var[φ(Xw⋆, y)] log(1/δ)

n

≤ Lφ(w⋆) +r 4BLφ(w⋆) log(1/δ)
≤ Lφ(w⋆) +q4r0Lφ(w⋆) + 4r0.

n

4B log(1/δ)

+

n
4B log(1/δ)

+

n

n + 20r0. Putting the two bounds together and using some simple upper bounds, we have, with probability

Set D0 = 45r⋆
at least 1 − 2δ,

which implies that

Lφ( ˆw) ≤qD0 ˆLφ(w⋆) + D0,
ˆLφ(w⋆) ≤qD0Lφ(w⋆) + D0.

Using √ab ≤ (a + b)/2 to simplify the ﬁrst term on the right gives us

Lφ( ˆw) ≤pD0rqD0Lφ(w⋆) + D0 + D0.
+ D0 = pD0Lφ(w⋆)
+pD0Lφ(w⋆) + D0

2

2

+ 2D0 .

Lφ( ˆw) ≤

D0
2

