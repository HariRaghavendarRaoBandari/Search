6
1
0
2

 
r
a

M
3

 

 
 
]

O
R
.
s
c
[
 
 

1
v
2
8
0
1
0

.

3
0
6
1
:
v
i
X
r
a

Towards the Speciﬁcation of Adaptive

Robotic Systems

Jeremy Morse, Dejanira Araiza-Illan, Jonathan Lawry and Kerstin Eder ∗†‡

March 4, 2016

Abstract

The widespread adoption of autonomous adaptive systems depends
on provided guarantees of safety and functional correctness, at both de-
sign time and runtime. Specifying adaptive systems is cognitively diﬃcult
when their aspects are in a large number and have complicated dependen-
cies.

We present a technique to construct and automatically explore a spec-
iﬁcation for systems that can degrade and/or adapt, towards analysis at
design time for veriﬁcation and validation. This technique combines and
constructs sections of a lattice (or Hasse diagram) of all the possible or-
dered system degradations/adaptations of interest, limited by desirability
or risk thresholds. The lattice allows the designer to understand the dif-
ferent levels and combinations of system degradations/adaptations. We
use the lattices (or sections) to systematically explore whether a system
is able to fulﬁl its task goals under a dynamic and uncertain environment,
through probabilistic model checking.

We illustrate the proposed speciﬁcation technique through a domestic
robotic assistant example. Systematically exploring the lattice allowed
comparing the probabilities of task success/failure, to determine which
degradation/adaptation combinations can be allowed in the ﬁnal system
implementation.

1

INTRODUCTION

Fully autonomous systems interacting with complex environments require adap-
tive behaviour, when the system degrades due to usage and malfunctions, when
requirements vary, or when the environment is uncertain and dynamic. Ideally,
∗This work was supported by the EPSRC grant EP/J01205X/1, project “RIVERAS: Ro-
†Jeremy Morse, Dejanira Araiza-Illan and Kerstin Eder are with the Department of
Computer Science, University of Bristol, UK. E-mail: {jeremy.morse, dejanira.araizaillan,
kerstin.eder}@bristol.ac.uk
‡Jonathan Lawry is with the Department of Engineering Mathematics, University of Bris-

bust Integrated Veriﬁcation of Autonomous Systems”.

tol, UK. E-mail J.Lawry@bristol.ac.uk

an autonomous robot meant to operate in the real world for a long period of
time should be able to cope with its own degradation and most of the changes
in its environment. Autonomous robotic systems need enough ﬂexibility to re-
spond to changes in the environment, to achieve its ultimate goal as best as
possible [1].

Traditional speciﬁcation conventions (e.g., formal such as temporal logics [2],
timed input/output automata [3] or the Z language [4], or more conventional
such as ﬂow charts or requirement documents [5, 6]) target non-adaptive sys-
tems or systems where all the adaptations can be clearly identiﬁed, through
exhaustively enumerating all functional possibilities and environmental scenar-
ios in models. These models can be subjected to proof or test to determine if
the system designs are ﬁt for the tasks and goals they need to achieve. Such
techniques are not suitable in dynamic environments, however, as all possible
environmental changes must be anticipated a priori and their eﬀects on the
requirements analysed. This becomes cognitively diﬃcult when aspects of the
model have complicated dependencies:
for example an obstacle may require
a robot to reduce speed to maintain a safety margin, aﬀecting it’s ability to
perform it’s function timely. Furthermore, traditional speciﬁcation frameworks
cannot express ﬂexibility in the requirements themselves. It may be desirable,
for example, to trade a slight relaxation in one safety requirement for a dramatic
improvement in others.

We wish to ask quantitative questions such as “what is the maximum amount
of degradation that a system can accept and still successfully complete a task
within a reasonable threshold of resources”, identifying the weakest possible
operational point of the robot while still meeting requirements. Being able
to automatically analyse and predict a priori how a degrading and/or adaptive
system would cope when its circumstances are altered would open opportunities
for the development of new veriﬁcation and validation techniques at design
time. We ask whether it is possible to build speciﬁcations that express such
degradation or adaptation, that can be used as the basis for automatic design
space exploration.

For example, suppose a domestic robotic assistant such as the Care-O-Bot1
has capabilities that degrade due to constant use, such as battery capacity, or
increased joint resistance from dust accumulation. Additionally, the robot must
adapt to the preferences of the user, who might be elderly or have chronic health
issues [7]. If the robot must frequently remind a person to take vital medicine
doses, but also periodically recharge batteries, a design decision must be made of
the maximum velocity the robot can operate at and the minimum battery level
before recharging. These choices must be balanced against the risk of collision,
of ﬂattening it’s batteries, and of missing a scheduled medicine reminder. The
acceptable risk levels of such occurrences is a matter for validation, and may
prompt exploration of the design space.

Most of the research on adaptive systems has focused on developing runtime
methods. By monitoring the satisfaction of requirements, adaptations are cho-

1http://www.care-o-bot-4.de/

sen and triggered. Without verifying these adaptive systems before deployment,
it is assumed that the triggered adaptations will be suitable in their context,
or that for each non-satisﬁed requirement, there will be suitable adaptation.
A remaining challenge is to be able to verify adaptive systems at design (or
requirements) time, to forecast what the adaptations and degradations will do
(e.g., worst case scenarios). Diﬀerent formal models have been proposed to try
to capture the adaptations of software systems, to be able to verify the code
at design time. Nonetheless, describing what the system can do is still mostly
hand-crafted through exhaustive enumeration, which is prone to errors.

In this paper, we focus on the problem of specifying systems that can de-
grade and/or adapt, that can be explored in an automated way that allows in
principle enumerating all the valid combinations of the system’s possible degra-
dations and/or adaptations. We present a technique to construct and auto-
matically explore a speciﬁcation, from ordered sets of degradations/adaptations
for individual variables, system parameters or behaviours (e.g., changes of con-
straints over speciﬁc parameters or functionalities). This technique combines
and constructs sections of a lattice (or Hasse diagram) of all the possible system
degradations/adaptations of interest, with each point representing desirability
or risk values. The lattice allows the designer to understand the diﬀerent levels
and combinations of system degradations/adaptations and their impact, en-
abling analysis at design time for veriﬁcation and validation. We use lattices
constructed through the proposed technique to systematically explore these or-
dered degradations/adaptations during system design, to discover whether a
system is able to fulﬁl its task goals in combination with models of a dynamic
and uncertain environment. In particular, we illustrate the use of the technique
for exploration of a domestic robotic assistant’s speciﬁcation.

Determining the desirability or risk value of a lattice node is performed by
composing a model of the dynamic environment with the operational point of
the lattice node. We use PRISM, a probabilistic model checker,2 to explore
the environmental model, combining a nondeterministic robot controller with
a probabilistic environment. When presented with an operational point, the
model checker ﬁnds the worst possible outcome for the best possible robot be-
haviour. i.e., assuming a robot that always makes decisions with the maximum
probability of success, what is the worst outcome, while operating at the given
point?

This technique produces a design-space lattice with associated desirability /
risk values. Selecting a particular point on the lattice is then a matter for the
designer, deciding whether the given degradation/adaption point is acceptable
to achieve the associated desirability/risk value.

Systematically exploring the lattice allows comparing the probabilities of
task success/failure, to determine which degradation and/or adaptation com-
binations are suitable or will be allowed in the ﬁnal system implementation.
Our algorithm and the exempliﬁed system design exploration approach can be
extended to other types of systems beyond robotic assistants, and even runtime

2http://www.prismmodelchecker.org/

veriﬁcation, in principle.

The paper proceeds as follows. Section 2 introduces the algorithm to au-
tomatically construct ordered speciﬁcations for degrading or adaptive systems.
Section 3 presents a case study of a domestic robotic assistant, and its veriﬁ-
cation at design time through model checking, based on preferences over the
ordered system speciﬁcation. In Section 4, we veriﬁed some properties, which
provided insights on preferred adaptations and worst case scenarios. We discuss
the presented approach in Section 5. Section 6 presents an overview of related
work on the speciﬁcation and veriﬁcation of adaptive systems. We conclude the
paper in Section 7 and give an outlook on future work.

2 SPECIFICATION OF SYSTEMS THAT DE-

GRADE

2.1 A Lattice to Capture Degrading System Behaviour

We present the computation of a lattice to specify a degrading/adaptive system,
based on a simple starting speciﬁcation. This avoids having to enumerate and
order these sets of possible degradations/adaptations by hand.
Let L be a language of propositional logic with a ﬁnite set of propositional
variables P, and connectives, ∧, ∨ and ¬. Let SL denote the sentences of L. A
speciﬁcation is formed by propositional logic expressions, such as “if x ≥ 0.25,
then y < 0.5”, i.e.

(1)
Deﬁnition Let S be a ﬁnite set of valuations on SL representing the set of
possible states of the world (the adaptive system and its environment), or our
current knowledge about that set.
Deﬁnition Entailment means that for θ, ϕ ∈ SL, θ |=S ϕ (θ entails ϕ) if and
only if ∀v ∈ S, v(θ) = 1 ⇒ v(ϕ) = 1.

ψ = ¬p2 ∨ ¬q3.

Deﬁnition The set of degradations/adaptations of a sentence are deﬁned as
W(pi) = {pj ∈ P : pi}, for pi ∈ P, that for propositional variables W(pi), it can
be totally ordered according to |=S.
Deﬁnition A partial ordering θ (cid:22) ϕ exists if and only if θ ∈ W(ϕ).

Deﬁnition Let Ψ be a speciﬁcation as in (1), then the degraded/adapted spec-
iﬁcation is the poset (W(ψ),(cid:22)).

We build a lattice or Hasse diagram for the poset, for exploration purposes.
The general goal of the construction and exploration is to identify the maximal
elements of (W(ψ) ∩ {θ :|=S θ},(cid:22)), which can be interpreted as ﬁnding “the
best adaptation under degradation”, or “the worst degradation allowed”. We
could employ other criteria to choose between competing maximal elements (i.e.,

¬p2 ∨ ¬q3
¬p3 ∨ ¬q3
¬p4 ∨ ¬q3
¬p5 ∨ ¬q3

¬p2 ∨ ¬q4
¬p3 ∨ ¬q4
¬p4 ∨ ¬q4
¬p5 ∨ ¬q4

¬p2 ∨ ¬q5
¬p3 ∨ ¬q5
¬p4 ∨ ¬q5
¬p5 ∨ ¬q5

Figure 1: Example of a Hasse diagram or lattice from the degradation of a
speciﬁcation ψ = ¬p2 ∨ ¬q3

those at the same lattice level), such as a cost function associated to risk, or the
shortest sequences of degradations/adaptations in the lattice.

To exemplify the lattice construction, consider a system with two variables
x and y, both taking values in [0, 1]. We then deﬁne the following proposi-
tional variables: p1 = x ≥ 0, p2 = x ≥ 0.25, p3 = x ≥ 0.5, p4 = x ≥ 0.75,
p5 = x = 1, and q1 = y ≥ 0, q2 = y ≥ 0.25, q3 = y ≥ 0.5, q4 = y ≥ 0.75,
q5 = y = 1. This would be equivalent to enumerating all the variables,
and their individual degradations/adaptations. The degradations/adaptations
for each proposition are: W(p1) = p1, W(p2) = p1, p2, W(p3) = p1, p2, p3,
W(p4) = p1, p2, p3, p4, W(p5) = p1, p2, p3, p4, p5, and W(q1) = q1, W(q2) =
q1, q2, W(q3) = q1, q2, q3, W(q4) = q1, q2, q3, q4, W(q5) = q1, q2, q3, q4, q5. Also,
W(¬p2) = ¬p2,¬p3,¬p4,¬p5 and W(¬q3) = ¬q3,¬q4,¬q5. Thus, W(¬p2 ∨
¬q3) = {¬p2 ∨ ¬q3,¬p2 ∨ ¬q4,¬p2 ∨ ¬q5,¬p3 ∨ ¬q3,¬p3 ∨ ¬q4,¬p3 ∨ ¬q5,¬p4 ∨
¬q3,¬p4 ∨ ¬q4,¬p4 ∨ ¬q5,¬p5 ∨ ¬q3,¬p5 ∨ ¬q4,¬p5 ∨ ¬q5}, for the speciﬁcation
in (1). Instead of enumerating all these possible combinations, which becomes
intractable for larger problems, a lattice is computed, . as shown in Fig. 1.
Acceptable (closest to the upper) degradations/adaptations to explore, would
be, e.g., ¬p3 ∨ ¬q3 and ¬p2 ∨ ¬q4.

The pseudo-algorithm for the Hasse diagram (or lattice) computation is

combinations. Consider a clause C = (cid:87)k
Then, if C(cid:48) ∈ W(C), C(cid:48) =(cid:87) li,ji, where ji ∈ {1, . . . , ki} for i = 1, . . . , k. Let the
Let the degradation/adaptation level of C(cid:48) be DL(C(cid:48)) = (cid:80)k

shown in Fig. 2. We compute the lattice breadth ﬁrst, one degradation/adaptation
level at a time, by enumerating all possible successive degradation/adaptation
i=1 li where li (cid:54)= lj and li (cid:54)= ¬lj, for
i (cid:54)= j. k is the number of degradations/adaptations per parameter, variable or
behaviour. Let W(li) = {li,1, . . . , li,ki}, where li,ki = li and li,s (cid:22) li,r, s ≤ r.
degradation/adaptation degree of the literal li in clause C(cid:48) be D(i, C(cid:48)) = kiji.
i=1 D(i, C(cid:48)).
In
the Hasse diagram there will be a link between C(cid:48)(cid:48) and C(cid:48)
if and only if
DL(C(cid:48)(cid:48)) = DL(C(cid:48)) + 1 and max{D(i, C(cid:48)(cid:48))D(i, C(cid:48)) : i} = 1.

Avoiding the whole lattice exploration (i.e. not computing the whole lattice
from the start) obeys the need to keep degradations/adaptations in realistic and
desirable levels (e.g., avoiding instances where the safest adaptation is not doing

1: Speciﬁcation ψ, ordered propositions e.g.
2: for k = max{m, n} do

tion/adaptation degrees, respectively

pm, qn, with m, n degrada-

3:

Enumerate lattice nodes (e.g. C(cid:48)) at the same degradation/adaptation
level, e.g., pm and qn s.t. DL = m + n
Build links between nodes at diﬀerent levels, e.g. C(cid:48)(cid:48) and C(cid:48)

4:
5: end for

k(cid:88)

Figure 2: Pseudo-algorithm to compute the lattice

anything at all, which is undesirable for a reactive robot). Consequently, con-
straints can be introduced when building the lattice, to limit the computed links
between levels according to the relations between degradations/adaptations and
desirability/reality thresholds.

The complexity of the algorithm to compute the Hasse diagram is dependent
on the size of the diﬀerent degradation/adaptation levels. The number of ele-
ments in level n is bounded above by the quantity of natural number solutions
to the equation

xi = n.

(2)

This upper bound is achieved when |W(li)|1 ≥ n for i = 1, . . . , k.

i=1

2.2 Model

Determining the desirability / risk value for a particular lattice node requires
evaluating a model of the robot in an environment, while operating at the given
operational point. To achieve this we construct models for use in the PRISM
probabilistic model checker, which supports veriﬁcation of properties given in
various temporal logics and quantifying the probability of a particular property
holding. The model is formed of three parts: the environment, the robot, and
a property generated for each lattice node.

The environmental model is created as a discrete time Markov chain written
in the PRISM modelling language, with variables for all relevant parts of the
environment. Non-robot actors are then speciﬁed using probabilistic behaviour
to represent uncertainty about their actions. For example, a human in a 2D
space may be speciﬁed as moving stochastically, with no particular purpose.
All dynamic or uncertain behaviours must be encoded in the environmental
model, to allow opportunity for the model checker to explore diﬀerent outcomes
based on those behaviours.

The robot controller is also written in the PRISM modelling language, but
is not deﬁned to have a particular implementation. Rather than specifying how
the robot works in all circumstances, we instead specify the set of actions that
it may chose to take, as nondeterministic choices. The model checker is then
able to pick which action the robot is to take, in any particular circumstance.

In essence, we give the model checker an abstract robot controller, and it then
synthesises a controller appropriate to the environmental model. The combined
environmental and robot models form a Markov decision process, or MDP.

Finally, we produce a veriﬁcation property for each lattice node, represent-
ing an operational point. This is written as a PRISM “ﬁlter” statement. We
pre-suppose a formula for the success of the robot activity using the “Pmax”
operator of PCTL, a probablistic temporal logic. We also pre-suppose a set of
start states, representing the range of normal operational states of the robot,
and use the “min” ﬁlter operator to select the state with the lowest probabil-
ity of the formula succeeding. The constraints represented by the operational
point are added to the start state preconditions and to the property formula
as a global invariant, limiting the set of choices available to the robot while
completing the activity.

Combining the environmental and robot models with the veriﬁcation prop-
erty causes PRISM to search for the set of choices for the robot behaviour that
maximises the chance of the formula being true. The “min” operator then picks
the lowest of these probabilities. The result is then, for the given conﬁguration,
the worst-case probability of success of the robot performing at that operational
point.

3 CASE STUDY: MODEL CHECKING TO VER-

IFY PROPERTIES OF A DOMESTIC ROBOTIC
ASSISTANT UNDER A SPECIFIC DEGRA-
DATION

We considered a domestic robotic assistant in an open-plan, conﬁned ﬂoor of a
house, represented by a grid. The robot is allowed to move in four directions,
north, south, east or west in the grid, with limited maximum velocity, or it can
choose to stay in the same cell. A human cohabits this space, also allowed to
move stochastically in the grid. A representation of this setup is shown in Fig. 3.
The robot has a battery, and it will seek recharge at a station ﬁxed in the grid,
when reaching a minimal energy threshold. The battery energy diminishes each
time the robot moves (one unit per motion cycle). The robot needs to coordinate
recharging, whilst servicing the person within a time threshold. Additionally,
the robot avoids colliding with the person for safety.

For the case study, we would like to determine how quickly the robot can
service the human with a particular maximum velocity, and what margin of
energy reserves are required to complete this task. Firstly, we computed a
lattice with the system parameters that can be adapted or degraded over its
operation and lifetime: the maximum velocity for the robot, pi = v ≤ i with
i = 1, . . . , 6, and the maximum allowed time to service the human, qj = j ≤ t
with j = 1, . . . , 10.

ψ = pi ∧ qi

(3)

Figure 3: A robotic assistant and a person, in an open-plan, conﬁned ﬂoor as a
grid

In an perfect world, the robot would be able to service the human as fast as
possible, and moving at a velocity that is not dangerous, at v ≤ 1 ∧ t ≤ 1 units.
Nevertheless, if this is not possible, we could settle for a suboptimal option, e.g.
v ≤ 3 ∧ t ≤ 5 units. We ask what are the best suboptimal options, and what is
the worst case?

To answer these questions, an enumeration of options is necessary, followed
by analysis mechanisms, e.g., computing the probability of achieving the human
servicing task given degraded/adapted thresholds. The lattice of the speciﬁca-
tion (3) was expanded completely, since we only used a small set of variables in
the case study. Fig. 4 shows only a segment of the resulting lattice, from the ideal
speciﬁcation to degradations/adaptations at level 5. A full expansion might not
be convenient for larger problems and state spaces of degradations/adaptations,
and sections of it can be computed instead. Secondly, from each node in the
lattice to be explored– i.e. a particular set of valuations (instantiations) for the
degraded/adapted parameters–, we model checked a parametrized model of the
robot-human system in PRISM.

In classical model checking, a ﬁnite-state model of a system is explored ex-
haustively, to determine if a temporal logic property is satisﬁed or not. PRISM
targets systems that exhibit non-deterministic behaviour, modelled as discrete-
time Markov chains, continuous-time Markov chains, or Markov decision pro-
cesses (MDPs), among others. Probabilities of property satisfaction are com-

p1 ∧ q1
p2 ∧ q1
p3 ∧ q1
p4 ∧ q1
p5 ∧ q1
p6 ∧ q1

p1 ∧ q2
p2 ∧ q2
p3 ∧ q2
p4 ∧ q2
p5 ∧ q2

p1 ∧ q3
p2 ∧ q3
p3 ∧ q3
p4 ∧ q3

p1 ∧ q4
p2 ∧ q4
p3 ∧ q4

p1 ∧ q5
p2 ∧ q5

p1 ∧ q6

Figure 4: Segment of lattice of degradations/adaptations for the domestic
robotic assistant

puted by the tool. The main drawback of model checking is the cost in com-
putational time and memory, due to the state space explosion problem. This
problem is due to the exponential growth in number of states to traverse, when
adding variables [8]. So far, the computational demands of model checking have
made it unsuitable for runtime veriﬁcation [9].

We constructed a parametrized model of the domestic robot setting de-
scribed before (with a person as the stochastic environment), in the form of
Markov decision processes. This model contains 5 modules, corresponding to
the human and robot motion, the timing, the energy in the battery, and the
servicing task. The parametrization prunes the state space of the model to the
particular degradation/adaptation, which otherwise would comprise all possible
limits for the parameters. Also, specifying initial conditions, such as the initial
battery charge, reduces the state space. The model checker computes a mini-
mum probability to complete the servicing of the human, subjected to particular
values of initial battery energy, maximum velocity for the robot, and maximum
allowed time to service the human. When the robot does not need to service
the person, the model checker chooses a velocity and direction for its motion,
from the available options, including the speciﬁed velocity upper limit. Also, the
robot can reach the battery recharge station either moving from north to south,
or from east to west, choosing randomly. The human moves stochastically at
all times in the grid.

The temporal logic property speciﬁcation language for MDPs is probabilistic
computation tree logic (PCTL). PRISM allows qualitative questions about the
probability (P) of properties (e.g. the probability of event X > 0.9, true or
false?), and also quantitative questions, i.e. computing actual probabilities (e.g.,
what is the maximum probability of event X, given some initial conditions?).
Allowed PCTL operators (to describe events happening along execution paths
in the model) include: X (next, or within the next time step), U (until, or
an event is true until another event is true), F (eventually, or sometime in
the future), G (always, or along all execution paths), W (weak until, which,
besides the U operation, allows the ﬁrst event to always be true if the second
is never true) and R (release, where an event is true until another true, or the
ﬁrst event is true forever). Bounded time can be added to the operators, e.g.

a U<=t
states before [10].

b is satisﬁed if b becomes true within t steps, and a is true in all the

The degradations are parametrized into the model for model checking, through

the “ﬁlter” construct in PRISM attached to the properties to verify. The ﬁlters
choose, from all the possible states, the ones where the speciﬁed parametriza-
tion is true. This helps reducing the state space explosion, pruning the explo-
ration and saving time and computational resources. Additional ﬁlters have
been added to eliminate other invalid or unrealistic model behaviours, such as
when the system does not perform any action at all over time, which lead to
false conclusions. This is equivalent to constrain the system to avoid the cases
where a controller chooses to not do anything ever, to comply with “always
being safe” or “never doing something wrong”.

An example of a property to verify, or a query about the model under a
particular degradation/adaptation is: “computing the minimum probability of
completing a servicing of the human in 5 robot motion steps, considering a
speciﬁc instance of servicing time threshold bound, the maximum velocity for
the robot, the minimum energy threshold, and the starting energy”.

f ilter(min, P max =?[(serviceHuman) ∧ (velocity ≤ 4)
U≤20(¬serviceHuman) ∧ (velocity ≤ 4)],
((serviceHuman ∧ serviceT imer = 0) ∨ ¬serviceHuman)
∧(velocity ≤ 4) ∧ (minenergy = 2) ∧ (energy = 25)
∧(tick = 0) ∧ ((robotX (cid:54)= humanX) ∨ (robotY (cid:54)= humanY )))

(4)

4 EXPERIMENTS AND RESULTS

The lattice was computed through a Python script. We ran the model checking
experiments on a PC with Intel i5-3230M 2.60 GHz CPU, 8 GB of RAM, Ubuntu
14.04, and PRISM 4.2.beta1. Model checking took less than 1 minute for each
experiment, with a minimal time of 10 seconds.

The graphs in Fig. 5 show the probability of servicing the human, for dif-
ferent degradations/adaptations comprising maximum velocity and maximum
servicing time. The grid size is 7 × 7 cells. The minimum threshold for bat-
tery recharging (min = 2 units), the initial locations of the human and the
robot in the grid (at (5, 5) and (4, 4), respectively), and the location of the
recharge station (at (0, 0) or top left corner), were left at speciﬁed ﬁxed val-
ues. The initial battery energy charge was varied for each graph, from the set
{25, 20, 15, 10, 5, 4, 3, 2, 1} units, respectively.

The graphs show that the probabilities of satisfying (4) under the diﬀerent
degradations/adaptations for the maximum velocity and the maximum allowed
servicing time are reduced considerably when the initial battery charge drops
below 5 units. Also, we can observe that the robotic assistant will be unable to
satisfy the property for the ideal condition where the velocity is minimal (i.e.
v ≤ 1) and the servicing time is minimal too (i.e. t ≤ 1). Nonetheless, we could
settle for degradation/adaptation thresholds of v ≤ 4 and t ≤ 4 (lattice node

Figure 5: Probabilities when degrading the maximum velocity limit for the
robot, and the maximum servicing time cycles, for diﬀerent initial battery energy
charges

p4 ∧ q4), v ≤ 3 and t ≤ 5 (lattice node p3 ∧ q5), or v ≤ 2 and t ≤ 6 (lattice
node p2 ∧ q6), all at level 6 of degradation/adaptation, if the battery is initially
charged at above 5 units. For safety reasons, as the velocity is less, we would
prefer p2 ∧ q6 over the other two options. Another critical threshold would be
to start with a battery charged at above 4 units, since degrading/adapting to
p4 ∧ q4 or p3 ∧ q5 provides an acceptable probability of success.

5 DISCUSSION

Our speciﬁcation technique has the beneﬁts provided by automation, for the
computation of the lattice: avoids the enumeration of all possible degrada-
tions/adaptations by hand, reduces the presence of errors due to manual input,
and it is applicable to many case studies.

Computing a lattice of speciﬁcations presents information about the adap-
tive system possibilities to the designer. Furthermore, exploring the lattice ex-
poses tradeoﬀs between choices in degradations/adaptations. An ordered lattice
provides expressiveness to specify systems that degrade or adapt in uncertain
environments, and meaning to exploration results (e.g., thresholds of what is
acceptable).

Our approach can be extended through providing new functions to order the
elements in the lattice, to express preferences. Also, although the lattice can be
computed totally, providing enough computational resources and motivation,
segments of it can be computed and explored instead according to relevance or
preference.

24681024600.51Servicing cycles(a) Initial energy charge = 25Max. velocitySuccess probability24681024600.51Servicing cycles(b) Initial energy charge = 20Max. velocitySuccess probability24681024600.51Servicing cycles(c) Initial energy charge = 15Max. velocitySuccess probability24681024600.51Servicing cycles(d) Initial energy charge = 10Max. velocitySuccess probability24681024600.51Servicing cycles(e) Initial energy charge = 5Max. velocitySuccess probability24681024600.51Servicing cycles(f) Initial energy charge = 4Max. velocitySuccess probability24681024600.51Servicing cycles(g) Initial energy charge = 3Max. velocitySuccess probability24681024600.51Servicing cycles(h) Initial energy charge = 2Max. velocitySuccess probability24681024600.51(i) Initial energy charge = 1Success probabilityServicing cyclesMax. velocityAnother advantage of our technique is the incorporation of formal analysis
methods during the exploration phase. Formal methods can provide proof of
requirement satisfaction or violation, which will guarantee the safety and func-
tional correctness of a model of the system and its environment, under degra-
dation/adaptation. The adoption of non-deterministic parametrized models en-
tails that choices over the system’s and environmental actions remain speciﬁed
as open as possible. The model checker decides what are the best actions that
the system can perform, according to the non-deterministic environment, under
a particular degradation/adaptation.

At the same time, the use of model checking brings the state-space explo-
sion problem, which impedes the exploration of models with a large number of
variables or possible states. Nonetheless, solving the state-space explosion prob-
lem is a major contemporary research topic in the formal methods community.
Furthermore, we would like to examine the substitution of model checking by
approximation and numerical methods based approaches.

6 RELATED WORK

Research on requirements engineering seeks to formulate requirements that cor-
rectly reﬂect a system’s adaptation. These approaches could be extended to deal
with adaptive system speciﬁcation. Fuzzy logic, probabilistic, and risk analysis
models can be used to formulate the system’s requirements when uncertainty is
present due to adaptation [11, 12, 13]. Qualitative requirements, or soft-goals,
which mean to be abstract to be applicable to a system that changes, can also
be modelled through fuzzy logic [14].

In contrast, [15] argues that the requirements should be formulated as “hard”
but contextualized (e.g., tolerances for the requirements according to the envi-
ronment), instead of fuzzy. This approach would allow veriﬁcation of adaptive
systems through existing formal methods. Nonetheless, all the tolerances and
contexts would have to be enumerated and explored at some point, for which
no proposal is provided.

Other research on requirements engineering has focused on extracting the
requirements that are satisﬁed by a system. Constraints can be removed sys-
tematically, to provide new requirements that can be satisﬁed by a system, as
it is done for linear temporal logic properties in [16].

Transitions between conﬁgurations of adaptive software have been repre-
sented through Markov chains, Petri Nets, dynamic decision networks, UML
diagrams, labelled transition systems, communicating sequential processes and
set theory [17, 18, 13]. Nonetheless, these models require enumeration and
construction of all possible conﬁguration adaptations, particularly for formal
analysis at design stage.

Domain speciﬁc languages have been developed to specify adaptive software.
In [19], the variability of the software is modelled by specifying, in ﬁrst-order
logic, variation points, which alternatives are available on each point, and con-
straints to indicate which variants (conﬁgurations) are valid. Properties to be

optimized during adaptation are also deﬁned, with priority rules to trigger par-
ticular property optimization according to contexts. Our approach does not
require any domain speciﬁc language, and its scope is more general than soft-
ware. Also, system conﬁgurations are ranked according to property satisfaction
in [19], whereas we order the diﬀerent degradations or adaptations according
to their desirability levels. Additionally, our approach allows to add other cost
functions over the adaptations/degradations such as risk.

Adaptive systems have been veriﬁed at design time or at runtime [18, 13].
At design time, a ﬁxed set of operators for adaptation [20] have been veriﬁed
through model checking. Model checking has also been employed to verify soft-
ware adaptive systems modelled as labelled transition systems, communicating
sequential processes, set theory or probabilistic timed automata [18]. Models
and implementations of adaptive systems have been tested in simulation [18, 21],
including stochastic ones [22].

At runtime, adaptive systems are monitored to see if they satisfy require-
ments, or to trigger new adaptations otherwise. Monitors for high-level or ab-
stract requirements, which are “generalized” to be applicable to any adaptation
by the system, have been proposed in [23]. Also, monitors have been used to
estimate probabilities of property satisfaction, via updating a Hidden Markov
Model with new observed information in [24]. In [25], monitors based on pro-
cess algebras are proposed, for requirements that need expressing concurrency.
In [26], a model of a distributed software adaptive system and its environment is
constructed in Z, which is modiﬁed according to feedback from monitoring the
veriﬁcation of requirements at runtime (reﬂecting about what has happened,
and changing accordingly). Consequently, this will trigger adaptations to the
real distributed system.

Formal methods at runtime, particularly model checking, have been em-
If-then-else rules that

ployed to trigger adaptation to satisfy a requirement.
change have been veriﬁed [9] in the model checker SPIN 3. Probabilistic parametrized
models for controllers that change have been veriﬁed in PRISM [9]. Both of these
model checking at runtime approaches are computationally expensive, thus not
feasible for realistic systems.

Another application of formal methods is synthesis of controllers or strate-
gies, or reﬁnement. For example, the best strategies or models of a system that
satisfy a property are computed in [27]. Strategies that violate a property the
least are computed in [28], from weighted automata and reward assignment.
Nonetheless, synthesis and reﬁnement processes at runtime are also computa-
tionally expensive [9]. These synthesis processes do not necessarily entail adap-
tation (as observed in [29]), as the systems to control normally are not allowed
to modify their operational spaces, nor is the environment allowed to change
dynamically, or non-deterministically.

By analysing a system at design time, it is possible to predict or give some
guidance with respect to what are the best or worst case scenarios. Furthermore,
we can determine what might happen when a system encounters rare, extreme,

3http://spinroot.com/spin/whatispin.html

and interesting circumstances. Most of the existing approaches to deal with
uncertainty and change in the environment are designed for runtime [19]; i.e.,
these systems will wait until something happens to adapt in response, and it
might be too late to prevent undesired adaptations or degradations.

In principle, our approach could be used at runtime, although today this
is limited by the use of formal methods (in particular model checking), which
are computationally expensive. In the future this limitation may be overcome
by dedicated and eﬃcient computational approaches instead of using model
checkers.

7 CONCLUSIONS

We presented a technique to construct and automatically explore a speciﬁca-
tion for systems that can degrade and/or adapt. This technique combines and
constructs sections of a lattice (or Hasse diagram) of all the possible system
degradations/adaptations of interest, as needed, limited by desirability or risk
thresholds. The system degradations/adaptations can refer to individual vari-
ables, system parameters, or behaviours (e.g., changes of constraints over speciﬁc
parameters). The lattice allows the designer to understand the diﬀerent levels
and combinations of system degradations/adaptations, following their speciﬁed
order/preference, towards applying these to analysis at design time for veriﬁca-
tion and validation.

In this paper, we used the lattices (or lattice sections) to systematically
explore ordered degradations/adaptations during system design (i.e., before de-
ployment or runtime), to ﬁnd out if a system is able to fulﬁl its task goals in
combination with models of a dynamic and uncertain environment. We illus-
trated this exploration through a domestic robotic assistant case study. For the
case study, we implemented a parametrized non-deterministic model of a robotic
assistant and its home environment (including people), based on Markov deci-
sion processes (MDP) in PRISM. This model encodes all possible robot degra-
dations and adaptations (i.e., all the possible “controllers” and “worlds”), such
as losing battery life, or changing its maximum speed limits. Providing a degree
of degradation/adaptation of the system from the lattice (i.e., a lattice node),
parametrization takes place in the model. The (minimum) probability of the
degraded/adapted system to satisfy a task goal (expressed as a temporal logic
property), given the dynamic and uncertain environment, and speciﬁed initial
conditions, is computed automatically by the model checker. Systematically
exploring the lattice allowed comparing the probabilities of task success/failure,
to determine which degradation and/or adaptation combinations were suitable
or allowed in the ﬁnal system implementation.

Whereas for small case studies, the whole lattice can be computed and ex-
plored and even visualized, partial segments of interest of a full lattice can be
used instead for the analysis of real-life complex systems. In the near future, we
will be applying this premise to a real autonomous system, for exploration and
veriﬁcation at design time. Additionally, we will research the application of our

technique to runtime veriﬁcation of adaptive systems.

References

[1] G. Sussman, “Building robust systems an essay,” in MIT, 2007.

[2] P. Bellini, R. Mattolini, and P. Nesi, “Temporal llogic for real-time system

speciﬁcation,” ACM Computing Surveys, vol. 32, no. 1, pp. 12–42, 2000.

[3] A. David, K. Larsen, A. Legay, U. Nyman, and A. Wasowski, “Methodolo-
gies for speciﬁcation of real-time systems using timed I/O automata,” in
Proc. FMCO, 2010, pp. 290–310.

[4] J. Spivey, “An introduction to Z and formal speciﬁcations,” Software En-

gineering Journal, vol. 4, no. 1, pp. 40–50, 1989.

[5] D. Hatley and I. Pirbhai, Strategies For Real-Time System Speciﬁcation.

Dorset House, 1988.

[6] J. Bowen and M. Hinchey, High-integrity system speciﬁcation and design.

Springer-Verlag, 1999.

[7] K. Kucher and D. Weyns, “A self-adaptive software system to support

elderly care,” in Proc. MIT, 2013.

[8] E. Clarke, W. Klieber, M. Nov´a˘cek, and P. Zuliani, “Model checking and

the state explosion problem,” in Proc. LASER, 2011, pp. 1–30.

[9] R. Calinescu and S. Kikuchi, “Formal methods at runtime,” in Proc. Mon-

terey Workshops, 2010, pp. 122–135.

[10] M. Kwiatkowska, G. Norman, and D. Parker, “PRISM 4.0: Veriﬁcation of
probabilistic real-time systems,” in Proc. 23rd International Conference on
Computer Aided Veriﬁcation (CAV’11), ser. LNCS, vol. 6806.
Springer,
2011.

[11] P. Sawyer, N. Bencomo, J. Whittle, E. Leiter, and A. Finkelstein,

“Requirements-aware systems,” in Proc. RE, 2010, pp. 95–103.

[12] J. Whittle, P. Sawyer, N. Bencomo, B. Cheng, and J. Bruel, “RELAX:
a language to address uncertainty in self-adaptive systems requirement,”
Requirements Engineering, vol. 15, no. 2, pp. 177–196, 2010.

[13] Z. Yang, Z. Li, Z. Jin, and Y. Chen, “A systematic literature review of
requirements modeling and analysis for self-adaptive systems,” in Proc.
REFSQ, 2014, pp. 55–71.

[14] M. Serrano, M. Serrano, and J. Sampaio do Prado Leite, “Dealing with

softgoals at runtime: A fuzzy logic approach,” in REatRunTime, 2011.

[15] A. Chopra, “Requirements-driven adaptation: Compliance, context, uncer-

tainty, and systems,” in Proc. REatRunTime, 2011, pp. 32–36.

[16] K. Kim, G. Fainekos, and S. Sankaranarayanan, “On the revision problem

of speciﬁcation automata,” in Proc. ICRA, 2012, pp. 5171–5176.

[17] R. Bruni, A. Corradini, F. Gadducci, A. Lluch Lafuente, and A. Vandin, “A
conceptual framework for adaptation,” in Proc. FASE, 2012, pp. 240–254.

[18] D. Weyns, “Towards an integrated approach for validating qualities of self-

adaptive systems,” in Proc. WODA, 2012, pp. 24–29.

[19] F. Fleurey and A. Solberg, “A domain speciﬁc modeling language support-
ing speciﬁcation, simulation and execution of dynamic adaptive systems,”
in Proc. MODELS, 2009, pp. 606–621.

[20] D. Gordon, “Asimovian adaptive agents,” Journal of Artiﬁcial Intelligence

Research, vol. 13, pp. 95–153, 2000.

[21] J. C´amara, R. de Lemos, N. Laranjeiro, R. Ventura, and M. Vieira, “Ro-
bustness evaluation of the Rainbow framework for self-adaptation,” in Proc.
SAC, 2014, pp. 376–383.

[22] C. Rouﬀ, R. Buskens, L. Pullum, X. Cui, and M. Hinchey, “The AdaptiV
approach to veriﬁcation of adaptive systems,” in Proc. C3S2E, 2012, pp.
118–122.

[23] B. Cheng, R. de Lemos, H. Giese, P. Inverardi, and J. Magee, “Software
engineering for self-adaptive systems: A research roadmap,” Software En-
gineering for Self-Adaptive Systems, pp. 1–26, 2009.

[24] E. Bartocci, R. Grosu, , A. Karmarkar, S. Smolka, S. Stoller, E. Zadok,
and J. Seyster, “Adaptive runtime veriﬁcation,” in Proc. RV, 2012, pp.
168–182.

[25] B. Abolhasanzadeh and S. Jailili, “Towards modeling and runtime veriﬁca-
tion of self-organizing systems,” Expert Systems with Applications, vol. 44,
pp. 230–244, 2016.

[26] D. Weyns, S. Malek, and J. Andersson, “FORMS: Unifying reference model
for formal speciﬁcation of distributed self-adaptive systems,” ACM Trans-
actions on Autonomous and Adaptive Systems, vol. 7, no. 1, 2012.

[27] W. Damm and B. Finkbeiner, “Does it pay to extend the perimeter of a

world model?” in Proc. FM, 2011, pp. 12–26.

[28] J. Tumova, L. Castro, S. Karaman, E. Frazzoli, and D. Rus, “Minimum-
violation LTL planning with conﬂicting speciﬁcations,” in Proc. ACC, 2013,
pp. 200–205.

[29] H. Kress-Gazit, G. Fainekos, and G. Pappas, “Temporal-logic-based reac-
tive mission and motion planning,” IEEE Transactions on Robotics, vol. 25,
no. 6, pp. 1370–1381, 2009.

