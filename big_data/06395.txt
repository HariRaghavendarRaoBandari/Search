Publications of the Astronomical Society of Australia (PASA)
c(cid:13) Astronomical Society of Australia 2016; published by Cambridge University Press.
doi: 10.1017/pas.2016.xxx.

Comparison of three Statistical Classiﬁcation Techniques
for Maser Identiﬁcation

6
1
0
2

 
r
a

 

M
1
2

 
 
]

M

I
.

h
p
-
o
r
t
s
a
[
 
 

1
v
5
9
3
6
0

.

3
0
6
1
:
v
i
X
r
a

Ellen M. Manning,1 Barbara R. Holland,1 Simon P. Ellingsen,1 Shari L. Breen,2 Xi Chen,3,4 Melissa Humphries1
1School of Physical Sciences, University of Tasmania, Private Bag 37, Hobart, Tasmania 7001, Australia
2CSIRO Asronomy and Space Science, PO Box 76, Epping, NSW 1710, Australia
3Key Laboratory for Research in Galaxies and Cosmology, Shanghai Astronomical Observatory, Chinese Academy of Sciences,
Shanghai 200030, China
4Key Laboratory of Radio Astronomy, Chinese Academy of Sciences, China

Abstract
We applied three statistical classiﬁcation techniques - linear discriminant analysis (LDA), logistic regres-
sion and random forests - to three astronomical datasets associated with searches for interstellar masers.
We compared the performance of these methods in identifying whether speciﬁc mid-infrared or millimetre
continuum sources are likely to have associated interstellar masers. We also discuss the ease, or otherwise,
with which the results of each classiﬁcation technique can be interpreted. Non-parametric methods have the
potential to make accurate predictions when there are complex relationships between critical parameters.
We found that for the small datasets the parametric methods logistic regression and LDA performed best,
for the largest dataset the non-parametric method of random forests performed with comparable accuracy to
parametric techniques, rather than any signiﬁcant improvement. This suggests that at least for the speciﬁc
examples investigated here accuracy of the predictions obtained is not being limited by the use of paramet-
ric models. We also found that for LDA, transformation of the data to match a normal distribution in the
input parameters led to big improvements in accuracy. The diﬀerent classiﬁcation techniques had signiﬁcant
overlap in their predictions, further astronomical observations will enable the accuracy of these predictions
to be tested.

Keywords: methods: classiﬁcation – masers – stars:formation

1 Introduction

In recent years astronomical instrumentation across a
range of wavelength bands has improved to the point
where high-resolution, sensitive surveys of large areas
of the sky are becoming much more common (e.g. ??).
The higher data rates from new instrumentation and
large surveys give the opportunity to collect detailed
information on very large numbers of sources and un-
dertake more sophisticated statistical investigations of
their properties. This will enable both more reliable
identiﬁcation of sub-groups within the broader popula-
tion, and identiﬁcation of rare or unusual objects. How-
ever, these new instruments also present the astronom-
ical community with a challenge of how best to extract
the maximum utility from large volumes of data.

The desire to accurately and eﬃciently classify astro-
nomical sources identiﬁed in large surveys into diﬀerent
groups is an increasingly common one. Attempts to de-
velop eﬃcient criteria for targeted searches for interstel-
lar masers, is one speciﬁc example of an application of

survey source classiﬁcation. A number of studies have
found that star formation regions with an associated
interstellar maser diﬀer signiﬁcantly in their infrared
or millimetre continuum properties from the majority
of the population (e.g. ???). In developing criteria for
targeting future searches it is desirable to identify a
large fraction of the population of interest while includ-
ing only a small number of sources which do not yield
detections. In the terminology of classiﬁcation it is im-
portant to minimise both the number of false-negatives
and false-positives. A related issue is in understanding
the characteristics through which the classiﬁcation has
been achieved. For example, if you are able to develop
eﬃcient criteria for targeting a search for interstellar
masers on the basis of infrared or millimetre contin-
uum properties, what is the physical meaning of those
characteristics - do they correspond to a particular mass
range, or evolutionary phase of the associated high-mass
star formation region?

Maser emission occurs naturally in a range of as-
trophysical environments, including the molecular gas

1

2

Manning et al.

close to newly forming stars, the envelopes of late-type
stars, and close to the nuclei of some active galaxies.
Masers have proven to be a reliable signpost of the
very early stages of high-mass star formation (e.g. ?);
with recent improvements in the availability of sensitive
large-area surveys at mid-infrared through millimetre
wavelengths, they are increasingly being used as tools
to study high-mass star formation (e.g. ?). Masers can
provide information on the dynamics of the star for-
mation region through observations of their kinematics
(e.g. ?), on the magnetic ﬁeld from observations of the
polarisation (e.g. ?), and potentially the presence and
absence of diﬀerent transitions can provide an evolu-
tionary timeline (e.g. ??). If it is possible to use clas-
siﬁcation techniques to reliably identify which regions
host diﬀerent types of maser transition, then an under-
standing of the physical properties of those regions in
combination with the maser-based evolutionary time-
line could provide important insights into the formation
of high-mass stars.

These types of classiﬁcation problems are commonly
encountered in a wide range of scientiﬁc disciplines,
and from the broader literature we have been able to
identify a number of commonly used classiﬁcation tech-
niques. When considering diﬀerent classiﬁcation meth-
ods, ? has suggested that there is a trade oﬀ between
parametric techniques that are easy to interpret but not
always as accurate, and non-parametric methods that
are more diﬃcult to interpret, but deliver a higher level
of accuracy. Here, we use three diﬀerent classiﬁcation
techniques to investigate their strengths and weaknesses
when applied to the speciﬁc problem of eﬃciently identi-
fying target sources for searches for interstellar masers.
The three methods we have chosen for our investigation
are linear discriminant analysis (LDA), logistic regres-
sion, and random forests. These three methods were
chosen because they have proven eﬀective across a wide
range of problem domains, they are relatively easy to
implement, and they include two parametric methods
(LDA and logistic regression) and one non-parametric
method (random forests).

LDA uses similar calculations and techniques to prin-
cipal component analysis (PCA) which is quite widely
used in astronomy (e.g. ??). ? used LDA in their classi-
ﬁcation of diﬀerent photospheric magnetic elements on
the Sun. They found that the predictions they were able
to make on the basis of LDA showed good agreement
with the results from previous studies. This can in part
be credited to the semi-artiﬁcial segregation between
the classes of photospheric magnetic elements, as the
variables chosen were those with the most signiﬁcant
diﬀerences in brightness values. Logistic regression has
been less commonly applied in astronomy than PCA, al-
though it has previously been used to successfully iden-
tify which star formation regions are more likely to host
diﬀerent types of interstellar masers (e.g. ??). ? and ?

PASA (2016)
doi:10.1017/pas.2016.xxx

have both shown that logistic regression can be an eﬀec-
tive means of predicting solar ﬂares. Random forests are
a relatively new, non-parametric classiﬁcation technique
which has proven to be very eﬀective in other ﬁelds, such
as ecology. ? compared the results of classifying ecolog-
ical data, using the same classiﬁcation methods as are
used here and found that random forests had the high-
est accuracy. Within astronomy, random forests have
been used by ? to improve the reliability of ﬁnding su-
pernovae from images, while ? used them to assign pho-
tometric redshifts. Recently they have also been used as
the basis of processes for automated rapid classiﬁcation
and decision making. ? used random forests as part of
a method for making time-eﬃcient recommendations as
to which gamma-ray burst events are likely to be high-
redshift in order to prioritise whether a speciﬁc event
deserves additional observing time. They found that by
observing the top 20 % of recommended events it was
possible to identify 56 % of the high-redshift bursts,
while using the top 40 % of recommendations allows
identiﬁcation of 84 % of high-redshift events. ? used ran-
dom forests to accurately classify whether unidentiﬁed
objects detected in Gamma-rays by the Fermi satellite
were likely to be Active Galactic Nuclei (AGN) or pul-
sars (they achieved accuracies of 97.7 and 96.5 % for
AGN and pulsar identiﬁcation, respectively).

To better understand the strengths and limitations of
these diﬀerent classiﬁcation techniques, both in terms of
their eﬃciency and the degree to which the outcomes of
the classiﬁcation process can be related to the proper-
ties of the astronomical sources, we compared their per-
formance on three published datasets (???). For each of
these three sets of data we applied the three classiﬁca-
tion techniques to make predictions as to which infrared
(or millimetre) sources are likely to also be associated
with masers. In Section 2 we describe in more detail
each of the classiﬁcation techniques used. The proper-
ties of each of the datasets are outlined in Section 3
where we examine the results of applying the diﬀerent
classiﬁcation techniques in each case.

2 Classiﬁcation Techniques

In the context of the current work our data typically
consists of astronomical sources for which a range of
parameters (e.g. the intensity in a particular wave-
length range) have been measured, along with param-
eters which are related to the quality or uncertainty
in the measurement and others which identify the par-
ticular astronomical object (e.g. the source number or
coordinates). These parameters are all potential inputs
to the diﬀerent classiﬁcation techniques and we refer
to these as predictor variables. In the ﬁeld of machine
learning these are often referred to as features, how-
ever, as that term frequently has a diﬀerent meaning
in astronomical literature we do not use that terminol-

Comparison of classiﬁcation techniques for masers

3

ogy here. For some (sometimes all), of the sources in
the data set we also have information as to whether or
not that source has an associated maser emission from
a speciﬁc molecular transition. Hence, we are seeking
to accurately classify our astronomical sources into two
classes, those with an associated interstellar maser and
those without.

2.1 Linear Discriminant Analysis

Linear discriminant analysis ﬁnds the linear combina-
tion of predictor variables which maximises the separa-
tion of the diﬀerent classes and minimises the variation
within classes (?). LDA can be visualised geometrically
as projection from a high-dimensional space onto a line.
When given a new source to classify, LDA uses this lin-
ear combination to convert the high-dimensional data
to a real number, and the classiﬁcation of the sample
is determined by comparing this number to a thresh-
old value. The technique is relatively simple and so is
unsuitable if there are complex, nonlinear interactions
between the variables. LDA is a technique of dimension-
ality reduction similar to principal component analysis
(PCA), which is more commonly used in astronomy.
Both LDA and PCA attempt to model the data with
linear combinations of the predictor variables; the diﬀer-
ence is that PCA does not use classiﬁcation information
in producing the model, whereas LDA does (?).

The assumptions of LDA are that the data follows a
multivariate normal distribution for each class, classes
may have diﬀerent means but are assumed to have the
same variance structure. This makes LDA a parametric
method in the sense that it assumes a particular model
of the data. Most astronomical data are not normally
distributed, so transformations of the variables are usu-
ally required. For each of the three datasets we studied,
LDA was applied to both the original data and to the
transformed data as a comparison. Data Set 2 required
an inverse function to normalise the data (each predic-
tor variable was transformed via a 1
x function). In the
case of Data Sets 1 and 3 where the samples were nat-
urally clustered, an inverse transformation would have
destroyed the bimodality present. For this reason, a log
transformation was selected as it improved the normal-
ity of the data while still being easy to interpret.

LDA models were ﬁtted using the lda function in R
(?, part of the MASS package), we left the prior input
parameter at its default setting which is to assume that
probability of being in a particular class is equal to the
relative frequency of the class in the training data.

2.2 Logistic Regression

Logistic regression is a form of generalised linear mod-
elling that is used to predict the probability of an event
occurring; in this case, whether or not an astrophysical

PASA (2016)
doi:10.1017/pas.2016.xxx

source has an associated interstellar maser. The proba-
bility of occurrence P is calculated from

P =

1

1 + e−z ,

where z = b0 + b1.x1 + ... + bn.xn, the b values are re-
gression coeﬃcients and the xi values are the predictor
variables. P is then compared to a cut-oﬀ threshold of
0.5 (50% likelihood) to determine whether an object
is predicted to have an associated maser, or not. Like
LDA, logistic regression is also a parametric method.

Linear regression assumes that the response variable
is normally distributed, in contrast logistic regression
assumes that the response variable follows a binomial
distribution (which is applicable in our case of two
classes). This means that the method of least squares
(used in linear regression), cannot be applied to logistic
regression (?). Instead, maximum likelihood ? is used
to estimate the parameters of the model. The likelihood
function is calculated using the product of contributions
to the model from each of the predictor variables (?).

Logistic regression was implemented using the func-
tion glm which is part of the base R package (?). To
perform a logistic regression the family option in glm is
set to binomial and the link function is set to logit. It
was not feasible to alter any other input parameters in
the function to produce our models.

2.3 Random Forests

Classiﬁcation trees are a non-parametric technique of
classiﬁcation (in contrast to both logistic regression and
LDA), which means that they do not assume an un-
derlying model of the data (??). Classiﬁcation trees
can be more accurate than parametric approaches when
complex interactions occur between the predictor vari-
ables. This could be the expected case for maser asso-
ciation with infrared or millimetre sources, as well as
a broad range of astronomical classiﬁcation problems.
Individual classiﬁcation trees may not be very accu-
rate, especially when there are more than a few pre-
dictor variables, however, a collection of trees grown
independently on randomly perturbed versions of the
data greatly increases the accuracy of predictions (??).
Random forests work by producing large numbers of
classiﬁcation trees and then determining the classiﬁca-
tion of a particular sample (in our case, an astronomical
source) by allowing each of these trees to “vote” and
then taking the majority rule (?). This voting system
is also how the probability of a sample being classiﬁed
into a certain group is calculated; by dividing the num-
ber of trees voting for a certain classiﬁcation by the
total number of trees.

To produce individual classiﬁcation trees in a random
forest a bootstrap sample is selected for each tree. For a
data set with N entries, N samples are taken. Because

4

Manning et al.

sampling is done with replacement, approximately two
thirds of the original data occurs at least once in each
bootstrap sample (?). ? showed that bootstrap sam-
pling causes the variance of the estimated class to con-
verge to a lower limit when more trees are added to the
forest, and so rarely overﬁt (?). A classiﬁcation tree is
grown from each bootstrap sample using recursive bi-
nary partitioning. The branching points of the trees are
called nodes. In standard trees, the predictor variable at
each node is chosen based on the best split, which is de-
termined by the Gini index (a measure of statistical dis-
persion, see ?, pg 271). In a random forest the variable
providing the best split is chosen from a random subset
of predictor variables (?). The predictor variable and
the subset of predictor variables from which it is chosen
is independent of any other nodes’ variable choices. This
approach decreases the dependence between individual
trees. The splitting process continues until further sub-
division no longer decreases the Gini index. The ﬁnal
classiﬁcation given by each tree depends on the termi-
nal node the source has been allocated to.

A nice feature of random forests is that they have an
inbuilt way of estimating the classiﬁcation error because
of the use of bootstrapping to select slightly diﬀerent
data for each tree. Data not included in the bootstrap
sample (approximately one third of observations) for a
particular tree are referred to as out-of-bag (oob) val-
ues. The tree grown from each bootstrap sample is used
to predict the classiﬁcation for each of the oob values,
giving an estimate of the classiﬁcation error as well as a
means to compare the importance of each variable in the
classiﬁcation process (?). The importance of a variable
is expressed by the diﬀerence between the probability of
predicting the class correctly in shuﬄed oob data (the
sample order is rearranged to eliminate systematic er-
rors) compared to the unshuﬄed oob data (?).

Random forests also give a natural metric for de-
termining the similarity of two diﬀerent astronomical
sources (or other groups of samples). Proximities be-
tween two sources are calculated in the random forest
process. If a pair of sources end up in the same termi-
nal node, their proximity is increased by one. Similar
source pairs end up in the same terminal node more of-
ten than dissimilar ones. The proximities are then nor-
malised (divided by the total number of trees) and the
proximity of a point and itself is set to be one. The
proximities are then expressed as a symmetric matrix,
where the diagonal entries all have the value one. The
proximity matrix can be used as input for multidimen-
sional scaling, as a way of visualising the classiﬁcation
results (displayed in Sec. 3).

A potential drawback of random forests is that they
cannot be used to directly test hypotheses (?). They
also do not give a clear representation of the actual
classiﬁcation process. However, although the internal
calculations are diﬃcult to interpret, they produce use-

PASA (2016)
doi:10.1017/pas.2016.xxx

ful properties such as relative variable importance and
an estimate of the classiﬁcation error without extra ex-
ternal calculations (?).

To create the random forests used in the mod-
elling and classiﬁcation, we used the R function
randomForest (in the randomForest package). For an
introduction to the usage and features of randomForest
functions in the R environment, see ?. There are a num-
ber of parameters that can be varied when growing the
random forest in order to optimise its classiﬁcation and
predictive accuracy. These include the number of trees
in the forest, the number of variables randomly sampled
as candidates at each split, and the maximum number
of terminal nodes in the trees. The minimum size of
the terminal nodes can also be varied, where a larger
number leads to smaller trees which take less time to
grow. Setting the node size to k means that no node
with fewer than k cases will be split (?). A terminal
node size of 1 is therefore the most accurate, but in
cases with large datasets, memory constraints may re-
quire this to be higher. We found that altering these
parameters did not consistently increase the sensitivity
or speciﬁcity signiﬁcantly, so the default values for the
parameters were used: 500 trees grown in the forest, a
node size of 1 (default for regression is 5), and the max-
imum possible number of terminal nodes. The default
number of variables chosen at each split is √p for clas-
siﬁcation and p/3 for regression (rounded to the nearest
integer), where p is the total number of predictor vari-
ables in the data set. Other factors that can be varied
are whether or not the cases are sampled with replace-
ment (the default, which we used, is with replacement),
and the prior probability of each class occurring can
also be set with the default being to assume equal class
probabilities.

For both Data Set 2 and 3 (where predictions were
done), random forests were grown using 3000 trees
rather than the default 500. Since each tree is grown
independently, this is equivalent to combining the re-
sults of multiple smaller forests. 3000 trees was chosen
for both data sets because this produced the most ac-
curate results in the cross validation. Generally random
forests is robust against over-ﬁtting (see ?), however in
the case of Data Set 2, due to the very small training
set compared to its number of predictor variables, more
than 3000 trees decreased the classiﬁcation accuracy. In
the case of Data Set 3, using more than 3000 trees had
no eﬀect.

2.4 Accuracy of classiﬁcation techniques

There are four possible outcomes of the classiﬁcation of
each astronomical source. The two desired outcomes are
that the classiﬁcation technique can correctly identify
a source which does have an associated maser (a “true
positive”), or it can correctly identify a source as not

Comparison of classiﬁcation techniques for masers

5

Classiﬁed as

Negative

Classiﬁed as

Positive

Know Negatives

Known Positives

True Negatives

False Negatives

False Positive

True Positive

Speciﬁcity

Sensitivity

(True Negative Rate)

(True Positive Rate)

Table 1 The relationship between the four possible classiﬁca-
tion results and the calculated values of the sensitivity and the
speciﬁcity.

having a maser (a “true negative”). A perfect classiﬁ-
cation would have all samples with one or the other of
these outcomes. There are however, two ways in which
the classiﬁcation scheme can give an incorrect outcome
and depending on the circumstances these are not nec-
essarily of equal importance. A “false positive” outcome
is where a source which does not have an associated
maser is classiﬁed as being associated with one, while a
“false negative” occurs when a source which does have
an associated maser is classiﬁed as not having one as-
sociated (see the Confusion Matrix in Table 1).

For each classiﬁcation method we calculated both the
sensitivity (known as recall in machine learning) and
speciﬁcity. In this context, the sensitivity, or true pos-
itive rate (TPR), is the percentage of maser associa-
tions correctly predicted by the model, and speciﬁcity
is the percentage of maser non-associations correctly
predicted, or the true negative rate (TNR).

Sensitivity (TPR) =

True Positives

True Postives + False Negatives

Speciﬁcity (TNR) =

True Negatives

True Negatives + False Positives

2.4.1 Predictor Variable Importance
Logistic regression performed using R has two tech-
niques for determining the importance of the variables
included in the model. The ﬁrst is a set of P-values
provided when the logistic regression is performed. The
second is the in-built stepAIC function, which includes
all possible predictor variables in the starting model
and iteratively removes variables which do not signiﬁ-
cantly contribute to the model to yield the most parsi-
monious model with the greatest predictive power. To
determine which variables to include in the logistic re-
gression models, we used a combination of the stepAIC
function and manual variable selection. Variables that
did not increase the accuracy of the model were ex-
cluded (see Sec. 3).

For LDA variable selection was done manually. We
used the logistic regression’s selection as a starting

PASA (2016)
doi:10.1017/pas.2016.xxx

point, and then included additional predictors if they
improved the prediction accuracy.

Random forests includes an internal calculation of the
Mean Decrease in Accuracy for each of the variables
utilised, which is a measure of how poorly the model
performs when that variable is not included. Thus, the
higher the value is, the more the predictor variable con-
tributes to the accuracy of the model. Negative values
decrease the accuracy and values close to zero oﬀer lit-
tle or no eﬀect. It is worth noting that random forests
is potentially robust enough to deal with all available
variables and so including them all in the model does
not generally decrease the accuracy signiﬁcantly (?).

2.4.2 Cross Validation
The aim of classiﬁcation is to build models that will
generalise well to new data. When constructing models
there is a danger in over-ﬁtting to the training data. In
order to determine the accuracy of each of the classiﬁ-
cation methods on the three data sets, we used a 10-fold
cross validation technique. Using a ﬁtted model that has
been trained on a randomly chosen 90% of the data, the
classiﬁcation of the remaining tenth is predicted. This
procedure of training and prediction is then repeated
1000 times in order to obtain an estimate of the clas-
siﬁcation error. Repeating the cross validation ensures
that a high number of the possible combinations of the
data are used, reducing sampling bias associated with
randomly folding the data. Repeated 10-fold cross vali-
dation of this kind is especially useful when modelling a
random forest as the over-ﬁtting associated with regres-
sion tree techniques is compensated for by the generous
error estimation of the cross validation (?).

In repeated 10-fold cross validation, the results from
the multiple runs are averaged. In this case the averaged
cross validation produced a mean probability of being
associated with a maser for each sample. A source was
classiﬁed as a maser if the probability was 50% or above.
The percentage of predicted classiﬁcations were then
compared to the actual classiﬁcations (maser source or
non-maser source) to determine the accuracy for each
model for each of the three data sets. Adjusting the cut-
oﬀ threshold for maser classiﬁcation from 50% was also
investigated to explore the trade-oﬀ between sensitivity
and speciﬁcity of the model. This is useful information
to have available when it is important to obtain all the
positive classiﬁcations, even when it means many false
positives are given, and alternatively the model can be
adjusted so that there is only a very small chance of
a false positive, at the expense of false negative classi-
ﬁcations. The receiver operating characteristic (ROC)
curves (explained in Sec. 2.4.3), display the results of
this analysis.

6

Manning et al.

2.4.3 Receiver Operating Characteristic Curves
A ROC curve plots the true positive rate (sensitivity)
against the false positive rate (1 − speciﬁcity), eﬀec-
tively showing the trade-oﬀ in prediction power for ac-
curacy in a given classiﬁcation model. The diagonal line
y = x represents randomly classifying the samples, with
half predicted as positive and half as negative. Any-
where in the space above this line means that the model
is better than random classiﬁcation, with the best pos-
sible system showing 100% sensitivity with no false pre-
dictions, resulting in a point in the top left hand corner.
ROC curves were plotted here to compare each classiﬁ-
cation method for each data set in Figures 2, 5 and 8.

3 Results

3.1 Water Masers associated with Star

formation regions in the RCW106 Giant
Molecular Cloud

? undertook a complete search for 22 GHz water masers
within the giant molecular cloud RCW 106. This search
detected nine 22 GHz water masers and the region
searched included 73 1.2-mm dust clumps observed and
characterised by ?. Seven of the dust clumps were found
to be associated with masers (?). ? used a form of lo-
gistic regression called binomial generalised linear mod-
elling (GLM) to investigate the properties of the as-
tronomical sources (in this case dust clumps) with and
without water masers in RCW106. They found that wa-
ter masers were associated with those sources which are
denser, more massive and have higher luminosity.

There are clear diﬀerences in the values of all the pre-
dictor variables between those sources with an associ-
ated water maser and those without, as is demonstrated
by the boxplots shown in Figure 1. However, it should
be noted that there are varying degrees of overlap in the
ranges observed for the maser associated sources and
those which are not. The obvious diﬀerence in the dis-
tributions for all the predictor variables means that we
might expect that they should all contribute to the clas-
siﬁcation and that the relative importance might also
be similar. The variable importance ratings returned
by the random forest classiﬁcation are a measure of the
degree to which the classiﬁcation trees utilised each pre-
dictor variable. The ﬁve predictor variables available as
inputs for the classiﬁcation process were : peak ﬂux den-
sity, source radius, total integrated ﬂux density, dust
mass (calculated assuming a temperature of 40 K and
optically thin dust emission) and column density. Using
only source radius and the total integrated ﬂux den-
sity provided the highest accuracy for random forests,
logistic regression and LDA, while LDA using the “nor-
malised” data (transformed using a log function, see
Sec. 2.1) was able to utilise the column density too. Ta-
ble 2 shows the comparison of which of the predictor

PASA (2016)
doi:10.1017/pas.2016.xxx

Random Logistic
Forests
10.68
16.35

Reg.
0.1388
0.2485

LDA

Y
Y

Norm.
LDA

Y
Y
Y

Radius
Int. Flux
Density

Table 2 The predictor variables that increased the classiﬁcation
accuracy of the various methods for Data Set 1. Random forests
provides an internal calculation of the Mean Decrease in Accuracy
(the higher the value, the more important the variable), logistic
regression provides P-values (the lower the value, the more signif-
icant the variable’s contribution to the model), and LDA provides
no internal measurement of the importance of each variable, so it
is just noted which variables were used (see Sec 2.4.1).

Reg.

Random Logistic
Forests
66
0
2
5
100
100

65
1
2
5
98.5
71.4

LDA
66
0
3
4
100
57.1

Norm.
LDA
65
1
2
5
98.5
71.4

True Neg.
False Pos.
False Neg.
True Pos.
Speciﬁcity%
Sensitivity%

Table 3 The results of cross-validating random forests, logistic
regression and LDA (without and with transformation of the pre-
dictor variables) classiﬁcation and prediction for Data Set 1.

variables were included in the models based on their
contributions to an increase in classiﬁcation accuracy.
? showed that their sample of water masers preferred
denser, more massive and more luminous sources. Our
models indicated that the radius, luminosity and in the
case of LDA on the normalised data, the density were
important variables in predicting whether the sources
were associated with a maser or not. Our results are
in agreement with ?, except that our models were not
improved by inclusion of mass as a predictor variable.
Table 3 summarises the results we obtained from
cross validation of the three diﬀerent classiﬁcation tech-
niques under consideration (for details see Sec. 2.4.2).
Speciﬁcity values were high, due to the fact that the ma-
jority of the sources were not associated with masers,
with the sensitivity values being lower in each case. For
Data Set 1, random forests performed the best consid-
ering both sensitivity and speciﬁcity. Notably, there are
very few false positive classiﬁcations over all the models,
which is most likely due to the data being unbalanced
in that the majority of the samples were not associated
with masers. Another clear result is that performing
LDA on the log transformed data increases the model’s
sensitivity, making it comparable to logistic regression
in this case. The advantage of transforming the data is
also obvious in the ROC shown below in Figure 2 (for
an explanation on ROC curves, see Sec. 2.4.3).

Figure 2 shows that LDA under-performs for Data
Set 1, however when LDA is applied to the transformed
data it is more accurate than logistic regression. The
relatively small data set causes the apparent steps in
the plot and this is also evident for Data Set 2 in Fig-

Comparison of classiﬁcation techniques for masers

7

Column Density

Mass

Radius

)
s
c
e
s
r
a
p
(
 
s
u
d
a
R

i

5
1

.

0

.

1

5

.

0

No Maser

Maser

Integrated Flux

No Maser

Maser

6

5

4

3

2

1

)
2
−

m
c
6
0
1
(
 
y
t
i
s
n
e
D

)
 
)

K
(
A
T
(
 
x
u
F
 
k
a
e
P

l

0
0
0
0
3

0
0
0
0
1

0

No Maser

Maser

Peak Flux

)
s
e
s
s
a
m

l

 
r
a
o
s
(
 
s
s
a
M

0
0
0
5
1

0
0
0
0
1

0
0
0
5

0

)
 
1
−
s
m
k
 
K

l

(
 
x
u
F
 
d
e
t
a
r
g
e
t
n
I

0
2
1

0
8

0
6

0
4

0
2

0

No Maser

Maser

No Maser

Maser

Figure 1. Boxplots comparing the variables from Data Set 1 between the sources with an associated water maser and those without.
The outline in each of the boxplots represents the range between the ﬁrst and third quartiles, with the median being the solid line
horizontally through the box. The vertical lines outside the box extend to the minimum and maximum values, with any outliers (values
separated from the quartiles by more than one and a half times the interquartile range) shown separately as dots. In this case, due to
the very small number of samples being associated with a maser in this data set, the individual sample points are also plotted.

ure 5. The ROC curves for Data Set 3 (Fig. 8) are much
smoother because there are 214 samples rather than 73,
or 32. Despite the apparent steps in the ROC curves,
the plot very clearly shows the most accurate classiﬁ-
cation technique for this data set (the non-parametric
method of random forests) and the least accurate (the
parametric method of LDA using untransformed data).
Figure 3 shows a multi-dimensional scaling (MDS)
plot for the full data set. MDS plots give a visual rep-
resentation of the distances between proximities identi-
ﬁed in the random forest implementation; sources that
the random forest process identiﬁes as being similar are
clustered within the MDS plot. The distance values are
arbitrary, they are simply relative magnitudes, plotted
here as Dimension[1] and Dimension[2]. Figure 3 shows
the four correctly identiﬁed maser sources in a group
at the top-left, separated from the non-maser sources.
“Border-line” classiﬁcations were samples with a pre-

PASA (2016)
doi:10.1017/pas.2016.xxx

dicted maser association between 45 and 55%, with the
last correctly classiﬁed maser shown just below the oth-
ers as such. The model was not sensitive enough to de-
tect the diﬀerences in the predictor variables for the
other maser-associated sources (which is why they were
classiﬁed as not having an associated water maser). This
is probably due to the small number of sources in this
data set.

3.2 The properties of water maser-associated

YSOs in the LMC

? used the Spitzer Space Telescope Surveying Agents of
Galaxy Evolution (SAGE) Legacy programme data (?),
along with other public data sets to identify high- and
intermediate-mass young stellar objects (YSOs) in the
Large Magellanic Cloud (LMC). ? identiﬁed 855 deﬁnite
YSOs in the LMC and compiled near- and mid-infrared

8

e
t
a
R
 
e
v
i
t
i
s
o
P
 
e
u
r
T

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
0

.

ROC Curve − Data Set 1

Random Forest Analysis (Data Set 1)

Manning et al.

i

]
2
[
n
o
s
n
e
m
D

i

True Positive
False Negative
True Negative

6
.
0

4
.
0

2
.
0

0
.
0

2
.
0
−

.

4
0
−

Random Forests
Logistic Regression
LDA
Normalised LDA

0.0

0.2

0.4

0.6

0.8

1.0

−0.6

−0.4

−0.2

0.0

0.2

False Positive Rate

Dimension[1]

Figure 2. Receiver operating characteristic curves showing the
results of the cross validation for Data Set 1. The diagonal line y =
x represents randomly classifying the samples, with half predicted
as positive and half as negative. For deﬁnitions of classiﬁcation
results, see Sec. 2.4.

photometric measurements for the sample. ? made Aus-
tralia Telescope Compact Array (ATCA) observations
for the 22 GHz transition of water towards all known
star formation maser sites in the LMC, resulting in a
total of 13 water masers in the LMC for which posi-
tions are known to arcsecond accuracy. The ﬁelds ob-
served for the water maser observations included a total
of 32 sources from the ? YSO catalogue. Of the 13 water
masers, 11 are within 2 arcseconds of a ? YSO, meaning
that from a total catalogue of 855 sources there are 11
which are known to have an associated water maser and
22 which are known not to. The 33 sources for which
there is information on whether or not they have an as-
sociated water maser can be used as a training set for
classiﬁcation/prediction.

? used the infrared data from ? to construct the spec-
tral energy distribution (SED) of each of the YSOs
using the online SED-ﬁtter of ? and this forms Data
Set 2. For some wavelength ranges the infrared data
for the ? sample is incomplete, hence there is miss-
ing data. However, the results of the SED modelling
contain no missing data (although there is likely to be
greater uncertainty in the ﬁtted SED parameters for
those sources which have less infrared photometric mea-
surements contributing to the ﬁtting process). All avail-

PASA (2016)
doi:10.1017/pas.2016.xxx

Figure 3. MDS plot of the proximity values produced by the
random forest classiﬁcation (Data Set 1). The values on the axes
are arbitrary, the graph just compares relative magnitudes. The
closer two points are on the plot, the more similar their properties
as determined by the random forest classiﬁcation. “Border-line”
classiﬁcations were samples with a predicted maser association
between 45 and 55%.

able information about a source is incorporated into the
SED model. According to ?, there is very little variation
in the amount of information available for each SED ﬁt,
with between seven and nine infrared intensities avail-
able for each source and in the majority of cases the
chi-squared values for the resulting SED ﬁts are reason-
able. Due to the large number of sources modelled, we
made no attempt to remove the sources where this was
not the case, with the exception of one maser-associated
source with more missing data than the others (making
our training sample 32 with 10 known masers, and the
total data set 854).

Fifteen predictor variables were extracted from the
SED ﬁtting results; distance to the source, age, radius,
mass and temperature of the central source, envelope
accretion or infall rate, outer and inner radius of the
envelope, cavity opening angle, disc mass, ambient den-
sity, inclination of source to line of sight (LoS), average
integrated ﬂux density (from the outside of the YSO
to the stellar surface, along the LoS), total luminosity,
and mass of the envelope. Table 4 shows which variables
were used in each model and how they contributed to
that model. Across the diﬀerent methods, the most im-

Comparison of classiﬁcation techniques for masers

9

Envelope Accretion Rate

Ambient Density

Mass of Central Source

)
r
a
e
y
 
r
e
p
 
s
e
s
s
a
m

l

 
r
a
o
s
(
 
n
o

i
t

e
r
c
c
A

6
0
0

.

0

4
0
0

.

0

2
0
0

.

0

0
0
0

.

0

No Maser

Maser

Inclination Towards LOS

No Maser

Maser

Total Luminosity

3
1

2
1

1
1

0
1

9

8

7

i

y
t
i
s
o
n
m
u
L
 
c
m
h
t
i
r
a
g
o
L

i

)
s
e
s
s
a
m

l

 
r
a
o
s
(
 
s
s
a
M

0
4

0
3

0
2

0
1

)
s
e
e
r
g
e
d
(
 
n
o
i
t
a
n

i
l

c
n
I

0
8

0
7

0
6

0
5

0
4

0
3

0
2

y
t
i
s
n
e
D
 
c
m
h

i

t
i
r
a
g
o
L

5

.

5
4
−

5

.

6
4
−

5

.

7
4
−

)
s
e
s
s
a
m

l

 
r
a
o
s
(
 
s
s
a
M

0
0
5
2

0
0
5
1

0
0
5

0

No Maser

Maser

Mass of Envelope

No Maser

Maser

No Maser

Maser

No Maser

Maser

Figure 4. Only the predictor variables from Data Set 2 showing noticeable diﬀerences between those YSO with and without an
associated water maser are shown. Due to the very small data set, the individual sample points are also plotted. Some of the variables
are on logarithmic scales to better illustrate the diﬀerences. For an explanation of boxplots, see Fig. 1.

portant predictor variables appeared to be the mass of
the central source, the outer envelope radius, the incli-
nation towards the LoS, and the mass of the envelope.
In comparison, ? found that the majority of YSOs with
an associated water maser have high luminosities, cen-
tral masses and ambient densities. They also tend to
have redder infrared colours than those YSOs which
are not associated with a maser. The distributions of
the high-importance variables are shown in Figure 4.

Unlike Data Set 1, these data include some sources
where the maser association is known (32) and some
where it is unknown (822). This means predictions can
be made on the unknown sources. To test how well the
various methods will generalise to data where maser
association is unknown, a cross-validation was applied.
For a full description of the technique used, see Sec-
tion 2.4.2. The predictions were then compared with
the actual maser association. The results are given in
Table 5.

Random Logistic
Forests

Reg.

LDA

Norm.
LDA

Distance
Age
Mass
Radius
Temperature
Accretion
Outer Env.
Inner Env.
Cavity Angle
Disc Mass
Amb. Density
Inclination
Av. Int. Flux
Total Lum.
Env. Mass

3.943
2.193
2.091
2.257

1.099
3.891

2.677

1.047
4.202

0.0805
0.395

0.1609

0.1935

0.2765

Y
Y
Y
Y
Y
Y
Y
Y
Y

Y
Y

Y

Y

Y

Y

Y
Y
Y

Y

Y

Table 4 The predictor variables that increased the classiﬁcation
accuracy of the various methods for Data Set 2. The value given
for random forests is the Mean Decrease in Accuracy, while logis-
tic regression provides P-values. The most important variables in
logistic regression and random forest models are shown in bold.
For further explanation see Table 2.

PASA (2016)
doi:10.1017/pas.2016.xxx

10

True Neg.
False Pos.
False Neg.
True Pos.
Speciﬁcity%
Sensitivity%

Reg.

Random Logistic
Forests
20
2
6
4
90.9
40.0

17
5
8
2
77.3
20.0

LDA
15
7
5
5
68.2
50.0

Norm.
LDA
20
2
4
6
90.9
60.0

Table 5 The results of cross-validating random forests, logistic
regression and LDA classiﬁcation and prediction for Data Set 2
(association of water masers with infrared YSO in the LMC) using
the full sample of 32 sources with known water maser association
status as the training sample. For deﬁnitions of classiﬁcation re-
sults, see Sec. 2.4.

The SED Data Set 2 had a fairly small number of en-
tries with known maser status (32), but a large number
of possible predictor variables (15). The results for the
cross validation are shown in Table 5 and in the form
of ROC curves in Figure 5.

The cross validation results show that overall, the
sensitivity values were quite poor, with LDA per-
formed on the normalised data being the most accurate
method. This was possibly due to the small data set,
methods could not construct an accurate model using
only 29 sources and then predicting on the remaining 3.
These results can be visualised in a ROC curve shown
in Figure 5, where in a number of cases the models
fall below the y = x diagonal line, meaning that they
perform worse that simple random classiﬁcation with a
50% chance of a source being associated with a maser.
Due to these poor results, we decided not to use this
training data to make predictions on the remaining 822
sources with unknown maser status.

It is evident from the MDS plot in Figure 6 why the
random forest model had a low sensitivity; the data is
not clustered in groups to the same extent as Data Set
1 (Fig. 3). The known maser sources are represented by
the black and red triangles, while the blue circles and
green diamonds represent the known non-maser sources.
This could be due to the model’s inability to condense
the 15 predictor variables (equivalent to 15 dimensions)
into a two-dimensional plot, or another bi-product of
the small sample size.

In summary, the variables that had the most inﬂu-
ence over the various classiﬁcation models were mass of
the central star, the outer envelope radius, the inclina-
tion towards the LoS, and the mass of the envelope (see
Table 4). The likelihood of a YSO being associated with
a water maser source did not appear to depend heavily
on variables such as the age of the source, mass of the
disc, ambient density or average integrated ﬂux. ? ap-
plied Mann-Whitney tests to the diﬀerent variables to
ﬁnd the diﬀerence in the medians of the distributions
of those associated with masers and those not associ-
ated. Statistically signiﬁcant diﬀerences were found in
the data for the mass of the central star, the outer radius

PASA (2016)
doi:10.1017/pas.2016.xxx

Manning et al.

ROC Curve − Data Set 2

Random Forests
Logistic Regression
LDA
Normalised LDA

e
t
a
R
 
e
v
i
t
i
s
o
P
 
e
u
r
T

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
0

.

0.0

0.2

0.4

0.6

0.8

1.0

False Positive Rate

Figure 5. Receiver operating characteristic curves showing the
results of the cross validation for Data Set 2. The diagonal line y =
x represents randomly classifying the samples, with half predicted
as positive and half as negative. For a full description of a ROC
curve, see Section 2.4.3.

of the envelope, ambient density, inclination towards the
line of sight and the total luminosity; results that agree
with our analysis. It was previously suggested that the
inclination angle is one of the most inﬂuential predictors
in determining the SED for young stellar objects (?). As
a result of the orientation of the cavity, the inclination
angle dictates the contribution from the inner, hotter re-
gions of the envelope to the SED. Hence, our classiﬁca-
tion results here agree with those from previous studies,
indicating that the physical variables mentioned above
are likely to dictate water maser-association with cer-
tain YSOs in the LMC.

3.3 The properties of dust continuum

emission associated with class I
methanol masers

The ﬁnal data set we investigated (hereafter, Data Set
3) was a search for 95 GHz class I methanol masers
targeted towards regions selected on the basis of both
their emission at mid-infrared and millimetre wave-
length ranges (?). The mid-infrared data was taken
from the Spitzer Space Telescope GLIMPSE (Galac-
tic Legacy Infrared Mid-Plane Survey Extraordinaire)
program, which provides photometric measurements in

Comparison of classiﬁcation techniques for masers

Random Forest Analysis (Data Set 2)

i

]
2
[
n
o
s
n
e
m
D

i

4
.
0

2
.
0

0
.
0

2
.
0
−

4
.
0
−

.

6
0
−

True Positive
False Positive
False Negative
True Negative
Border−line

−0.4

−0.2

0.0

0.2

0.4

Dimension[1]

Figure 6. The MDS plot for the random forest model used to
predict potential YSOs with an associated water maser in the
LMC (Data Set 2). “Border-line” predictions were samples with
a predicted maser association between 45 and 55%. For details on
multidimensional plots in random forest analysis, see Fig. 3.

four wavelength bands (3.6, 4.5, 5.8 and 8.0 µm; ??),
while the millimetre continuum data was from the Bolo-
cam Galactic Place Survey BGPS (?). The motiva-
tion for this survey was a previous search for 95 GHz
class I methanol masers by ?. The authors targeted in-
frared sources for which GLIMPSE images show ex-
tended emission with an excess in the 4.5-µm band
(thought to indicate an outﬂow from a high-mass YSO).
It was found that those GLIMPSE sources with an as-
sociated BGPS source (54 of the 62 sources which lay
within the BGPS region) were much more likely to ex-
hibit class I methanol maser emission. ? also found that
the GLIMPSE sources with redder mid-infrared colours
were more likely to be associated with methanol masers
and the higher the mass and density of the BGPS dust
clump, the stronger the class I maser emission.

? used the results of ? to identify 420 sources detected
in both the Spitzer GLIMPSE and BGPS catalogues
as likely to have an associated class I methanol maser.
They then observed a random selection of 214 of these
sources and detected 95 GHz class I methanol masers
towards 62 (hence 152 non-detections). For the classi-
ﬁcation process we used only the data from the BGPS
catalogue (version 1.0) which contains a total of 8358
sources (?). The predictor variables used in the classi-

PASA (2016)
doi:10.1017/pas.2016.xxx

Major Axis
Minor Axis
Position Angle
Angular Radius
40 arcseconds
80 arcseconds
120 arcseconds
Int. Flux Den.

Random Logistic
Forests
4.124
10.28
1.758
9.218
27.94
21.81
14.24
15.31

Reg.
0.2781
0.3867
0.0952
0.3252
0.0312
0.4267
0.6531
0.1158

11

LDA

Norm.
LDA

Y
Y

Y
Y
Y
Y

Y
Y
Y

Y

Y

Table 6 The predictor variables that increased the classiﬁcation
accuracy of the various methods for Data Set 3. The value given
for random forests is the Mean Decrease in Accuracy, while logis-
tic regression provides P-values. The most important variables in
logistic regression and random forest models are shown in bold.
For further explanation see Table 2.

ﬁcation models for Data Set 3 were the angular size of
the major and minor axis of the dust clump, as well as
its position angle, deconvolved angular radius and 1.1
mm ﬂux density within apertures of diameter 40, 80 and
120 arcseconds and the integrated ﬂux density. As with
the classiﬁcation of the other two data sets, here each
of the models were optimised by omitting superﬂuous
variables, as well as those that decreased the models’
accuracy. Both logistic regression and random forests
utilised all eight variables, while LDA performed better
without including all of them (see Table 6).

This data set was the primary focus of our analysis,
as it has a training set with several hundred sources, in-
cluding a large number of detections and there are also
a large number of BGPS sources which have not been
searched for class I methanol maser emission (8144)
which provide the opportunity to make testable pre-
dictions.

The variables with high importance in the random
forests calculations were the ﬂux densities (each of the
40, 80 and 120 arcsecond aperture values and the in-
tegrated) and also the angular size of the minor axis.
The most important variable was the ﬂux density within
40 arcseconds (the smallest angular scale measured by
Bolocam). Logistic regression also found the ﬂux den-
sity within 40 arcseconds to be the most important vari-
able with a P-value of 0.0312, with the next most signif-
icant variable being the position angle with a P-value of
0.0952, while the 80 arcsecond ﬂux had the next high-
est contribution. This is consistent with the results of ?
which showed that class I masers were preferentially as-
sociated with sources with the highest beam averaged
column density (which is directly proportional to the
40 arcsecond ﬂux density). There is no physical reason
why the position angle of the dust clump would eﬀect
the likelihood of a dust clump having an associated class
I methanol maser, but when this variable was omitted
from the classiﬁcation, the accuracy of the models de-
creased. However, while the P-value suggests that the
position angle is a signiﬁcant predictor variable in logis-

12

Manning et al.

Deconvoluted Angular Radius

Major Axis

Minor Axis

0
7

0
6

0
5

0
4

0
3

0
2

)
s
d
n
o
c
e
s
c
r
a
(
 
s
x
A

i

j

 
r
o
a
M

)
s
d
n
o
c
e
s
c
r
a
(
 
s
x
A

i

 
r
o
n
M

i

0
4

0
3

0
2

0
1

No Maser

Maser

Position Angle

No Maser

Maser

No Maser

Maser

Aperture Diameter 40’’ 

Aperture Diameter 80’’ 

l

y
t
i
s
n
e
D
 
x
u
F
 
c
m
h
t
i
r
a
g
o
L

i

2

1

0

1
−

2
−

3

2

1

0

1
−

2
−

3

2

1

0

1
−

2
−

l

y
t
i
s
n
e
D
 
x
u
F
 
c
m
h
t
i
r
a
g
o
L

i

No Maser

Maser

No Maser

Maser

Integrated Flux Density 

No Maser

Maser

)
s
d
n
o
c
e
s
c
r
a
(
 
s
u
d
a
R

i

0
2
1

0
8

0
6

0
4

0
2

0

)
s
d
n
o
c
e
s
c
r
a
(
 
e
g
n
A
 
n
o
i
t
i
s
o
P

l

0
5
1

0
0
1

0
5

0

No Maser

Maser

Aperture Diameter 120’’ 

l

y
t
i
s
n
e
D
 
x
u
F
 
d
e
t
a
r
g
e
t
n
I
 
c
m
h

i

No Maser

Maser

t
i
r
a
g
o
L

3

2

1

0

1
−

2
−

l

y
t
i
s
n
e
D
 
x
u
F
 
c
m
h
t
i
r
a
g
o
L

i

Figure 7. The variables used in the classiﬁcation and prediction of Data Set 3. Some of the variables are on logarithmic scales to
better illustrate the diﬀerences. For an explanation of boxplots, see Fig. 1.

tic regression, the change in accuracy was not signiﬁcant
compared to that of the other variables. This suggests
that we can dismiss it as an artefact of the classiﬁcation
method, but it does serve as a reminder to view results
such as this with a degree of scepticism. It is also worth

noting that random forests presented it with the lowest
variable importance.

Table 7 shows the results of the cross-validation of
the diﬀerent classiﬁcation techniques used on Data Set 3
(see Sec. 2.4.2). Here, random forests oﬀered the highest

PASA (2016)
doi:10.1017/pas.2016.xxx

Comparison of classiﬁcation techniques for masers

13

True Neg.
False Pos.
False Neg.
True Pos.
Speciﬁcity%
Sensitivity%

Reg.

Random Logistic
Forests
141
11
21
41
92.8
66.1

142
10
24
38
93.4
61.3

LDA
148
4
31
31
97.4
50.0

Norm.
LDA
145
7
23
39
95.4
62.9

True Neg.
False Pos.
False Neg.
True Pos.
Speciﬁcity%
Sensitivity%
Predictions

Reg.

Random Logistic
Forests
140
12
21
41
92.1
66.1
632

145
7
22
40
95.4
64.5
405

LDA
149
3
30
32
98.0
51.6
334

Norm.
LDA
147
8
22
40
96.7
64.5
460

Table 7 The results of cross-validating random forests, logistic
regression and LDA classiﬁcation and prediction for Data Set
3 (class I methanol masers associated with GLIMPSE sources).
Fig. 8 shows the ROC curve for each of the models. For deﬁnitions
of classiﬁcation results, see Sec. 2.4.

ROC Curve − Data Set 3

e

t

 

a
R
e
v
i
t
i
s
o
P
e
u
r
T

 

0
.
1

8

.

0

6
0

.

4
0

.

2
0

.

0
0

.

Random Forests
Logistic Regression
LDA
Normalised LDA

0.0

0.2

0.4

0.6

0.8

1.0

False Positive Rate

Figure 8. Receiver operating characteristic curves showing the
results of the cross validation for Data Set 3. The diagonal line y =
x represents randomly classifying the samples, with half predicted
as positive and half as negative.

sensitivity, while surprisingly performing LDA on the
untransformed data produced the highest speciﬁcity.
This is the ﬁrst instance in our studies where transform-
ing the data set to be closer to a normal distribution de-
creased the performance of LDA, although the decrease
was minor (2%) and likely not signiﬁcant. The ROC
curve in Figure 8 gives a more complete representation
of the models’ capabilities, showing that random forests,
logistic regression and LDA using the transformed data
performed to similar standards, while generally LDA on
the untransformed data performed the worst.

Figure 9 shows the MDS plot for the random forest
model generated using all the training data for Data Set
3. It is clear that the maser-associated dust clumps are

PASA (2016)
doi:10.1017/pas.2016.xxx

Table 8 The classiﬁcation results on the training data subset
(where the maser presence is known), and the number of pre-
dicted masers from the 8144 sources for which maser presence is
unknown, using Data Set 3 (class I methanol masers associated
with GLIMPSE sources). For deﬁnitions of classiﬁcation results,
see Sec. 2.4.

generally located in the bottom-right region of the plot.
Comparing this plot to the MDS plots for the other two
Data Sets (Fig.s 3 & 6), we can see that the maser as-
sociated sources are more clearly separated from those
without a maser-association. The green squares in Fig-
ure 9 represent sources which the random forests model
predicts to have an associated class I methanol maser,
but for which the observations of ? did not detect a
maser. Many of these sources lie very close on the MDS
plot to others where a maser was detected and it may
be that some of these non-detections have a weak class I
methanol maser which was not detected by ? due to the
limited sensitivity of those observations. The 95 GHz
class I methanol masers are in the same transition fam-
ily as the best studied class I methanol maser transi-
tion at 44 GHz. In general the 44 GHz class I methanol
masers have a peak ﬂux density approximately a factor
of 3 greater than the 95 GHz maser emission in the same
source (?). These sources would be good candidates for
sensitive observations in the 44 GHz transition to more
robustly determine if they are associated with class I
methanol masers.

The classiﬁcation models we have developed can also
be used to predict which of the BGPS sources that were
not observed by ? are the best candidates for having
an associated class I methanol maser. Since we have
four diﬀerent classiﬁcation models we can compare the
results of each, as those sources identiﬁed by all, or most
of the models would be expected to be the promising
targets for further searches.

For the prediction model, as with Data Set 2, we grew
a random forest using 3000 trees (instead of the default
500, see Sec. 3.2). Table A1 in the Appendix lists the 739
BGPS sources which were predicted to have an associ-
ated class I methanol maser by one or more of the four
classiﬁcation models for the 8144 BGPS sources which
have not yet been searched. Table 8 shows that of the
8144 potential BGPS target sources random forests pre-
dicts 632 to have an associated class I methanol maser
and this is signiﬁcantly more than any of the other
classiﬁcation models. There are 242 of the 8144 BGPS

14

Manning et al.

Random Forest Analysis (Data Set 3)

True Positive
False Positive
False Negative
True Negative
Border−line Classifications

i

]
2
[
n
o
s
n
e
m
D

i

4
0

.

2
.
0

0
.
0

2
.
0
−

4

.

0
−

−0.4

−0.2

0.0

0.2

0.4

Dimension[1]

Figure 9. The MDS plot for the random forest model used to predict potential millimetre dust-clumps with associated class I methanol
masers (Data Set 3). “Border-line” classiﬁcations were samples with a predicted maser association between 45 and 55%. For details on
multidimensional plots in random forest analysis, see Fig. 3.

PASA (2016)
doi:10.1017/pas.2016.xxx

Comparison of classiﬁcation techniques for masers

15

Random Logistic
Forests

Reg.

LDA

Norm.
LDA

632

364

317

377

405

254

371

334

256

460

Random
Forests

Logistic
Reg.

LDA

Norm.
LDA

Table 9 Number of maser predictions on sources from Data Set 3
shared by two classiﬁcation methods, with 242 sources predicted
to be masers using all four methods.

sources which all models predict will have an associated
class I methanol maser and these will be the prime tar-
gets for future searches. Table 9 shows the number of
sources predicted to be masers by one or two classiﬁ-
cation methods, which should be considered if there is
suﬃcient time to search additional targets.

4 Discussion

We applied three diﬀerent classiﬁcation techniques to
three diﬀerent searches for interstellar masers to investi-
gate each technique’s performance. We show the classiﬁ-
cation and prediction results of LDA performed on both
the normally distributed data and the un-transformed
data to demonstrate the diﬀerence (see Sec. 2.1). In
most cases, LDA performs signiﬁcantly better when ap-
plied to transformed data.

All three methods of classiﬁcation (both parametric
and non-parametric) used on Data Set 1 returned high
values for both sensitivity (correctly classifying sources
associated with masers) and speciﬁcity (correctly clas-
sifying non-maser sources). The highest accuracy was
achieved through the non-parametric method of ran-
dom forests, which in this case classiﬁed every source
correctly.

Data Set 2 had a relatively small training sample (32
sources) compared with the number of predictor vari-
ables (15), and we found here that LDA appeared to
give the best results, while random forests and logistic
regression performed quite poorly in correctly identify-
ing sources associated with a maser.

For Data Set 3, which has more than 50 detections
and more than 150 non-detections, the non-parametric
random forests had the highest sensitivity, while the
parametric method of LDA performed on the untrans-
formed data had the lowest, but also had the highest
speciﬁcity. Considering both sensitivity and speciﬁcity,
logistic regression and random forests were the most
accurate methods.

PASA (2016)
doi:10.1017/pas.2016.xxx

Based on the predictions of ? our initial expecta-
tion was that given suﬃcient training data the com-
plex relationship between the predictor variables and
the presence or absence of a related astrophysical phe-
nomenon would be more accurately represented by a
non-parametric approach than a simple linear model.
However, the training data sets were relatively small,
and so made it diﬃcult for the models to capture and
convey all the information contained within the predic-
tor variables. We ﬁnd that random forests does per-
form relatively better for the largest data set, but in
this case it is comparable with the accuracy of the non-
parametric techniques, not superior to them. It may be
that in order to outperform parametric methods the
non-parametric techniques require still larger amounts
of training data. However, it is more likely that for Data
Set 3 all techniques approximately reach the limit of the
information available within the measured parameters
of the data.

There are a number of factors related speciﬁcally to
the data which will lead to limitations in the accuracy of
any classiﬁcation model developed using it. One factor
is the intrinsic measurement uncertainty for parameters
such as the ﬂux density, angular size etc., which can in-
ﬂuence the results directly in the sense that it is always
possible that given observations with greater sensitiv-
ity additional sources would be detected. However, the
absence of these weaker sources does more than simply
qualifying the question that is being answered by the
classiﬁcation model. For example, the intensity of as-
trophysical masers depends in a complex and non-linear
manner on the physical parameters of the environment
and some of these parameters may not be represented
either directly or indirectly in any of the predictor vari-
ables being used as inputs to the classiﬁcation methods.
A second, less obvious factor which may limit the ac-
curacy of classiﬁcation techniques is that for derived
parameters there are often implicit assumptions. For
example the calculation of the mass of the dust clumps
used for Data Set 1 assumes that the emission at 1.2 mm
wavelength is optically thin (likely a reasonable assump-
tion), and that the dust is at a constant temperature of
40 K for all the dust. This second assumption is nec-
essary because we do not have any information on the
speciﬁc temperature distribution of the dust, but it in-
evitably leads to systematic errors in the relative mass
calculated for regions where the true dust temperature
is on average higher (or lower) than the assumed value.
Similarly, the distance to individual sources has been
estimated using kinematic distance models, which on
average provide a reasonable estimate, but which can
lead to signiﬁcant errors for individual sources. It is also
highly probable that our sensitivity values obtained af-
ter cross validation of the classiﬁcation techniques were
poor due to the unbalanced nature of the data, in that

16

Manning et al.

for all three data sets, the vast majority of the samples
are not associated with masers.

? tested the binomial generalised linear model of (?,
Data Set 1) by searching for 22 GHz water masers to-
wards 267 dust clumps. They found a high detection
rate towards dust clumps for which the binomial GLM
predicted a probability of greater than 10 % for the
presence of a water maser (20 of 27 sources). They also
found that while the detection rate dropped for sources
for which the model predicted a lower probability of
having an associated water maser, a substantial frac-
tion of water masers (approximately 70 %) were de-
tected towards sources for which the model predicted a
probability of less than 1 %. ? show that unreliable dis-
tance estimates for many of the dust clumps is in part
responsible for the misclassiﬁcation. This is consistent
with the assertion we make above that the combination
of measurement and systematic uncertainties in the un-
derlying data ultimately limit the accuracy which can
be obtained with any classiﬁcation technique.

When the diﬀerent classiﬁcation models we developed
using Data Set 3 are applied to the 8144 BGPS sources
which have not been searched for class I methanol maser
emission a total of 739 sources are predicted to have an
associated maser by one or more of the models, with 242
sources predicted by all four models (see Section 3.3 and
Appendix A). Figure 12 of ? plots the integrated ﬂux
density against the beam averaged H2 column density
for Data Set 3 and shows that the maser associated
sources are restricted to a limited range for these two
predictor variables. Figure 10 shows the integrated ﬂux
density versus the beam averaged H2 column density
for the 8144 BGPS sources not observed by ?. Those
sources for which one or more of the classiﬁcation mod-
els predict an associated class I methanol maser are in-
dicated with a red dot, with sources which no model
predicts to have an associated class I maser are indi-
cated with a black dot. Figure 10 shows that there is a
high level of agreement between the predictions of the
classiﬁcation models and the empirical criteria devel-
oped by ?. In total 1200 BGPS sources meet the crite-
ria identiﬁed by ?, approximately a factor of two more
than identiﬁed by any of the classiﬁcation models. In
their calculation of the beam averaged column density
? assumed a constant temperature of 20K for the dust
clumps and used the 40 arcsecond ﬂux density measure-
ment as the intensity of the dust continuum emission.
This means that the calculated beam average column
density is directly proportional to the BGPS 40 arcsec-
ond ﬂux density measurement. The relationship derived
by ? suggested this is the most important predictor vari-
able for the presence (or otherwise) of class I methanol
maser emission towards these sources. The results pre-
sented here also support this.

Ultimately, determining the relative accuracy of these
classiﬁcation models, and whether they are superior

PASA (2016)
doi:10.1017/pas.2016.xxx

3

2

1

0

]
y
J
[
 
)
t
n
i
S
(
g
o
l

−1

−2

20

21
logNH2

22

24
 (beam averaged) [cm−2 ]

23

25

Figure 10. The integrated ﬂux density versus the beam averaged
H2 column density for the 8144 BGPS sources not searched for
class I methanol masers by ?. Sources for which one or more of the
classiﬁcation models predicts the presence of a class I methanol
masers are represented with red dots, other sources are repre-
sented with black dots. The blue line shows the criteria developed
by ? to identify BGPS sources likely to have an associated class I
methanol maser.

to directly derived criteria (such as those of ?) is to
test them through future observations. There are cur-
rently approximately 400 diﬀerent class I methanol
maser sources which have been identiﬁed throughout
the Galaxy (see ?, and references therein), so a search
targeted towards the candidate BGPS sources we have
identiﬁed is likely to signiﬁcantly increase the number
of known sources.

5 Conclusions

In this paper we present three major ﬁndings regard-
ing the utilisation of diﬀerent classiﬁcation techniques
on diﬀerent size astronomical data sets. 1) For small
data sets parametric methods (such as LDA and logistic
regression perform better than random forests (a non-
parametric method). 2) For larger data sets, random
forests has the capability to out-perform the parametric
methods trialled here. 3) In almost all cases, transform-
ing the data to be closer to a normal distribution signiﬁ-
cantly increases the accuracy of LDA. In the case where
using transformed data slightly decreased the accuracy
of the model, the classiﬁcation results were very similar.
Since the process of transforming data is relatively easy,
this is a step that should be deﬁnitely employed if LDA
is utilised. This step has typically not been included

Comparison of classiﬁcation techniques for masers

17

A Classiﬁcation model predictions

Table A1 summarises the predictions for each of the classi-
ﬁcation models for class I methanol masers associated with
Bolocam sources.

when LDA has been applied to astronomical data used
in past studies.

Our results suggest that where there is very limited
training information parametric models which can only
predict based on simple combinations of the input vari-
ables are more accurate than non-parametric methods.
However, where there is more training data (such as
Data Sets 1 and 3) non-parametric models can per-
form as well (likely better in some circumstances) than
parametric techniques. Our results for Data Set 3 show
that random forests is comparable in accuracy to the
parametric methods, rather than exceeding them as ex-
pected (see ?).

Frequently in astrophysics relationships are sought
between two or three variables in the form of correla-
tions between them, such as the radio:far-infrared cor-
relation for galaxies, or colour-colour selection criteria
for Hii regions. In the past, this has often been because
of limited numbers of predictor variables being avail-
able for large samples of data, however, this is now less
of an issue. Mathematical classiﬁcation techniques such
as those utilised here potentially oﬀer signiﬁcant im-
provements over simple correlation relationships, but
the most appropriate technique to apply depends heav-
ily on the nature of the data available and the goal
of the investigation (e.g. detection prediction, physical
understanding of relationship between variables). Our
models determined which predictor variables were im-
portant in the classiﬁcation process, and for all three
Data Sets our results agreed with the previous studies
of ?, ?, and ? respectively.

For the speciﬁc goal of identifying millimetre dust
clumps which are more likely to have an associated class
I methanol maser, we ﬁnd that on the basis of cross-
validation tests and the predictions the models produce
on the training data, both the non-parametric method
of random forests and the parametric methods of logis-
tic regression and LDA are well suited for the task of
identifying likely targets for future searches. 242 sources
out of the 8144 in Data Set 3, were predicted by all four
of our techniques to have associated masers. The results
of future searches for class I methanol masers towards
BGPS sources will allow a direct test of each of the clas-
siﬁcation models and allow us to determine the validity
of these conclusions.

Acknowledgements

EMM undertook much of this work funded through a
University of Tasmania Dean’s Summer Research Schol-
arship. Shari Breen is the recipient of an Australian
Research Council DECRA Fellowship (project number
DE130101270). This research has made use of NASA’s
Astrophysics Data System Abstract Service.

PASA (2016)
doi:10.1017/pas.2016.xxx

18

Manning et al.

Bolocam

Catalogue #

BGPS
name

Random
Forests Regression LDA

Logistic

Normalised

LDA

4
5
7
8
18
20
22
24
27
32
35
38
39
41
43
47
48
56
57
61
62
72
79
81
83
84
87
89
91
96
99
102
103
106
109
112
115
116
123
124
126
127
130
135
141
148
149
168
170
171
173
175
186
190
193
196
199
203
206
207

G000.010+00.157
G000.016−00.017
G000.020+00.033
G000.020−00.051
G000.052+00.027
G000.054−00.209
G000.066−00.079
G000.070+00.175
G000.072+00.047
G000.094−00.109
G000.098+00.073
G000.104−00.005
G000.106−00.085
G000.110+00.001
G000.118+00.085
G000.120−00.513
G000.122−00.113
G000.140+00.021
G000.140−00.085
G000.156−00.091
G000.162−00.039
G000.184−00.003
G000.208−00.003
G000.212−00.517
G000.216−00.019
G000.216−00.045
G000.228−00.475
G000.234−00.089
G000.246−00.043
G000.254+00.013
G000.262+00.027
G000.274−00.085
G000.278−00.063
G000.282−00.481
G000.292−00.025
G000.296+00.043
G000.318−00.101
G000.320−00.201
G000.332−00.011
G000.332−00.075
G000.338+00.097
G000.340+00.053
G000.368−00.083
G000.378+00.041
G000.394−00.083
G000.412−00.503
G000.414+00.051
G000.472+00.019
G000.482−00.005
G000.492−00.111
G000.498+00.017
G000.500+00.187
G000.530+00.181
G000.546−00.003
G000.558−00.067
G000.572+00.023
G000.586−00.125
G000.590+00.007
G000.598−00.113
G000.606−00.033

0.43
0.84
0.81
0.92
0.78
0.82
0.86
0.55
0.60
0.55
0.55
0.90
0.94
0.84
0.51
0.21
0.58
0.87
0.83
0.85
0.56
0.62
0.71
0.93
0.79
0.75
0.61
0.54
0.72
0.96
0.98
0.60
0.58
0.64
0.58
0.86
0.72
0.86
0.61
0.76
0.52
0.71
0.59
0.96
0.59
0.44
0.86
0.94
0.95
0.58
0.78
0.29
0.89
0.92
0.53
0.68
0.56
0.66
0.59
0.86

PASA (2016)
doi:10.1017/pas.2016.xxx
Table A1 Bolocam Galactic Plane sources for which one or more of the mathematical classiﬁcation models predicted the presence of
an associated class I methanol maser (probability of a maser > 0.5). The maser probability for each model is listed, those which exceed
0.5 are in bold type. This list contains a total of 739 sources that were predicted to be masers by at least one of the four methods (242
of which were predicted by all methods), from a total of 8144 sources in version 1.0.1 of the Bolocam catalogue.

0.44
0.49
0.29
0.71
0.72
0.47
0.87
0.06
0.27
0.12
0.12
0.73
0.96
0.77
0.13
0.11
0.37
0.20
0.43
0.29
0.23
0.23
0.71
0.93
0.13
0.14
0.15
0.11
0.30
0.99
1.00
0.09
0.10
0.62
0.12
0.78
0.18
1.00
0.08
0.22
0.16
0.74
0.28
1.00
0.06
0.77
0.39
0.77
1.00
0.18
0.89
0.58
0.98
0.70
0.16
0.07
0.07
0.40
0.17
0.01

0.40
0.64
0.47
0.75
0.91
0.77
1.00
0.27
0.37
0.22
0.06
0.80
0.99
0.77
0.31
0.05
0.19
0.52
0.54
0.71
0.38
0.14
0.05
0.97
0.34
0.31
0.69
0.31
0.18
1.00
1.00
0.13
0.51
0.80
0.25
0.50
0.42
0.99
0.60
0.56
0.24
0.85
0.26
0.56
0.29
0.34
0.97
0.03
1.00
0.37
0.82
0.09
0.75
0.46
0.38
0.38
0.14
0.13
0.07
0.38

0.60
0.42
0.17
0.45
0.64
0.31
0.76
0.02
0.15
0.09
0.11
0.80
0.76
0.76
0.08
0.54
0.23
0.27
0.27
0.44
0.16
0.09
0.48
0.87
0.10
0.12
0.23
0.03
0.27
0.88
0.98
0.16
0.17
0.71
0.03
0.75
0.13
0.99
0.09
0.22
0.05
0.73
0.09
0.97
0.06
0.79
0.71
0.57
0.97
0.27
0.87
0.60
0.90
0.41
0.14
0.10
0.13
0.21
0.49
0.16

Comparison of classiﬁcation techniques for masers

19

Bolocam

Catalogue #

BGPS
name

Random
Forests Regression LDA

Logistic

Normalised

LDA

209
211
218
222
223
225
226
227
228
229
237
238
239
241
244
245
249
250
251
258
260
261
263
266
268
269
277
280
285
296
317
346
349
350
367
377
390
408
427
471
481
489
513
536
548
572
578
585
596
603
604
612
663
700
723
735
834
920
929
937

G000.608+00.001
G000.610−00.057
G000.630−00.095
G000.648+00.027
G000.656−00.045
G000.670−00.141
G000.674−00.097
G000.680−00.029
G000.684−00.169
G000.686−00.111
G000.738−00.051
G000.738−00.093
G000.738−00.157
G000.748+00.017
G000.760−00.069
G000.762+00.013
G000.772−00.109
G000.772−00.251
G000.776−00.187
G000.798−00.156
G000.802−00.098
G000.812+00.024
G000.826−00.212
G000.834−00.152
G000.836−00.200
G000.840+00.184
G000.862−00.054
G000.868−00.040
G000.886−00.036
G000.906−00.022
G000.950−00.080
G001.010−00.240
G001.020−00.122
G001.024+00.068
G001.092−00.030
G001.128−00.108
G001.150−00.126
G001.194−00.074
G001.234+00.056
G001.320−00.142
G001.338+00.096
G001.354+00.260
G001.406+00.328
G001.476+00.040
G001.518−00.194
G001.600+00.022
G001.610−00.172
G001.652−00.066
G001.676−00.130
G001.696−00.386
G001.698−00.366
G001.734−00.412
G002.144+00.006
G002.444+00.126
G002.534+00.198
G002.616+00.132
G003.094+00.164
G003.310−00.402
G003.350−00.080
G003.410+00.880

0.83
0.95
0.85
0.90
0.77
0.88
0.91
0.94
0.65
0.84
0.90
0.84
0.59
0.76
0.78
0.68
0.67
0.75
0.70
0.51
0.82
0.59
0.89
0.84
0.74
0.68
0.70
0.72
0.77
0.52
0.59
0.82
0.50
0.53
0.65
0.66
0.40
0.55
0.56
0.55
0.52
0.53
0.14
0.58
0.27
0.72
0.63
0.67
0.56
0.57
0.68
0.73
0.54
0.29
0.56
0.83
0.67
0.89
0.65
0.51

0.40
0.99
0.24
0.00
1.00
0.01
0.09
1.00
0.03
0.90
0.86
0.10
0.18
0.17
0.00
0.04
0.22
0.54
0.31
0.38
0.25
0.05
0.58
0.06
0.05
0.86
0.19
0.00
0.44
0.26
0.00
0.94
0.05
0.05
0.01
1.00
0.30
0.17
0.23
0.12
0.17
0.16
0.12
0.12
0.15
0.09
0.14
0.07
0.12
0.29
0.27
0.21
0.50
0.48
0.60
0.74
0.28
0.61
0.99
0.35

0.08
0.67
0.18
0.01
1.00
0.21
0.36
1.00
0.08
0.09
0.17
0.66
0.60
0.15
0.07
0.10
0.08
0.80
0.44
0.40
0.60
0.35
0.55
0.72
0.16
0.26
0.46
0.02
0.09
0.10
0.28
0.77
0.35
0.23
0.34
0.98
0.05
0.33
0.49
0.28
0.41
0.64
0.05
0.34
0.53
0.43
0.58
0.79
0.27
0.30
0.52
0.66
0.15
0.37
0.55
0.34
0.52
0.73
0.56
0.13

0.13
0.96
0.73
0.42
1.00
0.30
0.66
1.00
0.14
0.60
0.48
0.43
0.12
0.08
0.16
0.01
0.18
0.69
0.24
0.22
0.13
0.03
0.51
0.19
0.09
0.82
0.62
0.02
0.78
0.33
0.01
0.92
0.02
0.01
0.06
0.98
0.60
0.12
0.20
0.08
0.14
0.08
0.77
0.04
0.15
0.37
0.09
0.17
0.07
0.25
0.24
0.39
0.42
0.55
0.60
0.68
0.24
0.66
0.99
0.17

PASA (2016)
doi:10.1017/pas.2016.xxx

20

Manning et al.

Bolocam

Catalogue #

BGPS
name

Random
Forests Regression LDA

Logistic

Normalised

LDA

946
986
987
1018
1020
1039
1060
1114
1116
1129
1130
1135
1136
1138
1140
1141
1142
1175
1188
1216
1240
1250
1269
1281
1286
1305
1314
1316
1326
1337
1347
1354
1358
1359
1366
1377
1383
1399
1421
1435
1452
1454
1455
1456
1459
1462
1465
1474
1476
1480
1483
1495
1507
1518
1562
1566
1574
1584
1590
1655

PASA (2016)
doi:10.1017/pas.2016.xxx

G003.438−00.352
G003.910−00.002
G003.932−00.008
G004.418+00.124
G004.434+00.126
G004.681+00.277
G004.885−00.171
G005.621−00.081
G005.641+00.239
G005.833−00.511
G005.837−00.397
G005.883−00.357
G005.887−00.391
G005.897−00.319
G005.901−00.443
G005.903−00.429
G005.911−00.543
G006.191−00.359
G006.249−00.123
G006.553−00.097
G006.799−00.255
G006.919−00.225
G007.167+00.133
G007.269−00.529
G007.289−00.529
G007.475+00.061
G007.632−00.110
G007.636−00.194
G007.992−00.268
G008.141+00.224
G008.282+00.164
G008.352−00.318
G008.400−00.290
G008.407−00.350
G008.506−00.280
G008.670−00.356
G008.734−00.364
G008.874−00.494
G009.620+00.194
G009.986−00.030
G010.134−00.376
G010.150−00.408
G010.152−00.344
G010.166−00.360
G010.192−00.390
G010.204−00.348
G010.212−00.310
G010.286−00.120
G010.300−00.148
G010.324−00.162
G010.343−00.144
G010.446−00.018
G010.625−00.338
G010.681−00.028
G010.973−00.094
G010.989−00.084
G011.035+00.062
G011.083−00.536
G011.111−00.398
G011.904−00.140

0.95
0.61
0.64
0.43
0.91
0.67
0.06
0.36
0.96
0.75
0.28
0.54
0.98
0.79
0.99
0.99
0.70
0.97
0.69
0.93
0.99
0.87
0.22
0.59
0.14
0.85
0.48
0.61
0.96
0.75
0.55
0.10
0.69
0.71
0.19
0.95
0.60
0.56
0.95
0.50
0.57
0.60
0.73
0.83
0.59
0.71
0.69
0.83
0.98
0.98
0.78
0.82
0.56
0.23
0.26
0.51
0.22
0.34
0.67
0.85

1.00
0.33
0.11
0.51
0.95
0.55
0.30
0.66
0.99
0.39
0.25
0.51
1.00
0.70
1.00
1.00
0.31
1.00
0.45
0.98
1.00
0.64
0.33
0.64
0.29
0.96
0.68
0.13
0.96
1.00
0.10
0.49
0.70
0.54
0.14
1.00
0.34
0.14
1.00
0.65
0.41
0.32
1.00
0.99
0.60
0.75
0.20
0.91
1.00
1.00
0.99
0.80
0.96
0.55
0.27
0.25
0.30
0.45
0.80
0.88

0.98
0.10
0.09
0.13
0.59
0.15
0.04
0.29
0.96
0.72
0.53
0.23
1.00
0.37
0.99
1.00
0.18
0.93
0.13
0.87
0.93
0.61
0.14
0.33
0.13
0.81
0.11
0.09
0.76
1.00
0.11
0.35
0.44
0.60
0.07
1.00
0.34
0.24
0.99
0.13
0.19
0.16
0.62
0.99
0.08
0.16
0.04
1.00
1.00
0.99
0.73
0.22
0.70
0.27
0.29
0.26
0.26
0.55
0.65
0.63

0.99
0.24
0.10
0.89
0.89
0.49
0.58
0.86
0.95
0.62
0.22
0.17
1.00
0.50
0.99
0.99
0.24
0.96
0.28
0.95
0.98
0.64
0.64
0.51
0.58
0.83
0.77
0.19
0.91
0.99
0.05
0.62
0.64
0.64
0.66
1.00
0.24
0.11
1.00
0.42
0.84
0.24
0.98
0.93
0.41
0.62
0.44
0.80
1.00
0.88
0.94
0.63
0.96
0.70
0.59
0.19
0.52
0.46
0.75
0.74

Comparison of classiﬁcation techniques for masers

21

Bolocam

Catalogue #

BGPS
name

Random
Forests Regression LDA

Logistic

Normalised

LDA

1659
1676
1683
1684
1708
1710
1747
1758
1762
1771
1780
1792
1796
1801
1804
1805
1810
1813
1833
1869
1871
1876
1883
1894
1905
1954
1974
1984
1985
1995
1997
2007
2009
2011
2016
2019
2027
2050
2051
2054
2072
2081
2082
2101
2106
2136
2146
2147
2150
2151
2152
2153
2155
2156
2157
2159
2162
2165
2167
2168

PASA (2016)
doi:10.1017/pas.2016.xxx

G011.947−00.036
G012.113−00.128
G012.209−00.104
G012.215−00.118
G012.403−00.466
G012.419+00.506
G012.681−00.182
G012.721−00.216
G012.739−00.102
G012.773+00.334
G012.809−00.200
G012.853−00.226
G012.861−00.272
G012.879−00.288
G012.891−00.224
G012.895−00.282
G012.909−00.260
G012.917−00.334
G012.999−00.358
G013.211−00.142
G013.217+00.036
G013.245−00.084
G013.275−00.336
G013.333−00.038
G013.387+00.066
G013.874+00.281
G013.971−00.411
G014.012−00.175
G014.016−00.133
G014.089−00.557
G014.102+00.087
G014.181−00.529
G014.183−00.503
G014.194−00.193
G014.227−00.513
G014.244−00.071
G014.327−00.533
G014.466−00.089
G014.474−00.007
G014.492−00.139
G014.606+00.012
G014.633−00.574
G014.634+00.308
G014.736−00.102
G014.760−00.180
G014.918+00.068
G014.973−00.746
G014.983−00.692
G014.991−00.738
G015.004+00.010
G015.013−00.674
G015.021−00.620
G015.031−00.670
G015.031−00.746
G015.045−00.650
G015.057−00.624
G015.079−00.604
G015.093−00.676
G015.095−00.710
G015.097−00.734

0.60
0.65
0.96
0.85
0.38
0.96
0.87
0.91
0.15
0.91
0.93
0.79
0.57
0.53
0.65
0.62
0.77
0.66
0.67
0.98
0.76
0.93
0.56
0.51
0.28
0.97
0.28
0.40
0.57
0.52
0.79
0.66
0.74
0.62
0.93
0.78
0.55
0.76
0.60
0.80
0.51
0.90
0.67
0.02
0.13
0.36
0.56
0.85
0.89
0.71
0.79
0.85
0.82
0.59
0.89
0.83
0.69
0.83
0.99
0.84

0.90
0.22
1.00
0.89
0.52
1.00
0.99
0.91
0.25
0.81
1.00
0.97
0.88
0.37
0.28
0.39
1.00
0.39
0.84
0.98
0.23
0.94
0.12
0.28
0.54
1.00
0.30
0.55
0.52
0.41
0.93
0.59
0.21
0.96
1.00
0.88
0.31
0.48
0.27
0.77
0.89
1.00
0.72
0.05
0.27
0.51
0.18
1.00
0.91
0.24
1.00
0.99
1.00
0.68
0.94
0.91
0.52
0.36
0.99
0.65

0.34
0.08
0.76
0.01
0.41
0.99
1.00
0.61
0.51
0.79
1.00
0.18
0.21
0.10
0.16
0.09
1.00
0.29
0.54
0.95
0.84
0.59
0.36
0.25
0.36
1.00
0.05
0.17
0.64
0.15
0.45
0.14
0.24
0.70
0.88
0.67
0.11
0.27
0.44
0.51
0.75
0.98
0.46
0.10
0.06
0.23
0.09
0.96
0.73
0.07
1.00
0.24
1.00
0.25
0.00
0.44
0.43
0.36
0.22
0.73

0.96
0.17
0.99
0.53
0.48
0.98
0.96
0.85
0.30
0.67
1.00
0.91
0.61
0.40
0.61
0.37
0.99
0.44
0.77
0.82
0.26
0.88
0.04
0.43
0.48
0.98
0.62
0.36
0.60
0.85
0.85
0.57
0.40
0.91
0.96
0.81
0.11
0.43
0.14
0.67
0.95
0.95
0.68
0.59
0.56
0.49
0.89
0.98
0.70
0.15
1.00
0.77
1.00
0.93
0.97
0.56
0.74
0.84
0.79
0.70

22

Manning et al.

Bolocam

Catalogue #

BGPS
name

Random
Forests Regression LDA

Logistic

Normalised

LDA

2169
2170
2171
2181
2184
2189
2190
2191
2193
2195
2198
2224
2234
2248
2274
2275
2311
2312
2320
2325
2343
2351
2365
2375
2377
2386
2387
2388
2396
2424
2430
2431
2442
2455
2456
2510
2561
2573
2601
2602
2603
2612
2619
2673
2718
2720
2722
2741
2784
2788
2819
2854
2860
2864
2907
2971
3016
3018
3026
3027

PASA (2016)
doi:10.1017/pas.2016.xxx

G015.099−00.558
G015.099−00.600
G015.101−00.656
G015.137−00.674
G015.153−00.660
G015.182−00.158
G015.195−00.628
G015.201−00.442
G015.205−00.626
G015.234−00.612
G015.250−00.602
G015.557−00.463
G015.665−00.499
G016.144+00.009
G016.362−00.355
G016.364−00.209
G016.821−00.344
G016.832+00.080
G016.926+00.298
G016.946−00.074
G017.366−00.034
G017.638+00.154
G018.091−00.302
G018.150−00.286
G018.173−00.298
G018.260−00.246
G018.277−00.262
G018.302−00.390
G018.462−00.002
G018.608−00.074
G018.655−00.060
G018.666+00.032
G018.738−00.225
G018.830−00.483
G018.834−00.299
G019.077−00.287
G019.364−00.031
G019.474+00.171
G019.609−00.233
G019.612−00.137
G019.614−00.257
G019.702−00.263
G019.756−00.129
G020.366−00.011
G020.734−00.059
G020.750−00.091
G020.763−00.059
G020.984+00.097
G021.385−00.253
G021.423−00.541
G021.878+00.007
G022.353+00.067
G022.379+00.447
G022.417+00.315
G022.725−00.274
G023.012−00.410
G023.202−00.000
G023.208−00.378
G023.268+00.078
G023.272−00.258

0.55
0.82
0.90
0.85
0.64
0.26
0.94
0.56
0.82
0.74
0.68
0.23
0.64
0.63
0.20
0.86
0.66
0.67
0.22
0.36
0.26
0.95
0.71
0.84
0.54
0.55
0.52
0.97
0.85
0.19
0.78
0.26
0.90
0.84
0.33
0.86
0.91
0.94
0.98
0.38
0.63
0.72
0.16
0.07
0.82
0.80
0.53
0.30
0.60
0.76
0.67
0.65
0.32
0.57
0.53
0.93
0.86
0.94
0.60
0.87

0.43
0.43
0.94
0.66
0.67
0.47
1.00
0.54
0.14
0.39
0.48
0.55
0.62
0.17
0.03
0.92
0.75
0.40
0.05
0.78
0.07
1.00
0.23
0.94
0.62
0.36
0.76
1.00
0.99
0.55
0.76
0.30
0.96
0.38
0.38
1.00
0.93
1.00
1.00
0.70
0.62
0.65
0.54
0.39
0.93
0.81
0.46
0.23
0.88
0.71
0.72
0.80
0.50
0.13
0.27
0.99
0.77
1.00
0.88
0.81

0.30
0.42
0.64
0.73
0.17
0.33
0.71
0.50
0.89
0.37
0.05
0.23
0.34
0.11
0.05
0.97
0.46
0.13
0.06
0.11
0.05
0.98
0.60
0.43
0.45
0.82
0.14
0.99
0.35
0.21
0.44
0.56
0.83
0.96
0.14
0.93
0.91
0.67
0.97
0.34
0.45
0.28
0.28
0.16
0.29
0.54
0.10
0.61
0.28
0.45
0.41
0.39
0.19
0.61
0.10
0.98
0.60
0.70
0.32
0.81

0.33
0.30
0.80
0.35
0.30
0.67
0.96
0.55
0.57
0.88
0.67
0.67
0.59
0.15
0.51
0.84
0.62
0.31
0.53
0.86
0.59
0.98
0.17
0.70
0.57
0.48
0.82
0.98
0.91
0.70
0.72
0.57
0.92
0.56
0.58
0.97
0.89
0.99
1.00
0.76
0.56
0.61
0.57
0.59
0.90
0.86
0.31
0.16
0.89
0.71
0.62
0.87
0.45
0.14
0.15
0.94
0.80
1.00
0.75
0.77

Comparison of classiﬁcation techniques for masers

23

Bolocam

Catalogue #

BGPS
name

Random
Forests Regression LDA

Logistic

Normalised

LDA

3029
3039
3053
3065
3077
3078
3086
3116
3141
3155
3183
3186
3189
3200
3205
3212
3307
3313
3320
3322
3326
3329
3337
3343
3357
3409
3440
3461
3474
3497
3502
3507
3511
3519
3576
3582
3588
3591
3594
3645
3679
3685
3690
3766
3774
3782
3807
3852
3864
3899
3913
3921
3925
3929
3936
3939
3955
3998
4006
4014

PASA (2016)
doi:10.1017/pas.2016.xxx

G023.274−00.212
G023.321−00.298
G023.368−00.290
G023.414−00.228
G023.456+00.064
G023.456−00.018
G023.484+00.096
G023.571+00.014
G023.658−00.142
G023.711+00.170
G023.870−00.124
G023.888+00.060
G023.902+00.064
G023.955+00.150
G023.992−00.092
G024.018+00.048
G024.402−00.190
G024.414+00.102
G024.439+00.228
G024.443−00.228
G024.461+00.196
G024.472+00.490
G024.494−00.040
G024.510−00.220
G024.545−00.248
G024.757+00.091
G024.943+00.075
G025.155−00.275
G025.227+00.289
G025.353−00.193
G025.384−00.181
G025.400−00.141
G025.411+00.103
G025.456−00.211
G025.713+00.045
G025.737+00.213
G025.797+00.245
G025.805−00.041
G025.827−00.179
G026.209+00.025
G026.510+00.281
G026.545−00.293
G026.562−00.303
G027.187−00.083
G027.283+00.149
G027.367−00.167
G027.562+00.080
G027.903−00.016
G027.977+00.076
G028.149+00.148
G028.201−00.052
G028.241+00.058
G028.285−00.364
G028.305−00.388
G028.337+00.116
G028.344+00.058
G028.397+00.078
G028.565−00.236
G028.609+00.016
G028.651+00.026

0.52
0.50
0.77
0.89
0.69
0.24
0.75
0.81
0.53
0.73
0.74
0.15
0.27
0.90
0.35
0.56
0.53
0.57
0.61
0.95
0.73
0.68
0.83
0.84
0.53
0.69
0.52
0.61
0.53
0.63
0.98
0.92
0.61
0.54
0.70
0.54
0.57
0.56
0.93
0.24
0.99
0.58
0.80
0.40
0.51
0.94
0.95
0.15
0.69
0.12
0.84
0.64
0.92
0.58
0.43
0.52
0.99
0.53
0.92
0.90

0.29
0.44
0.83
0.03
0.89
0.05
0.57
0.86
0.11
0.97
0.50
0.14
0.66
0.96
0.15
0.32
0.29
0.31
0.13
0.99
0.60
0.38
1.00
0.66
0.57
0.86
0.75
0.34
0.83
0.35
1.00
1.00
0.39
0.60
0.75
0.24
0.52
0.88
1.00
0.09
1.00
0.41
0.73
0.90
0.41
1.00
0.69
0.44
0.35
0.08
1.00
0.09
0.97
0.29
0.67
0.34
1.00
0.62
0.92
0.96

0.40
0.58
0.26
0.91
0.33
0.06
0.36
0.47
0.13
0.49
0.84
0.14
0.11
0.64
0.70
0.26
0.15
0.63
0.36
0.87
0.19
0.97
0.98
0.60
0.56
0.25
0.05
0.49
0.30
0.61
0.96
0.98
0.17
0.73
0.37
0.32
0.35
0.32
0.92
0.06
0.91
0.13
0.47
0.25
0.52
0.99
0.84
0.36
0.52
0.03
0.99
0.30
0.91
0.57
0.29
0.18
0.97
0.66
0.46
0.39

0.32
0.55
0.68
0.31
0.85
0.73
0.46
0.73
0.12
0.93
0.45
0.65
0.72
0.83
0.31
0.30
0.25
0.47
0.05
0.95
0.38
0.66
0.99
0.60
0.42
0.74
0.92
0.39
0.71
0.47
0.94
0.95
0.24
0.52
0.72
0.11
0.51
0.87
1.00
0.58
1.00
0.47
0.85
0.93
0.52
1.00
0.64
0.61
0.46
0.62
1.00
0.03
0.90
0.42
0.72
0.31
0.97
0.61
0.85
0.83

24

Manning et al.

Bolocam

Catalogue #

BGPS
name

Random
Forests Regression LDA

Logistic

Normalised

LDA

4048
4049
4055
4061
4063
4121
4152
4154
4236
4239
4243
4252
4254
4258
4259
4261
4266
4272
4281
4384
4449
4468
4488
4499
4500
4509
4518
4526
4527
4530
4533
4537
4541
4546
4547
4553
4555
4560
4566
4573
4582
4583
4586
4594
4598
4633
4636
4654
4662
4695
4722
4736
4760
4764
4911
4916
4926
4933
4975
5041

PASA (2016)
doi:10.1017/pas.2016.xxx

G028.811+00.169
G028.817+00.363
G028.831−00.255
G028.863+00.065
G028.881−00.025
G029.225+00.023
G029.397−00.095
G029.435−00.177
G029.855−00.056
G029.863−00.048
G029.888−00.000
G029.913−00.046
G029.920−00.016
G029.933−00.064
G029.937−00.790
G029.943+00.072
G029.955−00.018
G029.975−00.050
G030.004−00.270
G030.387−00.106
G030.536+00.021
G030.590−00.043
G030.652−00.203
G030.688−00.261
G030.688−00.039
G030.704−00.067
G030.719−00.081
G030.746−00.059
G030.746+00.001
G030.756−00.051
G030.760+00.207
G030.768−00.039
G030.776−00.215
G030.788−00.025
G030.788+00.205
G030.802+00.115
G030.808−00.027
G030.820−00.055
G030.830+00.135
G030.850−00.081
G030.868+00.115
G030.870−00.155
G030.878+00.059
G030.896+00.139
G030.900+00.163
G030.974−00.139
G030.980+00.215
G031.028+00.265
G031.050+00.357
G031.160+00.049
G031.246−00.111
G031.282+00.063
G031.398−00.257
G031.414+00.307
G032.021+00.063
G032.044+00.059
G032.119+00.091
G032.152+00.135
G032.474+00.205
G032.744−00.075

0.63
0.34
0.86
0.73
0.64
0.54
0.58
0.60
0.60
0.58
0.24
0.87
0.54
0.90
0.29
0.14
0.97
0.93
0.85
0.84
0.39
0.81
0.66
0.39
0.87
0.91
0.94
0.89
0.72
0.90
0.78
0.96
0.60
0.81
0.96
0.32
0.98
0.94
0.51
0.74
0.94
0.34
0.58
0.56
0.49
0.67
0.24
0.53
0.58
0.55
0.68
0.91
0.87
0.88
0.81
0.96
0.31
0.90
0.61
0.90

0.26
0.81
1.00
0.98
0.28
0.36
0.56
0.12
0.36
0.60
0.06
0.70
0.34
0.79
0.08
0.06
1.00
0.93
0.82
0.71
0.86
1.00
0.81
0.60
0.63
1.00
1.00
0.99
0.09
1.00
0.58
0.91
0.50
0.94
0.99
0.10
0.97
1.00
0.05
0.76
1.00
0.29
0.20
0.43
0.55
0.32
0.44
0.06
0.35
0.42
0.98
1.00
1.00
1.00
0.40
1.00
0.53
0.99
0.40
0.99

0.61
0.18
0.66
0.60
0.12
0.10
0.20
0.12
0.41
0.32
0.52
0.90
0.09
0.55
0.07
0.04
0.97
0.72
0.89
0.50
0.32
0.91
0.74
0.46
0.93
1.00
0.91
0.96
0.53
0.97
0.48
0.09
0.52
0.99
0.33
0.04
0.75
1.00
0.21
0.34
0.50
0.09
0.40
0.30
0.21
0.43
0.23
0.11
0.38
0.32
0.61
0.94
0.67
1.00
0.58
0.94
0.39
0.91
0.53
0.74

0.40
0.87
0.99
0.94
0.19
0.15
0.53
0.05
0.50
0.70
0.04
0.55
0.19
0.46
0.53
0.62
1.00
0.75
0.80
0.73
0.89
0.99
0.78
0.83
0.78
1.00
0.99
0.90
0.11
0.96
0.56
0.85
0.60
0.83
0.93
0.79
0.87
1.00
0.02
0.56
0.97
0.88
0.10
0.50
0.60
0.28
0.68
0.04
0.28
0.46
0.99
1.00
0.99
1.00
0.48
0.91
0.71
0.97
0.38
0.91

Comparison of classiﬁcation techniques for masers

25

Bolocam

Catalogue #

BGPS
name

Random
Forests Regression LDA

Logistic

Normalised

LDA

5053
5057
5120
5171
5229
5263
5278
5306
5321
5340
5346
5384
5385
5433
5467
5530
5538
5627
5653
5654
5657
5695
5700
5756
5849
5850
5853
5864
5874
5879
5931
5956
5972
5980
6006
6024
6029
6082
6117
6118
6119
6120
6122
6126
6142
6162
6165
6172
6176
6177
6202
6254
6256
6286
6287
6291
6292
6299
6300
6310

PASA (2016)
doi:10.1017/pas.2016.xxx

G032.798+00.193
G032.820−00.329
G033.133−00.091
G033.414−00.002
G033.652−00.025
G033.810−00.187
G033.914+00.107
G034.091+00.015
G034.191−00.594
G034.258+00.154
G034.283+00.184
G034.454+00.006
G034.457+00.248
G034.712−00.596
G034.820+00.350
G035.026+00.350
G035.045−00.478
G035.466+00.138
G035.576+00.066
G035.576−00.032
G035.579+00.006
G035.750+00.152
G035.794−00.176
G036.405+00.020
G037.547−00.112
G037.555+00.200
G037.599+00.426
G037.737−00.112
G037.820+00.412
G037.875−00.400
G038.694−00.454
G038.920−00.352
G039.256−00.059
G039.389−00.143
G039.883−00.347
G040.283−00.221
G040.622−00.139
G041.741+00.095
G043.164−00.031
G043.169+00.009
G043.177−00.521
G043.237−00.047
G043.307−00.213
G043.795−00.125
G044.307+00.041
G045.069+00.133
G045.121+00.133
G045.453+00.061
G045.465+00.047
G045.477+00.135
G045.805−00.355
G048.579+00.056
G048.603+00.024
G048.895−00.410
G048.914−00.280
G048.989−00.300
G048.997−00.312
G049.070−00.350
G049.075−00.276
G049.170−00.208

0.93
0.34
0.60
0.58
0.55
0.20
0.82
0.83
0.63
0.92
0.33
0.49
0.57
0.76
0.92
0.92
0.58
0.86
0.84
0.92
0.84
0.81
0.64
0.36
0.66
0.85
0.64
0.51
0.83
0.99
0.21
0.81
0.60
0.44
0.18
0.97
0.70
0.62
0.96
0.91
0.98
0.89
0.73
0.98
0.81
0.99
0.98
0.95
0.97
0.89
0.57
0.66
0.74
0.53
0.71
0.86
0.73
0.51
0.27
0.60

1.00
0.62
0.99
0.24
0.18
0.49
1.00
0.47
0.41
1.00
0.55
0.55
0.49
0.56
0.94
1.00
0.05
0.98
0.35
1.00
0.50
0.58
0.85
0.46
0.83
0.93
0.31
0.81
0.90
1.00
0.22
0.93
0.12
0.41
0.52
1.00
0.83
0.23
1.00
1.00
0.99
0.99
0.97
1.00
0.64
1.00
1.00
1.00
1.00
0.93
0.25
0.37
0.92
0.20
0.04
1.00
0.32
0.21
0.09
0.35

0.99
0.30
0.85
0.32
0.46
0.33
1.00
0.72
0.11
1.00
0.05
0.39
0.52
0.73
0.78
0.54
0.25
0.91
0.64
0.67
0.69
0.67
0.45
0.27
0.54
0.50
0.08
0.49
0.25
0.97
0.19
0.81
0.16
0.19
0.33
0.74
0.56
0.11
1.00
1.00
0.92
0.90
0.21
0.64
0.42
0.79
1.00
0.92
0.58
0.90
0.19
0.65
0.54
0.09
0.90
0.99
0.07
0.17
0.15
0.68

1.00
0.80
0.99
0.19
0.22
0.51
0.99
0.45
0.37
1.00
0.20
0.90
0.45
0.40
0.87
0.97
0.02
0.92
0.35
0.99
0.36
0.62
0.79
0.81
0.83
0.90
0.21
0.88
0.75
0.98
0.53
0.92
0.06
0.59
0.62
0.99
0.82
0.17
1.00
1.00
0.92
0.88
0.97
0.97
0.53
0.99
0.99
0.98
0.94
0.84
0.26
0.36
0.87
0.09
0.51
0.97
0.92
0.11
0.65
0.43

26

Manning et al.

Bolocam

Catalogue #

BGPS
name

Random
Forests Regression LDA

Logistic

Normalised

LDA

6312
6313
6321
6334
6336
6337
6339
6340
6344
6362
6363
6365
6371
6389
6402
6406
6410
6425
6446
6448
6452
6467
6486
6495
6497
6502
6506
6508
6521
6523
6528
6529
6530
6547
6550
6555
6556
6562
6569
6588
6599
6602
6604
6631
6652
6657
6669
6683
6685
6686
6687
6690
6698
6703
6704
6712
6718
6721
6730
6741

PASA (2016)
doi:10.1017/pas.2016.xxx

G049.192−00.336
G049.210−00.342
G049.264+00.312
G049.367−00.302
G049.371−00.350
G049.375−00.262
G049.389−00.320
G049.390−00.310
G049.402−00.214
G049.489−00.370
G049.489−00.386
G049.529−00.346
G049.561−00.276
G050.283−00.390
G051.375−00.011
G052.752+00.336
G053.036+00.112
G053.259+00.040
G053.957+00.032
G054.108−00.049
G054.120−00.075
G056.250−00.160
G059.786+00.067
G060.887−00.129
G061.475+00.090
G063.115+00.340
G071.149+00.402
G072.954−00.028
G075.757+00.339
G075.784+00.341
G075.835+00.399
G075.841+00.367
G075.843+00.359
G076.156−00.287
G076.186+00.095
G076.358−00.601
G076.382−00.623
G077.475−01.083
G077.820−01.313
G077.978+00.577
G078.034+00.617
G078.106−00.317
G078.114−00.637
G078.379+01.017
G078.888+00.709
G078.978+00.351
G079.132−00.369
G079.289+01.301
G079.296+00.283
G079.308+01.307
G079.313+00.279
G079.335+00.341
G079.483−00.719
G079.563−00.767
G079.643+00.473
G079.879+01.179
G079.981+00.811
G079.986+00.839
G080.364+00.445
G080.635+00.686

0.93
0.89
0.63
0.87
0.70
0.87
0.92
0.88
0.67
0.77
0.95
0.63
0.78
0.18
0.52
0.62
0.63
0.43
0.55
0.55
0.51
0.25
0.82
0.90
0.90
0.59
0.75
0.57
0.90
0.95
0.84
0.85
0.80
0.84
0.88
0.84
0.92
0.57
0.55
0.26
0.58
0.68
0.89
0.41
0.95
0.83
0.87
0.50
0.68
0.82
0.84
0.51
0.55
0.94
0.62
0.93
0.45
0.60
0.87
0.73

0.88
0.97
0.09
1.00
0.25
0.84
0.87
0.97
0.14
1.00
1.00
0.33
0.59
0.50
0.25
0.59
0.71
0.32
0.38
0.10
0.14
0.03
1.00
1.00
1.00
0.61
0.36
0.14
1.00
1.00
1.00
0.74
0.66
0.59
0.53
0.69
1.00
0.45
0.43
0.03
0.23
0.69
0.57
0.50
1.00
0.95
0.95
0.36
0.48
0.55
0.72
0.35
0.49
1.00
0.95
0.92
0.52
0.13
0.37
0.55

0.74
1.00
0.10
1.00
0.52
0.66
0.86
0.53
0.34
0.96
1.00
0.37
0.61
0.19
0.63
0.09
0.18
0.11
0.13
0.26
0.08
0.11
0.96
1.00
1.00
0.29
0.13
0.14
1.00
1.00
1.00
0.69
0.61
0.72
0.78
0.68
1.00
0.05
0.14
0.04
0.06
0.66
0.71
0.54
0.99
0.99
0.87
0.45
0.39
0.28
0.52
0.60
0.18
0.54
0.12
0.93
0.32
0.48
0.65
0.64

0.94
0.92
0.07
0.98
0.20
0.55
0.97
0.87
0.10
1.00
1.00
0.18
0.43
0.61
0.35
0.35
0.79
0.57
0.33
0.05
0.54
0.56
0.98
0.93
1.00
0.49
0.29
0.10
0.99
0.99
0.98
0.77
0.96
0.67
0.46
0.60
1.00
0.22
0.39
0.61
0.18
0.73
0.42
0.40
0.99
0.84
0.87
0.35
0.63
0.38
0.67
0.57
0.42
0.95
0.71
0.84
0.50
0.07
0.27
0.52

Comparison of classiﬁcation techniques for masers

27

Bolocam

Catalogue #

BGPS
name

Random
Forests Regression LDA

Logistic

Normalised

LDA

6747
6753
6754
6762
6765
6788
6796
6808
6815
6820
6839
6840
6844
6859
6863
6872
6901
6909
6920
6926
6934
6941
6947
6955
7069
7097
7098
7099
7101
7104
7111
7121
7126
7140
7146
7149
7150
7151
7170
7213
7232
7235
7243
7244
7247
7248
7252
7257
7260
7305
7322
7331
7351
7352
7361
7364
7367
7374
7380
7392

PASA (2016)
doi:10.1017/pas.2016.xxx

G080.829+00.568
G080.863+00.384
G080.864+00.422
G080.941−00.126
G080.954−00.154
G081.117−00.140
G081.174−00.100
G081.260+00.984
G081.302+01.052
G081.344+00.760
G081.451+00.470
G081.457+00.018
G081.477+00.022
G081.542+00.986
G081.549+00.096
G081.582+00.104
G081.680+00.540
G081.721+00.572
G081.753+00.593
G081.765+00.641
G081.783+00.621
G081.831+00.853
G081.844+00.881
G081.875+00.783
G084.548+00.104
G084.774−01.184
G084.784−01.104
G084.805−01.112
G084.829−01.092
G084.844−01.084
G084.951−00.692
G085.042−00.144
G085.073−00.140
G085.412+00.002
G089.635+00.171
G098.978+03.960
G099.115+03.926
G099.981+04.168
G110.113+00.050
G111.284−00.664
G111.447+00.798
G111.484+00.746
G111.522+00.800
G111.528+00.818
G111.537+00.756
G111.545+00.776
G111.573+00.750
G111.597+00.806
G111.615+00.374
G111.787+00.586
G111.882+00.992
G111.945+00.808
G133.694+01.215
G133.715+01.217
G133.736+01.271
G133.748+01.197
G133.784+01.421
G133.890+01.137
G133.949+01.063
G134.203+00.753

0.74
0.79
0.79
0.69
0.61
0.72
0.93
0.33
0.96
0.84
0.57
0.61
0.71
0.33
0.64
0.68
0.93
0.97
0.86
0.23
0.68
0.32
0.83
0.84
0.36
0.55
0.37
0.84
0.73
0.66
0.37
0.72
0.18
0.53
0.50
0.61
0.62
0.98
0.48
0.82
0.49
0.62
0.91
0.79
0.95
0.95
0.68
0.57
0.84
0.52
0.84
0.53
0.98
0.93
0.64
0.98
0.70
0.26
0.87
0.79

0.69
0.96
0.98
0.61
0.45
0.28
0.99
0.06
1.00
1.00
0.10
0.29
0.09
0.70
0.68
0.80
1.00
1.00
1.00
0.55
0.37
0.53
0.30
1.00
0.67
0.28
0.14
0.73
0.06
0.22
0.55
0.21
0.14
0.78
0.08
0.35
0.53
1.00
0.20
0.14
0.05
0.18
0.81
0.57
1.00
1.00
0.96
0.08
0.98
0.12
0.56
0.18
1.00
1.00
0.25
1.00
0.05
0.46
1.00
0.17

0.55
0.69
0.90
0.75
0.19
0.61
0.74
0.04
0.98
0.91
0.33
0.32
0.77
0.17
0.48
0.35
1.00
1.00
1.00
0.08
0.12
0.15
0.63
1.00
0.20
0.09
0.03
0.19
0.45
0.26
0.17
0.78
0.59
0.41
0.14
0.17
0.22
0.97
0.68
0.81
0.54
0.80
0.40
0.51
0.99
1.00
0.84
0.66
0.94
0.31
0.73
0.38
1.00
1.00
0.35
0.72
0.49
0.10
1.00
0.53

0.61
0.94
0.97
0.72
0.34
0.18
0.99
0.75
0.99
0.95
0.06
0.57
0.33
0.71
0.60
0.71
1.00
1.00
0.98
0.20
0.85
0.66
0.19
1.00
0.68
0.17
0.80
0.51
0.29
0.42
0.80
0.28
0.22
0.70
0.06
0.20
0.52
0.93
0.43
0.25
0.03
0.33
0.57
0.56
1.00
1.00
0.98
0.08
0.96
0.15
0.56
0.06
1.00
1.00
0.13
0.92
0.03
0.82
1.00
0.36

28

Manning et al.

Bolocam

Catalogue #

BGPS
name

Random
Forests Regression LDA

Logistic

Normalised

LDA

7394
7396
7456
7459
7460
7461
7465
7466
7474
7481
7482
7483
7486
7492
7501
7502
7531
7536
7538
7540
7545
7546
7549
7558
7559
7560
7571
7577
7591
7603
7604
7605
7621
7628
7632
7635
7645
7646
7650
7651
7674
7677
7697
7711
7714
7716
7721
7725
7728
7733
7741
7745
7747
7748
7749
7751
7758
7759
7760
7761

PASA (2016)
doi:10.1017/pas.2016.xxx

G134.211+00.729
G134.218+00.787
G138.295+01.556
G138.503+01.646
G188.792+01.027
G188.948+00.883
G189.030+00.781
G189.032+00.793
G189.776+00.343
G189.804+00.355
G189.810+00.369
G189.831+00.343
G189.864+00.499
G189.951+00.331
G192.581−00.043
G192.596−00.051
G349.836−00.528
G349.978−00.560
G349.988−00.558
G350.016+00.432
G350.110+00.090
G350.120+00.060
G350.177+00.014
G350.298+00.122
G350.329+00.100
G350.341+00.138
G350.521−00.350
G350.689−00.492
G350.783−00.028
G350.975+00.546
G350.978−00.540
G351.040−00.338
G351.465−00.458
G351.555+00.206
G351.581−00.352
G351.614+00.164
G351.775−00.538
G351.785−00.514
G351.799−00.488
G351.802−00.448
G352.098+00.162
G352.112+00.178
G352.317−00.444
G352.519−00.154
G352.584−00.184
G352.608−00.192
G352.684−00.118
G352.858−00.202
G352.876−00.516
G353.019+00.504
G353.067+00.508
G353.069+00.452
G353.079+00.422
G353.091+00.446
G353.117+00.366
G353.216−00.246
G353.316−00.256
G353.334−00.294
G353.343−00.288
G353.343−00.290

0.67
0.73
0.91
0.53
0.75
0.81
0.96
0.91
0.91
0.82
0.55
0.60
0.60
0.61
0.98
0.78
0.79
0.38
0.22
0.78
0.98
0.90
0.59
0.72
0.88
0.59
0.39
0.94
0.68
0.32
0.68
0.96
0.72
0.92
1.00
0.95
0.98
0.98
0.72
0.96
0.70
0.34
0.97
0.73
0.64
0.59
0.56
0.98
0.57
0.56
0.74
0.84
0.66
0.52
0.67
0.53
0.64
0.53
0.58
0.70

0.04
0.26
0.96
0.62
0.56
1.00
1.00
0.47
1.00
0.71
0.17
0.18
0.24
0.14
1.00
1.00
0.63
0.05
0.64
1.00
1.00
0.99
0.58
0.35
0.99
0.42
0.55
0.96
0.96
0.35
0.31
1.00
0.30
1.00
1.00
1.00
1.00
1.00
0.77
0.97
0.85
0.49
1.00
0.95
0.69
0.19
0.66
1.00
0.34
0.50
0.76
0.76
0.45
0.32
0.40
0.85
0.13
0.16
0.49
0.31

0.16
0.30
0.82
0.38
0.79
1.00
0.99
0.72
0.85
0.37
0.19
0.36
0.37
0.25
0.99
0.93
0.68
0.09
0.08
0.24
1.00
0.69
0.66
0.15
0.47
0.14
0.37
0.77
0.45
0.12
0.10
0.95
0.83
0.99
1.00
0.98
1.00
0.85
0.24
0.85
0.26
0.16
0.95
0.34
0.32
0.17
0.14
0.97
0.05
0.25
0.19
0.17
0.07
0.15
0.71
0.18
0.61
0.10
0.26
0.32

0.26
0.33
0.85
0.60
0.52
0.99
0.94
0.66
0.98
0.58
0.26
0.16
0.20
0.09
0.98
0.98
0.51
0.52
0.31
0.96
0.97
0.84
0.56
0.26
0.85
0.27
0.51
0.81
0.87
0.71
0.30
0.97
0.19
0.99
1.00
0.89
1.00
0.88
0.65
0.87
0.78
0.52
0.92
0.83
0.60
0.13
0.39
0.95
0.18
0.53
0.52
0.73
0.53
0.09
0.42
0.81
0.25
0.31
0.65
0.43

Comparison of classiﬁcation techniques for masers

29

Bolocam

Catalogue #

BGPS
name

Random
Forests Regression LDA

Logistic

Normalised

LDA

7763
7764
7765
7767
7770
7771
7772
7779
7791
7794
7806
7811
7820
7832
7834
7836
7837
7839
7840
7843
7849
7855
7859
7860
7874
7877
7881
7897
7901
7906
7909
7917
7920
7931
7950
7961
7963
7976
7980
7983
8019
8131
8193
8198
8203
8207
8212
8217
8220
8222
8226
8230
8243
8245
8247
8248
8252
8258
8261
8263

PASA (2016)
doi:10.1017/pas.2016.xxx

G353.362−00.088
G353.365−00.166
G353.367−00.336
G353.384−00.336
G353.400−00.070
G353.412−00.360
G353.432−00.088
G353.548−00.016
G353.834+00.268
G353.978+00.260
G354.208−00.036
G354.343+00.474
G354.422+00.032
G354.600+00.474
G354.617+00.472
G354.662+00.484
G354.672+00.242
G354.711+00.292
G354.725+00.302
G354.769+00.326
G354.826+00.352
G354.946−00.540
G355.129−00.300
G355.186−00.418
G355.268−00.270
G355.346+00.148
G355.413+00.102
G355.742+00.132
G355.828−00.500
G356.007−00.424
G356.304−00.206
G356.430+00.104
G356.482+00.190
G356.662−00.264
G357.557−00.322
G357.968−00.164
G357.997−00.154
G358.389−00.484
G358.461−00.392
G358.513−00.374
G358.725−00.130
G359.141+00.028
G359.372+00.275
G359.384−00.021
G359.418+00.089
G359.424−00.171
G359.444−00.105
G359.470−00.037
G359.475+00.009
G359.480−00.151
G359.490−00.035
G359.500−00.141
G359.557−00.095
G359.566−00.161
G359.576+00.001
G359.576−00.209
G359.602−00.221
G359.617−00.243
G359.636−00.131
G359.639+00.017

0.50
0.81
0.72
0.66
0.89
0.91
0.74
0.74
0.63
0.54
0.35
0.53
0.71
0.92
1.00
0.72
0.61
0.85
0.94
0.57
0.47
0.68
0.61
0.99
0.89
0.90
0.56
0.61
0.22
0.73
0.21
0.52
0.55
0.84
0.98
0.98
0.46
0.98
0.97
0.57
0.50
0.43
0.56
0.19
0.40
0.48
0.70
0.65
0.32
0.81
0.71
0.60
0.71
0.57
0.55
0.55
0.95
0.89
0.65
0.40

0.22
0.99
0.26
0.63
1.00
1.00
0.86
0.44
0.60
0.20
0.75
0.12
0.23
0.95
1.00
0.97
0.47
0.99
1.00
0.44
0.23
0.84
0.31
1.00
0.95
1.00
0.87
0.49
0.29
0.26
0.48
0.31
0.58
0.97
1.00
1.00
0.53
1.00
1.00
0.47
0.29
0.70
0.19
0.15
0.18
0.11
1.00
0.28
0.20
0.35
0.14
0.00
0.01
0.11
0.07
0.09
0.80
1.00
0.30
0.17

0.19
0.46
0.34
0.47
0.45
1.00
0.14
0.46
0.18
0.21
0.22
0.24
0.22
0.09
0.63
0.66
0.06
0.20
0.56
0.19
0.07
0.47
0.08
0.85
0.51
0.31
0.10
0.23
0.60
0.15
0.16
0.14
0.12
0.69
0.75
0.86
0.55
0.91
0.82
0.73
0.36
0.32
0.18
0.06
0.53
0.59
1.00
0.32
0.32
0.16
0.36
0.05
0.23
0.23
0.32
0.17
0.37
1.00
0.66
0.06

0.38
0.87
0.56
0.75
0.89
1.00
0.68
0.40
0.56
0.06
0.74
0.05
0.22
0.80
0.99
0.91
0.19
0.84
0.96
0.74
0.78
0.65
0.33
0.99
0.88
0.97
0.79
0.51
0.35
0.18
0.58
0.21
0.63
0.84
0.98
0.99
0.49
0.91
0.98
0.51
0.22
0.79
0.18
0.87
0.10
0.10
0.99
0.52
0.82
0.20
0.09
0.01
0.09
0.05
0.03
0.02
0.70
0.99
0.30
0.57

30

Manning et al.

Bolocam

Catalogue #

BGPS
name

Random
Forests Regression LDA

Logistic

Normalised

LDA

8288
8289
8299
8319
8329
8332
8335
8337
8338
8342
8345
8353
8354
8355
8356
8361
8366
8367
8370

G359.713+00.045
G359.716−00.375
G359.752+00.037
G359.822+00.029
G359.864+00.019
G359.867−00.085
G359.891−00.071
G359.900+00.015
G359.906+00.041
G359.912−00.305
G359.920+00.025
G359.944+00.171
G359.946+00.153
G359.946−00.047
G359.947+00.023
G359.971−00.459
G359.978−00.071
G359.985+00.023
G359.994+00.107

0.53
0.82
0.51
0.49
0.53
0.89
0.80
0.70
0.73
0.10
0.75
0.87
0.94
0.91
0.78
0.59
0.85
0.65
0.60

0.20
0.74
0.07
0.05
0.14
0.16
0.98
0.15
0.43
0.32
0.32
0.82
1.00
1.00
0.14
0.99
0.00
0.11
0.11

0.42
0.54
0.36
0.51
0.35
0.25
1.00
0.23
0.15
0.49
0.13
0.49
0.39
1.00
0.45
0.83
1.00
0.43
0.26

0.11
0.74
0.03
0.12
0.22
0.95
0.94
0.11
0.45
0.56
0.16
0.77
0.95
1.00
0.08
0.97
0.84
0.10
0.04

PASA (2016)
doi:10.1017/pas.2016.xxx

