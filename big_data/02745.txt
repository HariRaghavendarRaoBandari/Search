6
1
0
2

 
r
a

M
9

 

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
5
4
7
2
0

.

3
0
6
1
:
v
i
X
r
a

Non-parametric latent modeling and network
clustering

Franc¸ois Bavaud

Abstract The paper exposes a non-parametric approach to latent and co-latent mod-
eling of bivariate data, based upon alternating minimization of the Kullback-Leibler
divergence (EM algorithm) for complete log-linear models. For categorical data, the
iterative algorithm generates a soft clustering of both rows and columns of the con-
tingency table. Well-known results are systematically revisited, and some variants
are presumably original. In particular, the consideration of square contingency tables
induces a clustering algorithm for weighted networks, differing from spectral clus-
tering or modularity maximization techniques. Also, we present a co-clustering al-
gorithm applicable to HMM models of general kind, distinct from the Baum-Welch
algorithm. Three case studies illustrate the theory.

1 Introduction: parametric and non-parametric mixtures

Two variables can be dependent, yet conditionally independent given a third one,
that is X ⊥ Y|G but X (cid:54)⊥ Y : in bivariate latent models of dependence M, joint bivari-
ate probabilities P(x,y) express as

P(x,y) =

m

∑

g=1

p(x,y,g) =

m

∑

g=1

p(g)p(x|g)p(y|g)

(1)

where x, y, g denote the values of X, Y , G, and p(x,y,g) their joint probability.

Bivariate data, such as summarized by normalized contingency tables F(x,y) =
n(•,•), where n(x,y) counts the number of individuals in x ∈ X and y ∈ Y , can be ap-
n(x,y)
proached by latent modeling, consisting in inferring a suitable model P(x,y) ∈ M of

Franc¸ois Bavaud
University of Lausanne, e-mail: fbavaud@unil.ch

1

2

Non-parametric latent modeling and network clustering

the form (1), typically closest to the observed frequencies F(x,y) in the maximum-
likelihood sense, or in the least squares sense. Mixture (1) also deﬁnes memberships
p(g|x) = p(x|g)p(g)/p(x) and p(g|y); hence latent modeling also performs model-
based clustering, assigning observations x and y among groups g = 1, . . . ,m.

Latent modeling and clustering count among the most active data-analytic re-
search trends of the last decades. The literature is simply too enormous to cite even
a few valuable contributions, often (re-)discovered independently among workers in
various application ﬁelds. Most approaches are parametric, typically deﬁning p(x|g)
and p(y|g) as exponential distributions of some kind, such as the multivariate nor-
mal (continuous case) or the multinomial (discrete case) (see e.g. Govaert and Nadif
2013 and references therein). Parametric modelling allows further hyperparametric
Bayesian processing, as in latent Dirichlet allocation (Blei et al. 2003).

By contrast, we focus on non-parametric models speciﬁed by the whole fam-
ily of log-linear complete models M corresponding to X ⊥ Y|G, namely (see e.g.
Christensen 2006)

M = {p| ln p(x,y,g) = a(x,g) + b(y,g) + c}

Equivalently,

M = {p| p(x,y,g) =

}

p(x,•,g) p(•,y,g)

p(•,•,g)

where “•” denotes the summation over the replaced argument. The correspond-
ing class of bivariate models M of the form (1) simply reads M = {P | P(x,y) =
∑g p(x,y,g) ≡ p(x,y,•) , for some p ∈ M}.

Observations consist of the joint empirical distribution F(x,y), normalized to
F(•,•) = 1. In latent modeling, one can think of the observer as a color-blind agent
perceiving only the margin f (x,y,•) of the complete distribution f (x,y,g), but not
the color (or group) g itself (see Fig. 1). Initially, any member f of the set

D = { f | f (x,y,•) = F(x,y)}

Fig. 1 Left: observed data,
where (x,y) are the object
coordinates. Right: complete
data (x,y,g), where the group
g is labeled by a color. In
psychological terms, (x,y)
is the stimulus, and (x,y,g)
the percept, emphasizing the
EM-algorithm as a possible
model for cognition.

Non-parametric latent modeling and network clustering

3

seems equally compatible with the observations F, and the role of a clustering algo-
rithm precisely consists in selecting a few good candidates f ∈ D, or even a unique
one, bringing color to the observer.

This paper exposes a non-parametric approach to latent and co-latent model-
ing of bivariate data, based upon alternating minimization of the Kullback-Leibler
divergence (EM algorithm) for complete log-linear models (section 2). For cate-
gorical data, the iterative algorithm generates a soft clustering of both rows and
columns of the contingency table. Well-known results are systematically revisited,
and some variants are presumably original. In particular, the consideration of square
contingency tables induces a clustering algorithm for weighted networks, differing
from spectral clustering or modularity maximization techniques (section 3). Also,
we present a co-clustering algorithm applicable to HMM models of general kind,
distinct from the Baum-Welch algorithm. Three case studies illustrate the theory:
latent (co-)betrayed clustering of a term-document matrix (section 2.3), latent clus-
tering of spatial ﬂows (section 3.2), and latent co-clustering of bigrams in French
(section 3.4).

2 EM latent clustering: a concise derivation from ﬁrst principles

The alternating minimisation procedure (Csisz´ar and Tusn´ady 1984) provides an
arguably elegant derivation of the EM algorithm; see also e.g. Cover and Thomas
(1991) or Bavaud (2009). The maximum likelihood model ˆP ∈ M of the form (1)
minimizes the Kullback-Leibler divergence K()

ˆP = arg min
P∈M

K(F(cid:107)P)

K(F(cid:107)P) = ∑

x,y

F(x,y)ln

F(x,y)
P(x,y)

where F(x,y) denotes the empirical bivariate distribution. On the other hand, the
complete Kullback-Leibler divergence K( f(cid:107)p) = ∑x,y,g f (x,y,g) ln f (x,y,g)
p(x,y,g), where
f (x,y,g) is the empirical “complete” distribution (see ﬁg. 1), enjoys the following
properties (see e.g. Bavaud (2009) for the proofs, standard in Information Theory):

ˆp(x,y,g) := arg min
p∈M

K( f||p) =

f (x,•,g) f (•,y,g)

f (•,•,g)

M-step

(2)

˜f (x,y,g) := argmin
f∈D

K( f||p) =

p(x,y,g)
p(x,y,•)

F(x,y)

E-step

(3)

Furthermore, min f∈D K( f||p) = K(F||P), and thus
min
f∈D

K(F||P) = min
p∈M

min
P∈M

K( f||p)

4
Non-parametric latent modeling and network clustering
Hence, starting from some complete model p(0) ∈ M , the EM-sequence f (t+1) :=
˜f [p(t)] deﬁned in (3) and p(t+1) := ˆp[ f (t+1)] deﬁned in (2) converges towards a local
minimum of K( f||p). Observe the margins to coincide after a single EM-cycle in the
sense p(t)(x,•,•) = F(x,•) and p(t)(•,y,•) = F(•,y) for all t ≥ 1.

For completeness sake, note that D and M are closed in the following sense, as
they are in other instances of the EM algorithm in general. Critically and crucially:
i) D is convex, that is closed under additive mixtures λ f1 + (1− λ ) f2; this turns

out to be the case for maximum entropy problems in general.

ii) M is log-convex, that is closed under multiplicative mixtures pλ

/Z(λ )
where Z(λ ) is a normalization constant; this is the case for exponential models,
as well as for non-parametric log-linear models in general.

2

1 p(1−λ )

2.1 Latent co-clustering

Co-clustering describes the situation where each of the observed variables is at-
tached to a distinct latent variable, the latter being mutually associated. That is,
X ⊥ Y|(U,V ), X ⊥ V|U and Y ⊥ U|V while X (cid:54)⊥ Y , and U (cid:54)⊥ V in general. Equiva-
lently, X → U → V → Y form a “Markov chain”, in the sense of Cover and Thomas
(1991). Bivariate joint probabilities express as

P(x,y) =

m1∑

u=1

m2∑

v=1

p(x,y,u,v) = ∑

u,v

p(u,v)p(x|u)p(y|v)

(4)

Complete models M , restricted models M and complete empirical distributions D
are

p(x• u•) p(•y• v) p(•• uv)

M = {p| p(x,y,u,v) =
p(•• u•) p(•••v)
M = {P| P(x,y) = p(x,y,•,•) with p ∈ M}
D = { f | f (x,y,•,•) = F(x,y)}

}

(5)

(6)
(7)

where F(x,y) denotes the observed empirical distribution. The steps of the former
section apply again, yielding the EM algorithm

ˆp(x,y,u,v) := arg min
p∈M

K( f||p) =

f (x• u•) f (•y• v) f (•• uv)

f (•• u•) f (•••v)

M-step

(8)

˜f (x,y,u,v) := argmin
f∈D

K( f||p) =

where K( f(cid:107)p) = ∑x,y,u,v f (x,y,u,v) ln f (x,y,u.v)
plete observations from the complete model.

F(x,y)

p(x,y,u,v)
p(x,y,•,•)
p(x,y,u,v) measures the divergence of the com-

E-step

(9)

Non-parametric latent modeling and network clustering
2.2 Matrix and tensor algebra for contingency tables

5

The material of sections (2) and (2.1) holds irrespectively of the continuous or dis-
crete nature of X and Y : in the continous case, integrals simply replace sums. In
the discrete setting, addressed here, categories are numbered as i = 1, . . . ,n for X,
as k = 1, . . . , p for Y and as g = 1, . . . ,m for G. Data consist of the relative n× p
contingency table Fik normalized to F•• = 1.

2.2.1 Latent co-clustering

Co-clustering models and complete models express as

Pik =

m1∑

u=1

m2∑

v=1

cuv au

i bv
k

pikuv = cuv au

i bv
k

(10)

• where cuv = P(U = u,V = v) = p(• • uv), obeying c•• = 1, is the joint latent
distribution of row, respectively column groups u and v
• au
i = p(i•u•)/p(••u•) (with au• = 1) is the row distribution conditionally to the
row group U = u, also referred to as emission probability (section 3)
• bv
k = p(•k • v)/p(• • •v) (with bv• = 1) is the column distribution or emission
probability conditionally to the column group V = v.

Hence, a complete model p is entirely determined by the triple (C,A,B), where
C = (cuv) is m1 × m2 and normalized to unity, A = (au
k) is
p× m2, both row-standardized.

i ) is n× m1 and B = (bv

It is straightforward to show that the successive application of the E-step (9) and

the M-step (8) to p ≡ (C,A,B) yields the new complete model ¨p ≡ ( ¨C, ¨A, ¨B) with

¨cuv = cuv ∑

Fjl
Pjl

au
j bv
l

¨au
i = au
i

¨bv
k = bv
k

jl
bv(cid:48)
∑lv(cid:48) cuv(cid:48) Fil
l
Pil
j bv(cid:48)
∑ jlv(cid:48) cuv(cid:48) Fjl
au
Pjl
au(cid:48)
Fjk
∑ ju(cid:48) cu(cid:48)v
j
Pjk
au(cid:48)
Fjl
j bv
∑ jlu(cid:48) cu(cid:48)v
l
Pjl

l

(11)

(12)

(13)

Also, after a single EM cycle, margins are respected, that is ¨Pi• = Fi• and ¨P•k = F•k.

In hard clustering, rows i are attached to a single group denoted u[i], that is au

i =
0 unless u = u[i]; similarly, bv
k = 0 unless v = v[k]. Restricting P in (10) to hard
clustering yields block clustering, for which K(F||P) = I(X : Y )− I(U : V ), where

6

Non-parametric latent modeling and network clustering

I() is the mutual information (e.g. Kullback (1959); Bavaud (2000); Dhillon et al.
(2003)).

The set M of models P of the form (10) is convex, with extreme points consisting
of hard clusterings. K(F(cid:107)P) being convex in P, its minimum is attained for convex
mixtures of hard clusterings, that is for soft clusterings.

2.2.2 Latent clustering

Setting m1 = m2 = m and C diagonal with cgh = ρg δgh yields the latent model

Pik =

m

∑

g=1

ρg ag

i bg
k

pikg = ρg ag

i bg
k

(14)

together with the corresponding EM-iteration p ≡ (ρ,A,B) → ¨p ≡ ( ¨ρ, ¨A, ¨B), namely

¨ρg = ρg κg

¨ag
i = ag
i

Fil
Pil

∑l bg
l
κg

¨bg
k = bg
k

Fjk
Pjk

∑ j ag
j
κg

(15)

Fjl
Pjl

jbg
l

where κg = ∑ jl ag
. Similar, if not equivalent updating rules have been proposed
in information retrieval and natural language processing (Saul and Pereira 1997;
Hofmann 1999), as well as in the non-negative matrix factorization framework (Lee
and Seung 2001; Finesso and Spreij 2006).

By construction, families of latent models (14) Mm with m groups are nested in

the sense Mm ⊆ Mm+1.
The case m = 1 amounts to independence models Pik = aibk, for which the ﬁxed
point ¨ai = Fi• and ¨bk = F•k is, as expected, reached after a single iteration, irrespec-
tively of the initial values of a and b.
By contrast, m ≥ rank(F) generates saturated models, exactly reproducing the
observed contingency table. For instance, assume that m = p = rank(F) ≤ n; then
taking ag
k = δkg and ρg = F•g (which already constitutes a ﬁxed point
of (15)) evidently satisﬁes Pik = Fik.

i = Fig/F•g, bg

2.3 Case study I: Reuters 21578 term-document matrix

The n× p = 20×1266 document-term normalized matrix F, constituting the Reuters
21578 dataset, is accessible through the R package tm (Feinerer et al. 2008). The
co-clustering algorithm (11) (12) (13) is started by randomly assigning uniformly
each document to a single row group u = 1, . . . ,m1, and by uniformly assigning each
term to a single column group v = 1, . . . ,m2. The procedure turns out to converge

Non-parametric latent modeling and network clustering

7

after about 1000 iterations (ﬁgure 2), yielding a locally minimal value Km1m2 of the
Kullback-Leibler divergence. By construction, Km1m2 decreases with m1 and m2.
Latent clustering (15) with m groups is performed analogously, yielding a locally
minimal value Km.

Experiments with three or four groups yield the typical results K3 = 1.071180
> K33 = 1.058654 > K43 = 1.038837 > K34 = 1.036647 > K4 = 0.877754 >
K44 = 0.873071. The above ordering is expected, although inversions are frequently
observed, under differing random initial conﬁgurations. Model selection procedures,
not addressed here, should naturally consider in addition the degrees of freedom,
larger for co-clustering models. The latter do not appear as particularly rewarding
here (at least for the experiments performed, and in contrast to the results associ-
ated to case study III of section 3.4): indeed, joint latent distributions C turn out
to be “maximally sparse”, meaning that row groups u and column groups v are es-
sentially the same. Finally, each of the 20 documents of the Reuters 2157 dataset
happens to belong to a single row group (hard clusters), while only a minority of the
1266 terms (say about 20%) belong to two or more column groups (soft clusters).

Fig. 2 Case study I: convergence of the latent and co-latent iterating procedure. Left: latent model
with m = 3. Middle: co-latent model with (m1,m2) = (3,4). Right: co-latent model with (m1,m2) =
(4,4).

3 Network clustering

When the observed categories x and y belong to the same set indexed by i, j =
1, . . . ,n, the relative square contingency table Fi j deﬁnes a directed weighted net-
work on n vertices: Fi j is the weight of edge (i j), Fi• is the outweight of vertex i
(relative outdegree) and F•i its inweight i (relative indegree), all normalized to unity.
Frequently, Fi j counts the relative number of units initially at vertex i, and at vertex
j after some ﬁxed time. Examples abound in spatial migration, spatial commuting,
social mobility, opinion shifts, confusion matrices, textual dynamics, etc.

A further restriction, natural in many applications of latent network modeling,
consists in identifying the row and column emission probabilities, that is in requiring

0500100015001.11.21.31.41.51.6iterationKullback-Leibler divergence0500100015001.11.21.31.41.51.6iterationKullback-Leibler divergence0500100015001.01.21.41.6iterationKullback-Leibler divergence8

Non-parametric latent modeling and network clustering

i = ag
bg
increasing ﬂexibility, namely

i . This condition generates four families of nested latent network models of

m

∑

g=1
m

∑

Pi j =

Pi j =

Pi j =

Pi j =

u,v=1

m

∑

u,v=1

m

∑

u,v=1

ρg ag

i ag
j

latent (symmetric) network model

(16)

cuv au

i av

j with cuv = cvu

cuv au

i av

j with cu• = c•u

co-latent symmetric network model (17)

co-latent MH network model

(18)

cuv au

i av
j

co-latent general network model

(19)

Models (16) and (17) P = P(cid:48), making latent and co-latent symmetric clustering
suitable for unoriented weighted networks with Fi j = Fji. By contrast, unrestricted
co-latent models (19) describes general oriented weighted networks. Symmetric ma-
trices F = (Fi j) appear naturally in reversible random walks on networks, or in spa-
tial modeling where they measure the spatial interaction between regions (spatial
weights), and constitute a weighted version of the adjacency matrix, referred to as
an exchange matrix by the author (Bavaud 2014 and references therein; see also
Berger and Snell 1957).

Latent models (16) are positive semi-deﬁnite or diffusive, that is endowed with
non-negative eigenvalues, characteristic of a continuous propagation process from a
one place to its neighbours. In particular, the diagonal part of P in (16) cannot be too
small. In contrast, co-latent symmetrical network models (17) are ﬂexible enough to
describe phenomena such as bipartition or periodic alternation, implying negative
eigenvalues.

The condition (18) of marginal homogeneity (MH) on the joint latent distribution
C is inherited by the restricted models, in the sense Pi• = P•i. They constitute appro-
priate models for the bigram distributions of single categorical sequences (of length
N, constituted of n types), for which Fi• = F•i + O(N−1); see the case study III of
Section 3.4. Formulation (18) describes m hidden states related by a Markov transi-
tion matrix p(v|u) = cuv/cu•, as well as n observed states related to the hidden ones
i = p(i|u). Noticeably enough, (18) precisely en-
by the emission probabilities au
compasses the ingredients of the hidden Markov models (HMM) (see e.g. Rabiner
1989).

Non-parametric latent modeling and network clustering
3.1 Network latent clustering

9

Approximating F by P in (16) amounts in performing a soft network clustering: the
membership of vertex i in group g (of weight ρg) is

zig = p(i|g) =

p(i)p(g|i)

p(g)

=

fi ag
i
ρg

with

fi = Fi• = F•i

and ρg =

n

∑

i=1

fi zig .

EM-updating rules for memberships (instead of emission probabilities, for a change)

Pi j = fi f j

m

∑

g=1

zigz jg
ρg

¨zig = zig ∑

j

Fi j
Pi j

f jz jg
ρg

¨ρg = ∑

i

fi ¨zig

(20)

deﬁne a soft clustering iterative algorithm for unoriented weighted networks, pre-
sumably original.

3.2 Case study II: inter-cantonal Swiss migrations

Consider the n× n matrix N = (Ni j) of inter-cantonal migratory ﬂows in Switzer-
land, counting the number of people inhabiting canton i in 1980 and canton j in
1985, i, j = 1, . . . ,n = 26, for a total of sum(N) = 6(cid:48)039(cid:48)313 inhabitants, 93%
of which lie on the diagonal (stayers). The symmetric, normalized matrix F =
2 (N + N(cid:48))/N•• is diffusive, largely dominated by its diagonal. As a consequence,
1
direct application of algorithm (20) from an initial random cantons-to-groups assig-
nation produces somewhat erratic results: a matrix F = (Fi j) too close to the identity
matrix I = (δi j) cannot by reasonably approximated by the latent model (16), unless
m = n, where each canton belongs to its own group.
Here, the difﬁculty lies in the shortness of the observation period (5 years, smaller
than the average moving time), making the off-diagonal contribution 1− trace(F)
too small. Multiplying the observation period by a factor λ > 1 generates, up to
O(λ 2), a modiﬁed relative ﬂow ˜Fi j = λ Fi j + (1−λ )δi j fi, where fi = Fi• = F•i is the
weight of canton i. The modiﬁed ˜F is normalized, symmetric, possesses unchanged
vertex weights ˜Fi• = fi, and its off-diagonal contribution is multiplied by λ . Of
course, λ cannot be too large, in order to insure the non-negativity of ˜F (λ ≤ 6.9
here) as well as its semi-positive deﬁniteness (λ ≤ 6.4 here).

Typical realizations of (20), with λ = 5, are depicted in ﬁgures (3) and (4): as

expected, spatially close regions tend to be regrouped.
3.3 Network general co-clustering

Latent co-clustering (19) applies to contingency tables F of general kind, possibly
asymmetric or marginally inhomogeneous, and possibly exhibiting diffusivity, al-
ternation, or a mixture of them. Implementing the common emission probabilities

10

Non-parametric latent modeling and network clustering

Fig. 3 Decrease of the
Kullback-Leibler divergence
for the two realizations of
ﬁgure 4, respectively. Hor-
izontal plateaux correspond
to metastable minima in the
learning of the latent struc-
ture, followed by the rapid
discovery of a better ﬁt.

Fig. 4 Case study II: two realizations of the network latent clustering algorithm (20), applied to the
modiﬁed ﬂow matrix ˜F, with random initial assignment to m = 6 groups, and ﬁnal hard assignment
of canton i to group argmaxg zig.

constraint in the M-step (8) yields together with (19) the updating rule
) av(cid:48)
Fj(cid:48)i
j(cid:48)
Pj(cid:48)i
Fj(cid:48)i(cid:48)
i(cid:48) av(cid:48)
Pj(cid:48)i(cid:48) ) au
j(cid:48)

Fi j(cid:48)
Pi j(cid:48) + cv(cid:48)u
Fi(cid:48) j(cid:48)
Pi(cid:48) j(cid:48) + cv(cid:48)u

∑i(cid:48) j(cid:48)v(cid:48)(cuv(cid:48)

∑ j(cid:48)v(cid:48)(cuv(cid:48)

¨au
i = au
i

¨cuv = cuv ∑

i j

Fi j
Pi j

au
i av
j

(21)

Let us recall that the classical Baum-Welch algorithm handles the HMM model-
ing of a single (large) sequence of tokens, with (almost) marginally homogeneous
bigram counts. By contrast, the presumably original iterative algorithm (21) also
seems to be able to handle marginally inhomogeneous network data, such that ag-
gregates of smaller sequences. Further experimentations are needed at this stage to
gauge the generality of model (19), and the efﬁciency of algorithm (21).

For symmetric data F = F(cid:48), the symmetric model (17) can be tackled by (21)
above, with the simplifying circumstance that the additive symmetrizing occurring
in the numerator and denominator of ¨au
i is not needed anymore, provided that the
initial joint probability cuv is symmetrical, which automatically insures the symme-
try of further iterates ¨cuv.

020040060080010000.500.550.600.650.700.75iterationKullback-Leibler divergence020040060080010000.500.550.60iterationKullback-Leibler divergenceNon-parametric latent modeling and network clustering
3.4 Case study III: modeling bigrams

group 1 2 3 4

group

1

2

3
100

4

100

11
8

1 97
13
3
35
9 61 26

61

25

45

3
10 1 1
12 2
1
4
3 1
2
1
1 3 13 5
2

17
8
17 9

4

13

2

1

10 6 4 3
6 28
7 26

8 10 3

8

7

1

5

2

1

a
b
c
d
e
f
g
h
i
j
k
l
m
n
o
p
q
r
s
t
u
v
w
x
y
z

89
87
5
90 10
2
87
71 26
65
4
100

100
2

77
76

a
b
c
d
e
f
g
h
i
j
k
l
m
n
o
p
q
r
s
t
u
v
100
w 100
x
y
z

21

24
53 47

9

88

89

100
2
12
40 21 22 17
20 80
26 74

27 57 16

100
60 40
100

11

1 2 3 4
1 0 0 1 21
2 0 3 12 2
3 17 7 0 7
4 5 8 17 0

1 2 3 4
1 0 0 7 93
2 0 15 71 14
3 54 24 0 22
4 17 27 57 0

1 22
2 18
3 31
4 30

Table 1 Case study III: emission probabilities A (left), memberships Z (middle), joint latent dis-
tribution C (right, top), latent probability transition matrix W (right, middle) and its corresponding
stationary distribution π (right, bottom). All values are multiplied by 100 and rounded to the near-
est integer.

We consider the ﬁrst chapters of the French novel “La Bˆete humaine” by Emile
Zola (1890). After suppressing all punctuation, accents and separators with excep-
tion of the blank space, and converting upper-case letters to lower-case, we are left
with a sequence of N = 725(cid:48)000 tokens on n = 27 types (the alphabet + the space),
containing 724’999 pairs of successive tokens or bigrams. The resulting n× n nor-
malized contingency table F = (Fi j) is far from symmetric (for instance, the bigram
qu occurs 6’707 times, while uq occurs only 23 times), but almost marginally ho-
mogenous, that is Fi• ∼= F•i + 0(N−1) (and exactly marginally homogenous if one
starts and ﬁnishes the textual sequence with the same type, such as a blank space).
Symmetrizing F as Fs = (F + F(cid:48))/2 does not makes it diffusive, and hence un-
suitable by latent modelling (16), because of the importance of large negative eigen-
values in Fs, betraying alternation, typical in linguistic data - think in particular of
the vowels-consonants alternation (e.g. Goldsmith and Xanthos 2009). This being
said, symmetric co-clustering of Fs (17) remains a possible option.

12

Non-parametric latent modeling and network clustering

Table 1 results from the general co-clustering algorithm (21) applied on the orig-
inal, asymmetric bigram counts F itself. Group 4 mainly emits the vowels, group
3 the blank, group 2 the s and t, and group 1 other consonants. Alternation is be-
trayed by the null diagonal of the Markov transition matrix W - with the exception
of group 2.

The property of marginal homogeneity Fi• = F•i permits in addition to obtain
the memberships Z from the emissions A, by ﬁrst determining the solution ρ of
∑g ρg ag
i = fi, where fi = Fi• = F•i is the relative frequency of letter i, and then by
deﬁning zig = ρg ag

i / fi.

References

1. Bavaud, F.: An Information Theoretical approach to Factor Analysis. In Proceedings of the
5th International Conference on the Statistical Analysis of Textual Data (JADT 2000), pp.
263–270 (2000)

2. Bavaud, F.: Information theory, relative entropy and statistics. In Sommaruga, G. (Ed.) Formal

Theories of Information, LNCS 5363 pp. 54–78, Springer (2009)

3. Bavaud, F.: Spatial weights: constructing weight-compatible exchange matrices from proxim-
ity matrices. In Duckham, M. et al. (Eds.) GIScience 2014, LNCS 8728, pp. 81–96, Springer
(2014)

4. Berger, J., Snell, J. L.: On the concept of equal exchange. Systems Research and Behavioral

5. Blei, D.M., Ng, A.Y., Jordan, M.I.: Latent Dirichlet allocation. The Journal of machine Learn-

Science 2, pp. 111–118 (1957)

ing research 3, pp. 993–1022 (2003)

6. Christensen, R: Log-linear models and logistic regression. Springer, (2006)
7. Cover, T. M., Thomas, J. A.: Elements of Information Theory. Wiley (1991)
8. Csisz´ar, I., Tusn´ady, G.: Information Geometry and Aternating Minimization Procedures. In:

Dedewicz, E.F. (ed.) Statistics and Decisions, Supplement Issue 1 pp. 205–237 (1984)

9. Dhillon, I.S., Mallela, S., Modha, D.S.: Information-theoretic co-clustering. In Proceedings of
the ninth ACM SIGKDD international conference on Knowledge discovery and data mining,
pp. 89–98 (2003)

10. Feinerer, I., Hornik, K., Meyer, D.: Text mining infrastructure in R. Journal of Statistical

Software, 25(5), pp. 1–54 (2008)

11. Finesso, L., Spreij, P. : Nonnegative matrix factorization and I-divergence alternating mini-

mization. Linear Algebra and its Applications 416, pp. 270–287 (2006)

12. Goldsmith, J., Xanthos, A.: Learning Phonological Categories. Language 85, pp. 4–38 (2009)
13. Govaert, G., Nadif, M. Co-Clustering. Wiley (2013)
14. Hofmann, T.: Probabilistic latent semantic indexing. In Proceedings of the 22nd annual inter-
national ACM SIGIR conference on Research and development in information retrieval, pp.
50–57 (1999)

15. Kullback, S.: Information theory and statistics. Wiley (1959)
16. Lee, D. D., Seung, H. S.: Algorithms for non-negative matrix factorization. In Advances in

neural information processing systems, pp. 556–562 (2001)

17. Rabiner, L. R.: A tutorial on hidden Markov models and selected applications in speech recog-

nition. Proceedings of the IEEE, 77(2), pp. 257–286 (1989)

18. Saul, L., Pereira, F.: Aggregate and mixed-order Markov models for statistical language pro-
cessing. In Proceedings of the 2nd International Conference on Empirical Methods in Natural
Language Processing (1997)

