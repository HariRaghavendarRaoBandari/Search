6
1
0
2

 
r
a

M
9

 

 
 
]
S
D
.
s
c
[
 
 

1
v
2
8
7
2
0

.

3
0
6
1
:
v
i
X
r
a

Bipartite Correlation Clustering – Maximizing Agreements

Megasthenis Asteris

Anastasios Kyrillidis

The University of Texas at Austin

The University of Texas at Austin

Dimitris Papailiopoulos

University of California, Berkeley

Alexandros G. Dimakis

The University of Texas at Austin

Abstract

In Bipartite Correlation Clustering (BCC)
we are given a complete bipartite graph G
with ‘+’ and ‘−’ edges, and we seek a ver-
tex clustering that maximizes the number
of agreements: the number of all ‘+’ edges
within clusters plus all ‘−’ edges cut across
clusters. BCC is known to be NP-hard [5].

We present a novel approximation algorithm
for k-BCC, a variant of BCC with an up-
per bound k on the number of clusters. Our
algorithm outputs a k-clustering that prov-
ably achieves a number of agreements within
a multiplicative (1 − δ)-factor from the opti-
mal, for any desired accuracy δ. It relies on
solving a combinatorially constrained bilin-
ear maximization on the bi-adjacency matrix
of G. It runs in time exponential in k and 1/δ,
but linear in the size of the input.

Further, we show that, in the (unconstrained)
BCC setting, an (1 − δ)-approximation can
be achieved by O(δ−1) clusters regardless of
In turn, our k-BCC
the size of the graph.
algorithm implies an Eﬃcient PTAS for the
BCC objective of maximizing agreements.

1

Introduction

Correlation Clustering (CC) [7] considers the task of
partitioning a set of objects into clusters based on
their pairwise relationships. It arises naturally in sev-
eral areas such as in network monitoring [1], document
clustering [7] and textual similarity for data integra-
tion [10]. In its simplest form, objects are represented
as vertices in a complete graph whose edges are la-
beled ‘+’ or ‘−’ to encode similarity or dissimilarity
among vertices, respectively. The objective is to com-
pute a vertex partition that maximizes the number of
agreements with the underlying graph, i.e., the total

number of ‘+’ edges in the interior of clusters plus
the number of ‘−’ edges across clusters. The num-
ber of output clusters is itself an optimization vari-
able –not part of the input.
It may be meaningful,
however, to restrict the number of output clusters to
be at most k. The constrained version is known as
k-CC and similarly to the unconstrained problem, it
is NP-hard [7, 13, 15]. A signiﬁcant volume of work
has focused on approximately solving the problem of
maximizing the number of agreements (MaxAgree),
or the equivalent –yet more challenging in terms of
approximation– objective of minimizing the number
of disagreements (MinDisagree) [7, 11, 9, 3, 19, 4].

Bipartite Correlation Clustering (BCC) is a natural
variant of CC on bipartite graphs. Given a complete
bipartite graph G = (U, V, E) with edges labeled ‘+’
or ‘−’, the objective is once again to compute a clus-
tering of the vertices that maximizes the number of
agreements with the labeled pairs. BCC is a special
case of the incomplete CC problem [9], where only a
subset of vertices are connected and non-adjacent ver-
tices do not aﬀect the objective function. The output
clusters may contain vertices from either one or both
sides of the graph. Finally, we can deﬁne the k-BCC
variant that enforces an upper bound on the number
of output clusters, similar to k-CC for CC.

The task of clustering the vertices of a bipartite graph
is common across many areas of machine learning. Ap-
plications include recommendation systems [18, 20],
where analyzing the structure of a large sets of pair-
wise interactions (e.g., among users and products) al-
lows useful predictions about future interactions, gene
expression data analysis [5, 16] and graph partitioning
problems in data mining [12, 21].

Despite the practical interest in bipartite graphs, there
is limited work on BCC and it is focused on the
theoretically more challenging MinDisagree objec-
tive:
[5] established an 11-approximation algorithm,
while [2] achieved a 4-approximation, the currently
best known guarantee. Algorithms for incomplete CC

Bipartite Correlation Clustering – Maximizing Agreements

[11, 9, 17] can be applied to BCC, but they do not
leverage the structure of the bipartite graph. More-
over, existing approaches for incomplete CC rely on
LP or SDP solvers which scale poorly.

Our contributions We develop a novel approxima-
tion algorithm for k-BCC with provable guarantees for
the MaxAgree objective. Further, we show that un-
der an appropriate conﬁguration, our algorithm yields
an Eﬃcient Polynomial Time Approximation Scheme
(EPTAS1) for the unconstrained BCC problem. Our
contributions can be summarized as follows:

1. k-BCC: Given a bipartite graph G = (U, V, E), a
parameter k, and any constant accuracy parameter
δ ∈ (0, 1), our algorithm computes a clustering of
U ∪ V into at most k clusters and achieves a num-
ber of agreements that lies within a (1 − δ)-factor
from the optimal. It runs in time exponential in k
and δ−1, but linear in the size of G.

2. BCC: In the unconstrained BCC setting, the op-
timal number of clusters may be anywhere from 1
to |U| + |V |. We show that if one is willing to set-
tle for a (1 − δ)-approximation of the MaxAgree
objective, it suﬃces to use at most O(δ−1) clusters,
regardless of the size of G. In turn, under an appro-
priate conﬁguration, our k-BCC algorithm yields
an EPTAS for the (unconstrained) BCC problem.

3. Our algorithm relies on formulating the k-BCC/
MaxAgree problem as a combinatorially con-
strained bilinear maximization

Tr(cid:0)X(cid:62)BY(cid:1) ,

max

X∈X ,Y∈Y

(1)

where B is the bi-adjacency matrix of G, and X ,
Y are the sets of cluster assignment matrices for U
and V , respectively. In Sec. 3, we brieﬂy describe
our approach for approximately solving (1) and its
guarantees under more general constraints.

We note that our k-BCC algorithm and its guaran-
tees can be extended to incomplete, but dense BCC
instances, where the input G is not a complete bipar-
tite graph, but |E| = Ω(|U|·|V |) For simplicity, we re-
strict the description to the complete case. Finally, we
supplement our theoretical ﬁndings with experimental
results on synthetic and real datasets.

1EPTAS refers to an algorithm that approximates the solu-
tion of an optimization problem within a multiplicative (1 − )-
factor, for any constant  ∈ (0, 1), and has complexity that scales
arbitrarily in 1/, but as a constant order polynomial (indepen-
dent of ) in the input size n. EPTAS is more eﬃcient than a
PTAS; for example, a running time of O(n1/) is considered a
PTAS, but not an EPTAS.

1.1 Related work

There is extensive literature on CC; see [8] for a list
of references. Here, we focus on bipartite variants.

BCC was introduced by Amit in [5] along with an
11-approximation algorithm for the MinDisagree ob-
jective, based on a linear program (LP) with O(|E|3)
In [2], Ailon et al. proposed two algo-
constraints.
rithms for the same objective:
i ) a deterministic 4-
approximation based on an LP formulation and de-
randomization arguments by [19], and ii ) a random-
ized 4-approximation combinatorial algorithm. The
latter is computationally more eﬃcient with complex-
ity scaling linearly in the size of the input, compared

to the LP that has O(cid:0)(|V | + |U|)3(cid:1) constraints.

For the incomplete CC problem, which encompasses
BCC as a special case,
[11] provided an LP-based
O (log n)-approximation for MinDisagree. A simi-
lar result was achieved by [9]. For the MaxAgree
objective,
[2, 17] proposed an SDP relaxation, simi-
lar to that for MAX k-CUT, and achieved a 0.7666-
approximation. We are not aware of any results explic-
itly on the MaxAgree objective for either BCC or
k-BCC. For comparison, in the k-CC setting [7] pro-
vided a simple 3-approximation algorithm for k = 2,
while for k ≥ 2 [13] provided a PTAS for MaxAgree
and one for MinDisagree.
[15] improved on the
latter utilizing approximations schemes to the Gale-
Berlekamp switching game. Table 1 summarizes the
aforementioned results.

Finally, we note that our algorithm relies on adapting
ideas from [6] for approximately solving a combinato-
rially constrained quadratic maximization.

2 k-BCC as a Bilinear Maximization

We describe the k-BCC problem and show that it can
be formulated as a combinatorially constrained bilin-
ear maximization on the bi-adjacency matrix of the
input graph. Our k-BCC algorithm relies on approx-
imately solving that bilinear maximization.

Maximizing Agreements An instance of k-BCC
consists of an undirected, complete, bipartite graph
G = (U, V, E) whose edges have binary ±1 weights,
and a parameter k. The objective, referred to as
MaxAgree[k], is to compute a clustering of the ver-
tices into at most k clusters, such that the total num-
ber of positive edges in the interior of clusters plus the
number of negative edges across clusters is maximized.
Let E+ and E− denote the sets of positive and nega-
tive edges, respectively. Also, let m = |U| and n = |V |.
The bipartite graph G can be represented by its
weighted bi-adjacency matrix B ∈ {±1}m×n, where

Ref.

[13]
[13]
[15]

[11, 9]

[17]
[5]
[2]
[2]

Ours

Min./Max.

Guarantee

MaxAgree

MinDisagree
MinDisagree
MinDisagree

MaxAgree

MinDisagree
MinDisagree
MinDisagree

MaxAgree
MaxAgree

(E)PTAS

PTAS
PTAS

O(log n)-OPT

0.766-OPT

11-OPT
4-OPT
4-OPT

(E)PTAS
(E)PTAS

Complexity

n/δ · kO(δ−2 log k log(1/δ))

nO(100k/δ2) · log n
nO(9k/δ2) · log n

LP
SDP
LP
LP
|E|

√

k/δ) · (δ−2 + k) · n + TSVD(δ−2)
2O(k/δ2·log
2O(δ−3·log δ−3) · O(δ−2) · n + TSVD(δ−2)

D/R

Setting

R
R
R
D
D
D
D
R

R
R

k-CC
k-CC
k-CC

Inc. CC
Inc. CC

BCC
BCC
BCC

k-BCC
BCC

Table 1: Summary of results on BCC and related problems. For each scheme, we indicate the problem setting, the
objective (MaxAgree/MinDisagree), the guarantees (c-OPT implies a multiplicative factor approximation),
and its computational complexity (n denotes the total number of vertices, LP/SDP denotes the complexity of
a linear/semideﬁnite program, and TSVD(r) the time required to compute a rank-r truncated SVD of a n × n
matrix). The D/R column indicates whether the scheme is deterministic or randomized.

Bij is equal to the weight of edge (i, j).
Consider a clustering C of the vertices U ∪ V into at
most k clusters C1, . . . , Ck. Let X ∈ {0, 1}m×k be
the cluster assignment matrix for the vertices of U
associated with C, that is, Xij = 1 if and only if vertex
i ∈ U is assigned to cluster Cj. Similarly, let Y ∈
{0, 1}n×k be the cluster assignment for V .
Lemma 2.1. For any instance G = (U, V, E) of
the k-BCC problem with bi-adjacency matrix B ∈
{±1}|U|×|V |, and for any clustering C of U ∪ V into k
clusters, the number of agreements achieved by C is

Agree(cid:0)C(cid:1) = Tr(cid:0)X(cid:62)BY(cid:1) + |E−|,

where X ∈ {0, 1}|U|×k and Y ∈ {0, 1}|V |×k are the
cluster assignment matrices corresponding to C.

Proof. Let B+ and B− ∈ {0, 1}|U|×|V | be indicator
matrices for E+ and E−. Then, B = B+ − B−. Given
a clustering C = {Cj}k
j=1, or equivalently the assign-
ment matrices X ∈ {0, 1}|U|×k and Y ∈ {0, 1}|V |×k,
the number of pairs of similar vertices assigned to the

same cluster is equal to Tr(cid:0)X(cid:62)B+Y(cid:1) . Similarly, the
ferent clusters is equal to |E−| − Tr(cid:0)X(cid:62)B−Y(cid:1) . The
Agree(cid:0)C(cid:1) = Tr(cid:0)X(cid:62)B+Y(cid:1) + |E−| − Tr(cid:0)X(cid:62)B−Y(cid:1)

number of pairs of dissimilar vertices assigned to dif-
total number of agreements achieved by C is

= Tr(cid:0)X(cid:62)BY(cid:1) + |E−|,

which is the desired result.

It follows that computing a k-clustering that achieves
the maximum number of agreements, boils down to a

constrained bilinear maximization:

Tr(cid:0)X(cid:62)BY(cid:1) + |E−|, (2)

where

MaxAgree[k] = max

X∈X ,Y∈Y

X(cid:44)(cid:8)X ∈ {0, 1}m×k : (cid:107)X(cid:107)∞,1 = 1(cid:9),
Y(cid:44)(cid:8)Y ∈ {0, 1}n×k : (cid:107)Y(cid:107)∞,1 = 1(cid:9).

(3)

Here, (cid:107)X(cid:107)∞,1 denotes the maximum of the (cid:96)1-norm of
the rows of X. Since X ∈ {0, 1}|U|×k, the constraint
ensures that each row of X has exactly one nonzero
entry. Hence, X and Y describe the sets of valid cluster
assignment matrices for U and V , respectively.

In the sequel, we brieﬂy describe our approach for
approximately solving the maximization in (2) under
more general constraints, and subsequently apply it to
the k-BCC/MaxAgree problem.

3 Bilinear Maximization Framework

We describe a simple algorithm for computing an ap-
proximate solution to the constrained maximization

Tr(cid:0)X(cid:62)AY(cid:1) ,

max

X∈X ,Y∈Y

(4)

where the input argument A is a real m × n matrix,
and X , Y are norm-bounded sets. Our approach is
exponential in the rank of the argument A, which
can be prohibitive in practice. To mitigate this ef-
fect, we solve the maximization on a low-rank approx-

imation (cid:101)A of A, instead. The quality of the output
solving (4), operating directly on the rank-r matrix (cid:101)A.

depends on the spectrum of A and the rank r of the
surrogate matrix. Alg. 1 outlines our approach for

Bipartite Correlation Clustering – Maximizing Agreements

If we knew the optimal value for variable Y, then the
optimal value for variable X would be the solution to

PX (L) = arg max

for L = (cid:101)AY. Similarly, with R = (cid:101)A(cid:62)X for a given X,

X∈X

(5)

Tr(cid:0)X(cid:62)L(cid:1) ,
Tr(cid:0)R(cid:62)Y(cid:1)

PY (R) = arg max

Y∈Y

(6)

is the optimal value of Y for that X. Our algorithm
requires that such a linear maximization oracles PX (·)
and PY (·) exist.2 Of course, the optimal value for ei-
ther of the two variables is not known. It is known,

however, that the columns of the m×k matrix L = (cid:101)AY
lie in the r-dimensional range of (cid:101)A for all feasible

Y ∈ Y. Alg. 1 eﬀectively operates by sampling can-
didate values for L. More speciﬁcally, it considers a
large –exponential in r · k– collection of r × k matri-
ces C whose columns are points of an -net of the unit
(cid:96)2-ball Br−1
. Each matrix C is used to generate a ma-
trix L whose k columns lie in the range of the input

matrix (cid:101)A and in turn to produce a feasible solution
in (4) close to the optimal one (for argument (cid:101)A).
Lemma 3.2. For any real m × n, rank-r matrix (cid:101)A,

pair X, Y by successively solving (5) and (6). Due to
the properties of the -net, one of the computed fea-
sible pairs is guaranteed to achieve an objective value

and sets X ⊂ Rm×k and Y ⊂ Rn×k, let

2

(cid:0)(cid:101)X(cid:63),(cid:101)Y(cid:63)

(cid:1)(cid:44) arg max

X∈X ,Y∈Y

Tr(cid:0)X(cid:62)(cid:101)AY(cid:1).

If there exist linear maximization oracles PX and PY

as in (5) and (6), then Alg. 1 with input (cid:101)A and accu-
racy  ∈ (0, 1) outputs (cid:101)X ∈ X and (cid:101)Y ∈ Y such that
Tr(cid:0)(cid:101)X(cid:62)(cid:101)A(cid:101)Y(cid:1) ≥ Tr(cid:0)(cid:101)X(cid:62)
k · (cid:107)(cid:101)A(cid:107)2 · µX · µY ,
in time O(cid:0)(cid:0)2
r/(cid:1)r·k·(cid:0)TX + TY + (m + n)r(cid:1)(cid:1) + TSVD(r).
truncated SVD of (cid:101)A, while TX and TY denote the run-

where µX (cid:44) maxX∈X (cid:107)X(cid:107)F and µY(cid:44) maxY∈Y (cid:107)Y(cid:107)F,

Here, TSVD(r) denotes the time required to compute the

(cid:1) − 2

(cid:63) (cid:101)A(cid:101)Y(cid:63)

√

√

ning time of the two oracles.

The crux of Lemma 3.2 is that if there exists an eﬃ-
cient procedure to solve the simpler maximizations (5)
and (6), then we can approximately solve the bilinear
maximization (4) in time that depends exponentially

on the intrinsic dimension r of the input (cid:101)A, but poly-
Recall that (cid:101)A is only a low-rank approximation of the

nomially on the size of the input. A formal proof is
given in Appendix Sec. A.

potentially full rank original argument A. The guar-
antees of Lemma 3.2 can be translated to guarantees
2In the case of the k-BCC problem (2), where X and Y cor-
respond to the sets of cluster assignment matrices deﬁned in (3),
such optimization oracles exist (see Alg. 2).

2

)⊗k do

Algorithm 1 BiLinearLowRankSolver

{ (cid:101)Σ ∈ Rr×r }

{L ∈ Rm×k}
{R ∈ Rk×n}

{See Lemma 3.2.}
{Candidate solutions}

1: C ← {}
3: for each C ∈ (-net of Br−1
4:
5: X ← PX (L)
7: Y ← PY (R)

input : m × n real matrix (cid:101)A of rank r,  ∈ (0, 1)
output (cid:101)X ∈ X , (cid:101)Y ∈ Y
2: (cid:101)U,(cid:101)Σ,(cid:101)V ← SVD((cid:101)A)
L ← (cid:101)U(cid:101)ΣC
6: R ← X(cid:62)(cid:101)A
C ← C ∪(cid:8)(X, Y)(cid:9)
10: ((cid:101)X,(cid:101)Y) ← arg max(X,Y)∈C Tr(cid:0)X(cid:62)(cid:101)U(cid:101)Σ(cid:101)V(cid:62)Y(cid:1)
Algorithm 2 PX (·), X(cid:44)(cid:8)X ∈ {0, 1}m×k : (cid:107)X(cid:107)∞,1 = 1(cid:9)
output (cid:101)X = arg maxX∈X Tr(cid:0)X(cid:62)L(cid:1)
1: (cid:101)X ← 0m×k
(cid:101)Xiji ← 1
ji ← arg maxj∈[k] Lij

2: for i = 1, . . . , m do
3:

input : m × k real matrix L

8:
9: end for

4:
5: end for

for the original matrix introducing an additional error
term to account for the extra level of approximation.
Lemma 3.3. For any A ∈ Rm×n, let

(cid:0)X(cid:63), Y(cid:63)

(cid:1)(cid:44) arg max

X∈X ,Y∈Y

Tr(cid:0)X(cid:62)AY(cid:1),

where X and Y satisfy the conditions of Lemma 3.2.

Let (cid:101)A be a rank-r approximation of A, and (cid:101)X ∈ X ,
(cid:101)Y ∈ Y be the output of Alg. 1 with input (cid:101)A and accu-
Tr(cid:0)X(cid:62)
≤ 2 ·(cid:16)
(cid:17) · µX · µY .

(cid:1) − Tr(cid:0)(cid:101)X(cid:62)A(cid:101)Y(cid:1)
k · (cid:107)(cid:101)A(cid:107)2 + (cid:107)A − (cid:101)A(cid:107)2

racy . Then,

(cid:63) AY(cid:63)
√



Lemma 3.3 follows from Lemma 3.2. A formal proof is
deferred to Appendix Sec. A. This concludes the brief
discussion on our bilinear maximization framework.

4 Our k-BCC Algorithm

The k-BCC/MaxAgree problem on a bipartite
graph G = (U, V, E) can be written as a constrained
bilinear maximization (2) on the bi-adjacency matrix
B over the sets of valid cluster assignment matrices (3)
for V and U . Adopting the framework of the previous
section to approximately solve the maximization in (2)
we obtain our k-BCC algorithm.

The missing ingredient is a pair of eﬃcient procedures
PX (·) and PY (·), as described in Lemma 3.2 for solv-
ing (5) and (6), respectively, in the case where X and

Y are the sets of valid cluster assignment matrices (3).
Such a procedure exists and is outlined in Alg. 2.
Lemma 4.4. For any L ∈ Rm×k, Algorithm 2 outputs

Tr(cid:0)X(cid:62)L(cid:1),

arg max

X∈X

where X is deﬁned in (3), in time O(k · m).

A proof is provided in Appendix, Sec. B.
Note that in our case, Alg. 2 is used as both PX (·) and
PY (·). Putting the pieces together, we obtain the core
of our k-BCC algorithm, outlined in Alg. 3. Given
a bipartite graph G, with bi-adjacency matrix B, we

ﬁrst compute a rank-r approximation (cid:101)B of B via the
maximization (2) with argument (cid:101)B. The output is a

truncated SVD. Using Alg. 1, equipped with Alg. 2
as a subroutine, we approximately solve the bilinear

pair of valid cluster assignment matrices for U and V .

Alg. 3 exposes the conﬁguration parameters r and 
that control the performance of our bilinear solver, and
in turn the quality of the output k-clustering. To sim-
plify the description, we also create a k-BCC “wrap-
per” procedure outlined in Alg. 4: given a bound k on
the number of clusters and a single accuracy parameter
δ ∈ (0, 1), Alg. 4 conﬁgures and invokes Alg. 3.

Theorem 1. (k-BCC.) For any instance (cid:0)G =
(U, V, E), k(cid:1) of the k-BCC problem with bi-adjacency
(0, 1), Algorithm 4 computes a clustering (cid:101)C (k) of U ∪ V

matrix B and for any desired accuracy parameter δ ∈

into at most k clusters, such that

Agree(cid:0)(cid:101)C (k)(cid:1) ≥(cid:0)1 − δ(cid:1) · Agree(cid:0)C (k)

(cid:63)

(cid:1),

where C (k)
√
2O(k/δ2·log

(cid:63)

is

the optimal k-clustering,

k/δ) · (δ−2 + k) · (|U| + |V |) + TSVD(δ−2).

in time

√

√

mn, and the fact that (cid:107)X(cid:107)F =

m
n for all valid cluster assignment matrix

In the remainder of this section, we prove Theorem 1.
We begin with the core Alg. 3, which invokes the low-
rank bilinear solver Alg. 1 with accuracy parameters

 and r on the rank-r matrix (cid:101)B and computes a pair
of valid cluster assignment matrices (cid:101)X(k), (cid:101)Y(k). By
Lemma 3.2, and taking into account that σ1((cid:101)B) =
σ1(B) ≤ (cid:107)B(cid:107)F ≤ √
and (cid:107)Y(cid:107)F =
Tr(cid:0)(cid:101)X(k)
(cid:62)(cid:101)B(cid:101)Y(k)
pairs, the output pair satisﬁes
where(cid:0)(cid:101)X(k)
(cid:63) ,(cid:101)Y(k)
(cid:107)B − (cid:101)B(cid:107)2 ≤ √
Tr(cid:0)X(k)

(cid:1) − Tr(cid:0)(cid:101)X(k)(cid:62)(cid:101)B(cid:101)Y(k)(cid:1) ≤ 2 ·  · √
(cid:1)(cid:44) maxX∈X ,Y∈Y Tr(cid:0)X(cid:62)(cid:101)BY(cid:1).
(cid:1) − Tr(cid:0)(cid:101)X(k)(cid:62)B(cid:101)Y(k)(cid:1)

In turn, by Lemma 3.3 taking into account that
r (see Cor. 1), on the original

bi-adjacency matrix B the output pair satisﬁes

√
mn/

k · mn,

(cid:63)

(cid:63)

(cid:63)

(cid:63)

k · mn + 2(r + 1)−1/2 · mn,

(cid:62)BY(k)
√
≤ 2

(cid:63)

(cid:63) , Y(k)
(cid:63)

where (cid:0)X(k)

(cid:1)(cid:44) maxX∈X ,Y∈Y Tr(cid:0)X(cid:62)BY(cid:1). The
let (cid:101)C (k) be the clustering in-
duced by the computed pair (cid:101)X(k), (cid:101)Y(k). From (7) and

unknown optimal pair X(k)
represents the opti-
mal k-clustering C (k)
(cid:63) , i.e., the clustering that achieves
the maximum number of agreements using at most
k clusters. Similarly,

(cid:63) , Y(k)
(cid:63)

Lemma 2.1, it immediately follows that

Agree(cid:0)C (k)

(cid:63)

(cid:1) − Agree(cid:0)(cid:101)C (k)(cid:1)

≤ 2 · (

k +

√

√

r + 1) · mn.

(8)

Eq. 8 establishes the guarantee of Alg. 3 as a function
of its accuracy parameters r and . Substituting those
parameters by the values assigned by Alg. 4, we obtain

(cid:1) − Agree(cid:0)(cid:101)C (k)(cid:1) ≤ 2−1 · δ · mn.

Agree(cid:0)C (k)

(9)

(cid:63)

Fact 1. For any BCC instance (or k-BCC with
k ≥ 2), the optimal clustering achieves at least mn/2
agreements.

Proof. If more than half of the edges are labeled
‘+’, a single cluster containing all vertices achieves at
least nm/2 agreements. Otherwise, two clusters cor-
responding to U and V achieve the same result.

In other words, Fact 1 states that

(cid:1) ≥ mn/2,

Agree(cid:0)C (k)
Agree(cid:0)(cid:101)C (k)(cid:1) ≥ (1 − δ) · Agree(cid:0)C (k)

∀k ≥ 2.

(cid:63)

(cid:63)

(10)

(cid:1),

Combining (10) with (9), we obtain

which is the desired result. Finally, the computational
complexity is obtained substituting the appropriate
values in that of Alg. 1 as in Lemma 3.2, and the time
complexity of subroutine Alg. 2 given in Lemma 4.4.
This completes the proof of Theorem 1.

5 An Eﬃcient PTAS for BCC

We provide an eﬃcient polynomial time approxima-
tion scheme (EPTAS) for BCC/MaxAgree, i.e., the
unconstrained version with no upper bound on the
number of output clusters. We rely on the following
key observation: any constant factor approximation of
the MaxAgree objective, can be achieved by constant
number of clusters.3 Hence, the desired approximation
algorithm for BCC can be obtained by invoking the k-
BCC algorithm under the appropriate conﬁguration.

Recall that in the unconstrained BCC setting, the
number of output clusters is an optimization variable;
the optimal clustering may comprise any number of
clusters, from a single cluster containing all vertices of

(7)

3A similar result for CC/MaxAgree exists in [7].

Bipartite Correlation Clustering – Maximizing Agreements

Algorithm 3 k-BCC via Low-rank Bilinear Maximization
input : • Bi-adjacency matrix B ∈ {±1}m,n of bipartite graph G = (U, V, E),

• Target number of clusters k,

Equivalently, cluster membership matrices (cid:101)X(k) ∈ X and (cid:101)Y(k) ∈ Y for U and V , respectively.

output Clustering (cid:101)C (k) of U ∪ V into k clusters –
1: (cid:101)B ← SVD(B, r)
2: Approx. solve the bilinear maximization (2) on (cid:101)B, over the sets X , Y of valid cluster assignment matrices (Eq. 3):
(cid:101)X(k),(cid:101)Y(k) ← BiLinearLowRankSolver(cid:0)(cid:101)B, , r(cid:1)

{Alg. 1, using Alg. 2 as PX (·) and PY (·).}

• Accuracy parameter  ∈ (0, 1).

• Approximation rank r,

{Truncated SVD.}

Algorithm 4 k-BCC/MaxAgree
input : Bi-adjacency matrix B ∈ {±1}m,n,

Target number of k,
Accuracy δ ∈ (0, 1).

output Clustering (cid:101)C (k) of U ∪ V such that
Agree(cid:0)(cid:101)C (k)(cid:1) ≥(cid:0)1 − δ(cid:1) · Agree(cid:0)C (k)

(cid:63)

(cid:1).

1: Set up parameters:

 ← 2−3 · δ · k−1/2, r ← 26 · δ−2 − 1.

2: Return output of Alg. 3 for input (B, k, r, ).

the graph, up to |U| + |V | singleton clusters. In prin-
ciple, one could solve MaxAgree[k] for all possible
values of the parameter k to identify the best cluster-
ing, but this approach is computationally intractable.

On the contrary, if one is willing to settle for an ap-
proximately optimal solution, then a constant number
of clusters may suﬃce. More formally,
Lemma 5.5. For any BCC instance, and 0 <  ≤ 1,
there exists a clustering C with at most k = 2 · −1 + 2
clusters such that Agree(C) ≥ Agree(C(cid:63)) −  · nm,
where C(cid:63) denotes the optimal clustering.

We defer the proof to the end of the section.

In conjunction with Fact 1, Lemma 5.5 suggests that to
obtain a constant factor approximation for the uncon-
strained BCC/MaxAgree problem, for any constant
arbitrarily close to 1, it suﬃces to solve a k-BCC in-
stance for a suﬃciently large –but constant– number
of clusters k. In particular, to obtain an (1 − δ)-factor
approximation for any constant δ ∈ (0, 1), it suﬃces
to solve the k-BCC problem with an upper bound
k = O(δ−1) on the number of clusters.
Alg. 5 outlines the approximation scheme for BCC.
For a given accuracy parameter δ, it invokes Alg. 3
under an appropriate conﬁguration of the parameters
k, r and , yielding the following guarantees:

(0, 1), Algorithm 5 computes a clustering (cid:101)C of U ∪ V

Theorem 2. (PTAS for BCC.) For any instance
G = (U, V, E) of the BCC problem with bi-adjacency
matrix B and for any desired accuracy parameter δ ∈
into (at most) 23 · δ−1 clusters, such that

Agree(cid:0)(cid:101)C(cid:1) ≥(cid:0)1 − δ(cid:1) · Agree(cid:0)C(cid:63)

(cid:1),

Algorithm 5 A PTAS for BCC/MaxAgree
input : Bi-adjacency matrix B ∈ {±1}m,n,

output Clustering (cid:101)C of U ∪ V (into at most
(cid:1).
Agree(cid:0)(cid:101)C(cid:1) ≥(cid:0)1 − δ(cid:1) · Agree(cid:0)C(cid:63)

Accuracy δ ∈ (0, 1).
23 · δ−1 clusters) such that

1: Set up parameters:

k ← 23 · δ−1,

 ← 2−6 · δ2, r ← 28 · δ−2 − 1.
2: Return output of Alg. 3 for input (B, k, r, ).

where C(cid:63) is an optimal clustering (with no constraint
on the number of clusters), in time

2O(δ−3·log δ−3) · δ−2 · (m + n) + TSVD(δ−2).

core k-BCC Alg. 3 returns a clustering (cid:101)C with at most

The proof of Theorem 2 follows from the guarantees of
Alg. 3 in (8) substituting the values of the parameters
k, r and  with the values speciﬁed by Alg 5. The
k0 = 23 · δ−1 clusters that satisﬁes

Agree(cid:0)(cid:101)C(cid:1) ≥ Agree(cid:0)C (k0)

(cid:63)

(cid:1) − δ

4 · mn,

(11)

(cid:63)

(cid:63)

where C (k0 )
is the best among the clusterings using at
most k0 clusters. Also, for k = k0, Lemma 5.5 implies
that Agree(C (k0 )
) ≥ Agree(C(cid:63)) − δ/4 · nm, where C(cid:63) is
the optimal clustering without any constraint on the
number of output clusters. Hence,

(cid:1) − δ/2 · mn.
Agree(cid:0)(cid:101)C(cid:1) ≥ Agree(cid:0)C(cid:63)
we conclude that the output (cid:101)C of Alg. 5 satisﬁes
(cid:1),
Agree(cid:0)(cid:101)C(cid:1) ≥ (1 − δ) · Agree(cid:0)C(cid:63)

Continuing from (12), and taking into account Fact 1,

(12)

(13)

which is the desired result. Finally, the computational
complexity of Alg. 5 follows from that of Alg. 3 substi-
tuting the parameter values in Lemma 3.2, taking into
account the running time TX = O(δ−2 · m) of Alg. 2
used as subroutine PX (·) and similarly TY = O(δ−2·n)
for PY (·). This concludes the proof of Theorem 2.
For any desired constant accuracy δ ∈ (0, 1), Alg. 5
outputs a clustering that achieves a number of agree-
ments within a (1− δ)-factor from the optimal, in time

that grows exponentially in δ−1, but linearly in the
size mn of the input.
In other words, Alg. 5 is an
EPTAS for BCC/MaxAgree.

We complete the section with a proof for Lemma 5.5,
which stated that a constant number of clusters suﬃces
to obtain an approximately optimal solution.

5.1 Proof of Lemma 5.5
It suﬃces to consider  > 2/(m + n − 2) as the lemma
holds trivially otherwise. Without loss of generality,
we focus on clusterings whose all but at most two clus-
ters contain vertices from both U and V ; one of the
two remaining clusters can contain vertices only from
U and the other only from V . To verify that, consider
an arbitrary clustering C and let C(cid:48) be the clustering
obtained by merging all clusters of C containing only
vertices of U into a single cluster, and those containing
only vertices of V into another. The number of agree-
ments is not aﬀected, i.e., Agree(C(cid:48)) = Agree(C).
We show that a clustering C with the properties de-
scribed in the lemma exists by modifying C(cid:63). Let
SU ⊆ C(cid:63) be the set of clusters that contain at most
m/2 vertices of U , and SV ⊆ C(cid:63) those containing at
most n/2 vertices of V . (SU and SV may not be dis-
joint). Finally, let B be the set of vertices contained in
clusters of SU ∪ SV . We construct C as follows. Clus-
ters of C(cid:63) not in SU ∪ SV are left intact. Each such
cluster has size at least (n + m)/2. Further, all ver-
tices in B are rearranged into two clusters: one for the
vertices in B ∩ U and one those in B ∩ V .
The above rearrangement can reduce the number of
agreements by at most nm. To verify that, consider
a cluster C in SU ; the cluster contains at most m/2
vertices of U . Let t(cid:44)|V ∩ C|. Splitting C into two
smaller clusters C∩U and C∩V can reduce the number
of agreements by 1
2 mt, i.e., the total number of edges
in C. Note that agreements on − edges are not aﬀected
by splitting a cluster. Performing the same operation
on all clusters in SU incurs a total reduction
2 mn.

2 m|V | = 1

|V ∩ C| ≤ 1

1
2 m

(cid:88)

C∈SU

Repeating on SV incurs an additional cost of at most
mn/2 agreements, while merging all clusters contain-
ing only vertices from U (similarly for V ) into a single
cluster does not reduce the agreements. We conclude
that Agree(C) ≥ Agree(C(cid:63)) − nm.
Finally, by construction all but at most two clusters
in C contain at least 1

n + m =

2 (n + m). In turn,
|C| ≥ (|C| − 2) · 1

2 (n + m),

(cid:88)

C∈C

from which the desired result follows.

6 Experiments

We evaluate our algorithm on synthetic and real data
and compare with PivotBiCluster [2] and the SDP-
based approach of [17]4 Although [5, 9, 11] provide
algorithms applicable to BCC, those rely on LP or
SDP formulations with a high number of constraints
which imposes limitations in practice.

We run our core k-BCC algorithm (Alg. 3) to obtain a
k-clustering. Disregarding the theoretical guarantees,
we apply a threshold on the execution time which can
only hurt the performance; equivalently, the iterative
procedure of Alg. 1 is executed for an arbitrary subset
of the points in the -net (Alg. 1, step 3).

6.1 Synthetic Data

We generate synthetic BCC instances as follows. We
arbitrarily set m = 100, n = 50, and number of clus-
ters k(cid:48) = 5, and construct a complete bipartite graph
G = (U, V, E) with |U| = m, |V | = n and assign bi-
nary ±1 labels to the edges according to a random
k(cid:48)-clustering of the vertices. Then, we modify the sign
of each label independently with probability p. We
consider multiple values of p in the range [0, 0.5]. For
each value, we generate 10 random instances as de-
scribed above and compute a vertex clustering using
all algorithms.

PivotBiCluster returns a clustering of arbitrary size;
no parameter k is speciﬁed and the algorithm deter-
mines the number of clusters in an attempt to maxi-
mize the number of agreements. The majority of the
output clusters are singletons which can be merged
into two clusters (see Subsec. 5.1). The algorithm is
substantially faster that the other approaches on ex-
amples of this scale. Hence, we run PivotBiCluster
with 3 · 104 random restarts on each instance and de-
pict best results to allow for comparable running times.
Contrary to PivotBiCluster, for our algorithm, which
is reﬀered to as BccBilinear, we need to specify the tar-
get number k of clusters. We run it for k = 5, 10 and
15 and arbitrarily set the parameter r = 5 and termi-
nate our algorithm after 104 random samples/rounds.
Finally, the SDP-based approach returns 4 clusters by
construction, and is conﬁgured to output best results
among 100 random pivoting steps.

Fig. 1 depicts the number of agreements achieved by
each algorithm for each value of p, the number of out-
put clusters, and the execution times. All numbers are
averages over multiple random instances.

We note that in some cases and contrary to intuition,

4All algorithms are prototypically implemented in Matlab.

[17] was implemented using CVX.

Bipartite Correlation Clustering – Maximizing Agreements

Dataset

m (Users)

n (Movies) Ratings

MovieLens100K
MovieLens1M
MovieLens10M

1000
6000
72000

1700
4000
10000

105
106
107

Table 2: Summary of MovieLens datasets [14].

tices. For a reference, we compare to PivotBiCluster
with 50 random restarts. Note, however, that Piv-
otBiCluster is not designed for the incomplete BCC
problem; to apply the algorithm, we eﬀectively treat
missing edges as edges of negative weight. Finally, the
SDP approach of [17], albeit suitable for the incom-
plete CC problem, does not scale to this size of input.
Table 3 lists the number of agreements achieved by
each method on each one of the three datasets and the
corresponding execution times.

7 Conclusions

We presented the ﬁrst algorithm with provable approx-
imation guarantees for k-BCC/MaxAgree. Our ap-
proach relied on formulating k-BCC as a constrained
bilinear maximization over the sets of cluster assign-
ment matrices and developing a simple framework to
approximately solve that combinatorial optimization.

In the unconstrained BCC setting, with no bound on
the number of output clusters, we showed that any
constant multiplicative factor approximation for the
MaxAgree objective can be achieved using a con-
stant number of clusters.
In turn, under the appro-
priate conﬁguration, our k-BCC algorithm yields an
Eﬃcient PTAS for BCC/MaxAgree.

Acknowledgments This research has been sup-
ported by NSF Grants CCF 1344179, 1344364,
1407278, 1422549 and ARO YIP W911NF-14-1-0258.
DP is supported by NSF awards CCF-1217058 and
CCF-1116404 and MURI AFOSR grant 556016.

MovieLens

100K

1M

10M

PivotBiCluster

BccBilinear

46134
(27.95)
68141
(6.65)

429277
(651.13)
694366
(19.50)

5008577
(1.5 · 105)
6857509
(1.2 · 103)

Table 3: Number of agreements achieved by the two
algorithms on incomplete (k)-BCC instances obtained
from the MovieLens datasets [14]. For PivotBiClus-
ter we present best results over 50 random restarts.
Our algorithm was arbitrarily conﬁgured with r = 4,
k = 10 and a limit of 104 samples (iterations). We also
note in parenteses the runtimes in seconds.

p = 0.0

PivotBiCluster [2] 124
69
BccBil. (k=5)
73
BccBil. (k=10)
BccBil. (k=15)
78
SDP Inc CC [17]
198

0.1

83
70
74
79
329

0.2

66
70
75
79
257

0.3

54
71
75
80
258

0.4

45
71
75
80
266

0.5

39
71
75
80
314

Average runtimes/instance (in seconds).

Figure 1: Synthetic data. We generate an m×n bipar-
tite graph with edge weights ±1 according to a ran-
dom vertex k-clustering and subsequently ﬂip the sign
of each edge with probability p. We plot the average
number of agreements achieved by each method over
10 random instances for each p value. The bar plot
depicts the average number of output clusters for each
scheme/p-value pair. Within each bar, a horizontal
line marks the number of singleton clusters.

our algorithm performs better for lower values of k,
the target number of clusters. We attribute this phe-
nomenon to the fact that for higher values of k, we
should typically use higher approximation rank and
consider larger number of samples.

6.2 MovieLens Dataset

The MovieLens datasets [14] are sets of movie ratings:
each of m users assigns scores in {1, . . . , 5} to a small
subset of the n movies. Table 2 lists the dimensions
of the datasets. From each dataset, we generate an
instance of the (incomplete) BCC problem as follows:
we construct the bipartite graph on the user-movie
pairs using the ratings as edge weights, compute the
average weight and ﬁnally set weights higher than the
average to +1 and the others to −1.
We run our algorithm to obtain a clustering of the ver-

0  0.10.20.30.40.5EdgeFlipprobability,p250030003500400045005000NumberofAgreements(|U|=100,|V|=50,Truek=10,10Instances)PivotBiCluster(30000rr)BccBilinear,k=5BccBilinear,k=10BccBilinear,k=15SDPInc.CC0  0.10.20.30.40.5EdgeFlipprobability,p010203040506070Avg#ofClustersPivotBiCluster(30000rr)BccBilinear,k=5BccBilinear,k=10BccBilinear,k=15SDPInc.CCReferences

[1] KookJin Ahn, Graham Cormode, Sudipto Guha,
Andrew McGregor, and Anthony Wirth. Corre-
lation clustering in data streams. In Proceedings
of the 32nd International Conference on Machine
Learning (ICML-15), pages 2237–2246, 2015.

[2] Nir Ailon, Noa Avigdor-Elgrabli, Edo Liberty,
and Anke Van Zuylen.
Improved approxima-
tion algorithms for bipartite correlation cluster-
ing. SIAM Journal on Computing, 41(5):1110–
1121, 2012.

[3] Nir Ailon, Moses Charikar, and Alantha New-
man. Aggregating inconsistent information: rank-
ing and clustering. Journal of the ACM (JACM),
55(5):23, 2008.

[4] Nir Ailon and Edo Liberty. Correlation clustering
revisited: The “true” cost of error minimization
problems. In Automata, Languages and Program-
ming, pages 24–36. Springer, 2009.

[5] Noga Amit. The bicluster graph editing problem.

Master’s thesis, Tel Aviv University, 2004.

[6] Megasthenis Asteris, Dimitris Papailiopoulos,
Anastasios Kyrillidis, and Alexandros G Dimakis.
Sparse pca via bipartite matchings. In Advances
in Neural Information Processing Systems 28,
pages 766–774. Curran Associates, Inc., 2015.

[7] Nikhil Bansal, Avrim Blum, and Shuchi Chawla.
Correlation clustering. Machine Learning, 56(1-
3):89–113, 2004.

[8] Francesco Bonchi, David Garcia-Soriano, and Edo
Liberty. Correlation clustering:
from theory to
practice. In Proceedings of the 20th ACM Inter-
national Conference on Knowledge Discovery and
Data mining, pages 1972–1972. ACM, 2014.

[9] Moses Charikar, Venkatesan Guruswami, and An-
thony Wirth. Clustering with qualitative informa-
tion. In Foundations of Computer Science, 2003.
Proceedings. 44th Annual IEEE Symposium on,
pages 524–533. IEEE, 2003.

[10] William W Cohen and Jacob Richman. Learn-
ing to match and cluster large high-dimensional
data sets for data integration. In Proceedings of
the 8th ACM International Conference on Knowl-
edge Discovery and Data mining, pages 475–480.
ACM, 2002.

[11] Erik D Demaine, Dotan Emanuel, Amos Fiat, and
Nicole Immorlica. Correlation clustering in gen-
eral weighted graphs. Theoretical Computer Sci-
ence, 361(2):172–187, 2006.

[12] Xiaoli Zhang Fern and Carla E Brodley. Solving
cluster ensemble problems by bipartite graph par-
titioning. In Proceedings of the 21st International

Conference on Machine Learning, page 36. ACM,
2004.

[13] Ioannis Giotis and Venkatesan Guruswami. Cor-
relation clustering with a ﬁxed number of clusters.
In Proceedings of the 17th annual ACM-SIAM
Symposium on Discrete algorithms, pages 1167–
1176. Society for Industrial and Applied Mathe-
matics, 2006.

[14] University

of Minnesota GroupLens Lab.
Movielens datasets.
http://grouplens.org/
datasets/movielens/, 2015. Accessed: 2015-
10-03.

[15] Marek Karpinski and Warren Schudy. Linear time
approximation schemes for the gale-berlekamp
game and related minimization problems. In Pro-
ceedings of the 41st annual ACM Symposium on
Theory of Computing, pages 313–322. ACM, 2009.

[16] Sara C Madeira and Arlindo L Oliveira. Bicluster-
ing algorithms for biological data analysis: a sur-
vey. IEEE/ACM Transactions on Computational
Biology and Bioinformatics, 1(1):24–45, 2004.

[17] Chaitanya Swamy. Correlation clustering: maxi-
mizing agreements via semideﬁnite programming.
In Proceedings of the 15th annual ACM-SIAM
Symposium on Discrete Algorithms, pages 526–
527. Society for Industrial and Applied Mathe-
matics, 2004.

[18] Panagiotis Symeonidis, Alexandros Nanopoulos,
Apostolos Papadopoulos, and Yannis Manolopou-
los. Nearest-biclusters collaborative ﬁltering with
constant values. In Advances in web mining and
web usage analysis, pages 36–55. Springer, 2007.

[19] Anke Van Zuylen and David P Williamson. Deter-
ministic pivoting algorithms for constrained rank-
ing and clustering problems. Mathematics of Op-
erations Research, 34(3):594–620, 2009.

[20] Michail Vlachos, Francesco Fusco, Charalambos
Mavroforakis, Anastasios Kyrillidis, and Vassil-
ios G Vassiliadis.
Improving co-cluster quality
with application to product recommendations. In
Proceedings of the 23rd ACM International Con-
ference on Information and Knowledge Manage-
ment, pages 679–688. ACM, 2014.

[21] Hongyuan Zha, Xiaofeng He, Chris Ding, Horst
Simon, and Ming Gu. Bipartite graph partition-
ing and data clustering.
In Proceedings of the
10th ACM International Conference on Informa-
tion and Knowledge Management, pages 25–32.
ACM, 2001.

Bipartite Correlation Clustering – Maximizing Agreements

Tr(cid:0)X(cid:62)(cid:101)AY(cid:1).
Tr(cid:0)X(cid:62)L(cid:1)
Tr(cid:0)R(cid:62)Y(cid:1)
(cid:1) − 2

√

A Bilinear Maximization Guarantees

Lemma 3.2. For any real m × n, rank-r matrix (cid:101)A

and arbitrary norm-bounded sets X ⊂ Rm×k and
Y ⊂ Rn×k, let

(cid:0)(cid:101)X(cid:63),(cid:101)Y(cid:63)

(cid:1)(cid:44) arg max

X∈X ,Y∈Y

If there exist operators PX : Rm×k → X such that

PX (L) = arg max

X∈X

and similarly, PY : Rn×k → Y such that

PY (R) = arg max

Y∈Y

√

(cid:63) (cid:101)A(cid:101)Y(cid:63)

with running times TX and TY , respectively, then Al-

where µX (cid:44) maxX∈X (cid:107)X(cid:107)F and µY(cid:44) maxY∈Y (cid:107)Y(cid:107)F,

gorithm 1 outputs (cid:101)X ∈ X and (cid:101)Y ∈ Y such that
Tr(cid:0)(cid:101)X(cid:62)(cid:101)A(cid:101)Y(cid:1) ≥ Tr(cid:0)(cid:101)X(cid:62)
k · (cid:107)(cid:101)A(cid:107)2 · µX · µY ,
in time O(cid:0)(cid:0)2
r/(cid:1)r·k·(cid:0)TX +TY +(m+n)r(cid:1)(cid:1)+TSVD(r).
Proof. In the sequel, (cid:101)U, (cid:101)Σ and (cid:101)V are used to denote
the r-truncated singular value decomposition of (cid:101)A.
the singular values of (cid:101)A by a factor of µX · µY . Then,
Let (cid:101)X(cid:63),(cid:101)Y(cid:63) be the optimal pair on (cid:101)A, i.e.,
Tr(cid:0)X(cid:62)(cid:101)AY(cid:1)
and deﬁne the r × k matrix (cid:101)C(cid:63)(cid:44)(cid:101)V(cid:62)(cid:101)Y(cid:63). Note that

Without loss of generality, we assume that µX = µY =
1 since the variables in X and Y can be normalized by
µX and µY , respectively, while simultaneously scaling
(cid:107)Y(cid:107)∞,2 ≤ 1, ∀ Y ∈ Y, where (cid:107)Y(cid:107)∞,2 denotes the
maximum of the (cid:96)2-norm of the columns of Y.

(cid:1)(cid:44) arg max
(cid:0)(cid:101)X(cid:63),(cid:101)Y(cid:63)
(cid:107)(cid:101)C(cid:63)(cid:107)∞,2 = (cid:107)(cid:101)V(cid:62)(cid:101)Y(cid:63)(cid:107)∞,2

X∈X ,Y∈Y

(cid:107)(cid:101)V(cid:62)[(cid:101)Y(cid:63)]:,i(cid:107)2 ≤ 1,

(14)

= max
1≤i≤k

(cid:107)Y(cid:107)∞,2 ≤ 1 ∀ Y ∈ Y and the columns of (cid:101)V are or-

with the last inequality following from the facts that
thonormal. Alg. 1 iterates over the points in (Br−1
)⊗k.
The latter is used to describe the set of r × k matrices
whose columns have (cid:96)2 norm at most equal to 1. At
each point, the algorithm computes a candidate solu-
tion. By (14), the -net contains an r × k matrix C(cid:93)
such that

2

Let X(cid:93), Y(cid:93) be the candidate pair computed at C(cid:93) by
the two step maximization, i.e.,

(cid:107)C(cid:93) −(cid:101)C(cid:63)(cid:107)∞,2 ≤ .
Tr(cid:0)X(cid:62)(cid:101)U(cid:101)ΣC(cid:93)

(cid:1)

X(cid:93)(cid:44) arg max
X∈X

and

Tr(cid:0)X(cid:62)

(cid:93) (cid:101)AY(cid:1).

Y(cid:93)(cid:44) arg max
Y∈Y

(15)

We show that the objective values achieved by the can-
didate pair X(cid:93), Y(cid:93) satisﬁes the inequality of the lemma
implying the desired result.

By the deﬁnition of (cid:101)C(cid:63) and the linearity of the trace,
Tr(cid:0)(cid:101)X(cid:62)
(cid:1)
(cid:63) (cid:101)A(cid:101)Y(cid:63)
= Tr(cid:0)(cid:101)X(cid:62)
(cid:63) (cid:101)U(cid:101)Σ(cid:101)C(cid:63)
= Tr(cid:0)(cid:101)X(cid:62)
(cid:63) (cid:101)U(cid:101)ΣC(cid:93)
≤ Tr(cid:0)X(cid:62)
(cid:93) (cid:101)U(cid:101)ΣC(cid:93)

(cid:63) (cid:101)U(cid:101)Σ(cid:0)(cid:101)C(cid:63) − C(cid:93)
(cid:63) (cid:101)U(cid:101)Σ(cid:0)(cid:101)C(cid:63) − C(cid:93)

(cid:1)
(cid:1) + Tr(cid:0)(cid:101)X(cid:62)
(cid:1) + Tr(cid:0)(cid:101)X(cid:62)

(cid:1)(cid:1)
(cid:1)(cid:1).

(16)

The inequality follows from the fact that (by deﬁni-
tion (15)) X(cid:93) maximizes the ﬁrst term over all X ∈ X .
We compute an upper bound on the right hand side

Y∈Y

(cid:107)(cid:101)V (cid:62)Y − C(cid:93)(cid:107)∞,2.

explicitly calculated.) Further, deﬁne the r× k matrix

of (16). Deﬁne(cid:98)Y(cid:44) arg min
(We note that (cid:98)Y is used for the analysis and is never
(cid:98)C(cid:44)(cid:101)V(cid:62)(cid:98)Y. By the linearity of the trace operator
Tr(cid:0)X(cid:62)
(cid:93) (cid:101)U(cid:101)ΣC(cid:93)
(cid:93) (cid:101)U(cid:101)Σ(cid:0)C(cid:93) −(cid:98)C(cid:1)(cid:1)
(cid:93) (cid:101)U(cid:101)Σ(cid:98)C(cid:1) + Tr(cid:0)X(cid:62)
= Tr(cid:0)X(cid:62)
(cid:93) (cid:101)U(cid:101)Σ(cid:0)C(cid:93) −(cid:98)C(cid:1)(cid:1)
(cid:93) (cid:101)U(cid:101)Σ(cid:101)V(cid:62)(cid:98)Y(cid:1) + Tr(cid:0)X(cid:62)
= Tr(cid:0)X(cid:62)
≤ Tr(cid:0)X(cid:62)
(cid:1) + Tr(cid:0)X(cid:62)
(cid:93) (cid:101)U(cid:101)Σ(cid:0)C(cid:93) −(cid:98)C(cid:1)(cid:1)
(cid:93) (cid:101)U(cid:101)Σ(cid:101)V(cid:62)Y(cid:93)
= Tr(cid:0)X(cid:62)
(cid:1) + Tr(cid:0)X(cid:62)
(cid:93) (cid:101)U(cid:101)Σ(cid:0)C(cid:93) −(cid:98)C(cid:1)(cid:1).
(cid:93) (cid:101)AY(cid:93)

(17)

(cid:1)

The inequality follows from the fact that (by deﬁni-
tion (15)) Y(cid:93) maximizes the ﬁrst term over all Y ∈ Y.
Combining (17) and (16), and rearranging the terms,
we obtain

(cid:93) (cid:101)U(cid:101)Σ(cid:0)C(cid:93) −(cid:98)C(cid:1)(cid:1).

By Lemma C.10,

(cid:1)

(cid:1) − Tr(cid:0)X(cid:62)
Tr(cid:0)(cid:101)X(cid:62)
(cid:93) (cid:101)AY(cid:93)
(cid:63) (cid:101)A(cid:101)Y(cid:63)
(cid:63) (cid:101)U(cid:101)Σ(cid:0)(cid:101)C(cid:63) − C(cid:93)
≤ Tr(cid:0)(cid:101)X(cid:62)
(cid:1)(cid:1) + Tr(cid:0)X(cid:62)
(cid:12)(cid:12)Tr(cid:0)(cid:101)X(cid:62)
(cid:1)(cid:1)(cid:12)(cid:12)
(cid:63) (cid:101)U(cid:101)Σ(cid:0)(cid:101)C(cid:63) − C(cid:93)
(cid:63) (cid:101)U(cid:107)F · (cid:107)(cid:101)Σ(cid:107)2 · (cid:107)(cid:101)C(cid:63) − C(cid:93)(cid:107)F
≤ (cid:107)(cid:101)X(cid:62)
≤ (cid:107)(cid:101)X(cid:63)(cid:107)F · σ1((cid:101)A) ·
X∈X (cid:107)X(cid:107)F · σ1((cid:101)A) ·
≤ σ1((cid:101)A) ·

≤ max

k · 
√

k · .

k · 

√

√

(18)

(19)

Similarly,

(cid:12)(cid:12)Tr(cid:0)X(cid:62)

(cid:93) (cid:101)U(cid:101)Σ(cid:0)C(cid:93) −(cid:98)C(cid:1)(cid:1)(cid:12)(cid:12) ≤ (cid:107)X(cid:93)(cid:101)U(cid:107)F · (cid:107)(cid:101)Σ(cid:107)2 · (cid:107)C(cid:93) −(cid:98)C(cid:107)F

Tr(cid:0)(cid:101)X(cid:62)(cid:101)A(cid:101)Y(cid:1)

By the linearity of the trace operator,

≤ max

X∈X (cid:107)X(cid:107)F · σ1((cid:101)A) ·
≤ σ1((cid:101)A) ·

k · .

√

√

k · 

(20)

The second inequality follows from the fact that by the

deﬁnition of (cid:98)C,
(cid:107)(cid:98)C − C(cid:93)(cid:107)∞,2 = (cid:107)(cid:101)V (cid:62)(cid:98)Y − C(cid:93)(cid:107)∞,2 ≤ (cid:107)(cid:101)V (cid:62)(cid:101)Y(cid:63) − C(cid:93)(cid:107)∞,2

By Lemma C.10,

Continuing from (22),

= Tr(cid:0)(cid:101)X(cid:62)A(cid:101)Y(cid:1) − Tr(cid:0)(cid:101)X(cid:62)(A − (cid:101)A)(cid:101)Y(cid:1)
≤ Tr(cid:0)(cid:101)X(cid:62)A(cid:101)Y(cid:1) +(cid:12)(cid:12)Tr(cid:0)(cid:101)X(cid:62)(A − (cid:101)A)(cid:101)Y(cid:1)(cid:12)(cid:12).
(cid:12)(cid:12)Tr(cid:0)(cid:101)X(cid:62)(A − (cid:101)A)(cid:101)Y(cid:1)(cid:12)(cid:12)
≤ (cid:107)(cid:101)X(cid:107)F · (cid:107)(cid:101)Y(cid:107)F · (cid:107)A − (cid:101)A(cid:107)2
≤ (cid:107)A − (cid:101)A(cid:107)2 · max
X∈X (cid:107)X(cid:107)F · max

Y∈Y (cid:107)Y(cid:107)F (cid:44) R.
Tr(cid:0)(cid:101)X(cid:62)(cid:101)A(cid:101)Y(cid:1) ≤ Tr(cid:0)(cid:101)X(cid:62)A(cid:101)Y(cid:1) + R.
Tr(cid:0)X(cid:62)
(cid:63) (cid:101)AY(cid:63)
= Tr(cid:0)X(cid:62)
≥ Tr(cid:0)X(cid:62)
≥ Tr(cid:0)X(cid:62)

(cid:1)
(cid:63) (A − (cid:101)A)Y(cid:63)
(cid:1)(cid:12)(cid:12)
(cid:63) (A − (cid:101)A)Y(cid:63)

(cid:1) − Tr(cid:0)X(cid:62)
(cid:1) −(cid:12)(cid:12)Tr(cid:0)X(cid:62)
(cid:1) − R.
Tr(cid:0)(cid:101)X(cid:62)A(cid:101)Y(cid:1) ≥ Tr(cid:0)(cid:101)X(cid:62)(cid:101)A(cid:101)Y(cid:1) − R
≥ γ · Tr(cid:0)X(cid:62)
(cid:63) (cid:101)AY(cid:63)
≥ γ ·(cid:0)Tr(cid:0)X(cid:62)
= γ · Tr(cid:0)X(cid:62)
≥ γ · Tr(cid:0)X(cid:62)

(cid:63) AY(cid:63)
(cid:63) AY(cid:63)
(cid:63) AY(cid:63)

(cid:63) AY(cid:63)
(cid:63) AY(cid:63)
(cid:63) AY(cid:63)

(cid:1)

(cid:1) − R − C
(cid:1) − R(cid:1) − R − C
(cid:1) − (1 + γ) · R − C
(cid:1) − 2 · R − C,

(22)

(23)

(24)

(25)

Combining the above, we have

Similarly,

where the ﬁrst inequality follows from (24) the second
from (21), the third from (25), and the last from the
fact that R ≥ 0. This concludes the proof.
Lemma 3.3. For any A ∈ Rm×n, let

(cid:0)X(cid:63), Y(cid:63)

(cid:1)(cid:44) arg max

X∈X ,Y∈Y

Tr(cid:0)X(cid:62)AY(cid:1),

where X ⊆ Rm×k and Y ⊆ Rn×k are sets satisfying

the conditions of Lemma 3.2. Let (cid:101)A be a rank-r ap-
proximation of A, and (cid:101)X ∈ X , (cid:101)Y ∈ Y be the output
of Alg. 1 with input (cid:101)A and accuracy . Then,
Tr(cid:0)X(cid:62)
≤ 2 ·(cid:16)
(cid:17) · µX · µY ,

(cid:1) − Tr(cid:0)(cid:101)X(cid:62)A(cid:101)Y(cid:1)
k · (cid:107)(cid:101)A(cid:107)2 + (cid:107)A − (cid:101)A(cid:107)2

(cid:63) AY(cid:63)
√



where µX (cid:44) maxX∈X (cid:107)X(cid:107)F and µY(cid:44) maxY∈Y (cid:107)Y(cid:107)F.

Proof. The proof follows the approximation guaran-
tees of Alg. 1 in Lemma 3.2 and Lemma A.6.

which implies that

= (cid:107)(cid:101)C(cid:63) − C(cid:93)(cid:107)∞,2 ≤ ,
(cid:107)(cid:98)C − C(cid:93)(cid:107)F ≤
(cid:1) ≥ Tr(cid:0)(cid:101)X(cid:62)
(cid:63) (cid:101)A(cid:101)Y(cid:63)

√

k · .

(cid:93) (cid:101)AY(cid:93)

(cid:1) − 2 ·  ·

Continuing from (18) under (19) and (20),
√

Tr(cid:0)X(cid:62)
k · σ1((cid:101)A).
Recalling that the singular values of (cid:101)A have been

scaled by a factor of µX · µY yields the desired result.
The runtime of Alg. 1 follows from the cost per iter-
ation and the cardinality of the -net. Matrix mul-
tiplications can exploit the truncated singular value

decomposition of (cid:101)A which is performed only once.
Lemma A.6. For any A,(cid:101)A ∈ Rm×n, and norm-

and

X∈X ,Y∈Y

X∈X ,Y∈Y

for some 0 < γ ≤ 1, we have

boudned sets X ⊆ Rm×k and Y ⊆ Rn×k, let

(cid:1)(cid:44) arg max
(cid:1)(cid:44) arg max

(cid:0)X(cid:63), Y(cid:63)
(cid:0)(cid:101)X(cid:63),(cid:101)Y(cid:63)
For any ((cid:101)X,(cid:101)Y) ∈ X × Y such that
Tr(cid:0)(cid:101)X(cid:62)(cid:101)A(cid:101)Y(cid:1) ≥ γ · Tr(cid:0)(cid:101)X(cid:62)
Tr(cid:0)(cid:101)X(cid:62)A(cid:101)Y(cid:1) ≥ γ · Tr(cid:0)X(cid:62)

Tr(cid:0)X(cid:62)AY(cid:1),
Tr(cid:0)X(cid:62)(cid:101)AY(cid:1).
(cid:1) − C
(cid:63) (cid:101)A(cid:101)Y(cid:63)
(cid:1) − C
− 2 · (cid:107)A − (cid:101)A(cid:107)2 · µX · µY .
Proof. By the optimality of (cid:101)X(cid:63),(cid:101)Y(cid:63) for (cid:101)A, we have
(cid:1).
(cid:1) ≥ Tr(cid:0)X(cid:62)
Tr(cid:0)(cid:101)X(cid:62)
(cid:63) (cid:101)AY(cid:63)
In turn, for any ((cid:101)X,(cid:101)Y) ∈ X × Y such that
(cid:1) − C
Tr(cid:0)(cid:101)X(cid:62)(cid:101)A(cid:101)Y(cid:1) ≥ γ · Tr(cid:0)(cid:101)X(cid:62)
(cid:63) (cid:101)A(cid:101)Y(cid:63)
(cid:1) − C.
Tr(cid:0)(cid:101)X(cid:62)(cid:101)A(cid:101)Y(cid:1) ≥ γ · Tr(cid:0)X(cid:62)
(cid:63) (cid:101)AY(cid:63)

where µX (cid:44) maxX∈X (cid:107)X(cid:107)F and µY(cid:44) maxY∈Y (cid:107)Y(cid:107)F.

for some 0 < γ < 1 (if such pairs exist), we have

(cid:63) (cid:101)A(cid:101)Y(cid:63)

(cid:63) AY(cid:63)

(21)

Bipartite Correlation Clustering – Maximizing Agreements

B Correctness of Algorithm 2
In the sequel, we use (cid:107)X(cid:107)∞,1 to denote the maximum
of the (cid:96)1 norm of the rows of X. When X ∈ {0, 1}d×k,
the constraint (cid:107)X(cid:107)∞,1 = 1 eﬀectively implies that each
row of X has exactly one nonzero entry.

Lemma 4.4. Let X(cid:44)(cid:8)X ∈ {0, 1}d×k : (cid:107)X(cid:107)∞,1 = 1(cid:9).

For any d × k real matrix L, Algorithm 2 outputs

(cid:101)X = arg max

X∈X

Tr(cid:0)X(cid:62)L(cid:1),

in time O(k · d)

Proof. By construction, each row of X has exactly one
nonzero entry. Let ji ∈ [k] denote the index of the
nonzero entry in the ith row of X. For any X ∈ X ,

Tr(cid:0)X(cid:62)L(cid:1) =

k(cid:88)
d(cid:88)

j=1

x(cid:62)
j lj =

k(cid:88)
Liji ≤ d(cid:88)

j=1

i=1

i=1

=

(cid:88)

1 · Lij

i∈supp(xj )

max
j∈[k]

Lij.

(26)

Algorithm 2 achieves equality in (26) due to the choice
of ji in line 3. Finally, the running time follows imme-
diately from the O(k) time required to determine the
maximum entry of each of the d rows of L.

C Auxiliary Lemmas

Lemma C.7. Let a1, . . . , an and b1, . . . , bn be 2n real
numbers and let p and q be two numbers such that
1/p + 1/q = 1 and p > 1. We have

(cid:33)1/p

(cid:32) n(cid:88)

(cid:32) n(cid:88)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤
(cid:33)1/q
(cid:12)(cid:12)(cid:104)A, B(cid:105)(cid:12)(cid:12)(cid:44)(cid:12)(cid:12)Tr(cid:0)A(cid:62)B(cid:1)(cid:12)(cid:12) ≤ (cid:107)A(cid:107)F(cid:107)B(cid:107)F.

Lemma C.8. For any A, B ∈ Rn×k,

|ai|p

|bi|q

aibi

i=1

i=1

i=1

·

.

Proof. Treating A and B as vectors, the lemma follows
immediately from Lemma C.7 for p = q = 2.

Lemma C.9. For any two real matrices A and B of
appropriate dimensions,

(cid:107)AB(cid:107)F ≤ min(cid:8)(cid:107)A(cid:107)2(cid:107)B(cid:107)F, (cid:107)A(cid:107)F(cid:107)B(cid:107)2

(cid:9).

Proof. Let bi denote the ith column of B. Then,

(cid:88)

2 ≤(cid:88)

i

(cid:107)AB(cid:107)2

(cid:107)Abi(cid:107)2

(cid:88)

F =
= (cid:107)A(cid:107)2

i

2

i

(cid:107)A(cid:107)2

2

2(cid:107)bi(cid:107)2
2(cid:107)B(cid:107)2
F.

(cid:107)bi(cid:107)2

2 = (cid:107)A(cid:107)2

Similarly, using the previous inequality,
2(cid:107)A(cid:62)(cid:107)2
(cid:107)AB(cid:107)2

F = (cid:107)B(cid:62)A(cid:62)(cid:107)2

F ≤ (cid:107)B(cid:62)(cid:107)2

F = (cid:107)B(cid:107)2

2(cid:107)A(cid:107)2
F.

The desired result follows combining the two upper
bounds.
Lemma C.10. For any real m × k matrix X, m × n
matrix A, and n × k matrix Y,

(cid:12)(cid:12)Tr(cid:0)X(cid:62)AY(cid:1)(cid:12)(cid:12) ≤ (cid:107)X(cid:107)F · (cid:107)A(cid:107)2 · (cid:107)Y(cid:107)F.

Proof. We have

(cid:12)(cid:12)Tr(cid:0)X(cid:62)AY(cid:1)(cid:12)(cid:12) ≤ (cid:107)X(cid:107)F · (cid:107)AY(cid:107)F ≤ (cid:107)X(cid:107)F · (cid:107)A(cid:107)2 · (cid:107)Y(cid:107)F,

with the ﬁrst inequality following from Lemma C.8 on
|(cid:104)X, AY(cid:105)| and the second from Lemma C.9.
Lemma C.11. For any real m × n matrix A, and
pair of m × k matrix X and n × k matrix Y such that
X(cid:62)X = Ik and Y(cid:62)Y = Ik with k ≤ min{m, n}, the
following holds:

(cid:0)A(cid:1)(cid:1)1/2

.

σ2
i

Proof. By Lemma C.8,

i=1

√

k ·(cid:0) k(cid:88)
(cid:12)(cid:12)Tr(cid:0)X(cid:62)AY(cid:1)(cid:12)(cid:12) ≤
|(cid:104)X, AY(cid:105)| =(cid:12)(cid:12)Tr(cid:0)X(cid:62)AY(cid:1)(cid:12)(cid:12)
F = Tr(cid:0)X(cid:62)X(cid:1) = Tr(cid:0)Ik
(cid:107)A(cid:98)Y(cid:107)2

≤ (cid:107)X(cid:107)F · (cid:107)AY(cid:107)F =

(cid:107)AY(cid:107)2

F =

F ≤ max
(cid:98)Y∈Rn×k
(cid:98)Y(cid:62)(cid:98)Y=Ik

√

k · (cid:107)AY(cid:107)F.

(cid:1) = k. Further, for any
k(cid:88)

σ2
i (A).

(27)

i=1

where the last inequality follows from the fact that
(cid:107)X(cid:107)2
Y such that YT Y = Ik,

Combining the two inequalities, the result follows.
Lemma C.12. For any real m× n matrix A, and any
k ≤ min{m, n},

(cid:33)1/2

σ2
i (A)

.

(cid:32) k(cid:88)

i=1

(cid:107)AY(cid:107)F =

max
Y∈Rn×k
Y(cid:62)Y=Ik

The above equality is realized when the k columns of Y
coincide with the k leading right singular vectors of A.

Proof. Let UΣV(cid:62) be the singular value decomposi-
tion of A, with Σjj = σj being the jth largest singular
value of A, j = 1, . . . , d, where d(cid:44) min{m, n}. Due
to the invariance of the Frobenius norm under unitary
multiplication,
(cid:107)AY(cid:107)2

F = (cid:107)UΣV(cid:62)Y(cid:107)2

F = (cid:107)ΣV(cid:62)Y(cid:107)2
F.

(28)

Continuing from (28),

(cid:107)ΣV(cid:62)Y(cid:107)2

Let zj(cid:44)(cid:80)k

j yi
individual zj satisﬁes

i=1

 yi

j

j · vjv(cid:62)
σ2
(cid:1)2

.

i

=

=

i=1

i=1

j=1

j=1

σ2

j yi

y(cid:62)

 d(cid:88)
j · k(cid:88)
(cid:0)v(cid:62)

F = Tr(cid:0)Y(cid:62)VΣ2V(cid:62)Y(cid:1)
k(cid:88)
d(cid:88)
(cid:0)v(cid:62)
(cid:1)2
0 ≤ zj(cid:44) k(cid:88)
(cid:0)v(cid:62)
k(cid:88)
d(cid:88)
(cid:0)v(cid:62)
k(cid:88)

k(cid:88)

d(cid:88)

(cid:1)2

j yi

j yi

i=1

j=1

j=1

i=1

i=1

=

(cid:107)yi(cid:107)2 = k.

zj =

=

i=1

(cid:1)2 ≤ (cid:107)vj(cid:107)2 = 1,

(cid:0)v(cid:62)

j yi

(cid:1)2

d(cid:88)

j=1

where the last inequality follows from the fact that the
columns of Y are orthonormal. Further,

r+k(cid:88)

i=r+1

r+k(cid:88)

i=1

l(cid:88)

i=1

Proof. By the Cauchy-Schwartz inequality,

r+k(cid:88)

i=r+1

σi(A) =

r+k(cid:88)

i=r+1

√

k ·

=

|σi(A)| ≤

(cid:32) r+k(cid:88)

i=r+1

(cid:32) r+k(cid:88)
(cid:33)1/2

i=r+1

σ2
i (A)

.

(cid:33)1/2

σ2
i (A)

(cid:107)1k(cid:107)2

Note that σr+1(A), . . . , σr+k(A) are the k smallest
among the r + k largest singular values. Hence,

i (A) ≤ k
σ2
r + k

i (A) ≤ k
σ2
r + k

σ2
i (A)

=

k

r + k

(cid:107)A(cid:107)2
F.

Combining the two inequalities, the desired result fol-
lows.
Corollary 1. For any real m × n matrix A, the
rth largest singular value σr(A) satisﬁes σr(A) ≤
(cid:107)A(cid:107)F/

√

r.

Proof. It follows immediately from Lemma C.13.
First, we deﬁne the (cid:107)·(cid:107)∞,2 norm of a matrix as the l2
norm of the column with the maximum l2 norm, i.e.,
for an r × k matrix C

(cid:107)C(cid:107)∞,2 = max
1≤i≤k

(cid:107)ci(cid:107)2.

k(cid:88)
(cid:18)

i=1

(cid:107)ci(cid:107)2

2

(cid:107)ci(cid:107)2

2 ≤ k · max
(cid:19)2
1≤i≤k

(cid:107)ci(cid:107)2

= k ·

max
1≤i≤k

= k · (cid:107)C(cid:107)∞,2.

(30)

, j = 1, . . . , d. Note that each

Combining the above, we conclude that

d(cid:88)

j=1

(cid:107)AY(cid:107)2

F =

j · zj ≤ σ2
σ2

1 + . . . + σ2
k.

(29)

Note that

(cid:107)C(cid:107)2

F =

Finally, it is straightforward to verify that if yi = vi,
i = 1, . . . , k, then (29) holds with equality.
Lemma C.13. For any real m × n matrix A, let
σi(A) be the ith largest singular value. For any r, k ≤
min{m, n},

r+k(cid:88)

i=r+1

σi(A) ≤

k√
r + k

(cid:107)A(cid:107)F.

