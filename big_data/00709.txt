6
1
0
2

 
r
a

M
2

 

 
 
]

G
L
.
s
c
[
 
 

1
v
9
0
7
0
0

.

3
0
6
1
:
v
i
X
r
a

Probabilistic Relational Model Benchmark

Generation

Mouna Ben Ishak, Rajani Chulyadyo, and Philippe Leray

LARODEC Laboratory, ISG, Universit´e de Tunis, Tunisia

DUKe research group, LINA Laboratory UMR 6241, University of Nantes, France

DataForPeople, Nantes, France

Abstract

The validation of any database mining methodology goes through
an evaluation process where benchmarks availability is essential.
In
this paper, we aim to randomly generate relational database bench-
marks that allow to check probabilistic dependencies among the at-
tributes. We are particularly interested in Probabilistic Relational
Models (PRMs), which extend Bayesian Networks (BNs) to a rela-
tional data mining context and enable eﬀective and robust reasoning
over relational data. Even though a panoply of works have focused, sep-
arately, on the generation of random Bayesian networks and relational
databases, no work has been identiﬁed for PRMs on that track. This
paper provides an algorithmic approach for generating random PRMs
from scratch to ﬁll this gap. The proposed method allows to generate
PRMs as well as synthetic relational data from a randomly generated
relational schema and a random set of probabilistic dependencies. This
can be of interest not only for machine learning researchers to evalu-
ate their proposals in a common framework, but also for databases
designers to evaluate the eﬀectiveness of the components of a database
management system.

Keywords: Probabilistic Relational Model, Relational data rep-

resentation, Benchmark generation

1

Introduction

Data mining is the central step in knowledge discovery in databases.
It
relies on several research areas including statistics and machine learning.
Usually, machine learning techniques are developed around ﬂat data repre-
sentation (i.e., matrix form) and are known as propositional learning ap-
proaches. However, due to the development of communication and storage
technologies, data management practices have taken further aspects. Data

1

can present a very large number of dimensions, with several diﬀerent types
of entities. With the growing interest in extracting patterns from such data
representation, relational data mining approaches have emerged with the
interest of ﬁnding patterns in a given relational database [11] and Statistical
Relational Learning (SRL) has emerged as an area of machine learning that
enables eﬀective and robust reasoning about relational data structures [18].
In this paper, we are particularly interested in Probabilistic Relational Mod-
els (PRMs)1 [22, 28], which represent a relational extension of Bayesian net-
works [27], where the probability model speciﬁcation concerns classes of ob-
jects rather than simple attributes. PRMs present a probabilistic graphical
formalism that enables ﬂexible modeling of complex relational interactions.
PRMs have proved their applicability in several areas (e.g., risk analy-
sis, web page classiﬁcation, recommender systems) [7, 12, 32] as they allow
to minimize data preprocessing and the loss of signiﬁcant information [30].
The use of PRMs implies their construction either by experts or by applying
learning algorithms in order to learn the model from some existing observa-
tional relational data. PRMs learning involves ﬁnding a graphical structure
as well as a set of conditional probability distributions that best ﬁt to the
relational training data. The evaluation of the learning approaches is usu-
ally done using randomly generated data coming from either real known net-
works or randomly generated ones. However, neither the ﬁrst nor the second
are available in the literature. Moreover, there is a growing interest from
the database community to produce database benchmarks to support and
illustrate decision support systems (DSSs). For real-world business tasks,
uncertainty is an unmissable aspect. So, benchmarks designed to support
DSSs should consider this task.

In this paper, we propose an algorithmic approach that allows to gen-
erate random PRMs from scratch, and then populate a database instance.
The originality of this process is that it allows to generate synthetic rela-
tional data from a randomly generated relational schema and a random set
of probabilistic dependencies. Since PRMs bring together two neighboring
subﬁelds of computer science, namely machine learning and database man-
agement, our process can be useful for both domains. It is imperative for
statistical relational learning researchers to evaluate the eﬀectiveness of their
learning approaches. On the other hand, it can be of interest for database
designers to evaluate the eﬀectiveness of a database management system
(DBMS) components. It allows to generate various relational schemas, from
simple to complex ones, and to populate database tables with huge number
of tuples derived from underlying probability distributions deﬁned by the

1Neville and Jensen [26] use the term ‘Relational Bayesian Network’ to refer to Bayesian
networks that have been extended to model relational databases [22, 28] and use the term
‘PRM’ in its more general sense to distinguish the family of probabilistic graphical models
that are interested in extracting statistical patterns from relational models. In this paper,
we preserve the term PRM as used by [22, 28].

2

generated PRMs. This paper presents an extended version of a preliminary
work published in [1, 2].

2 Background

This section ﬁrst provides a brief recall on Bayesian networks and relational
model, and then introduces PRMs.

2.1 Bayesian networks

Bayesian networks (BNs) [27] are directed acyclic graphs allowing to eﬃ-
ciently encode and manipulate probability distributions over high-dimensional
spaces. Formally, they are deﬁned as follows:
Deﬁnition 1 A Bayesian network B = (cid:104)G, Θ(cid:105) is deﬁned by:
1) A graphical component (structure): a directed acyclic graph (DAG) G =
(V, E), where V is the set of vertices representing n discrete random vari-
ables A = {A1, . . . , An}, and E is the set of directed edges corresponding
to conditional dependence relationships among these variables.

2) A numerical component (parameters): Θ = {Θ1, . . . , Θn} where each
Θi = P (Ai|P a (Ai)) denotes the conditional probability distribution (CPD)
of each node Ai given its parents in G denoted by P a (Ai).

Several approaches have been proposed to learn BNs from data [9]. The
evaluation of these learning algorithms requires either the use of known net-
works or the use of a random generation process. The former allows to
sample data and perform learning using this data in order to recover the
initial gold standard net. The latter allows to generate synthetic BNs and
data in order to provide a large number of possible models and to carry out
experimentation while varying models from simple to complex ones.

Random Bayesian networks generation comes to provide a graph
structure and parameters. Statnikov et al. [33] proposed an algorithmic ap-
proach to generate arbitrarily large BNs by tiling smaller real-world known
networks. The complexity of the ﬁnal model is controlled by the number of
tiling and a connectivity parameter which determines the maximum number
of connections between one node and the next tile. Some works have been
devoted to the generation of synthetic networks but without any guarantee
that every allowed graph is produced with the same uniform probability [21].
In [20], the authors have proposed an approach, called PMMixed algorithm,
that allows the generation of uniformly distributed Bayesian networks us-
ing Markov chains. Using this algorithm, constraints on generated nets can
be added with relative ease such as constraints on nodes degree, maximum

3

number of dependencies in the graph, etc. Once the DAG structure is gen-
erated, it is easy to construct a complete Bayesian network by randomly
generating associated probability distributions by sampling either Uniform
or Dirichlet distributions. Having the ﬁnal BN, standard sampling method,
such as forward sampling [19], can be used to generate observational data.

2.2 Relational model

The manner how the data is organized in a database depends on the cho-
sen database model. The relational model is the most commonly used one
and it represents the basis for the most large scale knowledge representa-
tion systems [11]. Formally, the relational representation can be deﬁned as
follows:

Deﬁnition 2 The relational representation consists of

• A set of relations (or tables or classes) X = {X1, . . . , Xn}. Each

relation Xi has two parts:

– The heading (relation schema): a ﬁxed set of attributes A(X) =
{A1, . . . , Ak}. Each attribute Ai is characterized by a name and
a domain denoted Di.

– The body: a set of tuples (or records). Each tuple associates for

each attribute Ai in the heading a value from its domain Di.

• Each relation has a key (i.e., a unique identiﬁer, a subset of the head-
ing of a relation Xi.) and, possibly, a set of foreign key attributes
(or reference slots ρ). A foreign key attribute is a ﬁeld that points
to a key ﬁeld in another relation, called the referenced relation. The
associated constraint is a referential constraint. A chain of such
constraints constitutes a referential path. If a referential path from
some relation to itself is found then it is called a referential cy-
cle2. Relation headings and constraints are described by a relational
schema R.

Usually the interaction with a relational database is ensured by specify-
ing queries using structured query language (SQL), which on their part use
some speciﬁc operators to extract signiﬁcant meaning such as aggregators.
An aggregation function γ takes a multi-set of values of some ground type,
and returns a summary of it. Some requests need to cross long reference
paths, with some possible back and forth. They use composed slots to de-
ﬁne functions from some objects to other ones to which they are indirectly
related. We call this composition of slots a slot chain K. We call a slot
chain single-valued when all the crossed reference slots end with a cardinality

2Database designs involving referential cycles are usually contraindicated [10].

4

equal to 1. A slot chain is multi-valued if it contains at least one reference
slot ending with cardinality equal to many. Multi-valued slot chains imply
the use of aggregators.

Generally, database benchmarks are used to measure the performance of
a database system. A database benchmark includes several subtasks (e.g.,
generating the transaction workload, deﬁning transaction logic, generating
the database) [16].

Random database generation consists on creating the database schema,

determining data distribution, generating it and loading all these compo-
nents to the database system under test. Several propositions have been
developed in this context. The main issue was how to provide a large num-
ber of records using some known distributions in order to be able to evaluate
the system results [4, 6]. In some research, known benchmarks 3 are used
and the ultimate goal is only to generate a large dataset [17]. Nowadays, sev-
eral software tools are available (e.g., DbSchema 4, DataFiller 5) to populate
database instances knowing the relational schema structure. Records are
then generated on the basis of this input by considering that the attributes
are probabilistically independent which is not relevant when these bench-
marks are used to evaluate decision support systems. The Transaction Pro-
cessing Performance Council (TPC)6 organization provides the TPC-DS7
benchmark which has been designed to be suitable with real-world business
tasks which are characterized by the analysis of huge amount of data. The
TPC-DS schema models sales and the sales returns process for an organiza-
tion. TPC-DS provides tools to generate either data sets or query sets for
the benchmark. Nevertheless, uncertainty management stays a prominent
challenge to provide better rational decision making.

2.3 Probabilistic relational models

Probabilistic relational models [15, 22, 28] are an extension of BNs in the
relational context. They bring together the strengths of probabilistic graph-
ical models and the relational presentation. Formally, they are deﬁned as
follows [15]:
Deﬁnition 3 A Probabilistic Relational Model Π for a relational schema R
is deﬁned by:
1) A qualitative dependency structure S : for each class (relation) X ∈
X and each descriptive attribute A ∈ A(X), there is a set of parents
3http://www.tpc.org
4http://www.dbschema.com/
5https://www.cri.ensmp.fr/people/coelho/dataﬁller.html
6http://www.tpc.org
7http://www.tpc.org/tpcds

5

P a(X.A) = {U1, . . . , Ul} that describes probabilistic dependencies. Each
Ui has the form X.B if it is a simple attribute in the same relation or
γ(X.K.B), where K is a slot chain and γ is an aggregation function.

2) A quantitative component, a set of conditional probability distributions

(CPDs), representing P (X.A|P a(X.A)).

The PRM Π is a meta-model used to describe the overall behavior of
a system. To perform probabilistic inference, this model has to be instan-
tiated. A PRM instance contains, for each class of Π, the set of objects
involved in the system and the relations that hold between them (i.e., tu-
ples from the database instance which are interlinked). This structure is
known as a relational skeleton σr [15].

Deﬁnition 4 A relational skeleton σr of a relational schema is a partial
speciﬁcation of an instance of the schema.
It speciﬁes the set of objects
σr(Xi) for each class and the relations that hold between the objects. How-
ever, it leaves the values of the attributes unspeciﬁed.

Given a relational skeleton, the PRM Π deﬁnes a distribution over the
possible worlds consistent with σr through a ground Bayesian network [15].

Deﬁnition 5 A Ground Bayesian Network (GBN) is deﬁned given a PRM
Π together with a relational skeleton σr. A GBN consists of:

1) A qualitative component:

• A node for every attribute of every object x ∈ σr(X), x.A.
• Each x.A depends probabilistically on a set of parents P a(x.A) =
u1, . . . ul of the form x.B or x.K.B, where each ui is an instance
of the Ui deﬁned in the PRM. If K is not single-valued, then the
parent is an aggregate computed from the set of random variables
{y|y ∈ x.K}, γ(x.K.B).

2) A quantitative component, the CPD for x.A is P (X.A|P a(X.A)) .

Example 1 An example of a relational schema is depicted in Figure 1, with
three classes X = {M ovie, V ote, U ser}. The relation V ote has a descriptive
attribute V ote.Rating and two reference slots V ote.U ser and V ote.M ovie.
V ote.U ser relates the objects of class V ote with the objects of class U ser.
Dotted links presents reference slots. An example of a slot chain would
be V ote.U ser.U ser−1.M ovie which could be interpreted as all the votes of
movies cast by a particular user.
V ote.M ovie.genre → V ote.rating is an example of a probabilistic depen-
dency derived from a slot chain of length 1 where V ote.M ovie.genre is a par-
ent of V ote.rating as shown in Figure 2. Also, varying the slot chain length

6

Figure 1: An example of relational schema

Figure 2: An example of probabilistic relational model

may give rise to other dependencies. For instance, using a slot chain of
length 3, we can have a probabilistic dependency from γ(V ote.U ser.U ser−1.M ovie.genre)
to V ote.rating. In this case, V ote.rating depends probabilistically on an ag-
gregate value of all the genres of movies voted by a particular user.

Figure 3 is an example of a relational skeleton of the relational schema
of Figure 1. This relational skeleton contains 3 users, 5 movies and 9 votes.

7

MovieUserVoteMovieUserGenderAgeOccupationReleaseDateGenreUserRatingMovieUserReleaseDateGenreAgeGenderOccupationUser.GenderMF0.40.6VoteRatingMovie.GenreVotes.RatingLowHighDrama, M0.50.5Drama, F0.30.7Horror, M0.20.8Horror, F0.90.1Comedy, M0.50.5Comedy, F0.60.4User.GenderFigure 3: An example of a relational skeleton

Figure 4: An example of a ground Bayesian network

Also it speciﬁes the relations between these objects, e.g., the user U 1 voted
for two movies m1 and m2.

Figure 4 presents the ground Bayesian network constructed from the re-
lational skeleton of Figure 3 and the PRM of Figure 2. It resumes the same

8

RatingAgeGenderOccupationAgeGenderGenreReleaseDateGenreGenreU1U2M1M2M3#U1, #M1Rating#U1, #M2Rating#U2, #M1Rating#U2, #M3Rating#U2, #M4ReleaseDateAgeOccupationGenderOccupationGenreGenreU3M4M5Rating#U3, #M1Rating#U3, #M2Rating#U3, #M3Rating#U3, #M5ReleaseDateReleaseDateReleaseDateRatingAgeGenderOccupationAgeGenderGenreReleaseDateGenreGenreU1U2M1M2M3#U1, #M1Rating#U1, #M2Rating#U2, #M1Rating#U2, #M3Rating#U2, #M4ReleaseDateAgeOccupationGenderOccupationGenreGenreU3M4M5Rating#U3, #M1Rating#U3, #M2Rating#U3, #M3Rating#U3, #M5ReleaseDateReleaseDateReleaseDatedependencies as well as CP Ds of the PRM at the level of objects. Here, we
have not reproduced the CP Ds to not overload the ﬁgure.

PRM structure learning has not been well studied in the literature.
Only few works have been proposed to learn PRMs [13] or almost similar
models [23, 24] from relational data.

Friedman et al. [13] proposed Relational Greedy Hill-Climbing Search
(RGS) algorithm. They applied a greedy search procedure to explore the
space of PRM structures while allowing increasingly large slot chains. PRM
structures are generated using the add edge, delete edge and reverse edge
operators and aggregation functions if needed (cf. Section 2.3). As for score
function, they used a relational extension of the Bayesian Dirichlet (BD) [8]
score expressed as follows:

(cid:88)

i

(cid:88)
−(cid:88)

(cid:88)
(cid:88)

i

A∈A(Xi)

u∈V (P a(Xi.A))

A∈A(Xi)

u∈V (P a(Xi.A))

RBDscore =

log[DM ({CXi.A[v, u]},{αXi.A[v, u]})]
(cid:88)
Γ((cid:80)
Γ((cid:80)
DM ({CXi.A[v, u]},{αXi.A[v, u]}) =
(cid:90) ∞

lengthK(Xi.A, P a(Xi.A))

v(α[v] + C[v]))

(cid:89)

v

v α[v])

Where

and

Γ(α[v] + C[v])

,

Γ(α[v])

(1)

Γ(x) =

tx−1e−tdt

is the Gamma function.

0

As for standard BNs, evaluating the eﬀectiveness of the proposed ap-
proaches is needed. However, neither relational benchmarks nor general
random generation process are available.

Random probabilistic relational models generation has to be es-
tablished in order to evaluate proposed learning approaches in a common
framework. [24] used a predeﬁned schema and have only generated a num-
ber of dependencies varying from 5 to 15 and the conditional probability
tables for attributes from a Dirichlet distribution. In [23], the authors have
generated relational synthetic data to perform experimentation. Their gen-
eration process is based only on a particular family of relational schemas,
with N classes (nodes) and N − 1 referential constraints (edges). Refer-
ential constraints are then expressed using relationship classes. This gives
rise to a ﬁnal relational schema containing 2N − 1 relations whereas in real

9

world cases, relational schemas may have more than N − 1 referential con-
straints. If the schema is fully connected (as described in [25]), it will have
a tree structure. Torti et al. [34] proposed a slightly diﬀerent representa-
tion of PRMs, developed in the basis of the object-oriented framework and
expert knowledge. Their main issue is probabilistic inference rather than
learning. In their experimental studies [35], they have randomly generated
PRMs using the layer pattern. The use of this architecture pattern imposes a
particular order when searching for connections between classes, generating
reference slots of the relational schema and also when creating the relational
skeleton. No indication has been made about the generation of probabilistic
dependencies between attributes. In addition, they are interested neither
in populating a relational database nor in communicating with a database
management system.

3 PRM Benchmark Generation

Due to the lack of famous PRMs in the literature, this paper proposes a syn-
thetic approach to randomly generate probabilistic relational models from
scratch and to randomly instantiate them and populate relational databases.
To the best of our knowledge, this has not yet been addressed.

3.1 Principle

As we are working with a relational variety of Bayesian networks, our gen-
eration process is inspired from classical methods of generation of random
BNs while respecting the relational domain representation.

The overall process is outlined in Algorithm 1. Roughly, the proposed

generation process is divided into three main steps:

• The ﬁrst step generates a random PRM. For this, a relational schema is
generated using Generate Relational Schema function(Section 3.2).
Then, a graph dependency structure is generated using Generate
Dependency Structure and Determine Slot Chains functions (Sec-
tion 3.3). And ﬁnally, conditional probability tables are generated by
the Generate CP D function in the same way than Bayesian networks
(cf. Section 2.1).

• The second step instantiates the model generated in the ﬁrst step.

First, a relational skeleton is generated using Generate Relational Skeleton
function (Section 3.4). Then, using Create GBN function, a ground
Bayesian Network is generated from both the generated PRM and the
generated relational skeleton.

• The third step presents the Sampling function. It involves database
instance population and can be performed using a standard sampling

10

Algorithm 1: Generate Random PRM-DB
Input: N : the number of relations, Kmax : The maximum slot chain
Output: Π :< R,S, CP D >, DB Instance
begin

length allowed

Step 1: Generate the PRM
Π.R ← Generate Relational Schema(N )
Π.S ← Generate Dependency Structure(Π.R)
Π.S ← Determine Slot Chains(Π.R, Π.S,Kmax)
Π.CP D ← Generate CP D(Π.S)
Step 2: Instantiate the PRM
σr ← Generate Relational Skeleton(Π.R)
GBN ← Create GBN (Π, σr)
Step 3: Database population
DB Instance ← Sampling(GBN )

method over the GBN (Section 3.5).

3.2 Generation of a random relational schema

The relational schema generation process is depicted in Algorithm 2. Our
aim is to generate a relational schema with a given number of classes such
that it does not contain any referential cycles and also respects the rela-
tional model deﬁnition presented in section 2.2. We apply concepts from
the graph theory for random schema generation. We associate this issue to
a DAG structure generation process, where nodes represent relations and
edges represent referential constraints deﬁnition. Xi → Xj means that Xi
is the referencing relation and Xj is the referenced one. Besides, we aim to
construct schemas where ∀Xi, Xi ∈ X there exists a referential path from
Xi to Xj. This assumption allows to browse all classes in order to dis-
cover probabilistic dependencies later and it is traduced by searching DAG
structures containing a single connected component (i.e., connected DAG).

Having a ﬁxed number of relations N , the Generate DAG function con-
structs a DAG structure G with N nodes, where each node ni ∈ G corre-
sponds to a relation Xi ∈ R following various possible implementation poli-
cies (cf. Section 5.2). For each class, we generate a primary key attribute
using Generate P rimary Key function. Then, we randomly generate the
number of attributes and their associated domains using Generate Attributes

11

Algorithm 2: Generate Relational Schema
Input: N : the required number of classes
Output: R : The generated relational schema
begin

repeatG ← Generate DAG(P olicy)
until G is a connected DAG;
for each relation Xi ∈ R do

Pk Xi ← Generate P rimary Key(Xi)
A(Xi) ← Generate Attributes(P olicy)
V(Xi.A) ← Generate States(P olicy)

for each ni → nj ∈ G do

F k Xi ← Generate F oreign Key(Xi, Xj, P k Xj)

and Generate States functions respectively. Note that the generated do-
mains do not take into account possible probabilistic dependencies between
attributes. For each ni → nj ∈ G, we generate a foreign key attribute in
Xi using the Generate F oreign Key function. Foreign key generation is
limited to one attribute as foreign keys reference simple primary keys (i.e.,
primary keys generated from only one attribute).

3.3 Generation of a random PRM

Relational schemas are not suﬃcient to generate databases when the at-
tributes are not independent. We need to randomly generate probabilistic
dependencies between the attributes of the classes in the schema. These
dependencies have to provide the DAG of the dependency structure S and
a set of CPDs which deﬁne a PRM (cf. Deﬁnition 3).

We especially focus on the random generation of the dependency struc-
ture. Once this latter is identiﬁed, conditional probability distributions may
be sampled in a similar way as standard BNs parameter generation.
The dependency structure S should be a DAG to guarantee that each
generated ground network is also a DAG [14]. S has the speciﬁcity that one
descriptive attribute may be connected to another with diﬀerent possible slot
chains. Theoretically, the number of slot chains may be inﬁnite. In practice
a user-deﬁned maximum slot chain length Kmax, is speciﬁed to identify the
horizon of all possible slot chains. In addition, the Kmax value should be at
least equal to N − 1 in order to not neglect potential dependencies between
attributes of classes connected via a long path. Each edge in the DAG has to

12

Algorithm 3: Generate Dependency Structure
Input: R : The relational schema
Output: S : The generated relational dependency structure
begin

for each class Xi ∈ R do

Gi ← Generate Sub DAG(P olicy)

S ←(cid:83)Gi

S ← Generate Super DAG(P olicy)

be annotated to express from which slot chain this dependency is detected.
We add dependencies following two steps. First we add oriented edges to
the dependency structure while keeping a DAG structure. Then we identify
the variable from which the dependency has been drawn by a random choice
of a legal slot chain related to this dependency.

3.3.1 Constructing the DAG structure

The DAG structure identiﬁcation is presented in Algorithm 3. The idea
here is to ﬁnd, for each node X.A, a set of parents from the same class
or from further classes while promoting intra-class dependencies in order
to control the ﬁnal model complexity as discussed in [14]. This condition
promotes the discovery of intra-class dependencies or those coming from
short slot chains. The longer the slot chain, the lower is the chance of
ﬁnding a probabilistic dependency through the slot chain. To follow this
condition, having N classes, we propose to construct N separated sub-DAGs,
each of which is built over attributes of its corresponding class using the
Generate Sub DAG function. Then, we construct a super-DAG over all the
previously constructed sub-DAGs. At this stage, the super-DAG contains
N disconnected components: The idea is to add inter-class dependencies in
such a manner that we connect these disconnected components while keeping
a global DAG structure.

To add inter-class dependencies, we constrain the choice of adding de-
pendencies among only variables that do not belong to the same class. For
an attribute X.A, the Generate Super DAG function chooses randomly an
attribute Y.B, where X (cid:54)= Y , then veriﬁes whether the super-DAG structure
augmented by a new dependency from X.A to Y.B remains a DAG. If so,
it keeps the dependency otherwise it rejects it and searches for a new one.
The policies that are used are discussed in Section 5.2.

13

Algorithm 4: Determine Slot Chains
Input: R : The relational schema, S : The dependency structure,
Output: S : The generated relational dependency structure with

Kmax : The maximum slot chain length

generated slot chains

begin

Kmax ← max(Kmax, card(XR) − 1)
for each X.A → Y.B ∈ S do
P ot Slot Chains List ←
Generate P otential Slot chains(X, Y,R, Kmax)
for each slot Chain ∈ P ot Slot Chains List do

l ← length(slot Chain)
−l
W [i] ← exp

nb Occ(l,P ot Slot Chains List)

Slot Chain∗ ← Draw(P ot Slot Chains List, W )
if N eeds Aggregator(Slot Chain∗) then

γ ← Random Choice Agg(list Aggregators)

if Slot Chain∗ = 0 then

S.P a(X.A) ← S.P a(X.A) ∪ Y.B % here X = Y
elseS.P a(X.A) ← S.P a(X.A) ∪ γ(Y.Slot Chain∗.B)

3.3.2 Determining slot chains

During this step, we have to take into consideration that one variable may
be reached through diﬀerent slot chains and the dependency between two
descriptive attributes will depend on the chosen one. Following [14], the gen-
eration process has to give more priority to shorter slot chains for selection.
Consequently, we have used the penalization term discussed in [14]. Longer
indirect slot chains are penalized by having the probability of occurrence of
a probabilistic dependency from a slot chain length l inversely proportional
to expl.
Having a dependency X.A → Y.B between two descriptive attributes
X.A and Y.B, we start by generating the list of all possible slot chains
(P ot Slot Chains List) of length ≤ Kmax from which X can reach Y in
the relational schema using the Generate P otential Slot chains function.
Then, we create a vector W of the probability of occurrence for each of
the slot chains found, with log(W [i]) ∝
nb Occ(l,P ot Slot Chains List) , where l
is the slot chain length and nb Occ is the number of slot chains of length
l ∈ P ot Slot Chains List. This value will rapidly decrease when the value

−l

14

of l increases, which allows to reduce the probability of selecting long slot
chains. We then sample a slot chain from P ot Slot Chains List following
W using the Draw function. If the chosen slot chain implies an aggrega-
tor, then we choose it randomly from the list of existing ones using the
Random Choice Agg function. The slot chain determination is depicted in
Algorithm 4.

Simplifying slot chains. While ﬁnding slot chains, duplicate slot chains
might be encountered. By ’duplicate’, we mean the slot chains which pro-
duce the same result. For example, in the schema of ﬁgure 1, V ote.U ser and
V ote.U ser.U ser−1.U ser are equivalent because traversing through the slot
chains, we obtain the same set of User objects. Similarly, V ote.M ovie−1.M ovie
is the same as an empty slot chain because this slot chain results in the tar-
get Movie object. When such duplicates are found, we pick the shorter one
to avoid redundant, unnecessary computations. This is an improvement to
our previous work [1, 2], where simpliﬁcation of slot chains had not been
considered. We apply the following rule to simplify slot chains.
A slot chain is represented as a sequence of reference slots and inverse
slots as ρ1.ρ2. . . . .ρn−1.ρn. If ρn−1 is an inverse slot and ρ−1
n = ρn−1, then
the slot chain can be simpliﬁed by eliminating the last two slots. So, the
simpliﬁed slot chain would, then, be ρ1.ρ2. . . . .ρn−3.ρn−2. This can be done
repetitively until no simpliﬁcation is possible.

3.4 GBN generation

The generated schema together with the added probabilistic dependencies
and generated parameters results in a probabilistic relational model. To
instantiate this latter, we need to generate a relational skeleton. The GBN
is, then, fully determined with this relational skeleton and the CPDs already
present at the meta-level.

A relational skeleton can be imagined as a DAG where nodes are objects
of diﬀerent classes present in the associated relational schema and edges are
directed from one object to another conforming to the reference slots present
in the relational schema. This graph is, in fact, a special case of k-partite
graph8 of deﬁnition 6.

Deﬁnition 6 Relational skeleton as a k-partite graph

A relational skeleton is a special case of k-partite graph, Gk = (Vk, Ek),

with the following properties:

1. The graph is acyclic,

8A k-partite graph is a graph whose vertices can be partitioned into k disjoint sets so

that there is no edge between any two vertices within the same set.

15

2. All edges are directed (an edge u → v indicates that the object u refers
to v, i.e., u has a foreign key which refers to the primary key of v),

3. Edges between two diﬀerent types of objects are always oriented in the
for all edges (u — v) between objects of U and
same direction, i.e.
V where u ∈ U , and v ∈ V , the direction of all edges must be either
u → v or u ← v and not both

4. For all edges u → v between the objects of U and V , out degree of

u = 1 but indegree of v can be greater than 1.

In this regard, relational skeleton generation process can be considered as
a problem of generating objects and assigning links (or foreign keys) between
them such that the resulting graph is a k-partite graph of deﬁnition 6. In
our previous work [1, 2], we presented an algorithm to generate relational
skeleton, where it generates nearly same number of objects of each class and
iteratively adds random edges between objects of a pair of classes such that
the direction of the edges conform to the underlying schema. This approach,
in fact, does not create realistic skeleton because in real world, relational
skeleton tends to be scale-free, i.e., degree of the vertices of the graph follows
power law. Hence, in real datasets, the number of objects for classes with
foreign keys tend to be very high compared to that for classes which do
not have foreign keys and are referenced by other classes. Thus, we took a
diﬀerent approach to generate relational skeleton. Our new and improved
approach to generating such k-partite graph is presented as Algorithm 5.
We adapt [5]’s directed scale-free graph generation algorithm for our special
k-partite graph and use Chinese Restaurant Process[29] to apply preferential
attachment.

The basic idea here is to iteratively generate an object of a class with
no parents in the relational schema DAG and then recursively add an edge
from this object to objects of its children classes. This process is essentially
a depth ﬁrst search (DFS), where we begin by generating an object of the
root node of the graph and then at each encounter of a node in DFS, we
add an edge from the object of the parent node to either a new or an ex-
isting object of the encountered node. The object of the parent node gets
connected to a new object with probability p = α/(np − 1 + α), where np
is the total of objects of the parent node generated so far, and α is a scalar
parameter for the process. When it gets attached to an existing object, an
object of the correct type is picked from the set of existing objects with
probability nk/(np − 1 + α), where nk is the indegree of the object to be
selected and n is the total number of objects generated so far. Thus, as the
skeleton graph grows, probability of getting connected to new objects will
decrease and the objects with higher indegree will be preferred for adding
new edges. At each iteration, a DFS is performed starting from one of the
nodes without parents in the relational schema DAG. Thus, if there is only

16

Algorithm 5: Generate Relational Skeleton
Input: Relational Schema as a DAG, G = (Vg, Eg); Total number of
objects in the resulting skeleton, Ntotal; Scalar parameter for
CRP, α

Output: A relational skeleton, I = (V, E)
begin

for node ∈ Vg do

N (node) ← 0 %Total number of objects of each type generated
so far

V ← {} %Set of objects
E ← {} %Set of directed edges between objects
m ← Number of nodes without any parents in G %number of roots
if m > 1 then

Divide G into m subgraphs such that each subgraph contains a
root and all of its descendants.

repeat

if m > 1 then

else

g ← one of the m subgraphs picked randomly
g ← G %i.e., if G has only one root
objroot ← A new object of the root of g
nroot ← nroot + 1
children ← Children of the root in g
((V (cid:48), E(cid:48)), N(cid:48)) ←
Generate SubSkeleton(objroot, g, children, N, α) %Perform
depth-ﬁrst search over g and add edges recursively
V ← V ∪ V (cid:48)
E ← E ∪ E(cid:48)
N ← N(cid:48) %Update the set of number of generated objects of
each type
n ← cardinality(V ) %Total number of objects generated so
far.

until n >= Ntotal;
I ← (V, E)

one node that does not have any parent, then each iteration will visit all
classes in the relational schema resulting in a complete set of objects and
relations for all classes, otherwise only a subset of classes will be visited

17

Algorithm 6: Generate SubSkeleton

Input: Parent object objp; Graph g; Parent node, parent; Children
nodes, children; Set of the number of objects of each class
generated so far, N ; Scalar parameter α

Output: Relational skeleton, I = (V, E); Set of the number of

objects of each class generated so far, N

begin

V ← {objp}
E ← {}
np ← N (parent) %Total number of parents generated so far
for C ∈ children do

nc ← N (C) %Total number of the child C generated so far
p ← α/(np − 1 + α)
r ← A random value between 0 and 1 %r ∈ [0, 1]
if r <= p then

objc ← Create a new object of type C
N (C) ← nc + 1
epc ← (objp, objc) %Add an edge from objp to objc
E ← E ∪ {epc}
childrenc ← Children of C in the graph g
((V (cid:48), E(cid:48)), N(cid:48)) ←
Generate SubSkeleton(objc, g, C, childrenc, N, α)
V ← V ∪ V (cid:48)
E ← E ∪ E(cid:48)
N ← N(cid:48)
objc ← An existing object of type C picked randomly with
probability nk/(np − 1) where nk = indegree of objc
epc ← (objp, objc) %Add an edge from objp to objc
E ← E ∪ {epc}

else

I ← (V, E)

in each iteration. So, at the beginning of each iteration, one of the nodes
without parents is picked randomly in the latter case. The iteration process
is continued until the skeleton contains the required number of objects.

18

Figure 5: Relational schema generation steps

3.5 Database population

This process is equivalent to generating data from a Bayesian network. We
can generate as many relational database instances as needed by sampling
from the constructed GBN. The speciﬁcity of the generated tuples is that
they are sampled not only from functional dependencies but also from prob-
abilistic dependencies provided by the randomly generated PRM.

4 Toy example

In this section, we illustrate our proposal through a toy example.

Relational schema generation. Figure 5 presents the result of run-
ning Algorithm 2, with N = 4 classes. For each class, a primary key has
been added (clazz0id, clazz1id, clazz2id and clazz3id). Then a number
of attributes has been generated randomly together with a set of possible
states for each attribute using the policies described in Section 5.2 (e.g.,
clazz0 has 3 descriptive attributes att0, att1 and att2. att0 is a binary vari-
able). Finally, foreign key attributes have been speciﬁed following the DAG
structure of the graph G (e.g., clazz2 references class clazz1 using foreign
key attribute clazz1f katt12).

19

Clazz0Clazz1Clazz2Clazz3n1n0n2n3Clazz0Clazz1Clazz2Clazz3att0att1att0att1att2att0att1att2att3att0Clazz0Clazz1Clazz2Clazz3clazz0idclazz1idclazz3idclazz2idclazz1idclazz0idclazz3idclazz2idClazz0Clazz1Clazz2Clazz3att0att1att0att1att2att0att1att2att3att0clazz1idclazz0idclazz3idclazz2id#clazz0fkatt03#claszz1fkatt13#clazz2fkatt23#clazz1fkatt12#clazz1fkatt10Ggenerate primary keysgenerate attributes+generate statesgenerate foreign keysgenerate foreign keys2344N=41generate DAGFigure 6: Graph dependency structure generation

Figure 7: Example of a generated relational schema where the dotted lines
represent referential constraints and the generated PRM dependency struc-
ture where the arrows represent probabilistic dependencies. Here, we have
excluded slot chains to not overload the ﬁgure. Details about slot chains from
which probabilistic dependencies have been detected are given in Paragraph
PRM generation.

20

Clazz0Clazz1Clazz2Clazz3att0att1att0att0att0#clazz1fkatt10clazz0fkatt03claszz1fkatt13clazz2fkatt23clazz1fkatt12att2att1att3att1att2Clazz0Clazz1Clazz2Clazz3att0att1att0att0att0clazz1fkatt10clazz0fkatt03claszz1fkatt13clazz2fkatt23clazz1fkatt12att2att1att3att1att2MODEMODEMODEPRM generation. We recall that this process is performed in two
steps: randomly generate the dependency structure S (Algorithm 3), then
randomly generate the conditional probability distributions which is similar
to parameter generation of a standard BN. The random generation of the S
is performed in two phases. We start by constructing the DAG structure,
the result of this phase is in Figure 6. Then, we ﬁx a maximum slot chain
length Kmax to randomly determine from which slot chain the dependency
has been detected. We use Kmax = 3, the result of this phase gives rise to
the graph dependency structure of Figure 7. S contains 5 intra-class and 5
inter-class probabilistic dependencies.
Three of the inter-class dependencies have been generated from slot chains
of length 1:
Clazz0.clazz1f katt10.att1 → Clazz0.att2;
M ODE(Clazz2.clazz2f katt23−1.att0) → Clazz2.att3 and;
Clazz2.clazz1f katt12.att1 → Clazz2.att3
One from slot chain of length 2:
M ODE(Clazz2.clazz1f katt12.clazz1f katt12−1.att0) → Clazz2.att3
One from slot chain of length 3:
M ODE(Clazz2.clazz2f katt23−1.claszz1f katt13.clazz1f katt10−1.att2) →
Clazz2.att3

GBN creation. Once the PRM is generated, we follow the two steps
presented in Section 3.4 to create a GBN and to populate the DB instance.
We create a relational skeleton for the relational schema by performing depth
ﬁrst search on the schema DAG (cf. Algorithm 5). The ﬁrst three iterations
of the DFS are shown in ﬁgures 8 and 9. As the schema has only one node
without any parent (i.e., a class without any foreign key), one complete DFS
returns a set of objects of each class as shown in ﬁgure 8. At each iteration,
we obtain diﬀerent number of objects. As we can see in ﬁgure 9, the ﬁrst
iteration created ﬁve objects whereas the second and third iteration resulted
in four and two objects respectively. We continue the iteration until we ob-
tain the required number of objects in the skeleton. We then instantiate the
probabilistic model generated in the previous step with the generation skele-
ton to obtain a ground Bayesian network. Sampling this GBN enables us to
populate values for all attributes of all objects in the relational skeleton. For
this example, we generated a random dataset with 2500 objects. The corre-
sponding schema diagram is shown in ﬁgure 10, which also shows the number
of objects of each class. The diagram is generated using SchemaSpy9.

9http://schemaspy.sourceforge.net/

21

(a)

(b)

(c)

(d)

(e)

(f)

Figure 8: Generating objects while performing Depth First Search (DFS) on
the relational schema. This ﬁgure shows one iteration of a DFS performed
on the schema of ﬁgure 5. In each of the sub-ﬁgures above, the upper graph
is the concerned relational schema and the lower graph is the relational
skeleton being generated. The node (and the edge) encountered at each
step of the DFS is shown by thick lines. Colors are used to only as a visual
aid to distinguish between objects of diﬀerent classes and add no signiﬁcant
meaning to the process. (8a) We begin by creating a new object of the node
‘Class3’ as it does not have any parent. (8b) Then, we traverse to one of the
children of this node in the schema (‘Class2’ here). As there is no object of
this class so far, a new object will be created. (8c) and (8d) Now, continuing
the DFS, we encounter the node ‘Class1’ (the child of ‘Class2’), and then
‘Class0’ (a child of ‘Class3’). Like earlier, new objects of ‘Class1’ and ‘Class0’
will be generated. (8e) As ‘Class0’ has a child, we reach ‘Class1’. At this
step, an object of ‘Class1’ is already present. So the object ‘Class01’ can
either create a new object or get connected to ‘Class11’. In this example,
it gets linked to a new object ‘Class12’. (8f) In the next step of the DFS,
‘Class1’ is encountered again as it is a child of ‘Class3’. Here, ‘Class31’ gets
attached to an existing object of ‘Class1’.

22

)
b
(

)
a
(

e
t
a
r
e
n
e
g

o
t

8

e
r
u
g
ﬁ

f
o

n
o
i
t
a
r
e
t
i

t
s
r
ﬁ

e
h
t

i

g
n
w
o
l
l
o
f

5

e
r
u
g
ﬁ

f
o

a
m
e
h
c
s

l
a
n
o
i
t
a
l
e
r

e
h
t

n
o

S
F
D

f
o

s
n
o
i
t
a
r
e
t
i

o
w
t

t
x
e
N

:
9

e
r
u
g
i
F

.
t
n
e
r
a
p

y
n
a

e
v
a
h

t
o
n

s
e
o
d

t
i

s
a

d
e
t
a
r
e
n
e
g

e
b

s
y
a
w
l
a

l
l
i

w

’
3
s
s
a
l
C

‘

f
o

t
c
e
j
b
o
w
e
n

a

,

n
o
i
t
a
r
e
t
i

h
c
a
e

t
A

.

h
p
a
r
g

n
o
t
e
l
e
k
s

l
a
n
o
i
t
a
l
e
r

e
h
t

,
e
r
e
H

.
s
t
c
e
j
b
o
w
e
n

e
h
t

r
o
f

n
o

s
e
o
g

g
n
i
h
t

e
m
a
s

e
h
t

d
n
a

,
e
n
o
w
e
n

a

r
o

t
c
e
j
b
o

g
n

i
t
s
i
x
e

n
a

o
t

d
e
k
n

i
l

e
b

n
e
h
t

l
l
i

w
t
c
e
j
b
o

e
h
T

n
o
i
t
a
r
e
t
i

d
r
i
h
t

e
h
t

s
a
e
r
e
h
w

,
s
t
c
e
j
b
o

w
e
n

r
u
o
f

s
e
t
a
e
r
c

n
o
i
t
a
r
e
t
i

d
n
o
c
e
s

e
h
T

.
s
t
c
e
j
b
o

e
v
ﬁ

s
a
h

n
o
i
t
a
r
e
t
i

t
s
r
ﬁ

e
h
t

r
e
t
f
a

n
o
t
e
l
e
k
s

.
s
t
c
e
j
b
o

o
w
t

y
l

n
o

s
e
t
a
e
r
c

23

Figure 10: Schema diagram of our toy example showing the number of
objects/rows in each class.

5

Implementation

This section explains the implementation strategy of our generator, identiﬁes
the chosen policies and discusses the complexity of the algorithms.

5.1 Software implementation

The proposed algorithms have been implemented in PILGRIM10 API, a
software platform that our lab is actively developing to provide an eﬃcient
tool to deal with several probabilistic graphical models (e.g., BNs, Dynamic
BNs, PRMs). Developed in C++, PILGRIM uses Boost graph library11
to manage graphs, ProBT API12 to manipulate BNs objects and Database
Template Library (DTL)13 to communicate with databases. Currently, only
PostgreSQL RDBMS is supported in this platform.

Besides the algorithms, we have also implemented serialization of PRMs.
Because there is currently no formalization of PRMs, we propose an en-
hanced version of the XML syntax of the ProbModelXML speciﬁcation14
to serialize our generated models. We have added new tags to specify

10http://pilgrim.univ-nantes.fr/
11http://www.boost.org/
12http://www.probayes.com/fr/Bayesian-Programming-Book/downloads/
13http://dtemplatelib.sourceforge.net/dtl introduction.htm
14http://www.cisiad.uned.es/techreports/ProbModelXML.pdf

24

notions related to relational schema deﬁnition and we used the standard
<AdditionalProperties> tags to add further notions related to PRMs (e.g.,
aggregators associated with dependencies, classes associated with nodes).

5.2

Implemented policies

Policy for generating the relational schema DAG structure. To
randomly generate the relational schema DAG structure, we use PMMixed
algorithm (cf. Section 2.1), which generates uniformly distributed DAGs
in the DAGs space. The structure generated by this algorithm may be a
disconnected graph whereas we are in need of a DAG structure containing
a single connected component. To preserve this condition together with the
interest of generating uniformly distributed examples, we follow the rejection
sampling technique. The idea is to generate a DAG following PMMixed
principle, if this DAG contains just one connected component, then it is
accepted, otherwise it is rejected. We repeat these steps until generating a
DAG structure satisfying our condition.

Policies for generating attributes and their cardinalities. Having
the graphical structure, we continue by generating, for each relation R, a pri-
mary key attribute, a set of attributes A, where card(A)− 1 ∼ P oisson(λ =
1), to avoid empty sets, and for each attribute A ∈ A, we specify a set of
possible states V(A), where card(V(A)) − 2 ∼ P oisson(λ = 1).

Policies for generating the dependency structure. We follow the
PMMixed principle to construct a DAG structure inside each class. Then,
in order to add inter-class dependencies, we use a modiﬁed version of the
PMMixed algorithm where we constrain the choice of adding dependencies
among only variables that do not belong to the same class.

5.3 Complexity of the generation process

We have reported this work to this stage as it is closely related to the choice
of the implementation policies. Let N be the number of relations (classes),
we report the average complexity of each step of the generation process.

Complexity of the relational schema generation process. Al-
gorithm 2 is structured of three loops. Namely, the most expensive one
is the ﬁrst loop dedicated for the DAG structure construction and uses
the PMMixed algorithm. Time complexity of the PMMixed algorithm is
O(N ∗ lg N ). This algorithm is called until reaching the stop condition (i.e.,
a connected DAG). Let T be the average number of calls of the PMMixed
algorithm. T is the ratio of the number of all connected DAG constructed
from N nodes [31] to the number of all DAGs constructed from N nodes [3].
Time complexity of Algorithm 2 is is O(T ∗ N ∗ lg N ).

Complexity of the dependency structure generation process.
As for Algorithm 2, the most expensive operation of Algorithm 3 is the

25

generation of the DAG structure inside each class Xi∈{1...N} ∈ X . Through
Algorithm 2, a set of attributes A(Xi) has been generated for each Xi. As
card(A(Xi))− 1 ∼ P oisson(λ = 1), following Section 5.2, Then the average
number of generated attributes for each class is lambda = 1 + 1 = 2. Then
time complexity of the algorithm is O(N ∗ 2 ∗ lg 2).

Complexity of the slot chains determination process. The most
expensive operation of Algorithm 4 is the Generate P otential Slot chains
method. This latter explores recursively the relational schema graph in
order to ﬁnd all paths (i.e., slot chains) of length k ∈ {0 . . . Kmax}. Time
complexity of this method is O(N Kmax).

Complexity of the relational skeleton generation process. The
relational skeleton generation algorithm is basically an iteration of depth
ﬁrst search over a relational schema. Thus, complexity of the algorithm
would be the same as that of a DFS, i.e. O(V + E) where V and E are
respectively the number of vertices and the number of edges in the graph.

6 Conclusion and perspectives

We have developed a process that allows to randomly generate probabilis-
tic relational models and instantiate them to populate a relational database.
The generated relational data is sampled from not only the functional depen-
dencies of the relational schema but also from the probabilistic dependencies
present in the PRM.

Our process can more generally be used by other data mining methods
as a probabilistic generative model allowing to randomly generated rela-
tional data. Moreover, it can be enriched by test query components to help
database designers to evaluate the eﬀectiveness of their RDBMS compo-
nents.

References

[1] M. Ben Ishak, P. Leray, and N. Ben Amor. Random generation and
population of probabilistic relational models and databases. In Proceed-
ings of the 26th IEEE International Conference on Tools with Artiﬁcial
Intelligence (ICTAI 2014), pages 756–763, 2014.

[2] M. Ben Ishak, P. Leray, and N. Ben Amor. Probabilistic relational
Intelligent

model benchmark generation: Principle and application.
Data Analysis International Journal (to appear), pages ?–?, 2016.

[3] E. A. Bender and R. W. Robinson. The asymptotic number of acyclic

digraphs, ii. J. Comb. Theory, Ser. B, 44(3):363–369, 1988.

26

[4] D. Bitton, C. Turbyﬁll, and D. J. Dewitt. Benchmarking database
systems: A systematic approach. In Proceedings of the 9th International
Conference on Very Large Data Bases, pages 8–19. ACM, 1983.

[5] B. Bollob´as, C. Borgs, J. Chayes, and O. Riordan. Directed scale-free
graphs.
In Proceedings of the fourteenth annual ACM-SIAM sympo-
sium on Discrete algorithms, pages 132–139. Society for Industrial and
Applied Mathematics, 2003.

[6] N. Bruno and S. Chaudhuri. Flexible database generators. In Proceed-
ings of the 31st International Conference on Very Large Data Bases,
pages 1097–1107. ACM, 2005.

[7] R. Chulyadyo and P. Leray. A personalized recommender system from
probabilistic relational model and users’ preferences. In Proceedings of
the 18th Annual Conference on Knowledge-Based and Intelligent Infor-
mation & Engineering Systems, pages 1063–1072, 2014.

[8] G. F. Cooper and E. Herskovits. A Bayesian method for the induction of
probabilistic networks from data. Machine Learning, 9:309–347, 1992.

[9] R. Daly, Q. Shen, and S. Aitken. Learning Bayesian networks: ap-
proaches and issues. The Knowledge Engineering Review, 26:99–157,
2011.

[10] C. J. Date. The Relational Database Dictionary, Extended Edition.

Apress, New York, 2008.

[11] S. Dzeroski and N. Lavrac, editors. Relational Data Mining. Springer

New York Inc., New York, NY, USA, 2001.

[12] E. Fersini, E. Messina, and F. Archetti. Probabilistic relational models
with relational uncertainty: An early study in web page classiﬁcation.
In Proceedings of the International Joint Conference on Web Intelli-
gence and Intelligent Agent Technology, pages 139–142. IEEE Computer
Society, 2009.

[13] N. Friedman, L. Getoor, D. Koller, and A. Pfeﬀer. Learning prob-
In Proceedings of the International Joint

abilistic relational models.
Conference on Artiﬁcial Intelligence, pages 1300–1309, 1999.

[14] L. Getoor. Learning statistical models from relational data. PhD thesis,

Stanford University, 2002.

[15] L. Getoor, D. Koller, N. Friedman, A. Pfeﬀer, and B. Taskar. Probabilis-
tic Relational Models, In Getoor, L., and Taskar, B., eds., Introduction
to Statistical Relational Learning. MA: MIT Press, Cambridge, 2007.

27

[16] J. Gray. Benchmark Handbook: For Database and Transaction Process-
ing Systems. Morgan Kaufmann Publishers Inc., San Francisco, CA,
USA, 1992.

[17] J. Gray, P. Sundaresan, S. Englert, K. Baclawski, and P. J. Weinberger.
Quickly generating billion-record synthetic databases. In Proceedings
of the 1994 ACM SIGMOD international conference on Management
of data, pages 243–252. ACM, 1994.

[18] D. Heckerman, C. Meek, and D. Koller. Probabilistic entity-relationship
models, PRMs, and plate models, In Getoor, L., and Taskar, B., eds.,
Introduction to Statistical Relational Learning. MA: MIT Press, Cam-
bridge, 2007.

[19] M. Henrion. Propagating uncertainty in Bayesian networks by proba-
bilistic logic sampling. In Proceedings of Uncertainty in Artiﬁcial In-
telligence 2 Annual Conference on Uncertainty in Artiﬁcial Intelligence
(UAI-86), pages 149–163, Amsterdam, NL, 1986. Elsevier Science.

[20] J. S. Ide and F. G. Cozman. Random generation of Bayesian networks.
In Brazilian symp.on artiﬁcial intelligence, pages 366–375. Springer-
Verlag, 2002.

[21] J. S. Ide, F. G. Cozman, and F. T. Ramos. Generating random Bayesian
networks with constraints on induced width. In Proceedings of the 16th
Eureopean Conference on Artiﬁcial Intelligence, pages 323–327, 2004.

[22] D. Koller and A. Pfeﬀer. Probabilistic frame-based systems. In Proc.

AAAI, pages 580–587. AAAI Press, 1998.

[23] M. Maier, K. Marazopoulou, D. Arbour, and D. Jensen. A sound and
complete algorithm for learning causal models from relational data. In
Proceedings of the Twenty-ninth Conference on Uncertainty in Artiﬁcial
Intelligence, pages 371–380, 2013.

[24] M. Maier, B. Taylor, H. Oktay, and D. Jensen. Learning causal mod-
els of relational domains. In Proceedings of the Twenty-Fourth AAAI
Conference on Artiﬁcial Intelligence, pages 531–538, 2010.

[25] M. E. Maier, K. Marazopoulou, and D. Jensen. Reasoning about
CoRR,

independence in probabilistic models of relational data.
abs/1302.4381, 2013.

[26] J. Neville and D. Jensen. Relational dependency networks. Journal of

Machine Learning Research, 8:653–692, 2007.

[27] J. Pearl. Probabilistic reasoning in intelligent systems. Morgan Kauf-

mann, San Franciscos, 1988.

28

[28] A. J. Pfeﬀer. Probabilistic Reasoning for Complex Systems. PhD thesis,

Stanford University, 2000.

[29] J. Pitman. Combinatorial stochastic processes. Lecture Notes for St.

Flour Summer School, 2002.

[30] L. De Raedt. Attribute-value learning versus inductive logic program-
In Proceedings of the Eighth International

ming: the missing links.
Conference on Inductive Logic Programming, pages 1–8, 1998.

[31] R. W. Robinson. Counting unlabeled acyclic digraphs, In C. H. C.
LITTLE, Ed., Combinatorial Mathematics V, volume 622 of Lecture
Notes in Mathematics. Springer, Berlin / Heidelberg, 1977.

[32] T. Sommestad, M. Ekstedt, and P. Johnson. A probabilistic relational
model for security risk analysis. Computers & Security, 29:659–679,
2010.

[33] A. R. Statnikov, I. Tsamardinos, and C. Aliferis. An algorithm for
generation of large Bayesian networks. Technical report, Department
of Biomedical Informatics, Discovery Systems Laboratory, Vanderbilt
University, 2003.

[34] L. Torti, P. H. Wuillemin, and C. Gonzales. Reinforcing the object-
oriented aspect of probabilistic relational models. In Proceedings of the
5th Probabilistic Graphical Models, pages 273–280, 2010.

[35] P. H. Wuillemin and L. Torti. Structured probabilistic inference. Int.

J. Approx. Reasoning, 53(7):946–968, 2012.

29

