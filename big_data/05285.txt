6
1
0
2

 
r
a

 

M
6
1

 
 
]

V
C
.
s
c
[
 
 

1
v
5
8
2
5
0

.

3
0
6
1
:
v
i
X
r
a

IMAGE LABELING BY ASSIGNMENT

F. ˚ASTR ¨OM, S. PETRA, B. SCHMITZER, C. SCHN ¨ORR

ABSTRACT. We introduce a novel geometric approach to the image labeling problem. Abstracting from
speciﬁc labeling applications, a general objective function is deﬁned on a manifold of stochastic matri-
ces, whose elements assign prior data that are given in any metric space, to observed image measure-
ments. The corresponding Riemannian gradient ﬂow entails a set of replicator equations, one for each
data point, that are spatially coupled by geometric averaging on the manifold. Starting from uniform
assignments at the barycenter as natural initialization, the ﬂow terminates at some global maximum,
each of which corresponds to an image labeling that uniquely assigns the prior data. Our geometric
variational approach constitutes a smooth non-convex inner approximation of the general image label-
ing problem, implemented with sparse interior-point numerics in terms of parallel multiplicative updates
that converge efﬁciently.

CONTENTS

1.
Introduction
1.1. Motivation
1.2. Approach: Overview
1.3. Further Related Work
1.4. Organization
2. The Assignment Manifold
2.1. Geometry of the Probability Simplex
2.2. Riemannian Means
2.3. Assignment Matrices and Manifold
3. Variational Approach
3.1. Basic Components
3.1.1. Features, Distance Function, Assignment Task
3.1.2. Distance Matrix
3.1.3. Likelihood Matrix
3.1.4. Similarity Matrix
3.2. Objective Function, Optimal Assignment
3.2.1. Objective Function
3.2.2. Assignment Mapping
3.2.3. Optimization Approach
3.3.
3.3.1. Assignment Normalization
3.3.2. Computing Riemannian Means

Implementation

2
2
4
5
7
7
7
9
10
11
11
11
11
12
13
13
13
14
14
16
16
16

Date: March 18, 2016.
2010 Mathematics Subject Classiﬁcation. 62H35, 65K05, 68U10, 62M40.
Key words and phrases.

Image labeling, assignment manifold, Fisher-Rao metric, Riemannian gradient ﬂow, replicator

equations, information geometry, neighborhood ﬁlters, nonlinear diffusion.

Support by the German Research Foundation (DFG) is gratefully acknowledged, grant GRK 1653.

1

2

F. ˚ASTR ¨OM, S. PETRA, B. SCHMITZER, C. SCHN ¨ORR

Illustrative Applications and Discussion

3.3.3. Optimization Algorithm
3.3.4. Termination Criterion
4.
4.1. Parameters, Empirical Convergence Rate
4.2. Vector-Valued Data
4.3. Patches
4.4. Unsupervised Assignment
4.5. Labeling with Adaptive Distances
4.6.
5. Conclusion and Further Work
Appendix A. Basic Notation
Appendix B. Proofs and Further Details
B.1. Proofs of Section 2
B.2. Proofs of Section 3 and Further Details
References

Image Inpainting

1. INTRODUCTION

17
18
18
18
18
19
25
26
29
29
31
33
33
34
36

1.1. Motivation. Image labeling is a basic problem of variational low-level image analysis. It amounts
to determining a partition of the image domain by uniquely assigning to each pixel a single element
from a ﬁnite set of labels. Most applications require such decisions to be made depending on other
decisions. This gives rise to a global objective function whose minima correspond to favorable label
assignments and partitions. Because the problem of computing globally optimal partitions generally
is NP-hard, relaxations of the variational problem only deﬁne computationally feasible optimization
approaches.

Continuous models and relaxations of the image labeling problem were studied e.g. in [LS11,
CCP12], including the speciﬁc binary case, where two labels are only assigned [CEN06] and the
convex relaxation is tight, such that the global optimum can be determined by convex programming.
Discrete models prevail in the ﬁeld of computer vision. They lead to polyhedral relaxations of the im-
age partitioning problem that are tighter than those obtained from continuous models after discretiza-
tion. We refer to [KAH+15] for a comprehensive survey and evaluation. Similar to the continuous
case, the binary partition problem can be efﬁciently and globally optimal solved using a subclass of
binary discrete models [KZ04].

Relaxations of the variational image labeling problem fall into two categories: convex and non-
convex relaxations. The dominant convex approach is based on the local-polytope relaxation, a par-
ticular linear programming (LP-) relaxation [Wer07]. This has spurred a lot of research on developing
speciﬁc algorithms for efﬁciently solving large problem instances, as they often occur in applications.
We mention [Kol06] as a prominent example and otherwise refer again to [KAH+15]. Yet, models
with higher connectivity in terms of objective functions with local potentials that are deﬁned on larger
cliques, are still difﬁcult to solve efﬁciently. A major reason that has been largely motivating our
present work is the non-smoothness of optimization problems resulting from convex relaxation – the
price to pay for convexity.

Major classes of non-convex relaxations are based on the mean-ﬁeld approach [Orl85], [WJ08,
Section 5] or on approximations of the intractable entropy of the probability distribution whose neg-
ative logarithm equals the functional to be minimized [YFW05]. Examples for early applications
of relaxations of the former approach include [HH93, HB97]. The basic instance of the latter class

IMAGE LABELING BY ASSIGNMENT

3

of approaches is known as the Bethe-approximation. In connection with image labeling, all these
approaches amount to non-convex inner relaxations of the combinatorially complex set of feasible
solutions (the so-called marginal polytope), in contrast to the convex outer relaxations in terms of the
local polytope discussed above. As a consequence, the non-convex approaches provide a mathemati-
cally valid basis for probabilistic inference like computing marginal distributions, which in principle
enables a more sophisticated data analysis than mere energy minimization or maximum a-posteriori
inference, to which energy minimization corresponds from a probabilistic viewpoint.

On the other hand, like non-convex optimization problems in general, these relaxations are plagued
by the problem of avoiding poor local minima. Although attempts were made to tame this problem by
local convexiﬁcation [Hes06], the class of convex relaxation approaches has become dominant in the
ﬁeld, because the ability to solve the relaxed problem for a global optimum is a much better basis for
research on algorithms and also results in more reliable software for users and applications.

Both classes of convex and non-convex approaches to the image labeling problem motivate the

present work as an attempt to address the following two issues.

• Smoothness vs. Non-Smoothness. Regarding convex approaches and the development of
efﬁcient algorithms, a major obstacle stems from the inherent non-smoothness of the corre-
sponding optimization problems. This issue becomes particularly visible in connection with
decompositions of the optimization task into simpler problems by dropping complicating con-
traints, at the cost of a non-smooth dual master problem where these constraints have to be
enforced. Advanced bundle methods [KSS12] then seem to be among the most efﬁcient meth-
ods. Yet, how to make rapid progress in systematic way does not seem obvious.

On the other hand, since the early days of linear programming, e.g. [BL89a, BL89b], it has
been known that endowing the feasible set with a proper smooth geometry enables efﬁcient
numerics. Yet, such interior point methods [NT02] are considered as not applicable for large-
scale problems of variational image analysis, due to dense numerical linear algebra steps that
are both too expensive and too memory intensive.

In view of these aspects, our approach may be seen as a smooth geometric approach to

image labeling based on ﬁrst-order, sparse numerical operations.

• Local vs. global optimality. Global optimality distinguishes convex approaches from other
ones and is the major argument for the former ones. Yet, having computed a global optimum of
the relaxed problem, it has to be projected to the feasible set of combinatorial solutions (label-
ings) in a post-processing step. While the inherent suboptimality of this step can be bounded
[LLS13], and despite progress has been made to recover the true combinatorial optimum as
least partially [SSK+16], it is clear that the beneﬁt of global optimality of convex optimization
has to be relativized when it constitutes a relaxation of an intractable optimization problem.
Turning to non-convex problems, on the other hand, raises the two well-known issues: local
optimality of solutions instead of global optimality, and susceptibility to initialization.

In view of these aspects, our approach enjoys the following properties. While being non-
convex, there is a single natural initialization only which makes obsolete the need to search
for a good initialization. Furthermore, the approach returns a global optimum (out of many),
which corresponds to an image labeling (combinatorial solution) without the need of further
post-processing.

Clearly, the latter property is typical for concave minimization formulations of combina-
torial optimization problems [HT96] where solutions of the latter problem are enforced by
weighting the concave penalty sufﬁciently large. Yet, in such cases, and in particular so when

4

F. ˚ASTR ¨OM, S. PETRA, B. SCHMITZER, C. SCHN ¨ORR

FIGURE 1.1. Overview of the variational approach. Given data and prior features
in a metric space F, inference corresponds to a Riemannian gradient ﬂow with re-
spect to an objective function J(W ) on the assignment manifold W. The curve of
matrices W (t) assigns at each t prior data PF to observed data f and terminates at a
global maximum W ∗ that constitutes a labeling, i.e. a unique assignment of a single
prior datum to each data point. Spatial coherence of the labeling ﬁeld is enforced by
geometric averaging over spatial neighborhoods. The entire dynamic process on the
assignment manifold achieves a MAP-labeling in a smooth, geometrical setting, real-
ized with sparse interior-point numerics in terms of parallel multiplicative updates.

working in high dimensions as in image analysis, the problem persists to determine good ini-
tializations and to carefully design the numerics (search direction, step-size selection, etc.), in
order to ensure convergence and a reasonable convergence rate.

1.2. Approach: Overview. Figure 1.1 illustrates our set-up and the approach. We distinguish the
feature space F, that models all application-speciﬁc aspects, and the assignment manifold W used for
modelling the image labeling problem and for computing a solution. This distinction avoids to mix
up physical dimensions, speciﬁc data formats etc. with the representation of the inference problem.
It ensures broad applicability to any application domain that can be equipped with a metric which
properly reﬂects data similarity. It also enables to normalize the representation used for inference, so
as to remove any bias towards a solution not induced by the data at hand.
We consider image labeling as the task to assign to the image data an arbitrary prior data set PF,
provided the distance of its elements to any given data element can be measured by a distance function
dF, which the user has to supply. Basic examples for the elements of PF include prototypical feature
vectors, patches, etc. Collecting all pairwise distance data into a distance matrix D, which could
be computed on the ﬂy for extremely large problem sizes, provides the input data to the inference
problem.
The mapping expW lifts the distance matrix to the assignment manifold W. The resulting likeli-
hood matrix L constitutes a normalized version of the distance matrix D that reﬂects the initial feature
space geometry as given by the distance function dF. Each point on W, like the matrices L, S and W ,
are stochastic matrices with strictly positive entries, that is with row vectors that are discrete probabil-
ity distributions having full support. Each such row vector indexed by i represents the assignment of
prior elements of PF to the given datum a location i, in other words, the labeling of datum i. We equip
the set of all such matrices with the geometry induced by the Fisher-Rao metric and call it assignment
manifold.

The inference task (image labeling) is accomplished by geometric averaging in terms of Riemann-
ian means of assignment vectors over spatial neighborhoods. This step transforms the likelihood ma-
trix L into the similarity matrix S. It also induces a dependency of labeling decisions on each other,

IMAGE LABELING BY ASSIGNMENT

5

akin to the prior (regularization) terms of the established variational approaches to image labeling, as
discussed in the preceding section. These dependencies are resolved by maximizing the correlation
(inner product) between the assignment in terms of the matrix W and the similarity matrix S, where
the latter matrix is induced by W as well. The Riemannian gradient ﬂow of the corresponding ob-
jective function J(W ), that is highly nonlinear but smooth, evolves W (t) on the manifold W until a
ﬁxed point is reached which terminates the loop on the right-hand side of Figure 1.1. The resulting
ﬁxed point corresponds to an image labeling which uniquely assigns to each datum a prior element of
PF.

Adopting a probabilistic Bayesian viewpoint, this ﬁxed-point iteration may be viewed as maximum
a-posterior inference carried out in a geometric setting with multiplicative, sparse and highly parallel
numerical operations.

1.3. Further Related Work. Besides current research on image labeling, there are further classes
of approaches that resemble our approach. We brieﬂy sketch each of them in turn and highlight
similarities and differences.

Neighborhood Filters: A large class of approaches to denoising of given image data f are de-

ﬁned in terms of neighborhood ﬁlters, that iteratively perform operations of the form

(cid:88)

j

(cid:80)

u(k+1)
i

=

K(xi, xj, u(k)
l K(xi, xl, u(k)

, u(k)
j )
, u(k)

i

i

l

u(k)
j

,

)

u(0) = f,

∀i,

(1.1)

where K is a nonnegative kernel function that is symmetric with respect to the two indexed
locations (e.g. i, j in the numerator) and may depend on both the spatial distance (cid:107)xi − xj(cid:107)
and the values |ui − uj| of pairs of pixels. Maybe the most promiment example is the non-
local means ﬁlter [BCM05] where K depends on the distance of patches centered at i and j,
respectively. We refer to [Mil13a] for a recent survey.

Noting that (1.1) is a linear operation with a row-normalized nonnegative (i.e. stochastic)

matrix, a similar situation would be

ui =

Lij(W )uj,

(1.2)

(cid:88)

j

with the likelihood matrix from Fig. 1.1, if we would replace the prior data PF with the given
image data f itself and adopt a distance function dF, in order to mimick the kernel function
K of (1.1).

In our approach, however, the likelihood matrix along with its nonlinear geometric trans-
formation, the similarity matrix S(W ), evolves along with the evolution of assignment matrix
W , so as to determine a labeling with unique assignments to each pixel i, rather than convex
combinations as required for denoising. Furthermore, the prior data set PF that is assigned in
our case, may be very different from the given image data and, accordingly, the assignment
matrix may have any rectangular shape rather than being a quadratic m × m matrix.
Conceptually, we are concerned with decision making (labeling, partitioning, unique as-
signments) rather than with mapping one image to another one. Whenever the prior data PF
comprise a ﬁnite set of prototypical image values or patches, such that a mapping of the form

ui =

Wijf∗
j ,

j ∈ PF ,
f∗

∀i,

(1.3)

(cid:88)

j

is well-deﬁned, then this does result in a transformed image u after having reached a ﬁxed
point of the evolution of W . This result then should not be considered as a denoised image,

6

F. ˚ASTR ¨OM, S. PETRA, B. SCHMITZER, C. SCHN ¨ORR

however. Rather, it merely illustrates the interpretation of the given data f in terms of the prior
data PF and a corresponding optimal assignment.

Nonlinear Diffusion: Neighborhood ﬁlters are closely related to iterative algorithms for nu-
merically solving discretized diffusion equations. Just think of the basic 5-point stencil of
the discrete Laplacian, the iterative averaging of nearest neighbors differences, and the large
class of adaptive generalizations in terms of nonlinear diffusion ﬁlters [Wei98]. More recent
work directly addressing this connection includes [BCM06, SSN09, Mil13b]. The author of
[Mil13b], for instance, advocates the approximation of the matrix of (1.1) by a symmetric
(hence, doubly-stochastic) positive-deﬁnite matrix, in order to enable interpretations of the
denoising operation in terms of the spectral decomposition of the assignment matrix, and to
make the connection to diffusion mappings on graphs.

The connection to our work is implicitly given by the discussion of the previous point, the
relation of our approach to neighborhood ﬁlters. Roughly speaking, the application of our
approach in the speciﬁc case of assigning image data to image data, may be seen as some
kind of nonlinear diffusion that results in an image whose degrees of freedom are given by the
cardinality of the prior set PF. We plan to explore the exact nature of this connection in more
detail in our future work.

Replicator Dynamics: Replicator dynamics and the corresponding equations are well known
[HS03]. They play a major role in models of various disciplines, including theoretical biol-
ogy and applications of game-theory to economy. In the ﬁeld of image analysis, such models
have been promoted by Pelillo and co-workers, mainly to efﬁciently determine by continuous
optimization techniques good local optima of intractable problems, like matchings through
maximum-clique search in an association graph [Pel99]. Although the corresponding objec-
tive functions are merely quadratic, the analysis of the corresponding equations is rather in-
volved [Bom02]. Accordingly, clever heuristics have been suggested to tame related problems
of non-convex optimization [BBPR21].

Regarding our approach, we aim to get rid of these issues – see the discussion of “Global
optimality” in Section 1.1 – through three ingredients: (i) a unique natural initialization, (ii)
spatial averaging that removes spurious local affects of noisy data, and (iii) adopting the Rie-
mannian geometry which determines the structure of the replicator equations, for both geo-
metric spatial averaging and numerical optimization.

Relaxation Labeling: The task of labeling primitives in images has been formulated as a prob-
lem of contextual decision making already 40 years ago [RHZ76, HZ83]. Originally, update
rules were merely formulated in order to ﬁnd mutually consistent individual label assignments.
Subsequent research related these labeling rules to optimization tasks. We refer to [Pel97] for a
concise account of the literature and for putting the approach on mathematically solid ground.
Speciﬁcally, the so-called Baum-Eager theorem was applied in order to show that updates in-
crease the mutual consistency of label assignments. Applications include pairwise clustering
[PP07] that boils down to determining a local optimum by continuous optimization of a non-
convex quadratic form, similar to the optimization tasks considered in [Pel99] and [Bom02].
We attribute the fact that these approaches have not been widely applied to the problems of
non-convex optimization discussed above.

The measure of mutual consistency of our approach is non-quadratic and the Baum-Eager
theorem about polynomial growth transforms does not apply. Increasing consistency follows
from the Riemannian gradient ﬂow that governs the evolution of label assignments. Regard-
ing the non-convexity from the viewpoint of optimization, we believe that the set-up of our

IMAGE LABELING BY ASSIGNMENT

7

approach displayed by Fig. 1.1 signiﬁcantly alleviates these problems, in particular through
the geometric averaging of assignments that emanates from a natural initialization.

We address again some of these points, that are relevant our future work, in Section 5.

1.4. Organization. Section 2 summarizes the geometry of the probability simplex in order to deﬁne
the assignment manifold, which is the basis of our variational approach. The approach is presented in
Section 3 by repeating the discussion of Figure 1.1, together with the mathematical details. Finally,
several numerical experiments are reported in Section 4. They are academical, yet non-trivial, and
supposed to illustrate properties of the approach as claimed in the preceding sections. Speciﬁc ap-
plications of image labeling are not within the scope of this paper. We conclude and indicate further
directions of research in Section 5.

Major symbols and the basic notation used in this paper are listed in Appendix A. In order not to
disrupt the ﬂow of reading and reasoning, proofs and technical details, all of which are elementary but
essentially complement the presentation and make this paper self-contained, are listed as Appendix B.

2. THE ASSIGNMENT MANIFOLD

In this section, we deﬁne the feasible set for representing and computating image labelings in terms
of assignment matrices W ∈ W, the assignment manifold W. The basic building block is the open
probability simplex S equipped with the Fisher-Rao metric. We collect below and in Appendix B.1
corresponding deﬁnitions and properties.

For background reading and much more details on information and Riemannian geometry, we refer

to [AN00] and [Jos05].
2.1. Geometry of the Probability Simplex. The relative interior S = ˚∆n−1 of the probability sim-
plex given by (A.3f) becomes a differentiable Riemannian manifold when endowed with the Fisher-
Rao metric. In the present particular case, it reads (cf. the notation (A.7))
∀u, v ∈ TpS,

(cid:104)u, v(cid:105)p :=(cid:10) u√

(cid:11),

(2.1)

,

v√
p

p

with tangent spaces given by

TpS = {v ∈ Rn : (cid:104)1, v(cid:105) = 0},

(2.2)
We regard the scaled sphere N = 2Sn−1 as manifold with Riemannian metric induced by the Eu-
clidean inner product of Rn. The following diffeomorphism ψ between Sn and the open subset
ψ(Sn) ⊂ N , was suggested e.g. by [Kas89, Section 2.1] and [AN00, Section 2.5].
Deﬁnition 2.1 (Sphere-Map). We call the diffeomorphism

p ∈ S.

ψ : S → N ,

√
p (cid:55)→ s = ψ(p) := 2

p,

(2.3)

sphere-map (see Fig. 2.1a).

The sphere-map enables to compute the geometry of S from the geometry of the 2-sphere.

Lemma 2.1. The sphere-map ψ (2.3) is an isometry, i.e. the Riemannian metric is preserved. Conse-
quently, lenghts of tangent vectors and curves are preserved as well.

Proof. See Appendix B.1.

(cid:3)

In particular, geodesics as critical points of length functionals are mapped by ψ to geodesics. As a

consequence, we have

8

F. ˚ASTR ¨OM, S. PETRA, B. SCHMITZER, C. SCHN ¨ORR

(b) The geometry of the probability simplex induced by the Fisher-Rao Metric. The
left panel shows Euclidean (black) and non-Euclidean geodesics (brown) connecting
the barycenter (red) and the blue points, along with the corresponding Euclidean and
Riemannian means: In comparison to Euclidean means, geometric averaging pushes
towards the boundary. The right panel shows contour lines of points that have the
same Riemannian distance from the respective center point (black dots). The different
sizes of these regions indicates that geometric averaging causes a larger effect around
the barycenters of both the simplex and its faces, where such points represent fuzzy
labelings, and a smaller effect within regions close to the vertices (unit vectors).

(a) The triangle encloses the
image ψ(S2) ⊂ 2S2 of the
simplex S2 under the sphere-
map (2.3).

FIGURE 2.1. Geometry of the probability simplex S2.

(cid:0)p(0), p(t)(cid:1) (nor-

FIGURE 2.2. Each curve represents the Riemannian distances dSn
malized to [0,1]; Eq. (2.4)) of points on the curve {p(t)}t∈[0,1] that linearly (i.e., Eu-
clidean) interpolates between the ﬁxed vertex p(0) = e1 of the simplex S n = ∆n−1
and the barycenter p(1) = p, for dimensions n = 2k, k ∈ {1, 2, 3, 4, 8}. As the
dimension n grows, the barycenter is located as far away from e1 as all other bound-
ary points ei, tei + (1 − t)ej, t ∈ [0, 1], i, j (cid:54)= 1, etc., which have disjoint supports.
This entails a normalizing effect on the Riemannian mean of points that are far away,
unlike with Euclidean averaging where this inﬂuence increases with the Euclidean
distance.

Lemma 2.2 (Riemannian Distance on S). The Riemannian distance on S is given by

dS(p, q) = 2 arccos

∈ [0, π).

(2.4)

(cid:19)

√

piqi

(cid:18)(cid:88)

i∈[n]

0.20.40.60.81.0t0.20.40.60.81.0dSn(p(0),p(t))IMAGE LABELING BY ASSIGNMENT

9

The objective function for computing Riemannian means (geometric averaging; see Deﬁnition 2.2
and Eq. (2.8) below) is based on the distance (2.4). Figure 2.1b visualizes corresponding geodesics and
level sets on S3, that differ for discrete distributions p ∈ S3 close to the barycenter and for low-entropy
distributions close to the vertices. See also the caption of Fig. 2.1b.

It is well known from the literature (e.g. [Bal97, Led01]) that geometries may considerably change
in higher dimensions. Figure 2.2 displays the Riemannian distances of points on curves that connect
the barycenter and vertices on S n (to which the distance (2.4) extends), depending on the dimension
n. The normalizing effect on geometric averaging, further discussed in the caption, increases with n
and is relevant to image labeling, where large values of n may occur in applications.
Let M be a smooth Riemannian manifold (see the paragraph around Eq. (A.5) introducing our
notation). The Riemannian gradient ∇Mf (p) ∈ TpM of a smooth function f : M → R at p ∈ M is
the tangent vector deﬁned by [Jos05, p. 89]

(cid:104)∇Mf (p), v(cid:105)p = Df (p)[v] = (cid:104)∇f (p), v(cid:105),

∀v ∈ TpM.

We consider next the speciﬁc case M = S = Sn.
Proposition 2.3 (Riemannian Gradient on Sn). For any smooth function f : S → R, the Riemannian
gradient of f at p ∈ S is given by

∇Sf (p) = p(cid:0)∇f (p) − (cid:104)p,∇f (p)(cid:105)1(cid:1).

Proof. See Appendix B.1.

The exponential map associated with the open probability simplex S is detailed next.

Proposition 2.4 (Exponential Map (Manifold S)). The exponential mapping

v (cid:55)→ Expp(v) = γv(1),

p ∈ S,

Expp : Vp → S,
(cid:16)

(cid:16)

is given by

γv(t) =

p − v2
v2
p(cid:107)vp(cid:107)2
p(cid:107)vp(cid:107)2
p +
√
p, p = γ(0), ˙γv(0) = v and
with t = 1, vp = v/

1
2

+

(cid:17)

cos(cid:0)(cid:107)vp(cid:107)t(cid:1) +

(cid:17)
vp(cid:107)vp(cid:107)
Vp =(cid:8)v ∈ TpS : γv(t) ∈ S, t ∈ [0, 1](cid:9).

1
2

p sin(cid:0)(cid:107)vp(cid:107)t(cid:1),

√

(2.7c)
(cid:3)
Proof. See Appendix B.1.
Remark 2.1. Checking the inclusion v ∈ Vp due to (2.7c), for a given tangent vector v ∈ TpS,
is inconvenient for applications. Therefore, the mapping exp is deﬁned below by Eq. (3.8a) which
approximates the exponential mapping Exp, with the feasible set Vp replaced by the entire space TpS
(Lemma 3.1).

Accordingly, geometric averaging as deﬁned next (Section 2.2) based on Exp, can be approximated

as well using the mapping exp. This is discussed in Section 3.3.2.
2.2. Riemannian Means. The Riemannian center of mass is commonly called Karcher mean or
Fr´echet mean in the more recent literature, in particular outside the ﬁeld of mathematics. We pre-
fer – cf. [Kar14] – the former notion and use the shorter term Riemannian mean.
Deﬁnition 2.2 (Riemannian Mean, Geometric Averaging). The Riemannian mean p of a set of points
{pi}i∈[N ] ⊂ S with corresponding weights w ∈ ∆N−1 minimizes the objective function

(2.5)

(2.6)
(cid:3)

(2.7a)

(2.7b)

(cid:88)

i∈[N ]

p (cid:55)→ 1
2

wid2S(p, pi)

(2.8)

10

F. ˚ASTR ¨OM, S. PETRA, B. SCHMITZER, C. SCHN ¨ORR

and satisﬁes the optimality condition [Jos05, Lemma 4.8.4]

(cid:88)

i∈[N ]

with the inverse of the exponential mapping Exp−1

meanS,w(P),

wi Exp−1

p (pi) = 0,
p : S → TpS. We denote the Riemannian mean by
(2.10)

w ∈ ∆N−1, P = {p1, . . . , pN},

(2.9)

N 1N .

and drop the subscript w in the case of uniform weights w = 1
Lemma 2.5. The Riemannian mean (2.10) deﬁned as minimizer of (2.8) is unique for any data P =
{pi}i∈[n] ⊂ S and weights w ∈ ∆n−1.
Proof. Using the isometry ψ given by (2.3), we may consider the scenario transferred to the domain
on the 2-sphere depicted by Fig. 2.1a. Due to [Kar77, Thm. 1.2], the objective (2.8) is convex along
geodesics and has a unique minimizer within any geodesic Ball Br with diameter upper bounded by
κ, where κ upper bounds the sectional curvatures in Br. For the 2-sphere N , we have κ = 1/4
2r ≤ π
√
2
constant, and hence the inequality is satisﬁed for the domain ψ(S) ⊂ N which has geodesic diameter
(cid:3)
π.

We call the computation of Riemannian means geometric averaging. The implementation of this
iterative operation and its efﬁcient approximation by a closed-form expression are adressed in Section
3.3.
2.3. Assignment Matrices and Manifold. A natural question is how to extend the geometry of S to
stochastic matrices W ∈ Rm×n with Wi ∈ S, i ∈ [m], so as to preserve the information-theoretic
properties induced by this metric (that we do not discuss here – cf. [ ˘C82, AN00]).
This problem was recently studied by [MRA14]. The authors suggested three natural deﬁnitions of
manifolds. It turned out that all of them are slight variations of taking the product of S, differing only
by the scaling of the resulting product metric. As a consequence, we make the following
Deﬁnition 2.3 (Assignment Manifold). The manifold of assignment matrices, called assignment man-
ifold, is the set

W = {W ∈ Rm×n : Wi ∈ S, i ∈ [m]}.

(2.11)

According to this product structure and based on (2.1), the Riemannian metric is given by

(cid:104)U, V (cid:105)W :=

(cid:104)Ui, Vi(cid:105)Wi,

U, V ∈ TWW.

(2.12)

(cid:88)

i∈[m]

Note that V ∈ TWW means Vi ∈ TWiS, i ∈ [m].

Remark 2.2. We call stochastic matrices contained in W assignment matrices, due to their role in the
variational approach (Section 3).

IMAGE LABELING BY ASSIGNMENT

11

3. VARIATIONAL APPROACH

We introduce in this section the basic components of the variational approach and the corresponding

optimization task, as illustrated by Figure 1.1.

3.1. Basic Components.

3.1.1. Features, Distance Function, Assignment Task. Let

f : V → F,

i (cid:55)→ fi,

i ∈ V = [m],

(3.1)

denote any given data, either raw image data or features extracted from the data in a preprocessing
step. In any case, we call f feature. At this point, we do not make any assumption about the feature
space F except that a distance function

is speciﬁed. We assume that a ﬁnite subset of F

dF : F × F → R,

PF := {f∗

j }j∈[n],

(3.2)

(3.3)

additionally is given, called prior set. We are interested in the assignment of the prior set to the data
in terms of an assignment matrix

W ∈ W ⊂ Rm×n,

(3.4)
with the manifold W deﬁned by (2.11). Thus, by deﬁnition, every row vector 0 < Wi ∈ S is a discrete
distribution with full support supp(Wi) = [n]. The element
i ∈ [m],

j |fi),
Wij = Pr(f∗
quantiﬁes the assignment of prior item f∗
j to the observed data point fi. We may think of this number
as the posterior probability that f∗
The assignment task asks for determining an optimal assignment W ∗, considered as “explanation”
of the data based on the prior data PF. We discuss next the ingredients of the objective function that
will be used to solve assignment tasks.
3.1.2. Distance Matrix. Given F, dF and PF, we compute the distance matrix
i ∈ [m],

D ∈ Rm×n, Di ∈ Rn, Dij =

j generated the observation fi.

j ∈ [n],

(3.6)

dF (fi, f∗
j ),

1
ρ

ρ > 0,

j ∈ [n],

(3.5)

where ρ is the ﬁrst (from two) user parameters to be set. This parameter serves two purposes. It
accounts for the unknown scale of the data f that depends on the application and hence cannot be
j , j ∈ [n]
known beforehand. Furthermore, its value determines what subset of the prior features f∗
effectively affects the process of determining the assignment matrix W . This will be explained in
detail in Section 3.1.3 in connection with the subsequent processing stage that uses D as input. We
call ρ selectivity parameter.

Furthermore, we set

W = W (0),

Wi(0) :=

1
n

1n,

i ∈ [m].

(3.7)

That is, W is initialized with the uninformative uniform assignment that is not biased towards a solu-
tion in any way.

12

F. ˚ASTR ¨OM, S. PETRA, B. SCHMITZER, C. SCHN ¨ORR

FIGURE 3.1. Illustration of Prop. 3.1. Various geodesics γvi(t), i ∈ [k], t ∈ [t, tmax]
(solid lines) emanating from p (red point) with the same speed (cid:107)vi(cid:107)p = (cid:107)vj(cid:107)p, ∀i, j,
are displayed together with the curves expp(uit), i ∈ [k], t ∈ [t, tmax], where the
vectors ui, vi, i ∈ [k] satisfy (3.9).

3.1.3. Likelihood Matrix. The next processing step is based on the following
Deﬁnition 3.1 (Lifting Map (Manifolds S,W)). The lifting mapping is deﬁned by

exp : TS → S,

(p, u) (cid:55)→ expp(u) =

exp : TW → W,

(W, U ) (cid:55)→ expW (U ) =

peu
(cid:104)p, eu(cid:105) ,

 expW1(U1)

. . .

 ,

expWm(Um)

(3.8a)

(3.8b)

where Ui, Wi, i ∈ [m] index the row vectors of the matrices U, W , and where the argument decides
which of the two mappings exp applies.
Remark 3.1. After replacing the arbitrary point p ∈ S by the barycenter 1
n1n, readers will recognize
the softmax function in (3.8a), i.e. (cid:104) 1
(cid:104)1,eu(cid:105). This function is widely used in
various application ﬁelds of applied statistics (e.g. [SB99]), ranging from parametrizations of distri-
butions, e.g. for logistic classiﬁcation [Bis06], to other problems of modelling [Luc59] not related to
our approach.

n1neu(cid:1) = eu

n1n, eu(cid:105)−1(cid:0) 1

The lifting mapping generalizes the softmax function through the dependency on the base point p.
In addition, it approximates geodesics and accordingly the exponential mapping Exp, as stated next.
We therefore use the symbol exp as mnemomic. Unlike Expp, the mapping expp is deﬁned on the
entire tangent space, cf. Remark 2.1.

Proposition 3.1. Let

v =(cid:0) Diag(p) − pp(cid:62)(cid:1)u,

v ∈ TpS.

Then expp(ut) given by (3.8a) solves

p(0) = p,
and provides a ﬁrst-order approximation of the geodesic γv(t) from (2.7b)

˙p(t) = p(t)u − (cid:104)p(t), u(cid:105)p(t),

expp(ut) ≈ p + vt,

(cid:107)γv(t) − expp(ut)(cid:107) = O(t2).

Proof. See Appendix B.2

(3.9)

(3.10)

(3.11)
(cid:3)

Figure 3.1 illustrates the approximation of geodesics γv and the exponential mapping Expp, respec-

tively, by the lifting mapping expp.

IMAGE LABELING BY ASSIGNMENT

13

peu+c1

Remark 3.2. Note that adding any constant vector c1, c ∈ R to a vector u does not change expp(u):
(cid:104)p,eu+c1(cid:105) = p(ec1)eu
(cid:104)p,eu(cid:105) = expp(u). Accordingly, the same vector v is generated by (3.9).
While the deﬁnition (3.8a) removes this ambiguity, there is no need to remove the mean of the vector
u in numerical computations.

(cid:104)p,(ec1)eu(cid:105) = peu

Given D and W as described in Section 3.1.2, we lift the matrix D to the manifold W by

L = L(W ) := expW (−U ) ∈ W,

Ui = Di − 1
n

(cid:104)1, Di(cid:105)1,

i ∈ [m],

(3.12)

with exp deﬁned by (3.8b). We call L likelihood matrix because the row vectors are discrete probabil-
ity distributions which separately represent the similarity of each observation fi to the prior data PF,
as measured by the distance dF in (3.6).

Note that the operation (3.12) depends on the assignment matrix W ∈ W.

3.1.4. Similarity Matrix. Based on the likelihood matrix L, we deﬁne the similarity matrix

S = S(W ) ∈ W,

Si = meanS{Lj}j∈ ˜NE (i),

i ∈ [m],

(3.13)

˜NE (i) = {i} ∪ NE (i),

where each row is the Riemannian mean (2.10) (using uniform weights) of the likelihood vectors,
indexed by the neighborhoods as speciﬁed by the underying graph G = (V,E),
NE (i) = {j ∈ V : ij ∈ E}.

(3.14)
Thus, S represents the similarity of the data within a local spatial neighborhood to the prior data PF.
Note that S depends on W because L does so by (3.12). The size of the neighbourhoods | ˜NE (i)|
is the second user parameter, besides the selectivity parameter ρ for scaling the distance matrix (3.6).
Typically, each ˜NE (i) indexes the same local “window” around pixel location i. We then call the
window size | ˜NE (i)| scale parameter.
Remark 3.3. In basic applications, the distance matrix D will not change once the features and the
feature distance dF are determined. On the other hand, the likelihood matrix L(W ) and the similarity
matrix S(W ) have to be recomputed as the assignment W evolves, as part of any numerical algorithm
used to compute an optimal assignment W ∗.

We point out, however, that more general scenarios are conceivable – without essentially changing
the overall approach – where D = D(W ) depends on the assignment as well and hence has to be
updated too, as part of the optimization process. Section 4.5 provides an example.
3.2. Objective Function, Optimal Assignment. We specify next the objective function as criterion
for assignments and the gradient ﬂow on the assignment manifold, to compute an optimal assignment
W ∗. Finally, based on W ∗, the so-called assignment mapping is deﬁned.
3.2.1. Objective Function. Getting back to the interpretation from Section 3.1.1 of the assignment
matrix W ∈ W as posterior probabilities,

The functional J together with the feasible set W formalizes the following objectives:

(1) Assignments W should maximally correlate with the feature-induced similarities S = S(W ),

as measured by the inner product which deﬁnes the objective function J(W ).

of assigning prior feature f∗
is

Wij = Pr(f∗

j |fi),

(3.15)
j to the observed feature fi, a natural objective function to be maximized

max
W∈W J(W ),

J(W ) := (cid:104)S(W ), W(cid:105).

(3.16)

14

F. ˚ASTR ¨OM, S. PETRA, B. SCHMITZER, C. SCHN ¨ORR

(3) Maximizers W ∗ should deﬁne image labelings in terms of rows W

(2) Assignments of prior data to observations should be done in a spatially coherent way. This is
accomplished by geometric averaging of likelihood vectors over local spatial neighborhoods,
which turns the likelihood matrix L(W ) into the similarity matrix S(W ), depending on W .
∗
i = eki ∈ {0, 1}n, i, ki ∈
[m], that are indicator vectors. While the latter matrices are not contained in the assignment
manifold W as feasible set, we compute in practice assignments W ∗ ≈ W
arbitrarily close
to such points. It will turn out below that the geometry enforces this approximation.
As a consequence, in view of (3.15), such points W ∗ maximize posterior probabilities, akin
to the interpretation of MAP-inference with discrete graphical models by minimizing corre-
sponding energy functionals. As discussed in Section 1, however, the mathematical structure
of the optimization task of our approach, and the way of fusing data and prior information, are
quite different.

∗

The following statement formalizes the discussion of the form of desired maximizers W ∗.
Lemma 3.2. We have

and the supremum is attained at the extreme points

:=(cid:8)W

W∗

sup
W∈W

J(W ) = m,

i = eki, i ∈ [m], k1, . . . , km ∈ [n](cid:9) ⊂ W,

∗

∗ ∈ {0, 1}m×n : W

(3.17)

(3.18)

corresponding to matrices with unit vectors as row vectors.
(cid:3)
Proof. See Appendix B.2
3.2.2. Assignment Mapping. Regarding the feature space F, no assumptions were made so far, except
for specifying a distance function dF. We have to be more speciﬁc about F only if we wish to
synthesize the approximation to the given data f, in terms of an assignment W ∗ that optimizes (3.16)
and the prior data PF. We denote the corresponding approximation by

u : W → F|V|,

W (cid:55)→ u(W ),

u∗ := u(W ∗),

(3.19)

and call it assignment mapping.
A trivial example of such a mapping concerns cases where prototypical feature vectors f∗j, j ∈ [n]
are assigned to data vectors f i, i ∈ [m]: the mapping u(W ∗) then simply replaces each data vector
by the convex combination of prior vectors assigned to it,

i ∈ [m].

(3.20)

(cid:88)

j∈[n]

u∗i =

W ∗
ijf∗j,
∗

And if W ∗ approximates a global maximum W
(almost) uniquely replaced by some u∗ki = f∗ki.

as characterized by Lemma 3.2, then each fi is

A less trivial example is the case of prior information in terms of patches. We specify the mapping

u for this case and further concrete scenarios in Section 4.
3.2.3. Optimization Approach. The optimization task (3.16) does not admit a closed-form solution.
We therefore compute the assignment by the Riemannian gradient ascent ﬂow on the manifold W,

˙Wij =(cid:0)∇W J(W )(cid:1)

ij = Wij

with

∇iJ(W ) :=

∂

∂Wi

(cid:16)(cid:0)∇iJ(W )(cid:1)
j −(cid:10)Wi,∇iJ(W )(cid:11)(cid:1), Wi(0) =
(cid:17)
(cid:16) ∂

∂

J(W ), . . . ,

J(W )

,

J(W ) =

∂Wi1

∂Win

1
n

1,

j ∈ [n], (3.21a)

i ∈ [m],

(3.21b)

IMAGE LABELING BY ASSIGNMENT

15
which results from applying (2.6) to the objective (3.16). The ﬂows (3.21), for i ∈ [m], are not
independent as the product structure of W (cf. Section 2.3) might suggest. Rather, they are coupled
through the gradient ∇J(W ) which reﬂects the interaction of the distributions Wi, i ∈ [m], due to
the geometric averaging which results in the similarity matrix (3.13).

Observe that, by (3.21a) and (cid:104)1, Wi(cid:105) = 1,

(cid:104)1,

˙Wi(cid:105) = (cid:104)1, Wi∇iJ(W )(cid:105) − (cid:104)Wi,∇iJ(W )(cid:105)(cid:104)1, Wi(cid:105) = 0,

(3.22)
that is ∇W J(W ) ∈ TWW, and thus the ﬂow (3.21a) evolves on W. Let W (t) ∈ W, t ≥ 0 solve
(3.21a). Then, with the Riemannian metric (2.12),

i ∈ [m],

J(cid:0)W (t)(cid:1) =(cid:10)∇W J(cid:0)W (t)(cid:1),

˙W (t)(cid:11)

= (cid:13)(cid:13)∇W J(cid:0)W (t)(cid:1)(cid:13)(cid:13)2

(3.21a)

W (t)

W (t) ≥ 0,

(3.23)

d
dt

that is, the objective function value increases until a stationary point is reached where the Riemannian
gradient vanishes. Clearly, we expect W (t) to approximate a global maximum due to Lemma 3.2,
which all satisfy the condition for stationary points W ,

0 = ˙W i = W i

(3.24)
∗
i = eki for some ki ∈ [n] makes the bracket vanish for the ki-th
Regarding interior stationary points W ∈ W with W ≥ 0 due to the deﬁnition of W, all brackets

because replacing W i in (3.24) by W
equation, whereas all other equations indexed by j (cid:54)= ki, j ∈ [n] are satisﬁed due to W

∗
ij = 0.

on the r.h.s. of (3.24) must vanish, which can only happen if the Euclidean gradient satisﬁes

i ∈ [m],

(cid:0)∇iJ(W ) − (cid:104)W i,∇iJ(W )(cid:105)1(cid:1),

including the case ∇J(W ) = 0. Inspecting the gradient of the objective function (3.16), we get

∇iJ(W ) = (cid:104)W i,∇iJ(W )(cid:105)1,

i ∈ [m]

(cid:88)

∂

∂Wij

k,l

(cid:0)Skl(W )Wkl

(cid:1)

Skl(W )

Wkl + Sij(W ) =: (cid:104)T ij(W ), W(cid:105) + Sij(W ),

(3.25)

(3.26a)

(3.26b)

(cid:104)S(W ), W(cid:105) =

∂

∂Wij

J(W ) =

=

∂Wij

∂

(cid:88)

(cid:16) ∂

∂Wij

k,l

(cid:17)

k,l

S(W ) depend in a smooth but involved way on the
where both matrices S(W ) and T ij(W ) = ∂
data (3.1) and (3.3) through the distance matrix (3.6), the likelihood matrix (3.12) and the geometric
averaging (3.13) which forms the similarity matrix S(W ). Regarding the second term on the r.h.s. of
(3.26b), a computation relegated to Appendix B.2 yields

∂Wij

(cid:88)

−(cid:16)(cid:0)H k(W )(cid:1)−1hk,ij(W )
(cid:17)

(cid:104)T ij(W ), W(cid:105) =

Wkl.

l

(3.27)

The way to compute the somewhat unwieldy explicit form of the r.h.s. is explained by (B.13d) and the
corresponding appendix. In terms of these quantities, condition (3.25) for stationary interior points
translates to
(cid:104)T ij(W ), W(cid:105) + Sij(W ) =

(cid:0)(cid:104)T ij(W ), W(cid:105) + Sij(W )(cid:1)W ij,

∀i ∈ [m],

∀j ∈ [n]

(cid:88)

(3.28)

j

including the special case Sij(W ) = −(cid:104)T ij(W ), W(cid:105), ∀i ∈ [m], j ∈ [n], corresponding to ∇J(W ) =
0. Note that condition (3.28) requires that for every i ∈ [m], the l.h.s. takes the same value for every
j ∈ [n], such that averaging with respect to Wi on the r.h.s. causes no change.

We do not have evidence for the non-existence of speciﬁc data conﬁgurations, for which the ﬂow
(3.21) may reach such very speciﬁc stationary interior points. Any such point, however, will not be a

16

F. ˚ASTR ¨OM, S. PETRA, B. SCHMITZER, C. SCHN ¨ORR

maximum and be isolated, by virtue of the local strict convexity of the objective function (2.8) for Rie-
mannian means (cf. Lemma 2.5 below), which determines the similarity matrix (3.13). Consequently,
any perturbation (e.g. by numerical computation) will let the ﬂow escape from such a point, in order
to maximize the objective due to (3.23).
We summarize this reasoning by the

Conjecture 3.1. For any data (3.1), (3.3), up to a subset of W of measure zero, the ﬂow W (t)
generated by (3.21) approximates a global maximum as deﬁned by (3.18) in the sense that, for any
0 < ε (cid:28) 1, there is a t = t(ε) such that

for some W

∗ ∈ W∗

.

(3.29)

(cid:13)(cid:13)W(cid:0)t(ε)(cid:1) − W

∗(cid:13)(cid:13) ≤ ε,

Remark 3.4.

(1) Since W∗ (cid:54)∈ W, the ﬂow W (t) cannot converge to a global maximum, and numerical prob-
lems arise when (3.29) holds for ε very close to zero. Our strategy to avoid such problems is
described in Section 3.3.1.
(2) Although global maxima are not attained, we agree to call a point W ∗ = W (t) maximum and
optimal assignment, that satisﬁes (3.29) for some ﬁxed small ε. The criterion which terminates
our algorithm is speciﬁed in Section 3.3.4.

(3) Our numerical approximation of the ﬂow (3.21) is detailed in Section 3.3.3.

3.3. Implementation. We discuss in this section speciﬁc aspects of the implementation of the varia-
tional approach.
3.3.1. Assignment Normalization. Because each vector Wi approaches some vertex W
by
construction, and because the numerical computations are designed to evolve on W, we avoid numer-
ical issues by checking for each i ∈ [m] every entry Wij, j ∈ [n], after each iteration of the algorithm
(3.36) below. Whenever an entry drops below ε = 10−10, we rectify Wi by

∗ ∈ W∗

Wi ←

1

(cid:104)1, ˜Wi(cid:105) ˜Wi,

˜Wi = Wi − min
j∈[n]

Wij + ε,

ε = 10−10.

(3.30)

In other words, the number ε plays the role of 0 in our impementation. Our numerical experiments
(Section 4) showed that this operation removed any numerical issues without affecting convergence in
terms of the criterion speciﬁed in Section 3.3.4.
3.3.2. Computing Riemannian Means. Computation of the similarity matrix S(W ) due to Eq. (3.13)
involves the computation of Riemannian means. In view of Deﬁnition (2.2), we compute the Rie-
mannian mean meanS(P) of given points P = {pi}i∈[N ] ⊂ S, using uniform weights, as ﬁxed point
p(∞) by iterating the following steps.

Given p(k), k ≥ 0, compute (cf. the explicit expressions (B.15a) and (2.7))

(1) Set p(0) =

1
n

1.

(2)

vi = Exp−1

p(k)(pi),

i ∈ [N ],

(cid:88)

i∈[N ]

vi,

(3)

v =

1
N

(3.31a)

(3.31b)

(3.31c)

(3.31d)
and continue with step (2) until convergence. In view of the optimality condition (2.9), our implemen-
tation returns p(k+1) as result if after carrying out step (3) the condition (cid:107)v(cid:107)∞ ≤ 10−3 holds.

(4) p(k+1) = Expp(k)(v),

IMAGE LABELING BY ASSIGNMENT

17

We point out that numerical problems arise at step (2) if identical vectors are averaged, as the
expression (B.15a) shows. Such situations may occur e.g. when computer-generated images are pro-
cessed. Setting ε = 1 − (cid:104)√

√
q(cid:105) for two vectors p, q ∈ S, we replace the expression (B.15a) by

p,

Exp−1

p (q) ≈ 9ε2 + 40ε + 480

240(cid:112)1 − ε/2

√
(

pq − (1 − ε)p)

ε < 10−3.

if

(3.32)

Although the iteration (3.31) converges quickly, carrying out such iterations as a subroutine, at each
pixel and iterative step of the outer iteration (3.36), increases runtime (of non-parallel implementa-
tions) noticeably. In view of the approximation of the exponential map Expp(v) = γv(1) by (3.11),
it seems natural to approximate the Riemannian mean as well by modifying steps (2) and (4) above
accordingly.
Lemma 3.3. Replacing in the iteration (3.31) above the exponential mapping Expp by the lifting map
expp (3.8a) yields the closed-form expression

meang(P)
(cid:104)1, meang(P)(cid:105) ,

meang(P) =

(cid:16) (cid:89)

pi(cid:17) 1

N

i∈[N ]

(3.33)

as approximation of the Riemannian mean meanS(P), with the geometric mean meang(P) applied
componentwise to the vectors in P.
Proof. See Appendix B.2

(cid:3)

3.3.3. Optimization Algorithm. A thorough analysis of various discrete schemes for numerically in-
tegrating the gradient ﬂow (3.21), including stability estimates, is beyond the scope of this paper and
will be separately addressed in follow-up work (see Section 5 for a short discussion).

Here, we merely adopted the following basic strategy from [LA83], that has been widely applied in
the literature and performed remarkably well in our experiments. Approximating the ﬂow (3.21) for
each vector Wi, i ∈ [m], by the time-discrete scheme

(cid:0)∇iJ(W (k)) − (cid:104)W (k)

,∇iJ(W (k))(cid:105)1(cid:1), W (k)

i

:= Wi(t(k)

i

),

(3.34)

i

W (k+1)
i
t(k+1)
i

− W (k)
− t(k)

i

i

= W (k)

i

and choosing the adaptive step-sizes t(k+1)

i

1

,∇iJ(W (k))(cid:105), yields the multiplicative updates

−t(k)

(cid:104)W (k)

i =

(cid:0)∇iJ(W (k))(cid:1)

i

,∇iJ(W (k))(cid:105) ,

i

W (k)
(cid:104)W (k)

i

W (k+1)

i

=

i ∈ [m].

(3.35)

We further simplify this update in view of the explicit expression (3.26) of the gradient ∇iJ(W ) of
the objective function, that comprises two terms. The ﬁrst one contributes the derivative of S(W ) with
respect to Wi, which is signiﬁcantly smaller than the second term Si(W ) of (3.26), because Si(W )
results from averaging (3.13) the likelihood vectors Lj(Wj) over spatial neighborhoods and hence
changes slowly. As a consequence, we simply drop this ﬁrst term which, as a byproduct, avoids the
numerical evaluation of the expensive expressions (3.27) specifying the ﬁrst term.

Thus, for computing the numerical results reported in this paper, we used the ﬁxed-point iteration

(cid:0)Si(W (k))(cid:1)

, Si(W (k))(cid:105) ,

i

W (k)
(cid:104)W (k)

i

W (k+1)

i

=

W (0)

i =

1
n

1,

i ∈ [m]

(3.36)

18

F. ˚ASTR ¨OM, S. PETRA, B. SCHMITZER, C. SCHN ¨ORR

together with the approximation due to Lemma 3.3 for computing Riemannian means, which deﬁne
by (3.13) the similarity matrices S(W (k)). Note that this requires to recompute the likelihood matrices
(3.12) as well, at each iteration k (see Fig. 1.1).

3.3.4. Termination Criterion. Algorithm (3.36) was terminated if the average entropy

(cid:88)

(cid:88)

− 1
m

i∈[m]

j∈[n]

W (k)
ij

log W (k)
ij

(3.37)

dropped below a threshold. For example, a threshold value 10−3 means in practice that, up to a tiny
fraction of indices i ⊂ [m] that should not matter for a subsequent further analysis, all vectors Wi are
j , j ∈ [n] to
very close to unit vectors, thus indicating an almost unique assignment of prior items f∗
the data fi, i ∈ [m]. Note that this termination criterion conforms to Conjecture 3.1 and was met in
all experiments.

4. ILLUSTRATIVE APPLICATIONS AND DISCUSSION

We focus in this section on few academical, yet non-trivial numerical examples, to illustrate and
discuss basic properties of the approach. Elaborating any speciﬁc application is outside the scope of
this paper.

4.1. Parameters, Empirical Convergence Rate. Figure 4.2 shows a color image and a noisy version
of it. The latter image was used as input data of a labeling problem. Both images comprise 31 color
vectors forming the prior data set PF = {f 1∗, . . . , f 31∗}. The labeling task is to assign these vectors
in a spatially coherent way to the input data so as to recover the ground truth image.
Every color vector was encoded by the vertices of the simplex ∆30, that is by the unit vectors
{e1, . . . , e31} ⊂ {0, 1}31. Choosing the distance dF (f i, f j) := (cid:107)f i − f j(cid:107)1, this results in unit
distances between all pairs of data points and hence enables to assess most clearly the impact of
geometric spatial averaging and the inﬂuence of the two parameters ρ and |Nε|, introduced in Sections
3.1.2 and 3.1.4, respectively. We refer to the caption for a brief discussion of the selectivity parameter
ρ and the spatial scale in terms of |Nε|.

The reader familiar with total variation based denoising, where a single parameter is only used to
control the inﬂuence of regularization, may ask why two parameters are used in the present approach
and if they are necessary. We refer again to Figure 4.2 and the caption where the separation of the
physical and spatial scale based on different parameter choices is demonstrated and discussed. The
total variation measure couples these scales as the co-area formula explicitly shows. As a consequence,
a single parameter is only needed. On the other, larger values of this parameter lead to the well-
known loss-of-contrast effect, which in the present approach can be avoided by properly choosing the
parameters ρ,|Nε| corresponding to these two scales.

Figure 4.1 shows how convergence of the iterative algorithm (3.36) is affected by these two pa-
rameters. It also demonstrates that few tens of massively parallel outer iterations sufﬁce to reach the
termination criterion of Section 3.3.4.

4.2. Vector-Valued Data. Let f i ∈ Rd denote vector-valued image data or extracted feature vectors
at locations i ∈ [m], and let

PF = {f∗1, . . . , f∗n}

(4.1)

IMAGE LABELING BY ASSIGNMENT

19

i

FIGURE 4.1. Parameter values and convergence rate. Average entropy (3.37) of
the assignment vectors W (k)
as a function of the iteration counter k and the two
parameters ρ and |Nε|, for the labeling task illustrated by Figure 4.2. The left panel
shows that despite high selectivity in terms of a small value of ρ, small spatial scales
necessitate to resolve more conﬂicting assignments through propagating information
by geometric spatial averaging. As a consequence, more iterations are needed to
achieve convergence and a labeling. The right panel, on the other hand, shows that
at a ﬁxed spatial scale |Nε| higher selectivity leads to faster convergence, because
outliers are simply removed from the averaging process, whereas low selectivity leads
to an assignment (labeling) taking all data into account.

denote the prior information given by prototypical feature vectors. In the example that follows below,
f i will be a RGB-color vector. It should be clear, however, that any feature vector of arbitrary dimen-
sion d could be used instead, depending on the application at hand. We used the distance function

dF (f i, f∗j) =

(cid:107)f i − f∗j(cid:107)1,

1
d

(4.2)

with the normalizing factor 1/d to make the choice of the parameter ρ insensitive with respect to the
dimension d of the feature space. Given an optimal assignment matrix W ∗ as solution to (3.16), the
prior information assigned to the data is given by the assignment mapping

ui = ui(W ∗) = EW ∗

[PF ],

i

i ∈ [m],

(4.3)

which merely replaces each data vector f i by the prior vector f∗j assigned to it through W ∗
i .

Figure 4.3 shows the assignment of 20 prototypical color vectors to a color image for various values
of the spatial scale parameter |Nε|, while keeping the selectivity parameter ρ ﬁxed. As a consequence,
the induced assignments and image partitions exhibit a natural coarsening effect in the spatial domain.

f ij ∈ Rd,

j ∈ Np(i),

4.3. Patches. Let f i denote a patch of raw image data (or, more generally, a patch of features vectors)
(4.4)
centered at location i ∈ [m] and indexed by Np(i) ⊂ V (subscript p indicates neighborhoods for
patches). With each entry j ∈ Np(i), we associate the Gaussian weight
i, j ∈ Np(i),

(4.5)
where the vectors xi, xj ∈ Rd correspond to the locations in the image domain indexed by i, j ∈ V.
Speciﬁcally, wp is chosen to be the discrete impulse response of a Gaussian lowpass ﬁlter supported on

ij := Gσ((cid:107)xi − xj(cid:107)),
wp

i ∈ [m],

Nϵ3x3Nϵ5x5Nϵ7x70510152025k0.0010.0100.1001Entropyρ0.01ρ0.01ρ0.1ρ1.002468101214k0.0010.0100.1001EntropyNϵ5x520

F. ˚ASTR ¨OM, S. PETRA, B. SCHMITZER, C. SCHN ¨ORR

(c) ρ = 0.01, |NE| =
3 × 3

(d) ρ = 0.01, |NE| =
5 × 5

(e) ρ = 0.01, |NE| =
7 × 7

(f) ρ = 0.1, |NE| =
3 × 3

(g) ρ = 0.1, |NE| =
5 × 5

(h) ρ = 0.1, |NE| =
7 × 7

(a) Ground truth im-
age.

(b) Noisy input im-
age.

(i) ρ = 1.0, |NE| =
3 × 3

(j) ρ = 1.0, |NE| =
5 × 5

(k) ρ = 1.0, |NE| =
7 × 7

FIGURE 4.2. Parameter inﬂuence on labeling. Panels (a) and (b) show a ground-
truth image and noisy input data. Both images and the prior data set PF are composed
of 31 color vectors. Each color vector is encoded as a vertex of the simplex ∆30. This
results in unit distances between all colors and thus enables an unbiased assessment
of the impact of geometric averaging and the two parameter values ρ,|Nε|. Panels
(c)-(k) show the assignments u(W ∗) for various parameter values where W ∗ maxi-
mizes the objective function (3.16). The spatial scale |NE| increases from left to right.
The results illustrate the compromise between sensitivity to noise and to the geometry
of signal transitions. The selectivity parameter ρ increases from top to bottom. If ρ
is chosen too small, then there is a tendency to noise-induced oversegmentation, in
particular at small spatial scales |NE|. Note, however, that depending on the appli-
cation, the ability to separate the physical and the spatial scale in order to recognize
outliers with small spatial support, while performing diffusion at a larger spatial scale
as in panels (c),(d),(f),(i), may be beneﬁcial. We point out that this separation of the
physical and spatial scales (image range vs. image domain) is not possible with to-
tal variation based regularization where these scales are coupled through the co-area
formula. All results were computed using the assignment mapping (3.20) without
rounding. This shows that the termination criterion of Section 3.3.4, illustrated by
Figure 4.1 leads to (almost) unique assignments.

IMAGE LABELING BY ASSIGNMENT

21

(a) Input image (left) and a section of it. 20 color vectors (right) forming the set prior
data set PF according to Eq. (4.1).

(b) Assignment u(W ∗), |Nε| = 3 × 3, ρ = 0.01.

(c) Assignment u(W ∗), |Nε| = 7 × 7, ρ = 0.01.

(d) Assignment u(W ∗), |Nε| = 11 × 11, ρ = 0.01.

FIGURE 4.3. Image labeling at different spatial scales. The two rightmost columns
show the same information using a random color code for the assignment of the 20
prior vectors to pixel locations, to highlight the induced image partitions. Increasing
the spatial scale |Nε| for a ﬁxed value of the selectivity parameter ρ induces a natu-
ral coarsening of the assignments and the corresponding image partitions along the
spatial scale.

Np(i), so that the scale σ directly depends on the patch size and does not need to be chosen by hand.
Such downweighting of values, that are less close to the center location of a patch, is an established
elementary technique for reducing boundary and ringing effects of patch (“window”)-based image
processing.

The prior information is given in terms of n prototypical patches

and a corresponding distance

PF = {f∗1, . . . , f∗n},

dF (f i, f∗j),

i ∈ [m],

j ∈ [n].

(4.6)

(4.7)

22

F. ˚ASTR ¨OM, S. PETRA, B. SCHMITZER, C. SCHN ¨ORR

Patch

(a)
generating
dictio-
the
nary
by
translation.

(b) Input image f.

(c) Patch assignment
u(W ∗).

Residual

(d)
image
v(W ∗) = f − u(W ∗).

FIGURE 4.4. A patch (a) supposed to represent prior knowledge about the structure
of an image f (b). The dictionary PF of Eq. (4.6) was generated by all translations of
(a) and assigned to the image (b), using a distance dF that adapts the two grayvalues
of each template to the data – see Eqns. (4.14) and (4.15). The resulting assignment
u(W ∗) is depicted by (c). Panel (d) shows the residual image v(W ∗) := f − u(W ∗)
by substracting (c) from (b) (rescaled for better visibility). The result (c) illustrates
how the approximation of f is restricted by the prior knowledge, leading to normal-
ized signal transitions regarding both the spatial geometry and the signal values. By
maximizing the objective (3.16), a patch-consistent and dense cover of the image is
computed. It induces a strong nonlinear image ﬁltering effect by fusing through as-
signment for each single pixel value more than 200 predictions of possible values
based on the patch dictionary PF. The approach enables to model additive image de-
compositions f = u + v (i.e. image = geometry + texture & noise) for speciﬁc image
classes, which are implicitly represented by the dictionary PF.

There are many ways to choose this distance depending on the application at hand. We refer to the
Examples 4.1 and 4.2 below. Expression (4.7) is based on the tacit assumption that patch f∗j is
centered at i and indexed by Np(i) as well.
Given an optimal assignment matrix W ∗, it remains to specify how prior information is assigned
to every location i ∈ V, resulting in a vector ui = ui(W ∗) that is the overall result of processing the
input image f. Location i is affected by patches that overlap with i. Let us denote the indices of these
patches by

Every such patch is centered at location j to which prior patches are assigned by

(4.8)

(4.9)

N i←j

p

:= {j ∈ V : i ∈ Np(j)}.

[PF ] =

EW ∗

j

W ∗
jkf∗k.

1(cid:80)

j(cid:48)∈N i←j

p

wp
jij

wp

j(cid:48)ij

j∈N i←j

p

(cid:88)

k∈[n]

(cid:88)

k∈[n]

(cid:88)

Let location i be indexed by ij in patch j (local coordinate inside patch j). Then, by summing over
all patches indexed by N i←j
p whose supports include location i, and by weighting the contributions to
location i by the corresponding weights (4.5), we obtain the vector

ui = ui(W ∗) =

W ∗
jkf∗kij

∈ Rd,

(4.10)

(cid:88)

wp

j(−i)

j∈Np(i)

(cid:88)

k∈[n]

(cid:88)

k∈[n]

(4.11)

(4.12)

(4.13)

IMAGE LABELING BY ASSIGNMENT

23
that is assigned by W ∗ to location i. This expression looks more clumsy than it actually is. In words,
the vector ui assigned to location i is the convex combination of vectors contributed from patches
overlapping with i, that itself are formed as convex combinations of prior patches. In particular, if we
consider the common case of equal patch supports Np(i) for every i, that additionally are symmetric
with respect to the center location i, then N i←j
p = Np(i). As a consequence, due to the symmetry of
the weights (4.5), the ﬁrst sum of (4.10) sums up all weights wp
ij. Hence, the normalization factor on
the right-hand side of (4.10) equals 1, because the low-pass ﬁlter wp preserves the zero-order moment
(mean) of signals. Furthermore, it then makes sense to denote by (−i) the location ip corresponding
to i in patch j. Thus (4.10) becomes

ui = ui(W ∗) =

W ∗
jkf∗k(−i).

Introducing in view of (4.9) the shorthand

[PF ] :=

Ei
W ∗

j

W ∗
jkf∗k(−i)

for the vector assigned to i by the convex combination of prior patches assigned to j, we ﬁnally rewrite
(4.10) due the symmetry wp

ij in the more handy form1

j(−i) = wp

ji = wp

ui = ui(W ∗) = Ewp(cid:2)Ei

[PF ](cid:3).

W ∗

j

The inner expression represents the assignment of prior vectors to location i by ﬁtting prior patches to
all locations j ∈ N (i). The outer expression fuses the assigned vectors. If they were all the same, the
outer operation would have no effect, of course.

We discuss further properties of this approach by concrete examples.

Example 4.1 (Patch Assignment). Figure 4.4 shows an image f and the corresponding assignment
u(W ∗) based on a patch dictionary PF that was formed as explained in the caption.

We chose the distance dF of Eq. (4.2),

dF (f i, f∗j) =

(4.14)
where here the arguments f i, f∗j stand for the vectorized scalar-valued patches centered at location i,
after adapting each prior template f∗j at each pixel location i to the data f, denoted by f∗j = f∗j(i)
in (4.14). Each such template takes two values that were adapted to the template f i to which it is
compared, i.e.

|Np(i)|(cid:107)f i − f∗j(i)(cid:107)1,

1

where

∀k,

∗j(i)
f
k

∈ {f i

low, f i

high},

low = median(cid:8)f i
high = median(cid:8)f i

f i
f i

j : j ∈ Np(i), f i
j : j ∈ Np(i), f i

j < median{f i
j ≥ median{f i

j}j∈Np(i)
j}j∈Np(i)

(cid:9),
(cid:9).

(4.15a)

(4.15b)
(4.15c)

The result u∗ = u(W ∗) demonstrates

• the “best explanation” of the given image f in term of the (rudimentary) prior knowledge,
• a pronounced nonlinear ﬁltering effect due to the consistent assignment of more than 200

patches at each pixel location and fusing the corresponding predicted values, and

1For locations i close to the boundary of the image domain where patch supports Np(i) shrink, the deﬁnition of the

vector wp has to be adapted accordingly.

24

F. ˚ASTR ¨OM, S. PETRA, B. SCHMITZER, C. SCHN ¨ORR

• a corresponding normalization of the irregular signal structure of f regarding both the spatial

geometry and the signal values.

It is also evident that the approach enables additive image decompositions

(4.16)
that are more discriminative, with respect to image classes modelled by the prior data PF and a corre-
sponding distance dF, than additive image decompositions achieved by convex variational approaches
(see, e.g., [AGCO06]) that employ various regularizing norms, for this purpose.

f = u(W ∗) + v(W ∗),

dark, f∗

Example 4.2 (Patch Assignment). Figure 4.5 shows a ﬁngerprint image characterized by two grey
values f∗
bright, that were extracted from the histogram of f after removing a smooth function of
the spatially varying mean value (panel (b)). The latter was computed by interpolating the median
values for each patch of a coarse 16 × 16 partition of the entire image.

Figure 4.5c shows the dictionary of patches modelling the remaining binary signal transitions. An
essential difference to Example 4.1 is the subdivision of the dictionary into classes of equivalent
patches corresponding to each orientation. The averaging process was set-up to distinguish only the
assignment of patches of different patch classes and to treat patches of the same class equally. This
makes geometric averaging particularly effective if signal structures conform to a single class on
larger spatial connected supports. Moreover, it reduces the problem size to merely 13 class labels: 12
orientations at k · 30◦, k ∈ [12] degrees, together with the single constant patch complementing the
dictionary.
The distance dF (f i, f∗j) between the image patch centered at i and the j-th prior patch was chosen
depending on both the prior patch and the data patch it was compared to: For the constant prior patch,
the distance was
dF (f i, f∗j) =

i f∗j(cid:107)1 with f∗

j}j∈Np(i) ≤ 1

(cid:40)

2 (f∗

dark + f∗

i =

bright),

f∗
dark
f∗
bright

if med{f i
otherwise.

1

|Np(i)|(cid:107)f i−f∗

(4.17)

For all other prior patches, the distance was

dF (f i, f∗j) =

1

|Np(i)|(cid:107)f i − f∗j(cid:107)1.

(4.18)
The center and bottom row of Figure 4.5, respectively, show the assignment u(W ∗) of the dictionary
of 3 × 3 patches (center row) and of 7 × 7 patches (bottom row). The center panels (f) and (i) depict
the class labels of these assignments according to the color code of panel (d). These images display
the interpretation of the image structure of f from panel (a). While the assignment of patches of size
3× 3 is slightly noisy, which becomes visible through the assignment of the constant template marked
by black in panel (f), the assignment of 5× 5 or 7× 7 patches results in a robust and spatially coherent,
accurate representation of the local image structure. The corresponding pronounced nonlinear ﬁltering
effect is due to the consistent assignment of a large number of patches at each pixel location and fusing
the corresponding predicted values.

Panels (g) and (j) show the resulting additive image decompositions

f = u(W ∗) + v(W ∗),

(4.19)

that seem difﬁcult to achieve when using established convex variational approaches (see, e.g., [AGCO06])
that employ various regularizing norms and duality, for this purpose.

Finally, we point out that it would be straighforward to add to the dictionary further patches mod-
elling minutiae and other features relevant to ﬁngerprint analysis. We do not consider in this paper
any application-speciﬁc aspects, however.

IMAGE LABELING BY ASSIGNMENT

25

(a) Input image f.

(b) Contourplot of a
smooth image computed
and subtracted from f as
a preprocessing step.

(c) Prior patches representing binary signal tran-
sitions at orientations 0◦, 30◦, . . . (top row), and
the corresponding translation invariant dictionary
(bottom row). Each row of patches constitutes an
equivalence class of patches.

(d) Color code in-
dicating oriented
bright-to-dark sig-
nal transitions.

(e) Assignment u(W ∗) of 3 × 3
patches to image f from (a).
(ρ = 0.02)

(f) Class label of assigned patches encoded
due to (d). Black means assignment of the
constant template that was added to the dic-
tionary (c).

(g) Residual image v(W ∗) = f −
u(W ∗) (rescaled for visualization).

(h) Assignment u(W ∗) of 7 × 7
patches to image f from (a).
(ρ = 0.02)

(i) Class label of assigned patches
encoded due to (d).

(j) Residual image v(W ∗) = f −
u(W ∗) (rescaled for visualization).

FIGURE 4.5. Analysis of the local signal structure of image (a) by patch assignment.
This process is twofold non-local: (i) through the assignment of 3 × 3 patches (cen-
ter row) and 7 × 7 patches, respectively, and (ii) due to the gradient ﬂow (3.21) that
promotes the spatially coherent assignment of patches corresponding to different ori-
entations of signal transitions, in order to maximize the similarity objective (3.16).

4.4. Unsupervised Assignment. We consider the case that no prior information is available.

The simplest way to handle the absence of prior information is to use the given data themselves as
prior information along with a suitable constraint, to enforce selection of the most important parts by
self-assignment.
In order to illlustrate this mechanism clearly, Figure 4.6 shows as example the assignment of
uniform noise to itself. As prior data PF, we uniformly discretized the rgb-color cube [0, 1]3 at
0, 0.2, 0.4, . . . , 1 along each axis, resulting in |PF| = 63 = 216 color vectors. Because there is

26

F. ˚ASTR ¨OM, S. PETRA, B. SCHMITZER, C. SCHN ¨ORR

(a) Uniform noise.

(b) Sparse assignment u(W ∗) (displayed after rescaling) of 63 color vectors corresponding to
a uniform discretization of the rgb-cube [0, 1]3 to the image (a) yields a noise-induced random
piecewise constant partition through geometric averaging (parameters: |Nε| = 7 × 7, ρ = 0.01).

(c) Relative frequencies of assignment of the prior color vec-
tors f∗j, j ∈ [63]. The 8 non-zero frequencies correspond
to vectors indicated in the color cube (d).

(d) 8 color vectors (out of 63) closest to grey (with equal dis-
tance) only were assigned to (a), resulting in (b). These col-
ors look differently in (b) due to rescaling the image u(W ∗)
to [0, 1]3 for better visibility.

FIGURE 4.6. Unsupervised assignment of uniform noise (a) to itself in terms of a
uniform discretization of the rgb-color cube [0, 1]3 that does not include the color
grey 0.5(1, 1, 1)(cid:62). The assignment selects the 8 colors (d) closest to grey with ran-
dom frequencies (c) and a spatially random partition (b) (rescaled to highlight the
partition).

no preference for any of these vectors, spatial diffusion of uniform noise at any spatial scale will
inherently end up with the average color grey, which however is excluded from the prior set, by con-
struction. Accordingly, the process terminated with a spatially random assignment of the 8 color
vectors closest to grey (Figs. 4.6b rescaled and 4.6d) solely induced by the input noise and geometric
averaging at a certain scale. Figure 4.6c depicts the relative frequencies each prior vector is assigned
to some location. Except for the 8 afore-mentioned vectors, all others are ignored.

A detailed elaboration of unsupervised scenarios based on our approach, for both vector- and patch-

valued data, will be studied in our follow-up work (Section 5).
4.5. Labeling with Adaptive Distances. In this section, we consider a simple instance of the more
general class of scenarios where the distance matrix (3.6) D = D(W ) depends on the assignment
matrix W , in addition to the likelihood matrix L(W ) and the similarity matrix S(W ).

Figure 4.7e displays a point pattern that was generated by sampling a foreground and background
process of randomly oriented rectangles, as explained by the remaining panels of Figure 4.7. The
task is to recover the foreground process among all possible rectangles (Fig. 4.7f) based on (i) unary
features given by the fraction of points covered by each rectangle, and on (ii) the prior knowledge
that unlike background rectangles, elements of the foreground process do not intersect. Rectangles of

50100150200[n]0.050.100.150.20#jIMAGE LABELING BY ASSIGNMENT

27

(a) Collection of rectangu-
lar areas that result in (e) after
uniform point sampling.

(b) Decomposition of
the
rectangles (a) into foreground
(dark, cf.
(c)), and back-
ground (light, cf. (d)).

(c) Randomly oriented fore-
ground rectangles that do not
intersect.

(d) Arbitrary sample of back-
ground rectangles from (f).

(e)
Input data: point pat-
tern resulting from uniformly
sampling the rectangles (a).

(f) All possible rectangles
densely cover the domain as
indicated in the center re-
gion (not completely shown
for better visibility).

(g) Assignment (labeling) of the rectangles (f) based on
the data (e): recognized foreground objects from (c) (black)
and recognized background objects from (d) (dashed). Two
foreground objects were erroneously labeled as background
(gray). All remaining rectangles from (f) also belong to
the background, four of which were erroneously labelled as
foreground (white).

FIGURE 4.7. Scenario for evaluating the approach of Section 4.5. (f) illustrates the
set of all rectangles and corresponding subsets (c), (d). Unlike (d), the rectangles (c)
do not intersect. Sampling the rectangles from both (c) and (d), shown together by (a)
and (b), produced the input data (e). The task is to recognize among (f) all foreground
objects (c) based on unary features (coverage of points) and disjunctive constraints
(rectangles should not intersect). Panel (g) shows and discusses the result.

the background process were slightly less densely sampled than foreground rectangles so as to make
the unary features indicative. Due to the overlap of many rectangles (Fig. 4.7a), however, these unary
features are noisy (“weak”).

As a consequence, exploiting the prior knowledge that foreground rectangles do not intersect be-
comes decisive. This is done by determining the intersection pattern of all rectangles (Fig. 4.7f) in
terms of boolean values that are arranged into matrices Rij, for each edge ij of the grid graph whose
vertices correspond to the centroids of the rectangles of Fig. 4.7f: (Rij)k,l = 1 if rectangle k at posi-
tion i intersects with rectangle l at position j, and (Rij)k,l = 0 otherwise. Due to the geometry of the
rectangles, a rectangle at position i may only intersect with 8 × 18 = 144 rectangles located within a
8-neighborhood j ∈ Nε(i). Generalizations to other geometries are straighforward.

The inference task to recover the foreground rectangles (Fig. 4.7c) from the point pattern (Fig. 4.7e)
may be seen as a multi-labeling problem based on an asymmetric Potts-like model: labels correspond

28

F. ˚ASTR ¨OM, S. PETRA, B. SCHMITZER, C. SCHN ¨ORR

(a) Inpainting of the regions marked by
grey through assignment leads to the result
on the right.

(b) Inpainting of the regions marked by
grey through assignment leads to the result
on the right.

FIGURE 4.8. Two instances shown on the left in (a) and (b), adopted from [LS11],
[CCP12] to study the tightness of convex outer relaxations of the image labeling prob-
lem. The task is both to inpaint and to label the grey regions. Our smooth non-convex
approach constitutes an inner approximation that yields the labeling results shown on
the right in (a) and (b), without the need of a separate rounding post-processing step
that projects the solution of convex relaxations onto the feasible set of label assign-
ments (parameters: ρ = 1, |NE (i)| = 3 × 3).

to equally oriented rectangles and have to be determined so as to maximize the coverage of points,
subject to the pairwise constraints that selected rectangles do not intersect. Alternatively, we may
think of binary “off-on” variables that are assigned to each rectangle of Fig. 4.7f, which have to be
determined subject to disjunctive constraints: at each location, at most a single variable may become
active, and pairwise active variables have to satisfy the intersection constraints. Note that in order to
suppress intersecting rectangles, penalizing costs are only encountered if (a subset of) pairs of vari-
ables receive the same value 1 (= active and intersecting). This violates the submodularity constraint
[KZ04, Eq. (7)] and hence rules out global optimization using graph cuts.
Taking all ingredients into account, we deﬁne the distance vector ﬁeld

Di = Di(W ) =

1
ρ

˜Di(W ) = −pi +

,

λ

|Nε(i)|

RijWj,

λ, σ > 0,

(4.20)

(cid:18) ˜Di(W )
(cid:19)

σ

(cid:88)

j∈Nε(i)

where ρ > 0 is the selectivity parameter from (3.6), σ > 0 represents the cost of the additional
label: “none rectangle”, vector pi collects the fractions of points covered by the rectangles at po-
sition i, and λ > 0 weights the inﬂuence of the intersection prior. This latter term is deﬁned
by the matrices Rij discussed above and given by the gradient with respect to W of the penalty

(λ/|Nε(i)|)(cid:80)

ij∈E(cid:104)Wi, RijWj(cid:105).

In [KS08], a continuous optimization approach using DC (difference of convex functions) program-
ming was proposed to compute local minimizers of non-convex functionals similar to (cid:104)D(W ), W(cid:105),
with D given by (4.20). This “Euclidean approach” – in contrast to the geometric approach proposed
here – entails to provide a DC-decomposition of the intersection penalty just discussed and to explic-
itly take into account the afﬁne constraints Wi ∈ ∆n−1. As a result, the DC-approach computes a
local minimizer by solving a sequence of convex quadratic programs.

In order to apply our present approach instead, we bypass the averaging step (3.13) because labels
will most likely be different at adjacent vertices i in our random scenario, and we thus set S(W ) =
L(W ) with L(W ) given by (3.12) based on (4.20). Applying then algorithm (3.36) implicitly handles
all constraints through the geometric ﬂow and computes a local minimizer by multiplicative updates,
within a small fraction of the runtime that the DC approach would need, and without compromising
the quality of the solution (Fig. 4.7g).

IMAGE LABELING BY ASSIGNMENT

29

4.6. Image Inpainting. Inpainting denotes the task to ﬁll in a known region where no image data
were observed or are known to be corrupted, based on the surrounding region and prior information.
Once the feature metric dF is ﬁxed, we assign to each pixel in the region to be inpainted as datum
the uninformativ feature vector f which has the same distance dF (f, f∗
j ) to every prior feature vector
j ∈ PF. Note that there is not need to explicitly compute this data vector f. It merely represents the
f∗
rule for evaluating the distance dF if one of its arguments belongs to a region to be inpainted.

Figure 4.8 shows two basic examples that were used by the authors of [LS11] and [CCP12], re-
spectively, to examine numerically the tightness of convex relaxations of the image labeling problem.
Unlike convex relaxations that constitute outer approximations of the combinatorically complex feasi-
ble set of assignments, our smooth non-convex approach may be considered as an inner approximation
that yields results without the need of further rounding, i.e. the need of a post-processing step for pro-
jecting the solution of a convex relaxed problem onto the feasible set.

5. CONCLUSION AND FURTHER WORK

We presented a novel approach to image labeling, formulated in a smooth geometric setting. The
approach contrasts with etablished convex and non-convex relaxations of the image labeling problem
through smoothness and geometric averaging. The numerics boil down to parallel sparse updates, that
maximize the objective along an interior path in the feasible set of assignments and ﬁnally return a
labeling. Although an elementary ﬁrst-order approximation of the gradient ﬂow was only used, the
convergence rate seems competitive. In particular, a large number of labels, like in Section 4.4, does
not slow down convergence as is the case of convex relaxations. All aspects speciﬁc to an application
domain are represented by a single distance matrix D and a single user parameter ρ. This ﬂexibility
and the absence ad-hoc tuning parameters should promote applications of the approach to various
image labeling problems.

Aspects and open points to be addressed in future work include the following.

Numerics: Many alternatives exist to the simple algorithm detailed in Section 3.3.3. An alter-
native ﬁrst-order example are exponential multiplicative updates [CS92], that result from an
explicit Euler discretization of the ﬂow (3.21) rewritten in the form

log(cid:0)Wi(t)(cid:1) = ∇iJ(W ) − (cid:104)Wi,∇iJ(W )(cid:105)1,

d
dt

i ∈ [m].

(5.1)

Of course, higher-order schemes respecting the geometry are conceivable as well. We point
out that the inherent smoothness of our problem formulation paves the way for systematic
progress.
Nonuniform geometric averaging: So far, we did not exploit the degrees of freedom offered
by the weights wi, i ∈ [N ], that deﬁne the Riemannian means by the objective (2.8). Possible
enhancements of the solution-driven adaptivity of the assignment process in this connection
need further investigation.

Connection to nonlinear diffusion: Referring to the discussion of neighborhood ﬁlters and non-
linear diffusion in Section 1.3, research making these connections explicit is attractive be-
cause, apparantly, our approach is not covered by existing work.
Unsupervised scenarios: The nonexistence of a prior data set PF in applications was only
brieﬂy addressed in Section 4.4. In particular, the emergence of labels along with assign-
ments and a corresponding generalization of our approach, deserves attention.

Learning and updating prior information: This fundamental problem ties in with the preced-
ing point: How can we learn and evolve prior information from many assignments over time?

30

F. ˚ASTR ¨OM, S. PETRA, B. SCHMITZER, C. SCHN ¨ORR

We hope for a better mathematical understanding of corresponding models and that our work will
stimulate corresponding research.

IMAGE LABELING BY ASSIGNMENT

31

APPENDIX A. BASIC NOTATION

For n ∈ N, we set [n] = {1, 2, . . . , n}. 1 = (1, 1, . . . , 1)(cid:62) denotes the vector with all components
equal to 1, whose dimension can either be inferred from the context or is indicated by a subscript,
e.g. 1n. Vectors v1, v2, . . . are indexed by lower-case letters and superscripts, whereas subscripts
vi, i ∈ [n], index vector components. e1, . . . , en denotes the canonical orthonormal basis of Rn.
We assume data to be indexed by a graph G = (V,E) with nodes i ∈ V = [m] and associated
locations xi ∈ Rd, and with edges E. A regular grid graph and d = 2 is the canonical example. But G
may also be irregular due to some preprocessing like forming super-pixels, for instance, or correspond
to 3D images or videos (d = 3). For simplicity, we call i location although this actually is xi.
If A ∈ Rm×n, then the row and column vectors are denoted by Ai ∈ Rn, i ∈ [m] and Aj ∈ Rm, j ∈
[n], respectively, and the entries by Aij. This notation of row vectors Ai is the only exception from
our rule of indexing vectors stated above.
The component-wise application of functions f : R → R to a vector is simply denoted by f (v), e.g.

∀v ∈ Rn,

√
√
v := (

√

vn)(cid:62),

(A.1)
Likewise, binary relations between vectors apply component-wise, e.g. u ≥ v ⇔ ui ≥ vi, i ∈ [n],
and binary component-wise operations are simply written in terms of the vectors. For example,

v1, . . . ,

etc.

exp(v) :=(cid:0)ev1, . . . , evn(cid:1)(cid:62)
(cid:17)(cid:62)

(cid:16)

:=

. . . ,

, . . .

,

p
q

pi
qi

pq := (. . . , piqi, . . . )(cid:62),

(A.2)

where the latter operation is only applied to strictly positive vectors q > 0. The support supp(p) =
{pi (cid:54)= 0 : i ∈ supp(p)} ⊂ [n] of a vector p ∈ Rn is the index set of all non-nonvanishing components
of p.
(cid:104)x, y(cid:105) denotes the standard Euclidean inner product and (cid:107)x(cid:107) = (cid:104)x, x(cid:105)1/2 the corresponding norm.

Other (cid:96)p-norms, 1 ≤ p (cid:54)= 2 ≤ ∞, are indicated by a corresponding subscript, (cid:107)x(cid:107)p =(cid:0)(cid:80)

except for the case (cid:107)x(cid:107) = (cid:107)x(cid:107)2. For matrices A, B ∈ Rm×n, the canonical inner product is (cid:104)A, B(cid:105) =
tr(A(cid:62)B) with the corresponding Frobenius norm (cid:107)A(cid:107) = (cid:104)A, A(cid:105)1/2. Diag(v) ∈ Rn×n, v ∈ Rn, is the
diagonal matrix with the vector v on its diagonal.

i∈[d] |xi|p(cid:1)1/p,

Other basic sets and their notation are

the positive orthant
the set of strictly positive vectors,
the ball of radius r centered at p,
the unit sphere
the probability simplex
and its relative interior

closure (not regarded as manifold)
the sphere with radius 2
and the assignment manifold
closure (not regarded as manifold)

+ = {p ∈ Rn : p ≥ 0},
Rn
++ = {p ∈ Rn : p > 0},
Rn
Br(p) = {p ∈ Rn : (cid:107)p(cid:107) ≤ r},
Sn−1 = {p ∈ Rn : (cid:107)p(cid:107) = 1},
+ : (cid:104)1, p(cid:105) = 1},
∆n−1 = {p ∈ Rn
S = ˚∆n−1 = ∆n−1 ∩ Rn
++,
Sn = S with concrete value of n (e.g. S3),
S = ∆n−1,
N = 2Sn−1,
W = S × ··· × S,
W = S × ··· × S,

(m times),
(m times).

(A.3a)
(A.3b)
(A.3c)
(A.3d)
(A.3e)
(A.3f)
(A.3g)
(A.3h)
(A.3i)
(A.3j)
(A.3k)

32

F. ˚ASTR ¨OM, S. PETRA, B. SCHMITZER, C. SCHN ¨ORR

For a discrete distribution p ∈ ∆n−1 and a ﬁnite set S = {s1, . . . , sn} vectors, we denote by

(cid:88)

i∈[n]

the mean of S with respect to p.

Ep[S] :=

pisi

(A.4)

Let M be a any differentiable manifold. Then TpM denotes the tangent space at base point p ∈ M
and TM the total space of the tangent bundle of M. If F : M → N is a smooth mapping between
differentiable manifold M and N , then the differential of F at p ∈ M is denoted by

DF (p) : v (cid:55)→ DF (p)[v].

DF (p) : TpM → TF (p)N ,

(A.5)
If F : Rm → Rn, then DF (p) ∈ Rn×m is the Jacobian matrix at p, and the application DF (p)[v] to a
vector v ∈ Rm means matrix-vector multiplication. We then also write DF (p)v. If F = F (p, q), then
DpF (p, q) and DqF (p, q) are the Jacobians of the functions F (·, q) and F (p,·), respectively.
whereas the Riemannian gradient of a function f : M → R deﬁned on Riemannian manifold M is
denoted by ∇Mf. Eq. (2.5) recalls the formal deﬁnition.

The gradient of a differentiable function f : Rn → R is denoted by ∇f (x) =(cid:0)∂1f (x), . . . , ∂nf (x)(cid:1)(cid:62)

,

γv(t)(cid:12)(cid:12)t=0 = v

d
dt

The exponential mapping [Jos05, Def. 1.4.3]
Expp : TpM → M,

v (cid:55)→ Expp(v) = γv(1),

γv(0) = p,

˙γv(0) =

(A.6)
maps the tangent vector v to the point γv(1) ∈ M, uniquely deﬁned by the geodesic curve γv(t)
emanating at p in direction v. γv(t) is the shortest path on M between the points p, q ∈ M that γv
connects. This miminal length equals the Riemannian distance dM(p, q) induced by the Riemannian
metric, denoted by
(A.7)
i.e. the inner product on the tangent spaces TpM, p ∈ M, that smoothly varies with p. Existence and
uniqueness of geodesics will not be an issue for the manifolds M considered in this paper.
Remark A.1. The exponential mapping Expp should not be confused with

(cid:104)u, v(cid:105)p,

• the exponential function ev used e.g. in (A.1);
• the mapping expp : TpS deﬁned by Eq. (3.8a).

The abbreviations ‘l.h.s.’ and ‘r.h.s.’ mean left-hand side and right-hand side of some equation, re-
spectively. We abbreviate with respect to by ‘wrt.’.

IMAGE LABELING BY ASSIGNMENT

33

APPENDIX B. PROOFS AND FURTHER DETAILS

B.1. Proofs of Section 2.
Proof of Lemma 2.1. Let p ∈ S and v ∈ TpS. We have

Dψ(p) = Diag(p)−1/2

(B.1)

√

and(cid:10)ψ(p), Dψ(p)[v](cid:11) = (cid:104)2
Put γ(t) = ψ−1(cid:0)s(t)(cid:1) = 1
(cid:112)γ(t) ˙s(t) and
(cid:90) b

(cid:107) ˙s(t)(cid:107)dt =

L(s) =

p(cid:105) = 2(cid:104)1, v(cid:105) = 0, that is Dψ(p)[v] ∈ Tψ(p)N . Furthermore,

p, v√

(cid:10)Dψ(p)[u], Dψ(p)[v](cid:11) =(cid:10)u/

√

√
p(cid:105) (2.1)
p, v/

= (cid:104)u, v(cid:105)p,

(B.2)
i.e. the Riemannian metric is preserved and hence also the length L(s) of curves s(t) ∈ N , t ∈ [a, b]:

2 ψ(cid:0)γ(t)(cid:1) ˙s(t) =

2 s(t) ˙s(t) = 1

4 s2(t) ∈ S, t ∈ [a, b]. Then ˙γ(t) = 1
(cid:90) b
(cid:90) b

(cid:29)1/2

(cid:28) ˙γ(t)(cid:112)γ(t)

,

(2.1)
=

dt

(cid:107) ˙γ(t)(cid:107)γ(t)dt = L(γ).

a

Proof of Prop. 2.3. Setting g : N → R, q (cid:55)→ g(s) := f(cid:0)ψ−1(s)(cid:1) with s = ψ(p) = 2

a

a

√

we have

∇N g(s) =

I − s
(cid:107)s(cid:107)

s(cid:62)
(cid:107)s(cid:107)

∇g(s),

(B.4)
because the 2-sphere N = 2Sn−1 is an embedded submanifold, and hence the Riemannian gradient
equals the orthogonal projection of the Euclidean gradient onto the tangent space. Pulling back the
vector ﬁeld ∇N g by ψ using

(cid:19)

(B.3)
(cid:3)

p from (2.3),

˙γ(t)(cid:112)γ(t)
(cid:18)

4
we get with (B.1), (B.4) and (cid:107)s(cid:107) = 2 and hence s/(cid:107)s(cid:107) = 1

p

=

√

1
2
2 ψ(p) =

s2(cid:17)
(cid:16) 1
s(cid:0)∇f (p)(cid:1),
∇g(s) = ∇f(cid:0)ψ−1(s)(cid:1) = ∇f
∇fS(p) =(cid:0)Dψ(p)(cid:1)−1(cid:0)∇N g(ψ(p))(cid:1)
(cid:16)(cid:0)I − √
p(cid:0)∇f (p)(cid:1)(cid:17)
(cid:62)(cid:1)√
= p(cid:0)∇f (p)(cid:1) − (cid:104)p,∇f (p)(cid:105)p,
(cid:69)

p(cid:0)∇f (p)(cid:1) − (cid:104)p,∇f (p)(cid:105)√

√
= Diag(

v√
p
= (cid:104)∇f (p), v(cid:105),
= (cid:104)∇f (p), v(cid:105) − (cid:104)p,∇f (p)(cid:105)(cid:104)1, v(cid:105) (2.2)

(cid:68)√

√
p

p)

p,

p

(B.6c)
which equals (2.6). We ﬁnally check that ∇fS(p) satisﬁes (2.5) (with S in place of M). Using (2.1),
we have

(cid:104)∇fS(p), v(cid:105)p =

∀v ∈ TpS.

(B.7b)
(cid:3)
Proof of Prop. 2.4. The geodesic on the 2-sphere emanating at s(0) ∈ N in direction w = ˙s(0) ∈
Ts(0)N is given by

2
√
Setting s(0) = ψ(p) and w = Dψ(p)[v] = v/

is given by ψ−1(cid:0)s(t)(cid:1) due to Lemma 2.1, which results in (2.7b) after elementary computations. (cid:3)

p, the geodesic emanating at p = γv(0) in direction v

2

s(t) = s(0) cos

t

+ 2

t

.

(B.8)

w
(cid:107)w(cid:107) sin

(cid:16)(cid:107)w(cid:107)

(cid:17)

(cid:16)(cid:107)w(cid:107)

(cid:17)

(B.5)

(B.6a)

(B.6b)

(B.7a)

34

F. ˚ASTR ¨OM, S. PETRA, B. SCHMITZER, C. SCHN ¨ORR

B.2. Proofs of Section 3 and Further Details.

Proof of Prop. 3.1. We have p = expp(0) and

d
dt

expp(ut) =

(cid:104)p, eut(cid:105)peutu − peut(cid:104)p, eutu(cid:105)

(cid:104)p, eut(cid:105)2

= p(t)u − (cid:104)p(t), u(cid:105)p(t),

(B.9)

which conﬁrms (3.10), is equal to (3.9) at t = 0 and hence yields the ﬁrst expression of (3.11). The
second expression of (3.11) follows from a Taylor expansion of (2.7b)

(cid:0)v2
p − (cid:107)vp(cid:107)2p(cid:1)t2,

γv(t) ≈ p + vt +

1
4

vp =

v√
p

.

(B.10)

0 ≤ J(W ) = (cid:80)

(cid:3)
Proof of Lemma 3.2. By construction, S(W ) ∈ W, that is Si(W ) ∈ S, i ∈ [m]. Consequently,
i∈[m] (cid:107)Si(W )(cid:107)(cid:107)Wi(cid:107) < m. The upper bound corresponds
) equal the same unit
(cid:3)

i∈[m](cid:104)Si(W ), Wi(cid:105) ≤ (cid:80)

∗ ∈ W and S(W
to matrices W
vector eki for some ki ∈ [m].

) where for each i ∈ [m], both W

∗
i and Si(W

∗

∗

Explicit form of (3.27). The matrices T ij(W ) = ∂
mality condition (2.9) that each vector Sk(W ), k ∈ [m], deﬁned by (3.13) has to satisfy,

S(W ) are implicitly given through the opti-

∂Wij

Sk(W ) = meanS{Lr(Wr)}r∈ ˜NE (k)

⇔

0 =

(cid:0)Lr(Wr)(cid:1).

Exp−1

(cid:88)
(cid:0)Lr(Wr)(cid:1),

r∈ ˜NE (k)

Sk(W )

(B.11)

(B.12)

Writing

φ(cid:0)Sk(W ), Lr(Wr)(cid:1) := Exp−1

Sk(W )

and temporarily dropping below W as argument to simplify the notation, and using the indicator
function δP = 1 if the predicate P = true and δP = 1 otherwise, we differentiate the optimality
condition on the r.h.s. of (B.11),

(cid:88)
(cid:16)

r∈ ˜NE (k)

∂

∂Wij

(cid:88)
(cid:16) (cid:88)

r∈ ˜NE (k)

0 =

=

=

r∈ ˜NE (k)

(cid:16) ∂

∂Wij

φ(cid:0)Sk(W ), Lr(Wr)(cid:1)

(cid:105)

(cid:17)

∂Wij

(cid:104) ∂
(cid:17)(cid:16) ∂
(cid:17)

∂Wij

=: H k(W )

Sk(W )

+ hk,ij(W ).

DSk φ(Sk, Lr)

Sk(W )

+ δi=rDLr φ(Sk, Lr)

DSk φ(Sk, Lr)

Sk(W )

+ δi∈ ˜NE (k)DLiφ(Sk, Li)

Li(Wi)

(cid:104) ∂

∂Wrj

(cid:105)(cid:17)

Lr(Wr)

(cid:16) ∂

∂Wij

(cid:17)

(B.13a)

(B.13b)

(B.13c)

(B.13d)

Since the vectors φ(Sk, Lr) given by (B.12) are the negative Riemannian gradients of the (locally)
strictly convex objectives (2.8) deﬁning the means Sk [Jos05, Thm. 4.6.1], the regularity of the matri-
ces H k(W ) follows. Thus, using (B.13d) and deﬁning the matrices

T ij(W ) ∈ Rm×n,

T ij
kl (W ) :=

∂Skl(W )

∂Wij

,

i, k ∈ [m],

j, l ∈ [n],

(B.14)

IMAGE LABELING BY ASSIGNMENT

35

results in (3.27). The explicit form of this expression results from computing and inserting into (B.13d)
the corresponding Jacobians Dpφ(p, q) and Dqφ(p, q) of

(cid:0)√

q(cid:105)2

q(cid:105)p(cid:1),

√

pq − (cid:104)√

p,

and

∂

φ(p, q) = Exp−1

p (q) =

dS(p, q)
√
p,

(cid:112)1 − (cid:104)√
(cid:0)ej − Li(Wi)(cid:1).
p (q) = −(cid:0)Dψ(p)(cid:1)−1(cid:16) 1

(cid:104)Wi, e−Ui(cid:105)

e−Uij

Li(Wi) =

∂Wij

Exp−1

∇N d2N(cid:0)ψ(p), ψ(q)(cid:1)(cid:17)

The term (B.15a) results from mapping back the corresponding vector from the 2-sphere N ,

(B.16)
where ψ is the sphere map (2.3) and dN is the geodesic distance on N . The term (B.15b) results from
(cid:3)
directly evaluating (3.12).
Proof of Lemma 3.3. We ﬁrst compute exp−1
p . Suppose
peu
(cid:104)p, eu(cid:105) ,

q = expp(u) =

p, q ∈ S,

u ∈ Rn.

(B.17)

2

,

(B.15a)

(B.15b)

(B.18)

(B.19)

(B.20a)

(B.20b)

(B.21)

(B.22a)

(B.22b)

(B.22c)

(B.23)
(cid:3)

Then

log(q) = log(p) + u − log((cid:104)p, eu(cid:105))1,

and

u = exp−1
Thus, in view of (3.9), we approximate

p (q) ≈ v =(cid:0) Diag(p) − pp(cid:62)(cid:1)u =(cid:0) Diag(p) − 1

p (q) = (I − 1
n
(cid:17)
(cid:16) q

=(cid:0) Diag(p) − pp(cid:62)(cid:1) log

Exp−1

.

n

Applying this to the point set P, i.e. setting

(cid:104)1, log(p) − log(q)(cid:105),

1
n

log((cid:104)p, eu(cid:105)) =

11(cid:62))(cid:0) log(q) − log(p)(cid:1).

p1(cid:62) − pp(cid:62) +

p1(cid:62)) log

1
n

(cid:16) q

(cid:17)

p

pi
p

,

i ∈ [N ],

p

vi =(cid:0) Diag(p) − pp(cid:62)(cid:1) log
(cid:88)

(cid:17)

log(pi) − N log(p)

(cid:0) Diag(p) − pp(cid:62)(cid:1)(cid:16) (cid:88)
(cid:19)
(cid:18) 1
(cid:16) (cid:89)
pi(cid:17) 1
(cid:17)
(cid:16) meang(P)
=:(cid:0) Diag(p) − pp(cid:62)(cid:1)u.

i∈[N ]

i∈[N ]

p

N

1
N

vi =

i∈[N ]

=(cid:0) Diag(p) − pp(cid:62)(cid:1) log
=(cid:0) Diag(p) − pp(cid:62)(cid:1) log

p

step (3) of (3.31) yields
1
N

v :=

Finally, approximating step (4) of (3.31) results in view of Prop. 3.1 in the update of p

expp(u) =

peu
(cid:104)p, eu(cid:105) =

meang(P)
(cid:104)1, meang(P)(cid:105) .

36

F. ˚ASTR ¨OM, S. PETRA, B. SCHMITZER, C. SCHN ¨ORR

REFERENCES

[AGCO06] J.-F. Aujol, G. Gilboa, T. Chan, and S. Osher, Structure-Texture Image Decomposition – Modeling, Algorithms,

[AN00]

[Bal97]

[BBPR21]

and Parameter Selection, Int. J. Comp. Vision 67 (2006), no. 1, 111–136.
S.-I. Amari and H. Nagaoka, Methods of Information Geometry, Amer. Math. Soc. and Oxford Univ. Press,
2000.
K. Ball, An elementary introduction to modern convex geometry, Flavors of Geometry, MSRI Publ., vol. 31,
Cambridge Univ. Press, 1997, pp. 1–58.
I.M. Bomze, M. Budinich, M. Pelillo, and C. Rossi, Annealed Replication: A New Heuristic for the Maximum
Clique Problem, Discr. Appl. Math. 2002 (121), 27–49.

[BCM05] A. Buades, B. Coll, and J.M. Morel, A Review of Image Denoising Algorithms, With a New One, SIAM Multi-

scale Model. Simul. 4 (2005), no. 2, 490–530.

[BCM06] A. Buades, B. Coll, and J.-M. Morel, Neighborhood ﬁlters and PDEs, Numer. Math. 105 (2006), 1–34.
[Bis06]
[BL89a]

C.M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006.
D.A. Bayer and J.C. Lagarias, The nonlinear geometry of linear programming. I. Afﬁne and projective scaling
trajectories, Trans. Amer. Math. Soc. 314 (1989), no. 2, 499–526.

[BL89b]

[Bom02]

[CCP12]

[CEN06]

[CS92]

[HB97]

[Hes06]

[HH93]

[HS03]

[HT96]
[HZ83]

, The nonlinear geometry of linear programming. II. Legendre transform coordinates and central tra-

jectories, Trans. Amer. Math. Soc. 314 (1989), no. 2, 527–581.
I. M. Bomze, Regularity versus Degeneracy in Dynamics, Games, and Optimization: A Uniﬁed Approach to
Different Aspects, SIAM Review 44 (2002), no. 3, 394–414.
A. Chambolle, D. Cremers, and T. Pock, A Convex Approach to Minimal Partitions, SIAM J. Imag. Sci. 5
(2012), no. 4, 1113–1158.
T.F. Chan, S. Esedoglu, and M. Nikolova, Algorithms for Finding Global Minimizers of Image Segmentation
and Denoising Models, SIAM J. Appl. Math. 66 (2006), no. 5, 1632–1648.
A. Cabrales and J. Sobel, On the Limit Points of Discrete Selection Dynamics, J. Economic Theory 57 (1992),
407–419.
T. Hofman and J.M. Buhmann, Pairwise Data Clustering by Deterministic Annealing,
Trans. Patt. Anal. Mach. Intell. 19 (1997), no. 1, 1–14.
T. Heskes, Convexity Arguments for Efﬁcient Minimization of the Bethe and Kikuchi Free Energies, J. Artif. In-
tell. Res. 26 (2006), 153–190.
L. H´erault and R. Horaud, Figure-Ground Discrimination: A Combinatorial Optimization Approach, IEEE
Trans. Patt. Anal. Mach. !ntell. 15 (1993), no. 9, 899–914.
J. Hofbauer and K. Siegmund, Evolutionary Game Dynamics, Bull. Amer. Math. Soc. 40 (2003), no. 4, 479–
519.
R. Horst and H. Tuy, Global Optimization: Deterministic Approaches, 3rd ed., Springer, 1996.
R.A. Hummel and S.W. Zucker, On the Foundations of
Trans. Patt. Anal. Mach. !ntell. 5 (1983), no. 3, 267–287.
J. Jost, Riemannian Geometry and Geometric Analysis, 4th ed., Springer, 2005.

the Relaxation Labeling Processes,

IEEE

IEEE

[Jos05]
[KAH+15] J.H. Kappes, B. Andres, F.A. Hamprecht, C. Schn¨orr, S. Nowozin, D. Batra, S. Kim, B.X. Kausler, T. Kr¨oger,
J. Lellmann, N. Komodakis, B. Savchynskyy, and C. Rother, A Comparative Study of Modern Inference Tech-
niques for Structured Discrete Energy Minimization Problems, Int. J. Comp. Vision 115 (2015), no. 2, 155–184.
H. Karcher, Riemannian Center of Mass and Molliﬁer Smoothing, Comm. Pure Appl. Math. 30 (1977), 509–
541.

[Kar77]

[Kar14]
[Kas89]
[Kol06]

[KS08]

[KSS12]

[KZ04]

[LA83]

[Led01]

, Riemannian Center of Mass and so called karcher mean, http://arxiv.org/abs/1407.2087.

R.E. Kass, The Geometry of Asymptotic Inference, Statist. Sci. 4 (1989), no. 3, 188–234.
V. Kolmogorov, Convergent Tree-Reweighted Message Passing for Energy Minimization,
Trans. Patt. Anal. Mach. Intell. 28 (2006), no. 10, 1568–1583.
J. Kappes and C. Schn¨orr, MAP-Inference for Highly-Connected Graphs with DC-Programming, Pattern Recog-
nition – 30th DAGM Symposium, LNCS, vol. 5096, Springer Verlag, 2008, pp. 1–10.
J. Kappes, B. Savchynskyy, and C. Schn¨orr, A Bundle Approach To Efﬁcient MAP-Inference by Lagrangian
Relaxation, Proc. CVPR, 2012.
V. Kolmogorov and R. Zabih, What Energy Functions Can Be Minimized via Graph Cuts?,
Trans. Patt. Analysis Mach. Intell. 26 (2004), no. 2, 147–159.
V. Losert and E. Alin, Dynamics of Games and Genes: Discrete Versus Continuous Time, J. Math. Biology 17
(1983), no. 2, 241–251.
M. Ledoux, The Concentration of Measure Phenomenon, Amer. Math. Soc., 2001.

IEEE

IEEE

IMAGE LABELING BY ASSIGNMENT

37

[LLS13]

[LS11]

J. Lellmann, F. Lenzen, and C. Schn¨orr, Optimality Bounds for a Variational Relaxation of the Image Partition-
ing Problem, J. Math. Imag. Vision 47 (2013), no. 3, 239–257.
J. Lellmann and C. Schn¨orr, Continuous Multiclass Labeling Approaches and Algorithms, SIAM J. Imaging
Science 4 (2011), no. 4, 1049–1096.
R.D. Luce, Individual Choice Behavior: A Theoretical Analysis, Wiley, New York, 1959.
P. Milanfar, A Tour of Modern Image Filtering, IEEE Signal Proc. Mag. 30 (2013), no. 1, 106–128.

[Luc59]
[Mil13a]
[Mil13b]
[MRA14] G. Mont´ufar, J. Rauh, and N. Ay, On the Fisher Metric of Conditional Probability Polytopes, Entropy 16 (2014),

, Symmetrizing Smoothing Filters, SIAM J. Imag. Sci. 6 (2013), no. 1, 263–284.

[NT02]

[Orl85]
[Pel97]

[Pel99]

[PP07]

no. 6, 3207–3233.
Y.E. Nesterov and M.J. Todd, On the Riemannian Geometry Deﬁned by Self-Concordant Barriers and Interior-
Point Methods, Found. Comp. Math. 2 (2002), 333–361.
H. Orland, Mean-ﬁeld theory for optimization problems, J. Phys. Lettres 46 (1985), no. 17, 763–770.
M. Pelillo, The Dynamics of Nonlinear Relaxation Labeling Processes, J. Math. Imag. Vision 7 (1997), 309–
323.

, Replicator equations, maximal cliques, and graph isomorphism, Neural Comp. 11 (1999), no. 8,

1933–1955.
M. Pavan and M. Pelillo, Dominant Sets and Pairwise Clustering, IEEE Trans. Patt. Anal. Mach. Intell. 29
(2007), no. 1, 167–172.

[RHZ76] A. Rosenfeld, R.A. Hummel, and S.W. Zucker, Scene labeling by relaxation operations, IEEE Trans. Systems,

Man, and Cyb. 6 (1976), 420–433.
R.S. Sutton and A.G. Barto, Reinforcement Learning, 2nd ed., MIT Press, 1999.

[SB99]
[SSK+16] P. Swoboda, A. Shekhovtsov, J.H. Kappes, C. Schn¨orr, and B. Savchynskyy, Partial Optimality by Pruning
for MAP-Inference with General Graphical Models, IEEE Trans. Patt. Anal. Mach. Intell. (2016), in press,
http://doi.ieeecomputersociety.org/10.1109/TPAMI.2015.2484327.
A. Singer, Y. Shkolnisky, and B. Nadler, Diffusion Interpretation of Non-Local Neighborhood Filters for Signal
Denoising, SIAM J. Imaging Sciences 2 (2009), no. 1, 118–139.
N.N. ˘Cencov, Statistical Decision Rules and Optimal Inference, Amer. Math.Soc., 1982.
J. Weickert, Anisotropic Diffusion in Image Processing, B.G. Teubner Verlag, 1998.
T. Werner, A Linear Programming Approach to Max-sum Problem: A Review, IEEE Trans. Patt. Anal. Mach. In-
tell. 29 (2007), no. 7, 1165–1179.
M.J. Wainwright and M.I. Jordan, Graphical Models, Exponential Families, and Variational Inference,
Found. Trends Mach. Learning 1 (2008), no. 1-2, 1–305.
J.S. Yedidia, W.T. Freeman, and Y. Weiss, Constructing free-energy approximations and generalized belief
propagation algorithms, IEEE Trans. Information Theory 51 (2005), no. 7, 2282–2312.

[ ˘C82]
[Wei98]
[Wer07]

[YFW05]

[SSN09]

[WJ08]

(F. ˚Astr¨om) HEIDELBERG COLLABORATORY FOR IMAGE PROCESSING, HEIDELBERG UNIVERSITY, GERMANY
E-mail address: freddie.astroem@iwr.uni-heidelberg.de
URL: http://hci.iwr.uni-heidelberg.de/Staff/fastroem/

(S. Petra) MATHEMATICAL IMAGE ANALYSIS GROUP, HEIDELBERG UNIVERSITY, GERMANY
E-mail address: petra@math.uni-heidelberg.de
URL: http://ipa.iwr.uni-heidelberg.de/dokuwiki/doku.php?id=people:spetra:start

(B. Schmitzer) CEREMADE, UNIVERSITY PARIS-DAUPHINE, FRANCE
E-mail address: schmitzer@ceremade.dauphine.fr
URL: https://www.ceremade.dauphine.fr/~schmitzer/
(C. Schn¨orr) IMAGE AND PATTERN ANALYSIS GROUP, HEIDELBERG UNIVERSITY, GERMANY
E-mail address, corresponding author: schnoerr@math.uni-heidelberg.de
URL: http://ipa.iwr.uni-heidelberg.de

