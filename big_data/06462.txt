Marginalized Bayesian ﬁltering with Gaussian

priors and posteriors

John-Olof Nilsson

1

Abstract—Marginalization techniques are presented for the
Bayesian ﬁltering problem under the assumption of Gaussian
priors and posteriors and a set of sequentially more constraining
state space model assumptions. The techniques provides the
capabilities to 1) reduce the ﬁltering problem to the essential
marginal moment integrals 2) combine model and numerical ap-
proximations to the moment integrals, and 3) decouple modelling
and system composition. Closed-form expressions of the posterior
means and covariances are developed as functions of subspace
projection matrices, subsystem models, and the marginal moment
integrals. Finally, we review how the marginalization techniques
connect to related (Kalman) ﬁltering techniques.

I. Introduction

Analytical solutions to the the Bayesian ﬁltering problem
are in general intractable and direct numerical approximations
such as particle ﬁlters are only applicable for low dimensional
problems. Beyond particle ﬁlters, marginalization, combined
with (conditionally) static linear and Gaussian state space
subspaces, can isolate the intractable part to the remainder
of the state space making particle ﬁlter techniques, so called
marginalized or Rao-Blackwellized particle ﬁlters, applicable
to higher dimensional systems. Unfortunately, for dynamic
subspace structures one is in general referred to ﬁltering with
the assumptions of Gaussian priors and posteriors, e.g. Kalman
ﬁltering. However, marginalization may still be highly valuable
and used to exploit structures in the models. Especially, in
this article we will present general marginalization techniques
which will give the following capabilities:

1) Focusing numerical approximations: The Gaussian prior
and posterior assumptions reduce the Bayesian ﬁltering to a
set of intractable moment integrals. Consequently, the related
ﬁltering techniques are essentially deﬁned by the diﬀerent
ways they approximate the moment integrals. However, with
subspace structures, only subdimensions of the integrals are
intractable and marginalization can reduce the integrals to
those intractable dimensions, thereby focusing the numerical
approximation techniques.

2) Combining model and numerical approximations: Tra-
ditionally, nonlinear non-Gaussian models are handled by
linearization and Gaussian approximations giving diﬀerent
ﬂavors of the Kalman ﬁlters. However, linearizing or making
a Gaussian approximation of a part of a model is just a way
of introducing related linear and Gaussian subspaces. Then,
marginalization can be used to eliminate such subspaces from
the moment integrals, hence enabling combination of model
and numerical approximations.

John-Olof Nilsson is with the Signal Processing Dept., ACCESS Linnaeus
Centre, KTH Royal Institute of Technology, Osquldas v¨ag 10, SE-100 44
Stockholm, Sweden. E-mail: jnil02@kth.se

3) Decoupling modelling and system composition: Filtering
problems often include multiple information sources and hence
multiple models. The diﬀerent output and state models are typ-
ically dependent on diﬀerent system states creating dynamic
subspace structures. However, the speciﬁc subspaces will be
dependent on the system composition creating an implicit
coupling between the modelling and the composition. In this
case, marginalization can be used to decouple the system
modelling and the system composition such that they can be
performed independently.

The core results of this article are closed form expressions of
the posterior means and covariances in terms of subspace pro-
jection matrices, subsystem models, and explicit marginal mo-
ment integrals, for a set of sequentially more constraining state
space model assumptions. The more constraining assumptions
(more structure) that can be made, the lower the dimensionality
of the resulting moment integrals. In the following subsection
we will pose the Bayesian ﬁltering problem and the state space
model assumptions. The expressions for the posterior means
and covariances are developed over Sections II-V. Following
this, in Sections VI-VIII we return to how they connect to
capabilities 1-3. Finally, in Section IX we will review how the
results connect to other ﬁltering techniques.

Note, the presented techniques share the Gaussian prior and
posterior assumptions with the Kalman ﬁlters but avoid the
jointly Gaussian assumption of the prior and the likelihood.
Consequently, they concern a more general setup and, in com-
bination with moment integral approximations, they constitute
a new class of Bayesian ﬁltering techniques.

Notation: Let A⊥⊤ = AA⊤ and A⊥+⊤ = A + A⊤. 0 and
I are zero and identity matrices of appropriate dimensions.
[· ·] and [· ; ·] are row and column vectors, respectively. p(a|b)
is the probability density function (pdf) of a given (where
applicable) b. Na(c, P) is the Gaussian pdf of a with mean c
and covariance P. Conditioning on information up to instant
k is denoted with ˆa and k − 1 with ˇa.

A. Problem formulation

Consider the general discrete state space system

xk = fk(xk−1, wk)
yk = hk(xk−1, vk) ⇒

p(xk|xk−1)
p(yk|xk)

where xk ∈ Rn and yk ∈ Rm and where wk ∼ p(wk) and
vk ∼ p(vk) are independent. In the latter implied (and suﬃ-
cient) density representation, p(xk|xk−1) and p(yk|xk) captures
both fk(·, ·) and hk(·, ·) and p(wk) and p(vk). For p(yk|xk) we
will not require quantiﬁed output but it may also encode any
side information yk. The problem is to recursively calculate the

6
1
0
2

 
r
a

 

M
1
2

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
2
6
4
6
0

.

3
0
6
1
:
v
i
X
r
a

2

conditional densities p(xk|Yk−1) and p(xk|Yk) where Yk = {yk}
is the set of all output up to instant k. This is the Bayesian
ﬁltering problem which in general cannot be solved analyti-
cally. However, with further assumptions, the intractability can
be limited to speciﬁc subspaces:

N) Gaussian priors and posteriors: While p(xk|xk−1) and
p(yk|xk) are arbitrary densities, the conditional state densities

p(xk|Yk−1) = Nxk (ˇxk, ˇPk) and p(xk|Yk) = Nxk(ˆxk, ˆPk)

i.e. the estimated distributions are Gaussian.

a) Dynamic active subspaces: There are matrices Ak ∈ Rαk ,n,
Bk ∈ Rn−αk ,n, Ck ∈ Rβk,n, and Dk ∈ Rn−βk,n, and corresponding
functions f a

k (·, ·) and ha
Skxk = h f a
yk = ha

k(·, ·), such that
k (Akxk−1, wk); Bkxk−1i
k(Ckxk, vk)

where Sk = [Ak; Bk] and Tk = [Ck; Dk] are invertible.

The structure of assumption a frequently arises due to diﬀerent
models being functions of diﬀerent state space subspaces
Akxk−1 and Ckxk. N and a are the basic assumptions here. Re-
sults for the ﬁltering problem will be presented for this general
case. However, further structure is often available, structure
which can greatly simplify the ﬁltering. Consequently, results
will also be presented for three further assumptions about the
active subspace:

b) Conditionally additive noise: There are functions f b
k(·), and matrices Gb
hb
Akxk = f b
yk = hb

k(Akxk) and Jk(Ckxk) such that
k (Akxk−1) + Gb
k(Ckxk) + Jb

k(Akxk−1)wk

k(Cxk)vk

k (·) and

where p(wk) has zero mean and covariance Pwk and Jb
is invertible.

k(Cxk)

k], Ck = [Cn

c) Conditionally linear and Gaussian subspace: There are
matrices Ak = [An
kxk),
Hc
k(Cn
k(·) such that
kxk−1) + Fc
kxk−1)wk
kxk) + Hc

k], Fc
k (·) and hc
k(An
k(Cnxk)vk

k; Al
kxk) and functions f c

k(Cn
Akxk = f c
yk = hc

kxk), and Jc
k (An
k(Cn

k(An
k(Cnxk)Cl

kxk−1 +Gc

kxk), Gc

kxk−1)Al

kxk + Jc

k(An

k; Cl

k(An

where wk ∼ Nwk (0, Pwk) and vk ∼ Nwv(0, Pvk), i.e. there is a
conditionally linear subspace with additive Gaussian noise in
the active subspace.

d) Dynamic active linear and Gaussian subspace: There are
matrices Fd
k such that

k and Jd

k, Gd

k, Hd
Akxk = fd
yk = hd

k, and vectors fd
kAkxk−1 + Gd
kCkxk + Jd
kvk

k + Fd
k + Hd

k and hd
kwk

where p(wk) has zero mean and covariance Pwk and vk ∼
Nwv(0, Pvk), i.e. the subspace state equation and the output
equation are linear and Gaussian.

Note that the assumptions about the state equation and the
output equation within the above assumptions are independent.

Consequently, they may be used separately. However, since
the analysis for the prediction and the update are similar,
they will be treated jointly. Further, note that p(wk) having a
ﬁnite covariance Pwk is strictly not required for the results of
assumption a. However, it is in general required for assumption
N to make sense. Similarly, wk ∼ Nwk (0, Pwk) is not strictly
required for the results for assumption d but it is implicit from
the linear state equation and assumption N.

II. Inactive subspace marginalization

Assumption N imply that only the means ˇxk and ˆxk, and
the covariances ˇPk and ˆPk need to be calculated, instead of
arbitrary densities p(xk|Yk−1) and p(xk|Yk). If, additionally,
assumption a is made, the desired means and covariances
can be expressed as functions of marginal moments in the
non-linear non-Gaussian subspace, as will be shown in the
following subsections. For brevity, hereinafter, time indices of
fk(·, ·), hk(·, ·), Sk and Tk and related quantities will be dropped.
Further, required indices of states, covariances, and outputs
will be indicated with ak = ˙a, ak−1 = ¨a.

A. Marginalization for prediction

Let U ∈ Rn,a and V ∈ Rn,n−a such that S−1 = [U V]. Further,
let M = B ˇ¨PA⊤(A ˇ¨PA⊤)−1. Assumption a imply p(S˙x|¨x) =
p(A˙x|A¨x)δ(B˙x − B¨x). This, and assumptions N and a, imply

Z p(S˙x, S¨x| ¨Y)d(B¨x)d(B˙x) = p(A˙x, A¨x| ¨Y)

Z p(S˙x, S¨x| ¨Y)d(A¨x)d(A˙x) = δ(B˙x − B¨x)p(B¨x| ¨Y)
Z B¨xp(S¨x| ¨Y)d(B ¨x) = (Bˇ¨x + M(A¨x − Aˇ¨x))p(A¨x| ¨Y).

Finally, introduce the marginal moments

Aˇ˙x = Z A˙xp(A˙x, A¨x| ¨Y)d(A¨x)d(A˙x)

ˇPA˙x = Z (A˙x − Aˇ˙x)⊥⊤ p(A˙x, A¨x| ¨Y)d(A¨x)d(A˙x)

ˇPA˙x,A¨x = Z (A˙x − Aˇ˙x)(A¨x − Aˇ¨x)⊤ p(A˙x, A¨x| ¨Y)d(A¨x)d(A˙x).

Together this gives the predicted mean

ˇ˙x = Z ˙xp(˙x|¨x)p(¨x| ¨Y)d¨xd˙x
= S−1Z S˙xp(S˙x|¨x)p(S¨x| ¨Y)d(S¨x)d(S˙x)
= S−1Z [A˙x; B˙x]p(A˙x|A¨x)δ(B˙x − B¨x)p(S¨x| ¨Y)d(S˙x)d(S¨x)
= UZ A˙xp(A˙x|A¨x)p(A¨x| ¨Y)d(A˙x)d(A¨x) + VBˇ¨x
= UAˇ˙x + VBˇ¨x.

Further, note that Bˇ˙x = BUAˇ˙x + BVBˇ¨x = Bˇ¨x. Together this
gives the predicted covariance

ˇ˙P = Z (˙x − ˇ˙x)⊥⊤ p(˙x, ¨x| ¨Y)d¨xd˙x
= Z S−1S(˙x − ˇ˙x)⊥⊤S⊤S−⊤ p(S˙x, S¨x| ¨Y)d(S¨x)d(S˙x)
= Z (cid:16)U(A˙x − Aˇ˙x)⊥⊤U⊤ + V(B˙x − Bˇ˙x)⊤⊥V⊤

+ (UA(˙x − ˇ˙x)⊥⊤B⊤V⊤)⊥+⊤(cid:17)p(S˙x, S¨x| ¨Y)d(S¨x)d(S˙x)

= Z U(A˙x − Aˇ˙x)⊥⊤U⊤ p(A˙x, A¨x| ¨Y)d(A¨x)d(A˙x)
+Z V(B˙x − Bˇ˙x)⊥⊤V⊤δ(B˙x − B¨x)p(B¨x| ¨Y)d(B¨x)d(B˙x)
+Z (cid:16)U(A˙x − Aˇ˙x)(B˙x − Bˇ˙x)⊤V⊤(cid:17)⊥+⊤
= U ˇPA˙xU⊤ + VB ˇ¨PB⊤V⊤
+Z (cid:16)U(A˙x − Aˇ˙x)(A¨x − Aˇ¨x)⊤M⊤V⊤(cid:17)⊥+⊤
= U ˇPA˙xU⊤ + VB ˇ¨PB⊤V⊤ + (U ˇPA˙x,A¨xM⊤V⊤)⊥+⊤.

p(A˙x|A¨x)δ(B˙x − B¨x)p(S¨x| ¨Y)d(S¨x)d(S˙x)

p(A˙x|A¨x)p(A¨x| ¨Y)d(A¨x)d(A˙x)

Consequently, with assumptions N and a, obtaining p(˙x| ¨Y)
from p(¨x| ¨Y), i.e. the prediction, boils down to calculating the
marginal moments Aˇ˙x, ˇPA˙x, and ˇPA˙x,A¨x.

The density p(A˙x, A¨x| ¨Y) is normally not known. However,

from the deﬁnition of conditional probability

p(A˙x, A¨x| ¨Y) = p(A˙x|A¨x, ¨Y)p(A¨x| ¨Y).

p(A˙x|A¨x, ¨Y) can be obtained from the state equation via

p(A˙x|A¨x) = Z δ(A˙x − f a(A¨x, ˙w))p( ˙w)d ˙w.

Inserting into the expressions for the marginal moments and
integrating out A˙x give

Aˇ˙x = Z f a(A¨x, ˙w)p( ˙w, A¨x| ¨Y)d([A¨x; ˙w])

ˇPA˙x = Z ( f a(A¨x, ˙w) − Aˇ˙x)⊥⊤ p( ˙w, A¨x| ¨Y)d([A¨x; ˙w])

ˇPA˙x,A¨x =Z ( f a(A¨x, ˙w) −Aˇ˙x)(A¨x−Aˇ¨x)⊤p( ˙w, A¨x| ¨Y)d([A¨x; ˙w]).
for uniformity, we have written p( ˙w)p(A¨x| ¨Y) =
where,
p( ˙w, A¨x| ¨Y). Consequently, the integration over A˙x is replaced
with the integration over ˙w and the density p(A˙x, A¨x| ¨Y) is
replaced with the known density p( ˙w, A¨x| ¨Y).

B. Marginalization for update

Let W ∈ Rn,b and Z ∈ Rn,n−b such that T−1 = [W Z].
let N = D ˇ˙PC⊤(C ˇ˙PC⊤)−1. Note that p(T˙x| ˙Y) =
Further,
p(D˙x|C˙x, ˙Y)p(C˙x| ˙Y). Assumption a and N, respectively, imply
p(D˙x|C˙x, ˙Y) = p(D˙x|C˙x, ¨Y) = ND˙x(cid:0)Dˇ˙x + N(C˙x − Cˇ˙x), D ˇ˙PD⊤ −

3

NC ˇ˙PD⊤(cid:1). Finally, introduce the marginal moments

Cˆ˙x = Z C˙xp(C˙x| ˙Y)d(C˙x)

ˆPC˙x = Z (C˙x − Cˆ˙x)⊥⊤ p(C˙x| ˙Y)d(C˙x).

Together this gives the updated mean

ˆ˙x = Z ˙xp(˙x| ˙Y)d ˙x
= T−1Z T˙xp(T˙x| ˙Y)d(T˙x)
= WZ C˙xp(C˙x| ˙Y)d(C˙x) + ZZ D˙xp(D˙x|C˙x)p(C˙x| ˙Y)d(T˙x)
= WCˆ˙x + ZZ (cid:16)Dˇ˙x + N(C˙x − Cˇ˙x)(cid:17) p(C˙x| ˙Y)d(C˙x)
= WCˆ˙x + Z(cid:0)Dˇ˙x + N(Cˆ˙x − Cˇ˙x)(cid:1).
Further, note that Dˆ˙x = DWCˆ˙x + DZ(cid:0)Dˇ˙x + N(Cˆ˙x − Cˇ˙x)(cid:1) =
Dˇ˙x + N(Cˆ˙x − Cˇ˙x). Together this gives the updated covariance
ˆ˙P = Z (˙x − ˆ˙x)⊥⊤ p(˙x| ˙Y)d˙x
= Z T−1T(˙x − ˆ˙x)⊥⊤T⊤T−⊤ p(T˙x| ˙Y)d(T˙x)
= Z (cid:16)W(C˙x − Cˆ˙x)⊥⊤W⊤ + Z(D˙x − Dˆ˙x)⊤⊥V⊤

+ (WC(˙x − ˆ˙x)⊥⊤D⊤Z⊤)⊥+⊤(cid:17)p(T˙x| ˙Y)d(T˙x)

= W ˆPC˙xW⊤

= Z W(C˙x − Cˆ˙x)⊥⊤W⊤ p(C˙x| ˙Y)d(C˙x)
+Z  Z(D˙x−Dˆ˙x)⊥⊤Z⊤+(cid:16)W(C˙x−Cˆ˙x)(D˙x−Dˆ˙x)⊤Z⊤(cid:17)⊥+⊤!
ND˙x(cid:0)Dˇ˙x+N(C˙x−Cˇ˙x), D ˇ˙PD⊤−NC ˇ˙PD⊤(cid:1)p(C˙x| ˙Y)d(C˙x)d(D˙x)
+Z Z(cid:16)D ˇ˙PD⊤−NC ˇ˙PD⊤ + N(C˙x − Cˆ˙x)⊥⊤N⊤(cid:17)Z⊤p(C˙x| ˙Y)d(C˙x)
+Z (cid:16)W(C˙x−Cˆ˙x)(Cˇ˙x+N(C˙x−Cˇ˙x)−Cˆ˙x)⊤Z⊤(cid:17)⊥+⊤
p(C˙x| ˙Y)d(C˙x)
=W ˆPC˙xW⊤+Z(D ˇ˙PD⊤−NC ˇ˙PD⊤+N ˆPC˙xN⊤)Z⊤+(W ˆPC˙xN⊤Z⊤)⊥+⊤.
Consequently, with assumptions N and a, obtaining p(˙x| ˙Y)
from p(˙x| ¨Y), i.e. the update, boils down to calculating the
marginal moments Cˆ˙x and ˆPC˙x.

The density p(C˙x| ˙Y) is not in general known. However, from
Bayes’ theorem and the Markovian assumption of the state
space model

p(C˙x| ˙Y) = νp(˙y|C˙x)p(C˙x| ¨Y)

where ν is a normalization constant. p(C˙x| ¨Y) is the known
prior and p(˙y|C˙x) is the likelihood function. If the likelihood
function is known, up to scale, then the the marginal moments
can directly be expressed as

Cˆ˙x = νZ C˙xp(˙y|C˙x)p(C˙x| ¨Y)d(C˙x)

ˆPC˙x = νZ (C˙x − Cˆ˙x)⊥⊤ p(˙y|C˙x)p(C˙x| ¨Y)d(C˙x).

4

If the likelihood function is not known, it can be obtained
from the output model ha(·, ˙v) and the density p(˙v) via

p(˙y|C˙x) = Z δ(˙y − ha(C˙x, ˙v))p(˙v)d˙v.

Inserting into the marginal moment integrals yields

Cˆ˙x = νZ C˙xδ(˙y − ha(C˙x, ˙v))p(˙v)p(C˙x| ¨Y)d(C˙x)d˙v

ˆPC˙x = νZ (C˙x − Cˆ˙x)⊥⊤δ(˙y − ha(C˙x, ˙v))p(˙v)p(C˙x| ¨Y)d(C˙x)d˙v.

Unfortunately, this increases the dimensionality of the integrals
and adds the Dirac factor which complicates the evaluation.
However, as we will be shown in Section VI, the moments are
still evaluable with standard statistical methods.

III. Exploiting conditionally additive noise structure

If, in addition to the assumptions N and a, assumption b
is made, the calculation of the marginal moments Aˇ˙x, ˇPA˙x,A¨x,
ˇPA˙x, Cˆ˙x, and ˆPC˙x can be facilitated by marginalizing out A˙x
(without introducing ˙w) and an explicit likelihood function.
For brevity, hereinafter let Gb = Gb(A¨x) and Jb = Jb(A¨x).

A. Current state marginalization

From the deﬁnition of conditional probability p(A˙x, A¨x| ¨Y) =
p(A˙x|A¨x, ¨Y)p(A¨x| ¨Y). Now, given A¨x, A˙x is an aﬃne transfor-
mation of ˙w and consequently

Aˇ˙x = Z A˙xp(A˙x|A¨x, ¨Y)p(A¨x| ¨Y)d(A¨x)d(A˙x)

= Z f b(A¨x)p(A¨x| ¨Y)d(A¨x)

ˇPA˙x = Z (A˙x − Aˇ˙x)⊥⊤ p(A˙x|A¨x, ¨Y)p(A¨x| ¨Y)d(A¨x)d(A˙x)
+ ( f b(A¨x) − Aˇ˙x)⊥⊤(cid:1)p(A¨x| ¨Y)d(A¨x)
ˇPA˙x,A¨x = Z (A˙x − Aˇ˙x)(A¨x − Aˇ¨x)⊤p(A˙x|A¨x, ¨Y)p(A¨x| ¨Y)d(A˙x)

= Z (cid:0)GbP ˙wGb⊤

= Z ( f b(A¨x) − Aˇ˙x)(A¨x − Aˇ¨x)⊤ p(A¨x| ¨Y)d(A¨x).

In contrast to the general case, p( ˙w) can be integrated out and
therefore, the dimensionality of the integrals is reduced.

B. Explicit likelihood function

Again, from Bayes’ theorem and the Markovian assumption
of the state space model, p(C˙x| ˙Y) = νp(˙y|C˙x)p(C˙x| ¨Y) where
ν is a normalization constant. Given C˙x,
˙y is an aﬃne
transformation of ˙v. Consequently, the likelihood function

p(˙y|C˙x) = |Jb|−1 p˙v(cid:0)(Jb)−1(˙y − hb(C˙x))(cid:1)

where the subscript p˙v(·) has been used to indicate that this is
the pdf of ˙v and where | · | denotes the determinant. Therefore

Cˆ˙x = νZ C˙x|Jb|−1 p˙v(cid:0)(Jb)−1(˙y − hb(C˙x))(cid:1)p(C˙x| ¨Y)d(C˙x)
ˆPC˙x = νZ (C˙x−Cˆ˙x)⊥⊤|Jb|−1 p˙v(cid:0)(Jb)−1(˙y−hb(C˙x))(cid:1)p(C˙x| ¨Y)d(C˙x).

In contrast to the general case, p(˙y|C˙x) is always known and
the noise ˙v does not add to the dimensionality of the integrals.

IV. Linear and Gaussian subspace marginalization

If, in addition to the assumptions N and a, assumption c
is made, the calculation of the marginal moments Aˇ˙x, ˇPA˙x,A¨x,
ˇPA˙x, Cˆ˙x, and ˆPC˙x can be facilitated by further marginalization
of the linear subspaces. For brevity, hereinafter let Fc =
Fc(An ¨x), Gc = Gc(An ¨x), Hc = Hc(Cn ˙x), and Jc = Jc(An ¨x).

A. Marginalization for prediction

Let Fc = [Fn; Fl], Gc = [Gn; Gl], and f c(·) = [ f n(·); f l(·)]
such that the divisions are consistent with the dimensions of
A = [An; Al]. From the deﬁnition of conditional probability
p(A˙x, A¨x| ¨Y) = p(Al ˙x, Al ¨x|An ˙x, An ¨x, ¨Y)p(An ˙x, An ¨x| ¨Y)

= p(Al ˙x, Al ¨x|An ˙x, An ¨x, ¨Y)p(An ˙x|An ¨x, ¨Y)p(An ¨x| ¨Y).

The two ﬁrst distributions can be derived in closed form.
Assumption N imply that

p(Al ¨x|An ¨x, ¨Y) = NAl ¨x(φ(An ¨x), Φ)

where

φ(An ¨x) = Al ˇ¨x + Al ˇ¨PAn⊤(An ˇ¨PAn⊤)−1(An ¨x − An ˇ¨x)

Φ = Al ˇ¨PAl⊤

− Al ˇ¨PAn⊤(An ˇ¨PAn⊤)−1An ˇ¨PAl⊤

.

Together with assumptions N and c, this implies
p(An ˙x|An ¨x, ¨Y) = NAn ˙x(π(An ¨x), Π(An ¨x))

where

π(An ¨x) = f n(An ¨x) + Fnφ(An ¨x)
Π(An ¨x) = FnΦFn⊤ + GnP ˙wGn⊤.

Further, the relations between Al ˙x, Al ¨x, An ˙x, and An ¨x can be
expressed as follows

0

I

"Al ˙x
Al ¨x# = "Fl
0#"Al ¨x
An ˙x = h0 Fni"Al ˙x

Al...x# +" f l(An ¨x)
# +"Gl
0 # ˙w
Al ¨x# + f n(An ¨x) + Gn ˙w.

0

Given An ˙x and An ¨x, the former relation can be interpreted
as a state equation and the latter as an output equation of
a linear Gaussian system. p(Al ¨x|An ¨x, ¨Y) provides a Gaussian
prior (note that Al...x is merely a placeholder since it does not
eﬀect the prediction) and therefore p(Al ˙x, Al ¨x|An ˙x, An ¨x, ¨Y) is
given be the Kalman ﬁlter recursion, giving
p(Al ˙x, Al ¨x|An ˙x, An ¨x, ¨Y) =

NAl ˙x,Al ¨x " f l(An ¨x) + Flφ(An ¨x) + Ξln(Ξn)−1(An ˙x − π(An ¨x))
# ,
" Ξl − Ξln(Ξn)−1Ξln⊤
FlΦ − Ξln(Ξn)−1FnΦ
(FlΦ − Ξln(Ξn)−1FnΦ)⊤ Φ − ΦFn⊤(Ξn)−1FnΦ#!

φ(An ¨x) + ΦFn⊤(Ξn)−1(An ˙x − π(An ¨x))

where

Ξl = FlΦFl⊤
+ GlP ˙wGl⊤
Ξln = FlΦFn⊤ + GlP ˙wGn⊤
Ξn = FnΦFn⊤ + GnP ˙wGn⊤.

Consequently, Al ˙x and Al ¨x and then An ˙x can be marginalized
out of the marginal moments Aˇ˙x, ˇPA˙x, and ˇPA˙x,A¨x. Introduce

α(An ¨x) = f l(An ¨x) + Flφ(An ¨x) − Al ˇ˙x
β(An ¨x) = φ(An ¨x) − Al ˇ¨x
γ(An ˙x) = π(An ¨x) − An ˇ˙x
δ(An ¨x) = An ¨x − An ˇ¨x
Pα(An ¨x) = Ξl − Ξln(Ξn)−1Ξln⊤
Pα,β(An ¨x) = FlΦ − Ξln(Ξn)−1FnΦ
ǫ(An ¨x) = f l(An ¨x) + Flφ(An ¨x).

Then, ﬁnally, straight-forward calculations give

Aˇ˙x = Z "π

ǫ# p(An ¨x| ¨Y)d(An ¨x)

γα⊤ + ΠΞ⊤

γβ⊤ + ΠΓ⊤

αγ⊤ + ΞΠ Pα + αα⊤ + ΞΠΞ⊤# p(An ¨x| ¨Y)d(An ¨x)
ˇPA˙x = Z " Π + γγ⊤
ˇPA˙x,A¨x = Z "γδ⊤
αδ⊤ Pα,β + αβ⊤ + ΞΠΓ⊤# p(An ¨x| ¨Y)d(An ¨x)
where Ξ = Ξln(Ξn)−1 and Γ = ΦFn⊤(Ξn)−1 and where, for
brevity, explicit dependencies on An ¨x have been dropped.
Consequently, the dimensionality of the moment integrals has
been further reduced by the dimensionality of Al ¨x.

B. Marginalization for update

From the deﬁnition of conditional probability
p(C˙x| ˙Y) = p(Cl ˙x|Cn ˙x, ˙Y)p(Cn ˙x| ˙Y).

Assumption c imply that given Cn ˙x, ˙y is linear in Cl ˙x with
additive Gaussian noise. Let

ψ(Cn ˙x) = Cl ˇ˙x + Cl ˇ˙PCn⊤(Cn ˇ˙PCn⊤)−1(Cn ˙x − Cn ˇ˙x)

Ψ = Cl ˇ˙PCl⊤

− Cl ˇ˙PCn⊤(Cn ˇ˙PCn⊤)−1Cn ˇ˙PCl⊤

.

Then assumption N imply p(Cl ˙x|Cn ˙x, ¨Y) = NCl ˙x (ψ(Cn ˙x), Ψ).
Therefore, the conditioning on ˙y = ˙Y \ ¨Y in p(Cl ˙x|Cn ˙x, ˙Y) is
given by a Kalman ﬁlter update. Introduce

ς(Cn ˙x) = hc(Cn ˙x) + Hcψ(Cn ˙x)
Σ(Cn ˙x) = HcΨHc⊤ + JcP˙vJc⊤

ω(Cn ˙x) = ψ(Cn ˙x) + ΨHc⊤Σ−1(Cn ˙x)(cid:0)˙y − ς(Cn ˙x)(cid:1)

κ(Cn ˙x) = ω(Cn ˙x) − Cl ˆ˙x
κ(Cn ˙x) = Cn ˙x − Cn ˆ˙x
Ω(Cn ˙x) = (I − ΨHc⊤Σ−1(Cn ˙x)Hc)Ψ.

This gives

p(Cl ˙x|Cn ˙x, ˙Y) = NCl ˙x (ω(Cn ˙x), Ω(Cn ˙x)) .

Consequently, Cl ˙x can be marginalized out of the marginal
moments Cˆ˙x and ˆPC˙x. Again from Bayes’ theorem p(Cn ˙x| ˙Y) =
νp(˙y|Cn ˙x, ¨Y)p(Cn ˙x| ¨Y), where ν is a normalization constant,
and the likelihood function p(˙y|Cn ˙x, ¨Y) = N˙y(ς(Cn ˙x), Σ(Cn ˙x)).
Together, this gives the marginal moments

Cˆ˙x = νZ "Cn ˙x

ω # N˙y(ς, Σ)p(Cn ˙x| ¨Y)d(Cn ˙x)

5

ˆPC˙x = νZ "κκ

κκ

⊤

κκ⊤

⊤ Ω + κκ⊤# N˙y(ς, Σ)p(Cn ˙x| ¨Y)d(Cn ˙x)

where, for brevity, explicit dependencies on Cn ˙x have been
dropped. Consequently, the dimensionality of the moment
integrals has been further reduced by that of Cl ˙x.

V. Closed form solutions for the linear case

If, in addition to N and a, assumption d is made, closed
form solutions for the marginal moments can be derived.
Substituting the formulas below back into the expressions for
the moments gives the Kalman ﬁlter predictions and updates.

A. Closed form solutions for prediction

Substituting fd + FdA¨x + Gd ˙w for f a(A¨x, ˙w) in the formulas
for the marginal moments given assumptions N and a, gives

Aˇ˙x = Z (f + FdA¨x + Gd ˙w)p( ˙w, A¨x| ¨Y)d([A¨x; ˙w])

= fd + FdAˇ¨x

ˇPA˙x = Z (f + FdA¨x + Gd ˙w − Aˇ˙x)⊥⊤ p( ˙w, A¨x| ¨Y)d([A¨x; ˙w])

= FdA ˇ¨PA⊤Fd⊤

+ GdP ˙wGd⊤

ˇPA˙x,A¨x =Z (f+FdA¨x+Gd ˙w−Aˇ˙x)(A¨x−Aˇ¨x)⊤p( ˙w,A¨x| ¨Y)d([A¨x; ˙w])

= FdA ˇ¨PA⊤.

B. Closed form solutions for update

With linear and Gaussian output, the marginal moments are

given by the Kalman ﬁlter update formulas
Cˆ˙x = Cˇ˙x+C ˇ˙PC⊤Hd⊤(HdC ˇ˙PC⊤Hd ⊤
ˆPC˙x = (I − C ˇ˙PC⊤Hd⊤(HdC ˇ˙PC⊤Hd ⊤

+JdP˙vJd ⊤)−1(˙y−hd−HdCˇ˙x)
+JdP˙vJd ⊤)−1Hd)C ˇ˙PC⊤.

VI. Numerical moment integral approximations

With assumption N, the Bayesian ﬁltering is reduced to

approximating the moments {ˇ˙x, ˇ˙P, ˆ˙x, ˆ˙P}. With assumption a
ˇ˙x = UAˇ˙x + VBˇ¨x
ˇ˙P = U ˇPA˙xU⊤ + VB ˇ¨PB⊤V⊤ + (U ˇPA˙x,A¨xM⊤V⊤)⊥+⊤
ˆ˙x = WCˆ˙x + Z(cid:0)Dˇ˙x + N(Cˆ˙x − Cˇ˙x)(cid:1)
ˆ˙P =W ˆPC˙xW⊤+Z(D ˇ˙PD⊤−NC ˇ˙PD⊤+N ˆPC˙xN⊤)Z⊤+(W ˆPC˙xN⊤Z⊤)⊥+⊤
and the approximations can be focused to the marginal mo-
ments {Aˇ˙x, ˇPA˙x, ˇPA˙x,A¨x, Cˆ˙x, ˆPC˙x}. Assumptions b and c enables
us to further narrow the focus, together giving capability 1. All
non-closed-form formulas derived for the marginal moments
are on the form

m = Z d(·)p(·| ¨Y)d(·)

where d(·) is a function of the subspace indicated by the
marginal density p(·| ¨Y) and d(·) is a diﬀerential of the cor-
responding subspace. Given assumptions N and a
p(·| ¨Y) = p( ˙w, A¨x| ¨Y) = p( ˙w)NAx(Aˇ¨x, A ˇ¨PA⊤)

d( ˙w, A¨x) = f a(A¨x, ˙w)
d( ˙w, A¨x) = ( f a(A¨x, ˙w) − Aˇ˙x)⊥⊤
d( ˙w, A¨x) = ( f a(A¨x, ˙w) − Aˇ˙x)(A¨x − Aˇ¨x)⊤ ⇒ m = ˇPA˙x,A¨x.

and
⇒ m = Aˇ˙x,
⇒ m = ˇPA˙x,

6

and additionally given p(˙y|C˙x)

p(·| ¨Y) = NC˙x(Cˇ˙x, C ˇ˙PC⊤)

and
⇒ m = Cˆ˙x,
d(C˙x) = νC˙xp(˙y|C˙x)
d(C˙x) = ν(C˙x−Cˆ˙x)⊥⊤ p(˙y|C˙x) ⇒ m = ˆPC˙x,

otherwise (if p(˙y|C˙x) is not available)

Substituting this back into d(·) enables use of numerical
integration as described above. Again, the suitable v(i, j) and
KZ(·)(·) depends on ha(·, ·) and p(˙v) and the method of choice.
An alternative to the density estimation, for assumptions N
and a, is a parametric Kalman update. Extending assumption
N to a jointly Gaussian assumption

˙w

˙v,

∂ f a

∂ha

∂ ˙w(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(A¨x,0)
∂˙v (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(C˙x,0)

.

∂ha

∂˙v (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(C˙x,0)

⇒ m = Cˆ˙x,
d(C˙x) = νC˙x(cid:16)R δ(˙y − ha(C˙x, ˙v))p(˙v)d˙v(cid:17)
d(C˙x) = ν(C˙x−Cˆ˙x)⊥⊤(cid:16)R δ(˙y−ha(C˙x,˙v))p(˙v)d˙v(cid:17) ⇒ m = ˆPC˙x.

Given assumptions N, a, and b

implies

p(·| ¨Y) = NAx(Aˇ¨x, A ˇ¨PA⊤)

and

p(C˙x, ˙y| ¨Y) = NC˙x,˙y " ˇC˙x

ˇ˙y # ," ˇPC˙x

ˇP⊤
C˙x,˙y

ˇPC˙x,˙y
ˇP˙y #!

Cˆ˙x = Cˇ˙x + ˇPC˙x,˙y( ˇP˙y)−1(cid:0)˙y − ˇ˙y(cid:1)
ˆ˙PC˙x = ˇ˙PC˙x − ˇPC˙x,˙y( ˇP˙y)−1 ˇP⊤

C˙x,˙y

d(A¨x) = f b(A¨x)
d(A¨x) = GbP ˙wGb⊤
d(A¨x) = ( f b(A¨x) − Aˇ˙x)(A¨x − Aˇ¨x)⊤

⇒ m = Aˇ˙x,
+ ( f b(A¨x) − Aˇ˙x)⊥⊤ ⇒ m = ˇPA˙x,

⇒ m = ˇPA˙x,A¨x

and

p(·| ¨Y) = NC˙x(Cˇ˙x, C ˇ˙PC⊤)
d(C˙x) = νC˙x|Jb|−1 p˙v(cid:0)(Jb)−1(˙y − hb(C˙x))(cid:1)
d(C˙x) = ν(C˙x−Cˆ˙x)⊥⊤|Jb|−1 p˙v(cid:0)(Jb)−1(˙y−hb(C˙x))(cid:1) ⇒m = ˆPC˙x.

Finally, given assumptions N, a, and c

⇒m = Cˆ˙x,

and

p(·| ¨Y) = NAnx(An ˇ¨x, An ˇ¨PAn⊤)

and

d(A¨x) = [π; ǫ]

⇒ m = Aˇ˙x,
αγ⊤ + ΞΠ Pα + αα⊤ + ΞΠΞ⊤# ⇒ m = ˇPA˙x,
αδ⊤ Pα,β + αβ⊤ + ΞΠΓ⊤#
⇒ m = ˇPA˙x,A¨x

d(A¨x) = " Π + γγ⊤
d(A¨x) = "γδ⊤

γα⊤ + ΠΞ⊤

γβ⊤ + ΠΓ⊤

and

p(·| ¨Y) = NCn ˙x(Cn ˇ˙x, Cn ˇ˙PCn⊤)

d(C˙x) = ν(cid:2)Cn ˙x; ω(cid:3) N˙y(ς, Σ)
d(C˙x) = ν"κκ

and
⇒ m = Cˆ˙x,
⊤ Ω + κκ⊤# N˙y(ς, Σ) ⇒ m = ˆPC˙x.

κκ⊤

κκ

⊤

which are the well-known Kalman ﬁlter update formulas.
Consequently, the problem is changed to that of approximating

ˇ˙y = Z ha(C˙x, ˙v)p(˙v)p(C˙x| ¨Y)d(C˙x)d˙v

ˇP˙y = Z (ˇ˙y − ha(C˙x, ˙v))⊥⊤ p(˙v)p(C˙x| ¨Y)d(C˙x)d˙v

ˇPC˙x,˙y = Z (C˙x − Cˇ˙x)(ˇ˙y − ha(C˙x, ˙v))p(˙v)p(C˙x| ¨Y)d(C˙x)d˙v,

which are amenable to numerical integration as described
above. Hence, the density estimation is avoided at the cost
of the extended Gaussian assumption, giving a Kalman ﬁlter.

VII. Introducing linearization and Gaussian approximations

To focus the computational eﬀort on the signiﬁcantly non-
linear, non-additive, and non-Gaussian model parts one may
exploit linearization and Gaussian approximation of benign
parts of the models. For simplicity, assume zero mean noise
(and hence a zero noise linearization point) and restate as-
sumptions b-c:

b’) Conditionally additive noise: Assumption b may alterna-
tively be expressed as

Suitable numerical integration methods for m comes in a
range of diﬀerent ﬂavors such as Gauss-Hermit quadrature,
importance sampling and Monte Carlo methods. All methods
may be viewed as a set of samples r(i) and weights w(i) giving

f a(A¨x, ˙w) = f a(A¨x, 0) +

ha(C˙x, ˙v) = ha(C˙x, 0) +

m ≈ (cid:18)Xi

−1

w(i)(cid:19)

Xi

d(r(i))w(i).

giving

The suitable r(i) depends on d(·) and the method of choice.

Given only assumptions N and a, the above formula cannot
be directly applied to the update since the integral within d(·)
cannot typically be evaluated analytically and in general we
have to resort to density estimation. With a kernel KZ(·)(·), a
bandwidth matrix Z(·), samples v(i, j), and weights v(i, j),

f b(A¨x) = f a(A¨x, 0), hb(C˙x) = ha(C˙x, 0)

Gb =

and Jb =

∂ f a

∂ ˙w(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(A¨x,0)

c’) Conditionally linear and Gaussian subspace: Assumption
c may alternatively be expressed as

Z δ(˙y − ha(r(i), ˙v))p(˙v)d˙v
v(i, j)(cid:19)

≈ (cid:18)Xj

−1

Xj

KZ(r(i))(cid:0)˙y − ha(s(i), v(i, j))(cid:1)v(i, j).

f a(A¨x, ˙w) =f a(cid:16)hAn ¨x
ha(C˙x,˙v) = ha(cid:16)hCn ˙x

Al ¯¨xi,0(cid:17)+
Cl ¯˙xi,0(cid:17)+

(Al ¨x−Al ¯¨x)+

(Cl ˙x−Cl ¯˙x)+

∂ f a

∂ha

∂Al ¨x(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:18)(cid:20)An ¨x
∂Cl ˙x(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:18)(cid:20)Cn ˙x

Al ¯¨x(cid:21),0(cid:19)

Cl ¯˙x(cid:21),0(cid:19)

∂ f a

˙w′

∂ha

∂ ˙w(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:18)(cid:20)An ¨x
∂˙v (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:18)(cid:20)Cn ˙x

Al ¯¨x(cid:21),0(cid:19)
˙v′,
Cl ¯˙x(cid:21),0(cid:19)

where ˙v′ ∼ N˙v′(0, P˙v) and ˙w′ ∼ N ˙w′(0, P ˙w) and Al ¯¨x and Cl ¯˙x
are linearizations points, giving
∂ f a

∂ f a

∂ha

Fc =

Jc =

Al ¯¨x(cid:21),0(cid:19)

∂ha

∂Al ¨x(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:18)(cid:20)An ¨x
∂˙v (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:18)(cid:20)Cn ˙x

Cl ¯˙x(cid:21),0(cid:19)

, Gc =

,

and

,

, Hc =

Al ¯¨x(cid:21),0(cid:19)

∂ ˙w(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:18)(cid:20)An ¨x
f c(An ¨x) = f a(cid:16)hAn ¨x
hc(Cn ˙x) = ha(cid:16)hCn ˙x

∂Cl ˙x(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:18)(cid:20)Cn ˙x
Cl ¯˙x(cid:21),0(cid:19)
Al ¯¨xi,0(cid:17) − FcAl ¯¨x
Cl ¯˙xi,0(cid:17) − HcCl ¯˙x.

d’) Linear and Gaussian subspace: Assumption c may alter-
natively be expressed as

f a(A¨x, ˙w) = f a(A¯¨x, 0) +

ha(C˙x, ˙v) = ha(C¯˙x, 0) +

(A¨x − A¯¨x) +

(C˙x − C¯˙x) +

∂ f a

∂ha

∂A¨x(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(A¯¨x,0)
∂C˙x(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(C¯˙x,0)

˙w

˙v′,

∂ f a

∂ha

∂ ˙w(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(A¯¨x,0)
∂˙v (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(C¯˙x,0)

where ˙v′ ∼ N˙v′(0, P˙v) and A¯¨x and C¯˙x are linearization points,
giving

Fd =

∂ f a

∂A¨x(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(A¯¨x,0)

, Gd =

, Hd =

, Jd =

∂ f a

∂ ˙w(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(A¯¨x,0)

∂ha

∂C˙x(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(C¯˙x,0)

∂ha

∂˙v (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(C¯˙x,0)

fd = f a(A¯¨x, 0) − FdA¯¨x,

and hd = ha(C¯˙x, 0) − HdC¯˙x.

In principle, the assumptions b/b′, c/c′, and d/d′ are equiv-
alent. In practice, the assumption versions b’, c’ and d’ can
be used to introduce model approximations by neglecting the
ﬁrst pair of relations in each assumption. This will trade model
approximations for lower dimensional moment integrals, and
gives capability 2.

The linearization points Al ¯¨x, Cl ¯˙x, A¯¨x, and C¯˙x are arbitrary
but in practice they will normally be the marginal prior means
Al ˇ¨x, Cl ˇ˙x, Aˇ¨x, and Cˇ˙x.

VIII. Decoupling modelling and system composition

A system model is composed of submodels. At least, as of
the state space description, it is composed of a state model and
an output model, but often there will be multiple submodels
of diﬀerent states and outputs. Assume submodels

˙xi = f i(¨xi, ˙wi),

˙y j = h j(˙x j, ˙v j) :

i, j ∈ {1, 2, . . . }

where xi and x j are states of the properties determining the
subsystem dynamics and the sybsystem outputs, respectively.
The system state x span the properties of {xi, x j}. The prop-
erties of {xi, x j} must overlap but
individual model states
will often not coincide with x and diﬀerent models may use
diﬀerent ordering and units of the states.

The discrepancies between x and {xi, x j} mean that the
submodels cannot directly be compounded. Naively, this is
solved by modifying the models or using wrapper models
such that they are expressed in x. However, this creates an
unfortunate coupling between modelling and system compo-
sition, a maintenance nightmare, and prevents exploitation of
subsystem structure (capability 1 and 2) since all structure is
hidden. Instead, using assumption a, the composition can be
performed by constructing A:s and C:s such that

7

The A:s and C:s will include scaling (unit transformations)
and state selection but may also include further linear trans-
formations [1]. This eﬀectively decouples the modelling and
the system composition giving capability 3.

IX. Connections to other filtering techniques

In the following subsection we will describe how the pre-
sented results relate to the Kalman ﬁlters and the marginalized
particle ﬁlters, and a few other directly related techniques.

A. Kalman ﬁlters

The Kalman ﬁlters handles a setup with assumption N and

the additional jointly Gaussian assumption

"ˇ˙x
p(˙x, ˙y| ¨Y) = N˙x,˙y
ˇ˙y# ,

ˇ˙P
ˇP⊤
˙x,˙y

.

ˇP˙x,˙y

ˇP˙y 


The (Kalman ﬁlter) update rules for this case, yet in the active
subspace, were presented in Section VI. Hence, ignoring the
marginalization of the inactive subspace, the nonlinear Kalman
ﬁlters may be viewed as parametric methods for ﬁltering given
assumption N, and in many cases additional assumptions
such as additive noise. See e.g. [2,3,4,5] in which diﬀerent
numerical approximation techniques for the moment integrals,
together with additional model assumptions, are presented,
giving the so called UKF, QKF, CKF and S2KF, respectively.
In the special case of assumption N and d, the jointly
Gaussian condition is implied. Therefore, the formulas for
this case give equivalent results to the Kalman ﬁlter. Fur-
ther, introducing a complete model linearization and Gaussian
approximation by using the latter part of assumption d’ and
again ignoring marginalization of the inactive subspace, gives
equivalent results to an explicitly linearized Kalman ﬁlter, e.g.
the extended Kalman ﬁlter (EKF).

Marginalization techniques for special cases of assumption c
have been presented in conjuncture with Kalman ﬁlters. In [6],
a marginalized QKF is presented in which a quadrature rule is
used to compute predicted and updated mean and covariance
for a nonlinear subspace. Subsequently, the mean, covariance
and cross-covariance of the conditionally linear and Gaussian
subspace are predicted and updated. In contrast, we present
techniques for jointly calculating the means covariances of the
corresponding subspaces. Further, in [7] marginalization of a
conditionally linear subspace in the state equation combined
with an UKF is presented. In [8], marginalization of an inactive
augmented noise state in the prediction is presented. Finally,
in [9] a combination of linear approximations and numerical
approximations for an UKF is presented.

B. Marginalized particle ﬁlters

Marginalized, a.k.a. Rao-Blackwellized, particle ﬁlters, see
e.g. [10], are diﬀerent in nature since they do not make
assumption N. However, they can only handle special cases
of assumptions a-c. In general, marginalized particle ﬁlters
require a static conditional linear and Gaussian subspace,

xi = Ax

and x j = Cx.

An
k = Cn

k = An = Cn

References

[1] J.-O. Nilsson, D. Zachariah, I. Skog, and P. H¨andel, “Cooperative local-
ization by dual foot-mounted inertial sensors and inter-agent ranging,”
EURASIP J. Adv. Sig. Pr., vol. 164, 2013.

[2] S. Julier, J. Uhlmann, and H. Durrant-Whyte, “A new method for
the nonlinear transformation of means and covariances in ﬁlters and
estimators,” IEEE Trans. Autom. Control, vol. 45, no. 3, pp. 477–482,
2000.

[3] K. Ito and K. Xiong, “Gaussian ﬁlters for nonlinear ﬁltering problems,”

IEEE Trans. Autom. Control, vol. 45, pp. 910–927, May 2000.

[4] I. Arasaratnam and S. Haykin, “Cubature Kalman ﬁlters,” IEEE Trans.

Autom. Control, vol. 54, pp. 1254–1269, June 2009.

[5] J. Steinbring and U. Hanebeck, “LRKF revisited: The smart sampling
Kalman ﬁlter (S2KF),” J. Adv. Inf. Fusion, vol. 9, pp. 106–123, Dec.
2014.

[6] P. Closas and C. Fernndez-Prades, “The marginalized square-root

quadrature Kalman ﬁlter,” in IEEE SPAWC, pp. 1–5, June 2010.

[7] G. Chang, “Marginal unscented Kalman ﬁlter for cross-correlated pro-
cess and observation noise at the same epoch,” IET Radar, Sonar Nav.,
vol. 8, pp. 54–64, January 2014.

[8] Y. Huang, Y. Zhang, X. Wang, and L. Zhao, “Gaussian ﬁlter for
nonlinear systems with correlated noises at the same epoch,” Automatica,
vol. 60, pp. 122 – 126, 2015.

[9] C. Liu, P. Shui, and S. Li, “Unscented extended Kalman ﬁlter for target
tracking,” J. Syst. Eng. and Electron., vol. 22, pp. 188–192, April 2011.
[10] T. Sch¨on, F. Gustafsson, and P. J. Nordlund, “Marginalized particle ﬁlters
for mixed linear/nonlinear state-space models,” IEEE Trans. Signal
Process., vol. 53, pp. 2279–2289, July 2005.

[11] D. Zachariah, I. Skog, M. Jansson, and P. H¨andel, “Bayesian estimation
with distance bounds,” IEEE Signal Process. Lett, vol. 19, pp. 880–883,
Dec 2012.

[12] J.-O. Nilsson and P. H¨andel, “Robust recursive network clock synchro-

nization,” in IEEE CONECCT, (Bangalore, India), 6-7 Jan. 2014.

8

i.e. the marginalized particle ﬁlter handle a special case of
assumption c. (Throughout the results we have dropped indices
k of the subspace matrices but it has been implicit that they
may vary with k.) The static subspace imply that

p(Al ˙x|An ˙x, ˙Y) = NAl ˙x(Al ˆ˙x, ˆ˙P)
p(Al ˙x|An ¨x, ˙Y) = NAl ˙x(Al ˇ˙x, ˇ˙P)

where {Al ˆ˙x, ˆ˙P, Al ˇ˙x, ˇ˙P} are given by Kalman ﬁlters. Such
Kalman ﬁlter are then run for each particle (hypothesis) of
An ˙x and An ¨x in a particle ﬁlter.

The marginalized particle ﬁlter also handles special cases
between assumption a and c. In case An = C, i.e. the output
is independent of the linear subspace of the state equation,
then the general output model of a can be combined with
the state equation of c. Vice versa if A = Cn, i.e. the state
equation is independent of the linear subspace of the output
model, then the general state equation of a can be combined
with the output model of c.

C. Other

A two step variant, in terms of mean and quadratic mean,
of the marginalization for update for a special case of C and
assumptions N and a can be found in [11]. A variant for an
arbitrary C can be found in [1]. A single dimensional version
in terms of mean and covariance can be found in [12].

X. Discussion and conclusion

In this article we have presented Bayesian ﬁltering tools
which, given Gaussian priors and posteriors, enables one to
exploit a set of sequentially more constraining model assump-
tions, condensing the ﬁltering problem to essential moment
integrals and enabling introduction of model approximations
in the same integrals. Combined with suitable moment integral
approximation techniques, e.g. cubature or importance sam-
pling, these tools provide a new class of ﬁltering techniques,
more general than the Kalman ﬁlters. Especially, in contrast to
Kalman ﬁlters, the tools can handle heavy-tailed output noise.
What are suitable approximations techniques depend on the
properties of the models and the speciﬁc moment integrals.
Further, the model assumptions enables system composition
(combination of diﬀerent submodels) by specifying subspace
projection matrices, decoupling the (sub-) system modelling
and the system composition. Moreover, the composition be-
comes transparent, enabling the Bayesian ﬁltering tools to
exploit the underlying structure.

In summary, we have shown how model structure and model
approximations, combined with marginalization, can reduce
the ﬁltering problem to low-dimensional moment integrals
which enables us to handle a more general class of ﬁltering
problems as compared to Kalman ﬁlters and which opens up
for a large range of numerical approximations techniques. Fur-
ther, for speciﬁc models and distributions, analytical solutions
to the marginal moment integrals may be possible.

