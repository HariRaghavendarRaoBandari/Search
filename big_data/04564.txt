Problems of Information Transmission,
vol. 51, no. 4, pp. 3-22, 2015.

M. V. Burnashev

ON RELIABILITY FUNCTION OF BSC: EXPANDING THE

REGION, WHERE IT IS KNOWN EXACTLY 1

The region of rates (“straight-line”), where the BSC reliability function is known

exactly, is expanded.

§ 1. Introduction and main results

In the paper notation from [1] is used. A binary symmetric channel (BSC) with
crossover probability 0 < p < 1/2 and q = 1 − p is considered. Let F n denote the set
of all 2n binary n-tuples, and d(x, y), x, y ∈ F n denote the Hamming distance between
x and y. A subset C = C(M, n) = {x1, . . . , xM} ⊆ F n is called a code of length n and
cardinality M. The minimum distance of the code C is d(C) = min{d(xi, xj) : i 6= j}.
Code rate is R(C) = n−1 log2 M. Everywhere below log z = log2 z.
Cardinality of a set A is denoted by |A|. Code spectrum (distance distribution) B(C) =
(B0, B1, . . . , Bn) is the (n + 1)–tuple with components
Bi = |C|−1 |{(x, y) : x, y ∈ C, d(x, y) = i}| ,

i = 0, 1, . . . , n.

(1)

In other words, Bi is average number of codewords y on the distance i from the codeword
x. Clearly, B0 + . . . + Bn = |C|. The total number of ordered codepairs x, y ∈ C with
d(x, y) = i equals |C|Bi.

The BSC reliability function E(R, p) is deﬁned as follows [2, 3, 4]

E(R, p) = lim sup

n→∞

1
n

, ln

1

Pe(R, n, p)

,

where Pe(R, n, p) – the minimal possible decoding error probability Pe for (n, R)–code.

Introduce the function [5]

6
1
0
2

 
r
a

M
 
5
1

 
 
]
T
I
.
s
c
[
 
 

1
v
4
6
5
4
0

.

3
0
6
1
:
v
i
X
r
a

G(α, τ ) = 2

α(1 − α) − τ (1 − τ )
1 + 2pτ (1 − τ )

=

1

2 −pτ (1 − τ ) −

(1 − 2α)2

2h1 + 2pτ (1 − τ )i

and deﬁne the function δGV (R) ≤ 1/2 (Gilbert- Varshamov bound) as

1 − R = h2(δGV (R)),

0 ≤ R ≤ 1,

1Supported in part by the Russian Foundation for Basic Research (project nos. 15-01-08051) and by

Russian Scientiﬁc Foundation (project nos. 13-01-12458 офи_м2).

1

(2)

(3)

where h2(x) = −x log2 x − (1 − x) log2(1 − x).
Deﬁne the value R = R(α, τ ) by the formula

R = 1 − h2(α) + h2(τ ),

0 ≤ τ ≤ α ≤ 1/2.

For R ∈ (0, 1) and α ∈ [δGV (R), 1/2] introduce also values

τR(α) = h−1
2

(h2(α) − 1 + R) ≤ α,

ωR(α) = G(α, τR(α))

and

ωR =

min

δGV (R)≤α≤1/2

ωR(α) = min{G(α, τ ) : h2(α) − h2(τ ) = 1 − R}.

The best known upperbound for the maximal relative code distance δ(R) (linear
programming bound) has the form [5, 6]

δ(R) ≤ δLP (R) = ωR.

(4)

(5)

(6)

(7)

If R ≤ R0 ≈ 0.30524 (R0 is deﬁned in (28) and (29)), then minimum in the right-hand

side of (6) is attained for α = 1/2, and the formula (7) takes simple form [5, 6]

δLP (R) = ωR =

R = h2(τ ).

Denote by αR, τR optimal values of parameters α, τ в (6),i.e.

1

2 −pτ (1 − τ ),

ωR = ωR(αR) = G(αR, τR).

(8)

(9)

The function ωR decreases monotonically for R ∈ (0, 1), and the function αR does not
increases in R.

Introduce critical rates Rcrit(p), R1(p) and R2(p), beginning with well-known

Rcrit(p) = 1 − h2(cid:18) √p

√p + √q(cid:19) .

The rate R1(p) was introduced in [1, формулы (6)]

R1(p) = h2(τ1(p)),

τ1(p) = (cid:0)1 − (4pq)1/4(cid:1)2
2(1 + √4pq) ≤

1
2

.

Introduce the important value
2√pq
1 + 2√pq

ω1(p) =

= G(1/2, τ1(p)),

0 ≤ p ≤ 1/2.

(10)

(11)

(12)

Note that the value ω1(p) is deﬁned by the condition t1(ω) = t2(ω, p) (см. (58)).

2

Introduce the critical rate R2 = R2(p) by the formula

ωR2 = ω1(p) =

2√pq
1 + 2√pq

,

0 < p < 1/2,

or, equivalently,

R2(p) = 1 − max

α,τ {h2(α) − h2(τ ) : G(α, τ ) = ω1(p)}.

(13)

(14)

In other words, R2(p) is the minimal rate for which it is possible to have G(α, τ ) = ω1(p).
On the contrary, R1(p) is the maximal such rate (it corresponds to α = 1/2). Functions
R = R2(p) and R = R1(p) decreases monotonically in p ∈ [0, 1/2].
Introduce the value p0 ≈ 0, 036587 as the unique root of the equation R2(p) = R1(p).
If p < p0, then in the optimizing value α < 1/2. If p ≥ p0, then the optimizing value
α = 1/2 and R2(p) = R1(p). We also have R2(0) = R1(0) = 1 and R2(p0) = R1(p0) =
R0 ≈ 0.30524, where R0 is deﬁned in (28)–(29).
Rcrit(p). Then we have

Introduce also the value p1 ≈ 0, 0078176 as the unique root of the equation R1(p) =

R2(p) < Rcrit(p),

R0 < R2(p) < R1(p),
R1(p) < Rcrit(p),

p < p0;
p > p1;
Rcrit(p) ≤ R0,

0 < p < 1/2;
R2(p) = R1(p) < R0,
R1(p) > Rcrit(p),
p ≥ 0, 05014.

p > p0;

p < p1;

(15)

In Fig. 1 plots of functions R1(p), R2(p), Rcrit(p) and C(p) are shown.
Remark 1. Although notations ωR(α) and ω1(p) (also τR(α) and τ1(p)) are not well

consistent, it should not imply any problems (for example, we always have R < 1).

In the region Rcrit(p) ≤ R ≤ C(p) = 1 − h2(p) the function E(R, p) is known since a

long time ago [2] and it coincides with the sphere-packing bound

E(R, p) = Esp(R, p),

Rcrit(p) ≤ R ≤ C(p),

where

Esp(R, p) = D (δGV (R)kp) ,

Esp(0, p) =

1
2

x
y

+ (1 − x) log

1 − x
1 − y

,

D(xky) = x log
log

= 2E(0, p).

1
4pq

(16)

(17)

The main result of the paper is
T h e o r e m 1. 1) For any 0 < p < 1/2 the inequality holds

E(R, p) = 1 − log2 (1 + 2√pq) − R,

R2(p) ≤ R ≤ Rcrit(p),

(18)

where R2(p), Rcrit(p) are deﬁned in (13) and (10), respectively.

3

1
4pq − µ(R, 1/2, ωR) =
2 −pτ (1 − τ ),
ωR =

1

R = h2(τ ),

2) For any 0 < p < 1/2 and 0 ≤ R ≤ min{R0, R2(p)} the bound is valid
E(R, p) ≤

1
4pq − h2(τ ) − h2(ωR) + 1,

ωR
2

ωR
2

log

log

(19)

where ωR и µ(R, 1/2, ω) are deﬁned in (6) and (33), respectively.

3) If p < p0 ≈ 0, 036587, then R0 < R2(p) and for R0 ≤ R ≤ R2(p) the bound holds

E(R, p) ≤ 1 + min

log

0≤α≤1/2(cid:26) G(α, τ )
≤ 1 − R +

2
ωR
2

log

1

4pq − L(G(α, τ ))(cid:27) − R ≤
1
4pq − L(ωR),

where (t1(ω) is deﬁned in (58))

L(ω) = 2h2 [t1(ω)] − ω − (1 − ω)h2(cid:20) 2t1(ω) − ω
2(1 − ω) (cid:21) .

(20)

(21)

In other words, for any 0 < p < 1/2 the function E(R, p) is linear for R2(p) ≤
R ≤ Rcrit(p). Earlier it was known only for R1(p) ≤ R ≤ Rcrit(p), if p is not too
small [7, 1]. Recall that R2(p) < R1(p), p < p0, and R2(p) = R1(p), p ≥ p0. Also
R2(p) ≤ min{R1(p), Rcrit(p)}, 0 < p < 1/2 (see also (15)).
Notice also that improvement in the formula (18) with respect to [1] is attained due
to using the values α < 1/2 (in [1] only α = 1/2 was used).

Inequalities (19)–(20) strengthen similar estimate form [1, теорема 1].
For comparison purpose, the best know lowerbound for E(R, p) has the form [4]

and

E(R, p) ≥ −δGV (R) log(2√pq),
E(R, p) ≥ 1 − log2 (1 + 2√pq) − R,

0 ≤ R ≤ Rmin(p),

Rmin(p) ≤ R ≤ Rcrit(p),

where δGV (R) is deﬁned in (4),

Rmin(p) = 1 − h2(cid:18) 2√pq

1 + 2√pq(cid:19) ,

(22)

(23)

(24)

and Rmin(p) < R2(p) < Rcrit(p), 0 < p < 1/2.

Denote by Eup(R, p) the right-hand sides of formulas (18)–(20), and by Elow(R, p)

the right-hand sides of formulas (22)–(23). Then for all R, p we have

Elow(R, p) ≤ E(R, p) ≤ Eup(R, p).

In Fig. 2 plots of functions Elow(R, p) and Eup(R, p) for p = 0, 01 are shown. In that

case Rcrit ≈ 0, 5591, R1 ≈ 0, 5518, R2 ≈ 0, 5370, Rmin ≈ 0, 3516, C ≈ 0, 9192.

4

Notice that Eup(R, p) − Elow(R, p) > 0 for R < R2(p) and all p.
Upper bounds (19) and (20) have simple meaning: we should apply the union bound
using the best known upperbound (7) for the value δ(R) and the best known lowerbound
(26) for the number of code neighbors.

When proving Theorem 1 we will need the function [9]:

µ(R, α, ω) = h2(α) − 2

ω/2

log

Z0

P +pP 2 − 4Qy2

Q

1 − ω (cid:19) ,
dy − (1 − ω)h2(cid:18) α − ω/2

(25)

P = P (y) = α(1 − α) − τ (1 − τ ) − y(1 − 2y),

(1 − 2y)2 − (1 − 2α)2

,

4

Q = Q(y) = (α − y)(1 − α − y) =
where τ ≤ 1/2 such that h2(τ ) = h2(α) − 1 + R.
(see (1)) is described by the following variant [9, Theorem 5] (see also proof in [1]).

Importance of the function µ(R, α, ω) and its relation to the code spectrum {Bi}
T h e o r e m 2. For any (R, n)–code and any α ∈ [δGV (R), 1/2] there exists ω, 0 ≤

ω ≤ G(α, τ ), where h2(τ ) = h2(α) − 1 + R and G(α, τ ) is deﬁned in (2), such that

1
n

log Bωn ≥ µ(R, α, ω) + o(1),

n → ∞,

(26)

where µ(R, α, ω) > 0 is deﬁned in (25) and for µ(R, α, ω) the nonintegral representation
(82) holds.

It should be noted that the parameter α determines a constant weight αn code, which

replaces the original code (using Elias-Bassalygo lemma) [5, 9, 1].

For 0 ≤ R ≤ R0 the best in Theorem 2 is α = 1/2 [8, Remark 4], since such
α simultaneously minimizes G(α, τ ) and maximizes µ(R, α, ω) for all ω. For R > R0,
probably, the optimal is α = αR (see (9)), i.e. minimization of G(α, τ ) over α < 1/2 (at
least, the value G(α, τ ) is in the estimate (20)). With α = αR we get from Theorem 2

C o r o l l a r y 1. For any (R, n)–code there exists ω, 0 ≤ ω ≤ ωR such that

1
n

log Bωn ≥ µ(R, αR, ω) + o(1),

n → ∞,

(27)

where ωR and αR are deﬁned in (6) and (9), respectively.

Remark 2. Theorem 1 is based on the important feature of inequalities (26) and (27).
Till 1999 the best upperbound for E(R, p) followed from the best upperbound (7) for
the maximal relative code distance δ(R) ≤ ωR [5]. From that point of view inequalities
(26)–(27) do not improve the estimate (7), but it follows from them that µ(R, αR, ωR) >
0. In other words, there are an exponential number of codewords on the minimal (or
smaller) code distance ωRn. That “correction” on µ(R, αR, ωR) in inequalities (19)–(20),
essentially, constitutes Theorem 1. Notice also that the randomly chosen (“typical”) code
has the spectrum n−1 log Bωn ≈ h2(ω)− h2(δGV (R)), ω ≥ δGV (R). It is possible to check

5

that for such code the maximal contribution (additive) to the decoding error probability
Pe is given by “neighbors” on the distance ω1(p)n (see (12)), from which the inequality
(24) follows for all R, p.

Introduce the value R0 by the formula [6, 8]

R0 = h2(τ0) ≈ 0, 30524,
where τ0 ≈ 0, 054507 – the unique root of the equation

(1 − 2τ )"1 +
2pτ (1 − τ )


(1 − 2τ )

For ﬁxed R we have

dG
dα

=

2(1 − 2α)

1 + 2pτ (1 − τ ) −

1

2pτ (1 − τ )# − ln

1 − τ
τ

= 0.

1 −

(1 − 2α)2

(cid:16)1 + 2pτ (1 − τ )(cid:17)2


For any R and 0 < ω < G(α, τ ) we also have [8, Proposition 1]

(28)

(29)

ln[(1 − α)/α]
ln[(1 − τ )/τ ]

. (30)

µ′

α(R, α, ω) > 0,

δGV (R) ≤ α < 1/2.

For R ≤ R0 the function G(α, τR(α)) monotonically decreases in α ∈ [δGV (R), 1/2]
and αR = 1/2. If R ∈ (R0, 1) then values 0 < τR < αR < 1/2 are uniquely deﬁned by
the system of equations

Also

τR =

1

2(1 −r1 −hp(1 − ωR)2 − (1 − 2α)2 − ωRi2) .

For α ∈ (0, 1/2) denote by τ (α) the unique root of the equation (31), such that
τ (α) < α. Denote also R(α) = 1 − h2(α) + h2(τ (α)), α ∈ (0, 1/2).
The function τ (α) monotonically increases, and R(α) monotonically decreases on α.
Each α ∈ (0, 1/2) deﬁnes R(α) > R0 and τ (α) < α. Similarly, each R > R0 uniquely
deﬁnes α < 1/2 and τ (α) < α. Note that

lim
α↑1/2

τ (α) = τ0 ≈ 0, 0545,

lim
α↑1/2

R(α) = R0 ≈ 0, 305,

lim
α↑1/2

p(α) = p0 ≈ 0, 036587.

For calculation purpose it is convenient ﬁrst to set the parameter α ∈ (0, 1/2),
and then ﬁnd sequentially corresponding values τ (α), R(α) from (31), (32) and ω(α) =
G(α, τ (α)). For p = p(α) (from (13)) we have R2(p) = R(α) and

p(α) =

1 − G − √1 − 2G

2(1 − G)

6

,

G = G(α, τ (α)).

dG
dα

= 0,

h2(α) − h2(τ ) = 1 − R.

(31)

(32)

In particular, for α → 0 we have
τ ≈ α2, G(α, τ (α)) ≈ α,
R2(p(α)) ≈ 1 − α log

α2
4

p(α) ≈
1
, Rcrit(p(α)) ≈ 1 −
α

, C(p(α)) ≈ 1 −
log

log

1
α

,

α2
2
1
α

.

α
2

Notice that due to Theorem 2 for a chosen α there exists ω such that n−1 log Bωn ≥
µ(R, α, ω) + o(1). In other words, the number of neighbors on the distance ωn for each
codeword xi satisﬁes in average that lowerbound. In fact, that property holds not only
in average, but also for every codeword xi from an “essential” part of all M codewords
(i.e. for Meo(n), n → ∞ codewords). That fact, established by the “cleaning procedure”,
regularly was used in the papers [10, 8, 1] (and earlier) and will be also used in the proof
of Theorem 1.

Remark 3. In the author’s paper [1] there are the following inaccuracies:
3a) There is a miscalculation in the formula (15) [1] for the function µ(R, 1/2, ω),
coming from the earlier paper [8, formula (23)]. The correct version of that formula is
[8, формула (23)]. Правильный вид этой формулы

µ(R, 1/2, ω) = −2(1 − ω) log(1 − ω) − log τ − 2(1 − τ ) log(1 − τ )+
+(1 − 2τ ) log(τ − ω + g) + log[1 − ω − (1 − 2τ )g] − 2ω log g − 2,

(33)

where

τ = τ (R) = h−1

2 (R),

g = g(τ, ω) =

1 − 2τ +p(1 − 2τ )2 − 4ω(1 − ω)

2

.

From (33) the useful formula, which has already appeared in [1, formula (16)], follows

µ(h2(τ ), 1/2, G(1/2, τ )) = h2(τ ) + h2(G(1/2, τ )) − 1,

τ ≥ 0.

(34)

Due to importance of the formula (34), in Appendix its derivation is presented. In
§4 the explicit (non-integral) representation for the function µ(R, α, ω) is obtained, from
which the formula (33) can be received as well. There also the generalization of the
formula (34) for arbitrary α is obtained (see (75)).

2b) There is the following inaccuracy in the formulation of Theorem 1 [1]: the function
W (ω, α, R, p) was deﬁned, using the value t2(ω, p) (for all ω). In fact, the proof of
Theorem 1 in [1] was performed using the right function t(ω, p) (and for that purpose
the function t(ω, p) was introduced in [1, formula (29)]). Due to the author’s fault, the
deﬁnition [1, formula (9)]) (coming from the earlier paper [8]) remained in [1, §1]. Those
changes do not inﬂuence validity of the corollary 1 [1].

2c) There is an inaccuracy in the proof of the Theorem 1 (noted by Litsyn S., for
which the author is grateful to him): the “cleaning” procedure in [1, §4] was performed
in such a way that formally [1, формула (44)] does not yet follow from [1, формула

7

(41)]. The same drawback remained in [14] as well. In §2 below we ﬁx that inaccuracy,
and, moreover, simplify the proof.

2d) The main diﬀerence of the paper with respect to [8] is that here it turned out
possible to investigate the case α < 1/2. An important role was played by the relation
(75). Also some proof details were simpliﬁed.

In §2 connection between Pe and a code spectrum is investigated. In §3 the proof of
Theorem 1 is given. In §4 the non-integral representation for µ(R, α, ω) is derived. That
representation is used in the proof of Theorem 1 in §3. Some calculations and proofs are
presented in Appendix.

§ 2. Lower bound for Pe and code spectrum

For xi ∈ C denote di(y) = d(xi, y) and for any integer tn, t ∈ (0, 1), introduce the

set

Xt(y) = {xi ∈ C : di(y) = tn} = (C + y)(tn),

y ∈ F n.

Also for any integer tn, t ∈ (0, 1) and each pair of codewords xi 6= xj in the output
space F n of the channel introduce the “ambiguity” set:

and for i = 1, . . . , M the set

Yij(t) = Yji(t) = {y : di(y) = dj(y) = tn}

Yi(t) =[j6=i

Yij(t) = {y : there exists xj 6= xi, such that di(y) = dj(y) = tn} .

Next result is a variant of [1, Proposition 2, formulas (20), (21)] (see also [8, лемма

2]).

L e m m a 1. For error probability Pe the lower bound holds (t = m/n)

qn
2M

Pe ≥

n

q(cid:19)m
Xm=1(cid:18) p

Xy:|Xt(y)|≥2

|Xt(y)| =

qn
2M

n

Xm=1(cid:18) p

q(cid:19)m M
Xi=1

|Yi(t)| .

(35)

P r o o f. We explain only the equality in the formula (35) (it was not done in [1]).

It is suﬃcient to check the relation

Xy:|Xt(y)|≥2

|Xt(y)| =

M

Xi=1

|Yi(t)| ,

t = m/n > 0.

Since

(cid:3)

For any point y with |Xt(y)| ≥ 2 those |Xt(y)| codewords {xi} give the same
contribution |Xt(y)| to the right-hand side of the equality.
|Xt(y)| − Xy:|Xt(y)|=1
tn(cid:19) − |{y : |Xt(y)| = 1}| ,

|Xt(y)| =Xy
= M(cid:18) n

Xy:|Xt(y)|≥2

|Xt(y)| =

8

then, in particular, from (35) for any t ∈ (0, 1) we have

Pe ≥

qn

2M (cid:18)p

q(cid:19)tn(cid:20)M(cid:18) n

tn(cid:19) − |{y : |Xt(y)| ≥ 1}|(cid:21) .

(36)

Using the inequality |{y : |Xt(y)| ≥ 1}| ≤ 2n and optimizing over t, we get from (36)
the sphere-packing bound (see [1, §3])

E(R, p) ≤ Esp(R, p),

0 ≤ R ≤ C(p),

(37)

where Esp(R, p) is deﬁned in (17). From the inequality (37) and similar lowerbound [2]
we get the formula (16). In other words, for R ≥ Rcrit(p) the lowerbound (35) for Pe is
logarithmically precise.

Important for us will be the following result, similar to Johnson’s bound

[11, Theorems 17.2.2 and 17.2.4] (see proof in Appendix).

L e m m a 2. Let C = {x1, . . . , xM} – code of length n and constant weight tn,

t ≤ 1/2. If for some ω < 1/2 and some δ ≥ 0 the following condition is fulﬁlled

and for some a > 0 the value t satisﬁes the inequality

X0<i<ωn

Bi ≤ δM,

then

t ≤

1 −p1 − 2(1 − δ)ω + 2a

2

,

M ≤

ω
a

.

(38)

(39)

(40)

In the sequel Lemma 2 will be used for small δ, a. Then (39) takes the form (see the

deﬁnition of the value t1(ω) in (58))

t ≤ (1 − √1 − 2ω)/2 + o(1),

n → ∞.

Consider some values related to sums in the right-hand side of (35). For codewords

xi, xj with dij = d(xi, xj) = ωn introduce the set

Zij(t, ω) = {y : di(y) = dj(y) = tn} .

(41)

Since the cardinality |Zij(t, ω)| does not depend on indices (i, j), denote it simply Z(t, ω).
For the value Z(t, ω), ω/2 ≤ t ≤ 1/2 we have
log2(cid:20)(cid:18) (1 − ω)n

(t − ω/2)n(cid:19)(cid:18) ωn

log2 Z(t, ω) =

1
n

1
n

u(t, ω) = ω + (1 − ω)h2(cid:18) 2t − ω

ωn/2(cid:19)(cid:21) = u(t, ω) − δ(t, ω),
2(1 − ω)(cid:19) ,

(42)

2
n

log2

n + 2

,

2

0 ≤ δ(t, ω) ≤

9

since for any 0 ≤ k ≤ n inequalities hold [12, formula (12.40)]
k(cid:19) ≤ 2nh(k/n).

2nh(k/n) ≤(cid:18)n

n + 1

1

For the function u(t, ω) we have

u′
ω = −

1
2

log

(1 − ω)2

(2t − ω)(2 − 2t − ω) ≤ 0,

(1 − 2t)2

(1 − ω)(2t − ω)(2 − 2t − ω) ln 2 ≤ 0,

u′′
ωω = −
u′
t = log

2 − 2t − ω
2t − ω ≥ 0,
4(1 − ω)

1
2

,

t ≤

u′′
tt =

(2t − ω)(2 − 2t − ω) ln 2 ≤ 0.

We call (xi, xj) ω–pair, if dij = ωn. Then the total number of ω–pairs in a code
equals MBωn. We say that a point y is (ω, t)–covered, if there exists ω–pair (xi, xj)
such that di(y) = dj(y) = tn. For the point y denote by K(y, ω, t) the number of her
(ω, t)–coverings (taking into account multiplicity of coverings), i.e.

K(y, ω, t) = | {(xi, xj) : dij = ωn, di(y) = dj(y) = tn} |,

ω > 0.

(44)

(43)

(45)

(46)

(47)

Then for any t, y

and for any t, ω we get

Therefore from (35) and (45) for any t, ω we have

K(y, ω, t),

|Xt(y)| (|Xt(y)| − 1) =Xω>0
|Xt(y)| ≥pK(y, ω, t).
q(cid:19)tn
2M (cid:18)p

Pe ≥

qn

Xy∈YpK(y, ω, t).

We modify the right-hand side of (46) as follows. For any set A denote

K(A, ω, t) =Xy∈A

K(y, ω, t),

where K(y, ω, t) is deﬁned in (44). In other words, K(A, ω, t) is the total number of
(ω, t)–coverings of the set A.

Introduce the set Y(ω, t) of all (ω, t)–covered points y, i.e.

Y(ω, t) = {y : K(y, ω, t) ≥ 1} =(cid:26)y :

there exist xi, xj, such that

dij = ωn and di(y) = dj(y) = tn (cid:27) .

10

Since every ω–pair (xi, xj) t-covers Z(t, ω) points y, then

K(Y, ω, t) = K(Y(ω, t), ω, t) = MBωnZ(t, ω).

For any subset Y′ ⊆ Y(ω, t) introduce the value
Kmax(Y′, ω, t) = max
y∈Y′

K(y, ω, t).

Then for any t, ω and Y′ ⊆ Y(ω, t) из (46) we have

(48)

(49)

(50)

Pe ≥

qn

2M (cid:18)p

q(cid:19)tn K(Y′, ω, t)
pKmax(Y′, ω, t)

.

Describe the scheme of proving Theorem 1 realized in the paper. Suppose that for
chosen ω, t it is possible to choose also a set Y′(ω, t) ⊆ Y(ω, t) such that the following
two conditions are fulﬁlled:

K(Y′(ω, t), ω, t) ≥ 2o(n)K(Y(ω, t), ω, t),

n → ∞

and

Then the inequality (50) can be continued as follows

K(y, ω, t) ≤ 2o(n),

y ∈ Y′(ω, t).

(51)

(52)

q(cid:19)tn
Pe ≥ 2o(n)qn(cid:18)p

q(cid:19)tn
K(Y(ω, t), ω, t) = 2o(n)qn(cid:18) p

BωnZ(t, ω).

(53)

Estimate (53) is the desired additive lowerbound for Pe (for values ω, t). After
optimization of the right-hand side of (53) over ω, t Theorem 1 will be proved.

Remark 4. For a good code the value K(y, ω, t) in (52), probably, can not be

exponential in n for an essential part of all points y. In other words, for a good code
it is unlikely that exponential number of codewords are more probable than the true
codeword (!?).

We set some ω and t = t(ω). Due to (48) there exists a collection of Mω points

{y1, . . . , yMω} such that

K(yi, ω, t) ∼

MBωnZ(t, ω)

Mω

,

i = 1, . . . , Mω.

For that purpose it is suﬃcient to “quantize” all values ln K(y, ω, t) ∼ n with a step of
order o(n), n → ∞. At that K(yi, ω, t) is the number of ω-pairs on the t-sphere around
the point yi. The total number of various ω-pairs on t-spheres around points {yi} has
the order MBωn, i.e. "all"(in exponential sense) ω-pairs are located on those spheres.
Therefore each ω-pair belongs to Z(t, ω) t-spheres, i.e. it covers Z(t, ω) points yi. Then,
essentially, each ω-pair covers only points yi.

11

If there are several such Mω then choose the maximal one.
As a result, there are Nω points on every t-sphere and each t-sphere there are
K(yi, ω, t) ω-pairs. We investigate the right-hand side of the inequality (53). Denoting

b(ω) =

1
n

log Bωn + o(1),

n → ∞,

represent (53) in an equivalent form (n → ∞)

where

1
n

log

1
Pe ≤ c(ω, t, p) − b(ω) + o(1),

c(ω, t, p) = t log

q
p − log q − u(t, ω),

(54)

(55)

(56)

and the function u(t, ω) is deﬁned in (42). For the function c(ω, t, p) using (43) we have

(1 − ω)2

1
2

log

c′
ω = −u′
c′′
ωω =

ω =

(2t − ω)(2 − 2t − ω) ≥ 0,
(1 − 2t)2
(1 − ω)(2t − ω)(2 − 2t − ω) ln 2 ≥ 0,

ω
2

+ (1 − ω)p,

(57)

c′
t = log

q(2t − ω)
p(2 − 2t − ω) ≤ 0,

t ≤ t2(ω, p) =
(2t − ω)(2 − 2t − ω) ln 2 ≥ 0.

4(1 − ω)

c′′
tt =

The function c(ω, t, p) from (56) has a simple meaning. Suppose that we distinguish
two codewords xi, xj with d(xi, xj) = ωn. Introduce the set of “ambiguity” Zij(t, ω)
from (41). If y ∈ Zij(t, ω) then with probability 1/2 decoding error occurs. Moreover,

P {y ∈ Zij(t, ω)|xi} ∼ 2−c(ω,t,p)n.

In order to choose the radius t introduce functions (see. [1, formula (29)])

t1(ω) =

1 − √1 − 2ω

2

,

t2(ω, p) =

ω
2

+ (1 − ω)p.

The function t2(ω, p) sometimes is called “Elias radius”. We set

t(ω, p) = min{t1(ω), t2(ω, p)} =(cid:26) (cid:0)1 − √1 − 2ω(cid:1) /2, ω ≤ ω1(p),

ω/2 + (1 − ω)p,

ω ≥ ω1(p),

(58)

(59)

where ω1(p) is deﬁned in (12). The threshold value ω1(p) will play very important role
in the sequel.

12

It follows from (57) that the function c(ω, t, p) monotonically decreases in t < t2(ω, p)

and monotonically increases in t > t2(ω, p). In particular,

min

t

c(ω, t, p) = c(ω, t2(ω, p), p) =

ω
2

log

1
4pq

.

For any ω we will alway choose t such that the following condition is satisﬁed

t ≤ t(ω, p).

(60)

(61)

There are two reasons for such choice:

1) we would like to minimize the function c(ω, t, p), which monotonically decreases

in t < t2(ω, p);

2) the condition t ≤ t1(ω) is necessary in order Lemma 2 be valid (and related with

it the condition (52)).

§ 3. Proof of Theorem 1

For a given R choose some α such that h−1

2 (1 − R) ≤ α ≤ 1/2 (see Theorem 2) and
2 (h2(α) − 1 + R) (such τ minimizes G(α, τ )). Due to Theorem 3 for some

set τ = h−1
δ ≤ δ0 = G(α, τ ) we have
1
n

log Bδn ≥ µ(R, α, δ) + o(1),

n → ∞.

Since K(Y, δ, s(δ)) = MBδnZ(s(δ), δ) for any radius s(δ), there exists a collection of

Nδ points {y1, . . . , yNδ} such that

K(yi, δ, s(δ)) =

2o(n)MBδnZ(s(δ), δ)

Nδ

,

i = 1, . . . , Nδ.

Therefore from (46) we get

Pe ≥ 2o(n) qn

√M (cid:18)p

q(cid:19)s(δ)n

pNδBδnZ(s(δ), δ).

(62)

(52) is fulﬁlled);

Now two cases are possible:
1) ln K(yi, δ, s(δ)) = o(n) for an essential part Nδ of points {yi} (i.t. the condition
2) ln K(yi, δ, s(δ)) ∼ n for for an essential part Nδ of points {yi}.
Consider sequentially those cases assuming s(δ) ≤ t1(δ).
1) If ln K(yi, δ, s(δ)) = o(n) for an essential part Nδ of points {yi}, then the bound

(62) takes additive form for δ and t = s(δ)

Pe ≥ 2o(n)qn(cid:18) p

q(cid:19)s(δ)n

13

BδnZ(s(δ), δ).

(63)

2) If ln K(yi, δ, s(δ)) ∼ n for an essential part Nδ of points {yi}, then consider s(δ)-
spheres around each point yi from that essential part. We may assume that on every
such s(δ)-sphere there is the same number m1 points and there are K(yi, δ, s(δ)) δ-pairs.
Denote by B′

ωn analogues of numbers Bωn for s(δ)-spheres. Then

m1B′

δn = K(yi, δ, s(δ)) =

2o(n)MBδnZ(s(δ), δ)

Nδ

.

Let for that essential part Nδ точек {y1, . . . , yNδ} the condition is satisﬁed

K(yi, ω, s(δ)) ≤ K(yi, δ, s(δ))/n.

(64)

Xω<δ

Since s(δ) ≤ t1(δ), it follows from Lemma 2 that the number points m1 on every such
s(δ)-sphere satisﬁes the inequality (40), i.e. it is non-exponential. Therefore in that case
the additive bound (63) holds.

It remains to consider the case when for some ω < δ the condition (64) is not satisﬁed,

i.e. for an essential part Nδ points {yi}

K(yi, ω, s(δ)) > K(yi, δ, s(δ))/n2.

(65)

If necessary, choose the minimal one among all possible ω < δ. Then we have (since
K(Y, ω, s) = MBωnZ(s, ω))

log [BωnZ(s(δ), ω)] ≥ log [BδnZ(s(δ), δ)] + o(n).

(66)

Therefore for an essential part Nδ of points {yi} we have log B′
δn + o(n). We
choose s(ω) ≤ t1(ω) such that the potential additive bound ω will be not less than the
right-hand side of (63), i.e. the inequality holds

ωn ≥ log B′

q(cid:19)s(δ)n
(cid:18)p

BδnZ(s(δ), δ) ≤(cid:18) p

q(cid:19)s(ω)n

Due to (66) for that purpose it is suﬃcient to have

BωnZ(s(ω), ω).

(67)

q(cid:19)s(δ)n
(cid:18)p

Z(s(δ), ω) ≤(cid:18)p

q(cid:19)s(ω)n

Z(s(ω), ω),

or, equivalently,

f = [s(δ) − s(ω)] log

q
p

+ u(s(ω), ω) − u(s(δ), ω) ≥ 0,

(68)

where u(t, ω) is deﬁned in (42). Using (43) we have

∂f

∂s(ω)

∂f

∂s(δ)

= log

= log

p[2 − 2s(ω) − ω]
q[2s(ω) − ω] ≥ 0,
q[2s(δ) − ω]
p[2 − 2s(δ) − ω] ≥ 0,

s(ω) ≤ t2(p, ω),

s(δ) ≥ t2(p, ω).

14

Therefore for all ω we set

s(ω) = t(p, ω) = min{t1(ω), t2(p, ω)}.

Since s(ω) ≤ s(δ), the inequalities (68) and (67) will be fulﬁlled. It means that a descent
from δ to ω does not decreases the potential additive bound. It should be noted that a
further descent from ω on a lower level ω1 is not possible, since the level ω was chosen
as the minimal possible one, for which the inequality (65) holds. It proves validity of the
additive bound (63) for s(ω) = t(p, ω), from which we get (h2(τ ) = h2(α) − 1 + R)

P r o p o s i t i o n 1. For any 0 ≤ R < C(p) and 0 < p < 1/2 the inequality holds

E(R, p) ≤ min

0≤α≤1/2

max

δ≤G(α,τ )(cid:26)t(p, δ) log

q

p − log q − µ(R, α, δ) − u(t(p, δ), δ)(cid:27) .

(69)

R2(p) and improves that Theorem in the less interesting region R2(p) ≤ R ≤ C(p).
extreme point δ = G(α, τ ).

That result coincides with [1, Theorem 1] in the most interesting region 0 ≤ R ≤
Remark 5. For large R maximum over δ ≤ G(α, τ ) in (69) is attained not in the
We will get the paper main Theorem 1 as a corollary from the bound (69). For that

purpose introduce the function

W (ω, α, R, p) =

ω
2

log

1
4pq − µ(R, α, ω).

(70)

As will be clear below, it is suﬃcient to consider the case 0 ≤ R ≤ R2(p). Then
min

G(α, τ ) ≥ ω1(p) (see (6) and (13)) and then t(p, δ) = t2(δ, p). Since

0≤α≤1/2

u(t2(δ, p), δ) = δ + (1 − δ)h2(p),

then for 0 ≤ R ≤ R2(p) we get from (69)
E(R, p) ≤ min

0≤α≤1/2

max

δ≤G(α,τ )

W (δ, α, R, p),

(71)

where the function W (δ, α, R, p) is deﬁned in (70).

We show that maximum over δ in the right-hand side of (71) is attained for δ =

G(α, τ ).

L e m m a 3. For any 0 ≤ τ ≤ α ≤ 1/2 such that R = 1 − h2(α) + h2(τ ) and

G(α, τ ) ≥ ω1(p) the formula holds

max

λ≤G(α,τ )

W (λ, α, R, p) = W (G(α, τ ), α, R, p) =

=

G(α, τ )

2

log

1
4pq − µ(R, α, G(α, τ )).

(72)

15

P r o o f. From (25) we have [8, Proposition 1] (P = P (ω/2), Q = Q(ω/2), a1 =

2[α(1 − α) − τ (1 − τ )])

µ′

ω(R, α, ω) =

1
2

log

= log

(1 − ω)2

(α − ω/2)(1 − α − ω/2) − log
(1 − ω)p(2α − ω)(2 − 2α − ω)

a1 − ω(1 − ω) +p(1 − 2τ )2ω2 − 2a1ω + a2

ωω(R, α, ω) < 0.

µ′′

1

P +pP 2 − Qω2

Q

=

,

(73)

For W (ω, α, R, p) we have from (70) and (73) [8, Proposition 2]

W ′′

ωω(ω, α, R, p) < 0,

W ′

ω(ω, α, R, p)(cid:12)(cid:12)(cid:12)ω=G

= log

G

√4pq(1 − G)

,

G = G(α, τ ).

Therefore, if G(α, τ ) ≥ ω1(p) then W ′
the right-hand side of (72) is attained for λ = G(α, τ ).

ω(ω, α, R, p)(cid:12)(cid:12)(cid:12)ω=G ≥ 0, and then maximum over δ in

(cid:3)

As a result, from (71) and (72) for 0 ≤ R ≤ R2(p) we get

E(R, p) ≤ min

0≤α≤1/2(cid:26) G(α, τ )

2

log

1

4pq − µ(R, α, G(α, τ ))(cid:27) .

(74)

It remained us to get for µ(R, α, G(α, τ )) from (74) an explicit expression. We use

the following analytical result (see proof in Appendix).

L e m m a 4. For any α, τ the formula holds

µ(R, α, G(α, τ )) = L(G(α, τ )) + R − 1,

(75)

where R = 1 − h2(α) + h2(τ ) and the function L(ω) is deﬁned in (21).

Using (75) and (74) we get
П р е д л о ж е н и е 2. For any 0 < p < 1/2 and 0 ≤ R ≤ R2(p) the bound holds

E(R, p) ≤ 1 − R + min

0≤α≤1/2(cid:26) G(α, τ )

2

log

1

4pq − L(G(α, τ ))(cid:27) ≤

≤ 1 − R +

ωR
2

log

1
4pq − L(ωR),

(76)

where ωR is deﬁned in (6).

In particular, from (76) the formula (20) follows. Concerning the formula (19) recall
that if R ≤ R0, then the best is α = 1/2. The bound (19) follows from (74) with α = 1/2
and (34).

It remained us to prove the formula (18). Note that

t1(ω1(p)) =

√p

√q + √p

.

16

Therefore if G(α, τ ) = ω1(p) then using (75) we get

µ(R, α, ω1(p)) =

ω1(p)

2

log

1
4pq

+ R + log(1 + 2√pq) − 1.

(77)

Then for any rate R, for which it is possible to have G(α, τ ) = ω1(p), from (74) and (77)
the inequality follows

E(R, p) ≤ 1 − log(1 + 2√pq) − R.

(78)

The rate R = R2(p) is the minimal of such rates (see (13)). For R = Rcrit(p) the formula
holds [2] (see (16))

E (Rcrit, p) = Esp (Rcrit, p) = 1 − log2 (1 + 2√pq) − Rcrit.

(79)

Therefore due to to the “straight-line upper bound” [3] the inequality (78) holds for all
R such that R2(p) ≤ R ≤ Rcrit(p), i.e.

E(R, p) ≤ 1 − log(1 + 2√pq) − R,

R2(p) ≤ R ≤ Rcrit(p).

(80)

On the other hand, for the function E(R, p) the random coding lower bound is known

[2]

E(R, p) ≥ 1 − log2 (1 + 2√pq) − R,

0 ≤ R ≤ Rcrit(p).

(81)

As result, from (80) and (81) the formula (18) follows, that completes Theorem 1 proof.

§ 4. Non-integral representation for µ(R, α, ω)

Proof of the next result represents a standard integration using Euler’s substitution.
That representation is used in deriving the formula (75) and in the proof of Theorem 1.

P r o p o s i t i o n 3. For the function µ(R, α, ω) the representation holds

µ(R, α, ω) = (1 − ω)h2(cid:18)α − ω/2

1 − ω (cid:19) − h2(α) + 2h2(ω) + ω log

2ω
e − T (A, B, ω),

(82)

where

and

τ = h−1

2 (h2(α) − 1 + R) ≤ 1/2,

A = 1 − 2α,
T (A, B, ω) = ω log(v − 1) − (1 − ω) log

B = 1 − 2τ,

+B log

v + B

v − B − A log

v + A

v − A −

B > A ≥ 0,
v2 − A2
v2 − B2 +
(v − 1)(B2 − A2)
(v2 − B2) ln 2

,

v = pB2ω2 − 2a1ω + a2

ω

1 + a1

,

a1 = 2[α(1 − α) − τ (1 − τ )].

17

(83)

(84)

P r o o f. Using notations (83) and the variable z = 2y, we have from (25)

µ(R, α, ω) = h2(α) − ω − (1 − ω)h2(cid:18)α − ω/2
f1 = z2 − z + a1 +qB2z2 − 2a1z + a2

1 − ω (cid:19) −

1,

ω

Z0

log

f1
g1

dz,

g1 = (1 − z)2 − A2.

log g1 dz = −2h2(α) − 2(1 − ω) log(1 − ω) + 2(1 − ω)h2(cid:18)α − ω/2

1 − ω (cid:19) − 2ω log

e
2

.

Then

ω

Z0

We also have

ω

Z0

log f1 dz = F (a1, B, ω) − F (a1, B, 0),

where F (a1, B, z) – primitive function for log f1. In order to ﬁnd F (a1, B, z), we use
Euler’s substitution

After standard integration we have

(B2 − A2)2I1 =

(v − 1)(B2 − A2)

+

(B2 + A2)

2A

(v2 − B2)

ln

v + A
v − A

+ B ln

v − B
v + B

.

= ln

v2 − A2
v2 − B2 +
Since 2a1 = B2 − A2 and

z(v) + v − 1 =

(v − 1)(v2 − A2)

v2 − B2

,

18

and then

Now

where

qB2z2 − 2a1z + a2
2a1(v − 1)
v2 − B2 ,

z′ =

z =

1 = zv − a1,
2a1(2v − v2 − B2)

.

(v2 − B2)2
F (a1, B, z) ln 2 =Z ln f1 dz = z ln z − z+
v − A − 4a2
+z(v) ln[z(v) + v − 1] +

v + A

a1
A

ln

1I1,

I1 =Z

(2v − v2 − B2)
(v2 − A2)(v2 − B2)2 dv.

we get

F (a1, B, z) = z log z −
v + B

+B log

z
ln 2

v − B − A log

+ z log(v − 1) − (1 − z) log
(v − 1)(B2 − A2)
v − A −
(v2 − B2) ln 2

v + A

,

v2 − A2
v2 − B2 +

where v is deﬁned in (84). Since v → ∞ as u → 0 и F (a, b, 0) = 0, then
µ(R, α, ω) = −h2(α) − 2(1 − ω) log(1 − ω) − 2ω log e+

+(1 − ω)h2(cid:18)α − ω/2

1 − ω (cid:19) − F (a1, B, ω),

from which Proposition 3 follows.

(cid:3)

P r o o f o f

f o r m u l a (33). For ω ≤ G(1/2, τ ) we have [8, formula (П. 2)]

APPENDIX

µ(R, 1/2, ω) ln 2 = −2ω(1 − ln 2) − 2(1 − ω) ln(1 − ω) − I,

I =

2ω

Z0

lnhb +p(1 − z)2 + b2 − 1i dz,

b = 1 − 2τ > 0.

In order to ﬁnd the integral I, we use variable u =p(1 − z)2 + b2 − 1. Then denoting

A

b

A =p(1 − 2ω)2 + b2 − 1,

and using integration by parts we have

v1 =

,

√1 − b2

v2 =

,

√1 − b2

I = −

A

Zb

ln(b + u) d√u2 + 1 − b2 =
Zv1

v2

√1 + v2
v + v1

dv.

= ln(2b) − (1 − 2ω) ln(b + A) + √1 − b2

Since

then

√1 + z2
z + a

dz =

Z
= √1 + z2 − a lnhz + √1 + z2i −

√1 + a2 lnp(1 + a2)(1 + z2) − za + 1

z + a

,

I = ln(2b) − (1 − 2ω) ln(b + A) − 2ω − b ln

1 − 2ω + A

1 + b

− ln

b(2 − 2ω − b2 − bA)

(b + A)(1 − b2)

.

19

After standard algebra with g = (b + A)/2 we get the formula (33).
l e m m a 2. Consider a code C average distance

P r o o f o f

(cid:3)

dav (C) = M −2

M

M

Xi=1

Xj=1

dij.

Similarly to Plotkin’s bound derivation [11, теорема 2.2.1] we have

dav (C) ≤ 2t(1 − t)n,

0 ≤ t ≤ 1.

(85)

Using the assumption (38) we can also lower bound the value dav (C)

dav (C) ≥

1

M Xi≥ωn
Bi − X0<i<ωn

ωn

Bi =

M Xi≥ωn
ω(M − 1 − δM)n

M

iBi ≥
Bi) ≥

=

ωn

M (Xi>0

Comparing that estimate with (85) we get the inequality (40).

.

(cid:3)

P r o o f o f

f o r m u l a (75). We use the representation (82) and notations from

(83) and (84). Since

G(α, τ ) =

a1

1 + 2pτ (1 − τ )

B2G2 − 2a1G + a2

1 = 0

и

v =

,

a1 = 2[α(1 − α) − τ (1 − τ )],

a1
G

= 1 + 2pτ (1 − τ ).

then

Also

v − B = 2√τ v ,

v2 − B2 = 4pτ (1 − τ )h1 + 2pτ (1 − τ )i ,
v − A = 2hpτ (1 − τ ) + αi ,

(v − 1)(B2 − A2)

v + B = 2p(1 − τ )v,
v + A = 2h1 +pτ (1 − τ ) − αi .

v2 − B2

= G,

Therefore (ω = G(α, τ ))

T (A, B, G) =

ω
2

log [τ (1 − τ )] − (1 − ω) logh1 +pτ (1 − τ ) − αihpτ (1 − τ ) + αi

+

+

(1 − 2τ )

2

log

1 − τ

τ − (1 − 2α) log

pτ (1 − τ )h1 + 2pτ (1 − τ )i
1 +pτ (1 − τ ) − α
pτ (1 − τ ) + α − ω(cid:18) 1
ln 2 − 1(cid:19)

20

and

µ(R, α, G) = (1 − ω)h2(cid:18)α − ω/2

1 − ω (cid:19) + R − 1 + 2h2(ω) + ω log ω+

+(1 − ω) logh1 +pτ (1 − τ ) − αihpτ (1 − τ ) + αi

+

.

h1 + 2pτ (1 − τ )i
1 +pτ (1 − τ ) − α
pτ (1 − τ ) + α
= hpτ (1 − τ ) + αi2
1 + 2pτ (1 − τ )
= h1 +pτ (1 − τ ) − αi2
1 + 2pτ (1 − τ )

,

,

+(1 − 2α) log

Using in the last expression formulas

2α − G

2

1 − G −

and also formulas

2α − G

2

ah (b/a) = a ln a − b ln b − (a − b) ln(a − b),

2t1(ω) − ω = 2t2

1(ω),

we get the formula (75).

(cid:3)

REFERENCES

1. Burnashev M. V. Code spectrum and reliability function: binary symmetric

channel // Probl. Inform. Transm. 2006. V. 42. № 4. P. 3–22.

2. Elias P. Coding for noisy channels // IRE Conv. Rec. 1955. March, P. 37–46.
Reprinted in D. Slepian, Ed., Key papers in the development of information theory,
IEEE Press, 1974, P. 102–111.

3. Shannon C. E., Gallager R. G., Berlekamp E. R. Lower Bounds to Error

Probability for Codes on Discrete Memoryless Channels. I, II // Inform. and
Control. 1967. V. 10. № 1. P. 65–103; № 5. P. 522–552.

4. Gallager R.G. A Simple Derivation of the Coding Theorem and some Applications

// IEEE Trans. Inform. Theory. 1965. V. 11. P. 3–18.

5. McEliece R. J., Rodemich E. R., Rumsey H., Jr., Welch L. R. New Upper Bounds
on the Rate of a Code via the Delsarte–MacWilliams Inequalities // IEEE Trans.
Inform. Theory. 1977. V. 23. № 2. P. 157–166.

21

6. Levenstein V. I. On a straight line bound for the undetected error exponent //

Probl. Inform. Transm. 1989. V. 25. № 1. P. 33–37.

7. Barg A., McGregor A. Distance Distribution of Binary Codes and the Error

Probability of Decoding // IEEE Trans. Inform. Theory. 2005. V. 51. № 12. P.
4237–4246.

8. Burnashev M. V. Upper bound sharpening on reliability function of binary

symmetric channel // Probl. Inform. Transm. 2005. V. 41. № 4. P. 3–22.

9. Litsyn S. New Bounds on Error Exponents // IEEE Trans. Inform. Theory. 1999.

V. 45. No. 2. P. 385–398.

10. Burnashev M. V. On the relation between the code spectrum and the decoding

error probability // Probl. Inform. Transm. 2000. V. 36. № 4. P. 3–24.

11. MacWilliams F. J., Sloane N. J. A. The Theory of Error-Correcting Codes.

Amsterdam, The Netherlands: North Holland. 1977.

12. Cover T. M., Thomas J. A. Elements of Information Theory. New York: Wiley.

1991.

13. Levenshtein V. I. Bounds for packings of metric spaces and some applications //

Probl. Kibern. M.: Nauka, 1983. V. 40. P. 43–110.

14. Burnashev M. V. Supplement to the paper: Code spectrum and reliability function:
binary symmetric channel // Probl. Inform. Transm. 2007. V. 42. № 1. P. 28–31.

Burnashev Marat Valievich

Kharkevich Institute for Information Transmission Problems,
Russian Academy of Sciences, Moscow

burn@iitp.ru

22

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

 
4

C(p)

 

Rcrit(p)

R1(p)

R2(p)

5

6

7

8

9

10

11

12
x 10−3

Fig. 1. Plots of functions R1(p), R2(p), Rcrit(p) and C(p)

23

1.2

1.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

(19)

(22)

(20)

(23)

0

0,05

0,1

0,15

0,2

0,25

0,3

0,35

0,4

0,45

0,5

0,55

p

Fig. 2. Plots of functions Elow(R, p) and Eup(R, p) for p = 0, 01

24

