6
1
0
2

 
r
a

M
3

 

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
5
9
2
1
0

.

3
0
6
1
:
v
i
X
r
a

SimultaneousInferenceforHigh-dimensionalLinearModelsXianyangZhang∗andGuangCheng†TexasA&MUniversityandPurdueUniversityFebruary24,2016AbstractThispaperproposesabootstrap-assistedproceduretoconductsimultaneousinfer-enceforhighdimensionalsparselinearmodelsbasedontherecentde-sparsifyingLassoestimator(vandeGeeretal.2014).Ourprocedureallowsthedimensionoftheparametervectorofinteresttobeexponentiallylargerthansamplesize,anditautomaticallyaccountsforthedependencewithinthede-sparsifyingLassoestimator.Moreover,oursimultaneoustestingmethodcanbenaturallycoupledwiththemarginscreening(FanandLv2008)toenhanceitspowerinsparsetestingwithareducedcomputationalcost,orwiththestep-downmethod(RomanoandWolf2005)toprovideastrongcontrolforthefamily-wiseerrorrate.Intheory,weprovethatoursimultaneoustestingprocedureasymptoticallyachievesthepre-speciﬁedsigniﬁcancelevel,andenjoyscertainoptimalityintermsofitspowerevenwhenthemodelerrorsarenon-Gaussian.Ourgeneraltheoryisalsousefulinstudyingthesupportrecoveryproblem.Tobroadentheapplicability,wefurtherextendourmainresultstogeneralizedlinearmodelswithconvexlossfunctions.Theeﬀectivenessofourmethodsisdemonstratedviasimulationstudies.Keywords:De-sparsifyingLasso,Highdimensionallinearmodels,Multipletesting,MultiplierBootstrap,Simultaneousinference,Supportrecovery.∗AssistantProfessor,DepartmentofStatistics,TexasA&MUniversity,CollegeStation,TX77843.E-mail:zhangxiany@stat.tamu.edu.†AssociateProfessor,DepartmentofStatistics,PurdueUniversity,WestLafayette,IN47906.E-mail:chengg@purdue.edu.Tel:+1(765)496-9549.Fax:+1(765)494-0558.ResearchSponsoredbyNSFCAREERAwardDMS-1151692,DMS-1418042,SimonsFellowshipinMathematics,OﬃceofNavalResearch(ONRN00014-15-1-2331)andagrantfromIndianaClinicalandTranslationalSciencesInstitute.GuangChengwasonsabbaticalatPrincetonwhilepartofthisworkwascarriedout;hewouldliketothankthePrincetonORFEdepartmentforitshospitalityandsupport.11IntroductionHigh-dimensionalstatisticshasbecomeincreasinglypopularduetotherapiddevelopmentofinformationtechnologiesandtheirapplicationsinscientiﬁcexperiments.Thereisahugebodyofworkonsparseestimationofhigh-dimensionalmodels.Thestatisticalpropertiesoftheseprocedureshavebeenextensivelystudiedintheliterature;seeB¨uhlmannandvandeGeer(2011).Theresearcheﬀorthasrecentlyturnedtostatisticalinferencesuchasconstructingconﬁdenceintervalandhypothesistestingforregressioncoeﬃcientsinhigh-dimensionalsparsemodels.Forinstance,WassermanandRoeder(2009),andMeinshausenetal.(2009)proposedsigniﬁcancetestsforhigh-dimensionalregressioncoeﬃcientsbasedonsamplesplitting.Lockhartetal.(2014)derivedasigniﬁcancetestforvariablesalongtheLassosolutionpath.Leeetal.(2015)proposedanexactpost-selectioninferenceprocedurebasedontheideaofpolyhedralselection.AnotherresearchlineforconductinginferenceistoexploittheideaoflowdimensionalprojectionorinvertingtheKarush-Kuhn-Tuckercondition(seee.g.vandeGeeretal.2014,ZhangandZhang2014,JavanmardandMontanari2014,Bellonietal.2014).Inthehighdimensionalregime,groupingofvariablesandexploitinggroupstructureisquitenatural(YuanandLin2006,Meieretal.2008).Leadingexamplesincludemultifactoranalysis-of-varianceandadditivemodeling.Fromapracticalviewpoint,whenresearchinter-estconcernsnotonlyasinglevariablebutratheragroupofvariables,itseemsindispensabletogobeyondanapproachofinferringindividualregressioncoeﬃcients.Theproblemofconductingsimultaneousinferenceorinferenceforgroupsofvariablesinhigh-dimensionalmodelshasgainedsomerecentattention.Basedonthebasispursuitsolution,Meinshausen(2015)proposedaninterestingproceduretoconstructconﬁdenceintervalsforgroupsofvariableswithoutrestrictiveassumptionsonthedesign.MandozziandB¨uhlmann(2014)extendedthehierarchicaltestingmethodinMeinshausen(2008)tothehigh-dimensionalsetting,whichisabletodetectthesmallestgroupsofvariablesandasymptoticallycontrolthefamily-wiseerrorrate(FWER).2Inthispaper,weproposeabootstrap-assistedproceduretoconductsimultaneousinfer-enceinsparselinearmodelswith(possibly)non-Gaussianerrors:Y=Xβ0+ǫ,whereYisaresponse,Xisadesignmatrix,andβ0=(β01,...,β0p)Tisavectorofunknownregressioncoeﬃcients.Speciﬁcally,weconsiderthefollowingsimultaneoustesting:H0,G:β0j=eβjforallj∈G⊆{1,2,...,p}versusthealternativeHa,G:β0j6=eβjforsomej∈G,whereeβjwithj∈Gisavectorofpre-speciﬁedvalues(e.g.,bydomainexperts).Wepointoutthattwoextremecases,whereG={1,2,...,p}or|G|issmall,havebeenconsideredintherecentliterature,seee.g.,Arias-Castroetal.(2011),ZhongandChen(2011),Fengetal.(2013),ZhangandZhang(2014),andvandeGeeretal.(2014).However,relativelyfewattentionhasbeenpaidtothecasewhereGliesbetweenthesetwoextremes.OurmethodallowsGtobeanarbitrarysubsetof{1,2,...,p},andthuscanbeappliedtoawiderrangeofrealproblemssuchastestingthesigniﬁcanceofagrowingsetofgenes(associatedwithcertainclinicaloutcome)conditionalonanothersetofgenes,whosesizeisallowedtogrowaswell.IncomparisonwithMeinshausen(2015)andMandozziandB¨uhlmann(2014),ourmethodisasymptoticallyexactandreliesonsuitableassumptions(Assumptions2.1&2.2)onthedesignwhicharediﬀerentfromthehighlycorrelateddesignconsideredinMandozziandB¨uhlmann(2014).Ourgeneralframeworkisbuiltuponthede-sparsifyingLassoestimator,denotedas˘β=(˘β1,...,˘βp)T,recentlydevelopedinvandeGeeretal.(2014)andZhangandZhang(2014),whosereviewisgiveninSection2.1.InSection2.2,ateststatisticisproposedasTn,G:=maxj∈G√n|˘βj−eβj|,whosecriticalvaluesareobtainedviaasimplemultiplierbootstrapmethod.Basedontheasymptoticlinearexpansionof˘β,weshowthattheproposedmultiplierbootstrapmethodconsistentlyapproximatesthenulllimitingdistributionofTn,G,andthus3thetestingprocedureachievesthepre-speciﬁedsigniﬁcancelevelasymptotically.Itisworthmentioningthattheproposedbootstrap-assistedprocedureisadaptivetothedimensionofthecomponentofinterest,anditautomaticallyaccountsforthedependencewithinthede-sparsifyingLassoestimators.Intheory,wealsoprovethatourtestingprocedureenjoyscertainminimaxoptimality(Verzelen2012)intermsofitspowerevenwhenthemodelerrorsarenon-GaussianandthecardinalityofGisexponentiallylargerthansamplesize.Moreover,ournewmethodologyisreadilyapplicabletosomeotherimportantstatisticalproblemsinthehigh-dimensionalsetting,suchassupportrecovery,testingforsparsesignals,andmultipletesting.ThesupportrecoveryprocedureisproposedinSection3.1asanimportantby-productofourgeneraltheory.IthasbeenshownthroughsimulationsthattheproposedprocedurecanbemoreaccurateinrecoveringsignalsthanLasso,thestabilityselection(MeinshausenandB¨ulmann2010),andthescreenandcleanprocedure(WassermanandRoeder2009).Theabovebootstrap-assistedtestmethodcanalsobecoupledwiththemarginscreeninginFanandLv(2008)toenhancethepowerperformanceinsparsetestingwithareducedcomputationalcost,whichisveryattractiveintheultra-highdimensionalsetting.Hence,inSection3.2weproposeathree-stepprocedurethatﬁrstrandomlysplitsthesampleintotwosubsamples,screensouttheirrelevantvariablesbasedontheﬁrstsubsample,andﬁnallyperformstheabovemaximum-typetestingonthereducedmodelbasedonthesecondsubsample.Anotherapplicationisamultipletestingproblem:foreachj∈GH0,j:β0j≤eβjversusHa,j:β0j>eβj.ToobtainastrongcontroloftheFWER,weincorporatetheabovebootstrapideaintothestep-downmethod(RomanoandWolf2005)inSection3.3.AsnotedinChernozhukovetal.(2013),thishybridmethodisasymptoticallynon-conservativeascomparedtotheBonferroni-Holmproceduresincethecorrelationamongsttheteststatisticshasbeentakenintoaccount.4Tobroadentheapplicability,wefurtherextendourmainresultstogeneralizedlinearmodelswithconvexlossfunctionsinSection4.TheusefulnessoftheabovesimultaneousinferencemethodsisillustratedviasimulationstudiesinSection5.Thetechnicaldetailsandadditionalnumericalresultsareincludedinasupplementﬁle.SometheoreticalderivationsinthispaperrelyonanimpressiveGaussianapproximation(GAR)theoryrecentlydevelopedinChernozhukovetal.(2013).TheapplicationofGARtheoryisnontrivialasoneneedstoverifysuitablemomentconditionsontheleadingtermofthede-sparsifyingLassoestimator,andquantifytheestimationeﬀectaswellastheimpactoftheremaindertermin(7)below.WealsowanttopointoutthattheGARtheoryisappliedwithoutconditioningonthedesignasrandomdesignisconsideredthroughoutthepaper.OurresultscomplementBellonietal.(2014)whoestablishthevalidityofuniformconﬁdencebandinthehigh-dimensionalleastabsolutedeviationregression.Finally,weintroducesomenotation.Forap×pmatrixB=(bij)pi,j=1,let||B||∞=max1≤i,j≤p|bij|and||B||1=max1≤j≤pPpi=1|bij|.Denoteby||a||q=(Ppi=1|ai|q)1/qand||a||∞=max1≤j≤p|aj|fora=(a1,...,ap)T∈Rpandq>0.ForasetA,denoteitscardinalityby|A|.Denoteby⌊a⌋theintegerpartofapositiverealnumbera.Fortwosequences{an}and{bn},writean≍bnifthereexistpositiveconstantscandCsuchthatc≤liminfn(an/bn)≤limsupn(an/bn)≤C.Alsowritean.bnifan≤C′bnforsomeconstantC′>0independentofn(andp).ThesymbolNp(µ,Σ)isreservedforap-dimensionalmultivariatenormaldistributionwithmeanµandcovariancematrixΣ.2MainTheory2.1De-sparsifyingLassoestimatorInthissection,wereviewthede-sparsifying(de-biased)estimatorproposedinvandeGeeretal.(2014),whichisessentiallythesameastheestimatorproposedinZhangandZhang(2014)butmotivatedfromadiﬀerentviewpoint,i.e.,invertingtheKarush-Kuhn-5Tucker(KKT)conditionofLasso.Considerahigh-dimensionalsparselinearmodel:Y=Xβ0+ǫ,(1)witharesponseY=(Y1,...,Yn)T,ann×pdesignmatrixX:=[X1,...,Xp],anerrorǫ=(ǫ1,...,ǫn)TindependentofX,andanunknownp×1regressionvectorβ0=(β01,...,β0p)T.Theparameterdimensionpcanbemuchlargerthansamplesizen.SupposeXhasi.i.drowshavingmeanzeroandcovariancematrixΣ=(σij)pi,j=1withΣ−1:=Θ=(θij)pi,j=1.WedenotetheactivesetofvariablesbyS0={1≤j≤p:β0j6=0}anditscardinalitybys0=|S0|.TheLassoestimator(Tibshirani,1996)isdeﬁnedasbβ=argminβ∈Rp(||Y−Xβ||22/n+2λ||β||1),(2)forsometuningparameterλ>0.Thede-sparsifyingestimatorisobtainedbyinvertingtheKKTcondition,˘β=bβ+bΘXT(Y−Xbβ)/n,(3)wherebΘisasuitableapproximationfortheinverseoftheGrammatrixbΣ:=XTX/n.Inwhatfollows,weconsidertheapproximateinversebΘgivenbyLassoforthenodewiseregressiononthedesignmatrixX;seeMeinshausenandB¨uhlmann(2006).LetX−jbethedesignmatrixwithoutthejthcolumn.Forj=1,2,...,p,considerbγj:=argminγ∈Rp−1(||Xj−X−jγ||22/n+2λj||γ||1)(4)withλj>0,wherewedenotebγj={bγj,k:1≤k≤p,k6=j}.LetbC=(bci,j)pi,j=1beap×pmatrixwithbci,i=1andbci,j=−bγi,jfori6=j.Letbτ2j=||Xj−X−jbγj||22/n+λj||bγj||1andwritebT2=diag(bτ21,...,bτ2p)asadiagonalmatrix.Finally,thenodewiseLassoestimatorfor6ΘisconstructedasbΘ=bT−2bC.Denotebyγj=argminγ∈Rp−1E||Xj−X−jγ||22,anddeﬁneηj=Xj−X−jγj=(η1,j,...,ηn,j)T.Deﬁneτ2j=E||ηj||22/n=1/θj,jforj=1,2,...,p.Letsj=|{1≤k≤p:k6=j,θjk6=0}|.DenotethejthrowofXandbΘbyeXj=(Xj1,...,Xjp)TandbΘj,respectively.Assumption2.1.ThedesignmatrixXhaseitheri.i.dsub-Gaussianrows(i.e.,sup||a||2≤1Eexp{|Ppj=1ajXij|2/C}≤1forsomelargeenoughpositiveconstantC)ori.i.drowssatisfyingforsomeKn≥1,max1≤i≤n,1≤j≤p|Xij|≤Kn(stronglyboundedcase),whereKnisallowedtogrowwithn.Inthestronglyboundedcase,weassumeinadditionthatmaxj||X−jγj||∞≤KnandmaxjEη41,j≤K4n.Assumption2.2.ThesmallesteigenvalueΛ2minofΣsatisﬁesthatc<Λ2min,andmaxjΣj,j≤C,wherec,Caresomepositiveconstants.Theorem2.1(Theorem2.4ofvandeGeeretal.2014).SupposeAssumptions2.1-2.2hold.Assumeinthesub-Gaussiancase,itholdsthatmax1≤j≤ppsjlog(p)/n=o(1)andinthestronglyboundedcase,max1≤j≤pK2nsjplog(p)/n=o(1).Thenwithsuitablychosenλj≍K0plog(p)/nuniformlyforj=1,2,...,p,whereK0=1inthesub-GaussiancaseandK0=Kninthestronglyboundedcase,wehave||bΘj−Θj||1=OP(K0sjplog(p)/n),||bΘj−Θj||2=OP(K0qsjlog(p)/n),(5)|bτ2j−τ2j|=OP(K0qsjlog(p)/n),(6)uniformlyfor1≤j≤p.Furthermore,suppose{ǫi}arei.i.dwithc′<σ2ǫ<candinthesub-GaussiancaseforX,Eexp(|ǫi|/C)≤1forsomepositiveconstantsc,c′,C>0.Assumethatλissuitablychosensuchthatλ≍K0plog(p)/n,andK0s0log(p)/√n=o(1)andmax1≤j≤pK0sjplog(p)/n=o(1).Then√n(˘β−β0)=bΘXTǫ/√n+∆,||∆||∞=oP(1),(7)7where∆=(∆1,...,∆p)T=−√n(bΘbΣ−I)(bβ−β0).Theorem2.1providesanexplicitexpansionforthede-sparsifyingLassoestimatorandstatesthattheremainderterm∆canbewellcontrolledinthesensethat||∆||∞=oP(1),whichisveryusefulinthesubsequentderivation.2.2SimultaneousinferenceproceduresInthehighdimensionalregime,itisnaturaltotestthehypothesisH0,G:β0j=eβjforallj∈G⊆{1,2,...,p}versusthealternativeHa,G:β0j6=eβjforsomej∈G.Forexample,weconsiderthesparsetesting,i.e.,eβj=0,inSection3.2.Throughoutthepaper,weallow|G|togrowasfastasp,whichcanbeofanexponentialorderw.r.t.n.Hence,ourresultsgobeyondtheexistingonesinvandeGeeretal.(2014),ZhangandZhang(2014),andJavanmardandMontanari(2014),where|G|isﬁxed.SimultaneousinferencehasbeenconsideredearlierviaBonferroniadjustment,whichleadstoveryconservativeprocedures.Incontrast,ourmethodisasymptoticallynonconservative.Inthissection,weproposetheteststatisticmaxj∈G√n|˘βj−eβj|with˘βjbeingthede-sparsifyingestimator.AswillbeseeninSections3.2and3.3,thisteststatisticcanbenaturallycoupledwiththemarginscreening(FanandLv2008)toenhanceitspowerinsparsetestingandalsoreducethecomputationalcostinnodewiseLasso,orwiththestep-downmethod(RomanoandWolf2005)toprovideastrongcontrolfortheFWER.Wenextdescribeasimplemultiplierbootstrapmethodtoobtainanaccuratecriticalvalue.8Theasymptoticlinearexpansionin(7)canbere-writtenas(bΘXTǫ/√n)j=nXi=1bΘTjeXiǫi/√n=nXi=1bξij/√n,bξij=bΘTjeXiǫi,where(a)j=ajfora=(a1,...,ap)T.Generateasequenceofrandomvariables{ei}ni=1i.i.d.∼N(0,1)anddeﬁnethemultiplierbootstrapstatistic,WG=maxj∈GnXi=1bΘTjeXibσǫei/√n,wherebσ2ǫisaconsistentestimatoroftheerrorvarianceσ2ǫ,e.g.,thevarianceestimatorfromthescaledLasso(SunandZhang2012).ThebootstrapcriticalvalueisgivenbycG(α)=inf{t∈R:P(WG≤t|(Y,X))≥1−α}.Thevalidityoftheabovebootstrapmethodrequiresthefollowingtwoassumptions.Assumption2.3.(i)IfXhasi.i.dsub-Gaussianrows,assumethat(log(pn))7/n≤C1n−c1forsomeconstantsc1,C1>0.Inthiscase,suppose{ǫi}arei.i.dsub-Gaussianwithc′<σ2ǫ<cforc,c′>0.(ii)Ifmax1≤i≤n,1≤j≤p|Xij|≤Kn,assumethatmax1≤j≤psjK2n(log(pn))7/n≤C2n−c2forsomeconstantsc2,C2>0.Inthiscase,suppose{ǫi}arei.i.dsub-exponential,i.e.,Eexp(|ǫi|/C′)≤1andc′<σ2ǫ<cforsomeconstantsc,c′,C′>0.Assumption2.4.Thereexistsasequenceofpositivenumbersαn→+∞suchthatαn/p=o(1),αn(logp)2maxjλj√sj=o(1)andP(αn(logp)2|bσ2ǫ−σ2ǫ|>1)→0.Assumption2.3requiresthat(i)theregressorsanderrorsarebothsub-Gaussianor(ii)theregressorsarestronglyboundedwhiletheerrorsaresub-exponential.Italsoimposessuitablerestrictionsonthegrowthrateofp.Wepointoutthattheexistenceofafactor(log(p))2inAssumption2.4isduetoanapplicationofTheorem2.1inChernozhukovetal.(2014)regardingthecomparisonforthemaximaoftwoGaussianrandomvectors.Assumption2.4isaverymildtechnicalcondition.Forexampleif|bσ2ǫ−σ2ǫ|=OP(1/√n)andλj≍K0plog(p)/nuniformlyforallj,thenAssumption2.4holdsprovidedthatK0(logp)5/2maxjpsj/n=o(1).9Itisworthnotingthatthe√nconvergencerateforbσ2ǫcanbeachievedinthehighdimensionalsetting(e.g.bythescaledLassoinSunandZhang2011).RecallthatK0=1inthesub-GaussiancaseandK0=Kninthestronglyboundedcase.Theorem2.2belowestablishesthevalidityofthebootstrapprocedureforonesidedtest.Theorem2.2.SupposethatAssumptions2.1-2.4hold.Assumethatmax1≤j≤pK20s2j(log(pn))3(log(n))2/n=o(1),(8)K40s20(log(p))3/n=o(1),(9)andλandλjaresuitablychosensuchthatλ≍K0plog(p)/n,λj≍K0plog(p)/nuniformlyforj.(10)ThenwehaveforanyG⊆{1,2,...,p},supα∈(0,1)(cid:12)(cid:12)(cid:12)(cid:12)P(cid:18)maxj∈G√n(˘βj−β0j)>cG(α)(cid:19)−α(cid:12)(cid:12)(cid:12)(cid:12)=o(1).(11)Thescalingcondition(8)isimposedtocontroltheestimationeﬀectcausedbyreplacingΘwithitsnodewiseLassoestimatorbΘ,whilecondition(9)isneededtoboundtheremainderterm∆.Onecrucialfeatureofourbootstrap-assistedtestingprocedureisthatitexplicitlyaccountsfortheeﬀectof|G|inthesensethatthebootstrapcriticalvaluecG(α)dependsonG.ThisisinsharpcontrastwiththeextremevalueapproachinCaietal.(2014);seeRemark2.4.Hence,ourapproachismorerobusttothechangein|G|.Sincemaxj∈G√n|˘βj−eβj|=√nmaxj∈Gmax{˘βj−eβj,eβj−˘βj},similarargumentsimplythatunderH0,G,supα∈(0,1)(cid:12)(cid:12)(cid:12)(cid:12)P(cid:18)maxj∈G√n|˘βj−eβj|>c∗G(α)(cid:19)−α(cid:12)(cid:12)(cid:12)(cid:12)=o(1),wherec∗G(α)=inf{t∈R:P(W∗G≤t|(Y,X))≥1−α}withW∗G=10maxj∈G|Pni=1bΘTjeXibσǫei/√n|.Thisresultisreadilyapplicabletoconstructsimultaneousconﬁdenceintervalsforβ0jwithj∈G.Remark2.1.Alternatively,wecanemployEfron’sempiricalbootstraptoobtainthecriticalvalue.Forsimplicity,wetakeG={1,2,...,p}.Letbhi=(bhi1,...,bhip)Twithbhij=bΘTjeXibσǫ.Letbh∗1,...,bh∗nbeasamplefromtheempiricaldistributionbasedon{bhi}ni=1.Deﬁnetheem-piricalbootstrapstatisticasW∗EB=max1≤j≤p|Pni=1(bh∗ij−Pni=1bhij/n)/√n|.Theempiricalbootstrapcriticalvalueisthengivenbyc∗EB(α)=inf{t∈R:P(W∗EB≤t|(Y,X))≥1−α}.FollowingtheargumentsinAppendixKofChernozhukovetal.(2013)andtheproofofTheorem2.2,wecanestablishtheasymptoticequivalencebetweentheempiricalbootstrapandthemultiplierbootstrap,whichtheoreticallyjustiﬁestheuseoftheformer.Weomitthetechnicaldetailsheretoconservespace.LetbΞ=(bωij)pi,j=1=bΘbΣbΘTbσ2ǫ.Wenextconsiderthestudentizedstatisticmaxj∈G√n(˘βj−eβj)/pbωjjforonesidedtest.Inthiscase,thebootstrapcriticalvaluecanbeob-tainedvia¯cG(α)=inf{t∈R:P(¯WG≤t|(Y,X))≥1−α},where¯WG=maxj∈GPni=1bΘTjeXibσǫei/pnbωjj.Thefollowingtheoremjustiﬁesthevalidityofthebootstrapprocedureforthestudentizedstatistic.Theorem2.3.UndertheassumptionsinTheorem2.2,wehaveforanyG⊆{1,2,...,p},supα∈(0,1)(cid:12)(cid:12)(cid:12)(cid:12)P(cid:18)maxj∈G√n(˘βj−β0j)/pbωjj>¯cG(α)(cid:19)−α(cid:12)(cid:12)(cid:12)(cid:12)=o(1).(12)FollowingtheargumentsintheproofofTheorem2.3,itisstraightforwardtoshowthatunderH0,G,supα∈(0,1)(cid:12)(cid:12)(cid:12)(cid:12)P(cid:18)maxj∈G√n|˘βj−eβj|/pbωjj>¯c∗G(α)(cid:19)−α(cid:12)(cid:12)(cid:12)(cid:12)=o(1),where¯c∗G(α)=inf{t∈R:P(¯W∗G≤t|(Y,X))≥1−α}with¯W∗G=maxj∈G|Pni=1bΘTjeXibσǫei/pnbωjj|.ComparedtoTheorem2inZhangandZhang(2014),ourtwo-sidedtestingprocedureaboveisstraightforwardtoimplementandasymptoticallyexact.11Inparticular,theunknownquantileparameterin(29)ofZhangandZhang(2014)seemsnotdirectlyobtainable.OurprocedureisalsoasymptoticallynonconservativeascomparedtothemethodinMeinshausen(2015).Remark2.2.Ourinferentialproceduresallowtheprecisionmatrixtobesparse.Infact,theyworkforanyestimationmethodforprecisionmatrixaslongastheestimationeﬀectsuchas||bΘT−ΘT||1and||bΘbΣ−I||∞canbewellcontrolledundersuitableassumptions(seee.g.Caietal.2011,JavanmardandMontanari2014).Remark2.3.Analternativewaytoconductsimultaneousinferenceisbasedonthesumofsquarestypestatistics,e.g.,ChenandQin(2010),whichisexpectedtohavegoodpoweragainstnon-sparsealternatives.However,suchatypeofteststatisticsmaynotworkwellinthecurrentsettingduetotheaccumulationofestimationerrorsespeciallywhen|G|islarge,i.e.,theerrortermPj∈G|∆j|mightbeoutofcontrol.Wenextturntothe(asymptotic)poweranalysisoftheaboveprocedure.Notethatwhen|G|isﬁxed,ourprocedureisknowntobe√n-consistent(implicitlyimpliedbyTheorem2.1).Infact,evenwhen|G|→∞,ourteststillenjoyscertainoptimalityinthesensethattheseparationrate(√2+ε0)plog(|G|)/nforanyε0>0derivedinTheorem2.4belowisminimaxoptimalaccordingtoSection3.2ofVerzelen(2012)undersuitableassumptions.Belowwefocusonthecasewhere|G|→∞asn→∞.DeﬁnetheseparationsetUG(c0)={β=(β1,...,βp)T:maxj∈G|βj−eβj|/√ωjj>c0plog(|G|)/n},(13)whereωjj=σ2ǫθjj.RecallthatΣ−1=Θ=(θij)pi,j=1.LeteΘ=(eθij)witheθij=θij/pθiiθjj.Assumption2.5.Assumethatmax1≤i6=j≤p|eθij|≤c<1forsomeconstantc.Theorem2.4.UndertheassumptionsinTheorem2.3andAssumption2.5,wehaveforanyε0>0,infβ0∈UG(√2+ε0)P(cid:18)maxj∈G√n|˘βj−eβj|/pbωjj>¯c∗G(α)(cid:19)→1.(14)12Theorem2.4saysthatthecorrectrejectionofourbootstrap-assistedtestcanstillbetriggeredevenwhenthereexistsonlyoneentryofβ0−eβwithamagnitudebeinglargerthan(√2+ε0)plog(|G|)/ninG.Hence,ourprocedureisverysensitiveindetectingsparsealternatives.Aspointedoutbyonereferee,whenΣisanidentitymatrix,theconstant√2turnsouttobeasymptoticallyoptimalintheminimaxsense(seeArias-Castroetal.2011andIngsteretal.2010).Wealsonotethatourprocedureismorepowerfulindetectingsigniﬁcantvariableswhen|G|getssmallerinviewofthelowerboundin(13).ThisobservationpartlymotivatesustoconsiderthescreeningprocedureinSection3.2.Remark2.4.AnimportantbyproductoftheproofofTheorem2.4isthatthedistri-butionofmax1≤j≤p√n|˘βj−β0j|/pbωjjcanbewellapproximatedbymax1≤j≤p|Zj|withZ=(Z1,...,Zp)∼dN(0,eΘ).Therefore,wehaveforanyx∈Randasp→+∞,P(cid:18)max1≤j≤pn|˘βj−β0j|2/bωjj−2log(p)+loglog(p)≤x(cid:19)→exp(cid:26)−1√πexp(cid:16)−x2(cid:17)(cid:27).(15)Incontrastwithourmethod,theabovealternativetestingprocedurehastorequireptodiverge.Inaddition,thecriticalvalueobtainedfromtheabovetypeIextremevaluedistri-butionmaynotworkwellinpracticesincethisweakconvergenceistypicallyslow.Instead,itissuggestedtoemployan“intermediate”approximationtoimprovetherateofconvergenceintheliterature,e.g.,Liuetal.(2008).3ApplicationsThissectionisdevotedtothreeconcreteapplicationsofthegeneraltheoreticalresultsdevelopedinSection2.Speciﬁcally,weconsider(i)supportrecovery;(ii)testingforsparsesignals;(iii)multipletestingusingthestep-downmethod.133.1ApplicationI:SupportrecoveryThemajorgoalofthissectionistoidentifysignallocationsinapre-speciﬁedseteG,i.e.supportrecovery.Itturnsoutthatthissupportrecoveryproblemiscloselyrelatedtothere-sparsifyingprocedure1appliedtothede-sparsiﬁedestimatorconsideredinvandeGeer(2014)(seeLemma2.3therein).IncomparisonwithLasso,stabilityselection(MeinshausenandB¨uhlmann2010),andthescreenandcleanprocedure(WassermanandRoeder2009),simulationresultsinSection5.2illustratethatourprocedurebelowcanbemoreaccurateinrecoveringsignals.OursupportrecoveryprocedureisconcernedwithsettingaproperthresholdτinthefollowingsetbS0(τ)={j∈eG:|˘βj|>λ∗j(τ)},whereλ∗j(τ)=pτbωjjlog(p)/nandbωjj=bσ2ǫbΘTjbΣbΘj.WeconsiderthemostchallengingscenariowhereeG=[p]with[p]:={1,2,...,p}.Inproposition3.1,weshowthattheabovesupportrecoveryprocedureisconsistentifthethresholdvalueissetasτ∗=2,andfurtherjustifytheoptimalityofτ∗.Proposition3.1.UndertheassumptionsinTheorem2.3andAssumption2.5,wehaveinfβ0∈Ψ(2√2)P(bS0(2)=S0)→1,(16)whereΨ(c0)={β=(β1,...,βp)T:minj∈S0|βj|/√ωjj>c0plog(p)/n}.Moreover,wehaveforany0<τ<2,supβ0∈S∗(s0)P(bS0(τ)=S0)→0,(17)whereS∗(s0)={β=(β1,...,βp)T∈Rp:Ppj=1I{βj6=0}=s0}.1Thisre-sparsifyingprocedurehasthemeritsthatitcanimprovethel∞-boundsofLassoandhaslq-boundssimilartoLasso(undersparsityconditions).14AkeystepintheproofofProposition3.1is(15)inRemark2.4.3.2ApplicationII:TestingforsparsesignalsInthissubsection,wefocusonthetestingproblem,H0,eG:β0j=0foranyj∈eG⊆{1,2,...,p}.ToimprovetheeﬃciencyofthetestingprocedureandreducethecomputationalcostinthenodewiseLasso,weproposeathree-stepprocedurethatﬁrstrandomlysplitsthesampleintotwosubsamples,screensouttheirrelevantvariables(leadingtoareducedmodel)basedontheﬁrstsubsample,andthenperformssimultaneoustestinginSection2.2onthereducedmodelbasedonthesecondsubsample.Supposethepredictorsareproperlycenteredandstudentizedwithsamplemeanzeroandstandarddeviationone.Thethree-stepprocedureisformallydescribedasfollows:1.Randomsamplesplitting:Randomlysplitthesampleintotwosubsamples{(eXi,Yi)}i∈D1and{(eXi,Yi)}i∈D2,whereD1∪D2={1,2,...,n},|D1|=⌊c0n⌋and|D2|=n−⌊c0n⌋forsome0<c0<1.2.MarginalscreeningbasedonD1:LetXD1bethesubmatrixofXthatcontainstherowsinD1andletYD1=(Yi)i∈D1.ComputethecorrelationW=(w1,...,wp)T=XTD1YD1andconsiderthesubmodelSγ={1≤j≤p:|wj|>γ}withγbeingapositivenumbersuchthat|Sγ|=|D2|−1or|Sγ|=⌊|D2|/log(|D2|)⌋.3.TestingafterscreeningbasedonD2:UnderthereducedmodelSγ,computethede-sparsifyingLassoestimator{˘βj}j∈Sγandthevarianceestimatorbωjjbasedon{(eXi,Yi)}i∈D2.DeﬁneeGγ=eG∩Sγ.DenotebyTnst,γ=maxj∈eGγ√n|˘βj|andTst,γ=maxj∈eGγ√n|˘βj|/pbωjjthenon-studentizedandstudentizedteststatistics,re-spectively(ifeGγ=∅,wesimplysetTnst,γ=Tst,γ=0anddonotrejectH0,eG).Letc∗eGγ(α)and¯c∗eGγ(α)bethecorrespondingbootstrapcriticalvaluesatlevelα.RejectthenullhypothesisifTnst,γ>c∗eGγ(α)(Tst,γ>¯c∗eGγ(α)).15Intheabovethreestepprocedure,wehaveemployedthedata-splittingstrategyassug-gestedinWassermanandRoeder(2009)toreducetheTypeIerrorrateduetotheselectioneﬀect(theso-calledselectiveTypeIerrorrate).Undersuitableassumptionsthatruleoutunfaithfulness(smallpartialcorrelations),seee.g.,FanandLv(2008),wehaveS0⊆Sγwithanoverwhelmingprobability,whichjustiﬁesthevalidityofthesecondstep.OntheeventthatS0⊆Sγ,thevalidityandoptimalityoftheinferenceprocedurebasedonTnst,γandTst,γhavebeenestablishedinSection2.2.Fromapracticalviewpoint,thethree-stepprocedureenjoystwomajoradvantagesoverthesinglestepprocedure:(1)thenodewiseLassoinvolvesthecomputationofpLassoproblems,whichcanbecomputationallyintensiveespeciallywhenpisverylarge(e.g.pcouldbetensofthousandsingenomicstudies).Thethree-stepprocedurelessensthiscomputationburdenasthenodewiseLassoisnowperformedunderthereducedmodelSγwithamuchsmallersize;(2)Duetothescreeningstep,areducedmodeliscreatedwhichcouldleadtomoreeﬃcientinferenceinsomecases,seeSection5.3.ThiscanalsobeseenfromTheorem2.4thatourtestingprocedureismorepowerfulwhen|G|getssmaller.3.3ApplicationIII:MultipletestingwithstrongFWERcontrolWeareinterestedinthefollowingmultipletestingproblem:H0,j:β0j≤eβjversusHa,j:β0j>eβjforallj∈G.Forsimplicity,wesetG=[p].ToobtainastrongcontroloftheFWER,wecouplethebootstrap-assistedtestingprocedurewiththestep-downmethodproposedinRomanoandWolf(2005).OurmethodisaspecialcaseofageneralmethodologypresentedinSection5ofChernozhukovetal.(2013)bysettingbβthereinasthede-sparsifyingestimator˘β.AspointedoutinChernozhukovetal.(2013),ourmethodhastwoimportantfeatures:(i)itappliestomodelswithanincreasingdimension;(ii)itisasymptoticallynon-conservativeas16comparedtotheBonferroni-Holmproceduresincethecorrelationamongsttheteststatisticsistakenintoaccount.Infact,wewillcomparetheﬁnitesampleperformanceofourmethodwiththatoftheBonferroni-HolmprocedureinSection5.4.WealsowanttopointoutthatanyprocedurecontrollingtheFWERwillalsocontrolthefalsediscoveryrate(BenjaminandHochberg1995)whenthereexistsometruediscoveries.DenotebyΩthespaceforalldatageneratingprocesses,andω0bethetrueprocess.EachnullhypothesisH0,jisequivalenttoω0∈ΩjforsomeΩj⊆Ω.Foranyη⊆[p],denotebyΩη=(∩j∈ηΩj)∩(∩j/∈ηΩcj)withΩcj=Ω\Ωj.ThestrongcontroloftheFWERmeansthat,supη⊆[p]supω0∈ΩηPω0(rejectatleastonehypothesisH0,j,j∈η)≤α+o(1).(18)LetTj=√n(˘βj−eβj)anddenotebycη(α)thebootstrappedestimateforthe1−αquantileofmaxj∈ηTj.Thestep-downmethodinRomanoandWolf(2005)incontrollingtheFWERisdescribedasfollows.Letη(1)=[p]attheﬁrststep.RejectallhypothesesH0,jsuchthatTj>cη(1)(α).Ifnohypothesisisrejected,thenstop.Ifsomehypothesesarerejected,letη(2)bethesetofindicesforthosehypothesesnotbeingrejectedattheﬁrststep.Atstepl,letη(l)⊆[p]bethesubsetofhypothesisesthatwerenotrejectedatstepl−1.RejectallhypothesisesH0,j,j∈η(l)satisfyingthatTj>cη(l)(α).Ifnohypothesisisrejected,thenstop.Proceedinthiswayuntilthethealgorithmstops.AsshowninRomanoandWolf(2005),thestrongcontrolofthefamily-wiseerrorholdsprovidedthatcη(α)≤cη′(α),forη⊆η′,(19)supη⊆[p]supω0∈ΩηPω0(cid:18)maxj∈ηTj>cη(α)(cid:19)≤α+o(1).(20)Therefore,wecanshowthatthestep-downmethodtogetherwiththemultiplierbootstrapprovidestrongcontroloftheFWERbyverifying(19)and(20).TheargumentsaresimilartothoseintheproofofTheorem2.2;alsoseeTheorem5.1ofChernozhukovetal.(2013).17Proposition3.2.UndertheassumptionsinTheorem2.2,thestep-downprocedurewiththebootstrapcriticalvaluecη(α)satisﬁes(18).4GeneralizationInthissection,ourresultsareextendedbeyondthelinearmodelstoageneralframeworkwithaconvexlossfunctionandapenaltyfunction.Fory∈Y⊆Randx∈X⊆Rp,consideralossfunctionLβ(y,x)=L(y,xTβ)whichisstrictlyconvexinβ∈Rp.Theregularizedestimatorbasedonthepenaltyfunctionρλ(·)isdeﬁnedasbβ=argminβ∈Rp(EnLβ+pXj=1ρλ(|βj|)),(21)whereEng=Pni=1g(yi,xi)/nand{(yi,xi)}ni=1isasequenceofi.i.dobservations.Notethatourformulation(21)slightlygeneralizestheframeworkinSection3ofvandeGeeretal.(2014)byconsideringageneralpenaltyfunction.Again,wewanttotestthehypothesisH0,G:β0j=eβjforallj∈G⊆[p]versusH0,G:β0j6=eβjforsomej∈G.Ourteststatisticisgivenbymaxj∈G√n|˘βj−eβj|with˘βjbeingthede-sparsifyingLassoestimatordeﬁnedbelow.WeuseanalogousnotationasinSection2butwithsomemodiﬁcationsforthecurrentcontext.Forexample,denoteβ0astheuniqueminimizerofβ7→ELβ.Deﬁne˙Lβ(y,x)=∂∂βLβ(y,x)=x˙L(y,xTβ),¨Lβ(y,x)=∂2∂β∂βTLβ(y,x)=xxT¨L(y,xTβ),where˙L=∂L(y,a)/∂aand¨L=∂2L(y,a)/∂2a.DeﬁnebΣ=En¨LbβandletbΘ:=bΘ(bβ)beasuitableapproximationfortheinverseofbΣ(seemoredetailsinSection3.1.1ofvandeGeeretal.2014).Whenthepenalizedlossfunctionin(21)isconvexinβ,theKKTconditiongivesEn˙Lbβ+bκλ=0,(22)18wherebκλ=(bκ1,...,bκp)Twithbκj=sign(bβj)˙ρλ(|bβj|)ifbβj6=0andsome|bκj|≤|˙ρλ(0+)|ifbβj=0.ByTaylorexpansion,wehaveEn˙Lbβ=En˙Lβ0+En¨Lbβ(bβ−β0)+R,whereRistheremainderterm.PluggingbacktotheKKTcondition,weobtain,En˙Lβ0+bΣ(bβ−β0)+R=−bκλ,implyingthatbβ−β0+∆/√n+bΘbκλ=−bΘEn˙Lβ0−bΘR,where∆=√n(bΘbΣ−I)(bβ−β0).FollowingvandeGeeretal.(2014),wedeﬁnethede-sparsifying/de-biasedestimatoras˘β=bβ+bΘbκλ=bβ−bΘEn˙Lbβ.Note˘β−β0=−bΘEn˙Lβ0−bΘR−∆/√n.Withsomeabuseofnotation,deﬁneξij=−ΘTj˙Lβ0(yi,xi)=ΘTjxiǫiwithǫi=−˙L(yi,xTiβ0),andbξij=−bΘTj˙Lβ0(yi,xi).Toconductinference,weemploythemultiplierbootstrapinthefollowingway.Generateasequenceofi.i.dstandardnormalrandomvariables{ei}anddeﬁnethebootstrapstatistic,fW∗G=maxj∈G(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)nXi=1bΘTjxi˙L(yi,xTieβ)ei/√n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12),whereeβisasuitableestimatorforβ0.Thebootstrapcriticalvalueisgivenbyec∗G(α)=inf{t∈R:P(fW∗G≤t|{(yi,xi)}ni=1)≥1−α}.WeﬁrstimposeAssumptions4.1–4.4thataresimilartoAssumptions(C1)-(C5)invandeGeeretal.(2014).DenotebyXthedesignmatrixwiththeithrowequaltoxTi.Assumption4.1.Thederivatives˙L(y,a)and¨L(y,a)existforallyanda.Forsomeδ-neighborhood,maxa0∈{xTiβ0:xi∈X}sup|a−a0|∨|a′−a0|≤δsupy∈Y|¨L(y,a)−¨L(y,a′)||a−a′|≤1.19Assumption4.2.Assumemaxi,j|xij|≤Kn.Assumption4.3.Assumethat||bβ−β0||1=OP(s0λ)and||X(bβ−β0)||22/n=OP(s0λ2).Assumption4.4.Assumethat||En¨LbβbΘj−1j||∞=OP(λ∗)forsomeλ∗>0,and||XbΘj||∞=OP(Kn)uniformlyforj.Here,1jdenotesthevectorwiththej-thelementoneandotherszero.Moreover,wemakethefollowingadditionalassumptions.Inparticular,Assumptions4.5-4.6areparalleltoAssumptions2.3-2.4,whileAssumption4.7isatechnicalonemotivatedbytheresultsinTheorem3.2ofvandeGeeretal.(2014).Assumption4.5.Supposemax1≤j≤psjK2n(log(pn))7/n≤C2n−c2forsomeconstantsc2,C2>0.Suppose{ǫi}arei.i.dsub-exponentialthatisEexp(|ǫi|/C′)≤1forsomelargeenoughconstantsC′>0,andc′<σ2ǫ<cforc,c′>0.LeteΓ=max1≤j,k≤p|bΘTjbΣeβbΘk−ΘTjΣβ0Θk|,wherebΣeβ=XTWeβX/nwithWeβ=diag(˙L2(y1,xT1eβ),...,˙L2(yn,xTneβ)),andΣβ0=cov(xi˙L(yi,xTiβ0)).Assumption4.6.Thereexistsasequenceofpositivenumbersαn→+∞suchthatαn/p=o(1)andP(αn(logp)2eΓ>1)→0.Assumption4.7.Assumethatuniformlyforj,itholdsthat||bΘj(bβ)−Θj(β0)||1=OP(Knsjplog(p)/n)+OP(cid:16)K2ns0((λ2/plog(p)/n)∨λ)(cid:17).Wearenowinpositiontopresentthemainresultinthissectionwhichjustiﬁestheuseofthebootstrapprocedure.Theorem4.1.SupposeAssumptions4.1-4.7hold.Supposepnlog(p)Kns0λ2=o(1),pnlog(p)λλ∗s0=o(1),maxjsjKn(log(p))3/2/√n=o(1),plog(p)K2ns0(cid:16)λ2√n∨λplog(p)(cid:17)=o(1),K2nlog(p)(log(n))2/n=o(1).20ThenwehaveforanyG⊆{1,2,...,p},supα∈(0,1)(cid:12)(cid:12)(cid:12)(cid:12)P(cid:18)maxj∈G√n|˘βj−β0j|>ec∗G(α)(cid:19)−α(cid:12)(cid:12)(cid:12)(cid:12)=o(1).(23)Remark4.1.LetfW∗∗G=maxj∈G(cid:12)(cid:12)(cid:12)Pni=1bΘTjxi˙L(yi,xTieβ)ei/pnbwjj(cid:12)(cid:12)(cid:12),whereωjj=bΘTjbΣeβbΘjand{ei}isasequenceofi.i.dstandardnormalrandomvariablesandbwjj=bΘTjbΣeβbΘj.Deﬁnethebootstrapcriticalvalueec∗∗G(α)=inf{t∈R:P(fW∗∗G≤t|{(yi,xi)}ni=1)≥1−α}.FollowingtheargumentsintheproofsofTheorem2.3andTheorem4.1,weexpectasimilarresultforthestudentizedteststatisticmaxj∈G√n|(˘βj−β0j)/pbωjj|.Thetechnicaldetailsareomittedhere.5NumericalResultsInthissection,weconductsomesimulationstudiestoevaluatetheﬁnitesampleperfor-manceofthemethodsproposedinSection3.Alltheresultsareobtainedbasedonsamplesizen=100,and1,000MonteCarloreplications.ToobtainthemainLassoestimator,weimplementedthescaledLassowiththetuningparameterλ0=√2˜Ln(k0/p)with˜Ln(t)=n−1/2Φ−1(1−t),whereΦisthecumulativedistributionfunctionforN(0,1),andk0isthesolutiontok=˜L41(k/p)+2˜L21(k/p)(seeSunandZhang2013).Weestimatethenoiselevelσ2usingthemodiﬁedvarianceestimatordeﬁnedin(24)below.ThetuningparametersλjsinthenodewiseLassoarechosenvia10-foldcross-validationamongallnodewiseregressionsthroughoutthesimulationstudies.5.1SimultaneousconﬁdenceintervalsWeconsiderthelinearmodelswheretherowsofXareﬁxedi.i.drealizationsfromNp(0,Σ)withΣ=(Σi,j)pi,j=1undertwoscenarios:(i)Toeplitz:Σi,j=0.9|i−j|;(ii)Exchange-able/Compoundsymmetric:Σi,i=1andΣi,j=0.8fori6=j.Theactivesetis{1,2,...,s0},21wheres0=|S0|=3or15.ThecoeﬃcientsofthelinearregressionmodelsaregeneratedaccordingtoUnif[0,2](uniformdistributionon[0,2]),andtheerrorsaregeneratedfromthestudentizedt(4)distribution,i.e.,t(4)/√2,orcenteralizedandthestudentizedGamma(4,1)distributioni.e.,(Gamma(4,1)−4)/2.Inoursimulations,wefoundthatthescaledLassotendstounderestimatethenoiselevelwhichcouldleadtoundercoverage.Toovercomethisproblem,wesuggestthefollowingmodiﬁedvarianceestimator(seeasimilarestimatorinReidetal.2014),bσ2=1n−||bβsc||0||Y−Xbβsc||22,(24)wherebβscdenotesthescaledLassoestimatorwiththetuningparameterλ0.Figure??inthesupplementprovidesboxplotsofbσ/σforthevarianceestimatordeliveredbythescaledLasso(denotedby“SLasso”)andforthemodiﬁedvarianceestimatorin(24)(denotedby“SLasso∗”).Clearly,themodiﬁedvarianceestimatorcorrectsthenoiseunderestimationissue,andthusismorepreferable.InTables1-2,wepresentthecoverageprobabilitiesandintervalwidthsforthesimul-taneousconﬁdenceintervalsinthreediﬀerentcases:G=S0,Sc0or[p](asimilarsetupwasconsideredinvandeGeeretal.2014).Foreachsimulationrun,werecordwhetherthesimultaneousconﬁdenceintervalcontainsβ0jforj∈Gandthecorrespondingintervalwidth.Thecoverageprobabilitiesandintervalwidthsarethencalculatedbyaveragingover1,000simulationruns.Itisnotsurprisingthatthecoverageprobabilityisaﬀectedbythedi-mensionp,thetuningparametersλj(inthenodewiseLasso),thecardinalitys0,andthecovariancematrixΣ.Overall,thenon-studentizedmethodprovidessatisfactorycoverageprobability.However,themethodbasedontheextremevaluedistributionapproximationisinvalidwhenthedimensionofthecomponentsofinterestislow;seeRemark2.4.Toavoidsendingamisleadingmessage,wechoosenottoprovidetheresultsbasedontheextremevaluedistributionapproximationwhenG=S0.22Morespeciﬁcally,weobservefromTables1-2that:(i)thecoverageisingeneralmoreaccurateforSc0(ascomparedtoS0),eventhough|Sc0|≫|S0|=s0.SimilarﬁndingisfoundinvandeGeeretal.(2014)wherethecoverageforasinglecoeﬃcientinSc0ismoreaccurate;(ii)thenon-studentizedteststatistictendstoprovidebettercoverage(butwithlargerwidth)ascomparedtoitsstudentizedversionwhens0islarge;(iii)whens0=15,itbecomesdiﬃculttoprovideaccuratecoverageforalltheactivevariablesamongthecandidates.Inthiscase,thecoveragefortheactivesetcanbesigniﬁcantlylowerthanthenominallevel.Additionalnumericalresultsinthesupplement(seeFigure??)indicatethattheundercoverageinthiscaseiscloselyrelatedwiththeremaindertermwhosemaximumnormgenerallyincreaseswiths0;(iv)comparedtotheconﬁdenceintervalsforindividualcoeﬃcientsinvandeGeeretal.(2014),theintervalwidthsforsimultaneousintervalsarewider,whichreﬂectsthepricewepayformultiplicityadjustment.5.2SupportrecoveryBelowwecomparetheﬁnitesampleperformanceofthesupportrecoveryprocedurede-scribedinSection3.1withthoseofLasso,stabilityselection(MeinshausenandB¨uhlmann,2010),andthescreenandcleanprocedureinWassermanandRoeder(2009).ConsiderthesimulationsetupinSection5.1,wherethecoeﬃcientsarenowgeneratedfromUnif[2,4],andthesupportsetS0={u1,...,us0}withu1,...,us0beingarealizationofs0i.i.ddrawswithoutreplacementfrom{1,...,p}.Toassesstheperformance,weconsiderthefollowingsimilaritymeasured(bS0,S0)=|bS0∩S0|q|bS0|·|S0|.Intheimplementationofstabilityselection,wechoosethethresholdforselectionfrequencytobe0.6andtheupperboundfortheexpectednumberoffalsepositivestobe2.5;seeMeinshausenandB¨uhlmann(2010).Forthescreenandcleanprocedure,werandomlysplitthedataintotwogroups,conductscreeningontheﬁrsthalfofthedataandcleaningonthe23secondhalf.ThistwosplitsprocedurewasadvocatedinthesimulationsofWassermanandRoeder(2009).Table3summarizesthemeanandstandarddeviationofd(bS0,S0)aswellasthenumbersoffalsepositives(FP)andfalsenegatives(FN)basedon1,000simulationruns.Whens0=3,theproposedsupportrecoveryprocedureclearlyoutperformsLasso,anditiscomparabletostabilityselectionandthescreenandcleanprocedure.Whens0=15,therecoveryprocedureingeneraloutperformsallotherthreecompetitors.Wenotethatwhens0=15andΣisexchangeable,thestabilityselectiongivesahighnumberofFN,andthusresultsinalowvalueofd(bS0,S0).ThisisnotsurprisinggiventhatstabilityselectionismainlydesignedforconservativelycontrollingtheexpectednumberofFP.Theupperbound2.5setabovemightbetoosmallfors0=15.Thescreenandcleanproceduregenerallyperformswellforp=120,butitsperformancedeterioratesforp=500ands0=15.Overall,theproposedmethodperformsquitewellascomparedtosomeexistingalternatives.5.3TestingforsparsesignalsThissubsectionisdevotedtoempiricallyexaminethethree-steptestingprocedurepro-posedinSection3.2.Weconsiderthefollowingtwoscenarios:(i)ΣisToeplitzwithΣi,j=0.9|i−j|,andβ0j=p10log(p)/nfor1≤j≤s0andzerootherwise;(ii)Σisex-changeable,i.e.,Σi,i=1andΣi,j=0.8fori6=j,andβ0j=10plog(p)/nfor1≤j≤s0andzerootherwise.Toimplementthethree-steptestingprocedure,wechoosethesplittingproportionc0=1/5incase(i)andc0=1/3incase(ii).Forthemarginalscreeningstep,wepickγsuchthat|Sγ|=|D2|−1.AspointedoutinFanandLv(2008),themarginalscreeningmaynotperformverywellinthecaseofstrongpairwisecorrelation,i.e.,whenΣisexchangeable.Toovercomethisproblem,weproposethefollowingremedyinspiredbytheiterativesureindependencescreeninginFanandLv(2008).WeﬁrstselectasubsetB1ofk1variablesusingLassobasedonthesubsampleD1.Letbri=Yi−eXTibβbethecorrespondingresidualswithbβbeingtheLassoestimator,andeXci=(Xij)j/∈B1.Wethentreatthoseresidualsasthe24newresponsesandapplythemarginalscreeningasdescribedinStep2to{(eXci,bri)}i∈D1toselectasubsetB2of|D2|−1−k1variables.Finally,weletB=B1∪B2,whichcontains|D2|−1variables.Incase(ii)withs0=3,thisremedyselectsallthethreerelevantvariableswithprobability0.98whichismuchhigherthan0.59deliveredbythemarginalscreening.ThenumericalresultsarereportedinTable4,whichcomparestheperformancebetweenthethree-stepprocedureandtheone-stepprocedurewithoutsamplesplittingandmarginalscreening.Someremarksareinorderregardingthesimulationresults:(1)fortheToeplitzcovariancestructure,thethree-stepprocedurehasreasonablesizeforterrors,anditssizeisslightlyupwarddistortedforGammaerrors.Incontrast,theone-stepprocedurehasdown-wardsizedistortion;forexchangeablecovariancestructure,bothproceduresshowupwardsizedistortions;(2)thethree-stepproceduregenerateshigherpowerfortheToeplitzco-variancestructure.Wenotethattheempiricalpowersofbothproceduresarecloseto1incase(ii)duetothehighersignal-to-noiseratio.Overall,thethree-stepprocedurehasbetterpowerpropertyincase(i).5.4TestingwithFWERcontrolInthissection,wecomparetheﬁnitesampleperformanceofthestep-downmethodinSection3.3withtheBonferroni-HolmprocedurewhoseﬁnitesampleperformancehasbeenstudiedinvandeGeeretal.(2014).Tothisend,weconsidermultipletwosidedtestingofhypothesisH0,j:β0j=0amongallj=1,2,...,p.ThedataisagaingeneratedfromthelinearmodelconsideredinSection5.1.ForΣ,wefocusonthefollowingtwocases:(i)Toeplitz:Σi,j=0.9|i−j|;(ii)Blockdiagonal:Σi,i=1,Σi,j=0.9for5(k−1)+1≤i6=j≤5kwithk=1,2,...,⌊p/5⌋,andΣi,j=0otherwise.Weemployboththestep-downmethodbasedonthestudentized/non-studentizedteststatistic,andtheBonferroni-Holmprocedure(basedonthestudentizedteststatistic)tocontroltheFWER.Table5reportsboththeFWERandtheaveragepower,whichisdeﬁnedasPj∈S0I{H0,jisrejected}/s0basedon1,000simulateddatasets.Asseenfromthetable,thetwoproceduresprovidesimilarcontrolontheFWER.25Andthestep-downmethoddeliversslightlyhigheraveragepoweracrossallcasesconsideredhere.6ConclusionInthispaper,wehaveintroducedabootstrap-assistedproceduretoconductsimul-taneousinferenceforhigh-dimensionalcomponentsofalargeparametervectorinsparselinearmodels.Ourprocedureisprovedtoachievethepre-speciﬁedsigniﬁcancelevelasymptoticallyandtoenjoycertainoptimalityintermsofitspower.Ourgeneraltheoryhasbeensuccessfullyappliedtothreeconcreteexamples,namelysupportrecovery,testingforsparsesignals,andmultipletestingusingthestep-downmethod.Belowwepointoutafewfutureresearchdirections.Theﬁrstdirectionistheautomaticandeﬃcientselectionofthetuningparameters(e.g.λjsinthenodewiseLasso)inthecontextofhypothesistestingandconﬁdenceintervalconstruction(seee.g.AppendixA.1ofDezeureetal.2014).Theseconddirectionistoadapttheproposedmethodtoconductinferenceonhighdimensionalconcentrationmatrix(seee.g.JankovaandvandeGeer2014).Finally,itisalsointerestingtoextendourresultstomorecomplicatedmodelse.g.Coxmodel,aftersometechnicalmodiﬁcations.AcknowledgmentsTheauthorswouldliketothanktheEditor,AssociateEditorandreviewersfortheirconstructivecommentsandhelpfulsuggestions,whichsubstantiallyimprovedthepaper.References[1]Arias-Castro,E.,Cand`es,E.J.andPlan,Y.(2011).Globaltestingundersparsealternatives:ANOVA,multiplecomparisonsandthehighercriticism.Ann.Statist.392533-2556.26[2]Belloni,A.,Chernozhukov,V.andKato,K.(2014).UniformpostselectioninferenceforLADregressionandotherZ-estimationproblems.BiometrikaToappear.[3]Belloni,A.,Chernozhukov,V.andWang,L.(2011).Square-rootlasso:Pivotalrecoveryofsparsesignalsviaconicprogramming.Biometrika98791-806.[4]Benjamin,Y.andHochberg,Y.(1995).Controllingthefalsediscoveryrate:apracticalandpowerfulapproachtomultipletesting.J.R.Stat.Soc.Ser.B57289-300.[5]B¨uhlmann,P.andvandeGeer,S.(2011).StatisticsforHigh-DimensionalData:Methods,TheoryandApplications.Springer.[6]Cai,T.,Liu,W.andLuo,X.(2011).Aconstrainedℓ1minimizationapproachtosparseprecisionmatrixestimation.J.Amer.Statist.Assoc.106594-607.[7]Cai,T.T.,Liu,W.D.andXia,Y.(2014).Two-sampletestofhighdimensionalmeansunderdependence.J.R.Stat.Soc.Ser.B76349-372.[8]Chen,S.X.andQin,Y.-L.(2010).Atwosampletestforhighdimensionaldatawithapplicationstogene-settesting.Ann.Statist.38808-835.[9]Chernozhukov,V.,Chetverikov,D.andKato,K.(2013).Gaussianapproxima-tionsandmultiplierbootstrapformaximaofsumsofhigh-dimensionalrandomvectors.Ann.Statist.412786-2819.[10]Chernozhukov,V.,Chetverikov,D.andKato,K.(2014).Comparisonandanti-concentrationboundsformaximaofGaussianrandomvectors.Probab.TheoryRelatedFields.Toappear.[11]Dezeure,R.,B¨uhlmann,P.Meier,L.andMeinshausen,N.(2014).High-dimensionalInference:Conﬁdenceintervals,p-valuesandR-Softwarehdi.arXiv:1408.4026v1.27[12]Fan,J.andLv,J.(2008).Sureindependencescreeningforultra-highdimensionalfeaturespace.J.R.Stat.Soc.Ser.B70849-911.[13]Feng,L.,Zou,C.,Wang,Z.andChen,B.(2013).Rank-basedscoretestsforhigh-dimensionalregressioncoeﬀcients.Electron.J.Stat.72131-2149.[14]Ingster,Y.I.,Tsybakov,A.B.andVerzelen,N.(2010).Detectionboundaryinsparseregression.Electron.J.Stat.41476-1526.[15]Jankova,J.andvandeGeer,S.(2014).Conﬁdenceintervalsforhigh-dimensionalinversecovarianceestimation.arXiv:1403.6752.[16]Javanmard,A.andMontanari,A.(2014).Conﬁdenceintervalsandhypothesistestingforhigh-dimensionalregression.arXiv:1306.3171.[17]Lee,J.D.,Sun,D.L.,Sun,Y.andTaylor,J.E.(2015).Exactpost-selectioninferencewiththelasso.Ann.Statist.Toappear.[18]Liu,W.-D.,Lin,Z.andShao,Q.-M.(2008).TheasymptoticdistributionandBerryEsseenboundofanewtestforindependenceinhighdimensionwithanapplicationtostochasticoptimization.Ann.Appl.Probab.182337-2366.[19]Lockhart,R.,Taylor,J.,Tibshirani,R.andTibshirani,R.(2014).Asignif-icancetestforthelasso.Ann.Statist.42413-468.[20]Mandozzi,J.andB¨uhlmann,P.(2014).Hierarchicaltestinginthehigh-dimensionalsettingwithcorrelatedvariables.arXiv:1312.5556.[21]Meier,L.,VanDeGeer,S.andBohlmann,P.(2008).TheGroupLassoforlogisticregression.J.R.Stat.Soc.Ser.B7053-71.[22]Meinshausen,N.(2008).Hierarchicaltestingofvariableimportance.Biometrika95265-278.28[23]Meinshausen,N.(2015).Group-bound:conﬁdenceintervalsforgroupsofvariablesinsparsehigh-dimensionalregressionwithoutassumptionsonthedesign.J.R.Stat.Soc.Ser.BToappear.[24]Meinshausen,N.andB¨uhlmann,P.(2006).High-dimensionalgraphsandvariableselectionwiththeLasso.Ann.Statist.341436-1462.[25]Meinshausen,N.andB¨uhlmann,P.(2010).Stabilityselection.J.R.Stat.Soc.Ser.B72417-473.[26]Meinshausen,N.,Meier,L.andB¨uhlmann,P.(2009).P-valuesforhigh-dimensionalregression.J.Amer.Statist.Assoc.1041671-1681.[27]Reid,S.,Tibshirani,R.andFriedman,J.(2014).Astudyoferrorvarianceestimationinlassoregression.arXiv:1311.5274.[28]Romano,J.andWolf,M.(2005).Exactandapproximatestep-downmethodsformultiplehypothesistesting.J.Amer.Stat.Assoc.10094-108.[29]Sun,T.andZhang,C.H.(2012).Scaledsparselinearregression.Biometrika99879-898.[30]Sun,T.andZhang,C.H.(2013).Sparsematrixinversionwithscaledlasso.J.MachineLearningResearch143385-3418.[31]Tibshirani,R.(1996).RegressionshrinkageandselectionviatheLasso.J.R.Stat.Soc.Ser.B58267-288.[32]vandeGeer,S.(2014).Worstpossiblesub-directionsinhigh-dimensionalmodels.arXiv:1403.7023.[33]vandeGeer,S.(2015).χ2-conﬁdencesetsinhigh-dimensionalregression.arXiv:1502.07131.29[34]vandeGeer,S.,B¨uhlmann,P.,Ritov,Y.andDezeure,R.(2014).Onasymp-toticallyoptimalconﬁdenceregionsandtestsforhighdimensionalmodels.Ann.Statist.421166-1202.[35]vanderVaart,A.W.andWellner,J.A.(1996).Weakconvergenceandempir-icalprocesses.SpringerVerlag,NewYork.[36]Verzelen,N.(2012).Minimaxrisksforsparseregressions:ultra-highdimensionalphenomenons.Electron.J.Stat.638-90.[37]Wasserman,L.andRoeder,K.(2009).Highdimensionalvariableselection.Ann.Statist.372178-2201.[38]Yuan,M.andLin,Y.(2006).Modelselectionandestimationinregressionwithgroupedvariables.J.R.Statist.Soc.B6849-67.[39]Zhang,C.H.andZhang,S.S.(2014).Conﬁdenceintervalsforlow-dimensionalparameterswithhigh-dimensionaldata.J.R.Stat.Soc.Ser.B76217-242.[40]Zhong,P.S.andChen,S.X.(2011).Testingforhigh-dimensionalregressioncoef-ﬁcientswithfactorialdesigns.J.Amer.Statist.Assoc.106260-274.30Table1:Coverageprobabilitiesandintervalwidthsforthesimultaneousconﬁdenceinter-valsbasedonthenon-studentized(“NST”)andstudentized(“ST”)teststatistics,whereS0={1,2,3}andp=120,500.Therow“EX”correspondstothecoveragebasedonthestudentizedteststatisticwiththeTypeIextremedistributionapproximation.“Cov”and“Len”denotethecoverageprobabilityandintervalwidthrespectively.Case(i)((ii))correspondstoToeplitzmatrix(exchangeablematrix).S0,(i)Sc0,(i)[p],(i)S0,(ii)Sc0,(ii)[p],(ii)95%99%95%99%95%99%95%99%95%99%95%99%p=120t(4)/√2NSTcvCov0.820.940.970.990.950.990.910.970.930.980.930.98Len0.991.221.491.671.501.670.971.181.491.671.491.67STcvCov0.820.930.970.990.960.990.920.970.920.980.920.98Len0.971.191.481.641.481.640.961.181.461.621.461.63EXcvCovNANA0.981.000.970.99NANA0.940.980.930.98LenNANA1.511.691.511.69NANA1.491.661.491.67GammaNSTcvCov0.840.930.960.990.940.980.910.970.930.980.930.98Len0.991.221.501.671.501.670.971.181.501.681.501.68STcvCov0.820.920.970.990.950.980.900.970.920.980.920.98Len0.971.191.481.651.481.650.971.181.461.631.461.63EXcvCovNANA0.970.990.960.99NANA0.930.990.930.99LenNANA1.511.691.511.69NANA1.491.671.491.67p=500t(4)/√2NSTcvCov0.760.900.960.990.940.980.920.980.920.970.920.97Len0.891.091.471.621.471.620.971.191.651.821.651.82STcvCov0.770.900.970.990.950.980.920.970.920.970.910.97Len0.881.081.461.601.461.600.971.181.621.771.621.77EXcvCovNANA0.980.990.960.98NANA0.920.970.920.97LenNANA1.481.631.481.63NANA1.641.811.641.81GammaNSTcvCov0.770.900.980.990.960.980.910.970.950.980.940.98Len0.901.101.491.631.491.640.981.191.661.831.661.83STcvCov0.770.900.980.990.960.980.900.970.930.970.930.97Len0.891.091.471.611.471.610.971.191.631.781.631.78EXcvCovNANA0.981.000.960.99NANA0.940.980.940.98LenNANA1.491.641.491.64NANA1.651.821.651.82Note:ThetuningparametersλjsinthenodewiseLassoarechosentobethesamevia10-foldcross-validationamongallnodewiseregressionsforNSTcv,STcv,andEXcv.t(4)/√2andGammadenotethestudentizedt(4)distributionandthecentralizedandstudentizedGamma(4,1)distributionrespectively.Thecoverageprobabilitiesandintervalwidthsarecomputedbasedon1,000simulationruns.Forthestudentizedtest,wereporttheaverageintervalwidthsoverdiﬀerentcomponents.31Table2:Coverageprobabilitiesandintervalwidthsforthesimultaneousconﬁdenceintervalsbasedonthenon-studentized(“NST”)andstudentized(“ST”)teststatistics,whereS0={1,2,3,...,15}andp=120,500.Therow“EX”correspondstothecoveragebasedonthestudentizedteststatisticwiththeTypeIextremedistributionapproximation.“Cov”and“Len”denotethecoverageprobabilityandintervalwidthrespectively.Case(i)((ii))correspondstoToeplitzmatrix(exchangeablematrix).S0,(i)Sc0,(i)[p],(i)S0,(ii)Sc0,(ii)[p],(ii)95%99%95%99%95%99%95%99%95%99%95%99%p=120t(4)/√2NSTcvCov0.680.870.991.000.870.950.730.880.920.980.890.96Len1.441.731.681.921.721.971.271.481.671.901.671.91STcvCov0.500.700.991.000.750.850.680.840.830.920.740.88Len1.301.501.551.731.561.741.231.421.531.701.531.71EXcvCovNANA0.991.000.770.88NANA0.850.940.770.92LenNANA1.581.771.591.78NANA1.551.741.561.75GammaNSTcvCov0.690.880.991.000.860.950.770.900.930.980.900.97Len1.441.741.691.931.721.971.271.491.681.911.681.92STcvCov0.520.700.991.000.740.860.710.870.850.940.780.91Len1.301.511.551.731.571.751.241.431.531.711.541.71EXcvCovNANA0.991.000.770.88NANA0.870.960.810.94LenNANA1.591.781.601.79NANA1.561.751.571.76p=500t(4)/√2NSTcvCov0.560.820.991.000.860.950.360.570.930.980.810.94Len1.351.651.741.951.751.971.321.552.012.382.012.38STcvCov0.340.570.991.000.760.860.350.550.610.770.490.66Len1.221.401.581.731.591.741.281.481.711.871.711.87EXcvCovNANA0.991.000.780.87NANA0.650.810.530.70LenNANA1.611.771.611.77NANA1.731.901.731.91GammaNSTcvCov0.520.801.001.000.860.950.380.570.940.990.830.96Len1.361.661.751.961.761.981.341.572.042.422.042.42STcvCov0.320.520.991.000.740.850.340.530.600.770.470.66Len1.221.411.591.751.601.751.291.501.731.901.731.90EXcvCovNANA0.991.000.760.87NANA0.640.800.500.69LenNANA1.611.781.621.78NANA1.751.931.761.93Note:ThetuningparametersλjsinthenodewiseLassoarechosenvia10-foldcross-validationamongallnodewiseregressionsforNSTcv,STcv,andEXcv.t(4)/√2andGammadenotethestudentizedt(4)distributionandthecentralizedandstudentizedGamma(4,1)distributionrespectively.Thecoverageprobabilitiesandintervalwidthsarecomputedbasedon1,000simulationruns.Forthestudentizedtest,wereporttheaverageintervalwidthsoverdiﬀerentcomponents.32Table3:Themeanandstandarddeviation(SD)ofd(bS0,S0),andthenumbersoffalsepositives(FP)andfalsenegatives(FN).Case(i)((ii))correspondstoToeplitzmatrix(ex-changeablematrix).s0=3s0=15MeanSDFPFNMeanSDFPFNp=120,(i)t(4)/√2SupRec0.980.050.160.000.980.030.680.02Stability0.930.080.520.000.350.070.6312.64Lassosc0.680.103.940.000.720.0413.870.00S&C0.990.040.040.010.870.150.183.14GammaSupRec0.970.060.200.000.980.030.710.00Stability0.930.080.580.000.360.070.6112.63Lassosc0.680.113.980.000.720.0413.940.00S&C0.990.030.030.010.870.150.143.30p=120,(ii)t(4)/√2SupRec0.970.060.240.000.980.020.550.01Stability0.970.060.260.000.000.000.1315.00Lassosc0.560.097.090.000.650.0420.240.00S&C0.990.040.040.010.800.240.164.41GammaSupRec0.970.060.230.000.980.020.600.00Stability0.960.060.270.000.000.000.1315.00Lassosc0.560.097.070.000.650.0420.200.00S&C0.990.030.040.000.810.220.244.40p=500,(i)t(4)/√2SupRec0.970.060.200.000.530.040.1410.66Stability0.840.101.320.000.360.031.7411.97Lassosc0.620.095.240.000.380.0319.257.37S&C0.960.130.360.120.120.1718.2614.04GammaSupRec0.970.060.180.000.530.040.1510.68Stability0.860.091.240.000.360.031.7511.93Lassosc0.620.095.380.000.380.0319.157.36S&C0.960.140.550.140.560.0818.2214.00p=500,(ii)t(4)/√2SupRec0.970.060.250.000.960.041.380.10Stability0.960.070.340.000.080.100.8614.59Lassosc0.410.0515.160.000.500.0245.640.00S&C0.980.080.050.050.040.1128.5114.61GammaSupRec0.970.070.270.000.960.041.430.04Stability0.950.070.360.000.080.100.8614.58Lassosc0.410.0515.180.000.500.0245.410.00S&C0.980.060.050.050.050.1127.5414.62Note:ThetuningparametersλjsinthenodewiseLassoarechosentobethesamevia10-foldcross-validationamongallnodewiseregressions.Thesubscript“sc”standsforthescaledLasso.t(4)/√2andGammadenotethestudentizedt(4)distributionandthecentralizedandstudentizedGamma(4,1)distributionrespectively.S&Cdenotesthescreenandcleanprocedure.Themean,SD,FPandFNarecomputedbasedon1,000simulationruns.33Table4:Empiricalsizes(upperpanel)andpowers(lowerpanel)basedontheone-step(withoutsamplesplittingandscreening)andthree-stepprocedureswiththenon-studentized(“NST”)andthestudentized(“ST”)statistics,whereS0={1,2,3}andeS0={1,2,3,...,15},andp=500.Case(i)((ii))correspondstoToeplitzmatrix(ex-changeablematrix).Thenominallevelsare5%and1%.One-StepProcedureThree-StepProceduret(4)/√2Gammat(4)/√2GammaαNSTSTNSTSTNSTSTNSTST(i),Sc05%0.030.030.020.030.060.060.070.071%0.010.000.000.010.010.010.030.02(i),eSc05%0.020.010.020.020.050.040.060.051%0.010.000.010.010.020.020.030.02(ii),Sc05%0.090.080.080.080.100.110.090.101%0.020.030.030.030.040.040.030.03(i),{3}∪Sc05%0.660.670.610.620.740.720.730.721%0.540.560.490.510.620.600.630.61(i),{2,3}∪Sc05%0.890.880.880.870.930.920.940.931%0.800.800.790.780.870.850.870.85(i),{15}∪eSc05%0.630.640.580.590.730.720.700.681%0.530.540.480.500.630.610.600.57(i),{14,15}∪eSc05%0.860.860.820.820.930.920.930.921%0.760.760.710.720.860.850.870.85(ii),{3}∪Sc05%1.001.001.001.000.990.991.001.001%1.001.001.001.000.990.991.001.00(ii),{2,3}∪Sc05%1.001.001.001.001.001.001.001.001%1.001.001.001.001.001.001.001.00Note:ThetuningparametersλjsinthenodewiseLassoarechosentobethesamevia10-foldcross-validationamongallnodewiseregressions.t(4)/√2andGammadenotethestudentizedt(4)distributionandthecentralizedandstudentizedGamma(4,1)distributionrespectively.Thesizesandpowersarecomputedbasedon1,000simulationruns.34Table5:FWERandpowerofmultipletestingbasedonthestep-downmethodwiththenon-studentized(“NST”)andstudentized(“ST”)teststatistics,andbasedontheBonferroni-Holmprocedure(“BH”),wherep=500,andthenominallevelis5%.Case(i)((ii))corre-spondstoToeplitzmatrix(blockdiagonalmatrix).s0=3,(i)s0=15,(i)s0=3,(ii)s0=15,(ii)FWERPowerFWERPowerFWERPowerFWERPowert(4)/√2NST0.0370.5480.0080.7320.0460.5940.0050.701ST0.0280.5340.0090.7220.0480.5600.0050.685BH0.0240.5280.0060.7170.0400.5550.0040.678GammaNST0.0340.5350.0140.7250.0390.5810.0010.701ST0.0330.5130.0160.7140.0460.5490.0040.685BH0.0230.5060.0110.7080.0380.5450.0040.680Note:ThetuningparametersλjsinthenodewiseLassoarechosentobethesamevia10-foldcross-validationamongallnodewiseregressions.t(4)/√2andGammadenotethestudentizedt(4)distributionandthecentralizedandstudentizedGamma(4,1)distributionrespectively.TheFWERandpowersarecomputedbasedon1,000simulationruns.35Supplementto“SimultaneousInferenceforHigh-dimensionalLinearModels”XianyangZhang∗andGuangCheng†TexasA&MUniversityandPurdueUniversityFebruary24,2016Thissupplementarymaterialprovidesproofsofthemainresultsinthepaperaswellassomeadditionalnumericalresults.1TechnicaldetailsWeﬁrstpresenttwolemmasthatwillbeusedintherestproofs.Deﬁneξij=ΘTjeXiǫi.Denotebyc,c′,C,C′,Cibesomegenericconstantswhichcanbediﬀerentfromlinetoline.Lemma1.1.UnderAssumptions2.1-2.3,wehaveforanyG⊆{1,2,...,p},supx∈R(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)P maxj∈GnXi=1ξij/√n≤x!−P maxj∈GnXi=1zij/√n≤x!(cid:12)(cid:12)(cid:12)(cid:12)(cid:12).n−c′,c′>0,where{zi=(zi1,...,zip)′}isasequenceofmeanzeroindependentGaussianvectorwithEziz′i=ΘTjΣΘjσ2ǫ.∗AssistantProfessor,DepartmentofStatistics,TexasA&MUniversity,CollegeStation,TX77843.E-mail:zhangxiany@stat.tamu.edu.†AssociateProfessor,DepartmentofStatistics,PurdueUniversity,WestLafayette,IN47906.E-mail:chengg@purdue.edu.Tel:+1(765)496-9549.Fax:+1(765)494-0558.ResearchSponsoredbyNSFCAREERAwardDMS-1151692,DMS-1418042,SimonsFellowshipinMathematics,OﬃceofNavalResearch(ONRN00014-15-1-2331)andagrantfromIndianaClinicalandTranslationalSciencesInstitute.GuangChengwasonsabbaticalatPrincetonwhilepartofthisworkwascarriedout;hewouldliketothankthePrincetonORFEdepartmentforitshospitalityandsupport.1ProofofLemma1.1.WeapplyCorollary2.1ofChernozhukovetal.(2013)tothesequence{ξij}byverifyingitsCondition(E.1).Forthesakeofclarity,westatetheconditionbelow,i.e.c0≤Eξ2ij≤C0,maxk=1,2E|ξij|2+k/Bk+Eexp(|ξij|/B)≤4,(1)uniformlyoverj,wherec0,C0>0,andBissomelargeenoughconstant.Inwhatfollows,weconsidertwocasesforX:(i)Xhasi.i.d.sub-Gaussianrows;(ii)Xisstronglybounded.(i)ByAssumption2.2,E(ΘTjeXi)2=ΘTjΣΘj=θjj:=1/τ2j,and1/c<Λ2min≤τ2j≤Σj,j=C,forsomeconstantsc,C>0.RecallthatΛ2ministheminimaleigenvalueofΣ.Thuswehavec1σ2ǫ≤Eξ2ij≤C1σ2ǫ.Bytheindependencebetween{eXi}and{ǫi},wehaveforlargeenoughCanduniformlyforallj,Eexp(|ξij|/C)=1++∞Xk=1E|ξij|kCkk!=1++∞Xk=1E|ΘTjeXi|kE|ǫi|kCkk!≤1++∞Xk=1kk(C′)kk!≤1++∞Xk=1(e/C′)k<∞,wherewehaveusedthefactthatk!≥(k/e)k,||Θj||2.Λ−1min=O(1)(because||Θj||22Λ2min≤c)andE|X|k≤(C′′)kkk/2withC′′beingsomepositiveconstantforsub-GaussianvariableX.Thuswehavemaxk=1,2E|ξij|2+k/Bk+Eexp(|ξij|/B)≤4uniformlyforsomelargeenoughconstantB.(ii)Inthestronglyboundedcase,usingthefactthat||Θj||22.Λ−2min=O(1)and||Θj||1≤√sj||Θj||2,wehave|ΘTjeXi|≤||Θj||1||eXi||∞≤Kn√sj||Θj||2.Itisstraightforwardtoverifythatmaxk=1,2E|ξij|2+k/Bkn+Eexp(|ξij|/Bn)≤4uniformlywithsomeBn≍Knmaxj√sjandB2n(log(pn))7/n≤C2n−c2underpart(ii)ofAssumption2.3.♦Remark1.1.TheconclusioninLemma1.1stillholdsifweassumethat(i)maxi,j|Xij|≤Knwithmax1≤j≤ps2jK4n(log(pn))7/n≤C1n−c1forsomeconstantsc1,C1>0;and(ii){ǫi}arei.i.dwithwithE|ǫi|4<∞andc′<σ2ǫforc′>0.Nextwequantifytheeﬀectbyreplacingξiwithbξi.2Lemma1.2.SupposeAssumptions2.1-2.3hold.AssumemaxjK20s2j(log(pn))3(log(n))2/n=o(1).RecallthatK0=1inthesub-GaussiancaseandK0=Kninthestronglyboundedcase.Thenwithλj≍K0plog(p)/nuniformlyforj,thereexistζ1,ζ2>0suchthatP max1≤j≤p(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)nXi=1bξij/√n−nXi=1ξij/√n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)≥ζ1!<ζ2,whereζ1p1∨log(p/ζ1)=o(1)andζ2=o(1).ProofofLemma1.2.LeteK0=log(np)log(n)inthesub-GaussiancaseandeK0=Knlog(n)inthestronglyboundedcase.UsingLemmaA.1inChernozhukovetal.(2013),wededucethatE(max1≤j≤p(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)nXi=1Xijǫi/n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)).σǫqmaxjΣj,jplog(p)/n+rEmaxi,j|Xijǫi|2log(p)/n.plog(p)/n+rEmaxi,jX2ijqEmaxiǫ2ilog(p)/n.plog(p)/n+eK0log(p)/n,wherewehaveusedthefactthatqEmaxiǫ2i.log(n)max1≤i≤n||ǫi||ψ1.lognwithψ1(x)=exp(x)−1and||·||ψ1beingthecorrespondingOrlicznorm,andsimilarresultforqEmaxi,jX2i,j(seeLemma2.2.2invanderVaartandWellner1996).Because||bΘj−Θj||1=OP(K0sjplog(p)/n)uniformlyforj,weobtain,(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)nXi=1bξij/√n−nXi=1ξij/√n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(bΘTj−ΘTj)nXi=1eXiǫi/√n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)≤||bΘj−Θj||1(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)nXi=1eXiǫi/√n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)∞=OP K0sjplog(p)/n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)nXi=1eXiǫi/√n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)∞!=OP(cid:16)K0sjlog(p)/√n+√nK0eK0sj(log(p)/n)3/2(cid:17)≤OP(cid:18)maxjsjK0log(p)/√n(cid:19),uniformlyforallj.Choosingζ1suchthatmaxjK0sjlog(p)/(√nζ1)=o(1)andζ1p1∨log(p/ζ1)=3o(1)(e.g.ζ21=O(maxjK0sjplog(p)/n)),wededucethatP max1≤j≤p(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)nXi=1bξij/√n−nXi=1ξij/√n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)≥ζ1!<ζ2,ζ2=o(1).♦Remark1.2.Withamoredelicateanalysis,onecanspecifytheorderofζ2inLemma1.2;seee.g.,Theorem6.1andLemma6.2ofB¨uhlmannandvandeGeer(2011).ProofofTheorem2.2.Withoutlossofgenerality,wesetG={1,2,...,p}.DeﬁneTG=maxj∈G√n(˘βj−β0j),T0,G=maxj∈GnXi=1ξij/√n.Letπ(v)=C2v1/3(1∨log(p/v))2/3withC2>0,andΓ=max1≤j,k≤p|bσ2ǫbΘTjbΣbΘk−σ2ǫΘTjΣΘk|,bΣ=XTX/n.Noticethat|TG−T0,G|≤max1≤j≤p(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)nXi=1bξij/√n−nXi=1ξij/√n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)+||∆||∞.BysimilarargumentsintheproofofTheorem2.4ofvandeGeeretal.(2014)andtheresultsinTheorem2.1,wehave||∆||∞≤||bβ−β0||1maxj√nλj/bτ2j=OP(K0plog(p)||bβ−β0||1)=OP(K20s0log(p)/√n),whereweusethefactthatmaxjλj/bτ2j=OP(K0plog(p)/n)and||bβ−β0||1=OP(s0λ)withλ=O(K0plog(p)/n).ThusbyLemma1.2andtheassumptionthatK40s20(log(p))3/n=o(1),wehaveP(|TG−T0,G|>ζ1)<ζ2,forζ1p1∨log(p/ζ1)=o(1)andζ2=o(1).Letcz,G(α)=inf{t∈R:P(maxj∈GPni=1zij/√n≤t)≥1−α},wherethesequence{zij}is4deﬁnedinLemma1.1.FollowingtheargumentsintheproofofLemma3.2inChernozhukovetal.(2013),wehaveP(cG(α)≤cz,G(α+π(v)))≥1−P(Γ>v),(2)P(cz,G(α)≤cG(α+π(v)))≥1−P(Γ>v).(3)ByLemma1.1,(2)and(3),wehaveforeveryv>0,supα∈(0,1)|P(T0,G>cG(α))−α|.supα∈(0,1)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)P maxj∈GnXi=1zij/√n>cG(α)!−α(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)+n−c′.π(v)+P(Γ>v)+n−c′.Moreover,bytheargumentsintheproofofTheorem3.2inChernozhukovetal.(2013),wehavesupα∈(0,1)|P(TG>cG(α))−α|.π(v)+P(Γ>v)+n−c′+ζ1p1∨log(p/ζ1)+ζ2.ByLemma5.3andLemma5.4ofvandeGeeretal.(2014),wehavemax1≤j,k≤p|bΘTjbΣbΘk−ΘTjΣΘk|=OP(maxjλj√sj).Since|ΘTjΣΘk|≤1/(τjτk)=O(1)uniformlyfor1≤j,k≤p,wehaveΓ=OP(cid:18)|bσ2ǫ−σ2ǫ|+maxjλj√sj(cid:19).UnderAssumption2.4,choosingv=1/(αn(log(p))2),wededucethatsupα∈(0,1)(cid:12)(cid:12)(cid:12)(cid:12)P(max1≤j≤p√n(˘βj−β0j)>cG(α))−α(cid:12)(cid:12)(cid:12)(cid:12)=o(1),whichcompletestheproof.♦5ProofofTheorem2.3.FromtheargumentsintheproofofTheorem2.2,wehaveΓ=max1≤j,k≤p|bσ2ǫbΘTjbΣbΘk−σ2ǫΘTjΣΘk|=OP(cid:18)|bσ2ǫ−σ2ǫ|+maxjλj√sj(cid:19),whichimpliesthatmax1≤j≤p|bωjj−ωjj|=OP(cid:0)|bσ2ǫ−σ2ǫ|+maxjλj√sj(cid:1)withωjj=σ2ǫθjj.WethenhaveP(ωjj/2<bωjj<2ωjjforall1≤j≤p)→1.(4)Thefactthat1/c<Λ2min≤τ2j=1/θjj≤Σj,j=Cimpliesthatωjjisuniformlyboundedawayfromzeroandinﬁnity.Deﬁne¯TG=maxj∈G√n(˘βj−β0j)/pbωjjand¯T0,G=maxj∈GPni=1ξij/√nωjj.Denoteby∆=(∆1,...,∆p)Tand¯∆=(¯∆1,...,¯∆p)Twith¯∆j=∆j/pbωjj.Thenwehave|¯TG−¯T0,G|≤max1≤j≤p(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)nXi=1bξij/qnbωjj−nXi=1ξij/√nωjj(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)+||¯∆||∞≤max1≤j≤p(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)nXi=1bξij/qnbωjj−nXi=1bξij/√nωjj(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)+max1≤j≤p(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)nXi=1bξij/√nωjj−nXi=1ξij/√nωjj(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)+||¯∆||∞≤C′max1≤j≤p(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)nXi=1bξij/√n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)max1≤j≤p(cid:12)(cid:12)(cid:12)(cid:12)qωjj/bωjj−1(cid:12)(cid:12)(cid:12)(cid:12)+C′′max1≤j≤p(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)nXi=1(bξij−ξij)/√n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)+||¯∆||∞,=I1+I2+I3,whereC′,C′′>0.6Ontheeventωjj/2<bωjj<2ωjjforall1≤j≤p,max1≤j≤p(cid:12)(cid:12)(cid:12)(cid:12)qωjj/bωjj−1(cid:12)(cid:12)(cid:12)(cid:12)≤max1≤j≤p|√ωjj−qbωjj|max1≤j≤pq2/ωjj≤max1≤j≤p(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)ωjj−bωjj√ωjj+pbωjj(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)max1≤j≤pq2/ωjj≤max1≤j≤p|ωjj−bωjj|max1≤j≤p1/ωjj=OP(cid:18)|bσ2ǫ−σ2ǫ|+maxjλj√sj(cid:19).Ontheotherhand,max1≤j≤p(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)nXi=1bξij/√n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)≤max1≤j≤p(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)nXi=1(bξij−ξij)/√n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)+max1≤j≤p(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)nXi=1ξij/√n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)=OP(plog(p)+maxj√sjeK0log(p)/√n)=OP(plogp),whereeK0=log(np)log(n)inthesub-GaussiancaseandeK0=Knlog(n)inthestronglyboundedcase.Therefore,ontheaboveevent,I1≤OP(cid:16)plog(p)|bσ2ǫ−σ2ǫ|+plog(p)maxjλj√sj(cid:17).UnderAssumption2.4,wecanﬁndζ′1suchthatP(I1>ζ′1)=o(1)andζ′1p1∨log(p/ζ′1)=o(1).Usingthefactthat||∆||∞≤OP(K20s0log(p)/√n),wecanprovethesameresultfor||¯∆||∞conditionalontheevent{ωjj/2<bωjj<2ωjjforall1≤j≤p}.ThusbyLemma1.2and(4),wehaveP(|¯TG−¯T0,G|>ζ1)≤P(I1+I2+I3>ζ1)<ζ2,forζ1p1∨log(p/ζ1)=o(1)andζ2=o(1).Let¯Γ=max1≤j,k≤p|bσ2ǫbΘTjbΣbΘk/pbωjjbωkk−σ2ǫΘTjΣΘk/√ωjjωkk|.Notethat|√ωjjωkk−qbωjjbωkk|=|ωjjωkk−bωjjbωkk|√ωjjωkk+pbωjjbωkk.7Ontheeventωjj/2<bωjj<2ωjjforall1≤j≤p,wehave|ωjjωkk−bωjjbωkk|√ωjjωkk+pbωjjbωkk≤|ωjjωkk−bωjjbωkk|√ωjjωkk+pωjjωkk/4≤(2/3)|ωjjωkk−bωjjbωkk|max1≤j≤p1/ωjj,whichimpliesthatmax1≤j,k≤p|qωjjωkk/bωjjbωkk−1|≤max1≤j,k≤p|√ωjjωkk−qbωjjbωkk|max1≤j≤p2/ωjj≤(4/3)max1≤j,k≤p|ωjjωkk−bωjjbωkk|max1≤j≤p1/ω2jj=OP(cid:18)|bσ2ǫ−σ2ǫ|+maxjλj√sj(cid:19).Usingsimilarargumentsabove,wecanshowthatP(¯Γ>v)=o(1)forv=1/(αn(log(p))2).TherestoftheproofsissimilartothoseintheproofofTheorem2.2.Weskipthedetails♦ProofofTheorem2.4.DeﬁneeTG=maxj∈G|√n(˘βj−β0j)/pbωjj|andeT0,G=maxj∈GPni=1|ξij/√nωjj|.UndertheassumptionsinTheorem2.3,wecanshowthatP(|eTG−eT0,G|>ζ1)<ζ2forζ1p1∨log(p/ζ1)=o(1)andζ2=o(1).Inanotherword,thedistributionofmaxj∈G√n|˘βj−β0|/pbωjjcanbeapproximatedbymaxj∈G|Zj|withZ=(Z1,...,Zp)∼dN(0,eΘ).UnderAssumption2.5,byLemma6ofCaietal.(2014),wehaveforanyx∈Randas|G|→+∞,P(cid:18)maxj∈G|Zi|2−2log(|G|)+loglog(|G|)≤x(cid:19)→F(x):=exp(cid:26)−1√πexp(cid:16)−x2(cid:17)(cid:27).ItimpliesthatP(cid:18)maxj∈Gn|˘βj−β0j|2/bωjj≤2log(|G|)−loglog(|G|)/2(cid:19)→1.(5)Thebootstrapconsistencyresultimpliesthat|(¯c∗G(α))2−2log(|G|)+loglog(|G|)−qα|=oP(1),(6)whereqαisthe100(1−α)thquantileofF(x).Consideranyj∗∈Gsuchthat|eβj∗−β0j∗|/√ωj∗j∗>8(√2+ε0)p(log|G|)/n.Usingtheinequality2a1a2≤δ−1a21+δa22foranyδ>0,wehaven|eβj∗−β0j∗|2/bωj∗j∗≤(1+δ−1)n|˘βj∗−β0j∗|2/bωj∗j∗+(1+δ)n|˘βj∗−eβj∗|2/bωj∗j∗,(7)wheren|˘βj∗−β0j∗|2/bωj∗j∗=op(log|G|)asj∗isﬁxedand|G|grows.FromtheproofofTheorem2.3,weknowthediﬀerencebetweenn|eβj∗−β0j∗|2/bωj∗j∗andn|eβj∗−β0j∗|2/ωj∗j∗isasymptoticallynegligible.Thusby(7)andthefactthatβ0∈UG(√2+ε0),wehave,maxj∈Gn|˘βj−eβj|2/bωjj≥11+δn(√2+ε0)2(log|G|)−op(log|G|)o.(8)Theconclusionthusfollowsfrom(8)and(6)providedthatδissmallenough.♦ProofofProposition3.1.SimilartotheproofofTheorem2.4,thedistributionofmax1≤j≤p√n|˘βj−β0|/pbωjjcanbeapproximatedbymax1≤j≤p|Zj|withZ=(Z1,...,Zp)d∼N(0,eΘ).UnderAssump-tion2.5,byLemma6ofCaietal.(2014),wehaveforanyx∈Randasp→+∞,P(cid:18)max1≤i≤p|Zi|2−2log(p)+loglog(p)≤x(cid:19)→exp(cid:26)−1√πexp(cid:16)−x2(cid:17)(cid:27).ItimpliesthatP(cid:18)maxj∈Sc0n|˘βj|2/bωjj≤2log(p)−loglog(p)/2(cid:19)→1.(9)Ontheotherhand,wenotethatminj∈S0n|β0j|2/bωjj≤2maxj∈S0n|˘βj−β0j|2/bωjj+2minj∈S0n|˘βj|2/bωjjBecausethediﬀerencebetweenminj∈S0n|β0j|2/bωjjandminj∈S0n|β0j|2/ωjjisasymptoticallynegli-gible,andP(2maxj∈S0n|˘βj−β0j|2/bωjj≤4log(p)−loglog(p))→1,weobtainP(cid:18)minj∈S0n|˘βj|2/bωjj>2logp(cid:19)≥P(cid:18)2minj∈S0n|˘βj|2/bωjj+4log(p)−loglog(p)>8log(p)(cid:19)→1.(10)9Hence,(16)followsfrom(9)and(10).Wenextprovetheoptimalityofτ∗=2,i.e.,(17).Forlargeenoughp,wecanchooseasetG∗suchthatβj=0forj∈G∗,and|G∗|=⌊pτ2⌋withτ/2<τ2<1.Followingtheabovearguments,weknowthatthedistributionofmaxj∈G∗√n|˘βj−β0j|/pbωjjcanbeapproximatedbymaxj∈G∗|Zj|withZ=(Z1,...,Zp)∼dN(0,eΘ).ThenwehaveP(cid:18)maxj∈G∗n|˘βj|2/bωjj≥clog(p)(cid:19)→1,whereτ<c<2τ2<2.Theconclusionthusfollowsimmediately.♦ProofofTheorem4.1.Forsimplicity,weonlyprovetheresultfortheone-sidedcase(theargumentsbelowcanbeeasilymodiﬁedforthetwo-sidedcase).DeﬁneTG=maxj∈G√n(˘βj−β0j)andT0,G=maxj∈GPni=1ξij/√n.LetecG(α)bethebootstrapcriticalvaluefortheone-sidedtestatlevelα.Weﬁrstshowthatthereexistζ1,ζ2>0suchthatP(|TG−T0,G|≥ζ1)<ζ2,(11)whereζ1p1∨log(p/ζ1)=o(1)andζ2=o(1).Noticethat|TG−T0,G|≤maxj∈G√n|(ΘTj−bΘTj)En˙Lβ0|+||∆||∞+√n||bΘR||.UndertheLipschitzcontinuityinAssumption4.1,wehavebΘTjEn˙Lbβ=bΘTjEn˙Lβ0+bΘTjEn¨Lbβ(bβ−β0)+Rj,whereRj=bΘTjR≤maxi|bΘTjxi|·||X(bβ−β0)||22/n=OP(Kns0λ2)(seetheproofofTheorem3.1invandeGeeretal.2014).Itthusimpliesthat√n||bΘR||∞=OP(√nKns0λ2).ByAssumptions4.3-4.4,wehave||∆||∞=||√n(bΘbΣ−I)(bβ−β0)||∞≤||bΘbΣ−I||∞√n||bβ−β0||1=OP(√nλλ∗s0)10FollowingtheargumentsintheproofofLemma1.2,itcanbeshownthatunderAssumption4.5maxj∈G√n|(ΘTj−bΘTj)En˙Lβ0|=maxj∈G(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)nXi=1bξij/√n−nXi=1ξij/√n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)=OP(Knmaxjsjlog(p)/√n)+OP(cid:16)K2ns0(cid:16)λ2√n∨λplog(p)(cid:17)(cid:17)Thus(11)followsfromaproperchoiceofζ1.ByLemma1.1,wehavesupx∈R(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)P maxj∈GnXi=1ξij/√n≤x!−P maxj∈GnXi=1zij/√n≤x!(cid:12)(cid:12)(cid:12)(cid:12)(cid:12).n−c′,c′>0,where{zi=(zi1,...,zip)′}isasequenceofmeanzeroindependentGaussianvectorwithEziz′i=ΘTjΣβ0Θj.BytheargumentsintheproofofTheorem3.2inChernozhukovetal.(2013),wehavesupα∈(0,1)|P(TG>ec∗G(α))−α|.π(v)+P(eΓ>v)+n−c′+ζ1p1∨log(p/ζ1)+ζ2,(12)whereπ(v)=C2v1/3(1∨log(p/v))2/3.Theconclusionfollowsbychoosingv=1/(αn(log(p))2)in(12).♦2AdditionalnumericalresultsWeconsiderthelinearmodelswheretherowsofXareﬁxedi.i.drealizationsfromNp(0,Σ)withΣ=(Σi,j)pi,j=1undertwoscenarios:(i)Toeplitz:Σi,j=0.9|i−j|;(ii)Exchangeable/Compoundsymmetric:Σi,i=1andΣi,j=0.8fori6=j.TheactivesetisS0={1,2,...,s0}withs0=3or15.ToobtainthemainLassoestimator,weimplementedthescaledLassowiththetuningparameterλ0=√2˜Ln(k0/p)with˜Ln(t)=n−1/2Φ−1(1−t),whereΦisthecumulativedistributionfunctionforN(0,1),andk0isthesolutiontok=˜L41(k/p)+2˜L21(k/p).Weestimatethenoiselevelσ2usingthemodiﬁedvarianceestimator.112.1ModiﬁedvarianceestimatorFigureS.1providesboxplotsofbσ/σforthevarianceestimatordeliveredbythescaledLasso(denotedby“SLasso”)andforthemodiﬁedvarianceestimatorin(24)ofthepaper(denotedby“SLasso∗”).Clearly,themodiﬁedvarianceestimatorcorrectsthenoiseunderestimationissueandthusispreferable.2.2ImpactoftheremaindertermWediscusstheimpactofthe(normalized)remainderterm∆onthecoverageaccuracy.Recallthelinearexpansion√n(˘β−β0)=bΘXTǫ/√n+∆,where∆=(∆1,...,∆p)T=−√n(bΘbΣ−I)(bβ−β0)withbΣbeingtheGrammatrixandbβbeingtheLassoestimator.Thestudentizedmaximumtypeteststatisticcanbewrittenasmax1≤j≤p√n|˘βj−β0j|pbωjj=max1≤j≤p(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Pni=1bξijpnbωjj+∆jpbωjj(cid:12)(cid:12)(cid:12)(cid:12)(cid:12).(13)Thusthecoverageaccuracycanbegreatlyaﬀectedbytheterm∆∗j:=∆j√bωjj.Notethatthis(normalized)remaindertermisdeterminedbybΘ.WenowconsiderthreediﬀerentmethodsinestimatingΘ:(i)nodewiseLassowithλjschosenby10-foldcrossvalidation;(ii)nodewiseLassowithλj=0.01;(iii)themethodinJavanmardandMontanari(2014)withthetuningparameterschosenautomaticallybytheiralgorithm.Toempiricallyevaluate∆∗:=(∆∗1,...,∆∗p)T,weconsiderthelinearmodelswitht(4)/√2errors,n=100andp=500.Deﬁne∆∗ac=(∆∗j)j∈S0and∆∗in=(∆∗j)j∈Sc0.FigureS.2presentstheboxplotsfor||∆∗ac||∞and||∆∗in||∞.ThenodewiseLassoclearlyoutperformsthemethodinJavanmardandMontanari(2014),andthechoiceofλj=0.01yieldsthesmallest||∆∗ac||∞inallcases.Inaddition,||∆∗ac||∞isrelativelylargewhenΣisexchangeable,s0=15andp=500,whichexplainsthelackofperformance/undercoverageinthiscase.Weobservethatthemaximumnormsof∆∗acand∆∗ingenerallyincreasewiths0.Overall,theabovediscussionssupportourobservationsinTables1-2ofthepaperinthesensethatthelowerthe(normalized)remaindertermis,themoreaccuratethecoverageis.12References[1]Javanmard,A.andMontanari,A.(2014).Conﬁdenceintervalsandhypothesistestingforhigh-dimensionalregression.arXiv:1306.3171.13SLasso  p=120SLasso*  p=120SLasso  p=500SLasso*  p=5001.01.52.02.5s0=3, Toeplitzs^sSLasso  p=120SLasso*  p=120SLasso  p=500SLasso*  p=5001.01.52.02.5s0=15, Toeplitzs^sSLasso  p=120SLasso*  p=120SLasso  p=500SLasso*  p=5001.01.52.02.5s0=3, Exchangeables^sSLasso  p=120SLasso*  p=120SLasso  p=500SLasso*  p=5000.51.01.52.02.5s0=15, Exchangeables^sFigureS.1:Boxplotsforbσ/σ,wheres0=3or15,ΣisToeplitzorexchangeable,andtheerrorsaregeneratedfromthestudentizedt(4)distribution.Here“SLasso”correspondstothevarianceestimatordeliveredbythescaledLassoand‘SLasso∗”correspondstothemodiﬁedvarianceestimator.14CV0.01JMTRUECV0.01JMTRUE0.050.100.200.501.002.005.0010.0020.00s0=3, p=500, Toeplitz Active set                        Inactive setD*CV0.01JMTRUECV0.01JMTRUE0.10.20.51.02.05.010.020.0s0=3, p=500, Exchangeable Active set                        Inactive setD*CV0.01JMTRUECV0.01JMTRUE0.51.02.05.010.020.0s0=15, p=500, Toeplitz Active set                        Inactive setD*CV0.01JMTRUECV0.01JMTRUE1251020s0=15, p=500, Exchangeable Active set                        Inactive setD*FigureS.2:Boxplotsfor||∆∗ac||∞and||∆∗in||∞,wheres0=3or15,p=500,ΣisToeplitzorexchangeable,andtheerrorsaret(4)/√2.Here“CV”,“0.01”,“JM”and“TRUE”denotethenodewiseLassowithλjschosenby10-foldcrossvalidationandλj=0.01,themethodinJavanmardandMontanari(2014)andthemethodwiththetrueΘrespectively.Notethatthey-axisisplottedonalogscale.15