Variational Neural Discourse Relation Recognizer

Biao Zhang1,2, Deyi Xiong2 and Jinsong Su1
Xiamen University, Xiamen, China 3610051
Soochow University, Suzhou, China 2150062

zb@stu.xmu.edu.cn, jssu@xmu.edu.cn

dyxiong@suda.edu.cn

6
1
0
2

 
r
a

 

M
2
1

 
 
]
L
C
.
s
c
[
 
 

1
v
6
7
8
3
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

Implicit discourse relation recognition is a crucial
component for automatic discourse-level analysis
and nature language understanding. Previous stud-
ies exploit discriminative models that are built on
either powerful manual features or deep discourse
representations. In this paper, instead, we explore
generative models and propose a variational neu-
ral discourse relation recognizer. We refer to this
model as VIRILE. VIRILE establishes a directed
probabilistic model with a latent continuous vari-
able that generates both a discourse and the relation
between the two arguments of the discourse. In or-
der to perform efﬁcient inference and learning, we
introduce a neural discourse relation model to ap-
proximate the posterior of the latent variable, and
employ this approximated posterior to optimize a
reparameterized variational lower bound. This al-
lows VIRILE to be trained with standard stochastic
gradient methods. Experiments on the benchmark
data set show that VIRILE can achieve competitive
results against state-of-the-art baselines.

1 Introduction
Discourse relation characterizes the internal structure and
logical relation of a coherent text. Automatically identify-
ing these relations not only plays an important role in dis-
course comprehension and generation, but also obtains wide
applications in many other relevant natural language process-
ing tasks, such as text summarization [Yoshida et al., 2014],
conversation [Higashinaka et al., 2014], question answering
[Verberne et al., 2007] and information extraction [Cimiano
et al., 2005]. Generally, discourse relations can be divided
into two categories: explicit and implicit, which can be illus-
trated in the following example:

The company was disappointed by the ruling.
(because) The obligation is totally unwarranted.

difﬁcult to be recognized. This is because almost no sur-
face information in these two sentences can signal this re-
lation. For successful recognition of this relation, in the
contrary, we need to understand the deep semantic corre-
lation between disappointed and obligation in the two sen-
tences above. Although explicit discourse relation recogni-
tion (DRR) has made great progress [Miltsakaki et al., 2005;
Pitler et al., 2008], implicit DRR still remains a serious chal-
lenge due to the difﬁculty in semantic analysis.

Conventional approaches to implicit DRR often treat the
relation recognition as a classiﬁcation problem, where dis-
course arguments and relations are regarded as the inputs and
outputs respectively. Generally, these methods ﬁrst generate
a representation for a discourse, denoted as x1 (e.g., man-
ual features in SVM-based recognition [Pitler et al., 2009;
Lin et al., 2009] or sentence embeddings in neural networks-
based recognition [Ji and Eisenstein, 2015; Zhang et al.,
2015]), and then directly model the conditional probability of
the corresponding discourse relation y given x, i.e. p(y|x).
In spite of their success, these discriminative approaches rely
heavily on the goodness of discourse representation x. So-
phisticated and good representations of a discourse, however,
may make models suffer from overﬁtting as we do not have
large-scale balanced data.

Instead, we assume that there is a latent continuous variable
z from an underlying semantic space. It is this latent variable
that generates both discourse arguments and the correspond-
ing relation, i.e. p(x, y|z). The latent variable enables us to
jointly model discourse arguments and their relations, rather
than conditionally model y on x. However, the incorporation
of the latent variable makes the modeling difﬁcult due to the
following three aspects: 1) the posterior distribution of the la-
tent continuous variable is intractable; 2) a relatively simple
approximation to the posterior, e.g. the mean-ﬁled approach,
may fail in capturing the true posterior of the latent variable;
3) a complicated approximation for the posterior will make
the inference and learning inefﬁcient.

Inspired by Kingma and Welling [2014] as well as Rezende
et al. [2014] who introduce a variational neural inference
model to the intractable posterior via optimizing a reparam-

With the discourse connective because, these two sentences
display an explicit discourse relation CONTINGENCY which
can be inferred easily. Once this discourse connective is re-
moved, however, the discourse relation becomes implicit and

1Unless otherwise speciﬁed, all the variables in the paper, e.g.,
x, y, z are multivariate. But for notational convenience, we treat
them as univariate variables in most cases. Additionally, we use bold
symbols to denote variables, and plain symbols to denote values.

model can indeed ﬁt the data set with respect to discourse
arguments and relations.

2 Related Work

There are two lines of research related to our work: implicit
discourse relation recognition and variational neural model,
which we describe in succession.

Implicit Discourse Relation Recognition Due to the release
of Penn Discourse Treebank [Prasad et al., 2008] corpus, con-
stantly increasing efforts are made for implicit DRR. Upon
this corpus, Pilter et al. [2009] exploit several linguistically
informed features, such as polarity tags, modality and lexical
features. Lin et al. [2009] further incorporate context words,
word pairs as well as discourse parse information into their
classiﬁer. Following this direction, several more powerful
features have been exploited: entities [Louis et al., 2010],
word embeddings [Braud and Denis, 2015], Brown cluster
pairs and co-reference patterns [Rutherford and Xue, 2014].
With these features, Park and Cardie [2012] perform feature
set optimization for better feature combination.

Different from feature engineering, predicting discourse
connectives can indirectly help the relation classiﬁcation
[Zhou et al., 2010; Patterson and Kehler, 2013].
In addi-
tion, selecting explicit discourse instances that are similar to
the implicit ones can enrich the training corpus for implicit
DRR and gains improvement [Wang et al., 2012; Lan et al.,
2013; Braud and Denis, 2014; Fisher and Simmons, 2015;
Rutherford and Xue, 2015]. Very recently, neural network
models have been also used for implicit DRR due to its ca-
pability for representation learning [Ji and Eisenstein, 2015;
Zhang et al., 2015].

Despite their successes, most of them focus on the discrim-
inative models, leaving the ﬁeld of generative models for im-
plicit DRR a relatively uninvestigated area.

Variational Neural Model In the presence of continuous la-
tent variables with intractable posterior distributions, efﬁcient
inference and learning in directed probabilistic models is re-
quired. Kingma and Welling [2014] as well as Rezende et
al. [2014] introduce variational neural networks that employ
an approximate inference model for intractable posterior and
reparameterized variational lower bound for stochastic gradi-
ent optimization. Kingma et al. [2014] revisit the approach to
semi-supervised learning with generative models and further
develop new models that allow effective generalization from
a small labeled dataset to a large unlabeled dataset. Chung et
al. [2015] incorporate latent variables into the hidden state
of a recurrent neural network, while Gregor et al. [2015]
combine a novel spatial attention mechanism that mimics the
foveation of human eyes, with a sequential variational auto-
encoding framework that allows the iterative construction of
complex images.

We follow the spirit of these variational models, but focus
on the adaptation and utilization of them onto implicit DRR,
which, to the best of our knowledge, is the ﬁrst attempt in this
respect.

Illustration of

the directed graph model
Figure 1:
of VIRILE. Solid lines denote the generative model
pθ(z)pθ(x|z)pθ(y|z), dashed lines denote the variational ap-
proximation qφ(z|x) to the intractable posterior pθ(z|x) for
inference. The variational parameters φ are learned jointly
with the generative model parameters θ.

eterized variational lower bound, we propose a VarIational
neuRal dIscourse reLation rEcognizer (VIRILE) with a latent
continuous variable for implicit DRR in this paper. The key
idea behind VIRILE is that although the posterior distribu-
tion is intractable, we can approximate it via a deep neural
network. Figure 1 illustrates the graph structure of VIRILE.
Speciﬁcally, there are three essential components:

• neural discourse recognizer: Since a discourse x and the
corresponding relation y is independent given the latent
variable z (as shown by the solid lines), we can formu-
late the generation of x and y from z in the equation
pθ(x, y|z) = pθ(x|z)pθ(y|z). These two conditional
probabilities in the right hand side are modeled via deep
neural networks in our neural discourse recognizer (see
section 4.1).

• neural posterior approximator: VIRILE assumes that
the latent variable can be inferred from discourse argu-
ments x (as shown by the dash lines). In order to infer
the latent variable, we employ a deep neural network to
approximate the intractable posterior qφ(z|x) (see sec-
tion 4.2), which makes the inference procedure efﬁcient.
• variational reparameterization: we introduce a repa-
rameterization technique to bridge the gap between the
above-mentioned components (see section 4.3). This al-
lows us to naturally use standard stochastic gradient as-
cent techniques for optimization (see section 4.4).

The main contributions of our work lie in the following two
aspects. 1) We exploit a generative graphic model for implicit
DRR. To the best of our knowledge, this has never been in-
vestigated before. 2) We develop a neural recognizer and a
neural posterior approximator speciﬁcally for implicit DRR,
which enables both the recognition and inference to be efﬁ-
cient.

We conduct a series of experiments for English implicit
DRR on the PDTB-style corpus to evaluate the effectiveness
of our proposed VIRILE model. Experiment results show that
our variational model achieves competitive results against
several strong baselines in term of F1 score. Extensive anal-
ysis on the variational lower bound further reveals that our

amssymbamsmathzxyθφN3 Background: Variational Autoencoder
In this section, we brieﬂy review the variational autoencoder
(VAE) [Kingma and Welling, 2014; Rezende et al., 2014],
one of the most classical variational neural models, which
forms the basis of our model.

Different from conventional neural autoencoders, VAE is a
generative model that can be regarded as a regularized version
of the standard autoencoder. The VAE signiﬁcantly changes
the autoencoder architecture by introducing a latent random
variable z, designed to capture the variations in the observed
variable x. With the incorporation of z, the joint distribution
is formulated as follows:

pθ(x, z) = pθ(x|z)pθ(z)

(1)
where pθ(z) is the prior over the latent variable, which is
usually equipped with a simple Gaussian distribution; and
pθ(x|z) is the conditional distribution that models the prob-
ability of x given the latent variable z. Typically, the VAE
parameterizes pθ(x|z) with a highly non-linear but ﬂexible
function approximator such as a neural network.
Although introducing a highly non-linear function im-
proves the learning capability of VAE, this makes the in-
ference of the posterior pθ(z|x) intractable. To tackle this
problem, the VAE further introduces an approximate poste-
rior qφ(z|x) to enable the following variational lower bound:
LV AE(θ, φ; x) = −KL(qφ(z|x)||pθ(z))
(2)

+Eqφ(z|x)[log pθ(x|z)] ≤ log pθ(x)

where KL(Q||P ) is Kullback-Leibler divergence between
two distributions Q and P , and qφ(z|x) is usually a diago-
nal Gaussian N (µ, diag(σ2)) whose mean µ and variance σ2
are parameterized by again, neural networks, conditioned on
x.

˜z = µ + σ (cid:12) 

To maximize the variational lower bound in Eq.

(2)
stochastically with respect to both θ and φ, the VAE intro-
duces a reparameterization trick that parameterizes the latent
variable z with the Gaussian parameters µ and σ in qφ(z|x):
(3)
where  is a standard Gaussian variable, and (cid:12) denotes an
element-wise product. Intuitively, the VAE learns the repre-
sentation of the latent variable not as single points, but as soft
ellipsoidal regions in latent space, forcing the representation
to ﬁll the space rather than memorizing the training data as
isolated representations. With this trick, the VAE model can
be trained through standard backpropagation technique with
stochastic gradient ascent.

4 The VIRILE Model
This section introduces our proposed VIRILE model. For-
mally, in VIRILE, there are two observed variables , x for a
discourse and y for the corresponding relation, and one latent
variable z. As illustrated in Figure 1, the joint distribution of
the three variables is formulated as follows:
pθ(x, y, z) = pθ(x, y|z)pθ(z)

(4)
We begin with this distribution to elaborate the major compo-
nents in VIRILE.

Figure 2: Neural networks for conditional probabilities
pθ(x|z) and pθ(y|z). The gray color denotes real-valued rep-
resentations while the white and black color 0-1 representa-
tions.

4.1 Neural Discourse Recognizer
The conditional distribution p(x, y|z) in Eq. (4) shows that
both discourse arguments and the corresponding relation are
generated from the latent variable. As shown in Figure 1, x
is d-separated from y by z. Therefore the discourse x and
the corresponding relation y is independent given the latent
variable z. The joint probability can be therefore formulated
as follows

pθ(x, y, z) = pθ(x|z)pθ(y|z)pθ(z)

(5)
We adopt the centered isotropic multivariate Gaussian as the
prior for the latent variable, pθ(z) = N (z; 0, I) following
previous work [Kingma and Welling, 2014; Rezende et al.,
2014]. With respect to the two conditional distributions, we
parameterize them via neural networks as shown in Figure 2.
Before we further explain the network structure, it is neces-
sary to brieﬂy introduce how discourse relations are annotated
in our training data. The PDTB corpus, our training corpus,
annotates implicit discourse relations between two neighbor-
ing arguments, namely Arg1 and Arg2. In VIRILE, we rep-
resent the two arguments with bag-of-word representations,
and denote them as x1 and x2.
To model pθ(x|z) (the bottom part in Figure 2), we project
the representation of the latent variable z ∈ Rdz onto a hidden
layer:
(6)
z + bh(cid:48)
1)
(7)
z + bh(cid:48)
1)
×dz are the transforma-
1 ∈ Rdh(cid:48)
where Wh(cid:48)
tion matrices, bh(cid:48)
2 are the bias terms, du
is the dimensionality of vector representations of u and f (·) is
an element-wise activation function, such as tanh(·), which
is used throughout our model.
Upon this hidden layer, we further stack a Sigmoid layer
to predict the probabilities of corresponding discourse argu-
ments:

h(cid:48)
1 = f (Wh(cid:48)
h(cid:48)
2 = f (Wh(cid:48)
×dz , Wh(cid:48)
1 , bh(cid:48)

2 ∈ Rdh(cid:48)
2 ∈ Rdh(cid:48)

x(cid:48)
1 = Sigmoid(Wx(cid:48)
x(cid:48)
2 = Sigmoid(Wx(cid:48)

(8)
(9)
where x(cid:48)
2 ∈ Rdx2 are the real-valued repre-
sentations of the reconstructed x1 and x2 respectively. Notice

1 ∈ Rdx1 and x(cid:48)

h(cid:48)
1 + bx(cid:48)
1)
h(cid:48)
2 + bx(cid:48)
2)

1 ∈ Rdh(cid:48)

1

2

1

2

1

2

zyx1x2pθ(x|z)pθ(y|z)h′1h′2where the mean µ and s.d. σ of the approximate posterior are
the outputs of the neural network as shown in Figure 3.

input x into a hidden representation:

Similar to the calculation of pθ(x|z), we ﬁrst transform the
(14)
(15)
where Wh1 ∈ Rdh1×dx1 , Wh2 ∈ Rdh2×dx2 are weight matri-
ces, and bh1 ∈ Rdh1 , bh2 ∈ Rdh2 are the bias terms. Notice
that dh1/dh2 are not necessarily equal to dh(cid:48)
We then obtain the Gaussian parameters µ and log σ2

h1 = f (Wh1x1 + bh1)
h2 = f (Wh2x2 + bh2)

/dh(cid:48)

.

1

2

through linear regression:

µ = Wµ1h1 + Wµ2 h2 + bµ

(16)

log σ2 = Wσ1 h1 + Wσ2h2 + bσ

(17)
where µ, σ ∈ Rdz. In this way, this posterior approximator
can be efﬁciently computed.
4.3 Variational Reparameterization
We have described how to calculate the likelihood pθ(x, y|z)
and the approximate posterior qφ(z|x). In order to optimize
our model, we need to further compute an expectation over
the approximate posterior, that is Eqφ(z|x)[log pθ(x, y|z)].
Since this expectation is intractable, we employ the Monte
Carlo method to estimate it with a reparameterization trick
similar to Eq. (3):

Eqφ(z|x)[log pθ(x, y|z)] (cid:39)

1
L

log pθ(x, y|˜z(l))

(18)

where ˜z = µ + σ (cid:12)  and  ∼ N (0, I)

where L is the number of samples. This reparameterization
bridges the gap between the likelihood and the posterior, and
enables the internal backpropagation in our neural network.
When testing new instances using the proposed model, we
simply ignore the noise  and set ˜z = µ to avoid uncertainty.
4.4 Parameter Learning
Given a training instance (x(t), y(t)), the joint training objec-
tive is deﬁned as follows:

(cid:18)

dz(cid:88)

j=1

L(θ, φ) (cid:39)

1
2

1 + log

(cid:16)
j )2(cid:17)
L(cid:88)

(σ(t)

(cid:16)

(cid:17)2

(cid:16)

µ(t)
j

−

σ(t)
j

−

(cid:17)2(cid:19)

1
L

+

l=1

log pθ(x(t), y(t)|˜z(t,l))
where ˜z(t,l) =µ(t) + σ(t) (cid:12) (l) and (l) ∼ N (0, I)
(19)
The ﬁrst term is the KL divergence which can be com-
puted and differentiated without estimation (see [Kingma and
Welling, 2014] for detail). Intuitively, this is a conventional
neural network with a special regularizer. The second term is
the approximate expectation shown in Eq. (18), which is also
differentiable.

There are two different sets of parameters in the proposed

model,

L(cid:88)

l=1

Figure 3: Neural networks for Gaussian parameters µ and
log σ in the approximated posterior qφ(z|x).
that the equality of dx1 = dx2, dh(cid:48)
is not necessary
though we assume so in our experiments. We assume that
pθ(x|z) is a multivariate Bernoulli distribution. Therefore the
logarithm of p(x|z) is calculated as the sum of probabilities
of words in discourse arguments as follows:

1 = dh(cid:48)

2

(cid:88)
(cid:88)

i

j

log p(x|z) =

+

x1,i log x(cid:48)

x2,j log x(cid:48)

1,i + (1 − x1,i) log(1 − x(cid:48)
2,j + (1 − x2,j) log(1 − x(cid:48)

1,i)

2,j)

(10)

dy(cid:88)

i=1

where ui,j is the jth element in ui.

y(cid:48)

In order to estimate pθ(y|z) (the top part in Figure 2), we
stack a softmax layer over the representation of the latent
variable z:
(11)
where Wy(cid:48) ∈ Rdy×dz , by(cid:48) ∈ Rdy are the weight matrix and
bias term. dy denotes the number of discourse relations. Sup-
pose that the true relation is y ∈ Rdy, the logarithm of p(y|z)
can be computed as follows:

= Sof tM ax(Wy(cid:48)z + by(cid:48))

log p(y|z) =

yi log y(cid:48)

i

(12)

In order to precisely estimate these conditional probabili-
ties, our model will force the representation z of the latent
variable to encode semantic information for both the recon-
structed discourse x(cid:48) (Eq. (10)) and predicted discourse rela-
tion y(cid:48) (Eq. (12)), which is exactly what we want.
4.2 Neural Posterior Approximator
For the joint distribution in Eq. (5), we can deﬁne a varia-
tional lower bound that is similar to Eq. (2). The difference
lies in the approximate posterior, which should be qφ(z|x, y)
for VIRILE. However, considering the absence of y during
discourse relation recognition, we assume that the latent vari-
able can be inferred from discourse arguments x alone. This
allows us to use qφ(z|x) rather than qφ(z|x, y) to approxi-
mate the true posterior.
Similar to previous work [Kingma and Welling, 2014;
Rezende et al., 2014], we let qφ(z|x) be a multivariate Gaus-
sian distribution with a diagonal covariance structure:
(13)

qφ(z|x) = N (z; µ, σ2I)

µx1x2h1h2logσ2Algorithm 1 Parameter Learning Algorithm of VIRILE.

Inputs: A, the maximum number of iterations;

M, the number of instances in one batch;
L, the number of samples;

θ, φ ← Initialize parameters
repeat

D ← getRandomMiniBatch(M)
 ← getRandomNoiseFromStandardGaussian()
g ← ∇θ,φL(θ, φ;D, )
θ, φ ← parameterUpdater(θ, φ; g)

until convergence of parameters (θ, φ) or reach the maxi-
mum iteration A

1

2

1

2

2

, bh(cid:48)

2

1

1

, bx(cid:48)

, bx(cid:48)

, bh(cid:48)

, Wx(cid:48)

, Wx(cid:48)

, Wh(cid:48)

• θ: Wh(cid:48)
, Wy(cid:48) and by(cid:48)
• φ: Wh1, Wh2, bh1, bh2, Wµ1 , Wµ2, bµ, Wσ1, Wσ2 and bσ
Since the objective function in Eq. (19) is differentiable, we
can optimize these parameters jointly using standard gradi-
ent ascent techniques. The training procedure for VIRILE is
summarized in Algorithm 1.

5 Experiments
We conducted a series of experiments on English implicit
DRR task to validate the effectiveness of VIRILE. We ﬁrst
brieﬂy review the PDTB dataset that we used to train our
model. We then present experiment setup, results and analy-
sis on the variational lower bound in this section.
5.1 Dataset
We used the largest hand-annotated discourse corpus PDTB
2.02 [Prasad et al., 2008] (PDTB hereafter). This corpus
contains discourse annotations over 2,312 Wall Street Jour-
nal articles, and is organized in different sections. Follow-
ing previous work [Pitler et al., 2009; Zhou et al., 2010;
Lan et al., 2013; Zhang et al., 2015], we used sections 2-
20 as our training set, sections 21-22 as the test set. Sections
0-1 were used as the development set for hyperparameter op-
timization.

In PDTB, discourse relations are annotated in a predicate-
argument view. Each discourse connective is treated as a
predicate that takes two text spans as its arguments. The
discourse relation tags in PDTB are arranged in a three-level
hierarchy, where the top level consists of four major seman-
tic classes: TEMPORAL (TEM), CONTINGENCY (CON), EX-
PANSION (EXP) and COMPARISON (COM). Because the top-
level relations are general enough to be annotated with a high
inter-annotator agreement and are common to most theories
of discourse, in our experiments we only use this level of an-
notations.

We formulated the task as four separate one-against-all bi-
nary classiﬁcation problems: each top level class vs. the other
three discourse relation classes. We also balanced the train-
ing set by resampling training instances in each class until the
number of positive and negative instances are equal. In con-
trast, all instances in the test and development set are kept in
nature. The statistics of various data sets is listed in Table 1.

2http://www.seas.upenn.edu/ pdtb/

Relation

COM
CON
EXP
TEM

#Instance Number
Train Dev Test
1942
152
279
3342
574
7004
760
85

197
295
671
64

Table 1: Statistics of implicit discourse relations for the train-
ing (Train), development (Dev) and test (Test) sets in PDTB.

5.2 Setup
We tokenized all datasets using Stanford NLP Toolkit3. For
optimization, we employed the Adagrad algorithm to update
parameters. With respect to the hyperparameters M, L, A
and the dimensionality of all vector representations, we set
them according to previous work [Kingma and Welling, 2014;
Rezende et al., 2014] and preliminary experiments on the de-
velopment set. Finally, we set M = 100, A = 1000, L =
1, dz = 20, dx1 = dx2 = 10001, dh1 = dh2 = dh(cid:48)
2 =
400, dy = 2 for all experiments. Notice that there is one di-
mension in dx1 and dx2 for unknown words.

We compared VIRILE against the following two different

1 = dh(cid:48)

baseline methods:
• SVM: a support vector machine (SVM) classiﬁer trained
with several manual features. We used the toolkit SVM-
light4 to train the classiﬁer in our experiments.
• SCNN: a shallow convolutional neural network pro-
posed by Zhang et al. [2015].
Features used in SVM are taken from the state-of-the-art im-
plicit discourse relation recognition model, including Bag
of Words, Cross-Argument Word Pairs, Polarity, First-Last,
First3, Production Rules, Dependency Rules and Brown clus-
ter pair [Rutherford and Xue, 2014].
In order to collect
bag of words, production rules, dependency rules, and cross-
argument word pairs, we used a frequency cutoff of 5 to re-
move rare features, following Lin et al. [2009].
5.3 Classiﬁcation Results
Because the development and test sets are imbalanced in
terms of the ratio of positive and negative instances, we chose
F1 score as our major evaluation metric. In addition, we also
provided the precision, recall and accuracy metrics for further
analysis. Table 2 summarizes the classiﬁcation results, where
the highest F1 score in four tasks are highlighted in bold.

From Table 2, we observe that the proposed VIRILE out-
performs SVM on EXP/TEM and SCNN on EXP/COM ac-
cording to their F1 scores. Although it fails on CON, VIRILE
achieves the best result on EXP. Overall, VIRILE is compet-
itive in comparison with the two state-of-the-art baselines.

Similar to other generative models, VIRILE obtains rel-
atively low precisions but high recalls in most cases. With
respect to the accuracy, our model does not yield substantial
improvements over the two baselines except for TEM. This
may be because that we used the F1 score rather than the

3http://nlp.stanford.edu/software/corenlp.shtml
4http://svmlight.joachims.org/

Model
Acc
SVM
63.10
SCNN
60.42
VIRILE 62.43

P

22.79
22.00
22.55

R

64.47
67.76
65.13

(a) COM vs Other
Acc
Model
SVM
60.71
SCNN
63.00
VIRILE 55.45

65.89
56.29
55.21

P

R

58.89
91.11
99.65

F1
33.68
33.22
33.50

F1
62.19
69.59
71.06

Model
Acc
SVM
62.62
SCNN
63.00
VIRILE 57.55

P

39.14
39.80
36.50

R

72.40
75.29
79.93

(b) CON vs Other
Acc
Model
SVM
66.25
SCNN
76.95
VIRILE 85.94

15.10
20.22
25.00

P

R

68.24
62.35
36.47

F1
50.82
52.04
50.11

F1
24.73
30.54
29.67

(c) EXP vs Other

(d) TEM vs Other

Table 2: Classiﬁcation results of different models on the implicit DRR task. P=Precision, R=Recall, and F1=F1 score. The
best F1 scores are highlighted in bold.

(a) COM vs Other

(b) CON vs Other

(c) EXP vs Other

(d) TEM vs Other

Figure 4:
Illustration of the variational lower bound (blue color) on the training set and F-score (brown color) on the devel-
opment set. Horizontal axis: the epoch numbers; Vertical axis: the F1 score for relation classiﬁcation (left) and the estimated
average variational lower bound per datapoint (right).

accuracy, as our selection criterion on the development set.
Nevertheless, more analysis should be done to understand the
deep reason.

Besides, we ﬁnd that the performance of our model is pro-
portional to the number of training instances. This suggests
that collecting more training instances (in spite of the noises)
may be beneﬁcial to our model.
5.4 Variational Lower Bound Analysis
In addition to the classiﬁcation performance, the efﬁciency
in learning and inference is another concern for variational
methods. Figure 4 shows the training procedure for four tasks
in terms of the variational lower bound on the training set. We
also provide F1 scores on the development set to investigate
the relations between the variational lower bound and recog-
nition performance.

We ﬁnd that our model converges toward the variational
lower bound considerably fast in all experiments (within 100
epochs), which resonates with the previous ﬁndings [Kingma
and Welling, 2014; Rezende et al., 2014]. However, the
change trend of the F1 score does not follow that of the lower
bound. Particularly to the four discourse relations, we further
observe that the change paths of the F1 score are completely
different. This may suggest that the four discourse relations
have different properties and distributions.

Speciﬁcally, the number of epochs when the best F1 score
reaches is also different for the four discourse relations. This
indicates that dividing the implicit DRR into four different
tasks according to the type of discourse relations is reasonable

and better than performing DRR on the mixtures of the four
relations.

6 Conclusion and Future Work
In this paper, we have presented a variational neural discourse
relation recognizer for implicit DRR. Different from conven-
tional discriminative models that directly calculate the con-
ditional probability of the relation y given discourse argu-
ments x, our model assumes that it is a latent variable from
an underlying semantic space that generates both x and y.
In order to make the inference and learning efﬁcient, we in-
troduce a neural discourse recognizer and a neural posterior
approximator as our generative and inference model respec-
tively. Using the reparameterization technique, we are able
to optimize the whole model via standard stochastic gradient
ascent algorithm. Experiment results in terms of classiﬁca-
tion and variational lower bound verify the effectiveness of
our model.

In the future, we would like to exploit the utilization of
discourse instances with explicit relations for implicit DRR.
For this we can start from two directions: 1) converting ex-
plicit instances into pseudo implicit instances and retrain our
model; 2) developing a semi-supervised model to leverage se-
mantic information inside discourse arguments. Furthermore,
we are also interested in adapting our model to other similar
tasks, such as nature language inference.

1-1241.5528.550722-244.7328.550723-226.1828.550724-253.528.550725-217.0328.550726-214.3228.550727-210.1628.550728-208.9128.550729-207.4128.5507210-205.7328.5507211-204.3528.5507212-203.528.5507213-202.3128.5507214-201.7328.5507215-201.1228.5507216-200.6728.5507217-200.1628.5507218-199.5228.5507219-199.5828.5507220-198.8428.5507221-198.5128.5507222-19828.5507223-197.828.5507224-197.8328.5507225-197.1728.5507226-196.5928.5507227-196.528.5507228-196.8528.5507229-196.3128.5507230-196.0428.5507231-196.0328.5507232-195.6928.5507233-195.628.5507234-195.3228.5507235-195.5228.5507236-195.0428.5507237-194.7328.5507238-194.9728.5507239-194.4428.5507240-194.7328.5507241-194.4428.5507242-194.2328.5507243-193.9528.5507244-193.5928.5507245-193.7428.5507246-193.5628.5507247-193.4828.5507248-193.0428.5507249-193.3328.5507250-19328.5507251-19328.5507252-192.5628.5507253-192.5128.5507254-192.3628.55072-300-280-260-240-220-200-180-160-140-1201820222426283032341101201301401501601701801901DevTrain1-835.8639.918812-229.8839.918813-212.9939.918814-208.3539.918815-203.6339.918816-201.1639.918817-198.8339.918818-197.8139.918819-197.1739.9188110-196.5639.8104311-194.5839.8104312-194.2239.7288113-193.6339.5904414-192.5539.5027615-191.9640.0282916-191.640.2429817-192.4340.6148918-190.8540.4655319-190.6339.8429820-189.8239.8286921-189.5839.1353822-189.3138.3030323-188.9337.4832724-188.2336.0056325-188.2135.3293426-188.0435.958427-187.634.822828-187.6134.5256629-187.5833.1738430-186.9834.2313831-186.9634.2857132-186.4933.811833-186.1934.0093634-185.9334.0222635-185.8134.2948736-185.6134.6456737-185.3935.6589138-185.2135.2395739-184.9635.3658540-184.8636.771341-184.5437.2832442-184.437.6237643-184.5837.5172444-184.137.6344145-184.0339.3741946-183.8539.7997547-183.8340.2409648-183.640.2860549-183.3740.9854450-183.2941.9319451-183.0841.8356552-182.7742.1052653-182.6242.7850754-182.6743.53877-300-280-260-240-220-200-180-160-140323436384042441101201301401501601701801901DevTrain1-518.0672.384032-223.3772.384033-210.2472.384034-202.4972.384035-198.1872.384036-196.372.384037-192.972.384038-191.2172.384039-19072.3840310-188.872.3840311-187.7672.3840312-187.3272.3840313-186.1472.3840314-185.6172.3840315-184.8372.3840316-184.2572.3840317-183.5272.3840318-183.0972.3840319-182.6972.3840320-182.1972.3840321-181.8772.3840322-181.3672.3840323-180.972.3840324-180.6372.3840325-180.2272.3840326-179.8572.3840327-179.6372.3840328-179.1372.3840329-178.7272.3840330-178.6572.3840331-178.2672.3840332-177.9272.3840333-177.6372.3840334-177.3972.3840335-177.0972.3840336-176.8572.3840337-176.6772.3840338-176.4472.3840339-176.1372.3840340-175.8172.3840341-175.5872.3840342-175.2872.3840343-175.1472.3840344-174.8772.3840345-174.6472.3151646-174.4372.3151647-174.1872.3151648-173.9272.3151649-173.7972.3151650-173.6472.3151651-173.3272.3151652-173.2872.3151653-172.9672.3151654-173.972.31516-300-280-260-240-220-200-180-160-1405055606570751101201301401DevTrain1-2994.2910.264642-268.2610.281123-299.0710.264644-244.2710.264645-236.8210.264646-233.5510.264647-230.210.264648-227.1110.264649-225.0310.2646410-223.5610.2646411-222.210.2646412-221.0310.2646413-219.7310.2646414-218.3710.2646415-217.7310.2646416-216.5910.2646417-215.8910.2646418-215.1110.2646419-214.1410.2646420-213.8610.2646421-213.1910.2646422-212.910.2646423-211.7510.2646424-211.3110.2646425-211.1710.2646426-210.2310.2646427-209.7310.2646428-209.810.2646429-209.4910.2646430-209.4610.2646431-208.5310.2646432-207.9210.2893933-208.1110.2893934-207.910.3059635-207.5610.3225836-207.0610.3643737-206.7310.4149738-206.4710.4660739-206.6510.3789140-205.7710.4825341-206.0910.4825342-205.6210.3678943-205.5610.3678944-205.4810.4906945-205.4910.4452146-205.4510.5445147-204.6110.5810948-204.7810.7205649-204.2610.7678750-204.3610.7678751-203.8110.7205652-203.5810.7017553-204.0110.6457254-203.6410.56277-300-280-260-240-220-200-180-160024681012141101201301401501601701801901DevTrainReferences
[Braud and Denis, 2014] Chlo´e Braud and Pascal Denis.
Combining natural and artiﬁcial examples to improve im-
In Proc. of COL-
plicit discourse relation identiﬁcation.
ING, pages 1694–1705, August 2014.

[Braud and Denis, 2015] Chlo´e Braud and Pascal Denis.
Comparing word representations for implicit discourse re-
In Proc. of EMNLP, pages 2201–
lation classiﬁcation.
2211, 2015.

[Chung et al., 2015] Junyoung Chung, Kyle Kastner, Lau-
rent Dinh, Kratarth Goel, Aaron C. Courville, and Yoshua
Bengio. A recurrent latent variable model for sequential
data. In Proc. of NIPS, 2015.

[Cimiano et al., 2005] Philipp Cimiano, Uwe Reyle, and Jas-
min ˇSari´c. Ontology-driven discourse analysis for in-
formation extraction. Data & Knowledge Engineering,
55:59–83, 2005.

[Fisher and Simmons, 2015] Robert Fisher and Reid Sim-
mons. Spectral semi-supervised discourse relation clas-
In Proc. of ACL-IJCNLP, pages 89–93, July
siﬁcation.
2015.

[Gregor et al., 2015] Karol Gregor,

Ivo Danihelka, Alex
Graves, and Daan Wierstra. DRAW: A recurrent neural
network for image generation. CoRR, abs/1502.04623,
2015.

[Higashinaka et al., 2014] Ryuichiro Higashinaka, Kenji
Imamura, Toyomi Meguro, Chiaki Miyazaki, Nozomi
Kobayashi, Hiroaki Sugiyama, Toru Hirano, Toshiro
Makino, and Yoshihiro Matsuo. Towards an open-domain
conversational system fully based on natural language
processing. In Proc. of COLING, pages 928–939, 2014.

[Ji and Eisenstein, 2015] Yangfeng Ji and Jacob Eisenstein.
One vector is not enough: Entity-augmented distributed
semantics for discourse relations. TACL, pages 329–344,
2015.

[Kingma and Welling, 2014] Diederik P Kingma and Max
In Proc. of

Welling. Auto-Encoding Variational Bayes.
ICLR, 2014.

[Kingma et al., 2014] Diederik P. Kingma, Shakir Mo-
hamed, Danilo Jimenez Rezende, and Max Welling. Semi-
supervised learning with deep generative models. In Proc.
of NIPS, pages 3581–3589, 2014.

[Lan et al., 2013] Man Lan, Yu Xu, and Zhengyu Niu.
Leveraging Synthetic Discourse Data via Multi-task
Learning for Implicit Discourse Relation Recognition. In
Proc. of ACL, pages 476–485, Soﬁa, Bulgaria, August
2013.

[Lin et al., 2009] Ziheng Lin, Min-Yen Kan, and Hwee Tou
Ng. Recognizing implicit discourse relations in the Penn
Discourse Treebank. In Proc. of EMNLP, pages 343–351,
2009.

[Louis et al., 2010] Annie Louis, Aravind Joshi, Rashmi
Prasad, and Ani Nenkova. Using entity features to classify
implicit discourse relations. In Proc. of SIGDIAL, pages
59–62, Tokyo, Japan, September 2010.

[Miltsakaki et al., 2005] Eleni Miltsakaki, Nikhil Dinesh,
Rashmi Prasad, Aravind Joshi, and Bonnie Webber. Ex-
periments on sense annotations and sense disambiguation
of discourse connectives. In Proc. of TLT2005, 2005.

[Park and Cardie, 2012] Joonsuk Park and Claire Cardie.
Improving Implicit Discourse Relation Recognition
Through Feature Set Optimization. In Proc. of SIGDIAL,
pages 108–112, Seoul, South Korea, July 2012.

[Patterson and Kehler, 2013] Gary Patterson and Andrew
Kehler. Predicting the presence of discourse connectives.
In Proc. of EMNLP, pages 914–923, 2013.

[Pitler et al., 2008] Emily Pitler, Mridhula Raghupathy,
Hena Mehta, Ani Nenkova, Alan Lee, and Aravind K
Joshi. Easily identiﬁable discourse relations. Technical
Reports (CIS), page 884, 2008.

[Pitler et al., 2009] Emily Pitler, Annie Louis, and Ani
Nenkova. Automatic sense prediction for implicit dis-
In Proc. of ACL-AFNLP, pages
course relations in text.
683–691, August 2009.

[Prasad et al., 2008] Rashmi Prasad, Nikhil Dinesh, Alan
Lee, Eleni Miltsakaki, Livio Robaldo, Aravind K Joshi,
and Bonnie L Webber. The penn discourse treebank 2.0.
In LREC. Citeseer, 2008.

[Rezende et al., 2014] Danilo Jimenez Rezende, Shakir Mo-
hamed, and Daan Wierstra. Stochastic backpropagation
and approximate inference in deep generative models. In
Proc. of ICML, pages 1278–1286, 2014.

[Rutherford and Xue, 2014] Attapol Rutherford and Nian-
wen Xue. Discovering implicit discourse relations through
brown cluster pair representation and coreference patterns.
In Proc. of EACL, pages 645–654, April 2014.

[Rutherford and Xue, 2015] Attapol Rutherford and Nian-
wen Xue.
Improving the inference of implicit discourse
relations via classifying explicit discourse connectives. In
Proc. of NAACL-HLT, pages 799–808, May–June 2015.

[Verberne et al., 2007] Suzan Verberne, Lou Boves, Nelleke
Oostdijk, and Peter-Arno Coppen. Evaluating discourse-
based answer extraction for why-question answering. In
Proc. of SIGIR, pages 735–736, 2007.

[Wang et al., 2012] Xun Wang, Sujian Li, Jiwei Li, and
Wenjie Li. Implicit discourse relation recognition by se-
In Proc. of COLING,
lecting typical training examples.
pages 2757–2772, 2012.

[Yoshida et al., 2014] Yasuhisa Yoshida, Jun Suzuki, Tsu-
tomu Hirao, and Masaaki Nagata. Dependency-based
discourse parser for single-document summarization.
In
Proc. of EMNLP, pages 1834–1839, October 2014.

[Zhang et al., 2015] Biao Zhang, Jinsong Su, Deyi Xiong,
Yaojie Lu, Hong Duan, and Junfeng Yao. Shallow con-
volutional neural network for implicit discourse relation
recognition. In Proc. of EMNLP, September 2015.

[Zhou et al., 2010] Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu,
Man Lan, Jian Su, and Chew Lim Tan. Predicting dis-
course connectives for implicit discourse relation recogni-
tion. In Proc. of COLING, pages 1507–1514, 2010.

