6
1
0
2

 
r
a

 

M
2
2

 
 
]
E
M

.
t
a
t
s
[
 
 

3
v
3
0
8
4
0

.

3
0
6
1
:
v
i
X
r
a

Reduced Wiener Chaos representation of random ﬁelds

via basis adaptation and projection

Panagiotis Tsiliﬁsa, Roger G. Ghanemb,∗

aDepartment of Mathematics, University of Southern California, Los Angeles, CA 90089,

bDepartment of Civil Engineering, University of Southern California, Los Angeles, CA

USA

90089, USA

Abstract

A new characterization of random ﬁelds appearing in physical models is
presented that is based on their well-known Homogeneous Chaos expansions.
We take advantage of the adaptation capabilities of these expansions where
the core idea is to rotate the basis of the underlying Gaussian Hilbert space, in
order to achieve reduced functional representations that concentrate the induced
probability measure in a lower dimensional subspace. For a smooth family of
rotations along the domain of interest, the uncorrelated Gaussian inputs are
transformed into a Gaussian process, thus introducing a mesoscale that captures
intermediate characteristics of the quantity of interest.

1. Introduction

Modeling, characterizing and propagating uncertainties in complex phys-
ical systems have been extensively explored in recent years as they straddle
engineering and the physical, computational, and mathematical sciences. The
computational burden associated with a probabilistic representation of these
uncertainties is a persistent related challenge. One class of approaches to this
challenge has been to seek proper functional representations of the quantities
of interest (QoI) under investigation that will be consistent with the observed
reality as well as with the mathematical formulation of the underlying physical
system, which for instance, is characterized within the context of partial diﬀer-
ential equations with stochastic parameters. Additionally, these representations
are equipped to serve as accurate propagators useful for prediction or statistical
inference purposes. Among the criteria that make such a functional repre-
sentation a successful candidate, are often the ability to provide a parametric
interpretation of the uncertainties involved in a subscale level of the governing

∗Corresponding author

Email addresses: tsilifis@usc.edu (Panagiotis Tsiliﬁs), ghanem@usc.edu (Roger G.

Ghanem )

Preprint submitted to Elsevier

March 23, 2016

physics, as well as its quality as an approximation of what is assumed to be the
reality and its discrepancy from it, in terms of several modes of convergence
such as distributional, almost sure or functional (L2).

The Homogeneous (Wiener) Chaos [35] representation of random processes
has provided a convenient way to characterize solutions of systems of equa-
tions that describe physical phenomena as was demonstrated in [15] and further
applied to a wide range of engineering problems [11, 25, 10, 12, 14]. Generaliza-
tion of these representations beyond the Gaussian white noise [36, 30] provided
the foundation for a multi-purpose tool for uncertainty characterization and
propagation [21, 27, 37], statistical updating [29, 23, 24] and design [17, 33] or
as a generic mathematical model in order to characterize uncertainties using
maximum likelihood techniques [6, 16], Bayesian inference [13, 2] or maximum
entropy [5]. Despite its wide applicability which has resulted in signiﬁcant gains,
including but not limited to computational eﬃciencies, its use can still easily be-
come prohibitive with the increase of the dimensionality of the stochastic input.
Several attempts using sparse representations [8, 7] have only partially man-
aged to sidestep the issue which still remains a major drawback. Recently, a new
method for adapted Chaos expansions in Homogeneous Chaos spaces has shown
some promising potential as a generic dimensionality reduction technique [32].
The core idea is based on rotating the independent Gaussian inputs through a
suitable isometry to form a new basis such that the new expansion expressed in
terms of that basis concentrates its probability measure in a lower dimensional
subspace, consequently, the basis terms of the Homogeneous Chaos spaces that
lie outside that subspace can be ﬁltered out via a projection procedure. Several
special cases along with intrusive and non-intrusive computational algorithms
were suggested which result in signiﬁcant model reduction while maintaining
high ﬁdelity in the probabilistic characterization of the scalar QoIs.

It is the main objective of the present paper to extend further the basis
adaptation technique from simple scalar quantities of interest to random ﬁelds
or vector valued quantities that admit a polynomial chaos expansion. Such
random ﬁelds emerge, for instance, as solutions of partial diﬀerential equations
with random parameters and can be found to have diﬀerent degree of dependence
on the stochastic inputs at diﬀerent spatio-temporal locations, therefore their
adapted representations and the corresponsing adapted basis should be expected
to exhibit such a spatio-temporal dependence. We provide a general framework
where a family of isometries are indexed by the same topological space used for
indexing the random ﬁeld of interest. Several important properties are proved
for the new adapted expansion, namely the new stochastic input is no longer a
vector of standard normal variables but a Gaussian random ﬁeld that admits a
Karhunen-Loeve [20, 22] expansion with respect to those variables. This new
quantity essentially merges uncertainties into a new basis that varies at diﬀerent
locations, thus introducing a new way of upscaling uncertainties with localized
information about the quantity of interest. In addition, new explicit formulas
are derived that allow the transformation of an existing chaos expansion to a
new expansion with respect to any chosen basis. One major beneﬁt of this
capability is that, once a chaos expansion is available, any suitable adaptation

2

can be achieved without further relying on intrusive and non-intrusive methods
that would require additional (repeated) evaluations of the mathematical model,
thus delivering us from further computational costs.

This paper is organized as follows: First we introduce the basis adaptation
framework for Homogeneous Chaos expansions and the reduction procedure
via projection on subspaces of the Hilbert space of square integrable random
ﬁelds. Next we demonstrate how the framework applies when stochasticity is
also present in the coeﬃcients of the chaos expansion and ﬁnally we provide the
theoretical foundations of an inﬁnite dimensional perspective of our approach
which shows that our derivations remain consistent and are nothing more but
a special case of Hilbert spaces of arbitrary dimension. Finally, our results are
illustrated with two numerical examples: That of an elliptic PDE with random
diﬀusion parameter, which explores various ways of obtaining reduced order
expansions that adapt well on the random ﬁeld of interest and an explicit chaos
expansion where its ﬁrst order coeﬃcients consist of a geometric series which
allows the comparison of inﬁnite dimensional adaptations and their truncated
versions.

2. Basis adaptation in Homogeneous Chaos expansions of random

ﬁelds

2.1. The Homogeneous (Wiener) Chaos

We consider a probability space (Ω,F , P) and G a d-dimensional Gaussian
Hilbert space, that is a closed vector space spanned by a set of d independent
standard (zero-mean and unit-variance) Gaussian random variables {ξi}d
i=1,
equipped with the inner product h·,·iG deﬁned as hξ, ζiG = E[ξζ] for ξ, ζ ∈ H,
where E[·] denotes the mathematical expectation with respect to the probability
measure P. For simplicity, throughtout this section we will drop the index G
and simply write h·,·i whenever there is no confusion. Let now F (G) be the
σ-algebra generated by the elements of G, then since all Gaussian variables have
ﬁnite second moments, it follows that G is a closed subspace of L2(Ω,F (G), P).
We also deﬁne G⋄n, for n ∈ N ∪ {−1, 0} to be the space of all polynomials of
exact order n, with the convention G⋄−1 := {0}. Then clearly G⋄0 is the space
of constants and G⋄1 = G and in fact from the Cameron-Martin theorem [3, 19]
we have that L2(Ω,F (G), P) = L∞n=0 G⋄n which has an orhogonal basis that
consists of the multidimensional Hermite polynomials deﬁned as

hα(ξ) =

hαi(ξi),

d

Yi=1

(1)

where α = (α1, ..., αd) ∈ J := Nd ∪ {0} and hαi (ξi) are the 1-dimensional
Hermite polynomials of order αi. More precisely {hα,|α| = n} spans G⋄n,
where |α| =Pi αi and by introducing the orthonormal basis that consists of

(2)

ψα(ξ) =

hα(ξ)
√α!

, α ∈ J ,

3

any u ∈ L2(Ω,F (G), P) can be represented by its Homogeneous Chaos expansion
(3)

uαψα(ξ),

u(ξ) = Xα∈J

i=1 αi!.

where the convergence of the inﬁnite summation is with respect to the L2(Ω,F (G), P)
norm and α! =Qd
Consider now a real-valued quantity of interest u(x, ξ) where x ∈ D ⊂ Rk,
D is typically bounded, and assume that u ∈ L2(Ω× D,F (G × D), P× λ), where
F (G × D) is the σ-algebra generated by the rectangles A × B ∈ G × D, λ is the
Lebesgue measure on Rk and P × λ is the product measure on Ω × D. Then is
holds that
Eh||u(x, ξ)||2
Then, for each x ∈ D we have u(x, ξ) ∈ L2(Ω,F (G), P) and as above it admits a
representation in terms of its orthogonal basis, that is the Hermite polynomials,

L2(D)p(ξ)dξ =ZΩZD |u(x, ξ)|2p(ξ)dλ(x)dξ < +∞. (4)

L2(D)i =ZΩ ||u(x, ξ)||2

u(x, ξ) = Xα∈J

uα(x)ψα(ξ),

(5)

and the above square-integrability condition (||u(x, ξ)||L2(D) < +∞, for ξ a.s.)
implies that ||uα(x)||L2(D) < +∞ for all α ∈ J , a condition that will be useful
below.

2.2. Change of basis for random ﬁelds

In what follows we work with a truncated representation of u, that is we

assume that only a ﬁnite number of terms of order up to p ∈ N are present

u(x, ξ) = Xα∈Jp

uα(x)ψα(ξ),

(6)

with Jp = {α ∈ J : |α| ≤ p}. The change of basis framework [32] presented
below can easily be generalized for the case of an inﬁnite series. Namely, we
consider an isometry A : Rd → Rd and we observe that η := Aξ is a basis in G
if and only if ξ is. Since the Cameron-Martin theorem applies for any basis in
G, then u can also be written as

uA(x, ξ) := u(x, η) = Xβ,|β|≤p

uA

β (x)ψβ(η),

(7)

and by denoting ψA
tween the polynomials we can write the new coeﬃcients as

β (ξ) := ψβ(η) = ψβ(Aξ) and using the orthogonality be-

uA

β (x) =Xα

βE,
uα(x)Dψα, ψA

∀x.

(8)

4

This can be seen as a pointwise convergence in L2(D) but in fact a stronger
result is true: For the new expansion we still have that ||uA
β (x)||L2(D) < +∞ so
the series actually converges in L2(D).
For the above it is clear that once a Homogeneous Chaos series of u(x, ξ) is
available, then given any isometry A, Eq. (8) gives the coeﬃcients of the series
expansion with respect to the new basis, as a function of the initial coeﬃcients
and the entries of A. Although this expession in the current form is computa-
tionally cumbersome, using properties of the Wick product [19] we are able to
derive analytic formulae with respect to the entries of A that, to the best of our
knowledge have not been presented before. Derivations of these formulae can
be found in Appendix A.

Note here that since the above expressions hold for any isometry A and all
x ∈ D, one might consider choosing diﬀerent A’s for various choices of x. To
illustrate this dependence of A := A(x) on x we take for instance the Gaussian
and the quadratic adaptation [32]. For the Gaussian case, the ﬁrst row of A is
deﬁned, for each x, through the mapping ξ → η1 given as

η1(x) =

1

(cid:16)Pd

i=1 u2

ǫi(x)(cid:17)1/2

uǫi(x)ξi,

(9)

d

Xi=1

where ǫi = (0, ..., 1, ..., 0) is the multi-index with 1 in the ith location and
zeros elsewhere. This represents the (normalized) centered Gaussian part of
u(x). Similarly, for the quadratic case, the matrix A is the unitary matrix that
satisﬁes, for each x,

where S has entries u2ǫi√2

S(x) = AT DA

along the diagonal and

uǫij√2

(10)

elsewhere.

As these cases indicate, the isometry A can depend on x and as a conse-
quence, η will also depend on x which implies that for each x, ξ is transformed
to a diﬀerent basis η(x). By construction, each component ηi(x) of the adapted
bases is a Gaussian process with covariance kernel

ki(x, y) = E[ηi(x)ηi(y)] =

d

Xj,k=1

aij(x)aik(y)E[ξj ξk] = ai(x)ai(y)T

(11)

where for convenience we denote by ai(x) = (ai1(x), ..., aid(x)) the ith row of
A(x). In fact, for the case where the dependence is such that the entries aij(x)
are square integrable, the following result holds:

Theorem 1. Provided that the entries of ai(x) are square-integrable, the

function ki(·,·) : D × D → R deﬁned in eq. (11) is a Hilbert-Schmidt kernel.

Proof. Detailed proof in Appendix B. (cid:3)
Remark 1. For an example, in the case of linear adaptation, the square-
integrability of uǫi(x) as mentioned in the previous subsection suﬃces to show
that ||a1j||L2(D) < +∞, therefore k1(x, y) is Hilbert-Schmidt.

5

Remark 2.

In fact, we will see below that ki(x, y) has at most d posi-
tive eigenvalues and the decomposition (11) is the one that follows by Mercer’s
theorem [26].

2.3. Reduced adapted decompositions via projection

Next, it is of interest to consider a projection of the above expansion on a
subspace of VI ⊂ L2(Ω × D) with VI being the space spanned by {ψβ : β ∈ I}
for some I ⊂ Jp, resulting in

uA,I(x, ξ) := uI(x, η) = Xβ∈I

uA
β (x)ψβ(η)

= Xβ∈I Xα∈Ip

uα(x)Dψα, ψA

βEψβ(η).

(12)

Such projections introduce an error that can be described as the diﬀerence
u− uA,I. Trivially in the case where I = Jp, this diﬀerence is zero. Futhermore
one can write u(x, η) as a series of {ψα(ξ)}α∈Jp

u(x, η) = Xγ∈Jp

uγ(x)ψγ (ξ),

which gives

uγ(x) = Xβ∈Jp Xα∈Jp

uα(x)Dψα, ψA

βEDψA

β , ψγE

(13)

(14)

and in the case of a projection on some I, the sum over β is simply taken in
I instead of Jp. We denote by w(x) and wA,I (x) the vectors with entries the
coeﬃcients {uα(x)}α∈Jp and {uγ(x)}Jp respectively and with |J | the cardinal-
ity of a set J . By introducing the |Jp| × |Jp| Grammian matrix C with entries
βE for β ∈ I and 0 otherwise, we can write the error associated
Cα,β =Dψα, ψA
with a projection I as

w(x) − wA,I (x) =(cid:0)I − CCT(cid:1) w(x),

(15)

which depends solely on I and A. Note that as mentioned previously, as I
approaches Jp the error becomes zero independently of A. However, for I being
a strict subset of Jp the error can vary as a function of the entries of A. A closer
look, using Proposition 2 from Appendix A, indicates that C is a block diagonal
matrix and so is CCT . Furthermore for the case of n-dimensional adaptations
(n < d), each block matrix of the diagonal has only n non-zero columns.

Several options are available for exploration of the error of a particular adap-
tation procedure. For instance, for each x ∈ D and a ﬁxed projection space I
one might wish to minimize, with respect to A, an appropriately chosen norm
of w − wA,I in order to locally adapt the chaos expansion of u(x) at the point

6

of interest x. Alternatively for a global adaptation one can also minimize an
L2(D) norm of w − wI , that is

||w(x) − wI (x)||L2(D) =(cid:18)ZD ||w(x) − wI (x)||2dx(cid:19)1/2

.

(16)

Further investigation of the interrelation between the error and the choice of
A falls beyond the scope of the present paper and can be the subject of future
work.

2.4. Basis adaptation of Chaos expansions with random coeﬃcients

In this subsection we consider the case where the coeﬃcients of the chaos
expansion are themselves taken to be random variables. We adopt the formu-
lation presented in [31] where the random coeﬃcients can be thought of as the
result of a reduced decomposition. More speciﬁcally, let two orthonormal bases
ξ ∈ G1 and ζ ∈ G2 with G1, G2 being d1- and d2-dimensional Gaussian Hilbert
spaces respectively, that are statistically independent and let G = G1 × G2
the closure of the product space G1 × G2. Then it is known [30] that any
u(x, ξ, ζ) ∈ L2(Ω × D,F (G), P) admits an expansion

u(x, ξ, ζ) = Xα∈J d1 Xβ∈J d2

uα,β(x)ψα(ξ)ψβ(ζ),

(17)

where J di := Nd1 ∪ {0}, i = 1, 2. The above expansion can be rearranged in
the form,
(18)

Uα(x, ζ)ψα(ξ),

uα,β(x)ψβ (ζ).

(19)

Thus, u(x, ξ, ζ) can be written as a polynomial chaos expansion with respect to
ξ with random coeﬃcients that depend on ζ and are independent of the basis
functions {ψα(ξ)}α∈J d1 . In order to proceed, we consider again the truncated
series
(20)

Uα(x, ζ)ψα(ξ),

uα,β(x)ψβ (ζ),

(21)

where with no loss of generality we take the order of truncation p to be common
in both series. Then the extension of the adaptation and projection procedures
presented in the previous subsection is straightforward. It is clear that for any
given isometry A, the coeﬃcients UA
α given in Eq. (8) will also be random since

7

where

and

u(x, ξ, ζ) = Xα∈J d1
Uα(x, ζ) = Xβ∈J d2

uJp (x, ξ, ζ) = Xα∈J d1
Uα(x, ζ) = Xβ∈J d2

p

p

the inner product used to project u(x, ξ, ζ) on the basis functions ψα(ξ) is the
merely expectation with respect to ξ. Namely,

UA

α = E(cid:2)uJp (x, ξ, ζ)ψα(ξ)(cid:3) = E(cid:2)uJp (x, ξ, ζ)ψα(ξ)|ζ(cid:3) .

It is also worth noting that in the case of the standard adaptation schemes
(Gaussian, quadratic), the isometry is itself a random matrix that depends on
the coeﬃcients of the reduced expansion (20) and more speciﬁcally its proba-
bility distribution depends on ζ. Denote by Φη(t) the characteristic function of
the new basis η = A(ζ)ξ. Then following some standard manipulations, taking
into account the independence between ζ and ξ and the almost sure constraint
that A(ζ)AT (ζ) = Id1 , where Id1 is the unit matrix in Rd1×d1, one can evaluate

(22)

Φη(t) = EheitTηi = e− 1

2 tTt,

t ∈ Rd1

(23)

thus concluding that the marginal distribution of η is indeed N (0, Id1 ) and that
the standard Hermite polynomial chaos expansions remain valid.

2.5. Extension to inﬁnite-dimensional spaces

In the previous subsections we have developed our basis adaptation method-
ology by initially taking the Gaussian Hilbert space G to be a ﬁnite dimensional
space. In this section we demonstrate that this can be viewed as a special case
of a space G that is of arbitraty dimension (countable or uncountable inﬁnite
dimensional). In order to do this, ﬁrst it is essential to provide some further in-
sight on the construction of such spaces. Next we will show that, for a family of
isometries {A(x)}x∈D, under suitable topological conditions, the elements of the
tranformed basis can be viewed as Gaussian ﬁelds that admit a Karhunen-Loeve
type expansion in terms of the initial basis.

We start with a necessary deﬁnition:
Deﬁnition 1. For any H real Hilbert space, we say that the Gaussian Hilbert

space G is indexed by H if there is a linear isometry χ 7→ ξχ, from H to G.
This deﬁnition provides a natural way to construct G, given some H. Namely,
if {ei}i∈I is a basis for H and {ξi}i∈I is a set of uncorrelated standard normal
variables with common index set I, then the mapping χ := P χiei 7→ P χiξi
is an isometry and G := span{ξi}i∈I is a Gaussian Hilbert space indexed by H.
Of course, in order for the above to make sense we need the sum P χiξi to be
deﬁned. In the case where H is ﬁnite dimensional, then P χiξi ∈ N (0,||χ||2
H )
and more generally, if ξ = (ξ1, ..., ξd) is H-valued, then the map χ 7→ hξ, χiH ∼
N (0,||χ||2
i=1 to be
scalar standard normal variables. This is actually the case upon which our
methodology has been built.

H ) deﬁnes an isometry. For instance, let H := Rd and {ξi}d

The main diﬃculty when H is inﬁnite dimensional is to ensure the existence
of some ξ = {ξi}i∈I such that hξ, χiH ∼ N (0,||χ||2
H ) for all χ ∈ H. Gaussian
measures on inﬁnite dimensional spaces are deﬁned in terms of real measures on
their dual space [9, 28]. In practice this means that often there is no H-valued
In the countable case, the construction of G then can
Gaussian variable ξ.

8

be obtained with the following procedure (see [19, 9] for technical details): A
locally convex topological vector space X must be identiﬁed such that H ⊂ X
for which there is a continuous inclusion mapping T : H 7→ X which is a
Hilbert-Schmidt operator. Then, we have that X ∗ ⊂ H and subsequently we
obtain the Gelfand triple X ∗ ⊂ H ⊂ X . It is possible to choose ξ ∈ X such
H ) for any χ ∈ X ∗ and we deﬁne the Gaussian Hilbert
that hξ, χiX ∼ N (0,||χ||2
space as G0 = {hξ, χiX|χ ∈ X}. Then the mapping ξ → hξ, χiX from X ∗ to
G0 is an isometry and by continuity it can be extended from H to the closure
G = G0. Then G is indexed by H. For an example, let H := ℓ2(N) the set of
real square summable sequences {an}n∈N and take ξ = (ξ1, ξ2, ...) with ξn i.i.d.
N (0, 1). Then clearly ξ /∈ l2(N) and we take X := R∞ and X ∗ = P∞i=1
R and
P∞n=1 anξn ∼ N (0,P a2
n) where the sum converges a.s. The basis adaptation
procedure here would consist of selecting an orthonormal basis {en}n∈N on ℓ2(N)
and then deﬁning the isometry A : ξ 7→ η with ηn =P ei
nξi. If {ξn}n∈N is an
orthonormal basis on G, then so is {ηn}n∈N and any Wiener chaos expansion
of elements in L2(Ω,F (G), P) can be taken with respect to the new basis. A
construction of a space G for the uncountably inﬁnite case can be found in [18].
At last, motivated by the adaptation schemes presented above, we explore
the case where an isometry is chosen to depend on parameters x, in a more
abstract setting. For simplicity, we consider the countably inﬁnite dimen-
sional case, however, all the theorems that we recall and prove below are also
valid in the uncountably inﬁnite case and we only need to interpret the in-
ﬁnite sums as limits of nets in L2. Let G be a Gaussian Hilbert space in-
dexed by a real Hilbert space H and D be any topological space. Let also
B = {{en}n∈N, orthonormal basis in H} the space of all orthonormal bases in
H and assume there is a map A : D 7→ B that is continuous and onto. Then
for any basis {en}n∈N there is x ∈ D such that A(x) = {en}n∈N and we write
{en(x)}n∈N, that is we assume that D is a continuous parameterization of the
space of rotations in H, therefore any basis can be indexed by some x ∈ D.
Moreover, in order to maintain the Hilbert-Schmidt structure of the kernels de-
ﬁned below we will assume that the entries of ||ei
n(x)||L2(D) ≤ ∞ for all the
entries of en(x). Let X ∗ ⊂ H ⊂ X be a Gelfand triple as described above,
ξ ∈ X an orthonormal basis in G and let
ηn(x) =Dξ, en(x)EH

ei
n(x)ξi, n ∈ N.

(24)

=

∞Xi=1

In what follows, for the sake of simplicity we drop n and we refer to an abritrary
component η(x) unless there is a need for further clariﬁcation. We have the
following lemma:

Lemma 1. For η(x) as above we have that G = span{η(x)}x∈D.
Proof. Detailed proof in Appendix C. (cid:3)
Now for given η(x) and for each ξ ∈ H deﬁne the mapping R : D 7→ R with
(25)

= E[ξη(x)]

Rη(ξ)(x) =Dξ, η(x)EG

9

and the Cameron-Martin space, corresponding to η
Rη(G) = {Rη(ξ) : ξ ∈ H}

(26)

which is the space of such mappings. Then we have the following:

Theorem 2. Let η(x) deﬁned as in Eq. (24). Then {ei(x)}i∈N spans the
Cameron-Martin space corresponding to η(x).
Proof. Detailed proof in Appendix D. (cid:3)
The above theorem essentially implies that for any n ∈ N, the ηn(x) obtained
after a change of basis transformation throught the isometries A(x), x ∈ D,
are Gaussian processes and the expression (24) is their Karhunen-Loeve type
expansion with a number of terms equal to the dimension of G. Consequently,
in ﬁnite dimensional spaces, the expansion consists of ﬁnite terms and their
corresponding covariance kernels of the form (11) have at most ﬁnitely many
positive eigenvalues.

3. Numerical examples

3.1. Elliptic PDE

We consider the following elliptic PDE

−∇ (κ(x, ξ) · ∇u(x, ξ)) = g(x), x ∈ D
(κ(x, ξ)∇u(x, ξ)) · n = 0,
x ∈ ∂D

(27)

that can be thought of as the pressure equation in a single ﬂow problem with
no-ﬂux boundary conditions. The transmissivity tensor κ(x, ξ) is modeled as
a random process, g(x) is a term that describes sinks and sources and n is the
unit vector, perpendicular to the boundary. In addition, the condition

Z∂D

u(x)dx = 0

(28)

is imposed to ensure well-posedness of the boundary-value problem.
In this
2-dimensional setting we take D = [0, 400]2 which is discretized in a 40 × 40
rectangular grid and we place a source and a sink at xso = (0, 0) and xsi =
(400, 400) respectively by taking

g(x) = s exp"−

1
2

2

Xi=1

so)2

(xi − xi
l2
i

# − s exp"−

1
2

si)2

(xi − xi
l2
i

#

2

Xi=1

(29)

with s = 0.5, l1 = l2 = 20. In what follows, equation (27) is solved using a
two-point ﬂux-approximation ﬁnite-volume scheme [1].

As the prior model of the transmissivity, we take κ = (κx, κy, κz) to be
isotropic (κx = κy = κz := κ0) where the components are a log-normally
distributed process, that is κ0(x, ξ) = exp (G(x, ξ)) where G(x, ξ) is a Gaus-
sian ﬁeld. We parameterize G(x, ξ) by considering its Karhunen-Loeve (KL)
expansion

∞Xi=1pλiξigi(x)

G(x, ξ) = G0(x) +

10

(30)

where {λi}i≥0 and {gi(x)}i≥0 are the eigenvalues and eigenvectors respectively
of its covariance kernel, which is taken to be a squared exponential kernel

k(x, y) = σ2 exp"−

1
2

(xi − yi)2

ℓ2
i

# .

2

Xi=1

(31)

For the sake of simplicity we take G0(x) = 0, whereas the kernel parameters
are σ2 = 0.5, ℓ1 = ℓ2 = 80. Then we truncate the KL expansion such that it
retains a 97% of the energy. That reduces to a ﬁnite expansion with 20 terms
therefore we have ξ ∈ Rd with d = 20.
u(x, ξ) = Xα∈J3

Next, a 3rd-order polynomial chaos expansion

uα(x)ψα(ξ)

(32)

of the solution of Eq. (27) was contructed. Due to the relatively high dimen-
sionality of the input, an ensemble of N = 105 Monte Carlo samples of the
20-dimensional Gaussian input was used in order to estimate the coeﬃcients

uα(x) =Du(x, ξ)ψα(ξ)E ≈

1
N

N

Xn=1

u(x, ξ(n))ψα(ξ(n)).

(33)

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

5 10 15 20 25 30 35

5 10 15 20 25 30 35

5 10 15 20 25 30 35

5 10 15 20 25 30 35

0.003
0.000
−0.003
−0.006
−0.009
−0.012
−0.015
−0.018
−0.021
−0.024

0.180

0.135

0.090

0.045

0.000

−0.045

−0.090

−0.135

0.045
0.030
0.015
0.000
−0.015
−0.030
−0.045
−0.060
−0.075
−0.090

0.090
0.072
0.054
0.036
0.018
0.000
−0.018
−0.036
−0.054

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

5 10 15 20 25 30 35

5 10 15 20 25 30 35

5 10 15 20 25 30 35

5 10 15 20 25 30 35

0.075
0.060
0.045
0.030
0.015
0.000
−0.015
−0.030
−0.045

0.064

0.032

0.000

−0.032

−0.064

−0.096

−0.128

−0.160

0.315
0.270
0.225
0.180
0.135
0.090
0.045
0.000
−0.045

0.06

0.04

0.02

0.00

−0.02

−0.04

−0.06

−0.08

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

5 10 15 20 25 30 35

5 10 15 20 25 30 35

5 10 15 20 25 30 35

5 10 15 20 25 30 35

0.016

0.000

−0.016

−0.032

−0.048

−0.064

−0.080

−0.096

0.72
0.63
0.54
0.45
0.36
0.27
0.18
0.09
0.00

0.16

0.12

0.08

0.04

0.00

−0.04

−0.08

−0.12

0.090
0.072
0.054
0.036
0.018
0.000
−0.018
−0.036
−0.054
−0.072

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

5 10 15 20 25 30 35

5 10 15 20 25 30 35

5 10 15 20 25 30 35

5 10 15 20 25 30 35

0.064

0.032

0.000

−0.032

−0.064

−0.096

−0.128

0.16

0.12

0.08

0.04

0.00

−0.04

−0.08

−0.12

0.30
0.24
0.18
0.12
0.06
0.00
−0.06
−0.12
−0.18
−0.24

0.09
0.06
0.03
0.00
−0.03
−0.06
−0.09
−0.12
−0.15

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

5 10 15 20 25 30 35

5 10 15 20 25 30 35

5 10 15 20 25 30 35

5 10 15 20 25 30 35

0.04

0.02

0.00

−0.02

−0.04

−0.06

−0.08

−0.10

0.24

0.18

0.12

0.06

0.00

−0.06

−0.12

−0.18

0.048

0.024

0.000

−0.024

−0.048

−0.072

−0.096

−0.120

0.072
0.054
0.036
0.018
0.000
−0.018
−0.036
−0.054
−0.072

Figure 1: The ﬁrst 20 eigenvectors of the covariance kernel of η1.

11

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

5 10 15 20 25 30 35

5 10 15 20 25 30 35

5 10 15 20 25 30 35

5 10 15 20 25 30 35

0.00
−0.06
−0.12
−0.18
−0.24
−0.30
−0.36
−0.42
−0.48

0.52
0.46
0.40
0.34
0.28
0.22
0.16
0.10
0.04

0.06
0.00
−0.06
−0.12
−0.18
−0.24
−0.30
−0.36
−0.42

0.152
0.140
0.128
0.116
0.104
0.092
0.080
0.068
0.056
0.044

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

5 10 15 20 25 30 35

5 10 15 20 25 30 35

5 10 15 20 25 30 35

5 10 15 20 25 30 35

0.18
0.09
0.00
−0.09
−0.18
−0.27
−0.36
−0.45
−0.54
−0.63

0.096

0.072

0.048

0.024

0.000

−0.024

−0.048

−0.072

0.360
0.315
0.270
0.225
0.180
0.135
0.090
0.045
0.000

0.270
0.225
0.180
0.135
0.090
0.045
0.000
−0.045
−0.090

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

5 10 15 20 25 30 35

5 10 15 20 25 30 35

5 10 15 20 25 30 35

5 10 15 20 25 30 35

0.360
0.315
0.270
0.225
0.180
0.135
0.090
0.045
0.000

0.048

0.024

0.000

−0.024

−0.048

−0.072

−0.096

0.06

0.00

−0.06

−0.12

−0.18

−0.24

−0.30

0.24
0.21
0.18
0.15
0.12
0.09
0.06
0.03
0.00

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

5 10 15 20 25 30 35

5 10 15 20 25 30 35

5 10 15 20 25 30 35

5 10 15 20 25 30 35

0.600
0.525
0.450
0.375
0.300
0.225
0.150
0.075
0.000
−0.075

0.32

0.24

0.16

0.08

0.00

−0.08

−0.16

−0.24

0.40

0.32

0.24

0.16

0.08

0.00

−0.08

−0.16

0.120
0.096
0.072
0.048
0.024
0.000
−0.024
−0.048
−0.072
−0.096

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

35

30

25

20

15

10

5

0
0

5 10 15 20 25 30 35

5 10 15 20 25 30 35

5 10 15 20 25 30 35

5 10 15 20 25 30 35

0.12
0.06
0.00
−0.06
−0.12
−0.18
−0.24
−0.30
−0.36
−0.42

0.224

0.192

0.160

0.128

0.096

0.064

0.032

0.000

0.000

−0.016

−0.032

−0.048

−0.064

−0.080

−0.096

−0.112

0.18

0.12

0.06

0.00

−0.06

−0.12

−0.18

−0.24

Figure 2: The 20 entries of the ﬁrst row of A(x) for the Gaussian adaptation.

3.1.1. Gaussian adaptation

First we construct the 1-dimensional adapted 2nd-order series

uA(x)(η) = uA(x)

0

+ uA(x)

1

η + uA(x)

2

η2 − 1
√2

(34)

using as A(x) family of isometries where the ﬁrst row is deﬁned as in Eq. (9),
that is the Gaussian adaptation. The kernel of the transformed input η, that is
k1(x, y) = a1(x)a1(y)T , has 20 strictly positive eigenvalues while the rest are
zero as was proved in the previous section. As expected, η has unit variance at
each location, k1(x, x) = 1 and the covariance takes smaller values elsewhere.
Its eigenvectors are shown in Fig. 1 and the entries of the ﬁrst row of A(x) are
shown in Fig. 2, which are essentially the normalized coeﬃcients {uǫi(x)}20
i=1 as
indicated in Eq. (9).

The coeﬃcients in expression (34) are shown in Fig. 3. As it seems by
construction, the ﬁner scales of ﬂuctuation that can be seen in the coeﬃcients
of the full expansion, are merged within η and are captured by its distribution
and its covariance kernel while the coeﬃcients of the adapted expansion display
only the coarse behavior. Analytically, it can be seen for instance (see Eq
A.20) that the ﬁrst order coeﬃcient is nothing but the norm of the ﬁrst order
coeﬃcients of the full expansion. The black dots indicate 9 locations where a
comparison of the probability densities of {uA(xi)(η)}9
i=1 was
performed, the results of which are shown in Fig. 4. The density functions of
the two chaos expansions demonstrate good agreement among the two random
quantities, with those of the adapted expansions being slightly more peaked and
with lighter tails, due to the relatively large number of terms being essentially
neglected via projection. Note that while the initial series consists of 1771

i=1 and {uA(xi)(η)}9

12

terms, the adapted series consists of only 3! At last, Fig. 5 shows an example
of realizations of the velocity ﬁelds

v = −κ(x, ξ)∇u(x, ξ), x ∈ D

(35)

computed for both u(x, ξ) and uA(x)(η).

350

300

250

200

150

100

50

0

−2

−4

−6

−8

−10

−12

−14

−16

350

300

250

200

150

100

50

50 100 150 200 250 300 350

50 100 150 200 250 300 350

4.2

3.6

3.0

2.4

1.8

1.2

0.6

0.0

350

300

250

200

150

100

50

50 100 150 200 250 300 350

0.24

0.12

0.00

−0.12

−0.24

−0.36

−0.48

Figure 3: Coeﬃcients uA(x)
Gaussian adaptation.

iǫ1

, i = 0, 1, 2 of the second-order one-dimensional

Pdfs at observation points

−25 −20 −15 −10 −5

0

5

10

Figure 4: Gaussian adaptation: Comparison of the pdfs of {u(xi, ξ)}9
i=1 and
{uA(xi)(η)}9
i=1, where xi, i = 1, ..., 9 are the points of interest. The black dashed
line corresponds to the original chaos expansion u(x, ξ), while the purple line
indicates the adapted chaos exansion uA(x)(η).

13

2
x

35

30

25

20

15

10

5

0
0

35

30

25

20

2
x

15

10

5

0
0

5

10 15 20 25 30 35 40

x1

5

10

15 20

25 30

35

x1

0.600

0.525

0.450

0.375

0.300

0.225

0.150

0.075

0.000

0.54

0.48

0.42

0.36

0.30

0.24

0.18

0.12

0.06

0.00

2
x

35

30

25

20

15

10

5

0
0

35

30

25

20

2
x

15

10

5

0
0

5

10 15 20 25 30 35 40

x1

5

10

15 20

25 30

35

x1

0.56

0.48

0.40

0.32

0.24

0.16

0.08

0.00

0.48

0.42

0.36

0.30

0.24

0.18

0.12

0.06

0.00

Figure 5: Sample of velocity ﬁelds v corresponding to u(x, ξ) (left column) and
uA(x)(η) (right column). Top row shows vx1 and bottom row shows vx2.

350

300

250

200

150

100

50

50 100 150 200 250 300 350

350

300

250

200

150

100

50

0.32

0.24

0.16

0.08

0.00

−0.08

−0.16

−0.24

−0.32

350

300

250

200

150

100

50

0.096

0.072

0.048

0.024

0.000

−0.024

−0.048

350

300

250

200

150

100

50

A

B

C

50 100 150 200 250 300 350

50 100 150 200 250 300 350

0.18

0.16

0.14

0.12

0.10

0.08

0.06

0.04

0.02

0.00

0.24

0.21

0.18

0.15

0.12

0.09

0.06

0.03

0.00

350

300

250

200

150

100

50

0.21

0.18

0.15

0.12

0.09

0.06

0.03

0.00

50 100 150 200 250 300 350

50 100 150 200 250 300 350

Figure 6: Coeﬃcients uA(x)
quadratic adaptation.

ii

, i = 1, ..., 5 of the second-order 5-dimensional

14

Pdfs at observation points

−25 −20 −15 −10 −5

0

5

10

Figure 7: Quadratic adaptation: Comparison of the pdfs of {u(xi, ξ)}9
i=1 and
{uA(xi)(η)}9
i=1, where xi, i = 1, ..., 9 are the points of interest. The black dashed
line corresponds to the original chaos expansion u(x, ξ), while the purple line
indicates the adapted chaos exansion uA(x)(η).

3.1.2. Quadratic adaptation

Next we construct a 5-dimensional quadratic adaptation, that is

uA(x)(η) = uA(x)

0

+

uA(x)
i

ηi +

5

Xi=1

uA(x)
ii

(η2

i − 1)
√2

,

5

Xi=1

(36)

ii

where A is constructed such that is satisﬁes Eq. (10). The quadratic adaptation
can be seen [32] to have exactly the same sum of the polynomial terms up to or-
der two with those of the full expansion without essentially discarding any terms
via projection and the second order coeﬃcients uA(x)
are proportional to the
eigenvalues of S (shown in Fig. 6). Due to the small order of our full expansion,
this might be expexted to adapt better than the Gaussian adaptation, given also
that we include an expansion with higher dimensionality than the 1-dimensional
Gaussian adaptation. Comparison of the density functions at 9 locations with
those of the full expansion can be seen in Fig. 7 which veriﬁes our argument and
shows particularly a better agreement between the tails of the two pdfs. The
two adaptations are also compared with themselves at three locations, labeled
A, B and C (shown in uA(x)
- Fig. 6) and the results are shown in Fig. 8 where
this time the distributions of a 5- and 10-dimensional Gaussian adaptations
are plotted together with the 5-dimensional quadratic adaptation. Again, good
agreement can be seen between the 3 pdfs with the quadratic adaptation being
slightly closer to the true distribution. Another interesting characteristic here is
that as we keep increasing the dimensionality of the expansion by adding only
terms of 1-dimensional series, that is, dropping polynomial terms that depend

22

15

jointly on two or more ξ’s, the contribution is small and it seems that the joint
terms are essential in achieving a full distributional equality (in fact the equality
will be almost surely). However, the agreement shown here can be considered
suﬃcient for estimating various statistics of interest. Further investigation in
order to identify the suitable rotations to optimally adapt the expansion while
maintaining low dimensionality could be pursued by minimizing an error func-
tion of the form (15),(16) or within the context of active subspaces [4] and is
beyond the scope of this work.

0.16

0.14

0.12

0.10

0.08

0.06

0.04

0.02

Full
5d-Quadr
5d-Gauss
10d-Gauss

0.16

0.14

0.12

0.10

0.08

0.06

0.04

0.02

0.00

−30 −25 −20 −15 −10 −5

0

5

10

15

0.00

−30 −25 −20 −15 −10 −5

0

5

10

15

0.16

0.14

0.12

0.10

0.08

0.06

0.04

0.02

0.00

−30 −25 −20 −15 −10 −5

0

5

10

15

Figure 8: Comparison of the Gaussian and quadratic adaptation for diﬀerent
choices of the dimension at the three locations A (up left), B (up right) and C
(bottom).

3.1.3. Adaptation on expansion with random coeﬃcients

At last, we test our approach on a reduced chaos expansion with random co-
eﬃcients as given in Eq. (20) where we have arbitrarily chosen ˆξ = (ξ1, ξ2, ξ3, ξ4)
and ˆζ = (ξ5, ..., ξ20). Although this can be seen as a way to separate ﬁne and
coarse random ﬂuctuations (and this is in fact our motivation behind this con-
struction, as was introduced in [31]), we do not claim this to be the case here
since the inﬂuence of the ﬁrst four ξ’s is not necessarily signiﬁcantly dominating
in this particular permeability model due to the relatively low correlation lengths
ℓ1, ℓ2. We restrict ourselves in presenting only how the adaptation methodology
applies in such a case and leave the costruction of a more illustrating example

16

for another paper. The 4-dimensional third-order expansion with respect to ˆξ
with the coeﬃcients being dependent on ˆζ is,

u(x, ˆξ, ˆζ) = U0(x, ˆζ) +P16
+Pα,|α|=2 Uα(x, ˆζ)ψα(ˆξ) +Pα,|α|=3 Uα(x, ˆζ)ψ(ˆξ)

i=1 Uiǫ1 (x, ˆζ)ψiǫ1 (ξi)+

where Uα are given in Eq. (21). We use again the Gaussian adaptation scheme
to construct a 1-dimensional second order expansion

(37)

uA(ζ)(ˆξ, η) = UA(x,ζ)

0

+ UA(x,ζ)

1

η + UA(x,ζ)

2

η2 − 1
√2

(38)

Note here that only the 4-dimensional ˆξ has been merged into a 1-dimensional η
while the inﬂuence of all dimensions incorporated in ˆζ is present both in the co-
eﬃcients and in the polynomials through the isometry A(x, ζ). The estimated
expected values of the adapted coeﬃcients UA(x,ζ)
are shown in Fig. 9. The
density functions shown in Fig. 10 are constructed by simultanesously sampling
from ˆζ and ˆξ, then evaluating Uα and A based on the values of ˆζ and sub-
sequently computing the coeﬃcients of the adapted expansion that at last are
evaluated on ˆξ. Again very good agreement is observed when compared to the
pdfs of the full expansions. Since we have only applied the basis rotation on 4
dimensions, upon re-expanding the series, this is a 17-dimensional expansion.

i

350

300

250

200

150

100

50

50 100 150 200 250 300 350

0.0

−2.4

−4.8

−7.2

−9.6

−12.0

−14.4

−16.8

350

300

250

200

150

100

50

0.00

−0.03

−0.06

−0.09

−0.12

−0.15

−0.18

−0.21

350

300

250

200

150

100

50

50 100 150 200 250 300 350

50 100 150 200 250 300 350

0.084

0.072

0.060

0.048

0.036

0.024

0.012

0.000

second-order one-dimensional Gaussian adaptation of u(x, ˆξ, ˆζ) coeﬃcients. As

Figure 9: Expectation of the random coeﬃcients EhUA(x,ζ)
expected EhUA(x,ζ)

i = uA(x)

0

0

= u0(x).

i

i, i = 0, 1, 2 of the

3.2. Inﬁnite dimensional expansion with geometric series coeﬃcients

We consider a simple random process that is written as a function of an

inﬁnite number of Gaussians given as

where

u(x, ξ) =

∞Xn=1

bn(x)ξn +  ∞Xn=1

bn(x)ξn!2

bn(x) = x(n−1)/2, x ∈ (−1, 1).

17

(39)

(40)

Pdfs at observation points

−25 −20 −15 −10 −5

0

5

10

z-coordinate

Figure 10: Adaptation with random coeﬃcients: Comparison of the pdfs of
{u(xi, ξ)}9
i=1, where xi, i = 1, ..., 9 are the points of interest.
The black dashed line corresponds to the original chaos expansion u(x, ξ), while
the purple line indicates the adapted chaos exansion uA(x)(η).

i=1 and {uA(xi)(η)}9

Since the sum of coeﬃcients bn is square summable with P∞n=1 b2
1−x , then
u(x, ξ) < +∞ a.s.
1−x ) and the variance of
the summand blows up for x → ±1. We apply the 1-dimensional Gaussian
adaptation which consists of transforming ξ to

for |x| < 1 with P bnξn ∼ N (0,

n = 1

1

and using expressions (A.20) and (A.21) we take

(P∞n=1 bn(x)2)1/2

η =

1

bn(x)ξn

(41)

∞Xn=1

u(x, η) = u1(x)η + u2(x)

(η2 − 1)

√2

where

u1(x) = 1√1−x
u2(x) = 1

1+x + √2 x

1−x2 .

(42)

(43)

Our goal is to compare the above analytical 1-dimensional adaptation with
two truncated versions. First, the summations in the initial representation (Eq.
(39)) are truncated at d terms

ud(x, ξ) =

d

Xn=1

bn(x)ξn +  d
Xn=1

bn(x)ξn!2

,

(44)

18

1.0

0.8

0.6

0.4

0.2

0.0

−5

0.35

0.30

0.25

0.20

0.15

0.10

0.05

u(x,η)
u(x,ˆη)
ˆud (x,ηd )

0

5

10

15

0.25

0.20

0.15

0.10

0.05

0.00

−10 −5

0

5

10

15

20

25

0.00

−10 −5

0

5

10

15

20

25

0.25

0.20

0.15

0.10

0.05

0.07

0.06

0.05

0.04

0.03

0.02

0.01

0.040

0.035

0.030

0.025

0.020

0.015

0.010

0.005

0.00

−80 −60 −40 −20

0

20

40

0.00

−80 −60 −40 −20

0

20

40

60

80

0.000

−80 −60 −40 −20

0

20

40

60

80

Figure 11: Top: Adaptation u(x, η) at x = 0.3 and its truncations using d = 10.
Middle: Adaptation at x = 0.9 and its truncations using d = 10 (left) and
d = 50 (middle). Bottom: Adaptation at x = 0.99 and its truncations using
d = 10 (left), d = 50 (middle) and d = 100 (right).

which after adaptation gives

(45)

(46)

(47)

where

and

d − 1(cid:1)√2

,

ˆud(x, ηd) = ˆu1ηd + ˆu2(x)(cid:0)η2
1−x (cid:17)1/2
(1−xd)(1+x) +

1−x2d

√2

ˆu1(x) =(cid:16) 1−xd

ˆu2(x) =

1−x (cid:17)
1−x2 − xd(1−xd)

1−xd (cid:16) x(1−x2d)
Xn=1

d

1

n=1 bn(x)2(cid:17)1/2
(cid:16)Pd

ηd =

bn(x)ξn.

Note here that ˆui(x) → ui(x), i = 1, 2 as d → ∞. Second, the adapted expansion
(42) is replaced by one where only the input η is truncated, depending only on
d terms, that is

u(x, ˆη) = u1(x)ˆη + u2(x)

19

(ˆη2 − 1)

√2

,

(48)

with

ˆη =

1

(P∞n=1 bn(x)2)1/2

bn(x)ξn.

(49)

d

Xn=1

Note that in the ﬁrst truncation, although the dimensionality is initially re-
duced to d terms, the adaptation procedure enforces η to be standard normally
distributed by construction while in the second truncation the truncated ˆη is
no longer standard normal (in fact it is N (0, 1 − xd)) but it shares the same
coeﬃcients with (42).
The pdfs of the three expansions are shown in Fig. 11 for various choices of
x and the truncation order d. Although for small choices of x the terms bn(x)
decay fast and both approximations behave well, as x approaches 1, the dis-
crepancy of ˆud(x, ηd) from u(x, η) increases dramatically while u(x, ˆη) remains
suﬃciently close, thus making a better approximation. This illustrates the fact
that a ﬁnite order truncation of the polynomial chaos expansion prior to any
adaptation can behave poorly compared to a truncation that takes place after
adapting the expansion. Note also that in this example the coeﬃcients decrease
geometrically and therefore their inﬂuence in the probability density of the QoI
u(x, η) vanishes rapidly. The consequences of such truncations can be even more
severe in a case where all coeﬃcients are of signiﬁcant importance.

4. Conclusions

We have presented a new formulation of random processes and random ﬁelds
using as starting point a homogeneous chaos expansion which allows merging
the dimensions of the initial functional without deformation of its probability
density structure. The tranformed input variables can be seen as an input ran-
dom ﬁeld with richer information about the quantity of interest than the simple
standard Gaussian inputs, that we think of as an intermediate scale between the
input and the chaos expansion. This novel represention has signiﬁcant potential
as a dimensionality reduction technique and can allow the exploration of higher
dimensional polynomial chaos expansions that appear in physical systems, an
area that undoubtedly has suﬀered a lot by the curse of dimensionality.

Appendix A. Computation of the coeﬃcients qA

β (x)

Appendix A.1. Derivation of the general formula

Our goal is to derive an explicit expression for the coeﬃcients uA

β deﬁned as

uA

β = Xα∈Jp

uαDψα(ξ), ψβ(Aξ)E, β ∈ Jp.

(A.1)

In order to prove our main result, we introduce some necessary tools that will
allow us to proceed. Let πn : L2(Ω) → G⋄n be the orthogonal projection of

20

L2(Ω) onto G⋄n. The Wick product for Gaussian variables ξi, ..., ξn denoted
with ⋄, is
(A.2)
that is the projection of the ordinary product ξ1 ··· ξn onto G⋄n. For the case
where ξ1 = .... = ξn we write ξ⋄n = ξ1 ⋄ ··· ⋄ ξn.
It is easy to see [19], for
instance, that for ξ ∼ N (0, 1), we have ξ⋄n = hn(ξ) and that for any {ξi}d
orthonormal basis in G, α ∈ J ,

ξ1 ⋄ ··· ⋄ ξn = πn(ξ1 ··· ξn)

i=1

ξ⋄α1
1

⋄ ··· ⋄ ξ⋄αd

d =

d

Yi=1

hαi (ξi) = hα(ξ).

(A.3)

A Feynman diagram γ of order n and rank r is a graph consisting of n vertices
and r edges such that no two edges share a common vertex. That means that
there are always 2r paired vertices and n − 2r unpaired ones. The diagram is
called complete when r = n/2. A graph where each vertex is labelled with a
Gaussian random variable ξi, i = 1, ..., n is said to have value

v(γ) =

r

Yk=1Dξik , ξjkEYi∈C

ξi

(A.4)

where (ξik , ξjk ), k = 1, ..., r are the pairs of vertices and C is the set of unpaired
ones. Clearly, when γ is complete, C is empty and v(γ) is a constant. Given
the above deﬁnitions, we can present the following ([19], Th. 3.12):

Proposition 1. Let {ζij}1≤i≤k,1≤j≤li be real jointly Gaussian random vari-

ables and deﬁne Yi = ζi1 ⋄ ··· ⋄ ζili

, then

E [Y1 ··· Yk] =Xγ

v(γ)

(A.5)

where the sum is taken over all complete Feynman diagrams such that no edge
joins any ζi1j1 , ζi2j2 with i1 = i2.

This is also known as Wick’s theorem [34]. Taking this into account, our

main result follows:

Proposition 2. Let {ξi}d

be an isometry and take any α, β ∈ J . Let also {ηi}d
Then

i=1 be an orthonormal basis in G, A : Rd → Rd
i=1 be such that η = Aξ.

Dhα(ξ), hβ(η)E =(cid:26) PAnQn

0,

k=1 aik,jk ,

|α| = |β|
|α| 6= |β|

(A.6)

where aik,jk are entries of A and the sum is taken over An, which is the number
of possible ways to choose n entries of A such that exactly αi of them are in the
ith column and βi of them are ith row, simultaneously, for all i = 1, ..., d.

Proof. Deﬁne {ζij}1≤i≤k,1≤j≤li with k = 2, l1 = |α| := n, l2 = |β| := m

where

{ζ1j}n

ξ1, ..., ξ1

j=1 :=
| {z }


α1

, ..., ξd, ..., ξd

αd

| {z }

,




(A.7)

21

Then for Y1 := ξ⋄α1
1
Prop. 1 gives that

j=1 :=


{ζ2j}m

η1, ..., η1

, ..., ηd, ..., ηd

β1

βd

|

|

}

{z

d = hα(ξ) and Y2 := η⋄α1

{z
⋄ ··· ⋄ ξ⋄αd
Dhα(ξ), hβ(η)E = E [Y1Y2] =Xγ

1


}


.

(A.8)

⋄ ··· ⋄ η⋄αd

d = hβ(η),

v(γ)

(A.9)

j=1 with {ζ2j}m

where the sum is taken over all complete Feynman diagrams with edges that
connect {ζ1j}n
j=1. Clearly for n 6= m there is no such complete
Feynman diagram and the sum is zero. For n = m, any such diagram γ can be
represented by its pairs(cid:8)(ζ1j , ζ2lj )(cid:9)n

j=1 and has value

α1

n

n

v(γ) =

Yj=1Dζ1j , ζ2ljE =
Observe that any ζ2lj ∈ {ηs}d

Yj=1Dξ1, ζ2ljE α1+α2

Yj=α1+1Dξ2, ζ2ljE···

Yj=n−αdDξd, ζ2ljE.

(A.10)

s=1 and that for any ηs

Dξj, ηsE =Dξj,

d

Xr=1

asrξrE = as,j

(A.11)

that follows from η = Aξ and as,j is the (s, j)th entry of A. Therefore, substi-

tuting in the above equation and noting that exactly αi of the products D·,·E

include ξi and exactly βi include ηi we obtain that exactly αi and βi entries of
A will be taken from the ith column and ith row respectively, which completes
the proof.

(cid:3)

An immediate consequence of the above proposition, when one wants to
compute the coeﬃcients of a chaos expansion with respect to a rotated basis
η = Aξ, is that the sum is reduced to

uA

β = Xα∈Jp

uαDψα(ξ), ψβ(Aξ)E = Xα,|α|=|β|

uαDψα(ξ), ψβ(Aξ)E.

(A.12)

The above formula can be further simpliﬁed for the case of 1-dimensional poly-
nomials:

Corollary 1. For any n ∈ N, α ∈ J with |α| = n and i = 1, ..., d, we have

Dhα(ξ), hn(ηi)E = n!

aαk
i,k

d

Yk=1

(A.13)

Proof. Let β ∈ J with β = nǫi = (0, ..., n, ..., 0) and by working as in the

proof of Prop. 2 with

{ζ2j}n

j=1 =


,

(A.14)

ηi, ..., ηi


| {z }


n

22

it is easy to see that all complete Feynman diagrams take the same value, that
is

v(γ) =

αd

α1

Yj=1Dξ1, ηiE···

Yj=1Dξd, ηiE = aαi

i,1 ··· aαd

i,d =

d

Yk=1

aαk
i,k

(A.15)

and the total number of such diagrams is n!.

(cid:3)

Appendix A.2. Coeﬃcients for 1-dimensional subspaces

Taking into account Corollary 1 from the previous paragraph, we are now
ready to derive explicit formulas for the coeﬃcients along 1-dimensional sub-
spaces of chaos expansion uA(η). Namely, for any β ∈ J with β = nǫi =
(0, ..., n, ..., 0), i = 1, ..., d and n ∈ N, we have

uA

β = Xα,|α|=n
= Xα,|α|=n
= √n! Xα,|α|=n

uαDψα(ξ), ψn(ηi)E =
uα√α!√n!Dhα(ξ), hn(ηi)E =

uα√α!

d

Yk=1

aαk
i,k.

The coeﬃcients of ψβ(η) of order up to 3 are given by:

(A.16)

(A.17)

(A.18)

(A.19)

(A.20)

(A.21)

uA
0 = u0
d

uA
ǫi =

ai,kuǫk

d

Xk=1
Xk=1
Xk=1
+ √6

d

uA
2ǫi =

ukka2

ukjai,kai,j

d

j>k

i,k + √2

Xk=1
i,k + √3
Xk=1

j>k

d

uA
3ǫi =

ukkka3

ukkj a2

i,kai,j +

(A.22)

d

Xk=1

j>k
l>j

ukjlai,kai,jai,l

(A.23)

23

Appendix B. Proof of Theorem 1

We have

ZDZD |ki(x, y)|2dxdy =
aij (y)aik(y)dy ≤

L2(D)||aik||2

L2(D) < +∞

=

d

Xj,k=1ZD

aij(x)aik(x)dxZD
Xj,k=1
≤

||aij||2

d

where the second row is derived after applying the Cauchy-Schwarz inequality.

Appendix C. Proof of Lemma 1

Clearly by deﬁnition η(x) ∈ G for all x ∈ D since {ξi}i∈N forms a basis in
G, therefore span{η(x)}x∈D ⊂ G. On the oher hand, for any ξ ∈ G there exists
χ ∈ H such that

(C.1)

ξ =Dχ, ξEH

=Xi

χiξi

where χn = hχ, eniH with {en}n∈N some basis in H. Set x1 ∈ A−1 ({en}n∈N)
and for n ≥ 2 choose xn such that en = ˆe1(xn) where ˆe1(xn) is the ﬁrst basis
element of A(xn). This is possible since we can continuously rotate any basis
until its n-th element becomes the ﬁrst element of another basis. Then

ξ = Dχ, ξEH
= Xi

χiDe(xi), xEH

=Xi

=Xi Dχi, eiEHDei, ξEH

χiη(xi),

therefore ξ ∈ span{η(x)}x∈D and G ⊂ span{η(x)}x∈D which completes the
proof.

Appendix D. Proof of Theorem 2

It is known ([19], Theorem 8.15) that the linear mapping Rη(·)(x) is an
isometry from span{η(x)}x∈D to Rη(G) and by using Lemma 1 we have that
G and Rη(G) have the same dimension. Also ([19], Corollary 8.16) Rη(G) is
spanned by the covariance kernels

ky(x) = Rη(η(y))(x) = E[η(y)η(x)], y ∈ D

and ([19], Theorem 8.22) η(x) admits a representation

η(x) =

ρi(x)ξi

∞Xi=1

24

(D.1)

(D.2)

where {ρi}i∈N is a basis in Rη(G) and {ξi} a basis in span{η(x)}x∈D = G and
the limit is taken in L2. Let {yi}i∈N such that ρi(x) = kyi(x). From the proof
of Lemma 1 we can see that it is possible to choose yi such that η(yi) = ξi, all
i ∈ N. That is due to the fact that the isometry Rη(·)(x) will map the basis
{η(yi)}i∈N to a basis {kyi(x)}i∈N in Rη(G). Then

E[ξiη(x)]ξi

η(x) = Xi

ρi(x)ξi =Xi
= Xi Dξi, η(x)EG
= Xi

ei(x)ξi,

kyi(x)ξi =Xi
ξi =Xi Xj

ej(x)Dξi, ξjEG

ξi

from where we obtain ρi(x) = ei(x).

References

[1] J. Aarnes, T. Gimse, and K.-A. Lie. An introduction to the numerics of ﬂow
in porous media using matlab. Geometric Modelling, Numerical Simulation
and Optimization, pages 265–306, 2007.

[2] M. Arnst, R. Ghanem, and C. Soize. Identiﬁcation of bayesian posteriors
for coeﬃcients of chaos expansions. Journal of Computational Physics,
229:3134–3154, 2010.

[3] R. Cameron and W. Martin. The orthogonal development of nonlinear
functionals in series of fourier-hermite functionals. Annals of Mathematics,
48:385–392, 1947.

[4] P. G. Constantine, E. Dow, and Q. Wang. Active subspace methods in
theory and practice: applications to kriging surfaces. SIAM Journal on
Scientiﬁc Computing, 36:A1500–A1524, 2014.

[5] S. Das, R. Ghanem, and J.C. Spall. Asymptotic sampling distribution for
polynomial chaos representation from data: a maximum entropy and ﬁsher
information approach. SIAM Journal on Scientiﬁc Computing, 30:2207–
2234, 2008.

[6] C. Desceliers, R. Ghanem, and C. Soize. Maximum likelihood estimation
of stochastic chaos representations from experimental data. International
Journal for Numerical Methods in Engineering, 66:978–1001, 2006.

[7] A. Doostan and G. Iaccarino. A least-squares approximation of partial
diﬀerential equations with high-dimensional random inputs. Journal of
Computational Physics, 228:4332–4345, 2009.

25

[8] A. Doostan and H. Owhadi. A non-adapted sparse approximation of pdes
with stochastic inputs. Journal of Computational Physics, 230:3015–3034,
2011.

[9] I.M. Gelfand and N. Ya. Vilenkin. Generalized functions, Vol 4: Applica-

tions to Harmonic Analysis. Academic Press, 1964.

[10] R. Ghanem. Scales of ﬂuctuation and the propagation of uncertainty in

random porous media. Water Resources Research, 34:2123–2136, 1998.

[11] R. Ghanem. Ingredients for a general purpose stochastic ﬁnite elements im-
plementation. Computer Methods in Applied Mechanics and Engineering,
168:19–34, 1999.

[12] R. Ghanem and S. Dham. Stochastic ﬁnite element analysis for multiphase
ﬂow in heterogeneous porous media. Transport in Porous Media, 32:239–
262, 1998.

[13] R. Ghanem and R. Doostan. Characterization of stochastic system param-
eters from experimental data: A bayesian inference approach. Journal of
Computational Physics, 217:63–81, 2006.

[14] R. Ghanem and J. Red-Horse. Propagation of probabilistic uncertainty
in complex physical systems using a stochastic ﬁnite element approach.
Physica D: Nonlinear Phenomena, 133:137–144, 1999.

[15] R. Ghanem and P. Spanos. Stochastic ﬁnite elements: A spectral approach.

Springer-Verlag, 1991.

[16] R.G. Ghanem, A. Doostan, and J. Red-Horse. A probabilistic construc-
tion of model validation. Computer Methods in Applied Mechanics and
Engineering, 197:2585–2595, 2008.

[17] X. Huan and Y.M. Marzouk. Simulation-based optimal bayesian experi-
mental design for nonlinear systems. Journal of Computational Physics,
232:288–317, 2013.

[18] K. Itˆo. An elementary approach to malliavin ﬁelds. In Asymptotic problems
in probability theory: Wiener functionals and asymptotics, pages 35–89,
Essex, 1990. Sanda and Kyoto.

[19] S. Janson. Gaussian Hilbert spaces. Cambridge University Press, 1999.

[20] K. Karhunen. ¨Uber lineare methoden in der wahrscheinlichkeits-rechnung.
Annals of Academic Science Fennicade Series A1, Mathematical Physics,
37:3–79, 1946.

[21] O.P. Le Maˆıtre, M.T. Reagan, H.N. Najm, R.G. Ghanem, and O.M. Knio.
A stochastic projection method for ﬂuid ﬂow: Ii. random process. Journal
of Computational Physics, 181:9–44, 2002.

26

[22] M. Lo´eve. Probability Theory, D. Van Nostrand, Princeton, New Jersey,

1955.

[23] Y. M. Marzouk, H. N. Najm, and L. Rahn. Stochastic spectral methods for
eﬃcient bayesian solution of inverse problems. Journal of Computational
Physics, 224:560–586, 2007.

[24] Y.M. Marzouk and H.N. Najm. Dimensionality reduction and polynomial
chaos acceleration of bayesian inference in inverse problems. Journal of
Computational Physics, 228:1862–1902, 2009.

[25] H.G. Matthies and C. Bucher. Finite elements for stochastic media prob-
lems. Computer Methods in Applied Mechanics and Engineering, 168:3–17,
1999.

[26] J. Mercer. Functions of positive and negative type, and their connection
with the theory of integral equations. Philosophical Transactions of the
Royal Society of London. Series A, containing papers of a mathematical or
physical character, 209:415–446, 1909.

[27] H.N. Najm. Uncertainty quantiﬁcation and polynomial chaos techniques in
computational ﬂuid dynamics. Annual Review of Fluid Mechanics, 41:35–
52, 2009.

[28] J. Red-Horse and R. Ghanem. Elements of a functional analytic approach to
probability. International Journal for Numerical Methods in Engineering,
80(6-7):689–716, 2009.

[29] G. Saad and R. Ghanem. Characterization of reservoir simulation models
using a polynomial chaos-based ensemble kalman ﬁlter. Water Resources
Research, 45:Art. W04417, 2009.

[30] C. Soize and R. Ghanem. Physical systems with random uncertainties:
chaos representations with arbitrary probability measure. SIAM Journal
on Scientiﬁc Computing, 26:395–410, 2004.

[31] C. Soize and R. Ghanem. Reduced chaos decomposition with random co-
eﬃcients of vector-valued random variables and random ﬁelds. Computer
Methods in Applied Mechanics and Engineering, 198:1926–1934, 2009.

[32] R. Tipireddy and R.G. Ghanem. Basis adaptation in homogeneous chaos

spaces. Journal of Computational Physics, 259:304–317, 2014.

[33] P. Tsiliﬁs, R.G. Ghanem, and P. Hajali. Eﬃcient bayesian experimen-
tation using an expected information gain lower bound. arXiv preprint,
arXiv:1506.00053v2, 2015.

[34] G.C. Wick. The evaluation of the collision matrix. Physical Review, 80:268–

272, 1950.

27

[35] N. Wiener. The homogeneous chaos. American Journal of Mathematics,

60:897–936, 1938.

[36] D. Xiu and G.E. Karniadakis. The wiener–askey polynomial chaos for
stochastic diﬀerential equations. SIAM Journal on Scientiﬁc Computing,
24:619–644, 2002.

[37] D. Xiu and G.E. Karniadakis. Modeling uncertainty in ﬂow simulations via
generalized polynomial chaos. Journal of Computational Physics, 187:137–
167, 2003.

28

