6
1
0
2

 
r
a

M
2

 

 
 
]

G
L
.
s
c
[
 
 

1
v
0
7
5
0
0

.

3
0
6
1
:
v
i
X
r
a

Without-Replacement Sampling for Stochastic Gradient Methods:
Convergence Results and Application to Distributed Optimization

Ohad Shamir

Weizmann Institute of Science

ohad.shamir@weizmann.ac.il

Abstract

Stochastic gradient methods for machine learning and optimization problems are usually analyzed
assuming data points are sampled with replacement. In practice, however, sampling without replace-
ment is very common, easier to implement in many cases, and often performs better. In this paper, we
provide competitive convergence guarantees for without-replacement sampling, under various scenarios,
for three types of algorithms: Any algorithm with online regret guarantees, stochastic gradient descent,
and SVRG. A useful application of our SVRG analysis is a nearly-optimal algorithm for regularized
least squares in a distributed setting, in terms of both communication complexity and runtime complex-
ity, when the data is randomly partitioned and the condition number can be as large as the data size
(up to logarithmic factors). Our proof techniques combine ideas from stochastic optimization, adversar-
ial online learning, and transductive learning theory, and can potentially be applied to other stochastic
optimization and learning problems.

1 Introduction

Many canonical machine learning problems boil down to solving a convex empirical risk minimization
problem of the form

F (w) =

min
w∈W

1
m

fi(w),

(1)

m

Xi=1

where each individual function fi(·) is convex (e.g. the loss on a given example in the training data), and the
set W ⊆ Rd is convex. In large-scale applications, where both m, d can be huge, a very popular approach is
to employ stochastic gradient methods. Generally speaking, these methods maintain some iterate wt ∈ W,
and at each iteration, sample an individual function fi(·), and perform some update to wt based on ∇fi(wt).
Since the update is with respect to a single function, this update is usually computationally cheap. Moreover,
when the sampling is done independently and uniformly at random, ∇fi(wt) is an unbiased estimator of
the true gradient ∇F (wt), which allows for good convergence guarantees after a reasonable number of
iterations (see for instance [33, 5, 9, 29]).
However, in practical implementations of such algorithms, it is actually quite common to use without-
replacement sampling, or equivalently, pass sequentially over a random shufﬂing of the functions fi. Intu-
itively, this forces the algorithm to process more equally all data points, and often leads to better empirical
performance. Moreover, without-replacement sampling is often easier and faster to implement, as it requires
sequential data access, as opposed to the random data access required by with-replacement sampling (see
for instance [6, 7, 17, 30, 19]).

1

1.1 What is Known so Far?

Unfortunately, without-replacement sampling is not covered well by current theory. The challenge is that
unlike with-replacement sampling, the functions processed at every iteration are not statistically indepen-
dent, and their correlations are difﬁcult to analyze. Since this lack of theory is the main motivation for our
paper, we describe the existing known results in some detail, before moving to our contributions.

To begin with, there exist classic convergence results which hold deterministically for every order in
which the individual functions are processed, and in particular when we process them by sampling without
[28]). However, these can be exponentially worse than those obtained using random
replacement (e.g.
without-replacement sampling, and this gap is inevitable (see for instance [30]).

More recently, Recht and R´e [30] studied this problem, attempting to show that at least for least squares
optimization, without-replacement sampling is always better (or at least not substantially worse) than with-
replacement sampling on a given dataset. They showed this reduces to a fundamental conjecture about
arithmetic-mean inequalities for matrices, and provided partial results in that direction, such as when the
individual functions themselves are assumed to be generated i.i.d. from some distribution. However, the
general question remains open.

Quite recently, G¨urb¨uzbalaban et al. [19] provided a new analysis of gradient descent algorithms for
solving Eq. (1) based on random reshufﬂing: Each epoch, the algorithm draws a new permutation on
{1, . . . , m} uniformly at random, and processes the individual functions in that order. When these are
quadratic functions, the authors managed to obtain ﬁnite-time convergence guarantees, which are in fact
better than with-replacement sampling as a function of the number of epochs k: As much as O(1/k2),
whereas with-replacement sampling only yields O(1/k). Moreover, the result is extendible to strongly con-
vex and smooth functions. Although a remarkable result, a closer look reveals some issues. In particular,
the factors hidden in the O(·) depend strongly on the data size m. Even for quadratics where the objective
function has a constant condition number, the convergence rate is1 O(m2/k2), whereas the convergence rate
of with-replacement sampling is O(1/mk). Thus, to get non-trivial convergence, one needs to do at least m
passes over the data, and “touch” each individual function fi(·) at least m times (which becomes m3 if the
goal is to compete with with-replacement sampling). However, the most interesting regime for stochastic
gradient methods is when we only need to process each fi(·) a small number of times, since otherwise they
are not competitive with other methods (e.g. deterministic gradient descent or accelerated gradient descent,
which compute an exact gradient of F (·) = 1
i=1 fi(·) at each iteration, but have linear convergence and
don’t require data reshufﬂing). Unfortunately, this is a regime which [19] does not address. A related issue
is that the algorithm studied in [19] requires us to repeatedly reshufﬂe the data, but this can be expensive
exactly in the scenarios where without-replacement sampling is desirable to begin with (e.g. because we do
not have cheap random access to the data).

mPm

1.2 Our Results

In this paper, we provide convergence guarantees for stochastic gradient methods, under several scenarios,
in the natural regime where the number of passes over the data is modest, and in particular that no data
reshufﬂing is necessary. We emphasize that our goal here will be more modest than those of [30, 19]:
Rather than show superiority of without-replacement sampling, we only show that it will not be signiﬁcantly
worse (in a worst-case sense) than with-replacement sampling. Nevertheless, such guarantees are novel, and

1See for instance theorem 4.1 and inequality 4.4, or theorem 4.3, in particular proof of part (ii) and inequality 4.19. These imply
that the Euclidean distance of the iterates to the optimum, as a function of m, k, scale as O(m/k). Therefore, the squared distance
(and hence the suboptimality in terms of the objective function) scales as O(m2/k2).

2

still justify the use of with-replacement sampling, especially in situations where it is advantageous due to
data access constraints or other reasons. Moreover, these results have a useful application in the context of
distributed learning and optimization, as we will shortly describe.

Our main contributions can be summarized as follows:
• For convex functions on some convex domain W, we consider algorithms which perform a single
pass over a random permutation of m individual functions, and show that their suboptimality can be
characterized by a combination of two quantities, each from a different ﬁeld: First, the regret which
the algorithm can attain in the setting of adversarial online convex optimization [10, 32, 20], and
second, the transductive Rademacher complexity of W with respect to the individual functions, a
notion stemming from transductive learning theory [40, 16].

• As a concrete application of the above, we show that if each function fi(·) corresponds to a convex
Lipschitz loss of a linear predictor, and the algorithm belongs to the class of algorithms which in
the online setting attain O(√T ) regret on T such functions (which includes, for example, stochastic
functions, is O(1/√T ). Up to constants, the guarantee is the same as that obtained using with-

gradient descent), then the suboptimality using without-replacement sampling, after processing T

replacement sampling.

• We turn to consider more speciﬁcally the stochastic gradient descent algorithm, and show that if
the objective function F (·) is λ-strongly convex, and the functions fi(·) are also smooth, then the
suboptimality bound becomes O(1/λT ), which matches the with-replacement guarantees (although
with replacement, smoothness is not needed, and the dependence on some parameters hidden in the
O(·) is somewhat better).

• In recent years, a new set of fast stochastic algorithms to solve Eq. (1) has emerged, such as SAG
[31], SDCA (as analyzed in [34, 35], SVRG [22], SAGA [12], Finito [13], S2GD [23] and others.
These algorithms are characterized by cheap stochastic iterations, involving computations of individ-
ual function gradients, yet unlike traditional stochastic algorithms, enjoy a linear convergence rate
(runtime scaling logarithmically with the required accuracy). To the best of our knowledge, all exist-
ing analyses require sampling with replacement. We consider a representative algorithm from this set,
namely SVRG, and the problem of regularized least squares, and show that similar guarantees can be
obtained using without-replacement sampling. This result has a potentially interesting implication:
Under the mild assumption that the problem’s condition number is smaller than the data size, we get
that SVRG can converge to an arbitrarily accurate solution (even up to machine precision), without
the need to reshufﬂe the data – only a single shufﬂe at the beginning sufﬁces. Thus, at least for this
problem, we can obatin fast and high-quality solutions even if random data access is expensive.

• A further application of the SVRG result is in the context of distributed learning: By simulating
without-replacement SVRG on data randomly partitioned between several machines, we get a nearly-
optimal algorithm for regularized least squares, in terms of communication and computational com-
plexity, as long as the condition number is smaller than the data size (up to logarithmic factors). This
builds on the work of Lee et al. [25], who were the ﬁrst to recognize the applicability of SVRG to
distributed optimization. However, their results relied on with-replacement sampling, and the com-
munication complexity is reasonable only for much smaller condition numbers.

We note that our focus is on scenarios where no reshufﬂings are necessary. In particular, the O(1/√T )
and O(1/λT ) bounds apply for all T up to T = m, i.e. we do one full pass over a random permutation

3

of the entire data. However, our techniques and results can be readily extended to the case where T > m
iterations are performed, by randomly reshufﬂing the data after every pass, and beginning a new pass. In a
similar vein, our SVRG result can be extended to a situation where each epoch of the algorithm is done on
an independent permutation of the data. We leave a full treatment of this to future work.

The paper is organized as follows. In Section 2, we introduce the basic notation and deﬁnitions used
throughout the paper. In Section 3, we discuss the case of convex and Lipschitz functions. In Section 4,
we consider strongly convex functions, and in Section 5, we provide the analysis for the SVRG algorithm.
We provide full proofs in Section 6, and conclude with a summary and open problems in Section 7. Some
technical proofs and results are relegated to appendices.

2 Preliminaries and Notation

We use bold-face symbols to denote vectors. Given a vector w, wi denotes it’s i-th coordinate. We utilize
the standard O(·), Θ(·), Ω(·) notation to hide constants, and ˜O, ˜Θ(·), ˜Ω(·) to hide constants and logarithmic
factors.
Given convex functions f1(·), f2(·), . . . , fm(·) from Rd to R, we deﬁne our objective function F : Rd →

R as

F (w) =

1
m

m

Xi=1

fi(w),

2 ,

2 (cid:13)(cid:13)w′ − w(cid:13)(cid:13)

2 .

µ

2 (cid:13)(cid:13)w′ − w(cid:13)(cid:13)

with some ﬁxed optimum w∗ ∈ arg minw∈W F (w). In machine learning applications, each individual fi(·)
usually corresponds to a loss with respect to a data point, hence will use the terms “individual function”,
“loss function” and “data point” interchangeably throughout the paper.

We let σ be a permutation over {1, . . . , m} chosen uniformly at random. In much of the paper, we
consider methods which draw loss functions without replacement according to that permutation (that is,
fσ(1)(·), fσ(2)(·), fσ(3)(·), . . .). We will use the shorthand notation

F1:t−1(·) =

1
t − 1

t−1

Xi=1

fσ(i)(·) , Ft:m(·) =

1

m − t + 1

m

Xi=t

fσ(i)(·)

to denote the average loss with respect to the ﬁrst t − 1 and last m − t + 1 loss functions respectively, as
ordered by the permutation (intuitively, the losses in F1:t−1(·) are those already observed by the algorithm at
the beginning of iteration t, whereas the losses in Ft:m(·) are those not yet observed). We use the convention
t−1Pt−1
that F1:1(·) ≡ 0, and the same goes for other expressions of the form 1
i=1 ··· throughout the paper,
when t = 1. We also deﬁne quantities such as ∇F1:t−1(·) and ∇Ft:m(·) similarly.

A function f : Rd → R is λ-strongly convex, if for any w, w′,
λ
f (w′) ≥ f (w) +(cid:10)g, w′ − w(cid:11) +

where g is any (sub)-gradient of f at w. Note that any convex function is 0-strongly convex. We also say a
function f is µ-smooth if for any w, w′,

µ-smoothness also implies that the function f is differentiable, and its gradient is µ-Lipschitz. Based on
these properties, it is easy to verify that if w∗ ∈ arg min f (w), and f is λ-strongly convex and µ-smooth,

f (w′) ≤ f (w) +(cid:10)g, w′ − w(cid:11) +

4

where r1, . . . , rm are i.i.d. random variables such that

1

u(cid:19) · Er1,...,rm"sup

v∈V

rivi# ,

m

Xi=1

s

+

Rs,u(V) =(cid:18) 1
ri =
w.p. p
1
−1 w.p. p

0

w.p. 1 − 2p

then

λ
2 kw − w∗k2 ≤ f (w) − f (w∗) ≤

µ
2 kw − w∗k2 .

We will also require the notion of trandsuctive Rademacher complexity, as developed by El-Yaniv and

Pechyony [16, Deﬁnition 1], with a slightly different notation adapted to our setting:
Deﬁnition 1. Let V be a set of vectors v = (v1, . . . , vm) in Rm. Let s, u be positive integers such that
s + u = m, and denote p := su
(s+u)2 ∈ (0, 1/2). We deﬁne the transductive Rademacher Complexity
Rs,u(V) as

This quantity is an important parameter is studying the richness of the set V, and will prove crucial in
providing some of the convergence results presented later on. Note that it differs slightly from standard
Rademacher complexity, which is used in the theory of learning from i.i.d. data (e.g. [3, 27, 33], where the
Rademacher variables ri only take −1, +1 values, and (1/s + 1/u) is replaced by 1/m).

3 Convex Lipschitz Functions

We begin by considering loss functions f1(·), f2(·), . . . , fm(·) which are convex and L-Lipschitz over some
convex domain W. We assume the algorithm sequentially goes over some permuted ordering of the losses,
and before processing the t-th loss function, produces an iterate wt ∈ W. Moreover, we assume that the
algorithm has a regret bound in the adversarial online setting, namely that for any sequence of T convex
Lipschitz losses f1(·), . . . , fT (·), and any w ∈ W,

T

T

Xi=1

fi(wt) −

fi(w) ≤ RT

Xi=1

for some RT scaling sub-linearly in T . For example, online gradient descent (which is equivalent to stochas-

tic gradient descent when the losses are i.i.d.), with a suitable step size, satisﬁes RT = O(BL√T ), where
L is the Lipschitz parameter and B upper bounds the norm of any vector in W [46]. A similar regret bound
can also be shown for other online algorithms (see [20, 32, 41]).
Since the ideas used for analyzing this setting will also be used in the more complicated results which
follow, we provide the analysis in some detail. We ﬁrst have the following straightforward theorem, which
upper bounds the expected suboptimality in terms of regret and the expected difference between the average
loss on preﬁxes and sufﬁxes of the data.
Theorem 1. Suppose the algorithm has a regret bound RT , and sequentially processes fσ(1)(·), . . . , fσ(T )(·)
where σ is a random permutation on {1, . . . , m}. Then in expectation over σ,

E" 1

T

T

Xt=1

F (wt) − F (w∗)# ≤

RT
T

+

1
mT

T

Xt=2
(t − 1) · E[F1:t−1(wt) − Ft:m(wt)].

5

The left hand side in the inequality above can be interpreted as an expected bound on F (wt) − F (w∗),
where t is drawn uniformly at random from 1, 2, . . . , T . Alternatively, by Jensen’s inequality and the fact
that F (·) is convex, the same bound also applies on E[F ( ¯wT ) − F (w∗)], where ¯wT = 1
The proof of the theorem relies on the following simple but key lemma, which expresses the expected
difference between with-replacement and without-replacement sampling in an alternative form, similar to
Thm. 1 and one which lends itself to tools and ideas from transductive learning theory. This lemma will be
used in proving all our main results.
Lemma 1. Let σ be a permutation over {1, . . . , m} chosen uniformly at random. Let s1, . . . , sm ∈ R be
random variables which conditioned on σ(1), . . . , σ(t − 1), are independent of σ(t), . . . , σ(m). Then

T PT

t=1 wt.

m

si − sσ(t)# =
E" 1
Xi=1
mPm
for t > 1, and for t = 1 we have E(cid:2) 1
i=1 si − sσ(t)(cid:3) = 0.

m

t − 1
m · E [s1:t−1 − st:m]

The proof of the Lemma appears in Subsection 6.2. We now turn to prove Thm. 1:

Proof of Thm. 1. Adding and subtracting terms, and using the facts that σ is a permutation chosen uniformly
at random, and w∗ is ﬁxed,

E" 1

T

T

Xt=1

F (wt) − F (w∗)# = E" 1
= E" 1

T

T

T

Xt=1(cid:0)fσ(t)(wt) − F (w∗)(cid:1)# + E" 1
Xt=1(cid:0)fσ(t)(wt) − fσ(t)(w∗)(cid:1)# + E" 1

Xt=1(cid:0)F (wt) − fσ(t)(wt)(cid:1)#
Xt=1(cid:0)F (wt) − fσ(t)(wt)(cid:1)#

T

T

T

T

T

Applying the regret bound assumption on the sequence of losses fσ(1)(·), . . . , fσ(T )(·), the above is at most

RT
T

+

1
T

T

Xt=1

E(cid:2)F (wt) − fσ(t)(wt)(cid:3) .

Since wt (as a random variable over the permutation σ of the data) depends only on σ(1), . . . , σ(t − 1), we
can use Lemma 1 (where si = fi(wt), and noting that the expectation above is 0 when t = 1), and get that
the above equals

RT
T

+

1
mT

T

Xt=2
(t − 1) · E[F1:t−1(wt) − Ft:m(wt)].

Having reduced the expected suboptimality to the expected difference E[F1:t−1(wt) − Ft:m(wt)], the
next step is to upper bound it with E[supw∈W (F1:t−1(w) − Ft:m(w))]: Namely, having split our loss
functions at random to two groups of size t − 1 and m − t + 1, how large can be the difference between the
average loss of any w on the two groups? Such uniform convergence bounds are exactly the type studied in
transductive learning theory, where a ﬁxed dataset is randomly split to a training set and a test set, and we
consider the generalization performance of learning algorithms ran on the training set. Such results can be
provided in terms of the transductive Rademacher complexity of W, and combined with Thm. 1, lead to the
following bound in our setting:

6

E" 1

T

T

Xt=1

F (wt) − F (w∗)# ≤

RT
T

+

2(cid:0)12 + √2L(cid:1) B

√m

.

Theorem 2. Suppose that each wt is chosen from a ﬁxed domain W, that the algorithm enjoys a regret
bound RT , and that supi,w∈W |fi(w)| ≤ B. Then in expectation over the random permutation σ,

T

E" 1

F (wt) − F (w∗)# ≤
where V = {(f1(w), . . . , fm(w) | w ∈ W}.

Xt=1

T

RT
T

+

1
mT

T

Xt=2
(t − 1)Rt−1:m−t+1(V) +

24B
√m

,

Thus, we obtained a generic bound which depends on the online learning characteristics of the algorithm,
as well as the statistical learning properties of the class W on the loss functions. The proof (as the proofs of
all our results from now on) appears in Section 6.
We now instantiate the theorem to the prototypical case of bounded-norm linear predictors, where the
loss is some convex and Lipschitz function of the prediction hw, xi of a predictor w on an instance x,
possibly with some regularization:
Corollary 1. Under the conditions of Thm. 2, suppose that W ⊆ {w : kwk ≤ B}, and each loss function
fi has the form ℓi(hw, xii) + r(w) for some L-Lipschitz ℓi, kxik ≤ 1, and a ﬁxed function r, then

As discussed earlier, in the setting of Corollary 1, typical regret bounds are on the order of O(BL√T ).
Thus, the expected suboptimality is O(BL/√T ), all the way up to T = m (i.e. a full pass over a random

permutation of the data). Up to constants, this is the same as the suboptimality attained by T iterations of
with-replacement sampling, using stochastic gradient descent or similar algorithms [33].

4 Faster Convergence for Strongly Convex Functions

We now consider more speciﬁcally the stochastic gradient descent algorithm on a convex domain W, which
can be described as follows: We initialize at some w1 ∈ W, and perform the update steps

wt+1 = ΠW (wt − ηtgt),

where ηt > 0 are ﬁxed step sizes, ΠW is projection on W, and gt is a subgradient of fσ(t)(·) at wt. Moreover,
we assume the objective function F (·) is λ-strongly convex for some λ > 0. In this scenario, using with-
replacement sampling (i.e. gt is a subgradient of an independently drawn fi(·)), performing T iterations
as above and returning a randomly sampled iterate wt or their average results in expected suboptimality
˜O(G2/λT ), where G2 is an upper bound on the expected squared norm of gt [29, 33]. Here, we study a
similar situation in the without-replacement case.
In the result below, we will consider speciﬁcally the case where each fi(w) is a Lipschitz and smooth
loss of a linear predictor w, possibly with some regularization. The smoothness assumption is needed to get
a good bound on the transductive Rademacher complexity of quantities such as h∇fi(w), wi. However, the
technique can be potentially applied to more general cases.
Theorem 3. Suppose W has diameter B, and that F (·) is λ-strongly convex on W. Assume that fi(w) =
ℓi(hwi, xii) + r(w) where kxik ≤ 1, r(·) is possibly some regularization term, and each ℓi is L-Lipschitz

7

and µ-smooth on {z : z = hw, xi , w ∈ W,kxk ≤ 1}. Furthermore, suppose supw∈W k∇fi(w)k ≤ G.
Then for any 1 < T ≤ m, if we run SGD for T iterations with step size ηt = 1/λt, we have

E" 1

T

T

Xt=1

F (wt) − F (w∗)# ≤ c ·
≤ c ·

where c is a universal positive constant.

(L + µB)2(cid:16)1 + log(cid:16) m

m−T +1(cid:17)(cid:17) + G2 log(T )

λT

((L + µB)2 + G2) log(T )

λT

,

As in the results of the previous section, the left hand side is the expected optimality of a single wt
where t is chosen uniformly at random, or an upper bound on the expected suboptimality of the average
¯wT = 1
t=1 wt. This result is similar to the with-replacement case, up to numerical constants and the
additional (L + µB2) term in the numerator. We note that in some cases, G2 is the dominant term anyway2.
However, it is not clear that the (L + µB2) term is necessary, and removing it is left to future work.

T PT

Remark 1 (log(T ) Factor). It is possible to remove the logarithmic factors in Thm. 3, by considering not
T PT
1
t=1 F (wt), but rather only an average over a sufﬁx of the iterates (wǫT , wǫT +1 . . . , wT for some ﬁxed
ǫ ∈ (0, 1)), or by a weighted averaging scheme. This has been shown in the case of with-replacement
sampling ([29, 24, 38]), and the same proof techniques can be applied here.

The proof of Thm. 3 is somewhat more challenging than the results of the previous section, since we

cant technical obstacle is that our proof technique relies on concentration of averages around expectations,

are attempting to get a faster rate of O(1/T ) rather than O(1/√T ), all the way up to T = m. A signiﬁ-
which on T samples does not go down faster than O(1/√T ). To overcome this, we apply concentration
results not on the function values (i.e. F1:t−1(wt) − Ft:m(wt) as in the previous section), but rather on
gradient-iterate inner products, i.e. h∇F1:t−1(wt) − ∇Ft:m(wt), wt − w∗i, where w∗ is the optimal so-
lution. To get good bounds, we need to assume these gradients have a certain structure, which is why we
need to make the assumption in the theorem about the form of each fi(·). Using transductive Rademacher
complexity tools, we manage to upper bound the expectation of these inner products by quantities of the
form qE[kwt − w∗k2]/√t (assuming here t < m/2 for simplicity). We now utilize the fact that in the
strongly convex case, kwt − w∗k itself decreases to zero with t at a certain rate, to get fast rates decreasing
as 1/t. The full proof appears in Subsection 6.5.

5 Without-Replacement SVRG for Least Squares

In this section, we will consider a more sophisticated stochastic gradient approach, namely the SVRG al-
gorithm of [22], designed to solve optimization problems with a ﬁnite sum structure as in Eq. (1). Unlike
purely stochastic gradient procedures, this algorithm does require several passes over the data. However, as-
suming the condition number µ/λ is smaller than the data size (where µ is the smoothness of each fi(·), and
λ is the strong convexity parameter of F (·)), only O(m log(1/ǫ)) gradient evaluations are required to get
an ǫ-accurate solution, for any ǫ. Thus, we can get a high-accuracy solution after the equivalent of a small
number of data passes. As discussed in the introduction, over the past few years several other algorithms

2G is generally on the order of L + λB, which is the same as L + µB if L is the dominant term. This happens for instance with

the squared loss, whose Lipschitz parameter is on the order of µB.

8

have been introduced and shown to have such a behavior. We will focus on the algorithm in its basic form,
where the domain W is unconstrained and equals Rd.
The existing analysis of SVRG ([22]) assumes stochastic iterations, which involves sampling the data
with replacement. Thus, it is natural to consider whether a similar convergence behavior occurs using
without-replacement sampling. As we shall see, a positive reply has at least two implications: The ﬁrst is that
as long as the condition number is smaller than the data size, SVRG can be used to obtain a high-accuracy
solution, without the need to reshufﬂe the data: Only a single shufﬂe at the beginning sufﬁces, and the
algorithm terminates after a small number of sequential passes (logarithmic in the required accuracy). The
second implication is that such without-replacement SVRG can be used to get a nearly-optimal algorithm for
convex distributed learning and optimization on randomly partitioned data, as long as the condition number
is smaller than the data size at each machine.

The SVRG algorithm using without-replacement sampling on a dataset of size m is described as Algo-
rithm 1. It is composed of multiple epochs (indexed by s), each involving one gradient computation on the
entire dataset, and T stochastic iterations, involving gradient computations with respect to individual data
points. Although the gradient computation on the entire dataset is expensive, it is only needed to be done
once per epoch. Since the algorithm will be shown to have linear convergence as a function of the number
of epochs, this requires only a small (logarithmic) number of passes over the data.

Algorithm 1 SVRG using Without-Replacement Sampling

Parameters: η, T , permutation σ on {1, . . . , m}
Initialize ˜w1 at 0
for s = 1, 2, . . . do

i=1 ∇fi( ˜ws)

w(s−1)T +1 := ˜ws
˜n := ∇F ( ˜ws) = 1
for t = (s − 1)T + 1, . . . , sT do
wt+1 := wt − η(cid:0)∇fσ(t)(wt) − ∇fσ(t)( ˜ws) + ˜n(cid:1)
end for
Let ˜ws+1 be the average of w(s−1)T +1, . . . , wsT , or one of them drawn uniformly at random.

mPm

end for

We will consider speciﬁcally the regularized least mean squares problem, where

ˆλ
2 kwk2 .
mPm

fi(w) =

1
2

(hw, xii − yi)2 +

(2)

for some xi, yi and ˆλ > 0. Moreover, we assume that F (w) = 1
i=1 fi(w) is λ-strongly convex (note
that necessarily λ ≥ ˆλ). For convenience, we will assume that kxik ,|yi|, λ are all at most 1 (this is without
much loss of generality, since we can always re-scale the loss functions by an appropriate factor). Note that
under this assumption, each fi(·) as well as F (·) are also 1 + ˆλ ≤ 2-smooth.
Theorem 4. Suppose each loss function fi(·) corresponds to Eq. (2), where xi ∈ Rd, maxi kxik ≤ 1,
maxi |yi| ≤ 1, ˆλ > 0, and that F (·) is λ-strongly convex, where λ ∈ (0, 1). Moreover, let B ≥ 1 be such
that kw∗k2 ≤ B and maxt F (wt) − F (w∗) ≤ B with probability 1 over the random permutation.
There is some universal constant c0 ≥ 1, such that for any c ≥ c0 and any ǫ ∈ (0, 1), if we run algorithm
1, using parameters η, T satisfying

η =

1
c

, T ≥

9
ηλ

, m ≥ c log2(cid:18) 64dmB2

λǫ (cid:19) T,

9

then after S = ⌈log4(9/ǫ)⌉ epochs of the algorithm, we obtain a point ˜wS+1 for which

E[F ( ˜wS+1) − min

w

F (w)] ≤ ǫ.

In particular, by taking η = Θ(1) and T = Θ(1/λ), the algorithm attains an ǫ-accurate solution after

λ

O(cid:16) log(1/ǫ)
(cid:17) stochastic iterations of without-replacement sampling, and O(log(1/ǫ)) sequential passes over
the data to compute gradients of F (·). This implies that as long as 1/λ (which stands for the condition
number of the problem) is smaller than O(m/ log(1/ǫ)), the number of without-replacement stochastic
iterations is smaller than the data size m. Therefore, assuming the data is randomly shufﬂed, we can get a
solution using only sequential passes over the data, without the need to reshufﬂe.

We make a few remarks about the result:

Remark 2 (B Parameter). The condition that maxt F (wt) − F (w∗) ≤ B with probability 1 is needed for
technical reasons in our analysis, which requires some uniform upper bound on F (wt) − F (w∗). That
being said, B only appears inside log factors, so even a very crude bound would sufﬁce.
In appendix
B, we indeed show that under the theorem’s conditions, there is always a valid B satisfying log(B) =
O (log(1/ǫ) log(T ) + log(1/λ)), which can be plugged into Thm. 4. Even then, we conjecture that this
bound on maxt F (wt) − F (w∗) can be signiﬁcantly improved, or perhaps even removed altogether from
the analysis.
Remark 3 (log(d) Factor). The logarithmic dependence on the dimension d is due to an application of
a matrix Bernstein inequality for without-replacement sampling on d × d matrices. However, with some
additional effort, it might be possible to shave off this log factor and get a fully dimension-free result, by
deriving an appropriate matrix concentration result depending on the intrinsic dimension (see Section 7 in
[39] for derivations in the with-replacement setting). We did not systematically pursue this, as it is not the
focus of our work.
Remark 4 (Extension to other losses). Our proof applies as-is in the slightly more general setting where
each fi(·) is of the form fi(w) = 1
2 w⊤Aiw + b⊤i w + ci, where kAik ,kbik are bounded and Ai is a
rank-1 positive semideﬁnite matrix (i.e. of the form uiu⊤i
for some vector ui). The rank-1 assumption
is required in one step of the proof, when deriving Eq. (14). Using a somewhat different analysis, we
can also derive a convergence bound for general strongly convex and smooth losses, but only under the
condition that the epoch size T scales as Ω(1/λ2) (rather than Ω(1/λ)), which means that the strong

context of without-replacement sampling, this is a somewhat trivial regime: For such λ, the analysis of

convexity parameter λ can be at most ˜O(1/√m) (ignoring logarithmic factors). Unfortunately, in the
with-replacement SVRG requires sampling less than ˜O(√m) individual loss functions for convergence.

But by the birthday paradox, with-replacement and without-replacement sampling are then statistically
indistinguishable, so we could just argue directly that the expected suboptimality of without-replacement
SVRG would be similar to that of with-replacement SVRG.

The proof of Thm. 4 (in Subsection 6.6) uses ideas similar to those employed in the previous section,
but is yet again more technically challenging. Recall that in the previous section, we were able to upper
bound certain inner products by a quantity involving both the iteration number t and kwt − w∗k2, namely
the Euclidean distance of the iterate wt to the optimal solution. To get Thm. 4, this is not good enough,
and we need to upper bound similar inner products directly in terms of the suboptimality F (wt) − F (w∗),
as well as F ( ˜ws) − F (w∗). We are currently able to get a satisfactory result for least squares, where the
Hessians of the objective function are ﬁxed, and the concentration tools required boil down to concentration
of certain stochastic matrices, without the need to apply transductive Rademacher complexity results. The
full proof appears in Subsection 6.6.

10

5.1 Application of Without-Replacement SVRG to distributed learning

An important variant of the problems we discussed so far is when training data (or equivalently, the indi-
vidual loss functions f1(·), . . . , fm(·)) are split between different machines, who need to communicate in
order to reach a good solution. This naturally models situations where data is too large to ﬁt at a single
machine, or where we wish to speed up the optimization process by parallelizing it on several computers.
There has been much research devoted to this question in the past few years, with some examples including
[43, 4, 2, 45, 1, 8, 26, 42, 14, 11, 15, 21, 37, 36, 44, 25].

A substantial number of these works focus on the setting where the data is split at random between
the machines (or equivalently, that data is assigned to each machine by sampling without replacement from
{f1(·), . . . , fm(·)})). Intuitively, this creates statistical similarities between the data at different machines,
which can be leveraged to aid the optimization process. Recently, Lee et al. [25] proposed a simple and
elegant solution, at least in certain parameter regimes. This is based on the observation that SVRG, according
to its with-replacement analysis, requires O(log(1/ǫ)) epochs, where in each epoch one needs to compute
an exact gradient of the objective function F (·) = 1
i=1 fi(·), and O(µ/λ) gradients of individual losses
fi(·) chosen uniformly at random (where ǫ is the required suboptimality, µ is the smoothness parameter of
each fi(·), and λ is the strong convexity parameter of F (·)). Therefore, if each machine had i.i.d. samples
from {f1(·), . . . , fm(·)}, whose union cover {f1(·), . . . , fm(·)}, the machines could just simulate SVRG:
First, each machine splits its data to batches of size O(µ/λ). Then, each SVRG epoch is simulated by
the machines computing a gradient of F (·), involving a fully parallel computation and one communication
round3, and one machine computing gradients with respect to one of its batches. Overall, this would require
log(1/ǫ) communication rounds, and runtime O(n + µ/λ) log(1/ǫ), where n is the maximal number of data
points per machine (ignoring communication time, and assuming constant time to compute a gradient of
fi(·)). Making the mild assumption that the condition number µ/λ is less than n, this requires runtime linear
in the data size per machine, with only a logarithmic number of communication rounds. Up to logarithmic
factors, this is essentially the best one can hope for with a distributed algorithm4.

mPm

Unfortunately, the reasoning above crucially relies on each machine having access to i.i.d. samples,
which can be reasonable in some cases, but is different than the more common assumption that the data is
randomly split among the machines. To circumvent this issue, [25] propose to communicate individual data
points / losses fi(·) between machines, so as to simulate i.i.d. sampling. However, by the birthday paradox,
this only works well in the regime where the overall number of samples required (O((µ/λ) log(1/ǫ)) is not
much larger than √m. Otherwise, with-replacement and without-replacement sampling becomes statisti-
cally very different, and a large number of loss functions would need to be communicated. In other words, if
communication is an expensive resource, then the solution proposed in [25] only works well up to condition
number µ/λ being roughly √m. In machine learning applications, the strong convexity parameter λ often
comes from explicit regularization designed to prevent over-ﬁtting, and needs to scale with the data size,
usually so that 1/λ is between √m and m. Thus, the solution above is communication-efﬁcient only when
1/λ is relatively small

However, the situation immediately improves if we can use a without-replacement version of SVRG,
which can easily be simulated with randomly partitioned data: The stochastic batches can now be simply

3Speciﬁcally, each machine computes a gradient with respect to an average of a different subset of {f1(·), . . . , fm(·)}, which
i=1 fi(·). For

can be done in parallel, and then perform distributed averaging to get a gradient with respect to F (·) = 1
simplicity we assume a broadcast model where this requires a single communication round, but this can be easily generalized.

m Pm

4Moreover, a lower bound in [25] implies that under very mild conditions, the number of required communication rounds is at
least ˜Ω(p(µ/λ)/n). Thus, a logarithmic number of communication rounds is unlikely to be possible once µ/λ is signiﬁcantly
larger than n.

11

subsets of each machine’s data, which are statistically identical to sampling {f1(·), . . . , fm(·)} without
replacement. Thus, no data points need to be sent across machines, even if 1/λ is large. We present the
pseudocode as Algorithm 2.

Algorithm 2 Distributed Without-Replacement SVRG

Parameters: η, T
Assume: {f1(·), . . . , fm(·)} randomly split to machines 1, 2, . . . , n (possibly different number at differ-
ent machines)
Each machine j splits its data arbitrarily to bj batches Bj
j := 1 , k := 1 , t := 1
All machines initialize ˜w1 at 0
for s = 1, 2, . . . , do

1, . . . , Bj

of size T

bi

Perform communication round to compute ˜n := ∇F ( ˜ws) = 1
Machine j performs w1 := ˜ws
for Each f in Bj

mPm
Machine j performs wt+1 := wt − η (∇f wt) − ∇f ( ˜ws) + ˜n)
t := t + 1

k do

i=1 ∇fi( ˜ws)

end for
Machine j computes ˜ws+1 as average of w1, . . . , wT , or one of them drawn uniformly at random.
Perform communication round to distribute ˜ws+1 to all machines
k := k + 1
If k > bj, let k := 1, j := j + 1

end for

Let us consider the analysis of no-replacement SVRG provided in Thm. 4. According to this analysis, by
setting T = Θ(1/λ), then as long as the total number of batches is at least Ω(log(1/ǫ)), and 1/λ = ˜O(m),
then Algorithm 2 will attain an ǫ-suboptimal solution in expectation. In other words, without any additional
communication, we extend the applicability of distributed SVRG (at least for regularized least squares) from

the 1/λ = ˜O(√m) regime to 1/λ = ˜O(m), which covers most scenarios relevant to machine learning.

We emphasize that this formal analysis only applies to regularized squared loss, which is the scope of
Thm. 4. However, Algorithm 2 can be applied to any loss function, and we conjecture that it will have
similar performance for any smooth losses and strongly-convex objectives.

6 Proofs

Before providing the proofs of our main theorems, we develop in Subsection 6.1 below some important
technical results on transductive Rademacher complexity, required in some of the proofs.

6.1 Results on Transductive Rademacher Complexity

In this subsection, we develop a few important tools and result about Rademacher complexity, that will come
handy in our analysis. We begin by the following theorem, which is a straightforward corollary of Theorem
1 from [16] (attained by simplifying and upper-bounding some of the parameters).
Theorem 5. Suppose V ⊆ [−B, B]m for some B > 0. Let σ be a permutation over {1, . . . , m} chosen
uniformly at random, and deﬁne v1:s = 1
j=s+1 vσ(j). Then for any δ ∈ (0, 1),

j=1 vσ(j), vs+1:m = 1

sPs

12

uPm

with probability at least 1 − δ,

(v1:s − vs+1:m) ≤ Rs,u(V) + 6B(cid:18) 1
√s

sup
v∈V

+

1

√u(cid:19)(cid:18)1 + log(cid:18)1

δ(cid:19)(cid:19) .

to the roles of vs+1:m and v1:s, and hence also implies the formulation above.

We note that the theorem in [16] actually bounds vs+1:m − v1:s, but the proof is completely symmetric
We will also need the well-known contraction property, which states that the Rademacher complexity of
a class of vectors V can only increase by a factor of L if we apply on each coordinate a ﬁxed L-Lipschitz
function:
Lemma 2. Let g1, . . . , gm are real-valued, L-Lipschitz functions, and given some V ⊆ Rm, deﬁne g ◦ V =
{(g1(v1), . . . , gm(vm)) : (v1, . . . , vm) ∈ V}. Then

This is a slight generalization of Lemma 5 from [16] (which is stated for g1 = g2 = . . . = gm, but the

proof is exactly the same).

Rs,u(g ◦ V) ≤ L · Rs,u(V).

In our analysis, we will actually only need bounds on the expectation of v1:s − vs+1:m, which is weaker
than what Thm. 5 provides. Although such a bound can be developed from scratch, we ﬁnd it more conve-
nient to simply get such a bound from Thm. 5. Speciﬁcally, combining Thm. 5 and Lemma 7 from Appendix
A, we have the following straightforward corollary:
Corollary 2. Suppose V ⊆ [−B, B]m for some B > 0. Let σ be a permutation over {1, . . . , m} chosen
uniformly at random, and deﬁne v1:t−1 = 1

j=t vσ(j). Then

m−t+1Pm
v1:t−1 − vt:m(cid:21) ≤ Rt−1,m−t+1(V) + 12B(cid:18) 1
√t − 1
Moreover, if supv∈V (v1:t−1 − vt:m) ≥ 0 for any permutation σ, then

j=1 vσ(j), vt:m = 1

t−1Pt−1

E(cid:20)sup

v∈V

+

1

√m − t + 1(cid:19) .

vuutE"(cid:18)sup

v∈V

v1:t−1 − vt:m(cid:19)2# ≤

√2 · Rs,u(V) + 12√2B(cid:18) 1
√t − 1

+

1

√m − t + 1(cid:19) .

We now turn to collect a few other structural results, which will be useful when studying the Rademacher

complexity of linear predictors or loss gradients derived from such predictors.
Lemma 3. Given two sets of vectors V ∈ [−BV , BV ]m,S ⊆ [−BS , BS ]m for some BV , BS ≥ 0, deﬁne

U = {(v1s1, . . . , vmsm) : (v1, . . . , vm) ∈ V, (s1, . . . , sm) ∈ S} .

Then

Proof. The proof resembles the proof of the contraction inequality for standard Rademacher complexity
(see for instance Lemma 26.9 in [33]).

Rs,u(U ) ≤ BS · Rs,u(V) + BV · Rs,u(S).

By deﬁnition of Rs,u, it is enough to prove that

Er1,...,rm"sup

v,s

m

Xi=1

rivisi# ≤ Er1,...,rm"sup

v,s

ri(BS · vi + BV · si)# ,

m

Xi=1

(3)

13

since the right hand side can be upper bounded by BS · E[supvPm

this, we will treat the coordinates one-by-one, starting with the ﬁrst coordinate and showing that

i=1 rivi] + BV · E[supsPm

i=1 risi]. To get

Er1,...,rm"sup

v,s

m

Xi=1

rivisi# ≤ Er1,...,rm"sup

v,s  r1(BS · v1 + BV · s1) +

rivisi!# .

m

Xi=2

Repeating the same argument for coordinates 2, 3, . . . , m will yield Eq. (3).

For any values v, v′ and s, s′ in the coordinates of some v ∈ V and s ∈ S respectively, we have

|vs − v′s′| = |vs − v′s + v′s − v′s′| ≤ |vs − v′s| + |v′s − v′s′|

≤ |v − v′| · |s| + |s − s′| · |v| ≤ BS|v − v′| + BV|s − s′|.

(4)

(5)

Recalling that ri are i.i.d. and take values of +1 and −1 with probability p (and 0 otherwise), we can write
the left hand side of Eq. (4) as

m

v,s  r1v1s1 +

Er1,...,rm"sup
= Er2,...,rm"p · sup

m

m

rivisi!#
Xi=2
rivisi! + p · sup
v,s  v1s1 +
Xi=2
v,s   m
rivisi!#
Xi=2
+(1 − 2p) sup
rivisi! + sup
v,s  p v1s1 + p
Xi=2
riv′′i s′′i!#
′′ (1 − 2p)
Xi=2
′′ p(v1s1 − v′1s′1) + p
Xi=2
Xi=2

riv′′i s′′i )!# .

+(1 − 2p)

+ sup
′′,s

sup
′,v

′,s

m

m

m

v

= Er2,...,rm"sup

= Er2,...,rm"

v,v

′′,s,s

′

s

v,s  −v1s1 +

rivisi!

m

Xi=2

′ −p v1s1 + p

v

riv′is′i!

m

Xi=2

rivisi + p

riv′is′i

m

Xi=2

14

Using Eq. (5) and the fact that we take a supremum over v, v′ and s, s′ from the same sets, this equals

m

Xi=2

rivisi + p

riv′is′i

m

Xi=2

m

Xi=2

rivisi + p

riv′is′i

m

Xi=2

rivisi! + p sup

′ p(−BSv′1 − BV s′1) + p

′,s

v

riv′is′i!

m

Xi=2

= Er2,...,rm"

v,v

sup
′,v

′′,s,s

′

s

+(1 − 2p)

= Er2,...,rm"

v,v

sup
′,v

′′,s,s

′

s

m

riv′′i s′′i!#

′′ p(cid:0)BS|v1 − v′1| + BV|s1 − s′1|(cid:1) + p
Xi=2
′′ p(cid:0)BS(v1 − v′1) + BV (s1 − s′1)(cid:1) + p
Xi=2

m

m

riv′′i s′′i!#
+(1 − 2p)
v,s  (BS · v1 + BV · s1) +
Xi=2
riv′′i s′′i!#
+(1 − 2p) sup
v,s  r1(BS · v1 + BV · s1) +
Xi=2

′′  m
Xi=2

v,s

m

= Er2,...,rm"p sup

= Er1,...,rm"sup

rivisi!#

as required.

Lemma 4. For some B > 0 and vectors x1, . . . , xm with Euclidean norm at most 1, let

VB = {(hw, x1i , . . . ,hw, xmi) : kwk ≤ B}.

Then

Rs,u(VB) ≤

√2B(cid:18) 1
√s

+

m

Proof. Using the deﬁnition of VB and applying Cauchy-Schwartz,
Xi=1

Er1,...,rm"sup
rivi# = Er1,...,rm" sup
w:kwk≤B*w,
Xi=1
≤ Bvuuut
Er1,...,rm
2
= Bvuut
rixi(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)



Xi=1

Er1,...,rm

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

v∈V

m

m

1

√u(cid:19) .
rixi+# ≤ Er1,...,rm"B(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Xi=1
Xi,j=1

rirj hxi, xji.

m

m

#

rixi(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

Recall that ri are independent and equal +1,−1 with probability p (and 0 otherwise). Therefore, for i 6= j,
E[rirj] = 0, and if i = j, E[rirj] = E[r2
i ] = 2p. Using this and the assumption that kxik ≤ 1 for all i, the
above equals

Bvuut

m

Xi=1

2phxi, xii ≤ Bp2pm.

15

s + 1

we get the upper bound

u(cid:1) B√2pm. Recalling that p = su
Therefore, Rs,u(VB) ≤(cid:0) 1
(s + u)2 (s + u) = √2B(cid:18) 1
u(cid:19)r2
B(cid:18) 1
= √2B(cid:18) 1
srs
s + u(cid:19) ≤

uru

s + u

su

+

+

u

1

1

s

s

s + u

s

1

+

u(cid:19)r su
√2B(cid:18) 1
√s

+

1

√u(cid:19) ,

(s+u)2 where s + u = m and plugging it in,

from which the result follows.

Combining Corollary 2, Lemma 2, Lemma 3 and Lemma 4, we have the following:

Corollary 3. Suppose the functions f1(·), . . . , fm(·) are of the form fi(w) = ℓi(hw, xii) + r(w), where
kxik ≤ 1 and r is some ﬁxed function. Let B > 0 such that the iterates w chosen by the algorithm are from
a set W ⊆ {w : kwk ≤ B} which contains the origin 0. Finally, assume ℓi(·) is L-Lipschitz and µ-smooth
over the interval [−B, B]. Then

vuutE"(cid:18) sup

w∈W(cid:28)∇F1:t−1(w) − ∇Ft:m(w) ,
≤ (19L + 2µB)(cid:18) 1
√t − 1

kwk(cid:29)(cid:19)2#
√m − t + 1(cid:19) .

+

w

1

Proof. By deﬁnition of fi, we have

(cid:28)∇fi(w),

w

kwk(cid:29) = ℓ′i(hw, xii)(cid:28) w
kwk

, xi(cid:29) +(cid:28)∇r(w),

w

kwk(cid:29) .

Therefore, the expression in the corollary statement can be written as

where

and

u∈U

vuutE"(cid:18)sup
u1:t−1 − ut:m(cid:19)2#,
ui = ℓ′i(hw, xii)(cid:28) w
, xi(cid:29)
kwk
, x1(cid:29) , . . . , ℓ′m(hw, xmi)(cid:28) w
kwk

U =(cid:26)(cid:18)ℓ′1(hw, x1i)(cid:28) w
kwk

, xm(cid:29)(cid:19) : kwk ∈ W(cid:27)

(note that the terms involving r get cancelled out in the expression u1:s − us+1:m, so we may drop them).
Applying Corollary 2 (noting that |ui| ≤ L, and that supu∈U u1:t−1 − ut:m ≥ 0, since u1:t−1 − ut:m = 0 if
we choose w = 0), we have

vuutE"(cid:18)sup

u∈U

Now, deﬁne

u1:t−1 − ut:m(cid:19)2# ≤

√2 · Rt−1,m−t+1(U ) + 12√2L(cid:18) 1
√t − 1

+

1

√m − t + 1(cid:19) .

(6)

V =(cid:8)(cid:0)ℓ′1(hw, x1i), . . . , ℓ′m(hw, xmi)(cid:1) : kwk ≤ B(cid:9)

16

and

and note that U as we deﬁned it satisﬁes

S = {(hw, x1i , . . . ,hw, xmi) : kwk = 1} ,

U ⊆ (cid:8)(ℓ′1(v1)s1, . . . , ℓ′m(vm)sm) : (v1, . . . , vm) ∈ V, (s1, . . . , sm) ∈ S(cid:9) .

Moreover, by construction, the coordinates of each v ∈ V are bounded in [−L, L], and the coordinates of
each s ∈ S are bounded in [−1, +1]. Applying Lemma 3, we get

Rt−1,m−t+1(U ) ≤ Rt−1,m−t+1(V) + L · Rt−1,m−t+1(S).

Using Lemma 4, we have

Rt−1,m−t+1(S) ≤

√2(cid:18) 1
√t − 1

+

1

√m − t + 1(cid:19) .

Finally, applying Lemma 2 (using the fact that each ℓ′i is µ-Lipschitz) followed by Lemma 4, we have

(7)

(8)

(9)

Rt−1,m−t+1(V) ≤

√2µB(cid:18) 1
√t − 1

+

1

√m − t + 1(cid:19) .

Combining Eq. (7), Eq. (8) and Eq. (9), plugging into Eq. (6), and slightly simplifying for readability, yields
the desired result.

6.2 Proof of Lemma 1

The lemma is immediate when t = 1, so we will assume t > 1. Also, we will prove it when the expectation
E is conditioned on σ(1), . . . , σ(t−1), and the result will follow by taking expectations over them. With this
conditioning, s1, . . . , sm have some ﬁxed distribution, which is independent of how σ permutes {1, . . . , m}\
{σ(1), . . . , σ(t − 1)}.
Recall that σ is chosen uniformly at random. Therefore, conditioned on σ(1), . . . , σ(t − 1), the value of
σ(t) is uniformly distributed on {1, . . . , m}\{σ(1), . . . , σ(t−1)}, which is the same set as σ(t), . . . , σ(m).
Therefore, the left hand side in the lemma statement equals

m

m

Xi=t
m − t + 1

1

1

m

m

m

t−1

m − t + 1

E" 1
Xi=1
si −
= E" 1
Xi=1
sσ(i) −
= E" 1
sσ(i) +(cid:18) 1
Xi=1
m −
= E" 1
Xi=1
sσ(i) −
m · E" 1
t − 1
t − 1

t−1

m

m

=

1

m

sσ(i)#
sσ(i)#
Xi=t
m − t + 1(cid:19) m
Xi=t
sσ(i)#
Xi=t

Xi=t
m − t + 1

m

m

1

m(m − t + 1)
t−1
Xi=1

sσ(i) −

t − 1

sσ(i)#

sσ(i)#

17

as required.

6.3 Proof of Thm. 2
Let V = {(f1(w), . . . , fm(w)) | w ∈ W} and applying Corollary 2, we have
F1:t−1(w) − Ft:m(w)(cid:21)
≤ Rt−1:m−t+1(V) + 12B(cid:18) 1
√t − 1

E[F1:t−1(wt) − Ft:m(wt)] ≤ E(cid:20) sup

w∈W

Plugging this into the bound from Thm. 1, we have

+

1

√m − t + 1(cid:19) .

E [F ( ¯wT ) − F (w∗)] ≤
Applying Lemma 6, the right hand side is at most

+

1
mT

(t − 1)(cid:18)Rt−1:m−t+1(V) + 12B(cid:18) 1
Xt=2
√t − 1

RT
T

T

RT
T

+

1
mT

T

Xt=2
(t − 1)Rt−1:m−t+1(V) +

24B
√m

.

+

1

√m − t + 1(cid:19)(cid:19)

(10)

6.4 Proof of Corollary 1
Since the Rademacher complexity term is obtained by considering the difference F1:t−1(wt) − Ft:m(wt),
the ﬁxed r(w) term in the loss function is cancelled out, so we may assume without loss of generality that
fi(w) = ℓi(hw, xii). Applying Lemma 2 and Lemma 4, we can upper the Rademacher complexity as
follows:

Rt−1:m−t+1(V) ≤ L · Rt−1:m−t+1 ({(hw, x1i , . . . ,hw, xmi) | w ∈ W})

≤

√2 · BL(cid:18) 1
√t − 1

+

1

√m − t + 1(cid:19) .

Plugging this into Thm. 2 and again applying Lemma 6, we get the bound stated in the theorem.

6.5 Proof of Thm. 3

Since the algorithm is invariant to shifting the coordinates or shifting all loss functions by a constant, we
will assume without loss of generality that W contains the origin 0 (and therefore W ⊆ {w : kwk ≤ B}),
that the objective function F (·) is minimized at 0, and that F (0) = 0. By deﬁnition of the algorithm and
convexity of W, we have

E[wt+1] = Eh(cid:13)(cid:13)ΠW (wt − ηt∇fσ(t)(wt))(cid:13)(cid:13)

2i ≤ Eh(cid:13)(cid:13)wt − ηt∇fσ(t)(wt)(cid:13)(cid:13)

t G2

≤ Ehkwtk2i − 2ηtE(cid:2)(cid:10)∇fσ(t)(wt), wt(cid:11)(cid:3) + η2
= Ehkwtk2i − 2ηtE [h∇F (wt), wti] + 2ηtE(cid:2)(cid:10)∇F (wt) − ∇fσ(t)(wt), wt(cid:11)(cid:3) + η2

2i

By deﬁnition of strong convexity, since F (·) is λ-strongly convex, minimized at 0, and assumed to equal 0
2 kwk2. Plugging this in, changing sides and dividing by 2ηt,
there, we have h∇F (wt), wti ≥ F (wt) + λ
we get
E[F (wt)] ≤ (cid:18) 1

2ηt ·E[kwt+1k2]+E(cid:2)(cid:10)∇F (wt) − ∇fσ(t)(wt), wt(cid:11)(cid:3)+

2(cid:19) E[kwtk2]−

G2. (11)

2ηt −

ηt
2

λ

1

t G2.

18

We now turn to treat the third term in the right hand side above. Since wt (as a random variable over the
permutation σ of the data) depends only on σ(1), . . . , σ(t− 1), we can use Lemma 1 and Cauchy-Schwartz,
to get

∇fi(wt) − ∇fσ(t)(wt), wt+#

m

m

E(cid:2)(cid:10)∇F (wt) − ∇fσ(t)(wt), wt(cid:11)(cid:3) = E"* 1

Xi=1
t − 1
m · E [(h∇F1:t−1(wt) − ∇Ft:m(wt), wti)]
m · E(cid:20)kwtk ·(cid:28)∇F1:t−1(wt) − ∇Ft:m(wt),
t − 1
m · E(cid:20)kwtk · sup
t − 1
m ·rEhkwtk2i ·vuutE"(cid:18) sup
t − 1

kwtk(cid:29)(cid:21)
kwk(cid:29)(cid:21)
w∈W(cid:28)∇F1:t−1(w) − ∇Ft:m(w),

w∈W(cid:28)∇F1:t−1(w) − ∇Ft:m(w),

=

=

≤

≤

wt

w

w

kwk(cid:29)(cid:19)2#

Applying Corollary 3 (using the convention 0/√0 = 0 in the case t = 1 where the expression above is 0
anyway), this is at most

m ·rEhkwtk2i · (19L + 2µB)(cid:18) 1
t − 1
√t − 1
(cid:18)√t − 1 +
= rEhkwtk2i ·
λ ≤ λ

19L + 2µB

m

1

+

√m − t + 1(cid:19)
√m − t + 1(cid:19) .
t − 1

Using the fact that for any a, b ≥ 0, √ab = qλa · b

inequality, the above is at most

2 a + 1

2λ b by the arithmetic-geometric mean

λ
2 · E[kwtk2] +

(19L + 2µb)2

2λm2

(cid:18)√t − 1 +

t − 1

√m − t + 1(cid:19)2

.

Since (a + b)2 ≤ 2(a2 + b2), this is at most

λ
2 · E[kwtk2] +
Plugging this back into Eq. (11), we get

(19L + 2µB)2

λm2

(cid:18)t − 1 +

(t − 1)2
m − t + 1(cid:19) .

E[F (wt)] ≤ (cid:18) 1

2ηt −

λ

2(cid:19) E[kwtk2]−

1

2ηt · E[kwt+1k2] +

(19L + 2µB)2

λm2

(cid:18)t − 1 +

(t − 1)2
m − t + 1(cid:19) +

ηt
2

G2.

Averaging both sides over t = 1, . . . , T , and using Jensen’s inequality, we have

E" 1

T

≤

T

F (wt)#
Xt=1
E[kwtk2](cid:18) 1
Xt=1
ηt −

T

1
2T

1

ηt−1 − λ(cid:19) +

(19L + 2µB)2

λm2T

T

Xt=1(cid:18)t − 1 +

(t − 1)2
m − t + 1(cid:19) +

G2
2T

ηt,

T

Xt=1

19

where we use the convention that 1/η0 = 0. Since T ≤ m, the second sum in the expression above equals

T−1

Xt=0 (cid:18)t +

T−1

T−1

t2

t2

2

t +

Xt=0

Xt=0

m − t(cid:19) =
≤
= m2(cid:18) 3

T (T − 1)
m − t ≤
+ m2 T−2
+ 1! ≤
Xt=0
m − t
+ log(cid:18)
m − T + 1(cid:19)(cid:19) .
m

m2
2

2

1

Plugging this back in, we get

+ m2

3m2

2

T−1

1

Xt=0
m − t
+ m2Z T−1

t=0

1

m − t

dt

E" 1

T

T

Xt=1

F (wt)# ≤

1
2T

+

T

1

E[kwtk2](cid:18) 1
ηt−1 − λ(cid:19)
Xt=1
ηt −
(19L + 2µB)2(cid:16) 3
2 + log(cid:16) m
m−T +1(cid:17)(cid:17)
t ≤ log(T ) + 1, we get that
m−T +1(cid:17)(cid:17)

2 + log(cid:16) m

λT

t=1

+

+

G2
T

1

λT

G2(log(T ) + 1)

,

ηt.

T

Xt=1

Now, choosing ηt = 1/λt, and using the fact thatPT
(19L + 2µB)2(cid:16) 3

E[F ( ¯wT )] ≤

λT

from which the result follows by a slight simpliﬁcation, and recalling that we assumed F (w∗) = F (0) = 0.
The last inequality in the theorem is by the simple observation that T (m − T + 1) ≥ m for any T ∈
{1, 2, . . . , m}, and therefore log(m/(m − T + 1)) ≤ log(T ).
6.6 Proof of Thm. 4

The proof is based on propositions 1 and 2 presented below, which analyze the expectation of the update
as well as its expected squared norm. The key technical challenge, required to get linear convergence,
is to upper bound these quantities directly in terms of the suboptimality of the iterates wt, ˜ws. To get
Proposition 1, we state and prove a key lemma (Lemma 5 below), which bounds the without-replacement
concentration behavior of certain normalized stochastic matrices. The proof of Thm. 4 itself is then a
relatively straightforward calculation, relying on these results.
Lemma 5. Let x1, . . . , xm be vectors in Rd of norm at most 1. Deﬁne ¯X = 1

i=1 xix⊤i , and

Mi = ( ¯X + ˆγI)−1/2xix⊤i ( ¯X + ˆγI)−1/2

mPm

for some γ ≥ 0, so that ¯X + ˆγI has minimal eigenvalue γ ∈ (0, 1). Finally, let σ be a permutation on
{1, . . . , m} drawn uniformly at random. Then for any α ≥ 2, the probability

Pr ∃s ∈ {1, . . . , m} : (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

1
s

is at most 4dm exp (−α/2).

s

Xi=1

1

Mσ(i) −
m − s
√γ (cid:18) 1
√s
+

α

>

m

Mσ(i)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Xi=s+1
√m − s(cid:19) +

1

α

γ (cid:18) 1

s

+

1

m − s(cid:19)(cid:19)

20

Proof. The proof relies on a without-replacement version of Bernstein’s inequality for matrices (Theorem 1
in [18]), which implies that for d × d Hermitican matrices ˆMi which satisfy
ˆM 2

ˆMi = 0 , max

m

m

for some v, c > 0, it holds that

1
m

Xi=1
Pr (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

1
s

s

Xi=1

1
m

≤ v,

ˆMi(cid:13)(cid:13)(cid:13) ≤ c , (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
i(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
i (cid:13)(cid:13)(cid:13)
Xi=1
4v (cid:17) z ≤ 2v/c
> z! ≤ (2d exp(cid:16)− sz2
2d exp(cid:0)− sz
2c(cid:1)

ˆMσ(i)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Mj = ( ¯X + γI)−1/2(cid:16)xix⊤i − ¯X(cid:17) ( ¯X + γI)−1/2.
Xj=1

z > 2v/c

m

ˆMi = Mi −

1
m

In particular, we will apply this on the matrices

(12)

Clearly, 1

ˆMi = 0. We only need to ﬁnd appropriate values for v, c.

i=1

mPm

First, by deﬁnition of ˆMi, we have

(cid:13)(cid:13)(cid:13)

ˆMi(cid:13)(cid:13)(cid:13) ≤(cid:13)(cid:13)(cid:13)

( ¯X + γI)−1/2(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)xixi − ¯X(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

( ¯X + γI)−1/2(cid:13)(cid:13)(cid:13)

.

Since both ¯X and xix⊤i are positive semideﬁnite and have spectral norm at most 1, the above is at most
γ−1/2 · 1 · γ−1/2 = γ−1. Therefore, we can take c = 1/γ.
over the index i = 1, . . . , m, and note that E[ ˆMi] = 0. Therefore, we have

We now turn to compute an appropriate value for v. For convenience, let E denote a uniform distribution

where in the last step we used the fact that Mi is positive semideﬁnite. Let us ﬁrst upper bound the second
term in the max, namely

E[M 2

i ](cid:13)(cid:13) ,(cid:13)(cid:13)

E2[Mi](cid:13)(cid:13)},

(13)

m

i(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

1
m

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

2

ˆM 2

E[M 2

E[ ˆMi

Xi=1

= (cid:13)(cid:13)

=(cid:13)(cid:13)(cid:13)
=(cid:13)(cid:13)

](cid:13)(cid:13)(cid:13)
E[(Mi − E[Mi])2](cid:13)(cid:13)
i ] − E2[Mi](cid:13)(cid:13) ≤ max{(cid:13)(cid:13)
E2[Mi](cid:13)(cid:13) = kE[Mi] · E[Mi]k ≤ kE[Mi]k2 = (cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)

2

.

si

m

1
m

E[M 2

Turning to the ﬁrst term in the max in Eq. (13), we have

Since the expression above is invariant to rotating the positive semideﬁnite matrix ¯X, we can assume without
≤ 1.

( ¯X + γI)−1/2 ¯X( ¯X + γI)−1/2(cid:13)(cid:13)(cid:13)
si+γ(cid:17)2
loss of generality that ¯X = diag(s1, . . . , sd), in which case the above reduces to (cid:16)maxi
( ¯X + γI)−1/2xix⊤i ( ¯X + γI)−1xix⊤i ( ¯X + γI)−1/2(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Xi=1
Xi=1(cid:16)x⊤i ( ¯X + γI)−1xi(cid:17) ( ¯X + γI)−1/2xix⊤i ( ¯X + γI)−1/2(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Xi=1(cid:13)(cid:13)( ¯X + γI)−1(cid:13)(cid:13) ( ¯X + γI)−1/2xix⊤i ( ¯X + γI)−1/2(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

i ](cid:13)(cid:13) = (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
= (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
≤ (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
= (cid:13)(cid:13)( ¯X + γI)−1(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

( ¯X + γI)−1/2 ¯X( ¯X + γI)−1/2(cid:13)(cid:13)(cid:13)

1
m

1
m

(cid:13)(cid:13)

(14)

(1)

m

m

21

where in (1) we used the facts that kxik ≤ 1 and each term ( ¯X + γI)−1/2xix⊤i ( ¯X + γI)−1/2 is positive
semideﬁnite. As before, the expression above is invariant to rotating the positive semideﬁnite matrix ¯X, so
we can assume without loss of generality that ¯X = diag(s1, . . . , sd), in which case the above reduces to

i

.

.

1

1

m

si

1
γ

1
γ

ˆM 2

γ(cid:27) =

1
γ · 1 =

si + γ(cid:19) ≤

≤ max(cid:26)1,

si + γ(cid:19)(cid:18)max
i(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
> z! ≤ (2d exp(cid:16)− γsz2
4 (cid:17) z ≤ 2
2d exp(cid:0)− γsz
2 (cid:1)
ˆMσ(i)(cid:13)(cid:13)(cid:13)
γs(cid:17)(cid:17) can be upper bounded
> α(cid:16) 1√γs + 1
sPs
γs(cid:19)2! ≤ 2d exp(cid:18)−
4 (cid:19)
γs(cid:19)(cid:19) ≤ 2d exp(cid:16)−
2(cid:17)

z > 2

α2

i=1

+

+

α

1

1

.

Therefore, Eq. (12) applies with v = c = 1/γ,, so we get that

Plugging these observations back into Eq. (13), we get that

i

s

1

by

1
s

1
m

(cid:18)max
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Xi=1
Pr (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
ˆMσ(i)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Xi=1
γs(cid:17), we get that Pr(cid:16)(cid:13)(cid:13)(cid:13)
Substituting z = α(cid:16) 1√γs + 1
2d exp −
γsα2(cid:18) 1
√γs
γsα(cid:18) 1
2d exp(cid:18)−
√γs
ˆMσ(i)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Pr (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Xi=1
Xi=1

> α(cid:18) 1
√γs
for any α ≥ 2. Recalling the deﬁnition of ˆMi, we get
Mi(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
> α 

Pr (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

in the ﬁrst case, and

Mσ(i) −

Mσ(i) −

Xi=1

1
m

1
m

1
2

1
4

1
s

1
s

m

m

m

1

s

s

m − s

Xi=s+1

Xi=1

Mi(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

Pr (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

in the second case. Assuming α ≥ 2, both expressions can be upper bounded by 2d exp (−α/2), so we get
that

+

1

γs(cid:19)! ≤ 2d exp(cid:16)−

α

2(cid:17)

> α(cid:18) 1
√γs

+

1

γs(cid:19)! ≤ 2d exp(cid:16)−

α

2(cid:17) .

(15)

Since the permutation is random, the exact same line of argument also works if we consider the last m − s
matrices rather than the ﬁrst s matrices, that is

Now, notice that for any matrices A, B, C and scalars a, b, it holds that

1

pγ(m − s)

+

1

γ(m − s)!! ≤ 2d exp(cid:16)−

α

2(cid:17) .

(16)

Pr(kA − Bk > a + b) ≤ Pr(kA − Ck > a) + Pr(kB − Ck > b)

22

(as the event kA − Bk > a + b implies kA − Ck +kB − Ck > a + b). Using this observation and Eq. (15),
Eq. (16), we have

Mσ(i) −

s

s

1
s

1
s

Pr (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Xi=1
≤ Pr (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
Xi=1
+ Pr (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
m − s
α
≤ 4d exp(cid:16)−
2(cid:17) .

1

Mσ(i) −
Xi=s+1

m

m

1

m

α

>

+

Mσ(i)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
√γ (cid:18) 1
Xi=s+1
√s
m − s
Mi(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
γs(cid:19)!
> α(cid:18) 1
1
Xi=1
√γs
m
Mi(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
> α 
Xi=1
Mσ(i) −

1
m

+

m

1

1

√m − s(cid:19) +

α

γ (cid:18) 1

s

+

1

m − s(cid:19)!

1

pγ(m − s)

+

1

γ(m − s)!!

The statement in the lemma now follows from a union bound argument over all possible s = 1, 2, . . . , m.

Proposition 1. Suppose each fi(·) is of the form in Eq. (2), where xi, w are in Rd, and F (·) is λ-strongly
convex with λ ∈ [1/m, 1]. Deﬁne

Then for any t ≤ m/2 and any ǫ ∈ (0, 1),

vi(t, s) = ∇fi(wt) − ∇fi( ˜ws) + ∇F ( ˜ws).

m

E"(cid:10)vσ(t)(t, s), wt − w∗(cid:11) −
Xi=1
log(cid:18) 64dmB2

18
√λm

1
m

ǫ
2

+

hvi(t, s), wt − w∗i#

≤
where d is the dimension.

λǫ (cid:19) · E [F (wt) + F ( ˜ws) − 2F (w∗)] ,

Proof. Deﬁne

ui(t, s) = hvi(t, s), wt − w∗i = h∇fi(wt) − ∇fi( ˜ws) + ∇F ( ˜ws) , wt − w∗i ,

in which case the expectation in the proposition statement equals

E"uσ(t)(t, s) −

1
m

ui(t, s)# .

m

Xi=1

Notice that ui(t, s) for all i is independent of σ(t), . . . , σ(m) conditioned on σ(1), . . . , σ(t − 1) (which
determine wt and ˜ws). Therefore, we can apply Lemma 1, and get that the above equals

t − 1
m · E [ut:m(t, s) − u1:t−1(t, s)] .

(17)

Recalling the deﬁnition of ui(t, s), and noting that the ﬁxed h∇F ( ˜ws), wt − w∗i terms get cancelled out in
the difference above, we get that Eq. (17) equals

t − 1
m · E [ˇut:m(t, s) − ˇu1:t−1(t, s)] .

23

(18)

where

ˇui(t, s) = h∇fi(wt) − ∇fi( ˜ws) , wt − w∗i

= hwt − ˜ws, xii · hxi, , wt − w∗i + ˆλhwt − ˜ws, wt − w∗i .

Again, the ﬁxed ˆλhwt − ˜ws, wt − w∗i terms get cancelled out in Eq. (18), so we can rewrite Eq. (18) as
(19)

t − 1
m · E [˘ut:m(t, s) − ˘u1:t−1(t, s)]

where

˘ui(t, s) = hwt − ˜ws, xii · hxi, , wt − w∗i

= hwt − w∗, xii · hxi, wt − w∗i + hw∗ − ˜ws, xii · hxi, wt − w∗i
= (wt − w∗)⊤xix⊤i (wt − w∗) + (w∗ − ˜ws)⊤xix⊤i (wt − w∗).

Therefore, we can rewrite Eq. (19) as

m · E"(wt − w∗)⊤ 
t − 1
m · E"(w∗ − ˜ws)⊤ 
t − 1

+

m

1

m − t + 1
1

Xi=t
Xi=t
m − t + 1

m

xσ(i)x⊤σ(i) −

xσ(i)x⊤σ(i) −

1
t − 1

t−1

xσ(i)x⊤σ(i)! (wt − w∗)#
Xi=1
xσ(i)x⊤σ(i)! (wt − w∗)#
1
Xi=1
t − 1

t−1

(20)

To continue, note that for any symmetric square matrix M, positive semideﬁnite matrix A, and vectors
w1, w2, we have

(1)

w⊤1 M w2

w⊤1 M w2

w⊤1 Aw1 + w⊤2 Aw2(cid:12)(cid:12)(cid:12)(cid:12)
|w⊤1 M w2| =(cid:16)w⊤1 Aw1 + w⊤2 Aw2(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)
w1,w2(cid:12)(cid:12)(cid:12)(cid:12)
w⊤1 Aw1 + w⊤2 Aw2(cid:12)(cid:12)(cid:12)(cid:12)
≤(cid:16)w⊤1 Aw1 + w⊤2 Aw2(cid:17) sup
w1,w2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
≤ (cid:16)w⊤1 Aw1 + w⊤2 Aw2(cid:17) sup
w1,w2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
≤ (cid:16)w⊤1 Aw1 + w⊤2 Aw2(cid:17) sup
2(cid:16)w⊤1 Aw1 + w⊤2 Aw2(cid:17)(cid:13)(cid:13)(cid:13)
A−1/2M A−1/2(cid:13)(cid:13)(cid:13)
≤

kw1k2 + kw2k2

2kw1kkw2k

w⊤1 A−1/2M A−1/2w2

w⊤1 A−1/2M A−1/2w2

(3)

1

(2)

,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

where (1) is by substituting A−1/2w1, A−1/2w2 in lieu of w1, w2 in the supremum, (2) is by the identity
a2 + b2 ≥ 2ab, and (3) is by the fact that for any square matrix X, |w⊤1 Xw2| ≤ kw1kkXkkw2k. Applying
this inequality with

M =

1

m − t + 1

m

Xi=t

xσ(i)x⊤σ(i) −

1
t − 1

t−1

Xi=1

24

xσ(i)x⊤σ(i) , A =

xix⊤i +

ˆλ
2

I,

1
m

m

Xi=1

w1 being either w∗− ˜ws or wt− w∗, and w2 = wt− w∗, we can (somewhat loosely) upper bound Eq. (20)
by

3(t − 1)

2m · Eh(cid:16)(wt − w∗)⊤A(wt − w∗) + ( ˜ws − w∗)⊤A( ˜ws − w∗)(cid:17)(cid:13)(cid:13)(cid:13)

A−1/2M A−1/2(cid:13)(cid:13)(cid:13)i .

Recalling that the objective function F (·) is actually of the form F (w) = w⊤Aw+b⊤w+c for the positive
deﬁnite matrix A as above, and some vector b and scalar c, it is easily veriﬁed that w∗ = − 1
2 A−1b, and
moreover, that

for any w. Therefore, we can rewrite the above as

(w − w∗)⊤A(w − w∗) = F (w) − F (w∗)

3(t − 1)

=

3(t − 1)

2m · Eh(F (wt) + F ( ˜ws) − 2F (w∗))(cid:13)(cid:13)(cid:13)
2m · E"(F (wt) + F ( ˜ws) − 2F (w∗))(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

A−1/2M A−1/2(cid:13)(cid:13)(cid:13)i
A−1/2 Pm

i=t xσ(i)x⊤σ(i)

m − t + 1 − Pt−1

t − 1

i=1 xσ(i)x⊤σ(i)

# .

! A−1/2(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

(21)

We now wish to use Lemma 5, which upper bounds the norm in the expression above with high probability.
However, since the norm appears inside an expectation and is multiplied by another term, we need to proceed
a bit more carefully. To that end, let N denote the norm in the expression above, and let D denote the
expression F (wt) + F ( ˜ws) − 2F (w∗). We collect the following observations:

• The Hessian of the objective function F (·) is 1

i=1 xix⊤i + ˆλI, whose minimal eigenvalue is at
least λ (since F (·) is assumed to be λ-strongly convex). Therefore, the minimal eigenvalue of A as
deﬁned above is at least λ/2. Applying Lemma 5, Pr(N > α · q(t)) ≤ 2dm exp(−α/2) for any
α ≥ 2, where

mPm

q(t) =r 2

λ(cid:18) 1
√t − 1

+

1

√m − t + 1(cid:19) +

2

λ(cid:18) 1
t − 1

+

1

m − t + 1(cid:19) .

for any t > 1, and q(1) = 0.

• By assumption, kwtk, k ˜wsk and kw∗k are all at most B. Moreover, since the objective function
F (·) is 1 + ˆλ ≤ 2 smooth, F (wt) − F (w∗) ≤ kwt − w∗k2 ≤ q4B2 and F ( ˜ws) − F (w∗) ≤
k ˜ws − w∗k2 ≤ 4B2. As a result, D as deﬁned above is in [0, 8B2].
• Since each xix⊤i has spectral norm at most 1, N is at most(cid:13)(cid:13)A−1/2(cid:13)(cid:13)

Combining these observations, we have the following:

≤ 2
λ.

2

E[DN ] = Pr(N > αq(t)) · E[DN|N > αq(t)] + Pr(N ≤ αq(t)) · E[DN|N ≤ αq(t)]

≤ 2dm exp(cid:16)−
≤

32dmB2

λ

α

2(cid:17) 16B2
exp(cid:16)−

λ
α

2(cid:17) + αq(t) · E[D]

+ αq(t) · Pr(N ≤ αq(t)) · E[D|N ≤ αq(t)]

25

for any α ≥ 2. In particular, picking α = 2 log(64dmB2/λǫ) (where recall that ǫ ∈ (0, 1) is an arbitrary
parameter), we get

Plugging in the deﬁnition of D, we get the following upper bound on Eq. (21):

E[DN ] ≤

ǫ
2

+ 2 log(cid:18) 64dmB2

λǫ (cid:19) q(t) · E[D].

2m (cid:18) ǫ
3(t − 1)

2

+ 2 log(cid:18) 64dmB2

λǫ (cid:19) q(t) · E [F (wt) + F ( ˜ws) − 2F (w∗)](cid:19) .

(22)

Recalling the deﬁnition of q(t) and the assumption t ≤ m/2 (and using the convention 0/√0 = 0), we have

3(t − 1)

2m · q(t) =

=

≤

=

=

1

3

+

+

+

3

3

3

m

2m  r 2
3(t − 1)
√2λ(cid:18)√t − 1
√2λ pm/2
√2λ(cid:18) 1
√2m
6
3
√λm
λm

m/2

√m − t + 1(cid:19) +
λ(cid:18) 1

λ(cid:18) 1
√t − 1
m√m − t + 1(cid:19) +
t − 1
mpm/2! +
λ(cid:18) 1
λ(cid:18) 1
m(cid:19)
√2m(cid:19) +
√λm(cid:18)1 +
√λm(cid:19) ,

m
2

m

m

m

+

+

+

+

=

1

3

3

1

3

which by the assumption λ ≥ 1/m (hence λm ≥ 1), is at most 9/√λm. Substituting this back into Eq. (22)

and loosely upper bounding, we get the upper bound

1

m − t + 1(cid:19)!

3

+

λ(cid:18) 1
t − 1
m(m − t + 1)(cid:19)
t − 1

+

m/2

m(m/2)(cid:19)

ǫ
2

+

18
√λm

log(cid:18) 64dmB2

λǫ (cid:19) · E [F (wt) + F ( ˜ws) − 2F (w∗)] ,

as required.

Proposition 2. Let

and suppose each fi(·) is µ-smooth. Then for any t ≤ m/2,

vi(t, s) = ∇fi(wt) − ∇fi( ˜ws) + ∇F ( ˜ws).

Proof. Since ∇F (w∗) = 0, we can rewrite vσ(t)(t, s) as

2] ≤ 6µ(F (wt) + F ( ˜ws) − 2F (w∗))

E[(cid:13)(cid:13)vσ(t)(t, s)(cid:13)(cid:13)
gσ(t)(wt) − gσ(t)( ˜ws) + (∇F ( ˜ws) − ∇F (w∗)) ,

where

gσ(t)(w) = ∇fσ(t)(w) − ∇fσ(t)(w∗).

26

Using the fact that (a + b + c)2 ≤ 3(a2 + b2 + c2) for any a, b, c, we have

1
3

m

m

m

Eh(cid:13)(cid:13)vσ(t)(t, s)(cid:13)(cid:13)
2i
≤ Eh(cid:13)(cid:13)gσ(t)(wt)(cid:13)(cid:13)
2i + Eh(cid:13)(cid:13)gσ(t)( ˜ws)(cid:13)(cid:13)
kgi(wt)k2# + E" 1
= E" 1
Xi=1
Xi=1
+ E"(cid:13)(cid:13)gσ(t)(wt)(cid:13)(cid:13)
kgi(wt)k2# + E"(cid:13)(cid:13)gσ(t)( ˜ws)(cid:13)(cid:13)
Xi=1
nPn

2i + Ehk∇F ( ˜ws) − ∇F (w∗)k2i
kgi( ˜ws)k2# + Ehk∇F ( ˜ws) − ∇F (w∗)k2i
kgi( ˜ws)k2#

We now rely on a simple technical result proven as part of the standard SVRG analysis (see equation (8)
in [22]), which states that if P (w) = 1
i=1 ψi(w), where each ψi is convex and µ-smooth, and P is
minimized at w∗, then for all w.

(23)

(24)

Xi=1

2 −

2 −

1
m

1
m

m

m

m

1
n

n

Xi=1

k∇ψi(w) − ∇ψi(w∗)k2 ≤ 2µ (P (w) − P (w∗))

(25)

Applying this inequality on each of the ﬁrst 3 terms in Eq. (24) (i.e. taking either ψi(·) = fi(·) and n = m,
or ψ(·) = F (·) and n = 1), we get the upper bound

2µ(F (wt) − F (w∗)) + 2µ(F ( ˜ws) − F (w∗)) + 2µ(F ( ˜ws) − F (w∗))
1
Xi=1
m

Xi=1

2 −

2 −

1
m

m

m

= 2µ (F (wt) − F (w∗)) + 4µ (F ( ˜ws) − F (w∗))

kgi( ˜ws)k2#

+ E"(cid:13)(cid:13)gσ(t)(wt)(cid:13)(cid:13)
+ E"(cid:13)(cid:13)gσ(t)(wt)(cid:13)(cid:13)

2 −

1
m

Loosely upper bounding this and applying Lemma 1, we get the upper bound

+

4µ (F (wt) + F ( ˜ws) − 2F (w∗)) +
i=t(cid:13)(cid:13)gσ(i)( ˜ws)(cid:13)(cid:13)
≤ 4µ (F (wt) + F ( ˜ws) − 2F (w∗)) +

m · E"Pm
t − 1

m − t + 1

≤ 4µ (F (wt) + F ( ˜ws) − 2F (w∗)) +

= 4µ (F (wt) + F ( ˜ws) − 2F (w∗)) +

m

kgi(wt)k2# + E"(cid:13)(cid:13)gσ(t)( ˜ws)(cid:13)(cid:13)
kgi(wt)k2# + E"(cid:13)(cid:13)gσ(t)( ˜ws)(cid:13)(cid:13)
Xi=1
m · E"Pm
i=t(cid:13)(cid:13)gσ(i)(wt)(cid:13)(cid:13)
t − 1
− Pt−1
i=1(cid:13)(cid:13)gσ(i)( ˜ws)(cid:13)(cid:13)
t − 1
m · E"Pm
t − 1
m · E"Pm
t − 1
m − t + 1 · E"Pm
t − 1

m − t + 1
#

i=t(cid:13)(cid:13)gσ(i)(wt)(cid:13)(cid:13)
i=1(cid:13)(cid:13)gσ(i)(wt)(cid:13)(cid:13)
i=1 kgi(wt)k2

m − t + 1

m − t + 1

m

2

2

2

27

2 −

1
m

m

Xi=1

kgi( ˜ws)k2# .

− Pt−1

i=1(cid:13)(cid:13)gσ(i)(wt)(cid:13)(cid:13)

t − 1

2

#

2

m − t + 1

+ Pm
i=t(cid:13)(cid:13)gσ(t)( ˜ws)(cid:13)(cid:13)
+ Pm
i=1(cid:13)(cid:13)gσ(i)( ˜ws)(cid:13)(cid:13)
m − t + 1
+ Pm
i=1 kgi( ˜ws)k2

m

#
#
# .

2

1

2 and 1

Since we assume t ≤ m/2, we have
mPm
mPm

m−t+1 ≤ m/2
t−1
i=1(cid:13)(cid:13)gσ(i)( ˜ws)(cid:13)(cid:13)
i=1(cid:13)(cid:13)gσ(i)(wt)(cid:13)(cid:13)
4µ (F (wt) + F ( ˜ws) − 2F (w∗)) + 1 · (2µ(F (wt) − F (w∗)) + 2µ(F ( ˜ws) − F (w∗)))
= 6µ(F (wt) + F ( ˜ws) − 2F (w∗))

m/2 = 1. Plugging this in, and applying Eq. (25) on the
2 terms, this is at most

as required.

Proof of Thm. 4. Consider some speciﬁc epoch s and iteration t. We have

where

Therefore,

wt+1 = wt − vσ(t)(t, s),

vi(t, s) = ∇fi(wt) − ∇fi( ˜ws) + ∇F ( ˜ws).

E[kwt+1 − w∗k2] = E[(cid:13)(cid:13)wt − ηvσ(t)(t, s)(cid:13)(cid:13)
Applying Proposition 1 and Proposition 2 (assuming that t ≤ m/2, which we will verify later, and noting
that λ ≥ 1/m by the assumptions on η, T and m, and that each fi(·) is 1 + ˆλ ≤ 2-smooth), Eq. (26) is at
most

= E[kwt − w∗k2] − 2η · E[(cid:10)vσ(t)(t, s), wt − w∗(cid:11)] + η2E[(cid:13)(cid:13)vσ(t)(t, s)(cid:13)(cid:13)

(26)

2].

2]

m

2

m

E[kwt − w∗k2] − 2η · E" 1
+ 2η(cid:18) ǫ
= E[kwt − w∗k2] − 2η · E [h∇F (wt), wt − w∗i]
+ ηǫ + 2η(cid:18)6η +

hvi(t, s), wt − w∗i# + 12η2(F (wt) + F ( ˜ws) − 2F (w∗))
Xi=1
log(cid:18) 64dmB2
λǫ (cid:19) · E [F (wt) + F ( ˜ws) − 2F (w∗)](cid:19)
log(cid:18) 64dmB2

λǫ (cid:19)(cid:19) · E [F (wt) + F ( ˜ws) − 2F (w∗)] .

18
√λm

18
√λm

+

Since F (·) is convex, h∇F (wt), wt − w∗i ≥ F (wt) − F (w∗), so we can upper bound the above by

E[kwt − w∗k2] + ηǫ + 2η(cid:18)6η +
18
√λm
log(cid:18) 64dmB2
+ 2η(cid:18)6η +
λǫ (cid:19)(cid:19) · E [F ( ˜ws) − F (w∗)] .

log(cid:18) 64dmB2

18
√λm

λǫ (cid:19) − 1(cid:19) · E [F (wt) − F (w∗)]

Recalling that this is an upper bound on E[kwt+1 − w∗k2] and changing sides, we get
λǫ (cid:19)(cid:19) · E [F (wt) − F (w∗)]
λǫ (cid:19)(cid:19) · E [F ( ˜ws) − F (w∗)] .

2η(cid:18)1 − 6η −
≤ E[kwt − w∗k2] − E[kwt+1 − w∗k2] + ηǫ
+ 12η(cid:18)η +

log(cid:18) 64dmB2
log(cid:18) 64dmB2

18
√λm

3
√λm

28

Summing over all t = (s− 1)T + 1, . . . , sT in the epoch (recalling that the ﬁrst one corresponds to ˜ws) and
dividing by ηT , we get

18
√λm

log(cid:18) 64dmB2

λǫ (cid:19)(cid:19) ·

1
T

sT

Xt=(s−1)T +1

E [F (wt) − F (w∗)]

2(cid:18)1 − 6η −
≤

1
ηT · E[k ˜ws − w∗k2] + ǫ
+ 12(cid:18)η +

3
√λm

log(cid:18) 64dmB2

λǫ (cid:19)(cid:19) · E [F ( ˜ws) − F (w∗)] .

Since F (·) is λ-strongly convex, we have k ˜ws − w∗k2 ≤ 2
simplifying a bit leads to

λ (F ( ˜ws) − F (w∗)). Plugging this in and

sT

1
T

E

≤ (cid:16) 2

Xt=(s−1)T +1
ηλT + 12(cid:16)η + 3√λm

F (wt) − F (w∗)

λǫ (cid:17)(cid:17)(cid:17) · E [F ( ˜ws) − F (w∗)] + ǫ
log(cid:16) 64dmB2
2(cid:16)1 − 6η − 18√λm

λǫ (cid:17)(cid:17)
log(cid:16) 64dmB2

.

The left hand side equals or upper bounds E[F ( ˜ws+1)] − F (w∗) (recall that we choose ˜ws+1 uniformly at
random from all iterates produced in the epoch, or we take the average, in which case E[F ( ˜ws+1)− F (w∗)]
is at most the left hand side by Jensen’s inequality). As to the right hand side, if we assume

η =

1
c

, T ≥

9
ηλ

, m ≥ c

log2(64dmB2/λǫ)

λ
4 · E[F ( ˜ws)− F (w∗)] + 2

3 ǫ. Therefore,

(27)

for a sufﬁciently large numerical constant c, we get that it is at most 1
we showed that

E [F ( ˜ws+1) − F (w∗)] ≤
Unwinding this recursion, we get that after s epochs,

1
4 · E[F ( ˜ws) − F (w∗)] +

2
3

ǫ.

E [F ( ˜ws+1) − F (w∗)] ≤ 4−s · E[F ( ˜w1) − F (w∗)] +

= 4−s · E[F ( ˜w1) − F (w∗)] +
< 4−s · E[F ( ˜w1) − F (w∗)] +

2
3

2
3
8
9

s−1

ǫ

4−i

Xi=0
1 − 4−s
ǫ ·
1 − 4−1
ǫ.

Since we assume that we start at the origin ( ˜w1 = 0), we have F ( ˜w1− F (w∗)) ≤ F (0) = 1
so we get

i=1 y2

i ≤ 1,

mPm

E [F ( ˜ws+1) − F (w∗)] ≤ 4−s +

8
9

ǫ.

This is at most ǫ assuming s ≥ log4(9/ǫ), so it is sufﬁcient to have ⌈log4(9/ǫ)⌉ epochs to ensure subopti-
mality at most ǫ in expectation.

29

Finally, note that we had ⌈log4(9/ǫ)⌉ epochs, in each of which we performed T stochastic iterations.
Therefore, the overall number of samples used is at most ⌈log4(9/ǫ)⌉T . Thus, to ensure the application of
Propositions 1 and 2 is valid, we need to ensure this is at most m/2, or that

Combining this with Eq. (27), it is sufﬁcient to require

m ≥ 2⌈log4(9/ǫ)⌉ · T.

η =

1
c

, T ≥

9
ηλ

, m ≥ c log2(cid:18) 64dmB2

λǫ (cid:19) T

for any sufﬁciently large c.

7 Summary and Open Questions

In this paper, we studied the convergence behavior of stochastic gradient methods using no-replacement
sampling, which is widely used in practice but is theoretically poorly understood. We provided results
in the no-replacement setting for various scenarios, including convex Lipschitz functions with any regret-
minimizing algorithm; convex and strongly convex losses with respect to linear predictors, using stochastic
gradient descent; and an analysis of SVRG in the case of regularized least squares. The latter results also
has an application to distributed learning, yielding a nearly optimal algorithm (in terms of communication
and computation) for regularized least squares on randomly partitioned data, under the mild assumption that
the condition number is smaller than the data size per machine.

Our work leaves several questions open. First, it would be interesting to remove the speciﬁc structural
assumptions we have made on the loss functions, making only basic geometric assumptions such as strong
convexity or smoothness. This would match the generality of the current theory on stochastic optimiza-
tion using with-replacement sampling. Second, all our results strongly rely on concentration and uniform
convergence, which are not needed when studying with-replacement stochastic gradient methods (at least
when considering bounds which holds in expectation), and sometimes appears to lead to looser bounds,
such as the L, µ factors in Thm. 3. Thus, it would be interesting to have a more direct proof approach.
Third, as discussed in the introduction, our theorems merely show that without-replacement sampling is not
signiﬁcantly worse than with-replacement sampling (in a worst-case sense) but this does not explain why
without-replacement sampling often performs better. Although [30, 19] provide some partial results in that
direction, the full question remains open. Finally, it would be interesting to extend our analysis of the SVRG
algorithm to other fast-stochastic algorithms of a similar nature.

Acknowledgments

This research is supported in part by an FP7 Marie Curie CIG grant, an Israel Science Foundation grant
425/13, and by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI).

References

[1] A. Agarwal, O. Chapelle, M. Dud´ık, and J. Langford. A reliable effective terascale linear learning

system. CoRR, abs/1110.4198, 2011.

30

[2] M.-F. Balcan, A. Blum, S. Fine, and Y. Mansour. Distributed learning, communication complexity and

privacy. In COLT, 2012.

[3] Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and

structural results. The Journal of Machine Learning Research, 3:463–482, 2003.

[4] R. Bekkerman, M. Bilenko, and J. Langford. Scaling up machine learning: Parallel and distributed

approaches. Cambridge University Press, 2011.

[5] Dimitri P Bertsekas. Nonlinear programming. Athena scientiﬁc, 1999.

[6] L´eon Bottou. Curiously fast convergence of some stochastic gradient descent algorithms. In Proceed-

ings of the symposium on learning and data science, Paris, 2009.

[7] L´eon Bottou. Stochastic gradient descent tricks.

In Neural Networks: Tricks of the Trade, pages

421–436. Springer, 2012.

[8] S.P. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical

learning via ADMM. Foundations and Trends in Machine Learning, 3(1):1–122, 2011.

[9] S´ebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends R(cid:13) in

Machine Learning, 8(3-4):231–357, 2015.

[10] Nicol`o Cesa-Bianchi and G´abor Lugosi. Prediction, learning, and games. Cambridge university press,

2006.

[11] A. Cotter, O. Shamir, N. Srebro, and K. Sridharan. Better mini-batch algorithms via accelerated gra-

dient methods. In NIPS, 2011.

[12] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in Neural Information Pro-
cessing Systems, pages 1646–1654, 2014.

[13] Aaron J Defazio, Tib´erio S Caetano, and Justin Domke. Finito: A faster, permutable incremental

gradient method for big data problems. arXiv preprint arXiv:1407.2710, 2014.

[14] O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction using

mini-batches. Journal of Machine Learning Research, 13:165–202, 2012.

[15] J. Duchi, A. Agarwal, and M. Wainwright. Dual averaging for distributed optimization: Convergence

analysis and network scaling. IEEE Trans. Automat. Contr., 57(3):592–606, 2012.

[16] Ran El-Yaniv and Dmitry Pechyony. Transductive rademacher complexity and its applications. Journal

of Artiﬁcial Intelligence Research, 35:193–234, 2009.

[17] Xixuan Feng, Arun Kumar, Benjamin Recht, and Christopher R´e. Towards a uniﬁed architecture for in-
rdbms analytics. In Proceedings of the 2012 ACM SIGMOD International Conference on Management
of Data, pages 325–336. ACM, 2012.

[18] David Gross and Vincent Nesme. Note on sampling without replacing from a ﬁnite collection of

matrices. arXiv preprint arXiv:1001.2738, 2010.

31

[19] Mert G¨urb¨uzbalaban, Asu Ozdaglar, and Pablo Parrilo. Why random reshufﬂing beats stochastic gra-

dient descent. arXiv preprint arXiv:1510.08560, 2015.

[20] Elad Hazan. Introduction to online convex optimization. Book draft, 2015.

[21] M. Jaggi, V. Smith, M. Tak´ac, J. Terhorst, S. Krishnan, T. Hofmann, and M. Jordan. Communication-

efﬁcient distributed dual coordinate ascent. In NIPS, 2014.

[22] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance re-

duction. In Advances in Neural Information Processing Systems, pages 315–323, 2013.

[23] Jakub Koneˇcn`y and Peter Richt´arik. Semi-stochastic gradient descent methods.

arXiv preprint

arXiv:1312.1666, 2013.

[24] Simon Lacoste-Julien, Mark Schmidt, and Francis Bach. A simpler approach to obtaining an o (1/t)
convergence rate for the projected stochastic subgradient method. arXiv preprint arXiv:1212.2002,
2012.

[25] Jason Lee, Tengyu Ma, and Qihang Lin. Distributed stochastic variance reduced gradient methods.

arXiv preprint arXiv:1507.07595, 2015.

[26] D. Mahajan, S. Keerthy, S. Sundararajan, and L. Bottou. A parallel SGD method with strong conver-

gence. CoRR, abs/1311.0636, 2013.

[27] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT

press, 2012.

[28] Angelia Nedi´c and Dimitri Bertsekas. Convergence rate of incremental subgradient algorithms.

In

Stochastic optimization: algorithms and applications, pages 223–264. Springer, 2001.

[29] Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly

convex stochastic optimization. arXiv preprint arXiv:1109.5647, 2011.

[30] Benjamin Recht and Christopher R´e. Beneath the valley of the noncommutative arithmetic-geometric

mean inequality: conjectures, case-studies, and consequences. In COLT, 2012.

[31] Nicolas L Roux, Mark Schmidt, and Francis R Bach. A stochastic gradient method with an exponential
convergence rate for ﬁnite training sets. In Advances in Neural Information Processing Systems, pages
2663–2671, 2012.

[32] Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in

Machine Learning, 4(2):107–194, 2011.

[33] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-

rithms. Cambridge University Press, 2014.

[34] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss.

The Journal of Machine Learning Research, 14(1):567–599, 2013.

[35] Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent for

regularized loss minimization. Mathematical Programming, 155(1-2):105–145, 2016.

32

[36] O. Shamir and N. Srebro. On distributed stochastic optimization and learning. In Allerton Conference

on Communication, Control, and Computing, 2014.

[37] O. Shamir, N. Srebro, and T. Zhang. Communication-efﬁcient distributed optimization using an ap-

proximate newton-type method. In ICML, 2014.

[38] Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Conver-
gence results and optimal averaging schemes. In Proceedings of The 30th International Conference on
Machine Learning, pages 71–79, 2013.

[39] Joel A Tropp. An introduction to matrix concentration inequalities. Foundations and Trends R(cid:13) in

Machine Learning, 8(1-2):1–230, 2015.

[40] Vladimir Naumovich Vapnik and Vlamimir Vapnik. Statistical learning theory. Wiley New York,

1998.

[41] Lin Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal

of Machine Learning Research, 11:2543–2596, 2010.

[42] T. Yang. Trading computation for communication: Distributed SDCA. In NIPS, 2013.

[43] Y. Zhang, J. Duchi, and M. Wainwright. Communication-efﬁcient algorithms for statistical optimiza-

tion. Journal of Machine Learning Research, 14:3321–3363, 2013.

[44] Y. Zhang and L. Xiao. Communication-efﬁcient distributed optimization of self-concordant empirical

loss. In ICML, 2015.

[45] M. Zinkevich, M. Weimer, A. Smola, and L. Li. Parallelized stochastic gradient descent. In NIPS,

2010.

[46] Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In ICML,

2003.

33

A Additional Technical Lemmas

In this appendix, we collect a couple of purely technical lemmas used in certain parts of the paper.
Lemma 6. If T, m are positive integers, T ≤ m, then
(t − 1) r 1
Xt=2
t − 1

m − t + 1! ≤

+r 1

2
√m

1
mT

T

.

Proof.

m − t + 1!

1
mT

=

=

T

T−1

1
mT

(t − 1) r 1
Xt=2
t − 1
t r 1
+r 1
Xt=1
mT  T−1
√t +
Xt=1
Xt=1

+r 1
m − t!
√m − t! .

T−1

1

t

t

Since √t and t/√m − t are both increasing in t, we can upper bound the sums by integrals as follows:

t

T

2

√m − T · (2m + T )(cid:17)(cid:19)

T

=

2

2

2

=

=

1

1

1

=

+

m

t=0

≤

t=0
2
3

dt(cid:19)

√t dt +Z T
mT (cid:18)Z T
√m − t
√m − t · (2m + t)(cid:19)(cid:12)(cid:12)(cid:12)
t=0(cid:19)
3 · T 3/2 +(cid:18)−
mT (cid:18) 2
3(cid:16)2m√m −
mT (cid:18) 2
3 · T 3/2 +
3 √T
+ 2(cid:18)√m
2m(cid:19)(cid:19)!
T − √m − T ·(cid:18) 1
3 √T
2m(cid:19)(cid:19)!
T (cid:18)√m − √m − T ·(cid:18)1 +
3 √T
T (cid:16)√m − √m − T(cid:17)!
3 √T
√m + √m − T(cid:19)!
T (cid:18)
3 √T
√m! .
3(cid:16) 1√m + 2√m(cid:17) = 2√m as required.
Since T ≤ m, the above is at most 2
Lemma 7. Let X be a random variable, which satisﬁes for any δ ∈ (0, 1)

≤

≤

m

m

m

m

2

2

2

2

2

=

+

+

+

+

1

T

T

2

Pr (X > a + b log(1/δ)) ≤ δ,

34

where a, b > 0. Then

Furthermore, if X is non-negative, then

E[X] ≤ a + b.

pE[X 2] ≤

√2 · (a + b).

Proof. The condition in the lemma implies that for any z ≥ a,
Pr(X > z) ≤ exp(cid:18)−

b (cid:19) .
z − a

Therefore,

E[X] ≤ E[max{0, X}] = Z ∞

z=0

Pr(max{0, X} ≥ z) dz ≤ a +Z ∞

z=a

= a +Z ∞

z=a

Pr(X ≥ z) dz ≤ a +Z ∞

z=a

exp(cid:18)−

b (cid:19) dz = a +Z ∞
z − a

z=0

= a + b.

Pr(max{0, X} ≥ z) dz

exp(cid:16)−

z

b(cid:17)

Similarly, if X is non-negative,

E[X 2] = Z ∞

z=0

Pr(X 2 ≥ z) dz ≤ a2 +Z ∞

Pr(X 2 ≥ z) dz = a2 +Z ∞
√z − a
b (cid:19) dz = a2 + 2b(b + a) ≤ 2(a + b)2,

exp(cid:18)−

z=a2

z=a2

z=a2

≤ a2 +Z ∞

Pr(X ≥ √z) dz

and the result follows by taking the square root.

B Uniform Upper Bound on F (wt) − F (w∗) for SVRG
Below, we provide a crude bound on the parameter B in Thm. 4, which upper bounds F (wt) − F (w∗) with
probability 1. Note that B only appears inside log factors in the theorem, so it is enough that log(B) is
reasonably small.
Lemma 8. Suppose we run SVRG (algorithm 1) with some parameter T and step size η ∈ (0, 1) for S
epochs, where each fi(·) is a regularized squared loss (as in Eq. (2), with kxik ,|yi| ≤ 1 for all i), and F (·)
is λ-strongly convex with λ ∈ (0, 1). Then for every iterate wt produced by the algorithm, it holds with
probability 1 that

λ(cid:19) .
log(F (wt) − F (w∗)) ≤ 2S · log(5T ) + log(cid:18) 4

Noting that Thm. 4 requires only S = O(log(1/ǫ)) epochs with T = Θ(1/λ) stochastic iterations per

epoch, we get that

log(F (wt) − F (w∗)) = O(cid:18)log(cid:18) 1

ǫ(cid:19) log(T ) + log(cid:18) 1

λ(cid:19)(cid:19)

with probability 1.

35

Proof. Based on the SVRG update step, we have

kwt+1k ≤(cid:13)(cid:13)wt − η∇fσ(t)(wt)(cid:13)(cid:13) + η(cid:13)(cid:13)∇fσ(t)( ˜ws)(cid:13)(cid:13) + η k∇F ( ˜ws)k .

Since we are considering the regularized squared loss, with kxik ≤ 1,|yi| ≤ 1 and 0 ≤ ˆλ ≤ λ ≤ 1, the ﬁrst
term on the right hand side is

(28)

(cid:13)(cid:13)(cid:13)

wt − η(cid:16)xσ(t)x⊤σ(t)wt − yσ(t)xσ(t) + ˆλwt(cid:17)(cid:13)(cid:13)(cid:13) ≤ (cid:13)(cid:13)(cid:13)(cid:16)(1 − ηˆλ)I − η · xσ(t)x⊤σ(t)(cid:17) wt(cid:13)(cid:13)(cid:13)
≤ (cid:13)(cid:13)(cid:13)(cid:16)(1 − ηˆλ)I − η · xσ(t)x⊤σ(t)(cid:17)(cid:13)(cid:13)(cid:13) kwtk + η ≤ kwtk + 1,

As to the second two terms on the right hand side of Eq. (28), we have for any i by similar calculations that

+ η(cid:13)(cid:13)yσ(t)xσ(t)(cid:13)(cid:13)

k∇fi(w)k =(cid:13)(cid:13)(cid:13)(cid:16)xix⊤i + ˆλI(cid:17) w − yixi(cid:13)(cid:13)(cid:13) ≤ (cid:16)1 + ˆλ(cid:17)kwk + 1 ≤ 2kwk + 1

as well as

Substituting these back into Eq. (28) and loosely upper bounding, we get

k∇F (w)k ≤ 2kwk + 1.

kwt+1k ≤ kwtk + 4 (k ˜wsk + 1) .

Recalling that each epoch is composed of T such iterations, starting from ˜ws, and where ˜ws+1 is the average
or a random draw from these T iterations, we get that

k ˜ws+1k ≤ k ˜wsk + 4T (k ˜wsk + 1) ≤ 5T (k ˜wsk + 1) ,

and moreover, the right hand side upper-bounds the norm of any iterate wt during that epoch. Unrolling this
inequality, and noting that k ˜w1k = 0, we get

k ˜wS+1k ≤ (5T )S · 1 = (5T )S ,

and (5T )S upper bounds the norm of any iterate wt during the algorithm’s run.

2 kwk2 > 1

Turning to consider w∗ = arg minw F (w), we must have kw∗k2 ≤ 1/λ (to see why, note that any w
2 , so w

with squared norm larger than 1/λ, F (w) ≥ F (w∗) + λ
cannot be an optimal solution). Moreover, F (·) is 2-smooth, so for any iterate wt,

2mPm
λ!2
F (wt) − F (w∗) ≤ kwt − w∗k2 ≤ (kwtk + kw∗k)2 =  (5T )S +r 1
λ · (5T )2S. Taking

√λ (cid:17)2
Since (5T )S andp1/λ are both at least 1, this can be upper bounded by(cid:16) 2(5T )S

a logarithm, the result follows.

2, yet F (0) = 1

i=1 y2

i ≤ 1

.

= 4

36

