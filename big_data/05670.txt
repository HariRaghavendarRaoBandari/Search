6
1
0
2

 
r
a

 

M
7
1

 
 
]
L
C
.
s
c
[
 
 

1
v
0
7
6
5
0

.

3
0
6
1
:
v
i
X
r
a

Bank distress in the news:

Describing events through deep learning

Samuel Rönnqvist1,2 and Peter Sarlin3,4

1 Turku Centre for Computer Science – TUCS

Department of Information Technologies,
Åbo Akademi University, Turku, Finland

sronnqvi@abo.fi

2 Applied Computational Linguistics Lab

Goethe University Frankfurt am Main, Germany

3 Department of Economics

Hanken School of Economics, Helsinki, Finland

Arcada University of Applied Sciences, Helsinki, Finland

4 RiskLab Finland

peter@risklab.fi

Abstract. While many models are purposed for detecting the occur-
rence of events in complex systems, the task of providing qualitative
detail on the developments is not usually as well automated. We present
a deep learning approach for detecting relevant discussion in text and
extracting natural language descriptions of events. Supervised by only a
small set of event information, the model is leveraged by unsupervised
learning of semantic vector representations on extensive text data. We
demonstrate applicability to the study of ﬁnancial risk based on news
(6.6M articles), particularly bank distress and government interventions
(243 events), where indices can signal the level of bank-stress-related re-
porting at the entity level, or aggregated at country or European level,
while being coupled with explanations. Thus, we exemplify how text, as
timely and widely available data, can serve as a useful complementary
source of information for ﬁnancial risk analytics.

1

Introduction

Text analytics presents both major opportunities and challenges. On the one
hand, text data is rich in information and can be harnessed in traditional ways
such as for prediction tasks, while its descriptive depth also supports qualitative
and exploratory, yet highly data-driven, analysis. On the other hand, decoding
and utilizing the expressive detail of human language is prohibitively diﬃcult.
In computational terms, text consists of high-dimensional and often ambiguous
symbolic input (words), the semantics of which is a product of complex inter-
actions between parts of the sequences in which they occur (phrases, sentences,

2

paragraphs etc.). Text is referred to as sparse data due to the high variability
relative to number of samples, and unstructured data as the underlying linguistic
structure must be inferred from the surface form as part of the analysis process.
We recognize that many applications of text analytics use linguistically rather
naïve methods, typically disregarding word order and operating at the symbolic
word-level alone. While these applications generally constitute pioneering work
in their respective areas, there is currently ample opportunity for advancement,
in particular in the intersection between machine learning, computational lin-
guistics and economics. Following the deep learning paradigm, recent develop-
ments in natural language processing open up for highly data-driven but lin-
guistically more aware analysis methods, which can be easily applied to new
domains and tasks. In this paper, we show how such an approach can be ap-
plied to the study of risks in the ﬁnancial system, with relatively little eﬀort
required in terms of collecting data for supervision in new tasks. The method we
put forward, applied to bank distress, can provide an index over stress-related
reporting in news over time, and retrieve descriptions of the events based on it.
Prediction of bank distress has been a major topic both before and following
the global ﬁnancial crisis. Many eﬀorts are concerned with identifying the build-
up of risk at early stages, oftentimes relying upon aggregated accounting data
to measure imbalances (e.g., [4,12,2]). Despite their rich information content,
accounting data pose two major challenges: low reporting frequency and long
publication lags. A more timely source of information is the use of market data
to indicate imbalances, stress and volatility (e.g., [5,14]). Yet, market prices
provide little or no descriptive information per se, and only yield information
about listed companies or companies’ traded instruments (such as Credit Default
Swaps). This points to the potential value of text as a source for understanding
bank distress. More generally, central banks are starting to recognize the utility
of text data in ﬁnancial risk analytics, too. [7,3]

The literature on text-based computational methods for measuring risk or
distress is still rather scant. For instance, Nyman et al. [17] analyze sentiment
trends in news narratives in terms of excitement/anxiety and ﬁnd increased
consensus to reﬂect pre-crisis market exuberance, while Soo [26] analyses the
connection between sentiment in news and the housing market. Both rely on
manually-crafted dictionaries of sentiment-bearing words. While such analysis
can provide interesting insight as early work on processing expressions in text to
study risk, the approach is generally limiting as dictionaries are cumbersome to
adapt to speciﬁc tasks, incomplete and unable to handle semantics beyond single
words well. Nevertheless, sentiment analysis based on such simple approaches
works quite well due to the fact that it relies on human emotions as strong priors
in a way that generalizes across tasks and data, and because lower recall may
be countered by the scale of the data. Malo et al. [11] explore a linguistically
more sophisticated approach that models ﬁnancial sentiment compositionally,
although without semantic generalization, supervised by a custom data set of
annotated phrases.

3

Data-driven approaches, such as Wang & Hua [29] predicting volatility of
company stocks from earning calls, may avoid the issues of handcrafted features
and manually annotated corpora. Their method, although allegedly providing
good predictive performance gains, oﬀers only limited insight into the risk-related
language of the underlying text data. It also leaves room for further improve-
ments with regards to the semantic modeling of individual words and sequences
of words, which we address. Further, Lischinsky [10] performs a crisis-related
discourse analysis of corporate annual reports using standard corpus-linguistic
tools, including some data-driven methods that enable exploration based on a
few seed words. His analysis focuses extensively on individual words and their
qualitative interpretation as part of a crisis discourse. Finally, Rönnqvist & Sar-
lin [19] construct network models of bank interrelations based on co-occurrence
in news, and assess the information centrality of individual banks with regards
to the surrounding banking system, a fully data-driven approach that could be
further enhanced by semantic modeling and conditioning.

We focus on a purely data-driven approach to detect and describe risk, in
terms of quantitative indices and extracted descriptions of relevant events. In
particular, we demonstrate this by learning to predict coinciding bank stress
based on news, where a central challenge is to link the sparse and unstructured
text to a small set of reference events. To this end, we demonstrate a deep
learning setup that learns semantic representations of text data for a predictive
model. We train the model to provide coinciding indices of distress reporting,
while, most importantly, connecting text and distress to provide descriptions.
These text descriptions help explain the quantitative response of the predictive
model and allow insight into the modeled phenomenon. The method is read-
ily adaptable to any phenomenon by selecting the type of reference events for
training.

In the following section, we discuss the text and event data we model on
and use to demonstrate the applicability of our approach to the study of stress.
The deep learning setup, including semantic modeling, predictive modeling and
evaluation, extraction of descriptions and the related indices is explained in
Section 3. Finally, we report our experiments with bank stress data and reﬂect
on the results in Section 4.

2 Event and text data

The modeling in our setup is founded on connecting two types of data, text and
event data, by chronology and entities. An event data set contains information
on dates and names of involved entities, relating to the speciﬁc type of event
to be modeled. Hence, a particular event is ﬁrst associated with speciﬁc text
documents using the date and positions in them based on occurrences of the
entity name. The model will then learn to generalize across examples and more
speciﬁcally associate speciﬁc language with the target event type.

For the particular study in this paper, the event data set covers data on large
European banks as entities, spanning periods before, during and after the global

4

ﬁnancial crisis of 2007–2009. We include 101 banks for which 243 distress events
have been observed during 2007Q3–2012Q2. Following Betz et al. [2], the events
include government interventions and state aid, as well as direct failures and
distressed mergers. In addition, we map each bank to the country or countries
where it is registered, to allow for aggregation of results to the country level.

The text data consist of news articles from Reuters online archive from the
years 2007 to 2014 (Q3). The data set includes 6.6M articles (3.4B words). Bank
name occurrences are located using a set of patterns deﬁned as regular expres-
sions that cover common spelling variations and abbreviations. The patterns
have been iteratively developed against the data to increase accuracy, with the
priority of avoiding false positives (in accordance to [19]). Scanning the corpus,
262k articles are found to mention any of the 101 target banks.

Each article containing matches is cross-referenced against the event data in
order to cast the article as coinciding or not, i.e., likely to discuss the event or not.
The labeling as coinciding is based on a time window around the event date that
the article’s date of publication should fall within. An outer time window deﬁnes
articles published outside as non-coinciding, whereas intermediate articles are
discarded to avoid ambiguity. We set the inner time window from 8 days before
to 45 days after the event, and the outer from 120 days before to 120 days after,
as optimized through the evaluation scheme discussed in Section 3.2.

The training data is organized as tuples of bank name, article and occurrence
position (e.g., sentence number), publication date and the assigned label. The
data points are later aggregated monthly and by entity for evaluation of the
predictive model and the analysis of distress levels over time.

3 The semantic deep learning setup

Characterized in part by the deep, many-layered neural networks, a prevailing
idea of the deep learning paradigm is that machine learning systems can become
more accurate and ﬂexible when we allow for abstract representations of data to
be successively learned, rather than handcrafted through classical feature engi-
neering. By modeling the input data before modeling speciﬁc tasks, the networks
can learn about regularities in the world and generalize over them, which im-
proves performance on supervised task learning. For a recent general survey on
deep learning confer Schmidhuber [22], and for a more explicit discussion of deep
learning in natural language processing see Socher & Manning [25].

While manually designed features help bring structure to the learning task
through the knowledge they encode, they often suﬀer problems of being over-
speciﬁed, incomplete and laborious to develop. Especially regarding natural lan-
guage processing, this limits the robustness of text mining systems and their
ability to generalize across languages, domains and tasks. By exploiting statis-
tical properties of the data, features can be learned in an unsupervised fashion
instead, which allows for large-scale training not limited by the scarcity of anno-
tated data. Such intensively data-driven, deep learning approaches have in recent
years led to numerous breakthroughs in application domains such as computer

vision and natural language processing, where a common theme is the use of
unsupervised pre-training to eﬀectively support supervised learning of deep net-
works [22]. We apply the same idea in modeling events in text, as discussed in
the following.

5

3.1 Modeling

We are interested in modeling the semantics of words and compositionality of
sequences of words to obtain suitable representations of the content of the news,
to use as features for predicting events and associating text descriptions. At the
word level, distributional semantics exploits the linguistic property that words of
similar meaning tend to occur in similar contexts [6]. Word contexts are modeled
to yield distributed representations of word semantics as vectors, as opposed to
declarative formats, which allow measuring of semantic similarities and detect-
ing analogies without supervision, given substantial amounts of text [23,24,13].
The distributional semantic modeling captures the nature of words in a broader
sense, in the directions of syntax and pragmatics. These word vectors provide an
embedding into a continuous semantic space where the symbolic input of words
can be geometrically related to each other, thus supporting both the predictive
modeling in this paper and a multitude of other natural language processing
tasks (e.g., tagging, parsing and relation extraction [25]).

While traditionally modeled by counting of context words, predictive models
have eventually taken the lead in terms of performance [1]. Neural network lan-
guage models in particular have proved useful for semantic modeling, and are
especially practical to incorporate into deep learning setups due to their dense
vectors and the uniﬁed neural framework for learning. Mikolov et al. [13] have
put forward an eﬃcient neural method that can learn highly accurate word vec-
tors as it can train on massive data sets in practical time (a billion words in the
order of a day on standard architecture).

Subsequently, Le & Mikolov [9] extended the model in order to represent
compositional semantics (cf. [15]) of sequences of words, from sentences to the
length of documents, which they demonstrated to provide state-of-the-art per-
formance on sentiment analysis of movie reviews. Methods based on other neural
architectures and explicit sentence structure have since gained slightly improved
performance [8,27], but require parse trees as pre-structured input and are there-
fore not as ﬂexible. Analogous to the sentiment analysis task, we employ the dis-
tributed memory method of Le & Mikolov to learn vectors for sentences in news
articles, where entities are mentioned, and use them for learning to predict the
probability of an event. Hence, when providing bank distress events, the task can
be understood as a type of risk sentiment analysis that models language speciﬁc
to the type of event, rather than more general expression of emotions explicitly.
Our deep neural network for predicting events from text, outlined in Fig. 1,
is trained in two steps: through learning of sentence vectors as pre-training (1a),
followed by supervised learning against the event signal (1b). The use of the
distributed memory model in the ﬁrst step is explained in the following.

6

Fig. 1. Deep neural network setup for (a) pre-training of semantic vectors, and (b)
supervised training against event signal e.

Sentence ID, sWord wiWord wi+1Word wi+n......Word wi+n+1Projection layerInput layerOutput layerMappingInput dataEvent signalHidden layerOutput layerInput layerSentence ID, sSemantic vectors, Va) Semantic pre-trainingb) Supervised training7

The modeling of word-level semantics works by running a sliding window over
text, taking a sequence of words as input and learning to predict the next word
(e.g., the 8th in a sequence), using a feed-forward topology where a projection
layer in the middle provides the semantic vectors once the connection weights
have been learned. A semantic vector Vi is the ﬁxed-length, real-valued pattern
of activations reaching the projection layer for network input i. The projection
layer provides a linear combination that enables eﬃcient training on large data
sets, which is important in achieving accurate semantic vectors. In addition, the
procedure of [9] for sentence vector training includes the sentence ID as input,
functioning as a memory for the model that allows the vector to capture the
semantics of continuous sequences rather than only single words; the sentence
ID in fact can be thought of as an extra word representing the sentence as global
context and informing the prediction of the next word. While the prediction from
word context to word constitutes a basic neural language model, the sentence ID
conditions the model on the sentence and forces the sentence vector to capture
the semantics that is particular to the sentence rather than the language overall.
Formally, the pre-training step seeks to maximize the average log probability:

t−nX

i=1

1
t − n

log p(wi+n+1|s, wi, ..., wi+n)

over the sequence of training words w1, w2, ..., wt in sentence s with word context
size n. In the neural network, an eﬃcient binary Huﬀman coding is used to map
sentence IDs and words (an index i) to activation patterns in the input layer,
which imposes a basic organization of words by frequency. The illustrated parallel
connections are element-wise averaged together as they reach the projection
layer.

The second modeling step (Fig. 1b) is a normal feed-forward network fed by
the sentence vectors Vs (pertaining to the set of sentences S), which we train by
Nesterov’s Accelerated Gradient [16] to predict distress events e ∈ {0, 1}. Hence,
the objective is to maximize the average log probability:

X

s∈S

1
|S|

log p(es|Vs)

The network has two output nodes for e ∈ {0, 1} in a softmax layer that
applies a cross-entropy loss function. In the trained network, the posterior prob-
ability p(ed = 1|Vs) reﬂects the relevance of sentence s to the modeled event
type.
Text sequences are also commonly modeled by recurrent neural networks, but
are not as eﬃcient as feed-forward topologies with ﬁxed context size. Compared
to sequential text, the sentence vector is a practical ﬁxed-size representation
suitable as input to a feed-forward network. For each word, the input text orig-
inally has a dimensionality equal to the vocabulary size (typically in the order
of a million words), but the semantic modeling provides reduction to the size of
the vector (typically 50–1000). Both these aspects help train the model against

8

a signal corresponding to a comparatively tiny number of events. In our experi-
ments, sentence vectors are learned for all entity-mentioning sentences and the
predictive network is trained on individual pairs of sentence vector and binary
event signal, matched together based on date and entity name occurrence as
discussed in Section 2. While this matching procedure furthers the predictive
modeling by expanding the example set, compared to the original event data
set, the semantic modeling provides necessary generalization of word semantics
and representation of compositional semantics.

3.2 Evaluation and aggregation
Assuming that the distribution of events for a particular entity is sparse over
time, the procedure for matching events to text produces examples with skewed
class frequencies. Moreover, it is likely that the user has an imbalanced preference
between types of errors, preferring a sensitive system to detect possible events
and provide means for further investigation in the form of descriptions, rather
than missing an event. This requires extra care in evaluation.

We evaluate the performance of the predictive model to guide hyperparame-
ter optimization and asses the quality of indices that it will produce, and impor-
tantly to provide a quantitative quality assurance for the information content
of the descriptions we extract. We use the relative Usefulness measure (Ur) by
Sarlin [21], as it is commonly used in distress prediction and intuitively incorpo-
rates both error type preference (µ) and relative performance gain of the model.
Based on the combination of negative/positive observations (obs ∈ {0, 1}) and
negative/positive predictions (pred ∈ {0, 1}), we obtain the cases of true nega-
tive (T N ≡ obs = 0∧ pred = 0), false negative (F N ≡ obs = 1∧ pred = 0), false
positive (F P ≡ obs = 0∧pred = 1) and true positive (T P ≡ obs = 1∧pred = 1),
for which we can estimate probabilities when evaluating our predictive model.
Further, we deﬁne the baseline loss Lb to be the best guess according to prior
probabilities p(obs) and error preferences µ (Eq. 1) and the model loss Lm (Eq.
2):

(

Lb = min

µ · p(obs = 1)
(1 − µ) · p(obs = 0)

(1)

(2)
From the loss functions we derive Usefulness in absolute (Ua) and relative

Lm = µ · p(F N) + (1 − µ) · p(F P)

terms (Ur):

Ur = Ua
Lb

= Lb − Lm

Lb

(3)

While absolute Usefulness Ua measures the gain vis-à-vis the baseline case,
relative Usefulness Ur relates gain to that of a perfect model (i.e., Eq. 5 with
Lm = 0 ⇒ Ua = Lb). Usefulness functions both as a proxy for benchmarking the
model (testing) and to optimize its hyperparameters (validation). Usefulness

9

can also be related to the in text mining widely used F-score[28] (based on
precision = p(obs = 1|pred = 1) and recall = p(pred = 1|obs = 1)):

Fβ = (1 + β2) ·

precision·recall

(β2 · precision) + recall

(4)

which similarly can account for varying preferences by its β parameter, although
not gain. The Fβ-score assigns β times as much importance to recall as to pre-
cision (i.e., preference for completeness over exactness)[28], which is analogous
to but not directly transferable to the µ parameter in the Usefulness measure.
While the F-score is commonly seen to maximize completeness versus exactness
of true positives, the parameter can also be seen as a priority to minimize false
negatives versus false positives (FN prioritized over FP when β > 1). As a heuris-
tic, we map the balanced, standard F1-score with β = 1 to Ur with µ = 0.5, and
match deviations from these preferences according to β = µ/(1 − µ).
In order to inﬂuence the sensitivity of the model, we may classify by threshold
on the positive-class posterior probability: p(ed = 1|Vs) ≥ t. The threshold is
optimized on the validation set with respect to Usefulness at a given preference,
and applied to the test set for evaluation. However, evaluating classiﬁcation at
the entity level rather than the level of sentence instances is more suitable to
the use case, and likely more robust as the classiﬁcation combines evidence from
multiple observed occurrences in the text. The entity-level classiﬁcation uses
the index deﬁned in Eq. 5 below instead of the direct posterior probability, i.e.,
according to I(p, b) ≥ t.

Furthermore, evaluation on the sentence vector level with a randomized set
split into train, validation and test set may produce somewhat optimistic results,
as speciﬁc language related to one particular event can be expected to be shared
among several instances. Thus, the evaluation would not truly reﬂect how well
the model can be expected to generalize across events of the same type, including
future occurrences. To counter the bias, we sample the cross-validation folds
according to a leave-N-entities-out strategy (or leave-N-banks-out), based by
entity rather than instance, such that discussion about a particular entity is
compartmentalized into a single set. In case of very frequent entities that would
cause very skewed fold sizes, the instances may be split by period such that
the more recent occurrences are placed in the latter set (e.g., test rather than
validation set) to minimize possible cross-contamination.

3.3 Event indices

The aggregated posterior probabilities form an index to reﬂect the level of event-
related reporting about an entity over time, thereby guiding exploration and
extraction of descriptions, while it also serves as the signal that we evaluate
against. The entity-level relevance index I : p × b → [0, 1] is formalized as:

M(Vs)

(5)

I(p, b) = 1
|Sp,b|

X

s∈Sp,b

10

over the sentences Sp,b that mention entity b in period p, where M(V ) = p(e =
1|V ) gives the posterior probability of the trained neural network model.

In order to obtain further overview, it may be motivated to further group
entities and aggregate their indices. In the experiments, we ﬁrst aggregate from
sentences to banks, and then from banks to countries to highlight national diﬀer-
ences across Europe. The second-level index (or country-level index) is a weighted
average, deﬁned as:

I0(p, c) = 1
|Bc|

X

b∈Bc

I(p, b) · |Sp,b|

(6)

where Bc is the set of entities in category/country c. Finally, we deﬁne a top-level
index that summarizes the level of relevant reporting for all modeled entities as
a global average of vectors:

I00(p) = 1
|Sp|

X

s∈Sp

M(Vs)

(7)

where Sp is the set of vectors for all entity-mentioning sentences in period p.

3.4 Extraction of descriptions

As the neural network in the second step of the deep setup has been trained
and the hyperparameters optimized by cross-validation, it can be applied to
sentence vectors V and the posterior probability M(V ) used as a relevance score
with respect to the event type. The indices (Eq. 5 and 6) provide overview over
time and can highlight peaks and periods with elevated volumes of event-related
discussion, which can be more closely investigated by retrieving descriptions of
the underlying events.

Given a speciﬁc period and entity or set of entities, the basic principle in
retrieving descriptions is to ﬁlter and rank pieces of text based on the posterior
probability of the predictive model for the corresponding semantic vector. In
the current setup, we perform the semantic modeling on the sentence level,
which simpliﬁes the process of retrieving relevant and speciﬁc passages. The
semantic modeling can be applied to any type of textual unit, including complete
documents, but that requires additional measures for locating the interesting
parts within the broader context. Rönnqvist and Sarlin [20] explore this by
applying the same semantic and predictive model on both documents and words,
to weight the relevance of the context and individual words respectively. In
current experiments, we found that, while their method works for document
vectors that are trained on a larger number of words per vector, it does not work
well for sentence vectors as they tend to be less similar to the word vectors of
the same model. Overall, the extracts as presented in Section 4 are qualitatively
better when produced based on sentence-level modeling.

Sentence vectors are trained only for sentences that mention target entity
names, as it would be infeasible in terms of memory to model each sentence

separately for a large corpus, and because the direct discussion about the enti-
ties is the primary interest. The near context of such sentences however tend to
support interpretation and are useful to include in presentation. The semantic
model supports inference of vectors for at train-time unseen sentences, although
with noisier results. We infer vectors and predict the relevance of the sentences
immediately before and after sentences in which entities occur, as there is strong
dependency between neighboring sentences and a combined score of the ex-
panded context may produce more robust predictions. The combined score for
an excerpt is calculated as:

M (Vsi)
M

(cid:16) 1
(cid:16) 1

n

n

M

Pn
Pn
j=1 V 0
j=1 V 0

si−1

si+1

(cid:17)
(cid:17)

11

(8)



xi = max

which includes one sentence before and after sentence si. V 0 is a stochastic,
inferred vector and n is the number of samples (e.g. 100).

The excerpts are ranked according to the score for presentation and oﬀer a
preview of the most prominent event-related discussion, which may be retrieved
in full from the individual articles. The experiments that follow demonstrate the
utility of the excerpts in highlighting the speciﬁc forces that drives the index
when applied to model bank distress.

4 Experiments
We test the deep neural network setup for modeling event-related language on
European bank distress events and news data, in order to demonstrate the value
it can bring in helping to identify and understand past, ongoing or mounting
events. As input for the ﬁrst step of training the vectors for all sentences that
mention any of the target banks are used. Corresponding sentence vectors are
learned for each of the 716k sentences (originating from 262k articles), while
the whole news corpus of 6.6M articles (3.4B words) is sampled in the training
process in multiple iterations. Through the semantic modeling on the target
sentences, the sentence vectors capture the semantics speciﬁcally of reporting
related to our banks of interest, while other text helps model the semantics of
English in news reporting in general. We optimized the sentence vector length
to 600 and context size to 5 by cross-validation. We also tested the inﬂuence
of text sequence lengths, and found that training a vector on multiple sentence
achieved slightly worse predictive performance, while vectors trained at sentence
and document level were comparable.

4.1 Predictive modeling and evaluation
Following the semantic pre-training, we train a predictive neural network model
with 3 layers. The input layer has 600 nodes, corresponding to the semantic
vectors and a two output nodes corresponding to distress/tranquil. As described

12

in Section 2, a set of tuples are complied as data to learn a predictive model of
distress based on sentences. The set consists of 368k cases, sentences that occur
within the time span of the event data and are categorized as either distress-
coinciding or non-coinciding. 9.0% of the cases are labeled as coinciding following
our matching procedure.

We evaluate the predictive model with the four combinations of sampling
method and level of evaluation, discussed in Section 3.2. The baseline evaluation
with random sampling at the level of sentence vectors is reported in Table 1
(left), providing 27.5% relative Usefulness, i.e., performing signiﬁcantly better
than majority class prediction even with the highly skewed class distributions.
By comparison, evaluation at the aggregated bank level (classifying by I(p, b)
(Eq. 5) rather than M(V )) reduces noise from single sentences and stabilizes pre-
diction, thereby increasing performance to 32.6% (Table 1, center). These results
show that the model is eﬀective in linking the relevant pieces of text to the bank
distress events, hence, providing a ﬁrst assurance of the quality of the descrip-
tions we will retrieve. Further, we evaluate based on leave-N-banks-out sampling,
i.e., the cross-validation folds of vectors are organized by bank, such that the
vectors of banks used for testing are held out of training. While this produces
lower Usefulness scores, it is a more realistic estimate of future performance
in the context of deploying the model on unseen banks or future data. With
vector-level evaluation we reach 8.3% relative Usefulness (Table 1, right), while
bank-level aggregation again stabilizes prediction and improves performance to
12.3% of available Usefulness (Table 2).

We ﬁnd the optimal network (50 rectiﬁed linear hidden nodes), hyperparame-
ters for the NAG training algorithm to train its weights, and threshold on M(V )
or I(b, p) for classifying e ∈ {0, 1}, after which we evaluate performance by Ur
of the optimal model. We trained the network by randomized 5-fold cross vali-
dation with one fold for validation and one for testing, in multiple reshuﬄes of
the data set. The evaluation yielded an area under the ROC curve of 0.712 with
a standard deviation σ = 0.008 with random sampling evaluated at vector level,
and an area of 0.645 (σ = 0.083) with leave-N-banks-out sampling evaluated at
bank level.
Following previous studies [2,18], we make use of a skewed preference µ ≈ 0.9
(i.e., missing a crisis is about 9 times worse than falsely signaling one). From the
viewpoint of policy, highly skewed preferences are particularly motivated when
a signal leads to an internal investigation, and reputation loss or other political
eﬀects of false alarms need not be accounted for. We conclude that at µ = 0.9
with vector-level evaluation and at µ = 0.875 with aggregated evaluation the
model has decent predictive performance by capturing up to 33% of available
Usefulness and 12% in the more conservative leave-N-banks-out sampled exercise
(cf. [2,18] for comparable results). While the model is not robust to low levels of
µ, we can see in Table 2 that Usefulness is positive and peaking for µ near 0.9.
Meanwhile, F-score is reaching its maximum at the extreme preference, which
is an indication of its failure to capture gain over the majority class baseline.

Vector-level
¯Ur(µ)
σU
0.004
-0.004
0.004
-0.007
0.005
0.002
0.007
0.013
0.038
0.011
0.019
0.095
0.026
0.157
0.028
0.207
0.275
0.054
0.041
0.253
0.106
0.044

Random sampling
¯Ur(µ)
-0.022
-0.015
-0.014
-0.015
0.027
0.156
0.260
0.326
0.268
0.148
-0.009

Aggregated
σU
0.029
0.013
0.010
0.012
0.030
0.029
0.030
0.030
0.031
0.040
0.038

µ
0.1
0.3
0.5
0.6
0.7
0.8
0.85
0.875
0.9
0.925
0.95

13

Leave-N-banks-out
¯Ur(µ)
-0.013
-0.032
-0.039
-0.038
-0.026
-0.008
0.025
0.039
0.083
0.040
-0.052

Vector-level
σU
0.013
0.026
0.036
0.039
0.029
0.044
0.048
0.133
0.114
0.109
0.153

Table 1. Cross-validated predictive performance as relative Usefulness over preferences
between types of error (µ), evaluated at vector and aggregated bank level with random
sampling, and at vector level with leave-N-banks-out sampling.

Leave-N-banks-out, aggregated

µ
0.1
0.3
0.5
0.6
0.7
0.8
0.85
0.875
0.9
0.925
0.95

¯Ur(µ)
-0.014
-0.011
-0.015
-0.013
-0.003
0.048
0.122
0.123
0.081
-0.006
-0.075

σU
0.042
0.022
0.029
0.027
0.038
0.154
0.147
0.173
0.162
0.173
0.160

¯Fβ
0.497
0.087
0.031
0.032
0.087
0.314
0.434
0.529
0.629
0.741
0.901

σF
0.000
0.015
0.013
0.020
0.063
0.171
0.153
0.174
0.189
0.190
0.125

¯TN
516
516
516
515
511
472
435
374
308
151
38

¯FN
68
68
68
68
65
53
45
38
31
14
4

¯FP
0
0
0
1
4
44
80
142
208
364
477

¯TP
0
0
0
0
3
15
22
30
37
54
64

Table 2. Cross-validated predictive performance as relative Usefulness and F -score
over preferences between types of error (µ) and recall/precision (β), evaluated at bank
level with leave-N-banks-out sampling. Mean confusion matrix values are included, too.

14

4.2 A descriptive stress index for Europe
Having trained the network and evaluated its predictive performance, we can
reliably extract descriptive indices of stress at the diﬀerent levels of aggregation.
First, Figure 2 provides an overview of the raw distress reporting in Europe
over the recent years, in terms of distributions of posterior probabilities of the
sentence vectors, illustrated through their percentiles. At the same time, this
distribution communicates the dynamics of the stress situation in Europe, while
the mean (index I00 of Eq. 7) summarizes the general trends.

The time span July 2007 to June 2012 is covered by the event data, and the
rest is produced by applying the trained model. The indices show a sharp double
peak starting September 2008, which coincides with the outbreak of the ﬁnancial
crisis. Prior to the most signiﬁcant peaks, one can also observe elevated values
for the indices between August and October 2007, pointing to early discussion on
the signiﬁcance of subprime activities overall and liquidity in European banks.
The outbreak of the ﬁnancial crisis in 2008 is followed by over a year of relatively
high stress, where a substantial part of the cross section is elevated. A second
signiﬁcant and similar peak of the stress index is reached in October 2009. At
the end of 2010 and 2011, one can observe notable jumps in the most extreme
percentiles, whereas the rest of the cross section remains largely unaﬀected.

At a general level, the peak in September 2008 can be seen to relate to the
overall distress in ﬁnancial markets due to the collapse of Lehman Brothers in
mid-September. However, the fact that values at the top of the distribution
appear rather unstable from month to month reﬂects that diﬀerent banks are
being mentioned over time and usually not persistently across months in distress
contexts. By observing increases and peaks in the index of an individual bank
or banks in a country, we can locate events of possible relevance to distress. The
ability to extract descriptions for these events then becomes useful in order to
discern what has happened in relation to banks and distress. Based upon the
method described in Section 3.4, these peaks can be described in more detail by
extracting top ranking distress-related excerpts. While the overview over Europe
supports such analysis, a breakdown into countries or banks brings more distinct
patterns forward and allow for deeper and better targeted qualitative analysis.
The following section continues the analysis at the country level.

Fig. 2. Raw distress reporting. Distribution of posterior probabilities over time for sentence vectors, indicating the levels of news reporting
relating to bank stress. The blue line indicates mean, faded lines every 2nd percentile, and dotted lines predictions outside the event
sample.

1
5

0.00.10.20.30.40.5MonthsPosterior probability2007.012007.072008.012008.072009.012009.072010.012010.072011.012011.072012.012012.072013.012013.072014.012014.0716

4.3 Country-level stress, descriptions and interpretation
From the general stress index for Europe, this section moves to a more granu-
lar perspective on stress. We measure stress-related discourse for countries for
a more targeted stress measure, which also allows more economic interpretation
of developments. Thus, we now aggregate posterior probabilities over time for
sentence vectors, indicating the levels of news reporting relating to bank stress,
but selectively at a country level (according to Eq. 6). Figure 3 show the de-
velopment in stress-related discussion for Belgium and Ireland and Figure 4 for
Germany and UK. The ﬁgures illustrate stress levels as time series, as well as
annotate peaks of distress levels with top-ranked excerpts. In the appendix, we
include plots in Figures 6 and 7 for the other countries whose banks we model.
In Figure 3, the stress levels for Belgium peak in September 2008. Looking
at top-ranked excerpts, September 27 is coupled with a range of rumours in
media, yet no oﬃcial release or actions to mitigate the weakened position of
particularly Fortis Bank. Then, the next days we see a bailout of Fortis being
discussed as the Belgian, Dutch and Luxembourg governments rescued Fortis.
Likewise, the lower chart for Ireland in Figure 3 shows increased concerns over
Bank of Ireland and other large Irish banks in November 2008, as both their
earnings and shares were signiﬁcantly falling. After a range of actions by the
state, distress levels were still peaking in September 2009, which is particularly
related to the amounts that Allied Irish Banks was putting into the Irish "bad
bank". Still, in March 2010 three large Irish banks were still transferring large
loans to the National Asset Management Agency (NAMA). Thereafter the most
acute stress decreased and has since been at lower levels, although remaining
somewhat volatile.

Figure 4 provides similar stress time series and top-ranked excerpts, but
for Germany and UK. Germany can be seen to signal already in August 2007,
when IKB’s problems were highlighted to potentially lead to "Germany’s worst
ﬁnancial crisis in more than 75 years". Three days after this news Deutsche Bank
cut a credit line to IKB, as they were worried about IKB’s subprime exposures,
which further triggered distress in the German banking sector. One reason to
the failure of IKB related to an oﬀshore portfolio that was kept oﬀ IKB’s balance
sheet by Rhineland Funding, which is said to have been explained to the largest
shareholder KfW. The same large shareholder is then a few months later involved
in helping IKB back on its feet with a hefty 4.8 billion euros, as well as additional
smaller support afterwards. For UK, stress increased in September 2008, relating
not only to previous aid to the UK-based Northern Rock but also to Germany’s
IKB. Only a few days later in conjunction with a strict clampdown on short-
selling, UK-based bank Lloyds Group bought rival HBOS in a rescue takeover.
Ironically, a few months later in February 2009 Lloyds in partly nationalized as
its HBOS unit made an 8.5 billion pounds loss last year.

4.4 The case of Fortis and IKB Bank
This section takes a ﬁnal step towards more granular output by providing a
stress measure for individual banks (according to Eq. 5). As with the country-

17

Fig. 3. Distress index for Belgium and Ireland, with key periods marked and informa-
tive excerpts selected from the top-10 of each period and country. Vertical lines indicate
distress events and dotted lines out-of-sample predictions. Quotes are from Reuters at
given dates.

IrelandBelgiumSaturday, 27 September 2008 (relevance 0.921, rank 2): "Fortis investors face a weekend of uncertainty after the banking and insurance group went out of its way on Friday to reassure them that it was solvent and in no danger of collapse following market talk the company could become another casualty of the credit crisis."Saturday, 27 September 2008 (relevance 0.917, rank 3): "As of Saturday, ﬁnancial authorities were contacting other institutions, a source familiar with the situation told Reuters, although no particular solution was preferred and nothing concrete was likely to emerge before Sunday."Sunday, 28 September 2008 (relevance 0.758, rank 6): "BRUSSELS (Reuters) - Belgium's national pride and thousands of jobs are at stake as the Belgian and Dutch governments, central banks and regulators seek to secure the future of ﬁnancial services group Fortis."Monday, 29 September 2008 (relevance 0.889, rank 5): "Belgian, Dutch and Luxembourg governments rescued Fortis over the weekend to prevent a domino-like spread of failure by buying its shares for 11.2 billion euros."Thursday, 13 November 2008 (relevance 0.55, rank 9): "DUBLIN (Reuters) - Bank of Ireland (BKIR.I) on Thursday posted a 34 percent fall in ﬁrst-half earnings, predicted it should be near to breakeven in the second half and cancelled its cash dividend to shore up its capital position."Friday, 21 November 2008 (relevance 0.677, rank 5): "Exposure to a falling property market has hit investor sentiment and shares in the four listed banks have fallen over 90 percent from highs set last year. Bank of Ireland said it had received unsolicited approaches from a number of unnamed parties seeking to invest in the group, Ireland's second-largest bank by market capitalisation."Monday, 22 March 2010 (relevance 0.860 rank 4):"Dublin has already pumped a total of 11 billion euros ($15 billion) into Allied Irish Banks (ALBK.I), Bank of Ireland (BKIR.I) and nationalised Anglo Irish Bank, all of which will need further capital as they transfer loans to the National Asset Management Agency (NAMA)."Wednesday, 9 September 2009 (relevance 0.795 rank 10):"DUBLIN, Sept 9 (Reuters) - Bank of Ireland (BKIR.I) and Allied Irish Banks PLC (ALBK.I) will still have to shoulder much of the risk arising from their commercial property loans, despite the creation of a ``bad bank,'' Ireland's junior coalition party said on Wednesday."Wednesday, 16 September 2009 (relevance 0.850, rank 5): "Allied Irish Banks (AIB) (ALBK.I), Ireland's second biggest bank which is putting 24 billion euros of assets into the ``bad bank,'' said it had options to boost capital, including by raising equity, getting an outside investor or selling assets."18

Fig. 4. Distress index for Germany and United Kingdom, with key periods marked and
informative excerpts selected from the top-10 of each period and country. Vertical lines
indicate distress events and dotted lines out-of-sample predictions. Quotes are from
Reuters at given dates.

United KingdomThursday, 2 August 2007 (relevance 0.814, rank 2): "IKB's problems last weekend prompted German ﬁnancial watchdog Baﬁn to warn a collapse of the bank could trigger Germany's worst ﬁnancial crisis in more than 75 years."Sunday, 5 August 2007 (relevance 0.808, rank 7): "Deutsche Bank (DBKGn.DE), worried about IKB's subprime exposure, had cut a credit line to the bank, a move which sparked the crisis and spurred watchdog Baﬁn into action."Thursday, 23 August 2007 (relevance 0.826, rank 1): "``IKB would never have started Rhineland Funding without telling their biggest shareholder,'' said one source, adding that it was explained to KfW how Rhineland worked."Thursday, 29 November 2007 (relevance 0.860, rank 4): "KfW said on Tuesday it expected losses from its rescue of IKB to nearly double to 4.8 billion euros ($7.11 billion)."Friday, 30 November 2007 (relevance 0.842, rank 10): "KfW said on Thursday those banks that helped IKB back onto its feet earlier would chip in another $520 million."Wednesday, 10 September 2008 (relevance 0.702, rank 1): "Some governments have already had to help banks in trouble, such as Britain's Northern Rock and Germany's IKB, and there is no consensus on who would bail out a failed multinational bank."Wednesday, 17 September 2008 (relevance 0.573, rank 3): "With the ﬁnancial landscape undergoing its most dramatic transformation since the Great Depression, reports emerged of takeovers involving No. 2 U.S. investment bank Morgan Stanley (MS.N), weakened top U.S. savings bank Washington Mutual (WM.N) and major UK mortgage lender HBOS HBOS.L."Thursday, 18 September 2008 (relevance 0.537, rank 4): "The move, the strictest major-market clampdown on short-selling to date, comes hours after British bank Lloyds TSB Group Plc agreed to buy rival HBOS Plc in a rescue takeover following a dramatic fall in the HBOS share price earlier this week."Wednesday, 11 February 2009 (relevance 0.533, rank 8): "On HBOS: ``We would not have had to take government money had we not bought HBOS."Monday, 16 February 2009 (relevance 0.542, rank 7): "Financial shares were one of the underperformingsectors in Europe after part-nationalised Lloyds Banking Group (LLOY.L) said on Friday its HBOS unit made an 8.5 billion pound loss last year, causing its shares to fall by a third."Germany19

level aggregates, we can aggregate posterior probabilities for sentence vectors
selectively by bank. This output could be derived for each of the 101 banks,
yet here we focus on the stress reporting for two banks, namely Fortis and IKB
Bank.

One of the early failures among European ﬁnancial institutions occurred to
the Benelux-based Fortis. As was also highlighted in the above described top
distress excerpts for Belgium, Fortis and the rescue procedure was at the core of
the discussion in the crisis. We herein focus on the evolution of the distress index
for Fortis, as is shown in Figure 5. To start with, we can observe that elevated
values for the stress index coincide with distress events.

Fig. 5. Indices (blue) for banks Fortis and IKB indicating the levels of bank stress-
related reporting, with faded lines showing every 4th percentile up to the 98th.

By the ﬁrst event in September 2008, the index rises to 0.30, which marks
the start of a prolonged period of elevated stress. The top-ranked excerpts relate
to a range of diﬀerent issues, such as worries about lacking conﬁdence in the
markets and the systemic nature of the unfolding crisis:

"Jean-Claude Juncker, also the prime minister of Luxembourg, was
asked whether the part nationalisation of Dutch-Belgian bank Fortis FOR.
-BR and a new injection of liquidity into money markets by the Euro-
pean Central Bank would restore market conﬁdence. “I can only hope

0.00.20.40.60.81.0Stress reporting on FortisMonthsIndex2007.012007.072008.012008.072009.012009.072010.012010.072011.012011.072012.012012.072013.012013.072014.012014.07lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.0Stress reporting on IKBMonthsIndex2007.012007.072008.012008.072009.012009.072010.012010.072011.012011.072012.012012.072013.012013.072014.012014.07lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll20

that conﬁdence will come back – ﬁnancial markets should not forget to
take a close look at the health of fundamental data of several banks –
and that this casino game, that’s going on independently from the good
fundamentals, stops,” he told reporters on the sidelines of a meeting in
parliament. Belgian, Dutch and Luxembourg governments rescued Fortis
over the weekend to prevent a domino-like spread of failure by buying its
shares for 11.2 billion euros." (Reuters 2008-09-29, relevance 0.963, rank
1)
"Investors also worried if a proposed U.S. rescue would stem the conta-
gion that pushed the British government to takeover troubled mortgage
lender Bradford & Bingley BB.L and three European governments to par-
tially nationalize banking and insurance group Fortis FOR.BRFOR.AS."
(Reuters 2008-09-29, relevance 0.923, rank 6)
In October 2008, the top excerpts discuss the continuing developments such
as the Benelux governments "carving up" Fortis to sell to private entities, includ-
ing French BNP Paribas buying control of the arms in Belgium and Luxembourg.
Further excerpts highlight the cross-border aspect of the interventions, and the
issues it entails:

"The Fortis deal is the biggest cross-border rescue since the full force of
the credit crisis swept across the Atlantic into Europe last month, upend-
ing banks and rattling saver conﬁdence." (Reuters 2008-10-06, relevance
0.945, rank 7)
"Dutch Finance Minister Wouter Bos fanned Belgian resentment by
telling journalists: ‘Many of the problems were hidden in the Belgian
part of the Fortis group.”" (Reuters 2008-10-05, relevance 0.945, rank 8)
This repeats the message of the already cited news for UK in September
2008, that "there is no consensus on who would bail out a failed multinational
bank".

Without a detailed analysis of the discussion around the IKB Bank, we can
again conclude from Figure 5 that the stress index takes high values during
the realized events. Generally, the top-ranked discussion herein correlates to
a large extent with the early top-ranked discussion for Germany, as was above
exempliﬁed. The discussion around the distress events relates to early indications
of stress, ties to other German banks and government actions taken during and
after the stress episodes. After a period of elevated stress during 2007–2008,
the ﬁgure illustrates that stress is still fairly volatile and that the most extreme
percentiles still take large values. This may relate to the fact that discussion
keeps relating to the 2007–2008 distress events, in that the solution to the stress
events was an acquisition by a investment company. The private equity ﬁrm Lone
Star acquired IKB Bank in 2008 with the aim of restructuring and selling the
bank, and accordingly any rumours still link it to the original stress discussion
during the global ﬁnancial crisis. Such references to past major stress events may
however also be an indication of current concerns about ﬁnancial stress.

5 Conclusions

21

We have presented a deep learning approach that combines two types of data,
news text and basic event information, with the aim of linking the two to describe
observed and predicted events. The approach entails unsupervised learning on
text in order to model its language and provide semantic vector representations
that are used as features for predictive modeling of events. The neural network-
based method that we put forward is able to work with a very small set of events
to discern what type of language and passages in the text are relevant to the
modeled event type. The semantic modeling utilizes large amounts of text data
to infer abstractions that counter the high variability and sparsity of language,
thus supporting prediction of infrequent events.

The semantic-predictive model can produce indices that indicate the level
of relevant discussion over time, overall or related to speciﬁc entities or groups
thereof. The indices can highlight interesting patterns and oﬀer guidance in
the search for relevant events, whereas the model very directly provides means
to rank and retrieve pieces of text from the news articles that describe the
quantitative signal.

We demonstrate the usefulness of the method and the possibilities of the
approach in general within the study of ﬁnancial risk, by modeling bank distress
events. The indices reﬂect the level of current reporting related to bank stress
over time at multiple levels: for Europe in general, individual countries and
speciﬁc banks. Guided by the indices, users may focus their search and retrieve
the relevant reporting of the time, in order to understand the developments
regarding, in this case, government interventions and rescues.

The method and our analysis exemplify how text may oﬀer an important
complementary source of information for ﬁnancial and systemic risk analytics,
which is readily available, current and rich in descriptive detail. We recognize
that deep learning approaches are useful in particular to handle the complexities
of such new types of data, while oﬀering necessary ﬂexibility when exploring new
ﬁelds of analysis. Seeking to harness the expressiveness of text, we should look
to computational linguistics for support in terms of theoretical foundations and
tools.

While we show that it is possible to predict relevance and retrieve informative
descriptions of events, we merely scratch the surface of the vast text material in
a cross section with our current method of presentation. A challenge remains in
developing methods that are able to meaningfully summarize the broader base
that may include a long tail of weakly signaling, subtile expression. Such signals
may be particularly important in order to register and track developments before
they materialize in severe and obvious events.

6 Appendix

Figures 6 and 7 provide country-level indices for the countries not included in
Figures 3 and 4, as well as the non-weighted average of all country indices.

22

Fig. 6. Distress index for Austria, Switzerland, Cyprus, Denmark, Spain, France,
Greece and average of all modeled countries. Vertical lines indicate bank-level distress
events and dotted lines out-of-sample predictions.

0.00.10.20.30.4Country averageMonthsScore0.00.10.20.30.4atMonthsIndex2007.012008.012009.012010.012011.012012.012013.012014.010.00.10.20.30.4chMonthsIndex2007.012008.012009.012010.012011.012012.012013.012014.010.00.10.20.30.4cyMonthsIndex2007.012008.012009.012010.012011.012012.012013.012014.010.00.10.20.30.4dkMonthsIndex2007.012008.012009.012010.012011.012012.012013.012014.010.00.10.20.30.4esMonthsIndex2007.012008.012009.012010.012011.012012.012013.012014.010.00.10.20.30.4frMonthsIndex2007.012008.012009.012010.012011.012012.012013.012014.010.00.10.20.30.4grMonthsIndex2007.012008.012009.012010.012011.012012.012013.012014.0123

Fig. 7. Distress index for Hungary, Italy, Luxembourg, Latvia, Netherlands, Portugal,
Sweden and Slovenia. Vertical lines indicate bank-level distress events and dotted lines
out-of-sample predictions.

0.00.10.20.30.4huMonthsIndex2007.012008.012009.012010.012011.012012.012013.012014.010.00.10.20.30.4itMonthsIndex2007.012008.012009.012010.012011.012012.012013.012014.010.00.10.20.30.4luMonthsIndex2007.012008.012009.012010.012011.012012.012013.012014.010.00.10.20.30.4lvMonthsIndex2007.012008.012009.012010.012011.012012.012013.012014.010.00.10.20.30.4nlMonthsIndex2007.012008.012009.012010.012011.012012.012013.012014.010.00.10.20.30.4ptMonthsIndex2007.012008.012009.012010.012011.012012.012013.012014.010.00.10.20.30.4seMonthsIndex2007.012008.012009.012010.012011.012012.012013.012014.010.00.10.20.30.4siMonthsIndex2007.012008.012009.012010.012011.012012.012013.012014.0124

Acknowledgment

The authors are grateful to Filip Ginter, József Mezei, Tuomas Peltonen and Niko
Schenk for their helpful comments. The paper also has beneﬁted from presenta-
tion at the Finnish Economic Association XXXVII Annual Meeting (KT-päivat),
12 February 2015, in Helsinki, Finland, the RiskLab/Bank of Finland/European
Systemic Risk Board (ESRB) Conference on Systemic Risk Analytics (SRA), 24
September 2015, in Helsinki, the workshop of GI-FG Neuronale Netze and Ger-
man Neural Networks Society, New Challenges in Neural Computation (NCˆ2),
10 October 2015, in Aachen, Germany, the IEEE Conference on Computational
Intelligence in Financial Engineering and Economics (CIFEr), 9 December 2015,
in Cape Town, South Africa, and the Financial Stability Seminar at the Riks-
bank, 12 January 2016, in Stockholm, Sweden.

References

1. M. Baroni, G. Dinu, and G. Kruszewski. Don’t count, predict! a systematic com-
parison of context-counting vs. context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association for Computational Linguistics, vol-
ume 1, pages 238–247, 2014.

2. F. Betz, S. Oprică, T. A. Peltonen, and P. Sarlin. Predicting distress in european

banks. Journal of Banking & Finance, 45:225–241, 2014.

3. D. Bholat, S. Hansen, P. Santos, and C. Schonhardt-Bailey. Text mining for cen-
tral banks. In Centre for Central Banking Studies Handbook, volume 33. Bank of
England, 2015.

4. R. A. Cole and J. W. Gunther. Predicting bank failures: A comparison of on- and
oﬀ-site monitoring systems. Journal of Financial Services Research, 13:103–117,
1998.

5. R. Gropp, J. Vesala, and G. Vulpes. Equity and bond market signals as leading
indicators of bank fragility. Journal of Money, Credit and Banking, 38(2):399–428,
2006.

6. Z. S. Harris. Distributional structure. Word, 10(23):146–162, 1954.
7. J. Hokkanen, T. Jacobson, C. Skingsley, and M. Tibblin. The Riksbank’s future
information supply in light of Big Data. In Economic Commentaries, volume 17.
Sveriges Riksbank, 2015.

8. O. Irsoy and C. Cardie. Deep recursive neural networks for compositionality in
language. In Advances in Neural Information Processing Systems, pages 2096–2104,
2014.

9. Q. Le and T. Mikolov. Distributed representations of sentences and documents. In
Proceedings of the 31st International Conference on Machine Learning (ICML-14),
pages 1188–1196, 2014.

10. A. Lischinsky. In times of crisis: a corpus approach to the construction of the global
ﬁnancial crisis in annual reports. Critical Discourse Studies, 8(3):153–168, 2011.
11. P. Malo, A. Sinha, P. Korhonen, J. Wallenius, and P. Takala. Good debt or bad
debt: Detecting semantic orientations in economic texts. Journal of the Association
for Information Science and Technology, 65(4):782–796, 2014.

12. K. Männasoo and D. G. Mayes. Explaining bank distress in Eastern European

transition economies. Journal of Banking & Finance, 33:244–253, 2009.

25

13. T. Mikolov, K. Chen, G. Corrado, and J. Dean. Eﬃcient estimation of word repre-
sentations in vector space. In Proceedings of Workshop at International Conference
on Learning Representations, 2013.

14. A. Milne. Distance to default and the ﬁnancial crisis. Journal of Financial Stability,

12:26–36, 2014.

15. J. Mitchell and M. Lapata. Composition in distributional models of semantics.

Cognitive science, 34(8):1388–1429, 2010.

16. Y. Nesterov. A method of solving a convex programming problem with convergence

rate o (1/k2). In Soviet Mathematics Doklady, volume 27, pages 372–376, 1983.

17. R. Nyman, D. Gregory, K. Kapadia, P. Ormerod, D. Tuckett, and R. Smith. News
and narratives in ﬁnancial systems: exploiting big data for systemic risk assessment.
BoE, mimeo, 2015.

18. T. Peltonen, A. Piloui, and P. Sarlin. Network linkages to predict bank distress.

ECB Working Paper, No. 1828, 2015.

19. S. Rönnqvist and P. Sarlin. Bank networks from text: Interrelations, centrality

and determinants. Quantitative Finance, 15(10), 2015.

20. S. Rönnqvist and P. Sarlin. Detect & describe: Deep learning of bank stress in the
news. In Proceedings of IEEE Symposium Series on Computational Intelligence,
pages 890–897, 2015.

21. P. Sarlin. On policymakers’ loss functions and the evaluation of early warning

systems. Economics Letters, 119(1):1–7, 2013.

22. J. Schmidhuber. Deep learning in neural networks: An overview. Neural Networks,

61:85–117, 2015.

23. H. Schütze. Dimensions of meaning. In Proceedings of the 1992 ACM/IEEE Con-
ference on Supercomputing, Supercomputing ’92, pages 787–796, Los Alamitos, CA,
USA, 1992. IEEE Computer Society Press.

24. H. Schütze and J. Pedersen. Information retrieval based on word senses. In Pro-
ceedings of the 4th Annual Symposium on Document Analysis and Information
Retrieval, pages 161–175, 1995.

25. R. Socher and C. Manning. Deep learning for natural language processing (with-
out magic). Keynote at the 2013 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies
(NAACL2013). http://nlp.stanford.edu/courses/NAACL2013/.

26. C. K. Soo. Quantifying animal spirits: news media and sentiment in the housing

market. Ross School of Business Paper No. 1200, 2013.

27. K.S. Tai, R. Socher, and C.D. Manning. Improved semantic representations from
tree-structured long short-term memory networks. In Association for Computa-
tional Linguistics (ACL), 2015.

28. C.J. Van Rijsbergen. Information Retrieval. Butterworth, 2nd ed., 1979.
29. W. Y. Wang and Z. Hua. A semiparametric gaussian copula regression model for
predicting ﬁnancial risks from earnings calls. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Linguistics (ACL), 2014.

