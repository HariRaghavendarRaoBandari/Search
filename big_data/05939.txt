6
1
0
2

 
r
a

 

M
8
1

 
 
]

C
D
.
s
c
[
 
 

1
v
9
3
9
5
0

.

3
0
6
1
:
v
i
X
r
a

Online Distributed Scheduling on a Fault-prone Parallel System

Elli Zavou ∗1,2 and Antonio Fern´andez Anta1

1IMDEA Networks Institute, 28918, Legan´es (Madrid), Spain

2Universidad Carlos III de Mdrid, Madrid, Spain

Abstract

We consider a parallel system of m identical machines prone to unpredictable crashes and restarts,
trying to cope with the continuous arrival of tasks to be executed. Tasks have diﬀerent computational
requirements (i.e., processing time or size). The ﬂow of tasks, their size, and the crash and restart of the
machines are assumed to be controlled by an adversary. Then, we focus on the study of online distributed
algorithms for the eﬃcient scheduling of the tasks. We use competitive analysis, considering as eﬃciency
metric the completed-load, i.e., the aggregated size of the completed tasks.

We ﬁrst present optimal completed-load competitiveness algorithms when the number of diﬀerent
task sizes that can be injected by the adversary is bounded. (It is known that, if it is not bounded,
competitiveness is not achievable.) We ﬁrst consider only two diﬀerent task sizes, and then proceed to k
diﬀerent ones, showing in both cases that the optimal completed-load competitiveness can be achieved.
Then, we consider the possibility of having some form of resource augmentation, allowing the schedul-
ing algorithm to run with a speedup s ≥ 1.
In this case, we show that the competitiveness of all
work-conserving scheduling algorithms can be increased by using a large enough speedup.

Keywords: Scheduling, Parallel Computation, Non-uniform Tasks, Failures, Competitiveness, Online

Algorithms

1

Introduction

With the widespread use of cloud computing (which is essentially equivalent to computing in large scale
data centers) and big data processing, parallel computing is taking new forms. Parallelism appears as the
execution of lightly coupled tasks (or jobs), like the map or reduce tasks of a map-reduce computation,
in a collection of decoupled processors (or cores). The large scale of both size and time of these types of
computation, has two important consequences. First, it makes it highly likely that processors will fail during
the computation (failures are the norm, not the exception [9, 28, 17, 6]), and hence recovery mechanisms
must be an intrinsic part of the task scheduler. Second, it is unlikely that the information about all the tasks
to be scheduled is available at the initial time of the computation, which means that the task scheduler must
make online decisions. These two aspects make most prior work on task scheduling on parallel machines not
applicable in these new environments (e.g., [10, 15, 16, 22, 25, 26]). For instance, some works tackle the issue
of dynamic job arrivals but do not consider failures (e.g., [5]), others consider failures but assume knowledge
of all the jobs a priori (e.g., [18, 21]), ﬁnally, some others consider energy eﬃciency issues but not machine
failures, or only jobs of the same computational demand (e.g., [8]).

In this paper, we explore the scheduling of tasks in a parallel system like the one depicted in Figure 1
(which is similar to the one introduced in [3]). The system has m identical machines1 prone to crashes and
restarts (controlled by an adversary). Independent idempotent tasks with diﬀerent computational demands

∗elli.zavou@imdea.org
1We use the terms machine and processor interchangeably.

1

Figure 1: The computing setting considered, with the m homogeneous machines, the shared Repository, and the
three operations; inject, for the dynamic task arrivals from the users, get, for the machines to obtain the set of pending
tasks, and inform for the repository to update the set of pending tasks.

In
(e.g., in terms of processing time) arrive into the system, to be executed by any of the machines.
order to remove a single point of failure, the system considered has no central scheduler.
Instead (see
Fig. 1), each task that arrives is held in a repository until some machine executes it and reports this fact.
Hence, the scheduling is done in a distributed way by the machines. The objective is to design eﬃcient
distributed scheduling algorithms that run at the machines. In summary, the characteristics of the parallel
system considered here are (1) continuous task arrivals, (2) machine failures and restarts, and (3) distributed
scheduling. We measure the performance of an algorithm by its completed-load (i.e., the total size of the
completed tasks) competitiveness.

One additional aspect of cloud computing is its impact in terms of energy consumption. It is known that
the power consumed by a processor grows with its processing speed (in cycles per second) [4]. In our model
we introduce this in the form of a speedup s ≥ 1, that can be used to allow processors to run faster than the
baseline. This is a form of resource augmentation. We show that this resource augmentation, increasing the
speed of the machines, improves the competitiveness of the system.

Related Work Probably, the most important research line related to this work is the study of machine
scheduling with availability constraints (e.g., [25, 15, 14, 18]). One of the most important outcomes of this
line is the necessity of algorithms that take into account unexpected machine breakdowns. Most works
allow preemptive scheduling [15, 16] and show optimality only for nearly online algorithms (for example,
algorithms that need to know the time of the next job arrival or machine availability). Among these works
some also consider energy issues, and use speed-scaling to tune the power consumption of the processors
(e.g., [1, 7, 8]).

The work of Georgiou and Kowalski [14] was the one that initiated our study. They looked at a rather
diﬀerent setting, consisting of a cooperative computing system of m message-passing processors prone to
crashes and restarts, having to collaborate in order to complete dynamically injected tasks. For the eﬃciency
of the system, they performed competitive analysis, focusing on the maximum number of pending tasks. They
proved competitiveness with unit-length tasks, and showed that if tasks have diﬀerent lengths competitiveness
cannot be achieved.

In [3] we looked at a setting similar to the one used here, with m machines, introduced the term of
speedup, representing the resource augmentation required, and showed competitiveness in terms of pending
load (sum of sizes of pending tasks). More precisely, tasks of at least two diﬀerent sizes were considered,
and it was found that the threshold for competitiveness was deﬁned by these two conditions: (a) s < ρ
and (b) s < 1 + γ/ρ, where ρ is the ratio between the largest and smallest task and γ is a parameter
that depends on ρ and s (see [3] for details). If both conditions hold then no deterministic algorithm is

2

Speedup

1 - machine

m - machines

C(ALGW ) = 0, any task size,
C(ALGW ) ≤ ¯ρ
C(SL-Preamble) ≥ ¯ρ

¯ρ+ρ ≈ 1/2,

¯ρ+ρ ≈ 1/2, two task sizes,

[11] C(ρm-Preamble) ≥ ¯ρ

¯ρ+ρ ≈ 1/2,

two task sizes

two task sizes

s = 1

C(Greedy) ≥ ¯ρ

¯ρ+ρ ≈ 1/2,

[19] C(k-Amortized) (cid:38) 1/2,

[12]

[11]

[Th. 1]

[Th. 2]

k task sizes, pairwise divisible

k task sizes, pairwise divisible

C(MGreedy) ≥ min
1≤j<i≤k

,

[19] C(M k-Amortized) ≥ min
1≤j<i≤k

ρi,j +ρi,j

, [Th. 3]

(cid:110) ρi,j

(cid:111)

ρi,j +ρi,j

(cid:110) ρi,j

(cid:111)

k task sizes π1, . . . , πk; ρi,j = πi/πj

k task sizes π1, . . . , πk; ρi,j = πi/πj

general

general

s ≥ ρ

s ≥ max{ρ, 2}

s ≥ 1 + ρ

C(ALGW ) ≥ 1/ρ,
C(LIS) ≥ 1
C(ALGW ) ≥ 1,

[12] C(ALGW ) ≥ 1/ρ,
[12] C(m-LIS) < 1, when m = s = ρ = 2
[12] C(ALGW ) ≥ 1,

[Th. 4]

[Th. 6]

[Th. 5]

Table 1: Negative (upper bounds) and positive (lower bounds) results on the completed-load competitiveness. ALGW
is any work conserving algorithm. Note that the negative results hold for both 1-machine and m-machines. Recall
that ρ = πmin/πmax and ¯ρ = (cid:98)ρ(cid:99).

pending-load competitive. Then, some algorithms were proposed that achieve competitiveness as soon as
one of the conditions does not hold; unfortunately imposing other restrictions, like considering only two
diﬀerent task costs. A follow-up work [12] compared popular scheduling algorithms (like FIFO, LIFO or
LIS – Longest in System) on the basic model of one machine, and looked at diﬀerent eﬃciency measures,
including the completed-load and latency competitiveness. Kowalski et al. [23], also inspired by [3], proved
that in a system with one machine, and for speedup satisfying the conditions (a) and (b) as described above,
no deterministic algorithm can be 1-completed-load competitive. They then proposed an algorithm that
achieves 1-completed-load competitiveness if s ≥ 1 + γ/ρ.

In [11], a diﬀerent setting was considered: an unreliable communication link between two nodes. The
problem of scheduling packets over such a link, is very related to the problem in this work, closely resembling
the problem of scheduling tasks in one single machine with crashes and restarts (and without speedup).
In [11], the authors proposed the metric of asymptotic throughput for the performance evaluation of scheduling
algorithms studied, which corresponds to the long term completed-load in our current setting. Assuming
only two packets lengths, they showed that for adversarial arrivals there is a tight value for the asymptotic
throughput, giving upper bound with a ﬁxed adversarial strategy and a matching lower bound with an online
algorithm. Jurdzinski et al. [19] extended that work, presenting optimal online algorithms for the case of k
ﬁxed packet lengths, matching the upper bounds on the asymptotic throughput shown in [11]. Finally, they
sketch a modiﬁcation to one of the algorithms, in order to adapt it for the case of f independent channels
(stating that the analysis is not trivial). This modiﬁed algorithm cannot be used in our setup because it
uses central scheduling; the sender has updated information and full control of the channel through which
each packet is transmitted.

Contributions As mentioned, in this work, we consider a setting with m machines prone to crashes and
restarts, controlled by an adversary (to model worst-case scenarios), and a shared repository (an entity that

3

provides the service by which the clients of the system submit the tasks to be executed and notiﬁes them of
their completion – see Fig. 1). Note that the shared repository is not a scheduler, since it does not make any
decisions on the execution of the tasks. It is basically a passive storage that behaves as an interface between
the clients that generate tasks and the machines. It also allows the machines to maintain information about
the tasks that have not been executed yet.
Tasks arrive in the system continuously and have diﬀerent computational demands (which is their size).
We assume that each task τ has size π(τ ) ∈ [πmin, πmax], where πmin and πmax correspond to the smallest
and largest possible values respectively, and that π(τ ) only becomes known at the moment of the task arrival.
Tasks are held in the shared repository, which is later accessed by the machines in order to decide which task
to execute next. Note, then, that the machines’ decisions are taken in a distributed manner and without any
communication between them. When a task is completed, the corresponding machine informs the shared
repository (which in turn notiﬁes the client).
As mentioned, we consider the possibility of having resource augmentation in the form of speedup s ≥
1 [20, 2] (i.e., we increase the computational speed of the machines) in order to cope with the performance
cutback from the machine failures (crashes) and restarts, as well as the lack of information for the future
task arrivals. More precisely, we consider uniform speedup s ≥ 1 for all the machines, under which a task τ
is executed s times faster, i.e., in time π(τ )/s.

Since the scheduling decisions must be made in a continuous manner and without future knowledge
(neither of the task arrivals nor of the machine crashes and restarts), we study the problem as an online
scheduling problem [24, 3, 11]. Hence, for the evaluation of the diﬀerent algorithms proposed, we use
competitive analysis [27], measuring the total completed load of the system: the sum of sizes of the completed
tasks. More precisely, an algorithm is considered α-completed-load competitive, also expressed as C(ALG) =
α, with speedup s if under any adversarial behavior its completed-load complexity is at least α times the
completed-load complexity of any algorithm X, running with no speedup and the same adversarial behavior.
Fully detailed speciﬁcations of the model used are given in Section 2.

In Table 1, we summarize our results, including also some relevant results found in previous works. Note
that the upper bounds found for the case of one machine, hold directly for the case of m machines, since
the adversary can simply crash all machines except one, and force the corresponding adversarial scenarios
to occur. However, the positive results (lower bounds) do not necessarily transfer from 1 to m machines.

The upper part of Table 1 presents the results obtained in Section 3 when the machines run without
speedup (i.e. s = 1). As can be seen, for this case we present algorithms that achieve optimal completed-load
competitiveness, in all the cases. Observe that, in [12], it was shown for one machine that no work-conserving
algorithm2 can achieve competitiveness if there is an arbitrary number of diﬀerent task sizes. This, being
a negative result, also holds for the case of m-machines. Then, we give three work-conserving algorithms,
focusing on the cases of two task sizes and bounded number of task sizes, with and without pairwise divisibility;
a property that holds between each pair of task sizes (explained further in Section 3). For the three cases,
algorithms ρm-Preamble, k-Amortized and Mk-Amortized respectively, achieve optimal competitiveness,
matching the upper bound shown in [11]. These algorithms are non-trivial generalization of algorithms
proposed in [11] and [19].

The lower part of Table 1 presents the results obtained in Section 4 for systems running with speedup
s > 1. The ﬁrst interesting observation from these results is that, contrary to intuition, to move from
one machine to multiple machines it is not enough to complement an algorithm that works for m = 1
with a mechanism that prevents redundant execution of tasks when m > 1. This is shown in the case
of s ≥ max{ρ, 2} with the algorithm m-LIS, proposed in [3], which is the natural adaptation of LIS to
multiple machines. As observed, while LIS guarantees 1-completed-load competitiveness in one machine,
m-LIS cannot achieve that level of competitiveness even with 2 machines. Fortunately, as shown, we have
been able to generalize two important general positive results obtained for work-conserving algorithms in
one machine to multiple machines.

2An algorithm is work conserving if it does not allow a machine to be idle if there are tasks to be executed in the repository.

4

2 Model and Deﬁnitions

The parallel system considered has m identical machines, prone to crashes and restarts, with unique ids in the
set {0, 1, 2, . . . , m−1}. Please see also Fig. 1 for the graphical representation of the system. As mentioned, the
machines have access to a shared repository. The repository supports three essential operations: inject, get,
and inform. The inject operation is executed by a client of the system to add a new task to the current
set of tasks to be executed. We assume here that this operation is controlled by an adversary (as will be
further discussed below). Operations get and inform are executed by the machines. A machine uses the
get operation to obtain the set of pending tasks, i.e., the tasks that are in the repository because they were
injected and no machine has notiﬁed their completion yet. For simplicity, we assume that the get operation
is blocking, i.e., if the repository is empty when executed, it waits until some new task is before returning
(the set of newly injected tasks). A machine then executes an inform operation when it has completed the
task scheduled, notifying the repository about its completion. Then the repository removes immediately this
task. We assume that the execution of these operations is instantaneous (takes negligible time), except a get
operation that blocks.

We consider machines running in processing cycles, controlled by the scheduling algorithm considered.
Each cycle, starts with a get operation, a task execution, and an inform operation (if the task is completed).
Since the repository operations (get and inform) are instantaneous, a processing cycle lasts the time needed
for the scheduled task to be completed. We assume that machines run with a speedup s ≥ 1 (s = 0 means no
speedup). Then, a processing cycle lasts a time equal to the size of the task divided by the speedup s. When
a machine crashes, the cycle is interrupted and the progress in the task execution is lost. If the machine
later recovers, it starts a new cycle.

Event ordering Since the injection of tasks by clients, the get operations, and the notiﬁcation via inform
operations of task completion by the machines may occur simultaneously, we deﬁne the following order
among these events. We assume that in an instant t the inform operations occur ﬁrst, then the injections,
and ﬁnally the get operations. Hence, a get operation executed at time t will include the tasks injected at
time t but not the ones completed at that time.

Tasks As already explained, computational tasks are injected to the system by the clients, with the inject
operation at the repository. We assume that this operation is controlled by an arrival pattern A (a sequence
of task injections) deﬁned by an omniscient adversary. Each task τ has an arrival time a(τ ) and a size π(τ ),
which is the time required to complete the task without speedup. The task attributes are only known at the
time of its injection. We use the term π-task to refer to a task of size π ∈ [πmin, πmax]. The values πmin and
πmax are the smallest and largest possible task sizes, and are usually assumed to be known by the scheduling
algorithm. We also deﬁne parameter ρ = πmax
πmin

to be the ratio between the largest and smallest task sizes.

We assume that tasks are atomic in the sense that not executing one completely due to a crash implies
that it has to be executed again from the start. On the other hand, we assume the tasks to be idempotent [13],
which means that executing the same task more than once has the same eﬀect as executing it only once.

Machine Crashes and Restarts For the machine crashes and restart, we consider an omniscient adver-
sary, which is the same entity responsible for the task injections at the repository described above. This
means that the adversary is expected to coordinate injections, crashes, and recoveries. In an execution of the
system, the adversary deﬁnes an error pattern E, which is a list of crash and restart events, each associated
with the time it occurs (e.g., crash(t, p) is the event that that machine p is crashed at time t). We consider
a machine p being alive in time interval I = [t, t(cid:48)], if it is operational at time t and does not crash at any
time t(cid:48)(cid:48) ≤ t(cid:48).

Deﬁnition 1 An adversarial pattern is a combination of arrival and error patterns A and E. It is admissible
only when at all time instants there is at least one machine alive. In our work we only consider admissible
adversarial patterns.

5

t (X, A, E) and Qs

Notation We consider it useful to provide all extensively used notation. Since it is essential to keep track
of injected, completed and pending tasks at each time instant in an execution, we introduce sets It(A),
N s
t (X, A, E), where X is an algorithm, A and E the arrival and error patterns respectively,
t the time instant under consideration and s the speedup of the machines. It(A) represents the set of injected
tasks within the interval [0, t], N s
t (X, A, E) the set
of pending tasks at time instant t. As implied by the event ordering deﬁned above, Qs
t (X, A, E) contains the
tasks that were injected by time t inclusively, but not the ones completed before and up to time t. Observe
that It(A) = N s
t (X, A, E) and that set I depends only on the arrival pattern A, while sets
N and Q also depend on the error pattern E, the algorithm run by the scheduler, X, and the speedup of
the machine, s. For simplicity, we omit the superscript s in further sections of the paper. However, the
appropriate speedup in each case is clearly stated at all times.

t (X, A, E) the set of completed tasks within [0, t] and Qs

t (X, A, E) ∪ Qs

We use Lπ to refer to the subset of Qs

t (X, A, E) that includes only pending tasks of size π, and we assume
an ascending order of tasks in each queue Lπ, according to their arrival time. To simplify the presentation
of the algorithms, in a list of pending tasks we number them starting with 0. Then, for instance, the tasks
in Lπ are numbered from 0 to |Lπ|. What is more, we will use notation |Lπ(X, t)| to refer to the number of
π-tasks pending in the execution of X at time t. In a similar way, we use notation |Np(X, t)| to denote the
number of completed tasks by machine p in the execution of algorithm X at time t.

Finally, we include some deﬁnition that originally appeared in [3] and will be used in the rest of the

paper.

Deﬁnition 2 ([3]) An algorithm is of type GroupLIS, if all the following hold:
• It separates the pending tasks into classes containing tasks of the same size.
• It sorts the tasks in each class in increasing order with respect to their arrival time.
• If a class contains at least m2 pending tasks and a machine p schedules a task from that class, then it

schedules the (p · m)th task in the class.

Eﬃciency Measures We evaluate our algorithms with the completed load measure. Given an algorithm
ALG running with speedup s ≥ 1, and adversarial arrival and error patterns A and E respectively, we look
at time t ≥ 0 of the execution and focus on the completed load complexity. This means, that we look at the
sum of sizes of the completed tasks up to time instant t:

(cid:88)

C s

t (ALG, A, E) =

π(τ )

τ∈N s

t (ALG,A,E)

Finding the algorithm, in other words computing the schedule, that maximizes the measure oﬄine (having
the knowledge of patterns A and E) is an NP-hard problem [3, 11].

We will also be using a slightly changed notation C X (t, π) – resp., P X (t, π) – to denote the completed
load – resp., pending load – of size π at time instant t in the execution of algorithm X. Similarly, C X (t, < π)
– resp., P X (t, < π) – refers to the completed load – resp., pending load – of size smaller than π at time
instant t in the execution of X.

As already mentioned, since the system is dynamic in respect to the task arrivals and machine crashes
and restarts, we view the scheduling problem of task as an online one, and pursue competitive analysis.
Speciﬁcally, considering any time t of an execution, any combination of arrival and error patterns, A and E,
and any algorithm X designed to solve the scheduling problem, the completed load competitiveness of an
algorithm ALG that runs with speedup s ≥ 1 is deﬁned as follows:
Algorithm ALG is α-completed-load competitive if ∀t, X, A, E, C s
t (X, A, E) + ∆C,
where parameter ∆C does not depend on t, X, A or E, and α is the completed-load competitive ratio, which
we denote by C(ALG). What is more, α is also independent of t, X, A and E, but it may depend on system
parameters like πmin, πmax, m or s, which are not considered as inputs of the problem; they are ﬁxed and
given upfront. The input of the problem is formed only by the adversarial arrival and error patterns A

t (ALG, A, E) ≥ α · C 1

6

Algorithm 1 ρm-Preamble (for machine p)

Parameters: m, πmin, πmax
Upon awaking or restart

Get Lπmin and Lπmax , from the Repository;
preamble ← FALSE
c ← 0;

Calculate ρ ←(cid:106) πmax

(cid:107)
If |Lπmin| ≥ ρ · m2 then
preamble ← TRUE;

πmin

//Reset preamble status

//Reset counter

Repeat

//At decision times

Get the queues of pending tasks, Lπmin and Lπmax , from the Repository;
Sort Lπmin and Lπmax by arrival time (ascending);
If preamble = TRUE ∧ (c < ρ) then

execute task at position p · m in Lπmin ;
c ← c + 1;
If |Lπmax| ≥ m2 then

else

|Lπmin| ≥ m2 then

else if
else if Lπmax (cid:54)= ∅ then

execute task at position p · m in Lπmax ;
execute task at position p · m in Lπmin ;
execute task at position (p · m) mod |Lπmax|
in Lπmax ;
execute task at position (p · m) mod |Lπmin|
in Lπmin ;

else if Lπmin (cid:54)= ∅ then

Inform the Repository for the task completion;

and E. Finally, let us clarify that the number of machines m is ﬁxed for a given execution, and that the
algorithm used may take it into consideration; hence diﬀerent m may result to diﬀerent performance of the
same algorithm, aﬀecting additive term in the competitiveness. The same holds for πmin, πmax and s.

3 No Speedup

Let us start with the section in which machines have no speedup, i.e., s = 1. We aim to show that the
upper bound of completed-load competitiveness shown in [11] can be achieved by some online algorithms in
the distributed setting of m machines. In particular, we propose three scheduling algorithms, ρm-Preamble,
k-Amortized and Mk-Amortized, and analyze their performance under worst-case arrival and error patterns
A and E, showing that the upper bound of completed-load competitiveness is guaranteed.

3.1 Two Task Sizes

Let us start with the ﬁrst algorithm, ρm-Preamble, that runs on the m machines of the system and considers
only two diﬀerent task sizes, πmin and πmax (see the algorithm’s pseudocode Alg. 1).

Algorithm description. Upon awaking or restart, machine p reads the two queues of pending tasks
from the Repository, Lπmin and Lπmax , and applies ascending sort by their arrival time, such that the
earliest injected task is at position 1 of the queue. It then calculates parameter ρ = (cid:98)ρ(cid:99) and along with
parameter preamble decides which is the next task to be scheduled, avoiding redundancy when enough
tasks are pending. More precisely, at each decision time, if there are at least ρ · m2 tasks of size πmin and
preamble = TRUE, the machine attempts to complete ρ πmin-tasks before continuing with a non-redundant

7

version of the Largest Size (LS) scheduling approach. Let us explain further: after the preamble is completed
by the machine (if there was enough time and the machine did not crash), it gives priority to the largest
tasks, given that it has enough of them, so that redundancy is avoided (see exact conditions in Algorithm 1).
Note that if there are at least m2 πmin-tasks and/or at least m2 of πmax-tasks, each machine will complete
a diﬀerent task of the same size. Hence, if there are not enough πmax-task but there are enough πmin ones,
it will schedule πmin-tasks instead of risking redundant executions with scheduling πmax ones.

Observe that algorithm ρm-Preamble belongs to the class of scheduling algorithms named GroupLIS
in [3]. They showed that the
(see deﬁnition in Section 2), which was initially deﬁned by Fern´andez et al.
algorithms in this class, considering speedup s ≥ 1, do not execute the same task twice, as long as there are
enough pending tasks (i.e., ≥ m2), and thus we show that the same holds for ρm-Preamble. Let us start with
the next lemma, that corresponds to the adaption of Lemma 8 in [3] for the case of no speedup.
Deﬁnition 3 ([3]) A full task execution of a task τ is the interval [t, t(cid:48)], during which a machine p
schedules τ at time t and reports its completion to the repository at t(cid:48), without stopping its execution within
the interval [t, t(cid:48)).

Lemma 1 For an algorithm ALG of type GroupLIS and a time interval T in which a queue Lπ has at least
m2 pending tasks, any two full task executions (in T ) by diﬀerent machines, are executions of diﬀerent tasks;
i.e., the full executions of tasks τ1, τ2 ∈ Lπ by machines p1 and p2 respectively, must have τ1 (cid:54)= τ2.

For the ease of presentation, let us use letter A to refer to our algorithm, ρm-Preamble, in its analysis.
Let us also deﬁne the following two intervals, during which there are suﬃcient tasks pending in order to
guarantee non-redundant task executions (from Lemma 1, we make the observation that follows):
T +: an interval where |Lπmax(A, t)| ≥ m2, ∀t ∈ T +
T −: an interval where |Lπmin(A, t)| ≥ m2, ∀t ∈ T −

Observation 1 All full executions of πmax-tasks in the execution of algorithm ρm-Preamble within any
interval T + appear exactly once. Similarly, all full executions of πmin-tasks in the execution of ρm-Preamble
within an interval T − appear exactly once.

Note now, that there are two possible types for the whole execution of Algorithm ρm-Preamble:
(a) ∀t,∃t(cid:48) > t such that

|Lπmin (A, t(cid:48))| < ρm2(cid:86)|Lπmax (A, t(cid:48))| < m2.
|Lπmin (A, t(cid:48))| ≥ ρm2(cid:87)|Lπmax (A, t(cid:48))| ≥ m2.

(b) ∃t such that ∀t(cid:48) > t

In the ﬁrst case, when an execution is of type (a), there will always be a time t(cid:48) after the current time
instant t, at which the queue Lπmin has less than ρm2 tasks and the queue Lπmax less than m2 tasks. In
this case, non-redundancy cannot be guaranteed.

In the second case, when an execution is of type (b), after time t the queue of pending tasks will never
become empty, it will instead have enough pending tasks in order to guarantee non-redundancy at all times.
The execution after time instant t can be described by a sequence of intervals, say Ti, where i simply denotes
their sequence. They are T + and/or T − intervals. More precisely, they belong to one of the following types:

(1) |Lπmin (A, t∗)| ≥ ρm2(cid:86)|Lπmax (A, t∗)| ≥ m2, ∀t∗ ∈ T

In this case, a machine following algorithm ρm-Preamble will schedule ρ πmin-tasks, followed by
continuously scheduled πmax-tasks, until a time instant t where either |Lπmin (A, t)| < ρm2 or
|Lπmax (A, t)| < m2, thus one of the next two types of period will follow.

8

(2) |Lπmin (A, t∗)| ≥ ρm2(cid:86)|Lπmax (A, t∗)| < m2, ∀t∗ ∈ T
(3) |Lπmin (A, t∗)| < ρm2(cid:86)|Lπmax (A, t∗)| ≥ m2, ∀t∗ ∈ T

In this case, a machine following algorithm ρm-Preamble will continuously schedule πmin-tasks until
the queues are such that one of the other two types of periods follow.

In the third case, machines following the ρm-Preamble continuously schedule πmax-tasks, until the
queues become such that one the previous two types of periods follow.

|Lπmin (A, t(cid:48))| < ρm2(cid:86)|Lπmax (A, t(cid:48))| < m2

Lemma 2 For executions of type (a), where ∀t,∃t(cid:48) > t s.t.
holds, the completed-load competitive ratio of algorithm ρm-Preamble goes to 1, i.e., C(ρm-Preamble) = 1.

Proof: First, let us ﬁx a pair of arrival and error patterns, such that executions of case (a) occur. We
focus on time instant t(cid:48) from the deﬁnition. Observe that at time t(cid:48), the total pending load of algorithm
ρm-Preamble is less than ρm2πmin + m2πmax, while the total pending load of X is at least zero. Let us
overload the notation of the set of injected tasks up to time t(cid:48) such as to represent the total injected load up
to that time; It(cid:48) will now represent the sum of sizes of all injected tasks up to time t(cid:48). Then, at time instant
t(cid:48) the completed load ratio is

(cid:1)
It(cid:48) −(cid:0)|Lπmin (A, t(cid:48))|πmin + |Lπmax (A, t(cid:48))|πmax
It(cid:48) −(cid:0)|Lπmin (X, t(cid:48))|πmin + |Lπmax (X, t(cid:48))|πmax
(cid:1)

Ct(cid:48)(ρm-Preamble) =

≥

Ct(cid:48)(ρm-Preamble)

=

Ct(cid:48)(X)

It(cid:48) −(cid:0)|Lπmin (X, t(cid:48))|πmin + |Lπmax (X, t(cid:48))|πmax

It(cid:48) − (ρm2πmin + m2πmax)

(cid:1)

which leads to a completed-load competitive ratio of 1 as time goes to inﬁnity; the total injected load

goes to inﬁnity as well:

C(ρm-Preamble) = lim

t→∞Ct(ρm-Preamble) = lim
t→∞

(cid:18)

(cid:19)

1 − ρm2πmin + m2πmax

It

= 1.

This completes the proof of the completed load competitiveness for executions of type (a) as claimed.

The next lemma follows mostly the idea of analysis of algorithm SL-Preamble, presented in [11], for the

case of packet scheduling over one communication link. The complete proof is included in the Appendix.
Lemma 3 For executions of type (b), where ∃t,∀t(cid:48) > t, s.t.
holds, the completed load competitive ratio of algorithm ρm-Preamble is C(ρm-Preamble) ≥ ρ

|Lπmin (A, t(cid:48))| ≥ ρm2(cid:87)|Lπmax (A, t(cid:48))| ≥ m2

ρ+ρ .

From Lemmas 2 and 3, that analyze the two types of executions, (a) and (b) respectively, we have the

lower bound for the completed-load competitiveness of ρm-Preamble, given by the following theorem.

Theorem 1 When algorithm ρm-Preamble runs without speedup (s = 1) under any arrival and error pat-
terns A and E, it has a completed-load competitive ratio C(ρm-Preamble, A, E) ≥ ρ

ρ+ρ .

3.2 Finite Task Sizes – Pairwise Divisible
We now move to the case of k > 2 diﬀerent task sizes. Let us denote them by πmin = π1 < π2 < ··· < πk =
πmax. We assume that each ratio ρi,j = πi/πj is an integer for any 1 ≤ j < i ≤ k, a property of the task
sizes called pairwise divisibility.

Inspired by the work of Jurdzinski et al. [19], we propose algorithm k-Amortized, that uses this property
to schedule the tasks among the m machines of the system, and analyze its completed-load competitiveness
when run without speedup (see the algorithm’s pseudocode in Alg. 2 and 3). The algorithm follows the
Shortest Size (SS) ﬁrst policy, but subject to some balancing constraints. It is based on scheduling tasks in

9

Algorithm 2 k-Amortized (for machine p)

1 Parameters: m,{π1, π2, . . . , πk}
2 Upon awaking or restart
3
4

Repeat

Get L1 to Lk from the Repository;

(cid:106) |Lk|

(cid:107)

m2

k−1(cid:80)

(cid:106)

While πk

+

πi

|Li|

m2+mρi+1

(cid:107)

< πk Do

execute task π at position (p · m) mod |Q| in Q;
Inform the Repository for the task completion;

i=1

Schedule Group(k);

5

6
7
8

2

3
4
5
6
7
8

Algorithm 3 Schedule Group(j)

1 Parameters: m, j,{π1, π2, . . . , πk},{L1, L2, . . . , Lj}

j−1(cid:80)

(cid:106)

If

πi

|Li|

(cid:107) ≥ πj then

m2+mρi+1

i=1
For α = 1 to ρj Do

Schedule Group(j−1);

Else

execute task πj at position p · m in Lj;
Inform the Repository for the task completion;

Return

batches (or groups) that balance the length of the next larger task. What is more, it considers redundancy
avoidance, demanding enough tasks available before scheduling. An important diﬀerence with algorithm
ρm-Preamble, apart from the fact that this one considers k diﬀerent task sizes, is that it continuously
schedules bursts of short tasks before going to the next larger task (if the tasks are available and the
machines do not crash).

, where i ∈ [2, k]; it represents the ratio between two consecutive task sizes.

Special Notation & Terminology Before looking at the details of the algorithm, let us introduce some
necessary notation and terminology that will be used extensively in this subsection. First, parameter ρi,j =
πi/πj, where 1 ≤ j < i ≤ k, as already mentioned at the beginning of the subsection, represents the ratio
between two task sizes and is considered to be an integer for this algorithm. A special case of this ratio used
in the algorithm, is ρi = πi
πi−1
Deﬁnition 4 We deﬁne adequate sizes of pending tasks, the task sizes whose pending queues have (cid:38) m2
tasks. More precisely, for size πk to be adequate there must be at least m2 tasks pending in the Lk queue,
while for any other πj size, where j ∈ [1, k − 1], the corresponding pending queue Lj must have at least
m2 + mρj+1 tasks. (In Lemma 4, we show that this is the necessary number of tasks in order to guarantee
the non-redundancy property of the algorithm.)
Deﬁnition 5 We deﬁne an i-group of tasks, where i ∈ [1, k], being the tasks completed in the execution of
a machine under the recursive call to the function Schedule Group(i). Note, that an i-group has a total size
of πi, but may be the result of the completion of several smaller tasks.

Deﬁnition 6 We consider a machine to be busy at time t if it is either executing some task, it has just
completed one, or it is crashed (it is either the time instant that the machine was just crashed, or the machine
has been crashed for some time). Observe that an interval from a crash to a restart instant, say T = [tc, tr],
belongs to the busy interval; during that interval, no algorithm is able to complete any task, hence it does
not aﬀect the completed load. Otherwise, it is considered to be idle.
We also consider a machine to be in an n-busy interval, say T = [t1, t2], where t1 < t2, satisfying the
following properties:

10

(1) The machine is busy at each time t ∈ T .
(2) The machine does not schedule tasks of size πi for i > n during the interval T .
(3) At the beginning of interval T , i.e., time instant t1, algorithm k-Amortized has at least as many tasks of
size πi pending as X, for each i ≤ n. Hence, P A(t1, πi) ≥ P X (t1, πi) for each i ∈ [1, n].

Finally, note that in the pseudo-code we refer to queue of pending tasks Q. Recall that in Section 2 we
deﬁne Q to be the set of pending tasks. Here, we overload its deﬁnition to make this set a uniﬁed queue of
all Li, sorted in an ascending order according to the task sizes.

Algorithm description. After awakening or restart, a machine schedules the task at position p · m of the
pending queue Q, until the sum of the adequate sizes of the pending tasks is at least πk. Following this
strategy, the algorithm guarantees the ability to cover a time interval of length πk, with non-redundant
task executions, if it is not interrupted by a machine crash. Otherwise, being work-conserving, it schedules
the task in the position already mentioned, but with no guarantees of non redundancy. Then, it calls the
recursive function Schedule Group(j) (starting with j = k) which checks whether the sum of adequate sizes
of the pending tasks smaller than πj is at least equal to πj (resp., πk). If the condition is true, the algorithm
calls to function Schedule Group(j − 1) (resp., Schedule Group(k − 1)) in order to cover the
makes ρj = πj
πj−1
corresponding time interval πj with ρj groups of πj−1 aggregate size; in other words, ρj (j − 1)-groups. In
the following recursion levels more recursive calls may occur, if there are again enough pending tasks, thus
covering the corresponding time intervals by tasks of smaller size each time. Otherwise, when there are not
enough tasks pending in a recursion level, a task of the current size, πj, is scheduled by the machine and
when completed returns to the previous recursion level.

We will now analyze algorithm k-Amortized, proving some important properties and showing that its
completed-load competitiveness is indeed optimal, i.e., C(k-Amortized) (cid:38) 1/2. We start with two lemmas
that lead to the non-redundancy property of the algorithm, omitting the case when lines 6 and 7 of Algo-
rithm 2 are executed. Note that in that case, the pending load of the algorithm is bounded, so it does not
aﬀect the completed-load competitiveness in the long run.

Lemma 4 Algorithm k-Amortized schedules a πj-task (in line 6 of Alg. 3), only when there are at least m2
tasks in the corresponding queue of pending tasks, Lj.

Proof: Let us start by looking at the algorithm description and its pseudo-code. The ﬁrst call to schedule
some tasks – calling function Schedule Group(k) – is done only if enough tasks are pending to cover the
πk-time without redundancy. A task size is accounted for only when it is adequate; only when there are
(cid:38) m2 tasks of that size pending (recall Deﬁnition 4).

Then, within the Schedule Group(j) function, starting by j = k, the algorithm checks whether it can
be covered non-redundantly by tasks of smaller size. If it does, it makes a recursive call to the function
with parameter j − 1, which corresponds to the next smaller task size, πj−1. A task πj cannot be covered
< πj (see condition of line 2 in Algorithm 3).
non-redundantly by smaller tasks when

j−1(cid:80)

(cid:106)

(cid:107)

|Li|

m2+mρi+1

πi

i=1

However, a function call with task size πj means that it was either called by algorithm k-Amortized directly
(and j = k), in which case the condition in line 8 of Alg. 2 does not hold, or it was called by the previous
recursion level; by function handling the next bigger task size, πj+1, in which case the condition in line 2
holds. We will now show that in either case, there are enough tasks of size πj to be scheduled by the system’s
machines without executing any of them redundantly.

We consider the function call Schedule Group(j), for which the if condition in line 2 does not hold. This

implies

(cid:106)

πi

j−1(cid:88)

i=1

(cid:107)

|Li|

m2 + mρi+1

< πj.

(1)

We then consider and analyze the two cases mentioned above, separately:

11

Case 1: A previous function call, Schedule Group(j + 1), in which the if condition in line 2 holds, i.e.,

(cid:106)

j(cid:80)

i=1

πi

|Li|

m2+mρi+1

(cid:107) ≥ πj+1, implies that

Combining the two equations 1 and 2 we have the following

(cid:106)

πj

m2 + mρj+1

(cid:107)

+

i=1

|Lj|

j−1(cid:88)
(cid:107) ≥ πj+1 − j−1(cid:88)
(cid:107)

i=1

> 0,

(cid:106)
⇒(cid:106)

πj

|Lj|

m2 + mρj+1

|Lj|

m2 + mρj+1

(cid:106)

|Li|

m2 + mρi+1

πi

(cid:106)

πi

|Li|

m2 + mρi+1

(cid:107) ≥ πj+1
(cid:107)

> πj+1 − πj > 0

which means that |Lj| ≥ m2 + mρj+1.

line 8 of algorithm k-Amortized. Hence, πk

Case 2: The function call Schedule Group(j) was actually Schedule Group(k) and came directly from

(cid:107) ≥ πk holds, which implies that
(cid:107)

(cid:106)|Lk|

m2

πk

πi

|Li|

m2+mρi+1

|Li|

m2 + mρi+1

(cid:106)
(cid:106)

m2

+

i=1

(cid:107)

(cid:106)|Lk|
k−1(cid:80)
(cid:107) ≥ πk − k−1(cid:88)
k−1(cid:80)
(cid:106)|Lk|

(cid:107)

πi

i=1

πi

i=1

|Li|

(cid:107)
(cid:106)
> 0 ⇒(cid:106)|Lk|

m2+mρi+1

m2

(cid:107)

> 0,

m2

Replacing j = k in equation 1, we have

< πk, which combined with equation 3 we can

(2)

(3)

easily see that

πk
which in its turn means that |Lk| ≥ m2.

In both cases, we have seen that there are at least m2 tasks of size πj or πk respectively. However, in
case 1 above, there will be ρj+1 iterations of the recursive function call Schedule Group(j) of line 4. We must
therefore make sure that there are at least m2 available tasks for all iterations.

Consider for example, the case in which at a time t all m machines are in a (j + 1)-group execution;
following the Schedule Group(j + 1) function, and having condition of line 2 true. Then, they all start the
ρj+1 iterations of scheduling j-groups of tasks, calling the recursive function Schedule Group(j). Consider
now, that in all corresponding conditions of line 2, are false. This means, that all m machines will
simultaneously execute one πj-task in every iteration. Therefore, having m2 + mρj+1 pending tasks of size
πj at the beginning of iterations, will guarantee that in every iteration there are still at least m2 tasks
pending in queue Lj. This completes the proof of the lemma.

Observe now, that algorithm ρm-Preamble is of type GroupLIS; it separates the pending tasks into classes
depending on their size, it sorts them with respect to their arrival time, and if a class contains at least m2
pending tasks, a machine p schedules the task at position (p · m). Hence, Lemma 1 also holds for it. Hence,
combining the two Lemmas 4 and 1, the following property for algorithm k-Amortized follows.

Observation 2 When Algorithm k-Amortized, schedules tasks through its function Schedule Group (Alg. 3),
it never completes the same task more than once. In particular, the same task cannot be simultaneously
executed in more than one machines of the system.

Lemma 5 When a task of size πj is scheduled by k-Amortized, through its function Schedule Group, say at

time instant t, the total size of smaller pending tasks is P A(t, < πj) ≤ j−1(cid:80)

(πj + πi)(m2 + mρi+1).

i=1

12

Proof: When a task of size πj is scheduled by k-Amortized in line 6 of Alg. 3, as we have seen also from
< πj. This also means that ∀i ∈ [1, j − 1],

Lemma 4, the following inequality must hold:

|Li|

πi

m2+mρi+1

j−1(cid:80)

(cid:106)

(cid:107)

the following is true:
|Li|

(cid:22)

πi

m2 + mρi+1

(cid:23)

(cid:18)

i=1

|Li|

m2 + mρi+1

(cid:19)

< πj ⇒ πi

− 1

< πj ⇒ πi|Li| < (πj + πi)(m2 + mρi+1).

j−1(cid:80)

πi|Li| ≤ j−1(cid:80)

i=1

i=1

(πj +πi)(m2+mρi+1)

Therefore, the sum of all pending tasks smaller than πj is P A(t, < πj) =

as claimed.

By Observation 2, we have that no task is executed more than once by algorithm k-Amortized, when
scheduled by its function Schedule Group. Hence, we can safely separate the analysis of each machine
individually, safely ignoring any task execution by line 6 of Alg. 2. We focus on one machine, say p, and
then generalizing for all m machines to give the ﬁnal result. We look at the machine’s n-busy intervals, for
some n ≤ k, and prove a completed load competitiveness of 1/2, provided that ρi,j ∈ N for 1 ≤ j < i ≤ k,
which is in fact optimal. (The omitted proofs can be found in the Appendix).
Lemma 6 For a machine p that is n-busy at a time interval T , where n ≤ k, its total completed load with
k-Amortized is at least as large as its completed load with X accounting only tasks of size ≥ πn, in interval
T , minus πk; i.e., C A
Lemma 7 For a machine p that is n-busy at a time interval T = [t1, t2], where n ≤ k, assume time t ∈ T
be any time when k-Amortized starts executing a πn-task. Then,

p (T,≥ πn) − πk.

p (T ) ≥ C X

(cid:0)[t1, t](cid:1) ≥ C X

p

(cid:0)[t1, t](cid:1) + P X (t, < πn) − n−1(cid:88)

2C A
p

(πn + πi)(m2 + mρi+1) − πk.

i=1

Lemma 8 Let fn be a function such that f1 = πk and fi+1 = fi +
For a machine p that is n-busy at a time interval T , where n ≤ k,
p (T ) − fn.

p (T ) ≥ C X

2C A

i(cid:80)

j=1

(πi+1 + πj)(m2 + mρj+1) + πi+1 + 2πk.

Theorem 2 Algorithm k-Amortized has an optimal completed load competitiveness of 1/2, provided that
πi/πi−1 ∈ N for each i ∈ [2, k].

Proof: First, by Observation 2, we know that each task completion within function Schedule Group, occurs
only once. Then, looking at Lemma 8, it implies that the completed-load competitiveness of algorithm
k-Amortized gets arbitrarily close to 1/2 on suﬃciently long periods of time in which it is busy and X starts
with the queue containing at most the same tasks of each size. On the other hand, k-Amortized cannot

guarantee non redundancy when its queue contains few tasks, i.e., when πk

+

πi

This means, that as time goes to inﬁnity, the completed load competitiveness goes to 1/2 as claimed.

In [11], Fern´andez Anta et al. showed that the completed load of any online algorithm for two diﬀerent

task sizes is at most
schedule merely two diﬀerent task sizes among the available k ones, it means that the completed load
competitiveness shown is in fact optimal.

¯ρ+ρ , which is equal to 1/2 when ρ ∈ N. Hence, since an adversary can decide to

¯ρ

13

(cid:106)|Lk|

(cid:107)

m2

(cid:106)

k−1(cid:80)

i=1

(cid:107)

|Li|

m2+mρi+1

< πk.

3.3 Finite Task Sizes – General
We now look at the case when ρi,j = πi/πj (cid:54)∈ N. Theorem 2 in [19] shows that the completed load competitive
. This upper bound
ratio of any scheduling algorithm running without speedup, is at most min

(cid:110) ρi,j

(cid:111)

i≤j<i≤k

ρi,j +ρi,j

also holds in the case of multiple machines, since the adversary can force only one of the machines to be
alive at all times, while keeping the rest crashed.

However, if algorithm k-Amortized is used in this case, the additional advantage of an oﬄine algorithm
X from rounding on each recursion level, can accumulate and worsen the competitiveness ratio proven in the
previous section. Thus, we must present a modiﬁed algorithm, that can reach the upper bound mentioned
and hence be optimal.
Let us assume a modiﬁcation such that instead of executing ρi (i− 1)-groups on the recursion level i− 1,
the algorithm keeps sending (i − 1)-groups while the completed load of the tasks in the groups completed
are less than πi − πi−1. We can then show the following.

Claim 3.1 The modiﬁed algorithm described above has a completed load competitiveness ratio at least
mini∈[2,k]

Proof: Looking at Lemmas 6, 7 and 8, and n-busy intervals, we use the association of each πi-task, for
i ≥ n, completed by X, with ρi,n n-groups completed by k-Amortized at the same interval, hence covering
the πi completed load. For the modiﬁed algorithm, a task πi completed by X would correspond to a group
of tasks of aggregate size at least πi − πi−1, which would translate to the following three equations:

(cid:8) ρi−1

2ρi−1

(cid:9).

(cid:19)

(cid:18)

i +

ρn
ρn − 1

ρn
ρn − 1

C A(T ) ≥ C X (T,≥ πn) − πk

C A(cid:0)[t1, t](cid:1) ≥ C X(cid:0)[t1, t](cid:1) + P X (t, < πn) − n(cid:88)

(cid:18)

1 +

ρn
ρn − 1

(cid:19)

i=1

C A(T ) ≥ C X (T ) − fn

(πn+1 + πi)(m2 + mρi+1) − πn

Note here, that since ρi is not an integer, for every ρi ≥ ρn task completed by X, it could be the case

Applying these inequalities in the proof of Theorem 2 we have the result claimed.
that more than (cid:98)ρi/ρn(cid:99) groups of length ρn are necessary to cover the execution time of ρi.

Nonetheless, the tasks completed in each i-group, for i ∈ [2, k] might be of diﬀerent sizes, making
the comparison of the completed load competitiveness of the above claim with the completed load
competitiveness of Theorem 2 ambiguous. Another modiﬁcation is therefore necessary in order to tackle
this uncertainty, and we present it with algorithm Mk-Amortized (see pseudo-code in Alg. 4).

(cid:111)

(cid:110)

(cid:12)(cid:12)(cid:12)πi

(cid:4)|Li|

(cid:5) ≥ ckπk

Algorithm description. Algorithm Mk-Amortized completes tasks of the same size as long as possible
and change only when it is necessary. For that, we split the execution into stages of total length ckπk, where
c ∈ N is a ﬁxed large constant. At the beginning of a stage, a set of candidate task sizes is established as
. Then, the appropriate size πi∗ is set as the minimum size in the set of candidate
C =
sizes, i.e., i∗ = min(C). This is the size of tasks that the algorithm will start executing. The appropriate
size is updated after every task completion, checking ﬁrst whether there has been some change in the set of
candidate tasks – due to new task injections. More details are given by the pseudo-code in Algorithms 4 and 5.

m2

i

As a ﬁrst observation, let us clearly note that following algorithm Mk-Amortized, only tasks of size πi∗
are scheduled, unless there are not enough tasks to guarantee non-redundancy (when set C = ∅), in which
case, a task π at position p · m of the whole queue Q is scheduled. To see this clearly, look at lines 6-12 in
Alg. 4 and lines 4,7 and 8 in Alg. 5. It is also important to note that parameter i∗ can only decrease in each
stage. This is because at the beginning of the stage there are enough pending πi-tasks for each candidate

14

Algorithm 4 Mk-Amortized (for machine p)

1 Parameters: m,{π1, π2, . . . , πk}
2 Upon awaking or restart
3
4

Repeat

C ←(cid:110)
C ←(cid:110)

While

;

m2

(cid:12)(cid:12)(cid:12)πi
(cid:106) |Li|
(cid:107) ≥ ckπk
(cid:111)
(cid:12)(cid:12)(cid:12)πi
(cid:107) ≥ ckπk
(cid:106) |Li|
(cid:110)
(cid:12)(cid:12)(cid:12)πi
(cid:106) |Li|
(cid:111)
(cid:107) ≥ ckπk

m2

m2

i

;

i

i

(cid:111)

i∗ ← min(C);
For a = 1 to ck Do

π(cid:48) ← Schedule Group(k);

5

6

7
8

9

10
11
12

Get all pending queues from the Repository, Li;

= ∅ Do

execute task π at position (p · m) mod |Q| in Q;
Inform the Repository for the task completion;

Algorithm 5 Schedule Group(j)

π ← 0;

1 Parameters: m, j,{π1, π2, . . . , πk},{L1, L2, . . . , Lj}
2
3 While π ≤ πj − πi∗ Do
4
5
6
7
8
9
10
11

If j > i∗ then
π(cid:48) ← Schedule Group(j − 1);
π ← π + π(cid:48);
execute task πj at position p · m in Lj;
If task πj completed successfully then

Else

Inform the Repository for the task completion;
π ← πj;

C ← C ∪(cid:110)

i
i∗ ← min(C);

(cid:12)(cid:12)(cid:12)πi

(cid:106) |Li|

m2

(cid:107) ≥ ckπk

(cid:111)

;

12

13
14

Return π

task size i ∈ C to cover a time interval of length ckπk (line 9 of Alg. 4). Also, like algorithm k-Amortized,
the modiﬁed algorithm Mk-Amortized belongs to the GroupLIS algorithms and has the property of non
redundancy when enough tasks are pending, which we show in the following lemma.

Lemma 9 Algorithm Mk-Amortized never completes the same task more than once within Schedule Group
(Alg. 5).

Proof: Let us begin by showing that the algorithm schedules a πj-task in function Schedule Group only
when there are at least m2 tasks in the corresponding pending queue Lj. Looking at the pseudo-code, a task
πj is scheduled in line 8 of Alg. 5, only when the condition in line 4 does not hold, and hence j = i∗. From
lines 9 and 10 of Alg. 4 and lines 12 and 13 of Alg. 5, we know that i∗ belongs to the set of candidate task
sizes, for which every task size has at least as many tasks pending as necessary to “cover” ckπk time, i.e.,
ck calls to the Schedule Group(j) function. This number of tasks is:

(cid:22)|Lj|

(cid:23)

m2

πj

(cid:22)|Lj|

(cid:23)

m2

≥ ckπk ⇒

≥ ck(cid:4)ρk,i

(cid:5) ⇒ |Li| ≥(cid:0)ck(cid:4)ρk,i

(cid:5) + 1(cid:1)m2.

What is more, since |Li| ≥ m2, Lemma 1 holds for algorithm Mk-Amortized as well (it belongs to the
GroupLIS algorithms), and hence combining the two, we have the property claimed.

15

Lemma 10 Algorithm Mk-Amortized has completed-load competitiveness at least min
where c(cid:48) is a constant that depends on parameter c of the algorithm. For large enough c, c(cid:48) can be arbitrarily
close to 1.

1≤j<i≤k

ρi,j +ρi,j

(cid:110) ρi,j

(cid:111) · c(cid:48),

Consider now adapting the value of c in the executions of algorithm Mk-Amortized; in particular, grad-
ually increasing it to 2c when the total completed load of the executed tasks is big enough to guarantee
the current competitive ratio close enough to c(cid:48)γ for the current value of c. Following this adaptation, the
completed load competitiveness will get arbitrarily close to γ after suﬃciently long time, giving the following
theorem.

Theorem 3 Algorithm Mk-Amortized

can

reach

the

optimal

completed-load

competitiveness,

(cid:110) ρi,j

(cid:111)

.

min

1≤j<i≤k

ρi,j +ρi,j

4 Speedup
Let us now look at the case in which the machines have speedup s ≥ 1. As we have mentioned, the negative
results (upper bounds) of completed-load competitiveness of any work-conserving algorithm ALGW shown
for the setting of 1 machine, still hold for the case of m machines. However, the positive results (lower bounds)
may not. In this case, we show that for speciﬁc amounts of speedup two positive results are preserved in the
multiple machine setting.

Theorem 4 Any distributed work-conserving algorithm ALGW , running on a system with m machines with
speedup s ≥ ρ, that guarantees non redundant executions while there are at least m2 tasks pending, has a
completed-load competitive ratio C(ALGW ) ≥ 1/ρ.
Proof: We consider any distributed work-conserving algorithm ALGW , running on m parallel machines
with speedup s ≥ ρ. For the proof of the theorem we consider only the periods of execution during which
there are at least m2 pending tasks and look at the number of pending and completed tasks. During the
remaining time of the executions, the completed load is bounded by the number of tasks pending (i.e.,
< m2πmax).

Let us then consider the execution of each machine of the system individually, and look at the number
of completed tasks. In particular, observe ﬁrst that for time instant t = 0, at the beginning of an execution
of each machine p, |Np(A, 0)| ≥ |Np(X, 0)|. Then consider any time instant t > 0 and a corresponding t(cid:48) < t
in its execution, such that t(cid:48) is the latest time before t that the machine has either crashed or restarted. By
the deﬁnition of t(cid:48) there are always at least m2 tasks within interval T = (t(cid:48), t]. By induction hypothesis, at
time t(cid:48), |Np(A, t(cid:48))| ≥ |Np(X, t(cid:48))|.

(cid:107)

(cid:106) t−t(cid:48)

(cid:107) ≥ |Np(A, t(cid:48))| +

(cid:106) t−t(cid:48)
s ≤ πmin time to execute any task, since s ≥ ρ = πmax

Now let IT be the number of tasks injected during interval T . Since ALG is a work-conserving algorithm,
it is continuously scheduling and executing tasks in the interval T . What is more, we know that it need
. This means that |Np(A, t)| ≥
at most πmax
|Np(A, t(cid:48))| +
. On the other hand, algorithm X needs at least πmin time to
. This results to |Np(A, t)| ≥ |Np(X, t)|.
complete a task, which means that |Np(X, t)| ≤ |Np(X, t(cid:48))| +
Combining the result for all machines of the system we have that |N (A, t)| =
|Ni(A, t)| ≥
|Ni(X, t)| = |N (X, t)|. This leads to the desired completed-load competitiveness, C(ALGW ) ≥ 1/ρ since

m(cid:80)

(cid:106) t−t(cid:48)

πmin

m(cid:80)

πmax/s

(cid:107)

πmin

πmin

i=1

i=1
ALGW may be completing only πmin-tasks while X completes πmax ones.

Theorem 5 Any distributed work-conserving algorithm ALGW running in a system with m machines and
speedup s ≥ 1 + ρ, that guarantees non redundant executions while there are at least m2 tasks pending, has
completed-load competitive ratio C(ALG) ≥ 1.

16

In [12], we studied some of the most popular algorithms in task scheduling, in the setting of one machine
and analyzed their complete-load competitiveness under various ranges of speedup. Algorithm LIS becomes
1-completed-load competitive as soon as s ≥ max{ρ, 2}. However, when looking at its performance in the
setting of m machines (see its pseudo-code Alg. 6 in the Appendix), we realized that even in the case of 2
machines, it may not achieve 1-completed-load competitiveness.

Theorem 6 When algorithm m-LIS runs in a parallel system of two machines (m = 2) and speedup s =
ρ = 2, it is not 1-completed-load competitive, i.e., C(m-LIS) < 1.

5 Conclusions

In this work, we present the problem of online distributed scheduling of tasks with diﬀerent computational
demands on fault-prone parallel systems. We conduct worst-case analysis of deterministic work-conserving
algorithms, looking at their completed-load competitiveness as the performance metric.

We show that the upper bound shown for the case of a single machine and no speedup in [11] can be
achieved in our setting with m machines, making the result a tight bound. Additionally, the algorithms
for scheduling packets of k packet lengths in one link in [19] can also be adapted to task scheduling in one
machine, and then non-trivially generalized to m machines. Hence, we present algorithms for the cases of
two or k diﬀerent task sizes that achieve optimal completed load when run without speedup.
We also show that in the case of speedup, s > 1, the competitiveness can be improved. In particular, when
speedup is s ≥ 1 + ρ, any deterministic work-conserving algorithm ALGW may achieve optimal completed
load C(ALG) = 1. However, we also give a negative result for algorithm m-LIS, the natural parallel version
of the popular Longest In System scheduling policy. We show, that while with 1 machine it achieves 1-
completed-load competitiveness with speedup s ≥ max{ρ, 2}, in a system of two machines running with
speedup s = ρ = 2 its completed-load competitiveness is C(m-LIS) < 1.

There are still a few open questions though, some of which we would like to answer in future works.
We believe that m-LIS is an important and interesting algorithm, for its popularity and fairness. As we
just mentioned, the completed-load competitiveness depends on the number of machines m. It would be
interesting to understand better the exact relation between m, s, and completed-load competitiveness in
m-LIS. A second concrete question is whether there is a scheduling algorithm that achieves 1-completed
load competitiveness with speedup s < 1 + ρ.

6 Acknowledgements

Supported in part by the grant TEC2014-55713-R of the Spanish Ministry of Economy and Competitiveness
(MINECO), the Regional Government of Madrid (CM) grant Cloud4BigData (S2013/ICE-2894, co- funded
by FSE & FEDER), the NSF of China grant 61520106005, and European Commission H2020 grants ReCred
and NOTRE. Also partially supported by the FPU12/00505 grant from the Spanish Ministry of Education,
Culture and Sports (MECD).

References

[1] S. Albers, A. Antoniadis, and G. Greiner. On multi-processor speed scaling with migration: extended
abstract. In Proceedings of the 23rd ACM symposium on Parallelism in algorithms and architectures,
SPAA ’11, pages 279–288, New York, NY, USA, 2011. ACM.

[2] S. Anand, N. Garg, and N. Megow. Meeting deadlines: How much speed suﬃces? In Proceedings of
the 38th International Colloquium on Automata, Languages and Programming (ICALP 2011), pages
232–243, 2011.

17

[3] Antonio Fern´andez Anta, Chryssis Georgiou, Dariusz R Kowalski, and Elli Zavou. Online parallel

scheduling of non-uniform tasks: trading failures for energy. Theoretical Computer Science, 2015.

[4] Jordi Arjona Aroca, Angelos Chatzipapas, Antonio Fern´andez Anta, and Vincenzo Mancuso. A
measurement-based analysis of the energy consumption of data center servers. In Proceedings of the 5th
International Conference on Future Energy Systems, e-Energy ’14, pages 63–74, New York, NY, USA,
2014. ACM.

[5] B. Awerbuch, S. Kutten, and D. Peleg. Competitive distributed job scheduling (extended abstract). In
Proceedings of the twenty-fourth annual ACM symposium on Theory of computing, STOC ’92, pages
571–580, New York, NY, USA, 1992. ACM.

[6] Anju Bala and Inderveer Chana. Fault tolerance-challenges, techniques and implementation in cloud

computing. IJCSI International Journal of Computer Science Issues, 9(1):1694–0814, 2012.

[7] H.L. Chan, J. Edmonds, and K. Pruhs. Speed scaling of processes with arbitrary speedup curves on a
multiprocessor. In Proceedings of the twenty-ﬁrst annual symposium on Parallelism in algorithms and
architectures, SPAA ’09, pages 1–10, New York, NY, USA, 2009. ACM.

[8] Ho-Leung Chan, Joseph Wun-Tat Chan, Tak-Wah Lam, Lap-Kei Lee, Kin-Sum Mak, and Prudence WH
Wong. Optimizing throughput and energy in online deadline scheduling. ACM Transactions on Algo-
rithms (TALG), 6(1):10, 2009.

[9] Jaliya Ekanayake and Geoﬀrey Fox. Cloud Computing: First International Conference, CloudComp 2009
Munich, Germany, October 19–21, 2009 Revised Selected Papers, chapter High Performance Parallel
Computing with Clouds and Cloud Technologies, pages 20–38. Springer Berlin Heidelberg, Berlin,
Heidelberg, 2010.

[10] Hesham El-Rewini, Hesham H Ali, and Ted Lewis. Task scheduling in multiprocessing systems. Com-

puter, 28(12):27–37, 1995.

[11] Antonio Fern´andez Anta, Chryssis Georgiou, Dariusz R. Kowalski, Joerg Widmer, and Elli Zavou.
Measuring the impact of adversarial errors on packet scheduling strategies. Journal of Scheduling, pages
1–18, 2015.

[12] Antonio Fern´andez Anta, Chryssis Georgiou, Dariusz R. Kowalski, and Elli Zavou. Adaptive resource
management and scheduling for cloud computing. In Adaptive Resource Management and Scheduling for
Cloud Computing - Second International Workshop, ARMS-CC 2015, held in Conjunction with ACM
Symposium on Principles of Distributed Computing, PODC 2015, Donostia-San Sebasti´an, Spain, July
20, 2015, Revised Selected Papers, pages 1–16, 2015.

[13] Ch. Georgiou and A.A. Shvartsman. Do-All Computing in Distributed Systems: Cooperation in the

Presence of Adversity. Springer, 2008.

[14] Chryssis Georgiou and Dariusz R Kowalski. On the competitiveness of scheduling dynamically injected
tasks on processes prone to crashes and restarts. Journal of Parallel and Distributed Computing, 2015.

[15] Anis Gharbi and Mohamed Haouari. Optimal parallel machines scheduling with availability constraints.

Discrete Applied Mathematics, 148(1):63–87, 2005.

[16] K.S. Hong and J.Y.-T Leung. On-line scheduling of real-time tasks. In Real-Time Systems Symposium,

1988., Proceedings., pages 244–250, 1988.

[17] Ravi Jhawar, Vincenzo Piuri, and Marco Santambrogio. Fault tolerance management in cloud comput-

ing: A system-level perspective. Systems Journal, IEEE, 7(2):288–297, 2013.

18

[18] B. Joan and E. Faith. Bounds for scheduling jobs on grid processors. In Andrej Brodnik, Alejandro
L´opez-Ortiz, Venkatesh Raman, and Alfredo Viola, editors, Space-Eﬃcient Data Structures, Streams,
and Algorithms, volume 8066 of Lecture Notes in Computer Science, pages 12–26. Springer Berlin
Heidelberg, 2013.

[19] Tomasz Jurdzinski, Dariusz R. Kowalski, and Krzysztof Lorys. Approximation and Online Algorithms:
12th International Workshop, WAOA 2014, Wroc(cid:32)law, Poland, September 11-12, 2014, Revised Selected
Papers, chapter Online Packet Scheduling Under Adversarial Jamming, pages 193–206. Springer Inter-
national Publishing, Cham, 2015.

[20] Bala Kalyanasundaram and Kirk Pruhs. Speed is as powerful as clairvoyance. Journal of the ACM

(JACM), 47(4):617–643, 2000.

[21] Bala Kalyanasundaram and Kirk R Pruhs. Fault-tolerant scheduling. In Proceedings of the twenty-sixth

annual ACM symposium on Theory of computing, pages 115–124. ACM, 1994.

[22] P.C. Kanellakis and A.A. Shvartsman. Fault-Tolerant Parallel Computation. Kluwer Academic Pub-

lishers, Norwell, MA, USA, 1997.

[23] Dariusz R Kowalski, Prudence WH Wong, and Elli Zavou. Fault tolerant scheduling of non-uniform
tasks under resource augmentation. In Proceedings of the 12th Workshop on Models and Algorithms for
Planning and Scheduling Problems, pages 244–246, 2015.

[24] Kirk Pruhs, Jiri Sgall, and Eric Torng. Online scheduling. Handbook of scheduling: algorithms, models,

and performance analysis, pages 15–1, 2004.

[25] Eric Sanlaville and G¨unter Schmidt. Machine scheduling with availability constraints. Acta Informatica,

35(9):795–811, 1998.

[26] Behrooz Shirazi, Mingfang Wang, and Girish Pathak. Analysis and evaluation of heuristic methods for

static task scheduling. Journal of Parallel and Distributed Computing, 10(3):222–232, 1990.

[27] D. Sleator and R.E. Tarjan. Amortized eﬃciency of list update and paging rules. Commun. ACM,

28(2):202–208, February 1985.

[28] Qi Zhang, Lu Cheng, and Raouf Boutaba. Cloud computing: state-of-the-art and research challenges.

Journal of Internet Services and Applications, 1(1):7–18, 2010.

A Omitted Proofs

Completed-load of algorithm ρm-Preamble

Proof of Lemma 3: Let us ﬁx a pair of arrival and error patterns, such that executions of case (b) occur.
Let us now look at the scheduling decisions and performance of each machine individually, after the deﬁned
time instant t. Note that in such a case, there will only be time intervals of type T + and/or T −. Otherwise,
the execution would be of case (a) since for every time instant t there would exist a future t(cid:48) > t for which

|Lπmin(A, t(cid:48))| < ρm2(cid:86)|Lπmax (A, t(cid:48))| < m2 would hold. We deﬁne two types of periods for the machine

status: the active and the inactive periods. During an active period the machine remains alive and the
queue of pending tasks does not become empty (recall that the queue of pending tasks never becomes empty
in the execution we are studying). An inactive period is a non-active one. In other words, a time interval
[tr, tc) is active if it starts with time instant tr such that it is the time right after a restart of the machine.
Correspondingly, it ends with time instant tc such that the machine crashes. We then focus on the active
periods3, with length λ, which are further categorized in the following four kinds of phases:

3We safely ignore the inactive ones since the queue of pending tasks does not become empty and the algorithm ρm-Preamble

is work-conserving. Hence inactive periods are only while the machine is still crashed.

19

1. Starts with πmin-tasks and has length λ < ρπmin.
2. Starts with πmin-tasks and has length λ ≥ ρπmin.
3. Starts with πmax-tasks and has length λ < πmax.
4. Starts with πmax-tasks and has length λ ≥ πmax.
Let as look at the ith period after time t in the execution of ρm-Preamble. Let us also denote by ai the
number of completed πmin-tasks, apart from the ρ preamble, by bi the number of completed πmax-tasks and
by ci the number of completed πmin-tasks in the preamble. For the execution of X we denote by a∗
i the
total number of completed πmin-tasks and by b∗
i the total number of completed πmax-tasks. Let also C A(ij)
and C X (ij) denote the total completed load within a phase i of type j by ρm-Preamble and X respectively.
Analyzing the four types of active periods, we make the following observations.

For phases of type 1, ρm-Preamble is not able to complete the ρ πmin tasks of the preamble, while X is

∀i

C A(i1).

only able to complete at most as much load, so(cid:80)
most πmax (i.e.,(cid:80)

C X (i1) ≤(cid:80)
(cid:0)C X (i2) − C A(i2)(cid:1) < πmax). Therefore,
·(cid:88)
C X (i4) ≤ 2(cid:80)

The same holds for phases of type 4 and hence(cid:80)
In phases of type 3, ρm-Preamble is not able complete any task and hence (cid:80)

∀i
≤ 1/2.)

C A(i2) ≥

πmax + ρπmin

(Observe that

(cid:88)

C X (i2).

C A(i4).

πmax+ρπmin

ρπmin

ρπmin

∀i

∀i

∀i

∀i

∀i

For phases of type 2, the total completed load by X minus the completed load by ρm-Preamble is at

might complete up to ((cid:100)ρ(cid:101) − 1)πmin tasks. There are two cases of executions to be considered then:

Case 1: The number of phases of type 3 is ﬁnite.
In this case, there is a phase i∗ such that ∀i > i∗ phase i is not of type 3. Then,

C A(i3) = 0, whereas X

∀i

simplicity, we overload notations A and X and deﬁne (cid:80)

Observe that the total progress completed by the end of phase i∗ by both algorithms is bounded. So for

C X (j) = X. Therefore,

(4)

C A(j) + (cid:80)
C X (j) + (cid:80)

j>i∗

(cid:80)
(cid:80)

j≤i∗

j≤i∗

C A(j)

C X (j)

C1(A) =

A + (cid:80)
X + (cid:80)

j>i∗

j>i∗

j≤i∗

A +

C A(j)

C X (j)

≥

C1(A) =

j≤i∗

j>i∗

ρπmin

πmax+ρπmin

C A(j) = A and (cid:80)
(cid:80)
X + (cid:80)
(cid:80)

j>i∗
C X (j)

j>i∗

C X (j)

C X (j)

.

Hence, the completed load competitiveness of ρm-Preamble at the end of each phase can be computed as
C(ρm-Preamble) = limt→∞ C1(A), i.e.,

C(ρm-Preamble) = lim
j→∞

A +

(cid:18)

πmax+ρπmin

ρπmin

X + (cid:80)

j>i∗

ρπmin

j>i∗
C X (j)

(πmax + ρπmin)(X + (cid:80)

(πmax + ρπmin)A − (ρπmin)X
C X (j))

+

j>i∗

(cid:19)

= lim
j→∞

πmax + ρπmin

=

ρπmin

πmax + ρπmin

=

ρ

ρ + ρ

.

20

limj→∞ (cid:80)

j>i∗

It is important to note that the assumption limt→∞ C X (t) = ∞ is used, which corresponds to the expression

C X (j) in the above equality.

The above analysis shows the completed-load competitiveness at the end of each phase. However, we
have to guarantee that the lower bound holds at all times within the phases. For this, consider any time
instant t of phase i > i∗. At that instant Ci(t) =
, where At and Xt represent the load
completed by ρm-Preamble and X within phase i up to time t. Using the above proof, and the fact that for
phases of type 1,2 and 4 we have

j∈(i∗ ,i−1] CA(j)+At
j∈(i∗ ,i−1] CX (j)+Xt

(cid:80)
(cid:80)

(cid:0)C A(i1) + C A(i2) + C A(i4)(cid:1) ≥

(cid:88)

∀i

(cid:0)C X (i1) + C X (i2) + C X (i4)(cid:1),

·(cid:88)

∀i

lim ρ

πmax + πminρ

we know that At ≥

lim ρ

πmax+πminρ · Xt as well. Hence,
(cid:80)
(cid:80)

Ci(t) ≥

πmax+ρπmin

ρπmin

j∈(i∗,i−1] C X (j) +
j∈(i∗,i−1] C X (j) + Xt

ρπmin

πmax+ρπmin

Xt

=

ρπmin

πmax + ρπmin

=

ρ

ρ + ρ

.

Case 2: The number of phases of type 3 is inﬁnite.

In this case we must show that the number of πmin and πmax-tasks completed are bounded for both
ρm-Preamble and X.

Claim A.1 Consider the time instant t at the beginning of a phase j of type 3. Then the number of πmin-
tasks completed by X by time t is no more than the number of πmin-tasks completed by ρm-Preamble, plus

ρ − 1, i.e.,(cid:80)

i ≤(cid:80)

i<j a∗

i<j(ai + ci) + (ρ − 1).

Proof: Consider the beginning of phase j of type 3. We know that at that time instant algorithm
ρm-Preamble has at most (ρ − 1) πmin-tasks pending. Recall that a machine following algorithm
ρm-Preamble, after restarting it ﬁrst completes a preamble of ρπmin tasks, before executing any πmax ones.
By the deﬁnition of type 3, it may only occur if there are not enough πmin-tasks pending at time instant
t. Hence, the amount of πmin-tasks completed by X by the beginning of phase j is no more than the ones
completed by algorithm ρm-Preamble (including the ones in preambles) plus ρ − 1.

Claim A.2 Considering all types of phases and the number of πmax-tasks completed, it holds that (cid:80)
(cid:80)
bi +(cid:80)

ci
ρ + 2, for every phase j.

i ≤
b∗

i≤j

(cid:117)(cid:116)

i≤j

i≤j

Proof: To prove this, we use induction on phase j.
Base Case: For j = 0 the claim is trivial.

Induction Hypothesis: It holds that (cid:88)

i ≤ (cid:88)

b∗

i≤j−1

i≤j−1

(cid:88)

i≤j−1

bi +

ci
ρ

+ 2.

Induction Step: We need to prove that the relationship holds up to the end of phase j. Consider ﬁrst that
during phase j there is a time when ρm-Preamble has no πmax-tasks pending, and let t be the latest such
time in the phase. We deﬁne b∗(t) and b(t) being the number of πmax-task completed up to time t by
algorithm X and ρm-Preamble respectively. We know that b∗(t) ≤ b(t). We also deﬁne x∗
j (t) and xj(t) to be
the number of πmax-tasks scheduled by X and ρm-Preamble respectively after time instant t and until the

21

j (t) ≤ xj(t) + 2. From our deﬁnitions, at time t algorithm ρm-Preamble
end of the phase j. We claim that x∗
is executing a πmin-task. Since it is the last instant that it has no πmax-task pending, the wort case is to be
at the beginning of the preamble (by inspection of the 4 types of phases). Then, if the phase ends at time
t(cid:48), period I = [t, t(cid:48)] is such that |I| < ρπmin + (xj(t) + 1)πmax ≤ (xj(t) + 2)πmax. (The +1 πmax-task is
because of the machine crash before completing the last πmax-task scheduled in the phase.) Observe that
X could be executing a πmax-task at time t, completed at some point in [t, t + πmax] and accounted for in
x∗
j (t). Therefore,

b∗
j = b∗(t) + x∗

j (t) ≤ b(t) + xj(t) + 2 =

bi + 2.

(cid:88)

i≤j

(cid:88)

i≤j

Now consider the case where at all times of phase j there are πmax-tasks pending for ρm-Preamble. By
inspection of the 4 types of phases, the worst case is when j is of type 2. After completing the preamble
of ρπmin tasks, the algorithm schedules πmax-tasks until the machine crashes again interrupting the last
one scheduled. On the same time, X is able to complete at most
j ≤ cj
is the length of the phase. Hence, in all types of phases b∗

(cid:107) ≤ bj + 1 πmax-tasks, where λj

(cid:106) λj

πmax

ρ + bj and by induction, the claim follows;
(cid:117)(cid:116)

(cid:80)

i≤j

j ≤ (cid:80)

b∗

i≤j

ρ +(cid:80)

ci

i≤j

bi + 2.

Combining the two claims above, the completed load competitiveness ratio of case 2 is as follows:

(bi + ci

ρ )πmax +2πmax

C2(A) =

[(ai + ci)πmin + biπmax]

i πmax]

=

i≤j

i≤j

i≤j

i≤j

C A(i)

C X (j)

(cid:80)

[a∗
i πmin + b∗

(cid:80)
(cid:80)

(cid:80)
(cid:80)
(cid:80)
(ai +ci)πmin +(ρ−1)πmin +(cid:80)
(cid:80)
(cid:80)
(cid:80)

i≤j
[(ai + ci)πmin + biπmax]

[(ai + ci)πmin + biπmax] + 3

i≤j

i≤j

i≤j

i≤j

i≤j

[(ai + 2ci)πmin + biπmax] + 3πmax

[(ai + ci)πmin + biπmax]

2 πmax − 3
[(ai + ci)πmin + biπmax] + 3πmax

2 πmax

≥

≥

=

−

≥ 1
2

3
2 πmax

[(ai + ci)πmin + biπmax] + 3πmax

.

2(cid:80)
2(cid:80)

i≤j

i≤j

t→∞ C2(A) ≥ 1
2 .
ρ+ρ ≤ 1
ρ
2 ,

Note that, due to the parameters ai, bi and ci, the second ratio tends to zero (the denominator tends to

inﬁnity) and hence the completed load competitive ratio tends to C(ρm-Preamble) = lim

Combining now the results from the two cases concerning the number of phases of type 3, since

the completed load of algorithm ρm-Preamble is at least

ρ
ρ+ρ as claimed.

Lemma

Completed-load of Algorithm Amortized

Proof of Lemma 6: Let us divide the n-busy interval T = [t, t(cid:48)] of p into two intervals; the ﬁrst being from
the beginning, t, to a time instant t∗ ≥ t such that the ﬁrst restart happens in the interval, and the second
being the remaining of the interval, from t∗ to t(cid:48). In other words, interval T1 = [t, t∗] and T2 = [t∗, t(cid:48)].
Looking ﬁrst at interval T2, it starts by a restart and then, either includes more crashes and restarts
or not, and never schedules tasks of size more than πn. Hence, at time t∗ the machine starts executing a

22

Looking now at interval T1, we must consider the following cases for the execution of p:

new task with both k-Amortized and X. Also, since p is busy at all times of the interval, for every πi-task
completed by X in T2 – say in interval Ti = [t1, t2] ∈ T2 where t2 = t1 + πi and i ≥ n – the machine is able to
complete ρi,n n-groups in Ti (each of size πn). These groups correspond to executions of the recursive function
Schedule Group(n). Hence, we can assign each n-group to the task completed by X at the moment when
p (T2,≥ πn).
the last task in the n-group is completed by k-Amortized, which gives inequality C A
(1) At time instant t there was a restart (t∗ = t) and hence the machine started executing a new task, with
both X or k-Amortized. In this case the analysis of interval T2 will hold.
(2) At time instant t it is already executing a task τ with X, scheduled before t and then, it either a) gets
interrupted by the crash at time t∗, or b) it completes it within the interval T1.
In the ﬁrst case, X is
not able to complete any task in T1 while k-Amortized may complete up to |T1|, for which it is trivial that
p (T1) ≥ C X
C A
p (T1) holds. In the latter, task τ will be of maximum size πk. Then for the rest of the interval,
the same analysis as for T2 holds, for every πi-task fully contained in the interval and completed by X, where
i ≥ n. Hence, C A
(cid:117)(cid:116)

From the two intervals, we have the claim of the lemma, C A

p (T1,≥ πn) − πk.

p (T ) ≥ C X

p (T,≥ πn) − πk.

p (T2) ≥ C X

p (T1) ≥ C X

Proof of Lemma 7: The idea of the proof for this lemma, is that tasks completed by algorithm k-Amortized
are associated to tasks completed by X in such a way, that: (a) each task completed by k-Amortized
corresponds to at most twice the size of tasks completed by X and (b) each task completed by X is associated
to tasks of the same aggregate size completed by k-Amortized. This amortization follows these two rules:

rth task of size πi completed by X within T (if completed).

1. the rth task of size πi completed by algorithm k-Amortized within T , for i < n, is associated to the
the completion of a task τ of size πi ≥ πn by X, corresponds to πi/πn n-groups completed by
k-Amortized, such that the execution of the last task of each of the groups is ﬁnished during the execution
of task τ .

2.

First, looking at rule #1 and interval [t1, t], the following two equations hold for the pending tasks at

the end of the interval:

P A(t, < πn) = P A(t1, < πn) + I([t1, t], < πn) − C A
P X (t, < πn) = P X (t1, < πn) + I([t1, t], < πn) − C X

p ([t1, t], < πn),

p ([t1, t], < πn),

where I([t1, t], < πn) is the set of tasks smaller than πn that were injected during the interval, up to time t.
Since they are the same for both algorithms, from the above equations we have:

P A(t, < πn) − P A(t1, < πn) + C A

p ([t1, t], < πn) = P X (t, < πn) − P X (t1, < πn) + C X

p ([t1, t], < πn)

which leads to the completed load of k-Amortized containing only small tasks, < πn, being bounded as:

p ([t1, t], < πn) ≥ C X
C A

p ([t1, t], < πn) + P X (t, < πn) − n−1(cid:88)

i=1

(πn + πi)(m2 + mρi+1).

(5)

tasks is P A(t, < πn) ≤ n−1(cid:80)

To see why the inequality holds, look ﬁrst at the pseudo-code of the algorithm, more precisely line 6 of
algorithm 3; a task of size πn is scheduled at time t only in the case when the total size of smaller pending
(πn + πi)(m2 + mρi+1). Recall also condition (3) of the n-busy interval of the
machine; i.e., P A(t1, πi) ≥ P X (t1, πi),∀i ∈ [1, n], which also means that P A(t1,≤ πn) ≥ P X (t1,≤ πn).
Combining these properties, the inequality follows.
Now, looking at rule #2, for any task τ of size πi ≥ πn completed by X, we have already shown in

i=1

Lemma 6 that

C A
p

(cid:0)[t1, t](cid:1) ≥ C X

p

(cid:0)[t1, t],≥ πn

(cid:1) − πk.

(6)

23

Combining the two equations, 5 and 6, the claim follows: 2C A
p

mρi+1) − πk.

(cid:0)[t1, t](cid:1) ≥ C X

p

(cid:0)[t1, t](cid:1) − n−1(cid:80)

i=1

(πn + πi)(m2 +
(cid:117)(cid:116)

p (T ) ≥ C X

p (T ) − πk holds directly.

Proof of Lemma 8: We prove this lemma by induction on n.
Base case. For n = 1, the result is immediate from Lemma 6. More precisely, since C X
then 2C A
Induction Hypothesis. We assume that the result holds for some n < k, i.e., 2C A
Inductive Step. We show that the result still holds for n + 1. For this, we split the (n + 1)-busy interval T
in three sub-intervals:
• T1 is the interval from the beginning of T to time instant t at which k-Amortized starts executing an

p (T,≥ π1) = C X

p (T ) ≥ C X

p (T ) − fn.

p (T ),

πn+1-task for the last time during T .

• T2 is the interval from t to t(cid:48) ∈ T s.t. either k-Amortized completes the πn+1-task, or it gives up
scheduling tasks of size πn+1 at t(cid:48) since it now has enough smaller tasks pending to cover the πn+1
time and there was a crash and restart of the machine.

• T3 from time instant t(cid:48) to the end of T .

For sub-interval T1 we know that Lemma 7 holds, hence

p (T1) + P X (t, < πn+1) − n(cid:88)

2C A

p (T1) ≥ C X

(πn+1 + πi)(m2 + mρi+1) − πk.

(7)

i=1

Let us now consider an oﬄine algorithm X(cid:48), which acts as X during T , except the fact that it starts sub-
interval T2 only with tasks with length at least πn+1 (no smaller ones), and stays idle whenever X executes
a task that was pending in its queue at time instant t but not in the queue of X(cid:48).
πn+1 time after time instant t, in other words, |T2| ≤ πn+1. Hence, C X(cid:48)
from the possibility of X(cid:48) scheduling a task before T2 and completing it within T2. Hence,

Note here, that algorithm k-Amortized ﬁnishes the last attempt to complete a πn+1-task, no longer than
p (T2) ≤ πn+1 + πk, where πk comes

2C A

p (T2) ≥ 0 ≥ C X(cid:48)

p (T2) − πn+1 − πk.

At the beginning of sub-interval T3, we have that P A(t(cid:48), πi) ≥ P X(cid:48)

(8)
(t(cid:48), πi) for each i ≤ n, since k-Amortized
only attempted the execution of a πn+1 during T2 and X(cid:48) starts the T2 without any tasks smaller than πn+1.
This means that the inductive hypothesis holds for sub-interval T3 for the largest task πn and oﬄine algorithm
X(cid:48) instead of X:

(9)
Now observe that at time instant t, at the beginning of interval T2, algorithm X has P X (t, < πn+1) more

tasks pending than X(cid:48). Hence, by the end of interval T3 the following will hold:

2C A

p (T3) ≥ C X(cid:48)

p (T3) − fn.

p (T2 ∪ T3) ≥ C X
C X(cid:48)

p (T2 ∪ T3) − P X (t, < πn+1).

(10)

Putting equations 7 to 10 together to calculate the completed load of the total interval T , we have
p (T ) ≥ C X

(πn+1 + πi)(m2 + mρi+1) − πn+1 − 2πk + fn

p (T1) + C X(cid:48)

2C A

p (T2 ∪ T3) + P X (t, < πn+1) − n(cid:88)
p (T2 ∪ T3) − n(cid:88)

i=1

(πn+1 +πi)(m2 +mρi+1)−πn+1−2πk−fn

i=1

(πn+1 + πi)(m2 + mρi+1) − πn+1 − 2πk − fn

≥ C X

p (T1) + C X

p (T ) − n(cid:88)

i=1

p (T ) − fn+1

≥ C X
≥ C X

which completes the induction step and thus the proof of the lemma.

24

(cid:117)(cid:116)

Completed-load of Algorithm MAmortized

The following deﬁnition and Claim are necessary for the proof of Lemma 10.

Deﬁnition 7 An execution of Schedule Group(k) is considered to be uniform, if the algorithm completes
tasks of a single ﬁxed size πi only, during the executions of the current Schedule Group(k) as well as the
previous one.
Claim A.3 There are at least ck − 2k uniform calls of Schedule Group(k) in a stage of Mk-Amortized.
Proof: As already mentioned, the value of i∗ can only decrease during a stage. Since there are up to k task
sizes, this can happen up to k − 1 times. However, for a Schedule Group(k) execution to be uniform, its
previous execution must also be uniform. Hence, there can be at least ck − 2k calls of Schedule Group(k) in
a stage that are uniform.

(cid:117)(cid:116)

Proof of Lemma 10: Let us start by assuming that all the executions of Schedule Group in Mk-Amortized
are uniform. Every task πi ≥ πn completed by X, will correspond to a group of (cid:98)πi/πn(cid:99) tasks of total size πi
completed by algorithm Mk-Amortized. In particular, to the group of tasks that completed their execution
successfully during the execution of the task πi by X.
{δi,j}. This means that
γ = 1/(1 + δ). The assignment of tasks completed by Mk-Amortized, to each πi-task completed by X, where
i ≥ n, makes the inequalities from Lemmas 6, 7 and 8 as follows:

. Let us also deﬁne δi,j = ρi,j
ρi,j

(cid:110) ρi,j

and δ = max
i>j

Now let γ = min

i≤j<i≤k

(cid:111)

ρi,j +ρi,j

(1 + δ) · C A([t1, t]) ≥ C X ([t1, t]) + P X (t, < πn) − n(cid:88)

δ · C A(T ) ≥ C X (T,≥ πn) − πk

(πn+1 + πi)(m2 + mρi+1) − πn

(1 + δ) · C A(T ) ≥ C X (T ) − fn

i=1

(cid:110) ρi,j

(cid:111)

ρi,j +ρi,j

Then, if we apply the above inequalities in the proof of Theorem 2, we obtain the claimed result, without
the c(cid:48) factor, since

.

1
1+δ = min

1≤j<i≤k

Nonetheless, the above case only covers the uniform executions. Let us now consider the cases where
there is some execution of Schedule Group that is not uniform. By Claim A.3, at most a fraction of calls
to Schedule Group(k) are not uniform; that is, 2/c of them. From Claim 3.1, even without the uniform

executions we have the completed load competitiveness at least η = min
i∈[2,k]

Combining the two cases, we separate the uniform and the non-uniform executions, and denote the
2 (T ) respectively. We therefore have the following

1 (T ) and C X

corresponding completed load of X by C X

relationships between the two algorithms:(cid:18)

(cid:19)

(cid:110) ρi−1

2ρi−1

(cid:111)

.

for constants c1 and c2 that depend on the task sizes. This means that

· C A(T ) ≥ γ · C X

1 − 2
c
· C A(T ) ≥ η · C X

2 (T ) − c2

1 (T ) − c1

2
c

(cid:19)

· C A(T ) ≥ γ ·(cid:16)

C X

1 (T ) + C X

2 (T )

(cid:17) − c1 − c2

(cid:18)

1 − 2
c

+

2γ
cη

which leads to the desired completed load competitiveness:

where we can deﬁne the c(cid:48) of the lemma equal to
arbitrarily close to 1. This completed the proof of the lemma.

1

C A(T ) ≥

1

1 + 2/(cη)

· γ · C X (T ) − c1 − c2
1+2/(cη) and choose such a c, large enough, to make c(cid:48)
(cid:117)(cid:116)

25

Completed-load in the case of speedup

Proof of Theorem 5: We consider any distributed work-conserving algorithm ALGW , running on m
parallel machines with speedup s ≥ 1 + ρ. For the proof of the theorem let us consider only the periods of
execution during which there are at least m2 pending tasks. During the remaining time of the executions,
the completed load is bounded by the number of tasks pending (i.e., < m2πmax).

t(cid:48)(ALG, A, E) ≥ C p

Consider then, the execution of machine p and the corresponding execution of oﬄine algorithm X in
the same machine. We will be looking at their completed load by machine p at diﬀerent time instances.
Let us look at any time t and deﬁne time instant t(cid:48) < t to be the latest time before t at which one of the
following two events happens: (1) and active period starts (t(cid:48) is a restart point of p), or (2) algorithm X has
successfully completed a task.
It is then trivial that C p

0 (ALG, A, E) ≥ C p

t(cid:48)(X, A, E) holds at time t(cid:48), we prove by induction that C p

assuming that C p
C p
execution of ALG by time t have at least the same total size as the ones completed by X.

0 (X, A, E) holds at the beginning of the executions. Now,
t (ALG, A, E) ≥
t (X, A, E) still holds at time t. This also means that the tasks successfully completed by machine p in the
Looking at the interval T = (t(cid:48), t], we have to consider the following two cases:
Case 1: X is not able to complete any task in the interval T . This means that C p
t(cid:48)(X, A, E).
For ALG it holds that C p
t(cid:48)(ALG, A, E), even if it is not able to complete any tasks in T .
Therefore, C p
Case 2: X completes a task in the interval T . Note that, due to the deﬁnition of t(cid:48), there can only be one task
completed by X within T , and it must be completed exactly at time instant t. Then, interval T has length
equal to a task π ∈ [πmin, πmax]; the size of the task completed by X. In T algorithm ALG executes tasks
continuously, whose aggregate size is at least πs− πmax. Then, the completed load of machine p with the two
t(cid:48)(ALG, A, E)+(πs−πmax).
algorithms at t satisﬁes C p
Observe that the fact that s ≥ 1 + ρ implies that πs − πmax ≥ π. Hence, C p

t (ALG, A, E) ≥ C p

t (ALG, A, E) ≥ C p

t (X, A, E) = C p

t(cid:48)(X, A, E)+π and C p

t (ALG, A, E) ≥ C p

t (X, A, E).

t (ALG, A, E) ≥ C p

t (X, A, E).

t (X, A, E) = C p

The above analysis shows that the completed-load competitiveness at any time of the execution of each
machine is 1. Therefore, taking the sum of completed loads of all machines, gives the claimed result;
the completed-load competitiveness ratio of any work-conserving distributed algorithm that guarantees non
redundant executions while there are at least m2 tasks pending, is C(ALG) ≥ 1, when s ≥ 1 + ρ.
(cid:117)(cid:116)

Let us present here the pseudo-code of algorithm m-LIS.

Algorithm 6 m-LIS (for machine p)

Repeat

1 Parameters: m, πmin, πmax
2 Upon awaking or restart
3
4
5
6
7
8
9

Get sorted queue Q from the Repository;
If |Q| ≥ m2 then
Schedule task π at position p · m in Q;
Schedule task π at position (p · m) mod |Q|;

Inform Repository of completion of task π;

else

Proof of Theorem 6 (Sketch): We look at the case when m-LIS runs in a parallel system of two machines
(m = 2) and speedup s = ρ = 2. Let us ﬁx an adversarial strategy, consisting of task arrival and machine
error patterns A and E, that work as follows:
We deﬁne δ = ρ1/5 ≈ 1.15, and use only tasks of sizes x· πmin, for x ∈ {1, δ, δ2, δ3, δ4, δ5}. For simplicity,

in the rest of the proof we remove the factor πmin, which only introduces a scaling factor.

The arrival pattern A is the following sequence of task sizes that is repeated over and over:

1, δ2, δ2, δ4, δ, δ3, δ3, δ5,···

(11)

26

Let the arrival of tasks be fast enough so that whenever algorithms m-LIS or X are supposed to schedule a
task in the description below such a task is in the repository.

The execution then behaves as follows: We divide the execution in epochs, so that in one epoch m-LIS
executes the tasks that are in positions 1 to 8 in the repository sorted by arrival time, as shown above. As
deﬁned by m-LIS, processor 1 always schedules the task in position 1, while processor 2 schedule the task in
position 3. In each epoch there are 8 phases as follows:

1. In the ﬁrst phase, processor 1 is crashed, while processor 2 restarts, is active for time δ3, and the
In this phase, X schedules and completes a task of length δ3. On its hand, m-LIS
crashes again.
schedules and completes a task of length δ2, and schedules a task of length δ4 that is interrupted, since
δ2+δ4

2 > δ3.

2. In the second phase, processor 2 stays crashed. Processor 1 is active for time δ.

In this phase X
schedules and completes a task of length δ. m-LIS schedules and completes a task of length 1, and
schedules a task of length δ2 that is interrupted.

3. In the third phase, processor 2 remains crashed. The tasks in positions 1 and 2 at the start of the
task have lengths δ2 and δ4. In this phase, processor 1 is active for δ3 time. As in the ﬁrst phase, X
completes a task of length δ3, while m-LIS only completes a task of length δ2.

4. In the fourth phase, processor 2 keeps being crashed. The tasks in positions 1 and 2 at the start of the
task have lengths δ4 and δ. In this phase, processor 1 is active for δ2 time. Hence, X completes a task
of length δ2 while m-LIS only completes a task of length δ4.

These ﬁrst four phases complete the execution of the 4 ﬁrst tasks in the sequence 11 above. The next three
phases of the epoch are similar to phases 1 to 3, but all task lengths have an additional factor δ. In the ﬁnal
eighth phase, only processor 1 is active, for δ2 time, X completes a task of length δ2, and m-LIS completes a
task of length δ5 (the next task in arrival order is the task of length 1 that starts a new sequence of 8 tasks
like sequence 11 above).
The total length of the tasks completed by X in the ﬁrst epoch is then C(X) = 2δ4 +2δ3 +3δ2 +δ ≈ 11.62,
while nLIS has completed C(m-LIS) = δ5 + δ4 + 2δ3 + 2δ2 + δ + 1 ≈ 11.56. Exactly the same behavior is
(cid:117)(cid:116)
repeated in every epoch. Hence, mLIS is not 1-competitive with s = ρ = m = 2.

27

