Inferential Privacy Guarantees for Differentially Private

Mechanisms

Arpita Ghosh∗

Robert Kleinberg†

Abstract

The correlations and network structure amongst individuals in datasets today—whether explicitly ar-
ticulated, or deduced from biological or behavioral connections—pose new issues around privacy guar-
antees, because of inferences that can be made about one individual from another’s data. This motivates
quantifying privacy in networked contexts in terms of ‘inferential privacy’—which measures the change
in beliefs about an individual’s data from the result of a computation—as originally proposed by Dale-
nius in the 1970’s. Inferential privacy is implied by differential privacy when data are independent, but
can be much worse when data are correlated; indeed, simple examples, as well as a general impossi-
bility theorem of Dwork and Naor, preclude the possibility of achieving non-trivial inferential privacy
when the adversary can have arbitrary auxiliary information. In this paper, we ask how differential pri-
vacy guarantees translate to guarantees on inferential privacy in networked contexts: speciﬁcally, under
what limitations on the adversary’s information about correlations, modeled as a prior distribution over
datasets, can we deduce an inferential guarantee from a differential one?

We prove two main results. The ﬁrst result pertains to distributions that satisfy a natural positive-
afﬁliation condition, and gives an upper bound on the inferential privacy guarantee for any differentially
private mechanism. This upper bound is matched by a simple mechanism that adds Laplace noise to the
sum of the data. The second result pertains to distributions that have weak correlations, deﬁned in terms
of a suitable “inﬂuence matrix”. The result provides an upper bound for inferential privacy in terms of
the differential privacy parameter and the spectral norm of this matrix.

6
1
0
2

 
r
a

M
4

 

 
 
]
S
D
.
s
c
[
 
 

1
v
8
0
5
1
0

.

3
0
6
1
:
v
i
X
r
a

∗Cornell University, Ithaca, NY, USA. arpitaghosh@cornell.edu
†Cornell University, Ithaca, NY, USA, and Microsoft Research New England, Cambridge, MA, USA. Partially supported by a

Microsoft Research New Faculty Fellowship. robert.kleinberg@cornell.edu

1 Introduction

Privacy has always been a central issue in the discourse surrounding the collection and use of personal data.
As the nature of data collected online grows richer, however, fundamentally new privacy issues emerge. In
a thought-provoking piece entitled “Networked Rights and Networked Harms” [24], the sociologists Karen
Levy and danah boyd argue that the ‘networks’ surrounding data today—whether articulated (as in explicitly
declared friendships on social networks), behavioral (as in connections inferred from observed behavior), or
biological (as in genetic databases)—raise conceptually new questions that current privacy law and policy
cannot address. Levy and boyd present case studies to demonstrate how the current individual-centric legal
frameworks for privacy do not provide a means to account for the networked contexts now surrounding
personal data.

An analogous question arises on the formal front. One of computer science’s fundamental contributions
to the public debate about private data—most prominently via the literature on differential privacy1 [11]—
has been to provide a means to measure privacy loss, which enables evaluating the privacy implications of
proposed data analyses and disclosures in quantitative terms. However, differential privacy focuses on the
privacy loss to an individual by her contribution to a dataset, and therefore—by design—does not capture
all of the privacy losses from inferences that could be made about one person’s data due to its correlations
with other data in networked contexts. For instance, the privacy implications of a database such as 23andme
for one individual depend not just on that person’s own data and the computation performed, but also on her
siblings’ data.

In this paper, we look to understand the implications of such ‘networked’ data for formal privacy guar-
antees. How much can be learnt about a single individual from the result of a computation on correlated
data, and how does this relate to the differential privacy guarantee of the computation?

Inferential privacy. A natural way of assessing whether a mechanism M protects the privacy of an indi-
vidual is to ask, “Is it possible that someone, after observing the mechanism’s output, will learn a lot about
the individual’s private data?” In other words, what is the inferential privacy—the largest possible ratio
between the posterior and prior beliefs about an individual’s data after observing the result of a computation
on the database? (This quantity is identical to the differential privacy parameter of the mechanism when
individuals’ data are independent; see §2 and [20].)
The inferential privacy guarantee will depend, of course, on both the nature of the correlations in the
database and on the precise mechanism used to perform the computation.
Instead of seeking to design
algorithms that achieve a particular inferential privacy guarantee—which would necessitate choosing a par-
ticular computational objective and correlation structure—we instead seek to analyze the inferential privacy
guarantees provided by differentially private algorithms. Speciﬁcally, we ask the following question: con-
sider the class of all mechanisms providing a certain differential privacy guarantee, say ε. What is the worst
possible inferential privacy guarantee for a mechanism in this class?

This question is pertinent to a policy-maker who can prescribe that analysts provide some degree of
differential privacy to individuals while releasing their results, but cannot control how—i.e., using what
speciﬁc algorithm—the analyst will provide this guarantee. In other words, rather than an algorithm designer
who wants to design an inferential privacy-preserving algorithm (for a particular scenario), this question
adopts the perspective of a policy-maker who can set privacy standards that analysts must obey, but is
agnostic to the analysts’ computational objectives. We choose the differential privacy guarantee as our
measure of privacy for many reasons: it is, at present, the only widely-agreed-upon privacy guarantee known
to provide strong protections even against arbitrary side information; there is a vast toolbox of differentially

1Differential privacy, which measures privacy via the relative amount of new information disclosed about an individual’s data

by her participation in a dataset, has emerged as the primary theoretical framework for quantifying privacy loss.

1

private algorithms and a well-understood set of composition rules for combining them to yield new ones;
ﬁnally, differential privacy is now beginning to make its way into policy and legal frameworks as a potential
means for quantifying privacy loss.

Measuring privacy loss via inferential privacy formalizes Dalenius’s [5] desideratum that “access to a
statistical database should not enable one to learn anything about an individual that could not be learned
without access”. While it is well known2 that non-trivial inferential privacy guarantees are incompatible
with non-trivial utility guarantees in the presence of arbitrary auxiliary information, our primary contribu-
tion is modeling and quantifying what degree of inferential privacy is in fact achievable under a particular
form of auxiliary information, such as that resulting from a known correlation structure or a limited set of
such structures. For example, as noted earlier, if the individuals’ rows in the database are conditionally inde-
pendent given the adversary’s auxiliary information, then the inferential privacy guarantee for any individual
collapses to her differential privacy guarantee. At the other extreme, when all individuals’ data are perfectly
correlated, the inferential privacy parameter can exceed the differential privacy parameter by a factor of n
(the number of individuals in the database) as we will see below. What happens for correlations that lie
somewhere in between these two extremes? Do product distributions belong to a broader class of distribu-
tions with benign correlations which ensure that an individual’s inferential privacy is not much worse than
her differential privacy? A key contribution of our paper (Theorem 4.2) answers this question afﬁrmatively
while linking it to a well-known sufﬁcient condition for ‘correlation decay’ in mathematical physics.

Correlations in networked datasets and their privacy consequences. We start with a caricature example
to begin exploring how one might address these questions in a formal framework. Consider a database which
contains an individual Athena and her (hypothetical) identical twin Adina, who is so identical to Athena that
the rows in the database corresponding to Athena and Adina are identical in (the databases corresponding to)
every possible state of the world. A differential privacy guarantee of ǫ to all database participants translates
to an inferential privacy guarantee of only 2ǫ to Athena (and her twin), since the “neighboring” database
where Athena and Adina are different simply cannot exist.3

The erosion of Athena’s privacy becomes even more extreme if the database contains n > 2 individuals
and they are all clones of Athena; a generalization of the preceding calculation now shows that the infer-
ential privacy parameter is nε. However, in reality one is unlikely to participate in a database with many
identical clones of oneself. Instead, it is interesting to consider cases with non-extreme correlations. For
example, suppose now that the database contains data from Zeus and all of his descendants, and that every
child’s bit matches the parent’s bit with probability p > 1
2. The degree of privacy afforded to Zeus now
depends on many aspects of the model: the strength of the correlation (p), the number of individuals in the
database (n), and structural properties of the tree of family relationships—its branching factor and depth,
for instance. Which of these parameters contribute most crucially to inferential privacy? Is Zeus more likely
to be implicated by his strong correlation with a few close relatives, or by a diffuse “dragnet” of correlations
with his distant offspring?

In general, of course, networked databases, and the corresponding inferential privacy guarantees, do not
come with as neat or convenient a correlation structure as in this example. In full generality, we can represent
the idea of networked similarity via a joint distribution on databases that gives the prior probability of each
particular combination of bits. So, for example, a world where all individuals in the database are “twins”
would correspond to a joint distribution which has non-zero probability only on the all-zeros and all-ones

2see, e.g., [10, 11]
3Differential privacy guarantees that the probability of an outcome o changes by at most a factor e

ǫ amongst databases at
Hamming distance one, so that if x1, x2, and x3 denote the databases where the bits of Athena and Adina are (0, 0), (1, 0) and
2ǫ · Pr(o|x3). From here, a simple calculation
(1, 1) respectively, differential privacy guarantees that Pr(o|x1) ≤ e
using Bayes’ Law—see equation (3) in Section 2—implies that: Pr(Athena=1|o)/ Pr(Athena=0|o)
, so that the inferential
privacy guarantee is 2ǫ.

ǫ · Pr(o|x2) ≤ e
Pr(Athena=1)/ Pr(Athena=0) ≤ e

2ǫ

2

databases, whereas a world where everyone’s data is independent has multiplicative probabilities for each
database.

Such a model of correlations allows capturing a rich variety of networked contexts: in addition to situ-
ations where a single database contains sensitive information about n individuals whose data have known
correlations, it also captures the situation—perhaps closest to reality—where there are multiple databases to
which multiple individuals contribute different (but correlated) pieces of information. In this latter interpre-
tation, an inferential privacy guarantee limits the amount that an adversary may learn about one individual’s
contribution to one database, despite the correlations both across individuals and between a single individ-
ual’s contributions to different databases.4

Our results. Consider a policy-maker who speciﬁes that an analyst must provide a certain differential
privacy guarantee, and wants to comprehend the inferential privacy consequences of this policy for the
population whose (correlated) data is being utilized. Our two main results can be interpreted as providing
guidance to such a policy maker. The ﬁrst result (Theorem 3.4) supplies a closed-form expression for
the inferential privacy guarantee as a function of the differential privacy parameter when data are positively
afﬁliated5[26]. The second result (Theorem 4.2) allows understanding the behavior of the inferential privacy
guarantee as a function of the degree of correlation in the population; it identiﬁes a property of the joint
distribution of data that ensures that the policy-maker can meet a given inferential privacy target via a
differential privacy requirement that is a constant-factor scaling of that target.

Among all mechanisms with a given differential privacy guarantee, which ones yield the worst inferential
privacy when data are correlated? Our ﬁrst main result, Theorem 3.4, answers this question when data are
positively afﬁliated, in addition to giving a closed-form expression for the inferential privacy guarantee. The
answer takes the following form: we identify a simple property of mechanisms (Deﬁnition 3.3) such that any
mechanism satisfying the property achieves the worst-case guarantee. Strikingly, the form of the worst-case
mechanism does not depend on the joint distribution of the data, but only on the fact that the distribution
satisﬁes positive afﬁliation. We also provide one example of such a mechanism: a “noisy-sum mechanism”
that simply adds Laplace noise to the sum of the data. This illustrates that the worst inferential privacy
violations occur even with one of the most standard mechanisms for implementing differential privacy,
rather than some contrived mechanisms.

The aforementioned results provide a sharp bound on the inferential privacy guarantee for positively
afﬁliated distributions, but they say little about whether this bound is large or small in comparison to the
differential privacy guarantee. Our second main result ﬁlls this gap: it provides an upper bound on the
inferential privacy guarantee when a bounded afﬁliation condition is satisﬁed on the correlations between
individuals’ rows in a database. Representing the strengths of these correlations by an inﬂuence matrix Γ,
Theorem 4.2 asserts that if all row sums of this matrix are bounded by 1−δ then every individual’s inferential
privacy is bounded by 2ǫ/δ, regardless of whether or not the data are positively afﬁliated. Thus, Theorem 4.2
shows that in order to satisfy ν-inferential privacy against all distributions with (1 − δ)-bounded afﬁliation,
it sufﬁces for the policy-maker to set ǫ = δν/2. We complement this result with an example showing that
the ratio of inferential privacy to differential privacy can indeed be as large as Ω( 1
δ ), as the row sums of
the inﬂuence matrix approach 1. Thus, the equivalence between inferential and differential privacy, ν = ǫ,
which holds for independent distributions, degrades gracefully to ν = O(ǫ) as one introduces correlation
into the distribution, but only up to a point: as the row sums of the inﬂuence matrix approach 1, the ratio
ν/ǫ can diverge to inﬁnity, becoming unbounded when the row sums exceed 1.

4We are grateful to Kobbi Nissim for suggesting this interpretation of our model.
5Positive afﬁliation (Deﬁnition 3.1) is a widely used notion of positive correlation amongst random variables. It is satisﬁed, for

example, by graphical models whose edges encode positively-correlated conditional distributions on pairs of variables.

3

Techniques. Our work exposes a formal connection between the analysis of inferential privacy in net-
worked contexts and the analysis of spin systems in mathematical physics. In brief, application of a dif-
ferentially private mechanism to correlated data is analogous to application of an external ﬁeld to a spin
system. Via this analogy, physical phenomena such as phase transitions can be seen to have consequences
for data privacy: they imply that small variations in the amount of correlation between individuals’ data, or
in the differential privacy parameter of a mechanism, can sometimes have gigantic consequences for infer-
ential privacy (§A.2 elaborates on this point). Statistical physics also supplies the blueprint for Theorem 4.2
and its proof: our bounded afﬁliation condition can be regarded as a multiplicative analogue of Dobrushin’s
Uniqueness Condition [6, 7], and our proof of Theorem 4.2 adapts the proof technique of the Dobrushin
Comparison Theorem [7, 12, 23] from the case of additive approximation to multiplicative approximation.
Since Dobrushin’s Uniqueness Condition is known to be one of the most general conditions ensuring ex-
ponential decay of correlations in physics, our Theorem 4.2 can informally be interpreted as saying that
differential privacy implies strong inferential privacy guarantees when the structure of networked correla-
tions is such that, conditional on the adversary’s side information, the correlations between individuals’ data
decay rapidly as their distance in the network increases.

Related work. Our paper adopts the term inferential privacy as a convenient shorthand for a notion that
occurs in many prior works, dating back to Dalenius [5], which is elsewhere sometimes called “before/after
privacy” [11], “semantic privacy” [20], or “noiseless privacy” [4]. Dwork and McSherry observed that
differentially private mechanisms supply inferential privacy against adversaries whose prior is a product
distribution; this was stated implicitly in [8] and formalized in [20]. However, when adversaries can have
arbitrary auxiliary information, inferential privacy becomes unattainable except by mechanisms that pro-
vide little or no utility; see [10, 21] for precise impossibility results along these lines. Responses to this
predicament have varied: some works propose stricter notions of privacy based on simulation-based seman-
tics, e.g. zero-knowledge privacy [14], others propose weaker notions based on restricting the set of prior
distributions that the adversary may have, e.g. noiseless privacy [4], and others incorporate aspects of both
responses, e.g. coupled-world privacy [3] and the Pufferﬁsh framework [22]. Our work is similar to some
of the aforementioned ones in that we incorporate restrictions on the adversary’s prior distribution, however
our goal is quite different: rather than proposing a new privacy deﬁnition or a new class of mechanisms, we
quantify how effectively an existing class of mechanisms (ε-differentially private mechanisms) achieves an
existing privacy goal (inferential privacy).

Relations between differential privacy and network analysis have been studied by many authors—e.g.
[19] and the references therein—but this addresses a very different way in which networks relate to privacy:
the network in those works is part of the data, whereas in ours it is a description of the auxiliary information.
The exponential mechanism of McSherry and Talwar [25] can be interpreted in terms of Gibbs measures,
and Huang and Kannan [18] leveraged this interpretation and applied a non-trivial fact about free-energy
minimization to deduce consequences about incentive compatibility of exponential mechanisms. Aside
from their work, we are not aware of other applications of statistical mechanics in differential privacy.

2 Deﬁning Inferential Privacy

In this section we specify our notation and basic assumptions and deﬁnitions. A population of n individuals
is indexed by the set [n] = {1, . . . , n}. Individual i’s private data is represented by the element xi ∈ X,
where X is a ﬁnite set. Except in §4 we will assume throughout, for simplicity, that X = {0, 1}, i.e. each
individual’s private data is a single bit. When focusing on the networked privacy guarantee for a particular
individual, we denote her index by a ∈ [n] and sometimes refer to her as “Athena”.

A database is an n-tuple x ∈ X n representing the private data of each individual. As explained in

4

Section 1, our model encodes the ‘network’ structure of the data using a probability distribution on X n; we
denote this distribution by µ. A computation performed on the database x, whose outcome will be disclosed
to one or more parties, is called a mechanism and denoted by M. The set of possible outcomes of the
computation is O, and a generic outcome will be denoted by o ∈ O. %
Differential privacy [8, 9, 11]. For a database x = (x1, . . . , xn) and an individual i ∈ [n], we use x−i to
denote the (n − 1)-tuple formed by omitting xi from x, i.e. x−i = (x1, . . . , xi−1, xi+1, . . . , xn). We deﬁne
an equivalence relation ∼i by specifying that x ∼i x′ ⇔ x−i = x′
−i. For a mechanism M and individual
i, the differential privacy parameter ǫi is deﬁned by

x ∼i x′, o ∈ O(cid:27) .

Pr(M(x′) = o)(cid:12)(cid:12)(cid:12)(cid:12)
eǫi = max(cid:26) Pr(M(x) = o)
For any vector ǫ = (ǫ1, . . . , ǫn) we say that M is ǫ-differentially private if the differential privacy parameter
of M with respect to i is at most ǫi, for every individual i.
Inferential privacy. We deﬁne inferential privacy as an upper bound on the (multiplicative) change in
Pr(xa=z1)
Pr(xa=z0) when performing a Bayesian update from the prior distribution µ to the posterior distribution after
observing M(x) = o. (If M has uncountably many potential outcomes, we must instead consider doing a
Bayesian update after observing a positive-probability event M(x) ∈ S for some set of outcomes S.)
Deﬁnition 2.1. We say that mechanism M satisﬁes ν-inferential privacy (with respect to individual a)
if the inequality Pr(xa=z1|M(x)∈S)
Pr(xa=z0) holds for all z0, z1 ∈ X and all S ⊂ O such that
Pr(M(x) ∈ S) > 0. The inferential privacy parameter of M is the smallest ν with this property.
Inferential versus differential privacy. A short calculation using Bayes’ Law illuminates the relation
between these two privacy notions.

Pr(xa=z0|M(x)∈S) ≤ eν · Pr(xa=z1)

Thus, the inferential privacy parameter of mechanism M with respect to individual a is determined by:

=

Pr(xa = z1 | M(x) ∈ S)
Pr(xa = z0 | M(x) ∈ S)
Pr(M(x) ∈ S | xa = z0)(cid:12)(cid:12)(cid:12)(cid:12)
eνa = sup(cid:26) Pr(M(x) ∈ S | xa = z1)

Pr(M(x) ∈ S | xa = z1)
Pr(M(x) ∈ S | xa = z0) ·

Pr(xa = z1)
Pr(xa = z0)

.

z0, z1 ∈ X, Pr(M(x) ∈ S) > 0(cid:27) .

Equivalently, if µ0, µ1 denote the conditional distributions of x−a given that xa = z0 and xa = z1, respec-
tively, then M is νa-inferentially private if

Pr(M(z1, y1) ∈ S) ≤ eνa Pr(M(z0, y0) ∈ S) when y0 ∼ µ0, y1 ∼ µ1.

For comparison, differential privacy asserts

Pr(M(z1, y) ∈ S) ≤ eǫa Pr(M(z0, y) ∈ S) ∀y.

When individuals’ rows in the database are independent, µ0 = µ1 and (3) implies (2) with νa = ǫa by
averaging over y. In other words, when bits are independent, ǫa-differential privacy implies ǫa-inferential
privacy. When bits are correlated, however, this implication breaks down because the databases y0, y1
in (2) are sampled from different distributions. The ‘twins example’ from §1 illustrates concretely why
this makes a difference: if µ0 and µ1 are point-masses on (0, . . . , 0) and (1, . . . , 1), respectively, then the
Pr(M(0,...,0)∈S)o. For an
inferential privacy parameter of M is determined by the equation eν = supSn Pr(M(1,...,1)∈S)

ǫ-differentially-private mechanism this ratio may be as large as enǫ since the Hamming distance between
(0, . . . , 0) and (1, . . . , 1) is n.

(1)

(2)

(3)

5

3 Positively Afﬁliated Distributions

Suppose a designer wants to ensure that Athena receives an inferential privacy guarantee of ν, given a joint
distribution µ on the data of individuals in the database. What is the largest differential privacy parameter
ǫ that ensures this guarantee? The question is very challenging even in the special case of binary data (i.e.,
when X = {0, 1}) because the ratio deﬁning inferential privacy (Equation 2) involves summing exponen-
tially many terms in the numerator and denominator. Determining the worst-case value of this ratio over
all differentially private mechanisms M can be shown to be equivalent to solving a linear program with
exponentially many variables (the probability of the event M(x) ∈ S for every potential database x) and
exponentially many constraints (a differential privacy constraint for every pair of adjacent databases).
Our main result in this section answers this question when individuals’ data are binary-valued and pos-
itively afﬁliated [13, 26], a widely used notion of positive correlation: Theorem 3.4 gives a closed-form
formula (Equation 6) that one can invert to solve for the maximum differential privacy parameter ǫ that guar-
antees inferential privacy ν when data are positively afﬁliated. The theorem also characterizes the ‘extremal’
mechanisms achieving the worst-case inferential privacy guarantee in (6) as those satisfying a ‘maximally
biased’ property (Deﬁnition 3.3). Intuitively, if one wanted to signal as strongly as possible that Athena’s
bit is 1 (resp., 0), a natural strategy—given that Athena’s bit correlates positively with everyone else’s—is
to have a distinguished outcome (or set of outcomes) whose probability of being output by the mechanism
increases with the number of 1’s (resp., the number of 0’s) in the database ‘as rapidly as possible’, subject
to differential privacy constraints. Theorem 3.4 establishes that this intuition is valid under the positive af-
ﬁliation assumption. (Interestingly, the intuition is not valid if one merely assumes that Athena’s own bit
is positively correlated with every other individual’s bit; see Remark 3.6.) Lemma 3.5 provides one simple
example of a maximally-biased mechanism, namely a “noisy-sum mechanism” that simply adds Laplace
noise to the sum of the bits in the database. Thus, the worst-case guarantee in Theorem 3.4 is achieved not
by contrived worst-case mechanisms, but by one of the most standard mechanisms in the differential privacy
literature.

We begin by deﬁning positive afﬁliation, a concept that has proven extremely valuable in auction the-
ory (the analysis of interdependent value auctions), statistical mechanics, and probabilistic combinatorics.
Afﬁliation is a strong form of positive correlation between random variables: informally, positive afﬁliation
means that if some individuals’ bits are equal to 1 (or more generally, if their data is ‘large’), other individu-
als’ bits are more likely to equal 1 as well (and similarly for 0). We formally deﬁne positive afﬁliation for our
setting below and then state a key lemma concerning positively afﬁliated distributions, the FKG inequality.
Deﬁnition 3.1 (Positive afﬁliation). Given any two strings x1, x2 ∈ {0, 1}n, let x1 ∨ x2 and x1 ∧ x2 denote
their pointwise maximum and minimum, respectively. A joint distribution µ on {0, 1}n satisﬁes positive
afﬁliation if

µ(x1 ∨ x2) · µ(x1 ∧ x2) ≥ µ(x1) · µ(x2)

for all possible pairs of strings x1, x2. Equivalently, µ satisﬁes positive afﬁliation if log µ(x) is a supermod-
ular function of x ∈ {0, 1}n.
Lemma 3.2 (FKG inequality; Fortuin et al. [13]). If f, g, h are three real-valued functions on {0, 1}n such
that f and g are monotone and log h is supermodular, then

"Xx

f (x)g(x)h(x)# "Xx

h(x)# ≥ "Xx

f (x)h(x)# "Xx

g(x)h(x)# .

(4)

In order to state the main result of this section, Theorem 3.4, we must deﬁne a property that characterizes
the mechanisms whose inferential privacy parameter meets the worst-case bound stated in the theorem. We

6

defer the task of describing a mechanism that satisﬁes the deﬁnition (or even proving that such a mechanism
exists) until Lemma 3.5 below.
Deﬁnition 3.3. For z ∈ {0, 1}, a mechanism M mapping {0, 1}n to outcome set O is called maximally
z-biased, with respect to a vector of differential privacy parameters ǫ = (ǫ1, . . . , ǫn), if there exists a set of
outcomes S ⊂ O such that Pr(M(x) ∈ S) ∝Qn
i=1 e−ǫi|xi−z| for all x ∈ {0, 1}n. In this case, we call S a
distinguished outcome set for M.
Theorem 3.4. Suppose the joint distribution µ satisﬁes positive afﬁliation. Then for any z ∈ {0, 1} and any
vector of differential privacy parameters, ǫ = (ǫ1, . . . , ǫn), the maximum of the ratio

Pr(M(x) ∈ S | xa = z)
Pr(M(x) ∈ S | xa 6= z)

,

(5)

over all ǫ-differentially private mechanisms M and outcome sets S, is attained when M is maximally
z-biased, with distinguished outcome set S. Therefore, the inferential privacy guarantee to individual a in
the presence of correlation structure µ and differential privacy parameters ǫ1, . . . , ǫn is given by the formula

νa = max

z∈{0,1}(ln(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Px=(z,y) µz(y) exp (−Pn
Px=(1−z,y) µ1−z(y) exp (−Pn

i=1 ǫi|xi − z|)

i=1 ǫi|xi − z|)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

) .

(6)

Proof. Suppose z = 0 and consider any ǫ-differentially private mechanism M and outcome set S. Letting
p(x) = Pr(M(x) ∈ S), we have the identity

Pr(M(x) ∈ S | xa = 0)
Pr(M(x) ∈ S | xa = 1)

=

Pr(M(x) ∈ S and xa = 0)
Pr(M(x) ∈ S and xa = 1)(cid:30) Pr(xa = 0)

Pr(xa = 1)

= Px=(0,y) µ0(y)p(x)
Px=(1,y) µ1(y)p(x)

.

(7)

When M is maximially 0-biased, with distinguished outcome set S, the right side of (7) is equal to
Px=(0,y) µ0(y)e−ǫ·x
Px=(1,y) µ1(y)e−ǫ·x . Thus, the z = 0 case of the theorem is equivalent to the assertion that

After cross-multiplying and simplifying, this becomes

Px=(1,y) µ1(y)p(x) ≤ Px=(0,y) µ0(y)e−ǫ·x
Px=(0,y) µ0(y)p(x)
Px=(1,y) µ1(y)e−ǫ·x
µ(x)p(x)
 · 
µ(x)p(x)
 ≤ 
 · 
 Xx=(1,y)
 Xx=(0,y)

µ(x)e−ǫ·x

 Xx=(1,y)

.


 Xx=(0,y)

µ(x)e−ǫ·x
 .

If we addhPx=(1,y) µ(x)p(x)i · hPx=(1,y) µ(x)e−ǫ·xi to both sides, we ﬁnd that (9) is equivalent to


 Xx∈{0,1}n

 · 
µ(x)p(x)
 Xx=(1,y)

µ(x)e−ǫ·x

 ≤ 

 Xx=(1,y)

 · 
µ(x)p(x)
 Xx∈{0,1}n

µ(x)e−ǫ·x
 .

To prove (10) we will apply the FKG inequality. Set h(x) = µ(x)e−ǫ·x and note that log h is the sum of
log µ—a supermodular function—and (−ǫ)· x, a linear function. Hence log h is supermodular. Now deﬁne
f (x) = p(x)eǫ·x and g(x) = xa. The differential privacy constraint for p implies that f is monotonically
non-decreasing; observe that g is monotonically non-decreasing as well. The FKG inequality implies

(8)

(9)

(10)

"Xx

f (x)h(x)# "Xx

g(x)h(x)# ≤"Xx

f (x)g(x)h(x)# "Xx

h(x)# .

(11)

Substituting the deﬁnitions of f, g, h into (11) we readily see that it is equivalent to (10), which completes
the proof.

7

Finally, as promised at the start of this section, we show that a noisy-sum mechanism that adds Laplace
noise to the sum of the bits in the database is maximally z-biased for every z ∈ {0, 1}. Together with
Theorem 3.4, this shows that any inferential privacy guarantee that can be proven for the noisy-sum mecha-
nism automatically extends to a guarantee for all differentially private mechanisms, when data are positively
afﬁliated.
Lemma 3.5. Suppose that all individuals have the same differential privacy parameter, i.e.
that ǫ =
(ǫ, ǫ, . . . , ǫ) for some ǫ > 0. Consider the noisy-sum mechanism NS that samples a random Y from
the Laplace distribution with scale parameter 1/ǫ and outputs the sum Y +Pn
i=1 xi. For all z ∈ {0, 1} the
mechanism NS is maximally z-biased.
Proof. For any x∈{0, 1}n, let |x| =Pn
i=1 xi. When z = 0 and ǫ = (ǫ, ǫ, . . . , ǫ), the deﬁnition of a maximally
z-biased mechanism requires the existence of an outcome set S such that Pr(NS(x) ∈ S) ∝ e−ǫ|x|. For the
set S = (−∞, 0], the event NS(x) ∈ S coincides with the event Y ≤ −|x|. Since Y is a Laplace random
variable with scale parameter 1/ǫ, this event has probability proportional to e−ǫ|x|, as desired. When z = 1
the proof of the lemma proceeds identically, using the set S = [n,∞).
Remark 3.6. Intuitively, one might expect Theorem 3.4 to hold whenever the joint distribution µ is such that
each pair of bits is positively correlated, a weaker property than positive afﬁliation which requires each pair
of bits to be positively correlated even after conditioning on any possible tuple of values for the remaining
bits. In Appendix A.1 we present an example illustrating that the theorem’s conclusion can be violated (in
fact, quite drastically violated) when one only assumes pairwise positive correlation. The basic reason is
that when bits are pairwise positively correlated, it may still be the case that one individual’s bit correlates
much more strongly with a non-monotone function of the others’ bits than with any monotone function.
Remark 3.7. The quantities appearing in Theorem 3.4 have precise analogues in the physics of spin systems,
and this analogy sheds light on inferential privacy. Appendix A.2 delves into this connection in detail; in this
remark we merely sketch a dictionary for translating between inferential privacy and statistical mechanics
and discuss some consequences of this translation.

2

ǫ (resp., − 1

In brief, an adversary’s prior distribution on {0, 1}n corresponds to the Gibbs measure of a two-spin
system with Hamiltonian H(x) = − ln µ(x). Under this correspondence, positively afﬁliated distributions
correspond to ferromagnetic spin systems. The adversary’s posterior distribution after applying a maximally
0-biased (resp., maximally 1-biased) mechanism is equivalent to the Gibbs measure of the spin system after
applying the external ﬁeld 1
ǫ). The worst-case inferential privacy guarantee for Athena in
2
Theorem 3.4 is therefore equivalent (up to a bijective transformation) to the magnetization at Athena’s site
when the external ﬁeld ±ǫ is applied to the spin system.
One of the interesting implications of this correspondence concerns phase transitions. Statistical-
mechanical systems such as magnets are known to undergo sharp transitions in their physical properties
as one varies thermodynamic quantities such as temperature and external ﬁeld strength. Translating these
results from physics to the world of privacy using the dictionary outlined above, one discovers that infer-
ential privacy guarantees can undergo surprisingly sharp variations as one varies a mechanism’s differential
privacy parameter or an adversary’s belief about the strength of correlations between individuals’ bits in a
database. Theorem A.2 in the appendix formalizes these observations about phase transitions in inferential
privacy.

4 Bounded Afﬁliation Distributions

In this section we present a general upper bound for inferential privacy that applies under a condition that we
call bounded afﬁliation. Roughly speaking, bounded afﬁliation requires that correlations between individu-
als are sufﬁciently weak, in the sense that the combined inﬂuence of all other individuals on any particular

8

one is sufﬁciently small. A very similar criterion in the statistical mechanics literature, Dobrushin’s unique-
ness condition [6, 7], is identical to ours except that it deﬁnes “inﬂuence” in terms of additive approximation
and we deﬁne it multiplicatively (Theorem 4.1). Dobrushin showed that this condition implies uniqueness
of the Gibbs measure for a speciﬁed collection of conditional distributions. Its implications for correlation
decay [16, 12, 23] and mixing times of Markov chains [2, 27, 17] were subsequently explored. Indeed,
our proof of network differential privacy under the assumption of bounded afﬁliation draws heavily upon
the methods of Dobrushin [7], Gross [16], and K¨unsch [23] on decay of correlations under Dobrushin’s
uniqueness condition.

Throughout this section (and its corresponding appendix) we assume that each individual’s private data
belongs to a ﬁnite set X rather than restricting to X = {0, 1}. This assumption does not add any complica-
tion to the theorem statements and proofs, while giving our results much greater generality. We now deﬁne
the notion of inﬂuence that is relevant to our results on distributions with bounded afﬁliation.

Deﬁnition 4.1. If x0, . . . , xn are jointly distributed random variables, the multiplicative inﬂuence of xj on
xi, denoted by γij, is deﬁned by the equation

−i) (cid:12)(cid:12)(cid:12)(cid:12)
e2γij = max(cid:26) Pr(xi ∈ S | x−i)
Pr(xi ∈ S | x′

S ⊆ supp(xi), x−i ∼j x′

−i(cid:27) .

In other words, the inﬂuence of xj on xi is one-half of the (individual) differential privacy parameter of xi
with respect to xj, when one regards xi as a randomized function of the database x−i. When i = j one
adopts the convention that γij = 0. The multiplicative inﬂuence matrix is the matrix Γ = (γij).

Theorem 4.2. Suppose that the joint distribution µ has a multiplicative inﬂuence matrix Γ whose spectral
norm is strictly less than 1. Let Φ = (φij ) denote the matrix inverse of I − Γ. Then for any mechanism with
individual privacy parameters ǫ = (ǫi), the inferential privacy guarantee satisﬁes

n

φijǫj.

Xj=1
∀i νi ≤ 2
If the matrix of multiplicative inﬂuences satisﬁes ∀i Pn
j=1 γijǫj ≤ (1−δ)ǫi for some δ > 0, then νi ≤ 2ǫi/δ
for all i.
Proof sketch. Let S be any set of potential outcomes of the mechanism M such that Pr(M(x) ∈ S) > 0.
Let π1 denote the conditional distribution on databases x ∈ X n, given that M(x) ∈ S, and let π2 denote
the unconditional distribution µ, respectively. For i ∈ {1, 2} and for any function f : X n → R, let
πi(f ) denote the expected value of f under distribution πi. Also deﬁne the Lipschitz constants ρi(f ) =
max{f (x) − f (x′) | x ∼i x′}. The heart of the proof lies in showing that if f takes values in R+ then

(12)

| ln π1(f ) − ln π2(f )| ≤

1
2

n

Xi,j=1

Φijǫjρi(ln f ).

(13)

This is done by studying the set of all vectors κ that satisfy | ln π1(f ) − ln π2(f )| ≤ Pn
i=1 κiρi(f )
for all f , and showing that this set is non-empty and is preserved by an afﬁne transformation T that is
a contracting mapping of Rn (when the spectral norm of Γ is less than 1) with ﬁxed point 1
2 Φǫ. To
derive (12) from (13), use the deﬁnition of νi to choose two distinct values z0 6= z1 in X such that
νi = (cid:12)(cid:12)(cid:12)
, where f, g are the indicator func-
tions of xi = z0 and xi = z1, respectively. Unfortunately ρi(ln f ) = ρi(ln g) = ∞ so direct application
of (13) is not useful; instead, we deﬁne a suitable averaging operator τ to smooth out f and g, thereby
improving their Lipschitz constants and enabling application of (13). A separate argument is then used to

ln(cid:16) Pr(xi=z1|M(x)∈S) / Pr(xi=z0|M(x)∈S)

= (cid:12)(cid:12)(cid:12)
π2(f )/π2(g)(cid:17)(cid:12)(cid:12)(cid:12)
ln(cid:16) π1(f )/π1(g)

Pr(xi=z1) / Pr(xi=z0)

(cid:17)(cid:12)(cid:12)(cid:12)

9

m=0 Γm. The full proof is presented in Appendix B.

δ

bound the error introduced by smoothing f and g using τ , which completes the proof of (12). Under the
hypothesis that Γǫ (cid:22) (1 − δ)ǫ, the relation ν (cid:22) 2
ǫ is easily derived from (12) by applying the formula
Φ =P∞
The bound νi ≤ 2ǫi/δ in the theorem is tight up to a constant factor. This is shown in §A.2 by con-
sidering an adversary whose prior is the Ising model of a complete d-ary tree T at inverse temperature
β = tanh−1(cid:0) 1−δ
d (cid:1) The entries of the inﬂuence matrix satisfy γij = β if (i, j) ∈ E(T ), 0 otherwise. Thus,
the row sumPn
sum is (d + 1) tanh−1(cid:0) 1−δ
d (cid:1) = 1 − δ − o(1) as d → ∞. In §A.2 we apply Theorem 3.4 to show that the

j=1 γij is maximized when i is an internal node, with degree d + 1, in which case the row

inferential privacy guarantee for the Ising model on a tree satisﬁes ν = Ω( ǫ
Theorem 4.2 up to a constant factor.

δ ), matching the upper bound in

5 Conclusion

A number of immediate questions are prompted by our results, such as incorporating (ε, δ)-privacy into
our analysis of inferential guarantees (for product distributions this was achieved in [20]) and extending the
analysis in §3 to non-binary databases where an individual’s data cannot be summarized by a single bit. A
key challenge here is to ﬁnd an analogue of positive afﬁliation for databases whose rows cannot naturally
be interpreted as elements of a lattice. More excitingly, however, the scenario of datasets with networked
correlations raises several broad directions for future work.
Designing for inferential privacy: Our work takes differentially private algorithms as a primitive and ana-
lyzes what inferential privacy is achievable with given differential privacy guarantees. This allows leveraging
the vast body of work on, and adoption of, differentially private algorithms, while remaining agnostic to the
data analyst’s objective or utility function. However if one instead assumes a particular measure of utility,
one can directly investigate the design of inferential-privacy preserving algorithms to obtain stronger guar-
antees: given some joint distribution(s) and utility objectives, what is the best inferential privacy achievable,
and what algorithms achieve it?
Inferential privacy and network structure: An intriguing set of questions arises from returning to the
original network structures that led to the model of correlated joint distributions. Note that our results in
Theorem 3.4 give the inferential privacy guarantee for a particular individual: how do inferential privacy
guarantees depend on the position of an individual in the network (for instance, imagine the central individ-
ual in a large star graph versus the leaf nodes), and how does the relation between the correlations and the
network structure play in?

Acknowledgements The authors gratefully acknowledge helpful discussions with Christian Borgs, danah
boyd, Jennifer Chayes, Cynthia Dwork, Kobbi Nissim, Adam Smith, Omer Tamuz, and Salil Vadhan.

References

[1] Ahlswede, R. and Daykin, D. E. (1978). An inequality for the weights of two families of sets, their

unions and intersections. Probability Theory and Related Fields, 43(3):183–185.

[2] Aizenman, M. and Holley, R. (1987). Rapid convergence to equilibrium of stochastic Ising models in
the Dobrushin-Shlosman regime. In Kesten, H., editor, Percolation Theory and Ergodic Theory of Inﬁnite
Particle Systems (Minneapolis, Minn., 1984), volume 8 of IMS Volumes in Math. and Appl., pages 1–11.
Springer.

10

[3] Bassily, R., Groce, A., Katz, J., and Smith, A. (2013). Coupled-worlds privacy: Exploiting adversarial
In Foundations of Computer Science (FOCS), 2013 IEEE 54th

uncertainty in statistical data privacy.
Annual Symposium on, pages 439–448. IEEE.

[4] Bhaskar, R., Bhowmick, A., Goyal, V., Laxman, S., and Thakurta, A. (2011). Noiseless database

privacy. In Advances in Cryptology–ASIACRYPT 2011, pages 215–232. Springer.

[5] Dalenius, T. (1977). Towards a methodology for statistical disclosure control. Statistik Tidskrift, 15(429-

444):2–1.

[6] Dobrushin, R. L. (1968). The description of a random ﬁeld by means of conditional probabilities and

conditions of its regularity. Theory of Probability and Its Applications, 13(2):197–224.

[7] Dobrushin, R. L. (1970). Prescribing a system of random variables by conditional distributions. Theory

of Probability & Its Applications, 15(3):458–486.

[8] Dwork, C. (2006). Differential privacy. In Automata, Languages and Programming (ICALP).

[9] Dwork, C., McSherry, F., Nissim, K., and Smith, A. (2006). Calibrating noise to sensitivity in private

data analysis. In Proceedings of the Third Conference on Theory of Cryptography, TCC’06.

[10] Dwork, C. and Naor, M. (2008). On the difﬁculties of disclosure prevention in statistical databases or

the case for differential privacy. Journal of Privacy and Conﬁdentiality, 2(1):8.

[11] Dwork, C. and Roth, A. (2013). The algorithmic foundations of differential privacy. Foundations and

Trends in Theoretical Computer Science.

[12] F¨ollmer, H. (1982). A covariance estimate for Gibbs measures. Journal of Functional Analysis,

46:387–395.

[13] Fortuin, C., Kasteleyn, P., and Ginibre, J. (1971). Correlation inequalities on some partially ordered

sets. Communications in Mathematical Physics, 22(2):89–103.

[14] Gehrke, J., Lui, E., and Pass, R. (2011). Towards privacy for social networks: A zero-knowledge based

deﬁnition of privacy. In Theory of Cryptography, pages 432–449. Springer.

[15] Graham, R. (1983). Applications of the fkg inequality and its relatives. In Mathematical Programming

The State of the Art, pages 115–131. Springer.

[16] Gross, L. (1979). Decay of correlations in classical lattice models at high temperature. Communica-

tions in Mathematical Physics, 68(1):9–27.

[17] Hayes, T. P. (2006). A simple condition implying rapid mixing of single-site dynamics on spin systems.

In Proc. 47th IEEE Symposium on Foundations of Computer Science (FOCS), pages 39–46. IEEE.

[18] Huang, Z. and Kannan, S. (2012). The exponential mechanism for social welfare: Private, truthful,
and nearly optimal. In Foundations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium
on, pages 140–149. IEEE.

[19] Kasiviswanathan, S. P., Nissim, K., Raskhodnikova, S., and Smith, A. (2013). Analyzing graphs with
node differential privacy. In Proceedings of the 10th Theory of Cryptography Conference on Theory of
Cryptography, TCC’13.

11

[20] Kasiviswanathan, S. P. and Smith, A. (2014). On the ‘semantics’ of differential privacy: A Bayesian

formulation. Journal of Privacy and Conﬁdentiality, 6(1):1.

[21] Kifer, D. and Machanavajjhala, A. (2011). No free lunch in data privacy. In Proceedings of the 2011

ACM SIGMOD International Conference on Management of data, pages 193–204. ACM.

[22] Kifer, D. and Machanavajjhala, A. (2014). Pufferﬁsh: A framework for mathematical privacy deﬁni-

tions. ACM Transactions on Database Systems (TODS), 39(1):3.

[23] K¨unsch, H. (1982). Decay of correlations under Dobrushin’s uniqueness condition and its applications.

Communications in Mathematical Physics, 84(2):207–222.

[24] Levy, K. and boyd, d. (2014). Networked rights and networked harms. working paper, presented at

Privacy Law School Conference (June 6, 2014) and Data & Discrimination (May 14, 2014).

[25] McSherry, F. and Talwar, K. (2007). Mechanism design via differential privacy. In Foundations of

Computer Science, 2007. FOCS’07. 48th Annual IEEE Symposium on, pages 94–103. IEEE.

[26] Milgrom, P. R. and Weber, R. J. (1982). A Theory of Auctions and Competitive Bidding. Economet-

rica, 50(5).

[27] Weitz, D. (2005). Combinatorial criteria for uniqueness of Gibbs measures. Random Structures &

Algorithms, 27(4):445–475.

A Appendix to §3: Positively Afﬁliated Distributions
This appendix contains material accompanying §3 that was omitted from that section for space reasons.
A.1 Pairwise positive correlation

A weaker condition than positive afﬁliation is pairwise positive correlation. This property of a joint dis-
tribution µ on databases x ∈ {0, 1}n requires that for each pair of indices i, j ∈ [n], the (unconditional)
marginal distribution of the bits xi, xj satisﬁes

E[xixj] ≥ E[xi] · E[xj].

If the inequality is strict for every i, j then we say µ is pairwise strictly positively correlated.

Recall Theorem 3.4, which establishes that when a joint distribution µ satisﬁes positive afﬁliation then
the worst-case inferential privacy guarantee is attained by any maximally z-biased distribution. The intuition
supporting the theorem statement might seem to suggest that the same conclusion holds whenever µ satisﬁes
pairwise positive correlation. In this section we show that this is not the case: if µ satisﬁes pairwise positive
correlation (or even strict pairwise positive correlation) there may be a mechanism whose inferential privacy
guarantee is much worse than that of any maximally z-biased mechanism.

Our construction applies when n is of the form n = 1 + rs for two positive integers r, s. For a database
x ∈ {0, 1}n we will denote one of its entries by xa and the others by xij for (i, j) ∈ [r] × [s]. The joint
distribution µ is uniform over the solution set of the system of congruences

xa +

s

Xj=1

xij ≡ 0 (mod 2)

for i = 1, . . . , r

(14)

12

Thus, to sample from µ one draws the bits xa and xij for (i, j) ∈ [r] × [s − 1] independently from the
uniform distribution on {0, 1}, then one sets xis for all i so as to satisfy (14).
The distribution µ is pairwise independent, hence it is pairwise positively correlated. (The calculation
of privacy parameters is much easier in the pairwise-independent case. At the end of this section we apply
a simple continuity argument to modify the example to one with pairwise strict positive correlation without
signiﬁcantly changing the privacy parameters.)

Let us ﬁrst calculate the inferential privacy for a mechanism M1 that calculates the number of odd

integers in the sequence

s

s

s

xa,

x1j,

x2j, . . . ,

xrj

(15)

Xj=1

Xj=1

Xj=1

and adds Laplace noise (with scale parameter 1/ǫ) to the result. This is ǫ-differentially private since changing
a single bit of x changes the parity of only one element of the sequence. However, when x is sampled from
µ the number of odd integers in the sequence (15) is either 0 if xa = 0 or n if xa = 1. Hence

Pr(M1(x) ≤ 0 | xa = 0) = 1
Pr(M1(x) ≤ 0 | xa = 1) = 1

2

2 e−(r+1)ǫ
implying that the inferential privacy parameter of M1 is at least (r + 1)ǫ.
Now let us calculate the inferential privacy parameter of a maximally 0-biased mechanism M2, with
outcome o ∈ O such that Pr(M2(x) = o | x) ∝ e−ǫ|x|, where |x| denotes the sum of the bits in x. Let
T0 (resp. T1) denote the set of bit-strings in {0, 1}s having even (resp. odd) sum, and let T r
1 denote
the rth Cartesian powers of these sets. The conditional distribution of (xij) given xa = 0 is the uniform
0 , and the conditional distribution of (xij) given xa = 1 is the uniform distribution on T r
distribution on T r
1 .
For y = (yij) ∈ {0, 1}rs and i ∈ [r], let yi∗ denote the s-tuple (yi1, . . . , yis). We have

0 , T r

Pr(M2(x) = o | x) · Pr(x | xa = 0)

0

Pr(M2(x) = o | xa = 0) = Xx=(0,y)
= Xy∈T r
= Xy∈T r
=
2(1−s) Xz∈T0
Pr(M2(x) = o | xa = 1) = e−ǫ ·

0

r

e−ǫ|y| · 2−r(s−1)
Yi=1(cid:16)e−ǫ|yi∗| · 21−s(cid:17)

r

.

e−ǫ|z|


Similarly,

(16)

(17)

r

.

2(1−s) Xz∈T1

e−ǫ|z|


(The extra factor of e−ǫ on the right side comes from the fact that xa = 1, which inﬂates the exponent in
the expression e−ǫ|x| by ǫ.) To evaluate the expressions on the right sides of (16)-(17), it is useful to let

13

A0 =Pz∈T0

e−ǫ|z| and A1 =Pz∈T1

e−ǫ|z|. Then we ﬁnd that

A0 + A1 = Xz∈{0,1}s
A0 − A1 = Xz∈{0,1}s

e−ǫ|z| = (1 + e−ǫ)s

(−1)|z| · e−ǫ|z| = (1 − e−ǫ)s

1

A0 =

A1 =

1

2(cid:2)(1 + e−ǫ)s + (1 − e−ǫ)s(cid:3)
2(cid:2)(1 + e−ǫ)s − (1 − e−ǫ)s(cid:3) .

Substituting these expressions into (16)-(17) we may conclude that

Pr(M2(x) = o | xa = 0)
Pr(M2(x) = o | xa = 1)

=

eǫ · Ar
Ar
1

0

(1 + e−ǫ)s − (1 − e−ǫ)s(cid:21)r
= eǫ(cid:20) (1 + e−ǫ)s + (1 − e−ǫ)s

.

(18)

The inferential privacy parameter of M2 is therefore given by

ν = ǫ + r ln(cid:20) (1 + e−ǫ)s + (1 − e−ǫ)s
(1 + e−ǫ)s − (1 − e−ǫ)s(cid:21)
< ǫ + r ln(cid:20)1 + 2(cid:18) 1 − e−ǫ

1 + e−ǫ(cid:19)s(cid:21)

= ǫ + r ln [1 + 2 tanhs(ǫ)] < ǫ + 2rǫs.

Comparing the inferential privacy parameters of M1 and M2, they are (r + 1)ǫ and ǫ + 2rǫs, respectively,
so the inferential privacy parameter of M1 exceeds that of the maximally 0-biased mechanism, M2, by an
unbounded factor as r, s → ∞.
Under the distribution µ we have analyzed thus far, the bits of x are pairwise independent. However,
we may take a convex combination of µ with any distribution in which all pairs of bits are strictly positively
correlated—for example, a distribution that assigns equal probability to the two databases (1, . . . , 1) and
(0, . . . , 0) and zero probability to all others. In this way we obtain a distribution µ′ which satisﬁes pairwise
strict positive correlation and may can be made arbitrarily close to µ by varying the mixture parameter of the
convex combination. Since the inferential privacy parameter of a mechanism with respect to a given prior
distribution is a continuous function of that distribution, it follows that the inferential privacy parameters of
M1 and M2 can remain arbitrarily close to the values calculated above while imposing a requirement that
the prior on x satisﬁes pairwise strict positive correlation.

A.2 Connection to Ferromagnetic Spin Systems

The quantities appearing in Theorem 3.4 have precise analogues in the physics of spin systems, and this
analogy sheds light on inferential privacy. In statistical mechanics, a two-spin system composed of n sites
has a state space {±1}n and an energy function or Hamiltonian, H : {±1}n → R. The Gibbs measure of the
spin system is a probability distribution assigning to each state a probability proportional to e−βH(σ) where
β > 0 is a parameter called the inverse temperature. Application of an external ﬁeld h ∈ Rn to the spin
system is modeled by subtracting a linear function from the Hamiltonian, so that it becomes H(σ) − h · σ.
The probability of state σ under the Gibbs measure then becomes

Pr(σ) = eβ[h·σ−H(σ)]/Z(β, h),

14

where Z(·) is the partition function

Z(β, h) = Xσ∈{±1}n

eβ[Pi hiσi−H(σ)].

Databases x ∈ {0, 1}n are in one-to-one correspondence with states σ ∈ {±1}n under the mapping
σi = (−1)xi and its inverse mapping xi = 1
2 (1− σ). Any joint distribution µ = {0, 1}n has a corresponding
Hamiltonian H(σ) = − ln µ(x) whose Gibbs distribution (at β = 1) equals µ. The positive afﬁliation
condition is equivalent to requiring that H is submodular, a property which is expressed by saying that the
spin system is ferromagnetic.

For a maximally 0-biased mechanism M with distinguished outcome set S, the probabilities p(x) =
Pr(M(x) ∈ S) satisfy p(x) ∝ exp(−Pn

2Pi ǫiσi), so

i=1 ǫixi) = exp(− n

2 + 1

µ(x)p(x) ∝ e−

n
2 +

1
2 Pi ǫiσi−H(σ).

Application of the mechanism M is thus analogous to application of the external ﬁeld 1
ǫ at inverse temper-
ature 1. (The additive constant − n
2 in the Hamiltonian is irrelevant, since the Gibbs measure is unchanged
by an additive shift in the Hamiltonian.) Similarly, applying a maximally 1-biased mechanism is analogous
to applying the external ﬁeld − 1
µ(xa=0) denote the prior probability ratio for Athena’s bit. For the networked privacy guarantee
in Theorem 3.4, when the maximum on the right side of (6) is achieved by a maximally 0-biased mechanism,
we have

ǫ at inverse temperature 1.

Let ρ = µ(xa=1)

2

2

eνa − ρ
eνa + ρ

= Px=(0,y) µ(x)p(x) −Px=(1,y) µ(x)p(x)
Px=(0,y) µ(x)p(x) +Px=(1,y) µ(x)p(x)

= Pσ σaeǫ·σ/2−H(σ)
Pσ eǫ·σ/2−H(σ)

= E[σa | h = ǫ
2 ],

where the operator E[· | h = ǫ
ﬁeld h = ǫ
side of (6) yields the relation eνa −ρ−1

2 ] denotes the expectation under the Gibbs measure corresponding to external
2. A similar calculation in the case that a maximally 1-biased mechanism maximizes the right

νa = max(cid:26)ln ρ + ln(cid:18) 1 + E[σa | h = ǫ/2]

2 ]. Combining these two cases, we arrive at:

eνa −ρ−1 = E[−σa | h = − ǫ
1 − E[σa | h = ǫ/2](cid:19) , − ln ρ − ln(cid:18) 1 + E[σa | h = −ǫ/2]

1 − E[σa | h = −ǫ/2](cid:19)(cid:27) .

(19)

i=1

We will refer to E[σa] as the magnetization at site a, by analogy with the usual deﬁnition of magnetization
nPn
in statistical mechanics as the average 1
E [σi]. Equation (19) thus shows that the inferential privacy
guarantee for a positively afﬁliated distribution is completely determined by the magnetization at site a when
an external ﬁeld of strength ±ǫ/2 is applied.
A.2.1 Ising models and phase transitions
Let us now apply this circle of ideas to analyze the “Zeus’s family tree” example from §1. Represent Zeus
and his progeny as the nodes of a rooted tree, and suppose that the joint distribution of the individuals’ bits
is deﬁned by the following sampling rule: sample the bits in top-down order (from root to leaves), setting
the root’s bit to 0 or 1 with equal probability and each other node’s bit equal to the parent’s value with
probability p > 1
2 and the opposite value otherwise. This leads to a probability distribution µ in which the
probability of any x ∈ {0, 1}V (T ) is proportional to pa(x)(1 − p)b(x) where a(x) denotes the number of tree
edges whose endpoints receive the same label, and b(x) is the number of edges whose endpoints receive
opposite labels. Letting J = tanh−1(2p− 1) so that ln(p) = ln(1− p) + 2J, and associating σ ∈ {±1}n to
x ∈ {0, 1}n via σi = (−1)xi as before, we ﬁnd that up to an additive constant, ln µ(x) = JP(i,j)∈E σiσj,

15

where E denotes the edge set of the tree. Hence, the joint distribution of Zeus’s family tree is equivalent to

the Gibbs measure of the Hamiltonian H(σ) = −JP(i,j)∈E σiσj. Models whose Hamiltonian takes this

form (for any graph, not just trees) are known as Ising models (with interaction strength J) and are among
the most widely studied in mathematical physics.

Ising models are known to undergo phase transitions as one varies the inverse temperature or external
ﬁeld. For example, in an inﬁnite two-dimensional lattice or ∆-regular tree, there is a phenomenon known as
spontaneous magnetization where the magnetization does not converge to zero as the external ﬁeld converges
to zero from above, but this phenomenon only occurs if the inverse temperature is above a critical value, βc,
that is equal to ln(1 +√2) for the two-dimensional lattice and to 1
∆−2 ) for the ∆-regular tree. This
phenomenon of phase transitions has consequences for inferential privacy, as articulated in Theorem A.2
below. To state the theorem it is useful to make the following deﬁnition.
Deﬁnition A.1. Let D be a family of joint distributions on {0, 1}∗, with each distribution µ ∈ D being
supported on {0, 1}n for a speciﬁc value n = n(µ). For a differential privacy parameter ǫ > 0, let ν(ǫ, D)
denote the supremum, over all joint distributions µ ∈ D, of the inferential privacy guarantee corresponding
to differential privacy parameter ǫ. We say that ν is differentially enforceable with respect to D if there
exists ǫ > 0 such that ν ≤ ν(ǫ, D).

2 ln(1 + 2

In other words, to say that ν is differentially enforceable means that a regulator can ensure ν-inferential
privacy for the individuals participating in a datasest by mandating that an analyst must satisfy ǫ-differential
privacy when releasing the results of an analysis performed on the dataset.

Theorem A.2. For a family of graphs G and a given J > 0, let D be the family of Ising models with
interaction strength J and zero external ﬁeld on graphs in G . Then

a. (Sensitivity to strength of correlations.) If G is the set of trees of maximum degree ∆ = d + 1 and

J = tanh−1(cid:0) 1−δ
Θ(ǫ/δ) for 0 < δ < ǫ ≪ 1. On the other hand, if J > tanh−1(cid:0) 1

d (cid:1) for some δ > 0, then every ν > 0 is differentially enforceable, and in fact ν(ǫ, D) =
d(cid:1) then the set of all differentially

enforceable ν has a strictly positive inﬁmum, νmin(J, ∆).

b. (Sensitivity to differential privacy parameter.) For any 0 < ǫ0 < ǫ1 and any 1 < r < R, there exists
a joint distribution µ whose inferential privacy guarantee satisﬁes ν/ǫ < r when ǫ = ǫ0 but ν/ǫ > R
when ǫ = ǫ1.

Part (b) is particularly striking because it implies, for instance, that when a policy-maker contemplates
whether to mandate differential privacy parameter ǫ = 0.19 or ǫ = 0.2, this seemingly inconsequential
decision could determine whether the inferential privacy guarantee will be ν = 0.2 or ν = 20.

§A.2.2 is devoted to proving the theorem. The proof combines known facts about phase transitions with

some calculations regarding magnetization of Ising models on a tree subjected to an external ﬁeld.

A.2.2 The Bethe lattice and the proof of Theorem A.2

The inﬁnite ∆-regular tree is known in mathematical physics as the Bethe lattice with coordination number
∆. Most of the results stated in Theorem A.2 can be derived by analyzing the Ising model on the Bethe lattice
and calculating the magnetization of the root when the lattice is subjected to an external ﬁeld. Throughout
this section, we will use the notation hσai to denote the expectation of the random variable σa under the
distribution deﬁned by the Ising model with interaction strength J at inverse temperature 1 and external
ﬁeld h.

16

Lemma A.3. For given J > 0, d ≥ 2, and h ∈ R, deﬁne a function y(x) by

eJ + e−J x(cid:19)d
y(x) = e2h(cid:18) eJ x + e−J

.

The sequence x0, x1, . . . deﬁned recursively by x0 = 1 and xn+1 = y(xn) for n ≥ 0 converges to a limit
point x = x(J, h). This limit point is determined as follows.

• If h = 0 then x(J, h) = 1.
• If h > 0 then x(J, h) is the unique solution of the equation x = y(x) in the interval (1,∞).
• If h < 0 then x(J, h) is the unique solution of the equation x = y(x) in the interval (0, 1).

The behavior of x(J, h) near h = 0 depends on the value of J.
continuously with h and limh→0 x(J, h) = 1. If tanh(J) > 1
h = 0, and it satisﬁes limhց0 x(J, h) > 1 and limhր0 x(J, h) < 1.

If tanh(J) < 1

d, then x(J, h) varies
d , then the function x(J, h) is discontinuous at

Proof. Rewriting the formula for y(x) as

y(x) = e2(h+Jd)(cid:20)1 −

eJ + e−J x(cid:21)d
eJ − e−3J

it is clear that for 0 ≤ x < ∞, y(x) is continuous and monotonically increasing in x and takes values
between e2(h−Jd) and e2(h+Jd). Since y is monotonic, the sequence x0, x1, x2, . . . deﬁned in the lemma
must be monotonic: if x0 ≤ x1 then an easy induction establishes that xn ≤ xn+1 for all n, and likewise if
x0 ≥ x1 then xn ≥ xn+1 for all n. Any monotonic sequence in a closed, bounded interval must converge to
a limit, so the limit point x(J, h) is well-deﬁned.
If h = 0 then a trivial calculation shows that xn = 1 for all n, and thus x(J, h) = 1. For h > 0
or h < 0 we must show that x(J, h) is the unique solution of y(x) = x in the interval (1,∞) or (0, 1),
respectively. First note that y(1) = e2Jh, so the sequence x0, x1, . . . is monotonically increasing when
h > 0 and decreasing when h < 0. Thus x = x(J, h) = limn→∞ xn belongs to (1,∞) when h > 0 and to
(0, 1) when h < 0. The continuity of y implies that

y(x) = lim
n→∞

y(xn) = lim
n→∞

xn+1 = x.

Thus, x satisﬁes x = y(x). It remains to show that this equation has a unique solution in (1,∞) when h > 0
and a unique solution in (0, 1) when h < 0.
A solution to x = y(x) is also a solution to ln x − ln y(x) = 0. The function g(x) = ln x − ln y(x) has

derivative

g′(x) =

1
x −

deJ

eJ x + e−J +

de−J

eJ + e−J x

=

1
x −

d(e2J − e−2J )

x2 + (e2J + e−2J )x + 1

The equation g′(x) = 0 is equivalent to the quadratic equation x2 − 2[d sinh(2J) − cosh(2J)]x + 1 = 0.
This has at most two real roots, and if it has any real roots at all then all roots are real and their product is
equal to 1. Therefore, it has at most one root in the interval (0, 1) and at most one root in the interval (1,∞).
Furthermore, g′(x) is strictly positive at x = 0 and as x → ∞. Summarizing this discussion, there exist
positive numbers x0 ≤ x1 such that x0 · x1 = 1 and the set {x | g′(x) > 0} intersects the intervals (0, 1)
and (1,∞) in the subintervals (0, x0) and (x1,∞), respectively.
Now suppose h > 0. The set {x | x > 1 and y(x) = x} is non-empty; for example, it contains
x(J, h). Let xinf denote the inﬁmum of this set. By continuity, y(xinf ) = xinf. Since g(xinf ) = 0 whereas

17

g(1) < 0, we must have g′(z) > 0 for some z in the interval (1, xinf ). Recalling the number x1 deﬁned in
the previous paragraph, we must have x1 < z < xinf. Consequently g′ is strictly positive throughout the
interval (xinf ,∞), implying that there are no other solutions of g(x) = 0 in that interval. Thus, xinf is the
unique solution of y(xinf ) = xinf in (1,∞), and x(J, h) = xinf. When h < 0 an analogous argument using
xsup = sup{x | x < 1 and y(x) = x} proves that x(J, h) = xsup is the unique solution of y(x) = x in the
interval (0, 1).
To analyze the behavior of x(J, h) near h = 0, it is useful to ﬁrst analyze the zero set of g′(x). Recall
that g′(x) = 0 if and only if x2 − 2[d sinh(2J)− cosh(2J)]x + 1 = 0. The discriminant test tells us that this
quadratic equation has zero, one, or two real roots according to whether d sinh(2J) − cosh(2J) − 1 is less
than, equal to, or greater than 0. Using the identities sinh(2J) = 2 sinh(J) cosh(J) = 2 cosh2(J) tanh(J)
and cosh(2J) = 2 cosh2(J) − 1 we ﬁnd that d sinh(2J) − cosh(2J) − 1 = 2 cosh2(J)[d tanh(J) − 1]. So
when tanh(J) > 1
d, g′(x) > 0 for all x and the equation y(x) = x has the unique solution x(J, h). Implicit
differentiation, applied to the equation x(J, h) = y(x(J, h)), yields:

∂x

∂h = ∂y

∂h + ∂y

∂x · ∂x

∂h

which can be rearranged to yield

The function y is C ∞ in the region x > 0, and at h = 0, x = 1 we have

∂x

∂h = ∂y/∂h

1−∂y/∂x .

∂y
∂x = d tanh(J)
∂y
∂h = 2
∂x
∂h =

1−d tanh(J) ,

2

(20)

(21)

(22)

so when tanh(J) < 1
function of h in a neighborhood of h = 0.

d the implicit function theorem implies that x(J, h) is a differentiable, increasing

When tanh(J) > 1

d and h = 0, we have g(1) = 0 and g′(1) < 0, so for some sufﬁciently small δ > 0
we have g(1 + δ) < 0, g(1 − δ) > 0. On the other hand, the fact that ln y(x) is bounded between −2Jd
and 2Jd implies that g(x) = ln x − ln y(x) tends to −∞ as x → 0 and to ∞ as x → ∞. The intermediate
value theorem implies that there exist x+ ∈ (1 + δ,∞) and x− ∈ (0, 1 − δ) such that g(x+) = g(x−) = 0.
In fact, the equation g(x) = 0 can have at most three solutions since g′(x) = 0 has only two solutions.
So, the entire solution set of g(x) = 0 is {x−, 1, x+}. Denote the function y(x) in the case h = 0 by
y0(x), to distinguish it from the case of general h; similarly deﬁne g0(x) = ln x − ln y0(x). Note that
g0(x) ≤ 0 when 1 ≤ x ≤ x+, so x ≤ y0(x) on that interval. When h > 0 we have y(x) > y0(x) for
all x, hence y(x) > x for 1 ≤ x ≤ x+. As x(J, h) is the unique solution of y(x) = x in the interval
(1,∞) it follows that x(J, h) > x+. On the other hand, for any δ > 0, we have g0(x+ + δ) > 0 and
hence, for sufﬁciently small h > 0, we also have g(x+ + δ) > 0. Since g(x+) < 0 and g(x(J, h)) = 0,
the intermediate value theorem implies x(J, h) belongs to the interval (x+, x+ + δ) for all sufﬁciently small
h > 0. In other words, limhց0 x(J, h) = x+. The analogous argument for h < 0 proves that x(J, h) < x−
and that limhր0 x(J, h) = x−.
Lemma A.4. If T is a subtree of T ′ and a is any node of T , let hσaiT and hσaiT ′ denote the expectation
of σa in the Ising models on T and T ′, respectively, with interaction strength J > 0. For h > 0 we have
hσaiT ′ ≥ hσaiT while for h < 0 we have hσaiT ′ ≤ hσaiT .
Proof. It sufﬁces to prove the lemma in the case that h > 0 (since the h < 0 case is symmetric under
exchanging the signs +1 and −1) and that T is obtained from T ′ by deleting a single leaf node, b. The
lemma then follows by induction, since any subtree can be obtained from a tree by successively deleting
leaves.

18

Let c denote the parent of b in T ′, i.e., assume that (b, c) is the unique edge of T ′ containing b. For state
σ ∈ {±1}V (T ), (σ, +) and (σ,−) denote the states in {±1}V (T ′) obtained by setting σb = +1 or σb = −1,
respectively, while keeping the spin at every node of T the same. If H(σ) = −JP(i,j)∈E(T ) σiσj is the
Hamiltonian of the Ising model on T , then the Hamiltonian of the Ising model on T ′ is given by

H ′(σ, +) = −Jσc + H(σ)
H ′(σ,−) = Jσc + H(σ).

Thus, the partition functions Z(β, h), Z ′(β, h) of T, T ′ respectively satisfy

eβ[h·σ−H(σ)]

Z(β, h) =Xσ
Z ′(β, h) =Xσ
= 2Xσ

eβ[h·(σ,+)−H ′(σ,+)] + eβ[h·(σ,−)−H ′(σ,−)]

cosh(β[h + Jσc]) eβ[h·σ−H(σ)].

Furthermore, we have

hσaiT =

hσaiT ′ =

=

1

Z(β, h)Xσ
Z ′(β, h)Xσ
Z ′(β, h)Xσ

2

1

σa eβ[h·σ−H(σ)]

σa eβ[h·(σ,+)−H ′(σ,+)] + eβ[h·(σ,−)−H ′(σ,−)]

σa cosh(β[h + Jσc]) eβ[h·σ−H(σ)].

Associating to each x ∈ {0, 1}n a state σ(x) ∈ {±1}n via σi = (−1)xi as before, we ﬁnd that the logarithm
of the function x 7→ eβ[h·σ(x)−H(σ(x))] is supermodular. Furthermore, the functions x 7→ (−1)xa and
x 7→ 2 cosh(β[h + (−1)xcJ]) are both monotonically decreasing. Thus, we may apply the FKG inequality
(Lemma 3.2) to conclude that

"Xσ

eβ[h·σ−H(σ)]# "2Xσ

σa cosh(β[h + Jσc])eβ[h·σ−H(σ)]#
cosh(β[h + Jσc]) eβ[h·σ−H(σ)]# "Xσ

≥"2Xσ

σaeβ[h·σ−H(σ)]# .
Dividing both sides by Z(β, h) · Z ′(β, h), we obtain the inequality asserted in the lemma.
Lemma A.5. If T is a ﬁnite tree of maximum degree ∆ = d + 1, a is any node of T , and hσai denotes the
expectation of σa in the Ising model on T with interaction strength J, inverse temperature 1, and external
ﬁeld h > 0, then

ln(cid:18) 1 + hσai

1 − hσai(cid:19) <

d + 1

d

ln x(J, h) −

2h
d

.

(23)

The difference between the left and right sides converges to zero as the distance from a to the nearest node
of degree less than ∆ tends to inﬁnity.

Proof. Deﬁne a sequence of rooted trees T0, T1, . . . recursively, by stating that T0 is a single node and Tn+1
consists of a root joined to d = ∆ − 1 children, each of whom is the root of a copy of Tn. Also deﬁne a

19

0 , T ∗

1 , . . ., by stating that T ∗

0 = T0 while for n > 0, T ∗

sequence of trees T ∗
children, each of whom is the root of a copy of Tn−1. (In other words, T ∗
to have ∆ instead of ∆ − 1 children.)
If T is any tree of maximum degree ∆ containing a node labeled a, then let r denote the distance from
a to the nearest node of degree less than ∆, and let s denote the distance from a to the farthest leaf. We can
embed T ∗
s with a at the root. Applying
Theorem A.4,

r as a subtree of T rooted at a, and we can embed T as a subtree of T ∗

n consists of a root joined to ∆
n is like Tn, with the root modiﬁed

hσaiT ∗

r ≤ hσaiT ≤ hσaiT ∗

s

.

To complete the proof we will show that ln(cid:16) 1+hσaiT ∗
n → ∞.

n
1−hσaiT ∗

For any tree T with root node k, let

n(cid:17) converges to d+1

d ln x(J, h) − 2h

d (from below) as

Z +(T ) = Xσ:σk=+1
Z −(T ) = Xσ:σk=−1

eJ Pi,j σiσj +h Pi σi

eJ Pi,j σiσj +h Pi σi

We have hσkiT = Z +(T )−Z −(T )
the quantity xn = Z +(Tn)/Z −(Tn) satisﬁes the recurrence

Z +(T )+Z −(T ) , so 1+hσkiT
1−hσkiT

= Z +(T )

Z −(T ) . For the tree Tn deﬁned in the preceding paragraph,

xn+1 =

eh(eJ Z +(Tn) + e−J Z −(Tn))d

eJ + e−J xn(cid:19)d
e−h(e−J Z +(Tn) + eJ Z −(Tn))d = e2h(cid:18)eJ xn + e−J
where the function y(·) is deﬁned as in Lemma A.3. Applying the conclusion of that lemma, we ﬁnd that xn
n )/Z −(T ∗
increases with n and xn → x(J, h) from below as n → ∞. Finally, for the quantity x∗
n )
we have

n = Z +(T ∗

= y(xn)

eh(eJ Z +(Tn−1) + e−J Z −(Tn−1))d+1

e−h(e−J Z +(Tn−1) + eJ Z −(Tn))d+1 = e2h(cid:18) eJ xn−1 + e−J

eJ + e−J xn−1(cid:19)d+1

= e−2h/dx(d+1)/d

n

.

x∗
n =

Finally,

ln(cid:18) 1 + hσai

1 − hσai(cid:19) = ln x∗

n = d+1

d ln(xn) − 2h
d .

The lemma follows because ln(xn) increases with n and converges to ln x(J, h) from below as n → ∞.
Corollary A.6. Let D denote the family of Ising models with interaction strength J and zero external ﬁeld
on trees of maximum degree ∆. For any ǫ > 0,

ν(ǫ, D) = ∆

∆−1 ln x(cid:0)J, ǫ

2(cid:1) − ǫ

∆−1 .

(24)

Proof. For the Ising model with zero external ﬁeld, the joint distribution µ is symmetric with respect to
ﬂipping each bit of the database x. This implies two simpliﬁcations in the formula for inferential privacy,
Equation (19). First, the odds ratio ρ = µ(xa=1)
µ(xa=0) is equal to 1. Second, both terms in the maximum on the

right-hand side of the equation are equal, so νa is equal to ln(cid:16) 1+hσai

the Gibbs measure (at inverse temperature 1) of the Ising model with interaction strength J and external
ﬁeld ǫ/2. Applying Lemma A.5 we obtain (24) as a direct consequence.

1−hσai(cid:17), where the h·i denotes averaging over

20

Proof of Theorem A.2. For part (a) of the theorem, Corollary A.6 justiﬁes focusing our attention on the
function ln x(J, h) where h = ǫ

2 and ǫ > 0 varies. In particular, when tanh(J) > 1

d , we have

lim
ǫց0

ν(ǫ, D) = ∆

∆−1 lim
hց0

ln x(J, h/2) > 0

by Lemma A.3. This implies that the set of differentially enforceable ν has a strictly positive inﬁmum, as
claimed in part (a) of Theorem A.2.

When tanh(J) = 1−δ

d , Eq. (22) for the partial derivative ∂x

∂h implies that ∂x

∂h = 2

δ at h = 0, x = 1. We

now ﬁnd that

dν(ǫ,D)

dǫ

(cid:12)(cid:12)(cid:12)ǫ=0

1

= ∆

∆−1(cid:17) ∂
=(cid:16) ∆
∆−1 ·
∆−1 · 1 · 1
=(cid:16) ∆
∆−1(cid:17) 1

∆−1(cid:17)
2(cid:1)(cid:3)ǫ=0 −(cid:16) 1
∂ǫ(cid:2)ln x(cid:0)J, ǫ
∆−1(cid:17)
∂h(cid:3)h=0 −(cid:16) 1
2 ·(cid:2) ∂x
x(J,0) · 1
∆−1(cid:17)
δ −(cid:16) 1
2 · 2
δ −(cid:16) 1
∆−1(cid:17) >

= ∆

1
δ

.

Thus, for sufﬁciently small ǫ > 0, we have ν(ǫ, D) > ǫ/δ, which completes the proof of part (a) of the
theorem.

To prove part (b) we consider rooted d-ary trees for some ﬁxed d ≥ 2. As in the proof of Lemma A.5 let
Tn denote the complete rooted d-ary tree of depth n, with root node denoted by a. For J > 0, h ∈ R deﬁne

wn(J, h) = ln(cid:18) 1 + hσai
1 − hσai(cid:19)

where hσai denotes the expectation of σa under the Ising model on Tn with interaction strength J and
external ﬁeld h. In the proof of Lemma A.5 we denoted exp(wn(J, h)) by xn and proved that xn → x(J, h)
from below as n → ∞.
Now consider an adversary whose prior µ is the Ising model with interaction strength J and external
ﬁeld h0. Note that the prior odds ratio ρ = µ(xa=1)

µ(xa=0) satisﬁes

ln ρ = ln(cid:18) µ(σa = −1)

µ(σa = 1) (cid:19) = ln(cid:18) 1 − hσai

1 + hσai(cid:19) = −wn(J, h0).

Substituting this into Eq. (19), we see that for a given differential privacy parameter ε the corresponding
inferential privacy guarantee is

ν(ε) = max{wn(J, h0 + ε

2 ) − wn(J, h0), wn(J, h0) − wn(J, h0 − ε

2 )}.

(25)

To prove Theorem A.2(b) consider any 0 < ε0 < ε1 and 1 < r < R. In setting up the adversary’s prior,
choose a value of h0 that satisﬁes 2h0 − ε1 < 0 < 2h0 − ε0. We aim to show that for all sufﬁciently large
J and all n > n0(J), we have ν(ε0) < r · ε0 but ν(ε1) > R · ε1.
Let w(J, h) = limn→∞ wn(J, h) = ln x(J, h). To prove that ν(ε0) < r · ε0 but ν(ε1) > R · ε1 for all
sufﬁciently large n, it is sufﬁcient to prove that

< r

< r

> R.

(26)

(27)

(28)

w(J,h0+ 1

2 ε0)−w(J,h0)
ε0

w(J,h0)−w(J,h0− 1

2 ε0)

ε0

w(J,h0)−w(J,h0− 1

2 ε1)

ε1

21

To prove (26)-(27) we will show that |∂w/∂h| is bounded above by 2r on the interval [h0 − 1
and apply the mean value theorem. Since w(J, h) = ln x(J, h) we have

2 ε0, h0 + 1

2 ε0]

∂w
∂h

=

∂x/∂h

x

=

(∂y/∂h)/x
1 − ∂y/∂x

=

2

1 − ∂y/∂x

,

(29)

where we have used Eq. (22) and the facts that ∂y/∂h = 2y and that y(x(J, h)) = x(J, h). Now, recalling
the deﬁnition of y(x) in Lemma A.3, we differentiate with respect to x and ﬁnd that

∂y
∂x

eJ + e−J x(cid:19)d−1
= e2h · d(cid:18) eJ x + e−J
·(cid:18) 1 − e−4J
(eJ + e−J x)2(cid:19)
(eJ x + e−J ) (eJ + e−J x)!
= y(x) · 
d ·(cid:0)1 − e−4J(cid:1)
y(x)
d
x ·
e2J .

<

(30)

Since y(x)/x = 1 when x = x(J, h), we may combine (29) with (30) to conclude that whenever J is large
enough that de−2J < 1 − 1
r , then the value of ∂y/∂x at x(J, h) is less than 1 − 1/r for all h > 0, and
consequently ∂w/∂h is bounded above uniformly by r, as desired.

Finally, to prove (28) we note that for x = e2h+Jd we have

y(x)

x

eJ + e−J x(cid:19)d
= e2h(cid:18) eJ x + e−J

e2h+Jd−J + eJ(cid:19)d
e−2h−Jd =(cid:18) e2h+Jd + e−2J

> 1

(31)

for J sufﬁciently large. Recalling from the proof of Lemma A.3 that for h > 0 we have y(x) > x when
1 < x < x(J, h) and y(x) < x when x > x(J, h), we see that x(J, h) > e2h+Jd provided that h > 0 and J
is sufﬁciently large. An analogous argument applying the h < 0 case of Lemma A.3 shows that x(J, h) <
e2h−Jd for h < 0 and J sufﬁciently large. Recalling that w(J, h) = ln x(J, h) and that h0 > 0 > h0 − ε1/2
we ﬁnd that

w(J, h0) > 2h0 + Jd > Jd

w(J, h0 − ε1/2) < 2h0 − ε1 − Jd < −Jd

w(J,h0)−w(J,h0− 1

2 ε1)

ε1

> 2Jd
ε1

> R

provided J is sufﬁciently large. This establishes (31) and concludes the proof of Theorem A.2(b).

B Appendix to §4: Bounded Afﬁliation Distributions

This appendix contains a full proof of Theorem 4.2. The proof requires developing a theory of “multiplica-
tive estimates” that is the multiplicative analogue of the notion of “estimate” used by Dobrushin [7], F¨ollmer
[12], and K¨unsch [23] in their proofs of the so-called Dobrushin Comparison Theorem. We deﬁne multi-
plicative estimates and build up the necessary machinery for dealing with them in §B.1. Then, in §B.2 we
prove Theorem 4.2.

B.1 Multiplicative estimates
Let S be any set of potential outcomes of the mechanism M such that Pr(M(x) ∈ S) > 0. Let π1 denote
the conditional distribution on databases x ∈ X n, given that M(x) ∈ S, and let π2 denote the unconditional
distribution µ, respectively.

22

For a ∈ {1, 2} and for any function f : X n → R, let πa(f ) denote the expected value of f under

distribution πa. Also deﬁne the Lipschitz constants

ρi(f ) = max{f (x) − f (x′) | x ∼i x′}.

(32)

Let us say that a vector κ = (κi) is a multiplicative estimate if for every function f : X n → R+ we have

| ln π1(f ) − ln π2(f )| ≤

κiρi(ln f ).

n

Xi=1

(33)

This section is devoted to proving some basic facts about multiplicative estimates that underpin the proof

of Theorem 4.2. To start, we need the following lemma.

Lemma B.1. Consider a probability space with two functions A, B taking values in the positive real num-
bers. If (sup A)/(inf A) ≤ e2a and (sup B)/(inf B) ≤ e2b then

E[AB]
E[A] E[B] ≤ 1 +

(e2a − 1)(e2b − 1)

(ea + eb)2

≤ eab.

(34)

Proof. The hypotheses and conclusion of the lemma are invariant under rescaling each of A and B, so we
may assume without loss of generality that A is supported in the interval [e−a, ea] and that B is supported
in the interval [e−b, eb]. At each sample point ω, the following two equations hold:

ea−e−a

h ea−A(ω)
h eb−B(ω)

ea−e−b

1

A(ω)−e−a

ea−e−a i (cid:20)1 e−a
ea−e−b i (cid:20)1 e−b

ea (cid:21) =(cid:2)1 A(ω)(cid:3)
eb (cid:21) =(cid:2)1 B(ω)(cid:3)

B(ω)−e−b

1

(35)

(36)

(37)

Therefore, if we deﬁne the matrix-valued random variable

we have

e−a

Integrating over ω we obtain

ea−e−a

eb−e−b

B(ω)−e−b

1

1

1

1

A(ω)−e−a

(cid:20) 1

eb (cid:21) =(cid:20) 1

eb−e−b i
A(ω) A(ω)B(ω)(cid:21) .

B(ω)

M (ω) =h ea−A(ω)
ea(cid:21) M (ω) (cid:20)1 e−b

ea−e−a i⊺ h eb−B(ω)
A(ω)(cid:21) (cid:2)1 B(ω)(cid:3) =(cid:20) 1
E[A] E[AB](cid:21)
ea(cid:21)−1 (cid:20) 1
1 (cid:21) (cid:20) 1
−1
=(cid:20) ea+b − eaE[B] − ebE[A] + E[AB]

eb (cid:21) =(cid:20) 1
E[M ] =(cid:20) 1
(cid:0)ea − e−a(cid:1)(cid:16)eb − e−b(cid:17) E[M ] =(cid:20) ea
−e−a

eb (cid:21)−1
E[A] E[AB](cid:21) (cid:20)1 e−b
E[A] E[AB](cid:21) (cid:20) eb −e−b
1 (cid:21)
−1
−ea−b + eaE[B] + e−bE[A] − E[AB]
−eb−a + e−aE[B] + ebE[A] − E[AB] e−a−b − e−aE[B] − e−bE[A] + E[AB](cid:21) .

ea(cid:21) E[M ] (cid:20)1 e−b

(cid:20) 1

e−a

E[B]

E[B]

1

e−a

E[B]

1

23

Each entry of the matrix on the left side is non-negative, hence the entries on the right side are non-negative
as well. This tells us that

E[AB] ≤ minneaE[B] + e−bE[A] − ea−b, ebE[A] + e−aE[B] − eb−ao

= E[A]E[B] + minn(ea − E[A])(E[B] − e−b), (eb − E[B])(E[A] − e−a)o .

Letting

α = 1
E[A] ,
we can multiply both sides of (38) by αβ to obtain

β = 1

E[B] ,

E[AB]
E[A] E[B] ≤ 1 + min{(eaα − 1)(1 − e−bβ), (ebβ − 1)(1 − e−aα)}.

(38)

(39)

Denote the right side of (39) by G(α, β). We aim to ﬁnd the maximum value of G(α, β) as (α, β) ranges
over the rectangle [e−a, ea]×[e−b, eb]. Note that G ≡ 1 on the boundary of this rectangle, whereas G > 1 on
the interior of the rectangle. At any point of the interior where (eaα− 1)(1− e−bβ) > (ebβ − 1)(1− e−aα)
we have ∂G
∂β = eb(1 − e−aα) > 0, and similarly at any point of the interior where (eaα − 1)(1 − e−bβ) <
(ebβ − 1)(1 − e−aα) we have ∂G
∂α > 0. Therefore if (α, β) is a global maximum of G we must have
(eaα − 1)(1 − e−bβ) = (ebβ − 1)(1 − e−aα). Let
eaα − 1
1 − e−aα

ebβ − 1
1 − e−bβ

(40)

r =

=

.

A manipulation using (40) yields

(eaα − 1)(1 − e−bβ) =(cid:0)e2a − 1(cid:1)(cid:16)e2b − 1(cid:17)

r

(r + e2a) (r + e2b)

(41)

and by setting the derivative of the right side to zero we ﬁnd that it is maximized at r = ea+b, when it
equates to (e2a−1)(e2b − 1)(ea + eb)−2. Therefore,

E[AB]

E[A] E[B] ≤ 1 + (cid:0)e2a − 1(cid:1)(cid:0)e2b − 1(cid:1)

(ea + eb)2

,

which establishes the ﬁrst inequality in (34). The prove the second inequality, we consider how 1 + (e2a −
1)(e2b − 1)(ea + eb)−2 varies as we vary a and b while holding their product ﬁxed at some value, x2. To
begin we compute the gradient of 1 + (e2a − 1)(e2b − 1)(ea + eb)−2.
∇h1 + (e2a − 1)(e2b − 1)(ea + eb)−2i =h(cid:16) 2e2a(e2b−1)
(ea+eb)2 − 2ea(e2a−1)(e2b−1)
2(ea+b − 1)
(ea + eb)3 (cid:2)ea(e2b − 1) eb(e2a − 1)(cid:3)
8(cid:0)1 − e−a−b(cid:1)

(cid:17) (cid:16) 2e2b(e2a−1)

(ea + eb)3

(ea+eb)3

=

=

(ea+eb)2 − 2eb(e2a−1)(e2b−1)

(ea+eb)3

(cid:17)i

(cid:2)sinh b

sinh a(cid:3)

Parameterizing the curve ab = x2 by a(t) = xt, b(t) = x/t, we have ˙a(t) = a/t and ˙b(t) = −b/t, so

d

dth1 + (e2a − 1)(e2b − 1)(ea + eb)−2i =

=

24

(ea + eb)3

8(cid:0)1 − e−a−b(cid:1)
(cid:2)sinh b
8ab(cid:0)1 − e−a−b(cid:1)
(ea + eb)3t (cid:18) sinh b
b −

sinh a(cid:3) (cid:20) a/t
−b/t(cid:21)
a (cid:19) .

sinh a

From the Taylor series sinh y
is an increasing function of y ≥ 0, so
along the curve a(t) = xt, b(t) = x/t, the function 1 + (e2a − 1)(e2b − 1)(ea + eb)−2 increases when a < b
(corresponding to t < 1) and decreases when a > b (corresponding to t > 1), reaching its maximum when
t = 1 and a = b = x. Hence

(2i+1)! y2i we see that sinh y

y = P∞

i=0

1

y

1 + (cid:0)e2a − 1(cid:1)(cid:0)e2b − 1(cid:1)

(ea + eb)2

2ex (cid:19)2
≤ 1 +(cid:18) e2x − 1

= 1 + sinh2 x = cosh2 x.

(42)

Finally, by comparing Taylor series coefﬁcients we can see that cosh x ≤ ex2/2 for all x ≥ 0, and squaring
both sides of this relation we obtain
(43)

= eab.

cosh2 x ≤ ex2

The second inequality in (34) follows by combining (42) with (43).
Lemma B.2. If κ is a multiplicative estimate, then for any i ∈ {1, . . . , n}, the vector Ti(κ) deﬁned by

is also a multiplicative estimate.

(Ti(κ))ℓ =(κℓ

ǫi

j=1 γijκj

2 +Pn

if ℓ 6= i
if ℓ = i

(44)

Proof. For a distribution π on X n, a database x ∈ X n, and an individual i, let π(· | x−i) denote the
conditional distribution of xi given x−i. In other words, π(· | x−i) is the probability distribution on X
given by

π(x | x−i) =

π(x, x−i)

Py∈X π(y, x−i)

.

(45)

Letting W denote the vector space of real-valued functions on X n, we deﬁne an averaging operator τi :
W → W which maps a function f to the function
τif (x) = Xy∈X

f (y, x−i) π(y | x−i).

Equivalently, τif is the unique function satisfying:

1. The value τif (x) depends only on x−i.

2. For any other function g whose value g(x) depends only on x−i, we have

π(f g) = π((τif )g).

(46)

Note that π(f ) = π(τif ), as can be seen from applying (46) to the constant function g(x) = 1.

For the distributions π1 and π2 deﬁned earlier, let us denote the corresponding averaging operators by

τ 1
i and τ 2

i . Using the identities π1(f ) = π1(τ 1
| ln π1(f ) − ln π2(f )| ≤ | ln π1(τ 1

i f ) and π2(f ) = π2(τ 2
i f ) − ln π1(τ 2

i f ), we ﬁnd that

i f )| + | ln π1(τ 2

i f ) − ln π2(τ 2

i f )|.

(47)

We bound the two terms on the right side separately. For the ﬁrst term, we write

i f )

π1(τ 2

ln(cid:18) π1(τ 1
i f )(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)

ln(cid:18)Px π1(x)τ 1
=(cid:12)(cid:12)(cid:12)(cid:12)
Px π1(x)τ 2

i f (x)

i f (x)(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)

25

≤ max

i f (x)
τ 2

ln(cid:18) τ 1
x (cid:12)(cid:12)(cid:12)(cid:12)
i f (x)(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)

.

(48)

For any particular x, we can bound the ratio τ 1
i f (x)
from above using Lemma B.1 applied to the probability
τ 2
i f (x)
space X, under the distribution π2(· | x−i). Letting A(x) = f (x, x−i), B(x) = π1(x | x−i)/π2(x | x−i)
we have

E[A(x)B(x)] = τ 1
E[A(x)] = τ 2
E[B(x)] = 1

i f (x)
i f (x)

sup A

inf A ≤ eρi(ln f )
inf B ≤ e2ǫi.

sup B

(49)
(50)
(51)

(52)

(53)

The ﬁrst four of these relations are straightforward, and the last requires some justiﬁcation. In the following
calculation we use the operator Pr(·) to denote probabilities of events in the sample space where x is
sampled from the original joint distribution µ, and randomized mechanism M is applied to x. Starting from
the deﬁnitions of π1 and π2, an application of Bayes’ Law yields the following calculation.

π1(x | x−i) =

=

Pr(x = (x, x−i) | M(x) ∈ S)
Pr(x ∈ X × {x−i} | M(x) ∈ S)
Pr(M(x) ∈ S | x = (x, x−i))
Pr(M(x) ∈ S | x ∈ X × {x−i}) ·

Pr(x = (x, x−i))
Pr(x ∈ X × {x−i})

.

The ﬁrst factor on the right-hand side is between e−ǫi and eǫi, while the second factor is equal to π2(x | x−i).
This completes the proof of (53). By combining Lemma B.1 with the contents of (48)-(53) we obtain the
bound

To bound the second term in (47), we will make use of the following inequality, a multiplicative analogue

| ln π1(τ 1

i f ) − ln π1(τ 2

i f )| ≤ 1

2 ǫiρi(ln f ).

(54)

of inequality (3.5) in [16].

∀i, j ρj(ln τ 2

i f ) ≤(0

ρj(ln f ) + γij ρi(ln f )

if i = j
if i 6= j.

(55)

The validity of (55) is evident when i = j, since the value τ 2
when i 6= j, we use the deﬁnition of ρj(·) to choose x, x′ ∈ X n such that x ∼j x′ and

i f (x) does not depend on xi. To prove (55)

i f (x)

ρj(ln τ 2

−i)f (x, x′

i f ) = ln τ 2

i f (x′) − ln τ 2
= ln Xx∈X
π2(x | x′
ln(cid:18)Px∈X π2(x | x′
≤(cid:12)(cid:12)(cid:12)(cid:12)
Px∈X π2(x | x′
Using the fact that f (x,x
f (x,x−i) ≤ eρj (ln f ) for all x ∈ X, we see that the ﬁrst term on the right side of (56) is
bounded above by ρj(ln f ). To bound the second term, we again make use of Lemma B.1, this time sub-
stituting A(x) = f (x, x−i) and B(x) = π2(x|x
π2(x|x−i) . Taking expectations under the probability distribution

π2(x | x−i)f (x, x−i)!
ln(cid:18)Px∈X π2(x | x′
Px∈X π2(x | x−i)f (x, x−i)(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)

−i)f (x, x−i)

−i)f (x, x′

−i)! − ln Xx∈X
+(cid:12)(cid:12)(cid:12)(cid:12)

−i)f (x, x−i)(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)

−i)

(56)

′

−i)

′

−i)

.

26

π2(x | x−i) we have

−i)f (x, x−i)

π2(x | x′
π2(x | x−i)f (x, x−i)

E[A(x)B(x)] = Xx∈X
E[A(x)] = Xx∈X
inf A ≤ eρi(ln f )
inf B ≤ e4γij ,

E[B(x)] = 1

sup B

sup A

where the last line is justiﬁed by observing that the deﬁnition of γij ensures that sup B ≤ e2γij and inf B ≥
e−2γij . An application of Lemma B.1 immediately implies that the second term on the right side of (56) is
bounded above by γijρi(ln f ). This completes the proof of (55).

Now, our hypothesis that κ is a multiplicative estimate implies, by deﬁnition, that

| ln π1(τ 2

i f ) − ln π2(τ 2

i f )| ≤

n

Xj=1

κjρj(ln τ 2

i f ) ≤Xj6=i

κjρj(ln f ) +

n

Xj=1

κjγij ρi(ln f )

(57)

where we have applied (55) to derive the second inequality. Combining (47), (54), and (57) we now have

| ln π1(f ) − ln π2(f )| ≤Xj6=i

κjρj(ln f ) +


1
2 ǫi +

n

Xj=1

γijκj

 ρi(ln f ).

n(cid:1) κ + 1

n Γκ is also a multiplicative estimate.

Lemma B.3. If κ is a multiplicative estimate, then for any i ∈ {1, . . . , n}, the vector T (κ) = 1
(cid:0)1 − 1

Proof. Averaging (44) over i = 1, . . . , n, we ﬁnd that T (κ) = 1
n Ti(κ). The lemma follows because the
set of multiplicative estimates is closed under convex combinations, as is evident from the deﬁnition of a
multiplicative estimate.

ǫ +

2n

Lemma B.4. If the inﬂuence matrix Γ has spectral norm strictly less than 1, then 1

2 Φǫ is an estimate.

Proof. To begin, let us prove that the set of multiplicative estimates is non-empty; in fact, it contains the
vector 1 = (1, . . . , 1). To see this, consider any f : X n = R+ and choose x ∈ arg max(f ), x′ ∈
arg min(f ). Deﬁne a sequence (x(k))n

k=0 by the formula

x

(k)

i =(xi

x′
i

if i > k
if i ≤ k

.

Note that x(0) = x, x(n) = x′, and x(k−1) ∼k x(k) for k = 1, . . . , n. Therefore,

| ln π1(f ) − ln π2(f )| ≤ | max(ln f ) − min(ln f )| = | ln f (x) − ln f (x′)|

n

so 1 is an estimate as claimed.

| ln f (x(k−1)) − ln f (x(k))| ≤

≤

Xk=1

27

ρk(ln f ),

n

Xk=1

Now let Υ =(cid:0)1 − 1

n(cid:1) I + 1

n Γ. Applying Theorem B.3 inductively, each element of the sequence

T m(1) =

1

2n m−1
Xk=0

Υk! ǫ + Υm1

is a multiplicative estimate. If kΓk < 1 (where k · k denotes spectral norm) then Υ also has spectral norm
less than 1 because it is a convex combination of Γ and I. This implies that the sequence (T m(κ))∞
converges to 1

m=0

2n (I − Υ)−1ǫ. Now,

(I − Υ)−1 = ( 1

n I − 1

n Γ)−1 = n(I − Γ)−1,

so the sequence (T m(κ))∞
multiplicative estimates is again a multiplicative estimate.

m=0 converges to 1

2 Φǫ. The proof concludes with the observation that a limit of

B.2 Proof of Theorem 4.2

Let us begin by restating Theorem 4.2.

Theorem B.5. Suppose that the joint distribution x has a multiplicative inﬂuence matrix Γ whose spectral
norm is strictly less than 1. Let Φ = (φij ) denote the matrix inverse of I − Γ. Then for any mechanism with
individual privacy parameters ǫ = (ǫi), the networked differential privacy guarantee satisﬁes

If the matrix of multiplicative inﬂuences satisﬁes

∀i νi ≤ 2

φijǫj.

n

Xj=1

for some δ > 0, then

∀i

n

Xj=1

γijǫj ≤ (1 − δ)ǫi

∀i νi ≤ 2ǫi/δ.

(58)

(59)

(60)

Proof. Above, in Lemma B.4, we proved that 1
f : X n → R+ it holds that

2 Φǫ is a multiplicative estimate. In other words, for any

(cid:12)(cid:12)ln π1(f ) − ln π2(f )(cid:12)(cid:12) ≤ 1

2

Φijǫjρi(ln f ).

(61)

n

Xi,j=1

To prove (58), we are required to show the following: if z0, z1 are any two distinct elements of X such that
Pr(xi = z0) and Pr(xi = z1) are both positive, then

(cid:12)(cid:12)(cid:12)(cid:12)
ln(cid:18) Pr(xi = z1 | M(x) ∈ S) / Pr(xi = z0 | M(x) ∈ S)

Pr(xi = z1) / Pr(xi = z0)

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)

≤ 2

n

Xj=1

Φijǫj.

(62)

We will do this by setting f and g to be the indicator functions of the events xi = z0 and xi = z1,
respectively. Then (62) can be rewritten in the form

ln(cid:18) π1(f )/π1(g)
(cid:12)(cid:12)(cid:12)(cid:12)
π2(f )/π2(g)(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)

28

≤ 2

n

Xj=1

Φijǫj.

(63)

If the Lipschitz constants of f and g satisﬁed ρj(ln f ) = ρj(ln g) = 0 for j 6= i and ρi(ln f ), ρi(ln g) ≤ 1,
then (63) would follow immediately by applying (61) to f and g separately. Instead ρi(f ) = ρi(g) = ∞ so
we will have to be more indirect, applying (61) to τ f and τ g where τ is an averaging operator designed to
smooth out f and g, thereby improving their Lipschitz constants. Speciﬁcally, deﬁne

τ f (x) = π2(z0, x−i),

τ g(x) = π2(z1, x−i).

It is useful to describe τ f and τ g in terms of the following sampling process: generate a coupled pair of
samples (x′, x′′) by sampling x′ from π2, then resampling x′′
−i),
and then assembling the database x′′ = (x′′
i = z0
given that x′ = x, and τ g is deﬁned similarly using z1 instead of z0. An important observation is that
the distribution of (x′, x′′) is exchangeable, i.e. (x′, x′′) and (x′′, x′) have the same probability. From this
observation we can immediately conclude that

i from the conditional distribution π2(· | x′

−i). Then τ f (x) is the conditional probability that x′′

i , x′

π2(τ f ) = π2(f ),

π2(τ g) = π2(g),

(64)

because π2(τ f ) is the probability that x′′
for g and z1. Our strategy for proving (63) will be to bound the left side using

i = z0 whereas π2(f ) is the probability that x′

i = z0, and similarly

ln(cid:18) π1(f )/π1(g)
(cid:12)(cid:12)(cid:12)(cid:12)
π2(f )/π2(g)(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)

ln(cid:18) π1(f )/π1(g)
π2(τ f )/π2(τ g)(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)
=(cid:12)(cid:12)(cid:12)(cid:12)
ln(cid:18) π1(f )/π1(g)
π1(τ f )/π1(τ g)(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)
≤(cid:12)(cid:12)(cid:12)(cid:12)

+(cid:12)(cid:12)(cid:12)(cid:12)

ln(cid:18) π1(τ f )/π1(τ g)
π2(τ f )/π2(τ g)(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)

(65)

and to bound the two terms on the last line separately. For the second term we will use (61) applied to τ f
and τ g separately. This requires us to bound the Lipschitz constants ρk(ln τ f ) and ρk(ln τ g). Since τ f (x)
and τ g(x) do not depend on xi, it is immediate that ρf (ln τ f ) = ρi(ln τ g) = 0. For k 6= i, the deﬁnition of
the multiplicative inﬂuence parameter γik leads to the bounds

Note that (66) also holds when k = i since γii = 0. Applying (61) to τ f and τ g yields the bound

ρk(ln τ f ), ρk(ln τ g) ≤ 2γik.

ln(cid:18) π1(τ f )/π1(τ g)
(cid:12)(cid:12)(cid:12)(cid:12)
π2(τ f )/π2(τ g)(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)

≤(cid:12)(cid:12)ln π1(τ f ) − ln π2(τ f )(cid:12)(cid:12) +(cid:12)(cid:12)ln π1(τ g) − ln π2(τ g)(cid:12)(cid:12)

n

(2γik)Φkjǫj = 2(ΓΦǫ)i

≤ 2 ·

1
2 ·

Xk,j=1

(66)

(67)

Recalling that Φ = (I − Γ)−1, we have (I − Γ)Φ = I and hence ΓΦ = Φ− I. Thus, we can rewrite (67) as

ln(cid:18) π1(τ f )/π1(τ g)
(cid:12)(cid:12)(cid:12)(cid:12)
π2(τ f )/π2(τ g)(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)

≤ 2

n

Xj=1

Φijǫj − 2ǫi.

(68)

Now we turn to bounding the ﬁrst term in (65). Letting o denote the random variable representing the
mechanism’s outcome, M(x). Bayes’ Law tells us that

π1(x) =

π2(x) Pr(o ∈ S | x)

Pr(o ∈ S)

.

29

Therefore,

π1(τ g)
π1(g)

= Px π2(z1 | x−i)π1(x)

Px g(x)π1(x)

Similarly,

Combining (69) with (70) yields the bound

The right side lies between e−ǫi and eǫi because each ratio Pr(o∈S|(z,x−i))

Pr(o∈S|(z1,x−i)) lies between e−ǫi and eǫi. Thus,

.

= Px π2(z1 | x−i)π2(x) Pr(o ∈ S | x)
Px g(x)π2(x) Pr(o ∈ S | x)
= Px π2(z1 | x−i)π2(x) Pr(o ∈ S | x)
π2(z1, x−i) Pr(o ∈ S | (z1, x−i))
Px−i
π2(z1 | x−i)Pz∈X π2(z, x−i) Pr(o ∈ S | (z, x−i))
= Px−i
π2(z1 | x−i)Pz∈X π2(z, x−i) Pr(o ∈ S | (z1, x−i))
Px−i
ln(cid:18) π1(τ g)
(cid:12)(cid:12)(cid:12)(cid:12)
π1(g) (cid:19)(cid:12)(cid:12)(cid:12)(cid:12)
ln(cid:18) π1(f )
π1(τ f )(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)
ln(cid:18) π1(f )/π1(g)
(cid:12)(cid:12)(cid:12)(cid:12)
π1(τ f )/π1(τ g)(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)

≤ 2ǫi.

≤ ǫi.

≤ ǫi

(69)

(70)

(71)

Combining (71) with (68) we obtain the bound (63), which ﬁnishes the proof of the ﬁrst inequality in the
theorem statement, namely (58).

To prove inequality (60), we use the partial ordering on vectors deﬁned by a (cid:22) b if and only if ai ≤ bi
for all i. The matrix Γ has non-negative entries, so it preserves this ordering: if a (cid:22) b then ∀i Pj γijaj ≤
Pj γijbj and hence Γa (cid:22) Γb. Rewriting the relation (59) in the form Γǫ (cid:22) (1− δ)ǫ and applying induction,
we ﬁnd that for all n ≥ 0, Γnǫ (cid:22) (1 − δ)nǫ. Summing over n yields

Φǫ =

∞

Xn=0

∞

Γnǫ (cid:22)

Xn=0
(1 − δ)nǫ = 1

δ

ǫ

which, when combined with (58), yields (60).

30

