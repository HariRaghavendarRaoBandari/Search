6
1
0
2

 
r
a

 

M
6
1

 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
6
9
2
5
0

.

3
0
6
1
:
v
i
X
r
a

Clustering of Sparse and Approximately Sparse Graphs by

Semideﬁnite Programming

Aleksis Pirinen ∗

Brendan P.W. Ames †

March 18, 2016

Abstract

As a model problem for clustering, we consider the densest k-disjoint-clique problem of
partitioning a weighted complete graph into k disjoint subgraphs such that the sum of the
densities of these subgraphs is maximized. We establish that such subgraphs can be recovered
from the solution of a particular semideﬁnite relaxation with high probability if the input graph
is sampled from a distribution of clusterable graphs. Speciﬁcally, the semideﬁnite relaxation is
exact if the graph consists of k large disjoint subgraphs, corresponding to clusters, with weight
concentrated within these subgraphs, plus a moderate number of outliers. Further, we establish
that if noise is weakly obscuring these clusters, i.e, the between-cluster edges are assigned very
small weights, then we can recover signiﬁcantly smaller clusters. For example, we show that in
approximately sparse graphs, where the between-cluster weights tend to zero as the size n of the
graph tends to inﬁnity, we can recover clusters of size polylogarithmic in n. Empirical evidence
from numerical simulations is also provided to support these theoretical phase transitions to
perfect recovery of the cluster structure.

1

Introduction

Clustering is a fundamental problem in machine learning and statistics, focusing on the identiﬁcation
and classiﬁcation of groups, called clusters, of similar items in a given data set. Clustering is
ubiquitous, playing a prominent role in varied ﬁelds such as computational biology, information
retrieval, pattern recognition, image processing and computer vision, and network analysis. This
problem is inherently ill-posed, as the partition or clustering of any given data set will depend
heavily on how we quantify similarity between items in the data set and how we characterize
clusters; it is not outside the realm of possibility to have two drastically diﬀerent clusterings of the
same data if two diﬀerent similarity metrics are used in the clustering process. Regardless of the

∗Centre for Mathematical Sciences, Lund University, Lund, Sweden, aleksis@maths.lth.se
†Department of Mathematics, The University of Alabama, Tuscaloosa, Alabama, AL 35487-0350, USA,

bpames@ua.edu

1

similarity metric used, clustering is a combinatorial optimization problem at its core: given data,
identify a partition or labeling of the data (approximately) maximizing some measure of quality of
the clustering. Due to the diﬃculties inherent with optimization over discrete sets, many popular
approaches for clustering involve the approximate solution of an NP-hard combinatorial optimization
problem; for example, the spectral clustering heuristic for the normalized cut problem [13, 23], the
convex relaxation approaches for the correlation clustering problem [21], robust principal component
analysis [10, 24], and the densest k-disjoint-clique problem [3, 6], among many others.

In spite of the inherent intractability of clustering, many recent analyses have established that
if data is sampled from some distribution of clusterable data, then one can eﬃciently recover the
underlying cluster structure using a variety of clustering heuristics; in particular, see [1–3, 6, 7, 9–
12, 14, 15, 18, 21, 22, 24, 26, 27, 30]. In particular, most of these results assume that the similarity
structure of the data can be modeled as a graph sampled from some generalization of the stochastic
block model [17]. In this model, the nodes of the graph, called the similarity graph of the data, are
associated with the items in the data set. An edge is drawn between two items with probability p if
the corresponding items belong to the same cluster, and with probability q < p if the corresponding
items belong to diﬀerent clusters. Under this block model, it can be shown that the block structure
of the data can be recovered with high probability provided that the smallest cluster in the data is
√
suﬃciently large, typically larger than ˜c
n, where n denotes the number of items in the data (and
nodes in the similarity graph) and ˜c is a polylogarithmic factor in n depending on p − q.

Although valuable in establishing suﬃcient conditions for data to be clusterable, these results
are not immediately applicable to data sets seen in many applications, particularly those arising
from the analysis of social networks. For example, statistical analysis of social networks suggests
that communities, playing the role of clusters, tend to be limited in size to several hundred users,
while the networks themselves can contain thousands, if not millions, of users [19, 20]. However,
several recent analyses [10, 12, 14, 28] suggest that these clusterability results are overly conservative
with respect to the size of clusters we can expect to recover. In particular, these results focus on the
clusterability of data that can be represented as a sparse graph. Speciﬁcally, the data is assumed to
be sampled from a generalized stochastic block model where the parameters p and q governing edge
formation are functions depending on the number of items n and one or both tends to 0 as n → ∞.
In the case where p tends to 0 much more slowly than q, the noise obscuring the block structure is
signiﬁcantly weaker than in the dense graph case (where p and q are assumed ﬁxed). In this case, it
has been shown that clusters signiﬁcantly smaller than
n can be recovered eﬃciently; speciﬁcally,
several heuristics have been shown to recover clusters with size polylogarithmic in n under certain
assumptions on the probability functions p and q (see [10, 12, 14, 28]).

√

The primary contribution of this paper is an analysis establishing similar clusterability results for
a particular convex relaxation of the clustering problem. That is, we present an analysis establishing
the following theorem, which provides conditions for perfect recovery of the underlying cluster
structure from the solution of a particular semideﬁnite program. As an immediate corollary, the
theorem establishes that one may identify clusters as small as ω(log2(n)), i.e., for every constant

2

c, the size of the smallest cluster is bounded below by c log2(n) for suﬃciently large n, with high
probability if the data is sampled from the sparse block model described above.

Theorem 1.1 Suppose that the n-node graph G = (V, E) is sampled from the generalized stochastic
block model, with k disjoint blocks, in-cluster edge probability p, and between-cluster edge probability
q. Let A ∈ Rn×n denote the adjacency matrix of G and let ˆr and ˜r denote the cardinality of the
smallest and largest clusters, respectively, in the block model for G. Then the columns of the optimal
solution X∗ of the semideﬁnite program

{Tr(AX) : Xe ≤ e, Tr(X) = k, X ≥ 0}

max
X∈Σn
+

are scalar multiples of the characteristic vectors of the clusters in our underlying block model with
probability at least

(cid:17)

√

ˆr

(cid:16)

1 − O

(cid:16)−2
(cid:111)
(cid:110)(cid:112)q(1 − q)n log n, log n

k2˜r exp

,

+ ˆr−5(cid:17)
(cid:110)(cid:112)p(1 − p)˜r log ˜r, log ˜r
(cid:111)

+ 6 max

+ 9ˆr3/4.

provided that

(p − q)ˆr ≥ 12 max

Moreover, in this case, every characteristic vector of a cluster in the block model is a scalar multiple
of at least one column of X∗.

Here, the characteristic vector of a set S ⊆ {1, 2, . . . , n} is the vector x ∈ {0, 1}n with ith

element

xi =

(cid:40)

if i ∈ S

1,
0, otherwise.

In Theorem 1.1, Tr(X) denotes the trace of the matrix X, e denotes the all-ones vector of appropriate
dimension, the notation X ≥ 0 indicates that the entries of X are nonnegative, and Σn
+ denotes
the cone of n × n symmetric positive semideﬁnite matrices.

Note that if G is sampled from the dense block model, i.e., p, q are independent of n, then
√
Theorem 1.1 suggests that we have exact recovery if ˆr ≥ c
n log n with high probability, where c is
a constant depending on p, q; this bound matches that established by [3] (among many others) up
to constant and logarithmic terms. On the other hand, when G is sampled from the sparse block
model, we see that Theorem 1.1 suggests that we may have perfect recovery of signiﬁcantly smaller
clusters. For example, suppose that p = 1 is ﬁxed and q = log(n)/n. Then we have exact recovery
with high probability if all clusters have size equal to ˆr = ω(log2(n)); see the discussion following
Theorem 2.2.

We will show that analogous phenomena occur in what we will call approximately sparse graphs.
In many practical applications, the expectation that we have a binary labeling indicating whether
any pair of items in a given data set are similar or dissimilar is unrealistic. However, it is often
possible to describe the level of similarity between any two items using some aﬃnity function based

3

on distance between the items in question. For example, we could consider the discrepancy in
pixel intensity and geographic location in image segmentation applications or Euclidean distance
between two items represented as vectors in a Euclidean space (or some other vector space with
corresponding norm). In this case, we can summarize the pairwise similarity relationships within
our data using a weighted graph, called a weighted similarity graph. Speciﬁcally, given a data set
with aﬃnity function f , the weighted similarity graph is the weighted complete graph with nodes
corresponding to the items in the data set, and edge weight wij between nodes i and j given by the
value of f (i, j). Clearly, this contains the similarity graphs discussed earlier as a special case where
wij = 1 if items i and j are known to be similar and wij = 0 otherwise; note that we assume that
we have an undirected graph with symmetric adjacency matrix.

We can generalize the stochastic block model in an identical fashion. That is, we assume that
items in the same cluster are signiﬁcantly more similar than pairs of items in diﬀerent clusters. This
corresponds to edge weights within clusters being larger, on average, than edge weights between
clusters. This motivates the following random graph model, which we will call the planted cluster
model. Let G = (V, W ) be the weighted complete graph whose node set represents the items in
some data set containing k clusters and (potentially) some outlier nodes. For each pair of nodes
i, j in the same cluster, we random sample edge weight wij ≥ 0, and wji by symmetry, from some
probability distribution Ω1 with mean α > 0. If i, j do not belong to the same cluster, we sample
wij = wji ≥ 0 from a second probability distribution Ω2 with mean β ∈ [0, α). Note that this model
contains the generalized stochastic block model discussed earlier as a special case when Ω1 and Ω2
are Bernoulli distributions with probabilities of success p and q, respectively.

It was shown by [3] that if G = (V, W ) is sampled from the planted cluster model with minimum
n, where C is a constant depending on α and β, then we can recover the

√

cluster size at least C
clusters from the optimal solution of the semideﬁnite program

(cid:110)

max
X∈Σn
+

Tr(W X) : Xe ≤ e, Tr(X) = k, X ≥ 0

(cid:111)

(1)

with high probability, where k is the number of clusters in the graph. We will show that these
results can be strengthened to establish that much smaller clusters can be recovered in the presence
of approximately sparse noise. That is, we will see that if the between-cluster edge weights have
expectation β approaching zero as n → ∞ suﬃciently quickly, then we may recover clusters
containing as few as ω(log2(n)) nodes with high probability. We will derive the semideﬁnite program
(1) as a relaxation of a particular model problem for clustering in Section 2.1 and formally state our
recovery guarantees in Section 2.2; we will see that these results immediately specialize to those
stated in Theorem 1.1 for the semideﬁnite program (1.1).

4

2 Semideﬁnite Relaxations of the Densest k-disjoint Clique Prob-

lem

In this section, we derive a semideﬁnite relaxation for the densest k-disjoint clique problem and
present an analysis illustrating a suﬃcient condition ensuring that this relaxation is exact. This
problem will act as a model problem for clustering and we will see that we should expect to accurately
recover the underlying cluster structure is the given data satisﬁes this suﬃcient condition.

2.1 The Densest k-disjoint Clique Problem

We begin by deriving a heuristic for the clustering problem based on semideﬁnite relaxation of
the densest disjoint clique problem. A similar discussion motivating the relaxation was originally
presented by [3]; we repeat it here for completeness. Let Kn = (V, W ) be a weighted complete
graph with vertex set V = {1, 2, . . . , n} and nonnegative edge weights wij ∈ [0, 1] for all i, j ∈ V .
Given a subgraph H of Kn, the density dH of H is the average edge weight incident at a vertex in
H:

(cid:88)

wij
|V (H)| .

dH =

ij∈E(H)

If we assume that Kn is the similarity graph of some data set consisting of k disjoint clusters and
that weight is concentrated more heavily on within-cluster edges than between-cluster edges, then we
may cluster this data set by ﬁnding the set of k disjoint subgraphs, corresponding to these clusters,
with maximum density; we call this problem the densest k-partition problem. Unfortunately, the
densest k-partition problem is NP-hard (see [25]). Moreover, this partition model excludes the case
where some items, called outliers, in the data set do not naturally associate with any of the clusters
in the data. To simultaneously motivate a convex relaxation of the densest k-partition problem and
address the inclusion of outliers, we consider the densest k-disjoint clique problem.

Given a graph G = (V, E), a clique of G is a pairwise adjacent subset of V . That is, C ⊆ V is a
clique of G if ij ∈ E for every pair of nodes i, j ∈ C or, equivalently, the subgraph G(C) induced by
C is complete. We say that H is a k-disjoint-clique subgraph of Kn if V (H) consists of k disjoint
cliques, i.e., H is the union of k disjoint complete subgraphs of Kn. The densest k-disjoint-clique
problem seeks a k-disjoint-clique subgraph H∗ maximizing the sum of the densities of the disjoint
complete subgraphs comprising H∗. Note that if we add the additional constraint that each node
in Kn belongs to exactly one k-disjoint-clique subgraph in Kn, then the densest k-disjoint-clique
problem becomes the densest k-partition problem. However, in general, the densest k-disjoint-clique
problem allows an assignment of nodes to clusters, represented by the disjoint cliques, which excludes
some nodes. For example, if outliers are present in the data, they would not be assigned to a cluster
by the optimal k-disjoint-clique subgraph.

The complexity of the densest k-disjoint-clique problem is unknown; in particular, no polynomial
time algorithm for its solution is known. To address this potential intractability, we will attempt to

5

approximately solve the k-disjoint-clique problem by convex relaxation. Suppose that v1, . . . , vk
are the characteristic vectors of a set of disjoint cliques C1, C2, . . . , Ck forming a k-disjoint-clique
subgraph of Kn. Using this notation, the density of the complete subgraph induced by Ci is equal to

(cid:88)

u,v∈Ci

dG(Ci) =

wuv
|Ci| =

vT
i W vi
vT
i vi

.

If we let P be the n × k matrix with ith column equal to vi/(cid:107)vi(cid:107), where (cid:107) · (cid:107) = (cid:107) · (cid:107)2 denotes the
standard Euclidean norm, then it is easy to see that

k(cid:88)

dG(Ci) = Tr(P T W P ).

i=1

We call such a matrix P a normalized k-cluster matrix and denote the set of normalized k-cluster
matrices of the vertex set V by ncm(V, k). It follows that the densest k-disjoint-clique problem may
be formulated as

max(cid:8)Tr(P T W P ) : X ∈ ncm(V, k)(cid:9) .

(2)

Again, the complexity of (2) is unknown, however, the maximization of quadratic functions subject
to combinatorial constraints is known to be NP-hard.

A process for relaxation of (2) using matrix lifting is described by [3]; a similar relaxation
technique was applied by [4–6]. In particular, each proposed cluster Ci, with characteristic vector
vi, corresponds to the rank-one symmetric matrix

X (i) =

vivT
i
vT
i vi

.

It is easy to see that the density of G(Ci) is equal to

dG(Ci) =

vT
i W vi
vT
i vi

= Tr(W X (i)).

Moreover, each of the matrices X (i) has row and column sums equal to either 0 or 1, and trace
equal to 1. Finally, for each proposed clustering C1, . . . , Ck, the corresponding rank-one matrices
are orthogonal in the trace inner product, due to the orthogonality of the characteristic vectors of
the corresponding disjoint clusters. Thus, the matrix

k(cid:88)

k(cid:88)

X =

X (i) =

i=1

i=1

vivT
i
vT
i vi

(3)

has rank equal to k. This suggests that we may relax (2) as the rank-constrained semideﬁnite
program

{Tr(W X) : Xe ≤ e, rank X = k, Tr X = k, X ≥ 0} .

max
X∈Σn
+

The relaxation (4) can be relaxed further to a semideﬁnite program by omitting the nonconvex rank
constraint:

{Tr(W X) : Xe ≤ e, Tr X = k, X ≥ 0} .

max
X∈Σn
+

(4)

(5)

6

We should note that the semideﬁnite program (5) is remarkably similar to the semideﬁnite relaxation
of the minimum sum of squared distance partition of [25] and the semideﬁnite relaxation of the
maximum likelihood estimate of the stochastic block model considered by [7], although our relaxation
approach diﬀers signiﬁcantly from those in these two papers.

2.2 Block Models and Recovery Guarantees

Given a set of clusterable data or, more accurately, a clusterable graph representation of data, [3]
established that one can recover the underlying cluster structure from the optimal solution of the
semideﬁnite program (5). Speciﬁcally, it is assumed that data with strong cluster structure should
correspond to similarity graphs with heavy weight assigned to edges within clusters, relative to that
between cluster edges. This corresponds to pairs of items within clusters being signiﬁcantly more
similar than pairs of items in diﬀerent clusters. This motivates the following block model.

Let H∗ be a k-disjoint-clique subgraph of Kn = (V, W ) with vertex set composed of the
disjoint cliques C1, . . . , Ck and let Σn denote the set of all n × n symmetric matrices. We consider
weight matrices W = [wij] ∈ Σn with entries sampled independently from one of two probability
distributions Ω1, Ω2 as follows.

• For each i = 1, . . . , k and each u, v ∈ V , we sample wuv = wvu from a distribution Ω1 such

that

for ﬁxed α ∈ (0, 1].

E[wuv] = E[wvu] = α,

0 ≤ wuv ≤ 1

• For each remaining edge uv, we sample the edge weight wuv = wvu from a second distribution

Ω2 such that

for ﬁxed β ∈ [0, α).

E[wuv] = E[wvu] = β,

0 ≤ wuv ≤ 1,

We should note that the assumption that the entries of W are bounded between 0 and 1 is made
for simplicity in the statement and proof of our main result; analogous recovery guarantees hold if
we assume that random variables sampled according to Ω1 and Ω2 are bounded and nonnegative
with high probability. We say that such random matrices W are sampled from the planted cluster
model. Note that if W is sampled from the planted cluster model, then weight is concentrated
on within-cluster edges (in expectation). This provides a natural generalization of the stochastic
block model considered by, for example, [10, 24, 26, 27], which in turn generalizes the planted
k-disjoint-clique subgraph model considered by [6]. Indeed, the stochastic block model corresponds
to the planted cluster model in the special case that Ω1 and Ω2 are Bernoulli distributions with
probabilities of success p and q, respectively. The following theorem, established in [3], provides a
suﬃcient condition for recovery of the planted cliques C1, . . . , Ck from the optimal solution of (5)
under the planted cluster model (see [3, Theorem 2.1]).

7

Theorem 2.1 Suppose that the vertex sets C1, . . . , Ck deﬁne a k-disjoint-clique subgraph H∗ of

the n-node weighted complete graph Kn(V, W ) and let Ck+1 := V \(cid:0)∪k

(cid:1). Let ri := |Ci| for all

i = 1, . . . , k + 1 and let ˆr = mini=1,...,k ri. Let W ∈ Σn be a random symmetric matrix sampled from
the planted cluster model according to distributions Ω1 and Ω2 with means α and β, respectively,
satisfying

i=1Ci

γ = γ(α, β, r) := α(1 + δ0,rk+1) − 2β > 0,

where δi,j is the Kronecker delta function deﬁned by δij = 1 if i = j and 0 otherwise. Let X∗ be
the feasible solution of (5) corresponding to C1, . . . , Ck deﬁned by (3). Then there exist scalars
c1, c2, c3 > 0 such that if

√

(cid:112)krk+1 + c3rk+1 ≤ γ ˆr,

(6)
then X∗ is the unique optimal solution of (5), and H∗ is the unique maximum density k-disjoint-
clique subgraph of Kn with probability tending exponentially to 1 as ˆr → ∞.

n + c2

c1

We present a new analysis that improves upon the recovery guarantee of Theorem 2.1. In particular,
Theorem 2.1 assumes that the expectations of Ω1 and Ω2 in the planted cluster model are ﬁxed
and that the variances are bounded by 1. In this paper, we generalize the recovery guarantee of
Theorem 2.1 for the case where the parameters α and β depend on the number of nodes n in the
graph. By doing so, we are able to more accurately quantify the role of the variances of Ω1 and
Ω2 in our recovery guarantee and, thus, expand the notion of a clusterable graph. We have the
following theorem.

Theorem 2.2 Suppose that the vertex sets C1, . . . , Ck deﬁne a k-disjoint-clique subgraph K∗ of the

weighted complete graph Kn = (V, W ) on n vertices and let Ck+1 = V \(cid:0)∪k

(cid:1). Let ri = |Ci|

for all i = 1, . . . , k + 1, and let ˆr = mini=1,...,k ri and ˜r = maxi=1,...,k ri. Let W ∈ Σn be a random
symmetric matrix sampled from the planted cluster model according to distributions Ω1 and Ω2 with
means α = α(n) and β = β(n), respectively, satisfying

i=1Ci

γ = γ(α, β) = α − β > 0.

(7)

1 = σ2

1(n) and σ2

Let σ2
2(n) denote the variances of the distributions Ω1 and Ω2, respectively.
Let X∗ be the feasible solution to (5) corresponding to C1, . . . , Ck deﬁned by (3). Then there exists
scalar d > 0 such that if

2 = σ2

γ ˆr − 9ˆr3/4 − 6 max(cid:8)σ1

˜r log ˜r, log ˜r(cid:9) − βrk+1 − d(cid:112)krk+1 ≥ 12 max(cid:8)σ2

√

n log n, log n(cid:9) ,

√

(8)

then X∗ is the unique optimal solution for (5), and K∗ is the unique maximum density k-disjoint-
clique subgraph of Kn with probability at least

(cid:16)−2ˆr1/2(cid:17)

.

(9)

1 − 4ˆr−5 − 2(7k + 10)k˜r exp

8

When α, β are ﬁxed, we obtain the same recovery guarantee as before, up to constants and
logarithmic terms: we have exact recovery with probability tending exponentially to 1 as n → ∞
provided ˆr ≥ c1
n log n and ˆr ≥ c2rk+1 for some constants c1 and c2 depending on Ω1 and Ω2.
Here, our new analysis provides an improvement on the gap γ = α − β in the presence of outlier
nodes from Theorem 2.1, expanding the set of graphs we may reasonably expect to cluster.

√

On the other hand, if noise in the form of between-cluster edges is approximately sparse, then
we should expect to be able recover much smaller clusters. For example, suppose that Ω2 is the
Bernoulli distribution, with probability of adding an edge q and that Ω1 is the Bernoulli distribution
with probability of adding an edge p = 1; the assumption that p = 1 is for the sake of simplicity in
this example and we can expect analogous recovery guarantees for any p tending slowly enough to 0.
Assume further that q(1 − q) ≤ log n/n. Finally, again for simplicity, assume that we have k equally
sized clusters of size ˆr = n/k and no outliers (rk+1 = 0). In this case, (8) holds if

γ ˆr ≥ 12 log n + 9ˆr3/4,
since the terms involving rk+1 and 1 − p are equal to zero.

(10)

Now, suppose that ˆr = ω(log2 n). For example, suppose that we can write ˆr = log2t n for some

t > 1. Then, the left-hand side of (10) is bounded below by

(cid:18)

1 − log n
n

ˆr =

(cid:19)

(cid:19)

1 − log n
n

(cid:18)
12(log n)1−2t + 9(log n)3/4−2t(cid:17)

log2t n ≥ 1
2

(cid:16)

log2t n

12 log n + 9 log3 n =

log2t n.

for suﬃciently large n. On the other hand, the right-hand side is given by

This implies that (10) is satisﬁed, for this choice of ˆr, if

≥ 12(log n)1−2t + 9(log n)3/4−2t,

1
2

which holds for all suﬃciently large n.

(11)

(12)

(13)

(14)

(15)

(16)

(17)

It remains to argue that we have perfect recovery with high probability if (13) is satisﬁed. To

do so, we argue that

for suﬃciently large n. Indeed, (14) holds if and only if

exp

√

(cid:17)

(cid:16)−2
= exp(cid:0)−2 logt n(cid:1) ≤ n−4
− 2 logt n ≤ log(cid:0)n−4(cid:1) = −4 log n.

ˆr

Let y = log n. Then (15) holds if and only −2yt ≤ −4y, which holds if and only if

2 ≥ yt−1 = (log n)t−1.

Now (16) holds if log n ≥ 21/(t−1), which holds for all

(cid:17)

.

n ≥ exp

1
t−1

2

(cid:16)

9

This shows that (14) holds for all suﬃciently large n and, thus,

√
1 − 2 (7k + 10) kˆr exp(−2

ˆr) − 4ˆr−5 ≥ 1 − 2

(cid:18) 7

(cid:19)

n−2 − 4 log−10t n

+

10
n
− 20
n3 − 4 log−10t n.

= 1 −

log2t n
14

n2 log2t n

This implies that the probability of recovery is tending polylogarithmically to 1 in n if ˆr > logξ n
and q < log n/n for all ξ > 2.

3 Derivation of the Recovery Guarantee

In this section, we show that if the hypothesis of Theorem 2.2 is satisﬁed then the solution X∗
constructed according to (3) is optimal for (5) and the corresponding k-disjoint-clique subgraph has
maximum density. In particular, we will show that X∗ satisﬁes the following suﬃcient condition for
optimality of a feasible solution of (5) (see [3, Theorem 4.1]).

Theorem 3.1 Let X be feasible for (5) and suppose that there exist some µ ∈ R, λ ∈ Rn
+,
Ξ ∈ Rn×n

and S ∈ Σn

+ such that

+

−W + λeT + eλT − Ξ + µI = S
λT (Xe − e) = 0
Tr(XΞ) = 0

Tr(XS) = 0.

(18)

(19)

(20)

(21)

Then X is optimal for (5).

Theorem 3.1 is a restriction of the Karush-Kuhn-Tucker optimality conditions to the semideﬁnite
program (5) (see, for example, [8, Section 5.5.3]). The goal of this section is to establish that we
can construct dual variables µ ∈ R, λ ∈ Rn
+ which satisfy the hypothesis
of Theorem (3.1) with high probability if the weight matrix W is sampled from a distribution of
clusterable block models. To motivate our proposed choice of dual variables, we note that the
complementary slackness condition Tr(XS) = 0 holds if and only if XS = 0 under the assumption
that both X and S are positive semideﬁnite. Therefore, the block structure of X implies that each
block of S corresponding to a cluster block in W must sum to zero.

+, Ξ ∈ Rn×n

and S ∈ Σn

+

Before we continue with the construction of our dual variables, let us ﬁrst remind ourselves of the
notation of Theorem 2.2. Let K∗ be a k-disjoint-clique subgraph of Kn with vertex set composed of
the disjoint cliques C1, . . . , Ck of sizes r1, . . . , rk and let X∗ be the corresponding feasible solution of
(5) deﬁned by (3). Let Ck+1 := V \ (∪k
i=1 ri be the size of the set of outliers
Ck+1. Moreover, let ˆr := mini=1,...,k ri and ˜r := maxi=1,...,k ri be the size of the smallest and largest

i=1Ci) and rk+1 := n−(cid:80)k

10

clusters, respectively, and let W ∈ Σn be a random symmetric matrix sampled from the planted
cluster model with planted clusters C1, . . . , Ck and outliers Ck+1 according to the distributions Ω1
and Ω2 with means α = α(n) and β = β(n), and variances σ1 = σ1(n) and σ2 = σ2(n), respectively.

We now propose a choice of dual variables satisfying the complementary slackness condition
XS = 0. Restricting this condition to the blocks X Cq,Cq and SCq,Cq of X and S with rows and
columns indexed by Cq, q ∈ {1, 2, . . . , k}, we see that X∗S = 0 holds if and only if

0 = SCq,Cq e = µe + rqλCq + (λT

by the block structure of X∗; note that ΞCq,Cq = 0 is chosen to satisfy the complementary slackness
condition (20). Solving this linear system for λCq gives

On the other hand, we have no restriction on the entries of λCk+1. In this case, we choose

Note that the formula for λCk+1 is equal to the expectation of an arbitrary λCq but with q = k + 1:

(22)

(23)

λCq =

1
rq

Cq e)e − W Cq,Cq e
(cid:33)(cid:33)

eT W Cq,Cq e

e.

rq

(cid:32)

µ +

(cid:32)
W Cq,Cq e − 1
2
(cid:18)

λCk+1 =

E(cid:2)λCq

(cid:3) =

(cid:19)
(cid:19)

1
2

α − µ
rk+1

e.

1
2

α − µ
rq

e.

(cid:18)

(cid:19)

Next, we use this choice of λ to construct the remaining dual variables.

Fix q, s ∈ {1, 2, . . . , k} such that q (cid:54)= s. We will choose ΞCq,Cs so that SCq,Cse = 0 and

SCs,Cq e = 0. In particular, we choose

(cid:18)

ΞCq,Cs =

α − β − µ
rq

− µ
rs

eeT + yq,seT + e(zq,s)T ,

(24)

where the vectors yq,s and zq,s are unknown vectors parametrizing the entries of ΞCq,Cs. That
Cs − W Cq,Cs plus the parametrizing
is, we choose ΞCq,Cs to be the expected value of λCq eT + eλT
term yq,seT + e(zq,s)T ; the vectors yq,s and zq,s are chosen to be solutions of the systems of linear
equations given by the complementary slackness conditions SCq,Cse = 0 and SCs,Cq e = 0. It is
reasonably straight-forward to show that we may choose

yq,s =

1
rs

where

(cid:32)

(cid:33)

bq,s − bT

q,se
rq + rs

e

zq,s =

bq,s = ΞCq,Cse − E(cid:2)ΞCq,Cs

(cid:33)

1
rq

(cid:32)
bs,q − bT
(cid:3) e;

s,qe
rq + rs

e

,

(25)

(26)

(27)

see [3, Section 4.2] for more details. Finally, we choose

µ = (α − β)ˆr = γ ˆr,

11

where  > 0 is a parameter to be chosen later.

The entries of S will be chosen according to the stationarity condition (18), but we will also

deﬁne an auxiliary variable ˜S ∈ Σn as the following (k + 1) × (k + 1) block matrix:



˜SCq,Cs =

αeeT − W Cq,Cs,
βeeT − W Cq,Cs,
βeeT − W Cq,Cs + (λCq − E[λCq ])eT ,
βeeT − W Cq,Cs + e(λCs − E[λCs])T ,

if q = s, q, s ∈ {1, . . . , k}
if q (cid:54)= s, q, s ∈ {1, . . . , k}
if s = k + 1
if q = k + 1.

(28)

We next state the following theorem, ﬁrst stated by [3, Theorem 4.2], which characterizes when the
proposed dual variables satisfy the hypothesis of Theorem 3.1.

Theorem 3.2 Suppose that the vertex sets C1, . . . , Ck deﬁne a k-disjoint-clique subgraph K∗ of
the weighted complete graph Kn = (V, W ), where W ∈ Σn is a random symmetric matrix sampled
from the planted cluster model according to distributions Ω1, Ω2 with means α and β respectively,
where α > β. Let r1, . . . , rk+1, and ˆr be deﬁned as in Theorem 2.2. Let X∗ be the feasible solution
for (5) corresponding to C1, . . . , Ck deﬁned by (3). Let µ ∈ R, λ ∈ Rn, and Ξ ∈ Rn×n be chosen
according to (22), (23), (24), and (27), and let ˜S be chosen according to (28). Suppose that the
entries of λ and Ξ are nonnegative. Then X∗ is optimal for (5), and K∗ is the maximum density
k-disjoint-clique subgraph of Kn corresponding to W if

(cid:107) ˜S(cid:107) ≤ (α − β)ˆr = γ ˆr.

(29)

Moreover, if (29) is satisﬁed and

(30)
for all q, s ∈ {1, . . . , k} such that q (cid:54)= s, then X∗ is the unique optimal solution of (5) and K∗ is
the unique maximum density k-disjoint-clique subgraph of Kn.

rseT W Cq,Cq e > rqeT W Cq,Cse

The proof of Theorem 3.2 is nearly identical to that by [3, Theorem 4.2], and is omitted.
Theorem 3.2 provides a clear roadmap for the remainder of the proof; if we can show that if W is
sampled from the planted cluster model satisfying (8) then λ and Ξ are nonnegative and (cid:107) ˜S(cid:107) ≤ γ ˆr
with high probability, then we will have established that we can recover the underlying block
structure with high probability in this case. We establish the necessary bounds on λ, Ξ, and (cid:107) ˜S(cid:107)
in the following sections.

3.1 Nonnegativity of λ and Ξ

We ﬁrst establish that the entries of Ξ, constructed according to (24), are nonnegative with high
probability. To do so, we will make repeated use of the following inequality of [16, Theorem 1],
which provides a bound on the tail of a sum of bounded independent random variables.

12

Theorem 3.3 Let x1, . . . , xm be independent identically distributed (i.i.d.) variables sampled from
a distribution satisfying 0 ≤ xi ≤ 1 for all i = 1, . . . , m. Let S = x1 + ··· + xm. Then

P r(|S − E[S]| > t) ≤ 2 exp

for all t > 0.

(cid:18)−2t2

(cid:19)

m

(31)

(32)

(33)

The following bound on the parametrizing vectors yq,s and zq,s in the deﬁnition of the (Cq, Cs)

block of Ξ, c.f., (24), is an immediate consequence of Hoeﬀding’s inequality (Theorem 3.3).

Lemma 3.1 It holds that (cid:107)yq,s(cid:107)∞ + (cid:107)zq,s(cid:107)∞ ≤ 9ˆr−1/4 for all q, s ∈ {1, . . . , k + 1} such that q (cid:54)= s

with probability at least 1 − 28˜r exp(cid:0)−2ˆr1/2(cid:1).

For q, s ∈ {1, . . . , k} such that q (cid:54)= s, we deﬁne yq,s and zq,s as in (25). To bound the
absolute values of the entries of yq,s and zq,s, we must estimate the sums eT W Cq,Cq e, eT W Cs,Cse
and eT W Cq,Cse; applying Theorem 3.3 to bound the tails of these sums yields Lemma 3.1. See
Appendix A for the full argument.

We also have the following bound on the entries of Ξ as an immediate consequence of Lemma 3.1.

Proposition 3.1 Suppose α and β satisfy (7). Then each entry of Ξ is nonnegative with probability
at least

1 − 14k(k + 1)˜r exp

(cid:16)−2ˆr1/2(cid:17)

if  satisﬁes

0 <  ≤ 1 − 9ˆr−1/4

γ

.

Proof: Fix i ∈ Cq, j ∈ Cs for some q, s ∈ {1, . . . , k + 1} such that q (cid:54)= s. By construction, we have

ΞCq,Cs = E(cid:2)λCq eT + eλT
(cid:19)

(cid:18)

(cid:18) 1

=

2

α − µ
rq

(cid:3) + yq,seT + e (zq,s)T
(cid:18)
(cid:19)
Cs − W Cq,Cs
α − µ
1
rs
2

(cid:19)

− β

+

eeT + yq,seT + e (zq,s)T

Using (7), (27), and Lemma 3.1, we see that

Ξij ≥ 1
2

(α − γ) +

(α − γ) − β − (cid:107)yq,s(cid:107)∞ − (cid:107)zq,s(cid:107)∞

1
2

≥ α − γ − β − 9ˆr−1/4 = γ − γ − 9ˆr−1/4
= (1 − )γ − 9ˆr−1/4,

where the second inequality holds with probability at least 1 − 28˜r exp(cid:0)−2ˆr1/2(cid:1) according to

Lemma 3.1. Note that (1 − )γ − 9ˆr−1/4 ≥ 0 if and only if
 ≤ 1 − 9ˆr−1/4

.

γ

13

Applying the union bound over all(cid:0)k+1
probability at least 1 − 14k(k + 1)˜r exp(cid:0)−2ˆr1/2(cid:1) if 0 <  ≤ 1 − (9ˆr−1/4)/γ.

(cid:1) blocks of Ξ shows that each entry of Ξ is nonnegative with

2

We have an analogous result ensuring that the entries of λ are nonnegative with high probability;

we present the proof of this result in Appendix B.

Proposition 3.2 Suppose α and β satisfy (7). Then each entry of λ is nonnegative with probability
at least

1 − 6k exp

(cid:16)−2ˆr1/2(cid:17)
0 <  ≤ α − 2ˆr−1/4 − ˆr−1/2

γ

.

(34)

(35)

if  satisﬁes

We conclude this section with a result ensuring that the uniqueness condition (30) of Theorem 3.2

is satisﬁed for all q, s ∈ {1, . . . , k} such that q (cid:54)= s; a proof was given by [3, Lemma 4.4].

Proposition 3.3 If ˆr > 9/(α − β)2 then rseT W Cq,Cq e > rqeT W Cq,Cse for all q, s{1, . . . , k} such
that q (cid:54)= s with high probability.

3.2 A Bound on ˜S

It remains to establish the following bound on the spectral norm of the matrix ˜S.

Proposition 3.4 There exists scalar d > 0 such that

(cid:110)

(cid:112)n log n, log n

(cid:111)

(cid:107) ˜S(cid:107) ≤ 12 max

σ2

(cid:110)

(cid:112)˜r log ˜r, log r

(cid:111)

+ βrk+1 + d(cid:112)krk+1

(36)

+ 6 max

σ1

with probability at least 1 − 4ˆr−5.

The proof of Proposition 3.4 follows the same structure as that of [3, Lemma 4.5]. In particular,

we decompose ˜S as ˜S = ˜S1 + ˜S2 + ˜S3, where

˜S1 = E[W ] − W ,

(cid:3)(cid:1) eT ,


(cid:0)λCq − E(cid:2)λCq
−βeeT ,

e (λCs − E [λCs])T ,
0,

otherwise.

0,

[ ˜S2]Cq,Cs =

[ ˜S3]Cq,Cs =

if s = k + 1

if q = k + 1

otherwise,

if q = s = k + 1

(37)

(38)

(39)

Note that (cid:107) ˜S3(cid:107) = β(cid:107)eeT(cid:107) = βrk+1. Moreover, [3] argued that there exists scalar d > 0 such

that (cid:107) ˜S2(cid:107) ≤ d(cid:112)krk+1 with probability tending exponentially to 1 as ˆr → ∞. It remains to bound

(cid:107) ˜S1(cid:107); we do so using the following lemma.

14

Lemma 3.2 Suppose that ˜S1 is constructed according to (37) for some W ∈ Σn sampled from the
planted cluster model. Then

(cid:110)

σ2

(cid:112)n log n, log n

(cid:111)

(cid:110)

(cid:111)
(cid:112)˜r log ˜r, log ˜r

+ 6 max

σ1

(40)

(cid:107) ˜S1(cid:107) ≤ 12 max
with probability at least 1 − 3˜r−5.

We delay the proof of Lemma 3.2 until Appendix C. Combining the three bounds on (cid:107) ˜S1(cid:107),

(cid:107) ˜S2(cid:107), and (cid:107) ˜S3(cid:107) and applying the triangle inequality one last time shows that

(cid:107) ˜S(cid:107) ≤ (cid:107) ˜S1(cid:107) + (cid:107) ˜S2(cid:107) + (cid:107) ˜S3(cid:107)

(cid:110)

(cid:112)n log n, log n

(cid:111)

≤ 12 max

σ2

(cid:110)

(cid:112)˜r log ˜r, log r

(cid:111)

+ βrk+1 + d(cid:112)krk+1

+ 6 max

σ1

with probability at least 1 − 4ˆr−5.

3.3 The Conclusion of the Proof

According to Theorem 3.2, it suﬃces to prove that (cid:107) ˜S(cid:107) ≤ γ ˆr is satisﬁed with high probability in
order to prove Theorem 2.2. According to Proposition 3.4, if

(cid:110)

(cid:112)n log n, log n

(cid:111)

12 max

σ2

+ 6 max

σ1

which is equivalent to

(cid:0)12 max(cid:8)σ2

n log n, log n(cid:9) + 6 max(cid:8)σ1

√

 ≥ 1

(cid:110)

(cid:111)
(cid:112)˜r log ˜r, log ˜r

+ βrk+1 + d(cid:112)krk+1 ≤ γ ˆr,
˜r log ˜r, log ˜r(cid:9) + βrk+1 + d(cid:112)krk+1
(cid:1) ,

√

γ ˆr

(41)
then (cid:107) ˜S(cid:107) ≤ γ ˆr holds with probability at least 1 − 4ˆr−5. Hence, we have three conditions, (33), (35)
and (41), on  > 0 that need to be satisﬁed simultaneously; choosing any  > 0 satisfying all three
establishes the desired recovery guarantee. We see that (33) and (41) are simultaneously fulﬁlled

1 − 9ˆr−1/4

γ ≥ 1

with probability at least 1 − 4ˆr−5 − 14k(k + 1)˜r exp(cid:0)−2ˆr1/2(cid:1) if
n log n, log n(cid:9) + 6 max(cid:8)σ1
˜r log ˜r, log ˜r(cid:9) − βrk+1 − d(cid:112)krk+1 ≥ 12 max(cid:8)σ2

(cid:0)12 max(cid:8)σ2
γ ˆr − 9ˆr3/4 − 6 max(cid:8)σ1
6k exp(cid:0)−2ˆr1/2(cid:1) if

which holds if and only if

˜r log ˜r, log ˜r(cid:9) + βrk+1 + d(cid:112)krk+1
n log n, log n(cid:9) .

√

(42)
Next, we see that (41) and (35) are simultaneously fulﬁlled with probability at least 1 − 4ˆr−5 −

(cid:1),

√

√

√

γ ˆr

α − 2r−1/4 − r−1/2

γ

≥ 1
γ ˆr

(cid:110)

σ1

(cid:111)
(cid:112)˜r log ˜r, log ˜r

+ 6 max

(cid:16)

(cid:110)
(cid:111)
(cid:112)n log n, log n
(cid:111)(cid:19)
(cid:110)
βrk+1 + d(cid:112)krk+1

σ2

,

12 max

+

1
γ ˆr

15

which holds if and only if

αˆr − 2ˆr3/4 − ˆr1/2 − 6 max(cid:8)σ1

˜r log ˜r, log ˜r(cid:9) − βrk+1 − d(cid:112)krk+1 ≥ 12 max(cid:8)σ2

√

√

Note that (43) is satisﬁed if (42) holds. Hence, if (42) holds, then (33), (35) and (41) hold
simultaneously with probability at least
1 − 4ˆr−5 − (14k(k + 1) + 6k) ˜r exp

= 1 − 4ˆr−5 − 2 (7k + 10) k˜r exp

(cid:16)−2ˆr1/2(cid:17)

.

(43)

n log n, log n(cid:9) .
(cid:16)−2ˆr1/2(cid:17)

This concludes the proof of Theorem 2.2.

4 Numerical Results

We conclude with a series of experiments that empirically verify the phase transitions predicted in
Section 2.2. In particular, we randomly sample graphs G = (V, W ) from the planted cluster model
and compare the optimal solution of (5) with the planted partition.

In each experiment, we solve (5) iteratively using the ADMM algorithm proposed by [3].

Speciﬁcally, we split the decision variable X to obtain the equivalent formulation

max(cid:8)Tr(W Y ) : X − Y = 0, Xe ≤ e, X ≥ 0, Tr(Y ) = k, Y ∈ ΣV

(cid:9) .

+

(44)

We then apply an approximate dual ascent scheme to maximize the augmented Lagrangian

Lβ(X, Y , Z) = Tr(W Y ) − Tr(Z(X − Y )) +

(cid:107)X − Y (cid:107)2
F ,

β
2

where β > 0 is a penalty parameter for violation of the linear equality constraint X − Y = 0. In
particular, we minimize Lβ with respect to Y and X successively, and then update Z = Z−β(X−Y ).
Please see the work of [3, Section 6] for implementation details.

We perform two sets of experiments, one to illustrate the recovery guarantee for dense graphs
and another to illustrate the guarantee when the noise is sparse. For the dense graph experiments,
we ﬁx n = 1000, and sample 10 graphs from the planted cluster model corresponding to the Bernoulli
distributions Ω1 = Bern(1) and Ω2 = Bern(q) with probabilities of success p = 1 and q for each
ˆr ∈ {20, 40, . . . , 500} and q ∈ {0, 0.05, 0.1, . . . , 0.7, 0.75} , respectively. For each graph G, we call
the ADMM algorithm sketched above to solve (5); in the algorithm, we use penalty parameter
β = min{max{5n/k, 80} , 500} /2, stopping tolerance  = 10−4, and maximum number of iterations
100. We declare the block structure of G to be recovered if (cid:107)X∗ − X 0(cid:107)2
F < 10−3, where
X∗ is the solution returned by the ADMM algorithm and X 0 is the proposed solution given by (3).

F /(cid:107)X 0(cid:107)2

We perform identical experiments for graphs sampled from the planted cluster model with sparse
noise. In particular, we ﬁx n = 1000 and p = 1 as before, and sample 10 graphs from the planted
cluster model corresponding to the Bernoulli distributions Ω1 = Bern(1) and Ω2 = Bern(q) for
each ˆr ∈ {2, 3, 4, . . . , 25} and q ∈ {0, 0.25 log(n)/n, 0.5 log(n)/n, . . . , 5 log(n)/n}. For each graph G,

16

(a) Dense noise.

(b) Sparse noise.

Figure 1: Empirical recovery rate for n-node graph with k planted cliques of size at least ˆr and W
generated according to the distributions Ω1 = Bern(1), Ω2 = Bern(q). Brighter colors indicate
higher rates of recovery, with black corresponding to 0 recoveries and white corresponding to 10
recoveries (out of 10 trials). The dashed curve indicates the phase transition to perfect recovery
predicted by (8).

we call the ADMM algorithm to solve (5) (with the same parameters as before) and declare the
block structure of G recovered if (cid:107)X∗ − X 0(cid:107)2

F < 10−3.

F /(cid:107)X 0(cid:107)2

(cid:110)(cid:112)q(1 − q)n log n, log n

(cid:111)

Figure 1 illustrates the empirical success rate for each choice of ˆr and q, as well as the curve
ˆr = 12
1−q max
illustrating the phase transition to perfect recovery predicted
by (8). It is clear that we are able to recover signiﬁcantly smaller clusters under sparse noise than
under dense noise, in accordance with (8). Speciﬁcally, we have perfect recovery for all ˆr ≥ 3 in all
sparse noise experiments, while we require clusters containing a few dozen nodes to ensure recovery
in the dense noise experiments. On the other hand, as can be seen from the empirical success rate
plots, our predicted recovery guarantees may be overly pessimistic, as we have perfect recovery for ˆr
smaller than that predicted for almost all choices of q.

5 Conclusions

We have established theoretical guarantees for graph clustering via a semideﬁnite relaxation of the
densest k-disjoint problem. These results add to the growing corpus of evidence that clustering,
while intractable in general, is possible if we seek to cluster clusterable data, i.e., data consisting
of well-deﬁned and well-separated groups of similar items. Moreover, our results provide further
√
evidence that the ω(
n) barrier can be broken for perfect cluster recovery in approximately sparse
graphs and, speciﬁcally, that the size of recoverable clusters scales polylogarithmically with n at

17

worst in the special case that all clusters are roughly the same size.

Our results suggest several areas of further research. The numerical simulations suggest that our
theoretical guarantees may be overly conservative, especially in the approximately sparse noise case;
further investigation is needed to determine if tighter estimates on the minimum size of clusters
eﬃciently recoverable exist. Moreover, our model assumes clusters are disjoint. This is clearly not
met in many practical applications; for example, returning to the social networking realm, users
may belong to several overlapping communities. It would be worthwhile to see how our model
and recovery guarantees can be modiﬁed to address overlapping clusters. Finally, our heuristic for
graph clustering requires the solution of a semideﬁnite program, which may be impractical for even
moderately large graphs; eﬃcient, scalable methods for solving our semideﬁnite relaxation, and
semideﬁnite programs in general, are needed.

Acknowledgements

We are grateful to John Bruer and Joel Tropp for their insights and helpful suggestions. Aleksis
Pirinen was supported by a California Institute of Technology Summer Undergraduate Research
Fellowship (SURF) using funds provided by Oﬃce of Naval Research (ONR) award N000014-11-1002.
Brendan Ames was supported by University of Alabama Research Grant RG14678.

18

A Proof of Lemma 3.1

In this appendix, we give the full proof of Lemma 3.1.
Proof: We ﬁx q, s ∈ {1, . . . , k} such that q (cid:54)= s and assume without loss of generality that rq ≤ rs.
The proof for the case when either q or s is equal to k + 1 is analogous1. By the deﬁnition (25) of
y := yq,s and the triangle inequality, we have

(cid:107)y(cid:107)∞ ≤ 1
rs

(cid:107)bq,s(cid:107)∞ +

|bT
q,se|
rq + rs

.

(45)

For simplicity, let b1 := bq,s and b2 := bs,q. It follows from (26) that the ith element of b1, denoted
b1
i , is given by

(cid:32)

(cid:19)

(cid:18)

(cid:18)

(cid:33)

(cid:19)

(cid:88)

j∈Cs

 .

b1
i = rs

λi − 1
2rq

(αrq − µ)

+

Cse − 1
λT
2

(αrs − µ)

−

wij − βrs

It follows from the deﬁnition (22) of λCs that

We now establish bounds on (46), (47), and (48). In (46), let  = r3/2

. Then we have

s

1For example, if q = k + 1, then

19

Let  > 0 be arbitrary. Then, according to (31), we have

which implies that

Moreover, we have

Finally,

In (47), let  = r3/4

s

s

Pr

s

.

1
2rs

λT

Cse =

1
2rs

− 22
r2
s

wij − βrs

(αrs − µ)

Cse − 1
2

(cid:0)eT W Cs,Cse − rsµ(cid:1) ,
(cid:12)(cid:12)(cid:12)(cid:12)λT
(cid:12)(cid:12)(cid:12)(cid:12) =
(cid:12)(cid:12) .
(cid:12)(cid:12)eT W Cs,Cse − αr2
(cid:18)
(cid:19)
(cid:12)(cid:12) ≥ (cid:9) ≤ 2 exp
Pr(cid:8)(cid:12)(cid:12)eT W Cs,Cse − αr2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ 
 ≤ 2 exp
(cid:19)
(cid:18)
(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) +
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12) ≤ 1
(cid:12)(cid:12)(cid:12)(cid:12)λi − 1
(cid:12)(cid:12)eT W Cq,Cq e − αr2
(cid:18)
(cid:19)
(cid:111) ≤ 2 exp
(cid:110)(cid:12)(cid:12)eT W Cs,Cse − αr2
(cid:12)(cid:12) ≥ r3/2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
 ≤ 2 exp
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ r3/4
(cid:33)
(cid:32)
(cid:88)
(cid:12)(cid:12)(cid:12) = 0 (compare to (48)).
(cid:12)(cid:12)(cid:12)λi − 1

(cid:16)−2r1/2

− 2r3/2
s
rs

(αrq − µ)

wij − αrq

wij − βrs

. Then we have

− 2r3
s
r2
s

(αrq − µ)

2rq

− 22
rs

.

(cid:88)

= 2 exp (−2rs) .

= 2 exp

s

rq

j∈Cq

1
2r2
q

j∈Cs

j∈Cs

s

s

2rq

Pr

Pr

s

(cid:17)

.

(46)

(47)

(48)

(cid:12)(cid:12) .

q

Finally, regarding (48), we bound both terms in the sum and get

by calculations identical to those above. Applying these bounds and the triangle inequality, we

conclude that(cid:12)(cid:12)b1

i

(cid:12)(cid:12) ≤ rs

≤ rs

= rs

(cid:17)

q

q

Pr

Pr

j∈Cq

(cid:88)

wij − αrq

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ r3/4
 ≤ 2 exp
(cid:16)−2r1/2
(cid:111) ≤ 2 exp (−2rq)
(cid:110)(cid:12)(cid:12)eT W Cq,Cq e − αr2
(cid:12)(cid:12) ≥ r3/2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12) +
(cid:12)(cid:12)(cid:12)(cid:12)λi − 1
(cid:12)(cid:12)(cid:12)(cid:12) +
(cid:12)(cid:12)(cid:12)(cid:12)λT
(cid:19)
(cid:18) 1
(cid:32)

Cse − 1
2

(αrq − µ)

(αrs − µ)

r3/2
s + r3/4

r3/4
q +

(cid:33)

r3/2
q

(cid:88)

2rq

+

s

q

q

j∈Cs

1
2rs
+ r−1/4

s

−1/2
r
s

1
2r2
q
−1/2
r
q

≤ 3rsr−1/4

q

rq
r−1/4

q

q

2

2

+

+

(cid:16)−2r1/2
(cid:17)
(cid:110)(cid:107)b1(cid:107)∞ ≥ 3rsr−1/4
(cid:18)

(cid:19)

q

(cid:111) ≤ 8rq exp

(cid:17)

q

(cid:16)−2r1/2
(cid:19)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

wij − βrs

.

(49)

+(cid:0)βrsrq − eT W Cq,Cse(cid:1) .

(αrs − µ)

with probability at least 1 − 8 exp

. Applying the union bound shows that

We next bound(cid:12)(cid:12)bT

1 e(cid:12)(cid:12). We have

Pr

(cid:18)

bT
1 e = rs

Cq e − 1
λT
2

(αrq − µ)

+ rq

By the above calculations, we have

with probability at least 1 − 2 exp (−2rq), and

(cid:12)(cid:12)(cid:12)(cid:12)λT
(cid:12)(cid:12)(cid:12)(cid:12)λT

Cq e − 1
2

(αrq − µ)

Cse − 1
2

(αrs − µ)

Cse − 1
λT
2

2rq

2rs

(cid:12)(cid:12)(cid:12)(cid:12) ≤ 1
(cid:12)(cid:12)(cid:12)(cid:12) ≤ 1
(cid:9) ≤ 2 exp
(cid:12)(cid:12)(cid:12)(cid:12)λT

Cse − 1
2
√

√
rs

√

rq
2

r3/2
q =

√

rs
2

r3/2
s =

(cid:32)
− 2r2
q rs
rqrs

(αrs − µ)

with probability at least 1 − 2 exp (−2rs). Moreover,

Pr(cid:8)(cid:12)(cid:12)βrsrq − eT W Cq,Cse(cid:12)(cid:12) ≥ rq
(cid:12)(cid:12)(cid:12)(cid:12) + rq
(cid:12)(cid:12)(cid:12)(cid:12)λT
1 e(cid:12)(cid:12) ≤ rs
(cid:12)(cid:12)bT

Cq e − 1
(αrq − µ)
√
2
√
rq
2

rs
2

+ rq

+ rq

≤ rs

√

rs ≤ 2rs

rq

20

Combining the above derivations with the triangle inequality yields

= 2 exp (−2rq) .

(cid:33)
(cid:12)(cid:12)(cid:12)(cid:12) +(cid:12)(cid:12)βrsrq − eT W Cq,Cse(cid:12)(cid:12)

(50)

(cid:19)

≤ 4r−1/4

q

. Since this holds for any q, s ∈ {1, . . . , k} such that

with probability at least 1 − 6 exp (−2rq). We conclude that

(cid:9) ≤ 6 exp (−2rq) .
(cid:18)

3rsr−1/4

q

+

√

rq

≤ 1
rs

√

2rs
rq
rq + rs

Pr(cid:8)(cid:12)(cid:12)bT

1 e(cid:12)(cid:12) ≥ 2rs
(cid:19)
(cid:17)
(cid:16)−2r1/2

q

Finally, we bound (cid:107)y(cid:107)∞ using (49) and (50):
|bT
1 e|
rq + rs

(cid:107)y(cid:107)∞ ≤ 1
rs

(cid:107)b1(cid:107)∞ +

(cid:18)

with probability at least 1 − 14rq exp
q (cid:54)= s, we conclude that

(cid:107)y(cid:107)∞ ≤ 4ˆr−1/4

with probability at least 1 − 14˜r exp(cid:0)−2ˆr1/2(cid:1). An identical argument shows that
with probability at least 1 − 14˜r exp(cid:0)−2ˆr1/2(cid:1). We conclude that
with probability at least 1 − 28˜r exp(cid:0)−2ˆr1/2(cid:1).

(cid:107)yq,s(cid:107)∞ + (cid:107)zq,s(cid:107)∞ ≤ 9ˆr−1/4

(cid:107)zq,s(cid:107)∞ ≤ 5ˆr−1/4

(51)

(52)

(53)

B Proof of Proposition 3.2

We next prove Proposition 3.2.
Proof: We follow the proof of Lemma 4.3 given by [3]. Fix q ∈ {1, . . . , k} and i ∈ Cq. It follows
from (22) that

(cid:88)
for each i ∈ Cq. Applying (31) with S =(cid:80)
(cid:88)

j∈Cq

λi =

wij − 1
2rq

eT W Cq,Cse − µ
2

wij and  = r3/4

q

j∈Cq
wij ≥ αrq − r3/4

yields

with probability at least 1 − 2 exp(cid:0)−2ˆr1/2(cid:1). Moreover, by a similar argument,

j∈Cq

q

with probability at least 1 − 4 exp (−2ˆr). Combining the above inequalities shows that

λi ≥ αˆr − ˆr3/4 − 1
2

= αˆr − ˆr3/4 − 1
2

(cid:16)

√

αˆr +

ˆr

(cid:17) − γ ˆr

2

(cid:0)αrq +

(cid:1)

√

rq

1
2rq

eT W Cq,Cse ≤ 1
2

(cid:16)

√

αˆr +

ˆr

(cid:17) − µ

2

21

with probability at least 1 − 6 exp(cid:0)−2ˆr1/2(cid:1). Since γ > 0 by (7), this implies that if
then λi ≥ 0 with probability at least 1 − 6 exp(cid:0)−2ˆr1/2(cid:1). Applying the union bound over all
i = 1, 2, . . . , k shows that each entry of λCq is nonnegative with probability at least 1−6k exp(cid:0)−2ˆr1/2(cid:1)

 ≤ α − 2ˆr−1/4 − ˆr−1/2

(54)

γ

if  is chosen to satisfy (54).

Now ﬁx i ∈ Ck+1. From (23) and (27), we see that
≥ 1
2

α − µ
rq

λi =

1
2

(cid:19)

(cid:18)

(α − γ) .

Note that α − γ ≥ 0 if and only if  ≤ α/γ, which holds if
 ≤ α − 2ˆr−1/4 − ˆr−1/2

≤ α
γ

This proves that each entry of λCk+1 is nonnegative with probability 1 ≥ 1 − 6k exp(cid:0)−2ˆr1/2(cid:1) if  is

γ

.

chosen to satisfy (54).

C Proof of Lemma 3.2

In this appendix we prove Lemma 3.2.

Proof: We will make repeated use of the following lemma, which follows as a consequence of the
Matrix Bernstein Inequality, established by [29, Theorem 1.4].

Lemma C.1 Let A = [aij] ∈ Σn be a random symmetric matrix with i.i.d. mean zero entries aij
having variance σ2 and satisfying |aij| ≤ B. Then

Pr

(55)

(cid:88)

E(cid:2)X 2

ij

M =

1≤i≤j≤n

Proof: (of Lemma C.1) This result is a slight modiﬁcation of that established by [4, Lemma
4.4] and the proof will follow a similar path to that provided by [4]. Let the set {X ij}1≤i≤j≤n ∈ Σn
be deﬁned by

where el is the lth standard basis vector in Rn. Note that A = (cid:80) X ij. It is easy to see that

(cid:107)X ij(cid:107) ≤ |aij| ≤ B for all 1 ≤ i ≤ j ≤ n. Furthermore, by the independence of the entries of A,

X ij =

(cid:3) (eieT

ij

i + ejeT
j )

 = σ2nI.

(cid:111)(cid:111) ≤ n−5.

(cid:110)(cid:107)A(cid:107) > 6 max
(cid:110)
σ(cid:112)n log n, B log n
aij(eieT
E(cid:2)a2
(cid:3) eieT

j + ejeT
aiieieT
i ,

(cid:3) =

n(cid:88)

n(cid:88)

E(cid:2)a2

i +

i ),

ii

i=1

j=i+1

22

if i (cid:54)= j
if i = j,

(cid:19)

Substituting into the Matrix Bernstein inequality (see [29, Theorem 1.4]), shows that for all

Pr{(cid:107)A(cid:107) ≥ } ≤ n exp
(56)
n log n, B log n}. We consider the following cases. Begin by assuming

n log n ≥ B log n, or equivalently that B ≤ σ(cid:112)n/ log n. Using (56) we see that

Suppose that  = 6 max{σ
that σ

6σ2n + 2B

√

√

−

.

32

(cid:110)(cid:107)A(cid:107) ≥ 6σ(cid:112)n log n

(cid:111) ≤ n exp

Pr

≤ n exp

(cid:19)
(cid:19)

3 · 36σ2n log n
√

6σ2n + 2B · 6σ

n log n

18σ2n log n

σ2n + 2σ2n log(n)/ log n

(cid:19)

(cid:18)

(cid:18)
(cid:18)
(cid:18)

−

−

n log n < B log n, or equivalently that σ < B(cid:112)log(n)/n. Using (56) again

= n exp

3

− 18 log n

= n−5.

(cid:18)
− 3 · 36B2 log2 n
(cid:18)
6σ2n + 2B · 6B log n
−

18B2 log2 n

B2 log n + 2B2 log n

(cid:19)
(cid:19)

(cid:18)
(cid:18)

= n exp

= n exp

− 18B2 log2 n
σ2n + 2B2 log n
= n−5.

− 18 log n

(cid:19)

3

(cid:19)

√

Next, assume that σ
yields

Pr{(cid:107)A(cid:107) ≥ 6B log n} ≤ n exp

≤ n exp

This concludes the proof.

Before we continue with the derivation of the desired bound on (cid:107) ˜S1(cid:107), we note that the entries
[ ˜S1]ij of ˜S1 all satisfy |[ ˜S1]ij| ≤ 1 if we assume that wij ∈ [0, 1] for all i, j; note that an identical
argument establishes the result if we make the weaker assumption that the entries of W are bounded
with high probability. On the other hand, note that the entries of ˜S1 are not i.i.d. since the entries
of W are sampled from either Ω1 or Ω2. Therefore, we decompose ˜S1 as

˜S1 = ˆS1 + ˆS2 + ˆS3,

where ˆS1 = D1 − E [D1] for n × n matrix D1 with all entries independently sampled from Ω2;
ˆS2 = D2 − E [D2] is a k × k block diagonal matrix with the entries of each diagonal block of
D2 independently sampled from Ω1; and ˆS3 is a k × k block diagonal matrix with entries of each
diagonal block equal to the corresponding elements in ˆS1 but with ﬂipped sign.

We now bound ˆS1, ˆS2 and ˆS3; applying the triangle inequality will then yield a bound on (cid:107) ˜S1(cid:107).

Recall that the variance of the entries of ˆS1 is σ2

2. Using this together with (55), we get that

Hence, (cid:107)M(cid:107) = σ2n.

 > 0,

with probability at least 1 − n−5. Moreover, we have

(cid:107) ˆS1(cid:107) ≤ 6 max

σ2

(cid:107) ˆS3(cid:107) ≤ (cid:107) ˆS1(cid:107) ≤ 6 max

(cid:110)

(cid:111)
(cid:112)n log n, log n
(cid:110)
(cid:112)n log n, log n

σ2

23

(cid:111)

(57)

(58)

with probability at least 1 − n−5. It remains to bound (cid:107) ˆS2(cid:107). It is well-known that the operator
norm of a block diagonal matrix is equal to the largest of the operator norms of its diagonal blocks.
Hence, according to (55) we have

(59)
with probability at least 1 − ˜r−5. Applying (57), (58), (59), and the triangle inequality, we see that

(cid:107) ˆS2(cid:107) ≤ 6 max

(cid:110)
(cid:112)n log n, log n

σ1

(cid:111)
(cid:112)˜r log ˜r, log ˜r
(cid:110)
(cid:111)
(cid:111)
(cid:112)˜r log ˜r, log ˜r

+ 6 max

σ1

(cid:110)

(cid:107) ˜S(cid:107) ≤ 12 max

σ2
with probability at least 1 − 3˜r−5.

References

[1] E. Abbe, A. S. Bandeira, and G. Hall. Exact recovery in the stochastic block model.

Information Theory, IEEE Transactions on, 62(1):471–487, 2016.

[2] N. Ailon, Y. Chen, and X. Huan. Breaking the small cluster barrier of graph clustering. arXiv

preprint arXiv:1302.4549, 2013.

[3] B. P. Ames. Guaranteed clustering and biclustering via semideﬁnite programming.

Mathematical Programming, 147(1-2):429–465, 2014.

[4] B. P. Ames. Guaranteed recovery of planted cliques and dense subgraphs by convex relaxation.

Journal of Optimization Theory and Applications, 167(2):653–675, 2015.

[5] B. P. Ames and S. A. Vavasis. Nuclear norm minimization for the planted clique and biclique

problems. Mathematical programming, 129(1):69–89, 2011.

[6] B. P. Ames and S. A. Vavasis. Convex optimization for the planted k-disjoint-clique problem.

Mathematical Programming, 143(1-2):299–337, 2014.

[7] A. A. Amini and E. Levina. On semideﬁnite relaxations for the block model. arXiv preprint

arXiv:1406.5647, 2014.

[8] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2009.

[9] T. T. Cai, X. Li, et al. Robust and computationally feasible community detection in the

presence of arbitrary outlier nodes. The Annals of Statistics, 43(3):1027–1059, 2015.

[10] Y. Chen, A. Jalali, S. Sanghavi, and H. Xu. Clustering partially observed graphs via convex

optimization. The Journal of Machine Learning Research, 15(1):2213–2238, 2014.

[11] Y. Chen, S. Sanghavi, and H. Xu. Improved graph clustering. Information Theory, IEEE

Transactions on, 60(10):6440–6455, 2014.

24

[12] Y. Chen and J. Xu. Statistical-computational phase transitions in planted models: The

high-dimensional setting. In ICML, pages 244–252, 2014.

[13] I. S. Dhillon, Y. Guan, and B. Kulis. Kernel k-means: spectral clustering and normalized cuts.

In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery
and data mining, pages 551–556. ACM, 2004.

[14] O. Gu´edon and R. Vershynin. Community detection in sparse networks via grothendieck’s

inequality. Probability Theory and Related Fields, pages 1–25, 2015.

[15] B. Hajek, Y. Wu, and J. Xu. Achieving exact cluster recovery threshold via semideﬁnite

programming. In Information Theory (ISIT), 2015 IEEE International Symposium on, pages
1442–1446. IEEE, 2015.

[16] W. Hoeﬀding. Probability inequalities for sums of bounded random variables. J. American

Statistical Assoc., 58:13–30, 1962.

[17] P. W. Holland, K. B. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps. Social

networks, 5(2):109–137, 1983.

[18] J. Lei, A. Rinaldo, et al. Consistency of spectral clustering in stochastic block models. The

Annals of Statistics, 43(1):215–237, 2015.

[19] J. Leskovec, K. J. Lang, A. Dasgupta, and M. W. Mahoney. Statistical properties of

community structure in large social and information networks. In Proceedings of the 17th
international conference on World Wide Web, pages 695–704. ACM, 2008.

[20] J. Leskovec, K. J. Lang, A. Dasgupta, and M. W. Mahoney. Community structure in large

networks: Natural cluster sizes and the absence of large well-deﬁned clusters. Internet
Mathematics, 6(1):29–123, 2009.

[21] C. Mathieu and W. Schudy. Correlation clustering with noisy input. In Proceedings of the

twenty-ﬁrst annual ACM-SIAM symposium on Discrete Algorithms, pages 712–728. Society for
Industrial and Applied Mathematics, 2010.

[22] A. Nellore and R. Ward. Recovery guarantees for exemplar-based clustering. arXiv preprint

arXiv:1309.3256, 2013.

[23] A. Y. Ng, M. I. Jordan, Y. Weiss, et al. On spectral clustering: Analysis and an algorithm.

Advances in neural information processing systems, 2:849–856, 2002.

[24] S. Oymak and B. Hassibi. Finding dense clusters via “low rank + sparse” decomposition.

Arxiv preprint arXiv:1104.5186, 2011.

[25] J. Peng and Y. Wei. Approximating k-means-type clustering via semideﬁnite programming.

SIAM Journal on Optimization, 18(1):186–205, 2007.

25

[26] T. Qin and K. Rohe. Regularized spectral clustering under the degree-corrected stochastic

blockmodel. In Advances in Neural Information Processing Systems, pages 3120–3128, 2013.

[27] K. Rohe, S. Chatterjee, and B. Yu. Spectral clustering and the high-dimensional stochastic

blockmodel. The Annals of Statistics, 39(4):1878–1915, 2011.

[28] K. Rohe, T. Qin, and H. Fan. The highest dimensional stochastic blockmodel with a

regularized estimator. arXiv preprint arXiv:1206.2380, 2012.

[29] J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of

Computational Mathematics, 12(4):389–434, 2012.

[30] R. K. Vinayak, S. Oymak, and B. Hassibi. Sharp performance bounds for graph clustering via

convex optimization. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE
International Conference on, pages 8297–8301. IEEE, 2014.

26

