6
1
0
2

 
r
a

 

M
4
1

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
1
8
3
4
0

.

3
0
6
1
:
v
i
X
r
a

A Ranking Approach to Global Optimization

C´edric Malherbe
Emile Contal
Nicolas Vayatis
CMLA - ENS Cachan
CNRS - Universit´e Paris-Saclay
94 235 Cachan cedex, France

malherbe@cmla.ens-cachan.fr
contal@cmla.ens-cachan.fr
vayatis@cmla.ens-cachan.fr

Abstract

In this paper, we consider the problem of maximizing an unknown function f over a com-
pact and convex set X ⊂ Rd using as few observations f (x) as possible. We observe that
the optimization of the function f essentially relies on learning the induced bipartite rank-
ing rule of f . Based on this idea, we relate global optimization to bipartite ranking which
allows to address problems with high dimensional input space, as well as cases of functions
with weak regularity properties. The paper introduces novel meta-algorithms for global op-
timization which rely on the choice of any bipartite ranking method. Theoretical properties
are provided as well as convergence guarantees and equivalences between various optimiza-
tion methods are obtained as a by-product. Eventually, numerical evidence is given to show
that the main algorithm of the paper which adapts empirically to the underlying ranking
structure essentially outperforms existing state-of-the-art global optimization algorithms
in typical benchmarks.
Keywords: global optimization, ranking, statistical analysis, convergence rate bounds

1. Introduction

In many applications such as complex system design or hyperparameter calibration for learn-
ing systems, the goal is to optimize some output value of a non-explicit function with as few
evaluations as possible. Indeed, in such contexts, one has access to the function values only
through numerical evaluations by simulation or cross-validation with signiﬁcant computa-
tional cost. Moreover, the operational constraints generally impose a sequential exploration
of the solution space with small samples. The generic problem of sequentially optimizing
the output of an unknown and potentially non-convex function is often referred to as global
optimization (Pint´er (1991)), black-box optimization (Jones et al. (1998)) or derivative-free
optimization (Rios and Sahinidis (2013)). There are several algorithms based on vari-
ous heuristics which have been introduced in order to address complicated optimization
problems with limited regularity assumptions, such as genetic algorithms, model-based al-
gorithms, branch-and-bound methods... see Rios and Sahinidis (2013) for a recent overview.
This paper follows the line of the approaches recently considered in the machine learning
literature (Bull (2011); Munos (2011); Sergeyev et al. (2013)). These approaches extend
the seminal work on Lipschitz optimization of Hansen et al. (1992); Jones et al. (1993) and
they led to signiﬁcant relaxations of the conditions required for convergence, e.g. only the

1

existence of a local smoothness around the optimum is required (Munos (2011); Grill et al.
(2015)). More precisely, in the work of Bull (2011) and Munos (2011), speciﬁc conditions
have been identiﬁed to derive a ﬁnite-time analysis of the algorithms. However, these
guarantees do not hold when the unknown function is not assumed to be locally smooth
around (one of) its optimum.

In the present work, we propose to explore concepts from ranking theory based on over-
laying estimated level sets (Cl´emen¸con et al. (2010)) in order to develop global optimization
algorithms that do not rely on the smoothness of the function. The idea behind this ap-
proach is simple: even if the unknown function presents arbitrary large variations, most of
the information required to identify its optimum may be contained in its induced ranking
rule, i.e. how the level sets of the function are included one in another. To exploit this idea,
we introduce a novel optimization scheme where the complexity of the function is character-
ized by the underlying pairwise ranking which it deﬁnes. Our contribution is twofold: ﬁrst,
we introduce two novel global optimization algorithms that learn the ranking rule induced
by the unknown function with a sequential scheme, and second, we provide mathematical
results in terms of statistical consistency and convergence to the optimum. Moreover, the
algorithms proposed lead to eﬃcient implementation and display good performance on the
classical benchmarks for global optimization as shown at the end of the paper.

The rest of the paper is organized as follows. In Section 2 we introduce the framework
and give the main deﬁnitions. In Section 3 we introduce and analyze the RankOpt algo-
rithm which requires a prior information on the ranking structure underlying the unknown
function. In Section 4, an adaptive version of the algorithm is presented. Companion results
which establish the equivalence between learning algorithms and optimization procedures
are discussed in Section 5 as they support implementation choices. Finally, the adaptive
version of the algorithm is compared to other global optimization algorithms in Section 6.
All proofs are postponed to the Appendix section.

2. Global optimization and ranking structure

2.1 Setup and notations

Setup. We consider the problem of sequentially maximizing an unknown real-valued func-
tion f : X → R where X ⊂ Rd is a compact and convex set. The objective is to identify
some point

x(cid:63) ∈ arg max

f (x)

x∈X

with a minimal amount of function evaluations. The setup we consider is the following:
at each iteration t = 1 . . . n − 1, an algorithm selects an evaluation point Xt+1 ∈ X which
depends on the previous evaluations {(Xi, f (Xi))}t
i=1 and receives the evaluation of the
unknown function f (Xt+1) at this point. After n iterations, the algorithm returns the
argument of the highest value observed so far:

Xˆın where ˆın ∈ arg max

i=1...n

f (Xi).

The analysis provided in the paper considers that the number n of evaluation points is not
ﬁxed and it is assumed that function evaluations are noiseless.

2

Notations. For any x = {x1 . . . xd} ∈ Rd, we deﬁne the standard (cid:96)2-norm as (cid:107)x(cid:107)2
i=1 x2

(cid:80)d
2 =
i , we denote by (cid:104)·,·(cid:105) the corresponding inner product and we denote by B(x, r) =
{x(cid:48) ∈ Rd : (cid:107)x − x(cid:48)(cid:107)2 ≤ r} the (cid:96)2-ball centered in x of radius r ≥ 0 . For any bounded set
X ⊂ Rd, we deﬁne its inner-radius as rad(X ) = max{r > 0 : ∃x ∈ X s.t. B(x, r) ⊆ X}, its
diameter as diam(X ) = max(x,x(cid:48))∈X 2 (cid:107)x − x(cid:48)(cid:107)2 and we denote by µ(X ) its volume where µ
stands for the Lebesgue measure. We denote by C0(X , R) the set of continuous functions
deﬁned on X taking values in R and we denote by Pk(X , R) the set of (multivariate)
polynomial functions of degree k > 1 deﬁned on X . Finally, we denote by U(A) the uniform
distribution over a bounded measurable domain A and we denote by I{·} the indicator
function taking values in {0, 1}.

2.2 The ranking structure of a real-valued function

In this section, we introduce the ranking structure as a complexity characterization for a
general real-valued function to be optimized. First, we observe that real-valued functions
induce an order relation over the input space X , and the underlying ordering induces a
ranking rule which records pairwise comparisons between evaluation points.
Deﬁnition 1 (Induced ranking rule) The ranking rule rf : X × X → {0,±1} induced
by a function f : X → R is deﬁned by:

+1

0
−1

f (x) > f (x(cid:48))
f (x) = f (x(cid:48))
f (x) < f (x(cid:48))

if
if
if

rf (x, x(cid:48)) =

for all (x, x(cid:48)) ∈ X 2.

The key argument of the paper is that the optimization of any weakly regular real-
valued function only depends on the nested structure of its level sets. Hence there is an
equivalence class of real-valued functions that share the same induced ranking rule as shown
by the following proposition.
Proposition 2 (Ranking rule equivalence) Let h ∈ C0(X , R) be any continuous func-
tion. Then, a function f : X → R shares the same induced ranking rule with h, i.e., ∀(x, x(cid:48)) ∈
X 2, rf (x, x(cid:48)) = rh(x, x(cid:48)) if and only if there exists a strictly increasing (not necessary con-
tinuous) function ψ : R → R such that h = ψ ◦ f .

Proposition 2 states that even if the unknown function f admits non-continuous or large
variations, up to a transformation ψ, there might exist a simpler function h = ψ ◦ f that

Figure 1: Three functions that share the same ranking rule

3

shares the same induced ranking rule. Figure 2.2 gives an example of three functions that
share the same ranking while they display highly diﬀerent regularity properties. As a second
example, we may consider the problem of maximizing the function f (x) = 1 − 1/|ln(x)|
if x (cid:54)= 0 and 1 otherwise over X = [0, 1/2]. In this case, the function f is not ’smooth’
around its unique global maximizer x(cid:63) = 0 but shares the same induced ranking rule with
h(x) = −x over X .

We can now introduce a complexity characterization of real-valued functions of a set
X through the complexity class of its induced ranking rule. We call this class a ranking
structure.

Deﬁnition 3 (Continuous Ranking Structure and Continuous Ranking Rules)
Let f be a real-valued function. We say that f has a continuous ranking rule if rf ∈ R∞
where R∞ := {rh : h ∈ C0(X , R)} denotes the set of continuous ranking rules (i.e. ranking
rules induced by continuous functions).

In the continuation of this deﬁnition, we further introduce two examples of more stringent
ranking structures.

Deﬁnition 4 (Polynomial Ranking Rules) The set of polynomial ranking rules of de-
gree k ≥ 1 is deﬁned as

RP(k) := {rh : h ∈ Pk(X , R)}.

We point out that even a polynomial function of degree k may admit a lower degree poly-
nomial ranking rule. For example, consider the polynomial function f (x) = (x2 − 3x + 1)9.
Since f (x) = ψ(x2− 3x) where ψ : x (cid:55)→ (x + 1)9 is a strictly increasing function, the ranking
rule induced by f is a polynomial ranking rule of degree 2.

The second class of ranking structures we introduce is a class of non-parametric ranking

rules.
Deﬁnition 5 (Convex Ranking Rules) The set of convex ranking rules of degree k ≥ 1
is deﬁned as
RC(k) := {r ∈ R∞ such that

for any x ∈ X , the set

{x(cid:48) ∈ X : r(x(cid:48), x) ≥ 0} is a union of k convex sets}.

It is easy to see that the ranking rule of a function f is a convex ranking rule of degree k if
and only all the level sets of the function f are unions of at most k convex sets.

2.3 Identiﬁability and regularity

We now state two conditions that will be used in the theoretical analysis: the ﬁrst condition
is about the identiﬁability of the maximum of the function and the second is about the
regularity of f around its maximum.
Condition 1 (Identifiability) The maximum of a function f : X → R is said to be
identiﬁable if for any ε > 0 arbitrary small,

µ({x ∈ X : f (x) ≥ max

x∈X f (x) − ε}) > 0.

4

Condition 1 prevents the function from having a jump on its maximum. It will be useful to
state asymptotic results of the type f (Xˆın) → maxx∈X f (x) when n → +∞.
Condition 2 (Regularity of the level sets) A function f : X → R has (cα, α)-
regular level sets for some cα > 0, α ≥ 0 if:
1. The global optimizer x(cid:63) ∈ X is unique
2. For any y ∈ Im(f ), the iso-level set f−1(y) = {x ∈ X : f (x) = y} satisﬁes

max

x∈f−1(y)

(cid:107)x(cid:63) − x(cid:107)2 ≤ cα · min
x∈f−1(y)

(cid:107)x(cid:63) − x(cid:107)1/(1+α)

2

.

Condition 2 guarantees that the points associated with high evaluations are close to the
unique optimizer with respect to the Euclidean distance. However, note that for any iso-
level set f−1(y) with ﬁnite distance to the optimum, the condition is satisﬁed with α = 0 and
cα = diam(X ) / minx∈f−1(y) (cid:107)x(cid:63) − x(cid:107)2. Hence, this condition concerns the local behavior of
the level sets when minx∈f−1(y) (cid:107)x(cid:63) − x(cid:107)2 → 0. As an example, the iso-level sets of three
simple functions satisfying the condition with diﬀerent values of α are shown in Figure 2.

Figure 2: Illustration of the regularity of the level sets on three simple functions. Left:
f (x1, x2) = −x2
2) where
α = 1/2. Right: f (x1, x2) = −x4

2 where α = 0. Middle: f (x1, x2) = exp (−|x1|3 − 1.4x2

1 − 1.4x2

1 − 1.4x2

2 where α = 1.

3. Optimization with ﬁxed ranking structure

In this section, we consider the problem of optimizing an unknown function f given the
prior knowledge that its ranking rf belongs to a given ranking structure R ⊆ R∞.

3.1 The RankOpt algorithm

The input of the RankOpt algorithm (displayed in Figure 3) are a number n of iterations,
the unknown function f , a compact and convex set X ⊂ Rd and a ranking structure R ⊆
R∞. At each iteration t < n, a point Xt+1 is uniformly sampled over X and the algorithm
decides, whether or not, to evaluate the function at this point. The decision rule involves
the active subset of R which contains the ranking rules that are consistent with the ranking
rule induced by f over the points sampled so far. We thus set Rt = {r ∈ R : Lt(r) = 0}
where Lt is the empirical ranking loss:

I{rf (Xi, Xj) (cid:54)= r(Xi, Xj)} .

5

(cid:88)

t(t + 1)

1≤i<j≤t

Lt(r) :=

2

x(cid:63)···1. Initialization: Let X1 ∼ U(X )

Evaluate f (X1), t ← 1
R1 ← R, ˆı1 ← 1

2. Iterations: Repeat while t < n:

Let Xt+1 ∼ U(X )
If there exists r ∈ Rt such that r(Xt+1, Xˆıt) ≥ 0 {Decision rule}

Evaluate f (Xt+1), t ← t + 1
Rt ← {r ∈ R : Lt(r) = 0}
ˆıt ∈ arg maxi=1...t f (Xi)

3. Output: Return Xˆın

Figure 3: The RankOpt(n, f,X ,R) algorithm

As a direct consequence of the deﬁnition of the active subset, if there does not exist any
ranking rule r ∈ Rt satisfying r(Xt+1, Xˆın) ≥ 0, it also implies that rf (Xt+1, Xˆıt) = −1
which means that f (Xt+1) < f (Xˆıt). Thus, the algorithm never evaluates the function at
a point that will not return certainly an evaluation at least equal to the highest evaluation
f (Xˆıt) observed so far.

written in terms of unions of level sets, i.e., Xt =(cid:83)

Remark 6 (Approximation of the Level sets) It is noteworthy that the sampling
area implicitly deﬁned by the decision rule Xt = {x ∈ X : ∃r ∈ Rt s.t. r(x, Xˆıt) ≥ 0} can be
{x ∈ X : f (x) ≥ f (Xˆıt)}. Thus,
since rf might be any ranking rule in Rt, the sampling area Xt is also the smallest subset
that contains certainly the true level set of the highest evaluation observed so far f (Xˆıt).

f : rf∈Rt

Remark 7 (Connection with active learning) The proposed algorithm might be seen
as an extension to ranking of the baseline active learning algorithm introduced in Cohn
et al. (1994) and further analyzed in Hanneke (2011). However, in active learning we aim
at estimating a binary classiﬁer h : X → {±1} where the goal in global optimization is to
estimate the winner of a tournament deriving from the ranking rule rf : X × X → {0,±1}.

Remark 8 (Adaptation to noisy evaluations) Following the steps of Dasgupta (2011)
and Hanneke (2011), the algorithm can be extended to noisy evaluations by considering a
relaxed version of the active subset Rδ,t = {r ∈ R : Lt(r) ≤ minr∈R Lt(r) + U B(δ, t)} where
U B(δ, t) comes out of some standard generalization bound on |Lt(rf ) − minr∈R Lt(r)|.

3.2 Convergence analysis

We now state some convergence properties of the RankOpt algorithm. The results are
stated in a probabilistic framework. The source of randomness comes from the algorithm
itself – which generates uniform random variables – and not from the evaluations which are
assumed noiseless. The next result will be important in order to formulate the consistency
property of the algorithm.

6

Proposition 9 Fix any n ∈ N(cid:63), let X ⊂ Rd be any compact and convex set, let R ⊆ R∞
be any ranking structure and let f : X → R be any function such that rf ∈ R. Then, if Xˆın
denotes the random output of the RankOpt(n, f,X ,R) algorithm, we have that ∀y ∈ R,

P(f (Xˆın) ≥ y) ≥ P(maxi=1...n f (X(cid:48)

i) ≥ y)

where {X(cid:48)

i}n
i=1 are n independent random variables, uniformly distributed over X .

Combining the previous proposition with the identiﬁability condition gives the following
asymptotic result.

Corollary 10 (Consistency) Consider the same notations and assumptions as in Propo-
sition 9. Then, if the function f has an identiﬁable maximum (Condition 1), we have that

f (Xˆın)

P−→ max

x∈X f (x).

We now provide our ﬁrst ﬁnite-sample bound on the distance (cid:107)x(cid:63) − Xˆın(cid:107)2 between the exact
solution and its approximation.

Theorem 11 (Upper bound) Consider the same assumptions as in Proposition 9. Then,
if function f has (cα, α)-regular level sets (Condition 2), for any n ∈ N(cid:63) and any δ ∈ (0, 1),
with probability at least 1 − δ,

(cid:107)x(cid:63) − Xˆın(cid:107)2 ≤ Cα,X ·

where Cα,X = c(2+α)/(1+α)

α

diam(X )1/(1+α)2

.

(cid:18) ln(1/δ)

(cid:19) 1

d(1+α)2

n

More surprisingly, a lower bound can also be derived by connecting the RankOpt
algorithm with a theoretical algorithm which uses the prior knowledge of the level sets of
the unknown function – Pure Adaptive Search (Zabinsky and Smith (1992)). The next
proposition shows the link between the two algorithms.
Proposition 12 Fix any n ∈ N(cid:63) and let {X (cid:63)
distributed as the Markov process deﬁned by:

i=1 be a sequence of n random variables

i }n

(cid:40)

1 ∼ U(X )
X (cid:63)
t+1| X (cid:63)
X (cid:63)
t = {x ∈ X : f (x) ≥ f (X (cid:63)

t ∼ U(X (cid:63)
∀t ∈ {1 . . . n − 1}
t )
t )} denotes the level set of f (X (cid:63)

where X (cid:63)
assumptions as in Proposition 9, we have that ∀y ∈ R,
P(f (Xˆın) ≥ y) ≤ P(f (X (cid:63)

n) ≥ y).

t ). Then, under the same

Equipped with Proposition 12, we are ready to establish our second ﬁnite-time bound.

7

Theorem 13 (Lower bound) Consider the same assumptions as in Proposition 9. Then,
if the function f has (cα, α)-regular level sets (Condition 2), for any δ ∈ (0, 1) and any
n ∈ N(cid:63), with probability at least 1 − δ,
(cid:16)
√

(cid:17)

n+

2n ln(1/δ)+ln(1/δ)

≤ (cid:107)x(cid:63) − Xˆın(cid:107)2

Cα,X · e
−(1+α)(2+α)
where Cα,X = c
α

− (1+α)2

d

rad(X )(1+α)2

.

Remark 14 (On the performance criterion) Observe that the level set assumption
– used in Theorem 11 and Theorem 13 – is invariant to any strictly increasing composition
ψ (i.e., if f has (cα, α)-regular level sets, so has ψ ◦ f ). It implies that the bounds derived
on the distance (cid:107)x(cid:63) − Xˆın(cid:107)2 hold independently of the smoothness of the unkown function.

4. Adaptive algorithm and stopping time analysis

We now consider the problem of optimizing a function f when no information is available
on its induced ranking rule rf .

4.1 The AdaRankOpt algorithm

The AdaRankOpt algorithm (shown in Figure 4.1) is an extension of the RankOpt algo-
rithm which involves model selection following the principle of Structural Risk Minimization
Vapnik (1992). We consider a parameter p ∈ (0, 1) and a nested sequence of ranking struc-
tures {Rk}k∈N(cid:63) satisfying

R1 ⊂ R2 ⊂ ··· ⊂ R∞.

(1)

1. Initialization: Let X1 ∼ U(X )

Evaluate f (X1), t ← 1
R ← R1, ˆı1 ← 1

2. Iterations: Repeat while t < n

Let Bt+1 ∼ B(p)
If Bt+1 = 1 {Exploration}
If Bt+1 = 0 {Exploitation}

Let Xt+1 ∼ U(X )
Let Xt+1 ∼ U({x ∈ X : ∃r ∈ R s.t. r(x, Xˆıt) ≥ 0})
Evaluate f (Xt+1), t ← t + 1
(cid:98)kt ← min{k ∈ N(cid:63) : minr∈Rk Lt(r) = 0} {Model Selection}
ˆıt ∈ arg maxi=1...t f (Xi)
R ← {r ∈ R(cid:98)kt

: Lt(r) = 0}

3. Output: Return Xˆın

Figure 4: The AdaRankOpt(n, f,X , p,{Rk}k∈N(cid:63)) algorithm

8

The algorithm is initialized by evaluating the function at a point X1 uniformly dis-
tributed over X and by considering the smallest ranking structure R1 of the sequence.
At each iteration t < n, a Bernoulli random variable Bt+1 of parameter p is sampled. If
Bt+1 = 1, the algorithm explores the space by evaluating the function at a point uniformly
sampled over X . If Bt+1 = 0, the algorithm exploits the previous evaluations by making
an iteration of the RankOpt algorithm with the smallest ranking structure Rˆkt
of the se-
made, the index (cid:98)kt+1 is updated. The parameter p thus drives the trade-oﬀ between the
quence that probably contains the true ranking rf . Once a new evaluation f (Xt+1) has been

exploitation phase and the exploration phase – which prevents the algorithm from getting
stuck in a local maximum.

4.2 Theoretical properties of AdaRankOpt

We start by casting the consistency result for the algorithm.
Proposition 15 (Consistency) Fix any p ∈ (0, 1) and let {Rk}k∈N(cid:63) be any nested se-
quence of ranking structures (1). Then, for any function f : X → R which has an iden-
tiﬁable maximum (Condition 1), if Xˆın denotes the random output of the AdaRankOpt
(n, f,X , p,{Rk}k∈N(cid:63)) algorithm, we have that

f (Xˆın)

P−→ max

x∈X f (x).

Informally, Proposition 15 states that even if the algorithm is poorly tuned (e.g., an inap-
propriate sequence of ranking structures or a bad choice of p), it remains consistent over
the class of functions that have an identiﬁable maximum.

To derive a ﬁnite-time analysis of the algorithm, we start to investigate the number of

iterations required to identify a ranking structure that contains the true ranking rule.
Deﬁnition 16 (Stopping Time) Let k(cid:63) = min{k ∈ N(cid:63) : rf ∈ Rk} be the index of the

smallest ranking structure that contains the true ranking rule and let {(cid:98)kt}t∈N(cid:63) be the sequence

of random variables driving the model selection, deﬁned in the algorithm. We deﬁne the
stopping time which corresponds to the number of iterations required to identify the index
k(cid:63) as

In order to bound τk(cid:63), we need to control the complexity of the sequence of ranking struc-
tures. Let us denote by L(r) = P(rf (X, X(cid:48)) (cid:54)= r(X, X(cid:48))) the true ranking loss where (X, X(cid:48))
is a couple of independent random variables uniformly distributed over X and deﬁne the
Rademacher average of a ranking structure R as

τk(cid:63) := min{t ∈ N(cid:63) :(cid:98)kt = k(cid:63)}.
i · I(cid:8)rf (Xi, X(cid:98)n/2(cid:99)+i) (cid:54)= r(Xi, X(cid:98)n/2(cid:99)+i)(cid:9)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:98)n/2(cid:99)(cid:88)

i=1

Rn(R) := sup
r∈R

1

(cid:98)n/2(cid:99)

i=1 are n independent copies of X ∼ U(X ) and {i}(cid:98)n/2(cid:99)

are (cid:98)n/2(cid:99) indepen-
where {Xi}n
dent Rademacher random variables (i.e., random symmetric sign variables), independent of
{Xi}n

i=1

i=1.

9

∀n ∈ N(cid:63), the Rademacher complexity of Rk(cid:63)−1 satisﬁes E [Rn(Rk(cid:63)−1)] ≤(cid:112)V /n. Then, for

Proposition 17 (Stopping Time Upper Bound) Assume that the index k(cid:63) > 1 is ﬁnite,
assume that inf r∈Rk(cid:63)−1 L(r) > 0 and assume that there exists a constant V > 0 such that
any δ ∈ (0, 1), with probability at least 1 − δ,

(cid:18) V + ln(2/δ)

inf r∈Rk(cid:63)−1 L(r)2

(cid:19)

.

τk(cid:63) ≤ 10
p

·

In the situation described above, a ranking structure which contains the true ranking rule
can be ideniﬁed in a ﬁnite number of iterations. It is then possible to recover an upper
bound similar to the one of Theorem 11 – where the structure structure is assumed to be
known.

Theorem 18 (Upper Bound) Consider the same assumptions as in Proposition 17 and
assume further that the function f has (cα, α)-regular level sets (Condition 2). Then, if Xˆın
denotes the random output of AdaRankOpt(n, f,X , p,{Rk}k∈N(cid:63)), for any δ ∈ (0, 1) and
any n > nδ = (cid:98)10(V + ln(4/δ))/(p · inf r∈Rk(cid:63)−1 L(r)2)(cid:99), we have that

(cid:107)Xˆın − x(cid:63)(cid:107)2 ≤ Cα,X ·

(cid:19) 1

d(1+α)2

(cid:18) ln(2/δ)

n − nδ

with probability at least 1 − δ where Cα,X is the same constant as in Theorem 11.

Remark 19 (On the complexity assumption) As pointed out in Cl´emen¸con (2011)
(Remark 2 therein), standard VC-type arguments can be used in order to bound E [Rn(Rk(cid:63)−1)].
If the set of functions Fk(cid:63)−1 = {(x, x(cid:48)) ∈ X 2 (cid:55)→ I{rf (x, x(cid:48)) (cid:54)= r(x, x(cid:48))} : r ∈ Rk(cid:63)−1)} is a VC

major class with ﬁnite VC dimension V , then E [Rn(Rk(cid:63)−1))] ≤ c ·(cid:112)V /n for a universal

constant c > 0. In particular, this covers the case of polynomial ranking rules.

Remark 20 (Related work) To the best of our knowledge, this is the ﬁrst analysis of
an optimization algorithm which learn the ranking rule induced by the unknown function.
However, for a diﬀerent approach to global optimization, we refer to the work of Munos
(2011) where the function is assumed to be locally smooth around (one of ) its optimum.
In the latter work, ﬁnite-time bounds on the diﬀerence maxx∈X f (x) − f (Xˆın) are obtained
when the smoothness is known (DOO) and the smoothness is unknown (SOO).

5. Computational aspects

We discuss here some technical aspects involved in the practical implementation of the
AdaRankOpt algorithm.

5.1 General ranking structures
Fix any nested sequence of ranking structures {Rk}k∈N(cid:63) and suppose that we have col-
lected a sample {(Xi, f (Xi))}n
pling Xn+1 uniformly over the non-trivial subset Xn = {x ∈ X : ∃r ∈ R(cid:98)kn
i=1 of n evaluations We address the questions of (i) sam-
0 and r(x, Xˆın) ≥ 0} and (ii) updating the index (cid:98)kn+1 once f (Xn+1) has been evaluated.
s.t. Ln(r) =

10

We start to show that both these steps can be done using a single generic procedure that
tests, for a given k ∈ N(cid:63), if there exists a ranking rule r ∈ Rk that perfectly ranks a sample
of n + 1 evaluations, i.e.,

min
r∈Rk

Ln+1(r) = 0.

(2)
(i) Sampling X ∼ U(X ) until X ∈ Xn allows to sample uniformly over Xn. By deﬁnition
of the non-trivial subset, we know that X ∈ Xn if and only if there exists a ranking rule
r ∈ R(cid:98)kn
∩{r : Ln(r) = 0} such that r(X, Xˆın) = 0 or r(X, Xˆın) = 1. Rewriting the previous
statement in terms of minimal error gives that X ∈ Xn if and only if:

Ln+1(r) = 0 where the empirical ranking loss is taken over the sample

- either minr∈R(cid:98)kn
i=1 ∪ (X, f (Xˆın)) (case r(Xˆın, X) = 0),
{(Xi, f (Xi))}n
Ln+1(r) = 0 where Ln+1 is taken over the sample {(Xi, f (Xi))}n
i=1 ∪
- or minr∈R(cid:98)kn
(X, f (Xˆın) + c) where c > 0 is any strictly positive constant (case r(Xˆın, X) = 1).
quence of ranking structures, it implies that the sequence {(cid:98)kt}t∈N(cid:63) is stricly increasing so
we have that(cid:98)kn+1 =(cid:98)kn + min{i ∈ N(cid:63) : minr∈R(cid:98)kn+i
i=1 . Hence, (cid:98)kn+1 can be updated by sequentially
is taken over the sample {(Xi, f (Xi))}n+1
testing if minr∈R(cid:98)kn+i

(ii) Assume now that f (Xn+1) has been evaluated. Since {Rk}k∈N(cid:63) forms a nested se-
Ln+1(r) = 0} where the empirical loss

Ln+1(r) = 0 for i = 0, 1, 2 . . .

As stated above, for any nested sequence of ranking structures, both the steps (i) and (ii)
can be done using a generic procedure that tests if (2) holds true. We now provide some
equivalences that can be used in order to design such a procedure for the two classes of
ranking structures introduced in Section 2. For simplicity, we will assume in the sequel that
all the evaluations of the sample are distinct, i.e.,

f (X(1)) < f (X(2)) < ··· < f (X(n+1))

(3)

where (1), (2), . . . (n + 1) denote the indexes of the corresponding reordering.

d

where dim(φk) =(cid:0)k+d

(cid:1) − 1, e.g., φ2(x1, x2) = {x1, x2, x1x2, x2

5.2 Polynomial ranking rules
Consider the sequence of polynomial ranking rules {RP(k)}k∈N(cid:63) and let φk : Rd → Rdim(φk)
be the function that maps Rd into the corresponding polynomial feature space of degree k
2}. We provide here some
equivalences that can be exploited in order to implement the algorithm with the polynomial
rankings. As a starting point, we start to show that (2) holds true if and only if the set of
points {φk(X(i+1)) − φk(X(i))}n
Proposition 21 Fix any degree k ∈ N(cid:63) and assume that the n+1 evaluations of the sample
are distinct (3). Then, there exists a polynomial ranking rule of degree k that perfectly ranks
the sample if and only if there exists an axis ω ∈ Rdim(φk) satisfying:
(cid:104)ω, φk(X(i+1)) − φk(X(i))(cid:105) > 0, ∀i ∈ {1 . . . n}.

i=1 satisﬁes some property.

1, x2

Since only the existence of a separating axis is required, one can then use the following
lemma, presented in the usual binary classiﬁcation framework and illustrated in Figure 5.2.

11

Lemma 22 (Separability) Let {(xi, yi)}n
i=1 be any set of binary classiﬁcation where
(xi, yi) ∈ Rd × {±1}, let CH{yi · xi}n
i=1 and let us de-
note by (cid:126)0 = {0 . . . 0} ∈ Rd the zero vector. Then, there exists a separating axis ω ∈ Rd
satisfying:

i=1 be the convex hull of {yi · xi}n

yi · (cid:104)ω, xi(cid:105) > 0, ∀i ∈ {1 . . . n}

if and only if

(cid:126)0 /∈ CH{yi · xi}n

i=1.

Thus, applying Lemma 22 on the sample {(φk(X(i+1)) − φk(X(i)), +1)}n
vertices representation of the associated convex hull leads us to the next equivalence.

i=1 and using the

Corollary 23 Under the same assumptions as in Proposition 21, there exists a polynomial
ranking rule of degree k that perfectly ranks the sample if and only if the polyhedral set
deﬁned by

(cid:110)

λ ∈ Rn : MkλT = (cid:126)0, (cid:104)(cid:126)1, λ(cid:105) = 1, λ (cid:23) (cid:126)0

(cid:111)

is empty where Mk is the (dim(φk), n)-matrix with its i-th column equal to (φk(X(i+1)) −
φk(X(i)))T and (cid:23) stands for the inequality ≥ component-wise (i.e., x (cid:23) x(cid:48) ∈ Rn ⇔ ∀i ∈
{1 . . . n}, xi ≥ x(cid:48)
i).

Notice that, in practice, the problem of testing the emptiness of a polyhedral set admits a
tractable solution, as discussed in Remark 26.

5.3 Convex ranking rules
Consider the non-parametric sequence of convex ranking rules {RC(k)}k∈N(cid:63). We provide here
two equivalences related to the practical implementation of the algorithm with the convex
rankings. First, following the steps of Cl´emen¸con and Vayatis (2010) allows to formulate
the next equivalence.
Proposition 24 (Overlaying classifiers) Fix any degree k ∈ N(cid:63), let d = 1 and assume
that all the evaluations are distinct. Then, (2) holds true if and only if there a exists a
sequence of classiﬁers {hi}n+1

i=1 of the form hi(x) =(cid:80)k

I{li,m ≤ x ≤ ui,m} satisfying:

m=1

1. hi(X(j)) = I{(j) ≥ i}, ∀(i, j) ∈ {1 . . . n + 1}2,
2. h1 ≥ h2 ≥ ··· ≥ hn+1.

Figure 5: Illustration of Lemma 22. Left: A separable sample {(xi, yi)}n
sample {yi · xi}n

i=1. Right: The convex hull of {yi · xi}n

i=1 next to the zero vector.

i=1. Middle: The

12

~0The problem of overlaying classiﬁers admits a tractable solution when d = 1. In the speciﬁc
case where d > 1 and k = 1, testing the existence of nested convex classiﬁers is equivalent
to testing the emptiness of a cascade of polyhedral sets, as shown in the next proposition.
Proposition 25 Fix any d ∈ N(cid:63), let k = 1 and assume that all the evaluations of the
sample are distinct. Then, (2) holds true if and only if for each k ∈ {1 . . . n} the polyhedral
set deﬁned by:

(cid:111)
(n+1−k), (cid:104)(cid:126)1, λ(cid:105) = 1, λ (cid:23) (cid:126)0

(cid:110)

λ ∈ Rk : Mkλ = X T

is empty where Mk is the (d, k)-matrix where its i-th column is equal to X T

(n+2−i).

Remark 26 (Algorithmic aspects) Testing the emptiness of a polyhedral set can be
seen as the problem of ﬁnding a feasible point of a linear program.
In particular, this
problem can be solved using the simplex algorithm. For further details, we refer to Chapter
11.4 in Boyd and Vandenberghe (2004) where practical examples as well as algorithmic
solutions are discussed.

6. Numerical experiments

The purpose of this section is to show that the main algorithm of the paper is empiri-
cally competitive with existing state-of-the-art global optimization algorithms. To do so,
we compare the general performances of the AdaRankOpt algotihm with four diﬀerent
algorithms, developed from various approaches of global optimization:

• BayesOpt (Martinez-Cantin (2014)) is a Bayesian optimization algorithm. It uses a
distribution over functions to build a surrogate model of the unkown function. The
parameters controlling the distribution are estimated during the optimization process.
• CMA-ES (Hansen (2006)) is an evolutionary algorithm. At each iteration, the new
evaluation points are sampled according to a multivariate normal distribution. The
covariance matrix of the distribution is estimated over the previous evaluations.

• CRS (Kaelo and Ali (2006)) is a controlled random search with local mutations. It
starts with a random population and randomly evolves these points by an heuristic
rule. The algorithm was taken from the NLOpt library (Johnson (2014)).

• DIRECT (Jones et al. (1993)) is a Lipschitz optimization algorithm where the Lip-
schitz constant is unknown. It uses a deterministic splitting technique of the search
space. The algorithm was also taken from the NLOpt library.

For a fair comparison, the tuning parameters were all set to default and the AdaRankOpt
algorithm was used with the polynomial ranking rules and a parameter p ﬁxed to 1/10.

In the ﬁrst series of experiments (displayed in Figure 6), four standard optimization
problems were considered and the the horizon n was set to 100 evaluations. For each
problem, the task consists in maximizing:
(a) The McCormick function f (x1, x2) = − sin(x1 + x2) − (x1 − x2)2 + 1.5x1 − 2.6x2 − 1

over the domain X = [−1.5, 4] × [−3, 4] which presents two local maxima.

13

(a)

(b)

(c)

(d)

Figure 6: Diﬀerence between the maximum of f and the expectation of its approximation
maxx∈X f (x)− E [f (Xˆıt)] in terms of iteration t = 1 . . . 100. The expectation was estimated
by running 1000 times each algorithm.

(b) A perturbed version of the Styblinski-Tang function f (x1, x2) = (cid:80)2

5xi)/2 + sin(x1 + x2) which has four local maxima over X = [−5, 5]2.

i=1(16x2

i − x4

i −

(c) The Levi N.13 function f (x) = − sin2(3πx1) − (x1 − 1)2(1 + sin2(3πx2)) − (y − 1)(1 +
sin2(2πx2)) over the domain X = [−10, 10]2. The Levi function has a strong global
structure but presents about 100 local maxima.

(d) The Himmelblau function f (x1, x2) = −(x2

1+x2−11)2−(x1+x2

2−7)2 over X = [−5, 5]2.

This function has four global maxima.

In the second series of experiments (displayed in Figure 6), the horizon n was set to 300
evaluations and the algorithms were compared on four speciﬁc problems. The objectives
consist in maximizing:
(e) The one-dimensional function f (x) = (0.1|cos(50(x − x(cid:63)))|3/2 − 1.5|x − x(cid:63)|1/2)I{x ≤
x(cid:63)} − (x1/2 + 0.05|sin(50x)|3/2)I{x > x(cid:63)} over X = [0, 1] where x(cid:63) = 0.499. This
function has 17 local maxima and presents a large discontinuity around its unique
optimum x(cid:63). For this problem, the convex rankings were temporally used.

14

2040608010001234AdaRankBayesOpDIRECTCMA-ESCRS20406080100020406080AdaRankBayesOpDIRECTCMA-ESCRS2040608010005101520AdaRankBayesOpDIRECTCMA-ESCRS204060801000510152025AdaRankBayesOpDIRECTCMA-ESCRS(e)

(f )

(g)

(h)

Figure 7: Diﬀerence between the maximum of the function and the expectation of its
approximation maxx∈X f (x)−E [f (Xˆıt)] in terms of iteration t = 1 . . . 300. The expectation
was estimated by running 1000 times each algorithm.

2/π|)| over the
domain X = [−10, 10]2. This function has a very weak global structure and presents
about 36 local maxima.

(f ) The H¨older table function f (x1, x2) = | sin(x1) cos(x2) exp(|1−(cid:112)x2
(g) The Griewank function f (x) = (cid:81)4
(h) The function f (x) = 1−|(cid:80)10
this function is that it is maximized over the whole hyperplane {x ∈ R10 :(cid:80)10

i=1(xi−4.5)/10|5/2 over X = [−5, 5]10. A special feature of
i=1 xi =

i /4000 − 1 over the do-
main X = [−300, 600]4. The Griewank function has many widespread and regulary
distributed local minima.

i=1 x2

1 + x2

√
i=1 cos(xi/

i) −(cid:80)4

45}.

In most cases, we remark that the AdaRankOpt converges fast and avoids falling
in local maxima, see for instance (a), (b), (d) and (e). Moreover, experiment (h) also
suggests that the algorithm is robust against the dimensionality of the input space when
the ranking structure of the unkown function is regular enough. These performances can
be explained by the ability of the algorithm to learn the ranking structure of the unkown
function, which allows to adapt to a wide variety of functions. However, experiments (c)
and (g) highlight some limits of the algorithm. Indeed, when the unkown function presents

15

5010015020025030000.20.40.60.8AdaRankBayesOpDIRECTCRS5010015020025030005101520AdaRankBayesOpDIRECTCMA-ESCRS50100150200250300010203040AdaRankBayesOpDIRECTCMA-ESCRS501001502002503000102030AdaRankBayesOpDIRECTCMA-ESCRSa strong global structure but has a large amount of local mimima, the algorithm succefully
manage to learn the global ranking structure but fails at locally optimizing the function.
An approach to solve this problem would be to extend the algorithm to noisy evaluations,
as stated in Remark 8.

7. Conclusion

We have provided a global optimization strategy based on a sequential estimation of the
ranking of the unknown function. We introduced two algorithms: RankOpt which requires
a prior knowledge of the ranking rule of the unknown function and its adaptive version
AdaRankOpt which performs model selection. A theoretical analysis is provided and the
adaptive algorithm is shown to be empirically competitive with the state-of-the-art methods
on synthetic optimization problems.

Appendix A. Proofs of Section 2
Proof of proposition 2 (⇐) Assume that there exists a strictly increasing function ψ :
R → R such that h = ψ ◦ f and let sgn : R → {0,±1} be the standard sign function deﬁned
by sgn(x) = I{x > 0} − I{x < 0}. Since ψ is a strictly increasing function, ∀(x, x(cid:48)) ∈ X 2,

rh(x, x(cid:48)) = sgn(ψ ◦ f (x) − ψ ◦ f (x(cid:48))) = sgn(f (x) − f (x(cid:48))) = rf (x, x(cid:48)).

(⇒)Assume that ∀(x, x(cid:48)) ∈ X 2, rf (x, x(cid:48)) = rh(x, x(cid:48)). If ∀(x, x(cid:48)) ∈ X 2, rf (x, x(cid:48)) = rh(x, x(cid:48)) =
0, then f = cf and h = ch are constant over X and we have that h = ψ ◦ f where
ψ : x (cid:55)→ x + (ch − cf ) is a strictly increasing function. Assume that f is not constant over
X and introduce the function M : X → [0, 1] deﬁned by:

I(cid:8)rf (x, x(cid:48)) < 0(cid:9) dx(cid:48) = µ({x(cid:48) ∈ X : f (x(cid:48)) < f (x)}), ∀x ∈ X .

(cid:90)

M (x) =

x(cid:48)∈X

We start to show that there exists a strictly increasing function ψ : R → R such that
f = ψ ◦ M . To properly deﬁne the function ψ, we show by contradiction that ∀y ∈ Im(M )
the function f is constant over the iso-level set M−1(y) = {x ∈ X : M (x) = y}. Fix
any y ∈ Im(M ), pick any (x1, x2) ∈ M−1(y)2 and assume, without loss of generality, that
f (x1) < f (x2). The equality of the rankings implies that (i) M (xi) = µ({x : h(x) < h(xi)}),
i ∈ {1, 2} and (ii) h(x1) < h(x2). Putting (i) and (ii) altogether and using the continuity
of h leads to the contradiction

M (x1) = µ({x ∈ X : h(x) < h(x1)}) < µ({x ∈ X : h(x) < h(x2)}) = M (x2).

Then, denoting by f (M−1(y)) the unique value of the function f over M−1(y), we can now
introduce the function ¯ψ : Im(M ) → R deﬁned by

¯ψ : y (cid:55)→ f (M−1(y)).

Since ∀x ∈ X , ¯ψ(M (x)) = f (x), the continuity of h implies that ∀y1 < y2 ∈ Im(M )2,
¯ψ(y1) < ¯ψ(y2). Therefore, f = ψ◦M where ψ : R → R is any strictly increasing extension of

16

the function ¯ψ over R. Reproducing the same steps, one can show that there exists a strictly
increasing function ψ(cid:48) : R → R such that h = ψ(cid:48) ◦ M . Hence h = ψ(cid:48) ◦ M = (ψ(cid:48) ◦ ψ−1) ◦ f
where ψ(cid:48) ◦ ψ−1 : R → R is a strictly increasing function.
(cid:3)

Appendix B. Proofs of the results of Section 3

B.1 Preliminary results and technical lemmas
Proposition 27 (RankOpt process) Let X ⊂ Rd be any compact and convex set, let
R ⊆ R∞ be any ranking structure and let f : X → R be any function such that rf ∈ R.
Then, the RankOpt(n, f, X , R) algorithm evaluates the function f on a random sequence
{Xi}n

i=1 deﬁned by:(cid:40)

X1 ∼ U(X )
Xt+1| {Xi}t

i=1 ∼ U(Xt)

∀t ∈ {1 . . . n − 1}

where Xt = {x ∈ X : ∃r ∈ Rt s.t. r(x, Xˆıt) ≥ 0} denotes the sampling area and ˆıt and Rt
are deﬁned as in the algorithm.
Proof The result is proved by induction. Since X1 ∼ U(X ), the result trivially holds
when n = 1. Assume that the satement holds for a given n ∈ N(cid:63). By deﬁnition of the
algorithm and using the induction assumption, we know that the RankOpt(n + 1, f,X )
algorithm evaluates the function on a sequence of random variables {X1, X2, . . . Xn, X(cid:48)
τ}
i ∈ Xn} and {X(cid:48)
i=1 ∼ RankOpt(n, f,X ), τ = inf{i ∈ N(cid:63) : X(cid:48)
i}i∈N(cid:63) is a
where {Xi}n
sequence of independant random variables uniformly distributed over X and independant
of {Xi}n
(cid:19)i−1
(cid:18)
∞(cid:88)

i=1. Therefore, for any borelian X (cid:48) ⊂ Xn,

∞(cid:88)

P(X(cid:48)

i ∈ X (cid:48))P(X(cid:48)

1 /∈ Xn)i−1 =

P(X(cid:48)

τ ∈ X (cid:48)|{Xi}n

i=1) =

µ(X (cid:48))
µ(X )

i=1

1 − µ(Xn)
µ(X )

i=1

and the result is proved by induction.

=

µ(X (cid:48))
µ(Xn)
(cid:3)

Lemma 28 Under the same assumptions as in Proposition 27, for all t ∈ {1 . . . n}, we
have that Xf (Xˆıt ) ⊆ Xt ⊆ X where Xf (Xˆıt ) = {x ∈ X : f (x) ≥ f (Xˆıt)} denotes the level set

of the highest value observed so far.
Proof Noticing that Xt is a subset of X gives the ﬁrst inclusion. To state the sec-
ond inclusion, pick any x ∈ {x ∈ X : f (x) ≥ f (Xˆıt)} and observe that rf (x, Xˆıt) =
sgn(f (x) − f (Xˆıt)) ≥ 0. Since rf always perfectly ranks the sample (i.e., Ln+1(rf ) = 0),
there exists r = rf ∈ Rt such that r(x, Xˆıt) ≥ 0 and we deduce that {x ∈ X : f (x) ≥
f (Xˆıt)} ⊆ {x ∈ X : ∃r ∈ Rt s.t. r(x, Xˆıt) ≥ 0} = Xt.
(cid:3)

Deﬁnition 29 (Pure Adaptive Search process, from Zabinsky and Smith (1992)).
Let X ⊂ Rd be any compact and convex set and let f : X → R be any function such that

17

rf ∈ R∞. We say that the sequence {X (cid:63)
process PAS(n, f,X ) if it has the same distribution as the Markov process deﬁned by:

i=1 is distributed as a Pure Adaptive Search

i }n

(cid:40)

1 ∼ U(X )
X (cid:63)
t+1| X (cid:63)
X (cid:63)

t ∼ U(X (cid:63)
t )

∀t ∈ {1 . . . n − 1},

t = {x ∈ X : f (x) ≥ f (X (cid:63)

where X (cid:63)
Lemma 30 Let {X (cid:63)
process. Then, for any u ∈ [0, 1], we have that

i }n

t )} is the level set of the highest value observed so far.

i=1 be a sequence of n random variables distributed as a PAS(n, f,X )

(cid:18) µ(X (cid:63)

n )
µ(X )

P

(cid:19)

(cid:32) n(cid:89)

i=1

≤ u

≤ P

Ui ≤ u

(cid:33)

n = {x ∈ X : f (x) ≥ f (X (cid:63)
n)}

where {Ui}n
i=1 are n independent copies of U ∼ U([0, 1]) and X (cid:63)
denotes the sampling area of the PAS(n, f,X ) process at time n.
Proof Note that if u(cid:63) = µ({x ∈ X : f (x) ≥ maxx∈X f (x)})/µ(X ) > 0 the result trivially
holds for any u < u(cid:63) and any n ∈ N(cid:63). Therefore, we assume without loss of generality
that u(cid:63) = 0 and we set some additional notations: ∀u ∈ [0, 1] deﬁne yu = min{y ∈ Im(f ) :
µ({x ∈ X : f (x) ≥ y}) ≤ u· µ(X )} and let Xyu = {x ∈ X : f (x) ≥ yu} be the corresponding
level set. Observing that ∀u ∈ [0, 1], µ(Xyu) ≤ u · µ(X ), we are ready to prove the Lemma
by induction.
Fix any u ∈ [0, 1] and let U1 ∼ U([0, 1]) be a random variable independent of X (cid:63)
1 ∼ U(X ) and P(U1 ≤ u) = u, we have that
X (cid:63)

1 . Since

(cid:18) µ(X (cid:63)

1 )
µ(X )

P

(cid:19)

≤ u

= P(X (cid:63)

1 ∈ Xyu) =

≤ u = P(U1 ≤ u)

µ(Xyu)
µ(X )

and the result holds when n = 1. Assume that the statement holds for a given n ∈ N(cid:63),
ﬁx any u ∈ [0, 1] and let {X (cid:63)
i=1 be a sequence of n + 1 random variables distributed as
a PAS(n + 1, f,X ) process. Since X (cid:63)
n ) (Deﬁnition 29), conditioning on X (cid:63)
n
gives that

n ∼ U(X (cid:63)

n+1| X (cid:63)

i }n+1
(cid:19)

= E(cid:2)P(X (cid:63)

(cid:18) µ(X (cid:63)

P

n+1)
µ(X )

≤ u

n)(cid:3) = E

(cid:20) µ(Xyu ∩ X (cid:63)

n )

(cid:21)

µ(X (cid:63)
n )

.

n+1 ∈ Xyu| X (cid:63)

P

(cid:20)

(cid:19)

n )) and so

(cid:18) µ(X (cid:63)

n}. Hence µ(Xyu ∩ X (cid:63)

n ⊆ Xyu} and (ii) {f (X (cid:63)

Since the level sets form a nested sequence, we have the following equivalences: (i) {f (X (cid:63)
yu} = {X (cid:63)
n )/µ(X (cid:63)
min(1, µ(Xyu)/µ(X (cid:63)

n) ≤ yu} = {Xyu ⊆ X (cid:63)
(cid:18)

(cid:19)(cid:21)
.
i }n+1
Now, let Un+1 ∼ U([0, 1]) be a random variable independent of {X (cid:63)
formly distributed over [0, 1] and independent of X (cid:63)
(cid:19)
P(Un+1 ≤ µ(Xyu)/µ(X (cid:63)

n, we have that min(1, µ(Xyu)/µ(X (cid:63)
(cid:19)(cid:21)
(cid:19)
≤ µ(Xyu)
µ(X )

(cid:18)
Un+1 · µ(X (cid:63)
n )
µ(X )

(cid:18)
Un+1 ≤ µ(Xyu)
µ(X (cid:63)
n )

(cid:20)
n )| X (cid:63)
P
= E

(cid:18) µ(X (cid:63)

µ(Xyu)
µ(X (cid:63)
n )

n). Therefore,

n+1)
µ(X )

n+1)
µ(X )

| X (cid:63)

n

min

1,

≤ u

≤ u

= E

= P

i=1 . Since Un+1 is uni-
n )) =

n) ≥
n ) =

P

.

18

Finally, using the fact that µ(Xyu) ≤ u · µ(X ) and plugging the induction assumption into
the previous equation gives (by independence) that:

(cid:33)

(cid:18) µ(X (cid:63)

P

(cid:19)

n+1)
µ(X )

≤ u

≤ P

(cid:18)
Un+1 · µ(X (cid:63)
n )
µ(X )

(cid:19)

(cid:32)n+1(cid:89)

i=1

≤ u

≤ P

Ui ≤ u

where {Ui}n+1

i=1 are n + 1 independent copies of U ∼ U([0, 1]).

(cid:3)

Lemma 31 Let {Ui}n

i=1 Ui < δ · e−n−√

i=1 be a sequence of n independent copies of U ∼ U([0, 1]). Then, for

2n ln(1/δ)(cid:17)
any δ ∈ (0, 1), we have that P(cid:16)(cid:81)n
Proof Taking the logarithm on both sides gives that (cid:81)n
i=1 − ln(Ui) > n +(cid:112)2n ln(1/δ) + ln(1/δ). Since Ui ∼ U([0, 1]), we have that − ln(Ui) ∼
(cid:80)n
Exp(1) and so (cid:80)n

2n ln(1/δ) ⇔
i=1 − ln(Ui) ∼ Gamma(n, 1). The result follows from the application of
concentration results of sub-gamma random variables (see Chapter 2.4 in Boucheron et al.
(cid:3)
(2013)).

i=1 Ui < δ · e−n−√

< δ.

Lemma 32 Let X ⊂ Rd be any compact and convex set and let f ∈ C0(X , R) be any func-
tion that has (α, cα)-regular level sets (Condition 2). Then, for any r ≤ maxx∈X (cid:107)x(cid:63) − x(cid:107)2
where x(cid:63) denotes the unique optimizer of the function f over X , we have that

X ∩ B(x(cid:63), (r/cα)1+α) ⊆ {x ∈ X : f (x) ≥ min
xr∈Sr

f (xr)} ⊆ B(x(cid:63), cα · r1/(1+α))

where Sr = {x ∈ X : (cid:107)x(cid:63) − x(cid:107)2 = r} denotes the intersection of X with the (cid:96)2-sphere
centered in x(cid:63) of radius r.
Proof To prove the second inclusion, we ﬁrst show that ∀y ∈ [minx∈Sr f (x), f (x(cid:63))], there
exists xy ∈ f−1(y) = {x ∈ X : f (x) = y} which satisﬁes (cid:107)x(cid:63) − xy(cid:107) ≤ r. Fix any y ∈
[minx∈Sr f (x), f (x(cid:63))], pick any xr ∈ arg minx∈Sr f (x) and introduce the function F : [0, 1] →
R deﬁned by

F : λ (cid:55)→ f ((1 − λ)x(cid:63) + λxr)

which returns the value of f over the segment [x(cid:63), xr]. Note that the function is contin-
uous and well-deﬁned due to the convexity of X and the continuity of f . Since F (0) =
f (x(cid:63)), F (1) = minx∈Sr f (x) and y ∈ [F (1), F (0)], applying the intermediate value the-
orem gives us that there exists λy ∈ [0, 1] such that Fxr (λy) = y. Hence, there exists
xy = λyx(cid:63) + (1 − λy)xr ∈ f−1(y) such that (cid:107)x(cid:63) − xy(cid:107)2≤(cid:107)x(cid:63) − xr(cid:107)2= r. Assume now that
y(cid:107)2> cαr1/(1+α). As a direct consequence, it
y ∈ f−1(y) such that (cid:107)x(cid:63) − x(cid:48)
there exists x(cid:48)
y(cid:107)2 > cαr1/(1+α). However, we also have that
implies that maxx∈f−1(y)(cid:107)x(cid:63) − x(cid:107)2 ≥ (cid:107)x(cid:63) − x(cid:48)
cα minx∈f−1(y)(cid:107)x(cid:63) − x(cid:107)1/(1+α)
≤ cαr1/(1+α) and so putting altogether
the previous inequalities with the level set assumption leads us to the next contradiction:

≤ cα(cid:107)x(cid:63) − xy(cid:107)1/(1+α)

2

2

max

x∈f−1(y)

(cid:107)x(cid:63) − x(cid:107)2 ≤ cα · min
x∈f−1(y)

(cid:107)x(cid:63) − x(cid:107)1/(1+α)

2

< max

x∈f−1(y)

(cid:107)x(cid:63) − x(cid:107)2 .

19

The contradiction holds for any y ∈ [minx∈Sr f (xr), f (x(cid:63))]. We deduce that {x ∈ X : f (x) ≥
minx∈Sr f (xr)} ⊆ B(x(cid:63), cα · r1/(1+α)) which proves the second inclusion.
We use a similar technique to prove the ﬁrst inclusion. Assume that there exists x(cid:48) ∈
X ∩ B(x(cid:63), (r/cα)1+α) s.t. f (x) < f (xr). By introducing the function F : λ (cid:55)→ f ((1 −
r(cid:107)2< (r/cα)1+α.
λ)x(cid:63) + λx(cid:48)), one can show that there exists x(cid:48)
Hence, cα · minx∈f−1(f (xr))(cid:107)x(cid:63) − x(cid:107)1/(1+α)
< r. On the other hand,
maxx∈f−1(f (xr))(cid:107)x(cid:63) − x(cid:107)2 ≥ (cid:107)x(cid:63) − xr(cid:107)2 = r.
It leads to a similar contradiction and we
deduce that X ∩ B(x(cid:63), (d/cα)1+α) ⊆ {x ∈ X : f (x) ≥ minxr∈Sr f (xr)}.
(cid:3)

r ∈ f−1(f (xr)) s.t. (cid:107)x(cid:63) − x(cid:48)

≤ cα(cid:107)x(cid:63) − x(cid:48)

r(cid:107)1/(1+α)

2

2

Lemma 33 (From Zabinsky and Smith (1992)). Let X ⊂ Rd be any compact and convex
set. Then, for any x ∈ X and any r ∈ (0, diam(X )), we have that

(cid:18)

(cid:19)d

.

µ(B(x, r) ∩ X )

µ(X )

≥

r

diam(X )

Proof Introduce the similarity transformation S : Rd → Rd deﬁned by:

S : x(cid:48) (cid:55)→ x +

r

diam(X )

(x(cid:48) − x)

and let S(X ) = {S(x(cid:48)) : x(cid:48) ∈ X} be the image of X by the similarity transformation.
By deﬁnition we have that maxx(cid:48)∈X (cid:107)x − x(cid:48)(cid:107)2 ≤ diam(X ). Hence, the convexity of X
implies that S(X ) ⊆ B(x, r) ∩ X and we have that µ(B(x, r) ∩ X ) ≥ µ(S(X )). Now,
since S is a similarity transformation (and conserves the ratios of the volumes before/after
transformation) we have that

µ(B(x, r) ∩ X )

µ(X )

≥ µ(S(X ))
µ(X )

=

µ(S(B(x, diam(X ))))
µ(B(x, diam(X )))

=

µ(B(x, r))

µ(B(x, diam(X )))

.

Noticing that ∀r ≥ 0, µ(B(x, r)) = πd/2rd/Γ(d/2 + 1) where Γ(·) stands for the standard
(cid:3)
gamma function gives the result.

B.2 Proofs of the results of Section 3
Proof of Proposition 9 The statement is proved by induction. Since X1 ∼ U(X ), the
result trivially holds when n = 1. Assume that the statement holds for a given n ∈ N(cid:63) and let
{Xi}n+1
i=1 be a sequence of n + 1 random variables distributed as a RankOpt(n + 1, f,X ,R)
process. Since the result trivially holds ∀y /∈ Im(f ), ﬁx any y ∈ [minx∈X f (x), maxx∈X f (x)]
and let Xy = {x ∈ X : f (x) ≥ y} be the corresponding level set. Applying Proposition 27
gives that

(cid:33)

(cid:32) n(cid:92)

(cid:33)
{Xi /∈ Xy} ∩ {Xn+1 ∈ Xy}

.

(4)

(cid:32) n(cid:91)

P(f (Xˆın+1) ≥ y) = P

{Xi /∈ Xy}

+ P

i=1

i=1

20

We start to bound the second term. Since Xn+1| {Xi}n
conditioning on {Xi}n

(cid:32) n(cid:92)

i=1

P

i=1 gives that
{Xi /∈ Xy} ∩ {Xn+1 ∈ Xy}

(cid:33)

(cid:40) n(cid:92)
(cid:40) n(cid:92)

i=1

(cid:34)
(cid:34)

I

I

= E

= E

i=1 ∼ U(Xn) (see Proposition 27),
(cid:41)
(cid:35)
(cid:41)

· P(Xn+1 ∈ Xy|{Xi}n
· µ(Xn ∩ Xy)
µ(Xn)

(cid:35)

i=1)

.

{Xi /∈ Xy}

{Xi /∈ Xy}

(cid:84)n
Lemma 28 gives the following inclusion of events: {Xy ⊆ Xn ⊆ X} ⊆ {f (Xˆın) < y} =
i=1{Xi /∈ Xy}n

i=1

(cid:32) n(cid:92)

(cid:33)
i=1. Hence
{Xi /∈ Xy} ∩ {Xn+1 ∈ Xy}
Plugging (5) into (4) and noticing that E [I{(cid:84)n

i=1

P

≥ µ(Xy)
µ(X )

(cid:34)

(cid:40) n(cid:92)

i=1

i=1{Xi /∈ Xy}}] = 1 − P(f (Xˆın) ≥ y) leads to

· E

I

{Xi /∈ Xy}

.

(5)

(cid:41)(cid:35)

the next inequality:

µ(Xy)
µ(X )

Now, let {X(cid:48)
the same steps as previously and using the fact that P(X(cid:48)
that

P(f (Xˆın+1) ≥ y) ≥ P(f (Xˆın) ≥ y) +
i}n+1
i=1 be a sequence of n + 1 independent copies of X ∼ U(X ). Reproducing
n+1 ∈ Xy) = µ(Xy)/µ(X ) gives
(cid:18)
(cid:19)(cid:19)

· (1 − P(f (Xˆın) ≥ y)).

(cid:19)

(cid:19)

(cid:18)

(cid:18)

(cid:18)

(6)

P

max

i=1...n+1

f (X(cid:48)

i) ≥ y

= P

f (X(cid:48)

i) ≥ y

+

max
i=1...n

1 − P

f (X(cid:48)

i) ≥ y

.

max
i=1...n

µ(Xy)
µ(X )

·

i) < maxx∈X f (x) − ε) where {X(cid:48)

(7)
Finally, plugging the induction assumption into (6) and comparing the result with (7) yields
(cid:3)
to the desired result.
Proof of Corollary 10 Pick any ε > 0 and let Xf (cid:63)−ε = {x ∈ X : f (x) ≥ maxx∈X f (x)− ε}
be the corresponding level set. Proposition 9 gives that ∀n ∈ N(cid:63), P(f (Xˆın) < maxx∈X f (x)−
i}n
ε) ≤ P(maxi=1...n f (X(cid:48)
i=1 are n independent copies of
X(cid:48) ∼ U(X ). Therefore, we have that

(cid:18)
1 − µ(Xf (cid:63)−ε)
µ(X )
Since 0 < µ(Xf (cid:63)−ε) ≤ µ(X ) by Condition 1, P(f (Xˆın) < maxx∈X f (x) − ε) −→
Proof of Theorem 11 Since rf ∈ R ⊆ R∞ is a continuous ranking, there exists h ∈
C0(X , R) such that ∀(x, x(cid:48)) ∈ X 2, rf (x, x(cid:48)) = rh(x, x(cid:48)) (see Proposition 2). Since all the
arguments only use function comparisons, one can assume without loss generality that f ∈
C0(X , R). We also set some additional notations: let rδ,n be the upper bound of the theorem,
δ,n/cα)1+α = diam(X ) (ln(1/δ)/n)1/d. Since the
let r(cid:48)
result trivially holds when rδ,n ≥ maxx∈X (cid:107)x − x(cid:63)(cid:107)2, assume that rδ,n < maxx∈X (cid:107)x − x(cid:63)(cid:107)2

P(f (Xˆın) < maxx∈X f (x) − ε)≤ P(X(cid:48)

δ,n = (rδ,n/cα)1+α and let r(cid:48)(cid:48)

1 /∈ Xf (cid:63)−ε)n =

δ,n = (r(cid:48)

n→∞ 0.

(cid:19)n

(cid:3)

.

21

δ,n

f (x)

(cid:19)

≥ P

≥ P

f (X(cid:48)

(Lemma 32)

– which also implies by the level set assumption that ln(1/δ) < n. Using the second inclusion
of Lemma 32 and Proposition 9 gives that

(cid:18)
P((cid:107)Xˆın − x(cid:63)(cid:107)2 ≤ rδ,n) = P(Xˆın ∈ B(x(cid:63), rδ,n))
(cid:18)
f (Xˆın) ≥ minx∈Sr(cid:48)

(cid:19)
f (x)
i=1 are n independent copies of X ∼ U(X ) and Sr(cid:48)
i}n
(cid:32)
1 − µ(X ∩ B(x(cid:63), r(cid:48)(cid:48)
(cid:19)n

where {X(cid:48)
Since the random variables {X(cid:48)
(cid:33)
applying the ﬁrst inclusion of Lemma 32 gives that
P((cid:107)Xˆın − x(cid:63)(cid:107)2 ≤ rδ,n) ≥ P
δ,n)}
i ∈ X ∩ B(x(cid:63), r(cid:48)(cid:48)
(cid:18)
δ,n/ diam(X ))d = ln(1/δ)/n. Therefore
δ,n))/µ(X ) ≤ (r(cid:48)(cid:48)
Lemma 33 gives that µ(X ∩ B(x(cid:63), r(cid:48)(cid:48)
P((cid:107)Xˆın − x(cid:63)(cid:107)2 ≤ rδ,n) ≥ 1 −
1 − ln(1/δ)

is deﬁned in Lemma 32.
i=1 are independent and uniformly distributed over X ,
i}n
(cid:33)n

i) ≥ minx∈Sr(cid:48)

(cid:32) n(cid:91)

(Proposition 9)

= 1−

µ(X )

max
i=1...n

{X(cid:48)

δ,n))

i=1

δ,n

δ,n

.

.

Finally, using the fact that ∀x ∈ R, 1 − x ≤ e−x yields to the desired result.

(cid:3)

n

(cid:20)

µ(Xy ∩ Xn)

P(f (Xˆın+1) ≥ y) = E

Lemma 28 gives the following inclusion {Xy ⊆ Xf (Xˆın ) ⊆ Xn} ⊆ {f (Xˆın) < y}. Hence

Proof of Proposition 12 The statement is proved by induction. Since X1 and X (cid:63)
1 are
both uniformly distributed over X , the result holds when n = 1. Assume that the statement
holds for a given n ∈ N(cid:63) and let {Xi}n+1
i=1 be a sequence of n+1 random variables distributed
as the RankOpt(n + 1, f,X ,R) process. Since the result trivially holds ∀y /∈ Im(f ), ﬁx
any y ∈ [minx∈X f (x), maxx∈X f (x)] and let Xy = {x ∈ X : f (x) ≥ y} be the corresponding
level set. Then, denoting by Xn the sampling area of Xn+1 and by Xf (Xˆın ) = {x ∈ X :
f (x) ≥ f (Xˆın)} the level set of the highest value observed so far, reproducing the same
steps as in the proof of Proposition 9 gives that
I{f (Xˆın) ≥ y} +
(cid:34)
I{f (Xˆın) ≥ y} +
(cid:34)
(cid:32)
µ(Xy)
µ(Xf (Xˆın ))
(cid:32)
(cid:32)
(cid:90) 1

µ(Xy)
(cid:33)(cid:35)
µ(Xf (Xˆın ))
For any random variable X ∈ [0, 1], we have that E [X] =(cid:82) 1
(cid:33)
µ(Xy)
(cid:18)
µ(Xf (Xˆın ))
µ(Xf (Xˆın )) <
P

(cid:21)
· I{f (Xˆın) < y}
(cid:35)

P(f (Xˆın+1) ≥ y) ≤ E

P(f (Xˆın+1) ≥ y) ≤

P(X > t) dt and so

I{f (Xˆın) < y}

dt
µ(Xy)

(cid:90) 1

µ(Xn)

(cid:33)

(cid:19)

= E

min

min

> t

(9)

(8)

dt.

P

1,

=

+

1,

.

0

.

0

µ(Xy)
µ(X )

µ(Xy )
µ(X )

22

t

i }n+1

Now, let {X (cid:63)
process. Reproducing the same steps as previously gives that
P(f (X (cid:63)

i=1 be a sequence of n + 1 random variables distributed as a PAS(n + 1, f,X )
(cid:19)

(cid:90) 1

(cid:19)(cid:21)

µ(Xy)

n+1) ≥ y) = E

(cid:18)

(cid:20)

min

dt.

P

+

=

1,

n)) <

µ(Xy)
µ(X )

µ(Xy)
µ(Xf (X (cid:63)
n))

µ(Xy )
µ(X )

(cid:18)
µ(Xf (Xˆın )) <

Finally, by the induction assumption ∀t ∈ (µ(Xy)/µ(X ), 1), we have that
P

= P(f (Xˆın) ≥ ¯yt) ≤ P(f (X (cid:63)

n) ≥ ¯yt) = P

µ(Xy)

(cid:19)

t

(cid:18)
µ(Xf (X (cid:63)
(cid:18)
µ(Xf (X (cid:63)

t

(10)

(cid:19)

µ(Xy)

t

n)) <

(11)
where ¯yt = min{y(cid:48) ∈ Im(f ) : µ(Xy(cid:48)) < µ(Xy)/t} and the proof is complete by plugging (11)
(cid:3)
into (8) and by comparing the result with (10).
Proof of Theorem 13 As mentioned in the proof of Theorem 11, since rf ∈ R ⊆ R∞, there
exists h ∈ C0(X , R) such that rf = rh and one can assume without loss of generality that f ∈
C0(X , R). We also set some additional notations: let rδ,n be the lower bound of the theorem,
let r(cid:48)
Using the ﬁrst inclusion of Lemma 32 and Proposition 12 gives that

= rad(X ) δ1/d exp(−(n +(cid:112)2n ln (1/δ))/d).

δ,n = cαr1/(1+α)
P((cid:107)Xˆın − x(cid:63)(cid:107)2 ≤ rδ,n) = P(Xˆın ∈ B(x(cid:63), rδ,n) ∩ X )

and let r(cid:48)(cid:48)

(cid:48)1/(1+α)
δ,n

δ,n = cαr

δ,n

≤ P

≤ P

f (Xˆın) ≥ minx∈Sr(cid:48)
n) ≥ minx∈Sr(cid:48)

f (X (cid:63)

δ,n

f (x)

δ,n

(Lemma 32)

f (x)

(Proposition 12)

i }n

where {X (cid:63)
Applying the second inclusion of Lemma 32 and denoting by X (cid:63)
the level set of the highest value observed so far gives that

i=1 is a sequence of n random variables distributed as the PAS(n, f,X ) process.
n = {x ∈ X : f (x) ≥ f (X (cid:63)
n)}

(cid:19)

n ) ≤ µ({x ∈ X : f (x) ≥ minx∈Sr(cid:48)

f (x)})

δ,n

P((cid:107)Xˆın − x(cid:63)(cid:107)2 ≤ rδ,n) ≤ P

≤ P

δ,n))

≤ µ(B(x(cid:63), r(cid:48)(cid:48)
µ(X )
(cid:33)

Since rad(X ) > 0, there exists x ∈ X such that B(x, rad(X )) ⊆ X which implies that
µ(X ) ≥ µ(B(x, rad(X ))) = πd/2 rad(X )d /Γ(d/2 + 1). Therefore

P((cid:107)Xˆın − x(cid:63)(cid:107)2 ≤ rδ,n) ≤ P

By deﬁnition of r(cid:48)(cid:48)
applying Lemma 30 gives that

δ,n, we have that (r(cid:48)(cid:48)

P((cid:107)Xˆın − x(cid:63)(cid:107)2 ≤ rδ,n) = P

(cid:32)

µ(X (cid:63)
n )
µ(X )

(cid:32) r(cid:48)(cid:48)

 µ(X (cid:63)

(cid:33)d .
≤ µ(B(x(cid:63), r(cid:48)
δ,n))
δ,n/ rad(X ))d = δ exp (−n −(cid:112)2n ln(1/δ)) and so,
µ(B(x, rad(X )))
(cid:32) n(cid:89)

n )
µ(X )

rad(X )

Ui ≤ δ · e−n−√

(cid:33)

= P

2n ln(1/δ)

≤

δ,n

where {Ui}n
rδ,n) < δ by Lemma 31.

i=1 are n independent copies of U ∼ U([0, 1]). We ﬁnally get that P((cid:107)Xˆın − x(cid:63)(cid:107)2 ≤
(cid:3)

i=1

23

(cid:18)
(cid:18)

(cid:18)
(cid:32)

µ(X (cid:63)
µ(X (cid:63)
n )
µ(X )

(cid:19)
(cid:19)

(cid:33)

.

Appendix C. Proofs of Section 4

C.1 Preliminary results
Proposition 34 (AdaRankOpt process) Let X ⊂ Rd be any compact and convex set, let
{Rk}k∈N(cid:63) be any sequence of nested ranking structures, ﬁx any p ∈ (0, 1) and let f : X → R
be any function such that rf ∈ R∞. Then, the AdaRankOpt(n, f , X , p, {Rk}k∈N(cid:63))
algorithm evaluates the function on a sequence of random variables {Xi}n

i=1 deﬁned by:

i=1 ∼ Bt+1 · U(X ) + (1 − Bt+1) · U(Xt)

∀t ∈ {1 . . . n − 1}

where Xt = {x ∈ X : ∃r ∈ R(cid:98)kt
parameter p independent of {(Xi, Bi)}t

, s.t. r(x, Xˆıt) ≥ 0}, Bt+1 is a Bernoulli random variable of

i=1 and ˆıt and(cid:98)kt are as deﬁned as in the algorithm.

Proof The result is a direct consequence of the deﬁnition of the algorithm.

(cid:3)

(cid:40)

X1 ∼ U(X )
Xt+1| Bt+1,{Xi}t

Proposition 35 (From Cl´emen¸con et al. (2010)) Let {Xi}n
i=1 be a sequence of n indepen-
dent copies of X ∼ U(X ), let R be any ranking structure, let f : X → R be any real-valued
function and let Ln be the empricial ranking loss taken over {(Xi, f (Xi))}n
i=1. Then, de-
noting by Rn(R) the Rademacher average of R and by L(r) the true ranking loss deﬁned in
Section 4, for any δ ∈ (0, 1), with probability at least 1 − δ,

∀r ∈ R, |Ln(r) − L(r)| ≤ 2E [Rn(R)] + 2

log(1/δ)
n − 1

.

(cid:114)

Proof See Section 3 in Cl´emen¸con et al. (2010) for a detailed proof.

(cid:3)

C.2 Proofs of the results of Section 4
Proof of Proposition 15 Fix any ε > 0 and let Xf (cid:63)−ε = {x ∈ X : f (x) ≥ maxx∈X f (x)−ε}
(cid:18)
be the corresponding level set. We show by induction that ∀n ∈ N(cid:63),
1 − p · µ(Xf (cid:63)−ε)
µ(X )
(cid:19)
(cid:18)
1 − µ(Xf (cid:63)−ε)
µ(X )

P(f (Xˆı1) < maxx∈X f (x) − ε) = P(X1 /∈ Xf (cid:63)−ε) =

Since X1 ∼ U(X ) and ˆı1 = 1, we have that

P(f (Xˆın) < maxx∈X f (x) − ε) ≤

1 − p · µ(Xf (cid:63)−ε)
µ(X )

.

(cid:18)

(12)

(cid:19)

(cid:19)n

≤

and the result holds when n = 1. Assume that the statement holds for a given n ∈ N(cid:63) and let
{Xi}n+1
i=1 be the sequence of n + 1 random variables deﬁned in Proposition 34. Conditioning

24

on {Xi}n

i=1 gives that

P(f (Xˆın+1) < maxx∈X f (x) − ε) = P

= E

(cid:32)n+1(cid:92)
(cid:34)

i=1

(cid:33)

{Xi /∈ Xf (cid:63)−ε}

P(Xn+1 /∈ Xf (cid:63)−ε|{Xi}n

i=1) · I

(cid:40) n(cid:92)

(cid:41)(cid:35)

{Xi /∈ Xf (cid:63)−ε}

.

i=1

Since P(Xn+1 /∈ Xf (cid:63)−| {Xi}n
1|{Xi}n

i=1) = 1 − p · µ(Xf (cid:63)−)/µ(X ) (see Proposition34), we have that

i=1) ≤ 1 − P(Xn+1 ∈ Xf (cid:63)−| Bn+1 = 1,{Xi}n

i=1) · P(Bn+1 =

P(f (Xˆın+1) < maxx∈X f (x) − ε) ≤

1 − p · µ(Xf (cid:63)−ε)
µ(X )

· P

{Xi /∈ Xf (cid:63)−ε}

.

(cid:19)

(cid:32) n(cid:92)

i=1

(cid:33)

(cid:18)

Therefore, (12) is proved by plugging the induction assumption into the previous equation.
We ﬁnally get that P(f (Xˆın) < maxx∈X f (x) − ε) −→
that 0 < µ(Xf (cid:63)−) ≤ µ(X ) (Condition 1).
Proof of Proposition 17 Fix any δ ∈ (0, 1), let n(cid:48)
nδ = (cid:98)10 · (V + log(2/δ))/(p · inf r∈RN (cid:63)−1 L(r)2)(cid:99) is the integer part of the upper bound
and let {(Xi, Bi}i∈N(cid:63) be the sequence of random variables deﬁned in Proposition 34. Then,
denoting by Lnδ the empirical ranking loss taken over the ﬁrst nδ samples {Xi}nδ
i=1 and since
{Rk}k∈N(cid:63) forms a nested sequence, we have that minr∈R1 Lnδ (r) ≤ minr∈R2 Lnδ (r) ≤ ··· ≤
minr∈Rk(cid:63)−1 Lnδ (r). Hence

n→∞ 0 using the fact that p ∈ (0, 1) and
(cid:3)
δ = (cid:98)p · nδ −(cid:112)nδ log(2/δ)/2(cid:99) where

(cid:18)
(cid:32)

P(τk(cid:63) ≤ nδ) = P

≥ P

min
r∈Rk(cid:63)−1

min
r∈Rk(cid:63)−1

(cid:19)
Lnδ (r) > 0| nδ(cid:88)

Lnδ (r) > 0

i=1

(cid:33)

· P

(cid:32) nδ(cid:88)

i=1

(cid:33)

.

Bi ≥ n(cid:48)

δ

Bi ≥ n(cid:48)

δ

We start to lower bound the empirical risk by only keeping the ﬁrst n(cid:48)
samples:

δ (i.i.d.) explorative

I{rf (Xi, Xj) (cid:54)= r(Xi, Xj)} · I(cid:8)(i, j) ∈ I 2(cid:9)

(13)

j)(cid:9)

i, X(cid:48)

j) (cid:54)= r(X(cid:48)

i, X(cid:48)

δ}. Conditioning on |I|, we know that
{Xi}i∈I||I| is a sequence of |I| independent random variables, uniformly distributed over X
δ}, the right hand
i=1 Bi ≥ n(cid:48)

δ} = {|I| = n(cid:48)

2

1≤i<j≤nδ

Lnδ (r) ≥

nδ(nδ − 1)

(cid:88)
where I = {i ∈ N(cid:63) : Bi = 1 and (cid:80)i
(see Proposition34). Therefore, on the event {(cid:80)nδ
I(cid:8)rf (X(cid:48)

term of inequality (13) has the same distribution as

j=1 Bi ≤ n(cid:48)

(cid:88)

(r) =

Ln(cid:48)

2

δ

nδ(nδ − 1)

1≤i<j≤nδ(cid:48)

25

δ

where {X(cid:48)
{Bi}nδ

i}n(cid:48)
δ independent copies of X ∼ U(X ), also independent of
i=1 is a sequence of n(cid:48)
(cid:18)
i=1. Hence

(cid:33)

(cid:19)

P(τk(cid:63) ≤ nδ) ≥ P

min

r∈Rk(cid:63)−1

Ln(cid:48)

δ

(r) > 0

· P

Bi ≥ n(cid:48)

δ

(14)

(cid:32) nδ(cid:88)

i=1

i}n(cid:48)
where the empirical ranking loss is taken over {X(cid:48)
i=1. For any r ∈ Rk(cid:63)−1 we have that
(cid:115)
L(r) ≥ minr∈Rk(cid:63)−1 L(r) and so, by Proposition 31, we have with probability at least 1− δ/2
that

(cid:115)

δ

min
r∈Rk(cid:63)−1

Ln(cid:48)

δ

(r) ≥ inf

r∈Rk(cid:63)−1

L(r) − 2

− 2

V
n(cid:48)

δ

log(2/δ)
δ − 1
n(cid:48)

.

Hence, noticing that the right hand term of the previous inequality is stricly positive, due to
(r) > 0) ≥ 1 − δ/2. Finally, by Hoeﬀding’s
the deﬁnition of n(cid:48)
inequality we have that

δ, gives that P(minr∈Rk(cid:63)−1 Ln(cid:48)

δ

(cid:32) nδ(cid:88)

P

(cid:33)

Bi ≥ n(cid:48)

δ

≥ 1 − δ/2

i=1

and we deduce that P(τk(cid:63) ≤ nδ) ≥ (1 − δ/2)2 ≥ 1 − δ.
(cid:3)
Proof of Theorem 18 Fix any δ ∈ (0, 1), let nδ/2 = (cid:98)10(V +ln(4/δ))/(p·inf r∈RN (cid:63)−1 L(r)2)(cid:99)
be the integer part of the upper bound of Proposition 17 (with probability 1 − δ/2) and
n > nδ/2 and let {(Xt,(cid:98)kt)}n
let rδ/2,n be the upper bound of the Theorem 11 (with probability 1 − δ/2). Now, ﬁx any
34. Then, denoting τk(cid:63) = inf{i = 1 . . . n :(cid:98)ki = k(cid:63)}, applying the Bayes rules gives that
t=1 be the sequence of random variables deﬁned in Proposition
Moreover, on the event {τk(cid:63) < nδ/2} =(cid:84)

{(cid:98)kt = k(cid:63)}, the true ranking structure Rk(cid:63)
Due to the deﬁnition of nδ/2, we have that P(τk(cid:63) < nδ/2) ≥ 1 − δ/2 by Proposition 17.
is identiﬁed for any nδ/2 < t ≤ n. Therefore, the distance (cid:107)Xˆın − x(cid:63)(cid:107)2 can be bounded
using the last n − nδ/2 samples with a similar technique as the one used in the proof
of Theorem 11 (where the ranking structure is assumed to be known) and we get that
P((cid:107)Xˆın − x(cid:63)(cid:107)2 ≤ rδ,n | τk(cid:63) < nδ/2) ≥ 1 − δ/2. Finally, noticing that (1 − δ/2)2 ≥ 1 − δ ends
(cid:3)
the proof.

P((cid:107)Xˆın − x(cid:63)(cid:107)2 ≤ rδ,n) ≥ P((cid:107)Xˆın − x(cid:63)(cid:107)2 ≤ rδ,n | τk(cid:63) < nδ/2) · P(τk(cid:63) < nδ/2).

t≥nδ/2

Appendix D. Proofs of Section 6

D.1 Preliminary results
Proposition 36 (Convex Hulls) Let {Xi}n
the convex hull of {Xi}n
i=1, denoted by CH{Xi}n

i=1 be any set of points where Xi ∈ Rd. Then,
i=1, admits the following representations:

1. CH{Xi}n
2. CH{Xi}n

i=1 = {(cid:80)n

i=1 is the smallest convex set that contains {Xi}n

i=1 λiXi : (λ1, . . . , λn) ∈ Rn, (cid:80)n

i=1,

i=1 λi = 1, λi ≥ 0, i = 1 . . . n}.

26

Proof See Berger (1977) for detailed a proof.

(cid:3)

Lemma 37 Let X ⊂ Rd be any compact and convex set and let  > 0 be any positive
constant. Then, the -ball of X , deﬁned by B(X , ) = {x ∈ Rd : d(x,X ) ≤ } where
d(x,X ) = minx(cid:48)∈X(cid:107)x − x(cid:48)(cid:107)2, is a convex set.
Proof Pick any (b1, b2) ∈ B(X , )2. By deﬁnition of the -ball, there exists (x1, 1) ∈ X ×Rd
such that b1 = x1 + 1 and (cid:107)1(cid:107)2≤  (resp. b2 = x2 + 2 where x2 ∈ X and (cid:107)2(cid:107)2≤ ). The
convexity of X implies that ∀λ ∈ (0, 1),

(1 − λ)b1 + λb2 = λx1 + (1 − λ)x2

+ λ1 + (1 − λ)2

.

(cid:124)

(cid:123)(cid:122)

∈X

(cid:125)

(cid:124)

(cid:123)(cid:122)

(cid:107)·(cid:107)2≤

(cid:125)

Hence (1 − λ)b1 + λb2 ∈ B(X , ) and we deduce that B(X , ) is a convex set.

(cid:3)

Lemma 38 Let R ⊆ R∞ be any continuous ranking structure and let {(Xi, f (Xi))}n+1
i=1 be
any sample satisfying f (X(1)) < f (X(2)) < ··· < f (X(n+1)). Then, if Ln+1 denotes the
empirical ranking loss taken over the sample, we have the following equivalence:
{r ∈ R : Ln+1(r) = 0} = {r ∈ R : r(X(i+1), X(i)) = 1, ∀i ∈ {1 . . . n}}.

Proof (⊆) Let r ∈ R be any ranking rule satisfying Ln+1(r) = 0. Since r perfectly ranks
the sample, for any i ∈ {1 . . . n}, we have that

r(X(i+1), X(i)) = rf (X(i+1), X(i)) = sgn(f (Xi+1) − f (X(i))) = 1.

(⊇) Let r ∈ R be any ranking rule satisfying r(X(i+1), X(i)) = 1, ∀i ∈ {1 . . . n}. Since
r ∈ R ⊆ R∞ is a continuous ranking, there exists a function h ∈ C0(X , R) such that
r(x, x(cid:48)) = sgn(h(x) − h(x)), ∀(x, x(cid:48)) ∈ X 2. Pick any j (cid:54)= k ∈ {1 . . . n + 1}2 and assume,
without loss of generality, that (j) > (k). Using the function h, we have that

r(X(j), X(k)) = sgn(h(X(j)) − h(X(k)))

(cid:33)

(cid:32)j−1(cid:88)
(cid:32)j−1(cid:88)

i=k

= sgn

= sgn

h(X(i+1)) − h(X(i))

(cid:33)
(cid:12)(cid:12)h(X(i+1)) − h(X(i))(cid:12)(cid:12) · sgn(h(X(i+1)) − h(X(i)))

i=k

= 1.

Hence, ∀(j, k) ∈ {1 . . . n+1}2, r(X(j), X(k)) = sgn(f (X(j))−f (X(k))) and so Ln+1(r) = 0. (cid:3)

27

D.2 Proofs of the results of Section 6
Proof of Proposition 21 (⇒) Assume that there exists r ∈ RP(k) satisfying Ln+1(r) = 0.
Since r is a polynomial ranking, there exists a polynomial function fr : x (cid:55)→ (cid:104)ωr, φk(x)(cid:105) + cr
where (ωr, cr) ∈ Rdim(φk) × R such that ∀(x, x(cid:48)) ∈ X 2, r(x, x(cid:48)) = sgn(f (x) − f (x(cid:48))). Since
Ln+1(r) = 0, applying Lemma 38 gives that ∀i ∈ {1 . . . n},

1 = r(X(i+1), X(i)) = sgn(fr(X(i+1)) − fr(X(i))) = sgn((cid:104)ωr, φk(X(i+1)) − φk(X(i))(cid:105)).
Hence, there exists ω = ωr ∈ Rdim(φk) s.t. (cid:104)ω, φk(X(i+1)) − φk(X(i))(cid:105) > 0, ∀i ∈ {1 . . . n}.
(⇐) Assume that there exists ω ∈ Rdim(φk) s.t. (cid:104)ω, φk(X(i+1))−φk(X(i))(cid:105) > 0, ∀i ∈ {1 . . . n}.
Pick any constant c ∈ R, deﬁne the polynomial function fω : x (cid:55)→ (cid:104)ω, φk(x)(cid:105) + c and let
rfω ∈ RP(k) be its induced polynomial ranking rule. Then, ∀i ∈ {1 . . . n}, we have that
sgn((cid:104)ω, φk(Xi+1) − φk(X(i))(cid:105)) = sgn(fω(X(i+1)) − fω(X(i))) = rfω (X(i+1), X(i)) = 1

and applying Lemma 38 gives that Ln+1(rfω ) = 0.
Proof of Lemma 22 Note that Yi · (cid:104)ω, Xi(cid:105) > 0 ⇔ (cid:104)ω, Yi · Xi(cid:105) > 0. Therefore, one can
assume without loss of generality that ∀i ∈ {1 . . . n}, Yi = 1 by replacing Xi by Yi · Xi.
(⇒) Assume that there exists ω ∈ Rd such that ∀i ∈ {1 . . . n}, (cid:104)ω, Xi(cid:105) > 0 and assume
by contradiction that (cid:126)0 ∈ CH{Xi}n
i=1, we know by Proposition 36
i=1 λi = 1 and λi ≥ 0,
i ∈ {1 . . . n} and it gives the contradiction

that there exists (λ1, . . . , λn) ∈ Rn such that (cid:126)0 = (cid:80)n

i=1 λiXi, (cid:80)n

i=1. Since (cid:126)0 ∈ CH{Xi}n

(cid:3)

0 = (cid:104)ω,(cid:126)0(cid:105) =

λi(cid:104)ω, Xi(cid:105) > 0.

n(cid:88)

i=1

i=1

i=1. Since n and d are ﬁnite, CH{Xi}n

(⇐) Assume that (cid:126)0 /∈ CH{Xi}n
i=1 is a closed, compact
(cid:107)x(cid:107)2 = d exists and the condition (cid:126)0 /∈ CH{Xi}n
and convex set and minx∈CH{Xi}n
i=1 implies
that d > 0. Now, let xd ∈ CH{Xi}n
i=1 be the (unique) point of the convex hull satisfying
i=1, (cid:104)x, xd(cid:105) ≥ d2. Assume that
(cid:107)xd(cid:107)2 = d. We show by contradiction that ∀x ∈ CH{Xi}n
i=1 s.t. (cid:104)x, xd(cid:105) < d2. The convexity of the convex hull implies that
there exists x ∈ CH{Xi}n
the whole line L = (x, xd) ⊆ CH{Xi}n
i=1 also belongs to the convex hull. However, since
(cid:107)xd(cid:107)2 = d and (cid:104)x, xd(cid:105) < (cid:107)xd(cid:107)2, the line L is not tangent to the ball B((cid:126)0, d) and intersects
it. Hence, there exists x(cid:48) ∈ L ∩ B((cid:126)0, d) s.t. (cid:107)x(cid:48)(cid:107)2 < d. Since x(cid:48) ∈ L ⊆ CH{Xi}n
i=1 belongs
to the convex hull, it leads us to the contradiction
(cid:107)x(cid:107)2 ≤ (cid:107)x(cid:48)(cid:107)2 < d =

(cid:107)x(cid:107)2 .

min

min

x∈CH{Xi}n
We deduce that ∀x ∈ CH{Xi}n
there exists ω = xd ∈ Rd such that ∀i ∈ {1 . . . n}, (cid:104)ω, Xi(cid:105) > 0.

i=1, (cid:104)xd, x(cid:105) ≥ d > 0. Finally, since {Xi}n

x∈CH{Xi}n

i=1

i=1

i=1 ∈ CH{Xi}n

i=1,
(cid:3)

28

Proof of Corollary 23 Combining Proposition 21 and Lemma 22 gives the next equiva-
lences:

Ln+1(r) = 0 ⇔ ∃ω ∈ Rdim(φk) s.t. (cid:104)ω, φk(X(i+1)) − φk(X(i))(cid:105) > 0, ∀i ∈ {1 . . . n}

min
r∈RP(k)

⇔ (cid:126)0 /∈ CH{(φk(X(i+1)) − φk(X(i)))}n

i=1.

i=1 λi(φN (X(i+1)) − φN (X(i))) = (cid:126)0, (cid:80)n

exists λ = (λ1, . . . , λn) ∈ Rn such that(cid:80)n

By Proposition 36, we know that (cid:126)0 ∈ CH{(φN (X(i+1)) − φN (X(i)))}n
i=1 if and only if there
i=1 λi = 1
and λi ≥ 0, i = 1 . . . n. Putting those constraints into matricial form gives the result.
(cid:3)
Proof of Proposition 24 (⇒) Assume that there exists r ∈ RC(k) such that Ln+1(r) = 0
and let {hi}n+1
Lemma 38, we know that ∀i ∈ {1 . . . n}, r(X(i+1), X(i)) = 1 and so h1 ≥ h2 ≥ ··· ≥ hn+1.
On the other hand, by deﬁnition of the convex rankings of degree k, we know that all the

i=1 be the sequence of classiﬁers deﬁned by hi(x) = I(cid:8)r(x, X(i)) ≥ 0(cid:9). By

classiﬁers are of the form hi(x) =(cid:80)k
(cid:80)k
hi(X(j)) = I{(j) ≥ i}. Deﬁne the non-continuous function ˆf (x) =(cid:80)n+1

(i) hi(x) =
I{li,m ≤ x ≤ ui,m}, (ii) h1 ≥ h2 ≥ ··· ≥ hn+1 and (iii) ∀(i, j) ∈ {1 . . . n + 1}2,
i=1 hi(x) and observe

(⇐)Assume that there exists a sequence of classiﬁers {hi}n+1

I{li,m ≤ x ≤ ui,m}.

satisfying:

m=1

m=1

i=1

that ∀(j, j(cid:48)) ∈ {1 . . . n + 1}2,

(cid:32) k(cid:88)

(cid:80)N

I{(j) ≥ i} − I(cid:8)(j(cid:48)) ≥ i(cid:9)(cid:33)


1 − l−x
1
1 − x−u
0

i=1





φ(x, l, u) =

if x ∈ [l − , l[
if x ∈ [l, u]
if x ∈]u, u + ]
otherwise.

r ˆf (X(j), X(j(cid:48))) = sgn

and so Ln+1(r) = 0. Now, let ˆf =(cid:80)n+1

i=1

function ˆf where

= sgn((j) − (j(cid:48))) = rf (X(j), X(j(cid:48)))

k=1 φ(x, li,k, ui,k) be an approximation of the

i=1 ∪ {li,k}k=1...N

i=1...n+1 ∪ {ui,k}k=1...N

First, by continuity of φ we know that ˆf is continuous. Second, note that ∀i ∈ {1 . . . n + 1}
and ∀ < 1 = min{|x1 − x2| : x1 (cid:54)= x2 ∈ {X(i)}n+1
i=1...n+1}, we
have that ˆf(X(i)) = ˆf (X(i)) and so Ln+1(rh) = Ln+1(rf ) = 0. Finally, using the same
decomposition, it is easy to see that ∀ < 2 = min{|x1 − x2| : x1 (cid:54)= x2 ∈ {li,k}k=1...N
i=1...n+1 ∪
i=1...n+1}/2, the level sets of ˆf are unions of at most k segements (convex sets). Hence,
{ui,k}k=1...N
(cid:3)
there exists r = r ˆf
Proof of Proposition 25 (⇒) Assume that there exists r ∈ RC(1) such that Ln+1(r) = 0.
By Lemma 38, ∀j (cid:54)= k ∈ {1 . . . n + 1}2 we have that r(X(j), X(k)) = 2I{(j) > (k)} − 1 and
so, for any k ∈ {1 . . . n},

∈ RC(k) satisfying Ln+1(r) = 0.

1. {X(i)}n+1

i=k+1 ∈ {x ∈ X : r(x, X(k+1)) ≥ 0},

29

2. X(k) /∈ {x ∈ X : r(x, X(k+1)) ≥ 0}.

Now, ﬁx any k ∈ {1 . . . n} and observe that the set {x ∈ X : r(x, X(k)) ≥ 0} is a convex set
by convexity of the ranking rule r. However, since CH{X(i)}n+1
i=k+1 is the smallest convex set
that contains {X(i)}n+1
i=k+1 ⊆
{x ∈ X : r(x, X(k+1)) ≥ 0}. Therefore, putting the previous statements altogether gives
that ∀k ∈ {1 . . . n},

i=k+1 (see Proposition 36), we necessarily have that CH{X(i)}n+1

Rk such that (cid:80)k

By Proposition 36, it implies that ∀k ∈ {1 . . . n}, there does not exist any λ = (λ1 . . . λk) ∈
i=1 λi = 1 and λi ≥ 0. Putting those con-

i=1 λiX(n+2−i) = X(n+1−k), (cid:80)k

i=k+1.

straints into matricial form gives the result.
(⇐) Assume that all the polyhedrons are empty. Reproducing the same (inverse) steps as
in the ﬁrst equivalence gives that ∀k ∈ {1 . . . n}, X(k) /∈ CH{X(i)}n+1
i=n ⊂ ··· ⊂ CH{X(i)}n+1
i=1 .

i=k+1 and so

X(k) /∈ CH{X(i)}n+1

CH{X(n+1)} ⊂ CH{X(i)}n+1

Now, deﬁne the non-continuous function ˆf (x) =(cid:80)n+1
Ln+1(r ˆf ) = 0 and let ˆf(x) =(cid:80)n+1

i=1

∀i ∈ {1 . . . n + 1},

I{x ∈ CH{X(j)}n+1
j=i }, observe that
i=1 φ,i(x) be an approximation of the function f , where
(cid:33)

1 − d(x, B(CH{X(j)}n+1

j=i , 2(n + 1 − i)))


(cid:32)

φ,i(x) =

j=i , 2(n + 1 − i)) ≤  and 0 otherwise. First, note that for any convex
if d(x, B(CH{X(j)}n+1
set X , the function x (cid:55)→ d(x,X ) is continuous and so is the function ˆf. Second, note that
∀i ∈ {1 . . . n + 1} and ∀ < (cid:63) = mini=1...n d(X(i), CH{X(j)}n+1
j=i+1)/(2n + 2), we have that
ˆf(X(i)) = ˆf (X(i)) and so Ln+1(rh) = Ln+1(rf ) = 0. Finally, since the -Ball B(X , ) of
any convex set X is also a convex set (see Lemma 37), ∀x ∈ X and ∀ < (cid:63), the level set
∈ RC(1) satisfying
{x(cid:48) ∈ X : ˆf(x(cid:48)) ≥ ˆf(x)} is a convex set. Hence, there exists r = r ˆf
(cid:3)
Ln+1(rh) = 0.

References

Marcel Berger. Geometry I. Springer Science & Business Media, 1977.

St´ephane Boucheron, G´abor Lugosi, and Pascal Massart. Concentration inequalities: A

nonasymptotic theory of independence. OUP Oxford, 2013.

Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press,

2004.

Adam D Bull. Convergence rates of eﬃcient global optimization algorithms. The Journal

of Machine Learning Research, 12:2879–2904, 2011.

St´ephan Cl´emen¸con. On U-processes and clustering performance. In Advances in Neural

Information Processing Systems, pages 37–45, 2011.

30

St´ephan Cl´emen¸con and Nicolas Vayatis. Overlaying classiﬁers: a practical approach to

optimal scoring. Constructive Approximation, 32(3):619–648, 2010.

St´ephan Cl´emen¸con, Gabor Lugosi, and Nicolas Vayatis. Ranking and empirical minimiza-

tion of u-statistics. The Annals of Statistics, pages 844–874, 2010.

David Cohn, Les Atlas, and Richard Ladner. Improving generalization with active learning.

Machine learning, 15(2):201–221, 1994.

Sanjoy Dasgupta. Two faces of active learning. Theoretical computer science, 412(19):

1767–1781, 2011.

Jean-Bastien Grill, Michal Valko, and R´emi Munos. Black-box optimization of noisy func-

tions with unknown smoothness. In Neural Information Processing Systems, 2015.

Steve Hanneke. Rates of convergence in active learning. The Annals of Statistics, 39(1):

333–361, 2011.

Nikolaus Hansen. The cma evolution strategy: a comparing review.

In Towards a new

evolutionary computation, pages 75–102. Springer, 2006.

Pierre Hansen, Brigitte Jaumard, and Shi-Hui Lu. Global optimization of univariate lip-
schitz functions: I. survey and properties. Mathematical programming, 55(1-3):251–272,
1992.

Steven G Johnson.

The NLopt nonlinear-optimization package, 2014.

http://ab-

initio.mit.edu/nlopt.

Donald R Jones, Cary D Perttunen, and Bruce E Stuckman. Lipschitzian optimization
without the lipschitz constant. Journal of Optimization Theory and Applications, 79(1):
157–181, 1993.

Donald R Jones, Matthias Schonlau, and William J Welch. Eﬃcient global optimization of

expensive black-box functions. Journal of Global Optimization, 13(4):455–492, 1998.

P Kaelo and MM Ali. Some variants of the controlled random search algorithm for global

optimization. Journal of optimization theory and applications, 130(2):253–264, 2006.

Ruben Martinez-Cantin. Bayesopt: A bayesian optimization library for nonlinear optimiza-
tion, experimental design and bandits. The Journal of Machine Learning Research, 15
(1):3735–3739, 2014.

R´emi Munos. Optimistic optimization of deterministic functions without the knowledge of

its smoothness. In Advances in neural information processing systems, 2011.

J´anos D Pint´er. Global optimization in action. Scientiﬁc American, 264:54–63, 1991.

Luis Miguel Rios and Nikolaos V Sahinidis. Derivative-free optimization: a review of algo-
rithms and comparison of software implementations. Journal of Global Optimization, 56
(3):1247–1293, 2013.

31

Yaroslav D Sergeyev, Roman G Strongin, and Daniela Lera. Introduction to global opti-

mization exploiting space-ﬁlling curves. Springer Science & Business Media, 2013.

Vladimir Vapnik. Principles of risk minimization for learning theory. In Advances in neural

information processing systems, pages 831–838, 1992.

Zelda B Zabinsky. Stochastic adaptive search for global optimization, volume 72. Springer

Science & Business Media, 2013.

Zelda B Zabinsky and Robert L Smith. Pure adaptive search in global optimization. Math-

ematical Programming, 53(1-3):323–338, 1992.

32

