6
1
0
2

 
r
a

 

M
7
1

 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
3
3
5
5
0

.

3
0
6
1
:
v
i
X
r
a

COMPRESSED SENSING OF DATA WITH A KNOWN

DISTRIBUTION

MATEO D´IAZ, MAURICIO JUNCA, FELIPE RINC ´ON, AND MAURICIO VELASCO

Abstract. Compressed sensing is a technique for recovering an unknown sparse
signal from a number of random linear measurements. The number of measure-
ments required for perfect recovery plays a key role and it exhibits a phase transi-
tion. If the number of measurements exceeds certain level related with the sparsity
of the signal, exact recovery is obtained with high probability. If the number of
measurements is below this level, exact recovery occurs with very small probability.
In this work we are able to reduce this threshold by incorporating statistical infor-
mation about the data we wish to recover. Our algorithm works by minimizing a
suitably weighted (cid:96)1-norm, where the weights are chosen so that the expected sta-
tistical dimension of the descent cones of a weighted cross-polytope is minimized.
We also provide Monte Carlo algorithms for computing intrinsic volumes of these
descent cones and estimating the failure probability of our methods.

1. Introduction

The sensing problem consists on trying to recover a signal x0 ∈ Rd from m linear
measurements encoded in a vector y0 := Ax0, where A is a given m× d matrix with
m < d. In the seminal works by Cand`es, Romberg, and Tao, [CT05, CRT06], the
following convex optimization algorithm is proposed as a possible solution:

(P)

∆(y0) := arg min

x∈Rd

(cid:107)x(cid:107)1 s.t. Ax = y0.

We say that the problem (P) is successful or that it performed a perfect recovery
for A and x0 if it has a unique solution and this solution is x0. We cannot expect
this method to work for arbitrary signals and measurements; by taking m strictly less
than d we are collapsing dimensions and consequently losing information. However,
if A ∈ Rm×d is a random matrix with i.i.d. entries N(0, 1), it is shown in [CRT06]
that this method is successful with very high probability for all suﬃciently sparse
vectors, i.e., vectors with a low number of non-zero entries. The authors show that
if a matrix A satisﬁes the so-called Restricted Isometry Property and x0 is sparse
enough, then (P) is successful for A and x0. They also show that random matrices
deﬁned as above satisfy this property with very high probability for suitable choices
of m and d.

Key words and phrases. Compressed sensing, Statistical dimension, intrinsic volumes, weighted

(cid:96)1-norm, Monte Carlo algorithm.

1

2

MATEO D´IAZ, MAURICIO JUNCA, FELIPE RINC ´ON, AND MAURICIO VELASCO

Although the Restricted Isometry Property is a suﬃcient condition for (P) to be
successful, it does not explain the phase transition phenomenon exhibited by the
probability of perfect recovery.

Later work has focused on understanding the geometry behind this phase transition
phenomenon. We now discuss these results in detail, as they are relevant for the
approach taken in this paper.
Deﬁnition 1.1. (Descent Cone) For a point x0 ∈ Rd and f : Rd → R a convex
function, the descent cone D(f, x0) of f at x0 is given by

D(f, x0) := cone{x − x0 : f (x) ≤ f (x0)}.

In other words, the descent cone is the cone generated by all directions from x0
in which f decreases. The following theorem from [CRPW12] presents a geometrical
formulation of success. Even though it has a very simple proof, it is a very powerful
tool to study the problem.

Theorem 1.2. ([CRPW12]) The compressed sensing method (P) is successful for A
and x0 if and only if D((cid:107) · (cid:107)1, x0) ∩ ker(A) = {0}.

If A ∈ Rm×d is a random matrix with i.i.d. entries N(0, 1) then ker(A) is uniformly
distributed over the Grassmannian Gr(d − m, Rd) of d − m-dimensional subspaces of
Rd. It follows that if K is a ﬁxed subspace of dimension d − m and Q ∈ O(d) is a
random rotation matrix, chosen with the Haar measure on O(d)

PA{(P) is successful for x0 and A} = PQ {D((cid:107) · (cid:107)1, x0) ∩ QK = {0}} .

The problem of computing the probability that a random subspace intersects a
ﬁxed cone has been studied in integral geometry for a long time, and there are
explicit formulas for this probability in terms of the so-called intrinsic volumes. For
the purposes of this paper we will only describe the basic concepts related to our
problem, but we refer the reader to [SW]. We follow the point of view presented
in [ALMT14] where the concept of statistical dimension of a cone is introduced as
the key invariant to understand phase transitions. An alternative approach to phase
transitions is presented in [CRPW12] in terms of Gaussian widths of descent cones.
Given C ⊂ Rd any closed convex set and x ∈ Rd, deﬁne the projection of x over
C as πC(x) := arg min{(cid:107)x − y(cid:107)2 : y ∈ C}.
Deﬁnition 1.3. (Intrinsic Volumes) Let C a polyhedral cone in Rd. For each
0 ≤ k ≤ d, the kth intrinsic volume νk(C) is given by

νk(C) := Pg{πC(g) lies in the interior of a k-dimensional face of C},

where g is a standard normal random vector in Rd.

There are many interesting properties about intrinsic volumes, and also many open
questions; see [SW, ALMT14]. One of these properties is that the intrinsic volumes
of a cone C deﬁne a discrete probability measure over the set {0, 1, . . . , d}. The
expected value of a random variable with this distribution is called the statistical
dimension of C.

COMPRESSED SENSING OF DATA WITH A KNOWN DISTRIBUTION

3

Deﬁnition 1.4. (Statistical Dimension) Let C ⊆ Rd a polyhedral cone, deﬁne
the statistical dimension δ(C) as

d(cid:88)

δ(C) :=

kνk(C).

k=0

For many theoretical results it is useful to have the following equivalent deﬁnition

of statistical dimension.
Lemma 1.5. ([ALMT14]) If C ⊂ Rd is a polyhedral cone then its statistical dimen-
sion is equal to

δ(C) := Eg[(cid:107)πC(g)(cid:107)2
2],
where g is a standard normal random vector in Rd.

The statistical dimension of the cone D((cid:107) · (cid:107)1, x0) seems to be very close to the
inﬂection point of the phase transition. Numerical experiments based on Monte
Carlo simulations support this statement. The following theorem is an attempt to
formalize this claim in a more general setting. Given a convex function f : Rd → R
and x0 ∈ Rd, consider the convex optimization problem
f (x) s.t. Ax = Ax0.

(Pf )
Theorem 1.6. ([ALMT14]) Fix a tolerance η ∈ (0, 1). Let x0 ∈ Rd, f : Rd → R a
convex function, and A ∈ Rm×d a random matrix with i.i.d. entries N(0, 1). Then,

min
x∈Rd

d =⇒ (Pf ) succeeds with probability ≤ η
d =⇒ (Pf ) succeeds with probability ≥ 1 − η,

m ≤ δ(D(f, x0)) − aη
m ≥ δ(D(f, x0)) + aη

where aη :=(cid:112)8 log 4/η.

√
√

The proof of this theorem is based on the kinematic formula from integral ge-
ometry, which relates the probability of success with the intrinsic volumes of the
corresponding descent cone.
Deﬁnition 1.7. Let C ⊂ Rd a closed convex cone. For each k ∈ {0, 1, . . . , d}, the
kth tail functional is deﬁned as

Similarly, the kth half-tail functional is deﬁned as

d(cid:88)
d(cid:88)

j=k

tk(C) :=

νj(C).

hk(C) :=

νj(C).

j=k

j−k even

Theorem 1.8. (Kinematic formula [SW]) Let C ⊂ Rd a closed convex cone and
L ⊂ Rd a linear subspace of dimension d − m. Then

PQ {C ∩ QL = {0}} = 1 − 2hm+1(C).

4

MATEO D´IAZ, MAURICIO JUNCA, FELIPE RINC ´ON, AND MAURICIO VELASCO

Remark 1.9. It is shown in [ALMT14] that for each closed convex cone C ⊂ Rd
that is not a linear subspace,

2hk(C) ≥ tk(C) ≥ 2hk+1(C)

for k = 0, 1, 2,··· , d − 1.

Thus, combining the previous two results, we conclude that 1 − tm(D((cid:107) · (cid:107)1, x0)) is
very close to the exact probability of perfect recovery.

Unfortunately, even for simple polyhedral cones such as D((cid:107) · (cid:107)1, x0), there is no
simple way of computing their intrinsic volumes. One of the contributions of this
work is to present an eﬃcient algorithm to estimate these intrinsic volumes via Monte
Carlo simulations.

The general setup of Compressed Sensing is made for arbitrary sparse signals
x0, but in real applications these signals are random vectors that follow a speciﬁc
distribution. In many cases we are able to know the underlying distribution of the
signals, or at least an approximation. This new setting opens the possibility of further
reducing the number of measurements m needed for perfect recovery by incorporating
this statistical information. A second contribution of this article is to propose a
methodology based on weighted (cid:96)1-minimization to approach this question.

Related work. Compressed sensing with prior information has been studied in the
past under diﬀerent models. In each model the known information is diﬀerent. For
example, the paper [MDR14] analyzes the case where a signal similar to the one to
be recovered is known beforehand. Also, [VL10, FMSY12] assume information about
the support of the signal x0. Speciﬁcally, the ﬁrst paper assumes to know part of
the support entries and the second one assumes to have prior knowledge about the
support location. All these analyses mainly rely on the Restricted Isometry Property,
an approach that we do not pursue on this work.

The idea of using weighted (cid:96)1-minimization in this subject was ﬁrst introduced
in [CWB08]. This paper proposes an iterative algorithm with dynamic weights to
reduce the number of necessary measurements to recover the signal. Weighted (cid:96)1-
minimization has also been investigated under probabilistic hypotheses. The papers
[Xu10, KXH+09] consider the case in which the allowed non-zero entries of the vector
fall into two sets, and each set has a diﬀerent probability of being non-zero; [KXH+11]
generalizes this model to n sets (our numerical example 5.1 falls into this setting).
The article by [MP15] considers a Bayesian setting, where the entries are independent
and the probability of being non-zero is given by a continuous function. This paper
independently obtained a result analogous to Theorem 2.7 below. We discuss diﬀer-
ences and similarities in the next section. The studies done in these works are based
on Grassmmann angles; our approach using intrinsic volumes is implicitly related to
these concepts.

Weighted (cid:96)1-minimization has also recently been used in other contexts. For in-
stance, [RW15] develops a theory about the use of weighted norms to better inter-
polate smooth functions that are also sparse. Similarly, [SRR15] uses a weighting
function w(·) to tackle the problem of superresolution imagining. In particular, the
paper studies how to recover a point measure that encodes a signal by using a convex
algorithm and a relatively small set of measurements for this signal.

COMPRESSED SENSING OF DATA WITH A KNOWN DISTRIBUTION

5

Outline. Section 2 describes the stochastic setting we study, introduces a method to
take advantage of this model by using weighted (cid:96)1-norms, and gives an eﬃcient way of
choosing good weights. In Section 3 we investigate the geometry of the descent cones
generated by weighted cross-polytopes and present a novel Monte Carlo algorithm
to estimate their intrinsic volumes. In Section 4 we use these results to develop a
method of steepest descent, based on a Monte Carlo algorithm, that minimizes the
expected statistical dimension in terms of the weights. We show a few numerical
examples where we apply our algorithms in Section 5. Finally, we conclude with
some open problems in Section 6.

2. Compressed sensing with a priori distributions

Let X0 ∈ Rd be the data we wish to recover and assume that X0 follows some
known distribution F. In order to reduce the number of necessary measurements m,
we aim to increase the probability of (P) being successful. Theorem 1.2 gives us a
good insight on what we can do. Imagine that we have a particular point x(cid:48) with
very high probability. Then, if we can modify the (cid:96)1-norm in order to sharpen the
descent cone at x(cid:48) and reduce the probability of intersection with the kernel of A, we
should be able to increase the probability of perfect recovery; see Figure 1. How to
obtain a good modiﬁcation of the (cid:96)1-norm based on the distribution F is explained
in this section.

Figure 1. The blue points are x(cid:48), the polytopes are the (cid:96)1 and (cid:96)w
balls and the black planes are the ker(A) + x(cid:48).

1

Deﬁnition 2.1. (w-weighted (cid:96)1-norm) For a ﬁxed vector w ∈ Rd
is given by

>0, the (cid:96)w

1 -norm

d(cid:88)

(cid:107)x(cid:107)w

1 =

wi|xi|.

Note that by modifying the weights we are able to deform the descent cones.

i=1

Consider the optimization problem

(Pw)

∆w(y0) = arg min

x∈Rd

(cid:107)x(cid:107)w

1

s.t. Ax = y0.

We say that the problem (Pw) is successful or performed a perfect recovery for A

and x0 if it has a unique solution and this solution is x0, where y0 := Ax0.

6

MATEO D´IAZ, MAURICIO JUNCA, FELIPE RINC ´ON, AND MAURICIO VELASCO

Deﬁnition 2.2. Let X0 ∼ F be a random vector in Rd and A ∈ Rm×d a random
matrix with i.i.d. entries N(0, 1) independent of X0. For a given vector w ∈ Rd
>0 we
deﬁne the success probability as

s(w) = PA,X0{(Pw) is successful for A and X0}.

1 ball of radius ||x||w

Denote [d] := {1, 2, . . . , d}. If x ∈ Rd, its support is supp(x) := {i ∈ [d] : xi (cid:54)= 0}.
If B is the (cid:96)w
1 , the support of x determines which face of B the
vector x lies in. It follows that all vectors with a ﬁxed support I ⊆ [d] have the same
descent cone D((cid:107) · (cid:107)w
1 , x)
where x has support I. In this way we focus on the distribution induced by F over
the subsets I of [d].
Deﬁnition 2.3. (Expected intrinsic volumes) For a ﬁxed vector w ∈ Rd
X0 ∼ F a random vector, we deﬁne the kth expected intrinsic volume as

1 , x). We will thus adopt the notation D(I, w) := D((cid:107) · (cid:107)w

>0 and

¯νk(w) = EX0 [νk(D (supp(X0), w))] ,

for k = 0, . . . , d.

We deﬁne ¯tk and ¯hk as the tail and the half-tail of the expected intrinsic volumes,
just as in Deﬁnition 1.7. It is easy to prove by conditioning that the failure probability
is given by

1 − s(w) = 1 − 2¯hm+1(w).

Since in the deterministic case this probability is very hard to compute, in the next
sections we will use our Monte Carlo algorithm to estimate this probability. We now
concentrate on the behavior of the phase transition in this setting, and we state an
equivalent version of Theorem 1.6.
Theorem 2.4. Fix a tolerance η ∈ (0, 1). Let w ∈ Rd
>0 be a ﬁxed vector of weights,
X0 ∼ F a random vector, and A ∈ Rm×d a random matrix with i.i.d. entries N(0, 1)
independent from X0. Then

(cid:8)m ≤ δ(D(supp(X0), w)) − aη/2
(cid:8)m ≥ δ(D(supp(X0), w)) + aη/2

d(cid:9) ≥ 1 − η/2 =⇒ s(w) ≤ η,
d(cid:9) ≥ 1 − η/2 =⇒ s(w) ≥ 1 − η,

√
√

PX0
PX0

where aη/2 :=(cid:112)8 log 8/η.

Proof. First, we prove the initial implication. For a ﬁxed support I ⊆ [d] we de-
ﬁne s(I, w) := PA{(Pw) is successful for A and x, where x has support I}. Also,
let qI := PX0{supp(X0) = I}. Fix an m, and let Γ be the collection of supports
containing all the subsets I that satisfy

m ≤ δ(D(I, w)) − aη/2

√

d.

(cid:88)

I∈Γ

(cid:88)

I∈Γc

(2.1)

By conditioning on the support we obtain

s(w) =

qI s(w, I) +

qI s(w, I).

COMPRESSED SENSING OF DATA WITH A KNOWN DISTRIBUTION

7

Now, by deﬁnition, any element of Γ satisﬁes (2.1), thus we may apply Theorem 1.6
to bound s(w, I) in the ﬁrst sum with η/2 and in the second one with 1. Then,

s(w) ≤(cid:88)

(cid:88)

I∈Γc

qI

qI

η
2

+

I∈Γ
≤ η
2
= η.

+

η
2

The last inequality follows from the hypothesis on Γ; namely, PX0{Γc} ≤ η/2. An
(cid:3)
analogous argument proves the second implication.

The above theorem implies that the more the distribution of the random variable
δ(D(supp(X0), w)) is concentrated around its mean, the sharper the phase transition
will be. Hence, we introduce the following deﬁnition.
Deﬁnition 2.5. (Expected statistical dimension) For a ﬁxed vector w ∈ Rd
and a X0 ∼ F a random vector, the expected statistical dimension is given by
(2.2)

δ(w) := EX0 [δ(D(supp(X0), w))] .

>0

Now, we propose to choose the weights w so as to minimize the expected statistical
dimension. Theorem 2.7 below provides an analytically tractable upper bound for
δ(w) and a description of the resulting optimal weights. We begin with a lemma
that allows us to bound the statistical dimension of the cones D(I, w).
Lemma 2.6. For any I ⊆ [d], the statistical dimension of the descent cone D(I, w)
satisﬁes

(cid:32)

(cid:33)

w2
i

+

(cid:32)(cid:88)

i∈I

(cid:88)

i(cid:54)∈I

2√
2π

(cid:90) ∞

τ wi

δ(D(I, w)) ≤ inf
τ≥0

|I| + τ 2

(u − τ wi)2e− u2

2 du

,

(cid:33)

where the inﬁmum is achieved at a unique τ > 0.

Proof. This is a special case of Proposition 4.1 in [ALMT14]. Recall that the subd-
iﬀerential of a convex function f : Rd → R at a point x is deﬁned as

∂f (x) := {z ∈ Rd : f (y) ≥ f (x) + (cid:104)z, y − x(cid:105) for all y ∈ Rd}.

If x ∈ Rd satisﬁes xi > 0 if i ∈ I and xi = 0 otherwise, then for τ ≥ 0 the τ scaling
of the subdiﬀerential of || · ||w

1 at x is given by

(cid:40)

(cid:40)

τ ∂||x||w

1 :=

(z1, . . . , zd) :

zi = τ wi if i ∈ I
|zi| ≤ τ wi if i (cid:54)∈ I

(cid:41)

.

Therefore,

dist2(g, τ ∂||x||w

1 ) =

(cid:88)

i∈I

(cid:88)

i(cid:54)∈I

(gi − τ wi)2 +

((|gi| − τ wi)+)2,

where (z)+ = z if z > 0 and (z)+ = 0 otherwise. Taking expected value with respect
(cid:3)
to a standard normal vector g in Rd, the statement follows.

8

MATEO D´IAZ, MAURICIO JUNCA, FELIPE RINC ´ON, AND MAURICIO VELASCO

Recall that qI = P{supp(X0) = I} for I ⊆ [d]. If i ∈ [d], let βi :=(cid:80)

I(cid:51)i qI. We

>0 and any τ > 0 the following inequality holds

βj(τ wj)2 + (1 − βj)

(u − τ wj)2e− u2

2 du

.

(cid:34)(cid:114) 2

(cid:90) ∞

π

τ wj

(cid:35)(cid:33)

assume that 0 < βi < 1 for all i.
Theorem 2.7. For any w ∈ Rd
(cid:32)
(2.3) δ(w) ≤ EX0 [| supp(X0)|] +

d(cid:88)

j=1

The right hand side is minimized if λi := τ wi satisfy the equation

(2.4)

λi

βi

(1 − βi)

=

(u − λi)e− u2

2 du.

(cid:114) 2

(cid:90) ∞

π

λi

Proof. Conditioning on supp(X0) we have that
EX0[δ(D(supp(X0), w))] =

δ(D(I, w)) qI.

(cid:88)

I⊆[d]

By Lemma 2.6 we know that the right hand side is bounded for any choice of τI > 0
by

(cid:32)

(cid:88)

I⊆[d]

(cid:33)

(cid:32)(cid:88)

i∈I

(cid:88)

i(cid:54)∈I

2√
2π

(cid:90) ∞

τiwi

|I| + τ 2

I

qI

w2
i

+

(cid:33)

(u − τIwi)2e− u2

2 du

.

In particular, the inequality holds when all τI coincide with a given value τ . Changing
the order of summations we conclude that EX0[δ(D(supp(X0), w))] is bounded above

by (cid:88)

(cid:34)

d(cid:88)

(cid:33)

(cid:32)(cid:88)

I(cid:51)j

(cid:32)(cid:88)

I(cid:54)(cid:51)j

τ 2w2
j

qI

+

qI

(cid:33)(cid:32)(cid:114) 2

(cid:90) ∞

|I| qI +

I

j=1

(u − τ wj)2e− u2

2 du

,

which proves the ﬁrst claimed inequality. Now, note that the right-hand side of (2.3)
is bounded above by

(cid:33)(cid:35)

(cid:35)(cid:33)

(cid:32)

d(cid:88)

(u − λj)2e− u2

2 du

h(λ) : = EX0 [| supp(X0)|] +

 (cid:88)

i∈supp(X0)

j=1

(gi − λi)2 +

βjλ2

j + (1 − βj)
(cid:88)

i(cid:54)∈supp(X0)

= Eg,X0

π

τ wj

(cid:34)(cid:114) 2

π

(cid:90) ∞
 ,

λj

((gi − λi)+)2

where g is a normally distributed random vector in Rd. Since the function inside the
expectation is convex in λ, h(λ) is also convex. It follows that h(λ) is minimized at
any point with ∇h(λ) = 0. The equation ∂h

= 0 is equivalent to

λi

βi

(1 − βi)

=

and thus (2.4) holds.

(u − λi)e− u2

2 du,

(cid:3)

(cid:114) 2

(cid:90) ∞

∂λi

π

λi

COMPRESSED SENSING OF DATA WITH A KNOWN DISTRIBUTION

9

The paper [MP15] independently found a very similar result. In this paper, Misra
and Parrilo consider a Bayesian setting where the entries are independent and the
probability of being non-zero is given by a continuous function, i.e., βi = p(i/d).
If one takes a discrete measure to integrate, then the two methods ﬁnd the same
optimal weights. The useful feature about our formulation is the fact that equation
(2.4) allows us to use a simple binary search algorithm to eﬃciently ﬁnd these weights.

3. Estimating intrinsic volumes for weighted cross-polytopes

The key point for estimating the statistical dimension and the failure probability
is being able to approximate the corresponding intrinsic volumes.
In order to do
so, we need to understand the geometry of the descent cones generated by weighted
cross-polytopes, i.e., cones of the form D(I, w). In particular, we seek to answer the
following question: given a descent cone C and a point x, what is the dimension of
the face containing πC(x)? An eﬃcient method to answer this question will allow us
to design a Monte Carlo algorithm for estimating the intrinsic volumes of such cones.

Let w = (w1, . . . , wd) ∈ Rd

>0. The closed unit ball Bw is

Bw := {z ∈ Rd : ||z||w

1 ≤ 1} = convex{± ei/wi : i = 1, . . . , d},

where the eis are the standard basis vectors of Rd. The polytope Bw is combinato-
rially equivalent to the d-dimensional cross-polytope.
Let y ∈ Rd, I := supp(y), and k = |I|. We will restrict to the non-trivial case
when k ≥ 1. The descent cone D(I, w) of the norm ||·||w
1 at y depends only on I and
w, so in order to compute it we can assume that y = 1
i∈I ei/wi. In particular,
||y||w
k

1 = 1. We then have

(cid:80)

D(I, w) = cone{z − y : z ∈ Bw}.

Proposition 3.1. Suppose k (cid:54)= d. The descent cone D(I, w) is isometric to the cone

where e0, e1, . . . , ed−k are the standard basis vectors of Rd−k+1, and a :=(cid:112)(cid:80)

D(cid:48)(I, w) := cone{e0/a ± ei/wi : i = 1, . . . , d − k} × Rk−1,

i∈I w2
i .
In particular, after modding out by its lineality space, D(I, w) is combinatorially
equivalent to a cone over the (d − k)-dimensional cross-polytope.

Proof. Our description of Bw gives
D(I, w) = cone{±ei/wi − y : i = 1, . . . , d}

= cone{±ei/wi − y : i /∈ I} + cone{−ei/wi − y : i ∈ I}

Since the generators of the last cone satisfy the relation(cid:80)

cone is the (k − 1)-dimensional subspace

+ cone{ei/wi − y : i ∈ I}.
i∈I(ei/wi − w) = 0, this

L = {z ∈ Rd :(cid:80)

i∈I wizi = 0 and zi = 0 for any i /∈ I}.

We thus have

D(I, w) = cone{±ei/wi − y : i /∈ I} + cone{−ei/wi − y : i ∈ I} + L.

10

MATEO D´IAZ, MAURICIO JUNCA, FELIPE RINC ´ON, AND MAURICIO VELASCO

Since k (cid:54)= d, the ﬁrst summand in this expression contains the vector −2y. It follows
that the middle summand in the expression is redundant, because for any i ∈ I we
have (−ei/wi − y) + 2y ∈ L. Therefore
(3.1)

D(I, w) = cone{±ei/wi − y : i /∈ I} + L.
i . It is easy to check that the vectors e(cid:48)
i∈I w2

Now, let a :=(cid:112)(cid:80)

(cid:80)

i∈I wiei and {e(cid:48)

1, . . . , e(cid:48)

0 := − 1

a

d−k deﬁned
d−k} := {ei : i /∈ I} form an orthonormal basis
as e(cid:48)
for the orthogonal complement L⊥. We can write the generators of the cone in the
right hand side of Equation (3.1) as ±ei/wi − y = (e(cid:48)
0/a). The
vector y + e(cid:48)
0/a ± ei/wi) − (y + e(cid:48)
0/a ± ei/wi : i /∈ I} + L
0/a ± e(cid:48)
from which the desired result follows.

0/a is in the subspace L, so we have
D(I, w) = cone{(e(cid:48)
= cone{e(cid:48)
= cone{e(cid:48)

0/a ± ei/wi) − (y + e(cid:48)
0/a) : i /∈ I} + L

i/wi : i = 1, . . . , d − k} + L,

(cid:3)

0, e(cid:48)

1, . . . , e(cid:48)

Proposition 3.1 allows us to understand projections onto the descent cone D(I, w)

by understanding projections onto the cone

D(I, w) := cone{e0/a ± ei/wi : i = 1, . . . , d − k} ⊂ Rd−k+1.

We now recall the concept of polarity for polyhedral cones [Roc15]. If C is any

polyhedral cone in Rl, its polar cone is

C◦ := {z ∈ Rl : z · x ≤ 0 for all x ∈ C}.

There is an inclusion reversing correspondence between faces of C and faces of C◦:
If F is a face of C, its corresponding polar face of C◦ is

F ◦ := {z ∈ C◦ : z · x = 0 for all x ∈ F}.

Our interest in polar cones comes from the following fact: If F is any face of C, the
set of points of Rl whose projection onto C lands in F is the polyhedral cone F + F ◦.
The following proposition describes explicitly the cone polar to the descent cone

D(I, w).

Proposition 3.2. The cone polar to D(I, w) is equal to

D(I, w)◦ = cone{−ae0 ± w1e1 ± ··· ± wd−ked−k} ⊂ Rd−k+1.

In particular, D(I, w)◦ is combinatorially equivalent to a cone over the (d − k)-
dimensional hypercube.
Proof. The cone D(I, w)◦ is generated by the rays that are polar to the facets of
D(I, w). If F is a facet of D(I, w) then F has the form

number a is deﬁned as a :=(cid:112)(cid:80)

i∈I w2
i .

F = cone{e0/a + siei/wi : i = 1, . . . , d − k},

where all the si are either 1 or −1. The ray polar to F is generated by the vector
−ae0 + s1w1e1 + s2w2e2 + ··· + sd−kwd−ked−k, so the result follows.
(cid:3)

We now use these ideas to understand projections onto D(I, w). Recall that the

COMPRESSED SENSING OF DATA WITH A KNOWN DISTRIBUTION

11

Theorem 3.3. Let z = (z0, z1, . . . , zd−k) ∈ Rd−k+1. Consider the permutation i1,
i2, . . . , id−k of the numbers 1, 2, . . . , d − k that satisﬁes |zi1|/wi1 ≥ |zi2|/wi2 ≥ ··· ≥
|zid−k|/wid−k. For j = 0, 1, . . . , d − k − 1 deﬁne

bj := wi1|zi1| + ··· + wij|zij| − a2 + w2

i1 + ··· + w2
wij+1

ij

|zij+1|,

and

bd−k := wi1|zi1| + ··· + wid−k|zid−k|.

Then the numbers bj satisfy b0 ≤ b1 ≤ ··· ≤ bd−k, and the projection of the point z
onto the cone D(I, w) lands in the interior of a face of dimension l, where l is such
that bl−1 < az0 ≤ bl (where by convention b−1 = −∞ and bd−k+1 = ∞).
Proof. For any 1 ≤ j ≤ d − k − 1, the diﬀerence between bj and bj−1 is
i1 + ··· + w2

a2 + w2

(cid:32)

(cid:33)

ij−1

ij

bj − bj−1 = −a2 + w2

|zij|

i1 + ··· + w2
wij+1
i1 + ··· + w2
wij+1
i1 + ··· + w2

ij

|zij+1| +

wij +

wij
i1 + ··· + w2
wij

ij

ij

a2 + w2

|zij+1| +

)(cid:0)|zij|/wij − |zij+1|/wij+1

(cid:1) ,

|zij|

= −a2 + w2

= (a2 + w2

which is always non-negative and thus bj−1 ≤ bj. Similarly, we have

bd−k − bd−k−1 = (a2 + w2

i1 + ··· + w2
and so the numbers bj satisfy b0 ≤ b1 ≤ . . . ≤ bd−k.

id−k

)(|zid−k|/wid−k) ≥ 0,

Now, let si := sign(zi) ∈ {−1, 1} (if zi = 0 take si = 1). Assume ﬁrst that l ≤ d−k.

Consider the l-dimensional face F of D(I, w) deﬁned by

F := cone{e0/a + sij eij /wij : j = 1, . . . , l}.

We will show that z ∈ int(F ) + F ◦, from which the result follows. For any j such
that l ≤ j ≤ d − k, consider the vector

vj := −ae0 +

Deﬁne the number λ ∈ R by(cid:32)

a2 +

j(cid:88)

r=1

sirwireir − d−k(cid:88)
(cid:33)

λ := −az0 +

r=j+1

l(cid:88)

r=1

w2
ir

sirwireir ∈ F ◦.
l(cid:88)

wir|zir|.

r=1

Our choice of l implies that, if l < d − k then λ ≥ |zil+1|/wil+1, and if l ≥ 1 then
λ < |zil|/wil. For 1 ≤ j ≤ l let

αj := wij|zij| − w2

ij

λ > 0.

z =

αj(e0/a + sij eij /wij ) +

j=1

j=l

βjvj ∈ int(F ) + F ◦,

as claimed.

Assume now that l = d − k + 1. In this case, one can write

d−k(cid:88)

d−k(cid:88)

12

MATEO D´IAZ, MAURICIO JUNCA, FELIPE RINC ´ON, AND MAURICIO VELASCO

If l < d − k, for l ≤ j ≤ d − k let βj ≥ 0 be deﬁned as

λ − |zil+1|/wil+1

|zij|/wij − |zij+1|/wij+1
|zid−k|/wid−k + λ

if j = l,
if l < j < d − k,
if j = d − k.

If l = d − k, take βd−k := λ. With this notation in place, it is routine to check that

2βj :=

l(cid:88)

z = αe0 +

αj(e0/a + sij eij /wij ),

with αj := wij|zij| ≥ 0 and α := z0 − bd−k/a > 0, which shows that z ∈ int(D(I, w))
(cid:3)
as claimed.

j=1

This last theorem allows us to better understand the geometry of the problem; with
this in mind, we propose Algorithm 1 for estimating such volumes. The algorithm
generates a random support I with a given distribution F and a standard normal
vector g, and returns the dimension V of the face in which πC(g) lies.
Let ∆ be the minimum index set that generates the face F = cone{e(cid:48)
0/a + siei/wi :
i ∈ ∆} on which the projection lands. For reasons that we will review in the next
section, it is useful to record this set along the way in the algorithm.

Algorithm 1 can be used to estimate the expected statistical dimension. To do
this, we need to draw enough points (Vi)i and then calculate its mean 1
i=0 Vi. But
n
how many points are enough to get a good estimation of ¯δ? Proposition 3.4 is an
answer to this question.

(cid:80)n

Proposition 3.4. Let (Vi) be i.i.d. random variables generated by Algorithm 1 with
input w and F. If ¯V = 1

i=0 Vi then

n

P(| ¯V − ¯δ(w)| > t) ≤ ε

when n ≥ log(2/ε)d2

2t2

.

Proof. The variables (Vi)n
hypotheses for Hoeﬀding’s inequality, [Hoe63], are fulﬁlled and

i have a bounded range, between 0 and d. Therefore, the

P(| ¯V − δ(C)| > t) ≤ 2 exp

By taking n as in the theorem the result follows.

.

(cid:3)

(cid:19)

(cid:18)

−2nt2
d2

(cid:80)n

COMPRESSED SENSING OF DATA WITH A KNOWN DISTRIBUTION

13

return V, ∆

(cid:46) The cone is trivial

absolute value in each entry

Algorithm 1 Generate point and return face dimension
1: Input: w,F
2: Output: V, ∆
3: V = 0, ∆ = ∅
4: I ← generate support with distribution F
5: k ← |I|
6: if k = 0 then
7:
8: end if
9: g ← generate a standard normal random vector of dimension d and take the
10: a ← (cid:107)w[I](cid:107)2
11: z0 ← −g[I] · w[I]/a
12: n ← d − k
13: z ← g[I c]
14: w ← w[I c]
15: z[i] ← z[i]
16: z, σ ← sort the elements of z in descent order and let σ be the new order of the
17: b ← (n + 1)-dimensional zero vector
18: b[1] ← −a2z[1]
19: for j ← 1 . . . (n − 1) do

b[j + 1] ← b[j] + (z[j] − z[j + 1])(a2 +(cid:80)j

w[i] ∀i

indices

i=1 w[σ(i)]2)

j ← j + 1

20:
21: end for
22: j ← 1
23: while az0 > b[j] and j < n + 1 do
24:
25: end while
26: if az0 > b[n + 1] then j = j + 1
27: end if
28: V ← j + k − 2
29: if j (cid:54)= 1 and j (cid:54)= n + 2 then

∆ = I c[σ[1 . . . (j − 1)]]

30:
31: end if
32: return V, ∆

4. Monte Carlo gradient descent algorithm

In this section we develop a numerical algorithm for minimizing δ(w). More con-
cretely, we describe a Monte Carlo gradient descent algorithm. For this purpose, we
need a way to estimate the gradient of the expected statistical dimension function.
We base our argument on the following proposition.
Proposition 4.1. Let X0 be a random vector with distribution F and I = supp(X0).
Take C := D(I, w) and ¯δ(w) the expected statistical dimension deﬁned in (2.2).

14

Then

MATEO D´IAZ, MAURICIO JUNCA, FELIPE RINC ´ON, AND MAURICIO VELASCO

(cid:18) ∂(cid:107)πC(g)(cid:107)2

(cid:19)

∂wl

∂¯δ(w)
∂wl

= EIEg

for any l = 1, . . . , d.
Proof. It is possible to commute the diﬀerential operator ∂(·)
with the two expected
values. Indeed, for the ﬁrst expected value we note that EI represents a simple sum
and the operator is linear. For Eg we use a measure theoretic version of the Leibniz
(cid:3)
integral rule.

∂wl

An explicit formula for ∂(cid:107)πC (g)(cid:107)2

would thus lead to a Monte Carlo estimation
of the gradient. Let g ∈ Rd be a vector and I be a support, both ﬁxed. Deﬁne
0/a ± ei/wi : i /∈ I} with e(cid:48)
C(cid:48) := cone{e(cid:48)
i∈I wiei. In proposition 3.1 we
proved that the descent cone decomposes as

0 = − 1

∂wl

a

(cid:80)

where L ⊂ Rd is a vector subspace and C(cid:48) ⊆ L⊥. Deﬁne πC(cid:48) and πL as the projections
onto C(cid:48) and L respectively. Then

C = C(cid:48) + L,

(cid:107)πC(g)(cid:107)2

2 = (cid:107)πC(cid:48)(g) + πL(g)(cid:107)2

2 = (cid:107)πC(cid:48)(g)(cid:107)2

2 + (cid:107)πL(g)(cid:107)2
2.

2, and use them to

Our goal is to give explicit formulas for (cid:107)πC(cid:48)(g)(cid:107)2
ﬁnd the gradient ∇(cid:107)πC(g)(cid:107)2

2 with respect to the variables w.

2 and (cid:107)πL(g)(cid:107)2

First, we study the projection onto C(cid:48). Recall that every polyhedral cone deﬁnes
a partition of the space, where every cell consists of the vectors that are projected
onto a particular face. If g was picked at random with normal standard distribution,
then it will almost surely belong to the interior of one of these cells. Thus, there is
a neighborhood of g that is linearly projected onto the same face. We use this fact
to “locally” describe the projection πC(cid:48)(g) as a linear operator.
Thanks to our algorithm in Section 3 we are able to get the smallest index set
∆ ⊆ I c that generates the face F = cone{bi := e(cid:48)
0/a + siei/wi : i ∈ ∆} on which
the projection πC(cid:48)(g) lies. Without loss of generality we can assume that g ≥ 0, and
consequently, the signs si are positive. For the seek of clarity, we will assume that
I is the set {1, . . . , k} and ∆ = {k + 1, . . . , k + r}. In order to write the projection
onto the subspace generated by the vectors bi, consider the matrix

By taking the product P := A(AT A)−1AT we obtain the desired linear projection.
After some computations we obtain the d × d matrix

A =

··· br

 b1
 .
 PI C 0

CT P∆ 0
0
0

0

 ,

P =

1

(cid:107)wI∪∆(cid:107)2

PI =

(cid:107)w∆(cid:107)2
(cid:107)wI(cid:107)2

COMPRESSED SENSING OF DATA WITH A KNOWN DISTRIBUTION

15

where PI is a k × k matrix, P∆ is an r × r matrix, and C is a k × r matrix, deﬁned
by

 w2

...

1

w1wk

P∆ =

. . . w1wk
. . .
. . .

 ,
 (cid:107)wI∪∆(cid:107)2 − w2

...
w2
k

...

−wk+1wk+r

C =

 −w1wk+1

−wkwk+1
−wk+1wk+r

...

k+1

. . .
. . .
. . .

...

(cid:107)wI∪∆(cid:107)2 − w2

k+r

 ,

. . . −w1wk+r
. . .
. . . −wkwk+r

...

This projection is equal to πC(cid:48)(·) in a neighborhood of g, where (cid:107)πC(cid:48)(g)(cid:107)2
gT Pg. By expanding this last term we get

2 = (cid:107)Pg(cid:107)2

2 =

(cid:107)πC(cid:48) (g)(cid:107)2 =

1

(cid:107)w∆∪I(cid:107)2

wiwj gigj

(cid:107)w∆(cid:107)2
(cid:107)wI(cid:107)2

− 2

wiwj gigj − 2

wiwj gigj +

((cid:107)w∆∪I(cid:107)2 − w2

i )g2
i

(cid:88)

i∈I

(cid:88)

j∈I

(cid:88)

i∈I
j∈∆

(cid:88)

i,j∈∆
i(cid:54)=j

 .

In order to obtain a closed formula for the gradient, we explicitly compute ∂(cid:107)πC(cid:48) (g)(cid:107)2
for the three possible cases.

∂wl

 .

(cid:88)

i∈∆

• Case 1: If l ∈ I,

∂(cid:107)πC(cid:48) (g)(cid:107)2

∂wl

=

2

(cid:107)w∆∪I(cid:107)2

(cid:107)w∆(cid:107)2

(cid:107)wI(cid:107)4

(cid:32)

(cid:107)wI(cid:107)2gl

(cid:32)

wlgl +

(cid:33)

(cid:88)

i∈I

− wl

wigi

(cid:88)

i∈I
i(cid:54)=l

(cid:33)

wiwj gigj

(cid:88)
(cid:88)

j∈I

−gl

i∈∆

(cid:88)

i∈∆

wigi + wl

i − wlgT Pg
g2

 .

• Case 2: If l ∈ ∆,

∂(cid:107)πC(cid:48) (g)(cid:107)2

∂wl

=

2

(cid:107)w∆∪I(cid:107)2

 wl

(cid:107)wI(cid:107)2

(cid:88)

(cid:88)

i∈I

j∈I

wiwj gigj − gl

(cid:88)

i∈I

wigi − gl

(cid:88)

i∈∆
i(cid:54)=l

wigi + wl

(cid:88)

i∈∆
i(cid:54)=l

i − wlgT Pg
g2

 .

• Case 3: Otherwise, ∂(cid:107)πC(cid:48) (g)(cid:107)2

= 0.

Analogously, if we choose a basis for L and repeat the same procedure, we get

(cid:107)πL(g)(cid:107)2 =

1

(cid:107)wI(cid:107)2

((cid:107)wI(cid:107)2 − w2

i − 2

i )g2

wiwjgigj

For this function, the directional derivative is equal to

∂(cid:107)πL(g)(cid:107)2

(cid:107)wI(cid:107)2
if l ∈ I and zero in any other case.

∂wl

=

(wlg2

i − glwigi) − wl(cid:107)πL(g)(cid:107)2

(cid:88)

i,j∈I
i(cid:54)=j

 .


∂wl

(cid:88)
(cid:88)

i∈I

i∈I
i(cid:54)=l

2

16

MATEO D´IAZ, MAURICIO JUNCA, FELIPE RINC ´ON, AND MAURICIO VELASCO

Using these directional derivatives, we are able to explicitly ﬁnd the gradient
∇(cid:107)πC(g)(cid:107)2
2 with respect to w. We combine these formulas and Proposition 4.1
to obtain a good approximation of ∇¯δ via a Monte Carlo method. Algorithm 2
Informally the algorithm does the following, it generates
summarizes these ideas.
i with distribution F and for each support, it generates N points
M supports (Ii)M
(gj)N
j with normal distribution. For each pair (Ii, gj) it computes the gradient of
the projection of gj onto the cone D(Ii, w). Finally, the algorithm averages these
quantities to obtain an approximation of ∇¯δ.

Algorithm 2 Estimate gradient
Input: w, M, N, d,F
Output: D
1: D ← d-dimensional zero vector
2: for i ← 0 . . . M do

I ← generate a support with distribution F
for j ← 0 . . . N do

solute value in each entry

g ← generate a standard normal vector of dimension d and take the ab-
∆ ← ﬁndDelta(g, w, I)
if ∆ = ∅ then

(cid:46) g belongs to C or to its polar

3:
4:
5:

6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16: end for
17: return D

end for

continue to next iteration
end if
Dg ← d-dimensional zero vector
for l ← 1 . . . d do

end for
D = D + Dg/(M N )

Dg[l] ← compute the directional derivative with data (l, g, w, I, ∆)

We developed a method of steepest descent for approximating the weights w that
minimize the expected statistical dimension ¯δ. This procedure uses Algorithm 2
to choose the descent direction and our Monte Carlo estimation of the statistical
dimension given in Section 3 to ﬁnd the step size.
In the following section, we
present some numerical examples on the practical performance of this algorithm.

5. Numerical examples

We consider three numerical examples. In two of them, ﬁrst and third, we obtain
promising results for the way we choose our weights: our recovery algorithm using
the suitably weighted (cid:96)1-norm outperforms the non-weighted approach. To select the
weights we employed two methods, the one described in Theorem 2.7 and the numer-
ical algorithm described in Section 4. In the second example we show a particular
setting in which our approach might not always be better.

COMPRESSED SENSING OF DATA WITH A KNOWN DISTRIBUTION

17

We shall note that we always initialize our numerical algorithm to ﬁnd the weights
with the vector w = (1,··· , 1). Interestingly, if we initialize the algorithm with the
weights of Theorem 2.7, the numeric method fails to ﬁnd a non-negligible step size
to continue.

All the experiments were performed using MATLAB and the CVX package with
Gurobi as solver. We present three kinds of ﬁgures: Recovery Frecuency, Expected
Intrinsic Volumes and Histograms of the statistical dimensions. To draw the Recov-
ery Frequency ﬁgures we executed the following procedure: for each m, number of
measurements, generate 100 independent instances of each problem with the given
distribution and with them estimate the frequency of perfect recovery. We deﬁned
10−5 to be our success tolerance. For the Expected Intrinsic Volumes ﬁgures, we ran
the next algorithm: generate 1000 supports with the given distribution and for each
support generate 100 points with Algorithm 1, count the frequency to estimate ¯νk
for all k. We only present the histogram in the ﬁrst experiment, where we explain
how we made it.

5.1. Independent Bernoulli entries. For the ﬁrst example we generate random
vectors X0 ∈ R128 using the following distribution: we partition the entries of X0
into 8 blocks of the same length, and in every block we take the entries to be i.i.d.
random variables with a Bernoulli distribution, where the distribution parameter is
deﬁned by the index of the block. The parameters are given by

(cid:124)

(cid:123)(cid:122)

(cid:125)(cid:124)

B(1, 2−1) B(1, 2−2)

(cid:123)(cid:122)

(cid:125)

. . .

(cid:124)

(cid:123)(cid:122)

B(1, 2−8)

(cid:125)

.

Since the probability of being non-zero decreases exponentially in every block, then
vectors with this distribution are sparse with high probability. For this particular
example we present an histogram of the statistical dimensions of cones generated
by the points with this distribuition.
In each histogram, we draw 1000 random
supports I and we estimate δ(I, w) with a Monte Carlo using 100 points generated
with Algorithm 1. For this case, these statistical dimensions are very concentrated,
as Figure 3 shows. Thus, the hypothesis for Theorem 2.4 are satisﬁed for as small
parameter η. Therefore, problem (Pw), with our weights, is guaranteed to have higher
success probability than (P).

5.2. A non-sharp case. For this experiment our choice of weights is not always the
best. Here we consider an artiﬁcial distribution with four possible supports, each one
with probability 1/4, as in Figure 4. One particular characteristic of this distribution
is that the intrinsic volumes for the descent cones corresponding to diﬀerent supports
are concentrated around diﬀerent locations. Since all the supports have equal prob-
ability, the expected intrinsic volumes are not concentrated, as we show in Figure
5. Intuitively, what Theorem 2.4 is showing in this case is that the transition is not
sharp, and therefore by minimizing ¯δ we are not necessarily reducing the probability

18

MATEO D´IAZ, MAURICIO JUNCA, FELIPE RINC ´ON, AND MAURICIO VELASCO

Figure 2. The ﬁgure on the left shows the estimated recovery fre-
quency; the ﬁgure on the right displays the expected intrinsic volumes
estimated with our Monte Carlo. The blue lines are the results with
our weights from Theorem 2.7, the red lines are the results with the
classical approach, and the green lines are the results with the weights
obtained with the algorithm of Section 4.

Figure 3. Histograms of the statistical dimensions of random cones
generated with independent Bernoulli entries. On the left the his-
togram with cones with weights one. On the right the histogram with
cones with the weights found with (2.4).

Figure 4. Possible supports: Each row represents a support; a blue
point is a 1 and white is a 0.

of failure, i.e., the tail of the intrinsic volumes. We ran the descent algorithm pro-
posed in Section 4 starting at weights one. After two iterations, we obtained very
similar weights to the ones found using the bound in Theorem 2.7.

COMPRESSED SENSING OF DATA WITH A KNOWN DISTRIBUTION

19

Figure 5. Expected instrinsic volumes of the non-sharpe experiment.
Same conventions as in Figure 2.

5.3. MRI. We took real brain MRI from 5 patients.
The resonances were composed of multiple 2D slides of
the brain. In order to promote a sensible distribution in
this setting, we restricted only to the slides at eye-level
height. Subsequently, we centered and cut the images,
increasing the ratio between the non-zero entries and
the size of the image as much as possible. After this
process we ended up with 47 images of size 215 × 184
pixels.

Figure 6. Heat map of
one of the weights found.

We performed a row-by-row reconstruction and tested
the weights described in Theorem 2.7, using a leave-one-
out cross-validation to measure the frequency of perfect
recovery for several number of measurements m.
In
other words, we selected the ith image and used the rows of the other 46 images
to obtain the empirical distribution ˆFi, which we then used to compute the weights
ˆwi and measure the frequency of perfect recovery of the ith image. We repeated the
procedure for all the images and took the average of the frequencies. Figure 7 shows
the results.

6. Conclusions and open questions

In this work, we showed that it is possible to take advantage of prior statistical
information from a signal, i.e. its support distribution, to improve the standard com-
pressed sensing method. In particular, we developed a method to shift the inﬂection
point of the statistical dimension by minimizing an appropriately weighted (cid:96)1-norm.
To do so, we presented two ways to ﬁnd good weights. Our methods pick the weights
aiming to minimize the expected statistical dimension, ¯δ. The ﬁrst method uses an
explicit formula, (2.4), based on an upper bound and the other uses a numerical
algorithm based on a Monte Carlo gradient descent procedure.

20

MATEO D´IAZ, MAURICIO JUNCA, FELIPE RINC ´ON, AND MAURICIO VELASCO

Figure 7. Frequency of perfect recovery of the MRI experiment.
Same conventions as in Figure 2.

Moreover we proved through experiments that the proposed methods are eﬀective
in many contexts in the sense that they increase the success probability for any m.
However, in the case where the expected intrinsic volumes were not concentrated
around their mean, our methods do not beat the classical approach. It appears that
under these circumstances minimizing the statistical dimensions may spread the ex-
pected intrinsic volumes, increasing their variance and their tail, and thus the failure
probability. It is therefore natural to ask: What to minimize when the statistical di-
mension does not work? and how to ﬁnd optimal weights in those cases? We believe
that one way to solve this problem is by ﬁxing m, the number of measurements, and
choosing weights that minimize the probability of failure.

Acknowledgments

The ﬁrst, second and fourth authors were supported by Universidad de los Andes
under the Grant “Fondo de Apoyo a Profesores Asistentes”(FAPA). We would like
to thank Professor Mario Andres Valderrama for providing us with the brain MRI
data. We would also like to thank Dennis Amelunxen, Martin Lotz and Javier Pe˜na
for useful conversations during the completion of this work.

References

[ALMT14] D. Amelunxen, M. Lotz, M. B. McCoy, and J. A. Tropp. Living on the edge: phase

transitions in convex programs with random data. Information and Inference, 2014.

[CRPW12] V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky. The convex geometry
of linear inverse problems. Foundations of Computational Mathematics, 12(6):805–849,
2012.
E. J. Cand`es, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal
reconstruction from highly incomplete frequency information. IEEE Trans. Inf. Theor.,
52(2):489–509, February 2006.

[CRT06]

COMPRESSED SENSING OF DATA WITH A KNOWN DISTRIBUTION

21

[CT05]

E. J. Cand`es and T. Tao. Decoding by linear programming. IEEE Trans. Inf. Theor.,
51(12):4203–4215, December 2005.

[CWB08] E. J. Cand`es, M. B. Wakin, and S. P. Boyd. Enhancing sparsity by reweighted (cid:96)1

minimization. Journal of Fourier analysis and applications, 14(5-6):877–905, 2008.

[FMSY12] M. P. Friedlander, H. Mansour, R. Saab, and ¨O. Yilmaz. Recovering compressively sam-
pled signals using partial support information. Information Theory, IEEE Transactions
on, 58(2):1122–1134, 2012.

[Hoe63] Wassily Hoeﬀding. Probability inequalities for sums of bounded random variables. Jour-

nal of the American statistical association, 58(301):13–30, 1963.

[KXH+09] M. A. Khajehnejad, W. Xu, B. Hassibi, et al. Weighted (cid:96)1 minimization for sparse
recovery with prior information. In Information Theory, 2009. ISIT 2009. IEEE Inter-
national Symposium on, pages 483–487. IEEE, 2009.

[MP15]

[MDR14]

[Roc15]
[RW15]

[KXH+11] M. A. Khajehnejad, W. Xu, B. Hassibi, et al. Analyzing weighted minimization for
sparse recovery with nonuniform sparse models. Signal Processing, IEEE Transactions
on, 59(5):1985–2001, 2011.
J. F. C. Mota, N. Deligiannis, and M. R. D. Rodrigues. Compressed sensing
with prior information: Optimal strategies, geometry, and bounds. arXiv preprint
arXiv:1408.5250, 2014.
S. Misra and P. A. Parrilo. Weighted-minimization for generalized non-uniform sparse
model. Information Theory, IEEE Transactions on, 61(8):4424–4439, 2015.
R. T. Rockafellar. Convex analysis. Princeton university press, 2015.
H. Rauhut and R. Ward. Interpolation via weighted (cid:96)1 minimization. Applied and Com-
putational Harmonic Analysis, 2015.
G. Schiebinger, E. Robeva, and B. Recht. Superresolution without separation. arXiv
preprint arXiv:1506.03144, 2015.
R. Schneider and W. Weil. Stochastic and Integral Geometry.
N. Vaswani and W. Lu. Modiﬁed-cs: Modifying compressive sensing for problems with
partially known support. Signal Processing, IEEE Transactions on, 58(9):4595–4607,
2010.
W. Xu. Compressive sensing for sparse approximations: constructions, algorithms, and
analysis. PhD thesis, California Institute of Technology, 2010.

[SRR15]

[SW]
[VL10]

[Xu10]

Departamento de Matem´aticas, Universidad de los Andes, Bogot´a, Colombia.

E-mail address: m.diaz565@uniandes.edu.co

Departamento de Matem´aticas, Universidad de los Andes, Bogot´a, Colombia.

E-mail address: mj.junca20@uniandes.edu.co

Department of Mathematics, University of Oslo, Oslo, Norway.

E-mail address: feliperi@math.uio.no

Departamento de Matem´aticas, Universidad de los Andes, Bogot´a, Colombia.

E-mail address: mvelasco@uniandes.edu.co

