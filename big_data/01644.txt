6
1
0
2

 
r
a

M
4

 

 
 
]

.

A
N
h
t
a
m

[
 
 

1
v
4
4
6
1
0

.

3
0
6
1
:
v
i
X
r
a

CONVERGENCE OF LCA FLOWS TO (C)LASSO SOLUTIONS

PING TAK PETER TANG∗

Abstract. This paper establishes several convergence results about ﬂows of the dynamical
system LCA (Locally Competitive Algorithm) to the mixed ℓ2-ℓ1 minimization problem LASSO and
the constrained version, called CLASSO here, where the parameters are required to be non-negative.
(C)LASSO problems are closely related to various important applications including eﬃcient coding,
image recognition and image reconstruction. That the solution of (C)LASSO can be determined by
LCA allows the former to be solved in novel ways such as through a physical realization of analog
circuits or on non-von Neumann computers. As discussed in the paper, previous works that show
convergence of LCA to LASSO are incomplete, and do not consider CLASSO. The main contributions
of this paper are a particular generalization of LaSalle’s invariance principle and its application to
rigorously establish LCA’s convergence to (C)LASSO.

Key words. Sparse coding, LASSO, locally competitive algorithm, dynamical systems, LaSalle

invariance principle.

AMS subject classiﬁcations. 65Kxx, 65Pxx

1. Introduction. The LASSO problem (least absolute shrinkage and selection
operator) is a regression problem with regularization. It was originally formulated
by Tibshirani in [14]. More recent works show that solving LASSO is an important
tool in various problems related to image processing, sparse coding and compressive
sensing [7], [6], [15]. LASSO can be solved by traditional computational methods
drawn from the optimization algorithms [4], [9], or [3]. Rozell et. al. [12] formu-
lated a dynamical system called LCA (locally competitive algorithm) closely related
to various optimization problems such as LASSO. The motivation is that instead of
using traditional numerical methods for diﬀerential equations, LCA can be solved
alternatively by running an analog circuit that is suitably conﬁgured to mimic some
neural networks [13] or by running a digital circuit that mimics some spiking neural
networks [8]. This approach is promising both conceptually, as it allows the use of
non-von Neumann computing devices, and practically, as these novel devices can be
extremely power eﬃcient. Consequently, ﬁrm theoretical understanding on conver-
gence behavior of LCA to LASSO solutions is invaluable.

The original work [12] that formulated LCA only discussed convergence brieﬂy.
Furthermore, those discussions are applicable only on LCA with activation functions
that are invertible. For LCA conﬁgured for LASSO, the corresponding activation
function is non-invertible, unbounded, but not necessarily radially unbounded (see
later discussions). Thus convergence of LCA to LASSO was not established in this
original work. Balavoine, Romberg and Rozell [1] concurred with this assessment.
After lucidly discussing why other related works (for example, References 15 through
26 cited in [1]) do not provide the needed convergence theory, the authors established
various results, including LCA’s convergence to a LASSO solution under certain mild
assumptions. In a later work [2], the same authors used the more advanced tool of
 Lojasiewicz inequality to not only re-establish but also strengthen their earlier conver-
gence results. Unfortunately, the proofs in both works have major gaps, detailed later
in Appendix D. Thus the need for convergence guarantee remains. Moreover, in the
situation of solving LCA with the use of a spiking neural network, the LASSO parame-
ters are naturally represented in terms of spiking rates, which are non-negative. Hence

∗Intel Corporation, 2200 Mission College Blvd, Santa Clara, CA 95054

1

the corresponding LASSO problem that LCA aims to solve is in fact a constrained
version of LASSO, called CLASSO here.
In the case of CLASSO, the activation
function in LCA diﬀers slightly from that of LASSO. The diﬀerence is material as
this modiﬁed activation function is no longer radially unbounded, a property that
contributes signiﬁcantly to global convergence behavior of dynamical systems.

In this paper, convergence of LCA to (C)LASSO is established through a suitable
generalization of the well-known LaSalle invariance principle [10]. In particular, the
(C)LASSO objective function value converges to its optimum along any arbitrary
LCA ﬂow (trajectory). Moreover, when the (C)LASSO optimal solution (coeﬃcients)
is unique, the “output” of the LCA along any arbitrary ﬂow also converge to that
optimal solution.

The rest of the paper is organized as follows. Section 2 reviews some standard
deﬁnitions and theories in dynamical systems as well as convex optimizations. It also
states a new generalization of the LaSalle invariance principle. Section 3 states the
(C)LASSO problems and the related LCA. In particular, theorems that relate the two
problems are given. With the set up of these two sections, Section 4 then describes
all the convergence results of this paper. Section 5 discusses possible generalizations
of the current results and an important question that is yet unsettled. Proofs of all
the technical results stated in Sections 2 through 4 are given in the Appendix.

2. Background. This section focuses on properties related to general dynamical
systems as well as convex optimizations that will be used in the sequel. While most
of these properties are well known and stated here for the sake of making this paper
self contained, Theorem 2 is new.

2.1. Dynamical Systems. Consider the system of diﬀerential equations de-

scribing a function u : RN → R:

d
dt

u(t) = F(u(t)),

or compactly

˙u = F(u),

(DS)

where F : RN → RN is locally Lipschitz. Standard theory of ordinary diﬀerential
equations shows that the system has a unique solution U(t, u(0)), t ≥ 0 where u(0) =
U(0, u(0)) is a given point in RN . Throughout this paper, U(t, u(0)), t ≥ 0, is called
the ﬂow with initial position u(0).

Definition 1.
1. A point u ∈ RN is called a ﬁxed point iﬀ F (u) = 0.
2. A set X ⊆ RN is called positive invariant if for any u(0) ∈ X , U(t, u(0)) ∈ X

for all t ≥ 0.

3. Given a set M ⊆ RN , a ﬂow U(t, u(0)) is said to converge to M, U(t, u(0)) →

M, if

lim
t→∞

dist(U(t, u(0)), M) = 0,

where dist(x, M) = inf y∈M kx − yk2.

4. A set D is called a domain of bounded ﬂows if for each u(0) ∈ D, there is a
K > 0 (possibly dependent on u(0)) such that kU(t, u(0))k2 ≤ K for all t ≥ 0.

The following theorem is well know [10].

Theorem 1. (LaSalle Invariance Principle) Consider a dynamical system (DS).
Let D ⊆ RN be compact and positive invariant, and V : RN → R be a scalar function
with continuous ﬁrst partial derivatives. Suppose

˙V (u) def= grad V (u) · F(u) = X ∂V (u)

∂un

2

Fn(u) ≤ 0

for all u ∈ RN . Then for all u(0) ∈ RN , U(t, u(0)) → M, where the set M is the
largest positive invariant set contained in S = {u |
The main result of this paper relies on an extension to Theorem 1. Theorem 2 relaxes
on the compactness and smoothness requirements on D and V , respectively. The
proof is given in Appendix A.

˙V (u) = 0}.

Theorem 2. Let D ⊆ RN be a domain of bounded ﬂows that is closed and
positive invariant. Suppose there are scalar functions V, W : RN → R such that V
is continuous and W is upper semicontinuous and non-positive: W (u) ≤ 0 for all
u ∈ RN . Moreover, for every ﬂow u(t) = U(t, u(0)),

d
dt

V (u(t)) = W (u(t))

a.e. in [0, ∞),

that is for all t ∈ [0, ∞) except possibly for a set of measure 0. Then for any u(0) ∈ D,
U(t, u(0)) → M, the largest positive invariant set inside S = {u | W (u) = 0}.
(cid:4)

2.2. Convex Optimizations. Consider a convex function E : RN → R and the

optimization problem

arg min

E(a)

a∈RN

subject to
hi(a) ≤ 0,

i = 1, 2, . . . , L, L ≥ 0,

(CO)

where each of the functions hi is aﬃne. Note that problem CO is not the most general
convex optimization problem as the only constraints are that of inequality constraints,
which are also deﬁned by aﬃne functions instead of general convex functions. L is
allowed to be zero, in which case, the problem is unconstrained.

Standard theory in convex optimizations shows that for Problem CO, the KKT
conditions are necessary and suﬃcient in characterizing optimal solutions (see [3] for
details), as stated below.

Theorem 3. A point a∗ ∈ RN is an optimal solution for CO iﬀ there is a

µ∗ ∈ RL such that all of the following conditions hold.

1. (Stationarity)

0 ∈ ∂E(a∗) +

L

Xi=1

µ∗
i grad hi(a∗)

where ∂E is the generalized gradient of E (see [5]).

2. (Complementarity)

i hi(a∗) = 0,
µ∗

i = 1, 2, . . . , L.

3. (Feasibility)

hi(a∗) ≤ 0, µ∗

i ≥ 0,

i = 1, 2, . . . , L.

If L = 0 (the case of unconstrained optimization), these conditions reduced to simply
0 ∈ ∂E(a∗).

3

3. LASSO and LCA. The original LASSO problem as formulated in [14] is an
unconstrained convex optimization: Given s ∈ RM , a matrix Φ = [φ1, φ2, . . . , φN ] ∈
RM ×N (φj ∈ RM is the j-th column) and a real number λ > 0, solve

arg min

a∈RN

1
2

ks − Φak2

2 + λkak1.

This paper also considers the constrained version (called CLASSO here) where the
vector of parameters a is restricted to having only non-negative components, denoted
as a ≥ 0. Both versions correspond to Problem CO where

E(a) =

1
2

ks − Φak2

2 + λkak1,

and hi(a) = −ai, i = 1, 2, . . . , L where either L = 0 (LASSO) or L = N (CLASSO).
The dynamical system LCA (locally competitive algorithm) is originally formu-
lated to solve LASSO [12] although [1] is a ﬁrst attempt to establish rigorously that
LCA solves LASSO. LCA can also be conﬁgured to address CLASSO. Speciﬁcally,
assuming the (C)LASSO problem is given as before but with the Φ matrix to have
columns scaled to have unit norm. LCA is a dynamical system of the form of DS,
˙u = F(u), F : RN → RN where

F(u) = b − u − (ΦT Φ − I)T(u),

b = ΦT s, and T : RN → RN is a nonlinear function that applies an identical scalar
function T : R → R to each component un of u. The function T is either T±λ for
LASSO and Tλ for CLASSO:

Tλ(x) = (cid:26) x − λ x > λ

x ≤ λ

0

and T±λ(x) = Tλ(x) + Tλ(−x).

That LCA solves (C)LASSO means that an LCA ﬂow u(t) = U(t, u(0)) for some
suitably (or arbitrarily) chosen initial position u(0) leads to T(u(t)) converging to an
optimal point for (C)LASSO. In practice, this convergence usually happens in tandem
with u(t) converging to a ﬁxed point of LCA. Consider a (C)LASSO problem and the
corresponding LCA system.

Definition 2.
1. C denotes the set of optimal solutions to (C)LASSO.
2. F denotes the set of ﬁxed points of LCA.
3. ˆF , deﬁned as ˆF def= T−1(C), is called the ﬁxed region. That is a u ∈ RN will

yield an optimal point via the T mapping iﬀ u ∈ ˆF .

4. Given a ∈ RN , denote the index set of nonzero and zero components by

and

A(a) = {n | an 6= 0}

I(a) = {n | an = 0}.

Whenever a ﬂow of a dynamical system converges, it necessarily converges to a
ﬁxed point. The next two theorems give the crucial relationship between F , ˆF and
C. Their proofs are given in Appendix B.

Theorem 4. Let C, F and ˆF be as deﬁned previously. Then the following hold:

4

1. T(F ) ⊆ C.
2. For each a∗ ∈ C, there is a u∗ ∈ F such that T(u∗) = a∗. In particular,

F ⊆ ˆF .

The ﬁxed region ˆF plays an important role in subsequent developments of this
paper. The next theorem sets a foundation by applying the KKT characterization of
Theorem 3 to ˆF .

Theorem 5. Let u ∈ RN . Then u ∈ ˆF if and only if both equations below hold

0 = bn − φT

0 = T (cid:0)bn − φT

n Φa(cid:1) ,

n Φa − λsign(an), n ∈ A(a),
n ∈ I(a),

where a = T(u).

4. Convergence of LCA to LASSO Solutions. This section establishes var-
ious convergence properties of LCA. The main tool is the extension of LaSalle invari-
ance principle stated in Theorem 2. In order to apply that theorem, the two scalar
functions V and W are deﬁned below in Deﬁnition 3 and their relevant properties are
stated in Lemma 1. With V and W appropriately deﬁned, the result of Theorem 2
says that all LCA ﬂows converge to the set M, the largest positive invariant set inside
the set S of all points at which W is zero. To further reﬁne this result, Theorem 6
states that this largest invariant set turns out to be the ﬁxed region of (C)LASSO.
Two convergence results then follow easily from these foundations. Proofs for all the
results of this section are given in Appendix C.

Definition 3. Given a (C)LASSO problem and the corresponding LCA system,

deﬁne scalar functions V, W : RN → R as follows.

V (u) def= E(a) =

1
2

ks − Φak2

2 + λ kak1,

where a = T(u), and

W (u) def= (cid:26) Pn∈A(a)

0

∂V
∂un

· Fn(u) A(a) 6= ∅
A(a) = ∅

Lemma 1. Consider V and W in Deﬁnition 3. The following hold.
1. V is continuous.
2. W is non-positive, that is, W (u) ≤ 0 for all u ∈ RN , and upper semicontin-

uous.

3. Given any LCA ﬂow u(t) = U(t, u(0)), u(0) ∈ RN ,

d
dt

V (u(t)) = W (u(t))

a.e. in t ∈ [0, ∞).

4. Given any u(0) ∈ RN , the set

D = {u ∈ RN | V (u) ≤ V (u(0))}

is closed, positive invariant, and a domain of bounded ﬂows.

Given Deﬁnition 3 and Lemma 1, the extension to LaSalle’s invariance principle
as stated in Theorem 2 is applicable. Thus U(t, u(0)) → M for any u(0) ∈ RN where
M is the largest positive invariant set inside S = {u | W (u) = 0}. The next theorem
shows that this M is in fact the ﬁxed region ˆF = T−1(C).

Theorem 6. Let M be the largest positive invariant set inside S = {u | W (u) =
0}. Then M = ˆF . In particular, given an arbitrary ﬂow u(t) = U(t, u(0)), u(t) → ˆF
and T(u(t)) → C.

5

The next theorem shows that LCA can be used to determine the optimal objective

function value of (C)LASSO.

Theorem 7. Let E ∗ = E(a∗), a∗ ∈ C, be the optimal objective function value of
(C)LASSO. Denote by u(t) = U(t, u(0)) the LCA ﬂow at an arbitrary starting point.
Then limt→∞ V (u(t)) = E ∗.

Because (C)LASSO is convex, the set of optimal solutions is a convex set.

In
particular an isolated optimal solution exists iﬀ the optimal solution is unique. In
this case a stronger convergence result can be established and LCA can be used to
determine the optimal (C)LASSO solution.

Theorem 8. Suppose (C)LASSO has a unique optimal solution a∗ ∈ C, then
given any u(0) ∈ RN u(t) = U(t, u(0)) converges to a ﬁxed point u∗ ∈ F : u(t) → u∗.
Furthermore T(u∗) = a∗.

5. Conclusion. In summary, one can determine the optimal objective function
value of (C)LASSO by approaching the corresponding LCA’s ﬁxed region arbitrarily
closely – which any ﬂow U(t, u(0)) always does. Furthermore, when the optimal

(C)LASSO parameters are unique, T(cid:0)U(t, u(0))(cid:1) will converge to that as well. It is

known (cf. [1] and [11]) that a more general optimization problem of the form

arg min

E(a), E(a) =

a

1
2

ks − Φak2

2 +

N

Xn=1

C(an)

is related to the LCA

˙u = b − u − (ΦT Φ − I) [T (u1), T (u2), . . . , T (uN )]T

where b = ΦT s and T : R → R is a function strictly increasing on {u | T (u) 6= 0}
such that for a 6= 0 C ′(a) = u − a, a = T (u). The results developed here in this paper
apply trivially when C(·) is convex. This includes for example a LASSO-like problem
called elastic net [16]:

arg min

E(a), E(a) =

a

1
2

ks − Φak2

2 + λ1kak1 + λ2kak2
2.

The corresponding “activation” function T : R → R are

Tce(u) = (cid:26) 0

u ≤ λ1
u−λ1
2λ2+1 u > λ1,

and Te(u) = Tce(u) + Tce(−u). LCA with T = Te corresponds to the elastic net
problem while T = Tce corresponds to adding non-negativity constraints to the elastic
net problem.

It is worth pointing our that when C(·) is not convex, the usefulness of a con-
verging LCA may be greatly diminished. In this case, while an LCA ﬁxed point u∗
still yield a critical point a∗ = T(u∗) of E(a) in the sense that 0 ∈ ∂E(a∗), the last
property alone is insuﬃcient for a∗, or even E(a∗) to be optimal.

Focusing back to (C)LASSO, note that while E(u(t)) → E ∗ monotonically, where
u(t) is a LCA ﬂow, Theorem 8 only guarantees u(t) → u∗ and T(u(t)) → a∗ should
the optimal a∗ is unique. While [2] states that both u(t) and T(u(t)) converge to some
u∗ and a∗, the proof’s gap outlined in Appendix D renders the convergence claims,
however welcome, unsupported. Settling the convergence question, aﬃrmatively or
otherwise, is a natural next step to the results given in this paper.

Appendix A. Proof Of Theorem 2.

6

Let u(0) ∈ D be an arbitrary starting point and L be the set of limit points of
the ﬂow U(t, u(0)). That is L is the limit points of the set {U(t, u(0)) | t ≥ 0}. The
ﬁrst lemma establishes some basic topological properties of L.

Lemma 2. The limit set L has the following properties.
1. L a compact subset of D.
2. L is positive invariant.
3. U(t, u(0)) → L as t → ∞.
Proof 1. Proof of (1): Since D is a domain of bounded ﬂows, there is a K such
that kU(t, u(0))k2 ≤ K for all t ≥ 0. By Bolzano-Weiestrass theorem, L is nonempty
where each of its elements must be bounded by K. Because D is closed, we must have
L ⊆ D. To show that L is compact, it suﬃces to show that it is closed. Consider
any sequence {uk}, uk ∈ L for all k, that is convergent to a certain limit ˆu ∈ RN .
If ˆu must necessarily belong to L as well, then L is closed. Since u1 ∈ L, there is
a time t1 ≥ 1 such that kU(t1, u(0)) − u1k2 < 1. Having obtained t1, there must be
a t2 > max{t1, 2} such that kU(t2, u(0)) − u2k2 < 1/2. Continuing this process, one
obtains a sequences 0 ≤ t1 < t2 < · · · , tk → ∞, that satisﬁes kU(tk, u(0)) − ukk2 <
1/k. Now for any ǫ > 0, pick K1 large enough so that kuk − ˆuk2 < ǫ/2 for all
k ≥ K1. Pick integer K2 so that K2 ≥ 2/ǫ, that is, 1/k < ǫ/2 for all k ≥ K2. Let
K = max{K1, K2}. Then for all k ≥ K,

kU(tk, u(0)) − ˆuk2 ≤ kU(tk, u(0)) − ukk2 + kuk − ˆuk2 < ǫ.

Thus U(tk, u(0)) → ˆu and ˆu must also belong to L, showing that L is closed.

Proof of (2): Let v(0) ∈ L be an arbitrary starting point, and let τ > 0 be an arbitrary
time. Let ˆv = U(τ, v(0)). It suﬃces to show ˆv ∈ L. Since v(0) ∈ L, there is a time
sequence {tk} such that uk → v(0), where uk = U(tk, u(0)). Consider the fact

U(tk + τ , u(0)) = U(τ, U(tk, u(0))) = U(τ, uk).

Now, deﬁne ˆtk = tk + τ . Since uk → v(0) and U(τ, uk) is continuous in its second
argument,

U(ˆtk, u(0)) → U(τ, v(0)) = ˆv.

This shows ˆv ∈ L, establishing positive invariance of L.

Proof of (3): Assume the contrary. This means that there is a ǫ > 0 and a time
sequence {tk} such that for all k = 1, 2, . . ., dist(U(tk, u(0)), L) ≥ ǫ. But {U(tk, u(0))}
is bounded and must have a convergent subsequence U(ˆtk, u(0)) → ˆu. Therefore, on the
one hand, ˆu ∈ L, that is, dist(ˆu, L) = 0, but on the other hand dist(ˆu, L) ≥ ǫ because
dist(U(ˆtk, u(0)), L) ≥ ǫ for all k. This contradiction shows that indeed U(t, u(0))
converges to L.
(cid:4)

The second lemma concerns properties of the scalar functions V and W .
Lemma 3. The function V (u) takes on a constant value on L and W (u) = 0 for

all u ∈ L.

Proof 2. Observe that for any 0 ≤ t1 ≤ t2,

V (U(t2, u(0)))

= V (U(t1, u(0))) +Z t2

t1

≤ V (U(t1, u(0))).

7

W (U(s, u(0))) ds

Thus V (U(t, u(0))) is non-increasing in t. Now assume that V (u) is not constant on
L: Let u(1) and u(2) ∈ L be such that

V (u(1)) = V (1) < V (1) + δ = V (2) = V (u(2))

for some δ > 0. u(1) ∈ L implies there is a time t1 large enough that the proximity of
U(t1, u(0)) to u(1) implies

Based on t1, pick t2 > t1 such that the proximity of U(t2, u(0)) to u(2) implies

(cid:12)(cid:12)(cid:12)
V (U(t1, u(0))) − V (1)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)
V (U(t2, u(0))) − V (2)(cid:12)(cid:12)(cid:12)

< δ/3.

< δ/3.

Thus we have t1 < t2 while

V (U(t1, u(0))) < V (U(t2, u(0))) − δ/3,

contradiction the fact that V (U(t1, u(0))) is non-increasing in time. This establishes
the ﬁrst assertion.

To prove the second assertion, assume the contrary that W (u(1)) 6= 0 for some u(1) ∈
L. Therefore W (u(1)) = −ǫ for some ǫ > 0 as W is assumed to be non-positive on
RN . Because W is also assumed upper semicontinuous, there exist positive ǫ and δu0
such that

W (u) ≤ −ǫ/2

for all ku − u(1)k2 < δu.

By continuity of ﬂows, there is a δt > 0 such that

kU(t, u(1)) − u(1)k2 < δu

for all 0 ≤ t ≤ δt.

Hence W (U(t, u(1))) ≤ −ǫ/2 for all 0 ≤ t ≤ δt. Let u(2) = U(δt, u(1)). Note that
u(2) ∈ L because the L is positive invariant. Thus as established previously that V
takes on constant value on L, V (u(2)) = V (u(1)). On the other hand, however,

V (u(2)) = V (u(1)) +Z δt

0

≤ V (u(1)) −

δt < V (u(1)).

ǫ
2

W (U(s, u(1))) ds

This is a contradiction and thus in fact W (u) = 0 for all u ∈ L.

(cid:4)

Appendix B. Proof Of Theorems in Section 3.
Proof 3. (For Theorem 4)
To establish the ﬁrst part of Theorem 4, T(F ) ⊆ C, it suﬃces to show that
a∗ def= T(u∗) ∈ C for each u∗ ∈ F . Given a u∗ ∈ F and deﬁning a∗ = T(u∗),
let the N components of these vectors be u∗
j , j = 1, 2, . . . , N . Deﬁne the vector
µ∗, g∗ ∈ RN componentwise via

j , a∗

µ∗

n = (cid:26)

0

n ∈ A(a∗)
n) n ∈ I(a∗)
−T±λ(u∗
8

and

g∗

n = (cid:26) −bn + φT

−bn + φT

n Φa∗ + λ sign(a∗
n Φa∗ + max{u∗

n) − µ∗
n
n, −λ} − µ∗

n ∈ A(a∗)
n n ∈ I(a∗)

Note that µ∗ ≥ 0 and that µ∗ = 0 for LASSO. Because

λ sign(a∗

n) = u∗

n − a∗
n,

for n ∈ A(a∗), and

max{u∗

n, −λ} − µ∗

n = u∗

n

for n ∈ I(a∗),

g∗ = −F(u∗) = 0. Observe now that g∗ ∈ ∂E(a∗) − µ∗. To see this, ﬁrst examine
∂E(a∗).

∂E(a) = grad(cid:18) 1

2

ks − Φak2

2(cid:19) + λ∂kak1

= −b + ΦT Φa + λ ∂kak1,

where the n-th component of ∂kak1 is either {sign(an)} if |an| > 0 or [−1, 1] for
an = 0. Clearly then

g∗

n ∈ (cid:26) {−bn + φT

−bn + φT

n Φa∗ + λ sign(a∗
n Φa∗ + [−λ, λ] − µ∗
n

n) − µ∗

n} n ∈ A(a∗)
n ∈ I(a∗)

,

that is, g∗ ∈ ∂E(a∗) − µ∗, implying 0 ∈ ∂E(a∗) − µ∗. Stationarity of KKT (Theo-
rem 3) is satisﬁed for (C)LASSO. In the case of CLASSO, the complementarity and
feasibility conditions are also satisﬁed as −µ∗
n = 0 for all n and µ∗ ≥ 0. Conse-
quently, a∗ ∈ C and the ﬁrst part of Theorem 4 is proved.

n a∗

To prove the second part, let a∗ ∈ C. In the case of CLASSO, because a∗ ∈ C,
there is a µ∗ ∈ RN that satisﬁes the three KKT conditions. In the case of LASSO,
simply deﬁne µ∗ to be the zero vector in RN . By stationarity of KKT,

0 ∈ (cid:26) {−bn + φT

−bn + φT

n Φa∗ + λ sign(a∗
n Φa∗ + [−λ, λ] − µ∗

n)} n ∈ A(a∗)
n n ∈ I(a∗)

(A-1)

Deﬁne u∗ ∈ RN as follows. For n ∈ A(a∗), deﬁne u∗
that in this case λ sign(a∗
is αn ∈ [−1, 1] such that

n) = u∗

n − a∗

n 6= 0. Note
n. For n ∈ I(a∗), Equation A-1 shows that there

n = T −1(a∗

n) as a∗

0 = −bn + φT

n Φa∗ + (αn − µ∗

n).

Deﬁne u∗
and the second part of this theorem is proved.

n and thus T (u∗

n = αn − µ∗

n) = 0. Consequently, T(u∗) = a∗ and F(u∗) = 0
(cid:4)

Proof 4. (For Theorem 5)
By deﬁnition of ˆF , u ∈ ˆF iﬀ a = T(u) ∈ C, iﬀ a satisﬁes KKT, iﬀ a is feasible

and the following two conditions hold:

0 = −bn + φT
0 ∈ −bn + φT

n ∈ A(a),
n Φa + λ sign(an)
n Φa + [−λ, λ] − µn n ∈ I(a),

where µn = 0 for LASSO and µn ≥ 0 for CLASSO. But the second condition is
equivalent to bn − φT

n Φa ∈ [−λ, λ] − µn, which is equivalent to

T (cid:0)bn − φT

n Φa(cid:1) = 0

9

where T = T±λ for LASSO and T = Tλ for CLASSO. This completes the proof. (cid:4)

Appendix C. Proofs for Section 4.
Proof 5. (For Lemma 1.)
Proof of (1): That V is continuous is obvious because k·k2, k·k1, and T(·) are all

continuous.

otherwise. Note that W (u) = PN

Proof of (2): Now deﬁne for n = 1, 2, . . . , N , χn(u) = 1 if |T (un)| > 0 and 0
n(u) · χn(u). Thus W (u) ≤ 0 for all u ∈ RN .
Observe that χn is lower semicontinuous, and thus F 2
n(u)χn(u) is lower semicontin-
uous, and its negation upper semicontinuous. Being a sum of upper semicontinuous
function, W (u) is upper semicontinuous as well.

n=1 −F 2

1 ∪ {βj}∞

|

|T (un(t))| > 0}.

1 . For any t0 ∈ Bc

Proof of (3): Let On = {t

It is open and thus can be
j=1(αj, βj). Let Bn =
n (complement of Bn) there is an open neighborhood
dt T (un(t)) exists. This derivative equals 1 if |T (un(t))| > 0; it

expressed as a countable union of disjoint open intervals On = ∪∞
{αj}∞
(t0 − ǫ, t0 + ǫ) in which d
equals 0 if t0 is in the interior of the set {t | T (un(t)) = 0}. The set O = (cid:0)∪N

is open and consists of the set [0, ∞) except possibly for a set of measure 0. For each
t ∈ O, V (u(t)) is diﬀerentiable and d

n=1Bn(cid:1)c

dt V (u(t)) = W (u(t)).

Proof of (4): Let u(0) ∈ RN be chosen arbitrarily and deﬁne D = {u | V (u) ≤
V (u(0))}. Denote V (u(0)) by η(0). D is closed because it is V −1([0, η(0)]) and V is
continuous. Since ˙V (u(t)) = W (u(t)) a.e. and W (u) ≤ 0 always, V (u(t)) is non-
increasing in t for any ﬂow u(t). Therefore given any v(0) ∈ D and t ≥ 0, we have

V (U(t, v(0))) ≤ V (U(0, v(0))) ≤ η(0),

implying that V (U(t, v(0))) ∈ D and D’s positive invariance. Finally, for any v(0) ∈
D, let u(t) = U(t, v(0)) be the ﬂow with initial position v(0) inside D. Because
V (u(t)) ≤ η(0) for all t ≥ 0 and V (u) ≥ λkT(u)k1, there is a constant K1 such that
kb + (ΦT Φ − I)T(u(t))k2 ≤ K1 for all t ≥ 0. Since ˙u = −u + (b + (ΦT Φ − I)T(u)),
u(t) satisﬁes

u(t) = v(0) + e−tZ t

0

es(cid:0)b + (ΦT Φ − I)T(u(s))(cid:1) ds.

Therefore there is K ≥ 0 such that ku(t)k2 ≤ K for all t ≥ 0, establishing that D is
a domain of bounded ﬂows.

The following Lemma is instrumental to proving Theorem 6.
Lemma 4. Let ˆu ∈ RN and ˆa = T(ˆu) such that

0 = −bn + φT

n Φˆa + λ sign(ˆan),

for all n ∈ A(ˆa).

Deﬁne the function v(t) by

vn(t) = (cid:26) ˆun

for all t ≥ 0

n ∈ A(ˆa)
αn − e−t(αn − ˆun) n ∈ I(ˆa)

def= bn − φT

where αn
and for all n ∈ I(ˆa), then v(t) = U(t, ˆu) for all t ∈ [0, τ ].

n Φˆa. If there exists a τ > 0 such that T (vn(t)) = 0 for all t ∈ [0, τ ]

Proof 6. (For Lemma 4)

10

By construction v(0) = ˆu. Let τ > 0 be as described. Then by deﬁnition of v(t),

we have T(v(t)) = ˆa for t ∈ [0, τ ]. For n ∈ I(ˆa) and t ∈ [0, τ ],

0 = (bn − φT

n Φˆa) − vn(t)

= bn − vn(t) − φT
= Fn(v(t)).

n ΦT(v(t)) + T (vn(t))

For n ∈ A(ˆa) and all t ≥ 0,

dotvn(t) = 0

= bn − φT
= bn − φT
= Fn(v(t)).

n Φˆa − λ sign(ˆan)
n Φˆa − (ˆun − ˆan)

Hence v(t) = F(v(t)) for t ∈ [0, τ ] and v(0) = ˆu, that is, v(t) = U(t, ˆu) for t ∈ [0, τ ].
(cid:4)

Proof 7. (For Theorem 6) ˆF = M is equivalent to ˆF ⊆ M and M ⊆ ˆF . To

prove ˆF ⊆ M, consider ˆu ∈ ˆF . Denote T(ˆu) by ˆa. By Theorem 5,

0 = (cid:26) −bn + φT
T (cid:0)bn + φT

n Φˆa(cid:1)

n Φˆa + λ sign(ˆan) n ∈ A(ˆa)
n ∈ I(ˆa).

Deﬁne v(t) as in Lemma 4:

vn(t) = (cid:26) ˆun
n Φˆa. Because T (αn) = 0 for all n ∈ I(ˆa) and T −1({0}) is
where αn
connected (either [−λ, λ] or (−∞, λ]), T (vn(t)) = 0 for all t ≥ 0 and n ∈ I(ˆa). By
Lemma 4, v(t) = U(t, ˆu) for all t ≥ 0. Observe that

n ∈ A(ˆa)
αn − e−t(αn − ˆun) n ∈ I(ˆa)

def= bn − φT

(A-1)

W (v(t)) = Xn∈A(T(v(t)))

−F 2

n(v(t)) ≥ Xn∈A(ˆa)

−F 2

n(v(t)) = 0,

which implies W (v(t)) = 0 for all t ≥ 0 as W (u) ≤ 0 for all u ∈ RN . Consequently,
ˆu ∈ M and ˆF ⊆ M is established.

Now consider a ˆu ∈ M and ˆa = T(ˆu). Thus W (ˆu) = 0 and we must have

0 = Fn(ˆu) = bn − ˆun − φT

n Φˆa + ˆan

for all n ∈ A(ˆa). This is equivalent to

0 = −bn + φT

n Φˆa + λ sign(ˆan)

for n ∈ I(ˆa). Deﬁne v(t) as in (A-1). Because T (vn(0)) = T (ˆun) = 0 for all
n ∈ I(ˆa), we have τ > 0 where

τ def= inf{t |

|T (vn(t))| > 0, n ∈ I(ˆa)}.

By Lemma 4, v(t) = u(t) def= U(t, ˆu) for all 0 ≤ t ≤ τ . If τ is ﬁnite, then there is an
n ∈ I(ˆa) such that |vn(τ )| = λ and ˙vn(t)vn(t) > 0. Thus we also have |un(τ )| = λ
and ˙un(t)un(t) > 0. By continuity of un(t) and ˙un(t), there is a δ > 0 such that

|T (un(τ + δ))| > 0

and | ˙un(τ + δ)| > 0.

11

Consequently, W (u(τ + δ)) < 0, contradicting the assumption that ˆu ∈ M which is a
positive invariant set where W (u) = 0 for all u ∈ M. Consequently τ must in fact be
inﬁnite. This means that T (vn(t)) = 0 for all t ≥ 0 and all n ∈ I(ˆa). Consequently,
T (αn) = 0 for all n ∈ I(ˆa), where αn = bn − φT
n Φˆa. Note also that W (ˆu) = 0 implies
for all n ∈ A(ˆa),

0 = Fn(ˆu)

= bn − ˆun − φT
= −bn + φT

n Φˆa + ˆan

n Φˆa + λ, sign(ˆan).

Using Theorem 5, one concludes that ˆu ∈ ˆF . Thus M ⊆ ˆF . Together with the
previously established fact ˆF ⊆ M, ˆF = M and the proof of this theorem is complete.
Now that ˆF = M, the convergence of an arbitrary ﬂow u(t) to ˆF , u(t) → ˆF
follows immediately from Theorem 2. Finally, because T(·) is uniformly continuous,
we have T(u(t)) → T( ˆF ) = C.

Proof 8. (For Theorem 7) This theorem follows easily from Theorem 6. Given
an arbitrary ﬂow u(t) = U(t, u(0)), u(0) ∈ RN , Theorem 6 states that T(u(t)) → C.
Thus for all t ≥ 0, there is a(t) ∈ C such that

ǫt = kT(u(t)) − a(t)k2 → 0 as t → ∞.

Note that

E(a) =

1
2

ks − Φak2

2 + λ kak1

is clearly Lipschitz in a. Thus, there is a K > 0 such that

|V (u(t)) − E ∗| = (cid:12)(cid:12)(cid:12)

E(T(u(t))) − E(a(t))(cid:12)(cid:12)(cid:12)

≤ Kǫt → 0.

This completes the proof.

(cid:4)
Proof 9. (For Theorem 8) Let a∗ be the unique (C)LASSO solution. By The-
orem 6 then, any ﬂow u(t) = U(t, u(0)), u(0) ∈ RN , has the convergence prop-
erty T(u(t)) → C, the latter of which is the singleton {a∗} by assumption. Thus
T(u(t)) → a∗. Consequently, the LCA

˙u(t) = b − u(t) − (ΦT Φ − I) T(u(t))

must necessarily converge to the ﬁxed point

u∗ = b − (ΦT Φ − I) a∗.

This completes the proof.

(cid:4)

Appendix D. Technical Gaps in [1] and [2].
As alluded to before, the paper [1] pointed out the need for theoretical results on
the convergence of LCA and proceeded to provide them. The formulation involves a
thresholding function denoted as T. While the components of T in [1] is more general
than T±λ here, The diﬀerence is irrelevant for the purpose of discussion here. In the
notation of [1], the LCA is

˙u(t) = b − u(t) − (ΦT Φ − I)a(t),

a(t) = T(u(t))

12

below (by −P 1/k2),

˙V (t) cannot converge to 0.

and V (x) is the corresponding “energy” function

V (x) =

1
2

ks − Φxk2 + λ

N

Xm=1

C(xm).

Again, for the purpose of the discussion here, C(x) can be considered to be the
function |x|. Note that this V function is diﬀerent from the one Deﬁnition 3.

The main results are found in Theorem 1 which states two convergence behavior of
a(t) = T(u(t)), u(t) being a LCA ﬂow, when the LASSO critical points are isolated.
The proof (presented in the appendix of the paper) states that V (a(t)) → V ∗ for
some V ∗ because V (a(t)) is non-increasing and bounded below. This is a sound
statement. The paper uses this fact to infer that (a) ˙V (a(t)) → 0, (b) k ˙a(t)k2 → 0,
and (c) a(t) → X where X is the set {a| ˙a(t) = 0}. The arguments in (b) and (c)
depend crucially on (a). The argument supporting (a) was that V (a(t)) being non-
increasing and bounded below. (To quote verbatim: “...V (a(t)) is nonincreasing for
all t ≥ 0. Since V (a(t)) is continuous, bounded below by zero, and nonincreasing,
V (a(t)) converges to a constant value V ∗, and its time derivative ˙V (a(t)) tends to
zero as t → ∞.”) This argument is invalid: Consider a continuous function V (t),
V (0) = 0 such that ˙V (t) = 0 for all [0, ∞) except taking on the value −1 on the
intervals (k, k + 1/k2) for k = 1, 2, . . . . While V (t) is nonincreasing and bounded

As the technical arguments in the sequel of (a) depend crucially on it, the con-

vergence proof in [1] is invalid.

In [2] the authors of [1] use a more advanced mathematical tool to prove a stronger
result. In addition to the original convergence result in [1], this work proves that the
LCA ﬂow u(t) (not just a(t)) converges to a u∗ ∈ RN , regardless the critical points
of V (a) or LCA ﬁxed points are isolated or not. This result is stronger than the one
established in this current paper. One crucial relationship that allows this advanced
mathematical tool to be used successfully is in Section IV:

k ˙a(t)k2 = k ˙aΓ(t)k2 ≥ βk ˙uΓ(t)k2

≥ β m(∂V (aΓ(t))) = β m(∂V (a(t))).

(A-1)

Here Γ is the set of active nodes (which is A(a(t)) in the current paper), ∂V (a) is
the generalized gradient at a speciﬁc point a, which is in general a set of vectors, and
m(∂V (a)) is

m(∂V (a)) def= inf{kgk2 | g ∈ ∂V (a)}.

The α and β are positive constants related to the speciﬁc T being used. As long as
they are positive, their speciﬁc values do not matter as far as the discussion below is
concerned. The technical relationship in Equation A-1 above is crucial as it allows
˙V (a(t)) be bounded in terms of ka(t)k2 m(∂V (a(t))). The Lojasiewicz inequality is
then applied, yielding a useful estimate on the integral of the form R tq

Unfortunately, Equation A-1 is problematic. First, it is unclear what m(∂V (aΓ(t)))
means. V is a function from RN to R. So generalized gradient (or gradient for that
matter) are deﬁned in terms of elements in RN . In general aΓ is a shorter vector.
Notwithstanding the clariﬁcation that is needed here, the result of Equation A-1,
namely

k ˙a(t)k2.

tp

k ˙a(t)k2 ≥ β m(∂V (a(t)))

(A-2)

13

cannot hold in general: Equation 8 of [2] states that

∂V (a(t)) = −b + ΦT Φa(t) + λ∂C(a(t)),

where b = ΦT y and y is a constant vector. For simplicity’s sake, consider the speciﬁc
case when C(a) = |a| (and β corresponds to 1). If u(t) = 0 (for example 0 is chosen
as the initial value for LCA), a(t) = ˙a(t) = 0 for at least a short period of time
(before any of the components of u(t) ventures outside [−λ, λ]). The left hand side
of Equation A-2 is 0. On the other hand,

∂V (0) = −b + λ∂C(0).

For any element in the set of generalized gradient, the n-th component is −bn + γ
where γ ∈ [−λ, λ]. Thus, as long as |bn| = λ + δ > λ for any one bn, the norm of any
element of generalized gradients is at least δ. In other words, m(∂V (0)) ≥ δ > 0. As
Equation A-1 is invalid, the proof of the main result in [2] has a major gap.

Appendix E. The author thanks Mike Davies, Tsung-han Lin and Justin Romberg

for fruitful discussions.

REFERENCES

[1] A. Balavoine, J. Romberg, and C. J. Rozell, Convergence and rate analysis of neural
networks for sparse approximation, IEEE Trans. Neural Netw., 23 (2012), pp. 1377–1389.
[2] A. Balavoine, C. J. Rozell, and J. Romberg, Convergence of a neural network for sparse
approximation using nonsmooth  Lojasiewicz inequality, in Proceedings of the International
Joint Conference on Neural Networks, Dalla, TX, August 2013.

[3] S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge University Press, Cam-

bridge, UK, 2004.

[4] S. Chen, D. Donoho, and M. Saunders, Atomic decomposition by basis pursuit, SIAM Rev.,

43 (2001), pp. 129–159.

[5] Frank H. Clarke, Optimization and Nonsmooth Analysis, Society for Industrial and Applied

Mathematics (SIAM), Philadelphia, PA, 1990.

[6] Y. C. Eldar, P. Kuppinger, and H. B¨olcskei, Block-sparse signals: Uncertainty relations

and eﬃcient recovery, IEEE Trans. Signal Process., 58 (2010), pp. 3042–3054.

[7] H. Fu, M. K. Ng, M. Nikolova, and J. L. Barlow, Eﬃcient minimization methods of mixed
ℓ2-ℓ1 and ℓ1-ℓ1 norms for image restoration, SIAM J. Sci. Comput., 27 (2006), pp. 1881–
1902.

[8] J. K. Kim, P. Knag, T. Chen, and Z. Zhang, A 640m pixel/s 3.5mv sparse event-driven
neuromorphic object recognition processor with on-chip learning, in Symp. VLSI Circuits,
Kyoto, Japan, June 2015, pp. 50–51.

[9] S.-J. Kim, K. Koh, M. Lustig, S. Boyd, and D. Gorinevsky, A interior-point method
for large scale l1-regularized least squares, IEEE J. Sel. Topics Signal Process., 1 (2007),
pp. 606–617.

[10] J. P. LaSalle, Some extensions of Liapunov’s second method, IRE Trans. Circuit Theory, 7

(1960), pp. 520–527.

[11] W. Lu and J. Wang, Convergence analysis of a class of nonsmooth gradient systems, IEEE

Trans. Circuits Syst. I, Reg. Papers, 55 (2008), pp. 3514–3527.

[12] C. J. Rozell, D. H. Johnson, R. G. Baraniuk, and B. A. Olshausen, Sparse coding via
thresholding and local competition in neural circuits, Neural Comput., 20 (2008), pp. 2526–
2563.

[13] S. Shapero, C. Rozell, and P. Hasler, Conﬁgurable hardware integrate and ﬁre neurons for

sparse approximation, Neural Netw., 45 (2013), pp. 134–143.

[14] R. Tibshirani, Regression shrinkage and selection via the lasso, J. Royal Statist. Soc B., 58

(1996), pp. 267–288.

[15] J. Troop, Algorithms for simultaneous sparse approximation. part ii: Convex relaxation, Sig-

nal Process., 86 (2006), pp. 589–602.

[16] Hui Zou and Trevor Hastie, Regularization and variable selection via the elastic net, J.

Royal Statist. Soc B., 67 (2005), pp. 301–320.

14

