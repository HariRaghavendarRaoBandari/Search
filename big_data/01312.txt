Learning Physical Intuition of Block Towers by Example

6
1
0
2

 
r
a

M
3

 

 
 
]
I

A
.
s
c
[
 
 

1
v
2
1
3
1
0

.

3
0
6
1
:
v
i
X
r
a

Adam Lerer
Facebook AI Research
Sam Gross
Facebook AI Research
Rob Fergus
Facebook AI Research

ALERER@FB.COM

SGROSS@FB.COM

ROBFERGUS@FB.COM

Abstract

Wooden blocks are a common toy for infants, al-
lowing them to develop motor skills and gain in-
tuition about the physical behavior of the world.
In this paper, we explore the ability of deep feed-
forward models to learn such intuitive physics.
Using a 3D game engine, we create small towers
of wooden blocks whose stability is randomized
and render them collapsing (or remaining up-
right). This data allows us to train large convolu-
tional network models which can accurately pre-
dict the outcome, as well as estimating the block
trajectories. The models are also able to gener-
alize in two important ways: (i) to new physical
scenarios, e.g.
towers with an additional block
and (ii) to images of real wooden blocks, where
it obtains a performance comparable to human
subjects.

1. Introduction
Interaction with the world requires a common-sense under-
standing of how it operates at a physical level. For example,
we can quickly assess if we can walk over a surface without
falling, or how an object will behave if we push it. Making
such judgements does not require us to invoke Newton’s
laws of mechanics – instead we rely on intuition, built up
through interaction with the world.
In this paper, we explore if a deep neural network can cap-
ture this type of knowledge. While DNNs have shown re-
markable success on perceptual tasks such as visual recog-
nition (Krizhevsky et al., 2012) and speech understanding
(Hinton et al., 2012), they have been rarely applied to prob-
lems involving higher-level reasoning, particularly those
involving physical understanding. However, this is needed
to move beyond object classiﬁcation and detection to a true
understanding of the environment, e.g. “What will happen

next in this scene?” Indeed, the fact that humans develop
such physical intuition at an early age (Carey, 2009), well
before most other types of high-level reasoning, suggests
its importance in comprehending the world.
To learn this common-sense understanding, a model needs
a way to interact with the physical world. A robotic plat-
form is one option that has been explored e.g. (Agrawal
et al., 2015), but inherent complexities limit the diversity
and quantity of data that can be acquired. Instead, we use
Unreal Engine 4 (UE4) (Epic Games, 2015), a platform for
modern 3D game development, to provide a realistic envi-
ronment. We chose UE4 for its realistic physics simulation,
modern 3D rendering, and open source license. We inte-
grate the Torch (Collobert et al., 2011) machine learning
framework directly into the UE4 game loop, allowing for
online interaction with the UE4 world.
One of the ﬁrst toys encountered by infants, wooden blocks
provide a simple setting for the implicit exploration of basic
Newtonian concepts such as center-of-mass, stability and
momentum. By asking deep models to predict the behav-
ior of the blocks, we hope that they too might internalize
such notions. Another reason for selecting this scenario is
that it is possible to construct real world examples, enabling
the generalization ability of our models to be probed (see
Fig. 1).
Two tasks are explored: (i) will the blocks fall over or not?
and (ii) where will the blocks end up? The former is a bi-
nary classiﬁcation problem, based on the stability of the
block conﬁguration. For the latter we predict image masks
that show the location of each block.
In contrast to the
ﬁrst task, this requires the models to capture the dynam-
ics of the system. Both tasks require an effective visual
system to analyze the conﬁguration of blocks. We explore
models based on contemporary convolutional networks ar-
chitectures (LeCun et al., 1989), notably Googlenet (Ioffe
& Szegedy, 2015), DeepMask (Pinheiro et al., 2015) and

Learning Physical Intuition of Block Towers by Example

experiments in vision, physical reasoning, and embodied
learning.

1.1. Related Work

The most closely related work to ours is (Battaglia et al.,
2013) who explore the physics involved with falling blocks.
A generative simulation model is used to predict the out-
come of a variety of block conﬁgurations with varying
physical properties, and is found to closely match human
judgment. This work complements ours in that it uses
a top-down approach, based on a sophisticated graphics
engine which incorporates explicit prior knowledge about
Newtonian mechanics.
In contrast, our model is purely
bottom-up, estimating stability directly from image pixels
and is learnt from examples.
Our pairing of top-down rendering engines for data genera-
tion with high capacity feed-forward regressors is similar in
spirit to the Kinect body pose estimation work of (Shotton
et al., 2013), although the application is quite different.
(Wu et al., 2015) recently investigated the learning of sim-
ple kinematics,
in the context of objects sliding down
ramps. Similar to (Battaglia et al., 2013), they also used
a top-down 3D physics engine to map from a hypothesis of
object mass, shape, friction etc. to image space. Inference
relies on MCMC, initialized to the output of convnet-based
estimates of the attributes. As in our work, their evalua-
tions are performed on real data and the model predictions
correlate reasonably with human judgement.
Prior work in reinforcement learning has used synthetic
data from games to train bottom-up models.
In particu-
lar, (Mnih et al., 2015) and (Lillicrap et al., 2015) trained
deep convolutional networks with reinforcement learning
directly on image pixels from simulations to learn policies
for Atari games and the TORCS driving simulator, respec-
tively.
A number of works in cognitive science have explored intu-
itive physics, for example, in the context of liquid dynamics
(Bates et al., 2015), ballistic motion (Smith et al., 2013) and
gears and pulleys (Hegarty, 2004). The latter ﬁnds that peo-
ple perform “mental simulation” to answer questions about
gears, pulleys, etc., but some form of implicit bottom-up
reasoning is involved too.
In computer vision, a number of works have used physical
reasoning to aid scene understanding (Zheng et al., 2015;
Koppula & Saxena, 2016). For example, (Jia et al., 2015)
ﬁt cuboids to RGBD data and use their centroids to search
for scene interpretations that are statically stable.

Figure 1. Block tower examples from the synthetic (left) and real
(right) datasets. The top and bottom rows show the ﬁrst and last
frames respectively.

ResNets (He et al., 2015). While designed for classiﬁca-
tion or segmentation, we adapt them to our novel task, us-
ing an integrated approach where the lower layers perceive
the arrangement of blocks and the upper layers implicitly
capture their inherent physics.
Our paper makes the following contributions:
Convnet-based Prediction of Static Stability: We show
that standard convnet models, reﬁned on synthetic data, can
accurately predict the stability of stacks of blocks. Cru-
cially, these models successfully generalize to (i) new im-
ages of real-world blocks and (ii) new physical scenarios,
not encountered during training. These models are purely
bottom-up in nature, in contrast to existing approaches
which rely on complex top-down graphics engines.
Prediction of Dynamics: The models are also able to pre-
dict with reasonably accuracy the trajectories of the blocks
as they fall, showing that they capture notions of accelera-
tion and momentum, again in a purely feed-forward man-
ner.
Comparison to Human Subjects: Evaluation of the test
datasets by participants shows that our models match their
performance on held-out real data (and are signiﬁcantly
better on synthetic data). Furthermore, the model predic-
tions have a reasonably high correlation with human judge-
ments.
UETorch: We introduce an open-source combination of
the Unreal game engine and the Torch deep learning en-
vironment, that is simple and efﬁcient to use. UETorch
is a viable environment for a variety of machine learning

Learning Physical Intuition of Block Towers by Example

Figure 2. Recorded screenshots and masks at 1-second intervals
from the Unreal Engine block simulation.

2. Methods
2.1. UETorch

UETorch is a package that embeds the Lua/Torch machine
learning environment directly into the UE4 game loop, al-
lowing for ﬁne-grained scripting and online control of UE4
simulations through Torch. Torch is well-suited for game
engine integration because Lua is the dominant scripting
language for games, and many games including UE4 sup-
port Lua scripting. UETorch adds additional interfaces to
capture screenshots, segmentation masks, optical ﬂow data,
and control of the game through user input or direct mod-
iﬁcation of game state. Since Torch runs inside the UE4
process, new capabilities can be easily added through FFI
without deﬁning additional interfaces/protocols for inter-
process communication. UETorch simulations can be run
faster than real time, aiding large-scale training. The
UETorch package can be downloaded freely at http:
//github.com/facebook/UETorch.

2.2. Data Collection
Synthetic
A simulation was developed in UETorch that generated ver-
tical stacks of 2, 3, or 4 colored blocks in random conﬁg-
urations. The block position and orientation, camera po-
sition, background textures, and lighting were randomized
at each trial to improve the transferability of learned fea-
tures. In each simulation, we recorded the outcome (did it
fall?) and captured screenshots and segmentation masks at
8 frames/sec. Frames and masks from a representative 4-
block simulation are shown in Fig. 2. A total of 180,000
simulations were performed, balanced across number of
blocks and stable/unstable conﬁgurations. 12,288 exam-
ples were reserved for testing.

Figure 3. The interface used for human experiments. At each turn,
the subject is shown an image on the left and tries to predict if the
stack will fall or not. No time limit is imposed. During train-
ing phase, the subject receives feedback on their prediction, by
showing them the outcome image on the right.

Real
Four wooden cubes were fabricated and spray painted red,
green, blue and yellow respectively. Manufacturing imper-
fections added a certain level of randomness to the stability
of the real stacked blocks, and we did not attempt to match
the physical properties of the real and synthetic blocks. The
blocks were manually stacked in conﬁgurations 2, 3 and 4
high against a white bedsheet. A tripod mounted DSLR
camera was used to ﬁlm the blocks falling at 60 frames/sec.
A white pole was held against the top block in each exam-
ple, and was then rapidly lifted upwards, allowing unstable
stacks to fall (the stick can be see in Fig. 1, blurred due to
its rapid motion). Note that this was performed even for
stable conﬁgurations, to avoid bias. Motion of the blocks
was only noticeable by the time the stick was several inches
away from top block. 493 examples were captured, bal-
anced between stable/unstable conﬁgurations. The totals
for 2, 3 and 4 block towers were 115, 139 and 239 exam-
ples respectively.

2.3. Human Subject Methodology

To better understand the challenge posed about our
datasets, real and synthetic, we asked 10 human subjects
to evaluate the images in a controlled experiment. Par-
ticipants were asked to give a binary prediction regarding
the outcome of the blocks (i.e. falling or not). During the
training phase, consisting of 50 randomly drawn examples,
participants were shown the ﬁnal frame of each example,
along with feedback as to whether their choice was correct
or not (see Fig. 3). Subsequently, they were tested using
100 randomly drawn examples (disjoint from the training
set). During the test phase, no feedback was provided to
the individuals regarding the correctness of their responses.

2.4. Model Architectures

We trained several convolutional network (CNN) architec-
tures on the synthetic blocks dataset. We trained some ar-

Will the blocks fall? (q = No, p = Yes)Prediction: stay; INCORRECTLearning Physical Intuition of Block Towers by Example

chitectures on the binary fall prediction task only, and oth-
ers on jointly on the fall prediction and mask prediction
tasks.
Fall Prediction
We trained the ResNet-34 (He et al., 2015) and Googlenet
(Szegedy et al., 2014) networks on the fall prediction task.
These models were pre-trained on the Imagenet dataset
(Russakovsky et al., 2015). We replaced the ﬁnal linear
layer with a single logistic output and ﬁne-tuned the entire
network with SGD on the blocks dataset. Grid search was
performed over learning rates.
Fall+Mask Prediction
We used deep mask networks to predict the segmenta-
tion trajectory of falling blocks at multiple future times
(0s,1s,2s,4s) based on an input image. Each mask pixel is
a multi-class classiﬁcation across a background class and
four foreground (block color) classes. A fall prediction is
also computed.
DeepMask (Pinheiro et al., 2015) is an existing mask pre-
diction network trained for instance segmentation, and has
the appropriate architecture for our purposes. We replaced
the binary mask head with a multi-class SoftMax, and repli-
cated this N times for mask prediction at multiple points in
time.
We also designed our own mask prediction network, Phys-
Net, that was suited to mask prediction rather than just
segmentation. For block masks, we desired (i) spatially
local and translation-invariant (i.e. convolutional) upsam-
pling from coarse image features to masks, and (ii) more
network depth at the coarsest spatial resolution, so the
network could reason about block movement. Therefore,
PhysNet take the 7 × 7 outputs from ResNet-34, and per-
forms alternating upsampling and convolution to arrive at
56×56 masks. The PhysNet architecture is shown in Fig. 4.
We use the Resnet-34 trunk in PhysNet for historical rea-
sons, but our experiments show comparable results with a
Googlenet trunk.
The training loss for mask networks is the sum of a binary
cross-entropy loss for fall prediction and a pixelwise multi-
class cross-entropy loss for each mask. A hyperparameter
controls the relative weight of these losses.
Baselines As a baseline, we perform logistic regression ei-
ther directly on image pixels, or on pretrained Googlenet
features, to predict fall and masks. To reduce the number
of parameters, the pixels-to-mask matrix is factored with an
intermediate dimension 128. For fall prediction, we also try
k-Nearest-Neighbors (k = 10) using Googlenet last-layer
image features.

(cid:34)

N(cid:88)

n=1

(cid:88)

c∈Cn

1
|Cn|

(cid:35)

2.5. Evaluation

We compare fall prediction accuracy on synthetic and real
images, both between models and also between model and
human performance. We also train models with a held-out
block tower size and test them on the held out tower size,
to evaluate the transfer learning capability of these models
models to different block tower sizes.
We evaluate mask predictions with two criteria: mean mask
IoU and log likelihood per pixel. We deﬁne mean mask IoU
as the intersection-over-union of the mask label with the
binarized prediction for the t = 4s mask, averaged over
each foreground class present in the mask label.

M IoU (m, q) =

1
N

IoU (mnc, ˆqnc)

|m1∩m2|
|m1∪m2|.

(1)
where mnc is the set of pixels of class c in mask n, Cn =
{c : c ∈ {1, 2, 3, 4} ∧ |mnc| > 0} is the set of fore-
ground classes present in mask n, ˆqnc is the set of pixels
in model output n for which c is the highest-scoring class,
and IoU (m1, m2) =
The mask IoU metric is intuitive but problematic because
it uses binarized masks. For example, if the model predicts
a mask with 40% probability in a region, the Mask IoU for
that block will be 0 whether or not the block fell in that
region. The quality of the predicted mask conﬁdences is
better captured by log likelihood.
The log likelihood per pixel is deﬁned as the log likelihood
of the correct ﬁnal mask under the predicted (SoftMax) dis-
tribution, divided by the number of pixels. This is essen-
tially the negative mask training loss.
Since the real data has a small number of examples (N =
493 across all blocks sizes), we report an estimated conﬁ-
dence interval for the model prediction on real examples.
We estimate this interval as the standard deviation of a bi-
nomial distribution with p approximated by the observed
accuracy of the model.

3. Results
3.1. Fall Prediction Results

Table 1 compares the accuracy for fall prediction of sev-
eral deep networks and baselines described in Section 2.4.
Convolutional networks perform well at fall prediction,
whether trained in isolation or jointly with mask predic-
tion. The best accuracy on synthetic data is achieved with
PhysNet, which is jointly trained on masks and fall predic-
tion. Accuracy on real data for all convnets is within their
standard deviation.
As an ablation study, we also measured the performance

Learning Physical Intuition of Block Towers by Example

Figure 4. Architecture of the PhysNet network.

Model

Fall Acc. (%)
(synthetic)

Fall Acc. (%)

(real)

Baselines
Random
Pixel Log. Reg
Googlenet Log. Reg.
Googlenet kNN
Classiﬁcation Models
ResNet-34
Googlenet
Googlenet
(no pretraining)
Mask Prediction Models
DeepMask
PhysNet

50.0
52.9
65.8
59.6

84.7
86.5
86.5

83.5
89.1

50.0 ± 2.2
49.3 ± 2.2
62.5 ± 2.2
50.9 ± 2.2

67.1 ± 2.1
69.0 ± 2.1
59.2 ± 2.2

66.1 ± 2.1
66.7 ± 2.1

Table 1. Fall prediction accuracy of convolutional networks on
synthetic and real data. The models substantially outperform
baselines, and all have similar performance whether trained singly
or jointly with the mask prediction task. Training Googlenet with-
out Imagenet pretraining does not affect performance on synthetic
examples, but degrades generalization to real examples. Baselines
are described in Section 2.4.

of Googlenet without Imagenet pretraining. Interestingly,
while the model performed equally well on synthetic data
with and without pretraining, only the pretrained model
generalized well to real images (Table 1).
Occlusion Experiments
We performed occlusion experiments to determine which
regions of the block images affected the models’ fall pre-
dictions. A Gaussian patch of gray pixels with standard de-
viation 20% of the image width was superimposed on the
image in a 14 × 14 sliding window to occlude parts of the
image, as shown in Fig. 5A. The PhysNet model was evalu-
ated on each occluded image, and the difference in the fall

probability predicted from the baseline and occluded im-
ages were used to produce heatmaps, shown in Fig. 5B-D.
These ﬁgures suggest that the model makes its prediction
based on relevant local image features rather than memo-
rizing the particular scene. For example, in Fig. 5B, the
model prediction is only affected by the unstable interface
between the middle and top blocks.
Model vs. Human Performance
Fig. 6 compares PhysNet to 10 human subjects on the same
set of synthetic and real test images. ROC curves compar-
ing human and model performance are generated by using
the fraction of test subjects predicting a fall as a proxy for
conﬁdence, and comparing this to model conﬁdences.
Overall, the model convincingly outperforms the human
subjects on synthetic data, and is comparable on real
data.
Interestingly, the correlation between human and
model conﬁdences on both real and synthetic data (ρ =
(0.69, 0.45)) is higher than between human conﬁdence and
ground truth (ρ = (0.60, 0.41)), showing that our model
agrees quite closely with human judgement.

3.2. Mask Prediction Results

Table 2 compares mask prediction accuracy of the Deep-
Mask and PhysNet networks described in Section 2.4.
PhysNet achieves the best performance on both Mean Mask
IoU and Log Likelihood per pixel (see Section 2.5), sub-
stantially outperforming DeepMask and baselines. Predict-
ing the mask as equal to the initial (t = 0) mask has a high
Mask IoU due to the deﬁciencies in that metric described
in Section 2.5.
Examples of PhysNet mask outputs on synthetic and real
data are shown in Fig. 7. We only show masks for exam-
ples that are predicted to fall, because predicting masks for

Learning Physical Intuition of Block Towers by Example

A

B

C

D

E

F

G

H

I

J

K

L

Figure 7. PhysNet mask predictions for synthetic (A–F) and real (G–L) towers of 2, 3, and 4 blocks. The image at the left of each
example is the initial frame shown to the model. The top row of masks are the ground truth masks from simulation, at 0, 1, 2, and 4
seconds. The bottom row are the model predictions, with the color intensity representing the predicted probability. PhysNet correctly
predicts fall direction and occlusion patterns for most synthetic examples, while on real examples, PhysNet overestimates stability (H,L).
In difﬁcult cases, Physnet produces diffuse masks due to uncertainty (D–F,I). B is particularly notable, as PhysNet predicts the red block
location from the small patch visible in the initial image.

Learning Physical Intuition of Block Towers by Example

A

C

B

D

Figure 5. A: Example of Gaussian occlusion mask, applied in
a sliding window to generate fall prediction heatmaps. B–D:
Heatmaps of predictions from occluded images. A green over-
lay means that an occlusion in this region increases the predicted
probability of falling, while a red overlay means the occlusion
decreases the predicted probability of falling. The model focuses
on unstable interfaces (B,C), or stabilizing blocks that prevent the
tower from falling (D).

Figure 6. Plots comparing PhysNet accuracy to human perfor-
mance on real (Top) and synthetic (Bottom) test examples. Left:
ROC plot comparing human and model predictions. Right: a
breakdown of the performance for differing numbers of blocks.
For humans, the mean performance is shown, along with the per-
formance of individual subjects (green circles). Overall, the Phys-
Net model is better than even the best performing of the human
subjects on synthetic data. On real data, PhysNet performs simi-
larly to humans.

stable towers is easy and the outputs are typically perfect.
The mask outputs from PhysNet are typically quite reason-
able for falling 2- and 3-block synthetic towers, but have
more errors and uncertainty on 4-block synthetic towers
and most real examples. In these cases, the masks are often
highly diffuse, showing high uncertainty about the trajec-
tory. On real examples, model predictions and masks are
also skewed overstable, likely because of different physical
properties of the real and simulated blocks.

3.3. Evaluation on Held-Out Number of Blocks

Table 3 compares the performance of networks that had ei-
ther 3- or 4-block conﬁgurations excluded from the training
set. While the accuracy of these networks is lower on the
untrained class relative to a fully-trained model, it’s still
relatively high – comparable to human performance. The
predicted masks on the untrained number of blocks also
continue to capture the fall dynamics with reasonably ac-
curacy. Some examples are shown in Fig. 8.

Model

DeepMask
PhysNet
Baseline
Pixel Log. Reg.
Googlenet Log. Reg.
Mask @ t = 0
Class-Constant

Mask IoU (%) Log Likelihood/px

(synthetic)

(synthetic)

42.4
75.4

29.6
23.8
72.0

0

-0.299
-0.107

-0.562
-0.492
−∞
-0.490

Table 2. Mask prediction accuracy of DeepMask and our PhysNet
network. The metrics used are described in Section 2.5; baselines
are described in Section 2.4. As an additional IoU baseline we
evaluate the t = 0 mask as a prediction of the ﬁnal mask, and
as a log likelihood baseline we predict each pixel as the average
likelihood of that class in the data. The PhysNet network provides
the highest accuracy in both metrics. Mask examples are shown
in Fig. 7.

00.20.40.60.8100.10.20.30.40.50.60.70.80.91p(False positive rate)p(True positive rate)Synthetic test examples  HumanPhysNet00.20.40.60.8100.10.20.30.40.50.60.70.80.91p(False positive rate)p(True positive rate)Real test examples  HumanPhysNetLearning Physical Intuition of Block Towers by Example

Model

Googlenet
Googlenet
Googlenet
PhysNet
PhysNet
PhysNet

# Blocks
Training
2,3,4
2,3
2,4
2,3,4
2,3
2,4

Accuracy (%) (synth.)
2
92.6
93.7
93.3
94.5
95.0
93.5

3
86.7
85.9
82.0
87.9
87.4
84.5

4
82.3
71.3
79.5
84.7
77.3
83.6

Accuracy (%) (real)

3

4

2

69.6 ± 4.3
65.2 ± 4.4
69.6 ± 4.3
66.1 ± 4.4
60.0 ± 4.6
55.7 ± 4.6

69.8 ± 3.9
66.9 ± 4.0
66.9 ± 4.0
65.5 ± 4.0
64.0 ± 4.1
67.6 ± 4.0

69.9 ± 3.0
69.0 ± 3.0
70.7 ± 2.9
73.2 ± 2.9
70.1 ± 2.9
69.9 ± 3.0

Mask Log Likelihood/px (synth.)

4

2

3

-0.035
-0.042
-0.040

-0.096
-0.125
-0.154

-0.190
-0.362
-0.268

Table 3. Fall prediction accuracy for Googlenet and PhysNet trained on subsets of the block tower sizes, and tested on the held-out block
tower size (blue cells). Prediction accuracy on the held-out class is reduced, but is still comparable to human performance (see Fig. 6).
On real block data, performance on the held out class is equivalent to the fully-trained model, to within standard deviation. PhysNet
mask predictions for held-out classes are only moderately degraded, and log likelihood scores are still superior to DeepMask predictions
(Table 1). Physnet masks for the held-out class are shown in Fig. 8.

A

B

C

D

Figure 8. PhysNet mask predictions on a tower size (3 or 4
blocks) that the network was not trained on. Mask predictions
for 3 blocks (A–B) still capture the dynamics well even though
the network never saw towers of 3 blocks. Mask predictions for 4
blocks capture some of the dynamics but show some degradation.

4. Discussion
Our results indicate that bottom-up deep CNN models can
attain human-level performance at predicting how towers
of blocks will fall. We also ﬁnd that these models’ per-
formance generalizes well to real images if the models are
pretrained on real data (Table 1).
Several experiments provide evidence that the deep models
we train are gaining knowledge about the dynamics of the
block towers, rather than simply memorizing a mapping
from conﬁgurations to outcomes. Most convincingly, the
relatively small degradation in performance of the models
on a tower size that is not shown during training (Table 3
& Fig. 8) demonstrates that the model must be making its
prediction based on local features rather than memorized
exact block conﬁgurations. The occlusion experiments in
Fig. 5 also suggest that models focus on particular regions
that confer stability or instability to a block conﬁguration.
Finally, the poor performance of k-nearest-neighbors on
Googlenet features in Table 1 suggests that nearby conﬁgu-
rations in Googlenet’s pretrained feature space are not pre-
dictive of the stability of a given conﬁguration.
Compared to top-down, simulation-based models such as
(Battaglia et al., 2013), deep models require far more train-
ing data – many thousands of examples – to achieve a high
level of performance. Deep models also have difﬁculty
generalizing to examples far from their training data. These
difﬁculties arise because deep models must learn physics
from scratch, whereas simulation-based models start with
strong priors encoded in the physics simulation engine.
Bottom-up and top-down approaches each have their ad-
vantages, and the precise combination of these systems in
human reasoning is the subject of debate (e.g. (Davis &
Marcus, 2016) and (Goodman et al., 2015)). Our results
suggest that deep models show promise for directly captur-
ing common-sense physical intuitions about the world that
could lead to more powerful visual reasoning systems.
We believe that synthetic data from realistic physical sim-

Learning Physical Intuition of Block Towers by Example

ulations in UETorch are useful for other machine learning
experiments in vision, physics, and agent learning. The
combination of synthetic data and mask prediction consti-
tutes a general framework for learning concepts such as
object permanence, 3D extent, occlusion, containment, so-
lidity, gravity, and collisions, that may be explored in the
future.

Acknowledgements
The authors would like to thank: Soumith Chintala and
Arthur Szlam for early feedback on experimental design;
Sainbayar Sukhbaatar for assistance collecting the real-
world block examples; Y-Lan Boureau for useful advice
regarding the human subject experiments; and Piotr Dollar
for feedback on the manuscript.

Learning Physical Intuition of Block Towers by Example

References
Agrawal, Pulkit, Carreira, Joao, and Malik, Jitendra.

Learning to see by moving. June 2015.

Bates, C.J., Yildirim, I., Tenenbaum, J. B., and Battaglia,
P. W. Humans predict liquid dynamics using probabilis-
In In Proc. 37th Ann. Conf. Cognitive
tic simulation.
Science Society, 2015.

Battaglia, Peter W., Hamrick, Jessica B., and Tenenbaum,
Joshua B. Simulation as an engine of physical scene
understanding. Proceedings of the National Academy of
Sciences, 110(45):18327–18332, 2013. doi: 10.1073/
pnas.1306572110. URL http://www.pnas.org/
content/110/45/18327.abstract.

Carey, Susan. The origin of concepts. Oxford University

Press, 2009.

Collobert, Ronan, Kavukcuoglu, Koray, and Farabet,
Cl´ement. Torch7: A matlab-like environment for ma-
In BigLearn, NIPS Workshop, number
chine learning.
EPFL-CONF-192376, 2011.

Davis, Ernest and Marcus, Gary. The scope and limits
of simulation in automated reasoning. Artiﬁcial Intel-
ligence, 233:60–72, 2016.

Epic Games.

Unreal engine 4.

unrealengine.com, 2015.

https://www.

Goodman, Noah D, Frank, Michael C, Grifﬁths, Thomas L,
Tenenbaum, Joshua B, Battaglia, Peter W, and Hamrick,
Jessica B. Relevant and robust a response to marcus
and davis (2013). Psychological science, 26(4):539–541,
2015.

He, K., Zhang, X., Ren, S., and Sun, J. Deep Residual
Learning for Image Recognition. ArXiv e-prints, Decem-
ber 2015.

Hegarty, Mary. Mechanical reasoning by mental simula-
tion. Trends in cognitive sciences, 8(6):280–285, 2004.

Hinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E,
Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior, An-
drew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath,
Tara N, et al. Deep neural networks for acoustic mod-
eling in speech recognition: The shared views of four
research groups. Signal Processing Magazine, IEEE, 29
(6):82–97, 2012.

Ioffe, Sergey and Szegedy, Christian. Batch normalization:
Accelerating deep network training by reducing inter-
nal covariate shift. CoRR, abs/1502.03167, 2015. URL
http://arxiv.org/abs/1502.03167.

Jia, Zhaoyin, Gallagher, A.C., Saxena, A., and Chen,
Tsuhan. 3d reasoning from blocks to stability. Pattern
Analysis and Machine Intelligence, IEEE Transactions
on, 37(5):905–918, 2015.

Koppula, Hema S and Saxena, Ashutosh. Anticipating
human activities using object affordances for reactive
robotic response. Pattern Analysis and Machine Intel-
ligence, IEEE Transactions on, 38(1):14–29, 2016.

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.
Imagenet classiﬁcation with deep convolutional neural
networks. In Pereira, F., Burges, C.J.C., Bottou, L., and
Weinberger, K.Q. (eds.), Advances in Neural Informa-
tion Processing Systems 25, pp. 1097–1105. Curran As-
sociates, Inc., 2012.

LeCun, Y., Boser, B., Denker, J. S., Henderson, D.,
Howard, R. E., Hubbard, W., and Jackel, L. D. Back-
propagation applied to handwritten zip code recognition.
Neural Computation, 1(4):541–551, Winter 1989.

Lillicrap, Timothy P., Hunt, Jonathan J., Pritzel, Alexander,
Heess, Nicolas, Erez, Tom, Tassa, Yuval, Silver, David,
and Wierstra, Daan. Continuous control with deep rein-
forcement learning. CoRR, abs/1509.02971, 2015. URL
http://arxiv.org/abs/1509.02971.

Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,
Rusu, Andrei A., Veness, Joel, Bellemare, Marc G.,
Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K.,
Ostrovski, Georg, Petersen, Stig, Beattie, Charles, Sadik,
Amir, Antonoglou, Ioannis, King, Helen, Kumaran,
Dharshan, Wierstra, Daan, Legg, Shane, and Hassabis,
Demis. Human-level control through deep reinforcement
learning. Nature, 518(7540):529–533, 02 2015. URL
http://dx.doi.org/10.1038/nature14236.

Pinheiro, Pedro O, Collobert, Ronan, and Dollar, Piotr.
Learning to segment object candidates. In Advances in
Neural Information Processing Systems, pp. 1981–1989,
2015.

Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan,
Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpa-
thy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg,
Alexander C., and Fei-Fei, Li.
ImageNet Large Scale
Visual Recognition Challenge. International Journal of
Computer Vision (IJCV), 115(3):211–252, 2015. doi:
10.1007/s11263-015-0816-y.

Shotton, Jamie, Sharp, Toby, Kipman, Alex, Fitzgibbon,
Andrew, Finocchio, Mark, Blake, Andrew, Cook, Mat,
and Moore, Richard. Real-time human pose recognition
in parts from single depth images. Communications of
the ACM, 56(1):116–124, 2013.

Learning Physical Intuition of Block Towers by Example

Smith, K. A., Battaglia, P. W., and Vul, E. Consistent
physics underlying ballistic motion prediction. In Proc.
35th Ann. Conf. Cognitive Science Society, 2013.

Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet,
Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Du-
mitru, Vanhoucke, Vincent, and Rabinovich, Andrew.
Going deeper with convolutions. CoRR, abs/1409.4842,
2014.

Wu, Jiajun, Yildirim, Ilker, Lim, Joseph J, Freeman, Bill,
and Tenenbaum, Josh. Galileo: Perceiving physical ob-
ject properties by integrating a physics engine with deep
learning.
In Cortes, C., Lawrence, N.D., Lee, D.D.,
Sugiyama, M., and Garnett, R. (eds.), Advances in Neu-
ral Information Processing Systems 28, pp. 127–135.
Curran Associates, Inc., 2015.

Zheng, Bo, Zhao, Yibiao, Yu, Joey, Ikeuchi, Katsushi, and
Zhu, Song-Chun. Scene understanding by reasoning sta-
bility and safety. International Journal of Computer Vi-
sion, 112(2):221–238, 2015.

