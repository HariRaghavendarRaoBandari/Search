6
1
0
2

 
r
a

 

M
1
1

 
 
]

.

R
P
h
t
a
m

[
 
 

1
v
1
7
5
3
0

.

3
0
6
1
:
v
i
X
r
a

Many Server Scaling of the N-System Under FCFS-ALIS

Dongyuan Zhan∗

Gideon Weiss†

March 14, 2016

Abstract

The N-System with independent Poisson arrivals and exponential server-dependent service
times under ﬁrst come ﬁrst served and assign to longest idle server policy has explicit steady
state distribution. We scale the arrival and the number of servers simultaneously, and obtain
the ﬂuid and central limit approximation for the steady state. This is the ﬁrst step towards
exploring the many server scaling limit behavior of general parallel service systems.

1 Introduction

In this paper we study the many server N-System shown in Figure 1, with Poisson arrivals and
exponential service times, under ﬁrst come ﬁrst served and assign to longest idle server policy
(FCFS-ALIS), as the number of servers becomes large. Before describing the model in detail, we
will ﬁrst discuss our motivation for studying this system.

Figure 1: The multi-server N-System

The N-System is one of the simplest special cases of the so called parallel server systems, as
deﬁned in [9, 16] and further studied in [15, 22, 7, 18, 6, 21, 13, 14, 1]. The general model has
customers of types i = 1, . . . , I, servers of types j = 1, . . . , J, and a bipartite compatibility graph
G where (i, j) ∈ G if customer type i can be served by server j. Arrivals are renewal with rate
λ, where successive customer types are i.i.d. with probabilities αi, there is a total of n servers,
of which nθj are of type j, and service times are generally distributed with rates µi,j. Assume
∗School of Management, University College London, Gower Street, London WC1E 6BT, United Kingdom; email:
†Department of Statistics, The University of Haifa, Mount Carmel 31905, Israel; email: gweiss@stat.haifa.ac.il

d.zhan@ucl.ac.uk

Research supported in part by Israel Science Foundation Grants 711/09 and 286/13.

1

1122λ2λ1n1n2µ1µ1µ2the system is operated under the FCFS-ALIS policy, that is servers take on the longest waiting
compatible customer, and arriving customers are assigned to the longest idle compatible server.
For this general system necessary and suﬃcient conditions for stability (positive Harris recurrence
for given λ), or for complete resource pooling (there exists critical λ0 such that the system is stable
for λ < λ0, and the queues of all customer types diverge for λ > λ0) cannot be determined by
1st moment information alone (as shown by an example of Foss and Chernova [9]). In particular,
under FCFS-ALIS calculation of the matching rates ri,j, which are long term average fractions of
services performed by servers of type j on customers of type i, is intractable.

In the special case that service rates depend only on the server type, and not on customer type,
with Poisson arrivals and exponential service times, the system has a product form stationary
distribution, as given in [5]. In that case matching rates can be computed from the stationary
distribution.

The following conjecture was made in [1]:

if the system is stable and has complete resource
pooling for given λ, n, and we let both become large together, the behavior of the system simpliﬁes:
there will exist βj such that servers of type j perform a fraction βj of the services, and the matching
rates ri,j will converge to the rates for the FCFS inﬁnite matching model with G, α, β, as calculated
in [4] (see also [2]). The conjecture is based on the following heuristic argument: in steady state the
times that each server becomes available form a stationary process which is only mildly correlated
with the other servers and so servers become available approximately as a superposition of almost
independent stationary processes which in the many server limit becomes a Poisson process, and
server types are then i.i.d. with probabilities βj, while customer types arrive as an i.i.d. sequence
with probabilities αi, which corresponds exactly to the model of FCFS inﬁnite matching.

In our current study of the many server N-System we shall verify the conjectured many server
behavior for this simple parallel server system. To do so we start from the known stationary
distribution of the N-System with many servers, as derived from [5], and study its behavior as
n → ∞. As it turns out, the product form stationary distribution even for this simple case is far
from simple, and the derivations of limits, which use summations over server permutations and
asymptotic expansions of various expressions are quite laborious. We feel that this emphasizes the
diﬃculty of verifying the conjectured behavior of the general system, which remains intractable
at this time.

We mention that the N-System with just two servers, has been the subject of several papers,
[12, 3, 11, 19]. In this paper, our focus is on the N-System with many servers under FCFS-ALIS
policy, and its limit property.

The rest of the paper is structured as follows: In Section 2 we describe the model, in Section
3 we use some heuristic arguments to obtain a guess at the limiting behavior. In Section 4 we
obtain the stationary behavior under many server scaling. In Section 5 we illustrate our results
with some numerical examples. To improve the readability of the paper we have put all the proofs
for Section 4 in the appendix.

2 The Model

In our N-System customers of types c1 and c2 arrive as independent Poisson streams, with rates
λ1, λ2. There are skill based parallel servers, n1 servers of type s1 which are ﬂexible and can
serve both types, and n2 servers of type s2 which can only serve type c1 customers. We assume
service times are all independent exponential, with server dependent rates. The service rate of an
s1 server is µ1, the service rate of an s2 server is µ2, see Fig 1. We let λ = λ1 + λ2, n = n1 + n2.
Service policy is FCFS-ALIS.

The system is obviously Markovian. In [3, 20, 5] the following state description for the server
dependent Poisson exponential system with J server types and I customer types was used: imagine
the customers arranged in a single queue by order of arrivals, and servers are attached to cus-
tomers which they serve, and the remaining idle servers are arranged by increasing idle time, see
Figure ?? The state is then s = (S1, q1, S2, q2, . . . , Sn−i, qn−i, Sn−i+1, . . . , Sn), where S1, . . . , Sn is
a permutation of the n servers, the ﬁrst n − i servers are the ordered busy servers, and the last

2

Figure 2: state description under FCFS-ALIS

i servers are the ordered idle servers, and where qj, j = 1, . . . , n − i are the queue lengths of the
customers waiting for one of the servers S1, . . . , Sj, and skipped (cannot be served) by servers
Sj+1, . . . , Sn.

For the special case of the N-System, the following three random quantities are important:
i1 = I1(s) the number of idle servers of type s1, i2 = I2(s) the number of idle servers of type s2,
and k = K(s) ≥ 0 the number of servers of type s2 which follow the last server of type s1 in the
sequence S1, . . . , Sn. We let i = I(s) be the total number of idle servers. Because of the structure
of the N-System, and the FCFS-ALIS policy the following properties hold for i = 0, . . . , n and
k = 0, . . . , n2:

(i) There are no customers waiting for any server which precedes the last s1 server in the permu-
tation. In other words, for all j < min(n − k, n − i) we have qj = 0. In particular, if there is
an idle type s1 server, in other words if i > k, then there are no waiting customers at all.

(ii) If there are any idle servers, then there are no type c1 customers waiting for service, in other

words, if i > 0 then all the waiting customers are of type c2.

(iii) If there are no idle servers, then only the last queue can contain type c1 customers, in other
words, if i = 0 then the last queue may contain customers of both types, but all the other
waiting customers are of type c2.

Denote

α =

λ1
λ

,

θ =

n1
n

,

ρ =

λ

n1µ1 + n2µ2

,

δ =

λ2
n1µ1

.

Then a necessary and suﬃcient condition for stability is

ρ < 1,

δ < 1

We shall require a stronger condition of complete resource pooling, deﬁned by

α + β > 1

where β equals the long run fraction of services performed by s1 servers. The value of β will be
calculated in the next Section.

Using the results of [4, 5] we can then write the exact stationary distribution of this system. We
wish to show that as the arrival rates and the number of servers increase the system simpliﬁes, and
we get very precise many server scaling limits. We will investigate the behavior of the system when
α, θ, ρ are ﬁxed, and n → ∞. To be precise, we shall then have n, λ = ρn, λ1 = αλ, λ2 = (1− α)λ,
n1 = (cid:100)θn(cid:101), n2 = n − n1, all of which go to ∞.

3 Fluid Calculations

We perform the following heuristic calculation: As long as the system is underloaded (ρ < 1), each
server of type s1 will have a cycle of service of mean length 1/µ1, followed by an idle period, and
similarly each server of type s2 will have service of mean length 1/µ2 followed by an idle period.
The key idea now is that when n → ∞, the idle periods should have the same length for both
types, because of ALIS. Let T be the average length of the idle time. The average cycle times will

3

CS**S1S2qn−iq1q2Sn−iSnSn−i+1be: 1/µ1 + T and 1/µ2 + T . Denote by β the long run fraction of services performed by s1 servers,
and 1− β for type s2. The ﬂow rate out of one type s1 server is 1/(1/µ1 + T ), the ﬂow rate out of
all type s1 servers should equal λβ. Similarly the ﬂow rate out of all type s2 servers should equal
λ(1 − β). That is,

λβ = n1/(1/µ1 + T ),

λ(1 − β) = n2/(1/µ2 + T ).

Now we solve for T and β: we rewrite

β =

n1
λ

1

1/µ1 + T

and eliminate β:

1 =

,

1

1 − β =

1

1/µ2 + T

n2
λ

1

n1
λ

+

n2
λ

1/µ1 + T

1/µ2 + T

to get a quadratic equation for T :

(cid:1)T + λ − n1µ1 − n2µ2 = 0.
g(T ) = λµ1µ2T 2 +(cid:0)λ(µ1 + µ2) − (n1 + n2)µ1µ2
(cid:17)2(cid:33)
(cid:16) 1

(cid:16) 1

(cid:115)

n1 − n2

(cid:32)

(cid:17)

Here g(0) < 0 by ρ < 1, so the equation has one positive and one negative root. Solving for
positive T we get:

T =

1
2

n
λ

− 1
µ1

− 1
µ2

+

n2
λ2 + 2

− 1
µ2

+

− 1
µ2

µ1

λ

µ1

(1)

Note: For the case of µ1 = µ2 = µ we get T = 1−ρ

1
µ .

ρ

From T and little’s law we can obtain the average number of idle servers in pool 1 and pool 2,

denoted by m1 and m2 respectively.

m1 = T λβ =

T n1

T + 1/µ1

,

m2 = T λ(1 − β) =

T n2

T + 1/µ2

.

(2)

The values of β and 1 − β are then:

β =

n1

λT + λ/µ1

,

1 − β =

n2

λT + λ/µ2

Note: both are positive, so 0 < β < 1. Also, when µ1 = µ2 we get β = θ.

The value of α does not come into the equation for T , or the calculation of β. Hence, once
we solve and obtain β, the property of complete resource pooling will consist of checking that
α > 1 − β.

We will show that the following holds for the stationary queue, as n → ∞:
• K(s) is distributed as a geometric random variable, taking values 0, 1, 2, . . . with probability

of success 1 − 1−β

α . It is independent of I1(s), I2(s).

• (I1(s), I2(s)) is close to a bivariate Normal, with means (m1, m2), variances

(cid:18) (n1 − m1)m1(n2m1 + m2

(n2 − m2)m2(n1m2 + m2
1)

2)

,

n1m2

2 + n2m2
1

n1m2

2 + n2m2
1

(cid:18) (n1 − m1)(n2 − m2)m1m2

(cid:19) 1

2

(n1m2 + m2

1)(n2m1 + m2
2)

.

(cid:19)

,

and correlation

• Successive idle servers except for the last K + 1 are i.i.d. of type s1 with probability β and

of type s2 with probability 1 − β.

4

4 Many server limit of the stationary distribution

4.1 Exact Stationary Distributions

We ﬁrst obtain the stationary distribution for each state s. We note that the stationary probabil-
ities depend mainly on the values of K(s), I1(s), I2(s). Let µ(Sj) denote the service rate of the
server at position j.

Theorem 1. The stationary distribution of the state s of the FCFS-ALIS many server N-system
is given by:



l=1

j=1

 l(cid:88)
n−i1−i2(cid:89)
 l(cid:88)
n−k−1(cid:89)
 l(cid:88)
n−k−1(cid:89)

j=1

l=1

l=1

j=1

B

B

B

µ(Sj)

(cid:19)k

,

λ1

λ

−1(cid:18) 1
(cid:19)i1+i2−k(cid:18) 1
−1
n−i2(cid:89)
−1
n−1(cid:89)

j=n−k

λqj
2

λqj
2

µ(Sj)

(µ1n1 + µ2(j − n1))qj +1

j=n−k

π(s) =

µ(Sj)

(µ1n1 + µ2(j − n1))qj +1

(cid:18) 1

(cid:19)i2

λ1

,

k = 0, . . . , n2,
i1 = 1, . . . , n1,
i2 = k, . . . , n2,

k = 1, . . . , n2,
i1 = 0,
i2 = 1, . . . , k,

λqn

(µ1n1 + µ2n2)qn+1 ,

k = 0, . . . , n2,
i1 = i2 = 0.

(3)

where B is a normalizing constant.

Proof. This follows for all three parts of (3) by utilizing properties (i),(ii),(iii) in Section 2 and
substituting into Equation (2.1), Theorem 2.1, in [5].

Before we manipulate equation (3), we introduce a lemma to facilitate the calculation.

Lemma 1. Let A1, . . . , Am denote a permutation of m given positive real numbers a1, . . . , am, we
have

(cid:88)

m(cid:89)

 l(cid:88)

−1

(cid:32) m(cid:89)

(cid:33)−1

(A1,...,Am)∈P(a1,...,am)

l=1

j=1

l=1

Aj

=

al

where P(a1, . . . , am) denotes the set of all the permutations of a1, . . . , am.

Now we can get the joint stationary distribution of K(s), I1(s), I2(s). We denote by π(k, i1, i2)

the stationary probability of K(s) = k, I1(s) = i1 and I2(s) = i2.

Theorem 2. The steady state joint distribution of K(s), I1(s), I2(s) is given by:

(cid:18)n1

(cid:19)(cid:18)n2

(cid:19) i1i2!(i1 + i2 − k − 1)!

B1

i1

i2

(i2 − k)!

µi1
1 µi2

2

(cid:19)k

(cid:18) 1

λ

(cid:19)i1+i2(cid:18) λ
(cid:19)i2
(cid:18) 1

λ1

,

k = 0, . . . , n2,
i1 = 1, . . . , n1,
i2 = k, . . . , n2,

k = 1, . . . , n2,
i1 = 0,
i2 = 1, . . . , k,

π(k, i1, i2) =

1

µ1n1 + µ2(j − n1) − λ2

λ1

,



B1

n1 n2!
(n2 − k)!

µ1µk
2

B1

n1 n2!
(n2 − k)!

µ1µk
2

n−i2(cid:89)
n−1(cid:89)

j=n−k

j=n−k

where B1 is a normalizing constant.

5

1

µ1n1 + µ2(j − n1) − λ2

1

µ1n1 + µ2n2 − λ

,

k = 0, . . . , n2,
i1 = i2 = 0.

(4)

4.2 The Distribution of (I1(s), I2(s)) Given K(s)

In this section we obtain the asymptotic distribution of (I1(s), I2(s)) conditional on K(s) = k, as
n → ∞. We ﬁrst show that as n → ∞, the probability of no idle servers of type s1 goes to zero,
and so the probability that customers need not wait goes to 1. Next we condition on K(s) = k
and show I1(s)/n

p−→ f1, I2(s)/n

p−→ f2, where

f1 =

m1
n

=

T θ

T + 1/µ1

,

f2 =

m2
n

=

T (1 − θ)
T + 1/µ2

,

where T is given in (1). Finally, we condition on K(s) = k and show that the scaled and centered
values of (I1(s), I2(s)) converge in distribution to a bivariate normal distribution.
Theorem 3. When n → ∞, as long as ρ < 1, δ < 1,

P (I1(s) = 0) → 0.

From this theorem we see that when n → ∞, P (I1(s) > 0) → 1. Therefore, P (K(s) =
k, I1(s) > 0) → P (K(s) = k) for any 0 ≤ k ≤ I2(s). From equation (4), given K(s) = k, the
limiting stationary distribution as n → ∞ is

P (I1(s) = i1, I2(s) → i2|K(s) = k) → P (I1(s) = i1, I2(s) = i2|K(s) = k, I1(s) > 0)

i1(i1 + i2 − k)!

µi1
1 µi2

2 λ−i1−i2−kλ−k

1

1

P (K(s) = k)

.

i2!

(i2 − k)!

(cid:16) I1(s)

(cid:17)

converge to (f1, f2) in probability for any

n , I2(s)
Theorem 4. Conditional on K(s) = k,
k ≥ 0. That is, for any  > 0, when n → ∞, we have

n

(cid:18)n1

(cid:19)(cid:18)n2

(cid:19)

i1

i2

= B1

P (|I1(s) − m1| ≥ n or |I2(s) − m2| ≥ n|K(s) = k) → 0.

After showing the ﬂuid limit result, we are now ready to show the central limit result.

Theorem 5. For any k ≥ 0, when n → ∞, we have

(cid:18) I1(s) − m1

√

n

I2(s) − m2

√

,

(cid:21)(cid:19)

(5)

ρσ1σ2

1
ρσ1σ2 σ2
2

where

ρ =

n

0,

⇒ N

(cid:12)(cid:12)(cid:12)(cid:12) K(s = k)
(cid:18)
(cid:19)
(cid:18) (θ − f1)(1 − θ − f2)f1f2
(cid:18) (θ − f1)f1((1 − θ)f1 + f 2
(cid:18) (1 − θ − f2)f2(θf2 + f 2

1 )((1 − θ)f1 + f 2
2 )
2 )

2 + (1 − θ)f 2

(θf2 + f 2

(cid:20) σ2
(cid:19) 1
(cid:19) 1
(cid:19) 1

θf 2

1

2

2

1 )

,

,

2

.

2 + (1 − θ)f 2

1

θf 2

σ1 =

σ2 =

4.3 The Distribution of K(s), the Location of the First Type s1 Server.
Theorem 6. For any k ≥ 0, as n → ∞,

P (K(s) = k) →

.

(6)

(cid:19)(cid:18) 1 − β

(cid:19)k

α

(cid:18)

1 − 1 − β

α

6

4.4 Summary of stationary distribution

Theorem 6 shows that K(s) converges in distribution to a geometric distribution, so P (K(s) <
∞) = 1. Therefore, we can extend Theorem 4 and Theorem 5 into unconditional versions.
Theorem 7. When n → ∞, K(s) becomes independent of I1(s) and I2(s).
converges in distribution to the bivariate Normal distribution described in 5.

(cid:16) I1(s)−m1√

, I2(s)−m2√

(cid:17)

n

n

Consider a special case when µ1 = µ2 = µ, we have θ = β. m1 and m2 can be easily solved:

(cid:16) I1(s)−(1−ρ)n1

m1 = (1 − ρ)n1, m2 = (1 − ρ)n2.
, I2(s)−(1−ρ)n2

√

(cid:17)

When n → ∞,
distribution with mean (0, 0), variance

√

n

n

converges in distribution to a bivariate Normal

and correlation

( ρθ(1 − ρ(1 − θ)), ρ(1 − θ)(1 − ρθ) ) ,

ρ(cid:112)θ(1 − θ)

(cid:112)(1 − ρ(1 − θ))(1 − ρθ)

.

The total idleness has mean of (1 − ρ)n and variance of

V ar(I1(s)) + V ar(I2(s)) + 2Cov(I1(s), I2(s)) = ρn.

4.5 Comparison to the bipartite FCFS inﬁnite matching model

In the inﬁnite matching model corresponding to the N-System there is an inﬁnite sequence of
customers, of types c1, c2, where the customer types are i.i.d., type is c1 with probability α and c2
with probability 1 − α, and an independent sequence of servers, of types s1, s2, where the server
types are i.i.d., type is s1 with probability β and s2 with probability 1−β, and compatibility graph
with arcs {(c1, s1), (c1, s2), (c2, s1)}. Successive customers and servers are matched according to
FCFS: each server is matched to the ﬁrst compatible customer that was not matched to a previous
server, and each customer is matched to the ﬁrst compatible server that was not matched to a
previous customer.

After n of the customers have been matched, consider the sequence of remaining servers. Let
Kn be the number of servers of type s2 that are ﬁrst in this sequence, preceding the ﬁrst server of
type s1. The (Kn)∞
n=0 is a Markov chain. The steady state distribution for this Markov chain is
, k ≥ 0, which is exactly the limiting distribution of K(s)
that P (K∞ = k) =
in (6).

(cid:17)(cid:16) 1−β

1 − 1−β

(cid:17)k

(cid:16)

α

α

5 Numerical Examples

We test our results by investigating an N-system. λ = 100, n1 = n2 = 100, µ1 = µ2 = 1, ρ = 0.5.
From our approximation results, as long as α + θ > 1, or α > 0.5, both pools should have similar
utilization. So the average number of idle servers in each pool is close to 50, with variance of
θρ(1 − ρ + θρ)n = (1 − θ)ρ(1 − ρ + (1 − θ)ρ)n = 37.5. We use exact stationary distribution to
verify this.

Now we can calculate the expectation and variance of idle number in each pool exactly, listed
in the following table. We can see that when α > 0.5, the approximation is very good. When
α < 0.5, the approximation does not work. In fact, when α + θ < 1 and system is large, complete
resource pooling disappears, and server pool 1 seldom serves type s1 customers. The N-system
operates like 2 separate queues: pool 1 serves type s2 and pool 2 serves type s1. The utilization
of pool 1 is (1−α)λ
. From previous results, the number of idles

and the utilization of pool 2 is αλ
n2

n1

7

Table 1: The exact calculation
E[I1]

V ar[I1]

E[I2]

V ar[I2]

49.8383
49.6482
49.1787
48.6055
47.333
39.981

37.7049
38.078
39.2148
40.8706
44.883
59.821

50.1617
50.3518
50.8213
51.3945
52.667
60.019

37.3814
37.3743
37.5722
38.0816
39.549
39.7854

α

0.8
0.7
0.6
0.55
0.5
0.4

servers in pool 1 can be approximated by a Normal distribution with mean n1 − (1 − α)λ = 40
and variance (1 − α)λ = 60; whereas the number of idles servers in pool 2 can be approximated
by a Normal distribution with mean n2 − αλ = 60 and variance αλ = 40.

When α goes down to 0.5, the approximation is getting worse. Note that E[K(s)] ≈ α

α+θ−1 is
large when α is close to 0.5, making it not negligible. We have a better approximation for α close
to 0.5. When K(s) = k, we use θ
n−1−k instead of θ. The approximated average number
of busy servers in pool 1 is θ

(k) = n1−1

(k)ρn.

(cid:48)

(cid:48)

(cid:88)

k=0

n1 − 1
n − 1 − k

n2

(cid:18) 1 − θ

(cid:19)k α + θ − 1

α

α

ρn

E[X] = E[E[X|K(s)]] = E[θ

(cid:48)

(K)(1 − ρ)n] =

To make it consistent, we need θ = E[X]

ρn . Solving this equation gives θ, and E[X]. In this example,

when α = 0.6, the solution to(cid:88)

(cid:18) 1 − θ

(cid:19)k α + θ − 1

α

α

= θ

n1 − 1
n − 1 − k

n2

k=0

is θ = 0.5093. E[I1] = 100 − θρn = 49.07, which is closer to the true value 49.18. The following
table lists the comparison of the improved approximation and the true value: We can see that

Table 2: The improved approximation
0.55

0.8

0.7

0.6

α

0.5

E[I1]

Approx.

49.8383
49.83

49.6482
49.62

49.1787
49.07

48.6055
48.29

47.333
46.46

even when α = 0.5, the improved approximation is not bad.

Acknowledgment

We are grateful to Ivo Adan for helpful discussion of this paper.

A Appendix: Proofs for Section 4

A.1 Proof of Lemma 1 and Theorem 2

Proof of Lemma 1. We prove this lemma by induction. Deﬁne the left-hand-side as Cm. Step 2:

C2 =

1

a1(a1 + a2)

+

1

a2(a1 + a2)

=

a1 + a2

a1a2(a1 + a2)

=

1

a1a2

8

−1
m−1(cid:89)

Aj

l=1

j=1

 l(cid:88)
m(cid:89)
(cid:88)
−1

aj

 l(cid:88)

−1

Aj

l=1 al

(A1,...,Am−1)∈P(aj :j(cid:54)=p)

l=1

j=1

Step m:

Cm =

=

=

=

=

(A1,...,Am)∈P(a1,...,am)

(cid:88)
m(cid:88)
(cid:89)
m(cid:88)
(cid:80)m
(cid:81)m
(cid:33)−1

j(cid:54)=p
p=1 ap
j=1 aj

p=1

p=1

1(cid:80)m
1(cid:80)m
1(cid:80)m
(cid:32) m(cid:89)

l=1

l=1 al

l=1 al

al

Proof of Theorm 2. Summation over the geometric terms qj = 0, . . . ,∞ in (3) gives



l=1

j=1

 l(cid:88)
n−i1−i2(cid:89)
 l(cid:88)
n−k−1(cid:89)
 l(cid:88)
n−k−1(cid:89)

j=1

l=1

l=1

j=1

B

B

B

µ(Sj)

µ(Sj)

µ(Sj)

(cid:19)k

,

λ1

λ

−1(cid:18) 1
(cid:19)i1+i2−k(cid:18) 1
−1
n−i2(cid:89)
−1
n−1(cid:89)

j=n−k

1

1

j=n−k

µ1n1 + µ2(j − n1) − λ2

µ1n1 + µ2(j − n1) − λ2

(cid:18) 1

(cid:19)i2

λ1

,

k = 0, . . . , n2,
i1 = 0, . . . , n1
i2 = k, . . . , n2,

k = 1, . . . , n2,
i1 = 0,
i2 = 1, . . . , k,

1

µ1n1 + µ2n2 − λ

,

k = 0, . . . , n2,
i1 = i2 = 0.

(cid:88)

q1,··· ,qn−i

π(s) =

Next we see that in this expression, permutations of S1, . . . , Sn with the same (k, i1, i2) have
a similar structure. We now sum over all the permutations of the appropriate Sj, 1 ≤ j ≤
n − max{k + 1, i1 + i2}. By Lemma 1 we obtain



(cid:19)k

,

λ1

λ

(cid:19)i1+i2−k(cid:18) 1
(cid:18) 1
n−i2(cid:89)
n−1(cid:89)

1

1

j=n−k

Bµi1−n1

1

µi2−n2

2

Bµ1−n1

1

µk−n2

2

Bµ1−n1

1

µk−n2

2

µ1n1 + µ2(j − n1) − λ2

j=n−k

µ1n1 + µ2(j − n1) − λ2

(cid:18) 1

(cid:19)i2

λ1

,

k = 0, . . . , n2,
i1 = 1, . . . , n1,
i2 = k, . . . , n2,

k = 1, . . . , n2,
i1 = 0,
i2 = 1, . . . , k,

(7)

1

µ1n1 + µ2n2 − λ

,

k = 0, . . . , n2,
i1 = i2 = 0.

Each permutation of the remaining servers, Sj, n − max{k + 1, i1 + i2} < j ≤ n has the same
stationary probability. It remains to count the number of permutations. When i1 = 0 we have
i2 ≤ k. For each permutation we choose 1 type s1 server and k out of n2 type s2 servers to form
the last k + 1 servers. The number of permutations is

(cid:18)n2

(cid:19)

k

n1

n1 n2!
(n2 − k)!

.

k! =

9

When i1 > 0, we have i2 ≥ k. For each permutation, we choose i1 out of n1 type s1 servers and
i2 out of n2 type s2 servers. We then choose 1 from the i1 idle servers of type s1, and k from the
i2 idle servers of type s2 to obtain the last k + 1 servers. The number of permutations is

(cid:18)n1

(cid:19)(cid:18)n2

(cid:19)

(cid:18)i2

(cid:19)

i1

i2

i1

k

(i1 + i2 − k − 1)!k! =

(cid:18)n1

(cid:19)(cid:18)n2

(cid:19) i1i2!(i1 + i2 − k − 1)!

(i2 − k)!

.

i1

i2

Multiplying the terms in (7) by the appropriate number of permutations and deﬁning B1 =
Bµ

gives (4).

−n2
−n1
1 µ
2

A.2 Proofs of Theorems 3, 4 and 5

Proof of Theorem 3. We prove the theorem in three steps:

(i) We show that

P (I1(s) = 0) ∼ B1

1
1 − δ

×



√
√

B1

(cid:16) 1−(1−α)ρ

2πn2 exp (n2 (− log κ + κ − 1))
2πn2/2
1
1−δ

1−ρ + 1
κ−1

(cid:17)

0 < κ < 1
κ = 1

κ > 1

. Note that by α+β > 1 we have κ > 1

1+µ2T . Note also that − log κ+κ−1 ≥

where κ := λ1
µ2n2
0.

(ii) We show that

(cid:18)

P (I1(s) = (cid:100)m1(cid:101), I2(s) = (cid:100)m2(cid:101), K(s) = 0)
∼ B1

(n1 − m1)(n2 − m2)m2

2πβn1n2

exp

(cid:19)1/2

(cid:20)

(cid:18)

(cid:18)

exp

−n2

log

1 − m2
n2

+

m2
n2

(cid:19)

(cid:20)

(cid:18)

(cid:18)

log

(cid:19)(cid:21)

(cid:19)

+

m1
n1

1 − m1
n1

−n1

(cid:19)(cid:21)

where m1 and m2 are deﬁned in (2).

(iii) We show that as n → ∞

P (I1(s) = (cid:100)m1(cid:101), I2(s) = (cid:100)m2(cid:101), K(s) = 0)

P (I1(s) = 0)

→ 0

which proves the proposition.

The details of the proofs of these three steps are as follows:

Proof of (i):
First we calculate

n2(cid:88)

k=0

=

n2(cid:88)
n−1(cid:89)

k=0

j=n−k

P (I1(s) = 0, I2(s) = 0) =

π(k, 0, 0)

B1

n1 n2!
(n2 − k)!

µ1µk
2

1

µ1n1 + µ2(j − n1) − λ2

1

µ1n1 + µ2n2 − λ

.

We use induction to calculate

Um :=

n2(cid:88)

k=m

µk
2

(n2 − k)!

n−1(cid:89)

j=n−k

10

1

µ1n1 + µ2(j − n1) − λ2

from m = n2 to m = 1. When m = n2,

Un2 =

µn2
2

(n2 − n2)!

1

µ1n1 − λ2

Suppose

then

Um+1 =

µm+1

2

(n2 − m − 1)!

n−1(cid:89)
n−1(cid:89)

j=n−n2+1

1

µ1n1 + µ2(j − n1) − λ2

.

1

µ1n1 − λ2

j=n−m

1

µ1n1 + µ2(j − n1) − λ2

n−1(cid:89)

j=n−m

1

µ1n1 + µ2(j − n1) − λ2

µm+1

2

(n2 − m − 1)!

µ1n1 − λ2

1

n−1(cid:89)

j=n−m

n−1(cid:89)

j=n−m

µm
2

+

(n2 − m)!

1

µ1n1 + µ2(j − n1) − λ2

µm
2

(n2 − m)!

1

µ1n1 + µ2(j − n1) − λ2

(cid:18) µ2(n2 − m)

µ1n1 − λ2

(cid:19)

+ 1

µm
2

(n2 − m)!

1

µ1n1 − λ2

1

µ1n1 + µ2(j − n1) − λ2

.

n−1(cid:89)

j=n+1−m

Um =

=

=

Therefore, the induction is valid and we have

U1 =

µ2

(n2 − 1)!

1

µ1n1 − λ2

.

P (I1(s) = 0, I2(s) = 0) = U1B1n1n2!

µ1

µ1n1 + µ2n2 − λ

+ π(0, 0, 0)

+ B1

µ1n1

µ1n1 + µ2n2 − λ

µ2n2

µ1n1

= B1

= B1

= B1

µ1n1

µ1n1 − λ2
µ1n1 − λ2
1
1 − δ

µ1n1 + µ2n2 − λ
µ1n1 + µ2n2 − λ2
µ1n1 + µ2n2 − λ
1 − ρ

.

1 − (1 − α)ρ

Next we calculate

P (I1(s) = 0, I2(s) > 0) =

n2(cid:88)

k(cid:88)

k=1

i2=1

π(k, 0, i2) =

n2(cid:88)

n2(cid:88)

i2=1

k=i2

π(k, 0, i2).

Similar to the induction calculating Um above, we can obtain

(cid:80)n2

k=i2

(cid:18) 1
(cid:18) 1

λ1

λ1

1
1 − δ

(cid:19)i2
(cid:19)i2
(cid:18) µ2

λ1

n2(cid:88)

k=i2

n1µ1n2!

n1µ1 n2!

(cid:19)i2

(n2 − i2)!
n2!

(n2 − i2)!

µk
2

(n2 − k)!
µi2
2

1

µ1n1 − λ2

π(k, 0, i2) = B1

= B1

= B1

n−i2(cid:89)

j=n−k

1

µ1n1 + µ2(j − n1) − λ2

11

Therefore,

P (I1(s) = 0, I2(s) > 0) = B1

1
1 − δ

n2!

= B1

1
1 − δ

n2!

1

i2=1

n2(cid:88)
(cid:18) λ1
(cid:18) µ2

µ2

µ2

(cid:19)−i2
(cid:18) λ1
(cid:19)−n2 n2−1(cid:88)
(cid:18) λ1
(cid:19)
(cid:18) λ1
(cid:19)n2

(cid:48)
i
1=0

µ2

exp

µ2

(n2 − i2)!

(cid:19)i

(cid:48)
2 1
(cid:48)
2!
i

P (X < n2)

= B1

= B1

1
1 − δ
1
1 − δ

n2!

λ1

P (X < n2)
P (X = n2)

,

(cid:18) λ1
(cid:19)n2
(cid:18) λ1
(cid:18)

µ2

µ2n2

(cid:18)
(cid:19)n2
(cid:18)

exp

(cid:19)
(cid:18)
(cid:18) λ1

− λ1
µ2

exp

exp

n2

log

exp (n2 (log κ + 1 − κ)) ,

(cid:19)

(cid:19)

n2 − λ1
µ2
+ 1 − λ1
µ2n2

µ2n2

(cid:19)(cid:19)

P (X = n2) =

∼

=

=

1
n2!
1√
2πn2
1√
2πn2
1√
2πn2

where X is a Poisson random variable with parameter λ1
µ2

. Using Stirling’s approximation,

Recall that κ = λ1
µ2n2
approximated by a Normal distribution with mean λ1
µ2
in 3 cases depending on κ.

and note that log κ + 1 − κ ≤ 0. Note also that when n → ∞, X can be
. Next we analyze P (X<n2)
P (X=n2)

and variance λ1
µ2

• When 0 < κ < 1, from the Normal distribution approximation, when n → ∞, P (X < n2) →

1. Therefore,

P (I1(s) = 0, I2(s) > 0) ∼ B1

1
1 − δ

(cid:0)√

2πn2 exp (−n2 (log κ + 1 − κ))(cid:1) .

• When κ = 1, − log κ + κ − 1 = 0. When n → ∞, the Normal distribution approximation

gives P (X < n2) → 1
2 .

P (I1(s) = 0, I2(s) > 0) ∼ B1

√

1
1 − δ

1
2

2πn2.

• When κ > 1, when n → ∞, the Normal distribution approximation gives P (X < n2) → 0.

We need more care to treat this case. For any 1 ≤ j ≤ n2,

=

n2!
2(n2 − j)!

κjnj

<

1
κj .

P (X = n2 − j)
P (X = n2)

=

Therefore,

In fact, for any ﬁxed j, when n → ∞,

1

(n2−j)!

µ2

(cid:16) λ1
(cid:17)n2−j
(cid:17)n2 1
(cid:16) λ1
≤ n2(cid:88)

µ2

n2!

j=1

P (X < n2)
P (X = n2)

1
κj <

1

κ − 1

.

P (X = n2 − j)
P (X = n2)

→ 1
κj .

12

For any  > 0, let J := (cid:100)− log 
n > N , for any 1 ≤ j ≤ J,

log κ (cid:101). We have  ≥ κ−J . There exists an N such that when

Therefore,

P (X < n2)
P (X = n2)

>

Therefore, when n → ∞,

P (X = n2 − j)
P (X = n2)

κj > − 
− 1

J

.

1

κj −  =

1 − κ−J
κ − 1

−  ≥ 1

κ − 1

− κ
κ − 1

.

P (X < n2)
P (X = n2)

→ 1

κ − 1

We have

P (I1(s) = 0, I2(s) > 0) ∼ B1

1
1 − δ

1

κ − 1

.

In summary, when κ ≤ 1, P (I1(s) = 0, I2(s) = 0) is negligible compared with P (I1(s) = 0, I2(s) > 0)

when n → ∞. We have

P (I1(s) = 0) ∼ B1

1
1 − δ

×

√
2πn2 exp (n2 (− log κ + κ − 1))
√
2πn2/2
1−(1−α)ρ
1−ρ + 1
κ−1

0 < κ < 1
κ = 1
κ > 1

J(cid:88)

j=1



Proof of (ii):
From equation (4) we have
P (I1(s) = (cid:100)m1(cid:101), I2(s) = (cid:100)m2(cid:101), K(s) = 0)

(cid:100)m1(cid:101)((cid:100)m1(cid:101) + (cid:100)m2(cid:101) − 1)!µ

(cid:100)m1(cid:101)
1

(cid:18) n1

(cid:19)(cid:18) n2

(cid:19)

m1

m2

λ
m1(m1 + m2 − 1)!µm1

1 µm2

2

(cid:100)m2(cid:101)
µ
2

(cid:18) 1
(cid:19)m1+m2

(cid:19)(cid:100)m1(cid:101)+(cid:100)m2(cid:101)
(cid:19)m1+m2
(cid:18) 1

λ

m1(m1 + m2 − 1)!µm1

1 µm2

m2
(n1 − m1)!m1!(n2 − m2)!m2!

n1!n2!

2

(cid:18) 1
(cid:19)1/2

λ

(m1 + m2)!µm1

1 µm2

2 λ−m1−m2

nn1
1 nn2
1 (n2 − m2)n2−m2 mm2
(n1 − m1)n1−m1mm1

2

2

m2

1m2(m1 + m2)2µ1µ2
B1

(cid:19)

(cid:19)

= B1

n5µ1µ2

∼ B1

n5µ1µ2

∼ B1

n5µ1µ2
×

B1

n5µ1µ2
×

B1

m1

m1

m1 + m2

(cid:18) n1(cid:100)m1(cid:101)
(cid:19)(cid:18) n2(cid:100)m2(cid:101)
(cid:18) n1
(cid:19)(cid:18) n2
(cid:18)
(cid:18) m1 + m2
(cid:18)
(cid:18) n1
(cid:18)
(cid:18)
(cid:18)

n1 − m1

(cid:18)

e

B1

B1

>

>

=

=

=

× exp(−m1 − m2)

(m1 + m2)(n1 − m1)(n2 − m2)m2

(m1 + m2)(n1 − m1)(n2 − m2)m2

λ

λ

(cid:19)m1+m2(cid:16) µ1
(cid:19)n1(cid:18) n2

2πm1n1n2

2πm1n1n2

n2 − m2

(cid:17)m1(cid:16) µ2
(cid:17)m2
(cid:19)1/2
(cid:19)n2(cid:18) m1 + m2
(cid:19)m1(cid:18) m1 + m2
(cid:19)m2
(cid:19)m1(cid:18) µ2(n2 − m2)
(cid:18) µ1(n1 − m1)
(cid:19)1/2(cid:18) n1
(cid:19)n1(cid:18) n2
(cid:19)1/2
(cid:18)
(cid:18)
(cid:18)
(cid:19)(cid:19)

n1 − m1
−n1

(cid:19)

exp

m1

m2

log

2πβn1n2

2πβn1n2

(cid:18)

λ

λ

n2 − m2
1 − m1
n1

(cid:19)m2

(cid:19)n2
(cid:19)

+

n5µ1µ2

(n1 − m1)(n2 − m2)m2

n5µ1µ2
× exp

(n1 − m1)(n2 − m2)m2
−n2

log

1 − m2
n2

+

m2
n2

.

13

exp(−m1 − m2)

(cid:19)(cid:19)

m1
n1

= 1 − β, µ1(n1−m1)

λ

= β, µ2(n2−m2)

λ

= 1 − β.

The second equality is due to m1

m1+m2

= β, m2

m1+m2

Proof of (iii):
Since log(1 − x) + x < 0 when 0 < x < 1, we have

(cid:19)

(cid:18)

(cid:16)

log

1 − m1
n1

+

m1
n1

< 0 and log

(cid:17)1/2

(cid:19)

(cid:18)

1 − m2
n2

+

m2
n2

< 0.

When n → ∞, note that
is of the order of n−1/2. Therefore, P (I1(s) =
(cid:100)m1(cid:101), I2(s) = (cid:100)m2(cid:101), K(s) = 0)/B1 increases exponentially. When κ > 1, P (I1(s) = 0)/B1 con-
verges to a constant; when κ = 1, P (I1(s) = 0)/B1 increases in the order of
n. Therefore, when
n → ∞ and κ ≥ 1,

(n1−m1)(n2−m2)m2

2πβn1n2

√

P (I1(s) = (cid:100)m1(cid:101), I2(s) = (cid:100)m2(cid:101), K(s) = 0)

P (I1(s) = 0)

→ 0.

(cid:19)(cid:19)

− log κ + κ − 1

.

(cid:19)1/2

m2
n2

+

n2

log

βn1(1 − δ)2
1 − m2
n2

(cid:18) (n1 − m1)(n2 − m2)m2
(cid:18)
(cid:19)
(cid:18)
(cid:18)
(cid:18) n2 − m2
(cid:19)
(cid:18) (n2 − m2)µ2
(cid:19)
(cid:19)
(cid:18) 1 − β
(cid:18)
1 − α + β − 1

µ2n2
λ1

(cid:19)

n2

λ1

+

α

+

+

α

α − (1 − β)

α

= log

< log

= log

λ1 − (n2 − m2)µ2

+
λ1 − (n2 − m2)µ2

n2µ2

κ

λ1

α + β − 1

α

< 0

(cid:18)

(cid:18)

(cid:19)

P (I1(s) = (cid:100)m1(cid:101), I2(s) = (cid:100)m2(cid:101), K(s) = 0)

P (I1(s) = 0)

× exp

n1

log

1 − m1
n1

+

m1
n1

∼

(cid:19)(cid:19)

exp

When κ < 1,

We have that

(cid:16)

(cid:18)
(cid:17)

log

1 − m2

n2

+ m2
n2

− log κ + κ − 1 = log

Therefore, when n → ∞,

P (I1(s) = (cid:100)m1(cid:101), I2(s) = (cid:100)m2(cid:101), K(s) = 0)

P (I1(s) = 0)

→ 0.

This completes the proof that when n → ∞,

P (I1(s) = 0) → 0

Proof of Theorem 5. First we show that the weak convergence is valid given K(s) = 0. Then
we show that the same holds when K(s) = k, for any ﬁxed k. When K(s) = 0, we prove the
convergence in probability in 2 steps.

(i) We show that for all states |I1(s)−m1| ≥ n or |I2(s)−m2| ≥ n, the conditional probability
is dominated by a bounded constant multiple of the conditional probability of some point
on the boundary of the rectangle |I1(s) − m1| ≤ n × |I2(s) − m2| ≤ n.

(ii) When n → ∞, we approximate the conditional probability of the points in the rectangle
|I1(s) − m1| ≤ n × |I2(s) − m2| ≤ n. We then show that the probability of points on the
boundary is negligible compared with the conditional probability at ((cid:100)m1(cid:101),(cid:100)m2(cid:101)).

14

Proof of (i):

(cid:18)n1

(cid:19)(cid:18)n2

P (I1(s) = i1, I2(s) = i2|K(s) = 0)
i1(i1 + i2 − 1)!µi1

= B2

(cid:19)

i2

i1
(n1 − i1)!(i1 − 1)!(n2 − i2)!i2!

n1! n2!

= B2

1 µi2

2 λ−i1−i2
(i1 + i2 − 1)!

(cid:16) µ1

(cid:17)i1(cid:16) µ2

(cid:17)i2

λ

λ

,

where B2 = B1/P (K(s) = 0).

P (I1(s) = i1 + 1, I2(s) = i2|K(s) = 0)
P (I1(s) = i1, I2(s) = i2|K(s) = 0)
P (I1(s) = i1, I2(s) = i2 + 1|K(s) = 0)
P (I1(s) = i1, I2(s) = i2|K(s) = 0)

=

(i1 + i2)(n1 − i1)µ1

i1λ

n1 − i1
n1 − m1

i1 + i2

i1

.

= β

=

(i1 + i2)(n2 − i2)µ2

(i2 + 1)λ

= (1 − β)

n2 − i2
n2 − m2

i1 + i2
i2 + 1

.

We look at several cases:

• When i1 ≤ m1 and (1−β)i1 < βi2, we have i1+i2

> 1

β . Therefore, P (I1(s)=i1+1,I2(s)=i2|K(s)=0)

P (I1(s)=i1,I2(s)=i2|K(s)=0) >

i1

• when i2 ≤ m2 and (1−β)i1 > βi2+1, we have i1+i2

i2+1 > 1

1−β . Therefore, P (I1(s)=i1,I2(s)=i2+1|K(s)=0)

P (I1(s)=i1,I2(s)=i2|K(s)=0) >

1;

1;

1;

• when i1 > m1, i2 > m2 and (1−β)i1 ≥ βi2, we have i1+i2

≤ 1

β . Therefore, P (I1(s)=i1+1,I2(s)=i2|K(s)=0)

P (I1(s)=i1,I2(s)=i2|K(s)=0) <

i1

• when i1 > m1, i2 > m2 and (1 − β)i1 ≤ βi2 + 1, we have i1+i2

i2+1 ≤ 1

1−β . Therefore,

P (I1(s)=i1,I2(s)=i2+1|K(s)=0)
P (I1(s)=i1,I2(s)=i2|K(s)=0) < 1;

• when βi2 ≤ (1 − β)i1 ≤ βi2 + 1, i1 ≤ m1 − n and i2 ≤ m2 − n. As long as n2−i2
n2−m2

we have P (I1(s)=i1,I2(s)=i2+1|K(s)=0)

P (I1(s)=i1,I2(s)=i2|K(s)=0) > 1. When n is large, this requires

i2
i2+1 > 1,

i2 > i∗

2 :=

1 − θ − f2

f2

.

As long as n1−i1
n1−m1
requires

i1−1
i1

> 1, we have P (I1(s)=i1+1,I2(s)=i2|K(s)=0)

P (I1(s)=i1,I2(s)=i2|K(s)=0) > 1. When n is large, this
i1 > i∗

.

1 :=

θ
f1

2, we can move the state to a neighbour state with larger steady state

1 or i2 > i∗

For all i1 > i∗
probability, shown as Figure 3.
the probability of any state (i1, i2) satisfying i1 > i∗
probability of some point at the boundary.

For any (i1, i2) satisfying i1 ≤ i∗

1 and i2 ≤ i∗

2, since

Eventually the movement stops at the boundary which are n away from (m1, m2). Therefore,
2 would be dominated by the

1 or i2 > i∗

P (I1(s) = i1 + 1, I2(s) = i2|K(s) = 0)
P (I1(s) = i1, I2(s) = i2|K(s) = 0)

> β and

P (I1(s) = i1, I2(s) = i2 + 1|K(s) = 0)
P (I1(s) = i1, I2(s) = i2|K(s) = 0)

> 1 − β,

We have
P (I1(s) = i1, I2(s) = i2|K(s) = 0) <

1

βi∗

1 +1(1 − β)i∗

2 +1 P (I1(s) = i∗

1 + 1, I2(s) = i∗

2 + 1|K(s) = 0)

15

Figure 3: The dominance of steady steady probability

and P (I1(s) = i∗
the boundary.

1 + 1, I2(s) = i∗

2 + 1|K(s) = 0) is dominated by the probability of some point at

Proof of (ii):
When i1 ∈ [m1 − n, m1 + n] and i2 ∈ [m2 − n, m2 + n], and n grows large, we can use

Stirling’s approximation.

(cid:19)(cid:18)n2

(cid:19)

P (I1(s) = i1, I2(s) = i2|K(s) = 0)
i1(i1 + i2 − 1)!µi1

= B2

1 µi2

i2
(n1 − i1)!i1!(n2 − i2)!i2!

n1!n2!

i1 + i2

i1
i1

(cid:18)n1
(cid:18)
(cid:18)

= B2

∼ B3

= B3

i1

i1

(i1 + i2)(n1 − i1)(n2 − i2)i2

(i1 + i2)(n1 − i1)(n2 − i2)i2

(cid:16) µ1

(cid:17)i1(cid:16) µ2

(cid:17)i2

λ

λ

(i1 + i2)!

2 λ−i1−i2
(cid:19)1/2
(cid:19)1/2

(i1 + i2)i1+i2 exp(−i1 − i2)

(n1 − i1)n1−i1ii1

1 (n2 − i2)n2−i2ii2

exp(cid:0)(i1 + i2) log(i1 + i2) − (n1 − i1) log(n1 − i1) − i1 log(i1)

λ

λ

(cid:17)i1(cid:16) µ2

(cid:17)i2

(cid:16) µ1
(cid:17) − i1 − i2

(cid:1)

2

(cid:16) µ2

(cid:16) µ1

(cid:17)

−(n2 − i2) log(n2 − i2) − i2 log(i2) + i1 log
2 en. Note that (n1−m1)µ1

+ i2 log

λ
= β and (n2−m2)µ2

λ

where B3 = B2n1!n2!(2π)− 3
and x2 := i2

n , we have x1 ∈ [f1 − , f1 + ] and x2 ∈ [f2 − , f2 + ].

λ

λ

= 1 − β. Deﬁne x1 := i1

n

(i1 + i2) log(i1 + i2) − (n1 − i1) log(n1 − i1) − i1 log(i1) − (n2 − i2) log(n2 − i2) − i2 log(i2)

= (i1 + i2) log(i1 + i2) − (n1 − i1) log(n1 − i1) − (n2 − i2) log(n2 − i2)

(cid:17)

(cid:16) µ1
(cid:18)

λ

+i1 log

+ i2 log

+i1 log

β

(n1 − m1)i1

(cid:16) µ2
(cid:19)

λ

(cid:17) − i1 − i2
(cid:18)

+ i2 log

(cid:18)

(cid:19)

1 − β

(n2 − m2)i2

− i1 − i2

= (i1 + i2) log(x1 + x2) + (i1 + i2) log n − (n1 − i1) log(θ − x1) − (n1 − i1) log n − (n2 − i2) log(1 − θ − x2)

−(n2 − i2) log n + i1 log

(1 − θ − f2)x2
= n((x1 + x2) log(x1 + x2) − (θ − x1) log(θ − x1) − (1 − θ − x2) log(1 − θ − x2)

(θ − f1)x1

β

− 2i1 log n + i2 log

− 2i2 log n − i1 − i2

+x1(log β − log(θ − f1) − log(x1)) + x2(log(1 − β) − log(1 − θ − f2) − log(x2)) − log n − x1 − x2)

(cid:18)

1 − β

(cid:19)

(cid:19)

16

(m1,m2) (0,0) i1 i2 (n1,n2) εn εn (i1*, i2*) (1-β)i1=βi2 (1-β)i1=βi2+1 We deﬁne

F (x1, x2) := (x1 + x2) log(x1 + x2) − (θ − x1) log(θ − x1) − (1 − θ − x2) log(1 − θ − x2)

+x1(log β − log(θ − f1) − log(x1)) + x2(log(1 − β) − log(1 − θ − f2) − log(x2)) − x1 − x2

The ﬁrst order derivatives on x1 and x2:

∂F
∂x1
∂F
∂x2

= log(x1 + x2) + log(θ − x1) − log(x1) − log
= log(x1 + x2) + log(1 − θ − x2) − log(x2) + log

β

θ − f1

= 0
1 − β

1 − θ − f2

= 0

Noting f1
f2

= β

1−β , we can verify that

x1 = f1,

x2 = f2

solve the ﬁrst order conditions. Look at the second order derivatives:

= − 1
= −

− 1
θ − x1
+
x1
− 1
1
1 − θ − x2
x2
1

1

< 0

x1 + x2
1

+

x1 + x2

< 0

∂2F
∂x2
1

∂2F
∂x2
2

∂2F

∂x1∂x2

(cid:18) ∂2F

x1 + x2

=

(cid:19)2

∂2F
∂x2
1

∂2F
∂x2
2

−

∂x1∂x2

=

x1x2(x1 + x2)(θ − x1)(1 − θ − x2)

> 0

1(1 − θ) + x2
x2
2θ

The Hessian matrix is negative deﬁnite. Therefore, F (x1, x2) is strictly concave on (0, θ)×(0, 1−θ)
and reaches its unique global maximum at (f1, f2). Since F (x1, x2) is strictly concave and reaches
its unique global maximum at (f1, f2). The maximum of F (x1, x2) on [δ, θ− δ]× [δ, 1− θ− δ]\(f1−
, f1 + ) × (f2 − , f2 + ) is on the boundary {(x1, x2)||x1 − f1| = ,|x2 − f2| = }. Since the
boundary is a compact set, the maximum is attainable, denoted by F (f1, f2) − η, where η > 0.

Note that(cid:18)

(cid:19)1/2

(cid:18)

i1

(i1 + i2)(n1 − i1)(n2 − i2)i2

=

(x1 + x2)(θ − x1)(1 − θ − x2)x2

x1

n−1

(cid:19)1/2

changes slowly when x1 and x2 change, compared with exp (nF (x1, x2)). We have

P (I1(s) = i1, I2(s) = i2|K(s) = 0)

P (I1(s) = (cid:100)m1(cid:101), I2(s) = (cid:100)m2(cid:101)|K(s) = 0)

∼ exp (n(F (x1, x2) − F (f1, f2))) < exp(ηn).

Therefore,

(cid:80)|i1−m1|>n or |i2−m2|>n P (I1(s) = i1, I2(s) = i2|K(s) = 0)

P (I1(s) = (cid:100)m1(cid:101), I2(s) = (cid:100)m2(cid:101)|K(s) = 0)

(cid:18)

<

n1n2 +

(cid:19)

exp(ηn)

1 + 1)(i∗
(i∗
2 + 1)
1 +1(1 − β)i∗
βi∗
2 +1

It converges to 0 when n → ∞.

When K(s) = k > 0, and n → ∞, similarly,

(cid:18)n1

(cid:19)(cid:18)n2

(cid:19)

P (I1(s) = i1, I2(s) = i2|K(s) = k) = B1
P (I1(s) = i1 + 1, I2(s) = i2|K(s) = k)
P (I1(s) = i1, I2(s) = i2|K(s) = k)
P (I1(s) = i1, I2(s) = i2 + 1|K(s) = k)
P (I1(s) = i1, I2(s) = i2|K(s) = k)

=

.
i1
i1 + i2 − k
i2 + 1 − k
We can use the similar 2-step argument to show that (I1(s)/n, I2(s)/n) converges to (f1, f2) in
probability given K(s) = k.

=
(i1 + i2 − k)(n2 − i2)µ2

n2 − i2
n2 − m2

(i2 + 1 − k)λ

= (1 − β)

= β

i1λ

.

i1

i2

i1(i1+i2−k−1)!
(i1 + i2 − k)(n1 − i1)µ1

i2!

(i2 − k)!

2 λ−i1−i2−kλk
i1 + i2 − k

1

µi1
1 µi2
n1 − i1
n1 − m1

(cid:14)P (K(s) = k)

17

√

Proof of Theorem 5. To obtain the asymptotic distribution of I1, I2 as n → ∞ we need to consider,
by Theorem 4, only values i1, i2 for which (i1 − m1)/n → 0 and (i2 − m2)/n → 0. We write i1 =
n → 0. Note that m1, m2, n1 − m1, n2 − m2
m1 + z1
are of the same order of magnitude as n, n1, n2, and we only consider i1, i2 of the same order of
magnitude.

√
n, with z1/

n → 0, z2/

n, i2 = m2 + z2

√

√

P (I1(s) = i1, I2(s) = i2|K(s) = 0)
n1!n2!

i1

(n1 − i1)!i1!(n2 − i2)!i2!

(i1 + i2)!µi1

(n1 − i1)−(n1−i1)i

−i2
2

(n2 − i2)−(n2−i2)

1 µi2

(cid:18) i1 + i2

(cid:19)i1+i2
2 λ−i1−i2

e

µi1
1 µi2

2 λ−i1−i2

i1 + i2
−i1
1

= B2

∼ B3i

(cid:18)

(cid:19)1/2

,

×

i1

(i1 + i2)i2(n1 − i1)(n2 − i2)

where the use of Stirling’s approximation is justiﬁed for large n. Here B2 = B1/P (K(s) = 0) and
B3 = B2n1!n2!(2π)− 3
We clearly have:

2 en.

(cid:18)

(cid:19)1/2 ∼

(cid:18)

i1

(i1 + i2)i2(n1 − i1)(n2 − i2)

(m1 + m2)m2(n1 − m1)(n2 − m2)

m1

(cid:19)1/2

.

So we can treat that part as a constant. Consider

−i1
1 = (m1 + z1

i

√

√

n)−(m1+z1

−(m1+z1
n) = m
1

√

n)

(cid:18)

1 +

then from the Taylor expansion of the logarithm function, we have

(cid:32)(cid:18)

log

1 +

= −(m1 + z1

n)

n

√

z1
m1
√

(cid:19)−(m1+z1
(cid:18) z1

√

n

m1

n)(cid:33)

√

− z2
1n
2m2
1

√

√

= −(m1 + z1

(cid:18) 1

(cid:19)(cid:19)

+ o

n) log
√

= −z1

n

Therefore,

n) log(m1) − z1
Similar expansions are valid for n1 − i1, i2, n2 − i2 and i1 + i2.

) ≈ −(m1 + z1

−i1
1

log(i

Therefore, we have

√

n − z2
1n
2m1

(cid:19)−(m1+z1

√

n)

,

√

n

z1
m1

(cid:18)

1 +

(cid:19)

√

n

z1
m1

n − z2
1n
2m2
1

+ o(1)

log P (I1(s) = i1, I2(s) = i2|K(s) = 0)
∼ log(B4) + z1

(cid:18) (n1 − m1)µ1
(cid:19)

(cid:18) (n2 − m2)µ2

n log

λm1

√

√

+z2

n log

−

(cid:19)

√

+(z1 + z2)

n log(m1 + m2) +

−

z2
1nn1

2(n1 − m1)m1
z2
2nn2

2(n2 − m2)m2
(z1 + z2)2n
2(m1 + m2)

,

λm2

(cid:17)1/2

.

(cid:16)

where B4 = B3

(m1+m2)m2(n1−m1)(n2−m2)

m1

We now use the calculations in Section 3 to evaluate all the
(n2 − m2)µ2

(n1 − m1)µ1

=

1
λT

,

λm1

λm2

18

√

n coeﬃcients. By (2) we have

=

1
λT

,

Therefore,
√

z1

n log

(cid:18) (n1 − m1)µ1

(cid:19)

λm1

+ z2

We are left with
P (I1(s) = i1, I2(s) = i2|K(s) = 0) ∼ exp(B4) exp

√
+ (z1 + z2)

n log(m1 + m2) = 0

(cid:19)

.

−

z2
2nn2

2(n2 − m2)m2

−

z2
1nn1

2(n1 − m1)m1

(cid:18)

n1

(cid:19)

n2

m1 + m2 = T

+

= λT

T + 1/µ1

T + 1/µ2

√

λm2

n log

(cid:18) (n2 − m2)µ2

(cid:19)
(cid:18) (z1 + z2)2n
(cid:18) (n1 − m1)(n2 − m2)m1m2
(cid:18) (n1 − m1)m1(n2m1 + m2
(cid:18) (n2 − m2)m2(n1m2 + m2

1)(n2m1 + m2
2)

(n1m2 + m2

2 + n2m2
1

2(m1 + m2)

n1m2

2)

1)

n1m2

2 + n2m2
1

(cid:18)

2

2

,

(cid:19) 1
(cid:19) 1
(cid:19) 1
(cid:18) z2

.

,

2

Deﬁne

We have

ρ =

σ1 =

σ2 =

P (I1(s) = i1, I2(s) = i2|K(s) = 0) ∼ exp(B4) exp

−

1

2(1 − ρ2)

1n
σ2
1

) given K(s) = 0 converges in distribution as n → ∞ to the bivariate

(cid:19)(cid:19)

,

+

z2
2n
σ2
2

− 2ρz1z2n
σ1σ2

Therefore, ( I1(s)−m1√
Normal distribution as stated in (5).

, I2(s)−m2√

n

n

When K(s) = k > 0, and n → ∞, similarly,

P (I1(s) = i1, I2(s) = i2|K(s) = k)

= B1

∼ B1αk

(cid:18)n1

(cid:19)(cid:18)n2

(cid:19)

i1

i2
i1 ik
2

(i1 + i2)k+1
√

i1(i1 + i2 − k − 1)!

i2!

(i2 − k)!

µi1
1 µi2

2 λ−i1−i2−kλk

1

n1!n2!

(n1 − i1)!i1!(n2 − i2)!i2!

(i1 + i2)!µi1

1 µi2

√

√

n, with z1/

√
n → 0, z2/
(m1 + m2)k+1 = β(1 − β)k.

m1 mk
2

i1 ik
2

(i1 + i2)k+1 →

(cid:14)P (K(s) = k)
2 λ−i1−i2(cid:14)P (K(s) = k).

n → 0. We then have

(cid:16) I1(s)−m1√

n

(cid:17)

, I2(s)−m2√

n

converge

We can now use the same approximation as for k = 0 to show that
to the same bivariate Normal distribution.

We again write i1 = m1 + z1

n, i2 = m2 + z2

A.3 Proof of Theorem 6
Proof of Theorem 6. Take a ﬁxed arbitrary  ∈ (0, min{f1, f2}). Fix k > 0, for any i1, i2 satisfying
|i1/n − f1| < , |i2/n − f2| <  and i1 ≥ 1, from (4), noting a+c
b for any 0 < a ≤ b and c > 0,
we have
(cid:19) (8)

b+c ≥ a
(cid:19)

≤ (f2 + )n + 1
i1 + (f2 + )n

(f1 − )n + (f2 + )n

π(k, i1, i2)
π(k − 1, i1, i2)

≤ i2 + 1
i1 + i2

i2 − k + 1
1
i1 + i2 − k
α
(f2 + )n + 1

(f2 + )n + 1

1
α
f2

(cid:18)

(cid:18)

<

=

1 +

+


f2

1 − β
α

=

1 +

+


f2

=

(f1 + f2)n

1
α

=

1
α
1
f2n

1
α
1
f2n

(f1 + f2)α

19

π(k, i1, i2) < π(0, i1, i2)

(cid:18) 1 − β

α

Therefore,

For ﬁxed k0 > 0,

(cid:19)k(cid:18)

1 +

P (K(s) ≥ k0, I1(s) = i1, I2(s) = i2) < π(0, i1, i2)

(cid:19)k

.


f2

+

1
f2n

α

(cid:16) 1−β
(cid:17)k0(cid:16)
(cid:17)(cid:16)
1 −(cid:16) 1−β
(cid:17)k0(cid:16)
(cid:16) 1−β
1 −(cid:16) 1−β
(cid:17)(cid:16)

α

α

α

(cid:17)k0
(cid:17) .
(cid:17)k0
(cid:17) .

1 + 
f2

+ 1
f2n

1 + 
f2

+ 1
f2n

1 + 
f2

+ 1
f2n

1 + 
f2

+ 1
f2n

Note the above inequality is valid for any i1, i2 satisfying |i1/n − f1| < , |i2/n − f2| < , we have

P (K(s) ≥ k0,|I1(s)/n − f1| < ,|I2(s) − f2| < )
P (K(s) = 0,|I1(s)/n − f1| < ,|I2(s) − f2| < )

<

From Theorem 4, there exists an N1 such that when n > N1,

P (|I1(s)/n − f1| < ,|I2(s)/n − f2| < ) > 1 − .

Then we have,
P (K(s) ≥ k0) < P ( K(s) ≥ k0||I1(s)/n − f1| ≥ ,|I2(s)/n − f2| ≥ ) P (|I1(s)/n − f1| < ,|I2(s)/n − f2| < )

+ P (|I1(s)/n − f1| ≥ ,|I2(s)/n − f2| ≥ )

(cid:17)k0(cid:16)
(cid:16) 1−β
1 −(cid:16) 1−β
(cid:17)(cid:16)

α

α

1 + 
f2

+ 1
f2n

1 + 
f2

+ 1
f2n

(cid:17)k0
(cid:17) (1 − ) + 

< P ( K(s) = 0||I1(s)/n − f1| ≥ ,|I2(s)/n − f2| ≥ )

(cid:16) 1−β
(cid:17)k0(cid:16)
(cid:17)(cid:16)
1 −(cid:16) 1−β

α

α

<

1 + 
f2

+ 1
f2n

1 + 
f2

+ 1
f2n

(cid:17)k0
(cid:17) (1 − ) + .

This upper bound can be arbitrarily close to 0 when choosing , n > N1 and k0. Therefore, we
have shown the tightness of K(s), that is

∞(cid:88)

k=0

n→∞ P (K(s) = k) = 1.
lim

(9)

Using
P (K(s) = k) = P (K(s) = k,|I1(s)/n − f1| < ,|I2(s)/n − f2| < )+P (K(s) = k,|I1(s)/n − f1| ≥ ,|I2(s)/n − f2| ≥ ) ,
for ﬁxed k > 0, when n > N1, the ratio P (K(s)=k)

P (K(s)=k−1) is lower bounded by
P (K(s) = k,|I1(s)/n − f1| < ,|I2(s)/n − f2| < )

P (K(s) = k − 1,|I1(s)/n − f1| < ,|I2(s)/n − f2| < ) + 

and upper bounded by

P (K(s) = k,|I1(s)/n − f1| < ,|I2(s)/n − f2| < ) + 
P (K(s) = k − 1,|I1(s)/n − f1| < ,|I2(s)/n − f2| < )

.

For any i1, i2 satisfying |i1/n − f1| < , |i2/n − f2| <  and i1 ≥ 1, in addition to (8), we have the
lower bound

π(k, i1, i2)
π(k − 1, i1, i2)

=

i2 − k + 1
i1 + i2 − k

1
α

≥ (f2 − )n − k + 1
i1 + (f2 − )n − k

1
α

(f2 − )n − k + 1

(f1 + )n + (f2 − )n − k

1
α

>

(f2 − )n − k + 1
(f1 + f2)n − k

1
α

.

=

20

(cid:20) (f2 − )n − k + 1

(f1 + f2)n − k

Now we have

π(k, i1, i2)
π(k − 1, i1, i2)

∈

1
α

,

(f2 + )n + 1

(f1 + f2)n

Therefore,(cid:80)|i1/n−f1|<,|i2/n−f2|< π(k, i1, i2)
(cid:80)|i1/n−f1|<,|i2/n−f2|< π(k − 1, i1, i2)

(cid:20) (f2 − )n − k + 1

(f1 + f2)n − k

∈

(cid:21)

.

1
α

(cid:21)

,

1
α

,

(f2 + )n + 1

(f1 + f2)n

1
α

that is,

(cid:20) (f2 − )n − k + 1

(cid:21)

.

P (K(s) = k,|I1(s)/n − f1| < ,|I2(s)/n − f2| < )
1
P (K(s) = k − 1,|I1(s)/n − f1| < ,|I2(s)/n − f2| < )
α
(10)
For ﬁxed k, as n → ∞, the lower bound and the upper bound in (10) both converge to 1−β
α .
Noting  can be arbitrarily close to 0, we have

(f1 + f2)n − k

(f2 + )n + 1

(f1 + f2)n

1
α

∈

,

lim
n→∞

P (K(s) = k)
P (K(s) = k − 1)

=

1 − α

β

This together with the tightness (9) proves (6).

References

[1] Adan, I., Boon, M., and Weiss, G., (2014). A design heuristic for skill based parallel service

systems. Preprint.

[2] Adan, I., Busic, A., Mairesse, J., Weiss, G. (2015). Reversibility and further properties of

FCFS inﬁnite bipartite matching. arXiv preprint arXiv:1507.05939.

[3] Adan, I., Foley, R., McDonald, D. (2009). Exact Asymptotics of the Stationary Distribution

of a Markov Chain: a Production Model. Queueing Systems, 62(4): 311-344.

[4] Adan, I. J. B. F., Weiss, G. (2012). Exact FCFS matching rates for two inﬁnite multi-type

sequences. Operations Research, 60(2), 475-489.

[5] Adan, I.J.B.F., Weiss, G. (2014). A queue with skill based service under FCFS-ALIS: steady
state, overloaded system, and behavior under abandonments. Stochastic Systems, 4(1):250-
299.

[6] Armony, M., Ward, A. R. (2010). Fair dynamic routing in large-scale heterogeneous-server

systems. Operations Research, 58(3), 624-637.

[7] Bell, S. L., Williams, R. J. (2001). Dynamic scheduling of a system with two parallel servers in
heavy traﬃc with resource pooling: asymptotic optimality of a threshold policy. The Annals
of Applied Probability, 11(3), 608-649.

[8] Busic, A., Gupta, V., Mairesse, J. (2013). Stability of the bipartite matching model. Advances

in Applied Probability, 45(2), 351-378.

[9] Foss, S., Chernova, N. (1998). On the stability of a partially accessible multi-station queue

with state-dependent routing. Queueing Systems, 29(1), 55-73.

[10] Feller, W. (1968). Introduction to Probability Theory and its Applications, 3rd Edition, Wiley.

[11] Ghamami, S., Ward, A. R. (2013). Dynamic scheduling of a two-server parallel server system
with complete resource pooling and reneging in heavy traﬃc: Asymptotic optimality of a
two-threshold policy. Mathematics of Operations Research, 38(4), 761-824.

21

[12] Green, L. (1985) A queueing system with genera-use and limited-use servers, Operations

Research 33:162–182.

[13] Gurvich, I., Whitt, W. (2009). Queue-and-idleness-ratio controls in many-server service

systems. Mathematics of Operations Research, 34(2), 363-396.

[14] Gurvich, I., Whitt, W. (2010). Service-Level Diﬀerentiation in Many-Server Service Sys-

tem Via Queue-Ratio Routing. Operations Research, 58(2), 316-328.

[15] Harchol-Balter, M., Crovella, M. E., Murta, C. D. (1999). On choosing a task assignment
policy for a distributed server system. Journal of Parallel and Distributed Computing, 59(2),
204-228.

[16] Harrison, J. M., Lopez, M. J. (1999). Heavy traﬃc resource pooling in parallel-server systems.

Queueing systems, 33(4), 339-368.

[17] Pinsky, Mark. The normal approximation to the hypergeometric distribution. Un-
https://www.dartmouth.edu/ chance/teaching_aids/books_-

published manusript,
articles/probability_book/pinsky-hypergeometric.pdf

[18] Rubino, M., Ata, B. (2009). Dynamic control of a make-to-order, parallel-server system with

cancellations. Operations Research, 57(1), 94-108.

[19] Tezcan, T., Dai, J. G. (2010). Dynamic control of N-systems with many servers: Asymptotic

optimality of a static priority policy in heavy traﬃc. Operations Research, 58(1), 94-110.

[20] Visschers, J., Adan, I. J. B. F., Weiss, G. (2012). A product form solution to a system with

multi-type customers and multi-type servers. Queueing Systems, 70(3), 269-298.

[21] Ward, A. R, Armony, M. (2013). Blind fair routing in large-scale service systems with het-

erogeneous customers and servers. Operations Research, 61(1), 228-243.

[22] Williams, R. J. (2000). On dynamic scheduling of a parallel server system with complete

resource pooling. Fields Institute Communications, 28, 49-71.

22

