Hard-Clustering with Gaussian Mixture Models

Johannes Bl¨omer, Sascha Brauer, and Kathrin Bujna

Department of Computer Science

Paderborn University

33102 Paderborn, Germany

March 22, 2016

Training the parameters of statistical models to describe a given data set is a
central task in the ﬁeld of data mining and machine learning. A very popular
and powerful way of parameter estimation is the method of maximum likelihood
estimation (MLE). Among the most widely used families of statistical models are
mixture models, especially, mixtures of Gaussian distributions.

A popular hard-clustering variant of the MLE problem is the so-called complete-
data maximum likelihood estimation (CMLE) method. The standard approach to
solve the CMLE problem is the Classiﬁcation-Expectation-Maximization (CEM)
algorithm [CG92]. Unfortunately, it is only guaranteed that the algorithm converges
to some (possibly arbitrarily poor) stationary point of the objective function.

In this paper, we present two algorithms for a restricted version of the CMLE
problem. That is, our algorithms approximate reasonable solutions to the CMLE
problem which satisfy certain natural properties. Moreover, they compute solutions
whose cost (i.e. complete-data log-likelihood values) are at most a factor (1 + ε)
worse than the cost of the solutions that we search for. Note the CMLE problem
in its most general, i.e. unrestricted, form is not well deﬁned and allows for trivial
optimal solutions that can be thought of as degenerated solutions.

6
1
0
2

 
r
a

 

M
1
2

 
 
]

G
L
.
s
c
[
 
 

1
v
8
7
4
6
0

.

3
0
6
1
:
v
i
X
r
a

1

1 Preliminaries

Given set of observations, the objective of the CMLE problem is to ﬁnd a Gaussian mixture
model and a hard clustering with maximum complete-data likelihood.
In this section, we
will ﬁrst describe and deﬁne this objective function. Then, we will present an alternating
optimization scheme for this problem. However, the problem is not well-deﬁned. Hence, we will
restrict the problem to reasonable instances and solutions.

1.1 Complete-Data Log-Likelihood

Let X ⊂ R
the likelihood that all x ∈ X have been drawn according to Nd(µ, σ) is given by

d be a ﬁnite set of observations. Given a spherical Gaussian distribution Nd(µ, σ),

assuming that the observations have been drawn independently at random.

Nd(x|µ, σ) ,

Yx∈X

Deﬁnition 1. Given a ﬁnite set X ⊂ R
µ ∈ R

d and variance σ2 ∈ R, let

d and a spherical Gaussian distribution with mean

LX(µ, σ2) := − ln Yx∈X

p(x|µ, σ2)! =

|X|d

2

ln(2πσ2

k) +

kx − µkk2 .

1
2σ2

k Xx∈X

We denote the minimal value by OP T (X, 1) = min(µ,σ2) LX(µ, σ2).

Now consider a Gaussian mixture model (GMM) given by parameters θ = {(wk, µk, σ2

k)}K

k=1.

Drawing an observation xn according to a GMM corresponds to a two-step process:

1. Draw a component zn ∈ [K] with probability p(zn = k|θ) = wk.

2. Draw an observation xn ∈ X according to Nd(µzn, σzn).

Note that the assignment zn ∈ [K] is a (latent) random variable in this two-step process. With
the help of this random variable, we can compute the likelihood that observation x ∈ X has
been generated by the k-th component of the GMM, i.e.

p(xn, zn = k|θ) = p(zn = k|θ) · p(xn|zn = k, θ) = wk · Nd(x|µk, σk) .

complete-data likelihood, while p(xn|θ) =PK

Since xn and zn completely describe the two-step process, the likelihood p(xn, zn|θ) is also called
zn=1 p(xn, zn|θ) is refered to as (marginal) likelihood.
n=1. Then,
the likelihood that all observations have been drawn according to a GMM θ and that each xn
has been generated by the zn-th component, is given by

Assume, we are given a set of observations X = {xn}N

n=1 and assignments {zn}N

N

N

p(xn, zn|θ) =

wzn · Nd(xn|µzn, σzn) ,

(1)

Yn=1

Yn=1

assuming that the observations have been drawn inpendently at random. Note that the assign-
ments {zn}N
k=1Ck via xn ∈ Ck iﬀ zn = k. Hence, we can also rewrite
Equation (1) as

n=1 deﬁne a partition C = ˙∪K

K

Yk=1 Yxn∈Ck

p(xn, zn = k|θ) =

K

Yk=1 Yxn∈Ck

2

wk · Nd(xn|µk, σk) .

By taking (negative) logarithm of this expression, we obtain

p(xn, zn = k|θ)


(ln(wk) + ln (Nd(xn|µk, Σk))

K

=

− log
Yk=1 Yxn∈Ck

Xk=1 Xxn∈Ck
Xk=1

LCk (µk, σ2

=

K

K

k) − ln(wk) · |Ck| .

Deﬁnition 2. Given a ﬁnite set X ⊂ R
of spherical Gaussians with parameters θ = {(wk, µk, σ2

k)}K

k=1, we call

d, a partition C = {C1, . . . , CK} of X, and a mixture

LX(θ, C) :=

K

Xk=1

LCk (µk, σ2

k) − ln(wk) · |Ck|

the complete-data negative log-likelihood.

Note that a solution maximizing the complete-data likelihood also minimizes the complete-
data negative log-likelihood, and vice versa. Therefore, we deﬁne the complete-cata maximum
likelihood estimation (CMLE) problem as follows.

Problem 3 (CMLE). Given a ﬁnite set X ⊂ R
{C1, . . . , CK } of X and a mixture of spherical Gaussians with parameters θ = {(wk, µk, σ2
minimizing LX(θ, C). We denote the minimal value by OP T (X, K).

d and an integer K ∈ , ﬁnd a partition C =

k)}K

k=1

For a ﬁxed model θ, we let LX(θ) = minC LX(θ, C). Analogously, for a ﬁxed clustering C, we

let LX(C) = minθ LX(θ, C).

Deﬁnition 4. Given parameters (wk, µk, σ2

k) and a cluster Ck ⊆ X, we let

Lx(wk, µk, σ2

k) :=

d
2

ln(2πσ2

k) +

1
2σ2
k

kx − µkk2 − ln(wk),

and

LCk (wk, µk, σ2

k) := Xx∈Ck

Lx(wk, µk, σ2

k) .

Remark 5. For all partitions C = {C1, . . . , CK }, we have

LX(C) =

K

Xk=1

OP T (Ck, 1) − ln(cid:18) |Ck|

|X|(cid:19) · |Ck| .

For all θ = {(w1, µ1, σ2

1), . . . , (wK , µK , σ2

K )}, we have

LX(θ) =

N

Xn=1

argmink∈[K]{Lx(wk, µk, σ2

k)} .

3

1.2 Alternating Optimization Scheme (CEM algorithm)

An alternating optimization algorithm for this problem is given by the following ﬁrst order
optimality conditions. Fixing the partition C = {Ck}K
k=1, the optimal mixture of spherical
Gaussians is given by θ = {(wk, µk, σ2

k)}K

k=1 with

wk =

|Ck|
|X|

,

µk =

1

|Ck| Xxn∈Ck

xn ,

σ2
k =

1

d|Ck| Xxn∈Ck

kxn − µkk2 .

Fixing the Gaussian mixture model θ = {(wk, µk, σ2
is given by assigning each point to its most likely component, i.e.

k)}K

k=1, the optimal partition C = {Ck}K

k=1

xn ∈ Ck ⇔ k = argmaxl∈[K] p(zn = l|xn, θ) ,

where

p(zn = k|xn, θ) =

wkN (xn|µk, σ2
k)
l=1 wlN (xn|µl, σ2
l )

,

PK

which is the posterior probability that xn has been generated by the k-th component of the
given mixture.

If we repeatedly compute these update formulas, the solution converges to a local extremum

or a saddlepoint of the likelihood function.

A proof of the correctenss of these update formulas (which we omit here) uses the following

lemma.

Lemma 6. Let X ⊂ R

d be a ﬁnite set. Deﬁne

Then, for all y ∈ R

d

µ(X) =

1

|X| Xx∈X

x .

Xx∈X

kx − yk2 = Xx∈X

kx − µ(X)k2 + |X| · ky − µ(X)k2 .

In particular, µ(X) = argminy∈R

dPx∈Xkx − yk2.

Note that an optimal CMLE solution is not changed by this algorithm. Hence, an optimal
CMLE solution is completely deﬁned by a partition or a Gaussian mixture model. Similarly, if
we refer to a partition or a Gaussian mixture as a CMLE solution we assume that the missing
parameters are as deﬁned by the update formulas given above, respectively.

1.3 Well-Deﬁned Instances

Unfortunately, the CMLE problem is not well deﬁned in this form. For example, you could
choose C1 = {x} and µ1 = x for some x ∈ X. Then, as σ1 → 0 we get that LK(X) → −∞.
Consequently, we impose the following restrictions on instances.

Deﬁnition 7. We call X = ˙S

1. for all k ∈ [K] :

|Ck| ≥ 2.

K
k=1Ck a well-deﬁned partition if

We call X itself a well-deﬁned instance if

4

2. ∀x, y ∈ X, x 6= y : kx − yk2 ≥ 4d
π .

We denote X = ˙S

is a well-deﬁned partition.

K
k=1Ck as a well-deﬁned solution if X is a well-deﬁned instance and {Ck}K

k=1

In the following, we prove that, with these restrictions, the CMLE problem is well deﬁned.
That is, the minimum in Problem 3 is well deﬁned (LK (X) > −∞). Moreover, we will see
(Lemma 9) that for the optimal solution we have σ2

k ≥ 1

2π or

2πσ2

k ≥ 1

for k ∈ [K].

(2)

First of all, note that the sum of squared distances between the points in X and the mean

µ(X) can be rewritten using pairwise distances (which are lower bounded in Restriction 2).

Lemma 8. Let X ⊂ R

d be a ﬁnite set and µ(X) := 1

|X|Px∈X its mean, then

kx − yk2.

kx − µ(X)k2 =

Xx∈X

2|X| Xx∈X Xy∈X

1

Proof.

Xx∈X Xy∈X

hx − y, x − yi

(hx, xi + hy, yi − 2 hx, yi

kx − yk2 = Xx∈X Xy∈X
= Xx∈X Xy∈X
= 2|X|Xx∈X
= 2|X|Xx∈X
= 2|X|Xx∈X
= 2|X|Xx∈X
= 2|X|Xx∈X

hx, yi

hx, µ(X)i

hx, xi − 2Xx∈X Xy∈X
hx, xi − 2|X|Xx∈X

hx, x − µ(X)i

hx − µ(X), x − µ(X)i

kx − µ(X)k2.

(using |X|Px∈X hµ(X), x − µ(X)i = 0)

Now using the restriction on the minimum pairwise diﬀerence between points (Restriction 2)
and on the minimum number of points (Restriction 1) in a cluster, we can lower bound the
variance of each cluster. This directly yields Equation (2) and our claim that the problem is
well-deﬁned under the restrictions given in Deﬁnition 7.

Lemma 9. Let Y be a subset of a set X that satisﬁes Restriction 2 from Deﬁnition 7 and that
contains at least two diﬀerent elements. Then,

σ(Y )2 =

1

|Y |d Xy∈Y

ky − µ(Y )k2 ≥

1
2π

.

5

Proof.

σ(Y )2 =

=

≥

≥

≥

1
8d
1
2π

ky − µ(Y )k2

1

1

|Y |d Xy∈Y
2|Y |2d Xx∈Y Xy∈Y
2|Y |2d(cid:18)|Y |

1

2 (cid:19) min

x,y∈Y,x6=y

kx − yk2

kx − yk2

(using Lemma 8)

min

x,y∈Y,x6=y

kx − yk2

(using Restriction 2)

Throughout the rest of this paper, we will restrict the search space of CMLE to well-deﬁned
solutions. In particular, we only consider the optimal solution among all well-deﬁned solutions.

1.4 Well-Balanced Instances

A central idea behind the algorithms that we present in this paper is that we do not allow
somewhat degenerate instances. This means that we can ﬁnd a function f in the number of
clusters that can be used to lower bound the number of points in a cluster and a function g
that can be used to lower bound the costs OP T (Ck, 1) of optimal clusters Ck.

Deﬁnition 10 (well-balanced). Let f, g :  → R. We denote a partition X = ˙S

f -balanced if for all k ∈ [K]

|Ck| ≥

|X|
f (K)

.

K
k=1Ck as

Furthermore, we denote the partition as an (f, g)-balanced CMLE solution if it is f -balanced

and additionally for all k ∈ [K]

OP T (Ck, 1) ≥

1

g(K)

·

OP T (Ck, 1) .

K

Xk=1

Deﬁnition 11. Given a ﬁnite set X ⊂ R

d and K ∈ , we let

OP Tdiam(X, K) = min

{C1,...,CK },
˙∪K
k=1Ck=X

max
k∈[K]

max
x,y∈Ck

kx − yk .

Lemma 12 (From f -balanced to (f, g)-balanced). An f -balanced solution X = ˙S

an (f, Γ · f )-balanced CMLE solution, where Γ ≤ 2 · ln (32π · OP Tdiam(X, K)) + ln(K) + 1.
Proof.

K
k=1Ck is also

(due to Lemma 20 and f balanced)

(due to Lem. 21)

≥

1

|X|d

f (K)

2

OP T (Ck, 1) ≥

≥

≥

|Ck|d

2

1

f (K) · Γ

1

f (K) · Γ

LK(X)

OP T (Ck, 1) .

K

Xk=1

6

2 Main Results (Theorems 13 and 15)

d, K ∈  and δ, ε ∈ [0, 1]. If X has an (f, g)-balanced optimal CMLE
Theorem 13. Let X ⊂ R
solution, then there exists an algorithm which computes a mixture of K spherical Gaussians
θ = {(wk, µk, σ2

k=1, such that

k)}K

P r [LX(θ) ≤ (1 + ε)OP T (X, K)] ≥ 1 − δ .

The runtime of the algorithm is bounded by

|X| · K · log(Γ) · log(g(K)) · 2

˜O(cid:16) f (K)
εδ (cid:17)

where Γ ≤ 2 · ln (32π · OP Tdiam(X, K)) + ln(K) + 1.

d, K ∈  and δ, ε ∈ [0, 1]. If X has an f -balanced optimal CMLE
Corollary 14. Let X ⊂ R
solution, then there exists an algorithm which computes a mixture of K spherical Gaussians θ,
such that

P r [LX(θ) ≤ (1 + ε)OP T (X, K)] ≥ 1 − δ .

The runtime of the algorithm is bounded by

|X| · K · log(Γ)2 · 2

εδ (cid:17)
˜O(cid:16) f (K)

where Γ ≤ 2 · ln (32π · OP Tdiam(X, K)) + ln(K) + 1.

K
Theorem 15. Let X ⊂ R
k=1Ck be a well-deﬁned solution for
the CMLE problem. There is an algorithm that computes a mixture of K spherical Gaussians
θ, such that

d, K ∈ , and δ, ε > 0. Let C = ˙S

Pr [LX(θ) ≤ (1 + ε)LX(C)] ≥ 1 − δ .

The running time of the algorithm is bounded by

ε2 (cid:17)(cid:17) (cid:0)log(log(∆2)) + 1(cid:1)K

(log(f (K)))K ,

|X| d log(cid:18) 1

δ(cid:19) 2

O(cid:16) K

ε ·log(cid:16) K

where ∆2 = maxx,y∈X{kx − yk2}.

3 Proof of Theorem 13

In the following we prove Theorem 13.

• In Section 3.1 we show that, if the parameters of a CMLE solution are suﬃcently close to
those of an optimal CMLE solution, then its complete-data log-likelihood is close to that
of the optimal CMLE solution. In Sections 3.2 and 3.3 we then show how to obtain such
parameter estimates.

• In Section 3.2 we deal with the problem of estimating the means. We use the superset
sampling technique introduced by [IKI94] to compute a set of candidate means which
contains a good candidate, i.e. a good estimation to the mean parameters of an optimal
solution.

7

• In Section 3.3 we use a grid search to obtain estimates of the weights and variances. The
core idea is to simply test all solutions lying on a speciﬁc grid in the search space. By
choosing a grid that is dense enough, we ensure that there are solutions on the grid which
are suﬃciently close to the parameters that we search for.

3.1 Estimate the Costs of Parameter Estimates

For an optimal (f, g)-balanced CMLE solutions, we can estimate the parameters of the the
respective optimal Gaussian mixture model and the likelihood of the optimal clusters. We can
show that the CMLE solution determined by these parameter estimates yields an approximation
with respect to the complete data log-likelihood.

Theorem 16. Let X ⊂ R

d, K ∈  and ε > 0. Assume X has an f -balanced optimal CMLE

solution X = ˙S

K
k=1Ck and let (˜µ1, . . . , ˜µK) such that for all k ∈ [K]

k˜µk − µ(Ck)k2 ≤

ε

|Ck| Xx∈Ck

kx − µ(Ck)k2 .

Let (n1, . . . , nK), such that for all k ∈ [K]

|Ck| ≤ nk ≤ (1 + ε)|Ck| .

K, such that for all k ∈ [K] it holds

˜σ2
k ≥ σ2
k

and ~˜σ = (˜σ2

1, . . . , ˜σ2

K) ∈ R

and

ln(˜σ2

k) − ln(σ2

k) ≤(cid:0)(1 + ε)2 − 1(cid:1)

2

|Ck|d

OP T (Ck, 1) .

Deﬁne ˜θ = {( ˜wk, ˜µk, ˜σ2

k)}k=1,...,K, where ˜wk = nk
PK

l=1 nl

. Then,

(3)

(4)

(5)

LX(˜θ) ≤ (1 + ε)4OP T (X, K).

Proof. Using that |Cl| ≤ nl ≤ (1 + ε)|Cl| for all l = 1, . . . , K, we obtain ˜wk ≥ 1

(1+ε) · |Ck|

|X| . Hence,

− ln( ˜wk) · |Ck| ≤ − ln(cid:18) 1

·

|Ck|

(1 + ε)

|X|(cid:19) |Ck|
≤ ln(1 + ε)|Ck| − ln(cid:18) |Ck|
≤ ε|Ck| − ln(cid:18) |Ck|
|X|(cid:19) · |Ck|
OP T (Ck, 1) − ln(cid:18) |Ck|

2ε
d

≤

|X|(cid:19) · |Ck|

|X|(cid:19) · |Ck|

(by Equation (3))

(since ln(1 + ε) ≤ ε)

(since OP T (Ck, 1) ≥ |Ck|·d

2

)

8

ln(2π˜σ2

k) +

kx − µkk2 (By Lemma 6 and property of ˜µk)

|Ck|d

(By def. of µk)

2
2

|Ck|d

OP T (Ck, 1) + ln(σ2

k)(cid:19) + (1 + ε)

|Ck|d

2

Furthermore, observe that

LCk (˜µk, ˜σk) =

|Ck|d

2

ln(2π˜σ2

k) +

(4)
≤

|Ck|d

2

ln(2π˜σ2

k) +

kx − ˜µkk2

1
2˜σ2

k Xx∈Ck
k Xx∈Ck
(1 + ε) Xx∈Ck

1
2σ2

1
2σ2
k

kx − ˜µkk2

|Ck|d

2

|Ck|d

2

|Ck|d

2

≤

=

=

(5)
=

=

ln(2π˜σ2

k) + (1 + ε)

|Ck|d

2

(ln(2π) + ln(˜σ2

k)) + (1 + ε)

|Ck|d

2 (cid:18)ln(2π) +(cid:0)(1 + ε)2 − 1(cid:1)

|Ck|d

|Ck|d

ln(2πσ2

k) + (1 + ε)

2

2

+(cid:0)(1 + ε)2 − 1(cid:1) OP T (Ck, 1)

≤ (1 + ε)OP T (Ck, 1) +(cid:0)(1 + ε)2 − 1(cid:1) OP T (Ck, 1)
≤(cid:0)(1 + ε)2 + ε(cid:1) OP T (Ck, 1)

≤ (1 + ε)3OP T (Ck, 1)

Overall, we have

K

LX(˜θ) =

|X|(cid:19) · |Ck|

≤

=

K

K

2ε
d

LCk (µk, σ2

k) − ln(wk) · |Ck|

(1 + ε)3OP T (Ck, 1) +

Xk=1
Xk=1
Xk=1(cid:18)(1 + ε)3 +
d (cid:19) OP T (Ck, 1) − ln(cid:18) |Ck|
d (cid:19) K
OP T (Ck, 1) − ln(cid:18) |Ck|
≤(cid:18)(1 + ε)3 +
Xk=1
=(cid:18)(1 + ε)3 +
d (cid:19) OP T (X, K)

OP T (Ck, 1) − ln(cid:18)|Ck|
|X|(cid:19) · |Ck|
|X|(cid:19) · |Ck|

2ε

2ε

2ε

≤ (1 + ε)4OP T (X, K)

3.2 Generate Candidate Means by Sampling

We reuse the following well-known lemma on superset sampling.

Lemma 17 (superset-sampling). Let X ⊂ R
α|X|. Let S ⊆ X be a uniform sample multiset of size at least

d be a ﬁnite set, α < 1 and X ′ ⊂ X with |X ′| ≥
2
αεδ . Then with probability at

9

least 1−δ
5

there is a subset S′ ⊆ S with |S′| = 1

εδ such that

kµ(S′) − µ(X ′)k2 ≤

kx − µ(X ′)k2.

ε

|X ′| Xx∈X ′

If we plug our notion of f -balanced solutions into this lemma, then we receive an algorithm

that samples good approximative means.

Theorem 18 (sampling means). For a ﬁnite set X ⊂ R

is an f -balanced partition, then there is an algorithm that computes a set of log(1/δ)·2
K-tuples of points from R
for all k ∈ [K]

K
k=1Ck
εδ ·log(cid:16) f (K)
εδ (cid:17)
d, such that with probability 1 − δ for one of these tuples it holds that

d, K ∈  and ε, δ > 0, if X = ˙S

K

kµk − µ(Ck)k2 ≤

kx − µ(Ck)k2 .

ε

|Ck| Xx∈Ck

The runtime of the algorithm is bounded by log(1/δ) · K ·(cid:18)|X| + 2

K

εδ ·log(cid:16) f (K)

εδ (cid:17)(cid:19).

Proof. Consider the following algorithm, which computes a candidate set of tuples of means.

Algorithm 1: Approx-Means(X, K)

d : input points

Input: X ⊂ R
K ∈  : number of clusters
Output: set of candidate tuples of means
P ← ∅;
for k = 1, . . . , K do

sample a multiset S of size 1

αεδ from X;

T ←(cid:8)µ(S′)|S′ ⊂ S, |S′| = ⌈ 1

P ← P × T ;

εδ ⌉(cid:9);

end

return P ;

Using Lemma 17 with α = 1

f (K) , we know that the output of a single run of Approx-Means

5 (cid:1)K
contains a tuple with the desired property with probability (cid:0) 1−δ

We know that

.

αεδ(cid:19)
|T | ≤(cid:18) 1

1
εδ

,

thus

The runtime is bounded by

|P | = |T |K ≤ 2

K

εδ ·log(cid:16) f (K)

εδ (cid:17).

K · |X| +

K

|T |k ≤ K(cid:18)|X| + 2
Xk=1

K

εδ ·log(cid:16) f (K)

εδ (cid:17)(cid:19) .

By executing Approx-Means log(1/δ) times we receive the desired success probability.

10

3.3 Generate Candidate Cluster Sizes and Variances by Using Grids

So far, we have formulated an algorithm that gives us good means. In the following, we will use
the gridding technique to determine a set of candidates for the the cluster sizes and variances.
First of all, we generate a set of cluster sizes that contains good approximations of the cluster
sizes of any f -balanced solutions. Then, we approximate the negative log-likelihood of optimal
k=1 OP T (Ck, 1) where the Ck are the optimal CMLE clusters. Then,
we present how to construct a candidate set of variances that contains good estimates of the
variances of any (f, g)-balanced optimal CMLE solution.

CMLE clusters, i.e. PK

3.3.1 Grid Search for Cluster Sizes

Theorem 19. Let X ⊂ R

there exists an algorithm that outputs a set S ⊆ 
(n1, . . . , nK) ∈ S such that

d, K ∈  and let X = ˙S

K
k=1Ck be an f -balanced partition. Then
, that contains a tuple

log(1+ε) (cid:17)K
K, |S| =(cid:16) log(f (K))

|Ck| ≤ nk ≤ (1 + ε)|Ck|.

(6)

for all k ∈ [K].

Proof. Since we assume a f -balanced solution, we know that for all k ∈ [K]

|X|
f (K)

≤ |Ck| ≤ |X|.

Thus, there exist a value i∗ ∈ {1, . . . , ⌈log1+ε(f (K))⌉} such that

(1 + ε)i∗−1 |X|
f (K)

≤ |Ck| ≤ (1 + ε)i∗ |X|
f (K)

.

Thus, we receive ⌈log1+ε(f (K))⌉ many values for each cluster size nk. The algorithm outputs
all possible combinations of these values.

3.3.2 Bounds on the Log-Likelihood of optimal CMLE clusters

Lemma 9 provides us with a lower bound on the negative log-likelihood of a cluster.

Corollary 20 (Lower Bound on the Optimal Log-Likelihood). Let X = ˙S

CMLE solution. Then, OP T (Ck, 1) ≥ |Ck|d

.

2

K
k=1Ck be an optimal

The next step is to ﬁnd an upper bound on the optimal complete-data likelihood value. We
use Gonzales algorithm to compute a value that gives us a tighter bound than just the maximum
spread (over the dimensions of the vectors in the data set).

d and
Lemma 21 (Upper Bound on the Optimal Complete-Data Log-Likelihood). Let X ⊂ R
K ∈ . A Value Γ can be computed in time O(K · d · |X|) such that the complete-data likelihood
of an optimal CMLE solution can be bounded by

and Γ = ln(2πs2) + 1 + ln(K) for some s ≤ 4 · OP Tdiam(X).

OP T (X, K) ≤

|X|d

2

· Γ

11

Proof. Run Gonzales algorithm. The output is a set of K points p1, . . . , pK ∈ X. Compute the
point z with maximum distance to its closest point in {p1, . . . , pK } and set s := mink=1,...,Kkz −
pkk. Consider the solution where the pk are the centers. Partition the points into point sets
C = {C1, . . . , CK }, with kx − pkk = mini=1,...,Kkx − pik for all x ∈ Ck. Notice that the distances
between any point and its center is at most s. Thus, when computing the optimal variance in

each cluster, it is at most s2. Then, for θ =(cid:8)(cid:0) 1

K , pk, σ(Xk, pk)(cid:1)(cid:9)K

k=1 we have

OP T (X, K) ≤ LX(θ, C) =

ln(2πσ(Ck, pk)2) +

− ln(wk) · |Ck|

|Ck|d

2

|Ck|d

2

K

Xk=1
≤  K
Xk=1

|X|d

=

≤

2

|X|d

2

|Ck|d

2

ln(2πs2) +

|Ck|d

2 ! − ln(cid:18) 1

K(cid:19) · |X|

ln(2πs2) +

|X|d

2

+ ln(K) · |X|

(cid:0)ln(2πs2) + 1 + ln(K)(cid:1)

Given two bounds, we can ﬁnd a constant factor approximation of the the sum of the negative

log-likelihoods of optimal CMLE clusters, i.e. PK

d, K ∈ , and ε > 0. Let
k=1Ck be an optimal CMLE solution. Then, there exists a set of log(3Γ/d)/ log(1 + ε)

Lemma 22 (Estimating the Optimal Log-Likelihood). Let X ⊂ R
X = ˙∪K
many values which contains a value Nest with

k=1 OP T (Ck, 1), using a grid search.

1

1 + ε

Nest ≤

K

Xk=1

OP T (Ck, 1) ≤ Nest .

Proof. Combining Corollary 20 and Lemma 21, we know that

|X|d

2

≤

K

Xk=1

OP T (Ck, 1) ≤ OP T (X, K) ≤

|X|d

2

Γ.

Thus, there exist a value i∗ ∈ {1, . . . , ⌈log1+ε(Γ)⌉} such that

(1 + ε)i∗−1 |X|d
2

≤

K

Xk=1

OP T (Ck, 1) ≤ (1 + ε)i∗ |X|d
2

.

The algorithm outputs all ⌈log1+ε(Γ)⌉ values.

Given this approximation of the sum of the negative log-likelihoods, we will be able to ﬁnd
an approximation of the negative log-likelihoods of a single cluster as we will see in the next
section.

3.3.3 Grid Search for Variances

Given the approximations of the size of the clusters and their negative log-likelihod, we are now
able to ﬁnd estimates of the variances.

12

Theorem 23. Let X ⊂ R

d, K ∈  and ε > 0. Assume X has an (f, g)-balanced CMLE

solution X = ˙S

K
k=1Ck. Let additionally Nest ∈ R, with

1

1 + ε

Nest ≤

K

Xk=1

OP T (Ck, 1) ≤ Nest,

and (n1, . . . , nK), such that for all k ∈ [K]

|Ck| ≤ nk ≤ (1 + ε)|Ck|.

(7)

(8)

Then there exists an algorithm that computes a set of size K · log(g(K))
(˜σ2

K), such that for all k ∈ [K] it holds

1, . . . , ˜σ2

log(1+ε) , that contains a tuple

and

k ≥ σ2
˜σ2
k

ln(˜σ2

k) − ln(σ2

k) ≤(cid:0)(1 + ε)2 − 1(cid:1)

2

|Ck|d

OP T (Ck, 1) .

(9)

(10)

Proof. Observe that

1

g(K)(1 + ε)

Nest ≤

1

g(K)

K

Xk=1

OP T (Ck, 1)

Def. 10

≤ OP T (Ck, 1) ≤

OP T (Ck, 1) ≤ Nest.

K

Xk=1

Denote the upper bound by ˆN := (1 + ε)j ∗Nest and set ˜σ2

(1 + ε)j ∗−1Nest ≤ OP T (Ck, 1) ≤ (1 + ε)j ∗

Thus, there exists a value j∗ ∈(cid:8)⌈− log1+ε(g(K))⌉, . . . , 0(cid:9) which satisﬁes
k := exp(cid:16) 2(1+ε)
(cid:0)ln(2πσ2
k + 1)(cid:1)

OP T (Ck, 1) = LCk (µk, σ2

Notice that

Nest .

|Ck|d

k) =

2

OP T (Ck, 1) − ln(2π) − 1

⇔ ln(σ2

k) =

2

|Ck|d

nkd

ˆN − ln(2π) − 1(cid:17).

Thus,

and

ln(˜σ2

k) =

2(1 + ε)

nkd

ˆN − ln(2π) − 1 ≥

2

|Ck|d

OP T (Ck, 1) − ln(2π) − 1 = ln(σ2
k)

ln(˜σ2

k) − ln(σ2

k) =

2(1 + ε)

nkd

ˆN −

2

|Ck|d

OP T (Ck, 1)

≤

2(1 + ε)2

|Ck|d

OP T (Ck, 1) −

2

|Ck|d

OP T (Ck, 1)

=(cid:0)(1 + ε)2 − 1(cid:1)

2

|Ck|d

OP T (Ck, 1)

13

4 Proof of Theorem 15

In the following we present the proof of Theorem 15.

• In Section 4.1 we show how to estimate the variances and the cluster sizes of a well-deﬁned
CMLE solution via gridding. The idea behind a grid search is simply to test all solutions
lying on a grid in the search space. By choosing a grid that is dense enough, we ensure
that there are solutions on the grid which are suﬃciently close to the parameters that we
search for.

• In Section 4.2, we show how one can ﬁnd good estimates of the means when given good
estimates of the weights and covariances. To this end, we adapt the sample-and-prune
technique presented in [ABS10].

4.1 Generate Candidates for Variances and Weights

Lemma 24. Let X ⊂ R
k}K
sponding variances {σ2

d, and {Ck}K
k=1 be a well-deﬁned CMLE solution for X, with corre-
k=1. Then, there exists an algorithm which outputs a set of at most

(cid:16) log(log(∆2))+1

log(1+ε)

(cid:17)K

tuples of variances, which contains a tuple (˜σ2

k)K

k=1, such that

∀k ∈ [K] : σ2

k ≤ ˜σ2

k ≤ (σ2

k)(1+ε) ,

where ∆2 = maxx,y∈X{kx − yk2}.
Proof. We know that optimal variances σ2
by

∀k ∈ [K] :

1
2π

≤ σ2
k.

k of a well-deﬁned solution are bounded from below

Furthermore, we know that these are also bounded from above by

∀k ∈ [K] : σ2

k =

1

|Ck|d Xx∈Ck

kx − µ(Ck)k2 ≤

1

|Ck|d Xx∈Ck

∆2 ≤ ∆2 .

Because 1/(2π) ≤ σ2

k ≤ ∆2, there exists a value

such that

k∗ ∈ {1, . . . , log1+ε(− log1/(2π)(∆2))}

(1/(2π))(1+ε)k∗

−1

≤ σ2

i ≤ (1/(2π))(1+ε)k∗

.

Thus, we receive l log(log(∆2))−log(log(2π))

all possible combinations of these values.

log(1+ε)

m many values for each variance. The algorithm outputs

The following result is the same as in Section 3.3.

Theorem 25. Let X ⊂ R

exists an algorithm that outputs a set S ⊆ 
S such that

d, K ∈  and let C = ˙S

K
k=1Ck be an f -balanced partition. Then there
, that contains {n1, . . . , nK} ⊂

log(1+ε) (cid:17)K
K, |S| =(cid:16) log(f (K))

for all k ∈ [K].

|Ck| ≤ nk ≤ (1 + ε)|Ck|.

(11)

14

Algorithm 2: Approx-Means(R, l, Mk−l, Σ)

d : set of remaining input points

Input:
R ⊂ X ⊂ R
l ∈  : number of means yet to be found
~µ = (µ1, . . . , µj) : tuple of j ≤ k − l candidate means
1, . . . , ˜σ2
(˜σ2
1, . . . , ˜w2
( ˜w2
Notation:
~S : vector containing the elements of set S in arbitrary order
~x ◦ ~y : concatenation of vectors, i.e. for ~x = (x1, . . . , xn) and ~y = (y1, . . . , ym),

k) : vector of k variances
k) : vector of k weights

~x ◦ ~y = (x1, . . . , xn, y1, . . . , ym)

Output: θ = {(wi, µi, σi)} containing at most k tuples of mean and variance
if l = 0 then
return ~P ;

else

if l ≥ |R| then

return θ = {(µi, σi)}i where ~µ ◦ ~R = (µi)i;

else

/* sampling phase */ ;
sample a multiset S of size 1

T ←(cid:8)µ(S′)|S′ ⊂ S, |S′| = 1
εδ(cid:9);

Mk ← ∅;
for t ∈ T do

αεδ from R;

Mk ← Mk ∪ Approx-Means(R, l − 1, {~µ ◦ (t)|~µ ∈ Mk−l}, Σ);

end

/* pruning phase */ ;
N ← set of |R|
log-likelihood cost wrt. the weighted component given by ( ˜wi, µi, ˜σ2
i.e.

2 points x from R with smallest minimum negative complete-data
i ) for i ∈ [j],

1
2˜σ2
i

kx − µik2 − ln( ˜wi)(cid:27)

min

i∈[j](cid:26) d

2

ln(2π˜σ2

i ) +

Mk ← Mk ∪ Approx-Means(R \ N, l, Mk−l, Σ);
return the candidate θ = {(wi, µi, σi)}i, (µi) ∈ Mk, which has minimal cost
LX(θ) ;

end

end

15

4.2 Applying the ABS Algorithm

In the following we analyze Algorithm 2. We show that the algorithm can be used to construct
means such that, together with appropriate approximations of the weights and variances, we
obtain a CMLE solution with costs close to the costs of the given CMLE solution.

Theorem 26. Let ˜σi ∈ [σ2
(X, k, ∅, (˜σ2

1 , . . . , ˜σ2

i , (σ2

i )(1+ε)] and ˜wk ≥ 1

(1+ε) wk for i ∈ [k]. Algorithm 2 started with

5 (cid:1)k
k)) computes a tuple (˜µ1, . . . , ˜µk) such that with probability at least (cid:0) 1−δ

LX(( ˜wi, ˜µi, ˜σ2

i )i∈[k]) ≤ (1 + ε)L(X) .

The running time of the algorithm is bounded by |X| d 2O(k/ε·log(k/ε2)).

k
i=1Ci be a partition of X into optimal CMLE clusters. We introduce

Let ˙S

C[i,j] =

Ct

t=i

j

˙[

as a short notation for the disjoint union of clusters i through j. We assume that the Ci
are numbered by the order their approximate means ˜µi are found by the superset-sampling
technique.

Now, let X = R0 ⊇ Ri ⊇ · · · ⊇ Rk−1 be a sequence of input sets computed by the algorithm,

such that

|Ci ∩ Ri−1| ≥ α|Ri−1|.

Without loss of generality assume that each Ri is the largest of these sets with this property.

By using Lemma 17, we obtain the following Lemma.

Lemma 27 (By Superset-Sampling). With probability at least ((1 − δ)/5)k we have

k˜µi − µ(Ci ∩ Ri−1)k2 ≤

ε

|Ci ∩ Ri−1| Xx∈Ci∩Ri−1

kx − µ(Ci ∩ Ri−1)k2

for all i ∈ [K].

By Ni := Ri−1 \ Ri we denote the set of points remove between two sampling phases. Using

these deﬁnitions we can see that

(Ci ∩ Ri−1) ˙∪

k

˙[

i=1

k

˙[
i=1(cid:0)C[i+1,k] ∩ Ni(cid:1)

is a disjoint partition of X. Each set Ci ∩ Ri−1 on the left side contains the points that the mean
˜µi has been sampled from. The sets C[i+1,k] ∩ Ni on the right side contain points incorrectly
assigned to {˜µ1, . . . , ˜µi} during the pruning phases between the sampling of ˜µi and ˜µi+1.

Denote by θi the parameters of the ﬁrst i weighted Gaussians obtained by the algorithm, i.e.

˜θi = (( ˜w1, ˜µ1, ˜σ1), . . . , ( ˜wi, ˜µi, ˜σi)) .

Lemma 28 (cf. Claim 4.8 in [Ack09]).

LC[i+1,k]∩Ni(˜θi) ≤ 8αkLC[1,i]∩Ri−1(˜θi)

Proof. As in [Ack09, p. 70ﬀ], with “cost“ replaced by ”L“.

16

Denote by cost(P, C) the k-means cost of a point set P wrt. a set of means C.

Lemma 29 (cf. Claim 4.9 in [Ack09]). For every i ∈ [k] we have

cost(Ci ∩ Ri−1, ˜µi) ≤ (1 + ε) cost(Ci, µi) .

Proof. As in [Ack09, p. 70ﬀ], using that optimal means in CMLE are means of the optimal
CMLE clusters.

Given appropriate approximate variances, we can conclude that a similar bound holds wrt.

the complete-data log-likelihood.

Lemma 30. Given ˜σi ∈ [σ2

i , (σ2

i )(1+ε)] and ˜wi = ni

|X| with ni ∈ [|Ci|, (1 + ε)|Ci|], we have

LCi∩Ri−1( ˜wi, ˜µi, ˜σ2

i ) ≤ (1 + ε)LCi(wi, µi, σ2

i ) .

Proof.

LCi∩Ri−1(˜µi, ˜σ2

i ) =

|Ci ∩ Ri−1|d

2

ln(2π˜σ2

i ) +

1
2˜σ2
i

cost(Ci ∩ Ri−1, ˜µi) − |Ci ∩ Ri−1| ln( ˜wi) .

We have

ln(2π˜σ2

i ) ≤ ln(2π(σ2

i )(1+ε)) = (1 + ε) ln(2πσ2

i ) .

Furthermore, Using that |Cl| ≤ nl ≤ (1+ε)|Cl| for all l = 1, . . . , K, we obtain ˜wk ≥ |Ck|

|X| . Hence,

− ln( ˜wi) · |Ci ∩ Ri−1| ≤ − ln( ˜wi) · |Ci|

≤ − ln(cid:18) |Ci|

|X|(cid:19) |Ci|

= − ln (wi) · |Ci|

(by Equation (3))

By Lemma 29 and ˜σ2

i ≥ σ2
i ,

1
2˜σ2
i

cost(Ci ∩ Ri−1, ˜µi) ≤ (1 + ε)

1
2σ2
i

cost(Ci, µi) .

From this and by using that σ2

i = 1

|Ci|d cost(Ci, µi), we conclude

LCi∩Ri−1 (˜µi, ˜σ2

i ) ≤ (1 + ε)

|Ci|d

2

ln(2πσ2

i ) + (1 + ε)

1
2σ2
i

cost(Ci, µi) − ln(wi)|Ci|

≤ (1 + 2ε)N1(Ci) − ln(wi)|Ci|
≤ (1 + 2ε)LCi (µi, σ2

i ) .

Running Algorithm 2 with ε/3 instead of ε yields the claim.

Analogously to [Ack09], we can prove Theorem 26 as follows.

17

Proof of Theorem 26. Let ˜θk = (˜µi, ˜σ2

i )i∈[k]. Then,

LX (˜θk) ≤

≤

≤

k

k

Xi=1
Xi=1
Xi=1

k

LCi∩Ri−1(˜µi, ˜σ2

i ) +

k−1

Xi=1

LCi∩Ri−1(˜µi, ˜σ2

i ) + 8αk

LCi∩Ri−1(˜µi, ˜σ2

i ) + 8αk

LC[i+1,k]∩Ni(˜θk)

k−1

k−1

Xi=1
Xi=1

LC[1,i]∩Ri−1(˜θk)

(due to Lemma 28)

LCt∩Ri−1(˜µt, ˜σ2

t ) .

i

Xt=1

Since Ri ⊆ Ri−1, we have Ct ∩ Ri−1 ⊆ Ct ∩ Rt−1. Hence,

k−1

Xi=1

i

Xt=1

LCt∩Ri−1(˜µt, ˜σ2

t ) ≤

LCt∩Rt−1(˜µt, ˜σ2
t )

≤ k

LCi∩Ri−1 (˜µi, ˜σ2

i ) .

k−1

i

Xi=1
Xt=1
Xi=1

k−1

Thus,

LX(˜θk) ≤

k

Xi=1

LCi∩Ri−1(˜µi, ˜σ2

i ) + 8αk2

k

LCi∩Ri−1(˜µi, ˜σ2
i )

k−1

Xi=1

≤ (1 + 8αk2)

LCi∩Ri−1(˜µi, ˜σ2
i )

Xi=1

≤ (1 + 8αk2)(1 + ε)L(X) .

(by Lemma 30)

Finally, running the algorithm for ε := ε/2 and α = θ(ε/k2) yields the theorem.

5 Special Cases

5.1 Weighted K-Means (Identical Covariances)

In this section we consider a restricted version of the CMLE problem where we are only interested
in Gaussian mixture models where all components share the same ﬁxed spherical covariance
matrix, i.e. parameters θ = {(wk, µk, Σk)}k∈[K] where Σk = 1
2β Id for all k ∈ [K]. We call this
problem the Weighted K-Means (WKM) problem.

Problem 31 (WKM). Given a ﬁnite set X ⊂ R
C = {C1, . . . , CK } of X into K disjoint subsets and K weighted means θ = {(wk, µk)}K
where µk ∈ R

d and an integer K ∈ , ﬁnd a partition
k=1,

k=1 wk = 1, minimizing

D, wk ∈ R, and PK
Xk=1

X (θ, C) =

Lwm

K

β
Xx∈Ck

kx − µkk2


− ln(wk) · |Ck| .

We denote the minimal value by OP Twm(X, K).

18

K
Corollary 32. Let X ⊂ R
k=1Ck be a well-deﬁned
solution for the WKM problem. There is an algorithm that computes K weighted means θ =
{( ˜wk, ˜µk)}K

d, K ∈ , and δ, ε > 0. Let X = ˙S

k=1 such that with probability at least 1 − δ

Lwm

X (( ˜wi, ˜µi)i∈[K]) ≤ (1 + ε)OP Twm(X, K) .

The running time of the algorithm is bounded by

|X| d 2O(K/ε·log(K/ε2)) · (log(f (K)))K .

Proof. Use a grid search to obtain candidates for the weights, then apply the ABS algorithm.

5.2 Uniform Weights

In this section we consider a restricted version of the CMLE problem where we are only interested
in Gaussian mixture models with ﬁxed uniform weights, i.e. parameters θ = {(wk, µk, Σk)}k∈[K]
where wk = 1/K for all k ∈ [K]. We denote this problem by Uniform Complete-Data Maximum
Likelihood Estimation (UCMLE).

d and an integer K ∈ , ﬁnd a partition
Problem 33 (UCMLE). Given a ﬁnite set X ⊂ R
C = {C1, . . . , CK } of X into K disjoint subsets and K spherical Gaussians with parameters
θ = {(µk, σ2

k=1 minimizing

k)}K

Lunif

X (θ, C) =

=

K

K

Xk=1
Xk=1

LCk (µk, σ2
k)

|Ck|d

2

ln(2πσ2

k) +

We denote the minimal value by OP Tunif (X, K).

1
2σ2

k 
Xx∈Ck

kx − µkk2
 .

K
Corollary 34. Let X ⊂ R
k=1Ck be a well-deﬁned
solution for the UCMLE problem. There is an algorithm that computes K spherical Gaussians
θ = {(˜µk, ˜σ2

d, K ∈ , and δ, ε > 0. Let X = ˙S

k=1 such that with probability at least 1 − δ

k)}K

Lunif
X ((˜µi, ˜σ2

i )i∈[K]) ≤ (1 + ε)OP Tunif (X, K) .

The running time of the algorithm is bounded by

|X| d log(1/δ) 2O(K/ε·log(K/ε2)) (cid:0)log(log(∆2)) + 1(cid:1)K

where ∆2 = maxx,y∈X{kx − yk2}.

,

Proof. Use a grid search to obtain candidates for the variances, then apply the ABS algorithm.

19

References

[ABS10] Marcel R. Ackermann, Johannes Bl¨omer, and Christian Sohler. Clustering for metric
and nonmetric distance measures. ACM Trans. Algorithms, 6(4):59:1–59:26, Septem-
ber 2010.

[Ack09] Marcel R. Ackermann. Algorithms for the Bregman k-Median Problem. PhD thesis,

University of Paderborn, 2009.

[CG92] Celeux and Govaert. A Classiﬁcation EM Algorithm for Clustering and Two Stochastic

Versions. Comput. Stat. Data Anal., 14(3), 1992.

[IKI94] M. Inaba, N. Katoh, and H. Imai. Applications of Weighted Voronoi Diagrams and
Randomization to Variance-based K-clustering. In Proceedings of the Tenth Annual
Symposium on Computational Geometry, SoCG ’94, pages 332–339, New York, NY,
USA, 1994. ACM.

20

