6
1
0
2

 
r
a

 

M
6
1

 
 
]

M

I
.

h
p
-
o
r
t
s
a
[
 
 

1
v
6
6
1
5
0

.

3
0
6
1
:
v
i
X
r
a

MNRAS 000, 1–22 (2015)

Preprint 17 March 2016

Compiled using MNRAS LATEX style ﬁle v3.0

Fifty Years of Pulsar Candidate Selection: From simple
ﬁlters to a new principled real-time classiﬁcation approach

R. J. Lyon 1⋆, B. W. Stappers 2, S. Cooper 2, J. M. Brooke 1, J. D. Knowles 1,3

1School of Computer Science, The University of Manchester, Manchester M13 9PL
2Jodrell Bank Centre for Astrophysics, School of Physics and Astronomy, The University of Manchester, Manchester M13 9PL
3School of Computer Science, University of Birmingham, Edgbaston, Birmingham B15 2TT

Accepted XXX. Received YYY; in original form ZZZ

ABSTRACT
Improving survey speciﬁcations are causing an exponential rise in pulsar candidate
numbers and data volumes. We study the candidate ﬁlters used to mitigate these
problems during the past ﬁfty years. We ﬁnd that some existing methods such as
applying constraints on the total number of candidates collected per observation, may
have detrimental eﬀects on the success of pulsar searches. Those methods immune
to such eﬀects are found to be ill-equipped to deal with the problems associated
with increasing data volumes and candidate numbers, motivating the development
of new approaches. We therefore present a new method designed for on-line operation.
It selects promising candidates using a purpose-built tree-based machine learning
classiﬁer, the Gaussian Hellinger Very Fast Decision Tree (GH-VFDT), and a new
set of features for describing candidates. The features have been chosen so as to i)
maximise the separation between candidates arising from noise and those of probable
astrophysical origin, and ii) be as survey-independent as possible. Using these features
our new approach can process millions of candidates in seconds (∼1 million every
15 seconds), with high levels of pulsar recall (90%+). This technique is therefore
applicable to the large volumes of data expected to be produced by the Square
Kilometre Array (SKA). Use of this approach has assisted in the discovery of 20 new
pulsars in data obtained during the LOFAR Tied-Array All-Sky Survey (LOTAAS).

Key words: pulsars: general, methods: statistical, methods: data analysis

1 INTRODUCTION

The search techniques used to isolate the radio emission
of pulsars, are designed to ﬁnd periodic broadband signals
exhibiting signs of dispersion caused by travel through
the interstellar medium. Signals meeting these criteria are
recorded as a collection of diagnostic plots and summary
statistics, in preparation for analysis. Together these plots
and statistics are referred to as a pulsar ‘candidate’,
a possible detection of a new pulsar. Each candidate
must be inspected by either an automated method, or a
human expert, to determine their authenticity. Those of
likely pulsar origin are highlighted for further analysis,
and possibly allocated telescope time for conﬁrmation
observations. The remainder are typically ignored. The
process of deciding which candidates are worthwhile
investigating has become known as candidate ‘selection’. It

is an important step in the search for pulsars since it allows
telescope time to be prioritised upon those detections likely
to yield a discovery. Until recently (early 2000’s) candidate
selection was a predominately manual
task. However
advances in telescope receiver design, and the capabilities
of supporting computational
infrastructures, signiﬁcantly
increased the number of candidates produced by modern
pulsar surveys (Stovall et al. 2013). Manual approaches
therefore became impractical, introducing what has become
known as the ‘candidate selection problem’. In response,
numerous graphical and automated selection methods were
developed (Johnston et al. 1992; Manchester et al. 2001;
Edwards et al. 2001; Navarro et al. 2003; Keith et al. 2009),
designed to ﬁlter candidates in bulk. The ﬁltering procedure
used ranged in complexity from a simple signal-to-noise
ratio (S/N) cut,
through to more complex functions
(Lee et al. 2013). In either case, automated approaches
enabled large numbers of candidates to be selected at speed

⋆ E-mail: robert.lyon@cs.man.ac.uk

c(cid:13) 2015 The Authors

2

R. J. Lyon et al.

in a reproducible way.

Despite these advances

the increasing number of
candidates produced by contemporary pulsar surveys, tends
to necessitate a pass of manual selection upon the candidates
selected by software. Many have therefore turned to machine
learning methods to build ‘intelligent’ ﬁlters (Eatough et al.
2010; Bates et al. 2012; Zhu et al. 2014; Morello et al.
2014), capable of reducing the dependence on human input.
This has achieved some success. However these methods are
often developed for a speciﬁc pulsar survey search pipeline,
making them unsuitable for use with other surveys without
modiﬁcation. As a consequence, new selection mechanisms
are often designed and implemented per survey. As more
methods continue to emerge, it becomes increasingly unclear
which of these best address the candidate selection problem,
and under what circumstances. It is also unclear which
are best equipped to cope with the trend for increasing
candidate numbers, the overwhelming majority of which
arise from noise. Existing approaches are not explicitly
designed to mitigate noise, rather they are designed to
isolate periodic detections. This does not achieve the
same eﬀect as explicitly mitigating noise. For example,
isolating periodic candidates as potential pulsars, does not
necessarily mitigate the impact of periodic noise. Thus it
is possible that these techniques will become less eﬀective
over time, as noise becomes responsible for an increasing
proportion of all candidates detected.

Existing ‘intelligent’ approaches are also ill-equipped
to deal with the data processing paradigm shift, soon to
be brought about by next-generation radio telescopes.
These instruments will produce more data than can be
stored, thus survey data processing,
including candidate
selection, will have to be done on-line in real-time (or
close to). In the real-time scenario it is prohibitively
expensive to retain all data collected (see Section 4.3.1). It
therefore becomes important to identify and prioritise data
potentially containing discoveries for storage. Otherwise
such data could be discarded and discoveries missed. Thus
new techniques are required (Keane et al. 2014) to ensure
preparedness for this processing challenge.

In this paper we describe a new candidate selection
approach designed for on-line operation, that mitigates
the impact of increasing candidate numbers arising from
noise. We develop our arguments for producing such a
technique in progressive stages. In Section 2 we describe the
candidate generation process. We show that improvements
in pulsar survey technical speciﬁcations have led to increased
candidate output, and infer a trend for exponential growth in
candidate numbers which we show to be dominated by noise.
We also demonstrate why restricting candidate output based
on simple S/N cuts, runs the risk of omitting legitimate
pulsar signals. The trend in candidate numbers and the
ineﬀectiveness of S/N ﬁlters, allows us to identify what we
describe as a ‘crisis’ in candidate selection. In Section 3
we review the diﬀerent candidate selection mechanisms
employed during the past ﬁfty years, to look for potential
solutions to the issues raised in Section 2. Based on
this review, in Section 4, we discuss these methods. We
identify how all will be challenged by the transition to on-

line processing required by telescopes such as the Square
Kilometre Array (SKA), motivating the development of new
approaches. In addition we critique the existing features
used to describe pulsar candidates, fed as inputs to the
machine learning methods employed by many to automate
the selection process. In Section 5 we present our own
set of eight candidate features, which overcome some of
these deﬁciencies. Derived from statistical considerations
and information theory, these features were chosen to
maximise the separation between noise and non-noise arising
candidates. In Section 6 we describe our new data stream
classiﬁcation algorithm for on-line candidate selection which
uses these features. Section 6 also presents classiﬁcation
results that demonstrate the utility of the new approach,
and its high level of pulsar recall. Finally in Section 7 we
summarise the paper, and comment on how the use of our
method has helped to ﬁnd 20 new pulsars during the LOFAR
Tied-Array All-Sky Survey (LOTAAS), though discovery
details will be published elsewhere.

2 CANDIDATE GENERATION

Since the adoption of the Fast Fourier Transform (FFT)
(Burns & Clark
1969; Taylor, Dura & Huguenin 1969;
Hulse & Taylor 1974) the general pulsar search procedure
has remained relatively unchanged. Signals focused at
the receiver of a radio telescope observing at a central
frequency fc (MHz), with bandwidth B (MHz), are sampled
and recorded at a predetermined rate at intervals of tsamp
(µs), chosen to maximise sensitivity to the class of signals
being searched for. The data are subsequently split in
to nchans frequency channels, each of width ∆v (kHz). An
individual channel contains stot samples of the signal taken
at the interval tsamp, over an observational period of length
tobs seconds, such that stot =
. Each unique observation
is therefore representable as an nchans × stot matrix M.

tobs
tsamp

A pulsar search involves a number of procedural
steps applied to the data in M. The principal steps
are similar for all searches, however the order in which
these are undertaken can vary, as too can their precise
implementation. In general, the ﬁrst step involves radio
frequency interference (RFI) excision, via the removal of
channels (rows of the matrix) corresponding to known
interference frequencies (Keith et al. 2010). Subsequently
‘Clipping’ (Hogden et al. 2012) may be applied to the data,
which aims to reduce the impact of strong interference.
This is achieved by setting to zero (or to the local mean)
those samples which exhibit intensities higher than some
pre-determined threshold in a given column in M (e.g. an
intensity 2σ above the mean). Once these initial steps are
complete, processing enters a computationally expensive
phase known as de-dispersion.

Dispersion by free electrons in the interstellar medium
(ISM) causes a frequency dependent delay in radio emission
as it propagates through the ISM. This delay temporally
smears legitimate pulsar emission (Lorimer & Kramer 2006)
reducing the S/N of their pulses. The amount of dispersive
smearing a signal receives is proportional to a quantity
called the Dispersion Measure (DM, Lorimer & Kramer

MNRAS 000, 1–22 (2015)

Fifty Years of Pulsar Candidate Selection

3

Sub-Bands

Period-DM Plane

A

1
0
0

8
0

D
M

6
0

D

0

30

60

90

120

150

180

Bin Number

Sub-Integrations

B

4
0

3
0

2
0

1
0

0

S
N
R

-4.0e+10 -2.0e+10
Topo Period Offset from 361.91384238ms

2.0e+10 4.0e+10 6.0e+10

0

DM Curve

E

0

100 200 300 400 500 600 700 800 900

DM

1
5

1
0

5

0

S
u
b
-
B
a
n
d
 
N
u
m
b
e
r

3
0

2
0

1
0

S
u
b
-
I
n
t
 
N
u
m
b
e
r

30

60

90

120

150

180

Bin Number

Profile

C

0

0

4
.
0
e
-
0
3
2
.
0
e
-
0
3

0

0

0.20

0.40

0.60

0.80
Phase

1

1.20

1.40

Bary Period
Bary Freq
Topo Period
Topo Freq
Acc
Jerk
DM
width
SNR

Initial
361.921792414
2.763027872
361.91384238
2.763088566
0.00
0.00
74.91
N/A
34.36

Optimised
361.922122747
2.76302535
361.91384238
2.763088566
0.00
0.00
72.73
0.01
45.04

ms
Hz
ms
Hz
m/s/s
m/s/s/s
cm/pc
periods
periods

Figure 1. An annotated example candidate summarising the detection of PSR J1706-6118. The candidate was obtained during processing
of High Time Resolution Universe Survey data by Thornton (2013).

2006). This represents the free electron column density
between an observer and a pulsar, integrated along the line
of sight. The degree to which a signal is dispersed for an
unknown pulsar cannot be known a priori (e.g. Keith et al.
2010; Levin 2012), thus several dispersion measure tests or
‘DM trials’ must be conducted to determine this value. This
can be used to mitigate the dispersive smearing, thereby
increasing the S/N of a signal (Lorimer & Kramer 2006).
For a single trial, each frequency channel (row in M) is
shifted by an appropriate delay before each time bin is
integrated in frequency. This produces 1 de-dispersed time
series for each DM trial value.

Periodic signals in de-dispersed time series data, can
be found using a Fourier analysis. This is known as a
periodicity search (Lorimer & Kramer 2006). The ﬁrst step
after performing the FFT of a periodicity search usually
involves ﬁltering the data to remove strong spectral features
known as ‘birdies’ (Manchester et al. 2001; Hessels et al.
2007). These may be caused by periodic or quasi-
periodic interference. Summing techniques are subsequently
applied, which add the amplitudes of harmonically related
frequencies to their corresponding fundamentals. This step
is necessary as in the Fourier domain, the power from
a narrow pulse is distributed between its fundamental
frequency and its harmonics (Lorimer & Kramer 2006).
Thus for weaker pulsars the fundamental may not rise above
the detection threshold, but the harmonic sum generally
will. Periodic detections with large Fourier amplitudes post
summing (above the noise background or a threshold level),
are then considered to be ‘suspect’ periods.

A further process known as sifting (e.g. Stovall et al.

MNRAS 000, 1–22 (2015)

2013) is then applied to the collected suspects, which
removes duplicate detections of the same signal at slightly
diﬀerent DMs, along with their related harmonics. A large
number of suspects survive the sifting process. Diagnostic
plots and summary statistics are computed for each of
these remaining suspects forming candidates, which are
stored for further analysis. The basic candidate consists of
a small collection of characteristic variables. These include
the S/N, DM, period, pulse width, and the integrated
pulse proﬁle. The latter is an array of continuous variables
that describe a longitude-resolved version of the signal
that has been averaged in both time and frequency. More
detailed candidates also contain data describing how the
signal persists throughout the time and frequency domains
(Eatough et al. 2010). This can be seen in plots (A) and
(B) in Figure 1. Here persistence in frequency (A) is
represented by a two-dimensional matrix showing pulse
proﬁles integrated in time, for a set of averaged frequency
channels (i.e. not full
frequency resolution). Persistence
is represented by a two-dimensional
through time (B),
matrix showing the pulse proﬁle integrated across similarly
averaged frequency channels as a function of time.

2.1 Modelling Candidate Numbers

Candidate numbers are anecdotally understood to be
increasing steadily over time. Here we provide historical
evidence supporting this view, obtained by reviewing most
of the large-scale pulsar surveys conducted since the initial
pulsar discovery by Hewish et al. (1968). The surveys
studied are listed in Tables 2 & 3. This information has
also been made available via an interactive on-line resource

Year Candidates Per Sq. Degree

1977

1983

1988

1991

1997

1998

2004

2006

2, 500

5, 405

∼ 150, 000
40, 000

8, 000, 000

> 200, 000

> 5, 000, 000

3, 500, 000

> 1, 200, 000

∼0.1
∼1
∼188
∼2

∼5,161
∼168∗
∼16,361∗
∼77,778 †

4

R. J. Lyon et al.

Survey

2nd Molonglo Survey(Manchester et al. 1978)

Phase II survey (Stokes et al. 1986)

Parkes 20 cm survey (Johnston et al. 1992)

Parkes Southern Pulsar Survey (Manchester et al. 1996)

Parkes Multibeam Pulsar Survey (Manchester et al. 2001)

Swinburne Int. Lat. Survey (Edwards et al. 2001)

Arecibo P-Alfa all conﬁgurations (Cordes et al. 2006; Lazarus 2012; P-Alfa Consortium 2015)

6.5 GHz Multibeam Survey (Bates et al. 2011a; Bates 2011)

Southern HTRU (Keith et al. 2010)

GBNCC survey (Stovall et al. 2014)

Northern HTRU (Barr et al. 2013; Ng 2012)

∼89∗
∼1,705
∼2,890∗
∼2,000
Table 1. Reported folded candidate numbers. Note ∗ indicates a lower bound on the number of candidates per square degree, calculated
from incomplete candidate numbers. † indicates very long integration times, with further details supplied in Tables 2 & 3.

LOTAAS (Cooper, private communication, 2015 )

> 80, 000, 000

55, 434, 300

2009

2010

2010

2013

39, 000, 000

found at www.jb.man.ac.uk/pulsar/surveys.html.

Candidate numbers reported in the literature are
summarised in Table 1, providing empirical evidence for
rising candidate numbers. The rise is understood to be
the result of expanding survey technical speciﬁcations
(Stovall et al. 2013) occurring during the period depicted
in Tables 2 & 3. Finer frequency resolution, longer dwell
times, and acceleration searches (Eatough et al. 2013), have
signiﬁcantly increased the candidate yield (Lyon 2015).
However at present there is no accepted method for
quantifying the eﬀects of improving survey speciﬁcations on
candidate numbers. It is therefore diﬃcult to understand
precisely how candidate numbers are changing, and what the
S/N distribution of candidates should look like in practice.
Such knowledge is needed if we are to design candidate
selection approaches robust to error, and accurately plan
survey storage requirements. Although it is diﬃcult to
capture all the steps involved in pulsar data analysis, we
describe a model here that can be used as a proxy for
estimating candidate numbers,
linked to the number of
dispersion trials undertaken per observation.

2.1.1 Approximate Model of Candidate Numbers

ii) RFI,

Selection begins in the spectral S/N regime as described
in Section 2. Here each suspect period associated with
a spectral S/N,
is found through a Fourier analysis of
a de-dispersed time series. However, we have incomplete
knowledge of the S/N distribution of spectral suspects,
which arise from either i) variations in Galactic background
noise,
iii) instrument noise, or iv) legitimate
phenomena. To overcome this we model only the most
signiﬁcant contributor of candidates, Gaussian distributed
background noise. Empirical
suggests most
candidates originate from background noise. Our analysis
of High Time Resolution Universe Survey (HTRU) data
(Thornton 2013)
this view, held by others
(Lee et al. 2013; Morello et al. 2014). It is also logically
consistent, since if most candidates arose from legitimate
phenomena discovery would be trivial. Whilst if most arose
from RFI, this would be concerning, as telescopes used

supports

evidence

for surveys are situated in low RFI environments. It thus
appears sensible to conclude that candidates are noise
dominated.

can estimate

By modelling candidates arising only from background
noise, we
the approximate number of
candidates a survey will yield. To achieve this, we assume
there is a 1:1 mapping from spectral suspects to folded
candidates1. We can then model the folded S/N distribution
of noise-originating candidates only, from there onwards.
By assuming at least 1 folded candidate is generated per
dispersion trial, which also subsequently survives sifting, it is
possible to calculate indicative candidate numbers. As folded
candidate S/Ns are empirically well approximated by a
Gaussian distribution2, we can also estimate the folded S/N
distribution using a simple Gaussian model. The number
of candidates arising from noise with a folded S/N of nσ
(i.e. 1σ, ..., nσ), is estimated as follows using a Gaussian
probability density function,

f (d, λ, µ, σ) =

1

σ √2π

1
2 ( λ−µ

σ )2

e−

· d,

(1)

where λ is the folded S/N, µ is the mean of the
noise distribution, σ its standard deviation, and d the
total number of dispersion trials. This model considers
each dispersion trial to be a single draw from the noise
distribution. Thus candidate numbers here are determined
by d, and not a top C candidate cut, as is often used
to limit candidate numbers (e.g. Thornton 2013). However
since cuts are used in practice to remove weak candidates
(arising from noise), we will
incorporate them into our
model. This is achievable whilst retaining knowledge of the
resulting folded S/N distribution for a cut C ∈ (0, ∞]. First
we compute the total number of candidates arising from
Gaussian distributed noise, with a folded S/N > nσ using,

1 A candidate obtained by folding a de-dispersed time series at a
speciﬁc suspect period.
2 Empirically observed in HTRU survey data.

MNRAS 000, 1–22 (2015)

Fifty Years of Pulsar Candidate Selection

5

σ

Figure 3. Candidate numbers predicted by Equation 4 (using
nσ = 7 and nmax
= 100), varied according to the total number
of survey pointings for a single beam receiver. Coloured dashed
lines indicate the total number of candidates returned when using
a conservative C = 100 cut. The corresponding solid colour lines
indicate the total number of candidates returned when the cut
is discarded. The solid lines are truncated such that they begin
where C = 100 to avoid overlapping lines complicating the plot.

number of noise-originating candidates to be evaluated will
increase with d. Indeed, Equation 4 implies the existence
of a discernible trend in candidate numbers. Much like the
exponential rise in data volumes described by Bates (2011),
this model shows candidate numbers to be increasing
exponentially as a function of d. This is shown more clearly
in Figure 3. This illustrates how candidate numbers change
as d and the number of survey pointings increase. The plot
is colour coded according to total pointings, with dashed
lines indicating candidate numbers when C = 100, and the
corresponding solid lines showing candidate numbers when
C is discarded. We note here that we have concentrated
on utilising d as the number of dispersion trials to show a
rise in candidate numbers. This should not be seen simply
as relating to the maximum DM being searched. As the
sampling time is increased and more channels are used,
either to preserve high time resolution at higher dispersion
measures, or as the bandwidth increases, or both, then
‘dispersion’ trials increases. Therefore d
the number of
is also a good proxy for survey sensitivity. Of course the
higher the time resolution, the greater stot increases, longer
observations also increase stot considerably. In both cases
this increases the likelihood of detecting more than one
candidate per DM trial. For simplicity this is not modelled
here, and so what we present can be considered a lower limit.

There are two strategies available for dealing with the
implied rise of noisy candidates. The ﬁrst is to increase

Figure 2. Diagram of 1 − CDF of Equation 1, showing the
relationship between nσ and constant cuts. This illustrates their
impact on the number of noise candidates making it through to
the candidate selection stage.

nσ

(2)

f (d, λ, µ, σ)dλ.

k(d, µ, σ, nσ) =Z ∞
In practice Gaussian noise possessing a S/N in excess of
30σ is rare. Thus we can replace the upper limit of ∞ with
nmax
= 30, beyond which a detection is almost certainly
σ
not noise. Here nσ is the cut oﬀ S/N that is n standard
deviations from the mean, and we do not count candidates
with S/Ns below this. Equation 2 is related to the cumulative
distribution function (CDF), of the probability distribution
in Equation 1, where k = 1−CDF as shown in Figure 2. From
this we can compute the number of nσ candidates surviving
a top C cut, using h( f , C − k). Here C − k gives the number of
remaining candidate places in a top C cut, and h is deﬁned
by,

0,
f ,
s,

h( f , s) =

s ≤ 0
f − s ≤ 0
f − s > 0,

(3)

where 0 is returned if there are no spaces in a top C cut,
f is returned if all nσ candidates make the cut, and ﬁnally
s is returned if some nσ candidates miss the cut. Now the
total number of candidates returned during a survey using a
single telescope, with an nbeam receiver making p pointings,
can be estimated by,

p ·(cid:0)nbeams · max(k, C)),

where k is given by Equation 2 and C is the numerical
cut-oﬀ per beam (e.g. C = 100). This allows us to identify
what we describe as a ‘crisis’ in candidate selection. Since
the functions f and k are linearly dependent on d, and
since we can see empirically from Tables 2 and 3 that
d is increasing, this means that even if nσ is ﬁxed, the

(4)

MNRAS 000, 1–22 (2015)

6

R. J. Lyon et al.

the lower S/N limit nσ in Equation 2. This eﬀectively
implements a S/N cut-oﬀ, used by many to ﬁlter in the
spectral domain (Foster et al. 1995; Hessels et al. 2007;
Burgay et al. 2013; Thornton 2013), and the folded domain
(Damashek et al. 1978; Manchester et al. 1978; Stokes et al.
1986; Manchester et al. 2001; Burgay et al. 2013). However
in practice this cut-oﬀ would become high enough to reject
weaker detections of interest (i.e. weaker pulsars, see Section
4.2.1) if it is to reduce candidate numbers. The second
option is to impose a smaller constant cut-oﬀ C to the
candidates collected per observation or beam, also done by
many (Edwards et al. 2001; Jacoby et al. 2009; Bates et al.
2012; Thornton 2013) and accounted for in our model.
Figure 2 shows these two methods to be fundamentally the
same. Imposing a ﬁxed limit C on the output of Equation 2,
can only be achieved by increasing the lower value of nσ in
the integral, since the integrand is ﬁxed by Equation 1. This
corresponds to setting a high S/N cut-oﬀ. Using either of
these approaches impacts our ability to detect legitimate
pulsar signals. This is particularly true of a top C cut,
as it would appear that noise alone can ﬁll up a top C
cut, without even taking into consideration the inﬂuence
of RFI, or legitimate phenomena. Taking d to the limit
increases the certainty that noise will dominate a candidate
cut, and reduces the likelihood of weak legitimate signals
making it through to analysis. We now turn our attention
to determining how to deal with these issues.

3 CANDIDATE SELECTION METHODS

3.1 Manual Selection

During the earliest surveys, manual selection involved the
inspection of analogue pen chart records for periodic signals
(Large et al. 1968; Manchester et al. 1978). This process
was subsequently replaced by digital data inspection, with
the adoption of early computer systems. From then on,
manual selection involved the inspection ‘by eye’ of digitally
produced diagnostic plots describing each candidate. Those
found exhibiting pulsar-like characteristics were recorded
for analysis, whilst the remainder were ignored (though
retained on disk for possible reanalysis).

During the

initial period of digitization, pulsar
surveys produced very few candidates with respect to
modern searches. The 2nd Molonglo survey conducted
during the 1970’s, produced only 2, 500 candidates in
total (Manchester et al. 1978). These yielded 224 pulsar
detections (Manchester et al. 2005), a hit rate of almost 9
per cent3. Thus during this period manual selection was
entirely practical. Soon after however, increasing candidate
numbers began to cause problems. The ﬁrst mention of
this within the literature (to the best of our knowledge)
was made by Clifton & Lyne (1986) regarding Jodrell
Survey B. The number of candidates produced during
this survey necessitated extensive manual selection on the
basis of pulse proﬁle appearance and S/N. Although such
heuristic judgements were not new, their explicit mention

3 The hit rate of the recent southern HTRU medium latitude
search was much lower, at around 0.01 per cent (Lyon 2015).

with respect to candidate selection indicated that a shift
in procedure had occurred. Whereas before it was possible
to evaluate most,
if not all candidates by eye, here it
became necessary to expedite the process using heuristics.
Contemporary surveys reacting to similar issues imposed
high S/N cut-oﬀs to limit candidate numbers directly. The
Arecibo Phase II survey used an 8σ S/N cut, thus only
∼5,405 candidates required manual inspection (Stokes et al.
1986).

The use of heuristics and S/N cuts proved insuﬃcient to
deal with candidate number problems. Additional processing
steps such as improved sifting were applied in response,
and these became increasingly important during this period.
However as these measures apply high up the processing
pipeline (close to the ﬁnal data products), their capacity
to reduce candidate numbers was limited. Consequently
attempts were made to automatically remove spurious
candidates lower down the pipeline, with the aim of
preventing them ever reaching human eyes. During the
Parkes 20-cm survey, two software tools were devised
by Johnston et al. (1992) to achieve this. Together these
encapsulated and optimised the general search procedure
discussed in Section 2. The software (‘MSPFind’ and another
unnamed tool) was explicitly designed to reduce the quantity
of spurious candidates, while maintaining sensitivity to
millisecond pulsars (MSPs). Only candidates with a S/N > 8
were allowed through the pipeline to manual inspection. It
is unclear how many candidates required manual inspection,
though the number was less than 150, 000 (Johnston et al.
1992). During the same period, a similar software tool
known as the Caltech Pulsar Package (Deich 1994), was
developed for the Arecibo 430 MHz Intermediate Galactic
Latitude Survey (Navarro et al. 2003). These represent some
of the earliest eﬀorts to systematise the search process in a
reproducible way.

3.2 Summary Interfaces

The success achieved via low-level ﬁltering and sifting,
continued to be undermined by ever-increasing candidate
numbers brought about by technological advances. By
the late 1990’s, manual selection was therefore becoming
increasingly infeasible. This
spawned many graphical
tools, designed to summarise and ﬁlter candidates for
speedy and concise evaluation. The ﬁrst of these, runview
(Burgay et al. 2006), was created to analyse data output
by the Parkes Multibeam Survey (PMPS, Manchester et al.
2001). During the Swinburne Intermediate-latitude survey,
Edwards et al. (2001) devised a similar graphical tool
that
candidate
parameters. A later reprocessing of PMPS data for binary
and millisecond pulsars, spawned the development of a
more sophisticated graphical tool
for candidate viewing
called reaper. reaper used a dynamic customizable plot
(Faulkner et al. 2004) that enabled heuristic judgements
of candidate origin to be made using multiple variables.
The use of reaper led to the discovery of 128 unidentiﬁed
pulsars in PMPS data. This corresponds to ∼ 15.4 per cent
of the known pulsars in PMPS data, given that 833 have
now been identiﬁed (Lorimer et al. 2015).

included distributional

information of

MNRAS 000, 1–22 (2015)

Following the success of reaper, an updated version
of the tool called jreaper was developed by Keith et al.
(2009). It incorporated algorithms which assigned numerical
scores to candidates based on their parameters, permitting
candidate rankings. By ignoring those candidates achieving
low rankings, the amount of visual
inspection required
was
reduced. When applied to data gathered during
the PMPS, use of jreaper led to the discovery of a
further 28 new pulsars (Keith et al. 2009), corresponding
to ∼ 3.4 per cent of known PMPS pulsars. Thus by
2009, summary interfaces had helped ﬁnd ∼ 18.7 per
cent of all PMPS pulsars illustrating the usefulness of
graphical approaches. More recently, web-based candidate
viewing systems incorporating similar scoring mechanisms
have appeared (Cordes et al. 2006; Deneva et al. 2009,
2013). One such tool, The Pulsar Search Collaboratory
(Rosen et al. 2010) 4, also incorporates human scoring via
the input of high school students. Students taking part
in the programme have discovered several new pulsars
(Rosen et al. 2013). This includes PSR J1930-1852, a pulsar
in a double neutron star system (Swiggum et al. 2015).

3.3 Semi-automated Ranking Approaches

Semi-automated selection approaches have recently begun
to emerge. Amongst the most popular are those employing
ranking mechanisms to prioritise promising candidates for
human attention. The most notable of these is the PEACE
system developed by Lee et al. (2013). PEACE describes
each candidate via six numerical features, combined linearly
to form a candidate score. Ranked candidates are then
analysed via graphical viewing tools by students in the
Arecibo Remote Command Centre Programme (ARCC). To
date PEACE has been used during the Greenbank Northern
Celestial Cap Survey (GBNCC, Stovall et al. 2014) and the
Northern High Time Resolution Universe Survey (HTRU
North, Ng 2012; Barr et al. 2013). Periodic and single-pulse
candidates obtained during the A0327 survey (Deneva et al.
2013), were similarly ranked using an algorithm based on
PEACE. Over 50 participants (of varying expertise) from
four universities, were then invited to view the A0327
candidates via a web-based interface.

3.4 Automated ‘Intelligent’ Selection

Intelligent selection techniques are gaining widespread
adoption. The nature of the intelligence arises from the
domain of statistical learning theory, more generally known
as machine learning (ML). In particular, from a branch
of ML known as statistical classiﬁcation. The aim of
classiﬁcation is to build functions that accurately map a
set of input data points, to a set of class labels. For pulsar
search this means mapping each candidate to its correct
label (pulsar or non-pulsar). This is known as candidate
supervised learning (Mitchell
classiﬁcation, a form of
1997; Duda et al. 2000; Bishop 2006). If S = {X1, . . . , Xn}
represents the set of all candidate data, then Xi
is an
individual
represented by variables known
as features. Features describe the characteristics of the

candidate

Fifty Years of Pulsar Candidate Selection

7

Figure 4. Example of the varying separability of features from
highly separable in (a), to poorly separable in (b).

i , ..., Xm

candidate such that Xi = {Xj
i }, where each feature
Xj
j = 1, . . . , m. The label y associated with
i ∈ R for
each candidate, may have multiple possible values such
that y ∈ Y = {y1, . . . , yk} (e.g. millisecond pulsar, RFI,
noise etc). However since the goal here is to separate
pulsar and non-pulsar candidates, we consider the binary
labels y ∈ Y = {−1, 1}, where y1 = −1 equates to non-
pulsar (synonymous with negative) and y2 = 1 to pulsar
(synonymous with positive).

features

that

the

separate

To build accurate classiﬁcation systems, it is desirable
to utilise
classes under
consideration. This is illustrated in Figure 4. An ML
function ‘learns’ to separate candidates described using
features, from a labelled input vector known as the training
set T . It contains pairs such that T = {(X1, y1), . . . , (Xn, yn)}.
The goal of classiﬁcation is to induce a mapping function
between candidates and labels based on the data in T ,
that minimises generalisation error on test examples
(Kohavi & John 1997). The derived function can then be
used to label new unseen candidates.

The ﬁrst application of ML approaches to candidate
selection was accomplished by Eatough et al. (2010). In
this work each candidate was reduced to a set of twelve
numerical feature values inspired by the scoring system
ﬁrst adopted in jreaper. A predictive model based on
a multi-layered perceptron (MLP), a form of artiﬁcial
neural network (Haykin 1999; Bishop 2006), was then
constructed. Using this model, a re-analysis of a sample of
PMPS data was completed and a new pulsar discovered
(Eatough 2009). Neural network classiﬁers based on the
MLP architecture were also developed to run on data
gathered during the HTRU survey. Bates et al. (2012)
modiﬁed the earlier approach by describing candidates
using 10 further numerical features (22 in total). The same
features were used to train neural network classiﬁers applied
to HTRU medium latitude data by Thornton (2013). More
recently the SPINN system developed by Morello et al.
(2014), utilised developments from the ﬁeld of computer
science to optimise neural network performance on a set
of 6 features. SPINN is currently being applied as part
of the Survey for Pulsars and Extragalactic Radio Bursts
(SUPERB, Barr 2014; Keane et al. in prep).

4 http://pulsarsearchcollaboratory.com .

Convolutional neural networks (CNN, Bengio 2009),

MNRAS 000, 1–22 (2015)

8

R. J. Lyon et al.

which achieved prominence due to their high accuracy
on diﬃcult learning problems such as speech and image
recognition, have been adapted for candidate selection. The
Pulsar Image-based Classiﬁcation System (pics) developed
by Zhu et al. (2014), uses the CNN and other types of
machine learning classiﬁer to perform image classiﬁcation
on candidate plots. pics is technically the most sophisticated
approach available, and it appears to possess high accuracy.
However this comes at the expense of high computational
costs. Particularly with respect to runtime complexity.

4 DISCUSSION

4.1 Critique of Manual Selection

Manual selection has retained a vital role in pulsar search
(Keith et al. 2010), as demonstrated by its use during recent
surveys (Bates et al. 2011a; Boyles et al. 2013; Coenen et al.
2014). The strongest argument in favour of manual selection
is its presumed accuracy i.e by Eatough (2009) and
Morello et al. (2014). However, to the best of our knowledge,
no study of the accuracy of expert selection has been
conducted. Although intuitively one would expect manual
accuracy to be high, studies in other domains indicate
otherwise. Most famously studies in medicine and ﬁnance
(Meehl 1954; Barber & Odean 2000) suggest that expert
decision-making is ﬂawed due to unconscious biases. Indeed
manual selection is already known to be a subjective and
error prone process (Eatough 2009; Eatough et al. 2010). In
any case, it is infeasible to continue using manual approaches
given the rise in candidate numbers predicted in Section
2.1, also anticipated by others (Keane et al. 2014). Thus
irrespective of the true accuracy of manual selection, it must
be supplanted to keep pace with increasing data capture
rates and candidate numbers.

4.2 Critique of Automated Approaches

2009),

2003;

safety

critical
Hodge & Austin

and astronomy (Borne.

2009)
2009; Way et al.

Machine learning approaches are becoming increasingly
important for automating decision making processes in
ﬁnance (Chandola et al. 2009), medicine (Markou & Singh
systems
2003; Chandola et al.
(Markou & Singh
2004;
Chandola et al.
2009;
Ball & Brunner.
2012). Given the
widespread adoption of ML, the continued application of
manual selection raises a fundamental question: why has
a transition to completely automated selection not yet
occurred? Speciﬁc barriers to adoption may be responsible,
such as the expertise required to implement and use
ML methods eﬀectively. Where this barrier is overcome,
approaches emerge that are typically survey and search
speciﬁc.

A further problem is the limited public availability of
pulsar speciﬁc code and data. Thus to adopt ML approaches
new systems generally need to be built from scratch.
Machine learning approaches also have to be ‘trained’
upon data acquired by the same pipeline they will be

deployed upon5. If training data is not shared, it has to
be collected before a survey begins. The cost of doing
so may be a further barrier to adoption. Perhaps more
simply, existing automated approaches may not yet be
accurate enough to be trusted completely. If this is the
case,
it is unlikely to be caused by the choice of ML
system (e.g. neural network, probabilistic classiﬁer, or any
other). Those methods described in Section 3.4 employ well
studied ML techniques, proven to be eﬀective for a variety
of problems. Drops in performance are more likely to be
due to deﬁciencies in i) the features describing candidates,
and ii) the data used to train learning algorithms. In
the following section we present evidence suggesting that
existing candidate features may well be sub-optimal.

4.2.1 Sub-optimal Candidate Features

Candidate features can be categorized as being either
fundamental to, or as being derived from candidate data.
The latter derive new information on the assumption
that it will possess some utility, whilst the former do
not. For instance the S/N or period of a candidate, can
be considered fundamental. A good example of a derived
feature is the χ2 value of a sine curve ﬁt to the pulse proﬁle
as used by Bates et al. (2012). Using curve ﬁttings in this
manner expresses an underlying hypothesis. In this case
Bates et al. (2012) suppose a good χ2 ﬁt to be indicative
of sinusoidal RFI. Whilst the reasoning is sound, such a
feature represents an untested hypothesis which may or
may not hold true.

The majority of existing features are derived (see
Eatough et al. 2010; Bates et al. 2012; Thornton 2013;
Morello et al. 2014) , and are based upon the heuristics
used when selecting candidates manually. As manual
selection is imperfect, we cannot rule out the possibility
of having designed features, and thereby automated
methods, which make the same mistakes as ourselves.
Some features
in use have been found to introduce
unwanted and unexpected biases against particular types
of pulsar candidate (Bates et al. 2012; Morello et al. 2014).
features are not necessarily better. For
Fundamental
example the folded or spectral S/N,
is often used as a
primitive ﬁlter and as a feature for learning. As noise
candidates possessing folded S/Ns of 6σ are common
(Nice et al. 1995), using an S/N cut at this level allows
large numbers of likely noise-originating candidates to be
rejected. However as noted by Bates et al. (2012), such
cuts are helpful only if one assumes all low S/N candidates
are attributable to noise. In practice the application of
cuts has prevented the detection of weaker pulsar signals
as warned in Section 2.1. PSR J0812-3910 went unseen
in High Latitude survey data (Burgay et al. 2006), as its
spectral S/N was below the survey’s threshold for folding.
Similarly PSR J0818-3049 went undetected during the same
survey, as its folded S/N was below the cut applied prior
to manual selection. What’s more, there is no agreed upon

5 The data an algorithm ‘learns’ from must possess the same
distribution as the data it will be applied to, otherwise its
performance will be poor.

MNRAS 000, 1–22 (2015)

Survey

survey 2

(a)

1st Molonglo Survey (Large et al. 1968)
Search at low Galactic Lat. (Davies et al.
1970)
Arecibo Survey 1 (Hulse & Taylor 1974)
Jodrell Survey A (Davies et al. 1977)
2nd Molonglo Survey (Manchester et al.
1978)
Green Bank Northern Hemisphere
Survey (Damashek et al. 1978, 1982)
Princeton-NRAO Survey (Dewey et al.
1985)
Green Bank short-period (Stokes et al.
1985)
Jodrell Survey B (Clifton & Lyne 1986)
Arecibo
II
- Phase
Princeton-NRAO (Stokes et al. 1986)
Arecibo survey 2 (b) (Stokes et al. 1986)
Jodrell Survey C Biggs & Lyne (1992)
Parkes Globular Cluster Survey (20cm)
(Manchester et al. 1990a,b)∗
Parkes Globular Cluster Survey (50cm)
(Manchester et al. 1990a,b)∗
Arecibo Survey 3 (Nice et al. 1995)
Parkes 20-cm Survey (I) (Johnston et al.
1992)
Parkes
(Johnston et al. 1992)
Arecibo 430 MHz Intermediate Galactic
Latitude Survey (Navarro et al. 2003)
High Galactic Latitude Pulsar Survey of
the Arecibo Sky (H1) (Foster et al. 1995)
High Galactic Latitude Pulsar Survey of
the Arecibo Sky (H2) (Foster et al. 1995)
High Galactic Latitude Pulsar Survey of
the Arecibo Sky (H3) (Foster et al. 1995)
High Galactic Latitude Pulsar Survey of
the Arecibo Sky (H4) (Foster et al. 1995)
High Galactic Latitude Pulsar Survey of
the Arecibo Sky (H5) (Foster et al. 1995)
Arecibo Survey 4 Phase I (Nice et al.
1993)
Arecibo Survey 4 Phase II (Camilo et al.
1993)
Parkes
1996)
Green
(Sayer et al. 1997)
PMPS (Manchester et al. 2001)
Swinburne
(Edwards et al. 2001)

(Manchester et al.

Southern

Survey

survey

Bank

fast

pulsar

survey

Int.

Lat.

20-cm

(II)

Fifty Years of Pulsar Candidate Selection

9

Fc (MHz)

B (MHz)

∆v (kHz)

nchans

tsamp(µs)

tobs(s)

DM Trials

408

408

430
408

408

400

390

390

1400

390

430

610/925/928/1420

1491

640

430

1434

1520

430

430

430

430

430

430

430

429

436

370

1374

1374

4

4

8
4

4

16

16

8

40

8

0.96

4/8/32

80/320

32

10

800

320

10

10

8

8

8

8

10

8

32

40

288

288

2000

?

250
2000

800

2000

2000

250

5000

250

60

125/500/1000

2

?

32
2

4

8

8

32

8

32

16
32

1000/5000

80/64

250

78.125

1000

5000

128

128

80

64

5000

50000

5600
40000

20000

16700

5556

2000

2000

2000

300
300

300

300

15

819

198
660

44.7

144

138

132

540

132

39
79

3000

3000/4500

516.625

300

67.7

78.6

1200

157.3

78.125

128

506.625

66.4

250

250

250

250

250

128

32

32

32

32

506

250

250

250

250

40

40

40

40

40

78.125

128

516.625

67.7

250

1250

78.125

3000

3000

64

256

512

96

96

250

300

256

250

125

40

157.3

134

2100

265

?

?

64
?

?

8

8

?

?

?

?
39

100

100

256

100

100

163

64

64

64

64

64

?

192

738

512

325

375

Year

1968

1969

197?
1972

1977

1977

1982-83

1983

1983-84

1983

1984-85
1985-87

1989-90

1988-90

198?

1988

1988

1989-91

1990

1991

1992

1993

1994-95

1991

1992

1991-93

1994-96

1997

1998-99

Table 2. Technical speciﬁcations of pulsar surveys conducted between 1968-1999. Here Fc (MHz) is the central observing frequency,
B (MHz) is the bandwidth, ∆v (kHz) is the channel width (to 3.d.p), nchans indicates the number of frequency channels, tsamp(µs) is the
sample frequency (to 3.d.p), and tobs(s) the length of the observation (to 1.d.p). Note * indicates more than one conﬁguration used during
the survey. The omission of a survey should be treated as an oversight as opposed to a judgement on its signiﬁcance.

S/N cut level for any stage in the search pipeline. Domain
experience usually plays a role in determining the level,
but this is often not speciﬁed and diﬃcult to quantify.
Levels used include 6σ (Damashek et al. 1978; Thornton
2013), 6.3σ (Manchester et al. 1978), 7σ (Foster et al. 1995;
Hessels et al. 2007), 7.5σ (Manchester et al. 1996), 8σ
(Stokes et al. 1986; Johnston et al. 1992; Manchester et al.
2001; Edwards et al. 2001; Burgay et al. 2006, 2013), 8.5σ
(Nice et al. 1995), 9σ (Jacoby et al. 2009; Bates et al.
2011a), and ﬁnally 9.5σ (Jacoby et al. 2009).

2013; Lee et al. 2013; Morello et al. 2014), are subject
to interpretation without precise deﬁnition (pulse width
used by Bates et al. 2011a; Thornton 2013; Lee et al.
2013; Morello et al. 2014), or
implicitly use external
algorithms which go undeﬁned (e.g. curve ﬁtting employed
by Bates et al. 2011a; Thornton 2013).
It is therefore
diﬃcult to build upon the work of others, as features
and reported results are not reproducible. Thus direct
comparisons between features are rare (Morello et al. 2014)
and impractical.

A further problem with many existing features is that
they are implementation-dependent. They are described
using concepts that can be expressed in various ways
mathematically (S/N used by Bates et al. 2011a; Thornton

4.2.2 Feature Evaluation Issues

The techniques most often used to evaluate features are
inadequate for determining how well they separate pulsar

MNRAS 000, 1–22 (2015)

10

R. J. Lyon et al.

Survey

Year

Fc (MHz)

B (MHz)

∆v (kHz)

Survey

(WAPP)

deep

northern

Galactic

Plane

(anticipated)

Parkes high-lat multibeam (Burgay et al. 2006)
Survey of the Magellanic Clouds (Manchester et al.
2006)
1.4 GHz Arecibo Survey (DM < 100) (Hessels et al.
2007)
1.4 GHz Arecibo Survey (DM > 100) (Hessels et al.
2007)
Large Area Survey for Radio Pulsars (Jacoby et al.
2009)
EGRET 56 Pulsar survey (Crawford et al. 2006)
EGRET error box survey (Champion et al. 2005)
A0327 Pilot (Deneva et al. 2013)
The Perseus Arm Pulsar Survey (Burgay et al.
2013)∗
The 8gr8 Cygnus Survey (Rubio-Herrera et al.
2007; Janssen et al. 2009)
Parkes
(Lorimer et al. 2013)
P-ALFA Survey (intial) (WAPP) (Cordes et al.
2006; Deneva et al. 2009)
P-ALFA
(Cordes et al. 2006; Deneva et al. 2009)∗
6.5 Ghz Multibeam Pulsar Survey (Bates et al.
2011a)
Green Bank 350 MHz Drift Scan (Boyles et al.
2013)
GBT350 (Spigot) (Deneva et al. 2013)
P-ALFA Survey (MOCK)
Deneva et al. 2009; Lazarus 2012)∗
GBNCC
Stovall et al. 2014)∗
Southern HTRU (LOW) (Keith et al. 2010)
Southern HTRU (MED) (Keith et al. 2010)
Southern HTRU (HIGH) (Keith et al. 2010)
A0327 (MOCK) (Deneva et al. 2013)
LPPS (Coenen et al. 2014)
LOTAS (Coenen et al. 2014)∗
Northern HTRU (LOW) (Barr et al. 2013; Ng
2012)∗
Northern HTRU (MED) (Barr et al. 2013; Ng
2012)∗
Northern HTRU (HIGH) (Barr et al. 2013; Ng
2012)∗
SPAN512 (Desvignes et al. 2012)
LOTAAS (Lofar Working Group 2013; Cooper
2014)
A0327 (PUPPI) (Deneva et al. 2013)
SUPERB (Barr 2014; Keane et al., in prep. )
GMRT High Resolution Southern Sky Survey
(MID) (Bhattachatyya 2014, 2015)
GMRT High Resolution Southern Sky Survey
(HIGH) (Bhattachatyya 2014, 2015)
FAST* (Smits et al. 2009b)
SKA** (Conﬁguration A) (Smits et al. 2009a)
SKA** (Conﬁguration B) (Smits et al. 2009a)

(Spitler et al. 2014;

(Deneva et al.

(GUPPI)

2013;

2000-03

2000-01

1374

1374

2001-02

1175

2001-02

1475

2001-02

2002-03

2003
2003

2004-09

2004

1374

1374
327
327

1374

328

2004-05

1374

2004

1420

2004-10

1420

2006-07

6591

2007

2007

350

350

288

288

100

100

288

288
25
25

288

10

288

100

300

576

50

50

3000

3000

390.625

195.313

3000

3000
48.828
48.828

3000

19.531

3000

24.414

24.414

2009-14

1375

322.6

336.042

2009-14

2010-12
2010-12
2010-12

2010
2010

2010-11

2010-14

350

1352
1352
1352
327
142
135

1360

2010-14

1360

2010-14

2012

2013

2014
2014

2014

2014

2016

2020-22
2020-22

1360

1486

135

327
1374

322

322

1315
1250
650

100

340
340
340
57
6.8
48

240

240

240

512

95

69
340

32

32

400
500
300

24.414

390.625
390.625
390.625
55.664
12.143
12.295

585.9

585.9

585.9

500

12.207

24.503
332.031

15.625

31.25

42.105

50
50

nchans
96

96

256

512

96

96
512
512

96

512

96

tsamp(µs)

125

1000

64

128

125

125
125
256

125

tobs(s)
265

8400

7200

7200

256

2100
260
60

2100

819.2

6872

125

4200

DM Trials

?

228

?

?

375

150
392
6358

183/325

488

496

96

96/1272

390.625

256

390.625

1024

64

64

134

134

3000

192

125

1055

286

2048

2048

960

4096

870
870
870
1024
560
3904

410

410

410

1024

2592

2816
1024

2048

1024

9500
9500
9500

81.92

82

140

140

?

?

65.5

120/300

5016

82

64
64
64
125
655
1300

54.61

120

4300
540
270
60

3420
1020

1500

17352/26532

?

1436
8000
6358
3487

16845/18100

406/3240

54.61

180

406/3240

54.61

64

491.52

82
32

60

30

100
64
64

90

1080

3600

60
540

1200

720

600
1800
1800

406/3240

?

7000

6358
1448

6000

6000

?
?
?

Table 3. Technical speciﬁcations of pulsar surveys conducted between 2000-present, and projected speciﬁcations for instruments under
development. X-ray pulsar searches undertaken during this period (Abdo et al. 2009; Ransom et al. 2011) are omitted. Here Fc (MHz)
is the central observing frequency, B (MHz) is the bandwidth, ∆v (kHz) is the channel width (to 3.d.p), nchans indicates the number of
frequency channels, tsamp(µs) is the sample frequency (to 3.d.p), and tobs(s) the length of the observation (to 1.d.p). Note * indicates more
than one conﬁguration used during the survey. The omission of a survey should be treated as an oversight as opposed to a judgement on
its signiﬁcance.

and non-pulsar candidates. The most common form of
evaluation is undertaken in two steps. The ﬁrst determines
the presence of linear correlations between features and
class labels (Bates et al. 2011a), the second compares the
performance of diﬀerent classiﬁers built using the features
(Bates et al. 2011a; Lee et al. 2013; Morello et al. 2014)
— the standard ‘wrapper’ method (Kohavi & John 1997;
Guyon & Elisseeﬀ 2003). This two-step evaluation considers
strong linear
classiﬁcation

and accurate

correlations

performance, characteristic of ‘good’ feature sets. However
this fails to consider the presence of useful non-linear
correlations in the data. Finally using classiﬁer outputs
to assess feature performance is known to give misleading
results
(Brown et al. 2012), as performance will vary
according to the classiﬁer used.

In order to build robust shareable features tolerant to
bias,
it is necessary to adopt standard procedures that

MNRAS 000, 1–22 (2015)

Fifty Years of Pulsar Candidate Selection

11

increase the data capture rate and thereby the total
volume of data generated during a survey. These have been
increasing over time as shown in Tables 2 and 3, a trend
likely to continue (Smits et al. 2009a,b; Keane et al. 2014).
If it does continue, it will become infeasible to store all raw
observational data permanently. It will similarly become
impractical to store all candidate data. This is perhaps
best illustrated via an example SKA scenario. Suppose
for a single observation there are 1, 500 beams and 4, 000
DM trials. If just one candidate is above the S/N selection
threshold per DM-acceleration combination,
this leads
to 4, 000 candidates produced per beam and 6 × 106 per
observation. If each candidate is 50kB in size6 then 0.3TB
of candidate data will be generated per observation. For a
hypothetical survey lasting 50 days, where there are 120
observations per day, this equates to 3.6 × 1010 individual
candidates, and 1.8PB of candidate data alone (raw data
storage requirements are much greater). In the absence of
suﬃcient archiving capacity, here it becomes important to
ﬁnd and prioritise candidates of scientiﬁc value for storage.
To achieve this, the processing of observational data will
have to be done in real-time, from candidate generation
to candidate selection. Given the real-time constraint it is
impossible to incorporate human decision making into the
candidate selection process, thus automated approaches
will have to be trusted to accurately determine which
data to retain, and which to discard. This will need to
be done at high levels of data throughput, with a strict
execution time constraint (i.e. before more data arrives).
The machine learning methods currently used for candidate
ﬁltering as described in Section 3, are not optimised
for real-time operation. Rather they are designed for
high accuracy, and as such their learning models are not
designed to be resource eﬃcient. Their memory and runtime
requirements typically grow linearly with the number of
candidates observed, whilst quadratic growth or worse is
also common. In environments with high data rates, these
ﬁlters can quickly become processing bottlenecks as their
runtime increases. Increasing data rates therefore present
two distinct problems for candidate selection: they make
it implausible to store all observational data reducing
the feasibility of oﬀ-line analysis, and restrict our use of
candidate selection approaches to those that can operate
within strict real-time constraints.

The shift to on-line processing has already occurred
in other domains in response to similar data pressures
(ATLAS Collaboration 2008). Indeed closer to home, some
pulsar/fast transient searches are already being undertaken
with real-time processing pipelines
(Thompson et al.
2011; Ait-Allal et al. 2012; Barr 2014; van Heerden et al.
2014). Real-time searches for fast radio bursts (FRBs,
Lorimer et al. 2007; Keane et al. 2012; Thornton et al.
2013) are also becoming increasingly common (Law et al.
2014; Petroﬀ et al. 2015; Karastergiou et al. 2015). These
concerns are returned to in Section 6.

6 Existing surveys already produce candidates larger than this
(Cooper 2014; Barr 2014; Bhattachatyya 2015).

Figure 5. Scatter plot showing the total number of samples per
second recorded by all pulsar surveys listed in Tables 2 & 3, as a
function of time.

reproducibility

and independent

facilitate
evaluation
within the pulsar search community. Morello et al. (2014)
began this process via the sharing of a fully labelled
data set, and by providing a clear set of design principles
used when creating their features. Here we make similar
recommendations, closely followed when designing and
evaluating the new feature set described in section 5. It is
recommended that features,

comparison and reproducibility

mathematical deﬁnitions allowing for reproducibility

• minimise biases & selection eﬀects (Morello et al. 2014)
• be survey-independent for data interoperability
• be implementation-independent, with concise
• be evaluated using a statistical framework that enables
• guard against high dimensionality (Morello et al. 2014)
• be accompanied by public feature generation code,
to facilitate co-operation and feature improvement
• be supplied in a standard data format
• be evaluated on multiple data sets to ensure robustness.

4.3 Future Processing Challenges

The number of samples per second recorded by pulsar
surveys has been increasing steadily over time, as shown
in Figure 5. This measure serves as a useful proxy for
estimating raw data throughput per second,

bits/s =  106

tsamp! · nchans · npol · nbeams · nbits,

(5)

where npol is the number of polarisations, nbits the number
of bits used to store an individual sample, and tsamp the
sampling rate expressed in microseconds. Finer frequency
resolution, faster sampling rates and longer observations,

MNRAS 000, 1–22 (2015)

12

R. J. Lyon et al.

5 NEW CANDIDATE FEATURES

Feature Description

Deﬁnition

are

and

rising

exponentially,

the largest contributor

The model introduced in Section 2.1 implies that candidate
numbers
increasingly
dominated by noise. We aim to address these problems by
ﬁnding candidate features that maximise the separation
between noise and non-noise candidates, reducing the
impact of
to high candidate
numbers. We also seek to minimise the number of features
we use, so as to avoid the problems associated with the
‘curse of dimensionality’ (Hughes 1968), which reduces
classiﬁcation performance.
In total we extracted eight
new features for this purpose from two components of the
typical pulsar candidate following the recommendations
of Section 4.2.1. These features are deﬁned in full in Table 4.

The ﬁrst four are simple statistics obtained from the
integrated pulse proﬁle (folded proﬁle). The remaining four
similarly obtained from the DM-SNR curve shown in plot
(E) in Figure 1. These features are fundamental to the
data, are dissociated with any speciﬁc hypothesis, and are
few in number. Likewise they possess no intrinsic biases,
except perhaps resolution, with respect to the number
of proﬁle/DM curve bins used to describe a candidate.
The
chosen features are also survey/implementation-
independent, provided integrated proﬁle and DM-SNR
curve data has the same numerical range, and the same
“natural” DM window7 for candidates output by diﬀerent
surveys.

”This is deﬁned as the range of DMs around the DM
that gives the highest spectral detection signiﬁcance for
the candidate. The limits of this range are deﬁned by the
change in dispersion measure that corresponds to a time
delay across the frequency band equivalent to the candidates
intial detection period.”

These features were selected by returning to ﬁrst
principles with respect to feature design. By incorporating
knowledge of the increasing trend in candidate numbers
predicted in Section 2.1, potential features were evaluated
according to how well they each separated noise and non-
noise candidates. Starting with simple lower-order statistics
as possible features (mean, mode, median etc.), the ability of
each to reject noise was considered statistically via a three-
stage process. Higher order statistics and derived features
described by Thornton (2013) were then added to the pool
of possible features, and evaluated similarly. Those achieving
the best separation, and the best classiﬁcation results when
used together with machine learning classiﬁers (see Section
6.3), were then selected for use. Thus these features were
chosen with no preconceived notions of their suitability or
expressiveness. Rather features were chosen on a statistical
basis to avoid introducing bias.

7 This is deﬁned as the range of DMs around the DM that yields
the highest spectral detection for a candidate. The limits of this
range are deﬁned by a change in the DM that corresponds to
a time delay across the frequency band equivalent to the initial
detection period of a candidate.

n

1
n

( 1

1

pi

Xi=1
i=1(pi − ¯P)2
n − 1

sPn
i=1(pi − ¯P)4)
n (Pn
i=1(pi − ¯P)2))2 − 3
n (Pn
i=1(pi − ¯P)3
n Pn
(cid:0)q 1
i=1(pi − ¯P)2(cid:1)3
n Pn
Xi=1
i=1(di − ¯D)2
n − 1

1
n

di

1

n

Pro fµ

Pro fσ

Pro fk

Pro fs

DMµ

of

the
proﬁle

Mean
integrated
P.
Standard deviation
of
the integrated
proﬁle P.
Excess
of
proﬁle P.
Skewness
integrated
P.

kurtosis
the integrated

the
proﬁle

of

Mean of the DM-
SNR curve D.

1

DMk

DMσ

Standard deviation
of
the DM-SNR
curve D.
Excess kurtosis of
the DM-SNR curve
D.

sPn
i=1(di − ¯D)4)
n (Pn
i=1(di − ¯D)2))2 − 3
n (Pn
i=1(di − ¯D)3
n Pn
(cid:0)q 1
i=1(di − ¯D)2(cid:1)3
n Pn
Table 4. The eight features derived from the integrated pulse
proﬁle P = {p1, . . . , pn}, and the DM-SNR curve D = {d1, . . . , dn}.
For both P and D, all pi and di ∈ N for i = 1, ..., n.

Skewness
the
DM-SNR curve D.

DMs

( 1

of

1

Dataset

HTRU 1

HTRU 2

LOTAAS 1

Examples Non-pulsars Pulsars

91,192

17,898

5,053

89,995

16,259

4,987

1,196

1,639

66

Table 5. The pulsar candidate data sets used.

5.1 Feature Evaluation

There are three primary considerations when evaluating
new features. A feature must i) be useful for discriminating
between the various classes of candidate, ii) maximise the
separation between them, and iii) perform well in practice
when used in conjunction with a classiﬁcation system. Three
separate evaluation procedures have therefore been applied
to the features listed in Table 4. The ﬁrst two forms of
evaluation are presented in the section that follows, whilst
classiﬁcation performance is described in Section 6.3, to
allow for a comparison between standard classiﬁers and
our stream algorithm described in Section 6. As features
in themselves are without meaning unless obtained from
data, we ﬁrst describe the data sets used during our analysis,
before presenting details of the evaluation.

5.1.1 Data

Three separate datasets were used to test the discriminating
capabilities of our features. These are summarised in
Table 5. The ﬁrst data set (HTRU 1) was produced by
Morello et al. (2014). It is the ﬁrst labelled8 candidate

8 Containing correctly labelled pulsar and non-pulsar candidates.

MNRAS 000, 1–22 (2015)

dataset made publicly available. It consists of 1, 196 pulsar
and 89, 995 non-pulsar candidates,
in pulsar hunter xml
ﬁles (.phcx ﬁles). These candidates were generated from a
re-processing of HTRU Medium Latitude data, using the
GPU-based search pipeline peasoup (Barr et al., in prep.).
The pipeline searched for pulsar signals with DMs from 0
to 400 cm−3pc, and also performed an acceleration search
between −50 to +50 m s−2. The HTRU 1 candidate sample
possesses varied spin periods, duty cycles, and S/Ns.

In addition two further data sets were used during
is made available for
this work. The ﬁrst (HTRU 2),
analysis9. It comprises 1, 639 pulsar and 16, 259 non-pulsar
candidates. These were obtained during an analysis of
HTRU Medium Latitude data by Thornton (2013), using
a search pipeline that searched DMs between 0 to 2000
cm−3pc. The pipeline produced over 11 million candidates
in total. Of these 1, 610 pulsar and 2, 592 non-pulsar
candidates were manually labelled by Bates et al. (2012) and
Thornton (2013). These were combined with an additional
13, 696 candidates, sampled uniformly from the same data
set according to observational session and month. These
additional candidates were manually inspected and assigned
their correct labels. Together the two sets of
labelled
candidates form HTRU 2. It contains 725 of the known
1,108 pulsars in the survey region (Levin 2012), along with
re-detections and harmonics. HTRU 2 also contains noise,
along with strong and weak forms of RFI. The third and
ﬁnal candidate data set (LOTAAS 1), was obtained during
the LOTAAS survey (Lofar Working Group 2013; Cooper
2014) and is currently private. The data set consists of 66
pulsar and 4, 987 non-pulsar candidates. Feature data was
extracted from these data sets using a new custom written
python tool, the Pulsar Feature Lab. This tool is made
available for use10.

5.1.2 General Separability

The discriminating capabilities of the new features when
applied to HTRU 1, are summarised in Figure 6 via
standard box and whisker plots. For each feature there are
two distinct box plots. A coloured box plot representing the
feature distribution of known pulsars, and a plain black box
plot showing the feature distribution of non-pulsars. As the
features have numerical ranges which diﬀer signiﬁcantly,
feature data was scaled to within the range [0, 1] prior to
plotting. This enables a separability comparison on the
same scale. For each individual feature, the median value
of the negative distribution was also subtracted. Thus the
plots are centred around the non-pulsar median, allowing
diﬀerences between pulsar and non-pulsar distributions to
be seen more clearly.

The visualisation shows there to be a reasonable amount
of separation between the pulsar and non-pulsar feature
distributions. This is initial evidence for the usefulness

Feature

Pro fµ
Pro fσ
Pro fk
Pro fs
DMµ
DMσ
DMk
DMs

-0.310

-0.084

0.545

0.601

-0.174

0.059

0.178

0.190

-0.673

-0.364

0.792

0.710

0.401

0.492

-0.391

-0.230

Avg. rpb

-0.512

-0.266

0.719

0.697

0.175

0.287

0.074

-0.508

-0.337

0.774

0.762

0.275

0.282

0.426

Fifty Years of Pulsar Candidate Selection

13

Dataset

HTRU 1 HTRU 2

LOTAAS 1

-0.211

-0.096

Table 6. The point-biserial correlation coeﬃcient for each feature
on the three test data sets.

of these features11 but only on a visual
level. Thus we
applied a two-tailed students t-test to feature data, in order
to determine if the means of the pulsar and non-pulsar
distributions were signiﬁcantly diﬀerent. A rejection of the
null hypothesis (no signiﬁcant diﬀerence) would provide
statistical evidence for the separability indicated in the box
plots. For all data sets, there was a statistically signiﬁcant
diﬀerence between the pulsar and non-pulsar distributions
at α = 0.01. A non-parametric Wilcoxon signed-rank test
(Wilcoxon 1945), was also undertaken with no diﬀerence
in results. This suggested the features to be worthy of
further, more rigorous investigation. The next step involved
determining the extent of any linear correlation between the
features and the target class variable.

5.1.3 Correlation Tests

the Pearson

(Pearson

The point-biserial correlation coeﬃcient rpb (Das Gupta
1960), measures the linear correlation between variables,
when the target variable is dichotomous. It is equivalent
to
1895;
Guyon & Elisseeﬀ 2003), though it is better suited to
candidate data, as it naturally assumes a discrete target
label y ∈ Y as described previously. The value of rpb for a
data sample is given by,

product moment

rpb =

¯x1 − ¯x2

σ

· r n2 · n1
n · (n − 1)

,

(6)

where n is the total number of samples, ¯x1 and ¯x2 the mean
value of groups one and two respectively, and σ the sample
standard deviation. Much like Pearson’s product moment,
the coeﬃcient obtains a value in the range [−1, 1]. A positive
correlation implies that moving from group one to group
two, is associated with an increase in the output variable
(high values tend to co-occur with group two). A negative
correlation implies that moving from group one to group
two, is associated with a decrease in the output variable.
Table 6 shows the correlation between the eight features and
the target class variable, for the three sample data sets. The
average (mean) correlation has also been computed. Since
rpb is non-additive, this average had to be determined using

9 https://dx.doi.org/10.6084/m9.figshare.3080389.v1 .
10 http://dx.doi.org/10.6084/m9.figshare.1536472

11 Similar levels of separability were observed when the same plot
was produced for both the HTRU 2 and LOTAAS 1 data sets.

MNRAS 000, 1–22 (2015)

14

R. J. Lyon et al.

Figure 6. Box plots (median and IQR) showing the linear separability of our new features. Feature data was extracted from 90, 000
labelled pulsar candidates produced by Morello et al. (2014), via the pulsar feature lab. There are two box plots per feature. The
coloured boxes describe the feature distribution for known pulsars, where corresponding coloured dots represent extreme outliers. Those
box plots in black describe the RFI/noise distribution. Note that the data of each feature was scaled to the interval [0, 1], before the
median of the RFI/noise distribution was subtracted to centre the non-pulsar plots on zero.

Fisher’s Z transformation (Fisher 1921),

z =

1
2

ln(cid:16)

1 + rpb

1 − rpb(cid:17).

(7)

Using Equation 7 the corresponding correlations of each
feature on the three datasets were transformed into additive
z values, summed, and the mean obtained. The mean z value
was then transformed back into a meaningful correlation
using the inverse of the Fisher-Z,

rpb =

e2·z − 1
e2·z + 1

.

(8)

The data in Table 6 shows there to be three features
that on average, exhibit strong correlations (> |0.5|). These
include the mean, excess kurtosis, and skew of the integrated
pulse proﬁle. All features exhibit a weak correlation on at
least one data set, which is stronger on others. The lowest
correlation witnessed on HTRU 1, between the standard
deviation of the DM-SNR curve and the target variable,
performed much better on HTRU 2. This is probably due
to diﬀerences between the DM ranges of candidates in each
dataset (0-400 cm−3pc for HTRU 1 and 0-2000 cm−3pc for
HTRU 2). Irrespective of this no features are completely
uncorrelated. Whilst there is variation in the eﬀective linear
separability of features across all data sets, it is surprising
that such simple measures possess discriminatory ability at
all. However, caution must be used when judging features

based upon their linear correlations. Those features which
possess linear correlations close to zero, may possess useful
non-linear correlations which are harder to discern. Thus
we turn to the tools of information theory (MacKay 2002;
Guyon & Elisseeﬀ 2003; Brown 2009) to look for such
relationships.

5.1.4 Information Theoretic Analysis

Information theory uses the standard rules of probability to
learn more about features and their interactions. Features
which at ﬁrst appear information-poor, may when combined
with one or more other features, impart new and meaningful
knowledge (Guyon & Elisseeﬀ 2003). Applying this theory
to candidate features enables their comparison, evaluation,
and selection within an established framework for the ﬁrst
time.

Information theory describes each feature Xj in terms
of entropy. Entropy is a fundamental unit of information
borrowed from Thermodynamics by (Shannon & Weaver
1949),
in the
distribution of Xj.

that quantiﬁes the uncertainty present

MNRAS 000, 1–22 (2015)

Fifty Years of Pulsar Candidate Selection

15

Feature

HTRU 1

Dataset

HTRU 2

LOTAAS 1

Avg.

H(Xj)

I(Xj; Y)

H(Xj)

I(Xj; Y)

H(Xj)

I(Xj; Y)

H(Xj)

I(Xj; Y)

Pro fk
Pro fµ
Pro fs
DMk
Pro fσ
DMσ
DMµ
DMs

1.062

1.993

0.545

1.293

2.011

2.231

1.950

0.138

0.073

0.065

0.063

0.021

0.007

0.004

0.028

0.013

1.549

2.338

0.523

2.295

1.972

2.205

0.835

1.320

0.311

0.269

0.245

0.146

0.115

0.171

0.114

0.041

0.948

1.986

0.114

1.842

2.354

0.013

0.015

2.243

0.088

0.085

0.074

0.083

0.061

0.006

0.008

0.045

1.186

2.106

0.394

1.810

2.112

1.483

0.933

1.233

0.157

0.139

0.127

0.083

0.061

0.060

0.050

0.033

Table 7. The entropy H(Xj), and mutual information I(Xj; Y) of each feature. Features are ranked according to their mutual information
content with respect to the class label Y. Higher mutual information is desireable.

The entropy of Xj is deﬁned as,

H(Xj) = −Xx∈Xj

P(x)log2P(x),

one variable provides about another (Brown et al. 2012). It
is desirable for features to possess high MI with respect to
pulsar/non-pulsar labelling.

(9)

where x corresponds to each value that Xj can take, and
P(x) the probability of x occurring. If a given value of x
occurs with a high probability, then the entropy of Xj is low.
Conceptually this can be understood to mean that there
is little uncertainty over the likely value of Xj. Likewise if
all possible values of a feature are equally likely, then there
is maximum uncertainty and therefore maximum entropy12.
Whilst entropy can provide an indication of the uncertainty
associated with a feature variable, its main usefulness arises
when conditioned on the target variable (true class label) Y.
The conditional entropy of Xj given Y is,

H(Xj|Y) = −Xy∈Y
where P(x|y) is the probability of x given y such that,

P(x|y)log2P(x|y),

p(y)Xx∈Xj

(10)

P(x ∩ y)
P(y)

.

(11)

P(x|y) =
This quantiﬁes the amount of uncertainty in Xj once the
value of Y is known. Using Equations 9-11 it is possible to
deﬁne the mutual information (MI, Brown et al. 2012)13
between the feature Xj, and the class label Y. This can
be considered another method of measuring the correlation
between a feature and the target variable which detects non-
linearities. Mutual information is deﬁned as,
I(Xj; Y) = H(Xj) − H(Xj|Y).
The MI expresses the amount of uncertainty in Xj removed
by knowing Y. If I(Xj|Y) = 0 then Xj and Y are independent.
Whereas if I(Xj|Y) > 0, then knowing Y helps to better
understand Xj. As mutual
symmetric,
knowing Xj equivalently helps to better understand Y. Thus
MI is often described as the amount of information that

information is

(12)

The MI metric helps identify relevant features, by
enabling them to be ranked according to those that result
in the greatest reduction of uncertainty. It is one of
the most common ﬁlter methods (Kohavi & John 1997;
Guyon & Elisseeﬀ 2003; Brown et al. 2012) used for feature
selection (Brown 2009). The entropy and MI of our features
are listed in Table 7, ranked according to their mean
MI content, where higher MI is desirable. To produce
this table feature data was discretised,
for reasons set
out by Guyon & Elisseeﬀ (2003), enabling use with the
information-theoretic feast14 and mitoolbox15 toolkits
developed by Brown et al. (2012). The data was discretised
using 10 equal-width bins using the ﬁlters within the weka
data mining tool16. Simple binning was chosen ahead of
more advanced Minimum Description Length (MDL) based
discretization procedures (Fayyad & Irani 1993), to simplify
feature comparisons.

The four features extracted from the integrated proﬁle
contain the largest amounts of MI. These are the most
relevant features. The MI content of
features extracted
from the DM-SNR is much lower. It is tempting therefore
to write oﬀ these low scoring features since their linear
correlation coeﬃcients were also shown to be low in Section
5.1.2. However whilst mutual information indicates which
features are relevant,
it is entirely possible for these to
contain redundant information (Guyon & Elisseeﬀ 2003).
Thus choosing the most relevant features may not produce
optimal feature subsets (Kohavi & John 1997), since these
could contain the same information. The joint mutual
information criterion (JMI, Yang & Moody 1999) can detect
and minimise such redundancy (Guyon & Elisseeﬀ 2003;
Brown et al. 2012). Given a set of features the JMI selects
those with complementary information, starting with the
information X1. In
feature possessing the most mutual

12 Max entropy for a feature with n possible values is given by
log2(n).
13 Also known as information gain, or a speciﬁc case of the
Kullback-Leibler divergence (MacKay 2002).

14 http://www.cs.man.ac.uk/~gbrown/fstoolbox.
15 http://www.cs.man.ac.uk/~pococka4/MIToolbox.html .
16 http://www.cs.waikato.ac.nz/ml/weka.

MNRAS 000, 1–22 (2015)

16

R. J. Lyon et al.

Dataset

HTRU 1 HTRU 2

LOTAAS 1

Avg. Rank

goal is realised, new stream-ready selection approaches are
required.

6.1 Unsuitability of Existing Approaches

Supervised machine learning methods induce classiﬁcation
models from labelled training sets (Mitchell 1997; Bishop
2006). Provided these are large, representative of rare and
majority class examples, and independent & identically
distributed (i.i.d.) to the data being classiﬁed (Bishop 2006)
good classiﬁcation performance can be expected to result.
However the notion of a training set does not exist within
a data stream. There are instead two general processing
models used for learning.

• Batch processing model: at time step i a batch b of
n unlabelled instances arrives, and is classiﬁed using some
model trained on batches b1 to bi−1. At time i +1 labels arrive
for batch bi, along with a new batch of unlabelled instances
bi+1 to be classiﬁed.

• Incremental processing model: a single data
instance arrives at time step i deﬁned as Xi, and is classiﬁed
using some model trained on instances X1 to Xi−1. At time i+1
a label arrives for Xi, along with a new unlabelled instance
Xi+1 to be classiﬁed.

In both models learning proceeds continually, as labelled
data becomes available. This allows for adaptive learning.
Standard supervised classiﬁers simply cannot be trained in
this way. Even if they could, the CPU and memory costs
of their training phases make them impractical for streams
(Gaber 2012). This was recognised by Zhu et al. (2014)
with respect to their pics system17.

Given these problems how should candidate selection
be addressed in streams? One may consider training an
existing supervised candidate classiﬁer oﬀ-line, which could
then be applied to a candidate stream. This is a plausible
approach, provided the classiﬁer processes each example
before the next one arrives. For this to be viable, the
classiﬁer must also be trained with data that is i.i.d. with
respect to the data in the stream. However data streams
are known to exhibit distributional shifts over varying time
periods. For example a changing RFI environment can
exhibit shifts over both short (minutes/hours), and/or long
(days/weeks/years) time-scales. In either case the shifts
cause violations of the i.i.d. assumption, a phenomena known
as ‘concept drift’ (Widmer & Kubat 1996; Gaber et al.
2005). To mitigate the impact of drift, adaptive algorithms
able to learn from distributional changes are required,
as pre-existing training data no longer characterises the
post-drift data distribution (Lyon 2015). Such algorithms
must be capable of completely reconstructing their internal
learning models in an eﬃcient manner per each signiﬁcant
distributional shift. Standard supervised learning models
are ‘static’,
i.e. they remain unchanged once learned.
A static classiﬁer applied to streaming data subject to
drifts, will exhibit a signiﬁcant deterioration in classiﬁcation
performance over time (Aggarwal et al. 2004). This makes
standard supervised learning unsuitable for data streams.

17 Zhu et al. (2014) indicated eﬀorts are under way to rectify this.

MNRAS 000, 1–22 (2015)

Feature

Pro fk
Pro fµ
DMσ
Pro fs
DMk
Pro fσ
DMµ
DMs

1

3

2

4

6

7

5

8

1

3

2

4

6

5

7

8

1

3

8

6

2

5

7

4

1

3

4

4.7

4.7

5.7

6.4

6.7

Table 8. The joint mutual information rank of each feature.
Features are ranked according to their average JMI across the
three test data sets, where a lower rank is better.

‘forward selection’ (Kohavi & John 1997; Guyon & Elisseeﬀ
2003), a common method of feature selection, a greedy
iterative process is used to decide which additional features
are most complementary to X1, using the notion of the JMI
score,

JMI(Xj) = XXk∈F

I(XjXk; Y),

(13)

where XjXk can be understood as a joint probability, and F
is the set of features. The iterative process continues until
a desired number of features are selected. This produces a
feature set that minimises redundancy. Alternatively, if the
desired number of features to select equals the total number
of those available, features are ranked according to the
JMI. Using the JMI in this manner, our features have been
ranked such that a lower rank is preferable. Upon applying
this criterion poor features are revealed to be useful. This
is shown in Table 8 which demonstrates that features
extracted from the DM-SNR curve impart complementary
information, and are therefore ranked higher than proﬁle
features which possess greater mutual
information. The
standard deviation of the DM-SNR curve in particular, is
ranked as the 2nd ‘best’ feature on two of the three test
datasets. Likewise the excess kurtosis and skewness of the
DM-SNR curve, are the second and fourth ‘best’ features for
LOTAAS data respectively. In the next section we describe
a new data stream classiﬁcation algorithm, which takes
advantage of these features.

6 STREAM CLASSIFICATION

Data streams are quasi-inﬁnite sequences of information,
which are temporally ordered and indeterminable in
size (Gaber et al. 2005; Lyon et al. 2013, 2014). Data
streams are produced by many modern computer systems
(Gaber et al. 2005) and are likely to arise from the
increasing volumes of data output by modern radio
telescopes, especially the SKA. However many of the
eﬀective supervised machine learning techniques used for
candidate selection do not work with streams (Lyon et al.
2014). Adapting existing methods for use with streams
is challenging,
it remains an active goal of data mining
research (Yang & Wu 2006; Gaber et al. 2007). Until that

...

...

...

...

...

...

Figure 7. An overview of how a streaming decision tree partitions
the data space to derive a classiﬁcation. Each candidate is passed
down the tree, and tested at each node it reaches including
the root. Each node test outcome determines which branch the
candidate continues down, until it reaches a leaf at the bottom of
the tree. The tree shown here assigns the class labels A, B and C
to examples reaching the leaf nodes.

In the next section we describe our new ‘intelligent’ data
stream classiﬁer, which overcomes these deﬁciencies.

6.2 Gaussian-Hellinger Very Fast Decision Tree

The Gaussian-Hellinger Very Fast Decision Tree (GH-
VFDT) is an incremental stream classiﬁer, developed
speciﬁcally for the candidate selection problem (Lyon et al.
2014). It is a tree-based algorithm based on the Very Fast
Decision tree (VFDT) developed by Hulten et al. (2001).
It is designed to maximise classiﬁcation performance on
candidate data streams, which are heavily imbalanced in
favour of the non-pulsar class. It is the ﬁrst candidate
selection algorithm designed to mitigate the imbalanced
learning problem (He & Garcia 2009; Lyon et al. 2013,
2014), known to reduce classiﬁcation accuracy when one
class of examples (i.e. non-pulsar) dominates the other.
The algorithm uses tree learning (Mitchell 1997) to achieve
this, whereby the data is partitioned using feature split
point tests (see Figures 7 and 8) that aim to maximise
the separation of pulsar and non-pulsar candidates. This
involves ﬁrst choosing the variable that acts as the best
class separator, and then ﬁnding a numerical threshold ‘test
point’ for that variable that maximises class separability.

The tree is ‘grown’ with labelled data to determine
optimal splits, using the Hoeﬀding bound (Hoeﬀding 1963).
The bound is used to choose statistically with high
probability, those split points that would have been selected,

MNRAS 000, 1–22 (2015)

Fifty Years of Pulsar Candidate Selection

17

if given access to all training data in advance (as in the
traditional learning scenario). By calculating the observed
mean ¯Xj of a feature, the bound is able to determine with
conﬁdence 1 − δ (where δ is user supplied), that the true
mean of the feature is at least ¯Xj − ǫ where,
ǫ = r R2 ln(1/δ)

(14)

2n

,

and R2 is the feature range squared. This ensures that
the statistically optimal split is always chosen. A split is
not made until enough examples in the stream have been
seen, i.e. until there is enough evidence to advocate its use.
The quality of the splits, and therefore the accuracy of the
approach, improve over time. This is because the model
of the underlying data distributions improves as more
examples are observed. The performance of the algorithm
approaches that of a non-streamed classiﬁer as the number
of examples observed approaches inﬁnity (Hulten et al.
2001). The tree is also able to adapt to change (Lyon 2015)
by updating the data distributions with each observed
labelled example. Once there is evidence to suggest an
alternative split point is better than one in use, the tree
replaces the sub-optimal split. This is achieved by pruning
the branch of the tree containing the sub-optimal split, and
replacing it with a new branch which begins to ‘grow’ from
the new split point.

The key feature of the GH-VFDT, is its use of the
skew-insensitive Hellinger distance measure (Hellinger 1909;
Nikulin 2001) to evaluate split points during learning. This
measure makes the classiﬁer robust to the imbalanced
learning problem, preventing the classiﬁer from becoming
biased towards the abundant non-pulsar class (Lyon 2015).
By modelling each feature distribution as a Gaussian,
the Hellinger distance between the pulsar and non-pulsar
distributions can be measured. If Q and N are the pulsar
and non-pulsar distributions respectively, the distance for a
single feature is given by,

dH(Q, N) = vut1 − s 2σ1σ2

+ σ2
2

σ2
1

1
4

e−

(µ1−µ2)2
σ2
1 +σ2
2 ,

(15)

where Q has mean µ1, variance σ2
1 and standard deviation
σ1, with N deﬁned similarly. The goal of split evaluation
is to choose the split which maximises the Hellinger
distance, maximising pulsar and non-pulsar separation.
This approach requires that only the mean and standard
deviation of each feature be known. This signiﬁcantly
reduces the GH-VFDT memory overheads, as knowledge
of the entire feature distribution(s) is not required for
learning. Therefore the runtime and memory requirements
of the algorithm are sub-linear with respect to the number
of examples processed, and grow in only constant time for
each new node added to the tree. This makes the algorithm
suitable for use upon very high throughput data streams
such as those described in Section 4.3.

A complete outline of the GH-VFDT is given in
Algorithm 1. On line 7 tree statistics used to compute
the Hellinger distance are updated.
In particular the
running mean and standard deviation maintained at each
leaf, for feature j, and class k are updated. The call to

18

R. J. Lyon et al.

Figure 8. An overview of how a decision tree partitions the data space using binary split point ‘tests’ at each node. The best feature
variable at each node is ﬁrst determined, then an optimal numerical split point threshold chosen. Candidates with feature values below
the threshold are passed down the left hand branch of the tree, and possibly subjected to further split tests. Similarly for candidates
with feature values above the threshold, except these are passed down the right hand branch. Eventually candidates reach the leaf nodes,
where they are assigned class labels.

Algorithm 1 Gaussian Hellinger Very Fast Decision Tree
Require: An input stream S = {..., (Xi, yi), ...}, such that each Xi
is a candidate, Xj
i its j-th feature and yi its class label. The
parameter δ ∈ (0, 1) is the conﬁdence desired, and τ ∈ (0, 1) a
parameter which if set, prevents split point ties.

Let DT be a decision tree with leaf l1
for i ← 1 to |S| do

1: procedure GH-VFDT(S , δ, τ)
2:
3:
4:
5:
6:

l ← sort(Xi, yi)
k ← yi
for j ← 1 to |Xj

i| do
update µjk(l, Xj
i)
update σjk(l, Xj
i)

⊲ For each stream instance.
⊲ Sort instance Xi to leaf l.
⊲ Get class.
⊲ For each feature.

⊲ Update observed µ at leaf.

⊲ Update observed σ at leaf.

Label l with majority class of instances seen at l
if all Xi seen at l don’t belong to same class then

⊲ Best feature.
⊲ 2nd best feature.
⊲ For each feature.

⊲ From equation 15.

Fa ← null
Fb ← null
for j ← 1 to |Xj
i| do
dist ← dH(Xj
i)
Fa, Fb ← getBest(dist, Xj
i)
ǫ = q R2 ln(1/δ)
if dH(Fa) − dH(Fb) > ǫ or ǫ < τ then

2n

⊲ Hoeﬀding bound.

Replace l with new leaf that splits on Fa
for each branch of split do

Add new leaf lm
for k ← 1 to |S| do
µijk(lm) ← 0
σijk(lm) ← 0

for j ← 1 to |Xi| do

⊲ For each class.
⊲ For each Xj
i.

return DT

7:

8:

9:
10:
11:
12:
13:

14:

15:

16:
17:
18:
19:
20:
21:
22:
23:
24:

25:

a data stream containing 10,000 non-pulsar candidates for
every legitimate pulsar (HTRU data obtained by Thornton
(2013)), it raised the recall rate from 30 to 86 per cent
(Lyon et al. 2014). This was achieved using candidate data
described using the features designed by Bates et al. (2012)
and Thornton (2013). A full implementation of the algorithm
can be found on-line for public use18.

6.3 Classiﬁcation Performance

Existing features and algorithms have been evaluated
predominantly in terms of classiﬁcation accuracy. Such
an analysis considers candidate selection as a binary
classiﬁcation problem, whereby candidates arising from
pulsars are considered positive (+), and those from non-
pulsars negative (-). There are then four possible outcomes
for an individual classiﬁcation decision. These outcomes are
summarised in Table 10 and are evaluated using standard
metrics such as those outlined in Table 9. The goal of
classiﬁcation is to minimise the number of false positives,
whilst maximising the true positives. Features in this domain
are most often chosen according to how well they maximise
classiﬁer recall (the fraction of legitimate pulsar candidates
correctly classiﬁed) and speciﬁcity (fraction of non-pulsar
candidates correctly classiﬁed)19. Those classiﬁers with high
recall and speciﬁcity exhibit high accuracy, often interpreted
to mean that underlying features are good discriminators.

This form of evaluation enables approaches to be
tested quickly, with readily interpretable results. However
using classiﬁer performance as a proxy to measure feature-
separability tests the classiﬁcation system used as much
as the features under investigation (Brown et al. 2012).
The choice of classiﬁer can inﬂuence the outcome of the
evaluation giving misleading results. Evaluation metrics
themselves can also be misleading. Pulsar data sets are
imbalanced with respect to the total number of pulsar and

getBest(dist, Xj
i) returns the best and second best features
found at a leaf. This is achieved by choosing those that
maximise the Hellinger distance via an iterative process. On
line 18 tree split points are ﬁrst generated and evaluated.
Here data is discretized using 10 equal-width bins, and a
binary split point chosen.

This approach has already been shown to signiﬁcantly
improve recall rates for pulsar data, above the levels
achieved by established stream classiﬁers. When applied to

18 https://github.com/scienceguyrob/GHVFDT
19 The approaches in Section 3.4 evaluate in this manner.

MNRAS 000, 1–22 (2015)

Statistic Description

Accuracy

Measure
overall
classiﬁcation accuracy.

of

Deﬁnition

(T P+T N)

(T P+FP+FN+T N)

False
positive
rate
(FPR)

G-Mean

Precision

Recall

F-Score

Speciﬁcity

Fraction of negative
instances
incorrectly
labelled positive.

Imbalanced
data
metric describing the
ratio between positive
and negative accuracy.

Fraction of
instances
positive.

retrieved
are

that

Fraction
instances
retrieved.

of

positive
are

that

FP

(FP+T N)

q T P
T P+FN × T N

T N+FP

T P

(T P+FP)

T P

(T P+FN)

Measure of accuracy
that
both
precision and recall.

considers

Fraction of negatives
correctly identiﬁed as
such.

precision×recall
precision+recall

2 ×

T N

(FP+T N)

Table 9. Standard evaluation metrics for classiﬁer performance.
True Positives (TP) are those candidates correctly classiﬁed as
pulsars. True Negatives (TN) are those correctly classiﬁed as not
pulsars. False Positives (FP) are those incorrectly classiﬁed as
pulsars, False Negatives (FN) are those incorrectly classiﬁed as
not pulsars. All metrics produce values in the range [0, 1].

Predicted

-

+

Actual

-

True Negative (TN)

False Positive (FP)

+ False Negative (FN)

True Positive (TP)

Table 10. Confusion matrix describing the outcomes of binary
classiﬁcation.

non-pulsar candidates within them (Lyon et al. 2013, 2014).
Thus for data sets consisting of almost entirely non-pulsar
examples, high accuracy can often be achieved by classifying
all candidates as non-pulsar. In these situations it is an
unhelpful metric.

To overcome these possible sources of inaccuracy when
evaluating the GH-VFDT, we make use of the G-mean
metric (He & Garcia 2009). This describes the ratio between
positive and negative accuracy, a measure insensitive to
the distribution of pulsar and non-pulsar examples in test
data sets. Additionally we employ multiple classiﬁers in our
evaluation which diﬀer greatly in terms of their internal
learning models. This allows for a more general view of
feature performance in practice to be revealed. This is also
useful for evaluating the performance of the GH-VFDT
with respect to standard static supervised classiﬁers, which
are at an advantage in such tests. Here we make use of
four standard classiﬁers found in the weka tool. These
include the decision tree algorithm C4.5 (Quinlan 2007),
MLP neural network (Haykin 1999), a simple probabilistic
classiﬁer Na¨ıve Bayes (NB, Bishop 2006), and the standard

MNRAS 000, 1–22 (2015)

Fifty Years of Pulsar Candidate Selection

19

linear soft-margin support vector machine (SVM, Cortes
1995).

6.3.1 GH-VFDT Classiﬁcation Evaluation Procedure

Feature data was extracted from the data sets listed in
Table 5, and then independently sampled 500 times. Each
sample was split into test and training sets. For HTRU 1 &
2, sampled training sets consisted of 200 positive and 200
negative examples, with remaining examples making up the
test sets. LOTAAS 1 training sets contained 33 positive
examples and 200 negative, with remaining examples
similarly making up the test sets. Each classiﬁer (ﬁve in
total) was then trained upon, and made to classify each
independent sample, therefore there were 3× 500× 5 = 7, 500
tests in total. The performance of each algorithm per data
set was then averaged to summarise overall performance. To
evaluate classiﬁer performance results, one-factor analysis
of variance (ANOVA) tests were performed, where the
algorithm used was the factor. Tukey’s Honestly Signiﬁcant
Diﬀerence (HSD) test (Tukey 1949), was then applied
to determine if diﬀerences in results were statistically
signiﬁcant at α = 0.01. The full results are shown in
Table 11.

These results indicate that it is possible to achieve high
levels of classiﬁer performance using the features described
in Section 5. What’s more, the classiﬁcation results are
consistent across all three data sets. Recall rates on all three
test data sets are high, with 98 per cent recall achieved
by the MLP on HTRU 1 and LOTAAS 1 data. High levels
of accuracy were observed throughout testing and G-mean
scores on HTRU 1 were particularly high. The algorithms
also exhibited high levels of speciﬁcity and generally low
false positive rates. The exception being the 6 per cent
false positive rate achieved by the NB classiﬁer on HTRU
2 data. This outcome is unremarkable for NB, the simplest
classiﬁer tested, as the HTRU 2 data set is populated with
noise and borderline candidates. Thus we suggest that these
represent the ﬁrst survey independent features developed
for the candidate selection problem.

The results also show that the GH-VFDT algorithm
in terms
consistently outperformed the static classiﬁers,
of both speciﬁcity and false positive return rate. This
is a highly desirable outcome for a stream classiﬁer,
since assigning positive labels too often will return an
unmanageable number of candidates. The classiﬁer does not
always predict ‘non-pulsar’ to give this result. It is precise,
achieving the best precision on two out of the three data sets.
G-mean and recall rates were also high for the GH-VFDT,
the latter reaching 92.8 per cent on HTRU 1 data. The recall
rates are lower on the remaining two data sets. However it
is worth noting that these data sets are considerably smaller
than HTRU 1. This is important, since the performance of
the GH-VFDT (and of other stream algorithms) improves
as more examples are observed. The lower levels of recall
on HTRU 2 and LOTAAS 1 are therefore to be expected
given the smaller dataset size. In terms of the usefulness
of this algorithm for SKA data streams, the GH-VFDT
returns consistently less than 1 per cent of candidates as false
positives. This greatly reduces the quantity of candidates

20

R. J. Lyon et al.

Dataset

Algorithm G-Mean F-Score Recall Precision Speciﬁcity FPR Accuracy

HTRU 1

HTRU 2

C4.5
MLP
NB
SVM

0.962∗
0.976
0.925
0.967
GH-VFDT 0.961∗
0.926
0.931
0.902
0.919
GH-VFDT 0.907

C4.5
MLP
NB
SVM

LOTAAS 1

C4.5
MLP
NB
SVM

0.969
0.988
0.977
0.949
GH-VFDT 0.888

0.839∗
0.891
0.837∗
0.922
0.941

0.740
0.752
0.692
0.789
0.862

0.623
0.846∗
0.782
0.932
0.830∗

0.961
0.976
0.877
0.947
0.928

0.904
0.913
0.863
0.871
0.829

0.948
0.979
0.959
0.901
0.789

0.748
0.820
0.801
0.898
0.955
0.635∗
0.650∗
0.579
0.723
0.899

0.494
0.753
0.673
0.966
0.875

0.962
0.975
0.975
0.988
0.995
0.949∗
0.950∗
0.943
0.969
0.992

0.991
0.998
0.996
0.999∗
0.999∗

0.038
0.025∗
0.025∗
0.012
0.005
0.051∗
0.050∗
0.057
0.031
0.008

0.009
0.002
0.004
0.001∗
0.001∗

0.962
0.975
0.965
0.984
0.988
0.946∗
0.947∗
0.937
0.961
0.978

0.990
0.997∗
0.996
0.999
0.998∗

Table 11. Results obtained on the three test data sets. Bold type indicates the best performance observed. Results with an asterisk
indicate no statistically signiﬁcant diﬀerence at the α = 0.01 level.

to be analysed. The GH-VFDT also classiﬁes candidates
rapidly. It classiﬁed candidates at a rate of ∼ 70, 000 per
second using a single 2.2 GHz Quad Core mobile CPU
(Intel Core i7-2720QM Processor) when applied to a larger
sample of HTRU 2 data consisting of 11 million examples. A
discussion of the statistics of the pulsars incorrectly classiﬁed
by the new methods will be discussed in a future paper.

will consider how these compare to those used previously,
and determine if combining them with those already in use
is worthwhile. Thus for the time being it is advisable to
construct as large a set of features as possible, and use the
tools described herein to select feature sets statistically.

7 SUMMARY

This paper has described the pulsar candidate selection
process, and contextualised its almost ﬁfty year history.
During this time candidate selection procedures have been
continually adapting to the demands of
increased data
capture rates and rising candidate numbers, which has
proven to be diﬃcult. We have contributed a new solution
to these problems by demonstrating eight new features
useful for separating pulsar and non-pulsar candidates, and
by developing a candidate classiﬁcation algorithm designed
to meet the data processing challenges of the future.
Together these enable a high fraction of legitimate pulsar
candidates to be extracted from test data, with recall rates
reaching almost 98 per cent. When applied to data streams,
the combination of these features and our algorithm enable
over 90 per cent of
legitimate pulsar candidates to be
recovered. The corresponding false positive return rate is
less than half a percent. Thus together these can be used
to signiﬁcantly reduce the problems associated with high
candidate numbers which make pulsar discovery diﬃcult,
and go some way towards mitigating the selection problems
posed by next generation radio telescopes such as the SKA.
The combination of these features and our classiﬁcation
algorithm has already proven useful, aiding in the discovery
of 20 new pulsars in data collected during the LOFAR
Tied-Array All-Sky Survey (Cooper 2014). Details of these
discoveries will be provided elsewhere, demonstrating the
utility of our contributions in practice.

The features described in this paper are amongst the
most rigorously tested in this domain. However whilst
we advocate their use on statistical grounds, we do not
demonstrate their superiority to other features. Future work

8 ACKNOWLEDGEMENTS

This work was supported by grant EP/I028099/1 from the
UK Engineering and Physical Sciences Research Council
(EPSRC). HTRU 2 data was obtained by the High
Time Resolution Universe Collaboration using the Parkes
Observatory, funded by the Commonwealth of Australia
and managed by the CSIRO. LOFAR data was obtained
with the help of the DRAGNET team, supported by ERC
Starting Grant 337062 (PI Hessels). We would also like to
thank Konstantinos Sechidis for some insightful discussions
with respect to information theoretic feature selection, Dan
Thornton for initially processing HTRU 2 data, and our
reviewer for their helpful feedback.

REFERENCES

ATLAS Collaboration, 2008, JINST, 3
Abdo A. A. et al., 2009, Science, 325, 5942, p.840
Aggarwal C. et al., 2004, Proc. of the tenth Int. Conf. on

Knowledge discovery and data mining, p.503-508

Ait-Allal D., Weber R., Dumez-Viou C., Cognard I., Theureau

G., 2012, C. R. Physique, 13, 1, p.80

Ball N., Brunner R. J., 2009, Int. J. Mod. Phys. D, 19, 7
Barber B. M., Odean T., 2000, Journal of Finance, 55, 2
Barr E. D. et al., 2013, MNRAS, 435, 2234
Barr E. D., 2014, presentation at ”Extreme-Astrophysics in
an Ever-Changing Universe: Time-Domain Astronomy
in the 21st Century”, Ierapetra, Crete, 16-20 June 2014.
http://www3.mpifr-bonn.mpg.de/div/jhs/Program ﬁles/
EwanBarrCrete2014.pdf (accessed January 6th, 2016)

Bates S. D. et al., 2011a, MNRAS, 411, 1575
Bates S. D. et al., 2011b, MNRAS, 416, 2455
Bates S. D., 2011, PhD thesis, Univ. Manchester
Bates S. D. et al., 2012, MNRAS, 427, 1052
Bengio J., 2009, Foundations and Trends in Machine Learning, 2,

p.1-127

MNRAS 000, 1–22 (2015)

Fifty Years of Pulsar Candidate Selection

21

Borne K. D., 2009, in Next Generation of Data Mining, CRC

Foster R. S., Cadwell B. J., Wolszczan A., Anderson S. B., 1995,

Press, p.91-114

ApJ, 454, 826

Bhattachatyya B., 2014, presentation at ”Transient Key science
project meeting 2014”, Manchester, UK, 9-10 September
2014.
http://www.jb.man.ac.uk/meetings/transients2014/
pdfs/Bhaswati.pdf (accessed January 6th, 2016)
Bhattachatyya B. et al., 2015, astro-ph/1509.07177
Biggs J. D., Lyne, A. G., 1992, MNRAS, 254, 257-263
Bishop C. M., 2006, Pattern Recognition and Machine Learning,

Springer

Boyles J. et al., 2013, ApJ, 763, 2
Brown G., 2009, Twelfth Int. Conf. on A.I. & Statistics, p.49-56
Brown G., Pocock A., Zhao Z., Luj´an M., 2012, JMLR, 13
Burgay M. et al.,2006, MNRAS, 368, 283
Burgay M. et al., 2013, MNRAS, 429, 579
Burns W. R. and Clark B. G., 1969, A&A, 2, 280
Camilo F., Nice D. J., Taylor J. H., 1993, ApJ, 412, p.L37
Carilli C. L., Rawlings, S., 2004, Science with the Square

Gaber M. M., Zaslavsky A., Krishnaswamy S., 2005, ACM

SIGMOD Record, 34, 2, p.18-26

Gaber M. M., Zaslavsky A., Krishnaswamy S., 2007, Advances in

Database Systems, p.39-59

Gaber M. M., 2012, Wiley Interdisciplinary Reviews: Data Mining

and Knowledge Discovery, 2, p.79-85

Guyon I., Elisseeﬀ A., 2003, JMLR, 3, p.1157-1182
van Heerden E., Karastergiou A., Roberts S. J., Smirnov O., 2014,

General Assembly and Scientiﬁc Symposium (URSI GASS)

Haensel P., Potekhin A. Y., Yakovlev D. G., 2007, Astrophysics

and Space Science Library, 326

He H., Garcia E. A., 2009, IEEE Transactions on Knowledge and

Data Engineering, 21, 9, p.1263-1284

Haykin S., 1999, Neural Networks A Comprehensive Foundation,

Prentice Hall

Hellinger E., 1909, Journal

f¨ur die reine und angewandte

Kilometre Array, New Astronomy Reviews

Mathematik (Crelle’s Journal), 136, p.210-271

Champion D. J., McLaughlin M. A., Lorimer D. R., 2005,

Hessels J. W. T., Ransom S. M., Stairs I. H., Kaspi V. M., Freire

MNRAS, 364, 1011

P. C. C., 2007, ApJ, 670, 363

Chandola V., Banerjee A., and Kumar V., 2009, ACM Comp.

Hewish A., Bell S. J., Pilkington J. D. H., Scott P. F., Collins

Surv., 41, 3

Clifton T. R., Lyne A. G., 1986, Nature, 320, 43
Coenen T. et al., 2014, A&A, 570, A60
Cooper S., 2014, presentation at LOFAR Science 2014,
2014.

Netherlands,

7-11

The

Amsterdam,
April
http://www.astron.nl/lofarscience2014/Documents/
Tuesday/Session%20III/Cooper.pdf
6th, 2016)

(accessed

January

Cordes J. M., Kramer M., Lazio T. J. W., Stappers B. W., Backer
D. C., Johnston S., 2004, New Astronomy Reviews, 4, 11-12,
p.1413

Cordes J. M. et al., 2006, ApJ, 637, 446
Cortes C., Vapnik, V., 1995, ML, 20, 3, p.273
Crawford F. et al., 2006, ApJ, 652, 2, 1499
Damashek M., Taylor J. H., Hulse R. A., 1978, ApJ, 225, L31-L33
Damashek M., Backus P. R., Taylor J. H., Burkhardt R. K., 1982,

ApJ, 253, L57-L60

Damour T., Esposito-Far`ese, G., 1998, Phys. Rev. D, 58, 4
Das Gupta S., 1960, Psychometrika, 25, 4
Davies J. G., Large M. I., Pickwick A. C., 1970, Nature, 227
Davies J. G., Lyne A. G., Seiradakis, J. H., 1977, MNRAS, 179,

635

Deich W. T. S., 1994, PhD thesis, California Institute of

Technology.

Deneva J. S. et al., 2009, ApJ, 703, 2, 2259
Deneva J. S., Stovall K., McLaughlin M. A., Bates S. D., Freire
P. C. C., Martinez J. G., Jenet F., Bagchi M., 2013, ApJ, 775,
1

Desvignes G., Cognard I., Champion D., Lazarus P., Lespagnol
P., Smith D. A., Theureau, G., 2012, IAU Symposium 291,
astro-ph/1211.3936

R. A., 1968, Nature, 217, 5130

Hodge V. J., Austin J., 2004, AI Review, 22, 2
Hoeﬀding W., 1963, Journal of

the American Statistical

Association, 58, 301, p.13-30

Hogden J., Wiel, S. V., Bower, G. C., Michalak, S., Siemion, A.,

Werthimer, D., 2012, astro-ph.IM/1201.1525

Hughes G., 1968, Information Theory, 14, 1, p.55-63
Hulse R. A. & Taylor J. H., 1974, ApJ, 191, L59
Hulten G., Spence L., Domingos P., 2001, Proc. of seventh ACM

SIGKDD.

Jacoby B. A., Bailes M., Ord S. M., Edwards R. T., Kulkarni

S. R., 2009, ApJ, 699, 2, p.2009

Janssen G. H., Stappers B. W., Braun R., van Straten W.,
Edwards R. T., Rubio-Herrera E., van Leeuwen J., Weltevrede
P., 2009, A& A, 498, 1, p.223-231

Johnston S., Lyne A. G., Manchester R. N., Kniﬀen D. A.,

D’Amico N., Lim J., Ashworth M., 1992, MNRAS, 255, 401

Karastergiou A. et al., 2015, astro-ph.IM/1506.03370
Keane E. F., Stappers, B. W., Kramer M., Lyne A. G., 2012,

MNRAS, 425, L71-L75

Keane E. F.

et al., E., 2014, Proceedings of Science,

PoS(AASKA14)040

Keane E. F., Petroﬀ E., 2015, MNRAS, 447, 2852
Keith M. J, Eatough R. P., Lyne A. G., Kramer M., Possenti A.,

Camilo F., Manchester R. N., 2009, MNRAS, 395, 837

Keith M. J. et al., 2010, MNRAS, 409, 619
Knispel B. et al., 2013, ApJ, 774, 2
Kohavi R., John G. H., 1997, A.I., 97, 1-2, p.273-324
Kramer M., Backer D. C., Cordes J. M., Lazio T. J. W., Stappers
B. W., Johnston S., 2004, New Astronomy Reviews, 48, 11-12,
p.993-1002

Dewey R. J., Taylor J. H., Weisberg J. M., Stokes G. H., 1985,

Large M. I., Vaughan A. E., Wielebinski R., 1968, Nature, 220,

ApJ, 294, 1, L25-L29

5169, p.753

Duda R. O., Hart P. E., Stork D. G., 2000, Pattern Classiﬁcation,

2nd Edition

Eatough R. P., 2009, PhD thesis, Univ. Manchester.
Eatough R. P., Molkenthin N., Kramer M., Noutsos A., Keith
M. J., Stappers B. W., Lyne A. G., 2010, MNRAS, 407, 2443
Eatough R. P., Kramer M., Lyne A. G., Keith M. J., 2013,

MNRAS, 431, 292

Edwards R. T., Bailes M., van Straten W., Britton M. C., 2001,

MNRAS, 326, 358

Faulkner A. J. et al., 2004, MNRAS, 355, 147
Fayyad U., Irani K., 1993, IJCAI, p.1022-1029
Fisher R. A., 1921, Metron 1, 3-32
Flach P. A., 2003, Proc. 20th Int. Conf. on ML, p.194-201

Law C. J. et al., 2014, astro-ph/1412.7536
Lazarus P., 2012, The PALFA Survey,

IAU Symposium
291, http://www.pulsarastronomy.net/IAUS291/download/
Oral/IAUS291 LazarusP.pdf (accessed January 6th, 2016)

Lee K. J.,Guillemot L., Yue Y. L., Kramer M., Champion D. J.,

2012, MNRAS, 424, 2832

Lee K. J. et al., 2013, MNRAS, 433, 688
Levin L., 2012, PhD thesis, Swinburne University
LOFAR Pulsar Working Group, 2013, presentation at LOFAR
Status Meeting, Dwingeloo, The Netherlands, March 6th,
2013
http://www.lofar.org/wiki/lib/exe/fetch.php?media=
public:lsm new:2013 03 06 hesself.pdf (accessed January 6th,
2016)

MNRAS 000, 1–22 (2015)

22

R. J. Lyon et al.

Lorimer D., Kramer, M., 2006, Cambridge Univ. Press
Lorimer D. et al., 2006, MNRAS, 372, 777
Lorimer D. R., Bailes M., McLaughlin M. A., Narkevic D. J.,

Crawford F., 2007, Science, 318, p.777-780

Lorimer D. R., Camilo F., McLaughlin M. A., 2013, MNRAS,

434, 347

Lorimer D. R. et al., 2015, astro-ph.IM/1501.05516
Lyon R. J., Brooke J. M., Knowles J. D., Stappers B. W., 2013,

SMC, p.1506-1511

Lyon R. J., Brooke J. M., Knowles J. D., Stappers B. W.,
2014, 22nd International Conference on Pattern Recognition,
p.1969-1974

Lyon R. J., 2015, PhD thesis, Univ. Manchester
MacKay D. J. C., 2002, Information Theory, Inference & Learning

Algorithms

Taylor, J. H. and Jura, M. and Huguenin, G. R., 1969, Nature,

223, 797

Thompson D. R., Majid W. A., Wagstaﬀ K., Reed C., 2011,

NASA Conference on Intelligent Data Understanding

Thornton D. et al., 2013, Science, 341, p.53-56
Thornton D., 2013, PhD thesis, Univ. Manchester
Tukey J., 1949, Biometrics, 5, 2, p.99-114
Way M. J., Scargle J. D., Ali K. M., Srivastava A. N.,
2012, Advances in Machine Learning and Data Mining for
Astronomy, 1st Edition

Widmer G., Kubat M., 1996, Machine Learning, 23, 1, p.69
Wilcoxon F., 1945, Biometrics Bulletin, 1, 6, p.80-83
Yang H. H. & Moody, J., 1999, NIPS, 12, p.687-693
Yang Q., Wu X., 2006, International Journal of Information

Technology & Decision Making, 5, 4, p.597-604

Manchester R. N., Lyne A. G., Taylor J. H., Durdin J. M., Large

Zhu W. W. et al., 2014, ApJ, 781, 2

M. I., Little A. G., 1978, MNRAS, 185, 409

Manchester R. N., Lyne A. G., D’Amico N., Johnston S., Lim J.,

Kniﬀen D. A., 1990a, Nature, 345, 598

Manchester R. N., Lyne A. G., Robinson C., D’Amico N., Bailes

M., Lim J., 1990b, Nature, 352, 219

Manchester R. N. et al., 1996, MNRAS, 279, 1235
Manchester R. N. et al., 2001, MNRAS, 328, 17
Manchester R. N., Hobbs G. B.,Teoh A., Hobbs M., 2005, Astron.

J., 129, 4

Manchester R. N., Fan G., Lyne A. G., Kaspi V. M., Crawford

F., 2006, ApJ, 649, 235

Markou M., Singh S., 2003, Signal Processing, 18, 12, p.2499-2521
Meehl P. E., 1954, Clinical versus statistical prediction: A

theoretical analysis and a review of the evidence

Mickaliger M. B. et al., 2012, ApJ, 759, 2
Mitchell T. M., 1997, Machine Learning, 1st Edition
Morello V., Barr E. D., Bailes M., Flynn C. M., Keane E. F., van

Straten W., 2014, MNRAS, 443, 1651

Navarro J., Anderson S. B., Freire P. C. C., 2003, ApJ, 594, 943
Ng C., 2012, IAU Symposium, S291, 8, p.53-56
Nice D. J., Taylor J. H., Fruchter A. S., 1993, ApJ, 402, L49-L52
Nice D. J., Fruchter A. S., Taylor J. H., 1995, ApJ, 449
Nikulin N. S. et al., 2001, Encyclopedia of Mathematics
Pearson K., 1895, R. Soc. Lond., 58, p.347-352
Petroﬀ E. et al., 2015, MNRAS, 447, 246
P-Alfa Consortium, 2015, web resource, ALFA Pulsar Studies,

http://www.naic.edu/alfa/pulsar/ (accessed January 6th,
2016)

Quinlan J. R., 1993, C4.5: programs for machine learning, Morgan

Kaufmann

Ransom S. M. et al., 2011, ApJ Letters, 727, L16
Rosen R. et al., 2010, The Pulsar Search Collaboratory,

Astronomy Education Review, 9, 1

Rosen R. et al., 2013, ApJ, 768, 85
Rubio-Herrera E., Braun R.,Janssen G.,van Leeuwen J., Stappers

B. W., 2007, astro-ph/0701183

Sayer R. W., Nice D. J., Taylor J. H., 1997, ApJ, 474
Shannon C. E., Weaver W., 1949, The mathematical theory of

communication, Univ. of Illinois Press

Smits R., Kramer M., Stappers B., Lorimer D. R., Cordes J.,

Faulkner A., 2009a, A&A, 493, 3, p.1161

Smits R., Lorimer D. R., Kramer M., Manchester R., Stappers B.,

Jin C. J., Nan R. D., Li D., 2009b, A&A, 505, 2, p.919-926

Spitler L. G. et al., 2014, ApJ, 790, 2
Stokes G. H., Taylor J. H., Weisberg J. M., Dewey R. J., 1985,

Nature, 317, p.787-788

Stokes G. H., Segelstein D. J. Taylor J. H., Dewey R. J., 1986,

ApJ, 311, p.694-700

Stovall K., Lorimer D. R., Lynch R. S., 2013, Class. Quantum

Grav., 30, 22

Stovall K. et al., 2014, ApJ, 791
Swiggum J. K. et al., 2015, ApJ, 805, 156

This paper has been typeset from a TEX/LATEX ﬁle prepared by
the author.

MNRAS 000, 1–22 (2015)

