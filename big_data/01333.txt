6
1
0
2

 
r
a

M
4

 

 
 
]
L
C
.
s
c
[
 
 

1
v
3
3
3
1
0

.

3
0
6
1
:
v
i
X
r
a

Joint Learning Templates and Slots for Event Schema Induction

Lei Sha, Sujian Li, Baobao Chang, Zhifang Sui

Key Laboratory of Computational Linguistics, Ministry of Education

School of Electronics Engineering and Computer Science, Peking University
Collaborative Innovation Center for Language Ability, Xuzhou 221009 China

shalei, lisujian, chbb, szf@pku.edu.cn

Abstract

Automatic event schema induction (AESI)
means to extract meta-event from raw text,
in other words, to ﬁnd out what types (tem-
plates) of event may exist in the raw text and
what roles (slots) may exist in each event type.
In this paper, we propose a joint entity-driven
model to learn templates and slots simultane-
ously based on the constraints of templates
and slots in the same sentence.
In addition,
the entities’ semantic information is also con-
sidered for the inner connectivity of the enti-
ties. We borrow the normalized cut criteria in
image segmentation to divide the entities into
more accurate template clusters and slot clus-
ters. The experiment shows that our model
gains a relatively higher result than previous
work.

1 Introduction

for

the

traditional

representation of
Event schema is a high-level
It
is very use-
a bunch of similar events.
ful
information extraction
(IE)(Sagayam et al., 2012) task. An example of
event schema is shown in Table 1. Given the bomb-
ing schema, we only need to ﬁnd proper words to ﬁll
the slots when extracting a bombing event.

There are two main approaches for AESI task.
Both of them use the idea of clustering the po-
tential event arguments to ﬁnd the event schema.
One of
them is probabilistic graphical model
(Chambers, 2013; Cheung, 2013). By incorporating
templates and slots as latent topics, probabilistic
graphical models learns those templates and slots

Bombing Template
person
Perpetrator:
person
Victim:
public
Target:
bomb
Instrument:

Table 1: The event schema of bombing event in MUC-4, it has
a bombing template and four main slots

that best explains the text. However, the graph-
ical models considers the entities independently
and do not
take the interrelationship between
entities into account. Another method relies on
ad-hoc clustering algorithms (Filatova et al., 2006;
Chambers and Jurafsky, 2011).
Sekine, 2006;
(Chambers and Jurafsky, 2011)
pipelined
approach.
In the ﬁrst step, it uses pointwise mu-
tual
information(PMI) between any two clauses
in the same document to learn events, and then
learns syntactic patterns as ﬁllers. However, the
pipelined approach suffers from the error prop-
agation problem, which means the errors in the
template clustering can lead to more errors in the
slot clustering.

is

a

This paper proposes an entity-driven model which
jointly learns templates and slots for event schema
induction. The main contribution of this paper are
as follows:

• To better model the inner connectivity between
entities, we borrow the normalized cut in image
segmentation as the clustering criteria.

• We use constraints between templates and be-
tween slots in one sentence to improve AESI
result.

A car bomb exploded in front of the U.S. embassy residence 

Sentence

entity 1
entity 1

in the Peruvian capital

entity 3

entity 2

Entity Representation

Entity 1: h=bomb, p=explode, d=subject,
    f={hyper={explosive, weaponry...} sentence=5, passage=41}

Entity 2: h=residence, p=explode, d=prep_in_front_of,
    f={hyper={diplomatic building...} sentence=5, passage=41}

Entity 3: h=capital, p=explode, d=prep_in,
    f={hyper={center, federal government...} sentence=5, 
         passage=41}

Figure 1: An entity example

2 Task Deﬁnition

Our model is an entity-driven model. This model
represents a document d as a series of entities Ed =
{ei|i = 1, 2, · · · }. Each entity is a quadruple e =
(h, p, d, f ). Here, h represents the head word of
an entity, p represents its predicate, and d repre-
sents the dependency path between the predicate and
the head word, f contains the features of the entity
(such as the direct hypernyms of the head word),
the sentence id where e occurred and the document
id where e occurred. A simple example is Fig 1.

Our ultimate goal is to assign two labels, a slot
variable s and a template variable t, to each entity.
After that, we can summarize all of them to get event
schemas.

3 Automatic Event Schema Induction

Inner Connectivity Between Entities

3.1
We focus on two types of inner connectivity: (1) the
likelihood of two entities to belong to the same tem-
plate; (2) the likelihood of two entities to belong to
the same slot;

3.1.1 Template Level Connectivity

Therefore,

It is easy to understand that entities occurred near
each other are more likely to belong to the same tem-
plate.
(Chambers and Jurafsky, 2011)
uses PMI to measure the correlation of two words
in the same document, but it cannot put two words
from different documents together. In the Bayesian
model of (Chambers, 2013), p(predicate) is the key
factor to decide the template, but it ignores the
fact that entities occurring nearby should belong to

In this paper, we try to put
the same template.
two measures together. That is, if two entities oc-
curred nearby, they can belong to the same tem-
plate; if they have similar meaning, they can also
belong to the same template. We use PMI to mea-
sure the distance similarity and use word vector
(Mikolov et al., 2013) to calculate the semantic sim-
ilarity.

A word vector can well represent the meaning of
a word. So we concatenate the word vector of the
j-th entity’s head word and its predicate, denoted as
vechp(i). We use the cosine distance coshp(i, j) to
measure the difference of two vectors.

Then we can get the template level connectivity
formula as shown in Eq 1. The P M I(i, j) is cal-
culated by the head words of entity mention i and
j.

WT (i, j) = P M I(i, j) + coshp(i, j)

(1)

3.1.2 Slot Level Connectivity

If two entities can play similar role in an event,
they are likely to ﬁll the same slot. We know that if
two entities can play similar role, their head words
may have the same hypernyms. We only consider
the direct hypernyms here. Also, their predicates
may have similar meaning and the entities may have
the same dependency path to their predicate. There-
fore, we give the factors equal weights and add them
together to get the slot level similarity.

WS(i, j) = cosp(i, j) + δ(dependi = dependj)
+ δ(hypernymi ∩ hypernymj 6= φ)

(2)
Here, the δ(·) has value 1 when the inner expression
is true and 0 otherwise. The “hypernym” is derived
from Wordnet(Miller, 1995), so it is a set of direct
hypernyms. If two entities’ head words have at least
one common direct hypernym, then they may belong
to the same slot. And again cosp(i, j) represents the
cosine distance between the predicates’ word vector
of entity i and entity j.

3.2 Template and Slot Clustering Using

Normalized Cut

Normalized cut intend to maximize the intra-class
similarity while minimize the inter class similarity,
which deals well with the connectivity between en-
tities.

We represent each entity as a point in a high-
dimension space. The edge weight between two
points is their template level similarity / slot level
similarity. Then the larger the similarity value is,
the more likely the two entities (point) belong to the
same template / slot, which is also our basis intu-
ition.

For simplicity, denote the entity set as E =
{e1, · · · , e|E|}, and the template set as T . We
use the |E| × |T | partition matrix XT to repre-
sent the template clustering result. Let XT =
[XT1 , · · · , XT|T | ], where XTl is a binary indicator
for template l(Tl).

XT (i, l) =( 1 ei ∈ Tl

0 otherwise

(3)

Usually, we deﬁne the degree matrix DT as:

DT (i, i) =Pj∈E WT (i, j), i = 1, · · · , |E|. Ob-

viously, DT is a diagonal matrix.
It contains in-
formation about the weight sum of edges attached
to each vertex. Then we have the template clus-
tering optimization as shown in Eq 4 according to
(Shi and Malik, 2000).

max ε1(XT ) =

1
|T |

X T
Tl
X T
Tl

WT XTl

DT XTl

(4)

|T |

Xl=1

s.t. XT ∈ {0, 1}|E|×|T | XT 1|T | = 1|E|

where 1|E| represents the |E| × 1 vector of all 1’s.

For the slot clustering, we have a similar opti-

mization as shown in Eq 5.

max ε2(XS ) =

1
|S|

X T
Sl
X T
Sl

WSXSl

DSXSl

(5)

|S|

Xl=1

s.t. XS ∈ {0, 1}|E|×|S| XS1|S| = 1|E|

where S represents the slot set, XS is the slot clus-
tering result with XS = [XS1 , · · · , XS|S|], where
XSl is a binary indicator for slot l(Sl).

entities in one sentence often belong to one template
but different slots.

The sentence constraint contains two types of
constraint, “template constraint” and “slot con-
straint”.

1. Template constraint: Entities in the same sen-
tence are usually in the same template. Hence
we should make the templates taken by a sen-
tence as few as possible.

2. Slot constraint: Entities in the same sentence
are usually in different slots. Hence we should
make the slots taken by a sentence as many as
possible.

Based on these consideration, we can add an extra
item to the optimization object. Let Nsentence be the
number of sentences. Deﬁne Nsentence × |E| matrix
J as the sentence constraint matrix, the entries of J
is as following:

J(i, j) =( 1 ei ∈ Sentencej

0 otherwise

(7)

Easy to show, the product GT = J T XT represents
the relation between sentences and templates. In ma-
trix GT , the (i, j)-th entry represents how many en-
tities in sentence i are belong to Tj.

Using GT , we can construct our objective. To rep-
resent the two constraints, the best objective we have
found is the trace value: tr(GT GT
T ). Each entry on
T is the square sum of
the diagonal of matrix GT GT
all the entries in the corresponding line in GT , and
the larger the trace value is, the less templates the
sentence would taken. Since tr(GT GT
T ) is the sum
of the diagonal elements, we only need to maximize
the value tr(GT GT
T ) to meet the template constraint.
For the same reason, we need to minimize the value
tr(GSGT

S ) to meet the slot constraint.

Generally, we have the following optimization ob-

XS(i, l) =( 1 ei ∈ Sl

0 otherwise

jective:

(6)

Joint Model With Sentence Constraints

3.3
For event schema induction, we ﬁnd an important
property and we name it “Sentence constraint”. The

ε3(XT , XS) =

(8)

tr(cid:0)X T
tr(cid:0)X T

T J J T XT(cid:1)
S J J T XS(cid:1)

The whole joint model is shown in Eq 9. The solving

method is in the attachment ﬁle.

Bombing

XT , XS = argmax
XT ,XS

ε1(XT ) + ε2(XS) + ε3(XT , XS )

Perpetrator

Victim

s.t. XT ∈ {0, 1}|E|×|T | XT 1|T | = 1|E|
XS ∈ {0, 1}|E|×|S| XS1|S| = 1|E|

El salvador
The guerrillas
The drag mafia
Drug traffickers

The police chief

Students

The diplomat

The Peruvian embassy

The police station

Target

ministry

The embassy

Instrument

explosives
car bomb
dynamite

organization

bridge

incendiary bomb

vehicle bomb

The Atlacatl battalion

soldiers

(9)

Attack

4 Experiment

4.1 Dataset
In this paper, we use MUC-4(Sundheim, 1991) as
our dataset, which is the same as previous works
(Chambers and Jurafsky, 2011; Chambers, 2013).
MUC-4 corpus contains 1300 documents in the
training set, 200 in development set (TS1, TS2) and
200 in testing set (TS3, TS4) about Latin American
news of terrorism events. We ran several times on
the 1500 documents (training/dev set) and choose
the best |T | and |S| as |T | = 6, |S| = 4. Then
we report the performance of test set. For each
document, it provides a series of hand-constructed
event schemas, which are called gold schemas. With
these gold schemas we can evaluate our results. The
MUC-4 corpus contains six template types: Attack,
Kidnapping, Bombing, Arson, Robbery, and
Forced Work Stoppage, and for each template,
there are 25 slots. Since most previous works do not
evaluate their performance on all the 25 slots, they
instead focus on 4 main slots like Table 1, we will
also focus on these four slots. We use the Stanford
CoreNLP toolkit to parse the MUC-4 corpus.

4.2 Performance

Fig 2 shows two examples of our learned schemas:
Bombing and Attacking. The ﬁve words in each
slot are the ﬁve randomly picked entities from the
mapped slots. The templates and slots that were
joint learned seem reasonable.

We compare our

results with four works
(Chambers and Jurafsky, 2011;
Cheung, 2013;
Chambers, 2013; Nguyen et al., 2015) as is shown
in Table 2. Our model has outperformed all of the
previous methods. The improvement of recall is
due to the normalized cut criteria, which can better
use the inner connectivity between entities. The
sentence constraint
improves the result one step
further.

Perpetrator

troops
criminals
combat
murder
person

Victim

driver
soldiers
children
civilians
journalists

Target

organization 
helicopter

person

livestock ministray building

vehicles

Instrument

rifles

weapons

gun

explosives
machinegun

Figure 2: Part of the result

C&J (2011)
Cheung (2013)
Chambers (2013)
Nguyen et al. (2015)
Our Model-SC
Our Model

Prec Recall
0.48
0.25
0.37
0.32
0.41
0.41
0.54
0.36
0.68
0.38
0.70
0.39

F1
0.33
0.34
0.41
0.43
0.49
0.50

Table 2: Comparison to state-of-the-art unsupervised systems,
“-SC” means without sentence constraint

5 Related Works

AESI task has been researched for many years.
Shinyama and Sekine (2006) proposed an approach
to learn templates with unlabeled corpus. They use
unrestricted relation discovery to discover relations
in unlabeled corpus as well as extract their ﬁllers.
Their constraints are that they need redundant doc-
uments and their relations are binary over repeated
named entities.
(Chen et al., 2011) also extract bi-
nary relations using generative model.

Kasch and Oates (2010),

Chambers and Jurafsky (2008),
Chambers and Jurafsky (2009),
Balasubramanian et al. (2013) captures
template-
like knowledge from unlabeled text by large-
scale learning of scripts and narrative schemas.
However,
are
(template/slot)
in a large corpus.
limited to frequent
Chambers and Jurafsky (2011)
idea,
and their goal is to characterize a speciﬁc domain
with limited data using a three-stage clustering
algorithm.

structures
topics

their

uses

their

Also, there are some state-of-the-art works us-
ing probabilistic graphic model (Chambers, 2013;
Cheung, 2013; Nguyen et al., 2015).

6 Conclusion

This paper presented a joint entity-driven model to
induct event schemas automatically.

This model uses word embedding as well as PMI
to measure the inner connection of entities and uses
normalized cut for more accurate clustering. Finally,
our model uses sentence constraint to extract tem-
plates and slots simultaneously. The experiment has
proved the effectiveness of our model.

Acknowledgments

This research is supported by National Key Basic
Research Program of China (No.2014CB340504)
and National Natural Science Foundation of China
(No.61375074,61273318). The contact authors of
this paper are Sujian Li and Baobao Chang.

References

[Baker et al.1998] Collin F Baker, Charles J Fillmore, and
John B Lowe. 1998. The berkeley framenet project.
In Proceedings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics and 17th In-
ternational Conference on Computational Linguistics-
Volume 1, pages 86–90. Association for Computa-
tional Linguistics.

[Balasubramanian et al.2013] Niranjan Balasubramanian,
Stephen Soderland, and Oren Etzioni Mausam. 2013.
Generating coherent event schemas at scale. Proceed-
ings of the Empirical Methods in Natural Language
Processing. ACM.

[Bunescu and Mooney2004] Razvan Bunescu and Ray-
mond J Mooney. 2004. Collective information ex-
traction with relational markov networks. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, page 438. Association for
Computational Linguistics.

[Chambers and Jurafsky2008] Nathanael Chambers and
Daniel Jurafsky. 2008. Unsupervised learning of nar-
rative event chains. In ACL, pages 789–797.

[Chambers and Jurafsky2009] Nathanael Chambers and
Dan Jurafsky. 2009. Unsupervised learning of nar-
rative schemas and their participants. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume

2-Volume 2, pages 602–610. Association for Compu-
tational Linguistics.

[Chambers and Jurafsky2011] Nathanael Chambers and
Dan Jurafsky. 2011. Template-based information ex-
traction without the templates. pages 976–986.

[Chambers2013] Nathanael Chambers.

Event
schema induction with a probabilistic entity-driven
model. EMNLP.

2013.

[Chen et al.2011] Harr Chen, Edward Benson, Tahira
Naseem, and Regina Barzilay. 2011. In-domain rela-
tion discovery with meta-constraints via posterior reg-
ularization. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies-Volume 1, pages 530–
540. Association for Computational Linguistics.

[Cheung2013] Jackie Chi Kit Cheung. 2013. Probabilis-
tic frame induction. arXiv preprint arXiv:1302.4813.
[Chieu et al.2003] Hai Leong Chieu, Hwee Tou Ng, and
Yoong Keok Lee. 2003. Closing the gap: Learning-
based information extraction rivaling knowledge-
engineering methods.
In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 216–223. Association for
Computational Linguistics.

[Chinchor et al.1993] Nancy Chinchor, David D Lewis,
and Lynette Hirschman. 1993. Evaluating message
understanding systems: an analysis of the third mes-
sage understanding conference (muc-3). Computa-
tional linguistics, 19(3):409–449.

[Filatova et al.2006] Elena Filatova, Vasileios Hatzivas-
siloglou, and Kathleen McKeown. 2006. Automatic
creation of domain templates. In Proceedings of the
COLING/ACL on Main conference poster sessions,
pages 207–214. Association for Computational Lin-
guistics.

[Kasch and Oates2010] Niels Kasch and Tim Oates.
2010. Mining script-like structures from the web. In
Proceedings of the NAACL HLT 2010 First Interna-
tional Workshop on Formalisms and Methodology for
Learning by Reading, pages 34–42. Association for
Computational Linguistics.

[Maslennikov and Chua2007] Mstislav Maslennikov and
Tat-Seng Chua. 2007. Automatic acquisition of do-
main knowledge for information extraction.
In Pro-
ceedings of the Association of Computational Linguis-
tics (ACL).

[Mikolov et al.2013] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013. Efﬁcient estimation
of word representations in vector space. arXiv preprint
arXiv:1301.3781.

[Miller1995] George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39–41.

[Nguyen et al.2015] Kiem-Hieu Nguyen, Xavier Tannier,
Olivier Ferret, and Romaric Besanc¸on. 2015. Gener-
ative event schema induction with entity disambigua-
tion. In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th
International Joint Conference on Natural Language
Processing (Volume 1: Long Papers), pages 188–197,
Beijing, China, July. Association for Computational
Linguistics.

[Patwardhan and Riloff2007] Siddharth Patwardhan and
Ellen Riloff. 2007. Effective information extraction
with semantic afﬁnity patterns and relevant regions. In
EMNLP-CoNLL, volume 7, pages 717–727.

[Patwardhan and Riloff2009] Siddharth Patwardhan and
Ellen Riloff.
2009. A uniﬁed model of phrasal
and sentential evidence for information extraction. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 1-
Volume 1, pages 151–160. Association for Computa-
tional Linguistics.

[Rau et al.1992] Lisa Rau, George Krupka, Paul Jacobs,
Ira Sider, and Lois Childs. 1992. Ge nltoolset: Muc-
4 test results and analysis. In Proceedings of the 4th
conference on Message understanding, pages 94–99.
Association for Computational Linguistics.

Chapter of the Association of Computational Linguis-
tics, pages 304–311. Association for Computational
Linguistics.

[Sudo et al.2003] Kiyoshi Sudo, Satoshi Sekine, and
Ralph Grishman. 2003. An improved extraction pat-
tern representation model for automatic ie pattern ac-
quisition. In Proceedings of the 41st Annual Meeting
on Association for Computational Linguistics-Volume
1, pages 224–231. Association for Computational Lin-
guistics.

[Sundheim1991] Beth Sundheim.

1991. Third mes-
sage understanding evaluation and conference (muc-
3): Phase 1 status report. In HLT.

[Surdeanu et al.2006] Mihai Surdeanu, Jordi Turmo, and
Alicia Ageno. 2006. A hybrid approach for the ac-
quisition of information extraction patterns. Adaptive
Text Extraction and Mining (ATEM 2006), page 48.

[Yangarber et al.2000] Roman Yangarber, Ralph Grish-
man, Pasi Tapanainen, and Silja Huttunen. 2000. Au-
tomatic acquisition of domain knowledge for informa-
tion extraction. In Proceedings of the 18th conference
on Computational linguistics-Volume 2, pages 940–
946. Association for Computational Linguistics.

[Riloff et al.2005] Ellen Riloff,

[Riloff and Schmelzenbach1998] Ellen Riloff and Mark
Schmelzenbach. 1998. An empirical approach to con-
ceptual case frame acquisition. In Proceedings of the
Sixth Workshop on Very Large Corpora, pages 49–56.
and
William Phillips. 2005. Exploiting subjectivity clas-
siﬁcation to improve information extraction. In Pro-
ceedings of the National Conference On Artiﬁcial In-
telligence, volume 20, page 1106. Menlo Park, CA;
Cambridge, MA; London; AAAI Press; MIT Press;
1999.

Janyce Wiebe,

[Sagayam et al.2012] R Sagayam, S Srinivasan, and
S Roshni.
2012. A survey of text mining: Re-
trieval, extraction and indexing techniques.
Inter-
national Journal Of Computational Engineering Re-
search, 2(5).

[Sekine2006] Satoshi Sekine.

2006. On-demand in-
formation extraction.
In Proceedings of the COL-
ING/ACL on Main conference poster sessions, pages
731–738. Association for Computational Linguistics.
[Shi and Malik2000] Jianbo Shi and Jitendra Malik.
2000. Normalized cuts and image segmentation. Pat-
tern Analysis and Machine Intelligence, IEEE Trans-
actions on, 22(8):888–905.

Shinyama

2006.

[Shinyama and Sekine2006] Yusuke

and
Preemptive information
Satoshi Sekine.
extraction using unrestricted relation discovery.
In
Proceedings of the main conference on Human Lan-
guage Technology Conference of the North American

6
1
0
2

 
r
a

M
4

 

 
 
]
L
C
.
s
c
[
 
 

1
v
3
3
3
1
0

.

3
0
6
1
:
v
i
X
r
a

The Solving Method of the Event Schema

Induction Joint Model

Lei Sha

March 7, 2016

1 The Model

Template clustering optimization is shown in Eq 1.

max ε1(XT ) =

1
|T |

X T
Tl
X T
Tl

WT XTl

DT XTl

|T |

Xl=1

s.t. XT ∈ {0, 1}|E|×|T | XT 1|T | = 1|E|

Here, 1|E| represents the |E| × 1 vector of all 1’s.
Slot clustering optimization is shown in Eq 2.

max ε2(XS) =

1
|S|

X T
Sl
X T
Sl

WS XSl

DSXSl

|S|

Xl=1

s.t. XS ∈ {0, 1}|E|×|S| XS1|S| = 1|E|

Here, S represents the slot set, XS is the slot clustering result with XS =
[XS1 , · · · , XS|S|], where XSl is a binary indicator for slot l(Sl).

XS(i, l) =( 1 ei ∈ Sl

0 otherwise

The original sentence constraint model is shown as follows:

(1)

(2)

(3)

(4)

ε3(XT , XS) =

tr(cid:0)X T
tr(cid:0)X T

T J J T XT(cid:1)
S J J T XS(cid:1)

However, this form of objective is hard to optimize, we can transfer the slot
constraint objective tr(GS GT
S ) (GS = J T XS) to something that should be max-
imized. Since tr(GS GT

S ) = tr(cid:0)X T
same as to maximize tr(cid:0)X T
vector. It can be proved that tr(cid:0)X T

S J J T XS)(cid:1) is the
S (E − J J T )XS)(cid:1) (E = 1 · 1T ). 1 represents an all 1

S J J T XS)(cid:1), to minimize tr(cid:0)X T
S (E − J J T )XS)(cid:1) is positive.

1

Generally, we have the following optimization objective:

max ε3(XT , XS) = tr(cid:0)X T

s.t. XT ∈ {0, 1}|E|×|T | XT 1|T | = 1|E|
XS ∈ {0, 1}|E|×|S| XS1|S| = 1|E|

T J J T XT(cid:1)tr(cid:0)X T

S (E − J J T )XS)(cid:1)

(5)

The whole joint model is shown in Eq 6. The ﬁrst item represents the goodness
of the templates clustering. The second item represents the goodness of the
slot clustering. The third item is the sentence constraint item. However, this
model is too complex to be solved by normal optimization method. Therefore,
we use the Alternating Maximization Procedure[2] to solve this problem in the
following section.

XT , XS = argmax
XT ,XS

ε1(XT ) + ε2(XS) + ε3(XT , XS)

s.t. XT ∈ {0, 1}|E|×|T | XT 1|T | = 1|E|
XS ∈ {0, 1}|E|×|S| XS1|S| = 1|E|

(6)

2 Solving Method: Alternating Maximization

Procedure(AMP)

In this section, the detailed solving method of the complex model shown in
Eq 6 will be illustrated. The ultimate objective in Eq 6 is the combination of
optimization objective in Eq 1, Eq 2 and Eq 5.

The ﬁrst two items in Eq 6 is the form of generalized Rayleigh quotient and
can be solved using the method in [3], which mainly contains two steps: 1) ﬁnd
the continuous optimal value 2) discretization. We use the AMP method to
get the numerical solution of Eq 6. The AMP algorithm can be viewed as a
joint maximization method by ﬁxing one argument and maximizing over the
other. After we ﬁxed XS or XT , we can transform the objective to the form of
generalized Rayleigh quotient which could be solved by the method in [3].

When XT is ﬁxed The ﬁrst term in Eq 6 is a constant in this case, so that

we ignore it for simplicity. Let f (XT ) = tr(cid:0)X T

T J J T XT(cid:1), then Eq 6 becomes:

|S|

max ε(XS; XT ) =

1
|S|

|S|

Xl=1

X T
Sl
X T
Sl

WS XSl

DSXSl

+ f (XT )

Xl=1

X T

Sl(E − J J T )XSl

(7)

We can reduce the fractions to a common denominator, then Eq 7 becomes:

1
|S| X T

Sl

|S|

Xl=1

WS XSl + f (XT ) ∗ X T

Sl(E − J J T )XSl X T

Sl

X T
Sl

DSXSl

2

DSXSl

(8)

Note that the term X T
it as a trace of a 1 × 1 matrix as shown in Eq 9.

Sl(E − J J T )XSl X T

Sl

DSXSl is a scalar, so that we can take

Sl DSXSl

Sl(E − J J T )XSl X T
X T
= tr(X T
= ΩSX T

Sl (E − J J T )XSl X T
Sl(E − J J T )DSXSl

Sl DSXSl)

(9)

Here, ΩS = X T
entities in the corresponding slot.

SlXSl is a diagonal matrix. Each diagonal entry is the number of

In order to represent Eq 8 to the form of Eq 10, we need to keep D∗

S is as Eq 11. In order to keep W ∗

S a symmetric matrix, we add 1

S = DS,
2 of

and the W ∗
Eq 9 to both sides of X T
Sl

WS XSl.

ε(XS; XT ) =

X T
X T
Sl

Sl W ∗
D∗

S XSl
SXSl

|S|

Xl=1

f (XT )DS(E − J J T )ΩS +

f (XT )ΩS(E − J J T )DS

1
|S|

WS

(10)

(11)

W ∗

S =

+

1
2
1
2

D∗

S = DS




When XS is ﬁxed Using the same method as the above, in order to get the
form of Eq 12, the value of W ∗

T are calculated as Eq 13.

T and D∗

X T
X T
Tl

Tl W ∗
D∗

T XTl
T XTl

|T |

Xl=1

1
|T |

WT

J J T DT ΩT +

ΩT DT J J T

ε(XT ; XS) =

W ∗

T =

+

1

2f (XS)

1

2f (XS)

D∗

T =DT




(12)

(13)

Stopping criteria According to [3], if XT , XS is a feasible solution to Eq 6,
so is {XT RT , XSRS|RT
S RS = I}, and they have the same objective
value: ε(XT RT , XSRS) = ε(XT , XS). Therefore, if Eq 14 is satisﬁed, the loop
ends.

T RT = I, RT

kX new
kX new

T − X old
S − X old

T RT k = 0
S RSk = 0

We can get the closed form of RT and RS as shown in Eq 15.

RT = (X (new)T
RS = (X (new)T

T

S

X new

T

X new

S

3

T

)−1X (new)T
)−1X (new)T

S

X old

T

X old

S

(14)

(15)

Therefore, the ultimate stop criteria becomes kRT
ǫ is very close to 0.

T RT − Ik + kRT

S RS − Ik < ǫ,

The total algorithm of the whole process is shown as Algorithm 1. Since
the optimization objective is a diﬀerentiable function, the convergence to the
optimum solution can be guaranteed by [2, 1].

Algorithm 1: The pseudo code of the optimum value ﬁnding process

Input:

Template level similarity matrix, WT ;
Slot level similarity matrix, WS;
sentence constraint matrix, J.

Output:

The partition matrix of template, XT ;
The partition matrix of slot, XS;

begin

Randomly initialize XT and XS;
while kRT

T RT − Ik + kRT

S RS − Ik > ǫ do

Fix XT , calculate Eq 11;
Find XS which can maximize Eq 10;
Fix XS, calculate Eq 13;
Find XT which can maximize Eq 12;
Calculate RT and RS by Eq 15;

end while
Discretize XT and XS;
return XT and XS

end

3 Experiment Setting

The ΩT and ΩS in Eq 13 and Eq 11 can be seen as a prior of the template
cluster size and slot cluster size. We use the most na¨ıve prior that all clusters
are of the same size.

References

[1] Trevor Hastie, Robert Tibshirani, Jerome Friedman, T Hastie, J Friedman,

and R Tibshirani. The elements of statistical learning. Springer, 2009.

[2] Iddo Naiss and Haim H Permuter. Alternating maximization procedure for

ﬁnding the global maximum of directed information. IEEE, 2010.

[3] Stella X Yu and Jianbo Shi. Multiclass spectral clustering. In Computer
Vision, 2003. Proceedings. Ninth IEEE International Conference on, pages
313–319. IEEE, 2003.

4

