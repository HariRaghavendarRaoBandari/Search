6
1
0
2

 
r
a

M
2

 

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
6
9
5
0
0

.

3
0
6
1
:
v
i
X
r
a

Randomly Weighted Averages: A Multivariate Case

Hazhir Homei

Department of Statistics, Faculty of Mathematical Sciences,
University of Tabriz, P.O.Box 51666–17766, Tabriz, IRAN.

homei@tabrizu.ac.ir

Abstract

Stochastic linear combinations of some random vectors are studied where the

distribution of the random vectors and the joint distribution of their coef-

ﬁcients are Dirichlet. A method is provided for calculating the distribution

of these combinations which has been studied before by some authors. Our

main result is a generalization of some existing results with a simpler proof.

Keywords: Multivariate randomly weighted average; Multivariate Stieltjes

transform; Dirichlet Distributions; Multivariate stable distributions; Randomly

Linear Transformations; Dependent Components; Lifetime.

1. Introduction

For given random variables X1,· · · , Xn the distribution of the stochas-
tic linear combination Z =Pn
i=1 WiXi is used for the problems in lifetime,
stochastic matrices, neural networks and other applications in sociology and

biology. Let Xi (1 6 i 6 n) be the lifetime measured in a lab and 0 6 Wi 6 1 be

the random eﬀect of the environment on it; so WiXi 6 Xi and thusPn

is the average lifetime in the environment (see Homei (2015)). Recently, sev-

i=1 WiXi

eral authors have focused on computing the lifetime of systems in the real

conditions. Indeed, the randomly linear combination of random vectors have

Preprint submitted to arXiv.org

March 3, 2016

many applications including traditional portfolio selection models, relation-

ship between attitudes and behavior, number of cancer cells in tumor biology,

stream ﬂow in hydrology (Nadarajah & Kotz (2005)), branching processes,

inﬁnite particle systems and probabilistic algorithms, and vehicle speed and

lifetime (cf. Homei (2015), Rezapour & Alamatsaz (2014) and the references

therein) so ﬁnding their distributions has attracted the attentions of numer-

ous researchers.

In this paper, considering the dependent components and the real envi-

ronmental conditions, the distribution of lifetimes (in case of the Dirichlet

distributions) is calculated showing that the main result obtained here cov-

ers several previous results and provides a simpler proof (cf. e.g. Johnson

& Kotz (1990a), Sethuraman (1994), van Assche (1987), Volodin & Kotz &

Johnson (1993)).

The inner product of two random vectors was introduced in Homei (2014)

and the exact distribution of this product was investigated for some random

vectors with Beta and Dirichlet distributions.

In this paper a new gener-

alization for the inner product of two random vectors is introduced. For a
random vector W′ = hW1,· · · , Wni and a vector X = hX1,· · · , Xni of ran-
dom vectors (each Xi being k-dimensional) the inner product of X and W is
essentially the linear transformation of W under the k × n matrix X which
is Z =Pn
i=1 WiXi. We assume that W is independent from X1,· · · , Xn and
has Dirichlet distribution; also Xi’s have Dirichlet distributions. Identifying

the distribution of Z usually requires

(i) either some long computations with combinatorial identities (see e.g.

Volodin & Kotz & Johnson (1993), Johnson & Kotz (1990), Johnson

2

& Kotz (1990a), Sethuraman (1994), Homei (2014));

(ii) or advanced techniques requiring performing certain transformations

and solving diﬀerential equations (see e.g. Homei (2012), Soltani &

Homei (2009), van Assche (1987) or Homei (2015) and the references

therein).

In this paper a new way is introduced to identify the distribution of randomly

linear combinations (of Dirichlet distributions) by which a class of stochastic

diﬀerential equations can be solved; this new method is much simpler than

the existing ones.

2. The Main Result

Our main result identiﬁes the distribution of the randomly linear combi-

nation when the coeﬃcients come from a Dirichlet distribution.

Theorem 2.1. If X1,· · · , Xn are independent k-variate random vectors with
respectively Dirichlet(α(1)),· · · , Dirichlet(α(n)) distributions, for some k-
dimensional vectors α(j) = hα(j)
k i (j = 1,· · · , n), and the random
vector W = hW1,· · · , Wni is independent from X1,· · · , Xn and has the dis-
tribution

1 ,· · · , α(j)

i=1 WiXi has the distribution

i

α(1)

Dirichlet  k
Xi=1

Xi=1
,· · · ,
then the randomly linear combination Z =Pn
α(j)
Xj=1
1 ,· · · ,

Dirichlet  n
Xj=1

k

n

α(n)

i ! ,

α(j)

k ! .

3

(The First) Proof. Let Yj (j = 1,· · · , n) be independent random variables
independent from (X1,· · · , Xn) that have the distribution Γ(Pk
, 1
µ ),

respectively. Thus,

i=1 α(j)

i

The k-dimensional vectors Tj = YjXj have independent components and

for j = 1,· · · , n.

Wj

d=

Yj

(cid:0)Pn
l=1 Yl(cid:1)
hΓ(α(j)
1 ,

1
µ

),· · · , Γ(α(j)
k ,

1
µ

)i

n

distribution. Thus, we have

WjXj.

n

n

Tj

l=1 Yl(cid:1) =
Pn

Xj=1(cid:0)
Xj=1
Pn
l=1 Yl(cid:1) has Dirichlet(Pn
j=1(cid:0) Tj

Pn

Yj
l=1 Yl

d=

Xj

Xj=1
1 ,· · · ,Pn

j=1 α(j)

k ) distribution, so
⊠⊞

j=1 α(j)
j=1 WjXj = Z has the same distribution.

Now, Pn
Pn
Corollary 2.2. Let the k-variate random variables X1,· · · , Xn be indepen-
dent and with common distribution. If W = hW1,· · · , Wni is independent
from X1,· · · , Xn having Dirichlet(kα,· · · , kα) distribution, then the ran-
domly linear combination Z = Pn
i=1 WiXi has the Dirichlet(nα,· · · , nα)
distribution if and only if Xi’s (i = 1,· · · , n) have Dirichlet(α,· · · , α) dis-
tributions.

Proof. The “if” part follows from Theorem 2.1 and the “only if” part follows

from the theorem of Volodin & Kotz & Johnson (1993).

⊠⊞

Some similar results (to Corollary 2.2) can be found in e.g. Alamat-

saz (1993). The following results are obtained as special cases of Theorem 2.1

and Corollary 2.2:

4

• Lemma 1 of Sethuraman (1994) (for n = 2), and
• Theorem of Volodin & Kotz & Johnson (1993) (when n = k and α(j)

i = a

for j = 1,· · ·, k, i = 1,· · ·, k),
which discusses the multivariate case, or the references below which discuss

the single-variable case (note that for k = 2 the Dirichlet distribution leads

to the Beta distribution):

• Theorem of van Assche (1987) (for n = k = 2, α(1)
• Theorem 2 of Johnson & Kotz (1990a) (for n = k = 2, α(1)
2 = a),
• Theorem 2.4 of Homei (2014) (for k = 2 and α(j)

1 = α(1)

1 = α, α(j)

α(2)

2 = α(2)

1 = α(2)

2 = 1
2 = α(2)

2 ),
1 =

1 = α(1)

2 = 1 − α for

j = 1,· · · , n),

• Theorem 1 of Homei (2013) (for k = 2);

and others; see the references in Homei (2015).

3. Four Other Proofs for Theorem 2.1 and a Variant of It

3.1. Moment Generation Method

The Second Proof. The generating moment function of Tj’s in the ﬁrst proof

are

E(et′Tj ) = EE(et′Yj Xj | Xj) = E(

By [9, page 77] we have

1

1 − t′Xj

i=1 α(j)

i

)Pk

.

MTj (t) = (

1

1 − tj

i=1 α(j)

i

)Pk

.

So, the components of the vector Tj are independent and have gamma dis-

tributions, which proves the theorem.

⊠⊞

5

3.2. Applying Basu’s Theorem

The Third Proof. We can write (for j = 1,· · · , n)

Xj ∼  Γ1j(α(j)

i=1 Γij(α(j)
i )

1 )

Γkj(α(j)
k )
i=1 Γij(α(j)

i )! ,

i=1 Γij(α(1)
i )

i=1 Γij(α(j)
i )

i=1 Γij(α(n)

i

)

i=1 Γij(α(j)

i )! .

and

So,

Pk
W ∼  Pk
j=1Pk
Pn
d=  Pk
j=1Pk
Pn

Z

,· · · ,

Pk
,· · · , Pk
j=1Pk
Pn
,· · · , Pk
j=1Pk
Pn

j=1 Γ1j(α(j)
1 )
i=1 Γij(α(j)
i )
i )’s (for i = 1,· · · , k and j = 1,· · · , n) are in-
dependent random variables with gamma distributions, which implies the

j=1 Γkj(α(j)
k )
i=1 Γij(α(j)

Let us recall that Γij(α(j)

i )! .

independence of the components of W from Xj’s (Basu’s Theorem).

⊠⊞

3.3. Mathematical Induction

The Fourth Proof. For n = 2 the theorem follows from [13, Lemma 1]. Sup-

pose the theorem holds for n (the induction hypothesis). We prove it for

n + 1 (the induction conclusion): By dividing the both sides of

n+1

Xi=1

YiXi =(cid:0)

n

Xi=1

Yj
l=1 Yl

Xj! + Yn+1Xn+1

i=1 Yi and using the induction hypothesis we have

n+1

by Pn+1
Xi=1

n

Xi=1

Xi =(cid:0)

Yi
i=1 Yi

Pn+1

in which the right hand side holds by [13, Lemma 1] (for n = 2).

Yj
l=1 Yl

Xj! +

Pn

Yn+1
i=1 Yi

Pn+1

Xn+1

⊠⊞

Yi(cid:1)  n
Xj=1

Pn
i=1 Yi(cid:1)  n
Xj=1
Pn+1

Yi

6

3.4. The Moments Method

n

The Fifth Proof. The general moments (s1, s2,· · · , sk) of Z are as follows:
E  k
Yj=1(cid:16)
Xi=1
where Phj

WjXij(cid:17)sj!= E


h1j, h2j,· · · , hnj(cid:19) n
Yi=1

(WjXij)hij(cid:17)


denotes summation over all non-negative integers

Yj=1(cid:16)Xhj (cid:18)

sj

n

k

hj = (h1j, h2j,· · · , hnj) subject to

This equation can be rearranged as

Xi=1

hij = sj

(j = 1, 2,· · · , n).

k

sj

h1j, h2j,· · · , hnj(cid:19) k
Yj=1(cid:18)
· · ·Xhk (cid:16)
Yj=1
h1j, h2j,· · · , hnj(cid:19)(
Yj=1(cid:18)
· · ·Xhk (cid:16)
Yi=1

sj

n

k

n

Yi=1

W hi∗

i

(WiXij)hij(cid:17)!
ij (cid:17)!
Yj=1

Yi=1

X

hij

)

n

k

j=1 hij) and also

sj

h1j, h2j,· · · , hnj(cid:19)E(cid:16)

n

Yi=1

W hi∗

i (cid:17)E(cid:16)

k

n

Yj=1

Yi=1

X

hij

ij (cid:17)! (‡)

Γ(Pn
i=1Pk
Γ(Pn
E  k
Yi=1
Yj=1

X

n

i=1Pk

j=1 α(i)

hij

ij ! =

j=1 α(i)
j )

j +Pk
Yi=1

n

j=1 sj)

E  k
Yj=1

n

Yi=1
ij ! ,

hij

X

)

j + hi∗
j=1 α(i)

j

j=1 α(i)

Γ(Pk
Γ(Pk

= E Xh1
= E Xh1
=Pk
· · ·Xhk   k
Yj=1(cid:18)

(where hi∗
=Xh1

W hi∗

i ! =

E  n
Yi=1

and also

By the Dirichlet distribution we have

and again by the Dirichlet distribution

E  k
Yj=1

X

hij

ij ! =

j=1 α(i)
j )

j=1 α(i)

j + hi∗

Γ(Pk
Γ(Pk

7

k

Yj=1

)

Γ(α(i)

j + hij)
Γ(α(i)
j )

.

k

Yj=1(cid:18)

n

=Xh1
Yi=1

So, by using (‡)
· · ·Xhk
Γ(Pk
Γ(Pk
Γ(Pn
i=1Pk

Γ(Pn

=

j=1 α(i)

j + hi.)

j=1 α(i)

j

i=1Pk

j=1 α(i)

k

j=1 sj)

Γ(α(i)

j=1 α(i)

j=1 α(i)
j )

i=1Pk

j +Pk

Γ(Pn
i=1Pk
j ) !
Yj=1
h1j, h2j,· · · , hnj(cid:19)
Yj=1(cid:18)

j + hij)
Γ(α(i)

sj

k

j=1 α(i)

j + hi.)

Γ(Pn

j=1 α(i)
j )

sj

h1j, h2j,· · · , hnj(cid:19)(cid:16)
Γ(Pk
(cid:17)  n
Yi=1
Γ(Pk
j=1 sj)Xh1
j +Pk
Yi=1
Yj=1

j=1 α(i)
j )

Γ(α(i)

n

k

· · ·Xhk

j + hij)
Γ(α(i)
j )

.

By considering the fact that the sum of the Dirichlet-multimonial distribu-

tions on their support equals to one, we have

which is the general moment of the k-variate

=

j + sj)

i=1 α(i)
j )

k

Yj=1

j=1 sj)

j=1 α(i)
j )

j +Pk

j=1 α(i)

i=1Pk

Γ(Pn
i=1Pk
Γ(Pn
Dirichlet  n
Xi=1

α(i)
1 ,

n

Xi=1

i=1 α(i)

Γ(Pn
Γ(Pn
k !

α(i)

Xi=1

n

α(i)
2 ,· · · ,

distribution, and since Z is a bounded random variable, its distribution is

uniquely determined by its moments. Thus the proof is complete.

⊠⊞

3.5. A Variant of Theorem 2.1

Theorem 3.1. The distribution of the randomly linear combination Z =

i=1 WiXi is

Pn

Dirichlet  1

2

+

αi! ,

n

Xi=1

8

where X1,· · · , Xn are two-dimensional independent multivariate random vec-
tors with

Dirichlet(cid:16) 1

2

+ α1(cid:17),· · · , Dirichlet(cid:16) 1

2

+ αn(cid:17)

distributions and the random vector W = hW1,· · · , Wni is independent from
(X1,· · · , Xn) and has the distribution

Dirichlet(α1,· · · , αn).

Proof. Let Yj (j = 1,· · · , n) be independent random variables independent
from (X1,· · · , Xn) that have the distribution Γ(αj, µ), respectively. It can
be seen, by some classic ways (e.g. E(cid:0)et′T(cid:1) =(cid:2)Ψ(t)(cid:3)Pj αj from Kubo & Kuo
& Namli (2013), Table 2), that the distribution of T(= Pj Tj = Pj YjXj)
is the same distribution of Tj with the parameter Pj αj. We can also write

T as

T =Xj

YjXj = Xi

Yi! Xj

Xj!

Yj

Pi Yi

and so we have T = Y Z in which Y has the gamma distribution with

deﬁne T′ = Y ′X′ in such a way that T′

the parameter Pj αj, and T has the F distribution with the parameter
Pj αj, and Y and Z are independent from each other. Of course, one can
d= Y ′ and X′ ∼
Dirichlet(cid:16) 1
2 +Pj αj(cid:17). One can conclude that Z and X′ have identical
E(cid:0)Z s1

distributions by calculating the general moments (s1, s2) of T and T′, i.e.,

2 (cid:1) = E(cid:0)(X′1)s1(X′2)s2(cid:1).

d= T, Pj Yj

1 Z s2

⊠⊞

Actually, the above proof also shows that:

Theorem 3.2. Let Yj (j = 1,· · · , n) be independent random variables inde-
pendent from (X1,· · · , Xn) that have the distribution Γ(αj, µ), respectively,

9

where Xi’s are independent from each other and have Dirichlet distributions.

If X has a bounded support and the independent random variable Y has

d= Y X, then X and Z =Pi WiXi
Γ(Pj αj, µ) distribution such thatPi YiXi
have identical distributions, where W = hW1,· · · , Wni is independent from
Xi’s and has Dirichlet(α1,· · · , αn) distribution.

4. Some Applications in Stochastic Diﬀerential Equations

In this section, using Theorem 2.1 and Corollary 2.2, we prove some inter-

esting mathematical facts. As an example consider the following diﬀerential

equation for each n (cf. Homei (2014)):

(1)

(−1)n−1
(n − 1)!

n − 1

2

dn−1

dzn−1 Z 1

0

(1 − t)(n−3)/2(z2 − t)−1/2dt =(cid:18)

1

√z2 − 1(cid:19)n

,

which could be of interest for some authors, who ﬁrst guess the solution and

then, by using techniques like Leibniz diﬀerentiations or change of variables

or integration by part, prove that it satisﬁes the equation (by some long

inductive arguments).

Let us recall that Theorem 1 of Homei (2015) identiﬁes the distribution

of (the 1-dimensional) Z from the distributions of Xi’s by means of the

diﬀerential equation:

(2)

(−1)n∗−1
(n∗ − 1)!

dn∗−1
dzn∗−1S(FZ , z) =

n

Yi=1

(−1)mi−1
(mi − 1)!

dmi−1
dzmi−1S(FXi , z),

where FY denotes the cumulative distribution function of a random variable
z−x FY (dx) for z ∈ C∩ (suppFY )∁

Y and S(FY , z) is deﬁned by S(FY , z) =RR

in which suppFY stands for the support of FY .

1

10

Let the distribution of Xi (i = 1,· · · , n) be Arcsin, that is S(FXi, z) =
. Also, let mi = 1 (i = 1,· · · , n) and n∗ = n. Then from the equa-

1

√z2−1
tion (2) we will have

(3)

(−1)n−1
(n − 1)!

dn−1

dzn−1S(FZ , z) =(cid:18)

1

√z2 − 1(cid:19)n

.

The solution of the equation (3) identiﬁes the Stieltjes transformation of

the distribution of Z. Alternatively, from Theorem 2.1 (or Corollary 2.2)

the distribution of Z is power semicircle (see Homei (2014) for more de-

is n−2

tails). Since the Stieltjes transformation of the power semicircle distribution
2 R 1
0 (1−t)(n−3)/2(z2−t)−1/2dt (see e.g. Arizmendi & Perez-Abreu (2010))
then by substituting it in the equation (2) we will get the equation (1) im-

mediately.

As another example consider the moment generating function on the vec-

tor Tj:

Using the double conditional expectation and the fact that the components

MTj (t′) = E(cid:0) exp(t′YjXj)(cid:1).

of Tj are independent with gamma distributions we have

i=1 α(j)

i =

1

1 − t′Xj(cid:1)Pk

E(cid:0)

k

Yi=1(cid:0)

1

i

1 − ti(cid:1)α(j)

which proves Proposition 4.4 of Kerov & Tsilevich (2004) (cf. also Karlin &

Micchelli & Rinott (1986), page 77).

5. Conclusions

Some sporadic works of other authors have been uniﬁed here; the main re-

sult (Theorem 2.1) has several other diﬀerent proofs (available upon request)

11

each of which can have various applications (the ﬁve proofs presented here

are dedicated to all the family members of Professor A.R. Soltani). Our

method reveals the advantage of the method of Stieltjes transforms for iden-

tifying the distribution of stochastic linear combinations, ﬁrst used by van

Assche (1987) and later generalized by others.

References

References

[1] Alamatsaz, M.H. (1993). On Characterizations of Exponential and

Gamma Distributions, Statistics and Probability Letters 17, 315–319.

[2] Arizmendi, O., Perez-Abreu, V. (2010). On the Non-classical Inﬁ-

nite Divisibility of Power Semicircle Distributions, Communications on

Stochastic Analysis 4, 161–178.

[3] Homei, H. (2015). A Novel Extension of Randomly Weighted Averages,

Statistical Papers 56, 933–946.

[4] Homei, H. (2014). Characterizations of Arcsin and Related Dis-

tributions Based on a New Generalized Unimodality, Commu-

nications

in Statistics – Theory and Methods,

to appear. doi:

10.1080/03610926.2015.1006788

[5] Homei, H. (2013). Uniform Random Sample and Symmetric Beta Dis-

tribution, arXiv:1309.2779 [math.ST].

[6] Homei, H. (2012). Randomly Weighted Averages with Beta Random

Proportions. Statistics and Probability Letters 82, 1515–1520.

12

[7] Johnson, N.L., Kotz, S. (1990). Randomly Weighted Averages:

Some Aspects and Extensions, The American Statistician 44, 245–249.

[8] Johnson, N.L., Kotz, S. (1990a). Use of Moments in Deriving Dis-

tributions and Some Characterizations, The Mathematical Scientist 15,

42–52.

[9] Karlin, S., Micchelli, C. A., Rinott, Y. (1986). Multivariate

Splines: a probabilistic perspective. Journal of Multivariate Analysis

20, 69–90.

[10] Kerov, S. V. E., Tsilevich, N. V. (2004). The Markov-Krein corre-

spondence in several dimensions. Journal of Mathematical Sciences 121,

2345–2359.

[11] Nadarajah, S., Kotz, S. (2005). On the Product and Ratio of

Gamma and Beta Random Variables, Allgemeines Statistisches Archiv

89, 435-449.

[12] Rezapour, M., Alamatsaz, M.H. (2014). Stochastic Comparison of

Lifetimes of Two (n-k+1)-out-of-n Systems with Heterogeneous Depen-

dent Components, Journal of Multivariate Analysis 130, 240–251.

[13] Sethuraman, J. (1994). A Constructive Deﬁnition of Dirichlet Priors,

Statistica Sinica 4, 639–650.

[14] Soltani, A.R. and Homei, H. (2009). Weighted Averages with Ran-

dom Proportions that are Jointly Uniformly Distributed over the Unit

Simplex. Statistics and Probability Letters 79, 1215–1218.

13

[15] van Assche, W. (1987). A Random Variable Uniformly Distributed

Between two Independent Random Variables, Sankhy¯a: The Indian

Journal of Statistics—Series A 49, 207–211.

[16] Volodin, N.A., Kotz, S., Johnson, N.L. (1993). Use of Moments

in Distribution Theory: A Multivariate Case, Journal of Multivariate

Analysis 46, 112–119.

14

