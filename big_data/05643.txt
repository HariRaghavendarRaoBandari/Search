6
1
0
2

 
r
a

 

M
7
1

 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
3
4
6
5
0

.

3
0
6
1
:
v
i
X
r
a

VarianceReductionforFasterNon-ConvexOptimizationZeyuanAllen-Zhuzeyuan@csail.mit.eduPrincetonUniversityEladHazanehazan@cs.princeton.eduPrincetonUniversityﬁrstcirculatedonFebruary5,2016AbstractWeconsiderthefundamentalprobleminnon-convexoptimizationofeﬃcientlyreachingastationarypoint.Incontrasttotheconvexcase,inthelonghistoryofthisbasicproblem,theonlyknowntheoreticalresultsonﬁrst-ordernon-convexoptimizationremaintobefullgradi-entdescentthatconvergesinO(1/ε)iterationsforsmoothobjectives,andstochasticgradientdescentthatconvergesinO(1/ε2)iterationsforobjectivesthataresumofsmoothfunctions.Weprovidetheﬁrstimprovementinthislineofresearch.Ourresultisbasedonthevariancereductiontrickrecentlyintroducedtoconvexoptimization,aswellasabrandnewanalysisofvariancereductionthatissuitablefornon-convexoptimization.Forobjectivesthataresumofsmoothfunctions,ourﬁrst-orderminibatchstochasticmethodconvergeswithanO(1/ε)rate,andisfasterthanfullgradientdescentbyΩ(n1/3).Wedemonstratetheeﬀectivenessofourmethodsonempiricalriskminimizationswithnon-convexlossfunctionsandtrainingneuralnets.1IntroductionNumerousmachinelearningproblemsarenaturallyformulatedasnon-convexoptimizationprob-lems.Examplesincludeinferenceingraphicalmodels,unsupervisedlearningmodelssuchastopicmodels,dictionarylearning,andperhapsmostnotably,trainingofdeepneuralnetworks.Indeed,non-convexoptimizationformachinelearningisoneoftheﬁelds’mainresearchfrontiers.Sinceglobalminimizationofnon-convexfunctionsisNP-hard,variousalternativeapproachesareapplied.Forsomemodels,probabilisticandotherassumptionsontheinputcanbeusedtogivespeciallydesignedpolynomial-timealgorithms[2,3,14].However,themultitudeanddiversityofmachinelearningapplicationsrequirearobust,genericoptimizationmethodthatcanbeappliedasatoolratherthanreinventedpereachspeciﬁcmodel.Oneapproachisthedesignofglobalnon-convexheuristicssuchassimulatedannealing[16]orbayesianoptimization[5].Althoughbelievedtofailintheworstcaseduetoknowncomplexityresults,suchheuristicsmanytimesperformwellinpracticeforcertainproblems.Anotherapproach,whichisbasedonmoresolidtheoreticalfoundationandisgaininginpopu-larity,istodropthe“globaloptimality”requirementandattempttoreachmoremodestsolutionconcepts.Themostpopularoftheseistheuseofiterativeoptimizationmethodstoreachastation-arypoint.Theuseofstochasticﬁrst-ordermethodsistheprimaryfocusofthisapproach,whichhasbecomethemostcommonmethodfortrainingdeepneuralnets.Formally,inthispaperweconsidertheunconstrainedminimizationproblemminx∈Rdnf(x)def=1nnXi=1fi(x)o,(1.1)whereeachfi(x)isdiﬀerentiable,possiblynon-convex,andhasL-Lipschitzcontinuousgradient(alsoknownasL-smooth)forsomeparameterL>0.1Manymachinelearning/imagingprocess-ingproblemsnaturallyfallintotheformof(1.1),includingtrainingneuralnets,trainingERM(empiricalriskminimization)objectiveswithnon-convexlossfunctions,andmanyothers.Followingtheclassicalbenchmarkfornon-convexoptimization(seeforinstanceGhadimiandLan[12]),wefocusonalgorithmsthatcaneﬃcientlyﬁndanapproximatestationarypointxsatisfyingk∇f(x)k2≤ε.Unlikeconvexoptimization,apointxwithsmallgradientmayonlybeclosetoasaddlepointoralocalminimum,ratherthantheglobalminimum.Therefore,suchanalgorithmisusuallycom-binedwithsaddle-pointorlocal-minimaescapingschemes,suchasgeneticalgorithmsorsimulatedannealing.Morerecently,Geetal.[11]alsodemonstratedthatasimplenoise-additionschemeissuﬃcientforstochasticgradientdescenttoescapeusfromsaddlepoints.However,tothebestofourknowledge,forthegeneralproblem(1.1)wheresmoothnessistheonlyassumptionandﬁndingapproximatestationarypointisthesimplegoal,theonlyknowntheoreticalconvergenceresultsremaintobethatforgradientdescent(GD)andstochasticgradientdescent(SGD).•Givenastartingpointx0,GDappliesanupdatex0←x−1L∇f(x)withaﬁxedsteplength1/Lperiteration.Inordertoproduceanoutputxthatisanε-approximatestationarypoint,GDneedsT=O(cid:0)L(f(x0)−f(x∗))ε(cid:1)iterationswherex∗istheglobalminimizeroff(·).Thisisafolkloreresultinoptimizationandincludedforinstancein[12].1Evenifeachfi(x)isnotsmoothbutonlyLipschitzcontinuous,standardsmoothingtechniquessuchasChapter2.3of[13]usuallyturneachfi(x)intoasmoothfunctionwithoutsacriﬁcingtoomuchaccuracy.1•SGDappliesanupdatex0←x−η∇fi(x)periteration,whereichosenuniformlyatrandomfrom[n]def={1,2,...,n}.Ifηisproperlytuned,onecanobtainanε-approximatestationarypointinT=O(cid:0)(cid:0)Lε+Lσ2ε2(cid:1)·(f(x0)−f(x∗))(cid:1)iterations,whereσisthevarianceofthestochasticgradient.ThisresultisperhapsﬁrstformalizedbyGhadimiandLan[12].Sincecomputingthefullgradient∇f(·)isusuallyntimesslowerthanthatof∇fi(x),eachiterationofSGDisusuallyntimesfasterthanthatofGD,butthetotalnumberofiterationsforSGDisverypoor.OurResult.Inthispaper,weprovethatvariancereductiontechniques,basedontheSVRGmethod[15],produceanε-stationarypointinonlyO(cid:0)n2/3L(f(x0)−f(x∗))ε(cid:1)iterations.SinceeachiterationofSVRGisasfastasSGDandntimesfasterthanthatofGD,SVRGisguaranteedtobeatleastΩ(n1/3)timesfasterthanGD.Tothebestofourknowledge,thisistheﬁrsttimetheperformanceofGDisoutperformedintheoryforproblem(1.1)withoutanyadditionalassumption,andalsotheﬁrsttimethatstochastic-gradientbasedmethodsareshowntohaveanon-trivial21/εconvergenceratethatisindependentofthevarianceσ2.OurproposedalgorithmisveryanalogoustoSVRG[15].RecallthatSVRGhasanouterloopofepochs,whereatthebeginningofeachepoch,SVRGdeﬁnesasnapshotvectorextobetheaveragevectorofthepreviousepoch,3andcomputesitsfullgradient∇f(ex).EachepochofSVRGconsistsofminneriterations,wherethechoiceofmusuallydependsontheobjective’sstrongconvexity.Ineachinneriterationinsideanepoch,SVRGpicksarandomi∈[n],deﬁnesthegradientestimatore∇kdef=1nnXj=1∇fj(ex)+∇fi(xk)−∇fi(ex),(1.2)andperformsanupdatex0←x−ηe∇kforsomeﬁxedsteplengthη>0acrossalliterationsandepochs.Inordertoproveourtheoreticalresultinthispaper,wemakethefollowingchangestoSVRG.First,wesetthenumberofinneriterationsmasaconstantfactortimesn.Second,wepickthesnapshotpointtobeanon-uniformaverageofthelastm2/3elementsofthepreviousepoch.Finally,weprovethattheaveragenormk∇f(xk)k2oftheencounteredvectorsxkacrossalliterationsissmall,soitsuﬃcestooutputxkforarandomk.OurTechnique.Toproveourresult,weneeddiﬀerenttechniquesfromallknownliteraturesonvariancereduction.Thekeyideausedbypreviousauthorsistoshowthatthevarianceofthegradientestimatore∇kisupperboundedbyeitherO(f(xk)−f(x∗))orO(kxk−x∗k2),andthereforeitconvergestozeroforconvexfunctions.Thisanalysisfailstoapplyinthenon-convexsettingbecausegradient-basedmethodsdonotconvergetotheglobalminimum.WeobserveinthispaperthatthevarianceisupperboundedbyO(kxk−exk2),thesquareddistancebetweenthecurrentpointandthemostrecentsnapshot.Bydividinganepochintom1/3subepochsoflengthm2/3,andperformingamirror-descentanalysisforeachsubepoch,wefurthershowthatthissquareddistanceisrelatedtotheobjectivedecreasef(ex)−f(xk).Thiswouldsuﬃceforprovingourtheorem:wheneverthissquareddistanceissmalltheobjectiveisdecreasedbyalot2Notehowever,designingastochastic-gradientmethodwithatrivial1/εrateisobvious.Forinstance,itisstraightforwardtodesignsuchamethodthatconvergesinO(cid:0)nL(f(x0)−f(x∗))ε(cid:1)iterations.However,thisisneverfasterthanGD.3Moreprecisely,SVRGprovidestwooptions,onedeﬁningextobetheaveragevectorofthepreviousepoch,andtheotherdeﬁningextobethelastiterateofthepreviousepoch.Whiletheauthorsareonlyabletoprovetheoreticalconvergenceresultforthe“average”deﬁnition,experimentalresultssuggestthatchoosingthelastiterateisbetter.2duetothesmallvariance,orotherwiseifthissquareddistanceislargewestillexperiencealargeobjectivedecreasebecauseitisrelatedtof(ex)−f(xk).Applications.Therearemanymachinelearningproblemsthatfallintocategory(1.1).Tomentionjusttwo:•Non-ConvexLossinERMEmpiricalriskminimization(ERM)problemsnaturallyfallintothecategoryof(1.1)ifthelossfunctionsarenon-convex.Forinstance,forbinaryclassiﬁcationproblems,thesigmoidfunction—ormorebroadly,anynaturalsmoothedvariantofthe0-1lossfunction—isnotonlyamorenaturalchoicethanartiﬁcialonessuchashingeloss,logisticloss,squaredloss,butalsogeneralizebetterintermsoftestingaccuracyespeciallywhenthereareoutliers[24].However,sincesigmoidlossisnon-convex,itwaspreviouslyconsideredhardtotrainanERMproblemwithit.Shalev-Shwartz,ShamirandSridharan[24]showedthatthisminimizationproblemisstillsolvableintheimproperlearningsense,withthehelpfromkernelmethodsandgradientdescent.However,theirtheoreticalconvergencehasapoorpolynomialdependenceon1/εandexponentialdependenceonthesmoothnessparameterofthelossfunction.OurresultinthispaperappliestoERMproblemswithnon-convexloss.Supposewearegivenntrainingexamples{(a1,‘1),...(an,‘n)},whereeachai∈Rdisthefeaturevectorofexampleiandeachli∈{−1,+1}isthebinarylabelofexamplei.Bysettingφ(t)def=11+ettobethesigmoidlossfunctionandsettingfi(x)def=φ(lihai,xi)+λ2kxk2,problem(1.1)becomes‘2ERMwithsigmoidloss.Weshalldemonstrateinourexperimentsectionthat,byusingSVRGtotrainERMwithsigmoidloss,itsrunningtimeisasgoodasusingSVRGtotrainERMwithotherconvexlossfunctions,butthetestingaccuracycanbesigniﬁcantlybetter.•NeuralNetworkTrainingneuralnetscanalsobeformalizedintoproblem(1.1).Forinstance,aslongastheactivationfunctionofeachneuralnodeissmooth,saythesigmoidfunctionorasmoothversionoftherectiﬁedlinearunit(ReLU)function(forinstance,thesoftplusalternative),wecandeﬁnefi(x)tobethetraininglosswithrespecttothei-thdatainput.Inthislanguage,computingthestochasticgradient∇fi(x)forsomerandomi∈[n]correspondstoperformingoneforward-backwardprorogationontheneuralnetwithrespecttosamplei.WeshalldemonstrateinourexperimentthatusingSVRGtotrainneuralnetscanenjoyamuchfasterrunningtimecomparingtoSGDorSVRG.OtherExtensions.Ourresultinthispapernaturallyextendstothemini-batchsetting:ifineachiterationweselectfi(·)formorethanonerandomindicesi,thenwecanaccordinglydeﬁnethegradientestimatorandtheresultofthispaperstillholds.NotethatthespeedupthatweobtaininthiscasecomparingtogradientdescentisO((n/b)1/3)wherebisthemini-batchsize.Therefore,thesmallerbisthebettersequentialrunningtimeweexpecttosee(whichisalsoobservedinourexperiments).Ourresultalsoappliestothesettingwheneachfi(·)enjoysadiﬀerentsmoothnessparameter.Inthissettingoneneedstoselectarandomindexi∈[n]withanon-uniformdistribution.Weomitthedetailsinthisversionofthepaper.OtherRelatedWork.Forconvexobjectives,ﬁndingstationarypoints(orequivalentlytheglobalminimum)forproblem(1.1)hasreceivedlotsofattentionsacrossmachinelearningandoptimizationcommunities;manyﬁrst-ordermethods[6,7,15,19,22,25,27]aswellastheir3Algorithm1ThesimpliﬁedSVRGmethodinthenon-convexsettingInput:xφastartingvector,Snumberofepochs,mnumberofiterationsperepoch,ηsteplength.1:x10←xφ2:fors←1toSdo3:eµ←∇f(xs0)4:fork←0tom−1do5:Pickiuniformlyatrandomin{1,···,n}.6:e∇←∇fi(xsk)−∇fi(xs0)+eµ7:xsk+1=xsk−ηe∇8:endfor9:xs+10←xsm10:endfor11:returnxskforsomerandoms∈{1,2,...,S}andrandomk∈{1,2,...,m}.accelerations[18,26,28]havebeenproposedinthepastafewyears.Eveninthecasewhenf(·)isconvexbuteachfi(·)isnon-convex,theproblem(1.1)canbesolvedeasily[10,23].TherecentinterestingresultsofLiandLin[17]andGhadimiandLan[12]unifythetheoryofnon-convexandconvexoptimizationinthefollowingsense.Theyprovidegeneralﬁrst-orderschemessuchthat,iftheparametersaretunedproperly,theschemescanconverge(1)asfastasgradientdescentintermsofﬁndinganapproximatestationarypointifthefunctionisnon-convex;and(2)asfastasacceleratedgradientdescent[21]intermsofminimizingtheobjectiveifthefunctionisconvex.However,fortheclassof(1.1),thebestperformanceoftheirmethodsareonlyasslowasGD;incontrast,inthispaperweprovetheoreticalconvergencethatisstrictlyfasterthanGD.2NotationsandAlgorithmAdiﬀerentiablefunctionf:Rn→RisL-smoothifforallpairsx,y∈Rnitsatisﬁesk∇f(x)−∇f(y)k≤Lkx−yk.Anequivalentdeﬁnitionsaysforforallx,y∈Rn:−L2kx−yk2≤f(x)−f(y)−h∇f(y),x−yi≤L2kx−yk2(2.1)Themainbodyofthispaperprovesourresultbasedonthreefalsesimpliﬁcationassumptions4.2,4.3and4.4forthesakeofsketchingthehigh-levelintuitionsandhighlightingthediﬀerencesbetweenourproofandknownresults.Ourformalconvergenceproofisquitetechnicalandincludedintheappendix.Inthishigh-levelproof,weconsiderAlgorithm1,asimpliﬁedversionofourﬁnalSVRGmethodforthenon-convexsetting.Notethatboththesnapshotpointandthestartingiteratexs0ofthes-thepochhavebeenchosenasthelastiterateofthepreviousepochinAlgorithm1.Remark2.1.Inourﬁnalproof,weinsteadchoosexs0tobeaweightedaverageofthelastm2/3iteratesfromthepreviousepoch.SeeAlgorithm2intheappendix.WedemonstrateinSection6thatthisalsoabetterchoiceinpractice.Throughoutthispaperwedenotebyxskthek-thiterateofthes-thepoch,by∇sk=∇f(xsk)thefullgradientatthisiterate,andbye∇sk=∇f(xs0)+∇if(xsk)−∇if(xs0)thegradientestimatorwhichclearlysatisﬁesEi[e∇sk]=∇sk.Wedenotebyisktherandomindexichosenatiterationkofepoch4s.Wealsodenoteby(σsk)2def=k∇sk−e∇skk2thevarianceofthegradientestimatore∇sk.Underthesenotations,oursimpliﬁedSVRGalgorithminAlgorithm1simplyperformsupdatexsk+1←xsk−ηe∇skforaﬁxedsteplengthη>0thatshallbespeciﬁedlater.Sincewefocusmostlyonanalyzingasingleepoch,whenitisclearfromthecontext,wedropthesuperscriptsanddenotebyxk,ik,∇k,e∇k,σ2krespectivelyforxsk,isk,∇sk,e∇sk,(σsk)2.WealsodenotebyH2kdef=k∇kk22,σ2i:jdef=Pjk=iσ2kandH2i:jdef=Pjk=iH2kfornotationalsimplicity.3TwoUsefulLemmasWeﬁrstobservetwosimplelemmas.Theﬁrstonedescribestheexpectedobjectivedecreasebetweentwoconsecutiveiterations.Thisisastandardstepthatisusedinanalyzinggradientdescentforsmoothfunctions,andweadditionallytakeintoaccountthevarianceofthegradientestimator.Lemma3.1(gradientdescent).Ifxk+1=xk−ηe∇kforsomegradientestimatore∇ksatisfyingE[e∇k]=∇k=∇f(xk),andifthesteplengthη≤1L,wehavef(xk)−E[f(xk+1)]≥η2∇2k−η2L2E(cid:2)σ2k(cid:3).Proof.Bythesmoothnessofthefunction,wehaveE[f(xk+1)]≤f(xk)+E(cid:2)h∇f(xk),xk+1−xki(cid:3)+EhL2kxk+1−xkk2i=f(xk)−ηk∇f(xk)k2+η2L2Ehke∇kk2i=f(xk)−ηk∇f(xk)k2+η2L2Ehk∇f(xk)k2+ke∇k−∇f(xk)k2i.ThisimmediatelyyieldsLemma3.1byusingtheassumptionthatη≤1L.(cid:3)Thenextlemmafollowsfromtheclassicalanalysisofmirrordescentmethods.4However,wemakenoveluseofitontopofanon-convexbutsmoothfunction.Lemma3.2(mirrordescent).Ifxk+1=xk−ηe∇kforsomegradientestimatore∇ksatisfyingE[e∇k]=∇k=∇f(xk),thenforeveryu∈Rditsatisﬁesf(xk)−f(u)≤η2(cid:0)H2k+E[σ2k](cid:1)+(cid:0)12η+L2(cid:1)kxk−uk2−12ηE(cid:2)kxk+1−uk2(cid:3).Proof.Weﬁrstwritethefollowinginequalitywhichfollowsfromclassicalmirror-descentanalysis.Foreveryu∈Rd,itsatisﬁesh∇k,xk−ui=E[he∇k,xk−ui]=E[he∇k,xk−xk+1i+he∇k,xk+1−ui]=E[he∇k,xk−xk+1i−12ηkxk−xk+1k2+12ηkxk−uk2−12ηkxk+1−uk2]≤E(cid:2)η2ke∇kk2+12ηkxk−uk2−12ηkxk+1−uk2(cid:3).(3.1)4Mirrordescentisaterminologymostlyusedinoptimizationliteratures,seeforinstancethetextbook[4].Inmachinelearningcontexts,mirror-descentanalysisisessentiallyidenticaltoregretanalysis.InourSVRGmethod,thedescentstepxsk+1←xsk−ηe∇skcanbeinterpretedasamirrordescentstepintheEuclideanspace(seeforinstance[1]),andthereforemirror-descentanalysisapplies.5Above,he∇k,xk+1−ui=−12ηkxk−xk+1k2+12ηkxk−uk2−12ηkxk+1−uk2isknownasthethree-pointequalityofBregmandivergence(inthespecialcaseofEuclideanspace).Theonlyinequalityisbecausewehave12kak2+12kbk2≥ha,biforanytwovectorsa,b.Classicallyinconvexoptimization,onewouldlowerboundthelefthandsideof(3.1)byf(xk)−f(u)usingtheconvexityoff(·).Wetakeadiﬀerentpathherebecauseourobjectivef(·)isnotconvex.Instead,weusethelowersmoothnesspropertyoffunctionfwhichistheﬁrstinequalityin(2.1)todeducethath∇k,xk−ui≥f(xk)−f(u)−L2kxk−uk2.Combiningthiswithinequality(3.1),andtakingintoaccountE[ke∇kk2]=k∇kk2+E[σ2k]bythedeﬁnitionofvariance,weﬁnishtheproofofLemma3.2.(cid:3)4UpperBoundingtheVarianceHigh-LevelIdeas.Thekeyideabehindallvariance-reductionliteratures(suchasSVRG[15],SAGA[6],andSAG[22])istoprovethatthevarianceE[(σsk)2]decreasesassorkincreases.However,theonlyknowntechniquetoachievesoistoupperboundE[(σsk)2]byO(cid:0)f(xsk)−f(x∗)(cid:1),theobjectivedistancetotheminimum.Perhapstheonlyexceptionistheworkonsum-of-non-convexbutstrongly-convexobjectives[10,23],wheretheauthorsupperboundE[(σsk)2]byO(cid:0)kxsk−x∗k2(cid:1),thesquaredvectordistancetotheminimum.Suchtechniquesfailtoapplyinournon-convexsetting,becausegradient-descentbasedmethodsdonotnecessarilyconvergetotheglobalminimum.Wetakeadiﬀerentpathinthispaper.WeupperboundE[(σsk)2]byO(cid:0)kxsk−xs0k2(cid:1),thesquaredvectordistancebetweenthecurrentvectorxskandtheﬁrstvector(i.e.,thesnapshot)xs0ofthecurrentepochs.Inotherwords,thelesswemoveawayfromthesnapshot,thebetterupperboundweobtainonthevariance.Thisisconceptuallydiﬀerentfromallexistingliteratures.Furthermore,weinturnsarguethatkxsk−xs0k2isatmostsomeconstanttimesf(xsk)−f(xs0).Toproveso,wewishtoselectu=xs0inLemma3.2andtelescopeitformultipleiterationsk,ideallyforalltheiterationskwithinthesameepoch.Thisispossibleforconvexobjectivesbutimpossiblefornon-convexones.Morespeciﬁcally,theratiobetween(1/2η+L/2)and(1/2η)canbemuchlargerthan1,preventingusfromtelescopingmorethanO(1/ηL)iterationsinanymeaningfulmanner(see(4.1)).Incontrast,thisratiowouldbeidenticalto1intheconvexsetting,orevensmallerthan1inthestronglyconvexsetting.Forthisreason,wedeﬁneη=1m2/3L,divideeachepochintoO(m1/3)subepochseachconsistingofO(m2/3)consecutiveiterations.NowwecantelescopeLemma3.2foralltheiterationswithinasubepochbecausem2/3≤O(1/ηL).Finally,weusevectorinequalities(see(4.5))tocombinetheseupperboundsforsub-epochsintoanupperboundontheentireepoch.TechnicalDetails.Wechooseη=1m0Lforsomeparameterm0thatdividesm.Wewillsetm0tobem2/3andthereasonwillbecomeclearattheendofthissection.Deﬁned=m/m0soanepochisdividedintodsub-epochs.WemakethefollowingparameterchoicesDeﬁnition4.1.Deﬁneβ0=1andβtdef=(1+ηL)−t=(1+1/m0)−tfort=1,...,m0−1.Wehave1≥βt≥1/e>1/3.Foreachk=0,1,...,m−m0,wesumupLemma3.2foriterationsk,k+1,...,k+m0−1withmultiplicativeweightsβ0,...,βm0−1respectively.Thenormsquaretermsshalltelescopein6thissummation,andwearriveatm0−1Xt=0βt(cid:0)f(xk+t)−f(u)(cid:1)≤η2m0−1Xt=0βt(cid:0)H2k+t+σ2k+t(cid:1)+(cid:0)12η+L2(cid:1)kxk−uk2−βm0−12ηkxk+m0−uk2.(4.1)Simpliﬁcation4.2.Sincetheweightsβ0,...,βm0−1arewithineachotherbyaconstantfactor,letusassumeforsimplicitythattheyareallequalto1.Ifwechooseu=xkandassumeβt=1forallt,wecanrewrite(4.1)as1m0m0−1Xt=0(cid:0)f(xk+t)−f(xk)(cid:1)≤η21m0(cid:0)H2k:k+m0−1+σ2k:k+m0−1(cid:1)−16ηm0kxk+m0−xkk2.(4.2)Simpliﬁcation4.3.Sincethelefthandsideof(4.2)isdescribingtheaverageobjectivevaluef(xk),f(xk+1),...,f(xk+m0−1)whichishardtoanalyze,letusassumeforsimplicitythatitcanbereplacedwiththelastiterationinthissubepoch,thatisf(xk+m0)−f(xk)≤η21m0(cid:0)H2k:k+m0−1+σ2k:k+m0−1(cid:1)−16ηm0kxk+m0−xkk2.(4.3)Usingtheaboveinequalityweprovideanovelupperboundonthevarianceofthegradientestimator:Eit(cid:2)σ2t(cid:3)=Eit(cid:2)(cid:13)(cid:13)(cid:0)∇fit(xt)−∇fit(x0)(cid:1)−(cid:0)∇f(xt)−∇f(x0)(cid:1)(cid:13)(cid:13)2(cid:3)≤Eit(cid:2)(cid:13)(cid:13)∇fit(xt)−∇fit(x0)(cid:13)(cid:13)2(cid:3)≤L2kxt−x0k2.(4.4)Above,theﬁrstinequalityisbecauseforanyrandomvectorζ∈Rd,itholdsthatEkζ−Eζk2=Ekζk2−kEζk2,andthesecondinequalityisbythesmoothnessofeachfi(·).Inparticular,fort=m,wecanupperboundσ2musing(4.4)andmultipletimesof(4.3):E[σ2m]≤L2E(cid:2)kxm−x0k2(cid:3)≤L2dE(cid:2)kxm−xm−m0k2+kxm−m0−xm−2m0k2+···+kxm0−x0k2(cid:3)≤L2dEh6ηm0(cid:0)f(x0)−f(xm)(cid:1)+3η2(cid:0)H20:m−1+σ20:m−1(cid:1)i.(4.5)Above,theﬁrstinequalityfollowsfromthevectorinequalitykv1+···+vdk2≤d(cid:0)kv1k2+···+kvdk2(cid:1),andthesecondinequalityfollowsfrom(4.3).Simpliﬁcation4.4.Supposethat(4.5)holdsnotonlyforσ2mbutactuallyforallσ20,...,σ2m−1,thenitsatisﬁes1mE[σ20:m−1]≤L2dEh6ηm0(cid:0)f(x0)−f(xm)(cid:1)+3η2(cid:0)H20:m−1+σ20:m−1(cid:1)i.(4.6)Aslongas3η2L2d≤12m,(4.6)furtherimplies1mE[σ20:m−1]≤12ηm0L2d·E(cid:2)f(x0)−f(xm)(cid:3).(4.7)Thisconcludesourgoalinthissectionwhichistoprovideanupperbound(4.7)onthe(average)variancebyaconstanttimestheobjectivediﬀerencef(x0)−f(xm).75FinalTheoremByapplyingthegradientdescentguaranteeLemma3.1totheentireepoch.Weobtainthatf(x0)−E[f(xm)]≥Ehη2H20:m−1−η2L2σ20:m−1i.Combiningthiswiththevarianceupperbound(4.7),weimmediatelyhavef(x0)−E[f(xm)]≥η2E[H20:m−1]−6η3m0mL3d·E[f(x0)−f(xm)].(5.1)Inotherwords,aslongas6η3m0mL3d≤12,wearriveatf(x0)−E[f(xm)]≥η3E[H20:m−1].(5.2)Notethat(5.2)isonlyforasingleepochandcanbewrittenasf(xs0)−E[f(xsm)]≥η3E[Pm−1t=0k∇f(xst)k2]inthegeneralnotation.Therefore,wecantelescopeitoverallepochss=1,2,...,S.Sincewehavechosenxs0,theinitialvectorinepochs,tobexs−1m,thelastvectorofthepreviousepoch,weobtainthat1SmSXs=1m−1Xt=0E(cid:2)k∇f(xst)k2(cid:3)≤3ηSm(f(x10)−f(xSm))≤3(f(xφ)−minxf(x))ηSm.Atthispoint,ifwerandomlyselects∈[S]andt∈[m]attheendofthealgorithm,weconcludethatE[k∇f(xst)k2]≤3(f(xφ)−minxf(x))ηSm.(WeremarkherethatselectinganaverageiteratetooutputisacommonstepalsousedbyGDorSGDfornon-convexoptimization.Thisstepisoftenunnecessarilyinpractice.)Finally,letusechoosetheparametersproperly.Wesimplyletm=nbetheepochlength.Sincewehaverequired3η2L2d≤12mand6η3m0mL3d≤12intheprevioussection,andboththeserequirementscanbesatisﬁedwhenm30≥12m2,wesetm0=Θ(m2/3)=Θ(n2/3).Accordinglyη=1m0L=Θ(cid:0)1n2/3L(cid:1).Insum,Theorem5.1.Underthesimpliﬁcationassumptions4.2,4.3and4.4,bychoosingm=nandη=Θ(cid:0)1n2/3L(cid:1),theproducedoutputxofAlgorithm1satisﬁesthat5E[k∇f(x)k2]≤O(cid:16)L(f(xφ)−minxf(x))Sn1/3(cid:17).Inotherwords,toobtainapointxsatisfyingk∇f(x)k2≤ε,thetotalnumberofiterationsneededforAlgorithm1isSn=O(cid:16)n2/3L(f(xφ)−minxf(x))ε(cid:17).Theamortizedper-iterationcomplexityofSVRGisatmosttwiceofSGD.Therefore,thisisafactorofΩ(n1/3)fasterthanthefullgradientdescentmethodonsolving(1.1).5LikeinSGD,onecaneasilyapplyaMarkovinequalitytoconcludethatwithprobabilityatleast2/3wehavethesameasymptoticupperboundonthedeterministicquantityk∇f(x)k2.80.9630.9680.9730.9780.9830.98801020304050(a)web,noﬂip0.9630.9680.9730.9780.9830.98801020304050(b)web,ﬂip1/8fraction0.9630.9680.9730.9780.9830.98801020304050(c)web,ﬂip1/4fractionFigure1:TestingaccuracycomparisononSVRGfor‘2-regularizedERMwithdiﬀerentlossfunc-tions.Thefullplotsforallthe4datasetscanbefoundinFigure4intheappendix.Blacksolidcurvesrepresentsigmoidloss,bluedashcurvesrepresentsquareloss,greendash-dottedcurvesrepresentlogisticloss,andthethreereddottedcurvesrepresenthingelosswith3diﬀerentsmooth-ingparameters.Theyaxisrepresentsthetestingclassiﬁcationaccuracy,andthexaxisrepresentsthenumberofpassestothedataset.6Experiments6.1EmpiricalRiskMinimizationwithNon-ConvexLossWeconsiderbinaryclassiﬁcationonfourstandarddatasetsthatcanbefoundontheLibSVMwebsite[9]:•theadult(a9a)dataset(32,561trainingsamples,16,281testingsamples,and123features).•theweb(w8a)dataset(49,749trainingsamples,14,951testingsamples,and300features).•thercv1(rcv1.binary)dataset(20,242trainingsamples,677,399testingsamples,and47,236features).•themnist(class1)dataset(60,000trainingsamples,10,000testingsamples,and780features,AccuracyExperiment.IntheﬁrstexperimentweapplySVRGontrainingthe‘2-regularizedERMproblemwithsixlossfunctions:logisticloss,squaredloss,smoothedhingeloss(withsmooth-ingparameters0.01,0.1and1resp.),andsmoothedzero-oneloss(alsoknownassigmoidloss).6Wewishtoseehownon-convexlosscomparestoconvexonesintermsoftestingaccuracy(andthusintermsofthegeneralizationerror).Foreachofthefourdatasets,wealsorandomlyﬂip1/4fraction,1/8fraction,orzerofractionofthetrainingexamplelabels.Thepurposeofthismanipulationistointroduceoutlierstothetrainingset.Wethereforehave4×3=12datasetsintotal.Wechooseepochlengthm=2nassuggestedbythepaperSVRGforERMexperiments,andusethesimpleAlgorithm1forbothconvexandnon-convexlossfunctions.WepresenttheaccuracyresultspartiallyinFigure1(andthefullversioncanbefoundinFigure4intheappendix).Theseplotsareproducedbasedonafairandcarefulparameter-tuningandparameter-validationprocedurethatcanbedescribedinthefollowingfoursteps.StepI:for6Forthesigmoidloss,wescaleitproperlysothatitssmoothnessparameterisexactly1.Unlikehingeloss,itisunnecessarytoconsidersigmoidlosswithdiﬀerentsmoothnessparameters:onecancarefullyverifythatbyscalingupordowntheweightofthe‘2regularizer,itisequivalenttochangingthesmoothnessofthesigmoidloss.90.120110.1201150.120120.1201250.120130.1201350.12014020406080100(a)web,λ=10−30.083930.083940.083950.083960.083970.083980.08399020406080100(b)web,λ=10−40.06680.066850.06690.066950.067020406080100(c)web,λ=10−50.05830.05850.05870.05890.05910.05930.0595020406080100(d)web,λ=10−6Figure2:TrainingerrorcomparisonbetweenSGDandSVRGon‘2-regularizedERMwithsigmoidloss.ThefullplotscanbefoundinFigure5intheappendix.Thebest-tunedSGDispresentedinsolidgreen,thebest-tunedSVRGwithconstantsteplengthispresentedindashedblue,andthebest-tunedSVRGispresentedindotedblack.Theyaxisrepresentsthetrainingobjectivevalue,andthexaxisrepresentsthenumberofpassestothedataset.eachofthe12datasets,wepartitionthetrainingsamplesrandomlyintoatrainingsetofsize4/5andavalidationsetofsize1/5.StepII:foreachofthe12datasetsandeachlossfunction,weenumerateover10choicesofλ,theregularizationparameter.Foreachλ,wetuneSVRGonthetrainingsetwithdiﬀerentsteplengthsηandchoosethebestηthatgivesthefastesttrainingspeed.StepIII:foreachofthe12datasetsandeachlossfunction,wetunethebestλusingthevalidationset.Thatis,weusetheselectedηfromStepIItotrainthelinearpredictor,andapplyitonthevalidationsettoobtainthetestingaccuracy.Wethenselecttheλthatgivesthebesttestingaccuracyforthevalidationset.StepIV:foreachofthe12datasetsandeachlossfunction,weapplythevalidatedlinearpredictortothetestingsetandpresentitinFigure1andFigure4.Wemakethefollowingobservationsfromthisexperiment.•Althoughsigmoidlossisonlycomparabletohingelossorlogisticlossfor“noﬂip”datasets,however,whentheinputhasalotofoutliers(see“ﬂip1/8”and“ﬂip1/4”),sigmoidlossisundoubtedlythebestchoice.Squarelossisalmostalwaysdominatedbecauseitisnotnecessarilyagoodchoiceforbinaryclassiﬁcation.•TherunningtimeneededforSVRGonthesedatasetsarequitecomparable,regardlessofthelossfunctionbeingconvexornot.Running-TimeExperiment.Inthissecondexperiment,weﬁxtheregularizationparameterλandcomparebacktobackthetrainingobjectiveconvergencebetweenSGDandSVRGforsigmoidlossonly.7WechoosefourdiﬀerentλperdatasetandpresentourplotspartiallyinFigure2(andthefullplotscanbefoundinFigure5intheappendix).Forafaircomparisonweneedtotunethesteplengthηforeachdatasetandeachchoiceofλ.ForSGD,weenumerateoverpolynomiallearningratesηk=α·(1+k/n)βwherekisthenumberofiterationspassed;wehavemade10choicesofα,consideredβ=0,0.1,...,1.0,andselectedthelearningratethatgivesafastestconvergence.ForSVRG,weﬁrstconsiderthevanillaSVRGusingaconstantηthroughoutalliterations,andselectthebestηthatgivesthefastestconvergence.ThiscurveispresentedindashedblueinFigure5.WealsoimplementSVRGwithpolynomiallearningratesηk=α·(1+k/n)βandtunethebestα,βparametersandpresenttheresultsindashedblackcurvesinFigure5.101.581.631.681.731.781.831.88050100150200SVRG-1SVRG-2SVRG-3SVRG-4SGDAdaGrad(a)cifar-10,λ=01.931.941.951.961.971.981.992050100150200SVRG-1SVRG-2SVRG-3SVRG-4SGDAdaGrad(b)cifar-10,λ=10−30.0180.0280.0380.0480.0580.0680.0780.088050100150200SVRG-1SVRG-2SVRG-3SVRG-4SGDAdaGrad(c)mnist,λ=00.1030.1130.1230.1330.1430.153050100150200SVRG-1SVRG-2SVRG-3SVRG-4SGDAdaGrad(d)mnist,λ=10−4Figure3:TrainingErrorComparisononneuralnets.Theyaxisrepresentsthetrainingobjectivevalue,andthexaxisrepresentsthenumberofpassestothedataset.Wemakethefollowingobservationsfromthisexperiment.•Consistentwiththeory,SVRGisnotnecessarilyabetterchoicethanSGDforlargetrainingerrorε.However,SVRGenjoysaveryfastconvergenceespeciallyforsmallε.•Thesmallerλis,themore“non-convex”theobjectivefunctionbecomes.WeseethatSGDperformsmorepoorerthanSVRGinthesecases.8•Withonlyoneexception(datasetwebwithλ=10−6),choosingapolynomiallearningratedoesnotseemnecessaryintermsofimprovingtherunningtimefortrainingERMproblemswithnon-convexloss.•AlthoughnotpresentedinFigure5,thebest-tunedpolynomiallearningratesforSVRGhavesmallerexponentsβascomparedtoSGDineachofthe16plots.6.2NeuralNetworkWeconsiderthemulti-class(infact,10-class)classiﬁcationproblemonCIFAR-10(60,000train-ingsamples)andMNIST(10,000trainingsamples),twostandardimagedatasetsforneuralnetstudies.Weconstructatoytwo-layeredneuralnet,with(1)64hiddennodesintheﬁrstlayer,eachconnectingtoauniformlydistributed4x4or5x5pixelblockoftheinputimageandhavingasmoothedrelu(alsoknownassoftplus)activationfunction;(2)10outputnodesonthesecondlayer,fullyconnectedtotheﬁrstlayerandeachrepresentingoneofthetenclassiﬁcationoutputs.Weconsidertrainingsuchneuralnetworkswiththemulti-classlogisticlossthatisafunctiononthe10outputsandthecorrectlabel.Foreachofthetwodatasets,weconsiderbothtrainingtheunregularizedversion,aswellasthe‘2regularizedversionwithweight10−3forCIFAR-10and10−4forMNIST,twoparameterssuggestedby[15].Weimplementtwoclassicalalgorithms:stochasticgradientdescent(SGD)withthebesttunedpolynomiallearningrateandadaptivesubGradientmethod(AdaGrad)of[8,20]whichisessentiallySGDbutwithanadaptivelearningrate.Wechooseamini-batchsizeof100forboththesemethods.WeconsiderfourvariantsofSVRG,allofwhichuseepochlengthm=5n:•SVRG-1,thesimpleAlgorithm1withabesttunedpolynomiallearningrateandmini-batchsize100.7Thisexperimentistheminimizationproblemwithrespecttoallthetrainingsamplesbecausethereisnoneedtoperformvalidation.8Notethattheplotsfordiﬀerentvaluesλarepresentedwithdiﬀerentverticalscales.Forinstance,at100passesofthedataset,theobjectivediﬀerencebetweenSVRGandSGDismorethan2×10−4forλ=10−6ondatasetweb,butlessthan5×10−6forλ=10−3.11•SVRG-2,ourfullAlgorithm2withabesttunedpolynomiallearningrateandmini-batchsize100.9•SVRG-3,usingadaptivelearningrate(similartoAdaGrad)ontopofSVRG-2withmini-batchsize100.•SVRG-4,sameasSVRG-3butwithmini-batchsize16.OurtrainingerrorperformanceispresentedinFigure3.(WealsoincludethetestingaccuracyinFigure6intheappendix.)FromtheplotsweclearlyseeaperformanceadvantageforusingSVRG-basedalgorithmsascomparedtoSGDorAdaGrad.Furthermore,weobservethatthefollowingthreefeaturesontopofSVRGcouldfurtherimproveitsrunningtime:1.BycomparingSVRG-2withSVRG-1,weseethatsettingtheinitialvectorofanepochtobeaweightedaverageofthelastafewiterationsofthepreviousepochisrecommended.2.BycomparingSVRG-3withSVRG-2,weseethatusingadaptivelearningratescomparingtotuningthebestpolynomiallearningrateisrecommended.3.BycomparingSVRG-4withSVRG-3,weseethatasmallermini-batchsizeisrecommendedintermsofthetotalcomplexity.Incontrast,reducingthemini-batchsizeisdiscouragedforSGDorAdaGradbecausethevariancecouldblowupandtheperformanceswouldbedecreased(thisisalsoobservedbyourexperimentbutnotincludedintheplots).Wehopethattheaboveobservationsprovidenewinsightsforexperimentalistsworkingondeeplearning.References[1]ZeyuanAllen-ZhuandLorenzoOrecchia.Linearcoupling:Anultimateuniﬁcationofgradientandmirrordescent.ArXive-prints,abs/1407.1537,July2014.[2]SanjeevArora,RongGe,YonatanHalpern,DavidM.Mimno,AnkurMoitra,DavidSontag,YichenWu,andMichaelZhu.Apracticalalgorithmfortopicmodelingwithprovableguaran-tees.InProceedingsofthe30thInternationalConferenceonMachineLearning,ICML2013,Atlanta,GA,USA,16-21June2013,pages280–288,2013.[3]SanjeevArora,RongGe,andAnkurMoitra.Newalgorithmsforlearningincoherentandovercompletedictionaries.InProceedingsofThe27thConferenceonLearningTheory,COLT2014,Barcelona,Spain,June13-15,2014,pages779–806,2014.[4]AharonBen-TalandArkadiNemirovski.LecturesonModernConvexOptimization.SocietyforIndustrialandAppliedMathematics,January2013.[5]EricBrochu,VladMCora,andNandodeFreitas.Atutorialonbayesianoptimizationofex-pensivecostfunctions,withapplicationtoactiveusermodelingandhierarchicalreinforcementlearning.eprintarXiv:1012.2599,arXiv.org,December2010.9Thatis,wesettheinitialvectorofeachepochtobeaweightedaverageofthelast(m/b)2/3vectorsfromthepreviousepoch;here,bisthemini-batchsize.12[6]AaronDefazio,FrancisBach,andSimonLacoste-Julien.SAGA:AFastIncrementalGradientMethodWithSupportforNon-StronglyConvexCompositeObjectives.InAdvancesinNeuralInformationProcessingSystems,NIPS2014,2014.[7]AaronJ.Defazio,Tib´erioS.Caetano,andJustinDomke.Finito:AFaster,PermutableIncrementalGradientMethodforBigDataProblems.InProceedingsofthe31stInternationalConferenceonMachineLearning,ICML2014,2014.[8]JohnDuchi,EladHazan,andYoramSinger.Adaptivesubgradientmethodsforonlinelearningandstochasticoptimization.TheJournalofMachineLearningResearch,12:2121–2159,2011.[9]Rong-EnFanandChih-JenLin.LIBSVMData:Classiﬁcation,RegressionandMulti-label.Accessed:2015-06.[10]DanGarberandEladHazan.FastandsimplePCAviaconvexoptimization.ArXive-prints,September2015.[11]RongGe,FurongHuang,ChiJin,andYangYuan.Escapingfromsaddlepoints—onlinestochasticgradientfortensordecomposition.InProceedingsofthe28thAnnualConferenceonLearningTheory,COLT2015,2015.[12]SaeedGhadimiandGuanghuiLan.Acceleratedgradientmethodsfornonconvexnonlinearandstochasticprogramming.MathematicalProgramming,pages1–26,feb2015.[13]EladHazan.DRAFT:Introductiontoonlineconvexoptimimization.FoundationsandTrendsinMachineLearning,XX(XX):1–168,2015.[14]DanielHsu,ShamM.Kakade,andTongZhang.Aspectralalgorithmforlearninghiddenmarkovmodels.J.Comput.Syst.Sci.,78(5):1460–1480,2012.[15]RieJohnsonandTongZhang.Acceleratingstochasticgradientdescentusingpredictivevari-ancereduction.InAdvancesinNeuralInformationProcessingSystems,NIPS2013,pages315–323,2013.[16]S.Kirkpatrick,C.D.Gelatt,andM.P.Vecchi.Optimizationbysimulatedannealing.SCI-ENCE,220(4598):671–680,1983.[17]HuanLiandZhouchenLin.AcceleratedProximalGradientMethodsforNonconvexProgram-ming.InAdvancesinNeuralInformationProcessingSystems-NIPS’15,pages379—-387,2015.[18]QihangLin,ZhaosongLu,andLinXiao.AnAcceleratedProximalCoordinateGradientMethodanditsApplicationtoRegularizedEmpiricalRiskMinimization.InAdvancesinNeuralInformationProcessingSystems,NIPS2014,pages3059–3067,2014.[19]JulienMairal.IncrementalMajorization-MinimizationOptimizationwithApplicationtoLarge-ScaleMachineLearning.SIAMJournalonOptimization,25(2):829–855,April2015.PreliminaryversionappearedinICML2013.[20]H.BrendanMcMahanandMatthewStreeter.AdaptiveBoundOptimizationforOnlineConvexOptimization.InProceedingsofthe23rdAnnualConferenceonLearningTheory-COLT’10,February2010.13[21]YuriiNesterov.IntroductoryLecturesonConvexProgrammingVolume:ABasiccourse,vol-umeI.KluwerAcademicPublishers,2004.[22]MarkSchmidt,NicolasLeRoux,andFrancisBach.Minimizingﬁnitesumswiththestochasticaveragegradient.arXivpreprintarXiv:1309.2388,pages1–45,2013.PreliminaryversionappearedinNIPS2012.[23]ShaiShalev-Shwartz.SDCAwithoutDuality.arXivpreprintarXiv:1502.06177,pages1–7,2015.[24]ShaiShalev-Shwartz,OhadShamir,andKarthikSridharan.Learningkernel-basedhalfspaceswiththe0-1loss.SIAMJournalonComputing,40(6):1623–1646,2011.[25]ShaiShalev-ShwartzandTongZhang.Stochasticdualcoordinateascentmethodsforregular-izedlossminimization.JournalofMachineLearningResearch,14:567–599,2013.[26]ShaiShalev-ShwartzandTongZhang.AcceleratedProximalStochasticDualCoordinateAs-centforRegularizedLossMinimization.InProceedingsofthe31stInternationalConferenceonMachineLearning,ICML2014,pages64–72,2014.[27]LinXiaoandTongZhang.AProximalStochasticGradientMethodwithProgressiveVarianceReduction.SIAMJournalonOptimization,24(4):2057—-2075,2014.[28]YuchenZhangandLinXiao.StochasticPrimal-DualCoordinateMethodforRegularizedEmpiricalRiskMinimization.InProceedingsofthe32ndInternationalConferenceonMachineLearning,ICML2015,2015.140.8410.8430.8450.8470.8490.85101020304050(a)adult,noﬂip0.8410.8430.8450.8470.8490.85101020304050(b)adult,ﬂip1/8fraction0.8410.8430.8450.8470.8490.85101020304050(c)adult,ﬂip1/4fraction0.9630.9680.9730.9780.9830.98801020304050(d)web,noﬂip0.9630.9680.9730.9780.9830.98801020304050(e)web,ﬂip1/8fraction0.9630.9680.9730.9780.9830.98801020304050(f)web,ﬂip1/4fraction0.940.9450.950.9550.9601020304050(g)rcv1,noﬂip0.940.9450.950.9550.9601020304050(h)rcv1,ﬂip1/8fraction0.940.9450.950.9550.9601020304050(i)rcv1,ﬂip1/4fraction0.9660.9710.9760.9810.9860.9910.99601020304050(j)mnist,noﬂip0.9660.9710.9760.9810.9860.9910.99601020304050(k)mnist,ﬂip1/8fraction0.9660.9710.9760.9810.9860.9910.99601020304050(l)mnist,ﬂip1/4fractionFigure4:TestingaccuracycomparisononSVRGfor‘2-regularizedERMwithdiﬀerentlossfunc-tions.Blacksolidcurvesrepresentsigmoidloss,bluedashcurvesrepresentsquareloss,greendash-dottedcurvesrepresentlogisticloss,andthethreereddottedcurvesrepresenthingelosswith3diﬀerentsmoothingparameters.Theyaxisrepresentsthetestingclassiﬁcationaccuracy,andthexaxisrepresentsthenumberofpassestothedataset.Appendix150.197430.197440.197450.197460.197470.197480.197490.1975020406080100(a)adult,λ=10−30.16860.168650.16870.168750.16880.168850.16890.168950.169020406080100(b)adult,λ=10−40.15580.15590.1560.15610.15620.15630.15640.1565020406080100(c)adult,λ=10−50.14990.15040.15090.15140.1519020406080100(d)adult,λ=10−60.120110.1201150.120120.1201250.120130.1201350.12014020406080100(e)web,λ=10−30.083930.083940.083950.083960.083970.083980.08399020406080100(f)web,λ=10−40.06680.066850.06690.066950.067020406080100(g)web,λ=10−50.05830.05850.05870.05890.05910.05930.0595020406080100(h)web,λ=10−60.0518890.0518910.0518930.0518950.0518970.051899020406080100(i)mnist,λ=10−30.023540.023550.023560.023570.023580.023590.0236020406080100(j)mnist,λ=10−40.012940.012990.013040.013090.013140.01319020406080100(k)mnist,λ=10−50.00830.00840.00850.00860.00870.0088020406080100(l)mnist,λ=10−60.22380.223850.22390.223950.224020406080100(m)rcv1,λ=10−30.093740.093840.093940.094040.094140.094240.094340.09444020406080100(n)rcv1,λ=10−40.0320.03220.03240.03260.03280.033020406080100(o)rcv1,λ=10−50.01040.01060.01080.0110.01120.01140.01160.01180.012020406080100(p)rcv1,λ=10−6Figure5:TrainingerrorcomparisonbetweenSGDandSVRGon‘2-regularizedERMwithsigmoidloss.Thebest-tunedSGDispresentedinsolidgreen,thebest-tunedSVRGwithconstantsteplengthispresentedindashedblue,andthebest-tunedSVRGispresentedindotedblack.Theyaxisrepresentsthetrainingobjectivevalue,andthexaxisrepresentsthenumberofpassestothedataset.ADetailedProofInthedetailedproof,weagainﬁrstconcentratingonanalyzingasingleepoch.Wechooseasbeforeη=1m0Lforsomeparameterm0thatdividesm.Thenaturalchoiceofm0shallbecomeclearattheendofthissection,andwouldbesettoaroundm2/3.Deﬁned=m/m0soanepochisdividedintodsub-epochs.Wedenotebyx−m0+1=···=x−1def=x0fornotationalconvenience,andsimilarlydeﬁne∇−m0+1=···=∇−1=e∇−m0+1=···=e∇−1=σ−m0+1=···=σ−1=0.Throughoutthis160.30.320.340.360.380.4050100150200SVRG-1SVRG-2SVRG-3SVRG-4SGDAdaGrad(a)cifar-10,λ=00.270.280.290.30.310.320.33050100150200SVRG-1SVRG-2SVRG-3SVRG-4SGDAdaGrad(b)cifar-10,λ=10−30.960.9610.9620.9630.9640.9650.9660.9670.9680.9690.97050100150200SVRG-1SVRG-2SVRG-3SVRG-4SGDAdaGrad(c)mnist,λ=00.960.9620.9640.9660.9680.970.9720.974050100150200SVRG-1SVRG-2SVRG-3SVRG-4SGDAdaGrad(d)mnist,λ=10−4Figure6:TestAccuracyComparisononneuralnets.Theyaxisrepresentsthetestingaccuracy,andthexaxisrepresentsthenumberofpassestothedataset.Algorithm2OurfullSVRGmethodinthenon-convexsettingInput:xφastartingvector,Snumberofepochs,mnumberofiterationsperepoch,m0subepochlength,ηsteplength.1:x10←xφ2:fors←1toSdo3:eµ←∇f(xs0)4:fork←0tom−1do5:Pickiuniformlyatrandomin{1,···,n}.6:e∇←∇fi(xsk)−∇fi(xs0)+eµ7:xsk+1=xsk−ηe∇8:endfor9:Deﬁneβ0,β1,...,βm0−1followingDeﬁnition4.110:Selectarandomms∈{m,m−1,...,m−m0+1}withprobabilityproportionaltonβm0−1,109βm0−1,109(βm0−1+βm0−2),...,109(βm0−1+···+β1)o.11:xs+10←xsms.12:endfor13:returnavectoruniformlyatrandomfromtheset{xst−1:s∈[S],t∈[ms]}section,wealsodroptheexpectationsignE[·]fornotationalsimplicity.Ourfullalgorithmforthenon-convexsettingisslightlydiﬀerentfromoursketchedproof,seeAlgorithm2.Mostimportantly,insteadofsettingtheﬁrstvectorofeachepochtobethelastiterateofthepreviousepoch,wesetittobeanon-uniformrandomiterateinthelastsubepochofthepreviousepoch.Thisstepiscrucialforouranalysistoholdwithoutthesimpliﬁcationassumptions.A.1UpperBoundingtheVarianceThefollowinglemmaisasimplecounterpartto(4.4)inoursketched-proofsection.Itupperboundstheaveragevarianceinsideanepochbytheaveragesquareddistancesbetweenvectorsthatarem0iterationsawayfromeachother.LemmaA.1.m−1Xt=0σ2t≤L2d2m−1Xt=0kxt+1−xt+1−m0k217Proof.Recallthatwehavethatforeveryt∈{0,1,...,m−1},wehaveσ2t≤L2kxt−x0k2≤L2d(kxt−xt−m0k2+kxt−m0−xt−2m0k2+···)Summingthisupoverallpossiblet’s,wehavem−1Xt=0σ2t≤L2d2m−1Xt=0kxt+1−xt+1−m0k2.Weemphasizethatthisanalysisreliesonourcarefulchoiceofx−m0+1=···=x−1def=x0whichsimpliﬁesournotations.(cid:3)WenextstateasimplevariantofLemma3.2thatallowsnegativeindices:LemmaA.2.Foreveryk∈{−m0+1,...,m−m0},everyt∈{0,...,m0−1},andeveryu∈Rd,wehavef(xk+t)−f(u)≤η2(cid:0)H2k+t+σ2k+t(cid:1)+(cid:0)12η+L2(cid:1)kxk+t−uk2−12ηkxk+t+1−uk2Wedeﬁnethesamesequenceofβ0,β1,...,βm0−1asbefore:DeﬁnitionA.3.Deﬁneβ0=1andβtdef=(1+ηL)−t=(1+1/m0)−tfort=1,...,m0−1.Wehave1≥βt≥1/e>1/3.BysummingupLemmaA.2withmultiplicativeratiosβtforeacht=0,1,...,m0−1,wearriveatthefollowinglemmawhichisacounterpartof(4.1)inthesketched-proofsection.LemmaA.4.Foreveryk∈{−m0+1,...,m−m0},andeveryu,m0−1Xt=0βt(cid:0)f(xk+t)−f(u)(cid:1)≤η2m0−1Xt=0βt(cid:0)H2k+t+σ2k+t(cid:1)+(cid:0)12η+L2(cid:1)kxk−uk2−βm0−12ηkxk+m0−uk2Inparticular,ifweselectu=xk,weobtainthatm0−1Xt=1βt(cid:0)f(xk+t)−f(xk)(cid:1)≤η2m0−1Xt=0βt(cid:0)H2k+t+σ2k+t(cid:1)−16ηkxk+m0−xkk2.(A.1)Thenextlemmasumsup(A.1)overallpossiblevaluesofk.Itcanbeviewedasaweighted,moresophisticatedversionof(4.6)inoursketched-proofsection.LemmaA.5.Aslongasm0≤16η2L2d2,wehaveβm0−1(cid:0)f(xm−1)−f(x0)−2ηH20:m−2(cid:1)+(βm0−1+βm0−2)(cid:0)f(xm−2)−f(x0)−2ηH20:m−3(cid:1)+···+(β1+···+βm0−1)(cid:0)f(xm−m0+1)−f(x0)−2ηH20:m−m0(cid:1)≤η2βm0−1H2m−1−112ηL2d2m−1Xt=0σ2t.18Proof.Bycarefullysummingup(A.1)fork∈{−m0+1,...,m−m0},wehavethatβm0−1f(xm−1)+(βm0−1+βm0−2)f(xm−2)+···+(β1+···+βm0−1)f(xm−m0+1)−(β1+···+βm0−1)f(x−m0+1)−(β2+···+βm0−1)f(x−m0+2)−···−βm0−1f(x−1)≤η2(cid:0)m0−1Xt=0βt(cid:1)(cid:0)m−1Xt=0σ2t(cid:1)+η2(cid:0)m0−1Xt=0βt(cid:1)(cid:0)m−m0Xt=0H2t(cid:1)+η2(cid:16)βm0−1H2m−1+(βm0−1+βm0−2)H2m−2+···+(β1+···+βm0−1)H2m−m0+1(cid:17)−16ηm−1Xt=0kxt+1−xt+1−m0k2.Usingthefactthatx−m0+1=···=x−1def=x0,wecanrewritethelefthandsideandgetβm0−1(cid:0)f(xm−1)−f(x0)(cid:1)+(βm0−1+βm0−2)(cid:0)f(xm−2)−f(x0)(cid:1)+···+(β1+···+βm0−1)(cid:0)f(xm−m0+1)−f(x0)(cid:1)≤η2(cid:0)m0−1Xt=0βt(cid:1)(cid:0)m−1Xt=0σ2t(cid:1)+η2(cid:0)m0−1Xt=0βt(cid:1)(cid:0)m−m0Xt=0H2t(cid:1)+η2(cid:16)βm0−1H2m−1+(βm0−1+βm0−2)H2m−2+···+(β1+···+βm0−1)H2m−m0+1|{z}x(cid:17)−16ηm−1Xt=0kxt+1−xt+1−m0k2.Usingthespeciﬁcvaluesofβt’s,wecanrelaxthetermsinxaboveandrewritetheaboveinequalityasβm0−1(cid:0)f(xm−1)−f(x0)−2ηH2m−2(cid:1)+(βm0−1+βm0−2)(cid:0)f(xm−2)−f(x0)−2ηH2m−3(cid:1)+···+(β1+···+βm0−1)(cid:0)f(xm−m0+1)−f(x0)−2H2m−m0(cid:1)≤η2(cid:0)m0−1Xt=0βt(cid:1)(cid:0)m−1Xt=0σ2t(cid:1)+η2(cid:0)m0−1Xt=0βt(cid:1)(cid:0)m−m0−1Xt=0H2t(cid:1)|{z}y+η2βm0−1H2m−1−16ηm−1Xt=0kxt+1−xt+1−m0k2.Nowwefurtherrelaxthetermsinyaboveandfurtherconcludethatβm0−1(cid:0)f(xm−1)−f(x0)−2ηH20:m−2(cid:1)+(βm0−1+βm0−2)(cid:0)f(xm−2)−f(x0)−2ηH20:m−3(cid:1)+···+(β1+···+βm0−1)(cid:0)f(xm−m0+1)−f(x0)−2ηH20:m−m0(cid:1)≤η2(cid:0)m0−1Xt=0βt(cid:1)(cid:0)m−1Xt=0σ2t(cid:1)+η2βm0−1H2m−1−16ηm−1Xt=0kxt+1−xt+1−m0k2|{z}z.19ApplyingthevarianceboundLemmaA.1onthesummationz,wehaveβm0−1(cid:0)f(xm−1)−f(x0)−2ηH20:m−2(cid:1)+(βm0−1+βm0−2)(cid:0)f(xm−2)−f(x0)−2ηH20:m−3(cid:1)+···+(β1+···+βm0−1)(cid:0)f(xm−m0+1)−f(x0)−2ηH20:m−m0(cid:1)≤η2(cid:0)m0−1Xt=0βt(cid:1)(cid:0)m−1Xt=0σ2t(cid:1)+η2βm0−1H2m−1−16ηL2d2m−1Xt=0σ2t.Finally,aslongasPm0−1t=0βt≤16η2L2d2(whichcanbesatisﬁedbecausem0≤16η2L2d2),wehaveβm0−1(cid:0)f(xm−1)−f(x0)−2ηH20:m−2(cid:1)+(βm0−1+βm0−2)(cid:0)f(xm−2)−f(x0)−2ηH20:m−3(cid:1)+···+(β1+···+βm0−1)(cid:0)f(xm−m0+1)−f(x0)−2ηH20:m−m0(cid:1)≤η2βm0−1H2m−1−112ηL2d2m−1Xt=0σ2t.ThisﬁnishestheproofofLemmaA.5.(cid:3)A.2ObjectiveDecreaseusingGradientDescentThefollowinglemmaisavariantof(5.1).However,insteadoflowerboundingtheobjectivedecreasef(x0)−f(xm)fortheentireepochasinthesketched-proofsection,wehavetocarefullylowerboundaweightedsumoff(x0)−f(xt)fort∈{m,m−1,...,m−m0+1},inordertomakeitconsistentwiththelefthandsideofLemmaA.5.LemmaA.6.βm0−1(cid:0)f(x0)−f(xm)−η2H20:m−1(cid:1)+βm0−1(cid:0)f(x0)−f(xm−1)−η2H20:m−2(cid:1)+(βm0−1+βm0−2)(cid:0)f(x0)−f(xm−2)−η2H20:m−3(cid:1)+···+(β1+···+βm0−1)(cid:0)f(x0)−f(xm−m0+1)−η2H20:m−m0(cid:1)≥−η2Lm02m−1Xt=0σ2tProof.Foreachj=1,2,...,m0,bytelescopingLemma3.1acrossiterationsk=0,1,...,m−j,wearriveatinequalityf(x0)−f(xm−j+1)≥η2H20:m−j−η2L2m−1Xt=0σ2t.20Nowwewritedownthesem0inequalitiesseparately,andmultiplyeachofthembyapositiveweight:βm0−1×nf(x0)−f(xm)≥η2H20:m−1−η2L2m−1Xt=0σ2toβm0−1×nf(x0)−f(xm−1)≥η2H20:m−2−η2L2m−1Xt=0σ2to(βm0−2+βm0−1)×nf(x0)−f(xm−2)≥η2H20:m−3−η2L2m−1Xt=0σ2to···(β1+···+βm0−1)×nf(x0)−f(xm−m0+1)≥η2H20:m−m0−η2L2m−1Xt=0σ2toSummingtheseinequalitiesup,weobtainourdesiredinequalityβm0−1(cid:0)f(x0)−f(xm)−η2H20:m−1(cid:1)+βm0−1(cid:0)f(x0)−f(xm−1)−η2H20:m−2(cid:1)+(βm0−1+βm0−2)(cid:0)f(x0)−f(xm−2)−η2H20:m−3(cid:1)+···+(β1+···+βm0−1)(cid:0)f(x0)−f(xm−m0+1)−η2H20:m−m0(cid:1)≥−η2Lm202m−1Xt=0σ2t.(cid:3)A.3FinalTheoremLetusnowputtogetherLemmaA.5andLemmaA.6,andderivethefollowinglemma:LemmaA.7.Aslongas6η3L3m20d2=1/9,wehaveβm0−1(cid:0)f(x0)−f(xm)−η4H20:m−1(cid:1)+10βm0−19(cid:0)f(x0)−f(xm−1)−η4H20:m−2(cid:1)+109(βm0−1+βm0−2)(cid:0)f(x0)−f(xm−2)−η4H20:m−3(cid:1)+···+109(β1+···+βm0−1)(cid:0)f(x0)−f(xm−m0+1)−η4H20:m−m0(cid:1)≥0.Proof.BydirectlycombiningLemmaA.5andLemmaA.6,wehaveβm0−1(cid:0)f(x0)−f(xm)−η2H20:m−1(cid:1)+βm0−1(cid:0)f(x0)−f(xm−1)−η2H20:m−2(cid:1)+(βm0−1+βm0−2)(cid:0)f(x0)−f(xm−2)−η2H20:m−3(cid:1)+···+(β1+···+βm0−1)(cid:0)f(x0)−f(xm−m0+1)−η2H20:m−m0(cid:1)≥12ηL2d2·η2Lm202·(cid:16)βm0−1(cid:0)f(xm−1)−f(x0)−2ηH20:m−2(cid:1)+(βm0−1+βm0−2)(cid:0)f(xm−2)−f(x0)−2ηH20:m−3(cid:1)+···+(β1+···+βm0−1)(cid:0)f(xm−m0+1)−f(x0)−2ηH20:m−m0(cid:1)−η2βm0−1H2m−1(cid:17)21Supposewehave12ηL2d2·η2Lm202=6η3L3m20d2=1/9,thenitsatisﬁesthatβm0−1(cid:0)f(x0)−f(xm)−η2H20:m−1(cid:1)+βm0−1(cid:0)f(x0)−f(xm−1)−η2H20:m−2(cid:1)+(βm0−1+βm0−2)(cid:0)f(x0)−f(xm−2)−η2H20:m−3(cid:1)+···+(β1+···+βm0−1)(cid:0)f(x0)−f(xm−m0+1)−η2H20:m−m0(cid:1)≥19(cid:16)βm0−1(cid:0)f(xm−1)−f(x0)−2ηH20:m−2(cid:1)+(βm0−1+βm0−2)(cid:0)f(xm−2)−f(x0)−2ηH20:m−3(cid:1)+···+(β1+···+βm0−1)(cid:0)f(xm−m0+1)−f(x0)−2ηH20:m−m0(cid:1)−η2βm0−1H2m−1(cid:17)Afterrearranging,wehaveβm0−1(cid:0)f(x0)−f(xm)−η2H20:m−1(cid:1)+10βm0−19(cid:0)f(x0)−f(xm−1)−η4H20:m−2(cid:1)+109(βm0−1+βm0−2)(cid:0)f(x0)−f(xm−2)−η4H20:m−3(cid:1)+···+109(β1+···+βm0−1)(cid:0)f(x0)−f(xm−m0+1)−η4H20:m−m0(cid:1)≥−η18βm0−1H2m−1.Afterrelaxingtherighthandside,weconcludethatβm0−1(cid:0)f(x0)−f(xm)−η4H20:m−1(cid:1)+10βm0−19(cid:0)f(x0)−f(xm−1)−η4H20:m−2(cid:1)+109(βm0−1+βm0−2)(cid:0)f(x0)−f(xm−2)−η4H20:m−3(cid:1)+···+109(β1+···+βm0−1)(cid:0)f(x0)−f(xm−m0+1)−η4H20:m−m0(cid:1)≥0.ThisﬁnishestheproofofLemmaA.7.(cid:3)LemmaA.8naturallyimpliesthatifweselectarandomstoppingvectorforthisepoch,wehavethefollowingcorollarywhichisacounterpartof(5.2)inoursketch-proofsection:CorollaryA.8.Ifwesetmstobearandomvariablein{m,m−1,...,m−m0+1},withprobabilitiesproportionalto(cid:8)βm0−1,109βm0−1,109(βm0−1+βm0−2),...,109(βm0−1+···+β1)(cid:9),thenLemmaA.7impliesthatwehaveE(cid:2)f(x0)−f(xms)−η4H20:ms−1(cid:3)≥0.NotethatCorollaryA.8isonlyforasingleepochandcanbewrittenasE[f(xs0)−f(xsms)]≥η4Ehms−1Xt=0k∇f(xst)k2iinthegeneralnotation.Therefore,wearenowreadytotelescopeitoveralltheepochss=1,2,...,S.Recallthatwehavechosenxs0,theinitialvectorinepochs,tobexs−1ms−1,therandomstoppingvectorfromthepreviousepoch.Therefore,weobtainthat1m1+···+mSSXs=1ms−1Xt=0E(cid:2)k∇f(xst)k2(cid:3)≤4ηS(m1+···+mS)(cid:0)f(x10)−E[f(xSmS)](cid:1)≤O(cid:16)f(xφ)−minxf(x)ηSm(cid:17).22Atthispoint,ifweselectuniformlyatrandomanoutputvectorxfromtheset{xst−1:s∈[S],t∈[ms]},weconcludethatE[k∇f(xst)k2]≤O(cid:16)f(xφ)−minxf(x)ηSm(cid:17).Finally,letusechoosetheparametersproperly.Wesimplyletm=nbetheepochlength.Sincewehaverequiredm0≤16η2L2d2and6η3L3m20d2=1/9inLemmaA.5andLemmaA.7respectively,andboththeserequirementscanbesatisﬁedwhenm30≥54m2,wesetm0=Θ(m2/3)=Θ(n2/3).Accordinglyη=1m0L=O(cid:0)1n2/3L(cid:1).Insum,TheoremA.9(FormalstatementofTheorem5.1).Bychoosingm=nandθ=Θ(cid:0)1n2/3L(cid:1),theproducedoutputxofAlgorithm2satisﬁesthat10E[k∇f(x)k2]≤O(cid:16)L(f(xφ)−minxf(x))Sn1/3(cid:17).Inotherwords,toobtainapointxsatisfyingk∇f(x)k2≤ε,thetotalnumberofiterationsneededforAlgorithm1isSn=O(cid:16)n2/3L(f(xφ)−minxf(x))ε(cid:17).10LikeinSGD,onecaneasilyapplyaMarkovinequalitytoconcludethatwithprobabilityatleast2/3wehavethesameasymptoticupperboundonthedeterministicquantityk∇f(x)k2.23