6
1
0
2

 
r
a

 

M
4
1

 
 
]

O
C

.
t
a
t
s
[
 
 

1
v
0
6
3
4
0

.

3
0
6
1
:
v
i
X
r
a

An Ensemble EM Algorithm for Bayesian Variable Selection

Jin Wang ∗1, Feng Liang †1, and Yuan Ji ‡2

1 Department of Statistics, University of Illinois at Urbana-Champaign

2Department of Biostatistics, University of Chicago

March 15, 2016

Abstract

We study the Bayesian approach to variable selection in the context of linear

regression. Motivated by a recent work by Roˇckov´a and George (2014), we propose an

EM algorithm that returns the MAP estimate of the set of relevant variables. Due to

its particular updating scheme, our algorithm can be implemented eﬃciently without

inverting a large matrix in each iteration and therefore can scale up with big data.

We also show that the MAP estimate returned by our EM algorithm achieves variable

selection consistency even when p diverges with n. In practice, our algorithm could

get stuck with local modes, a common problem with EM algorithms. To address

this issue, we propose an ensemble EM algorithm, in which we repeatedly apply the

EM algorithm on a subset of the samples with a subset of the covariates, and then

aggregate the variable selection results across those bootstrap replicates. Empirical

studies have demonstrated the superior performance of the ensemble EM algorithm.

1

Introduction

Consider a simple linear regression model with Gaussian noise:

(1)
where y = (y1, . . . , yn)T is the n× 1 response, e = (e1, . . . , en)T is a vector of iid Gaussian
random variables with mean 0 and variance σ2, and X is the n × p design matrix. The

y = Xβ + e

∗jinwang8@illinois.edu
†liangf@illinois.edu
‡jiyuan@uchicago.edu

1

unknown parameters are the regression parameter β = (β1, . . . , βp)T and the error variance
σ2.
In many real applications such as bioinformatics and image analysis, where linear

regression models have been routinely used, the number of potential predictors (i.e., p)

is large but only a small fraction of them is believed to be relevant. Therefore the linear

model (1) is often assumed to be “sparse” in the sense that most of the coeﬃcients βj’s are
zero. Estimating the set of relevant variables, S = {j : βj (cid:54)= 0}, is an important problem
in modern statistical analysis.

The Bayesian approach to variable selection is conceptually simple and straightforward.
First introduce a p-dimensional binary vector γ = (γ1, . . . , γp)T to index all the 2p sub-
models, where γj = 1 if the jth variable is included in this model and 0 if excluded. Usually
γj’s are modeled by independent Bernoulli distributions. Given γ, a popular prior choice
for β is the “spike and slab” prior (Mitchell and Beauchamp, 1988):

δ0(βj),

g(βj),

π(βj | γj) =

if γj = 0;

if γj = 1,

(2)

where δ0(·) is the Kronecker delta function corresponding to the density function of a
point mass at 0 and g is a continuous density function. After specifying priors on all

the unknowns, one needs to calculate the posterior distribution. Most algorithms for

Bayesian variable selection rely on MCMC such as Gibbs or Metropolis Hasting to obtain

the posterior distribution; for a review on recent developments in this area, see O’Hara

and Sillanp¨a¨a (2009). MCMC algorithms, however, are insuﬃcient to meet the growing

demand on scalability from real applications. Since the primary goal is variable selection,

we focus on eﬃcient algorithms that return the MAP estimate of γ, as an alternative to

these MCMC-based sampling methods that return the whole posterior distribution on all

the unknown parameters.

Recently, Roˇckov´a and George (2014) proposed a simple, elegant EM algorithm for

Bayesian variable selection. They adopted a continuous version of the “spike and slab”

prior—the spike component in (2) is replaced by a normal distribution with a small vari-

ance (George and McCulloch, 1993), and proposed an EM algorithm to obtain the MAP
estimate of the regression coeﬃcient β. The MAP estimate ˆβMAP, however, is not sparse,
and an additional thresholding step is needed to estimate γ.

In this paper, we develop an EM algorithm that directly returns the MAP estimate

of γ, so no further thresholding is needed. We adopt the same continuous “spike and

slab” prior. Diﬀerent from the algorithm by Roˇckov´a and George (2014) that returns

2

ˆβMAP by treating γ as latent, our algorithm returns the MAP estimate of the model
index, ˆγMAP, by treating β as latent. The special structure of our EM algorithm allows
us to use a computational trick to avoid inverting a big matrix at each iteration, which

seems unavoidable in the algorithm by Roˇckov´a and George (2014). Further we can show
that the ˆγMAP achieves asymptotic consistency even when p diverges to inﬁnity with the
sample size n.

Although shown to achieve selection consistency, in practice, our EM algorithm could

get stuck at a local mode due to the large discrete space in which γ lies. Borrowing

the idea of bagging, we propose an ensemble version of our EM algorithm (which we call

BBEM): apply the algorithm on multiple Bayesian bootstrap (BB) copies of the data, and

then aggregate the variable selection results. Bayesian bootstrap for variable selection

was explored before by Clyde and Lee (2001) for the purpose of prediction, where models

built on diﬀerent bootstrap copies are combined to predict the response. But the focus of

our approach is to summarize the evidence for variable relevance from multiple BB copies,

which is similar in nature to several frequentist ensemble methods for variable selection,

such as the AIC ensemble (Zhu and Chipman, 2006), stability selection (Meinshausen and

B¨uhlmann, 2010), and random Lasso (Wang et al., 2011).

The remaining of the paper is organized as follows. Section 2 describes the EM al-

gorithm in detail, Section 3 presents the asymptotic results, and Section 4 describes the

BBEM algorithm. Empirical studies are presented in Section 5 and conclusions and re-

marks in Section 6.

2 The EM Algorithm

2.1 Prior Speciﬁcation

We adopt the continuous version of “spike and slab” prior for β, i.e. a mixture of two

normal components with mean zero and diﬀerent variances:

N(0, σ2v0),

N(0, σ2v1),

π(βj | σ, γj) =

if γj = 0;

if γj = 1,

(3)

where v1 > v0 > 0. Alternatively, we can write the prior on β as

where

π(βj | σ2, γj) = N(0, σ2dγj ),

dγj = γjv1 + (1 − γj)v0.

3

For the remaining parameters, we specify independent Bernoulli priors on elements of

γ, and conjugate priors like Beta and Inverse Gamma on θ and σ2, respectively:

π(γ | θ) = Bern(θ),

π(θ) = Beta(a0, b0),
π(σ2) = IG(ν/2, νλ/2).

For hyper-parameters (a0, b0, ν, λ), we suggest the following non-informative choices unless
prior knowledge is available:

a0 = b0 = 1.1,

ν = λ = 1.

(4)

The choice for v0 and v1 will be discussed later.

2.2 The Algorithm

With the Gaussian model and prior distributions speciﬁed above, we can write down the

full posterior distribution:

π(γ, β, θ, σ2 | y) ∝ p(y | β, σ2) × π(β | σ, γ) × π(γ | θ) × π(θ) × π(σ2).

Treating β as the latent variable, we derive an EM algorithm that returns the MAP
estimation of parameters Θ = (γ, σ2, θ), whereas the roles of β and γ are switched in

Roˇckov´a and George (2014).

E Step

The objective function Q at the (t + 1)-th iteration in an EM algorithm is deﬁned as the

integrated logarithm of the full posterior with respect to β given y and the parameter
values from the previous iteration Θ(t) = (γ(t), σ2

(t), θ(t)), i.e.,

Q(Θ | Θ(t)) = E

β|Θ(t),y log π(Θ, β | y)

= − 1
2σ2

E
β|Θ(t),y

(cid:105)

p(cid:88)

j=1

β2
j
dγj

+ F (Θ),

(5)

(cid:104)(cid:107)y − Xβ(cid:107)2 +
p(cid:88)

where

F (Θ) = − n + p

2

log σ2 − 1
2

j=1

log dγj + π(γ | θ)

+ log π(θ) + log π(σ2) + Constant

4

is a function of Θ not depending on β.

It is easy to show that β follows a Normal distribution with mean m and covariance

matrix σ2

(t)V, given Θ(t) and y, where

m = V−1XT y, V =(cid:0)XT X + D−1

(cid:1)−1,

Dγ(t) = diag

d

j=1

γ(t)

γ(t)
j

(cid:16)

= diag

(cid:17)p

j v1 + (1 − γ(t)
γ(t)

(cid:16)
(cid:17)p
(cid:13)(cid:13)y − Xβ(cid:13)(cid:13)2 = σ2
(t)tr(XVXT ) +(cid:13)(cid:13)y − Xm(cid:13)(cid:13)2,
p(cid:88)
p(cid:88)

j )v0

σ2
(t)Vjj + m2
j
j )v0 + γ(t)

(1 − γ(t)

j v1

.

j=1

.

j=1

E
β|Θ(t),y

β2
j
dγj

=

E
β|Θ(t),y

Then the two expectation terms in (5) can be expressed as:

j=1

M Step

We sequentially update parameters (γ, θ, σ) to maximize the objective function Q.

1. Update γj’s. The terms involving γj in (5) are

(cid:35)

(cid:34) β2

j
dγj

− 1
2σ2
(t)

E
β|Θ(t),y

− 1
2

log dγj + log π(γj | θ(t)).

(6)

(7)

(8)

(9)

(10)

Plug in γj = 0 and γj = 1 to (9) respectively, then we have

γ(t+1)
j

= 1,

if E

β|Θ(t),y

(cid:2)β2

j

(cid:3) > r(t),

where

r(t) =

σ2
(t)

1/v0 − 1/v1

log

v1
v0

− 2 log

θ(t)
1 − θ(t)

2. Update (σ2, θ). Given γ(t+1), the updating equations for the other two parameters

are given by

E
β|Θ(t),y

σ2
(t+1) =

(cid:104)(cid:107)y − Xβ(cid:107)2 +(cid:80)p
(cid:80)p

n + p + ν

θ(t+1) =

+ a0 − 1

j=1 γ(t+1)
p + a0 + b0 − 2

j

.

j=1 β2

j /d

γ(t+1)
j

+ νλ

,

(11)

(12)

(cid:17)

.

(cid:105)

(cid:16)

5

Stopping Rule

The EM algorithm alternates between the E-step and M-step until convergence. A natural

stopping criterion is to check whether the change of the objective function Q is small. To

reduce the computation cost for evaluating the Q function, we adopt a diﬀerent stopping
rule as our main focus is γ: we stop our algorithm when the estimate γ(t) stays the same

for k0 iterations.
algorithm is summarized in Algorithm 1.

In practice, we suggest to set k0 = 3. The pseudo code of this EM

Algorithm 1: EM Algorithm
Input: X, y, v0, v1, a0, b0, ν, λ
Initialize Θ(0);
E-step: Calculate the two expectations in (7) and (8), denoted as EE(0);

for t = 1 : maxIter do

M-step: Update Θ(t) from Eq (10, 11, 12);
E-step: Update EE(t) from Eq (7, 8);
if γ(t) stays the same for k0 = 3 iterations then

break;

end

end

Return γ, m;

2.3 Computation Cost

At each E-step, updating the posterior of β given other parameters in (6) requires inverting
a p × p matrix

V(t) = (XT X + D−1

γ(t))−1,

(13)

which is the major computational burden of this algorithm. When p > n, we can use
the Sherman-Morrison-Woodbury formula to compute the inverse of an n × n matrix.
So the computation cost at each iteration is of order O(min(n, p)3). It is, however, still

time-consuming when both n and p are large.

Note that the only thing that changes in (13) from iteration to iteration is Dγ(t), a
diagonal matrix depending on the binary vector γ(t). From our experience, only a small
fraction of γ(t)
j

’s are changed at each iteration after the ﬁrst a couple of iterations. So the

6

idea is to use the following recursive formula to compute V(t):
γ(t) − D−1
γ(t−1))−1

V(t) = (XT X + D−1
(t−1) + D−1

γ(t−1) + D−1
γ(t) − D−1

= (V−1

γ(t−1))−1

(14)

γ(t)−D−1

where D−1
γ(t−1) is a diagonal matrix with the j-th diagonal entry being non-zero only
if the inclusion/exclusion status, i.e., the value of γj, is changed from the last iteration.
Let l denote the number of variables whose γj values are changed from iteration (t− 1) to
t. Then D−1
γ(t−1) is a rank l matrix. We can apply the Woodbury formula on (14)
to reduce the computation complexity from O(min(n, p)3) to O(l3).

γ(t) − D−1

For example, without loss of generality, suppose only the ﬁrst l covariates have their

γj values changed. Then, we can write

where A = (cid:0) 1

v0

− 1

v1

(cid:1)diag(2γ(t)

D−1
γ(t) − D−1
j − 1)l

γ(t−1) = Up×lAl×lU T ,

j=1 and U consists of the ﬁrst l columns from Ip.

Applying the Woodbury formula, we have

V(t) = V(t−1) − V(t−1)U (A−1 + U T V(t−1)U )−1U T V(t−1).

3 Asympototic Consistency

In this section, we study the asymptotic property of ˆγn, the MAP estimate of model
index returned by our EM algorithm. Assume the data yn are generated from a Gaussian
regression model:

(cid:0)Xnβ∗

yn ∼ Nn

n, σ2In

(cid:1).

n also vary with n. Suppose the true model is indexed by γ∗

Here we consider a triangular array set up: the dimension p = pn diverges with n and the
true coeﬃcients β∗
n, where
nj = 0 if β∗
γ∗
nj = 1 if β∗
nj = 0. We show that our EM algorithm has the
following selection consistency property:
P(ˆγn = γ∗

nj (cid:54)= 0 and γ∗

as n → ∞.

n) → 1,

First we list some regularity conditions needed in our proof. Let λmin(A) denote the

7

smallest eigenvalue of matrix A. We assume

(A1) λmin(XT

n Xn)−1 = O(n−η1), 0 < η1 ≤ 1;

(A2)

(cid:107)β∗

n(cid:107)2 = O(nη2), 0 < η2 < η1;

min(cid:8)|β∗

nj|, γ∗
n(η3−1)/2

nj = 1(cid:9)

lim inf

(A3)
(A4) a0 ∼ pn, b0 ∼ pn, ν = ∞, λ = 1,

n

≥ M, 0 ≤ η3 < 1;

where M is a positive constant, and (a0, b0, ν, λ) are the hyper-parameters from the Beta
and InvGamma priors.

Assumption (A1) controls the collinearity among covariates; in the traditional asymp-

totic setting where p is ﬁxed, we have η1 = 1. Assumption (A2) controls the sparsity (in
terms of L2 norm) of the true regression coeﬃcient vector. Assumption (A3) requires
√
that the minimal non-zero coeﬃcient cannot go to zero at a rate faster than 1/
n; in
the traditional asymptotic setting where β∗ is ﬁxed, we have η3 = 0. Assumption (A4) is
purely technical, which ensures that ˆθn and ˆσ2
n are bounded. In fact we could ﬁx ˆθn and
ˆσ2
n to be any constant, which does not aﬀect the proof. In our simulation studies, we still
recommend (4) as the choice for hyper-parameters unless p is large.
Theorem 3.1. Assume (A1-A4) and p = O(nα) where 0 ≤ α < 1. With v1 ﬁxed and v0
satisfying

0 < v0 = O(n−r0),

1 − η3 < r0 < min

η1 − α,

(cid:110)

(cid:111)

,

(η1 − η2)

2
3

the model returned by our EM algorithm, ˆγn, achieves the following selection consistency,

P(ˆγn = γ∗

n) → 1,

as n → ∞.

(15)

Proof. See Appendix.

4 The BBEM Algorithm

A common issue with EM algorithms is that they could be trapped at a local maximum.

There are some standard remedies available for dealing with this issue, for instance, trying

a set of diﬀerent initial values or utilizing some more advanced optimization procedures

at the M-step. Since our EM algorithm is searching for the optimal γ over a big discrete

space, all p-dimensional binary vectors, these remedies are less useful when p is large.

When doing optimization with γ, a discrete vector, the resulting solution is often not

stable, i.e., has a large variance. Bagging is an easy but powerful method (Breiman, 1996)

8

for variance reduction, which applies the same algorithm on multiple bootstrap copies

of the data, and then aggregates the results. We proposed the following ensemble EM

algorithm, in which we repeatedly run the EM variable selection algorithm, Algorithm 1

from Section 2.2, on Bayesian bootstrap replicates.

The original bootstrap repeatedly draws samples from the original data set {(xi, yi)}n

i=1
with replacement, i.e., each observation (xi, yi) is sampled with probability 1/n. In Bayesian
bootstrap (Rubin, 1981), instead of sampling a subset of the data, we assign a random

weight wi to the i-th observation and then ﬁt a weighted least squares regression model
In particular, following Rubin (1981), we generate the weights
on the whole data set.

w = (w1, . . . , wn) from a n-category Dirichlet distribution:

wn×1 ∼ Dir(1,··· , 1).

(16)

When applying Algorithm 1 on a weighted linear regression model, all the updating equa-

tions stay the same, except equation (6) for the posterior of β, which should be changed

to:

m = VXT diag(w)y, V = (XT diag(w)X + D−1

γ(t))−1.

(17)

Eq (7), the expectation of the weighted residual sum of squares, should also be changed

accordingly:

E
β|Θ(t),y

(cid:13)(cid:13)y − Xβ(cid:13)(cid:13)2

w = σ2

(t)tr(diag(w)XVXT ) + (y − Xm)T diag(w)(y − Xm).

(18)

It is well-known that in order to make the aggregation work, we should control the

correlation among estimates from bootstrap replicates. For example, in random forest

(Breiman, 2001), the number of variables used for choosing the optimal split of a tree is

restricted to a subset of the variables, instead of using all p variables. A similar idea was

implemented in Random Lasso (Wang et al., 2011), an ensemble algorithm for variable

selection. In the same spirit, we apply the EM algorithm only on a subset of the variables

at each Bayesian bootstrap iteration. A naive way is to randomly pick a subset from

the p variables. This, however, will be ineﬃcient when p is large and the true model is

sparse, since it is likely most random subsets will not contain any relevant variables. So

we employ a biased sampling procedure: sample the p variables based on a weight vector

˜π that is deﬁned as

˜πp×1 ∝ |XT y|/diag(XT X),

(19)

that is, variables are sampled based on their marginal eﬀect in a simple linear regression.

9

The ensemble EM algorithm operates as follows. First we sample a random set of L
variables according to the probability vector ˜π, and draw a n × 1 bootstrap weight vector
w from (16). Let ˜X be the new data matrix with the L columns. Then apply the EM
algorithm on ˜X with weight w. Let γk denote the model returned by the k-th Bayesian
bootstrap iteration, where the j-th element of γk is 1 if the j-th variable is selected and
zero otherwise; of course, the j-th element is zero if the j-th variable is not included in

the initial L variables. Deﬁne the ﬁnal variable selection frequency for the p variables as

K(cid:88)

k=1

φp×1 =

1
K

γk.

(20)

We can report the ﬁnal variable selection result by thresholding φj’s at some ﬁxed number,
for example, a half. Or we can produce a path-plot of φ as v0 varies, which could be a useful
tool to investigate the importance of each variable. We illustrate this in our simulation

study in Section 5.

As for the computational cost, the inversion of the L × L matrix in (17) is a big
improvement compared with that of a p × p matrix, while it can be further simpliﬁed
through the fast computing trick in Section 2.3. We call this algorithm, BBEM, which is

summarized in Algorithm 2.

5 Empirical Study

In this section, we ﬁrst compare the proposed EM algorithm (Algorithm 1) with other

popular methods on a widely used benchmark data set. Then we compare BBEM (Al-

gorithm 2) with other methods on two more challenging data sets of larger dimensions.

Finally, we applied BBEM on a restaurant revenue data from a Kaggle competition, and

showed that our algorithm outperforms the benchmark from random forest.

For the hyper-parameters v0 and v1, we set v1 = 100 as ﬁxed and tune an appropriate
value for v0 either based on 5-fold cross-validation or BIC. For the initial value for θ, we
suggest to use 1/2 for ordinary problems, but
n/p for large-p problems. The initial value
of σ2 is set as 1. In addition, there are two bootstrap parameters: the total number of

√

replicates K and the number of variables used in each bootstrap L. For eﬃciency, the

number of variables in each bootstrap replicate should not exceed the sample size n. We

use K = 100, and L = n/2 = 50 if p is large and L = p is p is small.

10

Algorithm 2: BBEM Algorithm

Input: X, y, v0, v1, a0, b0, ν, λ, K, L
Compute the variable weight ˜π from (19);

for k = 1 : K do

Generate a subset of L variables according to ˜π;
Make the replicate ˜Xk with the L variables;
Initialize Θ(0)
k ;
Generate bootstrap weight w from (16);
E-step: Calculate the two expectations in (8), denoted as EE(0)
k ;
for t = 1 : maxIter do
M-step: Update Θ(t)
k
E-step: Update EE(t)
k
if γ(t)
k
break;

stays the same for k0 = 3 iterations then

from Eq (10, 11, 12);

from Eq (18, 8);

end

end
Record γ(t)

k , m(t)
k ;

end

Return φ from Eq (20);

11

5.1 A widely used benchmark

First we apply our EM algorithm on a widely used benchmark data set (Tibshirani, 1996),

which has p = 8 variables, each from a standard normal distribution with pairwise corre-
lation ρ(xi, xj) = 0.5|i−j|. The response variable is generated from

where  ∼ N(0, σ2).

y = 3x1 + 1.5x2 + 2x5 + 

Following Fan and Li (2001), we repeat the experiment 100 times under two scenarios:

(1) n = 40, σ = 3 and (2) n = 60, σ = 1. The result is shown in Table 1, which reports the

average number of zero-coeﬃcients (i.e., no selection) among signal variables (x1, x2, x5)
and among noise variables, respectively. The results for SCAD1 (tuning parameter selected

by cross-validation), SCAD2 (tuning parameter ﬁxed) and LASSO are taken from Fan and

Li (2001). In the ﬁrst “small sample-size high noise” scenario, our EM algorithm has the

highest number of zero-coeﬃcients among noise variables, i.e., the lowest type I error.

The average number of signal variables missed by EM is slightly higher than SCAD1

(where the tuning parameter is chosen by cross-validation) but less than SCAD2 (where

the tuning parameter is pre-ﬁxed). But overall, our EM algorithm and the two SCAD

methods perform the best. In the second “large sample-size low noise” scenario, no signal

variables are missed by any method, but EM has the lowest type I error.

Following Wang et al. (2011) and Xin and Zhu (2012), we repeat the experiment

100 times with the same sample size n = 50 but two diﬀerent noise levels:

low noise

level (σ = 3) and high noise level (σ = 6). Table 2 reports the minimum, median,

maximum of being selected out of 100 simulations for the signal and the noise variables,

respectively. Both Lasso and random Lasso have a higher chance of selecting the signal

variables, but at the price of mistakenly including many noise variables. Overall, our

EM algorithm performs the best, along with PGA and stability selection, two frequentist

ensemble methods for variable selection.

5.2 A highly-correlated data

Next we demonstrate our two algorithms on a highly-correlated example from Wang et al.

(2011). The data has p = 40 variables and the response y is generated from

y = 3x1 + 3x2 − 2x3 + 3x4 + 3x5 − 2x6 + ,

12

Method

n = 40, σ = 3

EM

SCAD1

SCAD2

LASSO

Oracle

n = 60, σ = 1

EM

SCAD1

SCAD2

LASSO

Oracle

xj ∈ Noise
(j=3,4,6,7,8)

xj ∈ Signal
(j=1,2,5)

4.55

4.20

4.31

3.53

5.00

4.72

4.37

4.42

3.56

5.00

0.24

0.21

0.27

0.07

0.00

0.00

0.00

0.00

0.00

0.00

Table 1: A widely used benchmark. The average number of zero-coeﬃcients (i.e., no

selection) out of 100 simulations for each types of variable (Signal or Noise) are shown.

The results other than EM (Alg 1) are from Fan and Li (2001).

where  ∼ N(0, σ2) and σ = 6. Each xi is generated from a standard normal with the
following correlation structure among the ﬁrst six signal variables: the signal variables
are divided into two groups, V1 = {x1, x2, x3} and V2 = {x4, x5, x6}; the within group
correlation is 0.9 and the between-group correlation is 0.

We repeat the simulation 100 times with n = 50 and n = 100, and the results are

summarized in Table 3. For this example, due to the high correlation among features we

expect ensemble methods to perform better. Indeed, BBEM has the best performance

in terms of selecting true signal variables while controlling the error of including noise

variables. The performance of the EM algorithm, although not the best, is also comparable

with other top ensemble methods like random Lasso from Wang et al. (2011), and T2E

and PGA from Xin and Zhu (2012).

For illustration purpose, we apply BBEM on a data set with n = 50 and v0 varying
from 10−4 to 1. Figure 1 shows the path-plot of the selection frequency from BBEM.
There is clearly a gap between the signal variables and the noise ones. For a range of v0,
from 0.001 to 0.02, BBEM can successfully select the six true variables {x1, x2, . . . , x6} if
we threshold the selection frequency φj at 0.5.

13

Method

n = 50, σ = 3

EM

Lasso

Random Lasso

ST2E

PGA

Stability selection

λmin = 1
λmin = 0.5

n = 50, σ = 6

EM

Lasso

Random Lasso

ST2E

PGA

Stability selection

λmin = 1
λmin = 0.5

xj ∈ Signal (j=1,2,5) xj ∈ Noise (j=3,4,6,7,8)
Max
Min Median Max Min Median

91

99

95

89

82

81

90

53

76

92

68

54

59

76

97

100

99

96

98

83

98

67

85

94

69

76

61

84

100

100

100

100

100

100

100

91

99

100

96

94

92

100

3

48

33

4

4

0

4

6

47

40

9

9

4

30

6

55

40

12

7

2

8

10

49

48

13

14

8

42

12

61

48

20

11

9

22

14

53

58

21

16

18

50

Table 2: A widely used benchmark. The min, median, max number of being selected out

of 100 simulations for each types of variable (Signal or Noise) are shown. The results other

than EM (Alg 1) are from Xin and Zhu (2012).

5.3 A Large-p small-n example

Finally we apply BBEM on a large-p small-n example from Roˇckov´a and George (2014),

where p = 1000 and n = 100. Each of the p features is generated from a standard normal
with pairwise correlation to be 0.6|i−j| and the response y is generated from the following
linear model:

where  ∼ N(0, 3).

y = x1 + 2x2 + 3x3 + ,

For this large p example, we set the parameters in the BBEM algorithm as follows:

the initial value of θ is

√

n/p, the number of variables used in each bootstrap iteration

14

Method

n = 50, σ = 6

Lasso

Random Lasso

ST2E

PGA

EM

BBEM

n = 100, σ = 6

Lasso

Random Lasso

ST2E

PGA

EM

BBEM

xj ∈ Signal (j= 1:6)
Min Median Max Min Median Max

xj ∈ Noise

11

84

85

55

65

89

8

89

93

40

84

95

70

96

96

87

85.5

77

97

100

90

89

96

100

84

99

88

99

100

100

85

91

99

92

95

100

12

11

18

14

4

4

12

8

14

13

1

4

17

21

25

23

10

8

22

14

21

22

7

9

25

30

34

32

13

15

31

21

27

33

16

14

Table 3: A highly-correlated data. The min, median, max number of times being selected

(i.e., no selection) out of 100 simulations for each type of variables (Signal and Noise) are

shown. The results other than EM and BBEM are from Xin and Zhu (2012).

L = n/2 = 50 and the total number of replicates K = 100. It is well known that cross-

validation based on prediction accuracy tends to include more noise variables. So, for

this example where the true model is known to be sparse, we choose to tune v0 via BIC.
For illustration purpose, we also include BBEM with a ﬁxed tuning parameter v0 = 0.03
in the comparison group. We compare BBEM with the EMVS algorithm from Roˇckov´a

and George (2014), which is implemented by us using the annealing technique for β’s

initialization, and ﬁxed v0 = 0.5, v1 = 1000 as suggested in Roˇckov´a and George (2014).

Table 5.3 reports the average number of signal and noise variables being selected over

100 iterations for each method. BBEM with BIC tuning performs the best: it selects 2.99

signal variables out of 3 on average (i.e., only miss one variable, the weakest signal x1, once
in all 100 iterations) and meanwhile has the smallest type I error. The BBEM algorithm

with a ﬁxed tuning parameter has a similar result as EMVS but is much faster. The

computation advantage for BBEM comes from two aspects: the computation trick that

reduces the computation cost on matrix inversion and the sub-sampling step in Bayesian

15

Figure 1: Highly-correlated data n = 50. A path-plot of the average selection frequency

when v0 varies in the logarithm scale of base 10. Top 6 lines represent the true variables
x1:6 and the bottom 3 lines represent the maximum, median and minimum among the
noise variables x7:40.

bootstrap which allows us to deal with just a subset of variables of size smaller than p.

5.4 A real example

For TFI, a company that owns some of the world’s most well-known brands like Burger

King and Arby’s, decisions on where to open new restaurants are crucial. It usually takes

a big investment of both time and capital at the beginning to set up a new restaurant.

If a wrong location is chosen, likely the restaurant will soon be closed and all the initial
investment will be lost. TFI hosted a prediction competition on Kaggle1, where the goal

is to build a mathematical model to predict the revenue of a restaurant based on a set of

demographic, real estate, and commercial information. The data contains 137 restaurants

in the training set and 1000 restaurants in the test set. Features include the Open Date,

1https://www.kaggle.com/c/restaurant-revenue-prediction

16

0.000.250.500.751.00-4-3-2-10logv0phivariableX1X2X3X4X5X6Xn MaxXn MedXn MinBBEM (BIC)

BBEM (v0 = 0.03)
EMVS

Oracle

xj ∈ Signal xj ∈ Noise
0.24

2.99

2.96

2.97

3

0.27

0.29

0

Table 4: A large-p small-n example. The table shows the average number of signal and

noise variables being selected out of 100 iterations. In BBEM, v0 is either chosen by BIC
or ﬁxed at 0.03. EMVS is the algorithm proposed by Roˇckov´a and George (2014).

City, City Group, Restaurant Type, and three categories of obfuscated data (P1-P37,

numeric): demographic data, real estate data, and commercial data. The response is the

transformed restaurant revenue in a given year.

We ﬁrst transform the “Open Date” to a numeric feature called “Year Since 1900”

and merge the “City” column into the “City Group” column which now contains four

categories: Istanbul, Izmir, Ankara, and others (small cities). Then we crate dummy

variables for the categorical features like “City Group” and “Restaurant Type” and keep

all the obfuscated numeric columns P1-P37. The ﬁnal training set has 43 features and

137 samples.

After standardizing the data, we ﬁx v1 at 100 and tune v0 from 10−4.5 to 10−0.5 for
the BBEM algorithm, where each bootstrap sample uses L = 15 variables, and the total

number of replicates is K = 300. The path-plot of selection frequency for important

features is shown in Figure 5.4.

It is not surprising that “City Group”, “Years Since

1900” and “Restaurant Type” are important predictors for the revenue. Quite a few

obfuscated features are also selected as important predictors. Although we do not know

their meanings, they should provide valuable information for TFI to choose their next

restaurant’s location.

Since the evaluation metric for this speciﬁc competition is based on the rooted mean

square error (RMSE), we use the same metric in our 5-fold cross-validation. We tuned
v0 from the set {0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01}, and found v0 = 0.002 has
the smallest RMSE score. Then we ﬁx v0 at 0.002, and re-run BBEM on the whole training
data. Let m denote the averaged posterior mean of β from L bootstrap iterations, and
γ the averaged selection frequency for p variables. We then use m ∗ γ (where ∗ denotes
element-wise product) for prediction in the same spirit as the Bayesian model averaging.

Our ﬁnal Kaggle score is 1989762.52, which outperforms the random forest benchmark

17

Figure 2: Restaurant data. The path plot of selection frequency when v0 varies in the
logarithm scale of base 10. Only a subset of variables with high selection frequencies are

displayed.

(RMSE=1998014.94) provided by Kaggle2.

It is impressive for BBEM to outperform

random forest considering that BBEM does not use any nonlinear features but random

forest does.

6 Further Discussion

Variable selection is an important problem in modern statistics. In this paper, we study

the Bayesian approach to variable selection in the context of multiple linear regression. We

proposed an EM algorithm that returns the MAP estimate of the set of relevant variables.

The algorithm can be operated very eﬃciently and therefore can scale up with big data. In

2At Kaggle, each team can submit their prediction and see the corresponding performance on the test

data many times, so one can easily obtain a good score by keep tweaking the model to overﬁt the test

data. For this reason, we did not compare our result with those “low” scores on the leaderboard provided

by individual teams.

18

0.000.250.500.751.00-4-3-2-1logv0phivariableCityGroup2İstanbulCityGroup2İzmirCityGroup2OtherP13P2P21P23P28P30P32P34P6P9TypeFCTypeILYearsSince1900addition, we have shown that the MAP estimate from our algorithm provides a consistent

estimator of the true variable set even when the model dimension diverges with the sample

size. Further, we propose an ensemble version of our EM algorithm based on Bayesian

bootstrap, which, as demonstrated via real and simulated examples, can substantially

increase accuracy while maintaining the computation eﬃciency.

Although we restrict our discussion for the linear model, the two algorithm we proposed

can be easily extended to other generalized linear models by using latent variables (Polson

et al., 2013), an interesting topic for future research.

Appendix: Proof of theorem 3.1

Proof. Recall the EM algorithm returns

ˆγnj = 1,

if E

β|Θ(t),y

(cid:2)β2

j

where the threshold

ˆθn
1 − ˆθn
and the conditional second moment of βj is equal to m2

1/v0 − 1/v1

− 2 log

rn =

v1
v0

ˆσ2
n

log

= O(n−r0 log n)

j + ˆσ2

nVjj with

(cid:16)

(cid:3) > rn,
(cid:17)

m = (XT

n Xn + D−1)−1XT

n (Xnβ∗

n + en)

n Xn + D−1)−1D−1β∗

n + (XT

= β∗ − (XT
= β∗ − bn + Wn

V = (XT

n Xn + D−1)−1, D−1 = diag

n Xn + D−1)−1XT
(cid:19)

(cid:18) 1 − ˆγnj

n en

+

ˆγnj
v1

.

v0

equivalent to (cid:40)

Here we represent the posterior mean of β as three separate terms: the true coeﬃcient
n} is
vector β∗

n, the bias term bn and the random error term Wn. So the event {ˆγn = γ∗

(cid:41)

(cid:40)

∩

(cid:41)

min
j:γ∗
nj =1

m2

j + ˆσ2

nVjj > rn

max
j:γ∗
nj =0

m2

j + ˆσ2

nVjj < rn

.

(21)

First we prove the following results that quantify m2

j and Vjj.

(R1) Vjj is upper bounded by the largest eigenvalue of V,

Vjj ≤

1

λn1 + 1/v1

= O(n−η1) ≺ O(n−r0 log n) = rn,

(22)

where for two sequences {an} and {bn}, we write an ≺ bn if an/bn → 0.

19

(R2) The bias term bn is bounded by

|bnj| ≤ (cid:107)bn(cid:107)2 ≤ (cid:107)(XT

max

j

n Xn + D−1)−1(cid:107)2 · (cid:107)D−1β∗
1/v0

n(cid:107)2
n(cid:107)2 = O(nr0−η1+η2).

(cid:107)β∗

≤

λn1 + 1/v1

(23)

When r0 < 2(η1 − η2)/3, maxj |bnj|2 ≺ O(n−r0 log n) = rn.
The matrix L2 norm is deﬁned as (cid:107)A(cid:107)2 = sup(cid:107)v(cid:107)=1 (cid:107)Av(cid:107)2, which is equal to its
largest eigenvalue (singular value) when A is symmetric (non-symmetric).

(R3) Note that Wn is not a Gaussian random vector due to the dependence between D

and en, but it can be rewritten as

Wn = (XT

where A = (cid:0)XT

n Xn + D−1(cid:1)−1(cid:0)XT

n Xn + D−1)−1(XT

n Xn

n Xn)−1XT

n Xn)(XT

(cid:1) and ˜Wn = (XT

n Xn)−1XT

n en = A ˜Wn.

n en. Since A is a

matrix with norm bounded by 1, we have

|Wnj| ≤ (cid:107)A(cid:107)∞ max

j

max

j

| ˜Wnj| ≤ √

p(cid:107)A(cid:107)2 max

j

| ˜Wnj| ≤ √

| ˜Wnj|.

p max

j

(R4) ˜Wn = (XT

n Xn)−1XT

n en is a Gaussian random vector with covariance σ2(XT

n Xn)−1

and mean 0. So the variance for Wnj is upper bounded by σ2λ−1
n1 .
Recall the tail bound for Gaussian variables: for any Z ∼ N(0, τ 2),

P(|Z| > t) = P(|Z|/τ > t/τ ) ≤ τ
t

− t2

2τ 2 .

e

With Result (R3) and Bonferroni’s inequality, we can ﬁnd a constant M > 0 such

that

P(max

j

|Wnj| >

√

rn) ≤ P(max

| ˜Wnj| >(cid:112)rn/p)
≤ p · P(| ˜Wnj| >(cid:112)rn/p)
2pσ2 = O(cid:0)e−M nη1−r0−α(cid:1),

√
≤ p

− rnλn1

e

j

pσ√
rnλn1

which goes to 0 when r0 < η1 − α. So with probability going to 1, maxj |Wnj| is
upper bounded by

√

rn.

(R5) When 1 − η3 < r0, minj:γ∗

nj =1 |β∗

nj|2 ∼ nη3−1 (cid:31) O(n−r0 log n) = rn.

20

Now we prove (21). Given 1 − η3 < r0 < min{η1 − α, 2(η1 − η2)/3}, we have

(cid:32)

P

P

max
j:γ∗
nj =0

(m2

j + ˆσ2

nVjj) > rn

(cid:32)

(cid:33)

min
j:γ∗
nj =1

(m2

j + ˆσ2

nVjj) < rn

(cid:33)

j

(cid:18)(cid:0) max
(cid:18)
(cid:32)
(cid:18)

max

min
j:γ∗
nj =1

j

max

j

≤ P

≤ P

≤ P

≤ P

(cid:19)

|bnj| + max

j

j

√

n max

|Wnj|(cid:1)2 + ˆσ2
(cid:19)
= O(cid:0)e−M nη1−r0−α(cid:1),
nj|2 −(cid:0) max
(cid:19)
= O(cid:0)e−M nη1−r0−α(cid:1).

|bnj| + max

√

rn

j

j

rn

|Wnj| >

|Wnj| >

|β∗

Vjj > rn

(cid:33)

|Wnj|(cid:1)2 < rn

So (21) holds with probability 1 − O(e−M nη1−r0−α) → 1.

References

Breiman, L. (1996), “Bagging Predictors,” Machine Learning, 24, 123–140.

— (2001), “Random Forests,” Machine Learning, 45, 5–32.

Clyde, M. A. and Lee, H. K. H. (2001), “Bagging and Bayesian Bootstrap,” in Artiﬁcial

Intelligence and Statistics, eds. Richardson, T. and Jaakkola, T., pp. 169–174.

Fan, J. and Li, R. (2001), “Variable Selection via Nonconcave Penalized Likelihood and

its Oracle Properties,” Journal of the American Statistical Association, 96, 1348–1360.

George, E. I. and McCulloch, R. E. (1993), “Variable Selection via Gibbs Sampling,”

Journal of the American Statistical Association, 88, 881–889.

Laurent, B. and Massart, P. (2000), “Adaptive Estimation of A Quadratic Functional By

Model Selection,” The Annals of Statistics, 28, 1302–1338.

Mathai, A. and Provost, S. (1992), Quadratic Forms in Random Variables, Statistics: A

Series of Textbooks and Monographs, Taylor & Francis.

Meinshausen, N. and B¨uhlmann, P. (2010), “Stability Selection,” Journal of the Royal

Statistical Society: Series B (Statistical Methodology), 72, 417–473.

Mitchell, T. J. and Beauchamp, J. J. (1988), “Bayesian Variable Selection in Linear Re-

gression,” Journal of the American Statistical Association, 83, 1023–1032.

21

O’Hara, R. B. and Sillanp¨a¨a, M. J. (2009), “A Review of Bayesian Variable Selection

Methods: What, How and Which,” Bayesian Analysis, 4, 85–118.

Polson, N. G., Scott, J. G., and Windle, J. (2013), “Bayesian Inference for Logistic Models

Using Polya-Gamma Latent Variables,” Journal of the American Statistical Association,

108, 1339–1349.

Roˇckov´a, V. and George, E. I. (2014), “EMVS: The EM Approach to Bayesian Variable

Selection,” Journal of the American Statistical Association, 109, 828–847.

Rubin, D. B. (1981), “The Bayesian Bootstrap,” The Annals of Statistics, 9, 130–134.

Tibshirani, R. (1996), “Regression Shrinkage and Selection via the Lasso,” Journal of the

Royal Statistical Society: Series B (Statistical Methodology), 58, 267–288.

Wang, S., Nan, B., Rosset, S., and Zhu, J. (2011), “Random Lasso,” The Annals of Applied

Statistics, 5, 468–485.

Xin, L. and Zhu, M. (2012), “Stochastic Stepwise Ensembles for Variable Selection,”

Journal of Computational and Graphical Statistics, 21, 275–294.

Zhu, M. and Chipman, H. A. (2006), “Darwinian Evolution in Parallel Universes: A

Parallel Genetic Algorithm for Variable Selection,” Technometrics, 48, 491–502.

22

