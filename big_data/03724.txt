6
1
0
2

 
r
a

 

M
1
1

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
4
2
7
3
0

.

3
0
6
1
:
v
i
X
r
a

Eﬃcient Clustering of Correlated Variables and Variable

Selection in High-Dimensional Linear Models

Niharika Gauraha1, and Swapan K. Parui2

Indian Statistical Institute

Abstract

In this paper, we introduce Adaptive Cluster Lasso(ACL) method for variable
selection in high dimensional sparse regression models with strongly corre-
lated variables. To handle correlated variables, the concept of clustering or
grouping variables and then pursuing model ﬁtting is widely accepted. When
the dimension is very high, ﬁnding an appropriate group structure is as dif-
ﬁcult as the original problem. The ACL is a three-stage procedure where, at
the ﬁrst stage, we use the Lasso(or its adaptive or thresholded version) to do
initial selection, then we also include those variables which are not selected
by the Lasso but are strongly correlated with the variables selected by the
Lasso. At the second stage we cluster the variables based on the reduced
set of predictors and in the third stage we perform sparse estimation such as
Lasso on cluster representatives or the group Lasso based on the structures
generated by clustering procedure. We show that our procedure is consistent
and eﬃcient in ﬁnding true underlying population group structure(under as-
sumption of irrepresentable and beta-min conditions). We also study the
group selection consistency of our method and we support the theory using
simulated and pseudo-real dataset examples.

Keywords: High-Dimensional Data Analysis, Correlated Variable
Selection, Adaptive Cluster Lasso, Adaptive Cluster Representative Lasso,
Adaptive Cluster Group Lasso

1Indian Statistical Institute, Bangalore Centre
2Indian Statistical Institute, Kolkata

1. Introduction

We consider the usual linear regression model

Y = Xβ0 + ,

(1)

with response vector Yn×1, design matrix Xn×p, true underlying coeﬃcient
vector β0
p×1 and error vector n×1. When the number of predictors (p) is
much larger than the number of observations (n), p >> n, the ordinary least
squares estimator is not unique and may over ﬁt the data. The parameter
vector β0 can only be estimated based on given very few observations under
assumption of sparsity in β0. To infer the true active set S0 = {j; β0
j (cid:54)= 0},
the Lasso([15]), its variants and other regularized regression methods are
mostly used for sparse estimation and variable selection. However, variable
selection in situations involving high empirical correlation remains one of the
most important issues. This problem is encountered in many applications
such as in microarray analysis, a group of genes which participate in the
same biological pathway tend to have highly correlated expression levels(see
[14]) and it is often required to consider all of them if they are related to the
underlying biological process.

It has been proven that the design matrix must satisfy the following con-
ditions for the Lasso to perform exact variable selection: irrepresentable(IR)
condition([20]) and beta-min condition([4]). Having highly correlated vari-
ables implies that the design matrix violates the IR condition. To deal with
variable selection with correlated variables, mainly two approaches has been
proposed: simultaneous clustering and model ﬁtting and clustering followed
by the sparse estimation (i.e. group lasso). The former approach imposes
restrictive conditions on the design matrix. However, the time complexity for
clustering of variables severely limits the dimension of data sets that can be
processed by the later approach. Moreover, group selection in models with a
larger number of groups is more diﬃcult(see [18]). To overcome these limi-
tations we propose a three stage procedure, Adaptive Cluster Lasso method.
Basically, we try to reduce the dimensions ﬁrst using the Lasso(or its adaptive
or thresholded version) before clustering of variables.

At a high level our method works as follows. At the ﬁrst stage, the Lasso
is used to do initial selection of variables then we also select those variables
which are not selected by the Lasso but they are strongly correlated with the
variables selected by the Lasso. If the design matrix satisﬁes the beta-min
condition, then after the ﬁrst stage, the selected set of variables contains

2

the true active set and the dimensionality of the problem is reduced by a
huge amount. At the second stage, we perform clustering of variables on the
reduced model, so that strongly correlated variables are grouped together
in disjoint clusters. In the third stage, we do group-wise sparse estimation
based on the structures generated by clustering procedure. The second and
third stages of ACL together is the same as Cluster Group Lasso(CGL) or
Cluster Representative Lasso(CRL) with ordinary hierarchical correlation
based clustering, deﬁned in [3]. Hence, ACL method is an extension of the
clustering lasso methods proposed in [3].

Mainly, there are two lines of thought, the one is to ﬁnd an appropriate
and eﬃcient clustering of correlated variables and the other line of thought
is to avoid the false negatives. With these thoughts in mind, we develop
a computationally eﬃcient variable selection procedure ˆSACL, which iden-
tiﬁes the appropriate correlated group structures and selects all variables
from a group of correlated variables where at least one of them is active.
Assuming Group Irrepresentable(GIR) and group beta-min condition on the
design matrix, we prove that the ˆSACL selects the true model, with much
less computational complexity. We show that the dimensionality reduction
and subsequent clustering and CGL(or CRL) improves over the plain clus-
tering and CGL(or CRL). We illustrate the proposed method and compare
it with the methods proposed in [3] by extensive simulation studies and we
also apply it to a pseudo-real dataset.

The rest of this paper is organized as follows. In section 2, we provide
notations, assumptions, review of relevant work and we discuss our contri-
bution. In section 3, we review mathematical theory of the Lasso and group
Lasso. In section 4, we describe the proposed algorithm which mostly se-
lects more adequate models in terms of model interpretation and prediction
performance. In section 5, we study theoretical properties of the proposed
method. We also show that the variable selection is consistent for high di-
mensional sparse problems. In section 6, we provide numerical results based
on simulated and pseudo real dataset. Section 7 contains the computational
details and we shall provide conclusion in section 8.

2. Background and Notations

In this section, we state notations and assumptions, we deﬁne required

concepts and we also provide review of the relevant work.

3

2.1. Notations and Assumptions

The following notations,assumptions and deﬁnitions are applied through-

out this paper.
variable Y ∈ R and p-dimensional predictors Xi ∈ Rp:

We consider the usual linear regression set up with univariate response

Yi =

X (j)

i β0

j + i

i = 1, ..., n j = 1, ..., p

(2)

p(cid:88)

j=1

or, in matrix notation (as in Equation 1)

Y = Xβ0 + 

where β0 ∈ Rp are unknown true regression coeﬃcients to be estimated, and
the components of the noise vector  ∈ Rn are i.i.d. N (0, σ2). The columns
of the design matrix X are denoted by X j. We assume that the design matrix
X is ﬁxed, the data is centred and the predictors are standardized, so that

i=1 Yi = 0,(cid:80)n
(cid:80)n

i=1 X j

i = 0 and 1

X j = 1 for all j = 1, ..., p.

The L1-norm is deﬁned as:

The true active set S0 denotes the support of the subset selection solution(S0 =
supp(β0)) and deﬁned as

nX j(cid:48)

(cid:107)β(cid:107)1 =(cid:80)p
2 =(cid:80)p

(cid:107)β(cid:107)2

j=1 |βj|

j=1 β2
j

L2-norm squared is deﬁned as:

The L∞− norm is deﬁned as:

(cid:107)β(cid:107)∞ = max1≤i≤n||βj|

The sign function is deﬁned as:

sign(x) =

S0 = {j; β0

j (cid:54)= 0}

 −1

0
1

4

if x < 0
if x = 0
if x > 0

(3)

(4)

(5)

(6)

(7)

The (scaled)Gram matrix(covariance matrix) is deﬁned as

ˆΣ =

X(cid:48)X
n

The βS has zeroes outside the set S,as

βS = {βjI(j ∈ S)}

and β = βS + βSc.
For the given S ⊂ {1, 2, ..., p}, the covariance matrix can be partitioned as:

(cid:20) Σ11 = Σ(S)

Σ21(S)

(cid:21)

Σ =

Σ12(S)

Σ22 = Σ(Sc)

(8)

Minimum eigenvalue of a matrix A is denotes as Λmin(A).

2.2. Clustering of Variables

We use correlation based, bottom-up agglomerative hierarchical cluster-
ing methods to cluster predictors, which forms groups of variables based on
correlations between them. For further details on grouping of variables and
determining the number of clusters, we refer to [3].

2.3. The Lasso and the Group Lasso

The Least Absolute Shrinkage and Selection Operator (Lasso) was intro-
duced by Tibshirani [15]. It is a penalized least squares method that imposes
an L1-penalty on the regression coeﬃcients, which does both shrinkage and
automatic variable selection simultaneously due to the nature of the L1-
penalty.

We denote ˆβ, as a Lasso estimated parameter vector. Assume λ is the

regularization parameter, then then Lasso estimator is computed as:

ˆβ ∈ argmin
β∈Rp

{ 1
n

(cid:107)y − Xβ(cid:107)2

2 + λ(cid:107)β(cid:107)1}

and the estimated active set is denoted as ˆS and deﬁned as

ˆS = {j; ˆβj (cid:54)= 0}

The Lasso error vector is deﬁned as

∆ = ˆβ − β0

5

(9)

(10)

(11)

q(cid:88)

j=1

Y =

X (Gj )βGj + 

where X (Gj )βGj =(cid:80)mk

One of the major disadvantages of the the lasso is that, the Lasso tends
to select single or a few variables, from a group of highly correlated variables.
When the distinct groups or clusters among the variables are known a priory
and it is desirable to select or drop the whole group instead of single variables.
Then the Group Lasso (see [19]) or its variants are used, that imposes an L2-
penalty on the coeﬃcients within each group to achieve such group sparsity.
Here we deﬁne some more notations and state assumptions for the group
Lasso. We may interchangeably use β0 and β for the true regression coeﬃ-
cient vector, the later one is without the superscript. Let us assume that the
parameter vector β is structured into groups, G = {G1, ..., Gq}, where q < n,
denotes the number of groups. The partition G basically builds a partition
of the index set {1, ..., p} with ∪q
r (cid:54)= l.
The parameter vector β, then has the structure β = {βG1, ..., βGq} where
βGj = {βr : r ∈ Gj}.

r=1Gr = {1, ..., p} and Gr ∩ Gl = ∅,

The columns of the each group is represented by X Gj .

X = (X (1), ..., X (p)) = (X (G1), ..., X (Gq))

The response vector Y can also be written as

same as the loss function of the Lasso 1
is deﬁned as

(βGj )k. The loss function of the group Lasso is
2. The group Lasso penalty

k=1 X (Gj )

k

(cid:107)β(cid:107)2,1 =(cid:80)q

(cid:113) mj
n(cid:107)Y −Xβ(cid:107)2
j=1 (cid:107)X Gj βGj(cid:107)2

n

where mj = |Gj| is the group size. Since the penalty is invariant under
parametrizations within-group. Therefore, without loss of generality, we can
assume Σrr = I, the mr×mr identity matrix. Hence the group Lasso penalty
can be written as

q(cid:88)

√

(cid:107)β(cid:107)2,1 =

mj(cid:107)βGj(cid:107)2

The Group Lasso estimator(with known q groups) is deﬁned as

j=1

ˆβgrp ∈ argmin

(cid:107)Y − Xβ(cid:107)2

2 + λ(cid:107)β(cid:107)2,1}

(12)

{ 1
n

β

The group Lasso has the following properties:

6

• The group Lasso behaves like the lasso at the group level, depending
on the value of the regularization parameter λ, the whole group of
variables may drop out of the model.

• For singleton groups (when the group sizes are all one), it reduces

exactly to the lasso.

• The group Lasso penalty is invariant under orthonormal transformation

within the groups.

• The group Lasso estimator has similar oracle inequalities as the stan-
dard Lasso for prediction accuracy and estimation error. It has group
wise variable selection property(We discuss mathematical theory in the
section 3).

Let W denote the actives group set , W ⊂ {1, ..., q}, with cardinality
w = |W|. Throughout the article, the following assumption are made for the
group Lasso:

(A1): The size of the each group is less than the number of observations.

mmax < n

.

(A2): The number of active groups, w, is less than the number of observations

(sparsity assumption).

2.3.1. Cluster Group Lasso

When the group structure is not known then clusters G1, ..., Gq are gener-
ated from the design matrix X( using correlation based method etc.). Then
the group Lasso is applied to the resulting clusters. We denote the clusters
selected by the group Lasso as ˆSclust, and is deﬁned as

ˆSclust = {r : cluster Gr is selected, r = 1, ..., q}

The union of the selected clusters gives the selected set of variables.

ˆSCGL = ∪r∈ ˆSclust

Gr

7

(13)

(14)

2.3.2. Cluster Representative Lasso

Similar to the CGL, the cluster representative Lasso, ﬁrst identiﬁes groups
among the variables and then applies the lasso for cluster representatives (see
[3]). When sign of the regression coeﬃcients within a group is the same then
taking group representatives is advantageous, whereas when near cancellation
among β0
We deﬁne representative for each cluster as

j (j ∈ Gr) takes place then CGL is preferred.

(cid:88)

j∈Gr

¯X (r) =

1
|Gr|

X (j),

r = 1, ..., q.

The design matrix of cluster representatives is denoted as ¯Xn×q. Then opti-
mization problem for CRL is deﬁned using response Y and the design matrix
of cluster representatives ¯X as:
ˆβCRL ∈ argmin

((cid:107)y − ¯Xβ(cid:107)2

2 + λCRL(cid:107)β(cid:107)1)

(15)

β

The selected clusters are then denoted as:

ˆSclust,CRL = {r; ˆβCRL,r (cid:54)= 0, r = 1, ..., q}

and the selected variables are obtained as the union of the selected clusters

as:

ˆSCRL = ∪r∈ ˆSclust,CRL

Gr

2.4. Review of Relevant work and our Contribution

Here, we provide a brief review of relevant work in this area, and we also

show that how our proposal diﬀers or extends the previous studies.

The Lasso can not do variable selection in the situations where predictors
are highly correlated. As mentioned before, to handle correlated covariates in
variable selection methods, two algorithmic approaches have been developed
in the past: clustering of variables and model ﬁtting either simultaneously
or at two diﬀerent stages. Examples of the methods that do clustering and
model ﬁtting simultaneously are Elastic Net([21]), Fused LASSO([16]), oc-
tagonal shrinkage and clustering algorithm for regression(OSCAR, [7]) and
Mnet([9] ) etc. The Elastic Net uses a combination of the L1 and L2 penalties,
OSCAR uses a combination of L1 norm and and L∞ norm and Mnet uses
a combination of L2 and Minimum Concave Penalty(MCP). We note that

8

these methods use only combination of penalties, they do not use any speciﬁc
information on the correlation pattern among the predictors and hence they
do not reveal any group structure in the data.

Now, we discuss a few methods that perform clustering and model ﬁtting
at diﬀerent stages, i.e. Cluster Group Lasso(CGL, [3]), Cluster represen-
tative Lasso(CRL,[3]), Stability Feature Selection using Cluster Represen-
tative LASSO (SCRL, [6]) and sparse Laplacian shrinkage estimator(SLS,
[11]). CRL, CGL and SCRL use correlation based and canonical correlation
methods to perform hierarchical clustering. SLS also considers the correla-
tion patterns among predictors but requires that highly correlated variables
should have similar predictive eﬀects.The main disadvantage of this approach
is mainly due to clustering in the presence of unstructured data or noise fea-
tures.
It is diﬃcult to determine the exact group structures or the exact
number of groups in high-dimensions and in the presence of noise features.
Moreover, the CPU time taken by clustering algorithms is unacceptable when
the number of predictors are huge. To address these problems, we propose
to reduce the dimensionality before performing clustering which makes our
proposal diﬀerent from previous work.

Basically, our work can be viewed as an extension of the two stage proce-
dure, Cluster Lasso Methods with correlation based clustering, proposed in
[3]. The extension is that we add a dimensionality reduction stage prior to
performing the clustering, which leads to clustering of variables more accu-
rately and eﬃciently and thus consistent group variable selection. In partic-
ular, we consider Adaptive Clustering Group Lasso(ACGL), where the Lasso
is used as preprocessing step at the ﬁrst stage, correlation based clustering at
second stage and the group Lasso in the third stage(deﬁned in section 4). We
also consider the CGL method with ordinary hierarchical clustering, denoted
by CGLcor, see [3]. We compare ACGL and CGLcor in terms of predictive
performance, variable selection and CPU time expended, in section 5. Our
extensive simulation studies show that ACGL outperforms the CGLcor.

3. Mathematical Theory of the Lasso and the Group Lasso

In this section, we review the results required for proving consistent vari-
able selection( and group variable selection) in high dimensional linear mod-
els. For more details on the mathematical theory for the lasso and group
lasso, we refer to: [17], [20], [18] [3] and [4].

9

Deﬁnition 1 (Compatibility Condition) The Lasso compatibility condi-
tion holds for a ﬁxed set S ⊂ {1, ..., p} with cardinality s = |S|, a constant
φcomp(S) > 0 and if for all (cid:107)∆Sc(cid:107)1 ≤ 3(cid:107)∆S(cid:107)1 (cid:54)= 0 the following holds

(cid:26) s 1
n(cid:107)X∆(cid:107)2
φ2(S)

2

(cid:27)

(cid:107)∆S(cid:107)2

1 ≤

where φcomp(S) is called the compatibility constant.
The constant 3 is due to the condition λ ≥ 2λ0, which is required to overrule
the stochastic process part,(see [4] for details). Without loss of generality we
can assume S = {1, ..., s} and partition the covariance matrix in block-wise
form as given in equation 8. Assuming Σ−1
11 is invertible, the various form of
Irrepresentable(IR) Conditions are deﬁned as follows.

Deﬁnition 2 The strong irrepresentable condition is said to be met for the
set S, with cardinality s = |S|, if the following holds:

(cid:107)Σ12(S)Σ−1(S)τS(cid:107)∞ < 1,

∀τS ∈ Rs such that (cid:107)τS(cid:107)∞ ≤ 1

(16)

The weak irrepresentable condition holds for a ﬁxed τS ∈ Rs if

(cid:107)Σ12(S)Σ−1(S)τS(cid:107)∞ ≤ 1

For some 0 < θ < 1, the θ uniform irrepresentable condition holds if

(cid:107)Σ12(S)Σ−1(S)τS(cid:107)∞ ≤ θ

max
(cid:107)τS(cid:107)∞≤1

(17)

(18)

Suﬃcient conditions(eigenvalue and mutual incoherence) on design ma-

trix to hold IR are discussed in [20] and [8].

Deﬁnition 3 (Beta-min Condition) the Beta Min Condition is met for
the regression coeﬃcient β0, if min|β0| ≥ 4λs0

φ2(S)

Lemma 4 Under the following assumptions the Lasso selects the true active
set S0 with high probability:

(A3): Irrepresentable Condition holds for S0.

(A4): beta-min condition holds for β0.

10

The following inequality shows the bounds for prediction error and es-
timation error of the Lasso estimator.( for derivation and proof we refer to
[4]).

1
n

(cid:107)X∆(cid:107)2

2 + λ(cid:107)∆(cid:107)1 ≤ 4λ2s

φ2

comp

(19)

Our error analysis for the group Lasso is based on the pure active group and
pure noise group assumptions, that is,
(A5) : all variables are active variables within an active group and no vari-
ables are active in a noise group.

We deﬁne the group Lasso error as ∆Gr = βGr − β0

Gr, and also assume

the following.
(A6): We assume that clustering process identiﬁes the group structure cor-
rectly.

Deﬁnition 5 (The Group Lasso Compatibility Condition) The group
Lasso compatibility condition holds for a ﬁxed set W ⊂ {1, ..., q} with car-
r∈W c (cid:107)∆Gr(cid:107)2 ≤

dinality w = |W|, a constant φgrp(W ) > 0 and if for all (cid:80)
3(cid:80)

r∈W (cid:107)∆Gr(cid:107)2 (cid:54)= 0 the following holds

(cid:88)

(
r∈W

(cid:107)∆Gr(cid:107)2)2 ≤

(cid:27)

(cid:26) w 1
n(cid:107)X∆(cid:107)2
φ2
grp(W )

2

where φgrp(W ) is called the group Lasso compatibility constant.

The Lasso compatibility condition implies the group Lasso compatibility
condition, it is explained by the following Lemma(See Lemma 8.2 of the book
[4], for the proof).
Lemma 6 Let W ⊂ {1, ..., q} be a group index set, say, W = {1, ..., w}
Consider the full index set corresponding to W:

S = {(1, 1), ..., (1, m1), ..., (w, 1), ..., (w, mw)} = {1, ..., s}

, where (i,j) denotes jth member of ith group and s =(cid:80)w

j=1 mj. If compatible
condition holds for S with compatiblility constant φ(S) then the compatibility
condition holds for the φgrp(W ), and φgrp(W ) ≥ φ(S)

11

Deﬁnition 7 (Group Irrepresentability Condition) The group IR con-
dition is met for the set W with a constant 0 < θ < 1, if for all τ ∈ Rs with
(cid:107)τ(cid:107)2,∞ = max1≤r≤q (cid:107)τGr(cid:107)2 ≤ 1, the following holds

(cid:107)(Σ21Σ−1

11 Kτ )Gr(cid:107) ∀r (cid:54)∈ W,

1
mr

(20)

where K = diag(m1Im1, ..., mwImw)

We note that the GIR deﬁnition reduces to the Lasso IR condition for sin-
gleton groups(see [1]).

mr

Deﬁnition 8 (Group beta-min Condition) The group beta-min Condi-
√
tion is met for β0 , if (cid:107)βGr(cid:107)∞ ≥ Dλ
∀r ∈ W , where D > 0 is a constant
which depends on σ, φgrp and other constants used in cone constraints and
GIR conditions.
We note that, only one component of the βGr,∀r ∈ W has to be suﬃciently
large, because we aim to select groups as a whole, and not individual vari-
ables. For its exact form, we refer to [5].

n

Theorem 9 Under the following assumptions the group Lasso selects the
true active groups W with high probability:

(A7): GIR Condition holds for W .
(A8): Group beta-min condition holds for βGr,∀r ∈ W .

Next, we discuss suﬃcient condition for the GIR to hold. We denote
rX Gl/n, r, l ∈ {1, ..., q}. We partition the covariance matrix group
(here we assume that each Σr,r is non-singular, or we may use the

(cid:48)
Σr,l = X G
wise.
pseudo inverse)



RW =

I

...

−1/2
22 Σ21Σ

−1/2
11

Σ

−1/2
ww Σw1Σ

−1/2
11

Σ

−1/2
11 Σ12Σ

−1/2
22

Σ

I
...

−1/2
ww Σw2Σ

−1/2
22

Σ

... Σ
... Σ
. . .
...

−1/2
11 Σ1wΣ
−1/2
22 Σ2wΣ

−1/2
ww
−1/2
ww

...
I



We note that diagonal elements are Imr×mr identity matrix due to parame-
terization invariance properties.

12

Now Suppose that RW , has smallest eigenvalue Λmin(RW ) > 0 and that
canonical correlations between groups are small enough that intern implies
the incoherence assumptions. Therefore, under the eigenvalue and incoher-
ence condition of RW , the group irrepresentable condition holds(see [3]).

Now we prove that the Lasso IR condition implies the group Lasso IR(GIR)

condition.
Lemma 10 Let W ⊂ {1, ..., q} be a group index set, say, W = {1, ..., w}
Consider the full index set corresponding to W:

S = {(1, 1), ..., (1, m1), ..., (w, 1), ..., (w, mw)} = {1, ..., s}

where s =(cid:80)w

j=1 mj. If the Lasso IR condition holds for the set S then the

group Lasso IR condition holds for the set W.

Proof is trivial, the IR condition on the set S implies that Σ11 is invertible ,
Λmin(Σ11) > 0, and correlation between variables in S and between variables
in S and Sc are small enough. That implies small enough canonical corre-
lations within the groups in active groups W, and between the groups in W
and W c. The small enough canonical correlations between groups ensure the
incoherence assumptions and therefore the GIR condition holds.

The following inequality shows the similar bounds for prediction error

and estimation error of the group Lasso estimator([3]).

(cid:107)∆Gr(cid:107)2 ≤ 24λ2(cid:80)

q(cid:88)

r=1

r∈W mr

φ2
grp(W )

(cid:107)X∆(cid:107)2

2 + λ

1
n

4. The Adaptive Cluster Lasso Methods

It is known that the Lasso tends to select one or few variables from the
group of highly correlated variables, even though many or all of them belong
to the active set. We aim to avoid false negatives and solve clustering problem
eﬃciently and more accurately. To solve clustering problem eﬃciently, we
propose a preprocessing step to reduce the dimensionality using the Lasso
methods before clustering of variables. To avoid the false negative, we use the
concept of clustering the correlated variables and then selecting or dropping
the whole group instead of single variables same as the CGLcor(CRLcor)
proposed in [3]. The proposed procedure, ACL is a 3-stage procedure, where
we can choose to use diﬀerent methods at diﬀerent stages depending on the

13

nature of the problem. The diﬀerent stages of the ACL procedure is explained
as follows.

1. Dimensionality Reduction

For selecting initial set of variables, we use the Lasso(or its adaptive
or thresholded version). Since the Lasso tends to select one or a few
variables from the group of strongly correlated variables, therefore we
use the Lasso to select the group representative predictors. After we
have selected the initial set of variables(group representative members),
we get the rest of the group members by simple correlated screening.
In section 3, we have shown that for highly correlated structures, the
variables set selected by this approach always contains the true active
set under assumption of GIR and GBM on the design matrix. Let the
variables set selected by Lasso is given by

ˆSLasso = {j; ˆβLasso,j(λ1) (cid:54)= 0}.

(21)

Then we select correlated variables as
ˆScorr = {k;
k ∈ {1, ..., p} \ ˆSLasso, j ∈ ˆSLasso and corr(Xj, Xk) > ρ},
where λ1 is the tuning parameter used by Lasso and ρ > 0.7 denotes
the strong correlation between two variables. Then the selected set of
variable are given by

ˆS1 = ˆSLasso ∪ ˆScorr

(22)

2. Clustering of Variables

After ﬁrst stage there may be huge amount of reduction in the dimen-
sionality, we denote the reduced design matrix as Xred = {Xj; j ∈ ˆS1}.
On the reduced set of predictors, we apply correlation based clustering
methods to group strongly correlated variables into disjoint groups. We
denote the inferred clusters as G1, ..., Gq.

3. Supervised Selection of Clusters

From the reduced design matrix Xred, and inferred clusters G1, ..., Gq
as described in previous stages, we select the variables in a group-wise
fashion which involves selecting or dropping the group as a whole. Var-
ious methods have been proposed to achieve grouping eﬀect in case of
highly correlated variables, i. e. the group Lasso([19]), Group Square-
Root Lasso([5]), Adaptive group Lasso([18]) and Lasso on cluster rep-
resentatives etc.

14

Suppose, the selected set of groups are denoted by
ˆSG = {r : group Gr is selected}

The ﬁnal selected set of variables is then the union of the selected
groups.

ˆSACL = ∪rr ∈ ˆSG

Algorithm 1: ACGL Algorithm

Input: dataset (Y, X)
Output: ˆS:= set of selected variables
Steps: Perform Lasso on data (Y, X), Denote ˆSLasso as variable set
selected
S1 := ˆSLasso
for

each j ∈ ˆSLasso do

for

each k ∈ {1, ..., p}/ ˆSLasso do
S1 = S1 ∪ {k}

if corr(Xj, Xk) > ρ then

end

end

end
Let Xred = X S1 be the reduced design matrix
Perform Clustering of variables on data Xred,
Denote clusters as G1, ..., Gq and partition variable set as
ˆSG1, ..., ˆSGq
Perform group Lasso on (Y, Xred) with group information G1, ..., Gq,
denote the selected set of groups as
ˆScluster = {r; cluster Gris selected, r = 1, ..., q}.
The union of the selected groups is then the selected set of variables
ˆSACL = ∪rr ∈ ˆScluster
return ˆS

4.1. Complexity Analysis of the ACL Method

In this section, we compute time complexity of the ACL method at dif-

ferent stages.

15

1. First stage

The time complexity of the ﬁrst stage consists of the time complex-
ity of the Lasso plus time required for variable screening.
suppose
ˆs = | ˆSLasso|, denotes the number of variables selected by Lasso, then
time taken by variable screening is ˆs ∗ (p − ˆs), which is O(pˆs).

2. Second stage

Computationally, The second step can be completely avoided. Cluster-
ing can be done while screening correlated variables at the ﬁrst stage
itself. However, We opted to state the clustering method separately for
transparency and for deriving its theoretical properties, in particular
comparing it with other methods where clustering is performed at dif-
ferent stages i.e. CGLcor. But while implementing the algorithm, we
can eﬃciently combine variable screening and clustering. So no extra
computational cost is added at this stage.

3. Third stage

The same computational complexity as for the group Lasso, which de-
pends on the number of groups and size of each group.

Hence the overall time complexity of the proposed method is dominated
by the time required for the Lasso and the group Lasso, see [12] for complexity
analysis of the Lasso.

5. Theoretical Properties of the ACL Procedure

In this section, we study the theoretical properties of the ACL methods,
and we show that nothing is lost by using ACL methods instead of Clustering
Lasso methods with correlation based clustering as proposed in [3]. Particu-
larly, under the GIR and group beta-min condition, the ACGL method has
the same accuracy as the CGLcor, in terms of estimation, prediction and
variable selection. The gain is in terms of computations since the ACGL
performs clustering on the reduced set of predictors.

We introduce the following theorems which are needed for proving vari-

able selection consistency for the proposed algorithm.

Theorem 11 (Reduced-set IR condition for the Lasso) Suppose that,
the uniform-θ IR condition is met for the true active set S := S0, which in

16

turn implies that with large probability, the Lasso does not make false pos-
itive selection of variables. Then for any S1 ⊂ S, the uniform-θ1(≤ θ) IR
condition holds for the set S1.

Proof We invoke the result given in the book [4], Corollary 7.2. Since the
uniform-θ IR condition holds for the set S, then the following inequality also
holds.

√

(cid:113)(cid:80)

s maxj(cid:54)∈S

k∈S σ2
jk

Λ2

min(Σ11(S))

≤ θ

where σjk denotes the (jk)th entry of Σ.
Now we delete some variables from S, and denote the reduced subset as
S1 and corresponding partition of variance-covariance matrix is Σ11(S1) and
Σ21(S1). Since S1 ⊂ S, the following inequality holds because any symmetric
minor of Σ11(S) will have min-eigenvalues at least as big as Λmin(Σ11(S)).

Λmin(Σ11(S1)) ≥ Λmin(Σ11(S))

Therefore

√

s1 maxj(cid:54)∈S1

θ1 =

(cid:113)(cid:80)

(cid:113)(cid:80)

√
s maxj(cid:54)∈S

k∈S1

σ2
jk

≤

k∈S σ2
jk

≤ θ

Λ2
Hence the IR condition holds for the set S1 ⊂ S.

min(Σ11(S1))

Λ2

min(Σ11(S))

The similar result holds for the group Lasso which is given in the following

lemma.

Lemma 12 (Reduced-group IR condition for the group Lasso) Suppose
that, the uniform-θ IR condition is met for the set of true active groups
W ⊂ {1, ..., q}, It implies that with large probability, the Lasso does not make
false positive selection of groups. Then the uniform-θ1(≤ θ) GIR condition
holds for the following cases:

• for any W1 ⊂ W , when the number of groups are reduced.
• when |W1| = |W| but {Gr, r ∈ W1} ⊂ {Gr, r ∈ W}, group sizes are

reduced for some groups.

• when group size as well as number of groups are reduced, W1 ⊂ W and

{Gr : r ∈ W1} ⊂ {Gr : r ∈ W}.

17

w disjoint groups are formed within S such that S =(cid:80)

Proof is trivial. Suppose that IR condition holds for the active set S. If
j∈W mj, then IR for
the Lasso imply IR condition for the group Lasso. Since the Reduced-set IR
will hold for S1 ⊂ S, where the reduced set can be interpreted as change in
group structure in terms of reduced number of groups and/or reduced size
of groups, as

(cid:88)

r∈W1

S1 =

mr, W1 ⊂ W

Therefore Reduced-group IR will hold for the set S1.

5.1. Case Studies

In this section, we illustrate the variable selection consistency of the pro-
posed method using a couple of scenarios under assumption of GIR, group
beta-min and no noise case.

5.1.1. Orthonormal Case
The case Σ ≈ I corresponds to uncorrelated variables and hence IR condi-
tion holds for any S ⊂ {1, ..., p} and we also assume that beta-min condition
holds for a ﬁxed S.

We claim that ACGL selects the true active set ˆSACGL = S0 with high

probability.
Proof:
First, we perform the Lasso operation on the pair (Y,X), to get the initial set
of variables, say ˆS1. Then with high probability ˆS1 = S0 under assumption
of IR and beta-min condition. Since variables are uncorrelated no additional
variable will be pulled in when we do correlation screening. At the second
stage, we perform clustering on the reduced set of predictors. Clustering
process will report the singleton groups due to independence structure, and
ﬁnally at the third step CGL will select the true active set S0 again, due to
reduced-set IR condition.

We proved that ACGL consistently selects the true active set for the
orthonormal case. It is obvious that, there is no advantage of using AGCL
or the plain CGL/CRL methods over the standard Lasso for this case. But
ACL outperforms over CGL/CRL in terms of computations, since AGCL
considers the reduced set of predictors for clustering and the group lasso
is called for reduced number of groups, whereas plain CGL/CRL considers

18

all p variables for clustering which requires huge computations when the
dimension is ultra high.

5.1.2. Block Diagonal Case
The case Σ ≈ diag(T1,T2, ...,Tq) corresponds to uncorrelated groups and
hence group IR condition holds for any W ⊂ {1, ..., q} and we also assume
that group beta-min condition holds. Each Ti is a mi × mi matrix with
elements as

(cid:26) 1,

ρ,

(Ti)j, k =

j = k
else

where ρ > 0.7, since variables are highly correlated within each group. With-
out loss of generality we can assume that all the variables are ordered in a way
such that all active groups come ﬁrst. W ⊂ {1, ..., q} then W = {1, ..., w}.
and we also assume pure active or pure noise group. Let

S0 = {(1, 1), ..., (1, m1), ..., (w, 1), ..., (w, mw)} = {1, ..., s0},

be the true active set. The Lasso tends to select(depending on the amount of
regularization) one variable from each active block. Without loss of generality
we assume that the Lasso selects (j,1) variable from each j ∈ W , and the
selected variable set is ˆSLasso = {(1, 1), (2, 1), ..., (w, 1)}, Now we add all
variables from {1, ..., p}/SLasso which are strongly correlated with atleast of
the the variable from SLasso. Therefore we get
ˆS1 = {1, ..., s}
=⇒ ˆS1 = S0

Hence, after the ﬁrst stage of dimensionality reduction, the selected set of
variables contains the true active variables. Assuming that the clustering
procedure correctly identiﬁes the true underlying group structure, then the
group Lasso at the third stage correctly selects all the w groups, due to the
sub-group IR condition for the group Lasso. Hence, the proposed method
consistently selects the true active set under the assumption of GIR and
group beta-min condition for the block diagonal case as well.

One may argue that, there is no need for the second and the third stage.
Speciﬁcally, when the Lasso selects one variable per active group then correla-
tion screening will bring in those correlated variables which were not selected
by the Lasso. We refer to the discussion in [10], on using the group Lasso
over Lasso.

19

6. Numerical Results

In this section, we consider three simulation settings and a pseudo real
data example in order to empirically compare the performance of the pro-
posed method with the other existing methods. Since the comparison be-
tween the Lasso, CGL and CRL have already been studied in the paper [3],
here we only report the results for ACGL and CGLcor methods.

6.1. Simulation Study

Three examples are considered in this simulation. In each example, data
is simulated from the linear model in (equation 1) with ﬁxed design X. All
the three examples are the same as simulation examples used in the paper
[3].

For each example, our simulated dataset consisted of a training and an
independent validation set and 50 such datasets were simulated for each
example. The models were ﬁtted on the training data for each 50 datasets and
the model with the lowest test set Mean Squared Error(MSPE) was selected
as the ﬁnal model. For model interpretation, we consider true positive rate
as a measure of performance. We also measure the CPU time expended by
each methods. The MSE and the true positive rate are deﬁned as follows.

n(cid:88)

i=1

(23)

(24)

M SE =

1
n

T P R = f (| ˆS|) =

(yi − ˆyi)2

| ˆS(cid:84) S0|

|S0|

6.1.1. Block Diagonal Model
matrices T , and T is a 10 × 10 matrix, deﬁned as

We generate covariates from Np(0, Σ1), where Σ1 consists of 100 block

(cid:26) 1,

.9,

Tj,k =

j = k
else

For the regression coeﬃcient β0 the following four conﬁgurations are consid-
ered:
(E1.1) S0 = {1, 2, ..., 20} and for any j ∈ S0 we sample β0
from the set
{.1, .2, .3, ..., 2} without replacement (a new for each simulation run). This
set up has all the active variables in the ﬁrst two blocks of highly correlated

j

20

variables.
(E1.2) S0 = {1, 2, 11, 12..., 91, 92} and for any j ∈ S0 we sample β0
j from
the set {.1, .2, .3, ..., 2} without replacement (a new for each simulation run).
This set up has all the active variables in the ﬁrst and the second variables
of the ﬁrst ten blocks.
(E1.3) The β0 has the same conﬁguration as in (E1.1) but we change the sign
of randomly chosen half of the active parameters (a new for each simulation
run).
(E1.4) The β0 has the same conﬁguration as in (E1.2) but we change the sign
of randomly chosen half of the active parameters (a new for each simulation
run).
Simulation results are reported in table 1(MSE and standard deviation), ﬁg-
ure 1(TPR) and table 2(CPU time).

σ Method
ACGL
3
CGLcor
12 ACGL
CGLcor

E1.1

E1.2

E1.3

E1.4

12.46 (1.76)
14.97 (2.40)
188.23 (23.32)
206.19 (29.97)

21.95 (2.98)
37.05 (5.21)
149.97 (28.98)
186.61 (25.69)

9.308 (2.40)
13.34 (2.06)
129.29 (22.93)
160.31 (23.04)

20.90 (4.81)
24.31 (6.50)
165.91 (22.36)
168.26 (24.70)

Table 1: MSE(sd) for Example block diagonal model

σ Method E2.1 E2.2 E2.2 E2.4
3
6.46
2510
3.33
2510

ACGL
CGLcor
12 ACGL
CGLcor

4.25
2510
3.91
2510

5.04
2510
5.86
2510

2.09
2510
1.08
2510

Table 2: CPU times(in seconds) for block diagonal model

From table 1, we see that the ACGL method has lower prediction error
than the CGLcor for all four conﬁgurations and ﬁgure 1 shows that the ACGL
has higher TPR. From table 2 we see that ACGL is much eﬃcient, the CPU
time required for ACGL for all four conﬁgurations are much less than as
compared to the CPU time expended by CGLcor. Please note that CPU
time for CGLcor is approximately the same for all conﬁgurations.

21

| ˆS(cid:84) S0|

Figure 1: Plot of
CGLcor(blue dashed-dotted line)

|S0|

vs.

| ˆS| for block diagonal model. ACGL(green solid line) and

6.1.2. Single Block Design

We generate covariates from Np(0, Σ2), where Σ2 consisted of a single

group of strongly correlated variables of size 30, it is deﬁned as

 1,

Σ2;j,k =

j = k

0.9 i, j ∈ {1, ..., 30} and i! = j,
0

otherwise

The remaining 970 variables are uncorrelated. For the regression coeﬃcient
β0 we consider the following four conﬁgurations:
(E2.1) S0 = {1, 2, ..., 15} ∪ {31, 32, ..., 35} and for any j ∈ S0 we sample β0
from the set {.1, .2, .3, ..., 2} without replacement (new for each simulation
run). The correlated block contains 15, the most of the active predictors and
the remaining ﬁve active predictors are uncorrelated.
(E2.2) S0 = {1, 2, ..., 5} ∪ {31, 32, ..., 45} and for any j ∈ S0 we sample β0
from the set {.1, .2, .3, ..., 2} without replacement (new for each simulation

j

j

22

run). Here the correlated block contains only 5 active predictors, and the
remaining 15 predictors are uncorrelated.
(E2.3) The β0 has the same conﬁguration as in (E2.1) but we change the sign
of randomly chosen half of the active parameters (new for each simulation
run).
(E2.4) The β0 has the same conﬁguration as in (E2.2) but we change the sign
of randomly chosen half of the active parameters (new for each simulation
run).

Simulation results are reported in table 3, table 4 and ﬁgure 2.

σ Method
ACGL
3
CGLcor
12 ACGL
CGLcor

E2.1

E2.2

E2.3

E2.4

11.40 (4.2)

247.52 (28.74)
146.17(23.46)
384.78 (48.26)

29.94 (5.34)
54.73 (10.59)
192.64 (12.81)
191.26 (25.55)

15.01 3.28)
21.37 (9.51)
127.91 (22.02)
159.40 (23.88)

27.03 (3.9)
31.58 (14.17)
159.62 (26.40)
174.49 (25.40)

Table 3: MSE(sd) for single block model

σ Method E2.1 E2.2 E2.3 E2.4
5.63
3
2463
2.47
2463

ACGL
CGLcor
12 ACGL
CGLcor

2.63
2463
2.37
2463

7.58
2463
3.17
2463

5.52
2463
4.92
2463

Table 4: CPU times(in seconds) for single block model

Table 3 shows that the ACGL method has lower predictive performance
than the CGLcor for all four conﬁgurations and ﬁgure 2 shows that in terms
of variable selection, ACGL is clearly better than CGLcor. From table 3, we
see that CPU time required(in seconds) for ACGL for all four conﬁgurations
are much less than as compared to the CGLcor. The CPU time for CGLcor
is approximately the same for all conﬁgurations.

6.1.3. Duo Block Model
matrices T , and T is a 2 × 2 matrix, deﬁned as

We generate covariates from Np(0, Σ3), where Σ3 consists of 500 block

Tj,k =

j = k
else

(cid:26) 1,

.9,

23

| ˆS(cid:84) S0|

Figure 2: Plot of
CGLcor(blue dashed-dotted line)

|S0|

vs.

| ˆS| for single block model. ACGL(green solid line) and

For the regression coeﬃcient β0 we consider S0 = {1, 2, ..., 20} and for any
j ∈ S0

2,

log p
n σ

1.9

,

j ∈ {1, 3, 5, 7, 9, 11, 13, 15, 17, 19},
j ∈ {2, 4, 6, 8, 10, 12, 14, 16, 18, 20}

(cid:40)

√

β0
j =

1
3

In this setup, the β0 is the same for all 50 simulation runs. The Lasso would
not select the variables from {2, 4, 6, ..., 20}, since they do not satisfy the
beta-min condition but it would select the other from {1, 3, 5, ..., 19}. The
Table 5, Table 6 and Figure 3 show the simulation results for the duo block
model.

The results show that the ACGL performs better in terms of predic-
tive performance, variable selection and CPU time required. (We stopped
recording CPU time for CGLcor after 2500sec, here clustering of a thousand
of variables and then the group lasso for ≈500 clusters make the process very
slow.

24

σ Method
ACGL
3
CGLcor
12 ACGL
CGLcor

MSE(sd)

20.82 (5.94)
32.00 (6.50)
179.11 (19.35)
193.97 (27.05)

Table 5: MSE(sd) for duo block model

| ˆS(cid:84) S0|

Figure 3: Plot of
CGLcor(blue dashed-dotted line)

|S0|

| ˆS| for duo block model. ACGL(green solid line) and

vs.

6.2. Pseudo Real Data

We consider here a real dataset, riboﬂavin(n = 71, p = 4088) data for
the design matrix X with synthetic regression coeﬃcients β0 and simulated
Gaussian errors Nn(0, σ2I). See [2] for the details on riboﬂavin dataset. We
select the ﬁrst thousand covariates which have largest empirical variances.
We ﬁx the size of the active set to s0 = 10. For the true active set, we
randomly select a variable, say variable k(a new in each simulation), and

25

σ Method CPU time
3

7.69

ACGL
CGLcor
12 ACGL
CGLcor

> 2500

2.94

> 2500

Table 6: CPU time(in seconds) for duo block model

then we select other nine variables which have highest absolute correlation
to the variable k, and for each j ∈ S0 we set βj = 1. This conﬁguration is
exactly the same as pseudo real example used in [3].

The performance measures are reported in table 7, ﬁgure 4 and table
8, based on 50 independent simulation runs. Here we compare CRLcor([3])
with Adaptive Cluster Representative Lasso(ACRL) where the Lasso is used
as preprocessing step at the ﬁrst stage, correlation based clustering at second
step and the Lasso for cluster representatives in the third stage. The Group
Lasso is not appropriate for this setup, since k is chosen arbitrarily and the
group size may exceed the number of observations.

σ Method MSPE(std)
2.36 (0.52)
3
39.02 (25.15)
25.02 (4.03)
50.40 (27.68)

ACRL
CRLcor
15 ACRL
CRLcor

Table 7: MSE(sd) for Riboﬂavin dataset

σ Method
ACGL
3
CRLcor
12 ACGL
CRLcor

E3
1.47
2347
1.48
2471

Table 8: CPU times(in seconds) for Riboﬂavin dataset

The table 7, ﬁgure 4 and table 8 show that ACRL performs better than
CRLcor in terms of prediction, variable selection and CPU time consumption.

26

| ˆS(cid:84) S0|

Figure 4: Plot of
CRLcor(magenta dashed-dotted line)

|S0|

| ˆS| for Riboﬂavin dataset. ACRL(green solid line) and

vs.

7. Computational Details

Statistical analysis was performed in R 3.2.2. We used, the package “glm-
net” for penalized regression methods(the Lasso and adaptive Lasso), the
package “gglasso” to perform group Lasso and the package “ClustOfVar” for
clustering of variables.

8. Conclusion and Future Work

In this article, we proposed a three stage procedure for variable selection
for high-dimensional linear model with strongly correlated variables. Our
procedure is an extension of the algorithms proposed in [3]. A technical
extension compared with [3] is that we propose to reduce the dimension at

27

the ﬁrst stage using Lasso(or its adaptive or thresholded version) prior to
clustering at the second stage and then supervised selection of clusters in the
third stage. We proved that the variables selected by our algorithm contains
the true active set consistently under GIR and group beta-min conditions.
Our simulation studies show that reducing dimension improves the speed and
accuracy of the clustering process and then considering correlation structure
improves variable selection and predictive performance.

Since the theoretical results we developed for our algorithms are not re-
stricted to the squared error loss, it can be extended to the generalized linear
models, i.e, the preprocessing step of dimensionality reduction can be added
to the group Lasso for the logistic regression([13]), and this is our future
work.

References

References

[1] Basu, S., Shojaie, A., Michailidis, G., 2015. Network granger causality
with inherent grouping structure. Journal of Machine Learning Research,
417–453.

[2] B¨uhlmann, P., Kalisch, M., Meier, L., 2014. High-dimensional statistics
with a view towards applications in biology. Annual Review of Statistics
and its Applications 1, 255–278.

[3] B¨uhlmann, P., R¨utimann, P., van de Geer, S., Zhang, C.-H., 2012. Cor-
related variables in regression: clustering and sparse estimation. Journal
of Statistical Planning and Inference 143, 1835–1871.

[4] B¨uhlmann, P., van de Geer, S., 2011. Statistics for High-Dimensional

Data: Methods, Theory and Applications. Springer Verlag.

[5] Bunea, F., Lederer, J., She, Y., 2014. The group square-root lasso: The-
oretical properties and fast algorithms. nformation Theory, IEEE Trans-
actions on 60, 1313–1325.

[6] Gauraha, N., 2016. Stability feature selection using cluster representa-
tive lasso. In Proceedings of the 5th International Conference on Pattern
Recognition Applications and Methods, 381–386.

28

[7] H., B., B., R., 2008. Simultaneous regression shrinkage, variable selection

and clustering of predictors with oscar. Biometrics, 115–123.

[8] Hastie, T., Tibshirani, R., Wainwright, M., 2015. Statistical Learning

with Sparsity: The Lasso and Generalizations. CRC Press.

[9] Huang, J., Breheny, P., Ma, S., hui Zhang, C., 2010. The mnet method
for variable selection. Department of Statistics and Actuarial Science,
University of Iowa.

[10] Huangma, J., Zhang, T., 2010. The beneﬁt of group sparsity. Annals of

Statistics 38(4), 1978–2004.

[11] J., H., S, M., H., L., CH., Z., 2011. The sparse laplacian shrinkage
estimator for high-dimensional regression. statistical signal processing,
in SSP09. IEEE/SP 15th Workshop on Statistical Signal Processing,
2021–2046.

[12] Mairal, J., Yu, B., 2012. Complexity analysis of the lasso regulariza-
tion path. Proceedings of the 29th International Conference on Machine
Learning (ICML-12), 353–360.

[13] Meier, L., van de Geer, S., B¨uhlmann, P., 2008. The group lasso for

logistic regression. J. R. Statist. Soc 70, 53–71.

[14] Segal, M., Dahlquist, K., Conklin, B., 2003. Regression approaches for
microarray data analysis. Journal of Computational Biology 10, 961–
980.

[15] Tibshirani, R., 1996. Regression shrinkage and selection via the lasso. J.

R. Statist. Soc 58, 267–288.

[16] Tibshirani, R., Saunders, M., Rosset, S., Zhu, J., Knight, K., 2005. Spar-
sity and smoothness via the fused lasso. Journal of the Royal Statistical
Society Series B, 91–108.

[17] van de Geer, S., B¨uhlmann, P., 2009. On the conditions used to prove
oracle results for the lasso. Electronic Journal of Statistics 3, 1360–1392.

[18] Wei F, H. J., 2010. Consistent group selection in high-dimensional linear

regression. Bernoulli 16, 1369–1384.

29

[19] Yuan, M., Lin, Y., 2007. Model selection and estimation in regression

with grouped variables. J. R. Statist. Soc 68(1), 49–67.

[20] Zhao, P., Yu, B., 2006. On model selection consistency of lasso. Journal

of Machine Learning Research 7, 2541–2563.

[21] Zou, H., Hastie, T., 2005. Regularization and variable selection via the

elastic net. J. R. Statist. Soc 67, 301–320.

30

