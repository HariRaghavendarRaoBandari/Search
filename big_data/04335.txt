6
1
0
2

 
r
a

 

M
4
1

 
 
]
E
S
.
s
c
[
 
 

1
v
5
3
3
4
0

.

3
0
6
1
:
v
i
X
r
a

The landscape of software failure cause models

Lena Feinbube, Peter Tr¨oger, Andreas Polze

Abstract

The software engineering ﬁeld has a long history of classifying software failure
causes. Understanding them is paramount for fault injection, focusing testing
eﬀorts or reliability prediction. Since software fails in manifold complex ways,
a broad range of software failure cause models is meanwhile published in de-
pendability literature. We present the results of a meta-study that classiﬁes
publications containing a software failure cause model in topic clusters. Our
results structure the research ﬁeld and can help to identify gaps. We applied
the systematic mapping methodology for performing a repeatable analysis.

We identiﬁed 156 papers presenting a model of software failure causes. Their
examination conﬁrms the assumption that a large number of the publications
discusses source code defects only. Models of fault-activating state conditions
and error states are rare. Research seems to be driven mainly by the need for
better testing methods and code-based quality improvement. Other motiva-
tions such as online error detection are less frequently given. Mostly, the IEEE
deﬁnitions or orthogonal defect classiﬁcation is used as base terminology. The
majority of use cases comes from web, safety- and security-critical applications.

Keywords:
model

systematic map, mapping study, software, fault, bug, defect,

1. Introduction

More and more aspects of everyday life are supported, enriched, controlled
and managed by software systems. This is not only true for commodity devices,
but increasingly also for safety-critical environments. Examples exist in the
automotive domain, in medical environments and in home automation. This
trend seems to be irreversible, which makes software dependability a problem
of increasing relevance. Understanding and formalizing the underlying causes
and mechanisms leading to software failure is therefore an ever more important
task. Such knowledge can be applied in automated testing, reliability modelling,
developer education, bug statistics or fault tolerance mechanisms.

A practical example for the usefulness of failure cause models is testing by
fault injection. Fault injectors artiﬁcially insert faults and error states into a
running system. This raises the question of which faults to inject, and when.
The related work calls this problem fault representativeness [23]. It demands

Preprint submitted to Journal of Systems and Software

15/03/2016

a well-deﬁned and suﬃciently realistic model of erroneous software states and
their originating causes.

However, an unfortunate characteristic of the ﬁeld of software dependability
is the gap between theoretical research and industrial practice. During co-
operation with diﬀerent industry partners, we have learned that practicioners,
working with complex dependability-critical software systems, are often con-
strained by nondisclosure agreements and therefore reluctant to share their ex-
periences and models of software failures. Due to the realization that every
software ﬂaw poses a threat to security, real world software failure data is usu-
ally not made public. On the other hand, a vast body of academic research has
been published, which attempts to understand and classify the ways in which
software fails.

While this gap between theory and practice is hard to close entirely, the
problem can be mitigated by establishing a common terminology base and im-
proving the accessibility of research results. With this meta-study of software
failure cause models, we strive to structure existing theoretical research and
thus make it better accessible to practicioners.

In our description of the current landscape of published failure cause models,
we collect relevant publications in a repeatable way using the Systematic Map-
ping Study (SMS) methodology. We ﬁrst deﬁne a tag categorization scheme
which can be used to characterize all relevant publications in the ﬁeld. Subse-
quently, the result of our study is a clustering of the research ﬁeld according to
eight diﬀerent categories.

Starting from the fact that failure causes are denoted in diﬀerent ways in
the literature (e.g. “defect model”, “error model”, “bug model”), we ﬁrst deﬁne
a common terminology foundation in Section 2. Subsequently, we present our
primary research questions in Section 3 and the research methodology we applied
in Section 4. The tags and tag categories used for classiﬁcation are presented
in Section 6. Finally, the results are analyzed and discussed in Section 7.

2. Terminology

Models for describing the failure behaviour of software have been widely
discussed in diﬀerent software engineering areas. We experienced that published
models are not only diverse in their purposes and application domains, but
also in the terminology they use. They freely mix up problematic issues of
development, requirements speciﬁcation, and production phase. Static software
artefacts, such as source code and conﬁguration ﬁles, are often treated in the
same way as runtime phenomena leading to fault activation, such as environment
and race conditions. For fruitful discussions of software failure causes, it is
therefore essential to establish a base terminology ﬁrst.
In this section, we
brieﬂy describe the terminology basis we use – presented in more detail in [1].
Our terminology of software failure causes – a term we chose deliberately
to be unambiguous – relies on the terminology model by Laprie/Aviˇzienis . It
is driven by three basic concepts: failure, error, and fault. We use “software

2

failure cause” as an umbrella term for these concepts. With the words of La-
prie/Aviˇzienis [5],

A software failure is an event that occurs when the delivered
service deviates from correct service from user perspective. An er-
ror is that detectable part of the system state that may cause a
subsequent failure. A fault is the adjudged or hypothesized cause
of an error.

While the Laprie/Aviˇzienis terminology is dominant in classical engineering
domains, it seems to have mixed adoption in software-centric dependability
research. Publications in the software engineering domain often have their own
abstraction of error causes and error states. For example, “error” in the IEEE
software engineering glossary speciﬁcation [6] maps to the “fault” concept in
the Laprie/Aviˇzienis model.

In the Laprie/Aviˇzienis model, the term “fault” denotes error-causing in-
ternal structural deﬁciencies and error-causing external inﬂuences at the same
time. We adjust this interpretation in order to tailor it to the world of software
failure causes: We interpret faults only as design imperfections here, similarly
to [16] and [15], by using a slightly modiﬁed version of the fault deﬁnition by
Pretschner et al. [12]:

A software fault is a minimal set of code deviations from correct code, such

that the execution of the deviating code can trigger an error.

We expect the deviation from correct code to be minimal, meaning that all
parts of it are mandatory for having a potential error cause. The triggering of a
transition to an error state is not mandatory, but possible when the according
state conditions are given.

The circumstances of fault activation are widely acknowledged to be crucial,
but somehow form an elusive fuzzy aspect in software. As an extension to
the Laprie/Aviˇzienis terminology, we therefore introduce the notion of fault
activation conditions to better grasp the software-speciﬁc patterns which lead
from a disabled fault to an error state. They depend on the internal system
state only.

Based on the above mentioned terminology, we distinguish between four

types of models:

• The fault model, describing code-based static defects in the program;
• The (fault) activation model, describing the prerequisites for fault ac-

tivation;

• The error model, describing detectable error states in the investigated

system;

• The failure model, describing diﬀerent ways of failure, i.e., externally

visible deviations from the speciﬁed system behaviour.

3

The diﬀerentiation of these models is a fundamental characteristic of our

categorization scheme, as described later in Section 6.

3. Problem Statement and Research Questions

The overall goal of this meta-study is to structure existing research on soft-
ware failure cause models. Such a structuring of existing models is beneﬁcial in
multiple ways: For researchers, we want to provide insights into current trends
and research gaps, to guide further research. Furthermore, as our meta-study is
based on a systematic and exhaustive search of major databases, we ultimately
want to enable researchers to look up publications of their interest based on a
semantic structuring. For practitioners, theoretical work needs to be made more
accessible, hence tag categories (discussed in Section 6) should refer to practical
software engineering aspects.

To structure the broad research ﬁeld of software failure causes, we aimed to

answer the following research questions in our SMS:

Research Question 1. What kind of models – of faults, errors, failures, or
fault activations – are studied and referenced by the scientiﬁc community?

The terminology described in Section 2 and in [1] provides a starting point
to answer this question. Based on the diﬀerent steps or preconditions towards
a software failure, we strive to understand how much and which research eﬀort
is focussed on each step. This question is answered by semantically analyzing
the abstracts of all relevant papers (manually and independently by two of the
authors) and categorizing it into the four classes mentioned above.

Research Question 2. What are the most common terminology models for
describing software problems?

As discussed above in Section 2, we observed that software dependability
research uses various divergent sets of terminology. One goal of our study is
to verify this observation by systematically examining the terminology used in
each publication and tagging each distinct set of terminology we came across
diﬀerently. We thus hope to ﬁnd out which terminology is most popular in
diﬀerent domains of software dependability research.

Research Question 3. How are software problems categorized by related work?

One way of categorizing software failure cause models – as fault, error, fault
activation or failure model – has already been discussed. We are interested
in which other groupings and dimensions are relevant to precisely classify a
software problem. To answer this question, we develop eight tag categories
based on the scanned search result papers.

Research Question 4. What are the focal points in research on software fail-
ure causes? Do research gaps exist?

4

Research on software failure causes is diverse, unstructured and hardly ac-
cessible. Once an understanding of which relevant categories of software fail-
ure causes exist is established, the distribution of research papers across these
categories becomes interesting. We investigate whether certain categories, or
combinations thereof, exhibit eye-catching clusters (trending research topics) or
sparsity (either compelling research gaps, or irrelevant areas).

4. Research Methodology: Systematic Mapping Study

This section brieﬂy describes the systematic mapping process, a research
methodology we applied in the presented work. A more detailed explanation
can be found in the article by Petersen et al. [2].

A Systematic Mapping Study (SMS) is a deﬁned scientiﬁc method to “build
a classiﬁcation scheme and structure a (software engineering) ﬁeld of interest”.
Originating from the medical science domain, the systematic mapping approach
has gained attention also in the science of software engineering, due to the grow-
ing need for structured, evidence based approaches. This approach is applicable
when an area of research has matured to an extent that many diverse publi-
cations exist which need to be structured and summarized, in order to gain
insights into emerging trends and patterns. A systematic map helps in survey-
ing and analysing a broad research ﬁeld of interest – in our case, the entirety of
papers presenting software failure causes.

The SMS comprises the following steps:

1. One or more research questions, reﬂecting the goal of the study, are

deﬁned.

2. A list of relevant databases and publication forums for answering the re-
search questions is compiled. A primary search for relevant papers is
conducted on these databases. The search should be carried out in a sys-
tematic way, using a well deﬁned search string which includes all the
sub-topics of interest. Finding an adequate search string is an iterative
process, it should be repeated and ﬁne tuned to avoid bias.

3. Additional inclusion and exclusion criteria are deﬁned. These criteria
serve the purpose of selecting only papers relevant for answering the re-
search questions, and ﬁltering out publications which do not satisfy certain
formal criteria.

4. Based on search results and the inclusion and exclusion criteria, a ﬁnal
list of publications to be included in the mapping is assembled. For these
papers, a keyword classiﬁcation / tagging of the abstracts takes
place. Each abstract is assigned a number of keywords or tags (the term
we will use subsequently), which describe the publication’s position and
focus with regard to the research questions.

5

5. The tags ﬁnally form the basis of the systematic map, i.e., the data set
containing the relevant tags and the amount of papers assigned to them.
The systematic map can be visualized for instance as a bubble plot, where
trends and hotspots can be identiﬁed. Subsequent to a SMS, the trending
areas in the map can become topics for more detailed systematic literature
reviews [4].

5. Design of the Study

As sources for literature included in the mapping, we searched major on-
line databases and publication platforms used by the software reliability and
software engineering community, as shown in Table 3 (appendix). We omitted
the ResearchGate database, as its focus lies more upon social networking as-
pects and its search features are not powerful enough for our purpose. We also
omitted all public databases where English is not the main language.

Initially, we intended to use one generic search string for all databases, in
order to achieve the best possible comparability of results. This turned out to
be impossible in practice. Since the databases we searched use diﬀerent query
syntax formats of varying expressiveness, we had to express our search objectives
in a tailored way for each of them.

Due to the diverging terminologies representing failure causes, we had to

cover all combinations of the form
{fault|error|defect|bug} + {model|classification|taxonomy}
in the queries. The number of false positives in our search results was therefore
large. Especially the term classification attracted many papers from the
machine learning and data science domains. This was hard to ﬁlter out by tai-
lored queries, since classiﬁcation is a valid term in software engineering research
too. We ended up applying a manual ﬁltering process here.

Furthermore, we manually added publications we deemed relevant. The
manual adding of literature besides using a search string is not unusual, it was
demonstrated for example by Walia et al.
[63] in their systematic literature
review.

We used the group feature of the Zotero bibliography management tool1 for

the purpose of maintaining a collaborative bibliography collection.

5.1. Inclusion and Exclusion Criteria

The inclusion criteria are intended to put focus on the deﬁned research
questions. Each publication in our study, therefore, meets at least one of the
following criteria:

• Presentation of an own classiﬁcation, model or taxonomy of software fail-
ures or failure causes. The paper abstract indicates that an own software
failure cause model might be presented.

1https://www.zotero.org/, 15/03/2016

6

• Extension or critique of an existing classiﬁcation or model of software

failures or failure causes.

• A well-known meta study of software failures or failure causes is referenced

and extended or discussed.

The exclusion criteria, which lead to the ﬁltering out of a paper, were the

following:

• Focus mainly on hardware or hardware description languages.
• Research which is not available in English language.
• Research which is not published in conference, workshop or journal papers,

or as an indexed technical report in one of the searched databases.

The consistent deﬁnition of inclusion/exclusion criteria goes hand in hand
with the formulation of the search string. Both have to be validated in combi-
nation. When starting the study, we had a set of relevant publications in mind
which were known to us. Thus, a ﬁrst validity test for the search string and
the inclusion and exclusion criteria was whether these publications appeared
in the search results. When, in the process of the mapping, through related
work, we identiﬁed further “very relevant” papers, we included them in our set
of “must-have” publications, and re-iterated the search string accordingly.

6. Categorization of Software Failure Cause Models

When discussing a research ﬁeld as broad as that of software dependability,
establishing a common structure is essential. In this section, we contribute a
categorization of software failure cause models.

Based on the scanned papers, our background knowledge, and practical ex-
perience, we therefore deﬁned eight categories which represent relevant aspects
for characterizing software failure cause models. For each category, a number
of tags describing the sub-categories within was deﬁned. This categorization is
the basis for the evaluation of the study’s results, which is presented in Section
7.

6.1. Tag Categories for Software Failure Cause Models

Figure 1 shows our the categorization of tags, which was used to structure

and analyze the research papers.

The Type of Model category is pivotal to our study, as we use it to
apply our terminology foundation to classify each paper with the same (La-
prie/Aviˇzienis -based) vocabulary. We put special emphasis on this tag cate-
gory, because it involves a deeper understanding of the paper than the other
tags, and tagging includes our own interpretation of the contribution of the
individual papers. According to what the model describes, the following tags
exist:

7

Figure 1: Categorization of tag names used in the mapping. We used eight
independent categories of tags, which were established prior to the tagging, after
a preliminary scan of the relevant papers. The tag categories are orthogonal to
each other.

• fault – causes of error states in a program, usually mistakes in the pro-

gram code.

• activations – conditions on the program and environment state, which

must hold so a fault is activated and becomes an error.

• errors – undesired states of the program, which may lead to a failure.
• failures – externally visible deviations from the program’s speciﬁcation.
• meta – description of methodology for obtaining a fault, activation, error

or failure model from software.

The Target category describes what aspect or software artefact is targeted by
the model. Within this category, we use the following tags:

• code – (static) source code, which can be compiled or interpreted and is

thus executable.

• interface – interface(s) between software diﬀerent components.
• documentation – non-executable source code documentation artefacts.
• requirements – assets from the requirements elicitation phase of a soft-

ware development process.

8

• specification – non-executable description (separate from the source

code) of what the desired behaviour is2

• architecture – the higher level structure of software elements and their

interaction patterns.

• test – code or procedures for testing the system functionality.
• configuration – documents and scripts for deployment, compilation or

runtime property conﬁguration.

• human – human behaviour when interacting with the system, also including

usability aspects.

The Purpose category expresses the purpose of the proposed research.
• testing – derive better test cases or testing strategies, improve test cov-

erage.

• prediction – predict the occurrence of further failures, errors, or defect

ﬁndings.

• classification – categorize and classify faults, errors, and defects.
• quality improvement – ﬁnd process modiﬁcations, root cause analyses

or other means to improve overall software quality.

• recovery – derive automatic recovery and/or restoration mechanisms in

the software.

The Domain category describes the main application domain of the model.
Such domains include among others safety(-critical), realtime and embedded.
The Language category is used to tag papers which mainly target a speciﬁc pro-
gramming language. The Paradigm category characterizes the programming
paradigm, if the paper focusses on a speciﬁc one. For instance, we encountered
papers providing a conceptualization of software failure causes, which deals pri-
marily with the imperative, oo or logic programming paradigms.

Finally, we use two tag categories for describing the original terminology
used: Tags from the Naming category answer the question “If a terminology
diverging from the Laprie/Aviˇzienis terminology is used, how are faults named
in the paper?”. We observed that various terms which are used synonymously
for “fault” exist: flaw, error, defect and bug. Tags from the Terminology
category are used to tag papers which are explicitly based upon previous failure
cause terminologies. We take the following base terminologies from literature
into account:

2We are well aware that a debate about the precise boundary between “program” and
“speciﬁcation” exists. We use the term “speciﬁcation” for behaviour descriptions which can
be formal and analysable, but are not intended for actual execution.

9

Traditionally, the Laprie/Aviˇzienis terminology [5], which we also adopt,
has been used for describing both software and hardware systems. Cristian [24]
described a software fault model, which focusses on distributed systems, and has
been used mainly in that domain. Orthogonal Defect Classiﬁcation (ODC) [19]
is an integrated approach to classify software defects in the context of the entire
development process. The IEEE software engineering glossary [6], as already
mentioned, also deﬁnes some vocabulary for describing software failure causes.
Binder [20] published a book on software testing which is includes model of
software faults.

6.2. Consensus on Tags

To decrease the likelihood of mistakes and to increase the independence of
opinions, the two involved reviewers tagged all papers separately and without
discussing them together. After this ﬁrst round of individual tagging, consensus
on the tags was ensured in a joint session.

The “Type of Model” category for clustering the failure cause models is
most pivotal and gives the strongest hint about the focus of the paper, as it
describes which kind of failure cause model is discussed. This category directly
comes from our terminology foundation (see Section 2). Tagging papers in this
category includes a semantic interpretation of the paper content. Therefore, the
goal of the joint session was to agree upon the same set of “Type of Model” tags.
Initially, these sets were diﬀerent for 73 papers, but not completely disjunct.
For example, one reviewer had tagged “model of faults” and the other “model
of faults, model of activations” for the same paper. The joint tagging session
helped to eliminate some accidental mistakes and also sharpened the distinction
between the diﬀerent tags.

Papers discussing usability problems were tagged as “model of failures”, be-
cause usability becomes visible externally to the user, and is thus a deviation
from the (implicit) system speciﬁcation. Papers discussing distributed fault in-
jection were tagged as presenting a “model of faults”, because the failure of a
sub-component becomes a fault, i.e., a cause for an error state, in the investi-
gated distributed system. This corresponds to Laprie/Aviˇzienis ’s concept of
error propagation chain [5]. Papers presenting reliability growth models were
tagged as “model of faults”, because they mathematically model the occurrence
of bugs, which are faults in the source code. Papers with a strong security fo-
cus were hard to categorize, because they often intermingled error states and
fault patterns – both are relevant for discussing potential exploits and attacks.
Therefore, when in doubt, such papers were tagged “model of faults, model of
errors”.

Apart from the “Type of Model” category, the other categories did not
require much discussion. There, the sets of tags, when not identical, were merged
to result in the union of both sets. Commonly, one set of tags was a superset
of the other, so by merging the tags, we included the maximum of information.

10

7. Results

This section presents the SMS results, concentrating on certain points of

interest3 In total, both reviewers provided tags for 156 publications.

Figure 2: Number of papers in our study by publishing year.

As shown in Figure 2, the publishing years of the included publications span
several decades, from 1975 to today. There has been a recent trend towards
more publications in the area. The recent decline of the number of publications,
especially in 2014, might be explained by the fact that not all search indices have
been updated to include the latest publications at the time of search.

Table 1 shows how many of these 156 publications were tagged with one or

multiple tags per category.

All papers

Type
Convention
Terminology Basis
Target
Domain
Purpose
Language

156

148
68
43
124
62
150
28

100%

94.87%
43.59%
27.56%
79.49%
44.23%
96.15%
17.95%

Table 1: Total number and percentage of papers tagged per category.

It has to be noted that the dataset does not strictly consist of nominal
or categorical data: within each category an arbitrary number of tags can be
applied to one paper, i.e., tags are not mutually exclusive within a category.
Therefore, in all subsequent statistics, the total tag count per category not
necessarily sums up to the total number of papers.

3For the complete dataset of citations and tags, refer to https://gist.github.com/laena/

9b514aa89cc0f690a367/download.

11

19751976198019811983198419861987198819891990199119921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014year0246810121416number of publications111111111112414776464681281818121115181511129Figure 3: Tag occurrences within each category. The y axis shows the number
of tag occurrences (from a total of 156 papers).

As mentioned in Section 6.2, we ensured that each paper was tagged with
one or multiple tags describing the type of model, which the reviewers both
consented to. Table 2 shows the references per tag in the “Type of Model”
category.

Figure 3 depicts number of tag occurrences for all tag categories.

Model of... References

faults

[26, 27, 29, 31, 33, 34, 35, 38, 39, 40, 41, 42, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58,

60, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 83, 86, 87, 88, 89, 90, 91,

92, 93, 94, 95, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 110, 111, 113, 114, 115, 116, 117,

118, 119, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138,

139, 142, 143, 144, 146, 147, 150, 151, 153, 155, 157, 158, 159, 160, 161, 162, 163, 164, 166,

169, 170, 171, 172, 173, 174, 175, 177, 178, 180, 181, 182, 183, 185, 186, 187, 188, 189, 190]

[27, 32, 35, 37, 44, 45, 48, 49, 61, 68, 69, 79, 84, 91, 97, 109, 110, 111, 113, 118, 119, 132,

145, 157, 162, 165, 171, 172, 177, 178, 180, 182]

[37, 43, 48, 59, 61, 66, 79, 81, 82, 90, 91, 111, 113, 116, 119, 149, 164, 172]

[91, 96, 99, 100, 102, 118, 123, 126, 141, 147, 149, 156, 157, 161, 162, 167, 168, 174, 175, 179]

[41, 57, 63, 70, 77, 85, 90, 91, 92, 106, 107, 123, 136, 152, 153, 176, 190]

errors

activations
failures
meta

Table 2: References for tag category “Type of Model”.

12

clusterhighavailabilitytelcorealtimeembeddedsecuritysafetywebnone02040608010012014023351214142394domain  -  papers tagged in this facet: 62noneactivationsmetafailureserrorsfaults020406080100120140816171731123model  -  papers tagged in this facet: 148testdatarequirementsdochumanconfigspecsinterfacearchnonecode0204060801001201403469141516303132104target  -  papers tagged in this facet: 124functionalagentlogicaopcomponentimperativeoonone020406080100120140122591022115paradigm  -  papers tagged in this facet: 41phplustreprologjavascriptfortranopenmpadaaspectjeiffelc#c++cjavanone020406080100120140111111222241012129language  -  papers tagged in this facet: 27fault as flawfault as errorfault as bugfault as defectnone020406080100120140214173988naming  -  papers tagged in this facet: 68nonepredictionrecoveryclassificationqualityimprovementtesting02040608010012014061624375366purpose  -  papers tagged in this facet: 150cristianbinderlaprieieeeodcnone0204060801001201402281319113terminology  -  papers tagged in this facet: 43In the following plots, we therefore visualize tag combinations using a cor-
respondence value. The correspondence value is our measure of correlation be-
tween two tags. Consider a set of papers tagged simultaneously with tags A and
B, TA∧B, and the set of papers TA and TB tagged with A and B respectively.
The correspondence value is then computed as follows:

Correspondence =

|TA∧B|
|TA| + |TB| × 2

We scale the value by multiplying it with the number of dimensions considered,
so that the maximum value becomes 1.0. We thus have a relative measure of
how frequently two tags occur together, ranging from 0.0 to 1.0. If all papers
tagged with A were also tagged with B and vice versa, this is the highest possible
correspondence between two tags, with a resulting value of 1.0. We also provide
the absolute occurrence count and its relative frequency with regard to all papers
in the bubble plots.

Figure 4: Illustration of the visualization schema. For each tag combination,
the absolute count and the correspondence value are shown. At the axes, tag
names are annotated with the total number of occurrences of that tag in all
papers.

In our 2D bubble visualizations, we use circles and squares as visualiza-
tion primitives. As an example, consider Figure 4, which shows the entry at
(paradigm:
oo, target: code). The large circle is scaled with the corre-
spondence value describing that a large fraction of the papers tagged “oo” or
“code” was tagged with both these tags. The large square indicates that a large
absolute number of papers was tagged with both tags. The absolute and corre-
spondence values are two diﬀerent views on the data, depending on whether the
focus is on trending research areas with a large number of publications, or on
the correspondence and interdependence between two individual tags.

For the sake of understandability, we omit rows and columns where all tag
combinations have occurred zero times. This means that tags from one di-

13

mension, which have never been used together with any tag from the other
dimension, are not visualized. We justify this omission by arguing that our
sample size is too small to draw meaningful conclusions for tag combinations
which occur very rarely.

7.1. Discussion

In this section, we present selected views on the data using our customized
bubble plot visualization, in order to answer the research questions of our study
(Section 5).

While all 2-combinations of tag categories are certainly worth studying, Fig-
ures 5 to 12 (appendix) show a selection of the plots yielding the most insights.
For ease of reading, the analysis and interpretation per plot is attached to the
particular graph.

Research Question 1 targeted the diﬀerent classes of software failure
causes. Here, it is eye-catching that a majority of the papers – 123 out of
156 publications – discusses a fault model rather than an error, activation or
failure model. This also implies that much research has focussed on static code
features rather than dynamic phenomena such as activation patterns and error
states. The prevalence of “code” in the target category (see Figure 3) conﬁrms
this.

The fact that models of faults, which target code, dominate most of the
publications we studied, also becomes visible in subsequent bubble charts such
as Figure 5. As Figure 6 demonstrates, the majority of publications focusses on
software testing (frequently automatic test case generation) and general quality
improvement.

Research Question 3 aimed at understanding with which patterns the lit-
erature classiﬁes software failure causes. A large portion of the papers we stud-
ied are domain-, language- and/or paradigm-agnostic. The models discussed
therein are so general as to be applicable to diﬀerent technologies.

With Research Question 2, we asked which terminology is most relevant
in the research ﬁeld. A signiﬁcant portion of the papers is based on the rele-
vant basic terminology we identiﬁed during the tagging process, and for which
we created the terminology category. 43 publications explicitly name and cite
another terminology model as the basis of their research (see Figure 3). Among
these papers, the ODC terminology model [19] is clearly the most popular.

Figures 11 and 12 depict how the diﬀerent sets of terminology are used.
Figure 11 illustrates that the Laprie, IEEE and ODC terminologies are popular
for varying sets of targets. As Figure 11 shows, ODC terminology predomi-
nates when code and interface issues are described. On the other hand, IEEE
terminology is used for a broader range of targets, especially for architectural
discussions. Although we have noted that ODC is used primarily to discuss code
features, Figure 12 shows that it can nevertheless serve for models of dynamic
aspects. Failure, error, and fault activation models have also been described
using ODC terminology.

14

We were surprised that the traditional reliability terminologies by Cristian
[5] play only a secondary role when classifying

[9] and Laprie/Aviˇzienis et al.
software failures.

Research Question 4 aimed at identifying trends and research gaps. As
mentioned previously, automated software testing and fault models for this pur-
pose seem to be trending topics in the scientiﬁc community (see Figure 3).

We believe that research gaps exist in the areas of fault activation models and
error models for recovery and prediction (Figures 9 and 6). As fault activation
and potential error states depend largely on the runtime environment, we would
have expected an amount of domain- or language-speciﬁc research in these areas.
However, there is only a handful of such research papers (see Figures 10 and 6).

8. Related Work

Walia et al. [10] have conducted a systematic literature review to describe
and classify requirement errors. One insight from their extensive study of 149
papers provides further motivation for our research. This text uses the IEEE
terminology, i.e., “fault” corresponds to “error” in our sense, and “error” de-
scribes the human action which caused the issue:

While fault classiﬁcation taxonomies have proven beneﬁcial, faults
still occur. Therefore, to provide more insight into the faults, re-
search needs to focus on understanding the sources of the faults
rather than just the faults themselves. In other words, focus on the
errors that caused the faults.

Krsul [11] provides a thorough discussion of software security ﬂaws – mainly
defects/faults in our sense – and derives an own categorization from it. The
work is based on a selection of previous software ﬂaw taxonomies.

Barney et al.

[18] present a systematic map comprising 179 papers on the
topic of trade-oﬀs in software quality. Here, a SMS process with multiple re-
viewers involved is also described. Similarly to our experiences, the authors
noticed that an iterative reﬁning of tag categories and tag names is necessary.
A comparative study of how the SMS process has been conducted with
multiple reviewers, as was the case in this study, is presented in [17]. One result
of this study was that although the SMS process is meant to be formally well
deﬁned, the paper selection and tagging strategies diﬀer across teams. This ﬁts
our impression that there are still some subjective and underspeciﬁed aspects
of the SMS process.

The Common Weakness Enumeration (CWE) [25] is a community eﬀort of
classifying software faults based on taxonomies from the security domain. It
contains anecdotal evidence from failures of real-world software systems. Weak-
nesses are, amongst other categorizations, classiﬁed according to aﬀected pro-
gramming languages, their severity, and the kinds of error states they cause.

15

9. Future Work

The research ﬁeld of software dependability is vast and heterogeneous de-
pending on which purpose or domain is targeted. We believe that more meta-
studies could facilitate understanding and guide the focus of further research.
In particular, follow-up studies should be done to address some further research
questions on the basis of our results:

Foremost, it should be investigated whether the studied models of software
failure causes match observations from real world systems. This clearly demands
for up-to-date failure data about real software failures. The CWE database
could yield suitable starting points for such studies.

Also, the question arises whether current software dependability tools ﬁt
the prominent software failure cause models. Further literature studies can help
to understand the state of the art dependability tools such as static veriﬁers,
test case generators, fault injectors, monitoring and automated error recovery
solutions.

We intend to further research how fault injection approaches can beneﬁt
from the results of our study – especially, how future models focussing on fault
activation conditions can increase fault injection precision and coverage.

10. Conclusion

We presented a categorization of relevant papers and a structuring of pub-
lished software failure cause models. The systematic mapping process allowed
us to identify 156 relevant research articles discussing software failure causes in
a repeatable fashion. This vast amount of available software failure cause mod-
els was structured using eight tag categories and evaluated to observe trending
topics and research gaps.

With the results presented in this study, we hope to encourage further dis-
cussion, especially from practical viewpoints, about whether the landscape of
software failure causes as we have observed it, is accurate. On the basis of
our systematic map, practicioners as well as researchers can identify the rele-
vant existing literature cluster concerning any subﬁeld of software failure cause
modelling and use it to target further eﬀorts.

One striking insight was that the Laprie/Aviˇzienis terminology which stems
from the traditional reliability engineering domain, is not very popular for newer
software-focussed research. Instead, the IEEE deﬁnitions and the ODC termi-
nology concept are used widely, as they provide ﬁne-grained vocabulary for
describing software faults which can be incorporated into the development and
quality assurance process.

Our most crucial observation is that a majority of the research focusses on
fault models by discussing source code defects. The automated software testing
community is well represented. However, there are limits on the coverage of
purely code-based software dependability approaches. Many failures depend on
the timing behaviour, conﬁguration and environment of the system. Certain
bugs do not cause visible failures at once, but lead to error states which can

16

accumulate further. It is widely known that software failures frequently have
complex causes, which has led to the coining of terms such as “Heisenbug”,
“software ageing”, “Mandelbug” and “Schr¨odingbug” [3, 21, 22].

Notwithstanding the common awareness that static fault models do not cap-
ture the details of many software failures, our meta study shows that few models
of fault activation conditions and error states have been presented in the litera-
ture. These models, which take runtime features into account, are mainly used
for error recovery. We conclude that the research on software fault models needs
to be extended to better accommodate dynamic aspects such as fault activation
patterns and error states during runtime.

17

Figure 5: Occurrences of tag combinations from the model and target categories.
This ﬁgure shows that a large number of papers is concerned with fault models
which primarily target code (58% of all papers). Models of errors dealing with
code are also seen frequently, with 22 such papers in our study.
It is also
interesting to note that if a model of activations is discussed, as in 16 out of 156
papers, this model frequently targets the interface. Interpretation: The fact
that the combination “model of faults” and “target is code” occurs frequently
justiﬁes our decision to interpret faults in software as design/code imperfections
(see Section 2). The fact that we excluded papers with a focus on hardware
could explain the rarity of error models targeting data – data corruption is
frequently discussed in hardware fault and error models.

18

faults (123)errors (31)failures (17)activations (16)meta (17)model  -  papers tagged in this facet: 148code (104)interface (30)doc (9)requirements (6)specs (16)arch (31)test (3)config (15)human (14)data (4)target  -  papers tagged in this facet: 1249158.333%0.8022516.026%0.32795.769%0.13663.846%0.0931610.256%0.233019.231%0.3931.923%0.048159.615%0.217127.692%0.17531.923%0.0472214.103%0.32653.205%0.16421.282%0.100.0%0.021.282%0.08563.846%0.19410.641%0.05942.564%0.17431.923%0.13321.282%0.11485.128%0.13231.923%0.12810.641%0.07700.0%0.031.923%0.18221.282%0.08310.641%0.131.923%0.18831.923%0.19410.641%0.095117.051%0.18374.487%0.30400.0%0.000.0%0.010.641%0.06342.564%0.1710.641%0.10531.923%0.19421.282%0.13310.641%0.1127.692%0.19821.282%0.08510.641%0.07700.0%0.021.282%0.12131.923%0.12510.641%0.121.282%0.12521.282%0.12900.0%0.0Figure 6: Occurrences of tag combinations from the model and purpose cate-
gories. Not very surprisingly, fault models are mostly used for testing, quality
improvement and classiﬁcation, whereas error models are frequently used for
error recovery and quality improvement. Interpretation: Testing not only
assumes a fault model, but also a certain kind of erroneous behaviour which the
tests try to detect. However, not much literature deals with testing and error
models at the same time. Here, we see a need for future research.

19

faults (123)errors (31)failures (17)activations (16)meta (17)model  -  papers tagged in this facet: 148testing (66)prediction (16)classification (37)qualityimprovement (53)recovery (24)purpose  -  papers tagged in this facet: 1505837.179%0.614148.974%0.2012918.59%0.3624629.487%0.5231610.256%0.21863.846%0.12421.282%0.08553.205%0.147148.974%0.333117.051%0.442.564%0.09621.282%0.12163.846%0.22263.846%0.17142.564%0.19563.846%0.14610.641%0.06331.923%0.11374.487%0.20331.923%0.1563.846%0.14500.0%0.063.846%0.22242.564%0.11431.923%0.146Figure 7: Occurrences of tag combinations from the target and language cat-
egories. All papers discussing Eiﬀel target the speciﬁcation in their model.
Papers discussing the languages Java and C, on the other hand, are more fo-
cussed on code and architecture. No language-speciﬁc requirements, human or
data aspects obviously have been studied in the investigated set of papers. In-
terpretation: This visualization conﬁrms the validity of the mapped data, as
the results correspond to widely accepted facts about programming languages.
For example, Eiﬀel is well-known for embedding speciﬁcation within the lan-
guage. The code focus in Java and C papers might be due to the fact that a
large amount of bug data exists for these “traditional” programming languages.

20

code (104)interface (30)doc (9)specs (16)arch (31)config (15)target  -  papers tagged in this facet: 124php (1)prolog (1)lustre (1)javascript (1)java (12)fortran (1)c (10)ada (2)aspectj (2)eiffel (2)c# (2)openmp (1)c++ (4)language  -  papers tagged in this facet: 2710.641%0.01910.641%0.01910.641%0.01910.641%0.019106.41%0.17210.641%0.019106.41%0.17521.282%0.03821.282%0.03821.282%0.03821.282%0.03810.641%0.01942.564%0.07410.641%0.06500.0%0.000.0%0.010.641%0.06521.282%0.09500.0%0.010.641%0.0500.0%0.000.0%0.000.0%0.000.0%0.000.0%0.010.641%0.05900.0%0.000.0%0.000.0%0.000.0%0.010.641%0.09500.0%0.010.641%0.10500.0%0.000.0%0.000.0%0.000.0%0.000.0%0.010.641%0.15400.0%0.000.0%0.000.0%0.000.0%0.000.0%0.000.0%0.000.0%0.000.0%0.000.0%0.021.282%0.22200.0%0.000.0%0.000.0%0.000.0%0.000.0%0.000.0%0.000.0%0.042.564%0.18600.0%0.031.923%0.14610.641%0.06121.282%0.12100.0%0.010.641%0.06100.0%0.021.282%0.11400.0%0.000.0%0.000.0%0.000.0%0.010.641%0.07400.0%0.010.641%0.0800.0%0.000.0%0.000.0%0.010.641%0.11800.0%0.010.641%0.105Figure 8: Occurrences of tag combinations from the target and paradigm cat-
egories. Architecture and code are mainly targeted in papers about object
oriented software, whereas papers dealing with component based software fo-
cus more on interface problems. It is also interesting to note that speciﬁcation
is targeted mainly by agent-based and object oriented languages. Interpre-
tation: Interaction is a major aspect of many software problems. Intuitively,
interaction in component based software is deﬁned by the interfaces. The object
oriented community, on the other hand, focusses more on architectural issues as
the root cause for software defects.

21

code (104)interface (30)specs (16)arch (31)config (15)target  -  papers tagged in this facet: 124oo (22)aop (5)functional (1)logic (2)imperative (10)component (9)agent (2)paradigm  -  papers tagged in this facet: 411811.538%0.28642.564%0.07310.641%0.01921.282%0.038106.41%0.17542.564%0.07121.282%0.03800.0%0.000.0%0.000.0%0.000.0%0.000.0%0.053.205%0.25600.0%0.042.564%0.21100.0%0.000.0%0.010.641%0.11100.0%0.010.641%0.0821.282%0.22285.128%0.30231.923%0.16700.0%0.000.0%0.031.923%0.14631.923%0.1500.0%0.010.641%0.05400.0%0.000.0%0.000.0%0.010.641%0.0810.641%0.08300.0%0.0Figure 9: Occurrences of tag combinations from the domain and purpose cate-
gories. Papers dealing with web services frequently develop models for testing
and the classiﬁcation of failure causes. Recovery from errors tends to be studied
primarily in safety-critical and high availability scenarios. Interpretation:
The thorough discussion of error recovery schemes seems worthwhile in scenar-
ios, were error propagation and outage have severe consequences, such as high
availability and safety-critical applications.

22

web (23)embedded (12)telco (3)cluster (2)safety (14)security (14)highavailability (3)realtime (5)domain  -  papers tagged in this facet: 62testing (66)prediction (16)classification (37)qualityimprovement (53)recovery (24)purpose  -  papers tagged in this facet: 150106.41%0.22521.282%0.10374.487%0.23342.564%0.10531.923%0.12853.205%0.12821.282%0.14342.564%0.16363.846%0.18531.923%0.16710.641%0.02900.0%0.021.282%0.100.0%0.010.641%0.07410.641%0.02900.0%0.010.641%0.05100.0%0.000.0%0.053.205%0.12510.641%0.06731.923%0.11853.205%0.14953.205%0.26353.205%0.12510.641%0.06721.282%0.07874.487%0.20910.641%0.05300.0%0.000.0%0.000.0%0.000.0%0.031.923%0.22221.282%0.05600.0%0.010.641%0.04800.0%0.021.282%0.138Figure 10: Occurrences of tag combinations from the model and language cat-
egories. Among the papers concerned with Java, code-based fault models are
primarily studied. On the other hand, C and C++ papers tend to discuss error
states. Interpretation: Due to the complexity of features such as pointer
arithmetic, ﬁnding adequate fault patterns for C/C++ might be hard. In such
languages without automated memory management, studying error states such
as memory corruption might be more relevant than syntactic fault patterns.

23

faults (123)errors (31)failures (17)activations (16)model  -  papers tagged in this facet: 148php (1)prolog (1)lustre (1)javascript (1)java (12)fortran (1)c (10)ada (2)aspectj (2)eiffel (2)c# (2)openmp (1)c++ (4)language  -  papers tagged in this facet: 2710.641%0.01610.641%0.01610.641%0.01610.641%0.016117.051%0.16300.0%0.063.846%0.0921.282%0.03221.282%0.03221.282%0.03221.282%0.03200.0%0.021.282%0.03110.641%0.06300.0%0.000.0%0.010.641%0.06321.282%0.09310.641%0.06342.564%0.19500.0%0.000.0%0.000.0%0.010.641%0.06110.641%0.06331.923%0.17110.641%0.11100.0%0.000.0%0.010.641%0.11110.641%0.06900.0%0.000.0%0.000.0%0.000.0%0.000.0%0.000.0%0.000.0%0.000.0%0.000.0%0.000.0%0.000.0%0.000.0%0.021.282%0.14300.0%0.010.641%0.07700.0%0.000.0%0.000.0%0.010.641%0.11100.0%0.010.641%0.1Figure 11: Occurrences of tag combinations from the terminology and target
categories. The ODC terminology is mainly used to describe code and interface
problems, whereas the usage of the IEEE terminology is more widespread across
targets such as architecture, test and speciﬁcation. Interpretation: ODC
is tailored for code aspects mainly. The code and interface targets are well
covered by ODC, as they are explicitly described by the ODC “Target” and
“Defect Type” categories. Testing and requirements are also discussed in the
ODC papers, but mainly in the context of how a bug can be reproduced or
by mapping code defects to requirement. On the other hand, the IEEE has
standardized terminology not only for software faults, but also for many other
aspects in the whole software development life cycle. This might explain why
the IEEE terminology is used for a broader range of targets.

24

cristian (2)laprie (8)ieee (13)binder (2)odc (19)terminology  -  papers tagged in this facet: 43code (104)interface (30)doc (9)requirements (6)specs (16)arch (31)test (3)config (15)human (14)data (4)target  -  papers tagged in this facet: 12400.0%0.010.641%0.06300.0%0.000.0%0.000.0%0.000.0%0.000.0%0.000.0%0.000.0%0.000.0%0.053.205%0.08931.923%0.15800.0%0.000.0%0.000.0%0.010.641%0.05100.0%0.000.0%0.010.641%0.09100.0%0.0117.051%0.18821.282%0.09321.282%0.18221.282%0.21131.923%0.20763.846%0.27321.282%0.2521.282%0.14300.0%0.000.0%0.021.282%0.03800.0%0.000.0%0.000.0%0.000.0%0.010.641%0.06100.0%0.000.0%0.000.0%0.000.0%0.0159.615%0.24495.769%0.36721.282%0.14300.0%0.000.0%0.010.641%0.0400.0%0.053.205%0.29421.282%0.12110.641%0.087Figure 12: Occurrences of tag combinations from the model and terminology
categories. The diverging sets of terminology appear to be used for diﬀerent
types of models: while ODC and IEEE are strong with fault models, the Laprie
terminology also frequently used for error models, as well as meta studies. ODC
appears to have a broader range of applicability than the IEEE terminology.
Interpretation: The deﬁnitions of error states and failure events in ODC
are based on Laprie, and therefore use a precise state-based deﬁnition. On the
other hand, IEEE deﬁned “fault” (error in our terminology) as a “manifestation
of an error in software”, which is rather vague. It may be the case that IEEE
terminology is tailored mainly to suit mainly static assets.

25

faults (123)errors (31)failures (17)activations (16)meta (17)model  -  papers tagged in this facet: 148cristian (2)laprie (8)ieee (13)binder (2)odc (19)terminology  -  papers tagged in this facet: 4310.641%0.01674.487%0.107138.333%0.19121.282%0.032159.615%0.21100.0%0.031.923%0.15421.282%0.09100.0%0.010.641%0.0400.0%0.010.641%0.0800.0%0.000.0%0.021.282%0.11100.0%0.000.0%0.000.0%0.000.0%0.021.282%0.11400.0%0.021.282%0.1600.0%0.000.0%0.031.923%0.167d
n
a

”
e
l
c
i
t
r
a
“

e
p
y
t

t
n
e
t
n
o
c

o
t

d
e
n

i
a
r
t
s
n
o
c

-
x
e
y
l
l
a
u
n
a
m

,
”
e
c
n
e
i
c
s

r
e
t
u
p
m
o
c
“
e
n

i
l

p

i
c
s
i

d

-
n
w
o
d

e
r
o
f
e
b

s
e
v
i
t
i
s
o
p

e
s
l
a
f

y
n
a
m

d
e
d
u

l
c

s
n
o
i
t
a
t
i
c

g
n

i

d
a
o
l

s
l
a
n
r
u
o
j

d
e
t
a
l
e
r

e
c
n
e
i
c
s

r
e
t
u
p
m
o
c

d
e
t
c
e
l
e
s

”
e
c
n
e
i
c
s

r
e
t
u
p
m
o
c
“

y
r
o
g
e
t
a
c

d
n
a

a
i
r
e
t
i
r
C
r
e
h
t
r
u
F

d
n
a

”
e
l
c
i
t
r
a
“

e
p
y
t

t
n
e
t
n
o
c

o
t

d
e
n

i
a
r
t
s
n
o
c

d
e
d
u

l
c
x
e

,
”
e
c
n
e
i
c
s

r
e
t
u
p
m
o
c
“

e
n

i
l

p

i
c
s
i

d

-
n
o
n

d
e
d
u
l
c
x
e

,
s
e
t
a
c
i
l

p
u
d

d

i
o
v
a

o
t

E
E
E
I

s
l
a
n
r
u
o
j

d
n
a

s
e
c
n
e
r
e
f
n
o
c

d
e
t
a
l
e
r

n
w
o
h
s
(

”
h
t
i
w

t
c
a
r
t
s
b
a

d
n
ﬁ
“

:
s
e
h
c
r
a
e
s

o
w
t

-
l
a
n
a
(

”
e
l
t
i
t

h
t
i
w

d
n
ﬁ
“

d
n
a

)
s
g
n

i
r
t
s

h
c
r
a
e
s

T
O
N

a

r
o
f

t
r
o
p
p
u
s

o
n

s
a
w

e
r
e
h
t

,
)
y
l
s
u
o
g
o

d

l
e
ﬁ

h
c
r
a
e
s

-
b
A

-
b
A

r
o

”
s
t
l
u
a
f

e
r
a
w
t
f
o
s

r
o

”
s
t
c
e
f
e
d

e
r
a
w
t
f
o
s

f
o

f
o

l
e
d
o
m
”
:
t
c
a
r
t
s
b
A

r
o

”
s
r
o
r
r
e

e
r
a
w
t
f
o
s

l
e
d
o
m
”
:
t
c
a
r
t
s
b
A

r
o

”
n
o
i
t
a
c
i
f
i
s
s
a
l
c

g
u
b
”
:
t
c
a
r
t
s
b
A

r
o

r
o
r
r
e
”
:
t
c
a
r
t
s
b
A

r
o

”
y
m
o
n
o
x
a
t

t
l
u
a
f
”
:
t
c
a
r
t
s
b
A

r
o

”
s
g
u
b

e
r
a
w
t
f
o
s

)
”
y
m
o
n
o
x
a
t

g
u
b
”
:
t
c
a
r
t
s
b
A
r
o

”
y
m
o
n
o
x
a
t

t
c
e
f
e
d
”
:
t
c
a
r
t
s
b
A
r
o

”
y
m
o
n
o
x
a
t

f
o

f
o

l
e
d
o
m
”
:
t
c
a
r
t
s

l
e
d
o
m
”
:
t
c
a
r
t
s

t
l
u
a
f
”
R
O
”
l
e
d
o
m
g
u
b
”
R
O
”
l
e
d
o
m

t
c
e
f
e
d
”
R
O
”
l
e
d
o
m

r
o
r
r
e
”
R
O
”
l
e
d
o
m

t
l
u
a
f
”
(
(
(

-
s
a
l
c

g
u
b
”

R
O
”
n
o
i
t
a
c
i
f
i
s
s
a
l
c

t
c
e
f
e
d
”

R
O
”
n
o
i
t
a
c
i
f
i
s
s
a
l
c

r
o
r
r
e
”

R
O
”
n
o
i
t
a
c
i
f
i
s
s
a
l
c

R
O

”
s
r
o
r
r
e

e
r
a
w
t
f
o
s

f
o

l
e
d
o
m
”

R
O

”
s
t
l
u
a
f

e
r
a
w
t
f
o
s

f
o

l
e
d
o
m
”

R
O

”
n
o
i
t
a
c
i
f
i
s

”
y
m
o
n
o
x
a
t

t
l
u
a
f
”

R
O

”
s
g
u
b

e
r
a
w
t
f
o
s

f
o

l
e
d
o
m
”

R
O

”
s
t
c
e
f
e
d

e
r
a
w
t
f
o
s

f
o

l
e
d
o
m
”

-
t
f
o
s
”

D
N
A

)
”
y
m
o
n
o
x
a
t

g
u
b
”

R
O

”
y
m
o
n
o
x
a
t

t
c
e
f
e
d
”

R
O

”
y
m
o
n
o
x
a
t

r
o
r
r
e
”

R
O

D
N
A

”
B
A
L
T
A
M
”

T
O
N

D
N
A

”
t
i
u
c
r
i
c
”

T
O
N

D
N
A

”
e
r
a
w
d
r
a
h
”

T
O
N

D
N
A

”
e
r
a
w

)
)
”
K
N
I
L
U
M
I
S
”
T
O
N

t
l
u
a
f
”
R
O
”
l
e
d
o
m
g
u
b
”
R
O
”
l
e
d
o
m

t
c
e
f
e
d
”
R
O
”
l
e
d
o
m

r
o
r
r
e
”
R
O
”
l
e
d
o
m

t
l
u
a
f
”
(
(

-
s
a
l
c

g
u
b
”

R
O
”
n
o
i
t
a
c
i
f
i
s
s
a
l
c

t
c
e
f
e
d
”

R
O
”
n
o
i
t
a
c
i
f
i
s
s
a
l
c

r
o
r
r
e
”

R
O
”
n
o
i
t
a
c
i
f
i
s
s
a
l
c

R
O

”
s
r
o
r
r
e

e
r
a
w
t
f
o
s

f
o

l
e
d
o
m
”

R
O

”
s
t
l
u
a
f

e
r
a
w
t
f
o
s

f
o

l
e
d
o
m
”

R
O

”
n
o
i
t
a
c
i
f
i
s

”
y
m
o
n
o
x
a
t

t
l
u
a
f
”

R
O

”
s
g
u
b

e
r
a
w
t
f
o
s

f
o

l
e
d
o
m
”

R
O

”
s
t
c
e
f
e
d

e
r
a
w
t
f
o
s

f
o

l
e
d
o
m
”

-
t
f
o
s
”

D
N
A

)
”
y
m
o
n
o
x
a
t

g
u
b
”

R
O

”
y
m
o
n
o
x
a
t

t
c
e
f
e
d
”

R
O

”
y
m
o
n
o
x
a
t

r
o
r
r
e
”

R
O

D
N
A

”
B
A
L
T
A
M
”

T
O
N

D
N
A

”
t
i
u
c
r
i
c
”

T
O
N

D
N
A

”
e
r
a
w
d
r
a
h
”

T
O
N

D
N
A

”
e
r
a
w

)
”
K
N
I
L
U
M
I
S
”
T
O
N

R
O

)
”
l
e
d
o
m

t
c
e
f
e
d
”

R
O

)
”
l
e
d
o
m

r
o
r
r
e
”

R
O

”
l
e
d
o
m

t
l
u
a
f
”
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
:
E
L
T
I
T

-
s
a
l
c

t
c
e
f
e
d
”
R
O
)
”
n
o
i
t
a
c
i
f
i
s
s
a
l
c

r
o
r
r
e
”
R
O
)
”
n
o
i
t
a
c
i
f
i
s
s
a
l
c

t
l
u
a
f
”
R
O
)
”
l
e
d
o
m
g
u
b
”

f
o

l
e
d
o
m
”
R
O
)
”
s
t
l
u
a
f

e
r
a
w
t
f
o
s

f
o

l
e
d
o
m
”
R
O
)
”
n
o
i
t
a
c
i
f
i
s
s
a
l
c

g
u
b
”
R
O
)
”
n
o
i
t
a
c
i
f
i
s

)
”
s
g
u
b

e
r
a
w
t
f
o
s

f
o

l
e
d
o
m
”

R
O
)
”
s
t
c
e
f
e
d

e
r
a
w
t
f
o
s

f
o

l
e
d
o
m
”

R
O
)
”
s
r
o
r
r
e

e
r
a
w
t
f
o
s

g
u
b
”

R
O

)
”
y
m
o
n
o
x
a
t

t
c
e
f
e
d
”

R
O

)
”
y
m
o
n
o
x
a
t

r
o
r
r
e
”

R
O

)
”
y
m
o
n
o
x
a
t

t
l
u
a
f
”

R
O

r
o
r
r
e
”

R
O
”
l
e
d
o
m

t
l
u
a
f
”
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
:
C
I
P
O
T

R
O
)
”
e
r
a
w
t
f
o
s
”

D
N
A

)
”
y
m
o
n
o
x
a
t

-
r
e
”
R
O
)
”
n
o
i
t
a
c
i
f
i
s
s
a
l
c

t
l
u
a
f
”
R
O
)
”
l
e
d
o
m
g
u
b
”
R
O
)
”
l
e
d
o
m

t
c
e
f
e
d
”
R
O
)
”
l
e
d
o
m

l
e
d
o
m
”
R
O
)
”
n
o
i
t
a
c
i
f
i
s
s
a
l
c

g
u
b
”
R
O
)
”
n
o
i
t
a
c
i
f
i
s
s
a
l
c

t
c
e
f
e
d
”
R
O
)
”
n
o
i
t
a
c
i
f
i
s
s
a
l
c

r
o
r

-
e
d

e
r
a
w
t
f
o
s

f
o

l
e
d
o
m
”

R
O

)
”
s
r
o
r
r
e

e
r
a
w
t
f
o
s

f
o

l
e
d
o
m
”

R
O

)
”
s
t
l
u
a
f

e
r
a
w
t
f
o
s

f
o

)
”
y
m
o
n
o
x
a
t

r
o
r
r
e
”
R
O
)
”
y
m
o
n
o
x
a
t

t
l
u
a
f
”
R
O
)
”
s
g
u
b
e
r
a
w
t
f
o
s

f
o
l
e
d
o
m
”
R
O
)
”
s
t
c
e
f

)
”
e
r
a
w
t
f
o
s
”
D
N
A
)
”
y
m
o
n
o
x
a
t

g
u
b
”
R
O
)
”
y
m
o
n
o
x
a
t

t
c
e
f
e
d
”
R
O

t
l
u
a
f
”
R
O
”
l
e
d
o
m
g
u
b
”
R
O
”
l
e
d
o
m

t
c
e
f
e
d
”
R
O
”
l
e
d
o
m

r
o
r
r
e
”
R
O
”
l
e
d
o
m

t
l
u
a
f
”
(
(
(

-
s
a
l
c

g
u
b
”

R
O
”
n
o
i
t
a
c
i
f
i
s
s
a
l
c

t
c
e
f
e
d
”

R
O
”
n
o
i
t
a
c
i
f
i
s
s
a
l
c

r
o
r
r
e
”

R
O
”
n
o
i
t
a
c
i
f
i
s
s
a
l
c

R
O

”
s
r
o
r
r
e

e
r
a
w
t
f
o
s

f
o

l
e
d
o
m
”

R
O

”
s
t
l
u
a
f

e
r
a
w
t
f
o
s

f
o

l
e
d
o
m
”

R
O

”
n
o
i
t
a
c
i
f
i
s

”
y
m
o
n
o
x
a
t

t
l
u
a
f
”

R
O

”
s
g
u
b

e
r
a
w
t
f
o
s

f
o

l
e
d
o
m
”

R
O

”
s
t
c
e
f
e
d

e
r
a
w
t
f
o
s

f
o

l
e
d
o
m
”

-
t
f
o
s
”

D
N
A

)
”
y
m
o
n
o
x
a
t

g
u
b
”

R
O

”
y
m
o
n
o
x
a
t

t
c
e
f
e
d
”

R
O

”
y
m
o
n
o
x
a
t

r
o
r
r
e
”

R
O

D
N
A

”
B
A
L
T
A
M
”

T
O
N

D
N
A

”
t
i
u
c
r
i
c
”

T
O
N

D
N
A

”
e
r
a
w
d
r
a
h
”

T
O
N

D
N
A

”
e
r
a
w

)
)
”
K
N
I
L
U
M
I
S
”
T
O
N

g
u
b
”

R
O

)
”
l
e
d
o
m

t
c
e
f
e
d
”

R
O

)
”
l
e
d
o
m

r
o
r
r
e
”

R
O

”
l
e
d
o
m

t
l
u
a
f
”
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(
(

-
i
s
s
a
l
c

t
c
e
f
e
d
”

R
O

)
”
n
o
i
t
a
c
i
f
i
s
s
a
l
c

r
o
r
r
e
”

R
O

)
”
n
o
i
t
a
c
i
f
i
s
s
a
l
c

t
l
u
a
f
”

R
O

)
”
l
e
d
o
m

f
o

l
e
d
o
m
”

R
O
)
”
s
t
l
u
a
f

e
r
a
w
t
f
o
s

f
o

l
e
d
o
m
”

R
O
)
”
n
o
i
t
a
c
i
f
i
s
s
a
l
c

g
u
b
”

R
O
)
”
n
o
i
t
a
c
i
f

)
”
s
g
u
b

e
r
a
w
t
f
o
s

f
o

l
e
d
o
m
”

R
O
)
”
s
t
c
e
f
e
d

e
r
a
w
t
f
o
s

f
o

l
e
d
o
m
”

R
O
)
”
s
r
o
r
r
e

e
r
a
w
t
f
o
s

g
u
b
”

R
O

)
”
y
m
o
n
o
x
a
t

t
c
e
f
e
d
”

R
O

)
”
y
m
o
n
o
x
a
t

r
o
r
r
e
”

R
O

)
”
y
m
o
n
o
x
a
t

t
l
u
a
f
”

R
O

)
”
e
r
a
w
t
f
o
s
”
D
N
A
)
”
y
m
o
n
o
x
a
t

”
l
e
d
o
m

r
o
r
r
e
”
:
t
c
a
r
t
s
b
A

r
o

”
l
e
d
o
m

t
l
u
a
f
”
:
t
c
a
r
t
s
b
A
(

d
n
a

)
”
e
r
a
w
t
f
o
s
”
:
t
c
a
r
t
s
b
A
(

-
s
a
l
c

t
l
u
a
f
”
:
t
c
a
r
t
s
b
A

r
o

”
l
e
d
o
m

g
u
b
”
:
t
c
a
r
t
s
b
A

r
o

”
l
e
d
o
m

t
c
e
f
e
d
”
:
t
c
a
r
t
s
b
A

r
o

”
n
o
i
t
a
c
i
f
i
s
s
a
l
c

t
c
e
f
e
d
”
:
t
c
a
r
t
s
b
A

r
o

”
n
o
i
t
a
c
i
f
i
s
s
a
l
c

r
o
r
r
e
”
:
t
c
a
r
t
s
b
A

r
o

”
n
o
i
t
a
c
i
f
i
s

6
4
4

r
e
g
n
i
r
p
S

8
5
2

t
c
e
r
i
D
e
c
n
e
i
c
S

4
1
1

e
c
n
e
i
c
S
f
O
b
e
W

6

g
r
o
.
v
i
x
r
A

2
4

M
C
A

6
2
3

E
E
E
I

26

g
n
i
r
t
S

h
c
r
a
e
S

s
t
l
u
s
e
R

e
s
a
b
a
t
a
D

y
n
a
M

.

h
c
r
a
e
s

y
r
a
m

i
r
p

g
n

i
r
u
d

d
n
u
o
f

s
t
l
u
s
e
r

f
o

r
e
b
m
u
n

e
h
t

d
n
a

,
s

m
r
o
f
t
a
l

p

d
n
a

s
e
s
a
b
a
t
a
d

s
u
o
i
r
a
v

r
o
f

s
g
n
i
r
t
s

h
c
r
a
e
S

:
3

e
l
b
a
T

k
o
o
t

s
e
s
a
b
a
t
a
d

e
h
t

f
o

g
n

i

h
c
r
a
e
s

l
a
i
t
i

n

i

e
h
T

.
r
e
t
a
l

y
d
u
t
s

r
u
o

o
t

t
n
a
v
e
l
e
r
r
i

r
o

s
e
t
a
c
i
l

p
u
d

s
a

d
e
d
r
a
c
s
i
d

e
r
e
w
s
n
o
i
t
a
c
i
l
b
u
p

e
s
e
h
t

f
o

.
4
1
0
2

,
2
2

y
l
u
J

n
o

e
c
a
l

p

References

References

[1] P. Tr¨oger, L. Feinbube, M. Werner, What activates a bug? a reﬁnement
of the laprie terminology model, in: Software Reliability Engineering (IS-
SRE), 2015 IEEE 26th International Symposium on, IEEE, 2015.

[2] K. Petersen, R. Feldt, S. Mujtaba, M. Mattsson, Systematic mapping
studies in software engineering, in: 12th International Conference on Eval-
uation and Assessment in Software Engineering, Vol. 17, sn, 2008.

[3] K. Trivedi, M. Grottke, Fighting bugs:

remove,

and rejuvenate, Computer 40 (2) (2007) 107–109.
ieeecomputersociety.org/10.1109/MC.2007.55.

retry,

replicate,
doi:http://doi.

[4] S. Keele, Guidelines for performing systematic literature reviews in soft-
ware engineering, Tech. rep., Technical report, EBSE Technical Report
EBSE-2007-01 (2007).

[5] A. Avizienis, J.-C. Laprie, B. Randell, C. Landwehr, Basic concepts and
taxonomy of dependable and secure computing, Dependable and Secure
Computing, IEEE Transactions on 1 (1) (2004) 11–33.

[6] J. Radatz, A. Geraci, F. Katki, Ieee standard glossary of software engi-

neering terminology, IEEE Std 610121990 (121990) (1990) 3.

[7] F. Lanubile, F. Shull, V. R. Basili, Experimenting with error abstraction in
requirements documents, in: Software Metrics Symposium, 1998. Metrics
1998. Proceedings. Fifth International, IEEE, 1998, pp. 114–121.

[8] J. C. Munson, A. P. Nikora, Toward a quantiﬁable deﬁnition of software
faults, in: Software Reliability Engineering, 2002. ISSRE 2003. Proceed-
ings. 13th International Symposium on, IEEE, 2002, pp. 388–395.

[9] F. Cristian, Exception handling, IBM Thomas J. Watson Research Divi-

sion, 1987.

[10] G. S. Walia, J. C. Carver, A systematic literature review to identify and
classify software requirement errors, Inf. Softw. Technol. 51 (7) (2009)
1087–1109. doi:10.1016/j.infsof.2009.01.004.
URL http://dx.doi.org/10.1016/j.infsof.2009.01.004

[11] I. V. Krsul, Software vulnerability analysis, Ph.D. thesis, Purdue Univer-

sity (1998).

[12] A. Pretschner, D. Holling, R. Eschbach, M. Gemmar, A generic fault
model for quality assurance, in: Model-Driven Engineering Languages
and Systems, Springer, 2013, pp. 87–103.

27

[13] J.-C. Laprie, A. Avizienis, H. Kopetz (Eds.), Dependability: Basic Con-

cepts and Terminology, Springer-Verlag, Secaucus, NJ, USA, 1992.

[14] A. Avizienis, J.-C. Laprie, B. Randell, Fundamental Concepts of Depend-

ability, Tech. Rep. CSD Report 010028, UCLA (2001).

[15] J. Laski, Programming faults and errors: Towards a theory of software

incorrectness, Ann. Softw. Eng. 4 (1-4) (1997) 79–114.
URL http://dl.acm.org/citation.cfm?id=590565.590590

[16] M. Grottke, R. M. Jr., K. S. Trivedi, The Fundamentals of Software Ag-
ing, in: Proceedings of the 19th International Symposium on Software
Reliability Engineering, 2008.

[17] K. Petersen, N. Ali, Identifying strategies for study selection in systematic
reviews and maps, in: Empirical Software Engineering and Measurement
(ESEM), 2011 International Symposium on, 2011, pp. 351–354. doi:
10.1109/ESEM.2011.46.

[18] S. Barney, K. Petersen, M. Svahnberg, A. Aurum, H. Barney, Software
quality trade-oﬀs: A systematic map, Inf. Softw. Technol. 54 (7) (2012)
651–662. doi:10.1016/j.infsof.2012.01.008.
URL http://dx.doi.org/10.1016/j.infsof.2012.01.008

[19] R. Chillarege, I. S. Bhandari, J. K. Chaar, M. J. Halliday, D. S. Moebus,
B. K. Ray, M.-Y. Wong, Orthogonal defect classiﬁcation-a concept for
in-process measurements, Software Engineering, IEEE Transactions on
18 (11) (1992) 943–956.

[20] R. Binder, Testing object-oriented systems: models, patterns, and tools,

Addison-Wesley Professional, 2000.

[21] J. Gray, Why do computers stop and what can be done about it?, in:
Symposium on reliability in distributed software and database systems,
Los Angeles, CA, USA, 1986, pp. 3–12.

[22] D. L. Parnas, Software aging, in: Proceedings of the 16th international
conference on Software engineering, IEEE Computer Society Press, 1994,
pp. 279–287.

[23] R. Natella, D. Cotroneo, J. A. Duraes, H. S. Madeira, On fault represen-
tativeness of software fault injection, IEEE Transactions on Software En-
gineering 39 (1) (2013) 80–96. doi:http://doi.ieeecomputersociety.
org/10.1109/TSE.2011.124.

[24] F. Cristian, Understanding fault-tolerant distributed systems, Communi-

cations of The ACM 34 (1991) 56–78. doi:10.1145/102792.102801.

[25] S. Christey, R. Glenn, et al., Common weakness enumeration (2013).

28

[26] B. K. Aichernig, H. Jifeng, Mutation testing in UTP, Formal Aspects of

Computing 21 (1-2) (2009) 33–64. doi:10.1007/s00165-008-0083-6.

[27] A. B. Aissa, R. K. Abercrombie, F. T. Sheldon, A. Mili, Quantifying
security threats and their potential impacts: a case study, Innovations in
Systems and Software Engineering 6 (4) (2010) 269–281. doi:10.1007/
s11334-010-0123-2.

[28] N. Alaeddine, J. Tian, Analytic Model for Web Anomalies Classiﬁcation,

2007, pp. 395–396. doi:10.1109/HASE.2007.13.

[29] R. Alexander, J. Oﬀutt, J. Bieman, Syntactic fault patterns in OO pro-

grams, 2002, pp. 193–202. doi:10.1109/ICECCS.2002.1181512.

[30] O. Alshathry, H. Janicke, H. Zedan, A. Alhussein, Quantitative Quality
Assurance Approach, 2009, pp. 405–408. doi:10.1109/NISS.2009.114.

[31] H. Ammar, S. Yacoub, A. Ibrahim, A fault model for fault injection analy-
sis of dynamic UML speciﬁcations, 2001, pp. 74–83. doi:10.1109/ISSRE.
2001.989460.

[32] T. Anderson, J. Knight, A Framework for Software Fault Tolerance in
Real-Time Systems, IEEE Transactions on Software Engineering SE-9 (3)
(1983) 355–364. doi:10.1109/TSE.1983.237017.

[33] E. Arisholm, L. C. Briand, E. B. Johannessen, A systematic and com-
prehensive investigation of methods to build and evaluate fault pre-
diction models, Journal of Systems and Software 83 (1) (2010) 2–17.
doi:10.1016/j.jss.2009.06.055.

[34] T. Aslam, A taxonomy of security faults in the unix operating system,

Ph.D. thesis, Purdue University (1995).

[35] A. Aviienis, Approaches to Computer Reliability: Then and Now, AFIPS
doi:10.1145/

’76, ACM, New York, NY, USA, 1976, pp. 401–411.
1499799.1499859.

[36] B. Ayeb, A. Farhat, A ﬂexible formal framework for masking/demasking
faults, Information Sciences 159 (12) (2004) 29–52. doi:10.1016/j.ins.
2003.03.004.

[37] H. Aysan, S. Punnekkat, R. Dobrin, Error Modeling in Depend-
able Component-Based Systems, 2008, pp. 1309–1314. doi:10.1109/
COMPSAC.2008.99.

[38] C. Babu, H. R. Krishnan, Fault Model and Test-case Generation for the
Composition of Aspects, SIGSOFT Softw. Eng. Notes 34 (1) (2009) 1–6.
doi:10.1145/1457516.1457521.

29

[39] A. Bartel, B. Baudry, F. Munoz, J. Klein, T. Mouelhi, Y. Le-Traon, Model
Driven Mutation Applied to Adaptative Systems Testing, 2011, pp. 408–
413. doi:10.1109/ICSTW.2011.24.

[40] V. R. Basili, R. W. Selby, Comparing the Eﬀectiveness of Software Testing
Strategies, IEEE Transactions on Software Engineering 13 (12) (1987)
1278–1296. doi:10.1109/TSE.1987.232881.

[41] F. Belli, R. Crisan, Towards automation of checklist-based code-reviews,
, Seventh International Symposium on Software Reliability Engi-
in:
neering, 1996. Proceedings, 1996, pp. 24–33. doi:10.1109/ISSRE.1996.
558687.

[42] K. L. Bellman, The modeling issues inherent in testing and evaluating
knowledge-based systems, Expert Systems with Applications 1 (3) (1990)
199–215. doi:10.1016/0957-4174(90)90002-C.

[43] Y. Ben-Asher, Y. Eytani, E. Farchi, S. Ur, Noise Makers Need to Know
Where to be Silent #150; Producing Schedules That Find Bugs, 2006, pp.
458–465. doi:10.1109/ISoLA.2006.18.

[44] P. Black, Counting Bugs is Harder Than You Think, 2011, pp. 1–9. doi:

10.1109/SCAM.2011.24.

[45] E. Borin, C. Wang, Y. Wu, G. Araujo, Software-Based Transparent and
Comprehensive Control-Flow Error Detection,
in: Proceedings of the
International Symposium on Code Generation and Optimization, CGO
’06, IEEE Computer Society, Washington, DC, USA, 2006, pp. 333–345.
doi:10.1109/CGO.2006.33.

[46] G. Bougie, C. Treude, D. German, M. Storey, A comparative exploration
of FreeBSD bug lifetimes, in: 2010 7th IEEE Working Conference on
Mining Software Repositories (MSR), 2010, pp. 106–109. doi:10.1109/
MSR.2010.5463291.

[47] J. B. Bowen, Standard Error Classiﬁcation to Support Software Reliability
Assessment, AFIPS ’80, ACM, New York, NY, USA, 1980, pp. 697–705.
doi:10.1145/1500518.1500638.

[48] M. Bozzano, A. Cimatti, J.-P. Katoen, V. Y. Nguyen, T. Noll, M. Roveri,
Safety, Dependability and Performance Analysis of Extended AADL Mod-
els, Computer Journal 54 (5) (2011) 754–775, wOS:000290315200010.
doi:10.1093/comjnl/bxq024.

[49] S. Bruning, S. Weissleder, M. Malek, A Fault Taxonomy for Service-
Oriented Architecture, 2007, pp. 367–368. doi:10.1109/HASE.2007.46.

[50] M. Bruntink, A. van Deursen, T. Tourw, Discovering Faults in Idiom-
based Exception Handling, ICSE ’06, ACM, New York, NY, USA, 2006,
pp. 242–251. doi:10.1145/1134285.1134320.

30

[51] J. Bsekken, R. Alexander, A Candidate Fault Model for AspectJ Point-

cuts, 2006, pp. 169–178. doi:10.1109/ISSRE.2006.6.

[52] M. Buchler, Security Testing with Fault-Models and Properties, 2013, pp.

501–502. doi:10.1109/ICST.2013.74.

[53] A. B. Can, T. Bultan, M. Lindvall, B. Lux, S. Topp, Eliminating synchro-
nization faults in air traﬃc control software via design for veriﬁcation with
concurrency controllers, Automated Software Engineering 14 (2) (2007)
129–178, wOS:000249586000002. doi:10.1007/s10515-007-0008-2.

[54] C.-P. Chang, C.-P. Chu, Defect prevention in software processes: An
action-based approach, Journal of Systems and Software 80 (4) (2007)
559–570. doi:10.1016/j.jss.2006.09.009.

[55] I. Ciupa, B. Meyer, M. Oriol, A. Pretschner, Finding Faults: Manual
Testing vs. Random+ Testing vs. User Reports, 2008, pp. 157–166. doi:
10.1109/ISSRE.2008.18.

[56] I. Ciupa, A. Pretschner, M. Oriol, A. Leitner, B. Meyer, On the num-
ber and nature of faults found by random testing, Software Testing
Veriﬁcation & Reliability 21 (1) (2011) 3–28, wOS:000287607500002.
doi:10.1002/stvr.415.

[57] V. Claesson, S. Poledna, J. Soderberg, The XBW model for depend-
able real-time systems, 1998, pp. 130–138. doi:10.1109/ICPADS.1998.
741030.

[58] J. S. Collofello, L. B. Balcom, A proposed causative software error classi-
ﬁcation scheme, Department of Computer Science, College of Engineering
& Applied Sciences, Arizona State University, 1984.

[59] A. Da Silva, A. Gonzalez-Calero, J.-F. Martinez, L. Lopez, A.-B. Gar-
cia, V. Hernandez, Design and Implementation of a Java Fault Injec-
tor for Exhaustif #x0ae; SWIFI Tool, 2009, pp. 77–83. doi:10.1109/
DepCoS-RELCOMEX.2009.27.

[60] L.-O. Damm, L. Lundberg, Identiﬁcation of test process improvements by
combining fault trigger classiﬁcation and faults-slip-through measurement,
2005, pp. 10 pp.–. doi:10.1109/ISESE.2005.1541824.

[61] V. Darmaillacq, Security policy testing using vulnerability exploit chain-

ing, 2008, pp. 260–261. doi:10.1109/ICSTW.2008.37.

[62] R. de Lemos, On Architecting Software Fault Tolerance using Abstrac-
tions, Electronic Notes in Theoretical Computer Science 236 (2009) 21–32.
doi:10.1016/j.entcs.2009.03.012.

[63] S. Doan, A. Betin-Can, V. Garousi, Web application testing: A systematic
literature review, Journal of Systems and Software 91 (2014) 174–201.
doi:10.1016/j.jss.2014.01.010.

31

[64] R. J. Drebes, G. Jacques-Silva, J. M. Fonseca da Trindade, T. S. We-
ber, A kernel-based communication fault injector for dependability test-
ing of distributed systems, in: S. Ur, E. Bin, Y. Wolfsthal (Eds.), Hard-
ware and Software Veriﬁcation and Testing, Vol. 3875, 2006, pp. 177–190,
wOS:000237451000013.

[65] L. du Bousquet, M. Delaunay, Mutation analysis for Lustre programs:
Fault model description and validation, 2007, pp. 176–184. doi:10.1109/
TAIC.PART.2007.27.

[66] W. Du, A. Mathur, Testing for software vulnerability using environment

perturbation, 2000, pp. 603–612. doi:10.1109/ICDSN.2000.857596.

[67] T. Dumitra, P. Narasimhan, Why Do Upgrades Fail and What Can We Do
About It?: Toward Dependable, Online Upgrades in Enterprise System,
Middleware ’09, Springer-Verlag New York, Inc., New York, NY, USA,
2009, pp. 18:1–18:20.

[68] A. Duran, R. Ferrer, J. J. Costa, M. Gonzlez, X. Martorell, E. Ayguad,
J. Labarta, A Proposal for Error Handling in OpenMP, International
Journal of Parallel Programming 35 (4) (2007) 393–416. doi:10.1007/
s10766-007-0049-y.

[69] M. Eisenstadt, My hairiest bug war stories, Communications of the ACM

40 (4) (1997) 30–37. doi:10.1145/248448.248456.

[70] K. El Emam, I. Wieczorek, The repeatability of code defect classiﬁcations,

1998, pp. 322–333. doi:10.1109/ISSRE.1998.730897.

[71] S. G. Elbaum, J. C. Munson, Software Evolution and the Code Fault
Introduction Process, Empirical Software Engineering 4 (3) (1999) 241–
262. doi:10.1023/A:1009830727593.

[72] A. En-Nouaary, F. Khendek, R. Dssouli, Fault coverage in testing real-

time systems, 1999, pp. 150–157. doi:10.1109/RTCSA.1999.811206.

[73] A. Endres, An analysis of errors and their causes in system programs,
IEEE Transactions on Software Engineering SE-1 (2) (1975) 140–149.
doi:10.1109/TSE.1975.6312834.

[74] W. M. Evanco, J. Verner, Some architectural features of Ada systems
aﬀecting defects, in: J. P. Rosen, A. Strohmeier (Eds.), Reliable Soft-
ware Technologies - Ada-Europe 2003, Vol. 2655, 2003, pp. 232–245,
wOS:000184834100017.

[75] E. Farchi, Y. Nir, S. Ur, Concurrent bug patterns and how to test them,

2003, pp. 7 pp.–. doi:10.1109/IPDPS.2003.1213511.

32

[76] F. Ferrari, R. Burrows, O. Lemos, A. Garcia, J. Maldonado, Character-
ising Faults in Aspect-Oriented Programs: Towards Filling the Gap Be-
tween Theory and Practice, 2010, pp. 50–59. doi:10.1109/SBES.2010.
11.

[77] B. Freimut, C. Denger, M. Ketterer, An industrial case study of imple-
menting and validating defect classiﬁcation for process improvement and
quality management, 2005, pp. 10 pp.–19. doi:10.1109/METRICS.2005.
10.

[78] T. Furuyama, Y. Arai, K. Iio, Analysis of fault generation caused by stress
during software development, Journal of Systems and Software 38 (1)
(1997) 13–25. doi:10.1016/S0164-1212(97)00064-2.

[79] D. Gamrad, H. Oberheid, D. Soﬀker, Formalization and automated de-
tection of human errors, 2008, pp. 1761–1766. doi:10.1109/SICE.2008.
4654949.

[80] C. Gao, M. Duan, L. Tan, Y. Gong, Out-of-bounds array access fault
model and automatic testing method study, Tsinghua Science and Tech-
nology 12 (S1) (2007) 14–19. doi:10.1016/S1007-0214(07)70077-9.

[81] A. Gario, A. M. Andrews, Fail-Safe Testing of Safety-Critical Systems,

2014, pp. 190–199. doi:10.1109/ASWEC.2014.19.

[82] J. C. Godskesen, Fault models for embedded systems,

in: L. Pierre,
T. Kropf (Eds.), Correct Hardware Design and Veriﬁcation Methods, Vol.
1703, 1999, pp. 354–359, wOS:000088608500033.

[83] Y. Gong, W. Xu, X. Li, An expression’s single fault model and the testing

methods, 2003, pp. 110–113. doi:10.1109/ATS.2003.1250793.

[84] H. Gong, J. Li, Generating Test Cases of Object-Oriented Software Based
on EDPN and Its Mutant, in: Young Computer Scientists, 2008. ICYCS
2008. The 9th International Conference for, 2008, pp. 1112–1119. doi:
10.1109/ICYCS.2008.224.

[85] J.-J. Gras, End-to-end defect modeling, IEEE Software 21 (5) (2004) 98–

100. doi:10.1109/MS.2004.1331312.

[86] D. B. Hanchate, S. Sayyad, S. Shinde, Defect classiﬁcation as problem clas-
siﬁcation for quality control in the software project management by DTL,
Vol. 7, 2010, pp. V7–623–V7–627. doi:10.1109/ICCET.2010.5485644.

[87] M. J. Harrold, A. J. Oﬀutt, K. Tewary, An approach to fault modeling and
fault seeding using the program dependence graph, Journal of Systems and
Software 36 (3) (1997) 273–295. doi:10.1016/S0164-1212(96)00175-6.

[88] J. Hayes, Building a requirement fault taxonomy: experiences from a
NASA veriﬁcation and validation research project, 2003, pp. 49–59. doi:
10.1109/ISSRE.2003.1251030.

33

[89] J. Hayes, I. Raphael, E. Holbrook, D. Pruett, A case history of In-
ternational Space Station requirement faults, 2006, pp. 10 pp.–. doi:
10.1109/ICECCS.2006.1690351.

[90] J.-Z. He, Z.-H. Zhou, Z.-H. Zhao, S.-F. Chen, A general design technique
for fault diagnostic systems, Vol. 2, 2001, pp. 1307–1311 vol.2. doi:
10.1109/IJCNN.2001.939550.

[91] H. Hecht, A proposal for standardized software dependability data, 1995,

pp. 235–243. doi:10.1109/SESS.1995.525969.

[92] D. Holling, A Fault Model Framework for Quality Assurance, 2014, pp.

235–236. doi:10.1109/ICSTW.2014.44.

[93] L. Huang, V. Ng, I. Persing, R. Geng, X. Bai, J. Tian, AutoODC: Auto-
mated Generation of Orthogonal Defect Classiﬁcations, in: Proceedings of
the 2011 26th IEEE/ACM International Conference on Automated Soft-
ware Engineering, ASE ’11, IEEE Computer Society, Washington, DC,
USA, 2011, pp. 412–415. doi:10.1109/ASE.2011.6100086.

[94] Z. Hui, S. Huang, B. Hu, Z. Ren, A taxonomy of software security defects

for SST, 2010, pp. 99–103. doi:10.1109/ICISS.2010.5656736.

[95] U. Hunny, M. Zulkernine, K. Weldemariam, OSDC: Adapting ODC for
Developing More Secure Software, SAC ’13, ACM, New York, NY, USA,
2013, pp. 1131–1136. doi:10.1145/2480362.2480574.

[96] T. Huynh, J. Miller, Another viewpoint on evaluating web software re-
liability based on workload and failure data extracted from server logs,
Empirical Software Engineering 14 (4) (2009) 371–396. doi:10.1007/
s10664-008-9084-6.

[97] N. Ignat, B. Nicolescu, Y. Savaria, G. Nicolescu, Soft-error classiﬁcation
and impact analysis on real-time operating systems, Vol. 1, 2006, pp. 6
pp.–. doi:10.1109/DATE.2006.244063.

[98] G. Jacques-Silva, R. Drebes, J. Gerchman, J. Trindade, T. Weber,
I. Jansch-Porto, A Network-Level Distributed Fault Injector for Exper-
imental Validation of Dependable Distributed Systems, Vol. 1, 2006, pp.
421–428. doi:10.1109/COMPSAC.2006.12.

[99] Y. Jin, L. Xun, L. Ping, Y. Guangyu, Markov Reliability Model Based on
Error Classiﬁcation, in: 2012 Fifth International Symposium on Parallel
Architectures, Algorithms and Programming (PAAP), 2012, pp. 224–231.
doi:10.1109/PAAP.2012.40.

[100] A. Joshi, S. Miller, M. Whalen, M. Heimdahl, A proposal for model-based
safety analysis, Vol. 2, 2005, pp. 13 pp. Vol. 2–. doi:10.1109/DASC.2005.
1563469.

34

[101] M. Y. Jung, A Layered Approach for Identifying Systematic Faults of
Component-based Software Systems, WCOP ’11, ACM, New York, NY,
USA, 2011, pp. 17–24. doi:10.1145/2000292.2000296.

[102] N. Kandasamy, J. Hayes, B. Murray, Transparent recovery from intermit-
tent faults in time-triggered distributed systems, IEEE Transactions on
Computers 52 (2) (2003) 113–125. doi:10.1109/TC.2003.1176980.

[103] R. Karthik, N. Manikandan, Defect association and complexity prediction
by mining association and clustering rules, Vol. 7, 2010, pp. V7–569–V7–
573. doi:10.1109/ICCET.2010.5485608.

[104] F. Khomh, M. D. Penta, Y.-G. Guhneuc, G. Antoniol, An exploratory
study of the impact of antipatterns on class change- and fault-proneness,
Empirical Software Engineering 17 (3) (2012) 243–275. doi:10.1007/
s10664-011-9171-y.

[105] B. Kidwell, A decision support system for the classiﬁcation of software
coding faults: a research abstract, 2011, pp. 1158–1160. doi:10.1145/
1985793.1986028.

[106] S. D. Kim, S.-H. Chang, SOAR: An Extended Model-Based Reasoning
for Diagnosing Faults in Service-Oriented Architecture, 2009, pp. 54–61.
doi:10.1109/SERVICES-I.2009.57.

[107] A. Ko, B. Myers, Development and evaluation of a model of programming

errors, 2003, pp. 7–14. doi:10.1109/HCC.2003.1260196.

[108] M. Kumar, A. Sharma, S. Garg, A study of aspect oriented testing tech-
niques, Vol. 2, 2009, pp. 996–1001. doi:10.1109/ISIEA.2009.5356308.

[109] M. Lackovic, D. Talia, R. Tolosana-Calasanz, J. Banares, O. Rana, A
Taxonomy for the Analysis of Scientiﬁc Workﬂow Faults, in: 2010 IEEE
13th International Conference on Computational Science and Engineering
(CSE), 2010, pp. 398–403. doi:10.1109/CSE.2010.59.

[110] C. E. Landwehr, A. R. Bull, J. P. McDermott, W. S. Choi, A taxonomy
of computer program security ﬂaws, ACM Computing Surveys (CSUR)
26 (3) (1994) 211–254.

[111] J. Laski, Programming faults and errors: Towards a theory of software
incorrectness, Annals of Software Engineering 4 (1) (1997) 79–114. doi:
10.1023/A:1018966827888.

[112] M. Leeke, A. Jhumka, S. S. Anand, Towards the Design of Eﬃcient Er-
ror Detection Mechanisms for Transient Data Errors, Computer Jour-
nal 56 (6) (2013) 674–692, wOS:000319822100002. doi:10.1093/comjnl/
bxs049.

35

[113] M. Leszak, D. E. Perry, D. Stoll, A Case Study in Root Cause Defect
Analysis, Vol. 0, IEEE Computer Society, Los Alamitos, CA, USA, 2000,
p. 428. doi:10.1109/ICSE.2000.10151.

[114] H. K. Leung, Selective regression testingassumptions and fault detecting
ability, Information and Software Technology 37 (10) (1995) 531–537. doi:
10.1016/0950-5849(95)90928-N.

[115] N. Li, Z. Li, X. Sun, Classiﬁcation of Software Defect Detected by Black-
Box Testing: An Empirical Study, Vol. 2, 2010, pp. 234–240. doi:10.
1109/WCSE.2010.28.

[116] M. Li, W. Tao, D. Goldberg, I. Hsu, Y. Tamir, Design and validation of
portable communication infrastructure for fault-tolerant cluster middle-
ware, 2002, pp. 266–274. doi:10.1109/CLUSTR.2002.1137755.

[117] N. Li, Z. Li, L. Zhang, Mining Frequent Patterns from Software Defect
Repositories for Black-Box Testing, 2010, pp. 1–4. doi:10.1109/IWISA.
2010.5473578.

[118] L. Li, K. Xiaohui, L. Yuanling, X. Fei, Z. Tao, C. YiMin, Policy-based
Fault Diagnosis Technology for Web Service, 2011, pp. 827–831. doi:
10.1109/IMCCC.2011.210.

[119] Y. Liu, X. Liu, J. Wang, A software cascading faults model, Science
China Information Sciences 54 (11) (2011) 2454–2458. doi:10.1007/
s11432-011-4477-3.

[120] N. Looker, J. Xu, Assessing the dependability of OGSA middleware by
fault injection, 2003, pp. 293–302. doi:10.1109/RELDIS.2003.1238079.

[121] N. Looker, J. Xu, Assessing the dependability of SOAP RPC-based Web
services by fault injection, 2003, pp. 163–170. doi:10.1109/WORDS.2003.
1267504.

[122] N. Looker, B. Gwynne, J. Xu, M. Munro, An ontology-based approach
for determining the dependability of service-oriented architectures, 2005,
pp. 171–178. doi:10.1109/WORDS.2005.17.

[123] N. Looker, L. Burd, S. Drummond, J. Xu, M. Munro, Pedagogic data as
a basis for Web service fault models, 2005, pp. 117–125. doi:10.1109/
SOSE.2005.28.

[124] M. E. R. F. Lopes, C. H. Q. Forster, Application of human error theories
for the process improvement of Requirements Engineering, Information
Sciences 250 (2013) 142–161. doi:10.1016/j.ins.2013.07.010.

[125] G. Luo, G. Bochmann, B. Sarikaya, M. Boyer, Control-ﬂow based test-
ing of Prolog programs, 1992, pp. 104–113. doi:10.1109/ISSRE.1992.
285853.

36

[126] L. Ma, J. Tian, Web error classiﬁcation and analysis for reliability im-
provement, Journal of Systems and Software 80 (6) (2007) 795–804.
doi:10.1016/j.jss.2006.10.017.

[127] M. Mantyla, C. Lassenius, What Types of Defects Are Really Discovered
in Code Reviews?, IEEE Transactions on Software Engineering 35 (3)
(2009) 430–448. doi:10.1109/TSE.2008.71.

[128] A. Marchetto, F. Ricca, P. Tonella, Empirical Validation of a Web Fault
Taxonomy and its usage for Fault Seeding, 2007, pp. 31–38. doi:10.
1109/WSE.2007.4380241.

[129] L. Mariani, A Fault Taxonomy for Component-Based Software, Electronic
Notes in Theoretical Computer Science 82 (6) (2003) 55–65. doi:10.
1016/S1571-0661(04)81025-9.

[130] N. Mellegard, M. Staron, F. Torner, A Light-Weight Defect Classiﬁcation
Scheme for Embedded Automotive Software and Its Initial Evaluation,
2012, pp. 261–270. doi:10.1109/ISSRE.2012.15.

[131] J. Munson, A. Nikora, Toward a quantiﬁable deﬁnition of software faults,

2002, pp. 388–395. doi:10.1109/ISSRE.2002.1173299.

[132] P. Murthy, V. Kumar, T. Sharma, K. Rao, Quality Model Driven Dynamic

Analysis, 2011, pp. 360–365. doi:10.1109/COMPSAC.2011.54.

[133] A. Nadeem, Z. Malik, M. Lyu, An Automated Approach to Inheritance
and Polymorphic Testing using a VDM++ Speciﬁcation, in: IEEE Mul-
titopic Conference, 2006. INMIC ’06, 2006, pp. 224–230. doi:10.1109/
INMIC.2006.358168.

[134] N. Nagappan, L. Williams, J. Hudepohl, W. Snipes, M. Vouk, Preliminary
results on using static analysis tools for software inspection, 2004, pp. 429–
439. doi:10.1109/ISSRE.2004.30.

[135] N. K. Nagwani, S. Verma, A Comparative Study of Bug Classiﬁcation
Algorithms, International Journal of Software Engineering and Knowledge
Engineering 24 (1) (2014) 111–138, wOS:000337091600005. doi:10.1142/
S0218194014500053.

[136] N. Nagwani, S. Verma, K. Mehta, Generating taxonomic terms for soft-
ware bug classiﬁcation by utilizing topic models based on Latent Dirichlet
Allocation, in: 2013 11th International Conference on ICT and Knowl-
edge Engineering (ICT KE), 2013, pp. 1–5. doi:10.1109/ICTKE.2013.
6756268.

[137] T. Nakashima, M. Oyama, H. Hisada, N. Ishii, Analysis of software bug
causes and its prevention, Information and Software Technology 41 (15)
(1999) 1059–1068. doi:10.1016/S0950-5849(99)00049-X.

37

[138] S. Nakata, M. Sugaya, K. Kuramitsu, Fault model of foreign function
interface across diﬀerent domains, 2011, pp. 248–253. doi:10.1109/DSNW.
2011.5958850.

[139] S. K. Nath, R. Merkel, M. F. Lau, On the Improvement of a Fault
Classiﬁcation Scheme with Implications for White-box Testing, SAC ’12,
ACM, New York, NY, USA, 2012, pp. 1123–1130. doi:10.1145/2245276.
2231953.

[140] Neelofar, M. Javed, H. Mohsin, An Automated Approach for Software
Bug Classiﬁcation, in: 2012 Sixth International Conference on Complex,
Intelligent and Software Intensive Systems (CISIS), 2012, pp. 414–419.
doi:10.1109/CISIS.2012.132.

[141] M. Nesterenko, A. Arora, Dining philosophers that tolerate malicious

crashes, 2002, pp. 191–198. doi:10.1109/ICDCS.2002.1022256.

[142] A. Nigam, B. Nigam, C. Bhaisare, N. Arya, Classifying the bugs using
multi-class semi supervised support vector machine, 2012, pp. 393–397.
doi:10.1109/ICPRIME.2012.6208378.

[143] J. Oﬀutt, R. Alexander, Y. Wu, Q. Xiao, C. Hutchinson, A fault model for
subtype inheritance and polymorphism, 2001, pp. 84–93. doi:10.1109/
ISSRE.2001.989461.

[144] D. Oppenheimer, D. A. Patterson, Studying and Using Failure Data from
Large-scale Internet Services, EW 10, ACM, New York, NY, USA, 2002,
pp. 255–258. doi:10.1145/1133373.1133427.

[145] H. Owens, B. Womack, J. Gonzalez, M.J., Software error classiﬁcation

using Purify, 1996, pp. 104–113. doi:10.1109/ICSM.1996.564994.

[146] T. Oyetoyan, R. Conradi, D. Cruzes, A Comparison of Diﬀerent Defect
Measures to Identify Defect-Prone Components, 2013, pp. 181–190. doi:
10.1109/IWSM-Mensura.2013.34.

[147] L. Padgham, Z. Zhang, J. Thangarajah, T. Miller, Model-Based Test Or-
acle Generation for Automated Unit Testing of Agent Systems, IEEE
Transactions on Software Engineering 39 (9) (2013) 1230–1244. doi:
10.1109/TSE.2013.10.

[148] K. Pan, S. Kim, E. Whitehead, Bug Classiﬁcation Using Program Slicing

Metrics, 2006, pp. 31–42. doi:10.1109/SCAM.2006.6.

[149] J. Pan, P. Koopman, D. Siewiorek, A dimensionality model approach
to testing and improving software robustness, 1999, pp. 493–501. doi:
10.1109/AUTEST.1999.800419.

[150] K. Pan, S. Kim, E. J. W. Jr, Toward an understanding of bug ﬁx patterns,
Empirical Software Engineering 14 (3) (2009) 286–315. doi:10.1007/
s10664-008-9077-5.

38

[151] A. Paradkar, A quest for appropriate software fault models: Case studies
on fault detection eﬀectiveness of model-based test generation techniques,
Information and Software Technology 48 (10) (2006) 949–959. doi:10.
1016/j.infsof.2006.03.003.

[152] A. Paradkar, SALT-an integrated environment to automate generation of
function tests for APIs, 2000, pp. 304–316. doi:10.1109/ISSRE.2000.
885881.

[153] A. Paradkar, Towards model-based generation of self-priming and self-
checking conformance tests for interactive systems, Information and Soft-
ware Technology 46 (5) (2004) 315–322. doi:10.1016/j.infsof.2003.
09.005.

[154] R. Paul, F. Bastani, I.-L. Yen, V. Challagulla, Defect-based reliability
analysis for mission-critical software, 2000, pp. 439–444. doi:10.1109/
CMPSAC.2000.884761.

[155] U. Raja, All complaints are not created equal: text analysis of open source
software defect reports, Empirical Software Engineering 18 (1) (2013) 117–
138. doi:10.1007/s10664-012-9197-9.

[156] O. Raz, Research abstract for semantic anomaly detection in dynamic

data feeds with incomplete speciﬁcations, 2002, pp. 733–734.

[157] F. Ricca, P. Tonella, Web testing: a roadmap for the empirical research,

2005, pp. 63–70. doi:10.1109/WSE.2005.23.

[158] P. Robillard, T. Francois-Brosseau, Saying, ”I Am Testing,” is Enough to
Improve the Product: An Empirical Study, 2007, pp. 5–5. doi:10.1109/
ICCGI.2007.54.

[159] A. RoyChowdhury, P. Banerjee, Algorithm-based fault location and re-
covery for matrix computations on multiprocessor systems, Ieee Transac-
tions on Computers 45 (11) (1996) 1239–1247, wOS:A1996VV25400003.
doi:10.1109/12.544480.

[160] P. Runeson, C. Andersson, T. Thelin, A. Andrews, T. Berling, What Do
We Know about Defect Detection Methods?, IEEE Software 23 (3) (2006)
82–90.

[161] J. Silva, P. Prata, M. Rela, H. Madeira, Practical issues in the use of
ABFT and a new failure model, 1998, pp. 26–35. doi:10.1109/FTCS.
1998.689452.

[162] P. Straub, E. Ostertag, EDF: a formalism for describing and reusing soft-
ware experience, 1991, pp. 106–113. doi:10.1109/ISSRE.1991.145363.

[163] C. Stringfellow, A. A. Andrews, An Empirical Method for Selecting Soft-
ware Reliability Growth Models, Empirical Software Engineering 7 (4)
(2002) 319–343. doi:10.1023/A:1020515105175.

39

[164] M. Sullivan, R. Chillarege, A comparison of software defects in database
management systems and operating systems, 1992, pp. 475–484. doi:
10.1109/FTCS.1992.243586.

[165] L. Tan, A. Krings, A Hierarchical Formal Framework for Adaptive N-
variant Programs in Multi-core Systems, 2010, pp. 7–12. doi:10.1109/
ICDCSW.2010.30.

[166] K. Tewary, M. Harrold, Fault modeling using the program dependence

graph, 1994, pp. 126–135. doi:10.1109/ISSRE.1994.341362.

[167] A. Thakur, R. Iyer, Analyze-NOW-an environment for collection and anal-
ysis of failures in a network of workstations, IEEE Transactions on Reli-
ability 45 (4) (1996) 561–570. doi:10.1109/24.556579.

[168] T. Thelin, P. Runeson, C. Wohlin, T. Olsson, C. Andersson, Evalua-
tion of Usage-Based ReadingConclusions after Three Experiments, Em-
pirical Software Engineering 9 (1-2) (2004) 77–110. doi:10.1023/B:
EMSE.0000013515.86806.d4.

[169] F. Thung, D. Lo, L. Jiang, Automatic Defect Categorization, 2012, pp.

205–214. doi:10.1109/WCRE.2012.30.

[170] M. Torchiano, F. Ricca, A. Marchetto, Are web applications more defect-
prone than desktop applications?, International Journal on Software
Tools for Technology Transfer 13 (2) (2011) 151–166. doi:10.1007/
s10009-010-0182-6.

[171] K. Trivedi, R. Mansharamani, D. S. Kim, M. Grottke, M. Nambiar, Re-
covery from Failures Due to Mandelbugs in IT Systems, 2011, pp. 224–233.
doi:10.1109/PRDC.2011.34.

[172] K. Tsipenyuk, B. Chess, G. McGraw, Seven pernicious kingdoms: a tax-
onomy of software security errors, IEEE Security Privacy 3 (6) (2005)
81–84. doi:10.1109/MSP.2005.159.

[173] K. Vaidyanathan, K. Trivedi, A comprehensive model for software reju-
venation, IEEE Transactions on Dependable and Secure Computing 2 (2)
(2005) 124–137. doi:10.1109/TDSC.2005.15.

[174] S. G. Vilbergsdottir, E. T. Hvannberg, E. L.-C. Law, Assessing the
reliability, validity and acceptance of a classiﬁcation scheme of usabil-
ity problems (CUP), Journal of Systems and Software 87 (2014) 18–37.
doi:10.1016/j.jss.2013.08.014.

[175] S. G. Vilbergsdttir, E. T. Hvannberg, E. L.-C. Law, Classiﬁcation of
Usability Problems (CUP) Scheme: Augmentation and Exploitation,
NordiCHI ’06, ACM, New York, NY, USA, 2006, pp. 281–290. doi:
10.1145/1182475.1182505.

40

[176] S. Wagner, Defect Classiﬁcation and Defect Types Revisited, DEFECTS
’08, ACM, New York, NY, USA, 2008, pp. 39–40. doi:10.1145/1390817.
1390829.

[177] S. Weber, P. A. Karger, A. Paradkar, A Software Flaw Taxonomy: Aiming
Tools at Security, SESS ’05, ACM, New York, NY, USA, 2005, pp. 1–7.
doi:10.1145/1082983.1083209.

[178] L. Wei, Z. Yian, M. Chunyan, Z. Longmei, A Model Driven Approach for
Self-Healing Computing System, 2011, pp. 185–189. doi:10.1109/CIS.
2011.49.

[179] J. Widder, G. Gridling, B. Weiss, J.-P. Blanquart, Synchronous Consensus
with Mortal Byzantines, 2007, pp. 102–112. doi:10.1109/DSN.2007.91.

[180] C. Xu, S. C. Cheung, X. Ma, C. Cao, J. Lu, Adam: Identifying defects in
context-aware adaptation, Journal of Systems and Software 85 (12) (2012)
2812–2828. doi:10.1016/j.jss.2012.04.078.

[181] D. Xu, J. Ding, Prioritizing State-Based Aspect Tests, 2010, pp. 265–274.

doi:10.1109/ICST.2010.14.

[182] R.-D. Yang, C.-G. Chung, Path analysis testing of concurrent programs,
Information and Software Technology 34 (1) (1992) 43–56. doi:10.1016/
0950-5849(92)90093-5.

[183] I.-L. Yen, F. Bastani, On eﬃciently tolerating general failures in au-
tonomous decentralized multiserver systems, 1995, pp. 288–296. doi:
10.1109/ISADS.1995.398986.

[184] H. Younessi, P. Zeephongsekul, W. Bodhisuwan, A General Model of Unit
Testing Eﬃcacy, Software Quality Journal 10 (1) (2002) 69–92. doi:
10.1023/A:1015724900702.

[185] A. Yousef, Analysis and enhancement of software dynamic defect models,

2009, pp. 85–91. doi:10.1109/ICNM.2009.4907195.

[186] T.-J. Yu, V. Shen, H. Dunsmore, An analysis of several software defect
models, IEEE Transactions on Software Engineering 14 (9) (1988) 1261–
1270. doi:10.1109/32.6170.

[187] A. Zaryabi, A. B. Hamza, A neural network approach for optimal soft-
ware testing and maintenance, Neural Computing and Applications 24 (2)
(2014) 453–461. doi:10.1007/s00521-012-1251-4.

[188] H. Zhang, Research about Software Fault Injection Technology Based on

Distributed System, 2010, pp. 518–521. doi:10.1109/MVHI.2010.46.

41

[189] J. Zheng, L. Williams, N. Nagappan, W. Snipes, J. Hudepohl, M. Vouk,
On the value of static analysis for fault detection in software, IEEE
Transactions on Software Engineering 32 (4) (2006) 240–253.
doi:
10.1109/TSE.2006.38.

[190] C. Zhou, The Rule Extraction of Fault Classiﬁcation Based on Formal
Concept Analysis, in: WRI World Congress on Software Engineering,
2009. WCSE ’09, Vol. 2, 2009, pp. 155–159. doi:10.1109/WCSE.2009.
215.

42

